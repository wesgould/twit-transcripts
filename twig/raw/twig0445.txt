;FFMETADATA1
title=The Mortician's Wife
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=445
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:07.000]   It's time for Twig. This week in Google, we have a really packed show. Kevin Tofel is back
[00:00:07.000 --> 00:00:12.000]   from the IOT podcast. He's filming for Stacey Higginbotham, Jeff Jarvis, of course.
[00:00:12.000 --> 00:00:17.000]   And Joan Donovan from Data and Society. We're going to talk about the troll factories.
[00:00:17.000 --> 00:00:22.000]   We're going to talk about Twitter. What was the hot Twitter hashtag Twitter lockout?
[00:00:22.000 --> 00:00:27.000]   What that all means? We've got a big story in the New York Times saying
[00:00:27.000 --> 00:00:32.000]   Google has gotten too big, it needs to be broken up. We'll talk about all that in a whole lot more.
[00:00:32.000 --> 00:00:34.000]   Next on Twig.
[00:00:34.000 --> 00:00:39.000]   Netcast you love.
[00:00:39.000 --> 00:00:41.000]   From people you trust.
[00:00:41.000 --> 00:00:46.000]   This is Twig.
[00:00:46.000 --> 00:00:54.000]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:54.000 --> 00:00:58.000]   [Music]
[00:00:58.000 --> 00:01:06.000]   This is Twig. This week in Google, episode 445, recorded Wednesday, February 21, 2018.
[00:01:06.000 --> 00:01:09.000]   The Mortician's Wife.
[00:01:09.000 --> 00:01:12.000]   This week in Google is brought to you by Wordpress.
[00:01:12.000 --> 00:01:19.000]   Make WordPress.com your online home. Plan started just $4 a month. Go to wordpress.com/twig
[00:01:19.000 --> 00:01:23.000]   to get 15% off your brand new website today.
[00:01:23.000 --> 00:01:27.000]   And by Rocket Mortgage from Quick and Loans. Home plays a big role in your life.
[00:01:27.000 --> 00:01:30.000]   That's why Quick and Loans created Rocket Mortgage.
[00:01:30.000 --> 00:01:34.000]   It lets you apply simply and understand the entire mortgage process fully
[00:01:34.000 --> 00:01:41.000]   so you can be confident you're getting the right mortgage for you. Get started at rocketmortgage.com/twig.
[00:01:41.000 --> 00:01:47.000]   It's time for Twig. This week in Google will show where we cover the latest news from the Googleverse.
[00:01:47.000 --> 00:01:51.000]   It's Facebook, Twitter, and the media, everything, anything.
[00:01:51.000 --> 00:01:55.000]   Jeff Jarvis is here, professor of journalism at the City University of New York, author of
[00:01:55.000 --> 00:01:57.000]   "What Would Google Do? Public Parts?"
[00:01:57.000 --> 00:01:59.000]   Etcetera, Etcetera.
[00:01:59.000 --> 00:02:00.000]   Hello, Jeff.
[00:02:00.000 --> 00:02:01.000]   Hey, how are you?
[00:02:01.000 --> 00:02:05.000]   I'm great. You're in the country for two days in a row.
[00:02:05.000 --> 00:02:06.000]   I am. Yeah.
[00:02:06.000 --> 00:02:07.000]   Amazing.
[00:02:07.000 --> 00:02:09.000]   Well, here comes the cat.
[00:02:09.000 --> 00:02:10.000]   Yay.
[00:02:10.000 --> 00:02:11.000]   Yay, kitty cat.
[00:02:11.000 --> 00:02:16.000]   Hey, I'm really, we've got Stacey's in Berlin.
[00:02:16.000 --> 00:02:21.000]   So we have decided to take her place with not one but two great panelists
[00:02:21.000 --> 00:02:27.000]   to my right, Kevin Tofel, Kevin C Tofel, of giga-ohm fame for years.
[00:02:27.000 --> 00:02:31.000]   We couldn't talk to him because he was at Google briefly doing marketing.
[00:02:31.000 --> 00:02:36.000]   He's, of course, Stacey's partner on Stacey on IOT. Hello, Kevin.
[00:02:36.000 --> 00:02:39.000]   Hey, hey, it's good to be back after all this time.
[00:02:39.000 --> 00:02:42.000]   What have you been doing? We miss you.
[00:02:42.000 --> 00:02:45.000]   Lots of Netflix, Nintendo Switch.
[00:02:45.000 --> 00:02:47.000]   He's Netflix and chilling.
[00:02:47.000 --> 00:02:49.000]   Listen to the Beatles, of course.
[00:02:49.000 --> 00:02:51.000]   Absolutely, absolutely. Yeah.
[00:02:51.000 --> 00:02:55.000]   So yeah, I was over at Google for about 18 months, but I had to deal with some health issues
[00:02:55.000 --> 00:03:01.000]   last summer, which maybe focus on, you know, myself for a little bit,
[00:03:01.000 --> 00:03:05.000]   and some kind of chilling out and relaxing and now working with Stacey and,
[00:03:05.000 --> 00:03:07.000]   um, enjoying life.
[00:03:07.000 --> 00:03:11.000]   You couldn't pick a better partner and you guys do a really good job on that IOT podcast.
[00:03:11.000 --> 00:03:12.000]   Thank you.
[00:03:12.000 --> 00:03:13.000]   It's really good.
[00:03:13.000 --> 00:03:14.000]   Appreciate that.
[00:03:14.000 --> 00:03:19.000]   I'm coming back and boy, it couldn't be more timely from data and society.
[00:03:19.000 --> 00:03:22.000]   Joan Donovan, great to see you, Joan.
[00:03:22.000 --> 00:03:29.000]   Oh, she is responsible for reputation and media and fake news.
[00:03:29.000 --> 00:03:33.000]   You are in the hot seat right now at data and society.
[00:03:33.000 --> 00:03:38.000]   Yeah, it's been quite a week of troll hunting, I have to say.
[00:03:38.000 --> 00:03:43.000]   Was it an 18 alarm fire when the indictment came out last week?
[00:03:43.000 --> 00:03:49.000]   There's been that and then conspiracy after conspiracy about Parkland and then
[00:03:49.000 --> 00:03:54.000]   Twitter purge or what are they calling it? Twitter lockout last night.
[00:03:54.000 --> 00:03:56.000]   We're going to talk about that.
[00:03:56.000 --> 00:03:57.000]   Yeah.
[00:03:57.000 --> 00:04:04.000]   Weep of the bots and so really trying to understand how all of these things relate to one another.
[00:04:04.000 --> 00:04:10.000]   So after Parkland, we're told that the Russian trolls came out in force almost minutes later.
[00:04:10.000 --> 00:04:13.000]   And is it primarily Twitter?
[00:04:13.000 --> 00:04:16.000]   Do they still use Facebook?
[00:04:16.000 --> 00:04:22.000]   It feels like they're everywhere, but it's interesting because it's like gnats, right?
[00:04:22.000 --> 00:04:24.000]   They're just a swarm of flies.
[00:04:24.000 --> 00:04:33.000]   You can't really measure the impact of what it is that they do, but they do tend to intervene
[00:04:33.000 --> 00:04:39.000]   in debates that are highly polarized already and then share specific kinds of accounts
[00:04:39.000 --> 00:04:44.000]   and conspiracy theories to confuse the narrative.
[00:04:44.000 --> 00:04:51.000]   And so it's been a busy, busy week trying to map that online, but then also trying to understand
[00:04:51.000 --> 00:04:57.000]   is it really having an impact on the way that we're debating these issues or is it noise?
[00:04:57.000 --> 00:04:58.000]   Right?
[00:04:58.000 --> 00:05:01.000]   I mean, we're constantly trying to understand the problem.
[00:05:01.000 --> 00:05:02.000]   What's your view about that?
[00:05:02.000 --> 00:05:09.000]   Rasmus Nielsen at Oxford Institute has been posting stuff questioning how impactful it is.
[00:05:09.000 --> 00:05:11.000]   What do you think?
[00:05:11.000 --> 00:05:17.000]   From my point of view, it's interesting in the sense that the amplification effect when
[00:05:17.000 --> 00:05:25.000]   they're retweeting known celebrity accounts or known provocateurs accounts, they do tend
[00:05:25.000 --> 00:05:33.800]   to spread the disinformation much further, but we don't have anyone really studying the
[00:05:33.800 --> 00:05:41.600]   impact of this on people's lives and if they're even reading what is being shared, right?
[00:05:41.600 --> 00:05:45.640]   Lots and lots of things get shared on social media that no one ever really reads.
[00:05:45.640 --> 00:05:49.000]   They might glance at it and roll their eyes and keep moving.
[00:05:49.000 --> 00:05:56.000]   So we don't know the impact it's going to have on shared reality or public understanding.
[00:05:56.000 --> 00:06:01.000]   And so that's a problem, right?
[00:06:01.000 --> 00:06:05.000]   The data in society has been arguing for strategic silence.
[00:06:05.000 --> 00:06:12.000]   And there was a case today, of course, where the survivors of the terrorist attack in Parkland,
[00:06:12.000 --> 00:06:16.000]   let's call them that, were being accused of being actors.
[00:06:16.000 --> 00:06:21.000]   And that's the kind of thing you want to just not give it more air.
[00:06:21.000 --> 00:06:25.000]   But when the president's son treats it, then you've got to go on TV.
[00:06:25.000 --> 00:06:31.000]   So Ali Velsey and Stephanie Rule today on MSNBC did a great debunking end to end, but it also
[00:06:31.000 --> 00:06:34.000]   gave it more air, right?
[00:06:34.000 --> 00:06:39.000]   So first of all, I want to correct because I imply that you're in charge of fake news.
[00:06:39.000 --> 00:06:41.000]   You're not actually in charge.
[00:06:41.000 --> 00:06:43.000]   It's all Joe's fault.
[00:06:43.000 --> 00:06:47.000]   If you were a whole powerful Joe, it would make it much easier.
[00:06:47.000 --> 00:06:52.000]   But in fact, you study medium manipulation and you're the research lead for platform accountability
[00:06:52.000 --> 00:06:57.000]   at data and society, which means you're an expert on fake news and media manipulation.
[00:06:57.000 --> 00:06:58.000]   Yes.
[00:06:58.000 --> 00:07:02.240]   And we have a report that came out today that we are calling dead reckoning, navigating
[00:07:02.240 --> 00:07:09.000]   content moderation after fake news because whatever interventions we pose on fake news
[00:07:09.000 --> 00:07:16.000]   or on what are these conspiracy channel news is going to affect the nature of all content
[00:07:16.000 --> 00:07:17.000]   online.
[00:07:17.000 --> 00:07:22.000]   And so one of the things that we've been talking about is this notion of strategic silence,
[00:07:22.000 --> 00:07:27.000]   which is when should you and when should you not amplify stories?
[00:07:27.000 --> 00:07:34.000]   And so when we see the crisis actor conspiracy taking hold in places like Twitter, on the
[00:07:34.000 --> 00:07:41.000]   one hand, if you fan the flames, that's exactly the kind of attention they're looking for.
[00:07:41.000 --> 00:07:48.000]   But also because David, is it last name is Hogg or Hogue?
[00:07:48.000 --> 00:07:49.000]   Oh, two G's.
[00:07:49.000 --> 00:07:51.000]   I guess it's Hogg.
[00:07:51.000 --> 00:07:52.000]   Yeah.
[00:07:52.000 --> 00:07:59.000]   So David's way of presenting himself has been extremely poised in the media.
[00:07:59.000 --> 00:08:08.000]   And that's what makes him as a survivor, a very, very powerful advocate for gun reform in
[00:08:08.000 --> 00:08:09.000]   America.
[00:08:09.000 --> 00:08:14.000]   And that is very, very scary to certain conservatives.
[00:08:14.000 --> 00:08:19.000]   And, you know, we should point out he's one of the survivors of this.
[00:08:19.000 --> 00:08:24.000]   And he's the one that's being charged with being a crisis actor.
[00:08:24.000 --> 00:08:26.000]   Oh, they say this guy's an actor.
[00:08:26.000 --> 00:08:29.000]   Yeah, because they argue that because he used to live in California.
[00:08:29.000 --> 00:08:31.000]   So they say, see, you didn't even go to that school.
[00:08:31.000 --> 00:08:36.000]   God, meanwhile, this is this is the amazing, amazing young man who while they were locked
[00:08:36.000 --> 00:08:39.000]   up in classrooms was interviewing his classmates.
[00:08:39.000 --> 00:08:42.000]   That's a little weird, but okay.
[00:08:42.000 --> 00:08:43.000]   Well, he was.
[00:08:43.000 --> 00:08:45.000]   He was a jurisist.
[00:08:45.000 --> 00:08:51.000]   He was coming, you know, and so he was really doing due diligence.
[00:08:51.000 --> 00:08:55.000]   Yeah, there are of course immediately people saying it's a false flag that there is was
[00:08:55.000 --> 00:08:57.000]   no shooting, things like that.
[00:08:57.000 --> 00:09:06.000]   So normally that stuff, I mean, you know, this has been not just in the Twitter era or the
[00:09:06.000 --> 00:09:08.000]   Internet era, but always.
[00:09:08.000 --> 00:09:11.000]   And normally you just ignore this stuff.
[00:09:11.000 --> 00:09:15.000]   And frankly, I guess the first thing I'd say is why does Twitter, what?
[00:09:15.000 --> 00:09:19.000]   I mean, is it just the media that pays attention to Twitter?
[00:09:19.000 --> 00:09:25.000]   Or only has, you know, 300 million active users.
[00:09:25.000 --> 00:09:29.000]   I think the vast majority of Americans that are not on Twitter.
[00:09:29.000 --> 00:09:32.000]   So this was also being shared on Facebook and through YouTube.
[00:09:32.000 --> 00:09:36.000]   So you think that every war is in the gateway pendant?
[00:09:36.000 --> 00:09:42.000]   So all these people in aggregate reach a majority of Americans, you think?
[00:09:42.000 --> 00:09:43.000]   No, definitely not.
[00:09:43.000 --> 00:09:48.000]   I think what happens is you have this narrative of a crisis actor that starts to be a little
[00:09:48.000 --> 00:09:53.000]   bit more than a person that starts probably before Sandy Hook, but that's the first time
[00:09:53.000 --> 00:09:54.000]   that I've heard of it.
[00:09:54.000 --> 00:09:55.000]   Yeah, I remember hearing.
[00:09:55.000 --> 00:09:58.000]   And then it gets replayed through every mass shooting.
[00:09:58.000 --> 00:09:59.000]   Right.
[00:09:59.000 --> 00:10:03.000]   And so you no longer have to explain generally what you mean when you're talking about the
[00:10:03.000 --> 00:10:06.000]   Christ, the conspiracy theory of the crisis actor.
[00:10:06.000 --> 00:10:14.000]   Now what you have to combat is debunking these stories if you need to debunk them.
[00:10:14.000 --> 00:10:19.000]   And then also I think that right now those students that are survivors that are turning
[00:10:19.000 --> 00:10:22.000]   into advocates and organizers are really under attack.
[00:10:22.000 --> 00:10:28.000]   And showing, the media showing as well as public support showing through Twitter that
[00:10:28.000 --> 00:10:36.000]   they believe those students and that they believe in their cause is one way to combat
[00:10:36.000 --> 00:10:38.000]   some of this disinformation.
[00:10:38.000 --> 00:10:42.000]   And I think it's really important for people to stand up and say that they believe these
[00:10:42.000 --> 00:10:46.000]   students because these students are not calling for radical gun reform.
[00:10:46.000 --> 00:10:48.000]   It's very, very monolithic.
[00:10:48.000 --> 00:10:52.000]   I want to set aside the politics of it because this is a technology show.
[00:10:52.000 --> 00:11:00.000]   But what's interesting to us as a technology show is what is the Internet's responsibility?
[00:11:00.000 --> 00:11:03.000]   What are these companies' responsibilities?
[00:11:03.000 --> 00:11:05.000]   What is the power of the Internet?
[00:11:05.000 --> 00:11:11.000]   And so, but I gather it's enough to plant the seed and then all some, you know, I just
[00:11:11.000 --> 00:11:15.000]   imagine a small town where there's a couple of people who, you know, they're on the Internet.
[00:11:15.000 --> 00:11:19.000]   They see that, then they go to the coffee shop and they tell people in the coffee shop,
[00:11:19.000 --> 00:11:23.000]   you know, that guy was a crisis actor and that it spreads that way.
[00:11:23.000 --> 00:11:29.000]   It spreads beyond the Internet to the majority of people not using technology, I would assume.
[00:11:29.000 --> 00:11:33.000]   Yeah, it does and it's a provocative story to tell.
[00:11:33.000 --> 00:11:35.000]   But what is dangerous about this?
[00:11:35.000 --> 00:11:37.000]   We love these kinds of conspiracy theories.
[00:11:37.000 --> 00:11:42.000]   Oh, it's true, but this kid is a teenager and his real name is being used and he's being
[00:11:42.000 --> 00:11:45.000]   smeared all over the Internet.
[00:11:45.000 --> 00:11:50.000]   So now when you search for him, not only do you get his name and his likeness, but you
[00:11:50.000 --> 00:11:56.000]   also get these conspiracy theories that are disinformation about him at the which constitutes
[00:11:56.000 --> 00:11:57.000]   harassment.
[00:11:57.000 --> 00:12:03.000]   And so the videos that are popping up on YouTube and being shared through Facebook and being
[00:12:03.000 --> 00:12:11.000]   shared through Twitter are a kind of networked harassment that will plague him until we
[00:12:11.000 --> 00:12:17.000]   come up with some kind of regulatory framework that will allow him to get his name back.
[00:12:17.000 --> 00:12:19.000]   Welcome to the Internet.
[00:12:19.000 --> 00:12:20.000]   I have a question.
[00:12:20.000 --> 00:12:24.600]   I don't disagree with anything you just said, Joan, but you mentioned how his real name is
[00:12:24.600 --> 00:12:26.000]   out there and he's a teenager.
[00:12:26.000 --> 00:12:29.000]   Does he put his own videos out on YouTube?
[00:12:29.000 --> 00:12:34.000]   Do you prior put his name out there?
[00:12:34.000 --> 00:12:36.000]   To be honest with you, I don't know.
[00:12:36.000 --> 00:12:42.400]   I haven't investigated in his own social media, but teenagers, the way that they use social
[00:12:42.400 --> 00:12:45.000]   media, of course, they do have a public presence.
[00:12:45.000 --> 00:12:51.000]   And as we grow older, of course, things happen and things get tied to our names.
[00:12:51.000 --> 00:12:56.200]   But the fact that there's no remedy for this and there's no pushback and that there's
[00:12:56.200 --> 00:13:03.960]   no formal process that this teenager can go through in order to get his name and reputation
[00:13:03.960 --> 00:13:09.120]   back, I think it's important that we start to think about what is the lifelong effects
[00:13:09.120 --> 00:13:10.120]   that he's going to suffer.
[00:13:10.120 --> 00:13:11.120]   Well, I agree.
[00:13:11.120 --> 00:13:12.520]   But what would be the mechanism of that?
[00:13:12.520 --> 00:13:15.160]   I don't see any-- I mean, that's the right to be forgotten.
[00:13:15.160 --> 00:13:16.160]   That's the right to be forgotten.
[00:13:16.160 --> 00:13:20.280]   But ironically, hearing you and Joan talk about this, the right to be forgotten is about
[00:13:20.280 --> 00:13:23.320]   taking down facts about you that you don't like.
[00:13:23.320 --> 00:13:24.320]   This is about taking down--
[00:13:24.320 --> 00:13:25.320]   Why?
[00:13:25.320 --> 00:13:26.320]   Because of that.
[00:13:26.320 --> 00:13:27.480]   I don't know how you would do this.
[00:13:27.480 --> 00:13:29.600]   I don't know how it would even begin.
[00:13:29.600 --> 00:13:36.200]   I mean, we all-- I mean, look, I mean, it's worse, of course, he's a 17-year-old.
[00:13:36.200 --> 00:13:40.640]   But it happens all the time on the internet that people are at tarred with a brush that
[00:13:40.640 --> 00:13:43.400]   and are stained in a way that there's no way to fix it.
[00:13:43.400 --> 00:13:45.320]   That happens all the time.
[00:13:45.320 --> 00:13:50.320]   And there's a risk business and reputation defenders and things like that that are completely
[00:13:50.320 --> 00:13:51.320]   worthless.
[00:13:51.320 --> 00:13:55.000]   The argument that I made-- and this-- I think this is why it's a very big story today.
[00:13:55.000 --> 00:13:56.000]   There's three things together here.
[00:13:56.000 --> 00:14:01.000]   The argument I made about this with Google in the spring after the election was that
[00:14:01.000 --> 00:14:02.360]   you're being manipulated.
[00:14:02.360 --> 00:14:06.400]   You have to understand the manipulation, measure it, be transparent about it, and counteract
[00:14:06.400 --> 00:14:07.400]   it.
[00:14:07.400 --> 00:14:11.360]   And so when Google announced they were going to account for the reliability, authority,
[00:14:11.360 --> 00:14:15.400]   and quality of sources in search ranking, that was them doing that.
[00:14:15.400 --> 00:14:18.960]   Now you have Facebook that is also trying to say, we're going to throw out all those
[00:14:18.960 --> 00:14:22.120]   public stuffs, getting in trouble, and just bring back the quality stuff.
[00:14:22.120 --> 00:14:27.360]   And now today you have Twitter, which we'll get to in a few minutes, doing a block out
[00:14:27.360 --> 00:14:29.040]   of Russian bots.
[00:14:29.040 --> 00:14:35.240]   So all three of the platforms are beginning to make decisions about quality and credibility.
[00:14:35.240 --> 00:14:44.120]   Is it nice to say that really the burden is on us as people to just not believe this
[00:14:44.120 --> 00:14:45.420]   crap?
[00:14:45.420 --> 00:14:48.880]   If we can give people more signals to that enable them to make that decision well, which
[00:14:48.880 --> 00:14:50.880]   is where a new kind of journey comes at.
[00:14:50.880 --> 00:14:54.760]   But I would guess they're well predisposed towards it.
[00:14:54.760 --> 00:14:59.120]   It's not that it's like if I if I told you, oh, you know, that guy's an actor, nobody
[00:14:59.120 --> 00:15:03.000]   was shot in Parkland.
[00:15:03.000 --> 00:15:04.840]   A normal person was they get out of here.
[00:15:04.840 --> 00:15:05.840]   You're nuts.
[00:15:05.840 --> 00:15:06.840]   Right.
[00:15:06.840 --> 00:15:07.840]   Right.
[00:15:07.840 --> 00:15:09.360]   This is where the lack of trust to media goes.
[00:15:09.360 --> 00:15:10.360]   Sorry, John.
[00:15:10.360 --> 00:15:15.440]   Yeah, no, I agree that there's a lack of trust, but also you were hosting content that is
[00:15:15.440 --> 00:15:17.000]   harassing someone.
[00:15:17.000 --> 00:15:18.000]   Right?
[00:15:18.000 --> 00:15:20.440]   And we don't all get away with that.
[00:15:20.440 --> 00:15:27.320]   And so there has to be a new way of thinking about disinformation in particular because
[00:15:27.320 --> 00:15:36.040]   of the way that it's being used against platforms and to spread information that is not it's
[00:15:36.040 --> 00:15:39.320]   what's what's difficult about conspiracy theories.
[00:15:39.320 --> 00:15:41.240]   There's no falsifiability.
[00:15:41.240 --> 00:15:42.240]   There's no evidence.
[00:15:42.240 --> 00:15:43.240]   You can't prove the negative.
[00:15:43.240 --> 00:15:44.240]   Yes.
[00:15:44.240 --> 00:15:45.440]   That's always been the problem.
[00:15:45.440 --> 00:15:46.440]   That's life.
[00:15:46.440 --> 00:15:47.440]   Yes.
[00:15:47.440 --> 00:15:54.720]   So in these situations, when you know that you have hundreds of hours of content accumulating
[00:15:54.720 --> 00:15:59.720]   in your platform and you know that this this teenager is being harassed and he's being
[00:15:59.720 --> 00:16:07.960]   harassed because you are providing the distribution system in the archive for this content, then,
[00:16:07.960 --> 00:16:13.000]   you know, maybe he becomes the first person to sue one of these platforms for digital
[00:16:13.000 --> 00:16:14.000]   harms.
[00:16:14.000 --> 00:16:20.160]   Well, the case of Google, YouTube is one matter, but Google search, it's not really, it wouldn't
[00:16:20.160 --> 00:16:24.280]   be the platformer to be the originating site of where it's come from.
[00:16:24.280 --> 00:16:31.360]   Google will argue strenuously that you should be able to find anything including bad stuff.
[00:16:31.360 --> 00:16:35.520]   If you want to say who is out there saying the Holocaust is a hoax, you should be able
[00:16:35.520 --> 00:16:36.520]   to find that.
[00:16:36.520 --> 00:16:39.520]   Now, if you ask us the Holocaust real, you shouldn't find them.
[00:16:39.520 --> 00:16:44.000]   But if you want to find the bad actors, Google will say you should still be able to find
[00:16:44.000 --> 00:16:47.320]   them because that's part of their job so that you can do your work job.
[00:16:47.320 --> 00:16:49.320]   Yeah, I understand.
[00:16:49.320 --> 00:16:53.320]   But I do not like being awash in disinformation.
[00:16:53.320 --> 00:17:00.240]   And I know that when it's targeting a specific person, that that is a solvable and bounded
[00:17:00.240 --> 00:17:01.240]   problem.
[00:17:01.240 --> 00:17:02.240]   Right?
[00:17:02.240 --> 00:17:09.480]   And so the evidence is in the metadata and these platform companies do know who's
[00:17:09.480 --> 00:17:10.760]   uploading this content.
[00:17:10.760 --> 00:17:13.480]   They do know where it's being linked to.
[00:17:13.480 --> 00:17:20.360]   And they do have systems in place to deal with these issues, especially when it has
[00:17:20.360 --> 00:17:24.200]   to do with the network harassment of an individual.
[00:17:24.200 --> 00:17:29.760]   And ultimately, this kind of harassment will not stop.
[00:17:29.760 --> 00:17:31.520]   This will go on for years.
[00:17:31.520 --> 00:17:34.680]   It's going to go on for this is trolls be trolling.
[00:17:34.680 --> 00:17:35.960]   This is trolls be trolling.
[00:17:35.960 --> 00:17:37.960]   I feel bad for David.
[00:17:37.960 --> 00:17:38.960]   Absolutely.
[00:17:38.960 --> 00:17:43.360]   And he's on the side of the angels doing the right thing and it's a sad thing.
[00:17:43.360 --> 00:17:46.240]   But that's what happens nowadays.
[00:17:46.240 --> 00:17:52.360]   But it's not that we can't think about another internet or another way of interfacing with
[00:17:52.360 --> 00:17:54.840]   the internet and these platform companies.
[00:17:54.840 --> 00:17:58.200]   Well, I'm open to the idea, but what would you propose?
[00:17:58.200 --> 00:18:01.800]   Well, I don't work there, but if they want to.
[00:18:01.800 --> 00:18:05.120]   I mean, really though, I think this is a challenging thing.
[00:18:05.120 --> 00:18:06.120]   I mean, we've.
[00:18:06.120 --> 00:18:12.800]   We've, you know, railed against the right to be forgotten.
[00:18:12.800 --> 00:18:16.600]   The internet, I mean, look, I hate trolls as much as anybody.
[00:18:16.600 --> 00:18:19.000]   I've been trolled for a long time.
[00:18:19.000 --> 00:18:21.360]   But trolls be trolling.
[00:18:21.360 --> 00:18:23.160]   What are you going to do about it?
[00:18:23.160 --> 00:18:25.160]   I don't know how you stop it.
[00:18:25.160 --> 00:18:26.680]   This is, this is a problem.
[00:18:26.680 --> 00:18:30.720]   If we believe that the trolls own the internet and that anyone should be able to do whatever
[00:18:30.720 --> 00:18:35.440]   they want to do at any moment, what's, and what's going to end up happening is that you
[00:18:35.440 --> 00:18:42.560]   were going to continue to accumulate so much garbage that YouTube is going to become landfill.
[00:18:42.560 --> 00:18:43.560]   It's true.
[00:18:43.560 --> 00:18:47.360]   I mean, this is any, any public network.
[00:18:47.360 --> 00:18:50.920]   If you don't maintain it, we, for instance, we have many, many mods in our chat room and
[00:18:50.920 --> 00:18:55.760]   lots of technical tools to prevent trolling in our chat room.
[00:18:55.760 --> 00:18:58.840]   And you have to be on it constantly to prevent that.
[00:18:58.840 --> 00:19:02.120]   Well, so I think you're both headed to maybe a new standard here.
[00:19:02.120 --> 00:19:05.760]   I think we've forgotten, again, as a different thing because that's about erasing or rewriting
[00:19:05.760 --> 00:19:07.120]   history.
[00:19:07.120 --> 00:19:11.040]   This is about Joe, what Joan said is you're getting rid of harassment and you with your
[00:19:11.040 --> 00:19:12.960]   mods are getting rid of harassment.
[00:19:12.960 --> 00:19:18.640]   So part, you know, Twitter has a very low bar standard right now, which is child porn
[00:19:18.640 --> 00:19:20.400]   and such, right?
[00:19:20.400 --> 00:19:23.440]   And Twitter, I think, is showing that they want to raise that bar.
[00:19:23.440 --> 00:19:26.560]   So the question is, where's the bar?
[00:19:26.560 --> 00:19:30.680]   And there have been many complaints that women particularly get harassed and threatened
[00:19:30.680 --> 00:19:31.920]   on Twitter.
[00:19:31.920 --> 00:19:39.000]   If harassment and threatening no longer is it acceptable behavior on Twitter, is that
[00:19:39.000 --> 00:19:40.000]   a good standard?
[00:19:40.000 --> 00:19:41.000]   And is that something that's doable?
[00:19:41.000 --> 00:19:43.560]   Is there a harassment in the eye of the beholder?
[00:19:43.560 --> 00:19:45.000]   Well, that's the question.
[00:19:45.000 --> 00:19:50.280]   If somebody responds to a presidential tweet saying you're a nitwit, is that harassment?
[00:19:50.280 --> 00:19:53.000]   Is that person blocked on the Twitter?
[00:19:53.000 --> 00:20:03.000]   Well, I think that in some instances, Twitter has allowed for bad behavior from people that
[00:20:03.000 --> 00:20:07.240]   are of a certain status or of a certain distinction.
[00:20:07.240 --> 00:20:12.600]   Here we have a unique case that we can look at and say, here's a minor and this is someone
[00:20:12.600 --> 00:20:14.480]   that we should strive to protect.
[00:20:14.480 --> 00:20:15.480]   I agree.
[00:20:15.480 --> 00:20:23.360]   There's a lot of things that we should create in terms of service around, that there should
[00:20:23.360 --> 00:20:26.440]   be a process for making these appeals.
[00:20:26.440 --> 00:20:34.520]   And there should be rapid attention to these disinformation campaigns because part of them,
[00:20:34.520 --> 00:20:40.160]   if you live in this world like me and my research team do, there's so many that don't hit this
[00:20:40.160 --> 00:20:41.160]   strongly.
[00:20:41.160 --> 00:20:46.080]   There's probably hundreds of Parkland conspiracy theories out there.
[00:20:46.080 --> 00:20:49.680]   There's only a few that have been picked up and amplified.
[00:20:49.680 --> 00:20:52.120]   So pick the most damaging ones.
[00:20:52.120 --> 00:20:56.960]   And actually, I think a very good job has been done to refute the conspiracy theory about
[00:20:56.960 --> 00:20:57.960]   hog.
[00:20:57.960 --> 00:20:59.720]   Yeah, but this is what I'm talking about.
[00:20:59.720 --> 00:21:04.240]   But I mean, if I search for David Hogg now on Twitter, I mean, on Google, all I find is
[00:21:04.240 --> 00:21:06.120]   article saying he's not a crisis actor.
[00:21:06.120 --> 00:21:07.120]   That's not.
[00:21:07.120 --> 00:21:10.160]   If you're people who don't believe media, the refugee business doesn't matter.
[00:21:10.160 --> 00:21:14.880]   The Twitter fact checking flag, in fact, drove more traffic to those things.
[00:21:14.880 --> 00:21:15.880]   They expect.
[00:21:15.880 --> 00:21:16.880]   Yeah, they're Facebook.
[00:21:16.880 --> 00:21:17.880]   Facebook.
[00:21:17.880 --> 00:21:18.880]   That's actually the problem.
[00:21:18.880 --> 00:21:23.600]   This is partly a technology problem and partly just a human problem.
[00:21:23.600 --> 00:21:26.360]   Just the other day, folks in the chat room noticed it too.
[00:21:26.360 --> 00:21:33.080]   CNN went back to a Trump supporters house and said, look, it's been found that you were
[00:21:33.080 --> 00:21:36.880]   actually interfacing with Russian trolls, known trolls.
[00:21:36.880 --> 00:21:41.320]   You found this out, you were organizing events with them and so on and so forth.
[00:21:41.320 --> 00:21:43.760]   This person was like, no, that's all BS.
[00:21:43.760 --> 00:21:45.320]   No, no, no, you don't believe you.
[00:21:45.320 --> 00:21:46.320]   You're fake news.
[00:21:46.320 --> 00:21:47.320]   Yeah.
[00:21:47.320 --> 00:21:48.320]   Exactly.
[00:21:48.320 --> 00:21:49.320]   It's a very good piece to be shown.
[00:21:49.320 --> 00:21:50.640]   Maybe it's the other half of it.
[00:21:50.640 --> 00:21:52.720]   So how do you fix that?
[00:21:52.720 --> 00:21:54.720]   You can't with technology.
[00:21:54.720 --> 00:22:00.040]   We have to recognize that the internet is Times Square.
[00:22:00.040 --> 00:22:05.320]   You know, I had an editor today on Twitter said, well, we should be editing everything.
[00:22:05.320 --> 00:22:06.320]   You can't.
[00:22:06.320 --> 00:22:07.400]   It's an open conversation.
[00:22:07.400 --> 00:22:11.480]   So we've got to create, I don't want to go to a religious hierarchy.
[00:22:11.480 --> 00:22:14.240]   I don't want to go to a pay walled quality world.
[00:22:14.240 --> 00:22:17.600]   But I don't want to imagine that, you know, Gee, if we just train people, everything
[00:22:17.600 --> 00:22:18.600]   is okay.
[00:22:18.600 --> 00:22:22.240]   We've got a big trust issue to deal with and journalism has a huge trust problem.
[00:22:22.240 --> 00:22:25.480]   But at some point, if you recognize that it's Times Square, you're not going to listen
[00:22:25.480 --> 00:22:28.080]   to everybody on Times Square and think that they're right about everything.
[00:22:28.080 --> 00:22:32.400]   You're going to have some mechanism to judge.
[00:22:32.400 --> 00:22:37.360]   And the other point that we should make is that these are known information pathways
[00:22:37.360 --> 00:22:45.720]   where there are content creators that make and monetize this content, knowing full well
[00:22:45.720 --> 00:22:47.480]   that it is false.
[00:22:47.480 --> 00:22:54.400]   Sometimes they'll slap a, this is satire on it, on the content to make it something that
[00:22:54.400 --> 00:22:57.560]   cannot be flagged and taken down.
[00:22:57.560 --> 00:23:01.560]   And so they know what they're doing and they know why they're doing it and their economic
[00:23:01.560 --> 00:23:03.480]   incentives for it.
[00:23:03.480 --> 00:23:08.400]   And so it's not just true believers that are doing it because they're creating user-generated
[00:23:08.400 --> 00:23:09.400]   content.
[00:23:09.400 --> 00:23:16.720]   These are content creators and, you know, quote unquote media organizations that are
[00:23:16.720 --> 00:23:18.840]   making money off of this.
[00:23:18.840 --> 00:23:20.720]   Yeah, I like your dead reckoning.
[00:23:20.720 --> 00:23:25.880]   This is opposed to data and society where really what you're saying, you're urging the
[00:23:25.880 --> 00:23:34.640]   platforms to, well, news platforms to ignore this sort of, not to not spread it by unless
[00:23:34.640 --> 00:23:38.240]   it's passed over a line where you kind of have to, right, Joan?
[00:23:38.240 --> 00:23:39.240]   Yeah.
[00:23:39.240 --> 00:23:44.520]   And one of the findings from this report is that what we're seeing on these platform
[00:23:44.520 --> 00:23:50.960]   companies is that we're at a moment where every action or every direction forward seems
[00:23:50.960 --> 00:23:51.960]   impossible.
[00:23:51.960 --> 00:23:57.400]   And so it doesn't surprise me to hear that you're like, how would we ever solve something?
[00:23:57.400 --> 00:24:03.680]   And I think that my approach or the approach we should take is reason in terms of service.
[00:24:03.680 --> 00:24:10.440]   But then also looking at case by case, does this information need to come offline and can
[00:24:10.440 --> 00:24:17.840]   you create a process by which people can make those appeals and those appeals become transparent?
[00:24:17.840 --> 00:24:23.640]   But there has always been, I mean, Jeff and I are old enough to remember mainstream media.
[00:24:23.640 --> 00:24:26.440]   There was always a maybe you too, Kevin.
[00:24:26.440 --> 00:24:30.800]   There was always, I mean, you didn't say the names of minors.
[00:24:30.800 --> 00:24:31.800]   I mean, there were always.
[00:24:31.800 --> 00:24:33.480]   You didn't talk about suicide.
[00:24:33.480 --> 00:24:38.000]   There were understood rules that just were, I mean, I hate to use the phrase, but they
[00:24:38.000 --> 00:24:40.200]   were gentlemen's agreements.
[00:24:40.200 --> 00:24:41.960]   You know, I agree with you.
[00:24:41.960 --> 00:24:43.520]   Those are called standards.
[00:24:43.520 --> 00:24:44.520]   Standards.
[00:24:44.520 --> 00:24:45.520]   There's the better word.
[00:24:45.520 --> 00:24:49.320]   There were standards and there were even there used to be in the networks.
[00:24:49.320 --> 00:24:55.080]   Whole departments called standards and practices that would enforce those standards.
[00:24:55.080 --> 00:24:59.280]   And that was a good thing, but we don't live in a world of mainstream media anymore.
[00:24:59.280 --> 00:25:04.480]   And no, even the mainstream media is not behaving like the mainstream media.
[00:25:04.480 --> 00:25:10.880]   The problem with broadcast that we've really failed to reckon with is, you know, that this,
[00:25:10.880 --> 00:25:14.040]   there is an incentive to create this content because it makes money.
[00:25:14.040 --> 00:25:17.680]   There's an incentive to create this content because it creates political opportunities.
[00:25:17.680 --> 00:25:20.280]   And you've got a 24 hour news cycle and you got to fill that.
[00:25:20.280 --> 00:25:21.280]   Yeah.
[00:25:21.280 --> 00:25:23.000]   And we have an ethic that says openness, openness.
[00:25:23.000 --> 00:25:27.440]   I mean, after I just found this clear, after Robin Williams death, there was a 10% spike
[00:25:27.440 --> 00:25:28.440]   in suicides.
[00:25:28.440 --> 00:25:29.440]   Yeah.
[00:25:29.440 --> 00:25:30.640]   So you don't say it, right?
[00:25:30.640 --> 00:25:31.640]   Yeah.
[00:25:31.640 --> 00:25:36.240]   And that's part of the knowing what to amplify and what not to amplify and what are the public
[00:25:36.240 --> 00:25:39.760]   health benefits of amplifying certain information is.
[00:25:39.760 --> 00:25:44.600]   But I'm kind of with Kevin that it's also a human problem is pretty.
[00:25:44.600 --> 00:25:45.600]   Oh, yeah.
[00:25:45.600 --> 00:25:51.880]   I think it's kind of to blame it to say, well, it's all up to you platforms because more
[00:25:51.880 --> 00:25:53.400]   just too stupid.
[00:25:53.400 --> 00:25:54.400]   Yeah.
[00:25:54.400 --> 00:25:58.560]   Well, this is the other intervention that these platforms are coming up with is that
[00:25:58.560 --> 00:26:03.520]   they're a third party fact checking organizations as well as media literacy organizations that
[00:26:03.520 --> 00:26:08.640]   are under resourced and they don't have enough personnel to deal with the scale of the problem
[00:26:08.640 --> 00:26:15.480]   and implement sort of national digital literacy program that would cover a lot of what it
[00:26:15.480 --> 00:26:18.760]   means to debunk and read news critically these days.
[00:26:18.760 --> 00:26:19.760]   Is it a bigger?
[00:26:19.760 --> 00:26:22.200]   Is the problem bigger than it used to be?
[00:26:22.200 --> 00:26:23.200]   Like measurably.
[00:26:23.200 --> 00:26:27.360]   I would think it's because, well, it has to be because I have a job now.
[00:26:27.360 --> 00:26:28.360]   Right?
[00:26:28.360 --> 00:26:35.240]   And we have a team and we have lots of very smart people across the world working on these
[00:26:35.240 --> 00:26:40.040]   issues in every country and it isn't just political disinformation.
[00:26:40.040 --> 00:26:42.200]   It isn't just foreign spies.
[00:26:42.200 --> 00:26:52.000]   There are real incentives to use the broadcast capacity of the internet, which as an infrastructure
[00:26:52.000 --> 00:26:55.800]   has rapidly scaled without any regulation.
[00:26:55.800 --> 00:27:00.560]   It used to be that you, you know, I mean, you remember the days of downloading a JPEG.
[00:27:00.560 --> 00:27:01.560]   Right?
[00:27:01.560 --> 00:27:02.560]   Right.
[00:27:02.560 --> 00:27:04.560]   And it would sometimes take, you know, five minutes to go.
[00:27:04.560 --> 00:27:05.560]   That's not any more.
[00:27:05.560 --> 00:27:06.560]   Yeah.
[00:27:06.560 --> 00:27:07.560]   A picture.
[00:27:07.560 --> 00:27:14.040]   And so as it's become easier to transfer data and we've been, we've moved into this
[00:27:14.040 --> 00:27:19.720]   mobile web moment, we haven't thought about what are the educational campaigns and the
[00:27:19.720 --> 00:27:25.320]   necessary tools that we all will need to be able to be critical.
[00:27:25.320 --> 00:27:27.160]   It's happened so fast.
[00:27:27.160 --> 00:27:28.520]   But let me also work.
[00:27:28.520 --> 00:27:30.320]   You just not prepared for it.
[00:27:30.320 --> 00:27:31.320]   Sorry.
[00:27:31.320 --> 00:27:36.160]   I'm doing research now on kind of looking at the post mass world.
[00:27:36.160 --> 00:27:37.720]   And so I'm reading a lot of stuff.
[00:27:37.720 --> 00:27:40.720]   I just reread losing ourselves to death and other things.
[00:27:40.720 --> 00:27:46.320]   About 1960 to 1990, you know, so the TV.
[00:27:46.320 --> 00:27:47.320]   Oh, TV.
[00:27:47.320 --> 00:27:49.520]   People like Simon saw this coming though.
[00:27:49.520 --> 00:27:51.520]   Well, but they said it about TV.
[00:27:51.520 --> 00:27:52.520]   Right.
[00:27:52.520 --> 00:27:53.520]   Right.
[00:27:53.520 --> 00:27:54.520]   And they decried television.
[00:27:54.520 --> 00:27:55.520]   They decried what was going to happen.
[00:27:55.520 --> 00:28:00.880]   If you look at about 1960 after, you know, a, a, a, mustn't a decade of very popular televisions
[00:28:00.880 --> 00:28:01.880]   under our belt.
[00:28:01.880 --> 00:28:05.000]   There's a huge panic that goes on around that.
[00:28:05.000 --> 00:28:09.960]   And I think mass media has empowered this, but we burned witches in the 1600s too.
[00:28:09.960 --> 00:28:10.960]   Yeah, exactly.
[00:28:10.960 --> 00:28:15.200]   And I think that, that, um, uh, you know, the problem, Joan, that I have with this notion
[00:28:15.200 --> 00:28:19.840]   of media literacy and, and, and I get in trouble for this, but it's media centric.
[00:28:19.840 --> 00:28:23.160]   It's a little paternalistic to say, if you just read all of our media and if you understood
[00:28:23.160 --> 00:28:25.040]   how to do our media, then everything will be okay.
[00:28:25.040 --> 00:28:26.040]   Maybe there's something wrong with our media.
[00:28:26.040 --> 00:28:30.000]   And we're going to start there and then we have more respect to the public.
[00:28:30.000 --> 00:28:34.920]   You listen to these Margaret Sullivan's column this week about the teenagers in, in, in,
[00:28:34.920 --> 00:28:37.160]   in, in Parkland is really wonderful.
[00:28:37.160 --> 00:28:41.760]   You hear the narrative is, oh, teenagers are all apathetic and they're all illiterate and
[00:28:41.760 --> 00:28:43.680]   they can't spell and they can't speak.
[00:28:43.680 --> 00:28:48.960]   The teenagers of Parkland disabuse every such notion and one made them this way.
[00:28:48.960 --> 00:28:50.160]   What made them savvy?
[00:28:50.160 --> 00:28:56.120]   The internet did by the way, right on the front of that article, a video of David Hogg.
[00:28:56.120 --> 00:28:57.360]   Mm hmm.
[00:28:57.360 --> 00:28:58.360]   Yeah.
[00:28:58.360 --> 00:29:06.840]   I admire these students because they as much as anybody knew he knew that come that be
[00:29:06.840 --> 00:29:10.240]   becoming the public face of this would expose him to this.
[00:29:10.240 --> 00:29:12.000]   Maybe he didn't, but I would.
[00:29:12.000 --> 00:29:13.240]   He's being very mature about it.
[00:29:13.240 --> 00:29:16.680]   Now I heard him today on TV saying what about all this hoax about him?
[00:29:16.680 --> 00:29:18.600]   He said, well, it did job for me.
[00:29:18.600 --> 00:29:20.080]   It doubled my followers today.
[00:29:20.080 --> 00:29:21.080]   Yeah.
[00:29:21.080 --> 00:29:22.080]   Yeah.
[00:29:22.080 --> 00:29:23.080]   Yeah.
[00:29:23.080 --> 00:29:25.080]   So he has no point about mass media.
[00:29:25.080 --> 00:29:31.360]   I think that we also have to remember that we learned how to watch TV, War of the Worlds.
[00:29:31.360 --> 00:29:33.720]   We learned how to listen to the radio.
[00:29:33.720 --> 00:29:34.720]   That's right.
[00:29:34.720 --> 00:29:36.200]   But there's a format to it.
[00:29:36.200 --> 00:29:42.960]   And one of the Jesse Daniels, who's a researcher that wrote the book Cyber Racism, has a really
[00:29:42.960 --> 00:29:49.240]   beautiful earlier paper from maybe 10 years about using a usability study where she looked
[00:29:49.240 --> 00:29:54.000]   at how students learn to read the web and she was looking at these websites called cloaked
[00:29:54.000 --> 00:29:55.000]   websites.
[00:29:55.000 --> 00:30:00.320]   So back 10 years ago, if you search for Martin Luther King, you actually got a white nationalist
[00:30:00.320 --> 00:30:04.680]   website that had bought the domain and then was serving disinformation about Martin Luther
[00:30:04.680 --> 00:30:05.840]   King.
[00:30:05.840 --> 00:30:10.920]   And so students were taught to read into the infrastructure of the website, into the way
[00:30:10.920 --> 00:30:13.880]   that the content was presented.
[00:30:13.880 --> 00:30:19.960]   And in doing so, they were able to assess the quality of the news and the quality of
[00:30:19.960 --> 00:30:26.360]   the presentation in these websites and make an assessment of them.
[00:30:26.360 --> 00:30:31.360]   And so part of the way that we're doing media right now, though, is we have these generalized
[00:30:31.360 --> 00:30:34.640]   templates, everything starts to look the same.
[00:30:34.640 --> 00:30:42.240]   And so it's very difficult to tell the difference between legitimate news and what are these
[00:30:42.240 --> 00:30:46.400]   like incentivized monetized news outlets.
[00:30:46.400 --> 00:30:49.480]   The new is true in the news outlets themselves.
[00:30:49.480 --> 00:30:54.280]   I mean, our business model drives us to clickbait drives us to polarization.
[00:30:54.280 --> 00:30:59.400]   We in media have not been nearly honest enough with ourselves about our responsibility in
[00:30:59.400 --> 00:31:00.640]   all of this.
[00:31:00.640 --> 00:31:06.600]   Well, so what are some of the main recommendations that your report makes, Joan?
[00:31:06.600 --> 00:31:13.200]   In terms of what we're trying to understand here is that the report really does an overall
[00:31:13.200 --> 00:31:22.840]   assessment of these credibility indicators as well as looking at what does it mean to
[00:31:22.840 --> 00:31:29.680]   disincentivize or to ban content or to remove content from your platform.
[00:31:29.680 --> 00:31:34.760]   And there are really mixed feelings about this in the sense that there are edge cases
[00:31:34.760 --> 00:31:45.280]   that get caught up in banning content if you're banning all content that has to do with war
[00:31:45.280 --> 00:31:46.600]   or political speech.
[00:31:46.600 --> 00:31:48.680]   Obviously, that's a problem.
[00:31:48.680 --> 00:31:57.760]   But what we know is lots of disinformation flows in this content specifically.
[00:31:57.760 --> 00:32:03.960]   And so it's a fraught problem that the platform companies have, especially YouTube, has done
[00:32:03.960 --> 00:32:07.360]   a lot of work to demonetize certain accounts.
[00:32:07.360 --> 00:32:11.640]   And then of course, you know that Facebook has changed the way that they serve information
[00:32:11.640 --> 00:32:12.880]   in the newsfeed.
[00:32:12.880 --> 00:32:21.560]   And so I think we're seeing a lot of experiments and time will tell as we move on what this
[00:32:21.560 --> 00:32:29.720]   Twitter attack on the bots situation will also do to improve the information quality online.
[00:32:29.720 --> 00:32:30.720]   But for us--
[00:32:30.720 --> 00:32:31.720]   Can we talk about that now?
[00:32:31.720 --> 00:32:32.720]   Yeah, I'm eager to talk about that too.
[00:32:32.720 --> 00:32:39.120]   And then improving the information quality is something that time has come.
[00:32:39.120 --> 00:32:43.600]   And then also to think about what is the editorial and the curatorial strategies that
[00:32:43.600 --> 00:32:48.560]   these trending algorithms are going to make that pick the content that you should look
[00:32:48.560 --> 00:32:51.560]   at?
[00:32:51.560 --> 00:32:56.680]   So I woke up this morning and I saw a Twitter lockout going crazy.
[00:32:56.680 --> 00:32:57.680]   And I looked up the hashtag.
[00:32:57.680 --> 00:33:05.240]   It was amazing reading with people that had been followed by thousands of bots suddenly
[00:33:05.240 --> 00:33:06.800]   saying I have fewer followers.
[00:33:06.800 --> 00:33:09.080]   And it was on the right.
[00:33:09.080 --> 00:33:10.080]   I didn't lose any followers.
[00:33:10.080 --> 00:33:12.200]   I don't know if you guys did.
[00:33:12.200 --> 00:33:15.360]   Russian bots don't like me, I guess.
[00:33:15.360 --> 00:33:17.040]   I don't know.
[00:33:17.040 --> 00:33:21.960]   And it was really, I think, an important moment where Twitter decided to make a stand here.
[00:33:21.960 --> 00:33:23.800]   What did you guys think?
[00:33:23.800 --> 00:33:28.760]   Well, it just plays right into the narrative of those people who believe in the conspiracy
[00:33:28.760 --> 00:33:33.880]   theories and the left-wing media and left-wing Silicon Valley.
[00:33:33.880 --> 00:33:34.880]   So what do you do?
[00:33:34.880 --> 00:33:35.880]   Leave the bots?
[00:33:35.880 --> 00:33:37.520]   I don't know what you do.
[00:33:37.520 --> 00:33:39.520]   I feel like--
[00:33:39.520 --> 00:33:48.240]   My problem is I don't understand the mentality of people like Dinesh D'Souza, who--
[00:33:48.240 --> 00:33:56.000]   I don't understand how people-- I don't understand why are people confused about this?
[00:33:56.000 --> 00:33:59.320]   And I think that's part of the problem we have, and maybe we could thank Russian bots
[00:33:59.320 --> 00:34:03.720]   for that, I don't know, is that we're so polarized at this point that there is kind
[00:34:03.720 --> 00:34:06.600]   of a gulf, a massive gulf.
[00:34:06.600 --> 00:34:08.760]   Forget the generation gap.
[00:34:08.760 --> 00:34:11.680]   We have a gap of massive proportions.
[00:34:11.680 --> 00:34:12.680]   Reality gap.
[00:34:12.680 --> 00:34:13.680]   A reality gap.
[00:34:13.680 --> 00:34:19.800]   I see the New York Times article from about two weeks ago called the "Follower Factory,"
[00:34:19.800 --> 00:34:25.400]   which really went through how bots are sold to influencers and celebrities.
[00:34:25.400 --> 00:34:26.840]   Yes, we talked a lot about it.
[00:34:26.840 --> 00:34:28.320]   It's fascinating, yeah.
[00:34:28.320 --> 00:34:33.200]   Yeah, and so this is another iteration of that, which is that sometimes these aren't
[00:34:33.200 --> 00:34:42.160]   just what they call "suspicious automated activity," but that there are groups of people
[00:34:42.160 --> 00:34:48.160]   that are coordinating these accounts that are acting as a kind of flock.
[00:34:48.160 --> 00:34:55.800]   And so if you were to imagine a social media as a territory, and the social graph is something
[00:34:55.800 --> 00:35:02.320]   that these bots can really push content from one quadrant to another, right?
[00:35:02.320 --> 00:35:09.280]   And so it isn't just about intervening on and staying within the neighborhood of conservatives
[00:35:09.280 --> 00:35:10.920]   on the social graph.
[00:35:10.920 --> 00:35:19.040]   It's about pushing that kind of content into another vector and sometimes trolling other
[00:35:19.040 --> 00:35:23.800]   partisan groups or creating highly polarized debate.
[00:35:23.800 --> 00:35:28.280]   Did Twitter just kill a bunch of bots?
[00:35:28.280 --> 00:35:32.080]   I mean, they didn't pay attention to who they were following.
[00:35:32.080 --> 00:35:37.120]   I think they'd know, but they also, they killed bots and then they also apparently,
[00:35:37.120 --> 00:35:40.880]   I think accounts that seemed to have suspicious behavior, they made them verify with folks
[00:35:40.880 --> 00:35:41.880]   phone numbers, right?
[00:35:41.880 --> 00:35:42.880]   Okay.
[00:35:42.880 --> 00:35:43.880]   Yes.
[00:35:43.880 --> 00:35:50.240]   And the verification by phone number is something that will also allow them to see if you were
[00:35:50.240 --> 00:35:53.240]   running to three, ten different accounts.
[00:35:53.240 --> 00:35:56.880]   So when Richard Spencer says, "I lost a thousand followers in the past few hours," it's not
[00:35:56.880 --> 00:36:03.680]   because Twitter's targeting Richard Spencer's followers, it's just that he has a lot of these
[00:36:03.680 --> 00:36:08.560]   bots following him because that's the job of the Internet Research Agency and these other
[00:36:08.560 --> 00:36:13.640]   bot factories is to amplify these voices.
[00:36:13.640 --> 00:36:17.800]   What I find interesting is also that what is their agenda?
[00:36:17.800 --> 00:36:22.360]   Like why did the Russian bots leap in during in Parkland?
[00:36:22.360 --> 00:36:27.360]   Is it because they don't want gun control or is it as I would imagine more likely because
[00:36:27.360 --> 00:36:31.200]   they want strife period?
[00:36:31.200 --> 00:36:35.680]   Yeah, I would agree with you that strife is part of it and part of it is making it seem
[00:36:35.680 --> 00:36:41.160]   that the terms of the debate are so unreasonable that you are so far apart on this that it
[00:36:41.160 --> 00:36:43.960]   can't possibly be reconciled.
[00:36:43.960 --> 00:36:44.960]   And so...
[00:36:44.960 --> 00:36:47.000]   Oh, but I was just talking about that golf.
[00:36:47.000 --> 00:36:49.800]   In other words, I've been influenced by that.
[00:36:49.800 --> 00:36:55.160]   Yeah, and the other thing that's difficult about this is they've been around before the
[00:36:55.160 --> 00:36:59.760]   election and they've been influencing other social movement groups.
[00:36:59.760 --> 00:37:03.680]   Yeah, according to the indictment since 2014, but I think probably forever.
[00:37:03.680 --> 00:37:04.680]   I mean, I don't think...
[00:37:04.680 --> 00:37:07.520]   Yeah, but that's another problem of degrees.
[00:37:07.520 --> 00:37:13.440]   So Adrian Chen had did an investigative report in 2015 where he went to the Internet Research
[00:37:13.440 --> 00:37:17.680]   Agency and basically said that it was a small time operation.
[00:37:17.680 --> 00:37:22.720]   But it doesn't mean that this operation hasn't grown since then.
[00:37:22.720 --> 00:37:28.560]   And because we lack the ability to assess the impact and researchers, it's very frustrating
[00:37:28.560 --> 00:37:34.840]   in the sense that you find out about these things generally after the content has been
[00:37:34.840 --> 00:37:38.920]   deleted and we don't have access to the deleted information.
[00:37:38.920 --> 00:37:43.720]   There was a cache of Russian tweets that were released this week.
[00:37:43.720 --> 00:37:49.360]   I think about 200,000 tweets that people are able to do a social autopsy on.
[00:37:49.360 --> 00:37:54.600]   But generally speaking, outside auditing is impossible.
[00:37:54.600 --> 00:38:03.480]   And so it's been a difficult moment to assess and evaluate impact to say if it's anything
[00:38:03.480 --> 00:38:09.640]   at all or if part of the troll itself, which is what I suspect, is getting the American
[00:38:09.640 --> 00:38:20.520]   public to think that Russia has a much more impact on American politics than they do.
[00:38:20.520 --> 00:38:26.880]   I mean, this is another tactic, which is if you can fan the flames of discontent in the
[00:38:26.880 --> 00:38:31.560]   mainstream media, it's just as good as doing it or it's actually much better than doing
[00:38:31.560 --> 00:38:36.920]   it online because the audience is so much bigger.
[00:38:36.920 --> 00:38:42.360]   I'm so impressed now.
[00:38:42.360 --> 00:38:46.520]   Well, that's the bots doing their job, right?
[00:38:46.520 --> 00:38:47.520]   Yeah.
[00:38:47.520 --> 00:38:52.720]   The other thing is, it's really suspect to me that people were able to go back to the
[00:38:52.720 --> 00:38:55.320]   IRA and they're still in the same building.
[00:38:55.320 --> 00:39:02.000]   And, you know, like, wouldn't you just move your office if you are a cause of international
[00:39:02.000 --> 00:39:05.000]   investigation and indictments, right?
[00:39:05.000 --> 00:39:08.520]   Like, why do these people go back to work?
[00:39:08.520 --> 00:39:10.520]   Pays well.
[00:39:10.520 --> 00:39:12.520]   I don't think it does.
[00:39:12.520 --> 00:39:17.120]   Go outside stories that say that relatively it pays well.
[00:39:17.120 --> 00:39:18.320]   Okay, relatively.
[00:39:18.320 --> 00:39:23.760]   But it's still, to me, the whole thing feels orchestrated.
[00:39:23.760 --> 00:39:31.800]   But also, they've created such incitement around bots that it's been interesting to
[00:39:31.800 --> 00:39:41.880]   see the relationship of bots to known disinformation artists online.
[00:39:41.880 --> 00:39:47.640]   And so watching people respond in this Twitter lockout has been really interesting to me
[00:39:47.640 --> 00:39:56.160]   because I've also seen several people that I follow having lost quite a bit of followers
[00:39:56.160 --> 00:39:58.720]   that would be considered leftist.
[00:39:58.720 --> 00:40:03.400]   And so that, to me, has been interesting to think about in terms of what is that?
[00:40:03.400 --> 00:40:04.400]   What is that?
[00:40:04.400 --> 00:40:06.760]   Is that the Bernie effect?
[00:40:06.760 --> 00:40:12.000]   What the indictment said was that they tried to benefit both Trump and Bernie Sanders.
[00:40:12.000 --> 00:40:13.000]   Yeah.
[00:40:13.000 --> 00:40:17.320]   And so, and I think part of it is that some of the people that you fall or that I follow
[00:40:17.320 --> 00:40:24.160]   are would be considered, quote unquote, hyper-partisan, that is, or they're non-voters in the sense
[00:40:24.160 --> 00:40:28.360]   that they're so left of left that the system cannot survive.
[00:40:28.360 --> 00:40:33.120]   However, if you really want to visit the upside down, just read the hashtag Twitter lockout.
[00:40:33.120 --> 00:40:39.720]   Oh, yeah, it's pick out some random accounts in there.
[00:40:39.720 --> 00:40:40.560]   Okay.
[00:40:40.560 --> 00:40:43.080]   Do we know how many?
[00:40:43.080 --> 00:40:45.760]   Do we know whether Trump lost followers today?
[00:40:45.760 --> 00:40:46.760]   That's interesting.
[00:40:46.760 --> 00:40:47.760]   I've not been paying attention.
[00:40:47.760 --> 00:40:50.680]   I couldn't find a service to know where to see.
[00:40:50.680 --> 00:40:58.720]   A lot of people are saying, just re-follow me if you got kicked off as if these aren't
[00:40:58.720 --> 00:41:00.960]   bots that have been kicked off.
[00:41:00.960 --> 00:41:05.960]   Project Veritas added again, of course, saying, you know, with fake videos asserting that
[00:41:05.960 --> 00:41:10.120]   Twitter engineers are working to ban right-wingers.
[00:41:10.120 --> 00:41:12.400]   Here's the famous Ben Garrison.
[00:41:12.400 --> 00:41:17.400]   Big Bird Brother is watching you trying to mute somebody wearing a make America great
[00:41:17.400 --> 00:41:20.160]   hat shadow band.
[00:41:20.160 --> 00:41:21.160]   So but I don't know.
[00:41:21.160 --> 00:41:23.680]   What do you tell people who say, no, no, those aren't bots.
[00:41:23.680 --> 00:41:27.600]   You're banning conservative thinkers and tweeters.
[00:41:27.600 --> 00:41:31.440]   You know, I mean, this is clearly what these people think.
[00:41:31.440 --> 00:41:37.440]   In that, I mean, we don't know what signals Twitter is using in order to understand what
[00:41:37.440 --> 00:41:39.920]   is an automated account and what is not.
[00:41:39.920 --> 00:41:45.720]   One of the signals that has been used in the past is related to does this account only do
[00:41:45.720 --> 00:41:50.760]   retweets and do they have an authentic voice?
[00:41:50.760 --> 00:41:57.320]   Do they tend to post at the same time using the same exact tweet as a group of other accounts?
[00:41:57.320 --> 00:42:02.600]   So that would indicate a kind of net that's being controlled by a single individual.
[00:42:02.600 --> 00:42:07.720]   Another way that I've heard but I cannot confirm in any way a signal that they look for is related
[00:42:07.720 --> 00:42:13.320]   to does this account click with a mouse, which is something that can be tracked.
[00:42:13.320 --> 00:42:18.560]   And so if the account isn't clicking with a mouse, then it might be automated and coming
[00:42:18.560 --> 00:42:19.760]   from other places.
[00:42:19.760 --> 00:42:30.480]   Or is the account using a service, a social media service to broadcast the same information
[00:42:30.480 --> 00:42:35.560]   between Twitter and Instagram and other platforms.
[00:42:35.560 --> 00:42:40.920]   And so they have a mishmash of signals that they look to in order to talk about which
[00:42:40.920 --> 00:42:42.680]   accounts are bots.
[00:42:42.680 --> 00:42:48.800]   And then the other thing that I think serves them over time is that you get the same network
[00:42:48.800 --> 00:42:52.440]   effects related to certain stories.
[00:42:52.440 --> 00:42:57.360]   And so the same groups of people that are promoting the crisis actor narrative here
[00:42:57.360 --> 00:43:02.080]   have been creating or promoting crisis actor narratives in the past.
[00:43:02.080 --> 00:43:05.800]   And you can analyze those accounts and look at those networks and then look at those other
[00:43:05.800 --> 00:43:13.200]   signals to see if they're automated or only doing retweets and assess from there.
[00:43:13.200 --> 00:43:19.440]   And so I think that Twitter knew roundly what they were doing and we're looking at many
[00:43:19.440 --> 00:43:23.520]   signals to assess if something was a bot.
[00:43:23.520 --> 00:43:27.760]   But also, back to this is a socio-technical problem.
[00:43:27.760 --> 00:43:30.800]   Bots are controlled by people.
[00:43:30.800 --> 00:43:37.920]   And so that just means the difference between me having one account and me having 10,000.
[00:43:37.920 --> 00:43:42.920]   I don't know why anybody pays any attention to Twitter at all to be honest with you.
[00:43:42.920 --> 00:43:45.600]   Well, but here's the thing, Leo.
[00:43:45.600 --> 00:43:47.480]   It's what I've said about Facebook too.
[00:43:47.480 --> 00:43:51.440]   The problem is you go through your feed and I go through my feed.
[00:43:51.440 --> 00:43:53.040]   Joan has a job looking at this crap.
[00:43:53.040 --> 00:43:54.720]   But we don't.
[00:43:54.720 --> 00:43:56.520]   And I don't see Russians.
[00:43:56.520 --> 00:43:57.520]   I don't see bots.
[00:43:57.520 --> 00:43:59.640]   I don't see Nazis.
[00:43:59.640 --> 00:44:02.240]   I see people I like and respect.
[00:44:02.240 --> 00:44:06.800]   And the problem is we still have this media mindset that says if one page is schmutz,
[00:44:06.800 --> 00:44:08.480]   there's a bad word somewhere on the internet.
[00:44:08.480 --> 00:44:10.040]   The entire internet is ruined.
[00:44:10.040 --> 00:44:11.040]   All of Facebook is ruined.
[00:44:11.040 --> 00:44:12.640]   All of Twitter is ruined.
[00:44:12.640 --> 00:44:14.720]   It's not the case.
[00:44:14.720 --> 00:44:17.920]   And we've got to keep that kind of perspective in mind.
[00:44:17.920 --> 00:44:18.920]   Yeah.
[00:44:18.920 --> 00:44:21.360]   Well, I don't know if there's a lot of hope for Twitter.
[00:44:21.360 --> 00:44:22.360]   I don't know.
[00:44:22.360 --> 00:44:24.000]   Well, there's a world need Twitter.
[00:44:24.000 --> 00:44:26.160]   Well, I don't know.
[00:44:26.160 --> 00:44:27.160]   I don't know about that.
[00:44:27.160 --> 00:44:28.160]   Does the world need anything?
[00:44:28.160 --> 00:44:29.160]   I don't know.
[00:44:29.160 --> 00:44:30.160]   What do you really need?
[00:44:30.160 --> 00:44:31.160]   There's a philosophical thing.
[00:44:31.160 --> 00:44:34.400]   A good book, a nice glasses, a chart, and I'm happy.
[00:44:34.400 --> 00:44:37.160]   Yeah, I'm going to go up your head at night, you know, a forty-five year pocket.
[00:44:37.160 --> 00:44:40.600]   The heavens thinking, boy, I just deal with, you know, devices now.
[00:44:40.600 --> 00:44:41.600]   So much nicer.
[00:44:41.600 --> 00:44:42.600]   It's a lot easier.
[00:44:42.600 --> 00:44:46.640]   But you know, my guilt shows up.
[00:44:46.640 --> 00:44:49.600]   So we're going to switch over to some other topic.
[00:44:49.600 --> 00:44:55.380]   And I feel like it's almost bearing your head in the sand to talk about all Android
[00:44:55.380 --> 00:44:56.520]   phones.
[00:44:56.520 --> 00:45:05.280]   And when, you know, this mess is occurring on this thing that we loved called the internet.
[00:45:05.280 --> 00:45:08.200]   So I don't, I don't know.
[00:45:08.200 --> 00:45:09.200]   Maybe it's too easy.
[00:45:09.200 --> 00:45:11.040]   This is, you're right, Joe.
[00:45:11.040 --> 00:45:12.040]   They're getting to me.
[00:45:12.040 --> 00:45:13.040]   I want to see-
[00:45:13.040 --> 00:45:15.040]   Yeah, they shouldn't get to you.
[00:45:15.040 --> 00:45:16.040]   They're getting to me.
[00:45:16.040 --> 00:45:19.520]   You know, I've listened to you for years in California.
[00:45:19.520 --> 00:45:21.800]   I just want to celebrate.
[00:45:21.800 --> 00:45:23.200]   I want to celebrate technology.
[00:45:23.200 --> 00:45:24.200]   But it's hard to-
[00:45:24.200 --> 00:45:27.640]   You know, not, you know, put down the Kindle and looking at a book.
[00:45:27.640 --> 00:45:28.640]   What do you think?
[00:45:28.640 --> 00:45:30.440]   What do you do when you're with your time?
[00:45:30.440 --> 00:45:31.440]   Well, I-
[00:45:31.440 --> 00:45:34.800]   But the other thing is that these things are bound to certain features online.
[00:45:34.800 --> 00:45:39.680]   Jeff's right to say that if you curate your feed and you don't look at hashtags-
[00:45:39.680 --> 00:45:40.680]   Right.
[00:45:40.680 --> 00:45:43.520]   And speaking, you're not going to fall prey to this.
[00:45:43.520 --> 00:45:47.240]   This kind of disinformation happens in breaking news cycles.
[00:45:47.240 --> 00:45:49.600]   It happens in crisis.
[00:45:49.600 --> 00:45:51.680]   And those are bounded problems.
[00:45:51.680 --> 00:45:56.280]   It happens to individuals that can be solved for.
[00:45:56.280 --> 00:45:57.280]   And I have-
[00:45:57.280 --> 00:45:59.360]   Just don't click the moments button, whatever you do.
[00:45:59.360 --> 00:46:00.360]   In other words, yeah.
[00:46:00.360 --> 00:46:01.360]   Yeah.
[00:46:01.360 --> 00:46:05.600]   See, the other thing here is I think that the two people I know who have for their careers
[00:46:05.600 --> 00:46:09.400]   have had to deal with the worst crap of the internet are truly the nicest people I know.
[00:46:09.400 --> 00:46:11.280]   Just joined out of it and Matt cuts.
[00:46:11.280 --> 00:46:16.120]   So somehow they've managed to keep their sanity, but we can't.
[00:46:16.120 --> 00:46:25.160]   Well, I do a lot of this work, but at the same time I have to have faith in a humanity.
[00:46:25.160 --> 00:46:26.160]   Or else, you know-
[00:46:26.160 --> 00:46:27.160]   Or else it's not worth it.
[00:46:27.160 --> 00:46:28.160]   Yeah.
[00:46:28.160 --> 00:46:29.160]   I love-
[00:46:29.160 --> 00:46:31.600]   What pains me, Leo and Jeff is that I love the internet.
[00:46:31.600 --> 00:46:32.600]   I do.
[00:46:32.600 --> 00:46:33.600]   I love technology.
[00:46:33.600 --> 00:46:35.280]   I love toys.
[00:46:35.280 --> 00:46:37.000]   I love things that move.
[00:46:37.000 --> 00:46:38.360]   I love things that blink.
[00:46:38.360 --> 00:46:41.000]   I love tape and telephones.
[00:46:41.000 --> 00:46:42.000]   And so-
[00:46:42.000 --> 00:46:47.040]   But I don't think we've actually lost the fundamental infrastructure of the internet.
[00:46:47.040 --> 00:46:50.360]   These products are built on top of it, right?
[00:46:50.360 --> 00:46:56.840]   And so if we think about them as the end-all, be-all of the internet, then we're really
[00:46:56.840 --> 00:46:57.840]   only looking at one-
[00:46:57.840 --> 00:47:00.520]   You know, we have a one-dimensional problem.
[00:47:00.520 --> 00:47:03.040]   But fundamentally, the internet's still there.
[00:47:03.040 --> 00:47:04.520]   It's still critical infrastructure.
[00:47:04.520 --> 00:47:10.040]   And there's still many opportunities for us to either create smaller-
[00:47:10.040 --> 00:47:16.160]   Small world networks where people are, um, cloistered away from this or that it's more
[00:47:16.160 --> 00:47:19.880]   difficult to move across the social graph.
[00:47:19.880 --> 00:47:26.920]   Or we can temper this with a good dose of what I remember from the '90s, which is that
[00:47:26.920 --> 00:47:33.880]   we didn't believe what happened on the internet was real anyway.
[00:47:33.880 --> 00:47:36.960]   That's where we went wrong, starting to believe this crap.
[00:47:36.960 --> 00:47:43.400]   Well, part of it was, you know, all of our institutions moved online, education, media.
[00:47:43.400 --> 00:47:50.800]   You know, we all started, you know, our banking moved online and then things got really serious.
[00:47:50.800 --> 00:47:56.920]   And I think that we can, you know, one look back one step forward and try to understand
[00:47:56.920 --> 00:48:01.720]   that the fundamental infrastructure is still there and that we can remake it in another
[00:48:01.720 --> 00:48:02.720]   way.
[00:48:02.720 --> 00:48:08.960]   Let's take a break and we'll come to something Kevin can talk about.
[00:48:08.960 --> 00:48:11.400]   For Kevin.
[00:48:11.400 --> 00:48:12.400]   Kevin Tuffle is here.
[00:48:12.400 --> 00:48:13.400]   It's great to have you back, Kevin.
[00:48:13.400 --> 00:48:14.400]   I really.
[00:48:14.400 --> 00:48:15.400]   Kevin's been able to read through the rundown.
[00:48:15.400 --> 00:48:16.400]   Yeah.
[00:48:16.400 --> 00:48:18.000]   Now, you know, all the stories we're going to talk about next.
[00:48:18.000 --> 00:48:20.120]   I was getting rid of my bots.
[00:48:20.120 --> 00:48:25.320]   Did anybody, I didn't see, I don't remember what my Twitter followers were, but it's,
[00:48:25.320 --> 00:48:26.960]   I mean, it's roughly the same.
[00:48:26.960 --> 00:48:29.320]   Well, I look at, you know, it tells you whether they've gone up or not.
[00:48:29.320 --> 00:48:30.320]   Mine went up.
[00:48:30.320 --> 00:48:31.320]   Where does it tell you that?
[00:48:31.320 --> 00:48:33.280]   If you go to your, your, your Twitter page.
[00:48:33.280 --> 00:48:34.280]   That's part of the problem.
[00:48:34.280 --> 00:48:35.800]   I don't even know how to use this thing.
[00:48:35.800 --> 00:48:36.800]   I didn't know it either.
[00:48:36.800 --> 00:48:37.800]   It's on.
[00:48:37.800 --> 00:48:38.800]   Okay.
[00:48:38.800 --> 00:48:41.120]   If you look at, uh, where the heck is it?
[00:48:41.120 --> 00:48:44.760]   You look at the stats, follow analytics followers analytics.
[00:48:44.760 --> 00:48:47.600]   Yeah, I do have, I do have some analytics somewhere.
[00:48:47.600 --> 00:48:48.600]   Yeah.
[00:48:48.600 --> 00:48:49.600]   Thank you.
[00:48:49.600 --> 00:48:50.600]   Here we go.
[00:48:50.600 --> 00:48:53.680]   Uh, view your top tweets, your tweet activity.
[00:48:53.680 --> 00:48:56.240]   Is it, is it, is there a link to analytics?
[00:48:56.240 --> 00:48:57.240]   What is the?
[00:48:57.240 --> 00:49:00.240]   Yeah, there is picture at the top right, your icon.
[00:49:00.240 --> 00:49:01.240]   Oh, there's some.
[00:49:01.240 --> 00:49:04.120]   Thank you for helping me use this.
[00:49:04.120 --> 00:49:05.120]   That's wonderful.
[00:49:05.120 --> 00:49:06.120]   Analytics.
[00:49:06.120 --> 00:49:11.400]   So this will tell you whether you've lost followers or gained, uh, new followers.
[00:49:11.400 --> 00:49:13.040]   I'm down 244 bots.
[00:49:13.040 --> 00:49:15.840]   I mean followers.
[00:49:15.840 --> 00:49:18.440]   I'm up 726 followers.
[00:49:18.440 --> 00:49:20.440]   Kevin, what's your, what's your?
[00:49:20.440 --> 00:49:22.160]   I'm up 39.
[00:49:22.160 --> 00:49:24.760]   So obviously I'm the right winger here.
[00:49:24.760 --> 00:49:25.760]   Joan.
[00:49:25.760 --> 00:49:27.240]   I'm looking.
[00:49:27.240 --> 00:49:28.640]   I can't find it though.
[00:49:28.640 --> 00:49:31.560]   I'll go to your Twitter page, go to the picture.
[00:49:31.560 --> 00:49:32.560]   Kevin just said this.
[00:49:32.560 --> 00:49:33.880]   Click your head.
[00:49:33.880 --> 00:49:34.880]   Choose analytics.
[00:49:34.880 --> 00:49:35.880]   Okay.
[00:49:35.880 --> 00:49:37.720]   Click on my head.
[00:49:37.720 --> 00:49:38.720]   Choose analytics.
[00:49:38.720 --> 00:49:41.720]   Unless you don't have your head as a, uh, you know, it's really interesting.
[00:49:41.720 --> 00:49:45.560]   Some of the most vile people on Twitter don't use their own pictures.
[00:49:45.560 --> 00:49:46.560]   Isn't that interesting?
[00:49:46.560 --> 00:49:49.000]   Oh, well, the frog which is real.
[00:49:49.000 --> 00:49:51.520]   Pepe, whatever happened to the egg?
[00:49:51.520 --> 00:49:52.520]   The egg is gone.
[00:49:52.520 --> 00:49:55.000]   No, they don't use the egg anymore.
[00:49:55.000 --> 00:49:58.360]   So I've lost 244, which I'm, I'm perfectly happy to lose.
[00:49:58.360 --> 00:50:02.960]   I don't pay any attention to the number of my followers anyway.
[00:50:02.960 --> 00:50:09.760]   Not since, not since Kevin Rose beat me in 2007 for the number one most followed person
[00:50:09.760 --> 00:50:11.240]   on Twitter ever since then.
[00:50:11.240 --> 00:50:13.320]   I'm still bitter about that.
[00:50:13.320 --> 00:50:20.800]   Um, we were on a race to 5,000 and he beat me 5,000 followers, which was everybody.
[00:50:20.800 --> 00:50:22.920]   Yeah, it was.
[00:50:22.920 --> 00:50:24.720]   All right.
[00:50:24.720 --> 00:50:29.000]   All right, where am I looking for in audiences?
[00:50:29.000 --> 00:50:32.280]   Analytics, it's over and you get to that page.
[00:50:32.280 --> 00:50:33.440]   It's on the right hand side.
[00:50:33.440 --> 00:50:37.480]   There's a column that's February 2018 summary.
[00:50:37.480 --> 00:50:40.360]   Oh, that's 244 for February.
[00:50:40.360 --> 00:50:41.600]   So it's on the top.
[00:50:41.600 --> 00:50:43.320]   It says followers down 436.
[00:50:43.320 --> 00:50:44.320]   Oh, look.
[00:50:44.320 --> 00:50:45.320]   There you go.
[00:50:45.320 --> 00:50:47.120]   This is the 28 day summary.
[00:50:47.120 --> 00:50:49.960]   It just followers 28 day.
[00:50:49.960 --> 00:50:51.360]   That's probably cause I'm just boring.
[00:50:51.360 --> 00:50:53.680]   I don't, I don't realize.
[00:50:53.680 --> 00:50:55.480]   I don't post anything of interest on Twitter.
[00:50:55.480 --> 00:50:58.200]   I just point post news stories.
[00:50:58.200 --> 00:51:00.480]   I get 81 clicks a day.
[00:51:00.480 --> 00:51:02.040]   25 repeat.
[00:51:02.040 --> 00:51:03.760]   Three today.
[00:51:03.760 --> 00:51:05.000]   43 likes.
[00:51:05.000 --> 00:51:07.480]   I should, I should pay more attention to Twitter.
[00:51:07.480 --> 00:51:11.840]   Apparently I have 1.91 million impressions over the month.
[00:51:11.840 --> 00:51:13.680]   That's a pretty good number.
[00:51:13.680 --> 00:51:14.680]   I guess.
[00:51:14.680 --> 00:51:15.680]   I guess.
[00:51:15.680 --> 00:51:16.680]   I don't know.
[00:51:16.680 --> 00:51:17.920]   What does an impression mean?
[00:51:17.920 --> 00:51:20.200]   Does that mean somebody scrolled past my tweet?
[00:51:20.200 --> 00:51:21.200]   Yes.
[00:51:21.200 --> 00:51:22.200]   Yeah.
[00:51:22.200 --> 00:51:23.200]   It's a big deal.
[00:51:23.200 --> 00:51:24.200]   Yeah.
[00:51:24.200 --> 00:51:28.200]   I'm still not seeing followers for some reason.
[00:51:28.200 --> 00:51:30.200]   I don't know what's going on with me.
[00:51:30.200 --> 00:51:32.800]   But you have been blocked.
[00:51:32.800 --> 00:51:35.040]   It's a state secret for you.
[00:51:35.040 --> 00:51:36.040]   Yes.
[00:51:36.040 --> 00:51:37.040]   Let's take a break.
[00:51:37.040 --> 00:51:38.040]   Come back with more.
[00:51:38.040 --> 00:51:40.680]   Joan Donovan is also here from Data and Society.
[00:51:40.680 --> 00:51:44.240]   She is in charge of fake news.
[00:51:44.240 --> 00:51:46.640]   Today.
[00:51:46.640 --> 00:51:52.720]   She is, she is actually doing the most interesting job you can even imagine.
[00:51:52.720 --> 00:51:54.520]   That data in society.
[00:51:54.520 --> 00:52:02.960]   She's her technical, her title is Project Lead on Media Manipulation and Platform Accountability,
[00:52:02.960 --> 00:52:03.960]   which is fascinating.
[00:52:03.960 --> 00:52:04.960]   Yes.
[00:52:04.960 --> 00:52:08.360]   If you want to work for me, I'm hiring.
[00:52:08.360 --> 00:52:09.360]   Wow.
[00:52:09.360 --> 00:52:10.360]   Come on.
[00:52:10.360 --> 00:52:14.280]   What kind of skills does somebody need to come to work for you?
[00:52:14.280 --> 00:52:15.280]   Yeah.
[00:52:15.280 --> 00:52:16.880]   Probably a lead line stomach.
[00:52:16.880 --> 00:52:17.880]   Yeah.
[00:52:17.880 --> 00:52:18.880]   Yeah.
[00:52:18.880 --> 00:52:21.560]   I don't want to come back the next day.
[00:52:21.560 --> 00:52:25.440]   I honestly, I took Twitter and Facebook off my phones.
[00:52:25.440 --> 00:52:27.480]   I never feel good afterwards.
[00:52:27.480 --> 00:52:31.040]   And mostly I use things like Nuzzle to look at what's going on on Twitter because it
[00:52:31.040 --> 00:52:34.560]   just aggregates news stories.
[00:52:34.560 --> 00:52:35.720]   It just makes me feel bad.
[00:52:35.720 --> 00:52:37.880]   I don't like to talk about it.
[00:52:37.880 --> 00:52:40.440]   Well, I mean, it's like anything.
[00:52:40.440 --> 00:52:46.800]   It can get, once you get stuck in those echo chambers, you start seeing the same content
[00:52:46.800 --> 00:52:48.640]   over and over and over again.
[00:52:48.640 --> 00:52:51.520]   It just feels obnoxious, right?
[00:52:51.520 --> 00:52:54.600]   It just feels like, what am I even doing here?
[00:52:54.600 --> 00:52:56.640]   I'm reading the same thing over and over again.
[00:52:56.640 --> 00:52:57.640]   Yeah.
[00:52:57.640 --> 00:53:00.680]   Those Russian trolls are bringing me down, man.
[00:53:00.680 --> 00:53:01.880]   I can't believe they got you.
[00:53:01.880 --> 00:53:03.640]   Of all people, I don't even know.
[00:53:03.640 --> 00:53:06.760]   Are they all Russian or are they from other places too?
[00:53:06.760 --> 00:53:09.640]   This is the other thing is this is an agency for hire.
[00:53:09.640 --> 00:53:11.280]   They could be from anywhere.
[00:53:11.280 --> 00:53:12.280]   Yeah.
[00:53:12.280 --> 00:53:13.280]   It could be the phone.
[00:53:13.280 --> 00:53:18.280]   So like if Erno Juan and Turkey decided, I need to get my word out on Twitter.
[00:53:18.280 --> 00:53:23.120]   You could go to the Internet Research Association and say, can I get a few bots to help me out?
[00:53:23.120 --> 00:53:24.120]   They're not bots.
[00:53:24.120 --> 00:53:25.120]   They're people.
[00:53:25.120 --> 00:53:26.120]   By the way, we shouldn't call them bots.
[00:53:26.120 --> 00:53:27.120]   That's not fair.
[00:53:27.120 --> 00:53:28.120]   They're actual people.
[00:53:28.120 --> 00:53:31.200]   They are using, they're using two technologies.
[00:53:31.200 --> 00:53:36.480]   One is there's just the sort of human element where they're using sock puppet accounts pretending
[00:53:36.480 --> 00:53:39.320]   to be either organizations or individuals.
[00:53:39.320 --> 00:53:43.280]   And then the second layer of it is some of it is automated and they have bots set up
[00:53:43.280 --> 00:53:46.200]   to amplify those accounts to follow those accounts.
[00:53:46.200 --> 00:53:49.760]   And so that's what we refer to when we refer to a botnet.
[00:53:49.760 --> 00:53:55.360]   And then the other thing that's important to understand is like there's this, but then
[00:53:55.360 --> 00:54:03.720]   there's also an entire industry devoted to fake followers that has it has cropped up
[00:54:03.720 --> 00:54:07.920]   in order to gain these algorithmic systems.
[00:54:07.920 --> 00:54:08.920]   Right.
[00:54:08.920 --> 00:54:09.920]   That's just commerce.
[00:54:09.920 --> 00:54:10.920]   Yeah.
[00:54:10.920 --> 00:54:18.120]   But you can still buy and use those same followers in order to do the work that you're trying
[00:54:18.120 --> 00:54:19.120]   to do.
[00:54:19.120 --> 00:54:28.760]   What are the chances that the NSA or other US federal agencies don't have the same capabilities?
[00:54:28.760 --> 00:54:34.320]   I mean, like, I don't want to sound like a conspiracy theorist, but you know, in my
[00:54:34.320 --> 00:54:37.720]   work, if anybody has the right to Joan, you do.
[00:54:37.720 --> 00:54:43.000]   In my work with movements, you know, we know that movements have been infiltrated for years.
[00:54:43.000 --> 00:54:48.800]   Entire Co-Intell Pro programs have been set up to infiltrate movements and infiltrating
[00:54:48.800 --> 00:54:56.080]   movements through social media and using to, you know, influence movements.
[00:54:56.080 --> 00:54:57.800]   I think we have a bot cap.
[00:54:57.800 --> 00:55:01.000]   I think we need to get some bots in Russia.
[00:55:01.000 --> 00:55:02.640]   Let's go after the Russians.
[00:55:02.640 --> 00:55:03.640]   Yeah.
[00:55:03.640 --> 00:55:04.640]   Leo report hacks Russia.
[00:55:04.640 --> 00:55:05.640]   We need a bot.
[00:55:05.640 --> 00:55:06.640]   There's a bot cap.
[00:55:06.640 --> 00:55:11.920]   We need our own internet research agency there in McLean, Virginia to get that room.
[00:55:11.920 --> 00:55:12.920]   Let's do your new job.
[00:55:12.920 --> 00:55:13.920]   Go get the Russians.
[00:55:13.920 --> 00:55:15.720]   Screw with them.
[00:55:15.720 --> 00:55:16.720]   Yeah.
[00:55:16.720 --> 00:55:23.600]   This is the other thing is like, you know, they've dedicated a multi-year propaganda plan
[00:55:23.600 --> 00:55:30.640]   here and getting Americans to do things for that long without any specific reward.
[00:55:30.640 --> 00:55:37.360]   I mean, it's going to be hard to formalize that kind of civic engagement.
[00:55:37.360 --> 00:55:38.960]   Of course, I'm being facetious.
[00:55:38.960 --> 00:55:42.160]   It helps when you're a totalitarian regime.
[00:55:42.160 --> 00:55:43.160]   It does.
[00:55:43.160 --> 00:55:44.160]   It really does.
[00:55:44.160 --> 00:55:50.000]   And as well, we do know that the US has run social media operations in other countries
[00:55:50.000 --> 00:55:54.920]   in order to influence, you know, networks of actors.
[00:55:54.920 --> 00:55:58.440]   And so that stuff is well known.
[00:55:58.440 --> 00:56:03.040]   And so one of the things that I often refer to when I talk about our understanding of
[00:56:03.040 --> 00:56:05.680]   social media is that it's not a reflection of reality.
[00:56:05.680 --> 00:56:07.680]   It's a bit of a funhouse mirror.
[00:56:07.680 --> 00:56:13.640]   And if we don't understand its distortions through manipulation, then we don't understand
[00:56:13.640 --> 00:56:18.800]   the reality that we're living in, which is partly mediated by screens.
[00:56:18.800 --> 00:56:22.360]   And then there's the stuff we see with our own eyes.
[00:56:22.360 --> 00:56:23.360]   Jeff, it's your fault.
[00:56:23.360 --> 00:56:24.360]   I'm looking at August.
[00:56:24.360 --> 00:56:29.400]   I'm going to be a lost 878 followers because of your trader, Joe's queso tweet.
[00:56:29.400 --> 00:56:32.680]   I blame you.
[00:56:32.680 --> 00:56:38.240]   I blame you, Jeff Jarvis, City University of New York, Professor of Journalism, helping
[00:56:38.240 --> 00:56:43.680]   us navigate the mainstream and otherwise media.
[00:56:43.680 --> 00:56:47.160]   And Kevin Tofel, his triumphant return.
[00:56:47.160 --> 00:56:49.480]   And I don't tweet about politics or queso.
[00:56:49.480 --> 00:56:50.480]   Yeah, me neither.
[00:56:50.480 --> 00:56:51.720]   Safer that was.
[00:56:51.720 --> 00:56:52.720]   Safer.
[00:56:52.720 --> 00:56:56.520]   Kevin is at the IoT podcast with Stacy.
[00:56:56.520 --> 00:56:57.520]   Stacy's in Berlin.
[00:56:57.520 --> 00:56:59.520]   What's she doing in Berlin?
[00:56:59.520 --> 00:57:02.720]   She is freezing her behind all.
[00:57:02.720 --> 00:57:04.760]   It's very cold.
[00:57:04.760 --> 00:57:08.480]   She's at the Bosch IoT event that they have for several days.
[00:57:08.480 --> 00:57:09.480]   That's right.
[00:57:09.480 --> 00:57:10.480]   Not my thing.
[00:57:10.480 --> 00:57:12.040]   Well, we're glad you're with us.
[00:57:12.040 --> 00:57:13.040]   Thank you.
[00:57:13.040 --> 00:57:14.040]   Thank you.
[00:57:14.040 --> 00:57:15.040]   Yes.
[00:57:15.040 --> 00:57:17.840]   How do you feel about Chipotle queso?
[00:57:17.840 --> 00:57:21.040]   I've only had Chipotle once, to be honest.
[00:57:21.040 --> 00:57:22.040]   He's too healthy.
[00:57:22.040 --> 00:57:26.320]   No, no, no, no, no, no, don't let the running every day fool you.
[00:57:26.320 --> 00:57:27.320]   That's why you run.
[00:57:27.320 --> 00:57:31.260]   I had a meatball parm sandwich for a while, and I put it on Facebook today because the
[00:57:31.260 --> 00:57:34.800]   dog was trying to eat it.
[00:57:34.800 --> 00:57:38.440]   Now see, I don't follow any meatball parm account.
[00:57:38.440 --> 00:57:39.920]   No problem.
[00:57:39.920 --> 00:57:42.240]   That's my people.
[00:57:42.240 --> 00:57:43.720]   Jeff probably knows Wawa.
[00:57:43.720 --> 00:57:44.720]   That's meatball parms.
[00:57:44.720 --> 00:57:46.480]   Yeah, yeah, I do.
[00:57:46.480 --> 00:57:47.640]   My son loves Wawa.
[00:57:47.640 --> 00:57:48.640]   There you go.
[00:57:48.640 --> 00:57:49.840]   My son works for Wawa.
[00:57:49.840 --> 00:57:50.840]   Wow.
[00:57:50.840 --> 00:57:53.960]   How have I not heard of Wawa?
[00:57:53.960 --> 00:57:55.880]   Because it's an East Coast thing.
[00:57:55.880 --> 00:58:00.560]   It is an odd, it's a 7-Eleven, but it's an oddly cult-like.
[00:58:00.560 --> 00:58:01.880]   It's a convenience store?
[00:58:01.880 --> 00:58:02.880]   Yeah.
[00:58:02.880 --> 00:58:03.880]   With sandwiches.
[00:58:03.880 --> 00:58:05.240]   They have more fresh made food.
[00:58:05.240 --> 00:58:08.680]   Well, fresh may be a stretch, but they have more made food.
[00:58:08.680 --> 00:58:09.680]   That's fresh.
[00:58:09.680 --> 00:58:11.880]   I've seen that have the sausage is made.
[00:58:11.880 --> 00:58:16.640]   They started out as just milk delivery way, way, way back in the day in Pennsylvania,
[00:58:16.640 --> 00:58:20.760]   they've expanded to hundreds of stores throughout the Tri-State area and all the way down to
[00:58:20.760 --> 00:58:21.760]   Florida now.
[00:58:21.760 --> 00:58:22.760]   Yeah.
[00:58:22.760 --> 00:58:26.000]   And you go, it's very techie.
[00:58:26.000 --> 00:58:29.000]   You order on a little screen.
[00:58:29.000 --> 00:58:30.000]   A screen?
[00:58:30.000 --> 00:58:31.000]   Get your...
[00:58:31.000 --> 00:58:32.000]   Yeah.
[00:58:32.000 --> 00:58:34.000]   They don't serve bots either.
[00:58:34.000 --> 00:58:35.640]   Saver Wawa's hot hogies.
[00:58:35.640 --> 00:58:38.920]   That is a mouthful.
[00:58:38.920 --> 00:58:42.320]   They are not our sponsor today, however.
[00:58:42.320 --> 00:58:44.400]   I wish they were.
[00:58:44.400 --> 00:58:51.920]   I could use some of that Wawa hogie money, but I'm actually very happy to have WordPress
[00:58:51.920 --> 00:58:54.320]   as a sponsor because I use WordPress.
[00:58:54.320 --> 00:58:58.520]   I've been a WordPress fan since they started in the early 2000s.
[00:58:58.520 --> 00:59:02.680]   If you are a business or even an individual and you don't have your own personal website,
[00:59:02.680 --> 00:59:08.320]   if you're trying to make it as a Facebook page or a Twitter account, no man, get yourself
[00:59:08.320 --> 00:59:11.440]   a website of WordPress.
[00:59:11.440 --> 00:59:13.280]   WordPress.com makes it easy.
[00:59:13.280 --> 00:59:14.280]   I know you're intimidated.
[00:59:14.280 --> 00:59:17.200]   Maybe you think, "Oh, I have to know about web design," right?
[00:59:17.200 --> 00:59:18.200]   Or so.
[00:59:18.200 --> 00:59:19.200]   No.
[00:59:19.200 --> 00:59:20.200]   You don't need to experience.
[00:59:20.200 --> 00:59:21.200]   You don't need to hire somebody at WordPress.com.
[00:59:21.200 --> 00:59:23.920]   Guide you through the entire process from start to finish.
[00:59:23.920 --> 00:59:26.840]   They do all the technical stuff, including keeping it up to date.
[00:59:26.840 --> 00:59:30.840]   They've got hundreds of beautiful templates to choose from designs.
[00:59:30.840 --> 00:59:35.880]   They've got all the plug-ins that you could turn on Google app for speeding up your mobile
[00:59:35.880 --> 00:59:36.880]   loading.
[00:59:36.880 --> 00:59:37.880]   I do that.
[00:59:37.880 --> 00:59:41.720]   Built-in search engine optimization makes Google happy.
[00:59:41.720 --> 00:59:42.720]   Social sharing.
[00:59:42.720 --> 00:59:44.800]   You don't have to put it on Twitter.
[00:59:44.800 --> 00:59:48.200]   Your fans will tweet about you or Facebook about you.
[00:59:48.200 --> 00:59:51.760]   If you're a business and you don't have your own personal website with your own domain
[00:59:51.760 --> 00:59:55.280]   name, I feel like in this day and age you barely exist.
[00:59:55.280 --> 00:59:56.440]   You've got to do this.
[00:59:56.440 --> 01:00:00.120]   This is the—this is table stakes.
[01:00:00.120 --> 01:00:01.560]   What a great customer support team.
[01:00:01.560 --> 01:00:06.440]   You'll never be at sea because they're there 24 hours a day, Monday through Friday, weekends
[01:00:06.440 --> 01:00:08.000]   too to help you.
[01:00:08.000 --> 01:00:09.480]   They love helping you.
[01:00:09.480 --> 01:00:12.760]   I'm going to start as little as $4 a month.
[01:00:12.760 --> 01:00:13.760]   Here's the thing.
[01:00:13.760 --> 01:00:16.960]   29% of all the websites in the world.
[01:00:16.960 --> 01:00:20.600]   29% run on WordPress.
[01:00:20.600 --> 01:00:23.120]   You'll be joining a community of nice people.
[01:00:23.120 --> 01:00:27.000]   Get started today with 15% off any new plant purchase.
[01:00:27.000 --> 01:00:30.160]   Go to WordPress.com/twig.
[01:00:30.160 --> 01:00:32.760]   I'm there.
[01:00:32.760 --> 01:00:34.040]   Will you join me there?
[01:00:34.040 --> 01:00:36.680]   15% off your brand new website.
[01:00:36.680 --> 01:00:40.400]   Yes.com/twig.
[01:00:40.400 --> 01:00:45.000]   We thank WordPress so much for their support of this week in Google.
[01:00:45.000 --> 01:00:46.000]   All right.
[01:00:46.000 --> 01:00:48.000]   I just hired a new president or CEO.
[01:00:48.000 --> 01:00:49.000]   I forget what your name is.
[01:00:49.000 --> 01:00:50.000]   It's his name, Kinsey Wilson.
[01:00:50.000 --> 01:00:53.640]   I wanted to mention this because I saw your tweet and you said, "This is so good."
[01:00:53.640 --> 01:00:54.640]   Why is that so good?
[01:00:54.640 --> 01:01:00.080]   So Kinsey was a USA Today top editor and then he was in charge of digital and then all news
[01:01:00.080 --> 01:01:01.080]   content at NPR.
[01:01:01.080 --> 01:01:02.080]   So he's a journal.
[01:01:02.080 --> 01:01:03.320]   And then he was at the New York Times.
[01:01:03.320 --> 01:01:04.320]   Yeah.
[01:01:04.320 --> 01:01:06.400]   He's a really top notch media executive.
[01:01:06.400 --> 01:01:08.720]   So I think it's good for us in media.
[01:01:08.720 --> 01:01:09.720]   That's a wonderful guy.
[01:01:09.720 --> 01:01:10.800]   That's really good.
[01:01:10.800 --> 01:01:12.720]   So just now he's been hired.
[01:01:12.720 --> 01:01:15.240]   I mentioned this the last time and I couldn't remember his name because I saw your tweet
[01:01:15.240 --> 01:01:17.120]   about this.
[01:01:17.120 --> 01:01:18.280]   Nice.
[01:01:18.280 --> 01:01:19.280]   Nice.
[01:01:19.280 --> 01:01:21.360]   He is now at WordPress.
[01:01:21.360 --> 01:01:22.360]   Fantastic.
[01:01:22.360 --> 01:01:28.160]   As I think, I don't know, CEO or resident or I think Matt Mullen-Weg is chair or...?
[01:01:28.160 --> 01:01:29.160]   Yeah, Matt.
[01:01:29.160 --> 01:01:30.920]   I've known Matt for years.
[01:01:30.920 --> 01:01:31.920]   This is him, right?
[01:01:31.920 --> 01:01:32.920]   Yeah.
[01:01:32.920 --> 01:01:36.360]   I'm delighted to be joining the growing team at WordPress.com and parent company automatic.
[01:01:36.360 --> 01:01:38.200]   Iconic champion of the open web.
[01:01:38.200 --> 01:01:43.040]   Many more chapters to write.
[01:01:43.040 --> 01:01:45.080]   So that's pretty awesome.
[01:01:45.080 --> 01:01:46.840]   It is.
[01:01:46.840 --> 01:01:47.840]   Yeah.
[01:01:47.840 --> 01:01:50.560]   Well, we're...
[01:01:50.560 --> 01:01:53.760]   We only do advertisers for companies I feel good about.
[01:01:53.760 --> 01:01:54.760]   And I...
[01:01:54.760 --> 01:01:55.760]   This is one that's easy to feel good about.
[01:01:55.760 --> 01:01:57.160]   It's easy to feel good about them.
[01:01:57.160 --> 01:02:00.200]   I completely agree with you.
[01:02:00.200 --> 01:02:01.200]   Let's see here.
[01:02:01.200 --> 01:02:02.200]   All right.
[01:02:02.200 --> 01:02:03.200]   Perfect.
[01:02:03.200 --> 01:02:05.760]   Let's find something exciting about technology.
[01:02:05.760 --> 01:02:07.960]   Something wonderful.
[01:02:07.960 --> 01:02:16.200]   Google removes the image view button from Search because Getty talked him into it.
[01:02:16.200 --> 01:02:17.200]   Now this is going to lead into...
[01:02:17.200 --> 01:02:21.960]   Talked to the individual might be a gentle way to say that.
[01:02:21.960 --> 01:02:25.880]   This is going to lead into this big New York Times Sunday feature that's coming up this
[01:02:25.880 --> 01:02:28.680]   Sunday about the case against Google.
[01:02:28.680 --> 01:02:29.680]   That's the name of it.
[01:02:29.680 --> 01:02:32.160]   But it's a little more nuanced that article than that.
[01:02:32.160 --> 01:02:39.200]   But Getty was mad because like other sites, Google, in their image search would put Getty's
[01:02:39.200 --> 01:02:45.720]   custom images there and you could click a view image button and just download it.
[01:02:45.720 --> 01:02:49.760]   And Getty felt like that was, well, foul play.
[01:02:49.760 --> 01:02:52.040]   They actually sued.
[01:02:52.040 --> 01:02:55.760]   And it's believed this is part of the settlement.
[01:02:55.760 --> 01:02:58.320]   You can still right click on the image and download it, right?
[01:02:58.320 --> 01:02:59.320]   Yeah.
[01:02:59.320 --> 01:03:04.600]   There are already Google Chrome plugins that restore the download image button.
[01:03:04.600 --> 01:03:07.440]   But I don't tell Getty about that.
[01:03:07.440 --> 01:03:15.440]   See, this is the problem when you want to be the purveyor of all the world's information.
[01:03:15.440 --> 01:03:16.440]   Yeah.
[01:03:16.440 --> 01:03:23.160]   So Google's mission statement is what to put all the...
[01:03:23.160 --> 01:03:24.840]   Organize the rules and make it accessible.
[01:03:24.840 --> 01:03:27.520]   Make it easily accessible, right?
[01:03:27.520 --> 01:03:34.720]   But as Charles Duhigg points out in his New York Times magazine piece, Google's business
[01:03:34.720 --> 01:03:37.320]   is advertising.
[01:03:37.320 --> 01:03:42.640]   And the search is really just a way of driving advertising traffic.
[01:03:42.640 --> 01:03:46.520]   And he tells some stories in here that are pretty compelling.
[01:03:46.520 --> 01:03:52.080]   Google doesn't seem to be unwilling to inconvenience competitors.
[01:03:52.080 --> 01:03:56.600]   He starts with telling the story of a company called Found Me.
[01:03:56.600 --> 01:03:57.600]   Found Me.
[01:03:57.600 --> 01:03:58.600]   Found Me.
[01:03:58.600 --> 01:04:00.240]   Found Me was a...
[01:04:00.240 --> 01:04:01.640]   What they call a vertical search.
[01:04:01.640 --> 01:04:03.480]   So Google's regular search is horizontal.
[01:04:03.480 --> 01:04:05.800]   It searches everything.
[01:04:05.800 --> 01:04:09.360]   Found Me was designed to help you shop.
[01:04:09.360 --> 01:04:14.920]   And it's something Google historically wasn't very good at searching for white tennis shoes
[01:04:14.920 --> 01:04:20.160]   with free shipping that have wide sizes under $40.
[01:04:20.160 --> 01:04:22.280]   That's a hard thing to do on Google.
[01:04:22.280 --> 01:04:24.160]   But Found Me was designed to do that.
[01:04:24.160 --> 01:04:26.580]   And apparently they were very good at it.
[01:04:26.580 --> 01:04:32.180]   But you may never have heard of them because as soon as Found Me went public, they disappeared
[01:04:32.180 --> 01:04:35.280]   from Google's search index.
[01:04:35.280 --> 01:04:37.480]   Wow.
[01:04:37.480 --> 01:04:38.480]   Google.
[01:04:38.480 --> 01:04:42.720]   They have been chasing after Google for a while.
[01:04:42.720 --> 01:04:46.060]   They actually met with the FTC in the United States.
[01:04:46.060 --> 01:04:49.360]   As we know, the FTC declined to prosecute Google over this.
[01:04:49.360 --> 01:04:56.660]   But the EU did and they felt vindicated when the EU fined Google a record $2.7 billion
[01:04:56.660 --> 01:05:00.660]   just a few months ago.
[01:05:00.660 --> 01:05:03.460]   There are many examples in here.
[01:05:03.460 --> 01:05:12.020]   And Duheg, who has written a couple of books, basically implies that it's time to break
[01:05:12.020 --> 01:05:13.020]   up Google.
[01:05:13.020 --> 01:05:19.000]   Google is not only a monopoly, but is willing to use their monopoly position and search
[01:05:19.000 --> 01:05:22.240]   to get rid of competitors.
[01:05:22.240 --> 01:05:23.240]   Jeff Jarvis?
[01:05:23.240 --> 01:05:24.920]   Well, give me a minute.
[01:05:24.920 --> 01:05:26.880]   Then I'll leave.
[01:05:26.880 --> 01:05:32.080]   I think this piece is manifestly unfair to Google.
[01:05:32.080 --> 01:05:36.320]   Google has no word at all until six paragraphs from the end where it's also kind of blown
[01:05:36.320 --> 01:05:37.320]   off.
[01:05:37.320 --> 01:05:41.040]   We're savvy enough on this show to have talked about Panda and the changes that were made
[01:05:41.040 --> 01:05:42.600]   in Panda.
[01:05:42.600 --> 01:05:47.780]   And if you go to the data, it was the new search algorithm that did in fact hurt.
[01:05:47.780 --> 01:05:48.780]   Found him.
[01:05:48.780 --> 01:05:49.860]   Found him and it hurt others.
[01:05:49.860 --> 01:05:55.500]   So about calm where I consulted when the time has bought it was was invented a whole new
[01:05:55.500 --> 01:05:58.020]   form and then demand media came along and messed it up.
[01:05:58.020 --> 01:06:02.900]   And then there were all these content farmers messing it up and Google had to have a formula
[01:06:02.900 --> 01:06:05.540]   for its algorithms to say, what are we going to do by that?
[01:06:05.540 --> 01:06:08.220]   We're going to get rid of a bad content experience.
[01:06:08.220 --> 01:06:13.980]   And yes, demand media was put down, but so about calm went out, you know, was the dolphin
[01:06:13.980 --> 01:06:17.380]   caught in the tuna net and it went out too basically went way down.
[01:06:17.380 --> 01:06:20.780]   The case of found him and I like to found him back in the day when this these complaints
[01:06:20.780 --> 01:06:21.780]   came up.
[01:06:21.780 --> 01:06:25.340]   Google's complaint is they don't want sites that are just a whole bunch of lists of links
[01:06:25.340 --> 01:06:28.340]   to make money off those links and found them.
[01:06:28.340 --> 01:06:31.180]   In fact, appears to be just that it may be better than that.
[01:06:31.180 --> 01:06:36.780]   It may be fine that but but the mechanism here although doing points out that that's
[01:06:36.780 --> 01:06:38.780]   exactly what Google is.
[01:06:38.780 --> 01:06:44.780]   Well, that's exactly what Google does a bunch of links to make money.
[01:06:44.780 --> 01:06:45.780]   It's a huge.
[01:06:45.780 --> 01:06:49.280]   No, Google actually has links to content.
[01:06:49.280 --> 01:06:51.700]   This has links to just deals and advertisers.
[01:06:51.700 --> 01:06:55.180]   Yes, because it's a shopping search engine exactly what it should have.
[01:06:55.180 --> 01:06:56.180]   Give me my minute.
[01:06:56.180 --> 01:06:57.180]   Give me my minute.
[01:06:57.180 --> 01:06:58.180]   Okay.
[01:06:58.180 --> 01:07:03.740]   So the mechanism of taking an algorithm to take down the latest trick of the spammers or
[01:07:03.740 --> 01:07:06.620]   of the manipulators.
[01:07:06.620 --> 01:07:11.860]   In this case, there are lots and lots of sites that were just link farms.
[01:07:11.860 --> 01:07:12.860]   Found them looked like that.
[01:07:12.860 --> 01:07:14.820]   Now, Google may have done a bad job or found a man.
[01:07:14.820 --> 01:07:16.180]   In fact, have been that.
[01:07:16.180 --> 01:07:19.540]   I think that they're being martyred a bit.
[01:07:19.540 --> 01:07:21.020]   I don't think it was that good a site.
[01:07:21.020 --> 01:07:22.020]   I don't think it was that wonderful.
[01:07:22.020 --> 01:07:23.820]   When I saw it back in the day, now it's down.
[01:07:23.820 --> 01:07:25.540]   So you can't really compare.
[01:07:25.540 --> 01:07:29.260]   But this is the way that Google operates and there is in this story, there is zero
[01:07:29.260 --> 01:07:33.460]   acknowledgement of that kind of manipulation and spam management.
[01:07:33.460 --> 01:07:38.620]   If we have Matt cuts here, he could talk about, I think that from a higher level that's not
[01:07:38.620 --> 01:07:41.460]   in that story whatsoever.
[01:07:41.460 --> 01:07:45.620]   This is an effort to say this is advocacy as pure as it comes to say, let's break up
[01:07:45.620 --> 01:07:48.380]   Google and here's my sob stories about what happens.
[01:07:48.380 --> 01:07:54.020]   But a lot of those cases, A, they weren't necessarily good sites or B, like about.com,
[01:07:54.020 --> 01:07:59.620]   which is a good site when that formula got manipulated too much, it went out with that
[01:07:59.620 --> 01:08:01.380]   end of spiel.
[01:08:01.380 --> 01:08:05.300]   What about this is a different technique Google uses sky hook wireless.
[01:08:05.300 --> 01:08:07.420]   We've talked about sky hook before.
[01:08:07.420 --> 01:08:10.140]   This is a different methodology.
[01:08:10.140 --> 01:08:11.580]   And I don't know the facts of this case.
[01:08:11.580 --> 01:08:14.980]   I'm just taking do Higgs assertions as fact.
[01:08:14.980 --> 01:08:17.820]   So if somebody has a counter story, that's fine.
[01:08:17.820 --> 01:08:23.620]   Sky hook, we know about they were basically a location service using Wi-Fi and they went
[01:08:23.620 --> 01:08:27.460]   around and they mapped everything so that if you didn't have GPS in the device, you
[01:08:27.460 --> 01:08:32.020]   could based on triangulating the Wi-Fi signals, you could see figure out where you were.
[01:08:32.020 --> 01:08:34.340]   This competed obviously with Google's location software.
[01:08:34.340 --> 01:08:39.700]   In fact, Google Street View went out and started to do the same thing.
[01:08:39.700 --> 01:08:46.660]   So a lawsuit filed by Sky Hook against Google in the discovery revealed an email from a
[01:08:46.660 --> 01:08:53.220]   Google manager, an internal email that said sky hooks accuracy is better than ours.
[01:08:53.220 --> 01:08:58.260]   Not long after that note, it was written according to do Higgs and the lawsuit, a high ranking
[01:08:58.260 --> 01:09:06.260]   Google official pressured Samsung and Motorola to stop using sky hook and said, if you don't,
[01:09:06.260 --> 01:09:09.780]   we could make it difficult for you to ship your phones on time.
[01:09:09.780 --> 01:09:11.940]   Google's denied this obviously.
[01:09:11.940 --> 01:09:18.500]   So soon after Samsung and Motorola canceled their sky hook accounts, sky hook sued Google
[01:09:18.500 --> 01:09:22.540]   and though one suit was dismissed, Google ended up paying $90 million to settle a patent
[01:09:22.540 --> 01:09:24.300]   infringement claim.
[01:09:24.300 --> 01:09:29.700]   But by then it was too late, sky hook founders bereft of other partnership options had been
[01:09:29.700 --> 01:09:31.900]   forced to sell their company in a large discount.
[01:09:31.900 --> 01:09:36.060]   I don't know the facts of this case, so I'm just quoting here.
[01:09:36.060 --> 01:09:41.900]   But if that's true, is that not exactly the kind of anti-competitive behavior that a monopoly
[01:09:41.900 --> 01:09:48.860]   uses to protect itself and to enter new businesses, keep others out of that business?
[01:09:48.860 --> 01:09:51.340]   It's exactly what antitrust laws meant to do.
[01:09:51.340 --> 01:09:54.220]   But I'm just going on the evidence of how the Foundum case was handled.
[01:09:54.220 --> 01:09:56.620]   I don't think it was handled journalistically very well.
[01:09:56.620 --> 01:09:57.820]   So I don't know.
[01:09:57.820 --> 01:09:59.180]   Maybe he's got it wrong.
[01:09:59.180 --> 01:10:02.220]   Maybe, and listen, Google is not pure as the driven snow.
[01:10:02.220 --> 01:10:05.180]   Google is a gigantic company where people can do bad things.
[01:10:05.180 --> 01:10:07.460]   All these companies can and they can mess up.
[01:10:07.460 --> 01:10:12.540]   And so I'm not defending them wholeheartedly against all possible sins by any means.
[01:10:12.540 --> 01:10:18.140]   Just this Foundum case, I think, is a red herring that was also jumped on by Vistegar
[01:10:18.140 --> 01:10:22.860]   in the EU, which was an excuse to go after Google.
[01:10:22.860 --> 01:10:26.500]   But did Google do other things that were bad or misused their authority or their power?
[01:10:26.500 --> 01:10:27.500]   Maybe they did.
[01:10:27.500 --> 01:10:28.500]   I don't know.
[01:10:28.500 --> 01:10:34.220]   So a couple of things here, just from having the perspective of having worked for Google.
[01:10:34.220 --> 01:10:40.060]   I cannot name a single Googler that doesn't want to do the best thing for the end users.
[01:10:40.060 --> 01:10:41.700]   I just couldn't.
[01:10:41.700 --> 01:10:43.660]   They were all fantastic in that regard.
[01:10:43.660 --> 01:10:46.380]   And I think Google's vision was to do that.
[01:10:46.380 --> 01:10:52.380]   But once you become beholden to shareholders, you have incompatible goals now.
[01:10:52.380 --> 01:10:56.060]   And I think that's a large part of the problem, trying to balance that.
[01:10:56.060 --> 01:11:01.140]   Because, quite honestly, if Google's got a competitor such as a SaaS Skyhook wireless,
[01:11:01.140 --> 01:11:02.860]   that does have a better solution.
[01:11:02.860 --> 01:11:09.260]   And I remember when this all happened, and I remember Skyhook and it was a good solution.
[01:11:09.260 --> 01:11:13.060]   The rank and file, they want the best for the users.
[01:11:13.060 --> 01:11:17.700]   But at some point, management knows the buck stops with them and with the shareholders.
[01:11:17.700 --> 01:11:19.020]   So what do they do?
[01:11:19.020 --> 01:11:20.580]   I don't know how you reconcile that.
[01:11:20.580 --> 01:11:24.140]   And Google's now a big entity that is beholden to shareholders.
[01:11:24.140 --> 01:11:26.820]   So there's a big issue there.
[01:11:26.820 --> 01:11:32.260]   But Kevin, you could be beholden to shareholders in an evil way, Uber.
[01:11:32.260 --> 01:11:37.220]   Or you could be beholden to shareholders and say that the company that we are has a standard
[01:11:37.220 --> 01:11:39.540]   that we're going to behold, because that's our business.
[01:11:39.540 --> 01:11:41.300]   I think that's closer to Google.
[01:11:41.300 --> 01:11:42.300]   Are they perfect?
[01:11:42.300 --> 01:11:43.780]   I think Google, but I think you're right.
[01:11:43.780 --> 01:11:50.780]   Everybody I've ever met at Google holds rather religiously to users.
[01:11:50.780 --> 01:11:52.740]   And so that's at least the pressure that's within.
[01:11:52.740 --> 01:11:54.220]   That's the don't be evil.
[01:11:54.220 --> 01:11:56.620]   That's the license to be able to say that.
[01:11:56.620 --> 01:12:01.500]   So we could put that this particular conversation aside.
[01:12:01.500 --> 01:12:04.980]   Because I would like to say, and we talked about this in Windows Weekly, maybe the higher
[01:12:04.980 --> 01:12:06.700]   level conversation.
[01:12:06.700 --> 01:12:12.900]   And it brings us back to Scott Galloway's book, The Four, in which he says that these
[01:12:12.900 --> 01:12:17.540]   big tech behemoths, these companies that are so big they in Doohigs, where it's blot
[01:12:17.540 --> 01:12:24.100]   out the sun, Amazon, Facebook, Apple, and Google are bound to come under scrutiny soon
[01:12:24.100 --> 01:12:26.580]   because they are getting so big.
[01:12:26.580 --> 01:12:27.580]   Well, that's right.
[01:12:27.580 --> 01:12:31.580]   There's nothing, by the way, I would not buy into the notion that just because the company's
[01:12:31.580 --> 01:12:33.180]   big, it's bad.
[01:12:33.180 --> 01:12:35.580]   I think that that's a value judgment that some people make.
[01:12:35.580 --> 01:12:38.060]   But I don't think it's going to stay big on its own.
[01:12:38.060 --> 01:12:39.060]   Yeah.
[01:12:39.060 --> 01:12:40.060]   Microsoft.
[01:12:40.060 --> 01:12:41.060]   But I think, yeah.
[01:12:41.060 --> 01:12:43.100]   Well, and Doohigs uses the Microsoft example.
[01:12:43.100 --> 01:12:48.540]   He says, if the DOJ hadn't sued Microsoft and gotten them looking over their shoulder,
[01:12:48.540 --> 01:12:49.540]   there might not be a Google.
[01:12:49.540 --> 01:12:51.340]   Yeah, see, I don't know.
[01:12:51.340 --> 01:12:56.620]   Microsoft's taken its foot off the pedal, gave Google an opportunity.
[01:12:56.620 --> 01:12:59.500]   And that's really the larger philosophical question.
[01:12:59.500 --> 01:13:01.740]   I'd love to know what you guys think.
[01:13:01.740 --> 01:13:06.180]   There's really no, there is one current of thought that says technology is different
[01:13:06.180 --> 01:13:09.740]   than any other monopoly situation.
[01:13:09.740 --> 01:13:11.300]   Doohigs mentioned standard oil.
[01:13:11.300 --> 01:13:15.060]   There's AT&T's breakup and the 60s.
[01:13:15.060 --> 01:13:19.300]   In the world of technology, some innovator is going to come along and you're going to
[01:13:19.300 --> 01:13:20.500]   get out innovated.
[01:13:20.500 --> 01:13:24.540]   And no matter how big you are, and all these companies, including, by the way, standard
[01:13:24.540 --> 01:13:30.980]   oil, got big because of technological innovations they owned, that somebody's going to come along
[01:13:30.980 --> 01:13:32.140]   and disintermediate you.
[01:13:32.140 --> 01:13:34.420]   There might be some evidence that's already starting to happen.
[01:13:34.420 --> 01:13:37.540]   Facebook's starting to slip a little bit.
[01:13:37.540 --> 01:13:43.100]   So there's some who say, in the 21st century, we don't have to worry about this.
[01:13:43.100 --> 01:13:44.100]   Don't worry.
[01:13:44.100 --> 01:13:48.660]   Trust, antitrust regulation takes so long, took more than a decade in the case of Microsoft.
[01:13:48.660 --> 01:13:49.900]   It's too slow.
[01:13:49.900 --> 01:13:51.580]   It's not effective.
[01:13:51.580 --> 01:13:57.060]   Just let the market handle it because technology, it's the nature of technology that innovation
[01:13:57.060 --> 01:13:58.780]   will solve this problem.
[01:13:58.780 --> 01:14:03.060]   But there's the other side that says, maybe that's true, but if there's going to be a
[01:14:03.060 --> 01:14:09.860]   point where Google, Facebook, Microsoft, and Apple and Amazon particularly are so big,
[01:14:09.860 --> 01:14:12.460]   Amazon's already getting there.
[01:14:12.460 --> 01:14:14.780]   Look at Walmart, just stock just tanked.
[01:14:14.780 --> 01:14:18.060]   It lost $31 billion in market cap.
[01:14:18.060 --> 01:14:20.380]   10% of its stock value.
[01:14:20.380 --> 01:14:24.540]   Toys R Us is gone.
[01:14:24.540 --> 01:14:32.860]   They'll get to be a tipping point beyond which you can't fix it because all you got is Amazon.
[01:14:32.860 --> 01:14:33.860]   So costs.
[01:14:33.860 --> 01:14:40.660]   Yeah, in the article, one of the things that was compelling to me is, and one of the things
[01:14:40.660 --> 01:14:46.180]   that I wanted to know more about in terms of found them was, were they a relative competitor
[01:14:46.180 --> 01:14:52.020]   that could make a Google shopping service irrelevant?
[01:14:52.020 --> 01:14:59.780]   And then showing the case of Skyhook where the company eventually had to sell out because
[01:14:59.780 --> 01:15:05.780]   it couldn't compete, there's some evidence that even if Google isn't doing it on purpose,
[01:15:05.780 --> 01:15:15.820]   down ranking websites or forcefully pushing apps or technologies out of business, is that
[01:15:15.820 --> 01:15:23.100]   it's not a good look for a company that really has expanded to a bunch of different features
[01:15:23.100 --> 01:15:31.740]   and is known for acquiring technologies and sometimes, either killing them or integrating
[01:15:31.740 --> 01:15:33.900]   them poorly into the system.
[01:15:33.900 --> 01:15:40.860]   And then the other thing that I'm wary of and Jeff, we can argue over this, is the algorithm
[01:15:40.860 --> 01:15:50.060]   made me do it, is one of the most difficult excuses for me to swallow at this point knowing
[01:15:50.060 --> 01:15:58.300]   that these algorithms aren't, we know what they do.
[01:15:58.300 --> 01:16:07.080]   And in the design of them, these engineers would love us to believe as accidents are
[01:16:07.080 --> 01:16:09.680]   designed by design.
[01:16:09.680 --> 01:16:14.600]   And so I don't know the case, I don't know, I never worked for Google.
[01:16:14.600 --> 01:16:19.200]   And so I can't say that this is the case with that algorithm, but we've started to see
[01:16:19.200 --> 01:16:27.360]   a pattern of these companies blaming the technology and how the technology sorts things without
[01:16:27.360 --> 01:16:32.760]   taking accountability for the effects or the consequences of those algorithms.
[01:16:32.760 --> 01:16:38.240]   Yeah, let me respond, because I think it's not so much blaming the algorithm as it is the
[01:16:38.240 --> 01:16:45.600]   ever present desire to have a rule set under the excuse of scale.
[01:16:45.600 --> 01:16:50.360]   And ironically, paradoxically, it's almost saying this is their definition of fairness.
[01:16:50.360 --> 01:16:56.000]   Well about dot com, we now have a hundred competitors to you who take your formula and
[01:16:56.000 --> 01:16:57.080]   schmoots it up.
[01:16:57.080 --> 01:17:00.400]   But we got to treat all of you exactly the same, because that's what we do.
[01:17:00.400 --> 01:17:03.840]   We have a formula that says if you look like this, this is how we're going to treat you
[01:17:03.840 --> 01:17:04.840]   now.
[01:17:04.840 --> 01:17:08.400]   And so we're not going to make a gradation difference between about dot com, which is
[01:17:08.400 --> 01:17:10.640]   good and demand media, which is bad.
[01:17:10.640 --> 01:17:12.840]   Same thing happened to Facebook.
[01:17:12.840 --> 01:17:17.800]   Facebook said, because Facebook was far, was not nearly nuanced enough about its non-social
[01:17:17.800 --> 01:17:18.960]   public content.
[01:17:18.960 --> 01:17:22.800]   It considered all public content to be basically the same, what you made earlier about media.
[01:17:22.800 --> 01:17:26.640]   And so when bad social content got them in trouble, or public content got them in trouble,
[01:17:26.640 --> 01:17:32.000]   they threw out all public content because their formula wasn't nuanced and smart enough.
[01:17:32.000 --> 01:17:35.240]   So I'm not really defending them so much as to saying this is how they think.
[01:17:35.240 --> 01:17:36.960]   And they try to come up with that formula.
[01:17:36.960 --> 01:17:39.520]   They're not blaming the algorithm so much as they're saying, we have to have a rule
[01:17:39.520 --> 01:17:40.520]   set.
[01:17:40.520 --> 01:17:45.840]   Now, what I think I'll agree with you, Joan, is they need to have a smarter rule set.
[01:17:45.840 --> 01:17:51.440]   And I agree with Kevin that I'm sure everybody at Google, I'm sure, is wonderful and really
[01:17:51.440 --> 01:17:52.640]   just wants to help the customer.
[01:17:52.640 --> 01:17:55.720]   You could probably say the same thing about Facebook and Amazon and Apple.
[01:17:55.720 --> 01:18:00.280]   But companies, corporations act in aggregate sometimes in ways that the individuals wouldn't
[01:18:00.280 --> 01:18:02.160]   necessarily support.
[01:18:02.160 --> 01:18:05.280]   Would you say the same thing about Uber?
[01:18:05.280 --> 01:18:06.800]   That's an interesting question.
[01:18:06.800 --> 01:18:10.640]   So if Uber had a monopoly, there'd be no question.
[01:18:10.640 --> 01:18:11.880]   Yeah, that's company culture.
[01:18:11.880 --> 01:18:12.880]   But I mean...
[01:18:12.880 --> 01:18:14.640]   Well, in Europe, it isn't monopoly.
[01:18:14.640 --> 01:18:19.840]   It's being kicked out of places, but if you're in Berlin or London, there's no competitor.
[01:18:19.840 --> 01:18:20.840]   There's also...
[01:18:20.840 --> 01:18:25.560]   I think you probably put this article in this opinion piece in Bloomberg from Leonid Beresheade
[01:18:25.560 --> 01:18:31.000]   Skie, which is a Russian troll name, a bot name if I ever heard one.
[01:18:31.000 --> 01:18:38.680]   Now he's saying that Margarete de Vestiger was influenced by not by a desire to help
[01:18:38.680 --> 01:18:43.560]   end users or even improve competition, but by lobbying.
[01:18:43.560 --> 01:18:47.880]   And you've always mentioned this from publishers in Germany and so forth who didn't like the
[01:18:47.880 --> 01:18:50.720]   fact that Google was beating them in the marketplace.
[01:18:50.720 --> 01:18:58.000]   And that is for sure another avenue for this stuff is that competitors sue instead of winning,
[01:18:58.000 --> 01:19:01.720]   which is the case of some of those in the Google story.
[01:19:01.720 --> 01:19:06.600]   These are some competitors and they are disadvantaged competitors because they're smaller or true.
[01:19:06.600 --> 01:19:10.280]   But yeah, that's an issue.
[01:19:10.280 --> 01:19:11.720]   But that's not entirely...
[01:19:11.720 --> 01:19:15.440]   Jeff, are they just disadvantaged because they're smaller?
[01:19:15.440 --> 01:19:21.880]   I mean, in the article, Google is portrayed as the 800 pound gorilla.
[01:19:21.880 --> 01:19:25.600]   It's not so much that they're smaller, it's just that they just don't have the influence.
[01:19:25.600 --> 01:19:28.480]   I grew up when the internet was AOL.
[01:19:28.480 --> 01:19:29.480]   It was synonymous.
[01:19:29.480 --> 01:19:31.480]   Now Google is synonymous with...
[01:19:31.480 --> 01:19:32.480]   It is the internet.
[01:19:32.480 --> 01:19:33.480]   Yes, you're not on Google.
[01:19:33.480 --> 01:19:34.480]   You don't exist.
[01:19:34.480 --> 01:19:37.760]   Yeah, but what I'm saying is that I don't think Foundum was a competitor.
[01:19:37.760 --> 01:19:45.320]   I think Foundum was a type of site making money that Google in its rule said this site
[01:19:45.320 --> 01:19:48.880]   that looked like this are almost always a bad for users.
[01:19:48.880 --> 01:19:51.520]   I mean, I have to say, I'm a...
[01:19:51.520 --> 01:19:57.200]   Twitter is an example of a company that struggles because Google and Facebook are so good at
[01:19:57.200 --> 01:20:02.440]   selling advertising that it's hard for us to sell against them.
[01:20:02.440 --> 01:20:03.440]   That's a difference.
[01:20:03.440 --> 01:20:08.040]   I wouldn't sue them saying you're putting me out of business.
[01:20:08.040 --> 01:20:13.160]   They're doing their job and they're doing a better job of it for some advertisers.
[01:20:13.160 --> 01:20:14.160]   They want all that data.
[01:20:14.160 --> 01:20:17.480]   They want all that information about who they're selling it to.
[01:20:17.480 --> 01:20:18.480]   I'm not going to give them that.
[01:20:18.480 --> 01:20:22.320]   Well, I've long argued that where Google is most in trouble and Facebook for most in trouble
[01:20:22.320 --> 01:20:25.240]   when it comes to antitrust and monopoly is not in search.
[01:20:25.240 --> 01:20:26.240]   It's not in social.
[01:20:26.240 --> 01:20:27.400]   It's not in any of those areas.
[01:20:27.400 --> 01:20:28.840]   It is in the advertising market.
[01:20:28.840 --> 01:20:32.600]   But it's all, but see, that's the point of this article which is that Google's advertising
[01:20:32.600 --> 01:20:35.000]   revenue drives all these decisions.
[01:20:35.000 --> 01:20:36.760]   Well, I'm not saying that.
[01:20:36.760 --> 01:20:40.360]   I'm saying that Google has the power of God in advertising.
[01:20:40.360 --> 01:20:45.920]   If its rule puts you, if they decide that you are a bad actor, then you're out.
[01:20:45.920 --> 01:20:46.920]   They might have done that.
[01:20:46.920 --> 01:20:49.480]   Do we want any company to have that capability?
[01:20:49.480 --> 01:20:52.640]   So I have argued that I've long said, I've said this on the show a dozen times over the
[01:20:52.640 --> 01:20:59.000]   years since the beginning, I think Google would be wise to have juries of peers.
[01:20:59.000 --> 01:21:02.720]   If you look at this way, if they had 100 advertisers, trusted good advertisers and said, this
[01:21:02.720 --> 01:21:06.720]   place has come along and they've complained that found them says we're treating them wrong.
[01:21:06.720 --> 01:21:12.480]   Now the advertisers have an interest in having a credible marketplace.
[01:21:12.480 --> 01:21:16.240]   And if the advertisers say, no, no, no, they're actually okay, they're all right, then you
[01:21:16.240 --> 01:21:20.880]   go to Jones question of how do you change the rule to allow them in and say this defines
[01:21:20.880 --> 01:21:22.600]   good and this defines bad.
[01:21:22.600 --> 01:21:25.960]   But Google does have the power of God in that one instance.
[01:21:25.960 --> 01:21:28.480]   And I think that's what's going to get in trouble in the long run, yes.
[01:21:28.480 --> 01:21:32.840]   And they should have a couple of structures that were different to account for that.
[01:21:32.840 --> 01:21:39.160]   I mean, I believe I should say very clearly in our advertising model that it's more efficient,
[01:21:39.160 --> 01:21:43.960]   it works better than automated ads placements in Facebook and Google.
[01:21:43.960 --> 01:21:49.720]   But it's hard to get above the noise that Facebook and Google make and convince advertisers
[01:21:49.720 --> 01:21:50.720]   of that.
[01:21:50.720 --> 01:21:51.720]   But that's my job.
[01:21:51.720 --> 01:21:55.560]   That's not my job particularly, but that's our job as a company is to say, well, we believe
[01:21:55.560 --> 01:21:58.360]   we have a credible alternative.
[01:21:58.360 --> 01:22:01.640]   But if Google should decide for some reason that we were a threat, see, that's why I want
[01:22:01.640 --> 01:22:02.640]   to stay small.
[01:22:02.640 --> 01:22:03.640]   That's that's it.
[01:22:03.640 --> 01:22:08.240]   And say, oh, well, we're not going to put you in the search rankings.
[01:22:08.240 --> 01:22:12.040]   I mean, if that's what happened to found them, that's appalling.
[01:22:12.040 --> 01:22:16.000]   If, if, yeah.
[01:22:16.000 --> 01:22:20.560]   So now the so that's the higher level question is we don't I think we I think we could agree
[01:22:20.560 --> 01:22:25.080]   we don't have enough data to know if this stuff will rent right itself automatically.
[01:22:25.080 --> 01:22:28.640]   If it's the nature of the technology business cycle.
[01:22:28.640 --> 01:22:34.440]   But the problem is if we if we don't act, there will be a tipping point beyond which
[01:22:34.440 --> 01:22:35.640]   there's no recovery.
[01:22:35.640 --> 01:22:40.640]   I mean, it'll just be like, well, that's it's Google isn't you can't compete with Google.
[01:22:40.640 --> 01:22:41.680]   You just can't beat Amazon.
[01:22:41.680 --> 01:22:44.520]   You just Facebook owns this, right?
[01:22:44.520 --> 01:22:49.040]   Well, that gets to your question that you asked earlier about, you know, monopolies and
[01:22:49.040 --> 01:22:50.040]   competition.
[01:22:50.040 --> 01:22:54.040]   You know, there's always been some new innovation to dismantle or take the place of a large
[01:22:54.040 --> 01:22:55.560]   company.
[01:22:55.560 --> 01:23:03.760]   At some point, let me backtrack Google's currency is information.
[01:23:03.760 --> 01:23:06.200]   It just the product is information.
[01:23:06.200 --> 01:23:11.080]   All these prior examples of standard oil and Microsoft and all that, those were like
[01:23:11.080 --> 01:23:16.600]   physical products, but at such a granular level, can it be any more granular than information?
[01:23:16.600 --> 01:23:21.640]   How does somebody out Google Google at this point is, you know, with IoT with connected
[01:23:21.640 --> 01:23:22.640]   cars?
[01:23:22.640 --> 01:23:26.280]   Everything is information and information is currency today.
[01:23:26.280 --> 01:23:28.840]   What would it take to out Google Google?
[01:23:28.840 --> 01:23:30.640]   I don't think there could be.
[01:23:30.640 --> 01:23:32.120]   Here's the question.
[01:23:32.120 --> 01:23:36.760]   I think that if you go back to follow the money, I think that yeah, it's hard to be
[01:23:36.760 --> 01:23:38.360]   Google at information.
[01:23:38.360 --> 01:23:49.320]   I do think that the message based ad model at scale scale for scale sake is can be challenged.
[01:23:49.320 --> 01:23:52.880]   You see what ad agencies are finally hitting some rocks.
[01:23:52.880 --> 01:23:55.360]   That's why we do handmade advertising.
[01:23:55.360 --> 01:23:56.360]   Exactly.
[01:23:56.360 --> 01:24:02.480]   That's the name of our agency, by the way, is artisanal, Joan.
[01:24:02.480 --> 01:24:03.480]   So thank you.
[01:24:03.480 --> 01:24:04.480]   Oh my goodness.
[01:24:04.480 --> 01:24:05.480]   Do you want some make pickles?
[01:24:05.480 --> 01:24:09.920]   No, no, we don't have pickles or beer, but that'd be a better business, Joan.
[01:24:09.920 --> 01:24:15.240]   All our ads are handmade by craftsmen in our little workshop here in Petaluma, California.
[01:24:15.240 --> 01:24:17.520]   Actually, that's literally true.
[01:24:17.520 --> 01:24:23.920]   That's kind of our way of trying to compete.
[01:24:23.920 --> 01:24:33.080]   I'm not against Google because of scale, but they do have a lot of power.
[01:24:33.080 --> 01:24:37.960]   Joan, you're an expert on how much power Facebook has in the political arena.
[01:24:37.960 --> 01:24:39.960]   Yes.
[01:24:39.960 --> 01:24:44.760]   This is one of the things that people have been trying to measure over the last decade
[01:24:44.760 --> 01:24:56.280]   is how much political influence and opportunity does that present to political clients, right?
[01:24:56.280 --> 01:25:01.480]   And running social media through politics or running your politics through social media,
[01:25:01.480 --> 01:25:02.480]   rather.
[01:25:02.480 --> 01:25:10.280]   But also, I think that what's difficult about this case in particular with Foundum is that
[01:25:10.280 --> 01:25:17.720]   you have a part of the search engine that is devoted to making money, right, about
[01:25:17.720 --> 01:25:24.040]   sharing shopping links, which often has some kind of advertising revenue attached to it.
[01:25:24.040 --> 01:25:30.160]   And so if people in their mind find Foundum on Google and then know to go to Foundum every
[01:25:30.160 --> 01:25:35.800]   time they want to search for a thing, then it presents a problem for Google because there's
[01:25:35.800 --> 01:25:38.000]   no longer that starting point.
[01:25:38.000 --> 01:25:45.720]   And that's an incentive to move people or to move other search engines off the platform.
[01:25:45.720 --> 01:25:49.080]   For instance, if you're searching for a flight, you might have a search engine.
[01:25:49.080 --> 01:25:50.920]   Google now owns travel, right?
[01:25:50.920 --> 01:25:56.080]   Well, they're trying to, but I'll tell you, if you search for flights, depending on the
[01:25:56.080 --> 01:26:02.480]   machine that you're using and the platform that you start at, you will get the same flight
[01:26:02.480 --> 01:26:04.280]   offered to you at different prices.
[01:26:04.280 --> 01:26:05.280]   I know.
[01:26:05.280 --> 01:26:08.560]   And location, I think data is implicated in essence.
[01:26:08.560 --> 01:26:13.120]   You know, finding Joan is that I'm a United Flyer, fly everything United because that's
[01:26:13.120 --> 01:26:15.120]   where my miles are.
[01:26:15.120 --> 01:26:19.680]   And one trip that I was going to do coming up and I couldn't find the right flight fares
[01:26:19.680 --> 01:26:24.680]   and somebody used Google search and found far better fares on United than I was finding.
[01:26:24.680 --> 01:26:25.680]   Wow.
[01:26:25.680 --> 01:26:26.760]   Really interesting.
[01:26:26.760 --> 01:26:29.120]   So here's what Google says.
[01:26:29.120 --> 01:26:35.160]   When you come to Google, when you search for what's the weather on Google, you don't
[01:26:35.160 --> 01:26:37.080]   want to link to weather.com.
[01:26:37.080 --> 01:26:38.160]   You want the weather.
[01:26:38.160 --> 01:26:39.920]   So we're going to show you the weather.
[01:26:39.920 --> 01:26:45.160]   Now if I'm weather.com and they've scraped my weather and put it there.
[01:26:45.160 --> 01:26:49.360]   So the searcher no longer comes to weather.com.
[01:26:49.360 --> 01:26:51.920]   I think both sides, there's merit in both arguments.
[01:26:51.920 --> 01:26:52.920]   It's true.
[01:26:52.920 --> 01:26:55.240]   Google is serving the users interests.
[01:26:55.240 --> 01:26:57.760]   They're giving you the temperature and forecast.
[01:26:57.760 --> 01:27:01.320]   But boy, weather.com has a very good reason to be pissed off.
[01:27:01.320 --> 01:27:08.440]   Well, yeah, and this used to happen as an interesting story in Phil Lapsley's Exploding
[01:27:08.440 --> 01:27:13.400]   the Phone about this where people were calling operators for information.
[01:27:13.400 --> 01:27:16.600]   And this small town, there were two mortuaries.
[01:27:16.600 --> 01:27:21.800]   And so when someone died, they'd call the operator and the operator happened to be married
[01:27:21.800 --> 01:27:23.960]   to someone who owned a mortuary.
[01:27:23.960 --> 01:27:28.360]   So she would automatically give her husband's business name.
[01:27:28.360 --> 01:27:33.360]   And in that, the other mortician wasn't getting any business.
[01:27:33.360 --> 01:27:34.960]   And he couldn't understand why.
[01:27:34.960 --> 01:27:35.960]   And then he figured out--
[01:27:35.960 --> 01:27:37.760]   Well, his customers had died.
[01:27:37.760 --> 01:27:41.120]   Well, that would be good for business, right?
[01:27:41.120 --> 01:27:42.120]   In that case.
[01:27:42.120 --> 01:27:46.240]   But what was interesting is because we didn't have a telephone switch system, but we had
[01:27:46.240 --> 01:27:54.200]   humans networking the calls is that this business got a lot more attention.
[01:27:54.200 --> 01:28:01.160]   And so in some ways, we have someone networking the information in a way that advantages them
[01:28:01.160 --> 01:28:03.280]   and advantages in house products.
[01:28:03.280 --> 01:28:05.760]   Yeah, people's basically the mortician's wife in this.
[01:28:05.760 --> 01:28:08.960]   OK, but well, but OK, but let's look where Google gets it from all sides.
[01:28:08.960 --> 01:28:11.560]   So we talked about this before we were on the air.
[01:28:11.560 --> 01:28:19.200]   Edible arrangements is suing Google because 1,800 flowers is buying a search to give people
[01:28:19.200 --> 01:28:21.920]   an alternative to edible arrangements if you search for edible arrangements.
[01:28:21.920 --> 01:28:24.440]   And Edible arrangements is trying to say, no, people search for us.
[01:28:24.440 --> 01:28:26.120]   That's all you should show them.
[01:28:26.120 --> 01:28:28.240]   And that's going to be a hard case to me.
[01:28:28.240 --> 01:28:30.080]   Shouldn't get any alternatives there.
[01:28:30.080 --> 01:28:32.320]   There's no-- that's going to be a very hard case to win.
[01:28:32.320 --> 01:28:36.160]   And that would be like me saying, oh, Google, you're too good at advertising.
[01:28:36.160 --> 01:28:37.600]   I'm going to sue you.
[01:28:37.600 --> 01:28:40.160]   That's a company just-- you got to compete.
[01:28:40.160 --> 01:28:43.880]   You got to be better than Sherry's cherries.
[01:28:43.880 --> 01:28:48.120]   If the information were arranged in such a way that even if you Google that
[01:28:48.120 --> 01:28:51.960]   you had a couple of wonderful arrangements or one hated-- one, 800 flowers,
[01:28:51.960 --> 01:28:58.080]   what you'd come up with would be a sort of telephone book style information where,
[01:28:58.080 --> 01:29:01.320]   you know, it's all slotted under gifts, right?
[01:29:01.320 --> 01:29:03.240]   Well, here-- I think their position is--
[01:29:03.240 --> 01:29:06.960]   That's a way of serving the information differently in terms of the way that you
[01:29:06.960 --> 01:29:09.960]   organize the content related to the category itself.
[01:29:09.960 --> 01:29:10.960]   Right.
[01:29:10.960 --> 01:29:14.320]   So here's the-- I did an incognito page because as you point out, it won't look the same
[01:29:14.320 --> 01:29:15.720]   if I do it with my own search.
[01:29:15.720 --> 01:29:19.840]   I think Edible Arrangement's point is, well, look, somebody searched for Edible Arrangement's.
[01:29:19.840 --> 01:29:22.320]   Clearly, all they're interested in is us.
[01:29:22.320 --> 01:29:27.360]   Why do you allow our competitors to buy ads on the right-hand side?
[01:29:27.360 --> 01:29:30.040]   But that's the ad unit right there, right?
[01:29:30.040 --> 01:29:31.760]   Yeah, and I may not.
[01:29:31.760 --> 01:29:36.880]   I'm now used to saying that if I search for Nike, maybe I want to see what--
[01:29:36.880 --> 01:29:41.320]   Now, if Edible Arrangements didn't buy this ad at the top of the page, they would not
[01:29:41.320 --> 01:29:44.360]   be at the top of the column in search results.
[01:29:44.360 --> 01:29:46.040]   You see what happens here.
[01:29:46.040 --> 01:29:47.800]   And I think this is defensive.
[01:29:47.800 --> 01:29:50.640]   So these are all ad units.
[01:29:50.640 --> 01:29:52.640]   This block and this block ad units.
[01:29:52.640 --> 01:29:54.280]   This is the first organic search.
[01:29:54.280 --> 01:29:55.720]   It's Edible Arrangements.
[01:29:55.720 --> 01:29:58.280]   But it's almost off the page.
[01:29:58.280 --> 01:30:04.000]   If Edible Arrangements hadn't purchased this ad, the number one result from the point of
[01:30:04.000 --> 01:30:07.640]   view of a consumer doesn't matter that it says ad there is shop fruit arrangements from
[01:30:07.640 --> 01:30:09.480]   a competitor.
[01:30:09.480 --> 01:30:11.600]   So I could see their point of view.
[01:30:11.600 --> 01:30:13.480]   Leo searched for adable arrangements.
[01:30:13.480 --> 01:30:16.760]   Instead, the first result kind of organic results.
[01:30:16.760 --> 01:30:18.720]   This is actually the prior law on this.
[01:30:18.720 --> 01:30:21.320]   This is the trademark case.
[01:30:21.320 --> 01:30:22.600]   So I forget who it was.
[01:30:22.600 --> 01:30:29.600]   It was some cosmetics manufacturer said that if you search for Chanel, you shouldn't get--
[01:30:29.600 --> 01:30:30.600]   Right.
[01:30:30.600 --> 01:30:32.760]   Yeah, that's a counterfeit result.
[01:30:32.760 --> 01:30:33.760]   Interesting.
[01:30:33.760 --> 01:30:38.960]   Well, not so much counterfeit is that you were-- I was using somebody else's trademark.
[01:30:38.960 --> 01:30:39.960]   Oh.
[01:30:39.960 --> 01:30:41.160]   And what happened in that case?
[01:30:41.160 --> 01:30:42.160]   I forget.
[01:30:42.160 --> 01:30:43.160]   Oh.
[01:30:43.160 --> 01:30:44.160]   Jesus Christ.
[01:30:44.160 --> 01:30:51.880]   The real problem here is that there-- you can't trust jurists and juries in cases like
[01:30:51.880 --> 01:30:53.720]   this to come up with consistent answers.
[01:30:53.720 --> 01:30:55.560]   That's the real problem.
[01:30:55.560 --> 01:31:00.800]   And I point to this weird link embedding case.
[01:31:00.800 --> 01:31:06.080]   Do you mind if we move on to that one for edible arrangements?
[01:31:06.080 --> 01:31:07.080]   That's fine.
[01:31:07.080 --> 01:31:12.080]   I'm serious.
[01:31:12.080 --> 01:31:13.080]   What?
[01:31:13.080 --> 01:31:15.080]   I just searched for this.
[01:31:15.080 --> 01:31:16.080]   I haven't read it yet.
[01:31:16.080 --> 01:31:18.080]   But how can we resist this?
[01:31:18.080 --> 01:31:19.080]   Hold on.
[01:31:19.080 --> 01:31:20.080]   OK.
[01:31:20.080 --> 01:31:21.080]   Here we go.
[01:31:21.080 --> 01:31:26.080]   Is this in the Westlaw database?
[01:31:26.080 --> 01:31:30.080]   Is trademark use in Google AdWords trademark infringement?
[01:31:30.080 --> 01:31:31.080]   Wow.
[01:31:31.080 --> 01:31:32.080]   On May--
[01:31:32.080 --> 01:31:33.080]   Oh, yeah.
[01:31:33.080 --> 01:31:34.080]   TisC helps.
[01:31:34.080 --> 01:31:36.080]   Is those ugly Christmas sweaters?
[01:31:36.080 --> 01:31:42.600]   Well, the complaint against ugly Christmas sweater, Inc. based on Christmas sweater, advertised
[01:31:42.600 --> 01:31:47.240]   material that appeared as a result of googly in the term tipsy elves.
[01:31:47.240 --> 01:31:48.760]   See, that's the same thing.
[01:31:48.760 --> 01:31:50.520]   You're searching for tipsy elves.
[01:31:50.520 --> 01:31:53.720]   Instead, you're getting ugly Christmas sweater.
[01:31:53.720 --> 01:31:55.080]   And did they win?
[01:31:55.080 --> 01:31:59.000]   It's similar to a case of Rosetta Stone.
[01:31:59.000 --> 01:32:00.400]   I don't know.
[01:32:00.400 --> 01:32:02.360]   This is June 2017.
[01:32:02.360 --> 01:32:06.360]   They filed the complaint in 2017, so it's probably Stone's litigation.
[01:32:06.360 --> 01:32:10.120]   Rosetta Stone.
[01:32:10.120 --> 01:32:12.360]   I don't know.
[01:32:12.360 --> 01:32:16.600]   Well, I just hope tipsy elves wins.
[01:32:16.600 --> 01:32:21.320]   If you use tipsy elves as a show title, are you--
[01:32:21.320 --> 01:32:22.320]   Oh.
[01:32:22.320 --> 01:32:23.320]   --in the spreadsheet out there?
[01:32:23.320 --> 01:32:27.640]   Only if I tell people to buy tipsy elves ugly Christmas sweaters, not ugly Christmas
[01:32:27.640 --> 01:32:31.360]   sweaters, ugly Christmas sweaters.
[01:32:31.360 --> 01:32:34.840]   You know, one aspect we haven't touched upon.
[01:32:34.840 --> 01:32:37.640]   What about when everybody's got a Google Home or a--
[01:32:37.640 --> 01:32:38.640]   That's right.
[01:32:38.640 --> 01:32:39.640]   --whatever--
[01:32:39.640 --> 01:32:40.640]   Sorry, shouldn't have said her name.
[01:32:40.640 --> 01:32:41.640]   Cancel.
[01:32:41.640 --> 01:32:43.200]   She's talking right now.
[01:32:43.200 --> 01:32:44.800]   An echo, et cetera.
[01:32:44.800 --> 01:32:50.440]   Now you say, show me a product or whatever it is.
[01:32:50.440 --> 01:32:51.440]   It's just going to come in.
[01:32:51.440 --> 01:32:52.640]   You're not going to get a list to choose from.
[01:32:52.640 --> 01:32:54.920]   You're absolutely right.
[01:32:54.920 --> 01:32:57.280]   Absolutely right.
[01:32:57.280 --> 01:33:01.120]   And that's an issue all around is that-- is that-- how do you-- are they going to sell
[01:33:01.120 --> 01:33:02.120]   that position?
[01:33:02.120 --> 01:33:05.960]   Who's never sold search?
[01:33:05.960 --> 01:33:13.440]   Amazon has implied that it will sell its echo results.
[01:33:13.440 --> 01:33:20.000]   In some way, they've already said we're going to do some sort of ad sales around echo.
[01:33:20.000 --> 01:33:21.800]   That doesn't mean they're going to sell the recognition.
[01:33:21.800 --> 01:33:25.680]   So if you're Kevin and you ask for deodorant--
[01:33:25.680 --> 01:33:28.640]   I don't know why I came up with that.
[01:33:28.640 --> 01:33:29.640]   Yeah.
[01:33:29.640 --> 01:33:32.120]   Well, Amazon will do is to say--
[01:33:32.120 --> 01:33:40.240]   Now what it does right now, if I say I have stinky pits, Amazon echo, get me some deodorant.
[01:33:40.240 --> 01:33:47.240]   Right now we'll say, in your prime search results, what you've bought before is this.
[01:33:47.240 --> 01:33:52.360]   But what if it said, in the past you've bought this, but what about this?
[01:33:52.360 --> 01:33:53.560]   We've got a great deal.
[01:33:53.560 --> 01:33:55.720]   How about a coupon for this?
[01:33:55.720 --> 01:34:02.920]   Well, in fact, if you say to Pepsi, somebody searching for Pepsi, then you can buy the
[01:34:02.920 --> 01:34:04.400]   opportunity to push Coke.
[01:34:04.400 --> 01:34:05.400]   Yes.
[01:34:05.400 --> 01:34:06.800]   Which is a holy grail of the--
[01:34:06.800 --> 01:34:12.680]   Amazon is currently according to CNBC and talks with Clorox, Proctor and Gamel and others
[01:34:12.680 --> 01:34:17.320]   to promote their products on echo.
[01:34:17.320 --> 01:34:24.820]   They're testing various ad types, including videos and promoted paid search results.
[01:34:24.820 --> 01:34:27.280]   So this is exactly what you were talking about.
[01:34:27.280 --> 01:34:30.180]   Yes, it's also a huge issue for media.
[01:34:30.180 --> 01:34:33.680]   How do you know who give me the news?
[01:34:33.680 --> 01:34:34.680]   Right.
[01:34:34.680 --> 01:34:35.680]   Well, who do you want?
[01:34:35.680 --> 01:34:38.800]   And you can go on Google and you can choose places, but that's like doing my Yahoo.
[01:34:38.800 --> 01:34:39.720]   Nobody never did that.
[01:34:39.720 --> 01:34:48.420]   So for the new Apple HomePod, if you say that to, hey, Shlomo, she will say, "I'm playing
[01:34:48.420 --> 01:34:52.920]   NPR, but if you want me to play Fox News or CNN, let me know."
[01:34:52.920 --> 01:34:54.800]   That's right at the first time you'd play news.
[01:34:54.800 --> 01:34:56.740]   And from then on, you don't get a choice.
[01:34:56.740 --> 01:34:58.300]   Yeah, but I'm going to do it.
[01:34:58.300 --> 01:34:59.300]   I'll try and make--
[01:34:59.300 --> 01:35:00.300]   Do you have a HomePod?
[01:35:00.300 --> 01:35:02.300]   Give me the news.
[01:35:02.300 --> 01:35:03.300]   New news.
[01:35:03.300 --> 01:35:04.700]   NPR News, yeah.
[01:35:04.700 --> 01:35:08.660]   Is that HomePod or Amazon?
[01:35:08.660 --> 01:35:10.660]   That's, of course not.
[01:35:10.660 --> 01:35:14.300]   I didn't think it was.
[01:35:14.300 --> 01:35:16.060]   I was kind of interested.
[01:35:16.060 --> 01:35:17.060]   It's my Google Home.
[01:35:17.060 --> 01:35:18.060]   Yeah.
[01:35:18.060 --> 01:35:19.060]   So how dare you?
[01:35:19.060 --> 01:35:20.060]   How dare you?
[01:35:20.060 --> 01:35:21.620]   I have them all.
[01:35:21.620 --> 01:35:23.660]   In fact, I have a stack.
[01:35:23.660 --> 01:35:26.160]   I have a Google Home Max, and on top of that, I have an Echo show.
[01:35:26.160 --> 01:35:27.960]   And on top of that, I have the HomePod.
[01:35:27.960 --> 01:35:29.560]   And are they leaving rings on each other?
[01:35:29.560 --> 01:35:30.560]   Yeah, who are you kidding?
[01:35:30.560 --> 01:35:34.880]   That's my-- by the way, that's my joke, is I don't have to worry, because like any sensible
[01:35:34.880 --> 01:35:37.120]   person, I put a HomePod on top of an Echo.
[01:35:37.120 --> 01:35:42.080]   Actually, I put a top of a Sonos.
[01:35:42.080 --> 01:35:43.720]   So you were going to change subjects a while ago.
[01:35:43.720 --> 01:35:48.920]   I was going to talk about Lincoln betting, but I can't find the-- here it is.
[01:35:48.920 --> 01:35:55.500]   And the EFF's all in a Tizzy, Federal Judge says, "Embedding a tweet can be a copyright
[01:35:55.500 --> 01:35:56.500]   infringement."
[01:35:56.500 --> 01:35:58.460]   Did you see this?
[01:35:58.460 --> 01:36:00.140]   This is Tizzy worthy.
[01:36:00.140 --> 01:36:01.140]   You bet.
[01:36:01.140 --> 01:36:03.140]   Tizzy worthy.
[01:36:03.140 --> 01:36:06.340]   So a photographer is upset.
[01:36:06.340 --> 01:36:13.780]   Justin Goldman, he put on Snapchat a picture that he took of Tom Brady.
[01:36:13.780 --> 01:36:17.100]   Somebody tweeted his Snapchat picture.
[01:36:17.100 --> 01:36:23.600]   At which point, Breitbart, Time, Yahoo, Vox Media, and Boston Globe, among others, retweeted
[01:36:23.600 --> 01:36:27.920]   the tweet with his picture in it without giving him credit.
[01:36:27.920 --> 01:36:33.680]   Goldman is suing saying-- or sued, saying those stories infringe my copyright.
[01:36:33.680 --> 01:36:41.920]   Now this is not settled yet, because this is-- he asked for a-- I guess the newspapers
[01:36:41.920 --> 01:36:44.280]   asked for a summary judgment.
[01:36:44.280 --> 01:36:46.160]   And the judge said, "No, I don't think so.
[01:36:46.160 --> 01:36:47.860]   We're going to trial on this one."
[01:36:47.860 --> 01:36:51.700]   I think there's some merit in this.
[01:36:51.700 --> 01:36:54.980]   Judge Catherine Forrest of-- which circuit?
[01:36:54.980 --> 01:36:55.980]   I can't remember.
[01:36:55.980 --> 01:37:02.120]   The New York Circuit said, "You can infringe copyright by embedding a tweet in a web page
[01:37:02.120 --> 01:37:09.340]   contrary to the long-standing perfect 10 versus Amazon test."
[01:37:09.340 --> 01:37:15.840]   She said this test, which is whose server does the infringing material lie on, is based in
[01:37:15.840 --> 01:37:22.840]   part on a-- Catherine Forrest rejected the Ninth Circuit's server test based in part on a surprising
[01:37:22.840 --> 01:37:27.520]   approach, I'm quoting the EFF, to the process of embedding.
[01:37:27.520 --> 01:37:31.040]   The opinion describes a simple process of embedding a Twitter image, something done every day
[01:37:31.040 --> 01:37:35.600]   by millions of ordinary internet users, as if it were a highly technical process, done
[01:37:35.600 --> 01:37:37.760]   by coders.
[01:37:37.760 --> 01:37:42.780]   She says, "That process puts publishers, not servers in the driver's seat.
[01:37:42.780 --> 01:37:46.300]   When defendants cause the embedded tweets to appear on their websites, their actions
[01:37:46.300 --> 01:37:49.980]   violated the photographer's exclusive display right.
[01:37:49.980 --> 01:37:57.020]   The fact that the image was hosted on Twitter's server does not shield them from this result."
[01:37:57.020 --> 01:37:58.020]   Watch out internet.
[01:37:58.020 --> 01:37:59.020]   So--
[01:37:59.020 --> 01:38:00.580]   Call me crazy.
[01:38:00.580 --> 01:38:05.020]   Call me crazy, but if you are going to share-- if I'm going to share a photo on a social
[01:38:05.020 --> 01:38:06.500]   sharing service--
[01:38:06.500 --> 01:38:08.820]   Well, he shared it on Snapchat.
[01:38:08.820 --> 01:38:10.800]   I don't know if he shared it publicly.
[01:38:10.800 --> 01:38:12.560]   I'm not sure that's unclear.
[01:38:12.560 --> 01:38:15.160]   Then don't share it at all.
[01:38:15.160 --> 01:38:16.160]   Right.
[01:38:16.160 --> 01:38:17.160]   Right.
[01:38:17.160 --> 01:38:18.160]   You know, sell it to Getty.
[01:38:18.160 --> 01:38:22.120]   But sharing it on Snapchat, I don't think abrogates his copyright.
[01:38:22.120 --> 01:38:23.120]   No, no.
[01:38:23.120 --> 01:38:26.120]   I'm from a legal standpoint, I understand and agree with you.
[01:38:26.120 --> 01:38:28.800]   I'm just thinking, you know, it just comes in sense.
[01:38:28.800 --> 01:38:33.080]   We're setting ourselves up to fail when we forget how easy these tools are to use,
[01:38:33.080 --> 01:38:35.800]   how much they're used, and what they're used for.
[01:38:35.800 --> 01:38:42.800]   Absolutely.
[01:38:42.800 --> 01:38:46.440]   So yeah, I'm in a Tizzy too.
[01:38:46.440 --> 01:38:47.440]   You're in a Tizzy too.
[01:38:47.440 --> 01:38:53.220]   I think there's still going to be a trial on this, but I might be mistaken.
[01:38:53.220 --> 01:38:58.780]   I think this was a lot of places to file friends of the court applications in an appeal.
[01:38:58.780 --> 01:38:59.780]   Yeah.
[01:38:59.780 --> 01:39:06.260]   But it's like, I don't know, this is spinning my brain because isn't a retweet posting an
[01:39:06.260 --> 01:39:08.260]   embedded tweet?
[01:39:08.260 --> 01:39:11.340]   Like, wouldn't this trouble the entire platform?
[01:39:11.340 --> 01:39:12.340]   Yes.
[01:39:12.340 --> 01:39:13.740]   Oh, it'd be a huge problem.
[01:39:13.740 --> 01:39:21.540]   It'd make any sense for copyright to intervene in this way because that is the fundamental
[01:39:21.540 --> 01:39:28.900]   technology and yeah, sharing and amplification are part and parcel of that.
[01:39:28.900 --> 01:39:33.700]   So I'm just like, I'm just like, you know, I'm like looking into a mirror of a mirror,
[01:39:33.700 --> 01:39:39.180]   I'm trying to understand like where would it even end when it comes to sharing other
[01:39:39.180 --> 01:39:46.340]   people's thoughts and commenting on them and the conversant nature of social media.
[01:39:46.340 --> 01:39:53.340]   But yeah, if you do want copyright protections, then you're best to watermark your photos
[01:39:53.340 --> 01:39:57.580]   and crop them in such a way that if someone wants to use the whole thing, they have to
[01:39:57.580 --> 01:39:59.780]   contact you directly.
[01:39:59.780 --> 01:40:04.020]   You know, there are lots of ways in which people get paid for their content and those
[01:40:04.020 --> 01:40:06.140]   avenues are very clear.
[01:40:06.140 --> 01:40:13.820]   And so what is not clear is the avenues that are, you know, places where we know content
[01:40:13.820 --> 01:40:20.780]   production is intended to be shared for free.
[01:40:20.780 --> 01:40:28.180]   The judge at the end said, do not, we don't view this as being a decision that will have
[01:40:28.180 --> 01:40:30.580]   global consequences.
[01:40:30.580 --> 01:40:38.060]   She says there are a number of strong defenses to liability that separate this issue.
[01:40:38.060 --> 01:40:43.020]   In particular, genuine questions about whether the plaintiff effectively released his image
[01:40:43.020 --> 01:40:46.260]   of the public domain, we posted it to Snapchat.
[01:40:46.260 --> 01:40:50.780]   So she says there's also a very serious and strong fair use defense, defense under the
[01:40:50.780 --> 01:40:54.380]   DMCA and limitations on damages from innocent infringement.
[01:40:54.380 --> 01:40:59.700]   So she's merely throwing out the summary judgment without, without, she say, I don't, I think
[01:40:59.700 --> 01:41:01.580]   she doesn't want to set up precedent.
[01:41:01.580 --> 01:41:07.580]   It's an interesting, it's an interesting decision though, an interesting to read.
[01:41:07.580 --> 01:41:11.860]   Not the least of which because her first sentence says, when the Copyright Act was amended
[01:41:11.860 --> 01:41:19.420]   in 1976, the words tweet viral and embed, invoke thoughts of a bird as a zeeze and a reporter.
[01:41:19.420 --> 01:41:22.900]   This is a text savvy judge.
[01:41:22.900 --> 01:41:24.620]   Decades later, they've taken on new meetings.
[01:41:24.620 --> 01:41:29.340]   Yeah, there is the EFF was upset about the way she understood embedding, but I don't know
[01:41:29.340 --> 01:41:31.060]   if she's, I don't, I don't know.
[01:41:31.060 --> 01:41:34.020]   This interesting anyway, not over yet.
[01:41:34.020 --> 01:41:39.540]   Well, my saying is different than taking a screenshot, right?
[01:41:39.540 --> 01:41:40.540]   Right.
[01:41:40.540 --> 01:41:47.820]   So in a proper attribution then, uh, to the tweeter, not to the photographer.
[01:41:47.820 --> 01:41:51.820]   So then the first violation is really from the person who took it from Snapchat hosted
[01:41:51.820 --> 01:41:52.820]   on Twitter.
[01:41:52.820 --> 01:41:56.260]   This is section 230 question of, of the safe harbor.
[01:41:56.260 --> 01:41:57.260]   Right.
[01:41:57.260 --> 01:41:59.900]   Yeah, no, Twitter is not a defendant in this.
[01:41:59.900 --> 01:42:00.900]   Right.
[01:42:00.900 --> 01:42:04.500]   The publishers who retweeted the tweet are the defenders, defenders.
[01:42:04.500 --> 01:42:10.340]   Because they purposely did, but if they were, they not, with knowledge took that picture
[01:42:10.340 --> 01:42:11.340]   and retweets.
[01:42:11.340 --> 01:42:15.380]   Let's say time magazine is one of the defense retweeted that tweet.
[01:42:15.380 --> 01:42:17.820]   So I don't, it's an interesting case.
[01:42:17.820 --> 01:42:21.260]   I'm sure we'll be talking about it on Friday and this weekend law.
[01:42:21.260 --> 01:42:23.940]   Now, here's the good stuff Google does.
[01:42:23.940 --> 01:42:29.020]   Google AI can predict heart attacks just by looking at your eyes.
[01:42:29.020 --> 01:42:33.980]   Like now, I know all the privacy advocates are going to say, Oh, that's terrifying.
[01:42:33.980 --> 01:42:40.580]   This comes from, this comes from Verily Google's life extension, uh, moonshot.
[01:42:40.580 --> 01:42:44.460]   They can use photographs of the retina to predict factors for cardiovascular disease.
[01:42:44.460 --> 01:42:50.940]   It works about as well, uh, as currently used predictive methods in as far less invasive.
[01:42:50.940 --> 01:42:51.940]   Right.
[01:42:51.940 --> 01:42:58.420]   We, we actually talk about this on tomorrow's IoT podcast, uh, just because of the machine
[01:42:58.420 --> 01:43:01.060]   learning aspect of it, because that's what they've done here.
[01:43:01.060 --> 01:43:07.500]   Um, they have gone back and scanned the retinal images of prior patients from years back and
[01:43:07.500 --> 01:43:12.180]   then knowing five years later that those same patients did have heart disease.
[01:43:12.180 --> 01:43:17.260]   They look for the signs in the retina and then now apply that through machine learning
[01:43:17.260 --> 01:43:21.180]   to current retinal scans and they come up, I think it was 70% accuracy.
[01:43:21.180 --> 01:43:22.180]   70% yeah.
[01:43:22.180 --> 01:43:23.180]   Right.
[01:43:23.180 --> 01:43:30.340]   And the, the, it's 72% if they take blood, which is how the today's typical process works.
[01:43:30.340 --> 01:43:34.540]   So, so, but here's the based on the ice can, the algorithm was able to predict a person's
[01:43:34.540 --> 01:43:41.060]   age to within three and a quarter years, smoking status with 71% accuracy and blood pressure
[01:43:41.060 --> 01:43:44.580]   within 11 units of the upper number reported in the measurement.
[01:43:44.580 --> 01:43:50.020]   Uh, and using that data that could then predict major cardiovascular events.
[01:43:50.020 --> 01:43:51.020]   Huh?
[01:43:51.020 --> 01:43:52.020]   Huh?
[01:43:52.020 --> 01:43:56.300]   With, with, uh, see, as you say, 70% accuracy.
[01:43:56.300 --> 01:43:57.300]   That's very interesting.
[01:43:57.300 --> 01:44:01.700]   See, technology is not so bad.
[01:44:01.700 --> 01:44:02.700]   Yeah.
[01:44:02.700 --> 01:44:08.340]   Uh, until your insurance company starts taking pictures of you on the street looking to see
[01:44:08.340 --> 01:44:13.660]   if you're going to have a heart attack and denies you, sends you a picture with their
[01:44:13.660 --> 01:44:18.860]   red light camera saying, sorry, Jeff, you can't get a, uh, insurance because we could
[01:44:18.860 --> 01:44:22.580]   see from your eyes, you're going to get sick.
[01:44:22.580 --> 01:44:26.980]   That is very panoptically.
[01:44:26.980 --> 01:44:31.980]   Hey, I'm all about the panoptic on the show.
[01:44:31.980 --> 01:44:37.180]   One of the things that, um, is intriguing to me about network medicine and medicine derived
[01:44:37.180 --> 01:44:43.700]   from AI at this stage is, you know, a lot of people gave those samples, um, and never
[01:44:43.700 --> 01:44:51.180]   really were informed of the uses and the possibilities for what, uh, you know, so I'm
[01:44:51.180 --> 01:44:56.620]   wondering, you know, did it any time the people who's had their retina scanned and their medical
[01:44:56.620 --> 01:45:01.980]   records released, you know, what is the process by which Google created and contained that
[01:45:01.980 --> 01:45:03.140]   data set?
[01:45:03.140 --> 01:45:08.540]   Um, because there's a lot of questions in medicine, particularly that raise those very
[01:45:08.540 --> 01:45:13.220]   concerned, which concerns, which is that we have the health insurance privacy and portability
[01:45:13.220 --> 01:45:19.660]   act for a reason, which is that we are, um, somewhat suspect of corporations finding
[01:45:19.660 --> 01:45:24.380]   out our health information and using it, uh, against us.
[01:45:24.380 --> 01:45:32.900]   And so to me, you know, this leads to a different, um, you know, this is a similarly bad path,
[01:45:32.900 --> 01:45:35.940]   but I do wonder, you know, how that data set was generated.
[01:45:35.940 --> 01:45:39.660]   You know, John, you're, there was a case in the UK, not that long ago where there was
[01:45:39.660 --> 01:45:44.260]   data that would have enabled the prediction of kidney disease, uh, that was disallowed
[01:45:44.260 --> 01:45:48.300]   because that was not a use that was anticipated when the data was gathered, which sends me
[01:45:48.300 --> 01:45:53.860]   into a Tizzy because you can't anticipate in rule of AI, you by definition are not anticipating
[01:45:53.860 --> 01:45:57.100]   all future uses of data because you don't know what you're going to see.
[01:45:57.100 --> 01:45:58.100]   Yeah.
[01:45:58.100 --> 01:46:03.100]   And, you know, there's, I mean, there are ways when you go to the hospital, you sign, you
[01:46:03.100 --> 01:46:08.940]   know, that research is, um, you know, part of, you know, if you get a biopsy, often your
[01:46:08.940 --> 01:46:14.460]   blood that's collected is they only need a tiny, tiny bit of it to run those tests.
[01:46:14.460 --> 01:46:21.540]   And then the bio waste as it's called, um, circulates in, in a tissue economy and there's
[01:46:21.540 --> 01:46:27.540]   a heavy price on that bio waste in order to make those scientific findings.
[01:46:27.540 --> 01:46:33.300]   And so I'm interested to think about, you know, what is the, the waste of, of our x-rays
[01:46:33.300 --> 01:46:41.020]   and of our scans and of our MRIs and how is that being, how is that, um, comprising a
[01:46:41.020 --> 01:46:43.740]   market at this point?
[01:46:43.740 --> 01:46:46.740]   So here's an interesting story from the information.
[01:46:46.740 --> 01:46:52.220]   What if you were a little startup, maybe you have a smart lock and you took money from
[01:46:52.220 --> 01:46:58.500]   Amazon for a scalar fund, investment money, uh, to help you work better with Amazon's
[01:46:58.500 --> 01:47:04.300]   echo, then what if Amazon came along and made it a hundred million dollar offer to buy
[01:47:04.300 --> 01:47:06.380]   you an offer you considered too low.
[01:47:06.380 --> 01:47:08.060]   So you turn it down.
[01:47:08.060 --> 01:47:11.620]   Then what if Amazon came along and said, well, we're just going to build our own and
[01:47:11.620 --> 01:47:14.580]   announced Amazon key. That's what happened to August.
[01:47:14.580 --> 01:47:16.380]   And I know, Kevin, you know all about these.
[01:47:16.380 --> 01:47:19.460]   In fact, I think Stacy loves the August locks, right?
[01:47:19.460 --> 01:47:20.460]   I can't remember.
[01:47:20.460 --> 01:47:21.460]   Yeah, she does.
[01:47:21.460 --> 01:47:22.460]   She does.
[01:47:22.460 --> 01:47:23.980]   And, uh, it's a great lock.
[01:47:23.980 --> 01:47:27.860]   Um, this, it's interesting that this is coming up with August only.
[01:47:27.860 --> 01:47:32.060]   It's a timing thing because they, they were just purchased by a, uh, boy, bottom, a,
[01:47:32.060 --> 01:47:33.780]   a lot of people make, yeah, locks.
[01:47:33.780 --> 01:47:34.780]   Yeah.
[01:47:34.780 --> 01:47:35.780]   Right.
[01:47:35.780 --> 01:47:36.780]   150 million.
[01:47:36.780 --> 01:47:44.660]   One of the less recent instances of this was a company called Nucleus and they made, uh,
[01:47:44.660 --> 01:47:47.660]   with the fun backing, they made cancel.
[01:47:47.660 --> 01:47:48.660]   Sorry.
[01:47:48.660 --> 01:47:52.100]   They made, yeah, I'm sorry to, but we can't call it the echo fund because that's not
[01:47:52.100 --> 01:47:53.100]   its name.
[01:47:53.100 --> 01:47:54.100]   Right.
[01:47:54.100 --> 01:47:58.540]   They, they made a, um, a touchscreen device, probably about eight inches square.
[01:47:58.540 --> 01:47:59.980]   It would hang on your wall.
[01:47:59.980 --> 01:48:01.660]   You could do video chats with it.
[01:48:01.660 --> 01:48:05.260]   Uh, it would integrate with the echo voice services to provide data and so on and so
[01:48:05.260 --> 01:48:07.860]   forth and you could play music from it and whatnot.
[01:48:07.860 --> 01:48:11.060]   Well, doesn't it sound a lot like the echo show?
[01:48:11.060 --> 01:48:12.060]   Yeah.
[01:48:12.060 --> 01:48:13.060]   Yeah.
[01:48:13.060 --> 01:48:18.140]   Cause that came out after Nucleus got their funding from Amazon and tried to put the product
[01:48:18.140 --> 01:48:23.660]   out and then the show came and basically the nucleus people were like, what just happened?
[01:48:23.660 --> 01:48:27.820]   Amazon said, of course, told the information the company doesn't use information gained
[01:48:27.820 --> 01:48:31.300]   through investments to help it develop a competing product.
[01:48:31.300 --> 01:48:35.220]   But if we should happen to learn a thing or two, what would we do?
[01:48:35.220 --> 01:48:36.220]   Put it at height.
[01:48:36.220 --> 01:48:42.440]   Um, they, the, uh, information also tells a story, of course, of inform, imagination
[01:48:42.440 --> 01:48:46.700]   technologies, which is a GPU company that may graphics processors were Apple, Apple hired
[01:48:46.700 --> 01:48:50.940]   a few of their employees considered buying the firm and then just said, nah, we're going
[01:48:50.940 --> 01:48:54.940]   to make our own buy and imagination is pretty much gone.
[01:48:54.940 --> 01:48:55.940]   Um, interesting.
[01:48:55.940 --> 01:49:00.100]   Uh, you know, these companies are so big and so powerful.
[01:49:00.100 --> 01:49:05.060]   Uh, at the same time, you don't want to, you want to reward them for their, the reason
[01:49:05.060 --> 01:49:07.900]   they're big and powerful is they're changing the world.
[01:49:07.900 --> 01:49:08.900]   They're making great products.
[01:49:08.900 --> 01:49:09.900]   Yeah.
[01:49:09.900 --> 01:49:14.460]   They didn't get big and powerful by screwing people or did they?
[01:49:14.460 --> 01:49:18.460]   Well, in that case does prove that point.
[01:49:18.460 --> 01:49:23.700]   I mean, what's interesting to me as a sort of a scholar of technology is to think about,
[01:49:23.700 --> 01:49:27.700]   you know, when you talk about a smart home, so as if you're making a product for a smart
[01:49:27.700 --> 01:49:31.140]   home, you start to think about, okay, so what are the things in the home?
[01:49:31.140 --> 01:49:33.580]   People are going to want to control temperature.
[01:49:33.580 --> 01:49:38.580]   Definitely the locks, um, you know, ventilation.
[01:49:38.580 --> 01:49:45.380]   Um, if they're, if they have a swimming pool, the, you know, if the, you know, if the filter's
[01:49:45.380 --> 01:49:46.380]   on and off, right?
[01:49:46.380 --> 01:49:49.860]   So they're, you know, the electricity, do you want to be able to shut the lights on
[01:49:49.860 --> 01:49:50.860]   and off?
[01:49:50.860 --> 01:49:55.900]   And so it's not outside the bounds to reason that if you are someone that's building a
[01:49:55.900 --> 01:50:02.100]   smart home technology that locks would be, um, on your list of things to do.
[01:50:02.100 --> 01:50:03.100]   Yeah.
[01:50:03.100 --> 01:50:09.900]   And at the same time, you know, if you make a play and you get caught, you do have to
[01:50:09.900 --> 01:50:14.540]   own up to that and say, well, we knew this technology was already built and we didn't
[01:50:14.540 --> 01:50:17.500]   want to have to build it from the ground up.
[01:50:17.500 --> 01:50:22.260]   Um, or they wanted to, you know, to me, this is anti-competitive in the sense that they
[01:50:22.260 --> 01:50:31.300]   know that this is a rising star on this market and they want to acquire and then probably,
[01:50:31.300 --> 01:50:33.060]   um, kill, right?
[01:50:33.060 --> 01:50:36.500]   But at the same time, I mean, I think about what Facebook did with Snapchat, they, they,
[01:50:36.500 --> 01:50:42.140]   they, you know, Facebook's pattern is to buy competitors like Instagram, like WhatsApp
[01:50:42.140 --> 01:50:44.780]   and fold them into the Facebook family.
[01:50:44.780 --> 01:50:48.060]   That way if you're going to leave Facebook to use WhatsApp, well, at least to be using
[01:50:48.060 --> 01:50:53.300]   a Facebook product, uh, when they couldn't buy Snapchat, they just, uh, they enhanced
[01:50:53.300 --> 01:50:57.460]   Instagram to beat Snapchat and it worked, by the way, it seems to be working.
[01:50:57.460 --> 01:51:04.020]   But that I don't, I don't, doesn't feel like that's illegal, that's just, that's competitive.
[01:51:04.020 --> 01:51:05.020]   So this is the problem.
[01:51:05.020 --> 01:51:09.860]   It's hard and boy, Microsoft's famous for, you know, engulfing devour or what I can't
[01:51:09.860 --> 01:51:13.100]   remember what the term was we used to use about Microsoft, but that was the idea they
[01:51:13.100 --> 01:51:14.620]   would do exactly the same thing.
[01:51:14.620 --> 01:51:18.340]   They'd pretend to want to buy a company, they'd find out all about it and they'd not
[01:51:18.340 --> 01:51:20.660]   buy them and do something competitive.
[01:51:20.660 --> 01:51:25.900]   Um, but Microsoft was famous for being a, a hard charging competitor.
[01:51:25.900 --> 01:51:28.020]   Bill Gates always wanted to win.
[01:51:28.020 --> 01:51:34.220]   He played really hard and you want companies to play really hard, but so it's a, it's tricky.
[01:51:34.220 --> 01:51:36.260]   Yeah, there's that.
[01:51:36.260 --> 01:51:41.180]   And then, you know, ultimately they can't be everything to everyone, right?
[01:51:41.180 --> 01:51:47.820]   And so part of it is, is understanding as they expand the possibilities of these technologies
[01:51:47.820 --> 01:51:55.820]   and get into different markets and aggregate, you know, all of these users into a very large
[01:51:55.820 --> 01:52:04.740]   platform, that also makes it really difficult for anyone new to step into the market in,
[01:52:04.740 --> 01:52:13.740]   in, in small sections or subsections of those technology spaces because starting up a technology
[01:52:13.740 --> 01:52:17.500]   company is extremely expensive.
[01:52:17.500 --> 01:52:23.900]   And so one of the things that not only do you shift the kinds of innovation that are
[01:52:23.900 --> 01:52:31.180]   possible, but you also really limit the product space in a way that, um, you know, it'd be
[01:52:31.180 --> 01:52:39.740]   nice if, if we had, you know, many different phone cameras to pick from, um, and that we,
[01:52:39.740 --> 01:52:48.580]   you know, instead of having just a few that work well, here's a New York supermarket tops,
[01:52:48.580 --> 01:52:52.500]   files, bankruptcy sites, debt and Amazon.
[01:52:52.500 --> 01:52:57.940]   That might just be a convenient, a convenient fall guy.
[01:52:57.940 --> 01:52:58.940]   Yeah, it's debt.
[01:52:58.940 --> 01:52:59.940]   We had a lot of debt.
[01:52:59.940 --> 01:53:02.380]   Oh, and Amazon.
[01:53:02.380 --> 01:53:03.380]   It's got to be.
[01:53:03.380 --> 01:53:06.180]   I mean, if you're, if you're a brick and mortar store, if you're selling anything on
[01:53:06.180 --> 01:53:10.460]   the internet, if you're doing any commerce of any kind, even if you're just doing health,
[01:53:10.460 --> 01:53:15.060]   health insurance, you got to be worried about Amazon in every respect.
[01:53:15.060 --> 01:53:19.140]   But that's ultimately good for consumers as long as they play fair, I guess.
[01:53:19.140 --> 01:53:20.900]   Is that it play fair?
[01:53:20.900 --> 01:53:23.300]   We just don't know if they are.
[01:53:23.300 --> 01:53:24.660]   That's I think it's hard to know.
[01:53:24.660 --> 01:53:25.660]   Yeah.
[01:53:25.660 --> 01:53:27.660]   That's why we have courts.
[01:53:27.660 --> 01:53:32.700]   Um, let's see, Samsung S9, we're going to have live coverage on Sunday.
[01:53:32.700 --> 01:53:40.700]   The Robert Baliser and Ron Richards begins 9 AM Pacific noon Eastern time of the Samsung
[01:53:40.700 --> 01:53:43.740]   event in Barcelona from Mobile World Congress.
[01:53:43.740 --> 01:53:53.900]   They are expected to announce a new S9 for a lot more money with dual lenses.
[01:53:53.900 --> 01:53:54.900]   The FCC is inspecting.
[01:53:54.900 --> 01:53:56.660]   I just, I already talked about that.
[01:53:56.660 --> 01:53:59.420]   I just can't resist bringing it up again.
[01:53:59.420 --> 01:54:00.420]   Yeah.
[01:54:00.420 --> 01:54:02.220]   Let me say that again.
[01:54:02.220 --> 01:54:10.500]   The FCC is investigating its own chairman because, well, it's a little suspicious that
[01:54:10.500 --> 01:54:19.260]   the FCC at his behest dropped their media ownership rules right before Sinclair announced
[01:54:19.260 --> 01:54:24.380]   his intention to buy Tribune media for $3.9 billion.
[01:54:24.380 --> 01:54:25.980]   What do they know and when do they know it?
[01:54:25.980 --> 01:54:26.980]   Right?
[01:54:26.980 --> 01:54:27.980]   That's always the question.
[01:54:27.980 --> 01:54:28.980]   Yeah.
[01:54:28.980 --> 01:54:34.860]   All right, I could tell you're losing energy.
[01:54:34.860 --> 01:54:39.980]   So we're going to take a break and we will wrap this up with your, any picks or plugs
[01:54:39.980 --> 01:54:41.740]   or tips or tricks.
[01:54:41.740 --> 01:54:46.340]   Any, any, we just mentioned that day more lost is case against Google.
[01:54:46.340 --> 01:54:47.340]   Yeah.
[01:54:47.340 --> 01:54:48.340]   The record.
[01:54:48.340 --> 01:54:49.340]   Yes.
[01:54:49.340 --> 01:54:50.340]   Yes.
[01:54:50.340 --> 01:54:51.340]   Yeah.
[01:54:51.340 --> 01:54:52.340]   For the records, let the record show.
[01:54:52.340 --> 01:54:57.900]   Just let the record show that the National Labor Relations Board says, no, you can't create
[01:54:57.900 --> 01:54:59.660]   a hostile environment.
[01:54:59.660 --> 01:55:01.620]   Sorry.
[01:55:01.620 --> 01:55:05.580]   That's a good reason for getting fired.
[01:55:05.580 --> 01:55:06.580]   Jimmy.
[01:55:06.580 --> 01:55:07.580]   Yeah.
[01:55:07.580 --> 01:55:13.100]   And there was a very interesting Q&A that happened with him.
[01:55:13.100 --> 01:55:19.860]   He was on a panel at Portland State University either late last week or over the weekend.
[01:55:19.860 --> 01:55:27.260]   And I watched the Q&A on YouTube and students had some pretty strong criticisms of the biological
[01:55:27.260 --> 01:55:35.860]   theories that he was using to support his position that women aren't excellent coders
[01:55:35.860 --> 01:55:37.580]   and engineers.
[01:55:37.580 --> 01:55:42.220]   And so I was really proud of the students that stood up to him as well as stood up to
[01:55:42.220 --> 01:55:48.460]   the moderator and his co-panelist and said that, you know, we have a history of scientific
[01:55:48.460 --> 01:55:52.340]   racism and we thought that was good data at the time.
[01:55:52.340 --> 01:55:58.580]   And now we have you picking and choosing certain biological data to support your case and how
[01:55:58.580 --> 01:56:03.980]   should we know that this is verified and trusted data?
[01:56:03.980 --> 01:56:08.740]   And so I think the students are taking a really critical approach to thinking about what
[01:56:08.740 --> 01:56:13.700]   DeMoor was trying to push in terms of a public conversation.
[01:56:13.700 --> 01:56:15.900]   And I don't think they're taking the bait.
[01:56:15.900 --> 01:56:17.900]   Yep.
[01:56:17.900 --> 01:56:22.580]   Okay, anything else?
[01:56:22.580 --> 01:56:23.580]   Any last?
[01:56:23.580 --> 01:56:24.580]   I just want to get that one.
[01:56:24.580 --> 01:56:25.580]   I agree.
[01:56:25.580 --> 01:56:27.140]   Jeff, prepare your number.
[01:56:27.140 --> 01:56:31.100]   Kevin, if you've got an IoT device, we should all be buying.
[01:56:31.100 --> 01:56:33.060]   I've got my checkbook on the ready.
[01:56:33.060 --> 01:56:35.260]   Joan, if you've got a new troll.
[01:56:35.260 --> 01:56:38.220]   Joan, if you have any fake news, you want to spread.
[01:56:38.220 --> 01:56:45.180]   Yes, I'm getting back channeled by a reporter at Politico who's chasing down a troll right
[01:56:45.180 --> 01:56:48.620]   now and I'm trying to help him amidst of all this.
[01:56:48.620 --> 01:56:49.620]   I love it.
[01:56:49.620 --> 01:56:51.100]   Joan Donovan, troll hunter.
[01:56:51.100 --> 01:56:52.100]   I can help.
[01:56:52.100 --> 01:56:54.260]   Well, this is a YouTube registry.
[01:56:54.260 --> 01:56:55.260]   That's a strange troll.
[01:56:55.260 --> 01:57:01.180]   Once you know where they hang out, they're creatures of habit as we all are.
[01:57:01.180 --> 01:57:02.740]   I need to talk to you.
[01:57:02.740 --> 01:57:04.740]   Yeah, you come hang out in the office.
[01:57:04.740 --> 01:57:05.740]   I'd love to have you.
[01:57:05.740 --> 01:57:06.740]   I need some help.
[01:57:06.740 --> 01:57:07.740]   This is good.
[01:57:07.740 --> 01:57:08.740]   I like this.
[01:57:08.740 --> 01:57:12.820]   Our show today brought to you by Rocket Mortgage.
[01:57:12.820 --> 01:57:17.380]   If you are buying a new home, you probably are not bringing out the checkbook to pay
[01:57:17.380 --> 01:57:18.580]   for it in cash.
[01:57:18.580 --> 01:57:19.740]   You got to get a loan, right?
[01:57:19.740 --> 01:57:20.740]   A mortgage.
[01:57:20.740 --> 01:57:25.980]   Did you know the word mortgage comes from the French word for death?
[01:57:25.980 --> 01:57:31.380]   I think the intention was more like this is something you owe until you die.
[01:57:31.380 --> 01:57:36.380]   But really, if anybody's ever gone to a bank to apply for a loan, you understand where
[01:57:36.380 --> 01:57:38.060]   that derivation comes from.
[01:57:38.060 --> 01:57:39.780]   It's not fun.
[01:57:39.780 --> 01:57:41.180]   That's why Rocket Mortgage came along.
[01:57:41.180 --> 01:57:44.660]   From quick and loans, the best lender in the country, number one in customer satisfaction
[01:57:44.660 --> 01:57:46.420]   year after year.
[01:57:46.420 --> 01:57:49.820]   And I tell you, they're guaranteed to win that award again this year because of Rocket
[01:57:49.820 --> 01:57:50.820]   Mortgage.
[01:57:50.820 --> 01:57:56.420]   They put the entire mortgage process in your hands, in your phone, literally in your hands.
[01:57:56.420 --> 01:57:57.980]   You don't need to go to a bank.
[01:57:57.980 --> 01:58:00.140]   You don't need to go to the attic to find paperwork.
[01:58:00.140 --> 01:58:04.540]   All you need is to go to rocketmortgage.com/twig.
[01:58:04.540 --> 01:58:06.740]   Answer a few simple questions.
[01:58:06.740 --> 01:58:12.940]   Allow them to contact your financial institutions.
[01:58:12.940 --> 01:58:18.380]   They crunch the numbers and in minutes make you an offer.
[01:58:18.380 --> 01:58:20.740]   You could choose the rate, the term, the down payment.
[01:58:20.740 --> 01:58:21.740]   It's all up to you.
[01:58:21.740 --> 01:58:22.740]   It's completely transparent.
[01:58:22.740 --> 01:58:26.260]   Once you find a loan option you like, they say, "Good.
[01:58:26.260 --> 01:58:27.700]   You're approved."
[01:58:27.700 --> 01:58:28.700]   It's fast enough.
[01:58:28.700 --> 01:58:30.420]   You could do it at an open house.
[01:58:30.420 --> 01:58:32.300]   You literally get approved in minutes.
[01:58:32.300 --> 01:58:35.860]   This is such a revolution compared to the old way of doing it.
[01:58:35.860 --> 01:58:37.220]   Rocket Mortgage from Quick and Loans.
[01:58:37.220 --> 01:58:38.220]   Apply, simply.
[01:58:38.220 --> 01:58:41.140]   Understand fully and mortgage confidently to get started.
[01:58:41.140 --> 01:58:43.660]   Go to rocketmortgage.com/twig.
[01:58:43.660 --> 01:58:46.460]   Rocketmortgage.com/twig.
[01:58:46.460 --> 01:58:51.020]   They're an equal housing lender, of course, licensed in all 50 states and MLS consumer
[01:58:51.020 --> 01:58:54.540]   access.org number 3030.
[01:58:54.540 --> 01:58:56.180]   Rocketmortgage.com/twig.
[01:58:56.180 --> 01:59:00.260]   By the way, interest rates are, we know, going to be going up.
[01:59:00.260 --> 01:59:04.500]   You would behoove you to lock in a low interest rate with a refi.
[01:59:04.500 --> 01:59:06.260]   They do that too.
[01:59:06.260 --> 01:59:09.780]   Rocketmortgage.com/twig.
[01:59:09.780 --> 01:59:18.660]   Kevin, let's get a tip from Mr. IOT podcast at IOTPodcast.com.
[01:59:18.660 --> 01:59:22.260]   Can it be more Chromebook-y?
[01:59:22.260 --> 01:59:23.260]   Yeah.
[01:59:23.260 --> 01:59:24.260]   Heck yeah.
[01:59:24.260 --> 01:59:25.260]   Is it a Google show?
[01:59:25.260 --> 01:59:26.260]   Heck yeah.
[01:59:26.260 --> 01:59:27.260]   Jeff lives on his Chromebook.
[01:59:27.260 --> 01:59:28.820]   He loves his Chromebook.
[01:59:28.820 --> 01:59:32.780]   Are you, Kevin, let me ask you, do you use a Chromebook like Jeff does exclusively?
[01:59:32.780 --> 01:59:34.380]   Is that like your go-to?
[01:59:34.380 --> 01:59:38.980]   Well, it wasn't up until about two weeks ago.
[01:59:38.980 --> 01:59:41.100]   When I was working at Google, I was using a Chromebook.
[01:59:41.100 --> 01:59:46.100]   Either my own Chromebook Pixel or a work-issued Chromebook.
[01:59:46.100 --> 01:59:48.140]   Then I got away from it for a while.
[01:59:48.140 --> 01:59:51.060]   With my free time lately, I've been doing online coding.
[01:59:51.060 --> 01:59:57.900]   I'm like, how am I going to learn my programming classes and such with the Chromebook?
[01:59:57.900 --> 02:00:01.900]   There's tons of cloud-based programming things out there, but they still just weren't doing
[02:00:01.900 --> 02:00:02.900]   it for me.
[02:00:02.900 --> 02:00:05.300]   Jeff, I don't know if you're learning how to code or want to.
[02:00:05.300 --> 02:00:08.340]   It's really cool, I think, but that's neither here or there.
[02:00:08.340 --> 02:00:15.020]   You can do it on a Chromebook pretty easily if you use the Secure Shell Web App, which
[02:00:15.020 --> 02:00:19.060]   just find it right in the Chrome Web Store.
[02:00:19.060 --> 02:00:24.540]   What you do is, one of the things you can do, just buy a cheap Raspberry Pi for 30, 40
[02:00:24.540 --> 02:00:25.540]   bucks.
[02:00:25.540 --> 02:00:26.540]   Brilliant.
[02:00:26.540 --> 02:00:27.700]   What I did was I put a Raspberry Pi.
[02:00:27.700 --> 02:00:33.540]   We're running our IoT voicemail that I built in Python on that.
[02:00:33.540 --> 02:00:34.700]   It's got Python installed.
[02:00:34.700 --> 02:00:38.620]   I just use SSH, the Secure Shell, to log in.
[02:00:38.620 --> 02:00:42.860]   It's a very native experience in the Chromebook, but here's the best part.
[02:00:42.860 --> 02:00:46.500]   I actually write my code using a text editor on the Chromebook.
[02:00:46.500 --> 02:00:51.060]   They're native Chromaps or that text or carrot or two or the decent ones.
[02:00:51.060 --> 02:00:54.500]   I just save the files over to the Pi on my network.
[02:00:54.500 --> 02:01:00.180]   The funny thing is, the Secure Shell client allows for SFTP.
[02:01:00.180 --> 02:01:04.260]   I can mount the Pi in the files app of the Chromebook.
[02:01:04.260 --> 02:01:05.260]   It's just right there.
[02:01:05.260 --> 02:01:07.060]   It's a beautiful thing.
[02:01:07.060 --> 02:01:10.860]   I'm going to top you.
[02:01:10.860 --> 02:01:16.500]   I'm going to give you one that I used this SFTP for a long time.
[02:01:16.500 --> 02:01:20.140]   Since you do control your Raspberry Pi, if you can control the server that you're logging
[02:01:20.140 --> 02:01:28.300]   into, put MOSH on it and then get the MOS Chrome extension.
[02:01:28.300 --> 02:01:30.060]   It will look exactly the same to you.
[02:01:30.060 --> 02:01:31.060]   It's still SSH.
[02:01:31.060 --> 02:01:33.420]   It's secure.
[02:01:33.420 --> 02:01:36.140]   They have SFTP as well.
[02:01:36.140 --> 02:01:44.140]   What MOSH does is it preserves the connection.
[02:01:44.140 --> 02:01:46.860]   The one problem, sometimes the connection drops or whatever.
[02:01:46.860 --> 02:01:52.380]   Your session is preserved across disconnections, across disruptions.
[02:01:52.380 --> 02:01:53.380]   It's really handy.
[02:01:53.380 --> 02:01:54.580]   I really like MOSH.
[02:01:54.580 --> 02:01:56.300]   It's an open source project.
[02:01:56.300 --> 02:01:59.020]   Actually, now I'm looking.
[02:01:59.020 --> 02:02:02.220]   I know there's a MOSH extension because I use it on my Chromebook, but I don't see it
[02:02:02.220 --> 02:02:03.220]   here.
[02:02:03.220 --> 02:02:06.740]   You have to put MOSH on the Raspberry Pi because basically you're connecting with a
[02:02:06.740 --> 02:02:07.740]   MOSH server.
[02:02:07.740 --> 02:02:08.740]   It's easy to do.
[02:02:08.740 --> 02:02:09.740]   It's easy to do.
[02:02:09.740 --> 02:02:12.500]   You can do an apt get and get it.
[02:02:12.500 --> 02:02:19.540]   Take a look at that as another option for a mobile shell because it really helps.
[02:02:19.540 --> 02:02:20.940]   It's a little faster.
[02:02:20.940 --> 02:02:25.260]   It just helps with the trickiness of doing what you're doing, which is pretending that
[02:02:25.260 --> 02:02:28.260]   you're on the Raspberry Pi, in effect.
[02:02:28.260 --> 02:02:33.580]   I'm actually blown away by how much the Android apps and the Chrome OS experience has improved
[02:02:33.580 --> 02:02:34.940]   in the past year or so.
[02:02:34.940 --> 02:02:37.540]   It still has little ways to go.
[02:02:37.540 --> 02:02:39.140]   Just a little.
[02:02:39.140 --> 02:02:40.820]   We all love our Pixelbook, though.
[02:02:40.820 --> 02:02:41.820]   I do.
[02:02:41.820 --> 02:02:42.820]   It's fair to say.
[02:02:42.820 --> 02:02:43.820]   Yeah.
[02:02:43.820 --> 02:02:47.020]   I mean, when you spend that much money on a Chromebook, you better darn well.
[02:02:47.020 --> 02:02:48.020]   You better.
[02:02:48.020 --> 02:02:49.020]   You do it.
[02:02:49.020 --> 02:02:50.020]   Are you learning Python?
[02:02:50.020 --> 02:02:51.620]   What are you playing with?
[02:02:51.620 --> 02:02:53.380]   Some Java, some Python.
[02:02:53.380 --> 02:02:55.580]   Python just seems to be easiest for me to pick up these days.
[02:02:55.580 --> 02:02:58.740]   It's a great language to start with.
[02:02:58.740 --> 02:03:00.900]   Very nice.
[02:03:00.900 --> 02:03:04.220]   Mr. Jeff Jarvis, your pick of the week.
[02:03:04.220 --> 02:03:06.740]   Oh, this would be a little bitch.
[02:03:06.740 --> 02:03:12.980]   Also, analyst says on CNBC, "The Twitter and Snap will grow with the expense of Facebook."
[02:03:12.980 --> 02:03:15.700]   What just drew me to be about that is that how long ago was it about three weeks ago
[02:03:15.700 --> 02:03:18.300]   when, oh, Snap Twitter, they're nearly never.
[02:03:18.300 --> 02:03:19.300]   It's over.
[02:03:19.300 --> 02:03:20.300]   Over.
[02:03:20.300 --> 02:03:21.300]   Over.
[02:03:21.300 --> 02:03:23.300]   And now, suddenly, the fortunes change.
[02:03:23.300 --> 02:03:26.100]   Listen, the analysts, I think, is the rule.
[02:03:26.100 --> 02:03:27.100]   Yeah.
[02:03:27.100 --> 02:03:28.900]   Just a little.
[02:03:28.900 --> 02:03:32.820]   Actually, Snap is not, I like it that Snap is not giving up.
[02:03:32.820 --> 02:03:34.340]   No, I do too.
[02:03:34.340 --> 02:03:36.540]   Gumption would be the word for them now.
[02:03:36.540 --> 02:03:43.580]   They're getting a lot of heat from more than a million users who say, "I hate the new interface."
[02:03:43.580 --> 02:03:49.060]   But I should check the change.org petition, but the last time I checked it was.
[02:03:49.060 --> 02:03:53.020]   I think we had something in the rundown where you can revert or something.
[02:03:53.020 --> 02:03:55.940]   Yeah, I don't know how well that works.
[02:03:55.940 --> 02:03:59.500]   Here's how to reverse an update on Android.
[02:03:59.500 --> 02:04:00.500]   In theory.
[02:04:00.500 --> 02:04:03.260]   But, yeah, you'd see it.
[02:04:03.260 --> 02:04:05.420]   That's not going to be on now.
[02:04:05.420 --> 02:04:07.740]   Oh, here's Snap's response.
[02:04:07.740 --> 02:04:11.580]   1.2 million signatures to Nick and all of the Snap Chadders who signed the petition.
[02:04:11.580 --> 02:04:13.220]   We hear you.
[02:04:13.220 --> 02:04:15.620]   We appreciate you took the title list and how you feel.
[02:04:15.620 --> 02:04:19.740]   This is how a company should respond to these petitions.
[02:04:19.740 --> 02:04:22.300]   They're explaining why they're doing it.
[02:04:22.300 --> 02:04:25.100]   We are going to do some more changes.
[02:04:25.100 --> 02:04:26.340]   The new foundation is just the beginning.
[02:04:26.340 --> 02:04:29.820]   In other words, we're not going to do what you want us to do, which is a reverse.
[02:04:29.820 --> 02:04:30.820]   Which is very Facebook.
[02:04:30.820 --> 02:04:31.820]   At least they're responding.
[02:04:31.820 --> 02:04:32.820]   New speed.
[02:04:32.820 --> 02:04:33.820]   Everybody hated the newsfeed.
[02:04:33.820 --> 02:04:36.860]   And Zuckerberg said, "Nope, nope."
[02:04:36.860 --> 02:04:41.380]   I think companies have to have a strong vision and follow it.
[02:04:41.380 --> 02:04:46.380]   So I'm going to make your number 1.226252 million.
[02:04:46.380 --> 02:04:50.340]   That's how many people signed the petition.
[02:04:50.340 --> 02:04:53.900]   Joan, is there anything you'd like to plug or pick?
[02:04:53.900 --> 02:05:00.260]   Yeah, so there was a book that came out today that is really interesting for people
[02:05:00.260 --> 02:05:02.020]   who are interested in technology.
[02:05:02.020 --> 02:05:06.420]   It's called Algorithms of Oppression, How Search Engines Reinforced Racism by Sethi
[02:05:06.420 --> 02:05:08.420]   Enobel.
[02:05:08.420 --> 02:05:12.140]   She's an information study scholar at UCLA.
[02:05:12.140 --> 02:05:16.900]   He's been working on this book for several years.
[02:05:16.900 --> 02:05:22.220]   What's interesting is the way that she challenges the notion of the algorithm here.
[02:05:22.220 --> 02:05:31.100]   Then looks at what are the inputs and outputs and how do search engines discover and create
[02:05:31.100 --> 02:05:38.740]   and reinforce the biases that we are all very familiar with related to racism and sexism
[02:05:38.740 --> 02:05:40.820]   and homophobia.
[02:05:40.820 --> 02:05:46.420]   She's got some really interesting ways of doing search as well as of cataloging these
[02:05:46.420 --> 02:05:48.700]   instances and case studies.
[02:05:48.700 --> 02:05:57.780]   I should say that the kind of research that she does is impactful in the sense that these
[02:05:57.780 --> 02:06:06.020]   Google and corporations, they don't have teams devoted internally that are doing this
[02:06:06.020 --> 02:06:10.980]   kind of work in the way that she does.
[02:06:10.980 --> 02:06:18.700]   Her research to me is really provocative in the sense that it gets us an outsider's perspective
[02:06:18.700 --> 02:06:27.700]   on how to reimagine what it means to serve, curate, and distribute information.
[02:06:27.700 --> 02:06:33.820]   I'm really, really happy that the book is out and available through Amazon.
[02:06:33.820 --> 02:06:38.260]   Last I heard this morning, it was actually sold out right now.
[02:06:38.260 --> 02:06:43.660]   I don't know if it's still true, but it's ranking, I think, number one, according to
[02:06:43.660 --> 02:06:46.900]   your screenshot there.
[02:06:46.900 --> 02:06:52.740]   It's a very popular book and there's already a lot of conversation on social media about
[02:06:52.740 --> 02:07:03.540]   it and about what it would take to think about an information first perspective and
[02:07:03.540 --> 02:07:10.140]   also to think about search engines as having dimensions where people are affected or where
[02:07:10.140 --> 02:07:20.580]   groups are affected and where in a lot of ways our most harmful prejudices are amplified.
[02:07:20.580 --> 02:07:25.740]   Yeah, this is another approach to the same thing we've been talking about the whole show,
[02:07:25.740 --> 02:07:32.980]   which is how these companies affect us and sometimes unconsciously affect us.
[02:07:32.980 --> 02:07:37.380]   I'm glad that people are doing the studies and bringing it to the surface because it's
[02:07:37.380 --> 02:07:42.940]   so easy to have unconscious inputs and affect you and change your point of view in a way
[02:07:42.940 --> 02:07:43.940]   that's not right.
[02:07:43.940 --> 02:07:45.500]   I think this is really significant.
[02:07:45.500 --> 02:07:52.220]   There are a series of questions in the book, even on the cover, that if you were to search
[02:07:52.220 --> 02:07:58.900]   in auto-complete to this day, certain key words and questions, I know that Google is
[02:07:58.900 --> 02:08:04.660]   working on the problem but definitely hasn't solved for it.
[02:08:04.660 --> 02:08:10.220]   What's dangerous about the auto-complete in that sense is it is bringing you directly
[02:08:10.220 --> 02:08:16.380]   to biased information by suggesting it and I think that part of that has to do with the
[02:08:16.380 --> 02:08:23.300]   way people are asking questions of the platform itself when they are searching.
[02:08:23.300 --> 02:08:30.860]   It's a really interesting way of understanding how society gets encoded into the algorithms.
[02:08:30.860 --> 02:08:31.860]   Si.
[02:08:31.860 --> 02:08:35.740]   Oh, come on, Leo.
[02:08:35.740 --> 02:08:37.420]   I have fun.
[02:08:37.420 --> 02:08:38.420]   Are you an optimist?
[02:08:38.420 --> 02:08:39.420]   I hope you are.
[02:08:39.420 --> 02:08:40.420]   I am.
[02:08:40.420 --> 02:08:41.420]   I am.
[02:08:41.420 --> 02:08:44.340]   I also picked out this book which I should say is a very good read.
[02:08:44.340 --> 02:08:45.340]   It's called The Witches.
[02:08:45.340 --> 02:08:48.900]   We're just talking about burning people at the stake.
[02:08:48.900 --> 02:08:54.500]   Well, this book has made me think a lot about what it means to do fake news because it's
[02:08:54.500 --> 02:08:59.020]   a really neat historical take on the Salem Witch Trial.
[02:08:59.020 --> 02:09:03.780]   I grew up near Boston and had many, many experiences in Salem.
[02:09:03.780 --> 02:09:11.020]   It was an excellent read from a historian which is called The Witches, Salem, 1692, a
[02:09:11.020 --> 02:09:12.020]   history.
[02:09:12.020 --> 02:09:13.020]   I want to read that.
[02:09:13.020 --> 02:09:14.020]   That sounds great.
[02:09:14.020 --> 02:09:20.980]   Yeah, it's really dense and goofy and everything I love about a good history book.
[02:09:20.980 --> 02:09:26.740]   I was just reading somewhere that we kind of misunderstood what happened in Salem.
[02:09:26.740 --> 02:09:31.340]   We have a lot of urban myths or legends about it that are accurate.
[02:09:31.340 --> 02:09:34.220]   Well, I will read it.
[02:09:34.220 --> 02:09:35.220]   Thank you for being here.
[02:09:35.220 --> 02:09:38.620]   Joan Donovan works at Data and Society.
[02:09:38.620 --> 02:09:41.020]   You could find her at datasociety.net.
[02:09:41.020 --> 02:09:45.620]   On the Twitter @BostonJoan, she's Boston Strong.
[02:09:45.620 --> 02:09:48.420]   I hope you got to talk with your aunt a little bit because we thank you.
[02:09:48.420 --> 02:09:49.420]   I did.
[02:09:49.420 --> 02:09:50.420]   Thank you.
[02:09:50.420 --> 02:09:52.420]   Spending a little time with us instead of your family.
[02:09:52.420 --> 02:09:53.420]   Yeah, thanks for your time.
[02:09:53.420 --> 02:09:54.860]   Joan, we appreciate you dumping your aunt for us.
[02:09:54.860 --> 02:09:56.860]   Okay, that's what I wanted to say.
[02:09:56.860 --> 02:09:59.660]   Yeah, right place, right time.
[02:09:59.660 --> 02:10:01.660]   I appreciate you all having me on.
[02:10:01.660 --> 02:10:02.660]   It's truly like...
[02:10:02.660 --> 02:10:03.660]   Always a pleasure.
[02:10:03.660 --> 02:10:04.660]   You're a hit.
[02:10:04.660 --> 02:10:07.300]   I'd be able to hang out and talk with these people.
[02:10:07.300 --> 02:10:08.540]   We love your brains.
[02:10:08.540 --> 02:10:09.700]   It's nice to get your brains.
[02:10:09.700 --> 02:10:10.700]   Boy, there's no...
[02:10:10.700 --> 02:10:15.540]   Your beat is the most timely beat there can be right now.
[02:10:15.540 --> 02:10:16.540]   I know.
[02:10:16.540 --> 02:10:17.540]   I know.
[02:10:17.540 --> 02:10:18.540]   The work doesn't stop.
[02:10:18.540 --> 02:10:22.300]   But I feel like a normal job.
[02:10:22.300 --> 02:10:26.420]   When I'm ready for retirement, I can start...
[02:10:26.420 --> 02:10:30.580]   I can go back to the university and teach my classes.
[02:10:30.580 --> 02:10:32.300]   We need you on the front lines.
[02:10:32.300 --> 02:10:33.300]   I know.
[02:10:33.300 --> 02:10:34.300]   I know.
[02:10:34.300 --> 02:10:35.300]   Thank you.
[02:10:35.300 --> 02:10:36.300]   It's a strange job.
[02:10:36.300 --> 02:10:37.300]   We always have us between...
[02:10:37.300 --> 02:10:40.060]   Your all stands between us and Putin.
[02:10:40.060 --> 02:10:41.060]   No pressure.
[02:10:41.060 --> 02:10:42.060]   No.
[02:10:42.060 --> 02:10:44.460]   We still haven't evaluated that.
[02:10:44.460 --> 02:10:45.460]   In fact, claim.
[02:10:45.460 --> 02:10:49.220]   Well, we do some research, okay, and get back to us.
[02:10:49.220 --> 02:10:50.220]   I will.
[02:10:50.220 --> 02:10:52.620]   You are a digital arsenal.
[02:10:52.620 --> 02:10:56.220]   That's Jeff Jarvis, professor of journalism, city university of New York.
[02:10:56.220 --> 02:10:59.340]   All that stands between us and the Harvard comma.
[02:10:59.340 --> 02:11:00.340]   We are...
[02:11:00.340 --> 02:11:01.340]   Buzzmocksford.
[02:11:01.340 --> 02:11:02.340]   - Buzzmocksford.
[02:11:02.340 --> 02:11:03.340]   - Oxford comma.
[02:11:03.340 --> 02:11:05.340]   Actually, I'm sure you're a fan of the Oxford comma.
[02:11:05.340 --> 02:11:06.340]   - Yes, I am.
[02:11:06.340 --> 02:11:07.340]   - Okay.
[02:11:07.340 --> 02:11:08.340]   So it's the other way around.
[02:11:08.340 --> 02:11:09.340]   - Not just a fan.
[02:11:09.340 --> 02:11:10.540]   It's a moral responsibility.
[02:11:10.540 --> 02:11:11.540]   - Yes.
[02:11:11.540 --> 02:11:13.740]   He's all that stands between us and hanging participles.
[02:11:13.740 --> 02:11:14.740]   How about that?
[02:11:14.740 --> 02:11:15.740]   - That's about it.
[02:11:15.740 --> 02:11:16.740]   - Buzzmachine.com.
[02:11:16.740 --> 02:11:20.380]   Whatever happened to copy editors, that's what I want to know.
[02:11:20.380 --> 02:11:21.380]   Dang it.
[02:11:21.380 --> 02:11:23.020]   @Jeff Jarvis on the Twitter.
[02:11:23.020 --> 02:11:24.020]   Gone.
[02:11:24.020 --> 02:11:25.020]   Gone.
[02:11:25.020 --> 02:11:26.020]   Gone.
[02:11:26.020 --> 02:11:27.020]   Every week.
[02:11:27.020 --> 02:11:28.020]   You'll be back next week, I hope.
[02:11:28.020 --> 02:11:29.020]   - I plan to be.
[02:11:29.020 --> 02:11:30.020]   I feel happy.
[02:11:30.020 --> 02:11:31.980]   - I plan on it.
[02:11:31.980 --> 02:11:33.220]   Maybe we'll get Stacey back too.
[02:11:33.220 --> 02:11:36.020]   She's in Berlin right now with Bosch.
[02:11:36.020 --> 02:11:42.460]   But she sent an able replacement in Kevin Tofel, co-host on IOT Podcast, IOTpodcast.com
[02:11:42.460 --> 02:11:45.460]   @KevinC Tofel on the Twitter.
[02:11:45.460 --> 02:11:47.860]   Great to see you, Kevin.
[02:11:47.860 --> 02:11:48.860]   - Happy to be here.
[02:11:48.860 --> 02:11:49.860]   - Thanks again.
[02:11:49.860 --> 02:11:52.540]   - Any new Beatles and news, anything worth like those...
[02:11:52.540 --> 02:11:55.980]   All those Beatles documentaries on Netflix, anything good I should look for.
[02:11:55.980 --> 02:11:56.980]   - Nothing recent.
[02:11:56.980 --> 02:11:57.980]   Nothing recent.
[02:11:57.980 --> 02:12:01.780]   I know Ringo had some reissues of his first albums and whatnot.
[02:12:01.780 --> 02:12:03.340]   - I can live without those, I think.
[02:12:03.340 --> 02:12:05.340]   I'll be okay without those.
[02:12:05.340 --> 02:12:06.340]   - I'll let them know.
[02:12:06.340 --> 02:12:08.220]   - I almost bought a turntable the other day.
[02:12:08.220 --> 02:12:10.380]   I came this close.
[02:12:10.380 --> 02:12:12.380]   Then I thought, well, then I have to buy records.
[02:12:12.380 --> 02:12:14.700]   - They make it easy.
[02:12:14.700 --> 02:12:17.820]   I'm out in one whole set on Amazon, of course.
[02:12:17.820 --> 02:12:20.780]   - How many times have I bought the Beatles, actually?
[02:12:20.780 --> 02:12:22.500]   If I counted it would be depressing.
[02:12:22.500 --> 02:12:23.500]   Many, many, many.
[02:12:23.500 --> 02:12:25.940]   - That's a business model, yeah.
[02:12:25.940 --> 02:12:26.940]   Just keep buying it.
[02:12:26.940 --> 02:12:27.940]   We don't have to make new music.
[02:12:27.940 --> 02:12:29.180]   Just keep buying remixes.
[02:12:29.180 --> 02:12:32.060]   Thank you everybody for being here.
[02:12:32.060 --> 02:12:33.460]   We do this week in Google.
[02:12:33.460 --> 02:12:37.220]   It's the my last show of my week because now I take Thursday and Friday off and I'll be
[02:12:37.220 --> 02:12:41.260]   back on Saturday, but I do it about 1.30 Wednesday afternoon.
[02:12:41.260 --> 02:12:45.740]   That's Pacific Time, 4.30 Eastern Time, 2.130 UTC.
[02:12:45.740 --> 02:12:46.980]   You do the math.
[02:12:46.980 --> 02:12:50.620]   You can watch live at twit.tv/live.
[02:12:50.620 --> 02:12:54.500]   If you're live, you should by all means go in the chat room and the chat room's been
[02:12:54.500 --> 02:12:55.500]   great.
[02:12:55.500 --> 02:12:56.500]   - That's ever.
[02:12:56.500 --> 02:12:57.500]   - Today, thank you.
[02:12:57.500 --> 02:13:00.780]   As ever, IRC.tv.
[02:13:00.780 --> 02:13:02.780]   If you have an IRC client, you can use that.
[02:13:02.780 --> 02:13:07.500]   But if you don't, there's a website and a web client, but we do love having you in chat.
[02:13:07.500 --> 02:13:08.900]   Thank you for being here.
[02:13:08.900 --> 02:13:10.300]   You can also visit us in studio.
[02:13:10.300 --> 02:13:14.620]   We had a nice studio visit today who ran out of the room, ran out of the room.
[02:13:14.620 --> 02:13:15.700]   No, actually it's interesting.
[02:13:15.700 --> 02:13:21.940]   He plays the Contra-based clarinet for the United States Navy Band.
[02:13:21.940 --> 02:13:26.380]   He's performing at Sonoma State tonight.
[02:13:26.380 --> 02:13:28.980]   He just thought he had a little time to come and watch the show.
[02:13:28.980 --> 02:13:30.500]   - You thought I was afraid it was something I said.
[02:13:30.500 --> 02:13:31.500]   - He had to leave.
[02:13:31.500 --> 02:13:33.820]   No, no, he has to go tune up his clarinet.
[02:13:33.820 --> 02:13:39.020]   The reads on those Contra-basis take a long time to moisten.
[02:13:39.020 --> 02:13:41.500]   I don't know if that's true or not.
[02:13:41.500 --> 02:13:45.060]   If you want to be in studio, please don't surprise us.
[02:13:45.060 --> 02:13:50.140]   Please just email tickets@twit.tv so we can have a nice comfy chair set out for you.
[02:13:50.140 --> 02:13:51.900]   Roll out the welcome mat and all of that.
[02:13:51.900 --> 02:13:52.900]   It's @twit.tv.
[02:13:52.900 --> 02:13:58.380]   While you're at the website, Twit.tv, don't forget our podcast, Twit.tv/twig, on demand
[02:13:58.380 --> 02:14:00.900]   versions of the show available there, audio and video.
[02:14:00.900 --> 02:14:05.420]   Of course, you can always subscribe either there or on any podcast client of your choice
[02:14:05.420 --> 02:14:09.780]   and get it every single week automatically delivered to your phone or your tablet or
[02:14:09.780 --> 02:14:11.260]   your computer.
[02:14:11.260 --> 02:14:15.380]   And man, I love these new voice assistants because you can listen to the show on every
[02:14:15.380 --> 02:14:20.940]   single one of them, whether it's the Apple HomePod or the Amazon Echo or even like the
[02:14:20.940 --> 02:14:25.980]   Cortana device from Harmon Cardin, the Google Home device, all you have to do is basically
[02:14:25.980 --> 02:14:32.700]   in most every cases is say, "Listen to this week in Google or listen to Twig and you should
[02:14:32.700 --> 02:14:33.900]   be able to get the most resellable."
[02:14:33.900 --> 02:14:35.940]   Okay, Google.
[02:14:35.940 --> 02:14:38.580]   Play this week in Google.
[02:14:38.580 --> 02:14:42.340]   This is going to be a very inception moment.
[02:14:42.340 --> 02:14:43.260]   Yay.
[02:14:43.260 --> 02:14:46.940]   Is it playing?
[02:14:46.940 --> 02:14:49.940]   You didn't know it could do that?
[02:14:49.940 --> 02:14:53.740]   No I didn't. Oh yeah you can also. Sorry.
[02:14:53.740 --> 02:14:55.340]   You can also listen to Twit life.
[02:14:55.340 --> 02:14:57.460]   In many cases just say. Oh, wow.
[02:14:57.460 --> 02:14:58.780]   Yeah. You'll hear the live stream.
[02:14:58.780 --> 02:14:59.940]   Yeah.
[02:14:59.940 --> 02:15:02.100]   I think this is the biggest thing that's ever happened to podcast.
[02:15:02.100 --> 02:15:03.660]   Frankly, as he was. Yeah.
[02:15:03.660 --> 02:15:06.660]   Jeff, you could just hold that up to the microphone and take the weak off.
[02:15:06.660 --> 02:15:09.620]   You know that. Why last week show?
[02:15:09.620 --> 02:15:13.020]   And always does the same things anyway.
[02:15:13.020 --> 02:15:16.180]   No, you don't Jeffrey. You do not,
[02:15:16.180 --> 02:15:18.540]   we were doing, I think, a few more weeks of the survey.
[02:15:18.540 --> 02:15:20.500]   If you haven't done it yet, Twit.tv/survey.
[02:15:20.500 --> 02:15:24.100]   Because we don't collect information about you like Google and Facebook,
[02:15:24.100 --> 02:15:25.740]   we don't know anything about you.
[02:15:25.740 --> 02:15:27.860]   And sometimes it's nice when an advertiser says,
[02:15:27.860 --> 02:15:31.300]   well, do any women listen to be able to say, yes, there are a couple.
[02:15:31.300 --> 02:15:35.740]   So go to Twit.tv/survey and fill that out.
[02:15:35.740 --> 02:15:36.540]   Represent.
[02:15:36.540 --> 02:15:38.300]   Let us know. Yes. Thank you.
[02:15:38.300 --> 02:15:39.740]   Probably wrong, but I did it. Yeah.
[02:15:39.740 --> 02:15:41.580]   We, you know what? We do it in aggregate.
[02:15:41.580 --> 02:15:45.540]   We usually get more than 20,000 responses, so it'll dilute you.
[02:15:46.260 --> 02:15:49.780]   Anyway, we don't save any information about you personally.
[02:15:49.780 --> 02:15:51.980]   We don't share any information about you personally.
[02:15:51.980 --> 02:15:54.300]   Don't worry, your information is safe.
[02:15:54.300 --> 02:15:58.860]   But it does help us a lot. Twit.tv/survey.
[02:15:58.860 --> 02:16:01.140]   Thanks for joining us. I'll see you next week.
[02:16:01.140 --> 02:16:02.820]   Speaking. Bye bye.
[02:16:02.820 --> 02:16:11.340]   (Music)

