;FFMETADATA1
title=Ensconced in Felt
artist=Leo Laporte, Jeff Jarvis, Ant Pruitt, Cathy Gellis
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-02-23
track=704
language=English
genre=Podcast
comment=Section 230 in the Supreme Court, Google from the inside, Meta "verified"
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100

[00:00:00.000 --> 00:00:03.120]   It's time for Twig this week in Google.
[00:00:03.120 --> 00:00:04.980]   Jeff's here, Ants here.
[00:00:04.980 --> 00:00:07.440]   Stacey has the week up, but we've had a great replacement.
[00:00:07.440 --> 00:00:10.840]   Attorney Kathy Gellis was actually in the courtroom yesterday
[00:00:10.840 --> 00:00:12.600]   during oral arguments.
[00:00:12.600 --> 00:00:16.000]   The Supreme Court case Google versus Gonzalez.
[00:00:16.000 --> 00:00:18.840]   The future of the Internet hangs in the balance.
[00:00:18.840 --> 00:00:22.440]   We'll get her analysis and a lot more next on this week in Google.
[00:00:22.440 --> 00:00:28.440]   Podcasts you love from people you trust.
[00:00:29.640 --> 00:00:31.200]   This is Twig.
[00:00:31.200 --> 00:00:37.880]   This is Twig.
[00:00:37.880 --> 00:00:44.800]   This week in Google, episode 704 recorded Wednesday, February 22nd, 2023
[00:00:44.800 --> 00:00:47.200]   ensconced in felt.
[00:00:47.200 --> 00:00:53.960]   This week in Google is brought to you by HPE GreenLake orchestrated by the
[00:00:53.960 --> 00:00:59.600]   experts at CDW who can help you consolidate and manage all your data in one
[00:00:59.680 --> 00:01:03.720]   flexible edge to cloud platform to scale and innovate.
[00:01:03.720 --> 00:01:08.080]   Learn more at CDW.com/HPE.
[00:01:08.080 --> 00:01:12.920]   Thanks for listening to this show as an ad supported network.
[00:01:12.920 --> 00:01:17.280]   We are always looking for new partners with products and services that will
[00:01:17.280 --> 00:01:19.920]   benefit our qualified audience.
[00:01:19.920 --> 00:01:21.440]   Are you ready to grow your business?
[00:01:21.440 --> 00:01:25.920]   Reach out to advertise at twit.tv and launch your campaign now.
[00:01:27.360 --> 00:01:31.320]   It's time for Twig this week in Google to show we cover the latest news from
[00:01:31.320 --> 00:01:32.600]   Google.
[00:01:32.600 --> 00:01:34.440]   This time it's actually got some Google news.
[00:01:34.440 --> 00:01:39.000]   Stacy has the day off, but we are really fortunate to be able to get
[00:01:39.000 --> 00:01:39.680]   Catherine R.
[00:01:39.680 --> 00:01:41.680]   Gellis Esquire in studio.
[00:01:41.680 --> 00:01:44.520]   Kathy Gellis, you've seen her right for Tech Dirt.
[00:01:44.520 --> 00:01:50.800]   She is an attorney that practices law in the digital age in Northern
[00:01:50.800 --> 00:01:51.520]   California.
[00:01:51.520 --> 00:01:55.800]   And it's so great to have you, Kathy, because yesterday she was in the
[00:01:55.800 --> 00:02:00.600]   Supreme Court in the chambers listening to the oral arguments in
[00:02:00.600 --> 00:02:02.120]   Gonzales versus Google.
[00:02:02.120 --> 00:02:02.960]   Kathy, welcome.
[00:02:02.960 --> 00:02:04.360]   So glad we could get you on.
[00:02:04.360 --> 00:02:05.840]   Thanks for having me.
[00:02:05.840 --> 00:02:06.280]   Yeah.
[00:02:06.280 --> 00:02:10.960]   Like Masnick said, I'm going to write about this, but I'll wait until
[00:02:10.960 --> 00:02:14.000]   Kathy really waves in with the real deal.
[00:02:14.000 --> 00:02:15.440]   We're going to ask you about those oral arguments.
[00:02:15.440 --> 00:02:18.240]   But first, let's say hi to the rest of our panel.
[00:02:18.240 --> 00:02:22.240]   Of course, the fabulous aunt Pruitt from Hands On Photography and our
[00:02:22.480 --> 00:02:26.480]   community manager at Club Twit.
[00:02:26.480 --> 00:02:27.520]   Hello, aunt.
[00:02:27.520 --> 00:02:34.720]   And of course, Leonard Tao, Professor, journalistic blah, blah, blah, blah, blah,
[00:02:34.720 --> 00:02:36.280]   yada, yada, yada, yada.
[00:02:36.280 --> 00:02:37.360]   The cards way over there.
[00:02:37.360 --> 00:02:38.080]   I can't reach it.
[00:02:38.080 --> 00:02:38.960]   There's the cutest card.
[00:02:38.960 --> 00:02:40.400]   There it is.
[00:02:40.400 --> 00:02:44.720]   The Leonard Tao, Professor for journalistic innovation at the Craig
[00:02:44.720 --> 00:02:48.280]   Nilmark Graduate School of Journalism at the City University of New York.
[00:02:48.280 --> 00:02:49.520]   Hello, Mr.
[00:02:49.520 --> 00:02:50.400]   No, no theme.
[00:02:50.400 --> 00:02:51.800]   No, no, no.
[00:02:51.800 --> 00:02:52.920]   Craig, please, please.
[00:02:52.920 --> 00:02:56.360]   Can I know just ask your indulgence?
[00:02:56.360 --> 00:02:58.760]   We're having a little bit of trouble here in the studio today.
[00:02:58.760 --> 00:03:01.400]   We're coming 40 decimals over your own.
[00:03:01.400 --> 00:03:03.160]   Let's just not push it.
[00:03:03.160 --> 00:03:04.200]   Definite you.
[00:03:04.200 --> 00:03:07.640]   Jeff, we're doing the show unplugged.
[00:03:07.640 --> 00:03:09.520]   I like it.
[00:03:09.520 --> 00:03:11.360]   Call it's time for quick unplugged.
[00:03:11.360 --> 00:03:17.800]   So, Kathy, many think that yesterday was one of the most momentous days
[00:03:17.800 --> 00:03:20.600]   in the history of the internet, of course, the Supreme Court.
[00:03:20.800 --> 00:03:24.720]   Her had oral arguments on Gonzalez versus Google.
[00:03:24.720 --> 00:03:25.560]   This was the case.
[00:03:25.560 --> 00:03:27.440]   Sad story.
[00:03:27.440 --> 00:03:32.680]   The Gonzalez family filed against Google saying that ISIS recruitment
[00:03:32.680 --> 00:03:34.080]   videos on YouTube.
[00:03:34.080 --> 00:03:38.680]   Well, they don't even say it was the cause of their daughter's death in a
[00:03:38.680 --> 00:03:43.200]   2015 terrorist mass killing in Paris.
[00:03:43.200 --> 00:03:46.080]   But they say it's bad anyway.
[00:03:46.080 --> 00:03:50.560]   And so it's a it seemed to me it's a bad case in the face of it, because
[00:03:51.400 --> 00:03:54.640]   they're not there's no connection between those videos and their daughter's
[00:03:54.640 --> 00:03:56.560]   death. And I'm very sorry for their daughter's death.
[00:03:56.560 --> 00:03:59.440]   But they went after Google anyway.
[00:03:59.440 --> 00:04:04.840]   There is another case that was heard today similar, but not the same.
[00:04:04.840 --> 00:04:08.560]   Yesterday's case really was about Section 230.
[00:04:08.560 --> 00:04:13.200]   Today's case included Twitter and I think Facebook as well as Google.
[00:04:13.200 --> 00:04:20.440]   But I'm not sure if if they were official petitioners.
[00:04:20.440 --> 00:04:21.680]   But I may be wrong.
[00:04:21.680 --> 00:04:24.960]   I don't remember the caption, but it's definitely Twitter that was pulling the
[00:04:24.960 --> 00:04:25.680]   weight, I believe.
[00:04:25.680 --> 00:04:27.560]   You went to the oral arguments yesterday.
[00:04:27.560 --> 00:04:31.840]   Did you not feel that today's were as important or were you just excited?
[00:04:31.840 --> 00:04:34.280]   It's it's the latter.
[00:04:34.280 --> 00:04:37.880]   It's an oddly arduous experience physically.
[00:04:37.880 --> 00:04:41.560]   It's I mean, there was a lot of anxiety heading into it.
[00:04:41.560 --> 00:04:46.920]   And me and all my colleagues were coping with great amounts of stress
[00:04:46.920 --> 00:04:50.400]   about existential stress of what was going to happen and everything we
[00:04:50.400 --> 00:04:53.520]   work for and everything we care about was really hanging in the balance.
[00:04:53.520 --> 00:04:57.200]   So going into that and then you have to wake up really, really early to get
[00:04:57.200 --> 00:05:00.480]   online because you also you don't know how many people will be there and want to
[00:05:00.480 --> 00:05:03.040]   get in. You don't know how many people they will let in.
[00:05:03.040 --> 00:05:08.160]   I got to be on the special line for members of the Supreme Court bar.
[00:05:08.160 --> 00:05:13.840]   But even so, it was still unknown of how like the last one I went to, the
[00:05:13.840 --> 00:05:17.840]   Andy Warhol case, they didn't let everybody in who was a Supreme Court bar
[00:05:17.840 --> 00:05:21.600]   member and I didn't want to be like the last person who got left out.
[00:05:21.600 --> 00:05:26.920]   So I had to go early standing around, then you get brought into a different spot
[00:05:26.920 --> 00:05:29.920]   and you stand around some more and there's more standing and more sitting.
[00:05:29.920 --> 00:05:35.080]   So looking at it for this morning was sort of maybe I'll give it a miss and
[00:05:35.080 --> 00:05:38.440]   I'll take care of some other stuff and and just listen in.
[00:05:38.440 --> 00:05:43.200]   But on the other hand, the trade off is it is definitely different to be in that
[00:05:43.200 --> 00:05:48.440]   room and see the justices as human beings and see their body language and see the
[00:05:48.440 --> 00:05:52.120]   way that they sort of look at each other and react to each other physically.
[00:05:52.120 --> 00:05:56.800]   It is definitely there's something that is lost when you just put it into the
[00:05:56.800 --> 00:06:00.280]   sound that goes through the the radio or the live stream.
[00:06:00.280 --> 00:06:03.600]   So you just don't see them flipping the bird at each other as we hear the court
[00:06:03.600 --> 00:06:04.560]   is doing these days.
[00:06:04.560 --> 00:06:08.520]   I don't think actually that is something that has ever caught my eye on any of them
[00:06:08.520 --> 00:06:09.680]   that I've attended.
[00:06:09.680 --> 00:06:11.480]   But they actually see you go here.
[00:06:11.480 --> 00:06:14.480]   They were quite collegial.
[00:06:14.480 --> 00:06:15.480]   I thought it was.
[00:06:15.480 --> 00:06:17.840]   They are very collegial.
[00:06:17.840 --> 00:06:21.520]   I mean, there's a certain cognitive dissonance and there's going to be one that
[00:06:21.520 --> 00:06:25.400]   I'll end up talking about a little bit more, which is just to go a little
[00:06:25.400 --> 00:06:31.480]   spoilery. One of my favorite justices as this as either of these hearings
[00:06:31.480 --> 00:06:33.320]   unfolded is Justice Kavanaugh.
[00:06:33.320 --> 00:06:36.880]   Brett Kavanaugh actually seemed to know something about the Internet.
[00:06:37.720 --> 00:06:43.600]   In a very distinct way, he understood really important things where if I, you
[00:06:43.600 --> 00:06:47.560]   know, got to write a memo of bullet points of dear justices, this is what you need
[00:06:47.560 --> 00:06:49.800]   to know, which is essentially what I did in writing an immediate.
[00:06:49.800 --> 00:06:52.200]   You wrote a brief for the first case for Gonzalez.
[00:06:52.200 --> 00:06:52.800]   Yeah.
[00:06:52.800 --> 00:06:56.360]   I wrote one for the first case for Gonzalez, but I would have put a list of
[00:06:56.360 --> 00:06:59.520]   like, this is how you look at it and his questions were articulating and
[00:06:59.520 --> 00:07:02.320]   understanding that was I was also.
[00:07:02.320 --> 00:07:07.560]   I also appreciated Justice Kagan saying these are not the nine greatest
[00:07:07.560 --> 00:07:09.320]   experts on the Internet.
[00:07:09.320 --> 00:07:11.800]   She was at least humble.
[00:07:11.800 --> 00:07:17.200]   So, but before we get into the details of the argument, so there were two cases.
[00:07:17.200 --> 00:07:19.680]   The second was Twitter versus Tamna.
[00:07:19.680 --> 00:07:24.000]   So Twitter was the plaintiff or no, I don't know.
[00:07:24.000 --> 00:07:25.920]   Twitter was the defendant in this one.
[00:07:25.920 --> 00:07:30.760]   They were both the defendants, but it was reversed in terms of the petition for
[00:07:30.760 --> 00:07:31.840]   the certiori.
[00:07:31.840 --> 00:07:35.840]   OK, so Twitter asked for this one to go to the Supreme Court.
[00:07:35.840 --> 00:07:36.320]   Is that right?
[00:07:36.320 --> 00:07:37.400]   Twitter asked for this one.
[00:07:37.400 --> 00:07:40.600]   And it was a contingent petition that basically said, if you're going to take
[00:07:40.600 --> 00:07:42.600]   the other one, take this one too.
[00:07:42.600 --> 00:07:48.240]   And then, but that's why Twitter was sort of the lead in this one, where it's
[00:07:48.240 --> 00:07:49.320]   Twitter versus Tamna.
[00:07:49.320 --> 00:07:53.040]   But for the other one, it's Gonzalez versus Google because in that one, it was
[00:07:53.040 --> 00:07:53.560]   Gonzalez.
[00:07:53.560 --> 00:07:54.320]   It's a dear court.
[00:07:54.320 --> 00:07:55.080]   Please look at it.
[00:07:55.080 --> 00:07:55.600]   OK.
[00:07:55.600 --> 00:08:00.680]   And the Twitter case, apparently I've heard is not quite as directly an assault on
[00:08:00.680 --> 00:08:10.400]   Section 230, Tamna is accusing Twitter of promoting Islamic terrorism by allowing
[00:08:10.400 --> 00:08:13.880]   tweets, basically, and retweets on its site.
[00:08:13.880 --> 00:08:21.440]   But the first case, yesterday's case, I guess the first question I would ask is
[00:08:21.440 --> 00:08:24.760]   why did the court agree to review either of these cases?
[00:08:24.760 --> 00:08:26.480]   Yeah, they're not great cases.
[00:08:26.480 --> 00:08:30.480]   I think the court is currently asking itself that very question.
[00:08:31.120 --> 00:08:32.080]   It will end.
[00:08:32.080 --> 00:08:35.400]   And I say that somewhat flippantly, but I think
[00:08:35.400 --> 00:08:42.320]   what we think is that there had been an appetite to take a Section 230 case because
[00:08:42.320 --> 00:08:45.960]   we were seeing these and I'm forgetting, I think they were concurrence is not
[00:08:45.960 --> 00:08:49.200]   the sense or they were descents against denial of certiori.
[00:08:49.200 --> 00:08:52.920]   So we had the malware bites case and then there was another case later where
[00:08:52.920 --> 00:08:57.160]   just as Thomas wrote, at least one of them, I think, Alito may have read in another one
[00:08:57.360 --> 00:09:01.480]   where these were things of like, we have never considered Section 230 and boy, what a big
[00:09:01.480 --> 00:09:05.280]   deal, we better make sure that this big monumental law is really doing things
[00:09:05.280 --> 00:09:11.760]   because there were complaints by people that they politically related to who have
[00:09:11.760 --> 00:09:15.200]   been complaining about, well, Section 230 is the ban of all the existence.
[00:09:15.200 --> 00:09:18.120]   And this is why all these people with these political views are getting censored,
[00:09:18.120 --> 00:09:18.800]   et cetera, et cetera.
[00:09:18.800 --> 00:09:22.800]   This isn't actually the case, but this was a why this is a view that is held by
[00:09:22.800 --> 00:09:24.360]   people that they politically agree with.
[00:09:24.720 --> 00:09:28.120]   So they were expressing an appetite of we should take a case.
[00:09:28.120 --> 00:09:31.840]   And then all of a sudden this case comes and it looks kind of provocative.
[00:09:31.840 --> 00:09:33.240]   It's got terrible optics.
[00:09:33.240 --> 00:09:34.320]   It's like big, bad Google.
[00:09:34.320 --> 00:09:37.560]   Um, out of nowhere, they took it.
[00:09:37.560 --> 00:09:41.960]   I mean, I think, I think it really blindsided the community of lawyers that I'm
[00:09:41.960 --> 00:09:45.960]   operating with and we just didn't think this was a case that was going to go anywhere.
[00:09:45.960 --> 00:09:46.880]   Why was that?
[00:09:46.880 --> 00:09:48.480]   Is it because it's a weak case?
[00:09:48.480 --> 00:09:50.480]   It was a weak case.
[00:09:50.480 --> 00:09:55.000]   I think it was weak facially and it was just one in the many cases.
[00:09:55.000 --> 00:09:58.960]   There's, there's these plaintiffs lawyers who keep trying to terrible things happen.
[00:09:58.960 --> 00:10:03.840]   A terrorist has caused collateral damage to innocent people and well, I guess by their
[00:10:03.840 --> 00:10:07.840]   view, it's not collateral, but, um, but then you, there's all these people who are
[00:10:07.840 --> 00:10:09.360]   like, well, somebody must be blamed.
[00:10:09.360 --> 00:10:11.880]   These terrorists were using the end of the internet.
[00:10:11.880 --> 00:10:12.960]   So let's blame the internet.
[00:10:12.960 --> 00:10:17.000]   So there's certain lawyers who kept bringing case after case and then losing
[00:10:17.000 --> 00:10:18.080]   for a variety of reasons.
[00:10:18.080 --> 00:10:18.720]   They kept losing.
[00:10:18.880 --> 00:10:22.640]   Some, I think, lost in 230 grounds, some were losing on ATA grounds, some were
[00:10:22.640 --> 00:10:26.680]   losing because just the complaints, like you have to have causation of the harm
[00:10:26.680 --> 00:10:32.080]   and the, and the consequences have some sort of link, but lawsuits are expensive.
[00:10:32.080 --> 00:10:36.200]   So, you know, one of the points of 230 is that like the lawsuit goes away very
[00:10:36.200 --> 00:10:39.200]   quickly where it can't get to a point where you've really spent any real money
[00:10:39.200 --> 00:10:41.440]   to find out that you had no liability in the first place.
[00:10:41.440 --> 00:10:45.800]   But somehow this case got escaped velocity, partly because the ninth circuit, I
[00:10:45.800 --> 00:10:52.000]   think is getting very grumpy about 230 itself and is starting to kind of try to
[00:10:52.000 --> 00:10:55.960]   snip at it a little bit and cut its corners in ways that I think are not a
[00:10:55.960 --> 00:10:56.600]   good idea.
[00:10:56.600 --> 00:11:01.680]   In this case, I think they looked and said, well, precedent binds us to, um, get
[00:11:01.680 --> 00:11:05.960]   rid of the Gonzalez complaint because of section 230, but they had like all
[00:11:05.960 --> 00:11:07.800]   this language about like, but we don't like that.
[00:11:07.800 --> 00:11:08.840]   That, that, that doesn't seem right.
[00:11:08.840 --> 00:11:13.520]   Even Justice Kavanaugh said yesterday, this is really a matter for Congress, not us.
[00:11:14.520 --> 00:11:19.960]   Well, so to, to my long winded answer, your question is this came, they took the case,
[00:11:19.960 --> 00:11:23.520]   they ended up taking both cases, but then I think a couple things happened.
[00:11:23.520 --> 00:11:26.040]   One, the cases themselves are weak.
[00:11:26.040 --> 00:11:30.120]   I think they noticed once they started reading them more closely, I think once
[00:11:30.120 --> 00:11:33.720]   the briefing came in, they noticed it more closely because in Gonzalez case has a
[00:11:33.720 --> 00:11:38.240]   problem where certiori is granted, which is the Supreme Court review to say, we're
[00:11:38.240 --> 00:11:42.160]   going to look at this particular question of law and they, when they grant the case,
[00:11:42.160 --> 00:11:45.360]   they grant it with the question of what they're going to look at when they, when
[00:11:45.360 --> 00:11:46.360]   they do this review.
[00:11:46.360 --> 00:11:51.720]   And when the petitioners filed their brief, they changed the question and you're not
[00:11:51.720 --> 00:11:52.800]   supposed to do that.
[00:11:52.800 --> 00:11:55.920]   I mean, that's enough grounds to kick the case unto itself.
[00:11:55.920 --> 00:12:00.520]   But then the other thing that happened is a gazillion and I'm rounding slightly a
[00:12:00.520 --> 00:12:06.440]   gazillion amicus briefs came in, including yours, including mine, particularly on
[00:12:06.440 --> 00:12:11.440]   the side of Google where that, the bulk of people talking about the parade of
[00:12:11.440 --> 00:12:16.000]   cars that was going to happen is section 230 got cut in some way.
[00:12:16.000 --> 00:12:20.360]   I think really impressed the court where they started to realize there was a
[00:12:20.360 --> 00:12:25.600]   there, there and maybe, and I, they did seem very, with one possible exception,
[00:12:25.600 --> 00:12:29.840]   they seem generally much better educated on the issues than I think we had expected.
[00:12:29.840 --> 00:12:32.720]   And I think that education may have shaped their views where they
[00:12:32.720 --> 00:12:34.120]   realize that more was at stake.
[00:12:34.120 --> 00:12:38.080]   And especially with a weak case that was weekly pled and weekly briefed and
[00:12:38.080 --> 00:12:43.400]   weekly argued, I think now they're like, wait, big things are at stake and we're
[00:12:43.400 --> 00:12:44.880]   going to topple a lot.
[00:12:44.880 --> 00:12:46.600]   We see what's at risk over this.
[00:12:46.600 --> 00:12:49.960]   So I think they're having second thoughts about having granted for
[00:12:49.960 --> 00:12:53.040]   Gonzalez and also peripherally also with the time of case.
[00:12:53.040 --> 00:12:58.160]   Um, if they, if they get together in their, in their club room and say, oh, we
[00:12:58.160 --> 00:12:59.040]   shouldn't have taken this.
[00:12:59.040 --> 00:13:00.200]   There's really no issues here.
[00:13:00.200 --> 00:13:00.840]   There's really nothing.
[00:13:00.840 --> 00:13:04.320]   Can they just issue like a, a, a one page that says nevermind?
[00:13:04.600 --> 00:13:08.040]   Or can they now have to deal with much of what was argued?
[00:13:08.040 --> 00:13:11.040]   Um, I think they have choices.
[00:13:11.040 --> 00:13:14.160]   I don't know entirely technically what all the choices are, but on the other
[00:13:14.160 --> 00:13:18.360]   hand, they also have been making an awful lot as they up a whole lot as they, uh,
[00:13:18.360 --> 00:13:19.440]   go on these days.
[00:13:19.440 --> 00:13:21.560]   So, um, I think it'll be whatever they want to be.
[00:13:21.560 --> 00:13:24.080]   There's something that is referred to as a dig.
[00:13:24.080 --> 00:13:28.040]   Um, I've seen people refer to it and I don't know the full acronym, but it's
[00:13:28.040 --> 00:13:33.440]   basically improvidently granted is the problem where they're just going to like set it
[00:13:33.440 --> 00:13:33.880]   aside.
[00:13:33.880 --> 00:13:34.720]   We, we blew it.
[00:13:34.720 --> 00:13:35.520]   So that is a thing.
[00:13:35.520 --> 00:13:36.400]   Yeah.
[00:13:36.400 --> 00:13:37.760]   So they could do that.
[00:13:37.760 --> 00:13:39.080]   I mean, I think they have options.
[00:13:39.080 --> 00:13:40.640]   I think they could do that just blankly.
[00:13:40.640 --> 00:13:44.320]   I don't know what that looks like, but maybe it's a, maybe it is a one line order.
[00:13:44.320 --> 00:13:47.000]   Um, they could just find in favor.
[00:13:47.000 --> 00:13:50.680]   They could find in favor or as a perpurium and not write any language, I
[00:13:50.680 --> 00:13:52.480]   suppose they could write language.
[00:13:52.480 --> 00:13:57.800]   I don't, I don't know if it's even worth guessing or even looking up past, uh,
[00:13:57.800 --> 00:14:00.520]   Supreme Court process because they're going to do what they're going to do.
[00:14:00.520 --> 00:14:04.600]   But I think they do have options and the idea that they could say, oops,
[00:14:04.600 --> 00:14:05.640]   and not render a decision.
[00:14:05.640 --> 00:14:08.280]   The case is, I think, I think it's something that they could have.
[00:14:08.280 --> 00:14:10.360]   How did Google do in the arguments?
[00:14:10.360 --> 00:14:12.200]   How did they do in the arguments?
[00:14:12.200 --> 00:14:17.280]   Um, there were a couple of things I think they conceded that the compute,
[00:14:17.280 --> 00:14:18.880]   that the community will not like.
[00:14:18.880 --> 00:14:24.040]   Um, I point people to Eric Olpens blog and his objections to the Henderson case
[00:14:24.040 --> 00:14:27.080]   because Google said that they kind of liked the Henderson test.
[00:14:27.080 --> 00:14:30.360]   And even the justices were a little bit like, I think what, um,
[00:14:30.360 --> 00:14:31.960]   actually said, are you sure?
[00:14:31.960 --> 00:14:36.320]   Um, so I'm not entirely sure that was the best way to zig and zag.
[00:14:36.320 --> 00:14:42.320]   Um, I think the Google lawyer also had trouble connecting with justice Jackson.
[00:14:42.320 --> 00:14:49.480]   Um, just as Jackson, I think did, does not quite see the situation the way it needs
[00:14:49.480 --> 00:14:50.480]   to be seen yet.
[00:14:50.480 --> 00:14:54.000]   And several times that she was by the whole thing.
[00:14:54.000 --> 00:14:59.520]   Well, and I'm concerned also, she said section two 30 is a narrow statute.
[00:14:59.520 --> 00:15:02.880]   And I'm sort of like, well, I really wish you read my amicus brief.
[00:15:02.880 --> 00:15:05.760]   I have a whole section titled how it's a broads.
[00:15:05.760 --> 00:15:09.320]   So, um, let me read my brief.
[00:15:09.320 --> 00:15:11.160]   I think it'll, it'll educate you.
[00:15:11.160 --> 00:15:16.040]   Um, let me read, let me read section two 30, just so that it's very quick.
[00:15:16.040 --> 00:15:22.560]   It says, as you've pointed out a bunch of times, Jeff, it's 26 words, uh, that, uh,
[00:15:22.560 --> 00:15:25.400]   change the internet key, the key section of it.
[00:15:25.720 --> 00:15:31.280]   No provider or user of an interactive computer service shall be treated as the
[00:15:31.280 --> 00:15:40.200]   publisher or speaker of any information provided by another information content provider.
[00:15:40.200 --> 00:15:45.400]   So the, the point of this, and Ron Wyden wrote it, it kind of as an antidote to some
[00:15:45.400 --> 00:15:50.640]   of the features of the communications decency act, but the point of it was twofold.
[00:15:50.640 --> 00:15:55.520]   One, you're not responsible, whether you're Twitter, Google or Leo LePort running an
[00:15:55.520 --> 00:16:00.720]   IRC for something somebody puts in that IRC, they're responsible, not you.
[00:16:00.720 --> 00:16:01.640]   You're not the publisher.
[00:16:01.640 --> 00:16:08.560]   Uh, and two, if I decide to moderate any of these or Google or Facebook or Twitter
[00:16:08.560 --> 00:16:11.720]   does, I am not liable for taking it down.
[00:16:11.720 --> 00:16:15.120]   Uh, and in fact, uh, I think, um, Ms.
[00:16:15.120 --> 00:16:20.800]   Blatt, the, uh, attorney for Google, um, said you, you know, you're, if you throw over,
[00:16:20.800 --> 00:16:22.200]   turn this, you've got two choices.
[00:16:22.200 --> 00:16:26.120]   Either the internet's going to become this anodyne happy place where nobody says anything
[00:16:26.120 --> 00:16:31.400]   controversial or it's a hellscape because nobody can moderate and there's nothing in
[00:16:31.400 --> 00:16:31.840]   between.
[00:16:31.840 --> 00:16:36.040]   So this was quite a, quite a, I think brilliant, prudent.
[00:16:36.040 --> 00:16:43.760]   Uh, and I think very broad, uh, part of the code that really did make the internet a safer
[00:16:43.760 --> 00:16:45.440]   place for people like me.
[00:16:45.440 --> 00:16:50.280]   And that was, this is always the thing that irritated me is that everybody treated it
[00:16:50.280 --> 00:16:52.800]   like we're going after Google or Twitter or Facebook.
[00:16:52.800 --> 00:16:59.200]   Those guys actually could weather the loss of 230 much better than I could, but I have a, a
[00:16:59.200 --> 00:17:04.960]   forums, I have chat, I have a discord, I have comments, all of which I would suddenly be
[00:17:04.960 --> 00:17:08.360]   liable for and it would make me go ahead.
[00:17:08.520 --> 00:17:11.720]   Relevant to the brief, you have a mastodon server.
[00:17:11.720 --> 00:17:12.120]   Right.
[00:17:12.120 --> 00:17:17.520]   And so one of the key things about my amicus brief is one of the signatories is an admin
[00:17:17.520 --> 00:17:22.200]   of a mastodon server because one of the things that's happened as this whole case was unfolding,
[00:17:22.200 --> 00:17:26.760]   a cert was granted, then Musk took over Twitter and all of a sudden you had the exodus.
[00:17:26.760 --> 00:17:31.000]   So mastodon was becoming more and more prominent used by more and more people and the importance
[00:17:31.000 --> 00:17:34.240]   of having this alternative was getting more and more important over time.
[00:17:34.560 --> 00:17:39.480]   And it seemed to me that we should have that voice brought to bear to explain to the court
[00:17:39.480 --> 00:17:40.840]   that it isn't about Google.
[00:17:40.840 --> 00:17:46.600]   It is about individual people who are trying, either as a small business or just for the
[00:17:46.600 --> 00:17:52.200]   benevolence of trying to serve their community, offering a vehicle so that other people can
[00:17:52.200 --> 00:17:53.360]   speak to each other online.
[00:17:53.360 --> 00:18:01.200]   Yeah, I make, I make, I make no money on mastodon discourse or forums on, on our IRC.
[00:18:01.200 --> 00:18:02.600]   Those are all outbound.
[00:18:02.640 --> 00:18:05.640]   I pay for all those without any monetization.
[00:18:05.640 --> 00:18:12.480]   And as a result, if I were suddenly liable to the nice about 230, if somebody comes after me,
[00:18:12.480 --> 00:18:15.080]   I don't even, I think this is right, Kathy.
[00:18:15.080 --> 00:18:15.920]   I don't even have to defend it.
[00:18:15.920 --> 00:18:17.880]   The judge is going to say, no, they're protected by 230.
[00:18:17.880 --> 00:18:18.840]   That's dismissed.
[00:18:18.840 --> 00:18:19.720]   Right?
[00:18:19.720 --> 00:18:22.600]   There's no, I mean, you have to do a little bit of lawyering to that.
[00:18:22.600 --> 00:18:23.880]   It's not quite as automatic.
[00:18:23.880 --> 00:18:29.880]   You have to plead it, but it should basically be that the judge says there's nothing to go forward here.
[00:18:29.880 --> 00:18:30.800]   This is not a question.
[00:18:30.800 --> 00:18:31.680]   This is an easy thing.
[00:18:31.680 --> 00:18:32.720]   It's pretty automatic.
[00:18:32.720 --> 00:18:44.720]   And I could bear, I could bear that bird, but if I have to suddenly have to defend everything and believe me, there are plenty of people on the internet who would file frivolous lawsuits in an attempt to bring us down.
[00:18:44.720 --> 00:18:51.560]   If I had to defend every one of those, every decision, whether to moderate or not, I'd be out of business.
[00:18:51.560 --> 00:18:54.240]   So I think we would preemptively shut everything down.
[00:18:54.240 --> 00:18:56.320]   It would, it would chill discourse on the internet.
[00:18:56.320 --> 00:18:58.040]   It would chill discourse.
[00:18:58.040 --> 00:19:00.640]   It also chills your expressive discourse.
[00:19:00.760 --> 00:19:13.520]   You are very similar to Mike Masnick, who has a business and his business is to contribute to discourse, to get his own expression out there and to make sure that other expression that he agrees with in support with can can get out there.
[00:19:13.520 --> 00:19:14.680]   But he's a publisher, right?
[00:19:14.680 --> 00:19:18.120]   So he's liable as a publisher to 30 doesn't protect his opinion.
[00:19:18.120 --> 00:19:20.600]   No, not his directly.
[00:19:20.600 --> 00:19:30.640]   However, the point being is that some of these ancillary things where you're providing the forum for other people to speak is in line with furthering your own expressive interest to that.
[00:19:30.640 --> 00:20:00.440]   That you, as a person whose business is expressing yourself, needs the ability to have these ancillary avenues for both to use your own speech to make sure that you know your shows can be tweeted and shared around, but also that you can nurture the community of your audience, because you will make your business succeed by connecting with the audience and having the ability to host these forums is also helping you build your audience as an expressive entity, which the First Amendment protects and encourages.
[00:20:00.440 --> 00:20:05.160]   And democracy needs people like you to tell people things and keep the discourse going.
[00:20:05.160 --> 00:20:08.200]   So there's First Amendment stuff everywhere you look with it.
[00:20:08.200 --> 00:20:13.200]   And Section 230 is all about making that meaningful and redeeming and vindicating those values.
[00:20:13.200 --> 00:20:18.480]   And it's such a shame that people keep missing it and interpreting it as something exactly opposite to that.
[00:20:18.480 --> 00:20:28.160]   I was going to ask you if there was some type of middle ground with Section 230 versus the people that are like, I think Mike Masnick put it best.
[00:20:28.160 --> 00:20:31.160]   If you split the baby, the baby still dies.
[00:20:31.160 --> 00:20:33.680]   [laughter]
[00:20:33.680 --> 00:20:37.520]   Actually, did he say, I'm looking at my brief right now.
[00:20:37.520 --> 00:20:39.320]   Was that your brief?
[00:20:39.320 --> 00:20:42.640]   I had some line like that in the...
[00:20:42.640 --> 00:20:45.000]   He stole it in his post today.
[00:20:45.000 --> 00:20:47.360]   I can steal my best material.
[00:20:47.360 --> 00:20:48.800]   I'll have to take this out with him.
[00:20:48.800 --> 00:20:57.440]   But that's a great question, because in fact, I even said, well, because they hadn't modified the complaint to say, well, it's the algorithm that's the problem.
[00:20:57.440 --> 00:21:02.960]   And I was saying, well, maybe they could just say, no algorithmic recommendations in 230.
[00:21:02.960 --> 00:21:03.800]   We protected.
[00:21:03.800 --> 00:21:05.600]   Of course, Jeff spanked me hard on that.
[00:21:05.600 --> 00:21:06.720]   I'm still feeling it a little bit.
[00:21:06.720 --> 00:21:11.840]   And you, open-minded wise man that you are changed to mind about that.
[00:21:11.840 --> 00:21:12.640]   Oh, completely.
[00:21:12.640 --> 00:21:25.080]   So, is that what you're thinking is like there's impossible to kind of slice this a little bit so that you have some more protection, but that 230 is still intact?
[00:21:25.080 --> 00:21:33.160]   Yeah, but at the same time, our leaders are also saying, hey, these big tech companies can't censor us either.
[00:21:33.160 --> 00:21:35.760]   So, where's the line that we're going to draw?
[00:21:35.760 --> 00:21:37.160]   Not there.
[00:21:37.160 --> 00:21:39.160]   Not there, because...
[00:21:39.160 --> 00:21:47.560]   I mean, one of the things is that the way the ecosystem works, and let's look at Twitter and Mastodon as an example of that.
[00:21:47.560 --> 00:21:50.080]   So, must go on Twitter.
[00:21:50.080 --> 00:21:57.440]   And, okay, previously, must didn't own Twitter. So, Twitter was making decisions about who to allow to use Twitter and who not to.
[00:21:57.440 --> 00:22:04.640]   And these decisions, especially the ones that said, go away, were not liked by the people who had to go away.
[00:22:04.640 --> 00:22:08.160]   You can, I can understand the hurt feelings from that.
[00:22:08.160 --> 00:22:11.720]   So, I don't want to poo poo that, but Twitter could make those decisions.
[00:22:11.720 --> 00:22:20.040]   Now, Twitter's owned by somebody else, and he can make different decisions based on who he wants to be associated with, just as Leo can make decisions.
[00:22:20.040 --> 00:22:23.800]   With his Mastodon instance of who he's who he wants to be associated with.
[00:22:23.800 --> 00:22:28.040]   And I do every day when I approve and/or disapprove new users.
[00:22:28.040 --> 00:22:30.720]   And individual people can do this too.
[00:22:30.720 --> 00:22:46.920]   So, for instance, if you've got a Facebook page and you make a Facebook post that people can comment on, and somebody, like you've posted a picture of your kid, if somebody comes over and says your kids ugly, you certainly want to be able to get rid of that comment and potentially block that user from coming back and commenting,
[00:22:46.920 --> 00:22:53.880]   because that's not comfortable to you. And you can sort of understand the editorial freedom you have to be able to make that decision.
[00:22:53.880 --> 00:22:56.040]   Just similarly, the opposite is true.
[00:22:56.040 --> 00:23:02.680]   If somebody comes and praises your kid, you wouldn't want any law to be able to compel you to have to take that comment down, because that's your favorite thing.
[00:23:02.680 --> 00:23:03.680]   Oh, this is interesting.
[00:23:03.680 --> 00:23:05.280]   Just a little side note.
[00:23:05.280 --> 00:23:15.400]   The ability to block somebody, which is integral to both Mastodon and Twitter, would that also be, is that protected by 230? Would that also be at risk?
[00:23:15.400 --> 00:23:18.120]   My personal ability to block somebody on that site?
[00:23:18.120 --> 00:23:29.560]   If 230 should operate, if 230 starts to get clipped, then it's unclear exactly how much devastation could follow, because in theory, it could be all of it.
[00:23:29.560 --> 00:23:34.240]   There's a lot of thought about, "Oh, we'll just make it a narrow snip here and there."
[00:23:34.240 --> 00:23:44.920]   And no, that's why the baby and Mike and I are using the baby analogy, because the whole point of the Solomon story is that there are certain things where the compromise is impossible.
[00:23:44.920 --> 00:23:49.720]   The other part he was willing to split the baby that was brilliant, Solomon's decision was brilliant.
[00:23:49.720 --> 00:23:57.560]   And the other thing is that you can't have that split, because I think of what the way I phrased it in the brief, was it would be fatal to the purpose of what you're trying to do.
[00:23:57.560 --> 00:24:11.040]   The problem with, if you're a platform who's going to potentially be sued for all of the gazillions of bits of user provided information you have, even one of those lawsuits,
[00:24:11.040 --> 00:24:16.240]   even if it's a non-meritorious lawsuit where you would win, can wipe you out.
[00:24:16.240 --> 00:24:20.040]   And certainly, if you've got more than one, then you don't stand a chance.
[00:24:20.040 --> 00:24:23.520]   So you want to avoid the lawsuit, because you can't afford the lawyers.
[00:24:23.520 --> 00:24:32.880]   The problem with clipping away at Section 230 is that if all of a sudden you're having to spend your money deciding whether Section 230 applies to you,
[00:24:32.880 --> 00:24:37.920]   then it doesn't really matter whether it applies or not, because you're still going to drown in the litigation,
[00:24:37.920 --> 00:24:41.200]   because it was never about whether litigation was valid in the first place.
[00:24:41.200 --> 00:24:46.480]   The problem was the litigation is going to bleed you dry, and we need to make sure you don't get blood dry,
[00:24:46.480 --> 00:24:51.680]   because if you are, you're not going to be able to be in the business of helping people speak to each other online.
[00:24:51.680 --> 00:24:59.600]   So Eric Goldman writing about Sertjori in his technology and marketing law blog that you referred to said, I love this.
[00:24:59.600 --> 00:25:02.080]   I remain unclear why the court granted certain this case.
[00:25:02.080 --> 00:25:06.720]   The plaintiff's arguments were so weak, the justices really didn't know what to do with them.
[00:25:06.720 --> 00:25:15.920]   A tip off came with Justice Thomas' very first question asking if the ISIS recommendations were the product of a quote neutral algorithm.
[00:25:15.920 --> 00:25:18.640]   This is a Google favorable question.
[00:25:18.640 --> 00:25:26.240]   Justice Thomas had begged plaintiffs to bring him to 30 cases, and now that he has one, apparently he's like, "WTF?"
[00:25:26.240 --> 00:25:27.760]   [laughter]
[00:25:27.760 --> 00:25:31.280]   I imagine the other justices who voted for Sertj felt similar.
[00:25:31.280 --> 00:25:38.720]   Quams, I was really heartened by people like Gorsuch, Kavanaugh, and Thomas, and Alito even,
[00:25:38.720 --> 00:25:48.640]   people I thought, "Oh, these are the guys who really want to kill 230," really asking pretty astute questions, mostly in Google's favor, I think.
[00:25:48.640 --> 00:25:52.880]   Now, Kathy, before we go too much farther, I've said this before.
[00:25:52.880 --> 00:25:53.920]   I know others have said this.
[00:25:53.920 --> 00:25:57.840]   You can't judge too much based on the oral arguments, right?
[00:25:57.840 --> 00:26:00.240]   Absolutely true.
[00:26:00.240 --> 00:26:07.360]   However, I think the same thing that Eric was commenting on is probably something that is probably true.
[00:26:07.360 --> 00:26:23.200]   We were expecting a very hostile audience that was holding on to a lot of the myths that are so pervasive in public discourse about 230, what it does and why or why it's important or not.
[00:26:23.200 --> 00:26:32.560]   We didn't get that. We ended up getting a bench that was surprisingly informed and seemed to get it and seemed to understand what was at stake.
[00:26:32.560 --> 00:26:39.560]   We're kind of grappling with that because if you practice in this area, you're not used to having good days.
[00:26:39.560 --> 00:26:45.120]   It actually seemed like something like, "Wait, this must be too good to be true, so how could it be true?"
[00:26:45.120 --> 00:26:51.360]   I think though the problem is we might still get a little complacent because I think Google will win.
[00:26:51.360 --> 00:26:54.240]   I don't really think that the number of votes is in question.
[00:26:54.240 --> 00:27:00.000]   I mean, they might just get rid of the whole case, but if they decide it, I think it's not an issue where the petitioners are going to win.
[00:27:00.000 --> 00:27:02.640]   I didn't sense any appetite by the justices.
[00:27:02.640 --> 00:27:04.000]   They didn't seem impressed by that.
[00:27:04.000 --> 00:27:06.000]   Or Gonzalez argument at all.
[00:27:06.000 --> 00:27:07.680]   But it's not enough.
[00:27:07.680 --> 00:27:14.000]   The problem is if they kind of start to speculate about potential limits to section 230,
[00:27:14.000 --> 00:27:17.920]   even though that language is probably going to be dicta and so not particularly binding,
[00:27:17.920 --> 00:27:24.720]   it's just going to make an absolute mess of things and start to essentially become binding precedent as other courts interpret it.
[00:27:24.720 --> 00:27:28.640]   And it's going to be a roadmap for all the future litigation to challenge section 230.
[00:27:28.640 --> 00:27:31.280]   Yeah, let's not forget that until we get the decision.
[00:27:31.280 --> 00:27:37.680]   In the Henderson test, the judge quoted Thomas' dissent three times.
[00:27:37.680 --> 00:27:45.120]   So I think that might be why the justices were like, "Are you sure you like this because it wasn't something you absolutely hate at the root of it?"
[00:27:45.120 --> 00:27:47.200]   It was kind of shocking.
[00:27:47.200 --> 00:27:53.200]   So sometimes the justices will play devil's advocate.
[00:27:53.200 --> 00:28:00.640]   They will attempt to, they'll sound like they're in favor of an argument because they're attempting to find the holes in it, etc, etc.
[00:28:00.640 --> 00:28:02.720]   Which is why you don't know what's going to happen.
[00:28:02.720 --> 00:28:07.040]   This will be what they'll announce in if they do continue with the case in roughly in June, right?
[00:28:07.040 --> 00:28:09.200]   It's hard to say.
[00:28:09.200 --> 00:28:12.160]   I mean, last time I think I was here, I was talking about the Warhol case.
[00:28:12.160 --> 00:28:14.640]   So that was October and there's no decision there.
[00:28:14.640 --> 00:28:20.800]   So they're starting to render decisions, but on a schedule that is only known to them.
[00:28:20.800 --> 00:28:21.120]   Right.
[00:28:21.120 --> 00:28:22.400]   But it's at least months out.
[00:28:22.400 --> 00:28:25.040]   So we'll have you back then.
[00:28:25.040 --> 00:28:29.760]   It was slightly, but either to celebrate, we'll either have a cake or we'll all be dressed in black.
[00:28:29.760 --> 00:28:32.400]   Is it that dangerous? Is it that risky?
[00:28:32.400 --> 00:28:37.760]   Am I overselling the risks to myself if 230 falls?
[00:28:37.760 --> 00:28:39.120]   No, you're not.
[00:28:39.120 --> 00:28:39.600]   No.
[00:28:39.600 --> 00:28:45.760]   I mean, I think I've been asked before with what will happen and I think I said that we'll muddle
[00:28:45.760 --> 00:28:52.720]   through and the language of the decision will be the map by which we figure out how to muddle.
[00:28:52.720 --> 00:28:57.680]   But so it might be that like, you know, if I say the sky is going to fall and then,
[00:28:57.680 --> 00:29:02.080]   you know, we wake up the next day still breathing, then it sounds like I lose my credibility.
[00:29:02.080 --> 00:29:08.080]   But just because we may not fall into the sea that day does not mean that seriously horrific
[00:29:08.080 --> 00:29:13.360]   and shifting tectonics won't have happened to the ecosystem that won't be felt rather
[00:29:13.360 --> 00:29:15.120]   dramatically fairly soon.
[00:29:15.120 --> 00:29:17.600]   Can I ask you a question, Leo?
[00:29:17.600 --> 00:29:18.640]   Me?
[00:29:18.640 --> 00:29:19.680]   Okay.
[00:29:19.680 --> 00:29:20.800]   Yes, sir.
[00:29:20.800 --> 00:29:26.320]   Because I'm curious because because you were open-minded and changed your views on the algorithm,
[00:29:26.320 --> 00:29:29.920]   from what you and you, Kavanaugh said, hey, algorithms are useful.
[00:29:29.920 --> 00:29:31.120]   They tell me what kind of pizza to get.
[00:29:31.120 --> 00:29:31.920]   To start a Thomas.
[00:29:31.920 --> 00:29:32.640]   To start a Thomas.
[00:29:32.640 --> 00:29:33.120]   To start a Thomas.
[00:29:33.120 --> 00:29:33.920]   I was amazed.
[00:29:33.920 --> 00:29:34.400]   Right.
[00:29:34.400 --> 00:29:34.880]   Yeah.
[00:29:34.880 --> 00:29:40.880]   So I was curious since you did change your mind and I'm grateful you did what you thought of the
[00:29:40.880 --> 00:29:46.160]   argument and because the case then spun around the algorithm and promotion.
[00:29:46.160 --> 00:29:48.000]   What did you think about it from that perspective?
[00:29:48.000 --> 00:29:50.880]   The, I'll tell you what, and I bet you, Kathy, you were doing the same thing.
[00:29:50.880 --> 00:29:53.600]   When they got to thumbnails, I was screaming.
[00:29:53.600 --> 00:29:57.120]   I was saying, no, you idiots.
[00:29:57.120 --> 00:29:58.240]   That's content.
[00:29:58.240 --> 00:29:59.760]   That's not Google.
[00:29:59.760 --> 00:30:04.240]   So they somehow, I don't know how this black got sucked into this.
[00:30:05.040 --> 00:30:11.600]   Or Eric Schnapper, but they somehow got a debate over whether thumbnails were considered a recommendation
[00:30:11.600 --> 00:30:14.960]   and a hazard and algorithmic.
[00:30:14.960 --> 00:30:20.000]   When everybody knows the thumbnails just more from the content provider.
[00:30:20.000 --> 00:30:23.280]   Well, so I don't know where to begin on that.
[00:30:23.280 --> 00:30:26.880]   There's so many problems with that whole thumbnail thing.
[00:30:26.880 --> 00:30:27.600]   One of the problems-
[00:30:27.600 --> 00:30:31.040]   That was the only thing that worried me is they did so in a number of times got
[00:30:31.680 --> 00:30:36.320]   sidetracked by really inappropriate or insignificant issues, right?
[00:30:36.320 --> 00:30:38.240]   Well, okay.
[00:30:38.240 --> 00:30:43.600]   The number of problems with this, one of the problems is they're brief didn't talk about thumbnails,
[00:30:43.600 --> 00:30:44.160]   particularly.
[00:30:44.160 --> 00:30:46.240]   They're talked about URLs.
[00:30:46.240 --> 00:30:48.240]   So the thumbnails was kind of new.
[00:30:48.240 --> 00:30:52.240]   They kept saying thumbnails are URLs and it was like, what are you talking about?
[00:30:52.240 --> 00:30:52.960]   Oh, gosh.
[00:30:52.960 --> 00:30:57.120]   Secondly, then they were making a distinction to say that thumbnails were somehow different
[00:30:57.120 --> 00:31:01.600]   than screenshots because if the, their lawyer said at one point, a screenshot would be
[00:31:01.600 --> 00:31:03.200]   totally fine, but not a thumbnail.
[00:31:03.200 --> 00:31:08.880]   And then Lisa Blatt ended up pointing out that a thumbnail was a screenshot and also why are
[00:31:08.880 --> 00:31:10.480]   we talking about these things.
[00:31:10.480 --> 00:31:15.520]   So poor Lisa, I'm eating breakfast, listening to this and I'm screaming like,
[00:31:15.520 --> 00:31:22.560]   I was sitting next to reporters at the way I was seated.
[00:31:22.560 --> 00:31:26.800]   Like the press pen was next to me and I'm kind of listening to one of the lawyers who was sort
[00:31:26.800 --> 00:31:30.720]   of new to this space, kind of listening to this and being like, are you kidding?
[00:31:30.720 --> 00:31:35.600]   Like, and you're losing and you're sitting down like this poor lawyer was really not getting
[00:31:35.600 --> 00:31:39.600]   traction and like, where even lay people were understanding that like, I think there's
[00:31:39.600 --> 00:31:41.440]   something wrong with your argument here.
[00:31:41.440 --> 00:31:46.720]   But they did have an argument that wasn't the brief as applied to a URL where what they were
[00:31:46.720 --> 00:31:49.520]   trying to say and it was a bad idea.
[00:31:49.520 --> 00:31:53.760]   It would have eviscerated to 30, but the core of the argument they were trying to bring forward
[00:31:53.760 --> 00:32:00.000]   is that so 230 works on the pivots on the question of who created the content addition.
[00:32:00.560 --> 00:32:05.920]   If it is the platform who created it, it's their own stuff and you're always responsible for your
[00:32:05.920 --> 00:32:06.720]   own stuff.
[00:32:06.720 --> 00:32:11.600]   But if it was a third party who created it, then the platform is not liable because that would be
[00:32:11.600 --> 00:32:15.840]   a problem if a platform could be liable for all the stuff that they helped intermediate other people
[00:32:15.840 --> 00:32:20.160]   to say because then they could be available to help anybody say anything online.
[00:32:20.160 --> 00:32:27.120]   So the question that they had was they were sort of trying to argue, and I say sort of trying to
[00:32:27.120 --> 00:32:35.280]   argue because this was shifting that when you recommended something, the way you produced
[00:32:35.280 --> 00:32:42.800]   the content of do you want this next somehow amounted to creating content that was now your
[00:32:42.800 --> 00:32:43.680]   content.
[00:32:43.680 --> 00:32:48.880]   And they had this theory with like a URL that like, if you spit back a URL, well, that URL
[00:32:48.880 --> 00:32:50.400]   wasn't created by the user.
[00:32:50.400 --> 00:32:55.680]   You created that URL platform and therefore that is new information that you now own and you
[00:32:55.680 --> 00:32:57.520]   could be potentially liable for.
[00:32:57.520 --> 00:33:02.240]   This was not a distinction that ended up making sense because it would blow up everything.
[00:33:02.240 --> 00:33:04.960]   And the justices were smart.
[00:33:04.960 --> 00:33:05.520]   They got it.
[00:33:05.520 --> 00:33:08.800]   They said, well, how do you have a search engine if you don't have URLs?
[00:33:08.800 --> 00:33:10.400]   Right.
[00:33:10.400 --> 00:33:15.200]   And like, again, it's like, how can you be responsible for creating content by virtue of
[00:33:15.200 --> 00:33:19.520]   displaying content that already exists because somebody else created it?
[00:33:19.520 --> 00:33:19.920]   Yeah.
[00:33:19.920 --> 00:33:25.760]   So yeah, it was even at the best it was argued.
[00:33:25.760 --> 00:33:31.200]   It was a weak argument where it had huge logical problems of the statue would absolutely fall
[00:33:31.200 --> 00:33:32.640]   apart if this was the rule.
[00:33:32.640 --> 00:33:36.560]   And in oral argument, the lawyer was trying to sort of say, no, this is something more
[00:33:36.560 --> 00:33:38.000]   narrow and situational.
[00:33:38.000 --> 00:33:39.600]   It wouldn't apply here would apply there.
[00:33:39.600 --> 00:33:44.720]   But all of a sudden it was just such an esoteric situation they were trying to describe or
[00:33:44.720 --> 00:33:49.040]   kind of actually if they managed to carve out all the ways it wouldn't apply, it also kind
[00:33:49.040 --> 00:33:52.640]   of carved out their whole complaint where it wouldn't apply to their complaint either.
[00:33:52.640 --> 00:33:54.400]   It was it would be a problem.
[00:33:54.400 --> 00:33:57.440]   And what was really fortunate is I think the justices got it.
[00:33:57.440 --> 00:33:59.280]   They saw the number of flaws with this.
[00:33:59.280 --> 00:34:02.240]   I think that's what I'm surprised they don't spell URL.
[00:34:02.240 --> 00:34:07.600]   Alito actually said, this is what these this is the this is what you do.
[00:34:07.600 --> 00:34:11.120]   The algorithm is is integral to it.
[00:34:11.120 --> 00:34:14.640]   I don't understand why they'd be liable for this.
[00:34:14.640 --> 00:34:19.120]   I mean, even Alito, who is I think arguably the most conservative of the judges,
[00:34:19.120 --> 00:34:22.480]   Steve Gibson asked me something interesting yesterday.
[00:34:22.480 --> 00:34:26.880]   He said, wouldn't you be happy that a conservative court was hearing this?
[00:34:26.880 --> 00:34:32.880]   You know, that originalists were hearing this because wouldn't they protect section 230?
[00:34:32.880 --> 00:34:40.080]   What made everybody nervous going into this was all those concurrences and descents that were
[00:34:41.280 --> 00:34:45.280]   not getting to 30 and expressing suspicions about it.
[00:34:45.280 --> 00:34:50.720]   And we thought that that was sort of an ideological declaration and they were looking for the right
[00:34:50.720 --> 00:34:52.720]   vehicle to tear it up.
[00:34:52.720 --> 00:34:55.680]   And I mean, we thought it was a bad idea to tear it up.
[00:34:55.680 --> 00:34:59.040]   We were reading those descents and concurrences and thinking they're wrong,
[00:34:59.040 --> 00:35:01.520]   they're missing it and they're going to cause real damage.
[00:35:01.520 --> 00:35:07.040]   So we were afraid that we're going to get something ideological where they wanted to
[00:35:07.040 --> 00:35:09.520]   blow it up and they were going to find a way to blow it up.
[00:35:09.520 --> 00:35:13.440]   Whatever they want it, whatever they would find a way.
[00:35:13.440 --> 00:35:17.440]   So to find a reason and understanding of what's actually at stake,
[00:35:17.440 --> 00:35:23.120]   which in theory is an appropriate conservative value was confusing, but good.
[00:35:23.120 --> 00:35:29.600]   So all of a sudden I'm answering questions in a world I didn't know existed 48 hours ago.
[00:35:29.600 --> 00:35:30.080]   Yeah.
[00:35:30.080 --> 00:35:35.280]   So Kathy, before, besides being able to read your document, which I bet really did have
[00:35:36.000 --> 00:35:40.400]   impact, it must have felt good to kind of see that they got things right
[00:35:40.400 --> 00:35:43.760]   before oral arguments.
[00:35:43.760 --> 00:35:52.480]   Besides reading all the filings, is there a process of education within the Supreme Court where
[00:35:52.480 --> 00:35:55.440]   they say, hey, Clerk, I need a lesson.
[00:35:55.440 --> 00:36:00.000]   Go find me somebody who can explain it, who would answer my questions before we get in there,
[00:36:00.000 --> 00:36:01.600]   or something like that.
[00:36:01.600 --> 00:36:05.520]   Is there an educational process that they go through in addition to the filings?
[00:36:05.840 --> 00:36:10.880]   I don't think there's anything formal, but one of the problems with the institution and
[00:36:10.880 --> 00:36:18.880]   its perceived credibility these days is the barrier between their jurisprudence and their own
[00:36:18.880 --> 00:36:21.440]   modern lives are seeming incredibly porous.
[00:36:21.440 --> 00:36:24.800]   And well said, Councillor.
[00:36:24.800 --> 00:36:31.520]   And I think there's a lot of concern about the throughput of what information is reaching them.
[00:36:31.520 --> 00:36:37.760]   However, there are some upsides to it where probably some good information is getting through
[00:36:37.760 --> 00:36:40.480]   based on the nature of their lives and who they interact with.
[00:36:40.480 --> 00:36:45.280]   That was my answer to Steve, is these guys are only originalists when it serves their agenda.
[00:36:45.280 --> 00:36:52.960]   And when it doesn't serve their agenda, they can create new ideas and laws as they wish.
[00:36:52.960 --> 00:36:57.280]   Well, they also understood there was one other thing that I thought was important,
[00:36:57.280 --> 00:37:01.040]   and it did show up in my brief and it showed up in some other briefs, which was
[00:37:01.920 --> 00:37:06.640]   even if you hate 230 and can make a better argument than I'm making in its defense for why
[00:37:06.640 --> 00:37:09.680]   it should go away, this is Congress's job.
[00:37:09.680 --> 00:37:14.560]   And there was also a humility that the justices were expressing, which was
[00:37:14.560 --> 00:37:19.520]   this may be wrong, but it's not for us to fix and it belongs to Congress.
[00:37:19.520 --> 00:37:23.360]   And I like to cite and I cited it into my brief the Bostak case, which was like,
[00:37:23.360 --> 00:37:27.440]   I think a civil rights case where Justice Roberts, I think, wrote it.
[00:37:27.440 --> 00:37:33.280]   And he basically looked at it and said, look, we may think that what Congress did makes no sense
[00:37:33.280 --> 00:37:38.080]   is not good policy, but it's what Congress did. And that's what the statute says. And if that's
[00:37:38.080 --> 00:37:42.800]   the wrong policy, it's for Congress to fix. Because also if the court ended up helping itself to
[00:37:42.800 --> 00:37:48.640]   fix the policy and they get it wrong, how do you, you can vote out a member of Congress,
[00:37:48.640 --> 00:37:52.400]   I mean, this is idealistic, but in theory, you can vote out a member of Congress who screws up
[00:37:52.400 --> 00:37:57.680]   the internet, but you can't really, we can't vote out the justices or the judges who screw up the
[00:37:57.680 --> 00:38:02.240]   internet. We're sort of stuck with that. And if it turns out that Congress wrote the best policy,
[00:38:02.240 --> 00:38:08.000]   or at least the policy that the public animating it wanted, and they already wrote it, and the
[00:38:08.000 --> 00:38:13.440]   justices don't let it mean what it says on its face, how does Congress rate one that would,
[00:38:13.440 --> 00:38:18.880]   because they already did and it didn't matter. So the humility that they were sort of like,
[00:38:19.840 --> 00:38:25.520]   you hate it as much as you want, but it's not for us to change what the statute actually says.
[00:38:25.520 --> 00:38:32.000]   They were really trying to do a statutory interpretation and not break it. And I don't think I expected
[00:38:32.000 --> 00:38:37.280]   prior to Tuesday morning that it seemed like the majority of justices did not want to break it,
[00:38:37.280 --> 00:38:40.560]   because they recognized exactly what would break with it. Well, it's the last thing I
[00:38:40.560 --> 00:38:45.840]   thought I'd hear from Justice Kavanaugh, but that was his whole point was, okay, this is for Congress
[00:38:45.840 --> 00:38:51.120]   to do. And I don't remember who it was. It was Justice Kagan who said this could have a huge impact
[00:38:51.120 --> 00:38:56.720]   on the economy and businesses. I mean, they haven't not and Kagan Kagan used like this will open up
[00:38:56.720 --> 00:39:01.920]   a world of lawsuits and then Kavanagh echoed it and talked about the he was, I think, more touched
[00:39:01.920 --> 00:39:07.280]   by the economic impact of it. But basically, I think it is very fair to say that the amicus
[00:39:07.280 --> 00:39:12.480]   brief saved the day, because even in this voluminous thing, which normally is like, well, great,
[00:39:12.480 --> 00:39:17.040]   you know, I know that some justices didn't read mine because it was lost in the flood. But the
[00:39:17.040 --> 00:39:22.800]   effect of the flood was, I think it really woke the court up to the fact of what was at stake.
[00:39:22.800 --> 00:39:29.200]   And I think we did I accidentally got quoted in CNBC today, patting myself on the back for having
[00:39:29.200 --> 00:39:33.920]   written an amicus brief. But I think actually, like, I knew I didn't want to be on the sideline.
[00:39:33.920 --> 00:39:37.360]   If the internet was going to break, I wanted to roll up my sleeves and knew I did everything I
[00:39:37.360 --> 00:39:42.400]   could. And I think, you know, I think I can sleep easy in that sense. I had the right
[00:39:42.400 --> 00:39:46.240]   skill at the right time. And I'm glad I could do that. You did it for us, Kathy. And we are
[00:39:46.240 --> 00:39:51.600]   eternally grateful that you contributed to that flood. Although, as Mike Masnick says,
[00:39:51.600 --> 00:39:57.600]   I still am right reading from his post today. I still very much fear the outcome of this case.
[00:39:57.600 --> 00:40:06.080]   It seems very, very unlikely that Gonzalez wins overall. But there was still an awful lot of
[00:40:06.080 --> 00:40:12.000]   nonsense spouted by the justices, some of which might make it into a final ruling,
[00:40:12.000 --> 00:40:17.200]   where even some minor tiny little misunderstanding could have a massive impact on the future of the
[00:40:17.200 --> 00:40:26.320]   internet. Do you share that concern? Yes, generally, how tuned I am to thinking how likely they'll
[00:40:26.320 --> 00:40:31.040]   break it. I don't know. I'm partly I need to still get through the week. So I don't really,
[00:40:31.040 --> 00:40:36.960]   I'd rather take a more optimistic view. But yeah, the language is going to matter,
[00:40:36.960 --> 00:40:43.760]   because any little dalliance into that dicta of, this is all fine, this is all fine, this is all
[00:40:43.760 --> 00:40:50.400]   fine. But this might not be is not great. I mean, a lot of their hypos were kind of scary, but the
[00:40:50.400 --> 00:40:55.520]   hypos were not the facts and questions. So it's really important not to start adjudicating
[00:40:56.480 --> 00:41:00.960]   hypothetical situations that are not before the court, where they can't see any of the other
[00:41:00.960 --> 00:41:06.080]   ambient facts and interpret them. So I really, really hope they resist that temptation. If they
[00:41:06.080 --> 00:41:13.360]   can write a clean decision that focuses only on this, to say, given what was pled, there is no way
[00:41:13.360 --> 00:41:19.200]   this can go forward given section 230 and like stop there. And I think we're good. There might be
[00:41:19.200 --> 00:41:24.800]   a way where maybe they can even amplify that 230. I mean, I wouldn't mind maybe positive dicta,
[00:41:24.800 --> 00:41:29.600]   but basically dicta is dangerous. I would rather have them be simple.
[00:41:29.600 --> 00:41:36.000]   Congress's intent was clear. And this is clearly in Congress's intent. And no, your, your case is
[00:41:36.000 --> 00:41:40.960]   denied. So I'll give you the pessimistic view. By the way, you're the reason, hold on a second,
[00:41:40.960 --> 00:41:47.040]   the reason Mike is less than confident is today, the Supreme Court decided not to grant cert
[00:41:47.040 --> 00:41:53.840]   to the onion case, and essentially, essentially allowed law enforcement to arrest people for
[00:41:53.840 --> 00:41:56.960]   parity. And also they turned down the Wikimedia.
[00:41:56.960 --> 00:42:04.720]   That was a long shot. I thought that's the one that was saying the NSA's, you know, grab of data
[00:42:04.720 --> 00:42:10.480]   was should have been illegal. There's great concerns about the productivity of the court.
[00:42:10.480 --> 00:42:15.840]   And it's also a big deal. Why if they deny this case is improvidently granted, it's also an
[00:42:15.840 --> 00:42:21.360]   extra bit of egg on their face, which might be why they don't. They are, they, they're not taking
[00:42:21.360 --> 00:42:26.000]   a lot of cases. They're already taking many fewer than normal. They're not hearing as many cases as
[00:42:26.000 --> 00:42:31.920]   they used to. And then the ones that they're taking, if it turns out to be mistakes, that's
[00:42:31.920 --> 00:42:40.320]   really not great because they're not doing much then. And a court that is very deeply focused on a
[00:42:40.320 --> 00:42:44.480]   couple issues like, okay, maybe they really are very invested in
[00:42:44.480 --> 00:42:50.960]   internet free speech issues. And maybe if they're invested in a way that protects a great,
[00:42:50.960 --> 00:42:56.400]   but that can't possibly be the only thing that they're doing and recognizing some of the other
[00:42:56.400 --> 00:43:01.040]   First Amendment issues before them, they really should have the capacity to be able to weigh in,
[00:43:01.040 --> 00:43:05.680]   but they may have felt they have a big dance card already because they've got the Warhol case,
[00:43:05.680 --> 00:43:09.200]   which is fair use that touches on the First Amendment. They've got the 303 creative,
[00:43:09.200 --> 00:43:13.760]   which is the First Amendment case about what webmasters can do. And now they've got this.
[00:43:13.760 --> 00:43:17.680]   And then you've also got the net choice cases waiting in the wings for a grant of
[00:43:17.680 --> 00:43:23.760]   certiority. And that's not going to happen this term. So I don't know, like, they could be useful.
[00:43:23.760 --> 00:43:28.080]   I almost felt like they're heroes to the internet. Like if Kavanaugh writes these decisions,
[00:43:28.080 --> 00:43:32.240]   I think we might be in good shape. But if that's the only thing they're doing and a
[00:43:32.240 --> 00:43:35.600]   sea of other things going horribly wrong, we still have a problem.
[00:43:35.600 --> 00:43:38.880]   All right. Now, can I give you my my? Yes. Sorry. I mean,
[00:43:38.880 --> 00:43:40.880]   the rough, but I just wanted to say no.
[00:43:42.320 --> 00:43:49.760]   So here's a disaster scenario in which the Supreme Court says 230, Dandy, law, fine shape, keep it.
[00:43:49.760 --> 00:43:55.360]   We think it's great. And then that only gives more ammunition to Congress and the executive,
[00:43:55.360 --> 00:43:59.680]   who are ganging up in a Pinser movement right now left and right against tech companies and
[00:43:59.680 --> 00:44:06.640]   against 230 to say one, well, then we've got to do something and repeal 230. And nobody likes
[00:44:06.640 --> 00:44:11.600]   the Supreme Court anyway. So fine. We'll just go against them. And we end up in worse shape,
[00:44:11.600 --> 00:44:15.280]   legislatively, than even the court was going to put us in.
[00:44:15.280 --> 00:44:20.880]   Look, I'm sitting here, treating Kavanaugh, just as Kavanaugh is my hero here.
[00:44:20.880 --> 00:44:22.240]   Because he understands the best.
[00:44:22.240 --> 00:44:23.200]   That's how bad it is.
[00:44:23.200 --> 00:44:26.160]   And I'm like rooting for him to write these decisions because I think he'll actually write
[00:44:26.160 --> 00:44:26.160]   good.
[00:44:26.160 --> 00:44:27.920]   That's all by him. A beer, Kathy.
[00:44:27.920 --> 00:44:30.800]   What do I do? Try to find out this? Do you like it?
[00:44:30.800 --> 00:44:37.440]   How do I make the argument that this guy actually got something right and we should pay
[00:44:37.440 --> 00:44:42.560]   attention to it and that he's right and serious and credible? You're not wrong, but I'm not
[00:44:42.560 --> 00:44:46.560]   conceding that. I also think that we-
[00:44:46.560 --> 00:44:50.800]   I think it's the people that merits what they say and how they lay that out. And depending on
[00:44:50.800 --> 00:44:55.920]   also what they do say, there's a possibility that it can get a robust enough defense that
[00:44:55.920 --> 00:45:00.880]   it could ship the politics. And the politics are so screwy because there's screwy on the left
[00:45:00.880 --> 00:45:04.640]   and there's screwy on the right. If it haven't, it's the decision they make the politics on the
[00:45:04.640 --> 00:45:09.520]   left-worths, but they might actually help it on the right. So I don't know. I'll play out that
[00:45:09.520 --> 00:45:10.880]   hand when it's dull.
[00:45:10.880 --> 00:45:15.840]   Well, the very fact that the Ninth Circuit lab this to continue is that sign. I mean,
[00:45:15.840 --> 00:45:19.440]   the fact that the Supreme Court's hearing this case at all is really kind of-
[00:45:19.440 --> 00:45:25.200]   Well, we thought so. And now I think everybody's sort of regretting it and saying that maybe they-
[00:45:25.200 --> 00:45:29.520]   I think they were aware that they may have gotten it wrong. But I don't know, maybe they really
[00:45:29.520 --> 00:45:34.320]   want to set a- I don't know. It's possible that we could really have a happy ever after out of
[00:45:34.320 --> 00:45:38.160]   this, but it's who am I kidding? This is the 21st century.
[00:45:38.160 --> 00:45:42.880]   Whatever you do, don't write a parody post on Facebook about the local law officials because
[00:45:42.880 --> 00:45:50.480]   they can arrest you for that. There's going to be another arrest and then there'll be another legal
[00:45:50.480 --> 00:45:55.120]   challenge. And at some point, these things do eventually percolate up where they get the review
[00:45:55.120 --> 00:45:56.880]   that they need. You are an optimist.
[00:45:56.880 --> 00:46:04.080]   I'm being very optimistic, but I need a nap. And if I start thinking more realistically about
[00:46:04.080 --> 00:46:05.680]   this, I'm just full apart right now.
[00:46:05.680 --> 00:46:11.920]   Yeah. Well, we got to take the little victories that we can get to. And you know what?
[00:46:11.920 --> 00:46:15.920]   I am completely willing to say somebody's been rehabilitated by their time on the court and
[00:46:15.920 --> 00:46:20.480]   their exposure to other ideas and pat them on the back.
[00:46:20.480 --> 00:46:21.840]   We'll go overboard here, Leo.
[00:46:21.840 --> 00:46:23.360]   [laughter]
[00:46:23.360 --> 00:46:27.520]   I mean, we always hope that, right? We always hope they'll rise to the occasion.
[00:46:27.520 --> 00:46:29.040]   They so rarely do that-
[00:46:29.040 --> 00:46:30.800]   Don't forget what we know.
[00:46:30.800 --> 00:46:35.760]   No, I know. I know. Please don't remind me.
[00:46:35.760 --> 00:46:38.960]   And Kathy, we're living in our little world right now, okay?
[00:46:38.960 --> 00:46:42.720]   Only happy thoughts.
[00:46:42.720 --> 00:46:44.640]   Happy thoughts.
[00:46:44.640 --> 00:46:46.240]   Happy thoughts.
[00:46:46.240 --> 00:46:50.480]   Let me take a little break. And Kathy, if you want to get a beer, go right in.
[00:46:50.480 --> 00:46:50.800]   Great.
[00:46:50.800 --> 00:46:53.680]   I got to get through the rest of the show.
[00:46:53.680 --> 00:46:54.160]   Okay.
[00:46:54.160 --> 00:46:54.800]   I'm going to be my dad.
[00:46:54.800 --> 00:46:58.080]   I am so thrilled that you're here, Kathy.
[00:46:58.080 --> 00:46:58.880]   Oh, yes.
[00:46:58.880 --> 00:46:59.360]   Yes.
[00:46:59.360 --> 00:47:01.280]   This is what Kathy does for a living.
[00:47:01.280 --> 00:47:07.040]   She wrote an amicus brief that I'm sure some of the justices read, if not all,
[00:47:07.040 --> 00:47:08.000]   writes a tech dirt.
[00:47:08.000 --> 00:47:13.600]   Her law offices are at cgcouncil.com, C-O-U-N-S-E-L.
[00:47:13.600 --> 00:47:14.800]   .com.
[00:47:14.800 --> 00:47:21.920]   And the fact that she was in the room where it happens yesterday is such a boon for us.
[00:47:21.920 --> 00:47:23.600]   We're really grateful to you.
[00:47:23.600 --> 00:47:24.240]   I'm listening.
[00:47:24.240 --> 00:47:26.880]   I didn't know you were there when I'm listening, but I'm listening.
[00:47:26.880 --> 00:47:28.720]   You know, because they don't do video.
[00:47:28.720 --> 00:47:33.120]   They just do audio and you're trying to read the tea leaves and really listen.
[00:47:33.120 --> 00:47:38.960]   And I want, I said, we got to get Kathy on because I got to understand what I just heard.
[00:47:38.960 --> 00:47:40.080]   So thank you for being here.
[00:47:40.080 --> 00:47:41.040]   We really appreciate it.
[00:47:41.040 --> 00:47:42.480]   Good to be here.
[00:47:42.480 --> 00:47:43.200]   Yeah.
[00:47:43.200 --> 00:47:44.320]   It's good you're here too, Jeff.
[00:47:44.320 --> 00:47:45.360]   So are you.
[00:47:45.360 --> 00:47:49.280]   You know, I love you.
[00:47:49.280 --> 00:47:52.160]   And I mean, it's nice that Ant is there.
[00:47:52.160 --> 00:47:52.880]   Love you too, sir.
[00:47:52.880 --> 00:47:53.440]   Love you too.
[00:47:53.440 --> 00:47:55.120]   I'm just teasing Jeff.
[00:47:55.120 --> 00:47:56.000]   Jeff and I are.
[00:47:56.000 --> 00:47:57.120]   We're like brothers.
[00:47:57.120 --> 00:47:57.440]   Okay.
[00:47:57.440 --> 00:47:59.200]   We just, we have a little thing.
[00:47:59.200 --> 00:47:59.920]   Yeah, a little thing.
[00:47:59.920 --> 00:48:01.360]   Our show.
[00:48:01.360 --> 00:48:05.120]   Our show today is brought to you by like the Howard Stern show.
[00:48:05.120 --> 00:48:05.520]   I'm sorry.
[00:48:05.520 --> 00:48:08.800]   It's like, it's like, you know, we were you bust the gonads of the people.
[00:48:08.800 --> 00:48:10.320]   I know the people you love the most.
[00:48:10.320 --> 00:48:10.800]   The show.
[00:48:10.800 --> 00:48:11.760]   People you'll love the most.
[00:48:11.760 --> 00:48:12.720]   Yeah.
[00:48:12.720 --> 00:48:12.720]   Yeah.
[00:48:12.720 --> 00:48:12.960]   Yeah.
[00:48:12.960 --> 00:48:15.760]   And I just interrupted the commercial, which is going to get me.
[00:48:15.760 --> 00:48:16.320]   I'm going to get me.
[00:48:16.320 --> 00:48:17.920]   Are you my Baba Bowie?
[00:48:17.920 --> 00:48:18.880]   Who are you?
[00:48:18.880 --> 00:48:19.680]   I think I am.
[00:48:19.680 --> 00:48:20.160]   I think I am.
[00:48:20.160 --> 00:48:21.360]   Well, Jason could be.
[00:48:21.360 --> 00:48:22.320]   Well, I don't know.
[00:48:22.320 --> 00:48:23.440]   Well, I'm definitely not.
[00:48:23.440 --> 00:48:24.400]   Robin Quivis.
[00:48:24.400 --> 00:48:27.280]   Yeah.
[00:48:27.280 --> 00:48:28.880]   Actually, Jeff is my Robin Quivers.
[00:48:28.880 --> 00:48:30.480]   Actually, that's exactly who Jeff is.
[00:48:30.480 --> 00:48:30.960]   Yeah.
[00:48:30.960 --> 00:48:32.320]   Well, I don't laugh at you enough for that.
[00:48:32.320 --> 00:48:34.320]   No, you need to laugh more if you don't mind.
[00:48:34.320 --> 00:48:35.120]   Sorry, now you can go to your work.
[00:48:35.120 --> 00:48:35.600]   I'm sorry.
[00:48:35.600 --> 00:48:35.760]   I'm sorry.
[00:48:35.760 --> 00:48:35.920]   I'm sorry.
[00:48:35.920 --> 00:48:42.160]   Our show today brought to you by HPE GreenLake.
[00:48:42.160 --> 00:48:45.200]   Orchestrated by the experts at CDW.
[00:48:45.200 --> 00:48:47.200]   Oh, GreenLake is an amazing technology.
[00:48:47.200 --> 00:48:48.880]   They help people at CDW.
[00:48:48.880 --> 00:48:53.040]   Understand, look, your business, you need simple management.
[00:48:53.520 --> 00:48:55.280]   Over your big data, right?
[00:48:55.280 --> 00:48:59.680]   We all got big data, but with some needing to keep their workloads on-prem,
[00:48:59.680 --> 00:49:03.920]   whether it's for organizational or compliance requirements,
[00:49:03.920 --> 00:49:07.360]   it can really be challenging to organize and optimize your data.
[00:49:07.360 --> 00:49:12.960]   That's where CDW can help your organization by consolidating and managing all your data
[00:49:12.960 --> 00:49:19.360]   in one flexible, unified experience with the HPE GreenLake Edge to Cloud Platform.
[00:49:19.360 --> 00:49:21.840]   The experience you'll get with HPE GreenLake is unique,
[00:49:21.840 --> 00:49:24.800]   because no matter where your data or applications live,
[00:49:24.800 --> 00:49:29.360]   you could free up energy and resources with automated processes,
[00:49:29.360 --> 00:49:31.120]   streamlined management.
[00:49:31.120 --> 00:49:33.600]   I mean, everybody loves streamlining, right?
[00:49:33.600 --> 00:49:37.120]   Not only that HPE GreenLake creates a seamless Cloud experience
[00:49:37.120 --> 00:49:39.760]   among multiple data environments.
[00:49:39.760 --> 00:49:43.920]   Thanks to the as-a-service model, that meets your remote workforce at the edge.
[00:49:43.920 --> 00:49:48.560]   And with unrivaled scalability, you'll see an instant increase in capacity,
[00:49:48.560 --> 00:49:52.160]   allowing for greater flexibility and accelerated business growth.
[00:49:52.160 --> 00:49:56.240]   So your team can tackle bigger priorities like innovation.
[00:49:56.240 --> 00:50:03.040]   When you need to get more out of your technology, HPE makes data transformation possible.
[00:50:03.040 --> 00:50:05.760]   CDW makes it powerful.
[00:50:05.760 --> 00:50:12.080]   Learn more at cdw.com/hpe.
[00:50:12.080 --> 00:50:18.080]   We thank you so much for supporting our show.
[00:50:18.080 --> 00:50:22.640]   One of the things that perturbed me a little bit
[00:50:22.640 --> 00:50:29.200]   was that the government, the administration, wrote a brief in favor of Gonzalez.
[00:50:29.200 --> 00:50:37.120]   So you've got Senators Hawley and Ted Cruz on one side and President Biden on the other side.
[00:50:37.120 --> 00:50:38.320]   Yeah, that's my fear.
[00:50:40.320 --> 00:50:44.560]   What did you think of the President's brief, or I shouldn't say the President's but the government's brief?
[00:50:44.560 --> 00:50:53.920]   Deeply concerned, but also a little confused because they wrote in support of Twitter
[00:50:53.920 --> 00:50:57.840]   in the Tamna case, and that was a good brief.
[00:50:57.840 --> 00:51:07.040]   But today, in oral argument, they were taking, I thought, a much more profound position in favor
[00:51:07.040 --> 00:51:12.880]   of Twitter, which was a straightforward interpretation of the ATA, which for a government that wants
[00:51:12.880 --> 00:51:18.320]   to go after terrorists, I thought, was correct, but interesting politics.
[00:51:18.320 --> 00:51:24.160]   So today's argument was a good one by the government, where as a citizen,
[00:51:24.160 --> 00:51:26.080]   I am happy that the government took this position.
[00:51:26.080 --> 00:51:30.240]   I think it was tempered and appropriate and doesn't in any way compromise our ability
[00:51:30.240 --> 00:51:35.520]   to fight terrorists and was the brave and intellectually correct decision to take.
[00:51:35.520 --> 00:51:44.560]   But between the Gonzalez decision and also the brief that the United States did in the Warhol case,
[00:51:44.560 --> 00:51:51.760]   there is a really frightening lack of respect for the free speech principles that stake in these
[00:51:51.760 --> 00:51:56.800]   cases. And that is not something where me sitting here as a citizen is happy to hear the President
[00:51:56.800 --> 00:52:03.760]   of the United States support. I don't think actually as his administration, he necessarily gets it.
[00:52:03.760 --> 00:52:07.760]   I don't think he's ever really gotten what the interests are with the First Amendment interests
[00:52:07.760 --> 00:52:12.960]   are bound up in IP issues. He may be missing them in terms of internet issues, because he's
[00:52:12.960 --> 00:52:18.800]   echoed the politics of 0.230's bad and needs reform. And I don't think he gets it. So I'm really
[00:52:18.800 --> 00:52:24.080]   distressed that the government is spending resources to do things that will hurt the expressive interests
[00:52:24.080 --> 00:52:30.800]   and rights of Americans, especially if we're also trying to stave off, you know, fascists who would
[00:52:30.800 --> 00:52:36.480]   like to rewrite America in their own particular image. If we're going to be able to defend against
[00:52:36.480 --> 00:52:41.360]   that, we're going to need to be able to talk about it. So compromising our ability to speak
[00:52:41.360 --> 00:52:46.240]   against terrible, unconstitutional things is really important. And it should be priority one
[00:52:46.240 --> 00:52:51.840]   to defend. And if they're not doing it, then the answer to what I think about it is it's bad and
[00:52:51.840 --> 00:52:56.000]   a huge problem. And I'm deeply concerned about the brief that they're going to produce for the
[00:52:56.000 --> 00:53:01.760]   net choice cases, either in support or against the recommendation that the Supreme Court grants
[00:53:01.760 --> 00:53:07.840]   cert on it. On the other hand, the Tamna argument was actually pretty good. So I'm not quite sure
[00:53:07.840 --> 00:53:11.200]   what's going on with the government, but I have concerns.
[00:53:11.200 --> 00:53:20.560]   The Tamna case, the plaintiffs want to hold Twitter liable for posting content, radical Islamic
[00:53:20.560 --> 00:53:26.480]   content from ISIS and others. And as a result, the, you know, it was brought by a family of
[00:53:26.480 --> 00:53:32.400]   another victim of a nightclub shooting in Istanbul. And of course, just like the Gonzalez case,
[00:53:32.400 --> 00:53:39.120]   they didn't draw a direct line between Twitter and the killing, but they did want to hold
[00:53:39.120 --> 00:53:44.080]   Twitter responsible for aiding and abetting terrorism. And apparently the judges
[00:53:45.200 --> 00:53:52.880]   saw two possible arguments, one that Twitter couldn't possibly know what was going on
[00:53:52.880 --> 00:54:00.480]   on its services. And the other would be that Twitter didn't, what Twitter did was not considered
[00:54:00.480 --> 00:54:07.360]   substantial assistance along the lines of the law would require, like a bank say, giving money
[00:54:07.360 --> 00:54:16.480]   to a terrorist group. Sotomayor said that you can't really give Twitter a win based on not knowing
[00:54:16.480 --> 00:54:22.400]   what was going on, quote, because willful blindness is something we have said can constitute knowledge.
[00:54:22.400 --> 00:54:29.120]   But on the other hand, I think it seemed like, and I didn't listen to this one yet,
[00:54:29.120 --> 00:54:35.520]   I'll listen tonight, the justices did not think that Twitter's actions rose to the standard of
[00:54:35.520 --> 00:54:40.880]   actually aiding and abetting terrorism. The problem was they were not getting what they needed,
[00:54:40.880 --> 00:54:47.760]   which was, I don't think they had any appetite to give Tamna the win, or Twitter the loss,
[00:54:47.760 --> 00:54:54.880]   but they have to write a decision that explains why Twitter would not be liable under the ATA,
[00:54:54.880 --> 00:55:00.080]   and in theory, to do it in a way that does not blow up the precedent that applied to banks,
[00:55:00.640 --> 00:55:06.320]   which may be a mistake. The cases that decided that the banks could be on the hook might actually
[00:55:06.320 --> 00:55:12.000]   have been decided wrong, and the Twitter lawyer suggested that some analytical mistakes may have
[00:55:12.000 --> 00:55:16.720]   been made in those precedents. But that, somehow they have to thread this needle, and they were
[00:55:16.720 --> 00:55:24.240]   trying to get a coherent, simple test out of the Twitter lawyer and then the government lawyer,
[00:55:24.240 --> 00:55:29.840]   and they weren't getting what they wanted. And the only thing that, where I heard the thing that I
[00:55:29.840 --> 00:55:34.720]   think makes sense, which is that speech is involved because of the speech rates of the platforms and
[00:55:34.720 --> 00:55:38.240]   the speech that they're facilitating, and that this is not a bank, this is not something else,
[00:55:38.240 --> 00:55:43.680]   speech matters. So even if you can't otherwise get a test that would truly differentiate,
[00:55:43.680 --> 00:55:48.560]   the speech becomes the escape valve. But the only time I heard anything along those lines
[00:55:48.560 --> 00:55:52.400]   was from Justice Kavanaugh, who understood it. You're new, you're new.
[00:55:54.960 --> 00:56:01.200]   Kathy, were these Trump, I mean, we're sorry, were these Musk Twitter lawyers or pre-Musque
[00:56:01.200 --> 00:56:07.920]   Twitter lawyers? I believe, free. It was Eric Snapper. It was the same guy who argued the day
[00:56:07.920 --> 00:56:13.360]   before in Gonzalesport guy. Well, he was at the Twitter's lawyer, but Seth Waxman was arguing for
[00:56:13.360 --> 00:56:17.440]   Twitter. And I think, I'm sorry, Snapper was, you're right, I'm sorry, Snapper was for 10.
[00:56:17.440 --> 00:56:23.120]   Yeah. And Seth Waxman, I think, was, I mean, somebody filed a petition for certiori. That
[00:56:23.120 --> 00:56:29.520]   happened pre-Musque. We're not hearing that Musk is necessarily even paying all the lawyers that he
[00:56:29.520 --> 00:56:35.120]   owes for things that Twitter was going on. So I don't know what's going on. And we were deeply
[00:56:35.120 --> 00:56:39.040]   worried, what happens with certiori granted on this is Twitter even going to litigate it.
[00:56:39.040 --> 00:56:44.640]   Like, does Twitter even have a client contact? Does Waxman have a client contact? Like,
[00:56:44.640 --> 00:56:48.800]   okay, he showed up and he argued it. I'm not entirely happy with how he argued, but he showed up and he
[00:56:48.800 --> 00:56:54.240]   did his professional duty. But I've got to think that this is a very strange situation.
[00:56:54.240 --> 00:57:01.280]   Yeah. Lawiring on behalf of Twitter right now is something that makes me deeply nervous.
[00:57:01.280 --> 00:57:05.600]   And I think all the friends that I knew who were lawyers at Twitter are probably not there anymore.
[00:57:05.600 --> 00:57:11.040]   I think it's an ethical morass of how do you, when you're a lawyer, you'll have a job to do,
[00:57:11.040 --> 00:57:15.440]   like, that's responsive to the client, but you've also got ethical duties that define your job.
[00:57:15.440 --> 00:57:20.640]   And I think they may be frequently in conflict if Twitter is your client,
[00:57:20.640 --> 00:57:24.960]   where how do you serve the client's interests, where they have old liabilities that Twitter
[00:57:24.960 --> 00:57:29.600]   may or may not have recruit, they may have new liabilities that that Musk is busy accruing.
[00:57:29.600 --> 00:57:34.160]   How do you advise somebody who's not listening? How do you advise somebody who's not paying?
[00:57:34.160 --> 00:57:39.040]   What a mess. So what if Musk said, I'm sorry, there.
[00:57:40.400 --> 00:57:46.000]   You don't listen and you don't pay. And then the lawyer may not necessarily care about them in
[00:57:46.000 --> 00:57:50.880]   that order. Unless let's point out Waxman used to be a solicitor general. I mean,
[00:57:50.880 --> 00:57:54.640]   I'm sure he has deep respect for the court. Is it possible he's just doing this pro bono and
[00:57:54.640 --> 00:57:58.480]   saying, well, this needs to be argued? I mean, this was we were going to have,
[00:57:58.480 --> 00:58:02.640]   I don't know what would have happened if nobody pursued the case. I mean, I'm not particularly
[00:58:02.640 --> 00:58:06.640]   happy with how we argued it, but I think we would have a separate and unique and rather
[00:58:07.280 --> 00:58:11.840]   scary problem if nobody showed up today. So I guess somebody showed up.
[00:58:11.840 --> 00:58:17.120]   Yeah, or couldn't Musk have also said his contrarian self. No, I'm going to take the other side,
[00:58:17.120 --> 00:58:21.200]   screw Twitter. Well, there's also one of the things Twitter's been taking positions in all
[00:58:21.200 --> 00:58:26.960]   sorts of cases and then Musk has been shooting his mouth off, which is inconsistent with the
[00:58:26.960 --> 00:58:32.960]   defenses that he has inherited and really needs to have prevail or else he will be on the hook
[00:58:32.960 --> 00:58:38.640]   for the liability that people have accused Twitter. He took on the liabilities. So if Twitter loses
[00:58:38.640 --> 00:58:43.680]   some of the cases that are pending, he's going to be the one writing the checks. So his mouth
[00:58:43.680 --> 00:58:48.640]   thing off is not doing him and himself any favor. How does this work? So, so,
[00:58:48.640 --> 00:59:02.480]   so Waxman has to have, does he have to have consultation with Twitter under the new regime,
[00:59:02.480 --> 00:59:08.000]   even though he was presumably hired by and the suit was certainly filed by the old Twitter,
[00:59:08.000 --> 00:59:12.720]   but he can't continue to represent Twitter without Elon Musk.
[00:59:12.720 --> 00:59:18.560]   Twitter's been fruitful. Yeah, right. I haven't full, well, I'm speculating, which isn't useful,
[00:59:18.560 --> 00:59:22.400]   because also I haven't done the full research of all the ethical things, but it's not really
[00:59:22.400 --> 00:59:26.320]   supposed to work this way. And I don't know what's happened, but in theory, maybe the right
[00:59:26.320 --> 00:59:30.400]   conversations were happening. So it's probably not helpful to speculate and presume that the
[00:59:30.400 --> 00:59:35.360]   right conversations didn't happen. It's just we're a little more on a razor's edge. This is not
[00:59:35.360 --> 00:59:40.080]   business as usual. This is not the way things that unfold. It's very weird, isn't it? Yeah.
[00:59:40.080 --> 00:59:43.360]   I mean, we didn't know what would happen with the change of ownership. Normally, the change of
[00:59:43.360 --> 00:59:48.480]   ownership is a lot more rational anyway, where you don't really expect that changing horse's
[00:59:48.480 --> 00:59:52.480]   midstream means that all of a sudden you're crossing the stream in a different direction.
[00:59:52.480 --> 00:59:57.840]   Musk has created that issue. But normally it's sort of like you take on the liabilities. If you
[00:59:57.840 --> 01:00:02.000]   had lawyers in play trying to minimize those liabilities, you don't usually mess up their job.
[01:00:02.000 --> 01:00:06.240]   You're like, keep going, keep going, please, please do your job and minimize these liabilities.
[01:00:06.240 --> 01:00:10.400]   So in theory, that's probably essentially what happened with this case, where you had a lawyer
[01:00:10.400 --> 01:00:16.240]   trying to minimize Twitter's liability and somehow was able to manage to show up today to continue
[01:00:16.240 --> 01:00:22.080]   that exercise. But that hasn't necessarily been true for all the other cases. He's fired lawyers,
[01:00:22.080 --> 01:00:26.160]   he's not paid lawyers, and he's shot off his mouth in certain ways that otherwise undermine
[01:00:26.160 --> 01:00:28.720]   defenses that might have been pending. It's a mess.
[01:00:28.720 --> 01:00:36.880]   I've been searching for any connection between Waxman and Musk. I can't find any.
[01:00:36.880 --> 01:00:42.640]   So there's good thing. Well, but there's, yeah, well, it is or it isn't. I don't know. I mean,
[01:00:42.640 --> 01:00:48.400]   there's no, there's, here's a, here's from CNN will Musk weigh in on today's
[01:00:48.400 --> 01:00:54.160]   scotas case involving Twitter. There is no sign that Musk has been personally involved
[01:00:54.800 --> 01:00:59.920]   in the case, even though the outcome obviously could have implications. I mean, for Twitter's
[01:00:59.920 --> 01:01:07.520]   business and, and its bottom line, I'm sure Elon has an opinion, he hasn't tweeted about it.
[01:01:07.520 --> 01:01:15.680]   Very weird. Don't tell Elon, he may not know. He may not know what's going on, but I don't think
[01:01:15.680 --> 01:01:21.520]   Waxman could ethically argue the case without approval from the owners. Could he?
[01:01:21.520 --> 01:01:26.000]   I mean, it's so, I don't know quite what I'd have to think about it. The brief
[01:01:26.000 --> 01:01:32.160]   associated already been filed. So if it wasn't going to be abandoned, and it may have required
[01:01:32.160 --> 01:01:39.360]   something affirmative to abandon it, but I don't quite know. I have to, I think I'm assuming there
[01:01:39.360 --> 01:01:43.920]   was enough buy in that he could ethically show up and say he was truly representing the interests
[01:01:43.920 --> 01:01:48.400]   of his client, but it may have been the barest of minimum of things to tie him to.
[01:01:49.120 --> 01:01:53.360]   But I don't know what the options would be. It is really weird for all of a sudden,
[01:01:53.360 --> 01:01:56.560]   you know, you don't get certiary very much. So it's really not something you
[01:01:56.560 --> 01:02:02.720]   never mind. Right. Yeah. No, that's weird. And it would have broke it. Like what would have
[01:02:02.720 --> 01:02:07.600]   happened to the ecosystem if all of a sudden that wasn't pursued. So anyway, it got pursued. Somebody
[01:02:07.600 --> 01:02:12.720]   showed up, somebody argued it. I think we'll survive. I don't know how cleanly we'll survive.
[01:02:12.720 --> 01:02:15.280]   To knock some wood, please, Kathy, knock some wood for me.
[01:02:15.280 --> 01:02:16.960]   I don't think I have for my guy. I don't.
[01:02:16.960 --> 01:02:19.920]   Right behind you. You got that phrase. Go ahead and knock my frame.
[01:02:19.920 --> 01:02:22.240]   I can't read. I won't be like this.
[01:02:22.240 --> 01:02:25.840]   The one of three inches. The internet.
[01:02:25.840 --> 01:02:33.680]   You know, I hadn't really thought about all of that until you brought it up. It's an interesting
[01:02:33.680 --> 01:02:37.520]   question. We live in interesting times, don't we? And you're right. I mean, Elon,
[01:02:37.520 --> 01:02:42.800]   I mean, I don't think Elon loves terrorists. He might think that, you know, maybe maybe
[01:02:42.800 --> 01:02:50.080]   Matt is what he likes. Russia is just fine. Yeah. Maybe where he draws his lines may not be the
[01:02:50.080 --> 01:02:58.160]   place where you and I would draw. That's right. That's right. Have we done this to death? Is there
[01:02:58.160 --> 01:03:03.760]   anything more to say about this? We just have to wait till the decisions appear sometime this year?
[01:03:03.760 --> 01:03:09.920]   We may stumble on something, but yeah, I mean, there's lots to talk about, but it's sort of like,
[01:03:11.040 --> 01:03:15.040]   yeah, at a certain point, like the more we're speculating, the less useful it is.
[01:03:15.040 --> 01:03:20.480]   Yeah, I don't want to speak. We need more data. Yeah. But fascinating. And if you haven't ever
[01:03:20.480 --> 01:03:24.960]   listened to Supreme Court oral arguments, they're always interesting. These in particular, I guess
[01:03:24.960 --> 01:03:30.400]   maybe because we had a dog in that hunt. But and I don't know nowadays where you listen to them.
[01:03:30.400 --> 01:03:36.240]   I used to go to OIA, OIA, I don't know. I think they're actually on the Supreme Court site as it is.
[01:03:37.120 --> 01:03:43.440]   And then other sites may like scotus blog may refer to them. CNN was the streamer. So they
[01:03:43.440 --> 01:03:51.840]   may have them. Yeah, I listen to C spans stream and. Yeah. So interesting. All right. So you can go
[01:03:51.840 --> 01:03:56.960]   to the Supreme Court website and listen for yourself. Tell us what you think.
[01:03:56.960 --> 01:04:04.480]   It takes a lot of time as a caution. Each of these ran nearly three hours. Three hours. Yeah.
[01:04:04.480 --> 01:04:09.280]   Yeah, it was long. And I think today's was a lot longer than anybody, including the justices.
[01:04:09.280 --> 01:04:12.160]   So you showed up at what time and got out at what time, Kathy?
[01:04:12.160 --> 01:04:21.360]   Well, I didn't today. Yesterday I showed up at about 535 in the morning. And I got spat out at about
[01:04:21.360 --> 01:04:27.120]   1245, I think. So if you're asking why I didn't necessarily go today, that's why.
[01:04:27.120 --> 01:04:31.360]   Yeah, it's exhausting. Plus you you have a you have a yeah, right.
[01:04:31.360 --> 01:04:35.280]   Like play just play if you've never heard this. I could play a little bit of it.
[01:04:35.280 --> 01:04:40.400]   It won't take a snapper. Mr Chief Justice in May, it pleases the court.
[01:04:40.400 --> 01:04:45.040]   Section 230 C one distinguishes between claims that seek to hold.
[01:04:45.040 --> 01:04:46.720]   And this is the attorney for Gonzalez.
[01:04:46.720 --> 01:04:52.720]   Internet company liable for content created by someone else and claims based on the company's
[01:04:52.720 --> 01:04:57.680]   own conduct that distinction is drawn in each of the three sections of the statute.
[01:04:58.240 --> 01:05:03.760]   First section 230 C one. So I'll give you a little bit. I'll jump ahead and hear the justices
[01:05:03.760 --> 01:05:09.360]   question about the thumbnails and going to others. Here's Elena Kagan.
[01:05:09.360 --> 01:05:13.040]   Active collusion because there has to be a line somewhere in between.
[01:05:13.040 --> 01:05:23.360]   It can't be merely because you're a computer person that you can create an algorithm that
[01:05:23.360 --> 01:05:27.040]   discriminates against people. You have no problem with that, right?
[01:05:27.040 --> 01:05:34.000]   The writing of the algorithm would probably constitute aiding and abetting.
[01:05:34.000 --> 01:05:39.440]   Exactly. If you write one that discriminated against people for a.
[01:05:39.440 --> 01:05:42.400]   How does it work, Kathy? They have a certain amount of time.
[01:05:42.400 --> 01:05:45.520]   Or how does that work? Is there a light?
[01:05:45.520 --> 01:05:52.400]   It's changed. And I think we're all figuring it out. It used to be like kind of the way
[01:05:52.400 --> 01:05:58.720]   an oral argument works at the circuit courts where you're allocated like 15 minutes or 20 minutes
[01:05:58.720 --> 01:06:02.720]   or something like that. And you have some lights in front of you and they kind of,
[01:06:02.720 --> 01:06:06.400]   it's green when you got lots of time. Then it's like yellow at a two minute warning.
[01:06:06.400 --> 01:06:09.840]   And then read when it's like you finish your sentence and sit down unless the judge tells you
[01:06:09.840 --> 01:06:13.360]   to keep going. Just like all the other comedians in the country.
[01:06:13.360 --> 01:06:15.200]   Get the hook.
[01:06:15.200 --> 01:06:20.400]   But how about the justices? They take turns. In fact, it looked like they rotated through.
[01:06:21.280 --> 01:06:26.960]   So they changed things up when we went to remote because they kind of had to.
[01:06:26.960 --> 01:06:32.240]   And the way they started changing it and handling their own questions internally
[01:06:32.240 --> 01:06:36.160]   shifted things. I mean, one of the things that it shifted is now,
[01:06:36.160 --> 01:06:39.680]   just as Thomas opens his mouth, he never asked questions.
[01:06:39.680 --> 01:06:42.240]   I know he was very active. Yeah.
[01:06:42.240 --> 01:06:48.800]   In this new format, he asks questions and it's a very significant transformation because he really
[01:06:48.800 --> 01:06:53.360]   didn't believe in them. But now he does, I guess. And he seems to like asking
[01:06:53.360 --> 01:07:00.000]   some interesting and hypo's or their very colorful hypo's. I think he actually likes to hear
[01:07:00.000 --> 01:07:05.840]   himself tell these hypo's. But he may also care about the answers as well.
[01:07:05.840 --> 01:07:09.520]   But that sort of changed it. But I don't fully understand the new system.
[01:07:09.520 --> 01:07:14.800]   So I think the new system is every lawyer, when they start gets, I think, two minutes of silence,
[01:07:14.800 --> 01:07:17.600]   where they get to make their pitch and nobody's going to interrupt them.
[01:07:17.600 --> 01:07:22.000]   And then it turns into a hot bench. But I don't know if it's an unfettered hot bench
[01:07:22.000 --> 01:07:26.960]   where everybody asks their questions or if they go through in series or actually it may be some
[01:07:26.960 --> 01:07:32.320]   combination of both. Because you can kind of hear at the end, Justice Roberts go through the order
[01:07:32.320 --> 01:07:36.320]   of seniority, like anything else, Justice Thomas, anything else, Justice Alito, anything else.
[01:07:36.320 --> 01:07:42.080]   I think that's towards the end. And eventually when a Peter's out that there's no more questions
[01:07:42.080 --> 01:07:45.520]   than they're done. But you're a hot bitch. I like that. This show was a hot bench.
[01:07:46.320 --> 01:07:50.880]   Hot bench is a term that I've learned in law school. It describes if you're doing oral argument
[01:07:50.880 --> 01:07:55.520]   and you actually get peppered questions from the judges or justices, that's a hot bench.
[01:07:55.520 --> 01:08:01.440]   And it can be some people don't like it because the argument is going to go the way they take it
[01:08:01.440 --> 01:08:07.040]   as opposed to the way you planned it. But a lot of some lawyers like it a lot because otherwise
[01:08:07.040 --> 01:08:12.240]   you're just sitting there talking into a void and you have absolutely no sign of whether
[01:08:12.240 --> 01:08:15.840]   your arguments are landing or not. So it's kind of it gives you feedback at least.
[01:08:15.840 --> 01:08:22.800]   I did want to and I don't know if I can find it, but I did want to find the part where Justice
[01:08:22.800 --> 01:08:29.600]   Kagan says we're not the nine greatest experts on the internet. And the shame of it is the sound
[01:08:29.600 --> 01:08:34.560]   in the room was not great. So I never heard it like a son of a decide didn't know. There was
[01:08:34.560 --> 01:08:39.520]   there was a lot of laughter. There was laughter. There was stress. Why? Let me circulate in the
[01:08:39.520 --> 01:08:45.920]   video. There was say this site. You can organize it on the basis of what's more trustworthy than
[01:08:45.920 --> 01:08:49.920]   something else. And Justice Gorsuch was on the phone. That might matter.
[01:08:49.920 --> 01:08:55.360]   Justice Sotomayor anything further. So this is where Roberts is going around.
[01:08:55.360 --> 01:09:00.960]   Let's assume we're looking for a line presented with similar videos.
[01:09:00.960 --> 01:09:04.240]   So we're hearing it better than you could hear it.
[01:09:06.160 --> 01:09:10.880]   Yeah, because I don't think I had any amplification. I think I was just hearing the sound organically
[01:09:10.880 --> 01:09:17.280]   as a pastor in the room. So it was hard to hear the the people arguing because their voice is
[01:09:17.280 --> 01:09:22.720]   projected away from me. And I didn't I mean, maybe I'm wrong. Maybe it was amplified, but it
[01:09:22.720 --> 01:09:27.600]   didn't sound like it. It really just sort of sounded like hearing aid stuff. If they have you
[01:09:27.600 --> 01:09:33.920]   could have said you were hard of hearing and gotten an aid. Well, I suppose, but I that didn't
[01:09:33.920 --> 01:09:39.200]   occur to me. But I mean, I could hear most of it, but like small talk or things that went like
[01:09:39.200 --> 01:09:43.600]   more under, especially if they were talking over each other. I was not out of position to
[01:09:43.600 --> 01:09:48.640]   parse that out. Plus, I was also blocked. I was sitting behind people. So I'm sitting behind
[01:09:48.640 --> 01:09:54.800]   taller people and they're just absorbing all the sounds. Is it scary to be an attorney?
[01:09:54.800 --> 01:09:57.840]   You've done this, right? You've have you stood up at the bar?
[01:09:59.280 --> 01:10:07.280]   Not in this court and not at a court of appeal. I've argued motions and yeah, it is sort of terrifying
[01:10:07.280 --> 01:10:10.800]   because you also have to think that your feet. I mean, I'm listening to the, you know,
[01:10:10.800 --> 01:10:16.480]   Tom Thomas proposed some hypothetical and then Eric Snapper's got a, well,
[01:10:16.480 --> 01:10:22.560]   but they're very good. I have to say they obviously know what they're talking about and
[01:10:22.560 --> 01:10:27.360]   they're very good at, I think playing the game of saying, well, I don't want to go down that road
[01:10:27.360 --> 01:10:29.760]   or, you know, I thought it was very interesting.
[01:10:29.760 --> 01:10:36.080]   There are certain things where, I mean, I'm critical of the lawyers and maybe unfairly,
[01:10:36.080 --> 01:10:40.400]   because it is a hard job. You have to know your case backwards and forwards and know all sorts
[01:10:40.400 --> 01:10:44.960]   of things and also have your talking points and also know what you're going to steer around,
[01:10:44.960 --> 01:10:50.640]   where I've been critical of both of the Twitter lawyer and also the Google lawyer was,
[01:10:50.640 --> 01:10:54.880]   you also have to listen and really understand what you're getting back so you can adjust.
[01:10:54.880 --> 01:11:02.560]   And I was disappointed by the lack of adjustment for Justice Jackson, who I think you could
[01:11:02.560 --> 01:11:08.880]   reach her and explain it in a way that made sense, but that was, what was being said was not wrong,
[01:11:08.880 --> 01:11:13.200]   but it was not getting packaged in a way that this was going to be an accessible idea.
[01:11:13.200 --> 01:11:18.640]   She's brand new on the court and perhaps not yet fully acclimated, you think?
[01:11:19.920 --> 01:11:26.800]   I think she's very wary of some of these, like she commented in open court in the
[01:11:26.800 --> 01:11:31.120]   Warhol case that this was an area that she didn't have a lot of expertise and she was new to it.
[01:11:31.120 --> 01:11:36.560]   So I think for these technical areas, she, you know, if they're not something where she's been
[01:11:36.560 --> 01:11:41.760]   imbued for her legal career, she is going to be new at it. And I think, you know,
[01:11:41.760 --> 01:11:46.640]   if you're new to it, you're going to be circumspect. The problem is, is we need her to come up to speed
[01:11:46.640 --> 01:11:53.520]   immediately because the issues are that critical in her taking adjudication right now. And I was,
[01:11:53.520 --> 01:11:58.560]   I was concerned by some of her comments. I mean, things like she kept saying it's a narrow statute
[01:11:58.560 --> 01:12:02.880]   and please, please, please read my brief where I have a whole section on it's a broad statute,
[01:12:02.880 --> 01:12:08.320]   but also to listen to her colleagues. And I hope she does because like she's sitting,
[01:12:08.320 --> 01:12:13.360]   I think next to Justice Kavanaugh and he's talking about it's a broad statute. I think he used that
[01:12:13.360 --> 01:12:17.760]   word. And then for him to say it's broad and for her to say it's narrow, wait, hang on a second,
[01:12:17.760 --> 01:12:23.280]   like if you're disagreeing amongst yourselves, can you hopefully hash this out under yourselves
[01:12:23.280 --> 01:12:26.800]   so you can figure out is it broad or narrow and, you know, to the extent that that's going to
[01:12:26.800 --> 01:12:32.880]   matter to their adjudication? I don't think both views are correct. But, you know, so something like
[01:12:32.880 --> 01:12:40.000]   that where can you, but sometimes they ask the questions because, you know, a lot of the questions
[01:12:40.000 --> 01:12:44.720]   were framed as essentially if we're going to, if you're going to win and we're going to write this
[01:12:44.720 --> 01:12:50.080]   decision, how do we write it? And so a lot of the questions were, I think they were very much
[01:12:50.080 --> 01:12:54.880]   telegraphing of help the other side to win. We want this, but you've got to, you've got to give us
[01:12:54.880 --> 01:13:00.720]   the answers. And some of that I think got punted. I think it got more punted today than yesterday,
[01:13:00.720 --> 01:13:05.440]   but even yesterday, like agreeing to the Henderson test, I think was potentially a mistake.
[01:13:06.640 --> 01:13:12.960]   You're sure was your filing again? I have it here. Oh, no, I just want to see. Oh, that's cute. So
[01:13:12.960 --> 01:13:18.560]   you got it in a little pamphlet. They have to be. This is the format that it becomes a big problem
[01:13:18.560 --> 01:13:23.680]   because it's very prescriptive in how you file and how you file briefs at the court,
[01:13:23.680 --> 01:13:28.560]   which makes them very expensive. So it affects it becomes an access to justice issue because
[01:13:28.560 --> 01:13:32.400]   it's not enough to just get the lawyer whom I do a pro bono. You've got to get somebody who's
[01:13:32.400 --> 01:13:38.560]   going to be able to pay easily $1,000, potentially $3,000 or more if you've got a big record to get
[01:13:38.560 --> 01:13:43.920]   these things in the door. And it's a problem. But it was weird because I've written another
[01:13:43.920 --> 01:13:56.320]   room where. Go ahead. There was a we. Okay, there was a, there was a, when we had the thing for
[01:13:56.320 --> 01:14:01.120]   the Texas case where there was an emergency petition to the US Supreme Court and we were
[01:14:01.120 --> 01:14:06.240]   writing, it was a shadow docket request for relief from the Supreme Court. I wrote an
[01:14:06.240 --> 01:14:12.400]   amicus brief in support of the relief. And because that's not an official type of action before the
[01:14:12.400 --> 01:14:17.200]   court, there were no rules. So we set that on eight and a half by 11 white paper. So it was,
[01:14:17.200 --> 01:14:23.200]   it's weird. It's like the things with the rules become barriers, but the things with no rules,
[01:14:23.200 --> 01:14:26.080]   it's it's a company. How many copies do you send? You send nine copies?
[01:14:27.120 --> 01:14:32.800]   I have your printer to do this. So I forget, no, they send like 40 some odd, I think. So all
[01:14:32.800 --> 01:14:38.720]   the clerks have to get a copy. Here, here is the one of the highlights anyway. That's my concern
[01:14:38.720 --> 01:14:45.360]   is I could imagine the world where you write that none of this stuff gets protection. And, you know,
[01:14:45.360 --> 01:14:50.960]   every other industry has to internalize the costs of misconduct. Why is it that the tech industry
[01:14:50.960 --> 01:14:57.520]   gets a pass? A little bit unclear. On the other hand, I mean, we're a court. We really don't know
[01:14:57.520 --> 01:15:03.120]   about these things. You know, these are not like the nine greatest experts on the internet.
[01:15:03.120 --> 01:15:11.360]   I like what she said though, every other business has to internalize the costs of these things. So
[01:15:11.360 --> 01:15:17.760]   it there, you know, just because it's costly doesn't mean you shouldn't do it. Of course,
[01:15:17.760 --> 01:15:21.120]   it's costly to Google. What means a different thing than it's costly to Leo.
[01:15:21.120 --> 01:15:25.760]   Yeah, existentially costly is different than just expensive. Yeah. Yeah.
[01:15:25.760 --> 01:15:31.600]   Anyway, I love that moment. And I quite enjoyed listening to the arguments. Encourage you all
[01:15:31.600 --> 01:15:37.920]   to do that. Well, they've they've decided not to get the balloon. Another balloon was shot down
[01:15:37.920 --> 01:15:43.760]   today, by the way. This was probably yeah, this is a bad idea. So NORAD for the longest time,
[01:15:43.760 --> 01:15:52.880]   ignored these slow moving, floating out. Santa Santa's dude. Yeah. They, uh, they after the Chinese
[01:15:52.880 --> 01:15:56.160]   balloon was shot down in North Carolina and said, you know, maybe we should be paying attention to
[01:15:56.160 --> 01:16:02.480]   these slow moving objects. Now they've shot down four. And it's not at all clear that what they're
[01:16:02.480 --> 01:16:07.280]   shooting down are anything from China. In fact, they don't think it is. But the problem is this,
[01:16:07.280 --> 01:16:11.600]   this little balloon that was shot down over the Yukon, they can't get it because it's in the,
[01:16:11.600 --> 01:16:19.360]   you know, snowy and it's cold. And so they dispatched two F 22 Raptors. These are 300 million
[01:16:19.360 --> 01:16:27.040]   dollar plus airplanes, uh, cost $70,000 an hour to fly to chase down this balloon. Uh,
[01:16:27.040 --> 01:16:32.400]   did they got it? We got them with a $40,000. Sidewinder missile.
[01:16:32.400 --> 01:16:40.640]   Now they think in this balloon might have actually been a, a mylar, little mylar thing from the
[01:16:40.640 --> 01:16:49.680]   North Illinois bottle cap balloon brigade. They are hobbyists who launched little balloons,
[01:16:49.680 --> 01:16:55.440]   just to like go around the world. This balloon had been around for 123 days. Uh, had circled the
[01:16:55.440 --> 01:17:09.040]   earth six times. Uh, was last seen flying towards Alaska. Um, and, uh, then on February 11th, uh,
[01:17:09.920 --> 01:17:16.080]   disappeared. They haven't heard from it since, uh, after, uh, which is exactly the same time the
[01:17:16.080 --> 01:17:22.720]   F 22 used a name. Nine X side wire near missile to shoot it down, shoot something down. Uh,
[01:17:22.720 --> 01:17:29.360]   one balloon expert talked to by NPR said, I am 98% certain it was that it was that balloon.
[01:17:29.360 --> 01:17:35.840]   The little mylar balloons, uh, that are fairly fragile. So they, they talk to the club and the
[01:17:35.840 --> 01:17:41.840]   club, you know, let's, let's them go and they go up. They go pretty high, you know, but they're not
[01:17:41.840 --> 01:17:47.200]   a hazard to aviation because, uh, as they go up in the air, the, you know, the air pressure goes down.
[01:17:47.200 --> 01:17:52.320]   And so the mylar gets stretched pretty thin. Uh, he said, you know, just the jet wash probably
[01:17:52.320 --> 01:18:00.320]   pop it. Just right. You just fly by it. It's going to go. Uh, it carries a, whoa, it carries a
[01:18:00.320 --> 01:18:06.320]   tiny little, uh, payload of just a few ounces, about 16 point, not even an ounce, 16.4 grams,
[01:18:06.320 --> 01:18:11.520]   half an ounce, which, uh, concludes, includes a GPS module, a transmitter, because that's how they
[01:18:11.520 --> 01:18:18.320]   track it. A little computer and a tiny, uh, solar panel to, to give it power, uh, weighs about
[01:18:18.320 --> 01:18:23.040]   half an ounce, costs less than a hundred dollars for this balloon. So we just want to say, you know,
[01:18:23.040 --> 01:18:31.600]   I think we'll, you know, uh, a salute to a canine YO stroke 15. Uh, I feel safer, don't you?
[01:18:31.600 --> 01:18:39.680]   So, okay. So this is a hobbyist group, sir. Yeah. Well, I think so. Their name, their name kind of
[01:18:39.680 --> 01:18:47.360]   implies the Northern Illinois bottle cap balloon brigade. I don't think it's so official. I don't
[01:18:47.360 --> 01:18:51.440]   think they're official organization doing stuff like this. Don't you normally have to get some
[01:18:51.440 --> 01:18:57.520]   type of clearance. They're so small and so fragile. The FAA does not have to, uh, approve it. They,
[01:18:57.520 --> 01:19:02.800]   of course, have FCC approval for the little radio transmitter, but the FAA doesn't track these.
[01:19:02.800 --> 01:19:06.080]   They're just little hollies, balloons and they're not harmful. Because I'm just thinking about the
[01:19:06.080 --> 01:19:11.440]   tiny little drones that people still have to register to this day. But if they're under, what is it?
[01:19:11.440 --> 01:19:18.000]   There's under 250 grams. I don't have to register. So this thing, this thing weighs eight grams. I don't.
[01:19:18.480 --> 01:19:26.320]   Yeah. I think it's 16 grams. I don't think it's going to. Wow. And they said the FAA does not, uh,
[01:19:26.320 --> 01:19:32.240]   federal law requires most large flying objects to be registered, but amateur Pico balloons
[01:19:32.240 --> 01:19:35.520]   are so small and light they're not subject to these requirements.
[01:19:35.520 --> 01:19:43.760]   All right. Now. But they shot another one down today. And again, they're not, they're not
[01:19:43.760 --> 01:19:48.480]   getting them because they're so tiny and they're in this kind of cold and remote and
[01:19:48.480 --> 01:19:56.320]   I just imagine the pilot up there. You want me to shoot what?
[01:19:56.320 --> 01:20:02.000]   I just want to imagine like what they paint on their flames to honor the kills that they got.
[01:20:02.000 --> 01:20:08.080]   A little balloon, a little balloon on the plane.
[01:20:09.600 --> 01:20:14.640]   The, uh, the onion. Here's the onion story. US successfully shoots down kid jumping too high on
[01:20:14.640 --> 01:20:24.000]   trampoline. Uh, oh, good. Come on, man. That's a joke. Obviously, obviously, although pretty big
[01:20:24.000 --> 01:20:30.960]   explosion coming out of that nicely done anyway. Yeah. Yeah. Uh, let's see. Oh, I thought this was
[01:20:30.960 --> 01:20:38.480]   interesting. Speaking of Google, a post from, uh, an ex-Google or Praveen says Shraddi Adri, who
[01:20:38.480 --> 01:20:45.600]   is, uh, was, uh, joined Google, uh, because his startup app sheet was acquired. Um,
[01:20:45.600 --> 01:20:51.280]   he says the acquiring team and executives welcomed us and treated us well. We joined with great
[01:20:51.280 --> 01:20:57.760]   enthusiasm and commitment yet now at the expiry of my three year mandatory retention period.
[01:20:57.760 --> 01:21:04.000]   I have left Google understanding how a once great company has slowly ceased to function.
[01:21:04.000 --> 01:21:11.280]   Google has a, this is, by the way, is this kind of a, almost a tradition now from people leaving
[01:21:11.280 --> 01:21:18.800]   Google going back, uh, to the founder of Waze, uh, gnome, Bardeen, who, uh, left a similar note on
[01:21:18.800 --> 01:21:28.160]   his way out the door. Um, Praveen writes this on medium. He says Google has a 175,000 plus
[01:21:28.160 --> 01:21:33.520]   capable and well compensated employees who get very little done quarter over quarter,
[01:21:33.520 --> 01:21:39.120]   year over year, like mice. They are trapped in a maze of approvals, launch processes, legal
[01:21:39.120 --> 01:21:43.840]   reviews, performance reviews, exec reviews, documents, meetings, bug reports, triage,
[01:21:43.840 --> 01:21:49.440]   okay. R's H one plans followed by H two plans. All hands, summits, inevitable reorgs. The mice
[01:21:49.440 --> 01:21:55.520]   are regularly fed their cheese promotions, bonuses, fancy foods, fancier perks. And despite many
[01:21:55.520 --> 01:22:00.240]   wanting to experience personal satisfaction and impact from their work, the system trains them
[01:22:00.240 --> 01:22:05.200]   to quell these inappropriate desires and learn what it actually means to be googly.
[01:22:05.200 --> 01:22:13.760]   Just don't rock the boat. His contention is essentially that Google makes so much money
[01:22:13.760 --> 01:22:22.320]   with so little effort on, uh, search ads that they have gotten highly conservative about everything
[01:22:22.320 --> 01:22:29.520]   else that don't rock the boat really means, you know, don't jeopardize the business and they don't
[01:22:29.520 --> 01:22:38.080]   have to because they make so much money. He worked at Microsoft for a long time and kind of the dark
[01:22:38.080 --> 01:22:43.120]   days of Microsoft, he said Microsoft managed to turn things around, but it required exceptional
[01:22:43.120 --> 01:22:48.960]   leadership and good fortune. Google has a chance. I'll be rooting for it. The world will benefit
[01:22:48.960 --> 01:22:54.160]   immensely. He also says Google's in better shape. Read that part because he says that Google has,
[01:22:54.160 --> 01:23:00.480]   has a mission. Yeah, his self reflective. And there's a chance here, but yeah, I've heard
[01:23:00.480 --> 01:23:06.480]   people inside Google that it's like the Marines. The hierarchy is killer. Uh, the approvals are
[01:23:06.480 --> 01:23:11.040]   the they don't they still understand it don't have a budget for this year yet. The year is two
[01:23:11.040 --> 01:23:16.800]   months in. Oh my God. Oh, that I used to cut is what one person I know said we've never been told
[01:23:16.800 --> 01:23:23.920]   to cut. Right. I got that second hand, right? And now and now they are. Um, Eric Schmidt always said
[01:23:23.920 --> 01:23:31.120]   that Google's biggest problem will be size. It will grow too big. Yeah, it's it's more than doubled
[01:23:31.120 --> 01:23:38.400]   in the last two or three years. So it's really grown, uh, a huge leaps and bounds. When you wrote
[01:23:38.400 --> 01:23:45.200]   what would Google do? Obviously it was a leaner, meaner, smarter Google at the time. Um,
[01:23:45.200 --> 01:23:50.880]   I still think they're just a smarter. It's discussion we had about it is the smartest people,
[01:23:50.880 --> 01:23:57.440]   isn't it? I think it makes sense for a company as large as this to have all the different checks
[01:23:57.440 --> 01:24:02.320]   and balances in place far as approvals and whatnot. But yet at the same time, there's no excuse
[01:24:02.320 --> 01:24:07.440]   for some of the crap that's been thrown out into the public from Google that when you know you've
[01:24:07.440 --> 01:24:13.200]   had this stuff had to get approved at some point, but yet it's not working. That's that's leadership
[01:24:13.200 --> 01:24:19.040]   stuff still to me. He says he says it's got to start at the top. Um, and I think, well, here's the
[01:24:19.040 --> 01:24:27.280]   question. Is it is it is Sundar the right CEO for Google at this time? Good guy, but I don't think
[01:24:27.280 --> 01:24:34.080]   he's getting it done. Mike, you all have to wonder was go ahead. Oh, I was so I wonder was some of
[01:24:34.080 --> 01:24:40.080]   this where before I became a lawyer, I worked in tech and I worked at, um, I worked at a large
[01:24:40.080 --> 01:24:45.280]   company that grew and grew and grew and then all of a sudden had to cut. And one of the things that
[01:24:45.280 --> 01:24:52.560]   you sort of realize when you're in a big company is it is a really difficult thing to manage because
[01:24:52.560 --> 01:24:56.400]   essentially when the times are good, you have the cash, you have the department, they have the
[01:24:56.400 --> 01:25:02.640]   budgets and you hire people and the people's careers are depending on providing returns that get
[01:25:02.640 --> 01:25:07.360]   measured in ways that at least their department and immediate reporting structure can validate
[01:25:07.360 --> 01:25:12.320]   and verify. They may or may not plug in well with the mission of the rest of the company. So you
[01:25:12.320 --> 01:25:17.280]   can end up with very empowered people who do things that may actually not be consistent with the
[01:25:17.280 --> 01:25:22.160]   country company's interest because it's very difficult to plug into the mothership. And it's a
[01:25:22.160 --> 01:25:26.640]   huge organizational problem where you can look at companies and sort of say they're making mistakes
[01:25:26.640 --> 01:25:31.040]   and there might be really stupid ways of growing and less stupid ways of growing. But I don't think
[01:25:31.040 --> 01:25:35.360]   it's a precise science. I think it's there's a lot of challenges. And if you're going to look at it,
[01:25:35.360 --> 01:25:40.880]   I mean, maybe Sundar could be doing a better job or being more aware of this dynamic and managing
[01:25:40.880 --> 01:25:45.600]   through it. But I think there's a lot of things to look at. Like there's some crappy things that
[01:25:45.600 --> 01:25:51.680]   come out of tech in particular things that like make a mess in the privacy space. But I used to
[01:25:51.680 --> 01:25:58.480]   work in marketing for these big tech companies and the like in the 1.0 days and you get a marketing
[01:25:58.480 --> 01:26:03.040]   person who's like, well, how do I get a promising lead? How do I get like a return on this? How do I
[01:26:03.040 --> 01:26:07.920]   get contacts that we can continue to market to where they were being incentivized and success was
[01:26:07.920 --> 01:26:13.200]   described in terms that actually it turns out that if you did it, you were succeeding at your job,
[01:26:13.200 --> 01:26:18.400]   but you were doing a terrible thing for the company and ultimately messing up the public policy
[01:26:18.400 --> 01:26:22.480]   and doing things that were creepy for the customer. But you ended up with a disconnect because what
[01:26:22.480 --> 01:26:30.000]   they were being told to do and trained to do and encouraged to do were bad ideas. And so what do you
[01:26:30.000 --> 01:26:35.360]   do when you're you're at cross purposes like that? How does that happen? And how do you fix it?
[01:26:35.360 --> 01:26:39.360]   And what do you do with all your talented people who are like, but I just want my job and my career.
[01:26:39.360 --> 01:26:43.840]   How do you tell me now that the things that I used to specialize in are bad?
[01:26:43.840 --> 01:26:48.880]   You know, I also think, Kathy, you write, they specialized in things like engineering,
[01:26:48.880 --> 01:26:53.120]   engineering, engineering. And you know, I think what they're missing right now, you may laugh at me
[01:26:53.120 --> 01:27:00.880]   for this is Marissa Meyer, because she was a product person who talked about the customer.
[01:27:00.880 --> 01:27:01.440]   Good point.
[01:27:01.440 --> 01:27:04.480]   And that's the piece you read from Leo talks about this too.
[01:27:04.480 --> 01:27:09.200]   Cori doctorate, certification piece. Is that the one you're talking about? Or no,
[01:27:09.200 --> 01:27:13.040]   you're talking about this one I just read. I think you might be sure mispronouncing some of his
[01:27:13.040 --> 01:27:18.960]   confidence in the term that he's I have to say it that way. That's the good place.
[01:27:18.960 --> 01:27:20.640]   How does he say it? Miss Kathy.
[01:27:20.640 --> 01:27:25.040]   But that's what he does.
[01:27:25.040 --> 01:27:29.440]   Cori says, which is that any company, including Google Amazon, everybody else starts with a
[01:27:29.440 --> 01:27:33.600]   customer first focus, then becomes a business first focus and finally becomes profit first.
[01:27:33.600 --> 01:27:38.480]   Focus. You have weird incentives that are operating on it. I mean, especially as the
[01:27:38.480 --> 01:27:42.880]   companies go public, especially as all of a sudden, they need to return certain values to the share
[01:27:42.880 --> 01:27:46.880]   holders or the shareholders will sue. I mean, you did see a lot of tech companies try to
[01:27:46.880 --> 01:27:52.800]   establish themselves with structures that gave them more flexibility to continue to sort of
[01:27:52.800 --> 01:27:57.040]   do what they really believed in without having to necessarily worry about shareholder value,
[01:27:57.040 --> 01:28:02.560]   et cetera. But it may not be a panacea there either because then you get like a Facebook where
[01:28:02.560 --> 01:28:06.800]   corporate control ends up not necessarily being diversified in ways that maybe the company will
[01:28:06.800 --> 01:28:11.920]   suffer from. It's hard. Like these are hard problems. And I think we also have to look more
[01:28:11.920 --> 01:28:17.280]   broadly, especially, I mean, this annoys me in the section 230 discussion because everyone's like
[01:28:17.280 --> 01:28:22.480]   everything bad about the tech policy spaces because of section 230. And it's not. We have an
[01:28:22.480 --> 01:28:27.280]   awful lot of other law that is operating on the space that is potentially creating
[01:28:27.280 --> 01:28:32.400]   incentives for companies to act in ways that in the broad picture we really wouldn't like.
[01:28:32.400 --> 01:28:37.600]   So how do we change the regulatory terrain in all areas that law touches on it so that the
[01:28:37.600 --> 01:28:41.920]   companies do act in a way that does more of what we want and less of what we don't want?
[01:28:41.920 --> 01:28:49.120]   It's not just 230. And 230 is sort of an example of actually what it looks like when you can
[01:28:49.120 --> 01:28:53.200]   align the incentives better to get the most of what you want and the least of what you don't.
[01:28:53.200 --> 01:28:59.360]   But so far, 230 is crumbling under the weight of having to completely explain and manage perfectly
[01:28:59.360 --> 01:29:04.560]   every single externality produced by tech. And that's not what the law was designed for.
[01:29:04.560 --> 01:29:07.920]   And that's why it's creaking. And that's why it's getting unfairly criticized because it can't
[01:29:07.920 --> 01:29:11.520]   possibly fix everything that would go wrong with innovation.
[01:29:11.520 --> 01:29:17.200]   So it's really how can we screw big tech? Oh, let's do this. And that's not the problem, obviously.
[01:29:17.200 --> 01:29:24.160]   Cessrati says the way I see it, Google has four core cultural problems. They are all the
[01:29:24.160 --> 01:29:28.960]   net is it's not just size. They're all the natural consequences of having a money printing machine
[01:29:28.960 --> 01:29:35.600]   called ads that has kept growing relentlessly every year hiding all other sins. The sins he
[01:29:35.600 --> 01:29:42.640]   talks about no mission, no urgency, delusions of exceptionalism. Like we're Google, you know,
[01:29:42.640 --> 01:29:47.680]   we're the greatest. And I was a time make that was that was a big disease. Oh, yeah, we do it.
[01:29:47.680 --> 01:29:52.960]   We spend the most money on it. It must be the best. Yeah. I remember I had friends in the 80s who
[01:29:52.960 --> 01:29:58.560]   were at Atari, which at the time was King of the Hill in video games. And they just thought they
[01:29:58.560 --> 01:30:04.720]   were the bees knees and their shirts did not smell badly. But in fact,
[01:30:04.720 --> 01:30:12.400]   shortly thereafter, they were all out of work. Finally, he says it's mismanagement. You know,
[01:30:12.400 --> 01:30:20.080]   we told this story a couple of weeks ago that Google called a code red after chat GPT came out
[01:30:20.080 --> 01:30:26.800]   and brought in Larry and Sergey to kind of, I don't know what, but send our Pichai calls them.
[01:30:26.800 --> 01:30:32.320]   Yeah, to give my little goose. And then of course they rushed out and we tied this story last week.
[01:30:32.320 --> 01:30:38.720]   They rushed out a flawed announcement. By the way, and maybe Google's gone, yeah, we dodged that
[01:30:38.720 --> 01:30:45.200]   bullet. Microsoft's taken all the heat in the world for chat GPT. Google never did turn on
[01:30:45.200 --> 01:30:51.920]   bard, right? Right. And now they're saying, yeah, maybe we can delay that. Yeah, let's just
[01:30:51.920 --> 01:30:56.800]   lay that a little longer. A little bit more careful ones. All that all the things that
[01:30:56.800 --> 01:31:02.800]   guy writes about that piece about being careful and slow. That's a big advantage. No risk. No risk.
[01:31:02.800 --> 01:31:09.920]   I can't imagine some I don't know what the board's thinking. I think the board needs to kick the
[01:31:09.920 --> 01:31:15.840]   kicks and are out as nice a guy as he is. Or maybe not out just down. He's a wonderful or
[01:31:15.840 --> 01:31:21.600]   they need they need my view is that what they miss oddly is marketing. Think products and marketing
[01:31:21.600 --> 01:31:27.360]   anything product is king at Google for product. There means engineering product should mean customer.
[01:31:27.360 --> 01:31:35.840]   Well, and he's and he also Shoudry also says, you know, yeah, you could say customers first product
[01:31:35.840 --> 01:31:41.840]   first, but but really it's risk first, you know, let's let's respect the risk with two of course,
[01:31:41.840 --> 01:31:45.360]   Google's core values are respect the user and respect the opportunity and practice.
[01:31:45.360 --> 01:31:50.800]   The systems and processes are intentionally designed to respect risk risk mitigation
[01:31:50.800 --> 01:31:55.440]   trumps everything else. This makes sense of everything's going wonderfully. And the most
[01:31:55.440 --> 01:31:59.200]   important thing is to avoid rocking the boat and keep sailing on the rising tides of ad revenue.
[01:31:59.200 --> 01:32:06.080]   And such a world potential risk lies everywhere. You look and so they're very careful. I'd you
[01:32:06.080 --> 01:32:09.200]   know, we all look, we all see it. We all know what something's going on.
[01:32:11.920 --> 01:32:17.120]   And this makes as much sense as anything I've read. Speaking of which, there is a big transition
[01:32:17.120 --> 01:32:24.960]   at YouTube. We found this out right after the show last week, Susan Wojcie, stepping down from
[01:32:24.960 --> 01:32:30.800]   YouTube after nine years and 25 years of the company. Yeah, she was employee 16.
[01:32:30.800 --> 01:32:37.840]   Got to know. She got to know her in Sergei by renting them the garage that they built Google
[01:32:37.840 --> 01:32:44.880]   in after they left Stanford on her. Yeah, she's going to spend more time with her money.
[01:32:44.880 --> 01:32:54.720]   And I think that's wonderful. I bet she becomes a VC or something like that. But you know,
[01:32:54.720 --> 01:32:58.880]   if you spend that much time on a company 25 years, she's certainly invested hundreds of
[01:32:58.880 --> 01:33:07.120]   millions of dollars in stock. Yeah, no, no reason for her not to not to take off her off. She
[01:33:07.120 --> 01:33:11.280]   wrote high YouTubers. She's sold over the years. Oh, yeah, I know. I mean,
[01:33:11.280 --> 01:33:17.440]   well, you talk about Marissa Myers, you know, she was she was sitting pretty when she left Google.
[01:33:17.440 --> 01:33:24.240]   She didn't need to make money. 25 years ago, Susan Wojcie writes, I made the decision to join a
[01:33:24.240 --> 01:33:28.720]   couple of Stanford graduate students who were building a new search engine. Her names were Larry
[01:33:28.720 --> 01:33:32.960]   and Sergei. I saw the potential of what they were building, which is incredibly exciting.
[01:33:32.960 --> 01:33:37.840]   And although the company only had a few users and no revenue, I decided to join the team. It would
[01:33:37.840 --> 01:33:45.440]   be one of the best decisions of my life. One of one of today, after nearly 25 years, I've decided
[01:33:45.440 --> 01:33:50.240]   to step back for my role as the head of YouTube and start a new chapter focused on my family,
[01:33:50.240 --> 01:33:55.680]   health, and personal projects I'm passionate about. It concerns me a little bit to hear the word
[01:33:55.680 --> 01:34:03.440]   health in there. But I hope she has no that's not unusual, right? If you're CEO of a company,
[01:34:03.440 --> 01:34:08.400]   you're gonna have stress or true lead to some that's not good for you. Is it health stuff?
[01:34:08.400 --> 01:34:13.440]   It could cut both ways. But I think Leo's point is like, if it's getting called out, what is
[01:34:13.440 --> 01:34:18.240]   modifying motivating that specific call out? Like, is this something that that sentence would have
[01:34:18.240 --> 01:34:21.840]   been complete without it, except it happens to be something at forefront of her mind?
[01:34:21.840 --> 01:34:25.200]   Yeah. Well, I hope she's all right. I hope she's all the board of sales force, which is an
[01:34:25.200 --> 01:34:35.200]   interesting board these days. What's going on at Salesforce? Well, they messed up with the
[01:34:35.200 --> 01:34:45.200]   acquisition of Slack. There's co CEO left. There's pressure from outside investors against
[01:34:46.400 --> 01:34:53.280]   Betty off. It's interesting times. They were, you know, there was a part of their layoffs
[01:34:53.280 --> 01:34:57.120]   back in Carolina. Oh, really? I'm sorry.
[01:34:57.120 --> 01:35:03.680]   They were on top of the world. I mean, they built that beautiful building in downtown
[01:35:03.680 --> 01:35:08.480]   San Francisco. They built this building. I don't know if I'm going to completely
[01:35:08.480 --> 01:35:18.720]   ratify the adjective he used. That's beautiful. My wife shares your opinion, by the way, Kathy.
[01:35:18.720 --> 01:35:25.760]   Whenever we're down there and you see that for some reason, they from time time,
[01:35:25.760 --> 01:35:31.680]   I'm going to have a dancer dancing and then her silhouette projected on the at the tip of this
[01:35:32.880 --> 01:35:41.280]   building. I don't know. I probably can't say what my wife says in the polite company, but you can
[01:35:41.280 --> 01:35:45.120]   use your imagination. Just mispronounce all the consonants again. It'll be fine.
[01:35:45.120 --> 01:35:50.720]   I live in the good place. There it is. There's the picture of the, they call it the sales
[01:35:50.720 --> 01:35:54.720]   first building. Salesforce does not own it, but it is one of the, is the kind of the key tenant.
[01:35:54.720 --> 01:36:02.160]   There. Yeah, you know what? It's tough these days to be a big tech giant, right? You're either
[01:36:02.160 --> 01:36:06.960]   in the Supreme Court, you're testifying in front of Congress, you're getting me-tude.
[01:36:06.960 --> 01:36:13.360]   It's just a tough time or you're laying off tens of thousands of employees.
[01:36:13.360 --> 01:36:24.160]   Better to be like us and falling between the cracks. Basically, nobody cares. Nobody's paying
[01:36:24.160 --> 01:36:30.000]   any attention at all. Even NPR is laying off people because of designing ad revenue and podcasting.
[01:36:30.000 --> 01:36:41.360]   Yeah, podcasting has really, really sucks these days. I'm going to say, well, just switching
[01:36:41.360 --> 01:36:45.440]   subjects. I've been deeply alarmed and I'll have to just leave it here because it's been a while
[01:36:45.440 --> 01:36:52.800]   since I've read this, but some of the positions that NPR has taken in the copyright space have
[01:36:52.800 --> 01:36:58.240]   been deeply concerning for a form of media that decided that we're going to serve the public by
[01:36:58.240 --> 01:37:03.520]   being publicly supported has taken positions and I think are hostile to the public because they're
[01:37:03.520 --> 01:37:08.800]   so fixated on the monetization that obviously they have to have money somehow, but they're so
[01:37:08.800 --> 01:37:16.000]   focused on general commercial models of monetization that they are advocating for copyright issues
[01:37:16.000 --> 01:37:20.560]   that I think are detrimental to the overall public interest. Are they just in courts or are
[01:37:20.560 --> 01:37:25.520]   they just, I mean, are they suing people or what's going on? Well, I've seen some stuff in terms of
[01:37:25.520 --> 01:37:31.520]   comments at the copyright office, I believe, but also in the area cases, one of the lead
[01:37:31.520 --> 01:37:37.680]   plaintiffs was WNET, I believe, which was channel 13 in the New York City area. Public
[01:37:37.680 --> 01:37:44.640]   broadcasting in New York was being sued by NPR. I remember growing up in what? Well, not NPR,
[01:37:44.640 --> 01:37:50.880]   but and let me also say we, the general public, including me, doesn't necessarily get the distinctions
[01:37:50.880 --> 01:37:56.000]   between the various forms of public entities. And so this doesn't necessarily apply to all of them,
[01:37:56.000 --> 01:38:00.880]   but I have seen NPR in particular, I think, in some of the commons, but also other forms of public
[01:38:00.880 --> 01:38:07.840]   broadcasting. The WNET thing, suing Ariel made absolutely no sense to me. I remember growing up
[01:38:07.840 --> 01:38:12.240]   and watching, I mean, Sesame Street would be preempted while they did their pledge drives.
[01:38:12.240 --> 01:38:16.080]   What I talked about, how important was that the public supported it so kids could watch Sesame
[01:38:16.080 --> 01:38:21.200]   Street and all of a sudden they're trying to sue an antenna service that was delivering Sesame
[01:38:21.200 --> 01:38:25.680]   Street? How is that consistent? How do you go to the public and say support us so we produce
[01:38:25.680 --> 01:38:29.760]   this programming, which now we're going to make sure that nobody can watch because we sued the
[01:38:29.760 --> 01:38:34.480]   technology that was facilitating the watching? It makes no sense and it's a shonda and I,
[01:38:34.480 --> 01:38:40.640]   it's terrible. And I think there needs to be a reckoning of, I mean, I do believe in supporting
[01:38:40.640 --> 01:38:44.960]   public broadcasting in these various forms, but not if it's going to take hostile positions to
[01:38:44.960 --> 01:38:49.600]   the public interest because then what's the point? Was that the dime size little antenna? Yeah,
[01:38:49.600 --> 01:38:53.520]   that was the ones around the roof and yeah, the little tiny. Everybody gets their own,
[01:38:53.520 --> 01:39:00.480]   I put out a business by the Supreme Court and I guess by WNET. And then low-cast as well. I mean,
[01:39:00.480 --> 01:39:08.000]   we've got, I'm too busy surfing my section 230 temporary high to go into that, but I think
[01:39:08.000 --> 01:39:13.600]   everything Kathy money, greed, maybe not money, but greed, the promise. Let me ask them money.
[01:39:13.600 --> 01:39:17.840]   I'm going to say copyright law also ruins everything. Copyright law should not have allowed
[01:39:17.840 --> 01:39:24.160]   low cast to shut down and the public is being locked away from the airwaves we own. And I can't
[01:39:24.160 --> 01:39:28.960]   watch things that are being broadcast on public TV because I don't have my own antenna and I don't
[01:39:28.960 --> 01:39:33.520]   have cable and for various reasons, I can't get it. How is I not supposed to be able to watch the
[01:39:33.520 --> 01:39:37.520]   Super Bowl? Broadcasters, because I was somewhere where I could, but it is a little bit of greed
[01:39:37.520 --> 01:39:43.040]   because broadcasters got used to not only getting rad revenue from their free on the air,
[01:39:43.040 --> 01:39:48.800]   broadcast, but getting revenue from cable companies because of must carry laws and they
[01:39:48.800 --> 01:39:55.120]   basically are double dipping and they were reluctant to give up the latter, even though they were
[01:39:55.120 --> 01:40:01.200]   still making money on advertising. Anyway, NPR, go ahead. Yeah, well, transmission fees, I think,
[01:40:01.200 --> 01:40:09.520]   have been an extremely destructive economic force. They've been for the greed and I think
[01:40:09.520 --> 01:40:13.760]   it's tenuous enough in the cable space and I think it's absolutely abhorrent for any
[01:40:13.760 --> 01:40:21.280]   licensee of public spectrum to basically demand that the public pay for them getting to broadcast
[01:40:21.280 --> 01:40:28.080]   on public spectrum. That makes no sense whatsoever. Amen, sister. Amen. Councilor Sussan. NPR reducing
[01:40:28.080 --> 01:40:36.160]   10% of its workforce due to dropping ad sales. They operate on a $300 million budget revenue
[01:40:36.160 --> 01:40:41.120]   expected drop by 30 million. That's 10%. They did not say where the cuts would come, but they
[01:40:41.120 --> 01:40:46.880]   planned to eliminate already vacant positions. Well, that'd be the first ones I'd get rid of.
[01:40:46.880 --> 01:40:57.520]   That's sad. NPR plans to stay focused on podcasts, which have been the company's strong suit.
[01:40:57.520 --> 01:41:02.000]   Does they get a lot of subscriber revenue? Yeah, staff cuts will not fall disproportionately
[01:41:02.000 --> 01:41:10.080]   on employees of color. I don't know, he threw that in. Yeah, but they like the
[01:41:10.080 --> 01:41:15.120]   advice. Did I just remember? They canceled their intern program, I believe. Oh, and that was like
[01:41:15.120 --> 01:41:22.240]   a critical pipeline, especially for underrepresented people. Yeah, my school. Oh, now interesting. Yeah.
[01:41:22.240 --> 01:41:30.560]   All right, well, yeah, you noticed we only had one ad on our show today. It's an endemic and I
[01:41:30.560 --> 01:41:34.880]   don't understand it, frankly, because I don't think maybe I'm wrong, but is the economy really
[01:41:34.880 --> 01:41:39.840]   tanking? It feels like it's actually no starting to warm up a little bit and feel, I mean,
[01:41:39.840 --> 01:41:47.520]   unemployment's at record low. I think excuses. You know, I think, well, I mean,
[01:41:47.520 --> 01:41:53.440]   in fact, there's a nerve there's a nervousness. There is a nervousness in the air. People are
[01:41:53.440 --> 01:42:00.480]   worried about recession. Maybe. And so I think a lot of our advertising, the B2B business to
[01:42:00.480 --> 01:42:07.040]   business advertising has disappeared. I don't know why business to consumer is still strong.
[01:42:07.040 --> 01:42:11.120]   That's why you hear the Casper ads and stuff. Although the one we had today was B2B.
[01:42:11.120 --> 01:42:18.400]   I don't I just don't get it. I sat at a dinner before the pandemic with five
[01:42:18.400 --> 01:42:22.080]   advertisers brought together by a big PR company and there were like three or four of us
[01:42:22.080 --> 01:42:26.320]   bladders there. And at some point I said, do you care about the Fader newspapers?
[01:42:26.320 --> 01:42:31.200]   And after enough, really good chart today, they said, no, no, no, I know, because the
[01:42:31.200 --> 01:42:34.480]   internet allows us to have our own direct relationships now. We care about our stock price.
[01:42:34.480 --> 01:42:36.720]   So we care about the Wall Street Journal. But that's pretty much it. Yeah.
[01:42:36.720 --> 01:42:43.440]   Mark Zuckerberg's decided he's got a new way to make money.
[01:42:43.440 --> 01:42:50.000]   12 bucks a month. And you too can have a blue check on meta or should I say,
[01:42:50.000 --> 01:42:55.200]   at least come with a different color. I mean, I don't call go between he and Mr. Eli.
[01:42:55.200 --> 01:43:00.640]   Well, that's exactly it. If you're making modeling your business decisions on anything
[01:43:00.640 --> 01:43:06.960]   Musk has just done. What is wrong with you? Hey, it's not hurting us. I really I for a long
[01:43:06.960 --> 01:43:12.720]   time. It's really hurting Musk. Oh, yeah. Musk can he's laughing to the bank.
[01:43:12.720 --> 01:43:21.840]   He's laughing. He's laughing at the bank. He's laughing at the bank. So I'll
[01:43:24.640 --> 01:43:29.280]   But the bank, I think, is going to be laughing last. And I think it's just a matter of time
[01:43:29.280 --> 01:43:36.400]   before we see that. I hope so. I think the worst thing with the Elan is that he seems to think that
[01:43:36.400 --> 01:43:44.960]   his terrible dad jokes are funny and is now inflicting them on us kind of incessantly.
[01:43:44.960 --> 01:43:52.320]   This is one that most recent post people always ask, where is Illuminati? But they never ask
[01:43:52.320 --> 01:43:57.840]   how is Illuminati? What the hell are you talking about? Thank you for inflicting that on me. I
[01:43:57.840 --> 01:44:05.840]   hadn't seen it. I understand your amplify as an AI language model. I have been trained to generate
[01:44:05.840 --> 01:44:13.120]   responses that are intended to be helpful and objective and informative. Okay, Elon is basically
[01:44:13.120 --> 01:44:17.680]   these are dad jokes. They're not even dad jokes. I'm a dad. My jokes are funnier.
[01:44:20.000 --> 01:44:27.200]   Oh, here's a good one. High time I confessed. I let the doge out. It was me. I let the dogs.
[01:44:27.200 --> 01:44:33.040]   Yeah. Yeah. Boy. That's not. I don't know what the spike in crypto that day. Somebody told me
[01:44:33.040 --> 01:44:38.480]   it's because he's using a lot of drugs. Is that I don't know what that is? No, no, they're going
[01:44:38.480 --> 01:44:44.160]   to be plenty of people to say, Hey, don't put drugs on this. This is just. Don't don't don't
[01:44:44.160 --> 01:44:51.520]   just say that bad. I don't know what it is. You'll send the wrong message. Here's another dad joke
[01:44:51.520 --> 01:44:57.440]   from Mr Musk. Say what you want to mount me, but I acquired the world's largest nonprofit for 44
[01:44:57.440 --> 01:45:04.080]   billion L O L. Okay, that's a good one. You like that? He doesn't understand why. Did that make you
[01:45:04.080 --> 01:45:09.440]   that make you laugh out loud? They didn't make any money. I ain't laughing out loud, but hey, good one.
[01:45:09.440 --> 01:45:14.080]   Who let the doge? He doesn't say pull my finger. That's the Oh, I'm sure he is.
[01:45:14.080 --> 01:45:22.080]   Just let that sink in. So Mark Zuckerberg, I guess I'm starting to feel like,
[01:45:22.080 --> 01:45:30.080]   is this possible? Mark realizes that the metaverse is not going to happen. And now maybe we need to
[01:45:30.080 --> 01:45:34.880]   figure out how to make more money at Facebook. So we're going to charge you 12 bucks for a
[01:45:34.880 --> 01:45:41.440]   verified by the way, people have to further the development of the metaverse by getting more money.
[01:45:41.440 --> 01:45:46.400]   No, no, no, I think he's I think he's lost so much. He's lost 12 billion last year on the metaverse.
[01:45:46.400 --> 01:45:53.440]   I think I think what people talked about in the past was what if what would you what would you
[01:45:53.440 --> 01:46:01.040]   pay for an ad free algorithm free Facebook? You know, five bucks a year, 10 bucks a year might be
[01:46:01.040 --> 01:46:04.960]   that might be interesting. That'll be interesting. This doesn't do that. This is literally
[01:46:04.960 --> 01:46:10.640]   just a blue check. It's good for as well. It's the same problem with Twitter.
[01:46:10.640 --> 01:46:17.600]   Your verification is good for the users. It's not for the verify. Well, and you have to give them
[01:46:17.600 --> 01:46:23.120]   government ID. So the laughs on you, because you're paying 12 bucks a month for Facebook to even do
[01:46:23.120 --> 01:46:30.160]   more advertising. I mean, this looks the Twitter, the Facebook when I thought was a joke, like I
[01:46:30.160 --> 01:46:35.440]   thought it was an onion headlight. I could not believe that this was actually something that like
[01:46:35.440 --> 01:46:40.480]   if you're going to do what they're proposing to do, which I don't think is a good idea, but it's a
[01:46:40.480 --> 01:46:45.760]   serious proposal. And I think it's kind of hitting on the federated and federated identity management
[01:46:45.760 --> 01:46:49.600]   issues that have kind of been lurking for a while. But if you're going to roll out something,
[01:46:49.600 --> 01:46:54.720]   you roll it out with care and thinking about this, and it's got a month long of not years long
[01:46:54.720 --> 01:46:59.280]   trajectory before you release it. This looks like it was on the back of a cocktail napkin and
[01:46:59.280 --> 01:47:03.440]   implemented it. Which now we're starting from the must model.
[01:47:03.440 --> 01:47:08.000]   Exactly. It's the Twitter model. I am absolutely shocked that Zuckerberg,
[01:47:08.000 --> 01:47:13.200]   a like it's obviously like ripping off of Twitter's homework, like looking over
[01:47:13.200 --> 01:47:17.680]   must shoulder and we're going to be blue. I mean, Facebook was already blue, but like we're going
[01:47:17.680 --> 01:47:21.440]   to be blue if we're going to be verified. And we're going to be doing this because Twitter is and
[01:47:21.440 --> 01:47:26.800]   we're going to do it in the same haphazard, unfought out way that Twitter rolls out product innovations.
[01:47:26.800 --> 01:47:33.040]   About a couple of years ago, we showed was at Google's plan to like have this giant balloon
[01:47:33.040 --> 01:47:39.120]   and fell up you so you can have a silent conversation. I think that was Google. Oh, yeah. Yeah. Yeah.
[01:47:39.120 --> 01:47:46.480]   Yeah. So now maybe this is what the 12 bucks a month's going to meta has created a new way.
[01:47:46.480 --> 01:47:54.000]   This is not the onion. A new way to have a quieter cubicle. They invented the cubicle. They
[01:47:54.000 --> 01:48:00.320]   invented the cubicle with an opioid. It's got it's got it's called the cube. So it's not a
[01:48:00.320 --> 01:48:07.920]   cubicle, Jeff. It's a cube. It's a it's got noise canceling materials all around it.
[01:48:07.920 --> 01:48:14.800]   Oh my gosh. If you can't tell an onion headline from your press release,
[01:48:14.800 --> 01:48:17.200]   go back and try this again.
[01:48:19.920 --> 01:48:24.000]   Well, what it really is is acknowledgement that the open office plan was a terrible mistake.
[01:48:24.000 --> 01:48:27.600]   Employees hate it cared for that. They get nothing done.
[01:48:27.600 --> 01:48:33.200]   People hated cubicles and they went and said, well, the thing that will make a cube from better is no
[01:48:33.200 --> 01:48:40.800]   walls. So now we're got the wall. Bring who brought the walls back. Oh, oh, oh, oh.
[01:48:40.800 --> 01:48:44.480]   There it is. Now it's basically you're enveloped and felt.
[01:48:46.480 --> 01:48:52.400]   I just this voice over office ever. Oh yeah. Yeah. Maybe I could get some actually,
[01:48:52.400 --> 01:48:55.680]   you know what? When they changed their mind, this guy looks happy, but is he?
[01:48:55.680 --> 01:49:00.720]   He looks really maybe too happy. There's something going on.
[01:49:00.720 --> 01:49:08.720]   It's like a self cocoon says John Teneyne's vice president of global real estate facilities at meta.
[01:49:08.720 --> 01:49:14.800]   Yeah, what it reminds me of Leo, but less Nespans.
[01:49:14.800 --> 01:49:19.440]   Yes. He taped off. He taped off his space so that no one would end.
[01:49:19.440 --> 01:49:21.200]   He had to walk around. Yeah. Yeah.
[01:49:21.200 --> 01:49:27.520]   Yeah. Thank goodness the Wall Street Journal decided to put this close up image of the material.
[01:49:27.520 --> 01:49:30.400]   Was it the felt Wall Street Journal?
[01:49:30.400 --> 01:49:36.000]   The onion street journal? I don't know.
[01:49:38.480 --> 01:49:45.280]   The cube is made of a felt like recycled pet plastic. The soft material absorbs the sound
[01:49:45.280 --> 01:49:49.120]   rather than pushes it back. Wait a minute. I'm sorry. I should say that properly because this
[01:49:49.120 --> 01:49:55.840]   guy's got a doctorate. Is this soft material absorbs the sound rather than pushing it back?
[01:49:55.840 --> 01:50:01.680]   Says Dr. Nagy. Dr. Nagy has invented this very special material.
[01:50:04.560 --> 01:50:10.720]   On the early benefit of the cube, Dr. Nagy says is that it has reduced the strain of the company's
[01:50:10.720 --> 01:50:16.560]   existing meeting rooms. The last thing we want is for people to camp out and meeting rooms alone.
[01:50:16.560 --> 01:50:21.520]   The cube is solid for that need to do meaningful focused work.
[01:50:21.520 --> 01:50:28.080]   Oh, you know, that's interesting people doing that. Yeah. Anytime I have talked to anybody at
[01:50:28.080 --> 01:50:36.400]   Google or Facebook, it's always about the meeting room. You can't. At five minutes to the hour,
[01:50:36.400 --> 01:50:40.080]   oh, the people are staring at us out there. We got to leave. Right. Can't have a meeting.
[01:50:40.080 --> 01:50:44.560]   Right. Because there's a book for the next person. It's booked. Meeting room is the precious
[01:50:44.560 --> 01:50:49.280]   space there. Yeah. Yeah. But you wouldn't actually, I don't think it was ever that bad
[01:50:49.280 --> 01:50:54.400]   when I worked at Big Tech Company in cube farms. Like, this wasn't a problem because you didn't
[01:50:54.400 --> 01:50:59.440]   need to escape to the meeting room to get like the solitude you needed to concentrate and have
[01:50:59.440 --> 01:51:04.320]   a conversation by the way, that either this article in the Wall Street Journal was written by somebody
[01:51:04.320 --> 01:51:13.040]   named Chip Cutter. I think it's an AI. Honestly, I'll be honest. No, this is, this is an AI.
[01:51:13.040 --> 01:51:18.400]   Meta is already to work on future iterations of the cube testing new colors and patterns,
[01:51:18.400 --> 01:51:26.160]   such as a wood grain look. Yeah. Yeah. And working with multiple manufacturers to roll out the
[01:51:26.160 --> 01:51:32.560]   the cube globally, they've already ordered 7,000 of them and distributing them to 22 locations
[01:51:32.560 --> 01:51:38.160]   worldwide. About 10% of meta spaces will feature the cube and employees. Oh, no,
[01:51:38.160 --> 01:51:43.840]   employees can reserve them when needed. You have to reserve the cube.
[01:51:46.240 --> 01:51:50.160]   When I worked at the Chicago Tribune, low many years ago, they redesigned the newsroom
[01:51:50.160 --> 01:51:55.200]   and they put up cubicle walls and the journalists hated it because they wanted to be it. They're
[01:51:55.200 --> 01:51:59.680]   the original open open office. Right. They one day they came in with screwdrivers and tore them all down.
[01:51:59.680 --> 01:52:10.160]   Oh, Lord. All right. What else would I should go look at your section?
[01:52:10.960 --> 01:52:17.840]   That's not. Let's look at Jeff's Jeff section. Google reassessing timeline for massive
[01:52:17.840 --> 01:52:24.320]   San Jose campus. Like we don't need all that space now. Yeah, no kidding. Yeah. Everybody's got
[01:52:24.320 --> 01:52:31.520]   to be doing that now, right? I mean, oh, yeah. New York, there's tons of space in Hudson Yards.
[01:52:31.520 --> 01:52:34.800]   It was going to Facebook and company. They don't need it now. Oh, wow.
[01:52:36.240 --> 01:52:41.520]   Though Amazon is trying to get people back to the office and staff, there's like 15,000 people
[01:52:41.520 --> 01:52:49.200]   joined a slack to protest. I say, no, we're not coming back, Andy. The German government has been
[01:52:49.200 --> 01:52:58.160]   banned from having a Facebook page. The data protection authority says, in fact, this may go to court.
[01:52:58.160 --> 01:53:04.000]   Yeah. Ulrich Kelberg demanded on Wednesday that the federal government stop operating its
[01:53:04.000 --> 01:53:12.480]   Facebook page because Facebook is in his spy thing or what? Well, no, because Facebook does privacy.
[01:53:12.480 --> 01:53:19.120]   No, no. No. No. Okay. But there is no, is there a German Facebook? Remember, there used to be like,
[01:53:19.120 --> 01:53:23.520]   there would be if every country would have, you know, Spain's Facebook or Russia's
[01:53:23.520 --> 01:53:28.640]   well, Germany, especially that they had, they had a studio felt set study VW.
[01:53:30.080 --> 01:53:37.360]   BZ. I like VW better. So it was owned by
[01:53:37.360 --> 01:53:43.760]   Holt spring, big publisher, Ronesti Zite and on Handel's Blot and all this other stuff.
[01:53:43.760 --> 01:53:49.360]   And there came, I don't know if I'll tell this right now. There came a time when he had an offer
[01:53:49.360 --> 01:53:53.840]   from Zuckerberg to buy it. And his board stopped them. Oh,
[01:53:56.080 --> 01:54:04.000]   Pavel Durov, the founder of Facebook for Russia. Right. Got an offer he couldn't refuse. Yeah.
[01:54:04.000 --> 01:54:10.560]   Vladimir Putin and now lives in Dubai, but has, you know, rebounded with Telegram. He's the
[01:54:10.560 --> 01:54:18.000]   founder of Telegram as well. There is a sci fi magazine called Clark's world named after Arthur C.
[01:54:18.640 --> 01:54:28.800]   Clark. This week, they stopped accepting submissions for new work. Why? Because they have been receiving
[01:54:28.800 --> 01:54:38.000]   hundreds of AI written submissions. Clark's world, which is considered one of the top sci fi
[01:54:38.000 --> 01:54:43.680]   and fantasy literary publications. It's won several Hugo Awards. Regularly bans a small number of
[01:54:43.680 --> 01:54:48.800]   people from submitting works each week each month, mostly for alleged plagiarism. But as of Monday,
[01:54:48.800 --> 01:54:56.720]   it had banned more than 500 accounts. People keep submitting AI written stuff. Now they pay pretty
[01:54:56.720 --> 01:55:02.320]   well 10 to 12 cents a word. And you know, you get your work published. That's a that's how many
[01:55:02.320 --> 01:55:08.160]   sci fi authors started is getting in the pulp magazines. But what's wrong with this, sir? Because
[01:55:08.160 --> 01:55:13.520]   you have to be a pretty good prop master, if you will, to be able to tell this thing to spit out
[01:55:13.520 --> 01:55:19.520]   something, you know, coherent. So what's wrong with it? Neil Clark wrote in a tweet,
[01:55:19.520 --> 01:55:23.360]   thread submissions are currently closed. It shouldn't be hard to guess why.
[01:55:23.360 --> 01:55:33.440]   He wrote a blog post concerning trend. Oh, it's not named after Arthur C. Clark. Neil Clark
[01:55:33.440 --> 01:55:38.720]   is the editor of Clark's world. So it's his. Oh, well, it's his man. I just assumed it was AC
[01:55:38.720 --> 01:55:45.680]   Clark. But no. So here's a question for our for our counselor here.
[01:55:45.680 --> 01:55:52.080]   Because a court has also said, I think more than once now that that that AI images cannot be
[01:55:52.080 --> 01:55:57.760]   copyrighted. Should I presume we'll come over to text? Is there an issue there around around
[01:55:57.760 --> 01:56:03.600]   copyright and and generative AI? The court decided that copyright required a human
[01:56:05.040 --> 01:56:10.480]   at some point in the chain, right? A copyright copyright stuff. Is that right, Kathy?
[01:56:10.480 --> 01:56:15.120]   Well, you're I think you're referring to the monkey selfie case, which said this per
[01:56:15.120 --> 01:56:19.840]   yes, the human has to be involved. It can't. And but that was one that looked at animals.
[01:56:19.840 --> 01:56:26.800]   This is a thing that a lot of people are kicking around because I think there's also a question.
[01:56:26.800 --> 01:56:34.320]   If there's a copyright, who would own it? The creator of the of the AI, the or the person who
[01:56:34.320 --> 01:56:40.320]   deployed the AI or both or neither. There's some interesting analysis to do on it. But then there's
[01:56:40.320 --> 01:56:44.320]   also the question of does anybody actually need to own it? Because one thing we learned from the
[01:56:44.320 --> 01:56:49.600]   Naruto cases, it's actually okay if a work is created and immediately in the public domain.
[01:56:49.600 --> 01:56:54.160]   We do not need to make everything ownable. And we tend to lose sight of that back every time we
[01:56:54.160 --> 01:57:00.080]   try to ask these questions. Right. But then if keys, if keys, Kathy, everything can be owned.
[01:57:00.080 --> 01:57:04.800]   Yeah, but ask me this on a day when on a week when I'm not like exhausted from the 230,
[01:57:04.800 --> 01:57:09.440]   I can only hold up so much of the universe at one in their 230 bubble. According to the US
[01:57:09.440 --> 01:57:15.280]   copyright offices, copyright compendium, quote, the office will refuse to register a claim if
[01:57:15.280 --> 01:57:21.680]   it determines that a human being did not create the work. So you can't register it. If it if it
[01:57:21.680 --> 01:57:29.440]   was copyright office just rejected a mid journey image for a for a.
[01:57:29.440 --> 01:57:34.320]   I think that I agree with you, Kathy, that actually is not a hazard, right? That's good.
[01:57:34.320 --> 01:57:39.680]   That's fine. I mean, the point was with copyright was copyright was supposed to be the exception.
[01:57:39.680 --> 01:57:44.960]   Yes. You were you just wanted to make sure that the public had stuff. So how much do you need to
[01:57:44.960 --> 01:57:48.800]   provide a limited monopoly for a limited amount of time just to make sure you've got enough
[01:57:48.800 --> 01:57:52.960]   incentive so people keep creating stuff? Because why do we want to people to create stuff?
[01:57:52.960 --> 01:57:57.840]   So we have stuff. And now all of a sudden it's the rule. And it's the rule that means that the
[01:57:57.840 --> 01:58:02.160]   public doesn't get stuff anymore. That's all backwards. We turned upside down, didn't we?
[01:58:02.160 --> 01:58:05.760]   We turned it upside down and we're completely defeating why we would have the monopoly in the
[01:58:05.760 --> 01:58:09.760]   first place. By the way, newspapers and magazines in the earliest copyright were not included.
[01:58:09.760 --> 01:58:15.440]   I mean, a lot wasn't included. Like, and the problem was as soon as they had like the first
[01:58:15.440 --> 01:58:18.960]   statute of man, then like everybody else showed up with like, well, what about
[01:58:18.960 --> 01:58:24.160]   in musts engravers? We engraves stuff. We should have protection for engraving. So then they had
[01:58:24.160 --> 01:58:28.560]   an protection for their engravings and then sort of every media claimed it for themselves. So,
[01:58:28.560 --> 01:58:33.120]   I mean, it's weird. If it looks like a privilege, everyone's going to want it. And it's really
[01:58:33.120 --> 01:58:39.360]   hard to sort of argue that some forms of media don't get it. But then media also changed dramatically
[01:58:39.360 --> 01:58:45.920]   under the weight of technology changes. We kind of need to go back and rethink this.
[01:58:45.920 --> 01:58:50.800]   I've got a book about that coming out, Kathy. It's a good chapter.
[01:58:50.800 --> 01:58:53.040]   So Gutenberg parenthesis.
[01:58:53.040 --> 01:59:02.160]   bit.ly/buygootenberg. Right? Did I get that right? I did that from memory.
[01:59:02.160 --> 01:59:04.160]   Yes, you did. Wow. In person.
[01:59:04.160 --> 01:59:05.200]   Yeah, it is.
[01:59:05.200 --> 01:59:09.280]   Well, you said it enough. I probably should have.
[01:59:09.280 --> 01:59:11.440]   That was all part of his plan.
[01:59:11.440 --> 01:59:14.480]   Just teasing. Actually, there is an interesting question.
[01:59:14.480 --> 01:59:18.320]   By the way, I'm thinking about going to Michigan when it's on press.
[01:59:18.320 --> 01:59:23.920]   Figuring to kind of watch it come off the press. Yeah, that would be so fun.
[01:59:23.920 --> 01:59:29.360]   Why don't you call in from there and show us?
[01:59:29.360 --> 01:59:31.200]   Yeah. Yeah, exactly.
[01:59:31.200 --> 01:59:35.840]   I'm standing next to a 5,000 man line at that.
[01:59:35.840 --> 01:59:37.840]   That's the stapler going through now.
[01:59:37.840 --> 01:59:41.200]   Trunk, Trunk, Trunk, Trunk.
[01:59:41.200 --> 01:59:47.200]   What happens if, okay, so the copyright office says we won't copyright it for machine made it.
[01:59:47.200 --> 01:59:54.560]   Does that in any way affect an artist who's suing because the AI was trained on it's
[01:59:54.560 --> 01:59:56.560]   on the on the artist's copyright.
[01:59:56.560 --> 01:59:58.480]   That's a different copyright question.
[01:59:58.480 --> 02:00:04.320]   Given the caliber of people who are defending the AI from the claims,
[02:00:04.320 --> 02:00:06.960]   I think those claims are not going to proceed.
[02:00:06.960 --> 02:00:08.800]   And I think they probably tenuous.
[02:00:08.800 --> 02:00:12.080]   And because one of the other things is you also have,
[02:00:12.080 --> 02:00:14.800]   I mean, there's also the interaction of the First Amendment,
[02:00:14.800 --> 02:00:17.920]   including the right to read and if you have the right to read,
[02:00:17.920 --> 02:00:19.440]   can you send your bot to read it?
[02:00:19.440 --> 02:00:24.640]   I think the ability to state a cognizable claim and certainly at the scale of a class
[02:00:24.640 --> 02:00:29.200]   is going to be a challenging thing, but it's not frivolous necessarily.
[02:00:29.200 --> 02:00:33.280]   I think it's a bad claim, but it's transformative.
[02:00:33.280 --> 02:00:35.760]   Right?
[02:00:35.760 --> 02:00:39.600]   I mean, it transforms the original work beyond almost beyond recognition.
[02:00:39.600 --> 02:00:44.560]   Well, there's also issues with the femoral copies of like what you've
[02:00:44.560 --> 02:00:48.240]   ephemerally copied in order to train up your chat.
[02:00:48.240 --> 02:00:54.560]   So the problem with a lot of digital technology as applied to copyright is the digital technology
[02:00:54.560 --> 02:00:59.040]   ends up having to essentially make a copy in order to use it in any form of way.
[02:00:59.040 --> 02:01:02.720]   That was the issue with cable and on-demand movies.
[02:01:02.720 --> 02:01:06.560]   I mean, it's an issue all the time that shows up in weird things.
[02:01:06.560 --> 02:01:11.200]   Again, copyright was questionable enough before we added digital technology.
[02:01:11.200 --> 02:01:16.080]   And it's really not getting applied in ways that scale without weird byproducts.
[02:01:16.080 --> 02:01:20.560]   So it's so easy and ephemerally quick to digitally copy something.
[02:01:21.200 --> 02:01:26.480]   Does World Warhol have an impact, do you think, on the AI image lawsuits?
[02:01:26.480 --> 02:01:30.960]   It I think it's entirely depends, but potentially, but it really depends on what.
[02:01:30.960 --> 02:01:38.640]   That decision is going to end up speaking probably very loudly about what the rules of the road are
[02:01:38.640 --> 02:01:44.000]   for fair use and depending on how they get articulated, it's going to have an effect.
[02:01:44.000 --> 02:01:46.800]   But what the effect is will depend on how it's articulated.
[02:01:46.800 --> 02:01:48.400]   It wasn't hugely transformed.
[02:01:48.400 --> 02:01:55.680]   It looks like the original photograph, but the defense is that the intent,
[02:01:55.680 --> 02:01:58.800]   the use was transformative, not the look.
[02:01:58.800 --> 02:02:04.480]   Well, for Warhol that it expressed something different than the first work had.
[02:02:04.480 --> 02:02:09.600]   If it's expressing something different, it doesn't violate the derivative work.
[02:02:09.600 --> 02:02:13.440]   It's essentially now the transformative works of fair use would end up covering it because
[02:02:13.440 --> 02:02:18.240]   it said something different that the original one did not and fair use is supposed to enable that.
[02:02:19.200 --> 02:02:24.880]   So it can be transformative, not merely in the way it looks, but in its intent and its use and its
[02:02:24.880 --> 02:02:28.160]   message. That is the question before the court.
[02:02:28.160 --> 02:02:33.520]   I mean, whether the court recognizes that that's the question before it is also currently an open
[02:02:33.520 --> 02:02:38.480]   question, but I think that's ultimately what the arguments pivoted on where the photographers were
[02:02:38.480 --> 02:02:46.400]   arguing that basically we had a picture of Warhol. You used the picture to like you,
[02:02:46.400 --> 02:02:51.200]   you served our market for a picture of Warhol. You got used by the magazine instead of ours,
[02:02:51.200 --> 02:02:55.920]   even though yours wouldn't have existed but for ours because you copied it in the process of making
[02:02:55.920 --> 02:03:00.080]   yours. And they're like, we had ours, you made a copy, and then you got the business.
[02:03:00.080 --> 02:03:05.040]   How can that possibly be that violates our exclusive rights and the original copyright
[02:03:05.040 --> 02:03:09.200]   into controlled derivative works? The other argument on the other side was,
[02:03:09.200 --> 02:03:14.160]   yeah, we started with your picture, but we ended up changing it to a point where ultimately,
[02:03:14.160 --> 02:03:17.920]   yeah, we were left with a picture, but we were left with a picture that said something very
[02:03:17.920 --> 02:03:22.640]   different about the subject matter than your picture did. And the reason why we got the business
[02:03:22.640 --> 02:03:28.480]   by the magazine was because what we said about Prince was different than what you said about
[02:03:28.480 --> 02:03:32.960]   Prince and the magazine editor thought that what we said matched his article much better.
[02:03:32.960 --> 02:03:37.440]   So we weren't even competing in the same market for it. But depending on how you frame,
[02:03:37.440 --> 02:03:42.960]   what was going on and what happened and why depends on maybe how you apply the Fair Use
[02:03:42.960 --> 02:03:46.880]   argument, but also on the table is how we even concoct the Fair Use test.
[02:03:46.880 --> 02:03:52.320]   And it's a little long. I think the second circuit got things very wrong, but it's not quite clear
[02:03:52.320 --> 02:04:01.120]   what the Supreme Court is going to do to fix that. And then Bernie Sanders walks into a TikTok.
[02:04:01.120 --> 02:04:09.760]   Very confused too, I might add. It seemed quite confused. Like, why is the
[02:04:09.760 --> 02:04:13.280]   doorman and this young woman? Why are they dancing in front of my own cell?
[02:04:13.280 --> 02:04:21.360]   It's too bad he wasn't wearing mittens. He's kind of embraced that whole thing, hasn't he?
[02:04:21.360 --> 02:04:24.960]   Oh, yeah. The Bernie mittens. Yeah, I saw a story about that. Yeah.
[02:04:24.960 --> 02:04:28.320]   It's time, Kathy, you probably don't know about this.
[02:04:28.320 --> 02:04:33.360]   It's time for our Tik. I know everything. How dare you say that.
[02:04:33.360 --> 02:04:41.520]   It's our TikTok segment. New the Mercedes 24, 2024 eClass will not only have angry birds,
[02:04:41.520 --> 02:04:47.440]   you'll be able to take zoom cars, calls, and you'll be able to watch TikTok.
[02:04:47.440 --> 02:04:54.000]   Oh, make TikTok. Is it fake TikTok? No, make TikTok.
[02:04:54.000 --> 02:04:58.880]   Software in the car enables the installation of apps like Zoom, WebEx, and TikTok.
[02:05:00.080 --> 02:05:05.280]   Oh, yeah, I presume, I guess you could film TikTok videos, but only when the car is stopped,
[02:05:05.280 --> 02:05:10.640]   according to Mercedes Benz. And that's our TikTok.
[02:05:10.640 --> 02:05:20.800]   Jeff says, if only this if only this were a Taco Bell subway plans electric car charging
[02:05:20.800 --> 02:05:24.720]   oasis with a little park, a little playground. I've always seen more of this already.
[02:05:25.280 --> 02:05:30.560]   The fact that Starbucks hasn't decided to be the place where you buy the coffee and charge your
[02:05:30.560 --> 02:05:35.040]   car. Charger car. Yeah, perfect sense. Instead, they're pretty charging stations. The farthest
[02:05:35.040 --> 02:05:40.640]   reach of a parking lot where you're not going to go buy something in the store. It just makes
[02:05:40.640 --> 02:05:46.880]   perfect sense. Yeah. That's pretty smart. All right, let's do the Google change log and get the
[02:05:46.880 --> 02:05:55.120]   hell out of Dodge here. The Google change log. Kathy wants waffles. I what? No, it doesn't. She
[02:05:55.120 --> 02:06:00.960]   doesn't know it. She doesn't know it, but she wants waffles. Kathy deserves a beer with her new beer
[02:06:00.960 --> 02:06:09.280]   with a new buddy Brett on the court. Yeah. Yeah. Hey. You know, I go there you go, or something like
[02:06:09.280 --> 02:06:15.840]   an 18 year old McCallen maybe. I think I prefer the waffles quite frankly. The waffles are better,
[02:06:15.840 --> 02:06:20.720]   honestly, than any possible alcoholic beverage. But it has to be a good recipe for waffles. I don't
[02:06:20.720 --> 02:06:26.160]   like a bad recipe. I make my waffles the night before so they can rise. Is that a good recipe?
[02:06:26.160 --> 02:06:32.800]   Smart. Wow. I don't know. I use a recipe that my mom had that like it was very buttery and it
[02:06:32.800 --> 02:06:37.840]   can be pancakes or it can be waffles, but it doesn't have that graininess. It's got a very buttery taste
[02:06:37.840 --> 02:06:43.280]   and it's not about the fluff. It's about the buttery goodness. It's not about the fluff. It's
[02:06:43.280 --> 02:06:49.200]   about the buttery goodness. Yeah. I think it's about the crunch. But I have to say every time I
[02:06:49.200 --> 02:06:53.520]   make waffles, yes, part of one of the ingredients is melted butter. So I think that's probably a
[02:06:53.520 --> 02:07:00.240]   part of it. Yeah. Hey, everybody, Leo LaPorte here. I am the founder and one of the hosts at the Twit
[02:07:00.240 --> 02:07:05.280]   podcast network. I want to talk to you a little bit about what we do here at Twit because I think
[02:07:05.280 --> 02:07:14.480]   it's unique. And I think for anybody who is bringing a product or a service to a tech audience,
[02:07:14.480 --> 02:07:20.400]   you need to know about what we do here at Twit. We've built an amazing audience of engaged,
[02:07:20.400 --> 02:07:27.920]   intelligent, affluent listeners who listen to us and trust us when we recommend a product. Our
[02:07:27.920 --> 02:07:32.480]   mission statement is Twit is to build a highly engaged community of tech enthusiasts.
[02:07:33.840 --> 02:07:39.440]   Already, your ears should be perking up at that because highly engaged is good for you.
[02:07:39.440 --> 02:07:43.920]   Tech enthusiasts, if that's who you're looking for, this is the place. We do it by offering them
[02:07:43.920 --> 02:07:49.440]   the knowledge they need to understand and use technology in today's world. And I hear from our
[02:07:49.440 --> 02:07:54.640]   audience all the time. Part of that knowledge comes from our advertisers. We are very careful.
[02:07:54.640 --> 02:08:01.840]   We pick advertisers with great products, great services with integrity and introduce them
[02:08:01.840 --> 02:08:08.560]   to our audience with authenticity and genuine enthusiasm. And that makes our host red ads
[02:08:08.560 --> 02:08:14.880]   different from anything else you can buy. We are literally bringing you to the attention of our
[02:08:14.880 --> 02:08:22.080]   audience and giving you a big fat endorsement. We like to create partnerships with trusted brands,
[02:08:22.080 --> 02:08:28.400]   brands who are in it for the long run, long term partners that want to grow with us. And we have
[02:08:28.400 --> 02:08:35.280]   so many great success stories. Tim Broome, who founded ITProTV in 2013, started advertising
[02:08:35.280 --> 02:08:41.840]   with us on day one has been with us ever since. He said, quote, we would not be where we are today
[02:08:41.840 --> 02:08:48.160]   without the Twit network. I think the proof is in the pudding. Advertisers like ITProTV and Audible
[02:08:48.160 --> 02:08:54.240]   that have been with us for more than 10 years. They stick around because their ads work. And
[02:08:54.240 --> 02:09:00.320]   honestly, isn't that why you're buying advertising? You get a lot with Twit. We have a very full service
[02:09:00.320 --> 02:09:06.560]   attitude. We almost think of it as kind of artisanal advertising, boutique advertising. You'll get a
[02:09:06.560 --> 02:09:12.640]   full service continuity team. People who are on the phone with you, who are in touch with you,
[02:09:12.640 --> 02:09:19.520]   who support you from with everything from copywriting to graphic design. So you are not alone in this.
[02:09:20.080 --> 02:09:26.560]   We embed our ads into the shows. They're not added later. They're part of the shows. In fact,
[02:09:26.560 --> 02:09:31.600]   often they're such a part of our shows that our other hosts will chime in on the ad saying,
[02:09:31.600 --> 02:09:37.200]   yeah, I love that. Or just the other day, one of our hosts said, man, I really got to buy that.
[02:09:37.200 --> 02:09:43.600]   That's an additional benefit to you because you're hearing people, our audience trusts saying,
[02:09:43.600 --> 02:09:49.600]   yeah, that sounds great. We deliver, always over deliver on impressions. So you know you're
[02:09:49.600 --> 02:09:55.200]   going to get the impressions you expect. The ads are unique every time. We don't pre-record
[02:09:55.200 --> 02:10:00.480]   them and roll them in. We are genuinely doing those ads in the middle of the show. We'll give you
[02:10:00.480 --> 02:10:06.400]   great onboarding services, ad tech with pod sites that's free for direct clients. Gives you a
[02:10:06.400 --> 02:10:10.720]   lot of reporting gives you a great idea of how well your ads are working. You'll get courtesy
[02:10:10.720 --> 02:10:15.520]   commercials. You actually can take our ads and share them across social media and landing pages.
[02:10:15.520 --> 02:10:20.080]   That really extends the reach. There are other free goodies too, including mentions in our weekly
[02:10:20.080 --> 02:10:25.680]   newsletter that sent to thousands of fans, engaged fans who really want to see this stuff.
[02:10:25.680 --> 02:10:32.400]   We give you bonus ads and social media promotion too. So if you want to be a long-term partner,
[02:10:32.400 --> 02:10:40.000]   introduce your product to a savvy, engaged tech audience, visit twit.tv/advertise. Check out
[02:10:40.000 --> 02:10:44.880]   those testimonials. Mark McCrary is the CEO of Authentic. You probably know him, one of the
[02:10:44.880 --> 02:10:51.760]   biggest original podcast advertising companies. We've been with him for 16 years. Mark said,
[02:10:51.760 --> 02:10:56.560]   "The feedback from many advertisers over 16 years across a range of product categories.
[02:10:56.560 --> 02:11:03.200]   Everything from razors to computers is that if ads and podcasts are going to work for a brand,
[02:11:03.200 --> 02:11:08.480]   they're going to work on Twitch shows. I'm very proud of what we do because it's honest,
[02:11:08.480 --> 02:11:14.640]   it's got integrity, it's authentic, and it really is a great introduction to our audience
[02:11:14.640 --> 02:11:21.680]   of your brand. Our listeners are smart, they're engaged, they're tech savvy, they're dedicated
[02:11:21.680 --> 02:11:27.040]   to our network. That's one of the reasons we only work with high-integrity partners that we've
[02:11:27.040 --> 02:11:32.400]   personally and thoroughly vetted. I have absolute approval on everybody. If you've got a great
[02:11:32.400 --> 02:11:38.400]   product, I want to hear from you. Elevate your brand by reaching out today at advertise@twit.tv.
[02:11:38.800 --> 02:11:43.920]   Break out of the advertising norm. Grow your brand with host red ads on twit.tv.
[02:11:43.920 --> 02:11:51.440]   Visit twit.tv/advertise for more details or you can email us at advertise@twit.tv
[02:11:51.440 --> 02:11:55.600]   if you're ready to launch your campaign now. I can't wait to see your product. Give us a ring.
[02:11:55.600 --> 02:12:03.440]   Google messages is finally acknowledging its heritage and just saying, "Look, it's an RCS
[02:12:03.440 --> 02:12:10.480]   message." I want to be every product every six months. I want to I want to
[02:12:10.480 --> 02:12:16.960]   spurn this. This was terrible. This was an auto update that like, "Oh, update your text message.
[02:12:16.960 --> 02:12:20.240]   I like to keep my apps updated because they're going to be more secure." And all of a sudden,
[02:12:20.240 --> 02:12:26.160]   without me knowing or realizing the notification icon for text messages has changed. It's no longer
[02:12:26.160 --> 02:12:30.880]   the one I recognize. It is now something else and the something else looks exactly like what it
[02:12:30.880 --> 02:12:35.360]   looks like when I get a Twitter message. I no longer can tell when I have a text or I have an Elon
[02:12:35.360 --> 02:12:39.680]   Musk notification that got rammed down my throat. This is a terrible state of affairs.
[02:12:39.680 --> 02:12:44.720]   Everything is terrible. What are the tech companies doing? We got them out of her bubble.
[02:12:44.720 --> 02:12:53.680]   Give us a great waffle. No, we're screwed up. They now call it RCS chat instead of just Google
[02:12:53.680 --> 02:13:01.920]   chat. You'll know that you're getting rich text messages. That's not the icon to use.
[02:13:01.920 --> 02:13:07.920]   Changing the icon on the app is a terrible idea from a usability perspective anyway. To change it
[02:13:07.920 --> 02:13:13.040]   to something that looks awfully close to the notification icon of another major messaging service
[02:13:13.040 --> 02:13:17.840]   is a really dumb idea. It's not that I think that trademark lawsuit should be let loose and
[02:13:17.840 --> 02:13:22.720]   to fly whatever, but I don't know if I completely feel like if a lawn wants to pick a fight that
[02:13:22.720 --> 02:13:27.840]   I would necessarily disagree with this one. Wow. This is the most exciting change like we've ever had.
[02:13:27.840 --> 02:13:32.400]   Yeah. Yes. You showed me the icon and I just saw red.
[02:13:32.400 --> 02:13:43.520]   Kathy fired up. Let's see how she feels about this. Google has finally decided to unite tasks
[02:13:43.520 --> 02:13:48.400]   and reminders. They'll automatically migrate reminders created in the Google Assistant and
[02:13:48.400 --> 02:13:54.480]   calendar apps all into Google tasks. One task either of those to rule them all. I know.
[02:13:54.480 --> 02:14:00.560]   I know. Yeah. I actually just do. I mean, as long as this remains irrelevant to my interest,
[02:14:00.560 --> 02:14:04.640]   I don't care. Oh, good. It's if it gets rammed down my throat and becomes relevant to my interest
[02:14:04.640 --> 02:14:07.680]   that I'm going to care. Here's one. It's not going to be a happy caring. Here's when you will not
[02:14:07.680 --> 02:14:16.480]   care about but Jeff might. Chrome OS 110 is rolling out with super resolution audio. Select to speak
[02:14:16.480 --> 02:14:24.560]   channel labels and more. Okay. I actually have the new Acer Chromebook,
[02:14:24.560 --> 02:14:30.800]   which is ironically designed for gaming, Jeff. And I will be reviewing it, giving it a demo on
[02:14:30.800 --> 02:14:35.520]   Sunday on Ask the Tech guys. And what I wonder about is the is the HP.
[02:14:35.520 --> 02:14:40.080]   Blah, blah, blah, blah, blah, the Chrome of pro. Okay.
[02:14:40.080 --> 02:14:45.760]   HP high level Dragonfly pro Chromebook. Ooh, Dragonflies. Those are nice.
[02:14:45.760 --> 02:14:53.200]   Yeah. I like the my Google Pixelbook go because only one they replaced it with it for broke
[02:14:53.200 --> 02:14:59.600]   once is only a year old. The microphone just died. You which makes the perfect machine you'd
[02:14:59.600 --> 02:15:06.480]   like me to do for this podcast so you can't ever hear me. This is nice. It's light. 13 and a half
[02:15:06.480 --> 02:15:12.000]   inch screen. Oh, 1250. This is the five. This is the Dragonfly. There's a new one coming out.
[02:15:12.000 --> 02:15:16.800]   That's the Dragonfly pro. That's going to cost more than $1,255.
[02:15:16.800 --> 02:15:23.200]   This is too much. That's too much. No, don't pay that much for a Chromebook.
[02:15:23.200 --> 02:15:28.560]   No, I know. I know. I'd say the Acer, which is about 600 bucks is all even then feels a little
[02:15:28.560 --> 02:15:33.280]   tinny and cheesy. And what's the model? What is it? I don't remember the exact model, but it's
[02:15:33.280 --> 02:15:40.880]   their gaming Chromebook, which is funny, which is hysterical. But the idea is it has it can get
[02:15:40.880 --> 02:15:47.360]   up to an I five in it. But mostly, I mean, really, the whole point these days is to stream games
[02:15:47.360 --> 02:15:52.640]   through services like Xbox, Game Pass. Google should start a service to do that.
[02:15:52.640 --> 02:16:05.840]   G. Yeah. You can get a 2020 M1 MacBook Pro for 1200. And it runs Windows. Yes, it does.
[02:16:05.840 --> 02:16:15.360]   The two things I don't want. Yeah. It's the Chromebook 516 GE. And it has, you know, it's
[02:16:15.360 --> 02:16:21.600]   for gaming because the ASDF keys are specially highlighted. It's really. Of course. It's a
[02:16:21.600 --> 02:16:25.280]   real thing. I feel like they should just finish spelling my name. They shouldn't just start in
[02:16:25.280 --> 02:16:30.640]   that. Just two letters. They should keep going. G E L L I S the Chromebook 516.
[02:16:30.640 --> 02:16:35.040]   Gellis. Yeah. We're talking. Yeah. Now we're talking.
[02:16:35.040 --> 02:16:41.600]   It's like G four. I actually have a G fours now subscription. So I will, I will up it to the
[02:16:41.600 --> 02:16:47.360]   120 Hertz version. That's the big thing, right? 120 Hertz screen. You know, if you could play
[02:16:47.360 --> 02:16:54.240]   great games on it, why the heck not? Have you played Pentement? I have. People really excited
[02:16:54.240 --> 02:17:00.000]   about that mostly because it looks like a medieval book. Yeah, which is not what normally people
[02:17:00.000 --> 02:17:07.680]   are looking for in a video game. But yeah, I mean, it's kind of cool. It's kind of my period.
[02:17:07.680 --> 02:17:14.080]   Kootenberg. It's Kootenberg. You want to see a little bit of it? Sure. Here is a, here's a
[02:17:14.080 --> 02:17:20.000]   YouTube video. Watch out blood gourst sexual things. And strong language and fonts too.
[02:17:20.000 --> 02:17:22.960]   In smoking from obsidian.
[02:17:22.960 --> 02:17:30.720]   This is all up your alley, Jeff. Oh, sure is. It was a, with the monks in this script.
[02:17:30.720 --> 02:17:36.800]   Oh, look, go run. We all find expensive gaming.
[02:17:36.800 --> 02:17:43.280]   Yo, Han has invented the printing. Look at that. He's hitting it with a round mallet.
[02:17:43.280 --> 02:17:54.880]   Oh, no. Coach said one door. Holy. Oh, what? I will begin by inspecting his humus.
[02:17:54.880 --> 02:18:00.720]   Lord, they're burning witches. Where is Miss Briana?
[02:18:04.400 --> 02:18:10.080]   There's make out sessions in the library with nuns. What are you doing?
[02:18:10.080 --> 02:18:14.480]   They really, they're really sexed it up, aren't they?
[02:18:14.480 --> 02:18:22.400]   Kaspar. What was thought? So it's the name of the rose as a video game, basically.
[02:18:22.400 --> 02:18:32.640]   All right. Thank you. This is a whole pentamice. Yeah. Back to the Google change log.
[02:18:32.640 --> 02:18:39.760]   Oh, the game that attracts me. And that's a horrible game. I'm going to spend thousands of dollars
[02:18:39.760 --> 02:18:45.040]   for a rig to play beautiful games. It's no cuphead. I'll tell you that right now.
[02:18:45.040 --> 02:18:49.200]   Cool. Google weather. You'll be thrilled about this, Jeff.
[02:18:49.200 --> 02:18:55.920]   Gets an accidental dark theme. But it's an accident via Android system.
[02:18:55.920 --> 02:19:03.280]   Web view. Always an accident. Dark is always an accident. Well, that's a dark pattern.
[02:19:03.280 --> 02:19:09.360]   Yeah, dark pattern. Look at that. That looks great. I have my weather is dark. I like dark weather.
[02:19:09.360 --> 02:19:15.600]   I don't want I don't want my eyes to be hurt by a bright light in my eyes. I don't understand why
[02:19:15.600 --> 02:19:29.040]   you do, Jeff. The vampire demographic. Oh, yeah. I like it dark, man. Google Chrome's latest version
[02:19:29.040 --> 02:19:34.800]   includes tools to address its memory hog problem, memory saver and energy saver modes.
[02:19:34.800 --> 02:19:39.680]   Now available on Chrome Chrome 110. It's always been like the case on the Mac that the
[02:19:40.320 --> 02:19:46.640]   Chrome has just been a pig. I guess it just sleeps the tabs, mainly. Okay, that's the biggest.
[02:19:46.640 --> 02:19:50.400]   It now snoozes Chrome tabs that aren't currently in use.
[02:19:50.400 --> 02:19:56.800]   Energy saver. Similar limits. Any unnecessary background website activity,
[02:19:56.800 --> 02:20:02.160]   such as visual effects like smooth scrolling on animations or videos. In other words, it looks
[02:20:02.160 --> 02:20:07.600]   like crap, but hey, your battery is going to survive. And that's the Google change line.
[02:20:09.920 --> 02:20:11.280]   What do you look at the mean for?
[02:20:11.280 --> 02:20:21.840]   All right, kids, real quickly before the picks of the week, I just want to plug. We mentioned
[02:20:21.840 --> 02:20:27.680]   times or time like and sunglasses. Look at that. You got the Tom Cruise stand daylight.
[02:20:27.680 --> 02:20:32.480]   Yeah. Look at that. He's always dark. I'm going on here. So I got to protect my eyes.
[02:20:32.480 --> 02:20:35.680]   Is it your Joe Biden imitation? No, sir.
[02:20:36.960 --> 02:20:46.240]   It doesn't have an ice cream cone. Our club is helmed by this good looking fellow over here,
[02:20:46.240 --> 02:20:50.880]   Mr. Ann Pruitt. He is our community manager. It does a great job of making it fun.
[02:20:50.880 --> 02:20:57.280]   It is fun in so many ways. First of all, it originally was designed for ad-free versions
[02:20:57.280 --> 02:21:04.400]   of all of our shows. And you still get that. And you also get shows that we don't put out
[02:21:04.400 --> 02:21:09.600]   anywhere else like hands on Macintosh with Micah and Paul Thorett's hands on windows.
[02:21:09.600 --> 02:21:13.920]   You get the untitled Linux show. So there's a lot of extra content. If we do launch new shows,
[02:21:13.920 --> 02:21:17.280]   they're going to launch in the club first. In fact, we're talking about this week in AI or
[02:21:17.280 --> 02:21:22.080]   something like that. That would be in the club first. You like that idea? Yeah.
[02:21:22.080 --> 02:21:30.160]   You want to be on that one? Can I play? Sure. I play. Yeah. I want to find somebody who's
[02:21:30.160 --> 02:21:34.480]   like a great AI expert in there. Well, you know, I sent somebody to you and Jason.
[02:21:34.480 --> 02:21:39.200]   Oh, OK. I love woman. Oh, yes. Really, really. She'd be great. I saw that. That's right.
[02:21:39.200 --> 02:21:43.680]   Thank you for my isn't she. She's really good at explaining. She's very straightforward.
[02:21:43.680 --> 02:21:46.160]   That's what I need. She's young. You know,
[02:21:46.160 --> 02:21:52.960]   Yeah, then it's us out as old guys. We need another show where it's two old guys and a young woman.
[02:21:52.960 --> 02:21:58.160]   Do we really need that in the world? All right. There are enough shows like that.
[02:21:58.800 --> 02:22:04.560]   Anyway, if you join the club, you might get to hear that show. You certainly get to hear
[02:22:04.560 --> 02:22:09.280]   stuff you don't hear anywhere else. You also get the Twit Plus feed, which is all sorts of
[02:22:09.280 --> 02:22:14.880]   stuff that we don't put out as a regular podcast. But the best part, I think, is the discord. I love
[02:22:14.880 --> 02:22:21.360]   the Twit Plus discord because it's actually the bestest social community ever, of course.
[02:22:21.360 --> 02:22:29.120]   And it's meme-tastic. Meme-tastic. But of course, you know, the original part was
[02:22:29.120 --> 02:22:33.440]   just kind of like our chat room. It's all the shows have their own chat going on. This is the
[02:22:33.440 --> 02:22:41.760]   this week in Google chat. But but we also have events, Sam Abble, Sammett coming up in a couple
[02:22:41.760 --> 02:22:47.520]   of weeks, Stacy's book club, Victor Bachnot. And you've just added, oh, wow, Alex Wilhelm is
[02:22:47.520 --> 02:22:54.560]   going to do a chat. May 11th. Good get. I love Alex. He's been a little busy. If you get a new
[02:22:54.560 --> 02:23:00.320]   baby, I'm excited to get his get lies on as well with him. That'd be great. I'll look forward to
[02:23:00.320 --> 02:23:06.800]   that May 11th. So we do events. We have discussions, not just about the shows, but I mean, there are
[02:23:06.800 --> 02:23:12.880]   sections in this court for every possible geek topic under the sun, beer, wine, cocktails,
[02:23:12.880 --> 02:23:18.640]   autos, coding and comics and gaming and hacking and ham radio and pets and sci-fi.
[02:23:18.640 --> 02:23:29.280]   He gets to do that. And and all of this, all of this goodness for less than seven bucks, well,
[02:23:29.280 --> 02:23:36.560]   not less actually exactly seven bucks a month. You know what? I had my inflation.
[02:23:36.560 --> 02:23:41.360]   It'll actually be less than a bunch. I am mildly sick because she, you know, I said, well,
[02:23:41.360 --> 02:23:48.240]   she would make it 699. She says, come on, seven bucks a month, seven bucks, not 699.
[02:23:48.240 --> 02:23:55.520]   So yeah, but with inflation, it's it's less anyway. Please join the club. It helps us out is the
[02:23:55.520 --> 02:24:00.400]   best way going forward. And I promise, unlike NPR, we're not going to come down on your
[02:24:00.400 --> 02:24:07.680]   copyrights or anything like that. We are here to support open RSS supported podcasting.
[02:24:08.640 --> 02:24:13.840]   And unfortunately, the world is not exactly beating a path to our door. You can help.
[02:24:13.840 --> 02:24:16.800]   If you listen to the shows, if you like the shows, if you want to keep them on the air,
[02:24:16.800 --> 02:24:25.520]   twit.tv/clubtwit. Thank you very much. Kathy, you don't normally have to do anything on this
[02:24:25.520 --> 02:24:30.240]   show. You've already done more than enough. Yes, but I do like your picks every time you give
[02:24:30.240 --> 02:24:38.080]   us a pick, it's wacky. Did I do this one before? I like, this is new. I must have been a dream
[02:24:38.080 --> 02:24:43.440]   because I thought I did, but I was discussing it before and we couldn't find any record of it.
[02:24:43.440 --> 02:24:50.480]   So yes, okay, it'll happen now. This is an Italian. What? Detectives show?
[02:24:50.480 --> 02:24:58.720]   Yeah. So the only streaming network that I happily subscribe to is a network called MHC.
[02:24:58.720 --> 02:25:05.280]   And what they did, and there's some history about it, is they find TV shows that are popular
[02:25:05.280 --> 02:25:10.560]   around the world and they dub them. And then I found them originally because they used to be
[02:25:10.560 --> 02:25:16.400]   broadcast on KCSM locally, one of the local public broadcasting stations. And they used to have
[02:25:16.400 --> 02:25:22.800]   Monday evening mystery night and they were showing some of these programs that they picked up
[02:25:22.800 --> 02:25:27.520]   from around the world that they had dubbed. And one of them that was really popular is the
[02:25:27.520 --> 02:25:35.200]   Inspector Montalbano Mysteries. And it's about a Sicilian police detective. And it's based on
[02:25:35.200 --> 02:25:41.840]   books. There's a guy who I think started writing his mystery series in his 80s and he's just
[02:25:41.840 --> 02:25:47.120]   churned them out like one a year. And so for 20 years, they've been making a television production
[02:25:47.120 --> 02:25:54.400]   of the Inspector Montalbano Mysteries against it is, boy, the woman and live away with a portray
[02:25:54.400 --> 02:26:01.520]   woman. And it is and all that great. But it's Italian, whatever. But the male characters are
[02:26:01.520 --> 02:26:07.440]   extremely well cast, extremely well acted. The plots are interesting enough, but the characters
[02:26:07.440 --> 02:26:12.240]   are so personable that you just really warm to them. So I started watching it years and years
[02:26:12.240 --> 02:26:17.040]   ago, like 15 years ago, and the shows been made since the late 90s and still with making them up
[02:26:17.040 --> 02:26:25.840]   through 2020, at least. And I binged it recently. And it was a good binge. It was just it's actually
[02:26:25.840 --> 02:26:32.320]   novels. So some people are fans of the novels, but the television production is really interesting.
[02:26:32.320 --> 02:26:37.360]   And it's Italian. And it's just nice to sort of like watch the foreign television and just get
[02:26:37.360 --> 02:26:42.400]   some insight into the way, you know, people in those countries are turning on their TV and
[02:26:42.400 --> 02:26:45.760]   settling in and watching something. It's nice to kind of see what they're settling into.
[02:26:45.760 --> 02:26:50.080]   Do you prefer it with subtitles or dubbed subtitles? Yeah,
[02:26:50.080 --> 02:26:55.120]   dubbing is always terribly. Yeah, it's always terrible. You don't get good acting.
[02:26:55.120 --> 02:27:00.640]   Sometimes it can be good. Like, I remember seeing some documentaries about how some of the
[02:27:00.640 --> 02:27:05.120]   major motion pictures when I was living in France and the way they dubbed them for France, like,
[02:27:05.120 --> 02:27:10.400]   it was as much part of the production to get the casting right, the dialogue right,
[02:27:10.400 --> 02:27:18.000]   and the synchronization right to do it. So you can do it well, but and you can do it well where
[02:27:18.000 --> 02:27:24.960]   the voice acting has as much authority for the character acting as the original person. But in
[02:27:24.960 --> 02:27:29.200]   general, like, that takes a lot of money. It doesn't normally happen. I'd rather read it.
[02:27:29.200 --> 02:27:33.920]   And also sort of, especially if I slightly know the language, it kind of gives me some clues about
[02:27:33.920 --> 02:27:37.440]   what changed in the translation and what might have been lost or what might have been gained.
[02:27:38.560 --> 02:27:45.120]   Especially like the MHC networks also does stuff in French and my French is better than I think.
[02:27:45.120 --> 02:27:49.120]   And it's kind of interesting to sort of hear what they're saying and read what the translation
[02:27:49.120 --> 02:27:53.200]   is and figure out what the disconnects are. And also it helped my French to talk to some stuff.
[02:27:53.200 --> 02:28:00.160]   Go away to learn a language, really. The 37 episodes of Montalbano. So there's plenty to watch.
[02:28:00.160 --> 02:28:06.560]   I'm gonna I've never heard of MHC. I'm gonna check it out. That's cool. Yeah, that's cool.
[02:28:07.600 --> 02:28:11.040]   MHC choice.com.
[02:28:11.040 --> 02:28:17.440]   Like MHC seems to be sort of a parent initiative of which a variety of sub-initatives happened.
[02:28:17.440 --> 02:28:23.840]   But I think MHC networks is the streaming channel. But it's unique content where it was for something
[02:28:23.840 --> 02:28:29.760]   else where everybody's competing for which back catalog of American programming everybody's got.
[02:28:29.760 --> 02:28:34.000]   But this is stuff that you wouldn't normally see. Somebody went out and did it. And their
[02:28:34.000 --> 02:28:40.160]   translations are excellent. Like actually from Montalbano, there's points where there's one character
[02:28:40.160 --> 02:28:45.680]   who's who has actually I don't know the word I'm a which is very ironic right now. Malapropism's
[02:28:45.680 --> 02:28:51.760]   Malaprop's where you okay. And he gets things very wrong all the time and the translations are
[02:28:51.760 --> 02:28:56.480]   brilliant because he's getting it wrong in Italian. But the way it's getting translated,
[02:28:56.480 --> 02:29:01.040]   he's now getting it wrong in English in a way that actually makes sense connected to the error he
[02:29:01.040 --> 02:29:05.920]   made. It's not a word for word match up, but it gets like the gist of it. And that's how you
[02:29:05.920 --> 02:29:13.280]   do translations where you really capture the gist and you make it work. So yeah, it's a quality
[02:29:13.280 --> 02:29:17.120]   production. And I think Jeff will appreciate this. There's Italian food.
[02:29:17.120 --> 02:29:27.600]   So Montalbano's favorite food is what do you call it? Aran Chiana. I love those. Yeah.
[02:29:27.600 --> 02:29:34.720]   But this looks even better. This looks so good. Yeah. Oh my god. Oh my god.
[02:29:34.720 --> 02:29:41.680]   He has a housekeeper who Adelina who who cooks for him and leaves him behind. So it's totally
[02:29:41.680 --> 02:29:48.160]   sexist, but the food's good. It's totally sexist. The food's good. It's charming. And I don't know.
[02:29:48.160 --> 02:29:52.400]   It's it's a good entertainment. And it's something you're not going to bump into normally. Adelina
[02:29:52.400 --> 02:29:58.240]   makes the pasta and casiada that they're eating. And now I want both. Oh, here's a recipe.
[02:29:58.240 --> 02:30:05.040]   Love this. Look at that. You can make it yourself. I might I might make this and we're going to have
[02:30:05.040 --> 02:30:12.560]   a viewing session. We'll invite some people over. Actually, subscribe to Britbox for the same reason
[02:30:12.560 --> 02:30:16.400]   because it gives you at least a little variety. This looks even better. I can't wait. I'm going to
[02:30:16.400 --> 02:30:22.880]   do this right now. Thank you. Mr. Jeff Jarvis, do you have a number? Well, we could do a little
[02:30:22.880 --> 02:30:32.000]   URL moment. Okay. In court, new how to spell it. If you go to AI.com now, guess where it redirects.
[02:30:32.000 --> 02:30:42.400]   Where? One guess. Apple Google. Nope. Let me go there. Don't say it. Well, not quite. AI.com.
[02:30:43.600 --> 02:30:50.000]   And please stand by. Welcome to chat. GPT. Oh boy. They reported possible 10 million
[02:30:50.000 --> 02:30:56.560]   dollar purchase that you are all but 10 million buckaroos. But they got they got they got they got
[02:30:56.560 --> 02:31:01.360]   Elon Musk money and now Microsoft. Yeah. So that's one you are all the other you were all
[02:31:01.360 --> 02:31:06.960]   interested in around across an angry post this week. Yeah. That said never use you are all
[02:31:06.960 --> 02:31:12.240]   shortening. Oh, you see why? Because Guardian used to be original brand for Guardian was Guardian
[02:31:12.240 --> 02:31:18.800]   unlimited and they use G you calm as their shortening. Right. Well, now because those two letter
[02:31:18.800 --> 02:31:26.240]   URLs can sell for a lot of money. Guardian is reportedly looking to sell for two and a half
[02:31:26.240 --> 02:31:31.840]   million dollars. G you calm. Oh, don't. Well, all of those links that existed in the past is shortened.
[02:31:31.840 --> 02:31:40.240]   Bye bye. Yeah. That is a problem. There's also the problem of a can hide malware because you
[02:31:40.240 --> 02:31:46.560]   don't see the actual URL and so URL. I stupidly with what Google do I said for the footnotes,
[02:31:46.560 --> 02:31:50.160]   do we have to have these long URLs? Are we just short numbers? Sorry. No, no, no, no,
[02:31:50.160 --> 02:31:56.640]   because they can change. Good. Good. Yeah. So the one time I like the shortened ones is when I'm
[02:31:56.640 --> 02:32:02.640]   especially for the ones that are kind of long garbage because I use URLs and briefs, it's kind of
[02:32:02.640 --> 02:32:08.320]   nice to have a cleaner version where the court can get to it without me taking up all the characters
[02:32:08.320 --> 02:32:15.360]   for something that's kind of illegible. But I mean, if you're selling the thing, couldn't you sell it
[02:32:15.360 --> 02:32:19.920]   with an encumbered transfer where you have to keep a database of the of the URL? You know,
[02:32:19.920 --> 02:32:23.920]   honestly, since the Guardian is digital, right? Is there a print version?
[02:32:23.920 --> 02:32:29.200]   No, the Guardian? Yeah. But did they use the shortened URLs in the print version? You can't fix
[02:32:29.200 --> 02:32:33.520]   those. I don't know. Well, it's not. Yeah, I don't know. You could certainly on every page on the
[02:32:33.520 --> 02:32:39.760]   internet that the Guardian uses GU, you could fix that permanently. That wouldn't be a big deal.
[02:32:39.760 --> 02:32:45.920]   That's a simple thing to do. But the print version, yeah, you're stuck if you use them in the print
[02:32:45.920 --> 02:32:55.040]   version. Yeah. Yep. Mr. Ant Pruitt, what's your pick this week? My pick. I've spoken about
[02:32:55.040 --> 02:33:02.240]   D so it's before, but Boris FX and particle illusion has just recently recently, like maybe two weeks
[02:33:02.240 --> 02:33:09.680]   ago updated their particle illusion. Is this how you did the oral panic? Yeah. Yeah. Sure.
[02:33:09.680 --> 02:33:15.360]   You can use this app to make particles like what they just showed there on the screen.
[02:33:15.360 --> 02:33:22.400]   You can use it in conjunction with video editors such as Premiere and resolve and Final Cut so
[02:33:22.400 --> 02:33:28.240]   forth. But the standalone version like what was here is free. The plugins you have to pay for that,
[02:33:28.240 --> 02:33:35.600]   but the standalone works really good. That's cool. And they also have a tutorial service to get
[02:33:35.600 --> 02:33:40.240]   you started for free on YouTube. So I'll link to that because I think it's a pretty good deal for
[02:33:40.240 --> 02:33:46.800]   the stuff that you can create with it for free. Well, this used to be a very expensive flame box
[02:33:46.800 --> 02:33:52.080]   that TV stations would use and have to hire a very expensive artist and all of that stuff.
[02:33:52.080 --> 02:33:57.120]   Yep. It's gotten much, it's amazing to put on the desktop and do it yourself now for free.
[02:33:58.080 --> 02:34:03.200]   That's incredible. You know, Boris FX. I'm trying to make it a little bit more efficient and stuff for
[02:34:03.200 --> 02:34:08.560]   you know, for people that don't have the superpower for computers is just should run better. Nice.
[02:34:08.560 --> 02:34:16.160]   With the latest updates. Yo, R I S F X dot com particle illusion. And they have a free tutorial.
[02:34:16.160 --> 02:34:21.840]   Yep. That's on I think it's through their YouTube channel. And it's pretty nice. It's pretty
[02:34:23.120 --> 02:34:28.480]   straightforward and start out very basic because if you if you're brand new to it, you don't know
[02:34:28.480 --> 02:34:34.000]   what the heck you're looking at on the screen and they slow it down. I dig it. Are you going to buy
[02:34:34.000 --> 02:34:42.000]   this haggabas for your new Mac mini? Are you getting a Mac mini? Well, I'm not getting a Mac
[02:34:42.000 --> 02:34:50.800]   mini. I now have a Mac mini. Yay. I have a two M two M two pro. That's what I'm using right now.
[02:34:50.800 --> 02:34:58.080]   Oh, yeah. Oh, nice. And you do look extra good. So really the Mac made the difference.
[02:34:58.080 --> 02:35:02.800]   No, I'm trying to look exactly the same as the camera. It's a nice try. Yeah.
[02:35:02.800 --> 02:35:11.200]   But no, I I was remember we've spoken off offline last week or week before about heaven
[02:35:11.200 --> 02:35:15.680]   docks and so forth because Apple just don't put any dang on ports on it. Right. So I was looking
[02:35:15.680 --> 02:35:22.400]   at the Mac studio to begin with, but it's expensive and just I just couldn't afford it. And I ended up
[02:35:22.400 --> 02:35:28.640]   with the M two pro because a efficiency standpoint, it seemed like it was neck and neck with the
[02:35:28.640 --> 02:35:34.160]   studio. So I said, I tried and just find a doc. And this little doc here that some China branded
[02:35:34.160 --> 02:35:41.040]   doc is it's pretty legit because I can add a SSD on the side of it. I was wondering what the
[02:35:41.040 --> 02:35:47.280]   inside was for. Oh, you put an SSD in there and you get an extra bit of storage. Plus you get
[02:35:47.280 --> 02:35:52.560]   your card reader that the Mac mini does not have because you need an SD card reader, most of the
[02:35:52.560 --> 02:35:58.640]   time, and you get the additional ports. They also have a another version out there that supports M
[02:35:58.640 --> 02:36:05.600]   dot two drives as well. Nice. So there are some options out there and I was able to save a little
[02:36:05.600 --> 02:36:12.160]   bit of money going this route versus going out to get the studio. Yeah. Then I also found this other
[02:36:12.160 --> 02:36:20.480]   doc from my CES days. This was in a swag box. And it's I think it's from targets or what have you.
[02:36:20.480 --> 02:36:26.560]   Yeah, it's a targets doc that gives gives me a couple more USB ports, but also gives me display
[02:36:26.560 --> 02:36:32.240]   port and stuff like that. I'm just like, Oh, well, I can just use this for the US stuff. So I'm
[02:36:32.240 --> 02:36:39.120]   going to show a doc. You got me thinking about docs. I ordered a doc that doubles as a munderstand
[02:36:39.120 --> 02:36:45.120]   for my Mac mini here. And I'm going to show something like that. And it looked legit too. Yeah.
[02:36:45.120 --> 02:36:52.000]   I'll show that on the tech guys next week. But this is the Haggabas, which is obviously a made-up
[02:36:52.000 --> 02:36:58.320]   Jenny's name H sounds a little bit too much like a Scottish sheep guts dinner. Haggabas. There's
[02:36:58.320 --> 02:37:03.120]   that and there's a centeschi. There's another one. Oh, so techie's real. Sateke's legit. They've
[02:37:03.120 --> 02:37:10.560]   been around for a long time. Okay. So this is a Haggabas. Haggabas always takes user experience as the
[02:37:10.560 --> 02:37:16.880]   main theme follows the human eyes design concept. And here's to the concept of quality first and
[02:37:16.880 --> 02:37:23.440]   improves the quality of life for global consumers. So you know, it's got to be good. It is only 74 bucks.
[02:37:24.560 --> 02:37:30.640]   I think it's a good price. USB 3.2s Gen 2. So that's not thunderbolt, but that's why it's not
[02:37:30.640 --> 02:37:34.960]   a boat. I was fast enough. Yeah, yeah, yeah. Absolutely. Very cool.
[02:37:34.960 --> 02:37:40.320]   Thank you. I want to do a segment with you all on as tech guys, because I definitely
[02:37:40.320 --> 02:37:45.200]   have some thoughts on this transition from what I think that's a great idea. When do you want to do
[02:37:45.200 --> 02:37:52.400]   that? I'm going to do it next week or the week after. Give yourself some time. Yeah. Oh, I have
[02:37:52.400 --> 02:37:59.360]   thoughts. I have thoughts. He's already full of them. All right. Oh boy. Good. Well, I'm going to let
[02:37:59.360 --> 02:38:03.280]   you settle in a little bit before, you know, let the steam come off a little bit before I,
[02:38:03.280 --> 02:38:08.720]   before we make you talk about it. But yeah, you can do it this week or next week. Yeah, every Sunday,
[02:38:08.720 --> 02:38:13.440]   Mike and I answer tech questions. And it's a it's really our podcast version of the old radio
[02:38:13.440 --> 02:38:20.720]   show. I called ask the tech guy Sundays from 11 a.m. to 2 p.m. Pacific, which is 2 p.m. to 5 p.m.
[02:38:20.720 --> 02:38:25.120]   Eastern right before this weekend tech and coming soon, it proved it.
[02:38:25.120 --> 02:38:33.440]   Switching from windows to Mac. That's a big deal. That's a big deal. Yeah. I'd love to hear
[02:38:33.440 --> 02:38:40.000]   your experience. Boy, I know now. Oh, you regret it. You regret it? No, I don't regret it. You'll get
[02:38:40.000 --> 02:38:45.520]   used to it. I wanted a machine that's just going to be peppy enough. You know, is it peppy enough?
[02:38:46.480 --> 02:38:49.360]   It's definitely peppy enough. All right. That's good.
[02:38:49.360 --> 02:39:00.160]   April at Twitter TV slash H O P and uh, and put calm for his wonderful prints and T. P R U I T.
[02:39:00.160 --> 02:39:06.880]   T. Thank you, aunt. Thank you, Jeff Jarvis, townite professor of journalism at the Craig Newmark
[02:39:10.160 --> 02:39:16.880]   graduates school journalism at the city of New York buzz machine.com. Bitly B it. That's a URL
[02:39:16.880 --> 02:39:20.720]   shortener. I'm sorry, but I don't think it's going to change for a lot of B it that L Y
[02:39:20.720 --> 02:39:27.280]   slash by Gutenberg for his new book, which comes out soon. A couple months, right? June, June.
[02:39:27.280 --> 02:39:32.320]   Awesome. You know, else is coming out in June. Supreme Court decision on
[02:39:33.200 --> 02:39:41.040]   Sondra's Google. I am so glad that you presumably who knows I would be hysterical if they, uh,
[02:39:41.040 --> 02:39:46.000]   in a couple of days ago, you know, never mind. That would be very funny. You never know. You
[02:39:46.000 --> 02:39:52.960]   never know. But thank God we had Kathy Gellis here to explain it all. You are exactly who I
[02:39:52.960 --> 02:39:59.280]   was hoping we could get today. And you did not disappoint. Fantastic. Yes. She writes a tech
[02:39:59.280 --> 02:40:06.320]   dirt. She is living in a temporary, a wonderful rainbow bubble of delight after yesterday's
[02:40:06.320 --> 02:40:13.040]   oral arguments. We don't want to pop that might be slightly overstating the little bit of like
[02:40:13.040 --> 02:40:20.480]   sleep deprived afterglow. But she can't believe who she likes now. Oh my God. She's
[02:40:20.480 --> 02:40:28.880]   now understood the things that matter to me. Really? I love you. My amicus CG.
[02:40:28.880 --> 02:40:35.840]   CG Council dot com CO UNSE. Oh, don't worry. She's not going to talk up brick, Kevin off you call.
[02:40:35.840 --> 02:40:41.520]   You'll also find her. She's she's deprecating. I'm proud proud of her.
[02:40:42.000 --> 02:40:49.280]   Deprecating her Twitter as one does and is now on the mastodon mastodon cloud at
[02:40:49.280 --> 02:40:53.360]   Kathy Gellis. And I'm going to show you exactly what you do if you're on
[02:40:53.360 --> 02:41:02.240]   Twitch social. You just type in at Kathy G E L L I S hit return. There she is. You can tell it's
[02:41:02.240 --> 02:41:08.880]   her and you're my mom already following you. Look at that. No, that's very nice. Yeah, there she is.
[02:41:09.440 --> 02:41:14.800]   So make sure you follow on mastodon because we want to make sure Kathy sticks around
[02:41:14.800 --> 02:41:19.680]   because she's a big boon to the community. Really appreciate you coming on the show. Thank you so
[02:41:19.680 --> 02:41:24.960]   much. Thanks for having me. Yeah. Thank you all for being here. We do twig every Wednesday
[02:41:24.960 --> 02:41:33.040]   afternoon round about two Pacific five p.m. Eastern. 2200 UTC. You can watch us do it live if you
[02:41:33.040 --> 02:41:39.200]   want the freshest version at live.twit.tv. If you're watching live chat live, the IRC is open to all
[02:41:39.200 --> 02:41:46.560]   IRC.twit.tv. We also have a lovely discord for our club twit members. We thank our club
[02:41:46.560 --> 02:41:52.080]   twit members for their support. You can join that and get on in there and see all the fun.
[02:41:52.080 --> 02:42:00.560]   You will not be disappointed. You can also get copies of the show on the website twit.tv/twig.
[02:42:02.560 --> 02:42:07.200]   As soon as we're done editing it takes a little while, a couple hours or subscribing your favorite
[02:42:07.200 --> 02:42:11.600]   podcast player and you'll get it that way automatically. You won't have to even think about it. You'll
[02:42:11.600 --> 02:42:15.760]   just have it ready to listen to whenever you're in the mood for this week in Google. It also works
[02:42:15.760 --> 02:42:20.880]   by the way. I don't mention this enough, but if you have an Amazon Echo or a Google voice assistant,
[02:42:20.880 --> 02:42:26.160]   you know, you can ask them. In most cases, it's sufficient to say play this week in Google.
[02:42:27.760 --> 02:42:34.720]   You might have to qualify it by saying on YouTube or something like that or tune in. But generally,
[02:42:34.720 --> 02:42:39.360]   you could just say for all of our shows, play this week in Google, you can even say in some cases,
[02:42:39.360 --> 02:42:48.160]   play Twit Live and you can actually listen to the live stream, which goes 24/7 on your device.
[02:42:48.160 --> 02:42:53.760]   We have a YouTube channel dedicated to twig, youtube.com/twig or actually I think it's
[02:42:53.760 --> 02:42:58.800]   slash this week in Google. So that's another way to watch. Plenty of ways to get us. I hope you will
[02:42:58.800 --> 02:43:03.280]   and I hope you will get us next week because we'll have more. We'll stay CB back next week.
[02:43:03.280 --> 02:43:10.480]   I don't know. That's my note. Okay. We'll see Stacey back next week. We'll see you too, I hope
[02:43:10.480 --> 02:43:14.000]   on this week in Google. Bye bye. Bye bye.
[02:43:14.000 --> 02:43:20.240]   If you love all things Android, well, I'm going to show for you to check out. It's called
[02:43:20.240 --> 02:43:24.480]   All About Android and I'll give you three guesses what we talk about. We talk about Android,
[02:43:24.480 --> 02:43:30.160]   the latest news, hardware, apps, we answer feedback. It's me, Jason, howl, Ron Richards,
[02:43:30.160 --> 02:43:36.640]   WinTwitDow and a whole cast of awesome characters talking about the operating system that we love.
[02:43:36.640 --> 02:43:40.320]   You can find all about Android at twit.tv/aa.
[02:43:40.320 --> 02:43:51.560]   [Music]

