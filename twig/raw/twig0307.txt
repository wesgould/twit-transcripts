;FFMETADATA1
title=Give Peas a Chance
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=307
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons Attribution Non-Commercial Share-Alike license. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2015
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.000]   It's time for Twig this week in Google.
[00:00:04.000 --> 00:00:06.000]   Leo's not here, he's on a boat.
[00:00:06.000 --> 00:00:11.000]   I'm Mike Elgin, to its news director, Jeff and Gina are here, and we're going to talk about Google's
[00:00:11.000 --> 00:00:16.000]   anti-trust troubles, Facebook's news business, and Amazon's virtual assistant.
[00:00:16.000 --> 00:00:18.000]   It's all coming up next on Twig.
[00:00:18.000 --> 00:00:23.000]   Netcast you love.
[00:00:23.000 --> 00:00:25.000]   From people you trust.
[00:00:28.000 --> 00:00:30.000]   This is Twig.
[00:00:30.000 --> 00:00:34.000]   Bandwidth for this week in Google is provided by
[00:00:34.000 --> 00:00:38.000]   CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:38.000 --> 00:00:42.000]   [MUSIC]
[00:00:42.000 --> 00:00:44.000]   This is Twig.
[00:00:44.000 --> 00:00:50.000]   This week in Google, episode 307, recorded July 1, 2015.
[00:00:50.000 --> 00:00:52.000]   Give P's a chance.
[00:00:52.000 --> 00:00:58.000]   This episode of This Week in Google is brought to you by FreshBooks, a super simple cloud
[00:00:58.000 --> 00:01:02.000]   accounting and invoicing solution designed to help small business owners save time
[00:01:02.000 --> 00:01:04.000]   billing and get paid faster.
[00:01:04.000 --> 00:01:07.000]   Try it free at freshbooks.com/twig.
[00:01:08.000 --> 00:01:10.000]   And by LegalZoom.
[00:01:10.000 --> 00:01:16.000]   If you're looking to incorporate form an LLC, get a trademark and more, LegalZoom can help you get started the right way.
[00:01:16.000 --> 00:01:20.000]   LegalZoom is not a law firm but can connect you with an independent attorney,
[00:01:20.000 --> 00:01:27.000]   is at LegalZoom.com and use offer code TWIG in the referral box at checkout this month for your special discount.
[00:01:27.000 --> 00:01:29.000]   And by BrainTree.
[00:01:29.000 --> 00:01:34.000]   If you're working on a mobile app and searching for a simple payment solution, check out BrainTree.
[00:01:34.000 --> 00:01:38.000]   With one simple integration, you can offer your customers every way to pay.
[00:01:38.000 --> 00:01:39.000]   Period.
[00:01:39.000 --> 00:01:47.000]   To learn more and for your first $50,000 in transactions, fee free, go to braintreatpayments.com/twig.
[00:01:47.000 --> 00:01:50.000]   It's time for Twig.
[00:01:50.000 --> 00:01:56.000]   This week in Google, the show where we talk about Google, the cloud and whatever Jeff and Gina want to talk about.
[00:01:56.000 --> 00:01:57.000]   Leo's on vacation.
[00:01:57.000 --> 00:01:58.000]   My name is Mike Elgin.
[00:01:58.000 --> 00:02:00.000]   I'm the news director here at Twit.
[00:02:00.000 --> 00:02:07.000]   And we've got an action-packed show and a couple of action-packed regulars starting with Gina, Trippani, Gina, how the heck are you?
[00:02:07.000 --> 00:02:09.000]   Hi, I'm doing great. Glad to be here.
[00:02:09.000 --> 00:02:13.000]   So glad you're here, Gina, as the creator of ThinkUp.com and Makerbase.
[00:02:13.000 --> 00:02:20.000]   Can you explain one more time what Makerbase is going to be when it's born into the world?
[00:02:20.000 --> 00:02:24.000]   Makerbase isn't quite public yet. It's invitation only right now.
[00:02:24.000 --> 00:02:29.000]   But the idea is that it's basically like an IMDB for apps.
[00:02:29.000 --> 00:02:35.000]   So it's going to be a directory of people who make apps and websites and podcasts and other sort of digital things.
[00:02:35.000 --> 00:02:38.000]   And it's going to be totally user-editable, much like Wikipedia.
[00:02:38.000 --> 00:02:45.000]   It's kind of going to be like LinkedIn, but for coders, broadcasters, ebook authors, that kind of thing.
[00:02:45.000 --> 00:02:47.000]   So it's going to be a lot of fun.
[00:02:47.000 --> 00:02:53.000]   Well, I can't wait until you give Tech News today an exclusive on the announcement when it comes out.
[00:02:53.000 --> 00:02:55.000]   Absolutely.
[00:02:55.000 --> 00:02:59.000]   And of course, Gina is the founding editor of Life Hacker.
[00:02:59.000 --> 00:03:05.000]   And also joining us as usual is Jeff Jarvis, professor of journalism at the City University of New York, author of Public Parts,
[00:03:05.000 --> 00:03:07.000]   Google Dune, Gutenberg, the geek.
[00:03:07.000 --> 00:03:08.000]   Who needs the old publishing?
[00:03:08.000 --> 00:03:10.000]   We have Gina Trippani.
[00:03:10.000 --> 00:03:11.000]   Exactly.
[00:03:11.000 --> 00:03:17.000]   The office office is bowed under a bunch of old people and crews going down the river, having time of his life.
[00:03:17.000 --> 00:03:19.000]   We have genes.
[00:03:19.000 --> 00:03:27.000]   You know, there's something extraordinarily leisurely about taking a cruise on a river and taking pictures of Europe just as Europe goes by.
[00:03:27.000 --> 00:03:33.000]   You can announce the camera on the side of the boat and just have an app that just takes a picture of me.
[00:03:33.000 --> 00:03:36.000]   Anyway, someday, someday.
[00:03:36.000 --> 00:03:39.000]   Well, let's jump right into the stories today.
[00:03:39.000 --> 00:03:44.000]   Let's start with a sort of a Yelp funded...
[00:03:44.000 --> 00:03:45.000]   Wait, wait, wait, wait.
[00:03:45.000 --> 00:03:46.000]   Okay.
[00:03:46.000 --> 00:03:47.000]   Breaking news.
[00:03:47.000 --> 00:03:48.000]   Breaking news alert.
[00:03:48.000 --> 00:03:49.000]   What's up?
[00:03:49.000 --> 00:03:53.000]   The President of the United States, honest to God, the President of the United States has answered the controversy,
[00:03:53.000 --> 00:03:56.000]   but the New York Times telling people to put peas in guacamole.
[00:03:56.000 --> 00:03:57.000]   You're kidding.
[00:03:57.000 --> 00:03:58.000]   He's against it.
[00:03:58.000 --> 00:03:59.000]   Thank goodness.
[00:03:59.000 --> 00:04:03.000]   Respect the NYT, he says, but not buying peas in guac.
[00:04:03.000 --> 00:04:04.000]   Yeah, yeah, no.
[00:04:04.000 --> 00:04:07.000]   Okay, now back to our regular programming.
[00:04:07.000 --> 00:04:08.000]   I just...
[00:04:08.000 --> 00:04:09.000]   You just tweet that?
[00:04:09.000 --> 00:04:10.000]   Yeah, you did, yeah.
[00:04:10.000 --> 00:04:11.000]   Nice.
[00:04:11.000 --> 00:04:14.000]   Well, thank goodness that's over.
[00:04:14.000 --> 00:04:17.000]   Although, I wouldn't want to put that one before the Supreme Court.
[00:04:17.000 --> 00:04:22.000]   You don't know what they're going to come up with with some of the people they have on the Supreme Court, but...
[00:04:22.000 --> 00:04:23.000]   That's true.
[00:04:23.000 --> 00:04:24.000]   You don't know.
[00:04:24.000 --> 00:04:27.000]   Okay, sorry to interrupt, but I knew our audience were want this breaking news.
[00:04:27.000 --> 00:04:29.000]   Supreme Court, is that like a Taco Bell?
[00:04:29.000 --> 00:04:30.000]   Yeah, yeah.
[00:04:30.000 --> 00:04:32.000]   It's a burrito-supermium court.
[00:04:32.000 --> 00:04:34.000]   Taco Bell with guacamole in Supreme Court.
[00:04:34.000 --> 00:04:36.000]   Okay, I get it.
[00:04:36.000 --> 00:04:41.000]   All right, so here's what happened.
[00:04:41.000 --> 00:04:58.000]   A formally respected commentator came out and basically said that, yes, Google has been favoring its own results and sort of harming competitors in Google search results.
[00:04:58.000 --> 00:05:07.000]   His last name was Yu, and he basically came out with a study that is flawed for some points that we're going to talk about.
[00:05:07.000 --> 00:05:14.000]   He probably makes some good points as well, but the controversy is that this was partially funded by Yelp.
[00:05:14.000 --> 00:05:18.000]   And so with that, I will wind up Jeff Jarvis and turn him loose.
[00:05:18.000 --> 00:05:20.000]   (Grunting)
[00:05:20.000 --> 00:05:26.000]   So, what's the heck is wrong, Jeff, with taking money from one company to attack another?
[00:05:26.000 --> 00:05:28.000]   Well, that's the problem when you're an academic.
[00:05:28.000 --> 00:05:31.000]   I mean, I have a lot of respect for Tim Wu, don't we all?
[00:05:31.000 --> 00:05:45.000]   I'm the creator of the phrase, neutrality, fighter for rights about the internet, but took money from Yelp to do an attack on Google time perfectly to the anti-trust negotiations in Europe.
[00:05:45.000 --> 00:05:47.000]   So it wasn't just attacking the other company.
[00:05:47.000 --> 00:05:51.000]   It was attacking the other company in a lobbying way.
[00:05:51.000 --> 00:05:54.000]   And I'm surprised he did this.
[00:05:54.000 --> 00:05:59.000]   If he agrees with it and says so, that's his opinion.
[00:05:59.000 --> 00:06:05.000]   But to do the study on behalf of attacking Google, and the other problem was the study, I think, was fundamentally flawed.
[00:06:05.000 --> 00:06:21.000]   Matthew Ingram did a very good, but what they basically did was take a Google search with a Google One Box Universal Search results, and they took that out, they put up other results, and they said, "Oh, these other results would have Yelp like results is better."
[00:06:21.000 --> 00:06:24.000]   And how did they judge that it's better? Because it got more clicks.
[00:06:24.000 --> 00:06:27.000]   Well, I actually argue that probably it's the opposite.
[00:06:27.000 --> 00:06:28.000]   Yeah.
[00:06:28.000 --> 00:06:31.000]   That if you have to click too often, that's bad search.
[00:06:31.000 --> 00:06:39.000]   So I was in Twitter conversation about this too with Matthew and others, and Jeremy Stoppelman, the CEO of Yelp came in.
[00:06:39.000 --> 00:06:45.000]   And it also became clear that his argument is that listings without reviews are bad.
[00:06:45.000 --> 00:06:47.000]   That's because he's a review company.
[00:06:47.000 --> 00:06:59.000]   When I was looking for a, in fact, apart from all this, I had just gone in and tried to do a search for trying to find something new to take out in my neighborhood, which is town by town by town way out in the country.
[00:06:59.000 --> 00:07:02.000]   And the Yelp search was very frustrating.
[00:07:02.000 --> 00:07:04.000]   And the Google search was far better.
[00:07:04.000 --> 00:07:06.000]   It was a higher quality search.
[00:07:06.000 --> 00:07:09.000]   And I didn't have to click around as much, and that was better.
[00:07:09.000 --> 00:07:14.000]   So the study itself, so-called study, I think is flawed.
[00:07:14.000 --> 00:07:18.000]   And it goes so far as to argue that there is societal damage from this.
[00:07:18.000 --> 00:07:26.000]   It just gives, it puts bullets in the gun of the EU up to Google's head, just as they're doing the negotiations.
[00:07:26.000 --> 00:07:34.000]   Now, once again, I want to say, I wish that Google would do something really awful so I could yell at them and scream at them and be anti-Google and be nasty to them right now.
[00:07:34.000 --> 00:07:37.000]   But once again, I got to agree with Google on this one.
[00:07:37.000 --> 00:07:39.000]   Gina, what do you think?
[00:07:39.000 --> 00:07:41.000]   Yeah, this is tough.
[00:07:41.000 --> 00:07:45.000]   I'm curious, have you talked to Tim about it directly?
[00:07:45.000 --> 00:07:47.000]   Or have you just talked it to the Yelp folks?
[00:07:47.000 --> 00:08:01.000]   I mean, I'm trying to imagine, if I'm an academic researcher and Yelp comes to me and says, you know, we think that Google search results are not only aren't great, aren't certainly pointing, making users click through to us, but also aren't good.
[00:08:01.000 --> 00:08:10.000]   I mean, I mean, for someone who's concerned about net neutrality, you know, from his perspective, it might be like, you know, hey, is Google concentrating all this traffic kind of on the internet?
[00:08:10.000 --> 00:08:13.000]   Or is it all this traffic kind of on their own results?
[00:08:13.000 --> 00:08:19.000]   I think the year point about how to decide whether or not search results are good is a really good point.
[00:08:19.000 --> 00:08:24.000]   I mean, from a user perspective, certainly clicking less is better, right?
[00:08:24.000 --> 00:08:29.000]   But Google, you know, classically, Google didn't keep you on Google, right?
[00:08:29.000 --> 00:08:31.000]   Like, they've shifted in the years over the years.
[00:08:31.000 --> 00:08:34.000]   In the beginning, they were trying to send you off.
[00:08:34.000 --> 00:08:38.000]   When everybody was concentrating on sticky, they were concentrating on getting you to the right place.
[00:08:38.000 --> 00:08:42.000]   But yeah, this whole thing just seems really fishy and weird.
[00:08:42.000 --> 00:08:48.000]   And I'm kind of surprised that it sort of happened this way, particularly around the funding issue.
[00:08:48.000 --> 00:08:55.000]   My own view is that there's no -- first of all, if you want -- if anybody is harboring the illusion,
[00:08:55.000 --> 00:09:01.000]   that there is a such thing that there is a possibility of objective search results,
[00:09:01.000 --> 00:09:04.000]   please disabuse yourself of that notion entirely.
[00:09:04.000 --> 00:09:08.000]   There's no such thing. It's impossible to have objective search results.
[00:09:08.000 --> 00:09:09.000]   That's number one.
[00:09:09.000 --> 00:09:16.000]   Number two, the product that Google is offering is their opinion about what is the most relevant search results.
[00:09:16.000 --> 00:09:18.000]   That's what you're buying when you go to Google.
[00:09:18.000 --> 00:09:22.000]   And if you don't like theirs, then you go to Bing, or you go to some other search engine.
[00:09:22.000 --> 00:09:28.000]   And you're looking for their evaluation based on, you know, that's enshrined in their algorithms or whatever.
[00:09:28.000 --> 00:09:37.000]   Trying is the wrong word. And then the third thing is that Google optimizes, as you're applying, Jeff and Gina,
[00:09:37.000 --> 00:09:41.000]   their products to be search result friendly.
[00:09:41.000 --> 00:09:47.000]   And then they optimize their search results to be friendly toward their services and so on.
[00:09:47.000 --> 00:09:52.000]   In other words, they're trying to have the shortest distance between you and what you're looking for.
[00:09:52.000 --> 00:09:57.000]   And it would be really, really weird -- in fact, this happens all the time, and it is weird when it does happen --
[00:09:57.000 --> 00:10:02.000]   when Google does its best to have a service that they think is the most relevant and best service,
[00:10:02.000 --> 00:10:06.000]   and then to choose another service that another company makes.
[00:10:06.000 --> 00:10:14.000]   I mean, they make these products with an idea in mind, as all companies do, with what is the best result,
[00:10:14.000 --> 00:10:20.000]   what is the best experience, what is the best, you know, way to deliver the information.
[00:10:20.000 --> 00:10:27.000]   And then after they've done their best shot, it's amazing to me that they very often say,
[00:10:27.000 --> 00:10:30.000]   "Okay, we've done our best, but this other service is better."
[00:10:30.000 --> 00:10:34.000]   Exactly, Mike. I mean, how do I -- this is -- God's honest truth.
[00:10:34.000 --> 00:10:40.000]   How do I end up using Yelp? Always through Google search. Google leads me to Yelp.
[00:10:40.000 --> 00:10:45.000]   Maybe it doesn't do a first thing on the page, but I also think Yelp is not the best search.
[00:10:45.000 --> 00:10:52.000]   I think it's -- I think it's -- in fact, it's a very bad search itself, but it has -- I can -- Google always messes up.
[00:10:52.000 --> 00:10:56.000]   I call this restaurant regularly. Lombardi Pete's the company. Martinsville, New Jersey.
[00:10:56.000 --> 00:11:01.000]   It always gives me Lombardi and Manhattan. Google screws up in that case, all right?
[00:11:01.000 --> 00:11:05.000]   So how do I get the phone number so I can dial it from one button from the car through Yelp?
[00:11:05.000 --> 00:11:09.000]   I get to Yelp through other means, too, but I get to it through Google.
[00:11:09.000 --> 00:11:12.000]   And certainly Yelp doesn't point me to Google.
[00:11:12.000 --> 00:11:18.000]   And I know the arguments flaw there because Yelp's not a monopoly size, but they do that.
[00:11:18.000 --> 00:11:24.000]   One other thing, by the way, I've never said this before, but at Google Executive, one said to me,
[00:11:24.000 --> 00:11:32.000]   "We -- they respect the independence that I maintain from them and they, for me."
[00:11:32.000 --> 00:11:37.000]   And as I said before the show, as I've revealed in the show often, I have gone to Google events.
[00:11:37.000 --> 00:11:41.000]   I accept expenses, so I don't lose money on doing it. I accept no fees.
[00:11:41.000 --> 00:11:45.000]   And Google has also told me straight out that -- and I feel bad for my school,
[00:11:45.000 --> 00:11:49.000]   they basically said they don't think they can support the work of me at my school
[00:11:49.000 --> 00:11:52.000]   or my center of my school because it would make me look bad.
[00:11:52.000 --> 00:11:57.000]   It would take away from my independence when I do talk about Google.
[00:11:57.000 --> 00:12:01.000]   And that's if Google's benefit because the more independent I am, when I end up, you know,
[00:12:01.000 --> 00:12:04.000]   usually agreeing with Google, the better it is.
[00:12:04.000 --> 00:12:07.000]   But I treasure my independence in that way.
[00:12:07.000 --> 00:12:13.000]   And I have people pay me to do things, newspaper companies and such, and I disclose that on my about page.
[00:12:13.000 --> 00:12:17.000]   I'm not saying that academics don't take other gigs and other money.
[00:12:17.000 --> 00:12:21.000]   The system is a particular case. You get used.
[00:12:21.000 --> 00:12:27.000]   Yeah, absolutely. And just to -- you mentioned that the main way that you use Yelp is through Google,
[00:12:27.000 --> 00:12:32.000]   I actually don't use Yelp that way. I always go to the app, and an alternative to --
[00:12:32.000 --> 00:12:35.000]   I'm pretty much just looking for restaurants or whatever.
[00:12:35.000 --> 00:12:38.000]   The alternative of that is, of course, use Google Maps or something like that.
[00:12:38.000 --> 00:12:42.000]   Or you had a third party app or something like that.
[00:12:42.000 --> 00:12:46.000]   But I always go just looking for a place to eat. I'm looking for that Lombardi's pizzeria.
[00:12:46.000 --> 00:12:51.000]   I will go to Yelp. I'll search for it. I will look for pizza.
[00:12:51.000 --> 00:12:54.000]   I'll sort it by ratings or whatever.
[00:12:54.000 --> 00:12:56.000]   Make sure it's open and all that kind of stuff.
[00:12:56.000 --> 00:13:03.000]   And it's really -- if Google offered a better way to do it through search, I would do that.
[00:13:03.000 --> 00:13:08.000]   But in fact, Yelp has a great little app that is very functionally very good.
[00:13:08.000 --> 00:13:13.000]   Now, there are other problems with crowd-sourced evaluations.
[00:13:13.000 --> 00:13:21.000]   But it's just -- I think the vast majority of people, in fact, who are using mobile devices
[00:13:21.000 --> 00:13:27.000]   are using the Yelp app rather than some other means in terms of whether they're going to use Yelp or Google.
[00:13:27.000 --> 00:13:29.000]   But I could be wrong about that.
[00:13:29.000 --> 00:13:34.000]   But again, the bigger issue here is Tim Wu's taking money from Yelp,
[00:13:34.000 --> 00:13:40.000]   especially in this climate of antitrust targeting in Europe Google for this very thing.
[00:13:40.000 --> 00:13:42.000]   It is highly problematic.
[00:13:42.000 --> 00:13:49.000]   And I don't think any of us -- at least I personally don't trust the European authorities to get this right.
[00:13:49.000 --> 00:13:51.000]   I have the feeling that they're not going to get it right.
[00:13:51.000 --> 00:14:01.000]   They tend to be overly critical and damaging to companies like Google for this type of thing.
[00:14:01.000 --> 00:14:05.000]   And it's not about, as a lot of people think, by the way,
[00:14:05.000 --> 00:14:11.000]   it's not about European companies complaining to the European authorities about this big bad American company.
[00:14:11.000 --> 00:14:14.000]   It's American companies complaining to the European authorities about this big bad American company.
[00:14:14.000 --> 00:14:19.000]   In part, in part, it's also European publishers and such.
[00:14:19.000 --> 00:14:24.000]   But the American companies have exploited this by jumping on Microsoft Yelp.
[00:14:24.000 --> 00:14:26.000]   And a few others have exploited it.
[00:14:26.000 --> 00:14:29.000]   And what I think -- and I'll get in trouble for this --
[00:14:29.000 --> 00:14:35.000]   but I think this is basically an anti-American attack on Google and other American companies
[00:14:35.000 --> 00:14:40.000]   from the European authorities and for other American tech companies to then join in, especially Microsoft,
[00:14:40.000 --> 00:14:45.000]   which went through its own stupid hell of antitrust in Europe, and then to turn around and just say,
[00:14:45.000 --> 00:14:50.000]   "The Schadenfreude of that of going after Google, I think, is shameful."
[00:14:50.000 --> 00:14:55.000]   And by the way, I think since we're on the topic of Microsoft going after Google, Microsoft,
[00:14:55.000 --> 00:14:57.000]   I don't know if it's in the rundown or not. I think it may be.
[00:14:57.000 --> 00:15:03.000]   But Microsoft spun off most of its advertising to AOL,
[00:15:03.000 --> 00:15:07.000]   and especially in the US and a few other markets.
[00:15:07.000 --> 00:15:12.000]   And I have the feeling that Microsoft is gearing up its whole operation
[00:15:12.000 --> 00:15:17.000]   so that it can be like Apple and criticize Google for collecting personal information
[00:15:17.000 --> 00:15:19.000]   for the purpose of contextual advertising.
[00:15:19.000 --> 00:15:24.000]   I think it's mostly sort of washing its hands of advertising revenue for that purpose.
[00:15:24.000 --> 00:15:26.000]   Just my theory, we'll see where they go with it.
[00:15:26.000 --> 00:15:29.000]   They're still going to do search advertising.
[00:15:29.000 --> 00:15:32.000]   So we'll see if they spin that off to someone else as well.
[00:15:32.000 --> 00:15:38.000]   But that is an interesting sort of issue in the news that's related to Microsoft versus Google.
[00:15:38.000 --> 00:15:45.000]   Well, okay, there's another... So speaking of Google getting things wrong or not getting things wrong,
[00:15:45.000 --> 00:15:50.000]   there was a controversial story, probably the most controversial story today around Google,
[00:15:50.000 --> 00:15:57.000]   is that a user a couple of days ago, an African-American programmer based in New York,
[00:15:57.000 --> 00:16:03.000]   noticed that in Google Photos, it had tagged him and his friend as Gorillas.
[00:16:03.000 --> 00:16:07.000]   That was the tag that was put on a photo of these two people.
[00:16:07.000 --> 00:16:12.000]   And this, obviously, is highly controversial.
[00:16:12.000 --> 00:16:15.000]   This person posted a tweet about it.
[00:16:15.000 --> 00:16:18.000]   Jonathan Zungar jumped in there.
[00:16:18.000 --> 00:16:22.000]   He is the... I guess he's the main architect behind Google Plus,
[00:16:22.000 --> 00:16:26.000]   and presumably involved with Google Photos.
[00:16:26.000 --> 00:16:31.000]   And he responded with an F-bomb, holy F-bomb.
[00:16:31.000 --> 00:16:33.000]   And he said, "You're not...
[00:16:33.000 --> 00:16:35.000]   "He determined this is 100% not okay."
[00:16:35.000 --> 00:16:38.000]   And then he came back later and said...
[00:16:38.000 --> 00:16:40.000]   Had a really nice tweet.
[00:16:40.000 --> 00:16:46.000]   He said, "I wish it were until recently Google Photos was confusing white faces with dogs and seals.
[00:16:46.000 --> 00:16:47.000]   "Machine learning is hard.
[00:16:47.000 --> 00:16:52.000]   "I think machine learning is hard is the key issue there."
[00:16:52.000 --> 00:16:54.000]   But this...
[00:16:54.000 --> 00:16:56.000]   He's a great, great guy.
[00:16:56.000 --> 00:17:00.000]   I only know him through Google Plus, but he's really wonderful.
[00:17:00.000 --> 00:17:02.000]   They're very, very smart.
[00:17:02.000 --> 00:17:05.000]   And you feel for everybody in this case,
[00:17:05.000 --> 00:17:07.000]   because it certainly was not a purposeful thing.
[00:17:07.000 --> 00:17:10.000]   But Jesus, you wish he'd looked up for that.
[00:17:10.000 --> 00:17:13.000]   As a programmer, I saw that and I just...
[00:17:13.000 --> 00:17:17.000]   My stomach dropped, because that is the last thing you ever want your software to do.
[00:17:17.000 --> 00:17:21.000]   And you know, and I mean, could us them for reacting the way that they did in machine...
[00:17:21.000 --> 00:17:23.000]   Yes, machine learning is hard.
[00:17:23.000 --> 00:17:28.000]   But you know, that particular mistake and all algorithms make mistakes,
[00:17:28.000 --> 00:17:33.000]   alongside the conversation about diversity and the very low number of, say,
[00:17:33.000 --> 00:17:36.000]   African-American employees at Google, when you put those together,
[00:17:36.000 --> 00:17:38.000]   it's really hard not to think.
[00:17:38.000 --> 00:17:41.000]   There are so few African-American employees at Google
[00:17:41.000 --> 00:17:44.000]   who are all dog-fooding and beta-testing their own products,
[00:17:44.000 --> 00:17:49.000]   you think, well, if there were more people of color using these products
[00:17:49.000 --> 00:17:53.000]   before they get released to the world, those kinds of bugs would have been caught.
[00:17:53.000 --> 00:17:54.000]   You know what I mean?
[00:17:54.000 --> 00:17:59.000]   So to me, it felt just like, oh, this is a manifestation of the 1%
[00:17:59.000 --> 00:18:04.000]   making apps for the 100% in the worst way.
[00:18:04.000 --> 00:18:06.000]   And so, man, I did.
[00:18:06.000 --> 00:18:09.000]   I felt terrible for everybody, but it also did feel like this moment of like,
[00:18:09.000 --> 00:18:13.000]   oh, this is why you want diversity in your...
[00:18:13.000 --> 00:18:15.000]   That's such an excellent point.
[00:18:15.000 --> 00:18:17.000]   Yeah, that's...
[00:18:17.000 --> 00:18:19.000]   In fact, I'll use that in the future.
[00:18:19.000 --> 00:18:25.000]   If you want rationales for why diversity works besides political correctness
[00:18:25.000 --> 00:18:27.000]   or the other things that are thrown at it, it's the reason.
[00:18:27.000 --> 00:18:30.000]   No, it enriches the company and it enriches the perspective,
[00:18:30.000 --> 00:18:32.000]   and that's a highly practical example.
[00:18:32.000 --> 00:18:33.000]   Yeah.
[00:18:33.000 --> 00:18:34.000]   Yeah.
[00:18:34.000 --> 00:18:41.000]   I mean, listen, we, in developing ThinkUp, you know, we write copy that we try to make it very friendly
[00:18:41.000 --> 00:18:43.000]   and sort of conversational.
[00:18:43.000 --> 00:18:47.000]   You know, at one point, we had written copy about, like, friends of yours who haven't replied to you in a while,
[00:18:47.000 --> 00:18:49.000]   and we used the term "deadbeats."
[00:18:49.000 --> 00:18:52.000]   And I had a user write it and say, you know, you just call my mom a deadbeats,
[00:18:52.000 --> 00:18:53.000]   she's actually dead.
[00:18:53.000 --> 00:18:54.000]   Like, don't do this.
[00:18:54.000 --> 00:18:56.000]   And it was awful.
[00:18:56.000 --> 00:19:01.000]   It was totally one of those moments where you're like, oh, you know, like this, you know, the software just made a really, really big mistake.
[00:19:01.000 --> 00:19:06.000]   So my full, full, complete empathy, and it's terrible and it's tough to write for everybody,
[00:19:06.000 --> 00:19:10.000]   but it is one of those situations, especially when you have, you know, young folks, you know,
[00:19:10.000 --> 00:19:15.000]   in a certain demographic building software for everybody where it's like, oh, these are, you know, scenarios that we didn't think about,
[00:19:15.000 --> 00:19:17.000]   or that we didn't test for.
[00:19:17.000 --> 00:19:22.000]   It makes me wonder how you, I think your answer is the right one, is that you have enough people of diversity.
[00:19:22.000 --> 00:19:24.000]   It would kind of come up.
[00:19:24.000 --> 00:19:26.000]   How do you do a procedure to test that?
[00:19:26.000 --> 00:19:29.000]   How do you test against the stupid things that you can have an algorithm do?
[00:19:29.000 --> 00:19:30.000]   Yeah.
[00:19:30.000 --> 00:19:39.000]   I mean, you know, typically, in these situations, and I'm sure that Google, I mean, you know, engineers write automated tests that feed, you know, data, you know,
[00:19:39.000 --> 00:19:42.000]   at these algorithms and see what the results are, right?
[00:19:42.000 --> 00:19:44.000]   But like, you've got this sort of limited amount of test data.
[00:19:44.000 --> 00:19:48.000]   I mean, Google's got the most test data, probably of any company on the planet.
[00:19:48.000 --> 00:19:56.000]   But yeah, but honestly, you know, there's like automated tests that engineers create and then feed data into, and then there's like real people using.
[00:19:56.000 --> 00:20:01.000]   So that's why you have beta tests or alpha tests with previews so that you have, you know, real, actually people, actual people, because there's never, you know,
[00:20:01.000 --> 00:20:04.000]   there's always going to be data that you didn't expect.
[00:20:04.000 --> 00:20:07.000]   And then you have the whole world using it and you get all the edge cases.
[00:20:07.000 --> 00:20:14.000]   So look, it's impossible to anticipate every single K use case for any kind of software.
[00:20:14.000 --> 00:20:16.000]   It's painful and you know it every single time.
[00:20:16.000 --> 00:20:28.000]   And really, and I said this on the show before, the true test of how a company responds to either a security bug or an algorithmic problem like this is how quickly they respond and what their tone is and what their messaging is and how quickly the fix gets out.
[00:20:28.000 --> 00:20:31.000]   And in this case, they did really well with that.
[00:20:31.000 --> 00:20:33.000]   Well, I mean, it was obvious from the get go.
[00:20:33.000 --> 00:20:35.000]   This is an extremely sensitive issue.
[00:20:35.000 --> 00:20:37.000]   But no, the answer is it's impossible.
[00:20:37.000 --> 00:20:38.000]   It's impossible to catch every case.
[00:20:38.000 --> 00:20:42.000]   This didn't seem like that much of an edge case though, you know?
[00:20:42.000 --> 00:20:44.000]   It seemed like something that would have been avoidable.
[00:20:44.000 --> 00:20:45.000]   Yeah.
[00:20:45.000 --> 00:20:46.000]   It's absolutely true.
[00:20:46.000 --> 00:20:56.000]   And the other thing that this touches on is that Google is probably the poster child for the idea that you can replace things that people used to do.
[00:20:56.000 --> 00:21:00.000]   For example, the sorting and categorization of pictures is something people have always done.
[00:21:00.000 --> 00:21:12.000]   Now, we're all ooing and awning from Google I/O about Google's ability to do this sort of categorization to distinguish between dog breeds.
[00:21:12.000 --> 00:21:24.000]   I've typed in, you know, types of things like, I typed in the word delicious into Google photos and it came up with all my wife's cooking, which is all delicious.
[00:21:24.000 --> 00:21:32.000]   Plus, it came up with a gazelle being devoured by a cheetah. The picture I took in Africa, presumably that was delicious. I'll have to take Google's word for it.
[00:21:32.000 --> 00:21:42.000]   But Google is the poster child for algorithmic jobs, work, that's sort of like people are hands free.
[00:21:42.000 --> 00:21:53.000]   And lately, in the last few months, there have been a whole bunch of things coming out that have taken activities or jobs that we thought the algorithms were about to take over completely.
[00:21:53.000 --> 00:22:04.000]   And they're adding human beings to it. For example, Apple Music has human curation at multiple points, including radio station with DJs who are picking music.
[00:22:04.000 --> 00:22:12.000]   They have Google, in fact, came out with its response to Apple Music, which was a free tier for Google Play Music.
[00:22:12.000 --> 00:22:19.000]   Those sort of little channels or radio stations, whatever they call them, are all human curated.
[00:22:19.000 --> 00:22:26.000]   Pulse, for example, which launched as a news app for sorting the news was intensely algorithmic.
[00:22:26.000 --> 00:22:35.000]   That was his whole claim to fame. LinkedIn bought it what a couple years ago, a year or two ago, and now they just relaunched with human curation.
[00:22:35.000 --> 00:22:48.000]   Snapchat has its human curated stuff. It goes on and on. Twitter is coming out with something called Project Lightning, which is going to be a human curated way to organize the best stuff around the world.
[00:22:48.000 --> 00:23:07.000]   And so, this is one of the reasons why all these companies in Silicon Valley are backing off of having services where there's no person as the final filter for what is presented to the public.
[00:23:07.000 --> 00:23:16.000]   Now, of course, with Google Photos and that kind of search, you can have human intermediation in that, you know, it's just not that type of service.
[00:23:16.000 --> 00:23:23.000]   But I think this is an important conversation that we need to be having is we thought the algorithms were about to take over everything.
[00:23:23.000 --> 00:23:43.000]   And now, companies are competing and winning and being better than they're competing services by hiring a whole bunch of people and having those people use their judgment, their taste, their sensibilities to prevent exactly this sort of thing that happened with Google Photos from occurring and presenting that content to the world.
[00:23:43.000 --> 00:23:51.000]   So, I don't know how you guys, if you guys have noticed this trend, but it seems to me like a very, very strong trend toward human curation, human editing.
[00:23:51.000 --> 00:23:54.000]   It's a really good observation. Yeah.
[00:23:54.000 --> 00:24:00.000]   So, I saw this, I saw a talk by Mike Krieger, who was one of the co-founders of Instagram, and it was about machine learning and images.
[00:24:00.000 --> 00:24:11.000]   And it's one of my favorite, you know, sort of image recognition stories, which is that they fed the Mona Lisa, an image of the Mona Lisa to an image recognition algorithm.
[00:24:11.000 --> 00:24:18.000]   And I'm paraphrasing this maybe badly, but the result was a sad young woman taking a selfie.
[00:24:18.000 --> 00:24:23.000]   That's always my, the Mona Lisa got a bad rap.
[00:24:23.000 --> 00:24:26.000]   And she's famous because she's got a little smile on her face.
[00:24:26.000 --> 00:24:31.000]   She's significantly less sad than a lot of her contemporaries, contemporary subjects.
[00:24:31.000 --> 00:24:35.000]   Well, why don't we take a break and we'll come back and talk about some more stuff.
[00:24:35.000 --> 00:24:43.000]   I'm going to grill Gina about some developer issues that I don't quite have a perfect handle on yet, but I know I will after I grill Gina.
[00:24:43.000 --> 00:24:49.000]   But first, let's talk about our first sponsor today, which is FreshBooks.
[00:24:49.000 --> 00:24:57.000]   FreshBooks is a service that will let you, will let you invoice whoever it is that you're working for.
[00:24:57.000 --> 00:24:59.000]   I've been using FreshBooks for a long time.
[00:24:59.000 --> 00:25:10.000]   I think I learned about it from Leo a couple of years ago. And in addition to my day job of being the news director at Twit, I also write a couple of columns and I use FreshBooks for all of my billing.
[00:25:10.000 --> 00:25:21.000]   And one of the funny things about FreshBooks, just give you an example of how easy it is to use is that every single time I create an, and send an invoice right before I send it, I always have the same thought.
[00:25:21.000 --> 00:25:25.000]   And that thought is, that's it? That's all I had to do?
[00:25:25.000 --> 00:25:32.000]   I must have missed something because basically, I've got it set up. I've got my, the companies I work for already plugged in.
[00:25:32.000 --> 00:25:37.000]   I've got my fees already plugged in. I've got the fact that I don't want to bill through PayPal plugged in.
[00:25:37.000 --> 00:25:41.000]   It creates its own invoice number automatically.
[00:25:41.000 --> 00:25:45.000]   And so all I do is I go in there and I choose the publisher.
[00:25:45.000 --> 00:25:51.000]   And then I add the, I just paste in the headline of the columns that I've written.
[00:25:51.000 --> 00:25:53.000]   And that's it.
[00:25:53.000 --> 00:25:58.000]   And then I say thanks in the little boxes, a little box where you can add whatever extra notes that you want to add.
[00:25:58.000 --> 00:26:06.000]   And then I click a button. And then my editors, they get an email with an attachment that is a perfectly formatted invoice.
[00:26:06.000 --> 00:26:09.000]   They love it because it's always perfectly formatted.
[00:26:09.000 --> 00:26:13.000]   FreshBooks always make sure that I don't have anything missing from the invoice.
[00:26:13.000 --> 00:26:17.000]   I've never had a problem in all the time I've been using FreshBooks.
[00:26:17.000 --> 00:26:24.000]   And it is just a fantastic app. If you ever send invoice, you have to use FreshBooks because it just makes everything easy.
[00:26:24.000 --> 00:26:30.000]   And we'll also keep track of, you know, it'll tell you this person knows you this much money. They haven't paid you yet.
[00:26:30.000 --> 00:26:39.000]   Or it will sort of every once in a while, it'll offer you new ways, new ideas for how you can sort of perfect how you invoice and bill.
[00:26:39.000 --> 00:26:45.000]   And there's a complete record of everything you've done so you can easily search and go through and find out what happened in the past.
[00:26:45.000 --> 00:26:51.000]   It's just a fantastic, fantastic service. Getting started is simple and it's totally free for 30 days.
[00:26:51.000 --> 00:26:59.000]   No obligation. Just go to freshbooks.com/twig. And don't forget to enter Twig in the "How did you hear about us?" section.
[00:26:59.000 --> 00:27:05.000]   You can start your 30-day free trial today. And we thank FreshBooks for supporting this episode of "This Week in Google."
[00:27:05.000 --> 00:27:08.000]   Make sure you get paid and do it the easy way.
[00:27:08.000 --> 00:27:10.000]   Love that FreshBooks.
[00:27:10.000 --> 00:27:23.000]   Well, okay, so Gina, there is a thing in the universe called GitHub, which we've all heard of, which is this great repository for all of these resources.
[00:27:23.000 --> 00:27:32.000]   And Google has quietly launched a competitor to GitHub for the lack of a better term called Cloud Source Repositories.
[00:27:32.000 --> 00:27:44.000]   Gina, what do you know about Cloud Source Repositories A and B, what are the chances that anyone, including Google, can get people to stop obsessing over GitHub?
[00:27:44.000 --> 00:27:51.000]   I mean, it's like a lot of developers have invested a lot of time and energy into being hosted on GitHub.
[00:27:51.000 --> 00:27:54.000]   What is your take on this entire development here?
[00:27:54.000 --> 00:28:06.000]   You know, Git isn't a really interesting thing. So Git is a distributed source code repository. And by distributor, they mean that you have a copy of your Git repository locally.
[00:28:06.000 --> 00:28:12.000]   You have a copy on GitHub, and you have a copy on any server that you push to, a complete and total copy.
[00:28:12.000 --> 00:28:16.000]   It's kind of like your Dropbox folder, right? The whole file syncs.
[00:28:16.000 --> 00:28:31.000]   So what Google's doing here with this Cloud Source Repositories is that they're saying, if you use Google Cloud Platform, which they've been pushing really, really hard and continue to put a lot more weight behind, you can set up a remote Git repository here in the Google infrastructure,
[00:28:31.000 --> 00:28:35.000]   and we'll offer you all a bunch of tools around that.
[00:28:35.000 --> 00:28:42.000]   So GitHub just hosts your source code and adds on a bunch of social stuff, and it's very geared toward open source projects.
[00:28:42.000 --> 00:28:55.000]   So the thing about GitHub is that not only is your Git repository there, but then you can, you know, you can, they added on all this other stuff, like pull requests and ways to follow repositories and an issue tracker and wikis and hosted pages,
[00:28:55.000 --> 00:28:59.000]   all this kind of extra services that make the service very social.
[00:28:59.000 --> 00:29:09.000]   What Google's doing with this Cloud repositories is that they're saying, if you use Google Cloud Platform, you can host your code with us, and then we'll offer all these other tools.
[00:29:09.000 --> 00:29:24.000]   Like, for instance, you know, since you'll be hosting the code, the same place where it's running will offer you all these really cool debugging tools, where if someone's using your app and they run into an error, you can kind of live the bugs at breakpoints and see, you know, where your code is going wrong.
[00:29:24.000 --> 00:29:32.000]   And so the interesting thing about Git is because it's distributed, I could have different remotes. I could have a remote on GitHub.
[00:29:32.000 --> 00:29:42.000]   I could have a remote on Google Cloud Platform. I could have a remote on Amazon. I can really have a remote Git repository anywhere that I want because it's a full copy everywhere and it's distributed and it's peer to peer.
[00:29:42.000 --> 00:29:55.000]   So I think that GitHub will remain strong and people will use it. And like I said, it's particularly strong open source platforms, but Google's offering Git hosting is kind of as an add on to their Cloud Platform.
[00:29:55.000 --> 00:30:08.000]   They really want apps on their Cloud Platform. So I think this is going to be a good thing for them, but I don't really see it as a competitor. Google Cloud Platform is more of a competitor to Amazon Web Services than to GitHub.
[00:30:08.000 --> 00:30:14.000]   I think their Git hosting is more of a side feature, kind of than the main feature.
[00:30:14.000 --> 00:30:24.000]   Yeah, I see. And I think Microsoft's got one, Amazon's got one, as you said. And so, yeah, it's going to be interesting to see how they do with that.
[00:30:24.000 --> 00:30:40.000]   Watch is going off here. Dismiss. Okay. All right. Well, I mentioned a few minutes ago that AOL was basically going to take all of Microsoft's ad business.
[00:30:40.000 --> 00:30:56.000]   Part of that deal that they have is that Google has lost, they signed a 10 year contract, I think it was, for AOL to use Bing instead of Google search.
[00:30:56.000 --> 00:31:10.000]   Do either of you have a sense of how big, how many people do their search through AOL and formally through Google and in the future through Bing? Is this a significant loss for Google?
[00:31:10.000 --> 00:31:17.000]   I think the number I saw was like tens of millions of dollars, which when you make billions of dollars, it's not that much.
[00:31:17.000 --> 00:31:33.000]   And it was more strategic for Microsoft to get rid of its ad business for AOL to gain that ad stuff. I mean, Microsoft, I unfortunately got home too late to listen to Windows Weekly because I was curious what they had to say about this, because it's really about the new CEO focusing Microsoft.
[00:31:33.000 --> 00:31:44.000]   I think with admirable precision and getting them out of business. They bought a quantitative, they wrote that down to the tune of huge, huge dollars.
[00:31:44.000 --> 00:31:53.000]   They're now shifting the ad business over AOL, recognizing they're not in the ad business. AOL, thus Verizon, because I deal closed already, isn't better shape.
[00:31:53.000 --> 00:32:10.000]   So yes, Google, let's not forget that Tim Armstrong, who ran AOL and made it public and then sold it, was a Google executive. So he had Google search on AOL, but I don't think it was a huge loss.
[00:32:10.000 --> 00:32:24.000]   Yeah, definitely. Now, Google also did another interesting thing around search and ads. They've taken several steps to improve the ad experience by eliminating accidental clicks.
[00:32:24.000 --> 00:32:33.000]   And this is a great thing, I think, because of course, accidental clicks help Google theoretically, or do they?
[00:32:33.000 --> 00:32:49.000]   When somebody clicks on an accidental clicks accidentally on an ad and they go to that site, does that water down the sort of the click to action ratio or some other metric, or is that just a benefit because somebody seems to have clicked on the ad?
[00:32:49.000 --> 00:32:58.000]   I'm not sure if this is self-serving or if this is a virtuous loss that Google deliberately took here.
[00:32:58.000 --> 00:33:04.000]   I don't know, I would bet that probably you and I have all accidentally clicked on ads and I'll bet their advertisers do the same thing.
[00:33:04.000 --> 00:33:09.000]   And so they probably go after Google and say, "Well, are we paying for accidental clicks here?"
[00:33:09.000 --> 00:33:22.000]   And the more that Google can do, especially at an age when there is click fraud, not from Google, not from good, upstanding companies, but elsewhere, when only until recently were advertisers are paying for ads that were never seen.
[00:33:22.000 --> 00:33:28.000]   Viewability is a recent trend where ads are being charged if they actually are on the screen.
[00:33:28.000 --> 00:33:37.000]   So a huge amount, up to 50% Google set of ads were never ever seen, and across not just Google, but the web.
[00:33:37.000 --> 00:33:42.000]   So I think it behooves the industry and Google itself to be as credible as they can be.
[00:33:42.000 --> 00:33:43.000]   Yeah.
[00:33:43.000 --> 00:33:56.000]   I constantly, particularly in the midst of playing ad-supported games on my mobile device, which is small and very easy to tap, always tap on ads by mistake, and then have my game pause and I get sent to this other.
[00:33:56.000 --> 00:34:08.000]   I think what they should do is they should turn on the microphone and listen to the expletives that come out of the user's mouth at the point of which they tap, and that would be really good indicator whether or not it was an accidental click.
[00:34:08.000 --> 00:34:15.000]   And those whole screen takeover ads in mobile apps in particular are just, it makes me crazy.
[00:34:15.000 --> 00:34:19.000]   And it doesn't make me crazy that I'm looking at an ad because usually I'm using a free app.
[00:34:19.000 --> 00:34:22.000]   It's just that if I didn't mean to click on it, I don't want to go there.
[00:34:22.000 --> 00:34:28.000]   I don't want to launch a play store or a browser in the middle.
[00:34:28.000 --> 00:34:31.000]   So it would be nice if it was like, are you sure?
[00:34:31.000 --> 00:34:37.000]   Or if there were some other, I think any measures to prevent that are a good thing for everybody.
[00:34:37.000 --> 00:34:38.000]   Yeah.
[00:34:38.000 --> 00:34:39.000]   You know what drives me crazy?
[00:34:39.000 --> 00:34:43.000]   So when I go to a site that I go to all the time, I know where the click is.
[00:34:43.000 --> 00:34:49.000]   I instinctively hit that click and, oh no, the page is still loading, so the link moved and I clicked on an app.
[00:34:49.000 --> 00:34:50.000]   Yeah.
[00:34:50.000 --> 00:34:52.000]   Yeah, the jumping page, the loading page.
[00:34:52.000 --> 00:34:54.000]   Yeah, I hate that.
[00:34:54.000 --> 00:34:55.000]   That's exactly what.
[00:34:55.000 --> 00:35:04.000]   The examples that you guys are giving are exactly what this is aimed at, specifically for the full screen takeover ad that you were talking about, Gina.
[00:35:04.000 --> 00:35:12.000]   When you're doing it, you're doing a game and suddenly it takes over the full screen or it gives you that tiny little X and, you know, that actually hard to tap.
[00:35:12.000 --> 00:35:14.000]   Find the extra to find the X.
[00:35:14.000 --> 00:35:16.000]   That's exactly what they're doing.
[00:35:16.000 --> 00:35:25.000]   Like when an ad appears, they're going to have a certain amount of time so that if you tap on it immediately when it appears, it'll actually reject it.
[00:35:25.000 --> 00:35:28.000]   Because it'll say, oh, that was probably an inverter or accidental.
[00:35:28.000 --> 00:35:33.000]   They're going to widen the detectable area on the X.
[00:35:33.000 --> 00:35:42.000]   So even if it's a small X, it knows that you're intentionally hitting in that portion of the screen and, you know, it's not down to a pixel of like, oh, you're a pixel off of the X.
[00:35:42.000 --> 00:35:43.000]   So we're going to load the ad.
[00:35:43.000 --> 00:35:46.000]   It knows you're going for the X and it's going to be smarter about it.
[00:35:46.000 --> 00:35:50.000]   So it sounds like this is tackling exactly the issues that you guys have.
[00:35:50.000 --> 00:35:52.000]   You know, here's a weird use case.
[00:35:52.000 --> 00:35:57.000]   There are times when I'm on a web page and I just have this all the time in text.
[00:35:57.000 --> 00:36:00.000]   I think I'm done reading that story and I click off and I just constantly wanted to read.
[00:36:00.000 --> 00:36:01.000]   So I go back.
[00:36:01.000 --> 00:36:03.000]   The same thing happens to be with ads.
[00:36:03.000 --> 00:36:08.000]   There are moments when I get rid of the page and I actually wonder what that ad was.
[00:36:08.000 --> 00:36:10.000]   And I go back and of course it's reloaded entirely differently.
[00:36:10.000 --> 00:36:11.000]   Yeah, and you get something different.
[00:36:11.000 --> 00:36:12.000]   Yeah.
[00:36:12.000 --> 00:36:17.000]   And I would argue that if you go back quickly enough, they should recreate the page as it was.
[00:36:17.000 --> 00:36:19.000]   Just a little point.
[00:36:19.000 --> 00:36:20.000]   Yeah, that's a good point.
[00:36:20.000 --> 00:36:27.000]   Well, Mark Zuckerberg, the CEO of Facebook held a Q&A and it was a really interesting one
[00:36:27.000 --> 00:36:34.000]   in part because he got some questions from some famous people including Hawking, Schwarzenegger,
[00:36:34.000 --> 00:36:36.000]   and Jarvis, etc.
[00:36:36.000 --> 00:36:37.000]   Hey!
[00:36:37.000 --> 00:36:38.000]   Nice going, Jeff.
[00:36:38.000 --> 00:36:40.000]   Yeah, who's this Jarvis character?
[00:36:40.000 --> 00:36:42.000]   So tell us about this, Jeff.
[00:36:42.000 --> 00:36:48.000]   You're like really into this Q&A and besides the people that he talked to you, he also had
[00:36:48.000 --> 00:36:51.000]   some interesting answers including one that I like to call him out on.
[00:36:51.000 --> 00:36:52.000]   But go ahead, Jeff.
[00:36:52.000 --> 00:36:53.000]   What was your song about?
[00:36:53.000 --> 00:37:01.000]   I have to confess I was in a meeting and just went by the tabs and hit the Facebook tab
[00:37:01.000 --> 00:37:03.000]   for just a second and thought, "Oh, suck."
[00:37:03.000 --> 00:37:05.000]   For the next hour, I'll be answering questions.
[00:37:05.000 --> 00:37:08.000]   So I thought, "The BTF."
[00:37:08.000 --> 00:37:16.000]   So I'm on tension whore so I'll go in and try my luck at it and ask a question about
[00:37:16.000 --> 00:37:18.000]   instant articles and what's next for news.
[00:37:18.000 --> 00:37:19.000]   He didn't really answer that.
[00:37:19.000 --> 00:37:22.000]   He basically just sold the pitch for instant articles.
[00:37:22.000 --> 00:37:24.000]   He or someone on a staff who knows.
[00:37:24.000 --> 00:37:30.000]   But as time went on, the funny thing about this was that there's some poor, still sweating
[00:37:30.000 --> 00:37:36.000]   geek at one hacker way because the thing broke.
[00:37:36.000 --> 00:37:40.000]   And it may say break it, you know, break it often, but I don't think when the boss is trying
[00:37:40.000 --> 00:37:43.000]   to talk to the public, they want to break it.
[00:37:43.000 --> 00:37:48.000]   I would try to click on a link and it would say, "Oh, that's not here or you're no longer
[00:37:48.000 --> 00:37:49.000]   authorized to get it."
[00:37:49.000 --> 00:37:51.000]   You'd be like, "Oh, man, did I piss off Zuck?
[00:37:51.000 --> 00:37:53.000]   What happened?"
[00:37:53.000 --> 00:37:57.000]   And it would come and go and come and go the whole time he was on, it was having real
[00:37:57.000 --> 00:37:59.000]   trouble coming and going.
[00:37:59.000 --> 00:38:04.000]   And then for a long time afterwards, I couldn't even see his own answer to me.
[00:38:04.000 --> 00:38:09.000]   And the likes were behind by hundreds and it blew the system.
[00:38:09.000 --> 00:38:12.000]   Not a good thing to do with the boss, I would think.
[00:38:12.000 --> 00:38:17.000]   But then there were all kinds of other great questions and answers and I put a bunch of
[00:38:17.000 --> 00:38:20.000]   them into the rundown, which I loved.
[00:38:20.000 --> 00:38:25.000]   My favorite is probably his answer to Stephen Hawking.
[00:38:25.000 --> 00:38:26.000]   Yeah.
[00:38:26.000 --> 00:38:28.000]   So let me see if I can find that one.
[00:38:28.000 --> 00:38:30.000]   It's on the rundown.
[00:38:30.000 --> 00:38:31.000]   Yeah.
[00:38:31.000 --> 00:38:32.000]   I opened them all and now I'm going through.
[00:38:32.000 --> 00:38:36.000]   So Stephen Hawking says, "I would like to know a unified theory of gravity and the other
[00:38:36.000 --> 00:38:37.000]   forces.
[00:38:37.000 --> 00:38:41.000]   Which of the big questions and signs would you like to know the answer to and why?"
[00:38:41.000 --> 00:38:47.000]   So instead of asking a question, he demanded a question from Zuck.
[00:38:47.000 --> 00:38:49.000]   And what was Zuckenberg's answer?
[00:38:49.000 --> 00:38:52.000]   Oh shoot, no I can't find it because this is the problem.
[00:38:52.000 --> 00:38:55.000]   I find this is really difficult to navigate you guys.
[00:38:55.000 --> 00:38:58.000]   I had it earlier and then I didn't.
[00:38:58.000 --> 00:39:01.000]   You know, just to go back to what we were talking about earlier about, it's funny.
[00:39:01.000 --> 00:39:04.000]   So you asked what's the future of journalism on Facebook, Jeff?
[00:39:04.000 --> 00:39:07.000]   And yes, and Zuck did the Instant Articles pitch.
[00:39:07.000 --> 00:39:12.000]   But one point that he made was that reading news on mobile kind of sucks.
[00:39:12.000 --> 00:39:15.000]   You know, because images are slow and adds.
[00:39:15.000 --> 00:39:17.000]   Basically what we were just talking about, the page shumpiness and all that and how instant
[00:39:17.000 --> 00:39:19.000]   articles kind of solves that.
[00:39:19.000 --> 00:39:24.000]   And I got to say, I mean, I have mixed feelings about instant articles, but that was a good
[00:39:24.000 --> 00:39:25.000]   answer on his part.
[00:39:25.000 --> 00:39:28.000]   Well, why do you have mixed feelings about instant articles?
[00:39:28.000 --> 00:39:36.000]   You know, as a publisher, I don't like seeding that much control of my content and its presentation
[00:39:36.000 --> 00:39:40.000]   to a third party platform that I don't necessarily trust.
[00:39:40.000 --> 00:39:41.000]   And I get it.
[00:39:41.000 --> 00:39:43.000]   And I think we talked about it on the show before.
[00:39:43.000 --> 00:39:44.000]   Yeah, we did.
[00:39:44.000 --> 00:39:45.000]   The business model and all that.
[00:39:45.000 --> 00:39:49.000]   But I don't fundamentally trust Facebook mostly because their settings change a lot.
[00:39:49.000 --> 00:39:54.000]   And I think a lot of its ethics were based, you know, on a very young person building
[00:39:54.000 --> 00:39:55.000]   something very popular very early.
[00:39:55.000 --> 00:39:57.000]   And I know they come a long way.
[00:39:57.000 --> 00:40:01.000]   I just like if I were running Life Hacker, for instance, if I owned him was running Life Hacker right now,
[00:40:01.000 --> 00:40:04.000]   I don't, I'm not sure if I would have done instant articles.
[00:40:04.000 --> 00:40:07.000]   Yeah, you know, I'm trying to write a post right now.
[00:40:07.000 --> 00:40:12.000]   I debated with Emily Bell from Columbia at an event we had at CUNY, Street Street in Boston's
[00:40:12.000 --> 00:40:13.000]   social media weekend a week ago.
[00:40:13.000 --> 00:40:15.000]   And I'm still trying to write the post.
[00:40:15.000 --> 00:40:18.000]   It's not easy about the principles and the business terms that we need to deal with to
[00:40:18.000 --> 00:40:20.000]   have a good conversation with Facebook.
[00:40:20.000 --> 00:40:24.000]   And is Facebook responsible for informed society?
[00:40:24.000 --> 00:40:26.000]   I don't think they are.
[00:40:26.000 --> 00:40:32.000]   There's a lot of talk I hear about about the need for transparency into the algorithm.
[00:40:32.000 --> 00:40:36.000]   And you hear that all the time from news people.
[00:40:36.000 --> 00:40:40.000]   If I only knew what the algorithm was, well, if you only knew you'd game it, we know that.
[00:40:40.000 --> 00:40:45.000]   And so my idea is that maybe respected huge publishers that you choose to follow get kind
[00:40:45.000 --> 00:40:47.000]   of an allocation of attention time.
[00:40:47.000 --> 00:40:51.000]   The New York Times is telling you today, you may not care about North Carolina, but let me tell you,
[00:40:51.000 --> 00:40:53.000]   you care about North Carolina today.
[00:40:53.000 --> 00:40:54.000]   Yeah.
[00:40:54.000 --> 00:41:01.000]   And that you have a trusted, as you've made a point earlier, like a human editorial entity doing that.
[00:41:01.000 --> 00:41:03.000]   I think there's ways to deal with these questions.
[00:41:03.000 --> 00:41:07.000]   But then we get to the big questions of the universe like Stephen Hawking.
[00:41:07.000 --> 00:41:09.000]   And so here I found his answer.
[00:41:09.000 --> 00:41:12.000]   So I said, I'm most interested in questions about people.
[00:41:12.000 --> 00:41:14.000]   What will enable us to live forever?
[00:41:14.000 --> 00:41:16.000]   How do we cure all diseases?
[00:41:16.000 --> 00:41:17.000]   How does the brain work?
[00:41:17.000 --> 00:41:21.000]   How does learning work and how can we empower humans to learn a million times more?
[00:41:21.000 --> 00:41:26.000]   I'm also curious about whether there is a fundamental mathematical law underlying human
[00:41:26.000 --> 00:41:31.000]   social relationships that governs the balance of who and what we all care about.
[00:41:31.000 --> 00:41:32.000]   I bet there is.
[00:41:32.000 --> 00:41:37.000]   That to me is the most revelatory thing I've ever seen from Zuckerberg about his worldview.
[00:41:37.000 --> 00:41:38.000]   Yeah.
[00:41:38.000 --> 00:41:42.000]   Not to humble bright, but what I interviewed him, he talked about being a social engineer.
[00:41:42.000 --> 00:41:48.000]   And you know, hear what he's saying is that there is there is there is it's it's his
[00:41:48.000 --> 00:41:54.000]   relativity principle that there's there's a law, a universal law governing human behavior,
[00:41:54.000 --> 00:42:01.000]   which I'm sure people would hate the idea of, but he's got a point and he's trying to get
[00:42:01.000 --> 00:42:02.000]   that.
[00:42:02.000 --> 00:42:03.000]   We're a bunch of talking.
[00:42:03.000 --> 00:42:05.000]   It would be an expression of his business model.
[00:42:05.000 --> 00:42:06.000]   Yeah.
[00:42:06.000 --> 00:42:11.000]   The connections would would influence the things that people are interested in and their behavior.
[00:42:11.000 --> 00:42:14.000]   I mean, that's I mean, that's what he's been building his entire life, right?
[00:42:14.000 --> 00:42:15.000]   Yeah, really is.
[00:42:15.000 --> 00:42:20.000]   So we've been we've been bouncing back and forth between the Jarvis question and the Hawking
[00:42:20.000 --> 00:42:24.280]   question, I want to go back to the Jarvis question because there was a study that came out recently
[00:42:24.280 --> 00:42:29.720]   from Pew that looked about looked at where looked at where people get their news from.
[00:42:29.720 --> 00:42:30.720]   And it's really interesting.
[00:42:30.720 --> 00:42:34.880]   They divided up among baby boomers, generation X and millennials.
[00:42:34.880 --> 00:42:42.800]   So regarding Facebook, something like 39% of baby boomers get news from Facebook, 51
[00:42:42.800 --> 00:42:48.640]   of generation Xers get it from Facebook and 61% of millennials get it from Facebook,
[00:42:48.640 --> 00:42:50.760]   get their news from Facebook.
[00:42:50.760 --> 00:42:58.520]   What was interesting as well is that 60% of baby boomers get their news from local TV,
[00:42:58.520 --> 00:43:00.960]   which is a terrible place to get your news.
[00:43:00.960 --> 00:43:01.960]   Amen.
[00:43:01.960 --> 00:43:06.920]   If you ask me, and this, the first thing that jumped out at me is that how massively more
[00:43:06.920 --> 00:43:12.000]   substantive millennials are than baby boomers in terms of news consumption.
[00:43:12.000 --> 00:43:15.440]   Because if you're getting your news from Facebook, you assume that you're getting it in the
[00:43:15.440 --> 00:43:18.760]   form, they're at least reading it, they're reading something.
[00:43:18.760 --> 00:43:22.960]   Well, for now, as video was more important on Facebook, but go ahead, yes.
[00:43:22.960 --> 00:43:23.960]   Yeah.
[00:43:23.960 --> 00:43:28.960]   So, I mean, which is, you know, and so this is one of the, this is why it's a big, big
[00:43:28.960 --> 00:43:33.640]   deal that we should care what Mark Zuckerberg thinks about news and that Facebook should
[00:43:33.640 --> 00:43:36.640]   take a sense of responsibility about what's happening.
[00:43:36.640 --> 00:43:40.000]   I mean, this is one of the things that always bothered me about Comedy Central and the Daily
[00:43:40.000 --> 00:43:46.440]   Show is that, you know, is that John Stewart would get into these big arguments with CNN
[00:43:46.440 --> 00:43:48.720]   people and he would always win those arguments and so on.
[00:43:48.720 --> 00:43:50.160]   And he had a perfect defense.
[00:43:50.160 --> 00:43:57.080]   He would go on the attack and basically attack CNN for their approach to news, which is great.
[00:43:57.080 --> 00:43:58.520]   Love to see that.
[00:43:58.520 --> 00:44:02.440]   But then when they would attack him back, he would say, "Oh, we're just comedy show.
[00:44:02.440 --> 00:44:03.440]   I'm just comedy."
[00:44:03.440 --> 00:44:06.720]   We come on after a puppet show and stuff like that.
[00:44:06.720 --> 00:44:10.000]   But he wasn't taking responsibility of the fact that a huge number of people get their
[00:44:10.000 --> 00:44:12.400]   news from the Daily Show.
[00:44:12.400 --> 00:44:17.440]   And so I think that any, I think that there used to be a lot of responsibility around news
[00:44:17.440 --> 00:44:18.760]   that used to exist.
[00:44:18.760 --> 00:44:23.080]   For example, there would always be a paper of record for every city and they took a certain
[00:44:23.080 --> 00:44:24.080]   degree of responsibility.
[00:44:24.080 --> 00:44:28.840]   It used to be a deal if you had news on TV, if you're on the, on the over the airwaves
[00:44:28.840 --> 00:44:34.680]   TV, you did the news not as a way to make money, but as a responsibility as an organization
[00:44:34.680 --> 00:44:36.920]   that's using the public airwaves.
[00:44:36.920 --> 00:44:42.960]   And so I think that anybody, including and especially Facebook, who, but for whatever
[00:44:42.960 --> 00:44:48.840]   reason, has a huge number of people getting their information that they get as a voter
[00:44:48.840 --> 00:44:54.600]   and a citizen from them, they have a responsibility to not say, "Hey, it's not us."
[00:44:54.600 --> 00:44:55.600]   You know, we're just...
[00:44:55.600 --> 00:44:56.600]   Well, okay, let's stay on that.
[00:44:56.600 --> 00:44:58.640]   Let's stay on that, Mike, because that's part of what I'm trying to write.
[00:44:58.640 --> 00:45:01.680]   So you're going to help me write my post.
[00:45:01.680 --> 00:45:09.520]   If Facebook says that it has a responsibility for an informed society, let's back up.
[00:45:09.520 --> 00:45:11.400]   Facebook says that it does three things.
[00:45:11.400 --> 00:45:15.080]   It connects people with each other at number one with information number two and with entertainment
[00:45:15.080 --> 00:45:17.360]   number three.
[00:45:17.360 --> 00:45:23.960]   And if Facebook said that, okay, we have a responsibility, if so many people are getting
[00:45:23.960 --> 00:45:27.480]   their news from us, we have a responsibility to have an informed society, what that implies
[00:45:27.480 --> 00:45:30.400]   to me is that Facebook then hires journalists.
[00:45:30.400 --> 00:45:34.320]   And Emily Bell said, "Hey, good thing somebody's going to pay him, but I don't want Facebook
[00:45:34.320 --> 00:45:37.040]   to do that because I don't want Facebook to be to Gina's point.
[00:45:37.040 --> 00:45:40.360]   It's bad enough that you lose your distribution mechanism to Facebook.
[00:45:40.360 --> 00:45:43.400]   And my view is, you know, Gina, we've lost them already.
[00:45:43.400 --> 00:45:46.800]   So good that they're offering us a revenue stream and good that we can try to learn some
[00:45:46.800 --> 00:45:48.520]   more and do some more.
[00:45:48.520 --> 00:45:51.760]   And it's just the reality we have.
[00:45:51.760 --> 00:45:57.320]   But having lost that, at least we're still in control of the creation of the general judgment
[00:45:57.320 --> 00:46:03.040]   that occurs back at HQ, if Facebook said, well, you know, we got to judge our success
[00:46:03.040 --> 00:46:06.880]   now and whether we're there, the society is informed, then people will accuse them of
[00:46:06.880 --> 00:46:10.680]   manipulating society, manipulating voters and so on.
[00:46:10.680 --> 00:46:12.960]   That puts them in a difficult position as a platform.
[00:46:12.960 --> 00:46:16.320]   And I think they ended up having to hire journalists and editors and I don't think that
[00:46:16.320 --> 00:46:18.280]   other news organizations want that to happen.
[00:46:18.280 --> 00:46:19.280]   And Facebook doesn't want it to happen.
[00:46:19.280 --> 00:46:20.760]   They don't want to be in channel conflict.
[00:46:20.760 --> 00:46:26.600]   So I don't think Facebook has a responsibility itself to make sure that society is informed.
[00:46:26.600 --> 00:46:33.320]   However, having said that, I would argue that in partnership with news organizations,
[00:46:33.320 --> 00:46:36.800]   Facebook can help them do a better job of doing that.
[00:46:36.800 --> 00:46:42.400]   And certain things fall out of that, like this idea of an allocation of attention time
[00:46:42.400 --> 00:46:46.680]   for users who, if I follow the New York Times, I give the New York Times permission.
[00:46:46.680 --> 00:46:50.160]   Facebook gives them some opportunity to say, hello, Dumbo, you got to pay attention to
[00:46:50.160 --> 00:46:51.160]   this.
[00:46:51.160 --> 00:46:54.280]   If I don't follow the New York Times, should Facebook force the New York Times on me?
[00:46:54.280 --> 00:46:56.280]   No, I don't really think so.
[00:46:56.280 --> 00:47:00.400]   But if I've shown myself to want to be informed and read the New York Times and I had that
[00:47:00.400 --> 00:47:04.360]   breeding it more on Facebook, then maybe that ought to be a better experience in terms
[00:47:04.360 --> 00:47:08.440]   of the context and quality of what I get.
[00:47:08.440 --> 00:47:09.800]   But this is not going to be easy.
[00:47:09.800 --> 00:47:11.200]   This is hard stuff.
[00:47:11.200 --> 00:47:12.200]   Yeah.
[00:47:12.200 --> 00:47:16.400]   He also answered Ariana Huffington who asked about the role in news and he said that he
[00:47:16.400 --> 00:47:20.880]   saw two trends toward one richness and two speed and frequency.
[00:47:20.880 --> 00:47:24.920]   He said, we're seeing more rich content online instead of just text and photos.
[00:47:24.920 --> 00:47:28.120]   Now quoting him, we're now seeing more and more videos.
[00:47:28.120 --> 00:47:32.720]   This will continue into the future and we'll see more immersive content like VR.
[00:47:32.720 --> 00:47:36.520]   For now though, making sure news organizations are delivering increasingly rich content is
[00:47:36.520 --> 00:47:39.320]   important and that's what people want.
[00:47:39.320 --> 00:47:42.920]   Really interesting because that's what instant articles kind of forces you to do.
[00:47:42.920 --> 00:47:45.800]   If all you do is put up text, that's so great.
[00:47:45.800 --> 00:47:47.040]   You want to use the tools.
[00:47:47.040 --> 00:47:49.880]   The second point on speed and frequency.
[00:47:49.880 --> 00:47:59.160]   People news is thoroughly vetted, but this model has a hard time keeping us up to date,
[00:47:59.160 --> 00:48:02.320]   I guess, with important things happening constantly.
[00:48:02.320 --> 00:48:07.520]   There's an important place for news organizations that can deliver smaller bits of news faster
[00:48:07.520 --> 00:48:09.640]   and more frequently in pieces.
[00:48:09.640 --> 00:48:13.840]   This will replace the longer and more researched work, but I'm not sure anyone has fully nailed
[00:48:13.840 --> 00:48:14.840]   this yet.
[00:48:14.840 --> 00:48:19.520]   I think those are two really important insights, especially the latter.
[00:48:19.520 --> 00:48:26.200]   Circa, me at Reston piece, was trying to do that and didn't have a business model and
[00:48:26.200 --> 00:48:27.200]   no platform bottom.
[00:48:27.200 --> 00:48:31.240]   Though there's still time, Zaki could buy circa.
[00:48:31.240 --> 00:48:35.000]   And so you could deliver that quick hit of what's new and not have to rewrite the whole
[00:48:35.000 --> 00:48:37.080]   thing into a 600 word story.
[00:48:37.080 --> 00:48:39.880]   So those are the two newsy questions in the Zuck Q&A.
[00:48:39.880 --> 00:48:46.880]   Well, one of the things that strikes me is that we don't want Mark Zuckerberg and Facebook
[00:48:46.880 --> 00:48:52.200]   sort of manipulating society and taking an editorial role, becoming a newspaper and so
[00:48:52.200 --> 00:48:55.520]   on just because people are getting the news from Facebook.
[00:48:55.520 --> 00:49:01.000]   But we have to realize that they are manipulating everything that happens on Facebook.
[00:49:01.000 --> 00:49:02.400]   So do editors.
[00:49:02.400 --> 00:49:03.400]   So do marketers.
[00:49:03.400 --> 00:49:07.520]   Okay, but the editors do it for a different reason than Facebook does.
[00:49:07.520 --> 00:49:15.040]   Facebook, for example, has radically ramped up the number of videos that are viewed on
[00:49:15.040 --> 00:49:20.800]   Facebook and they've done it by tweaking their algorithm to favor videos.
[00:49:20.800 --> 00:49:24.320]   And they've done that because they want to sell video advertising because video ads make
[00:49:24.320 --> 00:49:25.760]   a lot more money.
[00:49:25.760 --> 00:49:28.520]   And so the argument also liked that people liked video and watch video.
[00:49:28.520 --> 00:49:31.080]   So maybe they're responding to what they see from the public?
[00:49:31.080 --> 00:49:34.480]   Well, but I mean, they liked it two years ago as well.
[00:49:34.480 --> 00:49:40.000]   But Facebook has increased by an order of magnitude, the number of views.
[00:49:40.000 --> 00:49:44.160]   The public's desire to watch videos hasn't changed much, but Facebook's desire to sell
[00:49:44.160 --> 00:49:48.080]   video ads has changed and their algorithm has changed accordingly.
[00:49:48.080 --> 00:49:53.280]   And they've also done some tweaking around how do you judge whether a video has been
[00:49:53.280 --> 00:49:54.280]   viewed?
[00:49:54.280 --> 00:49:57.600]   For example, if you listened to it, you liked it and you watched it and that counts as
[00:49:57.600 --> 00:49:59.840]   we get paid for that.
[00:49:59.840 --> 00:50:06.520]   But the larger point is that I think they have a responsibility when it comes to news, when
[00:50:06.520 --> 00:50:12.000]   it comes to where the public gets its information about citizen related things, about public,
[00:50:12.000 --> 00:50:20.040]   you know, about current events, to find alignments between what is in their interest and what
[00:50:20.040 --> 00:50:21.280]   is in the public interest.
[00:50:21.280 --> 00:50:23.280]   And I think Twitter is doing that beautifully.
[00:50:23.280 --> 00:50:27.440]   Twitter is working on this thing that I mentioned earlier called Project Lightning.
[00:50:27.440 --> 00:50:34.000]   The story was broke on BuzzFeed News, I believe it was Matt Honan.
[00:50:34.000 --> 00:50:38.720]   And Project Lightning is they're going to just let, they're going to wait until conversations
[00:50:38.720 --> 00:50:40.760]   occur spontaneously on Twitter.
[00:50:40.760 --> 00:50:45.400]   And then they're going to have human curation to come in and give you really great content
[00:50:45.400 --> 00:50:48.640]   around that conversation.
[00:50:48.640 --> 00:50:55.120]   So let's say it's a news event, for example, let's say there's a riot in Los Angeles and
[00:50:55.120 --> 00:50:57.560]   people start talking about that on Twitter.
[00:50:57.560 --> 00:51:03.360]   They're going to cherry pick news stories that are breaking news stories, videos from
[00:51:03.360 --> 00:51:09.720]   local news channels, photos from eyewitness accounts, vines, et cetera.
[00:51:09.720 --> 00:51:14.280]   Meaning that anybody is producing anywhere, they're going to bring it together into a
[00:51:14.280 --> 00:51:19.920]   subscribers product that essentially package it.
[00:51:19.920 --> 00:51:26.320]   And then you can follow that issue as if it was a user on Twitter that you could follow.
[00:51:26.320 --> 00:51:28.720]   This is very much in Twitter's interest to do this.
[00:51:28.720 --> 00:51:32.200]   And I think this is very much in the public interest for them to do this as well.
[00:51:32.200 --> 00:51:38.080]   When people want to know about something, Twitter's problematic when a billion people
[00:51:38.080 --> 00:51:40.880]   are posting the same tweet and retweeting the same thing.
[00:51:40.880 --> 00:51:41.880]   It's not useful.
[00:51:41.880 --> 00:51:43.080]   It's a waste of time.
[00:51:43.080 --> 00:51:46.960]   But if Twitter can go in there and give you essentially an instant newspaper, an instant
[00:51:46.960 --> 00:51:51.600]   multimedia newspaper, for lack of a better term, multimedia, nobody says that anymore.
[00:51:51.600 --> 00:51:57.600]   But this rich experience of this topic, I think that's going to go a long way toward
[00:51:57.600 --> 00:52:00.840]   better informing the public and also making Twitter more attractive.
[00:52:00.840 --> 00:52:04.800]   What do you, I've been dying to know what you think of this, Gina, since you live in
[00:52:04.800 --> 00:52:05.800]   Dyerick Twitter.
[00:52:05.800 --> 00:52:12.120]   Well, so one point I wanted to make is, I mean, I do think that there is a difference between
[00:52:12.120 --> 00:52:13.960]   commentary and news itself.
[00:52:13.960 --> 00:52:15.920]   What we're doing right now is commentary.
[00:52:15.920 --> 00:52:18.840]   We're not reporting the news, we're commenting on it.
[00:52:18.840 --> 00:52:24.640]   As a consumer and as a user, I take full responsibility for the fact that the way that
[00:52:24.640 --> 00:52:28.760]   I find news is commentary first and then the news.
[00:52:28.760 --> 00:52:31.760]   I mean, this is a Supreme Court decision which would greatly affect my life.
[00:52:31.760 --> 00:52:33.160]   I found out about that on Twitter.
[00:52:33.160 --> 00:52:35.960]   I found out about that from my friends celebrating.
[00:52:35.960 --> 00:52:40.560]   So I started at the commentary and then I backed up to the report of what had happened, the
[00:52:40.560 --> 00:52:41.560]   facts.
[00:52:41.560 --> 00:52:44.280]   So it didn't used to be like that.
[00:52:44.280 --> 00:52:48.120]   Back in the day when things were slower and different, you would read the news and the
[00:52:48.120 --> 00:52:49.120]   commentary would come later.
[00:52:49.120 --> 00:52:53.600]   So it's like the carp behind before the horse a little bit.
[00:52:53.600 --> 00:52:55.440]   So that's what people are getting on Facebook.
[00:52:55.440 --> 00:53:01.760]   They're getting commentary on the news but then clicking through if things go well to
[00:53:01.760 --> 00:53:02.760]   the source.
[00:53:02.760 --> 00:53:03.760]   So I'm just going to face it.
[00:53:03.760 --> 00:53:08.560]   A lot of people just read the snippet or the headline and then reshared or like it or comment
[00:53:08.560 --> 00:53:09.560]   on it.
[00:53:09.560 --> 00:53:12.240]   So I mean, and that's kind of another point for instant articles and other positive thing
[00:53:12.240 --> 00:53:17.080]   for instant articles is that it, people, more people might read it because the entire article
[00:53:17.080 --> 00:53:20.760]   is there and it's not just somebody's commentary on it.
[00:53:20.760 --> 00:53:24.600]   So that's, there's, that was one, the point I want to make earlier when we're talking about
[00:53:24.600 --> 00:53:29.400]   people getting their news on Facebook and Twitter and in social.
[00:53:29.400 --> 00:53:34.040]   The Twitter project lightning is really interesting because it first sounds a lot like circa,
[00:53:34.040 --> 00:53:35.880]   what circa was trying to do.
[00:53:35.880 --> 00:53:39.440]   And it sounds a lot like what really good reporters who are super Twitter savvy try to
[00:53:39.440 --> 00:53:40.440]   do, right?
[00:53:40.440 --> 00:53:42.280]   Who's your friend, Jeff?
[00:53:42.280 --> 00:53:44.520]   Andy, the NPR guy.
[00:53:44.520 --> 00:53:45.520]   Andy Carvin.
[00:53:45.520 --> 00:53:46.520]   Yeah, Andy Carvin.
[00:53:46.520 --> 00:53:47.520]   Andy Carvin.
[00:53:47.520 --> 00:53:51.320]   Now it reportedly, now it was a business built around this called reportedly.
[00:53:51.320 --> 00:53:52.320]   Yeah.
[00:53:52.320 --> 00:53:55.200]   So I mean, that's really, that's really interesting to me.
[00:53:55.200 --> 00:53:56.200]   And that's the kind of thing.
[00:53:56.200 --> 00:54:00.120]   When I do searches, when all the news was breaking last week, particularly that I was
[00:54:00.120 --> 00:54:03.440]   interested in, I was in Twitter searches watching hashtags and yeah, it's annoying to
[00:54:03.440 --> 00:54:06.040]   see the same tweets go by or the less substantive ones.
[00:54:06.040 --> 00:54:07.960]   I wanted to see the really good, the top tweets.
[00:54:07.960 --> 00:54:10.600]   I find the top tweets results in search results really good.
[00:54:10.600 --> 00:54:16.160]   And so I love the idea of project lightning and obviously I was sad when circa, you know,
[00:54:16.160 --> 00:54:19.760]   kind of announced that they were shutting down because I really liked it.
[00:54:19.760 --> 00:54:23.520]   I would follow stories and then kind of months later you would get these updates that were
[00:54:23.520 --> 00:54:24.520]   minor.
[00:54:24.520 --> 00:54:25.520]   They were interested in the story.
[00:54:25.520 --> 00:54:29.280]   They were interesting to you and maybe didn't bubble up in your circles.
[00:54:29.280 --> 00:54:30.280]   So there's something there.
[00:54:30.280 --> 00:54:31.920]   I think it's interesting.
[00:54:31.920 --> 00:54:35.280]   And really important there, Gene, I think circa, circa enabled two things.
[00:54:35.280 --> 00:54:36.760]   One was that you could follow a story.
[00:54:36.760 --> 00:54:37.920]   So you had that.
[00:54:37.920 --> 00:54:40.400]   It had your permission to bug you when something new happened.
[00:54:40.400 --> 00:54:41.400]   Yes.
[00:54:41.400 --> 00:54:44.480]   Second, because it, it already knew what you knew and it had all the background previously
[00:54:44.480 --> 00:54:47.000]   done as just what Zuck just said.
[00:54:47.000 --> 00:54:51.320]   If a story broke and all you had to say was, you know, one little thing, you didn't have
[00:54:51.320 --> 00:54:55.400]   to wait until you wrote the 800 word background and all that, you just put out the bullet
[00:54:55.400 --> 00:54:58.800]   and then it linked to all the following things that before.
[00:54:58.800 --> 00:55:03.680]   While we're on Twitter and the Supreme Court decision, I have to say that your tweet was
[00:55:03.680 --> 00:55:07.560]   my absolute favorite in the entirety of the wonders of Twitter that day.
[00:55:07.560 --> 00:55:11.080]   It was just succinct and elegant and profound.
[00:55:11.080 --> 00:55:12.080]   I missed it.
[00:55:12.080 --> 00:55:13.080]   What did she say?
[00:55:13.080 --> 00:55:14.080]   Thank you.
[00:55:14.080 --> 00:55:16.520]   I, I, I just said married period.
[00:55:16.520 --> 00:55:18.240]   That was, that was the whole thing.
[00:55:18.240 --> 00:55:22.240]   And it's funny, a lot of folks misunderstood and thought that I like run down a city hall
[00:55:22.240 --> 00:55:24.680]   and say, well, I got married that day.
[00:55:24.680 --> 00:55:27.680]   But my point was just like, we're married and there's no asterisk.
[00:55:27.680 --> 00:55:32.600]   Oh, in California 2008 and then grandfather day and after prop eight and then in New York,
[00:55:32.600 --> 00:55:35.680]   but just because we got grandfather, you know, there was no, there was no footnote.
[00:55:35.680 --> 00:55:36.680]   We just are now.
[00:55:36.680 --> 00:55:37.680]   So thank you, Jeff.
[00:55:37.680 --> 00:55:38.680]   I saw that.
[00:55:38.680 --> 00:55:39.680]   I saw your tweet.
[00:55:39.680 --> 00:55:41.280]   That was why I think one of my, maybe one of my most popular tweets ever.
[00:55:41.280 --> 00:55:46.040]   So probably do in, in, in some part.
[00:55:46.040 --> 00:55:49.840]   Well, if anyone would know whether it's your most popular tweet, you would.
[00:55:49.840 --> 00:55:50.840]   Yeah.
[00:55:50.840 --> 00:55:53.520]   If only there were a tool that was able to see that.
[00:55:53.520 --> 00:55:54.520]   Right.
[00:55:54.520 --> 00:55:55.520]   Yeah.
[00:55:55.520 --> 00:55:57.760]   Well, you know, we've still got a few more months left to the year.
[00:55:57.760 --> 00:55:59.120]   So my best work is ahead of me.
[00:55:59.120 --> 00:56:02.520]   So we'll see at the end of the year if it was my biggest tweet of 2015.
[00:56:02.520 --> 00:56:03.520]   All right.
[00:56:03.520 --> 00:56:07.520]   Before we leave these two topics, one is the Supreme Court's ban on marriage discrimination.
[00:56:07.520 --> 00:56:13.800]   The other one is the issue of commentary versus news on Facebook.
[00:56:13.800 --> 00:56:17.120]   Facebook immediately came out with a tool that enabled you to take your profile picture
[00:56:17.120 --> 00:56:20.360]   and turn it into a rainbow color, which lots and lots of people did.
[00:56:20.360 --> 00:56:24.000]   And in fact, not only did they use that on, on Facebook, but a lot of people took their
[00:56:24.000 --> 00:56:26.840]   Facebook profile picture and they moved it over to Twitter.
[00:56:26.840 --> 00:56:28.680]   And these proliferated literally millions.
[00:56:28.680 --> 00:56:34.400]   I think it was 26, 27 million people use the tool or, or something like that.
[00:56:34.400 --> 00:56:36.280]   I saw a number like that somewhere.
[00:56:36.280 --> 00:56:44.800]   Now is that, is that a case theoretically of Facebook facilitating a kind of commentary?
[00:56:44.800 --> 00:56:48.320]   I mean, you wouldn't, you wouldn't use that tool unless, you know, the, the point of
[00:56:48.320 --> 00:56:51.080]   using that tool is to say, I approve of this.
[00:56:51.080 --> 00:56:52.080]   Yeah.
[00:56:52.080 --> 00:56:54.000]   I'm the White House changing its colors.
[00:56:54.000 --> 00:56:55.000]   Yep.
[00:56:55.000 --> 00:56:56.000]   And I think that's their right to do.
[00:56:56.000 --> 00:56:57.000]   It's their right to do.
[00:56:57.000 --> 00:57:01.560]   But is that an approach like should that, that, and that's a positive thing for, of
[00:57:01.560 --> 00:57:07.240]   course, for the majority who's in favor of, of the Supreme Court decision, but what if
[00:57:07.240 --> 00:57:11.560]   they did the same thing, what if they offered a whole menu of ways for you to comment on
[00:57:11.560 --> 00:57:12.560]   the news?
[00:57:12.560 --> 00:57:13.560]   What if they went negative?
[00:57:13.560 --> 00:57:16.400]   What if they offered a simple way for you to have the Confederate flag with a red thing
[00:57:16.400 --> 00:57:17.400]   through it?
[00:57:17.400 --> 00:57:22.160]   And then they, they had a, you know, a pro life way to change your profile or something
[00:57:22.160 --> 00:57:23.160]   like that.
[00:57:23.160 --> 00:57:25.400]   Is this something Facebook should be doing?
[00:57:25.400 --> 00:57:26.400]   Yeah.
[00:57:26.400 --> 00:57:28.280]   That's a really good question.
[00:57:28.280 --> 00:57:32.440]   I couldn't believe we, you know, think of does kind of avatar changes, tracks, avatar changes,
[00:57:32.440 --> 00:57:36.280]   and I couldn't believe, I mean, I watched my entire network work on Facebook and, and
[00:57:36.280 --> 00:57:38.480]   Twitter and Instagram change to rainbow.
[00:57:38.480 --> 00:57:39.480]   It was crazy.
[00:57:39.480 --> 00:57:42.200]   Even like official New York City Twitter accounts.
[00:57:42.200 --> 00:57:44.240]   It was, it was kind of amazing to watch.
[00:57:44.240 --> 00:57:46.000]   And I mean, of course I was happy about it, right?
[00:57:46.000 --> 00:57:47.840]   Because this is an issue I care about.
[00:57:47.840 --> 00:57:51.400]   But it is, it's, it's an interesting question, you know, that, you know, why this issue and
[00:57:51.400 --> 00:57:52.400]   not another issue.
[00:57:52.400 --> 00:57:55.880]   I mean, I think, I think Facebook and sort of, you know, the, the kind of West Coast tech
[00:57:55.880 --> 00:58:02.160]   companies have all been very pro, you know, equality, particularly LGBT community.
[00:58:02.160 --> 00:58:03.640]   But it is a good question.
[00:58:03.640 --> 00:58:05.800]   Should, should there be options for this every single time?
[00:58:05.800 --> 00:58:08.720]   And this is, this is one of those issues that is just kind of part of the zeitgeist
[00:58:08.720 --> 00:58:09.720]   right now.
[00:58:09.720 --> 00:58:11.320]   And I had this like tipping point.
[00:58:11.320 --> 00:58:15.080]   And I think Facebook was, you know, offered a tool that they thought what people would
[00:58:15.080 --> 00:58:16.480]   love and would be cool.
[00:58:16.480 --> 00:58:19.160]   And people did, right?
[00:58:19.160 --> 00:58:23.680]   But you know, I mean, this was so closely tied to identity politics that I think it made
[00:58:23.680 --> 00:58:27.080]   sense to do the avatar thing.
[00:58:27.080 --> 00:58:30.880]   But there are lots of other holidays, news events, things that could happen where they
[00:58:30.880 --> 00:58:32.600]   could offer a similar, similar tool.
[00:58:32.600 --> 00:58:34.000]   And is it up to them to do that?
[00:58:34.000 --> 00:58:35.000]   I mean, I don't know.
[00:58:35.000 --> 00:58:38.160]   I think that Facebook is going to do the thing that think they think what people will use
[00:58:38.160 --> 00:58:39.160]   and that will be popular.
[00:58:39.160 --> 00:58:44.920]   And I think Facebook, Dr. Dejina also, Facebook made a, made a comment.
[00:58:44.920 --> 00:58:49.040]   It, it, it, it, and by the way, lots of brands did.
[00:58:49.040 --> 00:58:52.800]   I got into a little, a little discussion with an old friend of mine from People Magazine
[00:58:52.800 --> 00:58:59.480]   years ago who complained about a, I forget what the brand was, a bank or something, a
[00:58:59.480 --> 00:59:02.400]   corporate float at the gay pride parade.
[00:59:02.400 --> 00:59:06.400]   And I respond, I said, no, I find the fact that all these brands have been tripping over
[00:59:06.400 --> 00:59:13.680]   themselves to put rainbows up is great because it says, it's, it marginalizes the haters.
[00:59:13.680 --> 00:59:20.320]   And it says that it is mainstream, same, same exact thing for me with not just Univision,
[00:59:20.320 --> 00:59:26.240]   which is fairly obvious, not just big giant, often evil, Comcast, NBC, Universal, Behemoth,
[00:59:26.240 --> 00:59:27.800]   Corp.
[00:59:27.800 --> 00:59:32.560]   And also Macy's telling Donald Trump to shut up and go away.
[00:59:32.560 --> 00:59:34.600]   This is corporations taking a stand.
[00:59:34.600 --> 00:59:38.040]   Now you're right, Mike, they could take a stand, you know, something I don't like in favor
[00:59:38.040 --> 00:59:40.880]   of guns or against gay people.
[00:59:40.880 --> 00:59:44.600]   But I think it's, I think it's okay for Facebook to do this.
[00:59:44.600 --> 00:59:48.680]   Now the other discussion I saw was that, you know, I had a discussion with one of my,
[00:59:48.680 --> 00:59:50.000]   my social journalism students.
[00:59:50.000 --> 00:59:55.800]   It was okay for a news organization to change its colors.
[00:59:55.800 --> 00:59:57.760]   And she had more of a problem with that.
[00:59:57.760 --> 01:00:01.360]   I saw what about her individual reporter and she was okay with that.
[01:00:01.360 --> 01:00:04.240]   The arguments by the other side about the White House and the colors, the White House
[01:00:04.240 --> 01:00:06.400]   said, screw it, we're happy.
[01:00:06.400 --> 01:00:09.640]   This is, this is what we think society ought to be.
[01:00:09.640 --> 01:00:14.840]   Also Ben Smith, the editor of BuzzFeed said there are certain issues like gay marriage
[01:00:14.840 --> 01:00:17.520]   and the Confederate flag for which there aren't two sides.
[01:00:17.520 --> 01:00:18.520]   Damn it.
[01:00:18.520 --> 01:00:21.440]   And that's what their attitude is going to be.
[01:00:21.440 --> 01:00:25.160]   So yeah, I could be unhappy when it's something I disagree with.
[01:00:25.160 --> 01:00:28.880]   But right now I kind of like that Facebook did this and enabled people to have an expression.
[01:00:28.880 --> 01:00:32.720]   Now having said that, I also think changing your avatar and putting green in or blue
[01:00:32.720 --> 01:00:39.320]   in or whatever to support something I think is pretty much worthless except that this
[01:00:39.320 --> 01:00:43.800]   case the volume of it is the volumes.
[01:00:43.800 --> 01:00:50.240]   I think one of the reasons why they probably decided to do this is that they weren't doing
[01:00:50.240 --> 01:00:55.080]   it as you mentioned that it marginalized the haters but they didn't do it on an issue
[01:00:55.080 --> 01:00:58.480]   where that issue was about to be decided.
[01:00:58.480 --> 01:01:01.520]   They did it on an issue where it had been decided.
[01:01:01.520 --> 01:01:02.520]   Yes.
[01:01:02.520 --> 01:01:03.800]   And there was no influencing the outcome.
[01:01:03.800 --> 01:01:08.320]   The outcome was already had already come out so to speak.
[01:01:08.320 --> 01:01:13.160]   And so I think that may have, but what if it's an election?
[01:01:13.160 --> 01:01:18.920]   What if it's a proposition in California or something like that to legalize recreational
[01:01:18.920 --> 01:01:25.840]   marijuana or something that can let you turn your avatar smoky and green or something?
[01:01:25.840 --> 01:01:27.200]   They probably wouldn't I would think.
[01:01:27.200 --> 01:01:31.360]   I mean I think that's one of the things about this that is an interesting case as a political
[01:01:31.360 --> 01:01:33.160]   issue is that it's done.
[01:01:33.160 --> 01:01:34.960]   It's a done deal.
[01:01:34.960 --> 01:01:37.080]   And so maybe that's why they did it.
[01:01:37.080 --> 01:01:39.160]   Yeah, I mean it was it was interesting.
[01:01:39.160 --> 01:01:43.440]   And then Twitter also put in those little icons like if you if you tweeted the pride hashtag
[01:01:43.440 --> 01:01:46.440]   there was a little pride there was a little rainbow flag you treated the hashtag love
[01:01:46.440 --> 01:01:47.440]   wins.
[01:01:47.440 --> 01:01:50.400]   It was in rainbow heart and that was only for a couple of days they actually removed them
[01:01:50.400 --> 01:01:51.760]   now.
[01:01:51.760 --> 01:01:55.440]   And the last time they did something like that was when the Star Wars trailer was announced
[01:01:55.440 --> 01:01:59.080]   right if you if you tweeted like the BB8 hashtag that would show and that was clearly
[01:01:59.080 --> 01:02:01.600]   a deal right with the movie company.
[01:02:01.600 --> 01:02:05.960]   The pride thing was you know it just it you know there's just this like this cool factor.
[01:02:05.960 --> 01:02:09.560]   It was amazing for me as someone who grew up in New York City going to the Pride Parade.
[01:02:09.560 --> 01:02:13.160]   You know the Pride Parade went from like this you know place of like freaks and queers and
[01:02:13.160 --> 01:02:18.960]   weirdos right to like brands like these huge corporations that made me happy but it was
[01:02:18.960 --> 01:02:23.880]   just it became this thing where like it really wasn't cool if you weren't part of it.
[01:02:23.880 --> 01:02:28.680]   So you know mixed feelings about mostly positive though to see brands do that.
[01:02:28.680 --> 01:02:32.600]   And I think that's just really just a lot about Facebook and Twitter saying like this
[01:02:32.600 --> 01:02:35.040]   is the side that we kind that we align with.
[01:02:35.040 --> 01:02:37.040]   It wasn't a lot of it.
[01:02:37.040 --> 01:02:38.040]   Go ahead.
[01:02:38.040 --> 01:02:39.040]   No, no, go ahead.
[01:02:39.040 --> 01:02:40.040]   Please keep playing.
[01:02:40.040 --> 01:02:41.040]   No, no, no, keep going.
[01:02:41.040 --> 01:02:44.600]   There was a and I forget what the context for this was but someone was complaining somewhere
[01:02:44.600 --> 01:02:50.160]   about how BuzzFeed was was was too feminist and progressive and liberal and said like
[01:02:50.160 --> 01:02:55.640]   is this a feminist website and one of the higher up BuzzFeed editors just responded yes
[01:02:55.640 --> 01:03:00.240]   period like we are a feminist you know news organization which was also like you know
[01:03:00.240 --> 01:03:04.800]   that's that's a very you know that's that's a particular opinion that's a particular
[01:03:04.800 --> 01:03:08.920]   band of course BuzzFeed is commentary and not news I mean would I don't know what the
[01:03:08.920 --> 01:03:12.760]   New York Times describe itself as a feminist news organization.
[01:03:12.760 --> 01:03:14.280]   I don't know.
[01:03:14.280 --> 01:03:18.320]   They wouldn't because because they won't even describe themselves as liberal until their
[01:03:18.320 --> 01:03:25.920]   first public editor did but they are and and that's the old ethic and I think I think
[01:03:25.920 --> 01:03:27.480]   it's a new ethic.
[01:03:27.480 --> 01:03:33.560]   But we also have to recognize that the vox did a great visualization of the I mean this
[01:03:33.560 --> 01:03:37.800]   was been a long long long struggle generations and hundreds of years but once it's once the
[01:03:37.800 --> 01:03:44.440]   first domino fell the speed was phenomenal in terms of supporting marriage.
[01:03:44.440 --> 01:03:52.640]   Remember not long ago when Disney said that they give benefits to gay couples all the
[01:03:52.640 --> 01:03:57.000]   calls for boycotting Disney and Disney's awful and Disney's too filled with it.
[01:03:57.000 --> 01:04:00.000]   The change is incredible.
[01:04:00.000 --> 01:04:02.280]   Internet speed was Internet speed.
[01:04:02.280 --> 01:04:03.280]   Yeah.
[01:04:03.280 --> 01:04:04.600]   That only because it happened because the Internet I think.
[01:04:04.600 --> 01:04:05.600]   I think you're right.
[01:04:05.600 --> 01:04:06.800]   I think I think you're right.
[01:04:06.800 --> 01:04:11.280]   I went down to the to Stonewall the day of the announcement just because just because
[01:04:11.280 --> 01:04:16.960]   I wanted to be there and see the feeling and and it was really interesting because it was
[01:04:16.960 --> 01:04:21.920]   it was it was it was I was also heartened that it was straight couples with their kids
[01:04:21.920 --> 01:04:25.280]   wanting to be there for the history of this.
[01:04:25.280 --> 01:04:28.440]   And it was just a lot of people.
[01:04:28.440 --> 01:04:31.680]   But I also know that I don't know if you thought saw the piece the next day.
[01:04:31.680 --> 01:04:35.920]   Gina the New York Times saying that there was something potentially lost here too.
[01:04:35.920 --> 01:04:39.680]   There was a there was a common.
[01:04:39.680 --> 01:04:42.520]   Struggle that's once it's over.
[01:04:42.520 --> 01:04:46.440]   What do you do with the community around that struggle and it's interesting questions.
[01:04:46.440 --> 01:04:47.440]   Yeah.
[01:04:47.440 --> 01:04:48.440]   Yeah.
[01:04:48.440 --> 01:04:49.440]   Definitely.
[01:04:49.440 --> 01:04:55.240]   There's a hilarious New Yorker cartoon from a few years ago where there's a gay couple
[01:04:55.240 --> 01:05:00.680]   two men and one of them's on the phone talking to some unseen friend and he said no we're
[01:05:00.680 --> 01:05:02.520]   not going to the gay pride parade this year.
[01:05:02.520 --> 01:05:03.520]   We're here.
[01:05:03.520 --> 01:05:04.520]   We're queer.
[01:05:04.520 --> 01:05:05.520]   We used to it.
[01:05:05.520 --> 01:05:08.120]   That was like three or four years ago.
[01:05:08.120 --> 01:05:09.760]   Anyway, hilarious.
[01:05:09.760 --> 01:05:12.200]   But but you know you mentioned that this is because the Internet.
[01:05:12.200 --> 01:05:14.640]   I'm not so sure that that's actually true.
[01:05:14.640 --> 01:05:20.160]   I think that this is actually one of the longstanding characters of the American people
[01:05:20.160 --> 01:05:26.440]   which is that Americans are extreme in some negative way and then it almost overnight
[01:05:26.440 --> 01:05:28.640]   become extreme in the other way.
[01:05:28.640 --> 01:05:30.200]   And just let me give you a couple of examples.
[01:05:30.200 --> 01:05:32.160]   So that's maybe they're not related.
[01:05:32.160 --> 01:05:34.320]   Maybe marriage equality isn't related to these other things.
[01:05:34.320 --> 01:05:40.200]   But Americans used to be the biggest cigarette smokers in the world and just chain smoking.
[01:05:40.200 --> 01:05:45.000]   It was cigarette smoking was associated with Americans and it's something happened.
[01:05:45.000 --> 01:05:46.440]   When was it in the late 70s or something?
[01:05:46.440 --> 01:05:51.800]   I don't know when it happened where Americans suddenly became the most anti smoking country
[01:05:51.800 --> 01:05:54.160]   in the industrialized world.
[01:05:54.160 --> 01:05:55.920]   And same thing happened with littering.
[01:05:55.920 --> 01:05:57.520]   The same thing happened with a bunch of things.
[01:05:57.520 --> 01:06:02.880]   And I think this is something that happens with I think it's a it's a character is I
[01:06:02.880 --> 01:06:06.800]   think it's the most defining characteristic of of the United States is the ability to
[01:06:06.800 --> 01:06:11.800]   completely redefine yourself on something like this.
[01:06:11.800 --> 01:06:18.800]   And change so quickly so completely over something that normally in a regular country
[01:06:18.800 --> 01:06:23.360]   would be much more stable opinion over over the generations.
[01:06:23.360 --> 01:06:26.120]   And so anyway just just a thought that I have.
[01:06:26.120 --> 01:06:27.600]   Yeah, yeah, I think that's fair.
[01:06:27.600 --> 01:06:28.600]   I think that's fair.
[01:06:28.600 --> 01:06:31.480]   I think the internet had something to do with it because I mean I know that like internet
[01:06:31.480 --> 01:06:35.800]   access has has definitely been a factor in people's changing attitudes about religion
[01:06:35.800 --> 01:06:42.160]   which I actually think is tied to some of the attitudes around around marriage in a lot
[01:06:42.160 --> 01:06:47.600]   of ways just because when people have questions they can ask and get answers and do their
[01:06:47.600 --> 01:06:49.080]   own research kind of.
[01:06:49.080 --> 01:06:50.080]   But you're right.
[01:06:50.080 --> 01:06:53.640]   I mean that's what I mean I was probably love about about being here and living in
[01:06:53.640 --> 01:06:57.880]   this country and I have to say that this happened that decision last week happened a
[01:06:57.880 --> 01:06:58.880]   lot faster.
[01:06:58.880 --> 01:07:03.920]   I thought I thought we had you know back when I was a teenager and really scared I thought
[01:07:03.920 --> 01:07:08.040]   it was going to be a long time before you know my marriage saw this kind of recognition
[01:07:08.040 --> 01:07:12.240]   so I'm very I'm happy and I'm grateful and I'm glad we can change our minds this quickly.
[01:07:12.240 --> 01:07:17.120]   Yeah, when I was a little straight kid and we watched a video that was based on Alvin
[01:07:17.120 --> 01:07:21.520]   Toffler's Future Shock and they were showing all these scenarios from the book and I think
[01:07:21.520 --> 01:07:24.960]   I was probably in the fifth grade or something like that and they showed this movie about
[01:07:24.960 --> 01:07:28.280]   all this stuff that's going to happen in the year 2000 or whatever and one of them was
[01:07:28.280 --> 01:07:32.000]   they showed two men getting married I was like that is never going to happen.
[01:07:32.000 --> 01:07:33.520]   That is absolutely never going to happen.
[01:07:33.520 --> 01:07:37.600]   I remember distinctly thinking that and I thought that was really interesting and to
[01:07:37.600 --> 01:07:43.720]   your point Gina I thought Mexico was going to beat us to it a devoutly Catholic country
[01:07:43.720 --> 01:07:47.960]   I thought was going to get to nationwide marriage equality before the US.
[01:07:47.960 --> 01:07:51.840]   It was looking that way for right up until this is a great decision.
[01:07:51.840 --> 01:07:52.840]   Yep.
[01:07:52.840 --> 01:07:53.840]   Very interesting.
[01:07:53.840 --> 01:07:57.880]   All right well we got some more good goodness coming for you in just a sec but first let's
[01:07:57.880 --> 01:08:01.680]   take a break and talk about one of our sponsors which is LegalZoom.
[01:08:01.680 --> 01:08:06.280]   It's not a law firm it's a place where you can go to get very low cost legal related
[01:08:06.280 --> 01:08:11.280]   help and you can get all kinds of documents you can get all kinds of advice.
[01:08:11.280 --> 01:08:16.000]   There's so many resources there at LegalZoom and I want to encourage everybody out there
[01:08:16.000 --> 01:08:21.480]   who's thinking of being like Gina, Tripani and starting your own business.
[01:08:21.480 --> 01:08:23.800]   Lots of people dream of starting a business.
[01:08:23.800 --> 01:08:29.480]   Think someday I'd like to open some kind of business but it just sounds super scary and
[01:08:29.480 --> 01:08:33.160]   super complicated and what's involved and I don't know how to do this that or the other
[01:08:33.160 --> 01:08:34.560]   thing.
[01:08:34.560 --> 01:08:40.800]   Something you should try is go to LegalZoom and just browse and see what is involved in
[01:08:40.800 --> 01:08:44.000]   setting up the type of corporation you had.
[01:08:44.000 --> 01:08:46.360]   My wife and I set up a corporation a few years ago.
[01:08:46.360 --> 01:08:47.880]   We hadn't heard about LegalZoom yet.
[01:08:47.880 --> 01:08:51.120]   We spent a fortune.
[01:08:51.120 --> 01:08:55.800]   Nowadays completely unnecessarily because now we have LegalZoom.
[01:08:55.800 --> 01:08:58.240]   Just look at all the documents that you're going to require.
[01:08:58.240 --> 01:09:00.960]   Look at the regulations that you have to satisfy.
[01:09:00.960 --> 01:09:06.440]   Look at the issues around employment documentation and so on.
[01:09:06.440 --> 01:09:11.800]   What you'll discover is that there is actually a lot to it but that with LegalZoom's help
[01:09:11.800 --> 01:09:13.400]   you can actually do it.
[01:09:13.400 --> 01:09:19.040]   I think that LegalZoom if you spend some quality time there and let them help you think about
[01:09:19.040 --> 01:09:24.000]   the realities of starting a business, I think you'll be more likely to actually start that
[01:09:24.000 --> 01:09:29.600]   business because you'll see that it's affordable to get most of this stuff done and of course
[01:09:29.600 --> 01:09:36.520]   if you do need advice from an attorney they can point you in the right direction as well.
[01:09:36.520 --> 01:09:43.200]   LegalZoom is just a great resource that if you're going to start a business you've got
[01:09:43.200 --> 01:09:48.040]   to check out LegalZoom, see what everything that they have on offer and also let them
[01:09:48.040 --> 01:09:53.000]   help educate you about what running a business is really all about.
[01:09:53.000 --> 01:09:59.680]   They provide complete transparency with upfront pricing, customer reviews and a 100% satisfaction
[01:09:59.680 --> 01:10:01.240]   guarantee.
[01:10:01.240 --> 01:10:04.640]   Make the smart choice for your business at LegalZoom.com today and don't forget to
[01:10:04.640 --> 01:10:07.680]   enter Twig at checkout to save you even more.
[01:10:07.680 --> 01:10:14.280]   That's LegalZoom.com promo code Twig and remember that legal help is here as cheaper
[01:10:14.280 --> 01:10:15.640]   than you think.
[01:10:15.640 --> 01:10:19.440]   LegalZoom is not a law firm but can connect you with an independent attorney.
[01:10:19.440 --> 01:10:27.160]   All right well I have ordered but have not yet received an Amazon Echo and A) I'm really
[01:10:27.160 --> 01:10:32.760]   excited about getting it and B) believe that this is the future of every home that we're
[01:10:32.760 --> 01:10:36.960]   going to just be able to like on Star Trek just talk to the room and have it answer our
[01:10:36.960 --> 01:10:39.280]   questions and do things for us.
[01:10:39.280 --> 01:10:46.520]   Are either of you as excited about Echo or are either of you anti-Echoites?
[01:10:46.520 --> 01:10:49.520]   Really?
[01:10:49.520 --> 01:10:51.520]   Really?
[01:10:51.520 --> 01:10:57.880]   Now before we wake up Jeff, Gina we should point out that we shouldn't actually say
[01:10:57.880 --> 01:11:01.960]   the A word, the name of the virtual assistant.
[01:11:01.960 --> 01:11:07.200]   I got some nasty, not nasty, some humorous emails from people that when I've said this
[01:11:07.200 --> 01:11:12.360]   on Tech News today I woke up asking questions across the nation.
[01:11:12.360 --> 01:11:14.640]   What is he supposed to say?
[01:11:14.640 --> 01:11:16.640]   It's Alexa.
[01:11:16.640 --> 01:11:19.400]   Alexa, how old is Mike Elgin?
[01:11:19.400 --> 01:11:24.200]   All across America for the all the three people who are using Alexa they're now going
[01:11:24.200 --> 01:11:26.000]   to know.
[01:11:26.000 --> 01:11:29.480]   I don't remember myself so this is why I need Alexa.
[01:11:29.480 --> 01:11:36.120]   I have not tried the Echo but my co-founder has an Echo at home and he loves it which
[01:11:36.120 --> 01:11:38.560]   surprises me because he's kind of skeptical.
[01:11:38.560 --> 01:11:41.960]   My co-founder, Neil, you should have him on the show actually when you get it.
[01:11:41.960 --> 01:11:45.840]   He absolutely loves it, he lives in a small apartment in Manhattan so Alexa is sort of
[01:11:45.840 --> 01:11:51.240]   always around in the kind of the main room but he uses it to ask for the weather and play
[01:11:51.240 --> 01:11:52.240]   music.
[01:11:52.240 --> 01:11:57.080]   Personally, I talk to this thing on my wrist, I've got this thing on my wrist all the time
[01:11:57.080 --> 01:11:59.440]   and I've got two floors in my house and a backyard.
[01:11:59.440 --> 01:12:07.360]   I need the Echo, sorry, in a couple of different rooms and I don't, this is probably a little
[01:12:07.360 --> 01:12:13.640]   fashion of me but I don't love the idea of an always listening thing kind of in my midst
[01:12:13.640 --> 01:12:17.680]   although I guess I've already got that with my phone and I mean, you know, the connect
[01:12:17.680 --> 01:12:20.160]   although I have an older version of the connect.
[01:12:20.160 --> 01:12:25.400]   I just, I don't know, I have doubts about whether or not I would use it a lot particularly
[01:12:25.400 --> 01:12:30.120]   with my watch and my phone kind of always on me but there are folks out there who love
[01:12:30.120 --> 01:12:34.320]   the Echo, my co-founder included so I'll be interested to see what you think.
[01:12:34.320 --> 01:12:40.120]   Yeah, one little tidbit that, a little trivia point for Twit fans is that Alexa actually
[01:12:40.120 --> 01:12:45.560]   has ties to TuneIn radio so you can say, and I'm going to make this happen for those
[01:12:45.560 --> 01:12:49.760]   of you who have this, Alexa, play this week in Google on TuneIn.
[01:12:49.760 --> 01:12:50.760]   Well done.
[01:12:50.760 --> 01:12:51.760]   Okay.
[01:12:51.760 --> 01:12:53.080]   Well, there you go.
[01:12:53.080 --> 01:12:57.160]   The other thing is that it will play the live Twit feed, it's one of the few of the services
[01:12:57.160 --> 01:13:00.360]   that will actually play you, the TuneIn is one of the things that will play the live
[01:13:00.360 --> 01:13:04.800]   feed so you can actually tell Alexa to play Twit live, whatever's happening on the live
[01:13:04.800 --> 01:13:05.800]   stream.
[01:13:05.800 --> 01:13:09.080]   The other thing that's interesting about this to me is that I did a review on this and I
[01:13:09.080 --> 01:13:14.880]   loved it and then based on my review, my son Kevin ordered it, he had to wait many months,
[01:13:14.880 --> 01:13:18.640]   he finally got it and he loves it so much, he ordered a second one so that there's no
[01:13:18.640 --> 01:13:22.120]   corner of his house where he can't talk to this thing.
[01:13:22.120 --> 01:13:25.280]   What's the primary use case for him?
[01:13:25.280 --> 01:13:32.400]   He just gets arbitrary information, just trivia, who is that person in the movie or what the
[01:13:32.400 --> 01:13:37.080]   weather is, what's the traffic like, just miscellaneous information but the thing that
[01:13:37.080 --> 01:13:42.320]   he likes about it is he has a theory which is that the combination of the fact that it's
[01:13:42.320 --> 01:13:45.960]   just in the room, it's not something in your pocket or it's not something on your wrist
[01:13:45.960 --> 01:13:52.160]   and he does have an Apple watch but it's just there within earshot.
[01:13:52.160 --> 01:13:57.560]   It has a physical body and a human name and so you psychologically start to think of it
[01:13:57.560 --> 01:14:02.600]   as kind of a virtual person more than say Google now which doesn't have a human name
[01:14:02.600 --> 01:14:08.400]   and it doesn't have a physical body and is kind of less accessible really to get at no
[01:14:08.400 --> 01:14:09.920]   matter how you slice it.
[01:14:09.920 --> 01:14:12.920]   So the accessibility of Alexa is very interesting.
[01:14:12.920 --> 01:14:17.720]   The other use case that he used it for is he's got it tied into his home automation so
[01:14:17.720 --> 01:14:21.760]   he never touches a light switch anymore, he just tells Alexa to turn on the lights and
[01:14:21.760 --> 01:14:23.720]   to do all that kind of stuff.
[01:14:23.720 --> 01:14:25.720]   Oh, you're lazy son.
[01:14:25.720 --> 01:14:26.720]   Geez.
[01:14:26.720 --> 01:14:27.720]   Yeah, yeah.
[01:14:27.720 --> 01:14:32.440]   He's lazy in that it probably took him three hours to set it all up and all that kind of
[01:14:32.440 --> 01:14:33.440]   stuff.
[01:14:33.440 --> 01:14:40.560]   But this is about necessity, this is about psychology, this is about human psychology
[01:14:40.560 --> 01:14:44.240]   and just the fact that you know it's there.
[01:14:44.240 --> 01:14:47.840]   The other thing that he loves about it and he made a big point about this is that this
[01:14:47.840 --> 01:14:54.680]   is a very unusual bit of kit because it's a super high quality speaker with a microphone
[01:14:54.680 --> 01:14:56.360]   and a Wi-Fi connection and all that stuff.
[01:14:56.360 --> 01:14:58.200]   That's essentially all it is.
[01:14:58.200 --> 01:15:04.760]   Alexa is in the cloud, Alexa is on remote servers and so this is a rare example of a
[01:15:04.760 --> 01:15:08.840]   consumer electronics device that never needs to be replaced.
[01:15:08.840 --> 01:15:10.560]   It will always be a great speaker.
[01:15:10.560 --> 01:15:17.280]   It will always be a great microphone and they will update it in the cloud for decades presumably.
[01:15:17.280 --> 01:15:19.480]   So here's a question.
[01:15:19.480 --> 01:15:21.800]   I'm curious what your answer to this two genie.
[01:15:21.800 --> 01:15:26.600]   Alexa has a female name but it's this 2001 monolith looking thing.
[01:15:26.600 --> 01:15:28.600]   What if Alexa looked human?
[01:15:28.600 --> 01:15:29.600]   Yeah.
[01:15:29.600 --> 01:15:34.160]   What if, I mean cartoonish a little bit, you know, on the Howard Stern show they had the
[01:15:34.160 --> 01:15:38.600]   three years they had the Gary puppet where they'd make it sound like Gary Delabate,
[01:15:38.600 --> 01:15:39.520]   Baba Bui.
[01:15:39.520 --> 01:15:42.680]   You know, what if what if it were the Alexa puppet?
[01:15:42.680 --> 01:15:44.200]   What would your daughter's reaction be to it?
[01:15:44.200 --> 01:15:46.120]   Would it be more cute and cuddly?
[01:15:46.120 --> 01:15:51.040]   Would it be a little less ominous to have this obelisk listening to you?
[01:15:51.040 --> 01:15:55.040]   Well, there's a product in Japan called the Pepper Robot and they went on sale.
[01:15:55.040 --> 01:16:00.960]   They had a thousand for sale and this is from Softbank and Aldebaran, I think is how you
[01:16:00.960 --> 01:16:03.280]   say it and they sold out within one minute.
[01:16:03.280 --> 01:16:04.280]   Now that's Japan.
[01:16:04.280 --> 01:16:09.360]   That's a country where people have funerals for their I-bow dogs but this is the Pepper
[01:16:09.360 --> 01:16:10.360]   Robot here.
[01:16:10.360 --> 01:16:13.120]   Oh, interesting.
[01:16:13.120 --> 01:16:19.720]   It does hand gestures and it's sole purpose at this point is to be emotionally responsive
[01:16:19.720 --> 01:16:20.720]   and to introduce-
[01:16:20.720 --> 01:16:21.720]   It looks like a small child.
[01:16:21.720 --> 01:16:22.720]   Yeah.
[01:16:22.720 --> 01:16:27.160]   It reads human emotion and then it expresses human emotion.
[01:16:27.160 --> 01:16:29.160]   And it speaks Japanese.
[01:16:29.160 --> 01:16:54.440]   I do not want this.
[01:16:54.440 --> 01:17:00.960]   I have a small child who I need to care for that those like big eyes and the looking up
[01:17:00.960 --> 01:17:01.960]   at you.
[01:17:01.960 --> 01:17:03.960]   No, no, no, I want the monolith.
[01:17:03.960 --> 01:17:04.960]   I don't know.
[01:17:04.960 --> 01:17:06.440]   This freaks me out.
[01:17:06.440 --> 01:17:09.880]   He looks like Wally or the robot for Rocky 4.
[01:17:09.880 --> 01:17:14.000]   No, I don't need another human or like thing in my life.
[01:17:14.000 --> 01:17:18.920]   I mean, it's really cute and I would play with it for a little bit but then I'd want
[01:17:18.920 --> 01:17:20.520]   the monolith back.
[01:17:20.520 --> 01:17:21.520]   Yeah.
[01:17:21.520 --> 01:17:26.280]   Well, you know, this is an- I mentioned that people having funerals for their I-bow robot
[01:17:26.280 --> 01:17:29.640]   dogs and this is actually happening in Japan.
[01:17:29.640 --> 01:17:34.160]   This product that was discontinued a few years ago is reaching the end of life and people
[01:17:34.160 --> 01:17:35.960]   have grown attached to it.
[01:17:35.960 --> 01:17:41.960]   But the reality is that people in fact do ascribe human characteristics to inanimate
[01:17:41.960 --> 01:17:43.640]   objects like cars, for example.
[01:17:43.640 --> 01:17:45.840]   They give names to their cars.
[01:17:45.840 --> 01:17:49.480]   They think the car is either with them or against them based on whether it starts or
[01:17:49.480 --> 01:17:50.480]   whatever.
[01:17:50.480 --> 01:17:58.240]   And so I think it's a- you know, I wrote a column about this recently talking about the
[01:17:58.240 --> 01:18:04.120]   declining significance of the Turing test.
[01:18:04.120 --> 01:18:08.280]   Turing test essentially a test to determine if something can fool a human into thinking
[01:18:08.280 --> 01:18:12.200]   artificial intelligence can fool a human into thinking that it's a person.
[01:18:12.200 --> 01:18:19.560]   So you have a real person and an AI software answering questions and then if the person
[01:18:19.560 --> 01:18:22.880]   can't tell or just says, yeah, that's a person and it turns out to be AI.
[01:18:22.880 --> 01:18:28.160]   Okay, they pass the Turing test and that's this kind of singularity that we've been thinking
[01:18:28.160 --> 01:18:29.680]   about for decades.
[01:18:29.680 --> 01:18:35.320]   But I think that that is less relevant because essentially your emotional state, your heart,
[01:18:35.320 --> 01:18:41.640]   if you will, is going to accept these types of artificial intelligence agents or virtual
[01:18:41.640 --> 01:18:45.400]   assistants or pepper robots or whatever they are long before your brain will.
[01:18:45.400 --> 01:18:52.240]   So even if you know, even if it's not fooling you into thinking it's human, you will, because
[01:18:52.240 --> 01:18:57.960]   we're human beings, we are hardwired to interact in a human world where we interact with other
[01:18:57.960 --> 01:18:59.320]   people.
[01:18:59.320 --> 01:19:04.040]   We want our machines and our virtual assistants to be more human-like.
[01:19:04.040 --> 01:19:05.480]   That's what Google now is all about.
[01:19:05.480 --> 01:19:09.240]   That's what Siri is all about, Cortana and Alexa.
[01:19:09.240 --> 01:19:16.440]   And Siri, if you search Siri, if you ask Siri, what's zero divided by zero?
[01:19:16.440 --> 01:19:17.800]   In fact, let's do that.
[01:19:17.800 --> 01:19:22.920]   I happen to have an iPhone right here.
[01:19:22.920 --> 01:19:27.360]   What's zero divided by zero?
[01:19:27.360 --> 01:19:31.280]   Imagine that you have zero cookies and you split them evenly among zero friends.
[01:19:31.280 --> 01:19:33.560]   How many cookies does each person get?
[01:19:33.560 --> 01:19:35.400]   See, it doesn't make sense.
[01:19:35.400 --> 01:19:38.280]   And cookie monster is sad that there are no cookies.
[01:19:38.280 --> 01:19:43.240]   And you are sad that you have no friends.
[01:19:43.240 --> 01:19:45.040]   So this is where they're going.
[01:19:45.040 --> 01:19:51.040]   I mean, that's these virtual assistants becoming one of your own.
[01:19:51.040 --> 01:19:53.480]   Yeah, lose or you have no friends.
[01:19:53.480 --> 01:19:55.520]   But it's a human-like response.
[01:19:55.520 --> 01:20:01.240]   And if you go forward in five years in time and have the advanced future version of Siri,
[01:20:01.240 --> 01:20:07.520]   Cortana and Google now, it's going to be an increased customization, an increased agency,
[01:20:07.520 --> 01:20:15.560]   an increased Google now like preemptive action, which is now being introduced to iOS 9 in
[01:20:15.560 --> 01:20:17.360]   the future.
[01:20:17.360 --> 01:20:21.160]   It's going to seem like people to us.
[01:20:21.160 --> 01:20:25.160]   We know they're not, but we're going to interact with them like there are people who are looking
[01:20:25.160 --> 01:20:26.160]   out for us.
[01:20:26.160 --> 01:20:27.720]   We'll have affection for them.
[01:20:27.720 --> 01:20:28.720]   It's coming.
[01:20:28.720 --> 01:20:29.720]   That's all I'm saying.
[01:20:29.720 --> 01:20:30.720]   It's her.
[01:20:30.720 --> 01:20:31.720]   Her is going to happen.
[01:20:31.720 --> 01:20:32.720]   Yes.
[01:20:32.720 --> 01:20:33.720]   Her is going to happen.
[01:20:33.720 --> 01:20:34.720]   Yes.
[01:20:34.720 --> 01:20:35.720]   Yes.
[01:20:35.720 --> 01:20:39.920]   And then the distance from the machines and the object, like the objectiveness I have about
[01:20:39.920 --> 01:20:43.200]   the machines, like very comforting and like good for my mental health.
[01:20:43.200 --> 01:20:48.120]   I mean, I say things to Siri that I wouldn't ever say to a human being because I know it's
[01:20:48.120 --> 01:20:49.280]   a machine, right?
[01:20:49.280 --> 01:20:54.200]   Or Google now or I look at Google now and most of the time I spend thinking, hmm, why
[01:20:54.200 --> 01:20:55.200]   did it show me this?
[01:20:55.200 --> 01:20:56.200]   Oh, right.
[01:20:56.200 --> 01:20:57.200]   I searched that thing that one time.
[01:20:57.200 --> 01:21:00.080]   Like these are all the ways that Google could make this better, right?
[01:21:00.080 --> 01:21:03.360]   Like I'm very hyper aware that it's a machine.
[01:21:03.360 --> 01:21:06.920]   I hear what you're saying and that we are getting to that place, right?
[01:21:06.920 --> 01:21:11.120]   In an ideal world for Google and Apple and Amazon, there isn't that divide.
[01:21:11.120 --> 01:21:12.280]   There isn't that distance.
[01:21:12.280 --> 01:21:17.080]   People are interacting with machines as if they're friends and humans and aren't, don't
[01:21:17.080 --> 01:21:21.360]   have that distance of this is a machine trying to serve me in a certain way.
[01:21:21.360 --> 01:21:27.240]   But I think that really serves society in a good way too, that distance and that knowing.
[01:21:27.240 --> 01:21:29.120]   Do you know what?
[01:21:29.120 --> 01:21:33.320]   Resuming is not a computer bad voice, presuming that these things start to sound more human.
[01:21:33.320 --> 01:21:41.320]   What if Edda or Edda's child, for that far in advance, likes books read by Alexa more
[01:21:41.320 --> 01:21:42.920]   than by mom?
[01:21:42.920 --> 01:21:47.080]   I mean, it would mean a lot less reading for me.
[01:21:47.080 --> 01:21:54.400]   I mean, there's definitely going to be a time in her life and it's probably very close
[01:21:54.400 --> 01:21:59.240]   that she's going to prefer the company of her video games or, or, or, you know, friends,
[01:21:59.240 --> 01:22:02.520]   whether they're bots or not, you know, to chat with or text with, right?
[01:22:02.520 --> 01:22:05.840]   I mean, that that happens now that then spend time with me.
[01:22:05.840 --> 01:22:09.320]   And I don't necessarily think that's always a bad thing.
[01:22:09.320 --> 01:22:12.920]   But yeah, I've actually kind of, I've been thinking about getting her into audiobooks.
[01:22:12.920 --> 01:22:14.840]   It's really hard to read this.
[01:22:14.840 --> 01:22:18.200]   You know, Jeff, I don't know if you're referring to actual news, but in fact, Alexa does read
[01:22:18.200 --> 01:22:19.200]   books to you.
[01:22:19.200 --> 01:22:21.000]   And if you want to start a new book, it's one command.
[01:22:21.000 --> 01:22:22.800]   If you want to pick up where you left off.
[01:22:22.800 --> 01:22:27.400]   So if you're reading with it, Amazon Kindle and you're at certain point and then you put
[01:22:27.400 --> 01:22:31.600]   it aside in three days later, you come back, you can tell Alexa to, to read you that book
[01:22:31.600 --> 01:22:35.000]   and it'll pick up where you stop reading.
[01:22:35.000 --> 01:22:36.600]   It's pretty cool, actually.
[01:22:36.600 --> 01:22:42.560]   But the reason it's on the rundown today, we sort of like went down a rabbit hole there,
[01:22:42.560 --> 01:22:48.640]   is because one of the ways that Alexa and, and Echo is ahead of Cortana and Siri and
[01:22:48.640 --> 01:22:54.080]   all the rest is that they just open it up to third party developers.
[01:22:54.080 --> 01:22:58.840]   And you know, both Google and Apple are kind of grudgingly working with partners to kind
[01:22:58.840 --> 01:23:03.080]   of bring certain third party things into their virtual assistants.
[01:23:03.080 --> 01:23:06.400]   But Amazon is like, it's open, come and get it.
[01:23:06.400 --> 01:23:07.960]   And that is really interesting.
[01:23:07.960 --> 01:23:13.880]   And I think we'll result in some interesting applications.
[01:23:13.880 --> 01:23:16.840]   As with all things Amazon, we don't know what the numbers are on this.
[01:23:16.840 --> 01:23:19.880]   We don't know how many people are actually using Echo.
[01:23:19.880 --> 01:23:20.880]   No idea.
[01:23:20.880 --> 01:23:21.880]   Yeah.
[01:23:21.880 --> 01:23:27.000]   But Alexa could run on the device, say an Amazon branded Android device using like their
[01:23:27.000 --> 01:23:28.640]   assistant API or something.
[01:23:28.640 --> 01:23:31.600]   We can have Alexa running on my watch or phone.
[01:23:31.600 --> 01:23:32.600]   Yes.
[01:23:32.600 --> 01:23:33.600]   Yep.
[01:23:33.600 --> 01:23:35.680]   Yeah, it could be everywhere, which is pretty cool.
[01:23:35.680 --> 01:23:36.760]   Yeah, that is cool.
[01:23:36.760 --> 01:23:40.120]   So more breaking news.
[01:23:40.120 --> 01:23:44.440]   Under Facebook, Facebook just announced a way for publishers to make money on video
[01:23:44.440 --> 01:23:47.200]   heading to our earlier discussion.
[01:23:47.200 --> 01:23:53.200]   So it's an ad age story that that Facebook will put ads in the middle of streams and
[01:23:53.200 --> 01:23:58.920]   the publishers can get, I think it's 45% of the revenue based on the ads that appear
[01:23:58.920 --> 01:24:01.320]   next to their content.
[01:24:01.320 --> 01:24:02.320]   Very interesting.
[01:24:02.320 --> 01:24:07.840]   You know, just as an example of how devoted Facebook is to video, actually to promote
[01:24:07.840 --> 01:24:17.040]   tech news today, which is five times a week, I actually upload the entire HD file to Facebook,
[01:24:17.040 --> 01:24:18.040]   not just a link.
[01:24:18.040 --> 01:24:21.280]   But let me see how big this is here.
[01:24:21.280 --> 01:24:23.040]   I mean, this is a huge file.
[01:24:23.040 --> 01:24:27.720]   411 megabytes is the most recent episode.
[01:24:27.720 --> 01:24:32.520]   And it will accept that entire upload and then play it in a version of HD.
[01:24:32.520 --> 01:24:35.040]   It's a slightly reduced version.
[01:24:35.040 --> 01:24:40.880]   But man, to allow that, I think the cap is based on time.
[01:24:40.880 --> 01:24:47.680]   They'll let you upload a video of almost any resolution as long as it's not more than
[01:24:47.680 --> 01:24:49.200]   45 minutes, I believe.
[01:24:49.200 --> 01:24:51.600]   Well, that kills Twigby up there.
[01:24:51.600 --> 01:24:54.120]   Yeah, that's right.
[01:24:54.120 --> 01:24:55.640]   Uploaded into sections.
[01:24:55.640 --> 01:24:58.800]   And speaking of sections, why don't we take a break?
[01:24:58.800 --> 01:25:04.200]   And when we come back, we can do the number thing and toy of the week.
[01:25:04.200 --> 01:25:08.240]   But first, let's talk about speaking of developers, brain tree.
[01:25:08.240 --> 01:25:13.240]   Brain tree is a great way for you to add payments to your mobile app or to your website.
[01:25:13.240 --> 01:25:18.200]   And of course, if you are unless you're new to the planet, you have used brain tree before
[01:25:18.200 --> 01:25:24.240]   in apps like Uber, Airbnb, hotel tonight, living social and munchery.
[01:25:24.240 --> 01:25:30.080]   And one of the reasons these sites are successful is that they're so easy to use, especially
[01:25:30.080 --> 01:25:31.960]   from a payments perspective.
[01:25:31.960 --> 01:25:37.280]   I mean, with Uber, that's really the whole model is that you just plug in your basic financial
[01:25:37.280 --> 01:25:39.360]   information and then you stop worrying about payments.
[01:25:39.360 --> 01:25:44.200]   You just jump in a car, you get out, and then the payment, it informs you about the payment
[01:25:44.200 --> 01:25:46.040]   it is, and it's seamless.
[01:25:46.040 --> 01:25:52.320]   And one of the great things about brain tree is that it will accept just about everything.
[01:25:52.320 --> 01:25:58.880]   It will accept PayPal, Apple Pay, Venmo, credit cards, lots of different payment types, including
[01:25:58.880 --> 01:25:59.880]   Bitcoin.
[01:25:59.880 --> 01:26:00.880]   That's right.
[01:26:00.880 --> 01:26:05.800]   You can actually accept Bitcoin for your app, for your service, for your company.
[01:26:05.800 --> 01:26:09.120]   And it's just fantastic and super easy for the developers too.
[01:26:09.120 --> 01:26:13.040]   Sometimes you can plug this in depending on your experience in more or less 30 minutes
[01:26:13.040 --> 01:26:14.040]   or so.
[01:26:14.040 --> 01:26:15.040]   And you're up and running.
[01:26:15.040 --> 01:26:16.960]   It's the end of the old-fashioned way.
[01:26:16.960 --> 01:26:21.000]   We have to go and establish a relationship with each and every payment processor.
[01:26:21.000 --> 01:26:24.880]   You just do it all through brain tree and you're off and running.
[01:26:24.880 --> 01:26:29.000]   Brain tree gives you a full stack payment solution support for all payment types that
[01:26:29.000 --> 01:26:30.240]   your customers might want.
[01:26:30.240 --> 01:26:36.160]   130 currencies and you can start accepting PayPal, Apple Pay, Bitcoin, Venmo, credit cards,
[01:26:36.160 --> 01:26:40.280]   and so much more, all with one integration across all platforms with superior fraud
[01:26:40.280 --> 01:26:43.840]   protection, customer service, and really fast payouts.
[01:26:43.840 --> 01:26:50.840]   To learn more and for your first $50,000 in transactions fee-free, go to braintreepayments.com/twig.
[01:26:50.840 --> 01:26:57.960]   Well, Jeff, why don't we start with your number of the week?
[01:26:57.960 --> 01:26:59.360]   Let's go with this one.
[01:26:59.360 --> 01:27:04.440]   Google now says it translates 100 billion words a day.
[01:27:04.440 --> 01:27:05.440]   Wow.
[01:27:05.440 --> 01:27:08.240]   I just, it's mind-boggling what Translate does.
[01:27:08.240 --> 01:27:10.640]   I don't know whether I told the story or the show.
[01:27:10.640 --> 01:27:11.840]   I was on a plane.
[01:27:11.840 --> 01:27:12.840]   We had problems with equipment.
[01:27:12.840 --> 01:27:17.920]   I was sitting there and I was sitting next to a nice man who's Chinese who clearly speaks
[01:27:17.920 --> 01:27:19.560]   no English, none.
[01:27:19.560 --> 01:27:25.480]   I guess I don't know whether he's nice or not, but he seemed like he was nice.
[01:27:25.480 --> 01:27:28.240]   They came over first to ask about what he wanted.
[01:27:28.240 --> 01:27:30.760]   Do you want this or that for breakfast?
[01:27:30.760 --> 01:27:34.880]   I talked to the Translate app and it gives it back.
[01:27:34.880 --> 01:27:37.480]   The plane was applauding because it was amazing.
[01:27:37.480 --> 01:27:38.480]   Then we had the problems.
[01:27:38.480 --> 01:27:41.200]   I told him I explained to him, "Well, we're delayed an hour or this broken here."
[01:27:41.200 --> 01:27:46.320]   I used the Audible Google Translate to translate it into a demand.
[01:27:46.320 --> 01:27:47.320]   It's pretty amazing.
[01:27:47.320 --> 01:27:52.760]   Google now does 100 billion words a day and they've also improved that conversational
[01:27:52.760 --> 01:27:53.760]   effort.
[01:27:53.760 --> 01:27:56.360]   They're trying to be more and more conversational in what they do.
[01:27:56.360 --> 01:28:02.360]   Yes, it's imperfect as hell, but I still find Google Translate to be pretty much a miracle.
[01:28:02.360 --> 01:28:03.360]   Yeah.
[01:28:03.360 --> 01:28:04.360]   That is amazing.
[01:28:04.360 --> 01:28:09.840]   Google also owns, I believe, they own word lens, don't they?
[01:28:09.840 --> 01:28:12.680]   Which Translate signs?
[01:28:12.680 --> 01:28:16.560]   Yeah, that's built into the Translate app at this point.
[01:28:16.560 --> 01:28:19.800]   They have a Google Glass app, which is amazing.
[01:28:19.800 --> 01:28:26.040]   You just look at a sign and it will show you the English for that sign if it's in one of
[01:28:26.040 --> 01:28:28.400]   the accepted languages.
[01:28:28.400 --> 01:28:30.600]   I also use it in speaking of unpopular Google products.
[01:28:30.600 --> 01:28:34.840]   I also use it on Google Plus, the Translate app all the time for comments.
[01:28:34.840 --> 01:28:40.120]   Google Plus has gotten increasingly non-English speaking.
[01:28:40.120 --> 01:28:46.280]   I probably guess maybe 20%, 25% of the comments on my post these days are from people who
[01:28:46.280 --> 01:28:49.040]   are commenting in another language.
[01:28:49.040 --> 01:28:54.280]   It's somewhat trivial for them to see what I've written in my post, what other people
[01:28:54.280 --> 01:28:59.280]   have commented on, and then for me to click on the Translate button and see what they've
[01:28:59.280 --> 01:29:01.680]   said in their language.
[01:29:01.680 --> 01:29:02.680]   Sometimes it's spam.
[01:29:02.680 --> 01:29:04.200]   Sometimes it's a really good comment.
[01:29:04.200 --> 01:29:09.960]   I use Google Translate then oftentimes to reply to them natively in their language.
[01:29:09.960 --> 01:29:11.560]   It's just the greatest thing ever.
[01:29:11.560 --> 01:29:16.560]   If you were to go back in time 30 years and say that this would be this banality in the
[01:29:16.560 --> 01:29:19.240]   year 2015, nobody would have believed you.
[01:29:19.240 --> 01:29:25.360]   I was at a meeting of the parent company of El Pais in Madrid and I had to sit through
[01:29:25.360 --> 01:29:27.360]   the conversation and I'm horrible American.
[01:29:27.360 --> 01:29:28.360]   I speak, don't Spanish.
[01:29:28.360 --> 01:29:32.800]   I couldn't understand if it was being said, but they all use PowerPoint.
[01:29:32.800 --> 01:29:37.240]   And every screen I go walk over to the TV screen, point my Google Translate at it and it would
[01:29:37.240 --> 01:29:40.760]   translate live what was being said, phenomenal.
[01:29:40.760 --> 01:29:41.760]   Wow.
[01:29:41.760 --> 01:29:42.760]   That's amazing.
[01:29:42.760 --> 01:29:43.760]   Yeah.
[01:29:43.760 --> 01:29:46.560]   All right, Gina, you have a thing of the week.
[01:29:46.560 --> 01:29:50.600]   Yeah, my tip this week is a couple of updates to Gmail.
[01:29:50.600 --> 01:29:53.040]   I've been kind of obsessed with emoji lately.
[01:29:53.040 --> 01:29:55.240]   I just feel like I'm seeing emoji and I'm old.
[01:29:55.240 --> 01:29:56.240]   No, Gina.
[01:29:56.240 --> 01:29:57.240]   No, no, not you.
[01:29:57.240 --> 01:29:58.240]   No.
[01:29:58.240 --> 01:30:00.440]   They're everywhere, Jeff.
[01:30:00.440 --> 01:30:01.640]   You can't get away from them.
[01:30:01.640 --> 01:30:04.680]   And now you're going to see, I'm going to Gmail you some right after the show.
[01:30:04.680 --> 01:30:07.640]   You can email them to fully supports emoji.
[01:30:07.640 --> 01:30:10.920]   I'm going to send you like five or six emoji that's going to say a sentence that you'll
[01:30:10.920 --> 01:30:14.440]   never be able to figure out what it means.
[01:30:14.440 --> 01:30:16.000]   Gmail now fully supports emoji.
[01:30:16.000 --> 01:30:17.000]   This is just rolling out.
[01:30:17.000 --> 01:30:20.640]   I actually tried it in inbox and I was unsuccessful getting emoji in there.
[01:30:20.640 --> 01:30:24.520]   But in addition to emoji, there's a bunch of new themes, high res photos that you can
[01:30:24.520 --> 01:30:27.160]   choose from to customize how your Gmail inbox looks.
[01:30:27.160 --> 01:30:29.360]   You can also upload your own photo.
[01:30:29.360 --> 01:30:34.000]   So new themes and background images for Gmail and full emoji support.
[01:30:34.000 --> 01:30:35.000]   There you go.
[01:30:35.000 --> 01:30:36.000]   That's the email you're going to get, Jeff.
[01:30:36.000 --> 01:30:38.240]   I'm going to ask you how to launch all an emoji.
[01:30:38.240 --> 01:30:39.240]   Get ready.
[01:30:39.240 --> 01:30:42.000]   And if you can't figure out the emoji, you're going hungry, Jeff.
[01:30:42.000 --> 01:30:46.440]   I was going to say is that it might be grounds for you to decline my invitation.
[01:30:46.440 --> 01:30:48.920]   That's awesome.
[01:30:48.920 --> 01:30:50.440]   Well, emoji.
[01:30:50.440 --> 01:30:51.440]   Unbelievable.
[01:30:51.440 --> 01:30:55.360]   We're becoming post illiterate.
[01:30:55.360 --> 01:30:58.920]   My toy of the week is Yahoo's AVA 8 launcher for Android.
[01:30:58.920 --> 01:31:06.560]   Now I've used AVA 8 before Yahoo bought it and they've since improved it pretty slowly
[01:31:06.560 --> 01:31:07.560]   actually.
[01:31:07.560 --> 01:31:10.880]   Now what AVA 8 is is a contextual launcher.
[01:31:10.880 --> 01:31:11.880]   Essentially you put it on there.
[01:31:11.880 --> 01:31:15.240]   It becomes your main interface for your Android phone.
[01:31:15.240 --> 01:31:18.680]   And it pays attention to where you are, what time of day it is.
[01:31:18.680 --> 01:31:24.360]   And it tries to give you information and show you things that's relevant to your context.
[01:31:24.360 --> 01:31:25.560]   It's highly contextual.
[01:31:25.560 --> 01:31:26.560]   It's very nice.
[01:31:26.560 --> 01:31:27.560]   It doesn't always get it right.
[01:31:27.560 --> 01:31:30.360]   It's a very pleasant interface and I really enjoyed it.
[01:31:30.360 --> 01:31:35.640]   But now they've launched a new feature called SmartStream, which is kind of like Google
[01:31:35.640 --> 01:31:36.640]   Now.
[01:31:36.640 --> 01:31:39.080]   It's more information related.
[01:31:39.080 --> 01:31:44.200]   They claim that it's for the US only and Android 4.1 are newer, but people outside the
[01:31:44.200 --> 01:31:50.040]   USA that it does work outside the US, at least in India and a couple of other countries.
[01:31:50.040 --> 01:31:53.800]   And it gives you this focus menu which is located in the search bar in your home screen
[01:31:53.800 --> 01:31:56.040]   and it lets you search SmartStream as well.
[01:31:56.040 --> 01:32:01.160]   So not only is this something that pops up preemptively and sort of guides you through
[01:32:01.160 --> 01:32:05.000]   your day, but you can also then, if you remember something from earlier, you can go in there
[01:32:05.000 --> 01:32:07.560]   and search for it and find it again.
[01:32:07.560 --> 01:32:11.840]   So if you have an Android phone and you want to check this out and even if you've tried
[01:32:11.840 --> 01:32:17.280]   it in the past, you might want to try it again because SmartStream is a new feature that
[01:32:17.280 --> 01:32:19.040]   it's kind of another take on Google Now.
[01:32:19.040 --> 01:32:23.520]   So if you like Google Now, this may be another way to enjoy that type of thing.
[01:32:23.520 --> 01:32:25.520]   So check it out.
[01:32:25.520 --> 01:32:27.640]   All right.
[01:32:27.640 --> 01:32:32.880]   And you know, I'm curious, you know, what phone are you using these days?
[01:32:32.880 --> 01:32:35.440]   I've got the OnePlus one still going.
[01:32:35.440 --> 01:32:36.440]   Okay.
[01:32:36.440 --> 01:32:37.440]   Yeah.
[01:32:37.440 --> 01:32:38.440]   Yeah.
[01:32:38.440 --> 01:32:39.440]   Yeah.
[01:32:39.440 --> 01:32:43.880]   Although, well, my kid dropped it and then the hardware button stopped working.
[01:32:43.880 --> 01:32:47.960]   So I turned on the on screen buttons and I had and I basically had my mouse pointer hovering
[01:32:47.960 --> 01:32:51.800]   over the buy button on a Nexus 6, but I didn't.
[01:32:51.800 --> 01:32:56.680]   So hang it in there, hang it in there.
[01:32:56.680 --> 01:33:00.120]   Not tempted by the Samsung Galaxy Edge.
[01:33:00.120 --> 01:33:04.680]   Slightly, but I'm ready to go back to Nexus.
[01:33:04.680 --> 01:33:07.840]   I'm ready to go back to straight straight Nexus.
[01:33:07.840 --> 01:33:08.840]   All right.
[01:33:08.840 --> 01:33:09.840]   It just it just engineered.
[01:33:09.840 --> 01:33:14.920]   I mean, I got one plus one is a miracle of function and pricing and it's cool that
[01:33:14.920 --> 01:33:18.880]   it's on the operating system and all that's true.
[01:33:18.880 --> 01:33:21.400]   But the Nexus 6 is very well engineered.
[01:33:21.400 --> 01:33:22.400]   It just works better.
[01:33:22.400 --> 01:33:23.400]   Yeah.
[01:33:23.400 --> 01:33:24.400]   The end.
[01:33:24.400 --> 01:33:25.400]   I have problems like that.
[01:33:25.400 --> 01:33:26.400]   Yeah.
[01:33:26.400 --> 01:33:27.400]   Now I want the Nexus.
[01:33:27.400 --> 01:33:31.400]   I think that's I think that's where my next that's my next phone is going to be.
[01:33:31.400 --> 01:33:32.400]   Although I don't know.
[01:33:32.400 --> 01:33:35.040]   Maybe I'll hang in with this one plus one until it's until it's how long is going to
[01:33:35.040 --> 01:33:37.280]   be for a new Nexus do we think?
[01:33:37.280 --> 01:33:38.680]   I don't know.
[01:33:38.680 --> 01:33:42.720]   I'm not going to be late or later in the year for sure.
[01:33:42.720 --> 01:33:43.720]   This year.
[01:33:43.720 --> 01:33:44.720]   I mean, yeah.
[01:33:44.720 --> 01:33:45.720]   Oh, yeah.
[01:33:45.720 --> 01:33:46.720]   They're all definitely.
[01:33:46.720 --> 01:33:47.720]   No.
[01:33:47.720 --> 01:33:48.720]   Okay.
[01:33:48.720 --> 01:33:51.160]   But usually comes out towards the end of the year.
[01:33:51.160 --> 01:33:56.680]   I'd say probably like October-ish if previous years are to be believed.
[01:33:56.680 --> 01:33:58.880]   But I suppose we won't know until it happens.
[01:33:58.880 --> 01:34:03.440]   I would guess probably around the time that M becomes an official thing.
[01:34:03.440 --> 01:34:04.440]   Yep.
[01:34:04.440 --> 01:34:07.720]   You know, that it gets a name and gets released and it's going to release on the new Nexus.
[01:34:07.720 --> 01:34:09.240]   That's usually how they roll.
[01:34:09.240 --> 01:34:11.280]   That's going to be somewhere towards like Q4.
[01:34:11.280 --> 01:34:12.280]   I'd say.
[01:34:12.280 --> 01:34:13.280]   All right.
[01:34:13.280 --> 01:34:14.280]   Yeah.
[01:34:14.280 --> 01:34:15.280]   Sounds right to me.
[01:34:15.280 --> 01:34:17.480]   Anything else you guys want to talk about today?
[01:34:17.480 --> 01:34:19.480]   Something left on the agenda?
[01:34:19.480 --> 01:34:20.480]   Guacamole.
[01:34:20.480 --> 01:34:21.480]   Guacamole.
[01:34:21.480 --> 01:34:22.480]   Peace.
[01:34:22.480 --> 01:34:24.880]   I'm with the president on this one.
[01:34:24.880 --> 01:34:27.400]   I think Peas do not belong in Guacamole.
[01:34:27.400 --> 01:34:29.800]   New York Times shame on you.
[01:34:29.800 --> 01:34:34.720]   And by the way, the technology angle is that if you like completely unnatural acts like
[01:34:34.720 --> 01:34:42.200]   adding Peas to Guacamole, then you may like the Watson, you remember Watson's IBM's artificial
[01:34:42.200 --> 01:34:47.360]   intelligence thing that dominated on jeopardy and so on.
[01:34:47.360 --> 01:34:52.200]   A website now that's been open to the public I think about three weeks ago where the recipe
[01:34:52.200 --> 01:34:57.080]   generating attributes of Watson will be open to the public so you can get recipes that
[01:34:57.080 --> 01:34:58.240]   have been created.
[01:34:58.240 --> 01:35:01.000]   And these are wacko things like Peas and Guacamole.
[01:35:01.000 --> 01:35:03.600]   That sounds like a Watson move if you ask me.
[01:35:03.600 --> 01:35:06.920]   But they'll, you know, they have like chocolate burritos and, you know, all kinds of crazy
[01:35:06.920 --> 01:35:07.920]   stuff.
[01:35:07.920 --> 01:35:11.200]   Do you wear these arguments against marriage equality?
[01:35:11.200 --> 01:35:15.200]   Abominable acts, you know.
[01:35:15.200 --> 01:35:16.200]   Yeah, right.
[01:35:16.200 --> 01:35:17.200]   Yeah.
[01:35:17.200 --> 01:35:21.480]   But hey, hey, I'm all for the right of people to eat these things.
[01:35:21.480 --> 01:35:25.720]   Just, you know, yeah, it's just don't serve it to me.
[01:35:25.720 --> 01:35:28.280]   I don't want your peas and my guacamole.
[01:35:28.280 --> 01:35:29.280]   All right.
[01:35:29.280 --> 01:35:32.560]   Well, Gina Trapani is the creator of thinkup.com.
[01:35:32.560 --> 01:35:38.800]   She's a an occasional host and guest on Twitch shows, including all about Android.
[01:35:38.800 --> 01:35:40.520]   Jason, have you heard about all about Android?
[01:35:40.520 --> 01:35:41.920]   Yeah, it's a pretty great show.
[01:35:41.920 --> 01:35:42.920]   It's a great show.
[01:35:42.920 --> 01:35:45.360]   It's made even better when Gina Trapani is on us.
[01:35:45.360 --> 01:35:46.360]   Yes.
[01:35:46.360 --> 01:35:50.440]   And founding editor of Life Hacker, thank you so much for hosting me, really.
[01:35:50.440 --> 01:35:52.680]   You're the regular on this show.
[01:35:52.680 --> 01:35:54.520]   Anyway, Gina, a big fan.
[01:35:54.520 --> 01:35:59.240]   I've been really getting into thinkup.
[01:35:59.240 --> 01:36:03.400]   And I'm, I want to, actually that reminds me, I wanted to ask you something.
[01:36:03.400 --> 01:36:04.400]   Yeah.
[01:36:04.400 --> 01:36:10.200]   How hard was it for you to have an Instagram support for thinkup?
[01:36:10.200 --> 01:36:11.200]   Is that difficult?
[01:36:11.200 --> 01:36:12.760]   Just dealing with Instagram?
[01:36:12.760 --> 01:36:14.800]   I mean, how did that, what was that like?
[01:36:14.800 --> 01:36:15.960]   Yeah, that's it for season.
[01:36:15.960 --> 01:36:19.400]   Well, yeah, we just launched Instagram support in thinkup.
[01:36:19.400 --> 01:36:22.920]   And the Instagram API is very different than Twitter or Facebook.
[01:36:22.920 --> 01:36:24.360]   It was very, very interesting.
[01:36:24.360 --> 01:36:28.760]   You know, Instagram has been classically very close and very like fiercely protective
[01:36:28.760 --> 01:36:29.760]   of the user experience.
[01:36:29.760 --> 01:36:32.560]   You remember it was a mobile first app.
[01:36:32.560 --> 01:36:34.920]   You know, Instagram doesn't support links and comments.
[01:36:34.920 --> 01:36:37.520]   Like the only place you can have a link is in your profile.
[01:36:37.520 --> 01:36:42.720]   So they're really focused on keeping users in stream and in the mobile app.
[01:36:42.720 --> 01:36:46.600]   You can't upload photos via the web because they wanted to, you know, they wanted to make
[01:36:46.600 --> 01:36:51.040]   sure that all photos were high quality and their API is a bit limited.
[01:36:51.040 --> 01:36:56.760]   It does reflect some of that, some of that close approach.
[01:36:56.760 --> 01:36:58.560]   But it was really fun building for Instagram.
[01:36:58.560 --> 01:37:00.280]   I've been spending a lot of time on Instagram.
[01:37:00.280 --> 01:37:02.400]   You know, Instagram, there's no, there's not a lot.
[01:37:02.400 --> 01:37:03.400]   It's photos, right?
[01:37:03.400 --> 01:37:04.400]   It's visuals.
[01:37:04.400 --> 01:37:05.400]   It's photos and videos.
[01:37:05.400 --> 01:37:08.800]   A lot less drama, a lot less back and forth, a lot less BS.
[01:37:08.800 --> 01:37:11.280]   It's just kind of like pictures of people's nice days.
[01:37:11.280 --> 01:37:13.000]   So that's what I really like about Instagram.
[01:37:13.000 --> 01:37:16.360]   I feel like it's this happy social network, you know, it's just that place that you want
[01:37:16.360 --> 01:37:18.640]   to go to just kind of look at eye candy.
[01:37:18.640 --> 01:37:21.160]   So it was really fun to build.
[01:37:21.160 --> 01:37:25.320]   We didn't have as many opportunities to build really kind of in-depth insights as we did
[01:37:25.320 --> 01:37:27.240]   with, with, with Twitter, for example.
[01:37:27.240 --> 01:37:30.720]   I mean, I think Twitter really gets a bad rap for treating developers badly because they
[01:37:30.720 --> 01:37:34.200]   started out so open and then closed up things a little bit.
[01:37:34.200 --> 01:37:38.640]   But at the end of the day, like now that I've worked in depth with the Facebook API and
[01:37:38.640 --> 01:37:43.200]   the Twitter API and Instagram API, Twitter is actually pretty great.
[01:37:43.200 --> 01:37:44.200]   Yeah.
[01:37:44.200 --> 01:37:48.840]   So I mean, they're pretty good to developers when you really look at it.
[01:37:48.840 --> 01:37:49.840]   I mean, dumb things.
[01:37:49.840 --> 01:37:54.080]   Like, you know, there's no way to tell if someone is a private or a public user via
[01:37:54.080 --> 01:37:55.080]   the API.
[01:37:55.080 --> 01:37:57.160]   You have to make a kind of an extra call to check that out.
[01:37:57.160 --> 01:38:00.480]   And you don't know if someone's verified and it's hard to get comments.
[01:38:00.480 --> 01:38:04.880]   So it was a little bit harder, but a little bit more limiting, but it was really fun.
[01:38:04.880 --> 01:38:08.240]   And what I like now is that think up will show you kind of your most popular picture
[01:38:08.240 --> 01:38:11.000]   photo or your most popular video of the month and of the week.
[01:38:11.000 --> 01:38:15.200]   And it's really fun to see, to look at other people's streams and see what their best work
[01:38:15.200 --> 01:38:16.200]   is.
[01:38:16.200 --> 01:38:18.680]   And you get to find out your biggest admirers and people who like your stuff.
[01:38:18.680 --> 01:38:20.920]   And that's what happens primarily on Instagram.
[01:38:20.920 --> 01:38:21.920]   People just like stuff.
[01:38:21.920 --> 01:38:24.680]   It's a lot of hearts flying around versus replies or retweet.
[01:38:24.680 --> 01:38:26.640]   So that's a lot of fun.
[01:38:26.640 --> 01:38:28.120]   But yeah, thank you for asking.
[01:38:28.120 --> 01:38:32.960]   And we actually also just launched follower bio search for both Twitter and Instagram.
[01:38:32.960 --> 01:38:36.760]   So if you have a think up account and you've got kind of more people following you, then
[01:38:36.760 --> 01:38:37.960]   you follow.
[01:38:37.960 --> 01:38:44.160]   And I hope actually this will be useful to you here at Twitch for finding guests or finding,
[01:38:44.160 --> 01:38:47.800]   you know, folks, experts to come talk on the show is you can search for, you know, a keyword
[01:38:47.800 --> 01:38:54.440]   like Android or Google or X, you know, X Googler or Googler and you'll get a list back of people
[01:38:54.440 --> 01:38:55.920]   who follow you.
[01:38:55.920 --> 01:38:58.080]   And that's helpful because, you know, you could direct message them.
[01:38:58.080 --> 01:39:00.880]   You can kind of, you know, connect with them or follow them back.
[01:39:00.880 --> 01:39:02.160]   So that's available to everyone.
[01:39:02.160 --> 01:39:05.120]   And that's a free upgrade to everybody who has a think up account.
[01:39:05.120 --> 01:39:06.120]   So.
[01:39:06.120 --> 01:39:07.880]   And that'll be, I didn't know about that.
[01:39:07.880 --> 01:39:12.640]   And that'll be really helpful to me because I've made a point over the last couple of
[01:39:12.640 --> 01:39:17.400]   years to literally follow every technology journalist there is.
[01:39:17.400 --> 01:39:22.360]   And so I just, my followership is just, it's just super heavy on tech journalists.
[01:39:22.360 --> 01:39:25.600]   One of the things I like about think up is it kind of scolds you.
[01:39:25.600 --> 01:39:29.160]   It doesn't actually scold you, but it basically throws in your face.
[01:39:29.160 --> 01:39:32.640]   It's like, Mike, you really need to back off on the exclamation points.
[01:39:32.640 --> 01:39:34.880]   One third of your tweets have exclamation points.
[01:39:34.880 --> 01:39:37.560]   And Mike, you really talk about yourself a lot.
[01:39:37.560 --> 01:39:40.280]   You use the word me and I a lot.
[01:39:40.280 --> 01:39:41.840]   What's the problem there?
[01:39:41.840 --> 01:39:42.840]   So.
[01:39:42.840 --> 01:39:43.840]   Just awareness.
[01:39:43.840 --> 01:39:46.440]   It's low cost therapy, basically.
[01:39:46.440 --> 01:39:48.440]   It's reality therapy.
[01:39:48.440 --> 01:39:49.640]   Well, thank you.
[01:39:49.640 --> 01:39:50.640]   Yeah.
[01:39:50.640 --> 01:39:54.600]   Jeff Jarvis, professional professor and professional of journalism at the City University of New
[01:39:54.600 --> 01:39:56.040]   York, author of public parts.
[01:39:56.040 --> 01:39:57.040]   What would Google do?
[01:39:57.040 --> 01:39:59.360]   And Gutenberg, the geek, Jeff Jarvis.
[01:39:59.360 --> 01:40:01.040]   Are you working on a book right now?
[01:40:01.040 --> 01:40:02.040]   Are you?
[01:40:02.040 --> 01:40:03.040]   You're not.
[01:40:03.040 --> 01:40:04.680]   Oh, I'm going to record.
[01:40:04.680 --> 01:40:09.840]   I'm going to record the audio book of Geeks, Burying Gifts, Monday, Tuesday at Audible.
[01:40:09.840 --> 01:40:10.840]   Oh, really?
[01:40:10.840 --> 01:40:11.840]   Very cool.
[01:40:11.840 --> 01:40:12.840]   It's cool.
[01:40:12.840 --> 01:40:15.400]   I always wanted to do that, but I got to write a book first.
[01:40:15.400 --> 01:40:16.400]   Yes.
[01:40:16.400 --> 01:40:18.600]   That's the, that's the, well, I did my Kindle single too.
[01:40:18.600 --> 01:40:20.360]   So you can even do that in audio.
[01:40:20.360 --> 01:40:21.360]   Where do you stand?
[01:40:21.360 --> 01:40:26.080]   You're going to do a book and it's going to be a hefty book.
[01:40:26.080 --> 01:40:27.080]   Where are you going to get that published?
[01:40:27.080 --> 01:40:32.920]   You're going to go through Amazon still or what's your ideal way to publish a book these
[01:40:32.920 --> 01:40:33.920]   days?
[01:40:33.920 --> 01:40:35.640]   Yeah, I'm not even sure that a book is the right form.
[01:40:35.640 --> 01:40:36.640]   Depends on you.
[01:40:36.640 --> 01:40:38.480]   What I would want to do next.
[01:40:38.480 --> 01:40:42.720]   I'm not trying to say books are dead or anything like that, but, but I don't know.
[01:40:42.720 --> 01:40:44.960]   I really don't know.
[01:40:44.960 --> 01:40:45.960]   Nobody does.
[01:40:45.960 --> 01:40:46.960]   I can't.
[01:40:46.960 --> 01:40:52.900]   options and they're all fraught with benefits and constraints and all the rest, it's very
[01:40:52.900 --> 01:40:56.740]   difficult. I would love it for somebody like you, with a lot of experience publishing,
[01:40:56.740 --> 01:41:01.160]   and lots of books on lots of different ways to say, "Oh, this is the one you got it, you
[01:41:01.160 --> 01:41:03.900]   got to do it like this." Gina, would you publish another book?
[01:41:03.900 --> 01:41:10.080]   Oh goodness, no. No, no, no, I'll never do that again.
[01:41:10.080 --> 01:41:14.560]   What you're telling—what I'm hearing here is that both of your advice for would-be publishers
[01:41:14.560 --> 01:41:16.940]   for God's sake, don't do it.
[01:41:16.940 --> 01:41:18.940]   Don't write a book.
[01:41:18.940 --> 01:41:26.880]   It's a lot of work for me. It was a lot of work for a little bit of ego, like to say,
[01:41:26.880 --> 01:41:32.500]   to have this physical thing on my shelf. I found web publishing so much more gratifying
[01:41:32.500 --> 01:41:37.740]   and doing these shows, honestly. Much more interaction with your audience. You can evolve
[01:41:37.740 --> 01:41:42.880]   and you can correct and you can iterate and I'm just much more comfortable with that.
[01:41:42.880 --> 01:41:48.300]   But if you've got a book in you, do it. I don't have any more books than me.
[01:41:48.300 --> 01:41:53.860]   I've heard people say that if you were paid, actually paid by the hour if you divided it
[01:41:53.860 --> 01:41:59.640]   up, the typical author would be paid about five cents an hour, which is not a very good
[01:41:59.640 --> 01:42:05.660]   wage, significantly below the minimum wage. Anyway, all right, well, we do this week
[01:42:05.660 --> 01:42:11.700]   in Google every Wednesday at 1 p.m. Pacific, 4 p.m. Eastern. That's 2000 UTC. You can
[01:42:11.700 --> 01:42:16.460]   watch the show live at Twit.TV and press the Live, the Shining Live button, and you can
[01:42:16.460 --> 01:42:24.140]   subscribe to Twig at Twit.TV/Twig. I want to thank you for joining us today and we will
[01:42:24.140 --> 01:42:27.260]   see you next time right here on Twit.
[01:42:27.260 --> 01:42:37.260]   [music]

