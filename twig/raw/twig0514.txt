;FFMETADATA1
title=Alphabetical
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=514
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:07.200]   It's time for Twig this week in Google. Jeff Jarvis is here. Stacey Higginbotham's here. Matthew Ingram is. Congratulations to Matthew.
[00:00:07.200 --> 00:00:15.200]   He's winning a major journalism award. This week we will talk about McLuhan because it's a it's a award from the McLuhan folks.
[00:00:15.200 --> 00:00:22.400]   We'll also talk about the president. He's mad as heck about Twitter and he's not gonna take it anymore.
[00:00:22.400 --> 00:00:27.000]   And what they're doing in Toronto with Sidewalk. It's all coming up next.
[00:00:27.000 --> 00:00:34.000]   I'm Twig. Netcast you love. From people you trust.
[00:00:34.000 --> 00:00:39.000]   This is Twig.
[00:00:39.000 --> 00:00:52.000]   This is Twig. This week in Google. Episode 514. Recorded Wednesday June 26th, 2019. Alpha Beticle.
[00:00:52.000 --> 00:00:59.000]   It's time for Twig this week in Google. The show we cover the latest from the Google verse. Facebook, Twitter.
[00:00:59.000 --> 00:01:04.000]   Yeah, a little bit of Google. Two. Jeff Jarvis is in Toronto today.
[00:01:04.000 --> 00:01:07.000]   Oh, Canada.
[00:01:07.000 --> 00:01:14.000]   You know, I don't want to admit this. Well, we okay so Jeff's here professor at the blah blah blah.
[00:01:14.000 --> 00:01:25.000]   It's the townite center. Craig Newmark, officials chair of the thing at the CUNY Graduate School of Townmark, Nightmark, Journalism.
[00:01:25.000 --> 00:01:30.000]   And then I'm sorry. I'm sorry, Jeff. I just don't have it.
[00:01:30.000 --> 00:01:35.000]   If I can't read it. If I can't read it. Buzz machine. If people watch the show don't know who you are forget it.
[00:01:35.000 --> 00:01:42.000]   Also with this, of course, Stacy Higginbotham and her new digs in the Pacific Northwest. Hello, Stacy.
[00:01:42.000 --> 00:01:50.000]   I'm Patrick Mc in full. Look at that. I did. I put up my little sound panels. I've got one more thing coming and then I'm done.
[00:01:50.000 --> 00:01:56.000]   This is your studio now. Yep. Nice. It looks great. Is that a brother or an HP behind you?
[00:01:56.000 --> 00:02:01.000]   It's a brother. Oh, right. I was probably better be an absent.
[00:02:01.000 --> 00:02:07.000]   Hey, is your reformer in there? Your Pilates machine? No, this is a very tiny office.
[00:02:07.000 --> 00:02:14.000]   So it's downstairs in our gym where we have an elliptical, the reformer and a weight bench.
[00:02:14.000 --> 00:02:19.000]   Boy, you really have embraced the Washington lifestyle. The Washington state.
[00:02:19.000 --> 00:02:25.000]   And joining us also from Canada because he and Jeff are at the same conference.
[00:02:25.000 --> 00:02:28.000]   The great Matthew Ingram from the Columbia Review District.
[00:02:28.000 --> 00:02:33.000]   Matthew's here. Yeah, well, you know, he didn't have to go very far. Of course, Chief Digital Writer at CJR.
[00:02:33.000 --> 00:02:36.000]   Matthew's also going to do home, I believe. Yes.
[00:02:36.000 --> 00:02:40.000]   Yeah, this is not it. I'm at the lake.
[00:02:40.000 --> 00:02:45.000]   Oh, currently. Oh, you're at the lake. Matthew, is it the lake? Okay.
[00:02:45.000 --> 00:02:49.000]   You should have done the show on the kayak, Matthew. We're very disappointed.
[00:02:49.000 --> 00:02:56.000]   I know. The connectivity is not great. In the summertime, Matthew's Instagram is taken over by feet in a kayak.
[00:02:56.000 --> 00:03:02.000]   Yes. All lake all the time. But Matthew, I do have to point out one little thing.
[00:03:02.000 --> 00:03:05.000]   I think it's probably the rise and fall of the third Reich or something like that.
[00:03:05.000 --> 00:03:08.000]   But there is a swastika over your right shoulder.
[00:03:08.000 --> 00:03:12.000]   Yeah, and I don't want to give people the wrong impression.
[00:03:12.000 --> 00:03:15.000]   Yeah, just it's. Oh, yeah, there is. Yeah.
[00:03:15.000 --> 00:03:19.000]   It kind of it does jump out of one. And I don't want.
[00:03:19.000 --> 00:03:23.000]   It is the rise and fall. Yeah, I recognize the spine because it's one of my favorite books.
[00:03:23.000 --> 00:03:25.000]   Oh, there it is. Oh, I see it. Oh, yes.
[00:03:25.000 --> 00:03:29.000]   I just want to get this show banned in Germany or anything. So.
[00:03:29.000 --> 00:03:32.000]   Cover your swastika. Is that what's the request in here?
[00:03:32.000 --> 00:03:34.000]   Can you please cover the swastika?
[00:03:34.000 --> 00:03:36.000]   It's a paper over the next show title.
[00:03:36.000 --> 00:03:39.000]   Wait a minute. No, we can do it. We have the technology. Go ahead.
[00:03:39.000 --> 00:03:44.000]   I just have to tell you. There we go. We zoomed in and now the lower third covers it.
[00:03:44.000 --> 00:03:46.000]   Or me less swastika.
[00:03:46.000 --> 00:03:51.000]   Actually, I don't know a lot of people with bookshelves in their houses with glass doors.
[00:03:51.000 --> 00:03:53.000]   That's very, very.
[00:03:53.000 --> 00:03:58.000]   So this is actually my mom and dad's bookcase that they bought in Germany.
[00:03:58.000 --> 00:03:59.000]   Yes.
[00:03:59.000 --> 00:04:00.000]   The eight.
[00:04:00.000 --> 00:04:01.000]   Ha ha.
[00:04:01.000 --> 00:04:02.000]   That explains the swastika.
[00:04:02.000 --> 00:04:04.000]   It weighs about 1100 pounds.
[00:04:04.000 --> 00:04:06.000]   Anyway.
[00:04:06.000 --> 00:04:09.000]   Did they live there?
[00:04:09.000 --> 00:04:10.000]   They did. Yeah.
[00:04:10.000 --> 00:04:11.000]   Oh, they did.
[00:04:11.000 --> 00:04:15.000]   So they just go there on vacation and bring back an 1100 pound bookcase.
[00:04:15.000 --> 00:04:19.000]   Oh, they lived in Germany for four or five years.
[00:04:19.000 --> 00:04:20.000]   Oh, neat.
[00:04:20.000 --> 00:04:21.000]   I lived in Holland.
[00:04:21.000 --> 00:04:22.000]   Oh, that's neat.
[00:04:22.000 --> 00:04:25.000]   Were you raised in those countries? Were you raised in the low countries?
[00:04:25.000 --> 00:04:30.000]   I was born in Germany, but we only lived there for, I think about it, a year or two.
[00:04:30.000 --> 00:04:32.000]   Yeah, my dad was really--
[00:04:32.000 --> 00:04:33.000]   Strictly trying to watch.
[00:04:33.000 --> 00:04:35.000]   So he was stationed there.
[00:04:35.000 --> 00:04:36.000]   No.
[00:04:36.000 --> 00:04:37.000]   Nine.
[00:04:37.000 --> 00:04:38.000]   Nine is the word.
[00:04:38.000 --> 00:04:43.000]   So Matthew is going to be winning a major award on Friday.
[00:04:43.000 --> 00:04:45.000]   I'm sorry, I left.
[00:04:45.000 --> 00:04:49.000]   Let me do that again. Matthew is winning a major award on Friday.
[00:04:49.000 --> 00:04:51.000]   Very proud of me.
[00:04:51.000 --> 00:04:54.000]   Do we introduce him as an award-winning journalist yet?
[00:04:54.000 --> 00:04:56.000]   Award-winning journalist.
[00:04:56.000 --> 00:04:59.000]   What is the award and what is it for?
[00:04:59.000 --> 00:05:04.000]   It's called the James W. Carey Award for Outstanding Journalism.
[00:05:04.000 --> 00:05:08.000]   And it's from the Media Ecology Association.
[00:05:08.000 --> 00:05:10.000]   Wow, that's pretty sweet.
[00:05:10.000 --> 00:05:14.000]   So Carey was a media theorist.
[00:05:14.000 --> 00:05:17.000]   And yeah, fascinating guy.
[00:05:17.000 --> 00:05:24.000]   I'm really glad to be winning this award because he's been a kind of not a hero, but a guy
[00:05:24.000 --> 00:05:28.000]   I've looked to to sort of understand modern information.
[00:05:28.000 --> 00:05:30.000]   And Jeff, I think is an admirer.
[00:05:30.000 --> 00:05:31.000]   He's a hero of mine.
[00:05:31.000 --> 00:05:33.000]   And I've read his quotes on this show often.
[00:05:33.000 --> 00:05:37.000]   He's the one who talks about how society is a conversation.
[00:05:37.000 --> 00:05:41.000]   Media properly can think is a conversation or roles in conversation, which is also McLuhan,
[00:05:41.000 --> 00:05:45.000]   which is the inspiration of the conference we're going to, Media Ecology.
[00:05:45.000 --> 00:05:48.000]   He understood conversation and morality.
[00:05:48.000 --> 00:05:53.000]   Yeah, Carey talked about two sort of alternative views of information.
[00:05:53.000 --> 00:05:59.000]   One that journalists often think about as it's media just transmits information.
[00:05:59.000 --> 00:06:04.000]   So journalism tells people facts and they understand things and a story.
[00:06:04.000 --> 00:06:10.000]   But the other way I'm looking at media is that it's about community.
[00:06:10.000 --> 00:06:18.000]   So people don't necessarily look to or share information because it's factually true or accurate.
[00:06:18.000 --> 00:06:23.000]   It's because it makes them feel a certain way or because they want to belong to a certain group.
[00:06:23.000 --> 00:06:26.000]   And I think you see that a lot on social networks, for example.
[00:06:26.000 --> 00:06:27.000]   Oh, yeah.
[00:06:27.000 --> 00:06:33.000]   So this is this conference is about the it's called the Media Ecology, the Media Ecology Conference.
[00:06:33.000 --> 00:06:35.000]   Is that the name of it?
[00:06:35.000 --> 00:06:40.000]   So Media Ethics is this it's the Media Ecology Association.
[00:06:40.000 --> 00:06:41.000]   Okay.
[00:06:41.000 --> 00:06:44.000]   And this conference is called Media Ethics, Human Ecology in a Connected World.
[00:06:44.000 --> 00:06:49.000]   It's interesting because I'm just looking at Wikipedia's article on Media Ecology.
[00:06:49.000 --> 00:06:54.000]   And in place of the word media, you could easily use the word Internet.
[00:06:54.000 --> 00:06:55.000]   In this context.
[00:06:55.000 --> 00:07:02.000]   Media Ecology, according to, I guess, Neil Postman, looks into the matter of how media of communication
[00:07:02.000 --> 00:07:07.000]   affect human perception, understanding, feeling and value and how our interaction with media
[00:07:07.000 --> 00:07:11.000]   facilitates or impedes our chances of survival.
[00:07:11.000 --> 00:07:14.000]   Boy, that couldn't be a hotter topic right now.
[00:07:14.000 --> 00:07:16.000]   Amen, brother.
[00:07:16.000 --> 00:07:20.000]   So postman, I saw the, I've been kicking down a mclue and I saw the conference.
[00:07:20.000 --> 00:07:21.000]   I saw Matthew was speaking at the conference.
[00:07:21.000 --> 00:07:22.000]   I didn't even know about the word yet.
[00:07:22.000 --> 00:07:27.000]   And I asked him to introduce me to the organizer so I could, but I already bought the ticket immediately.
[00:07:27.000 --> 00:07:28.000]   I'm kicking down.
[00:07:28.000 --> 00:07:30.000]   It's a great, it's a great conference.
[00:07:30.000 --> 00:07:32.000]   It's a fun.
[00:07:32.000 --> 00:07:37.000]   Yeah, Postman, I guess, is the one who first coined that term, but it was based on mclue and's work.
[00:07:37.000 --> 00:07:41.000]   So I asked a shameful question before the show.
[00:07:41.000 --> 00:07:47.000]   Is Marshall, because Marshall McLuhan, when I was in high school, the medium is the message.
[00:07:47.000 --> 00:07:51.000]   In fact, there was a little book that all us high school kids read called the medium is the massage.
[00:07:51.000 --> 00:07:56.000]   I don't know what that meant, but I never did ever figure out what that meant, but it was about media.
[00:07:56.000 --> 00:08:01.000]   It was, I guess it was about this, how the media is the context, the Petri dish we're growing in,
[00:08:01.000 --> 00:08:03.000]   and our culture grows in.
[00:08:03.000 --> 00:08:11.000]   And it was very cool then in the 70s, and I just was wondering if it was still O'Korand today, if people, if it was considered as a...
[00:08:11.000 --> 00:08:19.000]   People think a lot, you know, I think about Alvin Toffler in future shock, which was also big in that time.
[00:08:19.000 --> 00:08:21.000]   And that hasn't aged all that well.
[00:08:21.000 --> 00:08:24.000]   How is Marshall McLuhan's philosophy?
[00:08:24.000 --> 00:08:26.000]   I think it's even more relevant.
[00:08:26.000 --> 00:08:40.000]   Like I think it's, you know, at the time, and even well into the sort of 80s, I would say, he seemed so out there, like his views seem to be so beyond what people were accustomed to thinking about.
[00:08:40.000 --> 00:08:47.000]   But as the internet and the digital age and disinformation and cyberspace and so on have become more and more part of our lives,
[00:08:47.000 --> 00:08:54.000]   I think the way he thought about media and information and culture is just becoming more and more relevant.
[00:08:54.000 --> 00:09:03.000]   And I use this, or I refer to this quote when we were talking about it before, one that I think sums up the disinformation environment that we're living in right now.
[00:09:03.000 --> 00:09:13.000]   He said the Third World War was going to be a guerrilla information war with no division between military and civilian participation waged in cyberspace.
[00:09:13.000 --> 00:09:24.000]   The Third World War will be an information war, a guerrilla information war, no division between military and civilian participation.
[00:09:24.000 --> 00:09:25.000]   Yeah.
[00:09:25.000 --> 00:09:27.000]   Wait, guerrillas can't type.
[00:09:27.000 --> 00:09:35.000]   But that's kind of what a guerrilla war implies is that there aren't combatants and noncombatants, that everybody's a combatant.
[00:09:35.000 --> 00:09:36.000]   Right.
[00:09:36.000 --> 00:09:38.000]   And he said that in the other thing he said.
[00:09:38.000 --> 00:09:39.000]   Go ahead, Brett.
[00:09:39.000 --> 00:09:40.000]   1970.
[00:09:40.000 --> 00:09:41.000]   Yeah.
[00:09:41.000 --> 00:09:42.000]   Wow.
[00:09:42.000 --> 00:09:45.000]   And but he didn't have any conception of cyberspace at the time.
[00:09:45.000 --> 00:09:47.000]   I mean, that didn't...
[00:09:47.000 --> 00:09:48.000]   He did actually.
[00:09:48.000 --> 00:09:49.000]   He did.
[00:09:49.000 --> 00:09:50.000]   He had a sense of the electric age.
[00:09:50.000 --> 00:09:51.000]   The electric.
[00:09:51.000 --> 00:09:53.000]   So for him it was TV, much as for Neil Postman.
[00:09:53.000 --> 00:09:54.000]   It was TV.
[00:09:54.000 --> 00:09:55.000]   Yeah.
[00:09:55.000 --> 00:09:56.000]   Yes.
[00:09:56.000 --> 00:09:57.000]   Okay.
[00:09:57.000 --> 00:10:11.000]   But he also talked about how this, I've talked about the Gutenberg-Prince is on the show, where how we cognate the world in an age of text, that when things are in packages and we think of the world in a long way,
[00:10:11.000 --> 00:10:14.000]   we think of the world in a linear fashion with a beginning and end.
[00:10:14.000 --> 00:10:18.000]   And the line he said, this sentence, this example, becomes our organizing principle.
[00:10:18.000 --> 00:10:19.000]   So he really looked.
[00:10:19.000 --> 00:10:29.000]   He wasn't necessarily a fan or a hater of what he wrote about, but he tried to look at the impact it had on our lives and how it affected our worldview.
[00:10:29.000 --> 00:10:30.000]   That's the ecology.
[00:10:30.000 --> 00:10:39.000]   And if you think about the internet, my friend, Hussein, has written about how, for him at least, the internet's becoming more and more like TV.
[00:10:39.000 --> 00:10:51.000]   So a lot of the things that Postman and McLuhan said about TV or saw in the future as TV grew have really come to pass, I think.
[00:10:51.000 --> 00:10:53.000]   Well, I think that's going to be temporary.
[00:10:53.000 --> 00:10:57.000]   I think it's an attempt at a reversion to form.
[00:10:57.000 --> 00:10:58.000]   And we'll see.
[00:10:58.000 --> 00:11:00.000]   As I always say, I'm too old to know.
[00:11:00.000 --> 00:11:02.000]   And back to your toddler, Point Leo.
[00:11:02.000 --> 00:11:06.000]   The problem is what McLuhan didn't predict the future.
[00:11:06.000 --> 00:11:08.000]   McLuhan understood transition.
[00:11:08.000 --> 00:11:12.000]   Future, I think futurist is the most ridiculous job title on earth.
[00:11:12.000 --> 00:11:16.000]   And Tophler tried to predict that future.
[00:11:16.000 --> 00:11:20.000]   By the way, media is a massage.
[00:11:20.000 --> 00:11:26.000]   McLuhan's son, who died about a year ago, year and half ago, said that McLuhan loved puns.
[00:11:26.000 --> 00:11:27.000]   Absolutely loved puns.
[00:11:27.000 --> 00:11:31.000]   And his son argued that it was a typo that came back from the printer.
[00:11:31.000 --> 00:11:37.000]   But he also liked the idea that it was the mess age.
[00:11:37.000 --> 00:11:38.000]   Right.
[00:11:38.000 --> 00:11:40.000]   It's the mess age.
[00:11:40.000 --> 00:11:43.000]   The way I'm hyphenated.
[00:11:43.000 --> 00:11:44.000]   Yeah.
[00:11:44.000 --> 00:11:46.000]   It's good because I never understood.
[00:11:46.000 --> 00:11:48.000]   I never understood for a minute.
[00:11:48.000 --> 00:11:49.000]   I didn't either.
[00:11:49.000 --> 00:11:50.000]   I should go try that.
[00:11:50.000 --> 00:11:52.000]   It was like watching 2001 of Space Odyssey.
[00:11:52.000 --> 00:11:54.000]   What does the obelisk mean?
[00:11:54.000 --> 00:11:55.000]   Yeah.
[00:11:55.000 --> 00:11:56.000]   Yeah.
[00:11:56.000 --> 00:11:57.000]   Well, that was the era that we grew up in.
[00:11:57.000 --> 00:11:58.000]   I never figured that out.
[00:11:58.000 --> 00:12:01.000]   When it was the era we grew up in, it was an era full of mysteries.
[00:12:01.000 --> 00:12:04.000]   And because it was a youth culture and youth are basically clueless.
[00:12:04.000 --> 00:12:05.000]   It's just perfect.
[00:12:05.000 --> 00:12:06.000]   Perfect timing.
[00:12:06.000 --> 00:12:07.000]   It's such a serious thing.
[00:12:07.000 --> 00:12:08.000]   We're not using it.
[00:12:08.000 --> 00:12:09.000]   We're found.
[00:12:09.000 --> 00:12:11.000]   Say, "Gave you man."
[00:12:11.000 --> 00:12:12.000]   Punch ball.
[00:12:12.000 --> 00:12:13.000]   We're all stoned.
[00:12:13.000 --> 00:12:14.000]   We didn't know.
[00:12:14.000 --> 00:12:15.000]   We thought it was cool.
[00:12:15.000 --> 00:12:16.000]   We're stoned.
[00:12:16.000 --> 00:12:17.000]   We're stoned in a clue.
[00:12:17.000 --> 00:12:23.800]   McLuhan also talked about surveillance and how the sort of electric future would enable
[00:12:23.800 --> 00:12:26.000]   universal womb to tomb surveillance.
[00:12:26.000 --> 00:12:28.000]   Well, it's a shame he...
[00:12:28.000 --> 00:12:29.600]   How long ago did he pass away?
[00:12:29.600 --> 00:12:33.000]   He would have been 108 today, I think.
[00:12:33.000 --> 00:12:38.000]   Well, okay, Google.
[00:12:38.000 --> 00:12:43.000]   And now we mentioned Google for the first time on the show.
[00:12:43.000 --> 00:12:47.000]   He was born in 1980.
[00:12:47.000 --> 00:12:51.000]   So he missed all of this.
[00:12:51.000 --> 00:12:52.000]   Yeah.
[00:12:52.000 --> 00:12:56.000]   Well, he did say it's the internet.
[00:12:56.000 --> 00:12:57.000]   Well, yeah.
[00:12:57.000 --> 00:12:59.000]   Without trying to foresee it, he foresaw it.
[00:12:59.000 --> 00:13:04.000]   So I'm going to bring up, as I often do on this show, one of my favorite...
[00:13:04.000 --> 00:13:06.000]   We've got to get her on the show.
[00:13:06.000 --> 00:13:09.000]   Zanep, to Fexy, to Fexy.
[00:13:09.000 --> 00:13:10.000]   To Fexy.
[00:13:10.000 --> 00:13:11.000]   To Fexy.
[00:13:11.000 --> 00:13:12.000]   To Fexy.
[00:13:12.000 --> 00:13:13.000]   I don't know how.
[00:13:13.000 --> 00:13:15.000]   It's a Turkish name and, of course, we're inevitably.
[00:13:15.000 --> 00:13:16.000]   No, it's just right.
[00:13:16.000 --> 00:13:17.000]   Yeah.
[00:13:17.000 --> 00:13:19.000]   Wonderful professor of sociology.
[00:13:19.000 --> 00:13:27.000]   Article this week in "On Wired," the internet has made dupes and cynics of us all.
[00:13:27.000 --> 00:13:32.000]   She has become the kind of Paul Revere of the internet era.
[00:13:32.000 --> 00:13:36.000]   But what she's talking about in here, I thought maybe is kind of timely.
[00:13:36.000 --> 00:13:46.000]   Professor at University of North Carolina at Chapel Hill, is this notion of trust that
[00:13:46.000 --> 00:13:51.000]   we have been in the past, in the United States, a high trust society.
[00:13:51.000 --> 00:13:52.000]   Low corruption.
[00:13:52.000 --> 00:13:59.000]   You could pretty much trust what people said, but that in the internet era, we become increasingly
[00:13:59.000 --> 00:14:01.000]   a low trust society.
[00:14:01.000 --> 00:14:03.000]   Fake reviews on Amazon.
[00:14:03.000 --> 00:14:06.000]   Russian bots on Twitter.
[00:14:06.000 --> 00:14:12.000]   Conversational AI is posing as humans, and, of course, a political class that seems to be
[00:14:12.000 --> 00:14:15.000]   corrupt, endlessly corrupt.
[00:14:15.000 --> 00:14:17.000]   Malware in your computers.
[00:14:17.000 --> 00:14:22.000]   Add, click fraud, and on and on and on.
[00:14:22.000 --> 00:14:27.600]   And she warns that this is a bad trend, this trend to becoming a low trust society.
[00:14:27.600 --> 00:14:30.560]   She says, "The internet is increasingly a low trust society.
[00:14:30.560 --> 00:14:38.640]   One where an assumption of pervasive fraud is simply built into the way many things function."
[00:14:38.640 --> 00:14:41.400]   She says, "People do adapt to low trust societies, of course.
[00:14:41.400 --> 00:14:45.800]   A world of mouth recommendations for familiar sources become more important.
[00:14:45.800 --> 00:14:50.560]   Doing business with family and local networks started taking precedence.
[00:14:50.560 --> 00:14:56.520]   But also mafia-like organizations spring up, imposing a kind of accountability to brutal
[00:14:56.520 --> 00:14:57.520]   costs.
[00:14:57.520 --> 00:15:01.320]   And ultimately, and this is maybe the most scary sentence in this article, "People in
[00:15:01.320 --> 00:15:09.240]   low trust societies may welcome an authoritarian ruler, someone who will impose order and consequences
[00:15:09.240 --> 00:15:10.240]   from on high.
[00:15:10.240 --> 00:15:12.080]   Sure, the tyrant is also corrupt and cruel.
[00:15:12.080 --> 00:15:19.640]   But the alternative is the tiring, emisserating absence of everyday safety and security."
[00:15:19.640 --> 00:15:20.640]   I love this line.
[00:15:20.640 --> 00:15:24.040]   "During the reign of Kublai Khan, it was said a maiden bearing a nugget of gold on her
[00:15:24.040 --> 00:15:27.960]   head could wander safely throughout the realm.
[00:15:27.960 --> 00:15:34.480]   The great Khan required absolute submission, and even repression has some perks."
[00:15:34.480 --> 00:15:35.480]   What do you think?
[00:15:35.480 --> 00:15:41.120]   I'll let you start, Jeff, because I think latency is kind of killing your ability to
[00:15:41.120 --> 00:15:42.120]   converse.
[00:15:42.120 --> 00:15:43.120]   So why don't you start?
[00:15:43.120 --> 00:15:44.120]   Sorry.
[00:15:44.120 --> 00:15:45.120]   No, it's not your fault.
[00:15:45.120 --> 00:15:48.560]   Well, I also just put another piece up in the bottom of the rundown, which is related,
[00:15:48.560 --> 00:15:54.360]   I think, which is Mike Godwin wrote a piece in Reason inspired by Neil Stevenson's new
[00:15:54.360 --> 00:15:56.240]   and very thick 800-page novel.
[00:15:56.240 --> 00:15:58.040]   Which I have finished.
[00:15:58.040 --> 00:15:59.040]   Which I have finished.
[00:15:59.040 --> 00:16:00.040]   Yeah.
[00:16:00.040 --> 00:16:06.320]   Which he argues that maybe this, the cure for disinformation is disinformation.
[00:16:06.320 --> 00:16:09.600]   Maybe when we all distrust everything, maybe that's where we need to get to.
[00:16:09.600 --> 00:16:12.920]   Yeah, you could tell Mike hasn't finished the novel, by the way.
[00:16:12.920 --> 00:16:15.960]   So what happens in the novel?
[00:16:15.960 --> 00:16:17.360]   And I think I might have brought this up.
[00:16:17.360 --> 00:16:18.360]   It's fascinating.
[00:16:18.360 --> 00:16:24.680]   What happens in the novel is one character, one important character, Maeve, is being slimed
[00:16:24.680 --> 00:16:27.360]   by trolls, never happens, right, on the internet.
[00:16:27.360 --> 00:16:31.240]   Just completely her reputation is being trashed for no apparent reason.
[00:16:31.240 --> 00:16:33.720]   It's almost a gamer-gate situation.
[00:16:33.720 --> 00:16:35.600]   And a geek comes up with a great idea.
[00:16:35.600 --> 00:16:41.760]   He creates many, many bots to put out information, misinformation, information of all kinds about
[00:16:41.760 --> 00:16:47.520]   Maeve, flood the internet with it so that when you go on the net, you can't tell, you
[00:16:47.520 --> 00:16:53.040]   know anything you read about Maeve is bogus because there's so much of it.
[00:16:53.040 --> 00:16:54.040]   She's a leftist.
[00:16:54.040 --> 00:16:55.040]   She's a writer.
[00:16:55.040 --> 00:16:56.040]   She's a feminist.
[00:16:56.040 --> 00:16:57.040]   She hates women.
[00:16:57.040 --> 00:17:00.240]   You put up enough contradictory bogus information.
[00:17:00.240 --> 00:17:01.240]   None of it is trusted.
[00:17:01.240 --> 00:17:03.920]   And so it drowns out the trolling.
[00:17:03.920 --> 00:17:10.920]   He hasn't finished the novel because the consequence of this is, and I think I mentioned this,
[00:17:10.920 --> 00:17:14.920]   that any, it's a spoiler, but it's not going to spoil the novel.
[00:17:14.920 --> 00:17:16.400]   It's a great novel.
[00:17:16.400 --> 00:17:20.720]   People who have enough money will hire editors on the internet, human curators to try to
[00:17:20.720 --> 00:17:23.320]   give you decent information.
[00:17:23.320 --> 00:17:28.800]   People who can't afford that, which is the vast majority, will be fed in such a volume
[00:17:28.800 --> 00:17:35.160]   of phony fake news that the country splits into faction.
[00:17:35.160 --> 00:17:37.720]   People who believe in vaccinations, people who don't.
[00:17:37.720 --> 00:17:39.520]   Does that sound familiar?
[00:17:39.520 --> 00:17:40.520]   They're already.
[00:17:40.520 --> 00:17:41.520]   Yeah.
[00:17:41.520 --> 00:17:46.520]   So that, and that no, there's no crazy theory that doesn't have its adherence.
[00:17:46.520 --> 00:17:51.040]   And as a result, it is very dangerous because as you travel through the country, you go
[00:17:51.040 --> 00:17:53.160]   into the different zone.
[00:17:53.160 --> 00:17:58.080]   There's a zone where people don't believe in Christ's crucifixion.
[00:17:58.080 --> 00:18:03.400]   They think that that was a fiction that was promulgated by somebody who didn't want anybody,
[00:18:03.400 --> 00:18:05.760]   whatever, they had some crazy, cracked reason.
[00:18:05.760 --> 00:18:08.920]   So they go around crucifying people to prove it's not real.
[00:18:08.920 --> 00:18:09.920]   It's just weird.
[00:18:09.920 --> 00:18:13.960]   Neil Stevenson is a great thinker about the future.
[00:18:13.960 --> 00:18:16.080]   I think sometimes not a great writer.
[00:18:16.080 --> 00:18:20.560]   Some of his wrote some of my favorite novels, but he also has some difficulty finishing them.
[00:18:20.560 --> 00:18:24.760]   And I'm sad to say the fall does not end in a very coherent way.
[00:18:24.760 --> 00:18:28.320]   However, it's a fascinating novel full of great ideas, but that is a very interesting
[00:18:28.320 --> 00:18:29.320]   one.
[00:18:29.320 --> 00:18:30.320]   Or is it worth reading or no?
[00:18:30.320 --> 00:18:31.320]   It's 888 pages.
[00:18:31.320 --> 00:18:34.320]   I listened and read.
[00:18:34.320 --> 00:18:39.760]   I have a new method, which really works well as to buy the Kindle version and the audio
[00:18:39.760 --> 00:18:44.800]   book version and go back and forth so I could really, and I was trying to remember we talked
[00:18:44.800 --> 00:18:48.360]   about the New York Times article by a novelist saying people should stop binging TV and start
[00:18:48.360 --> 00:18:49.360]   binging novels.
[00:18:49.360 --> 00:18:51.000]   I wanted to give that a chance.
[00:18:51.000 --> 00:18:55.480]   He said, you know, if the novel is written to be binged, to be immersed, immersive in
[00:18:55.480 --> 00:18:56.560]   your, in this world.
[00:18:56.560 --> 00:18:57.560]   So immerse yourself.
[00:18:57.560 --> 00:19:00.760]   Don't read 8 pages at a time, which is what I do.
[00:19:00.760 --> 00:19:01.760]   Mike Mosk?
[00:19:01.760 --> 00:19:03.520]   I feel like Neil also.
[00:19:03.520 --> 00:19:05.320]   Go ahead, Matt, and then Stacy.
[00:19:05.320 --> 00:19:12.640]   I love his books, but I would agree that I'm concerned if the book is at length.
[00:19:12.640 --> 00:19:18.040]   Lots of times he spends pages and pages describing, you know, the rotation of the earth or the
[00:19:18.040 --> 00:19:20.120]   way that certain.
[00:19:20.120 --> 00:19:26.840]   It's fascinating, but I often wonder whether, you know, maybe just a paragraph or two about
[00:19:26.840 --> 00:19:28.240]   that would be better.
[00:19:28.240 --> 00:19:29.240]   Yeah.
[00:19:29.240 --> 00:19:30.400]   Well, you will.
[00:19:30.400 --> 00:19:31.400]   Okay.
[00:19:31.400 --> 00:19:33.560]   Here's the best part about this novel.
[00:19:33.560 --> 00:19:35.360]   Still shorter than infinite chess.
[00:19:35.360 --> 00:19:36.360]   Yeah.
[00:19:36.360 --> 00:19:37.360]   No, it is.
[00:19:37.360 --> 00:19:38.360]   And I still haven't finished a chess.
[00:19:38.360 --> 00:19:39.360]   I have tried and tried and tried.
[00:19:39.360 --> 00:19:40.360]   I did.
[00:19:40.360 --> 00:19:41.360]   Okay.
[00:19:41.360 --> 00:19:42.360]   You're better than me.
[00:19:42.360 --> 00:19:43.360]   It was like Everest.
[00:19:43.360 --> 00:19:44.360]   Yeah.
[00:19:44.360 --> 00:19:45.360]   It was like Everest.
[00:19:45.360 --> 00:19:47.400]   The funny thing about this novel was it will make you question reality.
[00:19:47.400 --> 00:19:49.760]   So that's, you know, that's interesting.
[00:19:49.760 --> 00:19:50.760]   It sounds like reality.
[00:19:50.760 --> 00:19:54.760]   It sounds like it's just describing what's happening right now.
[00:19:54.760 --> 00:19:55.760]   I mean, you know what?
[00:19:55.760 --> 00:19:58.160]   It is worth reading.
[00:19:58.160 --> 00:19:59.880]   It is well worth reading.
[00:19:59.880 --> 00:20:03.760]   It's hard to say whether it's a great novel or just well worth reading, but it's well
[00:20:03.760 --> 00:20:04.760]   worth reading.
[00:20:04.760 --> 00:20:06.480]   Go ahead, Stacy.
[00:20:06.480 --> 00:20:08.320]   I have no idea now what I was going to say.
[00:20:08.320 --> 00:20:09.320]   Oh, I'm sorry.
[00:20:09.320 --> 00:20:10.320]   So it wasn't that important.
[00:20:10.320 --> 00:20:11.320]   I'm sorry.
[00:20:11.320 --> 00:20:12.880]   My fault.
[00:20:12.880 --> 00:20:15.920]   The thing, and people have been talking about this with Stevenson and he's even been giving
[00:20:15.920 --> 00:20:21.520]   interviews about this is this novel is clearly written to as a commentary on modern society
[00:20:21.520 --> 00:20:26.560]   and what the internet and fake news and trolling and all of this stuff has done.
[00:20:26.560 --> 00:20:31.920]   But the central theme of the novel is, you know how they say they always said they cut
[00:20:31.920 --> 00:20:36.640]   off Walt Disney's head and froze it so that in a future culture he that would figure out
[00:20:36.640 --> 00:20:37.960]   how to revive the brain.
[00:20:37.960 --> 00:20:39.440]   They could revive the brain.
[00:20:39.440 --> 00:20:45.720]   Well, in this near future, they realized that's a terrible kind of mechanical way of
[00:20:45.720 --> 00:20:46.720]   doing it.
[00:20:46.720 --> 00:20:51.000]   All we really need to do is scan the brain to create the connectome of all of the interconnections
[00:20:51.000 --> 00:20:55.960]   between all the neurons and then we can easily simulate the brain's activity in some future
[00:20:55.960 --> 00:20:57.800]   computer that has the capacity.
[00:20:57.800 --> 00:21:01.280]   It turns out to be quantum computers, but some future computers that have the capacity
[00:21:01.280 --> 00:21:04.040]   to recreate life.
[00:21:04.040 --> 00:21:06.160]   And then which is the plot of coercion, isn't it?
[00:21:06.160 --> 00:21:09.200]   I haven't read that either, but it's another new novel.
[00:21:09.200 --> 00:21:10.200]   It's out.
[00:21:10.200 --> 00:21:14.800]   Least half their book is the upshot of that.
[00:21:14.800 --> 00:21:18.080]   And it goes into the distant future as some of many...
[00:21:18.080 --> 00:21:19.080]   Incorporate.
[00:21:19.080 --> 00:21:20.560]   ...and having just read the reviews.
[00:21:20.560 --> 00:21:23.120]   They people have false memories.
[00:21:23.120 --> 00:21:25.200]   They believe they have a child they don't have.
[00:21:25.200 --> 00:21:26.200]   Yeah.
[00:21:26.200 --> 00:21:30.120]   And when the child they don't have dies, they can't handle it.
[00:21:30.120 --> 00:21:31.120]   So even in their own life.
[00:21:31.120 --> 00:21:33.200]   So we're grappling with this question of facts.
[00:21:33.200 --> 00:21:34.200]   Yes.
[00:21:34.200 --> 00:21:35.200]   What is reality?
[00:21:35.200 --> 00:21:36.200]   What are facts?
[00:21:36.200 --> 00:21:41.040]   I mean, the big change of the internet was that it made facts inexpensive, which was
[00:21:41.040 --> 00:21:43.880]   a big shift in the way and what we think of as knowledge.
[00:21:43.880 --> 00:21:49.720]   Facts were universal, but then what happened is facts got so devalued that you got alternate
[00:21:49.720 --> 00:21:54.600]   facts and they were indistinguishable from facts, factual facts.
[00:21:54.600 --> 00:21:59.520]   And I think that to go back to what Zaydap was saying, the idea of trust, this is something
[00:21:59.520 --> 00:22:06.840]   I've talked with a bunch of people about as trust relates to journalism, lots of projects
[00:22:06.840 --> 00:22:12.880]   and resources and experiments and companies are devoted to trust and increasing trust
[00:22:12.880 --> 00:22:17.960]   and journalists let's want to build trust and all of which is great.
[00:22:17.960 --> 00:22:23.560]   And I'm not saying those things aren't valuable, but the concept of trust as it applies to
[00:22:23.560 --> 00:22:29.280]   journalism is a very, very squishy concept and very difficult to kind of grapple with.
[00:22:29.280 --> 00:22:31.800]   Lots of people trust Breitbart.
[00:22:31.800 --> 00:22:36.800]   Lots of people trust their crazy uncle who believes all these incredible and conspiracy
[00:22:36.800 --> 00:22:37.800]   theories.
[00:22:37.800 --> 00:22:45.400]   But trust, in that sense, doesn't mean it just means I trust this person or this news
[00:22:45.400 --> 00:22:47.800]   that let to give me what I expect.
[00:22:47.800 --> 00:22:48.800]   Right.
[00:22:48.800 --> 00:22:57.280]   That's why it's so dangerous to have word of mouth become everyone's de facto standard.
[00:22:57.280 --> 00:22:58.280]   And yeah.
[00:22:58.280 --> 00:22:59.280]   Yeah.
[00:22:59.280 --> 00:23:05.800]   And this is the game that Jeff was saying about moving back towards a more oral culture.
[00:23:05.800 --> 00:23:10.640]   Twitter in a lot of ways is a lot more like speaking than it is like writing or publishing
[00:23:10.640 --> 00:23:13.720]   where it's in a weird place in between those two things.
[00:23:13.720 --> 00:23:19.040]   But to the extent that it's a lot more the way oral cultures used to be, it can actually
[00:23:19.040 --> 00:23:24.240]   exaggerate problems in a way that text did not.
[00:23:24.240 --> 00:23:31.080]   In the early days of text, text was not trusted because text was a little of the stakes.
[00:23:31.080 --> 00:23:32.920]   Anybody could make text.
[00:23:32.920 --> 00:23:37.360]   And all you trusted was the people you trusted telling you what was going on.
[00:23:37.360 --> 00:23:39.160]   So we've been here before.
[00:23:39.160 --> 00:23:40.160]   Oh, yeah.
[00:23:40.160 --> 00:23:43.320]   We've definitely been here before.
[00:23:43.320 --> 00:23:45.320]   Yes.
[00:23:45.320 --> 00:23:46.320]   Yeah.
[00:23:46.320 --> 00:23:47.320]   I love you.
[00:23:47.320 --> 00:23:48.320]   Station.
[00:23:48.320 --> 00:23:54.440]   No, no, no, no, we're going to talk about tech and not going to go there.
[00:23:54.440 --> 00:23:55.680]   Yeah, I'm going to go there.
[00:23:55.680 --> 00:24:02.400]   Well, yeah, but I mean, this is this is in a in a in a global way kind of what what has
[00:24:02.400 --> 00:24:03.400]   tech wrought.
[00:24:03.400 --> 00:24:08.960]   And I think it's important to talk about the outcomes as much as related.
[00:24:08.960 --> 00:24:11.920]   Like, I mean, YouTube has a disinformation problem.
[00:24:11.920 --> 00:24:15.440]   YouTube's recommendation algorithm has a disinformation problem.
[00:24:15.440 --> 00:24:18.800]   So that's a tech partially boy is YouTube in trouble now.
[00:24:18.800 --> 00:24:19.800]   Right?
[00:24:19.800 --> 00:24:20.800]   The FDC is the.
[00:24:20.800 --> 00:24:21.800]   The end is the.
[00:24:21.800 --> 00:24:22.800]   It's human problem.
[00:24:22.800 --> 00:24:23.800]   Human problem.
[00:24:23.800 --> 00:24:24.800]   It's human problem.
[00:24:24.800 --> 00:24:25.800]   It's human problem.
[00:24:25.800 --> 00:24:27.600]   We trust them and exploit it.
[00:24:27.600 --> 00:24:28.600]   Right.
[00:24:28.600 --> 00:24:29.600]   Go ahead, Stacey.
[00:24:29.600 --> 00:24:30.600]   Well, so we trust people.
[00:24:30.600 --> 00:24:37.040]   We don't we don't as people make the distinction between expertise.
[00:24:37.040 --> 00:24:42.240]   Like, I trust you, Matthew, on media issues.
[00:24:42.240 --> 00:24:46.920]   But if you tried to explain to me quantum computing, I'm probably wouldn't trust you
[00:24:46.920 --> 00:24:47.920]   that much.
[00:24:47.920 --> 00:24:51.760]   But for most people, we don't make those distinctions because, you know, like I like you, Matthew,
[00:24:51.760 --> 00:24:53.640]   of course, you're going to not lead me wrong.
[00:24:53.640 --> 00:24:59.360]   And I don't think you intentionally would, but you know, and I think we're we're stuck
[00:24:59.360 --> 00:25:00.360]   in this.
[00:25:00.360 --> 00:25:04.680]   And then as so you've got that layer, people are not very they don't appreciate the nuances
[00:25:04.680 --> 00:25:07.160]   of what they should trust from people.
[00:25:07.160 --> 00:25:11.720]   And then the other thing is we have this abusive culture, this capitalistic culture
[00:25:11.720 --> 00:25:17.000]   where you're like, you need to monetize your followers trust.
[00:25:17.000 --> 00:25:20.520]   And you may end up doing that to their detriment.
[00:25:20.520 --> 00:25:22.160]   And you know, that's that's it.
[00:25:22.160 --> 00:25:26.920]   You're not doing that, Matthew, but you know, that's kind of where we are in the the influencer
[00:25:26.920 --> 00:25:28.360]   culture, right?
[00:25:28.360 --> 00:25:32.520]   So I don't know how to get out from that.
[00:25:32.520 --> 00:25:33.520]   And I don't know if.
[00:25:33.520 --> 00:25:36.400]   And I think it was a problem to in writing.
[00:25:36.400 --> 00:25:38.200]   It's just it costs more to write.
[00:25:38.200 --> 00:25:39.680]   So you wouldn't want to have to.
[00:25:39.680 --> 00:25:40.680]   That's a good point.
[00:25:40.680 --> 00:25:42.600]   There's a lot more of it now.
[00:25:42.600 --> 00:25:43.600]   Exactly.
[00:25:43.600 --> 00:25:46.600]   But again, we could also hear voices who were not heard before.
[00:25:46.600 --> 00:25:51.160]   So we've got to come back to the positives that we were in a controlled old white man
[00:25:51.160 --> 00:25:52.160]   world, right?
[00:25:52.160 --> 00:25:53.160]   Right.
[00:25:53.160 --> 00:25:57.560]   The world was supposedly trusted was old white men saying trust me.
[00:25:57.560 --> 00:26:00.240]   And there were a lot of people who didn't didn't have the opportunity to say so.
[00:26:00.240 --> 00:26:04.160]   And now Stacey can say right here, I don't trust you all white guys.
[00:26:04.160 --> 00:26:07.480]   But that's why this is but that's why this is so challenging because of course we wanted
[00:26:07.480 --> 00:26:09.120]   to get rid of that.
[00:26:09.120 --> 00:26:14.480]   But at the same at the same time, a lot of the voices who are now empowered are not good
[00:26:14.480 --> 00:26:15.480]   trustworthy voices.
[00:26:15.480 --> 00:26:16.480]   Right.
[00:26:16.480 --> 00:26:17.480]   We'll figure it out.
[00:26:17.480 --> 00:26:20.960]   And it's a case of, you know, be be careful what you wish for.
[00:26:20.960 --> 00:26:25.160]   I mean, if I know in the early days, and I'm sure Jeff was the same and Leo, you probably
[00:26:25.160 --> 00:26:26.160]   were too early.
[00:26:26.160 --> 00:26:30.160]   Early days of Twitter, early days of internet, early days of blogging.
[00:26:30.160 --> 00:26:31.160]   This is great.
[00:26:31.160 --> 00:26:34.240]   The democratization of information that's super fantastic.
[00:26:34.240 --> 00:26:35.680]   It's going to be awesome.
[00:26:35.680 --> 00:26:39.160]   Everybody's going to, you know, be able to speak and exchange information.
[00:26:39.160 --> 00:26:40.320]   And that is all true.
[00:26:40.320 --> 00:26:43.960]   But worse, we've seen many, many downsides to that.
[00:26:43.960 --> 00:26:48.720]   So Stacey, I think you and I agree, maybe not, but I think you and I agree that the
[00:26:48.720 --> 00:26:55.960]   best solution to this is a literate and educated public, right?
[00:26:55.960 --> 00:26:56.960]   But you can't.
[00:26:56.960 --> 00:26:57.960]   Yes.
[00:26:57.960 --> 00:27:01.960]   The tech community can't be expected to fix this, fix this unilaterally.
[00:27:01.960 --> 00:27:06.200]   The best thing would be for your, I mean, you, you're right in the middle of it.
[00:27:06.200 --> 00:27:09.280]   You have a seven year old or eight year old and you're, and you're dealing with this
[00:27:09.280 --> 00:27:10.280]   right now.
[00:27:10.280 --> 00:27:12.400]   I have a 12 year old 12 year old.
[00:27:12.400 --> 00:27:13.800]   Yes, she is.
[00:27:13.800 --> 00:27:15.440]   But I am teaching her to be more literate.
[00:27:15.440 --> 00:27:22.280]   I will say though, that there are very insidious things that the tech companies do for engagement
[00:27:22.280 --> 00:27:25.400]   purposes that make it hard.
[00:27:25.400 --> 00:27:27.400]   It's not just our patterns.
[00:27:27.400 --> 00:27:35.720]   It's also, I mean, as educated as I might make my daughter, she's still 12, right?
[00:27:35.720 --> 00:27:41.280]   And up until she's probably 20 something, she's going to have some certain thought patterns
[00:27:41.280 --> 00:27:45.040]   that are, you know, just the same as young people everywhere.
[00:27:45.040 --> 00:27:48.320]   She's going to tend towards, yeah, she's going to tend towards extreme.
[00:27:48.320 --> 00:27:52.280]   She's going to see the world much more in black and white.
[00:27:52.280 --> 00:27:54.040]   And you know, and these aren't bad things.
[00:27:54.040 --> 00:27:59.240]   These are good things in some cases, but it also means that people can take advantage
[00:27:59.240 --> 00:28:04.160]   of that and it's very easy to harness a lot of, take advantage of that and then direct
[00:28:04.160 --> 00:28:07.520]   that through the internet for good or ill.
[00:28:07.520 --> 00:28:13.600]   I mean, that's, that's kind of why Obama's campaign way back in 2008 was so successful
[00:28:13.600 --> 00:28:18.320]   because it harnessed some of that youthful idealism and directed action.
[00:28:18.320 --> 00:28:25.240]   Did you read that article about the, but written by the mother whose team became radicalized
[00:28:25.240 --> 00:28:26.240]   to YouTube?
[00:28:26.240 --> 00:28:27.240]   Wasn't that funny?
[00:28:27.240 --> 00:28:28.240]   Yes, that was horrifying.
[00:28:28.240 --> 00:28:32.760]   I had a happy ending, but yeah, horrible.
[00:28:32.760 --> 00:28:35.080]   So Google has a deal with say it earlier.
[00:28:35.080 --> 00:28:36.880]   Sorry, go ahead, Jeff.
[00:28:36.880 --> 00:28:43.040]   Well, if we go back to our youth, us two old white guys here, Matthew, we were Canadian,
[00:28:43.040 --> 00:28:44.040]   so we were going to run to you.
[00:28:44.040 --> 00:28:46.480]   Oh, I was like, there were three of y'all.
[00:28:46.480 --> 00:28:47.480]   I'm pretty young.
[00:28:47.480 --> 00:28:49.720]   I'm leaving him off.
[00:28:49.720 --> 00:28:55.240]   But I mentioned the novel, "Reveluesturies," which I finished recently too, and it goes
[00:28:55.240 --> 00:28:57.560]   through the downfall of the '60s, right?
[00:28:57.560 --> 00:29:02.760]   The kind of, the good of the '60s then, the worse the '60s, then '60s becoming a parody
[00:29:02.760 --> 00:29:04.320]   of itself.
[00:29:04.320 --> 00:29:09.640]   And yeah, when we were young, we were that mob and we had a good cause trying to get
[00:29:09.640 --> 00:29:18.280]   out of the Vietnam War and equal rights, civil rights, but yeah, Stacy, we were like that
[00:29:18.280 --> 00:29:19.280]   too.
[00:29:19.280 --> 00:29:25.480]   Yeah, I mean, it's not a bit, it's just the way your brain works at that age and the
[00:29:25.480 --> 00:29:28.440]   lack of experience.
[00:29:28.440 --> 00:29:33.320]   So Google has a digital safety and citizenship curriculum.
[00:29:33.320 --> 00:29:35.120]   They call it B, Internet Awesome.
[00:29:35.120 --> 00:29:39.560]   I'm not sure what age this is aimed at, but I would guess middle school.
[00:29:39.560 --> 00:29:43.040]   I think about 95, this is what it should be enough.
[00:29:43.040 --> 00:29:44.920]   Smart, alert, strong kind, brave.
[00:29:44.920 --> 00:29:46.440]   This is the newest part.
[00:29:46.440 --> 00:29:49.280]   This just came out this month.
[00:29:49.280 --> 00:29:52.880]   This is a collaboration between Google, the Net Safety Collaborative, and the Internet
[00:29:52.880 --> 00:29:55.720]   Keep Safe Coalition.
[00:29:55.720 --> 00:30:00.400]   So the idea here is digital citizenship and safety.
[00:30:00.400 --> 00:30:01.720]   The Internet Code of Awesome.
[00:30:01.720 --> 00:30:03.680]   This is according to Google.
[00:30:03.680 --> 00:30:04.680]   I'll read it to you.
[00:30:04.680 --> 00:30:06.360]   This is aimed at middle schoolers.
[00:30:06.360 --> 00:30:08.040]   Share with care.
[00:30:08.040 --> 00:30:09.840]   Be Internet Smart.
[00:30:09.840 --> 00:30:11.800]   Don't fall for fake.
[00:30:11.800 --> 00:30:13.840]   Be Internet alert.
[00:30:13.840 --> 00:30:15.360]   Secure your secrets.
[00:30:15.360 --> 00:30:17.560]   Be Internet strong.
[00:30:17.560 --> 00:30:19.640]   It's cool to be kind.
[00:30:19.640 --> 00:30:21.400]   Be Internet kind.
[00:30:21.400 --> 00:30:23.240]   When in doubt, talk it out.
[00:30:23.240 --> 00:30:24.240]   Be Internet brave.
[00:30:24.240 --> 00:30:30.160]   Those seem like good precepts, especially for younger kids, kids your daughter's age.
[00:30:30.160 --> 00:30:34.920]   Stacy, what do you think your daughter's view of that would be?
[00:30:34.920 --> 00:30:37.080]   She does a lot of this already.
[00:30:37.080 --> 00:30:41.480]   So they've had training on this.
[00:30:41.480 --> 00:30:45.720]   I will say it's a little fake.
[00:30:45.720 --> 00:30:46.720]   Maybe?
[00:30:46.720 --> 00:30:47.720]   Well, no, no.
[00:30:47.720 --> 00:30:51.040]   That's just the table of contents.
[00:30:51.040 --> 00:30:52.880]   There are many activities.
[00:30:52.880 --> 00:30:53.880]   There's a lot.
[00:30:53.880 --> 00:30:58.720]   But let's go to the new one, which is don't fall for fake because this is...
[00:30:58.720 --> 00:31:01.320]   I think that could actually be valuable.
[00:31:01.320 --> 00:31:02.320]   Yeah.
[00:31:02.320 --> 00:31:03.720]   A rest of it I think is pretty...
[00:31:03.720 --> 00:31:05.320]   Remember, I'm all for it.
[00:31:05.320 --> 00:31:06.320]   I'll tell you why.
[00:31:06.320 --> 00:31:11.000]   I think that parents and educators don't have a guideline for this.
[00:31:11.000 --> 00:31:12.000]   And so it...
[00:31:12.000 --> 00:31:14.760]   Okay, maybe it's not perfect, but this gives them...
[00:31:14.760 --> 00:31:17.040]   I don't disagree with anything that's in here.
[00:31:17.040 --> 00:31:21.680]   And it gives them a good starting point for this conversation with kids.
[00:31:21.680 --> 00:31:22.680]   And I'm really...
[00:31:22.680 --> 00:31:23.680]   Let's...
[00:31:23.680 --> 00:31:24.680]   This is the new part here.
[00:31:24.680 --> 00:31:28.760]   We'll go right to it, which is don't fall for fake.
[00:31:28.760 --> 00:31:32.960]   And this is designed to teach media literacy with kids.
[00:31:32.960 --> 00:31:33.960]   Let me see if I can...
[00:31:33.960 --> 00:31:35.560]   Let's share with care.
[00:31:35.560 --> 00:31:38.160]   This is all available online for educators.
[00:31:38.160 --> 00:31:41.480]   And I can tell you what she's already been doing.
[00:31:41.480 --> 00:31:45.480]   Like her teachers have done at least for the...
[00:31:45.480 --> 00:31:51.920]   Since she was in fourth or even maybe third grade, she's been doing digital literacy online.
[00:31:51.920 --> 00:31:52.920]   Good.
[00:31:52.920 --> 00:31:56.720]   And they even had me come as a journalist and talk to them about how I find this...
[00:31:56.720 --> 00:31:57.720]   Oh, good.
[00:31:57.720 --> 00:31:59.720]   Is that a public school or private school?
[00:31:59.720 --> 00:32:01.840]   Oh, there was a private school.
[00:32:01.840 --> 00:32:05.480]   I wonder what kind of education they were getting in the public schools.
[00:32:05.480 --> 00:32:07.240]   In Austin, I don't know.
[00:32:07.240 --> 00:32:09.000]   Well, it's Texas.
[00:32:09.000 --> 00:32:10.000]   So...
[00:32:10.000 --> 00:32:12.680]   All right, let's see if you can...
[00:32:12.680 --> 00:32:16.760]   Let's see if you can do the lesson to be internet alert.
[00:32:16.760 --> 00:32:18.520]   Don't fall for fake.
[00:32:18.520 --> 00:32:22.000]   Staying away from fishing and scams.
[00:32:22.000 --> 00:32:23.400]   That's kind of an interesting.
[00:32:23.400 --> 00:32:27.160]   The themes, it's important for kids to understand the content they find online.
[00:32:27.160 --> 00:32:33.280]   Is it necessarily true or reliable and could involve malicious efforts to steal their information
[00:32:33.280 --> 00:32:34.280]   or identity?
[00:32:34.280 --> 00:32:38.040]   The focus I'd want to take on this.
[00:32:38.040 --> 00:32:39.040]   But...
[00:32:39.040 --> 00:32:41.400]   Did you speaking of fishing?
[00:32:41.400 --> 00:32:44.960]   Did you see that horrible story from Alaska?
[00:32:44.960 --> 00:32:51.280]   A teen killed her friend because she was someone online told her they'd pay her millions
[00:32:51.280 --> 00:32:52.280]   of dollars or she...
[00:32:52.280 --> 00:32:56.400]   So this is what you want to avoid, right?
[00:32:56.400 --> 00:33:00.520]   So there's catfishing, there's clickbait, there's spearfishing and fishing.
[00:33:00.520 --> 00:33:03.120]   They define all these terms.
[00:33:03.120 --> 00:33:07.600]   Should we look through and see if we would pass this test?
[00:33:07.600 --> 00:33:10.160]   It has activities.
[00:33:10.160 --> 00:33:12.040]   Is it things to ask?
[00:33:12.040 --> 00:33:13.760]   Is it asking for your personal information?
[00:33:13.760 --> 00:33:16.720]   Is the email offering you something for free?
[00:33:16.720 --> 00:33:22.600]   I think a lot of online games that my daughter plays, they actually do a lot of this training
[00:33:22.600 --> 00:33:23.600]   there.
[00:33:23.600 --> 00:33:26.000]   There are apps designed to do that, aren't there?
[00:33:26.000 --> 00:33:28.800]   Well, no, no, no, it's just as part of like she plays.
[00:33:28.800 --> 00:33:31.640]   I know I've talked to you about Star Stable's "Good Lord."
[00:33:31.640 --> 00:33:35.680]   But she still plays that, but she learned a lot of her chat room etiquette, both I think
[00:33:35.680 --> 00:33:42.160]   in school and maybe a little bit for me and then a ton from both the kids on the forum
[00:33:42.160 --> 00:33:44.800]   and then the forum rules itself.
[00:33:44.800 --> 00:33:45.800]   That's great.
[00:33:45.800 --> 00:33:46.800]   Like, in...
[00:33:46.800 --> 00:33:49.600]   Peer education at that age is the most valuable education, isn't it?
[00:33:49.600 --> 00:33:51.720]   I mean, your peers are who teach you.
[00:33:51.720 --> 00:33:57.520]   Yeah, and I wonder if there's value like when I was in high school, I did a thing called
[00:33:57.520 --> 00:34:01.080]   PALS and it was peer assisted leadership, I think is what it was stood for.
[00:34:01.080 --> 00:34:06.280]   We went to elementary and middle schools and butted up with a kid and we'd hang out with
[00:34:06.280 --> 00:34:10.160]   them for that period once a week.
[00:34:10.160 --> 00:34:17.120]   And I wonder if having like creating some sort of digital PALS program where high schoolers
[00:34:17.120 --> 00:34:24.480]   just hang out and spend a computer period with younger kids and try to explain the world.
[00:34:24.480 --> 00:34:29.080]   Part of it, I think parents are just so afraid of telling their kids the world is an awful
[00:34:29.080 --> 00:34:30.080]   scary bad place.
[00:34:30.080 --> 00:34:32.320]   Yeah, they want to do Santa Claus and the tooth fairy.
[00:34:32.320 --> 00:34:35.360]   Well, lots of them aren't that savvy either.
[00:34:35.360 --> 00:34:40.920]   Yeah, that's why I think curricula like this is great because you don't have to be savvy
[00:34:40.920 --> 00:34:41.920]   to go through this.
[00:34:41.920 --> 00:34:44.320]   I mean, you, in fact, parents could do this with their kids.
[00:34:44.320 --> 00:34:47.440]   They give you fake and real fishing scams.
[00:34:47.440 --> 00:34:48.440]   Is this real?
[00:34:48.440 --> 00:34:49.440]   Who are you?
[00:34:49.440 --> 00:34:51.440]   Is it okay to steal profile photos?
[00:34:51.440 --> 00:34:59.120]   It's definitely good to do it before you get to the sort of teens because I know when I
[00:34:59.120 --> 00:35:04.160]   was a teen, this stuff would have been ridiculed and scorned.
[00:35:04.160 --> 00:35:05.160]   Yeah.
[00:35:05.160 --> 00:35:10.000]   So if you get them earlier than that, everything from an adult is full of teens.
[00:35:10.000 --> 00:35:11.000]   Yeah.
[00:35:11.000 --> 00:35:13.640]   I think if you gave this to a teenager right now, they'd laugh at you.
[00:35:13.640 --> 00:35:14.640]   They know all this stuff.
[00:35:14.640 --> 00:35:16.960]   Like my daughter did this at the age of 10.
[00:35:16.960 --> 00:35:17.960]   Yeah.
[00:35:17.960 --> 00:35:20.680]   I think the most important thing.
[00:35:20.680 --> 00:35:22.440]   Seriously, grandpa, it doesn't.
[00:35:22.440 --> 00:35:24.120]   Yeah, honestly, you're right.
[00:35:24.120 --> 00:35:27.120]   That's what you're saying is this is really for old farts.
[00:35:27.120 --> 00:35:29.200]   The kids are messing up the world.
[00:35:29.200 --> 00:35:30.200]   Yeah.
[00:35:30.200 --> 00:35:33.600]   There was a study I mentioned on the show before that was done on Princeton and NYU that the
[00:35:33.600 --> 00:35:38.760]   90% of a corpus of fake news that they got was being shared by people who looked like
[00:35:38.760 --> 00:35:40.960]   the three of us except you, Stacy.
[00:35:40.960 --> 00:35:43.080]   Well, how about this one?
[00:35:43.080 --> 00:35:48.880]   Here's a story by a guy named Robert Heaton, he's a software engineer.
[00:35:48.880 --> 00:35:54.160]   And he says, "I was seven words away from being spearfished.
[00:35:54.160 --> 00:35:59.240]   He got what seemed to be a real email from the University of Cambridge asking him to
[00:35:59.240 --> 00:36:01.640]   judge the Adam Smith Prize for economics, even though he-
[00:36:01.640 --> 00:36:03.400]   Are you sure your award is real?
[00:36:03.400 --> 00:36:04.400]   Matthew?
[00:36:04.400 --> 00:36:05.400]   Yeah.
[00:36:05.400 --> 00:36:06.400]   Are you positive?
[00:36:06.400 --> 00:36:07.400]   Yeah, I should maybe look into it.
[00:36:07.400 --> 00:36:09.560]   I should point out that Robert is not an economist.
[00:36:09.560 --> 00:36:12.600]   He's a software developer.
[00:36:12.600 --> 00:36:19.320]   He says, "I've read capital in the 21st century, but he wasn't suspicious.
[00:36:19.320 --> 00:36:22.240]   He was suspicious enough to do all the things we would say do.
[00:36:22.240 --> 00:36:27.320]   For instance, he looked at the link in the email, which was to Cambridge University.
[00:36:27.320 --> 00:36:32.280]   In fact, a personal account, it was the address.
[00:36:32.280 --> 00:36:33.480]   He said it destroyed me a little odd.
[00:36:33.480 --> 00:36:39.160]   The page was hosted inside a personal directory, but it was on the Cambridge website.
[00:36:39.160 --> 00:36:42.560]   He visited the route to make sure it really was the domain.
[00:36:42.560 --> 00:36:44.200]   He googled the fella.
[00:36:44.200 --> 00:36:49.680]   He couldn't find much, but he said not everybody has a Twitter profile.
[00:36:49.680 --> 00:36:54.720]   He noticed some grammatical or typos errors, but he did reply.
[00:36:54.720 --> 00:36:56.760]   He said, "Thank you.
[00:36:56.760 --> 00:37:00.280]   I'm certainly interested, but tell me more what would it involve and who recommended
[00:37:00.280 --> 00:37:01.280]   me?"
[00:37:01.280 --> 00:37:03.280]   They went back and forth.
[00:37:03.280 --> 00:37:04.520]   He actually visited the page.
[00:37:04.520 --> 00:37:11.840]   He realized, though, he realized that he was just lucky because the page he visited had
[00:37:11.840 --> 00:37:12.840]   a word.
[00:37:12.840 --> 00:37:16.480]   When he visited it with Firefox, it would have stolen his passwords.
[00:37:16.480 --> 00:37:18.860]   It's an attack we talked about yesterday on security now.
[00:37:18.860 --> 00:37:22.240]   There was a zero day in Firefox.
[00:37:22.240 --> 00:37:24.360]   He fortunately visited on Google Chrome.
[00:37:24.360 --> 00:37:31.180]   The seven words he said, if the page just said, "Must be viewed in Firefox," he might
[00:37:31.180 --> 00:37:32.340]   have launched Firefox.
[00:37:32.340 --> 00:37:33.340]   He had it.
[00:37:33.340 --> 00:37:34.340]   They got crypto.
[00:37:34.340 --> 00:37:35.340]   Yeah.
[00:37:35.340 --> 00:37:36.340]   They got coin.
[00:37:36.340 --> 00:37:41.460]   What happened is they used it to attack Coinbase employees.
[00:37:41.460 --> 00:37:44.780]   Because if you store—this is just a great tip for everybody.
[00:37:44.780 --> 00:37:46.900]   Don't use your browser to store passwords.
[00:37:46.900 --> 00:37:49.260]   Use a password manager.
[00:37:49.260 --> 00:37:55.180]   In this case, Firefox, which is normally a very secure browser, had a flaw that allowed
[00:37:55.180 --> 00:38:01.980]   an attacker on a webpage without your knowledge to steal your passwords, your Firefox passwords.
[00:38:01.980 --> 00:38:03.260]   It was used against Coinbase.
[00:38:03.260 --> 00:38:08.540]   In fact, a lot of crypto was stolen from Coinbase, I believe.
[00:38:08.540 --> 00:38:11.260]   That's what he got bit by.
[00:38:11.260 --> 00:38:18.180]   He said, "In every respect, this looks kind of real, except it wasn't."
[00:38:18.180 --> 00:38:20.380]   That just points out, this is hard stuff.
[00:38:20.380 --> 00:38:21.980]   What did he pull back?
[00:38:21.980 --> 00:38:26.540]   What made him finally pull back and say, "What was the last flaw?"
[00:38:26.540 --> 00:38:29.220]   He didn't pull back.
[00:38:29.220 --> 00:38:31.100]   The point is he did the research.
[00:38:31.100 --> 00:38:34.540]   Had he done it in Firefox, he would have been hacked.
[00:38:34.540 --> 00:38:37.460]   He was lucky he used Chrome.
[00:38:37.460 --> 00:38:40.740]   The next one might be Chrome.
[00:38:40.740 --> 00:38:46.900]   The point is merely that he was lucky and had the attackers just gone one step farther
[00:38:46.900 --> 00:38:49.380]   and said, "Use Firefox to view this."
[00:38:49.380 --> 00:38:53.180]   He thought, "Oh, those guys, you can't figure out how to make it work in Chrome.
[00:38:53.180 --> 00:38:54.180]   Okay."
[00:38:54.180 --> 00:38:55.420]   I think we've talked about this before.
[00:38:55.420 --> 00:39:07.580]   A lot of the most successful fishing and even hacking is just relationship management or
[00:39:07.580 --> 00:39:14.140]   it's getting people to convincing people to give you things, not necessarily zero-day
[00:39:14.140 --> 00:39:15.140]   exploits.
[00:39:15.140 --> 00:39:16.140]   It's all...
[00:39:16.140 --> 00:39:17.140]   Social engineering?
[00:39:17.140 --> 00:39:18.140]   Right.
[00:39:18.140 --> 00:39:19.140]   Exactly.
[00:39:19.140 --> 00:39:20.140]   Social engineering.
[00:39:20.140 --> 00:39:26.060]   But at least I've got half a dozen friends whose parents, elderly parents, have been this
[00:39:26.060 --> 00:39:32.260]   close to sending money to somebody or because they believe the story that somebody told
[00:39:32.260 --> 00:39:33.260]   them.
[00:39:33.260 --> 00:39:34.500]   This is how easy it is.
[00:39:34.500 --> 00:39:35.900]   This is a sophisticated guy.
[00:39:35.900 --> 00:39:37.940]   He's a software engineer.
[00:39:37.940 --> 00:39:39.700]   I'm glad he wrote that.
[00:39:39.700 --> 00:39:41.460]   It's really a generous act for his...
[00:39:41.460 --> 00:39:44.780]   It's not easy or embarrassing to admit that's right.
[00:39:44.780 --> 00:39:45.780]   That's really important.
[00:39:45.780 --> 00:39:51.260]   The reason this worked is because he was flattered that they would ask him to judge the Adam Smith
[00:39:51.260 --> 00:39:53.500]   price for economics even though he's not in the economy.
[00:39:53.500 --> 00:39:56.620]   I'm just sitting here thinking, "I would totally never even fall for that."
[00:39:56.620 --> 00:39:57.860]   I'd be like, "What a..."
[00:39:57.860 --> 00:40:01.100]   No, this is not real because my ego would be like...
[00:40:01.100 --> 00:40:02.100]   This has been around...
[00:40:02.100 --> 00:40:03.100]   Not an economist.
[00:40:03.100 --> 00:40:07.540]   This has been around for decades because I know Jeff, you've received this snail mail
[00:40:07.540 --> 00:40:09.940]   and I have to inviting you to be in Who's Who.
[00:40:09.940 --> 00:40:10.940]   Yeah.
[00:40:10.940 --> 00:40:11.940]   Oh, yeah, the Who's Who piece.
[00:40:11.940 --> 00:40:12.940]   Yeah.
[00:40:12.940 --> 00:40:14.700]   You have to buy it.
[00:40:14.700 --> 00:40:19.100]   Well, certainly you would want a copy of Who's Who with your name in it.
[00:40:19.100 --> 00:40:22.980]   I think back, because I remember my dad was included in Who's Who.
[00:40:22.980 --> 00:40:25.820]   I'm thinking maybe that was the same scam.
[00:40:25.820 --> 00:40:26.820]   Back in.
[00:40:26.820 --> 00:40:29.620]   I mean, they were doing it when I was in high school.
[00:40:29.620 --> 00:40:33.100]   They were basically people they had.
[00:40:33.100 --> 00:40:36.980]   If you care, Zuck is on stage, I think now, at the Aspen Institute.
[00:40:36.980 --> 00:40:37.980]   Should we pull it up?
[00:40:37.980 --> 00:40:39.300]   I have a Sun State.
[00:40:39.300 --> 00:40:40.300]   I don't know.
[00:40:40.300 --> 00:40:44.180]   I just saw.
[00:40:44.180 --> 00:40:45.860]   What is he talking about?
[00:40:45.860 --> 00:40:47.060]   It's just a new step to share.
[00:40:47.060 --> 00:40:49.060]   It is a prize appearance.
[00:40:49.060 --> 00:40:52.300]   Cass Sunstein is interviewing him.
[00:40:52.300 --> 00:40:55.180]   People complain that it's not a journalist.
[00:40:55.180 --> 00:40:57.460]   I missed your line, Andrew.
[00:40:57.460 --> 00:41:01.740]   I said he announced he was stepping down.
[00:41:01.740 --> 00:41:02.740]   Yeah.
[00:41:02.740 --> 00:41:04.260]   From the stage.
[00:41:04.260 --> 00:41:06.380]   Yeah, it doesn't look like they stream that.
[00:41:06.380 --> 00:41:07.740]   I guess you have to pace already.
[00:41:07.740 --> 00:41:08.740]   Oh, okay.
[00:41:08.740 --> 00:41:09.740]   They do.
[00:41:09.740 --> 00:41:10.740]   Yeah.
[00:41:10.740 --> 00:41:11.740]   It's a pretty expensive one.
[00:41:11.740 --> 00:41:12.740]   Yeah.
[00:41:12.740 --> 00:41:13.740]   It's the Aspen Institute.
[00:41:13.740 --> 00:41:16.980]   We'll read about that in TechCrunch tomorrow.
[00:41:16.980 --> 00:41:20.260]   All right.
[00:41:20.260 --> 00:41:22.060]   Let's take-- oh, we don't need to take a break.
[00:41:22.060 --> 00:41:23.260]   It is streamed if you care.
[00:41:23.260 --> 00:41:24.260]   Oh, it is.
[00:41:24.260 --> 00:41:25.260]   It's on YouTube.
[00:41:25.260 --> 00:41:27.420]   I don't think that the controversial part of this is choice.
[00:41:27.420 --> 00:41:30.940]   I think that the harder part to define is your information.
[00:41:30.940 --> 00:41:36.540]   So I think that, you know, take it in the context of a social system.
[00:41:36.540 --> 00:41:39.740]   You know, one of the things that Facebook does is it shows you your friends' birthdays.
[00:41:39.740 --> 00:41:40.740]   Right?
[00:41:40.740 --> 00:41:42.380]   So it can remind you of your friends' birthdays.
[00:41:42.380 --> 00:41:43.380]   Okay.
[00:41:43.380 --> 00:41:50.020]   So we try to enable a developer ecosystem so that way people can bring their information
[00:41:50.020 --> 00:41:51.020]   out to other developers.
[00:41:51.020 --> 00:41:55.420]   I don't think anyone-- I would be very surprised if anyone in this room disagreed with the
[00:41:55.420 --> 00:42:00.820]   notion that you should be able to take your information from a service to another service.
[00:42:00.820 --> 00:42:01.820]   Right?
[00:42:01.820 --> 00:42:02.820]   I mean, that's non-controversial.
[00:42:02.820 --> 00:42:03.980]   The question is, all right.
[00:42:03.980 --> 00:42:05.380]   I have my friends on Facebook.
[00:42:05.380 --> 00:42:06.660]   You're my friend.
[00:42:06.660 --> 00:42:08.100]   You share with me your birthday.
[00:42:08.100 --> 00:42:09.380]   Facebook reminds me of that.
[00:42:09.380 --> 00:42:14.180]   Am I allowed to take that birthday and put it in my calendar app and should Facebook
[00:42:14.180 --> 00:42:17.580]   enable doing that so that now my other calendar that I use--
[00:42:17.580 --> 00:42:19.780]   It's just reminding people to do what they want.
[00:42:19.780 --> 00:42:22.740]   Is your birthday my information or yours?
[00:42:22.740 --> 00:42:24.420]   Who gets to decide that?
[00:42:24.420 --> 00:42:28.700]   If I want to export my friends' birthdays to a calendar, do I need to ask every single
[00:42:28.700 --> 00:42:30.140]   one of them for their permission?
[00:42:30.140 --> 00:42:33.860]   Because if I do, then that app probably isn't going to get built because that's a lot of
[00:42:33.860 --> 00:42:34.860]   friction.
[00:42:34.860 --> 00:42:38.460]   Or let's talk about another example.
[00:42:38.460 --> 00:42:39.460]   Newsfeed.
[00:42:39.460 --> 00:42:43.620]   One of the things that people have talked about for a while is, hey, wouldn't it be nice if
[00:42:43.620 --> 00:42:49.740]   I could bring my newsfeed, the content that I see for my friends to another app, either
[00:42:49.740 --> 00:42:51.740]   so that there could be competing newsfeeds.
[00:42:51.740 --> 00:42:55.380]   So another company could innovate on top of that.
[00:42:55.380 --> 00:42:57.140]   Or academics could do research on top of that.
[00:42:57.140 --> 00:43:01.660]   I mean, a lot of people want to be able to have people come in for studies and say, all
[00:43:01.660 --> 00:43:03.980]   right, can I see what's in your feed?
[00:43:03.980 --> 00:43:08.420]   And can I pull that data and then do a study on the content that's there?
[00:43:08.420 --> 00:43:10.740]   Including for reasons like the election issues.
[00:43:10.740 --> 00:43:13.260]   There are a lot of academics who want to study this stuff.
[00:43:13.260 --> 00:43:14.260]   Same issue.
[00:43:14.260 --> 00:43:18.220]   The content that you see in your feed, it's photos and links that your friends have shared,
[00:43:18.220 --> 00:43:20.540]   is that your information or theirs?
[00:43:20.540 --> 00:43:25.420]   And if it's theirs, then--and you give them a choice over whether you're allowed to pull
[00:43:25.420 --> 00:43:26.420]   it out.
[00:43:26.420 --> 00:43:32.260]   And let's say we're overwhelmingly successful and say that 90% of people opt in and say,
[00:43:32.260 --> 00:43:35.700]   I'm willing to let my friends share that information.
[00:43:35.700 --> 00:43:38.860]   And you know, are the studies that are done on that?
[00:43:38.860 --> 00:43:39.860]   Going to be valid?
[00:43:39.860 --> 00:43:44.420]   You know, are you really going to be able to build a competitive newsfeed if 10% of your
[00:43:44.420 --> 00:43:47.420]   friends' most interesting information is not there and the only way to get the full
[00:43:47.420 --> 00:43:49.380]   experience is through Facebook?
[00:43:49.380 --> 00:43:52.220]   These are really hard questions, I think.
[00:43:52.220 --> 00:43:57.260]   It's very--I actually come out more on the side of where you are, that this should be
[00:43:57.260 --> 00:43:58.980]   left to individual choice.
[00:43:58.980 --> 00:44:03.020]   But I don't want to leave the impression that these are obvious choices that don't have
[00:44:03.020 --> 00:44:06.820]   competing equities that we both--that we've all valued greatly.
[00:44:06.820 --> 00:44:13.300]   If your paramount concern is around innovation and competition and research and data portability,
[00:44:13.300 --> 00:44:18.380]   and then it's not clear that you want to give every individual the right to say, I don't
[00:44:18.380 --> 00:44:21.820]   want my information to be able to be taken to another service.
[00:44:21.820 --> 00:44:27.060]   So I think that this is very connected to the set of privacy debates and things that
[00:44:27.060 --> 00:44:30.060]   we're having as well.
[00:44:30.060 --> 00:44:34.260]   There's no question in my mind that people need to have the right to be able to say who
[00:44:34.260 --> 00:44:37.100]   has access to their information.
[00:44:37.100 --> 00:44:43.940]   But I do think that as a society, there is--we need to decide where we want to be on the
[00:44:43.940 --> 00:44:48.660]   spectrum between asking companies to fully lock down people's information and on the
[00:44:48.660 --> 00:44:50.700]   other hand making it portable.
[00:44:50.700 --> 00:44:51.700]   Because right now--
[00:44:51.700 --> 00:44:54.900]   That's--I think he's making a reasonable point, I hate to say it.
[00:44:54.900 --> 00:44:59.540]   He is, but he also doesn't let academics access the newsfeed.
[00:44:59.540 --> 00:45:00.540]   Right.
[00:45:00.540 --> 00:45:03.780]   So that's--one of his examples is something that he doesn't actually let--
[00:45:03.780 --> 00:45:04.780]   Right.
[00:45:04.780 --> 00:45:05.780]   It doesn't happen.
[00:45:05.780 --> 00:45:09.260]   Well, they started SSR-T, but they're--yeah, they're--they're--they're done shy because
[00:45:09.260 --> 00:45:10.260]   of Cambridge.
[00:45:10.260 --> 00:45:12.700]   Well, a lot of people would--a lot of people would point out--
[00:45:12.700 --> 00:45:16.300]   They were going to try way prior to--they came down very hard on people--
[00:45:16.300 --> 00:45:17.300]   Yeah, very.
[00:45:17.300 --> 00:45:18.300]   --at 20% of these teams.
[00:45:18.300 --> 00:45:24.700]   No--nobody really is disputing that Facebook guards the information it has about you carefully.
[00:45:24.700 --> 00:45:27.140]   That's frankly a value to them.
[00:45:27.140 --> 00:45:29.700]   That's of financial value.
[00:45:29.700 --> 00:45:31.060]   That's not the problem.
[00:45:31.060 --> 00:45:35.060]   It's what the information--what they do with the information they have about you.
[00:45:35.060 --> 00:45:39.300]   Not what I do with my birthdays or the newsfeed or what even--
[00:45:39.300 --> 00:45:40.300]   Right.
[00:45:40.300 --> 00:45:42.300]   --it's what they do with the information that's problematic.
[00:45:42.300 --> 00:45:46.460]   And it's just--it's in his interest to cheat--to take the most--
[00:45:46.460 --> 00:45:47.980]   They focus--in the focus--
[00:45:47.980 --> 00:45:48.980]   --exampled--
[00:45:48.980 --> 00:45:49.980]   Right.
[00:45:49.980 --> 00:45:50.980]   --and get you--
[00:45:50.980 --> 00:45:51.980]   Of course.
[00:45:51.980 --> 00:45:52.980]   --because of course you would agree.
[00:45:52.980 --> 00:45:53.980]   And they--and they--by the way, it's also in his interest.
[00:45:53.980 --> 00:45:55.220]   And this is something everybody does now.
[00:45:55.220 --> 00:45:57.300]   You say, "Oh no, we protect your data carefully.
[00:45:57.300 --> 00:45:58.860]   We'd work really hard to protect your data."
[00:45:58.860 --> 00:46:01.620]   They do because--because they don't want--
[00:46:01.620 --> 00:46:02.620]   Because they want it.
[00:46:02.620 --> 00:46:04.900]   A competitor to get it.
[00:46:04.900 --> 00:46:05.900]   They do protect it.
[00:46:05.900 --> 00:46:10.340]   Nobody's disputing that Facebook--whatever Facebook knows about you, it keeps to itself.
[00:46:10.340 --> 00:46:16.220]   Well, and it reminds me of those congressional hearings when everybody focused on, you know,
[00:46:16.220 --> 00:46:17.460]   why do you sell data?
[00:46:17.460 --> 00:46:18.780]   You shouldn't be allowed to sell data.
[00:46:18.780 --> 00:46:21.900]   And that was an easy argument to refute because they don't sell data.
[00:46:21.900 --> 00:46:22.900]   So I'll mark how to sell--
[00:46:22.900 --> 00:46:23.900]   The sell access to you.
[00:46:23.900 --> 00:46:24.900]   --the sell access to you.
[00:46:24.900 --> 00:46:25.900]   Right.
[00:46:25.900 --> 00:46:26.900]   We give it away.
[00:46:26.900 --> 00:46:29.780]   And then we'll give us money in exchange for what we can do with it.
[00:46:29.780 --> 00:46:34.020]   But they don't--I mean, if you go to Facebook and say, "I want to know how will Leo support?"
[00:46:34.020 --> 00:46:37.940]   Is you can't--as an advertiser, you can't get that information.
[00:46:37.940 --> 00:46:41.300]   You can say, "I want everybody between 45 and 55."
[00:46:41.300 --> 00:46:42.300]   I want a real white man.
[00:46:42.300 --> 00:46:43.300]   Yeah.
[00:46:43.300 --> 00:46:44.300]   Yeah.
[00:46:44.300 --> 00:46:45.300]   Well, you can't do it in an aggregate.
[00:46:45.300 --> 00:46:46.300]   Because they don't want it.
[00:46:46.300 --> 00:46:49.900]   Yeah, they don't--but they--they wouldn't want to give my information to some other person
[00:46:49.900 --> 00:46:52.860]   because somebody could then compile a database and sell against them.
[00:46:52.860 --> 00:46:57.460]   They keep--all these companies keep this private to themselves unless they can sell the data
[00:46:57.460 --> 00:47:01.260]   in aggregate and usually that's for advertising purposes.
[00:47:01.260 --> 00:47:02.260]   So--
[00:47:02.260 --> 00:47:04.540]   The state--stacy, you're quite right about the research.
[00:47:04.540 --> 00:47:07.180]   It's been a huge frustration for researchers who--
[00:47:07.180 --> 00:47:08.180]   They love this information.
[00:47:08.180 --> 00:47:12.020]   --want to be able to find out the impact.
[00:47:12.020 --> 00:47:15.580]   And Alex Stamos is--Stamos is really articulate about this.
[00:47:15.580 --> 00:47:17.580]   That's about saying, "Okay, well, here's where there's--"
[00:47:17.580 --> 00:47:18.580]   Sorry.
[00:47:18.580 --> 00:47:20.020]   Now my computer's making too much time.
[00:47:20.020 --> 00:47:21.020]   Go ahead.
[00:47:21.020 --> 00:47:22.020]   I've muted it.
[00:47:22.020 --> 00:47:23.020]   I'm done.
[00:47:23.020 --> 00:47:30.340]   I--I--Stacy, do you want to respond?
[00:47:30.340 --> 00:47:31.660]   He's just saying you're right.
[00:47:31.660 --> 00:47:32.660]   Wait, what--I'm sorry.
[00:47:32.660 --> 00:47:33.660]   I can't hear--
[00:47:33.660 --> 00:47:34.660]   You're right.
[00:47:34.660 --> 00:47:35.660]   Just say thank you.
[00:47:35.660 --> 00:47:36.660]   I was saying you're right.
[00:47:36.660 --> 00:47:37.660]   Thank you.
[00:47:37.660 --> 00:47:38.660]   Oh, no, Jeff.
[00:47:38.660 --> 00:47:39.660]   This is terrible.
[00:47:39.660 --> 00:47:40.660]   This can't go on.
[00:47:40.660 --> 00:47:43.580]   This show is predicated on us disagreeing.
[00:47:43.580 --> 00:47:45.140]   Come on, man.
[00:47:45.140 --> 00:47:46.140]   No.
[00:47:46.140 --> 00:47:47.140]   No.
[00:47:47.140 --> 00:47:48.140]   Oh, but it's more fun.
[00:47:48.140 --> 00:47:49.140]   No.
[00:47:49.140 --> 00:47:50.140]   What is that?
[00:47:50.140 --> 00:47:51.140]   And I'm feeling feisty or too.
[00:47:51.140 --> 00:47:52.140]   I'm feeling feisty or too.
[00:47:52.140 --> 00:47:53.140]   You're wrong about that.
[00:47:53.140 --> 00:47:54.140]   No.
[00:47:54.140 --> 00:47:55.140]   You're wrong, Stacy.
[00:47:55.140 --> 00:47:56.140]   Are you ready to engage Stacy?
[00:47:56.140 --> 00:47:57.140]   Are you going to--
[00:47:57.140 --> 00:48:01.780]   So Stacy just said Stacy just disagreed that it's good to agree.
[00:48:01.780 --> 00:48:02.780]   So there we are.
[00:48:02.780 --> 00:48:04.780]   Well, who's right?
[00:48:04.780 --> 00:48:05.780]   Just not that.
[00:48:05.780 --> 00:48:08.500]   I'm just having fun now.
[00:48:08.500 --> 00:48:09.500]   So--
[00:48:09.500 --> 00:48:11.500]   That's what Stacy called.
[00:48:11.500 --> 00:48:12.500]   Come on.
[00:48:12.500 --> 00:48:13.500]   See, we got it.
[00:48:13.500 --> 00:48:14.500]   Yeah, Stacy, I got it.
[00:48:14.500 --> 00:48:15.500]   Yeah.
[00:48:15.500 --> 00:48:21.060]   A new bipartisan bill, Mark Warner from Virginia and Josh Hawley, our favorite Republican
[00:48:21.060 --> 00:48:22.060]   from Missouri.
[00:48:22.060 --> 00:48:23.060]   Great law.
[00:48:23.060 --> 00:48:25.060]   The Dashboard Act.
[00:48:25.060 --> 00:48:28.980]   Stupidest idea for legislation ever made.
[00:48:28.980 --> 00:48:33.980]   By the way, an acronym, maybe a retronym, designing accounting safeguards to help broader
[00:48:33.980 --> 00:48:37.020]   oversight and regulations on data.
[00:48:37.020 --> 00:48:38.020]   Dashboard.
[00:48:38.020 --> 00:48:47.140]   The idea is consumers should know what their information is worth to Google, to Facebook,
[00:48:47.140 --> 00:48:48.140]   to Amazon.
[00:48:48.140 --> 00:48:49.940]   Sounds great, doesn't it?
[00:48:49.940 --> 00:48:51.940]   Yeah, what can we run with that for this health?
[00:48:51.940 --> 00:48:52.940]   Wait, what about--
[00:48:52.940 --> 00:48:54.660]   What could go wrong?
[00:48:54.660 --> 00:48:57.460]   What about-- you know how we talked last week about ambient privacy?
[00:48:57.460 --> 00:48:58.460]   Yeah.
[00:48:58.460 --> 00:49:03.620]   I think sometimes you can't sell your information without outing other people, right?
[00:49:03.620 --> 00:49:05.260]   Or you can't--
[00:49:05.260 --> 00:49:06.260]   Right.
[00:49:06.260 --> 00:49:09.500]   And there's a lot here.
[00:49:09.500 --> 00:49:15.260]   I don't think you have-- you shouldn't be allowed to sign away your data rights or
[00:49:15.260 --> 00:49:19.900]   your privacy rights for money because that puts the onus on people who don't have--
[00:49:19.900 --> 00:49:23.140]   So they're not saying here, we're going to give you money for it.
[00:49:23.140 --> 00:49:25.140]   They're saying under this act, companies are required.
[00:49:25.140 --> 00:49:31.860]   I know-- I think this sounds like GDPR to disclose what kind of data they collect, the
[00:49:31.860 --> 00:49:33.060]   value of that data.
[00:49:33.060 --> 00:49:34.860]   Okay, we could say, you know, what--
[00:49:34.860 --> 00:49:35.860]   That's the funniest.
[00:49:35.860 --> 00:49:36.860]   That's silly.
[00:49:36.860 --> 00:49:37.860]   And how the data is used.
[00:49:37.860 --> 00:49:39.020]   So we'll just take that value--
[00:49:39.020 --> 00:49:42.860]   Well, 500.1 and 3.2 is what's got the headlines though.
[00:49:42.860 --> 00:49:44.700]   And point two is the stupid part.
[00:49:44.700 --> 00:49:45.700]   Yeah, we'll take that.
[00:49:45.700 --> 00:49:47.700]   I mean, transparency is good.
[00:49:47.700 --> 00:49:48.700]   Transparency, although--
[00:49:48.700 --> 00:49:54.620]   They would have to file an annual report disclosing the harvested user's data aggregate
[00:49:54.620 --> 00:49:58.580]   value along with any third party contracts, regarding such information.
[00:49:58.580 --> 00:50:01.420]   Again, this is all a red herring, which by the way--
[00:50:01.420 --> 00:50:05.060]   It's a real-- I think you have to reveal the value of every viewer.
[00:50:05.060 --> 00:50:06.060]   Yeah, yeah.
[00:50:06.060 --> 00:50:08.260]   Well, I could do a math calculation, say.
[00:50:08.260 --> 00:50:09.980]   But you don't want to do that.
[00:50:09.980 --> 00:50:10.980]   You don't want to do that.
[00:50:10.980 --> 00:50:11.980]   No, I could gladly do that.
[00:50:11.980 --> 00:50:14.860]   No, in fact, I know what the value of every viewer is because that's how we charge for
[00:50:14.860 --> 00:50:15.860]   advertising.
[00:50:15.860 --> 00:50:16.860]   I don't want to--
[00:50:16.860 --> 00:50:17.860]   It's like you all are priceless.
[00:50:17.860 --> 00:50:18.860]   It's what--
[00:50:18.860 --> 00:50:24.340]   We charge $90 per thousand listeners or viewers.
[00:50:24.340 --> 00:50:28.420]   So that means you're each worth about a nickel or a dime.
[00:50:28.420 --> 00:50:30.020]   I don't know.
[00:50:30.020 --> 00:50:31.980]   Yeah, money, man.
[00:50:31.980 --> 00:50:32.980]   Nice.
[00:50:32.980 --> 00:50:35.420]   Money makes that felt good.
[00:50:35.420 --> 00:50:38.500]   But so all you know, that value thing-- this is ridiculous.
[00:50:38.500 --> 00:50:41.940]   Data's aggregate value, third party contracts regarding such information.
[00:50:41.940 --> 00:50:44.580]   Well, we already know that that doesn't work.
[00:50:44.580 --> 00:50:48.660]   But this is important, give users the ability to delete all our specific portions of their
[00:50:48.660 --> 00:50:50.660]   data.
[00:50:50.660 --> 00:50:52.300]   That this is like GDPR.
[00:50:52.300 --> 00:50:54.300]   What data you collect, how the data is used--
[00:50:54.300 --> 00:50:55.900]   Go to the number you just said.
[00:50:55.900 --> 00:50:58.500]   I tell you it's my birthday.
[00:50:58.500 --> 00:51:00.620]   You say congratulations, Jeff.
[00:51:00.620 --> 00:51:01.620]   It's Jeff's birthday.
[00:51:01.620 --> 00:51:03.300]   You make a post out of that.
[00:51:03.300 --> 00:51:05.940]   Do I have the freedom to make you take that down?
[00:51:05.940 --> 00:51:07.620]   That's the kind of question you get into.
[00:51:07.620 --> 00:51:11.620]   And Facebook has intentionally intermingled all of this to make it hard.
[00:51:11.620 --> 00:51:14.100]   In fact, when you delete your Facebook account--
[00:51:14.100 --> 00:51:15.100]   Oh, the reality.
[00:51:15.100 --> 00:51:18.940]   When you delete your Facebook account, they even say, well, we're going to delete everything
[00:51:18.940 --> 00:51:22.220]   you-- in your account, but there may be pictures of you.
[00:51:22.220 --> 00:51:25.900]   There may be stuff that people have saved that you've said we can't do anything with
[00:51:25.900 --> 00:51:26.900]   that.
[00:51:26.900 --> 00:51:27.900]   And of course you can.
[00:51:27.900 --> 00:51:28.900]   I wouldn't expect that.
[00:51:28.900 --> 00:51:29.900]   I can't--
[00:51:29.900 --> 00:51:30.900]   I can't really--
[00:51:30.900 --> 00:51:36.940]   I guarantee if they implemented some of the deletion or some of the other points that
[00:51:36.940 --> 00:51:41.140]   they mentioned in there, it would be buried 15 sub-menus deep.
[00:51:41.140 --> 00:51:42.140]   Right.
[00:51:42.140 --> 00:51:45.980]   On a page that would be incomprehensible and no one would ever do it.
[00:51:45.980 --> 00:51:53.860]   They also said that the additional transparency would encourage market competition and allow
[00:51:53.860 --> 00:51:57.860]   antitrust enforcers to identify potentially anti-competitive practices.
[00:51:57.860 --> 00:52:00.140]   That might be the reason for the dollar amount.
[00:52:00.140 --> 00:52:01.660]   Maybe we're not senators.
[00:52:01.660 --> 00:52:02.660]   Maybe there's some technical--
[00:52:02.660 --> 00:52:03.660]   No.
[00:52:03.660 --> 00:52:11.100]   The reason for the dollar amount is it got a pet alliance and it worked.
[00:52:11.100 --> 00:52:13.100]   You guys could be cynical, so can I.
[00:52:13.100 --> 00:52:16.060]   Well, it may not just be for headlines.
[00:52:16.060 --> 00:52:19.740]   It could also be like, hey, each person is really only $0.10 for us.
[00:52:19.740 --> 00:52:20.980]   This isn't worth it.
[00:52:20.980 --> 00:52:25.420]   You could make a lot of arguments about, look, this is not-- we measure most of our harms
[00:52:25.420 --> 00:52:28.500]   today in economics, right, in dollars.
[00:52:28.500 --> 00:52:33.100]   So if we can put a value on the data or a value on the harm it might cause, then we can
[00:52:33.100 --> 00:52:34.100]   actually--
[00:52:34.100 --> 00:52:35.100]   Facebook can make the same--
[00:52:35.100 --> 00:52:36.100]   Our courts can deal with it.
[00:52:36.100 --> 00:52:37.580]   Facebook can make the same calculus.
[00:52:37.580 --> 00:52:39.580]   I just did.
[00:52:39.580 --> 00:52:42.140]   If you divide their revenue by the number of users--
[00:52:42.140 --> 00:52:43.140]   Oh, sure.
[00:52:43.140 --> 00:52:45.340]   $6.42 per user worldwide.
[00:52:45.340 --> 00:52:46.820]   That's some of the users--
[00:52:46.820 --> 00:52:48.660]   $30 per user in the Canada and US.
[00:52:48.660 --> 00:52:51.500]   Yeah, in the US and Canada, you're worth more.
[00:52:51.500 --> 00:52:52.500]   $30.
[00:52:52.500 --> 00:52:53.500]   That's all--
[00:52:53.500 --> 00:52:57.020]   Google, you can't really do that because you've got other services.
[00:52:57.020 --> 00:52:58.020]   Right.
[00:52:58.020 --> 00:53:00.620]   Amazon, you've got other services like AWS and things.
[00:53:00.620 --> 00:53:01.620]   Yeah.
[00:53:01.620 --> 00:53:05.100]   By the way, MarketWatch is glad to sit-- just a factoid.
[00:53:05.100 --> 00:53:09.540]   2.5 quintillion bytes of data are created every day.
[00:53:09.540 --> 00:53:14.780]   If you laid 2.5 quintillion pennies out flat, they would cover the earth five times.
[00:53:14.780 --> 00:53:15.780]   That was useful.
[00:53:15.780 --> 00:53:16.780]   Okay.
[00:53:16.780 --> 00:53:17.780]   That's handy.
[00:53:17.780 --> 00:53:24.180]   This is why Jeff teaches journalism students about data journalism.
[00:53:24.180 --> 00:53:26.180]   This is not data journalism.
[00:53:26.180 --> 00:53:27.180]   It's not used.
[00:53:27.180 --> 00:53:31.820]   I'm disappointed they didn't use the international measuring system, which is the football field.
[00:53:31.820 --> 00:53:36.900]   I'm surprised it didn't show us how many library of Congress it would--
[00:53:36.900 --> 00:53:44.540]   It just cracks me up that they decided to illustrate the fact that 2.5 quintillion bytes
[00:53:44.540 --> 00:53:49.580]   of data are created every day by saying if those were pennies, they would cover the earth
[00:53:49.580 --> 00:53:50.580]   five times.
[00:53:50.580 --> 00:53:52.380]   It just cracks me up.
[00:53:52.380 --> 00:53:54.780]   I didn't even say what a quintillion went once.
[00:53:54.780 --> 00:53:57.740]   They probably didn't know.
[00:53:57.740 --> 00:54:01.780]   Sometimes I'll be honest, when I'm looking up some of these big data numbers, I'm like,
[00:54:01.780 --> 00:54:02.780]   "Oh, God.
[00:54:02.780 --> 00:54:05.060]   How many zeros is that?"
[00:54:05.060 --> 00:54:08.020]   They must have known because that's a lot of calculations.
[00:54:08.020 --> 00:54:11.700]   You got to divide the total surface area of the globe.
[00:54:11.700 --> 00:54:12.700]   Oh, no.
[00:54:12.700 --> 00:54:17.260]   Dude, that was in a press release somewhere and they just cut and wasted.
[00:54:17.260 --> 00:54:18.260]   Sure.
[00:54:18.260 --> 00:54:19.260]   Exactly.
[00:54:19.260 --> 00:54:21.260]   Would somebody please check that for us?
[00:54:21.260 --> 00:54:24.180]   What is the area of a penny?
[00:54:24.180 --> 00:54:27.860]   What is the-- you'd have to do it in squares even though a penny is round.
[00:54:27.860 --> 00:54:28.860]   The earth.
[00:54:28.860 --> 00:54:35.020]   What is the total surface square footage of the earth or square penny of the earth?
[00:54:35.020 --> 00:54:36.020]   And then--
[00:54:36.020 --> 00:54:42.900]   However, in Canada, you couldn't do that because I learned today, I paid for my latte, $6.03
[00:54:42.900 --> 00:54:46.340]   with $0.03 and the woman said, "Oh, you're using pennies.
[00:54:46.340 --> 00:54:47.980]   We don't use pennies anymore."
[00:54:47.980 --> 00:54:48.980]   Yeah.
[00:54:48.980 --> 00:54:49.980]   What?
[00:54:49.980 --> 00:54:50.980]   Explain this to me.
[00:54:50.980 --> 00:54:56.420]   The surface area of the earth is 5.49 times 10 to the 15th square feet.
[00:54:56.420 --> 00:54:59.940]   So we're going to help you a little bit.
[00:54:59.940 --> 00:55:06.500]   Now Google, what is the surface area of a penny?
[00:55:06.500 --> 00:55:08.780]   I bet Wolfram Alphra could do this calculation.
[00:55:08.780 --> 00:55:09.780]   Sure, the chart really--
[00:55:09.780 --> 00:55:11.260]   Oh, yeah.
[00:55:11.260 --> 00:55:15.620]   The surface area of a penny.
[00:55:15.620 --> 00:55:16.780]   Well, wait a minute.
[00:55:16.780 --> 00:55:18.340]   Front and back?
[00:55:18.340 --> 00:55:19.340]   No.
[00:55:19.340 --> 00:55:20.340]   No.
[00:55:20.340 --> 00:55:24.100]   It is .442 square inches.
[00:55:24.100 --> 00:55:26.300]   Is the surface area or depth?
[00:55:26.300 --> 00:55:30.340]   Were they stacked or they laid side to side?
[00:55:30.340 --> 00:55:31.340]   No, they had side to side.
[00:55:31.340 --> 00:55:32.340]   Oh, my God.
[00:55:32.340 --> 00:55:34.500]   Well, there's some of them are stacked because it's 5 times over.
[00:55:34.500 --> 00:55:37.140]   If it was going to the moon, you would stack them.
[00:55:37.140 --> 00:55:40.420]   It is a quarter of the area of a human retina.
[00:55:40.420 --> 00:55:45.220]   It is a 60% of a typical postage stamp.
[00:55:45.220 --> 00:55:51.620]   So I'm sure if I do a surface area of the penny cover--
[00:55:51.620 --> 00:55:52.620]   I mean, how many--
[00:55:52.620 --> 00:55:53.780]   Let's just ask it.
[00:55:53.780 --> 00:56:00.700]   How many pennies would it take to cover the earth?
[00:56:00.700 --> 00:56:03.020]   Wolfram Alphra is amazing on stuff like this.
[00:56:03.020 --> 00:56:07.220]   Oh, there you go.
[00:56:07.220 --> 00:56:11.380]   1.4 times 10 to the 18th.
[00:56:11.380 --> 00:56:12.620]   So it's done.
[00:56:12.620 --> 00:56:14.700]   That's it Wolfram Alphra.
[00:56:14.700 --> 00:56:16.140]   How many is in a quintillion?
[00:56:16.140 --> 00:56:16.900]   It's about the same number.
[00:56:16.900 --> 00:56:18.380]   Yeah, so now we just have to define
[00:56:18.380 --> 00:56:20.300]   about what a quintillion is.
[00:56:20.300 --> 00:56:22.180]   What about going up and down mountains?
[00:56:22.180 --> 00:56:23.380]   Do pennies float?
[00:56:23.380 --> 00:56:26.740]   I think the number is actually wrong.
[00:56:26.740 --> 00:56:27.980]   I think that their numbers are wrong.
[00:56:27.980 --> 00:56:29.420]   Their numbers are wrong.
[00:56:29.420 --> 00:56:32.300]   In fact, a quintillion-- what is it?
[00:56:32.300 --> 00:56:33.540]   2 and 1/2 quintillion for an hour?
[00:56:33.540 --> 00:56:34.500]   It's 10 to the watch.
[00:56:34.500 --> 00:56:38.260]   2 and 1/2 quintillion pennies would barely cover a quarter
[00:56:38.260 --> 00:56:39.860]   of the earth.
[00:56:39.860 --> 00:56:41.220]   OK, a quintillion is 10 of the--
[00:56:41.220 --> 00:56:41.980]   That was a scandal.
[00:56:41.980 --> 00:56:43.940]   10 of the 18th?
[00:56:43.940 --> 00:56:46.700]   What was the original stat that we're trying to get?
[00:56:46.700 --> 00:56:47.540]   I forgot.
[00:56:47.540 --> 00:56:48.460]   2.5 quintillion.
[00:56:48.460 --> 00:56:53.340]   OK, so a quintillion-- so 2.5 quintillion.
[00:56:53.340 --> 00:56:55.860]   No, it would cover the earth twice.
[00:56:55.860 --> 00:56:56.940]   So you're wrong.
[00:56:56.940 --> 00:56:58.940]   You're wrong, Mark, you watch.
[00:56:58.940 --> 00:57:00.260]   It said five times.
[00:57:00.260 --> 00:57:01.900]   They said five times.
[00:57:01.900 --> 00:57:03.860]   Pretty sure they just made it up, because who's going to say it?
[00:57:03.860 --> 00:57:05.900]   That's because they used all the extra Canadian pennies
[00:57:05.900 --> 00:57:07.020]   nobody's using anymore.
[00:57:07.020 --> 00:57:07.940]   Well, that's a good point.
[00:57:07.940 --> 00:57:08.940]   What kind of penny?
[00:57:08.940 --> 00:57:11.900]   A British penny has a lot more surface area.
[00:57:11.900 --> 00:57:15.020]   You know, it costs, I think, $0.03 to make a penny.
[00:57:15.020 --> 00:57:17.140]   That's why they won't take it.
[00:57:17.140 --> 00:57:20.340]   Matthew, so when my bill was 6.03,
[00:57:20.340 --> 00:57:22.180]   she was so amazed I gave her--
[00:57:22.180 --> 00:57:23.180]   3 pennies.
[00:57:23.180 --> 00:57:24.380]   That's change.
[00:57:24.380 --> 00:57:25.140]   Was it sport?
[00:57:25.140 --> 00:57:27.020]   Do they round up to 6.05?
[00:57:27.020 --> 00:57:27.980]   Yeah.
[00:57:27.980 --> 00:57:28.780]   Usually.
[00:57:28.780 --> 00:57:29.780]   Why did you get pennies?
[00:57:29.780 --> 00:57:32.020]   You paid in dollars at a Canadian place.
[00:57:32.020 --> 00:57:34.700]   I had a bag of Canadian dollars.
[00:57:34.700 --> 00:57:35.220]   Canadian dollars.
[00:57:35.220 --> 00:57:35.940]   Like, carry it with me.
[00:57:35.940 --> 00:57:37.100]   And I had tons of change.
[00:57:37.100 --> 00:57:38.420]   And I had a lot of pennies.
[00:57:38.420 --> 00:57:39.380]   You don't use them.
[00:57:39.380 --> 00:57:40.740]   So I shot you.
[00:57:40.740 --> 00:57:41.580]   I gave her 6.03.
[00:57:41.580 --> 00:57:45.540]   She said, oh, you were the American pennies?
[00:57:45.540 --> 00:57:46.740]   No, they were Canadian pennies.
[00:57:46.740 --> 00:57:47.700]   They were old.
[00:57:47.700 --> 00:57:48.980]   They were vintage Canadian pennies.
[00:57:48.980 --> 00:57:51.780]   I have Canadian pennies at home as well, as a matter of fact.
[00:57:51.780 --> 00:57:52.780]   Yeah.
[00:57:52.780 --> 00:57:53.780]   So did she accept them?
[00:57:53.780 --> 00:57:54.780]   Is it still legal tender?
[00:57:54.780 --> 00:57:55.780]   Yeah, she did.
[00:57:55.780 --> 00:57:56.780]   She said, oh, it's OK.
[00:57:56.780 --> 00:57:57.780]   You're like, you're poor, dumb American.
[00:57:57.780 --> 00:57:58.780]   OK, it's all right.
[00:57:58.780 --> 00:57:59.780]   We have to take it.
[00:57:59.780 --> 00:58:00.780]   Yeah, there's Canadian.
[00:58:00.780 --> 00:58:01.780]   They're going to take it.
[00:58:01.780 --> 00:58:02.780]   Stock up on loonies.
[00:58:02.780 --> 00:58:03.780]   That's the best.
[00:58:03.780 --> 00:58:04.780]   Loonies are the best.
[00:58:04.780 --> 00:58:05.780]   Loonies.
[00:58:05.780 --> 00:58:08.380]   Why try to give a guy a euro instead of a lead today?
[00:58:08.380 --> 00:58:09.380]   And he said, I'll take the euro.
[00:58:09.380 --> 00:58:11.780]   I'll be happy to, but it's not the right one.
[00:58:11.780 --> 00:58:12.780]   Canadians are so nice.
[00:58:12.780 --> 00:58:13.780]   While we're in.
[00:58:13.780 --> 00:58:20.780]   While we're in stupid legislators' ideas, there's another one in the government section.
[00:58:20.780 --> 00:58:21.780]   Tune soon.
[00:58:21.780 --> 00:58:24.780]   Is it the section to a 31?
[00:58:24.780 --> 00:58:25.780]   No, no.
[00:58:25.780 --> 00:58:26.780]   That's another one.
[00:58:26.780 --> 00:58:27.780]   That's dangerous.
[00:58:27.780 --> 00:58:28.780]   That's dangerous.
[00:58:28.780 --> 00:58:33.780]   This one is that he wants Google to require to give you search without an algorithm.
[00:58:33.780 --> 00:58:35.780]   All right, no algorithms.
[00:58:35.780 --> 00:58:36.780]   Yeah.
[00:58:36.780 --> 00:58:37.780]   What is that?
[00:58:37.780 --> 00:58:39.780]   That's like, I'm going to have a trash fire.
[00:58:39.780 --> 00:58:42.780]   I mean, like, it's like going to the dump and asking to see, like, your mail.
[00:58:42.780 --> 00:58:43.780]   Give you the entire screen.
[00:58:43.780 --> 00:58:46.780]   It's going to be out the entire internet.
[00:58:46.780 --> 00:58:47.780]   Good luck.
[00:58:47.780 --> 00:58:51.140]   I think Senator Thune does not know what an algorithm is.
[00:58:51.140 --> 00:58:53.780]   I do not think that word means what you think of this.
[00:58:53.780 --> 00:58:54.780]   Or Google.
[00:58:54.780 --> 00:58:55.780]   That's...
[00:58:55.780 --> 00:58:56.780]   He says...
[00:58:56.780 --> 00:58:57.780]   Hey, buy.
[00:58:57.780 --> 00:58:59.780]   So you want these guys to be running the internet?
[00:58:59.780 --> 00:59:00.780]   No.
[00:59:00.780 --> 00:59:02.780]   Well, wait a minute, though.
[00:59:02.780 --> 00:59:03.780]   There is some reasonable...
[00:59:03.780 --> 00:59:08.700]   So, for instance, in the Facebook news feed, if they would just give me in chronological
[00:59:08.700 --> 00:59:11.780]   order all the shares from the people I follow.
[00:59:11.780 --> 00:59:12.780]   You can have that now.
[00:59:12.780 --> 00:59:13.780]   Yeah, but it turns it off.
[00:59:13.780 --> 00:59:14.780]   Twitter, you can have that now.
[00:59:14.780 --> 00:59:15.780]   Yeah, but it turns it off.
[00:59:15.780 --> 00:59:16.780]   Yeah, you can have it.
[00:59:16.780 --> 00:59:17.780]   They turn it back.
[00:59:17.780 --> 00:59:18.780]   Yeah, it's default.
[00:59:18.780 --> 00:59:19.780]   Yeah.
[00:59:19.780 --> 00:59:20.780]   They...
[00:59:20.780 --> 00:59:21.780]   I'm constantly fighting that.
[00:59:21.780 --> 00:59:22.780]   It's a horrible experience, but fine.
[00:59:22.780 --> 00:59:23.780]   You want to do it?
[00:59:23.780 --> 00:59:24.780]   Go ahead.
[00:59:24.780 --> 00:59:25.780]   Let's get up on those.
[00:59:25.780 --> 00:59:26.780]   It's a better...
[00:59:26.780 --> 00:59:27.780]   No, I always did that.
[00:59:27.780 --> 00:59:29.780]   It's a much better experience than the algorithmic one.
[00:59:29.780 --> 00:59:32.780]   So that's what he's saying is not unreasonable.
[00:59:32.780 --> 00:59:35.780]   Although Facebook would say, "Oh, no, we already give you that."
[00:59:35.780 --> 00:59:39.780]   Google, I don't know how a Google search could be non-algorithmic.
[00:59:39.780 --> 00:59:41.780]   What does that even mean?
[00:59:41.780 --> 00:59:42.780]   It wouldn't work.
[00:59:42.780 --> 00:59:43.780]   It means browsing.
[00:59:43.780 --> 00:59:44.780]   It means you...
[00:59:44.780 --> 00:59:45.780]   It was a blank.
[00:59:45.780 --> 00:59:46.780]   It was a blank.
[00:59:46.780 --> 00:59:47.780]   Yahoo.
[00:59:47.780 --> 00:59:52.020]   Maybe it would look like Yahoo or as Matthew said, alphabetical or maybe it could be chronological.
[00:59:52.020 --> 00:59:53.260]   Like this is the most choice.
[00:59:53.260 --> 00:59:54.260]   Oh, yeah.
[00:59:54.260 --> 00:59:55.260]   It was highly prestigious.
[00:59:55.260 --> 00:59:56.260]   Oh, I love this.
[00:59:56.260 --> 01:00:00.820]   Imagine if you do an internet search and it gives you the results in alphabetical order.
[01:00:00.820 --> 01:00:02.780]   It'd be like the phone book.
[01:00:02.780 --> 01:00:03.780]   A-a-a-a-a-a conspiracy.
[01:00:03.780 --> 01:00:04.780]   That's right.
[01:00:04.780 --> 01:00:05.780]   They're a 1.0.2.
[01:00:05.780 --> 01:00:07.180]   Talk to your company.
[01:00:07.180 --> 01:00:10.020]   And then gaming it would be easy.
[01:00:10.020 --> 01:00:11.020]   Yeah.
[01:00:11.020 --> 01:00:12.020]   You just...
[01:00:12.020 --> 01:00:13.660]   Just like they do the phone book.
[01:00:13.660 --> 01:00:15.740]   That's why there's so many muffler shops named A-a-a.
[01:00:15.740 --> 01:00:17.460]   A-a-a-a-a-a-a, muffler shops.
[01:00:17.460 --> 01:00:18.460]   A-a-a-a-a.
[01:00:18.460 --> 01:00:26.380]   Were you an eye on a Twitter discussion this week about great old website of the day stuff?
[01:00:26.380 --> 01:00:27.460]   Yeah.
[01:00:27.460 --> 01:00:29.420]   I mentioned Dogpile.
[01:00:29.420 --> 01:00:30.420]   Do you remember that one?
[01:00:30.420 --> 01:00:31.420]   Oh, yeah.
[01:00:31.420 --> 01:00:32.420]   Yeah.
[01:00:32.420 --> 01:00:33.420]   Yeah.
[01:00:33.420 --> 01:00:34.420]   That was an aggregator.
[01:00:34.420 --> 01:00:39.100]   Would aggregate the results from many different sites?
[01:00:39.100 --> 01:00:44.300]   I also pointed out that this is another little humble brag.
[01:00:44.300 --> 01:00:49.140]   I had a website called High Tech Investor, if you can believe it, because I worked for
[01:00:49.140 --> 01:00:53.020]   the stock market session in the paper.
[01:00:53.020 --> 01:00:56.900]   It was literally just a list of places you could get stock quotes because there were
[01:00:56.900 --> 01:01:01.060]   only about a dozen of them at the time.
[01:01:01.060 --> 01:01:03.580]   This was 1995.
[01:01:03.580 --> 01:01:08.820]   That was Worth Magazine's website of the day in 1995.
[01:01:08.820 --> 01:01:09.820]   Congratulations.
[01:01:09.820 --> 01:01:12.700]   I had a few cool sites of the day.
[01:01:12.700 --> 01:01:13.700]   Thank you.
[01:01:13.700 --> 01:01:17.100]   Yuckiest site on the internet was a cool site of the day.
[01:01:17.100 --> 01:01:18.100]   Hm.
[01:01:18.100 --> 01:01:19.100]   Wait a minute.
[01:01:19.100 --> 01:01:20.100]   Yuckiest.
[01:01:20.100 --> 01:01:21.100]   That was your site?
[01:01:21.100 --> 01:01:22.100]   Was the yuckiest site on the internet?
[01:01:22.100 --> 01:01:23.100]   Mm-hmm.
[01:01:23.100 --> 01:01:24.100]   What was-
[01:01:24.100 --> 01:01:25.100]   Were you doing porn?
[01:01:25.100 --> 01:01:26.100]   [laughter]
[01:01:26.100 --> 01:01:27.900]   Tell me this again.
[01:01:27.900 --> 01:01:33.700]   How your site, you had a site called the yuckiest site on the internet.
[01:01:33.700 --> 01:01:34.700]   Okay.
[01:01:34.700 --> 01:01:36.300]   Well, this is an collaboration with Liberty Science Center.
[01:01:36.300 --> 01:01:38.900]   It was about snot and cockroaches and all the time.
[01:01:38.900 --> 01:01:39.900]   Oh, nice.
[01:01:39.900 --> 01:01:41.180]   We're in how to make sites.
[01:01:41.180 --> 01:01:45.860]   My very first site I made on the internet was called Rainer Shine because I hate TV weathermen
[01:01:45.860 --> 01:01:49.700]   and I said, "Just give me the damn five day forecast and nothing but."
[01:01:49.700 --> 01:01:52.300]   So it was a site that gave you nothing but the five day forecast.
[01:01:52.300 --> 01:01:53.300]   Oh, a single one.
[01:01:53.300 --> 01:01:54.300]   Full of Rainer Shine.
[01:01:54.300 --> 01:01:55.300]   Full of Rainer Shine.
[01:01:55.300 --> 01:01:56.300]   Free sold to AcuWeather.
[01:01:56.300 --> 01:01:58.620]   And we sold Yuckiest site to Discovery.
[01:01:58.620 --> 01:02:04.660]   Because later on, one of my favorite sites was Two-O-I-Need-a-Jack-It, I think it was
[01:02:04.660 --> 01:02:05.660]   called.
[01:02:05.660 --> 01:02:07.780]   And that's all I told you.
[01:02:07.780 --> 01:02:10.500]   By the way, I think you can now do that with Amazon's Echo.
[01:02:10.500 --> 01:02:13.500]   It was just a gift that said yes.
[01:02:13.500 --> 01:02:14.500]   Always.
[01:02:14.500 --> 01:02:15.500]   Always.
[01:02:15.500 --> 01:02:16.500]   Always.
[01:02:16.500 --> 01:02:18.500]   Oh, yeah, you need to check it.
[01:02:18.500 --> 01:02:19.500]   Always.
[01:02:19.500 --> 01:02:22.660]   So that's so John Thune, somebody needs to explain to him that you could do that with
[01:02:22.660 --> 01:02:28.500]   Facebook, but it's nonsense to say a Google search that has no algorithm is nonsense.
[01:02:28.500 --> 01:02:29.500]   Would be useless.
[01:02:29.500 --> 01:02:30.500]   Yeah.
[01:02:30.500 --> 01:02:31.500]   And it's so small.
[01:02:31.500 --> 01:02:39.740]   And I would say this also plays into something else we've referred to, which is this tendency
[01:02:39.740 --> 01:02:45.580]   on the right and in the Trump administration to argue that these sites are biased against
[01:02:45.580 --> 01:02:46.580]   them.
[01:02:46.580 --> 01:02:48.580]   Oh, should we play the video?
[01:02:48.580 --> 01:02:49.580]   Let's talk about that.
[01:02:49.580 --> 01:02:50.580]   Should we play the video?
[01:02:50.580 --> 01:02:51.580]   Yes, play the video.
[01:02:51.580 --> 01:02:52.580]   It's unbelievable.
[01:02:52.580 --> 01:02:53.580]   So...
[01:02:53.580 --> 01:02:58.380]   And notice that Maria Bartiromo doesn't question his assertion at all.
[01:02:58.380 --> 01:03:03.900]   When Maria Bartiromo talked about finance, I used to like her.
[01:03:03.900 --> 01:03:05.740]   Oh, she went to the workside.
[01:03:05.740 --> 01:03:07.020]   She kind of did go over.
[01:03:07.020 --> 01:03:08.740]   She's a TV host.
[01:03:08.740 --> 01:03:10.740]   Yeah, that's part of the problem.
[01:03:10.740 --> 01:03:12.500]   She's a TV host.
[01:03:12.500 --> 01:03:13.500]   All right.
[01:03:13.500 --> 01:03:16.260]   So let me pause this.
[01:03:16.260 --> 01:03:23.460]   This is the president on the phone with Fox Business with Maria Bartiromo.
[01:03:23.460 --> 01:03:24.940]   Let me see.
[01:03:24.940 --> 01:03:26.220]   Oh, I muted him again.
[01:03:26.220 --> 01:03:27.220]   I keep doing that.
[01:03:27.220 --> 01:03:28.220]   Let me unmute.
[01:03:28.220 --> 01:03:29.700]   I think that's against the law, now Leo.
[01:03:29.700 --> 01:03:30.700]   You can't trust the president.
[01:03:30.700 --> 01:03:31.700]   I'm not allowed to mute the president.
[01:03:31.700 --> 01:03:32.700]   All right.
[01:03:32.700 --> 01:03:34.700]   And they're going to do this right up to the 2020 election.
[01:03:34.700 --> 01:03:37.060]   Well, then you got to go back to the back.
[01:03:37.060 --> 01:03:38.060]   I mean, what they did.
[01:03:38.060 --> 01:03:39.700]   I thought that was the beginning.
[01:03:39.700 --> 01:03:43.300]   And they're going to do this right up to the 2020 election.
[01:03:43.300 --> 01:03:45.180]   Well, they're doing it to me on Twitter.
[01:03:45.180 --> 01:03:48.780]   I mean, what they did to me on Twitter is incredible.
[01:03:48.780 --> 01:03:51.340]   You know, I have millions and millions of followers.
[01:03:51.340 --> 01:03:53.780]   And then what did they do to him on Twitter?
[01:03:53.780 --> 01:03:54.780]   Keep going.
[01:03:54.780 --> 01:03:55.780]   I'm saying the algorithm is...
[01:03:55.780 --> 01:04:00.420]   I will tell you, they make it very hard for people to join me on Twitter.
[01:04:00.420 --> 01:04:04.140]   And they make it very much harder for me to get out the message.
[01:04:04.140 --> 01:04:05.140]   They make it...
[01:04:05.140 --> 01:04:07.140]   What are you doing?
[01:04:07.140 --> 01:04:08.140]   Mr. President.
[01:04:08.140 --> 01:04:09.140]   What do you mean?
[01:04:09.140 --> 01:04:10.820]   These companies have an enormous amount of power.
[01:04:10.820 --> 01:04:11.980]   Oh, shut up, Maria.
[01:04:11.980 --> 01:04:13.420]   Wait a minute.
[01:04:13.420 --> 01:04:18.420]   He says all you have to do is go to twitter.com/therealdownjason.
[01:04:18.420 --> 01:04:19.420]   She said the algorithm is...
[01:04:19.420 --> 01:04:21.180]   She just took it and ran with it.
[01:04:21.180 --> 01:04:23.180]   Oh, they're being so mean to you, Mr. President.
[01:04:23.180 --> 01:04:24.180]   Click follow.
[01:04:24.180 --> 01:04:28.860]   If they could even stop the president of the free world in terms of getting his message
[01:04:28.860 --> 01:04:29.860]   out.
[01:04:29.860 --> 01:04:30.860]   Wait a minute.
[01:04:30.860 --> 01:04:32.340]   He's now the president of the free world.
[01:04:32.340 --> 01:04:34.340]   That is the president of the world.
[01:04:34.340 --> 01:04:36.340]   It's totally biased toward Democrats.
[01:04:36.340 --> 01:04:37.340]   No, no.
[01:04:37.340 --> 01:04:39.900]   We call him the leader of the free world.
[01:04:39.900 --> 01:04:42.060]   The president implies he's elected.
[01:04:42.060 --> 01:04:43.060]   Oh.
[01:04:43.060 --> 01:04:44.900]   Okay, yeah.
[01:04:44.900 --> 01:04:49.900]   The leader is more a position of authority.
[01:04:49.900 --> 01:04:52.820]   The president implies, hey, yeah, no, he got elected president of free world.
[01:04:52.820 --> 01:04:53.820]   Weren't you watching?
[01:04:53.820 --> 01:04:57.860]   I announced tomorrow that I'm going to become a nice liberal Democrat.
[01:04:57.860 --> 01:05:00.460]   I would pick up five times and more followers.
[01:05:00.460 --> 01:05:05.260]   I was picking up 100,000 followers every few days.
[01:05:05.260 --> 01:05:09.300]   And all of a sudden, and I'm much hotter now than I was a number of months ago.
[01:05:09.300 --> 01:05:12.420]   Okay, a number of months ago, then all of a sudden it stopped.
[01:05:12.420 --> 01:05:15.700]   And now I pick up a lot, but I don't pick up nearly what I did if you...
[01:05:15.700 --> 01:05:18.340]   That's why Devin Nunes is suing Twitter.
[01:05:18.340 --> 01:05:20.300]   Devin Nunes is suing Twitter right now.
[01:05:20.300 --> 01:05:21.300]   No, he...
[01:05:21.300 --> 01:05:22.300]   Who's suing Twitter?
[01:05:22.300 --> 01:05:23.300]   Devin Nunes, Congressman Nunes.
[01:05:23.300 --> 01:05:24.300]   You know.
[01:05:24.300 --> 01:05:25.300]   That's great.
[01:05:25.300 --> 01:05:26.300]   I think he's great.
[01:05:26.300 --> 01:05:27.300]   There's a lot of ways he's great.
[01:05:27.300 --> 01:05:28.300]   And they'll probably do well.
[01:05:28.300 --> 01:05:29.540]   He's right about it.
[01:05:29.540 --> 01:05:31.860]   Twitter is just terrible what they do.
[01:05:31.860 --> 01:05:32.860]   This is so ironic.
[01:05:32.860 --> 01:05:35.940]   I've had so many people come to me.
[01:05:35.940 --> 01:05:38.100]   Twitter is his platform.
[01:05:38.100 --> 01:05:39.500]   Twitter is his platform.
[01:05:39.500 --> 01:05:41.500]   He owns Twitter.
[01:05:41.500 --> 01:05:42.500]   And he should be.
[01:05:42.500 --> 01:05:44.700]   They have a special room with a shrine.
[01:05:44.700 --> 01:05:46.620]   I don't shake Trump in it.
[01:05:46.620 --> 01:05:47.620]   For him to say...
[01:05:47.620 --> 01:05:51.900]   He violates their terms of service all the time and they let him go.
[01:05:51.900 --> 01:05:52.900]   That's hysterical.
[01:05:52.900 --> 01:05:55.300]   It's the newsworthy exception.
[01:05:55.300 --> 01:05:56.900]   So he's going to sue Twitter.
[01:05:56.900 --> 01:05:57.900]   Yes, no, Matthew, right.
[01:05:57.900 --> 01:06:09.460]   He's going to sue Twitter, Facebook and Google because they're totally biased towards Democrats.
[01:06:09.460 --> 01:06:13.660]   By the way, it's not illegal to be biased towards Democrats or we would be in trouble.
[01:06:13.660 --> 01:06:15.660]   I see it may soon be.
[01:06:15.660 --> 01:06:18.220]   He must be biased in favor of Democrats.
[01:06:18.220 --> 01:06:22.620]   Yeah, no, but I'm saying it's not illegal for a private company to have a bias.
[01:06:22.620 --> 01:06:24.580]   Well, maybe soon it will be.
[01:06:24.580 --> 01:06:25.580]   Okay.
[01:06:25.580 --> 01:06:28.300]   I mean, that's where the Section 230 stuff comes in.
[01:06:28.300 --> 01:06:29.300]   Right.
[01:06:29.300 --> 01:06:33.620]   There's a bunch of people in the government who are arguing that platforms should be neutral,
[01:06:33.620 --> 01:06:37.740]   that their algorithms should not favor one thing or another or that they shouldn't remove
[01:06:37.740 --> 01:06:39.500]   certain types of content.
[01:06:39.500 --> 01:06:40.500]   And they're argumentative.
[01:06:40.500 --> 01:06:41.500]   That's the point of an algorithm.
[01:06:41.500 --> 01:06:42.500]   Right.
[01:06:42.500 --> 01:06:43.500]   What do you guys think?
[01:06:43.500 --> 01:06:47.660]   If they are not so-called neutral, they will lose Section 230 protection.
[01:06:47.660 --> 01:06:49.420]   That's the other last piece.
[01:06:49.420 --> 01:06:50.420]   Stupid one.
[01:06:50.420 --> 01:06:54.580]   I know Larry Page is going, is up in his special mountain lair thinking we'll just make it
[01:06:54.580 --> 01:06:57.620]   all alphabetical, then you'll see.
[01:06:57.620 --> 01:07:00.220]   Then you'll see.
[01:07:00.220 --> 01:07:02.660]   Enjoy the Internet morons.
[01:07:02.660 --> 01:07:03.660]   He's made enough money.
[01:07:03.660 --> 01:07:08.820]   He doesn't, you know, I think there's a road runner cartoon in this of the Acme winning
[01:07:08.820 --> 01:07:09.820]   finally.
[01:07:09.820 --> 01:07:10.820]   Yeah.
[01:07:10.820 --> 01:07:11.820]   God.
[01:07:11.820 --> 01:07:12.820]   Wow.
[01:07:12.820 --> 01:07:14.820]   That's just amazing.
[01:07:14.820 --> 01:07:23.420]   Google, by the way, has told its employees not to protest Google at the Pride parade.
[01:07:23.420 --> 01:07:24.420]   So yeah.
[01:07:24.420 --> 01:07:25.420]   That's disturbing.
[01:07:25.420 --> 01:07:26.420]   Do not.
[01:07:26.420 --> 01:07:31.140]   You can, you can, so the, it was actually a debate about whether they should, the Pride
[01:07:31.140 --> 01:07:34.740]   parade should allow Googlers.
[01:07:34.740 --> 01:07:36.700]   Jesus.
[01:07:36.700 --> 01:07:42.980]   And then because of harassment of LGBTQ plus members, community members.
[01:07:42.980 --> 01:07:46.100]   Oh, is that a thing that's been happening?
[01:07:46.100 --> 01:07:47.100]   I'm so sorry.
[01:07:47.100 --> 01:07:48.100]   I have not paid attention to that.
[01:07:48.100 --> 01:07:49.100]   Well, I don't know.
[01:07:49.100 --> 01:07:52.940]   And then, you know, according to some, yes.
[01:07:52.940 --> 01:07:53.940]   Yeah.
[01:07:53.940 --> 01:07:56.540]   I also saw somebody I trust on Twitter.
[01:07:56.540 --> 01:08:01.180]   They was saying that now you got far right within Google doing things.
[01:08:01.180 --> 01:08:04.780]   You had veritas went after, was it Google or Facebook executive?
[01:08:04.780 --> 01:08:08.300]   So it's become technology companies have become red hot.
[01:08:08.300 --> 01:08:09.300]   Well, they're terrified.
[01:08:09.300 --> 01:08:15.020]   I can't say anything in any direction because somebody will be mad about it.
[01:08:15.020 --> 01:08:21.180]   So anyway, now, now that the Pride parade will feature Googlers, but they won't be protesting
[01:08:21.180 --> 01:08:22.900]   against Google.
[01:08:22.900 --> 01:08:27.420]   Yeah, as it's Google said, and this is probably the case for every company, you have non-isperagement
[01:08:27.420 --> 01:08:30.700]   clause and you can't disparage the company you work for.
[01:08:30.700 --> 01:08:31.700]   So please don't.
[01:08:31.700 --> 01:08:32.700]   And you know what?
[01:08:32.700 --> 01:08:33.700]   What are they going to do arrest them?
[01:08:33.700 --> 01:08:35.700]   There's not, I mean, maybe I don't even think they'd fire them.
[01:08:35.700 --> 01:08:42.500]   Well, I think there's also a point of don't, don't use the Pride parade as the opportunity
[01:08:42.500 --> 01:08:46.460]   to protest, you let the Pride parade have its own dignity.
[01:08:46.460 --> 01:08:47.460]   Yeah.
[01:08:47.460 --> 01:08:48.460]   I guess.
[01:08:48.460 --> 01:08:49.460]   Have you been to a Pride parade?
[01:08:49.460 --> 01:08:52.460]   I mean, they're a heap of fun.
[01:08:52.460 --> 01:08:53.460]   It's a private toss.
[01:08:53.460 --> 01:08:54.460]   It's not a way.
[01:08:54.460 --> 01:08:55.460]   It's a shame.
[01:08:55.460 --> 01:08:59.340]   It's hard to talk about the youth.
[01:08:59.340 --> 01:09:04.500]   If you're wearing assless chaps, there's not.
[01:09:04.500 --> 01:09:10.020]   I covered Pride parades in San Francisco when they started.
[01:09:10.020 --> 01:09:11.020]   Good for you, Jeff.
[01:09:11.020 --> 01:09:12.020]   Good to do.
[01:09:12.020 --> 01:09:13.020]   Mark.
[01:09:13.020 --> 01:09:16.020]   By the way, the last time I used the phrase, "assless" chaps, somebody wrote to me and said,
[01:09:16.020 --> 01:09:18.260]   "All chaps are assless."
[01:09:18.260 --> 01:09:19.260]   Fair point.
[01:09:19.260 --> 01:09:20.260]   Fair point.
[01:09:20.260 --> 01:09:21.260]   Fair point.
[01:09:21.260 --> 01:09:22.260]   It's a redundancy.
[01:09:22.260 --> 01:09:23.260]   It's a fair point.
[01:09:23.260 --> 01:09:24.260]   Fair point.
[01:09:24.260 --> 01:09:25.940]   It's just a great phrase.
[01:09:25.940 --> 01:09:27.940]   That's all.
[01:09:27.940 --> 01:09:32.980]   Wow, this show takes it out of me.
[01:09:32.980 --> 01:09:33.980]   I don't know.
[01:09:33.980 --> 01:09:34.980]   Is it taking out of you guys?
[01:09:34.980 --> 01:09:35.980]   It's so cute.
[01:09:35.980 --> 01:09:36.980]   I've gone all the way.
[01:09:36.980 --> 01:09:37.980]   Let's see.
[01:09:37.980 --> 01:09:39.980]   Okay, we talk about Blue Dot because I think that's really interesting.
[01:09:39.980 --> 01:09:40.980]   Yes.
[01:09:40.980 --> 01:09:41.980]   Blue Dot.
[01:09:41.980 --> 01:09:42.980]   What does Blue Dot stay?
[01:09:42.980 --> 01:09:43.980]   I don't know what it was really good.
[01:09:43.980 --> 01:09:44.980]   Oh, let me make sure.
[01:09:44.980 --> 01:09:45.980]   It is Blue Dot.
[01:09:45.980 --> 01:09:46.980]   Okay.
[01:09:46.980 --> 01:09:47.980]   That's also a furniture company.
[01:09:47.980 --> 01:09:49.060]   So I was like, "What did I just screw that up?"
[01:09:49.060 --> 01:09:56.500]   This is a peer-to-peer, they call it counseling, but it's really training a subset of your
[01:09:56.500 --> 01:10:00.380]   employees in compassionate listening, which is just open-minded listening, not trying
[01:10:00.380 --> 01:10:02.980]   to solve a problem and not making judgments.
[01:10:02.980 --> 01:10:03.980]   Nice.
[01:10:03.980 --> 01:10:07.140]   Oh, this is Google's Mental Health Company Run Mental.
[01:10:07.140 --> 01:10:08.980]   It's the Boy Run Mental Health Project.
[01:10:08.980 --> 01:10:09.980]   Yeah.
[01:10:09.980 --> 01:10:10.980]   It is, yeah.
[01:10:10.980 --> 01:10:12.780]   It's an employee-run mental health effort.
[01:10:12.780 --> 01:10:18.780]   These employees who are trained this way, mark themselves with a Blue Dot on their badges.
[01:10:18.780 --> 01:10:23.660]   I like the example of how this happens, which is you might be hanging out with somebody
[01:10:23.660 --> 01:10:28.300]   and are just passing by and if you're having a bad day or something, you're just like,
[01:10:28.300 --> 01:10:29.300]   "Oh, hey, you're a Blue Dot."
[01:10:29.300 --> 01:10:30.300]   I love this idea.
[01:10:30.300 --> 01:10:31.900]   And you, can I talk to you?
[01:10:31.900 --> 01:10:32.900]   Right, right.
[01:10:32.900 --> 01:10:33.900]   Yeah.
[01:10:33.900 --> 01:10:34.900]   So we should do this.
[01:10:34.900 --> 01:10:37.260]   Google+, no data on it.
[01:10:37.260 --> 01:10:40.940]   Blue Dot volunteers don't coach people or even offer opinions they practice compassionate
[01:10:40.940 --> 01:10:45.460]   listening promising openness and kindness to coworkers who come to them.
[01:10:45.460 --> 01:10:47.700]   It's just a neuron's own.
[01:10:47.700 --> 01:10:52.900]   Then Google saw that they added training to it.
[01:10:52.900 --> 01:10:53.900]   Which is a good idea.
[01:10:53.900 --> 01:10:54.900]   Yeah.
[01:10:54.900 --> 01:10:55.900]   Yeah.
[01:10:55.900 --> 01:10:59.140]   400 people, I think it's great to do that.
[01:10:59.140 --> 01:11:05.140]   So it sounds like this one woman uses her 20% time to manage any sort of administrative
[01:11:05.140 --> 01:11:06.500]   task associated with it.
[01:11:06.500 --> 01:11:07.500]   So I guess-
[01:11:07.500 --> 01:11:10.140]   The joke is 20% time is 120% time.
[01:11:10.140 --> 01:11:11.140]   Right.
[01:11:11.140 --> 01:11:12.140]   Yes.
[01:11:12.140 --> 01:11:15.500]   I'm just pointing it out that that's where they say.
[01:11:15.500 --> 01:11:18.020]   And I would assume that it would be difficult.
[01:11:18.020 --> 01:11:20.220]   Because if someone's like, "Hey, can I talk?"
[01:11:20.220 --> 01:11:23.220]   You know, it's not like if you're working on something pressing, you don't want to be
[01:11:23.220 --> 01:11:27.460]   like, "Uh, I have to get this out of the box."
[01:11:27.460 --> 01:11:30.220]   I know you're trouble right now.
[01:11:30.220 --> 01:11:34.380]   There is definitely a trend in technology companies.
[01:11:34.380 --> 01:11:36.540]   And I would guess it's all companies these days.
[01:11:36.540 --> 01:11:40.900]   But there's definitely a trend in technology companies to acknowledge that there's depression,
[01:11:40.900 --> 01:11:46.500]   there are other psychological impacts to the hard, the long work hours, the hard work,
[01:11:46.500 --> 01:11:49.620]   the isolated nature of some of the work.
[01:11:49.620 --> 01:11:54.300]   And I think there's a very active movement within Silicon Valley and in tech companies
[01:11:54.300 --> 01:11:57.380]   in general to kind of acknowledge that and address it.
[01:11:57.380 --> 01:12:01.980]   And not in the more formal traditional ways that an HR department might, you know, every
[01:12:01.980 --> 01:12:06.740]   company in the world has a sign saying, "Here's the 800 number if you feel like, you know,
[01:12:06.740 --> 01:12:08.180]   it's not worth living.
[01:12:08.180 --> 01:12:09.180]   Call this number."
[01:12:09.180 --> 01:12:11.780]   This is something much, I think much more effective, much better.
[01:12:11.780 --> 01:12:15.140]   We just had to all take our harassment training.
[01:12:15.140 --> 01:12:19.620]   And among one of the things I really liked about this is State of the California mandated
[01:12:19.620 --> 01:12:21.100]   so everybody had to do it.
[01:12:21.100 --> 01:12:23.180]   Management, including me, had to do it extra two out.
[01:12:23.180 --> 01:12:26.180]   We had like a special two hour training.
[01:12:26.180 --> 01:12:28.180]   But the point of it, they kept emphasizing it.
[01:12:28.180 --> 01:12:32.820]   Yeah, they said the point of it is not to keep your company from getting sued.
[01:12:32.820 --> 01:12:34.180]   That's the point of it, by the way.
[01:12:34.180 --> 01:12:39.540]   But they in public, they say the point of it is not to be a jerk to your coworkers.
[01:12:39.540 --> 01:12:40.540]   That's right.
[01:12:40.540 --> 01:12:45.780]   The point of it is to make the workplace a respectful place to other, you know, and I
[01:12:45.780 --> 01:12:46.820]   thought that was really good.
[01:12:46.820 --> 01:12:51.100]   And they talked about listening and not have judging, not having an opinion.
[01:12:51.100 --> 01:12:55.620]   If there's an incident to immediately take, you know, to take action.
[01:12:55.620 --> 01:12:57.940]   So it was really, I thought it was really, I was surprised.
[01:12:57.940 --> 01:12:59.460]   I was prepared to hate it.
[01:12:59.460 --> 01:13:01.180]   I had to take it.
[01:13:01.180 --> 01:13:04.140]   And you know, I hate the every, anytime you work for a big company.
[01:13:04.140 --> 01:13:06.580]   I know, Jeff, you know this anyway.
[01:13:06.580 --> 01:13:11.500]   And you have to take these training for the radio show.
[01:13:11.500 --> 01:13:15.500]   I Heart Media has so many consent decrees.
[01:13:15.500 --> 01:13:22.060]   I have to take, I have, I have an hour long training on why you shouldn't play the emergency
[01:13:22.060 --> 01:13:28.740]   tones on your radio show unless it's an actual emergency because some dumb DJ in New Orleans
[01:13:28.740 --> 01:13:29.740]   did it.
[01:13:29.740 --> 01:13:33.740]   And now every I Heart radio employee has to take this training.
[01:13:33.740 --> 01:13:34.740]   Why not to?
[01:13:34.740 --> 01:13:37.580]   Obviously not a good idea.
[01:13:37.580 --> 01:13:39.500]   And but there's a lot of them.
[01:13:39.500 --> 01:13:43.300]   And it's, you know, most of the time I hate it, but this one was, I thought very, very
[01:13:43.300 --> 01:13:44.300]   good.
[01:13:44.300 --> 01:13:48.540]   And it, I guess the state of California is required, requires us to do that.
[01:13:48.540 --> 01:13:53.380]   So I think there is a trend in this direction to making a workplace, the workplace a kinder,
[01:13:53.380 --> 01:13:57.340]   gentler, more inclusive place in California.
[01:13:57.340 --> 01:14:01.780]   You can't discriminate against somebody because of their gender choice, for instance, or
[01:14:01.780 --> 01:14:07.040]   their pronoun choice or, you know, I mean, there's some really good, I think fairly good
[01:14:07.040 --> 01:14:10.300]   rules that are completely appropriate.
[01:14:10.300 --> 01:14:14.060]   And I want to go on record saying.
[01:14:14.060 --> 01:14:15.060]   Noted.
[01:14:15.060 --> 01:14:16.700]   All right.
[01:14:16.700 --> 01:14:17.700]   Thank you.
[01:14:17.700 --> 01:14:20.900]   No, I'm so protesting you in the Pride Parade.
[01:14:20.900 --> 01:14:21.900]   So you're pleased.
[01:14:21.900 --> 01:14:25.180]   Don't protest against me in the Pride Parade.
[01:14:25.180 --> 01:14:27.060]   So YouTube is in the hot seat a little bit.
[01:14:27.060 --> 01:14:32.260]   The Washington Post had a scoop that said that the government has in the late stages
[01:14:32.260 --> 01:14:38.220]   of an investigation of YouTube for violating the Children's Online Privacy Protection Act.
[01:14:38.220 --> 01:14:43.140]   That forbids tracking and targeting of users younger than the age of 13.
[01:14:43.140 --> 01:14:50.260]   The probe launched after numerous complaints from consumer groups and privacy advocates
[01:14:50.260 --> 01:14:57.220]   because apparently YouTube has been somehow either failing to protect kids or improperly
[01:14:57.220 --> 01:15:00.340]   collecting their data.
[01:15:00.340 --> 01:15:05.020]   Are they collecting data, knowing it's from kids or just kids on YouTube and there are
[01:15:05.020 --> 01:15:07.300]   another blip in the Google machine?
[01:15:07.300 --> 01:15:11.820]   You know, probably it's the latter, but what they're saying is Google has to ascertain people's
[01:15:11.820 --> 01:15:12.820]   age.
[01:15:12.820 --> 01:15:16.780]   You can't just assume, well, they must be an adult because they have a YouTube account.
[01:15:16.780 --> 01:15:17.780]   Yeah.
[01:15:17.780 --> 01:15:22.620]   You have to assume they haven't lied.
[01:15:22.620 --> 01:15:25.780]   These companies, according to the Post, say their services are intended for adults and
[01:15:25.780 --> 01:15:29.020]   they take action when they find users who are underage.
[01:15:29.020 --> 01:15:33.220]   Facebook says you can't join Facebook if you're under 13, but that doesn't stop.
[01:15:33.220 --> 01:15:36.340]   Google won't give people email addresses unless they're...
[01:15:36.340 --> 01:15:37.340]   Same reason.
[01:15:37.340 --> 01:15:38.340]   Yeah.
[01:15:38.340 --> 01:15:45.500]   But my daughter has a Gmail address because I signed up for one for her because a 13-year-old
[01:15:45.500 --> 01:15:47.220]   without an email address is just ridiculous.
[01:15:47.220 --> 01:15:48.220]   Great.
[01:15:48.220 --> 01:15:52.180]   Now, why wouldn't you create a kid's version of an account that just doesn't track anything?
[01:15:52.180 --> 01:15:55.220]   Because that's totally doable, but that would give up that.
[01:15:55.220 --> 01:15:57.940]   If you fail out of your liability, it's probably higher.
[01:15:57.940 --> 01:16:04.340]   Well, reportedly, YouTube is considering moving all the kids' videos to YouTube kids to really
[01:16:04.340 --> 01:16:10.260]   make that a separate place, maybe turn off auto play on YouTube kids by default, maybe
[01:16:10.260 --> 01:16:11.260]   turn off...
[01:16:11.260 --> 01:16:12.260]   Yeah, turn off...
[01:16:12.260 --> 01:16:13.260]   Yeah.
[01:16:13.260 --> 01:16:14.260]   Turn off.
[01:16:14.260 --> 01:16:15.260]   Yeah.
[01:16:15.260 --> 01:16:16.260]   It's not just collecting.
[01:16:16.260 --> 01:16:18.020]   It's not just collecting info.
[01:16:18.020 --> 01:16:23.740]   I mean, there's been some story after story about inappropriate videos showing up and recommendations.
[01:16:23.740 --> 01:16:29.420]   And then, you know, pedophiles using, like, coded messages and comments to...
[01:16:29.420 --> 01:16:34.220]   And I mean, YouTube, I think, has admitted, I don't know if they have or not, that nobody
[01:16:34.220 --> 01:16:35.460]   goes to YouTube kids.
[01:16:35.460 --> 01:16:36.460]   Right.
[01:16:36.460 --> 01:16:39.620]   Kids, unless their parents force them to, I guess.
[01:16:39.620 --> 01:16:40.620]   So this was in the Wall Street...
[01:16:40.620 --> 01:16:41.620]   That's all the stuff, honestly.
[01:16:41.620 --> 01:16:46.580]   You know, in the Wall Street Journal last week, but according to the post, a person close to
[01:16:46.580 --> 01:16:51.580]   the company said that option is highly improbable, difficult to implement because of the sheer
[01:16:51.580 --> 01:16:55.220]   volume of content and potentially costly because of lost advertising revenues.
[01:16:55.220 --> 01:16:57.660]   Well, yeah, because you're not supposed to advertise them anyway.
[01:16:57.660 --> 01:16:58.660]   Yeah.
[01:16:58.660 --> 01:16:59.660]   Well, yeah, that's the point.
[01:16:59.660 --> 01:17:01.220]   That's the point.
[01:17:01.220 --> 01:17:06.260]   But the person close to the company said that other changes were on the table.
[01:17:06.260 --> 01:17:09.460]   YouTube kids gets a tiny fraction of YouTube's audience.
[01:17:09.460 --> 01:17:13.500]   So could we go to the other end of this for a second?
[01:17:13.500 --> 01:17:15.300]   Well, we'll ruin every conversation.
[01:17:15.300 --> 01:17:16.300]   YouTube...
[01:17:16.300 --> 01:17:18.660]   Is there going to be YouTube olds for the olds?
[01:17:18.660 --> 01:17:19.660]   Because I would like that.
[01:17:19.660 --> 01:17:21.620]   Full of conspiracy theories.
[01:17:21.620 --> 01:17:22.620]   Yeah.
[01:17:22.620 --> 01:17:23.860]   In keto diet tips?
[01:17:23.860 --> 01:17:24.860]   Yeah.
[01:17:24.860 --> 01:17:27.460]   It's called AARP Tube.
[01:17:27.460 --> 01:17:30.100]   Okay, go ahead, Jeff, seriously.
[01:17:30.100 --> 01:17:33.700]   So a lot of us are going to go to identity, approval identity.
[01:17:33.700 --> 01:17:34.700]   Yeah.
[01:17:34.700 --> 01:17:36.500]   And so two threads to this.
[01:17:36.500 --> 01:17:41.460]   Right, in the UK, they're about to implement a law requiring you to have a certified identity
[01:17:41.460 --> 01:17:42.460]   to see...
[01:17:42.460 --> 01:17:44.460]   They put that in the back burner.
[01:17:44.460 --> 01:17:45.460]   Oh, okay.
[01:17:45.460 --> 01:17:46.460]   Oh, they did it.
[01:17:46.460 --> 01:17:47.460]   They did it.
[01:17:47.460 --> 01:17:48.460]   It came out because they didn't tell...
[01:17:48.460 --> 01:17:49.460]   It's good to worry about it.
[01:17:49.460 --> 01:17:51.460]   They didn't tell the EU.
[01:17:51.460 --> 01:17:55.300]   Well, that'll suit, not matter.
[01:17:55.300 --> 01:17:56.300]   Yeah.
[01:17:56.300 --> 01:18:00.060]   But we just thought we wouldn't be part of the EU by now.
[01:18:00.060 --> 01:18:01.060]   Yeah.
[01:18:01.060 --> 01:18:02.060]   Yeah.
[01:18:02.060 --> 01:18:03.420]   But then you look at the Libra story.
[01:18:03.420 --> 01:18:09.340]   But the story you guys put up there about how the hidden interesting thing in the Libra
[01:18:09.340 --> 01:18:15.420]   announcement was a new structure of verifiable identity, which sounds interesting, but also
[01:18:15.420 --> 01:18:19.820]   sounds in certain regimes dangerous.
[01:18:19.820 --> 01:18:24.260]   And will you have to verify your identity and thus your age before you can get into
[01:18:24.260 --> 01:18:26.740]   YouTube or an email address or all of that?
[01:18:26.740 --> 01:18:30.700]   This is the MIT technology review, Mike Orkut.
[01:18:30.700 --> 01:18:37.740]   It's a radical idea hiding inside Facebook's digital currency proposal.
[01:18:37.740 --> 01:18:39.340]   And yeah, that's an interesting point.
[01:18:39.340 --> 01:18:45.300]   I mean, that actually goes to the question about the blockchain, because the blockchain
[01:18:45.300 --> 01:18:47.860]   is the record of transactions.
[01:18:47.860 --> 01:18:49.580]   And in YouTube's...
[01:18:49.580 --> 01:18:55.660]   Sorry, Facebook's proposal with Libracoin, the blockchain is not a public ledger, but
[01:18:55.660 --> 01:19:03.580]   it is managed by these initial companies, 100 strong or whatever, that are part of it.
[01:19:03.580 --> 01:19:06.780]   They're going to be the nodes and they're going to manage the blockchain.
[01:19:06.780 --> 01:19:09.700]   So it isn't a completely decentralized cryptocurrency.
[01:19:09.700 --> 01:19:13.180]   Yeah, it's kind of a hybrid.
[01:19:13.180 --> 01:19:14.180]   Yeah.
[01:19:14.180 --> 01:19:15.820]   And there's a reason for that.
[01:19:15.820 --> 01:19:19.340]   If you are completely decentralized, transactions take forever.
[01:19:19.340 --> 01:19:22.620]   And that's been the problem on Bitcoin.
[01:19:22.620 --> 01:19:27.420]   The Facebook coins supposedly will be able to handle 1,000 transactions a second because
[01:19:27.420 --> 01:19:29.380]   it is not fully decentralized.
[01:19:29.380 --> 01:19:33.940]   But that's still a tiny fraction for a billion or 2 billion users.
[01:19:33.940 --> 01:19:36.420]   A thousand transactions a second is not going to make it.
[01:19:36.420 --> 01:19:37.860]   So I don't know.
[01:19:37.860 --> 01:19:41.660]   I wonder about how this is going to work.
[01:19:41.660 --> 01:19:43.700]   It is interesting, though.
[01:19:43.700 --> 01:19:48.980]   You remember the years ago when Open Identity standards were...
[01:19:48.980 --> 01:19:55.220]   Google was working on one and there was a consortium of groups that were trying to
[01:19:55.220 --> 01:20:02.580]   come up with a way of logging into things without requiring you to use Facebook or Twitter
[01:20:02.580 --> 01:20:06.340]   or any other thing.
[01:20:06.340 --> 01:20:08.380]   And none of them went anywhere.
[01:20:08.380 --> 01:20:13.620]   And it was because they couldn't get enough agreement from the places that would then
[01:20:13.620 --> 01:20:15.620]   use that standard.
[01:20:15.620 --> 01:20:20.900]   But the standards themselves weren't good just that it was hard to get kind of buy-in.
[01:20:20.900 --> 01:20:26.420]   That is the problem with standards.
[01:20:26.420 --> 01:20:30.580]   Bitcoin can process about five transactions a second.
[01:20:30.580 --> 01:20:32.620]   That's very, very slow.
[01:20:32.620 --> 01:20:34.420]   Visa does 17...
[01:20:34.420 --> 01:20:35.420]   24,000.
[01:20:35.420 --> 01:20:38.900]   Visa does 1,700 transactions a second on average.
[01:20:38.900 --> 01:20:41.100]   And I'm sure it peaks in much higher numbers.
[01:20:41.100 --> 01:20:45.620]   So if Laborcoin does half what Visa has to do, it's not going to be.
[01:20:45.620 --> 01:20:47.820]   It's a non-starter.
[01:20:47.820 --> 01:20:48.820]   Right?
[01:20:48.820 --> 01:20:49.820]   I don't...
[01:20:49.820 --> 01:20:52.740]   I thought Visa did many more.
[01:20:52.740 --> 01:20:54.260]   Well, the calculator...
[01:20:54.260 --> 01:20:58.260]   Visa says we do 150 million transactions a day.
[01:20:58.260 --> 01:20:59.260]   Divide that by 24.
[01:20:59.260 --> 01:21:01.940]   If each one were a penny, how long...
[01:21:01.940 --> 01:21:03.460]   No, I'm sorry.
[01:21:03.460 --> 01:21:05.260]   It would cover the world, my friend.
[01:21:05.260 --> 01:21:06.260]   It's not going to work.
[01:21:06.260 --> 01:21:07.260]   Cover the world.
[01:21:07.260 --> 01:21:08.260]   Visa is capable of...
[01:21:08.260 --> 01:21:11.420]   This is where that number comes from, Stacey.
[01:21:11.420 --> 01:21:14.620]   A capable of 24,000 per second.
[01:21:14.620 --> 01:21:16.780]   So that's the peak speed.
[01:21:16.780 --> 01:21:17.780]   That's a lot more.
[01:21:17.780 --> 01:21:19.180]   That's 24 times Laborcoin.
[01:21:19.180 --> 01:21:22.260]   I wonder now that I'm thinking about it.
[01:21:22.260 --> 01:21:25.540]   What is the point of Laborcoin?
[01:21:25.540 --> 01:21:27.660]   Maybe it is this authentication.
[01:21:27.660 --> 01:21:31.340]   It's not designed to replace Visa, I don't think.
[01:21:31.340 --> 01:21:34.940]   It's not designed to replace every transaction.
[01:21:34.940 --> 01:21:36.780]   But Visa's just one credit card.
[01:21:36.780 --> 01:21:40.220]   It's not every transaction even.
[01:21:40.220 --> 01:21:42.180]   Well, there...
[01:21:42.180 --> 01:21:43.940]   I mean, we buy a lot of stuff.
[01:21:43.940 --> 01:21:45.860]   They're a big one, but I'm just saying there's also a MasterCard.
[01:21:45.860 --> 01:21:46.860]   There's also American Express.
[01:21:46.860 --> 01:21:48.380]   There's also a Discover Card.
[01:21:48.380 --> 01:21:49.380]   There's also cash.
[01:21:49.380 --> 01:21:50.380]   There's also checks.
[01:21:50.380 --> 01:21:54.500]   If you really wanted to cover the financial world, a thousand transactions a second is
[01:21:54.500 --> 01:21:55.500]   not even close.
[01:21:55.500 --> 01:21:58.580]   Well, they don't want to cover the financial world.
[01:21:58.580 --> 01:22:02.980]   I think they want to cover a peer-to-peer kind of quick payment.
[01:22:02.980 --> 01:22:03.980]   Yeah.
[01:22:03.980 --> 01:22:04.980]   What Venmo?
[01:22:04.980 --> 01:22:05.980]   I don't know.
[01:22:05.980 --> 01:22:11.420]   I think like Venmo or like in China, how everybody can take a picture of a Scanning QR code.
[01:22:11.420 --> 01:22:12.780]   Boom, they've paid for something.
[01:22:12.780 --> 01:22:14.780]   I wish we had that.
[01:22:14.780 --> 01:22:15.780]   I'm not going to lie.
[01:22:15.780 --> 01:22:16.780]   You may get it.
[01:22:16.780 --> 01:22:20.780]   I wish we had it in a way.
[01:22:20.780 --> 01:22:21.780]   Who would you...
[01:22:21.780 --> 01:22:30.500]   I would trust Amazon to do it.
[01:22:30.500 --> 01:22:33.340]   I would actually trust my banks to do it.
[01:22:33.340 --> 01:22:35.340]   Apple will probably do it.
[01:22:35.340 --> 01:22:36.340]   Apple's got a credit card.
[01:22:36.340 --> 01:22:37.340]   They're on their way.
[01:22:37.340 --> 01:22:38.340]   I don't like Apple.
[01:22:38.340 --> 01:22:40.820]   They're pre-go systems too, insular for me.
[01:22:40.820 --> 01:22:41.820]   I agree.
[01:22:41.820 --> 01:22:42.820]   Yep.
[01:22:42.820 --> 01:22:43.820]   Sorry.
[01:22:43.820 --> 01:22:44.820]   I'm like, eh, not coming.
[01:22:44.820 --> 01:22:45.820]   Amazon, you would trust.
[01:22:45.820 --> 01:22:47.580]   Yeah, I would do it.
[01:22:47.580 --> 01:22:51.420]   I do because I know what Amazon's agenda and goals are.
[01:22:51.420 --> 01:22:53.180]   They want me to buy a bunch of stuff from them.
[01:22:53.180 --> 01:22:58.700]   So I feel like in my data when it goes into Amazon, it doesn't go out of Amazon.
[01:22:58.700 --> 01:23:02.660]   Amazon keeps that they make stupid choices when they buy companies like Ring and they're
[01:23:02.660 --> 01:23:03.660]   stupid.
[01:23:03.660 --> 01:23:07.580]   But Amazon locks that stuff down because they don't want to give the advantage to other
[01:23:07.580 --> 01:23:08.580]   people.
[01:23:08.580 --> 01:23:13.100]   And the advantage to them is Stacey will stay in our world and will buy a crap ton of stuff
[01:23:13.100 --> 01:23:14.100]   from them.
[01:23:14.100 --> 01:23:17.100]   But not to raise a straw man, but let me raise a straw man.
[01:23:17.100 --> 01:23:18.100]   Okay.
[01:23:18.100 --> 01:23:20.100]   Right now, Amazon's goal is to...
[01:23:20.100 --> 01:23:21.580]   What about a strong woman?
[01:23:21.580 --> 01:23:25.140]   Okay, a strong person.
[01:23:25.140 --> 01:23:27.540]   Let's raise a strong person.
[01:23:27.540 --> 01:23:30.180]   Or as they say, a paper tiger.
[01:23:30.180 --> 01:23:33.580]   Is Amazon right now wants to just sell you stuff?
[01:23:33.580 --> 01:23:37.180]   What if at some point Amazon had other goals?
[01:23:37.180 --> 01:23:38.180]   So that is a question.
[01:23:38.180 --> 01:23:41.660]   Like with the Amazon's health insurance or health efforts, I'm like, well, what could
[01:23:41.660 --> 01:23:43.100]   that look like?
[01:23:43.100 --> 01:23:44.980]   That kind of makes me a little nervous.
[01:23:44.980 --> 01:23:52.420]   At some point in time, I would want to make sure my eggs are not all in the Amazon basket.
[01:23:52.420 --> 01:23:55.700]   And if that wasn't an option, then I would cry foul.
[01:23:55.700 --> 01:24:01.940]   And isn't that the best way to avoid all of this is to spread yourself out?
[01:24:01.940 --> 01:24:02.940]   Yeah.
[01:24:02.940 --> 01:24:03.940]   Mm-hmm.
[01:24:03.940 --> 01:24:05.940]   And not to be...
[01:24:05.940 --> 01:24:07.740]   It's a lot of work though.
[01:24:07.740 --> 01:24:11.620]   Yeah, it's sure easier if you're all apple, right?
[01:24:11.620 --> 01:24:13.860]   If everything you have is apple, everything works together.
[01:24:13.860 --> 01:24:14.860]   And it works.
[01:24:14.860 --> 01:24:19.620]   Stacey and IOT, isn't it all better if it's all home kid or all smart things or all...
[01:24:19.620 --> 01:24:21.980]   Oh, it works better, sure.
[01:24:21.980 --> 01:24:23.820]   Yeah.
[01:24:23.820 --> 01:24:25.300]   That's why these companies try to do this.
[01:24:25.300 --> 01:24:27.660]   You know, one of the companies is not doing that.
[01:24:27.660 --> 01:24:28.660]   There's two.
[01:24:28.660 --> 01:24:31.100]   Microsoft and Google actually don't really care.
[01:24:31.100 --> 01:24:32.100]   Google gets it.
[01:24:32.100 --> 01:24:33.100]   Google just...
[01:24:33.100 --> 01:24:35.900]   No, Google with their nest thing just kind of changes up.
[01:24:35.900 --> 01:24:36.900]   Maybe they do.
[01:24:36.900 --> 01:24:37.900]   Okay.
[01:24:37.900 --> 01:24:38.900]   So the nest thing is different.
[01:24:38.900 --> 01:24:39.900]   Yeah.
[01:24:39.900 --> 01:24:40.900]   That was a little different.
[01:24:40.900 --> 01:24:41.900]   Okay.
[01:24:41.900 --> 01:24:42.900]   I'm going to pause for a second.
[01:24:42.900 --> 01:24:43.900]   Without ads, I have to go run and give a cup of coffee.
[01:24:43.900 --> 01:24:44.900]   Have a cup of coffee.
[01:24:44.900 --> 01:24:46.700]   While I talk about how Verizon...
[01:24:46.700 --> 01:24:47.700]   I'd love a...
[01:24:47.700 --> 01:24:48.700]   Oh, yeah.
[01:24:48.700 --> 01:24:49.700]   Get one for Matthew too.
[01:24:49.700 --> 01:24:50.700]   Yeah.
[01:24:50.700 --> 01:24:51.700]   Yeah.
[01:24:51.700 --> 01:24:58.660]   Honestly, if you're going to do this show, you ought to have a couple of things in the
[01:24:58.660 --> 01:24:59.860]   studio with you.
[01:24:59.860 --> 01:25:04.660]   A coffee maker, a beverage machine and perhaps a toilet.
[01:25:04.660 --> 01:25:07.460]   Because these shows go on and on and on.
[01:25:07.460 --> 01:25:08.460]   We're almost done.
[01:25:08.460 --> 01:25:09.460]   Without an ad.
[01:25:09.460 --> 01:25:12.260]   Without an ad, there is no break.
[01:25:12.260 --> 01:25:13.260]   I should...
[01:25:13.260 --> 01:25:14.260]   Wait a minute, I got an ad.
[01:25:14.260 --> 01:25:17.340]   Don't we have a house ad that we're all supposed to be doing here?
[01:25:17.340 --> 01:25:19.140]   Can you get me a house ad?
[01:25:19.140 --> 01:25:21.140]   Okay, I came back.
[01:25:21.140 --> 01:25:25.140]   And now a house ad.
[01:25:25.140 --> 01:25:26.140]   Watch.
[01:25:26.140 --> 01:25:27.140]   Wait.
[01:25:27.140 --> 01:25:28.140]   Watch.
[01:25:28.140 --> 01:25:29.140]   Wait, wait, wait.
[01:25:29.140 --> 01:25:30.140]   No, no, it's better than that.
[01:25:30.140 --> 01:25:31.140]   It's subtler.
[01:25:31.140 --> 01:25:32.140]   Correct.
[01:25:32.140 --> 01:25:33.140]   We truly appreciate you listening to this podcast.
[01:25:33.140 --> 01:25:37.700]   Every day, it takes a team of hosts, producers, editors, engineers and support staff, not to
[01:25:37.700 --> 01:25:42.620]   mention the person who gets Leo's dry cleaning to get our content to you.
[01:25:42.620 --> 01:25:46.180]   As you know, our podcasts are free.
[01:25:46.180 --> 01:25:47.180]   You didn't pay anything.
[01:25:47.180 --> 01:25:49.340]   Patreon is the Patreon pitch starting here?
[01:25:49.340 --> 01:25:50.340]   No, no, no, no.
[01:25:50.340 --> 01:25:52.460]   And ad supported.
[01:25:52.460 --> 01:25:56.320]   The easiest thing you can do to help our network besides sending us cash.
[01:25:56.320 --> 01:25:58.220]   No, no, no, no, no, no, no, no, no, no.
[01:25:58.220 --> 01:25:59.720]   It's not easy.
[01:25:59.720 --> 01:26:00.720]   It's not easy.
[01:26:00.720 --> 01:26:01.720]   Subscribe to it.
[01:26:01.720 --> 01:26:04.940]   This show and all your favorite Twitter podcast because that way you get it instantly when
[01:26:04.940 --> 01:26:05.940]   it's available.
[01:26:05.940 --> 01:26:09.660]   It helps us in some magical way I've never really understood.
[01:26:09.660 --> 01:26:13.660]   But I believe Lisa when she says it.
[01:26:13.660 --> 01:26:15.940]   And she gets the dry cleaning, so it's okay.
[01:26:15.940 --> 01:26:16.940]   Thanks for listening.
[01:26:16.940 --> 01:26:22.300]   And remember to subscribe on your favorite podcatcher or just go to Twitter.tv/subscribe
[01:26:22.300 --> 01:26:23.300]   and we'll tell you.
[01:26:23.300 --> 01:26:25.260]   And send a nice time to Lisa.
[01:26:25.260 --> 01:26:30.420]   And thank Lisa and all those people who make the actually there are a lot of people.
[01:26:30.420 --> 01:26:34.260]   It takes about one, two, three, they're really nice people.
[01:26:34.260 --> 01:26:37.140]   They deserve your support.
[01:26:37.140 --> 01:26:42.460]   And of course, let's not forget Stacy, Jeff and Matthew.
[01:26:42.460 --> 01:26:43.900]   They all count.
[01:26:43.900 --> 01:26:49.980]   So this was wild on the 24th of June, two days ago.
[01:26:49.980 --> 01:26:54.460]   I saw I started seeing tweets of cloud flare was gone down and I thought, oh my God, cloud
[01:26:54.460 --> 01:26:56.980]   flare, that can't go down.
[01:26:56.980 --> 01:27:04.380]   Cloud flare provides data protection and uptime protection for much of the internet.
[01:27:04.380 --> 01:27:11.260]   And cloud flare had a post on this and it turns out it was Verizon's fault.
[01:27:11.260 --> 01:27:17.340]   Well, no, somebody else's fault, but Verizon was like, they certified it.
[01:27:17.340 --> 01:27:19.620]   Basically was.
[01:27:19.620 --> 01:27:25.780]   So essentially what happened is there's a border gateway protocol, BGP routers where
[01:27:25.780 --> 01:27:33.740]   you you essentially broadcast the internet, an internet route to the world normally.
[01:27:33.740 --> 01:27:38.380]   If it's done properly, you broadcast a route that is just for traffic to you.
[01:27:38.380 --> 01:27:43.660]   And you say, here's how traffic gets to me or to the people I serve.
[01:27:43.660 --> 01:27:52.860]   Unfortunately, a small steel company in Allegheny, Pennsylvania, Allegheny Technologies, Incorporated.
[01:27:52.860 --> 01:28:00.340]   Now I'm not sure who specified these routes, but these routes were broadcast that all the
[01:28:00.340 --> 01:28:06.340]   internet should go through Allegheny technologies in Pennsylvania.
[01:28:06.340 --> 01:28:08.100]   How do they do that?
[01:28:08.100 --> 01:28:14.460]   Well, this is a known massive problem, frankly, with the way the internet works.
[01:28:14.460 --> 01:28:19.060]   This has been happening for decades.
[01:28:19.060 --> 01:28:24.820]   Didn't Poland recent like a year ago, Poland did it?
[01:28:24.820 --> 01:28:26.300]   Yeah.
[01:28:26.300 --> 01:28:30.500]   There are protections that could be introduced to prevent it.
[01:28:30.500 --> 01:28:32.540]   Verizon does not.
[01:28:32.540 --> 01:28:37.100]   One of the protections would of course be to notice if a massive amount of traffic was
[01:28:37.100 --> 01:28:40.260]   coming through your network.
[01:28:40.260 --> 01:28:45.940]   About 15% of CloudFlare, because CloudFlare was on this route, CloudFlare's traffic got
[01:28:45.940 --> 01:28:52.900]   routed instead of going to where it was supposed to go, got routed to the leaky BGP process
[01:28:52.900 --> 01:28:56.100]   on Verizon and then went to Allegheny Technologies.
[01:28:56.100 --> 01:28:58.420]   So did Allegheny do this?
[01:28:58.420 --> 01:29:00.540]   Did they reconfigure something?
[01:29:00.540 --> 01:29:01.540]   I think it's Verizon.
[01:29:01.540 --> 01:29:02.540]   It's said they did.
[01:29:02.540 --> 01:29:04.700]   So Allegheny made the original mistake.
[01:29:04.700 --> 01:29:10.300]   But what happened is they sent their original mistake up to Verizon and Verizon.
[01:29:10.300 --> 01:29:11.300]   Rubber station.
[01:29:11.300 --> 01:29:12.300]   Right.
[01:29:12.300 --> 01:29:15.780]   Which would presumably be better at this than Allegheny.
[01:29:15.780 --> 01:29:16.780]   Yeah.
[01:29:16.780 --> 01:29:18.540]   Allegheny is an internet service provider.
[01:29:18.540 --> 01:29:19.540]   They...
[01:29:19.540 --> 01:29:20.780]   So why are Russians do this all the time?
[01:29:20.780 --> 01:29:22.740]   Yeah, it would be a good way to take down the net.
[01:29:22.740 --> 01:29:25.020]   Well, because unlike Verizon...
[01:29:25.020 --> 01:29:27.540]   Most people catch it.
[01:29:27.540 --> 01:29:33.340]   Most of these people, by the way, the stunning thing is CloudFlare tried to reach Verizon
[01:29:33.340 --> 01:29:38.340]   and they have pictures in their blog post of all of the emails, all of the pager duties,
[01:29:38.340 --> 01:29:39.340]   all of the calls.
[01:29:39.340 --> 01:29:43.980]   Verizon, I guess a little too busy to respond.
[01:29:43.980 --> 01:29:46.540]   Route leak from your customer, all of this stuff.
[01:29:46.540 --> 01:29:53.700]   So what CloudFlare says is Verizon and everybody who does this kind of thing should be implementing
[01:29:53.700 --> 01:29:58.620]   a framework that called RPKI, which is very simple.
[01:29:58.620 --> 01:30:04.460]   If a BGP session could be configured with a hard limit of prefixes to be received, if
[01:30:04.460 --> 01:30:12.140]   all of a sudden the entire internet comes a knock-in, you should shut that route down.
[01:30:12.140 --> 01:30:15.380]   CloudFlare writes, "Had Verizon had such a prefix limit in place, this would not have
[01:30:15.380 --> 01:30:16.860]   occurred.
[01:30:16.860 --> 01:30:19.060]   It is a best practice to have such limits in place.
[01:30:19.060 --> 01:30:21.300]   It doesn't cost a provider anything.
[01:30:21.300 --> 01:30:22.300]   And there's good...
[01:30:22.300 --> 01:30:25.140]   This is my favorite line in this blog post.
[01:30:25.140 --> 01:30:30.620]   And there's no good reason other than sloppiness or laziness that Verizon wouldn't have such
[01:30:30.620 --> 01:30:31.620]   limits in place."
[01:30:31.620 --> 01:30:33.620]   Or possibly both.
[01:30:33.620 --> 01:30:34.620]   Or both.
[01:30:34.620 --> 01:30:38.660]   And there's also a filtering system that you could use.
[01:30:38.660 --> 01:30:39.660]   Or hit greek.
[01:30:39.660 --> 01:30:41.460]   Well, it's not even greek.
[01:30:41.460 --> 01:30:42.700]   It's not greek.
[01:30:42.700 --> 01:30:48.420]   I think the challenge here, or a challenge here, is CloudFlare had the luxury of building
[01:30:48.420 --> 01:30:49.860]   up their infrastructure.
[01:30:49.860 --> 01:30:52.380]   CloudFlare literally designed its own servers, right?
[01:30:52.380 --> 01:30:55.620]   This is a true cloudy internet company.
[01:30:55.620 --> 01:31:00.180]   Whereas Verizon's a telco that has all this old line infrastructure.
[01:31:00.180 --> 01:31:02.300]   And they make so little money.
[01:31:02.300 --> 01:31:03.300]   I'm just...
[01:31:03.300 --> 01:31:04.300]   I'm just...
[01:31:04.300 --> 01:31:08.580]   They're mindset is not...
[01:31:08.580 --> 01:31:12.300]   It's not proactive or even...
[01:31:12.300 --> 01:31:16.940]   They're not seeking to optimize the way internet companies are, because they don't have to.
[01:31:16.940 --> 01:31:19.340]   Until stuff like this happens.
[01:31:19.340 --> 01:31:20.340]   Or even...
[01:31:20.340 --> 01:31:21.340]   Yeah.
[01:31:21.340 --> 01:31:22.340]   It's humorous.
[01:31:22.340 --> 01:31:25.060]   But it's ingrained in their culture.
[01:31:25.060 --> 01:31:26.060]   It's been known that way.
[01:31:26.060 --> 01:31:27.060]   It's impossible.
[01:31:27.060 --> 01:31:29.060]   Like that's not a document.
[01:31:29.060 --> 01:31:30.060]   But...
[01:31:30.060 --> 01:31:31.060]   But internet is very...
[01:31:31.060 --> 01:31:32.860]   It's surprisingly fragile to be honest.
[01:31:32.860 --> 01:31:34.220]   Just a series of tubes.
[01:31:34.220 --> 01:31:35.220]   Yeah.
[01:31:35.220 --> 01:31:36.220]   Yeah.
[01:31:36.220 --> 01:31:37.220]   So what a story though.
[01:31:37.220 --> 01:31:38.220]   I mean, just what a wild story.
[01:31:38.220 --> 01:31:42.540]   It was three hours before a DQE fixed the problem for Verizon.
[01:31:42.540 --> 01:31:44.380]   And I remember reading about that.
[01:31:44.380 --> 01:31:49.020]   That's a cloudflare problems, but I didn't look into what happens.
[01:31:49.020 --> 01:31:50.020]   Yeah.
[01:31:50.020 --> 01:31:51.580]   I understand RPKI.
[01:31:51.580 --> 01:31:56.820]   So I'm like, I really want to dig into this a little bit more.
[01:31:56.820 --> 01:31:58.980]   We're big fans of cloudflare.
[01:31:58.980 --> 01:31:59.980]   They've been...
[01:31:59.980 --> 01:32:01.980]   You could learn about it and then tell us about it.
[01:32:01.980 --> 01:32:02.980]   That would be great.
[01:32:02.980 --> 01:32:03.980]   Yeah.
[01:32:03.980 --> 01:32:04.980]   I mean, I can.
[01:32:04.980 --> 01:32:05.980]   I love...
[01:32:05.980 --> 01:32:11.580]   I mean, cloudflare to me is one of those companies, like Amazon, that when you talk to them,
[01:32:11.580 --> 01:32:15.940]   they see the way the internet is growing and changing.
[01:32:15.940 --> 01:32:20.060]   And they're trying to solve problems that are going to appear three to four years out.
[01:32:20.060 --> 01:32:22.260]   So I always love talking to them.
[01:32:22.260 --> 01:32:26.340]   They say AT&T have already enabled us successfully on their network, so they're not going to
[01:32:26.340 --> 01:32:28.140]   be the next one to do this.
[01:32:28.140 --> 01:32:31.620]   And I think there's a little pressure probably on Verizon to do it.
[01:32:31.620 --> 01:32:34.860]   It basically filters the origin network and the prefix size.
[01:32:34.860 --> 01:32:36.380]   So if you advertise a non...
[01:32:36.380 --> 01:32:40.700]   Basically, it's a sanity check on the route that you're advertising.
[01:32:40.700 --> 01:32:46.140]   If you're advertising a nonsense route, it'll be blocked or stopped.
[01:32:46.140 --> 01:32:49.260]   No, this isn't going to happen.
[01:32:49.260 --> 01:32:50.860]   So that's a simple way to...
[01:32:50.860 --> 01:32:53.260]   There's actually many simple ways to do this.
[01:32:53.260 --> 01:32:56.340]   The fact that Verizon didn't is kind of shocking.
[01:32:56.340 --> 01:32:57.740]   How long was it out?
[01:32:57.740 --> 01:32:58.740]   Three hours.
[01:32:58.740 --> 01:33:00.540]   Not completely out.
[01:33:00.540 --> 01:33:01.740]   Same single two calendar.
[01:33:01.740 --> 01:33:02.740]   Yeah.
[01:33:02.740 --> 01:33:05.580]   Now we don't know what happened with Google Calendar, but that was...
[01:33:05.580 --> 01:33:07.180]   That was lovely.
[01:33:07.180 --> 01:33:08.180]   I loved it.
[01:33:08.180 --> 01:33:11.660]   I really not go to work because they're calendaring until they had to.
[01:33:11.660 --> 01:33:12.660]   Because that was the story.
[01:33:12.660 --> 01:33:16.820]   You know how to do conference calls as old a conference call data was in the calendar?
[01:33:16.820 --> 01:33:17.820]   Well, that's true.
[01:33:17.820 --> 01:33:18.820]   That's where I put meetings.
[01:33:18.820 --> 01:33:20.500]   Yeah, no, that's a good point.
[01:33:20.500 --> 01:33:21.500]   I put it in the calendar.
[01:33:21.500 --> 01:33:24.380]   But, but here's the other things that broke.
[01:33:24.380 --> 01:33:26.260]   Far more important than all of this.
[01:33:26.260 --> 01:33:27.260]   What?
[01:33:27.260 --> 01:33:28.260]   Far more irritating.
[01:33:28.260 --> 01:33:29.260]   What?
[01:33:29.260 --> 01:33:33.820]   Autocomplete in Chrome is borked.
[01:33:33.820 --> 01:33:36.020]   I did not notice, but wow.
[01:33:36.020 --> 01:33:41.700]   So if I start typing WA, it does WashingtonPost.com and I go there, right?
[01:33:41.700 --> 01:33:44.660]   Now I have to type out WashingtonPost.com before it gets it.
[01:33:44.660 --> 01:33:45.660]   Oh, no.
[01:33:45.660 --> 01:33:46.660]   Oh, my God.
[01:33:46.660 --> 01:33:47.660]   Okay.
[01:33:47.660 --> 01:33:48.660]   So I'm doing it right now.
[01:33:48.660 --> 01:33:49.660]   WA, you're right.
[01:33:49.660 --> 01:33:50.660]   Doesn't do it.
[01:33:50.660 --> 01:33:51.660]   S does.
[01:33:51.660 --> 01:33:54.300]   But that's because I've been to the Washington Post recently.
[01:33:54.300 --> 01:33:55.300]   Maybe.
[01:33:55.300 --> 01:33:58.660]   They put out a fix that I don't think I have in Chrome West yet.
[01:33:58.660 --> 01:33:59.660]   Jarvis is...
[01:33:59.660 --> 01:34:00.660]   Autocomplete for domain.
[01:34:00.660 --> 01:34:01.660]   Hey.
[01:34:01.660 --> 01:34:02.660]   It's a problem.
[01:34:02.660 --> 01:34:07.660]   It's a problem.
[01:34:07.660 --> 01:34:09.060]   It's autocomplete for domains.
[01:34:09.060 --> 01:34:10.060]   It's a problem.
[01:34:10.060 --> 01:34:12.300]   By the way, they fixed that so you can't type it.
[01:34:12.300 --> 01:34:13.300]   It used to be able...
[01:34:13.300 --> 01:34:14.300]   Let me see.
[01:34:14.300 --> 01:34:15.620]   Leo Laport is a...
[01:34:15.620 --> 01:34:17.020]   Used to be able to type that, right?
[01:34:17.020 --> 01:34:18.020]   And find out.
[01:34:18.020 --> 01:34:19.020]   Right.
[01:34:19.020 --> 01:34:20.020]   And they've stopped it.
[01:34:20.020 --> 01:34:21.020]   Just for people.
[01:34:21.020 --> 01:34:22.020]   Wait a minute.
[01:34:22.020 --> 01:34:23.020]   Did you see how fast that happened?
[01:34:23.020 --> 01:34:24.020]   They showed it.
[01:34:24.020 --> 01:34:25.020]   Wow.
[01:34:25.020 --> 01:34:27.020]   No, we're not going to show you that.
[01:34:27.020 --> 01:34:28.020]   Whoa.
[01:34:28.020 --> 01:34:29.020]   Whoa.
[01:34:29.020 --> 01:34:30.020]   Whoa.
[01:34:30.020 --> 01:34:31.180]   That's pretty funny.
[01:34:31.180 --> 01:34:34.260]   So there's something going on there.
[01:34:34.260 --> 01:34:35.260]   Let's see.
[01:34:35.260 --> 01:34:36.260]   Stacey...
[01:34:36.260 --> 01:34:38.180]   But it's still...
[01:34:38.180 --> 01:34:40.020]   Leo, it's still means.
[01:34:40.020 --> 01:34:41.260]   Yeah, no, domain completion.
[01:34:41.260 --> 01:34:42.260]   Yeah.
[01:34:42.260 --> 01:34:43.260]   But now Leo's having fun.
[01:34:43.260 --> 01:34:44.260]   Well, I'm having too much fun.
[01:34:44.260 --> 01:34:45.260]   So I'm right here.
[01:34:45.260 --> 01:34:47.500]   So it takes three letters for me.
[01:34:47.500 --> 01:34:49.220]   Three or four letters.
[01:34:49.220 --> 01:34:50.820]   I think you have the fix.
[01:34:50.820 --> 01:34:52.660]   I can't do that still.
[01:34:52.660 --> 01:34:53.660]   Okay.
[01:34:53.660 --> 01:34:55.660]   You should try these things called bookmarks.
[01:34:55.660 --> 01:34:57.660]   You just click on a little.
[01:34:57.660 --> 01:35:01.140]   Oh, I have 15 years of bookmark.
[01:35:01.140 --> 01:35:02.140]   Yes.
[01:35:02.140 --> 01:35:04.340]   Oh, I just deleted my bookmark today.
[01:35:04.340 --> 01:35:05.340]   It was so...
[01:35:05.340 --> 01:35:06.340]   Wait a minute.
[01:35:06.340 --> 01:35:07.340]   Why did you delete your bookmarks?
[01:35:07.340 --> 01:35:08.340]   Oh, I...
[01:35:08.340 --> 01:35:10.340]   I cleaned up my bookmarks.
[01:35:10.340 --> 01:35:11.340]   Oh.
[01:35:11.340 --> 01:35:12.340]   Yeah.
[01:35:12.340 --> 01:35:13.340]   So now I only have six bookmarks.
[01:35:13.340 --> 01:35:14.340]   What?
[01:35:14.340 --> 01:35:15.340]   I'm a contour fanatic.
[01:35:15.340 --> 01:35:16.340]   I know.
[01:35:16.340 --> 01:35:17.340]   I know.
[01:35:17.340 --> 01:35:18.980]   You want to see how cray-cray my bookmarks are?
[01:35:18.980 --> 01:35:21.100]   This is my news folder.
[01:35:21.100 --> 01:35:22.820]   This is my Leo folder.
[01:35:22.820 --> 01:35:24.780]   This is my social...
[01:35:24.780 --> 01:35:25.780]   There's just...
[01:35:25.780 --> 01:35:26.780]   This is my favorite one.
[01:35:26.780 --> 01:35:27.780]   Programming.
[01:35:27.780 --> 01:35:31.100]   Within programming, there's many folders, a folder per language.
[01:35:31.100 --> 01:35:33.780]   With many links within each language.
[01:35:33.780 --> 01:35:36.900]   Go, go, go, go, baby.
[01:35:36.900 --> 01:35:37.900]   This is a...
[01:35:37.900 --> 01:35:39.260]   What's wrong with this?
[01:35:39.260 --> 01:35:40.260]   What is that?
[01:35:40.260 --> 01:35:41.260]   What is wrong with?
[01:35:41.260 --> 01:35:42.260]   This is my list folder.
[01:35:42.260 --> 01:35:43.260]   That's nothing wrong with that.
[01:35:43.260 --> 01:35:45.060]   That doesn't occupy any space.
[01:35:45.060 --> 01:35:46.460]   It's just a little folder there.
[01:35:46.460 --> 01:35:47.460]   I know.
[01:35:47.460 --> 01:35:48.460]   It's just...
[01:35:48.460 --> 01:35:50.420]   I mean, the point of bookmarks is to get to something quickly.
[01:35:50.420 --> 01:35:52.300]   So if it's cluttered, you can't get to it quickly.
[01:35:52.300 --> 01:35:53.300]   Well, it's not cluttered.
[01:35:53.300 --> 01:35:54.300]   See, my toolbar.
[01:35:54.300 --> 01:35:55.300]   I don't even go to it.
[01:35:55.300 --> 01:35:57.020]   Do you just use a toolbar?
[01:35:57.020 --> 01:35:58.020]   Folders is the secret.
[01:35:58.020 --> 01:35:59.020]   Yeah.
[01:35:59.020 --> 01:36:00.020]   I'll just use a toolbar.
[01:36:00.020 --> 01:36:01.020]   Yeah.
[01:36:01.020 --> 01:36:03.620]   I have a folder for recipes, but that's about it.
[01:36:03.620 --> 01:36:04.620]   Yeah.
[01:36:04.620 --> 01:36:06.700]   I think I should probably look into that.
[01:36:06.700 --> 01:36:08.700]   What works are your resources?
[01:36:08.700 --> 01:36:09.700]   I just search.
[01:36:09.700 --> 01:36:10.700]   I know.
[01:36:10.700 --> 01:36:11.700]   I just...
[01:36:11.700 --> 01:36:14.060]   If I can remember two words about it, I just search.
[01:36:14.060 --> 01:36:15.900]   Yeah, that's me.
[01:36:15.900 --> 01:36:17.980]   That's a good point.
[01:36:17.980 --> 01:36:18.980]   I know.
[01:36:18.980 --> 01:36:19.980]   I trust...
[01:36:19.980 --> 01:36:20.980]   Who's Josh Ben?
[01:36:20.980 --> 01:36:21.980]   What about him?
[01:36:21.980 --> 01:36:22.980]   Josh Ben.
[01:36:22.980 --> 01:36:23.980]   This is the main major...
[01:36:23.980 --> 01:36:24.980]   Even lab.
[01:36:24.980 --> 01:36:25.980]   Oh, yeah.
[01:36:25.980 --> 01:36:30.980]   And he talks about how he literally has 500...
[01:36:30.980 --> 01:36:33.620]   500 tabs in the Chrome browser.
[01:36:33.620 --> 01:36:35.020]   I've never understood that.
[01:36:35.020 --> 01:36:40.060]   So Mike Masnick, a tector, I think is in the four to 500 range at all times.
[01:36:40.060 --> 01:36:41.060]   Wow, I have to talk.
[01:36:41.060 --> 01:36:42.060]   Wow, I have to talk.
[01:36:42.060 --> 01:36:43.060]   100 gigs of RAM.
[01:36:43.060 --> 01:36:44.060]   I don't know how.
[01:36:44.060 --> 01:36:45.540]   Yeah, because every tab in Google is...
[01:36:45.540 --> 01:36:46.540]   Yeah.
[01:36:46.540 --> 01:36:47.540]   ...promise.
[01:36:47.540 --> 01:36:48.540]   It's a...
[01:36:48.540 --> 01:36:49.540]   It's a...
[01:36:49.540 --> 01:36:50.540]   ...process.
[01:36:50.540 --> 01:36:51.540]   Yeah.
[01:36:51.540 --> 01:36:55.540]   People joke that's why Apple made a new computer with 1.4 terabytes of RAM so you could have...
[01:36:55.540 --> 01:36:57.540]   Actually, you could read Chrome.
[01:36:57.540 --> 01:36:58.940]   You could call it your tabs open.
[01:36:58.940 --> 01:37:00.220]   I've never been a tab person.
[01:37:00.220 --> 01:37:01.980]   I close tabs when I'm done with them.
[01:37:01.980 --> 01:37:02.980]   Oh my gosh.
[01:37:02.980 --> 01:37:03.980]   I have...
[01:37:03.980 --> 01:37:05.180]   Right now I'm good.
[01:37:05.180 --> 01:37:06.700]   I only have 12 of it.
[01:37:06.700 --> 01:37:09.500]   So you really should be crowing about your bookmark, I mean...
[01:37:09.500 --> 01:37:10.500]   I'm not...
[01:37:10.500 --> 01:37:13.460]   Instead of bookmarking stuff, you just leave the tab open.
[01:37:13.460 --> 01:37:15.900]   Well, no, I shut them down at the end of the day.
[01:37:15.900 --> 01:37:16.900]   Oh, okay.
[01:37:16.900 --> 01:37:17.900]   Oh, you do?
[01:37:17.900 --> 01:37:18.900]   I don't know.
[01:37:18.900 --> 01:37:19.900]   Yeah.
[01:37:19.900 --> 01:37:20.900]   Well, I leave...
[01:37:20.900 --> 01:37:21.900]   Okay, that's not true.
[01:37:21.900 --> 01:37:22.900]   I shouldn't say that.
[01:37:22.900 --> 01:37:23.900]   I shut most of them down.
[01:37:23.900 --> 01:37:28.540]   I will only leave like two or three in addition to email overnight.
[01:37:28.540 --> 01:37:34.220]   The only time I shut down a tab is when it's auto playing video with the audio.
[01:37:34.220 --> 01:37:39.500]   Every time you close a tab an angel gets its wings.
[01:37:39.500 --> 01:37:40.500]   I believe that.
[01:37:40.500 --> 01:37:41.500]   Yes.
[01:37:41.500 --> 01:37:42.500]   Just remember that.
[01:37:42.500 --> 01:37:47.940]   640K of RAMs would be plenty.
[01:37:47.940 --> 01:37:51.700]   Anything else you guys want to talk about because I'm talked out here.
[01:37:51.700 --> 01:37:52.700]   Anything...
[01:37:52.700 --> 01:37:54.060]   Do we want it?
[01:37:54.060 --> 01:37:57.340]   Talk about sidewalk labs or...
[01:37:57.340 --> 01:37:58.340]   Yeah, yeah, yeah, yeah.
[01:37:58.340 --> 01:37:59.940]   You were interested about this.
[01:37:59.940 --> 01:38:00.940]   Yeah.
[01:38:00.940 --> 01:38:01.940]   So this is actually in Toronto.
[01:38:01.940 --> 01:38:02.940]   We have to do it.
[01:38:02.940 --> 01:38:03.940]   This is a...
[01:38:03.940 --> 01:38:04.940]   This is a Toronto.
[01:38:04.940 --> 01:38:07.820]   They want to build a smart city in the old railroad yards, right?
[01:38:07.820 --> 01:38:09.380]   Or something like that.
[01:38:09.380 --> 01:38:10.860]   It's a down by the port.
[01:38:10.860 --> 01:38:11.860]   It's on the key.
[01:38:11.860 --> 01:38:12.860]   The key.
[01:38:12.860 --> 01:38:13.860]   It's a port.
[01:38:13.860 --> 01:38:14.860]   What is that called?
[01:38:14.860 --> 01:38:15.860]   Keyside.
[01:38:15.860 --> 01:38:16.860]   Keyside.
[01:38:16.860 --> 01:38:17.860]   Yeah, it's Keyside.
[01:38:17.860 --> 01:38:18.860]   Keys with it.
[01:38:18.860 --> 01:38:19.860]   These whiny Canadians don't know how good they're getting.
[01:38:19.860 --> 01:38:21.780]   I actually like Keyside Toronto.
[01:38:21.780 --> 01:38:23.820]   I think that's kind of neat area.
[01:38:23.820 --> 01:38:27.300]   Is that where you get the boats to the airport, right?
[01:38:27.300 --> 01:38:28.300]   No.
[01:38:28.300 --> 01:38:37.220]   But you seem to be able to get a boat to Rochester, but nobody wanted to go there.
[01:38:37.220 --> 01:38:43.640]   So sidewalk labs are just a Google thing and Toronto, waterfront Toronto, announced a
[01:38:43.640 --> 01:38:49.780]   plan to take this area and redevelop it, but make it a smart city.
[01:38:49.780 --> 01:38:53.980]   Which did, of course, stimulate some protest because the smart city meant it would gather
[01:38:53.980 --> 01:38:56.620]   data about you, about traffic, about where people go.
[01:38:56.620 --> 01:39:00.940]   What gathered, what generated the protest wasn't that.
[01:39:00.940 --> 01:39:07.740]   It was that basically there's a separate development corporation that was formed to work with Google
[01:39:07.740 --> 01:39:11.180]   or work with sidewalk labs on this.
[01:39:11.180 --> 01:39:15.660]   So sidewalk labs and alphabet company, just like Google is.
[01:39:15.660 --> 01:39:21.900]   And they basically came up with a plan and they had no actual input from any of the citizenry
[01:39:21.900 --> 01:39:23.300]   who was going to live there.
[01:39:23.300 --> 01:39:28.900]   And so the protests, I think, were actually very legit in the sense that, wait, why are
[01:39:28.900 --> 01:39:30.620]   you, what is happening here?
[01:39:30.620 --> 01:39:33.900]   We live here and you're doing this deal with Google and you're treating it like a real
[01:39:33.900 --> 01:39:38.420]   estate deal, but it's really going to put a lot of sensors in and gather all the state.
[01:39:38.420 --> 01:39:40.140]   No, wait a minute.
[01:39:40.140 --> 01:39:41.140]   Nobody lives there, right?
[01:39:41.140 --> 01:39:42.140]   Nobody lives there.
[01:39:42.140 --> 01:39:44.140]   Well, nobody lives there yet.
[01:39:44.140 --> 01:39:48.740]   Well, if you don't want to live there, there's a place down the road you could live.
[01:39:48.740 --> 01:39:51.980]   Well, but there's also, I mean, there's going to be businesses located there.
[01:39:51.980 --> 01:39:55.580]   I mean, well, only by choice.
[01:39:55.580 --> 01:40:00.660]   But the principle is still so the principle was who controls the data?
[01:40:00.660 --> 01:40:01.660]   Is it Google?
[01:40:01.660 --> 01:40:03.820]   Does Google get access to that data?
[01:40:03.820 --> 01:40:05.620]   Does the province get access to that data?
[01:40:05.620 --> 01:40:10.500]   Does the federal government get access to other companies get access to that data?
[01:40:10.500 --> 01:40:12.020]   And it's not clear.
[01:40:12.020 --> 01:40:14.340]   So all the benefits are clear.
[01:40:14.340 --> 01:40:19.180]   All everything would run perfectly and you'd have hot and cold running everything and
[01:40:19.180 --> 01:40:20.180]   automated cars.
[01:40:20.180 --> 01:40:22.540]   And if you don't like it, they'll live there.
[01:40:22.540 --> 01:40:23.540]   Right.
[01:40:23.540 --> 01:40:28.420]   But then the data on your usage, which Google says it would use to make things better, who
[01:40:28.420 --> 01:40:35.060]   controls it, who owns it, who gets to give it to others or use it in different ways.
[01:40:35.060 --> 01:40:40.540]   Given the choice between the province and Google, I think I do Google.
[01:40:40.540 --> 01:40:46.100]   So, yes, look with the CBP, the customer's in border protection.
[01:40:46.100 --> 01:40:48.100]   The province is right now.
[01:40:48.100 --> 01:40:49.100]   It's Ford.
[01:40:49.100 --> 01:40:51.300]   He's a nut top there.
[01:40:51.300 --> 01:40:52.300]   So that's fair.
[01:40:52.300 --> 01:40:55.020]   But I don't trust the province as far as I can throw it.
[01:40:55.020 --> 01:40:56.020]   Google at least has a.
[01:40:56.020 --> 01:41:00.660]   But that's a democratic process and you can change the rules associated with it.
[01:41:00.660 --> 01:41:03.180]   And Google isn't a democratic process.
[01:41:03.180 --> 01:41:07.380]   And remember what we talked about with ambient privacy and it's easy to say, don't go there.
[01:41:07.380 --> 01:41:12.260]   But it's hard to say that if a government office is based there, or maybe they put a
[01:41:12.260 --> 01:41:14.860]   school there or your kids daycare is there.
[01:41:14.860 --> 01:41:18.420]   I mean, but you're the one who said you should judge.
[01:41:18.420 --> 01:41:24.260]   What the outcome by who profits, what the business motive is.
[01:41:24.260 --> 01:41:25.260]   And my.
[01:41:25.260 --> 01:41:29.820]   No, I said that you should judge it based on the outcome and how it affects those, how
[01:41:29.820 --> 01:41:30.820]   it affects people.
[01:41:30.820 --> 01:41:31.820]   Yeah, because you can understand.
[01:41:31.820 --> 01:41:33.820]   How it affects people who don't have power.
[01:41:33.820 --> 01:41:34.820]   Oh, okay.
[01:41:34.820 --> 01:41:36.380]   Maybe it wasn't you who said it.
[01:41:36.380 --> 01:41:40.540]   But somebody said that the best way to judge whether your data was safe with the company
[01:41:40.540 --> 01:41:43.420]   is what the company's profit, you know.
[01:41:43.420 --> 01:41:45.220]   Oh, that's what I said about Amazon.
[01:41:45.220 --> 01:41:46.220]   Yes.
[01:41:46.220 --> 01:41:47.220]   I understand it's profit motive.
[01:41:47.220 --> 01:41:49.900]   Amazon's not collecting my data outside.
[01:41:49.900 --> 01:41:52.940]   Amazon's collecting my data when I interact with it.
[01:41:52.940 --> 01:41:55.340]   This is a different level of interaction.
[01:41:55.340 --> 01:41:59.380]   This is walking down a city street or somebody look at whether it's I mean, even if you're
[01:41:59.380 --> 01:42:04.580]   not in Keysight, somebody's collecting your the province is collecting your data all the
[01:42:04.580 --> 01:42:05.580]   time.
[01:42:05.580 --> 01:42:06.580]   They've got cameras everywhere.
[01:42:06.580 --> 01:42:11.540]   They're collecting data all the time, but they have they usually have rules associated
[01:42:11.540 --> 01:42:13.140]   with how that data gets seen.
[01:42:13.140 --> 01:42:17.460]   But my point being that a they may have rules which change all the time depending on who
[01:42:17.460 --> 01:42:22.100]   gets elected and who's corrupt and be they don't have any security policy.
[01:42:22.100 --> 01:42:25.540]   Look what happened to customs and border protection.
[01:42:25.540 --> 01:42:29.620]   Hackers stole license plate images, travelers IDs and now as we learn more.
[01:42:29.620 --> 01:42:32.020]   So they stole that though from a third party.
[01:42:32.020 --> 01:42:33.020]   From a provider.
[01:42:33.020 --> 01:42:36.820]   A third party that had the data because CPB let them have the data.
[01:42:36.820 --> 01:42:37.820]   Right.
[01:42:37.820 --> 01:42:39.380]   But that Sony was victim of a similar.
[01:42:39.380 --> 01:42:40.380]   I understand.
[01:42:40.380 --> 01:42:42.420]   I'm just saying who you trust with your data.
[01:42:42.420 --> 01:42:46.980]   Google or the province of Ontario.
[01:42:46.980 --> 01:42:47.980]   So that's a fair point.
[01:42:47.980 --> 01:42:51.780]   I mean, it's not as though it's not as though it's there's an easy answer.
[01:42:51.780 --> 01:42:54.820]   But I think Google knows more about security than Mr. Ford.
[01:42:54.820 --> 01:42:56.620]   The answer is what?
[01:42:56.620 --> 01:42:58.620]   Toronto, if you want to whine about it fine.
[01:42:58.620 --> 01:43:00.100]   Google go to Newark, New Jersey.
[01:43:00.100 --> 01:43:01.420]   They could use this.
[01:43:01.420 --> 01:43:03.020]   Toronto is in great shape.
[01:43:03.020 --> 01:43:04.860]   Real estate costs are insane.
[01:43:04.860 --> 01:43:07.940]   Go to a city that same with Amazon and their damned headquarters.
[01:43:07.940 --> 01:43:08.940]   Go to Newark, New Jersey.
[01:43:08.940 --> 01:43:11.380]   Go to a place that needs it.
[01:43:11.380 --> 01:43:13.380]   Everybody wants those parents data.
[01:43:13.380 --> 01:43:14.380]   Yeah.
[01:43:14.380 --> 01:43:17.220]   That's kind of where yeah, that's the progress.
[01:43:17.220 --> 01:43:18.220]   That's hard.
[01:43:18.220 --> 01:43:19.580]   I mean, it is.
[01:43:19.580 --> 01:43:24.820]   I mean, you know, it's a candid benefit from better traffic management or safer policing
[01:43:24.820 --> 01:43:25.820]   tactics.
[01:43:25.820 --> 01:43:32.300]   To me, there's a happy medium there somewhere where Google agrees to share data and there's
[01:43:32.300 --> 01:43:38.100]   a, you know, there's a sort of arms length entity that includes Google, but also includes
[01:43:38.100 --> 01:43:39.100]   the province.
[01:43:39.100 --> 01:43:41.180]   I mean, this is not an unsolvable problem.
[01:43:41.180 --> 01:43:43.740]   And I think you probably could learn a lot.
[01:43:43.740 --> 01:43:47.140]   You could learn a lot from that city, from people's behavior.
[01:43:47.140 --> 01:43:51.380]   You could learn things that would improve other cities that would make things easier,
[01:43:51.380 --> 01:43:52.380]   that would.
[01:43:52.380 --> 01:43:56.260]   And they've created a third party to deal with.
[01:43:56.260 --> 01:43:58.060]   I can't think of the name of it right now.
[01:43:58.060 --> 01:43:59.060]   What is it called?
[01:43:59.060 --> 01:44:00.060]   Hold on.
[01:44:00.060 --> 01:44:02.660]   The urban data trust.
[01:44:02.660 --> 01:44:03.740]   I think the issue.
[01:44:03.740 --> 01:44:09.100]   One of the issues in the plan that they released though was it's still not clear who's
[01:44:09.100 --> 01:44:15.620]   in the urban data trust, what they, how they can protect the public's interest in elements
[01:44:15.620 --> 01:44:16.620]   associated.
[01:44:16.620 --> 01:44:20.460]   Like a lot of this has come only with people pushing Google.
[01:44:20.460 --> 01:44:21.460]   Yes.
[01:44:21.460 --> 01:44:26.340]   Only with, you know, the former privacy commissioner resigning and saying this is not a workable
[01:44:26.340 --> 01:44:27.340]   model.
[01:44:27.340 --> 01:44:32.500]   And I think Google in the same way they and a lot of other companies do, they thought,
[01:44:32.500 --> 01:44:36.060]   hey, we're going to give you a bunch of free stuff for your city.
[01:44:36.060 --> 01:44:37.660]   You should be thanking us.
[01:44:37.660 --> 01:44:41.740]   You know, it's obviously going to be good because we're Google and we're great instead
[01:44:41.740 --> 01:44:47.620]   of, instead of kind of trying to meet some of these questions head on, they had to be
[01:44:47.620 --> 01:44:50.140]   dragged into it.
[01:44:50.140 --> 01:44:56.100]   You know, in 1994, Walt Disney, the company, not the person, built a town in Florida called
[01:44:56.100 --> 01:45:00.300]   Celebration, a planned community.
[01:45:00.300 --> 01:45:08.620]   It's by the way, it's 88% white in an area where it's only 59% white.
[01:45:08.620 --> 01:45:10.100]   But so there are other issues.
[01:45:10.100 --> 01:45:13.820]   There are people who don't like the fact that everything's planned down to the manhole
[01:45:13.820 --> 01:45:16.020]   covers.
[01:45:16.020 --> 01:45:21.180]   But there are people who are very happy to live there.
[01:45:21.180 --> 01:45:25.180]   And then there are some people who say it's kind of creepy, but honestly.
[01:45:25.180 --> 01:45:28.340]   It's the good, it's the good place.
[01:45:28.340 --> 01:45:31.140]   It's exactly what it is.
[01:45:31.140 --> 01:45:36.780]   I imagine Disney got a lot of information about people who lived there and all that, right?
[01:45:36.780 --> 01:45:37.780]   I don't know.
[01:45:37.780 --> 01:45:41.220]   I mean, they built that in 1947, 49.
[01:45:41.220 --> 01:45:42.220]   No, 94.
[01:45:42.220 --> 01:45:43.220]   Oh, 94.
[01:45:43.220 --> 01:45:44.220]   Yeah.
[01:45:44.220 --> 01:45:45.220]   Okay.
[01:45:45.220 --> 01:45:48.820]   Oh, I know they're collecting data.
[01:45:48.820 --> 01:45:50.100]   So is, yeah, it's not the picture.
[01:45:50.100 --> 01:45:52.060]   Why is there a sheriff's line there in the picture?
[01:45:52.060 --> 01:45:54.900]   It was a crime scene of the first ever murder.
[01:45:54.900 --> 01:45:58.780]   Oh, that's too young, but the houses are falling apart.
[01:45:58.780 --> 01:46:02.340]   This was the first ever murder of 14 years after it was built.
[01:46:02.340 --> 01:46:03.340]   Who is it?
[01:46:03.340 --> 01:46:04.340]   This is interesting.
[01:46:04.340 --> 01:46:05.340]   Yeah, this is Gizmodo.
[01:46:05.340 --> 01:46:08.060]   Well, it really was a very poor craftsmanship.
[01:46:08.060 --> 01:46:10.100]   All the builders were underqualified.
[01:46:10.100 --> 01:46:11.100]   Yeah.
[01:46:11.100 --> 01:46:14.180]   But in other words, it's not, and I mean, Levitt town.
[01:46:14.180 --> 01:46:15.180]   Yeah.
[01:46:15.180 --> 01:46:18.580]   It's not a, it's not a new idea to, to have a planned community.
[01:46:18.580 --> 01:46:21.260]   And I think there are a lot of people think, oh, that's a great idea.
[01:46:21.260 --> 01:46:25.260]   And if you have digital technology and you could plan traffic and, and other things
[01:46:25.260 --> 01:46:28.860]   using digital technology, I don't know if that's such a terrible idea.
[01:46:28.860 --> 01:46:34.140]   And who cares if a Google knows where the traffic goes?
[01:46:34.140 --> 01:46:39.220]   Oh, there's a lot of information that can be gleaned from traffic as we've figured out.
[01:46:39.220 --> 01:46:42.140]   Is Google's Google track up when you go?
[01:46:42.140 --> 01:46:44.660]   Yeah, they, well, they would know when you left your house.
[01:46:44.660 --> 01:46:46.460]   They would know when you got into a camera.
[01:46:46.460 --> 01:46:48.860]   They know right now when I leave my house.
[01:46:48.860 --> 01:46:51.340]   I'm not even in a planned community.
[01:46:51.340 --> 01:46:52.340]   Right.
[01:46:52.340 --> 01:46:53.340]   That they know that.
[01:46:53.340 --> 01:46:58.780]   So they know that now, but if you were an Apple phone user, you might say, okay, well,
[01:46:58.780 --> 01:47:03.780]   yeah, I have a way of not, I have a way of not having them know, but not living in, in
[01:47:03.780 --> 01:47:06.260]   Keysight, Toronto.
[01:47:06.260 --> 01:47:08.260]   But I still think it would be, you're right.
[01:47:08.260 --> 01:47:11.340]   People don't have to choose to live there if they don't want to, but I still think it
[01:47:11.340 --> 01:47:15.380]   would be useful to have some kind of assurances about.
[01:47:15.380 --> 01:47:19.500]   Who gets access to the data, what data they got to access?
[01:47:19.500 --> 01:47:21.420]   I'd like to have that from Facebook.
[01:47:21.420 --> 01:47:23.780]   I'd like to have it from lots of companies.
[01:47:23.780 --> 01:47:25.860]   So I would like to have it from Google.
[01:47:25.860 --> 01:47:27.420]   If they are running one amount.
[01:47:27.420 --> 01:47:32.700]   I just would be sad if because of some imagine, and this is more like.
[01:47:32.700 --> 01:47:33.700]   Here, look at it.
[01:47:33.700 --> 01:47:35.260]   Look at it this way.
[01:47:35.260 --> 01:47:38.500]   Actually this will appeal to you, Jeff, because you're a journalist.
[01:47:38.500 --> 01:47:43.900]   If I have questions about things in my city today, I can go to a city council.
[01:47:43.900 --> 01:47:49.540]   I can file a FOIA request and I can get whatever data exists, right?
[01:47:49.540 --> 01:47:51.340]   It's a pain, but I can do it.
[01:47:51.340 --> 01:47:58.060]   If Google has that data and this is a quasi-government, whatever project, then I find, if I want to
[01:47:58.060 --> 01:48:03.020]   find out information about, you know, maybe I noticed that there are no people of color
[01:48:03.020 --> 01:48:04.260]   in this place.
[01:48:04.260 --> 01:48:08.620]   Maybe I notice, I don't know, that the toilets back up a whole lot.
[01:48:08.620 --> 01:48:11.620]   I can't get any information necessarily about that.
[01:48:11.620 --> 01:48:12.620]   Yeah.
[01:48:12.620 --> 01:48:19.940]   So there are reasons to wonder about public access to what is effectively city data.
[01:48:19.940 --> 01:48:22.620]   So there are, I think it's a very good point, Stacey, excellent point.
[01:48:22.620 --> 01:48:26.500]   I think there are some other analogies you can go to, which is condominiums.
[01:48:26.500 --> 01:48:27.500]   Yeah, right.
[01:48:27.500 --> 01:48:31.620]   I mean, HOA's homeowners in New York can be very own.
[01:48:31.620 --> 01:48:32.620]   Yeah, co-ops.
[01:48:32.620 --> 01:48:37.020]   Co-ops are the ultimate capitalistic society where there's only one value that's property.
[01:48:37.020 --> 01:48:38.020]   Right.
[01:48:38.020 --> 01:48:39.020]   Right.
[01:48:39.020 --> 01:48:40.020]   In New York.
[01:48:40.020 --> 01:48:45.620]   So there are some analogies there where it's quasi-public.
[01:48:45.620 --> 01:48:49.900]   So what happens is you have separate, if you look at that as an analog, you have separate
[01:48:49.900 --> 01:48:56.180]   regulation in the government about how apartments are run, right?
[01:48:56.180 --> 01:49:01.460]   Whether you're a landlord or whether you're a co-op or a condo, you can impose things
[01:49:01.460 --> 01:49:04.900]   externally, which is what governments are doing.
[01:49:04.900 --> 01:49:09.060]   You're basically creating, if you were in Oryx and Craig, you're basically creating
[01:49:09.060 --> 01:49:16.620]   those company-based communities that, where the company's control, it becomes truly a
[01:49:16.620 --> 01:49:19.620]   network of company towns, which is maybe-
[01:49:19.620 --> 01:49:22.220]   Which exists to some extent today.
[01:49:22.220 --> 01:49:23.220]   I don't think it exists.
[01:49:23.220 --> 01:49:27.300]   I might as well move to a planned community next to us, because they said you couldn't
[01:49:27.300 --> 01:49:28.300]   put up a flagpole.
[01:49:28.300 --> 01:49:31.380]   We had no desire to put up a flagpole, but we resented the idea that somebody was going
[01:49:31.380 --> 01:49:35.540]   to tell us you can't put up a flagpole or paint your door red or whatever.
[01:49:35.540 --> 01:49:39.020]   I can see you out to the island next to you.
[01:49:39.020 --> 01:49:40.020]   But what is over time?
[01:49:40.020 --> 01:49:42.020]   But this is well beyond a condo.
[01:49:42.020 --> 01:49:46.540]   Yeah, over time, this is going to be better services.
[01:49:46.540 --> 01:49:47.540]   I should see this.
[01:49:47.540 --> 01:49:52.580]   But what I'm saying is there's precedent about how to deal with these questions with
[01:49:52.580 --> 01:49:53.580]   things like condoms.
[01:49:53.580 --> 01:49:58.900]   So what I'm saying is the role here, I think, I think, should be that government should-
[01:49:58.900 --> 01:50:01.780]   It's regulation and Kennedy is love regulation.
[01:50:01.780 --> 01:50:07.660]   They should say, rather than expecting Google or the private company to do it, government
[01:50:07.660 --> 01:50:11.740]   should be the one that insists upon it, and they're going to do the permits.
[01:50:11.740 --> 01:50:14.420]   Just like government can say, you have to build so many schools or you've got to make
[01:50:14.420 --> 01:50:15.420]   the roads this big.
[01:50:15.420 --> 01:50:16.420]   That's government's job.
[01:50:16.420 --> 01:50:17.420]   Yeah.
[01:50:17.420 --> 01:50:19.780]   And I think we're just negotiating how that has.
[01:50:19.780 --> 01:50:20.780]   That's right.
[01:50:20.780 --> 01:50:21.780]   Good.
[01:50:21.780 --> 01:50:22.780]   Like I don't think-
[01:50:22.780 --> 01:50:23.780]   Just negotiation.
[01:50:23.780 --> 01:50:24.780]   Right.
[01:50:24.780 --> 01:50:25.780]   As long as I just don't want to see-
[01:50:25.780 --> 01:50:26.780]   Leave the government-
[01:50:26.780 --> 01:50:27.780]   Leave the government-
[01:50:27.780 --> 01:50:28.780]   Don't need you.
[01:50:28.780 --> 01:50:29.780]   Come to Newark.
[01:50:29.780 --> 01:50:30.780]   Come to Newark.
[01:50:30.780 --> 01:50:31.780]   I'll move in.
[01:50:31.780 --> 01:50:32.780]   You come to Newark, I'll move in.
[01:50:32.780 --> 01:50:33.780]   I'll tell you what, Newark-
[01:50:33.780 --> 01:50:35.780]   That is not a selling point.
[01:50:35.780 --> 01:50:37.620]   Newark looks way better than the point-side line.
[01:50:37.620 --> 01:50:38.620]   So-
[01:50:38.620 --> 01:50:44.300]   Does Canada have a FOIA Freedom of Information Act?
[01:50:44.300 --> 01:50:45.300]   We do.
[01:50:45.300 --> 01:50:46.300]   The equivalent.
[01:50:46.300 --> 01:50:47.300]   Yeah.
[01:50:47.300 --> 01:50:48.300]   Yeah.
[01:50:48.300 --> 01:50:49.300]   We may not.
[01:50:49.300 --> 01:50:50.300]   We may not in the United States.
[01:50:50.300 --> 01:50:54.100]   The Supreme Court last week, actually two days ago, expanded-
[01:50:54.100 --> 01:50:55.100]   That's not a decision.
[01:50:55.100 --> 01:50:56.100]   Yeah.
[01:50:56.100 --> 01:50:57.100]   In the Argus Leader case-
[01:50:57.100 --> 01:51:02.660]   The companies, if it involves a company, it can now be determined to be confidential.
[01:51:02.660 --> 01:51:03.660]   Private.
[01:51:03.660 --> 01:51:04.660]   Private.
[01:51:04.660 --> 01:51:05.660]   It was the-
[01:51:05.660 --> 01:51:06.660]   That's horrible, horrible.
[01:51:06.660 --> 01:51:11.340]   The Food Marketing Institute versus Argus Leader Media.
[01:51:11.340 --> 01:51:18.180]   The Supreme Court ruled that Argus Media does not-
[01:51:18.180 --> 01:51:24.900]   Argus Leader Media does not have the right to that information because they were doing
[01:51:24.900 --> 01:51:29.900]   a FOIA with the U.S. Department of Agriculture seeking the names and addresses of all retail
[01:51:29.900 --> 01:51:34.420]   stores that participate in the food stamp program, SNAP.
[01:51:34.420 --> 01:51:37.340]   And each store's annual SNAP redemption data.
[01:51:37.340 --> 01:51:41.220]   This would be, of course, from a journalist's point of view of some very interesting data.
[01:51:41.220 --> 01:51:42.740]   That's a certain argument.
[01:51:42.740 --> 01:51:44.540]   USDA said, "No."
[01:51:44.540 --> 01:51:55.540]   Argus Leader sued the Supreme Court, said it's commercial information and can be kept confidential
[01:51:55.540 --> 01:51:58.780]   because it comes from companies.
[01:51:58.780 --> 01:51:59.780]   So they over-
[01:51:59.780 --> 01:52:02.780]   And the interesting thing is they didn't say-
[01:52:02.780 --> 01:52:08.100]   They didn't say that those companies have to meet any test where they say it's confidential
[01:52:08.100 --> 01:52:12.700]   because- or if we release it, "X, bad thing is going to happen."
[01:52:12.700 --> 01:52:16.180]   It's just ipso facto, everything is considered confidential.
[01:52:16.180 --> 01:52:17.180]   Right.
[01:52:17.180 --> 01:52:18.180]   And I would ask-
[01:52:18.180 --> 01:52:21.900]   The Institute has standing to appeal the disclosure of the contested data would cause
[01:52:21.900 --> 01:52:27.300]   its members financial injury in the highly competitive grocery industry.
[01:52:27.300 --> 01:52:33.100]   This concrete injury is directly traceable to the judgment ordering disclosure.
[01:52:33.100 --> 01:52:34.100]   And-
[01:52:34.100 --> 01:52:37.180]   And Argus won all the way up to the Supreme Court.
[01:52:37.180 --> 01:52:40.500]   Like it won every decision, obviously.
[01:52:40.500 --> 01:52:42.940]   But it's interesting to read the dissent.
[01:52:42.940 --> 01:52:45.020]   It's quite scathing.
[01:52:45.020 --> 01:52:46.020]   Yeah.
[01:52:46.020 --> 01:52:47.980]   Because it undermines FOIA.
[01:52:47.980 --> 01:52:48.980]   I think it's-
[01:52:48.980 --> 01:52:52.420]   I can't remember who delivered the dissent.
[01:52:52.420 --> 01:52:53.420]   It's right there.
[01:52:53.420 --> 01:52:54.420]   That is a shortage.
[01:52:54.420 --> 01:52:56.980]   Let me look at SCOTUS blog here.
[01:52:56.980 --> 01:53:04.880]   "In opinion by Justice Neil Gorsuch for a six-justice majority, Justice Eleanor Kagan-
[01:53:04.880 --> 01:53:07.420]   Well, oh wait a minute.
[01:53:07.420 --> 01:53:09.740]   Eleanor Kagan was on the majority.
[01:53:09.740 --> 01:53:13.380]   Let me see who wrote the dissent."
[01:53:13.380 --> 01:53:19.620]   Anyway, that's an interesting- Some say this might undermine FOIA.
[01:53:19.620 --> 01:53:20.620]   Yeah.
[01:53:20.620 --> 01:53:21.620]   Yeah.
[01:53:21.620 --> 01:53:22.620]   Yeah.
[01:53:22.620 --> 01:53:29.220]   Ruth Bader Ginsburg, Sonia Sotomayor, and Justice Stephen Breyer, partial dissent.
[01:53:29.220 --> 01:53:31.020]   Weird mixes.
[01:53:31.020 --> 01:53:32.020]   Yeah.
[01:53:32.020 --> 01:53:33.020]   Yeah.
[01:53:33.020 --> 01:53:34.980]   That's- I think weird mixes are good.
[01:53:34.980 --> 01:53:39.580]   At least it's not all long kind of party lines here, right?
[01:53:39.580 --> 01:53:41.940]   Still a bad decision.
[01:53:41.940 --> 01:53:42.940]   Yeah.
[01:53:42.940 --> 01:53:44.740]   Well, FOIA is a great thing.
[01:53:44.740 --> 01:53:49.740]   The Freedom of Information Act has proven to be a very powerful tool for journalists and
[01:53:49.740 --> 01:53:50.740]   for-
[01:53:50.740 --> 01:53:51.740]   Yeah.
[01:53:51.740 --> 01:53:52.740]   Yeah.
[01:53:52.740 --> 01:53:53.740]   Not just journalists.
[01:53:53.740 --> 01:53:56.740]   I mean, normal citizens, parents use that a lot to figure out like-
[01:53:56.740 --> 01:53:57.740]   Yeah.
[01:53:57.740 --> 01:53:58.740]   Right.
[01:53:58.740 --> 01:53:59.740]   Yeah.
[01:53:59.740 --> 01:54:07.020]   I want to retract many of the conversations we've had over the past few years about the
[01:54:07.020 --> 01:54:09.820]   distinction between a platform and a publisher.
[01:54:09.820 --> 01:54:10.820]   Yes.
[01:54:10.820 --> 01:54:11.820]   All right.
[01:54:11.820 --> 01:54:15.180]   You're all can come in a moment.
[01:54:15.180 --> 01:54:18.860]   Mike Masnick, who we do like a lot, he's with TechDirt.
[01:54:18.860 --> 01:54:20.060]   He's the best.
[01:54:20.060 --> 01:54:21.980]   He said, "I'm going to say it again.
[01:54:21.980 --> 01:54:25.460]   There's no legal distinction between a platform and a publisher.
[01:54:25.460 --> 01:54:35.020]   Everybody, the way that section 230 is written is that everybody is an internet platform.
[01:54:35.020 --> 01:54:38.980]   If you're online, you're an internet platform and you're protected.
[01:54:38.980 --> 01:54:42.620]   That companies don't need to worry about whether they're a publisher or a platform.
[01:54:42.620 --> 01:54:45.780]   If they're online, they're protected.
[01:54:45.780 --> 01:54:47.420]   You disagree, Jeff?
[01:54:47.420 --> 01:54:50.020]   In regards to 230, I agree.
[01:54:50.020 --> 01:54:56.180]   But that's why I hate that being the test because there's a lot else that falls into
[01:54:56.180 --> 01:54:57.180]   place that-
[01:54:57.180 --> 01:54:58.180]   Ah, okay.
[01:54:58.180 --> 01:55:02.420]   In regards to 230, yeah, it's about enabling conversation.
[01:55:02.420 --> 01:55:05.780]   And many parties do that.
[01:55:05.780 --> 01:55:06.780]   And as-
[01:55:06.780 --> 01:55:07.780]   230 protects-
[01:55:07.780 --> 01:55:10.940]   The author of the bill said they give a shield and a sword.
[01:55:10.940 --> 01:55:11.940]   Right.
[01:55:11.940 --> 01:55:18.100]   230 protects all, quote, interactive computer services, including publishers who host third-party
[01:55:18.100 --> 01:55:19.660]   content.
[01:55:19.660 --> 01:55:21.660]   You don't have to figure out-
[01:55:21.660 --> 01:55:23.940]   You don't have to be as Facebook.
[01:55:23.940 --> 01:55:28.940]   We've- I have said mistakenly, "Oh, Facebook's got to figure out as a platform or a publisher."
[01:55:28.940 --> 01:55:31.260]   No, it's an internet company.
[01:55:31.260 --> 01:55:33.100]   In regards to 230, no.
[01:55:33.100 --> 01:55:34.100]   Okay.
[01:55:34.100 --> 01:55:35.100]   Where does it matter?
[01:55:35.100 --> 01:55:36.100]   That's what I'm supporting.
[01:55:36.100 --> 01:55:37.500]   You're not considered a publisher.
[01:55:37.500 --> 01:55:38.500]   Just because- Yeah.
[01:55:38.500 --> 01:55:41.380]   It's- By the way, I'm going to plug it again.
[01:55:41.380 --> 01:55:45.180]   Everybody out there read Jeff Kossoff's book, The 26 Words that Created the Internet.
[01:55:45.180 --> 01:55:49.940]   Is it excellent biography of the law and where it stands?
[01:55:49.940 --> 01:55:52.700]   And I imagine that he delves into this there.
[01:55:52.700 --> 01:55:53.700]   Oh, quite a bit.
[01:55:53.700 --> 01:55:54.700]   Yeah.
[01:55:54.700 --> 01:55:55.700]   It's really- You ought to have one on the show.
[01:55:55.700 --> 01:55:57.500]   It's really good.
[01:55:57.500 --> 01:55:58.500]   Okay.
[01:55:58.500 --> 01:55:59.500]   So I have-
[01:55:59.500 --> 01:56:06.820]   And in fact, the whole point of the law was to make it so that interactive service providers
[01:56:06.820 --> 01:56:09.700]   could delete content and curate-
[01:56:09.700 --> 01:56:10.700]   Oh, yes.
[01:56:10.700 --> 01:56:16.300]   All those other things without being subject to different rules.
[01:56:16.300 --> 01:56:18.140]   It wouldn't be a slippery slope.
[01:56:18.140 --> 01:56:23.740]   They have the safe harbor protection of Section 230, regardless of what they do to the content
[01:56:23.740 --> 01:56:25.740]   that they're publishing, whether they edit or delete it.
[01:56:25.740 --> 01:56:26.740]   They don't have to just be a pipe.
[01:56:26.740 --> 01:56:27.740]   Right.
[01:56:27.740 --> 01:56:28.740]   That distributes without it.
[01:56:28.740 --> 01:56:29.740]   I highly recommend it.
[01:56:29.740 --> 01:56:31.420]   Have you read the book, Matthew?
[01:56:31.420 --> 01:56:32.580]   No, I have.
[01:56:32.580 --> 01:56:33.580]   It's on my house.
[01:56:33.580 --> 01:56:34.580]   I highly recommend it.
[01:56:34.580 --> 01:56:35.580]   I highly recommend it.
[01:56:35.580 --> 01:56:38.500]   It's because it's the fight that we're going into, right?
[01:56:38.500 --> 01:56:42.700]   So you see that effort from the right last week to say that if you're not neutral, you're
[01:56:42.700 --> 01:56:44.700]   going to lose 230 protection.
[01:56:44.700 --> 01:56:45.700]   You shouldn't have it.
[01:56:45.700 --> 01:56:50.180]   There's going to be efforts to knock down pieces of 230.
[01:56:50.180 --> 01:56:54.940]   And Weiden, who is a co-author with Roth, Weiden and the Senate Roth in the house when
[01:56:54.940 --> 01:56:57.940]   he was there, not Roth.
[01:56:57.940 --> 01:56:58.940]   No.
[01:56:58.940 --> 01:56:59.940]   It's where he else.
[01:56:59.940 --> 01:57:01.900]   But Weiden and the Senate.
[01:57:01.900 --> 01:57:07.740]   And Weiden is very unhappy that the platformers didn't stand up to protect 230 in what was
[01:57:07.740 --> 01:57:08.740]   it called?
[01:57:08.740 --> 01:57:13.220]   Fosca, Sissa, Fisca, Fisca, whatever you do.
[01:57:13.220 --> 01:57:14.220]   Fosca, Cesta.
[01:57:14.220 --> 01:57:15.220]   Fosca, Cesta.
[01:57:15.220 --> 01:57:16.220]   Fosca, Cesta.
[01:57:16.220 --> 01:57:17.220]   Thanks.
[01:57:17.220 --> 01:57:24.180]   Because the fear is there's going to be a lot more carve outs of 230.
[01:57:24.180 --> 01:57:29.860]   And there was, so really testimony this week arguing that there ought to be an additional
[01:57:29.860 --> 01:57:36.540]   phrase there that is extended to those that do responsible moderation.
[01:57:36.540 --> 01:57:44.300]   But then, if I want to be a Chan and everything goes away after 30 seconds or whatever however
[01:57:44.300 --> 01:57:48.180]   long it is, it goes away, that's a choice on the internet.
[01:57:48.180 --> 01:57:50.500]   Why should that be considered bad?
[01:57:50.500 --> 01:57:55.060]   One thing Mike points out, which I think is really important, is he points out that
[01:57:55.060 --> 01:57:57.700]   this doesn't just protect Google and Facebook.
[01:57:57.700 --> 01:58:00.820]   It protects newspapers that have common sessions.
[01:58:00.820 --> 01:58:03.020]   It protects any publisher.
[01:58:03.020 --> 01:58:04.460]   It protects review sites.
[01:58:04.460 --> 01:58:10.260]   It protects literally anybody who offers a service where people comment.
[01:58:10.260 --> 01:58:15.340]   So it's not just that it protects these giant platforms that get away with whatever you
[01:58:15.340 --> 01:58:17.380]   think they're getting away with.
[01:58:17.380 --> 01:58:22.580]   Protects anyone who's offering a service where people are expressing their opinions.
[01:58:22.580 --> 01:58:28.620]   When the Guardian started to comment as free a little bit years ago, they're really robust
[01:58:28.620 --> 01:58:34.820]   discussion platform, which was groundbreaking at the time, it was really gutsy because
[01:58:34.820 --> 01:58:39.180]   there's no 230 nor a First Amendment, of course, in the UK.
[01:58:39.180 --> 01:58:43.900]   And they were liable for things set on the platform.
[01:58:43.900 --> 01:58:47.580]   And they, against the advice of a lot of people in media, went ahead and did it, but most
[01:58:47.580 --> 01:58:52.620]   people in media didn't want to be anywhere near it because they would have liability.
[01:58:52.620 --> 01:58:53.620]   We went through that.
[01:58:53.620 --> 01:58:58.300]   I worked at when I was involved in comments and social media.
[01:58:58.300 --> 01:59:02.540]   It was a huge because there's no site for 230 and Canada either.
[01:59:02.540 --> 01:59:03.780]   That's interesting.
[01:59:03.780 --> 01:59:07.180]   So what's it like when did no Section 230?
[01:59:07.180 --> 01:59:10.020]   Well, everyone is very, very nervous.
[01:59:10.020 --> 01:59:11.820]   Yeah, all of a sudden.
[01:59:11.820 --> 01:59:16.220]   Especially lawyers because you just never know.
[01:59:16.220 --> 01:59:17.460]   There have been a couple of decisions.
[01:59:17.460 --> 01:59:21.140]   I remember taking part in a case involving comments.
[01:59:21.140 --> 01:59:28.820]   And there have been a couple of decisions, lower courts, that did uphold the right of
[01:59:28.820 --> 01:59:36.740]   publishers to moderate comments and said that that didn't by definition sort of lay
[01:59:36.740 --> 01:59:38.500]   them open to lawsuits.
[01:59:38.500 --> 01:59:44.820]   So it wasn't as though you have to keep your hands off or you won't be covered, but it's
[01:59:44.820 --> 01:59:46.420]   still a very gray area.
[01:59:46.420 --> 01:59:49.580]   It's very cool that we have this Section 230.
[01:59:49.580 --> 01:59:51.140]   This is part of the way before.
[01:59:51.140 --> 01:59:52.140]   We got it defended.
[01:59:52.140 --> 01:59:53.140]   Yeah, we got it defended.
[01:59:53.140 --> 01:59:54.140]   And I just, I just.
[01:59:54.140 --> 01:59:56.140]   It's the last part thing Congress did.
[01:59:56.140 --> 01:59:59.340]   It was 1996 really has been that long.
[01:59:59.340 --> 02:00:05.020]   It was when it was, you think about how early it was in the process of the Internet starting.
[02:00:05.020 --> 02:00:11.540]   It was very prescient to realize the principle of this is goes to James Kerry, the principle
[02:00:11.540 --> 02:00:14.020]   of defending conversation in a democracy.
[02:00:14.020 --> 02:00:15.020]   Yeah.
[02:00:15.020 --> 02:00:17.780]   And I guarantee that would ever be passed now.
[02:00:17.780 --> 02:00:18.780]   Oh God, no.
[02:00:18.780 --> 02:00:20.980]   Well, that's why it's under assault.
[02:00:20.980 --> 02:00:23.140]   I just ordered the book.
[02:00:23.140 --> 02:00:24.140]   Libraries.
[02:00:24.140 --> 02:00:28.220]   If you tried to create a library now, if there weren't libraries, you said, we're going
[02:00:28.220 --> 02:00:29.540]   to have a bunch of books.
[02:00:29.540 --> 02:00:31.660]   We're going to give them away to anybody who wants them.
[02:00:31.660 --> 02:00:33.140]   They would never happen.
[02:00:33.140 --> 02:00:36.580]   No one's ever allowed you to create a socialist.
[02:00:36.580 --> 02:00:37.580]   You're a socialist.
[02:00:37.580 --> 02:00:38.580]   You're going to give away.
[02:00:38.580 --> 02:00:44.700]   So in Congress books, there was a case of a bookstore that had literature viewed as
[02:00:44.700 --> 02:00:46.260]   pornographic.
[02:00:46.260 --> 02:00:49.820]   And the argument was the bookstore owner was responsible.
[02:00:49.820 --> 02:00:55.540]   But the contrary argument was the bookstore owner doesn't read everything in the bookstore.
[02:00:55.540 --> 02:00:59.540]   And therein that debate leads you to 230.
[02:00:59.540 --> 02:01:00.540]   It's fascinating.
[02:01:00.540 --> 02:01:01.540]   Yeah.
[02:01:01.540 --> 02:01:06.340]   I find it interesting that there's a debate about, do you remember the media men list?
[02:01:06.340 --> 02:01:07.340]   Yes.
[02:01:07.340 --> 02:01:09.060]   Was it Google Doc that went around?
[02:01:09.060 --> 02:01:12.540]   Yeah, it was a meet, a list of meet two people accused of meet two.
[02:01:12.540 --> 02:01:17.540]   Right.
[02:01:17.540 --> 02:01:22.540]   And so someone sued has sued over being named in that list.
[02:01:22.540 --> 02:01:23.540]   Oh, really?
[02:01:23.540 --> 02:01:27.540]   And the creator of the list is trying to argue that that Google document was effectively
[02:01:27.540 --> 02:01:29.540]   an interactive service.
[02:01:29.540 --> 02:01:36.540]   And therefore, she shouldn't be responsible for things that were contributed to that list
[02:01:36.540 --> 02:01:37.540]   because they were contributed by.
[02:01:37.540 --> 02:01:38.540]   She's got a bunch.
[02:01:38.540 --> 02:01:39.540]   It's an interesting topic.
[02:01:39.540 --> 02:01:42.540]   It was effectively an interactive document.
[02:01:42.540 --> 02:01:43.540]   Right.
[02:01:43.540 --> 02:01:44.540]   Anyway, interesting case.
[02:01:44.540 --> 02:01:47.540]   Jeff, there was a case on the rundown.
[02:01:47.540 --> 02:01:52.540]   Jeff, K-O-S-S-E-F-F, the 26 words that created the Internet, if you want to follow up.
[02:01:52.540 --> 02:01:53.540]   I just ordered it.
[02:01:53.540 --> 02:01:54.540]   I'm going to read it.
[02:01:54.540 --> 02:01:55.540]   I think I need to.
[02:01:55.540 --> 02:02:05.860]   There was a case in Australia on the rundown today of a woman who lost $500,000 case from
[02:02:05.860 --> 02:02:15.860]   a plastic surgeon, she had a live old defamed in a Google review about her cheek reduction.
[02:02:15.860 --> 02:02:16.860]   Right.
[02:02:16.860 --> 02:02:17.860]   That's not me right there.
[02:02:17.860 --> 02:02:19.860]   A cheek reduction?
[02:02:19.860 --> 02:02:21.500]   Well, you had Jake Cheeks.
[02:02:21.500 --> 02:02:27.140]   If you were one of us, one of the sufferers, one of us who suffered from Jake Cheeks syndrome
[02:02:27.140 --> 02:02:33.700]   or BCS, yes, you would not make light of this.
[02:02:33.700 --> 02:02:35.700]   No, he's a scrawny little cuss.
[02:02:35.700 --> 02:02:37.700]   Oh, God, no.
[02:02:37.700 --> 02:02:42.140]   But anyway, old moon face Leo, I know how she feels.
[02:02:42.140 --> 02:02:44.820]   You got to reduce your cheeks chipmunk there?
[02:02:44.820 --> 02:02:46.180]   Yeah, I'm thinking.
[02:02:46.180 --> 02:02:48.940]   But I'm not going to this plastic surgeon.
[02:02:48.940 --> 02:02:49.940]   I can tell you.
[02:02:49.940 --> 02:02:50.940]   I read the review.
[02:02:50.940 --> 02:02:53.700]   Plastic surgeon one.
[02:02:53.700 --> 02:02:54.700]   He won.
[02:02:54.700 --> 02:02:56.380]   See, that's shocking to me.
[02:02:56.380 --> 02:02:57.380]   If you can't.
[02:02:57.380 --> 02:02:59.700]   I'm going to because it was, it was, it was, it was demonstrably false.
[02:02:59.700 --> 02:03:01.420]   Oh, she lied about it.
[02:03:01.420 --> 02:03:02.700]   Oh, she lied about it.
[02:03:02.700 --> 02:03:03.700]   Oh, that's different.
[02:03:03.700 --> 02:03:05.540]   It was a false review.
[02:03:05.540 --> 02:03:07.700]   And so people think on the internet, oh, I can put this stuff up.
[02:03:07.700 --> 02:03:09.500]   No, libel law pertains.
[02:03:09.500 --> 02:03:10.500]   Okay.
[02:03:10.500 --> 02:03:11.500]   Yeah.
[02:03:11.500 --> 02:03:12.500]   So this is a good app.
[02:03:12.500 --> 02:03:13.500]   Yeah.
[02:03:13.500 --> 02:03:18.180]   Just like, just like if I said something slanderous, slander, which still pertain.
[02:03:18.180 --> 02:03:19.180]   Yeah.
[02:03:19.180 --> 02:03:20.180]   Yeah.
[02:03:20.180 --> 02:03:21.180]   Yeah.
[02:03:21.180 --> 02:03:22.180]   So I'm going to turn on so it's a little different.
[02:03:22.180 --> 02:03:24.700]   Twitter has been involved in a, in a bunch of cases like that.
[02:03:24.700 --> 02:03:31.900]   I think didn't Courtney Love was sued for defaming somebody on Twitter.
[02:03:31.900 --> 02:03:35.540]   But see, but the, in this case, Yelp wouldn't be liable.
[02:03:35.540 --> 02:03:36.540]   Twitter wouldn't be liable.
[02:03:36.540 --> 02:03:37.540]   Twitter was not liable.
[02:03:37.540 --> 02:03:40.700]   No, but the person posted it might be, you know, yeah.
[02:03:40.700 --> 02:03:41.700]   All right.
[02:03:41.700 --> 02:03:42.900]   That makes sense.
[02:03:42.900 --> 02:03:48.500]   And that probably is the, the answer to that spreadsheet that was going around is that,
[02:03:48.500 --> 02:03:52.900]   well, the person who created the spreadsheet might not be liable, but whoever put people
[02:03:52.900 --> 02:03:57.380]   who contributed libel statements in it would be liable.
[02:03:57.380 --> 02:04:02.380]   Lie way, libel, L.A.B.E.L versus L.I.A.B.L.E. I should pronounce those differently.
[02:04:02.380 --> 02:04:04.020]   You should be liable for your libel.
[02:04:04.020 --> 02:04:05.020]   Libel is liable.
[02:04:05.020 --> 02:04:06.020]   Liable.
[02:04:06.020 --> 02:04:07.020]   Liable.
[02:04:07.020 --> 02:04:08.020]   Don't lie.
[02:04:08.020 --> 02:04:09.020]   Don't do your book.
[02:04:09.020 --> 02:04:10.020]   Dreyer's English.
[02:04:10.020 --> 02:04:11.020]   Dreyer's English.
[02:04:11.020 --> 02:04:15.740]   If you read that one, no, man, the copy sheet for Random House, it's delightful.
[02:04:15.740 --> 02:04:16.740]   It is, believe it or not.
[02:04:16.740 --> 02:04:18.340]   It's a book about copy editing and it's delightful.
[02:04:18.340 --> 02:04:20.460]   I love that kind of stuff.
[02:04:20.460 --> 02:04:22.900]   That one I might buy in hardcover Benjamin Dreyer.
[02:04:22.900 --> 02:04:23.900]   Oxford comma.
[02:04:23.900 --> 02:04:24.900]   Yeah.
[02:04:24.900 --> 02:04:25.900]   Yeah.
[02:04:25.900 --> 02:04:26.900]   Right on, dude.
[02:04:26.900 --> 02:04:29.900]   You should also eat shoots and leaves.
[02:04:29.900 --> 02:04:30.900]   Yes, that's an Oxford.
[02:04:30.900 --> 02:04:31.900]   Why not that?
[02:04:31.900 --> 02:04:32.900]   Yeah, great book.
[02:04:32.900 --> 02:04:33.900]   Actually, I think-
[02:04:33.900 --> 02:04:35.300]   I have tons of grammar books, actually.
[02:04:35.300 --> 02:04:38.660]   Stacey might have written this one.
[02:04:38.660 --> 02:04:40.660]   Miss Thistlebottom's Haba-Globlins.
[02:04:40.660 --> 02:04:45.940]   Therefore, writers, dead in Taboos, bugbears and outmoded rules of English.
[02:04:45.940 --> 02:04:47.540]   Thistlebottom are cousins, I think.
[02:04:47.540 --> 02:04:48.540]   Stacey Thistlebottom.
[02:04:48.540 --> 02:04:49.540]   Yeah.
[02:04:49.540 --> 02:04:51.540]   That is my British cousin, right there.
[02:04:51.540 --> 02:04:53.540]   That was just from the doors.
[02:04:53.540 --> 02:04:56.860]   Are we actually doing our picks?
[02:04:56.860 --> 02:04:57.860]   Yes.
[02:04:57.860 --> 02:04:58.860]   I don't know what to do now that we're-
[02:04:58.860 --> 02:04:59.860]   Give us a pick.
[02:04:59.860 --> 02:05:00.860]   Give us a show.
[02:05:00.860 --> 02:05:01.860]   Give us a pick.
[02:05:01.860 --> 02:05:02.860]   Thistlebottom.
[02:05:02.860 --> 02:05:05.660]   Tell us about the Thistlebottom's first, though.
[02:05:05.660 --> 02:05:06.660]   All right.
[02:05:06.660 --> 02:05:12.180]   Well, my pick is not a tech pick because I bought the most ingenious thing that I love
[02:05:12.180 --> 02:05:13.580]   and I must share it with you.
[02:05:13.580 --> 02:05:14.580]   Okay.
[02:05:14.580 --> 02:05:17.180]   It is the Oxo Good Grips salad dressing shaker.
[02:05:17.180 --> 02:05:19.220]   I love everything Oxo Good Grips does.
[02:05:19.220 --> 02:05:20.220]   So what now?
[02:05:20.220 --> 02:05:25.540]   I didn't know they made this, but I just have been shaking up my dressings in an old
[02:05:25.540 --> 02:05:31.140]   dressing jar, but when we moved, I didn't have a mason jar and I didn't have a cup or
[02:05:31.140 --> 02:05:32.140]   anything.
[02:05:32.140 --> 02:05:34.540]   So I just bought this because I was like, "You know, I might use this."
[02:05:34.540 --> 02:05:36.100]   Oh, my gosh, it's awesome.
[02:05:36.100 --> 02:05:39.620]   And if you want to make it even better for people, if there's like one or two dressings
[02:05:39.620 --> 02:05:46.820]   that you guys normally make all the time in your house, take a Sharpie and write the amounts
[02:05:46.820 --> 02:05:49.980]   on the dressing, make the dressing and write like a little to this point.
[02:05:49.980 --> 02:05:52.580]   Oh, that's a good idea because anybody can make it.
[02:05:52.580 --> 02:05:53.580]   Then anyone can use it.
[02:05:53.580 --> 02:05:54.980]   But it doesn't leak.
[02:05:54.980 --> 02:05:56.500]   You can use it to shake stuff up.
[02:05:56.500 --> 02:05:57.500]   It's very easy to clean.
[02:05:57.500 --> 02:05:58.500]   Oh, I'm getting this right now.
[02:05:58.500 --> 02:05:59.500]   Nice.
[02:05:59.500 --> 02:06:04.060]   So because I have, I bought a six pack of squeeze bottles, you know, like the mustard
[02:06:04.060 --> 02:06:05.060]   and ketchup bottles.
[02:06:05.060 --> 02:06:08.900]   Yes, yes, because somebody told me to do that and it was a great idea.
[02:06:08.900 --> 02:06:09.900]   Yeah.
[02:06:09.900 --> 02:06:10.900]   We bought something.
[02:06:10.900 --> 02:06:11.900]   Yeah.
[02:06:11.900 --> 02:06:13.900]   And so I make my dressing and put it in there, but this would be better because you have
[02:06:13.900 --> 02:06:16.500]   to put your finger over the hole when you're shaking it.
[02:06:16.500 --> 02:06:17.500]   So this would be good.
[02:06:17.500 --> 02:06:18.500]   Yeah.
[02:06:18.500 --> 02:06:23.100]   We use the ketchup bottles, the squeezable bottles for barbecue sauce that we make.
[02:06:23.100 --> 02:06:26.260]   I keep a bottle of olive oil by the stove all the time.
[02:06:26.260 --> 02:06:28.900]   That's really the best use because then you can always squirt a little olive oil in a
[02:06:28.900 --> 02:06:29.900]   pan or whatever.
[02:06:29.900 --> 02:06:30.900]   And it's very fast and easy.
[02:06:30.900 --> 02:06:31.900]   Yeah.
[02:06:31.900 --> 02:06:35.060]   And this comes in two sizes, a one and a half cup or one cup.
[02:06:35.060 --> 02:06:36.060]   That's not smart.
[02:06:36.060 --> 02:06:41.220]   I would get to like anything by Bluetooth or there is nothing smart unless you do your
[02:06:41.220 --> 02:06:42.660]   little dressing recipes on it.
[02:06:42.660 --> 02:06:43.660]   Then it's smart.
[02:06:43.660 --> 02:06:44.660]   Oh, mine's green.
[02:06:44.660 --> 02:06:46.300]   I don't know why yours is black, but there you go.
[02:06:46.300 --> 02:06:47.300]   There you go.
[02:06:47.300 --> 02:06:49.620]   You should get one in each color in that way.
[02:06:49.620 --> 02:06:50.620]   You can have one.
[02:06:50.620 --> 02:06:52.620]   You need more tea in it if you need, if you wanted to.
[02:06:52.620 --> 02:06:54.620]   Man, you totally could.
[02:06:54.620 --> 02:06:57.980]   Oh, that is a great option.
[02:06:57.980 --> 02:06:58.980]   There you go.
[02:06:58.980 --> 02:07:02.020]   They don't have the large and green here.
[02:07:02.020 --> 02:07:03.020]   What?
[02:07:03.020 --> 02:07:04.020]   Huh?
[02:07:04.020 --> 02:07:05.820]   Well, maybe that's because I'm on the wrong page.
[02:07:05.820 --> 02:07:06.820]   There you go.
[02:07:06.820 --> 02:07:07.820]   There you go.
[02:07:07.820 --> 02:07:09.580]   I don't know.
[02:07:09.580 --> 02:07:11.780]   But I love this thing.
[02:07:11.780 --> 02:07:16.500]   Now I've only had it for four weeks now, but man, I have washed it.
[02:07:16.500 --> 02:07:17.500]   I've used it.
[02:07:17.500 --> 02:07:18.500]   It has a gasket.
[02:07:18.500 --> 02:07:20.500]   So when you put it in the fridge, it doesn't smell of the place.
[02:07:20.500 --> 02:07:21.500]   Oh, it's nice.
[02:07:21.500 --> 02:07:23.620]   So, yeah, it's good.
[02:07:23.620 --> 02:07:25.980]   I have not found a problem with it yet.
[02:07:25.980 --> 02:07:27.420]   And it will.
[02:07:27.420 --> 02:07:31.700]   You can also get the two in one's dressing shaker with a citrus juicer on top of it.
[02:07:31.700 --> 02:07:36.100]   Yeah, that seems like a lot of that seemed like more.
[02:07:36.100 --> 02:07:37.940]   Yeah, too much.
[02:07:37.940 --> 02:07:39.900]   But if you want, let me know how that works.
[02:07:39.900 --> 02:07:40.900]   I'm buying this.
[02:07:40.900 --> 02:07:41.900]   This is awesome.
[02:07:41.900 --> 02:07:44.620]   Yeah, I was really, I didn't even know these existed.
[02:07:44.620 --> 02:07:46.300]   I was very excited.
[02:07:46.300 --> 02:07:48.380]   I should just take it one of everything oxo.
[02:07:48.380 --> 02:07:49.380]   Oh, wait though.
[02:07:49.380 --> 02:07:50.900]   Oh, I should tell you before you buy it.
[02:07:50.900 --> 02:07:52.740]   I bought mine for $9.99 at Target.
[02:07:52.740 --> 02:07:55.540]   Yeah, this seems like this seems like there.
[02:07:55.540 --> 02:07:56.540]   You know what?
[02:07:56.540 --> 02:07:59.500]   This is one of those guys who goes to Target, buys a hundred of them and sells them on Amazon
[02:07:59.500 --> 02:08:00.980]   for five bucks more.
[02:08:00.980 --> 02:08:01.980]   Guaranteed.
[02:08:01.980 --> 02:08:02.980]   Yeah.
[02:08:02.980 --> 02:08:03.980]   Yeah.
[02:08:03.980 --> 02:08:07.340]   So that's my only caveat, but you know, I liked it.
[02:08:07.340 --> 02:08:08.340]   Nice.
[02:08:08.340 --> 02:08:09.660]   Good pick.
[02:08:09.660 --> 02:08:11.820]   Any cooking pick is okay with me.
[02:08:11.820 --> 02:08:17.620]   Oh, now that I've gone to the one that's more expensive, I swear to God.
[02:08:17.620 --> 02:08:18.860]   I'll show you the cheaper one.
[02:08:18.860 --> 02:08:25.660]   I swear to God, Amazon has raised the price on the one that was $9 is now also $15.
[02:08:25.660 --> 02:08:28.540]   Oh, and then it was tired of you being indecisive.
[02:08:28.540 --> 02:08:31.180]   I swear to God, wasn't this, am I wrong, folks?
[02:08:31.180 --> 02:08:33.900]   Wasn't this $9?
[02:08:33.900 --> 02:08:34.900]   And now it's 15.
[02:08:34.900 --> 02:08:35.900]   You don't know.
[02:08:35.900 --> 02:08:40.500]   She's Amazon's algorithmic pricing at work again.
[02:08:40.500 --> 02:08:41.500]   I don't want to look back up there.
[02:08:41.500 --> 02:08:42.980]   They stay chat room.
[02:08:42.980 --> 02:08:47.460]   Matthew Ingram, pick something for me.
[02:08:47.460 --> 02:08:51.780]   So I picked something out of the rundown that we didn't get to.
[02:08:51.780 --> 02:09:01.100]   It looks like Twitter has finally answered my prayers from about six years ago, maybe.
[02:09:01.100 --> 02:09:03.300]   And they're highlighting lists.
[02:09:03.300 --> 02:09:05.180]   They're back, baby.
[02:09:05.180 --> 02:09:06.580]   Twitter lists are back.
[02:09:06.580 --> 02:09:10.060]   They're much more prominent in the redesign.
[02:09:10.060 --> 02:09:13.380]   So there's a menu and lists is right in there.
[02:09:13.380 --> 02:09:20.180]   So instead of having to click six nested sub menus to get to your lists, I use lists a
[02:09:20.180 --> 02:09:21.180]   lot.
[02:09:21.180 --> 02:09:24.180]   I don't know if other people do, but it's one of the only ways I can.
[02:09:24.180 --> 02:09:25.180]   So where do I get?
[02:09:25.180 --> 02:09:26.180]   How do I get?
[02:09:26.180 --> 02:09:28.180]   So here's my Twitter page.
[02:09:28.180 --> 02:09:29.580]   And then I may not have the.
[02:09:29.580 --> 02:09:30.580]   I may not have it.
[02:09:30.580 --> 02:09:32.180]   But would it be over here on the left?
[02:09:32.180 --> 02:09:33.940]   It would be on the left.
[02:09:33.940 --> 02:09:34.940]   Yeah.
[02:09:34.940 --> 02:09:37.700]   Yes, where lists used to be when they first created lists.
[02:09:37.700 --> 02:09:38.700]   Yeah.
[02:09:38.700 --> 02:09:44.580]   So we've got home, explore, notifications, messages, bookmarks, lists, profile are all
[02:09:44.580 --> 02:09:48.100]   in a menu on the left instead of sort of up at the top.
[02:09:48.100 --> 02:09:49.420]   Oh, I hope to get that.
[02:09:49.420 --> 02:09:50.420]   Yeah.
[02:09:50.420 --> 02:09:51.420]   I use lists.
[02:09:51.420 --> 02:09:52.420]   I don't know.
[02:09:52.420 --> 02:09:57.060]   I think I remember, I think it was at Williams or somebody admitted that they should have
[02:09:57.060 --> 02:09:58.500]   done way more with lists.
[02:09:58.500 --> 02:10:03.940]   It was such an obvious way of looking to create their streams and they just just ignored
[02:10:03.940 --> 02:10:04.940]   them forever.
[02:10:04.940 --> 02:10:06.860]   They're redhead and stepped out of.
[02:10:06.860 --> 02:10:10.020]   I use in TweetDeck, I make columns out of.
[02:10:10.020 --> 02:10:13.020]   So I have, for instance, here's a list of tech journalists.
[02:10:13.020 --> 02:10:16.660]   And so I just have the tech journalist list as one of the columns in TweetDeck.
[02:10:16.660 --> 02:10:19.100]   And that way I'm going to make sure I see all the tweets.
[02:10:19.100 --> 02:10:21.980]   Yeah, this is a way of doing that without TweetDeck basically.
[02:10:21.980 --> 02:10:22.980]   Yeah.
[02:10:22.980 --> 02:10:23.980]   Yeah.
[02:10:23.980 --> 02:10:24.980]   Well, this is using the list.
[02:10:24.980 --> 02:10:27.180]   You can make them public.
[02:10:27.180 --> 02:10:28.180]   Yep.
[02:10:28.180 --> 02:10:29.180]   Nice.
[02:10:29.180 --> 02:10:30.180]   Very nice.
[02:10:30.180 --> 02:10:31.980]   Well, thank you, Twitter.
[02:10:31.980 --> 02:10:32.980]   Thank you, Twitter.
[02:10:32.980 --> 02:10:36.700]   But get rid of the Nazis as well.
[02:10:36.700 --> 02:10:41.660]   Jeff Jarvis, do you have a number or a pick or something like that?
[02:10:41.660 --> 02:10:48.100]   Well, I was going to talk about how Google has bought up almost all the real estate between
[02:10:48.100 --> 02:10:52.380]   15th and 16th streets in 8th Avenue in the river to start our own little tech village
[02:10:52.380 --> 02:10:55.580]   and how I actually think they've been issued put some apartments in there and I would have
[02:10:55.580 --> 02:10:56.580]   a.
[02:10:56.580 --> 02:10:58.580]   Oh, oh, oh, Toronto.
[02:10:58.580 --> 02:11:04.820]   He's saying he would happily live there.
[02:11:04.820 --> 02:11:06.820]   Toronto is censoring.
[02:11:06.820 --> 02:11:07.820]   Yes, I would.
[02:11:07.820 --> 02:11:09.820]   Rogers is finally having with Jeff.
[02:11:09.820 --> 02:11:11.220]   But I'm not going to do it.
[02:11:11.220 --> 02:11:12.580]   I'm not going to do that because it's an old joke.
[02:11:12.580 --> 02:11:15.020]   So I just want to get so Benjamin dryer I mentioned before.
[02:11:15.020 --> 02:11:16.020]   He's great on Twitter too.
[02:11:16.020 --> 02:11:19.540]   His today he has a copy editing pro tip.
[02:11:19.540 --> 02:11:22.900]   If you spit on the president's son today tomorrow you can say that you spit on him
[02:11:22.900 --> 02:11:27.780]   or that you spat on him either is correct.
[02:11:27.780 --> 02:11:31.140]   So past 10 just spit is spit or spat.
[02:11:31.140 --> 02:11:32.140]   He would say spat.
[02:11:32.140 --> 02:11:33.340]   You saw that story, by the way.
[02:11:33.340 --> 02:11:34.340]   Yes.
[02:11:34.340 --> 02:11:36.580]   Spit on him in a spat.
[02:11:36.580 --> 02:11:42.140]   If I'm having a spat with him and I spit on him, did I spit during the spat or spat during
[02:11:42.140 --> 02:11:43.140]   the spat?
[02:11:43.140 --> 02:11:44.140]   I don't know.
[02:11:44.140 --> 02:11:46.340]   I spat during the spat.
[02:11:46.340 --> 02:11:51.820]   I'll turn it usage spout.
[02:11:51.820 --> 02:11:57.700]   I did not see that story, but I am not going to fall for your leftist plan.
[02:11:57.700 --> 02:11:59.140]   That's right.
[02:11:59.140 --> 02:12:04.940]   So there I presume that you will be glued to the television later this evening when the
[02:12:04.940 --> 02:12:09.060]   3000 candidates for president on the Democratic Party will be on stage.
[02:12:09.060 --> 02:12:11.540]   He's trying to get one word in edgewise.
[02:12:11.540 --> 02:12:15.660]   It'll be much like this show was five hours.
[02:12:15.660 --> 02:12:16.660]   No, I don't.
[02:12:16.660 --> 02:12:18.060]   This was a this is not five hours.
[02:12:18.060 --> 02:12:19.060]   Was it?
[02:12:19.060 --> 02:12:20.060]   No, I'm not hungry.
[02:12:20.060 --> 02:12:21.780]   No, the debates are going to be debates are going to be.
[02:12:21.780 --> 02:12:22.780]   Aren't they constrained?
[02:12:22.780 --> 02:12:26.940]   I mean, aren't they like over when they can't have just keep going like this show can.
[02:12:26.940 --> 02:12:27.940]   Sure they can.
[02:12:27.940 --> 02:12:28.940]   They can.
[02:12:28.940 --> 02:12:30.540]   No, they have a moderator.
[02:12:30.540 --> 02:12:32.340]   That's what the moderators for.
[02:12:32.340 --> 02:12:33.340]   Yeah.
[02:12:33.340 --> 02:12:34.340]   Hurry up.
[02:12:34.340 --> 02:12:35.340]   Dream job.
[02:12:35.340 --> 02:12:40.340]   I imagine technology will be the topic.
[02:12:40.340 --> 02:12:43.940]   Any bets on how many candidates will demand the breakup of Google?
[02:12:43.940 --> 02:12:46.700]   Well, we know Warren was with Warren on tonight.
[02:12:46.700 --> 02:12:47.700]   She will.
[02:12:47.700 --> 02:12:48.700]   Yeah.
[02:12:48.700 --> 02:12:50.940]   But don't you think that that will then make everybody else do that?
[02:12:50.940 --> 02:12:54.340]   Like, oh, we got to keep up with the Warrens.
[02:12:54.340 --> 02:12:59.540]   No, because a lot of their funding is going to come from these companies.
[02:12:59.540 --> 02:13:02.980]   Oh, they are for all all Democrats.
[02:13:02.980 --> 02:13:05.820]   Oh, yes, all of them.
[02:13:05.820 --> 02:13:09.780]   Man, I can't wait for people like, you know, the you line people.
[02:13:09.780 --> 02:13:14.340]   The problem is right now tech tech is such a not you know what?
[02:13:14.340 --> 02:13:15.340]   Let's in the show.
[02:13:15.340 --> 02:13:16.340]   We're good.
[02:13:16.340 --> 02:13:17.340]   We're good.
[02:13:17.340 --> 02:13:18.340]   Yeah.
[02:13:18.340 --> 02:13:19.340]   I'm not going.
[02:13:19.340 --> 02:13:23.660]   I don't know what the hell they mean, but I'm going to assume that they were cogent and
[02:13:23.660 --> 02:13:25.540]   say thank you for your contribution.
[02:13:25.540 --> 02:13:26.540]   Stacy Thistlebottom.
[02:13:26.540 --> 02:13:27.540]   Hip, hip, cheerio.
[02:13:27.540 --> 02:13:32.540]   Hello, Stacy and Johnny Goode.
[02:13:32.540 --> 02:13:34.780]   Stacy on IOT.com is her website.
[02:13:34.780 --> 02:13:37.980]   That's where you can sign up for a free newsletter on the Internet of Things.
[02:13:37.980 --> 02:13:42.380]   You should also listen to her IOT podcast with Kevin Tofol every week.
[02:13:42.380 --> 02:13:48.100]   And if I have not offended her to the marrow, she will be back next week on this week in
[02:13:48.100 --> 02:13:49.100]   Google.
[02:13:49.100 --> 02:13:52.700]   Maybe could be perhaps for chance.
[02:13:52.700 --> 02:13:54.540]   I'm not easily offended.
[02:13:54.540 --> 02:13:55.540]   Thank you, Stacy.
[02:13:55.540 --> 02:14:00.820]   Thank you for being here and putting up with us old white men.
[02:14:00.820 --> 02:14:05.340]   Chief of Wuhum is of course the great Jeff Jarvis, professor of journalism.
[02:14:05.340 --> 02:14:08.580]   The whitest oldest white man around.
[02:14:08.580 --> 02:14:10.820]   I think though, I think we're of an age.
[02:14:10.820 --> 02:14:14.700]   So I don't know if you can actually say you're the whitest oldest man here.
[02:14:14.700 --> 02:14:19.180]   But you know, my Chromebook camera sure beats the heck out of that independent camera.
[02:14:19.180 --> 02:14:20.380]   I actually look like a human.
[02:14:20.380 --> 02:14:21.380]   This one looks good.
[02:14:21.380 --> 02:14:22.380]   It's doing well.
[02:14:22.380 --> 02:14:23.380]   Yeah.
[02:14:23.380 --> 02:14:24.380]   Yeah.
[02:14:24.380 --> 02:14:25.380]   Nice.
[02:14:25.380 --> 02:14:28.700]   Jeff, maybe you're just meant to be seeing the Chromebook, Jeff.
[02:14:28.700 --> 02:14:29.700]   Could be.
[02:14:29.700 --> 02:14:35.260]   Will you be appearing on us on MSNBC or as a TV guide, former TV guide critic anytime
[02:14:35.260 --> 02:14:36.260]   soon?
[02:14:36.260 --> 02:14:37.820]   That's a good schedule.
[02:14:37.820 --> 02:14:38.820]   Okay.
[02:14:38.820 --> 02:14:41.340]   Want to give you a chance to plug that.
[02:14:41.340 --> 02:14:43.940]   And ladies and gentlemen, of course, the great Matthew Ingram.
[02:14:43.940 --> 02:14:50.340]   He is in charge of digital writing and soon to win the fabulous.
[02:14:50.340 --> 02:14:51.500]   James W.K.
[02:14:51.500 --> 02:14:52.500]   James W.K.
[02:14:52.500 --> 02:14:53.500]   Award.
[02:14:53.500 --> 02:14:54.500]   I shouldn't forget that name.
[02:14:54.500 --> 02:14:56.300]   I used to work with a guy named James Carey.
[02:14:56.300 --> 02:14:57.300]   He was a home of.
[02:14:57.300 --> 02:14:58.300]   Not Jim Carey.
[02:14:58.300 --> 02:14:59.300]   No, not Jim Carey.
[02:14:59.300 --> 02:15:00.300]   James Carey.
[02:15:00.300 --> 02:15:01.300]   He was one of the Carey brothers.
[02:15:01.300 --> 02:15:03.700]   He went, they were home of Brumans specialists.
[02:15:03.700 --> 02:15:05.580]   But that's not the same one.
[02:15:05.580 --> 02:15:06.900]   I don't think the James W.K.
[02:15:06.900 --> 02:15:07.900]   I don't think so.
[02:15:07.900 --> 02:15:11.860]   Award for media ethics or something.
[02:15:11.860 --> 02:15:13.420]   Thank you.
[02:15:13.420 --> 02:15:16.220]   That's the official name or something.
[02:15:16.220 --> 02:15:17.220]   There's something.
[02:15:17.220 --> 02:15:18.540]   Thank you.
[02:15:18.540 --> 02:15:21.700]   Make digital writer at cjr.org.
[02:15:21.700 --> 02:15:25.820]   Thank you all for joining us.
[02:15:25.820 --> 02:15:28.820]   We do this week in Google.
[02:15:28.820 --> 02:15:34.500]   The car wreck that it is every Wednesday, 130 Pacific, 430 Eastern.
[02:15:34.500 --> 02:15:35.500]   I love it.
[02:15:35.500 --> 02:15:37.420]   It's the messiest show we do and I love it.
[02:15:37.420 --> 02:15:38.420]   20, 30 UTC.
[02:15:38.420 --> 02:15:43.140]   If you want to watch or listen live, you can do that at twitter.tv/live.
[02:15:43.140 --> 02:15:44.660]   The chat room awaits.
[02:15:44.660 --> 02:15:49.580]   If you're doing it live at irc.twit.tv, a bunch of great people.
[02:15:49.580 --> 02:15:53.100]   Congratulations Matthew right now on his major award.
[02:15:53.100 --> 02:15:57.540]   We also invite you to get on-demand versions at our website, twit.tv/twig.
[02:15:57.540 --> 02:16:02.700]   Or as I mentioned earlier, we really appreciate you supporting us by subscribing.
[02:16:02.700 --> 02:16:06.340]   That way you'll get it automatically every single week, the minute it's available.
[02:16:06.340 --> 02:16:07.340]   And then you don't have to worry.
[02:16:07.340 --> 02:16:09.100]   You just say, "Hey, it's Thursday morning.
[02:16:09.100 --> 02:16:10.100]   What can I listen to?"
[02:16:10.100 --> 02:16:11.620]   Oh yeah, twig.
[02:16:11.620 --> 02:16:12.620]   Thanks everybody.
[02:16:12.620 --> 02:16:13.860]   We'll see you next time on this week in Google.
[02:16:13.860 --> 02:16:23.860]   [MUSIC]

