;FFMETADATA1
title=Mechanical Turking
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=396
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.080]   It's time for Twig this week in Google. Jeff Jarvis has the week off but hey the great
[00:00:04.080 --> 00:00:08.680]   Danny Sullivan from Search Engine land joins us as we talk about South by Southwest Stacey
[00:00:08.680 --> 00:00:12.360]   Digginbotham's the expert. She lives there. She's even presenting. We'll find out what
[00:00:12.360 --> 00:00:17.160]   was new there. The latest from Google's cloud next last week's big Google conference and
[00:00:17.160 --> 00:00:21.360]   Stacey's going to help me rewire my house. It's all coming up next on Twig.
[00:00:21.360 --> 00:00:34.400]   NetCasts you love. From people you trust. This is Twig.
[00:00:34.400 --> 00:00:47.160]   Bandwidth for this week in Google is provided by cash fly. This is Twig this week in Google
[00:00:47.160 --> 00:00:55.760]   episode 396 recorded Wednesday March 15th 2017. Mechanical Turking. This week in Google
[00:00:55.760 --> 00:01:00.480]   is brought to you by Eero. I settle for just a Wi-Fi router when you can have a brilliant
[00:01:00.480 --> 00:01:06.320]   hyper-fast super simple Wi-Fi system. No more buffering. No more dead zones. Finally Wi-Fi
[00:01:06.320 --> 00:01:13.160]   that works. To get Eero visit Eero.com Best Buy or Amazon today.
[00:01:13.160 --> 00:01:18.400]   And by Curry the real live robot with a great personality. Curry is available for preorder
[00:01:18.400 --> 00:01:27.600]   now at hey curry.com that's h-e-y-k-u-r-i.com. And by segment the leading unified customer
[00:01:27.600 --> 00:01:33.120]   data platform. Segment collects data across all your websites, mobile apps and cloud platforms
[00:01:33.120 --> 00:01:38.880]   and delivers it to over 173rd party tools and apps. Get three months free on the segment
[00:01:38.880 --> 00:01:46.840]   team plan when you sign up at segment.com/twit. It's time for Twig. This week in Google is
[00:01:46.840 --> 00:01:53.680]   show we cover the latest news of the cloud including Google with everything else.
[00:01:53.680 --> 00:01:59.000]   Stacy Higginbotham joining us as always from Austin Texas home a South by Southwest taking
[00:01:59.000 --> 00:02:07.800]   a break from the party round to be with us today. She's at IOT podcast.com. Hi Stacy.
[00:02:07.800 --> 00:02:13.240]   Are you being silent? Oh yes. I'm sorry. I was like. And do you.
[00:02:13.240 --> 00:02:19.840]   Audio program. If you nod no one will hear you scream. I wasn't even nodding. I was
[00:02:19.840 --> 00:02:27.920]   just kind of like. I'm going to be great today. Let me tell you.
[00:02:27.920 --> 00:02:32.520]   Well, I'm going to ask you about South by just a second. I also join is so we were going
[00:02:32.520 --> 00:02:39.200]   to have Jeff from London. But it turns out as one would expect that his pixel C on London
[00:02:39.200 --> 00:02:44.520]   Wi-Fi is not a good mix. So we're going to instead have somebody great Danny Sullivan
[00:02:44.520 --> 00:02:47.160]   for search engine land. Always a pleasure to have you on.
[00:02:47.160 --> 00:02:52.520]   Good to be here. I shall try to channel Jeff's cremeging dalingas. Get cranky. Get cranky.
[00:02:52.520 --> 00:02:59.120]   Is that golden gate? I think I can do it. Behind you there. Oh yeah. It's an old like
[00:02:59.120 --> 00:03:02.440]   Disney sold them once. It's like a monorail. I got to put the monorail back up. Oh, I'm
[00:03:02.440 --> 00:03:07.360]   going to go through it. Yeah, that's cool. You have a monorail in your house. Wow. Yeah,
[00:03:07.360 --> 00:03:13.760]   I can't even see it's a cropped out on my. Well, we see it just in case, you know, if
[00:03:13.760 --> 00:03:18.360]   a child comes from scrolling in. Oh yeah, my wife's all set to make that.
[00:03:18.360 --> 00:03:21.160]   She's ready to do. You're like, you're going to be doing a thing today. I'm going to send
[00:03:21.160 --> 00:03:26.400]   the boys in. Oh, that was the best. It was so funny. We're talking about the BBC reporter,
[00:03:26.400 --> 00:03:30.480]   which of course is old news now, but it's so funny because on Twitter, it would be no
[00:03:30.480 --> 00:03:35.960]   big deal on any of our shows with kids come in and we go, Oh, who's that? But on the BBC,
[00:03:35.960 --> 00:03:40.520]   of course, it's very serious. He also we found out was not wearing dress pants. He's
[00:03:40.520 --> 00:03:44.960]   wearing blue jeans. Yeah. Let's not talk about the sweats on. So you didn't want to
[00:03:44.960 --> 00:03:49.440]   see that. I don't want to let down my t-shirt. Did you see the Wall Street Journal? They
[00:03:49.440 --> 00:03:54.720]   did a follow up. Yeah. It was great. And they tweeted today that their interview that
[00:03:54.720 --> 00:03:58.960]   they did with them afterwards has been the most viewed thing the Wall Street Journal has ever
[00:03:58.960 --> 00:04:08.480]   had. Isn't that historical? He's now by the way called BBC dad. Wow, it's most successful
[00:04:08.480 --> 00:04:11.720]   piece ever. Yeah, because every once in a while, what's the deal? There's there's his
[00:04:11.720 --> 00:04:17.080]   wife in the background grabbing the son who was on a rolly, curly things. And then the
[00:04:17.080 --> 00:04:22.520]   kind of guy. Great. He's talking about how he was like, he heard his daughter come in.
[00:04:22.520 --> 00:04:25.560]   He's trying to hold it together. He says, and then when he heard the baby roll, what he
[00:04:25.560 --> 00:04:34.000]   like, then I knew it was over. It was over any pretense of seriousness or dignity out
[00:04:34.000 --> 00:04:38.800]   the window. But best take on it was Ben Thompson. And we read it on Twitter Sunday, but Ben
[00:04:38.800 --> 00:04:43.520]   Thompson of Stratecary, who was in a similar situation, completely, deeply understood what
[00:04:43.520 --> 00:04:49.560]   was going on. And he he'd glossed the whole thing beautifully. So South by is in Austin.
[00:04:49.560 --> 00:04:57.840]   I heard a piece on the 60 DB today. 60 DB's founder was in Austin and was lined up to
[00:04:57.840 --> 00:05:04.320]   go to tech crunch's event. The line was so long, he couldn't get in. He didn't, you know,
[00:05:04.320 --> 00:05:08.760]   the whole event is to enter, you know, to show, you know, like entrepreneurs to other
[00:05:08.760 --> 00:05:13.480]   anyway, couldn't get in. So he goes down the line to see who's who's in this line. They
[00:05:13.480 --> 00:05:20.840]   were all Austinites just looking for free food. Apparently there's a spreadsheet circulating
[00:05:20.840 --> 00:05:29.120]   in Austin that says which parties have free food. And so he said there wasn't a single
[00:05:29.120 --> 00:05:33.600]   entrepreneur in the line for the tech crunch party. Are you one of those?
[00:05:33.600 --> 00:05:41.320]   Wow. No, no, I, I don't wait in line for anything. And not because I'm snobby. It's
[00:05:41.320 --> 00:05:46.720]   just because I'm like, eh, it ain't worth it. There ain't no time for that.
[00:05:46.720 --> 00:05:54.360]   So this will be my 16th South by Wow. And this year I took it pretty easy. So I can't
[00:05:54.360 --> 00:05:58.560]   tell you too many exciting things. I can tell you about like Google's project,
[00:05:58.560 --> 00:06:04.560]   Jacar jacket, which looks pretty cool. And is delayed even more. And just is yeah, it's
[00:06:04.560 --> 00:06:12.040]   very hard to put, you know, haptic feedback and silver fabric and to weave silver fabric
[00:06:12.040 --> 00:06:17.120]   into a $350 jacket. Did they have? Give them a break. Did they have one? Oh, look,
[00:06:17.120 --> 00:06:26.680]   here's here's the spreadsheet free at SSSW. So it used to be that South by this was a
[00:06:26.680 --> 00:06:33.240]   huge deal because like, I remember when I started going, you know, in the early aughts,
[00:06:33.240 --> 00:06:38.000]   everybody had food. There was beer and wine and hard alcohol everywhere, but you could
[00:06:38.000 --> 00:06:43.880]   never get food. Interesting. But now it's, it's really easy to get food. Yeah, I see
[00:06:43.880 --> 00:06:47.480]   all these people complaining about not having an Uber and Lyft and I'm like, back in the
[00:06:47.480 --> 00:06:49.640]   day. Oh, you had to walk.
[00:06:49.640 --> 00:06:54.680]   Let's talk. Well, and you know, of course, South, first of all, Austin's a great town.
[00:06:54.680 --> 00:06:58.920]   And South by is more than South by Southwest interactive. It's a film festival and it's
[00:06:58.920 --> 00:07:05.960]   a music festival and and it's an interactive festival over three weeks. It, you know, it
[00:07:05.960 --> 00:07:09.680]   made its name in the interactive. I mean, used to be very small, right? Just a few thousand
[00:07:09.680 --> 00:07:15.640]   people made its name in the interactive space after Twitter made their debut there in 2006
[00:07:15.640 --> 00:07:22.080]   or maybe it was 2007 2007. And then four square the next year and maybe go all which is an
[00:07:22.080 --> 00:07:29.440]   Austin version of four square. And who else? Meer cat, I guess a few years ago was all
[00:07:29.440 --> 00:07:34.640]   the rage. Meer cat highlight was one of the ones that everyone was for a while. I'm the
[00:07:34.640 --> 00:07:43.940]   breakout app. Yeah. You know, really, I love South by it is now it is mostly marketers
[00:07:43.940 --> 00:07:47.760]   talking to other marketers and I guess for Austin, I it's looking for free food. Yeah.
[00:07:47.760 --> 00:07:53.280]   And it's kind of sad in South by the people who put it on have really started to like
[00:07:53.280 --> 00:08:03.520]   put the hammer down on like having registered events close to the convention center as part
[00:08:03.520 --> 00:08:10.080]   of this way to like crack down and get people here. The other thing is Spotify wasn't here this
[00:08:10.080 --> 00:08:15.600]   year. I didn't see a Facebook like presence. They used to have like a house or a bar they would
[00:08:15.600 --> 00:08:23.360]   rent out. Samsung didn't have a big presence there. They sent people. So it feels very much like
[00:08:23.360 --> 00:08:32.240]   it's kind of drifting away in relevance for the when I saw that CNN had taken over an entire
[00:08:32.240 --> 00:08:38.080]   restaurant and venue and and and they did by the way they were there. Jake Tapper did some good
[00:08:38.080 --> 00:08:44.800]   interviews a bit with it. I thought this has become mainstream. This is no longer a geek conference
[00:08:45.440 --> 00:08:50.000]   and I guess there's some very good seminars. In fact, I know a lot of people, you know,
[00:08:50.000 --> 00:08:56.320]   people in our circle who had great seminars there. But it doesn't seem to be some really
[00:08:56.320 --> 00:09:04.320]   South by these days. Is it a party? So that's it's like a big party and some really intellectually
[00:09:04.320 --> 00:09:10.960]   cool content. Like honestly, this sounds incredibly nerdy, but I have more fun going to
[00:09:11.680 --> 00:09:16.640]   the the seminars and the panels. Yeah. And not the big ones like Joe Biden and all of that.
[00:09:16.640 --> 00:09:24.240]   It's more like the guy who kind of discovered the CRISPR gene editing. Yeah, that would be great.
[00:09:24.240 --> 00:09:29.840]   Talking about see, so there's that kind of stuff in previous years, I had gone to see like Slack
[00:09:29.840 --> 00:09:35.440]   physicists talking about like beyond Moore's law, how to use like the spin of electrons to
[00:09:36.880 --> 00:09:44.640]   make a transistor. That's neat. Like again, super nerdy, but there's also like very cool social
[00:09:44.640 --> 00:09:51.200]   stuff and there's cool stuff about like medicine and food. But you kind of have to find it on your
[00:09:51.200 --> 00:09:57.120]   own and it's totally easy to get lost and just going to party to party to party to party. Yeah.
[00:09:57.120 --> 00:10:03.360]   Yeah. Yeah. It became a social event and less of a tech event, although I envy you because you're
[00:10:03.360 --> 00:10:09.520]   in Austin. So it's easy for you to go. And if I, you know, it's hard to fly down there,
[00:10:09.520 --> 00:10:13.760]   impossible to get a hotel unless you arrange for it a year ahead of time. So by being the
[00:10:13.760 --> 00:10:20.240]   other. Yeah, you could have stayed at Casa de Stacy. The Higginbotham. Yes. No, I wouldn't.
[00:10:20.240 --> 00:10:22.560]   I would have printed out the list of commands. So you could
[00:10:22.560 --> 00:10:30.000]   echo turn on the light, make me breakfast. So you saw the
[00:10:31.520 --> 00:10:34.240]   you saw the connected smart jacket or did they have one there?
[00:10:34.240 --> 00:10:40.080]   They had a prototype there. Yeah. Because they're going to be delayed a little bit, right?
[00:10:40.080 --> 00:10:44.240]   Yes. They were supposed to be spring of the year and now they're going to be fall.
[00:10:44.240 --> 00:10:51.040]   This is the jacar technology that we were talking about at WW, I'm sorry, Google I/O last year.
[00:10:51.040 --> 00:10:56.960]   Was that last year or two years ago? It's been a while. Yeah. Yeah. I feel like it's been a long
[00:10:56.960 --> 00:11:01.600]   time. It's cool. A-tap project. So this is the device. Yes, they do.
[00:11:01.600 --> 00:11:08.960]   They still do. They didn't sell a tap or anything. So there's, this is a denim jacket that's connected.
[00:11:08.960 --> 00:11:16.480]   How? I mean, all I see right now is he's tapping a button on the sleeve to play his music. Big deal.
[00:11:16.480 --> 00:11:23.200]   It connects to your phone. There's also, so there's a, the jacket, what's cool about jacar
[00:11:23.200 --> 00:11:30.800]   is actually that it's a way to make fabric that is conductive. So silver infused fabric.
[00:11:30.800 --> 00:11:35.280]   Okay. So that's confusing in this video because he has a big plastic button on his cuff, but that's
[00:11:35.280 --> 00:11:43.360]   not not required. So, well, so I'm not sure about that. What it has is you add to that fabric,
[00:11:43.360 --> 00:11:49.440]   it's got somewhere on there a pouch where you put electronics. Right. And that's what
[00:11:50.320 --> 00:11:56.640]   unquote powers it. I don't know if it's the pack is your smartphone or there's an additional.
[00:11:56.640 --> 00:11:58.800]   Do you have to have a battery for your jacket?
[00:11:58.800 --> 00:12:05.920]   Oh, I read something that like you have a little cuff link thing that's part of it.
[00:12:05.920 --> 00:12:09.840]   So like this little cuff link part that's that's that little silver thing.
[00:12:09.840 --> 00:12:15.520]   That's the power. It's horrific. Take that off before you wash it. I would always remember to
[00:12:15.520 --> 00:12:20.800]   take that off. Oh, yeah, of course. I would never, I would never wash it like this fit, which by the
[00:12:20.800 --> 00:12:25.680]   way, these hold up really well in the wash. I can attest to that. The original ones did not,
[00:12:25.680 --> 00:12:32.400]   because I washed through or for. Yes. Okay. So there's a plastic thing that you have to
[00:12:32.400 --> 00:12:36.080]   remove before you wash it. That must, okay. So that's a little, there is a little battery in it.
[00:12:36.080 --> 00:12:40.400]   I don't know. And it's going to cost by the way, $350.
[00:12:42.560 --> 00:12:47.360]   Okay. I paid $350 for a wool coat that does nothing but keep me warm.
[00:12:47.360 --> 00:12:52.000]   Okay. So you're going to run out and buy this when it comes out this fall.
[00:12:52.000 --> 00:12:59.520]   So I can't tell my left from my right and I'm kind of excited by the idea of like, how to keep back.
[00:12:59.520 --> 00:13:05.840]   Oh, so, oh, so I know why. So it's not merely connective. It actually buzzes and vibrates that
[00:13:05.840 --> 00:13:10.560]   you with the whole jacket or just part of it. I think it's just the cuff. Oh,
[00:13:12.000 --> 00:13:17.840]   I'm reading something that says it's just that cuff. Yeah. Like big deal, why don't you just
[00:13:17.840 --> 00:13:24.560]   sell me like a cuff I can put on my jacket. I'm more fascinated by the knowing the left and right,
[00:13:24.560 --> 00:13:30.800]   because I'm that way too. And my son's that way as well. And it was great. I really, I don't know.
[00:13:30.800 --> 00:13:34.320]   Yeah. No, when I was teaching him to drive, I would say go left and he meant,
[00:13:34.320 --> 00:13:39.200]   I meant to go right, but he interpreted me selling him to go left as going right,
[00:13:39.200 --> 00:13:43.360]   because he was getting mixed up and it was great. But I'm ready for a support society.
[00:13:43.360 --> 00:13:47.840]   Do you think the society can help of being freaky that like,
[00:13:47.840 --> 00:13:51.520]   because in the real world, most people know they're left from their right.
[00:13:51.520 --> 00:13:57.120]   You know, I don't know. Did you lose this overtime or did you have it and lose it?
[00:13:57.120 --> 00:14:01.600]   No, I always like to work for the Pledge of Allegiance. I'd be lost.
[00:14:01.600 --> 00:14:05.600]   That's a thing. If you wear a ring, you know, that's, you know, that's your ring hand or something.
[00:14:05.600 --> 00:14:11.680]   I mean, somebody like that. No, I wear. I'm like, left. I think it's a brain thing.
[00:14:11.680 --> 00:14:16.560]   I really do. I always say make a make a Stacy if I'm driving. So then that means
[00:14:16.560 --> 00:14:21.040]   make the left or make or make somebody, whoever my passengers, make a Stacy.
[00:14:21.040 --> 00:14:26.000]   So that's my pro here. So make go go my way or go your way.
[00:14:26.000 --> 00:14:31.440]   And then when I drive, or passenger, wow, in Brittany, we get confusing because I would
[00:14:31.440 --> 00:14:35.040]   drive there. And I would always just think of left turns as dangerous turns and right turns
[00:14:35.040 --> 00:14:38.720]   a safe turn. So then my wife would tell me to turn right. And I would think,
[00:14:38.720 --> 00:14:43.360]   oh, turn right. That's the safe turn. But then I start to go left. But she's no, I said go right.
[00:14:43.360 --> 00:14:45.520]   Yeah, she's screaming how old and go.
[00:14:45.520 --> 00:14:50.160]   Ah, that's we're going to die. Man, it should survive.
[00:14:50.160 --> 00:14:57.600]   All right. So there's the jacket of perhaps dubious value. What else did you see? Anything else?
[00:14:57.600 --> 00:15:02.240]   Like South by that you that we we pour slobs over yet. Let's see.
[00:15:03.200 --> 00:15:04.960]   No, it's only half way through. No, I didn't.
[00:15:04.960 --> 00:15:07.600]   I've had a panel.
[00:15:07.600 --> 00:15:13.120]   Without Uber and Lyft. They say, I don't even know how you survive. How are you getting around?
[00:15:13.120 --> 00:15:20.960]   It's so hard. Well, that's the very first day, Sunday, the two local ride sharing services had
[00:15:20.960 --> 00:15:25.440]   problems, right? Well, the first day was actually Friday. Oh, okay.
[00:15:27.360 --> 00:15:35.520]   We had really terrible weather. Like, so when or sorry, on Saturday, like it was raining and cold
[00:15:35.520 --> 00:15:41.520]   the whole day. So all of the ride shares were overwhelmed. And we have four of them. And
[00:15:41.520 --> 00:15:46.560]   like I got down and it was like 2x surge pricing. So nothing terrible.
[00:15:46.560 --> 00:15:53.040]   And then yeah, so Saturday night was really hard for people. I have a friend who actually
[00:15:53.040 --> 00:15:56.480]   took a ride in some stranger's car. I was just like, I'll do that.
[00:15:56.480 --> 00:16:02.960]   Yeah, she just walked in. It's like, this guy is like, oh, I'm an independent ride sharing service.
[00:16:02.960 --> 00:16:04.240]   Do you want to ride?
[00:16:04.240 --> 00:16:10.160]   Chris Messina. Chris Messina was saying, I wanted the hail and Uber. I couldn't. So I just
[00:16:10.160 --> 00:16:14.320]   called a cab and then two other people got in the back. And I said, we've by the way, Chris used to
[00:16:14.320 --> 00:16:20.240]   be on the Uber board. We've created Uber sharing and then you drove off. So that's, you know what?
[00:16:20.240 --> 00:16:26.480]   That's awesome. That's great. So ride Austin is the big one, right? Is that the big local ride
[00:16:26.480 --> 00:16:33.200]   sharing? Right Austin is the big one. Then there's fasten, which is, I think gives more money to the
[00:16:33.200 --> 00:16:43.760]   drivers. And then there's fair. So apparently, uh, fasten got really slow on Saturday night.
[00:16:43.760 --> 00:16:49.920]   Busy hour, the busy night. So because of that, people moved over to ride Austin.
[00:16:50.480 --> 00:16:56.800]   Which then crashed for several hours, five hours. Yeah. Right Austin's app was terrible.
[00:16:56.800 --> 00:17:02.960]   Yeah. And so all this did is tell all the geeks from Silicon Valley, aren't you lucky you have
[00:17:02.960 --> 00:17:08.000]   Uber? Ryan Hoover of product hunts tweeted, spend an hour trying to find a ride. Austin
[00:17:08.000 --> 00:17:12.800]   is broken without Uber or Lyft. Other ride sharing apps aren't working. And all the taxis are full.
[00:17:12.800 --> 00:17:22.880]   He was a wet, mad as a wet hen. So slate actually had a great article talking about this. And the
[00:17:22.880 --> 00:17:30.080]   key point that I thought was so awesome about it was these people, the people coming in, it shows how
[00:17:30.080 --> 00:17:37.200]   for well off people, transportation isn't broken. They never think about public transport.
[00:17:37.200 --> 00:17:43.760]   And that has huge implications for cities. That's right. And maybe this is a little fairy tale,
[00:17:43.760 --> 00:17:50.000]   but I actually thought that was a really good point that people are complaining about. Yeah.
[00:17:50.000 --> 00:17:54.480]   Politician, it's the same thing with healthcare, by the way. Politicians don't ever have to take
[00:17:54.480 --> 00:18:00.960]   subways or worry about health insurance. So it really changes their perspective on the whole thing.
[00:18:01.680 --> 00:18:10.640]   And what's really said, you look at a city like LA where for whatever reasons you could talk
[00:18:10.640 --> 00:18:15.200]   about all the conspiracy theories, they never really got a very robust mass transit system.
[00:18:15.200 --> 00:18:21.440]   It became a car culture. And it makes it hard. It's not a very good city because it's spread out
[00:18:21.440 --> 00:18:27.200]   so much because of all the cars. And so you really could damage a city and then you look at New
[00:18:27.200 --> 00:18:34.080]   York, which is forced to coaxes down a small island. And the subways were built before politicians
[00:18:34.080 --> 00:18:41.760]   came around or whatever. Although it's even harder with LA because it's not. It's like LA is one of
[00:18:41.760 --> 00:18:47.520]   something like 70 cities in the greater Southern California area. It's hard to get anywhere because
[00:18:47.520 --> 00:18:54.560]   you have to drive on congested freeways that are congested 24/7. I know the people,
[00:18:55.440 --> 00:19:01.040]   we don't have it here in Orange County, but the growth of the Metro and LA has become so strong that
[00:19:01.040 --> 00:19:05.680]   now people are complaining they can't get on. There's not enough cars. So this is a regular. So
[00:19:05.680 --> 00:19:10.240]   I want to clear it's been successful to the degree that now they're losing people because they can't
[00:19:10.240 --> 00:19:15.280]   accommodate them all. I don't know if that's successful. They are trying to do light rail
[00:19:15.280 --> 00:19:18.800]   right. And I think the buses are getting better. I mean, I think there's some movement to get some
[00:19:18.800 --> 00:19:24.160]   mass transit in LA. Oh, there's huge. No, I mean, there's been huge. LA's got a huge mass transit
[00:19:24.160 --> 00:19:28.960]   system now. That's what the Metro light rail system is all about. The whole thing where you can go
[00:19:28.960 --> 00:19:33.040]   down to the ocean, you can ride all over the way. They keep adding and adding and adding more to it.
[00:19:33.040 --> 00:19:38.560]   But that's been the problem. It's actually become successful to the degree that they can't accommodate
[00:19:38.560 --> 00:19:43.920]   the people that keep coming onto the network as it's grown. Now, figure out how did they handle
[00:19:43.920 --> 00:19:48.080]   adding on even more cars. Good. But the ice phase cars, you didn't mean automobiles. You
[00:19:48.080 --> 00:19:54.880]   meant train cars. Train cars. Yeah, that's what I meant. They're so full that they have to leave
[00:19:54.880 --> 00:20:04.800]   without people being old. That's good. All right. Any you're doing a panel on what Stacy?
[00:20:04.800 --> 00:20:13.360]   Oh, intelligent living in the smart home. It's got Amazon, Sonos and smart things. Oh, fun.
[00:20:13.360 --> 00:20:18.800]   And you know what I'm going to ask them about? What? The Amazon Sonos integration and when
[00:20:18.800 --> 00:20:25.200]   that's going to happen. Yes, please. And spy. Please. I hope you will. I'm like, I'm more excited
[00:20:25.200 --> 00:20:28.960]   about one as opposed to the other. But yeah, they're not going to talk too much about spying. But
[00:20:28.960 --> 00:20:33.120]   it's worth a shot. I bet. So that's really the question. Will they talk more about spying or
[00:20:33.120 --> 00:20:39.120]   the Sonos integration? I'll give them that choice. I'll be like, here's your topics. Pick one.
[00:20:41.760 --> 00:20:47.200]   I think they haven't talked much about either ever since last year. Well, that'll be fun.
[00:20:47.200 --> 00:20:52.160]   Good. So you're doing a panel and you have any. Is there any way we could see that? Do they
[00:20:52.160 --> 00:20:59.520]   stream those now or? They stream some of them. I guess I should figure that out. Because I'd
[00:20:59.520 --> 00:21:06.960]   love to watch it. I just mere cat it. That's what I'll do. Well, now I'm looking. I went to
[00:21:06.960 --> 00:21:14.720]   southbysouthwest.com/live and they are doing live streaming of limited conferences.
[00:21:14.720 --> 00:21:20.640]   Actually, conversations doesn't look like seminars. They want you to pay the seminar fee. So I'd be
[00:21:20.640 --> 00:21:27.040]   very surprised if they'd stream any of those panels. So there are some things. Some of them do ones
[00:21:27.040 --> 00:21:32.960]   that they feel are going to be popular. Let's see. You're on Wednesday or Thursday. So
[00:21:32.960 --> 00:21:39.280]   did you preview Bailey and Covenant? Now you have very quick eyes and Buzz Aldrin was there. The Wu
[00:21:39.280 --> 00:21:50.800]   Tang Clan. Chuck Todd. Wu Tang. Dang. Raker's while. Steve Case. All right. Let's go to Thursday.
[00:21:50.800 --> 00:21:55.520]   Somehow I don't see Stacy on here. Steve Case is there everywhere. I know. Steve is here.
[00:21:56.800 --> 00:22:03.680]   Founder and Chief Strategy Officer for Dot Blockchain Music and Pledge Music. Art installation.
[00:22:03.680 --> 00:22:10.400]   No, yeah, they're not streaming much. Yeah. Yeah. Okay. Next week they'll stream. Sorry.
[00:22:10.400 --> 00:22:16.000]   Yeah. Do they? I'll tell you all about it. Do they ever? Oh, so okay. You could do if you go to
[00:22:16.000 --> 00:22:21.440]   their YouTube channels, they put past videos up. So you might maybe we'll catch Stacy later.
[00:22:22.880 --> 00:22:29.200]   That would be really nice. Or you can come on down. Come on down. If you're in the Austin area,
[00:22:29.200 --> 00:22:38.880]   even if you're not, come on down. I like staying at the Driscoll. Oh, yeah. That's my favorite hotel.
[00:22:38.880 --> 00:22:42.800]   Yeah. And I know there's secret entrance to get past the line.
[00:22:42.800 --> 00:22:48.720]   You go downstairs, you walk across, you go up all the way around, you come down the back
[00:22:48.720 --> 00:22:53.760]   staircase, and then you're in the bar. There must be annoying to native Austinites like you, Stacy,
[00:22:53.760 --> 00:22:59.120]   that all of this tech people say, oh, I know my way around Austin.
[00:22:59.120 --> 00:23:07.360]   Hey, if it weren't for Southside. Doesn't bother me at all. It's good for Austin, right?
[00:23:07.360 --> 00:23:12.960]   You know, exactly. I'm like, do I find Southside a little bit annoying? Yes.
[00:23:13.520 --> 00:23:18.720]   Do I am I mad at it? No, because it brings like $150 million. Yeah.
[00:23:18.720 --> 00:23:22.320]   Are you close enough to town that ties up your traffic and stuff like that?
[00:23:22.320 --> 00:23:30.240]   It's pretty localized to downtown and I'm about 12 to 15 minutes away. Oh, yes. So you're fine. Yeah.
[00:23:30.240 --> 00:23:37.360]   I just like I sometimes am sad because I'm like, oh, I'm downtown and I can eat at this place.
[00:23:37.360 --> 00:23:43.120]   And then it's already taken over by Southside. Three weeks a year. Three weeks a year.
[00:23:44.080 --> 00:23:49.920]   Austin is a really fun city. You've got Alamo draft house. Some of my greatest memories are
[00:23:49.920 --> 00:23:55.360]   in Austin's crowd surfing at stubs. Things like that. Okay.
[00:23:55.360 --> 00:23:59.360]   I remember you one year walking around with what looks like a Ghostbusters backpack.
[00:23:59.360 --> 00:24:02.640]   That's exactly it. And I had you had a camera on a staff.
[00:24:02.640 --> 00:24:09.600]   Yeah, eight different things. I was like the Dunesbury character with the transmitter on my
[00:24:09.600 --> 00:24:16.320]   back. Yeah. That was fun. Nice. Well, you know, I've only been to South by two or three times and
[00:24:16.320 --> 00:24:20.960]   I was told before I went the first time by Robert Scoble. He said, well, it's really the main
[00:24:20.960 --> 00:24:28.080]   point of South by and this take take consider the source is the parties. And so I was prepared
[00:24:28.080 --> 00:24:33.200]   to go and cover the parties. And that was the remote unit. Well, no, that's and that's true. I mean,
[00:24:33.200 --> 00:24:38.640]   consider your source here. I'm a I'm a giant huge nerd who's actually like
[00:24:38.640 --> 00:24:45.440]   in to tech. So I'm like, Oh, I want to go to crazy talks by physicist. Right. No, but I'm with you.
[00:24:45.440 --> 00:24:53.040]   That sounds really cool. That I would do. So the only one to one panel. It was Mark Cuban's
[00:24:53.040 --> 00:24:59.040]   panel a few years ago. He started talking and he came off the biggest jerk ever.
[00:24:59.040 --> 00:25:06.160]   And then the fire alarm went off, cleared the room. And Lisa and I looked at each other and we
[00:25:06.160 --> 00:25:11.120]   said, yeah, let's not go back. So that's it. That's my only South by Southwest seminar experience.
[00:25:11.120 --> 00:25:17.600]   Let's take a break. Come back one more. We're going to talk about Google. More Google. We got
[00:25:17.600 --> 00:25:23.280]   all sorts of good Google stuff, including Google's new futuristic headquarters. They get the go ahead.
[00:25:23.280 --> 00:25:32.640]   Re-capture just got invisible invisible recapture. And are you ready? Are you going to buy the new
[00:25:32.640 --> 00:25:37.520]   Google digital whiteboard? What do they call it? The snap jam.
[00:25:37.520 --> 00:25:42.160]   Sneaky back. What are you jam board? What?
[00:25:42.160 --> 00:25:44.960]   Staling it back.
[00:25:44.960 --> 00:25:52.960]   Five, a $5,000 TV from Google. Before we do that, though, we were talking before the show about
[00:25:52.960 --> 00:25:56.160]   mesh. We were and Stacy was schooling me on how mesh works.
[00:25:56.160 --> 00:26:02.160]   And we were talking about the king of the mesh. This is the company that brought mesh to the
[00:26:02.160 --> 00:26:10.080]   consumer space and is transforming Wi-Fi. So many people nowadays are using three or four,
[00:26:10.080 --> 00:26:16.800]   five-year-old Wi-Fi routers that aren't getting the job done. Don't get a Wi-Fi router. Get a
[00:26:16.800 --> 00:26:25.360]   super simple hyper-fast, brilliantly designed Wi-Fi system. Get the Eero. It's easy to set up. You
[00:26:25.360 --> 00:26:30.400]   can set up just a few minutes. You download the Eero app and they walk you through every step of
[00:26:30.400 --> 00:26:35.520]   the process. The units are attractive. You don't mind having that in your living room. There's a
[00:26:35.520 --> 00:26:40.400]   piece of tape on there. You'll take that off. In fact, you can even control the little blue lights
[00:26:40.400 --> 00:26:46.720]   on this. And you can even say, "Hey, Echo, dim the light on my Eero." But my favorite Echo command
[00:26:46.720 --> 00:26:52.000]   for the Eero. When you set up the Eero, you see all the devices that are attached to your network.
[00:26:52.000 --> 00:26:58.720]   You can assign ownership to each of the devices. So I went through our 14-year-olds laptop and
[00:26:58.720 --> 00:27:05.440]   Chromebook and iPad and phone and game machine and Xbox and PlayStation. And I named them all
[00:27:05.440 --> 00:27:12.560]   Michael's stuff. And now I could say, "Hey, Echo, pause Michael's internet." And we, Lisa and I,
[00:27:12.560 --> 00:27:20.480]   continue to surf with impunity. But Michael goes, "Hey!" And we go, "Go to bed!" By the way, he cannot
[00:27:20.480 --> 00:27:25.840]   turn it back on again with the Echo. You have to use the very, the app, which is even better.
[00:27:25.840 --> 00:27:30.480]   It is amazing. The Eero app lets you manage your network from the palm of your hand.
[00:27:30.480 --> 00:27:35.840]   I set up my mom with Eros because she was having trouble because she has a studio that's out in
[00:27:35.840 --> 00:27:40.240]   the backyard. And it was hard for her. She said, "I can't go to the studio because I can't use my
[00:27:40.240 --> 00:27:45.600]   Echo out there. I can't use my iPad out there." So when I was out a couple of months ago, I set her
[00:27:45.600 --> 00:27:50.640]   up, put the Eros out there. And now I can launch the Eero app and see how her network is doing.
[00:27:50.640 --> 00:27:55.520]   I can manage it from here. The other thing I think is really important to mention is the
[00:27:55.520 --> 00:28:03.840]   Eero is overbuilt. It comes with more processor, more memory than it really needs. And that is
[00:28:03.840 --> 00:28:11.520]   so it can be upgraded over time. Features, performance can be improved with new software.
[00:28:11.520 --> 00:28:16.880]   When they roll that mesh 2.0, they got a whole, it's like getting a whole new router. Of course,
[00:28:16.880 --> 00:28:21.600]   WPA2 encryption, guest networks, all the features you want, even some advanced features. I have a
[00:28:21.600 --> 00:28:26.560]   server on my Eero network and I was able to set up port reservation. So it always had the same port
[00:28:26.560 --> 00:28:32.240]   and port forwarding really nicely done. The Eero 30-day money back guarantee and because it's their
[00:28:32.240 --> 00:28:39.120]   first birthday, they're celebrating with permanently lowered prices. If you've been waiting, you're
[00:28:39.120 --> 00:28:47.760]   in luck. 3 pack of Eros, $399, $299 for the 2 pack. So it's $100 off the 3 pack, $50 off the 2 pack.
[00:28:47.760 --> 00:28:52.400]   It is a great deal. You can also buy individual Eros and expand your network as needed.
[00:28:52.400 --> 00:29:00.880]   E-E-R-O dot com or Amazon or Best Buy, Eero, no more buffering, no more dead zones. Finally,
[00:29:00.880 --> 00:29:08.800]   Wi-Fi, that works. Thank you, Eero. Eero is a hero to people all over the country.
[00:29:08.800 --> 00:29:14.560]   They were just, I hear from people all the time. I never was able to get my Wi-Fi working.
[00:29:16.480 --> 00:29:25.760]   Thank you, Eero. There was an 8K VR ride at Southbud. Did you do that? Stacey? I want to do that.
[00:29:25.760 --> 00:29:28.160]   No, I did not. I did not see the 8K VR ride.
[00:29:28.160 --> 00:29:34.400]   Well, it's not too late. It's not too late. Here's a Devindra Hardawar did a piece on it for
[00:29:34.400 --> 00:29:40.640]   Engadget. There's Devindra. I guess it's a bare bones thing because you're not wearing a
[00:29:40.640 --> 00:29:45.440]   headset. They've got a circular 8K screen and you're sitting on...
[00:29:45.440 --> 00:29:48.640]   Oh, is this in the trade show? Yeah, it looks like it's on the show floor.
[00:29:48.640 --> 00:29:54.320]   You're sitting on a servo-controlled hydraulic seat, which looks like it's made from... I hate
[00:29:54.320 --> 00:29:59.760]   to say it, but maybe this is a prototype, but don't those look like plastic lawn chairs?
[00:29:59.760 --> 00:30:06.080]   You know, you might... I mean, I feel like... Yeah, those are... I'm sorry. I might want to work
[00:30:06.080 --> 00:30:09.440]   on those a little bit. I hope there's a seat belt. I think there is.
[00:30:12.400 --> 00:30:15.440]   Is this like those rides at Disney? Yeah, I guess it is. I guess it is.
[00:30:15.440 --> 00:30:17.520]   Or a amusement park where they stick you... Yeah.
[00:30:17.520 --> 00:30:22.160]   I mean, that's just what it looks like. Universal has a number of great VR rides.
[00:30:22.160 --> 00:30:27.040]   You must... Danny, you must have been the universe. I love Harry Potter, baby.
[00:30:27.040 --> 00:30:31.120]   That's Disney, though. We hit Universal last year. You know, Harry Potter is universal.
[00:30:31.120 --> 00:30:36.560]   Is it? Yeah. Oh, it's huge. Yeah. You get to fly. You know what? I have to say, and that's the one
[00:30:36.560 --> 00:30:41.680]   where you put visors on, right? Except that no, no, there are 3D glasses because you're still
[00:30:41.680 --> 00:30:46.400]   screens. It's still projection screens. But you're in... You told me about it, Stacy,
[00:30:46.400 --> 00:30:51.120]   and then we did it. You're in a chair that does move. Yeah.
[00:30:51.120 --> 00:30:54.880]   Does move as a lot of these 3D rides. Yeah, it's kind of like crazy pattern.
[00:30:54.880 --> 00:30:58.720]   But it also tilts you and you really feel like you're flying. I love that.
[00:30:58.720 --> 00:31:02.960]   I really... That was one of my favorite virtual reality rides ever.
[00:31:02.960 --> 00:31:07.440]   They also have a good Transformers ride. That's a little bit older, but you're right.
[00:31:07.440 --> 00:31:11.600]   Harry Potter, that's great. Maybe it's because I had three or four butter beers before I got
[00:31:11.600 --> 00:31:18.640]   on it. Those things are disgusting. They're not... The non-alcoholic. It's really disgusting.
[00:31:18.640 --> 00:31:22.240]   Do not do the butter beer. They were so gross. My husband loved them.
[00:31:22.240 --> 00:31:26.560]   They're very sweet. He must have a sweet tooth. Yes.
[00:31:26.560 --> 00:31:30.160]   So last week, we were talking a little bit about Google Cloud Next.
[00:31:30.160 --> 00:31:36.560]   10,000 attendees. Google says 100 announcements, three days of keynotes.
[00:31:37.840 --> 00:31:43.680]   Holy cow. This is the first time they've done a Google Cloud show.
[00:31:43.680 --> 00:31:49.040]   They've done cut downs. They have the day one keynote in four minutes,
[00:31:49.040 --> 00:31:53.040]   the day two keynote in five minutes if you want to just whiz through it. But here's some of the
[00:31:53.040 --> 00:31:59.520]   announcements. We mentioned this last week, they acquired Kaggle, which is a big data company.
[00:31:59.520 --> 00:32:05.040]   Appbridge. Competition company. Sorry. It's competition?
[00:32:07.360 --> 00:32:09.440]   They run big data competitions. That's right.
[00:32:09.440 --> 00:32:13.760]   Competitions. A community of data scientists and machine learning enthusiasts.
[00:32:13.760 --> 00:32:23.200]   Appbridge, which is some sort of enterprise migrate data from on-prem file servers into G Suite
[00:32:23.200 --> 00:32:30.400]   and Google Drive thing. Identity-aware proxy for Google Cloud platform. Data loss prevention.
[00:32:30.400 --> 00:32:36.320]   Key manager. I go on and on and on and on. This is amazing. I wonder if this is now going to
[00:32:36.320 --> 00:32:43.600]   be as big as Google I/O. It would make sense. They've really got to amp their clouds.
[00:32:43.600 --> 00:32:49.440]   Everything. Yeah. I mean, and the focus on the enterprise is going to be so important for them.
[00:32:49.440 --> 00:32:55.840]   It was interesting for me because I hadn't been to the cloud thing before, and I haven't really
[00:32:55.840 --> 00:33:04.240]   followed Google's cloud efforts that closely. It felt like this coming out party. The keynotes
[00:33:04.240 --> 00:33:10.880]   were fairly. Then I think the real highlight was at the end of the first one when Eric Schmidt came
[00:33:10.880 --> 00:33:17.920]   out. He actually set this vision of, look, we have spent, I think he said, it was either 35
[00:33:17.920 --> 00:33:23.280]   billion or 3 billion or 5 billion. We've spent billions building this cloud platform. You could
[00:33:23.280 --> 00:33:27.920]   go build the cloud platform if you want and spend the same billions or you can build on top of it.
[00:33:29.040 --> 00:33:33.840]   For me, it was really this crystallization of Google saying, look, we have a cloud OS.
[00:33:33.840 --> 00:33:39.760]   You can use our cloud OS and do awesome things. If you're trying to figure out whether or not
[00:33:39.760 --> 00:33:44.800]   you should be using the Amazon cloud OS or the Microsoft cloud OS, ours comes with awesome
[00:33:44.800 --> 00:33:51.920]   machine learning stuff. What do you got to think about? I was struggling getting a handle on the
[00:33:51.920 --> 00:33:56.320]   audience because most of the stuff was pitched like we're trying to convince enterprises they
[00:33:56.320 --> 00:34:01.520]   should build on it. Yet the audience seemed to be made up of your typical Google developers who
[00:34:01.520 --> 00:34:08.000]   are building stuff rather than the end clients, which I'm sure were there. When I would talk to
[00:34:08.000 --> 00:34:11.200]   people, they were mainly developers who are like, yeah, I'm building stuff on the Google cloud.
[00:34:11.200 --> 00:34:16.400]   I've got clients who want this sort of stuff or whatever. It was really interesting. I think at
[00:34:16.400 --> 00:34:22.400]   1.2, they said that Drive was coming up at 800 million users or something. It would be their next
[00:34:22.400 --> 00:34:29.840]   1 billion users. I need 100 million users. Is that page users?
[00:34:29.840 --> 00:34:33.600]   I don't think they said page users. That must include paid and free.
[00:34:33.600 --> 00:34:38.880]   Well, it doesn't everybody says that for Gmail get a cloud drive account.
[00:34:38.880 --> 00:34:43.280]   You get Drive. I don't know. Yeah, but I know that everybody has to use it.
[00:34:43.280 --> 00:34:50.640]   They have said a Gmail itself is a billion user type of things. I guess the drive is catching up.
[00:34:51.360 --> 00:34:54.960]   That's not one of the interesting things. I haven't played with it more with the whole
[00:34:54.960 --> 00:35:02.080]   Drive streaming thing. I run into that problem where I've got 500 gigabytes of data on my
[00:35:02.080 --> 00:35:06.560]   main laptop that's synced to the cloud. Then I go traveling on my MacBook that it can only
[00:35:06.560 --> 00:35:11.440]   hold 250 gigabytes. I can't sync everything from the cloud. Now it's, I guess, one of the
[00:35:11.440 --> 00:35:16.720]   features is you can see all your files. If you want one, you stream the files on demand.
[00:35:17.440 --> 00:35:21.360]   There's no more feeling that you had to have them all downloaded, which is kind of nice.
[00:35:21.360 --> 00:35:25.360]   It's interesting what you say about the disconnect between the people who were there and the people
[00:35:25.360 --> 00:35:29.120]   they thought would be there. Because it sounds like this is more like a sales
[00:35:29.120 --> 00:35:37.840]   dream force kind of Salesforce style enterprisey pitch. Instead, they got the Google IO crowd there.
[00:35:37.840 --> 00:35:43.200]   I mean, it's hard for me to tell because there were thousands of people there and I talked to
[00:35:43.200 --> 00:35:49.520]   a handful of people. It might have been I was only catching developers. But the pitch really was,
[00:35:49.520 --> 00:35:55.600]   especially on that first day. You had this guy from HSBC come out and like, "Yeah, we're HSBC."
[00:35:55.600 --> 00:35:59.760]   The low point of the whole keynote. I hate it when they do those things.
[00:35:59.760 --> 00:36:01.440]   Still, but saying we're doing it, or you had...
[00:36:01.440 --> 00:36:08.080]   Well, yeah, they said, "Cold day Disney eBay, HSBC, Lush, Odin, Planet, Slumberger, the Home Depot,
[00:36:08.080 --> 00:36:12.480]   Verizon." And like Verizon came out and they're like, "We've moved to..." And then I'm also trying to
[00:36:12.480 --> 00:36:17.200]   process and catch up with the terminology because Verizon is like, "We've moved to G Suite or whatever."
[00:36:17.200 --> 00:36:23.840]   And you realize, "Oh, you're saying that you guys are using Gmail now for all your employees."
[00:36:23.840 --> 00:36:27.760]   Right. I was excited about the Verizon thing and then...
[00:36:27.760 --> 00:36:31.120]   They're not using the office suite.
[00:36:31.120 --> 00:36:35.280]   They're using Gmail rather than Microsoft Office. I get it. But it was interesting because they
[00:36:35.280 --> 00:36:39.840]   talked about how they're trying to get early adopters and people going and convincing they
[00:36:39.840 --> 00:36:44.560]   want to shift over. And I was thinking, "Are you really having that much trouble getting people in
[00:36:44.560 --> 00:36:50.160]   your organization who already use Gmail for their personal work to say, "I want to use it at work?"
[00:36:50.160 --> 00:36:57.280]   It's like, we had to... Then certain people would convince people that the G Suite mail was great.
[00:36:57.280 --> 00:37:02.640]   And it's like, "Yeah, I think they knew it was great." They're still working off of... I don't know.
[00:37:02.640 --> 00:37:07.040]   There's a real head-to-head, though, between Microsoft, Amazon, and Google. Those are the big
[00:37:07.040 --> 00:37:09.760]   three, right? And I guess maybe... Yeah. Well, then...
[00:37:09.760 --> 00:37:16.880]   Go ahead, Stacey. Google has had no luck or less luck getting enterprises even to look
[00:37:16.880 --> 00:37:22.400]   at their cloud offering. Like, Microsoft's kind of one. And so with Diane Greene coming in,
[00:37:22.400 --> 00:37:28.800]   this, to me, feels like them creating something that, yes, can meet the developer needs, but is
[00:37:28.800 --> 00:37:37.440]   trying to prove that they have the support, the services, what you need as a company.
[00:37:37.440 --> 00:37:42.480]   Greene founded VMware. She did the keynote, right? She was keynote number one was Diane Greene.
[00:37:42.480 --> 00:37:48.480]   So it's an interesting... Except... To bring in the founder and CEO of VMware to Google
[00:37:48.480 --> 00:37:53.280]   and then put her up on stage. Yeah, that's... The message is very clear. We understand...
[00:37:53.280 --> 00:37:55.520]   But they bought her company for a lot of money. Right.
[00:37:55.520 --> 00:38:01.200]   Right. They bought Diane Greene's enterprise cloud company for a ton of money just to get Diane Greene.
[00:38:01.200 --> 00:38:04.640]   Right. Although I don't think I made her happy if the press conference happened.
[00:38:04.640 --> 00:38:08.880]   Like, you ask. Well, they had the press conference afterward and they're like,
[00:38:08.880 --> 00:38:12.240]   "Well, Google Cloud, it's awesome. Maybe you did it on a hike set." So,
[00:38:12.240 --> 00:38:17.920]   Google kind of has this history of a company that disrupts and puts other companies out of business.
[00:38:17.920 --> 00:38:22.000]   So now you're telling these companies they should build stuff on their platform,
[00:38:22.000 --> 00:38:26.480]   but how do you reassure them that this is a great thing to do because you're not going to put
[00:38:26.480 --> 00:38:29.920]   them out of business? You're like, "We don't put people out of Google doesn't do that." I'm like,
[00:38:29.920 --> 00:38:38.480]   "I think you do, but okay." So their message takeaway was, "No, I guess you can totally trust
[00:38:38.480 --> 00:38:42.640]   us where Google and it would be fine." But that was really kind of the challenge to me.
[00:38:42.640 --> 00:38:47.680]   I mean, it was interesting, like, for example, that Snap has built so much of their business
[00:38:47.680 --> 00:38:54.080]   on Google's platform when Google would love to be Snap, right? And would love to have that
[00:38:54.080 --> 00:38:57.520]   audience and have that. That's a really interesting point because Microsoft,
[00:38:57.520 --> 00:39:03.680]   probably people feel much safer trusting Microsoft with their cloud stuff because Microsoft's
[00:39:03.680 --> 00:39:08.640]   probably not going to be a competitor. And perhaps even more so, I would say with Amazon,
[00:39:08.640 --> 00:39:16.080]   although not that Amazon doesn't have tentacles that go out. Well, Amazon pushes a lot of features
[00:39:16.080 --> 00:39:22.000]   to their cloud that kill their partners. I mean, every year at AWS invent, everyone goes and they're
[00:39:22.000 --> 00:39:28.240]   like, "Are we going to be the company that becomes a feature of AWS and gets knocked out?"
[00:39:28.240 --> 00:39:35.120]   That's kind of the nature of ecosystems. You're hoping you could be a little fish and survive with
[00:39:35.120 --> 00:39:39.360]   the big fish around. It's like being a remora-e. I think more interesting.
[00:39:41.200 --> 00:39:47.760]   Go ahead. But for me, it's a... It's Stacy. Go on, go on, Danny. No, I'm going to be
[00:39:47.760 --> 00:39:56.240]   the arbiter. Stacy. There's a delay on the... I could tell you got a delay and that's hurting a
[00:39:56.240 --> 00:40:01.440]   little bit because, yeah, you're a second behind us. No, it's not your fault. We'll fix it.
[00:40:01.440 --> 00:40:08.560]   Oh, but now I forgot what I was going to say. Oh, I remember the issue with Google is,
[00:40:09.360 --> 00:40:15.840]   I think one of longevity for features and platform things that you like because Google is not shy
[00:40:15.840 --> 00:40:22.240]   about saying, "Eh, this sucks." They'll cut anything. Well, just look at their message,
[00:40:22.240 --> 00:40:26.480]   the massive messaging that they're making and of course they announced meat at Next, which is,
[00:40:26.480 --> 00:40:34.400]   "What is meat?" It's their slack. Oh, I was so sad about that. Why? What is it? So,
[00:40:35.440 --> 00:40:40.640]   it's Hangouts Plus. I don't understand. It's... It's... It's... It's... It's... It's... It's... It's...
[00:40:40.640 --> 00:40:44.240]   It's yet another messaging app because you need it. But this one is for teams.
[00:40:44.240 --> 00:40:49.680]   But... And... And slack has text, but meat is really about video. So, it's... It's the Hangouts
[00:40:49.680 --> 00:40:57.280]   video bit. But I think you get text as well. Oh, okay. And... And do you get remote access too? Is it like,
[00:40:57.280 --> 00:41:02.080]   is it like a... Is it like a go-to-meeting kind of thing where you have video meetings and you share
[00:41:02.080 --> 00:41:07.920]   screens and that kind of thing or...? I guess I haven't explored it. Dialing... They've got
[00:41:07.920 --> 00:41:13.120]   dial and phone numbers integrated with G Suite connected to meeting rooms.
[00:41:13.120 --> 00:41:20.480]   And you're right, Stacey, that there's always this fear that, "Well, this is... You know,
[00:41:20.480 --> 00:41:29.440]   if you were an enterprise who had relied on Hangouts, you might be very nervous. What's going on with
[00:41:29.440 --> 00:41:33.280]   Hangouts?" You know? And then for a while, Hangouts was on the outs and now it seems like they're
[00:41:33.280 --> 00:41:39.120]   adding features to Hangouts. And... I don't know. I... But... For me... It's clear that Google is...
[00:41:39.120 --> 00:41:46.960]   God, Danny. We... We need like in the expanse where they're... When they're talking to the
[00:41:46.960 --> 00:41:51.680]   moon, it always says in the corner, "One second delay or talking to Mars." Yeah. Yeah. Pretend you're
[00:41:51.680 --> 00:41:57.520]   talking to Mars. But I... I was going to say, for me, for someone who doesn't watch the cloud stuff
[00:41:57.520 --> 00:42:05.200]   that closely, my big takeaway was really like, "Wow, it's not..." The cloud has always felt so nebulous.
[00:42:05.200 --> 00:42:09.760]   And to the degree that I've thought of it, it's like you store your pictures up there, right? Or
[00:42:09.760 --> 00:42:14.480]   other things. I know, but that's... You know, that's been my main interaction with it. And I really kind
[00:42:14.480 --> 00:42:21.280]   of came away with thinking, "Wow, you know, these aren't clouds. These are OSSes. This is Google OS
[00:42:21.680 --> 00:42:28.480]   versus Amazon OS versus Microsoft OS. And they've got the... As your name, the Microsoft AWS,
[00:42:28.480 --> 00:42:31.600]   but these are... These are software as a service. These are software as a service.
[00:42:31.600 --> 00:42:36.960]   Systems that run in the cloud rather than on your desktop that you're developing,
[00:42:36.960 --> 00:42:43.040]   or... And it's just going to be this fascinating thing to watch how people choose which of those
[00:42:43.040 --> 00:42:47.520]   operating systems they want to build their businesses on. And you know, which is their eyes going forward.
[00:42:47.520 --> 00:42:54.000]   And I think some of the resistance... And I'm not an enterprise expert, but I do watch... I do play
[00:42:54.000 --> 00:42:59.680]   one on TV. I watch people in enterprises, and I actually own an enterprise. I think enterprises,
[00:42:59.680 --> 00:43:05.360]   which at first were reluctant to put their business processes up there, are getting much,
[00:43:05.360 --> 00:43:10.400]   much more comfortable with it. And there's a lot of advantages to it. And then some of it goes down
[00:43:10.400 --> 00:43:14.160]   to, "Well, we've always been a Microsoft shop, so we're going to give our business to Microsoft.
[00:43:14.160 --> 00:43:19.360]   Google and Amazon have to fight a little bit harder to get that business." But there are so many advantages.
[00:43:19.360 --> 00:43:24.640]   For a long time, for instance, we use QuickBooks accounting locally as an app.
[00:43:24.640 --> 00:43:30.880]   But it makes much more sense to use it in the cloud as a software as a service instead of...
[00:43:30.880 --> 00:43:37.680]   But that's the other thing to point out, Danny, is you don't have to go all... It is an operating
[00:43:37.680 --> 00:43:42.720]   system, but you can piecemeal it. You can treat it as, "Well, I'm going to use QuickBooks and Google
[00:43:42.720 --> 00:43:49.520]   Mail, and maybe I'll use Microsoft Office." You can piecemeal it from all three providers. You
[00:43:49.520 --> 00:43:53.680]   don't have to go with one provider. Do companies do that? We do.
[00:43:53.680 --> 00:44:01.840]   No, no, no, no. Well, if you're talking about running different suites and buying
[00:44:01.840 --> 00:44:06.400]   software as a service from different providers, sure. But if you're talking about architecting a
[00:44:06.400 --> 00:44:12.640]   business process or some back-end system in the cloud, you pick one because you're going to
[00:44:12.640 --> 00:44:21.120]   optimize for costs. The way you architect your system will depend on the platform you are,
[00:44:21.120 --> 00:44:26.720]   because maybe in this gets into things like databases and who charges what for what. But
[00:44:26.720 --> 00:44:34.240]   this is not something that you just redo overnight. It's actually a big investment.
[00:44:34.240 --> 00:44:37.840]   So that's where I'm not familiar with how big enterprises do it. Because, for instance,
[00:44:38.800 --> 00:44:46.080]   we're a company with about 25 employees and a bunch of contractors. We use Gmail, Google Docs,
[00:44:46.080 --> 00:44:49.840]   but some people are using QuickBooks, some people are using Office.
[00:44:49.840 --> 00:44:57.600]   It's a real hodgepodge. We use HipChat. But Microsoft, with its teams, is trying to
[00:44:57.600 --> 00:45:02.640]   win that Slack HipChat business. But I think there are a lot of enterprises that use Slack.
[00:45:02.640 --> 00:45:10.320]   It's a unicorn, standalone app that you just integrate into whatever your other cloud is.
[00:45:10.320 --> 00:45:15.120]   But I think it's the difference between whether or not you're using cloud applications or using
[00:45:15.120 --> 00:45:21.200]   cloud to develop your platform on. So that's right. Stacey said work. Probably not going to
[00:45:21.200 --> 00:45:25.280]   mix and match. But just because your business app runs, say on Google Cloud, doesn't mean that
[00:45:25.280 --> 00:45:30.560]   your business might not also use Slack. Our business app runs on Azure. Oh, and Drupal.
[00:45:32.160 --> 00:45:39.680]   So our production workflow runs on Drupal servers on a company called BlackMesh.
[00:45:39.680 --> 00:45:46.480]   That's our cloud for production. So I mean, we're a mess. And the reason is,
[00:45:46.480 --> 00:45:52.560]   I think like a lot of companies, you did it all piecemeal. So we use HipChat for messaging.
[00:45:52.560 --> 00:45:56.240]   Yeah, sure. We could go to Microsoft's teams because we're using Microsoft Office or we could
[00:45:56.240 --> 00:46:00.800]   use Hangouts because we're using Google. But we're already in Slack. And I bet you a lot of
[00:46:00.800 --> 00:46:04.400]   companies that did a piecemeal. Sure, if you're going to re-architect your business process,
[00:46:04.400 --> 00:46:09.680]   you're going to say, go to Microsoft and say, let's build it all on Microsoft. But most people
[00:46:09.680 --> 00:46:17.120]   do it bit by bit, right? Well, large companies will do these IT consolidation efforts.
[00:46:17.120 --> 00:46:22.960]   Right. And these are like two here. Yeah. Millions of dollars, millions of millions.
[00:46:22.960 --> 00:46:29.200]   Yeah. So it depends on what you're talking about. And some of them, I was at
[00:46:30.080 --> 00:46:37.120]   when I was at time, Inc, we used Slack. And then we also kept adding Trello and random other things.
[00:46:37.120 --> 00:46:39.760]   Yeah. See? Bit piecemeal, bit by bit. Yeah.
[00:46:39.760 --> 00:46:46.560]   But then as I was leaving, they wanted to consolidate a bit. And suddenly, you weren't allowed to use
[00:46:46.560 --> 00:46:51.680]   certain apps anymore. See, to me, every time they do that, there's this productivity hit.
[00:46:51.680 --> 00:46:59.600]   Yeah. But there's also, you're not paying for license fees. You're assuming that you can get it.
[00:46:59.600 --> 00:47:04.880]   You can regain the productivity hit somewhere else. I don't know.
[00:47:04.880 --> 00:47:09.600]   I know nothing about enterprise. I hated the enterprise IT world. Me too too.
[00:47:09.600 --> 00:47:16.720]   Go away. I eschewed that. Let's talk about capture. Everybody hates capture.
[00:47:16.720 --> 00:47:22.320]   Nobody likes capture. Those are those. If you're a robot, go ahead and try to figure out this
[00:47:22.320 --> 00:47:26.240]   number. The problem is robots probably do a better job than humans. And if there do be a
[00:47:26.240 --> 00:47:33.120]   security value effect, we've seen things like people will create an adult website that has a
[00:47:33.120 --> 00:47:39.040]   capture to sign in, except it's not really their capture. It's the capture on some banking site,
[00:47:39.040 --> 00:47:43.600]   and they're just getting humans to solve it for them. And that's the whole purpose for the adult
[00:47:43.600 --> 00:47:50.320]   website, things like that. Google bought a Carnegie Mellon tool called ReCAPTCHA. I remember ReCAPTCHA
[00:47:50.320 --> 00:47:57.440]   was a great idea because you got two words, one of which helped with book scanning,
[00:47:57.440 --> 00:48:02.640]   and one of them was actual capture. But even that's so annoying. So now they've said,
[00:48:02.640 --> 00:48:09.040]   we don't really need capture. Well, guess what? Google knows a lot about you. And they know so
[00:48:09.040 --> 00:48:14.800]   much about you that they could pretty much tell if you're a human or a bot just from how you use
[00:48:14.800 --> 00:48:20.000]   the computer, from your cookies, from your history. And so if you're trustworthy, you won't even see
[00:48:20.880 --> 00:48:26.560]   a capture. And if they have any questions, that's where you get that little, you know, for a while,
[00:48:26.560 --> 00:48:30.400]   they were just doing that little box that says, I'm not a robot, you check it, and that was easy.
[00:48:30.400 --> 00:48:37.600]   Now they don't even put that up anymore. It's sad, though. This is great. It's a loss. The loss
[00:48:37.600 --> 00:48:42.800]   of capture is sad because I remember Louise went on who I think is the author of ReCAPTCHA talking
[00:48:42.800 --> 00:48:47.840]   about how great it was that people who wanted to watch porn were helping better improve the
[00:48:47.840 --> 00:48:52.320]   digitization of classical books. They were getting something, right? They were taking all these words
[00:48:52.320 --> 00:48:56.960]   out of books that they couldn't identify. You could watch your porn, you helped figure out what
[00:48:56.960 --> 00:49:03.920]   this burden of Shakespeare was correctly, correctly scanned. So, you know, I mean, it's great. It's
[00:49:03.920 --> 00:49:08.720]   great that we'll save a little bit of time. Great. I guess you can get directly to your porn, but,
[00:49:08.720 --> 00:49:13.440]   you know, I feel sorry for the books. Or this, it was they were also using it apparently for street
[00:49:13.440 --> 00:49:18.160]   view according to Ron Amadio in Ars Technica. I remember you would see, sometimes you'd see
[00:49:18.160 --> 00:49:22.960]   numbers like numbers on a house. Yeah. And that's what you were helping solve that.
[00:49:22.960 --> 00:49:28.880]   That's kind of clever. Google bought ReCAPTCHA in 2009 for that exact purpose
[00:49:28.880 --> 00:49:34.240]   to help them with scanning books and street view. And you know, so the money that Louise
[00:49:34.240 --> 00:49:40.960]   Fanon made for that went to him creating Duolingo. Right on, right on, right on. Language learning.
[00:49:40.960 --> 00:49:47.120]   So, you know, it's, I think it's great because it's bringing contextual analysis into the real
[00:49:47.120 --> 00:49:52.080]   world in a way that people are going to not be creeped out by. So I'm like, do it.
[00:49:52.080 --> 00:49:57.760]   Well, and it doesn't, I mean, he's this guy's smart because doesn't Duolingo, it's free because
[00:49:57.760 --> 00:50:01.520]   they're due because you're doing eventually translations in a translation. Translations.
[00:50:01.520 --> 00:50:08.800]   Love that's to me. That's thinking. That's using your noggin. Anyway, because
[00:50:08.800 --> 00:50:14.080]   it's mechanical turking. Mechanical turking. That's exactly what it is. And if you don't know what that is,
[00:50:14.080 --> 00:50:22.080]   you're listening to the wrong show. I'm surprised how many people don't know about the
[00:50:22.080 --> 00:50:26.320]   mechanical Turk, actually. I tell people about that all the time. Okay, it's Amazon service where
[00:50:26.320 --> 00:50:30.640]   you can pay to get lots of people to do things for you. You really can't ease you pay pennies.
[00:50:30.640 --> 00:50:36.240]   Yeah. And they get a lot of cloud powered by humans. Right. Right. The mechanical Turk.
[00:50:36.240 --> 00:50:40.080]   And it's named after. I'm picturing all these little people on bicycles, power,
[00:50:40.080 --> 00:50:43.680]   and like the. It's like black. It's like black. Oh, no, there's like a black.
[00:50:43.680 --> 00:50:46.720]   Black mirror. It's black mirror. Absolutely.
[00:50:46.720 --> 00:50:52.720]   So it is. I got to continue searching for these results and clicking on the ones that
[00:50:52.720 --> 00:50:57.840]   match. Okay. Okay. Oh, I've done my day. Now I can buy an hour of free ads.
[00:50:57.840 --> 00:51:03.440]   It is telling though, how much Google knows about you that they don't really need to capture
[00:51:03.440 --> 00:51:07.360]   it and know whether you're you. Yeah. Just come on in. We know. Hi, Leo.
[00:51:07.360 --> 00:51:14.960]   You're cool. Come on in. They can even they do, you know, it's it kind of goes back to that
[00:51:14.960 --> 00:51:21.280]   super cookie story where they, you know, each, each system is very uniquely identifiable based
[00:51:21.280 --> 00:51:26.240]   on the plugins you've installed on the screen resolution and how you move your mouse on this
[00:51:26.240 --> 00:51:31.280]   huge number of factors that they can uniquely identify you. So they don't really need capture.
[00:51:32.480 --> 00:51:40.720]   They got something better knowledge. Google is going to break ground on new headquarters.
[00:51:40.720 --> 00:51:46.880]   I always felt like these, these ridiculous headquarters like Apple's spaceship campus were
[00:51:46.880 --> 00:51:52.160]   were really, they always used to say, I think was in the Wall Street Journal, when a company builds a
[00:51:52.160 --> 00:51:58.800]   massive corporate headquarters, that's really the company that's peak and it's all downhill from there.
[00:52:00.560 --> 00:52:06.400]   But why the Armadillo design? This is in Mountain View.
[00:52:06.400 --> 00:52:11.680]   He's a spaceship. Didn't they, didn't they originally have this kind of indoor outdoor
[00:52:11.680 --> 00:52:14.240]   complex that they were going to build? What happened to that?
[00:52:14.240 --> 00:52:19.920]   Well, you have to understand that our Googlers are very precious and the more that we can put
[00:52:19.920 --> 00:52:23.440]   them into a contained, hermetically sealed environment, the less likely they're going to
[00:52:23.440 --> 00:52:28.640]   mean to go out and be outside world where they might be exposed to have to do chores on their own
[00:52:28.640 --> 00:52:33.520]   and not have everything provided for them, which reduces our work productivity.
[00:52:33.520 --> 00:52:37.120]   This is actually not nearly as grandiose as Apple's campus, which I think
[00:52:37.120 --> 00:52:43.360]   houses 20,000 employees. This is a tenth that size. 595,000 square feet, two stories tall.
[00:52:43.360 --> 00:52:46.160]   It won't be done for three years, two or three years.
[00:52:46.160 --> 00:52:54.160]   It's actually not very far away from Apple Park. It's kind of down the road from Apple
[00:52:54.160 --> 00:52:58.240]   Park if you want to come visit. Yeah, this does look like an Armadillo, doesn't it?
[00:52:58.240 --> 00:53:02.000]   I want to see some new nickname. Let's all call it that.
[00:53:02.000 --> 00:53:05.520]   I see some drawings for the answer. Armadillo. Armadillo. Armadillo.
[00:53:05.520 --> 00:53:11.760]   It's Charleston East. Here's another fine story. We're having trouble getting
[00:53:11.760 --> 00:53:15.840]   Congress to investigate the Russian connection to the election, but no problem,
[00:53:15.840 --> 00:53:24.320]   Justice Department's glad to indict four Russians for hacking. Yahoo! Two of them worked for the FSB
[00:53:25.520 --> 00:53:30.000]   and I think some Canadians were involved. The Justice Department said today they indicted four
[00:53:30.000 --> 00:53:39.440]   hackers responsible for a breach of half a billion user records at Yahoo. Two were Russian spies
[00:53:39.440 --> 00:53:48.160]   from the FSB. Two were "hired criminals." They were looking for a dirt on politicians,
[00:53:48.160 --> 00:53:56.320]   so it might actually be related to the DNC hacks. The spies wanted dirt on politicians.
[00:53:56.320 --> 00:54:01.440]   The hackers were scavenging for profits, all four charged with wire fraud, trade
[00:54:01.440 --> 00:54:10.640]   theft and economic espionage. Karim Badatov was one of the hackers. He was in Canada. He was
[00:54:10.640 --> 00:54:16.080]   arrested yesterday. The other three are in Russia, which means it might be a little tricky to get them
[00:54:17.440 --> 00:54:24.880]   out of here. What was the final discount Verizon got for the hacking? Nothing. It was a tiny amount
[00:54:24.880 --> 00:54:29.360]   compared to the... I thought it was like three to twenty-five. Three hundred twenty-five millions.
[00:54:29.360 --> 00:54:34.720]   That's not nothing. Yes, sorry, not dollars. Three hundred twenty-five dollars.
[00:54:34.720 --> 00:54:37.280]   There's a coup. And a coupon for Denny's.
[00:54:37.280 --> 00:54:44.960]   But it's a multi-billion dollar purchase, so I guess it's better than nothing. We were
[00:54:44.960 --> 00:54:50.960]   talking initially about maybe they'd get a billion off. I think they asked for nine hundred and twenty-five.
[00:54:50.960 --> 00:54:58.000]   Anyway, the deal continues to go through. And Marissa Meyer, apparently not going to get the
[00:54:58.000 --> 00:55:03.760]   gold in handcuffs or the gold and parachute that we thought the rumors last year were like $53 million.
[00:55:03.760 --> 00:55:12.400]   I think she's only going to get twenty-three million dollars.
[00:55:12.400 --> 00:55:15.680]   Twenty-three. But here's the thing that should be... Let me correct all...
[00:55:15.680 --> 00:55:22.240]   I was just going to... Verizon saw a nine hundred and twenty-five million dollar discount,
[00:55:22.240 --> 00:55:24.880]   and it got a three hundred and forty million dollar discount. That's what you said.
[00:55:24.880 --> 00:55:30.800]   I think that's exactly what you said, but I may be wrong. We could check the tape later.
[00:55:30.800 --> 00:55:38.000]   Here's something though that will get you a little mad. The new Yahoo CEO will make double
[00:55:38.000 --> 00:55:44.080]   Marissa Meyer's salary. Oh my god, it's like a comedy of errors.
[00:55:44.080 --> 00:55:50.800]   Yeah, because maybe that was what went wrong. We just didn't pay the CEO enough to save Yahoo.
[00:55:50.800 --> 00:55:59.280]   I feel confused at what the CEO is going to be running. I've lost track of what parts of...
[00:55:59.280 --> 00:56:03.200]   Baba? No, this is not Alt-Baba. That's something else.
[00:56:04.400 --> 00:56:09.520]   That's not the part that Verizon got. That's what's left of Yahoo, and it really consists entirely
[00:56:09.520 --> 00:56:15.520]   of their stake in Alibaba. And maybe Yahoo Japan. I think a little Yahoo Japan stock sewed in there too.
[00:56:15.520 --> 00:56:20.560]   So let's see what the paycheck is. A twenty-three million dollar severance package,
[00:56:20.560 --> 00:56:24.720]   sixty-nine million dollars worth of unexercise stock options,
[00:56:24.720 --> 00:56:29.520]   ninety-seven million dollars of existing stock, which she can't sell till she leaves.
[00:56:30.240 --> 00:56:38.400]   So a total of one hundred and eighty-nine million dollars. So when they took twenty million for the
[00:56:38.400 --> 00:56:45.280]   cyber attacks as a punishment, not a big deal. Not a big deal. One hundred eighty-nine million
[00:56:45.280 --> 00:56:56.720]   dollars. She'll be replaced by a former chief of IAC, Thomas McInerney, IAC owns dating sites
[00:56:56.720 --> 00:57:01.680]   like Match.com. He's going to get a starting base salary of two million dollars to become Yahoo's CEO.
[00:57:01.680 --> 00:57:09.360]   That's double what Marat and Mercer Meyer was getting. And he'll also get four million dollars bonus
[00:57:09.360 --> 00:57:11.680]   in his first year. Jeebus.
[00:57:11.680 --> 00:57:20.880]   That's twenty-five percent. I'm in the wrong line of work. I know. But and you know, exactly,
[00:57:20.880 --> 00:57:27.840]   what does he run? He runs OMG.com and TV.com and Flickr and Yahoo mail and
[00:57:27.840 --> 00:57:33.120]   anyway. Katie Korik's show. Katie the Katie Korik show.
[00:57:33.120 --> 00:57:39.440]   Katie Korik and David Pogue, who both I think, aren't they like million dollar salaries?
[00:57:39.440 --> 00:57:42.240]   Are they really? Just refresh my memory again. So
[00:57:42.240 --> 00:57:48.800]   all to Baba has nothing to do with the all of Baba's stake that is actually going to be part of
[00:57:48.800 --> 00:57:52.560]   the Yahoo that Verizon isn't buying. No, no, that's Altaba is.
[00:57:52.560 --> 00:57:58.320]   Oh, I'm confused. No, Altaba is the holding company.
[00:57:58.320 --> 00:58:04.960]   But that's what McInery is going to head up. Oh, he's holding up Altaba. Oh, I'm now I am confused.
[00:58:04.960 --> 00:58:09.360]   You've got me because she has to step down from. So Verizon doesn't need a CEO.
[00:58:09.360 --> 00:58:15.040]   Well, he's not doing anything. What is he doing? He's just hanging out.
[00:58:17.360 --> 00:58:23.680]   Altaba. Altaba's was left after Verizon acquires. There we go. Here we go. The bones.
[00:58:23.680 --> 00:58:31.600]   Okay. This is a new verb. I've never heard the verb concrete's. Yahoo to be renamed Altaba
[00:58:31.600 --> 00:58:38.000]   after Verizon purchase concrete's. Is that a word? You guys are writers. No, that's
[00:58:38.000 --> 00:58:45.840]   headline jargon right there. That's like, oh, I think I thought that all Bob according to Washington
[00:58:45.840 --> 00:58:51.120]   Post is a combination of the words. Now wait for it. Alternative and Alibaba.
[00:58:51.120 --> 00:58:56.800]   Yeah, it's left. It's Alt Alibaba. Alt Alibaba. Yep.
[00:58:56.800 --> 00:59:06.000]   So this guy isn't the head of what's left of Yahoo. He's the head of Alt Baba. That's even more
[00:59:06.000 --> 00:59:15.440]   ridiculous of a salary. Yeah. So she's going to step down. So they're buying Yahoo, which we know
[00:59:15.920 --> 00:59:23.840]   and love as that thing we used to know and love. And that'll go to Verizon. And Marissa's not going
[00:59:23.840 --> 00:59:30.080]   to go with that. But because she is still technically the head of that right now, she is going to get
[00:59:30.080 --> 00:59:35.760]   paid a severance because when the deal is done, they're going to be like, thanks for handling Yahoo
[00:59:35.760 --> 00:59:42.960]   over all those years. Here's the $23 million on your set. But meanwhile, because she's still
[00:59:42.960 --> 00:59:49.040]   going to be head of the new Alt Baba company, she would have stayed there. But apparently,
[00:59:49.040 --> 00:59:55.120]   she's going to be a cop. She's saying, oh, we don't need you anymore either. So you can step down
[00:59:55.120 --> 00:59:58.880]   and we're going to put this Thomas guy in there and we're going to pay him twice of what we're paying
[00:59:58.880 --> 01:00:04.080]   you because he's a man. But actually, that makes sense because Alt Baba really is just an
[01:00:04.080 --> 01:00:09.360]   investment company at this point. And so he'll be running as he did at IAC, a bunch of disparate
[01:00:09.360 --> 01:00:16.240]   brands. So he has, he was a CFO at IAC. So he has some experience in running a holding company.
[01:00:16.240 --> 01:00:24.560]   All right. Not that one of the things that I always thought, oh, Yahoo should have done was
[01:00:24.560 --> 01:00:30.640]   instead of pursue a product strategy, pursue a financial kind of strategy that didn't happen.
[01:00:30.640 --> 01:00:35.040]   So maybe that's what's going to happen with the remainder of it. I don't know.
[01:00:35.040 --> 01:00:38.000]   We're going to take a little break and then we're going to come back with the one thing
[01:00:38.000 --> 01:00:40.080]   everybody who has an iPhone wants.
[01:00:40.080 --> 01:00:43.200]   What's that? An entry phone? Yeah.
[01:00:43.200 --> 01:00:47.120]   Sign of laugh. But first,
[01:00:47.120 --> 01:00:53.120]   oh, they're all going, what are you talking about? You're crazy, Leo. You're crazy.
[01:00:53.120 --> 01:01:00.000]   Well, not crazy like a fox. Hang on. But first, a word from Curry, the real life robot pal,
[01:01:00.000 --> 01:01:07.280]   your first robot. Your kids are going to love Curry. I love Curry. Curly.
[01:01:07.280 --> 01:01:13.280]   Curry is so cute. She's knee high. She has expressive, really expressive eyes. You really look at her
[01:01:13.280 --> 01:01:18.560]   and she immediately, she has a personality. You feel like you, you know, there's something in there.
[01:01:18.560 --> 01:01:22.800]   She responds to human touch. She has face recognition. She'll know who's who.
[01:01:22.800 --> 01:01:26.080]   She maps your house and knows where things are. And you could say, this is the living room,
[01:01:26.080 --> 01:01:30.160]   Curry. This is the den. And then you could say, hey, go into the living room and see if the
[01:01:30.160 --> 01:01:34.960]   dogs in the couch, it'll take a picture, send it back to your phone. She knows how to avoid
[01:01:34.960 --> 01:01:40.320]   obstacles like stairs and furniture, but she also can route around new obstacles because she's got
[01:01:40.320 --> 01:01:48.480]   LiDAR and recognition. She can totally navigate the place. She doesn't get tripped up on carpets
[01:01:48.480 --> 01:01:52.880]   or thresholds or door jams. She can walk around. She got a little tank, tread feet. When she's low
[01:01:52.880 --> 01:01:58.160]   on battery, she just goes home to her dock and charges. She greets you differently when you come
[01:01:58.160 --> 01:02:03.760]   home and she really greets you. She does not speak. This is really important. They decided not to make
[01:02:03.760 --> 01:02:08.800]   her too anthropomorphic. They wanted to be cute cuddly and they didn't want her to speak. So she
[01:02:08.800 --> 01:02:15.040]   beeps and boops and burps, but you can talk to her and you can't ask her to do things. For instance,
[01:02:15.040 --> 01:02:19.440]   you can say, wake me up in the morning and play my favorite podcast this week at Google.
[01:02:19.440 --> 01:02:24.080]   Curry will wake you up, start playing your podcast. You can say, Curry, follow me. And while you go
[01:02:24.080 --> 01:02:27.360]   to the kitchen and make breakfast or brush your teeth, she'll follow you around the house.
[01:02:28.720 --> 01:02:35.280]   She'll read the kids a bedtime story. And when you're not home, she can check on things. In fact,
[01:02:35.280 --> 01:02:40.240]   if there's a loud noise, she'll go investigate and take photos or video and send it right to your
[01:02:40.240 --> 01:02:45.360]   phone. Curry, she's so cute. I spent a little time with her down there at the Mayfield Robotics
[01:02:45.360 --> 01:02:50.720]   place that I kind of fell in love. So I immediately put down a hundred dollar refundable deposit.
[01:02:50.720 --> 01:02:56.320]   Curry is made by Mayfield Robotics, which is a division of Bosch. So they have a big manufacturing
[01:02:56.320 --> 01:03:01.360]   arm in Germany. She will be beautifully made. They're working on finalizing the software now.
[01:03:01.360 --> 01:03:07.200]   They expect to have Curry in your house by this fall. If you want to get in line, put down the
[01:03:07.200 --> 01:03:13.920]   hundred dollar refundable deposit. Curry will be 699. So 599 plus tax and shipping due at
[01:03:13.920 --> 01:03:21.840]   time of purchase. Curry, K-U-R-Y. Find out more, read up on Curry. Some of the smartest roboticists
[01:03:22.480 --> 01:03:30.000]   are working on this. And I really think they got something here, a real live robot pal. H-E-Y-K-U-R-I.
[01:03:30.000 --> 01:03:38.720]   Hey, Curry.com. And I think I'm torn because they said we're going to get one of the first
[01:03:38.720 --> 01:03:42.640]   off the line because I put my deposit down early. And I don't know. I think I'm going to have her in
[01:03:42.640 --> 01:03:48.000]   the studio coming and saying hi during shows and stuff like that. I'd like to have her at the house.
[01:03:48.000 --> 01:03:55.040]   You can send her to my house. You can have a visit from Curry. We could share Curry.
[01:03:55.040 --> 01:04:00.560]   You have a 7 year old, right? I think that would be really perfect for Curry.
[01:04:00.560 --> 01:04:05.280]   You know why I want Curry? Because when our dog passed away, after their dog passed away,
[01:04:05.280 --> 01:04:09.600]   the nice thing about a dog cats don't do this. We have cats too. But cats don't do this. Dogs
[01:04:09.600 --> 01:04:16.320]   kind of add a person to your house. You know, like your dog is like, and when Aussie passed away,
[01:04:16.960 --> 01:04:21.760]   it got really quiet. And I felt like we need, we need to have somebody, we need a presence in the
[01:04:21.760 --> 01:04:26.320]   house. And then shortly after I met Curry, I thought, you know what, a lot easier than a dog.
[01:04:26.320 --> 01:04:31.040]   But with much of the same kind of personality and she's happy to see you and stuff, I thought,
[01:04:31.040 --> 01:04:34.880]   she can't play fetch. So if that's important, you should get a dog.
[01:04:34.880 --> 01:04:39.520]   Neither can my dog. Yeah, I know. Aussie didn't. He would get it and run away.
[01:04:40.800 --> 01:04:48.240]   So I don't know. This is kind of a silly idea, but it's called the eye. It's a Kickstarter
[01:04:48.240 --> 01:04:55.920]   for an iPhone case that's an Android phone. What do you think? I don't understand this at all.
[01:04:55.920 --> 01:05:03.120]   I really think I have exactly what? What? What?
[01:05:03.120 --> 01:05:09.120]   iPhone case for an Android phone. On the back is the Android phone. On the front is the iPhone.
[01:05:09.840 --> 01:05:15.440]   Oh, I could use that. Exactly. It's both. I could use that because I always carry around two
[01:05:15.440 --> 01:05:20.240]   phones. Right. It's not anymore because it's a pain. So I just carry one. I actually carry a pixel
[01:05:20.240 --> 01:05:25.920]   and an iPhone. Oh, sorry. My phone is literally ringing out. Is that ironic? I'd have scrawling
[01:05:25.920 --> 01:05:30.160]   you now. Is that a real phone or is that your cell phone making a real phone? No, it's my cell
[01:05:30.160 --> 01:05:35.680]   phone making the noise. I like that. It's a retro. Well, now you can have your Android phone make
[01:05:35.680 --> 01:05:43.600]   one ring and your iPhone make the other ring. That could be handy. Although I think it's the way.
[01:05:43.600 --> 01:05:50.160]   I don't know. But you know, there is a market for this. They were trying to raise money on Kickstarter.
[01:05:50.160 --> 01:05:57.520]   They wanted $95,000. They raised $235,000. In just a few days, they have 30 days left.
[01:05:57.520 --> 01:06:05.360]   So there's clearly a market for this. It's not super expensive. They plan to get it out at $189.
[01:06:06.240 --> 01:06:12.800]   So the case holds both your phones. It's a phone case. No, I think. Well, that's a good question.
[01:06:12.800 --> 01:06:18.320]   Oh, maybe it's just a case that holds two phones. But which Android phone does it hold?
[01:06:18.320 --> 01:06:24.160]   No, it's actually an Android op. I thought it was a separate phone. It's an Android phone that
[01:06:24.160 --> 01:06:30.160]   is a case for your iPhone. It's got a five inch AMOLED display. Oh, you comes with an Android
[01:06:30.160 --> 01:06:38.400]   phone. Yes, for $95. It's got 28 milliamps of power. Always on display two SIM slots, NFC.
[01:06:38.400 --> 01:06:48.000]   Unlike the I case. The I E Y E. If you kick starter, I the smart iPhone case, you'll find it.
[01:06:48.000 --> 01:06:55.600]   Five inch display, 256. You could add an SD card for 256 gigs of storage. It has an IR blaster.
[01:06:56.160 --> 01:06:58.880]   So you can use it for remote control. Isn't that a crazy idea?
[01:06:58.880 --> 01:07:04.720]   Can you take pictures with this thing? I don't know. You're asking tough questions. Actually,
[01:07:04.720 --> 01:07:11.040]   I think this would get in the way of your camera. That's what I would think. I mean, I'm okay with
[01:07:11.040 --> 01:07:18.080]   that because. Oh, you could do selfies all the time. All selfies all the time.
[01:07:24.400 --> 01:07:29.120]   No, I've gone off it now. I don't need it. You've gone. You're done. I don't need it. You're done.
[01:07:29.120 --> 01:07:34.560]   It's not getting the point. It's one of those strange, but it's weird. They've raised so much
[01:07:34.560 --> 01:07:42.720]   money. Obviously, people want to do something like that. HTC is saying they will have an
[01:07:42.720 --> 01:07:51.040]   unexpected surprise on March 20th. Spring is coming. It's funny because I just got my HTC
[01:07:51.600 --> 01:07:55.840]   U-Ultra phone. That was the phone I thought I was getting last week, but actually came.
[01:07:55.840 --> 01:08:04.320]   I kind of like it. You know what I really like? I really wish that it's not a pixel. The pixel is
[01:08:04.320 --> 01:08:10.320]   the best Android phone. Although, I got the beautiful blue and they have it. That's nice.
[01:08:10.320 --> 01:08:14.320]   Yeah, it's pretty. That's a nice Samsung. Yeah. It's not a Samsung.
[01:08:14.960 --> 01:08:23.680]   Well, it looks like it on the back. Knock it off. It has a 16, no, 12 megapixel camera
[01:08:23.680 --> 01:08:30.400]   and a 12 megapixel selfie camera. Actually, the camera has the Zoes, which I always like to have
[01:08:30.400 --> 01:08:39.600]   HTC. This is actually kind of fun. I can do a pano selfie. Let me go to the selfie pan around.
[01:08:39.600 --> 01:08:45.120]   Oh, that sounds cool. So, I take a picture myself, right? And then I pan it over.
[01:08:45.120 --> 01:08:52.240]   And then I pan it over the other side. And I get an extra wide selfie. Whoops.
[01:08:52.240 --> 01:08:58.400]   Oh, you can't do that. Oh, that's kind of cool. That changes the selfie game.
[01:08:58.400 --> 01:09:03.120]   It changes the selfie game. If you had been at the Oscars, you could have fit every.
[01:09:03.120 --> 01:09:07.200]   Everybody could be fit in, right? That's what I was thinking.
[01:09:07.200 --> 01:09:11.040]   It changes your selfie game. I really like it. It's a very good camera.
[01:09:11.040 --> 01:09:14.320]   You would like this with the iPhone and that's two. Yeah, no, no, no.
[01:09:14.320 --> 01:09:22.320]   What else? It's not pure Android. It's very HTC-sensy with the Blink feed and all that stuff,
[01:09:22.320 --> 01:09:27.840]   which, you know, I think the HTC Sensei. I wonder if you already got Google Assistant on it too.
[01:09:27.840 --> 01:09:31.440]   That's nice. You get Google Assistant on it. That's one of the phones that Google has now
[01:09:31.440 --> 01:09:37.920]   upgraded assistant, which is good. It is 7.0, not 7.1.1. And it only has the January security update that
[01:09:37.920 --> 01:09:44.320]   sometimes. You know what really happens since January? That's probably good enough security.
[01:09:44.320 --> 01:09:48.000]   You're now you're being facetious, aren't you?
[01:09:48.000 --> 01:09:57.600]   That kind of bugs me. I really feel like that's kind of the minimum on an Android phone is you
[01:09:57.600 --> 01:10:01.680]   ought to be getting the monthly updates in a timely fashion. Yeah. And that's one of the
[01:10:01.680 --> 01:10:05.520]   reasons the Pixel is a winner. But the Pixel is kind of uninspiring design. It's nice,
[01:10:05.520 --> 01:10:10.960]   but it doesn't get yet excited. This is a little more exciting, don't you think? A little prettier,
[01:10:10.960 --> 01:10:14.320]   a little more. That's so pretty. It's pretty. 5.6.
[01:10:14.320 --> 01:10:20.080]   You know what? I'm just going to say, I don't need the design. I don't care about the design
[01:10:20.080 --> 01:10:25.760]   because they're all going into the case. What I need, just if I can rant, I just need a decent
[01:10:25.760 --> 01:10:33.360]   credit card case, right? Right. That works for the Galaxy. That works for the iPhone. You know what
[01:10:33.360 --> 01:10:37.040]   you get for the Pixel? What? You got to try to find something that's basically glued on.
[01:10:37.040 --> 01:10:42.160]   Yeah, you get the glue on patch. Yeah, the glue on patch. If you want the Pixel to be successful,
[01:10:42.160 --> 01:10:47.280]   you build me a decent credit card case because I can't go back to having a wallet and I'm just
[01:10:47.280 --> 01:10:51.680]   done. Okay. I think that's kind of a special. I don't care about the design. I don't need to
[01:10:51.680 --> 01:10:56.880]   design. Look at how pretty who walks around with their phone, not in the case.
[01:10:56.880 --> 01:11:03.920]   Me? Oh, my husband doesn't. I don't like prices. Actually, a lot of my former colleagues don't.
[01:11:03.920 --> 01:11:12.000]   It's his phone. Too dangerous. Now, when I hold this and drop it, I've never dropped a phone.
[01:11:12.000 --> 01:11:20.480]   He's very careful. Yeah. Oh my God. My phone, like, if my phone doesn't have a case, it's too slippery.
[01:11:21.600 --> 01:11:25.280]   This also, one other thing, this has a little screen at the top. It's a dual screen thing.
[01:11:25.280 --> 01:11:29.440]   This is not a good idea. It has a little screen up here that can show you notifications.
[01:11:29.440 --> 01:11:35.360]   You know, I have call buttons. I could play my music. It's just a little extra screen. I could
[01:11:35.360 --> 01:11:40.160]   see the weather. I could have little icons. The problem is it kind of gets in the way when you're
[01:11:40.160 --> 01:11:44.000]   doing notifications. I mean, you still slide over it, but it just feels weird that it's there.
[01:11:44.000 --> 01:11:49.520]   Nevertheless, I mean, that kind of like it. And it's part of the always on. It's the always on screen.
[01:11:49.520 --> 01:11:56.000]   So, well, I didn't do it. But if I raise the phone, let's turn it off and raise it. Yeah,
[01:11:56.000 --> 01:11:59.920]   then you can kind of see you can't see it very well, but that's the always on screen, too.
[01:11:59.920 --> 01:12:02.880]   Well, that's nice for at night, too. It's a clock. Yeah, it's a little clock.
[01:12:02.880 --> 01:12:06.240]   That's what I like about the S7 Edge. You can run the clock along the edge.
[01:12:06.240 --> 01:12:12.560]   Right. I never thought that that was that useful. So this is like the edge, but it's on the top,
[01:12:12.560 --> 01:12:16.400]   basically. It's a little bit. Yeah. No, the only thing I ever use the edge for is for that clock.
[01:12:16.400 --> 01:12:22.800]   And I think it is well worth buying an $800 phone so I can turn it to a clock on the rare occasions
[01:12:22.800 --> 01:12:29.760]   I'm traveling. And I am unable to use my Android watch as my clock, which is the only thing I use
[01:12:29.760 --> 01:12:38.160]   that for on my nightstand now. So I do wish technology. Oh, what? Oh, yes. I have a question.
[01:12:38.160 --> 01:12:45.520]   Yes. About clocks. Yes. I actually, someone asked me, they want a smart alarm clock,
[01:12:45.520 --> 01:12:48.240]   and they asked if I had done any reviews about it. And I thought,
[01:12:48.240 --> 01:12:53.680]   "Ha, I have a smartphone and an echo, so I never thought about needing something."
[01:12:53.680 --> 01:12:55.680]   So my echo is my small alarm clock. You guys know of any...
[01:12:55.680 --> 01:13:01.760]   Yeah. So, but... What does that mean? What does that mean? What does that mean? What does that mean?
[01:13:01.760 --> 01:13:05.600]   What does it mean? You have a phone. What do you need? What do you need? What does a smart alarm clock mean?
[01:13:05.600 --> 01:13:09.440]   I don't know what it means. They want something. So this is what I ask. They want something
[01:13:09.440 --> 01:13:13.680]   where in the middle of the night, they can open their eyes and see the time. And then in the morning,
[01:13:13.680 --> 01:13:18.320]   it wakes them with an alarm and the news and whatever traffic feed or whatever.
[01:13:18.320 --> 01:13:22.320]   That would be a great task for it. That would be a great thing.
[01:13:22.320 --> 01:13:26.400]   It's called an alarm clock. That's called an alarm radio. That would be great. Actually,
[01:13:26.400 --> 01:13:32.880]   if the Amazon dot had a... Because I like that when I wake up at night, I want to know the time.
[01:13:32.880 --> 01:13:37.200]   And I've got my Android watch that's there and I could see it and that's perfect. But an Amazon
[01:13:37.200 --> 01:13:41.920]   dot that had the time built into it would be wonderful. That's... That's...
[01:13:41.920 --> 01:13:44.560]   Okay. So maybe a case that goes over the Amazon dot.
[01:13:44.560 --> 01:13:47.840]   Well... Someone, get on Kickstarter. We have...
[01:13:47.840 --> 01:13:52.960]   Or you could probably buy like a $29, like a $10 LED thing and just glue it to the end of your
[01:13:52.960 --> 01:13:56.560]   Amazon dot. We have the Betty. And then it would give you the time.
[01:13:56.560 --> 01:14:02.400]   We have the Betty here. And this was an Indiegogo. And Megan has one in the other room. We showed
[01:14:02.400 --> 01:14:10.080]   it today on iOS today. It's an app-enabled alarm clock speaker that wakes you up with a music and
[01:14:10.080 --> 01:14:14.240]   a light. It does a slow glow wake up thing where the light gets brighter and brighter.
[01:14:14.240 --> 01:14:24.400]   You can dim the clock to nothing or you can have a very... It has an app-controlled alarm clock and
[01:14:24.400 --> 01:14:28.560]   auto time sync, multiple USB charging ports on the back, which is very handy.
[01:14:28.560 --> 01:14:34.880]   This... I thought it was less than this. This is 100 bucks, but I guess that's what it was.
[01:14:37.120 --> 01:14:40.800]   You can... It's got a whole slot for your phone. It does not dock.
[01:14:40.800 --> 01:14:44.400]   It just sits in the slot because no one knows what the next...
[01:14:44.400 --> 01:14:45.520]   Does it charge the phone?
[01:14:45.520 --> 01:14:49.760]   No. Well, yeah, because there's a USB jack in the back. So you could put a cable...
[01:14:49.760 --> 01:14:51.680]   Not USB-C?
[01:14:51.680 --> 01:14:55.360]   It's whatever you want it to be. It's a tight... It's the old USB.
[01:14:55.360 --> 01:14:56.240]   So you...
[01:14:56.240 --> 01:14:57.280]   Then you run your own cable.
[01:14:57.280 --> 01:15:00.320]   Yeah, you run your own cable. See how she's got that plugged in.
[01:15:00.320 --> 01:15:04.880]   And I don't blame them because how many times you go to a hotel, like the fine Driscoll Hotel in
[01:15:04.880 --> 01:15:08.960]   Austin, Texas, and they've got an i-home with a 30-pin dock on it.
[01:15:08.960 --> 01:15:13.520]   Like it's like they don't want to buy another clock radio until somebody figures out what the
[01:15:13.520 --> 01:15:14.960]   next standard's gonna do.
[01:15:14.960 --> 01:15:18.800]   I keep... and I keep shoving my phone on there and just those pins will not go into the lightning
[01:15:18.800 --> 01:15:19.280]   connector.
[01:15:19.280 --> 01:15:25.280]   And so when I'm supposed to carry around a 30-pin, the lightning adapter before I go to the hotel,
[01:15:25.280 --> 01:15:29.200]   every hotel I've been in ever has a 30-pin clock radio.
[01:15:29.200 --> 01:15:34.160]   And I'm sure when they put that in, they said this is the height of modern technology.
[01:15:34.160 --> 01:15:37.840]   That's Apple just being Apple.
[01:15:37.840 --> 01:15:39.120]   It's just Apple being Apple.
[01:15:39.120 --> 01:15:40.480]   So this is...
[01:15:40.480 --> 01:15:42.880]   I would tell her get the Betty.
[01:15:42.880 --> 01:15:44.800]   Okay.
[01:15:44.800 --> 01:15:45.280]   From Whitty.
[01:15:45.280 --> 01:15:45.760]   Is it him?
[01:15:45.760 --> 01:15:46.240]   But okay.
[01:15:46.240 --> 01:15:47.840]   Tell him to get a Betty from Whitty.
[01:15:47.840 --> 01:15:49.520]   Is it shipping now though?
[01:15:49.520 --> 01:15:52.720]   Because I've been burnt on that Lily drone and I don't want to buy anything else that's
[01:15:52.720 --> 01:15:52.960]   promised to me.
[01:15:52.960 --> 01:15:53.920]   I believe it is.
[01:15:53.920 --> 01:15:55.520]   This was last December.
[01:15:55.520 --> 01:15:59.040]   It got funded December 9th of two years ago, 2015.
[01:15:59.040 --> 01:16:00.000]   So...
[01:16:00.000 --> 01:16:01.360]   She yet has a review on it.
[01:16:01.360 --> 01:16:02.160]   Yeah, we have one.
[01:16:02.160 --> 01:16:03.040]   So...
[01:16:03.680 --> 01:16:05.680]   Oh yeah, you can actually buy it on Amazon.
[01:16:05.680 --> 01:16:06.480]   Yeah, you can buy it on Amazon.
[01:16:06.480 --> 01:16:06.480]   Amazon.
[01:16:06.480 --> 01:16:07.280]   From Whitty Design.
[01:16:07.280 --> 01:16:07.280]   Amazon.
[01:16:07.280 --> 01:16:07.760]   From Whitty Design.
[01:16:07.760 --> 01:16:13.440]   Although I gotta feel like in about a month it's going to be outdated when Amazon Echo
[01:16:13.440 --> 01:16:16.160]   Clock charged dock port.
[01:16:16.160 --> 01:16:20.480]   I love that idea of adding just a little clock to it.
[01:16:20.480 --> 01:16:22.320]   It's easy.
[01:16:22.320 --> 01:16:22.880]   You should do that.
[01:16:22.880 --> 01:16:23.280]   You could see...
[01:16:23.280 --> 01:16:24.400]   Our refrigerator has a clock.
[01:16:24.400 --> 01:16:26.400]   Why can't my smart thing have a clock?
[01:16:26.400 --> 01:16:28.320]   I put my Apple watch.
[01:16:28.320 --> 01:16:31.920]   I have a little tiny Mac classic Apple watch holder.
[01:16:31.920 --> 01:16:33.360]   It looks like a little tiny Macintosh.
[01:16:33.360 --> 01:16:36.880]   Put the watch in there and it looks like the screen on the Macintosh.
[01:16:36.880 --> 01:16:38.560]   You have to hit it.
[01:16:38.560 --> 01:16:41.280]   Oh yeah, no, but that's great.
[01:16:41.280 --> 01:16:44.080]   Because with the Android watch, Leo, with my...
[01:16:44.080 --> 01:16:46.160]   Because I have both of them there and with the Android watch,
[01:16:46.160 --> 01:16:49.440]   it's so annoying because you wake up in the middle of the night and you want to know what
[01:16:49.440 --> 01:16:53.760]   time it is and it's just there and a dimmed mode always telling you what the time is.
[01:16:53.760 --> 01:16:55.440]   And how annoying is that?
[01:16:55.440 --> 01:16:58.880]   To where I want to know the time with my Android watch, I can wake up,
[01:16:58.880 --> 01:17:02.960]   I can hit my desk and then the light...
[01:17:02.960 --> 01:17:06.080]   Then it comes on and tells you the time, which is such a great feature.
[01:17:06.080 --> 01:17:09.680]   It's that's the kind of usability that you like from Apple.
[01:17:09.680 --> 01:17:10.320]   It just works.
[01:17:10.320 --> 01:17:16.320]   Apple does not make this, but it is my favorite little gadget on my bedside table.
[01:17:16.320 --> 01:17:17.280]   It turns your Apple watch.
[01:17:17.280 --> 01:17:20.720]   They should make a version of that that constantly vibrates so that it would keep...
[01:17:20.720 --> 01:17:23.360]   Remind you it's there.
[01:17:23.360 --> 01:17:26.320]   $13 it's from Elago, E-L-A-G-O.
[01:17:26.320 --> 01:17:27.120]   It's on Amazon.
[01:17:27.120 --> 01:17:28.640]   Elago W-3 stand.
[01:17:28.640 --> 01:17:33.680]   I happen to love this and it's nicely made and the watch charger goes...
[01:17:33.680 --> 01:17:41.360]   You put your watch charger puck in it and it works so it charges and everything.
[01:17:41.360 --> 01:17:42.000]   It's very nice.
[01:17:42.000 --> 01:17:42.480]   It's cute.
[01:17:42.480 --> 01:17:42.720]   Yeah.
[01:17:42.720 --> 01:17:44.720]   It goes into nightstand mode.
[01:17:44.720 --> 01:17:47.680]   That's actually my favorite smart clock radio.
[01:17:47.680 --> 01:17:53.840]   Anyway, I'm glad I could tell you about the witty because we did this on iOS today on Monday
[01:17:53.840 --> 01:17:55.280]   and I thought, "Who would want that?"
[01:17:55.280 --> 01:17:56.560]   But now I know Stacy's friend.
[01:17:57.520 --> 01:17:58.560]   There you go.
[01:17:58.560 --> 01:17:59.440]   Thank you.
[01:17:59.440 --> 01:18:07.600]   Mark Zuckerberg announces the creation of a female user.
[01:18:07.600 --> 01:18:12.240]   I was going to say, "Is this his baby?"
[01:18:12.240 --> 01:18:12.480]   Yes.
[01:18:12.480 --> 01:18:17.920]   Thank you, Gizmodo, for the weirdest headline of the week.
[01:18:17.920 --> 01:18:20.880]   So there's a new zuck on the way.
[01:18:20.880 --> 01:18:25.440]   Max, their one-year-old daughter will have a little baby girl.
[01:18:25.440 --> 01:18:26.160]   Yep.
[01:18:26.160 --> 01:18:31.920]   Mark posted on Facebook where else, after our difficult experience having Max,
[01:18:31.920 --> 01:18:33.680]   we weren't sure what to expect or whether we'd be able to...
[01:18:33.680 --> 01:18:35.680]   I didn't know they had a difficult experience.
[01:18:35.680 --> 01:18:37.520]   Or whether we'd be able to have another child.
[01:18:37.520 --> 01:18:41.120]   When Priscilla and I first found out she was pregnant again,
[01:18:41.120 --> 01:18:42.640]   our first hope was the child would be healthy.
[01:18:42.640 --> 01:18:44.800]   My next hope was it would be a girl.
[01:18:44.800 --> 01:18:49.600]   I always like the pictures that are posted.
[01:18:49.600 --> 01:18:50.640]   They're so natural.
[01:18:50.640 --> 01:18:50.960]   Yeah.
[01:18:50.960 --> 01:18:51.520]   Yeah.
[01:18:52.880 --> 01:18:55.200]   With this strange thread-locked dog.
[01:18:55.200 --> 01:18:57.760]   He's got that arm like seven people walking around taking pictures of them.
[01:18:57.760 --> 01:19:01.600]   Yeah, you could tell Mark and Priscilla are always just sitting around like this
[01:19:01.600 --> 01:19:02.800]   with the dreadlock dog.
[01:19:02.800 --> 01:19:05.280]   And who's far as we know...
[01:19:05.280 --> 01:19:08.080]   At the time, the camera went off just at the right moment.
[01:19:08.080 --> 01:19:08.640]   Just at the right moment.
[01:19:08.640 --> 01:19:11.440]   Yeah, you know, there's like 15 lights.
[01:19:11.440 --> 01:19:14.880]   There's a stylist, there's a makeup artist, there's a PR person, there's a photographer.
[01:19:14.880 --> 01:19:18.080]   The testing is they had a whole selection of photos to choose from.
[01:19:18.080 --> 01:19:20.560]   And they said, "Let's go for the one where his eyes are closed,
[01:19:20.560 --> 01:19:22.560]   because that'll make it feel natural."
[01:19:22.560 --> 01:19:23.680]   That's I think you're right.
[01:19:23.680 --> 01:19:26.800]   Priscilla, it's your turn to have your eyes closed this time.
[01:19:26.800 --> 01:19:28.640]   I'll do it next time.
[01:19:28.640 --> 01:19:34.480]   They had difficulty in that they had a miscarriage or a miscarriage.
[01:19:34.480 --> 01:19:34.880]   Oh, I didn't know that.
[01:19:34.880 --> 01:19:38.560]   And they were very open about that, which was a big deal for...
[01:19:38.560 --> 01:19:39.600]   It was a very big deal.
[01:19:39.600 --> 01:19:44.160]   It was a part of like erasing the shame of having miscarriages.
[01:19:44.160 --> 01:19:46.720]   Well, I'm glad that they're expecting again.
[01:19:46.720 --> 01:19:48.560]   And...
[01:19:51.680 --> 01:19:54.000]   That was shared March 9th.
[01:19:54.000 --> 01:19:55.760]   And you know, I don't know how I missed that until now.
[01:19:55.760 --> 01:20:01.440]   I Facebook mustn't be giving me the latest news from my friends.
[01:20:01.440 --> 01:20:02.960]   If it's not politics, you don't see it.
[01:20:02.960 --> 01:20:04.320]   You know, it's really true.
[01:20:04.320 --> 01:20:05.600]   I did not see that post.
[01:20:05.600 --> 01:20:08.000]   There was no Trump in it.
[01:20:08.000 --> 01:20:10.480]   I could share something on Facebook,
[01:20:10.480 --> 01:20:11.760]   and be like, "Is this huge?
[01:20:11.760 --> 01:20:13.280]   Shazhi Google is like two likes."
[01:20:13.280 --> 01:20:15.200]   Like, "Look at what he's done now!"
[01:20:15.200 --> 01:20:16.640]   Yeah, boom.
[01:20:16.640 --> 01:20:19.920]   It's like that Monty Python scam kit.
[01:20:20.880 --> 01:20:23.440]   It's not got much Trump in it.
[01:20:23.440 --> 01:20:24.640]   It's got a little bit of Trump.
[01:20:24.640 --> 01:20:28.800]   Right here.
[01:20:28.800 --> 01:20:34.880]   By the way, you'll be glad no Facebook has banned the use of user data for surveillance.
[01:20:34.880 --> 01:20:37.600]   Yes.
[01:20:37.600 --> 01:20:46.400]   Now, my big question is, how do they just change that?
[01:20:46.400 --> 01:20:47.280]   I mean, they'll fill it.
[01:20:47.280 --> 01:20:49.760]   Political campaigns target you, but you know...
[01:20:49.760 --> 01:20:49.920]   Yes.
[01:20:50.640 --> 01:20:52.480]   [laughter]
[01:20:52.480 --> 01:20:54.240]   Well, yeah, because so I didn't know this,
[01:20:54.240 --> 01:20:58.800]   but it makes sense like with the pipeline, the Dakota pipeline protests,
[01:20:58.800 --> 01:21:01.280]   they set...
[01:21:01.280 --> 01:21:03.120]   Actually, I don't think this actually happened because...
[01:21:03.120 --> 01:21:05.680]   Remember, there was a whole thing.
[01:21:05.680 --> 01:21:09.040]   Go check in at the Dakota pipeline protest,
[01:21:09.040 --> 01:21:12.480]   even if you're here in San Francisco or in Austin,
[01:21:12.480 --> 01:21:17.760]   because we want to confuse the police who are using people's check-ins to target them?
[01:21:18.480 --> 01:21:23.840]   Yes, but it was also things like police would arrest protesters based on
[01:21:23.840 --> 01:21:27.680]   saying they were going to be at a Black Lives Matter protest,
[01:21:27.680 --> 01:21:31.360]   or they were scouting people who were there and matching them up against people they had
[01:21:31.360 --> 01:21:34.080]   warrants against, and then they would go arrest them.
[01:21:34.080 --> 01:21:34.720]   So that's...
[01:21:34.720 --> 01:21:37.520]   Yeah, but I don't know how they stopped that, really.
[01:21:37.520 --> 01:21:39.840]   And don't either.
[01:21:39.840 --> 01:21:42.880]   But at least it's no longer approved.
[01:21:44.720 --> 01:21:48.720]   It's not... Well, I think there was also companies that were doing it.
[01:21:48.720 --> 01:21:50.800]   They had like an AP...
[01:21:50.800 --> 01:21:55.840]   They were using the API to produce some of that data easily and provide it to law enforcement.
[01:21:55.840 --> 01:22:00.640]   Well, it reminds me of Tim Berners-Lee's...
[01:22:00.640 --> 01:22:02.720]   Tim Berners-Lee post...
[01:22:02.720 --> 01:22:04.240]   Tim Berners-Lee's post.
[01:22:04.240 --> 01:22:09.360]   He's the course inventor of the web, which is, I think now, 28 years old.
[01:22:09.360 --> 01:22:09.680]   It was...
[01:22:09.680 --> 01:22:12.240]   It's birthday was this past week, the World Wide Web.
[01:22:13.200 --> 01:22:21.680]   And he wrote at the time an open letter talking about the three issues he's increasingly worried about
[01:22:21.680 --> 01:22:24.160]   with the World Wide Web.
[01:22:24.160 --> 01:22:27.040]   And I think it'd be good to get you guys to weigh in on this.
[01:22:27.040 --> 01:22:29.120]   One, we've lost control of our personal data.
[01:22:29.120 --> 01:22:30.320]   I think that's pretty obvious.
[01:22:30.320 --> 01:22:35.520]   Widespread data collection by companies has...
[01:22:35.520 --> 01:22:40.000]   You know, many of us agree to this, free content in exchange for personal data.
[01:22:40.560 --> 01:22:45.680]   But fundamentally, we do not mind some information being collected in exchange for free services.
[01:22:45.680 --> 01:22:47.200]   But he says, "We're missing a trick.
[01:22:47.200 --> 01:22:50.880]   Our data is then held in proprietary silos out of sight.
[01:22:50.880 --> 01:22:55.120]   We lose out on the benefits we could realize if we had direct forever,
[01:22:55.120 --> 01:22:58.400]   if we had direct control and could choose with whom and..."
[01:22:58.400 --> 01:23:00.240]   I agree to share.
[01:23:00.240 --> 01:23:01.760]   I agree that this is a problem.
[01:23:01.760 --> 01:23:07.600]   I'm not sure how you solve it, but I think some have talked about having some sort of personal data
[01:23:07.600 --> 01:23:14.640]   silo, which you then contract to release your data to people in return for benefits of some kind,
[01:23:14.640 --> 01:23:16.080]   whether it's money or free services.
[01:23:16.080 --> 01:23:17.440]   There's a custody that does that.
[01:23:17.440 --> 01:23:19.520]   I love the idea.
[01:23:19.520 --> 01:23:22.080]   I think it's too complicated.
[01:23:22.080 --> 01:23:24.480]   Nura does it for health data.
[01:23:24.480 --> 01:23:29.600]   And they partner with certain device makers to do that.
[01:23:29.600 --> 01:23:31.600]   He also refers to...
[01:23:31.600 --> 01:23:32.640]   That's a company trying to build that.
[01:23:32.640 --> 01:23:37.920]   What Facebook was talking about, which is that governments can use this information.
[01:23:37.920 --> 01:23:40.080]   Bloggers can be arrested or killed.
[01:23:40.080 --> 01:23:41.520]   Political opponents can be monitored.
[01:23:41.520 --> 01:23:44.720]   And it creates a chilling effect on free speech.
[01:23:44.720 --> 01:23:46.160]   So I'd agree with point one.
[01:23:46.160 --> 01:23:47.840]   I don't know exactly how you solve it.
[01:23:47.840 --> 01:23:49.200]   I agree with all three points.
[01:23:49.200 --> 01:23:52.720]   I just think that the points two and three are very problematic.
[01:23:52.720 --> 01:23:55.440]   It's too easy for misinformation to spread on the web.
[01:23:55.440 --> 01:23:57.280]   Who would disagree with that?
[01:23:57.280 --> 01:24:02.320]   But I don't think any proposal to get rid of misinformation on the web
[01:24:02.960 --> 01:24:08.000]   is doable only because one man's misinformation is another man's truism.
[01:24:08.000 --> 01:24:10.000]   And who's going to make that?
[01:24:10.000 --> 01:24:10.800]   Not on Google.
[01:24:10.800 --> 01:24:11.680]   Not anymore.
[01:24:11.680 --> 01:24:14.880]   Google, the one true source.
[01:24:14.880 --> 01:24:19.040]   No, that's like I can talk about that change if you're not going to get to it.
[01:24:19.040 --> 01:24:22.880]   I would like to talk about that because Google had a problem,
[01:24:22.880 --> 01:24:26.160]   especially with the Google Home, where you would say you'd ask Google Home is Obama
[01:24:26.160 --> 01:24:29.840]   planning a coup and it would say yes, as a matter of fact it is.
[01:24:30.720 --> 01:24:33.760]   Because they were pulling news from false fix.
[01:24:33.760 --> 01:24:35.280]   That was just true.
[01:24:35.280 --> 01:24:36.480]   I mean, that's factual.
[01:24:36.480 --> 01:24:38.560]   Well, that's kind of my point.
[01:24:38.560 --> 01:24:40.240]   Who's going to decide this?
[01:24:40.240 --> 01:24:41.840]   I guess Google feels like they can.
[01:24:41.840 --> 01:24:45.040]   So that's the thing they announced this week.
[01:24:45.040 --> 01:24:50.640]   I mean, that thing's the thing that this week,
[01:24:50.640 --> 01:24:51.920]   but the story that we've got.
[01:24:51.920 --> 01:24:55.600]   If you ask them now, if you ask them now, it'll just say I don't know.
[01:24:55.600 --> 01:24:56.640]   Yeah.
[01:24:56.640 --> 01:25:00.320]   I don't know what you're talking about, which is actually worse.
[01:25:00.960 --> 01:25:04.640]   Is Obama spying on me through my microwave oven?
[01:25:04.640 --> 01:25:05.760]   I don't know what you're talking about.
[01:25:05.760 --> 01:25:06.960]   Is a scary answer.
[01:25:06.960 --> 01:25:10.640]   So what's new then to this story?
[01:25:10.640 --> 01:25:19.360]   Well, so they announced yesterday that Google has this army of what they call quality reviewers.
[01:25:19.360 --> 01:25:22.080]   It's about 10,000 over 10,000 contractors.
[01:25:22.080 --> 01:25:25.760]   And what these people do is they are given real searches.
[01:25:25.760 --> 01:25:29.600]   They're told search for this and then evaluate the results that came up
[01:25:29.600 --> 01:25:32.480]   and rate how well you think they satisfied the search you did.
[01:25:32.480 --> 01:25:35.040]   So if you were to search for, is the sky blue?
[01:25:35.040 --> 01:25:37.920]   Did someone would do that search and they'd look at the results?
[01:25:37.920 --> 01:25:41.360]   And if there was one that was saying red, they'd say this didn't satisfy it and so on.
[01:25:41.360 --> 01:25:45.840]   And they try to rate how the quality of the results they get,
[01:25:45.840 --> 01:25:50.800]   so that Google can take all that data and then go build algorithms to improve the
[01:25:50.800 --> 01:25:52.000]   things that are weak.
[01:25:52.000 --> 01:25:55.040]   And so Google's added this new thing for all the raters to start using,
[01:25:55.040 --> 01:25:57.600]   which is called the offensive.
[01:25:57.600 --> 01:25:58.720]   I can't remember the other one.
[01:25:58.720 --> 01:26:00.720]   Upsetting offensive context.
[01:26:00.720 --> 01:26:01.520]   Upsetting offensive.
[01:26:01.520 --> 01:26:06.240]   And this was designed now because in December you had these examples like
[01:26:06.240 --> 01:26:11.440]   when you would search is the Holocaust real, you would get these Holocaust denial sites
[01:26:11.440 --> 01:26:13.440]   dominating the search results.
[01:26:13.440 --> 01:26:17.840]   And so now what they're telling the raters is do these kinds of searches.
[01:26:17.840 --> 01:26:24.080]   And if you find stuff that looks offensive, looks like it's designed to promote hate,
[01:26:25.520 --> 01:26:31.920]   flag it as offensive or upsetting. And then they want to go back and use that data now to then
[01:26:31.920 --> 01:26:36.000]   try to better identify content like that and prevent it from coming up.
[01:26:36.000 --> 01:26:40.080]   When you do these kind of factually oriented searches, so you're not getting this offensive
[01:26:40.080 --> 01:26:45.600]   material. Now, if you really want to get to that white supremacist site and you know it by name
[01:26:45.600 --> 01:26:50.240]   and you type it in, you would still get that because Google would say, well, you're explicitly
[01:26:50.240 --> 01:26:56.800]   asking for it. But if you were to do a search for our white people in bread,
[01:26:56.800 --> 01:27:00.640]   the assumption in that case is going to be that you're looking for information about it,
[01:27:00.640 --> 01:27:04.000]   you're not looking to reconfirm any preconceptions you might have.
[01:27:04.000 --> 01:27:08.400]   And they're going to go towards, we're not going to try to give you this offensive or
[01:27:08.400 --> 01:27:12.960]   inflammatory material. We're going to try to give you factual, thought out,
[01:27:12.960 --> 01:27:15.600]   reasonable material to come back.
[01:27:16.480 --> 01:27:21.280]   And so that may help with some of the stuff. It may also help with that sort of what I call the
[01:27:21.280 --> 01:27:25.680]   one true answer problem that you talked about. The coup thing came from me because I was like,
[01:27:25.680 --> 01:27:30.160]   it was the day that the Trump stuff was going crazy and the wiretapping and I had typed that
[01:27:30.160 --> 01:27:35.360]   into my phone and I could say, wow, it's, and I did a stream of these kinds of examples. We've had
[01:27:35.360 --> 01:27:40.640]   any number of people doing these kinds of examples now, but they also said, and part of these changes
[01:27:40.640 --> 01:27:47.760]   is that they're also trying to get the evaluators to go a little bit heavier on making sure that
[01:27:47.760 --> 01:27:53.280]   stuff is accurate rather than authoritative. So apparently sometimes you can get these sites and
[01:27:53.280 --> 01:28:00.800]   they gave me one example I didn't get into, but people would be doing these searches for Hitler's
[01:28:00.800 --> 01:28:04.640]   daughter and I don't think that was the answer. It was some other search that they would do.
[01:28:05.200 --> 01:28:09.600]   And the raiders would tend to lean towards saying, well, it was very authoritative.
[01:28:09.600 --> 01:28:16.320]   The site seemed to have all these reputations, special, but whether or not the facts were
[01:28:16.320 --> 01:28:21.680]   correct was another thing. And so the emphasis now is going to be more on the fact checking aspect
[01:28:21.680 --> 01:28:25.200]   of it so that you can flag things that are not seeming to be factually correct.
[01:28:25.200 --> 01:28:26.480]   How well do you think this will work?
[01:28:26.480 --> 01:28:32.640]   You know, it's one of those that remain to be seen types of things. They said that they did a
[01:28:32.640 --> 01:28:39.520]   small trial of this. And then in fact, they rolled out an update that was designed to deal with the
[01:28:39.520 --> 01:28:45.200]   Holocaust kind of search. And as I noted in my story, that stuff has changed. Now, one of the
[01:28:45.200 --> 01:28:49.120]   reasons the Holocaust search changed is because enough people were upset that you were getting
[01:28:49.120 --> 01:28:53.440]   these terrible answers, that they started writing good things to explain that yes,
[01:28:53.440 --> 01:28:59.120]   the Holocaust really happened to your spectrum. So they changed the result merely by changing the
[01:28:59.120 --> 01:29:05.600]   web. >> But there were other searches like when I would search for was Obama born in Kenya.
[01:29:05.600 --> 01:29:13.120]   The top result I used to get on Google was of YouTube video, where he's explaining that and
[01:29:13.120 --> 01:29:17.280]   admitting on video that yes, he was born in Kenya, except that he didn't do that.
[01:29:17.280 --> 01:29:22.320]   They just took a video of him talking as someone edited it and put in words, right?
[01:29:22.320 --> 01:29:27.440]   So that's no longer at the top. And in fact, the results are much different when you search for
[01:29:27.440 --> 01:29:30.720]   that now than when it looks before. >> How much energy are these content
[01:29:30.720 --> 01:29:38.160]   readers going to put into researching that video? I mean, is it their job then to go look at every
[01:29:38.160 --> 01:29:43.760]   bit of content on the net and verify its accuracy? >> I mean, part of it apparently is that, and
[01:29:43.760 --> 01:29:45.760]   they're already supposed to be doing something. >> Well, they don't have enough people then,
[01:29:45.760 --> 01:29:51.280]   they need more people. >> But the goal isn't that they have to research every single fact that
[01:29:51.280 --> 01:29:55.520]   you could possibly search on Google. I mean, Google's dealing with hundreds of millions of
[01:29:55.520 --> 01:30:01.760]   searches per day. The goal that they're hoping is if they can identify things that are common
[01:30:01.760 --> 01:30:08.080]   to pages that tend to give false facts, that they maybe they have enough signals that they can build
[01:30:08.080 --> 01:30:13.040]   either the human written algorithms or the machine learning can identify it. That's still a tough
[01:30:13.040 --> 01:30:21.040]   challenge because how do you figure out what's true? And they may have more success in identifying
[01:30:21.040 --> 01:30:26.960]   signals from content that's not as trustworthy and being able to filter more of that stuff out
[01:30:26.960 --> 01:30:31.760]   and knowing that it happens. But like another example I gave when you did the factual thing is
[01:30:31.760 --> 01:30:39.120]   you could ask is Donald Trump paranoid, right? And I think this will still work on Google home
[01:30:39.120 --> 01:30:43.440]   in the Google Assistant, but it would come back and say Donald Trump is paranoid, Donald Trump
[01:30:43.440 --> 01:30:50.720]   is mentally ill. And okay, I'm not a Trump fan. I certainly think he's paranoid and probably
[01:30:50.720 --> 01:30:56.480]   mentally ill. But from a medical standpoint, I'm not a doctor and we don't have any medical
[01:30:56.480 --> 01:31:00.160]   things that have said that. And in fact, the article wasn't a medical thing that was saying
[01:31:00.160 --> 01:31:04.320]   that either. It was a column that was written by a little known publication called Slate,
[01:31:04.320 --> 01:31:08.160]   which is a highly authoritative site. >> So here's the problem.
[01:31:08.160 --> 01:31:13.360]   Here's exactly the problem. >> Yeah. So I just think this is a nonce,
[01:31:13.360 --> 01:31:21.280]   this is impossible. And the problem for the potential for abuse, mistakes, and a mess is just
[01:31:21.280 --> 01:31:25.280]   where I commend them for trying to solve it. But boy, this doesn't play.
[01:31:25.280 --> 01:31:29.840]   >> You're not going to get perfect. You're never going to get perfect facts at report.
[01:31:29.840 --> 01:31:36.800]   But what they may have success on doing is at least reducing the things where you get these
[01:31:36.800 --> 01:31:42.080]   really offensive things that come up, where you search for our women evil. And the first result
[01:31:42.080 --> 01:31:46.960]   you come up is this weird diatribe where someone's telling you why they're all evil and they're
[01:31:46.960 --> 01:31:51.760]   prostituted hard. >> I think it's telling that the tag they're using is upsetting offensive,
[01:31:51.760 --> 01:31:57.920]   not wrong. You're saying this is upsetting or offensive. But that's a culturally defined
[01:31:57.920 --> 01:32:04.400]   judgment, highly subjective. >> It used cultural norms in your own locale.
[01:32:04.400 --> 01:32:07.440]   >> They have 200 pages of guidelines, by the way.
[01:32:07.440 --> 01:32:11.920]   This is, I don't know, what do you think, Casey, is this a good idea?
[01:32:11.920 --> 01:32:19.680]   >> So I was going to back it up just a sec, like just a bit to this idea that Tim Burtorsley was
[01:32:19.680 --> 01:32:22.960]   saying, which is it's too easy for misinformation spread on the web.
[01:32:22.960 --> 01:32:25.680]   >> Right. This is Google's way of trying to solve that, right?
[01:32:25.680 --> 01:32:32.640]   >> Right. So I don't actually think, what if you take that and say, no, it's actually not any
[01:32:32.640 --> 01:32:40.320]   easier than it has been, what it is, it's scalable. So it spreads farther. So it was always easy,
[01:32:40.320 --> 01:32:45.200]   like anybody who's been in high school can say, hey, it's so easy for misinformation to spread,
[01:32:45.200 --> 01:32:48.160]   right? Somebody says something and then everybody suddenly believes that,
[01:32:48.160 --> 01:32:53.440]   I'm thinking of terrible things. >> I know, I'm trying not to say them either.
[01:32:53.440 --> 01:32:56.640]   >> I'm like, oh, that's bad. >> That's bad.
[01:32:56.640 --> 01:33:00.400]   >> But I think that's true. >> Right.
[01:33:00.400 --> 01:33:03.440]   >> Terbles. >> Terbles are a good example. >> Yeah, gerbils are excellent.
[01:33:03.440 --> 01:33:04.400]   >> Terbles. >> Yeah.
[01:33:04.400 --> 01:33:12.560]   >> So I think the scalability is the problem. It's not that misinformation is any easier
[01:33:12.560 --> 01:33:15.040]   harder. It's just that it gets to more places quickly. >> Right.
[01:33:15.040 --> 01:33:19.280]   >> And so how do you solve that? >> When you created --
[01:33:19.280 --> 01:33:23.200]   >> What we created is the ultimate publishing medium
[01:33:23.200 --> 01:33:30.240]   that goes to everybody that everybody has access to. This didn't come up in the past because only
[01:33:30.240 --> 01:33:37.280]   people with lots of money could have printing presses or TV and radio studios or motion picture
[01:33:37.280 --> 01:33:44.320]   studios. But now everybody has access, the means to publish, and the publishing that they do can
[01:33:44.320 --> 01:33:53.840]   get to everybody. And inevitably, you're going to have a huge amount of crap. And I don't understand
[01:33:53.840 --> 01:33:59.440]   how you propose to change that because -- >> Well, I mean, I think you've got twin problems.
[01:33:59.440 --> 01:34:04.800]   You've got the virality or the viral nature that some of this stuff can spread in, in particular,
[01:34:04.800 --> 01:34:10.240]   on Facebook. And I think they have a challenge as to is there more they can do to --
[01:34:10.240 --> 01:34:13.440]   >> Yeah, there are challenges different than Google's because they're not a suggestion.
[01:34:13.440 --> 01:34:18.240]   >> I know, but they're two different things, but they go hand in hand. So are there things they
[01:34:18.240 --> 01:34:22.560]   can do to better flag stuff to say to people, maybe you need to think twice about this or just
[01:34:22.560 --> 01:34:29.760]   stop outright false stuff that you can tell. But then the challenge Google has is these things
[01:34:29.760 --> 01:34:35.600]   aren't viral on Google, but Google has a different issue where people who actually may want to go
[01:34:35.600 --> 01:34:44.320]   to research this stuff get exposed to like one-sided things or just completely non-authoritative stuff.
[01:34:44.320 --> 01:34:49.360]   And then it's like, so now they can't even debunk the stuff that they may have heard on a viral nature.
[01:34:49.360 --> 01:34:52.560]   >> Why big problem? >> What if instead of --
[01:34:52.560 --> 01:34:56.320]   >> Good. >> So what if instead of looking for search terms,
[01:34:56.320 --> 01:35:01.280]   Google spent more effort grouping like information together and by like,
[01:35:01.280 --> 01:35:11.200]   you get more perspectives? And I'm thinking of -- so kind of like your snopes results would
[01:35:11.200 --> 01:35:16.400]   always come up when you did a search for something, right? That sounds plausible.
[01:35:16.400 --> 01:35:21.360]   >> That's kind of what Facebook's doing, right? Facebook is putting a flag and saying,
[01:35:21.360 --> 01:35:25.360]   here's what you might want to read to or something like that.
[01:35:25.360 --> 01:35:27.600]   That's what -- >> Because I --
[01:35:27.600 --> 01:35:32.480]   >> -- is this so subjective? Here's why I have a problem with it. And I'll let you finish. But
[01:35:32.480 --> 01:35:37.840]   first, here's why I have a problem with it. And I was really worried when we started using this
[01:35:37.840 --> 01:35:42.720]   label fake news. And immediately, the thing I was most worried about happened, which is,
[01:35:42.720 --> 01:35:47.600]   we have our fake news. Suddenly, they're using the term fake news to describe what we think is
[01:35:47.600 --> 01:35:52.080]   real news. And that's the problem. As soon as you start labeling stuff fake news,
[01:35:52.080 --> 01:36:00.400]   the whole thing goes out the window because everybody has a different opinion on it.
[01:36:00.400 --> 01:36:05.520]   And you and I might agree that there are facts and there are non-facts. But
[01:36:07.040 --> 01:36:12.960]   that's a small subset of this overall problem. And I don't know if -- is Google supposed to be a
[01:36:12.960 --> 01:36:15.920]   fact engine or is supposed to be a -- >> Well, yes.
[01:36:15.920 --> 01:36:18.240]   >> -- yes. >> Wait a minute. Is it Google?
[01:36:18.240 --> 01:36:22.000]   >> Yes. >> That's a misunderstanding of it, I think. Google's supposed to find whatever you're
[01:36:22.000 --> 01:36:23.440]   looking for. >> Google's own understanding of it.
[01:36:23.440 --> 01:36:26.880]   >> Well, they're wrong. And that's the problem with snippets and that's probably the knowledge
[01:36:26.880 --> 01:36:31.760]   graph. They're not -- they're a search engine. They're supposed to enable you to find something
[01:36:31.760 --> 01:36:36.640]   you're looking for, some piece of content on the net. And they shouldn't be judging factual
[01:36:36.640 --> 01:36:38.560]   the factual nature of it. I think that's a mistake.
[01:36:38.560 --> 01:36:44.880]   Okay. >> So I'm going to throw a thesis I've got out here for you because this is something I've
[01:36:44.880 --> 01:36:50.080]   actually been thinking a lot about with Google. And my thought is it needs to move from becoming
[01:36:50.080 --> 01:36:54.640]   or from being an advertising company to being a services-oriented company.
[01:36:54.640 --> 01:36:59.920]   >> Yeah, I agree. >> And that helps solve this problem in some ways because a service it would
[01:36:59.920 --> 01:37:06.320]   provide clearly is factual information, right? Not alternative facts, just facts.
[01:37:06.320 --> 01:37:13.920]   And then it's becoming important not just because of this issue, but also because of, you know,
[01:37:13.920 --> 01:37:20.720]   IoT, its emphasis on machine learning. In the way we're going to start interacting with devices
[01:37:20.720 --> 01:37:27.520]   that don't have screens, right? And you can't, in a voice world, be a search engine. You have to
[01:37:27.520 --> 01:37:31.440]   provide a service and that service would be the facts or the truth.
[01:37:31.440 --> 01:37:36.640]   >> Why can't it just be a search engine? >> Well, if I want to know what to-
[01:37:36.640 --> 01:37:40.000]   >> So here's a search engine. >> Let me give you an example.
[01:37:40.000 --> 01:37:42.000]   God exists.
[01:37:42.000 --> 01:37:50.160]   Some people will say that's factual. Some people say it's not. It's not Google's job to determine that.
[01:37:50.160 --> 01:37:55.600]   >> Correct. And in fact, I've used that before when people have said,
[01:37:55.600 --> 01:37:59.920]   well, we should only have factual stuff and they get really on a high horse about it. And I said,
[01:37:59.920 --> 01:38:06.320]   okay, so when I search for that exact type of thing, is God real, I should not get any religious
[01:38:06.320 --> 01:38:12.720]   information coming up because by its nature, religions are based on faith, not fact. And so,
[01:38:12.720 --> 01:38:17.520]   if you're going to say you can't only have factual information in Google,
[01:38:17.520 --> 01:38:22.960]   >> Google should not be not weighing on the accuracy of whatever its search results show.
[01:38:22.960 --> 01:38:28.800]   It should be pure search results. >> No, but I think there are verifiable facts.
[01:38:28.800 --> 01:38:32.720]   >> There's no such thing. >> No, there are verifiable facts in the world.
[01:38:32.720 --> 01:38:37.200]   >> I think there are things as search results. >> No, no, sorry, there's no such thing as pure
[01:38:37.200 --> 01:38:38.480]   search results. >> Why not?
[01:38:38.480 --> 01:38:42.640]   >> There are such things as verifiable facts. >> I think it's exact opposite.
[01:38:42.640 --> 01:38:45.280]   >> No, it's not. >> It's search engine.
[01:38:45.280 --> 01:38:48.720]   >> You know Google bombing? >> So why-
[01:38:49.360 --> 01:38:52.560]   >> So Google- >> So that changes what a search
[01:38:52.560 --> 01:38:58.160]   results in, and that's manipulated. >> It's a combination of things.
[01:38:58.160 --> 01:39:02.480]   You have some things that you search for that are verifiable facts. For example,
[01:39:02.480 --> 01:39:07.280]   if I search for who invented homework, that should be a verifiable fact, right?
[01:39:07.280 --> 01:39:12.960]   >> You shouldn't get some guy in 1905. >> Right, and the Google is reporting and listing that it's
[01:39:12.960 --> 01:39:21.680]   this guy in 1905, it's number, and it's like, no, that's just wrong. It's not even a fake news
[01:39:21.680 --> 01:39:27.920]   political debate thing. It's literally just wrong. And it's embarrassing for Google when it's
[01:39:27.920 --> 01:39:32.000]   trying to provide facts and does it in that kind of thing that is actually showing
[01:39:32.000 --> 01:39:36.160]   misleaved facts. >> I would submit that what Google should be
[01:39:36.160 --> 01:39:42.160]   saying is do not use Google to determine facts. You could use Google to search for
[01:39:42.160 --> 01:39:49.440]   sources and read the source, and then you as a human have to decide whether that is factual or not.
[01:39:49.440 --> 01:39:52.640]   Google should not be the arbiter of facts. It's a search engine.
[01:39:52.640 --> 01:39:58.480]   >> I agree with you. That's like saying- >> I agree with you in the sense that,
[01:39:58.480 --> 01:40:04.320]   I agree with you in the sense that it would behoove Google and everybody to understand that
[01:40:05.840 --> 01:40:11.360]   we all need better critical thinking skills. And that Google is just a tool, like a library
[01:40:11.360 --> 01:40:17.120]   was a tool, and that you need to go through on many things that you search for and actually do
[01:40:17.120 --> 01:40:21.680]   research. >> Google's a card catalog. The card catalog in the library should be able to find
[01:40:21.680 --> 01:40:28.240]   completely species-flicious BS books just as much as it finds real books. That's what the card
[01:40:28.240 --> 01:40:32.880]   catalog does. Google should be the card catalog for the internet, not some librarian saying,
[01:40:32.880 --> 01:40:39.280]   well, that's not true. Well, that is true. Because that way lies danger.
[01:40:39.280 --> 01:40:42.880]   >> But people don't think of Google that way. People think of it-
[01:40:42.880 --> 01:40:44.960]   >> Google's job should be educated people.
[01:40:44.960 --> 01:40:51.040]   >> No, this is the same problem we have with Tesla and it's calling its car a self-driving car
[01:40:51.040 --> 01:40:57.120]   when it's really autopilot. And then Tesla getting mad at people for not paying attention while
[01:40:57.120 --> 01:41:02.880]   they drive their car off the road. I mean, tech companies and all companies have a history of
[01:41:02.880 --> 01:41:09.200]   saying, misstating what they do and we buy into it. >> I think Google's pretending it's Yahoo,
[01:41:09.200 --> 01:41:14.720]   which was a human-generated index. >> That's a big unit.
[01:41:14.720 --> 01:41:18.880]   >> But that didn't scale, did it? I mean, you can't do it. >> Let me put it in a different way. So
[01:41:20.160 --> 01:41:27.440]   search engines biggest challenge had been up until recently that they had to deal with
[01:41:27.440 --> 01:41:32.160]   adversarial information. And by adversarial, I mean that that was the term that they would use.
[01:41:32.160 --> 01:41:36.880]   But what I mean is they had to deal with spam. That unlike our other research tools before we
[01:41:36.880 --> 01:41:42.240]   had search engines, they would go through a small collection of books and trusted sources.
[01:41:42.240 --> 01:41:44.720]   And no one was actively trying to mislead them. >> Right.
[01:41:44.720 --> 01:41:48.000]   >> Right. >> And that's what Google bombing also is.
[01:41:48.000 --> 01:41:50.560]   >> Not just Google bombing. >> No, I understand.
[01:41:50.560 --> 01:41:52.880]   >> When the search engines just came along. >> You just say Google bombing is a form of spam.
[01:41:52.880 --> 01:41:55.040]   So that's the form of error. >> Right. But when the searches came along,
[01:41:55.040 --> 01:41:58.960]   you had people actively trying to subvert the search listings to get
[01:41:58.960 --> 01:42:01.760]   poor quality information, but to get a top-ranked getting money off it.
[01:42:01.760 --> 01:42:06.160]   And that was, Google's claim to fame was it had better protection against that.
[01:42:06.160 --> 01:42:10.720]   It used links, it tried to figure out authoritative information, and that's why it wiped off all
[01:42:10.720 --> 01:42:17.440]   the other search engines by and large from the map and took over. But now we're in this weird
[01:42:17.440 --> 01:42:24.240]   situation where it's not just outright spam and people doing keyword stuffing and buying
[01:42:24.240 --> 01:42:29.440]   links and all that stuff. It's that you have a flood of unauthoritative information that's coming
[01:42:29.440 --> 01:42:35.520]   in. And it doesn't relieve search engine from the duty of trying to pick out the best of the
[01:42:35.520 --> 01:42:41.280]   resources that are there. And I think that's the thing. There are some things where Google cannot
[01:42:41.280 --> 01:42:45.680]   fit. There are some things where Google could tell it is a fact. And we could say these are
[01:42:45.680 --> 01:42:49.440]   facts and they should provide that. There's also a greater chunk of things where
[01:42:49.440 --> 01:42:56.320]   we don't know the direct answer to it. It's not even just maybe politically charged or where you
[01:42:56.320 --> 01:43:00.800]   are on the political spectrum. It's just that we might not know what the right answer is if you
[01:43:00.800 --> 01:43:05.520]   want to fix something or if you want to buy a refrigerator or whatever. So you need a variety
[01:43:05.520 --> 01:43:10.480]   of resources. But the challenge is that you get into some of these kinds of things where
[01:43:11.040 --> 01:43:15.760]   you're not like with the Holocaust search, you weren't getting a variety of resources trying to
[01:43:15.760 --> 01:43:20.320]   help you understand whether the Holocaust is real or not. You are getting heavily pages that were
[01:43:20.320 --> 01:43:25.680]   heavily dominated by denial stuff. Because the people who write about the Holocaust never occurs
[01:43:25.680 --> 01:43:30.240]   to them that there are other people out there who are searching for things that is not real and
[01:43:30.240 --> 01:43:34.880]   writing content to explain why it's not happening. So there's a lot of stuff that's going to be
[01:43:34.880 --> 01:43:39.520]   changing. None of it is going to be solved instantly by Google. There's a lot of the owners that
[01:43:39.520 --> 01:43:44.800]   remains on human beings to do it. But they do need to do something. They do need to look at this because
[01:43:44.800 --> 01:43:52.160]   it is a problem that in the end causes them to lose faith. Until now, Google by and large has
[01:43:52.160 --> 01:43:56.080]   been able to go along and say, "We have the best search results." Unquestioned, everybody, "Oh, yeah,
[01:43:56.080 --> 01:44:00.240]   Google's results are great." And when you do one of these searches and you get one of these featured
[01:44:00.240 --> 01:44:06.160]   snippets and your jaw goes, you realize, "Wow, Google didn't know what it's talking about."
[01:44:06.160 --> 01:44:09.360]   And that is not a good thing to be in if you're a search engine.
[01:44:09.360 --> 01:44:14.560]   Well, I agree with you. Spam is the problem. But I would also submit Google made a mistake when
[01:44:14.560 --> 01:44:20.880]   it said, "Well, we should give you featured snippets because that's saying we're going to give you the
[01:44:20.880 --> 01:44:25.680]   facts." And that's in the-- I know, but that's their key product differentiator.
[01:44:25.680 --> 01:44:29.040]   We need Amazon Echo and Google Home, otherwise, as much as by the Echo.
[01:44:29.040 --> 01:44:36.480]   I know, they shouldn't be doing that. But so I think, Stacy, okay, I agree with both of you in the
[01:44:36.480 --> 01:44:43.120]   sense that Google bombing is a form of spam that you're going to get skewed search results because
[01:44:43.120 --> 01:44:52.560]   some groups are intentionally or maybe even not intentionally flooding the web with a position.
[01:44:52.560 --> 01:44:56.880]   I just worry that if we start getting in this-- I don't want Google to arbitrate whether something's a
[01:44:56.880 --> 01:44:59.840]   factor or not. That gives them way too much power.
[01:44:59.840 --> 01:45:06.880]   Well, maybe it's go to our app to see more results. I mean, maybe you get that equivalent.
[01:45:06.880 --> 01:45:11.760]   I don't know what to say. I mean, the problem is we're going to be moving into a world where we
[01:45:11.760 --> 01:45:17.920]   have a limited output, right? We don't have a screen everywhere that you can see things and
[01:45:17.920 --> 01:45:20.160]   interact with things. So how do you-- Yeah, so that's why snippets, right?
[01:45:20.160 --> 01:45:22.640]   How do you design for that? Right. That's a good point too.
[01:45:22.640 --> 01:45:26.720]   Yeah. All right. Good conversation. This doesn't go right back to the Tim Berners'
[01:45:26.720 --> 01:45:32.640]   lead because he's kind of saying this. My problem was it was too hard.
[01:45:32.640 --> 01:45:38.320]   Well, I don't understand how it's going to happen without introducing some other form of bias.
[01:45:38.320 --> 01:45:43.120]   His third point is even more difficult, which is political advertising needs transparency
[01:45:43.120 --> 01:45:52.720]   and understanding. He's worried about how the sophisticated ad networks, particularly on Facebook,
[01:45:52.720 --> 01:46:01.200]   are able to serve thousands of different ads. And you don't see all the different ads.
[01:46:01.200 --> 01:46:07.280]   Only the people they're targeted to and targeted advertising, he says, allows a campaign to say
[01:46:07.280 --> 01:46:12.640]   completely different, possibly even conflicting things to different groups. That's undemocratic.
[01:46:12.640 --> 01:46:16.240]   He says, he's got a problem. I'm not saying I don't see it as a problem, but I don't see how
[01:46:16.240 --> 01:46:21.360]   you solve that because of-- Well, you've got to have political speech is a form of free speech.
[01:46:22.240 --> 01:46:29.040]   So these are-- Look, these are thorny problems, but I think they come along with what you invented,
[01:46:29.040 --> 01:46:33.920]   what we in-- Tim Berners' Lee invented, which was a publishing medium open to everyone.
[01:46:33.920 --> 01:46:39.600]   Right. It's not a bug. It's a feature. Yeah. I mean, that's what you created.
[01:46:39.600 --> 01:46:44.720]   Yeah. It's just a fault. I played Tim Berners' lead. Well, now he's saying we ought to
[01:46:44.720 --> 01:46:50.000]   regulate this, and I don't know how you regulate it because by its very nature, any form of
[01:46:50.000 --> 01:46:53.840]   regulation is going to introduce a bias that some people are going to say, that's wrong.
[01:46:53.840 --> 01:47:02.400]   We'll find ways to regulate stuff in the ways that we find ways to regulate things in the real
[01:47:02.400 --> 01:47:08.160]   world. For example, with the political ads he's so concerned about, very simple to say, you know what?
[01:47:08.160 --> 01:47:14.000]   If you're going to create political ads and you're a political campaign, you have to post all the
[01:47:14.000 --> 01:47:17.840]   political ads you're running in a place that are accessible. That's a simple thing to do.
[01:47:17.840 --> 01:47:22.240]   That could be done by long. That's a great idea. That's easy to do. That's a great idea.
[01:47:22.240 --> 01:47:26.160]   Yeah. There's other stuff that you can use. It's like a fairness doctrine for the internet age.
[01:47:26.160 --> 01:47:32.320]   Yeah. No, that's actually a great idea. No hidden ads. Every ad has to be viewable at a publicly
[01:47:32.320 --> 01:47:37.840]   accessible site. I think that's a great idea. And then you have to hope that the press has lots
[01:47:37.840 --> 01:47:43.680]   of time to go through your 50,000 different ads. By the way, speaking of big numbers,
[01:47:43.680 --> 01:47:50.480]   according to CBS News, study reveals 48,000 Twitter accounts are actually bots. That seems low.
[01:47:50.480 --> 01:47:54.880]   I don't, maybe I'm wrong. That's it. University of Southern California.
[01:47:54.880 --> 01:48:02.240]   I did I say a thousand million. Yeah. That is, that would be low. University of Southern California
[01:48:02.240 --> 01:48:07.440]   says nine to 15 percent of Twitter accounts are bots controlled, not by humans, but by software.
[01:48:08.960 --> 01:48:13.840]   So since Twitter has 319 million monthly active users, 48 million would be
[01:48:13.840 --> 01:48:19.520]   bots. Yeah, but to be fair, I mean, at least they're very politically active.
[01:48:19.520 --> 01:48:25.680]   Those bots voted. Let me tell you. Well, I was going to say that there are some useful bots
[01:48:25.680 --> 01:48:30.080]   out there. Like I have a couple of Twitter bots. I have a bot. I have a Twitter bot.
[01:48:30.080 --> 01:48:36.960]   You have a Leo bot? It's called Leo bot as a matter of fact. Yeah. It's pretty benign.
[01:48:38.720 --> 01:48:44.000]   I think it's chief twit bot actually. Chief twit bot. Yeah, I call it Leo
[01:48:44.000 --> 01:48:47.920]   the bot. But all it does is it has when new shows come out.
[01:48:47.920 --> 01:48:56.880]   Yeah, that's a combination of if this than that and zap your scripts.
[01:48:56.880 --> 01:49:02.640]   Right. It used to announce when I drove up, that's when I was driving a
[01:49:02.640 --> 01:49:06.240]   internal combustion engine vehicle with an automatic.
[01:49:08.080 --> 01:49:13.600]   An ice and ice. You love it. I have Tesla people call them ice is ice. I do.
[01:49:13.600 --> 01:49:21.280]   You Tesla people actually, uh, lots of Tesla news, right?
[01:49:21.280 --> 01:49:27.120]   Or just Elon Musk news. Well, there's always Elon Musk news. There's some question about
[01:49:27.120 --> 01:49:32.000]   whether one of the two people who will be going to Mars next year via SpaceX is one of the might
[01:49:32.000 --> 01:49:39.120]   be Elon Musk. I won't. I don't think we should let Elon go. We can't afford the game. Really
[01:49:39.120 --> 01:49:46.080]   isn't it? He won't say that that was sort of like James Cameron. I want to make Titanic. It
[01:49:46.080 --> 01:49:49.200]   wasn't because he wanted to make a movie about the Titanic. He just wanted to dive down there and
[01:49:49.200 --> 01:49:56.720]   needed something to pay for it. Elon just wants to go to the Mars. The moon. Yeah. He also, I'm
[01:49:56.720 --> 01:50:01.840]   okay with that. He says, I can fix South Australia's power network in a hundred days or it's
[01:50:01.840 --> 01:50:08.800]   free. He wants to build a hundred megawatt battery storage farm for Australia. I guess Southern
[01:50:08.800 --> 01:50:17.920]   Australia has a energy woes woes. So this battery thing is actually, you know, the gig
[01:50:17.920 --> 01:50:24.320]   factory and Elon's capacity of making low cost high capacity batteries seems to be exploding.
[01:50:24.320 --> 01:50:28.800]   Well, that's a bad word to use with batteries, but the power thing just their own fault because
[01:50:28.800 --> 01:50:34.000]   they're on that. We're a half hour off of the rest of Australia. They would just,
[01:50:34.000 --> 01:50:41.200]   that's their doing if they would just be one hour and not this one thirty thing. Then it'd be fine.
[01:50:41.200 --> 01:50:48.880]   Really? No. They really are half hour off. He's joking. He's teasing. No, I'm not. Well, I know it's true,
[01:50:48.880 --> 01:50:51.120]   but it's not what the power problem. But that's something to do with the power.
[01:50:51.120 --> 01:50:57.920]   Okay. That's, I was like, how does this affect power at all? Well, the sun goes down earlier,
[01:50:57.920 --> 01:51:03.520]   half an hour earlier. There we go. That's right. So there, it's dark longer. Yeah, it's dark longer.
[01:51:03.520 --> 01:51:11.120]   Tesla is looking to raise one and a half billion dollars to help make model threes.
[01:51:11.120 --> 01:51:18.160]   You'll handle by 25 million of that. If you want to get some stock in the Tesla, this might be a
[01:51:18.160 --> 01:51:26.240]   good time to do it. I have, you know what, I, we are, you and I already invested in Tesla,
[01:51:26.240 --> 01:51:34.320]   Stacey, by just by buying. I'm not. That's enough. Well, so legit question. We're, oh, go on.
[01:51:34.320 --> 01:51:40.000]   I was, I did the anti investment just bought the bolt. So I don't know. That's a good one.
[01:51:40.000 --> 01:51:44.480]   How do you like it? Yeah. I love the bowl. I really want to get a bowl. I liked the
[01:51:44.480 --> 01:51:49.440]   bowl, which I first saw it South by Southwest, but that is a little gasoline assist. The
[01:51:49.440 --> 01:51:54.880]   bolt is all electric and yeah, it's all 200 miles, 250 mile range. Good range. Yeah.
[01:51:54.880 --> 01:52:01.360]   Yeah. Have you been getting that? Well, we, we don't drive it that far. So like,
[01:52:01.360 --> 01:52:06.720]   occasionally we'll go into LA, but the nice thing is all I can drive into LA, it's, you know, 45
[01:52:06.720 --> 01:52:11.600]   miles away and come back 45. I don't even think about it. I did do a trip this past week and where
[01:52:11.600 --> 01:52:17.200]   I went up to Santa Clarita and I stayed overnight to go on this tour thing. And that, that involved
[01:52:17.200 --> 01:52:21.520]   me trying to find a fast charger because I didn't know if I'd have enough range to get back with
[01:52:21.520 --> 01:52:25.680]   all the trips I wanted to do. And I probably would have made it, but that's where Elon really was
[01:52:25.680 --> 01:52:31.600]   smart that fast, the supercharger really smart. But I found, I did find one and in like, it topped
[01:52:31.600 --> 01:52:36.560]   my car up to 80% 30 minutes. So you can do that on a bolt. You can find a fast charger. Yep.
[01:52:36.560 --> 01:52:42.640]   Is it a 40 amp? What is, what is fast for a bolt? 72. The fast charger is a DC. I think it's 50 amp.
[01:52:42.640 --> 01:52:50.080]   Okay. DC. And it, it's supposedly can add 90 miles in a half hour. So, and then at home,
[01:52:50.080 --> 01:52:56.560]   we have the, the one that does 25 miles in an hour. But yeah, no, it's been, it's been wonderful.
[01:52:56.560 --> 01:53:01.120]   And I think the other thing for me, it's just been like driving a smart car. It's like driving
[01:53:01.120 --> 01:53:05.280]   in electric cars, like having a new smartphone or something. I don't know. It's just, but it's
[01:53:05.280 --> 01:53:10.320]   been great. Yeah. I'm really impressed. Stacy, when you do your panel, I want you to talk about the
[01:53:10.320 --> 01:53:16.640]   audio synchronization that Amazon is proposing to offer to the echo in the dot. This could be bad
[01:53:16.640 --> 01:53:24.080]   for Sonos, right? Well, especially if they don't have the, the Sonos integration done. I mean,
[01:53:24.080 --> 01:53:28.880]   I'm not going to ditch my Sonos speakers. No, they're good speakers. Audio synchronization.
[01:53:28.880 --> 01:53:31.280]   That was the, that's the secret sauce for Sonos, right?
[01:53:31.280 --> 01:53:35.680]   Let me see if I get that. I think Google whole already did that.
[01:53:35.680 --> 01:53:40.800]   Secret sauce for Sonos is, yeah, Google Home does that. So, Secret sauce for Sonos is actually
[01:53:40.800 --> 01:53:45.680]   its Wi-Fi network and how it works. But this is, I was going to say, this is more response to
[01:53:45.680 --> 01:53:49.840]   Google, I think. Oh, I didn't realize Google Home did that in the Google Home. So let me look,
[01:53:49.840 --> 01:53:56.720]   because apparently in some people's Amazon Echo apps, it's not called the Echo app. It's called
[01:53:56.720 --> 01:54:01.760]   the other word, the A word app. In under smart home, you're getting synchronized audio. So,
[01:54:01.760 --> 01:54:05.760]   let me just look and see if I've got it. Oh, really? Yeah. Yeah.
[01:54:05.760 --> 01:54:12.320]   Smart home. No, no, no, no, no, no, don't got it. But wait a minute. Oh, I, maybe I would,
[01:54:12.320 --> 01:54:16.640]   if I connected, it says use groups to control multiple devices at once. I don't even know about
[01:54:16.640 --> 01:54:23.920]   that. There's groups, huh? Hmm. Oh, I have groups. See, then look in your A app, A word app,
[01:54:23.920 --> 01:54:27.760]   and see if you could make your game. My A app takes forever to load.
[01:54:27.760 --> 01:54:31.040]   Oh, are we avoiding saying the word because we don't have to set off people's devices?
[01:54:31.040 --> 01:54:33.840]   Thank you, Danny. I had a sent out of my mo. Because we're.
[01:54:33.840 --> 01:54:41.760]   I just this morning, I did a hit on the biggest station LA, KFI with Bill Handel.
[01:54:41.760 --> 01:54:44.240]   And he kept saying the A word. I said, Bill, you're going to stop that.
[01:54:44.240 --> 01:54:49.680]   You're buying people's stuff. You're triggering the problem is people like him think this is funny.
[01:54:49.680 --> 01:54:56.720]   Yeah. It's not funny. It's not funny. So is it under smart home, you said? Yeah, supposedly.
[01:54:57.360 --> 01:55:01.440]   If you get it, it's one of those. So mine does not have it. You don't have it yet. Okay.
[01:55:01.440 --> 01:55:06.480]   So how well does the Google home do that? I only have one, so I never tried it.
[01:55:06.480 --> 01:55:17.680]   I can't be bothered to do it. I like the echo and I because the echo, I think, has great speakers.
[01:55:17.680 --> 01:55:22.000]   Once I put that in my wife's like, this is, I don't know. This is great. So I don't think the echo
[01:55:22.000 --> 01:55:26.240]   has great speakers. But I have done is I can use the Google home in the bedroom and you say things
[01:55:26.240 --> 01:55:32.320]   like play this in the kitchen. And then the music comes on in the kitchen. Oh, I like that.
[01:55:32.320 --> 01:55:38.000]   Yeah. So that's nice. I only have one. So I can't really test any of those things.
[01:55:38.000 --> 01:55:42.960]   Yeah. But I guess they can both play music at the same time. Chromecast audio was supposed to have
[01:55:42.960 --> 01:55:48.320]   that feature too, where it would see the trick is sync because if they're not perfectly synced,
[01:55:48.320 --> 01:55:51.520]   you know, if there's any latency at all, it sounds like you've got this big echo box.
[01:55:54.800 --> 01:56:02.640]   It's awkward. Awkward. Awkward to say the least. I was going to ask if you guys had, I have noticed
[01:56:02.640 --> 01:56:10.560]   that my Google home, when I ask it to play songs, it has given me some really random results lately.
[01:56:10.560 --> 01:56:11.760]   Do you not say you just,
[01:56:11.760 --> 01:56:15.280]   I go down and ask the same play music? You're not saying what kind of music.
[01:56:15.280 --> 01:56:24.480]   Don't judge me. I asked it to play. What was it? Oh, God. Oh, I asked it to play afterglow
[01:56:24.480 --> 01:56:29.120]   by churches. And it was like, I can't do that. But if I asked the echo to do it,
[01:56:29.120 --> 01:56:34.480]   it did it. And then I asked it to play like, I can't remember this. It's, it's a Usher song.
[01:56:34.480 --> 01:56:40.400]   I asked it to play. All right. I don't mind by Usher. And instead of playing me, the actual Usher song,
[01:56:40.400 --> 01:56:45.040]   it played me a karaoke version. Oh, this is a big problem. That's weird.
[01:56:45.040 --> 01:56:48.800]   This is one of the things the new Pandora music they said we're going to try to solve that.
[01:56:48.800 --> 01:56:53.920]   Everybody has access to roughly the same 40 million songs. The problem is there's a lot of karaoke,
[01:56:54.560 --> 01:57:00.640]   covers, you know, bad versions of songs. You'll say, you know, played the Beatles. Here comes the sun,
[01:57:00.640 --> 01:57:05.840]   and you'll get some Japanese cover band doing it. And so Pandora says we're going to try to
[01:57:05.840 --> 01:57:11.040]   eliminate those from our music. If they do that, that would be reason enough to pay for 10 bucks a
[01:57:11.040 --> 01:57:15.680]   month for Pandora's new streaming service. Yeah, that's just the problem.
[01:57:15.680 --> 01:57:20.160]   The echo has been great about giving me generally it gives me the right results.
[01:57:20.160 --> 01:57:26.880]   What music services it tied to Spotify for you? Spotify. Okay, so that just means Spotify is
[01:57:26.880 --> 01:57:34.000]   better than Google. Are you not using Spotify? No, I am using Spotify on Google. So it's the
[01:57:34.000 --> 01:57:38.880]   exact same service. That's weird. That's what's so whack. Oh, that's whack. Well, it's really whack.
[01:57:38.880 --> 01:57:48.720]   Afterglow can be hit or miss says Reverb Mike in our chat room. I'm sure he's talking about the
[01:57:48.720 --> 01:57:52.240]   song home assistant problems. Yes, don't you hate it?
[01:57:52.240 --> 01:58:01.920]   I saw a video that made me feel terrible on, I think it was on Facebook, where it's a bunch of
[01:58:01.920 --> 01:58:09.760]   poor Africans standing in front of their tiny little sad buildings are saying, I just need
[01:58:09.760 --> 01:58:14.240]   another Wi-Fi router. I can't get throughout my house or don't you hate it when your Google
[01:58:14.240 --> 01:58:18.960]   assistant just doesn't make you breakfast. It's like all these first world problems and it just
[01:58:18.960 --> 01:58:27.120]   makes you feel so, so, so bad. That's why I'm selling all my stuff. It's gonna all be for sale.
[01:58:27.120 --> 01:58:33.200]   I'm gonna take a break and when we come back, your picks of the week in the mood.
[01:58:33.200 --> 01:58:40.000]   Sounds good. That's something good. I'm game. Are you game? I think I do. I'm game. I can already
[01:58:40.000 --> 01:58:44.960]   did mine with the phone, but I'll see if I can find something else. I tipped my hand too early.
[01:58:44.960 --> 01:58:53.680]   I showed they brought to you by segment the data hub platform. Now, it's hard for me to
[01:58:53.680 --> 01:58:57.840]   describe what this is, but let me put it this way. If you've got a website or an app and you collect
[01:58:57.840 --> 01:59:05.280]   analytics, in most cases, like in our case, you might have multiple, you know, little JavaScript
[01:59:05.280 --> 01:59:10.800]   one liners in your website. We've got chart beat and quant cast and Google analytics and a bunch of
[01:59:10.800 --> 01:59:14.880]   different things, you know, for different analytic purposes or to measure how people use the site
[01:59:14.880 --> 01:59:20.240]   or get heat maps. And the problem is you get all this energy and all this interaction and you get
[01:59:20.240 --> 01:59:28.400]   all these bugs. So segment makes this completely simple. If think of it as plumbing, it's the data
[01:59:28.400 --> 01:59:33.600]   hub platform that collects and unifies and acts makes it possible for you to act in your customer
[01:59:33.600 --> 01:59:39.600]   data across all of your websites, your mobile apps, your cloud platforms. Instead of putting stuff,
[01:59:39.600 --> 01:59:45.920]   you put one bug on your page in your app on your cloud platform. And then you go to your dashboard
[01:59:45.920 --> 01:59:53.120]   at segment, segment.com/twit. And you say, okay, I want to add, and then you add Google analytics
[01:59:53.120 --> 01:59:58.800]   or optimisely or mix panel or whatever tools, hundreds of choices, you create the plumbing on
[01:59:58.800 --> 02:00:05.040]   their website. The other beauty part of this is the segment app, you know, tag will continue to
[02:00:05.040 --> 02:00:10.320]   collect data, even if one of these is down, you don't lose any bits of data. And when the site comes
[02:00:10.320 --> 02:00:16.400]   back up, segment will feed it the data that it missed. So there is one platform that labels
[02:00:16.400 --> 02:00:23.120]   writing integration code once. It's a single straightforward API that unifies your data,
[02:00:23.120 --> 02:00:28.960]   you're performing analytics on any platform from market to mail chimp. And that's just
[02:00:28.960 --> 02:00:35.200]   dams. Companies that use it, Atlassian to it, new Silicon Reuters, you can do advertising
[02:00:35.200 --> 02:00:40.080]   analytics, a be testing attribution and on and on and on. And really, there's so if you look at
[02:00:40.080 --> 02:00:45.840]   the site, there's so much stuff, I want you to go to segment.com/twit. Save time on engineering,
[02:00:45.840 --> 02:00:51.760]   save money on engineering with segment, a single developer can easily and integrate a new analytics
[02:00:51.760 --> 02:00:56.160]   tool for the data driven account manager and other for the app happy marketer and other for
[02:00:56.160 --> 02:01:01.200]   the data scientist who needs the flexibility of a single unified data warehouse. You can dig deeper
[02:01:01.200 --> 02:01:06.400]   in your data, bring data from any source, import historical data, customize what ends up in your
[02:01:06.400 --> 02:01:11.680]   data warehouse of choice. It is awesome customer data platform that both developers and analysts
[02:01:11.680 --> 02:01:17.760]   love because of its simple elegant API and that complete partner ecosystem that just page after
[02:01:17.760 --> 02:01:24.000]   page, it works with everything. Zendesk, intercom, app nexus, I can go on and on and on.
[02:01:24.000 --> 02:01:28.160]   Spend your time using your analytics data and acting on it, not struggling with custom
[02:01:28.160 --> 02:01:33.520]   integrations. Thanks to segment, your CEO will love your quickly deployed integrations.
[02:01:33.520 --> 02:01:38.160]   She might even give you a smooch. I love your quickly deployed data integrations.
[02:01:38.160 --> 02:01:43.280]   You'll also love as a CEO, you'll love how much money you save. 8,000 of the world's best companies
[02:01:43.280 --> 02:01:47.440]   use segment to drive growth and revenue. And now you can try the segment team plan
[02:01:47.440 --> 02:01:56.800]   free for three months, three months free at segment.com/twitsegment.com/twit.
[02:01:56.800 --> 02:02:06.240]   Simplify your life, simplify your engineering, simplify your analytics segment.com/twit.
[02:02:06.240 --> 02:02:13.200]   Danny Sullivan, search engine land is here. Stacey Higginbotham, Stacey on IOT is here. Jeff will be
[02:02:13.200 --> 02:02:19.200]   back next week. Why don't we let the guest start? Danny, your pick of the week.
[02:02:19.200 --> 02:02:27.440]   So I'm going to pick my own article that I did yesterday. It's your guide to Google Assistant
[02:02:27.440 --> 02:02:34.800]   and the Google search app. And if you have an Android phone, you're starting to get Google Assistant.
[02:02:34.800 --> 02:02:41.600]   I've seen it finally roll out to my Galaxy S7 and one of my Moto plays. So your phone,
[02:02:41.600 --> 02:02:46.480]   there you have it, but it's a big change. Probably the most dramatic change is going to be that if
[02:02:46.480 --> 02:02:51.200]   you're used to hitting the home button and then either speaking or typing your search,
[02:02:51.200 --> 02:02:56.320]   that's changed. Now all you can do is speak your search to the Google Assistant. And you may like
[02:02:56.320 --> 02:03:02.000]   that or you might find it very awkward. Google Assistant is sometimes very useful, giving you answers.
[02:03:02.000 --> 02:03:06.480]   And then sometimes it's kind of a pain, especially if you try to do the tasks that it said it was
[02:03:06.480 --> 02:03:11.360]   going to do way back when and it doesn't turn out to do some of that stuff as well. But it's
[02:03:11.360 --> 02:03:14.800]   definitely coming to your phone. And so if you're wondering what's happening with it,
[02:03:14.800 --> 02:03:18.560]   the guide is designed to explain to you where that's going, what's that's happening.
[02:03:18.560 --> 02:03:23.360]   And then I'm also kind of refreshing your memory about, well, what if you want to use the Google app?
[02:03:23.360 --> 02:03:29.760]   People may remember using Google now. Google now is not part of the Google Assistant. You actually
[02:03:29.760 --> 02:03:34.640]   have to have the Google app separately, either on iPhone or Android to get it. And oh, by the way,
[02:03:34.640 --> 02:03:39.360]   it's not called Google now anymore. It's just called predictive search card things.
[02:03:39.360 --> 02:03:43.920]   Oh, that's terrible. I hate them. I hate them.
[02:03:43.920 --> 02:03:49.680]   You're so much. The thing that Google had that they used to call Google now on tap,
[02:03:49.680 --> 02:03:55.200]   that screen search that still exists. And it still exists even if you have Google Assistant.
[02:03:55.200 --> 02:03:59.040]   In the past, you used to be on any screen. You could hold down the home button and it would
[02:03:59.040 --> 02:04:02.880]   tell you and things related to what you were looking at. You can still do that with Google
[02:04:02.880 --> 02:04:08.000]   Assistant. It just kind of involves a little bit of activating it and then swiping up. It can be
[02:04:08.000 --> 02:04:13.280]   a little tricky. And just a few other things in terms of how you can do your searching. It's like
[02:04:13.280 --> 02:04:18.560]   basically all ways to search using Google on either the Android or the iPhone in this new world
[02:04:18.560 --> 02:04:23.520]   of the Google Assistant being out there. I actually love the Google Assistant. I only had on the
[02:04:23.520 --> 02:04:28.240]   Pixel. To me, if you're going to get an Android phone, you want to make sure it's one that supports
[02:04:28.240 --> 02:04:31.760]   Google Assistant. As Google's basically said, though, it's going to be, if you can get NuGet,
[02:04:31.760 --> 02:04:36.720]   you're going to have Assistant, right? Eventually. Yeah. It's great. I mean, I was really pleased
[02:04:36.720 --> 02:04:43.040]   when I opened up this HTC Ultra. It was there. I was like, yay. Very happy.
[02:04:43.040 --> 02:04:49.040]   So how do you do now on tap? Because it's the same thing pressing and holding the home button.
[02:04:49.040 --> 02:04:53.440]   If you hold the home button and then after Google Assistant launches, you got to swipe up on the
[02:04:53.440 --> 02:05:01.200]   assistant itself. It's crazy. It's a little bit complicated. No one uses now on tap.
[02:05:01.200 --> 02:05:11.120]   So that's the key. You're not going to use it anyway. So you're not going to worry about it.
[02:05:11.120 --> 02:05:15.200]   Yeah. I think the biggest shift really will be for people who are used to holding the home button
[02:05:15.200 --> 02:05:19.040]   as a way of activating search and they would type. You just can't do that anymore.
[02:05:19.040 --> 02:05:24.400]   And figuring out whether Google now cards. That's the hardest thing for me is you got the Google
[02:05:24.400 --> 02:05:31.040]   Assistant, but then you got Google now and there's two separate areas. I kind of like them when they
[02:05:31.040 --> 02:05:36.080]   were united in the Google search app. But I imagine at some point the Google search app itself will
[02:05:36.080 --> 02:05:42.640]   get more Google Assistant-like. Yeah. What I just did is I put a Google button on my front page,
[02:05:42.640 --> 02:05:48.800]   which launches the now cards. Or if you use the pixel launcher, you could swipe left and the
[02:05:48.800 --> 02:05:56.160]   now cards are still there. I just don't use the pixel launcher. Oh, you guys, this is stressing
[02:05:56.160 --> 02:06:04.960]   me out. Oh, what kind of phone do you use? Stacy? I have the Nexus 5X. Oh, that's right. You have
[02:06:04.960 --> 02:06:10.400]   this conversation every time. Well, everybody has a different phone and I don't I should have a
[02:06:10.400 --> 02:06:15.840]   database. Oh, no, as what, but that would be normal. And you should keep it in your head.
[02:06:15.840 --> 02:06:22.480]   In normal people call my mind, but I have no mind. So I don't know. What is your pick of the weeks,
[02:06:22.480 --> 02:06:28.320]   Stacy? Oh, by the way, I should mention that article Danny mentioned is on search engineland.com.
[02:06:28.320 --> 02:06:33.440]   And you could Google as I did your guide to using Google Assistant on search engineland.com.
[02:06:33.440 --> 02:06:40.560]   And you'd find it very helpful. As always, you're great. Now, Stacy, your pick of the week.
[02:06:40.560 --> 02:06:47.200]   Now this is my pick. I actually installed this so I can't show it to you because now it's in a
[02:06:47.200 --> 02:06:57.680]   wall. But this is a show the website. So this is a Zigbee based Lutron Pico remote. And you don't
[02:06:57.680 --> 02:07:02.560]   have to have Lutron because this is just Pico. And the reason I'm showing this to you is because
[02:07:02.560 --> 02:07:09.600]   this will control anything that uses the Zigbee light link protocol. So that includes things like
[02:07:09.600 --> 02:07:16.000]   hue taps or sorry, hue taps, hue lights, Cree lights, GE link lights. I think the Oz Ram lights,
[02:07:16.000 --> 02:07:23.600]   although I haven't tried it. Now, as a caveat, the hue lights will, they don't support
[02:07:23.600 --> 02:07:30.880]   these non-hue accessories, but it does work. So this I am using to replace the hue,
[02:07:30.880 --> 02:07:36.800]   hue makes a dimmer switch that is hideously ugly. Whereas this is beautiful. And if you
[02:07:36.800 --> 02:07:45.520]   this is just the beautiful part because I showed the wall insert, which was not beautiful.
[02:07:45.920 --> 02:07:49.360]   But once you mount the whole thing, this is what you get, right?
[02:07:49.360 --> 02:07:56.720]   That's well, you get it with the lights, the the claro pad. So you have to buy all three of these
[02:07:56.720 --> 02:08:00.880]   things that Leo is showing you to get if you want to mount it on a switch. You put this in your dry
[02:08:00.880 --> 02:08:05.680]   wall. You just want to, yes. And then you put this over it. So if you were going to install this,
[02:08:05.680 --> 02:08:12.320]   yeah, that goes over it. So if you wanted to install this, and this is also good for if you've
[02:08:12.320 --> 02:08:17.200]   got hue lights and people keep turning on the light switch and off the light switch and you're
[02:08:17.200 --> 02:08:20.960]   like, darn it, guys, you can't touch the light switch anymore. I've stopped smart lighting.
[02:08:20.960 --> 02:08:22.320]   Leave it on. It's smart.
[02:08:22.320 --> 02:08:28.960]   This is a way. So for the 30 bucks for this whole setup, what you'll do is you'll take off your
[02:08:28.960 --> 02:08:33.760]   actual light switch, turn off your power before you do any of this, and then you just take some
[02:08:33.760 --> 02:08:41.280]   wire nuts and close off those wires. Or you'll uninstall your switch, close off those wires.
[02:08:41.280 --> 02:08:49.360]   And then you pop this guy in, and then now it is controlled. So it's less, yes, you still have to
[02:08:49.360 --> 02:08:54.160]   do a little bit of wiring, but it's not like crazy scary. Or if you don't want to do any of that,
[02:08:54.160 --> 02:08:57.520]   just duct tape over all of your lights, which is and just stick this next to it. But that's ugly.
[02:08:57.520 --> 02:09:06.240]   So where, how does, I don't, maybe you need to be an electrician probably to understand how to make
[02:09:06.240 --> 02:09:11.280]   this the wires that are going to come through one of these, like, does it, is that a cut out?
[02:09:11.280 --> 02:09:15.440]   Oh, I see this slides up that plastic. It holds this kind of thing like that. So it's a separate
[02:09:15.440 --> 02:09:23.840]   remote. Oh, I see. It doesn't. So it's not wired. It's just a whole, like, old stuff. I get it.
[02:09:23.840 --> 02:09:30.560]   Okay. That plate thing. See those screws on the top as you just screwed out where your original
[02:09:30.560 --> 02:09:34.560]   switch box was. Because when a switch box comes out, do I have one?
[02:09:34.560 --> 02:09:38.000]   You'll have little wires, but you don't have, you could just could cover those up.
[02:09:38.000 --> 02:09:44.000]   So here's a switch, right? Here's the back stuff. This is what's inside your wall.
[02:09:44.000 --> 02:09:50.080]   Hold it up a little higher. And this plate, a little higher. Yeah. And oh, I froze.
[02:09:50.080 --> 02:09:56.560]   So when you pull, and this isn't what you'll get there, you'll just get the front pace and none
[02:09:56.560 --> 02:10:02.880]   of this back stuff. Got it. Did that make sense? And then you put this in front of where all this
[02:10:02.880 --> 02:10:12.160]   used to live. Makes perfect sense to me, Ollie. Can't wait to try it. I don't have anything smart.
[02:10:12.160 --> 02:10:18.960]   But if you do try it, if you do try it, guys, turn the power off first.
[02:10:18.960 --> 02:10:29.760]   Not afterwards. The lutron. Turn it off first. The lutron things too. This is the one I have.
[02:10:29.760 --> 02:10:35.600]   It controls the lights in my office. But this isn't, I think it's a different model. But one of the
[02:10:35.600 --> 02:10:40.720]   things I like about the lutrons is if you buy like their $60 one, you can replace any of your
[02:10:40.720 --> 02:10:46.160]   switches for your regular lights. And it turns all your lights into smart lights. And then your
[02:10:46.160 --> 02:10:51.040]   Amazon Echo can control them or whatever. And so these are dumb lights over my head,
[02:10:51.040 --> 02:10:56.560]   but I have got a smart wireless switch on the wall. And then my Echo can control them. And it is
[02:10:56.560 --> 02:11:00.960]   awesome unless you ask my wife because she doesn't like how they flick when they first come on.
[02:11:00.960 --> 02:11:05.760]   But I think they're cool. I see. I don't have any of this stuff. Different light bulbs can help that.
[02:11:05.760 --> 02:11:13.280]   Oh, really? Oh, really? Yeah. CNET. Ry Chris, Ry Chris over at CNET actually has done
[02:11:13.280 --> 02:11:17.680]   a breakdown of which light bulbs work best with the lutron dimmers.
[02:11:17.680 --> 02:11:23.760]   Oh. So you can go check that out if you like. The other thing I should tell you about these,
[02:11:24.960 --> 02:11:32.720]   because I love lutron like you. I love lutron. You can actually get a lutron bridge and that can
[02:11:32.720 --> 02:11:43.280]   actually work in a dual phone home, a dual OS home with home kit and with like the Echo or a
[02:11:43.280 --> 02:11:51.120]   Wink Hub. So oh, oh, and I forgot. If you have a Wink Hub and you have this guy, you can actually
[02:11:51.120 --> 02:11:54.880]   program it to control your lights or you can leave it on the table and program it to do shortcuts.
[02:11:54.880 --> 02:11:59.680]   So then you could be like, "Movie night!" But that's neither here nor there.
[02:11:59.680 --> 02:12:04.880]   I still just walk over to the switch on the wall and turn it on and off. I'm very boring, I think.
[02:12:04.880 --> 02:12:13.520]   Too much work. You know, if once you tie your lights to your Echo, I feel like you will never go
[02:12:13.520 --> 02:12:18.240]   back. I really do. And like, even, yeah. Really?
[02:12:21.440 --> 02:12:27.040]   I installed all these wireless things in the house and it was like, "I just want to use the switch."
[02:12:27.040 --> 02:12:33.360]   I just, and now everybody's like, "A word, turn on the lights here. A word, turn on the lights there."
[02:12:33.360 --> 02:12:38.640]   No, you know, everyone's used to it and they like it. To the degree that when we walk into a hotel,
[02:12:38.640 --> 02:12:42.480]   people are like, "A word, turn on the lights." Yeah, you're spoiled, huh? Yeah.
[02:12:42.480 --> 02:12:47.920]   I mean, I know a guy who has echoes in every room. So I certainly have that part down.
[02:12:48.880 --> 02:12:54.960]   I just need to, so the best way to do this, I think, would be to do what you did, Danny, is
[02:12:54.960 --> 02:12:59.040]   because I have some hues, but I don't want to put hues in everywhere. I just replaced the switches.
[02:12:59.040 --> 02:13:03.520]   No, no. Do the Lutron's. Do the Lutron's switch. The $65 switch. Okay.
[02:13:03.520 --> 02:13:08.080]   It's a $65 switch on the room. Set of lights, though, throughout the house.
[02:13:08.080 --> 02:13:12.640]   Every set of light. Every square, there's a switch. And if you have,
[02:13:13.920 --> 02:13:19.040]   where there's a switch now, but, but. What if I have two switches? If you have a three-way light,
[02:13:19.040 --> 02:13:24.480]   a three-way. If you have a three-way situation, which is two switches,
[02:13:24.480 --> 02:13:29.920]   you buy a kit that has basically a Pico remote, what I just showed you. It has exactly that
[02:13:29.920 --> 02:13:35.360]   all sold as one plus a dimmer switch. And then you put the actual switch in,
[02:13:35.360 --> 02:13:41.200]   for one, I did this in my kitchen. And you're going to wire those up. And then you're going to take
[02:13:41.200 --> 02:13:47.520]   off your existing switch, cap off the wires inside with a wire nut, and then put that
[02:13:47.520 --> 02:13:51.280]   Pico remote and plate on it. Okay. That's what it is. And then you program that.
[02:13:51.280 --> 02:13:56.160]   Almost all of our lights have dual switches, which is so, so one of them would have a real
[02:13:56.160 --> 02:14:02.000]   switch that would make the Lutites happy. And then the other one would be the one you talk to.
[02:14:02.000 --> 02:14:07.600]   And you won't even know because the one's the hardwired switch and the other one's the remote,
[02:14:07.600 --> 02:14:11.680]   but it's on the wall. So it just looks like a hardwired switch once you mount it on the wall.
[02:14:11.680 --> 02:14:15.440]   Right. It doesn't do anything though, right? That's going to frustrate people because you're
[02:14:15.440 --> 02:14:20.800]   going to walk up to it and tap it. No, it'll do things. It works. That's why it's beautiful.
[02:14:20.800 --> 02:14:22.800]   Ah, it still works. It works. It works. Okay.
[02:14:22.800 --> 02:14:28.480]   Yes. And I should also tell you, because we talked about the Lutron makes a hub,
[02:14:28.480 --> 02:14:34.640]   but it also works with the Wink Hub. Wink Hub is, and SmartThings just did a Lutron integration,
[02:14:34.640 --> 02:14:37.920]   although I haven't gotten mine to work yet. So I don't know if that's enough.
[02:14:37.920 --> 02:14:42.000]   I have generation two SmartThings Hub, so I could use that if I got it all working.
[02:14:42.000 --> 02:14:46.560]   Yeah. No, actually for SmartThings, it's a cloud to cloud. So you need the Lutron Bridge.
[02:14:46.560 --> 02:14:53.120]   No, it's the SmartThings. You need the Lutron Bridge to work with the SmartThings Hub. Sorry.
[02:14:53.120 --> 02:14:55.840]   That's sad it's been hitting my head on the microphone.
[02:14:55.840 --> 02:15:01.360]   Okay. I know. You know what? If you want, you're going to come to my house and do this for me.
[02:15:01.360 --> 02:15:06.800]   I can send you my, I totally could, but I could send you my version one of the Wink Hub,
[02:15:06.800 --> 02:15:10.000]   and then you'll have the Lutron radio. It basically becomes a Lutron.
[02:15:10.000 --> 02:15:13.440]   No, I should, I don't mind buying all this stuff. I should and do it.
[02:15:13.440 --> 02:15:18.080]   Well, and there's a kit you can get that'll be a switch, the Lutron hub,
[02:15:18.080 --> 02:15:22.320]   and you're set to wire everything up and get one of your bank's lights going.
[02:15:22.320 --> 02:15:25.200]   It's very easy. And I even wired them myself.
[02:15:25.200 --> 02:15:29.360]   So is that the, is that the Cassetta wireless smart lighting kit?
[02:15:30.800 --> 02:15:34.000]   Probably. It's got one smart bridge, two in Waltons.
[02:15:34.000 --> 02:15:35.600]   Yes. Yes. There you go. Two people.
[02:15:35.600 --> 02:15:40.240]   It's two tabletop pedestals, works with Echo. $189.
[02:15:40.240 --> 02:15:44.400]   Well, you, yeah. Oh, yeah. No, that's good though, because you're getting a bridge,
[02:15:44.400 --> 02:15:48.320]   you're getting two that's 60 and 60 for each of your switches, and you're getting the bridge,
[02:15:48.320 --> 02:15:49.920]   which is a $60 bridge. Okay.
[02:15:49.920 --> 02:15:51.520]   Do I build a fight a cheaper and something?
[02:15:51.520 --> 02:15:54.080]   It's an $80 bridge. Oh, so, yeah. Yeah.
[02:15:54.080 --> 02:15:56.160]   And the, you do, you need the bridge, right?
[02:15:56.160 --> 02:15:58.880]   Yes. Is that how you talk?
[02:15:58.880 --> 02:16:00.640]   You need a bridge or a Wink Hub.
[02:16:00.640 --> 02:16:03.040]   Okay. I could use their bridge or the Wink one.
[02:16:03.040 --> 02:16:06.240]   And you said use the Wink, but if I'm going to buy this kit, I'd have the bridge.
[02:16:06.240 --> 02:16:10.560]   Yeah. Yeah. I mean, if you, so only with the Lutron bridge,
[02:16:10.560 --> 02:16:16.000]   can I sleep on your couch or my wife divorces me for doing all this to the house?
[02:16:16.000 --> 02:16:22.000]   Believe me. So, actually, this is, she'll be sleeping on, she'll.
[02:16:22.000 --> 02:16:23.040]   The most popular thing.
[02:16:23.040 --> 02:16:25.920]   This is the most popular thing.
[02:16:25.920 --> 02:16:29.920]   It is the most popular thing. It is the most popular thing I've ever done in my home.
[02:16:29.920 --> 02:16:30.560]   Really?
[02:16:30.560 --> 02:16:33.600]   My family. And actually, I'm actually about to, okay,
[02:16:33.600 --> 02:16:37.360]   so there's actually things to know before we tell people, and I don't know if this is too much
[02:16:37.360 --> 02:16:41.920]   for y'all, but Lutron only works with incandescent lights.
[02:16:41.920 --> 02:16:42.640]   Uh-oh.
[02:16:42.640 --> 02:16:43.840]   Dimmable LEDs.
[02:16:43.840 --> 02:16:44.160]   Yep.
[02:16:44.160 --> 02:16:46.640]   It, well, the, because these are dimmers.
[02:16:46.640 --> 02:16:47.120]   Right.
[02:16:47.120 --> 02:16:48.320]   So, you have to make sure.
[02:16:48.320 --> 02:16:49.120]   You have a dimmable LED.
[02:16:49.120 --> 02:16:50.560]   They're dimmable lights that you're working on.
[02:16:50.560 --> 02:16:51.760]   Some LEDs are not dimmable.
[02:16:51.760 --> 02:16:54.560]   So, don't put them with like a CFL or something fluorescent.
[02:16:54.560 --> 02:16:55.760]   Good. I hate CFLs anyway.
[02:16:55.760 --> 02:16:56.480]   But, yeah.
[02:16:56.480 --> 02:16:59.520]   They make a switch that's just on off.
[02:16:59.520 --> 02:17:00.240]   You could do with those.
[02:17:00.240 --> 02:17:04.240]   Because I actually am about to buy one of those to stick in my, in, with some fluorescent lights.
[02:17:04.240 --> 02:17:07.760]   So, this wouldn't affect turn everything into a dimmer too, which is nice.
[02:17:07.760 --> 02:17:11.040]   But, I'm going to need, so this does,
[02:17:11.040 --> 02:17:14.560]   this would do two switches, right?
[02:17:14.560 --> 02:17:16.640]   Because I get two remotes and two smart dimmers.
[02:17:16.640 --> 02:17:20.320]   You might also, Leo, when you look at it, I think I bought the,
[02:17:20.320 --> 02:17:24.640]   there's another thing you can get that's the wireless kit that is this,
[02:17:25.200 --> 02:17:29.840]   it's one switch and it's one bridge and that's $100 on Amazon.
[02:17:29.840 --> 02:17:35.920]   And then I think you can buy just the, another switch for one of your things that's like 60.
[02:17:35.920 --> 02:17:37.760]   So, there you go. You'll save like 60 bucks.
[02:17:37.760 --> 02:17:39.440]   Oh boy.
[02:17:39.440 --> 02:17:42.080]   Okay. Some day when I have some time.
[02:17:42.080 --> 02:17:46.480]   And people, so there's a lot of people in the chatroom talking about it being a proprietary
[02:17:46.480 --> 02:17:49.440]   radio protocol and it is.
[02:17:49.440 --> 02:17:50.080]   Yeah.
[02:17:50.080 --> 02:17:53.360]   But, like Sonos, which uses their proprietary.
[02:17:53.360 --> 02:17:53.840]   Yeah. What is it?
[02:17:53.840 --> 02:17:55.040]   It's a clear connect.
[02:17:55.040 --> 02:17:55.280]   Right.
[02:17:55.280 --> 02:18:02.320]   But it's, it's such a good one and your lights will work in pairing things with
[02:18:02.320 --> 02:18:07.920]   lutron is so easy. I mean, believe me, I play with this stuff all day long and.
[02:18:07.920 --> 02:18:10.160]   Okay. I love them.
[02:18:10.160 --> 02:18:10.800]   I'm kind of.
[02:18:10.800 --> 02:18:13.360]   And if you got a home kit, it's all on, on your phone.
[02:18:13.360 --> 02:18:14.320]   Right. I do.
[02:18:14.320 --> 02:18:18.240]   And then we have, we already have all the echoes and I, yeah.
[02:18:18.240 --> 02:18:21.680]   All right. So I'm, I, because I didn't even put the hues back in.
[02:18:21.680 --> 02:18:24.720]   It was like, I don't really need the RGB thing I don't care about.
[02:18:24.720 --> 02:18:25.040]   Yeah.
[02:18:25.040 --> 02:18:28.720]   I just, it would nice though to be able to say, turn on the kitchen lights.
[02:18:28.720 --> 02:18:32.400]   And you could also have a command right to turn all the lights in the house off.
[02:18:32.400 --> 02:18:32.640]   Right.
[02:18:32.640 --> 02:18:34.880]   Oh, Leo.
[02:18:34.880 --> 02:18:35.440]   Yes.
[02:18:35.440 --> 02:18:36.080]   Yeah.
[02:18:36.080 --> 02:18:39.120]   I just set up the best command.
[02:18:39.120 --> 02:18:40.480]   If you've got stringify.
[02:18:40.480 --> 02:18:40.880]   Yeah.
[02:18:40.880 --> 02:18:42.560]   This is, this is great.
[02:18:42.560 --> 02:18:48.080]   So I say, a she who shall not be named, tell stringify good night.
[02:18:48.080 --> 02:18:54.160]   And what it does is it turns off my luchron light in the bedroom, master bedroom lights.
[02:18:54.160 --> 02:18:58.720]   It turns off a, a wemo light that my husband's lamp is plugged into.
[02:18:58.720 --> 02:19:02.880]   It turns off the hue light bulb that might, that is in my actual lamp.
[02:19:02.880 --> 02:19:09.920]   It sets my thermostat down to 70 something degrees and make sure my blinds are closed.
[02:19:09.920 --> 02:19:11.120]   And it's all one command.
[02:19:11.120 --> 02:19:11.520]   Wow.
[02:19:11.520 --> 02:19:14.080]   It's amazing.
[02:19:14.080 --> 02:19:14.560]   Okay.
[02:19:14.560 --> 02:19:15.040]   That's it.
[02:19:15.040 --> 02:19:16.320]   Can I turn on the, the attack?
[02:19:16.320 --> 02:19:17.120]   So you should buy this.
[02:19:17.120 --> 02:19:17.680]   It's amazing.
[02:19:17.680 --> 02:19:19.440]   Let me ask you one more question.
[02:19:19.440 --> 02:19:25.040]   And that is which thermostat do you recommend?
[02:19:25.040 --> 02:19:26.000]   Which one do you use?
[02:19:26.000 --> 02:19:27.200]   Do you use a nest?
[02:19:27.200 --> 02:19:30.640]   I have a nest and I have an ecobi.
[02:19:30.640 --> 02:19:37.040]   I'm tempted because here's why I want the ecobi because I want one with remote thermometer mesh
[02:19:37.040 --> 02:19:41.840]   because, because the thermostats in the hall, but, but it overheats our bedroom because the
[02:19:41.840 --> 02:19:43.440]   hall is colder than the bedroom.
[02:19:43.440 --> 02:19:46.880]   So I want to have a little thermostat, a little thermometer in the bedroom.
[02:19:47.520 --> 02:19:48.080]   That's ecobi.
[02:19:48.080 --> 02:19:51.280]   The speculation is that, yeah, that's ecobi.
[02:19:51.280 --> 02:19:54.480]   There is speculation that Google's about to launch that though.
[02:19:54.480 --> 02:19:55.760]   Just so you know.
[02:19:55.760 --> 02:19:59.280]   And what, and Danny raised his hand.
[02:19:59.280 --> 02:19:59.680]   I have a sensey.
[02:19:59.680 --> 02:20:00.960]   A sensey.
[02:20:00.960 --> 02:20:03.040]   That's on the wall over there.
[02:20:03.040 --> 02:20:03.600]   Yeah.
[02:20:03.600 --> 02:20:07.520]   Thanks to the good folks at Wirecutter.
[02:20:07.520 --> 02:20:10.880]   It was a recommended thing and it's about $60 and it's,
[02:20:10.880 --> 02:20:14.560]   you know, I have a nest downstairs, but I have
[02:20:14.560 --> 02:20:17.600]   senseys on the two thermostats and two different rooms upstairs and they,
[02:20:17.600 --> 02:20:18.720]   they're great.
[02:20:18.720 --> 02:20:23.840]   Just, I mean, for the three times a year, I have to turn on the, the heating in the house, right?
[02:20:23.840 --> 02:20:28.720]   Or the air conditioning, but you know, you've got remote access to them and you can get them that way as well.
[02:20:28.720 --> 02:20:31.360]   Senseys spelled S-E-N.
[02:20:31.360 --> 02:20:33.200]   S-N-S-I.
[02:20:33.200 --> 02:20:37.520]   S-N-S-I, of course, because why should it be easy?
[02:20:37.520 --> 02:20:38.640]   I know.
[02:20:38.640 --> 02:20:40.800]   The best smart there.
[02:20:40.800 --> 02:20:45.440]   Well, you know what? Wirecutter now says the nest is the best smart thermostat.
[02:20:45.440 --> 02:20:52.480]   So, this is a sensey work with Amazon Echo.
[02:20:52.480 --> 02:20:55.200]   I think it does.
[02:20:55.200 --> 02:20:56.000]   That's how you will.
[02:20:56.000 --> 02:20:58.800]   Tea just haven't gotten around to setting it up.
[02:20:58.800 --> 02:21:01.040]   Emerson makes a sensey.
[02:21:01.040 --> 02:21:01.760]   How interesting.
[02:21:01.760 --> 02:21:02.080]   I don't know.
[02:21:02.080 --> 02:21:02.880]   They were still on.
[02:21:02.880 --> 02:21:05.280]   All right.
[02:21:05.280 --> 02:21:05.520]   Yeah.
[02:21:05.520 --> 02:21:07.200]   The Wirecutter is a good, a great resource.
[02:21:07.200 --> 02:21:07.760]   I have to say.
[02:21:08.960 --> 02:21:11.120]   They now are saying the nest is the best.
[02:21:11.120 --> 02:21:15.600]   But they did talk about the sensey.
[02:21:15.600 --> 02:21:16.560]   Yeah.
[02:21:16.560 --> 02:21:17.360]   All right.
[02:21:17.360 --> 02:21:20.640]   I think it depends on your smart thermostat needs too, right?
[02:21:20.640 --> 02:21:24.880]   For me, like the nest has all that fancy stuff, all that all these things.
[02:21:24.880 --> 02:21:26.480]   You don't need that because you would have orange county.
[02:21:26.480 --> 02:21:27.040]   It doesn't ever.
[02:21:27.040 --> 02:21:27.360]   Yeah.
[02:21:27.360 --> 02:21:32.400]   It's like two times a year, I turn on the air conditioning and three times a year,
[02:21:32.400 --> 02:21:33.360]   I turn on the heating.
[02:21:33.360 --> 02:21:36.320]   It's mainly so I don't have to get off the couch.
[02:21:36.320 --> 02:21:36.800]   Right.
[02:21:36.800 --> 02:21:40.720]   You know, if I don't get off the couch, I will gain even more weight.
[02:21:40.720 --> 02:21:45.280]   So I better probably not put any of this stuff in Stacy covers all this.
[02:21:45.280 --> 02:21:46.160]   You need a bit of charge too.
[02:21:46.160 --> 02:21:47.440]   I have that.
[02:21:47.440 --> 02:21:51.520]   Stacy covers all this on her podcast Stacy on IOT.
[02:21:51.520 --> 02:21:53.680]   You can find it at IOTpodcast.com.
[02:21:53.680 --> 02:21:54.960]   Show she does with her.
[02:21:54.960 --> 02:21:57.520]   Great friend Kevin Tofel, please give him my regards.
[02:21:57.520 --> 02:22:06.560]   And you can also visit her here every Wednesday for this week in Google or tweet her at gigastacy.
[02:22:07.520 --> 02:22:14.400]   The great Danny Sullivan search engine land.com marketing engine land.com at Danny Sullivan on the
[02:22:14.400 --> 02:22:16.320]   Twitter anything else you want to plug?
[02:22:16.320 --> 02:22:19.440]   No, that's it.
[02:22:19.440 --> 02:22:23.200]   We have our big martech show that will be coming up.
[02:22:23.200 --> 02:22:27.920]   So if you're interested in marketing technology, check that out and coming up in May and we'll
[02:22:27.920 --> 02:22:28.320]   find that.
[02:22:28.320 --> 02:22:31.200]   We have a third site now called martech today.
[02:22:31.200 --> 02:22:32.800]   It says all about marketing technology.
[02:22:32.800 --> 02:22:33.600]   Nice.
[02:22:33.600 --> 02:22:35.600]   Remember the family.
[02:22:35.600 --> 02:22:37.200]   You are just killing it.
[02:22:37.200 --> 02:22:39.120]   Keeps us busy.
[02:22:39.120 --> 02:22:40.800]   Great to talk to you.
[02:22:40.800 --> 02:22:42.240]   Great being here.
[02:22:42.240 --> 02:22:44.880]   Thank you all for being here.
[02:22:44.880 --> 02:22:48.960]   We do but this week in Google, which is as you can see this week in whatever the hell we want
[02:22:48.960 --> 02:22:55.520]   to talk about every Wednesday, 130 Pacific, 430 Eastern time, 20, 30 UTC.
[02:22:55.520 --> 02:22:56.720]   Yes, we're in summertime now.
[02:22:56.720 --> 02:22:58.560]   So it's 20, 30 UTC.
[02:22:58.560 --> 02:22:59.360]   Please stop by.
[02:22:59.360 --> 02:23:01.360]   Join us live if you can.
[02:23:01.360 --> 02:23:02.480]   Join us in the chat room.
[02:23:02.480 --> 02:23:07.280]   If you can't, you can always get on demand versions after the fact that our website
[02:23:07.280 --> 02:23:10.320]   Twitter TV slash twig or wherever you get your podcasts.
[02:23:10.320 --> 02:23:11.680]   But do subscribe.
[02:23:11.680 --> 02:23:13.120]   We don't want you to miss an episode.
[02:23:13.120 --> 02:23:15.760]   Thanks for being here and we'll see you next time on this weekend.
[02:23:15.760 --> 02:23:26.240]   Okay.

