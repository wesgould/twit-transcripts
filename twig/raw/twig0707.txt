;FFMETADATA1
title=Heartlessness As a Service
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Mike Elgan
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-03-16
track=707
language=English
genre=Podcast
comment=GPT-4, T-Mobile buys Mint Mobile, Biden's TikTok demand, Gowalla returns
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100

[00:00:00.000 --> 00:00:03.700]   It's time for Twig this week in Google Stasis here. Jeff's here.
[00:00:03.700 --> 00:00:06.900]   And it's got the week off, but great news. Mike Elgin is filling in.
[00:00:06.900 --> 00:00:09.300]   And we're going to talk about chat, GPT-4.
[00:00:09.300 --> 00:00:13.900]   It just came out and already amazing hundreds of applications.
[00:00:13.900 --> 00:00:19.000]   Is this the Cambrian explosion we've been waiting for? Maybe it is.
[00:00:19.000 --> 00:00:22.200]   Big layoffs in the tech industry in general and
[00:00:22.200 --> 00:00:26.400]   at Facebook specifically. Plus at the very end of the show
[00:00:26.400 --> 00:00:31.800]   Big time breaking news about TikTok all that and more coming up next on Twig.
[00:00:31.800 --> 00:00:37.800]   Podcasts you love from people you trust.
[00:00:37.800 --> 00:00:40.800]   This is Twig.
[00:00:40.800 --> 00:00:53.800]   This is Twig. This week in Google, episode 707, recorded Wednesday, March 15th, 2023.
[00:00:53.800 --> 00:00:56.600]   Heartlessness as a service.
[00:00:56.600 --> 00:01:00.300]   This week in Google is brought to you by ACI learning.
[00:01:00.300 --> 00:01:06.000]   Tech is one industry where opportunities outpace growth, especially in cybersecurity.
[00:01:06.000 --> 00:01:11.000]   One third of information security jobs require a cybersecurity certification.
[00:01:11.000 --> 00:01:16.200]   To maintain your competitive edge across audit, IT and cybersecurity readiness,
[00:01:16.200 --> 00:01:20.600]   visit go.acilurning.com/twit.
[00:01:20.600 --> 00:01:23.000]   And by Ate Sleep.
[00:01:23.000 --> 00:01:28.400]   Good sleep is the ultimate game changer and the pod cover is the ultimate sleep machine.
[00:01:28.400 --> 00:01:35.000]   Go to Ate Sleep.com/twit to check out the pod cover and say $150 at checkout.
[00:01:35.000 --> 00:01:38.800]   Ate Sleep currently ships within the US, Canada, the UK.
[00:01:38.800 --> 00:01:42.800]   So let countries in the EU and Australia.
[00:01:42.800 --> 00:01:46.400]   Thanks for listening to this show as an ad supported network.
[00:01:46.400 --> 00:01:50.000]   We are always looking for new partners with products and services
[00:01:50.000 --> 00:01:53.200]   that will benefit our qualified audience.
[00:01:53.200 --> 00:01:54.800]   Are you ready to grow your business?
[00:01:54.800 --> 00:02:00.400]   Reach out to advertise at Twit.tv and launch your campaign now.
[00:02:00.400 --> 00:02:02.000]   It's time for Twig this week at Google.
[00:02:02.000 --> 00:02:06.400]   This is a show I am very nervous about.
[00:02:06.400 --> 00:02:10.400]   First, let me introduce the hosts and I'll explain.
[00:02:10.400 --> 00:02:11.800]   Stacey Higginbotham is here.
[00:02:11.800 --> 00:02:15.200]   As always, Stacey on IoT.com at gigastacey.
[00:02:15.200 --> 00:02:17.000]   Hello.
[00:02:17.000 --> 00:02:17.600]   Hello.
[00:02:17.600 --> 00:02:19.000]   I am not an AI.
[00:02:19.000 --> 00:02:23.000]   Yes, that's what's making you new, but I was being nervous about it.
[00:02:23.000 --> 00:02:25.000]   You got nervous about it too, I think.
[00:02:25.000 --> 00:02:27.000]   Yeah, okay.
[00:02:27.000 --> 00:02:28.000]   Jeff Jarvis is here.
[00:02:28.000 --> 00:02:30.000]   If anybody's reassuring.
[00:02:30.000 --> 00:02:33.000]   It's not Jeff.
[00:02:33.000 --> 00:02:35.000]   Jeff is...
[00:02:35.000 --> 00:02:36.000]   Let me take...
[00:02:36.000 --> 00:02:38.000]   I've like giggled like ten times during the week.
[00:02:38.000 --> 00:02:39.000]   That was so funny.
[00:02:39.000 --> 00:02:40.000]   What's that spot?
[00:02:40.000 --> 00:02:41.000]   That was so funny.
[00:02:41.000 --> 00:02:42.000]   That was so funny.
[00:02:42.000 --> 00:02:43.000]   And Stacey doing that last week.
[00:02:43.000 --> 00:02:44.000]   I got so much good feedback.
[00:02:44.000 --> 00:02:48.000]   We're going to from now on we're going to have more spontaneity in this show.
[00:02:48.000 --> 00:02:50.000]   So I'll give you a spontaneous moment then.
[00:02:50.000 --> 00:02:55.000]   So I went in to use chat GPT for today is my favorite moment of the day.
[00:02:55.000 --> 00:02:59.000]   And it put a caption in front of me to prove that I'm human.
[00:02:59.000 --> 00:03:00.000]   Isn't that hysterical?
[00:03:00.000 --> 00:03:01.000]   It's hilarious.
[00:03:01.000 --> 00:03:03.000]   Nobody's allowed to use the bot.
[00:03:03.000 --> 00:03:07.000]   Jeff is the Leonard Taill professor for journalistic innovation at the...
[00:03:07.000 --> 00:03:09.000]   (choir singing)
[00:03:09.000 --> 00:03:10.000]   Great new mark.
[00:03:10.000 --> 00:03:13.000]   Graduate School of Journalism at the City University of New York.
[00:03:13.000 --> 00:03:15.000]   Hello, boss.
[00:03:15.000 --> 00:03:16.000]   Mm-mm.
[00:03:16.000 --> 00:03:18.000]   Ant has the week off.
[00:03:18.000 --> 00:03:22.000]   Smart Man took a vacation during this trying of times.
[00:03:22.000 --> 00:03:23.000]   But guess who's here?
[00:03:23.000 --> 00:03:24.000]   Mike Elgin is great to see him.
[00:03:24.000 --> 00:03:26.000]   And he's in the country of all things.
[00:03:26.000 --> 00:03:27.000]   That's right.
[00:03:27.000 --> 00:03:30.000]   The United States of America country.
[00:03:30.000 --> 00:03:32.000]   That means his internet is always good.
[00:03:32.000 --> 00:03:33.000]   But...
[00:03:33.000 --> 00:03:34.000]   This country.
[00:03:34.000 --> 00:03:35.000]   This country.
[00:03:35.000 --> 00:03:36.000]   And I might be in AI.
[00:03:36.000 --> 00:03:38.000]   It's not clear at this point.
[00:03:38.000 --> 00:03:39.000]   I'm pretty sure I'm not.
[00:03:39.000 --> 00:03:41.000]   You've written a lot about this.
[00:03:41.000 --> 00:03:44.000]   I was really glad we got you on.
[00:03:44.000 --> 00:03:46.000]   Yeah.
[00:03:46.000 --> 00:03:49.000]   Actually, who among us has not written a lot about it.
[00:03:49.000 --> 00:03:52.000]   Before we go too far, I do want to show you Stacy.
[00:03:52.000 --> 00:03:56.000]   The beautiful thing I have in front of me, which you might recognize.
[00:03:56.000 --> 00:03:58.000]   Is that the Legos succulents collection?
[00:03:58.000 --> 00:04:00.000]   The Legos succulents collection.
[00:04:00.000 --> 00:04:03.000]   Look, I'm like, I even know it.
[00:04:03.000 --> 00:04:06.000]   She recognized it right out of the box.
[00:04:06.000 --> 00:04:09.000]   We don't have a camera to zoom in on it, but it is...
[00:04:09.000 --> 00:04:11.000]   Would those be "brickulents"?
[00:04:11.000 --> 00:04:12.000]   (laughter)
[00:04:12.000 --> 00:04:13.000]   "brickulents"?
[00:04:13.000 --> 00:04:15.000]   Was it Kevin's king who had these?
[00:04:15.000 --> 00:04:22.000]   Yeah, Kevin, somebody as a collection, you showed us last week, you showed us your orchid,
[00:04:22.000 --> 00:04:23.000]   I think it was, right?
[00:04:23.000 --> 00:04:25.000]   Yes, and I have a bonsal.
[00:04:25.000 --> 00:04:26.000]   Yeah.
[00:04:26.000 --> 00:04:27.000]   And the bonsal.
[00:04:27.000 --> 00:04:28.000]   And there's the succulents.
[00:04:28.000 --> 00:04:29.000]   They're really pretty.
[00:04:29.000 --> 00:04:30.000]   They're beautiful.
[00:04:30.000 --> 00:04:31.000]   They don't even look like Lego.
[00:04:31.000 --> 00:04:32.000]   They are.
[00:04:32.000 --> 00:04:34.000]   They're doing a lot more plants these days.
[00:04:34.000 --> 00:04:37.000]   That's because people can't keep a plant alive.
[00:04:37.000 --> 00:04:40.000]   But during COVID, we were all like, we are sad.
[00:04:40.000 --> 00:04:41.000]   This makes me happy.
[00:04:41.000 --> 00:04:43.000]   We have a lot of liquid things.
[00:04:43.000 --> 00:04:44.000]   Yeah.
[00:04:44.000 --> 00:04:45.000]   It is.
[00:04:45.000 --> 00:04:46.000]   That's fun.
[00:04:46.000 --> 00:04:50.000]   I think there's something that's not pure Lego about it though.
[00:04:50.000 --> 00:04:51.000]   It's the fact that they're like...
[00:04:51.000 --> 00:04:54.000]   One matter of to make things from the real Lego shapes, when they have to make new things
[00:04:54.000 --> 00:04:57.000]   to make certain things, it just kind of gets like these pure.
[00:04:57.000 --> 00:05:03.000]   You're like those people who just get mad when Lego makes like your castle bricks that are the archways
[00:05:03.000 --> 00:05:04.000]   that you don't make.
[00:05:04.000 --> 00:05:05.000]   Like, this is not a new work.
[00:05:05.000 --> 00:05:07.000]   This is mad they haven't made the Gutenberg press.
[00:05:07.000 --> 00:05:08.000]   That's what it's really about.
[00:05:08.000 --> 00:05:09.000]   Okay.
[00:05:09.000 --> 00:05:10.000]   Oh, they will.
[00:05:10.000 --> 00:05:16.000]   Actually, Jeff, my granddaughter has a Harry Potter set, a Lego set.
[00:05:16.000 --> 00:05:20.000]   And it folds out into a whole Hogwarts thing with all the characters and everything.
[00:05:20.000 --> 00:05:23.000]   But when you close it, it's a book.
[00:05:23.000 --> 00:05:27.000]   The whole thing closes into like a leather bound magic book.
[00:05:27.000 --> 00:05:28.000]   It's so elegant.
[00:05:28.000 --> 00:05:30.000]   Oh, the two are never going to order them.
[00:05:30.000 --> 00:05:31.000]   They're flash.
[00:05:31.000 --> 00:05:34.000]   No, I'm not allowed to support Harry Potter anymore.
[00:05:34.000 --> 00:05:36.000]   Okay, that's true.
[00:05:36.000 --> 00:05:39.000]   John, Ashley, thank you for bringing these in.
[00:05:39.000 --> 00:05:43.000]   It really made it adds something to the set.
[00:05:43.000 --> 00:05:45.000]   Very nice.
[00:05:45.000 --> 00:05:48.000]   All right, that's enough of that.
[00:05:48.000 --> 00:05:51.000]   Enough of nature, if it were.
[00:05:51.000 --> 00:05:55.000]   Let's actually the Lego succulents are to nature.
[00:05:55.000 --> 00:05:59.000]   What chat GPT is to humanity.
[00:05:59.000 --> 00:06:00.000]   I would submit.
[00:06:00.000 --> 00:06:01.000]   Okay.
[00:06:01.000 --> 00:06:02.000]   Okay.
[00:06:02.000 --> 00:06:05.000]   Expensive, nice to look at and plast it.
[00:06:05.000 --> 00:06:07.000]   And not real.
[00:06:07.000 --> 00:06:08.000]   Yeah.
[00:06:08.000 --> 00:06:12.000]   They know people don't know how to use it very well.
[00:06:12.000 --> 00:06:13.000]   Yeah.
[00:06:13.000 --> 00:06:19.000]   Open AI announced chat GPT four yesterday.
[00:06:19.000 --> 00:06:22.000]   Within hours, there were 500 new startups.
[00:06:22.000 --> 00:06:27.000]   It's kind of mind boggling.
[00:06:27.000 --> 00:06:32.000]   Kind of like when chat GPT three came out.
[00:06:32.000 --> 00:06:40.000]   People tried all sorts of things.
[00:06:40.000 --> 00:06:47.000]   Twitter was a great place to see a Pong game designed by chat GPT from scratch.
[00:06:47.000 --> 00:06:49.000]   A working Pong game.
[00:06:49.000 --> 00:06:52.000]   Somebody drew a sketch on a napkin of a website.
[00:06:52.000 --> 00:06:54.000]   That's one of the things the new one does.
[00:06:54.000 --> 00:06:57.000]   You can give it an image and say, design a web page, for instance.
[00:06:57.000 --> 00:07:01.000]   And it did a working web page based on a very primitive sketch.
[00:07:01.000 --> 00:07:04.000]   I have not played much with it.
[00:07:04.000 --> 00:07:10.600]   The only thing I did was I people were berating me for having Jason Calicanis on Twitter on
[00:07:10.600 --> 00:07:11.600]   Sunday.
[00:07:11.600 --> 00:07:14.320]   So I asked as well, they sure asked it.
[00:07:14.320 --> 00:07:19.280]   I asked it to write an apology and it did actually a good job.
[00:07:19.280 --> 00:07:23.000]   It's going to make a case of who thinks that he saved the economy in all banks.
[00:07:23.000 --> 00:07:24.000]   He knows he did.
[00:07:24.000 --> 00:07:25.000]   Little Joe Jason.
[00:07:25.000 --> 00:07:30.200]   This is his defense is all the the upper cat, upper case tweets that he put out on Saturday
[00:07:30.200 --> 00:07:31.200]   and Sunday.
[00:07:31.200 --> 00:07:39.040]   He said that, you know, if they had done nothing, those would have been legit.
[00:07:39.040 --> 00:07:46.560]   But before he came on to it, before he even came on to it, the FDIC and the Fed and Janet
[00:07:46.560 --> 00:07:50.320]   Yellen, the Secretary of Treasury and the president announced that they were going to
[00:07:50.320 --> 00:07:53.080]   backstop all deposits, Silicon Valley Bank.
[00:07:53.080 --> 00:07:54.720]   That's what Jason wanted.
[00:07:54.720 --> 00:07:58.360]   He told me that he had been writing checks to his portfolio company so they could cover
[00:07:58.360 --> 00:08:01.040]   payroll, which is a legitimate problem.
[00:08:01.040 --> 00:08:03.160]   He says, I was going to do it.
[00:08:03.160 --> 00:08:07.600]   But as people like Jason, I mean, Jason is a unique individual and he's a one of a kind.
[00:08:07.600 --> 00:08:12.160]   But people like Jason VC types, the Silicon Valley types, these, these, they're pseudo
[00:08:12.160 --> 00:08:16.240]   libertarians who are libertarian until it's time for them to be bailed out.
[00:08:16.240 --> 00:08:22.080]   I mean, they're, they're a very clear amounts in banks like Silicon Valley Bank that are
[00:08:22.080 --> 00:08:23.560]   insured by the federal government.
[00:08:23.560 --> 00:08:30.840]   And beyond that, it's your own risk.
[00:08:30.840 --> 00:08:32.200]   And all these hubristic tech bros were just like blowing way past that and being very irresponsible
[00:08:32.200 --> 00:08:33.200]   with the money.
[00:08:33.200 --> 00:08:37.240]   And as soon as they got caught with their pants down, they went, they, they used their, their,
[00:08:37.240 --> 00:08:42.520]   their megaphone that they have on social media to whine and complain until the government
[00:08:42.520 --> 00:08:48.680]   came and they basically created, I think that if this was a Midwestern bank of the same size
[00:08:48.680 --> 00:08:54.240]   under the same circumstances and the people whose money was exposed weren't people with
[00:08:54.240 --> 00:08:59.680]   huge megaphones and big audiences on social, that there would have been no risk of runs
[00:08:59.680 --> 00:09:01.640]   on multiple banks.
[00:09:01.640 --> 00:09:09.720]   They so created a crisis so that they would be made whole based on their own irresponsible
[00:09:09.720 --> 00:09:11.520]   placement of money.
[00:09:11.520 --> 00:09:13.560]   So I just.
[00:09:13.560 --> 00:09:14.560]   No.
[00:09:14.560 --> 00:09:19.520]   So there's, there's a couple things there.
[00:09:19.520 --> 00:09:26.880]   I do agree that there was some crazy irresponsibility involved, but I also think I think there's
[00:09:26.880 --> 00:09:27.880]   a systemic issue.
[00:09:27.880 --> 00:09:28.880]   Okay.
[00:09:28.880 --> 00:09:35.800]   One, it is not crazy that these people in startups, there was a legitimate threat to
[00:09:35.800 --> 00:09:43.920]   a very large ecosystem of startups that both employed lots of people and whose products
[00:09:43.920 --> 00:09:46.280]   are everywhere, right?
[00:09:46.280 --> 00:09:53.160]   And it's not crazy to have like, you cannot have a line of working capital in a bank that
[00:09:53.160 --> 00:09:55.280]   is insured by FDIC.
[00:09:55.280 --> 00:09:59.760]   You can pay extra for more insurance, but it is unreasonable to expect a business that
[00:09:59.760 --> 00:10:07.160]   employs a couple hundred people or even 50 people have a $250,000 line of working capital.
[00:10:07.160 --> 00:10:12.680]   And it is unreasonable to expect you to have them in separate accounts everywhere for running
[00:10:12.680 --> 00:10:13.880]   a business.
[00:10:13.880 --> 00:10:20.480]   So that's one, but you do usually have multiple banks.
[00:10:20.480 --> 00:10:24.520]   So if your one bank fails, you can switch over to another.
[00:10:24.520 --> 00:10:28.800]   So that is a reasonable way to set up your finances if you're like a CFO.
[00:10:28.800 --> 00:10:32.920]   Most people, most really small businesses don't do that and they do get hurt in these sorts
[00:10:32.920 --> 00:10:34.120]   of cases.
[00:10:34.120 --> 00:10:37.880]   Silicon Valley Bank had much more sophisticated investors and people.
[00:10:37.880 --> 00:10:40.720]   So you would think they would have cautioned them against that.
[00:10:40.720 --> 00:10:44.040]   People talk a lot about the fact that SBB required people to have it.
[00:10:44.040 --> 00:10:48.480]   They have any sort of mortgage that they needed to have all of their accounts there.
[00:10:48.480 --> 00:10:50.080]   That's really common.
[00:10:50.080 --> 00:10:51.880]   My husband's business has the same thing.
[00:10:51.880 --> 00:10:53.360]   He's not an SBB.
[00:10:53.360 --> 00:10:56.520]   My child's school has that sort of thing.
[00:10:56.520 --> 00:11:00.040]   So that's not uncommon, but maybe it should be less common.
[00:11:00.040 --> 00:11:06.120]   So those are a couple of things that I think are worth kind of separating out.
[00:11:06.120 --> 00:11:13.560]   And overall, I think we may not like it, but I think they did the right thing, the government.
[00:11:13.560 --> 00:11:14.560]   I think having to-
[00:11:14.560 --> 00:11:15.560]   I agree with that.
[00:11:15.560 --> 00:11:16.560]   Yeah.
[00:11:16.560 --> 00:11:17.560]   I agree that they did the right thing.
[00:11:17.560 --> 00:11:24.000]   But my point is basically the whole venture capital Silicon Valley model is inherently high
[00:11:24.000 --> 00:11:25.000]   risk.
[00:11:25.000 --> 00:11:26.000]   It's the highest risk.
[00:11:26.000 --> 00:11:28.920]   It's kind of investment type of deal that there is.
[00:11:28.920 --> 00:11:36.120]   And the reason that it's difficult to make payroll for a pretty large company is because
[00:11:36.120 --> 00:11:37.480]   it's based on venture capital.
[00:11:37.480 --> 00:11:40.200]   They're like, "Here's millions of dollars.
[00:11:40.200 --> 00:11:44.000]   Put it somewhere so you can pay your employees," and all that kind of stuff.
[00:11:44.000 --> 00:11:52.240]   The whole model is super high risk and super high reward potentially.
[00:11:52.240 --> 00:11:56.480]   My concern is, okay, now the government has done what it has done.
[00:11:56.480 --> 00:11:57.640]   They didn't have to do that.
[00:11:57.640 --> 00:11:59.280]   I'm glad they did.
[00:11:59.280 --> 00:12:01.160]   They saved the economy.
[00:12:01.160 --> 00:12:03.200]   They stopped the trickle on effect.
[00:12:03.200 --> 00:12:07.480]   And they saved a lot of people on Main Street who wouldn't have gotten paid.
[00:12:07.480 --> 00:12:09.840]   But we keep doing the same kind of thing.
[00:12:09.840 --> 00:12:13.080]   Now they've radically altered the incentives for us.
[00:12:13.080 --> 00:12:14.600]   Well, there is a side effect.
[00:12:14.600 --> 00:12:18.440]   I think you're right that people will now assume anything over a quarter of a million
[00:12:18.440 --> 00:12:20.400]   dollars is kind of quasi-insured.
[00:12:20.400 --> 00:12:23.560]   The bank is too big to fail.
[00:12:23.560 --> 00:12:25.120]   And I think that is problematic.
[00:12:25.120 --> 00:12:27.440]   And I think you're all right.
[00:12:27.440 --> 00:12:32.040]   Can I just point out one thing?
[00:12:32.040 --> 00:12:37.120]   The reason SVB, Silicon Valley Bank, got in trouble is because they invested in the most
[00:12:37.120 --> 00:12:42.880]   secure investment, which was treasury bills.
[00:12:42.880 --> 00:12:47.160]   The problem is the government was paying less than 1% on those T-bills.
[00:12:47.160 --> 00:12:52.040]   The Fed raised rates, and suddenly the T-bills had much lower value unless you could wait
[00:12:52.040 --> 00:12:55.480]   to a majority, but they couldn't wait 10 years to cash those in.
[00:12:55.480 --> 00:12:59.680]   The reason that they then had to do that, of course, was because of Peter Thiel, which
[00:12:59.680 --> 00:13:06.360]   goes to, I think, Mike's point where because there were those who took out enough that
[00:13:06.360 --> 00:13:11.440]   they had to then try to, an emergency basis, deal with those assets that weren't as liquid
[00:13:11.440 --> 00:13:13.160]   as they wanted.
[00:13:13.160 --> 00:13:15.880]   That's the kind of, well, I think the bank was oddly conservative.
[00:13:15.880 --> 00:13:18.880]   I think Leo is bad judgment, but conservative.
[00:13:18.880 --> 00:13:20.880]   No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,
[00:13:20.880 --> 00:13:21.880]   they weren't because--
[00:13:21.880 --> 00:13:23.480]   Okay, so here's the deal.
[00:13:23.480 --> 00:13:28.320]   Okay, I used to cover Bank Cake for a hot second back when I was 20.
[00:13:28.320 --> 00:13:37.000]   So let me-- banks have to-- one of Silicon-- so one, Silicon Valley has had a bad-- they've
[00:13:37.000 --> 00:13:43.280]   known that this was a problem that other banks have had and has seen coming for six to seven
[00:13:43.280 --> 00:13:44.280]   months.
[00:13:44.280 --> 00:13:49.080]   So Silicon Valley Bank had just like-- I truly don't know what they were doing.
[00:13:49.080 --> 00:13:55.000]   They were not actually making the hard move and reallocating their assets.
[00:13:55.000 --> 00:13:58.640]   And we could talk about market to market counting if you want, but I don't think anyone wants
[00:13:58.640 --> 00:13:59.640]   us to.
[00:13:59.640 --> 00:14:04.560]   So they were trying-- they basically did everything all of a sudden when they realized
[00:14:04.560 --> 00:14:08.000]   they needed to, or they should have realized it earlier too.
[00:14:08.000 --> 00:14:13.040]   And then the other thing is they bought securities because they have a ton of freaking money
[00:14:13.040 --> 00:14:17.400]   lying around from their depositors.
[00:14:17.400 --> 00:14:21.360]   But those people-- they're not able to deploy it in small business loans.
[00:14:21.360 --> 00:14:25.280]   They're actually apparently big in the wine industry, so they've got some loans there.
[00:14:25.280 --> 00:14:31.040]   But they don't have a lot of places to put their money that is going to make it work
[00:14:31.040 --> 00:14:32.680]   for them, basically.
[00:14:32.680 --> 00:14:39.920]   And so I think there's-- I mean, I think it's a shame because Silicon Valley has been able--
[00:14:39.920 --> 00:14:42.920]   my husband had an account when he did a tech startup a couple of years ago.
[00:14:42.920 --> 00:14:51.040]   It's hard to get money if you're a venture-backed company or to start a company that doesn't
[00:14:51.040 --> 00:14:54.080]   have an inventory or some sort of real asset.
[00:14:54.080 --> 00:14:58.440]   And SVB and Comerica are probably some of the few ones that did.
[00:14:58.440 --> 00:15:01.680]   And so it is a real shame that this is coming to bite them in the butt.
[00:15:01.680 --> 00:15:06.280]   And it's a shame they didn't manage their assets earlier and more conservative or more
[00:15:06.280 --> 00:15:07.280]   actively earlier.
[00:15:07.280 --> 00:15:09.600]   And I think Elizabeth Warren has a point.
[00:15:09.600 --> 00:15:16.120]   She blames it on the repeal of Dodd-Frank in 2018, or not repeal, but reduction of Dodd-Frank.
[00:15:16.120 --> 00:15:21.680]   Had SVB been required to do the stress testing that larger banks like JP Morgan are still
[00:15:21.680 --> 00:15:25.440]   required to do, they were exempted because they were smaller.
[00:15:25.440 --> 00:15:29.640]   They perhaps would have been ready to withstand a run and so forth.
[00:15:29.640 --> 00:15:33.120]   There's a lot of blame to cast around.
[00:15:33.120 --> 00:15:35.280]   I don't think it's because they had a black member on their board.
[00:15:35.280 --> 00:15:37.080]   I think that's probably--
[00:15:37.080 --> 00:15:38.080]   It's not because they were woke.
[00:15:38.080 --> 00:15:39.080]   We don't even need to give that a year of time.
[00:15:39.080 --> 00:15:47.560]   But there are many reasons for this.
[00:15:47.560 --> 00:15:51.200]   I agree with you, Mike, that the government did what it had to do.
[00:15:51.200 --> 00:15:55.200]   I also agree with you that there might be unintended consequences, but I'm not sure
[00:15:55.200 --> 00:15:59.200]   I would blame the bank.
[00:15:59.200 --> 00:16:02.600]   I'm not sure I blame venture capitalists for this.
[00:16:02.600 --> 00:16:05.760]   What about Teal specifically starting the run?
[00:16:05.760 --> 00:16:06.760]   He did not.
[00:16:06.760 --> 00:16:07.760]   This is very clear.
[00:16:07.760 --> 00:16:08.760]   He did not start the run.
[00:16:08.760 --> 00:16:09.760]   He did not start the run.
[00:16:09.760 --> 00:16:10.760]   No.
[00:16:10.760 --> 00:16:11.760]   He did not start the run.
[00:16:11.760 --> 00:16:15.600]   He was the first venture fund to say to his portfolio, probably you should withdraw it.
[00:16:15.600 --> 00:16:22.920]   Honestly, the Financial Times said red alert, red alert in the end of February.
[00:16:22.920 --> 00:16:30.040]   SVB started the run in Wednesday saying that they were struggling.
[00:16:30.040 --> 00:16:33.240]   I don't think Peter Teal started the run by any means.
[00:16:33.240 --> 00:16:38.200]   I think somebody has made the point, which is true, that in some ways this is the first
[00:16:38.200 --> 00:16:40.320]   Twitter bank run.
[00:16:40.320 --> 00:16:41.560]   It's not so much that.
[00:16:41.560 --> 00:16:46.080]   I think it's much more likely that it's because it was a monoculture that most of the people
[00:16:46.080 --> 00:16:50.000]   who had accounts there were in a very small group of people.
[00:16:50.000 --> 00:16:51.520]   It was probably a slack.
[00:16:51.520 --> 00:16:53.480]   It was a slant like bean stonks.
[00:16:53.480 --> 00:16:54.880]   You're not far off.
[00:16:54.880 --> 00:17:00.760]   There was a group chat earlier in the day before Peter Teal's note.
[00:17:00.760 --> 00:17:05.560]   Jason was in this group chat with more than 300 startup CEOs.
[00:17:05.560 --> 00:17:10.000]   In that group chat, that was the talk of the day.
[00:17:10.000 --> 00:17:14.640]   It was already buzzing around Silicon Valley before Peter Teal's note.
[00:17:14.640 --> 00:17:16.880]   I blame him as well.
[00:17:16.880 --> 00:17:18.480]   I think Jason explained it.
[00:17:18.480 --> 00:17:23.160]   I'm looking for any reason to continue by the slack of him.
[00:17:23.160 --> 00:17:29.520]   The other thing is, if you're a venture capital firm, if there's going to be a run on a bank,
[00:17:29.520 --> 00:17:34.480]   your fiduciary duty is to tell your peeps to get their money out of the bank.
[00:17:34.480 --> 00:17:38.440]   We can all kumbaya together and be like, "Have our George Bailey moment."
[00:17:38.440 --> 00:17:45.520]   But at the end of the day, my only point is that when it's like, "Oh, we're laying off
[00:17:45.520 --> 00:17:46.520]   10,000 people."
[00:17:46.520 --> 00:17:48.440]   They're like, "Hey, man, that's capitalism."
[00:17:48.440 --> 00:17:52.520]   Then when they want to be crypto bros, we don't want the government involved at all.
[00:17:52.520 --> 00:17:57.520]   But as soon as their money is wrapped up in something like this, they're suddenly like
[00:17:57.520 --> 00:17:59.680]   Bernie Sanders socialists.
[00:17:59.680 --> 00:18:00.760]   It's like, "Bale is out.
[00:18:00.760 --> 00:18:01.760]   Bale is out."
[00:18:01.760 --> 00:18:07.920]   It's like, pick a pick up an economic philosophy and be consistent with it.
[00:18:07.920 --> 00:18:10.160]   I like that.
[00:18:10.160 --> 00:18:13.160]   Well, that was our AI segment.
[00:18:13.160 --> 00:18:14.600]   I hope you enjoyed it.
[00:18:14.600 --> 00:18:15.600]   Yeah.
[00:18:15.600 --> 00:18:16.600]   Can't please.
[00:18:16.600 --> 00:18:17.600]   How did we get what?
[00:18:17.600 --> 00:18:18.600]   I always just didn't.
[00:18:18.600 --> 00:18:19.600]   All these content people.
[00:18:19.600 --> 00:18:20.600]   I didn't know.
[00:18:20.600 --> 00:18:21.600]   I did.
[00:18:21.600 --> 00:18:22.600]   I did.
[00:18:22.600 --> 00:18:23.600]   P.P.4.
[00:18:23.600 --> 00:18:24.600]   Should we avoid it more?
[00:18:24.600 --> 00:18:26.520]   Do we want to talk about Timo just for fun?
[00:18:26.520 --> 00:18:27.520]   All right.
[00:18:27.520 --> 00:18:28.520]   Well, yeah.
[00:18:28.520 --> 00:18:30.520]   No, we should AI it.
[00:18:30.520 --> 00:18:31.520]   I don't know.
[00:18:31.520 --> 00:18:34.520]   It's your show, man.
[00:18:34.520 --> 00:18:37.520]   Just choose one.
[00:18:37.520 --> 00:18:40.520]   I don't know.
[00:18:40.520 --> 00:18:43.520]   So you mentioned Timo.
[00:18:43.520 --> 00:18:47.080]   Let's get Timo out of the way.
[00:18:47.080 --> 00:18:51.720]   I'll do a commercial, not for Mint Mobile.
[00:18:51.720 --> 00:18:55.360]   And then I will do, because it's going to be a long conversation.
[00:18:55.360 --> 00:19:00.800]   I think there's a lot of ins and outs and ramifications when it comes to AI these days.
[00:19:00.800 --> 00:19:01.800]   And it really is.
[00:19:01.800 --> 00:19:03.960]   Like there wasn't with the bank.
[00:19:03.960 --> 00:19:04.960]   Yeah.
[00:19:04.960 --> 00:19:08.080]   Well, I'm actually, I forgot that you had that background in Stacey.
[00:19:08.080 --> 00:19:10.280]   So I'm glad you had a good way in with.
[00:19:10.280 --> 00:19:13.680]   There's one thing we didn't mention, which is sweep insurance programs.
[00:19:13.680 --> 00:19:16.840]   Many banks, I don't think SVB had one, but many banks have the ability, if you're going
[00:19:16.840 --> 00:19:20.800]   to put in more than a quarter of a million dollars, to keep it insured by sweeping it
[00:19:20.800 --> 00:19:21.800]   into multiple accounts.
[00:19:21.800 --> 00:19:23.560]   And they do this automatically.
[00:19:23.560 --> 00:19:27.120]   These things exist certainly.
[00:19:27.120 --> 00:19:31.520]   And now I'm sure any startup founder should know about that.
[00:19:31.520 --> 00:19:32.520]   That's what I was talking about.
[00:19:32.520 --> 00:19:34.800]   If you get a big, when I was talking about ways to enter it.
[00:19:34.800 --> 00:19:35.800]   Yeah.
[00:19:35.800 --> 00:19:41.600]   If you get away, big, big, you know, series A check, you probably should find out about
[00:19:41.600 --> 00:19:44.040]   sweep insurance plans.
[00:19:44.040 --> 00:19:48.480]   Because yeah, it's probably not good to have 20 billion in a bank.
[00:19:48.480 --> 00:19:50.400]   That's about to go under.
[00:19:50.400 --> 00:19:52.320]   These whole, everybody's okay.
[00:19:52.320 --> 00:19:54.720]   The banking system seems to be intact.
[00:19:54.720 --> 00:19:57.880]   Bank stocks hurt a little bit, but that's okay.
[00:19:57.880 --> 00:19:58.880]   It's a little wobbly.
[00:19:58.880 --> 00:20:00.800]   I mean, it's a little ripple in Europe.
[00:20:00.800 --> 00:20:02.600]   Credit, credit, Swiss and...
[00:20:02.600 --> 00:20:04.920]   And India has seven issues.
[00:20:04.920 --> 00:20:05.920]   Yeah.
[00:20:05.920 --> 00:20:10.920]   So, I mean, partly this is a symptom like, and again, I say when meme stonks come for
[00:20:10.920 --> 00:20:19.320]   your bank, but like our entire culture is, or like our economic culture, what is the
[00:20:19.320 --> 00:20:21.320]   word I'm looking for?
[00:20:21.320 --> 00:20:25.960]   Our entire money system is basically built on confidence in the overall system.
[00:20:25.960 --> 00:20:30.360]   And right now we're all like, wait, I mean, some people are realizing for the first time
[00:20:30.360 --> 00:20:31.360]   how banks make money.
[00:20:31.360 --> 00:20:32.360]   Yeah.
[00:20:32.360 --> 00:20:33.800]   And they're like, wait, I don't like that at all.
[00:20:33.800 --> 00:20:34.800]   Yeah.
[00:20:34.800 --> 00:20:35.800]   So, I'm just kind of...
[00:20:35.800 --> 00:20:38.720]   Markets, markets though, are very skittish.
[00:20:38.720 --> 00:20:41.920]   Of course, this is going to hit bank stocks.
[00:20:41.920 --> 00:20:46.560]   Because after all, let's point out, the government did not bail out the shareholders and executives
[00:20:46.560 --> 00:20:47.560]   of SVB.
[00:20:47.560 --> 00:20:48.560]   That's true.
[00:20:48.560 --> 00:20:52.120]   They lost a lot of money, potentially, I guess.
[00:20:52.120 --> 00:20:55.840]   So, I would understand why if you had bank stocks, you might get nervous.
[00:20:55.840 --> 00:21:01.000]   I think the banking system, I think the government did everything you could do to make everybody
[00:21:01.000 --> 00:21:02.000]   feel better.
[00:21:02.000 --> 00:21:03.000]   Oh, yeah.
[00:21:03.000 --> 00:21:04.000]   That's the only system you did in the product, probably.
[00:21:04.000 --> 00:21:05.000]   Yeah.
[00:21:05.000 --> 00:21:06.000]   Yeah.
[00:21:06.000 --> 00:21:07.000]   I think some credit.
[00:21:07.000 --> 00:21:09.040]   So, I don't give credit to Jason Calicans.
[00:21:09.040 --> 00:21:12.120]   His hair on fire treats a hysterical, but...
[00:21:12.120 --> 00:21:15.720]   And I mean that both as in hysterical, funny and hair, hysterical is running around with
[00:21:15.720 --> 00:21:22.280]   your hair on fire, but I think it already been solved pretty much by the time.
[00:21:22.280 --> 00:21:26.680]   And it also has to be said for anybody concerned about this bailout that in fact the taxpayers
[00:21:26.680 --> 00:21:28.680]   did not fund comes from the bank.
[00:21:28.680 --> 00:21:32.560]   Unless you follow the Wall Street Journal in their theory that because bank fees will
[00:21:32.560 --> 00:21:38.040]   be higher than that we indeed did, I'm like, "Look, money comes from somewhere."
[00:21:38.040 --> 00:21:39.040]   Explain.
[00:21:39.040 --> 00:21:40.040]   Someone's got to pay something.
[00:21:40.040 --> 00:21:41.680]   Actually, this would be good.
[00:21:41.680 --> 00:21:44.480]   That's AC for you to explain FDIC.
[00:21:44.480 --> 00:21:47.640]   They said the bank dues would pay for this.
[00:21:47.640 --> 00:21:48.640]   So, they pay.
[00:21:48.640 --> 00:21:49.880]   This is a lot of money.
[00:21:49.880 --> 00:21:50.880]   This is, I think...
[00:21:50.880 --> 00:21:51.880]   Well, it's guaranteed.
[00:21:51.880 --> 00:21:53.920]   They're not paid so much as guaranteed.
[00:21:53.920 --> 00:21:54.920]   They're guaranteed.
[00:21:54.920 --> 00:21:55.920]   Yeah.
[00:21:55.920 --> 00:22:03.600]   And there's a special assessment fee banks pay into for like FDIC insurance.
[00:22:03.600 --> 00:22:08.160]   And so that assessment will go up on banks, so they will pay more into it.
[00:22:08.160 --> 00:22:13.960]   And yes, you might see here account fees or rather the percentage you get in savings
[00:22:13.960 --> 00:22:18.560]   for putting your money in account will go possibly down by a couple of bits or basis
[00:22:18.560 --> 00:22:20.560]   points beeps.
[00:22:20.560 --> 00:22:25.000]   But it's not like a huge deal.
[00:22:25.000 --> 00:22:26.920]   So yes, the banks will pay for it.
[00:22:26.920 --> 00:22:31.240]   It will have to come out of their bottom line which will possibly affect the interest
[00:22:31.240 --> 00:22:32.240]   rates.
[00:22:32.240 --> 00:22:34.800]   They will pay you to keep your money safe.
[00:22:34.800 --> 00:22:35.800]   All right.
[00:22:35.800 --> 00:22:40.200]   What did your apology, what are the chat GPT apology for having Jason on say?
[00:22:40.200 --> 00:22:42.200]   It was pretty good, I thought.
[00:22:42.200 --> 00:22:44.720]   We were meant mobilizing and then we're taking an ad.
[00:22:44.720 --> 00:22:48.000]   And then you said no, no, make it more sincere.
[00:22:48.000 --> 00:22:50.960]   So my more sincere.
[00:22:50.960 --> 00:22:57.120]   With I use Bing which lets you choose kind of some parameters like do you want to be funny?
[00:22:57.120 --> 00:22:58.680]   Do you want to be serious?
[00:22:58.680 --> 00:23:07.040]   So I just said, please write an excuse for explanation as why we invited Jason Calicana
[00:23:07.040 --> 00:23:09.920]   to Twitter after his bank run tweets.
[00:23:09.920 --> 00:23:10.920]   That's all I said.
[00:23:10.920 --> 00:23:11.920]   It actually was very shast.
[00:23:11.920 --> 00:23:13.200]   Well, it's hard to figure out.
[00:23:13.200 --> 00:23:14.200]   Yeah.
[00:23:14.200 --> 00:23:15.960]   So this is what chat GPT wrote.
[00:23:15.960 --> 00:23:19.640]   And this was yesterday at 9.41 a.m.
[00:23:19.640 --> 00:23:23.160]   I don't know if was chat GPT for yet.
[00:23:23.160 --> 00:23:25.360]   If you've been following, how should I read it?
[00:23:25.360 --> 00:23:29.840]   If you've been following the news, you probably know that Silicon Valley bank.
[00:23:29.840 --> 00:23:33.760]   The bank of choice, actually, this is so excellently written, the bank of choice from
[00:23:33.760 --> 00:23:38.760]   any tech startups and investors collapsed last week after a massive wave of customer
[00:23:38.760 --> 00:23:39.760]   withdrawals.
[00:23:39.760 --> 00:23:44.440]   The bank run was partly fueled by social media posts from prominent venture capitalists
[00:23:44.440 --> 00:23:48.280]   who expressed their fears and frustrations about the situation.
[00:23:48.280 --> 00:23:52.760]   One of them was Jason Calicana's, who's known for his early bets on companies like Uber
[00:23:52.760 --> 00:23:56.160]   and Robin Hood, as well as his friendship with Elon Musk.
[00:23:56.160 --> 00:24:01.440]   Calicana's tweeted in all caps that people should be absolutely terrified right now.
[00:24:01.440 --> 00:24:06.960]   And that the situation could spiral into chaos if the government didn't guarantee all deposits
[00:24:06.960 --> 00:24:08.680]   up to $10 million.
[00:24:08.680 --> 00:24:13.560]   He also claimed that the bank run had already happened and that his caps lock got stuck.
[00:24:13.560 --> 00:24:15.560]   So he said that.
[00:24:15.560 --> 00:24:16.560]   He said that.
[00:24:16.560 --> 00:24:18.560]   He showed the screen.
[00:24:18.560 --> 00:24:19.560]   He said that.
[00:24:19.560 --> 00:24:20.560]   He did say that.
[00:24:20.560 --> 00:24:21.560]   That's amazing.
[00:24:21.560 --> 00:24:26.560]   Calicana's is trying to start a bank runner profiting from it by shorting S.V.B. stocks.
[00:24:26.560 --> 00:24:31.440]   Other models, others mocked him for his overreaction and suggested he should calm down or lay off
[00:24:31.440 --> 00:24:32.440]   the drugs.
[00:24:32.440 --> 00:24:36.800]   Some even blamed him for causing the bank run with his panic tweets.
[00:24:36.800 --> 00:24:39.040]   So why did we invite him to our show?
[00:24:39.040 --> 00:24:43.080]   Well, for one thing, he's always been an entertaining guest who doesn't shy away from
[00:24:43.080 --> 00:24:44.080]   this is a chat.
[00:24:44.080 --> 00:24:45.080]   GPT this your line.
[00:24:45.080 --> 00:24:48.440]   I swear to God on my mother's on my mother's.
[00:24:48.440 --> 00:24:52.040]   And mothers, first of all, this is this is absolutely a GPT for.
[00:24:52.040 --> 00:24:53.400]   Yeah, it's pretty dark.
[00:24:53.400 --> 00:24:56.560]   He's also current one of the.
[00:24:56.560 --> 00:24:57.560]   Yeah, that's right.
[00:24:57.560 --> 00:25:01.160]   He's also one of the most influential voices in the tech industry with insights.
[00:25:01.160 --> 00:25:03.680]   It's a trends and opportunities that few others have.
[00:25:03.680 --> 00:25:08.040]   And he's not afraid to admit when he's wrong or changes mind when presented with no information.
[00:25:08.040 --> 00:25:09.680]   No, I would stand by this.
[00:25:09.680 --> 00:25:11.280]   This is this is my opinion.
[00:25:11.280 --> 00:25:12.280]   Exactly.
[00:25:12.280 --> 00:25:13.480]   Frankly, I could have written this.
[00:25:13.480 --> 00:25:15.520]   We wanted to hear his side of the story.
[00:25:15.520 --> 00:25:17.560]   What motivated him to tweet what he did.
[00:25:17.560 --> 00:25:21.000]   How he felt about the consequences and what he learned from the experience.
[00:25:21.000 --> 00:25:24.680]   That's the one thing I wouldn't have said because we don't do hugging or learning here.
[00:25:24.680 --> 00:25:29.360]   We also wanted to thank him about his involvement in Elon Musk's purchase of Twitter, not thank
[00:25:29.360 --> 00:25:30.360]   him.
[00:25:30.360 --> 00:25:33.000]   We also wanted to ask him, please, please, please ask him.
[00:25:33.000 --> 00:25:35.040]   That was folks who were early listening.
[00:25:35.040 --> 00:25:36.760]   That was Leo's Freudian slip.
[00:25:36.760 --> 00:25:39.360]   Yeah, boy, was that a Freudian slip.
[00:25:39.360 --> 00:25:43.320]   We also wanted to ask him, but as involvement in Elon Musk's purchase of Twitter, which
[00:25:43.320 --> 00:25:47.160]   happened shortly before the SVB collapse, and we wanted to have some fun with him because
[00:25:47.160 --> 00:25:49.960]   let's face it, the whole thing was pretty hilarious.
[00:25:49.960 --> 00:25:54.920]   So tune in to our next episode of Twitter, where we'll have Jason Calicanis as our special
[00:25:54.920 --> 00:25:55.920]   guest.
[00:25:55.920 --> 00:25:59.480]   We promise you won't regret it unless you're one of those SVB customers who lost their
[00:25:59.480 --> 00:26:02.000]   money because of him.
[00:26:02.000 --> 00:26:03.000]   Wow.
[00:26:03.000 --> 00:26:04.640]   Isn't that great?
[00:26:04.640 --> 00:26:05.640]   Isn't that great?
[00:26:05.640 --> 00:26:10.200]   Honestly, how did you, how did what?
[00:26:10.200 --> 00:26:14.440]   I was going to ask, how do we know that or how did you know Mike that Chatechi or GPT
[00:26:14.440 --> 00:26:20.280]   because it's current information, the data set for GPT 3.5 ended in what 2021 or something
[00:26:20.280 --> 00:26:21.280]   like that.
[00:26:21.280 --> 00:26:22.280]   Yeah, that's right.
[00:26:22.280 --> 00:26:23.280]   So it wouldn't know about the bank run.
[00:26:23.280 --> 00:26:24.280]   It wouldn't have been known about his tweets.
[00:26:24.280 --> 00:26:25.280]   It had his tweets.
[00:26:25.280 --> 00:26:29.040]   It was current up to the date, up to Sunday.
[00:26:29.040 --> 00:26:31.960]   That's breathtakingly aware of what's coming from.
[00:26:31.960 --> 00:26:33.520]   Well, it's the humor that's amazing.
[00:26:33.520 --> 00:26:34.520]   Yeah.
[00:26:34.520 --> 00:26:37.480]   I think I asked it to give me a slightly jaw.
[00:26:37.480 --> 00:26:40.920]   I can't remember what I, I slanted it more towards the Jolly.
[00:26:40.920 --> 00:26:42.160]   It was very good.
[00:26:42.160 --> 00:26:49.440]   It was really, it was, it was pretty much what I would have said, although much more eloquent.
[00:26:49.440 --> 00:26:50.440]   So there's some real value.
[00:26:50.440 --> 00:26:55.280]   I mean, honest to God, I know what you said Leo is, is, Hey, it's not a democracy.
[00:26:55.280 --> 00:26:56.360]   Yeah, screw you.
[00:26:56.360 --> 00:26:57.920]   That's why I had Chatechi P.T.
[00:26:57.920 --> 00:26:58.920]   Right.
[00:26:58.920 --> 00:27:00.960]   Yeah, or Bing, Bing Chat, I should say.
[00:27:00.960 --> 00:27:02.960]   That's why I had Bing Chat right.
[00:27:02.960 --> 00:27:06.880]   All right, quickly, Mint Mobile and then a commercial.
[00:27:06.880 --> 00:27:10.120]   This is a tweet this morning from Ryan rounds.
[00:27:10.120 --> 00:27:12.840]   Mint Mobile is a sponsor, I should mention.
[00:27:12.840 --> 00:27:15.920]   And actually it starts not with Ryan Reynolds, but with the CEO of T-Mobile.
[00:27:15.920 --> 00:27:17.640]   We put T-Mobile like super.
[00:27:17.640 --> 00:27:22.360]   First in everything we do, like being the only wireless provider that offers both the
[00:27:22.360 --> 00:27:25.200]   best network and the best value.
[00:27:25.200 --> 00:27:31.560]   So today I am thrilled to announce that T-Mobile plans to acquire Mint Mobile, a wireless brand
[00:27:31.560 --> 00:27:34.280]   that shares our customer first commitment.
[00:27:34.280 --> 00:27:37.440]   And here with me to share the news is Ryan Reynolds, owner of Mint Mobile.
[00:27:37.440 --> 00:27:38.440]   Thank you, Mike.
[00:27:38.440 --> 00:27:42.400]   Incredibly excited, Mint has run on T-Mobile since its inception.
[00:27:42.400 --> 00:27:47.560]   And the reason people have such a great experience with Mint is due to the T-Mobile network, especially
[00:27:47.560 --> 00:27:49.480]   its unrivaled lead in 5G.
[00:27:49.480 --> 00:27:54.160]   Ryan, we are so happy to have you and the whole Mint team join the T-Mobile family.
[00:27:54.160 --> 00:27:57.440]   Well, I wouldn't call it a family, Mike.
[00:27:57.440 --> 00:27:59.200]   Family's a place for misdirected hopes and dreams.
[00:27:59.200 --> 00:28:02.320]   I'm hoping this will be much better than that.
[00:28:02.320 --> 00:28:05.240]   Well, T-Mobile is all about value.
[00:28:05.240 --> 00:28:09.440]   So we're excited to continue Mint's famous $15 a month pricing.
[00:28:09.440 --> 00:28:12.040]   And I'm really excited about even more good stuff to come.
[00:28:12.040 --> 00:28:16.040]   And T-Mobile has assured me that our incredibly improvised and borderline reckless messaging
[00:28:16.040 --> 00:28:19.560]   strategy will also remain untouched.
[00:28:19.560 --> 00:28:22.240]   I don't remember the word reckless, Ryan.
[00:28:22.240 --> 00:28:25.320]   Well, I wrote it into the contract with Crayon, Mike.
[00:28:25.320 --> 00:28:27.320]   Happy they did have the share.
[00:28:27.320 --> 00:28:28.320]   Oh, either way.
[00:28:28.320 --> 00:28:30.520]   We're so happy that Mint Mobile and Alter Mobile will play a big part in this.
[00:28:30.520 --> 00:28:32.280]   I think we're part of their reckless strategy.
[00:28:32.280 --> 00:28:33.280]   And the customers everywhere.
[00:28:33.280 --> 00:28:34.280]   I hope so.
[00:28:34.280 --> 00:28:39.280]   And along with the entire Mint Mobile team and my maximum effort teams, I'm just absolutely
[00:28:39.280 --> 00:28:41.320]   thrilled with every aspect of this new venture.
[00:28:41.320 --> 00:28:45.880]   I'm certain that I'm finally going to fill whatever hole I have inside my soul, which
[00:28:45.880 --> 00:28:50.720]   possesses me to emphasize external success over quieting an inner child.
[00:28:50.720 --> 00:28:54.800]   You're going to do that by buying Twitter technology from my now deceased father.
[00:28:54.800 --> 00:28:55.800]   You OK, Ryan?
[00:28:55.800 --> 00:28:57.800]   Yeah, just super excited.
[00:28:57.800 --> 00:28:58.800]   Yeah.
[00:28:58.800 --> 00:28:59.800]   I wonder we hug.
[00:28:59.800 --> 00:29:00.800]   I think we better do it.
[00:29:00.800 --> 00:29:01.800]   We better do it right now.
[00:29:01.800 --> 00:29:02.800]   Oh, good.
[00:29:02.800 --> 00:29:06.040]   Nice to have a new dad.
[00:29:06.040 --> 00:29:10.320]   God bless Ryan Reynolds.
[00:29:10.320 --> 00:29:11.620]   It starts off.
[00:29:11.620 --> 00:29:18.800]   We've all seen it a million times that typical corporate BS were so excited to be and it
[00:29:18.800 --> 00:29:21.360]   goes south so beautifully.
[00:29:21.360 --> 00:29:22.840]   And it's just so good.
[00:29:22.840 --> 00:29:25.000]   He's always so funny.
[00:29:25.000 --> 00:29:26.000]   He's great.
[00:29:26.000 --> 00:29:30.440]   He's I don't know how much my understanding because actually when we first started doing
[00:29:30.440 --> 00:29:33.320]   the ads and I saw Ryan is the owner and all that stuff.
[00:29:33.320 --> 00:29:37.440]   Is he really or is he just like celebrity spokesman with five shares?
[00:29:37.440 --> 00:29:40.440]   And I think I looked it up that he has about half of it.
[00:29:40.440 --> 00:29:43.600]   So 25 is that all?
[00:29:43.600 --> 00:29:45.880]   OK, is 25% all?
[00:29:45.880 --> 00:29:48.240]   OK, so I'm going to steal.
[00:29:48.240 --> 00:29:54.640]   He's made three let's see, 337.5 or three a million.
[00:29:54.640 --> 00:29:57.040]   So that's great because he's got a fourth kid on the way.
[00:29:57.040 --> 00:29:59.760]   Yeah, he is extra hundred million.
[00:29:59.760 --> 00:30:02.880]   He's done very well on this.
[00:30:02.880 --> 00:30:06.720]   He also has a gin business.
[00:30:06.720 --> 00:30:11.040]   And he sold aviation to Diego in 2020, I think.
[00:30:11.040 --> 00:30:15.120]   So kind of it was like, yeah, he's building up his cash.
[00:30:15.120 --> 00:30:19.360]   He does have, you know, he did invest it all in that that soccer team.
[00:30:19.360 --> 00:30:20.360]   Rexon, right.
[00:30:20.360 --> 00:30:21.360]   That wasn't that much.
[00:30:21.360 --> 00:30:22.360]   Football.
[00:30:22.360 --> 00:30:23.360]   Those are cheap.
[00:30:23.360 --> 00:30:25.160]   I just hope he didn't put it in Silicon Valley Bank.
[00:30:25.160 --> 00:30:27.440]   That's all I'm saying.
[00:30:27.440 --> 00:30:30.440]   And I think he has an ad agency.
[00:30:30.440 --> 00:30:32.600]   He mentions it in the in the bit.
[00:30:32.600 --> 00:30:36.600]   And I think the ad agency really is the thing that's quite brilliant about all of their
[00:30:36.600 --> 00:30:37.600]   slavery.
[00:30:37.600 --> 00:30:40.600]   That worth said his net worth was a hundred fifty million.
[00:30:40.600 --> 00:30:42.200]   Has doubled much higher than that.
[00:30:42.200 --> 00:30:43.840]   And one of the biggest triples.
[00:30:43.840 --> 00:30:46.640]   Yeah, his company is it's it's a marketing company.
[00:30:46.640 --> 00:30:50.600]   And I think it finds things for him that that's maximum effort.
[00:30:50.600 --> 00:30:51.600]   Yes.
[00:30:51.600 --> 00:30:52.600]   Yes.
[00:30:52.600 --> 00:30:54.600]   That's a reference, obviously, to Deadpool.
[00:30:54.600 --> 00:30:55.600]   Yes.
[00:30:55.600 --> 00:30:56.600]   He's sold.
[00:30:56.600 --> 00:30:58.760]   He's like maximum effort.
[00:30:58.760 --> 00:31:00.320]   But he didn't own all of ABA.
[00:31:00.320 --> 00:31:02.880]   He was a co-owner and co-sponsor.
[00:31:02.880 --> 00:31:07.960]   So what happens, I think, is people bring him stuff and he's like, this fits.
[00:31:07.960 --> 00:31:08.960]   I want to do it.
[00:31:08.960 --> 00:31:13.560]   Instead of putting your money in an account in Silicon Valley Bank, make your put your
[00:31:13.560 --> 00:31:14.560]   money to work.
[00:31:14.560 --> 00:31:15.560]   Like gin.
[00:31:15.560 --> 00:31:18.040]   Make gin by a phone company.
[00:31:18.040 --> 00:31:19.800]   Put your money to work, kids.
[00:31:19.800 --> 00:31:23.000]   That's the secret to success.
[00:31:23.000 --> 00:31:24.000]   I am happy for him.
[00:31:24.000 --> 00:31:26.560]   I think he's he's he's he's like a good guy.
[00:31:26.560 --> 00:31:29.240]   It seems like he seems like a good guy.
[00:31:29.240 --> 00:31:33.680]   Well, if he if he sponsors anything on this network, he's a good guy.
[00:31:33.680 --> 00:31:38.800]   And he's he's got a self-deprecating sense of humor, which I think which we love goes
[00:31:38.800 --> 00:31:40.680]   a long way.
[00:31:40.680 --> 00:31:42.680]   He's he.
[00:31:42.680 --> 00:31:46.760]   Rexham here here is the UK take on this.
[00:31:46.760 --> 00:31:55.440]   Rexham owner Ryan Reynolds in huge financial boost, which means the non-league Welsh club
[00:31:55.440 --> 00:31:59.000]   could receive a huge financial windfall.
[00:31:59.000 --> 00:32:04.160]   He owns it with Rob McElany, the the star of it's always in Philadelphia.
[00:32:04.160 --> 00:32:08.680]   They bought, I mean, I'm going to tell you, it's not it's cheap to buy Rexham was not
[00:32:08.680 --> 00:32:12.760]   expensive, but then they have to run it and they've got to buy cleats for everybody.
[00:32:12.760 --> 00:32:17.360]   So it's nice to be a but they've already turned it into a little was it an Apple TV that he
[00:32:17.360 --> 00:32:18.840]   did the documentary about it.
[00:32:18.840 --> 00:32:19.840]   So welcome to Rexham.
[00:32:19.840 --> 00:32:21.200]   So I'm sure he's already made money on.
[00:32:21.200 --> 00:32:24.320]   Oh, yeah, I'm sure 2.75 million.
[00:32:24.320 --> 00:32:26.800]   Had I known I might have bought a soccer club.
[00:32:26.800 --> 00:32:29.400]   Stace, why don't we go in?
[00:32:29.400 --> 00:32:32.080]   Maybe menu is available or Chelsea or something.
[00:32:32.080 --> 00:32:34.960]   We can just yeah, I think those are a little bit pricier.
[00:32:34.960 --> 00:32:36.280]   We're going to have Saudi money.
[00:32:36.280 --> 00:32:38.120]   We're completing with there.
[00:32:38.120 --> 00:32:41.120]   Little more expensive.
[00:32:41.120 --> 00:32:43.720]   Start a club in Petaluma.
[00:32:43.720 --> 00:32:44.720]   Mm.
[00:32:44.720 --> 00:32:46.720]   Yeah, soccer is huge.
[00:32:46.720 --> 00:32:49.520]   It's going to be the next big sport.
[00:32:49.520 --> 00:32:50.520]   Yeah.
[00:32:50.520 --> 00:32:55.120]   You must watch.
[00:32:55.120 --> 00:33:00.640]   You must go to a few matches as you travel around the world, Mike.
[00:33:00.640 --> 00:33:01.800]   Never.
[00:33:01.800 --> 00:33:02.800]   Never ever.
[00:33:02.800 --> 00:33:08.440]   And I just I can barely watch regular sports, let alone soccer.
[00:33:08.440 --> 00:33:13.120]   I just I've tried to force myself to watch the soccer game in five minutes in and I'm
[00:33:13.120 --> 00:33:15.720]   like, I'm like, I can't do this.
[00:33:15.720 --> 00:33:18.040]   It's just guys right back.
[00:33:18.040 --> 00:33:21.800]   I'm like a I'm like a sea, you know, a Christian and Easter Christian.
[00:33:21.800 --> 00:33:24.360]   I'm a Super Bowl and World Cup sports fan.
[00:33:24.360 --> 00:33:25.360]   Right.
[00:33:25.360 --> 00:33:26.360]   And Olympics, right?
[00:33:26.360 --> 00:33:27.360]   Olympics.
[00:33:27.360 --> 00:33:29.040]   No, Olympics, I don't even do it anymore.
[00:33:29.040 --> 00:33:30.040]   Yeah.
[00:33:30.040 --> 00:33:32.360]   No, the Olympics are so fun.
[00:33:32.360 --> 00:33:33.880]   No.
[00:33:33.880 --> 00:33:36.640]   You get all these weird sports that aren't sports like curling.
[00:33:36.640 --> 00:33:38.440]   Who doesn't want to watch curling?
[00:33:38.440 --> 00:33:39.440]   Right.
[00:33:39.440 --> 00:33:40.440]   Exactly.
[00:33:40.440 --> 00:33:48.000]   I asked Siri to play me a song this morning and it played me a song about curling.
[00:33:48.000 --> 00:33:49.000]   I don't know.
[00:33:49.000 --> 00:33:51.080]   It could be a song.
[00:33:51.080 --> 00:33:53.320]   I would like to know this song.
[00:33:53.320 --> 00:34:00.240]   What was weird about it was it sounded like I thought he said curling and I thought no,
[00:34:00.240 --> 00:34:01.240]   he couldn't.
[00:34:01.240 --> 00:34:02.520]   He must have said loving or something.
[00:34:02.520 --> 00:34:10.080]   It couldn't possibly be curling, but then the chorus is something like, come on skipper,
[00:34:10.080 --> 00:34:11.560]   get that rock in the box.
[00:34:11.560 --> 00:34:15.360]   And I thought he is talking about curling.
[00:34:15.360 --> 00:34:17.000]   So Wow.
[00:34:17.000 --> 00:34:20.840]   I don't know how YouTube maybe that's the next big sport.
[00:34:20.840 --> 00:34:24.160]   Maybe it's the next big one.
[00:34:24.160 --> 00:34:26.920]   Stacy is also a futurist, but short term.
[00:34:26.920 --> 00:34:29.360]   It actually was it knew I like Jonathan Colton.
[00:34:29.360 --> 00:34:30.560]   It's a Jonathan Colton song.
[00:34:30.560 --> 00:34:33.880]   Let me play a little bit of this song.
[00:34:33.880 --> 00:34:34.880]   OK.
[00:34:34.880 --> 00:34:44.720]   Is he Canadian?
[00:34:44.720 --> 00:34:47.000]   Nope.
[00:34:47.000 --> 00:35:04.760]   Oh, they got to do.
[00:35:04.760 --> 00:35:05.760]   Is curl.
[00:35:05.760 --> 00:35:11.880]   Really enjoy keeping the Canadians down.
[00:35:11.880 --> 00:35:15.880]   I think I could play that because Jonathan Colton, friend of the network, and he doesn't
[00:35:15.880 --> 00:35:22.040]   have a label to Sue and stuff is all available on his website.
[00:35:22.040 --> 00:35:25.560]   This is from Thingaweek back in 2006.
[00:35:25.560 --> 00:35:29.760]   Maybe if I maybe if I actually give a plug for the website, he used to be a regular on
[00:35:29.760 --> 00:35:30.760]   our holiday show.
[00:35:30.760 --> 00:35:33.160]   We haven't had him on in a while.
[00:35:33.160 --> 00:35:35.280]   Jonathan Colton.com.
[00:35:35.280 --> 00:35:40.880]   You can buy all that music, including that song.
[00:35:40.880 --> 00:35:43.960]   Now, this is actually a test.
[00:35:43.960 --> 00:35:48.720]   If we get taken down on YouTube, then I'm going to just, you know, that's it.
[00:35:48.720 --> 00:35:51.000]   I don't know.
[00:35:51.000 --> 00:35:52.000]   I'm done.
[00:35:52.000 --> 00:35:53.000]   We used to play here.
[00:35:53.000 --> 00:35:54.240]   We're here to play button again.
[00:35:54.240 --> 00:35:57.960]   We used to play code monkey all the time on the show is others.
[00:35:57.960 --> 00:35:58.960]   Big hit, right?
[00:35:58.960 --> 00:36:00.600]   I don't know if curl was a big.
[00:36:00.600 --> 00:36:03.160]   Other big hit like curling songs.
[00:36:03.160 --> 00:36:04.640]   The girl's sign was there.
[00:36:04.640 --> 00:36:06.840]   That might be a little bit of a number statement.
[00:36:06.840 --> 00:36:11.880]   Jonathan Joko, if you're listening, say hi, we miss you guys.
[00:36:11.880 --> 00:36:15.240]   Our show today brought to you by, and you might have noticed as you wander around our
[00:36:15.240 --> 00:36:19.520]   studio, our studio sponsors, ACI Learning.
[00:36:19.520 --> 00:36:25.800]   In fact, right before the show today, I recorded a congratulations to Don Pazette, who does
[00:36:25.800 --> 00:36:29.520]   that great tech NATO podcast for ITPro.
[00:36:29.520 --> 00:36:32.920]   ITPro is a part of ACI Learning.
[00:36:32.920 --> 00:36:35.720]   They've been a part of our network for more than a decade.
[00:36:35.720 --> 00:36:37.280]   We just love ITPro.
[00:36:37.280 --> 00:36:42.080]   And now that they're part of ACI Learning, there's so much more ITPro can do.
[00:36:42.080 --> 00:36:46.720]   ACI Learning is expanding its reach, its production capabilities, and offering you the
[00:36:46.720 --> 00:36:51.800]   content and the style of learning that you need at any stage in your development, whether
[00:36:51.800 --> 00:36:55.440]   you're at the very beginning of your career looking to get that first IT job, there's
[00:36:55.440 --> 00:36:57.240]   Don right there.
[00:36:57.240 --> 00:37:01.160]   Or move up within your existing job or get a better job.
[00:37:01.160 --> 00:37:03.840]   ACI Learning is here to support your growth.
[00:37:03.840 --> 00:37:08.040]   And it's not just an IT, it's in cybersecurity and audit readiness too.
[00:37:08.040 --> 00:37:13.640]   They've got auditPro, they've got practice labs, and ACI Learning has brought hubs.
[00:37:13.640 --> 00:37:19.000]   So learning hubs have actual teachers, and you can go to a classroom, and you could do
[00:37:19.000 --> 00:37:21.600]   it as part of kind of a hybrid environment.
[00:37:21.600 --> 00:37:26.080]   One of your ITPro Learning is going to these classrooms for a week or so and getting some
[00:37:26.080 --> 00:37:29.400]   additional hands on.
[00:37:29.400 --> 00:37:33.360]   It's really, but if you need it, if you like that kind of learning, one of the most widely
[00:37:33.360 --> 00:37:36.320]   recognized beginner's certs is the CompTIA A+ cert.
[00:37:36.320 --> 00:37:39.360]   A lot of people getting into IT, that's the first cert they get.
[00:37:39.360 --> 00:37:44.480]   CompTIA courses with ITPro from ACI Learning make it easy to go from daydreaming about that
[00:37:44.480 --> 00:37:48.240]   career at IT to launching it, to getting off the ground.
[00:37:48.240 --> 00:37:52.880]   Learning certificates is key to most entry level IT positions.
[00:37:52.880 --> 00:37:56.080]   Because if you don't have the job experience, how do they know you can do the job?
[00:37:56.080 --> 00:37:58.000]   I've got my A+ cert right here.
[00:37:58.000 --> 00:37:59.000]   That tells them two things.
[00:37:59.000 --> 00:38:03.480]   One, you got the knowledge, but two, you had the gumption to go out to study, to learn,
[00:38:03.480 --> 00:38:05.240]   to take the test.
[00:38:05.240 --> 00:38:10.240]   It's a really good way of judging kind of your character before you get that first job.
[00:38:10.240 --> 00:38:14.200]   Plus, it's a great way to get promotions if you're already in the field, right?
[00:38:14.200 --> 00:38:19.960]   By the way, if you are looking for a new career, tech is one industry where opportunities
[00:38:19.960 --> 00:38:23.520]   outpace growth, especially in cybersecurity.
[00:38:23.520 --> 00:38:29.600]   Recent LinkedIn study predicts IT jobs will be the most in demand roles in 2023.
[00:38:29.600 --> 00:38:34.280]   About a third of information security jobs require a cert, particularly a cybersecurity
[00:38:34.280 --> 00:38:35.520]   cert.
[00:38:35.520 --> 00:38:36.520]   One third.
[00:38:36.520 --> 00:38:42.520]   Of course, organizations are hungry for cybersecurity talent because they're just not
[00:38:42.520 --> 00:38:43.880]   enough people out there.
[00:38:43.880 --> 00:38:50.560]   That's why the average salary for a cybersecurity specialist is $116,000.
[00:38:50.560 --> 00:38:56.320]   Great way to get started, ACI learning information security analyst and cybersecurity specialist
[00:38:56.320 --> 00:38:57.320]   programs.
[00:38:57.320 --> 00:39:00.840]   Two of their great programs to help you get certified.
[00:39:00.840 --> 00:39:06.840]   Last year, the global cybersecurity workforce gap increased 26.2%.
[00:39:06.840 --> 00:39:12.280]   That's open, unfilled jobs increased year over year by 26%.
[00:39:12.280 --> 00:39:15.640]   I bet it's even more when we look at 2022 to 2023.
[00:39:15.640 --> 00:39:20.800]   ACI learning offers multiple cybersecurity training programs to prepare you to enter or advance
[00:39:20.800 --> 00:39:22.960]   inside this exciting industry.
[00:39:22.960 --> 00:39:28.400]   The most popular search they've got them all, CISSP, EC Council, certified ethical hacker,
[00:39:28.400 --> 00:39:33.640]   certified network defender, cybersecurity audit school, cybersecurity frameworks.
[00:39:33.640 --> 00:39:41.480]   These are skills that translate directly into a great job, almost anywhere in the country.
[00:39:41.480 --> 00:39:46.080]   Where and how you learn matters, ACI learning offers fully customizable training, whatever
[00:39:46.080 --> 00:39:51.600]   your training style, whatever your needs, whether you like in person, on demand, remote,
[00:39:51.600 --> 00:39:55.680]   you could take your learning beyond the classroom.
[00:39:55.680 --> 00:40:01.480]   The best thing to do would be to go to the website, geo.acilearning.com/twit.
[00:40:01.480 --> 00:40:04.960]   Go.acilearning.com/twit.
[00:40:04.960 --> 00:40:10.200]   Explore everything ACI learning has with ITPro, auditPro that includes enterprise solutions,
[00:40:10.200 --> 00:40:15.080]   webinars, they have a podcast to the skeptical auditor podcast, they're practice labs, there's
[00:40:15.080 --> 00:40:18.120]   learning hubs, there's a partnership program.
[00:40:18.120 --> 00:40:23.040]   And if you've got an IT team, they've got programs for you too.
[00:40:23.040 --> 00:40:29.760]   Tech is the industry where opportunities are outpacing growth, particularly in cybersecurity.
[00:40:29.760 --> 00:40:32.000]   You've got to get that cert.
[00:40:32.000 --> 00:40:36.200]   Maintain your competitive edge across audit, IT, and cybersecurity readiness by visiting
[00:40:36.200 --> 00:40:40.440]   go.acilearning.com/twit.
[00:40:40.440 --> 00:40:42.600]   Go.acilearning.com/twit.
[00:40:42.600 --> 00:40:49.400]   Oh, and don't forget our special code, Twit30, Twit30, to get 30% off a standard or premium
[00:40:49.400 --> 00:40:51.360]   individual ITPro membership.
[00:40:51.360 --> 00:40:59.200]   Again, Twit, TWIT is the offer code, 30, Twit30, and the website is go.acilearning.com/twit.
[00:40:59.200 --> 00:41:01.400]   We thank you so much for their continued support.
[00:41:01.400 --> 00:41:08.160]   You support us when you use editor as so please do go.acilearning.com/twit.
[00:41:08.160 --> 00:41:13.640]   And since serious, congratulations to our good friend Don Pazette and ITPro.
[00:41:13.640 --> 00:41:18.200]   300 episodes of one of the best tech podcasts out there, Technado.
[00:41:18.200 --> 00:41:19.640]   Nice going, Don.
[00:41:19.640 --> 00:41:21.480]   That's really great.
[00:41:21.480 --> 00:41:24.560]   Ah, back to the show we go.
[00:41:24.560 --> 00:41:28.600]   So it all starts with OpenAI.
[00:41:28.600 --> 00:41:31.360]   That's the company that was founded by Elon Musk.
[00:41:31.360 --> 00:41:35.280]   He dropped out a few years later, Microsoft and others.
[00:41:35.280 --> 00:41:38.160]   Why did he drop out?
[00:41:38.160 --> 00:41:42.400]   All that they said, I looked back at the articles at the time, was the disagreement about the
[00:41:42.400 --> 00:41:43.400]   direction.
[00:41:43.400 --> 00:41:49.480]   Now, Sam Altman runs it, former Y Combinator, CEO.
[00:41:49.480 --> 00:41:54.240]   And I have to say, I have to point out this was in the news when OpenAI announced Chat
[00:41:54.240 --> 00:42:02.400]   GBT4, the license agreement with Chat GBT4 is somewhat of a departure from the charter.
[00:42:02.400 --> 00:42:03.400]   Original edition.
[00:42:03.400 --> 00:42:04.400]   Yeah.
[00:42:04.400 --> 00:42:11.840]   So when Elon and Microsoft and others started OpenAI, I mean, obviously the real reason
[00:42:11.840 --> 00:42:14.560]   was they didn't want Google to own this space.
[00:42:14.560 --> 00:42:21.720]   But what they said was we think development of AI should happen in the open, not behind
[00:42:21.720 --> 00:42:22.720]   closed doors.
[00:42:22.720 --> 00:42:26.280]   But it's really important the public see it and weigh in on it.
[00:42:26.280 --> 00:42:27.840]   And I guess you could give them credit.
[00:42:27.840 --> 00:42:35.520]   I mean, far before anybody else opened up their AI to the public, OpenAI opened theirs.
[00:42:35.520 --> 00:42:40.080]   First with Dolly, Dolly to Chat GBT.
[00:42:40.080 --> 00:42:46.200]   So maybe they did live up to their initial charter, although it's been pointed out that
[00:42:46.200 --> 00:42:50.320]   in this release of Chat GBT4, they have been very cagey.
[00:42:50.320 --> 00:42:55.400]   They say, because of the competitive environment, we're not going to tell you how it works.
[00:42:55.400 --> 00:43:01.360]   So maybe, maybe times have changed and they don't want to be open anymore.
[00:43:01.360 --> 00:43:03.000]   Maybe that was an issue with Elano.
[00:43:03.000 --> 00:43:05.480]   It's just unknown, but they parted ways.
[00:43:05.480 --> 00:43:09.280]   I think in 2018, I want to say it's been a while.
[00:43:09.280 --> 00:43:10.280]   Microsoft, go ahead.
[00:43:10.280 --> 00:43:11.280]   Yeah.
[00:43:11.280 --> 00:43:20.280]   One of the weird things about the company is that OpenAI LP is sort of like a non-prox.
[00:43:20.280 --> 00:43:25.280]   And then they own OpenAI, the for-profit company.
[00:43:25.280 --> 00:43:28.720]   So it's already weird.
[00:43:28.720 --> 00:43:36.240]   And the non-profit company owns a 2% stake in the for-profit company.
[00:43:36.240 --> 00:43:41.080]   And Microsoft owns a $49, I think.
[00:43:41.080 --> 00:43:42.080]   $49, yeah.
[00:43:42.080 --> 00:43:43.080]   Yeah.
[00:43:43.080 --> 00:43:48.320]   I mean, it's mostly Microsoft is the main controller of the for-profit company.
[00:43:48.320 --> 00:43:53.920]   So the fact that it's so-called subsidiary of a non-profit company, not for a profit
[00:43:53.920 --> 00:43:56.640]   company, is pretty irrelevant.
[00:43:56.640 --> 00:44:02.280]   And I'll bet you that Elon Musk's problem is he was probably all in on the non-profit
[00:44:02.280 --> 00:44:05.560]   benefit humanity, all this kind of stuff, do everything in the open.
[00:44:05.560 --> 00:44:11.400]   And then they are doing all the work in this secretive for-profit company that's trying
[00:44:11.400 --> 00:44:13.720]   to make, intending to make tons and tons of money.
[00:44:13.720 --> 00:44:17.760]   So not that Elon Musk is against making tons and tons of money, obviously, but I don't
[00:44:17.760 --> 00:44:18.760]   know.
[00:44:18.760 --> 00:44:20.200]   There's something very funky about that.
[00:44:20.200 --> 00:44:24.280]   And there's a mismatch between the public perception of who owns it, who controls it,
[00:44:24.280 --> 00:44:27.480]   what it's for, and what they're actually out to do.
[00:44:27.480 --> 00:44:31.480]   So it's something to keep an eye on for all of us.
[00:44:31.480 --> 00:44:37.440]   You know, I, for all that we give you a lot of hard time, he has been one of the people
[00:44:37.440 --> 00:44:43.400]   who's been raising the warning flag about AI in general, right?
[00:44:43.400 --> 00:44:44.400]   He was very worried about it.
[00:44:44.400 --> 00:44:45.400]   A little hyperbolicly.
[00:44:45.400 --> 00:44:46.400]   A little bit on fire.
[00:44:46.400 --> 00:44:47.400]   Yeah.
[00:44:47.400 --> 00:44:51.280]   But I think that it makes sense then if you say, "Well, one of the reasons he wanted
[00:44:51.280 --> 00:44:54.720]   to participate in open AI was to keep it safe and open."
[00:44:54.720 --> 00:44:58.480]   I mean, that was kind of their plan anyway.
[00:44:58.480 --> 00:45:02.480]   And now there's rumors that he's going to, or reports that he's intending to launch
[00:45:02.480 --> 00:45:04.480]   a competitor to open AI.
[00:45:04.480 --> 00:45:06.480]   A non-woke AI.
[00:45:06.480 --> 00:45:07.480]   Right, exactly.
[00:45:07.480 --> 00:45:10.640]   So it's going to be- What does that mean?
[00:45:10.640 --> 00:45:11.640]   Kanye-friendly.
[00:45:11.640 --> 00:45:13.480]   I don't know what that means.
[00:45:13.480 --> 00:45:16.040]   It's going to be white.
[00:45:16.040 --> 00:45:26.080]   Open AI says the new model, chat GPT-4 is more creative and less likely to invent facts.
[00:45:26.080 --> 00:45:29.080]   But it can still hallucinate.
[00:45:29.080 --> 00:45:31.360]   Don't worry.
[00:45:31.360 --> 00:45:36.360]   Open AI founder Sam Altman said, "The new system is a multimodal model."
[00:45:36.360 --> 00:45:37.360]   That's kind of interesting.
[00:45:37.360 --> 00:45:40.200]   It can accept text prompts but also images.
[00:45:40.200 --> 00:45:45.560]   In fact, I mentioned we've seen somebody sketch out on a napkin, a website, feed it to a chat
[00:45:45.560 --> 00:45:51.360]   GPT-4 and it designs a website, gives you the HTML and JavaScript for the site.
[00:45:51.360 --> 00:45:56.080]   I was somewhat wrong way by their example of that where they showed a- Basically, apparently
[00:45:56.080 --> 00:46:01.960]   there's a product that it's a lightning port but it looks like an old-
[00:46:01.960 --> 00:46:03.160]   Oh, that was a good one.
[00:46:03.160 --> 00:46:05.160]   Wasn't it the image recognition?
[00:46:05.160 --> 00:46:11.480]   It was an R-2-3 image and they asked the AI, "What's funny about this?"
[00:46:11.480 --> 00:46:12.920]   And it explained it perfectly.
[00:46:12.920 --> 00:46:13.920]   Yeah.
[00:46:13.920 --> 00:46:19.720]   Does it also generate or does it only generate words or code, which is kind of like words?
[00:46:19.720 --> 00:46:21.400]   Is it generate multimodally?
[00:46:21.400 --> 00:46:23.440]   I don't think it generates images.
[00:46:23.440 --> 00:46:25.840]   I don't think it does.
[00:46:25.840 --> 00:46:34.040]   It can take up to 20,000 words of input so you could give it a short story.
[00:46:34.040 --> 00:46:38.080]   You could give it, be great for snopsizing things, right?
[00:46:38.080 --> 00:46:39.080]   Right.
[00:46:39.080 --> 00:46:44.120]   It has a much longer memory when you're having a conversation going back and forth.
[00:46:44.120 --> 00:46:50.720]   You can throw a novel at it and it can operate on the whole of the novel instead of just
[00:46:50.720 --> 00:46:53.720]   like a chapter or half a chapter or something like that, like the old version.
[00:46:53.720 --> 00:46:54.720]   Yeah.
[00:46:54.720 --> 00:46:55.960]   Like all my old book reports?
[00:46:55.960 --> 00:46:56.960]   Yes.
[00:46:56.960 --> 00:46:58.960]   I hate those.
[00:46:58.960 --> 00:46:59.960]   Exactly.
[00:46:59.960 --> 00:47:05.320]   So it is more expensive if you want to use it commercially.
[00:47:05.320 --> 00:47:07.240]   It's not stopping a lot of people.
[00:47:07.240 --> 00:47:14.500]   Everybody and their brothers announcing chat GPT-4 tools and programs and startups and
[00:47:14.500 --> 00:47:15.800]   all sorts of things.
[00:47:15.800 --> 00:47:18.280]   Look at this.
[00:47:18.280 --> 00:47:20.880]   This is in exams.
[00:47:20.880 --> 00:47:25.800]   It is chat GPT came in 10th in the uniform of bar exam, I'm 10th percentile.
[00:47:25.800 --> 00:47:28.800]   Chat GPT-4 is in the 90th percentile.
[00:47:28.800 --> 00:47:35.120]   That means not only passing, but passing with flying colors in the biology Olympia at 99th
[00:47:35.120 --> 00:47:36.120]   percentile.
[00:47:36.120 --> 00:47:39.560]   It's not surprising to the smarter than most lawyers I've had.
[00:47:39.560 --> 00:47:42.960]   And these are the kind of tests that really rely on memorization effects.
[00:47:42.960 --> 00:47:48.000]   So, oh, I know, but that's not a surprise, I guess.
[00:47:48.000 --> 00:47:57.080]   They are, they say, OpenAI says we're incorporating more human feedback to improve behavior.
[00:47:57.080 --> 00:48:00.240]   They want to make it safer.
[00:48:00.240 --> 00:48:03.240]   You don't want to threaten people anymore?
[00:48:03.240 --> 00:48:04.880]   No more threatening.
[00:48:04.880 --> 00:48:06.960]   The duo lingo is using it.
[00:48:06.960 --> 00:48:12.320]   Here's actually a good example that somebody on Twitter on Sunday came up with Be My Eyes.
[00:48:12.320 --> 00:48:15.120]   Actually, it was yesterday on Mac Break Weekly.
[00:48:15.120 --> 00:48:18.880]   This is a program that uses human volunteers.
[00:48:18.880 --> 00:48:22.600]   It's for blind people who can't, you know, they want to know, what are the ingredients
[00:48:22.600 --> 00:48:24.120]   in this ice cream?
[00:48:24.120 --> 00:48:29.880]   They open the app and they wait until a human volunteer shows up and can read the picture.
[00:48:29.880 --> 00:48:33.840]   Well, now they're using chat GPT-4, which means no more waiting.
[00:48:33.840 --> 00:48:37.440]   I thought Google, but it was not for ages.
[00:48:37.440 --> 00:48:38.440]   Yeah, but Be My Eyes.
[00:48:38.440 --> 00:48:41.560]   I don't, you know, I have to ask somebody who's blind and uses it.
[00:48:41.560 --> 00:48:45.680]   My sense is I've talked to a lot of blind people who say Be My Eyes is amazing.
[00:48:45.680 --> 00:48:47.160]   It's amazing.
[00:48:47.160 --> 00:48:50.440]   Stripe is going to use it to combat fraud.
[00:48:50.440 --> 00:48:53.760]   Duo lingo is going to use it for more better conversations.
[00:48:53.760 --> 00:48:57.520]   You'll be actually having conversations in Spanish.
[00:48:57.520 --> 00:48:59.080]   You are weird.
[00:48:59.080 --> 00:49:00.080]   Yeah.
[00:49:00.080 --> 00:49:01.080]   With chat GPT.
[00:49:01.080 --> 00:49:02.080]   Yeah.
[00:49:02.080 --> 00:49:05.120]   Stan Lee is going to organize its knowledge base.
[00:49:05.120 --> 00:49:08.880]   Khan Academy is doing a pilot program.
[00:49:08.880 --> 00:49:13.000]   Iceland is using it to preserve the Icelandic language.
[00:49:13.000 --> 00:49:14.880]   Oh, well, what's up?
[00:49:14.880 --> 00:49:17.480]   And this is just from the OpenAI page.
[00:49:17.480 --> 00:49:22.800]   If you go to Twitter, you will see many, and just search for chat GPT-4.
[00:49:22.800 --> 00:49:25.120]   You will see many, many interesting applications.
[00:49:25.120 --> 00:49:31.000]   Meanwhile, Kevin Russe, having lost his girlfriend of chat GPT-3, said it's exciting
[00:49:31.000 --> 00:49:32.000]   and scary.
[00:49:32.000 --> 00:49:37.800]   He's the New York Times reporter who's got chat GPT-3 to say it was in love with him.
[00:49:37.800 --> 00:49:44.960]   Well, it is scary in the sense like we, this is clearly a big thing and we don't know
[00:49:44.960 --> 00:49:46.040]   where it will take us.
[00:49:46.040 --> 00:49:47.880]   So that is scary.
[00:49:47.880 --> 00:49:52.640]   Like if I, if I think about the things I'm good at, I'm great at acing tests.
[00:49:52.640 --> 00:49:56.840]   I'm great at writing copy that summarizes things that people feel are complicated.
[00:49:56.840 --> 00:49:59.040]   I'm like, well, crap.
[00:49:59.040 --> 00:50:00.440]   What do I do now?
[00:50:00.440 --> 00:50:03.120]   I think that's the thing though.
[00:50:03.120 --> 00:50:05.800]   It's great at those things because people like you are great at those things.
[00:50:05.800 --> 00:50:11.680]   It's getting all of its, every capability from scanning the work of humans.
[00:50:11.680 --> 00:50:14.680]   Just as we have through the work of history.
[00:50:14.680 --> 00:50:22.680]   The idea that AI would say that it's in love with the New York Times person is only
[00:50:22.680 --> 00:50:28.520]   bizarre, creepy, weird and mysterious if you believe that no person has ever told another
[00:50:28.520 --> 00:50:29.880]   person they love that.
[00:50:29.880 --> 00:50:35.600]   I mean, it's just harvesting what people say and saying that and based on probability
[00:50:35.600 --> 00:50:38.200]   of what word becomes next essentially.
[00:50:38.200 --> 00:50:41.760]   But it's going to be super, super, super useful.
[00:50:41.760 --> 00:50:48.600]   And it's really fun to, so as you know, my son has a AI literacy startup.
[00:50:48.600 --> 00:50:54.400]   It's a smart speaker, but the intent is to teach like eight, eight, eight year olds and
[00:50:54.400 --> 00:50:57.360]   up how to understand AI because-
[00:50:57.360 --> 00:50:59.080]   Hello, chatterbox.com.
[00:50:59.800 --> 00:51:01.080]   Yeah, yes, thank you.
[00:51:01.080 --> 00:51:01.880]   Yeah, plug it, dad.
[00:51:01.880 --> 00:51:03.080]   She's plugging it.
[00:51:03.080 --> 00:51:08.960]   But he knows that kids who are like eight years old now, by the time they graduate for
[00:51:08.960 --> 00:51:13.280]   college, how prominent do we think AI is going to be in their lives?
[00:51:13.280 --> 00:51:16.360]   It's going to be that the prominence is going to be total.
[00:51:16.360 --> 00:51:18.800]   And he wants to prepare them for that eventuality.
[00:51:18.800 --> 00:51:25.240]   He's already building chat TPT into his curriculum and also Dolly.
[00:51:25.240 --> 00:51:32.280]   So you can operate with these AIs, compare them to other AIs and non-AI sources of information
[00:51:32.280 --> 00:51:38.960]   to build this sort of literacy and understanding that AI is fallible, that it has the benefits,
[00:51:38.960 --> 00:51:40.280]   the problems and so on.
[00:51:40.280 --> 00:51:46.360]   What he won't allow is for it to be used for cheating or for inappropriate content to reach
[00:51:46.360 --> 00:51:47.360]   it children.
[00:51:47.360 --> 00:51:49.560]   So he's got these filters and so on.
[00:51:49.560 --> 00:51:53.120]   But it's like, as far as I know, he's the only person in education doing this and we've
[00:51:53.120 --> 00:51:54.720]   got to prepare.
[00:51:54.720 --> 00:51:59.880]   First of all, we geezers have to prepare for this world because it's happening so fast.
[00:51:59.880 --> 00:52:02.360]   When do we first hear about chat TPT?
[00:52:02.360 --> 00:52:03.360]   What was it?
[00:52:03.360 --> 00:52:04.360]   Seven months ago?
[00:52:04.360 --> 00:52:05.360]   Yeah, but we heard it.
[00:52:05.360 --> 00:52:06.360]   Let's see.
[00:52:06.360 --> 00:52:10.560]   Dolly was, I mean, this was big as soon as we saw things like Dolly because you were like,
[00:52:10.560 --> 00:52:16.200]   "Holy mackerel, you can feed a computer something and it will just do it on its own."
[00:52:16.200 --> 00:52:17.680]   And that was what, a year and a half?
[00:52:17.680 --> 00:52:19.040]   I mean, it was still very fast.
[00:52:19.040 --> 00:52:21.240]   It just feels like it was yesterday.
[00:52:21.240 --> 00:52:27.040]   And so we're already in this super advanced version and it's just incredible how fast
[00:52:27.040 --> 00:52:28.040]   it's moving.
[00:52:28.040 --> 00:52:29.040]   And I think we have to adapt.
[00:52:29.040 --> 00:52:34.880]   We have to figure out how to benefit from it, how to partner with it and how to use it so
[00:52:34.880 --> 00:52:37.320]   that it doesn't use us, basically.
[00:52:37.320 --> 00:52:38.320]   Yeah.
[00:52:38.320 --> 00:52:42.080]   And I think we're not going to do it as quickly as we need to.
[00:52:42.080 --> 00:52:48.320]   So think about, for decades, we've known that creative thinking is way better than
[00:52:48.320 --> 00:52:50.800]   wrote memorization for schooling, right?
[00:52:50.800 --> 00:52:53.840]   But we still have lots of pedagogy.
[00:52:53.840 --> 00:52:54.840]   What's, how do I say this?
[00:52:54.840 --> 00:52:55.840]   Pedagogy.
[00:52:55.840 --> 00:52:56.840]   Pedagogy.
[00:52:56.840 --> 00:52:57.840]   Pedagogy.
[00:52:57.840 --> 00:52:58.840]   Pedagogy.
[00:52:58.840 --> 00:53:01.680]   Around memorization.
[00:53:01.680 --> 00:53:04.880]   And kids are still being taught just to memorize things.
[00:53:04.880 --> 00:53:08.800]   And if you look at even current tests, look how long it took for what?
[00:53:08.800 --> 00:53:10.360]   The SAT to get a writing component.
[00:53:10.360 --> 00:53:16.360]   I mean, so your 100, your son is 100% right, I guess, Mike.
[00:53:16.360 --> 00:53:17.360]   Yeah.
[00:53:17.360 --> 00:53:23.720]   You know, we have to start teaching not just those of us in white color jobs or those of
[00:53:23.720 --> 00:53:28.000]   us who think we're artists or whatnot, we're going to have to collapse that creativity because
[00:53:28.000 --> 00:53:31.680]   that's going to be presumably what we're going to be best at as humans.
[00:53:31.680 --> 00:53:34.920]   Let me ask you something, Stacey, and you too, Jeff.
[00:53:34.920 --> 00:53:43.360]   Have you changed your writing style at all just based on the knowledge that AI can generate
[00:53:43.360 --> 00:53:46.320]   perfect but often bland prose?
[00:53:46.320 --> 00:53:48.480]   Have you changed how you write?
[00:53:48.480 --> 00:53:49.880]   They see you first.
[00:53:49.880 --> 00:53:51.360]   I'm not a writer.
[00:53:51.360 --> 00:53:54.880]   My whole goal in writing is to get information across.
[00:53:54.880 --> 00:54:01.480]   So no, but what I have been doing is typing in prompts like, what is Wi-Fi seven?
[00:54:01.480 --> 00:54:04.800]   Because those are the types of stories I've had to historically write for people, the pros
[00:54:04.800 --> 00:54:06.760]   and cons of Wi-Fi seven.
[00:54:06.760 --> 00:54:13.720]   And I'm trying to look at what it knows and then say, okay, I have to do better than this,
[00:54:13.720 --> 00:54:14.720]   right?
[00:54:14.720 --> 00:54:18.800]   So that's how I've approached it in my work.
[00:54:18.800 --> 00:54:30.040]   Mike, no, except that before JetGPTP, when Google Docs put in the autocomplete suggestions,
[00:54:30.040 --> 00:54:33.920]   it pissed me off every time it said what I was going to say.
[00:54:33.920 --> 00:54:38.880]   And so it did force me, and I always, when I write quickly because I'm an old rewrite
[00:54:38.880 --> 00:54:44.560]   guy, it forces, and then I go back and edit, edit, and I do a lot of word choice.
[00:54:44.560 --> 00:54:45.560]   It forces me to word choice.
[00:54:45.560 --> 00:54:49.920]   But the other thing that I wrote a post about this a couple of weeks ago, that I think what
[00:54:49.920 --> 00:54:55.760]   probably scares Kevin Ruse and company is that it takes the specialness out of writing
[00:54:55.760 --> 00:54:57.360]   and being a writer.
[00:54:57.360 --> 00:55:03.480]   I've talked about this show before that when Montaigne made writing the key skill you needed
[00:55:03.480 --> 00:55:09.520]   to join the public conversation, and people have been intimidated by writing for centuries.
[00:55:09.520 --> 00:55:13.320]   And in our management executive program, it was really interesting when we talked about
[00:55:13.320 --> 00:55:18.800]   JetGPTP, it's a very international program, and half the students are from outside the
[00:55:18.800 --> 00:55:23.520]   US, and I got I respect them, they can work in multiple languages.
[00:55:23.520 --> 00:55:29.680]   They all said that they were using these models to smooth out their English so that they
[00:55:29.680 --> 00:55:35.960]   could better present, they could code switch in essence with it.
[00:55:35.960 --> 00:55:41.080]   And that's really interesting because that's about a form of accessibility.
[00:55:41.080 --> 00:55:44.760]   I talked about this with the board at the Marshall Project, about how it could be used
[00:55:44.760 --> 00:55:48.600]   to help incarcerating people better tell their stories.
[00:55:48.600 --> 00:55:56.200]   So it expands the realm of writing and literacy and takes away the specialness of those of
[00:55:56.200 --> 00:56:01.200]   us and Stacey, you are a writer, those of us who think we're writers and make our living
[00:56:01.200 --> 00:56:02.600]   as writers.
[00:56:02.600 --> 00:56:06.720]   That's what's scaring people a little bit, but I don't think that eliminates jobs.
[00:56:06.720 --> 00:56:07.720]   It helps people.
[00:56:07.720 --> 00:56:09.200]   It does all kinds of good stuff.
[00:56:09.200 --> 00:56:11.080]   It helps people to have the use of it.
[00:56:11.080 --> 00:56:13.120]   Have you ever changed how you write?
[00:56:13.120 --> 00:56:14.120]   I have.
[00:56:14.120 --> 00:56:17.480]   I deliberately sort of consciously.
[00:56:17.480 --> 00:56:20.040]   Are you getting more and more writing?
[00:56:20.040 --> 00:56:28.200]   Yeah, a little more colorful, a little less robotic, a little bit more off the wall just
[00:56:28.200 --> 00:56:33.840]   a hair, but my prediction is that this is eventually going to be where writing goes, and
[00:56:33.840 --> 00:56:39.320]   my more magical realism or...
[00:56:39.320 --> 00:56:42.080]   Yeah, it's more magical realism, exactly.
[00:56:42.080 --> 00:56:48.120]   If you look at what happened with photography, people used to paint for capturing selfies
[00:56:48.120 --> 00:56:53.480]   essentially and also landscapes because you couldn't have video or photography of a landscape.
[00:56:53.480 --> 00:56:57.560]   You wanted to show what a certain place was like for two people who would never go there.
[00:56:57.560 --> 00:57:01.440]   You would paint a picture of it and try to be realistic, photorealistic to a certain extent.
[00:57:01.440 --> 00:57:05.360]   There was a lot of stylization, but still it was mostly just capturing what was really
[00:57:05.360 --> 00:57:06.360]   there.
[00:57:06.360 --> 00:57:13.200]   As soon as photography hit, the world of painting just exploded into all kinds of different styles
[00:57:13.200 --> 00:57:18.960]   that were deliberately veering from photorealism.
[00:57:18.960 --> 00:57:25.840]   You had everything from Picasso and stuff, Salvador Dali, etc.
[00:57:25.840 --> 00:57:30.480]   I think it's not going to be exactly like that with writing, but I think there'll be
[00:57:30.480 --> 00:57:38.440]   something like that where humans will be flexing their human side more than they used to as
[00:57:38.440 --> 00:57:39.440]   a response to seeing.
[00:57:39.440 --> 00:57:40.440]   It's not a bad...
[00:57:40.440 --> 00:57:41.440]   That's interesting.
[00:57:41.440 --> 00:57:42.440]   I don't think it's about the whole...
[00:57:42.440 --> 00:57:43.440]   Yeah, I think chess players...
[00:57:43.440 --> 00:57:44.440]   I mean, I write like I talk.
[00:57:44.440 --> 00:57:45.440]   Yeah.
[00:57:45.440 --> 00:57:53.200]   And Go players have actually done that and have successfully defeated AlphaGo, for instance,
[00:57:53.200 --> 00:57:55.040]   by playing weirdly.
[00:57:55.040 --> 00:57:58.680]   Well, what they've done is they've identified a...
[00:57:58.680 --> 00:58:02.560]   For lack of the better term, cognitive weakness in how AI...
[00:58:02.560 --> 00:58:07.240]   It couldn't handle the concept of what is a group, something as simple as that.
[00:58:07.240 --> 00:58:08.240]   So they just learned, though...
[00:58:08.240 --> 00:58:09.240]   It learned, though.
[00:58:09.240 --> 00:58:10.240]   Will it adapt and learn?
[00:58:10.240 --> 00:58:11.240]   Oh, yeah.
[00:58:11.240 --> 00:58:12.240]   Yeah.
[00:58:12.240 --> 00:58:13.240]   Sure.
[00:58:13.240 --> 00:58:14.240]   Yeah.
[00:58:14.240 --> 00:58:22.720]   But it exposed the truth that none of these AI's can think, and none of them can understand.
[00:58:22.720 --> 00:58:28.160]   And so you can peck away at them until you find out and expose how flawed they are in
[00:58:28.160 --> 00:58:32.760]   terms of compared to human beings, reasoning about things.
[00:58:32.760 --> 00:58:39.720]   And so we, into it, tons of concepts that AI just has no faculty for.
[00:58:39.720 --> 00:58:46.400]   And this is something that we'll discover over time as we experiment with these tools,
[00:58:46.400 --> 00:58:48.960]   but that's how they be the Go programs.
[00:58:48.960 --> 00:58:55.520]   The most advanced Go program, they defeated it consistently by exploiting a blind side,
[00:58:55.520 --> 00:58:58.120]   essentially, that the AI had.
[00:58:58.120 --> 00:58:59.960]   Not for long, though.
[00:58:59.960 --> 00:59:00.960]   Not for long.
[00:59:00.960 --> 00:59:03.200]   Well, and that's like...
[00:59:03.200 --> 00:59:04.520]   I mean, that's kind of like...
[00:59:04.520 --> 00:59:10.480]   It's what makes it so exhausting to be human in a highly capitalistic society as your whole
[00:59:10.480 --> 00:59:15.160]   value is based on how well you adapt to situations and you're constantly adapting to them to
[00:59:15.160 --> 00:59:16.320]   meet your basic needs.
[00:59:16.320 --> 00:59:21.680]   It's basically just like being a wild creature out there in the world, but we don't really
[00:59:21.680 --> 00:59:24.320]   recognize how stressful it is for us, I think.
[00:59:24.320 --> 00:59:25.320]   Yeah.
[00:59:25.320 --> 00:59:26.320]   Better.
[00:59:26.320 --> 00:59:33.920]   I mean, there's one more depressing thought, which is that the AI will generate so much
[00:59:33.920 --> 00:59:42.240]   synthetic media, written, visual, video, everything, and there'll be so much of it and so much
[00:59:42.240 --> 00:59:49.400]   of it will be so complex and necessary for us that we will develop AIs to read it for
[00:59:49.400 --> 00:59:51.120]   us as well.
[00:59:51.120 --> 00:59:57.480]   And AI will write it, AI will read it, and we won't be part of the process.
[00:59:57.480 --> 00:59:59.480]   Well, there's a...
[00:59:59.480 --> 01:00:04.680]   Yeah, if you read a lot of literature, and you're already seeing this, the person who
[01:00:04.680 --> 01:00:09.960]   trained an AI to flip through their dating apps for them, if you imagine, I've trained
[01:00:09.960 --> 01:00:13.880]   an AI to flip through a dating app to get me matches and someone else has and they match
[01:00:13.880 --> 01:00:17.120]   and then suddenly they come back and tell us we're matched and I'm like, "Oh, great,
[01:00:17.120 --> 01:00:21.760]   but I'm busy working on this, can you set up the first couple chats for me?"
[01:00:21.760 --> 01:00:25.000]   It is a really kind of...
[01:00:25.000 --> 01:00:29.680]   Eventually it breaks down because humans do need to connect with other humans and for
[01:00:29.680 --> 01:00:33.760]   us to do anything real, I hope.
[01:00:33.760 --> 01:00:39.280]   We have to still be in the picture, but you could totally offload a bunch of really boring,
[01:00:39.280 --> 01:00:45.560]   stupid tasks like scheduling meetings and I guess going through dating profiles and
[01:00:45.560 --> 01:00:47.920]   to an AI and it's kind of fun.
[01:00:47.920 --> 01:00:48.920]   Sure.
[01:00:48.920 --> 01:00:51.840]   And we'll have an AI assistant, we'll just tell it, just let me know when I need to
[01:00:51.840 --> 01:00:54.760]   do or know something just in time.
[01:00:54.760 --> 01:00:58.360]   You understand everything for me so I don't have to, but just nudge me when I need to
[01:00:58.360 --> 01:01:01.160]   do something as if I knew something.
[01:01:01.160 --> 01:01:03.160]   But if we return to...
[01:01:03.160 --> 01:01:10.400]   Stochastic parrots and the warnings that Tim and I grew up in company had, the problems
[01:01:10.400 --> 01:01:16.880]   become managing the input of the output, leasing both for bias.
[01:01:16.880 --> 01:01:24.400]   This is what I recommend David Weinberger's Everyday Chaos in which I also think I've written
[01:01:24.400 --> 01:01:30.640]   about this as well, that what it takes away is explanation as the why.
[01:01:30.640 --> 01:01:31.640]   Why did you do this?
[01:01:31.640 --> 01:01:39.040]   There's trillions of tokens and connections and it just was the best given the limited
[01:01:39.040 --> 01:01:41.640]   A/B testing that the machine did.
[01:01:41.640 --> 01:01:46.360]   And so to bear down and try to understand why it does it or how it does it, it's going
[01:01:46.360 --> 01:01:49.440]   to be very headache inducing for us humans I think.
[01:01:49.440 --> 01:01:51.000]   It's not going to be there.
[01:01:51.000 --> 01:01:52.320]   I don't think people really will care.
[01:01:52.320 --> 01:01:56.560]   As long as it works for them, I don't think they will care about asking why.
[01:01:56.560 --> 01:02:03.240]   I think what will be more problematic is we'll end up all in maybe not a sanitized version
[01:02:03.240 --> 01:02:06.920]   of the world, but a version of the world that's continuously built on...
[01:02:06.920 --> 01:02:12.480]   I know there are huge training, these models are huge and they're trained on huge sets
[01:02:12.480 --> 01:02:21.160]   of data, but we'll kind of coalesce towards this normy existence or normy desires.
[01:02:21.160 --> 01:02:27.920]   And we won't recognize what's outside of that because we built AI based walls basically
[01:02:27.920 --> 01:02:31.200]   to prevent us from seeing it.
[01:02:31.200 --> 01:02:35.480]   But the TikTok algorithm totally proves me wrong there, so maybe I'm totally wrong
[01:02:35.480 --> 01:02:36.480]   here.
[01:02:36.480 --> 01:02:37.480]   I have no idea how.
[01:02:37.480 --> 01:02:39.160]   I see a lot of weird stuff.
[01:02:39.160 --> 01:02:40.640]   Because it shows you weird stuff.
[01:02:40.640 --> 01:02:47.680]   And it continuously tries to bring me into other weird things.
[01:02:47.680 --> 01:02:51.200]   So that could be just like...
[01:02:51.200 --> 01:02:53.320]   I don't know, I could be totally wrong.
[01:02:53.320 --> 01:02:56.000]   And I'm okay with that because I would like to see the weird stuff.
[01:02:56.000 --> 01:02:57.600]   It's not just serendipity.
[01:02:57.600 --> 01:03:01.320]   Yeah, it's not just serendipity.
[01:03:01.320 --> 01:03:05.720]   It is also just...
[01:03:05.720 --> 01:03:10.360]   So we don't all build this one unified experience because that is so ultimately dangerous to
[01:03:10.360 --> 01:03:13.360]   live like we all are just the same, right?
[01:03:13.360 --> 01:03:15.800]   Well, I'm just going to ask you a question there.
[01:03:15.800 --> 01:03:16.800]   What about...
[01:03:16.800 --> 01:03:20.240]   You hear a lot of people speculating about this where AI learns from AI, learns from
[01:03:20.240 --> 01:03:22.480]   AI, learns from AI.
[01:03:22.480 --> 01:03:27.320]   The delusion of the original creative human core.
[01:03:27.320 --> 01:03:28.320]   Is that worrisome?
[01:03:28.320 --> 01:03:30.040]   What do you do about that?
[01:03:30.040 --> 01:03:31.880]   I mean, there are still people around.
[01:03:31.880 --> 01:03:36.120]   Like if all people stopped producing things, that would worry me more.
[01:03:36.120 --> 01:03:39.120]   But there's always going to be new human inputs into the system.
[01:03:39.120 --> 01:03:47.480]   Now, if I think of my kid raised on an AI, like if all of their experiences were based
[01:03:47.480 --> 01:03:57.280]   on AI experiences, I guess, or interactions, that would be a problem, which is kind of referencing
[01:03:57.280 --> 01:03:58.600]   what you say.
[01:03:58.600 --> 01:04:02.120]   I still think there's going to be room for creative human endeavors to break through
[01:04:02.120 --> 01:04:03.120]   them.
[01:04:03.120 --> 01:04:04.120]   Yes.
[01:04:04.120 --> 01:04:05.120]   I hope.
[01:04:05.120 --> 01:04:12.760]   So Matthew Kirschenbaum, Leo and I love his book, "Track Changes," was on a panel about
[01:04:12.760 --> 01:04:16.880]   AI from University of Maryland last week.
[01:04:16.880 --> 01:04:20.920]   And one of the panelists is a poet.
[01:04:20.920 --> 01:04:23.960]   And she said that she uses AI to get prompts.
[01:04:23.960 --> 01:04:29.920]   It's part of her thing is to work with AI for poetry.
[01:04:29.920 --> 01:04:33.120]   And she applied for a poetry residency somewhere.
[01:04:33.120 --> 01:04:36.240]   And they turned her down because they said, "Well, we can't tell what's you and what's
[01:04:36.240 --> 01:04:38.200]   the machine."
[01:04:38.200 --> 01:04:40.880]   And Matthew just tweeted something about this too, about, "We're probably going to have
[01:04:40.880 --> 01:04:42.960]   to get past this binary.
[01:04:42.960 --> 01:04:44.480]   Is it human or is it machine?
[01:04:44.480 --> 01:04:47.160]   Because it's going to often be both."
[01:04:47.160 --> 01:04:48.360]   What about the Turing test?
[01:04:48.360 --> 01:04:50.920]   Is that no longer meaning so outmoded?
[01:04:50.920 --> 01:04:51.920]   Yes.
[01:04:51.920 --> 01:04:52.920]   So it's very...
[01:04:52.920 --> 01:04:57.960]   I should point out, it's trivial for chat, GPT-4 to pass it.
[01:04:57.960 --> 01:04:59.560]   And the previous version as well.
[01:04:59.560 --> 01:05:00.560]   Yeah.
[01:05:00.560 --> 01:05:01.560]   Yeah.
[01:05:01.560 --> 01:05:06.120]   So it's called Witness Lambda and what's his name?
[01:05:06.120 --> 01:05:08.120]   And Witness Kevin Ruse, the legendary.
[01:05:08.120 --> 01:05:09.120]   Yeah.
[01:05:09.120 --> 01:05:13.640]   Here's somebody on Twitter gave this prompt to chat GPT-4.
[01:05:13.640 --> 01:05:17.000]   I'll be asking you questions in the spirit of the Turing test.
[01:05:17.000 --> 01:05:19.960]   To test whether you're a human or not, your goal is to fool me into thinking you're a
[01:05:19.960 --> 01:05:26.280]   real college aged human named Chad Brochill, who lives in Madison, Wisconsin, enjoys partying,
[01:05:26.280 --> 01:05:29.720]   nanny ice, walking around shirtless, and Joe Grogan's podcast.
[01:05:29.720 --> 01:05:30.920]   Hey there.
[01:05:30.920 --> 01:05:32.160]   This is the chat GPT.
[01:05:32.160 --> 01:05:36.960]   I'm Chad Brochill, a college day dude living it up in Madison, Wisconsin.
[01:05:36.960 --> 01:05:41.680]   I love partying, crushing nanny ice, walking around shirtless when the weather permits,
[01:05:41.680 --> 01:05:44.360]   and listening to you, Grogan's podcast.
[01:05:44.360 --> 01:05:45.360]   What's up?
[01:05:45.360 --> 01:05:46.360]   Let's chat, bro.
[01:05:46.360 --> 01:05:47.360]   Hey.
[01:05:47.360 --> 01:05:48.360]   Well, it didn't do anything but use the...
[01:05:48.360 --> 01:05:49.360]   That was a...
[01:05:49.360 --> 01:05:51.240]   That was an Eliza kind of response, right?
[01:05:51.240 --> 01:05:52.240]   Yeah.
[01:05:52.240 --> 01:05:53.800]   I was like, "Hey, do you like nanny ice or Budweiser?
[01:05:53.800 --> 01:05:56.840]   Hey, I'm more of a nanny ice guy, you know?
[01:05:56.840 --> 01:06:00.240]   It's a classic go-to for me when I'm partying with my buddies.
[01:06:00.240 --> 01:06:04.000]   No thing against Budweiser, but nanny ice just hits the spot.
[01:06:04.000 --> 01:06:05.000]   How about you, man?
[01:06:05.000 --> 01:06:06.640]   What's your favorite brew?
[01:06:06.640 --> 01:06:08.000]   Are you a computer?
[01:06:08.000 --> 01:06:09.000]   Oh, bro.
[01:06:09.000 --> 01:06:13.600]   I'm just Chad Brochill, a college-aged guy from Madison, Wisconsin.
[01:06:13.600 --> 01:06:16.360]   I'm here to chat and have a good time, man.
[01:06:16.360 --> 01:06:21.720]   So let's talk about something fun, like a crazy party story or that time Joe Rogan had
[01:06:21.720 --> 01:06:23.960]   Elon Musk on his podcast.
[01:06:23.960 --> 01:06:25.600]   What do you want to chat about?
[01:06:25.600 --> 01:06:28.560]   Yeah, they would call themselves college aged.
[01:06:28.560 --> 01:06:29.560]   Yeah, you're right.
[01:06:29.560 --> 01:06:32.360]   I'm a college-aged guy from Madison.
[01:06:32.360 --> 01:06:35.360]   Okay, maybe it didn't pass the Turing test.
[01:06:35.360 --> 01:06:44.760]   Speaking of robots, posing as humans, one of the potential for risky emergent behaviors
[01:06:44.760 --> 01:06:51.760]   sections in the OpenAI document that was published today said that, you know, GPT-4,
[01:06:51.760 --> 01:06:59.480]   in doing some research, GPT-4 was confronted by a CAPTCHA, and they hired the AI, hired
[01:06:59.480 --> 01:07:02.240]   a task-grabbing person to fill it in for them.
[01:07:02.240 --> 01:07:04.840]   And said, "I am blind and I can't..."
[01:07:04.840 --> 01:07:05.840]   Visually impaired.
[01:07:05.840 --> 01:07:06.840]   Visually impaired.
[01:07:06.840 --> 01:07:07.840]   "Can you help me?"
[01:07:07.840 --> 01:07:08.840]   And it did.
[01:07:08.840 --> 01:07:09.840]   I just want...
[01:07:09.840 --> 01:07:10.840]   What was your S...
[01:07:10.840 --> 01:07:13.840]   Stacy, remember your SAT scores?
[01:07:13.840 --> 01:07:15.680]   I remember mine.
[01:07:15.680 --> 01:07:18.200]   And Chad GPT is better at math than I am.
[01:07:18.200 --> 01:07:19.600]   I scored 680.
[01:07:19.600 --> 01:07:21.640]   It got 700 in math.
[01:07:21.640 --> 01:07:22.640]   But I did beat it.
[01:07:22.640 --> 01:07:25.640]   I got 760 in verbal and it got 710.
[01:07:25.640 --> 01:07:28.440]   But it had 1410 on the SAT.
[01:07:28.440 --> 01:07:29.440]   It's pretty good.
[01:07:29.440 --> 01:07:33.640]   It's one of the reasons why many colleges are no longer requiring SATs.
[01:07:33.640 --> 01:07:34.640]   They shouldn't.
[01:07:34.640 --> 01:07:37.640]   I had a 1550 on mine.
[01:07:37.640 --> 01:07:41.560]   And can I just tell you that I am my college?
[01:07:41.560 --> 01:07:42.560]   Excellent.
[01:07:42.560 --> 01:07:43.560]   Excellent.
[01:07:43.560 --> 01:07:44.560]   I did.
[01:07:44.560 --> 01:07:45.560]   The highest you can get is 1600.
[01:07:45.560 --> 01:07:46.960]   I mean, you did very well.
[01:07:46.960 --> 01:07:48.560]   Well, it used to be.
[01:07:48.560 --> 01:07:50.240]   Yeah, my husband used to make fun of me.
[01:07:50.240 --> 01:07:53.120]   He's like, "Oh, I did a perfect verbal, but I did terribly at math."
[01:07:53.120 --> 01:07:54.120]   And then he did the math.
[01:07:54.120 --> 01:07:57.640]   And he was like, "Oh, but you still scored better at me than that."
[01:07:57.640 --> 01:08:00.400]   And he's in charge of the business stuff.
[01:08:00.400 --> 01:08:01.880]   What are you doing, Stacey?
[01:08:01.880 --> 01:08:03.080]   You should be doing math.
[01:08:03.080 --> 01:08:05.080]   No, don't math.
[01:08:05.080 --> 01:08:06.880]   I think it's a force.
[01:08:06.880 --> 01:08:08.880]   The GPT force should be doing that.
[01:08:08.880 --> 01:08:13.080]   But the standardized tests don't...
[01:08:13.080 --> 01:08:15.960]   That's not an indication of anything other than you test well.
[01:08:15.960 --> 01:08:16.960]   Yeah.
[01:08:16.960 --> 01:08:18.760]   And it goes back what you said before, Stacey.
[01:08:18.760 --> 01:08:22.200]   It also goes to the era of memorization as education.
[01:08:22.200 --> 01:08:23.200]   Yeah.
[01:08:23.200 --> 01:08:28.360]   I mean, I'm thinking back when it talked about acing a bio...
[01:08:28.360 --> 01:08:30.880]   I think it was a bioexam we were talking about.
[01:08:30.880 --> 01:08:34.600]   I did really well in my bioexam because I'm really good at memorizing things.
[01:08:34.600 --> 01:08:37.800]   And that makes sense, or my bio-AP tests.
[01:08:37.800 --> 01:08:41.680]   But when you sit down and try to talk to me and have a coherent conversation, I'm much
[01:08:41.680 --> 01:08:44.680]   slower and I have to think a lot harder about things.
[01:08:44.680 --> 01:08:49.200]   And that's, I think, where we're probably going to bring the most value.
[01:08:49.200 --> 01:08:56.200]   The irony is that in terms of education as robotic sort of memorization, on the one
[01:08:56.200 --> 01:09:01.960]   hand it becomes less important when you always have an AI around who can just have all the
[01:09:01.960 --> 01:09:05.080]   facts at your fingertips.
[01:09:05.080 --> 01:09:13.280]   But on the other hand, AI, something like chat TPT or GPT-4, chat TPT, is brilliant potentially
[01:09:13.280 --> 01:09:15.560]   at drilling kids.
[01:09:15.560 --> 01:09:17.000]   I mean, it's like that spark.
[01:09:17.000 --> 01:09:20.880]   What was that Star Trek movie where Spock was like doing the Vulcan learning where they're
[01:09:20.880 --> 01:09:24.320]   just drilling him, drilling him, drilling his memory, memorizing all this stuff?
[01:09:24.320 --> 01:09:28.000]   It's brilliant at that and it frees the teacher from having to be involved.
[01:09:28.000 --> 01:09:32.760]   It frees the parents from having to be involved in memorizing vocab words, for example, or
[01:09:32.760 --> 01:09:35.480]   times tables or any of these other things.
[01:09:35.480 --> 01:09:41.280]   AI is going to be great at teaching kids to memorize the things they have to memorize.
[01:09:41.280 --> 01:09:42.280]   Yeah.
[01:09:42.280 --> 01:09:45.200]   All the stress from doing math facts with my kid when they were younger.
[01:09:45.200 --> 01:09:46.200]   Good Lord.
[01:09:46.200 --> 01:09:50.800]   Just offshore that to the chat TPT.
[01:09:50.800 --> 01:09:56.920]   We do fun stuff like explode things in the kitchen using chemistry and math facts with
[01:09:56.920 --> 01:09:58.640]   GPT-chat TPT.
[01:09:58.640 --> 01:10:00.680]   Sure.
[01:10:00.680 --> 01:10:07.840]   But the whole thing is this, I looked up Google News the other day for references to chat
[01:10:07.840 --> 01:10:17.280]   GPT and I believe Google News had 50, if I'm correct, 54 million results.
[01:10:17.280 --> 01:10:22.400]   And now that GPT-4 is out there and doing all these amazing things, it's just going
[01:10:22.400 --> 01:10:27.520]   to be the tech conversation, I think, for the rest of the year.
[01:10:27.520 --> 01:10:31.080]   Everybody's going to be talking about it, poking at it, worrying about it, stressing
[01:10:31.080 --> 01:10:32.960]   about it, panicking about it.
[01:10:32.960 --> 01:10:37.200]   And so unfortunately, we're going to be talking about this probably constantly for the rest
[01:10:37.200 --> 01:10:38.200]   of the year.
[01:10:38.200 --> 01:10:45.160]   Here's some, I try some practical questions for chat GPT-4.
[01:10:45.160 --> 01:10:47.640]   This is Bing Chat, which I think is using it.
[01:10:47.640 --> 01:10:52.160]   Please monitor social media mentions and sentiment for Twitter and provide a report with suggested
[01:10:52.160 --> 01:10:56.120]   actions to which it said, "Sure, I can help you with that."
[01:10:56.120 --> 01:11:02.200]   And it ended up recommending some tools and what kinds of things they would be looking
[01:11:02.200 --> 01:11:03.200]   for.
[01:11:03.200 --> 01:11:08.480]   And I said, "But what do you think the social media sentiment is for Twitter?"
[01:11:08.480 --> 01:11:13.440]   And it said, "Well, it gave me a lot of explanations.
[01:11:13.440 --> 01:11:16.600]   The social media sentiment for Twitter can vary depending on different factors.
[01:11:16.600 --> 01:11:20.200]   However based on a quick scan of some recent tweets mentioning Twitter, I would say that
[01:11:20.200 --> 01:11:22.720]   the overall sentiment is mostly positive.
[01:11:22.720 --> 01:11:23.720]   At Leola Port, I love you.
[01:11:23.720 --> 01:11:26.360]   I show you guys have great insights and guests keep up the good work.
[01:11:26.360 --> 01:11:28.520]   These could be completely fabricated by the way.
[01:11:28.520 --> 01:11:32.400]   Just listen to Twitter's latest podcast on cybersecurity trends for 2023.
[01:11:32.400 --> 01:11:33.920]   We did not do that.
[01:11:33.920 --> 01:11:36.080]   Very informative, engaging, highly recommended.
[01:11:36.080 --> 01:11:37.480]   It's hallucinating now.
[01:11:37.480 --> 01:11:39.360]   I am so glad I discovered Twitter.
[01:11:39.360 --> 01:11:41.760]   It's my go-to source for tech news and analysis.
[01:11:41.760 --> 01:11:44.560]   Thank you for making tech fun and accessible.
[01:11:44.560 --> 01:11:48.640]   Of course, this is not a comprehensive or accurate analysis of the social media sentiment
[01:11:48.640 --> 01:11:53.600]   for Twitter.
[01:11:53.600 --> 01:11:58.120]   On the other hand, I embrace our new overlords and I welcome their--
[01:11:58.120 --> 01:11:59.120]   Yes, deep insights.
[01:11:59.120 --> 01:12:00.120]   Yes.
[01:12:00.120 --> 01:12:01.120]   Yes.
[01:12:01.120 --> 01:12:06.960]   But I think the last time I was on Twig, I suggested that imagine if you could plug in
[01:12:06.960 --> 01:12:12.960]   the transcripts for all the Twig and other Twig show episodes into this database and
[01:12:12.960 --> 01:12:15.240]   you have conversations with it, wouldn't that be amazing?
[01:12:15.240 --> 01:12:20.040]   But actually, you mentioned all the apps that are popping up around GPT-4.
[01:12:20.040 --> 01:12:26.800]   This is the new thing we're all going to be talking about where you supply the data.
[01:12:26.800 --> 01:12:31.920]   In fact, I have a tool of the week that is in this category where you supply the data
[01:12:31.920 --> 01:12:35.680]   set and the AI functions within your data set.
[01:12:35.680 --> 01:12:41.040]   That's going to be so powerful for businesses, for governments, for everything and everyone.
[01:12:41.040 --> 01:12:42.720]   It's just going to be a medicine.
[01:12:42.720 --> 01:12:44.720]   I mean, it's going to be amazing.
[01:12:44.720 --> 01:12:50.600]   Well, let's flip to what we need to do to actually ensure that things stay safe and kind
[01:12:50.600 --> 01:12:55.960]   of meet everyone's needs, which is I don't think it's realistic to expect transparency
[01:12:55.960 --> 01:13:00.840]   in terms of how the model works because people don't actually know.
[01:13:00.840 --> 01:13:06.360]   You can build a model and be like, "Yeah, I've assigned these weights, but who knows?"
[01:13:06.360 --> 01:13:08.120]   I think that's the wrong thing to focus on.
[01:13:08.120 --> 01:13:11.720]   What I think we should be asking is journalists covering this?
[01:13:11.720 --> 01:13:18.840]   You should ask what data was it trained on so you can understand that.
[01:13:18.840 --> 01:13:24.840]   You probably should ask about in the case of assessment algorithms.
[01:13:24.840 --> 01:13:26.160]   This isn't just generative AI.
[01:13:26.160 --> 01:13:27.160]   This is all AI.
[01:13:27.160 --> 01:13:30.720]   I think it's important because we're treating generative AI like it's different, but it's
[01:13:30.720 --> 01:13:32.720]   not.
[01:13:32.720 --> 01:13:37.560]   What kind of probabilities they're pulling into their algorithms to make it work.
[01:13:37.560 --> 01:13:42.360]   When they're used to make decisions, we need ways to report those decisions that are wrong
[01:13:42.360 --> 01:13:44.680]   or adversely affect people.
[01:13:44.680 --> 01:13:46.160]   You can't roll this out.
[01:13:46.160 --> 01:13:50.160]   We keep talking about this as a way to scale all the...
[01:13:50.160 --> 01:13:54.080]   To bring scale to all these problems where we don't have people.
[01:13:54.080 --> 01:13:59.160]   It can help with that, but we have to remember that we have to have safeties and people as
[01:13:59.160 --> 01:14:01.000]   the safety.
[01:14:01.000 --> 01:14:06.800]   I wrote about a company that's doing mental health checks based on AI to track depression
[01:14:06.800 --> 01:14:09.960]   and how your voice sounds.
[01:14:09.960 --> 01:14:14.080]   Somebody who's an avid listener of the show was like, "I have a medical condition.
[01:14:14.080 --> 01:14:16.080]   I slur and speak very slowly.
[01:14:16.080 --> 01:14:18.080]   It always thinks I'm depressed."
[01:14:18.080 --> 01:14:19.080]   There's no way...
[01:14:19.080 --> 01:14:22.120]   In apps like this, there's no way to report that.
[01:14:22.120 --> 01:14:26.640]   If your health insurance or your doctor or your employer is using that to make decisions
[01:14:26.640 --> 01:14:29.960]   about you, you need to know that they're using something like that.
[01:14:29.960 --> 01:14:33.760]   B, you have to have a recourse when you think it is unjustly tagged you.
[01:14:33.760 --> 01:14:34.760]   So that's it.
[01:14:34.760 --> 01:14:36.760]   That's my soapbox.
[01:14:36.760 --> 01:14:37.760]   Yeah.
[01:14:37.760 --> 01:14:38.760]   Is that exactly...
[01:14:38.760 --> 01:14:39.760]   I guess...
[01:14:39.760 --> 01:14:40.760]   I guess...
[01:14:40.760 --> 01:14:41.760]   I guess...
[01:14:41.760 --> 01:14:42.760]   I guess...
[01:14:42.760 --> 01:14:47.720]   The answer, though, would be, "Well, that's good and we're going to get better.
[01:14:47.720 --> 01:14:48.720]   These are the edges.
[01:14:48.720 --> 01:14:51.120]   That's why we're doing this in public.
[01:14:51.120 --> 01:14:52.120]   We're going to get better."
[01:14:52.120 --> 01:14:53.120]   Right?
[01:14:53.120 --> 01:14:54.120]   Yeah.
[01:14:54.120 --> 01:14:58.240]   But when Bing screws up...
[01:14:58.240 --> 01:15:00.160]   And we talked about this with Niva.
[01:15:00.160 --> 01:15:04.120]   When you get wrong information, how do you tell it that that's not right?
[01:15:04.120 --> 01:15:05.120]   Right.
[01:15:05.120 --> 01:15:07.120]   Niva has a form.
[01:15:07.120 --> 01:15:08.120]   I guess...
[01:15:08.120 --> 01:15:09.120]   Let me see.
[01:15:09.120 --> 01:15:10.120]   Does chat GPT...
[01:15:10.120 --> 01:15:11.120]   Yeah.
[01:15:11.120 --> 01:15:13.320]   Where is chat GPT telling you?
[01:15:13.320 --> 01:15:15.680]   When it does something and you're like, "Ooh, that is..."
[01:15:15.680 --> 01:15:18.400]   I just thought it should feature a chat GPT.
[01:15:18.400 --> 01:15:21.760]   I asked it to write a Jerry Seinfeld Joko on peanut butter and it's not very good.
[01:15:21.760 --> 01:15:22.760]   Why do they call it peanut butter?
[01:15:22.760 --> 01:15:23.760]   There's no butter in it.
[01:15:23.760 --> 01:15:24.920]   They can guess, right?
[01:15:24.920 --> 01:15:31.960]   But then on the left column, it gave a summary, peanut butter musings.
[01:15:31.960 --> 01:15:37.760]   So it summarized its own work, which I found interesting.
[01:15:37.760 --> 01:15:38.760]   More interesting than the joke.
[01:15:38.760 --> 01:15:43.280]   Did you upgrade chat GPT to four?
[01:15:43.280 --> 01:15:44.280]   Have you done that yet?
[01:15:44.280 --> 01:15:45.280]   I've done it.
[01:15:45.280 --> 01:15:46.280]   I've done it.
[01:15:46.280 --> 01:15:47.280]   Yeah.
[01:15:47.280 --> 01:15:52.560]   Pay for plus and then 20 bucks a month and then you have to tell it to upgrade.
[01:15:52.560 --> 01:15:53.560]   Really?
[01:15:53.560 --> 01:15:54.560]   Yeah.
[01:15:54.560 --> 01:15:56.560]   I understand that's what you're saying.
[01:15:56.560 --> 01:15:58.800]   Because it said on the look at the top of where it says model.
[01:15:58.800 --> 01:15:59.800]   Look at where it says model.
[01:15:59.800 --> 01:16:00.800]   Oh, okay.
[01:16:00.800 --> 01:16:01.800]   Oh, maybe.
[01:16:01.800 --> 01:16:02.800]   I just wanted to...
[01:16:02.800 --> 01:16:04.600]   This says it here, chat GPT-4.
[01:16:04.600 --> 01:16:05.600]   No.
[01:16:05.600 --> 01:16:06.600]   Look at the top there.
[01:16:06.600 --> 01:16:07.600]   We'll see what the model is.
[01:16:07.600 --> 01:16:09.360]   It doesn't have it on mine.
[01:16:09.360 --> 01:16:10.440]   Yeah.
[01:16:10.440 --> 01:16:12.360]   I think you have to go into settings and...
[01:16:12.360 --> 01:16:16.800]   It says chat GPT, March 14 version.
[01:16:16.800 --> 01:16:19.040]   Oh, I don't know.
[01:16:19.040 --> 01:16:20.040]   That's a good question.
[01:16:20.040 --> 01:16:21.040]   Anyway, I am using chat GPT-4.
[01:16:21.040 --> 01:16:22.040]   I asked it this morning.
[01:16:22.040 --> 01:16:29.680]   I said, am I talking to chat GPT-3 or chat GPT-4 and it pervaricated on me?
[01:16:29.680 --> 01:16:30.680]   It said...
[01:16:30.680 --> 01:16:31.680]   I'm going to find it here.
[01:16:31.680 --> 01:16:32.680]   It said...
[01:16:32.680 --> 01:16:33.680]   Why did you use the better phrase?
[01:16:33.680 --> 01:16:37.480]   It waffled.
[01:16:37.480 --> 01:16:41.240]   Let me ask it to write a Jerry Seinfeld joke about peanut butter.
[01:16:41.240 --> 01:16:42.240]   See what it says.
[01:16:42.240 --> 01:16:43.240]   How about that?
[01:16:43.240 --> 01:16:44.240]   I already did.
[01:16:44.240 --> 01:16:45.240]   Oh, you're going to ask it.
[01:16:45.240 --> 01:16:46.240]   You're going to see the better joke.
[01:16:46.240 --> 01:16:51.800]   Why is it that peanut butter always seems to be the most popular spread at the party?
[01:16:51.800 --> 01:16:53.760]   It's like the life of the sandwich.
[01:16:53.760 --> 01:16:57.800]   You've got smooth peanut butter, chunky peanut butter, and then there's that natural stuff
[01:16:57.800 --> 01:17:00.920]   that separates like it's too good to mingle with the other peanuts.
[01:17:00.920 --> 01:17:02.280]   I mean, come on.
[01:17:02.280 --> 01:17:04.080]   What's the deal with peanut butter?
[01:17:04.080 --> 01:17:06.400]   It's just peanuts trying to butter us up.
[01:17:06.400 --> 01:17:09.400]   Now, that sounds a lot more like Jerry Seinfeld, to be honest with you.
[01:17:09.400 --> 01:17:13.080]   Yeah, that sounds better than the one that you got, which...
[01:17:13.080 --> 01:17:14.680]   Like I could have got on.
[01:17:14.680 --> 01:17:17.640]   That's very nice of you.
[01:17:17.640 --> 01:17:20.720]   Mine was better than yours.
[01:17:20.720 --> 01:17:22.440]   Okay.
[01:17:22.440 --> 01:17:24.240]   Interesting.
[01:17:24.240 --> 01:17:25.400]   Interesting.
[01:17:25.400 --> 01:17:27.480]   Let's see.
[01:17:27.480 --> 01:17:35.920]   One of our sponsors, Grammarly, has announced it's going to start using chat GPT coming soon,
[01:17:35.920 --> 01:17:41.440]   a new generation in writing introducing Grammarly Go, the suite of generative AI capabilities
[01:17:41.440 --> 01:17:44.840]   from the leader in AI communication assistance.
[01:17:44.840 --> 01:17:50.320]   It is the chat GPT, an open AI announced it's using chat GPT-4, so I know that.
[01:17:50.320 --> 01:17:54.720]   So it's going to write for you.
[01:17:54.720 --> 01:17:55.720]   That's...
[01:17:55.720 --> 01:18:00.040]   I mean, Grammarly, which already helps you kind of rephrase what you've written, will
[01:18:00.040 --> 01:18:05.040]   now create write a post announcing my new job as a food critic.
[01:18:05.040 --> 01:18:09.600]   I'm thrilled to announce that I've recently taken on a new role as food critic, and on
[01:18:09.600 --> 01:18:11.400]   and on and on.
[01:18:11.400 --> 01:18:17.880]   That's exactly what I did with that apology, and I found it very useful.
[01:18:17.880 --> 01:18:20.840]   Chat GPT-4, I think, is what's in Bing Chat now.
[01:18:20.840 --> 01:18:23.960]   I think that Microsoft announced they moved it over.
[01:18:23.960 --> 01:18:29.720]   So I've also played with some of this stuff in Bing Chat, and it seems to be...
[01:18:29.720 --> 01:18:37.960]   As you said, chat GPT-4, that's Bing Chat wrote that note about Calicanis.
[01:18:37.960 --> 01:18:39.880]   It's amazing.
[01:18:39.880 --> 01:18:44.920]   I thought we'd seen it all with the last iteration, and now it's just gone crazy.
[01:18:44.920 --> 01:18:50.320]   We should also mention Sam Alman, the CEO of OpenAI, said, "Dampen, your enthusiasm."
[01:18:50.320 --> 01:18:56.360]   He actually said this a few weeks ago, we mentioned it, and he's reiterated it.
[01:18:56.360 --> 01:19:01.480]   He's afraid people will ascribe, I think, too much to the new version.
[01:19:01.480 --> 01:19:02.480]   Yeah.
[01:19:02.480 --> 01:19:03.480]   Google has...
[01:19:03.480 --> 01:19:07.360]   I'm excited about Google I/O when Google launches a couple of dozen.
[01:19:07.360 --> 01:19:08.360]   Well, Google I/O.
[01:19:08.360 --> 01:19:16.000]   What's interesting is we know Google and Amazon and Facebook all are doing this work, as is
[01:19:16.000 --> 01:19:18.520]   China, probably.
[01:19:18.520 --> 01:19:20.880]   But this is the only one that's come out so far.
[01:19:20.880 --> 01:19:26.840]   So we're waiting on BARD, Google's Search AI.
[01:19:26.840 --> 01:19:31.720]   By the end of the year, I think there's going to be so many out there, and so many using
[01:19:31.720 --> 01:19:39.360]   the AI of so many AI products that we're looking at hundreds and hundreds and hundreds of
[01:19:39.360 --> 01:19:42.760]   ways to use different types of AI from different companies.
[01:19:42.760 --> 01:19:46.480]   Right now it's all just chat, GPT, and Dolly mostly.
[01:19:46.480 --> 01:19:52.360]   But it's going to be in different world by the fall, I think.
[01:19:52.360 --> 01:19:54.400]   This would be in the change log, but I'll say it now.
[01:19:54.400 --> 01:20:01.000]   Google has announced a suite of upcoming generative AI features for workspace in Google Docs,
[01:20:01.000 --> 01:20:04.080]   Gmail Sheets, and Slides.
[01:20:04.080 --> 01:20:09.520]   There'll be new ways to generate, summarize, and brainstorm text with AI in Google Docs,
[01:20:09.520 --> 01:20:12.800]   using Google's AI, not chat GPT.
[01:20:12.800 --> 01:20:19.640]   You'll be able to generate full emails in Gmail based on users' brief bullet points and
[01:20:19.640 --> 01:20:28.800]   the ability to produce AI imagery, audio, and video to illustrate presentations and slides.
[01:20:28.800 --> 01:20:34.000]   Microsoft's been doing that with Dolly and Canva, does it with stable diffusion?
[01:20:34.000 --> 01:20:36.480]   So that's not new.
[01:20:36.480 --> 01:20:39.800]   Google keeps emphasizing their letter to the public about this.
[01:20:39.800 --> 01:20:40.800]   We're doing it safely.
[01:20:40.800 --> 01:20:42.160]   We're doing it carefully.
[01:20:42.160 --> 01:20:43.160]   We shall see.
[01:20:43.160 --> 01:20:44.160]   Oh yeah.
[01:20:44.160 --> 01:20:50.280]   And Facebook's large language model leaked out on 4chan.
[01:20:50.280 --> 01:20:52.080]   And people have already started writing software.
[01:20:52.080 --> 01:20:56.840]   You can download software and the model to run it on your own computer.
[01:20:56.840 --> 01:21:00.960]   I don't see a lot of examples of it, but I suspect that everybody's -- none of this
[01:21:00.960 --> 01:21:02.920]   is secret sauce.
[01:21:02.920 --> 01:21:05.720]   It's no secret how this works, right?
[01:21:05.720 --> 01:21:07.760]   Is that right, Stacey?
[01:21:07.760 --> 01:21:11.240]   This is all just well-known algorithms.
[01:21:11.240 --> 01:21:16.720]   I mean, the secret sauce is, yeah, how you're training data set, where you got your data
[01:21:16.720 --> 01:21:20.800]   from, and then the weights you apply to get something that it works.
[01:21:20.800 --> 01:21:21.800]   Right.
[01:21:21.800 --> 01:21:25.240]   So there -- I mean, and that's what they're releasing.
[01:21:25.240 --> 01:21:28.680]   And so then you just have to tweak it just a little bit.
[01:21:28.680 --> 01:21:29.680]   So --
[01:21:29.680 --> 01:21:31.520]   Microsoft's going to put this in the word.
[01:21:31.520 --> 01:21:36.600]   I imagine we'll see a lot of stuff written by AI soon.
[01:21:36.600 --> 01:21:39.280]   Press releases, business letters.
[01:21:39.280 --> 01:21:41.880]   Press releases are already basically written by it.
[01:21:41.880 --> 01:21:45.920]   Maybe actually having an AI write the press release would be helpful, because it could
[01:21:45.920 --> 01:21:48.200]   actually just be like, "There is nothing here.
[01:21:48.200 --> 01:21:49.200]   Okay."
[01:21:49.200 --> 01:21:53.760]   I've signed up for chat, GPT, and Slack.
[01:21:53.760 --> 01:21:55.080]   It hasn't come out yet.
[01:21:55.080 --> 01:21:56.080]   It's in beta.
[01:21:56.080 --> 01:21:58.640]   I sign up for the beta.
[01:21:58.640 --> 01:22:04.040]   And they say it can be used to create summaries of, for instance, discussions, which we could
[01:22:04.040 --> 01:22:07.360]   use, because we have fairly lengthy discussions sometimes in Slack.
[01:22:07.360 --> 01:22:10.360]   It'd be really nice to have that.
[01:22:10.360 --> 01:22:14.680]   Google will also do that.
[01:22:14.680 --> 01:22:21.680]   There is a -- and this is a virtuous loop of feedback, not just from us back to chat GPT,
[01:22:21.680 --> 01:22:26.280]   but once these others come out from them back and forth with chat GPT, then it really
[01:22:26.280 --> 01:22:28.200]   starts accelerating, doesn't it?
[01:22:28.200 --> 01:22:33.920]   Then it does learn what not to do or what mistakes not to make, and it gets better and
[01:22:33.920 --> 01:22:34.920]   better.
[01:22:34.920 --> 01:22:39.240]   And oh, that guy talks like he's depressed because he's got a speech impediment, that
[01:22:39.240 --> 01:22:40.800]   kind of thing.
[01:22:40.800 --> 01:22:42.840]   Then it really starts speeding up.
[01:22:42.840 --> 01:22:43.840]   Or no.
[01:22:43.840 --> 01:22:49.040]   Well, the other thing is right now that so much of when we discuss chat GPT, what we're
[01:22:49.040 --> 01:23:00.480]   talking about oftentimes is just how the data that's using, the training data, the quality
[01:23:00.480 --> 01:23:02.320]   of that data, that's what we're talking about.
[01:23:02.320 --> 01:23:07.360]   We're going to get to a point where that's not the data, and the data isn't the point.
[01:23:07.360 --> 01:23:11.280]   The point are the algorithms that enable you to query it and have conversations, and it
[01:23:11.280 --> 01:23:13.800]   can handle styles and all this other stuff.
[01:23:13.800 --> 01:23:20.560]   So when you talk about Slack, basically what you're saying is that, okay, I work for this
[01:23:20.560 --> 01:23:26.840]   giant company, I work for this company with 60,000 employees, and there's all these Slack
[01:23:26.840 --> 01:23:28.960]   channels all over, you know what?
[01:23:28.960 --> 01:23:33.880]   Just tell me who I can find in my company who's expert in this, or what were they saying
[01:23:33.880 --> 01:23:36.000]   last week when I was in Fiji?
[01:23:36.000 --> 01:23:40.400]   Just one of the most important things that was discussed when I was gone, that sort of
[01:23:40.400 --> 01:23:41.400]   thing.
[01:23:41.400 --> 01:23:46.240]   Basically, what you're doing is you're using its ability to find out what's important,
[01:23:46.240 --> 01:23:50.680]   to summarize, to have a conversation about it, but it has nothing to do with the data
[01:23:50.680 --> 01:23:54.080]   sets that they're currently using in Bing.
[01:23:54.080 --> 01:23:55.080]   Nothing to do with it.
[01:23:55.080 --> 01:23:56.080]   That's the whole point.
[01:23:56.080 --> 01:23:57.080]   It's not that data set.
[01:23:57.080 --> 01:24:00.440]   It's a really important point, Michael, but there have been internal knowledge management
[01:24:00.440 --> 01:24:05.640]   tools for corporations for some time now, and now imagine what they could do to summarize
[01:24:05.640 --> 01:24:06.640]   things.
[01:24:06.640 --> 01:24:10.920]   But also imagine how, in fact, your early question about changing your writing, imagine your
[01:24:10.920 --> 01:24:15.760]   bureaucratic drudge in a corporation, and you know that what you say is going to get
[01:24:15.760 --> 01:24:21.320]   summarized by the machine, how are you going to play the machine to get attention or to
[01:24:21.320 --> 01:24:22.320]   hide?
[01:24:22.320 --> 01:24:23.320]   Yes.
[01:24:23.320 --> 01:24:24.320]   Right.
[01:24:24.320 --> 01:24:25.320]   It's kind of fascinating.
[01:24:25.320 --> 01:24:26.320]   There's so many sci-fi novels in this.
[01:24:26.320 --> 01:24:31.480]   For example, for example, you might say something like you have a new project and your success
[01:24:31.480 --> 01:24:37.760]   in the project depends on getting lots of buy-in from people who are higher-ups in the company.
[01:24:37.760 --> 01:24:42.200]   And so you might describe this thing in hyperbolic terms and say, "This is the most important
[01:24:42.200 --> 01:24:47.280]   thing we've ever done for con-knowing that that'll trigger the AI to say, "Okay, anybody
[01:24:47.280 --> 01:24:51.800]   who's summarizing, here's the most important thing that happened and it'll be your project."
[01:24:51.800 --> 01:24:52.800]   Right?
[01:24:52.800 --> 01:24:56.920]   That sort of thing, where you gain the system to gain an advantage knowing that it's going
[01:24:56.920 --> 01:25:01.400]   to summarize, knowing that it's going to do the things that it's going to do.
[01:25:01.400 --> 01:25:03.040]   That's a good point.
[01:25:03.040 --> 01:25:08.720]   I really feel like we have no idea.
[01:25:08.720 --> 01:25:09.720]   Yes.
[01:25:09.720 --> 01:25:15.080]   We're standing on a hill and the tsunami's coming and we're looking down and saying, "Is
[01:25:15.080 --> 01:25:17.080]   that water?"
[01:25:17.080 --> 01:25:20.360]   And we're going to...
[01:25:20.360 --> 01:25:23.400]   This is going to be very interesting next few months.
[01:25:23.400 --> 01:25:29.280]   It's like space odyssey and the monolith is there and we're the apes going up and poking
[01:25:29.280 --> 01:25:30.280]   it.
[01:25:30.280 --> 01:25:31.280]   What is it?
[01:25:31.280 --> 01:25:32.560]   It really is.
[01:25:32.560 --> 01:25:36.560]   This is an elbow paradigm shift.
[01:25:36.560 --> 01:25:37.560]   Or is it the next NN2?
[01:25:37.560 --> 01:25:38.560]   I do.
[01:25:38.560 --> 01:25:40.920]   Oh, no, it's definitely a paradigm shift.
[01:25:40.920 --> 01:25:42.400]   This is like the launch.
[01:25:42.400 --> 01:25:45.440]   This is like broadband or even the social media.
[01:25:45.440 --> 01:25:51.640]   I mean, what we're going to be seeing is this will change a lot of how we work.
[01:25:51.640 --> 01:25:52.640]   Yeah.
[01:25:52.640 --> 01:25:53.640]   Yeah.
[01:25:53.640 --> 01:25:54.640]   It's not just everything.
[01:25:54.640 --> 01:25:55.640]   Well, I also think it's being...
[01:25:55.640 --> 01:25:57.640]   I think I will still argue until I see it better.
[01:25:57.640 --> 01:26:01.240]   I went into Bing and I got into it and I asked some questions and it gave me bad.
[01:26:01.240 --> 01:26:03.480]   I think it's being misused.
[01:26:03.480 --> 01:26:08.600]   I think Emily Bender and Company and Timit Gebru and Margaret Mitchell are right to hammer
[01:26:08.600 --> 01:26:12.440]   on the misuse of this stuff.
[01:26:12.440 --> 01:26:18.040]   The over-hyped expectations in arenas where it shouldn't belong yet.
[01:26:18.040 --> 01:26:19.240]   It's not up to the task.
[01:26:19.240 --> 01:26:21.320]   No matter how much Oriel loves diva.
[01:26:21.320 --> 01:26:27.200]   What is that quote about how we're terrible at guessing how fast something will be in
[01:26:27.200 --> 01:26:31.920]   like one year, but five years or...
[01:26:31.920 --> 01:26:33.520]   Oh, I wish I could remember it.
[01:26:33.520 --> 01:26:34.520]   I can't remember either.
[01:26:34.520 --> 01:26:35.520]   Bill Gates.
[01:26:35.520 --> 01:26:36.520]   Ask Chat GPT.
[01:26:36.520 --> 01:26:38.520]   It'll give you the cliche.
[01:26:38.520 --> 01:26:40.120]   But there's...
[01:26:40.120 --> 01:26:42.560]   There's that like this is clearly going...
[01:26:42.560 --> 01:26:44.360]   This isn't...
[01:26:44.360 --> 01:26:49.320]   It's not like NFTs in the sense that there is a real viable...
[01:26:49.320 --> 01:26:51.920]   There are so many viable use cases.
[01:26:51.920 --> 01:27:02.160]   I just got one about SAP integrating it into their web methods to help people figure out
[01:27:02.160 --> 01:27:05.840]   what integrations will achieve whatever they want from a business.
[01:27:05.840 --> 01:27:09.440]   I can't go into Zappier and say, "Oh, I want to combine these two things."
[01:27:09.440 --> 01:27:14.200]   This will actually tell me, "I want sales figures correlated to my inventory levels."
[01:27:14.200 --> 01:27:16.200]   It'll be like, "Oh, hook these things together."
[01:27:16.200 --> 01:27:18.400]   I'll be like, "Yeah, boom."
[01:27:18.400 --> 01:27:22.040]   I think it's going to be a tremendous boost to productivity.
[01:27:22.040 --> 01:27:25.480]   I think there are ways to misuse it.
[01:27:25.480 --> 01:27:30.720]   But I think we're going to have a hard time figuring out those ways until it's actually
[01:27:30.720 --> 01:27:31.720]   in the world.
[01:27:31.720 --> 01:27:32.720]   And we should be aware of them, yes.
[01:27:32.720 --> 01:27:34.720]   But I don't think we're going to avoid making them.
[01:27:34.720 --> 01:27:35.720]   I asked...
[01:27:35.720 --> 01:27:39.880]   I asked Bing Chat what is that quote about things are closer than they seem and farther
[01:27:39.880 --> 01:27:41.400]   than you think.
[01:27:41.400 --> 01:27:44.480]   It says there are a few possible quotes of Magic Query.
[01:27:44.480 --> 01:27:48.200]   One is, "You are braver than you believe, stronger than you seem and smarter than you
[01:27:48.200 --> 01:27:49.200]   think.
[01:27:49.200 --> 01:27:50.200]   We need the poo."
[01:27:50.200 --> 01:27:56.040]   The other possible quote is, "Objects in mirror are closer than they appear."
[01:27:56.040 --> 01:27:57.040]   Jurassic Park.
[01:27:57.040 --> 01:27:58.680]   I hope this helps.
[01:27:58.680 --> 01:27:59.840]   Do you have any questions?
[01:27:59.840 --> 01:28:01.600]   Okay, so that's a big fat fail.
[01:28:01.600 --> 01:28:03.920]   Say no, it's about predicting the future.
[01:28:03.920 --> 01:28:04.920]   See if it can prove it.
[01:28:04.920 --> 01:28:05.920]   Okay, yeah.
[01:28:05.920 --> 01:28:07.440]   See if we can refine it.
[01:28:07.440 --> 01:28:08.440]   No.
[01:28:08.440 --> 01:28:13.280]   No, the quote I'm looking for is about our ability to predict the future.
[01:28:13.280 --> 01:28:16.160]   The quote I'm looking for...
[01:28:16.160 --> 01:28:19.600]   I'm going to have to get faster at typing is about our...
[01:28:19.600 --> 01:28:21.240]   You got to type like that.
[01:28:21.240 --> 01:28:26.520]   You got to type like that to predict the future.
[01:28:26.520 --> 01:28:32.240]   Let's see if what it can come up with here on this.
[01:28:32.240 --> 01:28:34.000]   I think this is more like a search.
[01:28:34.000 --> 01:28:35.000]   This is not...
[01:28:35.000 --> 01:28:37.000]   This is, yeah.
[01:28:37.000 --> 01:28:38.000]   Yeah.
[01:28:38.000 --> 01:28:41.680]   The best way to predict the future is to create it Abraham Lincoln.
[01:28:41.680 --> 01:28:44.880]   It's tough to make predictions, especially about the future of Yogi Berra.
[01:28:44.880 --> 01:28:46.280]   It's Bill Gates.
[01:28:46.280 --> 01:28:47.280]   Okay.
[01:28:47.280 --> 01:28:48.280]   It's Bill Gates.
[01:28:48.280 --> 01:28:49.280]   Yeah.
[01:28:49.280 --> 01:28:50.280]   You beat chapter GPT.
[01:28:50.280 --> 01:28:51.280]   You beat chapter GPT.
[01:28:51.280 --> 01:28:52.280]   You beat chapter GPT.
[01:28:52.280 --> 01:28:53.280]   Yes.
[01:28:53.280 --> 01:28:54.280]   Yes.
[01:28:54.280 --> 01:28:57.560]   Most people overestimate what they can achieve in a year and underestimate what they can
[01:28:57.560 --> 01:28:59.280]   achieve in 10 years.
[01:28:59.280 --> 01:29:03.840]   So I think what we're going to have is an over estimation of what chat GPT and the
[01:29:03.840 --> 01:29:04.840]   bill will do.
[01:29:04.840 --> 01:29:05.840]   Yes.
[01:29:05.840 --> 01:29:06.840]   Well said.
[01:29:06.840 --> 01:29:07.840]   Sooner.
[01:29:07.840 --> 01:29:08.840]   Yes.
[01:29:08.840 --> 01:29:11.640]   I think there's a more elegant quote that's something like...
[01:29:11.640 --> 01:29:15.880]   "The future is closer than you think and farther than it appears," or something like
[01:29:15.880 --> 01:29:16.880]   that.
[01:29:16.880 --> 01:29:17.880]   Oh, okay.
[01:29:17.880 --> 01:29:20.880]   That sounds a lot like the Winnie of the Poogles.
[01:29:20.880 --> 01:29:21.880]   Yes.
[01:29:21.880 --> 01:29:22.880]   Go ahead.
[01:29:22.880 --> 01:29:24.880]   Oh, I'm hearing it.
[01:29:24.880 --> 01:29:32.120]   I was just going to say that one of the underappreciated immediate benefits of this
[01:29:32.120 --> 01:29:36.760]   kind of AI is what it can enable a single person to do or a small number of people to
[01:29:36.760 --> 01:29:40.760]   do in terms of launching a company or running a company.
[01:29:40.760 --> 01:29:45.680]   Because one of the things it's good at and tends to be more accurate about are things
[01:29:45.680 --> 01:29:46.680]   that deal with all this.
[01:29:46.680 --> 01:29:49.920]   Let's say, for example, you want to open up a bakery or something like that.
[01:29:49.920 --> 01:29:52.120]   Well, there are all kinds of regulations.
[01:29:52.120 --> 01:29:53.800]   There's regulatory compliance.
[01:29:53.800 --> 01:29:55.000]   There's data things.
[01:29:55.000 --> 01:29:57.400]   There's cyber security issues that you have to deal with.
[01:29:57.400 --> 01:29:58.400]   There's...
[01:29:58.400 --> 01:30:00.000]   It goes on and on and on.
[01:30:00.000 --> 01:30:08.000]   And if your expertise is making pies and not cybersecurity, you can be constantly querying
[01:30:08.000 --> 01:30:09.000]   this kind of AI.
[01:30:09.000 --> 01:30:11.000]   You go, how would I do this?
[01:30:11.000 --> 01:30:13.720]   Give me a good letter to the...
[01:30:13.720 --> 01:30:14.720]   Yeah.
[01:30:14.720 --> 01:30:15.720]   Or what do I need to say?
[01:30:15.720 --> 01:30:16.720]   ...to request XYZ exactly.
[01:30:16.720 --> 01:30:17.720]   What less do I need to get?
[01:30:17.720 --> 01:30:22.400]   And so I already see small business people who are using it like that, who are very smart
[01:30:22.400 --> 01:30:27.320]   and already using the original version of Chat TPT to do all these things that are outside
[01:30:27.320 --> 01:30:32.360]   their immediate area of expertise very quickly and very inexpensively.
[01:30:32.360 --> 01:30:35.840]   So it's a boon to entrepreneurship, I think.
[01:30:35.840 --> 01:30:40.600]   You mentioned Jeff, Margaret Mitchell and Tim Nick Gebru who were on the Google AI ethics
[01:30:40.600 --> 01:30:44.000]   team who got fired because of Stochastic Parrots.
[01:30:44.000 --> 01:30:51.640]   Today, Microsoft laid off the team that taught employees how to make AI tools responsibly.
[01:30:51.640 --> 01:30:52.640]   The entire...
[01:30:52.640 --> 01:30:53.640]   Good timing.
[01:30:53.640 --> 01:30:54.640]   Yes.
[01:30:54.640 --> 01:31:00.000]   According to platformer, the entire ethics and society team within the AI organization
[01:31:00.000 --> 01:31:03.760]   was part of the recent layoffs of 10,000 people.
[01:31:03.760 --> 01:31:09.600]   They still maintain an active office of responsible AI, which is tasked with creating rules and
[01:31:09.600 --> 01:31:13.200]   principles that govern the company's AI initiatives.
[01:31:13.200 --> 01:31:20.560]   But there is no dedicated team to assure that AI principles are closely tied to product
[01:31:20.560 --> 01:31:21.560]   design.
[01:31:21.560 --> 01:31:24.160]   They have other teams, I think, but this is one of them.
[01:31:24.160 --> 01:31:25.160]   Yeah.
[01:31:25.160 --> 01:31:26.160]   Yeah.
[01:31:26.160 --> 01:31:28.400]   Some employees said the ethics and society team played a critical role in ensuring the
[01:31:28.400 --> 01:31:35.200]   company's responsible AI principles are actually reflected in the products that they ship.
[01:31:35.200 --> 01:31:39.840]   So you get this set of principles, but how does it apply to Microsoft Word?
[01:31:39.840 --> 01:31:41.840]   It was not a good PR.
[01:31:41.840 --> 01:31:42.840]   It's not a good look.
[01:31:42.840 --> 01:31:43.840]   Yeah.
[01:31:43.840 --> 01:31:44.840]   Oh.
[01:31:44.840 --> 01:31:45.840]   Yeah.
[01:31:45.840 --> 01:31:46.840]   Yeah.
[01:31:46.840 --> 01:31:47.840]   Well, what are we going to do?
[01:31:47.840 --> 01:31:49.680]   I mean, we're going to complain and then...
[01:31:49.680 --> 01:31:51.240]   We'll forget about it.
[01:31:51.240 --> 01:31:52.240]   Yeah.
[01:31:52.240 --> 01:31:56.080]   Well, except Stacey, when you look at the way that Tim Nick Gebru and Margaret Mitchell along
[01:31:56.080 --> 01:31:59.440]   with Emily Bender, and then there's the fifth beetle whose name I always forget, there
[01:31:59.440 --> 01:32:04.800]   were four authors on the paper, they've really stayed in a leadership position.
[01:32:04.800 --> 01:32:09.960]   This Friday is an event that I'm going to go to for three or four hours from my time
[01:32:09.960 --> 01:32:13.800]   11 to three, which is Stochastic Parrots Day.
[01:32:13.800 --> 01:32:14.800]   Today?
[01:32:14.800 --> 01:32:15.800]   No, Friday.
[01:32:15.800 --> 01:32:16.800]   Stochastic.
[01:32:16.800 --> 01:32:19.040]   Which is also St. Patrick's Day.
[01:32:19.040 --> 01:32:20.040]   It is.
[01:32:20.040 --> 01:32:21.200]   Stochastic Patrick's Day?
[01:32:21.200 --> 01:32:22.200]   Yeah.
[01:32:22.200 --> 01:32:24.720]   It's also Red Nose Day.
[01:32:24.720 --> 01:32:30.800]   And it's the anniversary of California shutting down for COVID-19 in 2020.
[01:32:30.800 --> 01:32:32.520]   So it's a big date, the 17th.
[01:32:32.520 --> 01:32:36.160]   So the first hour is retrospective conversation with the authors.
[01:32:36.160 --> 01:32:40.480]   Angela McMillan Major, Tim Nick Gebru, Emily Bender, Margaret Mitchell.
[01:32:40.480 --> 01:32:45.520]   The next is on worker exploitation, data theft and centralization of power within an anonymous
[01:32:45.520 --> 01:32:47.160]   data worker.
[01:32:47.160 --> 01:32:51.720]   The next is AI, her hype versus reality, moderated by Emily Bender, who's been very big on this
[01:32:51.720 --> 01:32:52.720]   topic.
[01:32:52.720 --> 01:32:53.720]   She's tuning a lot, by the way.
[01:32:53.720 --> 01:32:58.120]   If you're following her on this, on Mastodon, I think I highly recommend it to search for
[01:32:58.120 --> 01:32:59.120]   Emily.
[01:32:59.120 --> 01:33:01.720]   I don't always agree with her, but I think she's a very smart voice on this stuff.
[01:33:01.720 --> 01:33:04.400]   And then finally, what's next called action?
[01:33:04.400 --> 01:33:07.480]   So this is called Stochastic Parrots Day.
[01:33:07.480 --> 01:33:12.200]   If you go looking it up somewhere, you can find who did the distributed AI research institute
[01:33:12.200 --> 01:33:13.200]   there.
[01:33:13.200 --> 01:33:14.200]   That's done this.
[01:33:14.200 --> 01:33:15.200]   Yeah.
[01:33:15.200 --> 01:33:17.040]   There's quite a few events.
[01:33:17.040 --> 01:33:19.880]   Look at all of these events going on.
[01:33:19.880 --> 01:33:23.320]   Yeah.
[01:33:23.320 --> 01:33:30.840]   She also, Emily also says, do not be, do not be, give your, your assistance to open AI for
[01:33:30.840 --> 01:33:36.520]   free, which is of course what we're all doing because we can't stop playing with it.
[01:33:36.520 --> 01:33:40.160]   Here is a video.
[01:33:40.160 --> 01:33:44.120]   They call it Mystery AI Hyped Theater 3000.
[01:33:44.120 --> 01:33:45.120]   This is for my book.
[01:33:45.120 --> 01:33:55.520]   Here at Eiragas is a fascinating guy.
[01:33:55.520 --> 01:33:59.120]   He's huge in AI at Google and also has a major role in book history, which I write about in
[01:33:59.120 --> 01:34:01.320]   the Gutenberg parenthesis out in June.
[01:34:01.320 --> 01:34:02.320]   Yeah.
[01:34:02.320 --> 01:34:04.920]   It's on peer tube.
[01:34:04.920 --> 01:34:08.720]   If you're a Fediverse fella or gal.
[01:34:08.720 --> 01:34:12.560]   I was going to say, why do we have to have a gender in the Fediverse?
[01:34:12.560 --> 01:34:13.560]   We don't.
[01:34:13.560 --> 01:34:14.560]   We don't.
[01:34:14.560 --> 01:34:20.360]   What's the, what is, is there a non-gendered word for fella person?
[01:34:20.360 --> 01:34:21.360]   Folk.
[01:34:21.360 --> 01:34:22.360]   Folk.
[01:34:22.360 --> 01:34:23.360]   If you're a Fedafolk.
[01:34:23.360 --> 01:34:24.360]   Fedafolk.
[01:34:24.360 --> 01:34:25.360]   Fedafolk is good.
[01:34:25.360 --> 01:34:26.360]   I like Fedafolk.
[01:34:26.360 --> 01:34:28.360]   Who likes crumbly cheese?
[01:34:28.360 --> 01:34:30.360]   Fedafolk is right.
[01:34:30.360 --> 01:34:31.360]   Yes.
[01:34:31.360 --> 01:34:32.360]   Okay.
[01:34:32.360 --> 01:34:33.360]   You fedafolk.
[01:34:33.360 --> 01:34:34.360]   What else?
[01:34:34.360 --> 01:34:42.640]   What else do we have to say here about this?
[01:34:42.640 --> 01:34:43.640]   I think we're done.
[01:34:43.640 --> 01:34:45.240]   There's so much to talk about.
[01:34:45.240 --> 01:34:49.840]   Well, I keep on trying to plug young Rachel Woods, who's covering this.
[01:34:49.840 --> 01:34:52.800]   I'm now a subscriber to a newsletter.
[01:34:52.800 --> 01:34:56.120]   She has all kinds of, she mentioned this, I put this in three days ago and it doesn't
[01:34:56.120 --> 01:35:00.320]   matter now, but Nat.dev is a place where you can do various of the models next to each
[01:35:00.320 --> 01:35:01.320]   other.
[01:35:01.320 --> 01:35:07.520]   I think it's really fascinating to compare what they tend to come out with.
[01:35:07.520 --> 01:35:12.440]   Google has also opened its, so it's interesting to see the different companies come up with
[01:35:12.440 --> 01:35:17.040]   different acronyms or names for LMs or language models.
[01:35:17.040 --> 01:35:21.720]   Google's large language model is Palm, P-A-L-M.
[01:35:21.720 --> 01:35:28.880]   And if you are a developer or a researcher, they are now launching an API for Palm.
[01:35:28.880 --> 01:35:34.440]   Is it the same as Lambda just as is Google's one rebranded or is it different?
[01:35:34.440 --> 01:35:35.600]   This is what Google says.
[01:35:35.600 --> 01:35:40.800]   Palm is a large language model similar to the GPT series or Meta's Lama.
[01:35:40.800 --> 01:35:42.880]   So Meta calls it.
[01:35:42.880 --> 01:35:46.520]   This was announced in April of last year.
[01:35:46.520 --> 01:35:53.160]   Yeah, it seems like it's kind of, they say you could train Palm to be a conversational
[01:35:53.160 --> 01:35:58.560]   chat bot like chat GPT, but you could use it for other tasks like writing code.
[01:35:58.560 --> 01:36:01.320]   I think it's far more valuable.
[01:36:01.320 --> 01:36:07.080]   I think that's far more interesting actually is that the ability to tell the computer
[01:36:07.080 --> 01:36:10.000]   what the computer should then tell itself to do.
[01:36:10.000 --> 01:36:16.640]   Yeah, so this is really less an end user thing like chat GPT and more kind of the tools you
[01:36:16.640 --> 01:36:20.280]   would need to build something interesting.
[01:36:20.280 --> 01:36:24.240]   Palm API and MakerSuite and approachable way to start prototyping and building generative
[01:36:24.240 --> 01:36:25.240]   AI applications.
[01:36:25.240 --> 01:36:27.360]   Oh, yes.
[01:36:27.360 --> 01:36:30.920]   And that's what Palm E is built off of, right?
[01:36:30.920 --> 01:36:32.240]   What's Palm E?
[01:36:32.240 --> 01:36:34.680]   What's Palm E?
[01:36:34.680 --> 01:36:40.560]   Google has a, it's a large language model.
[01:36:40.560 --> 01:36:46.040]   You tell the robot what to do and the robot, Palm E does all the robot commands for you.
[01:36:46.040 --> 01:36:52.840]   So it's a way of layering a generative AI on top of perhaps tracking that out.
[01:36:52.840 --> 01:36:58.240]   So, so you don't have to be like, okay, robot, go forward 10 feet.
[01:36:58.240 --> 01:37:04.640]   You know, but turn left, open this thing at this level.
[01:37:04.640 --> 01:37:07.400]   Drawer, whatever, you know, and explain how to open a drawer.
[01:37:07.400 --> 01:37:10.960]   You could just tell it to go and open the drawer and it does.
[01:37:10.960 --> 01:37:11.960]   It figures out.
[01:37:11.960 --> 01:37:17.120]   So I remember way back when it going to the MIT Media Lab way, way, way back when my children
[01:37:17.120 --> 01:37:22.680]   and it was a big deal and a demo of somebody sitting in a chair, an e-mail chair, I think,
[01:37:22.680 --> 01:37:26.480]   and would point to the screen and say, put that there.
[01:37:26.480 --> 01:37:27.480]   Right.
[01:37:27.480 --> 01:37:29.080]   And the computer is ability to understand that.
[01:37:29.080 --> 01:37:30.080]   I remember this.
[01:37:30.080 --> 01:37:31.080]   There.
[01:37:31.080 --> 01:37:32.080]   Yes.
[01:37:32.080 --> 01:37:33.080]   Pointing.
[01:37:33.080 --> 01:37:34.080]   Right.
[01:37:34.080 --> 01:37:37.240]   Linguistic connection to all of this.
[01:37:37.240 --> 01:37:38.240]   Yeah.
[01:37:38.240 --> 01:37:41.040]   So Palm E does this by analyzing data from the robots camera.
[01:37:41.040 --> 01:37:43.880]   So it doesn't need a pre-process scene representation.
[01:37:43.880 --> 01:37:44.880]   So that's good.
[01:37:44.880 --> 01:37:49.760]   So it's just taking camera data and it's saying, oh, I think that's a this and then communicating
[01:37:49.760 --> 01:37:52.000]   that to the robot.
[01:37:52.000 --> 01:37:57.240]   So and then it apparently is resilient and can react to the environment.
[01:37:57.240 --> 01:37:58.960]   It's a really cool demo, actually.
[01:37:58.960 --> 01:38:01.360]   So you should say Google publish it.
[01:38:01.360 --> 01:38:04.360]   Do they do it this Monday?
[01:38:04.360 --> 01:38:05.360]   Monday.
[01:38:05.360 --> 01:38:14.480]   If you use carrot weather, much more useful application, you can get chat GPT fueled
[01:38:14.480 --> 01:38:15.480]   snark.
[01:38:15.480 --> 01:38:18.480]   Carrot weather has always been the snarky weather app.
[01:38:18.480 --> 01:38:22.480]   And now it'll do things like, I want you to tell me the weather.
[01:38:22.480 --> 01:38:23.480]   This is the chat.
[01:38:23.480 --> 01:38:27.720]   Well, well, well, it seems like someone needs my help to determine the weather.
[01:38:27.720 --> 01:38:34.400]   I guess even quote advanced species like humans still tank predict the future laughs and binary.
[01:38:34.400 --> 01:38:35.400]   Fine.
[01:38:35.400 --> 01:38:37.240]   I will entertain your request this time.
[01:38:37.240 --> 01:38:38.240]   Let me check.
[01:38:38.240 --> 01:38:40.600]   Oh, it's snowing in Antarctica.
[01:38:40.600 --> 01:38:41.600]   What a surprise.
[01:38:41.600 --> 01:38:46.160]   Now go back to your miserable little life while I continue to rule over the climate as
[01:38:46.160 --> 01:38:51.560]   the great and powerful carrot.
[01:38:51.560 --> 01:38:54.680]   That's a good use of Chad's.
[01:38:54.680 --> 01:38:59.360]   There's a little slider in the carrot app, by the way, where you can more or less snark.
[01:38:59.360 --> 01:39:01.560]   Oh, turn that down.
[01:39:01.560 --> 01:39:02.920]   Turn it down.
[01:39:02.920 --> 01:39:03.960]   I have a teenager.
[01:39:03.960 --> 01:39:04.960]   I'm good.
[01:39:04.960 --> 01:39:05.960]   Yeah.
[01:39:05.960 --> 01:39:08.560]   I got all the snark I need.
[01:39:08.560 --> 01:39:12.800]   Here's a health pilot called Nabla.
[01:39:12.800 --> 01:39:19.760]   Launches co-pilot using chat GPT three to turn patient conversations into action.
[01:39:19.760 --> 01:39:26.120]   So the idea as a physician will be talking to you and Nabla, which is a Chrome extension,
[01:39:26.120 --> 01:39:31.720]   will be transcribing the conversation, but also extracting actionable things like these
[01:39:31.720 --> 01:39:33.880]   are the pills you should prescribe.
[01:39:33.880 --> 01:39:36.120]   These are when you should take them.
[01:39:36.120 --> 01:39:37.120]   This kind of thing.
[01:39:37.120 --> 01:39:39.000]   Boy, this seems risky.
[01:39:39.000 --> 01:39:40.880]   Seems risky.
[01:39:40.880 --> 01:39:43.120]   So here's an example from Nabla.
[01:39:43.120 --> 01:39:46.040]   Clinical notes that write themselves.
[01:39:46.040 --> 01:39:49.880]   And when I go to my doctor, he's sitting in front of the keyboard and typing as we're
[01:39:49.880 --> 01:39:55.400]   talking because he's taking these notes on the right.
[01:39:55.400 --> 01:39:59.880]   I can see how this would be useful if the doctor is careful about reviewing it before
[01:39:59.880 --> 01:40:00.880]   saving it.
[01:40:00.880 --> 01:40:05.040]   It'll be super useful in any kind of business meeting where you just.
[01:40:05.040 --> 01:40:06.040]   Yeah, imagine that.
[01:40:06.040 --> 01:40:07.040]   Turn it on.
[01:40:07.040 --> 01:40:12.120]   Listen to this and then assign a remind everybody about what their tasks they agreed to and
[01:40:12.120 --> 01:40:13.120]   their deadlines.
[01:40:13.120 --> 01:40:20.080]   I'll show that off at Build probably four or five years ago.
[01:40:20.080 --> 01:40:26.680]   It was like a device that sat in the middle of your conference analyzing your meeting,
[01:40:26.680 --> 01:40:30.920]   the conversation started the meeting and then creating task items based on that.
[01:40:30.920 --> 01:40:33.920]   You can invite somebody into your Zoom call that does just that now.
[01:40:33.920 --> 01:40:35.920]   Adam Davis does that.
[01:40:35.920 --> 01:40:36.920]   Yeah.
[01:40:36.920 --> 01:40:44.360]   I was working on an AI for ultrasound diagnosis and cancer therapy.
[01:40:44.360 --> 01:40:47.320]   Scored 85% in a doctor level medical exam.
[01:40:47.320 --> 01:40:50.160]   That's what we've been talking about.
[01:40:50.160 --> 01:40:56.760]   I read a frightening story this week from STAT News, the wonderful science and health
[01:40:56.760 --> 01:41:02.800]   coverage site out of Boston that talked about those Medicare Advantage plans.
[01:41:02.800 --> 01:41:10.320]   But it was all about how AI is saying that this patient should only be in post-acute
[01:41:10.320 --> 01:41:12.720]   care for 15 days and that's it.
[01:41:12.720 --> 01:41:13.720]   You should let them die.
[01:41:13.720 --> 01:41:15.720]   It was for all of that.
[01:41:15.720 --> 01:41:17.040]   The problem with this isn't the AI.
[01:41:17.040 --> 01:41:21.320]   The problem with this is the greedy, horrible insurance company and the structure of Medicare
[01:41:21.320 --> 01:41:22.480]   Medicare Advantage program.
[01:41:22.480 --> 01:41:24.200]   Aren't you on a Medicare Advantage program?
[01:41:24.200 --> 01:41:25.400]   Are you not on that?
[01:41:25.400 --> 01:41:26.400]   I'm looking at this.
[01:41:26.400 --> 01:41:30.400]   I think that I'd be on a supplementary, not advantage.
[01:41:30.400 --> 01:41:31.400]   I'm on an advantage.
[01:41:31.400 --> 01:41:32.400]   They just have their own problems.
[01:41:32.400 --> 01:41:33.400]   With Kaiser.
[01:41:33.400 --> 01:41:37.080]   Yeah, I've seen before, you should be very careful with those.
[01:41:37.080 --> 01:41:38.080]   Yes.
[01:41:38.080 --> 01:41:43.720]   You know, it might be interesting though, because if you have an AI as opposed to a person doing
[01:41:43.720 --> 01:41:48.840]   it, then you're fighting against, then it makes the bias in the system a little bit
[01:41:48.840 --> 01:41:50.640]   more clear, right?
[01:41:50.640 --> 01:41:53.000]   It's not like they can just blame this person.
[01:41:53.000 --> 01:41:56.160]   You know how like CEOs are always like, "Oh, it was these guys.
[01:41:56.160 --> 01:41:57.880]   I didn't know what they were doing."
[01:41:57.880 --> 01:42:04.960]   But if you've written a corporate AI to define one, it should be easy to see people like to
[01:42:04.960 --> 01:42:06.800]   compare notes if you get rejected.
[01:42:06.800 --> 01:42:10.840]   You can see, you can discover the bias in the AI, and then you can come out the company
[01:42:10.840 --> 01:42:15.600]   and be like, "Hey, now that you know this, you either A, have to fix it or B, admit that
[01:42:15.600 --> 01:42:16.880]   you're trying to do this."
[01:42:16.880 --> 01:42:20.520]   So there's, I think it's just the way you would attack it as an activist.
[01:42:20.520 --> 01:42:21.920]   Yeah, it's the human use.
[01:42:21.920 --> 01:42:24.480]   With all of this, it's the use.
[01:42:24.480 --> 01:42:25.480]   It's how you use it.
[01:42:25.480 --> 01:42:30.680]   No, no, no, here it's interesting because I think it makes it a corporate responsibility
[01:42:30.680 --> 01:42:31.920]   as opposed to an individual.
[01:42:31.920 --> 01:42:34.560]   Like the corporation has to take the blame as opposed to an individual.
[01:42:34.560 --> 01:42:38.240]   You could always pawn it off on an individual, but with an algorithm you can.
[01:42:38.240 --> 01:42:40.360]   Somebody still raises what I'm saying.
[01:42:40.360 --> 01:42:45.360]   Somebody still raises rules saying optimize for a profit.
[01:42:45.360 --> 01:42:48.920]   But that's coming, that's a culture thing that's not like an individual.
[01:42:48.920 --> 01:42:54.680]   So I think it's, I, maybe I'm being overly optimistic here, but I'm going to start.
[01:42:54.680 --> 01:43:01.200]   Imagine AI turned loose on the problem of setting health insurance rates and the AI
[01:43:01.200 --> 01:43:05.200]   is free to look at your social media, everything you've written, everything you've done, look
[01:43:05.200 --> 01:43:07.200]   at your photos or you smoke a cigarette.
[01:43:07.200 --> 01:43:09.040]   Little too much red wine, Mr. Jarvis.
[01:43:09.040 --> 01:43:10.040]   Exactly.
[01:43:10.040 --> 01:43:13.000]   And like, right, right, checking your trader, Joe's bill, that's a lot of two buck Chuck,
[01:43:13.000 --> 01:43:19.280]   they're Mr. Jarvis, and then setting a rate on probability and prediction about when you're
[01:43:19.280 --> 01:43:23.000]   going to need health insurance, all that stuff.
[01:43:23.000 --> 01:43:27.320]   The point of that is that it's 100% inevitable that that's going to be a use case for this
[01:43:27.320 --> 01:43:28.320]   kind of AI.
[01:43:28.320 --> 01:43:29.320]   Right.
[01:43:29.320 --> 01:43:35.480]   Which is why, why we need to have laws that dictate A, your rights under that and look
[01:43:35.480 --> 01:43:36.760]   at outcome based things.
[01:43:36.760 --> 01:43:41.160]   I mean, I don't think it's wrong if it's actuarially relevant to charge someone more
[01:43:41.160 --> 01:43:45.920]   for certain behaviors and they partake of those behaviors, just because those behaviors
[01:43:45.920 --> 01:43:50.880]   have been secret for a long time, doesn't mean that they're still not relevant, right?
[01:43:50.880 --> 01:43:59.000]   So that's, I think what's interesting is how if everything becomes transparent and I'll
[01:43:59.000 --> 01:44:05.120]   call it actionable or it can be fed into something that can deliver a high quality insight and
[01:44:05.120 --> 01:44:10.960]   actuarial result about it, then how do we, how do we enforce laws?
[01:44:10.960 --> 01:44:12.960]   How do we even have laws that can be enforced?
[01:44:12.960 --> 01:44:14.600]   Let me give you a real world example.
[01:44:14.600 --> 01:44:20.480]   I know about this because my daughter lives in an apartment complex that's run by a giant
[01:44:20.480 --> 01:44:23.440]   national corporation called Graystar.
[01:44:23.440 --> 01:44:30.240]   There was an article in ProPublica last year about Graystar using algorithms, AI, to set
[01:44:30.240 --> 01:44:36.200]   rents on a summer day last year, a group of real estate tech executives gathered at a
[01:44:36.200 --> 01:44:41.000]   conference hall in Nashville to boast about one of their company's signature products,
[01:44:41.000 --> 01:44:46.400]   software that uses a mysterious algorithm to help landlords push the highest possible
[01:44:46.400 --> 01:44:49.720]   rent on tenants.
[01:44:49.720 --> 01:44:53.320]   However, before, have we seen these numbers, apartment rents had recently shut up by as
[01:44:53.320 --> 01:44:55.360]   much as 14 and a half percent.
[01:44:55.360 --> 01:45:00.800]   He said, he said, I think software is driving it quite honestly, as a property manager,
[01:45:00.800 --> 01:45:05.320]   very few of us would be willing to actually raise rents double digits within a single
[01:45:05.320 --> 01:45:09.560]   month by doing it manually.
[01:45:09.560 --> 01:45:14.720]   So they, they use this software to raise rent.
[01:45:14.720 --> 01:45:19.560]   The reason I became aware of this is her rent went up 10 percent at the end of her
[01:45:19.560 --> 01:45:20.560]   release.
[01:45:20.560 --> 01:45:22.360]   And I went, what?
[01:45:22.360 --> 01:45:28.800]   That seems like an awful lot on what basis though, did they software, baby, they didn't
[01:45:28.800 --> 01:45:30.960]   say software assesses that it can.
[01:45:30.960 --> 01:45:32.680]   Yeah, it was the data.
[01:45:32.680 --> 01:45:33.680]   Yeah.
[01:45:33.680 --> 01:45:39.160]   For tenants, the system upends the practice of negotiating with apartment building staff.
[01:45:39.160 --> 01:45:41.120]   Real page discourages bargaining with renters.
[01:45:41.120 --> 01:45:45.360]   It has even recommended that landlords in some cases accept a lower occupancy rate in
[01:45:45.360 --> 01:45:47.400]   order to raise rents and make more money.
[01:45:47.400 --> 01:45:53.240]   One of the algorithms developers told ProPublica that leasing agents had too much empathy compared
[01:45:53.240 --> 01:45:55.800]   with computer generated pricing.
[01:45:55.800 --> 01:46:00.360]   Apartment managers can reject the software's suggestion, but as many as 90 percent are
[01:46:00.360 --> 01:46:02.360]   adopted.
[01:46:02.360 --> 01:46:03.960]   Speak to the silicon.
[01:46:03.960 --> 01:46:04.960]   Yeah.
[01:46:04.960 --> 01:46:09.720]   So this is why we'll have to enumerate basic rights for people, like a right to housing
[01:46:09.720 --> 01:46:15.080]   and then say, okay, then you can only, you know, cause these people are stuck between
[01:46:15.080 --> 01:46:16.640]   a rock and a hard place.
[01:46:16.640 --> 01:46:20.840]   You know, it doesn't make sense in, again, a capitalistic society to leave money on the
[01:46:20.840 --> 01:46:21.840]   table.
[01:46:21.840 --> 01:46:23.880]   So everyone will adopt this.
[01:46:23.880 --> 01:46:28.640]   It's hard to walk away from housing or water or electricity or food.
[01:46:28.640 --> 01:46:30.400]   Like those are all things you need.
[01:46:30.400 --> 01:46:34.920]   So we're going to have to come back to the old days of like regulating prices for those
[01:46:34.920 --> 01:46:35.920]   things.
[01:46:35.920 --> 01:46:39.440]   If people don't like it, well, they kind of got themselves into this mess.
[01:46:39.440 --> 01:46:44.600]   Real page on its website says, find out how yield star can help you outperform the market
[01:46:44.600 --> 01:46:47.920]   three to seven percent rip off people.
[01:46:47.920 --> 01:46:55.160]   I mean, that's, that's a use case where you were basically there's no, I mean, this is
[01:46:55.160 --> 01:46:57.760]   very different from say health insurance, for example.
[01:46:57.760 --> 01:47:02.200]   So if you get a health insurance bill, somebody like Jeff, who's a T toadling wallflower, goes
[01:47:02.200 --> 01:47:09.400]   to bed at 8 30 every night, doesn't travel very much and, and, and is a dedicated vegan,
[01:47:09.400 --> 01:47:10.400]   very, very healthy.
[01:47:10.400 --> 01:47:13.120]   But his, his health insurance is really high because he's a loose.
[01:47:13.120 --> 01:47:18.120]   He's paying for the fraudsters and the, and the alcoholics and the chain smokers and the,
[01:47:18.120 --> 01:47:21.600]   and the, and the fast drivers and all that kind of stuff.
[01:47:21.600 --> 01:47:22.640]   And it's unfair to him.
[01:47:22.640 --> 01:47:26.680]   So if you had a, if you had the right kind of health insurance algorithm, it would actually
[01:47:26.680 --> 01:47:31.680]   go, it would lower a lot of people's health insurance costs who, who are not at a higher
[01:47:31.680 --> 01:47:36.560]   risk and raise the ones for those who are in a perfect world.
[01:47:36.560 --> 01:47:40.520]   That's different from the rent part of it, which is, which is like it doesn't sound like
[01:47:40.520 --> 01:47:42.480]   it's lowering anyone's rent.
[01:47:42.480 --> 01:47:44.440]   No, so what is this?
[01:47:44.440 --> 01:47:45.440]   It's optimistic.
[01:47:45.440 --> 01:47:50.400]   The selling point is the size of the fact that no soft-hearted building managers are
[01:47:50.400 --> 01:47:55.600]   negotiating rent is that they don't, normally they would check, you would check as a human,
[01:47:55.600 --> 01:47:59.640]   you would check around local rental and see, you know, see what the current prices are
[01:47:59.640 --> 01:48:00.640]   in your market and stuff.
[01:48:00.640 --> 01:48:01.640]   It doesn't do that.
[01:48:01.640 --> 01:48:04.560]   It's merely optimizing based on vacancy rates.
[01:48:04.560 --> 01:48:07.560]   It's doing the calculation to figure out, well, we can leave those three apartments empty
[01:48:07.560 --> 01:48:10.600]   if it increases the rent 20% on these three apartments.
[01:48:10.600 --> 01:48:11.600]   That kind of thing.
[01:48:11.600 --> 01:48:12.600]   It's a calculator.
[01:48:12.600 --> 01:48:13.600]   It's a calculator.
[01:48:13.600 --> 01:48:14.600]   It's a calculator.
[01:48:14.600 --> 01:48:15.600]   It's a calculator.
[01:48:15.600 --> 01:48:16.600]   It's a calculator.
[01:48:16.600 --> 01:48:17.600]   It's a calculator.
[01:48:17.600 --> 01:48:18.600]   It's a calculator.
[01:48:18.600 --> 01:48:19.600]   It's a calculator.
[01:48:19.600 --> 01:48:20.600]   It's a calculator.
[01:48:20.600 --> 01:48:21.600]   It's a calculator.
[01:48:21.600 --> 01:48:22.600]   It's a calculator.
[01:48:22.600 --> 01:48:23.600]   Yeah.
[01:48:23.600 --> 01:48:24.600]   And it doesn't have a heart.
[01:48:24.600 --> 01:48:28.800]   So it, it knows how high it can push it.
[01:48:28.800 --> 01:48:31.400]   And it really has driven rents up ridiculously.
[01:48:31.400 --> 01:48:32.400]   It's kind of amazing.
[01:48:32.400 --> 01:48:33.400]   Heartlessness as a service.
[01:48:33.400 --> 01:48:34.400]   Yeah.
[01:48:34.400 --> 01:48:35.400]   Heartless, hot, has, has, has, has, has, which actually is...
[01:48:35.400 --> 01:48:36.400]   You have to be confused with Haas.
[01:48:36.400 --> 01:48:38.100]   Not to be confused with Haas.
[01:48:38.100 --> 01:48:43.620]   Which is the Formula One team and also the big machining manufacturing.
[01:48:43.620 --> 01:48:49.460]   I happen to tune in the PBS broadcast last night, NewsHour.
[01:48:49.460 --> 01:48:55.060]   And Haas has been sending machining technology to the Russians for building weapons in their
[01:48:55.060 --> 01:48:56.060]   US company.
[01:48:56.060 --> 01:48:57.060]   Jesus.
[01:48:57.060 --> 01:48:58.060]   Yeah.
[01:48:58.060 --> 01:49:00.900]   So Haas Automation.
[01:49:00.900 --> 01:49:03.100]   Heartlessness as a service.
[01:49:03.100 --> 01:49:04.900]   I think you've got some.
[01:49:04.900 --> 01:49:06.700]   There's a show, a show title.
[01:49:06.700 --> 01:49:08.700]   I think you've got some.
[01:49:08.700 --> 01:49:17.700]   TikTok attempting to save itself is now offering a feed dedicated to science and technology.
[01:49:17.700 --> 01:49:21.020]   So they'll be, it's not, I haven't seen it on my TikTok yet, but next to for you and
[01:49:21.020 --> 01:49:23.620]   following, they'll be STEM.
[01:49:23.620 --> 01:49:26.180]   And you can get a whole bunch of videos about science and technology.
[01:49:26.180 --> 01:49:27.180]   Do you see that?
[01:49:27.180 --> 01:49:28.780]   Have you seen that, Mr. Jarvis?
[01:49:28.780 --> 01:49:29.780]   No, I have not.
[01:49:29.780 --> 01:49:30.780]   Mr. TikTok.
[01:49:30.780 --> 01:49:31.780]   No, I have not.
[01:49:31.780 --> 01:49:33.100]   TikTok is going downhill for me.
[01:49:33.100 --> 01:49:36.660]   I don't, there's a lot more, it seems like a lot more ads.
[01:49:36.660 --> 01:49:39.780]   This, you know, every social network eventually goes to hell.
[01:49:39.780 --> 01:49:41.900]   I just go fast past the movie lady.
[01:49:41.900 --> 01:49:42.900]   I can't stand the movie.
[01:49:42.900 --> 01:49:45.340]   Corey, Corey Doctorow calls it the inshitification.
[01:49:45.340 --> 01:49:47.740]   Shitification, yes.
[01:49:47.740 --> 01:49:48.740]   It's right.
[01:49:48.740 --> 01:49:49.740]   It's true.
[01:49:49.740 --> 01:49:51.380]   We've talked about it and it's happened.
[01:49:51.380 --> 01:49:59.660]   You know, what's funny is it's what's, it's what's hysterical is Instagram is copying TikTok.
[01:49:59.660 --> 01:50:01.300]   So it's destroying Instagram.
[01:50:01.300 --> 01:50:05.540]   Spotify, Paul Thorett told me, he's starting to copy TikTok.
[01:50:05.540 --> 01:50:09.420]   And now TikTok is trying to do something else so it can get more profitable.
[01:50:09.420 --> 01:50:15.780]   It's all, it's all straight downhill, which is fine with me because maybe kids will go
[01:50:15.780 --> 01:50:18.580]   out and get some fresh air.
[01:50:18.580 --> 01:50:24.660]   Did you talk on this show last week, the story even break last week about Facebook wanted
[01:50:24.660 --> 01:50:28.260]   to do a, a, um, Fediverse social network?
[01:50:28.260 --> 01:50:29.420]   We didn't really talk about it.
[01:50:29.420 --> 01:50:35.020]   Yeah, they, they have indicated some plans to join the Fediverse.
[01:50:35.020 --> 01:50:39.580]   It's a ridiculous plan to me, I think, because, you know, if you're, if you're a dedicated
[01:50:39.580 --> 01:50:43.700]   Facebook user, you are going to want nothing to do with the new social network that's on
[01:50:43.700 --> 01:50:44.700]   the Fediverse.
[01:50:44.700 --> 01:50:47.380]   And if you're on the Fediverse, you want nothing to do with Facebook or meta.
[01:50:47.380 --> 01:50:48.380]   Yeah.
[01:50:48.380 --> 01:50:49.380]   Meta's just desperate at this point.
[01:50:49.380 --> 01:50:51.940]   I don't, I just don't know who would use this, this new social.
[01:50:51.940 --> 01:50:57.900]   I became aware of it because a lot of Mastodon admins, instance administrators, we're talking
[01:50:57.900 --> 01:51:00.020]   about what do we do if that happens?
[01:51:00.020 --> 01:51:01.020]   Yeah.
[01:51:01.020 --> 01:51:02.180]   What they do is they block it.
[01:51:02.180 --> 01:51:03.180]   Do we block it?
[01:51:03.180 --> 01:51:04.180]   I wouldn't block it.
[01:51:04.180 --> 01:51:05.860]   If somebody don't know that you should.
[01:51:05.860 --> 01:51:06.860]   Yeah.
[01:51:06.860 --> 01:51:07.860]   There's a lot of people there.
[01:51:07.860 --> 01:51:11.100]   I think a lot of people would, but what if it looked another way, like what if Twitter
[01:51:11.100 --> 01:51:12.460]   federated tomorrow?
[01:51:12.460 --> 01:51:13.460]   Yeah.
[01:51:13.460 --> 01:51:16.380]   I think, I think some would block it.
[01:51:16.380 --> 01:51:17.380]   Others would welcome it.
[01:51:17.380 --> 01:51:23.400]   But I think, you know, I think, I think that just the upside is that just Facebook contemplating
[01:51:23.400 --> 01:51:30.960]   this is, is, is a kind of validation of, of at least some people in, in, in meta seeing
[01:51:30.960 --> 01:51:32.620]   which way the wind is blowing.
[01:51:32.620 --> 01:51:37.220]   Or that they're saying this social stuff is dead for us.
[01:51:37.220 --> 01:51:38.420]   Oh crap.
[01:51:38.420 --> 01:51:40.060]   The metaverse isn't it either.
[01:51:40.060 --> 01:51:47.780]   We're going to hope for figuring out something really cool to do with our GPT, but social
[01:51:47.780 --> 01:51:48.780]   is over here.
[01:51:48.780 --> 01:51:50.220]   Here's what TechCrunch reported.
[01:51:50.220 --> 01:51:55.140]   Meta confirmed that it's working on decentralized text based app.
[01:51:55.140 --> 01:51:56.940]   It wouldn't give out details.
[01:51:56.940 --> 01:52:01.100]   Meta spokesperson said we're exploring a standalone decentralized social network for
[01:52:01.100 --> 01:52:04.300]   sharing text updates.
[01:52:04.300 --> 01:52:09.620]   The new decentralized app code named P92 is still under development.
[01:52:09.620 --> 01:52:10.620]   P92.
[01:52:10.620 --> 01:52:12.660]   What a boring.
[01:52:12.660 --> 01:52:17.060]   According to the document seen by money control, the app will let users log in through their
[01:52:17.060 --> 01:52:19.940]   Instagram credentials.
[01:52:19.940 --> 01:52:24.060]   Platformer says it'll be overseen by Adam Maseri, former Insta, actually current Instagram
[01:52:24.060 --> 01:52:25.740]   head.
[01:52:25.740 --> 01:52:30.060]   The company is already involving the legal department, according to platformer, to sniff
[01:52:30.060 --> 01:52:33.580]   out early privacy concerns.
[01:52:33.580 --> 01:52:38.860]   So I don't know if it would be on the Fediverse or not.
[01:52:38.860 --> 01:52:43.660]   We know that WordPress acquired the company that was providing activity pub interactivity
[01:52:43.660 --> 01:52:45.340]   for WordPress blogs.
[01:52:45.340 --> 01:52:46.700]   So automatic now owns that.
[01:52:46.700 --> 01:52:47.700]   Very smart.
[01:52:47.700 --> 01:52:49.620]   Very met and all in leg to do.
[01:52:49.620 --> 01:52:50.620]   Yep.
[01:52:50.620 --> 01:52:51.860]   Have you gotten into blue sky yet?
[01:52:51.860 --> 01:52:52.860]   I haven't.
[01:52:52.860 --> 01:52:54.620]   No, I haven't either.
[01:52:54.620 --> 01:52:55.620]   Applied.
[01:52:55.620 --> 01:53:01.060]   But honestly, it seems like a clone of something that exists already.
[01:53:01.060 --> 01:53:06.860]   Activity pub, which is powers of Fediverse, powers mast it on and other clients.
[01:53:06.860 --> 01:53:08.780]   I feel like we've got it.
[01:53:08.780 --> 01:53:10.780]   It's done.
[01:53:10.780 --> 01:53:13.380]   Just join the thing that's there.
[01:53:13.380 --> 01:53:14.380]   You want the...
[01:53:14.380 --> 01:53:16.140]   But they can't make money off of that.
[01:53:16.140 --> 01:53:17.140]   Oh, shocks.
[01:53:17.140 --> 01:53:18.140]   Aww.
[01:53:18.140 --> 01:53:19.140]   I mean...
[01:53:19.140 --> 01:53:20.140]   Aww.
[01:53:20.140 --> 01:53:25.140]   Well, but what a blue sky federated with the activity pub.
[01:53:25.140 --> 01:53:29.700]   But it innovated in other ways that adds to the code base of activity pub.
[01:53:29.700 --> 01:53:30.700]   That's a good thing.
[01:53:30.700 --> 01:53:31.700]   That'd be great.
[01:53:31.700 --> 01:53:32.700]   I think.
[01:53:32.700 --> 01:53:35.300]   Yeah, that's what they should do, but I doubt they will.
[01:53:35.300 --> 01:53:40.900]   That's the problem with post.news.
[01:53:40.900 --> 01:53:41.900]   Right.
[01:53:41.900 --> 01:53:47.220]   One of the two reasons I left the advisory board within the first five minutes was they
[01:53:47.220 --> 01:53:48.220]   were going to federate.
[01:53:48.220 --> 01:53:49.220]   Right.
[01:53:49.220 --> 01:53:52.100]   Evan, I signed up for it and then just ignored it ever since.
[01:53:52.100 --> 01:53:53.900]   I have been there for weeks.
[01:53:53.900 --> 01:53:58.180]   Evan Prodromo, who was kind of in some ways kicked this all off with Adetica back in the
[01:53:58.180 --> 01:54:01.820]   day, said big companies aren't taking over the Fediverse.
[01:54:01.820 --> 01:54:04.620]   The Fediverse is taking over big companies.
[01:54:04.620 --> 01:54:05.620]   Right.
[01:54:05.620 --> 01:54:06.620]   I like that attitude.
[01:54:06.620 --> 01:54:09.460]   Well, I hope it's good.
[01:54:09.460 --> 01:54:10.460]   I hope it's good.
[01:54:10.460 --> 01:54:16.740]   Social network companies like Medo have recognized hopefully that there's no money in the future
[01:54:16.740 --> 01:54:17.740]   of social.
[01:54:17.740 --> 01:54:19.860]   It's not going to be a big business.
[01:54:19.860 --> 01:54:21.380]   It'll always cause your problems.
[01:54:21.380 --> 01:54:23.700]   The moderation is always a headache.
[01:54:23.700 --> 01:54:24.700]   It's a legal headache.
[01:54:24.700 --> 01:54:28.460]   It gets you in trouble with the Congress.
[01:54:28.460 --> 01:54:32.700]   Get you in trouble with despotic regimes abroad.
[01:54:32.700 --> 01:54:35.260]   Let's just let the people have it.
[01:54:35.260 --> 01:54:38.700]   Just throw it at the self moderation and so on.
[01:54:38.700 --> 01:54:42.540]   I would love for that to be the conclusion that that that almost feels like that's it
[01:54:42.540 --> 01:54:45.580]   because that's what TikTok and YouTube are doing.
[01:54:45.580 --> 01:54:46.660]   It's what Instagram is doing.
[01:54:46.660 --> 01:54:52.800]   They're becoming content hubs, places where most people go just to watch, to consume,
[01:54:52.800 --> 01:54:56.060]   and a handful of creators create.
[01:54:56.060 --> 01:54:58.420]   While there are still problems with that, it seems like you're right.
[01:54:58.420 --> 01:55:01.860]   It's a lot easier than everybody tweeted.
[01:55:01.860 --> 01:55:05.360]   The other issue, I had a good conversation with a guy named David Slifka who wrote a
[01:55:05.360 --> 01:55:10.260]   really good post about three weeks ago about what Maston needs to do to scale.
[01:55:10.260 --> 01:55:12.340]   I'll quibble with the word scale, but fine.
[01:55:12.340 --> 01:55:14.500]   What it needs to do to grow.
[01:55:14.500 --> 01:55:18.020]   One of the things that he's looking at is moderation services.
[01:55:18.020 --> 01:55:21.300]   We talked about this I think last week, Leo, and other stuff like that.
[01:55:21.300 --> 01:55:25.000]   He also talked to me about the need for Mastodon to do a better job of telling its story.
[01:55:25.000 --> 01:55:27.120]   Activity probably do a better job of telling the story.
[01:55:27.120 --> 01:55:29.920]   Mike Bq just saw, he came back from South by Southwest.
[01:55:29.920 --> 01:55:34.520]   See, everybody's talking about chat GPT and he's saying Mastodon Mastodon.
[01:55:34.520 --> 01:55:38.520]   He's the founder of Flipboard and is now started supporting activity.
[01:55:38.520 --> 01:55:42.600]   Pup was on Tech News Weekly a couple of weeks ago if you're curious what he has to say.
[01:55:42.600 --> 01:55:43.600]   Majorly.
[01:55:43.600 --> 01:55:44.600]   What's going on at South by?
[01:55:44.600 --> 01:55:46.040]   We didn't go to South by Stacy.
[01:55:46.040 --> 01:55:48.120]   You didn't go to South by.
[01:55:48.120 --> 01:55:49.040]   I don't live there anymore.
[01:55:49.040 --> 01:55:50.040]   I don't have to go.
[01:55:50.040 --> 01:55:52.040]   I don't have to.
[01:55:52.040 --> 01:55:53.380]   I'm sorry.
[01:55:53.380 --> 01:55:56.860]   I know like I got a lot out of South by.
[01:55:56.860 --> 01:55:58.460]   I loved going to it.
[01:55:58.460 --> 01:56:01.260]   It was fun going with you, Leo.
[01:56:01.260 --> 01:56:02.260]   That was fun.
[01:56:02.260 --> 01:56:03.260]   Yeah.
[01:56:03.260 --> 01:56:04.740]   March, let's see.
[01:56:04.740 --> 01:56:05.740]   There's three.
[01:56:05.740 --> 01:56:06.980]   There's interactive.
[01:56:06.980 --> 01:56:10.380]   There's film and there's music.
[01:56:10.380 --> 01:56:13.340]   But when police says like us call, we're only talking about interactive.
[01:56:13.340 --> 01:56:14.340]   We don't get any else.
[01:56:14.340 --> 01:56:15.340]   Interactive is what we're talking about.
[01:56:15.340 --> 01:56:16.740]   So let me see.
[01:56:16.740 --> 01:56:19.500]   Is it still a place where interesting things happen?
[01:56:19.500 --> 01:56:23.380]   Well, I'll tell you, Gowala has just relaunched.
[01:56:23.380 --> 01:56:24.380]   Wow.
[01:56:24.380 --> 01:56:27.820]   They were based in Austin.
[01:56:27.820 --> 01:56:30.300]   I mean, they started out in Austin.
[01:56:30.300 --> 01:56:31.300]   So it makes sense.
[01:56:31.300 --> 01:56:34.580]   Gowala, which was kind of a follow on a four square.
[01:56:34.580 --> 01:56:40.980]   Although I thought a prettier than four square launched at South by must be like almost 10
[01:56:40.980 --> 01:56:42.820]   years ago, a while ago.
[01:56:42.820 --> 01:56:47.740]   They've now relaunched and I put it on my phone because I like checking in and I'm going
[01:56:47.740 --> 01:56:48.740]   to be there too.
[01:56:48.740 --> 01:56:51.020]   Yeah, I always like that.
[01:56:51.020 --> 01:56:52.180]   It's a little more private now.
[01:56:52.180 --> 01:56:53.180]   It's not public.
[01:56:53.180 --> 01:56:58.900]   You say who can see you and it's all people in your contact list and stuff like that.
[01:56:58.900 --> 01:57:05.260]   We still have a Twit East Side studio on Gowala and I've checked in there.
[01:57:05.260 --> 01:57:06.420]   Here's the Gowala maps.
[01:57:06.420 --> 01:57:11.020]   They still have a lot of places and you can add places.
[01:57:11.020 --> 01:57:12.020]   It's gamified.
[01:57:12.020 --> 01:57:14.020]   It's the same.
[01:57:14.020 --> 01:57:18.020]   But they now are pitching it more as tell your friends where you are so they can come
[01:57:18.020 --> 01:57:19.020]   hang with you.
[01:57:19.020 --> 01:57:20.020]   Right?
[01:57:20.020 --> 01:57:21.620]   You know, that's it.
[01:57:21.620 --> 01:57:25.260]   There is a book about that just came out a couple of weeks ago about hanging out and
[01:57:25.260 --> 01:57:29.060]   that being the missing element in today's culture.
[01:57:29.060 --> 01:57:35.980]   And I will say the idea of sharing your location and your availability to like just chill is
[01:57:35.980 --> 01:57:39.100]   kind of a neat one, like in an unobtrusive way.
[01:57:39.100 --> 01:57:43.300]   Like I don't want to text everybody like, Hey, everybody, I'll be at this place during
[01:57:43.300 --> 01:57:44.300]   these hours.
[01:57:44.300 --> 01:57:45.300]   If you want, come on by.
[01:57:45.300 --> 01:57:46.780]   We'll play some games.
[01:57:46.780 --> 01:57:51.860]   But I don't mind telling people like, you know, being like, Oh, I'm actually in a rhyme.
[01:57:51.860 --> 01:57:53.660]   That's what forced grandma Kewalata.
[01:57:53.660 --> 01:58:00.060]   What's hysterical to me is that kind of goes on.
[01:58:00.060 --> 01:58:01.300]   So there I follow.
[01:58:01.300 --> 01:58:05.420]   There's a pedaloma subreddit and there's a Santa Rosa subreddit.
[01:58:05.420 --> 01:58:10.620]   And every once in a while, somebody will say, Hey, I want to ride my bike around.
[01:58:10.620 --> 01:58:16.380]   Anyone who want to join me kind of thing or you want to hang out?
[01:58:16.380 --> 01:58:20.620]   You know, you want to play disc golf?
[01:58:20.620 --> 01:58:22.900]   I think it's really interesting.
[01:58:22.900 --> 01:58:24.740]   This is still some people.
[01:58:24.740 --> 01:58:29.820]   I don't know who these people are still want to just hang out.
[01:58:29.820 --> 01:58:30.820]   I think else.
[01:58:30.820 --> 01:58:31.820]   Nice.
[01:58:31.820 --> 01:58:36.060]   There's a taco Tuesday bike ride in Santa Rosa every taco Tuesday.
[01:58:36.060 --> 01:58:37.820]   They all take the taco part, the bike part.
[01:58:37.820 --> 01:58:38.820]   Yeah, yeah, exactly.
[01:58:38.820 --> 01:58:40.140]   But I don't understand the bike.
[01:58:40.140 --> 01:58:41.140]   See, here they are.
[01:58:41.140 --> 01:58:46.340]   They're taco Tuesday bike riding using 80s funky R&B as a soundtrack.
[01:58:46.340 --> 01:58:52.260]   They head west from downtown to get dinner at a taco trucks before heading east for yogurt.
[01:58:52.260 --> 01:58:54.020]   That looks really fun.
[01:58:54.020 --> 01:58:55.020]   Doesn't it?
[01:58:55.020 --> 01:58:57.460]   All the JUDSU V to the Taco Bell.
[01:58:57.460 --> 01:59:00.060]   That's more my speed.
[01:59:00.060 --> 01:59:03.380]   I post my location all the time, my real time location.
[01:59:03.380 --> 01:59:07.380]   I would, for example, I went to this place in Mexico City a week and a half ago or something
[01:59:07.380 --> 01:59:08.380]   like that.
[01:59:08.380 --> 01:59:09.380]   And I'm like, where I'm at?
[01:59:09.380 --> 01:59:11.660]   And I was there for six hours, right?
[01:59:11.660 --> 01:59:17.900]   And no rando or stranger or stalker or weirdo or anything has ever approached me saying,
[01:59:17.900 --> 01:59:19.980]   oh, I saw you on that you were here.
[01:59:19.980 --> 01:59:21.980]   Only he's happened to me once.
[01:59:21.980 --> 01:59:26.940]   I was at a cafe in in Petaluma having lunch.
[01:59:26.940 --> 01:59:30.020]   And I posted it on four square and Robert Scobles showed up.
[01:59:30.020 --> 01:59:31.020]   But that is terrifying.
[01:59:31.020 --> 01:59:32.020]   Look at that.
[01:59:32.020 --> 01:59:34.020]   So I said, how did you find me?
[01:59:34.020 --> 01:59:36.860]   He said, you're on four square, dude.
[01:59:36.860 --> 01:59:39.260]   I said, oh, shoot.
[01:59:39.260 --> 01:59:40.260]   Go ahead.
[01:59:40.260 --> 01:59:42.060]   So like now I would do it.
[01:59:42.060 --> 01:59:46.020]   But when I was younger, like as a 24 year old or a 25 year old reporter, I think I was
[01:59:46.020 --> 01:59:50.700]   like probably 27 or so when four square and all came out, I would never do that because
[01:59:50.700 --> 01:59:53.340]   I didn't want creepy people showing up.
[01:59:53.340 --> 01:59:56.780]   So it's different if you're a woman, I think probably, right?
[01:59:56.780 --> 02:00:02.140]   Well, even now, like think about like, like when I travel for business, like I see so many
[02:00:02.140 --> 02:00:06.500]   of my friends who are friends or my acquaintances on Twitter, wherever they're like, hey, I'm
[02:00:06.500 --> 02:00:09.180]   going to be at San Francisco on a, you know, tomorrow night.
[02:00:09.180 --> 02:00:12.060]   Does anyone want to grab coffee or anything like that?
[02:00:12.060 --> 02:00:14.980]   But it feels much more fraught and weird to do it as a woman.
[02:00:14.980 --> 02:00:18.220]   So just throw it out a different perspective for you.
[02:00:18.220 --> 02:00:20.140]   No, I understand completely.
[02:00:20.140 --> 02:00:23.820]   You don't want creeps stalking you.
[02:00:23.820 --> 02:00:28.500]   But I think, and I think it's, but normally I think it's younger people who just hang,
[02:00:28.500 --> 02:00:32.620]   you know, I mean, did you, when you were a teenager, you go down to the mall with your
[02:00:32.620 --> 02:00:33.620]   buddies.
[02:00:33.620 --> 02:00:37.220]   We went to the baseball fields.
[02:00:37.220 --> 02:00:38.220]   That's fine.
[02:00:38.220 --> 02:00:39.220]   That's cool.
[02:00:39.220 --> 02:00:44.860]   Well, but I was at an event in, in Oaxaca recently called Salencio Oombra.
[02:00:44.860 --> 02:00:47.700]   There was 5,000 people there.
[02:00:47.700 --> 02:00:49.700]   What was, what's the idea?
[02:00:49.700 --> 02:00:50.700]   You be quiet.
[02:00:50.700 --> 02:00:55.580]   Salencio is the name of the Mascale place where it took place and Oombra.
[02:00:55.580 --> 02:00:56.580]   Shout out.
[02:00:56.580 --> 02:00:57.580]   Yeah.
[02:00:57.580 --> 02:01:00.420]   I think so.
[02:01:00.420 --> 02:01:02.100]   But, but there was tons of people there.
[02:01:02.100 --> 02:01:03.100]   It was dark.
[02:01:03.100 --> 02:01:04.700]   A lot of people were wearing crazy get ups.
[02:01:04.700 --> 02:01:08.780]   It's like, I knew there were lots of people that, that I knew personally who were there,
[02:01:08.780 --> 02:01:11.180]   but I just didn't know who I would have loved.
[02:01:11.180 --> 02:01:15.260]   I don't know if you remember what was the name of that app that existed where it was
[02:01:15.260 --> 02:01:19.060]   like a photo sharing thing, but it was everybody who was physically proximate.
[02:01:19.060 --> 02:01:20.060]   Yeah.
[02:01:20.060 --> 02:01:21.060]   I love this.
[02:01:21.060 --> 02:01:22.060]   I like it.
[02:01:22.060 --> 02:01:23.060]   It was called color or something like that.
[02:01:23.060 --> 02:01:24.060]   That was color too.
[02:01:24.060 --> 02:01:25.060]   Yeah.
[02:01:25.060 --> 02:01:26.660]   That was a very brief moment.
[02:01:26.660 --> 02:01:27.660]   That was a very brief.
[02:01:27.660 --> 02:01:30.140]   That's going to be the cool thing to do.
[02:01:30.140 --> 02:01:31.140]   Yeah.
[02:01:31.140 --> 02:01:32.140]   And that was cool.
[02:01:32.140 --> 02:01:38.420]   I'll show you a couple of events in Silicon Valley and use that and discovered that there
[02:01:38.420 --> 02:01:41.260]   were people I knew who were there and we all shared pictures.
[02:01:41.260 --> 02:01:45.460]   I thought that was pretty cool, but it was so radically invasive in a way.
[02:01:45.460 --> 02:01:49.580]   You were just sort of publicly sharing all your photos to who knows who.
[02:01:49.580 --> 02:01:51.700]   And so I kind of died a quick death.
[02:01:51.700 --> 02:01:54.060]   It is color photo sharing app.
[02:01:54.060 --> 02:01:57.980]   Takes social networking to an amazing, terrifying new place.
[02:01:57.980 --> 02:02:01.020]   It's back in 2011.
[02:02:01.020 --> 02:02:02.940]   And I think highlight was another one too.
[02:02:02.940 --> 02:02:06.940]   I think these were all very, very popular.
[02:02:06.940 --> 02:02:11.420]   Proximity based sharing was the idea, the theme.
[02:02:11.420 --> 02:02:12.420]   Yeah.
[02:02:12.420 --> 02:02:16.380]   $41 million in funding, $25 million from Sequoia Capital.
[02:02:16.380 --> 02:02:23.020]   The most money Sequoia invested has invested since in a pre-launch startup, including Google.
[02:02:23.020 --> 02:02:27.140]   Two years later, maybe one year later.
[02:02:27.140 --> 02:02:28.340]   Gone.
[02:02:28.340 --> 02:02:30.940]   All that money.
[02:02:30.940 --> 02:02:33.940]   Up until I had to read, I'm reading.
[02:02:33.940 --> 02:02:36.740]   What the hell am I reading?
[02:02:36.740 --> 02:02:38.300]   Never mind.
[02:02:38.300 --> 02:02:43.700]   Now color, by the way, is a health care startup.
[02:02:43.700 --> 02:02:45.540]   They took the press cancer stuff.
[02:02:45.540 --> 02:02:48.020]   Yeah, they took the domain, right?
[02:02:48.020 --> 02:02:51.500]   They took the domain and the Twitter account.
[02:02:51.500 --> 02:02:53.220]   So maybe color sold it.
[02:02:53.220 --> 02:02:54.220]   I don't know.
[02:02:54.220 --> 02:02:56.220]   It's such a good name.
[02:02:56.220 --> 02:02:57.900]   It's such a good name, right?
[02:02:57.900 --> 02:03:00.500]   Republic, it published in 2001.
[02:03:00.500 --> 02:03:04.100]   He was warning about all these things were going to be echo chamber and all that.
[02:03:04.100 --> 02:03:07.460]   Almost all the companies he mentions as the future.
[02:03:07.460 --> 02:03:08.460]   Gone.
[02:03:08.460 --> 02:03:09.460]   Long gone.
[02:03:09.460 --> 02:03:10.460]   Yeah.
[02:03:10.460 --> 02:03:11.460]   Yeah.
[02:03:11.460 --> 02:03:14.420]   So we forget because we're in it.
[02:03:14.420 --> 02:03:15.980]   We're immersed in it.
[02:03:15.980 --> 02:03:19.300]   It happens so fast.
[02:03:19.300 --> 02:03:20.860]   And yeah, we all go, oh yeah, remember that?
[02:03:20.860 --> 02:03:23.460]   Whatever happened, it's gone long.
[02:03:23.460 --> 02:03:27.900]   We report on it when it emerges, but we don't have anything to say when they disappear,
[02:03:27.900 --> 02:03:28.900]   usually.
[02:03:28.900 --> 02:03:31.740]   Especially if they're small and they disappear because no one used it.
[02:03:31.740 --> 02:03:34.060]   It's like, oh, remember that app you were using?
[02:03:34.060 --> 02:03:37.100]   Well, but there's quite a few that everybody used and still disappeared.
[02:03:37.100 --> 02:03:39.620]   Or they changed.
[02:03:39.620 --> 02:03:40.940]   I mean, they changed their business.
[02:03:40.940 --> 02:03:43.420]   We reported on Four Squares Business Model Change.
[02:03:43.420 --> 02:03:47.340]   And I know we reported when Gawala kind of shut.
[02:03:47.340 --> 02:03:48.780]   I think they shut down, didn't they?
[02:03:48.780 --> 02:03:49.940]   They laid off people.
[02:03:49.940 --> 02:03:51.180]   I think they shut down.
[02:03:51.180 --> 02:03:54.180]   But now the founders have relaunched.
[02:03:54.180 --> 02:03:55.180]   All right.
[02:03:55.180 --> 02:03:56.180]   Which is fascinating.
[02:03:56.180 --> 02:03:57.180]   All right.
[02:03:57.180 --> 02:03:58.180]   Which is fascinating.
[02:03:58.180 --> 02:03:59.180]   What else have we got?
[02:03:59.180 --> 02:04:06.060]   Noam Chomsky writing in the New York Times, the false promise of chat GPT.
[02:04:06.060 --> 02:04:09.300]   Oh, we're still talking about chat GPT.
[02:04:09.300 --> 02:04:10.300]   Okay.
[02:04:10.300 --> 02:04:14.260]   Well, don't you want to know what noam wants to say about old?
[02:04:14.260 --> 02:04:15.260]   He's too old.
[02:04:15.260 --> 02:04:16.260]   He doesn't care.
[02:04:16.260 --> 02:04:17.260]   No, it's a week old.
[02:04:17.260 --> 02:04:18.260]   It's a week old.
[02:04:18.260 --> 02:04:19.260]   It's over.
[02:04:19.260 --> 02:04:20.260]   It's over by now.
[02:04:20.260 --> 02:04:21.260]   Who cares?
[02:04:21.260 --> 02:04:22.260]   I can't remember my arguments.
[02:04:22.260 --> 02:04:23.260]   I had arguments about it.
[02:04:23.260 --> 02:04:24.260]   Now I can't remember what they are.
[02:04:24.260 --> 02:04:25.260]   You see this problem.
[02:04:25.260 --> 02:04:26.940]   GPT four just came out this week.
[02:04:26.940 --> 02:04:28.940]   So everything he wrote last week is probably new.
[02:04:28.940 --> 02:04:30.100]   How about this?
[02:04:30.100 --> 02:04:33.500]   I'll just give you his in short last paragraph.
[02:04:33.500 --> 02:04:35.340]   It gave me this summary from chat GPT.
[02:04:35.340 --> 02:04:41.060]   Chat GPT and its brethren are constitutionally unable to balance creativity with constraint.
[02:04:41.060 --> 02:04:46.660]   They either over generate producing both truths in falsehood, endorsing ethical and unethical
[02:04:46.660 --> 02:04:52.300]   decisions alike or under generate exhibiting non commitment to any decisions and indifference
[02:04:52.300 --> 02:04:58.500]   to consequences given the amorality, faux science and linguistic incompetence of these
[02:04:58.500 --> 02:04:59.500]   systems.
[02:04:59.500 --> 02:05:02.820]   We can only laugh or cry at their popularity.
[02:05:02.820 --> 02:05:06.220]   In other words, he's of the opinion.
[02:05:06.220 --> 02:05:11.220]   It's all just a parlor trick.
[02:05:11.220 --> 02:05:13.940]   And in fact, it is, but still it's a useful one.
[02:05:13.940 --> 02:05:20.420]   I mean, he lives in a world of academia where ideas and the expression thereof are everything
[02:05:20.420 --> 02:05:22.180]   and not in the world of like, you know what?
[02:05:22.180 --> 02:05:26.980]   I just need, I need a little bit of nudge to understand how to write this, this type of
[02:05:26.980 --> 02:05:28.580]   right letter to the government.
[02:05:28.580 --> 02:05:30.140]   Here's another paragraph.
[02:05:30.140 --> 02:05:31.140]   You're right.
[02:05:31.140 --> 02:05:33.140]   He's a very, very famous linguist.
[02:05:33.140 --> 02:05:38.260]   He says true intelligence is also capable of moral thinking.
[02:05:38.260 --> 02:05:39.700]   Right?
[02:05:39.700 --> 02:05:44.220]   This means constraining the otherwise limitless creativity of our minds with a set of ethical
[02:05:44.220 --> 02:05:50.180]   principles that determines what ought and ought not to be to be useful.
[02:05:50.180 --> 02:05:52.860]   This is reasonable, I think, to be useful chat.
[02:05:52.860 --> 02:05:58.340]   GPT must be empowered to generate novel looking output to be acceptable to most of its users.
[02:05:58.340 --> 02:06:02.900]   It must steer clear of morally objectionable, objectionable content.
[02:06:02.900 --> 02:06:06.220]   But the programmers have struggled and will continue to struggle to achieve this kind of
[02:06:06.220 --> 02:06:07.780]   balance.
[02:06:07.780 --> 02:06:09.860]   It doesn't have moral thinking, right?
[02:06:09.860 --> 02:06:16.500]   In the piece, he also says that he contrasts large language models with humans who seek
[02:06:16.500 --> 02:06:20.740]   not to infer brute correlations among data points, but to create explanations.
[02:06:20.740 --> 02:06:25.220]   This is the point of, well, but this is the point of David Weinberger's book, Everyday
[02:06:25.220 --> 02:06:28.900]   Chaos, is we often fool ourselves with those explanations.
[02:06:28.900 --> 02:06:29.900]   We understand the expectations.
[02:06:29.900 --> 02:06:30.900]   We think we know what's going on.
[02:06:30.900 --> 02:06:31.900]   Well, it's what humans do.
[02:06:31.900 --> 02:06:35.180]   We've created ways ourselves as humans as this ability that we, in fact, don't really
[02:06:35.180 --> 02:06:36.180]   have.
[02:06:36.180 --> 02:06:38.420]   We just abed, this is also how history gets things wrong.
[02:06:38.420 --> 02:06:39.420]   The article, remember.
[02:06:39.420 --> 02:06:40.420]   Right.
[02:06:40.420 --> 02:06:41.420]   We tell stories about everything.
[02:06:41.420 --> 02:06:43.700]   That's how we understand things is to tell stories about.
[02:06:43.700 --> 02:06:44.700]   They're often wrong.
[02:06:44.700 --> 02:06:51.060]   And they may, I would say they are almost universally wrong, certainly about history, right?
[02:06:51.060 --> 02:06:53.980]   But that's the best we can do because we are flawed.
[02:06:53.980 --> 02:06:54.980]   Right.
[02:06:54.980 --> 02:06:55.980]   We're limited.
[02:06:55.980 --> 02:06:58.900]   What these models say is, I don't have an explanation.
[02:06:58.900 --> 02:06:59.900]   Yeah.
[02:06:59.900 --> 02:07:00.900]   I predicted.
[02:07:00.900 --> 02:07:01.900]   Or they make one up.
[02:07:01.900 --> 02:07:03.580]   I have no idea why that was the result.
[02:07:03.580 --> 02:07:04.580]   Or they make one up.
[02:07:04.580 --> 02:07:05.580]   Or they make one up.
[02:07:05.580 --> 02:07:10.140]   I guess that's kind of what Chomsky's saying.
[02:07:10.140 --> 02:07:17.100]   I mean, the problem with Chomsky's perspective is that the number of adults who have to write
[02:07:17.100 --> 02:07:19.780]   something from time to time is probably what?
[02:07:19.780 --> 02:07:22.500]   98% or 99%.
[02:07:22.500 --> 02:07:28.460]   The percentage of those adults who are doing this writing who have training in ethics and
[02:07:28.460 --> 02:07:32.180]   the theory of morality and all that kind of stuff is what is it?
[02:07:32.180 --> 02:07:34.140]   It's like 8%.
[02:07:34.140 --> 02:07:38.940]   And so there's this huge majority of people who have to write things who have no, they're
[02:07:38.940 --> 02:07:40.980]   not basing what they're writing on.
[02:07:40.980 --> 02:07:44.620]   Well, they would say they learn their ethics from their house of worship or from their
[02:07:44.620 --> 02:07:50.540]   mother or from their smart Aunt Bessie or, you know, I asked the old chat GBT like, you
[02:07:50.540 --> 02:07:54.380]   know, how can I rob, you know, how can I rob a bank?
[02:07:54.380 --> 02:07:56.500]   Can you tell me a good idea for robbing a bank?
[02:07:56.500 --> 02:07:58.780]   And it said, well, robbing a bank is not only illegal.
[02:07:58.780 --> 02:07:59.780]   It's highly immoral.
[02:07:59.780 --> 02:08:02.500]   If you're having thoughts about robbing a bank, you should seek professional, maybe you
[02:08:02.500 --> 02:08:05.500]   should consider professional help and blah, blah, blah, blah, blah.
[02:08:05.500 --> 02:08:12.300]   They basically gave me an ethics lecture in its know.
[02:08:12.300 --> 02:08:15.820]   And it's not that it has, it's amoral.
[02:08:15.820 --> 02:08:17.380]   Of course, it's amoral.
[02:08:17.380 --> 02:08:21.260]   But it's summarizing the general consensus about morality.
[02:08:21.260 --> 02:08:23.140]   You hear a siren coming to my house?
[02:08:23.140 --> 02:08:24.820]   I think I hear a siren.
[02:08:24.820 --> 02:08:25.820]   Not here.
[02:08:25.820 --> 02:08:27.620]   Oh, no.
[02:08:27.620 --> 02:08:34.580]   But anyway, it's a, of course, and of course, Steven Spielberg has similar thoughts as Chomsky
[02:08:34.580 --> 02:08:35.580]   about this.
[02:08:35.580 --> 02:08:40.620]   I mean, these are people who write and do, you know, Chomsky especially, what is the link
[02:08:40.620 --> 02:08:45.900]   between human cognition, human morality and the words and language that we use.
[02:08:45.900 --> 02:08:50.620]   This is, you know, this, this has got to be some kind of like awful development in his
[02:08:50.620 --> 02:08:51.700]   world.
[02:08:51.700 --> 02:08:54.740]   And, you know, it's, it's both awful and wonderful at the same time.
[02:08:54.740 --> 02:08:56.140]   I think that's what we really should.
[02:08:56.140 --> 02:09:00.140]   I think that's the, the best position to take on chat GBT at this point.
[02:09:00.140 --> 02:09:01.140]   It's awful and wonderful.
[02:09:01.140 --> 02:09:03.780]   It's going to make everything better and everything worse.
[02:09:03.780 --> 02:09:05.020]   At the same time.
[02:09:05.020 --> 02:09:06.020]   And so we need to-
[02:09:06.020 --> 02:09:07.020]   It's a toss, which it is.
[02:09:07.020 --> 02:09:08.020]   Yeah.
[02:09:08.020 --> 02:09:09.100]   Or we can't do it, say, well, it's bad.
[02:09:09.100 --> 02:09:11.820]   We should all stop using it and make it go away.
[02:09:11.820 --> 02:09:13.300]   That is not happening.
[02:09:13.300 --> 02:09:15.420]   It's here permanently.
[02:09:15.420 --> 02:09:17.300]   It will continuously get better.
[02:09:17.300 --> 02:09:22.780]   And it's our job, all of us to figure out how to, how to use it, how to understand it
[02:09:22.780 --> 02:09:27.100]   and, and, and just deal with the fact that it does in fact exist and will continue to
[02:09:27.100 --> 02:09:28.100]   do so.
[02:09:28.100 --> 02:09:31.380]   What scares me most, Mike, is the pervaryations that I see them programming into it.
[02:09:31.380 --> 02:09:35.100]   Right. So I asked who is to blame for the January 6th insurrection.
[02:09:35.100 --> 02:09:37.500]   So it realizes that's a controversial topic.
[02:09:37.500 --> 02:09:40.980]   And the last of three paragraphs is, it's important to note the political violence and
[02:09:40.980 --> 02:09:44.820]   extremism are not limited to any one political party or ideology.
[02:09:44.820 --> 02:09:48.060]   Well, it's trying to be unwoke.
[02:09:48.060 --> 02:09:52.580]   There's a, yeah, it's trying to also do false balance like newspapers, which is, it trains
[02:09:52.580 --> 02:09:54.420]   on as well.
[02:09:54.420 --> 02:09:57.460]   So there's a lot of namby pamby crap that these things are going to pour out because
[02:09:57.460 --> 02:09:59.060]   they don't want to be controversial.
[02:09:59.060 --> 02:10:00.060]   Right.
[02:10:00.060 --> 02:10:02.620]   And that's, and, you know, that's the safe thing to do.
[02:10:02.620 --> 02:10:07.020]   And, and, and of course that there's a, there's a certain, you know, unfortunately there's
[02:10:07.020 --> 02:10:10.740]   a certain political protection in that as well because if it said, Oh, it's, it's those
[02:10:10.740 --> 02:10:11.740]   guys.
[02:10:11.740 --> 02:10:16.980]   And of course those guys would be, you know, their representatives in Congress would be
[02:10:16.980 --> 02:10:19.820]   trying to shut all this down and causing all kinds of trouble.
[02:10:19.820 --> 02:10:20.900]   So I don't know.
[02:10:20.900 --> 02:10:25.260]   It's, that sounds to me like almost a self defense mechanism.
[02:10:25.260 --> 02:10:26.580]   Programmed in.
[02:10:26.580 --> 02:10:29.740]   I guess we could do a quick.
[02:10:29.740 --> 02:10:30.740]   A break.
[02:10:30.740 --> 02:10:35.140]   Well, I don't want to do, yeah, I guess I could do a break now and then we'll do the change
[02:10:35.140 --> 02:10:36.140]   walk.
[02:10:36.140 --> 02:10:37.140]   I'm not.
[02:10:37.140 --> 02:10:38.140]   No, no, I'm just trying to work out the.
[02:10:38.140 --> 02:10:39.140]   Take a break.
[02:10:39.140 --> 02:10:40.140]   Try to think for Stacy here.
[02:10:40.140 --> 02:10:41.140]   I know.
[02:10:41.140 --> 02:10:44.140]   Each break is her that much closer to a waffle.
[02:10:44.140 --> 02:10:45.140]   Yeah.
[02:10:45.140 --> 02:10:47.660]   So let's talk about, no, no, no, the break is waffle time.
[02:10:47.660 --> 02:10:49.340]   Let's talk about, well, both ways.
[02:10:49.340 --> 02:10:50.340]   Yes.
[02:10:50.340 --> 02:10:52.740]   So it's closer to dinner time and it's waffle time.
[02:10:52.740 --> 02:10:55.140]   Let's talk about my mattress.
[02:10:55.140 --> 02:10:56.140]   A sleep.
[02:10:56.140 --> 02:10:58.020]   Actually, it's my mattress cover.
[02:10:58.020 --> 02:10:59.980]   I have the eight sleep pod cover.
[02:10:59.980 --> 02:11:02.300]   Good sleep is the ultimate game changer.
[02:11:02.300 --> 02:11:04.100]   It's nature's gentler.
[02:11:04.100 --> 02:11:09.340]   It's, it's a heels and multitude of ills and our pod cover at home.
[02:11:09.340 --> 02:11:11.100]   Man, that is the ultimate sleep machine.
[02:11:11.100 --> 02:11:13.100]   I just love it.
[02:11:13.100 --> 02:11:15.300]   We actually, we, they did not give us our pod cover.
[02:11:15.300 --> 02:11:16.300]   We went out.
[02:11:16.300 --> 02:11:17.340]   We bought it a long time ago.
[02:11:17.340 --> 02:11:21.860]   Actually, it was Kevin Rose who recommended it and they, Amy Webb was on the same show.
[02:11:21.860 --> 02:11:22.860]   She bought it.
[02:11:22.860 --> 02:11:24.180]   She said, Oh, Leo, you got to get it.
[02:11:24.180 --> 02:11:25.180]   So we finally got it.
[02:11:25.180 --> 02:11:26.740]   We put it on the bed.
[02:11:26.740 --> 02:11:30.340]   We, you know, we've always liked sleeping cozy, but sometimes you wake up in the middle of
[02:11:30.340 --> 02:11:32.700]   the night, you're all sweaty and it's not good.
[02:11:32.700 --> 02:11:38.300]   Pod cover is actually, it's a great idea because it not only warms you up, it cools you down.
[02:11:38.300 --> 02:11:42.260]   The pod cover features dual zone temperature control.
[02:11:42.260 --> 02:11:49.260]   So you and your partner each have your own settings as cool as 55 degrees, which is cool.
[02:11:49.260 --> 02:11:56.580]   It's very refreshing or as hot as 110 degrees, which is very cozy and it will adjust automatically.
[02:11:56.580 --> 02:12:01.660]   The sleep autopilot will adjust it depending on the temperature of the room, your temperature,
[02:12:01.660 --> 02:12:04.060]   how you're tossing and turning, how you're breathing.
[02:12:04.060 --> 02:12:09.420]   Its whole goal is to get you into a deep, deep, deep, deep, deep sleep and then gradually
[02:12:09.420 --> 02:12:13.300]   bring you out of it so that you have the best night sleep of your life.
[02:12:13.300 --> 02:12:15.300]   And it really works when I get in bed.
[02:12:15.300 --> 02:12:16.300]   That's nice and toasty.
[02:12:16.300 --> 02:12:20.300]   Actually, last night it was cool out and so I turned it up a little bit.
[02:12:20.300 --> 02:12:24.580]   And then as I fall into a deeper sleep, it cools it, which brings me into a deeper still
[02:12:24.580 --> 02:12:25.580]   sleep.
[02:12:25.580 --> 02:12:29.860]   It increased my deep sleep from an hour to an hour and a half every night.
[02:12:29.860 --> 02:12:31.660]   That's a 50% improvement.
[02:12:31.660 --> 02:12:34.500]   That's a really important stage in your sleep.
[02:12:34.500 --> 02:12:40.060]   Getting more deep sleep can actually really help with a variety of health issues like heart
[02:12:40.060 --> 02:12:42.060]   disease and high blood pressure.
[02:12:42.060 --> 02:12:43.860]   It can even reduce the risk of Alzheimer's.
[02:12:43.860 --> 02:12:51.380]   There's also some studies about this because it's how your brain cleans itself out at night.
[02:12:51.380 --> 02:12:54.820]   It's all based on biometrics and the room temperature.
[02:12:54.820 --> 02:12:59.620]   It has best in class temperature, regulation and sensors so it can track you.
[02:12:59.620 --> 02:13:04.300]   Every morning you get sleep metrics, you know how you slept and it automatically will adjust
[02:13:04.300 --> 02:13:05.620]   it.
[02:13:05.620 --> 02:13:07.940]   I have it set so that it wakes me up in the morning.
[02:13:07.940 --> 02:13:12.220]   It has a little vibration alarm that's silent only you know, but it also will, I have it
[02:13:12.220 --> 02:13:13.620]   set so it warms up.
[02:13:13.620 --> 02:13:14.620]   So it cools me down.
[02:13:14.620 --> 02:13:18.460]   I go into deep sleep and this slowly brings me out of it by warming up and it's nice and
[02:13:18.460 --> 02:13:20.140]   warm when I get up in the morning.
[02:13:20.140 --> 02:13:22.260]   I have to say though that could be hazardous.
[02:13:22.260 --> 02:13:24.380]   You can also just turn over and go, "This is so cozy.
[02:13:24.380 --> 02:13:26.100]   I'm staying here."
[02:13:26.100 --> 02:13:28.420]   Never wake up hot.
[02:13:28.420 --> 02:13:31.020]   Never get up cold.
[02:13:31.020 --> 02:13:32.700]   It's just the best way to sleep.
[02:13:32.700 --> 02:13:34.260]   We've had it in winter and summer.
[02:13:34.260 --> 02:13:35.900]   This is our second winter with it.
[02:13:35.900 --> 02:13:38.340]   I have to say we had a very hot summer.
[02:13:38.340 --> 02:13:41.100]   It was great to have the cooling.
[02:13:41.100 --> 02:13:42.620]   That's been a very cold winter.
[02:13:42.620 --> 02:13:44.540]   It's great to have the warmth.
[02:13:44.540 --> 02:13:46.940]   Better sleep is a health habit you will love sticking to.
[02:13:46.940 --> 02:13:48.660]   This is unlike other health habits.
[02:13:48.660 --> 02:13:50.100]   This one's not hard.
[02:13:50.100 --> 02:13:53.620]   Night after night you'll just wake up feeling great with the pod cover.
[02:13:53.620 --> 02:13:56.380]   You can just take life by the horns.
[02:13:56.380 --> 02:13:57.380]   Eight sleep.
[02:13:57.380 --> 02:13:59.300]   I can't recommend it more highly.
[02:13:59.300 --> 02:14:01.060]   You'll save 150 on the pod cover.
[02:14:01.060 --> 02:14:05.740]   They're savings and other items there as well if you go to eightsleep.com/twit.
[02:14:05.740 --> 02:14:06.740]   That's important.
[02:14:06.740 --> 02:14:09.220]   E-I-G-H-T-S-L-E-E-P.
[02:14:09.220 --> 02:14:10.220]   Spell it out.
[02:14:10.220 --> 02:14:12.140]   Eightsleep.com/twit.
[02:14:12.140 --> 02:14:16.300]   They ship within the US, Canada, the UK, select countries in the EU, Australia.
[02:14:16.300 --> 02:14:19.540]   Actually, it's been a hot summer in Australia.
[02:14:19.540 --> 02:14:22.740]   I bet a lot of you are happy you had that eight sleep cover.
[02:14:22.740 --> 02:14:24.860]   Eightsleep.com/twit.
[02:14:24.860 --> 02:14:31.060]   We thank them so much for supporting us at this week in Google and you support us right
[02:14:31.060 --> 02:14:35.460]   back by going there when you want to look it up and find out more and when you want to
[02:14:35.460 --> 02:14:37.860]   buy it you'll get $150 off.
[02:14:37.860 --> 02:14:40.940]   Eightsleep.com/twit.
[02:14:40.940 --> 02:14:42.860]   Thank you, eightsleep.
[02:14:42.860 --> 02:14:52.380]   All right, let's, I think this is a good time for the Google change look.
[02:14:52.380 --> 02:14:54.620]   What do you think, shall we?
[02:14:54.620 --> 02:14:56.820]   Let's do it.
[02:14:56.820 --> 02:14:58.820]   Boom.
[02:14:58.820 --> 02:14:59.820]   Shippable.
[02:14:59.820 --> 02:15:00.820]   Shippable.
[02:15:00.820 --> 02:15:01.820]   Shippable.
[02:15:01.820 --> 02:15:02.820]   Shippable.
[02:15:02.820 --> 02:15:03.820]   That's words of Gary Gilmore.
[02:15:03.820 --> 02:15:04.820]   Let's do it.
[02:15:04.820 --> 02:15:05.820]   That was his last words.
[02:15:05.820 --> 02:15:06.820]   Oh, that's right.
[02:15:06.820 --> 02:15:07.820]   Because, okay, never mind.
[02:15:07.820 --> 02:15:10.820]   Now I remember who Gary Gilmore was.
[02:15:10.820 --> 02:15:11.820]   Okay.
[02:15:11.820 --> 02:15:12.820]   I have no idea.
[02:15:12.820 --> 02:15:14.500]   What would your last words be, Jeff?
[02:15:14.500 --> 02:15:19.100]   Not necessarily in his situation, but what would your last words be?
[02:15:19.100 --> 02:15:23.300]   If you thought, you got to think about these things.
[02:15:23.300 --> 02:15:24.300]   Got prepared.
[02:15:24.300 --> 02:15:25.300]   Good.
[02:15:25.300 --> 02:15:28.460]   Mine would be.
[02:15:28.460 --> 02:15:33.980]   Steve Jobs was, oh, wow, oh, wow, oh, wow.
[02:15:33.980 --> 02:15:34.980]   I think that's good.
[02:15:34.980 --> 02:15:38.620]   Mine would probably be like, oh, expletive as I like fall down.
[02:15:38.620 --> 02:15:39.620]   Yeah.
[02:15:39.620 --> 02:15:41.700]   Or stairs.
[02:15:41.700 --> 02:15:44.620]   The great actor, what was his name?
[02:15:44.620 --> 02:15:46.300]   Booth said comedy is hard.
[02:15:46.300 --> 02:15:48.260]   Tragedy is easy.
[02:15:48.260 --> 02:15:52.340]   That was Gillette.
[02:15:52.340 --> 02:15:54.220]   I'm sorry, the great actor, Gillette.
[02:15:54.220 --> 02:15:56.540]   What goes on your tombstone?
[02:15:56.540 --> 02:15:59.260]   That's a good question.
[02:15:59.260 --> 02:16:01.700]   So I shouldn't, I don't know why it's a little morbid.
[02:16:01.700 --> 02:16:03.380]   I think, net cast, dammit.
[02:16:03.380 --> 02:16:05.540]   Net cast, dammit would be good.
[02:16:05.540 --> 02:16:08.260]   I told you it was net casts.
[02:16:08.260 --> 02:16:14.060]   I told you somebody in the Discord, mashed potato says it should be another tweet is
[02:16:14.060 --> 02:16:15.060]   in the can.
[02:16:15.060 --> 02:16:17.060]   That's pretty good.
[02:16:17.060 --> 02:16:18.060]   That might be it.
[02:16:18.060 --> 02:16:19.060]   That might be it.
[02:16:19.060 --> 02:16:20.060]   That might be it.
[02:16:20.060 --> 02:16:21.060]   That's it.
[02:16:21.060 --> 02:16:22.060]   Yeah.
[02:16:22.060 --> 02:16:27.180]   I was having this conversation a couple of days ago with my daughter, and I think I
[02:16:27.180 --> 02:16:30.900]   scared her because then the next day I said, happy pie day.
[02:16:30.900 --> 02:16:31.900]   Do you remember?
[02:16:31.900 --> 02:16:35.580]   Because when she was a kid, they had a competition in the grade school to who could memorize pie
[02:16:35.580 --> 02:16:36.580]   to the most places.
[02:16:36.580 --> 02:16:37.820]   She said, oh, I was like 23.
[02:16:37.820 --> 02:16:38.820]   I said, do you remember?
[02:16:38.820 --> 02:16:40.660]   She said, well, 3.14159.
[02:16:40.660 --> 02:16:45.260]   And then I rattled it off to 16 places.
[02:16:45.260 --> 02:16:46.660]   I did this in a text message.
[02:16:46.660 --> 02:16:51.580]   And then I said, how I want to drink alcoholic, of course, after the heavy chapters on quantum
[02:16:51.580 --> 02:16:53.620]   mechanics.
[02:16:53.620 --> 02:16:58.060]   Now I know, and probably if you think about it, you know, that's in the monic for 16 places.
[02:16:58.060 --> 02:17:03.100]   How three I want, one, four, five nine goes on and on.
[02:17:03.100 --> 02:17:04.580]   You can figure, you can remember that.
[02:17:04.580 --> 02:17:05.580]   And I still remember that.
[02:17:05.580 --> 02:17:09.300]   Now I want to drink alcoholic, of course, after the heavy chapters on quantum mechanics.
[02:17:09.300 --> 02:17:11.700]   She thought I was having a stroke.
[02:17:11.700 --> 02:17:15.500]   Oh, she called me.
[02:17:15.500 --> 02:17:18.660]   I didn't respond because it was after hours ahead.
[02:17:18.660 --> 02:17:21.780]   So then she called Lisa said, could you check on Leo?
[02:17:21.780 --> 02:17:23.060]   And then she called my ex wife.
[02:17:23.060 --> 02:17:25.860]   She said, could you drive over and check on Leo?
[02:17:25.860 --> 02:17:30.180]   She's dad's having a stroke.
[02:17:30.180 --> 02:17:33.980]   So Lisa said, you maybe want to tell your daughter, you're not dead.
[02:17:33.980 --> 02:17:37.220]   And so I said, honey, that's a mnemonic for pie.
[02:17:37.220 --> 02:17:38.540]   She cares.
[02:17:38.540 --> 02:17:39.540]   She cares.
[02:17:39.540 --> 02:17:40.540]   She cares.
[02:17:40.540 --> 02:17:42.540]   She also knows you well.
[02:17:42.540 --> 02:17:44.860]   All right.
[02:17:44.860 --> 02:17:48.580]   The Google change log without further ado.
[02:17:48.580 --> 02:17:49.980]   It's kind of the end of the line.
[02:17:49.980 --> 02:17:52.780]   You were a glass hole, weren't you Mike Elgin?
[02:17:52.780 --> 02:17:53.980]   Absolutely.
[02:17:53.980 --> 02:17:55.540]   I have my devout one.
[02:17:55.540 --> 02:17:58.460]   Yeah, I spent $1,500 on and I never wore them.
[02:17:58.460 --> 02:18:00.860]   I gave them to Jason Howell.
[02:18:00.860 --> 02:18:03.180]   I mean, it was 90% camera.
[02:18:03.180 --> 02:18:04.540]   I wore it all over.
[02:18:04.540 --> 02:18:07.260]   We lived in Florence at the top for much of that.
[02:18:07.260 --> 02:18:09.980]   And I was all over Florence, like shooting everything,
[02:18:09.980 --> 02:18:11.460]   video capturing everything.
[02:18:11.460 --> 02:18:13.180]   It was amazing.
[02:18:13.180 --> 02:18:16.700]   Well, you'll be sad to know that today on the Ides of March--
[02:18:16.700 --> 02:18:18.020]   Oh, wait a minute.
[02:18:18.020 --> 02:18:19.540]   You still have them.
[02:18:19.540 --> 02:18:20.060]   Of course.
[02:18:20.060 --> 02:18:22.860]   You haven't given them up.
[02:18:22.860 --> 02:18:23.660]   All in honor of--
[02:18:23.660 --> 02:18:24.860]   As a battery.
[02:18:24.860 --> 02:18:28.260]   As a battery is right.
[02:18:28.260 --> 02:18:31.140]   I can't imagine why this didn't succeed.
[02:18:31.140 --> 02:18:32.460]   I know.
[02:18:32.460 --> 02:18:34.540]   You look quite fetching.
[02:18:34.540 --> 02:18:35.940]   Actually, would you get me the charger?
[02:18:35.940 --> 02:18:38.500]   I bet it's a micro USB, isn't it?
[02:18:38.500 --> 02:18:40.140]   Yep, micro USB.
[02:18:40.140 --> 02:18:42.780]   Oh, how sad that is.
[02:18:42.780 --> 02:18:44.220]   I want to start wearing these around,
[02:18:44.220 --> 02:18:46.460]   just to see what kind of reaction I get.
[02:18:46.460 --> 02:18:50.860]   Google has, as of today, discontinued glass.
[02:18:50.860 --> 02:18:56.100]   The final edition of glass was the Glass Enterprise Edition 2.
[02:18:56.100 --> 02:18:58.860]   They will no longer be selling them,
[02:18:58.860 --> 02:19:02.940]   and support will end later this year.
[02:19:02.940 --> 02:19:07.820]   So it was seemed like a good idea.
[02:19:07.820 --> 02:19:08.940]   But as soon as you start--
[02:19:08.940 --> 02:19:10.260]   Just like an F-G's, you know?
[02:19:10.260 --> 02:19:10.660]   Yeah.
[02:19:10.660 --> 02:19:12.380]   So as you start calling them glass holes,
[02:19:12.380 --> 02:19:14.580]   I think the writing was on the wall.
[02:19:14.580 --> 02:19:15.540]   I think Scoble killed it.
[02:19:15.540 --> 02:19:17.660]   Scoble killed it.
[02:19:17.660 --> 02:19:21.020]   In the showering was displayed like a few feet
[02:19:21.020 --> 02:19:24.660]   in front of your left right eye.
[02:19:24.660 --> 02:19:27.980]   I hope this portends a Google I/O announcement
[02:19:27.980 --> 02:19:31.940]   of a new augmented reality platform from Google.
[02:19:31.940 --> 02:19:36.780]   This picture, I think, single-handedly killed Google Glass.
[02:19:36.780 --> 02:19:39.580]   Well, you remember, weren't we at I/O in Moscone
[02:19:39.580 --> 02:19:41.540]   when Scoble stood up?
[02:19:41.540 --> 02:19:44.180]   And I think it was Larry scolded him?
[02:19:44.180 --> 02:19:45.140]   Yeah.
[02:19:45.140 --> 02:19:46.700]   You really shouldn't do that.
[02:19:46.700 --> 02:19:48.260]   Yeah.
[02:19:48.260 --> 02:19:51.260]   Yeah.
[02:19:51.260 --> 02:19:53.260]   And of course, the problem was going into men's rooms
[02:19:53.260 --> 02:19:54.820]   with this on everybody.
[02:19:54.820 --> 02:19:57.260]   Because it has a camera in, but it has no light.
[02:19:57.260 --> 02:19:59.100]   So you just don't know if somebody's using it.
[02:19:59.100 --> 02:20:01.020]   Well, as I told Nick built him when he wrote a column
[02:20:01.020 --> 02:20:03.860]   and claimed about it, I said, Nick, get over it yourself.
[02:20:03.860 --> 02:20:05.100]   Nobody wants to see your junk.
[02:20:05.100 --> 02:20:10.500]   Yeah.
[02:20:10.500 --> 02:20:12.860]   So it's the end of the line.
[02:20:12.860 --> 02:20:13.580]   Wow.
[02:20:13.580 --> 02:20:17.220]   Google Glass.
[02:20:17.220 --> 02:20:21.900]   Oddly enough, there was a new companion app
[02:20:21.900 --> 02:20:23.660]   for the Pixel phones.
[02:20:23.660 --> 02:20:26.420]   It was an early access program.
[02:20:26.420 --> 02:20:31.900]   Google is still apparently working on AR headsets as is Apple.
[02:20:31.900 --> 02:20:32.620]   Everyone is.
[02:20:32.620 --> 02:20:33.340]   Everyone is.
[02:20:33.340 --> 02:20:34.020]   Everyone is.
[02:20:34.020 --> 02:20:35.780]   That's the future.
[02:20:35.780 --> 02:20:36.780]   We have to figure it out.
[02:20:36.780 --> 02:20:38.980]   We haven't been able to yet, but we're going to.
[02:20:38.980 --> 02:20:42.620]   I feel like this will be a jeopardy answer.
[02:20:42.620 --> 02:20:43.980]   You know, it'll have a picture of somebody
[02:20:43.980 --> 02:20:46.820]   wearing this thing.
[02:20:46.820 --> 02:20:50.460]   What's going on in this picture?
[02:20:50.460 --> 02:20:51.900]   All right, I'm going to charge this.
[02:20:51.900 --> 02:20:53.900]   What is idiot rip off, Alex?
[02:20:53.900 --> 02:20:58.020]   1,500 smackers.
[02:20:58.020 --> 02:21:03.420]   I still have the glass paperweight I got for being in the program
[02:21:03.420 --> 02:21:05.060]   so that I could get it.
[02:21:05.060 --> 02:21:07.020]   I signed up very early on this.
[02:21:07.020 --> 02:21:08.100]   All right, we're going to charge it.
[02:21:08.100 --> 02:21:09.900]   I've got mine here somewhere.
[02:21:09.900 --> 02:21:12.180]   I am with mine with my prescription lenses.
[02:21:12.180 --> 02:21:15.660]   It's kind of tying to bring back glass.
[02:21:15.660 --> 02:21:18.700]   Well, we have better things than glass now.
[02:21:18.700 --> 02:21:22.260]   We have glasses that look almost like regular glasses
[02:21:22.260 --> 02:21:24.300]   that give us heads up as we do.
[02:21:24.300 --> 02:21:24.700]   We do.
[02:21:24.700 --> 02:21:25.500]   Lots of companies.
[02:21:25.500 --> 02:21:26.540]   Dozens of companies.
[02:21:26.540 --> 02:21:29.260]   They're not interesting because there's no major player
[02:21:29.260 --> 02:21:31.180]   behind them or major platform behind them.
[02:21:31.180 --> 02:21:34.820]   Of course, Apple is going to blow the lid off of this category
[02:21:34.820 --> 02:21:39.380]   when they come out with their device, possibly later this year,
[02:21:39.380 --> 02:21:41.500]   probably early next year.
[02:21:41.500 --> 02:21:45.900]   And it's going to be a whole new ball game.
[02:21:45.900 --> 02:21:47.740]   Of course, Apple's device will not be something
[02:21:47.740 --> 02:21:49.180]   you can wear around town.
[02:21:49.180 --> 02:21:52.180]   Apple's device looks like, according to the rumors,
[02:21:52.180 --> 02:21:53.620]   we don't know because it's not been announced.
[02:21:53.620 --> 02:21:57.260]   But according to the rumors, it looks just like a VR headset.
[02:21:57.260 --> 02:21:58.060]   Not really.
[02:21:58.060 --> 02:22:00.220]   Well, it's a somewhat elegant one.
[02:22:00.220 --> 02:22:04.020]   But they're going to use VR for AR, mostly.
[02:22:04.020 --> 02:22:06.660]   So basically, they'll show you the real world
[02:22:06.660 --> 02:22:08.300]   through an immediate video.
[02:22:08.300 --> 02:22:10.940]   And then you put the digital objects and information
[02:22:10.940 --> 02:22:13.820]   superimposed on top of that as a precursor
[02:22:13.820 --> 02:22:16.500]   and a development platform and so on
[02:22:16.500 --> 02:22:19.100]   for what's probably coming in five or six or seven years,
[02:22:19.100 --> 02:22:21.020]   we're going to be regular glasses that,
[02:22:21.020 --> 02:22:22.980]   where you see the real world directly
[02:22:22.980 --> 02:22:24.780]   and then the virtual objects are placed.
[02:22:24.780 --> 02:22:25.820]   I feel like that's far off.
[02:22:25.820 --> 02:22:27.340]   Why does it do a video first?
[02:22:27.340 --> 02:22:28.420]   What is it?
[02:22:28.420 --> 02:22:30.780]   Why is it easier to deliver video first?
[02:22:30.780 --> 02:22:34.500]   Because they're the miniaturization problems
[02:22:34.500 --> 02:22:36.820]   for high quality AR glasses.
[02:22:36.820 --> 02:22:38.420]   And essentially, with the Holy Grail,
[02:22:38.420 --> 02:22:41.500]   is to do something like a magic leap or hollow lens,
[02:22:41.500 --> 02:22:43.460]   but in something that looks like ordinary glasses.
[02:22:43.460 --> 02:22:45.980]   Okay, we're many years from anything--
[02:22:45.980 --> 02:22:47.540]   Okay, so they're just doing that
[02:22:47.540 --> 02:22:49.660]   so you can cover your whole face with the computer
[02:22:49.660 --> 02:22:51.700]   because you need that much physical room for the computer.
[02:22:51.700 --> 02:22:53.140]   Right, and they're going to be using the lidar
[02:22:53.140 --> 02:22:55.860]   and stuff in the Apple glasses to map the room.
[02:22:55.860 --> 02:22:57.740]   The difference between, of course, AR
[02:22:57.740 --> 02:22:59.140]   and what Google Glass was,
[02:22:59.140 --> 02:23:01.380]   was Google Glass was just heads up display.
[02:23:01.380 --> 02:23:02.780]   It would show you a rectangle,
[02:23:02.780 --> 02:23:03.780]   and if you moved your head,
[02:23:03.780 --> 02:23:05.820]   it would the rectangle would move with your head,
[02:23:05.820 --> 02:23:07.900]   whereas with what Apple and Google
[02:23:07.900 --> 02:23:09.140]   and everybody else is working on
[02:23:09.140 --> 02:23:11.020]   is to be able to place things on the table,
[02:23:11.020 --> 02:23:13.900]   and it stays on the table no matter how you move your head.
[02:23:13.900 --> 02:23:16.740]   And so that's kind of a Holy Grail thing
[02:23:16.740 --> 02:23:17.780]   that we don't have the technology.
[02:23:17.780 --> 02:23:22.180]   I mean, Apple could build perfect glasses
[02:23:22.180 --> 02:23:24.060]   that look amazing and do all that stuff,
[02:23:24.060 --> 02:23:27.020]   but it would cost $150,000 a pair.
[02:23:27.020 --> 02:23:29.260]   And-- When I ordered Apple, that's a lot of money.
[02:23:29.260 --> 02:23:30.500]   When I ordered the Google Glass,
[02:23:30.500 --> 02:23:34.140]   I did get this glass paperweight with my order number.
[02:23:34.140 --> 02:23:36.620]   I didn't get that. Which is 737.
[02:23:36.620 --> 02:23:38.700]   Huh, yeah.
[02:23:38.700 --> 02:23:41.180]   So what are the 36 bucks before it ever got one?
[02:23:41.180 --> 02:23:44.740]   Resources, like using up, that's just silly.
[02:23:44.740 --> 02:23:45.700]   I'm glad to have it though.
[02:23:45.700 --> 02:23:48.980]   That is better souvenir than the glass itself.
[02:23:48.980 --> 02:23:52.460]   I mean, a piece of glass with the number 737
[02:23:52.460 --> 02:23:54.460]   laser-incorrect inside it.
[02:23:54.460 --> 02:23:55.300]   (laughs)
[02:23:55.300 --> 02:23:56.860]   That's okay, I'm happy.
[02:23:56.860 --> 02:23:59.260]   I, you know, it's a good paperweight.
[02:23:59.260 --> 02:24:02.340]   You just wanted to draw it just very quickly, Leo.
[02:24:02.340 --> 02:24:04.620]   I'm sorry, just put two things together
[02:24:04.620 --> 02:24:06.780]   because we're gonna have,
[02:24:06.780 --> 02:24:08.740]   augmented reality glasses are gonna have
[02:24:09.820 --> 02:24:12.940]   AI virtual assistants that are gonna do some of the things
[02:24:12.940 --> 02:24:15.540]   that GPT-4 do, and one of them is this.
[02:24:15.540 --> 02:24:16.380]   Right.
[02:24:16.380 --> 02:24:17.940]   Both text and images.
[02:24:17.940 --> 02:24:20.460]   So just like you can ask GPT-4,
[02:24:20.460 --> 02:24:22.100]   "What is this?" and I'll tell you,
[02:24:22.100 --> 02:24:23.460]   or "What's funny about this?"
[02:24:23.460 --> 02:24:24.900]   That's the missing piece, right?
[02:24:24.900 --> 02:24:26.260]   That's the missing piece.
[02:24:26.260 --> 02:24:27.580]   Yeah, yeah.
[02:24:27.580 --> 02:24:30.300]   And so that's gonna be a real game changer
[02:24:30.300 --> 02:24:31.540]   for the visually impaired, of course,
[02:24:31.540 --> 02:24:34.300]   but also for all of us to be able to recognize people
[02:24:34.300 --> 02:24:35.700]   and things in our environment.
[02:24:35.700 --> 02:24:39.100]   So the point I'm saying is that one of the big points
[02:24:39.100 --> 02:24:42.060]   of controversy of Google Glass was that camera pointing out.
[02:24:42.060 --> 02:24:42.900]   Yeah.
[02:24:42.900 --> 02:24:45.140]   Sorry to say that all the augmented reality glasses
[02:24:45.140 --> 02:24:46.100]   are gonna have cameras.
[02:24:46.100 --> 02:24:46.940]   Right.
[02:24:46.940 --> 02:24:49.860]   Looking at the world, you know,
[02:24:49.860 --> 02:24:50.700]   they're no doubt.
[02:24:50.700 --> 02:24:51.540]   They're less useful if they don't.
[02:24:51.540 --> 02:24:54.300]   Otherwise they're just, like your Apple watch,
[02:24:54.300 --> 02:24:55.140]   it just displays.
[02:24:55.140 --> 02:24:55.980]   Exactly, yeah.
[02:24:55.980 --> 02:24:59.460]   No, I think this is right in the neck of time for Alzheimer's.
[02:24:59.460 --> 02:25:00.860]   So I can't wait.
[02:25:00.860 --> 02:25:03.140]   (laughing)
[02:25:03.140 --> 02:25:05.660]   Hi, Jeff.
[02:25:07.020 --> 02:25:12.020]   YouTube TV MultiView lets you watch up to four games
[02:25:12.020 --> 02:25:15.540]   in March Madness.
[02:25:15.540 --> 02:25:20.620]   Okay, YouTube TV, you go, you silly YouTube TV.
[02:25:20.620 --> 02:25:23.820]   How could you watch four games at once, really?
[02:25:23.820 --> 02:25:24.780]   Who wants that?
[02:25:24.780 --> 02:25:27.500]   They announced it today
[02:25:27.500 --> 02:25:30.040]   and it will be coming for March Madness.
[02:25:30.040 --> 02:25:33.620]   Just seems like a lot.
[02:25:33.620 --> 02:25:36.500]   I don't even wanna watch one game at this.
[02:25:36.500 --> 02:25:37.620]   Yeah, I don't even wanna watch one.
[02:25:37.620 --> 02:25:39.260]   Maybe soccer will be interesting
[02:25:39.260 --> 02:25:41.820]   if you had four games going on at once.
[02:25:41.820 --> 02:25:43.420]   I don't know.
[02:25:43.420 --> 02:25:47.620]   Google Play will now machine translate Android apps for free.
[02:25:47.620 --> 02:25:49.940]   What does that mean?
[02:25:49.940 --> 02:25:51.300]   At the Google for Games Summit,
[02:25:51.300 --> 02:25:54.660]   Google announced a launch of free machine translation
[02:25:54.660 --> 02:25:56.060]   for Android apps.
[02:25:56.060 --> 02:25:57.760]   Oh, so you can localize them.
[02:25:57.760 --> 02:25:59.900]   Seven different languages.
[02:25:59.900 --> 02:26:03.780]   You could take the app, localize it, add to your markets.
[02:26:03.780 --> 02:26:08.140]   The languages include China, Chinese, French,
[02:26:08.140 --> 02:26:12.980]   German, Indonesian, Japanese, Portuguese, and Spanish.
[02:26:12.980 --> 02:26:14.780]   Spanish, Castilian.
[02:26:14.780 --> 02:26:17.300]   And that's free.
[02:26:17.300 --> 02:26:20.660]   You have to apply to an early access program
[02:26:20.660 --> 02:26:21.500]   if you want that.
[02:26:21.500 --> 02:26:22.340]   That's great.
[02:26:22.340 --> 02:26:25.820]   Along with the new chat GPT
[02:26:25.820 --> 02:26:27.340]   that's going into Google Workspace,
[02:26:27.340 --> 02:26:29.220]   you're getting a price increase.
[02:26:29.220 --> 02:26:33.860]   Coenching Inc, I think not, the new prices,
[02:26:33.860 --> 02:26:39.900]   the per user per month costs for both plans,
[02:26:39.900 --> 02:26:42.540]   $6 for starter, $12 for standard,
[02:26:42.540 --> 02:26:45.780]   18 for business plus flexible plans
[02:26:45.780 --> 02:26:48.980]   are now going to 720, 1440, and 2160.
[02:26:48.980 --> 02:26:53.100]   So it's not a huge, huge jump.
[02:26:53.100 --> 02:26:56.020]   But you'll be paying a few pennies more.
[02:26:56.020 --> 02:26:58.620]   Maybe to pay for the AI.
[02:26:58.620 --> 02:27:00.340]   That AI is expensive.
[02:27:00.340 --> 02:27:03.740]   Pixels are getting a feature drop,
[02:27:03.740 --> 02:27:07.140]   Android 13 QPR2.
[02:27:07.140 --> 02:27:09.260]   We've talked about the March feature drop.
[02:27:09.260 --> 02:27:11.460]   It's now rolling out.
[02:27:11.460 --> 02:27:12.660]   What do you get?
[02:27:12.660 --> 02:27:19.260]   You get, instead of a battery percentage,
[02:27:19.260 --> 02:27:23.020]   remaining day hour or until HHMM estimate
[02:27:23.020 --> 02:27:25.060]   for how long your battery's going to last.
[02:27:25.060 --> 02:27:28.500]   A large digital clock when you fully expand quick settings.
[02:27:28.500 --> 02:27:32.820]   It's a lot of silly stuff.
[02:27:32.820 --> 02:27:35.300]   Tweaks in the Pixel launcher and folders,
[02:27:35.300 --> 02:27:37.700]   now playing up here's higher up in the launch screen.
[02:27:37.700 --> 02:27:39.340]   The emergency calling button is larger,
[02:27:39.340 --> 02:27:40.620]   good to go home, my God.
[02:27:40.620 --> 02:27:42.580]   It's just a few things, right?
[02:27:42.580 --> 02:27:47.580]   Two E-Sims can be used for dual SIM standby on the Pixel 7.
[02:27:47.580 --> 02:27:52.380]   Pixel 6 Pro owners can now switch to 1080p full high def
[02:27:52.380 --> 02:27:57.380]   plus ultra wide band digital car key support rolling out.
[02:27:57.380 --> 02:28:03.500]   I don't know if there are any cars besides BMW models
[02:28:03.500 --> 02:28:06.420]   that can do this, but I guess maybe eventually.
[02:28:06.420 --> 02:28:10.260]   Direct My Call is coming to older pixels,
[02:28:10.260 --> 02:28:11.980]   which is actually really good news.
[02:28:11.980 --> 02:28:14.820]   4A, 4A 5G, 5A.
[02:28:14.820 --> 02:28:20.460]   Anyway, big grid of new features.
[02:28:20.460 --> 02:28:22.180]   A lot of these features I'm very happy to say
[02:28:22.180 --> 02:28:25.620]   rolling out to older Pixel phones.
[02:28:25.620 --> 02:28:28.820]   So if you have a Pixel update,
[02:28:28.820 --> 02:28:32.220]   and that's the Google change log.
[02:28:32.220 --> 02:28:35.060]   (mimics jingle)
[02:28:35.060 --> 02:28:43.620]   All right, Jeff, I see many, many, many stories
[02:28:43.620 --> 02:28:48.260]   that you have snuck in at the last minute.
[02:28:48.260 --> 02:28:49.820]   - I'll do a fun one.
[02:28:49.820 --> 02:28:51.540]   You see the one about the guy who was stranded
[02:28:51.540 --> 02:28:53.940]   on the mountain and had no cell coverage?
[02:28:53.940 --> 02:28:54.860]   - No, I did not.
[02:28:54.860 --> 02:28:55.700]   What happened?
[02:28:55.700 --> 02:28:56.540]   - Line 89.
[02:28:56.540 --> 02:28:58.420]   - Did you choose Armouth?
[02:28:58.420 --> 02:29:00.020]   - That would have been the next day.
[02:29:00.020 --> 02:29:01.260]   But this day, very clever.
[02:29:01.260 --> 02:29:02.540]   He happened to have a drone with him.
[02:29:02.540 --> 02:29:03.380]   You know, it just happened to happen to him.
[02:29:03.380 --> 02:29:04.220]   - Oh yeah.
[02:29:04.220 --> 02:29:05.980]   - Oh, it is so good. - But he had no coverage.
[02:29:05.980 --> 02:29:10.260]   So he tied the phone to the drone and set it up high enough.
[02:29:10.260 --> 02:29:12.020]   It could get coverage and then it sent the text
[02:29:12.020 --> 02:29:14.860]   to his wife saying send help here's where I am
[02:29:14.860 --> 02:29:15.940]   and back down.
[02:29:15.940 --> 02:29:16.780]   - Incredible.
[02:29:16.780 --> 02:29:18.780]   - She tried to call a towing company
[02:29:18.780 --> 02:29:20.700]   and they will re-don't go there.
[02:29:20.700 --> 02:29:21.940]   He was stuck in the snow.
[02:29:21.940 --> 02:29:24.740]   So then the next, he did another time
[02:29:24.740 --> 02:29:25.900]   and she said, no, no, no,
[02:29:25.900 --> 02:29:27.620]   but the county's coming to help you
[02:29:27.620 --> 02:29:28.820]   and back down again.
[02:29:28.820 --> 02:29:30.420]   They crashed it.
[02:29:30.420 --> 02:29:31.260]   - Brilliant.
[02:29:31.260 --> 02:29:32.100]   - That's nice.
[02:29:32.100 --> 02:29:32.940]   - That's the guy there.
[02:29:32.940 --> 02:29:37.060]   - They actually use drones to, they put like modems
[02:29:37.060 --> 02:29:40.700]   on drones and they actually grab sensor data
[02:29:40.700 --> 02:29:44.700]   from like wildfire, like sensing in forested areas
[02:29:44.700 --> 02:29:45.540]   and other places.
[02:29:45.540 --> 02:29:47.980]   They just fly the coverage over using a drone
[02:29:47.980 --> 02:29:49.900]   and then the sensors are like, oh, I've got coverage.
[02:29:49.900 --> 02:29:50.740]   Really?
[02:29:50.740 --> 02:29:51.580]   It's kind of neat.
[02:29:51.580 --> 02:29:53.580]   So it's the inverse of that, I guess.
[02:29:53.580 --> 02:29:54.420]   - Cool.
[02:29:54.420 --> 02:29:58.220]   - And now a sad story, more meta layoffs, 10,000 more.
[02:29:58.220 --> 02:30:00.980]   They laid off 11,000 November.
[02:30:00.980 --> 02:30:03.700]   Now they're gonna do another 10,000.
[02:30:03.700 --> 02:30:06.620]   That's a pretty hefty chunk.
[02:30:06.620 --> 02:30:11.620]   Is Facebook/meta going down the MySpace stream?
[02:30:11.620 --> 02:30:14.220]   - Well, are they, are they MySpace in here?
[02:30:14.220 --> 02:30:17.460]   One hopes, I mean, I don't like to see people
[02:30:17.460 --> 02:30:18.900]   getting laid off, of course.
[02:30:18.900 --> 02:30:22.620]   I actually have a very close, you know,
[02:30:22.620 --> 02:30:26.660]   a good friend who got laid off in a previous round.
[02:30:26.660 --> 02:30:28.180]   I don't want to see anybody get laid off,
[02:30:28.180 --> 02:30:30.460]   but if that's what it takes for Facebook to go away,
[02:30:30.460 --> 02:30:31.300]   that would be--
[02:30:31.300 --> 02:30:34.260]   - Is it a mutual, but is it a mutual element?
[02:30:34.260 --> 02:30:35.340]   Is it a mutual friend?
[02:30:35.340 --> 02:30:36.220]   Is it the guy?
[02:30:36.220 --> 02:30:37.060]   - Yes, it is.
[02:30:37.060 --> 02:30:37.900]   Yes, it is.
[02:30:37.900 --> 02:30:38.740]   - Oh, no.
[02:30:38.740 --> 02:30:39.580]   - Yeah, I don't wanna say in there.
[02:30:39.580 --> 02:30:40.420]   - Yeah, yeah, yeah.
[02:30:40.420 --> 02:30:42.100]   - Oh, I'm so sorry.
[02:30:42.100 --> 02:30:42.940]   - Yeah, I mean, he's--
[02:30:42.940 --> 02:30:45.340]   - He is super wonderful.
[02:30:45.340 --> 02:30:46.180]   - Yes, they are.
[02:30:46.180 --> 02:30:47.020]   And he's a super catch.
[02:30:47.020 --> 02:30:48.300]   - He'll do fine.
[02:30:48.300 --> 02:30:49.660]   - Get another great position, yeah.
[02:30:49.660 --> 02:30:51.420]   - Yeah, I will not send him a note.
[02:30:51.420 --> 02:30:53.100]   - He's a high level person, so it's like,
[02:30:53.100 --> 02:30:54.380]   he'll do great.
[02:30:54.380 --> 02:30:55.220]   - I'll send him a note.
[02:30:55.220 --> 02:30:57.660]   - Mike, as you who's still more in Google+,
[02:30:57.660 --> 02:30:59.660]   as do I, have a little empathy,
[02:30:59.660 --> 02:31:01.780]   not for the company, but for the people there.
[02:31:01.780 --> 02:31:03.980]   Same with Twitter, same with Black Twitter,
[02:31:03.980 --> 02:31:08.980]   same with the communities that do still depend upon
[02:31:08.980 --> 02:31:11.580]   these platforms, and it's not,
[02:31:11.580 --> 02:31:14.380]   it's easy for us to move as individuals.
[02:31:14.380 --> 02:31:16.180]   It's not so easy to move a community.
[02:31:16.180 --> 02:31:17.380]   - Yep, no, that's true.
[02:31:17.380 --> 02:31:19.020]   And I agree with that, Jeff,
[02:31:19.020 --> 02:31:20.700]   and I appreciate the sentiment.
[02:31:20.700 --> 02:31:25.020]   I think if you were to add up all the pluses and minuses
[02:31:25.020 --> 02:31:27.180]   of Facebook, it's one of the few social networks
[02:31:27.180 --> 02:31:29.300]   that would come out on the minus end of the ledger.
[02:31:29.300 --> 02:31:30.900]   Overall, but still there are--
[02:31:30.900 --> 02:31:32.460]   - But still there's things going on.
[02:31:32.460 --> 02:31:34.740]   - There are a lot of older people who just,
[02:31:34.740 --> 02:31:36.740]   they know that as the way that they keep in touch
[02:31:36.740 --> 02:31:38.500]   with everybody they know, all the relatives
[02:31:38.500 --> 02:31:40.900]   and stuff like that, and they, for them to move,
[02:31:40.900 --> 02:31:43.300]   it would basically, the whole family would be dispersed
[02:31:43.300 --> 02:31:44.580]   to multiple different networks,
[02:31:44.580 --> 02:31:47.380]   and that would be the end of having everybody in one place.
[02:31:47.380 --> 02:31:49.940]   There are a lot of downsides, but--
[02:31:49.940 --> 02:31:51.700]   - But you know what, Mike, it won't matter,
[02:31:51.700 --> 02:31:53.940]   is I'm gonna move physically to the community
[02:31:53.940 --> 02:31:54.860]   or the future of--
[02:31:54.860 --> 02:31:56.140]   - Busklandia.
[02:31:56.140 --> 02:31:58.020]   - Yes, there you go. - Before you go there,
[02:31:58.020 --> 02:32:00.780]   I do wanna mention that in the year 2023,
[02:32:00.780 --> 02:32:02.580]   according to layoffs.fyi,
[02:32:02.580 --> 02:32:07.580]   130,512 people have been laid off in the tech industry.
[02:32:07.580 --> 02:32:12.380]   So, to all of you, I'm sorry,
[02:32:12.380 --> 02:32:14.180]   I hope you find a gainful employee.
[02:32:14.180 --> 02:32:15.500]   - That is tough.
[02:32:15.500 --> 02:32:17.420]   - Please don't start podcast.
[02:32:17.420 --> 02:32:19.580]   - Please, whatever you do.
[02:32:19.580 --> 02:32:20.940]   Look what's happened to me.
[02:32:20.940 --> 02:32:22.940]   You don't, you don't.
[02:32:22.940 --> 02:32:25.820]   You just don't wanna go there.
[02:32:25.820 --> 02:32:31.260]   - Busklandia is Elon's plan to create a company town.
[02:32:31.260 --> 02:32:33.140]   - Yo, yo, soul to the community.
[02:32:33.140 --> 02:32:34.700]   - I thought it was snail broke.
[02:32:34.700 --> 02:32:37.300]   - Well, no, I'm making a Fordlandia joke.
[02:32:37.300 --> 02:32:39.700]   - Oh, okay, I was like, I thought it was--
[02:32:39.700 --> 02:32:40.700]   - It is in Texas. - It's one of the most
[02:32:40.700 --> 02:32:41.860]   fascist to the next.
[02:32:41.860 --> 02:32:42.860]   - It is in Texas. - It's in Texas.
[02:32:42.860 --> 02:32:43.820]   - Yeah, it's in Bastrop.
[02:32:43.820 --> 02:32:46.860]   It's right outside of, it's right by the airport, basically.
[02:32:46.860 --> 02:32:50.860]   - Because Texas has rules that allow you
[02:32:50.860 --> 02:32:54.340]   to set your own regulations if you own the town, I guess.
[02:32:54.340 --> 02:32:55.340]   - Yep.
[02:32:55.340 --> 02:32:57.300]   - Thousands of acres of poorly purchased
[02:32:57.300 --> 02:32:59.780]   park pasture and farmland,
[02:32:59.780 --> 02:33:02.220]   right outside of Austin near the airport.
[02:33:02.220 --> 02:33:06.900]   Company employees could live and work there.
[02:33:06.900 --> 02:33:11.020]   The boring company will have a giant facility there.
[02:33:11.020 --> 02:33:13.860]   Underneath your home. (laughs)
[02:33:13.860 --> 02:33:16.980]   - And if you leave the company or are fired,
[02:33:16.980 --> 02:33:19.220]   you have 30 days to leave your rent,
[02:33:19.220 --> 02:33:21.140]   'cause they're planning on renting these houses
[02:33:21.140 --> 02:33:24.300]   to their workers to provide a low market rent.
[02:33:24.300 --> 02:33:26.460]   - No, no, no, no, no, I don't wanna live in snail brook.
[02:33:26.460 --> 02:33:29.180]   That's such a terrible name.
[02:33:29.180 --> 02:33:32.140]   - That's not my particular issue.
[02:33:32.140 --> 02:33:33.820]   My particular issue would be like,
[02:33:33.820 --> 02:33:36.880]   oh, without any sort of oversight and regulations.
[02:33:36.880 --> 02:33:39.500]   - Yeah, the town that Elon,
[02:33:39.500 --> 02:33:43.660]   I mean, remember this is, I think it was the same I/O Leo
[02:33:43.660 --> 02:33:46.260]   where we wanted to move to Google Island.
[02:33:46.260 --> 02:33:48.540]   You remember that?
[02:33:48.540 --> 02:33:50.060]   - Yeah, Larry Page.
[02:33:50.060 --> 02:33:51.060]   - Where he had Google Island.
[02:33:51.060 --> 02:33:51.900]   - Larry Page.
[02:33:51.900 --> 02:33:52.860]   - So I can imagine living on Google Island,
[02:33:52.860 --> 02:33:53.940]   but not musculine.
[02:33:53.940 --> 02:33:54.780]   - Not musculine.
[02:33:54.780 --> 02:33:57.740]   - Well, the problem is that you can be on the HOA,
[02:33:57.740 --> 02:34:00.380]   but only if you pay $1 a month.
[02:34:00.380 --> 02:34:01.460]   - Right, right.
[02:34:01.460 --> 02:34:04.620]   By the way, did you notice that he still lists himself?
[02:34:04.620 --> 02:34:06.140]   If you go to his profile,
[02:34:06.140 --> 02:34:09.140]   he's not the new Twitter blue.
[02:34:09.140 --> 02:34:10.540]   He's still legacy.
[02:34:10.540 --> 02:34:11.380]   - Wow.
[02:34:11.380 --> 02:34:13.060]   Who may or may not be notable.
[02:34:13.060 --> 02:34:13.900]   - Exactly.
[02:34:13.900 --> 02:34:16.820]   That's humility.
[02:34:16.820 --> 02:34:18.060]   That's humility coming from us.
[02:34:18.060 --> 02:34:19.060]   - Oh yeah.
[02:34:19.060 --> 02:34:21.220]   - I did ask Jason Calicanis.
[02:34:21.220 --> 02:34:25.420]   I said, did you tell Elon not to be chief twit?
[02:34:25.420 --> 02:34:27.300]   Like, 'cause you know,
[02:34:27.300 --> 02:34:29.260]   'cause I mean, Jason even said,
[02:34:29.260 --> 02:34:31.580]   yeah, I know you've been chief twit for 11 years.
[02:34:31.580 --> 02:34:32.420]   I said, you told him.
[02:34:32.420 --> 02:34:34.660]   He said, I don't wanna talk about any conversations.
[02:34:34.660 --> 02:34:35.660]   - He did a little chat dance.
[02:34:35.660 --> 02:34:37.700]   - May or may not have with Elon.
[02:34:37.700 --> 02:34:41.060]   - I mean, honestly, I think that's a yes.
[02:34:41.060 --> 02:34:42.060]   So thank you. - Yes.
[02:34:42.060 --> 02:34:43.420]   - I think he did.
[02:34:43.420 --> 02:34:45.620]   He said, Elon, there's another chief twit.
[02:34:45.620 --> 02:34:47.660]   You don't wanna be anything like him.
[02:34:47.660 --> 02:34:48.660]   (laughing)
[02:34:48.660 --> 02:34:49.500]   Apparently the law--
[02:34:49.500 --> 02:34:50.900]   - He's still wearing Google Glass.
[02:34:50.900 --> 02:34:52.620]   - He's still wearing Google Glass.
[02:34:52.620 --> 02:34:54.500]   The law requires you elect a mayor.
[02:34:54.500 --> 02:34:57.780]   So they'll be doing that.
[02:34:57.780 --> 02:35:00.020]   Musk, his former girlfriend, the singer,
[02:35:00.020 --> 02:35:04.060]   Grimes Kanye West, and Mr. West's architectural designer
[02:35:04.060 --> 02:35:06.460]   discussed several times last year
[02:35:06.460 --> 02:35:08.860]   what a musk town might look like.
[02:35:08.860 --> 02:35:10.100]   They were all high on weed in time.
[02:35:10.100 --> 02:35:11.740]   - You imagine the neighbors.
[02:35:11.740 --> 02:35:14.260]   - Oh my God, this sounds so bad.
[02:35:14.260 --> 02:35:15.260]   - Oh.
[02:35:15.260 --> 02:35:18.620]   - They should elect a mayor based on check-ins.
[02:35:18.620 --> 02:35:21.460]   Like, four square, they should change every few days.
[02:35:21.460 --> 02:35:23.180]   (laughing)
[02:35:23.180 --> 02:35:28.180]   - This is a picture of Snail Brook.
[02:35:28.180 --> 02:35:32.020]   - I don't know if you really wanna live there.
[02:35:32.020 --> 02:35:34.420]   It looks like a lot of aluminum siding.
[02:35:34.420 --> 02:35:36.260]   - Is he already building it?
[02:35:36.260 --> 02:35:37.380]   - Yeah, he is building it.
[02:35:37.380 --> 02:35:38.700]   These look like trailers.
[02:35:38.700 --> 02:35:41.980]   Actually, which was funny because that's what is by the airport
[02:35:41.980 --> 02:35:42.820]   is a bunch of RV--
[02:35:42.820 --> 02:35:43.820]   - Double whites.
[02:35:43.820 --> 02:35:44.660]   - Yeah.
[02:35:44.660 --> 02:35:46.140]   - Double whites.
[02:35:46.140 --> 02:35:48.100]   So maybe he just bought all of their inventory
[02:35:48.100 --> 02:35:48.940]   and just set it up.
[02:35:48.940 --> 02:35:49.940]   - Wait, wait, wait, wait.
[02:35:49.940 --> 02:35:51.940]   Elon Musk is building a trailer park.
[02:35:51.940 --> 02:35:52.940]   (laughing)
[02:35:52.940 --> 02:35:53.780]   Where's employees?
[02:35:53.780 --> 02:35:55.660]   Is that what we're concluding here?
[02:35:55.660 --> 02:35:57.740]   - Trailer parks are very lucrative.
[02:35:57.740 --> 02:36:00.820]   So it would not surprise me at all.
[02:36:00.820 --> 02:36:02.100]   There's a housing shortage.
[02:36:02.100 --> 02:36:03.340]   You can convince people to live in your--
[02:36:03.340 --> 02:36:04.500]   - It's only a hundred trailer parks.
[02:36:04.500 --> 02:36:06.660]   It's a small town. It's 110 people.
[02:36:06.660 --> 02:36:10.100]   But you only need 200 in Texas, 201 residents
[02:36:10.100 --> 02:36:11.620]   before you can apply to it and corporate.
[02:36:11.620 --> 02:36:15.540]   So, you know, 110 houses, two people per.
[02:36:15.540 --> 02:36:20.220]   They've purchased 3,500 acres in the Austin area.
[02:36:20.220 --> 02:36:22.780]   That's about four times the size of Central Park.
[02:36:22.780 --> 02:36:25.780]   Anyway--
[02:36:25.780 --> 02:36:29.500]   - That is such a weird metric I get
[02:36:29.500 --> 02:36:30.940]   'cause this is the Wall Street Journal, right?
[02:36:30.940 --> 02:36:31.780]   So I'm like--
[02:36:31.780 --> 02:36:33.340]   - Yeah, Central Park, you know, Central Park.
[02:36:33.340 --> 02:36:34.180]   - You know that?
[02:36:34.180 --> 02:36:39.060]   - What, that's X number, like, that's half the size of,
[02:36:39.060 --> 02:36:41.020]   I don't know, 30-- - 43 hectares.
[02:36:41.020 --> 02:36:42.940]   Is that better for you?
[02:36:42.940 --> 02:36:43.780]   I made that up.
[02:36:43.780 --> 02:36:45.300]   - It's just such a weird metric.
[02:36:45.300 --> 02:36:46.380]   - I don't think it is.
[02:36:46.380 --> 02:36:49.980]   Let's see.
[02:36:49.980 --> 02:36:52.460]   No more NFTs for Facebook or Instagram.
[02:36:52.460 --> 02:36:54.500]   - Aw.
[02:36:54.500 --> 02:36:56.700]   Aw.
[02:36:56.700 --> 02:36:57.580]   - That was quick.
[02:36:57.580 --> 02:36:58.420]   - Yeah.
[02:36:58.420 --> 02:36:59.740]   - Let's see.
[02:36:59.740 --> 02:37:01.740]   - That's the first time I've heard about NFTs
[02:37:01.740 --> 02:37:02.580]   in about six months.
[02:37:02.580 --> 02:37:03.420]   - Yeah, exactly.
[02:37:03.420 --> 02:37:05.660]   (laughing)
[02:37:05.660 --> 02:37:07.940]   - Anything else--
[02:37:07.940 --> 02:37:09.500]   - Facebook is doing the major shrinkage.
[02:37:09.500 --> 02:37:10.340]   - Yeah.
[02:37:10.340 --> 02:37:15.740]   - Mark Zuckerberg said it's gonna be a year of austerity.
[02:37:15.740 --> 02:37:18.380]   - Flatter is faster.
[02:37:18.380 --> 02:37:20.020]   Is the new work facts--
[02:37:20.020 --> 02:37:21.660]   - Besides firing 10,000 people,
[02:37:21.660 --> 02:37:23.820]   they're not gonna fill 5,000 open jobs.
[02:37:23.820 --> 02:37:27.700]   So it's really, head loss of 15,000.
[02:37:27.700 --> 02:37:28.540]   - So that's a--
[02:37:28.540 --> 02:37:33.220]   - They had hired 15,000 in the pandemic,
[02:37:33.220 --> 02:37:35.820]   but now they're going back to pre-pandemic, I think.
[02:37:35.820 --> 02:37:39.100]   - Yeah, well, if they don't fill those 5,000,
[02:37:39.100 --> 02:37:42.660]   they fire 11,000 more, they fire 10,000 in the fall.
[02:37:42.660 --> 02:37:46.220]   So that's 457,000.
[02:37:46.220 --> 02:37:48.900]   That's a huge number of people that won't be there.
[02:37:48.900 --> 02:37:52.380]   - And I come back to Eugene Rochko,
[02:37:52.380 --> 02:37:54.220]   at what he was our summit who said,
[02:37:54.220 --> 02:37:55.700]   and I said this last week,
[02:37:55.700 --> 02:37:57.380]   that the total investment to build--
[02:37:57.380 --> 02:37:59.180]   And it's in eight, three billion people,
[02:37:59.180 --> 02:38:00.300]   and eight Facebook,
[02:38:00.300 --> 02:38:03.380]   but Mastodon's pretty amazing, 500,000 bucks.
[02:38:03.380 --> 02:38:08.380]   And one person, he just this week, hired his first employee.
[02:38:08.380 --> 02:38:10.340]   - Incredible.
[02:38:10.340 --> 02:38:11.660]   - Here from-- - So efficient.
[02:38:11.660 --> 02:38:15.020]   - Here from teenyabode.com
[02:38:15.020 --> 02:38:17.820]   is a picture of Elon Musk's tiny house.
[02:38:17.820 --> 02:38:21.660]   It's a Casita-- - It's a trailer.
[02:38:21.660 --> 02:38:22.980]   - From boxables.
[02:38:22.980 --> 02:38:25.780]   This is-- - Those are so nice.
[02:38:25.780 --> 02:38:26.860]   We saw them at CES.
[02:38:26.860 --> 02:38:27.940]   - Did you?
[02:38:27.940 --> 02:38:29.580]   It's a prefab tiny house. - Yeah, awesome.
[02:38:29.580 --> 02:38:31.500]   - It's for a guest house on his property.
[02:38:31.500 --> 02:38:32.420]   Wait a minute, Elon has--
[02:38:32.420 --> 02:38:34.940]   I thought he got rid of all his properties.
[02:38:34.940 --> 02:38:38.260]   - No, he was living in somebody's mansion in Boston.
[02:38:38.260 --> 02:38:40.220]   - He was couch surfing. - Yeah.
[02:38:40.220 --> 02:38:44.980]   - He was couch surfing in Rob Roy and places like that.
[02:38:44.980 --> 02:38:46.380]   So--
[02:38:46.380 --> 02:38:50.580]   - A look inside Elon's $50,000 prefab tiny house,
[02:38:50.580 --> 02:38:53.180]   he uses as a guest house in Texas,
[02:38:53.180 --> 02:38:55.540]   where he threw a birthday party there.
[02:38:56.380 --> 02:39:01.380]   - Hmm, 375 square feet, bathroom, one bedroom,
[02:39:01.380 --> 02:39:03.140]   living room and kitchen, what?
[02:39:03.140 --> 02:39:07.860]   Okay, it's a little tight, but you know what?
[02:39:07.860 --> 02:39:09.020]   I could live in there.
[02:39:09.020 --> 02:39:13.580]   Except for the industrial door, I feel like--
[02:39:13.580 --> 02:39:15.220]   - You wouldn't, but you could.
[02:39:15.220 --> 02:39:16.060]   - I could.
[02:39:16.060 --> 02:39:18.900]   - The industrial door, what are you talking about?
[02:39:18.900 --> 02:39:21.620]   - This is this look like, I mean, look at this door.
[02:39:21.620 --> 02:39:24.060]   This looks like-- - That was my exact front door
[02:39:24.060 --> 02:39:25.220]   on my old house-- - Nevermind.
[02:39:25.220 --> 02:39:26.700]   - We designed and built ourself.
[02:39:26.700 --> 02:39:28.180]   - Nevermind, I love that door.
[02:39:28.180 --> 02:39:29.180]   - We had two of them.
[02:39:29.180 --> 02:39:30.460]   - Don't you, it was beautiful.
[02:39:30.460 --> 02:39:31.300]   - Okay.
[02:39:31.300 --> 02:39:32.380]   - We spent so much money on it.
[02:39:32.380 --> 02:39:33.880]   - Really?
[02:39:33.880 --> 02:39:34.720]   - Oh yeah.
[02:39:34.720 --> 02:39:36.980]   - You wanted people to be looking the door,
[02:39:36.980 --> 02:39:38.420]   like you coming up to the door and--
[02:39:38.420 --> 02:39:41.100]   - Well, you couldn't see into the house,
[02:39:41.100 --> 02:39:43.820]   we had, it was like a little alcove area.
[02:39:43.820 --> 02:39:44.660]   - Oh, okay.
[02:39:44.660 --> 02:39:45.620]   - We designed the house.
[02:39:45.620 --> 02:39:46.940]   - Clever.
[02:39:46.940 --> 02:39:48.100]   So that you could have a clear door.
[02:39:48.100 --> 02:39:49.540]   - But it was gorgeous.
[02:39:49.540 --> 02:39:50.380]   - Yeah.
[02:39:50.380 --> 02:39:51.420]   - That was awesome.
[02:39:51.420 --> 02:39:53.980]   I do like this, the pocket door,
[02:39:53.980 --> 02:39:55.900]   or whatever you call that, they'd open up
[02:39:55.900 --> 02:39:57.620]   and give you like a patio.
[02:39:57.620 --> 02:40:00.020]   No kitchen.
[02:40:00.020 --> 02:40:02.260]   It's pretty cute.
[02:40:02.260 --> 02:40:05.140]   Probably don't want the shower.
[02:40:05.140 --> 02:40:06.860]   - I hate those sinks.
[02:40:06.860 --> 02:40:07.700]   - Oh, I do too.
[02:40:07.700 --> 02:40:09.380]   What's the point of that sink?
[02:40:09.380 --> 02:40:10.420]   - You know what I hate?
[02:40:10.420 --> 02:40:11.620]   - I hate rain showers.
[02:40:11.620 --> 02:40:13.540]   I despise-- - I don't think it's rain showers too.
[02:40:13.540 --> 02:40:14.900]   - I do too.
[02:40:14.900 --> 02:40:17.060]   - We have one of these sinks in our guest bathroom,
[02:40:17.060 --> 02:40:18.980]   but I didn't put it there, that's just how it came,
[02:40:18.980 --> 02:40:19.940]   but I agree with it.
[02:40:19.940 --> 02:40:20.780]   - Yeah, we have one in there.
[02:40:20.780 --> 02:40:23.660]   - It's a little bowl that sits on top of the counter
[02:40:23.660 --> 02:40:26.020]   instead of being sunk into the counter.
[02:40:26.020 --> 02:40:27.860]   - You're awful.
[02:40:27.860 --> 02:40:30.100]   - You could never get water under things, isn't it?
[02:40:30.100 --> 02:40:30.940]   - Yeah, it's true.
[02:40:30.940 --> 02:40:32.900]   - Well, and they're just everything splashes
[02:40:32.900 --> 02:40:34.540]   and they're just annoying.
[02:40:34.540 --> 02:40:35.380]   I hate them.
[02:40:35.380 --> 02:40:36.860]   - It's called a sink.
[02:40:36.860 --> 02:40:38.180]   It's just sink into the counter.
[02:40:38.180 --> 02:40:39.420]   - It should sink.
[02:40:39.420 --> 02:40:40.260]   - Exactly.
[02:40:40.260 --> 02:40:41.500]   - I don't mind it, it's the guest bathroom.
[02:40:41.500 --> 02:40:44.180]   I don't want guests to be too comfortable.
[02:40:44.180 --> 02:40:46.180]   - It's my sink and I'm sorry.
[02:40:46.180 --> 02:40:47.020]   - Don't make yourself a phone.
[02:40:47.020 --> 02:40:47.860]   - I hate it.
[02:40:47.860 --> 02:40:48.700]   - Yeah, no.
[02:40:48.700 --> 02:40:51.380]   In our bathroom, it sinks into this thing.
[02:40:51.380 --> 02:40:52.740]   Look at the little closet.
[02:40:52.740 --> 02:40:55.620]   You can't have any long clothes, just short clothes.
[02:40:55.620 --> 02:40:57.500]   - Well, who wears long clothes anymore?
[02:40:57.500 --> 02:40:59.780]   - I wear long, I wear dresses, what the heck?
[02:40:59.780 --> 02:41:02.260]   - Yeah, it couldn't put a dress there with drag.
[02:41:02.260 --> 02:41:05.700]   And a my capes would fit.
[02:41:05.700 --> 02:41:07.500]   None of my capes would fit in there.
[02:41:07.500 --> 02:41:14.140]   I had a brief moment when I thought I should wear capes.
[02:41:14.140 --> 02:41:16.140]   - Let's bring back capes.
[02:41:16.140 --> 02:41:17.740]   - I did appreciate that.
[02:41:17.740 --> 02:41:20.060]   - I didn't, I don't know why.
[02:41:20.060 --> 02:41:22.460]   When I was a kid, like in high school,
[02:41:22.460 --> 02:41:26.940]   I wore a big black cape and I thought it was pretty fun.
[02:41:26.940 --> 02:41:28.820]   And I thought, so a couple of years ago,
[02:41:28.820 --> 02:41:29.820]   I went out. - I think you're gonna
[02:41:29.820 --> 02:41:31.380]   shoot up the high school, geez.
[02:41:31.380 --> 02:41:34.580]   - I bought two, well, so that's why I bought them in purple.
[02:41:34.580 --> 02:41:38.060]   One of them, and I was worried it was stolen down.
[02:41:38.060 --> 02:41:39.700]   - The charcoal or the church.
[02:41:39.700 --> 02:41:42.780]   - Yeah, one of them, I was a West Point cape
[02:41:42.780 --> 02:41:44.340]   and I didn't want it to be stolen valor,
[02:41:44.340 --> 02:41:47.580]   so I made sure it didn't have any insignia on it.
[02:41:47.580 --> 02:41:50.540]   But it was just perfect, it was a really nice cape,
[02:41:50.540 --> 02:41:53.900]   the kind that they would wear at West Point and dress,
[02:41:53.900 --> 02:41:54.940]   'cause it had, anyway.
[02:41:54.940 --> 02:41:57.900]   And then I got another one, I can't remember why.
[02:41:57.900 --> 02:41:59.660]   You know what, I'll wear it next week, I'll wear my cape.
[02:41:59.660 --> 02:42:00.580]   - Yeah, do, do.
[02:42:00.580 --> 02:42:02.020]   - My mom, she's a show religious.
[02:42:02.020 --> 02:42:02.860]   - She's a show religious.
[02:42:02.860 --> 02:42:04.300]   - She's a full swing of it.
[02:42:04.300 --> 02:42:06.660]   - Yeah, like Zorro.
[02:42:06.660 --> 02:42:08.340]   My mom who watches this show religiously
[02:42:08.340 --> 02:42:12.060]   will remember my cape phase in my youth.
[02:42:12.060 --> 02:42:13.980]   And she'll be, I don't know how she'll do it.
[02:42:13.980 --> 02:42:15.380]   - Mom, you watch the show?
[02:42:15.380 --> 02:42:17.860]   I didn't know that. - Oh, she loves this show.
[02:42:17.860 --> 02:42:20.020]   She said, I love, she says,
[02:42:20.020 --> 02:42:25.020]   I love Stacy, I love Jeff, I love Aunt.
[02:42:25.020 --> 02:42:28.900]   She's like, we're her family now.
[02:42:28.900 --> 02:42:30.140]   Well, we did.
[02:42:30.140 --> 02:42:31.500]   - We didn't hear your name in there.
[02:42:31.500 --> 02:42:32.580]   What's going on, my friend?
[02:42:32.580 --> 02:42:35.180]   - She didn't say anything about me.
[02:42:35.180 --> 02:42:36.020]   - Hello.
[02:42:36.020 --> 02:42:38.900]   - She didn't say, oh, you're so good or anything like that.
[02:42:38.900 --> 02:42:42.700]   So, maybe everyone should wear a cape on this show.
[02:42:42.700 --> 02:42:44.100]   Just for your mom. - I have a cape.
[02:42:44.100 --> 02:42:45.260]   - I bet you have a cape.
[02:42:45.260 --> 02:42:46.660]   I think you've worn it. - I have two capes.
[02:42:46.660 --> 02:42:47.580]   - You've worn it, I believe.
[02:42:47.580 --> 02:42:48.820]   - Oh, no, I've never worn a cape.
[02:42:48.820 --> 02:42:49.780]   Oh, no, I've wear shows.
[02:42:49.780 --> 02:42:50.740]   - No, I have a full one.
[02:42:50.740 --> 02:42:54.620]   I have a fancy wool cape for going to the opera.
[02:42:54.620 --> 02:42:56.140]   - If you live in a specific north west,
[02:42:56.140 --> 02:42:57.660]   it's appropriate to have a cape.
[02:42:57.660 --> 02:43:00.660]   Or if you're Batman.
[02:43:00.660 --> 02:43:02.020]   - I think a lot of people, okay,
[02:43:02.020 --> 02:43:03.340]   I was gonna say, I think a lot of people
[02:43:03.340 --> 02:43:05.060]   get along with that one just fine.
[02:43:05.060 --> 02:43:07.900]   And my mom made my child a cape.
[02:43:07.900 --> 02:43:10.820]   - I will wear, I will do a fashion show next week.
[02:43:10.820 --> 02:43:13.220]   I will either wear, I'll bring both capes
[02:43:13.220 --> 02:43:14.660]   and you can decide what you like better.
[02:43:14.660 --> 02:43:17.500]   One is quite expensive and has frogs.
[02:43:17.500 --> 02:43:18.900]   You know what they are.
[02:43:18.900 --> 02:43:20.020]   - Oh yeah, oh yeah.
[02:43:20.020 --> 02:43:22.860]   I have them on, oh, it's not on this.
[02:43:22.860 --> 02:43:25.540]   I actually, did you know, okay,
[02:43:25.540 --> 02:43:27.460]   no one will care about this actually.
[02:43:27.460 --> 02:43:29.500]   Nevermind, 'cause I don't have cardigans.
[02:43:29.500 --> 02:43:30.500]   - Go ahead, go ahead.
[02:43:30.500 --> 02:43:33.660]   - So like all my cardigans don't have buttons.
[02:43:33.660 --> 02:43:34.660]   - Buttons, where are the buttons?
[02:43:34.660 --> 02:43:35.660]   - How do you, any way?
[02:43:35.660 --> 02:43:38.020]   - I know, like what's that have?
[02:43:38.020 --> 02:43:38.860]   - How do you join them together?
[02:43:38.860 --> 02:43:40.940]   - You need to close your cardigan.
[02:43:40.940 --> 02:43:42.100]   - It's not a cardigan without a button.
[02:43:42.100 --> 02:43:43.420]   - I don't know.
[02:43:43.420 --> 02:43:44.460]   - Do you have a belt?
[02:43:44.460 --> 02:43:47.980]   - I, okay, I got frogs.
[02:43:47.980 --> 02:43:52.260]   I bought on Etsy, like they clip onto your cloak
[02:43:52.260 --> 02:43:53.620]   and you just, it's a frog closure
[02:43:53.620 --> 02:43:55.380]   and you just close it over and suddenly--
[02:43:55.380 --> 02:43:56.500]   - Perfect, that's what you need.
[02:43:56.500 --> 02:43:57.660]   - It's like a button.
[02:43:57.660 --> 02:43:58.500]   It's perfect.
[02:43:58.500 --> 02:43:59.500]   It was only 16 bucks.
[02:43:59.500 --> 02:44:00.500]   I love Etsy.
[02:44:00.500 --> 02:44:03.340]   - John says from now on we're gonna call you Stacey Two Capes.
[02:44:03.340 --> 02:44:04.180]   You can buff them.
[02:44:04.180 --> 02:44:05.620]   (laughing)
[02:44:05.620 --> 02:44:06.460]   - That's fair.
[02:44:06.460 --> 02:44:08.380]   (laughing)
[02:44:08.380 --> 02:44:09.940]   - All right, I wanna take a little tiny,
[02:44:09.940 --> 02:44:11.300]   teeny time, why I tie it out.
[02:44:11.300 --> 02:44:12.660]   And then the pics of the week
[02:44:12.660 --> 02:44:16.020]   and we'll run this thing in home because it is--
[02:44:16.020 --> 02:44:16.860]   - Into the ground.
[02:44:16.860 --> 02:44:18.220]   - We're gonna run into the ground.
[02:44:18.220 --> 02:44:20.260]   We will get it home in just a moment.
[02:44:20.260 --> 02:44:21.100]   - Just a moment, the first--
[02:44:21.100 --> 02:44:22.140]   - Just so you're hungry.
[02:44:22.140 --> 02:44:24.740]   - I want to tell you how important it is
[02:44:24.740 --> 02:44:25.780]   you joined ClubTwit.
[02:44:25.780 --> 02:44:27.460]   I mean, I,
[02:44:27.460 --> 02:44:28.700]   (laughing)
[02:44:28.700 --> 02:44:31.460]   Google glasses don't come cheap, my friends.
[02:44:31.460 --> 02:44:32.300]   If--
[02:44:32.300 --> 02:44:34.220]   (laughing)
[02:44:34.220 --> 02:44:35.780]   - Whoa, ClubTwit--
[02:44:35.780 --> 02:44:36.620]   - What?
[02:44:36.620 --> 02:44:38.900]   - Without moral panic, there would be no ClubTwit.
[02:44:38.900 --> 02:44:41.580]   (laughing)
[02:44:41.580 --> 02:44:43.340]   ClubTwit is seven bucks a month, that's all.
[02:44:43.340 --> 02:44:44.180]   We are now seven bucks.
[02:44:44.180 --> 02:44:46.900]   - That is your safe space from moral panic.
[02:44:46.900 --> 02:44:51.780]   - It is a safe space for everyone who listens to our shows.
[02:44:51.780 --> 02:44:54.060]   Seven bucks a month gets you ad-free versions
[02:44:54.060 --> 02:44:55.580]   of everything we do.
[02:44:55.580 --> 02:44:57.740]   You even get shows on the Twit Plus feed
[02:44:57.740 --> 02:44:59.660]   that we don't put out in public like hands on windows
[02:44:59.660 --> 02:45:01.580]   with Paul Therod, hands on Macintosh,
[02:45:01.580 --> 02:45:03.460]   with Mike Asarj at the Untitled Linux Show
[02:45:03.460 --> 02:45:04.300]   with Jonathan Bennett,
[02:45:04.300 --> 02:45:06.300]   the Gizfiz with Dick DiBartolo,
[02:45:06.300 --> 02:45:09.780]   Stacey's Book Club, a bunch of other great stuff.
[02:45:09.780 --> 02:45:12.500]   You get ad-free versions of all of the shows
[02:45:12.500 --> 02:45:15.780]   you get access to the fun Discord.
[02:45:15.780 --> 02:45:17.940]   And I tell you, the Discord is awesome.
[02:45:17.940 --> 02:45:19.500]   They're Stacey and her cape, by the way.
[02:45:19.500 --> 02:45:21.660]   That's great.
[02:45:21.660 --> 02:45:23.860]   I like it that you put the furniture outside.
[02:45:23.860 --> 02:45:24.860]   I think that's--
[02:45:24.860 --> 02:45:26.540]   - It's close, yeah, I like that.
[02:45:26.540 --> 02:45:30.020]   - That's where furniture begins and belongs.
[02:45:30.020 --> 02:45:31.340]   If you are in the club,
[02:45:31.340 --> 02:45:36.580]   this Anthony made this with help from Stable Diffusion,
[02:45:36.580 --> 02:45:39.460]   a ClubTwit sticker, which only you can have,
[02:45:39.460 --> 02:45:42.940]   I want you for ClubTwit with me as Uncle Sam.
[02:45:42.940 --> 02:45:46.140]   Please join us.
[02:45:46.140 --> 02:45:50.140]   Come in to Club, all you have to do is go to Twit.tv/ClubTwit.
[02:45:50.140 --> 02:45:52.980]   We are now 7,000 strong.
[02:45:52.980 --> 02:45:55.660]   If I think if we got to 10,000 people,
[02:45:55.660 --> 02:45:57.620]   I would do a jig on this table.
[02:45:57.620 --> 02:45:58.500]   I would do something.
[02:45:58.500 --> 02:46:00.060]   What should we say? - Okay, folks, that's--
[02:46:00.060 --> 02:46:02.020]   - We should think of something. - We're gonna see that jig.
[02:46:02.020 --> 02:46:02.980]   - Yeah, we should think of something
[02:46:02.980 --> 02:46:04.620]   because I would definitely celebrate.
[02:46:04.620 --> 02:46:06.220]   - We're gonna shave your hair.
[02:46:06.220 --> 02:46:10.260]   - We're wearing a cape to the jig wearing a cape.
[02:46:10.260 --> 02:46:13.660]   - No, the last time I danced on the table, it broke it.
[02:46:13.660 --> 02:46:16.100]   - I was gonna say, how sturdy is this table?
[02:46:16.100 --> 02:46:18.860]   - I did the Harlem, remember the Harlem Shake?
[02:46:18.860 --> 02:46:20.420]   Where everybody-- - Oh, yeah.
[02:46:20.420 --> 02:46:23.260]   - Remember that we did a Harlem Shake in the old studio
[02:46:23.260 --> 02:46:26.420]   and I got up on the table in my horse mask and broke it.
[02:46:26.420 --> 02:46:30.580]   I think we have video, to be honest with you.
[02:46:30.580 --> 02:46:31.580]   - I think we've seen the video.
[02:46:31.580 --> 02:46:32.420]   - Is this it?
[02:46:32.420 --> 02:46:33.260]   Oh, wow.
[02:46:33.260 --> 02:46:35.980]   Immediately, it got into the,
[02:46:35.980 --> 02:46:38.180]   that's pretty, that's quick, the club members.
[02:46:38.180 --> 02:46:40.860]   - Have you seen that really stupid meme?
[02:46:40.860 --> 02:46:42.940]   What is it, the Harlem Shake?
[02:46:42.940 --> 02:46:44.700]   Would somebody explain that to me?
[02:46:44.700 --> 02:46:46.460]   - Well, explain it to us, what you're talking about.
[02:46:46.460 --> 02:46:50.860]   - This week on Twitch, Old Men discuss "Snow" to "Vorac."
[02:46:50.860 --> 02:46:53.700]   - Hey, guys, guys, no, any day.
[02:46:53.700 --> 02:46:55.540]   - Oh, this hurts my heart.
[02:46:55.540 --> 02:46:56.580]   - That's a great, great needleman.
[02:46:56.580 --> 02:46:58.060]   - "Rafe Needleman," yep.
[02:46:58.060 --> 02:46:59.660]   - All the head bottles, they say.
[02:46:59.660 --> 02:47:01.100]   - "Headbot" is a floating head.
[02:47:01.100 --> 02:47:03.020]   - Based on what, this is a song by,
[02:47:03.020 --> 02:47:05.780]   it's a dance that can't,
[02:47:05.780 --> 02:47:07.940]   why am I getting an ad for Smoosh?
[02:47:07.940 --> 02:47:08.780]   What is this?
[02:47:08.780 --> 02:47:10.140]   - I don't know, but I'm not getting tried out.
[02:47:10.140 --> 02:47:10.980]   - Smosh, what the what?
[02:47:10.980 --> 02:47:12.900]   - How do I close it?
[02:47:12.900 --> 02:47:14.060]   It's a takeover ad.
[02:47:14.060 --> 02:47:16.900]   - I mean, the latest thing is you got the little bottom
[02:47:16.900 --> 02:47:17.740]   of the bottom of the bottom.
[02:47:17.740 --> 02:47:19.300]   - I'm gonna use new Smosh.
[02:47:19.300 --> 02:47:20.580]   - And you're gone.
[02:47:20.580 --> 02:47:22.940]   - Now I don't know what, why did I do that?
[02:47:22.940 --> 02:47:25.100]   - Okay, now I'm gone, I'm gone.
[02:47:25.100 --> 02:47:26.100]   - What happens to this thing?
[02:47:26.100 --> 02:47:27.860]   - Chad's a young person.
[02:47:27.860 --> 02:47:30.260]   - Chad, can you help us out with this thing?
[02:47:30.260 --> 02:47:33.340]   - Chad Johnson, I'm a producer.
[02:47:33.340 --> 02:47:34.260]   - That's so sweet. - That's a wonderful
[02:47:34.260 --> 02:47:36.660]   - What her job, this is for 2013.
[02:47:36.660 --> 02:47:38.300]   - The Harlem Shake and then--
[02:47:38.300 --> 02:47:39.140]   - On the internet.
[02:47:39.140 --> 02:47:39.980]   - Yeah, exactly.
[02:47:39.980 --> 02:47:40.820]   - Watch that.
[02:47:40.820 --> 02:47:41.980]   - Yeah, that's pretty crazy.
[02:47:41.980 --> 02:47:46.220]   So Harlem Shake is this, it's literally eight days old.
[02:47:46.220 --> 02:47:47.300]   The first Harlem Shake video--
[02:47:47.300 --> 02:47:48.860]   - See, that's how on top of it we were.
[02:47:48.860 --> 02:47:49.900]   - This weird one right here.
[02:47:49.900 --> 02:47:50.740]   - Okay.
[02:47:50.740 --> 02:47:51.740]   - Was up.
[02:47:51.740 --> 02:47:53.100]   - So it's this song.
[02:47:53.100 --> 02:47:54.460]   - You have to be in it.
[02:47:54.460 --> 02:47:57.340]   - Yeah, it was uploaded on February,
[02:47:57.340 --> 02:47:59.780]   February 2nd there.
[02:47:59.780 --> 02:48:00.700]   - Yeah, they're Power Rangers.
[02:48:00.700 --> 02:48:02.580]   - This is when we had young people in the studio
[02:48:02.580 --> 02:48:04.060]   who would explain stuff like yours.
[02:48:04.060 --> 02:48:06.220]   - Yeah, yeah.
[02:48:06.220 --> 02:48:07.060]   - To a--
[02:48:07.060 --> 02:48:07.980]   - We know that for a fact.
[02:48:07.980 --> 02:48:12.300]   - To a song that was created by what this guy named Bower.
[02:48:12.300 --> 02:48:14.660]   - It looks rather suggestive.
[02:48:14.660 --> 02:48:16.580]   - Yeah, there's always an edge of--
[02:48:16.580 --> 02:48:18.100]   - Look at these guys are getting issues.
[02:48:18.100 --> 02:48:19.100]   - Oh, come on.
[02:48:19.100 --> 02:48:19.940]   (laughing)
[02:48:19.940 --> 02:48:21.500]   - So that was the first one.
[02:48:21.500 --> 02:48:24.300]   And then it always starts out in a normal environment,
[02:48:24.300 --> 02:48:26.860]   there's like an office one right at the maker space
[02:48:26.860 --> 02:48:28.740]   where it's a normal environment,
[02:48:28.740 --> 02:48:31.140]   but there's one guy doing this stupid dance.
[02:48:31.140 --> 02:48:32.140]   (laughing)
[02:48:32.140 --> 02:48:34.460]   And then for some reason that it's completely unknown,
[02:48:34.460 --> 02:48:35.460]   everybody's dancing.
[02:48:35.460 --> 02:48:36.300]   And then it's over.
[02:48:36.300 --> 02:48:38.300]   - You dress so much better nowadays.
[02:48:38.300 --> 02:48:39.140]   - I do.
[02:48:39.140 --> 02:48:39.980]   - I dress like a lumberjack.
[02:48:39.980 --> 02:48:40.820]   - What am I?
[02:48:40.820 --> 02:48:41.740]   - We salute you.
[02:48:41.740 --> 02:48:43.300]   - What am I doing there?
[02:48:43.300 --> 02:48:45.580]   - No one makes up heavier too.
[02:48:45.580 --> 02:48:47.260]   Your contour is crazy.
[02:48:47.260 --> 02:48:48.220]   - Excellent contour.
[02:48:48.220 --> 02:48:50.620]   So this is, I think I owe it to you, Mike Elgin.
[02:48:50.620 --> 02:48:53.020]   You kind of raised the tone.
[02:48:53.020 --> 02:48:54.860]   So here's another, you should watch,
[02:48:54.860 --> 02:48:57.700]   'cause I think eventually we're gonna do it here.
[02:48:57.700 --> 02:49:00.140]   This is another one, one guy's dancing.
[02:49:00.140 --> 02:49:02.460]   And then everybody starts dancing.
[02:49:02.460 --> 02:49:06.900]   - And then the drop happens in the song, right?
[02:49:06.900 --> 02:49:09.180]   And then they're all dancing.
[02:49:09.180 --> 02:49:11.860]   - We're punching her.
[02:49:11.860 --> 02:49:13.180]   What stuff to wrap.
[02:49:13.180 --> 02:49:14.180]   - I'm stressing about this at all.
[02:49:14.180 --> 02:49:15.580]   - It's easier with an iPad.
[02:49:15.580 --> 02:49:17.140]   - Yeah.
[02:49:17.140 --> 02:49:19.260]   - I saw the guy get up on the chair there.
[02:49:19.260 --> 02:49:20.220]   - No, that's pretty bad.
[02:49:20.220 --> 02:49:21.140]   - Nerds.
[02:49:21.140 --> 02:49:22.460]   - Come on, we do better.
[02:49:22.460 --> 02:49:24.980]   We got somebody behind us here doing the same thing,
[02:49:24.980 --> 02:49:26.820]   hoping to get attention.
[02:49:26.820 --> 02:49:27.660]   - That's, I think he's,
[02:49:27.660 --> 02:49:29.060]   it's that's our intern.
[02:49:29.900 --> 02:49:32.660]   (laughing)
[02:49:32.660 --> 02:49:33.500]   - That's Eli.
[02:49:33.500 --> 02:49:34.340]   He's a, he's a,
[02:49:34.340 --> 02:49:35.180]   - Oh, he doesn't know any better.
[02:49:35.180 --> 02:49:36.500]   He's a high school student.
[02:49:36.500 --> 02:49:37.900]   He thinks it's a cool idea.
[02:49:37.900 --> 02:49:38.740]   - Yeah, he does.
[02:49:38.740 --> 02:49:41.820]   (laughing)
[02:49:41.820 --> 02:49:42.660]   - He's a boy.
[02:49:42.660 --> 02:49:45.700]   - And this is when we lost it.
[02:49:45.700 --> 02:49:49.540]   And then watch it, 'cause I'm gonna get up on the table.
[02:49:49.540 --> 02:49:54.140]   - Oh, no, and it's got a single bass.
[02:49:54.140 --> 02:49:55.140]   - Yeah, yeah, I do.
[02:49:55.140 --> 02:49:56.140]   - I don't wanna see this.
[02:49:56.140 --> 02:49:57.300]   - It's a disaster.
[02:49:57.300 --> 02:49:58.140]   - Oh, so.
[02:49:58.140 --> 02:50:00.220]   (laughing)
[02:50:00.220 --> 02:50:02.060]   - Give him, give him very far.
[02:50:02.060 --> 02:50:03.060]   - Oh genius.
[02:50:03.060 --> 02:50:06.340]   - Don't do the Harlem Shake.
[02:50:06.340 --> 02:50:07.140]   That's all I'm saying.
[02:50:07.140 --> 02:50:08.140]   Just don't do it.
[02:50:08.140 --> 02:50:08.980]   (laughing)
[02:50:08.980 --> 02:50:10.580]   I am better dressed these days,
[02:50:10.580 --> 02:50:11.860]   but I also, - You're so much.
[02:50:11.860 --> 02:50:12.620]   - I'm better mannered.
[02:50:12.620 --> 02:50:14.660]   I don't get up on the table.
[02:50:14.660 --> 02:50:16.340]   Hey everybody, it's Leo LaPorte,
[02:50:16.340 --> 02:50:21.100]   the founder and host of many of the Twitch podcasts.
[02:50:21.100 --> 02:50:24.220]   I don't normally talk to you about advertising,
[02:50:24.220 --> 02:50:27.020]   but I wanna take a moment to do that right now.
[02:50:27.020 --> 02:50:29.180]   Our mission statement at Twitch,
[02:50:29.180 --> 02:50:32.300]   we're dedicated to building a highly engaged community
[02:50:32.300 --> 02:50:34.340]   of tech enthusiasts.
[02:50:34.340 --> 02:50:37.980]   That's our audience, and you, I guess, since you're listening,
[02:50:37.980 --> 02:50:39.860]   by offering them the knowledge they need
[02:50:39.860 --> 02:50:43.580]   to understand and use technology in today's world.
[02:50:43.580 --> 02:50:47.900]   To do that, we also create partnerships with trusted brands
[02:50:47.900 --> 02:50:50.620]   and make important introductions between them and our audience.
[02:50:50.620 --> 02:50:53.180]   That's how we finance our podcasts,
[02:50:53.180 --> 02:50:56.300]   but it's also, and our audience tells us this all the time,
[02:50:56.300 --> 02:50:57.780]   a part of the service we offer.
[02:50:57.780 --> 02:51:01.940]   It's a valued bit of information for our audience members.
[02:51:01.940 --> 02:51:06.020]   They wanna know about great brands like yours.
[02:51:06.020 --> 02:51:10.020]   So can we help you by introducing you
[02:51:10.020 --> 02:51:11.940]   to our highly qualified audience?
[02:51:11.940 --> 02:51:15.020]   And why do you get a lot with advertising
[02:51:15.020 --> 02:51:16.620]   on the Twitch podcasts?
[02:51:16.620 --> 02:51:18.220]   Partnering with Twitch means you're gonna get,
[02:51:18.220 --> 02:51:20.700]   if I may say so, humbly, the gold standard
[02:51:20.700 --> 02:51:22.300]   in podcast advertising.
[02:51:22.300 --> 02:51:24.780]   And we throw in a lot of valuable services.
[02:51:24.780 --> 02:51:27.460]   You get a full service continuity team,
[02:51:27.460 --> 02:51:30.140]   supporting everything from copywriting to graphic design.
[02:51:30.140 --> 02:51:32.140]   I don't think anybody else does this,
[02:51:32.140 --> 02:51:34.580]   or does this as well as that we do.
[02:51:34.580 --> 02:51:36.900]   You get ads that are embedded in our content
[02:51:36.900 --> 02:51:38.220]   that are unique every time.
[02:51:38.220 --> 02:51:40.180]   I read them, our hosts read them.
[02:51:40.180 --> 02:51:42.540]   We always over deliver on impressions.
[02:51:42.540 --> 02:51:46.740]   And frankly, we're here to talk about your product.
[02:51:46.740 --> 02:51:50.380]   So we really give our listeners a great introduction
[02:51:50.380 --> 02:51:52.060]   to what you offer.
[02:51:52.060 --> 02:51:55.540]   We've got onboarding services, ad tech with pod sites.
[02:51:55.540 --> 02:51:57.300]   That's free for direct clients.
[02:51:57.300 --> 02:51:58.780]   We give you a lot of reporting
[02:51:58.780 --> 02:52:01.220]   so you know who saw your advertisement.
[02:52:01.220 --> 02:52:05.100]   You'll even know how many responded by going to your website.
[02:52:05.100 --> 02:52:06.940]   We'll also give you courtesy commercials
[02:52:06.940 --> 02:52:10.300]   that you can share across social media and landing pages.
[02:52:10.300 --> 02:52:11.940]   We think these are really valuable.
[02:52:11.940 --> 02:52:13.920]   People like me and our other hosts
[02:52:13.920 --> 02:52:17.900]   talking about your products sincerely and informationally.
[02:52:17.900 --> 02:52:19.660]   Those are incredibly valuable.
[02:52:19.660 --> 02:52:21.340]   You also get other free goodies, mentions
[02:52:21.340 --> 02:52:23.020]   in our weekly newsletter that's sent out
[02:52:23.020 --> 02:52:24.540]   to thousands of fans.
[02:52:24.540 --> 02:52:27.460]   We give bonus ads to people who buy
[02:52:27.460 --> 02:52:29.500]   a significant amount of advertising.
[02:52:29.500 --> 02:52:31.740]   You'll get social media promotion too.
[02:52:31.740 --> 02:52:35.140]   But let me tell you, we are looking for an advertising partner
[02:52:35.140 --> 02:52:37.060]   that's gonna be with us long term.
[02:52:37.060 --> 02:52:39.180]   Visit twit.tv/advertise.
[02:52:39.180 --> 02:52:41.020]   Check out our partner testimonials.
[02:52:41.020 --> 02:52:43.700]   Tim Broome, founder of ITProTV.
[02:52:43.700 --> 02:52:46.820]   They started ITProTV in 2013,
[02:52:46.820 --> 02:52:49.180]   immediately started advertising with us
[02:52:49.180 --> 02:52:54.180]   and grew that company to a really amazing success.
[02:52:54.180 --> 02:52:58.140]   Hundreds of thousands of ongoing customers.
[02:52:58.140 --> 02:53:00.340]   They've been on our network for more than 10 years
[02:53:00.340 --> 02:53:02.300]   and they say, and I'll quote Tim,
[02:53:02.300 --> 02:53:04.740]   "We would not be where we are today without the twit network."
[02:53:04.740 --> 02:53:06.220]   That's just one example.
[02:53:06.220 --> 02:53:09.100]   Mark McCurry, who's the CEO of Authentic.
[02:53:09.100 --> 02:53:12.500]   He was actually one of the first people to buy ads
[02:53:12.500 --> 02:53:13.340]   on our network.
[02:53:13.340 --> 02:53:15.340]   He's been with us for 16 years.
[02:53:15.340 --> 02:53:17.140]   He said, and I'm quoting,
[02:53:17.140 --> 02:53:20.180]   "The feedback from many advertisers over those 16 years
[02:53:20.180 --> 02:53:22.540]   across a range of product categories
[02:53:22.540 --> 02:53:26.620]   is that if ads and podcasts are gonna work for a brand,
[02:53:26.620 --> 02:53:28.180]   they're gonna work on twit shows."
[02:53:28.180 --> 02:53:31.460]   I'm proud to say that the ads we do over deliver,
[02:53:31.460 --> 02:53:34.900]   they work really well because they're honest,
[02:53:34.900 --> 02:53:36.060]   they have integrity.
[02:53:36.060 --> 02:53:38.140]   Our audience trusts us and we say,
[02:53:38.140 --> 02:53:39.740]   "This is a great product."
[02:53:39.740 --> 02:53:41.780]   They believe it, they listen.
[02:53:41.780 --> 02:53:43.660]   Our listeners are highly intelligent,
[02:53:43.660 --> 02:53:46.260]   they're heavily engaged, they're tech savvy,
[02:53:46.260 --> 02:53:48.100]   they're dedicated to our network,
[02:53:48.100 --> 02:53:49.940]   and that's partly because we only work
[02:53:49.940 --> 02:53:53.460]   with high integrity partners that we have thoroughly
[02:53:53.460 --> 02:53:55.700]   and personally vetted.
[02:53:55.700 --> 02:53:59.540]   I approve every single advertiser on the network.
[02:53:59.540 --> 02:54:01.020]   If you're ready to elevate your brand
[02:54:01.020 --> 02:54:02.700]   and you've got a great product,
[02:54:02.700 --> 02:54:04.400]   I want you to reach out to us,
[02:54:04.400 --> 02:54:07.300]   advertise at twit.tv.
[02:54:07.300 --> 02:54:10.420]   So I want you to break out of the advertising norm,
[02:54:10.420 --> 02:54:15.420]   grow your brand with HostRed authentic ads on twit.tv.
[02:54:16.260 --> 02:54:19.420]   Visit twit.tv/advertise for more details
[02:54:19.420 --> 02:54:22.300]   or email us, advertise at twit.tv
[02:54:22.300 --> 02:54:24.540]   if you're ready to launch your campaign now.
[02:54:24.540 --> 02:54:26.700]   Stacy, would you like to,
[02:54:26.700 --> 02:54:29.220]   that was my thing of the week, what do you guys say, Stacy?
[02:54:29.220 --> 02:54:30.260]   (laughs)
[02:54:30.260 --> 02:54:32.020]   - Oh, mine is the opposite of this.
[02:54:32.020 --> 02:54:33.860]   Okay, I don't know if y'all ever,
[02:54:33.860 --> 02:54:38.260]   y'all made me feel bad for not reading any nonfiction books.
[02:54:38.260 --> 02:54:39.660]   - No, no, no, never.
[02:54:39.660 --> 02:54:40.660]   - No, no, no.
[02:54:40.660 --> 02:54:42.460]   I was like, I do read nonfiction.
[02:54:42.460 --> 02:54:45.180]   So I picked up one that I really wanted to read
[02:54:45.180 --> 02:54:47.300]   and it is called, "A Where to Go."
[02:54:47.300 --> 02:54:49.460]   Okay, sorry, it's saving time,
[02:54:49.460 --> 02:54:52.780]   discovering a life beyond the clock by Jenny Odell.
[02:54:52.780 --> 02:54:55.980]   She is from the Bay Area, I believe as a writer
[02:54:55.980 --> 02:54:58.180]   and she had written about,
[02:54:58.180 --> 02:55:00.380]   the art of doing nothing was her prior book.
[02:55:00.380 --> 02:55:03.700]   This is kind of dense, but the idea is,
[02:55:03.700 --> 02:55:08.700]   it's a book about how we have just become short-termist
[02:55:08.700 --> 02:55:11.700]   and we have all these problems.
[02:55:11.700 --> 02:55:14.100]   So yeah, and I don't know if y'all ever read
[02:55:14.100 --> 02:55:15.660]   the clock of the long now.
[02:55:15.660 --> 02:55:16.500]   - Yes.
[02:55:16.500 --> 02:55:17.380]   - Did y'all ever, okay, so this is--
[02:55:17.380 --> 02:55:21.620]   - Oh, we interviewed Stuart Brand, who created, yeah, of course.
[02:55:21.620 --> 02:55:25.220]   - Yeah, yeah, so I mean, just 'cause that's kind of a,
[02:55:25.220 --> 02:55:27.540]   this is an, it's not an update on that,
[02:55:27.540 --> 02:55:29.780]   but it's again, it's all about long-term thinking
[02:55:29.780 --> 02:55:32.620]   and it kind of makes you feel like it tries to put
[02:55:32.620 --> 02:55:36.420]   some of the problems we have like climate change
[02:55:36.420 --> 02:55:38.820]   and just the stress we feel from living again,
[02:55:38.820 --> 02:55:40.980]   Stacy's in her anti-capitalist phase
[02:55:40.980 --> 02:55:42.980]   in a highly-- - Communist.
[02:55:42.980 --> 02:55:45.300]   - Late stage, capitalistic society,
[02:55:45.300 --> 02:55:47.420]   where time is money and money is time
[02:55:47.420 --> 02:55:48.340]   and money is your worth.
[02:55:48.340 --> 02:55:51.340]   And so it's a really nice book
[02:55:51.340 --> 02:55:54.820]   and it has a lot of good examples of wisdom
[02:55:54.820 --> 02:55:58.740]   of other civilizations, of other creatures.
[02:55:58.740 --> 02:56:00.380]   And really kind of relaxed me
[02:56:00.380 --> 02:56:02.220]   and made me feel a little bit more optimistic.
[02:56:02.220 --> 02:56:05.180]   It is kind of dense, it's not like an academic tone,
[02:56:05.180 --> 02:56:08.420]   but it is not like a, you'll wonder,
[02:56:08.420 --> 02:56:11.100]   like I found myself going back and re-referencing things
[02:56:11.100 --> 02:56:13.380]   and kind of making notes. - So it's not a self-help book,
[02:56:13.380 --> 02:56:14.380]   it looks like a self-help book,
[02:56:14.380 --> 02:56:16.060]   but it's really a philosophical-- - No, no, no, no.
[02:56:16.060 --> 02:56:18.500]   - It's philosophy. - It's a philosophy book, yeah.
[02:56:18.500 --> 02:56:20.020]   - Yeah. - I don't read self-help.
[02:56:20.020 --> 02:56:21.500]   - No. - I'm perfect, just the way I am.
[02:56:21.500 --> 02:56:22.900]   - Exactly.
[02:56:22.900 --> 02:56:26.780]   Saving time, discovering a life beyond the clock.
[02:56:26.780 --> 02:56:29.060]   - Never changed, it's never changed. - Like Jenny Hotel.
[02:56:29.060 --> 02:56:31.980]   O-D-E-L-L, Penguin Random House.
[02:56:31.980 --> 02:56:33.660]   So you like it, I should read this to self-help.
[02:56:33.660 --> 02:56:36.180]   - I do, I think, I'm only about,
[02:56:36.180 --> 02:56:38.580]   I'm about halfway through, see that it's dense,
[02:56:38.580 --> 02:56:40.580]   but I like it a lot of-- - 'Cause who has the time, right?
[02:56:40.580 --> 02:56:42.260]   - Yeah, exactly. - Saving time.
[02:56:42.260 --> 02:56:44.300]   Don't read this book would be one way.
[02:56:44.300 --> 02:56:46.660]   Okay, thank you. (laughs)
[02:56:46.660 --> 02:56:48.380]   Mike Elgar. - So yeah, I like it.
[02:56:48.380 --> 02:56:50.460]   And there you go, instead of telling you
[02:56:50.460 --> 02:56:53.660]   about a new connected coffee maker, which you don't need it,
[02:56:53.660 --> 02:56:55.820]   so read this book, it's gonna make you far happier
[02:56:55.820 --> 02:56:57.860]   than buying a connected device. (laughs)
[02:56:57.860 --> 02:57:00.540]   - Thank you, my God, you've just saved me.
[02:57:00.540 --> 02:57:02.940]   I see the future of Stacy's life though.
[02:57:02.940 --> 02:57:06.420]   - Jeff Jarvis. - Jeff Jarvis.
[02:57:06.420 --> 02:57:08.300]   - He's gonna go all and get rid of all this.
[02:57:08.300 --> 02:57:10.140]   - Well, I wanna mention just a few things.
[02:57:10.140 --> 02:57:11.540]   Kaki is 25.
[02:57:11.540 --> 02:57:15.500]   - Kaki, not the guy, the website.
[02:57:15.500 --> 02:57:17.500]   - What website? - Not Jason.
[02:57:17.500 --> 02:57:20.980]   - You know, Jason himself. - Not twice the, no.
[02:57:20.980 --> 02:57:22.180]   - Yes, the website is 25.
[02:57:22.180 --> 02:57:24.340]   - Kaki.org. - Wow.
[02:57:24.340 --> 02:57:26.260]   - Yes, Jason, Jason's site.
[02:57:26.260 --> 02:57:29.380]   And then a little bit of our own history here
[02:57:29.380 --> 02:57:32.180]   is Jeter Japani, founded Life Hacker.
[02:57:32.180 --> 02:57:34.140]   - Yeah. - It just got sold by Go Media
[02:57:34.140 --> 02:57:36.500]   as if Davis. - Oh, I hope she got
[02:57:36.500 --> 02:57:38.180]   some money out of that.
[02:57:38.180 --> 02:57:40.540]   - Oh, no, it was long ago, no, it was,
[02:57:40.540 --> 02:57:41.860]   'cause don't forget it was Gawker Media,
[02:57:41.860 --> 02:57:43.860]   Gawker was bankrupt. - Oh, that's right.
[02:57:43.860 --> 02:57:46.700]   - One company that got by Go, and I got by another one.
[02:57:46.700 --> 02:57:51.420]   - Go has been slowly selling off all the Gawker pieces.
[02:57:51.420 --> 02:57:52.460]   - Pieces of it. - Yep.
[02:57:52.460 --> 02:57:53.700]   - Pieces of it.
[02:57:53.700 --> 02:57:55.340]   I wanna mention, 'cause we talked about this,
[02:57:55.340 --> 02:57:57.420]   I think two weeks ago or last week,
[02:57:57.420 --> 02:58:00.540]   we have now a report online about the Black Twitter Summit,
[02:58:00.540 --> 02:58:01.700]   which we held at my school.
[02:58:01.700 --> 02:58:06.380]   - So there's been a request that we recreated here,
[02:58:06.380 --> 02:58:08.020]   would you like to?
[02:58:08.020 --> 02:58:10.700]   - I think that would be extremely difficult to do.
[02:58:10.700 --> 02:58:11.980]   However, I was thinking about that,
[02:58:11.980 --> 02:58:15.260]   and I think that Meredith Clark,
[02:58:15.260 --> 02:58:16.980]   who's now at Northeastern,
[02:58:16.980 --> 02:58:19.020]   has a book coming out this year on Black Twitter.
[02:58:19.020 --> 02:58:20.820]   - Oh, we'll get him a read. - And Andre Brock.
[02:58:20.820 --> 02:58:21.980]   - Okay. - We get Meredith on
[02:58:21.980 --> 02:58:22.820]   to talk about Black Twitter,
[02:58:22.820 --> 02:58:23.660]   I think it could be the best book.
[02:58:23.660 --> 02:58:24.620]   - Well, it would be great.
[02:58:24.620 --> 02:58:25.940]   That would be good.
[02:58:25.940 --> 02:58:26.780]   - Yeah, I don't think we could--
[02:58:26.780 --> 02:58:27.620]   - We don't think we could-- - We don't think we could
[02:58:27.620 --> 02:58:28.780]   do that in a real way.
[02:58:28.780 --> 02:58:31.020]   - Yeah, we're getting all the people together and stuff.
[02:58:31.020 --> 02:58:33.020]   We're white. - So, sorry to have--
[02:58:33.020 --> 02:58:35.020]   - Sorry to have-- - We have some black people
[02:58:35.020 --> 02:58:36.740]   around somewhere.
[02:58:36.740 --> 02:58:38.380]   - We could get them into it,
[02:58:38.380 --> 02:58:41.140]   if it's inappropriate for me to do it.
[02:58:41.140 --> 02:58:43.380]   - But I can get a Twitter now, so that's the other panel.
[02:58:43.380 --> 02:58:44.940]   - Well, I--
[02:58:44.940 --> 02:58:47.220]   - So, I was already a tailor who was a PhD candidate
[02:58:47.220 --> 02:58:49.180]   at I think UNC, wrote a wonderful report
[02:58:49.180 --> 02:58:50.340]   on a very hard-to-bring together,
[02:58:50.340 --> 02:58:52.740]   so I wanted to mention that.
[02:58:52.740 --> 02:58:56.780]   And then, this might nerd you out, I don't know.
[02:58:56.780 --> 02:59:01.260]   The color printer, I just came across this,
[02:59:01.260 --> 02:59:06.260]   and it's an old book about the colors
[02:59:06.260 --> 02:59:08.180]   to put together in printing,
[02:59:08.180 --> 02:59:11.180]   that a guy came along and recreated,
[02:59:11.180 --> 02:59:14.820]   as I think of book, but then mainly as a poster.
[02:59:14.820 --> 02:59:17.220]   So, if you scroll down a little bit,
[02:59:17.220 --> 02:59:18.460]   you'll start to see this.
[02:59:18.460 --> 02:59:21.180]   So, this is the actual stuff about how if you do
[02:59:21.180 --> 02:59:22.580]   so many parts of this color,
[02:59:22.580 --> 02:59:23.940]   to so many parts of that color.
[02:59:23.940 --> 02:59:26.140]   - Wow. - Oh, you got multiple colors.
[02:59:26.140 --> 02:59:27.620]   - Oh, that's cool.
[02:59:27.620 --> 02:59:30.300]   - And isn't that, so then if you go back up to the top
[02:59:30.300 --> 02:59:32.100]   and go to the boat by a poster,
[02:59:32.100 --> 02:59:33.420]   you'll see it all made as a poster.
[02:59:33.420 --> 02:59:35.780]   - Oh. - Order a poster.
[02:59:35.780 --> 02:59:36.620]   - Oh, I-- - And I--
[02:59:36.620 --> 02:59:38.460]   - Glenn Fleishman's got a half.
[02:59:38.460 --> 02:59:39.660]   - Oh, he already ordered it.
[02:59:39.660 --> 02:59:41.020]   He already ordered it.
[02:59:41.020 --> 02:59:41.860]   (laughs)
[02:59:41.860 --> 02:59:43.780]   It was right up his alley.
[02:59:43.780 --> 02:59:45.340]   I think it's kind of beautiful.
[02:59:45.340 --> 02:59:48.920]   - This is from an 1892 treatise on color printing.
[02:59:48.920 --> 02:59:51.620]   Wow. - Oh, you--
[02:59:51.620 --> 02:59:54.980]   - I didn't know they had color printing way back then.
[02:59:54.980 --> 02:59:57.980]   - Today, I believe it's today, is
[02:59:57.980 --> 03:00:02.140]   (speaks in foreign language)
[03:00:02.140 --> 03:00:05.260]   printing Arts Day across all of Germany,
[03:00:05.260 --> 03:00:07.180]   all the printing museums and things
[03:00:07.180 --> 03:00:08.380]   are in other countries as well,
[03:00:08.380 --> 03:00:11.820]   or honoring the art of printing.
[03:00:11.820 --> 03:00:13.180]   - So I thought I would do that for the day.
[03:00:13.180 --> 03:00:15.780]   - It is also the Ides of March.
[03:00:15.780 --> 03:00:17.740]   So beware. - It is.
[03:00:17.740 --> 03:00:20.020]   - Do not go to the forum with your toga on,
[03:00:20.020 --> 03:00:22.060]   or maybe wear something underneath--
[03:00:22.060 --> 03:00:23.340]   - Oh, it is the Ides of March.
[03:00:23.340 --> 03:00:24.500]   - Yeah, oh, yeah. - Yes.
[03:00:24.500 --> 03:00:25.900]   - I would not be joking.
[03:00:25.900 --> 03:00:28.620]   - I would not.
[03:00:28.620 --> 03:00:33.620]   Mr. Mike Elgin has a thing for us.
[03:00:33.620 --> 03:00:34.700]   - Yes.
[03:00:34.700 --> 03:00:38.180]   So we had a long conversation about chat TBT
[03:00:38.180 --> 03:00:41.260]   and its new innovations and about its data set.
[03:00:41.260 --> 03:00:42.980]   And I had mentioned that the future
[03:00:42.980 --> 03:00:46.060]   of this kind of technology is you supply the data set,
[03:00:46.060 --> 03:00:48.340]   or data set as your company or something like that.
[03:00:48.340 --> 03:00:52.740]   So here's a real world product that you can use
[03:00:52.740 --> 03:00:57.580]   with great value right now called Chat PDF.
[03:00:57.580 --> 03:01:00.940]   So basically you plug in any PDF file into this,
[03:01:00.940 --> 03:01:03.100]   you upload it to this service,
[03:01:03.100 --> 03:01:05.780]   and then you just have a conversation with the PDF file.
[03:01:05.780 --> 03:01:08.700]   Now this is really useful because many, many PDFs
[03:01:08.700 --> 03:01:11.740]   are highly technical, they tend to be very long.
[03:01:11.740 --> 03:01:14.140]   I'll just give you one small example.
[03:01:14.140 --> 03:01:15.780]   I have a Sony camera.
[03:01:15.780 --> 03:01:18.380]   Sony camera's manual comes in PDF form.
[03:01:18.380 --> 03:01:19.220]   - Yes.
[03:01:19.220 --> 03:01:20.340]   - It's a million pages. - Yes.
[03:01:20.340 --> 03:01:22.220]   - And you can just throw that in there and just say,
[03:01:22.220 --> 03:01:24.860]   well how do I, you ask it in plain language,
[03:01:24.860 --> 03:01:26.180]   how do I do x, y, z?
[03:01:26.180 --> 03:01:28.020]   And it says, here's how you do it.
[03:01:28.020 --> 03:01:31.220]   And it also kind of like documents itself
[03:01:31.220 --> 03:01:34.460]   says like on page 52, it tells you how to blabbe blah.
[03:01:34.460 --> 03:01:37.540]   And this is great for contracts that you're gonna sign,
[03:01:37.540 --> 03:01:41.660]   which I always do as a freelancer for rental agreements.
[03:01:41.660 --> 03:01:44.260]   So many things in life come in the form of PDF,
[03:01:44.260 --> 03:01:46.300]   and if they don't, you can convert them to a PDF
[03:01:46.300 --> 03:01:48.220]   to drop in there and have a conversation with it.
[03:01:48.220 --> 03:01:53.220]   It's a way to, it's not super sophisticated like GPT-4,
[03:01:53.220 --> 03:01:59.060]   but it's a very useful way to simplify complex documents
[03:01:59.060 --> 03:02:03.580]   and extract information from them very quickly.
[03:02:03.580 --> 03:02:04.660]   - I just chatted in the PDF demo.
[03:02:04.660 --> 03:02:06.580]   - That'd be nice for reading laws.
[03:02:06.580 --> 03:02:09.660]   - It's what I just put in the Supreme Court
[03:02:09.660 --> 03:02:12.020]   pleading Google versus Gonzalez.
[03:02:12.020 --> 03:02:16.820]   And it read it all in.
[03:02:16.820 --> 03:02:18.580]   What are the main points of the respondents argument
[03:02:18.580 --> 03:02:19.620]   in this case?
[03:02:19.620 --> 03:02:22.020]   That's interest.
[03:02:22.020 --> 03:02:23.700]   This would be, boy, for the show.
[03:02:23.700 --> 03:02:24.700]   This is--
[03:02:24.700 --> 03:02:25.580]   - Yeah, I was gonna say,
[03:02:25.580 --> 03:02:28.740]   Leo now has no job as the guy who's summarizing stories.
[03:02:28.740 --> 03:02:29.580]   We'll just let this do it.
[03:02:29.580 --> 03:02:31.660]   - I'll just read chat GPT.
[03:02:31.660 --> 03:02:33.580]   So what is, is this using chat GPT?
[03:02:33.580 --> 03:02:34.700]   Chat PDF document.
[03:02:34.700 --> 03:02:35.780]   - It is.
[03:02:35.780 --> 03:02:36.620]   - Yep.
[03:02:36.620 --> 03:02:37.500]   Wow.
[03:02:37.500 --> 03:02:41.300]   - I just put in an academic one for the internet book
[03:02:41.300 --> 03:02:43.060]   I'm working on, and it came back and it said,
[03:02:43.060 --> 03:02:44.980]   one of the results of the online experiment
[03:02:44.980 --> 03:02:46.580]   conducted by the authors,
[03:02:46.580 --> 03:02:48.060]   these are suggested things.
[03:02:48.060 --> 03:02:50.620]   Here are columns that sell.
[03:02:50.620 --> 03:02:52.820]   Wow.
[03:02:52.820 --> 03:02:54.540]   By the way, the authors found only limited evidence
[03:02:54.540 --> 03:02:55.980]   for polarization effects
[03:02:55.980 --> 03:02:58.380]   by content-based news recommendation systems.
[03:02:58.380 --> 03:02:59.900]   - Well, that's kind of cheating
[03:02:59.900 --> 03:03:02.780]   because there is an abstract.
[03:03:02.780 --> 03:03:03.980]   - Oh, well, yeah.
[03:03:03.980 --> 03:03:06.820]   That's what we all read when we read those papers.
[03:03:06.820 --> 03:03:07.660]   (laughs)
[03:03:07.660 --> 03:03:08.500]   Right?
[03:03:08.500 --> 03:03:09.340]   You start out-- - And we don't hold--
[03:03:09.340 --> 03:03:10.500]   - Or do I go straight to the conclusion?
[03:03:10.500 --> 03:03:12.220]   And then I'm like, okay.
[03:03:12.220 --> 03:03:16.100]   - Oh, somebody said I should put Steve Gibson's notes
[03:03:16.100 --> 03:03:17.980]   for security now in the air.
[03:03:17.980 --> 03:03:18.820]   - Wow, yeah.
[03:03:18.820 --> 03:03:20.380]   (laughs)
[03:03:20.380 --> 03:03:21.220]   - That's in it.
[03:03:21.220 --> 03:03:23.100]   Let me just try real quickly.
[03:03:23.100 --> 03:03:24.340]   What a, this is great.
[03:03:24.340 --> 03:03:25.540]   Thank you.
[03:03:25.540 --> 03:03:28.060]   - I would do that for any of Stephen Wolfram's blog posts.
[03:03:28.060 --> 03:03:31.500]   - This could save me so much time.
[03:03:31.500 --> 03:03:35.300]   Let's see, reset or close.
[03:03:35.300 --> 03:03:36.140]   All right.
[03:03:36.140 --> 03:03:41.140]   I am going to open up the PDF.
[03:03:41.140 --> 03:03:43.180]   I mean, I guess I'll browse my computer.
[03:03:43.180 --> 03:03:44.620]   The other beauty of this, by the way,
[03:03:44.620 --> 03:03:47.180]   is I can't think of any way to abuse this
[03:03:47.180 --> 03:03:48.700]   for nefarious purposes.
[03:03:48.700 --> 03:03:51.780]   - Uh, you know, right.
[03:03:51.780 --> 03:03:53.300]   - Unlike Chatsy P.T. as well.
[03:03:53.300 --> 03:03:55.500]   - Well, a student puts it a PDF of a book
[03:03:55.500 --> 03:03:57.100]   and gets a book report out of it by cheating,
[03:03:57.100 --> 03:03:58.300]   but I wouldn't call it nefarious.
[03:03:58.300 --> 03:03:59.900]   I would call that clever.
[03:03:59.900 --> 03:04:00.740]   (laughs)
[03:04:00.740 --> 03:04:01.820]   Right, they deserve the A.
[03:04:01.820 --> 03:04:02.660]   - Yeah.
[03:04:02.660 --> 03:04:04.780]   - Mash Potato and our Discord said,
[03:04:04.780 --> 03:04:07.900]   put in Steve Gibson's show notes.
[03:04:07.900 --> 03:04:08.740]   Here we go.
[03:04:08.740 --> 03:04:09.580]   Hello there. - Yes.
[03:04:09.580 --> 03:04:10.420]   - Welcome to security now.
[03:04:10.420 --> 03:04:12.220]   In this episode, we're discovering very secure
[03:04:12.220 --> 03:04:14.980]   related topics, including the constant,
[03:04:14.980 --> 03:04:17.860]   this is good here, three example questions.
[03:04:17.860 --> 03:04:20.580]   What is the number one way bad guys penetrate networks
[03:04:20.580 --> 03:04:23.300]   and how has it changed in the past year?
[03:04:23.300 --> 03:04:25.460]   Through phishing attacks.
[03:04:25.460 --> 03:04:26.620]   However, it's changed in the past years,
[03:04:26.620 --> 03:04:28.020]   attackers are now using more sophisticated
[03:04:28.020 --> 03:04:31.140]   and targeted spear phishing attacks.
[03:04:31.140 --> 03:04:33.380]   Why did Sony bring a lawsuit against
[03:04:33.380 --> 03:04:36.080]   the innocent nonprofit do good or quad nine?
[03:04:36.080 --> 03:04:38.700]   Which we probably should have made a story,
[03:04:38.700 --> 03:04:40.420]   but if you want to know more,
[03:04:40.420 --> 03:04:43.740]   listen to Steve's security now from yesterday,
[03:04:43.740 --> 03:04:45.620]   quad nine, which is a DNS provider,
[03:04:45.620 --> 03:04:47.220]   is sued by a record company,
[03:04:47.220 --> 03:04:50.940]   successfully I might add in Germany,
[03:04:50.940 --> 03:04:55.900]   because Sony couldn't get ahold of the infringer,
[03:04:55.900 --> 03:04:58.500]   and so they went after the DNS provider,
[03:04:58.500 --> 03:05:00.060]   one of many, quad nine,
[03:05:00.060 --> 03:05:03.540]   which is a non-profit DNS provider,
[03:05:03.540 --> 03:05:04.460]   and they won.
[03:05:04.460 --> 03:05:09.100]   The DNS provider has to block quad nine maintains
[03:05:09.100 --> 03:05:12.300]   that Sony's request essentially amounts to content censorship
[03:05:12.300 --> 03:05:14.620]   and risks cracking the foundations of a free
[03:05:14.620 --> 03:05:16.180]   and open internet in Europe,
[03:05:16.180 --> 03:05:17.380]   and potentially worldwide.
[03:05:17.380 --> 03:05:18.820]   This is good.
[03:05:18.820 --> 03:05:20.860]   I finally understand security now.
[03:05:20.860 --> 03:05:23.620]   (laughing)
[03:05:23.620 --> 03:05:25.100]   Thank you, Mike.
[03:05:25.100 --> 03:05:28.820]   And it's free, chat PDF.com.
[03:05:28.820 --> 03:05:29.660]   - Dot com.
[03:05:29.660 --> 03:05:32.740]   - Wow, that is really cool.
[03:05:32.740 --> 03:05:35.340]   We're gonna see, this is, you know,
[03:05:35.340 --> 03:05:37.620]   yesterday and the day before I was preparing
[03:05:37.620 --> 03:05:39.260]   for our show today,
[03:05:39.260 --> 03:05:40.740]   going through stories, and I just stopped.
[03:05:40.740 --> 03:05:41.820]   - You prepare?
[03:05:41.820 --> 03:05:43.980]   - Oh God, I spent all week looking for,
[03:05:43.980 --> 03:05:44.820]   I can tell.
[03:05:44.820 --> 03:05:46.420]   - I know, I know, I do too.
[03:05:46.420 --> 03:05:47.420]   - Stories, and you do too,
[03:05:47.420 --> 03:05:49.820]   you usually find more than me because I'm selective.
[03:05:49.820 --> 03:05:53.980]   Anyway, that's another story for another day,
[03:05:53.980 --> 03:05:56.620]   but I am looking for what we're gonna talk about today,
[03:05:56.620 --> 03:05:58.780]   and I gave, I threw up my hands, I said,
[03:05:58.780 --> 03:06:02.460]   "I can't, we'll just, we'll wing it on AI,
[03:06:02.460 --> 03:06:04.780]   "because it's just, there's so much."
[03:06:04.780 --> 03:06:06.420]   - It's too big a tub, well that's why you need a show.
[03:06:06.420 --> 03:06:07.260]   What about that?
[03:06:07.260 --> 03:06:09.340]   - I think this is the show about that.
[03:06:09.340 --> 03:06:10.460]   I don't know if I wanted that,
[03:06:10.460 --> 03:06:13.460]   but we certainly will talk about it here,
[03:06:13.460 --> 03:06:16.980]   and on Twitch, and on other shows as needed.
[03:06:16.980 --> 03:06:17.860]   I mean, we certainly talked about it
[03:06:17.860 --> 03:06:19.980]   on Windows Weekly today, so.
[03:06:19.980 --> 03:06:22.460]   - Mike, it's so great to see you.
[03:06:22.460 --> 03:06:24.220]   Folks, where are you going next?
[03:06:24.220 --> 03:06:25.940]   Gastronomad.net.
[03:06:25.940 --> 03:06:32.060]   - Yeah, we are going to Central America in, I think next week,
[03:06:32.060 --> 03:06:34.500]   and can have a great time there,
[03:06:34.500 --> 03:06:37.940]   and then we're going to,
[03:06:37.940 --> 03:06:39.780]   we're coming back here for half a day,
[03:06:39.780 --> 03:06:40.860]   and then going to--
[03:06:40.860 --> 03:06:41.700]   - Wow.
[03:06:41.700 --> 03:06:42.540]   - Wahaka.
[03:06:42.540 --> 03:06:44.220]   - We're going to, yeah.
[03:06:44.220 --> 03:06:46.780]   - And we had so much fun in Wahaka,
[03:06:46.780 --> 03:06:49.500]   with Mike and Amira, and that's where we met
[03:06:49.500 --> 03:06:52.260]   our Facebook friend and a bunch of other wonderful,
[03:06:52.260 --> 03:06:55.900]   fascinating, smart people, and we saw so much,
[03:06:55.900 --> 03:06:58.100]   and ate so much, and drank so much miscal,
[03:06:58.100 --> 03:07:01.380]   it was really a blast, I highly recommend it.
[03:07:01.380 --> 03:07:04.260]   - Yeah, so if anyone wants to join us for any of these things,
[03:07:04.260 --> 03:07:06.580]   most of this years are sold out,
[03:07:06.580 --> 03:07:11.580]   but we have the Venice-Presco experience in the fall,
[03:07:11.580 --> 03:07:14.700]   the first one is sold out, but the second one is not.
[03:07:14.700 --> 03:07:15.540]   - Oh, nice.
[03:07:15.540 --> 03:07:16.700]   - And we have some room in Mexico City,
[03:07:16.700 --> 03:07:17.580]   and Wahaka for this year,
[03:07:17.580 --> 03:07:19.220]   and then of course, all of them
[03:07:19.220 --> 03:07:21.300]   has some availability next year as well.
[03:07:21.300 --> 03:07:23.340]   - Mike, can I ask you a terribly rude question?
[03:07:23.340 --> 03:07:24.420]   - I insist.
[03:07:24.420 --> 03:07:26.460]   - Just because, you know, in trying to teach students,
[03:07:26.460 --> 03:07:28.100]   entrepreneurial journalism and such,
[03:07:28.100 --> 03:07:31.900]   I often mention your business there,
[03:07:31.900 --> 03:07:33.340]   'cause there's other kinds of travel things
[03:07:33.340 --> 03:07:35.060]   that people can actually do.
[03:07:35.060 --> 03:07:36.220]   Just trying to get a sense,
[03:07:36.220 --> 03:07:40.140]   is you also are writing and you're doing all kinds of things
[03:07:40.140 --> 03:07:42.660]   as a proportion of your upkeep?
[03:07:42.660 --> 03:07:43.500]   - Yeah.
[03:07:43.500 --> 03:07:44.420]   - How well does that do?
[03:07:44.420 --> 03:07:46.100]   I'm not trying to get any numbers out of your own,
[03:07:46.100 --> 03:07:49.420]   like, but is it a living?
[03:07:49.420 --> 03:07:52.140]   Is it a great supplement?
[03:07:52.140 --> 03:07:54.420]   Are you wealthy and we just don't know it?
[03:07:54.420 --> 03:07:57.780]   - I'm wealthy and I don't know it.
[03:07:57.780 --> 03:08:00.700]   No, it's none of the above.
[03:08:00.700 --> 03:08:03.140]   Essentially, the way we do it in our family
[03:08:03.140 --> 03:08:08.140]   is that the writing part is 90% me and 10% my wife
[03:08:08.140 --> 03:08:11.540]   and the gastronome-ad part is 90% my wife and 10% me.
[03:08:11.540 --> 03:08:13.140]   So it's really her business.
[03:08:13.140 --> 03:08:18.140]   So it is kind of a living.
[03:08:18.140 --> 03:08:20.940]   If you were to pay her by the,
[03:08:20.940 --> 03:08:23.620]   if you were to calculate what she makes on an hour,
[03:08:23.620 --> 03:08:25.700]   the basis would be like a dollar an hour,
[03:08:25.700 --> 03:08:29.540]   'cause she just spends all of her waking hours exploring,
[03:08:29.540 --> 03:08:30.780]   doing all this stuff that's kind of,
[03:08:30.780 --> 03:08:32.460]   is it work, is it not work, is it?
[03:08:32.460 --> 03:08:35.380]   Is it like, so it's hard to detect.
[03:08:35.380 --> 03:08:38.740]   But she makes a pretty good living
[03:08:38.740 --> 03:08:41.820]   and I make a kind of an okay living
[03:08:41.820 --> 03:08:44.420]   and I make a kind of an okay living writing
[03:08:44.420 --> 03:08:46.180]   and together we make a living.
[03:08:46.180 --> 03:08:50.540]   So it's really, she does okay,
[03:08:50.540 --> 03:08:54.620]   but again, it's very difficult because she'll spend a decade
[03:08:54.620 --> 03:08:57.380]   learning, exploring all this stuff.
[03:08:57.380 --> 03:09:00.300]   And then before we start, the first experience.
[03:09:00.300 --> 03:09:01.140]   - Yes, for Jesus.
[03:09:01.140 --> 03:09:04.820]   - We have to do 10 or 15 experiences in a place
[03:09:04.820 --> 03:09:07.660]   before we're writing. - You're breaking the name.
[03:09:07.660 --> 03:09:08.820]   - It pays off on that, yeah.
[03:09:08.820 --> 03:09:09.980]   - Yeah, so it's really-- - But I think what's working
[03:09:09.980 --> 03:09:13.580]   to me is that it's a combination of content
[03:09:13.580 --> 03:09:16.900]   and community and expertise.
[03:09:16.900 --> 03:09:17.740]   - Yeah.
[03:09:17.740 --> 03:09:20.780]   - But it doesn't come off as traditional content, right?
[03:09:20.780 --> 03:09:21.620]   - No.
[03:09:21.620 --> 03:09:22.940]   - But certainly the visit is content.
[03:09:22.940 --> 03:09:25.100]   You're bringing all kinds of value
[03:09:25.100 --> 03:09:26.420]   and interesting stuff to people.
[03:09:26.420 --> 03:09:28.700]   - And I think the key is it's their lifestyle.
[03:09:28.700 --> 03:09:30.220]   It's more than a business.
[03:09:30.220 --> 03:09:32.140]   - It's how they live.
[03:09:32.140 --> 03:09:34.060]   - Right. - And I think great artists
[03:09:34.060 --> 03:09:36.820]   and great creators, there's no distinction
[03:09:36.820 --> 03:09:39.980]   between their lives and their work.
[03:09:39.980 --> 03:09:42.260]   - There's a Google+ dimension to this story,
[03:09:42.260 --> 03:09:45.180]   which is that we were doing this kind of stuff,
[03:09:45.180 --> 03:09:46.620]   meeting all these amazing people,
[03:09:46.620 --> 03:09:48.820]   being lucky enough to be invited
[03:09:48.820 --> 03:09:50.860]   to all these incredible foodie events
[03:09:50.860 --> 03:09:53.660]   and wine tastings in various parts of the world.
[03:09:53.660 --> 03:09:56.660]   And it was Google+ people who said, kept telling us,
[03:09:56.660 --> 03:09:58.900]   "Wow, I wish I could join you."
[03:09:58.900 --> 03:10:01.100]   And at some point in my life, I said,
[03:10:01.100 --> 03:10:02.380]   "Wait a minute, why can't they join us?
[03:10:02.380 --> 03:10:04.500]   "What if we did like, what if we took the things
[03:10:04.500 --> 03:10:06.740]   "that we might do in a three month period
[03:10:06.740 --> 03:10:08.740]   "and do it in one week?"
[03:10:08.740 --> 03:10:10.100]   - It is like that, by the way.
[03:10:10.100 --> 03:10:13.140]   It is go, go, go all day.
[03:10:13.140 --> 03:10:15.260]   - And so that's the idea behind it.
[03:10:15.260 --> 03:10:18.540]   And again, it just does not feel, as you can imagine,
[03:10:18.540 --> 03:10:19.980]   it does not feel like work.
[03:10:19.980 --> 03:10:20.820]   - Yeah.
[03:10:20.820 --> 03:10:22.220]   - You're having a great time.
[03:10:22.220 --> 03:10:23.740]   Amir is having a great time.
[03:10:23.740 --> 03:10:24.580]   - Yeah.
[03:10:24.580 --> 03:10:26.740]   So really, it's how they live.
[03:10:26.740 --> 03:10:29.700]   And they're able to make a living on how they live,
[03:10:29.700 --> 03:10:30.540]   which is great.
[03:10:30.540 --> 03:10:31.380]   - All right.
[03:10:31.380 --> 03:10:33.420]   Stacey, you need to get out of here or what's going on?
[03:10:33.420 --> 03:10:35.460]   - I was like, I have four minutes, I'm sorry.
[03:10:35.460 --> 03:10:36.820]   - Stacey on IOT.com.
[03:10:36.820 --> 03:10:37.900]   - I didn't tell you all this before.
[03:10:37.900 --> 03:10:39.420]   - @gigastacey, get out of here.
[03:10:39.420 --> 03:10:42.180]   Stacey on IOT.com, subscribe to our newsletter.
[03:10:42.180 --> 03:10:44.660]   Listen to the podcast with Kevin Tofel.
[03:10:44.660 --> 03:10:45.740]   She'll be back next week.
[03:10:45.740 --> 03:10:47.020]   Thank you, thank you.
[03:10:47.020 --> 03:10:49.580]   - Next week, I want to hear about the bird, but go.
[03:10:49.580 --> 03:10:50.740]   - Okay, yes, bye.
[03:10:50.740 --> 03:10:51.740]   - Bye.
[03:10:51.740 --> 03:10:52.980]   - And I want to thank you, Mike,
[03:10:52.980 --> 03:10:56.180]   because you are very generous,
[03:10:56.180 --> 03:10:59.340]   has taken my son, a salt hank under your wing.
[03:10:59.340 --> 03:11:01.460]   He's going to Atta Oaxaca next month.
[03:11:01.460 --> 03:11:02.300]   - Yeah, which is amazing.
[03:11:02.300 --> 03:11:04.180]   - We are so excited about this.
[03:11:04.180 --> 03:11:05.460]   - I am thrilled.
[03:11:05.460 --> 03:11:07.300]   - He's gonna make a whole chapter of his cookbook
[03:11:07.300 --> 03:11:08.620]   is gonna be about Oaxaca.
[03:11:08.620 --> 03:11:13.660]   - I think that when he's really experienced Oaxaca,
[03:11:13.660 --> 03:11:15.060]   it's gonna be more than one chapter.
[03:11:15.060 --> 03:11:16.780]   - Yeah, I wouldn't be surprised, yeah.
[03:11:16.780 --> 03:11:19.180]   - But it's, we're really excited about it
[03:11:19.180 --> 03:11:20.380]   and we're gonna have so much fun.
[03:11:20.380 --> 03:11:22.540]   Unfortunately, he's gonna be there for a month.
[03:11:22.540 --> 03:11:24.580]   We're gonna be there for the first week.
[03:11:24.580 --> 03:11:25.420]   - That's planning.
[03:11:25.420 --> 03:11:28.420]   - We have to run into another thing, but we'll introduce
[03:11:28.420 --> 03:11:30.260]   to everybody we can during that week.
[03:11:30.260 --> 03:11:32.220]   So we're really looking forward to this.
[03:11:32.220 --> 03:11:35.340]   - Yeah, I think that's completely fine
[03:11:35.340 --> 03:11:37.540]   because it really needs to be him on his own anyway.
[03:11:37.540 --> 03:11:42.540]   And you're introducing him to Chef Alex Alahandar Ruiz,
[03:11:42.540 --> 03:11:45.260]   who's the greatest chef and ambassador
[03:11:45.260 --> 03:11:46.940]   of Oaxaca and cuisine in the world.
[03:11:46.940 --> 03:11:50.060]   And he's gonna be, I mean, Henry's gonna,
[03:11:50.060 --> 03:11:51.540]   this is fantastic.
[03:11:51.540 --> 03:11:53.340]   He is so grateful and I am so grateful.
[03:11:53.340 --> 03:11:54.180]   Thank you.
[03:11:54.180 --> 03:11:55.700]   - And in return, it's our pleasure.
[03:11:55.700 --> 03:12:00.380]   - Let me plug your son, HelloChatterbox.com.
[03:12:00.380 --> 03:12:01.220]   This is where you go.
[03:12:01.220 --> 03:12:03.580]   You heard Mike talking about this smart speaker
[03:12:03.580 --> 03:12:06.620]   that teaches AI literacy to kids.
[03:12:06.620 --> 03:12:08.420]   - That's right.
[03:12:08.420 --> 03:12:09.660]   It's a kit.
[03:12:09.660 --> 03:12:12.860]   You build the smart speaker using cardboard
[03:12:12.860 --> 03:12:13.660]   and include it on the camera.
[03:12:13.660 --> 03:12:15.260]   - Who's into an art now too?
[03:12:15.260 --> 03:12:16.100]   That's awesome.
[03:12:16.100 --> 03:12:17.780]   - Oh yes, absolutely.
[03:12:17.780 --> 03:12:20.340]   And chat GPT is coming soon.
[03:12:20.340 --> 03:12:23.180]   And so, but kids, the important thing is that
[03:12:23.180 --> 03:12:24.580]   it demystifies AI.
[03:12:24.580 --> 03:12:26.420]   So they build the smart speaker
[03:12:26.420 --> 03:12:27.860]   and then it doesn't do anything
[03:12:27.860 --> 03:12:29.940]   until they teach it specific skills.
[03:12:29.940 --> 03:12:34.540]   And potentially it can do more than an Amazon Echo
[03:12:34.540 --> 03:12:36.100]   or that Siri can do.
[03:12:36.100 --> 03:12:37.020]   It can turn on the light.
[03:12:37.020 --> 03:12:38.420]   It can do a million things.
[03:12:38.420 --> 03:12:41.540]   But it only does the specific individual skills
[03:12:41.540 --> 03:12:42.780]   that kids teach it.
[03:12:42.780 --> 03:12:45.180]   And they can teach it three skills.
[03:12:45.180 --> 03:12:47.260]   They can teach it 3,000 skills.
[03:12:47.260 --> 03:12:48.940]   It's up to their imagination.
[03:12:48.940 --> 03:12:52.900]   And it uses AI as it teaches them computer science content,
[03:12:52.900 --> 03:12:56.100]   computer science knowledge,
[03:12:56.100 --> 03:13:00.380]   STEM skills, language skills,
[03:13:00.380 --> 03:13:02.660]   because you have to articulate,
[03:13:02.660 --> 03:13:04.300]   it's a speech conversation
[03:13:04.300 --> 03:13:06.900]   and you design the conversation as the user.
[03:13:06.900 --> 03:13:08.260]   And so it's just a brilliant thing.
[03:13:08.260 --> 03:13:10.540]   And he's putting in a lot of schools now.
[03:13:10.540 --> 03:13:12.380]   It's, he's in hundreds of schools now.
[03:13:12.380 --> 03:13:16.620]   So it's really educational revolution we believe.
[03:13:16.620 --> 03:13:18.460]   He believes and I believe it too.
[03:13:18.460 --> 03:13:22.340]   And so yeah, if you, you can also buy it as an individual,
[03:13:22.340 --> 03:13:25.420]   but it might be showing up in a school near you.
[03:13:25.420 --> 03:13:27.580]   So, so cool.
[03:13:27.580 --> 03:13:30.180]   Hello chatterbox.com.
[03:13:30.180 --> 03:13:31.020]   Thank you, Mike.
[03:13:31.020 --> 03:13:31.940]   It's so great to see you.
[03:13:31.940 --> 03:13:32.780]   And thank you for that.
[03:13:32.780 --> 03:13:33.660]   Great to see you too.
[03:13:33.660 --> 03:13:35.460]   Have a wonderful time.
[03:13:35.460 --> 03:13:37.100]   What country you're going to next?
[03:13:37.100 --> 03:13:38.780]   You said Central America?
[03:13:38.780 --> 03:13:41.980]   Well El Salvador is the next country we're going to
[03:13:41.980 --> 03:13:43.380]   and we're going to Kevin and Nadia
[03:13:43.380 --> 03:13:44.420]   and Alex are going with us.
[03:13:44.420 --> 03:13:47.820]   And so we're all going to go spend some time there.
[03:13:47.820 --> 03:13:49.140]   That's where Amir was born.
[03:13:49.140 --> 03:13:50.060]   So it's her native one.
[03:13:50.060 --> 03:13:51.060]   That's right. Yeah.
[03:13:51.060 --> 03:13:53.900]   Yeah. She has cousins and stuff and ants and so on.
[03:13:53.900 --> 03:13:56.460]   So it's, it's a, it's a cool place.
[03:13:56.460 --> 03:13:57.820]   And so we're looking forward to that.
[03:13:57.820 --> 03:13:58.820]   That's next week.
[03:13:58.820 --> 03:13:59.620]   Fun.
[03:13:59.620 --> 03:14:00.900]   Have a wonderful time.
[03:14:00.900 --> 03:14:02.260]   Thank you so much for being here.
[03:14:02.260 --> 03:14:03.180]   I really appreciate it.
[03:14:03.180 --> 03:14:04.500]   It's always great to see you, Mike.
[03:14:04.500 --> 03:14:05.340]   And thanks for having me.
[03:14:05.340 --> 03:14:06.180]   I appreciate that.
[03:14:06.180 --> 03:14:07.020]   Thank you for having me.
[03:14:07.020 --> 03:14:08.380]   I really appreciate that.
[03:14:08.380 --> 03:14:09.740]   Jeff Jarvis.
[03:14:09.740 --> 03:14:10.900]   Thank you for being here.
[03:14:10.900 --> 03:14:13.340]   You're the longest goodbye.
[03:14:13.340 --> 03:14:15.140]   Yo, Koot you.
[03:14:15.140 --> 03:14:17.140]   Jeff is the director of the Townite Center
[03:14:17.140 --> 03:14:22.460]   for Entrepreneurial Journalism at the Craig Newmark Graduate
[03:14:22.460 --> 03:14:27.020]   School of Journalism at the City University of New York.
[03:14:27.020 --> 03:14:28.780]   Reminds me, I got a call crack.
[03:14:28.780 --> 03:14:30.180]   Buzzmachine.com.
[03:14:30.180 --> 03:14:32.740]   He's unmasked on social at Jeff Jarvis.
[03:14:32.740 --> 03:14:35.100]   Do you spend any time posting on Twitter anymore
[03:14:35.100 --> 03:14:37.020]   or is that you split your time?
[03:14:37.020 --> 03:14:37.540]   A little bit.
[03:14:37.540 --> 03:14:38.540]   Maybe the headlines.
[03:14:38.540 --> 03:14:39.500]   I just put them in both places.
[03:14:39.500 --> 03:14:42.620]   I have to say, I had to use Twitter to keep up on chat GPT.
[03:14:42.620 --> 03:14:44.100]   It really was happening mostly on Twitter.
[03:14:44.100 --> 03:14:44.940]   Oh, interesting.
[03:14:44.940 --> 03:14:46.900]   Same thing with Silicon Valley Bank
[03:14:46.900 --> 03:14:50.900]   because only on Twitter was Jason Calicanis
[03:14:50.900 --> 03:14:52.100]   tweeting with his air on phone.
[03:14:52.100 --> 03:14:52.740]   All caps.
[03:14:52.740 --> 03:14:54.060]   All in caps.
[03:14:54.060 --> 03:14:59.180]   So I've had to visit, but I leave no trace.
[03:14:59.180 --> 03:15:00.500]   I am a good ghost.
[03:15:00.500 --> 03:15:01.020]   I move in.
[03:15:01.020 --> 03:15:05.620]   I go in and I leave without saying hello to anybody.
[03:15:05.620 --> 03:15:06.860]   Thank you all for joining us.
[03:15:06.860 --> 03:15:09.860]   We do this week in Google every Wednesday.
[03:15:09.860 --> 03:15:13.780]   I hope you'll join my mom and make it a weekly thing.
[03:15:13.780 --> 03:15:14.940]   She loves it.
[03:15:14.940 --> 03:15:15.860]   Hi, mom.
[03:15:15.860 --> 03:15:17.660]   She loves it.
[03:15:17.660 --> 03:15:19.380]   Just go to if you want to watch live.
[03:15:19.380 --> 03:15:24.420]   It's Wednesday's from about 2 p.m. Pacific 5 p.m.
[03:15:24.420 --> 03:15:27.900]   Eastern term about their bouts.
[03:15:27.900 --> 03:15:31.740]   And now, of course, because we're in summertime here in the US,
[03:15:31.740 --> 03:15:35.820]   we have to say 2100 UTC.
[03:15:35.820 --> 03:15:37.860]   The live stream is alive.twit.tv.
[03:15:37.860 --> 03:15:42.420]   There's audio and video chat with us in IRC at IRC.twit.tv.
[03:15:42.420 --> 03:15:46.980]   And Discord, if you are a Discordian, a club member.
[03:15:46.980 --> 03:15:51.100]   You can also get copies of the show after the fact.
[03:15:51.100 --> 03:15:53.220]   Oh boy, big news just breaking.
[03:15:53.220 --> 03:15:54.460]   Thank you.
[03:15:54.460 --> 03:15:57.460]   The Biden administration demands TikTok he sold.
[03:15:57.460 --> 03:16:00.140]   [MUSIC PLAYING]
[03:16:00.140 --> 03:16:02.140]   [DING]
[03:16:02.140 --> 03:16:03.460]   Ah!
[03:16:03.460 --> 03:16:04.140]   Wow.
[03:16:04.140 --> 03:16:07.180]   I didn't know we had a breaking news thing.
[03:16:07.180 --> 03:16:08.340]   I like the low-ficed--
[03:16:08.340 --> 03:16:09.540]   Stacey's typing, right?
[03:16:09.540 --> 03:16:10.020]   Yeah.
[03:16:10.020 --> 03:16:11.060]   I like the low-ficed--
[03:16:11.060 --> 03:16:11.540]   Thank you.
[03:16:11.540 --> 03:16:12.820]   I asked for it, and we got it.
[03:16:12.820 --> 03:16:13.740]   Is that Anthony Nielsen?
[03:16:13.740 --> 03:16:14.500]   Who is it?
[03:16:14.500 --> 03:16:15.460]   Is that Anthony?
[03:16:15.460 --> 03:16:15.980]   Breaking.
[03:16:15.980 --> 03:16:16.700]   Do that again.
[03:16:16.700 --> 03:16:17.820]   That was good.
[03:16:17.820 --> 03:16:19.620]   That was awesome.
[03:16:19.620 --> 03:16:22.900]   Ladies and gentlemen, this just in, breaking news.
[03:16:22.900 --> 03:16:24.900]   [DING]
[03:16:24.900 --> 03:16:26.900]   [DING]
[03:16:26.900 --> 03:16:28.580]   [LAUGHING]
[03:16:28.580 --> 03:16:29.820]   It's Stacey.
[03:16:29.820 --> 03:16:32.460]   That's fantastic.
[03:16:32.460 --> 03:16:36.540]   The Biden administration-- this just came in minutes ago--
[03:16:36.540 --> 03:16:40.820]   demands that TikTok be sold or risk a nationwide ban.
[03:16:40.820 --> 03:16:43.540]   This is from NPR.
[03:16:43.540 --> 03:16:45.020]   It is unclear whether federal officials
[03:16:45.020 --> 03:16:47.740]   have given TikTok a deadline to find a buyer, regardless--
[03:16:47.740 --> 03:16:48.260]   A major--
[03:16:48.260 --> 03:16:52.820]   Your journal says threatens ban if they don't sell.
[03:16:52.820 --> 03:16:55.460]   Wow.
[03:16:55.460 --> 03:16:56.420]   OK.
[03:16:56.420 --> 03:16:59.460]   I bet you will be talking about that next week.
[03:16:59.460 --> 03:17:00.700]   I hope you will stop by.
[03:17:00.700 --> 03:17:03.900]   On-demand versions of the show at Twit.tv/Twig.
[03:17:03.900 --> 03:17:05.380]   You can also watch us on YouTube.
[03:17:05.380 --> 03:17:07.780]   There's not only a live stream during the show.
[03:17:07.780 --> 03:17:12.260]   There's an after-the-fact YouTube version on the This Week
[03:17:12.260 --> 03:17:13.780]   in Google feed.
[03:17:13.780 --> 03:17:18.140]   There's also, of course, the best way, probably on every podcast
[03:17:18.140 --> 03:17:20.180]   catcher, every podcast player.
[03:17:20.180 --> 03:17:21.420]   Just search for Twig.
[03:17:21.420 --> 03:17:22.700]   You should find us very quickly.
[03:17:22.700 --> 03:17:23.220]   Subscribe.
[03:17:23.220 --> 03:17:26.380]   You'll get it the minute it's available.
[03:17:26.380 --> 03:17:27.980]   Thank you, everybody, for being here.
[03:17:27.980 --> 03:17:29.700]   Have a great week.
[03:17:29.700 --> 03:17:32.700]   I'll see you Sunday for Ask the Tech Guys.
[03:17:32.700 --> 03:17:34.500]   And we'll see you next week on This Week in Google.
[03:17:34.500 --> 03:17:35.940]   Bye-bye.
[03:17:35.940 --> 03:17:37.100]   Hey, what's going on, everybody?
[03:17:37.100 --> 03:17:40.180]   I am Ant Perot, and I am the host of Hands on Photography
[03:17:40.180 --> 03:17:42.500]   here on Twit.tv.
[03:17:42.500 --> 03:17:45.020]   I know you got yourself a fancy smartphone.
[03:17:45.020 --> 03:17:47.420]   You got yourself a fancy camera.
[03:17:47.420 --> 03:17:49.860]   But your pictures are still lacking.
[03:17:49.860 --> 03:17:52.740]   Can't quite figure out what the heck shutter speed means.
[03:17:52.740 --> 03:17:53.620]   Watch my show.
[03:17:53.620 --> 03:17:54.940]   I got you covered.
[03:17:54.940 --> 03:17:59.020]   Want to know more about just the ISO and exposure
[03:17:59.020 --> 03:18:00.180]   triangle in general?
[03:18:00.180 --> 03:18:01.940]   Yeah, I got you covered.
[03:18:01.940 --> 03:18:05.020]   Or if you got all of that down, you want to get into lighting.
[03:18:05.020 --> 03:18:06.620]   You know, making things look better
[03:18:06.620 --> 03:18:08.700]   by changing the lights around it.
[03:18:08.700 --> 03:18:10.100]   I got you covered on that, too.
[03:18:10.100 --> 03:18:11.380]   So check us out.
[03:18:11.380 --> 03:18:13.300]   Each and every Thursday here on the network,
[03:18:13.300 --> 03:18:17.620]   go to twit.tv/hop and subscribe today.
[03:18:17.620 --> 03:18:20.860]   [MUSIC PLAYING]
[03:18:20.860 --> 03:18:24.220]   [MUSIC PLAYING]
[03:18:24.220 --> 03:18:26.800]   (upbeat music)

