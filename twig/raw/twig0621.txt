;FFMETADATA1
title=Put Another Lettuce on the Barbie
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2021-07-22
track=621
language=English
genre=Podcast
comment=NSO spyware, Jeff Bezos' rocket, Tweetdeck revamp, ASUS CX9
encoded_by=Uniblab 5.3
date=2021
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.040]   It's time for Twig this week at Google. Jeff Jarvis is here. Stacey Higginbotham's here.
[00:00:05.040 --> 00:00:11.200]   Mr. Ant Pruitt's here and I'm back from Hawaii. We got lots to talk about including the NSO
[00:00:11.200 --> 00:00:18.320]   spyware revelation. The asset affects Google as well. Why you may want to avoid dark mode.
[00:00:18.320 --> 00:00:24.880]   How to hack your tweet deck. And why does Jeff Bezos' rocket ship look like that?
[00:00:25.760 --> 00:00:33.040]   It's all coming up next on Twig. Podcasts you love.
[00:00:33.040 --> 00:00:37.040]   From people you trust. This is Twig.
[00:00:37.040 --> 00:00:49.760]   This is Twig. This week in Google, episode 621 recorded Wednesday, July 21st, 2021.
[00:00:50.480 --> 00:00:57.680]   Put another lettuce on the bobby. It's time for Twig this week in geek. I'm going to rename it.
[00:00:57.680 --> 00:01:01.360]   We never talk about Google. Here we do.
[00:01:01.360 --> 00:01:06.320]   Ostensibly this week in Google, the show where we really talk about whatever we feel like.
[00:01:06.320 --> 00:01:11.440]   Generally internet stuff. Stacey Higginbotham is here. She's the IOT and
[00:01:11.440 --> 00:01:17.920]   CPU expert on the show Stacey on IOT.com. Good to see you Stacey in the Pacific Northwest.
[00:01:18.800 --> 00:01:19.760]   Good to be here.
[00:01:19.760 --> 00:01:20.560]   Son is sure.
[00:01:20.560 --> 00:01:21.360]   Is it in your back?
[00:01:21.360 --> 00:01:26.320]   Yes. Thank you to the Jeff Jarvis who both celebrated his birthday and
[00:01:26.320 --> 00:01:29.600]   hosted the show last Wednesday. Hi Jeff Jarvis.
[00:01:29.600 --> 00:01:33.840]   Good to have you back. I was honored to sit in the virtual chair.
[00:01:33.840 --> 00:01:34.640]   The virtual chair.
[00:01:34.640 --> 00:01:35.440]   I just grew up.
[00:01:35.440 --> 00:01:36.640]   Well you didn't get to sit in the virtual chair.
[00:01:36.640 --> 00:01:37.840]   He did a good job Leo.
[00:01:37.840 --> 00:01:38.880]   He did.
[00:01:38.880 --> 00:01:43.680]   Oh good. I'm glad he of course I knew he would. He's a college professor.
[00:01:43.680 --> 00:01:47.200]   He's a man of many degrees. He's a storied
[00:01:48.000 --> 00:01:51.040]   no just TV guide critic. He was what's called a bum.
[00:01:51.040 --> 00:01:57.280]   He is the I don't have the card so I'm just a damping at this point.
[00:01:57.280 --> 00:02:00.640]   He is the Leonard Taill Professor for journalistic innovation at the
[00:02:00.640 --> 00:02:04.880]   Great New York New York.
[00:02:04.880 --> 00:02:10.240]   Graduate School of Journalism at the City University of New York New York.
[00:02:10.240 --> 00:02:15.040]   And that's an pruit providing our
[00:02:15.760 --> 00:02:16.760]   my goodness.
[00:02:16.760 --> 00:02:18.320]   Art is a real tender.
[00:02:18.320 --> 00:02:19.360]   He's wearing a tie.
[00:02:19.360 --> 00:02:23.520]   Mostly because you gave him a hard time Jeff for wearing a
[00:02:23.520 --> 00:02:24.000]   tie. I did.
[00:02:24.000 --> 00:02:25.440]   Tech News Weekly last week.
[00:02:25.440 --> 00:02:26.720]   I was hurt. I was hurt.
[00:02:26.720 --> 00:02:28.160]   You know where what are we?
[00:02:28.160 --> 00:02:29.680]   Shop delivery doesn't dress up for us.
[00:02:29.680 --> 00:02:32.720]   I see him on the next show and there he is all dressed up.
[00:02:32.720 --> 00:02:34.160]   Some geeze.
[00:02:34.160 --> 00:02:35.440]   We're friends Jeff.
[00:02:35.440 --> 00:02:36.160]   You should be hurt.
[00:02:36.160 --> 00:02:38.880]   You should be honored that he's hanging with us.
[00:02:38.880 --> 00:02:41.280]   That was all for Mr. Sergeant.
[00:02:41.280 --> 00:02:41.280]   Yeah.
[00:02:41.280 --> 00:02:43.280]   You know I love Mr. Micah Sergeant.
[00:02:43.280 --> 00:02:49.520]   Netman walks around here and he's always looking so sharp and he makes me feel really really old.
[00:02:49.520 --> 00:02:50.240]   For us he's
[00:02:50.240 --> 00:02:52.640]   answering his slippers today.
[00:02:52.640 --> 00:02:59.440]   That's why by the way no I learned you call them slip not flip flops slippers slippers
[00:02:59.440 --> 00:03:00.880]   slip.
[00:03:00.880 --> 00:03:01.360]   Slippers.
[00:03:01.360 --> 00:03:02.080]   You're good.
[00:03:02.080 --> 00:03:03.360]   Slippers.
[00:03:03.360 --> 00:03:07.680]   Anyway hello and how did everything go?
[00:03:07.680 --> 00:03:10.160]   Well in my absence Jeff you did a great job.
[00:03:10.160 --> 00:03:12.080]   Happy belated happy birthday to you.
[00:03:12.080 --> 00:03:14.000]   There was only one more panic.
[00:03:14.000 --> 00:03:14.640]   Only one.
[00:03:14.640 --> 00:03:15.600]   Yeah I think that was one.
[00:03:15.600 --> 00:03:15.840]   Yeah.
[00:03:15.840 --> 00:03:16.480]   Alright what was that?
[00:03:16.480 --> 00:03:16.960]   Yeah.
[00:03:16.960 --> 00:03:23.520]   I'll be curious what kind of reaction we get to the big story of the week which is
[00:03:23.520 --> 00:03:26.320]   the NSO spyware.
[00:03:26.320 --> 00:03:30.240]   Of course it was a big story on Mac break weekly because iPhones were impacted.
[00:03:30.240 --> 00:03:34.800]   Actually iPhones were more impacted than Google phones but probably mostly because
[00:03:34.800 --> 00:03:39.440]   it was difficult for researchers to tell if the Google phones were hacked.
[00:03:39.440 --> 00:03:45.520]   NSO is a Israeli company that provides well we'll call it spyware.
[00:03:45.520 --> 00:03:46.560]   They call it.
[00:03:46.560 --> 00:03:47.760]   It is what it is.
[00:03:47.760 --> 00:03:54.000]   You know infiltration software designed to get into people's phones
[00:03:54.000 --> 00:04:02.320]   and they claim that they are very careful they have a board and they're very careful
[00:04:02.320 --> 00:04:08.720]   who they sell their software to but an investigation that has been going on for some time
[00:04:09.440 --> 00:04:10.560]   finally went public.
[00:04:10.560 --> 00:04:20.000]   The Pegasus project went public was done by Amnesty International and a NGO
[00:04:20.000 --> 00:04:26.560]   called store what was it I forgot the name forbidden stories a Paris based
[00:04:26.560 --> 00:04:33.760]   journalism nonprofit they got a list of 50,000 phone numbers.
[00:04:33.760 --> 00:04:37.840]   It's clear where this list came from by the way.
[00:04:37.840 --> 00:04:39.920]   Yeah there's no idea from that.
[00:04:39.920 --> 00:04:44.720]   They're a little cagey what they did is they did their research did a lot of research on this
[00:04:44.720 --> 00:04:49.040]   and then passed the story along to the Washington Post and the Guardian and let them write their
[00:04:49.040 --> 00:04:50.080]   stories on it.
[00:04:50.080 --> 00:04:54.880]   But so these phone numbers according to the poster were concentrated in countries known
[00:04:54.880 --> 00:05:01.280]   to engage in surveillance of their citizens but also known to be clients of the NSO group.
[00:05:03.360 --> 00:05:08.400]   It doesn't identify no one identifies where the numbers came from.
[00:05:08.400 --> 00:05:14.720]   It's unknown how many of the phones were targeted or surveilled but forensic analysis
[00:05:14.720 --> 00:05:20.880]   was performed on the 37 smartphones that they were able to get that were hacked
[00:05:20.880 --> 00:05:28.000]   and shows according to the post a tight correlation between timestamps associated with a number on
[00:05:28.000 --> 00:05:33.680]   the list and the initial initiation of surveillance in some cases just a few seconds.
[00:05:33.680 --> 00:05:38.240]   So it sounds like the list was auto generated as the phones were hacked probably then came
[00:05:38.240 --> 00:05:44.240]   from the NSO group and that's why we might assume this 50,000 phone list is a list of all phones
[00:05:44.240 --> 00:05:45.360]   that have been hacked.
[00:05:45.360 --> 00:05:51.440]   The numbers on the list are unattributed but reporters doing a lot of work figured out that
[00:05:51.440 --> 00:05:56.480]   many of them belong to well-known political figures including Emmanuel Macron the
[00:05:56.480 --> 00:06:03.600]   president of France several Arab royal family members 65 business executives 85 human rights
[00:06:03.600 --> 00:06:10.160]   activists 189 journalists and more than 600 politicians and government officials including
[00:06:10.160 --> 00:06:15.120]   cabinet ministers diplomats military and security officers several heads of states and prime
[00:06:15.120 --> 00:06:17.920]   ministers. Any and everybody that you'd want to shut down.
[00:06:19.600 --> 00:06:31.280]   This isn't the first issue NSO back in 2019 the same organization WhatsApp
[00:06:31.280 --> 00:06:38.160]   a company called Citizen Labs it is a research lab at the University of Toronto.
[00:06:38.160 --> 00:06:44.400]   They actually determined that governments were using the NSO software to break into
[00:06:44.400 --> 00:06:45.280]   WhatsApp accounts.
[00:06:47.200 --> 00:06:50.720]   What's scary about this is that the NSO group they collect
[00:06:50.720 --> 00:06:58.320]   holes floss security exploits do not reveal into the company responsible for them which is the
[00:06:58.320 --> 00:07:02.800]   normal way you would disclose these exploits so they could be fixed but instead hold on to them
[00:07:02.800 --> 00:07:09.200]   and affect weaponize them then sell these weaponized hacks to these governments they say we don't
[00:07:09.200 --> 00:07:15.840]   sell it to repressive governments only good governments although at least one set was sold to
[00:07:15.840 --> 00:07:19.280]   Victor Orban the somewhat repressive prime minister.
[00:07:19.280 --> 00:07:21.600]   All of this sounds quite.
[00:07:21.600 --> 00:07:24.640]   He feels pretty repressive.
[00:07:24.640 --> 00:07:30.560]   The scary thing is some of these exploits at least one of the iPhone exploits was a zero
[00:07:30.560 --> 00:07:39.280]   click exploit. So for instance as an example Kishoggi's fiance Kishoggi of course the Saudi
[00:07:39.280 --> 00:07:44.560]   Arabian journalist who was murdered we think by the Saudi Arabians dismembered in the embassy
[00:07:44.960 --> 00:07:49.920]   for writing critical pieces about the Saudi royal family.
[00:07:49.920 --> 00:07:56.560]   Her phone was one of the 37 phones analyzed had been hacked it was an iPhone was an up-to-date
[00:07:56.560 --> 00:08:03.040]   iPhone with the latest version of iOS at the time on it and it had been hacked by a what they
[00:08:03.040 --> 00:08:11.840]   call a zero click. In other words you get a message on iMessage that without your acting
[00:08:11.840 --> 00:08:17.920]   upon it reading it even knowing it's there then compromises your phone and allows
[00:08:17.920 --> 00:08:19.680]   this spyware to turn on the microphone.
[00:08:19.680 --> 00:08:21.600]   Are you not even able to see all the data?
[00:08:21.600 --> 00:08:22.720]   You know the message is there.
[00:08:22.720 --> 00:08:28.240]   In this case she did although it's possible once they compromised the phone to delete the message
[00:08:28.240 --> 00:08:34.160]   so you may not in fact ever know and most of these people did not know that they had been
[00:08:34.160 --> 00:08:39.440]   compromised. There is a tool now that you can download and look at your logs and find out if
[00:08:39.440 --> 00:08:44.160]   you've been compromised but I think most of us don't have to worry they're pretty this looks like
[00:08:44.160 --> 00:08:49.920]   this software is mostly used by nation states to spy on dissidents and other states politicians.
[00:08:49.920 --> 00:09:00.160]   So there most of these phones all but three were iPhones three were android phones although that
[00:09:00.160 --> 00:09:03.920]   don't if you're an android user that shouldn't give you any good feelings.
[00:09:03.920 --> 00:09:08.720]   Mm-hmm. I some shoddenfreude but maybe but maybe not not any feeling of security because
[00:09:08.720 --> 00:09:13.440]   that's only because android phones delete those logs automatically and those logs are what
[00:09:13.440 --> 00:09:18.960]   were used to analyze phones. Does this have to be um how do I ask this um does this have to be
[00:09:18.960 --> 00:09:22.880]   intentionally aimed at a given phone or this is not a mass attack.
[00:09:22.880 --> 00:09:27.040]   Yes. I can't really pass something on and they can clearly target it.
[00:09:27.040 --> 00:09:33.280]   This is not a mass attack. In some cases the uh compromise requires that the spy has to have
[00:09:33.280 --> 00:09:38.720]   access to the network maybe the phone network so that's why it's often used by countries
[00:09:38.720 --> 00:09:43.440]   you know governments who do have access to their uh country's phone network.
[00:09:43.440 --> 00:09:50.480]   It's always targeted and there's actually a I really liked there was a good article
[00:09:51.200 --> 00:09:58.480]   by one of my favorite crypto guys from Johns Hopkins uh Matthew Green who talked about something
[00:09:58.480 --> 00:10:04.800]   called security nihilism. Oh boy. When this comes up a lot of times in fact on Mac Break
[00:10:04.800 --> 00:10:10.560]   Weekly it came up. What are you going to do if a nation state wants to spend millions
[00:10:10.560 --> 00:10:18.480]   and resources to attack a single individual it's almost impossible to defend against them
[00:10:19.680 --> 00:10:25.440]   uh which you know you'll hear that a lot and it is to be fair some I've even said it somewhat
[00:10:25.440 --> 00:10:30.800]   true. I remember a security expert saying look the normal person walking down the street in a
[00:10:30.800 --> 00:10:38.160]   big city is safe but if a team of ninjas decides to attack and prove it there's not much we can do
[00:10:38.160 --> 00:10:43.120]   about that. They better pack a lunch. Yeah yeah yeah. That's because it's gonna take a while.
[00:10:43.120 --> 00:10:48.880]   But there's not much we can do to defend ourselves if we are the target of an attack
[00:10:49.440 --> 00:10:56.720]   Matthew writes that that he says a case against security nihilism. A perverse reaction I've seen
[00:10:56.720 --> 00:11:01.200]   from some security experts is to shrug and say well there's no such thing as perfect security.
[00:11:01.200 --> 00:11:06.400]   More concretely some folks argue this kind of well-resourced targeted attack is fundamentally
[00:11:06.400 --> 00:11:13.600]   impossible to prevent. At the extremes this argument is not wrong. The NSO is not some script
[00:11:13.600 --> 00:11:18.320]   kitty toy deploying it costs hundreds of thousands of dollars and fighting attackers with that
[00:11:18.320 --> 00:11:25.840]   level of resources is always difficult. But this is a never-ending story. Yeah essentially.
[00:11:25.840 --> 00:11:31.360]   Still he says that doesn't mean today's versions of those products Apple and Google
[00:11:31.360 --> 00:11:37.120]   are doesn't mean they're doing everything they could be to stop attacks. There is certainly more
[00:11:37.120 --> 00:11:41.440]   the corporations like Apple and Google could be doing to protect their users but the only way
[00:11:41.440 --> 00:11:46.080]   we're going to get those changes is if we demand them. Well Apple it's a little bit of a cat and
[00:11:46.080 --> 00:11:50.880]   mouse game so Apple is always trying to improve stuff they added something recently called blast
[00:11:50.880 --> 00:11:55.120]   door which is an effective firewall. Here's I'll give you an example the reason Apple gets hit by
[00:11:55.120 --> 00:12:03.360]   this stuff easily. I message uses a variety of programs not all from Apple many of them quite old
[00:12:03.360 --> 00:12:11.520]   to render the media the pictures the audio and the video that come in I messages and those things
[00:12:11.520 --> 00:12:17.280]   are often vectors from attacks we've seen this on on Windows as well as Apple stuff that anything
[00:12:17.280 --> 00:12:25.360]   that interprets data is potentially a compromise about something like Adobe Acrobat. Yeah Acrobat
[00:12:25.360 --> 00:12:33.040]   was the biggest one big constantly a problem so Apple stuff is very hard to fix and very old
[00:12:33.040 --> 00:12:37.280]   they're very dependent it would effectively require a complete rewrite of iMessage.
[00:12:38.320 --> 00:12:42.640]   So what Apple did is they put up a firewall called blast door but according to Matthew Green
[00:12:42.640 --> 00:12:47.600]   and these most recent attacks even blaster was insufficient to protect people against the most
[00:12:47.600 --> 00:12:54.160]   recent attacks many of these attacks were against iPhones with 14.6 iOS 14.6 on them which is not
[00:12:54.160 --> 00:12:58.720]   quite the latest because Apple released a new one yesterday interestingly right at the same time
[00:12:58.720 --> 00:13:08.000]   as a story came out. So Apple needs to do more yes they're doing blast door but that wasn't enough
[00:13:08.000 --> 00:13:12.240]   maybe they do need to rewrite iMessage. Yes that'll be expensive complicated and more
[00:13:12.240 --> 00:13:17.840]   forward to Apple's concern is that it might cause incompatibilities and you know a certain
[00:13:17.840 --> 00:13:24.080]   percentage of people are gonna it's gonna break things he says but we still need to do it and
[00:13:24.080 --> 00:13:29.600]   the thing we need to do is demand that they do a better job. I wonder what that percentage is
[00:13:29.600 --> 00:13:35.680]   because they always have the push to get people on a newer device because certain versions of the
[00:13:35.680 --> 00:13:42.240]   OS won't work on those older devices and nobody seems to fuss why not just continue to go down
[00:13:42.240 --> 00:13:46.160]   that road and say hey we're having some issues security wise and we really want to make sure
[00:13:46.160 --> 00:13:51.200]   everybody's up. The problem is because Shogi's fiance said and others I think have said the same
[00:13:51.200 --> 00:13:56.400]   thing but I was using an iPhone because they're secure oh yeah there's that and a late model.
[00:13:56.400 --> 00:14:04.960]   Apple added like a there's us normal people who might not be worried like I mean if you're in a
[00:14:04.960 --> 00:14:09.360]   certain field like if you're the president of a country or the head of the spy agency or
[00:14:09.360 --> 00:14:16.240]   head of an NGO or an activist you would be interested in paying more for a highly secured phone and
[00:14:16.240 --> 00:14:22.240]   maybe Apple could create a package for those people that's slightly more active would break
[00:14:22.240 --> 00:14:27.520]   some functionality but still and you know it can work on their existing hardware. I'm just trying
[00:14:27.520 --> 00:14:31.680]   to think like because not everybody's going to want to do something because most of us value
[00:14:31.680 --> 00:14:38.560]   convenience far more than security so is there. PC's I mean your rice day see it's if you use
[00:14:38.560 --> 00:14:47.280]   Google's you know hardware thing. It's functionality but it makes thank you for putting the actual
[00:14:47.280 --> 00:14:54.880]   language in there I appreciate that. What's the thing called again? The Titan key?
[00:14:54.880 --> 00:15:04.480]   Thank you the Titan thing. So you lose functionality but you believe you have more
[00:15:04.480 --> 00:15:08.160]   security so the question is what would that equivalent be for people who are vulnerable.
[00:15:08.160 --> 00:15:12.560]   By the way a vulnerable phone Titan keys were compromised at one point so yeah
[00:15:14.000 --> 00:15:19.360]   nothing's but nothing's perfect but try and you got to really push Google and Apple.
[00:15:19.360 --> 00:15:26.080]   It's so important you got to make it basically what Matthew Green's arguing is you got to make it
[00:15:26.080 --> 00:15:32.240]   economically unfeasible for NSO group to continue in business. Right now they can do it because they
[00:15:32.240 --> 00:15:37.440]   can do these masks they can do 50,000 phone numbers. What about the phone companies? They too.
[00:15:39.040 --> 00:15:44.720]   They're absolutely be a perfect example is all the phones including the late model phones we all use
[00:15:44.720 --> 00:15:51.280]   have SS7 in it which is a software defined radio that's used by cell phones. It's hideously
[00:15:51.280 --> 00:15:58.000]   insecure and is well known to be insecure and in fact the security flaws are well known
[00:15:58.000 --> 00:16:06.400]   and and really can't be fixed that they're actually SS7 is not a software it's a set of protocols
[00:16:06.400 --> 00:16:10.880]   but it's a very insecure set of protocols and it's not been fixed.
[00:16:10.880 --> 00:16:18.560]   We have Google doing monthly updates to Android air quote security updates and Microsoft
[00:16:18.560 --> 00:16:23.360]   Thorne errors is that not enough anymore is it safe to say that? Is it asking for you
[00:16:23.360 --> 00:16:32.560]   much to do every it's enough to say send my monthly for sure but if so this is the question if
[00:16:32.560 --> 00:16:39.040]   you're a dissident or Stacy I had to say it a journalist. I mean journalists are just as often.
[00:16:39.040 --> 00:16:43.600]   Oh yeah yeah just I mean well certain kind of like I'm less worried about being hacked.
[00:16:43.600 --> 00:16:50.320]   I don't think the IOT industry is not to get you but but yes no for certain types of journalists.
[00:16:50.320 --> 00:16:57.920]   No there are certain stories that I am very like if I there are certain things when I report
[00:16:57.920 --> 00:17:03.360]   on them or I talk to people at specific companies I I have a set of security protocols that I enact
[00:17:03.360 --> 00:17:09.760]   to talk to them just to make sure I'm not screwing their lives over not like literal lives but just
[00:17:09.760 --> 00:17:16.640]   like so they don't get fired. But you know I mean so so what should Kishoggi's fiance do knowing
[00:17:16.640 --> 00:17:21.760]   that she and her fiance are well that's why I think there's a market for some sort of
[00:17:22.880 --> 00:17:29.440]   I guess. I mean you could use just a burner you could I'm trying to think of all the ways that
[00:17:29.440 --> 00:17:36.000]   people got around this way. Well burners don't have iMessage for one thing. Number two
[00:17:36.000 --> 00:17:42.000]   if you've got a government involved when you know I sat in the editor's office at the Guardian
[00:17:42.000 --> 00:17:46.160]   in the midst of the Stoughton story and they thought they were doing the right thing with
[00:17:46.160 --> 00:17:52.400]   burners and no a preponderance of burners in an area is a great signal. Right and these days by the
[00:17:52.400 --> 00:17:58.800]   way a lot of burners are running old and right. You don't want to waste money. Yeah right.
[00:17:58.800 --> 00:18:08.080]   No it's a it's a bad situation and I guess the details sell what do they what do they say is the
[00:18:08.080 --> 00:18:14.160]   benefit of using it. So how do they with apparent legitimacy say this is a good thing to have.
[00:18:14.160 --> 00:18:21.440]   Oh god you should hear their statement it is it is nauseating it is offensive.
[00:18:21.440 --> 00:18:26.880]   Let me see if I can like and find it because if I got to read this to you because I first of all
[00:18:26.880 --> 00:18:34.880]   I can use a really nerdy voice for it. It is I was I was I was so offended. Let me see.
[00:18:36.160 --> 00:18:42.320]   In a song group. Finally denies false claims made in your report.
[00:18:42.320 --> 00:18:49.040]   Let me see this is anyway they deny it but let me see why they say they're a good thing.
[00:18:49.040 --> 00:18:58.240]   It's a long denial they say we had nothing to do with the murder heinous murder of Jamal Khashoggi
[00:18:58.240 --> 00:19:03.840]   or we can confirm of course offering no evidence that our technology was not used to listen
[00:19:03.840 --> 00:19:07.680]   monitor tracker collect information regarding him as family members mentioned in your inquiry
[00:19:07.680 --> 00:19:12.160]   we previously investigated this claim which again is being made without validation.
[00:19:12.160 --> 00:19:18.000]   So people presented evidence to the contrary NSO group you're saying you're just denying it
[00:19:18.000 --> 00:19:26.160]   without presenting any evidence that it's not so but the worst thing is they're justification.
[00:19:26.160 --> 00:19:32.000]   NSO group technologies have helped prevent terror attacks gun violence car explosions
[00:19:32.000 --> 00:19:37.840]   and suicide bombings. The technologies are also being used every day to break up pedophilia
[00:19:37.840 --> 00:19:44.400]   sex and drug trafficking rings. Oh there we go missing and kidnapped children locate survivors
[00:19:44.400 --> 00:19:50.720]   trapped under collapsed buildings and protect airspace against disruptive penetration by dangerous
[00:19:50.720 --> 00:19:58.640]   drones. So how did that get in there? I don't know how they do this but they are wonderful aren't they?
[00:19:59.920 --> 00:20:05.760]   Simply put NSO group is on a life-saving mission and the company will faithfully execute this
[00:20:05.760 --> 00:20:11.040]   mission undeterred despite any and all continued attempts to discredit it on false grounds.
[00:20:11.040 --> 00:20:18.400]   Well and if you look at I mean it's coming from Israel. Israel is constantly under attack.
[00:20:18.400 --> 00:20:23.120]   They are constantly dealing with they're in a part of the world where everybody hates them so I can see
[00:20:23.920 --> 00:20:31.280]   I'm a little paranoid as justified I understand. From their point of view this is probably helpful.
[00:20:31.280 --> 00:20:35.600]   Their government is probably very excited to be able to keep an eye on other governments.
[00:20:35.600 --> 00:20:40.720]   By the way that's another thing the Matthew Green says is we really got to tell governments
[00:20:40.720 --> 00:20:46.000]   just especially freedom-lovering governments. He said in a perfect world US and European
[00:20:46.000 --> 00:20:51.600]   governments would wake up and realize that arming authoritarianism is really bad for democracy
[00:20:52.480 --> 00:20:57.920]   and that whatever trivial benefit they get from NSO because they use it is vastly outwied by the
[00:20:57.920 --> 00:21:02.400]   very real damage this technology is doing to journalism and democratic governments worldwide.
[00:21:02.400 --> 00:21:08.800]   So that's the argument. The free governments that also use this really got to stop it.
[00:21:08.800 --> 00:21:13.760]   We just got to stop it. This is not he also says I'm not holding my breath for this to happen.
[00:21:17.120 --> 00:21:22.880]   Everybody benefits including our government our intelligence agencies.
[00:21:22.880 --> 00:21:30.640]   It is a shame but these exploits so when flaws in software discovered there really two roots.
[00:21:30.640 --> 00:21:37.040]   There's the White Hats. There's people like Google's Zero Day and Project and Travis Kalanick
[00:21:37.040 --> 00:21:40.240]   who then will do responsible disclosure. They'll call Apple as Apple.
[00:21:40.240 --> 00:21:41.760]   Hey, this is what we found this problem.
[00:21:41.760 --> 00:21:45.760]   Here's it. If you're moving in 90 days to fix it if you don't I'll reveal it.
[00:21:46.560 --> 00:21:51.360]   Apple fixes it. He's then revealed but it's not that threat anymore to people who are updated.
[00:21:51.360 --> 00:21:56.480]   That's the responsible and then there's companies like Zorodium and others that do this.
[00:21:56.480 --> 00:21:59.040]   Trend Micro. They're another one.
[00:21:59.040 --> 00:22:03.680]   Yeah, they're companies actually that pay bug bounties for zero days.
[00:22:03.680 --> 00:22:10.960]   So Zorodium, we've talked about this on them. Security now, Zorodium says we pay big bounties.
[00:22:10.960 --> 00:22:17.200]   Zorodium is the world's leading exploit acquisition platform for advanced zero-day research and
[00:22:17.200 --> 00:22:24.640]   cybersecurity capabilities. So some of these like Zorodium follow this responsible disclosure
[00:22:24.640 --> 00:22:31.360]   premise. There are others like NSO group that go, yeah, we'll give you a million dollars for that hack.
[00:22:31.360 --> 00:22:34.320]   Thank you. And then they sell it along.
[00:22:34.320 --> 00:22:40.560]   And God knows what's coming out of it. I don't mean to demonize but out of Russia and China and
[00:22:41.520 --> 00:22:44.320]   that isn't being sold to the open market.
[00:22:44.320 --> 00:22:48.960]   Yeah. No, in fact, that's the problem.
[00:22:48.960 --> 00:22:54.880]   There's the responsible disclosure and then there's your responsible keeping it to yourself.
[00:22:54.880 --> 00:22:59.520]   You don't know. Yeah. And NSA has plenty of these. They don't want to burn them. We know
[00:22:59.520 --> 00:23:02.240]   after the Snowden revelations, NSA co-opsies as well.
[00:23:02.240 --> 00:23:10.000]   There's two takeaways. One, don't assume that because you're carrying an iPhone,
[00:23:10.000 --> 00:23:16.560]   you're safe. In fact, if you're carrying any smart device, if you're connected to the internet,
[00:23:16.560 --> 00:23:24.560]   or any sort, at any time, bad news, exercise, caution, yes, extreme caution. And that's why
[00:23:24.560 --> 00:23:34.480]   great people like, what's his name, your friend, Dan Patterson. Dan Patterson years,
[00:23:35.360 --> 00:23:41.680]   years past his train dissidents on Oppsec, on how to secure yourself and what not to do.
[00:23:41.680 --> 00:23:49.600]   Dan is quite careful to say the least with all of his travels, especially like at Davos.
[00:23:49.600 --> 00:23:55.360]   I've heard some interesting stories. So that's one thing is, know the risks,
[00:23:55.360 --> 00:24:00.480]   especially if you're a target and do the right thing. The other thing is we all need to put
[00:24:00.480 --> 00:24:07.600]   pressure on companies like Google and Apple and say, you gotta make sure you do everything you
[00:24:07.600 --> 00:24:11.440]   can to prevent this. I think they are. But the thing is, I was going to say, I think Apple has
[00:24:11.440 --> 00:24:18.640]   actually taken this seriously though. They always have, right? It's just they can't necessarily stay
[00:24:18.640 --> 00:24:22.880]   ahead of the best guy. Okay, again, I'm going to quote Matthew Green, because he says,
[00:24:22.880 --> 00:24:29.840]   "They chose the easy inexpensive fix, blast door, which is a firewall, instead of the expensive
[00:24:29.840 --> 00:24:36.800]   complicated and potentially breaking fix of rewriting iMessage to not be using all these renderers."
[00:24:36.800 --> 00:24:43.280]   So I didn't play your role of Davos Advocate last week, but I will this week.
[00:24:43.280 --> 00:24:46.880]   The late host fight. No, no, no, you don't get your chance now.
[00:24:46.880 --> 00:24:57.200]   So the number of people of fairness is Davos Advocate people, so just for the sake of the
[00:24:57.200 --> 00:25:04.160]   discussion. The number of people potentially affected by this, though each individual case
[00:25:04.160 --> 00:25:10.560]   can be profound, is a small number. That's right. And so 50,000 out of the billion and a half
[00:25:10.560 --> 00:25:17.040]   iPhones is nothing. Exactly. So if you look at the economics here of affecting everything you do on
[00:25:17.040 --> 00:25:21.200]   every single phone and potentially having an impact on the functionality of people as they see
[00:25:21.200 --> 00:25:26.960]   behind convenient. There is an equation that occurs there. Like how much does this affect our bottom
[00:25:26.960 --> 00:25:34.960]   line? Yeah. And so there's also the poignant. Yeah. And I get all that. I'm Davos Advocate.
[00:25:34.960 --> 00:25:41.280]   Okay. But it actually goes to your point, Stacy, at some point, I almost think the company should
[00:25:41.280 --> 00:25:47.920]   say, "Okay, here is every person level of security. If you need this level, we're going to provide
[00:25:47.920 --> 00:25:56.480]   them, but you have responsibility to." And that includes not revealing national secrets,
[00:25:56.480 --> 00:26:03.120]   on a cell phone, no matter what. It must be doable because they give the president a secure phone,
[00:26:03.120 --> 00:26:08.960]   a blackberry. It's a blackberry. Was it a blackberry? Is it still a blackberry? Yeah. I know it was
[00:26:08.960 --> 00:26:13.280]   for Obama, but I don't, I mean, we know that President Trump was using an iPhone. I thought
[00:26:13.280 --> 00:26:18.480]   everyone was a iPhone. I thought it was an iPhone after that. I don't know what Biden uses. In fact,
[00:26:18.480 --> 00:26:23.120]   they don't want us to know, right? But usually the way they secure these funds is by eliminating a
[00:26:23.120 --> 00:26:29.440]   lot of capabilities, right? Yeah. The third thing, I didn't, actually, just thought of a third thing,
[00:26:29.440 --> 00:26:34.560]   we should put pressure on our own government not to use these tools and to encourage others,
[00:26:34.560 --> 00:26:42.320]   not to, because ultimately it is bad for freedom. But government is going to justify that. Government
[00:26:42.320 --> 00:26:48.000]   is going to justify that surveillance as a tool of safety, of protectiveness. We're getting pedophiles
[00:26:48.000 --> 00:26:53.120]   here. What back doors to everything? All right. We're stopping drone violence.
[00:26:53.120 --> 00:27:02.240]   I remember years ago, speaking to this company, ProVoro, they make hardware for protecting
[00:27:02.240 --> 00:27:08.320]   iPhone privacy, ProVoro.com. And I remember in that conversation just sort of being blown away,
[00:27:08.320 --> 00:27:12.160]   because all I ever heard was, Hey, the iPhone is the most secure,
[00:27:12.160 --> 00:27:17.120]   phone blah, blah, blah, blah, blah. But yet there's still tools out there trying to protect this
[00:27:17.120 --> 00:27:22.320]   hardware. Now with all of these software exploits, none of this stuff even matters, right? But wait a
[00:27:22.320 --> 00:27:27.920]   minute, all this is, is you take your phone and you put in a fair day, can't just a fair day.
[00:27:27.920 --> 00:27:33.280]   Put the box. And yeah, they had a, they had a couple other things over the years. So,
[00:27:33.280 --> 00:27:37.840]   ooh, and it blocks out the microphones and things like that. Wow, look at that.
[00:27:39.360 --> 00:27:43.200]   Whatever it is, it must be good. It doesn't fit well in your jeans. Oh, yeah.
[00:27:43.200 --> 00:27:49.920]   That's what those compromises, Jeff. You need a body person who carries your phone in their bag.
[00:27:49.920 --> 00:27:56.080]   I like, I think what Dan Patterson used to do and others do now, training people in how to keep
[00:27:56.080 --> 00:28:00.640]   secure is good. I mean, that's another good thing to do. Yeah. Let every let's. And we,
[00:28:00.640 --> 00:28:04.640]   we teach it a lot more in journalism school now. You have to. That's what Stacy said. It's for
[00:28:04.640 --> 00:28:09.280]   the sake of your sources. It's not as much for you at all. Yeah. But for your, for your sources.
[00:28:09.280 --> 00:28:14.160]   He has said that as well. You know, you're a terrible devil's advocate. You just
[00:28:14.160 --> 00:28:21.280]   rolled right over. I'm like, I'm a nice guy. And I just can't, I can't keep it going. You see,
[00:28:21.280 --> 00:28:26.480]   I get a gross, some horns. Rationality, reality,
[00:28:28.560 --> 00:28:34.320]   these are things I recognized. I balanced. I'm a little, I'm a little depressed because I
[00:28:34.320 --> 00:28:43.360]   learned yesterday that a podcast called a call her daddy. Have you heard of this podcast before?
[00:28:43.360 --> 00:28:46.720]   Do I want to have heard of it? It was on Barstool.
[00:28:46.720 --> 00:28:54.080]   She had the host Alex Cooper had a famous fight with the head of Barstool because he only wanted
[00:28:54.080 --> 00:29:00.320]   a pair of half a million dollars a year. So she's now sold it to a pocket, one podcast called
[00:29:00.320 --> 00:29:08.320]   Car Her Daddy sold it to Spotify, Spotify exclusive for a three year contract, 20 million dollars a
[00:29:08.320 --> 00:29:11.440]   year. Oh, wow. One podcast. Wow.
[00:29:11.440 --> 00:29:20.160]   Brunchy talk and something. And female empowerment. Wow. It's, you know, I listen, so after I heard
[00:29:20.160 --> 00:29:25.760]   this news, I went, I picked the wrong business. Wait a minute, that is my business.
[00:29:25.760 --> 00:29:33.040]   It's a dog. I picked the wrong haircut. We just have a different version of this business
[00:29:33.040 --> 00:29:39.360]   and and gender and makeup artists. But it's, you know, it's the influencer. She's young. It's,
[00:29:39.360 --> 00:29:44.480]   uh, it's this Gen Z kind of, I listened to the most recent one. It was like,
[00:29:44.480 --> 00:29:48.800]   just weird. It was like they were, they apparently there was a thunderstorm and the
[00:29:48.800 --> 00:29:53.200]   house power went out and they were all scared. And we've never done anything about
[00:29:53.200 --> 00:30:00.240]   thunderstorms on this show. That was the whole, as the Jarvis house shakes.
[00:30:00.240 --> 00:30:08.560]   Yeah. Here's the Wall Street Journal. Uh, caller daddy used to be best known for
[00:30:08.560 --> 00:30:16.720]   raunchy sex talk and pioneering concepts like the gluck gluck 9000. Do what? I don't even want
[00:30:16.720 --> 00:30:22.160]   to tell you what that is. Uh, apparently more, more recently.
[00:30:22.160 --> 00:30:32.480]   Uh, I don't know. She calls her fans, the daddy gang. Um, it's a basically a woman who talks to
[00:30:32.480 --> 00:30:37.120]   other women and has done a good job doing it and his rank it in the dough. Oh, I'm,
[00:30:37.120 --> 00:30:42.560]   hey, I'm thrilled for her, but I'm just, she's getting more money than, you know,
[00:30:43.520 --> 00:30:49.600]   Shaq. Well, Joe, Joe Rogan got a ton of money. Yeah. I mean, yeah.
[00:30:49.600 --> 00:30:56.480]   Yeah. That puzzles her audience. It does. I'm just, I'm like, look, I, I'm getting a whiff of
[00:30:56.480 --> 00:31:01.040]   this, this young lady is doing this and she has a podcast that we don't know.
[00:31:01.040 --> 00:31:04.480]   It's not because she's a lady. I feel the same way about Joe Rogan. Trust me. It's not.
[00:31:04.480 --> 00:31:13.360]   I do believe he's just, she's just, she's just bizarre. I don't, uh, she claims,
[00:31:13.360 --> 00:31:18.240]   a million downloads a month. Well, then there's, you look at the other side of it. Look at it at the
[00:31:18.240 --> 00:31:24.160]   TV standpoint. What's that show called? The view. Have you ever watched that show? Yeah.
[00:31:24.160 --> 00:31:31.840]   Wildly popular, but they don't get, I guarantee you $20 million a year. I don't dig. I think they do.
[00:31:31.840 --> 00:31:35.760]   I don't know. Maybe a minute. What in the way? No way. In your day when you were a TV guide critic,
[00:31:35.760 --> 00:31:39.360]   how much did a network news anchor? What did you see? What did you see? Yes,
[00:31:39.360 --> 00:31:43.360]   is probably getting a ton of ad money for that show. Oh, I know they are, but I don't think they
[00:31:43.360 --> 00:31:49.360]   get 20 million a year for it. Casey Hunt is getting a reporter for going, be installed on
[00:31:49.360 --> 00:31:54.160]   by CNN. It's getting a reported million a year. A million a year. Okay. Okay.
[00:31:54.160 --> 00:31:59.280]   I would argue these people have more influence. Right? I mean, uh, they're talking to more people
[00:31:59.280 --> 00:32:05.200]   than a million people a month, but you know, so I'm sure Spotify also knows what it's doing.
[00:32:05.200 --> 00:32:10.160]   They've decided to spend half a billion dollars to own podcasting. I think my biggest complaint
[00:32:10.160 --> 00:32:16.240]   isn't that she's getting all this money. It was a little like shocked, but, uh, is that I think it's
[00:32:16.240 --> 00:32:20.400]   bad for podcasting because it's going to be Spotify exclusive. The reason they can give her that kind
[00:32:20.400 --> 00:32:25.440]   of money is because they can go to advertisers that we know exactly who's listening when and,
[00:32:25.440 --> 00:32:29.040]   and all of that. And we can't do that. We don't want to do that. And, uh, it makes it hard for us
[00:32:29.040 --> 00:32:34.720]   to sell advertising, frankly. So, yeah, fortunately, I'm not, I don't think our listeners are in the
[00:32:34.720 --> 00:32:40.160]   same demographic as Alice Cooper. So, that's probably good.
[00:32:40.160 --> 00:32:48.000]   Oh, this says because she this the lat, the kicker on the W. S. J. Story is nice.
[00:32:48.000 --> 00:32:56.000]   It it's saying that basically people, the guy's hate her because she she makes women feel confident.
[00:32:56.000 --> 00:33:02.480]   And you know what? There's a big market for that. Well, yeah, good. That's great. Well, no, I mean,
[00:33:02.480 --> 00:33:08.720]   but like women are a huge market. We spend a lot. We. Oh, yeah. You should get a daddy gang flag
[00:33:08.720 --> 00:33:17.680]   sweatshirt. You know, you should join the club. Listen, I'll tell you what, listen to a podcast
[00:33:17.680 --> 00:33:23.120]   and let me know what you think of it. Because the one I listened to was bizarre. Okay. Now,
[00:33:23.120 --> 00:33:26.480]   I just want to, I just want to so imagine so I heard about this thing called twig.
[00:33:26.480 --> 00:33:31.520]   So I listened to one. Yeah, that's true. And they talk about now, but we're going to talk about
[00:33:31.520 --> 00:33:39.200]   less because I think our mistake, my mistake, I'll take full responsibility was focusing on content,
[00:33:39.200 --> 00:33:46.000]   information, trying to give you news, that kind of thing. Helpful tips, mistake, mistake. So next
[00:33:46.000 --> 00:33:50.880]   week, we're going to lock ourselves in a house without power in a thunderstorm and see what happens.
[00:33:50.880 --> 00:33:55.440]   You have edits, sir. It's going to be so exciting. I'll see you to find all about it.
[00:33:55.440 --> 00:34:01.680]   Power in women. So we can totally do that. Yes. That's why Spotify hired Joe Rogan for a hundred
[00:34:01.680 --> 00:34:07.120]   million dollars. Last. Oh, last week, Leo, you missed it, but I empowered women to embrace their
[00:34:07.120 --> 00:34:14.800]   inner snoring. You did that the week before as well. Oh, there you go. It's a theme here.
[00:34:14.800 --> 00:34:20.880]   All right. Let's move. I'm sorry. I didn't mean to bring up the call her daddy thing. It's just,
[00:34:20.880 --> 00:34:27.520]   I'm still reeling. That's topical. What we call the therapy. It's also the business we're in.
[00:34:27.520 --> 00:34:33.200]   We talked to Spotify about acquiring Twitter at one point would have gone for a little less than
[00:34:33.200 --> 00:34:41.040]   60 million dollars. Just a little. Apparently I'm just not empowering women. So it's okay. It's okay,
[00:34:41.040 --> 00:34:53.120]   sir. Let's move on. The Wall Street Journal. Let's let's do this one here. I'm sure you put this in,
[00:34:53.120 --> 00:35:00.560]   Jeff. I can tell you. How can you tell? The Wall Street Journal has figured out how TikTok
[00:35:00.560 --> 00:35:08.240]   inside TikTok's highly secretive algorithm. Really? I wonder what it could be.
[00:35:08.880 --> 00:35:12.560]   Investigation. It's an investigation.
[00:35:12.560 --> 00:35:17.600]   How? That must be a problem. Profounded. Surprising outcome.
[00:35:17.600 --> 00:35:23.760]   Here's out your deepest desires. The Wall Street Journal created dozens of automated accounts
[00:35:23.760 --> 00:35:31.360]   that watched hundreds of thousands of videos to reveal how the social network knows you so well.
[00:35:32.000 --> 00:35:38.000]   And what did they learn, Jeff? That they know what you watch and give you more of it.
[00:35:38.000 --> 00:35:43.920]   That's it. That's the investigation.
[00:35:43.920 --> 00:35:49.200]   So I cautioned everybody. I put a TikTok on Twitter and I said,
[00:35:49.200 --> 00:35:54.160]   I'm telling you, beware, you watch this cute puppy. You're going to get more of them.
[00:35:54.160 --> 00:35:58.400]   I knew that. I knew that. I know that because I get a lot of bikini clad women.
[00:35:58.400 --> 00:36:01.760]   I'm in power. We've gone over this in the show. We've gone on this. Yeah.
[00:36:01.760 --> 00:36:04.400]   You admitted things. I don't think you should admit.
[00:36:04.400 --> 00:36:12.320]   Actually, if you think about it, really, this is the secret sauce for Google too.
[00:36:12.320 --> 00:36:19.120]   It goes even back before that. I remember interviewing somebody about gaming addiction.
[00:36:19.120 --> 00:36:24.240]   I was talking about World of Warcraft. Going back to early online gaming,
[00:36:24.800 --> 00:36:30.320]   they said World of Warcraft is the is Vegas wishes they had this kind of data.
[00:36:30.320 --> 00:36:36.160]   World of Warcraft, of course, their whole goal is you want to keep you playing longer.
[00:36:36.160 --> 00:36:41.120]   You play the more money they make. The game mechanics is basically you go on raids with other
[00:36:41.120 --> 00:36:46.800]   people and there's loot drops. So they know exactly which loot drops get you coming back for more,
[00:36:46.800 --> 00:36:51.920]   how much time you spent when you left all of that and they don't even have to think about it.
[00:36:51.920 --> 00:36:57.280]   They just tell the algorithm, optimize. That's all YouTube says.
[00:36:57.280 --> 00:37:01.760]   That's complicated compared to TikTok. TikTok is bikinis, puppies.
[00:37:01.760 --> 00:37:07.440]   No, no, it's the same. It's the same because basically you know exactly on TikTok,
[00:37:07.440 --> 00:37:12.240]   you know exactly how they want long they watch before you swipe up right in the next video.
[00:37:12.240 --> 00:37:16.480]   So that was 0.3 seconds. We know exactly what content was in that 0.3 seconds.
[00:37:16.480 --> 00:37:21.440]   Do it again. Do it again. Do it again. They know what you watch the longest.
[00:37:22.240 --> 00:37:24.720]   They watch the two seconds. So let's feed this to one.
[00:37:24.720 --> 00:37:28.400]   Increase that and it's a feedback loop. That's the other beauty of it.
[00:37:28.400 --> 00:37:32.720]   So we can tweak this. That worked. That worked. But they don't even, it's make it so much.
[00:37:32.720 --> 00:37:39.280]   I go as I swipe up on bikini pictures as fast as I possibly can.
[00:37:39.280 --> 00:37:41.440]   Same here. No, no, no, no. I'm not that kind of man.
[00:37:41.440 --> 00:37:44.000]   No, no, no, no, no. By the way, I keep trying.
[00:37:44.000 --> 00:37:45.680]   After you mentioned, I keep trying.
[00:37:48.240 --> 00:37:51.840]   After you, I never knew what underbroop was until at TikTok. Oh boy.
[00:37:51.840 --> 00:37:54.960]   After. Can you edit that out? Oh God.
[00:37:54.960 --> 00:38:03.680]   After. I'm learning about the youngs. I'm learning about the youngs.
[00:38:03.680 --> 00:38:08.480]   So I tried your technique. I swipe up very fast and then you're right.
[00:38:08.480 --> 00:38:13.120]   I got rid of that now for some reason. I get a lot of home construction videos.
[00:38:14.560 --> 00:38:20.720]   I get that. All they know about you is you're a guy. So they're trying to get you oriented.
[00:38:20.720 --> 00:38:23.440]   Oh, that's it. They know my gender. I watch ancient.
[00:38:23.440 --> 00:38:26.960]   You can even shoot. Nope. Nope. Didn't work for naked women. Let's try hard hats.
[00:38:26.960 --> 00:38:29.920]   Home construction. Yeah. Technology will be somewhere in there.
[00:38:29.920 --> 00:38:32.640]   You'll start getting. I get server stuff every now and then.
[00:38:32.640 --> 00:38:37.920]   I think you heard my accent. So it started serving up a bunch of people from the South
[00:38:37.920 --> 00:38:40.720]   cat shoot going out cat fishing and stuff like that.
[00:38:40.720 --> 00:38:45.360]   Really you get cat fishing. I deleted it because I've just insulted by.
[00:38:45.360 --> 00:38:49.040]   Oh geez. I haven't deleted it yet. I've still been experimenting.
[00:38:49.040 --> 00:38:54.320]   I put one out the other day. You're doing it. Just just in the just in the
[00:38:54.320 --> 00:39:02.400]   experiment. I didn't put a hashtag. I put it. I put it online 170 just because only have like 50
[00:39:02.400 --> 00:39:05.680]   followers or something crazy. Wait, who are you? I'm going to follow you right now, dude.
[00:39:05.680 --> 00:39:10.400]   It's not worth following. I promise you. No, it's worth it. It's worth it. I'm doing it right now.
[00:39:10.880 --> 00:39:18.480]   I've talked to a lot of people who don't use TikTok anymore on their phone because it's just too
[00:39:18.480 --> 00:39:23.200]   addictive. You wake up at three in the morning and you're drooling on your phone because you
[00:39:23.200 --> 00:39:27.920]   fill a sleep watch at TikTok. See, I don't have that problem. I only put it on there. I do.
[00:39:27.920 --> 00:39:35.600]   I put it on there to support Queen Pruitt with her plant acid potheads. But I still don't quite get
[00:39:35.600 --> 00:39:39.840]   it. I have about I want to say it's like 50 followers. But the videos that I put up are
[00:39:39.840 --> 00:39:47.760]   53. 53. Well, we can increase that. Let's fix that. What line was it? 170 is right below my picks.
[00:39:47.760 --> 00:39:53.280]   Oh, that's where it is way down there. Oh, you just passed it. Way to low. See, I'm watching a delightful
[00:39:53.280 --> 00:40:00.880]   dog right now licking up all the ingredients. This video in a. This is a ant underscore
[00:40:00.880 --> 00:40:06.160]   proof which is your handle everywhere else. Now, this video has got like 500 views.
[00:40:06.160 --> 00:40:11.600]   Are you just staring at me? Listen. TikTok.
[00:40:11.600 --> 00:40:21.280]   That's all I did. You got 500 views. Kind of creep me out there, actually.
[00:40:21.280 --> 00:40:26.400]   I'm going to give you a little more TikTok or ever. I'm going to give you a little TikTok hint.
[00:40:27.200 --> 00:40:35.120]   I see this a lot. You put at the very beginning the text, wait for it. Oh, okay. Nothing ever
[00:40:35.120 --> 00:40:39.920]   happens on those. But I waited for it and they got me to watch the writing. And that's feed in
[00:40:39.920 --> 00:40:46.320]   algorithm. Yeah. My son knows this because he says, yeah, the longer they watch, it's not just
[00:40:46.320 --> 00:40:51.280]   whether they like it or follow you. It's the longer they watch, then at some point, you get
[00:40:51.280 --> 00:40:56.160]   perfect sense. Makes perfect sense. And you need more cute dogs. I put no effort into that.
[00:40:56.720 --> 00:41:02.160]   And the ones that I don't get and I can tell. And the one that's all you're doing. But the ones
[00:41:02.160 --> 00:41:06.960]   that I actually put some effort in, they had like no views on them. I know it's depressing. You
[00:41:06.960 --> 00:41:12.080]   want to see a great video from my son. Please. This is good. This is good. And with the McDonald
[00:41:12.080 --> 00:41:20.320]   background, he's making a homemade McDonald's. That's what good potatoes sound like.
[00:41:20.320 --> 00:41:27.200]   Apparently that's a TikTok hint. Oh, look at that. Sash brand sausage egg. McMuffin.
[00:41:27.200 --> 00:41:31.520]   Sir Rajasas. Good.
[00:41:31.520 --> 00:41:39.120]   Okay. Now, so I saw my son this morning. We worked out together. He was excited. He totally needs
[00:41:39.120 --> 00:41:47.040]   you with all of that. He said 240 likes in the first 30 minutes. That's a good sign. Yeah.
[00:41:47.040 --> 00:41:52.880]   Because it can go ballistic. But then sometimes you get it and then it doesn't quite make it.
[00:41:52.880 --> 00:41:58.720]   And because the whole trick is to get on the for you. For you. You pay. So because mostly you're
[00:41:58.720 --> 00:42:03.040]   if you you follow people, but the for you pages, all the stuff you don't follow. If you get on there,
[00:42:03.840 --> 00:42:09.840]   you can get a lot of stuff. Yeah. Yeah. Twitter's making some changes on Tweet deck. I haven't seen
[00:42:09.840 --> 00:42:13.200]   them yet. Anybody else seen them yet? Well, I need you to tell me how to do it. I don't just
[00:42:13.200 --> 00:42:19.120]   tweet that there's a hint there. Oh, I've got it. I just got it. I already get it. This is new.
[00:42:19.120 --> 00:42:22.400]   No, maybe that's not. No, that's just dark mode.
[00:42:22.400 --> 00:42:31.440]   I think Tweet deck is in dark mode. There's no light mode. Is there a light mode?
[00:42:32.560 --> 00:42:37.760]   My tweet decks always in dark. When I use. Oh, there is a dark mode.
[00:42:37.760 --> 00:42:42.480]   Oh, there is a light mode. I've been waiting for that. Look at that. Oh, you've changed my life.
[00:42:42.480 --> 00:42:46.960]   Look how much better that is. That's horrible. It's horrible. Let's watch out.
[00:42:46.960 --> 00:42:53.280]   I don't get it. Should look. We're going to we're going to do a very nice contrast.
[00:42:53.280 --> 00:42:56.640]   There's some. Did you kill that story? There's something wrong with him. I don't know.
[00:42:56.640 --> 00:43:01.200]   Did you kill that story? Where's that story? I put it in there. No, it's 24. Line 24. Don't
[00:43:01.200 --> 00:43:06.000]   hit it. Don't hit it. Let's go. Good. Good. I don't know. Can I just go? Goodbye,
[00:43:06.000 --> 00:43:11.040]   Joe. Oh, you're getting people who are tweeting you with suggestions now. Yes. Yes.
[00:43:11.040 --> 00:43:17.520]   Got a couple of those. All right. So if you go on her Twitter, let's go. Love a dark mode.
[00:43:17.520 --> 00:43:23.600]   Here's why you may steal a want to avoid it. Oh, I see. No. See, it has a lot of reasons why dark
[00:43:23.600 --> 00:43:30.880]   mode is bad for you. No. It's not a modern invention. They're actually downsides to switching your apps
[00:43:30.880 --> 00:43:34.480]   to dark mode. Who is who? What nip? It wrote this.
[00:43:34.480 --> 00:43:39.600]   One. Jeffrey Jarvis. No.
[00:43:39.600 --> 00:43:45.360]   Basically, the downsides are that it doesn't it tells you it's saving power, but it doesn't
[00:43:45.360 --> 00:43:51.040]   actually save those monitors. It does. Wait a minute. It doesn't. Amalid displays.
[00:43:51.040 --> 00:43:55.520]   Yeah, but not everything else is and not your main computers are not amalid.
[00:43:55.520 --> 00:43:59.440]   Oh, no, but all your phones almost all your phones are amalid now. Well, modern ones.
[00:44:00.560 --> 00:44:06.800]   Yes. But it feels like I don't do it to save the power. Yeah. We're not doing it because we care
[00:44:06.800 --> 00:44:12.720]   about the environment. No. We're doing it because we just like how it looks. We think we are.
[00:44:12.720 --> 00:44:17.200]   People that don't want to drive big diesel trucks. It says here. In fact, it says you could have
[00:44:17.200 --> 00:44:22.640]   an astigmatism, which in fact, I do have a stick. I do have a stick. I do have a stick. We all do
[00:44:22.640 --> 00:44:26.800]   because we read our naturally adept at seeing things more clearly in the daytime and not so
[00:44:26.800 --> 00:44:31.680]   much at life. See, right? That's not just logical. That's not me. So evolved to see.
[00:44:31.680 --> 00:44:38.320]   Yeah, but that just says you may not enjoy dark mode if you have an astigmatism. It's not saying
[00:44:38.320 --> 00:44:45.280]   that you know, so you're going to I do confess. I know the reasons why you shouldn't use dark mode.
[00:44:45.280 --> 00:44:52.080]   I've known them for years. And it's why books are black text on white paper. That's
[00:44:52.640 --> 00:45:04.160]   because you're black paper. Because you're ambient lighting in the old days. You know,
[00:45:04.160 --> 00:45:08.400]   you were in a room that was lit. You don't want to be looking at something darker with the light
[00:45:08.400 --> 00:45:14.080]   behind you. But nowadays, we all sit in dark and chambers. Right? That's right. And the real
[00:45:14.080 --> 00:45:18.960]   problem is our computer monitors brightness are turned way too high. And I've known this for years.
[00:45:18.960 --> 00:45:23.120]   Comment used to say this to people all the time. Turn down your brightness is way too high.
[00:45:23.120 --> 00:45:29.440]   It's blinding. But because it is, we don't want it. We want dark mode. So comment on that.
[00:45:29.440 --> 00:45:33.680]   Yeah, maybe light mode wouldn't be so bad. So it says that our eyes have to strain more. It
[00:45:33.680 --> 00:45:38.000]   causes eye strain. That's right. Your mother was right. So when you're watching
[00:45:38.000 --> 00:45:42.560]   two of us turn on a light, right? But see, I'm the person that I see better at night than I do
[00:45:42.560 --> 00:45:47.520]   during the daytime. That's that's real talk. That's because you're a werewolf. But that's another
[00:45:47.520 --> 00:45:52.320]   show. I was going to say that's because he's an angel. Maybe you're saying when it suns out,
[00:45:52.320 --> 00:45:57.600]   you don't see things as like ray text or anything. It hurts. It's too bright. I really have to have
[00:45:57.600 --> 00:46:02.400]   my sunglasses because you live in them. You could move to Seattle and it looks good in them. Or that too.
[00:46:02.400 --> 00:46:12.400]   But I do I do see a lot better at night. It's it it just blows my mind. Okay. So meanwhile,
[00:46:12.400 --> 00:46:18.480]   Twitter. Oh, yeah, go back to Twitter and dark. Oh, I know how to do it. That's the story.
[00:46:18.480 --> 00:46:24.720]   I didn't understand. Thank you. So the tweet deck revamp, which I thought I had, but I didn't.
[00:46:24.720 --> 00:46:28.720]   But eventually you can turn it on if you go to line 93 and I didn't understand. Oh, there's a way
[00:46:28.720 --> 00:46:34.560]   to turn on. Let's go. Let's go look here. How to understand this is your geek from Wong M. Jane.
[00:46:34.560 --> 00:46:40.000]   Oh, Jane Wong, I know her. Oh, you do document dot. Oh, I can't read. It's dark.
[00:46:40.320 --> 00:46:50.160]   Equals tweet deck version. Baby, do that. Well, let's go into the CSS. Oh, my gosh.
[00:46:50.160 --> 00:46:53.440]   Let's see. We go to developer mode.
[00:46:53.440 --> 00:47:01.760]   You bet as you cancel probably in this one. The thing is, no, not that this won't tear up the
[00:47:01.760 --> 00:47:09.200]   show like the terminal will. Oh, give me a chance. Give me. I was like, we are we are hitting. We're
[00:47:09.200 --> 00:47:15.680]   going to almost full terminal. Okay. Okay. Yeah. Stacy.
[00:47:15.680 --> 00:47:19.360]   Hey, where did I go? She just ran. She's like, I don't want to see this. How come,
[00:47:19.360 --> 00:47:23.280]   how come Jane didn't say where to put it? That's what I'm wondering.
[00:47:23.280 --> 00:47:28.880]   Document cookie. Maybe, maybe somebody put it in the comment. I wonder what all of the replies
[00:47:28.880 --> 00:47:34.480]   are saying. Let's see. What are the real ones? Maybe it's in the cookies. Oh, sorry. I just launched
[00:47:34.480 --> 00:47:40.480]   TikTok. Maybe it's not in the CSS. Maybe it's that looks like a cookie. So let's go to our preferences,
[00:47:40.480 --> 00:47:46.160]   kids. Everybody joined me now. We go to settings and we go into, uh, to cookies,
[00:47:46.160 --> 00:47:51.680]   which I don't know where that is. This is how they started.
[00:47:51.680 --> 00:47:57.440]   893 is really cool. There we go. Let's go into cookies. Let's say here,
[00:47:58.080 --> 00:48:04.720]   manage data. Here's some cookies. Now I don't know how to put a new one in though, but that looked
[00:48:04.720 --> 00:48:08.560]   like a cookie, right? You're so disappointing me. I thought you said, no, no, I think I can do this.
[00:48:08.560 --> 00:48:14.960]   I just create a cookie. What did she say? Twitter? This is totally not worth it though.
[00:48:14.960 --> 00:48:21.520]   Docu... Doc cookie. I don't know. Where am I? Can y'all hear me? We can hear you.
[00:48:21.520 --> 00:48:25.600]   Yes, we can hear you. Your cameras are not really. Apparently Stacy Higginbot.
[00:48:26.400 --> 00:48:32.480]   My camera's on. She's a bot. We thought it was a real person.
[00:48:32.480 --> 00:48:37.040]   That is the one I owe to you as a bot. It makes perfect sense now.
[00:48:37.040 --> 00:48:38.080]   Very impressive. Oh, my camera?
[00:48:38.080 --> 00:48:44.320]   Click on the console tab if it's not in the devolve view. Maybe you copy and paste the following
[00:48:44.320 --> 00:48:50.800]   into the button. Document cookie. Okay, good. All right. Twitter, a scooter ex. I'm gonna call
[00:48:50.800 --> 00:48:56.560]   Twitter ex is showing us how we're in the console. We go to add the button up there.
[00:48:56.560 --> 00:49:03.920]   Okay. Yeah, yeah, yeah, yeah. And he says, just paste that in before the slash body,
[00:49:03.920 --> 00:49:10.640]   before the end of the body. So into the issue. Oh, the text entry box.
[00:49:10.640 --> 00:49:16.560]   So I have to go into the console. Yeah. And then there. This is great.
[00:49:16.560 --> 00:49:22.560]   Scam warning. Take care when pasting things you don't understand. Screw you. I understand.
[00:49:22.560 --> 00:49:27.120]   Right. You trust that code that she put in there. This random person on Twitter.
[00:49:27.120 --> 00:49:32.720]   Not. Let me put this in. Hold on. Let me slide away from the four. Please type,
[00:49:32.720 --> 00:49:43.040]   allow pasting below. A L L O W pasting to allow pasting. Okay. Now I'm going to paste that in.
[00:49:43.040 --> 00:49:50.160]   Now we see you miss Stacy. Good. Tweeted version beta. All right. Now let's go back to the tape.
[00:49:50.160 --> 00:49:55.680]   But you got a refresh refresh. And now ladies and gentlemen, I give you wider columns.
[00:49:55.680 --> 00:49:59.040]   It's back to the same. No, no, I guess that's that's big. I wider columns.
[00:49:59.040 --> 00:50:03.120]   No, people were complaining there were no columns. There's a picture of what it's supposed to look
[00:50:03.120 --> 00:50:12.000]   like. This looks. Maybe go back to the story to a hard refresh. I thought I did. But no.
[00:50:12.960 --> 00:50:19.280]   Well, I don't know. I just maybe I didn't do it in the right price. Let me do it again.
[00:50:19.280 --> 00:50:26.320]   Mark mode. This is. I don't know. I just don't know. It has to be disabled. Dark mode has to be
[00:50:26.320 --> 00:50:34.560]   disabled. That'd be funny. Then what do I do? All right. I'm on the tweet deck tab. Now do I
[00:50:34.560 --> 00:50:43.440]   refresh now? I haven't used tweet deck. Oh, I do have the new version. First,
[00:50:43.440 --> 00:50:52.160]   want to import your existing columns? Yes. Whatever that means. Now take the tweet deck tour.
[00:50:52.160 --> 00:50:55.680]   You made a great choice on switching to treat deck.
[00:50:55.680 --> 00:51:00.560]   On the bus everybody, we're going on the. Now let's take a quick tour to show you around.
[00:51:01.520 --> 00:51:06.720]   Make a column. Okay. Well, that's not different. Look at the columns are all different looking,
[00:51:06.720 --> 00:51:12.000]   though. Yeah, that's what they are. Stay organized with decks for a cleaner, more focused experience,
[00:51:12.000 --> 00:51:17.600]   organize your workspace. Oh, boy. Multiple decks. This is like tabs for tweet deck. Oh, boy. It is
[00:51:17.600 --> 00:51:23.040]   like tabs for tweet. Move a column. Use these dots. All right. Customize delete order tweets.
[00:51:23.040 --> 00:51:27.120]   Change. I bet they took out. You can reorder tweets? That's interesting.
[00:51:27.840 --> 00:51:33.200]   So let's, so I can move this, right? Yeah. You can do that before. Yeah. But now look
[00:51:33.200 --> 00:51:39.520]   at the different sizes, right? And let's look here. We can top tweets first or latest tweets first.
[00:51:39.520 --> 00:51:45.760]   We want latest tweets. And I bet that sticky auto refresh you bet. Column with, it's really
[00:51:45.760 --> 00:51:50.320]   wide right now. I can make a little less wide, right? Rename column. You know what they took out?
[00:51:50.320 --> 00:51:57.440]   The one feature tweet deck I used it for. Mute scheduling? No, and maybe it's somewhere else.
[00:51:57.440 --> 00:52:01.920]   But I turn off retweets. Because I found that that was often
[00:52:01.920 --> 00:52:08.640]   about that somewhere. A problem. Yeah. Maybe it's somewhere else. I do like it that I could do
[00:52:08.640 --> 00:52:15.760]   chronological. So you can do chronological and normal Twitter. Yeah. They may not have like
[00:52:15.760 --> 00:52:25.040]   under settings by column Stacy or not. I'm sorry. You turn off. My column is Twitter. Oh,
[00:52:25.040 --> 00:52:30.080]   and your home. Well, not in normal in normal Twitter, you only get your your
[00:52:30.080 --> 00:52:34.560]   call with like your traditional column. This is also new. There's an explore tab.
[00:52:34.560 --> 00:52:40.720]   And there are no columns in Twitter. Twitter. Are there? You know, I'm really actually glad at
[00:52:40.720 --> 00:52:44.400]   least that we're getting some attention for tweet deck because they hadn't changed anything in years.
[00:52:44.400 --> 00:52:50.080]   But I used to love it. Yeah. That's all I use for Twitter. Yeah.
[00:52:51.440 --> 00:52:56.800]   I just use this pretty much the same thing for me except it's not all on the same screen.
[00:52:56.800 --> 00:53:01.840]   Yeah. I have your COVID list for instance, Jeff. And yeah, there's here's another list.
[00:53:01.840 --> 00:53:07.200]   So I obviously have to do a deck because I've got too many. Oh boy. Too many columns here.
[00:53:07.200 --> 00:53:12.000]   What's a deck meet? What's that mean? That's workspaces. So I can have a new workspace.
[00:53:12.000 --> 00:53:19.360]   I'm going to say this is for tech journalists or tech news, let's say. And the emoji should be
[00:53:20.000 --> 00:53:28.400]   a geek. How about there you go. I got it. Okay. And it could be a angry person. Yeah.
[00:53:28.400 --> 00:53:34.160]   And now I could add the various lists that I follow of tech journalists. So that's nice.
[00:53:34.160 --> 00:53:37.360]   There's also a button if you don't want to keep doing this. It's under the bug.
[00:53:37.360 --> 00:53:42.800]   John, can you remove the bug? There you go. Leave tweet deck preview. So once you do that,
[00:53:42.800 --> 00:53:52.320]   so once again, go into the console of the developer tools, paste in document.cookie=
[00:53:52.320 --> 00:53:59.840]   tweet deck underscore version equals beta end quote. And that will enable it. You'll then have
[00:53:59.840 --> 00:54:06.000]   to refresh the page and you'll get the new version. That was fun. Thank you, Jane.
[00:54:06.000 --> 00:54:13.440]   Thank you, Jane. Manchin Wong. Sure. I really appreciate it. Fascinating. Yeah, that was fun.
[00:54:13.440 --> 00:54:19.840]   We took a tour. Okay, everybody. We ought to get back on the ship now. All right. So long tweet.
[00:54:19.840 --> 00:54:25.680]   Hey, speaking of ships, I got some bad news for you, Jeff. Oh, no, you're not going to go on this?
[00:54:25.680 --> 00:54:28.640]   No, we are. But you're not invited.
[00:54:32.000 --> 00:54:37.680]   So we have a Twitter cruise coming up in Alaska next year. It's a year from now.
[00:54:37.680 --> 00:54:39.680]   And it's going to be great. It's going to be great.
[00:54:39.680 --> 00:54:46.320]   My birthday. Yeah, and you should go. I understand. Lisa and I are going and we were told by the
[00:54:46.320 --> 00:54:51.600]   organizers that if we get 60 people to sign up, we can get a second cabin. And I thought,
[00:54:51.600 --> 00:54:55.920]   well, how to do this equitably? How can I keep Jeff Jarvis out?
[00:54:57.920 --> 00:55:04.320]   Oh, I was close. No, no, how can I do this equitably? I thought, well, I'll go by seniority.
[00:55:04.320 --> 00:55:09.520]   So I asked Steve Gibson, who is the second person to join the network. There's some people before
[00:55:09.520 --> 00:55:17.920]   him, but they're gone. And so we don't talk about them anymore. So Steve, I think I think the
[00:55:17.920 --> 00:55:21.440]   second show I did was with him. Very behind the bricks. She doesn't work for Twitter anymore.
[00:55:21.440 --> 00:55:26.640]   She's a Canadian. So I know. And then so Steve and Steve asked his wife Lori, she said,
[00:55:26.640 --> 00:55:33.280]   under no circumstances, am I getting on a COVID boat? And so I think Lori, I think she
[00:55:33.280 --> 00:55:41.120]   miss out. Cruz? Yeah, she miss spoke. So the next one, I think, is is Paul Therat.
[00:55:41.120 --> 00:55:45.200]   And I asked Paul Therat. And he said, well, the only Cruz I ever wanted to go on was Alaska Cruz.
[00:55:45.200 --> 00:55:49.440]   I'm in. So Paul and his wife, Stephanie are going to go. Great. So we would have probably been next,
[00:55:49.440 --> 00:55:52.640]   I think, Jeff, I'm trying to think of seniority. But I've been hanging around that long. You've
[00:55:52.640 --> 00:55:56.320]   been trying to go all these years. Yeah. You've been around more than 10 years, right?
[00:55:57.200 --> 00:55:59.920]   The keys are Cruz. The keys and geezers.
[00:55:59.920 --> 00:56:07.200]   I'm so sorry. You don't have to. I love how you empower old men. It's great.
[00:56:07.200 --> 00:56:11.280]   Y'all don't need anymore empowerment. I promise.
[00:56:11.280 --> 00:56:14.800]   It's not a zero self game.
[00:56:14.800 --> 00:56:19.920]   See, wow. There we go. Nice. I'm going to wave. I'm going to wave. You'll go buy it. The docks.
[00:56:21.840 --> 00:56:28.880]   We are going to Seattle. These are Seattle. Stacey has to come to the goodbye ceremony.
[00:56:28.880 --> 00:56:34.000]   If you do something the night before or something like that, definitely show up.
[00:56:34.000 --> 00:56:37.440]   Yeah, we'll definitely know. And then unless you don't want me there, Paul, no, we do.
[00:56:37.440 --> 00:56:43.360]   And then I think there's a cocktail party in a lecture or something like that. But Paul's wife,
[00:56:43.360 --> 00:56:48.640]   Stephanie, who is a mixologist extraordinaire, will be announced that she will be doing a cocktail
[00:56:48.640 --> 00:56:54.160]   hour where she'll introduce some of her specialties. Well, now I might go. Yeah, I know. And that's
[00:56:54.160 --> 00:56:58.800]   sound good. She's really good. Like there's an exologist. Yes. So it's going to be a lot of
[00:56:58.800 --> 00:57:04.080]   fun. There's no polar bears, but I understand there'll be salmon, grizzly bears and bald eagles.
[00:57:04.080 --> 00:57:08.080]   So it's going to end glaciers. Let's not forget everything Alaska's made of. If you want to know
[00:57:08.080 --> 00:57:14.320]   more Cruz that Twitter TV, there are cabins left. But I think I've been I think you should hurry.
[00:57:14.320 --> 00:57:17.520]   I know it's a year off and most people aren't thinking about what they're going to do next
[00:57:17.520 --> 00:57:23.680]   July. But I think you shouldn't delay because they told us if demand suddenly increases for this
[00:57:23.680 --> 00:57:29.440]   cruise, we can we could take some cabins back. So once you book, you're good. But but you got a
[00:57:29.440 --> 00:57:34.480]   book if you want to guarantee they could take that cabins back. Yeah, you know, the cruise line.
[00:57:34.480 --> 00:57:38.320]   And I think that they're just as most cruise lines are at this point, uncertain about what
[00:57:38.320 --> 00:57:43.280]   it's going to look like in July. So true. Yeah. So they they reserve the right to say, hey, we're
[00:57:43.280 --> 00:57:46.880]   getting a huge demand for this trip. So we're not going to promise. We're not going to give
[00:57:46.880 --> 00:57:50.480]   you all the cabins you promised. We promised you only people have already reserved by then.
[00:57:50.480 --> 00:57:54.400]   We'll be able to go good stuff. So reserve, Cruz that Twitter TV is going to be a lot of fun.
[00:57:54.400 --> 00:57:59.360]   Paul Theroden and I will get drunk on peanut butter whiskey while our wives will dance.
[00:57:59.360 --> 00:58:07.120]   With bald eagles. What if they just shake their heads and shame? Because that seems more like
[00:58:07.120 --> 00:58:15.600]   tea. I feel very like I can I can see Lisa just like, oh,
[00:58:15.600 --> 00:58:22.320]   she loves me, but you know, yeah, you know how it is. You know how it is. People still be like,
[00:58:22.320 --> 00:58:27.200]   why are they doing that? Yeah. Yeah. That's basically what she says all the time. Why did you do that?
[00:58:27.200 --> 00:58:32.720]   Yeah. My husband does the same thing to me. He's like, what did you do that? What were you thinking?
[00:58:32.720 --> 00:58:37.680]   Why did you make the blinds only open by a voice command? Oh boy.
[00:58:37.680 --> 00:58:46.000]   Retail stores packed with unchecked facial recognition. Our next story in just a moment.
[00:58:46.000 --> 00:58:53.360]   I just want to reintroduce our fabulous crew. Stacy Higginboth is here. She's the
[00:58:53.360 --> 00:59:01.440]   replacement for Stacy Higginbotham who has today on. Like I could try to do very realistic, I must say.
[00:59:01.440 --> 00:59:08.000]   We've already had that gift on Stacy on IHOT.com and put from the hands on photography. And of
[00:59:08.000 --> 00:59:19.040]   course, the one the only the inimitable Jarvis of Buzz machine calm. Moving along. Retail stores
[00:59:19.040 --> 00:59:24.720]   packed with unchecked face recognition, according to civil rights organizations, more than 35
[00:59:24.720 --> 00:59:31.120]   organizations demanding top US retailers cease facial recognition, Walmart Home Depot target,
[00:59:31.120 --> 00:59:37.360]   and even though Apple says it does not apparently, I saw a story that said Apple used to.
[00:59:37.360 --> 00:59:46.320]   So it's used. It's used for deterring a shoplifting, I think. Identifying known shoplifting.
[00:59:46.320 --> 00:59:51.360]   There's a couple. There's identifying new shoplifters, but there's also identifying
[00:59:51.360 --> 00:59:57.200]   people that you don't want in the store. People you've banned in the past. Black-balled people. Yeah.
[00:59:58.080 --> 01:00:03.600]   And those people could be banned for shoplifting for weird returns. They could be banned for all
[01:00:03.600 --> 01:00:09.440]   kinds of things. So yes. And I did see a story about a young black man who was kicked out of
[01:00:09.440 --> 01:00:18.080]   somewhere because facial recognition misidentified him. Yeah. It's just another day in face ID. Yeah.
[01:00:18.080 --> 01:00:24.720]   Yeah, because it's particularly inaccurate with people of color because they don't use
[01:00:24.720 --> 01:00:28.640]   black people in training. That's right. It's all a bunch of white at least. At least not yet.
[01:00:28.640 --> 01:00:35.520]   Yeah. On the other hand, devil's advocate, but I kind of believe this Taylor Swift, for instance,
[01:00:35.520 --> 01:00:41.760]   who's had problems with fans stalking her and things has used face recognition at her concert
[01:00:41.760 --> 01:00:48.480]   entrances to identify potential problem people and pull them aside. If facial recognition were
[01:00:48.480 --> 01:00:53.120]   perfect, yeah, we wouldn't have a problem with that, right? Oh, no, we would. Hell,
[01:00:53.120 --> 01:01:00.240]   oh, yeah, we would. Why? Well, okay. Let's talk about having widespread facial recognition and
[01:01:00.240 --> 01:01:03.280]   what that means. Well, no, I'm just talking about that Taylor Swift. Let's start with a single
[01:01:03.280 --> 01:01:09.440]   exhibit. Forget your computer. Just somebody. Somebody sits there with a list of with a page of photos.
[01:01:09.440 --> 01:01:14.000]   A cop could do it. A security guard. A cop sits there into it. I see your photo on this list.
[01:01:14.000 --> 01:01:18.560]   Yeah. That's where you're starting. And probably is not very accurate and has to have a long list
[01:01:18.560 --> 01:01:25.040]   apparently for Taylor Swift. So if you could have face recognition and it could be accurate,
[01:01:25.040 --> 01:01:30.240]   wouldn't that be a good and benign use of it? Or you disagree, Stacey?
[01:01:30.240 --> 01:01:34.960]   No, no, no. Okay. If you use it for those kind of things, but they're also using it for security
[01:01:34.960 --> 01:01:40.080]   in schools, theoretically, security in schools where they're banning people, but there's no due
[01:01:40.080 --> 01:01:44.640]   process around this. So what we're doing is we're introducing a tool without any regulatory things.
[01:01:44.640 --> 01:01:49.440]   So we had actually talked about this a couple weeks ago. It's a company that provides facial
[01:01:49.440 --> 01:01:56.000]   recognition software to schools. And the schools that are using it are tracking students up to like
[01:01:56.000 --> 01:02:02.240]   5,000 times a day. So they're seeing everywhere the students go in the school. They're also using
[01:02:02.240 --> 01:02:08.320]   it to keep people they banned from campus off campus, including former students and parents who
[01:02:08.320 --> 01:02:14.480]   have argued with security guards. So you look at this and you're like, it's a very,
[01:02:14.480 --> 01:02:18.880]   it's a very useful tool, but it's a tool that we really need to start putting
[01:02:18.880 --> 01:02:25.040]   guidelines around. I would agree 100%. Yes. So the problem is,
[01:02:25.040 --> 01:02:30.800]   you I have with these stores. Yeah, the problem is accuracy and lack of due process, right?
[01:02:30.800 --> 01:02:34.960]   And lack of transparency about using it. You go into most places, they say we're using video
[01:02:34.960 --> 01:02:41.200]   cameras, they should also say we're using facial recognition. Right. And so this site,
[01:02:41.200 --> 01:02:48.720]   banfacial recognition calm is putting social pressure on stores not to do it. They've actually
[01:02:48.720 --> 01:02:54.240]   asked the top retailers in the country if they use it and asked them to commit. And there's a big
[01:02:54.240 --> 01:02:59.200]   tweet them button and a email button next to many of these asked them to commit not to use it. Walmart
[01:02:59.200 --> 01:03:04.720]   has agreed they will not use Kroger will not use in the future. Walgreens has not responded.
[01:03:04.720 --> 01:03:08.560]   So they might use Home Depot won't use target. So this seems like it's working.
[01:03:08.560 --> 01:03:12.800]   Lowe's won't. Albertsons does use it. You have it at the same time.
[01:03:12.800 --> 01:03:19.680]   HGB does use it. Macy's does use it. But at the same time, a Walmart could come back and just say,
[01:03:19.680 --> 01:03:24.400]   you know what, not it's actually working better for us to do this. So them just do
[01:03:24.400 --> 01:03:29.040]   one add on the surface. There's no, I mean, they just say it. There's no contract.
[01:03:29.040 --> 01:03:34.640]   And then didn't you have to think about whatever it is here in California with the whole $950
[01:03:34.640 --> 01:03:40.080]   being a misdemeanor versus the felony and people are just shoplifting like crazy. You think they
[01:03:40.080 --> 01:03:44.320]   care about facial recognition? There's a problem in San Francisco. Yeah, people just coming and
[01:03:44.320 --> 01:03:50.640]   taking stuff and they're on camera and they don't care. Hey, I'm good. I'm only getting $200 worth
[01:03:50.640 --> 01:03:54.560]   of stuff and I can deal with a misdemeanor. This story from a couple of months ago,
[01:03:54.560 --> 01:03:59.440]   Apple and its security contractor security industry specialist sued in Massachusetts.
[01:04:00.560 --> 01:04:05.360]   Part of a multi jurisdictional defamation and malicious prosecution complaint
[01:04:05.360 --> 01:04:11.120]   brought on behalf of a New York resident misidentified multiple times. Yikes.
[01:04:11.120 --> 01:04:17.840]   As a shop lifter. So there is a good example of a lack of due process and be in accuracy.
[01:04:17.840 --> 01:04:25.840]   Right. So, you know, and I have the same feelings about the police department using it. It used it,
[01:04:25.840 --> 01:04:33.280]   I guess appropriately and it's up with constraints. If it were accurate and it would be useful in
[01:04:33.280 --> 01:04:40.960]   solving crimes. Nothing is guaranteed accurate. So you have to have a process of check to process
[01:04:40.960 --> 01:04:46.560]   health. Right. Yeah. So if you have a guy at the Taylor Swift concert with an actual list of pictures
[01:04:46.560 --> 01:04:51.280]   of people and somebody gets pulled out, they don't say anything. They say, "Excuse me, sir.
[01:04:52.240 --> 01:04:55.360]   I want to ask you a question." And then identify, "Do you have an ID?"
[01:04:55.360 --> 01:05:01.120]   Right. And I cross check. Yeah. I just want to cross check with this image with your ID.
[01:05:01.120 --> 01:05:10.240]   Could be a mistake. Yeah. But there's also the, how do they communicate and share that facial
[01:05:10.240 --> 01:05:15.040]   recognition data and data about people they catch in those systems, even if I'm just shopping at
[01:05:15.040 --> 01:05:21.760]   Macy's, if the police come and they're like, "Hey, there was a crime over here. We're looking at Stacy."
[01:05:21.760 --> 01:05:26.720]   Right. A probable person, you know, "Was she in your store?" This is a very stupid example.
[01:05:26.720 --> 01:05:31.840]   Right. No, but I guess that's still going back to the due process side of things. They can't just
[01:05:31.840 --> 01:05:35.680]   come in there without a warrant. Right. Is that what you're saying? Well, we don't, we don't know if
[01:05:35.680 --> 01:05:41.840]   they can or not. Right. So they could access, you know, my entire history if they wanted to,
[01:05:41.840 --> 01:05:46.880]   based on private surveillance cameras. If they do they need a warrant? I don't know.
[01:05:46.880 --> 01:05:52.640]   Here's the nightmare scenario for this poor kid in New York. He's an honors student
[01:05:52.640 --> 01:05:59.840]   at Bronx, Latin, in New York City High School. He's black. He got a temporary learner's permit
[01:05:59.840 --> 01:06:04.080]   for driving a couple of years ago when he was 17. It had his height, weight, date,
[01:06:04.080 --> 01:06:06.960]   of birth, and eye collected. They have a photograph. His learner's permits don't usually.
[01:06:06.960 --> 01:06:12.240]   Right. He lost it. Then got a permanent laminated copy that included his picture.
[01:06:13.760 --> 01:06:20.080]   He was detained. Actually, no. Later that year somebody was detained for stealing store merchandise
[01:06:20.080 --> 01:06:25.360]   in an Apple store, handed over the temporary learner's permit, the one he'd found.
[01:06:25.360 --> 01:06:30.000]   Even though the ID says it's temporary not to be used for identification purposes,
[01:06:30.000 --> 01:06:36.560]   Apple then said, "Ah, you're this guy." Even though the guy they arrested was 6'1" and
[01:06:36.560 --> 01:06:41.840]   the students only 5'7". So the AR doesn't even matter if there's still people involved.
[01:06:41.840 --> 01:06:46.240]   Yeah. That's right. In fact, the so-called "dupe-rauses" was worse.
[01:06:46.240 --> 01:06:51.040]   But then his name is put into this system. He's flagged.
[01:06:51.040 --> 01:06:54.400]   And he got arrested again. Wow.
[01:06:54.400 --> 01:07:01.520]   Yeah. We have to have ways of rectifying errors in these computerized databases.
[01:07:01.520 --> 01:07:05.680]   Clearly it's a problem. For sure. The GDPR actually gives people that right.
[01:07:05.680 --> 01:07:10.000]   The current California Privacy Act is not. So if we were going to look at a federal law,
[01:07:10.000 --> 01:07:12.320]   that's one of the things we'd want to do. Interesting. Good to know.
[01:07:12.320 --> 01:07:18.560]   I heard that there were some type of surveillance in the UK. I don't know
[01:07:18.560 --> 01:07:24.240]   much about it. Is that still going on? Where it's like a regular, everybody's sort of expects to
[01:07:24.240 --> 01:07:30.160]   be watched and wherever they are out. They have a lot of CCTV around
[01:07:30.160 --> 01:07:34.400]   there everywhere. So in public places and on their roads.
[01:07:34.400 --> 01:07:38.960]   What about Amazon's ghost stores? I mean, not only are you being photographed,
[01:07:38.960 --> 01:07:42.320]   but every action you take in the groceries being photographed. But you know it.
[01:07:42.320 --> 01:07:45.920]   I guess you know when you go in. You know how it works.
[01:07:45.920 --> 01:07:48.480]   And if you didn't like it, you can stay out of it. You know what's happening?
[01:07:48.480 --> 01:07:54.160]   Getting a receipt of what happened? That is one of the things that this site claims stores are
[01:07:54.160 --> 01:07:57.920]   doing is if you go in a store and don't buy anything, you walk out without buying anything,
[01:07:57.920 --> 01:08:01.680]   they send you a coupon. They say, "Come on back, we got a deal for you."
[01:08:01.680 --> 01:08:05.600]   Well, so you got a good deal. Yeah, but that may be a little creepy.
[01:08:05.600 --> 01:08:07.760]   I was like, "It's still creepy, ain't it?" Yeah.
[01:08:07.760 --> 01:08:11.680]   Yeah, you're like, I mean, people don't want to think about it. I mean,
[01:08:11.680 --> 01:08:17.200]   you don't want every single interaction you have with any part of society to
[01:08:17.200 --> 01:08:23.200]   relegated to a transaction, right? Like, let's say you're just walking into a store just to look at
[01:08:23.200 --> 01:08:31.760]   stuff, you know? So here's a question for you, Jeff Jarvis. Is Facebook killing people?
[01:08:33.760 --> 01:08:38.800]   Let's ask the president. They don't ask him. He said they were, but then he said, "Well, no,
[01:08:38.800 --> 01:08:45.520]   I don't mean Facebook's doing it." They're talking about vaccines. And there is some evidence that
[01:08:45.520 --> 01:08:52.160]   Facebook has not done a great job of blocking or taking down misinformation. Facebook can
[01:08:52.160 --> 01:08:54.960]   certainly do better. They could do better. They all could.
[01:08:54.960 --> 01:09:02.240]   Biden said he meant to accuse a dozen social media platform users. Which I think actually,
[01:09:02.240 --> 01:09:07.600]   it was a classic Biden gaffy thing, because I listened to it again. And I actually don't think
[01:09:07.600 --> 01:09:11.600]   he was trying to say Facebook was. I think it was trying to say they, like these people who are using
[01:09:11.600 --> 01:09:18.560]   it, those dastardly souls, and it came off wrong and Facebook reacted. But the problem here is,
[01:09:18.560 --> 01:09:25.360]   as various of the things I put in the rundown, said, this kerfuffle is noticing the point
[01:09:25.360 --> 01:09:30.480]   on both sides, right? Facebook needs to do a better job. Government needs to recognize what's
[01:09:30.480 --> 01:09:38.640]   going on. It doesn't include Fox. Look at Hannity's about face in the discussion. We need a smarter
[01:09:38.640 --> 01:09:45.520]   discussion than ensued in 24 hours after this moment. And I'm sympathetic, because while it's
[01:09:45.520 --> 01:09:51.680]   not Facebook killing people, there's a lot of virus misinformation being spread. And as I was
[01:09:51.680 --> 01:09:57.520]   talking about, I think it was before the show, I was sitting beside somebody on the airplane on
[01:09:57.520 --> 01:10:00.960]   the way home and said, Oh, I'm not going to get it. I'm not going to put anything on none of my body.
[01:10:00.960 --> 01:10:09.120]   It was clearly getting misinformation about the at this point. I know it's it's not officially
[01:10:09.120 --> 01:10:13.840]   approved yet. It's an experimental approved, but hundreds of millions of people have received
[01:10:13.840 --> 01:10:18.720]   these vaccines. Yeah, a lot more people in that time have died of COVID. And I've got that have
[01:10:18.720 --> 01:10:25.840]   even been sickened by the vaccine. So I don't know, I can understand the sentiment, which is,
[01:10:27.120 --> 01:10:30.000]   we got to do something about this. People are dying, you know?
[01:10:30.000 --> 01:10:34.640]   Well, I think I think that the study, and this is part of what Fox went after
[01:10:34.640 --> 01:10:41.840]   his press office, the study that says the 12 people are promoting most of the most
[01:10:41.840 --> 01:10:48.080]   of the misinformation comes from these critical because it is you can find low-key of networks
[01:10:48.080 --> 01:10:52.640]   and attacked them. So this idea you've got to try to get everything possible.
[01:10:52.640 --> 01:10:56.400]   Yeah, yeah, yeah, yeah, he's smart about it. Don't go after those don't attack Facebook.
[01:10:56.400 --> 01:11:01.120]   Facebook should be doing something about those, but we'll accept the research 12 people,
[01:11:01.120 --> 01:11:05.360]   and they could get a lot done that way. Yeah. And Farhan for how Manju
[01:11:05.360 --> 01:11:09.680]   wrote in the New York Times, it's the anti-vax movement is much bigger than just Facebook.
[01:11:09.680 --> 01:11:14.080]   And we agree. Very much. Yeah. I'm Marjorie Taylor Green got banned by Twitter for 12
[01:11:14.080 --> 01:11:20.880]   hours. Seems like a weird band, but for 12 hours for saying that vaccines don't work.
[01:11:22.160 --> 01:11:29.680]   So our friend, near at Weissblad, I think a very good piece, Dr. Dr. near at Weissblad. Yes.
[01:11:29.680 --> 01:11:37.840]   The dichotomous debate sucks was her subhead. This is line 70. And saying what I think is very
[01:11:37.840 --> 01:11:42.880]   true is the is the it saves people it kills people those extremes are getting us nowhere.
[01:11:42.880 --> 01:11:49.280]   The discussion is dumb that way. It's not as bad as it's presented by the dystopian. It's not as
[01:11:49.280 --> 01:11:53.040]   good as the companies would present. And we can always do better. And so let's figure out how to
[01:11:53.040 --> 01:11:55.040]   do better. God, Timmy. Yeah.
[01:11:55.040 --> 01:12:00.240]   Wait, Jeff, are you telling us to be best?
[01:12:00.240 --> 01:12:11.600]   Yeah, I mean, both we've said that on the show. Both sides are true. The internet has been amazing.
[01:12:11.600 --> 01:12:17.280]   And technology has been amazing, but there's also and I wouldn't be covering technologies.
[01:12:17.280 --> 01:12:23.360]   My life vocation, if it weren't for the amazing benefits of it. But there,
[01:12:23.360 --> 01:12:26.080]   but the reason I was going to say if you had known you could get 20 million.
[01:12:26.080 --> 01:12:33.280]   If I had known I could be a powering women and make 20 million a year, I would have done that
[01:12:33.280 --> 01:12:40.080]   No, I wanted part of what I wanted to do was talk about not just the benefits, because there's,
[01:12:40.080 --> 01:12:45.120]   you know, when I started, there was a lot of cheerleading for technology. And what I wanted to do,
[01:12:45.120 --> 01:12:49.920]   I love technology. In fact, it was kind of the tagline at the end of the screen savers, you know,
[01:12:49.920 --> 01:12:55.520]   because we worked for Ziff Davis, which it's motto at the time was love technology.
[01:12:55.520 --> 01:13:01.440]   And I would always say love technology, but back up your hard drive or love technology,
[01:13:01.440 --> 01:13:07.200]   but make sure you get vaccinated. So it's both it isn't it isn't one of the other.
[01:13:07.200 --> 01:13:10.560]   I've always said that. And one of the reasons I do what I do is because I want to help people
[01:13:10.560 --> 01:13:15.760]   understand it so that they know how to make the most out of it takes awesome, but it's not perfect.
[01:13:15.760 --> 01:13:21.520]   It's not perfect. There's a lot of scams and imperfections and a lot of places you can waste
[01:13:21.520 --> 01:13:25.840]   your money. And you know, we got to get phone companies to make these things more recyclable
[01:13:25.840 --> 01:13:30.640]   and repairable. We got to get them to be more secure. And there's also it's life. It's a
[01:13:30.640 --> 01:13:34.880]   good supply. It's a process, folks. We always got to grow. Always got to do better. We learn how
[01:13:34.880 --> 01:13:40.000]   things operate. We learn how to do them. It took us a long time and a 30 years more to get over the
[01:13:40.000 --> 01:13:46.320]   beings of print. But as I've said before, those big wide platitudes of saving lives and killing
[01:13:46.320 --> 01:13:51.040]   people as the doctors said there are those they don't still sell clicks. They suck clicks.
[01:13:51.040 --> 01:14:00.080]   I do have to say, I don't know, Jeff, if I agree with you that this is just another example of,
[01:14:00.080 --> 01:14:05.680]   you know, Gutenberg moment, I do think it's somewhat different. And maybe it isn't. But I do I do think
[01:14:06.640 --> 01:14:13.120]   that the reach the the reach of social networks, especially with technology and the internet
[01:14:13.120 --> 01:14:21.360]   is so vast. Its speed is so incredible that it is an order of magnitude more risky than the printing
[01:14:21.360 --> 01:14:28.080]   press. We were burning people at the stake because of the praying press. Right. We were starting
[01:14:28.080 --> 01:14:34.960]   peasants wars. We were burning downtowns because of what was said on the praying press. No,
[01:14:34.960 --> 01:14:39.360]   I think it was worse back then. I think I'd go with a pinker moment here and say that actually
[01:14:39.360 --> 01:14:45.040]   society is improving and we refuse to recognize it. But again, fair enough work.
[01:14:45.040 --> 01:14:51.440]   Higginbot had a tough one. Was that do you need some oil?
[01:14:51.440 --> 01:14:59.680]   I appreciate Pinker. He's somewhere behind me on my bookshelves for the fattest.
[01:14:59.680 --> 01:15:09.840]   The fattest book there is. But I think what we see with social media is a lot more
[01:15:09.840 --> 01:15:14.800]   individual harms and it's much easier. It's kind of like email making spam possible.
[01:15:14.800 --> 01:15:17.920]   It's death by a thousand cuts. Instead of getting burned at the stake,
[01:15:17.920 --> 01:15:23.600]   you're getting whittled down. But it's also enabling people again who've never been heard to be heard.
[01:15:23.600 --> 01:15:29.440]   You've got to know. No one denies that. No one's trying to shut down these
[01:15:29.440 --> 01:15:33.200]   platforms as platforms. We're just saying, be sure some of the harms.
[01:15:33.200 --> 01:15:40.080]   Let's think about how to mitigate those harms. That's all. Without having an impact on some of
[01:15:40.080 --> 01:15:44.800]   the potential. I don't think you cannot have an impact. I think that's kind of a silly thing.
[01:15:44.800 --> 01:15:52.080]   Well, without without. But understanding and looking at, hey, when that impact happens,
[01:15:52.720 --> 01:16:00.400]   how far does it go? Can people still come out from it? Okay. You can't make legislation
[01:16:00.400 --> 01:16:06.560]   without affecting some people. Killing session 230, which some would say is what we should do,
[01:16:06.560 --> 01:16:11.920]   would have a huge impact on the speech of. I'm not a fan of killing section 230.
[01:16:11.920 --> 01:16:22.160]   Yeah, Tech Dirt had a good article on net neutrality. It was Carl Bodie, I think. Let me see if I can
[01:16:22.880 --> 01:16:27.600]   Oh, you know him. You should have him on the show. Really? Let's get him on. He's great.
[01:16:27.600 --> 01:16:34.000]   Well, he's working at Gigaton. He's inflammatory. No, he never worked at Kiga. But he he's he lives
[01:16:34.000 --> 01:16:38.720]   up here in Seattle now, but he's my buddy, man. He's so smart. Well, you'll like his link,
[01:16:38.720 --> 01:16:45.360]   baby headline Axios, parrots, a lot of dumb debunked nonsense about net neutrality.
[01:16:45.360 --> 01:16:47.760]   How did I miss that headline? Isn't that a great one?
[01:16:48.880 --> 01:16:53.360]   He this was this was today, actually, I think it came out. Maybe that's why I missed it,
[01:16:53.360 --> 01:16:59.760]   because it just came out. He is pointing out this both-sideism that you see a lot. And I know
[01:16:59.760 --> 01:17:04.320]   Jeff, I'm sure you talk about it. I know Jay Rosen does all the time. This this idea that news
[01:17:04.320 --> 01:17:10.480]   is fair and balanced when you quote both sides equally as if they have equal merit. So he has
[01:17:10.480 --> 01:17:14.640]   the Axios article he's referring to as a lot of quotes from, you know,
[01:17:14.640 --> 01:17:18.640]   Who is the reporter on that article? Was it Kim Hart? Because if it was, she used to work
[01:17:18.640 --> 01:17:27.520]   and do PR for the FCC. Oh, interesting. Let me see if he doesn't actually mention is Margaret McGill.
[01:17:27.520 --> 01:17:34.640]   Let me see which articles he talking about. Let's see. It's net neutrality battle looms from last week.
[01:17:34.640 --> 01:17:41.040]   Oh, Margaret Harding McGill. Okay. Yeah. It wasn't Kim. No, but apparently in order to write this
[01:17:41.040 --> 01:17:45.840]   article, and I you know what, I think I would do this too. You get quotes from both sides, right?
[01:17:45.920 --> 01:17:53.520]   And you know, you're not trying to inject an opinion in there. You're trying to get the story.
[01:17:53.520 --> 01:18:00.720]   Well, yes and no. I think there's a job to be and this is like a giga-ohm. When we we sat there,
[01:18:00.720 --> 01:18:05.200]   like objective journalism isn't really possible. So you know, you're going to come out one side or
[01:18:05.200 --> 01:18:10.640]   the other. We always came out pro-consumer. That doesn't mean I didn't listen to all the sides.
[01:18:11.920 --> 01:18:17.440]   But you know, at the end of the day, Wall Street, the telecom companies, they have millions of
[01:18:17.440 --> 01:18:21.920]   lobbyists. That's literally their job they're paying for. So all the consumer has is us.
[01:18:21.920 --> 01:18:25.440]   But I thought the job as a reporter was to report the facts.
[01:18:25.440 --> 01:18:33.680]   How you report the facts and whose facts you report, I can tell you like municipal broadband
[01:18:34.400 --> 01:18:42.400]   is unprofitable for municipalities on a cost basis in a lot of cases. So the anti-
[01:18:42.400 --> 01:18:49.120]   the cable companies will say it costs taxpayers dollars. It's an unprofitable business. They should
[01:18:49.120 --> 01:18:54.000]   keep it in our court. Whereas the municipalities will say it's actually an economic development
[01:18:54.000 --> 01:19:00.560]   effort and we should be doing as much as we can to build up our broadband because it'll entice
[01:19:00.560 --> 01:19:06.080]   people to come to our area. It also makes our current citizens lives better. So it may not be
[01:19:06.080 --> 01:19:13.440]   entirely profitable, but it is a service that we think is valuable. And those kind of nuances are
[01:19:13.440 --> 01:19:21.200]   really hard to convey in an objective way, right? And readers don't even want that. They want you to
[01:19:21.200 --> 01:19:27.600]   say exactly what I said and then say, I like to talk with which machine should I buy?
[01:19:27.600 --> 01:19:32.400]   Right. Oh my God. I'll be objective. Tell me what you're doing. Here's what Carl wrote.
[01:19:32.400 --> 01:19:36.960]   If you're a reporter and you feel the need to give an industry lobbyist ample room to make
[01:19:36.960 --> 01:19:42.160]   various claims, you should at least point out where that lobbyist might not be telling the truth.
[01:19:42.160 --> 01:19:46.480]   So your readers might have some vague idea where the truth actually is. That's kind of what you're
[01:19:46.480 --> 01:19:51.680]   saying. That's true. And that's I mean, when I would report from, you know, I'd be like, hey,
[01:19:51.680 --> 01:19:57.360]   AT&T says this. Here's why they're saying this and here's their agenda in this. Here's their dog
[01:19:57.360 --> 01:20:00.720]   in this house. Well, here's what they could get out of it or here's where they could benefit.
[01:20:00.720 --> 01:20:07.200]   Well, to support your argument that the telco spent a lot of money to get heard and consumers don't,
[01:20:07.200 --> 01:20:15.920]   telecom giants. This is a stunning, it's a joint or a study from common cause in the
[01:20:15.920 --> 01:20:26.560]   communication workers of America Union. Telecommunications lobbyists spent 234 million dollars on lobbying
[01:20:26.560 --> 01:20:33.680]   during the 116th Congress, the current Congress that's hard to spend $320,000 a day.
[01:20:33.680 --> 01:20:44.560]   Comcast the biggest 43 million AT&T 36 million. And so when you wonder why we have monopolies in
[01:20:44.560 --> 01:20:50.400]   broadband, why our broadband is worse than many other developed nations, more expensive, slower.
[01:20:51.920 --> 01:20:58.320]   It's because Congress is bought and paid for by these people. All of these people we voted for.
[01:20:58.320 --> 01:21:04.240]   The study said the largest ISPs have used their outsized influence in Congress to block any
[01:21:04.240 --> 01:21:09.920]   legislation that would undermine their stranglehold over the broadband marketplace.
[01:21:09.920 --> 01:21:14.720]   Net neutrality is just part of it. In fact, you when you bring up municipal
[01:21:14.720 --> 01:21:21.040]   broadband mapping, municipal broadband. That's a really good example because Philadelphia started
[01:21:21.040 --> 01:21:26.800]   a municipal Wi-Fi service that was really quite good. And it pissed off the telecoms so much they
[01:21:26.800 --> 01:21:33.920]   went to the Pennsylvania State Legislature and said, "Hey, you don't really want this and you
[01:21:33.920 --> 01:21:41.360]   don't want government competing with private industry. That's not a free market." And they've
[01:21:41.360 --> 01:21:46.240]   actually got that law passed not only in Pennsylvania now, but almost all, I think 28 states.
[01:21:46.240 --> 01:21:53.040]   Municipal Wi-Fi is dead because they would have a point if there were no monopoly. But when
[01:21:53.040 --> 01:21:58.080]   there's a monopoly, that's the issue. That's all they argue that there isn't a monopoly because it's
[01:21:58.080 --> 01:22:04.400]   it's facilities based. So you've got cable versus DSL. Believe me, we could talk about this all day
[01:22:04.400 --> 01:22:13.840]   long. And by the way, Comcast headquartered Philadelphia. AT&T, Texas. Texas has had like
[01:22:13.840 --> 01:22:20.960]   Texas utilities couldn't put up smart metering systems for the longest time because it was a
[01:22:20.960 --> 01:22:29.680]   municipally owned Wi-Fi system. Sorry, municipally owned wireless system. Yeah. Yeah. It's this is
[01:22:29.680 --> 01:22:34.960]   from the vice article quoting the Common Cause study. The groups found that one of the industry's
[01:22:34.960 --> 01:22:39.200]   top targets during the last Congress was the Save the Internet Act, which would have restored
[01:22:39.200 --> 01:22:44.960]   net neutrality. And the FCC Consumer Protection Authority stripped away during the Trump administration
[01:22:44.960 --> 01:22:51.040]   amidst a flood of empty promises. That's what Carl Bodie's talking about. The telecoms said,
[01:22:51.040 --> 01:22:57.920]   "Oh, no. If you don't do this net neutrality thing, we'll do all this investment. Jobs will be saved.
[01:22:57.920 --> 01:23:01.600]   They fired people. They reduced investment as soon as they didn't have to.
[01:23:01.600 --> 01:23:05.600]   Telecom lobbies also fought against the accessible affordable internet for all act,
[01:23:05.600 --> 01:23:10.320]   which included money to help fund the local community broadband. They successfully derailed
[01:23:10.320 --> 01:23:16.000]   the Resilient Networks Act and attempt to shore up the Puerto Rican network resiliency after prolonged
[01:23:16.000 --> 01:23:22.560]   outages after the hurricanes Irma and Maria. 83 million Americans live under a broadband
[01:23:22.560 --> 01:23:28.160]   monopoly, usually a regional cable giant like Comcast or Charter. Millions more, including us
[01:23:28.160 --> 01:23:36.560]   live under a duopoly consisting of either a love vice, an apathetic phone company or their local
[01:23:36.560 --> 01:23:42.240]   cable provider. The end result is obvious. Spotty access, slow speeds, high prices, and generally
[01:23:42.240 --> 01:23:46.160]   terrible customer service. One of the things I've been hearing about a lot from our listeners
[01:23:46.800 --> 01:23:56.880]   is that these companies have variable pricing from block to block depending on how much the
[01:23:56.880 --> 01:24:02.400]   market will bear. This is why Google was so effective. Google, actually, one of the most
[01:24:02.400 --> 01:24:09.600]   effective things it did was just by launching broadband service announcing pricing. Whenever it
[01:24:09.600 --> 01:24:16.320]   said it was going to go into an area, you would see the prices both lower and normalize across the
[01:24:16.320 --> 01:24:22.400]   spread. Yeah, little competition goes a long way. And by the way, that's the free market.
[01:24:22.400 --> 01:24:30.960]   That's called the free market. A little competition spurs innovation, price improvements.
[01:24:30.960 --> 01:24:38.640]   You want me to have a show Stacey complains about broadband and rails against the utilize.
[01:24:38.640 --> 01:24:44.160]   How's your fiber going? Your fiber's good. Yeah, it's lovely. I'm not on it right now,
[01:24:44.160 --> 01:24:48.000]   but it's great. I don't think it's great. You want to rank you on it.
[01:24:48.000 --> 01:24:52.480]   Because my Wi-Fi is okay. It's so good that you're not using it right now, huh?
[01:24:52.480 --> 01:24:55.760]   Well, you're on your fiber to the house.
[01:24:55.760 --> 01:25:02.400]   Well, I'm just not wired. Don't tell me that. I'll get very upset. I'm just trolling.
[01:25:02.400 --> 01:25:05.920]   No, I'm on my Euro Pro 6s. It's Wi-Fi 6s.
[01:25:05.920 --> 01:25:09.360]   Okay, I have to say it's been very good. It hasn't been problematic.
[01:25:10.240 --> 01:25:15.440]   Maybe they fixed that collision problem that we always had in the past. How about Jeff Bezos?
[01:25:15.440 --> 01:25:18.640]   I was about to ask you about that. How about Mr.
[01:25:18.640 --> 01:25:22.880]   We're talking about that. How about Mr. Clean in space yesterday?
[01:25:22.880 --> 01:25:26.160]   They acted as if it was somehow historic.
[01:25:26.160 --> 01:25:32.480]   Notice the big first human flight under a tiny little blue origin, first human flight.
[01:25:32.480 --> 01:25:39.760]   Of course. It ain't the first human flight. It's only 50 years after the more than that,
[01:25:39.760 --> 01:25:45.520]   almost 60 after the first human flight. The other thing though, and I hope you don't think I'm
[01:25:45.520 --> 01:25:53.120]   crude for saying this, but isn't it a little weird that this billionaire's rocket is so phallic?
[01:25:53.120 --> 01:25:58.320]   Okay. Is it just me? No, no, no, no. Here comes the best story. Go to slate.
[01:25:58.320 --> 01:26:00.800]   It makes sense to engineers. Why does Jeff Bezos' rocket?
[01:26:00.800 --> 01:26:07.040]   It's an article that's called Why Does Jeff Bezos' rocket look so much like a penis?
[01:26:07.040 --> 01:26:11.440]   We asked an actual rocket scientist. So they talked to a mess.
[01:26:11.440 --> 01:26:20.720]   So Lisa, I have to say, Jeff's going to go up into a little space-ish.
[01:26:20.720 --> 01:26:29.120]   And let's watch it. And the first thing she said is what? What are they writing in?
[01:26:29.120 --> 01:26:31.040]   But it worked.
[01:26:32.080 --> 01:26:39.840]   Okay. So let's see what the rocket scientist said. We spoke to Daniel Ramsbacher, a propulsion
[01:26:39.840 --> 01:26:46.960]   engineer at NASA's Goddard Space Center, who requested we clearly stayed up front. He is not
[01:26:46.960 --> 01:26:53.200]   speaking on behalf of his employer. Okay. Let's get right to it.
[01:26:53.200 --> 01:27:00.080]   Asks slate. Why is Jeff Bezos' rocket shaped so much like a male member?
[01:27:00.960 --> 01:27:05.520]   Daniel says, well, for lots of reasons, honestly, I mean, obviously most rockets are quite phallic.
[01:27:05.520 --> 01:27:10.640]   This one specifically, they took the approach of pretty much all the commercial space programs
[01:27:10.640 --> 01:27:15.280]   where they go suborbital so they don't go fully into orbit. Similar to what we at NASA did back in
[01:27:15.280 --> 01:27:22.320]   the 60s. I'm talking about the 1960s. So the rocket's relatively small. Everything in our
[01:27:22.320 --> 01:27:26.080]   new season more or less based on heritage, it's like, well, what have we used in the past?
[01:27:26.080 --> 01:27:30.960]   What can we leverage? How can we reduce risk or reduce costs? So the rocket is,
[01:27:30.960 --> 01:27:36.800]   would you say the reporter interrupts? Because apparently Matthew was getting a little nervous
[01:27:36.800 --> 01:27:43.200]   at this point. Would you say there's a heritage of shapes and rocketry leading toward Bezos' rocket?
[01:27:43.200 --> 01:27:45.840]   It seems to me this one's kind of a leap forward.
[01:27:45.840 --> 01:27:51.920]   No, well, specifically for this, this unusual rocket was the new shepherd. The new Glenn is
[01:27:51.920 --> 01:27:56.640]   their next launch vehicle, which is much larger and designed specifically for the capsule that
[01:27:56.640 --> 01:28:03.920]   sits on top of the new shepherd. Oh, so yes, the capsule sitting on top of it is designed for a
[01:28:03.920 --> 01:28:13.840]   larger, it gets better shaft for the head so that it won't. So the reason why it's a little bit
[01:28:13.840 --> 01:28:17.120]   larger is because the launch vehicle, the rocket, the next version is wider, what you're looking at
[01:28:17.120 --> 01:28:22.320]   in the rocket is basically a bunch of tanks, little tanks literally stacked upon one another.
[01:28:22.320 --> 01:28:28.400]   All that that's all it basically is. Yeah, that is some computers in the rocket engines at the base.
[01:28:28.400 --> 01:28:32.240]   They knew they weren't going in orbit, so they sized the tanks appropriately. It's a relatively
[01:28:32.240 --> 01:28:39.760]   small diameter. It had a circumscision. They designed the capsule for the next series of rockets.
[01:28:39.760 --> 01:28:45.280]   See, they didn't want to make the tanks too large. So all right, they reduced the
[01:28:45.280 --> 01:28:51.600]   girth of the shaft, but kept the capsule at the top the same size, which gave it that
[01:28:51.600 --> 01:29:00.400]   distinctive shape. Yeah, unfortunately, what advice would you give Bezos if he wanted to make the
[01:29:00.400 --> 01:29:05.840]   next stage of this project, the new Glenn or whatever, even more phallic? Oh, I would not do that.
[01:29:05.840 --> 01:29:13.760]   I think it's going to end up making a worse for him. What's next for the future of rocketry,
[01:29:13.760 --> 01:29:18.320]   the reporter asked, so there are any new developments that are going to make rockets look more like
[01:29:18.320 --> 01:29:25.680]   penises? No, I would say they're going to be more and more like just long slender tubes.
[01:29:25.680 --> 01:29:31.120]   This is probably the most phallic looking spacecraft you're going to see if I had to guess. Can I go now?
[01:29:31.120 --> 01:29:38.800]   Do you think this was the most hilarious? Do you? Everyone was wondering it.
[01:29:39.760 --> 01:29:45.680]   Do you think that will be Bezos's last legacy in space that he built the rocket that looked the
[01:29:45.680 --> 01:29:52.320]   most like a dick? I don't think so. Hopefully, you'll make it a little bit easier and cheaper to
[01:29:52.320 --> 01:29:58.960]   get into space. Well, competing visions, I guess, for the future there. Yeah. Just want to mention,
[01:29:58.960 --> 01:30:08.560]   I am not representing my employer. Okay. So I just wasn't just made. He made it back to work.
[01:30:08.720 --> 01:30:14.560]   Yeah. I hope so. There was a 1974 very soft core, a very soft core, poor movie called
[01:30:14.560 --> 01:30:20.640]   Flesch, Ford. Oh, yeah. And if you go to line 38, there was Flesch's rocket ship.
[01:30:20.640 --> 01:30:24.800]   Oh, interesting. And by the way, somebody has saved this unpinterist.
[01:30:24.800 --> 01:30:29.360]   Yes. Well, yeah, see the little curve, I think, at some point,
[01:30:29.360 --> 01:30:36.560]   Genesee Qua, but it doesn't look like Jeff could be in there. I don't want to dwell on this too much
[01:30:36.560 --> 01:30:42.480]   longer. Are we still going to have our E rating on Apple? I don't know. I'm telling you that I've
[01:30:42.480 --> 01:30:47.920]   been influenced. I've been influenced by the $20 million. Okay. All right. Yeah. I'm trying to do
[01:30:47.920 --> 01:30:52.960]   sexy talk. I've got here in J. I'll come in later. That you just did. We're at Pays.
[01:30:52.960 --> 01:31:01.840]   By the way, the tech workers are coming back to the Bay Area. Turns out maybe, you know, moving to
[01:31:01.840 --> 01:31:07.520]   Montana wasn't always cracked up to be. Or intended to like them. Yeah, maybe.
[01:31:07.520 --> 01:31:18.320]   They quoted a developer named Greg Osuri, who decided he had enough of the Bay Area. It's just a
[01:31:18.320 --> 01:31:24.720]   hell hole living here, he said, the founder of a cloud computing company called Akash Network.
[01:31:24.720 --> 01:31:30.160]   He left for his sister's townhouse in the suburbs of Columbus, Ohio. But by March,
[01:31:31.440 --> 01:31:37.040]   you guessed it. Mr. Osuri was itching to return. He missed city life, meeting new people, running
[01:31:37.040 --> 01:31:41.360]   to acquaintances on the street, getting drinks with colleagues. I guess he came back to that
[01:31:41.360 --> 01:31:46.160]   hell hole in April. I'm sorry. You know what? He probably missed like talking tech everywhere.
[01:31:46.160 --> 01:31:50.640]   I mean, because like whenever I'm in San Francisco, I have to admit I get really sick of it simply
[01:31:50.640 --> 01:31:56.320]   because I'm sick of like talking tech. Yeah. Talking tech. Oh, yeah. Yeah. But I mean, if you're
[01:31:56.320 --> 01:32:03.440]   if you're keen on that, it's probably a real letdown to meet real people. That's why I stay here in my
[01:32:03.440 --> 01:32:12.960]   hermetically sealed chamber. This I'm kind of stay on business for one second. I'm curious.
[01:32:12.960 --> 01:32:18.640]   Well, first, let us acknowledge is PR staff. Amazing.
[01:32:18.640 --> 01:32:21.360]   And work at all. Oh, they were terrible.
[01:32:22.640 --> 01:32:29.360]   Tell me for him to say thank you, everybody, for buying my all my employees. That was a big mistake.
[01:32:29.360 --> 01:32:34.720]   I didn't catch that. Yeah. So he he thanked Amazon customers and employees for allowing him to fly
[01:32:34.720 --> 01:32:41.680]   in a space for paying for. Yeah. He is he is said in the past that he considers himself a lottery
[01:32:41.680 --> 01:32:46.960]   winner for becoming the richest man in the world worth almost 200 billion dollars.
[01:32:49.120 --> 01:32:54.960]   Yeah. Yeah. That was a little tone deaf. And then the cowboy hat. It was very Dr.
[01:32:54.960 --> 01:32:59.360]   Strange love. It was a little weird, but he's ball that was playing to the crowd though. Right.
[01:32:59.360 --> 01:33:04.800]   Wasn't it? We didn't know where I had if you were going to go up 80 kilometers above the Earth's
[01:33:04.800 --> 01:33:09.440]   surface closer to the sun. Yes, I would cover this. Yeah, I got to cover the cover.
[01:33:09.440 --> 01:33:17.840]   Cover the dome, baby. Yeah, that was a little tone. But I think maybe PR probably doesn't have
[01:33:17.840 --> 01:33:22.160]   a lot to do with Jeff anymore. Right. But do you think he's got a full time? Not at the
[01:33:22.160 --> 01:33:28.480]   to fifth. Right. Wasn't that the day to fifth? Yeah, that's he's done. I'm sure. No, but
[01:33:28.480 --> 01:33:34.240]   the Amazon PR team is definitely like, gosh, dang it, Jeff.
[01:33:34.240 --> 01:33:39.280]   More. Son of a big kid. Why'd you do that to us?
[01:33:39.280 --> 01:33:46.000]   All right. But here's my question to you. If if you if money were and money in morals were not an
[01:33:46.000 --> 01:33:52.560]   object and the environment and everything else, would you take it? And if so, which ride would you take?
[01:33:52.560 --> 01:33:59.920]   I would 100% without morals. Yes, I would go into space. I am so eager to see our planet from
[01:33:59.920 --> 01:34:04.960]   about. Oh, yeah. I don't blame them for doing that. It's, you know, every boy's dream to sit at
[01:34:04.960 --> 01:34:11.920]   top of giant penis. I was going to say every person's dream. And then I'm like, Nope,
[01:34:11.920 --> 01:34:16.880]   Nope, you kept going. And I was like, Nope, you're right. You didn't want to go.
[01:34:16.880 --> 01:34:23.920]   I would I would take the trip as long as I'm coming back. It's that whole going to another planet
[01:34:23.920 --> 01:34:27.840]   and staying. Oh, no, no, that's a bad idea. No, I won't know part of that. But I would like
[01:34:27.840 --> 01:34:33.200]   figure I would like a couple of orbits. I'd go die on Mars. I'm not doing orbits either.
[01:34:33.200 --> 01:34:40.480]   Just just let me go up just just far enough to see Jeff more of how big this planet is. Mars is not
[01:34:41.120 --> 01:34:45.040]   the place to raise a kid. I hear it's cold as he's done raising kids.
[01:34:45.040 --> 01:34:48.800]   Oh, yeah. I'm saying I'm old. I'll go there and die. Yeah. That's my figure.
[01:34:48.800 --> 01:34:50.720]   So unfortunately, Mars does not need journalists.
[01:34:50.720 --> 01:34:55.520]   I mean, you just say, no folks did it die.
[01:34:55.520 --> 01:35:01.520]   No, I'm saying me. I'd go there and just say, okay, I know that I'm not coming back. It's okay.
[01:35:01.520 --> 01:35:06.240]   I'm old. Yeah. I know. Okay. Yeah. All right. All right. But which one which I'm starting
[01:35:06.240 --> 01:35:13.360]   is my question. Which one would you Oh, definitely Jeff's because he had bigger windows and his was
[01:35:13.360 --> 01:35:18.640]   less really that's your selling point. Yeah. Wait a minute. You're going up there to see the view.
[01:35:18.640 --> 01:35:25.760]   You're in that suite. He had better windows. His had done more launches. So we'll say for and he
[01:35:25.760 --> 01:35:30.880]   was environmentally better. Yeah, because it didn't use yet to use hydrozene or whatever it used,
[01:35:30.880 --> 01:35:36.800]   hydrogen. So yeah, I would definitely go to Jeff. I don't think the height makes that much
[01:35:36.800 --> 01:35:43.840]   difference. I hear it's the it's the girth. I'm stepping away from that. I'm gonna I'm gonna push
[01:35:43.840 --> 01:35:50.640]   you out of your chair and just push him off the set away. This is this is the time when we're just
[01:35:50.640 --> 01:35:57.360]   well again. I just give kudos to he and Branson and their staff for just further into research that
[01:35:57.360 --> 01:36:03.440]   just may make well I think we kind of knew how to do all that already. Right. I mean, it wasn't like
[01:36:03.440 --> 01:36:09.520]   we didn't do it. Maybe there's a way this would be cheaper down the road. Who knows? You know what?
[01:36:09.520 --> 01:36:15.040]   Let's be honest. It's all about creating a business quarter of million dollar ride business. Right.
[01:36:15.040 --> 01:36:20.400]   Although Bayzo and all of that stuff trickles down. People didn't didn't just jump on commercial
[01:36:20.400 --> 01:36:26.000]   air flights because they could back in the days. That was a line of privilege to be able to do that.
[01:36:26.000 --> 01:36:30.720]   So apparently the Washington Post is slagging Bezos for wearing the cowboy hat.
[01:36:30.720 --> 01:36:35.760]   Why? He's in Texas. He's in Texas.
[01:36:35.760 --> 01:36:44.160]   It could have worn a space helmet, I guess. Well, look up, look up, Dr. Strange Love cowboy hat.
[01:36:44.160 --> 01:36:49.200]   Oh, really? Oh, yeah. Oh, yeah. The final scene of Dr. Strange Love with the guys riding the
[01:36:49.200 --> 01:36:53.440]   Oh, yeah. Yeah. He is riding the bomb down. Yeah. Yeah. Yeah. That's
[01:36:53.440 --> 01:36:59.280]   what's his name? The great slim pickens. The great slim pickens.
[01:36:59.280 --> 01:37:08.000]   Right. I'm cowboy. Five gallon hat though says chicken and in the chat.
[01:37:08.000 --> 01:37:13.760]   It wasn't a thing. It wasn't a thing. They do say, and I think it's true in this case,
[01:37:13.760 --> 01:37:20.080]   all hat, no cattle. Right. Although maybe you never know, Jeff might own, you know, quite a bit of
[01:37:20.080 --> 01:37:25.440]   cattle. Let's do. Do they have ridden a horse? We don't know. Why these from Texas? His father
[01:37:25.440 --> 01:37:31.760]   was a world renowned unicyclist. Interesting. That's not going to do with cowboys. Interesting.
[01:37:31.760 --> 01:37:37.840]   Dribble basketballs at the same time. That is a skill. That is a skill. Let's do the Google
[01:37:38.720 --> 01:37:46.960]   changelog. Let's do let's take two. Let's do the Google changelog.
[01:37:46.960 --> 01:38:01.920]   There it is. DDoS Google has a new tool to defend against attacks launched by
[01:38:02.720 --> 01:38:09.520]   botnets. Actually, Google's had a project shield for some time, which is a free service from Google
[01:38:09.520 --> 01:38:14.240]   that protect human rights, government, and media organizations against DDoS attacks.
[01:38:14.240 --> 01:38:18.320]   Really, the best way to defend against a big DDoS attack is to have an even bigger pipe.
[01:38:18.320 --> 01:38:24.160]   That's how DDoS works is jamming the pipe. They unveiled a public preview, I think today,
[01:38:24.160 --> 01:38:31.680]   actually, of cloud armors, adaptive protection, a machine learning powered method of detecting
[01:38:31.680 --> 01:38:39.280]   and protecting enterprise applications and services from layer seven DDoS attacks.
[01:38:39.280 --> 01:38:46.880]   Layer seven is the application. I'm sorry. I'm not up to snuff on my OSI model.
[01:38:46.880 --> 01:38:52.000]   Oh, very nice. Very nice. Good job, aunt. Very good. It was all set.
[01:38:52.000 --> 01:38:56.880]   You were ready to jump in, Stacy? I can tell you what layer seven is.
[01:38:58.240 --> 01:39:02.640]   I don't. I think it's kind of I can tell you one and two, but between three and
[01:39:02.640 --> 01:39:07.600]   it's application of presentation. Oh, good. In an application or presentation? There you go.
[01:39:07.600 --> 01:39:11.280]   I think it's application layers. Okay. Yeah. Because basically,
[01:39:11.280 --> 01:39:13.920]   that's application layers. These are zenac floods and things like that.
[01:39:13.920 --> 01:39:18.880]   Well, five through seven is like the application layer, basically, isn't it? Let's see.
[01:39:18.880 --> 01:39:21.440]   I don't remember. I don't really care. I'm looking at it.
[01:39:21.440 --> 01:39:26.720]   Article recently said this, this OSI seven layer model has been really enough to do with real
[01:39:26.720 --> 01:39:34.720]   to change. You're right. Mr. Jarvis is like,
[01:39:34.720 --> 01:39:40.160]   layer seven application layer six presentation layer five presentation layer four transport
[01:39:40.160 --> 01:39:45.680]   layer three network layer two data length. There is no layer one. I need to.
[01:39:45.680 --> 01:39:48.880]   There is two. It's the it's the physical physical. Physical.
[01:39:48.880 --> 01:39:54.480]   Physical. Physical. Two is your Mac layer. Three is your network protocol kind of stuff.
[01:39:55.120 --> 01:40:02.880]   Well, so is four. But anyway, is a pretty picture of these seven layers of OSI,
[01:40:02.880 --> 01:40:09.680]   which honestly does not reflect modern networking. I think, but that's fine.
[01:40:09.680 --> 01:40:13.600]   Moving right along. Sorry about that. Sorry to.
[01:40:13.600 --> 01:40:22.160]   There is a big Chrome zero day. So get your Chrome updated kids. There's also a new version of
[01:40:22.160 --> 01:40:26.800]   Chrome Chrome 92 for iOS that has some nice features, including
[01:40:26.800 --> 01:40:33.760]   biometric unlock for incognito tabs. So you can if you don't want your mom to see what you've
[01:40:33.760 --> 01:40:40.160]   been looking at, unless your mom looks exactly like you, you can actually apparently Florence.
[01:40:40.160 --> 01:40:45.040]   I got there's that. Was it Georgia Dower Florence? Florence Ions mom looks exactly like.
[01:40:45.040 --> 01:40:49.520]   Oh, no, it was Rosemary orchard. Enough liker that she can unlike her phone.
[01:40:50.720 --> 01:40:55.920]   So you got to trust your mom, I guess. Also full page screenshot support. That's a very
[01:40:55.920 --> 01:40:59.680]   nice feature that you see in Android now where you have a page that you don't see the whole thing,
[01:40:59.680 --> 01:41:03.840]   but it'll screenshot the whole thing, which is very nice. So get your Chrome 92.
[01:41:03.840 --> 01:41:09.280]   So you had this happen to you, Jeff. I were locked out of your Chromebook.
[01:41:09.280 --> 01:41:14.160]   So what happened was people were getting locked on other Chromebooks and the warning went out.
[01:41:15.520 --> 01:41:21.440]   Don't update your machine. Don't turn it on again. Don't notice just yet.
[01:41:21.440 --> 01:41:27.920]   All right. And so, but because I have my old old old grandpa,
[01:41:27.920 --> 01:41:35.760]   Pixelbook and the batteries going was on, and I thought that the switch on my on my strip was
[01:41:35.760 --> 01:41:42.720]   on when it was off suddenly boom power gone. Because it goes down it goes from like 65% to zero.
[01:41:44.960 --> 01:41:50.480]   So, okay, turn back on. And I've always had this fear. What if I forget this password? I know this
[01:41:50.480 --> 01:41:54.880]   password, but I, you know, I go through lots of passwords. I use it password. I hope long,
[01:41:54.880 --> 01:42:00.160]   strong and random. Yes, it is. Right. But I've had variations on it over time. Right. So I can
[01:42:00.160 --> 01:42:05.760]   I can mix it up. I couldn't get it. Oh, I thought I blame myself. I thought, Oh, geez,
[01:42:05.760 --> 01:42:11.440]   no, they had a bad update. I guess variation and variation and variation and variation. And I then
[01:42:11.440 --> 01:42:18.160]   had to, and I gave up and I was afraid I didn't know the limit on the number of tries it was.
[01:42:18.160 --> 01:42:24.400]   So then I had to do forgotten password for my machine, which means I had to go through all kinds
[01:42:24.400 --> 01:42:30.720]   of McGillis all over. I had to do blood tests. I had to get x-rays. I had to do all kinds of things.
[01:42:30.720 --> 01:42:37.360]   And I got messed up in other strange ways. But because I could remember an old recent
[01:42:37.360 --> 01:42:42.400]   passport password, it let me restore otherwise it was about to say, okay, you can get your machine
[01:42:42.400 --> 01:42:46.080]   back fella, but I'm racing everything on it. It wasn't the big deal on a Chromebook. Right.
[01:42:46.080 --> 01:42:52.320]   Nonetheless, I remember the old password. I got it back and I thought, well, that was W. Jeff.
[01:42:52.320 --> 01:42:56.480]   And I'm not so sure it was done with Jeff. It might have been, but I think I think I got struck
[01:42:56.480 --> 01:43:02.720]   by this. When was this? It happened? It wasn't two days ago. I forget. I don't ever since the
[01:43:02.720 --> 01:43:09.360]   pandemic. I don't know what it is. This story is from yesterday. Google posted on the system
[01:43:09.360 --> 01:43:15.200]   status pages working on it was last night. Okay, good. Yeah, that's good. Ben, the fixes out.
[01:43:15.200 --> 01:43:20.080]   It's updated. They also pulled the update. What is it now? Yes. So if you go to the system status
[01:43:20.080 --> 01:43:25.680]   page, I think this is cool. They have status, not just of server issues, but of bugs and other
[01:43:25.680 --> 01:43:34.240]   problems. So fully resolved, it says users unable to log in after updating the 9104472.165.
[01:43:34.240 --> 01:43:37.680]   Sweet. That was fast. And if you got that update and you haven't yet installed it, don't.
[01:43:37.680 --> 01:43:43.120]   But you shouldn't, they've pulled it. But I don't know if you got locked out. I don't know how you
[01:43:43.120 --> 01:43:49.280]   get back in. Yeah, that's a problem. I mean, they probably have flags on those records.
[01:43:49.280 --> 01:43:53.840]   According to Google, the only options immediately regain access to your Chrome OS device
[01:43:54.560 --> 01:44:00.400]   will lead to the local data on the device being wiped. It does. If you can wait until the new fix
[01:44:00.400 --> 01:44:06.160]   is released, though, your device should automatically download that update as well.
[01:44:06.160 --> 01:44:10.720]   Maybe that'll fix it. Or you can manually trigger the update using the guest account
[01:44:10.720 --> 01:44:15.280]   if your computer is down to the bad update, but not installed it. Turn it off. Don't do it.
[01:44:15.280 --> 01:44:18.720]   And of course, you can always power wash. That's the beauty of these things.
[01:44:18.720 --> 01:44:23.840]   But one guy, one report said he couldn't. I don't know what the point would happen. But yeah, maybe
[01:44:23.840 --> 01:44:31.840]   he'd never, never power washed. I didn't know I was part of a, of a changelog story in my own life.
[01:44:31.840 --> 01:44:38.160]   I didn't realize it is what's going on. Google Maps now helps you avoid people on the train.
[01:44:38.160 --> 01:44:44.640]   That's a very specific feature for someone who's who's dreading going back to New York.
[01:44:44.640 --> 01:44:50.480]   There's many reasons I like that feature. Well, technically what it allows you to know is how
[01:44:50.480 --> 01:44:59.840]   crowded a subway car is. And they've expanded that feature to more than 10,000 transit agencies
[01:44:59.840 --> 01:45:06.720]   in over 100 countries. And in New York and Sydney, you can even see live crowd data down to each
[01:45:06.720 --> 01:45:11.040]   transit car. So you won't merely know how busy the platform is. You can actually see how
[01:45:11.920 --> 01:45:16.320]   busy the car is. I guess that would help you choose an empty your car, right?
[01:45:16.320 --> 01:45:19.040]   You can even sit in the first time.
[01:45:19.040 --> 01:45:24.800]   Go ahead. No, I was just gonna say that headline. I refused to click on it just because of that
[01:45:24.800 --> 01:45:29.040]   headline. Because I knew it was like, there's no way this is worth it. So look at it.
[01:45:29.040 --> 01:45:32.000]   Look at it. You didn't get my click. Look at it. Here it is. This is the
[01:45:32.000 --> 01:45:37.520]   Jamaica train. The least crowded cars are the 865 and one car.
[01:45:37.520 --> 01:45:43.200]   That's 10 car train. That's pretty cool. Yeah, that's cool. We think this is cool, but it's also
[01:45:43.200 --> 01:45:49.120]   surveillance. Yes. Oh, no. How would they know that? Because it's just saying how many people are.
[01:45:49.120 --> 01:45:57.600]   I'm just relaxed. Time for the Mortal Panic bug. I don't have it here. I'm not to draw on it.
[01:45:57.600 --> 01:46:05.760]   So I went into the city last Thursday for the first full day. I was honestly, completely drained,
[01:46:05.760 --> 01:46:10.880]   completely exhausted. I slept for nine hours. I took a nap. The emotionally exhausted or everything.
[01:46:10.880 --> 01:46:20.000]   Commuting in the feeling of the city was sad. The nerves, was it kind of quiet or were there?
[01:46:20.000 --> 01:46:23.440]   No, no, no, that would have been better. It's just the things that are shut down.
[01:46:23.440 --> 01:46:28.800]   Yeah. There's a lot of businesses shuttered in our neck of the woods as well.
[01:46:28.800 --> 01:46:32.720]   I love New York. I always love New York. But the smell of the sound that I was used to before,
[01:46:32.720 --> 01:46:37.680]   something I wasn't used to. The people around me that mass on. Now you know why Lisa thinks New York
[01:46:37.680 --> 01:46:43.280]   smells. You could finally smell it. She's right. Lisa, you're right. You and I are just used to it.
[01:46:43.280 --> 01:46:50.160]   We like that smell. Google Maps, though, be careful when you follow directions because,
[01:46:50.160 --> 01:46:57.520]   according to some, they offered some bad suggestions. In fact, a potentially fatal hiking trail in
[01:46:57.520 --> 01:47:02.720]   Scotland. This is so silly. What? Because if you are,
[01:47:02.720 --> 01:47:10.160]   no one should use Google Maps while they're hiking. You need to use a dedicated trail app
[01:47:10.160 --> 01:47:15.840]   because there's not GPS. There's not as many people. I mean, they don't send their vehicles out there.
[01:47:15.840 --> 01:47:23.680]   It just strikes me as like, if you are going to go into the wilderness, which, if you're doing a,
[01:47:23.680 --> 01:47:29.600]   like a real hike, then you need to prepare for it. You can't just grab. I mean, there's so many
[01:47:29.600 --> 01:47:33.360]   people getting rescued every year because they just walk into the wilderness with their smartphone.
[01:47:33.360 --> 01:47:37.600]   Yeah, but those people aren't dedicated hikers either. They're just sort of casual.
[01:47:37.600 --> 01:47:40.240]   This is how we're at on vacations. I know, but if you're good.
[01:47:40.240 --> 01:47:46.720]   So there's come the John, your trust and mountaineering Scotland, the national representative body
[01:47:46.720 --> 01:47:52.960]   for mountaineering hill walking climbing and snow spots warned that hiking routes to Ben Nevis,
[01:47:52.960 --> 01:47:57.520]   Scotland's second highest mountain could put people at risk. They cite a route being provided
[01:47:57.520 --> 01:48:03.680]   on Google Maps, which directs users to a car park at the head of Glen Nevis, all fair, but then
[01:48:03.680 --> 01:48:09.360]   displays a dotted line that appears to show the path to the top of the mountain. The path,
[01:48:09.360 --> 01:48:15.520]   well, this is what Heather Morning had to say. Even the most experienced mountaineer would have
[01:48:15.520 --> 01:48:22.640]   difficulty following this route. The line goes, you know, if it's in Scotland, I have to do this.
[01:48:22.640 --> 01:48:27.760]   He's going to do it. The line goes through very steep, rocky and pathless terrain,
[01:48:27.760 --> 01:48:31.760]   where even in good visibility would be challenging to find a safe line.
[01:48:31.760 --> 01:48:39.600]   And it looked cloud and rain and the suggestive Google line is potentially f early.
[01:48:41.760 --> 01:48:45.360]   So don't take that route. Okay, just because it's there.
[01:48:45.360 --> 01:48:51.040]   That's not good. I tried to say to you, what is it? Hey, do you, does anybody use Google? What are
[01:48:51.040 --> 01:49:00.560]   you talking about? Does anybody use Google bookmarks? Yeah, I mean, I just saw this. I'm very
[01:49:00.560 --> 01:49:04.640]   upset about this. I mean, you know, he wants to actually maps. I don't know why maps is in here.
[01:49:04.640 --> 01:49:09.920]   Like bookmarks when you go bookmark a site, they're going to get rid of that. No, that's not what
[01:49:09.920 --> 01:49:16.880]   that means. The service will no longer be supported after September 30th. To save your bookmarks,
[01:49:16.880 --> 01:49:21.760]   click on export bookmarks. I didn't even know Google had a bookmarking service.
[01:49:21.760 --> 01:49:29.600]   You go up to the bar and there's a car there. No, no, no, no. That's your local bookmarks.
[01:49:29.600 --> 01:49:33.840]   This is a sync across your account, too. Yeah, but this is across everything.
[01:49:33.840 --> 01:49:36.320]   Yeah, but that's I don't I don't think that's what they're talking about.
[01:49:36.320 --> 01:49:40.640]   So unclear and it's it has to be very upset. Now I have to log into Google just to show you.
[01:49:40.640 --> 01:49:44.480]   Now I got to actually look up with the stories.
[01:49:44.480 --> 01:49:50.960]   I'm not talking about my god, you guys are driving me. Okay, people are so Google bookmarks is
[01:49:50.960 --> 01:49:57.920]   a service that predated Google's bookmarks sync in Chrome. Yeah, this is this is like
[01:49:57.920 --> 01:50:03.040]   delicious. Remember delicious? Oh, yeah. Or pinboard. Oh, what effect browser bookmarks?
[01:50:03.040 --> 01:50:08.640]   In fact, I don't have I don't have very many bookmarks in here. The ones I do.
[01:50:08.640 --> 01:50:12.240]   Actually, I'm kind of glad I have these are really old from like.
[01:50:12.240 --> 01:50:19.280]   And by the way, some of them are from maps. That's why I guess when you pin something in a map,
[01:50:19.280 --> 01:50:23.360]   it actually makes it. How do you find out what's that? What's that you or all you go to?
[01:50:23.360 --> 01:50:27.440]   So I click it and it will take me to. No, no, no, no, that was how did you get this list?
[01:50:27.440 --> 01:50:35.200]   Bookmarks.google.com. So for some reason, I bookmarks this hotel in Germany, I guess I was staying
[01:50:35.200 --> 01:50:42.640]   there. Or here I bookmarked. Is this your address? 11301 domain drive Austin, Texas, 78758.
[01:50:42.640 --> 01:50:48.800]   Why a book like that? That's a shopping center in Austin. Oh, you're right. Yeah.
[01:50:48.800 --> 01:50:54.400]   It's like Nordstrom's or Sax. Yeah, it's right next to the Shake Shack. I might have been going
[01:50:54.400 --> 01:50:59.360]   there. I don't know. Oh, yeah. I know exactly where that is. Yeah. I don't even know how this
[01:50:59.360 --> 01:51:05.520]   got in here, but I have one in it. I have a college world series, Colorado Rockies Clemson
[01:51:05.520 --> 01:51:12.880]   Monitor.com. My most recent is from 2019. My oldest is from it looks like 2011.
[01:51:12.880 --> 01:51:19.200]   So these are pretty old. I bookmarked the Harper concert hall in Reykjavik. I've never
[01:51:19.200 --> 01:51:24.080]   been there, but I want to go there. Oh, wait a minute. Maybe you want to go? I do have older ones.
[01:51:24.080 --> 01:51:29.040]   Apparently I can't book loads. You use this service. Apparently I bookmarked pages.
[01:51:29.040 --> 01:51:35.600]   You used everything. I got my spaces on here. How old is it? He uses more than once. Wow.
[01:51:35.600 --> 01:51:44.000]   Wow. This interesting. Yeah. You're promising me my bookmarks, which are vital for book research
[01:51:44.000 --> 01:51:49.840]   that I have coming up are safe. Yes. This is not the bookmarks on Chrome. That's something
[01:51:49.840 --> 01:51:54.320]   this is a really old bookmarks of mine, which is actually fascinating.
[01:51:54.320 --> 01:52:00.800]   This is, you know, who knew this? Where are you? 10 years ago. Yeah. It's kind of cool.
[01:52:00.800 --> 01:52:06.400]   Like this is where our website was running on Softlayer way back in 2011. That's a Texas company.
[01:52:06.400 --> 01:52:10.960]   There's probably links in there too. I love Facebook and Twitter is cool. Yeah. Yeah.
[01:52:12.000 --> 01:52:19.920]   I'm surprised to see my space. I must say, I must say. Here's friend feed. My
[01:52:19.920 --> 01:52:25.040]   friend feed. My link on friend feed, which for some reason, did they Facebook by friend feed?
[01:52:25.040 --> 01:52:30.160]   I think they did. That's right. They bought him. Yeah. Well, they didn't really buy him either.
[01:52:30.160 --> 01:52:36.800]   Yeah. Anyway, so if you use bookmarks, export them, they'll be gone September 30th. I actually
[01:52:36.800 --> 01:52:40.560]   kind of fun to take that trip down memory. Yeah. Are you going to export them?
[01:52:40.560 --> 01:52:45.600]   Or just introduce it to me because I didn't even know this existed. Most of what's on there for
[01:52:45.600 --> 01:52:50.240]   me is from maps, starts places on maps. Okay. So that's why they mentioned maps. Okay.
[01:52:50.240 --> 01:52:56.800]   Good news. Thank God. Google has redesigned its emoji to be more authentic.
[01:52:56.800 --> 01:53:01.200]   Well, it's the process of doing. Oh, we talked about emotions.
[01:53:01.200 --> 01:53:05.440]   We're going to change the bikini is. The bikini emoji will be more authentic.
[01:53:06.080 --> 01:53:12.480]   I think that's all about empowering women, probably the pie emojis. The pie emoji
[01:53:12.480 --> 01:53:18.240]   looks like an American pumpkin pie right now. That's fine for Americans, but in the UK,
[01:53:18.240 --> 01:53:24.560]   the design looks more like a tart. The new more universal design should fix this according to the
[01:53:24.560 --> 01:53:31.520]   verge. Okay. Okay. The bikinis. I always get confused about this. Here's a moji where universal.
[01:53:33.280 --> 01:53:39.520]   Yes. Google's emoji different. I'm so glad you asked. I know you.
[01:53:39.520 --> 01:53:46.880]   Emoji Pedia emoji. Pedia is the place to go. So the way it works, the Unicode consortium,
[01:53:46.880 --> 01:53:52.640]   the folks who define the emojis, which Jenny Aitley is a member of. Yes. As is Jeremy Broome,
[01:53:52.640 --> 01:54:00.560]   who is the creator of emoji Pedia. They only give you a text description and a suggested emoji,
[01:54:00.560 --> 01:54:08.240]   right? But every company can design its own. So in fact, if you go to emoji Pedia, you can see
[01:54:08.240 --> 01:54:14.480]   what apples look like versus clubhouse versus discord versus Facebook versus Google. Look at
[01:54:14.480 --> 01:54:21.120]   all these. Each one of them has their own style and look of emojis. And so you could pick the
[01:54:21.120 --> 01:54:29.200]   the one you like the best. These are apples emojis. And when you when you put an emoji in your email,
[01:54:29.920 --> 01:54:38.480]   you get it's a unique character. No, it's rendered by the recipient. That's right. You saw or what
[01:54:38.480 --> 01:54:45.760]   as what they see what you basically send is a Unicode number. And that number corresponds
[01:54:45.760 --> 01:54:52.560]   to the emoji, but does not say anything about this. I could put in a good decent pie emoji or
[01:54:52.560 --> 01:54:56.400]   bikini emoji. But the other person could see something very different from what I thought
[01:54:56.400 --> 01:55:01.360]   they're going to see. Yes, that's right. Oh, I know I can take it. I mean, it will be it'll still be a
[01:55:01.360 --> 01:55:05.600]   pie, Jeff. It just won't be the pie you're looking at. But we're like, I don't know the bikini was
[01:55:05.600 --> 01:55:08.720]   going to be improved. There could be something wrong with the old bikini one. I'm now nervous.
[01:55:08.720 --> 01:55:13.840]   Well, the old one apparently had an invisible person wearing the bikini and right, they decided,
[01:55:13.840 --> 01:55:20.240]   well, that's too scary. So now we'll have a person, I guess. I don't want to put a person in the bikini
[01:55:20.240 --> 01:55:26.160]   now. You know, because they used to just yeah, I'll have to read the article. Be the floating
[01:55:26.160 --> 01:55:30.240]   garment. I saw the word. Okay, keep going on the change log.
[01:55:30.240 --> 01:55:31.280]   We go garment.
[01:55:31.280 --> 01:55:40.960]   It's a ghost in the bikini. So the bikini emoji no longer looks like it's being worn by an
[01:55:40.960 --> 01:55:45.840]   invisible person. The face mask emoji now shows a face with its eyes open instead of closed.
[01:55:45.840 --> 01:55:53.360]   Because masks have become a universal way of showing kindness to others rather than a symbol.
[01:55:53.360 --> 01:55:57.280]   True that used to show some a looking say interesting and then it shows them a happy.
[01:55:57.280 --> 01:56:03.520]   So look, here's the old one. I'm sick of you, Brad. I'm happy because I'm saving the world.
[01:56:03.520 --> 01:56:08.640]   Interesting. Yeah. So it looks self righteous. No.
[01:56:08.640 --> 01:56:13.840]   I'm so sorry, you guys. I'm just going to stop.
[01:56:13.840 --> 01:56:22.160]   This is not the first time. There's actually a list of times Google has tweaked its emojis.
[01:56:22.480 --> 01:56:28.000]   Yes. Yeah. Its emojis are not as good as other people. I will be. I mean, as someone on an
[01:56:28.000 --> 01:56:34.000]   Android phone, I have some serious emoji envy. Yeah. Yeah, that's a question. Who do you like?
[01:56:34.000 --> 01:56:38.240]   Remember, they used to have these thumbs when the thumb tips emojis, but yeah,
[01:56:38.240 --> 01:56:44.000]   Googles are very material design. Very flat. Who has the best of those apples that you were
[01:56:44.000 --> 01:56:48.480]   looking at? No, no, no, no, no, Google's looks just like that. This is no apples look better than
[01:56:48.480 --> 01:56:53.760]   Google's. Yeah, yeah, yeah. This is Google. There's charts that will compare them. Apple's more
[01:56:53.760 --> 01:57:00.320]   kind of 3D. Oh, I see it has a little more shade. Yeah. Okay. Yeah. And there's, I think the apples
[01:57:00.320 --> 01:57:05.120]   have more detail. Although I've always had a complaint with a dancing woman, I feel like her
[01:57:05.120 --> 01:57:11.040]   skirt is a little too short. I just, I just want to say the apple has made it a little
[01:57:11.040 --> 01:57:16.560]   more provocative. She's she's risque. Did you ever find it? Did you ever find it?
[01:57:16.560 --> 01:57:23.360]   I've never seen it. I've never seen it. I've seen it. See, apples, see these emojis,
[01:57:23.360 --> 01:57:26.560]   they have little books in front of them. Yeah, of course they do.
[01:57:26.560 --> 01:57:30.240]   There's the ninja. Wow.
[01:57:30.240 --> 01:57:35.760]   Wow. I love emojis. Hell a lot. Well, Unicode has what? 16,676?
[01:57:35.760 --> 01:57:40.000]   All of these things just really make me feel old as I can never understand them in
[01:57:40.000 --> 01:57:46.320]   the context day to day because they're not all obvious. Doesn't Queen Pruitt send you messages
[01:57:46.320 --> 01:57:49.760]   and emojis? Yes, which is why I never understand.
[01:57:49.760 --> 01:57:55.280]   Don't your children send you emojis? They know not to because I don't understand.
[01:57:55.280 --> 01:57:59.360]   Oh, you're so old. Dead. See how nice the fruits and the vegetables look in the animal.
[01:57:59.360 --> 01:58:03.200]   Their animals look great. Their animals are so real. So good.
[01:58:03.200 --> 01:58:06.800]   That's great detail. Yeah. They really, I think apple takes great care.
[01:58:06.800 --> 01:58:12.320]   Look at that donut. Doesn't it make you just want a donut? Here's apples. Come here. That's the
[01:58:12.320 --> 01:58:20.320]   googles. Look how crappy googles looks. Samsung's okay. Microsoft might take the cake for crappy.
[01:58:20.320 --> 01:58:25.120]   There's no cake in that donut. WhatsApp has pink icing.
[01:58:25.120 --> 01:58:30.240]   Microsomes today have bigger fish. Twitter is also very kind of abstract. Facebook's is the
[01:58:30.240 --> 01:58:33.840]   most realistic. You could that you could taste that donut, can't you?
[01:58:33.840 --> 01:58:39.120]   Sky News is a strange podcast that just heard where people just talked about nothing but donut
[01:58:39.120 --> 01:58:43.920]   emojis. You know, if something really happened like lightning and thunder,
[01:58:43.920 --> 01:58:48.320]   then I could imagine having a real conversation. But just out of nowhere,
[01:58:48.320 --> 01:58:53.920]   they spent, I don't know, half an hour. Not on donuts. That might be okay. But donut emoji.
[01:58:53.920 --> 01:58:57.440]   I ain't gonna kid you, Jeff. That's $60 million is calling.
[01:58:57.440 --> 01:59:03.440]   Or you could or you could start one for some stack, you know. Oh, yeah. They're doing a podcast
[01:59:03.440 --> 01:59:07.840]   network, aren't they? Yeah. Yeah. Sub-stake controversial people. Yeah.
[01:59:08.480 --> 01:59:17.360]   Google is rolling out a new feature that you might like, which I had for this show. Delete the last 15 minutes.
[01:59:17.360 --> 01:59:25.200]   Oh, yeah. Well, in this case, you're gonna go now. It's gonna be really interesting.
[01:59:25.200 --> 01:59:31.040]   In this case, it's a search history. So this is another one where if mom can unlock your phone,
[01:59:31.040 --> 01:59:35.040]   you might want to take advantage of this. I heard there's good sadaoku videos.
[01:59:35.040 --> 01:59:41.200]   Yeah. I forgot where they are. I'm gonna search for that again.
[01:59:41.200 --> 01:59:47.760]   Oh, no. And I watched that whole day. Honestly, that, you know, now that I know there's actual money
[01:59:47.760 --> 01:59:57.280]   in podcasting, I'm gonna change the whole whole tone of this show. And that's the Google change log.
[02:00:00.640 --> 02:00:08.640]   Oh boy. So Google change log, we had OSI model. We had a lot of rat holes, fake donuts and what else?
[02:00:08.640 --> 02:00:14.640]   Rabbit hole. Did you guys last week talk about Anthony Bourdain and the deep-faked voice?
[02:00:14.640 --> 02:00:19.200]   No, sir. That was only on the show. No, because it had come out like during the show or right after.
[02:00:19.200 --> 02:00:24.000]   By the way, isn't deep fake a little strong? Is it was an electronic personation? Right.
[02:00:24.000 --> 02:00:28.960]   So they had higher nectar deep-faked makes the sound so it's so it's no, it wasn't an actor.
[02:00:28.960 --> 02:00:35.360]   It was done. I don't know what to say. Yeah. So it's a documentary called Roadrunner,
[02:00:35.360 --> 02:00:41.040]   a film about Anthony Bourdain, who I loved, really did admired greatly.
[02:00:41.040 --> 02:00:49.440]   The film had the usual, you know, clips and so forth. But there were three places where the
[02:00:49.440 --> 02:00:54.000]   filmmaker Morgan Neville said he wanted to use material born. Bourdain had written. So it was
[02:00:54.000 --> 02:01:00.960]   his words. But he never said him out loud. So he went to a software company. And this was
[02:01:00.960 --> 02:01:04.960]   something we talked about before, which is adding and Google's been good at this, but there are
[02:01:04.960 --> 02:01:10.160]   a number of companies now that do this where they take a speech synthesized by a computer,
[02:01:10.160 --> 02:01:13.680]   but you know how lifeless and robotic that sounds and then apply an actor's
[02:01:13.680 --> 02:01:19.760]   prosody on top of it to make it sound real. That's what prosody. Oh, how fast they say.
[02:01:19.760 --> 02:01:25.840]   They're inflections in the way they say they're. So, so radio pro of you. Whoa.
[02:01:25.840 --> 02:01:35.760]   Now how much would you pay? So $10. Yeah, it's going down. It's a problem.
[02:01:35.760 --> 02:01:42.800]   So, and so I'm really intrigued by this because they basically made it sound like Anthony Bourdain
[02:01:42.800 --> 02:01:50.320]   was saying these words. It's only in three different clips. And frankly, I guess they revealed it.
[02:01:50.320 --> 02:01:54.880]   Otherwise, we wouldn't probably wouldn't even know. No, they didn't reveal it. Someone realized
[02:01:54.880 --> 02:02:00.160]   that the things couldn't have been said by him. Who was it who asked? It was it was actually a
[02:02:00.160 --> 02:02:06.320]   journalist realized it called or emailed the director and they admitted it. So it wasn't actually
[02:02:06.320 --> 02:02:13.520]   a. So they hit it. Well, I think the director didn't think of it as. I mean, I still think they
[02:02:13.520 --> 02:02:18.240]   were wrong to not disclose, but I am a journalist, not a documentary filmmaker. So I don't know
[02:02:18.240 --> 02:02:23.600]   what rules they work under. Yeah, maybe I think the best thing to do would be leave that out,
[02:02:23.600 --> 02:02:31.280]   to be honest. But you know, documentaries, if you we forget how easily the filmmaker can
[02:02:32.560 --> 02:02:38.720]   use the documentary to change the whole film. Still wants to make a story. Yeah, it's just a
[02:02:38.720 --> 02:02:46.720]   documentary. New York's New Yorker, staff writer Helen Rosner noticed one line of Bourdain's dialogue
[02:02:46.720 --> 02:02:53.760]   that turned out to be computer generated and asked him about it. He said, you probably don't know
[02:02:53.760 --> 02:02:58.960]   what the other lines are that were spoken by the AI and you're not going, though. We can have a
[02:02:58.960 --> 02:03:05.840]   documentary ethics panel about it later. That's a little salty. Yep. Yep. So that sounds like he
[02:03:05.840 --> 02:03:09.760]   you should never get salty with the press on something like that, especially what they're going
[02:03:09.760 --> 02:03:16.080]   to do. Yeah, they're going to get revenge. Yeah. And they're not even going to feel like they're
[02:03:16.080 --> 02:03:22.240]   getting they're not even going to feel like bad about it. If this was disclosed beforehand,
[02:03:22.240 --> 02:03:28.640]   does this does it discredit the issue? It should say on the screen. So everybody voice impersonated
[02:03:28.640 --> 02:03:35.040]   or something. I wouldn't use it. I think it's wrong to use it, period. You could show on. You could
[02:03:35.040 --> 02:03:37.600]   look, there's lots of ways to put a caption up and put music on your mind. You could put a caption
[02:03:37.600 --> 02:03:43.040]   up and have and have David Attenborough read it. Yeah. I mean, there's there's lots of ways to do
[02:03:43.040 --> 02:03:50.480]   a lot of ways to do it. Yeah. Yeah. Don't have to make it fake and real. But believe it, believe me,
[02:03:50.480 --> 02:03:56.240]   this is going to be a regular occurrence. This is just the beginning. Well, last week we went over
[02:03:56.240 --> 02:04:04.400]   the audio things that you had done on on Twitter before of the voices and how good they are now.
[02:04:04.400 --> 02:04:11.120]   Yeah. And I really gotten very thing of the things meant to speak Polish that when you give them
[02:04:11.120 --> 02:04:15.680]   English words sound like a Polish accent. Yeah. Was that interesting? Yeah. I was so great. Yeah.
[02:04:15.680 --> 02:04:24.000]   I think that podcast you really want to talk about that. Well, it's kind of interesting because
[02:04:24.000 --> 02:04:30.560]   it's controversial nowadays controversial people. So Bob Garth was a friend of mine over the years,
[02:04:30.560 --> 02:04:37.120]   got fired from on media at WNYC public radio. He did. Oh, I used to be angry.
[02:04:37.120 --> 02:04:43.760]   Oh, for being angry. He was that was inappropriate. I mean, firing him was inappropriate, I think.
[02:04:43.760 --> 02:04:49.840]   What did he buzzy angry about? I forget. Well, never kind of really came out. And so I don't know.
[02:04:49.840 --> 02:04:56.160]   And clearly the two hoes yelled a little bit. You know, so then there's John McWhorter,
[02:04:56.160 --> 02:05:01.680]   who's work we like, but who's very controversial these days. I love Garfield. I will be glad.
[02:05:01.680 --> 02:05:08.880]   I will be glad to hear him come back, actually. So it's a fire from New York public radio in May
[02:05:08.880 --> 02:05:14.240]   for violating its anti bullying policy. All right. Okay, that's a policy. That's a bully people.
[02:05:14.240 --> 02:05:18.720]   Shouldn't shouldn't make people cry. I've made many a producer cry. I regret it now.
[02:05:19.680 --> 02:05:26.400]   But they cry so easily. Have you noticed that? Oh, don't be a jerk. I have had so many editors
[02:05:26.400 --> 02:05:34.960]   be a jerk to me. I'm not even gonna. Yeah. Did they make you cry Stacy? I'm sorry. I'm sorry.
[02:05:34.960 --> 02:05:39.840]   They did. I apologize. You're not empowered. I had one that started crying in front of me.
[02:05:39.840 --> 02:05:46.400]   And that was just really hard. Yeah, that's bad. But I probably have been. Yes, I've had people
[02:05:46.400 --> 02:05:52.160]   make me cry. Yes, I have. And I quit over it too. Good for you. Good for you. They shouldn't
[02:05:52.160 --> 02:05:56.640]   ever talk to you like that. Then the CEO called me back and told me not to resign. And they would
[02:05:56.640 --> 02:06:01.760]   like, we're very sorry. Substack sees the venture. It's called book smart studios
[02:06:01.760 --> 02:06:06.880]   as a way to deepen its commitment to podcasting on its platform.
[02:06:06.880 --> 02:06:11.680]   Well, some stats doing as it has Andrew Sullivan, it has it. McWhorter gold.
[02:06:12.560 --> 02:06:18.560]   It's yes. Okay. Because I know he's not doing the Mexican Valley anymore. John McWhorter.
[02:06:18.560 --> 02:06:23.040]   So he's taking on controversial people is becoming a substack thing.
[02:06:23.040 --> 02:06:27.760]   Well, to be clear, I don't think McWhorter is controversial. In some quarters, he is.
[02:06:27.760 --> 02:06:35.360]   Culture stuff. Yeah. Really? Yeah. Oh, yeah. He signed the Harper's letter. Oh, he's out there
[02:06:35.360 --> 02:06:39.120]   doing a lot of talking about race in ways that I didn't realize.
[02:06:39.920 --> 02:06:44.160]   Gosh. People find controversial. Yeah. I feel bad now. I like to. I like this.
[02:06:44.160 --> 02:06:48.080]   I'm not saying is no, it's not whether he's right or wrong. I'm not saying that's not
[02:06:48.080 --> 02:06:52.000]   taking sides controversial. I'm just saying, you know what? If you want to make money in
[02:06:52.000 --> 02:06:56.560]   podcasting, you better hire some controversial people. That's where we are. My God. My God.
[02:06:56.560 --> 02:07:04.560]   Yeah. No, look, as a longtime radio guy, tell me a radio, a good radio host, a successful radio host,
[02:07:04.560 --> 02:07:08.320]   who's not controversial. That's true. That's how you that's how you get ready.
[02:07:08.320 --> 02:07:12.960]   Just going to move the needle. You fire people fire people up. Yeah. That's what happened to radio.
[02:07:12.960 --> 02:07:20.160]   Or talk radio. I'm sorry to say. That's what's happening to talk news.
[02:07:20.160 --> 02:07:24.400]   I think it migrated from talk radio into talk news. I think that's exactly what happened.
[02:07:24.400 --> 02:07:31.680]   Everybody sort of started with rush and right wing talk radio and getting people upset and angry.
[02:07:31.680 --> 02:07:37.760]   And that worked so well in radio. People said, I am. Well, you also had trash. You had Oprah.
[02:07:37.760 --> 02:07:40.720]   Oh, he's already in trash TV. Robert Downey Jr.
[02:07:40.720 --> 02:07:47.360]   Morton Danny, Jordan Robert. I was like, what an iron man contribute to this.
[02:07:47.360 --> 02:07:54.880]   Morton down. It wasn't his name. Morton Danny Jr. Yeah. He used to have a talk show
[02:07:54.880 --> 02:08:01.200]   that people would throw chairs all the time. Well, people don't realize is the is the Oprah
[02:08:02.000 --> 02:08:09.520]   Phil Donnie who started the whole TV talk show. Now I'm talking history. Daytime. Daytime. Daytime.
[02:08:09.520 --> 02:08:14.880]   Yeah. Talk show. Yeah. And Oprah took the format and made it even bigger. Yeah.
[02:08:14.880 --> 02:08:21.120]   And then she started getting pretty trashy. And then a lot of people started copying her.
[02:08:21.120 --> 02:08:26.240]   Ricky Lake, a whole bunch of shows. And then suddenly she decided to see the burning bush.
[02:08:26.240 --> 02:08:30.880]   I actually refused to be on the show. She wanted me to be on the show
[02:08:30.880 --> 02:08:40.400]   for her kind of seeing the light. And I see. So interesting. The sub stack podcast will be free
[02:08:40.400 --> 02:08:46.640]   as ours is, but they also, as we do, will offer a subscription for $7 a month, the same amount we
[02:08:46.640 --> 02:08:50.800]   charge. The most for listening to you to be a founding member. It's a nice number.
[02:08:50.800 --> 02:08:56.000]   founding members will get additional perks like merchandise, custom video and voice messages.
[02:08:58.960 --> 02:09:01.520]   Leo's thinking, Oh God, do I have to record 100 voice messages?
[02:09:01.520 --> 02:09:07.840]   Substack will take a 10% cut of podcast. Oh, this is their newsletter model too. They're hiring
[02:09:07.840 --> 02:09:14.960]   people, but they want others to come along for free. They'll take a 10% cut of revenue. And then
[02:09:14.960 --> 02:09:19.040]   each once it's advanced payment expires after the first year, each advanced payment offers
[02:09:19.040 --> 02:09:24.240]   individually tailored. They got some big names. They're saying six. I guess it's, I mean, it's,
[02:09:24.800 --> 02:09:31.520]   it's my model podcast and newsletter. Yeah, exactly. Exactly. We found many writers want to at least
[02:09:31.520 --> 02:09:37.840]   try podcasting or wish to supplement their written material with audio. And you can already do that.
[02:09:37.840 --> 02:09:42.160]   I'm casting is easy. Yeah, it is. You think I'd be doing it if it weren't?
[02:09:42.160 --> 02:09:48.960]   There is apparently not. Let's do the research. That's what I have you for.
[02:09:48.960 --> 02:09:54.640]   I bring on people to do the research. So I could just be here and do Scottish accents. That's my
[02:09:54.640 --> 02:09:59.200]   job. That's fair. I can't do a Scottish accent. So I have nothing to offer. I've been told I can't
[02:09:59.200 --> 02:10:08.960]   either say something. We can't sing. I mean, geez, what can you do? Yeah, I can, I can video or I
[02:10:08.960 --> 02:10:14.400]   can record my snores and play them. And don't even snore. Are you saying something about the show
[02:10:14.400 --> 02:10:22.320]   right now? No, no, and I can hold you all accountable for for being
[02:10:23.680 --> 02:10:28.160]   here's a question. No, do you think they're one of the reasons subsets doing this is because
[02:10:28.160 --> 02:10:32.720]   podcasting is so hot and maybe they're hoping this will sexy them up. So somebody will come along
[02:10:32.720 --> 02:10:38.400]   and buy them up. Maybe somebody called Spotify. Spotify and get into the newsletter. Okay. I think
[02:10:38.400 --> 02:10:46.800]   it's a smart. It's a smart way to make your content providers happy and to give them a source of
[02:10:46.800 --> 02:10:52.240]   revenue. So maybe long term, they're like maybe Spotify, but maybe they're thinking big like
[02:10:53.040 --> 02:11:00.000]   if their model is to be like the infrastructure for independent content providers, you have to
[02:11:00.000 --> 02:11:07.440]   have a newsletter content events events events events. You need a website too. Because if you're
[02:11:07.440 --> 02:11:14.800]   running a newsletter, you got to put your stuff somewhere so it gets searched. Yeah. So, yeah.
[02:11:14.800 --> 02:11:19.520]   All right. Let's let's wrap this guy up unless you got something else you want to. This made me
[02:11:19.520 --> 02:11:26.000]   think about I saw a tweet from Mr. Mike Elgin. Tumblr starts testing post plus, which allows
[02:11:26.000 --> 02:11:34.080]   users to charge their followers a monthly fee. Yeah, tumblers back because they got sold to
[02:11:34.080 --> 02:11:40.000]   automatic and automatic word press. He tweets he says charging for tumbler might be going too far
[02:11:40.000 --> 02:11:45.760]   and I just responded why so. And he says, well, because tumbler is mostly a static old school medium
[02:11:45.760 --> 02:11:51.680]   that is perfectly positioned to be to benefit from being one of the last remaining bastions of 100%
[02:11:51.680 --> 02:11:56.720]   free content on internet. I hope they can do something with tumbler because I think Yahoo.
[02:11:56.720 --> 02:12:02.880]   You know, tumbler was pretty interesting. Yahoo bought it. They didn't hurt it too badly.
[02:12:02.880 --> 02:12:08.080]   They left it alone. Then they they kind of started to censor content and then got sold to get sold
[02:12:08.080 --> 02:12:14.320]   to Verizon was a part of Verizon is when they started. I think Yahoo did it before, but
[02:12:14.880 --> 02:12:19.600]   Verizon obviously wasn't going to allow adult content on tumbler. And I, you know, I feel like
[02:12:19.600 --> 02:12:22.960]   it's kind of fallen off the face of the earth. The funny thing is, yeah, there was a lot of
[02:12:22.960 --> 02:12:27.200]   a done adult content, but it was also the easiest way to make a really good looking blog and a
[02:12:27.200 --> 02:12:33.760]   blog. Young people, really young people like, you know, kids, 13, 14, 15 year olds,
[02:12:33.760 --> 02:12:36.880]   used it for their, you know, their, their interest. And I thought it was a cool one.
[02:12:36.880 --> 02:12:40.800]   Remember the founder of tumbler was about that age. Yeah, he started tumbler. Yeah. Yeah.
[02:12:42.080 --> 02:12:47.360]   Yeah, that's right. I liked tumbler. David carp. David carp. Alex carp was now a millionaire.
[02:12:47.360 --> 02:12:55.200]   Thanks to Spotify. No, that's somebody else. I think the next thing we should do,
[02:12:55.200 --> 02:12:58.720]   because Stacy wants to go have a waffle. She's already had her waffles.
[02:12:58.720 --> 02:13:01.920]   Yeah. I had my waffles. It's time for dinner.
[02:13:01.920 --> 02:13:10.480]   Would be to get our picks of the week coming up next. Let's start as we always do on the
[02:13:10.480 --> 02:13:12.960]   picks of the week with Stacy Higginbotham.
[02:13:12.960 --> 02:13:19.840]   All right. This is, this is a book recommendation brought on by, by my latest obsession, which is
[02:13:19.840 --> 02:13:25.920]   rowing. This is an old book, but apparently they're making a movie of it. And it's called
[02:13:25.920 --> 02:13:29.760]   The Boys in the Boat. It's a good time to read it if you're into this sort of thing, because it's
[02:13:29.760 --> 02:13:34.960]   summer and you can imagine yourself rowing. And also because it is the story of the 1936 Olympics
[02:13:34.960 --> 02:13:40.640]   in Berlin and the US rowing team and their improbable win there. And
[02:13:40.640 --> 02:13:48.000]   you know, this guy could use an editor, but it's a really nice story. And as I told my husband,
[02:13:48.000 --> 02:13:52.080]   it feels like a guidebook because there's a lot of history in there and there's a lot of sports.
[02:13:52.080 --> 02:13:59.680]   So if you like history and you like sports and it has a lot of Washington state, so as a person
[02:13:59.680 --> 02:14:04.320]   who lives here, it was really interesting to see kind of Washington during the, the depression era,
[02:14:04.320 --> 02:14:10.160]   basically the 30s, the early 30s. And then it's, you know, if you're into rowing, you already know
[02:14:10.160 --> 02:14:15.360]   about this book. Edward, people should. It's a good book. Edward Herrmann reads it on Audible.
[02:14:15.360 --> 02:14:21.040]   So I like his narration. So that looks good. I might, I'll add that to my Audible Wishlist,
[02:14:21.040 --> 02:14:24.000]   the boys. Yeah, you can add it to your Olympic Audible playlist.
[02:14:24.000 --> 02:14:30.640]   And of course, there's always three men in a boat. You've read that. It's not quite as many
[02:14:30.640 --> 02:14:35.200]   as the nine boys in a boat. I'm not kidding. If you ever read three men in a boat,
[02:14:35.200 --> 02:14:42.560]   I'm going to go with no. To say nothing to say nothing of the dog. It is one of the best
[02:14:42.560 --> 02:14:53.120]   old English books about rowing down the Thames. It's hysterical. Well, it came out in 1889. It
[02:14:53.120 --> 02:15:02.720]   was hysterical back then. I see. I think you have to understand, it's a period piece, but it's a
[02:15:02.720 --> 02:15:08.080]   classic, you should read it by Jerome K Jerome, three men in a boat to say nothing of the dog.
[02:15:08.080 --> 02:15:15.760]   You can go with your, the boys in the boat to say nothing of the Nazis. And your pick of the week
[02:15:15.760 --> 02:15:24.960]   are your number of the week, Mr. Jeff Jarvis. So my number is either CX nine or CX five. CX nine.
[02:15:24.960 --> 02:15:32.160]   A wonderful user named Woody Nash, DM me that he might have found my new Chromebook.
[02:15:32.160 --> 02:15:40.720]   Oh, seven hundred fifty bucks. I had a conversation in DMS with Mr. Tofol. We'll get back to that in a
[02:15:40.720 --> 02:15:46.800]   second, but we'll start with the CX nine. He, as you know, he likes acesers, but this is a sous.
[02:15:46.800 --> 02:15:51.200]   This is a sous. No Kevin. Well, we're going to get back to Kevin in a second here. This is the
[02:15:51.200 --> 02:16:00.720]   a sous six nine very thin. Lots of that weird screen thing. It is strange. How's that going to
[02:16:00.720 --> 02:16:04.640]   feel a little lap? Yeah, I'm not sure. I might keep the heat off the lap. I think it's better in
[02:16:04.640 --> 02:16:10.320]   the lap because you won't notice that that on your lap. I've tested a Lenovo or,
[02:16:10.320 --> 02:16:14.080]   yeah, a number of people are starting to do this. I think at least you won't feel a different.
[02:16:14.080 --> 02:16:16.640]   I think it's a nice idea. Nice for the ventilation.
[02:16:16.640 --> 02:16:22.320]   Okay, so it's it's a nice looking machine. It's
[02:16:22.320 --> 02:16:32.560]   Thunderbolt four USB C USB 3.2 HDMI micro SD card slot. It comes in the i three five and seven
[02:16:33.280 --> 02:16:42.400]   models up to 16 gigs of RAM 512 gigs of SSD M.2 SSD. So it's a it's basically a high end PC running
[02:16:42.400 --> 02:16:50.560]   Chrome OS 1.5 kilograms. Yeah, it even has Stacy's favorite Wi-Fi six in it.
[02:16:50.560 --> 02:17:00.640]   Wow. It has a the trackpad can become a numeric pad. That's a little weird. A sous does that. It's
[02:17:00.640 --> 02:17:06.160]   kind of strange. But I guess if you need a number pad, you could see the the keys
[02:17:06.160 --> 02:17:14.480]   show up to track 14 hours, they say. They won't get 14 hours on a 4k screen.
[02:17:14.480 --> 02:17:21.600]   And so I was getting excited. I was going to do my Leo imitation and almost just buy it.
[02:17:21.600 --> 02:17:26.160]   Well, you should. He sold out. It's already sold out. Wow. Yeah, they just announced it yesterday.
[02:17:26.720 --> 02:17:34.080]   Yeah, what tophal say tophal I see him and being a contrarian that he is. He says,
[02:17:34.080 --> 02:17:40.560]   stop emailing me. Yeah, he says Jarvis, leave me alone. Jesus get a light. It has a finger
[02:17:40.560 --> 02:17:44.240]   reader to it. Does that work with Chrome OS? Could you unlock with a fingerprint? That would be
[02:17:44.240 --> 02:17:48.880]   just the weird thing about that. It also says touchscreen optional. Why would you ever get a
[02:17:48.880 --> 02:17:54.480]   Chrome with no touch? Yeah. Yeah. Anyway, that's so they can do a little he was saying on Twitter
[02:17:54.480 --> 02:17:59.840]   that he was lost in after and thinking about an actual purchase of his first replacement in a
[02:17:59.840 --> 02:18:08.640]   long time, the Asus Chromebook Flip CX 5400. Yes. A lot of people like that as well. That's the
[02:18:08.640 --> 02:18:16.080]   yeah. So that's less expensive, I think, isn't it? By $100. Yeah. It's it's 1049. And the
[02:18:17.200 --> 02:18:28.000]   what the CX 9? Wow. $1,100. Holy. I'm ready to spend. My wife may not be for me, but I'm going to
[02:18:28.000 --> 02:18:36.160]   speak it in. So Kevin said, I'll go back to my DMs here. So I said, I see your
[02:18:36.160 --> 02:18:38.720]   attention. Wait, did you ask? If I could read this.
[02:18:38.720 --> 02:18:46.880]   You asked his permission to quote your DMs? Oh, he was public about. I'm just checking as a
[02:18:46.880 --> 02:18:51.120]   journalist, Professor, where do you stand on this? Yeah, it's a good point. He was speaking the
[02:18:51.120 --> 02:18:58.080]   same points in Twitter. And so I thought that I'd get him to say, Oh, we're right, Jarvis, you must
[02:18:58.080 --> 02:19:03.760]   get the CX 9. But no, he's hanging by the other one. He likes, he likes to get him. You know,
[02:19:03.760 --> 02:19:09.280]   that's him having an opinion, man. You were complaining. Yeah. He likes the flip. He likes the flip.
[02:19:09.280 --> 02:19:15.600]   Like your family telling you that you've got a, you know, that I need to go over the bridge and
[02:19:15.600 --> 02:19:24.160]   get over it. You know, it's a what? I don't know. I don't like the silver as well as I like the
[02:19:24.160 --> 02:19:31.840]   more expensive dark. He has a break. He says the CX 9 has a type C on only one side, only on the
[02:19:31.840 --> 02:19:36.800]   left. Uh, one of both. Is that a stop? Kevin? I don't know. Kevin loves
[02:19:36.800 --> 02:19:40.720]   a bunch of less. It's nice to have one on each side because then you could plug in on either side.
[02:19:40.720 --> 02:19:47.360]   Exactly. And he thinks that these screen hinge things weird. So I need Kevin to get a demo model
[02:19:47.360 --> 02:19:53.680]   to try to lap test it. The few laptops that we've had for hands on tech that had that
[02:19:53.680 --> 02:19:59.040]   interesting looking hinge, it does not make a difference sitting on your lap. It barely makes
[02:19:59.040 --> 02:20:04.880]   a difference sitting on the desk. But it, what it helps is just allowing those fans to not fire up
[02:20:04.880 --> 02:20:09.520]   in struggle as much because you get inadequate air. So, uh, especially if you're going to have
[02:20:09.520 --> 02:20:16.560]   all of that power. I think the CX 9 looks better, a lot better. That's a pretty lab. Don't you think?
[02:20:16.560 --> 02:20:24.320]   Yeah. The CX, the five, the Kevin likes his fan list. And I'm fan phobic. Oh, interesting.
[02:20:24.320 --> 02:20:28.800]   I would think that you're right. That's what would be probably fan. In fact, you might want a fan
[02:20:28.800 --> 02:20:36.720]   because honestly, uh, that's a conceit that I think these faster processors really aren't going
[02:20:36.720 --> 02:20:44.080]   to run very well without a fan. Um, this is pretty. This is the executive Chromebook.
[02:20:44.080 --> 02:20:49.600]   This is designed for the first level office. I deserve this. Yeah. So which one, which one would
[02:20:49.600 --> 02:20:55.040]   you go? Well, I'd get the CX 9. I'd go with the nine. Yeah. It's that. Can you get the other one?
[02:20:55.040 --> 02:20:58.960]   The other one's the one that I think it's not out yet. Oh, it's not out yet. Okay.
[02:20:58.960 --> 02:21:04.560]   Yes. I'm on the, I'm on the, uh, the notification list for the CX. And you're all about your touch
[02:21:04.560 --> 02:21:11.280]   screen. Why not just get a tablet? What is, why the pro baby? So this part of the mill spec,
[02:21:11.280 --> 02:21:17.840]   it's mill spec, right? Part of the mill standard eight, 10 H is an altitude test. It operates
[02:21:17.840 --> 02:21:24.400]   normally at 15,000 feet. Well, who flies at 15? Well, when I go up with Jeff, 10 minutes,
[02:21:24.400 --> 02:21:31.760]   I want to make sure my machine operates. I remember my last flight was at 37,000 feet, but
[02:21:31.760 --> 02:21:36.400]   okay. I guess on the way you can use it. Yeah, you cruise there for about 10 minutes. Yeah.
[02:21:36.400 --> 02:21:43.600]   That's interesting. Maybe military flights don't go above 15,000 feet. So would they say maybe
[02:21:43.600 --> 02:21:50.960]   that's well, they're not pressure. Yeah. Hey, if it's mill spec, that means nothing.
[02:21:50.960 --> 02:22:04.560]   So there. All right. Ant Pruitt. All right. My pick yesterday was a big day for Adobe. I put it up
[02:22:04.560 --> 02:22:11.920]   on my blog at antproot.com where they announced some announced a July update for video for after
[02:22:11.920 --> 02:22:19.600]   FX for Premiere Pro and for character animator. And of course, the whole M1 Mac support. I mean,
[02:22:19.600 --> 02:22:25.360]   you knew that was coming and working for quite a while. And that's going to be nice. M1 Mac support.
[02:22:25.360 --> 02:22:33.280]   But the cool thing is the subtitles and captioning because in video today, you're seeing more and
[02:22:33.280 --> 02:22:38.880]   more subtitles on the social media side. On TikTok, Facebook started. We talked about this yesterday.
[02:22:38.880 --> 02:22:45.440]   Yeah. Oh, yeah. I did talk about it on Mac. Right. So that's just another tool to go ahead and
[02:22:45.440 --> 02:22:49.840]   optimize for social. Good for social. And unlike the previous transcription utility,
[02:22:49.840 --> 02:22:53.840]   which really looks more like subtitles, this is pretty. You can get color. It can be.
[02:22:53.840 --> 02:22:59.280]   You can mark your friends. Yeah. It looks like it belongs on TikTok or Facebook or Twitter or
[02:22:59.280 --> 02:23:03.280]   someone. Yeah. Because a lot of people, I don't listen to, I don't have the audio on when I'm
[02:23:03.280 --> 02:23:08.000]   looking at this stuff. I don't want to bother Lisa or whatever. And so if it doesn't have captions,
[02:23:08.000 --> 02:23:11.760]   I'm not going to know what's about it. It's pretty common. Yeah. Supposedly,
[02:23:11.760 --> 02:23:18.240]   there's a stat that one in five people have hearing impaired issues. Wow. That's
[02:23:18.240 --> 02:23:26.240]   its accessibility. That's just age related ear degeneration. It's another problem.
[02:23:26.240 --> 02:23:30.880]   Sorry. Well, I was trying to get my father. We got him. We got him a a text phone.
[02:23:30.880 --> 02:23:36.720]   He's 95 years old. God bless him. He's doing great. But I was trying to scream at him on the
[02:23:36.720 --> 02:23:42.720]   regular phone last night. I can't understand you. Yes. Use the other phone. What? I know.
[02:23:42.720 --> 02:23:47.920]   I've done that with my mom. I can't hear you on this iPad. You should. Well,
[02:23:47.920 --> 02:23:54.080]   use the other iPad, but I can't hear you on this iPad. I had that. He was calling me to order toilet
[02:23:54.080 --> 02:23:58.960]   paper. So every time I would say new telephone, he got new that he said, yeah, new toilet paper.
[02:23:58.960 --> 02:24:06.160]   I told you. I need to do toilet paper. Oh, we're going to be like that in about three minutes, Jeff.
[02:24:06.160 --> 02:24:11.120]   Oh, yeah. Oh, yeah. Can't wait. Who's on first story next time? Yeah. What?
[02:24:11.120 --> 02:24:17.840]   And my other pick is this is a Netflix documentary has been out for a while. This
[02:24:17.840 --> 02:24:22.320]   the chef's table seal. Oh, you told me about this. I want to see this for barbecue. Oh,
[02:24:22.320 --> 02:24:27.840]   in particular, episode three is the one in particular because that's for Mr. Rodney Scott
[02:24:27.840 --> 02:24:36.000]   down in Charleston, South Kecky likey. Have you had Rodney's good, good barbecue? I haven't had
[02:24:36.000 --> 02:24:41.040]   his yet, but I watched that episode just this past weekend. And this is why I'm sharing it because it
[02:24:41.040 --> 02:24:46.640]   just let's go to Charleston right now. And dear in the heart, let's go. His whole
[02:24:46.640 --> 02:24:53.120]   hog barbecue. Yes. His story is so great. He was, you know, discovered about in New York times,
[02:24:53.120 --> 02:24:58.800]   based on some word of mouth and things just grew and grew and grew from there. And as he's,
[02:24:58.800 --> 02:25:03.920]   as I was watching this, because this was the one day I had pretty much the day to myself,
[02:25:03.920 --> 02:25:08.000]   because the hard hits were out doing their things and Queen Perot is out doing everything. So I just
[02:25:08.000 --> 02:25:12.640]   watched time to watch barbecue videos. I just watched some Netflix and I watched that. And man,
[02:25:12.640 --> 02:25:18.160]   it just, I cried a little bit because it made me think about my grandparents because this stuff
[02:25:18.160 --> 02:25:23.440]   is everything that my grandparents and my great grandparents used to do in really
[02:25:23.440 --> 02:25:28.800]   place, sacri-lining. They used to barbecue used to get cinder blocks to make our pits and all of
[02:25:28.800 --> 02:25:33.600]   it. And I'm looking at it as like, they used to steal drums to do it. Now, where's all cinder blocks
[02:25:33.600 --> 02:25:39.680]   and big grates in the ground? And it was just, I was like, yeah, that's home, you know? Oh, man.
[02:25:39.680 --> 02:25:44.720]   And it's such a well done documentary by Netflix. It's beautiful. Look at this. I can't watch it,
[02:25:44.720 --> 02:25:49.840]   though. I'll, I'll just, I'll die. Oh my God. Yeah. I just be like, I'm getting me sweats. Just
[02:25:49.840 --> 02:25:56.240]   looking. That's the lady in Texas. She's a, I can't remember that brisket. Oh my God. You can't do
[02:25:56.240 --> 02:26:01.600]   barbecue. No, no, no, no. Let us barbecue. He was in, he was in Australia. Yeah, no wonder.
[02:26:01.600 --> 02:26:05.360]   Cookie, but he's good. For some greens on the Bobby.
[02:26:05.360 --> 02:26:13.040]   Caramel accent. I don't know who did it there. Sticky.
[02:26:13.040 --> 02:26:18.480]   Put another lettuce on the Bobby. Never be. Oh Lord. But right. Yeah.
[02:26:18.480 --> 02:26:25.040]   Rodney Scott. I want this. Oh, oh, oh man. Can we just go to Charleston? You could take me around.
[02:26:25.040 --> 02:26:29.920]   I'm all for it. Let's just do it. I'm all for Lisa. We're going to Charleston. Yeah. Never mind
[02:26:29.920 --> 02:26:35.280]   the cruise. You're cruising. Charleston. Well, well, we'll do the cruise in July. But what if we
[02:26:35.280 --> 02:26:41.840]   did a Twit barbecue tour? Oh, well, I will, I will preface this. I don't want to go to Charleston,
[02:26:41.840 --> 02:26:48.400]   South Carolina in July. No, no, no. We'll do that later in the year. And then Jeff, I want you to do
[02:26:48.400 --> 02:26:54.000]   Jeff Jarvis's pizza tour because Jeff is an expert. Please and thank you. Yes. He, he,
[02:26:54.000 --> 02:27:03.760]   two of them. A case of. And you get to do the case. Oh, man. He's waffles. Waffles and queso.
[02:27:03.760 --> 02:27:09.120]   I'd rather do queso for being real or the margarita. Okay. All right. All right.
[02:27:09.120 --> 02:27:15.600]   Reno tour be good. Yeah. Guys, get so fat. You're so mean to me.
[02:27:15.600 --> 02:27:20.880]   Hey, I just want to show you something real quickly because people everybody says RSS is dead.
[02:27:22.000 --> 02:27:25.440]   But we still use it. Right? Those of us who have to look at news sites.
[02:27:25.440 --> 02:27:31.520]   And in the space, I was reading it went in a space. They used RSS space. That's the name of the
[02:27:31.520 --> 02:27:39.680]   over the over the hatch is our base. O's is a phallic symbol. It was the RSS first step. Oh,
[02:27:39.680 --> 02:27:45.040]   what is that? I don't know what the RSS stood for. I like the RSS was one of those. No, no, no, no,
[02:27:45.040 --> 02:27:50.880]   no, don't stop right there. Stop. Thank you. And you could be my punch Leo button.
[02:27:50.880 --> 02:27:59.040]   Shaft. I'm thinking. I don't know. That's what I would think. No, not going.
[02:27:59.040 --> 02:28:00.960]   You didn't punch him hard enough. He still tried.
[02:28:00.960 --> 02:28:10.720]   So I was really into RSS for a long time. And I was reading an article on hacker news and
[02:28:10.720 --> 02:28:16.560]   somebody said, Oh, I use Sumi news. I said, what? I never heard of that. S U M I dot N E W S.
[02:28:16.560 --> 02:28:26.320]   It is a bare bones RSS reader. You can use it for free. But if you want to add more than 10 feeds,
[02:28:26.320 --> 02:28:31.840]   then you'll pay them 20 bucks flat fee one time only, which I did.
[02:28:31.840 --> 02:28:39.200]   Oh, I was so nicely done. You get a email address, which I won't show that you can mail articles
[02:28:39.200 --> 02:28:44.400]   to to add to your sumi news. But what I've done is I've added the websites that I go to every morning
[02:28:44.400 --> 02:28:49.840]   anyway, like tech meme and slash detector and ours, tech, and so forth. And I've just added those
[02:28:49.840 --> 02:28:55.360]   and this makes it the easiest way. Now this is what's nice about this is it doesn't do a summary. It
[02:28:55.360 --> 02:29:01.760]   has maybe like a one line summary. But these links go right to the site. So basically, it is just a
[02:29:01.760 --> 02:29:06.960]   way you can quickly review headlines and maybe one line of content and know if you want to read more.
[02:29:06.960 --> 02:29:11.760]   And then I can just click, click, click, click, click. And it's really great. I just thought I
[02:29:11.760 --> 02:29:18.800]   tell people about it. It's a very bare bones. Did I just see a NFL reference on your screen, sir?
[02:29:18.800 --> 02:29:22.400]   Well, let me look you in sports, sports ball. It's probably NFT.
[02:29:22.400 --> 02:29:29.440]   No, I'm pretty sure I saw Tom Brady's name just now. Well, yeah, Tom's in the news. He was at the
[02:29:29.440 --> 02:29:34.240]   White House. Oh, yeah. That is awesome. I was like, what are you doing looking up? Yeah, but no,
[02:29:34.240 --> 02:29:38.800]   this is all tech stuff. I have other news feeds from my news. So I don't know. Maybe there's a
[02:29:38.800 --> 02:29:42.800]   there it is Tom Brady's NFT bump. There's still
[02:29:42.800 --> 02:29:43.520]   there you go. Your
[02:29:43.520 --> 02:29:43.680]   Your
[02:29:43.680 --> 02:29:43.680]   Your
[02:29:43.680 --> 02:29:44.720]   See
[02:29:44.720 --> 02:29:45.120]   Tech
[02:29:45.120 --> 02:29:53.200]   Yeah, yeah, I don't know. Now I want to read Tom Brady's NFT bump. The NFT producing startup
[02:29:53.200 --> 02:30:01.200]   co-founded by Tom Brady has signed exclusive multi-year dual deals with Tiger Woods Wayne
[02:30:01.200 --> 02:30:05.120]   Gretzky Derek Jeter Naomi Osaka and Tony Hawk. Oh, wow.
[02:30:05.120 --> 02:30:08.640]   It's called autograph. I guess these are NFT autographs. You know, that's one of the
[02:30:08.640 --> 02:30:13.920]   problems with autographs is okay proving providence, right? Interest. So I was like, is
[02:30:13.920 --> 02:30:19.840]   the fact that they're not digital? Yes, that's the problem. People have to pay you's paper and pen.
[02:30:19.840 --> 02:30:24.000]   What good is that? Physically go someplace and meet the person. Oh, and
[02:30:24.000 --> 02:30:33.360]   no bother them when they don't want it. Stacey Hagan-Botham is at Stacey on IOT. You've got to
[02:30:33.360 --> 02:30:39.680]   get to her newsletters free. Subscribe. Yeah, we didn't talk about this. What? Read my story.
[02:30:39.680 --> 02:30:45.040]   I'm not going to talk about it now because I got to go. But read my story. Amazon is going to support
[02:30:45.040 --> 02:30:50.000]   matter, which is the smart home interoperability standard on most of its echo devices. That's
[02:30:50.000 --> 02:30:54.480]   very good news. I thought they already said that. That's that's new. Nope. Amazon was the only one
[02:30:54.480 --> 02:31:00.960]   who hadn't said it. They were the whole that's fantastic. Google did it. Apple did it. And now
[02:31:00.960 --> 02:31:06.720]   Amazon. That's the big three. Nice. Yep. That's very good. Except for well, Stacey float well.
[02:31:06.720 --> 02:31:13.680]   She's going to row. Stroke. Go row. Stroke. I'm going to eat and then I'm going to
[02:31:13.680 --> 02:31:20.240]   eat then row. That's my life story. No, just the first half. See it's talk eat row. Talky
[02:31:20.240 --> 02:31:26.640]   row. It's like a version of eat love pray. Thank you, Stacey. Don't forget her podcast with Kevin
[02:31:26.640 --> 02:31:34.880]   Tofoli IOT podcast. Mr. and Pruitt hands on photography. What's coming up on hop? I get to speak with
[02:31:34.880 --> 02:31:41.840]   Miss Vanessa Joy and Kenan explore of light. And we're going to talk about using budget
[02:31:41.840 --> 02:31:47.520]   photography gear to do professional work. And boy, she's really, really good. And quite frankly,
[02:31:47.520 --> 02:31:52.800]   I got a hunt. She's going to break the internet when you watch this episode. Wow. Just just just
[02:31:52.800 --> 02:32:00.320]   the warning. Wow. That's exciting. Twitter.tv/hop. That's right. And he will be nicely dressed in his
[02:32:00.320 --> 02:32:06.000]   Clemson orange and gray and the tie. I expect the tie. Thank you for that. Is that a Clemson? You're
[02:32:06.000 --> 02:32:17.760]   done at the district. That's one one and done. That's Jeff Jarvis. He is the townite. I can't do
[02:32:17.760 --> 02:32:22.480]   it without the card. Where did my card go? They cleaned up the studio. No, I have to do it.
[02:32:22.480 --> 02:32:26.320]   No, we got to do it. The director of the townite center for the entrepreneurial journalist at the
[02:32:31.520 --> 02:32:38.000]   Graduate School of Journalism at the City University of New York. Thank you, Jeff. Thank you. Great to see
[02:32:38.000 --> 02:32:44.320]   you. Get that. Get that. You know, I should just buy it for your birthday. You can't buy it. Oh,
[02:32:44.320 --> 02:32:57.520]   shot. Oh, rats. She's so close. Man, they frustrate you at every turn. Thank you, everybody. We'll
[02:32:57.520 --> 02:33:06.000]   see you next time on Twig. Bye. Bye. You know what's fun, Android. You know what's even more fun,
[02:33:06.000 --> 02:33:10.880]   though? All about Android. That's my show, Jason Howell, along with my co-host Ron Richards,
[02:33:10.880 --> 02:33:16.800]   Florence Ion. And we welcome guests on each and every week from throughout the Android ecosystem,
[02:33:16.800 --> 02:33:22.880]   developers, Googlers, journalists, people who are all geeked out about the Android operating system.
[02:33:22.880 --> 02:33:35.040]   We tell you everything you need to know, twit.tv/AAA every Tuesday. We'll see you there.
[02:33:35.040 --> 02:33:37.620]   (upbeat music)
[02:33:37.620 --> 02:33:38.620]   [whooshing]
[02:33:38.620 --> 02:33:40.620]   [ static ]

