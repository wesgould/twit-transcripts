;FFMETADATA1
title=Oyez, Oyez, Oyez
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=480
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.240]   It's time for Twig this week in Google Jeff and Stacey are here.
[00:00:03.240 --> 00:00:04.080]   They're both back.
[00:00:04.080 --> 00:00:05.640]   We have lots to talk about.
[00:00:05.640 --> 00:00:09.480]   Google gets his day in the Supreme Court today.
[00:00:09.480 --> 00:00:15.320]   We'll talk about the harassment charges at Google and what it means to have a great corporate
[00:00:15.320 --> 00:00:16.320]   culture.
[00:00:16.320 --> 00:00:17.880]   And is it possible in Silicon Valley?
[00:00:17.880 --> 00:00:20.640]   It's all coming up next on Twig.
[00:00:20.640 --> 00:00:24.640]   Netcast you love.
[00:00:24.640 --> 00:00:26.640]   From people you trust.
[00:00:26.640 --> 00:00:36.680]   This is Twig.
[00:00:36.680 --> 00:00:38.160]   This is Twig.
[00:00:38.160 --> 00:00:44.480]   This week in Google, Episode 480 recorded Wednesday, October 31, 2018.
[00:00:44.480 --> 00:00:47.440]   Oye yay, oye yay, oye yay.
[00:00:47.440 --> 00:00:50.760]   This week in Google is brought to you by Privacy.com.
[00:00:50.760 --> 00:00:53.720]   Stop using the same card number everywhere.
[00:00:53.720 --> 00:00:58.000]   Use privacy virtual cards and get a $5 credit off your first purchase.
[00:00:58.000 --> 00:01:04.720]   Learn more and sign up in under a minute with your debit or checking account at privacy.com/twig.
[00:01:04.720 --> 00:01:06.480]   And by WordPress.
[00:01:06.480 --> 00:01:10.600]   Reach more customers when you build your business website on wordpress.com.
[00:01:10.600 --> 00:01:18.240]   Plan start at just $4 a month and you'll get 15% off any new plan purchase at wordpress.com/twig.
[00:01:18.240 --> 00:01:23.800]   And by Slidebelts by Brig Taylor, high quality, comfortable ratchet belts that are easy to
[00:01:23.800 --> 00:01:24.800]   adjust.
[00:01:24.800 --> 00:01:31.120]   If you want a better belt, go to slidebelts.com/twig and use the code to it for 20% off.
[00:01:31.120 --> 00:01:34.760]   It's time for Twig this week in Google to show you how to cover the latest news from
[00:01:34.760 --> 00:01:37.200]   Google and hail, hail the gang's all here.
[00:01:37.200 --> 00:01:42.840]   I'm so pleased to see Jeff Jarvis from the City University of New York at buzzmachine.com
[00:01:42.840 --> 00:01:44.360]   and his lovely home.
[00:01:44.360 --> 00:01:48.000]   Will trick or treaters be pestering us this evening?
[00:01:48.000 --> 00:01:53.440]   We live on large acre lots so they have to be really good coming down a long, hilly
[00:01:53.440 --> 00:01:54.440]   driveway.
[00:01:54.440 --> 00:01:55.440]   It's not worth it.
[00:01:55.440 --> 00:01:59.040]   I never buy candy because we're in a cul-de-sac in the country.
[00:01:59.040 --> 00:02:02.440]   Oh, you guys, it's the best part of Halloween.
[00:02:02.440 --> 00:02:07.560]   That's Stacy Hagenbotham and she will be leaping to her feet.
[00:02:07.560 --> 00:02:11.720]   She's monitoring the front door with her nest cam and at any moment she may have to run
[00:02:11.720 --> 00:02:15.760]   and what are we doing for candy this week?
[00:02:15.760 --> 00:02:17.400]   I put a sample up here.
[00:02:17.400 --> 00:02:19.160]   We have Sour Patch Kids.
[00:02:19.160 --> 00:02:22.800]   Also Swedish Fish but I didn't bring those up because they're not.
[00:02:22.800 --> 00:02:23.800]   Oh, good.
[00:02:23.800 --> 00:02:25.200]   I would eat those if I had them.
[00:02:25.200 --> 00:02:28.240]   I thought you were going to say I'm not going to bring them because I'll eat them.
[00:02:28.240 --> 00:02:29.240]   Nope.
[00:02:29.240 --> 00:02:30.240]   Nope.
[00:02:30.240 --> 00:02:32.040]   And I have both peanut and plain M&Ms.
[00:02:32.040 --> 00:02:33.040]   Yum, I would eat those.
[00:02:33.040 --> 00:02:37.840]   I want Stacy to do to the kids what she does to her husband with her IOT stuff.
[00:02:37.840 --> 00:02:42.400]   You know, use the ring doorbell to scare the Jesus out of them.
[00:02:42.400 --> 00:02:47.320]   You can have your hue lights, play spooky sounds, motion sensitive, all that.
[00:02:47.320 --> 00:02:51.360]   We do and we have done Halloween lights.
[00:02:51.360 --> 00:02:55.520]   One year Anna and I did set up a Halloween thing at our door.
[00:02:55.520 --> 00:02:58.760]   So when you triggered a motion sensor at the bottom of the steps coming up to our front
[00:02:58.760 --> 00:03:04.240]   door, it would turn on a blow dryer that would make a glow go woo.
[00:03:04.240 --> 00:03:06.240]   Oh, that's good.
[00:03:06.240 --> 00:03:07.240]   There's a lot of latency.
[00:03:07.240 --> 00:03:11.240]   So really what happened was the ghost just would randomly to the kids perspective just
[00:03:11.240 --> 00:03:14.240]   go woo.
[00:03:14.240 --> 00:03:20.560]   But you know, latency is the death of a great Halloween scare.
[00:03:20.560 --> 00:03:21.760]   Yeah, exactly.
[00:03:21.760 --> 00:03:24.160]   Because everything's about timing when it's Halloween.
[00:03:24.160 --> 00:03:27.160]   Yeah, I find latency scary all the time on the show.
[00:03:27.160 --> 00:03:28.160]   Yeah.
[00:03:28.160 --> 00:03:29.160]   Yes.
[00:03:29.160 --> 00:03:33.160]   But since latency is hard when you're doing Skype as well.
[00:03:33.160 --> 00:03:37.000]   Well, Jeff, is there anything that's happened over the last few weeks that you that we that
[00:03:37.000 --> 00:03:40.000]   we we might have talked about, but you didn't get to?
[00:03:40.000 --> 00:03:41.000]   Oh, God knows.
[00:03:41.000 --> 00:03:43.000]   Don't say that don't remind me the last few weeks.
[00:03:43.000 --> 00:03:45.720]   What about here's one.
[00:03:45.720 --> 00:03:54.320]   Jimmy Wales firing the journalists at his journalistic startup, the wiki Tribune.
[00:03:54.320 --> 00:03:55.320]   Yep.
[00:03:55.320 --> 00:03:58.280]   And I haven't actually heard from him yet.
[00:03:58.280 --> 00:04:00.640]   And I was a funder of wiki Tribune.
[00:04:00.640 --> 00:04:02.880]   So I should find out what's going on.
[00:04:02.880 --> 00:04:08.720]   I think that Jimmy's model of having the professional journalist work with the public,
[00:04:08.720 --> 00:04:11.320]   I think was him trying and it kind of didn't work.
[00:04:11.320 --> 00:04:12.320]   And now why?
[00:04:12.320 --> 00:04:17.600]   He's the public has a professional journalist were dismissive of the public.
[00:04:17.600 --> 00:04:20.800]   So that we have one of our of our social journalism students working there, graduates
[00:04:20.800 --> 00:04:25.480]   working there and he wasn't he would be no, he was not.
[00:04:25.480 --> 00:04:31.920]   I mean, journalists in some ways we are dismissive because we have a very real culture thing.
[00:04:31.920 --> 00:04:32.920]   Right.
[00:04:32.920 --> 00:04:38.200]   Like I remember hiring someone who was not a journalist who I didn't find out until like
[00:04:38.200 --> 00:04:42.240]   and actually someone else hired him, I didn't until I was this managing editor.
[00:04:42.240 --> 00:04:45.120]   He actually owned shares in Apple and was our Apple reporter.
[00:04:45.120 --> 00:04:46.120]   Yeah.
[00:04:46.120 --> 00:04:51.160]   Yeah, there's a, which I miss the journalistic standard to be honest with you.
[00:04:51.160 --> 00:04:56.200]   And I do think that, I mean, you could come, it's not hard to communicate that and put
[00:04:56.200 --> 00:05:00.120]   it in writing and have him sign something that says you don't own shares in companies
[00:05:00.120 --> 00:05:05.560]   who cover, you know, and there's some basic tenets that I think aren't that hard to communicate.
[00:05:05.560 --> 00:05:08.160]   No, well, there's, there's, so I'm, I'm involved in the,
[00:05:08.160 --> 00:05:12.160]   the reporters without borders, journalism, trust initiative.
[00:05:12.160 --> 00:05:15.600]   That's scary Halloween stuff.
[00:05:15.600 --> 00:05:21.120]   And me in Paris, that's the nice part.
[00:05:21.120 --> 00:05:25.720]   But I got shouted out of the room practically when I said that's our job to listen to the
[00:05:25.720 --> 00:05:30.320]   public and understand the public and be responsive to many communities, diverse communities
[00:05:30.320 --> 00:05:31.320]   and all this.
[00:05:31.320 --> 00:05:36.360]   The journalists are still on this, many of them, this naughty mode that we know best.
[00:05:36.360 --> 00:05:42.720]   Well, that's not going to serve them well in the new, no, it's not fascist stick era
[00:05:42.720 --> 00:05:45.120]   that this kind of reading right now.
[00:05:45.120 --> 00:05:49.840]   Well, it's entering Paul stars creation of media, rather, rather magisterial history
[00:05:49.840 --> 00:05:51.440]   of media in the US.
[00:05:51.440 --> 00:05:59.160]   And if I may class, I found it really interesting that before the Bill of Rights, when the fight
[00:05:59.160 --> 00:06:04.520]   was going on between the federalists, the others about the Bill of Rights, that the
[00:06:04.520 --> 00:06:12.400]   original versions kind of, now I'm not going to find it.
[00:06:12.400 --> 00:06:15.800]   The Pennsylvania Bill of Rights, for example, I'm not going to find it right away.
[00:06:15.800 --> 00:06:18.640]   But it said, here we go.
[00:06:18.640 --> 00:06:24.080]   The Pennsylvania Declaration of Rights stating, quote, that the people, the people have a
[00:06:24.080 --> 00:06:27.120]   right to freedom of speech and of writing and publishing their sentiments.
[00:06:27.120 --> 00:06:30.440]   Therefore the freedom of the press ought not to be restrained.
[00:06:30.440 --> 00:06:31.600]   What comes first there?
[00:06:31.600 --> 00:06:32.600]   The people.
[00:06:32.600 --> 00:06:33.600]   The voices of the people.
[00:06:33.600 --> 00:06:39.400]   And as it got mushed together and rewritten and added in and made it into a First Amendment,
[00:06:39.400 --> 00:06:43.120]   that spirit that was very much part of the founders view those who believe in the First
[00:06:43.120 --> 00:06:48.480]   and the Bill of Rights got kind of lost.
[00:06:48.480 --> 00:06:50.200]   Journalists are people first.
[00:06:50.200 --> 00:06:51.200]   And they're right.
[00:06:51.200 --> 00:06:52.200]   The people cover people first.
[00:06:52.200 --> 00:06:55.840]   But that's the point is that the journalists have rights because the people have rights.
[00:06:55.840 --> 00:06:56.840]   Yes, exactly.
[00:06:56.840 --> 00:07:01.680]   And it's the job of the journalists to help the people to be heard.
[00:07:01.680 --> 00:07:02.680]   Yes.
[00:07:02.680 --> 00:07:05.480]   Okay, we've done our, we've done our.
[00:07:05.480 --> 00:07:06.480]   That's our obligatory.
[00:07:06.480 --> 00:07:07.480]   Yeah, obligatory.
[00:07:07.480 --> 00:07:08.480]   Journalists.
[00:07:08.480 --> 00:07:12.080]   We have a loss the entire audience in the first three minutes.
[00:07:12.080 --> 00:07:15.200]   Yeah, I was like, that's brave starting off with journalism.
[00:07:15.200 --> 00:07:18.360]   How about we start with sexism?
[00:07:18.360 --> 00:07:22.080]   Andy Rubin paid $90 million.
[00:07:22.080 --> 00:07:26.880]   And we, I think we had some suspicions about why Andy Rubin, the guy who created Android,
[00:07:26.880 --> 00:07:33.960]   left so abruptly left Google, started a essential Larry Page in his farewell, his public farewell
[00:07:33.960 --> 00:07:37.760]   said, I want to wish Andy all the best with what's next with Andrade created something
[00:07:37.760 --> 00:07:41.040]   truly remarkable with the billion plus happy users.
[00:07:41.040 --> 00:07:45.280]   But what Google did not make public we've learned from the report in the New York Times
[00:07:45.280 --> 00:07:52.320]   was that he had been accused of sexual misconduct.
[00:07:52.320 --> 00:08:00.360]   People investigated, found the claim credible and asked for Andy Rubin's resignation.
[00:08:00.360 --> 00:08:07.240]   But instead of escorting them out the door, they handed them a $90 million exit package
[00:08:07.240 --> 00:08:13.160]   two million a month for four years.
[00:08:13.160 --> 00:08:15.800]   Google said this apparently has happened in a number of cases.
[00:08:15.800 --> 00:08:20.000]   Google said that the New York Times story contains numerous inaccuracies.
[00:08:20.000 --> 00:08:21.000]   Actually Andy Rubin said.
[00:08:21.000 --> 00:08:28.280]   I'm sorry, numerous inaccuracies and wild exaggerations about my compensation.
[00:08:28.280 --> 00:08:34.480]   And he denies the sexual, uh, con, misconduct allegation.
[00:08:34.480 --> 00:08:40.480]   After the publication of the article, Sundar Pichai, Google CEO, wrote an email to employees
[00:08:40.480 --> 00:08:44.920]   with Eileen Naughton, their vice president for people operations that the company had
[00:08:44.920 --> 00:08:49.640]   fired 48 people for sexual harassment over the last two years and that none of them received
[00:08:49.640 --> 00:08:55.560]   an exit package, not addressing specifically the, uh, Andy Rubin case.
[00:08:55.560 --> 00:08:57.800]   Stay to see your first.
[00:08:57.800 --> 00:08:58.800]   Okay.
[00:08:58.800 --> 00:09:03.200]   Cause I have some things to say about this one.
[00:09:03.200 --> 00:09:07.800]   Andy Rubin, unfortunately, we've decided to make Andy Rubin a poster child and his issue,
[00:09:07.800 --> 00:09:14.400]   um, while not great was not actually to me the most problematic.
[00:09:14.400 --> 00:09:24.520]   So his was consensual, uh, sexual contact, I guess with a, with a colleague, um, whereas
[00:09:24.520 --> 00:09:29.480]   I found some of the other allegations such as the hardware, the head of Google X hardware,
[00:09:29.480 --> 00:09:32.800]   I found his allegations way more troubling.
[00:09:32.800 --> 00:09:37.400]   Now he's out by the way, he just resigned and without a currently compensation.
[00:09:37.400 --> 00:09:38.400]   Good.
[00:09:38.400 --> 00:09:44.240]   And I also found, so what's interesting here is there is the degree of, uh, the, the,
[00:09:44.240 --> 00:09:50.440]   uh, stories and how, how they kind of fall across a spectrum of incredibly egregious,
[00:09:50.440 --> 00:09:52.800]   such as the hardware dude, uh, proposition.
[00:09:52.800 --> 00:09:54.960]   Sexual misconduct covers old broad range.
[00:09:54.960 --> 00:09:56.120]   Yes.
[00:09:56.120 --> 00:10:03.200]   So that guy propositioning a 24 year old during a job interview feels like, Hey buddy, that's
[00:10:03.200 --> 00:10:04.200]   just not cool.
[00:10:04.200 --> 00:10:05.200]   Well, it's even worse than that.
[00:10:05.200 --> 00:10:09.400]   Cause he, he says he knew she wasn't going to get hired, but he misrepresented it and
[00:10:09.400 --> 00:10:11.960]   invited her to burning man with him.
[00:10:11.960 --> 00:10:13.560]   Yeah, is that worse?
[00:10:13.560 --> 00:10:17.040]   I think it's worse just to proposition someone in a job interview.
[00:10:17.040 --> 00:10:18.000]   I'm going to go with that.
[00:10:18.000 --> 00:10:20.800]   I mean, had she gotten hired in this situation?
[00:10:20.800 --> 00:10:21.800]   Would that have been great?
[00:10:21.800 --> 00:10:22.800]   Yeah.
[00:10:22.800 --> 00:10:23.800]   That's bad too.
[00:10:23.800 --> 00:10:27.440]   But, but she didn't get hired and it was it because she turned him down.
[00:10:27.440 --> 00:10:29.040]   We're talking about Rich Duvall.
[00:10:29.040 --> 00:10:33.960]   He, uh, is the guy behind, one of the founders of Project Loon and, uh, he's, he did not
[00:10:33.960 --> 00:10:36.480]   receive an exit package when he resigned today.
[00:10:36.480 --> 00:10:37.480]   Yay.
[00:10:37.480 --> 00:10:46.720]   And then the Drummond case was also troubling in a different way because it showed how men
[00:10:46.720 --> 00:10:53.520]   get protected in a way even among a consensual relationship between people in their departments
[00:10:53.520 --> 00:10:59.440]   without a policy put in place by HR dealing with dating between colleagues.
[00:10:59.440 --> 00:11:04.040]   He apparently fathered a child with a subordinate.
[00:11:04.040 --> 00:11:09.320]   Which again, that's, that's totally consensual, which is very, you know, which.
[00:11:09.320 --> 00:11:10.320]   Yeah.
[00:11:10.320 --> 00:11:14.120]   So some of this is a violation of company policy because you're not supposed to have a relationship
[00:11:14.120 --> 00:11:19.840]   with a subordinate, a direct report and they moved her from being a direct report.
[00:11:19.840 --> 00:11:24.520]   But unfortunately they moved her and she went down in the company and she moved out of the
[00:11:24.520 --> 00:11:27.600]   legal department to sales.
[00:11:27.600 --> 00:11:32.280]   She was a contract manager on Google's legal team and she felt like she was kind of pushed
[00:11:32.280 --> 00:11:34.280]   aside basically.
[00:11:34.280 --> 00:11:35.280]   Yes.
[00:11:35.280 --> 00:11:37.720]   And continued as chief counsel or general counsel.
[00:11:37.720 --> 00:11:42.800]   In this fall, I mean, that is unsurprising to me, but it follows along with an entire
[00:11:42.800 --> 00:11:50.200]   culture of, uh, promiscuity is the wrong thing, but the founders of Google, the CEO of Google,
[00:11:50.200 --> 00:11:55.280]   the former CEO of Google, sorry, uh, they dated people within the company.
[00:11:55.280 --> 00:12:01.720]   They had a well publicized kind of laissez-faire attitude towards relationships with people,
[00:12:01.720 --> 00:12:05.840]   which is not in and of itself terrible.
[00:12:05.840 --> 00:12:11.960]   But when you pull all of that together, you see how this happened and it is incredibly
[00:12:11.960 --> 00:12:12.960]   frustrating.
[00:12:12.960 --> 00:12:16.720]   And that is that sort of culture is terrible for women.
[00:12:16.720 --> 00:12:18.480]   I mean, it's a goal.
[00:12:18.480 --> 00:12:19.480]   You never know.
[00:12:19.480 --> 00:12:26.720]   Emits and go who was the head of, uh, a search at Google was, uh, forced out, went to Uber
[00:12:26.720 --> 00:12:31.160]   and then got fired at Uber because he didn't tell them he'd left Google over a sexual harassment
[00:12:31.160 --> 00:12:32.160]   allegation, right?
[00:12:32.160 --> 00:12:36.040]   Well, the thing that I found interesting in the story, I think the time story was very
[00:12:36.040 --> 00:12:43.040]   good is that part of the compensation of Andy Rubin is that, is that Larry Page saying
[00:12:43.040 --> 00:12:47.800]   that Andy Rubin didn't get full value for what he brought to Google in Android.
[00:12:47.800 --> 00:12:49.320]   Google made billions and billions.
[00:12:49.320 --> 00:12:50.320]   Exactly.
[00:12:50.320 --> 00:12:51.320]   It continues to make billions on Android.
[00:12:51.320 --> 00:12:55.920]   But what's interesting to me in that is that there, there, there's a screwed up structure
[00:12:55.920 --> 00:13:00.480]   here of kind of the star system and the person versus the product, right?
[00:13:00.480 --> 00:13:06.800]   I mean, in a proper world, any Rubin should have had being simplistic here, but such appropriate
[00:13:06.800 --> 00:13:10.560]   ownership in Android, if he was that valuable to it, that he got competent, that the compensation
[00:13:10.560 --> 00:13:15.880]   came through that instead of saying that he's the star who deserves the money, right?
[00:13:15.880 --> 00:13:19.440]   The money should flow through the company and the creation in a way, if that makes any
[00:13:19.440 --> 00:13:21.080]   sense at all.
[00:13:21.080 --> 00:13:26.760]   But it became kind of a guy, guy, star, star thing where, well, he brought such value.
[00:13:26.760 --> 00:13:29.480]   We did make such a freaking fortune on what he brought.
[00:13:29.480 --> 00:13:34.000]   That's, that's, that's way of good by doing, but paying a lot of money.
[00:13:34.000 --> 00:13:37.400]   To my mind, he should have already had that compensation built in somehow.
[00:13:37.400 --> 00:13:41.240]   And it was, it was a judgment of Android's value versus his value.
[00:13:41.240 --> 00:13:42.480]   That makes any sense.
[00:13:42.480 --> 00:13:44.000]   Well, it does.
[00:13:44.000 --> 00:13:48.440]   Now, when you buy a company, you, it's kind of like investing in a house, right?
[00:13:48.440 --> 00:13:53.320]   When you buy a company through M&A, you are making an investment in most cases.
[00:13:53.320 --> 00:14:00.240]   So they invested in that in Google, in Android, the value accelerated.
[00:14:00.240 --> 00:14:05.440]   And yes, his, his current compensation should reflect that.
[00:14:05.440 --> 00:14:09.760]   But once he's pushed out, you have to, you have to try the line.
[00:14:09.760 --> 00:14:14.760]   You have to say, hey, I know it would be awesome as as we keep making money on Android, you
[00:14:14.760 --> 00:14:19.480]   keep making money on Android, but A, the deal wasn't set up that way in the first place.
[00:14:19.480 --> 00:14:24.960]   And B, you shouldn't be able to benefit from an investment on something if you've been
[00:14:24.960 --> 00:14:27.320]   kicked out of the company for sexual misconduct.
[00:14:27.320 --> 00:14:30.800]   I don't, I mean, we don't let felons tell you.
[00:14:30.800 --> 00:14:31.800]   Yeah, yeah, yeah.
[00:14:31.800 --> 00:14:36.520]   So I guess really for us, the big story is not, I mean, not these individual cases, but
[00:14:36.520 --> 00:14:44.120]   is there some sort of corporate culture, toxic culture at Google?
[00:14:44.120 --> 00:14:46.120]   And does it persist today?
[00:14:46.120 --> 00:14:47.120]   Are we asking that question?
[00:14:47.120 --> 00:14:53.880]   Because yes, yes and yes, and there has been and there will be until we get people to actually
[00:14:53.880 --> 00:14:54.880]   clean it up.
[00:14:54.880 --> 00:14:57.920]   I mean, this is actually probably a really good sign.
[00:14:57.920 --> 00:15:04.000]   And I think this reckoning that's happening across Silicon Valley is, it's really a good
[00:15:04.000 --> 00:15:05.000]   sign.
[00:15:05.000 --> 00:15:09.920]   And I'm glad that companies are doing this as painful and as excruciating as it is for
[00:15:09.920 --> 00:15:10.920]   them.
[00:15:10.920 --> 00:15:12.880]   You think Google's reformed?
[00:15:12.880 --> 00:15:14.440]   I think they're trying to reform.
[00:15:14.440 --> 00:15:15.440]   They're trying.
[00:15:15.440 --> 00:15:16.440]   I think it's right.
[00:15:16.440 --> 00:15:19.240]   I mean, how many employees does Google have now?
[00:15:19.240 --> 00:15:20.240]   Lots.
[00:15:20.240 --> 00:15:21.240]   Lots.
[00:15:21.240 --> 00:15:22.720]   Like official answer.
[00:15:22.720 --> 00:15:24.480]   Tens of thousands.
[00:15:24.480 --> 00:15:25.480]   Yes.
[00:15:25.480 --> 00:15:32.440]   So you don't, you can't change that overnight, but they should be trying to change that.
[00:15:32.440 --> 00:15:38.520]   And hopefully they can and will, I think hiring, you know, hey, more women broadening your hiring
[00:15:38.520 --> 00:15:41.040]   pool will probably help.
[00:15:41.040 --> 00:15:48.560]   So maybe you don't go to the same universities or maybe you don't take recommendations for
[00:15:48.560 --> 00:15:49.560]   certain people.
[00:15:49.560 --> 00:15:51.560]   I don't know.
[00:15:51.560 --> 00:15:52.560]   Yeah.
[00:15:52.560 --> 00:15:57.880]   And the other question is with lack of diversity, I think you're right Stacey, the diverse hiring
[00:15:57.880 --> 00:15:59.360]   is the key solution.
[00:15:59.360 --> 00:16:03.240]   And diverse promoting more important is key solution.
[00:16:03.240 --> 00:16:09.880]   And one wonders with such low numbers on race, what the situation is like for people of color
[00:16:09.880 --> 00:16:12.280]   in the company too, but it's Stacey's right.
[00:16:12.280 --> 00:16:14.000]   It's endemic not just to Google.
[00:16:14.000 --> 00:16:15.000]   It's the valley.
[00:16:15.000 --> 00:16:17.080]   Is it also the valley in an intense field?
[00:16:17.080 --> 00:16:22.520]   I feel like Google is kind of the same as Uber in this respect that when they started,
[00:16:22.520 --> 00:16:25.240]   it was small, it was a bunch of graduate students.
[00:16:25.240 --> 00:16:27.400]   They had very loose kind of.
[00:16:27.400 --> 00:16:28.400]   Yes.
[00:16:28.400 --> 00:16:30.960]   Pop practices.
[00:16:30.960 --> 00:16:32.280]   And they didn't mature.
[00:16:32.280 --> 00:16:38.240]   The company as it got bigger and more valuable and more importantly, had a greater impact on
[00:16:38.240 --> 00:16:39.240]   society.
[00:16:39.240 --> 00:16:40.240]   They didn't mature.
[00:16:40.240 --> 00:16:43.600]   They still kind of maintain that party atmosphere.
[00:16:43.600 --> 00:16:50.360]   I think there is a culture of immaturity across most of the startup world.
[00:16:50.360 --> 00:16:53.960]   And part of that is the, I mean, we can call it the bro culture.
[00:16:53.960 --> 00:16:59.480]   I don't know if it really is, but it is the work hard, move fast, break things, right?
[00:16:59.480 --> 00:17:04.360]   That is very much an immature philosophy, right?
[00:17:04.360 --> 00:17:11.080]   It is very cavalier in a way that maturity tends to change you and make you less cavalier
[00:17:11.080 --> 00:17:12.080]   about the world.
[00:17:12.080 --> 00:17:14.920]   And that has its pros and cons, right?
[00:17:14.920 --> 00:17:20.960]   And I think the other thing to think about here is how Wall Street has rewarded the tech
[00:17:20.960 --> 00:17:21.960]   community.
[00:17:21.960 --> 00:17:28.040]   And as the tech community became such a hot commodity and generated so much wealth, a
[00:17:28.040 --> 00:17:33.960]   lot of the people who used to want to work on Wall Street went to tech.
[00:17:33.960 --> 00:17:40.400]   And I think you have to think about the type of person who values incredible amounts of
[00:17:40.400 --> 00:17:42.320]   money and is willing to risk it all.
[00:17:42.320 --> 00:17:48.040]   We portray this as geeky engineers trying to do things, but that's actually not entirely
[00:17:48.040 --> 00:17:49.040]   true.
[00:17:49.040 --> 00:17:53.640]   It is a geeky engineer who is willing to do maybe cross some ethical lines.
[00:17:53.640 --> 00:17:59.600]   Maybe it's a geeky engineer who's willing to work, you know, a hundred hours a week.
[00:17:59.600 --> 00:18:04.360]   There's a very similar persona at work there, if that makes sense.
[00:18:04.360 --> 00:18:07.560]   Did I not explain that?
[00:18:07.560 --> 00:18:15.320]   Yeah, on the one hand, there's an optimism and an empowerment that I so admire about
[00:18:15.320 --> 00:18:17.320]   the technologists on the other hand.
[00:18:17.320 --> 00:18:19.680]   I don't think that's actually true.
[00:18:19.680 --> 00:18:20.760]   I think they come out.
[00:18:20.760 --> 00:18:22.560]   I was about to the other hand.
[00:18:22.560 --> 00:18:23.560]   I was sorry.
[00:18:23.560 --> 00:18:25.960]   I'm like, okay, on the other hand.
[00:18:25.960 --> 00:18:32.960]   I think there's, I think the architecturally there is that and these tools are empowering.
[00:18:32.960 --> 00:18:35.000]   And that is what I admire about Silicon Valley.
[00:18:35.000 --> 00:18:39.880]   But the problem is you have to have a little foot and boldness to reinvent the world.
[00:18:39.880 --> 00:18:41.560]   And yes, there are arrogance too, right?
[00:18:41.560 --> 00:18:45.920]   You have to, I mean, Steve Jobs said the thing that changed his life was realizing that the
[00:18:45.920 --> 00:18:46.920]   world was created.
[00:18:46.920 --> 00:18:49.720]   The rules were created by people not any smarter than themselves.
[00:18:49.720 --> 00:18:53.200]   And he could do whatever he wanted and look at Elon Musk and look, I think that that
[00:18:53.200 --> 00:18:55.200]   kind of world changing vision.
[00:18:55.200 --> 00:18:59.600]   I mean, you might not want to be friends with these people, but it is.
[00:18:59.600 --> 00:19:01.680]   I so fundamentally disagree.
[00:19:01.680 --> 00:19:04.840]   And I think that is the mythic founder.
[00:19:04.840 --> 00:19:06.960]   That's the founder myth that we put up there.
[00:19:06.960 --> 00:19:10.680]   But there are founders who are not.
[00:19:10.680 --> 00:19:16.840]   There are founders whose companies do well, who actually treat people well.
[00:19:16.840 --> 00:19:21.960]   And so yes, you have to be willing to take risks, but you don't have to be willing to
[00:19:21.960 --> 00:19:28.680]   take risks at the expense of an entire population of people or at the expense of like doing,
[00:19:28.680 --> 00:19:30.600]   knowing that you're doing something wrong.
[00:19:30.600 --> 00:19:34.960]   You know, I think you see Mark Betty off trying to be a model in that way.
[00:19:34.960 --> 00:19:40.720]   But even running the small business that I run, one of the things that's hard about
[00:19:40.720 --> 00:19:45.600]   running a business is that sometimes you have to kind of be in it.
[00:19:45.600 --> 00:19:48.920]   You have to do things that as a human, you wouldn't want to do like fire people.
[00:19:48.920 --> 00:19:50.720]   You wouldn't want to do that.
[00:19:50.720 --> 00:19:54.440]   Okay, there's a good of the business for often for the good of the business, you have to
[00:19:54.440 --> 00:19:56.000]   be a little ruthless.
[00:19:56.000 --> 00:19:57.000]   Yeah.
[00:19:57.000 --> 00:20:01.080]   There is a difference between being a little bit of an in being.
[00:20:01.080 --> 00:20:04.480]   And I'm hoping that I'm only a little bit.
[00:20:04.480 --> 00:20:08.160]   We're going to have to believe this episode 10 times.
[00:20:08.160 --> 00:20:12.880]   And you can't you can't I mean, firing someone for cause is actually not being an.
[00:20:12.880 --> 00:20:13.880]   I'm sorry.
[00:20:13.880 --> 00:20:16.240]   It's someone saying, let me be a different scenario.
[00:20:16.240 --> 00:20:17.240]   Okay.
[00:20:17.240 --> 00:20:22.280]   And I was just I was just reading about a company.
[00:20:22.280 --> 00:20:27.560]   And I can't remember what it was where a long time employee just didn't have a role
[00:20:27.560 --> 00:20:28.760]   anymore in the company.
[00:20:28.760 --> 00:20:31.440]   I think it was Amazon.
[00:20:31.440 --> 00:20:36.640]   And and I think he Bezos just, you know, kind of, Oh, no, Netflix.
[00:20:36.640 --> 00:20:37.840]   This was the Netflix debate.
[00:20:37.840 --> 00:20:38.840]   Okay.
[00:20:38.840 --> 00:20:42.240]   We can bring Netflix in the Netflix culture, which is radical.
[00:20:42.240 --> 00:20:46.920]   They say, and I don't know one way or the other radical transparency and honesty.
[00:20:46.920 --> 00:20:53.000]   But it sometimes it looks pretty ruthless because the fat, the founder of Netflix read
[00:20:53.000 --> 00:20:54.880]   Hastings had to fire.
[00:20:54.880 --> 00:21:01.440]   I think it was a high school friend because he he just no longer was of use to the company,
[00:21:01.440 --> 00:21:04.320]   not that he did something wrong.
[00:21:04.320 --> 00:21:06.240]   But that is an okay reason to fire.
[00:21:06.240 --> 00:21:10.920]   Why you have to if you, you know, Steve Jobs always said, you don't want any, you need
[00:21:10.920 --> 00:21:14.240]   all employees because you bring in one C employee.
[00:21:14.240 --> 00:21:18.680]   They may not do anything wrong, but they bring the whole level down because everybody stops
[00:21:18.680 --> 00:21:19.800]   performing.
[00:21:19.800 --> 00:21:22.120]   You have to be a little ruthless in those.
[00:21:22.120 --> 00:21:27.680]   By the way, I am not that ruthless, but everybody was a day.
[00:21:27.680 --> 00:21:28.680]   I know.
[00:21:28.680 --> 00:21:30.160]   They're all a is it.
[00:21:30.160 --> 00:21:36.720]   But I have had to let go of somebody not because he was bad or wasn't a great hard worker,
[00:21:36.720 --> 00:21:38.120]   but because we didn't have a role for him.
[00:21:38.120 --> 00:21:39.120]   Yeah.
[00:21:39.120 --> 00:21:40.120]   And that's very, very hard.
[00:21:40.120 --> 00:21:42.280]   It is hard, but it is fair.
[00:21:42.280 --> 00:21:43.840]   And there's ways you can do that.
[00:21:43.840 --> 00:21:48.400]   So at Giga, only we had a person who in Giga may not be a shining example because it did
[00:21:48.400 --> 00:21:54.680]   implode, but we had a person who their role just disappeared and that's why he gave them
[00:21:54.680 --> 00:22:00.080]   two months and said, Hey, we, we don't have a spot for you.
[00:22:00.080 --> 00:22:06.040]   Please go use this time, still contribute to the company, but also find another job.
[00:22:06.040 --> 00:22:07.120]   And that's what this person did.
[00:22:07.120 --> 00:22:08.960]   And it's because we had trust among our employees.
[00:22:08.960 --> 00:22:14.120]   And I guarantee you a place like 37 signals, no, yes, that's the name of the company.
[00:22:14.120 --> 00:22:15.840]   Yeah, they used to be 37 signals.
[00:22:15.840 --> 00:22:17.160]   They've changed their name, but I know.
[00:22:17.160 --> 00:22:18.160]   Oh, now there's something there.
[00:22:18.160 --> 00:22:20.080]   I mean, there is a way to do this.
[00:22:20.080 --> 00:22:23.600]   And maybe yes, you're not going to generate all the profits.
[00:22:23.600 --> 00:22:28.400]   That's why Giga owns a terrible example, because right now we are cutting costs in people,
[00:22:28.400 --> 00:22:35.920]   you put making more money in front of how you treat people and a company is a company.
[00:22:35.920 --> 00:22:37.920]   Here's here's the company.
[00:22:37.920 --> 00:22:38.920]   Here's the Wall Street Journal.
[00:22:38.920 --> 00:22:39.920]   You can make money.
[00:22:39.920 --> 00:22:44.840]   Here's the Wall Street Journal article from last week about Netflix's ruthless culture.
[00:22:44.840 --> 00:22:47.880]   People who speak highly of Netflix's culture say the company's critics included former
[00:22:47.880 --> 00:22:51.320]   employees were fired for performance reasons and are disgruntled about their personal
[00:22:51.320 --> 00:22:52.320]   experiences.
[00:22:52.320 --> 00:22:54.360]   They talk about the keeper test.
[00:22:54.360 --> 00:22:59.120]   Several formal colleagues describe Mr. Hastings in complimentary terms as quote unencumbered
[00:22:59.120 --> 00:23:00.820]   by emotion.
[00:23:00.820 --> 00:23:04.040]   And just the past year and a half, Mr. Hastings has employed the keeper test.
[00:23:04.040 --> 00:23:08.400]   Let me see if I get the keeper test.
[00:23:08.400 --> 00:23:09.600]   Is this person a keeper?
[00:23:09.600 --> 00:23:10.840]   Is this person worth keeping?
[00:23:10.840 --> 00:23:11.840]   Yeah.
[00:23:11.840 --> 00:23:13.120]   And are they worth fighting for it?
[00:23:13.120 --> 00:23:14.120]   That's the other thing.
[00:23:14.120 --> 00:23:16.960]   Yeah, would you employees are encouraged to give one another blunt feedback?
[00:23:16.960 --> 00:23:20.760]   Managers are told to imply a keeper test to their staff asking themselves whether they
[00:23:20.760 --> 00:23:26.680]   would fight to keep a given employee, a mantra for firing people who don't fit the culture
[00:23:26.680 --> 00:23:28.120]   and ensuring only the strongest survive.
[00:23:28.120 --> 00:23:31.720]   In other words, if you wouldn't fight to keep that employee, then we're getting rid of
[00:23:31.720 --> 00:23:32.720]   them.
[00:23:32.720 --> 00:23:37.080]   Last year, Hastings fired Neil Hunt, the longtime chief product officer who had helped create
[00:23:37.080 --> 00:23:40.560]   Netflix famed algorithm that curates programming for viewers.
[00:23:40.560 --> 00:23:46.440]   He was one of Netflix earliest employees and had been Mr. Hastings close friend for decades.
[00:23:46.440 --> 00:23:50.440]   Mr. Hastings told Mr. Hunt a lot had changed.
[00:23:50.440 --> 00:23:54.200]   And one of your underlings, Greg Peters, is now better for the job.
[00:23:54.200 --> 00:23:56.880]   You're gone.
[00:23:56.880 --> 00:23:57.880]   So that's pretty ruthless.
[00:23:57.880 --> 00:23:58.880]   Look at GE.
[00:23:58.880 --> 00:24:04.000]   GE, which was vaunted now GE is nearing death practically.
[00:24:04.000 --> 00:24:05.800]   But GE is at its height.
[00:24:05.800 --> 00:24:08.480]   The belief was you constantly called.
[00:24:08.480 --> 00:24:10.200]   I consulted for a company for a while.
[00:24:10.200 --> 00:24:14.060]   See, I think that seems like a horrible environment and I wouldn't want to run a company that
[00:24:14.060 --> 00:24:15.060]   way.
[00:24:15.060 --> 00:24:19.200]   But honestly, this isn't the most successful company ever.
[00:24:19.200 --> 00:24:24.680]   And if you want to be an Amazon or a Netflix or a Google, you might have to be that way.
[00:24:24.680 --> 00:24:31.400]   I was in a company that was acquired and part of the rule when it was acquired was you
[00:24:31.400 --> 00:24:36.280]   have A, B and C employees and you need to get rid of your C employees and B is the new
[00:24:36.280 --> 00:24:37.280]   C.
[00:24:37.280 --> 00:24:40.880]   And that wasn't wrong in an acquisition.
[00:24:40.880 --> 00:24:45.000]   You know, Stacy's earlier point, Stacy, the thing that's even worse than what you describe
[00:24:45.000 --> 00:24:47.520]   is where I've hired the wrong person.
[00:24:47.520 --> 00:24:48.520]   I made a mistake.
[00:24:48.520 --> 00:24:50.280]   Well, it's always that way for me, right?
[00:24:50.280 --> 00:24:52.200]   Because it's all people I've hired.
[00:24:52.200 --> 00:24:55.360]   Well, sometimes people just screw up and do the wrong things and you can say, well, they're
[00:24:55.360 --> 00:24:56.640]   not doing the job anymore.
[00:24:56.640 --> 00:25:01.840]   But I feel awful when after the fact I realized, well, I shouldn't have that person the first
[00:25:01.840 --> 00:25:03.640]   place, they weren't right for it.
[00:25:03.640 --> 00:25:07.200]   And it's my fault that they have to uproot their lives.
[00:25:07.200 --> 00:25:10.000]   Now, that's what it feels the works to me.
[00:25:10.000 --> 00:25:11.080]   Oh, it's horrible.
[00:25:11.080 --> 00:25:13.800]   It's never ever ever.
[00:25:13.800 --> 00:25:18.920]   Even if it's for cause, it's never a pleasant thing ever.
[00:25:18.920 --> 00:25:22.720]   And actually I take pride in the fact and Lisa does too that we hate it.
[00:25:22.720 --> 00:25:24.720]   Because that tells me at least I'm not horrible.
[00:25:24.720 --> 00:25:25.720]   They're nice to you.
[00:25:25.720 --> 00:25:26.720]   It breaks my heart.
[00:25:26.720 --> 00:25:27.720]   Oh, I had one employer.
[00:25:27.720 --> 00:25:30.080]   I had one employer that I stole this to you.
[00:25:30.080 --> 00:25:34.920]   One guy burst into tears.
[00:25:34.920 --> 00:25:37.120]   And then I so I said, okay, this was my mistake.
[00:25:37.120 --> 00:25:40.040]   I said, okay, you can keep your job.
[00:25:40.040 --> 00:25:43.640]   And then we had a fire them a few months later because he continued to screw up.
[00:25:43.640 --> 00:25:45.120]   Oh, Leo.
[00:25:45.120 --> 00:25:46.120]   Wow.
[00:25:46.120 --> 00:25:48.240]   Well, I don't want to be that guy.
[00:25:48.240 --> 00:25:52.160]   That's my point is that sometimes this kind of brutal culture.
[00:25:52.160 --> 00:25:53.360]   Well, wait, okay.
[00:25:53.360 --> 00:25:55.360]   I think you guys are confusing some things.
[00:25:55.360 --> 00:25:57.960]   And I think it's really important to not confuse those.
[00:25:57.960 --> 00:26:03.960]   There is brutal culture where you're like, you know, fight club, call everybody who
[00:26:03.960 --> 00:26:08.800]   doesn't perform 100% or even 95 and above all the time, right?
[00:26:08.800 --> 00:26:11.920]   Which I think is a pretty harsh culture, brutal.
[00:26:11.920 --> 00:26:17.280]   And there's firing someone or having a culture where you expect people to perform most of
[00:26:17.280 --> 00:26:18.280]   the time.
[00:26:18.280 --> 00:26:19.880]   And if they don't, they're gone.
[00:26:19.880 --> 00:26:23.320]   I mean, having fired people, it is not fun.
[00:26:23.320 --> 00:26:29.040]   And you have a duty to do it in a way that is respectful of what they've given the company.
[00:26:29.040 --> 00:26:30.040]   Yes.
[00:26:30.040 --> 00:26:33.640]   But it is not brutal to fire someone if they're not performing.
[00:26:33.640 --> 00:26:37.400]   It's not brutal to fire someone if you're going in a different direction.
[00:26:37.400 --> 00:26:42.680]   It is brutal to fire someone if you are callous about it.
[00:26:42.680 --> 00:26:54.280]   And it's culture, talk overdone, Stacey and Leo, so much talk in Silicon Valley is about
[00:26:54.280 --> 00:26:57.320]   instilling a culture and instilling the culture and all your new employees and doing all these
[00:26:57.320 --> 00:26:58.320]   things.
[00:26:58.320 --> 00:26:59.680]   Is that a bunch of who we or is it real?
[00:26:59.680 --> 00:27:00.680]   Oh, thanks.
[00:27:00.680 --> 00:27:07.560]   I think it's maybe more talked about in Silicon Valley because it's a culture of its culture.
[00:27:07.560 --> 00:27:11.360]   There I'm saying it's a there are a lot of engineering people.
[00:27:11.360 --> 00:27:13.440]   I'm quote unquote data driven.
[00:27:13.440 --> 00:27:18.240]   So anything that's fluffy or not, you can't put metrics to.
[00:27:18.240 --> 00:27:20.640]   I think that becomes kind of culture.
[00:27:20.640 --> 00:27:25.240]   And then you can use it poorly and say, hey, this person is in a culture fit because their
[00:27:25.240 --> 00:27:29.840]   name isn't Jared and they didn't go to my college.
[00:27:29.840 --> 00:27:36.600]   Or you can say, hey, the people at this company tend to behave a certain way and we value
[00:27:36.600 --> 00:27:38.080]   the following things.
[00:27:38.080 --> 00:27:42.800]   So in corporate America, instead of culture, we say mission statement, right?
[00:27:42.800 --> 00:27:47.720]   That's in Silicon Valley kind of the way I think it is.
[00:27:47.720 --> 00:27:49.720]   I think you're going to get a culture.
[00:27:49.720 --> 00:27:51.840]   Well, yeah, you're going to get a culture.
[00:27:51.840 --> 00:27:52.840]   One way or the other.
[00:27:52.840 --> 00:27:53.840]   Always behave one way.
[00:27:53.840 --> 00:27:54.940]   Yeah, you're going to get a culture.
[00:27:54.940 --> 00:28:02.000]   So better to make a conscious or at least attempt to make a conscious and consciously an appropriate
[00:28:02.000 --> 00:28:11.320]   and see, the problem is the human beings aren't businesses, even despite Supreme Court rulings
[00:28:11.320 --> 00:28:13.400]   to the contrary.
[00:28:13.400 --> 00:28:16.720]   And the business is really a very different thing.
[00:28:16.720 --> 00:28:21.640]   And as I don't think particularly humane, I mean, I don't know.
[00:28:21.640 --> 00:28:22.640]   Right.
[00:28:22.640 --> 00:28:29.160]   I'm a terrible businessman, but it strikes me that you'd be better if you were completely
[00:28:29.160 --> 00:28:31.560]   ruthless in business.
[00:28:31.560 --> 00:28:33.440]   No, because no one would want to work for you.
[00:28:33.440 --> 00:28:40.040]   Well, but was the sense that you reward excellence and that you let go anything less than excellent
[00:28:40.040 --> 00:28:41.960]   and that it would want to work for you.
[00:28:41.960 --> 00:28:44.360]   Because you know it going in.
[00:28:44.360 --> 00:28:45.600]   This is the way that was a Netflix.
[00:28:45.600 --> 00:28:47.400]   Apparently is that way as well?
[00:28:47.400 --> 00:28:48.400]   Well, that's true.
[00:28:48.400 --> 00:28:52.160]   And as well, I mean, in some cases, as long as it's advertised, like Netflix has this quote
[00:28:52.160 --> 00:28:53.160]   unquote brutal culture.
[00:28:53.160 --> 00:28:54.720]   Would I want to work there?
[00:28:54.720 --> 00:28:55.720]   Probably not.
[00:28:55.720 --> 00:28:58.720]   No, you'd be happy working here, I think.
[00:28:58.720 --> 00:29:01.720]   Because we're very...
[00:29:01.720 --> 00:29:03.920]   Yes, because apparently if you fire me, I can just cry.
[00:29:03.920 --> 00:29:06.080]   You cry and I'll go, I'm sorry.
[00:29:06.080 --> 00:29:07.080]   I'm sorry.
[00:29:07.080 --> 00:29:09.080]   I didn't mean it.
[00:29:09.080 --> 00:29:10.080]   I didn't mean it.
[00:29:10.080 --> 00:29:13.440]   But I don't know where I was going with that.
[00:29:13.440 --> 00:29:14.440]   This is...
[00:29:14.440 --> 00:29:16.800]   I fantasize about working for you, Leo.
[00:29:16.800 --> 00:29:18.320]   I would be a good boss.
[00:29:18.320 --> 00:29:19.320]   I fantasize about that.
[00:29:19.320 --> 00:29:21.160]   I think that would be an enjoyable life.
[00:29:21.160 --> 00:29:22.600]   Well, in a way, you both do, right?
[00:29:22.600 --> 00:29:24.440]   I mean, you're hosts on the show.
[00:29:24.440 --> 00:29:25.840]   We pay you to be on the show.
[00:29:25.840 --> 00:29:28.880]   And I mean, you're not employees, you're contractors.
[00:29:28.880 --> 00:29:31.920]   But yeah, I mean, I think we have good goals.
[00:29:31.920 --> 00:29:35.200]   We try to make the people who work here happy.
[00:29:35.200 --> 00:29:36.880]   We have massages once a month.
[00:29:36.880 --> 00:29:38.880]   We used to have them weekly.
[00:29:38.880 --> 00:29:42.120]   We fed everybody lunch today, every Wednesday you get lunch.
[00:29:42.120 --> 00:29:46.200]   We're not Google, you don't get lunch every day.
[00:29:46.200 --> 00:29:51.600]   If you think about business, most companies think about business only in terms of profit,
[00:29:51.600 --> 00:29:52.600]   right?
[00:29:52.600 --> 00:29:54.000]   So how much money you've made.
[00:29:54.000 --> 00:29:55.400]   There are other businesses out there.
[00:29:55.400 --> 00:29:59.160]   I know it is, but that is not the only way to think about a business.
[00:29:59.160 --> 00:30:00.160]   You can actually...
[00:30:00.160 --> 00:30:01.160]   I mean, there are social...
[00:30:01.160 --> 00:30:02.640]   What are those corporations called?
[00:30:02.640 --> 00:30:06.320]   Yeah, I feel like we have a family here, but we're a small business too.
[00:30:06.320 --> 00:30:07.760]   It's only 20 people.
[00:30:07.760 --> 00:30:08.760]   Yeah.
[00:30:08.760 --> 00:30:15.440]   And when you think about making money, if you think about not just making money, but
[00:30:15.440 --> 00:30:19.880]   making optimizing for every penny, you're going to make different decisions than if
[00:30:19.880 --> 00:30:24.920]   you were a business that was going to optimize for worker productivity, right?
[00:30:24.920 --> 00:30:27.080]   So you've got to think about that.
[00:30:27.080 --> 00:30:30.120]   And part of this will depend on the type of industry you're in.
[00:30:30.120 --> 00:30:35.800]   In tech where there's high margins, it makes sense actually to invest in productivity.
[00:30:35.800 --> 00:30:41.680]   And it may be kindness, so you hire a different and wider worldview.
[00:30:41.680 --> 00:30:42.680]   I don't know.
[00:30:42.680 --> 00:30:45.680]   Text also highly competitive, right?
[00:30:45.680 --> 00:30:47.200]   In some areas.
[00:30:47.200 --> 00:30:52.080]   And there's this huge amount of paranoia.
[00:30:52.080 --> 00:30:56.520]   The growth of CEO Intel wrote, "Only the paranoid survive."
[00:30:56.520 --> 00:30:58.120]   You have to always be looking over your shoulder.
[00:30:58.120 --> 00:30:59.560]   The other guys catching up with you.
[00:30:59.560 --> 00:31:02.720]   Those are the kinds of things that make a culture ruthless in a company.
[00:31:02.720 --> 00:31:06.400]   No, I mean, I'm always looking over my shoulder for my business.
[00:31:06.400 --> 00:31:09.560]   I'm like, "Oh, shoot, what's happening?
[00:31:09.560 --> 00:31:11.960]   What kind of trends do I need to keep aware of?"
[00:31:11.960 --> 00:31:14.080]   So I'm always looking over for that.
[00:31:14.080 --> 00:31:15.400]   But it doesn't...
[00:31:15.400 --> 00:31:18.480]   I still have a choice in how I behave towards people.
[00:31:18.480 --> 00:31:22.880]   If I look up and I say, "Hey, IoT is only going to be hot for six more months."
[00:31:22.880 --> 00:31:25.760]   Kevin's not any good at AI, which is the next big thing.
[00:31:25.760 --> 00:31:29.360]   So let me just fire Kevin and bring out some now.
[00:31:29.360 --> 00:31:30.560]   What are you watching?
[00:31:30.560 --> 00:31:31.560]   Evan?
[00:31:31.560 --> 00:31:32.560]   Evan?
[00:31:32.560 --> 00:31:33.560]   I mean...
[00:31:33.560 --> 00:31:34.560]   Better put that resume out.
[00:31:34.560 --> 00:31:35.560]   Got it?
[00:31:35.560 --> 00:31:36.560]   No.
[00:31:36.560 --> 00:31:38.160]   That would be a hard thing to do, wouldn't it?
[00:31:38.160 --> 00:31:40.720]   Yes, but when I look over my shoulder, let's say I'm looking over...
[00:31:40.720 --> 00:31:42.320]   I'm paranoid because I am.
[00:31:42.320 --> 00:31:43.480]   I never think.
[00:31:43.480 --> 00:31:46.680]   I'm like, "Oh, my God, tomorrow everything could dry up."
[00:31:46.680 --> 00:31:51.920]   So if I'm thinking along those lines, then I might say, "Oh, crap, Kevin, hey, I'm seeing
[00:31:51.920 --> 00:31:53.680]   this coming.
[00:31:53.680 --> 00:31:54.800]   What do you want to do about it?
[00:31:54.800 --> 00:31:56.800]   Here are our options."
[00:31:56.800 --> 00:32:00.160]   And Kevin can say, "Stacey, I hate AI.
[00:32:00.160 --> 00:32:01.640]   I hate industrial IoT.
[00:32:01.640 --> 00:32:06.280]   I'm going to go work on my Chromebook site aboutcrobooks.com."
[00:32:06.280 --> 00:32:09.840]   And that's a nice...
[00:32:09.840 --> 00:32:10.840]   Not nice.
[00:32:10.840 --> 00:32:15.320]   I think that's a human way of looking at it versus me saying, "Oh, this is coming" and
[00:32:15.320 --> 00:32:17.560]   freaking out and then killing.
[00:32:17.560 --> 00:32:18.560]   Yeah.
[00:32:18.560 --> 00:32:20.480]   I don't know.
[00:32:20.480 --> 00:32:24.840]   Maybe I'm never going to be a multi-billionaire founder of a media company.
[00:32:24.840 --> 00:32:25.840]   That's exactly my point.
[00:32:25.840 --> 00:32:27.680]   But I also want that.
[00:32:27.680 --> 00:32:29.840]   I don't want it either.
[00:32:29.840 --> 00:32:31.680]   Fortunately, Lisa does.
[00:32:31.680 --> 00:32:32.680]   You two would lose.
[00:32:32.680 --> 00:32:34.680]   I never had the courage to do that.
[00:32:34.680 --> 00:32:38.200]   And you're fired!
[00:32:38.200 --> 00:32:39.200]   I don't know.
[00:32:39.200 --> 00:32:44.160]   I understand why corporate culture can be different and that sometimes they can be ruthless.
[00:32:44.160 --> 00:32:47.520]   And I think Silicon Valley in particular lends itself to that.
[00:32:47.520 --> 00:32:51.280]   It's also different when a company is big because it's not as personal.
[00:32:51.280 --> 00:32:56.680]   And so the less personal becomes the easier it becomes to do the "right thing" rather
[00:32:56.680 --> 00:32:58.680]   than the humane thing.
[00:32:58.680 --> 00:33:01.280]   I don't know.
[00:33:01.280 --> 00:33:05.600]   I didn't expect to get into this conversation, but I think it's a good conversation.
[00:33:05.600 --> 00:33:07.320]   Like from sexual harassment?
[00:33:07.320 --> 00:33:08.320]   Was that what we were talking about?
[00:33:08.320 --> 00:33:11.160]   We started with Andy Rubin and that's how this became.
[00:33:11.160 --> 00:33:12.520]   Let's take a little break.
[00:33:12.520 --> 00:33:14.280]   You guys pick your next story.
[00:33:14.280 --> 00:33:15.280]   Oh, okay.
[00:33:15.280 --> 00:33:16.760]   Oh, we got to work for our money.
[00:33:16.760 --> 00:33:18.640]   We got to work for your money now.
[00:33:18.640 --> 00:33:22.600]   Well, I have so many good ones and I don't know where we want to go next.
[00:33:22.600 --> 00:33:27.240]   But I'll let you each pick something that you think is worth studying.
[00:33:27.240 --> 00:33:30.280]   While I mention privacy.com, I...
[00:33:30.280 --> 00:33:32.320]   This is a new sponsor.
[00:33:32.320 --> 00:33:36.120]   But a company I've been using for a long time, I can't remember when I set up my account
[00:33:36.120 --> 00:33:37.880]   almost a year ago now.
[00:33:37.880 --> 00:33:42.840]   Because I wanted a way to use a credit card online and feel good about it, to feel secure
[00:33:42.840 --> 00:33:43.840]   about it.
[00:33:43.840 --> 00:33:45.640]   And how many times have your credit card been stolen online?
[00:33:45.640 --> 00:33:47.560]   Mine, half a dozen times.
[00:33:47.560 --> 00:33:51.040]   And it's kind of a pain, especially because you have your credit card attached to a variety
[00:33:51.040 --> 00:33:53.280]   of accounts and you've got to go change in everything.
[00:33:53.280 --> 00:33:58.200]   With privacy.com, I never use the same credit card twice.
[00:33:58.200 --> 00:34:00.040]   You don't use the same password on every site.
[00:34:00.040 --> 00:34:01.040]   You know that's a bad idea.
[00:34:01.040 --> 00:34:03.120]   Same thing with a credit card.
[00:34:03.120 --> 00:34:07.840]   Privacy.com generates a brand new virtual Visa card number for every purchase you make
[00:34:07.840 --> 00:34:08.920]   online with one click.
[00:34:08.920 --> 00:34:13.840]   The minute that number is used by a merchant, it locks to that merchant.
[00:34:13.840 --> 00:34:16.160]   So only that merchant can use it.
[00:34:16.160 --> 00:34:21.440]   You can set a limit on spending per charge per month per year.
[00:34:21.440 --> 00:34:25.000]   And it's easy to freeze or unfreeze cards.
[00:34:25.000 --> 00:34:28.120]   So you'll never worry about being billed twice or upgraded without your consent.
[00:34:28.120 --> 00:34:29.280]   It's easy to...
[00:34:29.280 --> 00:34:31.280]   I always use it for subscriptions.
[00:34:31.280 --> 00:34:34.400]   You don't have to worry about forgetting to cancel subscriptions or free trials.
[00:34:34.400 --> 00:34:36.000]   Just cancel the card.
[00:34:36.000 --> 00:34:37.000]   And on your...
[00:34:37.000 --> 00:34:40.000]   The dashboard, you'll immediately know...
[00:34:40.000 --> 00:34:45.600]   In fact, you might know that a company's been hacked before the company knows thanks
[00:34:45.600 --> 00:34:47.440]   to privacy.com.
[00:34:47.440 --> 00:34:54.840]   Another feature I really love about privacy.com is that I don't have to use my real address
[00:34:54.840 --> 00:34:56.560]   when using the card.
[00:34:56.560 --> 00:35:02.400]   So when I create a credit card and I use it, I can use any address I want.
[00:35:02.400 --> 00:35:05.560]   I don't want to three Main Street, any town USA.
[00:35:05.560 --> 00:35:06.840]   And privacy approves the charge.
[00:35:06.840 --> 00:35:10.400]   The merchant does not get my address.
[00:35:10.400 --> 00:35:11.760]   Complete anonymity.
[00:35:11.760 --> 00:35:14.160]   I love that.
[00:35:14.160 --> 00:35:17.360]   Privacy.com is easy to use because they have an extension for Chrome.
[00:35:17.360 --> 00:35:22.080]   They have an Android and iOS app that will quickly generate the cards easily on demand
[00:35:22.080 --> 00:35:24.320]   within the browser if you want.
[00:35:24.320 --> 00:35:27.760]   The cards are tied to your checking account or your debit card account.
[00:35:27.760 --> 00:35:29.480]   So these are not...
[00:35:29.480 --> 00:35:30.840]   These are not credit cards.
[00:35:30.840 --> 00:35:33.000]   It's a debit card in effect.
[00:35:33.000 --> 00:35:35.680]   That means no interest, but it's also no cost to you.
[00:35:35.680 --> 00:35:39.360]   They make their money on the charge card fees to the merchant.
[00:35:39.360 --> 00:35:46.120]   In fact, they make so much money, you even can get money back up to 5% back depending
[00:35:46.120 --> 00:35:48.160]   on how much you use the card.
[00:35:48.160 --> 00:35:50.480]   I just love this.
[00:35:50.480 --> 00:35:53.240]   I just think it's the best way to go.
[00:35:53.240 --> 00:35:55.840]   Privacy.com's PCIDSS compliant.
[00:35:55.840 --> 00:35:58.880]   That's the same security standard your bank card sell to.
[00:35:58.880 --> 00:36:01.880]   They use of course high-grade encryption.
[00:36:01.880 --> 00:36:09.120]   In fact, they use split-key crypto so that even at privacy.com, no individual employee
[00:36:09.120 --> 00:36:10.360]   can see your account.
[00:36:10.360 --> 00:36:13.360]   The partial keys are hit by separate employees.
[00:36:13.360 --> 00:36:17.400]   So access to the data on the server requires multiple keys to decrypt.
[00:36:17.400 --> 00:36:20.320]   It's kind of like the nuclear codes.
[00:36:20.320 --> 00:36:21.600]   I think this is brilliant.
[00:36:21.600 --> 00:36:25.200]   I use two-factor authentication, of course.
[00:36:25.200 --> 00:36:29.560]   They provide customers access to transaction data via a webhook API.
[00:36:29.560 --> 00:36:33.680]   So you can write your own little tool if you wish to scrape your privacy account.
[00:36:33.680 --> 00:36:35.040]   I get text messages.
[00:36:35.040 --> 00:36:37.600]   There's a great website with all sorts of information.
[00:36:37.600 --> 00:36:39.360]   100% free to use.
[00:36:39.360 --> 00:36:40.360]   So do it now.
[00:36:40.360 --> 00:36:42.160]   Just go to privacy.com/twig.
[00:36:42.160 --> 00:36:45.920]   A little incentive will give you $5 off your first purchase.
[00:36:45.920 --> 00:36:47.920]   A little credit towards your first purchase.
[00:36:47.920 --> 00:36:51.520]   If you go to privacy.com/twig.
[00:36:51.520 --> 00:36:56.640]   privacy.com has saved customers over $100 million and unwanted and unauthorized charges due to
[00:36:56.640 --> 00:37:02.320]   compromised cards, hidden fees, forgotten subscriptions.
[00:37:02.320 --> 00:37:04.200]   It's the only card I use online now.
[00:37:04.200 --> 00:37:05.280]   I love it.
[00:37:05.280 --> 00:37:08.280]   privacy.com/twig.
[00:37:08.280 --> 00:37:11.640]   I could show you, actually.
[00:37:11.640 --> 00:37:12.640]   I did the other day.
[00:37:12.640 --> 00:37:16.400]   I could show you all my privacy.com cards because you can't use them.
[00:37:16.400 --> 00:37:19.560]   Only the merchant that uses the card first gets to use it.
[00:37:19.560 --> 00:37:20.760]   It's locked into that merchant.
[00:37:20.760 --> 00:37:21.760]   I love it.
[00:37:21.760 --> 00:37:26.400]   privacy.com/twig.
[00:37:26.400 --> 00:37:33.680]   Stacey Higgenbotham, StaceyOnIOT.com, the Stacey OnIOT podcast with Kevin Tofel.
[00:37:33.680 --> 00:37:37.880]   Apparently who's on the chopping block.
[00:37:37.880 --> 00:37:41.120]   We might be watching this.
[00:37:41.120 --> 00:37:42.600]   No, we're teasing Kevin.
[00:37:42.600 --> 00:37:45.680]   You know you're not on the chopping block.
[00:37:45.680 --> 00:37:47.360]   Actually I could see it move towards AI.
[00:37:47.360 --> 00:37:49.760]   Isn't AI kind of the next big thing?
[00:37:49.760 --> 00:37:55.160]   Well, A, it's the current big thing and it's an integral part of IoT.
[00:37:55.160 --> 00:37:56.160]   So I'm not.
[00:37:56.160 --> 00:37:57.160]   For instance.
[00:37:57.160 --> 00:37:58.160]   It's not hard.
[00:37:58.160 --> 00:37:59.160]   Here's my story.
[00:37:59.160 --> 00:38:04.880]   Google and Roomba are working together because Roomba is that vacuum cleaner.
[00:38:04.880 --> 00:38:06.920]   We have one that goes around your house vacuuming.
[00:38:06.920 --> 00:38:08.880]   It has a map of your home.
[00:38:08.880 --> 00:38:13.120]   Googling and Roomba want to share that data.
[00:38:13.120 --> 00:38:18.560]   The aim to make smart homes more thoughtful by leveraging the unique data set collected
[00:38:18.560 --> 00:38:22.520]   by iRobot, a map of your house.
[00:38:22.520 --> 00:38:25.440]   You must have covered this on the IOT podcast.
[00:38:25.440 --> 00:38:26.440]   We did.
[00:38:26.440 --> 00:38:27.440]   We did talk about it this morning.
[00:38:27.440 --> 00:38:28.440]   Yeah.
[00:38:28.440 --> 00:38:29.440]   What do you think?
[00:38:29.440 --> 00:38:31.840]   So one, it's opt in.
[00:38:31.840 --> 00:38:38.280]   Two, the data that's pulled from this is not going to go to Google's advertising business.
[00:38:38.280 --> 00:38:43.040]   So from our perspective, there's a couple of things that are important to note here.
[00:38:43.040 --> 00:38:47.520]   One, Google's actually letting you say you want to share this data.
[00:38:47.520 --> 00:38:51.920]   They're also offering consumers something in exchange for this data, which is, and I
[00:38:51.920 --> 00:38:56.720]   don't know how valuable this is, you get to make that call, you can say, hey, gee, send
[00:38:56.720 --> 00:38:58.160]   the Roomba to the kitchen.
[00:38:58.160 --> 00:38:59.160]   Yeah.
[00:38:59.160 --> 00:39:00.160]   And that's the kitchen.
[00:39:00.160 --> 00:39:02.320]   So that's like, oh, is that worth it?
[00:39:02.320 --> 00:39:03.560]   I might be.
[00:39:03.560 --> 00:39:04.560]   Yeah.
[00:39:04.560 --> 00:39:08.200]   And then Roomba, get the hell out of my room.
[00:39:08.200 --> 00:39:09.280]   Yeah.
[00:39:09.280 --> 00:39:14.080]   So with this information, though, Google can start providing context in the smart home,
[00:39:14.080 --> 00:39:19.800]   which is actually really important because right now we don't have context or good context.
[00:39:19.800 --> 00:39:24.600]   And we suffer for it because we have to program all this stuff and explain all these things
[00:39:24.600 --> 00:39:26.640]   via weird.
[00:39:26.640 --> 00:39:29.240]   No man cloud like turn on movie night or all of that.
[00:39:29.240 --> 00:39:36.960]   So and if you don't want to do this, you are not automatically opted in.
[00:39:36.960 --> 00:39:37.960]   So you have to opt out.
[00:39:37.960 --> 00:39:39.760]   So that's really important.
[00:39:39.760 --> 00:39:40.760]   I agree.
[00:39:40.760 --> 00:39:42.880]   And this is only a certain type of Roomba, by the way, y'all.
[00:39:42.880 --> 00:39:43.880]   I agree.
[00:39:43.880 --> 00:39:44.880]   So, Casey, how did you work on this?
[00:39:44.880 --> 00:39:45.880]   Sure.
[00:39:45.880 --> 00:39:46.880]   I saw in the story.
[00:39:46.880 --> 00:39:47.880]   Sorry.
[00:39:47.880 --> 00:39:48.880]   As a question.
[00:39:48.880 --> 00:39:50.840]   I saw on the story, it also said that it could help in terms of things like saying which
[00:39:50.840 --> 00:39:55.560]   light to turn on and what part of the room and how does that work?
[00:39:55.560 --> 00:39:58.920]   Not just with the Roomba, but with the rest of IoT.
[00:39:58.920 --> 00:40:01.480]   What could Google bring you in its home suite?
[00:40:01.480 --> 00:40:06.400]   Well, so if it knows that there are lights on in a room and it knows you're not home,
[00:40:06.400 --> 00:40:10.640]   maybe it can send you a thing saying, hey, you're not here, but all your lights are on.
[00:40:10.640 --> 00:40:13.800]   Do you want me to turn some off?
[00:40:13.800 --> 00:40:14.800]   I don't know.
[00:40:14.800 --> 00:40:18.280]   You're going to need this kind of information if you want a smart home.
[00:40:18.280 --> 00:40:20.040]   If you want a smart home, you will need this.
[00:40:20.040 --> 00:40:24.480]   If you want a home that can do this for you, what I really don't want to see, though, is
[00:40:24.480 --> 00:40:29.320]   I would rather, and we'll see, this is going to be a really arduous, crappy process.
[00:40:29.320 --> 00:40:34.080]   Amazon does this already with hunches, where if you do something, it's like, hey, by the
[00:40:34.080 --> 00:40:35.640]   way, I see this is happening.
[00:40:35.640 --> 00:40:38.440]   Do you want to fix that?
[00:40:38.440 --> 00:40:41.320]   I don't actually want my home to talk to me so much.
[00:40:41.320 --> 00:40:43.280]   My home has never taught me.
[00:40:43.280 --> 00:40:50.520]   I want it to get good really fast so it doesn't actually require my input, but we are going
[00:40:50.520 --> 00:40:52.680]   to go through this awkward adolescent space.
[00:40:52.680 --> 00:40:57.200]   Let me ask a couple of things here.
[00:40:57.200 --> 00:41:04.040]   What was the Google cell phone thing where you could map the room for AR and stuff?
[00:41:04.040 --> 00:41:05.040]   What was that called?
[00:41:05.040 --> 00:41:06.040]   The project.
[00:41:06.040 --> 00:41:07.040]   Tango.
[00:41:07.040 --> 00:41:16.840]   So is this a less ambitious version, is this data almost as good as what Tango would deliver?
[00:41:16.840 --> 00:41:18.200]   I don't know, actually.
[00:41:18.200 --> 00:41:22.160]   So these cameras, they have low-light, low-light.
[00:41:22.160 --> 00:41:28.520]   They have low-resolution cameras on them, and they have an understanding of depth.
[00:41:28.520 --> 00:41:33.000]   I don't know if they can identify the brand of dishwasher you're using, for example.
[00:41:33.000 --> 00:41:34.000]   Right.
[00:41:34.000 --> 00:41:35.600]   But here's the other thing about this.
[00:41:35.600 --> 00:41:39.240]   What I keep on fantasizing about, and that I'm surprised I haven't seen more action in
[00:41:39.240 --> 00:41:42.960]   this, is imagine the utility of this for blind people.
[00:41:42.960 --> 00:41:53.160]   I've long thought that you have something in your ear that gives you a sound thing.
[00:41:53.160 --> 00:41:57.440]   You follow the sound a little bit left, a little bit right to go where you want to go.
[00:41:57.440 --> 00:42:03.040]   But if you map environments, internal and external, I just think that could be incredibly
[00:42:03.040 --> 00:42:06.160]   freeing, and I'm shocked that I haven't seen more efforts to do that.
[00:42:06.160 --> 00:42:09.160]   Maybe I'm just being naive, maybe they're out there, I don't know.
[00:42:09.160 --> 00:42:13.120]   But this kind of data makes that possible.
[00:42:13.120 --> 00:42:16.560]   You're muted, Stacey.
[00:42:16.560 --> 00:42:18.560]   You're muted, Stacey.
[00:42:18.560 --> 00:42:19.560]   Sorry.
[00:42:19.560 --> 00:42:24.400]   So focus on this for a moment, Jeff, and Leo, you talk, and I will tell you in a moment
[00:42:24.400 --> 00:42:25.840]   once I look it up.
[00:42:25.840 --> 00:42:29.840]   The company that is doing this for blind people with a pair of glasses.
[00:42:29.840 --> 00:42:30.840]   Yeah.
[00:42:30.840 --> 00:42:33.560]   It's kind of like that.
[00:42:33.560 --> 00:42:37.080]   Anyway, I think it's a good idea.
[00:42:37.080 --> 00:42:38.080]   It's a good idea.
[00:42:38.080 --> 00:42:45.000]   And there is an AI, I had a caller on my radio show this weekend.
[00:42:45.000 --> 00:42:46.000]   Aria.
[00:42:46.000 --> 00:42:47.000]   Aria.
[00:42:47.000 --> 00:42:49.000]   No, A-I-R-A.
[00:42:49.000 --> 00:42:50.000]   I-R-A.
[00:42:50.000 --> 00:42:51.000]   Sorry.
[00:42:51.000 --> 00:42:52.000]   Yes.
[00:42:52.000 --> 00:42:53.000]   That makes more sense.
[00:42:53.000 --> 00:42:54.000]   I've been in reality.
[00:42:54.000 --> 00:42:55.520]   Oh, that's neat.
[00:42:55.520 --> 00:42:56.520]   Using AI.
[00:42:56.520 --> 00:42:57.520]   Wow.
[00:42:57.520 --> 00:42:59.200]   Wait, is this the same thing?
[00:42:59.200 --> 00:43:02.240]   I think this is what the guy called me on the radio show to tell me about.
[00:43:02.240 --> 00:43:03.800]   Oh, there you go.
[00:43:03.800 --> 00:43:04.800]   Yeah.
[00:43:04.800 --> 00:43:05.800]   Yeah.
[00:43:05.800 --> 00:43:10.720]   Which way connects them to a trained professional agent, which dedicated to further enhanced
[00:43:10.720 --> 00:43:11.720]   this book is a few.
[00:43:11.720 --> 00:43:12.720]   This is like Be My Eyes.
[00:43:12.720 --> 00:43:13.720]   Be My Eyes does this too.
[00:43:13.720 --> 00:43:14.720]   Oh, yes.
[00:43:14.720 --> 00:43:19.680]   But there is one that will, there is an Android app that will read money, tell you what color
[00:43:19.680 --> 00:43:24.400]   something is, will give you a lot of feedback about your environment.
[00:43:24.400 --> 00:43:29.160]   And I think it's just the next step is to say, you're on Main Street.
[00:43:29.160 --> 00:43:32.920]   There's going to be an intersection in 30 feet.
[00:43:32.920 --> 00:43:39.040]   The button to press the walk sign is on the left, that kind of thing.
[00:43:39.040 --> 00:43:44.200]   And I have seen, maybe it sticks that have sound as you move around.
[00:43:44.200 --> 00:43:45.680]   But I have seen stuff like that.
[00:43:45.680 --> 00:43:46.680]   It's out there.
[00:43:46.680 --> 00:43:48.240]   There's a cane with haptic feedback.
[00:43:48.240 --> 00:43:49.240]   Yeah, that's it.
[00:43:49.240 --> 00:43:50.480]   It was a project though.
[00:43:50.480 --> 00:43:51.480]   Yeah.
[00:43:51.480 --> 00:43:52.480]   It's not a--
[00:43:52.480 --> 00:43:57.560]   If you consider what you can do with driving now, and you consider the level of data that
[00:43:57.560 --> 00:44:03.400]   exists around environments, and you consider the abilities to give various forms of feedback.
[00:44:03.400 --> 00:44:09.120]   Words, sounds, haptic warnings.
[00:44:09.120 --> 00:44:10.880]   It's not a big market.
[00:44:10.880 --> 00:44:13.000]   There's not a lot of money to be had there.
[00:44:13.000 --> 00:44:16.240]   But it just strikes me that it could be so life changing.
[00:44:16.240 --> 00:44:17.240]   This is the--
[00:44:17.240 --> 00:44:18.240]   Thanks to the chat room.
[00:44:18.240 --> 00:44:19.960]   This is the app.
[00:44:19.960 --> 00:44:23.240]   It's called Seeing AI.
[00:44:23.240 --> 00:44:26.440]   And it is-- actually, maybe it was Johnny Jett told us about that.
[00:44:26.440 --> 00:44:30.520]   It's a free app that narrates the world around you.
[00:44:30.520 --> 00:44:39.120]   So as you walk around, the app will tell you what's going on.
[00:44:39.120 --> 00:44:40.120]   So that's--
[00:44:40.120 --> 00:44:41.800]   Is that a face recognition too?
[00:44:41.800 --> 00:44:43.400]   Yeah, it looks like it had face for this.
[00:44:43.400 --> 00:44:45.760]   By the way, folks, a good use of it.
[00:44:45.760 --> 00:44:47.680]   Yes, that's right.
[00:44:47.680 --> 00:44:48.680]   Seeing AI.
[00:44:48.680 --> 00:44:52.440]   And it's a experiment to describe your surroundings with Seeing AI.
[00:44:52.440 --> 00:44:56.040]   Seeing AI is an ongoing research project.
[00:44:56.040 --> 00:45:00.880]   And in the app, we've also included some of our more experimental features.
[00:45:00.880 --> 00:45:04.280]   There you have to turn those on in settings.
[00:45:04.280 --> 00:45:10.040]   One of these is the ability to describe a general scene.
[00:45:10.040 --> 00:45:11.040]   When you take a photo--
[00:45:11.040 --> 00:45:12.040]   I'm civic in that.
[00:45:12.040 --> 00:45:13.040]   See all the other--
[00:45:13.040 --> 00:45:14.040]   The algorithm.
[00:45:14.040 --> 00:45:15.040]   Take more things in the world.
[00:45:15.040 --> 00:45:16.040]   Yeah, yeah, yeah.
[00:45:16.040 --> 00:45:17.040]   Processing.
[00:45:17.040 --> 00:45:20.200]   Probably a man sitting on a couch using a laptop.
[00:45:20.200 --> 00:45:21.200]   Wow.
[00:45:21.200 --> 00:45:23.200]   Take picture.
[00:45:23.200 --> 00:45:24.200]   Processing.
[00:45:24.200 --> 00:45:26.800]   A bus that is parked on the side of the road.
[00:45:26.800 --> 00:45:29.480]   Button.
[00:45:29.480 --> 00:45:31.320]   Take picture.
[00:45:31.320 --> 00:45:32.600]   Processing.
[00:45:32.600 --> 00:45:36.240]   Probably a box filled with different types of food on a table.
[00:45:36.240 --> 00:45:40.200]   So this is kind of what we were talking about the other day.
[00:45:40.200 --> 00:45:48.280]   With the bananas, Seeing AI, it's Microsoft.com/Seeing-AI.
[00:45:48.280 --> 00:45:51.800]   And it is available for--
[00:45:51.800 --> 00:45:53.600]   It looks like iOS only.
[00:45:53.600 --> 00:45:55.600]   It's not Android.
[00:45:55.600 --> 00:45:57.120]   Well, why are we there?
[00:45:57.120 --> 00:46:00.200]   Of course, you can scan barcodes using audio beeps to guide you.
[00:46:00.200 --> 00:46:02.760]   Speak text, point it as sign, read text.
[00:46:02.760 --> 00:46:04.840]   Recognize friends, Jeff.
[00:46:04.840 --> 00:46:09.160]   And describe faces and emotions.
[00:46:09.160 --> 00:46:10.080]   Wow.
[00:46:10.080 --> 00:46:13.680]   Well, the other use of that is for autism.
[00:46:13.680 --> 00:46:14.280]   Right.
[00:46:14.280 --> 00:46:14.800]   Emotions.
[00:46:14.800 --> 00:46:20.160]   Being able to describe emotions and give cues that someone can't see.
[00:46:20.160 --> 00:46:23.880]   It's just where people get all moral panicky about, oh my god, all about things they can
[00:46:23.880 --> 00:46:24.080]   do.
[00:46:24.080 --> 00:46:31.040]   They just-- I just want a constant refrain of yes, but look to what this can do.
[00:46:31.040 --> 00:46:32.760]   Well, here's one to keep in mind.
[00:46:32.760 --> 00:46:38.360]   And you can totally see how with spectacles and augmented reality, this could be huge.
[00:46:38.360 --> 00:46:39.680]   Oh, yeah.
[00:46:39.680 --> 00:46:42.440]   It can also be a little--
[00:46:42.440 --> 00:46:47.640]   we'll have to adjust to this because I don't, for example, want to walk into a bar and have
[00:46:47.640 --> 00:46:52.240]   everyone look at me and know instantly who I am.
[00:46:52.240 --> 00:46:52.520]   Right.
[00:46:52.520 --> 00:46:53.640]   I mean, that is--
[00:46:53.640 --> 00:46:54.440]   No, no, I agree.
[00:46:54.440 --> 00:46:55.280]   --the point we're describing.
[00:46:55.280 --> 00:46:57.320]   And it is--
[00:46:57.320 --> 00:46:58.600]   But that's a matter of norms.
[00:46:58.600 --> 00:47:04.040]   If the blind person can find you, and it's only somebody in your contact list, presumably.
[00:47:04.040 --> 00:47:04.520]   Right.
[00:47:04.520 --> 00:47:09.600]   I mean, there's ways you can say there's appropriate uses and inappropriate uses of any technology.
[00:47:09.600 --> 00:47:09.880]   Right.
[00:47:09.880 --> 00:47:14.240]   But we tend to, in the tech world, open it up to everyone, realize that, oh, maybe that
[00:47:14.240 --> 00:47:15.680]   wasn't a great idea.
[00:47:15.680 --> 00:47:18.240]   And then-- Because that's up to us as the people to--
[00:47:18.240 --> 00:47:19.760]   now, the tech world shouldn't police us.
[00:47:19.760 --> 00:47:21.440]   We should police ourselves.
[00:47:21.440 --> 00:47:21.920]   No.
[00:47:21.920 --> 00:47:23.400]   Think about something like this.
[00:47:23.400 --> 00:47:25.520]   Let's pretend that I had a stalker.
[00:47:25.520 --> 00:47:25.880]   Or--
[00:47:25.880 --> 00:47:27.640]   No, I understand why you don't want this.
[00:47:27.640 --> 00:47:30.080]   I think it's absolutely legitimate.
[00:47:30.080 --> 00:47:32.560]   And Jeff, I would encourage you, actually.
[00:47:32.560 --> 00:47:34.920]   So in college, I did have a stalker.
[00:47:34.920 --> 00:47:36.680]   It was a very scary experience.
[00:47:36.680 --> 00:47:41.600]   And I will tell you that I come to technology with a very different perspective
[00:47:41.600 --> 00:47:46.160]   because of that experience that is not techno panic.
[00:47:46.160 --> 00:47:48.320]   It is really a matter of self-preservation.
[00:47:48.320 --> 00:47:53.600]   So just run-- when you get excited about something, run that through the hands--
[00:47:53.600 --> 00:47:59.240]   or run that through the scenario of, if I am a person who does not want to be recognized
[00:47:59.240 --> 00:48:04.560]   because it's actively dangerous for me, what are the tools in place to make that safer?
[00:48:04.560 --> 00:48:05.240]   Good advice.
[00:48:05.240 --> 00:48:08.200]   Good advice, yes.
[00:48:08.200 --> 00:48:14.080]   But I also want to say that before we get regulators ready to ban something
[00:48:14.080 --> 00:48:16.920]   as a German regulator was going to ban the use of geo and face
[00:48:16.920 --> 00:48:22.760]   and recognition in the same thing, imagine the benefits at the same time.
[00:48:22.760 --> 00:48:24.560]   Well, and that gets kind of back to this whole--
[00:48:24.560 --> 00:48:26.200]   And then you have a discussion about what you do.
[00:48:26.200 --> 00:48:28.280]   Yeah, and that gets back to this culture thing, right?
[00:48:28.280 --> 00:48:31.560]   Like, there's lots of people in the world of Silicon Valley
[00:48:31.560 --> 00:48:33.400]   that have never had a stalker, right?
[00:48:33.400 --> 00:48:35.600]   They're not thinking like that.
[00:48:35.600 --> 00:48:40.400]   Whereas if you get-- yeah, if you get more people involved to have that,
[00:48:40.400 --> 00:48:45.520]   then this is one way that diversity actually helps improve technology
[00:48:45.520 --> 00:48:50.000]   in a way that maybe makes regulation less necessary.
[00:48:50.000 --> 00:48:51.880]   Yeah.
[00:48:51.880 --> 00:48:53.800]   All right, I said you guys could pick stories.
[00:48:53.800 --> 00:48:56.360]   Stacey, what's your story?
[00:48:56.360 --> 00:49:00.760]   My story is from the Atlantic-- oh, we want to talk about techno panic.
[00:49:00.760 --> 00:49:03.480]   This is about the Amazon selling machine.
[00:49:03.480 --> 00:49:08.200]   And I liked the story to a certain point.
[00:49:08.200 --> 00:49:11.000]   I think it's really important to draw people's attention
[00:49:11.000 --> 00:49:16.200]   to how Amazon's ad business works, where it gets its data from,
[00:49:16.200 --> 00:49:19.800]   and how we're increasingly buying products like Kindles and tablets
[00:49:19.800 --> 00:49:24.840]   and God Only Knows What Else that are showing us highly relevant ads
[00:49:24.840 --> 00:49:29.800]   and why advertisers are really excited about it because Amazon has
[00:49:29.800 --> 00:49:32.360]   so much data about purchase intent.
[00:49:32.360 --> 00:49:38.160]   And this comes from Amazon's quarterly result, which said that they are a huge ad company.
[00:49:38.160 --> 00:49:43.680]   They are now number three after Facebook and Google selling digital ads.
[00:49:43.680 --> 00:49:44.360]   Yep.
[00:49:44.360 --> 00:49:45.360]   Wow.
[00:49:45.360 --> 00:49:50.920]   Brands will spend $4.6 billion in advertising on the Amazon platform this year,
[00:49:50.920 --> 00:49:53.240]   according to e-marketer.
[00:49:53.240 --> 00:49:54.920]   Wow.
[00:49:54.920 --> 00:49:58.040]   And I think it's going to reach--
[00:49:58.040 --> 00:49:59.720]   $16 billion by 2020.
[00:49:59.720 --> 00:50:03.240]   $8 billion this year, $20, $20, $16 billion,
[00:50:03.240 --> 00:50:05.520]   according to Piper Jeffery.
[00:50:05.520 --> 00:50:09.960]   And in fact, that means it will make them more money than its big profit center right now,
[00:50:09.960 --> 00:50:11.880]   Amazon Web Services.
[00:50:11.880 --> 00:50:13.880]   That assumes Web Services stays still.
[00:50:13.880 --> 00:50:14.560]   Yeah.
[00:50:14.560 --> 00:50:19.480]   But I guess the point is that Amazon now has a huge incentive
[00:50:19.480 --> 00:50:20.920]   to beef up its advertising.
[00:50:20.920 --> 00:50:22.160]   And it obviously has been--
[00:50:22.160 --> 00:50:23.840]   Well, it's less an incentive.
[00:50:23.840 --> 00:50:28.560]   It's more that it's drawing attention to the fact that Amazon has this huge advertising
[00:50:28.560 --> 00:50:30.880]   business that none of us really think about.
[00:50:30.880 --> 00:50:33.840]   And the key to that is data.
[00:50:33.840 --> 00:50:34.440]   Right.
[00:50:34.440 --> 00:50:36.400]   They have so much data.
[00:50:36.400 --> 00:50:38.360]   Well, and they have the most important kind of data.
[00:50:38.360 --> 00:50:41.840]   So search data used to be the best data because you were like, well, someone's looking for it.
[00:50:41.840 --> 00:50:43.080]   They really want it.
[00:50:43.080 --> 00:50:45.320]   Amazon has purchased data.
[00:50:45.320 --> 00:50:45.800]   Right.
[00:50:45.800 --> 00:50:47.280]   Purchased that intent.
[00:50:47.280 --> 00:50:48.800]   And intent.
[00:50:48.800 --> 00:50:53.360]   When I search Amazon, I'm searching for a product that I actually want to buy.
[00:50:53.360 --> 00:50:55.040]   I don't just window shop on Amazon.
[00:50:55.040 --> 00:50:55.680]   OK, I do.
[00:50:55.680 --> 00:51:03.560]   But anyway, but the reason I picked the story is because there is this--
[00:51:03.560 --> 00:51:10.880]   the perspective to make all this sound really evil was, oh, now Amazon has ads everywhere.
[00:51:10.880 --> 00:51:14.240]   And we're going to move to this era of hyper consumerism.
[00:51:14.240 --> 00:51:15.680]   And that's bad.
[00:51:15.680 --> 00:51:20.760]   And the implication is that it's Amazon's fault because they're showing us more ads.
[00:51:20.760 --> 00:51:27.160]   And I think going in with the social-- like changing our social habits and mores as we
[00:51:27.160 --> 00:51:34.320]   advance technology, this is a really important thing to talk about the role that consumerism
[00:51:34.320 --> 00:51:39.760]   needs to play in our lives as people become more adept at marketing to us.
[00:51:39.760 --> 00:51:44.120]   I think most people realize that just buying random stuff is a fairly hollow thing.
[00:51:44.120 --> 00:51:46.560]   I think we need to start having those conversations.
[00:51:46.560 --> 00:51:48.200]   We need to start talking to our kids about it.
[00:51:48.200 --> 00:51:52.760]   We need to start practicing what we preach on this front.
[00:51:52.760 --> 00:51:55.040]   And so I thought the story was interesting.
[00:51:55.040 --> 00:51:57.240]   I didn't love how it approached it.
[00:51:57.240 --> 00:52:00.080]   But I do think it's an important thing we should start talking about.
[00:52:00.080 --> 00:52:03.840]   And like everything we talk about, this is exactly the kind of thing you talk about,
[00:52:03.840 --> 00:52:06.600]   Jeff, where you could see some real benefits to consumers.
[00:52:06.600 --> 00:52:12.680]   If Amazon knew that I was running out of laundry powder and said, do you want some more?
[00:52:12.680 --> 00:52:13.680]   That would be a value.
[00:52:13.680 --> 00:52:17.760]   There's a lot of valuable things Amazon could do that would be advertising as well as many
[00:52:17.760 --> 00:52:23.040]   things that would not be so good and encouraging rank consumers.
[00:52:23.040 --> 00:52:28.560]   No, what still gets me is, and you could put in preferences, you could say that I want
[00:52:28.560 --> 00:52:32.920]   only things that come from renewable sources and they can start rating things, they can
[00:52:32.920 --> 00:52:34.360]   do all kinds of good things.
[00:52:34.360 --> 00:52:39.800]   But what gets me is the stupidity of programmatic retargeting and advertising.
[00:52:39.800 --> 00:52:43.520]   That is to say, you look at the boots on Amazon and they follow you around for months.
[00:52:43.520 --> 00:52:45.720]   Amazon knows you bought them.
[00:52:45.720 --> 00:52:46.720]   You buy the boots on Amazon.
[00:52:46.720 --> 00:52:48.720]   They follow you around for months.
[00:52:48.720 --> 00:52:50.440]   It's advertising, exactly.
[00:52:50.440 --> 00:52:52.800]   So that it's still, I've been pounding the desk.
[00:52:52.800 --> 00:52:53.800]   It's stupid.
[00:52:53.800 --> 00:52:57.120]   And figures, maybe you bought them before, maybe you want another pair.
[00:52:57.120 --> 00:52:58.120]   Yeah.
[00:52:58.120 --> 00:53:00.640]   Well, they're not thinking like people.
[00:53:00.640 --> 00:53:01.640]   No.
[00:53:01.640 --> 00:53:03.640]   So, yes, that's a thing.
[00:53:03.640 --> 00:53:07.880]   But I think the issue to think about here and what we should start talking about more
[00:53:07.880 --> 00:53:16.400]   is how do we teach our kids, how do we internalize within ourselves that we don't need to buy
[00:53:16.400 --> 00:53:20.000]   as much stuff, that we don't need to buy everything we see, that it's not going to
[00:53:20.000 --> 00:53:21.360]   make us happy.
[00:53:21.360 --> 00:53:22.360]   Because I think it's real.
[00:53:22.360 --> 00:53:23.360]   That's not up to Amazon.
[00:53:23.360 --> 00:53:24.360]   Yeah.
[00:53:24.360 --> 00:53:28.080]   I know that's, this story portrays like it's Amazon's fault.
[00:53:28.080 --> 00:53:33.080]   But I think it does illustrate an issue that we actually really do need to start talking
[00:53:33.080 --> 00:53:34.080]   about.
[00:53:34.080 --> 00:53:37.680]   And, you know, there's, there's the idea of like, is this like tobacco?
[00:53:37.680 --> 00:53:39.640]   Is it inherently addictive?
[00:53:39.640 --> 00:53:40.640]   I don't think so.
[00:53:40.640 --> 00:53:44.000]   But maybe if you show, I think for children it might be.
[00:53:44.000 --> 00:53:45.000]   You could weaponize it.
[00:53:45.000 --> 00:53:49.980]   I would never put it beyond Silicon Valley to figure out a way to skin her eyes, skin
[00:53:49.980 --> 00:53:52.960]   her box Amazon purchases.
[00:53:52.960 --> 00:53:54.680]   So, yeah.
[00:53:54.680 --> 00:53:55.680]   You get gold points.
[00:53:55.680 --> 00:54:00.560]   I don't know what SNH green stamps were really.
[00:54:00.560 --> 00:54:03.800]   Well, they have those now for like loot boxes.
[00:54:03.800 --> 00:54:06.400]   And can I just tell you for children today?
[00:54:06.400 --> 00:54:07.400]   Oh, loot boxes.
[00:54:07.400 --> 00:54:09.600]   Well, it's not just loot boxes.
[00:54:09.600 --> 00:54:13.400]   It's like every toy now has a secret.
[00:54:13.400 --> 00:54:17.800]   It's like the, I guess it's kind of like cracker jacks, but you buy like a six or $8 figuring
[00:54:17.800 --> 00:54:22.600]   and you having, and you have no idea what is inside it.
[00:54:22.600 --> 00:54:26.240]   My daughter is like sucked on these like little unicorn things.
[00:54:26.240 --> 00:54:27.480]   I don't even know what they are.
[00:54:27.480 --> 00:54:29.400]   Unicornios, unicorn mermaid combos.
[00:54:29.400 --> 00:54:30.880]   Here's the good news.
[00:54:30.880 --> 00:54:32.120]   She will grow out of it.
[00:54:32.120 --> 00:54:33.120]   I hope so.
[00:54:33.120 --> 00:54:34.120]   Oh, yeah.
[00:54:34.120 --> 00:54:37.600]   My daughter, you know, we went through beanie babies.
[00:54:37.600 --> 00:54:38.600]   We will.
[00:54:38.600 --> 00:54:39.600]   Yeah, but what are subscription boxes?
[00:54:39.600 --> 00:54:43.360]   Just some of them are just basically like, Oh, here's a mystery.
[00:54:43.360 --> 00:54:46.160]   Yeah, we're like, Oh, that's what loot boxes are.
[00:54:46.160 --> 00:54:47.160]   You bet.
[00:54:47.160 --> 00:54:48.880]   You know, it's going to be something good.
[00:54:48.880 --> 00:54:49.880]   It's gambling.
[00:54:49.880 --> 00:54:55.520]   Yeah, it's it's it is training them in a way and it's it's kind of affecting their brains.
[00:54:55.520 --> 00:54:56.520]   Yeah.
[00:54:56.520 --> 00:54:57.520]   Yeah.
[00:54:57.520 --> 00:54:58.520]   Yeah.
[00:54:58.520 --> 00:55:00.680]   Writers are from the chat room.
[00:55:00.680 --> 00:55:04.240]   Kids apps may have a lot more ads than you think.
[00:55:04.240 --> 00:55:09.200]   Those cute little apps your child plays with are likely flooded with ads, some of which
[00:55:09.200 --> 00:55:13.480]   are totally age inappropriate according to researchers at the Journal of Developmental
[00:55:13.480 --> 00:55:15.600]   and Behavioral Pediatrics.
[00:55:15.600 --> 00:55:24.240]   95% of commonly downloaded apps market to were played by children ages five and under
[00:55:24.240 --> 00:55:28.760]   contain advertising 95% five and under.
[00:55:28.760 --> 00:55:29.760]   Wow.
[00:55:29.760 --> 00:55:31.760]   That's because it's free.
[00:55:31.760 --> 00:55:33.860]   It's free.
[00:55:33.860 --> 00:55:38.040]   The kids often can't tell where the game leaves off and the ad begins.
[00:55:38.040 --> 00:55:41.400]   There's science to show that children aged eight and younger can't distinguish between
[00:55:41.400 --> 00:55:43.400]   content and advertising.
[00:55:43.400 --> 00:55:45.240]   They can't.
[00:55:45.240 --> 00:55:50.760]   And that's I mean, there is like when you advertise to children, you are really you have
[00:55:50.760 --> 00:55:53.400]   we should have much stricter regulations about that, I think.
[00:55:53.400 --> 00:55:54.400]   Yeah.
[00:55:54.400 --> 00:55:55.600]   Just because I've watched.
[00:55:55.600 --> 00:55:56.600]   Yeah.
[00:55:56.600 --> 00:56:00.800]   Well, we had them on TV, but I guess we haven't gotten around getting them for.
[00:56:00.800 --> 00:56:01.800]   Do you remember?
[00:56:01.800 --> 00:56:05.760]   Oh, they're bringing all these back now, but I just watched the Care Bears movie with my
[00:56:05.760 --> 00:56:08.580]   daughter because she was feeling nostalgic.
[00:56:08.580 --> 00:56:12.680]   And I was like, wow, this actually has content, but I rewatch T man.
[00:56:12.680 --> 00:56:15.580]   And I was like, oh, this was terrible.
[00:56:15.580 --> 00:56:16.780]   This was really bad.
[00:56:16.780 --> 00:56:20.640]   And I love that show and I have a Castle Gray school and all the she-ras and everything.
[00:56:20.640 --> 00:56:22.160]   This stuff is not new.
[00:56:22.160 --> 00:56:27.360]   No, any more than SNH green stamps is new, but it's been weaponized.
[00:56:27.360 --> 00:56:30.200]   It's become so much more effective than it used to be.
[00:56:30.200 --> 00:56:32.480]   All right, Jeff, you're pick.
[00:56:32.480 --> 00:56:41.160]   So either one that kind of I really loved this week was the dot new domain.
[00:56:41.160 --> 00:56:43.920]   Dot new.
[00:56:43.920 --> 00:56:49.440]   So that now if you want to start a new doc, you just put in doc.new and Google knows
[00:56:49.440 --> 00:56:52.640]   in Google Docs to give you a new doc like doc.
[00:56:52.640 --> 00:56:55.440]   Do you want me to call up Andrew?
[00:56:55.440 --> 00:56:56.440]   Wow.
[00:56:56.440 --> 00:56:58.600]   Andrew doesn't like this or is he?
[00:56:58.600 --> 00:57:01.800]   No, he's he's a I mean, it's just Google using a top level domain.
[00:57:01.800 --> 00:57:02.800]   No, it's a good.
[00:57:02.800 --> 00:57:03.920]   Yeah, it's all it is.
[00:57:03.920 --> 00:57:06.440]   But why wasn't it done 10 years ago?
[00:57:06.440 --> 00:57:07.440]   It's right.
[00:57:07.440 --> 00:57:10.680]   It's one of those things that seems so obvious in high-hand.
[00:57:10.680 --> 00:57:13.360]   Same top level domains.
[00:57:13.360 --> 00:57:16.800]   But how long we had top level domains?
[00:57:16.800 --> 00:57:19.800]   This is where we have to ask Andrew only for the last few years.
[00:57:19.800 --> 00:57:21.040]   But we've had a review.
[00:57:21.040 --> 00:57:22.440]   You're saying you ask Andrew, please.
[00:57:22.440 --> 00:57:28.600]   Yeah, if you're saying if I go to this week in Google dot new, it will create a new.
[00:57:28.600 --> 00:57:37.280]   So if you if you simply in on Chrome type doc.new, yeah, it opens up a new Google Doc for you.
[00:57:37.280 --> 00:57:39.920]   Oh, you got it?
[00:57:39.920 --> 00:57:43.240]   Yeah, doc.new.
[00:57:43.240 --> 00:57:49.240]   And then because I'm logged into Google, exactly because it knows you and you do doc.sheet
[00:57:49.240 --> 00:57:50.240]   and doc.us.
[00:57:50.240 --> 00:57:51.240]   Yes, I guess.
[00:57:51.240 --> 00:57:52.480]   There's various forms of it.
[00:57:52.480 --> 00:57:53.480]   Yes.
[00:57:53.480 --> 00:57:54.480]   It's just the.
[00:57:54.480 --> 00:57:55.480]   I gave you your questions.
[00:57:55.480 --> 00:57:56.480]   Andrew's coming.
[00:57:56.480 --> 00:57:57.480]   What?
[00:57:57.480 --> 00:57:58.480]   You don't want him.
[00:57:58.480 --> 00:57:59.480]   Tell me why.
[00:57:59.480 --> 00:58:00.480]   How does he know all about this?
[00:58:00.480 --> 00:58:01.480]   Because he knows me.
[00:58:01.480 --> 00:58:02.840]   He's a domain name person.
[00:58:02.840 --> 00:58:08.040]   Oh, and this gives me a chance to go check in on Halloween costume for a daughter.
[00:58:08.040 --> 00:58:12.800]   So this is this is doubly what is the daughter going to be this year?
[00:58:12.800 --> 00:58:17.560]   She is going as a bodyguard for one of her friends who is going as a pop musician that
[00:58:17.560 --> 00:58:18.560]   I can't recall.
[00:58:18.560 --> 00:58:22.640]   So she's got all black and a bubble gun.
[00:58:22.640 --> 00:58:25.400]   And she's going to stand most of the time.
[00:58:25.400 --> 00:58:29.840]   She and her other best friend are going to stand on each side of her other.
[00:58:29.840 --> 00:58:34.800]   I noted this is a trend this year in Halloween is these group costume.
[00:58:34.800 --> 00:58:36.840]   I mean, it's not fun.
[00:58:36.840 --> 00:58:37.840]   It's not that new.
[00:58:37.840 --> 00:58:43.080]   But yeah, we went to a haunted house that was attended mostly by high school and younger,
[00:58:43.080 --> 00:58:45.160]   but mostly high school kids.
[00:58:45.160 --> 00:58:50.040]   And the one popular one, which I was thought it was a little sad, was the Playboy punny
[00:58:50.040 --> 00:58:51.920]   and Hugh Hefner.
[00:58:51.920 --> 00:58:57.760]   So a guy would wear a smoking jacket and they all they were like a half dozen of these groups
[00:58:57.760 --> 00:59:02.560]   and a little yawning cap and then he'd be surrounded by three or four women and or girls.
[00:59:02.560 --> 00:59:08.280]   They were really girls in bunny ears and various forms of scatty costumes.
[00:59:08.280 --> 00:59:09.440]   That was a little weird.
[00:59:09.440 --> 00:59:11.200]   Yeah, that is weird.
[00:59:11.200 --> 00:59:17.040]   I went as a chicken one year in high school and my friend went, no, I was a giant chicken
[00:59:17.040 --> 00:59:19.920]   and my friend went as a farmer with a machete.
[00:59:19.920 --> 00:59:20.920]   Oh, that's cute.
[00:59:20.920 --> 00:59:21.920]   That's a little fake.
[00:59:21.920 --> 00:59:22.920]   A machete.
[00:59:22.920 --> 00:59:23.920]   That's even better.
[00:59:23.920 --> 00:59:25.480]   I'm a sherry, but yeah, chicken soup.
[00:59:25.480 --> 00:59:26.480]   Anybody.
[00:59:26.480 --> 00:59:30.080]   I was yeah, I stayed far away from him.
[00:59:30.080 --> 00:59:32.240]   It was it was fun, but they don't.
[00:59:32.240 --> 00:59:33.440]   Yeah, that's not a sexy custom.
[00:59:33.440 --> 00:59:38.000]   Are there other things I can do with this stock dot new thing?
[00:59:38.000 --> 00:59:40.160]   Here comes Andrew.
[00:59:40.160 --> 00:59:42.440]   Here comes the hubby system.
[00:59:42.440 --> 00:59:44.040]   I'll be back.
[00:59:44.040 --> 00:59:45.040]   We've never.
[00:59:45.040 --> 00:59:49.040]   So this is the Halloween exchange.
[00:59:49.040 --> 00:59:53.600]   Stacy is putting on her costume as Andrew Higginbotham.
[00:59:53.600 --> 00:59:55.520]   And boy, it's a really good costume.
[00:59:55.520 --> 00:59:57.560]   Stacy looks just like her husband.
[00:59:57.560 --> 00:59:59.200]   It's it's hey, Andrew.
[00:59:59.200 --> 01:00:00.200]   We've not met.
[01:00:00.200 --> 01:00:01.200]   Well, I've met in person.
[01:00:01.200 --> 01:00:02.200]   We've not met on the show.
[01:00:02.200 --> 01:00:03.680]   Well, you've been on the show once, I think.
[01:00:03.680 --> 01:00:04.680]   Oh, has he been on before?
[01:00:04.680 --> 01:00:05.680]   Okay.
[01:00:05.680 --> 01:00:06.680]   Well, maybe with a guest host.
[01:00:06.680 --> 01:00:07.680]   Okay.
[01:00:07.680 --> 01:00:08.680]   Yeah, exactly.
[01:00:08.680 --> 01:00:13.600]   So we were talking about this weird thing Google's doing with dot new and Stacy says you're
[01:00:13.600 --> 01:00:15.080]   a domain name expert.
[01:00:15.080 --> 01:00:16.080]   Why?
[01:00:16.080 --> 01:00:17.680]   How are you a domain name expert?
[01:00:17.680 --> 01:00:18.680]   That's what I do.
[01:00:18.680 --> 01:00:24.680]   That's I blog about domain names on domain name wire dot com and it's a little niche,
[01:00:24.680 --> 01:00:28.080]   but it's kind of exciting, I guess probably not to most people.
[01:00:28.080 --> 01:00:29.080]   No, yeah.
[01:00:29.080 --> 01:00:30.160]   That's awesome.
[01:00:30.160 --> 01:00:34.400]   You should be on the show sometime domain name wire dot com.
[01:00:34.400 --> 01:00:36.440]   Yeah.
[01:00:36.440 --> 01:00:40.080]   So what is this dot new and how does that work?
[01:00:40.080 --> 01:00:41.080]   Yeah.
[01:00:41.080 --> 01:00:43.440]   So this is actually pretty clever what Google's done.
[01:00:43.440 --> 01:00:48.760]   So back in 20, let's see, when was it 2012 you could apply for these new extensions?
[01:00:48.760 --> 01:00:53.680]   And that's when Google got a dot app, which I believe might be what I talked about on
[01:00:53.680 --> 01:00:56.680]   the show when you weren't on it one time.
[01:00:56.680 --> 01:00:59.080]   But they applied for a lot of these names.
[01:00:59.080 --> 01:01:03.280]   They got dot Google, for example, which they use for a few different things.
[01:01:03.280 --> 01:01:05.840]   And so dot new was one of the ones I got.
[01:01:05.840 --> 01:01:07.840]   I don't know why they got it.
[01:01:07.840 --> 01:01:09.640]   Oh, Stacy's going to laugh.
[01:01:09.640 --> 01:01:11.360]   My last name is not Haganbotham.
[01:01:11.360 --> 01:01:14.960]   Oh, oh, she's laughing.
[01:01:14.960 --> 01:01:16.960]   Yeah, yeah, you could hear it in a bit.
[01:01:16.960 --> 01:01:19.560]   Can we hyphenate it?
[01:01:19.560 --> 01:01:21.280]   It's it's all of them.
[01:01:21.280 --> 01:01:22.280]   All of them.
[01:01:22.280 --> 01:01:24.000]   But you're not going to spell that.
[01:01:24.000 --> 01:01:25.480]   You can just say Andrew.
[01:01:25.480 --> 01:01:28.480]   Andrew, Andrew, I think he should be Andrew Haganbotham.
[01:01:28.480 --> 01:01:29.480]   I really do.
[01:01:29.480 --> 01:01:31.480]   I think it's just a Haganbotham.
[01:01:31.480 --> 01:01:33.480]   I think that's.
[01:01:33.480 --> 01:01:38.600]   So keeping it, Andrew.
[01:01:38.600 --> 01:01:43.680]   So dot new was one of the ones that they bought.
[01:01:43.680 --> 01:01:50.360]   And so this is pretty clever, I think, creating a shortcut out of these domain names.
[01:01:50.360 --> 01:01:55.880]   So a lot of people are creating forwards from one of these new domain names to another.
[01:01:55.880 --> 01:02:00.240]   But for example, I just just this afternoon needed to create a new Google Doc.
[01:02:00.240 --> 01:02:03.920]   So I typed doc dot new into the browser.
[01:02:03.920 --> 01:02:06.120]   And it's essentially a web forward, right?
[01:02:06.120 --> 01:02:10.840]   It's forwarding it to a web address that's then opening up a new doc.
[01:02:10.840 --> 01:02:16.800]   But it's so in the DNS, they it's a script or is it simply a DNS?
[01:02:16.800 --> 01:02:20.320]   Is it like an A name entry or?
[01:02:20.320 --> 01:02:23.120]   I imagine it's just like a URL forward, right?
[01:02:23.120 --> 01:02:28.960]   So if I were to kind of a generic URL that creates a doc, but because you're logged into
[01:02:28.960 --> 01:02:32.080]   your Google account, that doc is saved under your Google account.
[01:02:32.080 --> 01:02:33.080]   Exactly.
[01:02:33.080 --> 01:02:34.080]   Exactly.
[01:02:34.080 --> 01:02:36.440]   So I think that's all right.
[01:02:36.440 --> 01:02:40.440]   That structure, they could have had a different name, wasn't that new or something else.
[01:02:40.440 --> 01:02:47.440]   But to use the URL to do a function within Google, because starting a new doc is a pain.
[01:02:47.440 --> 01:02:51.880]   You got to open, drive, you got to go to the thing, you got to say, what is new, what is
[01:02:51.880 --> 01:02:52.880]   it?
[01:02:52.880 --> 01:02:53.880]   Is it a new blank document?
[01:02:53.880 --> 01:02:55.440]   Yeah, yeah, yeah, yeah, you just give it to me.
[01:02:55.440 --> 01:02:58.960]   Now it's it's it's in I think it's incredibly clever.
[01:02:58.960 --> 01:03:00.720]   I think it's just elegantly clever.
[01:03:00.720 --> 01:03:01.720]   I agree.
[01:03:01.720 --> 01:03:08.840]   Yeah, I mean, right now I keep Wordpad and notepad, the doc on my PC, so that if I need to make
[01:03:08.840 --> 01:03:10.640]   some quick notes, I just do it in there.
[01:03:10.640 --> 01:03:16.520]   But now if I can just type new dot doc or new dot sheet and open up a spreadsheet, I think
[01:03:16.520 --> 01:03:17.920]   that is pretty clever.
[01:03:17.920 --> 01:03:22.000]   And you're right, Jeff, I mean, you they could have done that for a long time.
[01:03:22.000 --> 01:03:25.520]   They didn't need a new top level domain name to do that.
[01:03:25.520 --> 01:03:28.280]   But I think they're probably looking at it and they're like, what are we going to do
[01:03:28.280 --> 01:03:29.280]   with this name?
[01:03:29.280 --> 01:03:30.280]   Not a wee owner.
[01:03:30.280 --> 01:03:35.200]   That that new, you know, originally these companies thought that people there just be
[01:03:35.200 --> 01:03:41.000]   a lot of demand for for these new top level domain names so that people would we go register,
[01:03:41.000 --> 01:03:45.160]   you know, twit dot new or something like that or twit dot app.
[01:03:45.160 --> 01:03:50.160]   And the demand hasn't really been there, at least to the level that people expected.
[01:03:50.160 --> 01:03:54.560]   And so I think this is kind of a clever way that also brings awareness that there's more
[01:03:54.560 --> 01:03:56.960]   than dot com to the right of the right.
[01:03:56.960 --> 01:04:01.120]   And Andrew, what's fascinating to me too is that we tend to we in media think of everything
[01:04:01.120 --> 01:04:02.440]   as content.
[01:04:02.440 --> 01:04:04.640]   So a domain is going to point to content.
[01:04:04.640 --> 01:04:12.000]   The fact that a domain is used to launch a function is just a different way to think.
[01:04:12.000 --> 01:04:13.000]   It is.
[01:04:13.000 --> 01:04:14.000]   It is.
[01:04:14.000 --> 01:04:15.000]   Right.
[01:04:15.000 --> 01:04:17.800]   Have you seen anything like that from anyone else?
[01:04:17.800 --> 01:04:19.720]   I can't think of anything.
[01:04:19.720 --> 01:04:22.280]   And I think I would have heard about it.
[01:04:22.280 --> 01:04:26.360]   I mean, there are a lot of people that are taking names and just forwarding it somewhere
[01:04:26.360 --> 01:04:27.360]   else.
[01:04:27.360 --> 01:04:32.480]   But you're exactly right to have it do a function, which I guess is made possible by the fact
[01:04:32.480 --> 01:04:34.640]   that there are web apps.
[01:04:34.640 --> 01:04:36.880]   If you want to call Google Docs that.
[01:04:36.880 --> 01:04:37.880]   Yes.
[01:04:37.880 --> 01:04:43.320]   I think that's I think it's clever and I think it's novel as well.
[01:04:43.320 --> 01:04:45.720]   That only works within Google, obviously.
[01:04:45.720 --> 01:04:47.760]   Well, see, I'm going to I'm going to do this.
[01:04:47.760 --> 01:04:54.080]   I'm going to create a incognito tab so that I'm not logging in Google and enter a doc
[01:04:54.080 --> 01:04:55.080]   to do it.
[01:04:55.080 --> 01:04:56.520]   I don't think I think this will still work.
[01:04:56.520 --> 01:04:58.520]   I think the redirect.
[01:04:58.520 --> 01:04:59.520]   Yeah.
[01:04:59.520 --> 01:05:02.360]   The first thing though is going to have me do is say sign in.
[01:05:02.360 --> 01:05:03.360]   Yeah.
[01:05:03.360 --> 01:05:04.360]   Yeah.
[01:05:04.360 --> 01:05:07.600]   So the other way they could have done it is a redirect that creates a doc.
[01:05:07.600 --> 01:05:12.040]   But until you save it, it doesn't see as soon as you save it, it says, okay, but you
[01:05:12.040 --> 01:05:15.720]   got to save it to your account would be another way to work.
[01:05:15.720 --> 01:05:16.720]   It's interesting.
[01:05:16.720 --> 01:05:17.720]   Docs.
[01:05:17.720 --> 01:05:18.720]   I like that.
[01:05:18.720 --> 01:05:19.720]   That was my thing for the week.
[01:05:19.720 --> 01:05:20.720]   I thought that was that was.
[01:05:20.720 --> 01:05:21.720]   Okay.
[01:05:21.720 --> 01:05:22.720]   Andrew, thank you.
[01:05:22.720 --> 01:05:23.720]   Nice to meet.
[01:05:23.720 --> 01:05:24.720]   Nice to meet.
[01:05:24.720 --> 01:05:25.720]   I'm just waiting back there on the plot.
[01:05:25.720 --> 01:05:26.720]   I see her on the plot.
[01:05:26.720 --> 01:05:27.720]   He's reformers.
[01:05:27.720 --> 01:05:30.520]   Stacy, can you can you do a couple of little.
[01:05:30.520 --> 01:05:31.520]   Yeah.
[01:05:31.520 --> 01:05:33.720]   No, she's not going to do it for us.
[01:05:33.720 --> 01:05:39.320]   While Stacey and Andrew are rearranging, let me let me get talk about our sponsor.
[01:05:39.320 --> 01:05:40.760]   How about that?
[01:05:40.760 --> 01:05:47.120]   WordPress.com WordPress is where I make my home on the net leola port.com.
[01:05:47.120 --> 01:05:50.560]   In fact, I just got my 12 year badge from the WordPress app.
[01:05:50.560 --> 01:05:51.560]   I love the WordPress app.
[01:05:51.560 --> 01:05:57.200]   By the way, it's a great way to post Android or iOS when we were on vacation.
[01:05:57.200 --> 01:06:00.640]   It was the easiest way for me to take a picture and put it up because I didn't have Instagram,
[01:06:00.640 --> 01:06:02.080]   Facebook or Twitter.
[01:06:02.080 --> 01:06:04.360]   I put it all on the blog and I think that's just great.
[01:06:04.360 --> 01:06:07.760]   It made it just like posting on Instagram using the WordPress app.
[01:06:07.760 --> 01:06:10.400]   I can also use the app to moderate comments.
[01:06:10.400 --> 01:06:16.000]   By the way, the best comment meant moderation and anti spam stuff in the world at WordPress.com.
[01:06:16.000 --> 01:06:21.320]   To see my stats, I even get notifications when somebody new follows me, which makes
[01:06:21.320 --> 01:06:22.320]   you feel pretty good.
[01:06:22.320 --> 01:06:28.640]   WordPress.com is the place to make your website, whether it's a business or an individual.
[01:06:28.640 --> 01:06:31.960]   In fact, every individual ought to have their own website, their own place on the web that's
[01:06:31.960 --> 01:06:36.480]   truly yours where stuff you upload stays with you.
[01:06:36.480 --> 01:06:41.640]   Images, video, audio and more go right to WordPress.com and you can import and export
[01:06:41.640 --> 01:06:44.600]   content to and from your site because it's yours.
[01:06:44.600 --> 01:06:46.560]   It's your home, your content.
[01:06:46.560 --> 01:06:49.120]   It's not Mark Zuckerberg's content.
[01:06:49.120 --> 01:06:51.280]   It's not Jack's content.
[01:06:51.280 --> 01:06:53.520]   It's not Larry's content.
[01:06:53.520 --> 01:06:54.520]   It's your content.
[01:06:54.520 --> 01:06:59.840]   You could choose from hundreds of designs to match your vision, to establish your brand.
[01:06:59.840 --> 01:07:01.240]   You don't need design experience.
[01:07:01.240 --> 01:07:02.320]   You don't need web experience.
[01:07:02.320 --> 01:07:04.480]   You just need to have something to say.
[01:07:04.480 --> 01:07:08.360]   If you're a business, you've got to have a place on the web that's yours, not anybody
[01:07:08.360 --> 01:07:09.360]   else's.
[01:07:09.360 --> 01:07:10.360]   That's where you can sell stuff too.
[01:07:10.360 --> 01:07:12.280]   They've got great e-commerce solutions.
[01:07:12.280 --> 01:07:16.440]   Grow your audience, reach new customers with built-in search engine optimization.
[01:07:16.440 --> 01:07:20.240]   That's part of the reason you have to have a website because you want people when they
[01:07:20.240 --> 01:07:23.400]   search for your name or your business name to find your business.
[01:07:23.400 --> 01:07:28.120]   Not something somebody said about your business, but your business.
[01:07:28.120 --> 01:07:31.560]   That's what WordPress.com is so great at.
[01:07:31.560 --> 01:07:35.200]   24/7 help when you need it anytime.
[01:07:35.200 --> 01:07:37.560]   WordPress plans started just $4 a month.
[01:07:37.560 --> 01:07:46.000]   Some of the biggest companies in the world use WordPress, including by the way, 32% of
[01:07:46.000 --> 01:07:47.920]   all the websites in the world.
[01:07:47.920 --> 01:07:48.920]   That's going up.
[01:07:48.920 --> 01:07:51.520]   Almost a third of all the websites in the world.
[01:07:51.520 --> 01:07:54.960]   One third of all the websites in the world run on WordPress.
[01:07:54.960 --> 01:08:00.400]   Right now you'll get 15% off any new plan purchase when you go to WordPress.com/twig
[01:08:00.400 --> 01:08:02.360]   to create your new website.
[01:08:02.360 --> 01:08:04.360]   WordPress.com/twig.
[01:08:04.360 --> 01:08:08.320]   Get 15% off right now, WordPress.com/twig.
[01:08:08.320 --> 01:08:10.560]   We thank WordPress for their support.
[01:08:10.560 --> 01:08:12.560]   How's the costume going, Stacey?
[01:08:12.560 --> 01:08:13.840]   She's all body guarded up.
[01:08:13.840 --> 01:08:16.000]   Oh, you're muted.
[01:08:16.000 --> 01:08:18.000]   The key part is done.
[01:08:18.000 --> 01:08:19.000]   The key part.
[01:08:19.000 --> 01:08:20.920]   The getting on of the wig.
[01:08:20.920 --> 01:08:21.920]   Oh, it's all very.
[01:08:21.920 --> 01:08:26.160]   I was going to have a purple wig, but my daughter decided she wanted the purple wig.
[01:08:26.160 --> 01:08:27.920]   Oh, so you're going to do a costume too?
[01:08:27.920 --> 01:08:30.080]   No, no, because I'm not going trick or treating.
[01:08:30.080 --> 01:08:31.920]   You have to do candy.
[01:08:31.920 --> 01:08:35.960]   I do have an Elsa costume in the closet, but it's not 2014.
[01:08:35.960 --> 01:08:38.920]   Nobody would know who you are.
[01:08:38.920 --> 01:08:42.000]   Is it Elsa the lioness from Born Free?
[01:08:42.000 --> 01:08:43.000]   That Elsa?
[01:08:43.000 --> 01:08:44.000]   Really?
[01:08:44.000 --> 01:08:45.000]   Really?
[01:08:45.000 --> 01:08:46.000]   Really, Leo?
[01:08:46.000 --> 01:08:47.000]   Really?
[01:08:47.000 --> 01:08:49.600]   Let's just let this go, okay?
[01:08:49.600 --> 01:08:54.400]   It's time for the Google change log.
[01:08:54.400 --> 01:08:55.400]   That one.
[01:08:55.400 --> 01:08:56.400]   There we go.
[01:08:56.400 --> 01:08:59.920]   I did this this morning with Gboard, the Google keyboard.
[01:08:59.920 --> 01:09:02.680]   You can use it on iOS or Android.
[01:09:02.680 --> 01:09:04.120]   And I set it up on Android.
[01:09:04.120 --> 01:09:11.280]   I'll show you my new Google, Gomoji.
[01:09:11.280 --> 01:09:13.080]   I don't know what you call it.
[01:09:13.080 --> 01:09:14.080]   Sticker?
[01:09:14.080 --> 01:09:15.080]   That's me?
[01:09:15.080 --> 01:09:16.080]   G-mo-g.
[01:09:16.080 --> 01:09:19.200]   Oh, my G-mo-g.
[01:09:19.200 --> 01:09:25.800]   You take a selfie and the G-mo-g turns it into you.
[01:09:25.800 --> 01:09:27.640]   And then you can use it as a sticker.
[01:09:27.640 --> 01:09:29.040]   Or did I do it on the other phone?
[01:09:29.040 --> 01:09:30.520]   I did it on this phone.
[01:09:30.520 --> 01:09:32.520]   Let's do it on this phone.
[01:09:32.520 --> 01:09:34.640]   Did it, did it, did it.
[01:09:34.640 --> 01:09:37.440]   Somewhere on the rundown I have mine that I made.
[01:09:37.440 --> 01:09:38.440]   Did you?
[01:09:38.440 --> 01:09:40.720]   Yes, under other, is it?
[01:09:40.720 --> 01:09:41.720]   Okay, we'll find that.
[01:09:41.720 --> 01:09:43.520]   You guys all made G-mo-gies?
[01:09:43.520 --> 01:09:44.520]   Well it just happened.
[01:09:44.520 --> 01:09:46.520]   You have time, you won't be--
[01:09:46.520 --> 01:09:48.120]   So how do I make a G-mo-g?
[01:09:48.120 --> 01:09:50.120]   Well I was just about to show you if I can.
[01:09:50.120 --> 01:09:53.760]   But I got to first I got to find somewhere to type.
[01:09:53.760 --> 01:09:56.760]   Okay, here we go.
[01:09:56.760 --> 01:09:59.440]   And then I get the keyboard.
[01:09:59.440 --> 01:10:02.360]   And then boy, I can't remember now.
[01:10:02.360 --> 01:10:05.320]   Is it this sticker here?
[01:10:05.320 --> 01:10:07.120]   Search stickers.
[01:10:07.120 --> 01:10:08.280]   Oh, let's add a new sticker.
[01:10:08.280 --> 01:10:09.200]   Yeah, yeah.
[01:10:09.200 --> 01:10:10.040]   And here it is.
[01:10:10.040 --> 01:10:13.960]   It says, your minis, customized stickers based on you.
[01:10:13.960 --> 01:10:14.960]   Create.
[01:10:14.960 --> 01:10:18.240]   So after I allow, I have to get a picture of me.
[01:10:18.240 --> 01:10:19.240]   Okay, ready?
[01:10:19.240 --> 01:10:25.240]   Okay, now it's G-mo-gying me.
[01:10:25.240 --> 01:10:27.600]   Creating a mini.
[01:10:27.600 --> 01:10:30.960]   So you need to be using the G-board on Android or iOS.
[01:10:30.960 --> 01:10:33.920]   And that kind of looks like me, right?
[01:10:33.920 --> 01:10:34.760]   Do you like it?
[01:10:34.760 --> 01:10:36.840]   So where's my sticker option?
[01:10:36.840 --> 01:10:41.680]   And I can of course add different hair styles.
[01:10:41.680 --> 01:10:44.520]   And it knew I have gray hair and kind of grayish eyebrows.
[01:10:44.520 --> 01:10:45.760]   So that's kind of cool.
[01:10:45.760 --> 01:10:47.320]   Got my face a little dark.
[01:10:47.320 --> 01:10:48.600]   I'll make it a little lighter.
[01:10:48.600 --> 01:10:51.160]   I don't have facial hair, but I could add it.
[01:10:51.160 --> 01:10:52.640]   Facial marks, yeah.
[01:10:52.640 --> 01:10:54.040]   Old man marks.
[01:10:54.040 --> 01:10:55.560]   And then I got to have some glasses.
[01:10:55.560 --> 01:10:56.560]   How about a monocle?
[01:10:56.560 --> 01:10:58.520]   Oh, how about it?
[01:10:58.520 --> 01:11:00.560]   And now I'm going to save that out.
[01:11:00.560 --> 01:11:04.600]   Now what it's done is it's created a bunch of stickers based on that.
[01:11:04.600 --> 01:11:07.040]   I'm done.
[01:11:07.040 --> 01:11:09.040]   So there's all my, that's me.
[01:11:09.040 --> 01:11:10.040]   See?
[01:11:10.040 --> 01:11:12.040]   I'm seeing this option.
[01:11:12.040 --> 01:11:13.040]   I feel so bereft.
[01:11:13.040 --> 01:11:14.040]   Breft.
[01:11:14.040 --> 01:11:17.480]   I see what, I want this now.
[01:11:17.480 --> 01:11:19.120]   This is actually something I would use.
[01:11:19.120 --> 01:11:20.480]   I'm an active sticker user.
[01:11:20.480 --> 01:11:25.040]   What's nice is they're not, they don't have the uncanny valley of the.
[01:11:25.040 --> 01:11:29.120]   Oh, someone pointed out that if I'm in Texas because of the biometric stuff.
[01:11:29.120 --> 01:11:30.680]   Oh, you're kidding me.
[01:11:30.680 --> 01:11:33.320]   I'm going to Texas and North California.
[01:11:33.320 --> 01:11:34.320]   For all paddocks.
[01:11:34.320 --> 01:11:35.760]   See what goes to you?
[01:11:35.760 --> 01:11:37.320]   You can't do a jimoji.
[01:11:37.320 --> 01:11:41.360]   Okay, yes, but the reason why I can't do a jimoji is because they don't want my face
[01:11:41.360 --> 01:11:43.360]   being used to unlock weird things.
[01:11:43.360 --> 01:11:47.080]   So can you do me emoji on your iPhone?
[01:11:47.080 --> 01:11:50.440]   I don't have an iPhone because I'm not a fan of Apple.
[01:11:50.440 --> 01:11:52.880]   We can talk later about that.
[01:11:52.880 --> 01:11:55.360]   That seems, that seems rude.
[01:11:55.360 --> 01:11:57.120]   Doesn't it though?
[01:11:57.120 --> 01:11:59.640]   Maybe I can do it on my computer.
[01:11:59.640 --> 01:12:00.640]   Maybe you can.
[01:12:00.640 --> 01:12:03.240]   You need a VPN to pretend you're somewhere else.
[01:12:03.240 --> 01:12:07.600]   Oh, I have one, but if I turn on the VPN, our connection goes to heck.
[01:12:07.600 --> 01:12:11.200]   And then apparently how well did Jeff do is?
[01:12:11.200 --> 01:12:12.200]   Well, I guess not.
[01:12:12.200 --> 01:12:13.400]   It's not what you just did.
[01:12:13.400 --> 01:12:14.480]   It's something different though.
[01:12:14.480 --> 01:12:15.480]   What are you doing?
[01:12:15.480 --> 01:12:16.720]   You've got a cat.
[01:12:16.720 --> 01:12:20.120]   Look, you look like Mark Zuckerberg in that picture.
[01:12:20.120 --> 01:12:24.040]   I saw no way to put my wrinkles on.
[01:12:24.040 --> 01:12:26.040]   What is going on?
[01:12:26.040 --> 01:12:29.800]   Jeff, I got bad news for you.
[01:12:29.800 --> 01:12:31.040]   Okay.
[01:12:31.040 --> 01:12:33.360]   That's Bitmoji.
[01:12:33.360 --> 01:12:38.760]   That's, yeah, this is not, I guess Bitmoji is another company.
[01:12:38.760 --> 01:12:39.760]   And you can do that.
[01:12:39.760 --> 01:12:40.760]   So Bitmoji, I can do that.
[01:12:40.760 --> 01:12:41.760]   That's owned by Snapchat.
[01:12:41.760 --> 01:12:42.760]   That's just Bitmoji.
[01:12:42.760 --> 01:12:44.520]   Yeah, that's owned by Snapchat.
[01:12:44.520 --> 01:12:46.240]   And you can do Bitmoji in your Snapchat app.
[01:12:46.240 --> 01:12:49.040]   And you can get to it, you can get to it through the Google keyboard now.
[01:12:49.040 --> 01:12:50.040]   Ah, nice.
[01:12:50.040 --> 01:12:51.040]   Oh, that is nice.
[01:12:51.040 --> 01:12:52.040]   That's the issue.
[01:12:52.040 --> 01:12:55.040]   Oh, you guys, this is just like.
[01:12:55.040 --> 01:12:59.640]   Emoji minis are emoji sized stickers that look to you like you.
[01:12:59.640 --> 01:13:00.640]   Okay.
[01:13:00.640 --> 01:13:05.080]   Yeah, that's what I did is an emoji mini.
[01:13:05.080 --> 01:13:07.960]   It's gotten the nomenclatures out of control.
[01:13:07.960 --> 01:13:12.680]   No, I don't even know where to check on which keyboard I'm using.
[01:13:12.680 --> 01:13:14.400]   You should have, show me your keyboard.
[01:13:14.400 --> 01:13:15.800]   I'll tell you if it's Gboard.
[01:13:15.800 --> 01:13:18.600]   Okay, we're going to do a keyboard diagnostic.
[01:13:18.600 --> 01:13:22.400]   The first time ever on national podcasts.
[01:13:22.400 --> 01:13:26.800]   Of course, it's too much contrast.
[01:13:26.800 --> 01:13:27.800]   You're not going to see it.
[01:13:27.800 --> 01:13:28.800]   It's too bright.
[01:13:28.800 --> 01:13:29.800]   Yeah.
[01:13:29.800 --> 01:13:31.800]   So you go to your app.
[01:13:31.800 --> 01:13:32.800]   Oops.
[01:13:32.800 --> 01:13:33.800]   Sorry, go Leo.
[01:13:33.800 --> 01:13:35.480]   Well, you just go to my settings.
[01:13:35.480 --> 01:13:36.480]   Go into settings.
[01:13:36.480 --> 01:13:39.200]   And then search for keyboard.
[01:13:39.200 --> 01:13:40.200]   Oh, I guess search.
[01:13:40.200 --> 01:13:41.200]   Okay.
[01:13:41.200 --> 01:13:43.280]   Yes, search is your best friend in settings.
[01:13:43.280 --> 01:13:46.080]   And then it's under languages and inputs.
[01:13:46.080 --> 01:13:49.960]   And the virtual keyboard, you should have several choices.
[01:13:49.960 --> 01:13:55.720]   But on an Android, I mean, I pick Google device, Gboard should be the first one.
[01:13:55.720 --> 01:13:58.320]   But you can add keyboards if you want.
[01:13:58.320 --> 01:13:59.320]   Right.
[01:13:59.320 --> 01:14:00.320]   I think I do have it then.
[01:14:00.320 --> 01:14:01.320]   Yeah.
[01:14:01.320 --> 01:14:02.320]   That's the default on pixels.
[01:14:02.320 --> 01:14:03.320]   Yeah.
[01:14:03.320 --> 01:14:04.320]   Yeah.
[01:14:04.320 --> 01:14:05.320]   Advanced.
[01:14:05.320 --> 01:14:06.320]   Can I, let's see.
[01:14:06.320 --> 01:14:07.720]   Can I for, oh, go back to me.
[01:14:07.720 --> 01:14:08.720]   No, no.
[01:14:08.720 --> 01:14:11.040]   I'm going to see if I can force the update.
[01:14:11.040 --> 01:14:13.120]   Maybe that's all you need as an update.
[01:14:13.120 --> 01:14:19.960]   Meanwhile, the change log continues with a improvement in capture.
[01:14:19.960 --> 01:14:22.560]   Let me see if I can pull this up.
[01:14:22.560 --> 01:14:27.000]   This comes from the Google Webmaster blog captures of those really annoying things.
[01:14:27.000 --> 01:14:29.000]   Oh, find the buses and the pictures.
[01:14:29.000 --> 01:14:30.640]   Yeah, I really hate this one.
[01:14:30.640 --> 01:14:33.320]   So this is Google's version of it called recapture.
[01:14:33.320 --> 01:14:38.200]   They bought the recapture tool Carnegie Mellon first created.
[01:14:38.200 --> 01:14:48.840]   And initially recapture was used to, in effect, get people to do text recognition on books
[01:14:48.840 --> 01:14:51.880]   that were poorly scanned or couldn't quite be recognized.
[01:14:51.880 --> 01:14:52.880]   So you'd get two words.
[01:14:52.880 --> 01:14:54.360]   Remember this in the early days?
[01:14:54.360 --> 01:14:55.360]   You'd get two words.
[01:14:55.360 --> 01:14:58.960]   One that was identifying you as not a robot, but the other was helping them with books.
[01:14:58.960 --> 01:15:01.200]   And then it turned into street signs.
[01:15:01.200 --> 01:15:03.640]   Remember that it was street numbers.
[01:15:03.640 --> 01:15:04.640]   Same thing.
[01:15:04.640 --> 01:15:07.080]   You were helping Street View figure out where it was on street numbers.
[01:15:07.080 --> 01:15:09.760]   They couldn't easily read.
[01:15:09.760 --> 01:15:13.560]   So there was recapture one and recapture two.
[01:15:13.560 --> 01:15:14.560]   Recapture three.
[01:15:14.560 --> 01:15:19.360]   We are fundamentally changing how sites can test for human versus bot activities by returning
[01:15:19.360 --> 01:15:23.880]   a score to tell you how suspicious an interaction is.
[01:15:23.880 --> 01:15:26.920]   And then eliminating the need to interrupt users with challenges at all.
[01:15:26.920 --> 01:15:29.960]   This would be nice.
[01:15:29.960 --> 01:15:35.200]   Recapture version three runs adaptive risk analysis in the background to alert you of
[01:15:35.200 --> 01:15:41.240]   suspicious traffic while letting human users enjoy a frictionless experience in your site.
[01:15:41.240 --> 01:15:45.000]   We're introducing a new concept called action, a tag you can use to define the key steps of
[01:15:45.000 --> 01:15:51.760]   your user journey and enable recapture to run its risk analysis on those steps.
[01:15:51.760 --> 01:15:57.800]   So users won't get a pop up, but if they're doing things like clicking too fast or acting
[01:15:57.800 --> 01:16:02.520]   in an unhuman like way, it might be more suspicious than you might.
[01:16:02.520 --> 01:16:12.000]   You'll get a score and you might as the site decide to pop up something more difficult.
[01:16:12.000 --> 01:16:18.400]   So I'm completely not clear exactly what it's doing, but we've already seen with Google,
[01:16:18.400 --> 01:16:23.680]   you'll get a check box that says, just merely, I'm not a robot and it doesn't give you anything
[01:16:23.680 --> 01:16:25.120]   else.
[01:16:25.120 --> 01:16:26.520]   Remember that?
[01:16:26.520 --> 01:16:27.520]   Right.
[01:16:27.520 --> 01:16:31.360]   Lately, they're doing those terrible ones where you get eight, nine images and you have
[01:16:31.360 --> 01:16:33.120]   to pick the street signs out.
[01:16:33.120 --> 01:16:34.120]   Awful.
[01:16:34.120 --> 01:16:35.880]   Horrible.
[01:16:35.880 --> 01:16:43.000]   This one is apparently better because you won't get interfered with as long as you're
[01:16:43.000 --> 01:16:44.720]   acting like a user.
[01:16:44.720 --> 01:16:53.920]   Remember, Google knows so much about you.
[01:16:53.920 --> 01:16:58.720]   They know whether really that the owner of the phone is a human or not, you know, that
[01:16:58.720 --> 01:16:59.720]   the interaction.
[01:16:59.720 --> 01:17:03.720]   They know if you are human or if you're just a bot.
[01:17:03.720 --> 01:17:07.960]   They know if you're going to buy something and maybe you are.
[01:17:07.960 --> 01:17:08.960]   Maybe if you're hot.
[01:17:08.960 --> 01:17:09.960]   That's appropriate for Google, I guess.
[01:17:09.960 --> 01:17:10.960]   Yeah.
[01:17:10.960 --> 01:17:11.960]   That's what I'm saying.
[01:17:11.960 --> 01:17:12.960]   Yeah.
[01:17:12.960 --> 01:17:13.960]   Hot or not.
[01:17:13.960 --> 01:17:21.400]   The report is hearing right now, at word today, an internet privacy case involving Google.
[01:17:21.400 --> 01:17:25.880]   It's an $8.5 million settlement that's been challenged by an official at a Washington-based
[01:17:25.880 --> 01:17:29.680]   conservative think tank.
[01:17:29.680 --> 01:17:34.720]   And apparently, the, well, I'll give you the case first.
[01:17:34.720 --> 01:17:41.680]   A California resident, Paloma Gaos, or Gaos filed a proposed class action lawsuit in 2010
[01:17:41.680 --> 01:17:46.840]   claiming Google's search protocols violated federal privacy law by disclosing users' search
[01:17:46.840 --> 01:17:48.920]   terms to other websites.
[01:17:48.920 --> 01:17:54.400]   As you know, when you do a search, the referral from Google will include how you got to that
[01:17:54.400 --> 01:17:55.400]   site.
[01:17:55.400 --> 01:17:56.400]   And I've used that.
[01:17:56.400 --> 01:17:59.640]   Which was incredibly useful for the other sites and other sites.
[01:17:59.640 --> 01:18:00.640]   And then we can't.
[01:18:00.640 --> 01:18:01.640]   Absolutely.
[01:18:01.640 --> 01:18:06.280]   Google, a lower court upheld the settlement.
[01:18:06.280 --> 01:18:11.800]   The company agreed to pay in 2013 to resolve the claims.
[01:18:11.800 --> 01:18:16.640]   The settlement's known as C.Pray Awards, or Unfair, says critics, and encourage frivolous
[01:18:16.640 --> 01:18:20.920]   lawsuits, conflicts of interest, and collusion between both sides to minimize damages for
[01:18:20.920 --> 01:18:25.480]   defendants while maximizing fees for plaintiffs' lawyers.
[01:18:25.480 --> 01:18:29.080]   Google did agree in the settlement to disclose on its website how users' search terms are
[01:18:29.080 --> 01:18:31.760]   shared, but did not change its behavior.
[01:18:31.760 --> 01:18:35.200]   The three main plaintiffs got $5,000 each.
[01:18:35.200 --> 01:18:42.800]   The attorneys, $2.1 million under the settlement, the rest of the $8 million would go to organizations
[01:18:42.800 --> 01:18:49.960]   or products, projects that promote internet privacy, Stanford, AARP, but nothing to the
[01:18:49.960 --> 01:18:54.960]   other millions of Google users who the plaintiffs were to have represented in the class action.
[01:18:54.960 --> 01:18:58.640]   C.Pray Awards apparently are rare.
[01:18:58.640 --> 01:18:59.640]   They give money.
[01:18:59.640 --> 01:19:00.640]   And you know what?
[01:19:00.640 --> 01:19:03.640]   I think this is probably a good thing because how many times have you gotten a postcard?
[01:19:03.640 --> 01:19:09.280]   I got a postcard from a Popschips class action lawsuit that entitled me to a free bag of
[01:19:09.280 --> 01:19:10.280]   Popschips.
[01:19:10.280 --> 01:19:14.760]   Sometimes it's like for a buck 50, right?
[01:19:14.760 --> 01:19:17.440]   Because they spread the class out over so many people.
[01:19:17.440 --> 01:19:22.560]   So C.Pray Awards give money that can't feasibly be distributed to participants to unrelated
[01:19:22.560 --> 01:19:26.400]   entities as long as it's in the plaintiff's interests.
[01:19:26.400 --> 01:19:33.200]   So the judges are hearing arguments about whether this C.Pray Award is legit.
[01:19:33.200 --> 01:19:40.120]   And apparently some of the liberal justices, remember the court is now split five to four
[01:19:40.120 --> 01:19:44.520]   conservative liberal, emphasize such settlements can funnel money to good uses.
[01:19:44.520 --> 01:19:48.960]   In other words, did it make sense?
[01:19:48.960 --> 01:19:53.320]   Justice Alito, one of the conservatives raised concern the money would go to groups.
[01:19:53.320 --> 01:19:56.400]   Some plaintiffs might not like.
[01:19:56.400 --> 01:19:58.480]   So how could that be a sensible system?
[01:19:58.480 --> 01:20:03.280]   Justice Roberts, another conservative said that AARP engages a political activity.
[01:20:03.280 --> 01:20:10.640]   So that was actually when I saw the double ARP on there, I was like, they don't fight
[01:20:10.640 --> 01:20:11.640]   for privacy.
[01:20:11.640 --> 01:20:15.480]   They fight for like, I actually thought that was kind of a questionable use of sending
[01:20:15.480 --> 01:20:16.480]   this.
[01:20:16.480 --> 01:20:22.500]   So Robert's also said it was fishy that settlement money could be directed to institutions to
[01:20:22.500 --> 01:20:27.560]   which Google already was a donor, including the alma mater of the lawyer involved in
[01:20:27.560 --> 01:20:28.560]   the case.
[01:20:28.560 --> 01:20:32.640]   Brett Kavanaugh noting that one.
[01:20:32.640 --> 01:20:40.320]   Ruth Bader Ginsburg said at least plaintiffs get an indirect benefit from the settlement.
[01:20:40.320 --> 01:20:42.880]   Seems like the system is working, said Joe and Justice Sotomayor.
[01:20:42.880 --> 01:20:46.880]   So you can see how the fault lines of the court are falling out here.
[01:20:46.880 --> 01:20:52.760]   Although it's really hard in arguments to know how the court's going to go or even what
[01:20:52.760 --> 01:20:54.640]   the justices think.
[01:20:54.640 --> 01:20:56.160]   Often I've listened to enough.
[01:20:56.160 --> 01:21:03.360]   By the way, these are all recorded and available online going way, way back.
[01:21:03.360 --> 01:21:04.560]   And Supreme Court decisions.
[01:21:04.560 --> 01:21:07.480]   Yeah, not decisions hearings arguments.
[01:21:07.480 --> 01:21:10.120]   Oye, Oye, Oye, Oye, Oye, I think is the site.
[01:21:10.120 --> 01:21:12.200]   Let me see if I can find it.
[01:21:12.200 --> 01:21:13.200]   Oye.org.
[01:21:13.200 --> 01:21:21.440]   Oye, Z.org, a multimedia archive of Supreme Court cases.
[01:21:21.440 --> 01:21:26.240]   You can listen to Brown versus Board of Education if you want.
[01:21:26.240 --> 01:21:27.760]   Apple versus Pepper.
[01:21:27.760 --> 01:21:29.200]   Yeah, isn't that cool?
[01:21:29.200 --> 01:21:30.200]   Let's go back in time.
[01:21:30.200 --> 01:21:31.840]   Let's see how far back this goes.
[01:21:31.840 --> 01:21:32.840]   Oh, yeah.
[01:21:32.840 --> 01:21:33.840]   1850.
[01:21:33.840 --> 01:21:34.840]   What?
[01:21:34.840 --> 01:21:37.840]   Did they have recorded mediums or anything?
[01:21:37.840 --> 01:21:41.960]   No, no, no, no, no, no.
[01:21:41.960 --> 01:21:46.440]   Maybe some of these don't have media, but you can see a lot.
[01:21:46.440 --> 01:21:48.760]   This is a, I love Oye, by the way, great site.
[01:21:48.760 --> 01:21:51.560]   So yeah, not all of them have media, but that many of them do.
[01:21:51.560 --> 01:21:54.920]   They do record these arguments.
[01:21:54.920 --> 01:21:58.040]   So you can, you can actually listen to them.
[01:21:58.040 --> 01:21:59.600]   I really like Oye.
[01:21:59.600 --> 01:22:00.600]   Oye, Z.org.
[01:22:00.600 --> 01:22:10.840]   That's what they say at the beginning when they, when you let all who have a business
[01:22:10.840 --> 01:22:13.400]   with the Supreme Court of the United States of America.
[01:22:13.400 --> 01:22:15.600]   Oye, Oye, what does that mean?
[01:22:15.600 --> 01:22:18.640]   It means come and see and be seen.
[01:22:18.640 --> 01:22:19.640]   Something like that.
[01:22:19.640 --> 01:22:20.640]   Really?
[01:22:20.640 --> 01:22:26.520]   Oye is like ear and like Latin languages, but I don't know.
[01:22:26.520 --> 01:22:29.600]   A traditional interjection said two or three times a succession.
[01:22:29.600 --> 01:22:33.160]   And we know that in the court of law, it means hear ye.
[01:22:33.160 --> 01:22:34.400]   Oh, that's your right.
[01:22:34.400 --> 01:22:35.400]   So it's here.
[01:22:35.400 --> 01:22:37.240]   Hear ye, hear ye, hear ye.
[01:22:37.240 --> 01:22:44.080]   It's Anglo-Norman, the plural imperative form of Oye from the French ear to here.
[01:22:44.080 --> 01:22:47.640]   Oh, because French used to be the language of, of government.
[01:22:47.640 --> 01:22:48.640]   Yeah.
[01:22:48.640 --> 01:22:56.080]   And the Guillaume, like, okay, wandering over the channel and took care of those savages.
[01:22:56.080 --> 01:22:57.960]   1066.
[01:22:57.960 --> 01:23:00.440]   You want to hear it?
[01:23:00.440 --> 01:23:02.240]   You want to hear what it sounds like?
[01:23:02.240 --> 01:23:07.240]   I am the honorable, the chief justice and the associate justices of the Supreme Court
[01:23:07.240 --> 01:23:09.240]   of the United States.
[01:23:09.240 --> 01:23:16.760]   Oye, Oye, Oye, Oye, all persons having business before the honorable, the Supreme Court of
[01:23:16.760 --> 01:23:20.960]   the United States are admonished to draw near and give their attention for the court
[01:23:20.960 --> 01:23:21.960]   is now sitting.
[01:23:21.960 --> 01:23:25.560]   God save the United States in this honorable court.
[01:23:25.560 --> 01:23:26.960]   Excuse me, chills.
[01:23:26.960 --> 01:23:27.960]   I can't tell.
[01:23:27.960 --> 01:23:29.360]   Does that not give you chills?
[01:23:29.360 --> 01:23:31.360]   I get chills right here.
[01:23:31.360 --> 01:23:32.360]   It's also, oh, yay.
[01:23:32.360 --> 01:23:34.760]   Oh, yay, oh, yay, oh, yay, oh, yay.
[01:23:34.760 --> 01:23:36.560]   I love it.
[01:23:36.560 --> 01:23:39.040]   So yeah, that's a great site, by the way.
[01:23:39.040 --> 01:23:44.680]   And so I guess you probably could at some point out the arguments were heard today, so
[01:23:44.680 --> 01:23:46.720]   they probably aren't up yet.
[01:23:46.720 --> 01:23:58.200]   And that, with a little sidebar, is your Google change log.
[01:23:58.200 --> 01:24:01.240]   Here's one that'll make Jeff see red.
[01:24:01.240 --> 01:24:08.440]   You don't like it when I start a story like that, do you, Jeff?
[01:24:08.440 --> 01:24:13.800]   A, a blockchain site, a crypto site called Breaker Magazine.
[01:24:13.800 --> 01:24:16.880]   I guess that they were getting, and I've been getting, I get these every day, and I'm
[01:24:16.880 --> 01:24:18.320]   sure you do too, Stacey.
[01:24:18.320 --> 01:24:21.800]   Lots of solicitations to put stuff on your site.
[01:24:21.800 --> 01:24:22.800]   Even I do.
[01:24:22.800 --> 01:24:23.800]   Yeah.
[01:24:23.800 --> 01:24:27.000]   We got no audience left on the blog.
[01:24:27.000 --> 01:24:28.000]   Yeah.
[01:24:28.000 --> 01:24:29.840]   You know, here's an example.
[01:24:29.840 --> 01:24:31.400]   Hello.
[01:24:31.400 --> 01:24:33.760]   I'm representing a blockchain PR company from Moscow.
[01:24:33.760 --> 01:24:37.000]   I would like information on the rate for advertising on your website.
[01:24:37.000 --> 01:24:38.000]   Thanks, Nikolai.
[01:24:38.000 --> 01:24:43.360]   Hey, except that this was not, this was written by a phony person, Nikolai Kosterev, a journalist
[01:24:43.360 --> 01:24:53.360]   created by BreakerMag to see how many Bitcoin news sites would accept paid content.
[01:24:53.360 --> 01:24:57.360]   And the Bitcoin content world is nefarious.
[01:24:57.360 --> 01:25:01.360]   The, if the outlet would try, it's, yeah, well, it's a test.
[01:25:01.360 --> 01:25:05.360]   They're not, it's not in traffic if it's not by the cops.
[01:25:05.360 --> 01:25:08.640]   The outlet replied to that email, then they would send another one saying, Hey, thanks
[01:25:08.640 --> 01:25:14.240]   to their client information, many of my ICO, initial coin offering clients want coverage
[01:25:14.240 --> 01:25:19.640]   written about them, but some would like it not to be marked sponsored.
[01:25:19.640 --> 01:25:21.480]   Is that possible?
[01:25:21.480 --> 01:25:27.000]   Of the 22 outlets who replied 12 of them, more than half were willing to publish paid content
[01:25:27.000 --> 01:25:30.320]   without disclosing it as such.
[01:25:30.320 --> 01:25:37.840]   The prices quoted range from a low of $240 for, that was crypto ninjas to a high of $4,500
[01:25:37.840 --> 01:25:41.240]   for news BTC.
[01:25:41.240 --> 01:25:44.240]   Now I have to admit, this is from a competing site that created this BreakerMag.
[01:25:44.240 --> 01:25:47.000]   So, yeah, but it's not a surprise.
[01:25:47.000 --> 01:25:51.960]   And it is the case that we all get these solicitations all the time.
[01:25:51.960 --> 01:25:56.080]   I thought you were going to bait me with the story about the journalist telling Twitter
[01:25:56.080 --> 01:25:57.400]   to get rid of the RT.
[01:25:57.400 --> 01:25:59.840]   Well, you, we could do that one too.
[01:25:59.840 --> 01:26:04.240]   Twitter should kill the retreat rights Taylor Lawrence in Atlantic.
[01:26:04.240 --> 01:26:08.840]   The future derails, I'm sorry, the feature derails healthy conversation and praise on
[01:26:08.840 --> 01:26:10.280]   users worst instincts.
[01:26:10.280 --> 01:26:12.040]   I don't disagree with this.
[01:26:12.040 --> 01:26:16.720]   In fact, I told you some time ago when I still was using Twitter that my trick to make Twitter
[01:26:16.720 --> 01:26:20.000]   more useful was to turn off retweeted content.
[01:26:20.000 --> 01:26:25.360]   And you could do that in what you just follow jerks is the problem you had.
[01:26:25.360 --> 01:26:29.600]   I follow smart people and I find the RT is extremely valuable.
[01:26:29.600 --> 01:26:30.600]   Okay.
[01:26:30.600 --> 01:26:36.800]   They tend though to gin up more controversy, right?
[01:26:36.800 --> 01:26:38.800]   Because thousands of retweets and.
[01:26:38.800 --> 01:26:39.800]   All right.
[01:26:39.800 --> 01:26:40.800]   Yes, there are certain retweets.
[01:26:40.800 --> 01:26:43.320]   I mean, like, I retweet stuff often.
[01:26:43.320 --> 01:26:44.920]   I'll see a cool study.
[01:26:44.920 --> 01:26:48.680]   I see someone making a really good point.
[01:26:48.680 --> 01:26:51.240]   And I'm like, oh, hey, look at this.
[01:26:51.240 --> 01:26:54.760]   And I mean, people seem to like it, but I can see.
[01:26:54.760 --> 01:26:57.720]   I think there are people who retweet crap.
[01:26:57.720 --> 01:26:58.720]   So.
[01:26:58.720 --> 01:26:59.720]   Yeah.
[01:26:59.720 --> 01:27:04.240]   I figure I'll see the original without seeing the retweet.
[01:27:04.240 --> 01:27:10.160]   I don't like I like it because I follow a really diverse group of people and they retweet
[01:27:10.160 --> 01:27:11.680]   stuff from their social circles.
[01:27:11.680 --> 01:27:14.600]   So like, it makes sense.
[01:27:14.600 --> 01:27:19.840]   I recuse myself on this topic, not having any knowledge thereof.
[01:27:19.840 --> 01:27:25.360]   I think if you follow really awesomely smart people who are really passionate about particular
[01:27:25.360 --> 01:27:28.800]   subjects, you'll get some really cool stuff.
[01:27:28.800 --> 01:27:35.120]   So I deactivated Leo Loport, but I did have a links for Twitch account that I used to
[01:27:35.120 --> 01:27:40.840]   use to put all the links that as you see on this page that we were going to talk about.
[01:27:40.840 --> 01:27:43.360]   And I took that and that wasn't following anything.
[01:27:43.360 --> 01:27:50.200]   And I only follow news accounts like journalists like you guys, but also mostly like news.
[01:27:50.200 --> 01:27:51.520]   And that's actually been fairly useful.
[01:27:51.520 --> 01:27:55.800]   And I preserved that account just in case somebody I hear that somebody dies and I want
[01:27:55.800 --> 01:27:57.720]   to find out if they really did.
[01:27:57.720 --> 01:27:58.720]   You want to up your game?
[01:27:58.720 --> 01:27:59.720]   Yeah.
[01:27:59.720 --> 01:28:00.720]   Follow.
[01:28:00.720 --> 01:28:01.720]   And I don't know if there's a list of these.
[01:28:01.720 --> 01:28:02.960]   Someone should totally create it.
[01:28:02.960 --> 01:28:06.880]   Follow the PR accounts of universities research offices.
[01:28:06.880 --> 01:28:08.400]   Oh, that's interesting.
[01:28:08.400 --> 01:28:09.400]   Oh, yeah.
[01:28:09.400 --> 01:28:10.920]   You get all kinds of cool stuff.
[01:28:10.920 --> 01:28:11.920]   Yeah.
[01:28:11.920 --> 01:28:14.680]   I mean, maybe you don't love that, but it's it's act.
[01:28:14.680 --> 01:28:15.680]   I love it.
[01:28:15.680 --> 01:28:16.680]   Yeah.
[01:28:16.680 --> 01:28:17.680]   Just like, oh, look what people are doing.
[01:28:17.680 --> 01:28:19.000]   They're making crazy stuff.
[01:28:19.000 --> 01:28:20.000]   Yay.
[01:28:20.000 --> 01:28:21.000]   Science.
[01:28:21.000 --> 01:28:22.000]   Yeah, that's fun.
[01:28:22.000 --> 01:28:23.000]   Lists are very useful.
[01:28:23.000 --> 01:28:27.480]   If you can find good lists that helps a lot.
[01:28:27.480 --> 01:28:34.000]   It does get rid of if you follow like legitimate news sources, it gets rid of a lot of the spam
[01:28:34.000 --> 01:28:36.840]   and bot activity, right?
[01:28:36.840 --> 01:28:38.800]   I'm sorry.
[01:28:38.800 --> 01:28:39.800]   I'm sorry.
[01:28:39.800 --> 01:28:40.800]   I yawned.
[01:28:40.800 --> 01:28:41.800]   I did.
[01:28:41.800 --> 01:28:42.800]   Oh, that was just a yawn.
[01:28:42.800 --> 01:28:43.800]   Oh, that wasn't a no.
[01:28:43.800 --> 01:28:44.800]   I was so ignorant.
[01:28:44.800 --> 01:28:47.800]   I don't know how you live.
[01:28:47.800 --> 01:28:52.960]   No, I took I took some suit of it and it was a bad mistake.
[01:28:52.960 --> 01:28:57.120]   Street street car street view car was swept away by a flood.
[01:28:57.120 --> 01:28:59.000]   I want to see those images.
[01:28:59.000 --> 01:29:00.000]   Where?
[01:29:00.000 --> 01:29:04.080]   I think the UK, this is from the independent.
[01:29:04.080 --> 01:29:07.520]   The video takes forever to get going.
[01:29:07.520 --> 01:29:10.360]   A car used to map roads for Google Street view.
[01:29:10.360 --> 01:29:11.360]   Oh, West Texas.
[01:29:11.360 --> 01:29:12.360]   Oh, OK.
[01:29:12.360 --> 01:29:15.160]   Yeah, we have the problem is the guy.
[01:29:15.160 --> 01:29:16.160]   The guy.
[01:29:16.160 --> 01:29:18.000]   There it goes.
[01:29:18.000 --> 01:29:22.120]   Decided to go around the barrier because I guess you know, I'm busy here.
[01:29:22.120 --> 01:29:23.120]   I'm Google.
[01:29:23.120 --> 01:29:24.120]   Sorry.
[01:29:24.120 --> 01:29:25.120]   I'm from barriers.
[01:29:25.120 --> 01:29:26.120]   Waters will part for me.
[01:29:26.120 --> 01:29:27.120]   What was this?
[01:29:27.120 --> 01:29:28.680]   The bottom age.
[01:29:28.680 --> 01:29:33.080]   So this is OK, guys, we may mock this, but as someone who's been in defensive driving
[01:29:33.080 --> 01:29:37.360]   several times in Texas, well, we just had a school bus actually try to go around the
[01:29:37.360 --> 01:29:38.360]   barriers as well.
[01:29:38.360 --> 01:29:41.280]   Now, are you still boiling your water in Austin?
[01:29:41.280 --> 01:29:45.040]   No, I was so mad that the times did not cover that story.
[01:29:45.040 --> 01:29:49.240]   I was like, we are a city of like a million people having to boil our water for a week.
[01:29:49.240 --> 01:29:50.840]   And that was due to floods.
[01:29:50.840 --> 01:29:51.840]   That was due to floods.
[01:29:51.840 --> 01:29:52.840]   Yeah.
[01:29:52.840 --> 01:29:55.440]   Unintended, unthought of consequence of climate change for my opinion.
[01:29:55.440 --> 01:29:58.800]   I probably shouldn't bring this up at the end of the show, which because we are coming
[01:29:58.800 --> 01:30:00.960]   to the end of the show.
[01:30:00.960 --> 01:30:03.640]   But I'm just that kind of guy.
[01:30:03.640 --> 01:30:09.960]   New York Times, a dark consensus about screens and kids.
[01:30:09.960 --> 01:30:15.440]   Oi, begins to emerge in Silicon Valley.
[01:30:15.440 --> 01:30:16.800]   Here's the pull quote.
[01:30:16.800 --> 01:30:20.960]   I'm convinced the devil lives in our phones.
[01:30:20.960 --> 01:30:24.800]   Well, talking about moral panic.
[01:30:24.800 --> 01:30:25.800]   Exhibitor.
[01:30:25.800 --> 01:30:28.800]   The devil lives in our phones.
[01:30:28.800 --> 01:30:36.200]   But it does talk about a lot of people in Silicon Valley.
[01:30:36.200 --> 01:30:40.600]   Well that was Athena Chavarria, who works as an executive assistant at Facebook and is
[01:30:40.600 --> 01:30:45.760]   now at Mark Zuckerberg's philanthropic arm, the Chan Zuckerberg initiative, who said,
[01:30:45.760 --> 01:30:46.760]   I am convinced.
[01:30:46.760 --> 01:30:48.960]   I'm sure she doesn't talk like this.
[01:30:48.960 --> 01:30:54.920]   The devil lives in our phones and is wreaking havoc on our children.
[01:30:54.920 --> 01:30:58.200]   She does not let her children have cell phones at Elle High School.
[01:30:58.200 --> 01:31:02.920]   Even now, bands phone use in the car and severely limits it at home.
[01:31:02.920 --> 01:31:07.600]   And so this article talks to Chris Anderson, who is of course, former editor at Wired,
[01:31:07.600 --> 01:31:13.360]   geekdad.com is now as well known as being anti tech on the scale between candy and crack
[01:31:13.360 --> 01:31:14.360]   cocaine.
[01:31:14.360 --> 01:31:16.400]   It's closer to crack cocaine.
[01:31:16.400 --> 01:31:18.360]   Mr. Anderson said of screens.
[01:31:18.360 --> 01:31:21.840]   We thought we could control it and this is beyond our power control.
[01:31:21.840 --> 01:31:24.760]   It's gone straight to the pleasure centers of the developing brain.
[01:31:24.760 --> 01:31:30.560]   This is beyond our capacity as regular parents to understand.
[01:31:30.560 --> 01:31:34.880]   I didn't know what we were doing to their brains until I started to observe the symptoms
[01:31:34.880 --> 01:31:36.160]   and consequences.
[01:31:36.160 --> 01:31:39.320]   He said this is a scar tissue talking.
[01:31:39.320 --> 01:31:43.840]   We've made every mistake in the book and I think we got it wrong with some of my kids.
[01:31:43.840 --> 01:31:44.840]   Mr. Anderson.
[01:31:44.840 --> 01:31:49.160]   My favorite part is where he's like some of our kids are totally screwed up.
[01:31:49.160 --> 01:31:51.280]   We have this is a great quote.
[01:31:51.280 --> 01:31:55.240]   We have glimpsed into the chasm of addiction.
[01:31:55.240 --> 01:32:01.560]   And there were some lost years, which we feel bad about.
[01:32:01.560 --> 01:32:05.360]   His children attended private elementary school where he saw the administration introduce
[01:32:05.360 --> 01:32:11.520]   iPads and smart whiteboards only to quote, descend into chaos and then pull back from
[01:32:11.520 --> 01:32:13.200]   it all.
[01:32:13.200 --> 01:32:17.640]   Some cook said he wouldn't let his nephew join social networks, Bill Gates banned cell phones
[01:32:17.640 --> 01:32:19.000]   until his kids were teens.
[01:32:19.000 --> 01:32:25.560]   And of course, Steve Jobs famously wouldn't let his kids anywhere near iPads.
[01:32:25.560 --> 01:32:28.200]   I think there's some merit in this.
[01:32:28.200 --> 01:32:30.960]   I think you do have to.
[01:32:30.960 --> 01:32:34.960]   I mean, we I sit in restaurants sometimes and I see families.
[01:32:34.960 --> 01:32:36.520]   Oh, what the phone?
[01:32:36.520 --> 01:32:37.520]   Yeah.
[01:32:37.520 --> 01:32:41.720]   But so I sit there and I say that and you know, it's easy for me to judge.
[01:32:41.720 --> 01:32:43.800]   I don't know the situation.
[01:32:43.800 --> 01:32:50.400]   I will say that having a 12 year old, she wants to be on the computer all the time.
[01:32:50.400 --> 01:32:56.720]   And we actually started monitoring what she's doing on there and measuring the time because
[01:32:56.720 --> 01:32:58.040]   I think that's really important.
[01:32:58.040 --> 01:33:02.720]   And we just have a conversation with her because I noticed she stopped reading as much.
[01:33:02.720 --> 01:33:07.040]   And you know, I it is really easy though.
[01:33:07.040 --> 01:33:12.760]   I mean, I don't know if we've ever had something that will captivate your kid so much for such
[01:33:12.760 --> 01:33:20.880]   a long amount of time as parents and parenting, especially young children is freaking hard.
[01:33:20.880 --> 01:33:24.680]   And you know, I feel like we don't.
[01:33:24.680 --> 01:33:26.400]   There's a lot of things to think about here.
[01:33:26.400 --> 01:33:32.440]   There's the why is my why am I or my husband, the only person with my three year old, for
[01:33:32.440 --> 01:33:34.160]   example, she's not three now.
[01:33:34.160 --> 01:33:37.360]   But you know, we don't have these group settings.
[01:33:37.360 --> 01:33:40.440]   They're harder to get to with your kids where they could be entertained.
[01:33:40.440 --> 01:33:45.400]   We also have this constant message of like, you must be doing something with your child
[01:33:45.400 --> 01:33:47.800]   to engage their brain.
[01:33:47.800 --> 01:33:50.840]   You know, so there's a lot here.
[01:33:50.840 --> 01:33:56.400]   And I still remember some, you know, you read too many books back in the day.
[01:33:56.400 --> 01:33:57.400]   Oh, yeah.
[01:33:57.400 --> 01:33:58.400]   Get outside.
[01:33:58.400 --> 01:33:59.400]   Get some fresh air.
[01:33:59.400 --> 01:34:00.400]   Oh, yeah.
[01:34:00.400 --> 01:34:01.400]   You'll spoil your eyes.
[01:34:01.400 --> 01:34:02.400]   Oh, yeah.
[01:34:02.400 --> 01:34:03.400]   Oh, yeah.
[01:34:03.400 --> 01:34:04.400]   You're living.
[01:34:04.400 --> 01:34:05.400]   Yeah.
[01:34:05.400 --> 01:34:12.080]   So I think it's hard to though that a lot of parents don't know how much screen time
[01:34:12.080 --> 01:34:14.520]   their child is actually having.
[01:34:14.520 --> 01:34:20.200]   And like, I mean, I gave my daughter a laptop and she can take it wherever she wants.
[01:34:20.200 --> 01:34:22.800]   But sometimes that's not the best strategy.
[01:34:22.800 --> 01:34:25.000]   And that's why we actually started implementing.
[01:34:25.000 --> 01:34:29.400]   That's why I started measuring things and starting to implement harder rules for this.
[01:34:29.400 --> 01:34:31.040]   And she will go for it.
[01:34:31.040 --> 01:34:36.240]   She will sit there for hours and hours if we don't stop it.
[01:34:36.240 --> 01:34:42.680]   And you know, is she going to turn into like some sort of, I don't know this is Chris Anderson's
[01:34:42.680 --> 01:34:44.000]   bad child?
[01:34:44.000 --> 01:34:45.680]   I feel like he's got some bad children.
[01:34:45.680 --> 01:34:47.040]   He's got a bad kid.
[01:34:47.040 --> 01:34:48.840]   Oh, we lost one.
[01:34:48.840 --> 01:34:50.920]   This was broken.
[01:34:50.920 --> 01:34:52.680]   I'm trying to since you baited me.
[01:34:52.680 --> 01:34:53.880]   You got to give me a plug.
[01:34:53.880 --> 01:34:54.880]   Okay.
[01:34:54.880 --> 01:34:58.960]   So we think about the thing I was in California a few weeks ago to do.
[01:34:58.960 --> 01:34:59.960]   We announced today.
[01:34:59.960 --> 01:35:00.960]   Oh, good.
[01:35:00.960 --> 01:35:01.960]   Fine.
[01:35:01.960 --> 01:35:03.560]   Yes, we were wondering what you were up to.
[01:35:03.560 --> 01:35:04.560]   Yes.
[01:35:04.560 --> 01:35:09.120]   So we announced a project disclosure funded by the Facebook Children's and Project by
[01:35:09.120 --> 01:35:13.480]   independent of Facebook and I received no money from the platforms myself and of disclosure
[01:35:13.480 --> 01:35:18.360]   to aggregate the signals of quality in news and make them available and more usable.
[01:35:18.360 --> 01:35:24.200]   We hope form to platforms and ad networks and at agencies and such.
[01:35:24.200 --> 01:35:31.560]   So I was out in California meeting with Twitter, Google search, Google news, YouTube, Mozilla,
[01:35:31.560 --> 01:35:33.240]   Microsoft and such.
[01:35:33.240 --> 01:35:37.840]   And so we announced the project today and it's an incredible, it's a white paper, which
[01:35:37.840 --> 01:35:42.280]   is another way to say really long blog post.
[01:35:42.280 --> 01:35:45.480]   Incredibly long as I try to go into the details of what it is and what we're doing.
[01:35:45.480 --> 01:35:46.480]   You see the device machine?
[01:35:46.480 --> 01:35:49.200]   No, it's at the town night saying it's on the rundown.
[01:35:49.200 --> 01:35:52.040]   It's the third from the last in the other.
[01:35:52.040 --> 01:35:53.920]   Okay.
[01:35:53.920 --> 01:35:56.600]   So the town night, it's a project of town night.
[01:35:56.600 --> 01:35:57.680]   Is it, this is so boring.
[01:35:57.680 --> 01:35:58.680]   We lost Stacy.
[01:35:58.680 --> 01:36:03.440]   She's going to feed some candy to the neighborhood show.
[01:36:03.440 --> 01:36:06.600]   Yeah, because they, they just won't stay on their screens.
[01:36:06.600 --> 01:36:07.600]   Oh.
[01:36:07.600 --> 01:36:13.720]   So anyway, so it's a, it's a project to try to drive more attention and dollars in the
[01:36:13.720 --> 01:36:14.720]   long run.
[01:36:14.720 --> 01:36:18.520]   We hope to quality news and to help the platforms and advertisers understand what quality news
[01:36:18.520 --> 01:36:20.320]   is because frankly, they're not very good at it.
[01:36:20.320 --> 01:36:21.320]   And I think we need to help.
[01:36:21.320 --> 01:36:22.820]   I like it.
[01:36:22.820 --> 01:36:24.960]   So I just wanted to announce that.
[01:36:24.960 --> 01:36:25.960]   Congratulations.
[01:36:25.960 --> 01:36:26.960]   Thank you.
[01:36:26.960 --> 01:36:31.520]   That was my thing for your announcement there, Jeff.
[01:36:31.520 --> 01:36:32.520]   Thank you.
[01:36:32.520 --> 01:36:33.520]   Thank you.
[01:36:33.520 --> 01:36:34.520]   I noted it board you so much.
[01:36:34.520 --> 01:36:35.520]   You had to leave the room.
[01:36:35.520 --> 01:36:37.320]   Stacy's got, my dog was whining.
[01:36:37.320 --> 01:36:39.400]   Stacy's got candy to deliver.
[01:36:39.400 --> 01:36:45.200]   I have things to do and, and I've got my Pikachu costume to put on and it's going to take a
[01:36:45.200 --> 01:36:47.200]   while because it's inflatable.
[01:36:47.200 --> 01:36:51.600]   And so we're going to, we're going to.
[01:36:51.600 --> 01:36:54.420]   I should show you, but honestly it takes a while.
[01:36:54.420 --> 01:36:57.900]   We're going to go to our break and get our picks of the week so we can wrap things up.
[01:36:57.900 --> 01:37:04.340]   Jeff Jarvis, town night foundation, the post aggregating signals of quality and news is
[01:37:04.340 --> 01:37:09.300]   at medium.com and his blog is bus machine.com.
[01:37:09.300 --> 01:37:17.140]   Stacy Higginbotham, Stacy on IOT.com and our picks of the week coming up at first.
[01:37:17.140 --> 01:37:19.100]   I want to show you my belt.
[01:37:19.100 --> 01:37:25.820]   My slide belts from Brig Taylor, high quality, comfortable ratchet belts that are easy to
[01:37:25.820 --> 01:37:26.820]   adjust.
[01:37:26.820 --> 01:37:32.540]   Many of us, many of us don't like belts because they just don't do the job they're supposed
[01:37:32.540 --> 01:37:33.540]   to do.
[01:37:33.540 --> 01:37:35.380]   Hold their pants up.
[01:37:35.380 --> 01:37:39.460]   And I've really had a, you know, battle with belts over the years, especially because most
[01:37:39.460 --> 01:37:43.940]   belts have a couple of holes and you just, they don't, they, they're not flexible enough.
[01:37:43.940 --> 01:37:45.900]   That's when I found these slide belts.
[01:37:45.900 --> 01:37:51.300]   By the way, this slide belt has a brass, no, I don't think this is brass buckle, which
[01:37:51.300 --> 01:37:52.660]   one is this one.
[01:37:52.660 --> 01:38:00.540]   This one is a vegan belt, which looks just like leather, vegan leather belt with a beautiful,
[01:38:00.540 --> 01:38:02.660]   I think, gun metal.
[01:38:02.660 --> 01:38:05.060]   Nah, it looks like chrome more to me.
[01:38:05.060 --> 01:38:08.100]   And you see how I've engraved it with twi, but let me show you how the slide belt works.
[01:38:08.100 --> 01:38:12.460]   So when you first get the slide belt, you can cut it to length if you want.
[01:38:12.460 --> 01:38:15.260]   It makes it, the buckles are very easily changed.
[01:38:15.260 --> 01:38:21.300]   So you can easily change the material or the buckle on your belt du jour if you choose.
[01:38:21.300 --> 01:38:23.500]   I'm going to lock the end in here.
[01:38:23.500 --> 01:38:29.140]   And then this is the key is the ratcheting mechanism on the other side patented, patented
[01:38:29.140 --> 01:38:31.860]   ratchet design with 32 adjustment options.
[01:38:31.860 --> 01:38:34.700]   So you always have a belt that fits just right.
[01:38:34.700 --> 01:38:42.260]   This is the sound you'll hear in the morning as I dress, as I ratchet my belt in just right.
[01:38:42.260 --> 01:38:45.860]   And this is the sound you'll hear later in the day after a big meal.
[01:38:45.860 --> 01:38:51.020]   I say silently expand the belt with 32 settings.
[01:38:51.020 --> 01:38:57.020]   There's always a perfect fit with my slide belt and they look great.
[01:38:57.020 --> 01:39:01.500]   A wide selection of straps from full grain and top grain leathers to high quality canvas.
[01:39:01.500 --> 01:39:03.460]   I'll show you one here.
[01:39:03.460 --> 01:39:05.860]   This is, I call this my Cub Scout belt.
[01:39:05.860 --> 01:39:10.740]   It's an antique brass buckle with kind of the blue canvas.
[01:39:10.740 --> 01:39:13.780]   This is kind of a grown up Cub Scout belt, but it looks great.
[01:39:13.780 --> 01:39:17.020]   And because you could change the buckles and belts very easily, it's kind of like an Apple
[01:39:17.020 --> 01:39:18.020]   watch.
[01:39:18.020 --> 01:39:20.500]   You always have new looks to try out.
[01:39:20.500 --> 01:39:22.900]   They could be worn casually around the house at the office.
[01:39:22.900 --> 01:39:23.900]   They're great for date night.
[01:39:23.900 --> 01:39:24.900]   They've got all kinds.
[01:39:24.900 --> 01:39:30.660]   Oh, it's also a great travel belt because it's very easy to take off at security and
[01:39:30.660 --> 01:39:32.980]   put right back on.
[01:39:32.980 --> 01:39:36.540]   And I have to show you, this is my favorite buckle.
[01:39:36.540 --> 01:39:41.060]   This is the slide belt survival belt.
[01:39:41.060 --> 01:39:43.100]   All right, get ready for this.
[01:39:43.100 --> 01:39:44.100]   How about a buckle?
[01:39:44.100 --> 01:39:52.820]   It looks like a normal everyday belt buckle, but contains an ultra sharp knife, beer opener.
[01:39:52.820 --> 01:39:55.460]   Let me close the knife before I hurt myself.
[01:39:55.460 --> 01:39:56.460]   Flashlight.
[01:39:56.460 --> 01:40:01.700]   And it's a right, really bright one, by the way, and fire starter.
[01:40:01.700 --> 01:40:04.460]   This is the belt you wear out in the wilderness.
[01:40:04.460 --> 01:40:08.980]   I wouldn't recommend wearing it into a court, however, it is.
[01:40:08.980 --> 01:40:09.980]   But that's nice.
[01:40:09.980 --> 01:40:12.860]   You just change the buckle and you can wear something else.
[01:40:12.860 --> 01:40:16.780]   They have unisex survival belts like this, colorful belts for kids, skinny belts for
[01:40:16.780 --> 01:40:18.060]   women.
[01:40:18.060 --> 01:40:25.860]   Coming soon, slide belt watches with swappable watch bands, again, for ultimate customization.
[01:40:25.860 --> 01:40:30.420]   And they offer a one year warranty, free exchanges and no hassle returns.
[01:40:30.420 --> 01:40:32.060]   I've got a special offer for you.
[01:40:32.060 --> 01:40:38.260]   But before you check that out, check out their gift sets tab at slidebelts.com/twit for some
[01:40:38.260 --> 01:40:39.580]   great offers.
[01:40:39.580 --> 01:40:43.340]   Now those sets are already discounted, so this offer doesn't apply to those.
[01:40:43.340 --> 01:40:47.180]   When you're ready for a better belt, go to slidebelts.com/twit and use the code "twit"
[01:40:47.180 --> 01:40:51.060]   at checkout and receive 20% off your order.
[01:40:51.060 --> 01:40:54.220]   So really, every belt is on sale for us.
[01:40:54.220 --> 01:40:58.380]   At slidebelts.com/twit, it's now the only belt I wear.
[01:40:58.380 --> 01:41:01.540]   Use the code "twit" at checkout for 20% off your order.
[01:41:01.540 --> 01:41:07.820]   And it is not only good looking, practical, but it'll actually hold your pants up.
[01:41:07.820 --> 01:41:09.780]   It's an amazing invention.
[01:41:09.780 --> 01:41:10.940]   Slidebelts.
[01:41:10.940 --> 01:41:14.500]   Rethink your belt, slidebelts.com/twit.
[01:41:14.500 --> 01:41:17.020]   This should be their signature sound.
[01:41:17.020 --> 01:41:21.940]   I love it.
[01:41:21.940 --> 01:41:23.740]   Andrew is a big slide belt user.
[01:41:23.740 --> 01:41:24.740]   He's got a bunch of them.
[01:41:24.740 --> 01:41:25.740]   Oh yeah.
[01:41:25.740 --> 01:41:28.780]   I'm sad that now "twit" gets 20% off.
[01:41:28.780 --> 01:41:31.860]   I'm like, "Oh, I could have back good at selling money."
[01:41:31.860 --> 01:41:32.860]   You could save lots of money.
[01:41:32.860 --> 01:41:33.860]   And they have a big range.
[01:41:33.860 --> 01:41:36.060]   I mean, the leather belts are fairly pricey.
[01:41:36.060 --> 01:41:39.500]   I don't remember how much I paid, like 60 bucks, but then they have very inexpensive ones,
[01:41:39.500 --> 01:41:40.500]   too.
[01:41:40.500 --> 01:41:41.500]   They have all kinds, even kits go.
[01:41:41.500 --> 01:41:42.500]   Oh my gosh.
[01:41:42.500 --> 01:41:43.500]   That is not pricey.
[01:41:43.500 --> 01:41:44.500]   I pay hundreds of dollars.
[01:41:44.500 --> 01:41:45.500]   I know.
[01:41:45.500 --> 01:41:47.700]   Ladies belts can be very, very expensive.
[01:41:47.700 --> 01:41:49.260]   For little, tiny, thin belts.
[01:41:49.260 --> 01:41:50.260]   I know.
[01:41:50.260 --> 01:41:51.860]   Little, little, little, little, little, bigs.
[01:41:51.860 --> 01:41:52.860]   Terrible.
[01:41:52.860 --> 01:41:53.860]   Yes.
[01:41:53.860 --> 01:41:55.700]   Stacy, how about a pick of the week from you?
[01:41:55.700 --> 01:41:56.700]   My friend.
[01:41:56.700 --> 01:41:57.700]   All right.
[01:41:57.700 --> 01:41:59.340]   My pick of the week.
[01:41:59.340 --> 01:42:00.340]   Yes.
[01:42:00.340 --> 01:42:03.380]   It's because it's seasonal and it also tells you something really good.
[01:42:03.380 --> 01:42:07.340]   You may have seen this because it was pretty big, but the New York Times did an AI costume
[01:42:07.340 --> 01:42:13.700]   explainer by actually one of my favorite bloggers in the whole wide world.
[01:42:13.700 --> 01:42:15.900]   It's an opinion column by Janelle Shane.
[01:42:15.900 --> 01:42:20.580]   She goes to AIweirdness.com and she runs like recipes.
[01:42:20.580 --> 01:42:25.660]   She runs all kinds of things through neural networks just to see what weirdness they generate.
[01:42:25.660 --> 01:42:32.260]   But this article did an excellent job like being funny, which she has trademark her,
[01:42:32.260 --> 01:42:35.540]   and also explaining how neural nets work, which is really hard.
[01:42:35.540 --> 01:42:36.540]   Wow.
[01:42:36.540 --> 01:42:42.220]   And it's remarkably depressing how quickly sexy starts running up at the...
[01:42:42.220 --> 01:42:44.940]   It helps you pick your costume basically.
[01:42:44.940 --> 01:42:46.780]   And it's sexy.
[01:42:46.780 --> 01:42:50.780]   It ends up there like the second or third iteration of the AI.
[01:42:50.780 --> 01:42:54.660]   Well, that's poorly trained AI, don't you think?
[01:42:54.660 --> 01:42:55.660]   Well...
[01:42:55.660 --> 01:42:56.980]   Or maybe AI is a warning.
[01:42:56.980 --> 01:42:59.980]   This talks about how you...
[01:42:59.980 --> 01:43:04.180]   Because it's not necessarily poorly trained, it's just how it's training itself.
[01:43:04.180 --> 01:43:05.180]   But it's trained with it.
[01:43:05.180 --> 01:43:06.180]   Yeah, what it's doing with it.
[01:43:06.180 --> 01:43:07.180]   Yeah.
[01:43:07.180 --> 01:43:11.540]   Although I loved the New York Times diagram for Sexy Printer, which is a printer that is
[01:43:11.540 --> 01:43:12.980]   putting out a pixelated image.
[01:43:12.980 --> 01:43:19.100]   I was like, "That is my Halloween costume, Sexy Printer."
[01:43:19.100 --> 01:43:23.260]   So if you haven't seen this yet, go read it.
[01:43:23.260 --> 01:43:24.260]   Go learn something.
[01:43:24.260 --> 01:43:26.220]   You'll also be heartily abused.
[01:43:26.220 --> 01:43:31.260]   And if you really want to get bonus points, visit her website because it's freaking awesome.
[01:43:31.260 --> 01:43:32.260]   Wow.
[01:43:32.260 --> 01:43:33.900]   That's awesome.
[01:43:33.900 --> 01:43:35.100]   Very cool.
[01:43:35.100 --> 01:43:36.860]   And I had it up here and now it's gone again.
[01:43:36.860 --> 01:43:38.140]   Let me pull it up again.
[01:43:38.140 --> 01:43:40.620]   I must have opened something on top of it.
[01:43:40.620 --> 01:43:43.580]   It's Halloween.
[01:43:43.580 --> 01:43:44.580]   Let your...
[01:43:44.580 --> 01:43:49.060]   Janelle Shane, let your algorithm choose your Halloween costume.
[01:43:49.060 --> 01:43:51.380]   So scroll down and you'll see the process.
[01:43:51.380 --> 01:43:56.900]   Oh, at Epoch 1, it shows like it's just looking at words.
[01:43:56.900 --> 01:43:58.420]   Oh, wow.
[01:43:58.420 --> 01:44:01.660]   The letters used in the costumes, they fed it.
[01:44:01.660 --> 01:44:08.540]   So Epoch 1 has, if you keep scrolling down, those are the first iteration.
[01:44:08.540 --> 01:44:09.540]   So that's what came out.
[01:44:09.540 --> 01:44:10.540]   You're like, "What?"
[01:44:10.540 --> 01:44:15.940]   But then go down to Epoch 3 and you start seeing Captain Kirk in Doroth Vader.
[01:44:15.940 --> 01:44:18.180]   Kirk, with a K, Kirk.
[01:44:18.180 --> 01:44:20.380]   And Wim is Z, a rat.
[01:44:20.380 --> 01:44:21.380]   Wow.
[01:44:21.380 --> 01:44:22.380]   Jashmashk.
[01:44:22.380 --> 01:44:26.180]   Epoch 5, sexy the babbie cahapper.
[01:44:26.180 --> 01:44:29.740]   Frog shadown, ficking sister.
[01:44:29.740 --> 01:44:30.740]   Regun.
[01:44:30.740 --> 01:44:31.740]   It's getting worse.
[01:44:31.740 --> 01:44:34.580]   Princess Leia, well, that's good.
[01:44:34.580 --> 01:44:36.140]   Dulk.
[01:44:36.140 --> 01:44:37.900]   Haka your leg.
[01:44:37.900 --> 01:44:40.460]   Haka your on leg.
[01:44:40.460 --> 01:44:41.460]   Dodge.
[01:44:41.460 --> 01:44:42.460]   Trick-tancer.
[01:44:42.460 --> 01:44:43.460]   Giant.
[01:44:43.460 --> 01:44:45.860]   Robot B was one of my favorites too.
[01:44:45.860 --> 01:44:46.860]   I'm like, nice.
[01:44:46.860 --> 01:44:52.580]   Now I have to point out that the illustrations are human driven, not AI driven.
[01:44:52.580 --> 01:44:55.700]   So Strawberry Clown by Epic 9, we're actually getting things.
[01:44:55.700 --> 01:44:59.300]   Gummy Cow, Ruth Bader Hat Guy.
[01:44:59.300 --> 01:45:01.060]   Buttery Classic.
[01:45:01.060 --> 01:45:06.940]   I see the best little kid, like Ruth Bader Baby costumes and stuff.
[01:45:06.940 --> 01:45:07.940]   Oh, nice.
[01:45:07.940 --> 01:45:08.940]   I like it.
[01:45:08.940 --> 01:45:09.940]   I like it.
[01:45:09.940 --> 01:45:10.940]   We love Ruth.
[01:45:10.940 --> 01:45:11.940]   Ruth Baby Ginsburg.
[01:45:11.940 --> 01:45:12.940]   Jeff.
[01:45:12.940 --> 01:45:13.940]   Baby Genshurg.
[01:45:13.940 --> 01:45:14.940]   Thank you.
[01:45:14.940 --> 01:45:15.940]   Thank you.
[01:45:15.940 --> 01:45:17.380]   You've got sexy beta marks.
[01:45:17.380 --> 01:45:21.620]   Kevin is a vampire chick shark today.
[01:45:21.620 --> 01:45:23.540]   Centaur Meiji, Dragon Ninja.
[01:45:23.540 --> 01:45:25.580]   This is getting pretty good by Epic 11.
[01:45:25.580 --> 01:45:26.580]   What is an Epic?
[01:45:26.580 --> 01:45:28.380]   Is it a bunch of training?
[01:45:28.380 --> 01:45:32.340]   It's the next, it's each iteration where you tweak it and then you run it through again
[01:45:32.340 --> 01:45:34.460]   to see what it comes up with.
[01:45:34.460 --> 01:45:36.980]   This is related.
[01:45:36.980 --> 01:45:43.300]   It's Google's Fright Geist at Fright Geist.withGoogle.com.
[01:45:43.300 --> 01:45:46.100]   This is Halloween's most popular costumes.
[01:45:46.100 --> 01:45:48.740]   Fortnite, trending up.
[01:45:48.740 --> 01:45:52.620]   Number one, nationwide.
[01:45:52.620 --> 01:45:55.860]   Video games make up 4% of all costume searches.
[01:45:55.860 --> 01:45:58.220]   Spiderman.
[01:45:58.220 --> 01:45:59.460]   Trending up.
[01:45:59.460 --> 01:46:00.460]   Number two, nationwide.
[01:46:00.460 --> 01:46:05.980]   And you can see what regions, unicorn, dinosaur, which Harley Quinn, that's last years.
[01:46:05.980 --> 01:46:10.980]   Superhero pirate, rabbit, clown Wonder Woman, monster ninja doll pet.
[01:46:10.980 --> 01:46:11.980]   Well, prize bet.
[01:46:11.980 --> 01:46:12.980]   It's a little bit more fun there.
[01:46:12.980 --> 01:46:15.900]   Yeah, you know, I think it's just too scary.
[01:46:15.900 --> 01:46:18.420]   Yeah, we're living that reality.
[01:46:18.420 --> 01:46:19.420]   It's done.
[01:46:19.420 --> 01:46:20.420]   Jeff Jarvis.
[01:46:20.420 --> 01:46:24.500]   Well, so this is related to Stacey's, but it's a kind of a contrast.
[01:46:24.500 --> 01:46:26.820]   Stacey's is AI for what?
[01:46:26.820 --> 01:46:29.820]   First to Pinity for wasted time for yucks.
[01:46:29.820 --> 01:46:30.820]   Now we have Google.
[01:46:30.820 --> 01:46:31.820]   Yeah.
[01:46:31.820 --> 01:46:36.580]   Google is trading a $25 million fund, AI for humanity.
[01:46:36.580 --> 01:46:37.900]   Oh, nice.
[01:46:37.900 --> 01:46:44.180]   A global competition and that solve big problems, including the examples that Google gives on
[01:46:44.180 --> 01:46:50.500]   its own post is wildlife conservation employment in South Africa.
[01:46:50.500 --> 01:46:58.340]   A program matched 50,000 candidates with jobs, flood prediction, wildfire prevention,
[01:46:58.340 --> 01:47:01.980]   infant health, solving problems.
[01:47:01.980 --> 01:47:10.540]   I would imagine the the verge story says like racial bias and other biases in AI.
[01:47:10.540 --> 01:47:14.180]   So yeah, AI doesn't have to be scary.
[01:47:14.180 --> 01:47:16.540]   It doesn't have to be trick or treat.
[01:47:16.540 --> 01:47:19.820]   It can just be treat.
[01:47:19.820 --> 01:47:25.140]   I will say to you, I actually think that story that I did was a great thing to show your journalism
[01:47:25.140 --> 01:47:27.980]   students because it's a fun way to engage people about.
[01:47:27.980 --> 01:47:28.980]   Oh, yeah.
[01:47:28.980 --> 01:47:29.980]   I just was one of the people.
[01:47:29.980 --> 01:47:32.460]   I was like, it was a gisily.
[01:47:32.460 --> 01:47:33.860]   No, no, no, it was just pure.
[01:47:33.860 --> 01:47:35.940]   No, but that is that's a good point.
[01:47:35.940 --> 01:47:37.940]   I was trying for a segue, you see.
[01:47:37.940 --> 01:47:38.940]   Oh, okay.
[01:47:38.940 --> 01:47:40.940]   Well, I was trying for that.
[01:47:40.940 --> 01:47:48.420]   I was trying to tie into yours, but but no, no, your standards are just too high to sincere.
[01:47:48.420 --> 01:47:49.420]   I'm sorry.
[01:47:49.420 --> 01:47:50.620]   Yes, war.
[01:47:50.620 --> 01:47:54.820]   My pick of the week came from last month, but I actually did it last night.
[01:47:54.820 --> 01:48:01.780]   Coach Tony on medium from the better humans medium section, how to configure your iPhone
[01:48:01.780 --> 01:48:03.540]   to work for you, not against you.
[01:48:03.540 --> 01:48:09.460]   Now I'm not saying everything coach Tony recommends in here is what you should do, but
[01:48:09.460 --> 01:48:15.180]   there's well, a lot of good things in here, including turning off almost all notifications.
[01:48:15.180 --> 01:48:20.420]   Notifications are just a way to steal your brain.
[01:48:20.420 --> 01:48:26.500]   Side social media slot machines, I did coach Tony one better by deleting them entirely.
[01:48:26.500 --> 01:48:29.740]   Disable app review requests done.
[01:48:29.740 --> 01:48:32.540]   Turn on do not disturb all around the clock.
[01:48:32.540 --> 01:48:37.900]   The Apple allows you to say only allow phone calls from favorites or contacts if you want,
[01:48:37.900 --> 01:48:39.340]   if you want to be a little broader.
[01:48:39.340 --> 01:48:44.900]   And you know, that's the best way to fight robo calling is just only only get rings when
[01:48:44.900 --> 01:48:46.820]   it's people you know.
[01:48:46.820 --> 01:48:48.220]   Everybody else goes to voicemail.
[01:48:48.220 --> 01:48:51.140]   And guess what robo calls never leave voicemails.
[01:48:51.140 --> 01:48:55.980]   He says be strategic about your wallpaper actually suggested black wallpaper, which
[01:48:55.980 --> 01:48:57.420]   I'm using.
[01:48:57.420 --> 01:49:02.140]   He says because it makes your phone less interesting.
[01:49:02.140 --> 01:49:03.140]   It works.
[01:49:03.140 --> 01:49:10.300]   And of course, a lot of phones now have techniques, both Android and iOS to improve downtime,
[01:49:10.300 --> 01:49:12.260]   to limit screen time and so forth.
[01:49:12.260 --> 01:49:15.380]   He's got some good app suggestions in here as well.
[01:49:15.380 --> 01:49:20.380]   This is on the medium blog, the title of it is and you can just Google how to configure
[01:49:20.380 --> 01:49:22.780]   your iPhone to work for you.
[01:49:22.780 --> 01:49:23.780]   Not against you.
[01:49:23.780 --> 01:49:27.180]   It's long with lots of details and I don't think anybody's going to do everything he
[01:49:27.180 --> 01:49:28.180]   suggests.
[01:49:28.180 --> 01:49:33.860]   But there's some really good ideas in here if you're using an iPhone, even some podcasts
[01:49:33.860 --> 01:49:35.580]   you should subscribe to.
[01:49:35.580 --> 01:49:36.900]   He doesn't mention ours.
[01:49:36.900 --> 01:49:39.100]   Oops, that link is dead.
[01:49:39.100 --> 01:49:44.940]   But frankly, I think subscribing to Twig is a very good use of your time.
[01:49:44.940 --> 01:49:47.220]   I encourage you to do so.
[01:49:47.220 --> 01:49:49.700]   If you want to hear Twig, you can come by, watch it live.
[01:49:49.700 --> 01:49:53.460]   We do it every Wednesday, 130 Pacific, 430 Eastern, 2030 UTC.
[01:49:53.460 --> 01:50:00.620]   Actually, it's going to be 2130 UTC next starting next week as we go back to standard time,
[01:50:00.620 --> 01:50:05.540]   not daylight Pacific standard time on Sunday.
[01:50:05.540 --> 01:50:12.740]   So we move UTC doesn't, but that means we'll be at 2100 UTC, 2130 UTC.
[01:50:12.740 --> 01:50:13.980]   Next Wednesday.
[01:50:13.980 --> 01:50:16.780]   Go to twit.tv/live to watch.
[01:50:16.780 --> 01:50:23.220]   You can't watch us on YouTube live as of right now because yesterday, YouTube dinged us for
[01:50:23.220 --> 01:50:30.540]   a copyright violation because we dared to come on an Apple's stream.
[01:50:30.540 --> 01:50:34.060]   We've appealed and I'm sure it'll get reversed as that's fair use, right?
[01:50:34.060 --> 01:50:35.140]   You journalistic types.
[01:50:35.140 --> 01:50:36.540]   We I would say so.
[01:50:36.540 --> 01:50:37.540]   We always run the comm.
[01:50:37.540 --> 01:50:38.780]   I would argue it is.
[01:50:38.780 --> 01:50:41.060]   And then we always run their thing, but we don't run it alone.
[01:50:41.060 --> 01:50:46.620]   We run it in a box with our explanations of what it means, our ex.
[01:50:46.620 --> 01:50:50.180]   Does that mean that Apple complained or is it an automated process?
[01:50:50.180 --> 01:50:51.180]   It happened so fast.
[01:50:51.180 --> 01:50:52.180]   It happened during the broadcast.
[01:50:52.180 --> 01:50:57.580]   I suspect it was automated, although it did identify the offending candidates from Apple.
[01:50:57.580 --> 01:50:59.140]   So I don't know.
[01:50:59.140 --> 01:51:03.580]   Maybe, maybe Apple preloaded something to do that.
[01:51:03.580 --> 01:51:04.580]   The appeal.
[01:51:04.580 --> 01:51:05.580]   They've never done that before.
[01:51:05.580 --> 01:51:10.900]   And we've done this for 12 years.
[01:51:10.900 --> 01:51:13.100]   The appeal will, I think, go to Apple.
[01:51:13.100 --> 01:51:16.900]   Usually, the copyright holder then is notified and they get to decide.
[01:51:16.900 --> 01:51:20.380]   So if we're no longer on YouTube live, thank you, Apple.
[01:51:20.380 --> 01:51:23.860]   Well, does YouTube do a verification process?
[01:51:23.860 --> 01:51:24.860]   Yeah.
[01:51:24.860 --> 01:51:25.860]   Legitimate media entities?
[01:51:25.860 --> 01:51:26.860]   Are you?
[01:51:26.860 --> 01:51:27.860]   Oh, yeah.
[01:51:27.860 --> 01:51:33.260]   And most always, when we get dinged, we appeal it and we say it's fair use and they say yes.
[01:51:33.260 --> 01:51:35.900]   They say, OK, sorry about that.
[01:51:35.900 --> 01:51:40.940]   YouTube live is tough because you doesn't take more than one strike to ban you from YouTube
[01:51:40.940 --> 01:51:41.940]   live.
[01:51:41.940 --> 01:51:43.420]   So how low is the appeal process?
[01:51:43.420 --> 01:51:44.420]   I don't know.
[01:51:44.420 --> 01:51:45.620]   It's underway right now.
[01:51:45.620 --> 01:51:46.860]   It could be a few days.
[01:51:46.860 --> 01:51:47.860]   I'm just saying.
[01:51:47.860 --> 01:51:48.860]   Oye.
[01:51:48.860 --> 01:51:49.860]   Oye.
[01:51:49.860 --> 01:51:50.860]   Oye.
[01:51:50.860 --> 01:51:55.380]   We're going to the Supreme Court with the case of Twitch versus Apple.
[01:51:55.380 --> 01:52:00.980]   No, but you still plenty of other places to watch us at twitch.tv/live.
[01:52:00.980 --> 01:52:04.060]   That's why I recommend that site because it has a list of all of audio and video, by
[01:52:04.060 --> 01:52:05.060]   the way.
[01:52:05.060 --> 01:52:08.180]   You can download shows at twitch.tv/twig.
[01:52:08.180 --> 01:52:10.740]   Best yet, subscribe in your favorite podcast app.
[01:52:10.740 --> 01:52:13.260]   No one will ever prevent you from getting it there.
[01:52:13.260 --> 01:52:17.420]   And you can watch and listen to every show every week the minute it's available.
[01:52:17.420 --> 01:52:19.780]   Thank you so much.
[01:52:19.780 --> 01:52:21.260]   Stacy Higginbotham.
[01:52:21.260 --> 01:52:23.380]   Good luck with Halloween.
[01:52:23.380 --> 01:52:24.380]   Thank you.
[01:52:24.380 --> 01:52:25.380]   I'm excited.
[01:52:25.380 --> 01:52:26.580]   Do you go out with them and follow them?
[01:52:26.580 --> 01:52:29.460]   Or are they too old for that now?
[01:52:29.460 --> 01:52:33.580]   They're actually so I this year I'm staying home because I was doing this show.
[01:52:33.580 --> 01:52:34.580]   So they just left.
[01:52:34.580 --> 01:52:35.580]   And she didn't wear her wig.
[01:52:35.580 --> 01:52:36.980]   I could have been in a purple wig.
[01:52:36.980 --> 01:52:37.980]   Oh, man.
[01:52:37.980 --> 01:52:38.980]   Guys, go.
[01:52:38.980 --> 01:52:39.980]   Oh, man.
[01:52:39.980 --> 01:52:40.980]   Ooh.
[01:52:40.980 --> 01:52:43.460]   That's a bummer.
[01:52:43.460 --> 01:52:44.460]   Thank you, Stacy.
[01:52:44.460 --> 01:52:45.460]   Have a great day.
[01:52:45.460 --> 01:52:46.460]   Have a great week.
[01:52:46.460 --> 01:52:47.460]   See you next time.
[01:52:47.460 --> 01:52:48.460]   You guys too.
[01:52:48.460 --> 01:52:54.260]   You too, Jeff Jarvis, Professor of Journalism at the University of New York Town Heights.
[01:52:54.260 --> 01:52:57.820]   Craig Newmark Graduate School of Journalism at the City of University of New York.
[01:52:57.820 --> 01:52:58.820]   Thank you.
[01:52:58.820 --> 01:52:59.820]   That was easy.
[01:52:59.820 --> 01:53:00.820]   See, buzzmachine.com.
[01:53:00.820 --> 01:53:02.380]   Read all of his great books.
[01:53:02.380 --> 01:53:03.380]   What would Google do?
[01:53:03.380 --> 01:53:05.940]   Public parts, ski, experience, gifts and all.
[01:53:05.940 --> 01:53:07.820]   And you can find all that at buzzmachine.com.
[01:53:07.820 --> 01:53:09.900]   Thank you, everybody, for being here.
[01:53:09.900 --> 01:53:12.140]   We'll see you next time on This Week in Google.
[01:53:12.140 --> 01:53:12.700]   Bye-bye.
[01:53:12.700 --> 01:53:19.700]   [MUSIC PLAYING]
[01:53:19.700 --> 01:53:24.700]   [ Music ]

