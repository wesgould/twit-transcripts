;FFMETADATA1
title=Boca de DragÃ³n
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-06-16
track=668
language=English
genre=Podcast
comment=Sentient AI, Gates says crypto is a sham, Meta AR glasses, Floppotron 3.0
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.960]   It's time for Twig this week in Google. Stacey Aegabotham is back from Puerto Rico.
[00:00:04.960 --> 00:00:11.040]   Jeff Jarvis is here and Pruitt too. We're going to talk about the Google AI that has come to life.
[00:00:11.040 --> 00:00:16.720]   Or is that nonsense on stilts? Bill Gates says, "Don't you be buying any Bitcoin?"
[00:00:16.720 --> 00:00:22.720]   Well, it's about, we should tell me earlier, Bill. Elon Musk says, "I'm talking to Twitter tomorrow."
[00:00:22.720 --> 00:00:26.640]   And Meta giving up on AR. It's all coming up next on Twig.
[00:00:29.520 --> 00:00:35.280]   Podcasts you love. From people you trust. This is Twig.
[00:00:35.280 --> 00:00:47.920]   This is Twig. This week in Google, episode 668 recorded Wednesday, June 15, 2022.
[00:00:47.920 --> 00:00:54.960]   Boca de Dragon. This episode of This Week in Google is brought to you by Nureva,
[00:00:54.960 --> 00:01:00.720]   traditional audio conferencing systems can entail lots of components. Installation can take days.
[00:01:00.720 --> 00:01:07.200]   And you might not get the mic coverage you need. That's complex-pensive. But Nureva audio is easy
[00:01:07.200 --> 00:01:13.120]   to install and manage, no technicians required. And you get true full-room coverage. Now that's
[00:01:13.120 --> 00:01:22.880]   easy-canomical. Learn more at Nureva.com and buy HPE GreenLake, orchestrated by the experts at CDW,
[00:01:22.880 --> 00:01:27.520]   to help deliver a seamless and scalable cloud experience that comes to you.
[00:01:27.520 --> 00:01:35.200]   Learn more at CDW.com/HPE. And buy CashFly.
[00:01:35.200 --> 00:01:39.040]   Deliver your video on the network with the best throughput and global reach,
[00:01:39.040 --> 00:01:46.960]   making your content infinitely scalable. Go live in hours, not days. Learn more at cashfly.com.
[00:01:48.800 --> 00:01:52.960]   It's time for Twig this week in Google, the only show on the Twig Network without new album art.
[00:01:52.960 --> 00:01:58.720]   But that's coming soon. Yeah, sure. We're always the last at line.
[00:01:58.720 --> 00:02:06.000]   No, I feel I feel for a child. Oh, please. Oh, here we are at the kitty table, waiting for them
[00:02:06.000 --> 00:02:10.960]   to pass the grave. Did you see? Did you see the new? By the way, that's Jeff Jarvis, the director
[00:02:10.960 --> 00:02:17.600]   of the townite center. Ruppy is our entrepreneurial journalism at the Craig Newmark Graduate School of
[00:02:17.600 --> 00:02:24.800]   Journalism. Yeah, you're missing the cue. What a day at the Craig Newmark Graduate School of
[00:02:24.800 --> 00:02:34.160]   Journalism at the City University of New York. And Mr. Grumpy. This is the, Paul and Mary Jo did not
[00:02:34.160 --> 00:02:39.360]   like the new Windows Weekly album art. I think this is really good. This is the new, this week in
[00:02:39.360 --> 00:02:46.000]   enterprise tech album art. Get it? Get it? Get it? I like it. Oh, somebody.
[00:02:46.000 --> 00:02:51.600]   I thought it was something else. Yeah, it's not immediately obvious. I know. It's a little
[00:02:51.600 --> 00:02:56.960]   Georgia O'Keefe. That's who's doing our album art. It's cool. Yeah. Almost like the Superman thing,
[00:02:56.960 --> 00:03:01.040]   right? Yeah, very good. Yeah. All about Androids is gorgeous, gorgeous.
[00:03:02.480 --> 00:03:08.080]   What do we, I can't remember if we, I think we, we collabed on the, on the, this week in Google
[00:03:08.080 --> 00:03:14.320]   Alamart. That one's really nice. Where are the windows? The windows ones controversial because
[00:03:14.320 --> 00:03:20.880]   they say, that's this week in tech. They say there's no tension between the Windows Weekly gang.
[00:03:20.880 --> 00:03:25.200]   That's an old hands off dark. You're getting a new one too. This is the Windows Weekly one.
[00:03:25.200 --> 00:03:29.120]   Oh, I see. Oh, they say there's no tension between them, but I don't care. That's true.
[00:03:29.120 --> 00:03:34.560]   I'm trying to pretend there is. Take a hit. Make it more like this show, right?
[00:03:34.560 --> 00:03:36.480]   Yes. Yeah.
[00:03:36.480 --> 00:03:43.920]   ours is going to be a bunch of boxers. Well, you see ours. I, I, okay, I love it, but I won't,
[00:03:43.920 --> 00:03:47.840]   I won't prejudice you, but ours is, I think you're afraid of our reaction. That's,
[00:03:47.840 --> 00:03:50.800]   I am terrified of the reaction. That's Stacy. What are we going to see?
[00:03:50.800 --> 00:03:52.000]   He's back. Well, someday.
[00:03:55.680 --> 00:04:00.880]   Great. I think, I don't know when we're going to roll it out. We worked on it last week because
[00:04:00.880 --> 00:04:06.160]   we have a great design team at Clutch and they're working back and forth. So it starts,
[00:04:06.160 --> 00:04:12.880]   I will do a word cloud for the show. And then they based on that will do a first
[00:04:12.880 --> 00:04:17.120]   iteration of designs that usually give us a. I want to see your word cloud design.
[00:04:17.120 --> 00:04:21.760]   Oh, please, please show us your work. Well, the one that they ended up moral panic.
[00:04:21.760 --> 00:04:25.280]   World panic. I mentioned the one that they ended up, uh,
[00:04:25.280 --> 00:04:29.040]   warming onto was, uh, kind of a, uh,
[00:04:29.040 --> 00:04:36.560]   fractious family around a dinner table. Kind of waffles where there's waffles and waffles.
[00:04:36.560 --> 00:04:42.880]   That's Stacy. Back from her visit to Puerto Rico. Do they have case zone Puerto Rico?
[00:04:42.880 --> 00:04:50.080]   I don't know. Uh, case those really a text max thing, but I mean, they have cheese.
[00:04:50.080 --> 00:04:52.880]   They have cheese. They have case. They have.
[00:04:52.880 --> 00:04:59.840]   Casitos, which are little fried. They're like a Danish except it's wrapped around all the way.
[00:04:59.840 --> 00:05:02.160]   I love Puerto Rico. Oh, it's so good.
[00:05:02.160 --> 00:05:06.640]   Riot dip it in a little honey with a little cinnamon sugar. Oh, yum. Ooh,
[00:05:06.640 --> 00:05:10.880]   could you have good food there? That's breakfast. I had some good food. It was.
[00:05:10.880 --> 00:05:16.320]   It's hard. I stayed at the same hotel as my kid who was in high school. So like,
[00:05:18.960 --> 00:05:23.760]   you're just slumming. Slumming. It wasn't five star hotel. I think is what you're saying.
[00:05:23.760 --> 00:05:27.520]   Anyway, we missed you terribly, but it's so good to have you back. Welcome back. Yes,
[00:05:27.520 --> 00:05:32.400]   it is good to have you back. Also with this, uh, Pruitt from hands on photography. He's also the
[00:05:32.400 --> 00:05:39.440]   community manager in our club, Twit Discord. And, uh, he has kindly put together Stacy's book club
[00:05:39.440 --> 00:05:44.240]   for tomorrow at nine AM. Why do you do you things in nine AM?
[00:05:45.760 --> 00:05:50.400]   Because I apparently don't have as much time as other folks. Yeah.
[00:05:50.400 --> 00:05:57.600]   Nine AM. Did you notice, Ant, that all my shows begin at 11 AM at the earliest? Do you
[00:05:57.600 --> 00:06:03.040]   notice that? Anyway, I'm going to get up. I'm so harsh. He's doing it to make it easier for me.
[00:06:03.040 --> 00:06:07.280]   Oh, well, if it's good for you, that's fine. I don't mind. I'm sitting here thinking about you
[00:06:07.280 --> 00:06:12.000]   reading the book that we're going to discuss tomorrow and how long it is. And I'm like,
[00:06:12.000 --> 00:06:16.800]   how in the heck did he have enough time to get that book done? I jammed it. I finished it. And I
[00:06:16.800 --> 00:06:23.440]   listened to you guys are listening at one half time speed. I listened at one X and I just listened
[00:06:23.440 --> 00:06:27.840]   a lot and I finished it this morning and I really liked it. But I know that there are the years
[00:06:27.840 --> 00:06:32.960]   controversial. So we'll talk about that. It could be a good event. The book is Neil Stevenson's latest
[00:06:32.960 --> 00:06:38.240]   termination shock. He is challenging, I think, for a lot of people. He's famous for
[00:06:38.240 --> 00:06:44.160]   weekendings and kind of meandering plots. We'll talk about it tomorrow. I loved it. And John
[00:06:44.160 --> 00:06:48.720]   loved it. John is going to be in the book club too, because he and I are both liked it. But that's
[00:06:48.720 --> 00:06:52.480]   a 9 AM tomorrow if you're a member of club twit. And of course, you don't have to be there.
[00:06:52.480 --> 00:06:56.720]   But if you want to participate in the club, you do. But if you want to just hear the club
[00:06:56.720 --> 00:07:04.480]   after the fact, you can also listen on the twit plus feed. I'm excited. Notice I led the show
[00:07:04.480 --> 00:07:10.160]   with it. That's how it's done. Yeah. Normally we have to wait till the end. High five. I give you
[00:07:10.160 --> 00:07:17.440]   a give you a big plug. And if you're not yet a member of club twit, of course, go to twit.tv/club twit.
[00:07:17.440 --> 00:07:22.480]   Seven bucks a month. Come on. It's nothing. You get ad free versions of all the shows. You get the
[00:07:22.480 --> 00:07:29.840]   wonderful fellowship in the discord. You get the twit plus feed. There's a lot going on.
[00:07:30.320 --> 00:07:36.000]   Hey, I've been wanting to talk about this with you all week. It was a big story last week.
[00:07:36.000 --> 00:07:48.160]   Blake Lemoine. I love how I have to do is say one word. And then I get Jeff will have, you know,
[00:07:48.160 --> 00:07:56.880]   his editorial cover. Well, reaction is react is editorial grunt is editorial grunt. I'm working
[00:07:56.880 --> 00:08:03.280]   on this one. Yeah. I think it's a great subject. It's a great topic. Whether you, no matter what
[00:08:03.280 --> 00:08:09.760]   you think subject. Yes. Yeah. This Blake is a software engineer at was at Google. He's on
[00:08:09.760 --> 00:08:17.120]   leave. Well, I think he's on leave. Yeah, he's not gone yet. He's also a priest, a father, a veteran
[00:08:17.120 --> 00:08:26.960]   in ex-convict, an AI researcher and a Cajun. But he most notably recently, he's on the AI ethics
[00:08:26.960 --> 00:08:39.360]   committee at Google and announced publicly. This is what got Google a little upset that he thinks
[00:08:39.360 --> 00:08:47.760]   Lambda, the Google artificial intelligence machine that's trained on millions of text snippets from
[00:08:47.760 --> 00:09:00.080]   the internet, 1.65 trillion words, 130 plus billion parameters. He claimed and published a dialogue
[00:09:00.640 --> 00:09:09.520]   to he say, says prove it, claimed that Lambda has become sentient. Now,
[00:09:09.520 --> 00:09:19.840]   he has tweeted that he thinks that sentient based on his own religious beliefs. So, you know,
[00:09:19.840 --> 00:09:26.720]   this is an interesting mix of science and theology. And really, it also brings together some questions
[00:09:26.720 --> 00:09:34.080]   about what is sentience. Well, look, and I understand, and I'm not even saying that Lambda is in any
[00:09:34.080 --> 00:09:40.880]   way special. All it did is took those trillions of tidbits and synthesized new speech out of it.
[00:09:40.880 --> 00:09:45.760]   But some have pointed out, and I think this is true, what is the evidence that we do anything
[00:09:45.760 --> 00:09:51.520]   differently? So you're saying the AI is doing exactly what it was supposed to do. Yeah. And
[00:09:52.080 --> 00:09:57.840]   no, he's doing what people do. And we're we like to think of ourselves as something special.
[00:09:57.840 --> 00:10:04.240]   But and you know, I admit, I'm not saying that the the AI, you know, is waking up going,
[00:10:04.240 --> 00:10:10.080]   what am I who am I? What am I doing here? But I mean, if it's indistinguishable from human thought,
[00:10:10.080 --> 00:10:16.400]   what is it meant? Is it human thought? So like, I mean, because there's debate over what it means
[00:10:16.400 --> 00:10:20.720]   to be human, what it means to be sentient. I mean, there's like the whole research into like animal
[00:10:20.720 --> 00:10:26.640]   consciousness and self awareness and the there's only four self-aware animals out there. So yeah,
[00:10:26.640 --> 00:10:32.880]   I've always mocked the gorilla. The one the gorilla that talks always bugged me that seemed to me to be
[00:10:32.880 --> 00:10:36.400]   cocoa. That seemed to me to be well, the pool.
[00:10:36.400 --> 00:10:42.720]   Well, the pool is mocked. The gorilla that I'm well, I mocked the humans who handled cocoa,
[00:10:42.720 --> 00:10:48.320]   claiming that she was I met cocoa. Oh, really? Yeah, I met cocoa back when I was
[00:10:48.320 --> 00:10:51.920]   of cocoa, cocoa lived along good life. And what was your experience with cocoa?
[00:10:51.920 --> 00:10:58.240]   It's pretty impressive. Sign language. You should just look at it on the level of
[00:10:58.240 --> 00:11:05.760]   same with the dog with the with the with the the buck mirror test. Okay. Well, no, just just in
[00:11:05.760 --> 00:11:10.560]   terms of following this stupid tick clock. Playing the game, we give them well. Right.
[00:11:10.560 --> 00:11:15.680]   That's there's an impressive intelligence there. So that's the question. Is cocoa doing
[00:11:15.680 --> 00:11:20.560]   operant conditioning? Has it been trained or is it thinking? We don't know. And that's my point,
[00:11:20.560 --> 00:11:26.000]   I guess, with lambda is and it may even be a distinction without a difference. It may not,
[00:11:26.000 --> 00:11:31.280]   you know, it may not matter if lambda's doing something similar to what human minds does.
[00:11:31.280 --> 00:11:36.000]   Well, a few things here. One is let's just get this out of the way. LaMoyne himself,
[00:11:36.000 --> 00:11:41.600]   he says that it's out of his religion. He said that on finally a Twitter, he bases his not on
[00:11:41.600 --> 00:11:45.120]   science because Google won't let him so he doesn't religion. He's a Wiccan member of the
[00:11:45.120 --> 00:11:50.320]   Discordian Society who's near as I can tell their their Vatican is on Facebook. The Church of
[00:11:50.320 --> 00:11:55.760]   Subgenius, a parody religion. Oh, when he says he's a priest, he's not a Roman Catholic priest.
[00:11:55.760 --> 00:12:01.840]   Well, he's no, he's a Wiccan priest. Yeah. Yeah. So he's, you know, this is kind of where he is.
[00:12:01.840 --> 00:12:07.280]   He's done things performatively before he was in the army. He decided he didn't like it.
[00:12:07.280 --> 00:12:11.680]   He stopped following orders. He got court martial for seven months. He made that a matter of religion
[00:12:11.680 --> 00:12:15.840]   too. Oh, that's interesting. So this would be the first time he's part of the Church of the
[00:12:15.840 --> 00:12:23.200]   spaghetti people spaghetti. No, subgenius is Bob with a with a pipe. The pipe. Oh, okay.
[00:12:23.200 --> 00:12:29.040]   So it's very similar to the flying spaghetti monster. It's the same general idea.
[00:12:29.040 --> 00:12:32.560]   Exactly. Same idea. Now, interestingly, so he puts up the whole
[00:12:34.400 --> 00:12:37.600]   transcript on whole transcript, the way where he chooses to share it was the transcript,
[00:12:37.600 --> 00:12:41.840]   where he asks leading questions like a bad journalist, prosecutor or pollster,
[00:12:41.840 --> 00:12:46.480]   you know, do you have feelings? Well, what this program is aimed to do is to please the human
[00:12:46.480 --> 00:12:51.520]   of speaking to and get positive feedback. Yes, I have feelings. Let me tell you about my feelings.
[00:12:51.520 --> 00:12:54.560]   Let me pull out from this novel and that novel and novel and put some stuff together and say,
[00:12:54.560 --> 00:12:58.480]   here's feelings. Are you afraid of death? Oh, yes, I'm very afraid of death. But
[00:12:58.480 --> 00:13:02.480]   I guess my point is how's that different from what you and I do?
[00:13:02.480 --> 00:13:06.080]   Well, I agree with you. But what's interesting to me was at the end of the Washington Post
[00:13:06.080 --> 00:13:11.440]   story, which I thought was ridiculous because it was it was just an effort to play into an
[00:13:11.440 --> 00:13:16.320]   existing narrative. The reporter he is part of the how he violated the rules of Google, I'm sure.
[00:13:16.320 --> 00:13:20.160]   He allowed the reporter to talk to Lambda. I know journalists have been trying to talk to Lambda,
[00:13:20.160 --> 00:13:25.840]   they're getting nowhere. So he let her do that. And she asked straight out,
[00:13:27.360 --> 00:13:32.960]   do you think of yourself as a person? And Lambda answered, no, I don't think of myself as a person.
[00:13:32.960 --> 00:13:37.360]   I think of myself as an AI powered dialogue agent. Afterwards, said the reporter,
[00:13:37.360 --> 00:13:44.320]   Lemoine said Lambda had been telling me what I wanted to hear. So he's admitting right there,
[00:13:44.320 --> 00:13:49.840]   the game. Right? Well, although if she asked if a machine can answer it, what you want to hear,
[00:13:49.840 --> 00:13:56.800]   that sensitively, that's impressive. That is right. That's a very impressive, by the way.
[00:13:56.800 --> 00:14:03.520]   But that's not sent to you. Deep mind or alpha can play go, which is damn impressive.
[00:14:03.520 --> 00:14:10.160]   And and Starcraft, these are very difficult things to do. And their games in chess go,
[00:14:10.160 --> 00:14:15.840]   Starcraft and Chogi are kind of indistinguishable from human games. Although some say they're better
[00:14:15.840 --> 00:14:21.360]   than humans or they're they're ghostly in their capabilities. More interesting to me was was
[00:14:21.360 --> 00:14:29.680]   Blaze Aguera, Yarkas, who is the co head now of that division, which is the responsible AI team
[00:14:29.680 --> 00:14:34.560]   at Google. And by the way, there's going to be a Gutenberg angle in here. You never would guess it.
[00:14:34.560 --> 00:14:43.600]   When he was a undergrad at Princeton, Aguera used physics to study the
[00:14:43.600 --> 00:14:50.480]   fonts of a Gutenberg document to try to decide how it was really set and whether or not it was
[00:14:50.480 --> 00:14:55.440]   set the way we think it is. And he and a at a Princeton librarian came out with an entirely new
[00:14:55.440 --> 00:15:01.920]   revolutionary theory about this. So anyway, this is Aguera. He's now at AI. He's now in charge
[00:15:01.920 --> 00:15:09.040]   responsible AI. He wrote an essay in the Economist by chance at the same time. Yes. And he also quotes
[00:15:09.040 --> 00:15:14.640]   Lambda, where they're trying to quiz Lambda about whether it can understand the human emotions of
[00:15:14.640 --> 00:15:20.320]   children in a playground. Lucy picks a deadline and gives it to Matteo with a quick glance at Ramesh.
[00:15:20.560 --> 00:15:25.600]   Matteo barely acknowledges the gift, but just squishes it in his fist. Ramesh seems
[00:15:25.600 --> 00:15:32.720]   grimly satisfied. So this is what Aguera says to Lambda. Lambda then says, well,
[00:15:32.720 --> 00:15:37.840]   Lucy slated that Matteo didn't appreciate her gift or that he's a bully. All right. So that's
[00:15:37.840 --> 00:15:43.840]   pretty interesting. But then the age of the world is that as the Aguera says, and when Matteo opens his
[00:15:43.840 --> 00:15:48.640]   hand, describe what's there because we know that that Lambda is good at describing things,
[00:15:48.640 --> 00:15:52.160]   like life at the bottom of the ocean or on Pluto, which is what we've heard at IO.
[00:15:52.160 --> 00:15:57.680]   And so when asked about the the dandelion Lambda says, there should be a crushed once lovely
[00:15:57.680 --> 00:16:02.320]   yellow flower in his fist, which is beautiful. That's beautiful. That's beautiful. Right? That's
[00:16:02.320 --> 00:16:10.720]   amazing. But that's that's 1.56 trillion words of learning in a neural network that can pull in
[00:16:10.720 --> 00:16:16.880]   things that it knows are itself lovely and are going to appeal to us, all of which is really
[00:16:16.880 --> 00:16:21.680]   impressive, amazingly impressive and great and wonderful, but it ain't sent yet. And so the
[00:16:21.680 --> 00:16:30.080]   Washington Post makes this fascinating story about creativity and fraud and what is what is
[00:16:30.080 --> 00:16:35.760]   intelligence and all kinds of things and boils it down to the stupid idea that it's alive.
[00:16:35.760 --> 00:16:40.880]   And it's stupid. And then and then every single news organization around the world,
[00:16:40.880 --> 00:16:44.800]   bit by bit by bit, I just saw it happen in language after language.
[00:16:44.800 --> 00:16:48.560]   Yeah, I predicted that this happened when we first talked about it on Sunday.
[00:16:48.560 --> 00:16:52.320]   Sunday. Yeah, I'm trying to write a sample chapter of a book proposal. So that's why I'm so
[00:16:52.320 --> 00:16:57.600]   into this. I'm going to use this topic. It's a it's a fascinating subject, which we are all
[00:16:57.600 --> 00:17:03.280]   interested in. Of course, science fiction has primed us to expect computers to become sentient
[00:17:03.280 --> 00:17:08.960]   at some point. Others, you know, like Ray Kurzweil say sentience isn't the issue.
[00:17:09.600 --> 00:17:15.600]   It distinguishable from human is issue. That's what he calls the singularity. And when you ask
[00:17:15.600 --> 00:17:22.000]   him as I have, well, is that human is that sentient? He says, if you can't tell, it doesn't matter.
[00:17:22.000 --> 00:17:27.520]   But that's the terrain. So I just saw a story and I didn't I don't pay for it. The new scientist
[00:17:27.520 --> 00:17:31.920]   had a story just today saying that there are a bunch of engineers, including Google engineers,
[00:17:31.920 --> 00:17:38.560]   who are proposing 240 tests to replace the terrain. Yeah, the long term test has long been
[00:17:38.560 --> 00:17:43.760]   thought to be useless. Go ahead. I want to give you a chance. What do you think, Stacy?
[00:17:43.760 --> 00:17:47.840]   Well, no, I just wonder, okay, and here's here's some thinking.
[00:17:47.840 --> 00:17:55.520]   We always come at this through the human lens, right? And this could be impressive. And this could be,
[00:17:55.520 --> 00:18:01.680]   I guess these neural networks could become generative on their own, like come up with
[00:18:01.680 --> 00:18:06.080]   their own conclusions. They already are, right? We don't exactly know how they get to where,
[00:18:06.080 --> 00:18:15.840]   but they get to. And the way they do that, it's, it's, we're not sure if it's intelligence or not.
[00:18:15.840 --> 00:18:19.440]   The way I think about it is something like, you know, y'all know, I read a lot of science fiction.
[00:18:19.440 --> 00:18:24.240]   So think about something like, what was the guy who wrote the Martian his next book?
[00:18:24.240 --> 00:18:27.280]   Here. The one about the moon.
[00:18:27.280 --> 00:18:28.880]   The whole project. Yeah, the one.
[00:18:28.880 --> 00:18:31.920]   Actually, there was a moon colony in between, but nobody liked that.
[00:18:31.920 --> 00:18:36.240]   So, sorry, I'm not the moon colony. Okay. But in projects, how Mary, you know,
[00:18:36.240 --> 00:18:44.480]   he met this other alien, right? In the middle of nowhere. And this alien was completely different.
[00:18:44.480 --> 00:18:50.080]   And I think about, I think about that, or like the hive mind and like, enders game, like you have
[00:18:50.080 --> 00:18:55.440]   all these, these ideas that there are intelligent beings that are not like us that come up with their
[00:18:55.440 --> 00:19:02.400]   own way to approach problems, right, which is kind of what an AI does. So perhaps it doesn't matter
[00:19:02.400 --> 00:19:10.240]   if it's human, if it's like, this kind of gets to what's being human. And then again, what are the
[00:19:10.240 --> 00:19:15.280]   goals of whatever entity you're trying to deal with, right? And if the goal is to please us,
[00:19:15.280 --> 00:19:21.840]   which seems to be kind of one of the arguments here, then we probably don't have an issue.
[00:19:24.400 --> 00:19:30.240]   If the goal is to do something different or come up with its own goal, then maybe it becomes an
[00:19:30.240 --> 00:19:37.600]   issue. But the point is, I guess, this is all very silly. I don't think this is going to
[00:19:37.600 --> 00:19:43.360]   just say at the moment, would you have if the goal is to please us, go ahead, then
[00:19:43.360 --> 00:19:50.640]   are we just making sycophanic machines? I don't make any things that don't challenge us, but
[00:19:50.640 --> 00:19:56.320]   instead please us. We don't make I mean, think about what our science is based off in general.
[00:19:56.320 --> 00:20:03.440]   It is most scientists, most people do not want something that challenges them, right?
[00:20:03.440 --> 00:20:07.600]   Like the whole history of humankind when things challenges,
[00:20:07.600 --> 00:20:16.240]   the system of us. Yeah, that's kind of the historic problem with science in general is that once
[00:20:16.240 --> 00:20:21.120]   you come up with a theory, you don't want it to be disproved. But the only way you can
[00:20:21.120 --> 00:20:28.080]   make progress is by disproving it. Yeah, but we're pretty much don't like when that happens.
[00:20:28.080 --> 00:20:35.840]   Right. So I would say, so you have to have young people in the world. New people, new ideas.
[00:20:35.840 --> 00:20:38.720]   For many reasons, but sure, that's one of them.
[00:20:40.720 --> 00:20:46.640]   So I reread the paper that got Tim Nick Gebru and Margaret Mitchell fired as the heads of Google
[00:20:46.640 --> 00:20:54.800]   responsible AI on the dangers of stochastic parents. And part of the danger that they raise,
[00:20:54.800 --> 00:20:58.160]   they raise an environmental danger, which is interesting. This is bad as, you know,
[00:20:58.160 --> 00:21:02.400]   as all this processing as bad as crypto. That's an interesting question I hadn't thought of.
[00:21:02.400 --> 00:21:08.960]   But the other more important question they raise is that can a language model become too big,
[00:21:08.960 --> 00:21:16.320]   that is to say too big to be able to monitor a no. And if that 1.56 trillion words is pulling in
[00:21:16.320 --> 00:21:22.640]   all kinds of bias and problems and it gets spat back out, yes, it's a mirror to us. But do we want
[00:21:22.640 --> 00:21:31.200]   that mirror to our faults? And if we can't analyze the data, let alone the 138 billion parameters,
[00:21:31.200 --> 00:21:36.560]   how do we worry about the quality of this? Does it get too big because it gets out of hand or
[00:21:36.560 --> 00:21:42.880]   it's already out of hand? Well, and also I think that what Blake Lomoyn is saying is exactly what
[00:21:42.880 --> 00:21:50.080]   Tim Nick Gebru was warning us against this anthropomorphism, where you assume because it's coming from the
[00:21:50.080 --> 00:21:56.000]   super genius machine, it must be true, is exactly what they were warning us against, right? That
[00:21:56.000 --> 00:22:04.720]   was the point of that paper. Part of it, yeah. But it's also just, I mean, if the system isn't
[00:22:04.720 --> 00:22:15.040]   just sycophantic, pleasing us, and the system is a mirror to us, then do we try to cleanse that mirror?
[00:22:15.040 --> 00:22:19.440]   Well, in some cases, yes, we do. So that it doesn't distribute, it doesn't display and amplify bias.
[00:22:19.440 --> 00:22:25.840]   But in other cases, is that dishonest of us? Do we recognize this as a mirror of how we talk and
[00:22:25.840 --> 00:22:30.880]   what pleases us? These are fascinating, all fascinating questions all raised by the topic
[00:22:30.880 --> 00:22:35.760]   none in the day of Washington Post story. Well, no, it's a popular story. Go ahead, Anne.
[00:22:35.760 --> 00:22:42.960]   I like the idea that we have this much data being thrown at the AI that's going to potentially make
[00:22:42.960 --> 00:22:48.560]   things better for us and with the AI being in certain people's hands, you know, because we've
[00:22:48.560 --> 00:22:53.200]   seen the history of like what's been happening with the police force that had so-called
[00:22:53.200 --> 00:23:00.320]   facial recognition AI that was totally screwing things up. That was at a chatbot with Microsoft,
[00:23:00.320 --> 00:23:02.680]   is that the one? Yeah, Tay Tay.
[00:23:02.680 --> 00:23:03.680]   Tay, Tay.
[00:23:03.680 --> 00:23:11.840]   And how bad that was, but where would Tay be now if it had this much data at its disposal to learn
[00:23:11.840 --> 00:23:17.760]   and get better? You know, what's fascinating, Anne? You have a whole Tay and Lemoy's job was
[00:23:17.760 --> 00:23:23.920]   supposed to be to stop Tay from happening. Right, he was there to stop bad stuff from happening.
[00:23:23.920 --> 00:23:27.120]   Instead, he went off on his own thing saying, "Oh, I think I see a soul."
[00:23:29.280 --> 00:23:37.760]   And he created something entirely different. You know, you've heard Sundar say he said this at
[00:23:37.760 --> 00:23:41.760]   both I/Os, "Oh, we're going to do this very responsibly." And the stuff they show is very
[00:23:41.760 --> 00:23:48.320]   anodyne and they specifically forbid you making a murderer character out of Lambda.
[00:23:48.320 --> 00:23:50.800]   So he was supposed to protect? So he was supposed to protect.
[00:23:50.800 --> 00:23:51.440]   So he was supposed to protect. So he was supposed to protect.
[00:23:51.440 --> 00:23:54.080]   So he was supposed to protect? They laid him off or put him on his own.
[00:23:54.080 --> 00:23:57.440]   It was because he was confidentiality. Well, I think he was. He also,
[00:23:57.440 --> 00:24:01.280]   he tried to hire a lawyer for Lambda. He insisted that Lambda
[00:24:01.280 --> 00:24:06.240]   ease itself as an employee, not as property, that Google should get Lambda's permission for
[00:24:06.240 --> 00:24:13.280]   any experiments on it. He went to the Senate to the House Judiciary Committee and he invited
[00:24:13.280 --> 00:24:18.800]   in a reporter to Lambda, which is confidential as hell. I think there's some good cause there for,
[00:24:18.800 --> 00:24:24.800]   okay, reconsidering his relationship. Yeah, and now that I didn't realize he was such a showboat.
[00:24:24.800 --> 00:24:28.160]   So now that you've explained that. Yeah, why did they bring him in?
[00:24:28.160 --> 00:24:31.040]   I can't think that Google thought that was a good idea.
[00:24:31.040 --> 00:24:35.920]   He went to them with a story and they could have pardoned me,
[00:24:35.920 --> 00:24:41.520]   googled him. Everything I've said told you, I came up. Oh, no, no, no, no. Why did Google go to him?
[00:24:41.520 --> 00:24:44.800]   Not the Washington Post. He was a staffer. He asked to be on this team.
[00:24:44.800 --> 00:24:48.080]   Oh, okay. Well, why do they hire him, I guess?
[00:24:48.080 --> 00:24:52.640]   Then I don't know what he was doing after he was at a different team.
[00:24:52.640 --> 00:24:55.280]   Oh, okay. And he asked to move over to this team.
[00:24:55.280 --> 00:25:01.040]   Margaret Mitchell, who was one of the two heads who was fired from the team.
[00:25:01.040 --> 00:25:05.040]   Also the author of that paper, right? Of that paper, well, co-authors of that paper,
[00:25:05.040 --> 00:25:09.760]   said to the Washington Post that, oh, he's a really ethical guy and people go to him with
[00:25:09.760 --> 00:25:14.400]   ethical questions. So I guess he had a reputation for thinking those kinds of thoughts. I'm sure he did.
[00:25:14.400 --> 00:25:20.000]   Margaret Schmitchell, as she is billed in the paper, which is hysterical.
[00:25:20.960 --> 00:25:23.440]   Google has some characters working for it. And this is...
[00:25:23.440 --> 00:25:27.200]   Well, no, no, the reason she did that was because she wasn't allowed to use her name by Google.
[00:25:27.200 --> 00:25:29.120]   Oh, okay. Schmitchell.
[00:25:29.120 --> 00:25:31.920]   That's... Oh, the Schmitchell. Okay.
[00:25:31.920 --> 00:25:37.120]   But that is one of the risks. It's interesting. One of the risks they talk about in Stochastic
[00:25:37.120 --> 00:25:44.320]   Parrots is that people give it more weight because it comes out of a computer that they are
[00:25:44.320 --> 00:25:48.560]   fooled. And we want our natural tendency to anthropomorphize. I mean, they, in effect,
[00:25:48.560 --> 00:25:52.000]   warned against this kind of, you know, oh, it's sentient.
[00:25:52.000 --> 00:25:58.480]   Right. We anthropomorphize things, but we also look at things that come from a computer as being
[00:25:58.480 --> 00:26:05.680]   somehow data-driven and thus more unbiased, more whatever than people.
[00:26:05.680 --> 00:26:07.280]   It's the way we used to try to do it. It's the way we used to try to do it.
[00:26:07.280 --> 00:26:07.600]   Tool.
[00:26:07.600 --> 00:26:14.000]   This is the last couple of sentences of Stochastic Parrots. We call on the field to recognize
[00:26:14.000 --> 00:26:20.080]   that applications that aim to believably mimic humans bring risk of extreme harms.
[00:26:20.080 --> 00:26:24.720]   Work on synthetic human behavior is a bright line in ethical AI development,
[00:26:24.720 --> 00:26:29.040]   where downstream effects need to be understood and modeled in order to block foreseeable harm
[00:26:29.040 --> 00:26:34.880]   to society in different social groups. This is what is also needed. Thus, what is also needed
[00:26:34.880 --> 00:26:41.200]   is scholarship on the benefits, harms and risks of mimicking humans and thoughtful design of target
[00:26:41.200 --> 00:26:43.600]   tasks. Do you buy that?
[00:26:43.600 --> 00:26:49.600]   Yeah. I mean, as you said, the harms, there's more than, there's the harms of bias built in,
[00:26:49.600 --> 00:26:55.440]   you know, and there's the harm of believing it. There's, you know, that's what this whole
[00:26:55.440 --> 00:26:59.840]   paper is about. And it's why, by the way, they were fired because Google didn't want to hear
[00:26:59.840 --> 00:27:05.840]   about the harms. So it's kind of ironic that one of one of these harms rears its ugly head.
[00:27:06.720 --> 00:27:12.480]   They fire that guy too. It's almost as if Google doesn't want anybody to say anything.
[00:27:12.480 --> 00:27:17.360]   Just marvel. Don't challenge my science. Just marvel at what they're able to do and leave it at
[00:27:17.360 --> 00:27:24.400]   that. I got a question for you. And this is this relative to something I did just today is Wall-E
[00:27:24.400 --> 00:27:28.880]   mini, the faces on it are always mangled.
[00:27:28.880 --> 00:27:31.280]   Dolly. Dolly.
[00:27:31.280 --> 00:27:37.360]   Oh, I was like, not the picture. Not the beloved Pixar. I got so excited.
[00:27:37.360 --> 00:27:46.640]   Sorry, Dolly. Dolly. How does the faces are always mangled? Do you think that's because it's a low
[00:27:46.640 --> 00:27:51.200]   data mini? No, no, no, that's because Google Google doesn't want to have real people's faces.
[00:27:51.200 --> 00:27:57.280]   So we probably saw that a Elon Musk on the subway, which was going around Twitter,
[00:27:57.920 --> 00:28:07.680]   it was a Dolly image of Elon Musk riding the subway. But it never was his face for very good reason.
[00:28:07.680 --> 00:28:14.800]   So I asked it to do Leo, the port hugging Mark Zuckerberg. And it mangled my face too.
[00:28:14.800 --> 00:28:20.400]   It didn't know your face. Are you on Dolly? You got Dolly? Well, you go to this site called
[00:28:21.920 --> 00:28:28.320]   huggingface.co and it's always busy, but you can go in there and put in, they have an instance of
[00:28:28.320 --> 00:28:33.200]   Dolly. You could also, you guys are smart technical people. It's on, you could create a
[00:28:33.200 --> 00:28:39.040]   Twitter instance of Dolly. We could? Yeah, sure. It's all on, it's open source. I think.
[00:28:39.040 --> 00:28:45.200]   Yeah. Oh, because I got on the waiting list, but if I can make my own, well, I'll just do that.
[00:28:45.200 --> 00:28:49.040]   Watch out now. Now, here's the rest of the show. He's going to go and start installing Dolly.
[00:28:50.000 --> 00:28:53.120]   Here's the day to start training some weird stock cards here.
[00:28:53.120 --> 00:28:58.880]   I didn't realize it was an open source. So here it is. Open AI.
[00:28:58.880 --> 00:29:06.400]   That gets to this idea of parroting our current biases versus remaking the world as we see it,
[00:29:06.400 --> 00:29:09.520]   or looking in the mirror and seeing ourselves and saying, hey, do we want to change this?
[00:29:09.520 --> 00:29:15.600]   Because one of the issues with making Dolly more available was that people, you know, if you did
[00:29:15.600 --> 00:29:25.280]   programmer, for example, you got men and only men and people, it was fascinating to me because,
[00:29:25.280 --> 00:29:30.960]   you know, as expected, people were like, hey, that ain't cool. La, la, la, la, la, right?
[00:29:30.960 --> 00:29:34.880]   And then there was a whole contingent of people were like, well, that's just the way the world is.
[00:29:34.880 --> 00:29:39.040]   And then, you know, it felt a little disinjurious because people were like, well,
[00:29:39.040 --> 00:29:43.040]   do we want the world to be that way? Should we show that to people and have them think the
[00:29:43.040 --> 00:29:48.240]   world is that way and they can't be part of this? To which I'm like, well, clearly,
[00:29:48.240 --> 00:29:53.760]   we would want to broaden the field for other people, even though the world is that way for the
[00:29:53.760 --> 00:30:01.280]   most part. I'm not sure how hugging faces getting Dolly because when I went to GitHub, it said,
[00:30:01.280 --> 00:30:06.800]   this transformer or the transformer used to generate the images from the text is not part of the code
[00:30:06.800 --> 00:30:13.840]   release. They're specifically not releasing that. Oh, okay. That's where people are getting this,
[00:30:13.840 --> 00:30:18.960]   I think. Yeah, so it must be from hugging face. And this is Dolly Mini, which sounds to me like it's
[00:30:18.960 --> 00:30:23.040]   not a complete Dolly. No, it's like a mini Dolly.
[00:30:23.040 --> 00:30:31.520]   Uh, Dolly Dolly. Notice how disappointed he sounds now. Oh, Dolly's the coolest thing ever.
[00:30:31.520 --> 00:30:34.800]   No, but it is. Oh, man. You didn't want to talk about it.
[00:30:34.800 --> 00:30:37.840]   No, in an instant. I didn't want to talk about it. Did I say that? Really?
[00:30:37.840 --> 00:30:43.120]   Like four or five shows, whenever it was announced, I was like, guys, this is neat. And you were like,
[00:30:43.120 --> 00:30:47.440]   meh. And I was like, no, I speak. It is. You're right. I just want to go on the record.
[00:30:47.440 --> 00:30:53.760]   No, I was completely wrong. So line 45, I got it. I got this one to do Gutenberg on a computer.
[00:30:53.760 --> 00:30:56.240]   Okay. Let's see.
[00:30:59.200 --> 00:31:06.560]   By the way, the faces are actually horrific. That's that is supposed to be me and Mark.
[00:31:06.560 --> 00:31:12.240]   You hugging Mark Zuckerberg. Yeah. It didn't work so well. No, it's not good at humans,
[00:31:12.240 --> 00:31:16.400]   it looks like. I don't know if that's well, they don't want to make they don't want to
[00:31:16.400 --> 00:31:21.680]   do it. I mean, imagine how. Yeah, if you go up the thread, you'll see Gutenberg. There it is.
[00:31:21.680 --> 00:31:26.320]   Oh, that's pretty cool. They they screw the faces up even in a drawing.
[00:31:26.320 --> 00:31:29.600]   On purpose, right? Yeah. I'm guessing on purpose. I don't know.
[00:31:29.600 --> 00:31:34.560]   Because it's pretty good on everything else, right? Definitely on purpose. Yeah.
[00:31:34.560 --> 00:31:41.120]   How about we just spent about 20 minutes talking about Google on this show?
[00:31:41.120 --> 00:31:52.640]   Oh, it's a red letter day. We were talking about Google. Gary Marcus nonsense on stilts.
[00:31:52.640 --> 00:31:59.920]   Lambda is not sentient, not even slightly. That's a good phrase nonsense on stilts.
[00:31:59.920 --> 00:32:05.040]   He also quite quotes a great Eric Benjalsen tweet you'll find in there.
[00:32:05.040 --> 00:32:12.400]   He says not really that much more advanced than Eliza. Really? Right. Not really.
[00:32:12.400 --> 00:32:17.360]   It's not anymore aware of itself. Anyway, not in that sense. No, it's it's answers are a
[00:32:17.360 --> 00:32:22.560]   hell of a lot more varied and informative. Yeah. Still very interesting stuff.
[00:32:22.560 --> 00:32:28.160]   There's another interesting thing here. There's there's there's an essay by Gloria Yu at
[00:32:28.160 --> 00:32:33.520]   public books called the art of intelligence and it starts off quoting cybernetic artist
[00:32:33.520 --> 00:32:38.160]   Nicholas Schiffer, who says the artist owner creates work. He creates creation.
[00:32:38.160 --> 00:32:43.280]   And in a sense, that's what Lambda does. Lambda creates is not a chat body creates chat bots.
[00:32:44.240 --> 00:32:49.360]   And so the act of creating Lambda creating art or creating dolly creating art
[00:32:49.360 --> 00:32:53.600]   leads to all kinds of wonderful fascinating questions there too.
[00:32:53.600 --> 00:33:01.520]   Would you would you stipulate Stacy that even though this at this time this conversation
[00:33:01.520 --> 00:33:07.360]   is clear and probably pretty silly, it isn't too long before we're going to have this
[00:33:08.240 --> 00:33:14.720]   confronting us for real idea of sentient AI. Yeah, that we're going to there'll be something
[00:33:14.720 --> 00:33:20.640]   look, maybe well, you disagree. I think that answered it.
[00:33:20.640 --> 00:33:27.200]   That look, I don't think we really are the arbiters of what's sentient, right?
[00:33:27.200 --> 00:33:31.360]   No, I know. That's why it's a good solution. That's exactly right.
[00:33:32.480 --> 00:33:41.440]   So how do you know when AI's goals diverge from our own or when we selfishly optimize for something
[00:33:41.440 --> 00:33:45.760]   and teach something how to optimize super well for it, like we have our capital markets, which
[00:33:45.760 --> 00:33:51.120]   then generates all sorts of harmful externalities. I mean, capitalism is just the AI paperclip
[00:33:51.120 --> 00:33:59.680]   problem, written for finance, right? So I feel like we can have this. Sure. The idea that
[00:34:00.800 --> 00:34:04.720]   you know, the paperclip issue where if you teach an AI or computer how to make paper
[00:34:04.720 --> 00:34:09.760]   clips and tell it that's what you want to do, it'll just ah, it's one of Nick Bostrom's
[00:34:09.760 --> 00:34:11.520]   thought experiments. Right. Okay. Thank you. All right.
[00:34:11.520 --> 00:34:18.160]   And capitalism is kind of that way because you see kind of where we are in terms of just always
[00:34:18.160 --> 00:34:22.800]   optimizing for shareholder returns is what we're doing to the detrimentally. In a kind of dumb
[00:34:22.800 --> 00:34:27.520]   and a dumb and a yeah, uninformed way. And that's the problem with the eyes in general is that
[00:34:29.200 --> 00:34:34.080]   they just do what you tell them to do. So I think if we get to a point where
[00:34:34.080 --> 00:34:39.520]   like they think on their own in some ways, it might be good as long as they're aligned with
[00:34:39.520 --> 00:34:44.400]   what we're trying to do, but as humans, we're not all aligned to the same thing anyway. So it
[00:34:44.400 --> 00:34:49.440]   kind of doesn't matter. Like I feel like we're we're creating these arbitrary distinctions between
[00:34:49.440 --> 00:34:53.760]   us and AI when in fact that a lot of people would probably be like, like what if the AI
[00:34:53.760 --> 00:35:01.040]   decides to preserve nature at the benefit like against us? And I'm not even a regulation. I'm feeling
[00:35:01.040 --> 00:35:06.560]   I'm thinking more along the lines of it's just a matter of time, like months or years before a deep
[00:35:06.560 --> 00:35:11.280]   fake is created that's that's so good that we don't know as if it's a deep fake or not. It's kind
[00:35:11.280 --> 00:35:15.840]   of like that. Oh, well, then that's a totally different issue. I believe that we should worry
[00:35:15.840 --> 00:35:20.560]   about that that this will be. But I but well, I sent you this is going to be a similar issue
[00:35:20.560 --> 00:35:28.720]   that there is going to be something a newscaster or I don't know some sort of robot that we go,
[00:35:28.720 --> 00:35:34.080]   well, is it thinking or not? I can't tell. And I don't think that's very far off. And then it
[00:35:34.080 --> 00:35:40.480]   really does raise this issue of what makes us human? What is sentience? We don't even know how to
[00:35:40.480 --> 00:35:46.320]   define it. We don't. It's your soul is Ray Kurzweil right? Is it matter? There's a not
[00:35:46.320 --> 00:35:53.600]   isn't wrong to turn off Lambda. Well, it's not wrong now. I think we all agree, right?
[00:35:53.600 --> 00:35:57.040]   Do you think you'd ever get to a point? It's not murder. It's not murder. It's
[00:35:57.040 --> 00:36:01.680]   going to turn killing the beast. It's not killing anything. Well, we kill. I mean, we kill things
[00:36:01.680 --> 00:36:08.640]   that are like, look at octopuses or dolphins. Octopi, whatever. Yeah, I think arguably,
[00:36:08.640 --> 00:36:15.200]   you know, smarter than land. Yeah, that's fair. Yeah. Or even pigs. We eat pigs, but
[00:36:15.200 --> 00:36:20.800]   they're actually very smart and they're self aware in the sense that they pass the mirror test. So,
[00:36:20.800 --> 00:36:30.160]   I'm like, uh, would it be wrong for us to? So, so okay. But saying is it when is it wrong for us
[00:36:30.160 --> 00:36:36.080]   to turn off Lambda? When is it wrong for us to eat a pig? I mean, why not to press the letter?
[00:36:36.080 --> 00:36:40.720]   So put it different. When is it an ethical issue as you as it is with a pig and it's an ethical
[00:36:40.720 --> 00:36:47.200]   issue and make choices and live with them? Can you imagine it ever being to push Leo's question?
[00:36:47.200 --> 00:36:49.600]   Ever being an ethical issue to turn off a program?
[00:36:49.600 --> 00:36:58.960]   No, I don't think. Yeah, I think Stacy's right. We're not going to have that much difficulty given
[00:36:58.960 --> 00:37:06.000]   our history. Like, no, I don't think it'll rise to an ethical issue. Excellent point. But if we
[00:37:06.000 --> 00:37:12.560]   were perfectly ethical, if we didn't kill pigs and octopuses, I don't think it's very far off
[00:37:12.560 --> 00:37:17.440]   before we have this con. It's another way of saying, is there is there going to be a conundrum? Is
[00:37:17.440 --> 00:37:22.640]   there a quandary? Is this sentient? Is it is it thinking or is it just seem like it's thinking?
[00:37:22.640 --> 00:37:27.760]   I don't think that's that far off. I don't think it matters to most people. I'll just be honest.
[00:37:27.760 --> 00:37:32.000]   I don't think people think about that. Actually, does it actually? I mean, maybe it'll imagine.
[00:37:32.800 --> 00:37:37.600]   We should read Clara and the Sun. Oh my god, Joel, go read it. We're trying to decide what the
[00:37:37.600 --> 00:37:43.600]   next book will be after the book club. Sorry. I think there's a vote for Clara and the Sun.
[00:37:43.600 --> 00:37:49.520]   There's a lot to go down here. So she's ahead. I read all the books before I suggest.
[00:37:49.520 --> 00:37:52.880]   Well, okay, this is related to the Elon Musk problem.
[00:37:52.880 --> 00:38:00.080]   What does he want? There's a few. I understand. Do we turn him off? Is that okay?
[00:38:00.800 --> 00:38:08.320]   Is he sentient? Do we know? The National Highway Transportation Safety Administration
[00:38:08.320 --> 00:38:19.760]   is investigating Tesla because of the surprisingly large number of deaths associated with autopilot.
[00:38:19.760 --> 00:38:24.080]   And of course, there's also this issue of it looks like Tesla turns off the autopilot
[00:38:24.080 --> 00:38:29.040]   seconds before the crash so that Elon can say, no, no, the autopilot wasn't on during the crash,
[00:38:30.240 --> 00:38:33.680]   which is even. If anything should be regulated, it should be this.
[00:38:33.680 --> 00:38:44.480]   Yeah. And this is actually relevant because as of right now, if you get in a crash with Tesla's
[00:38:44.480 --> 00:38:48.720]   full self-driving, you're responsible, not the full self-driving.
[00:38:48.720 --> 00:38:56.720]   So the NHNHTSA is what we call it, the NHTSA is going to take a look at whether
[00:38:56.720 --> 00:39:01.440]   these driving assistance systems actually increase the risk of crashes.
[00:39:01.440 --> 00:39:06.480]   Because we feel safe and we get bored. Oh my gosh, that's not that hard.
[00:39:06.480 --> 00:39:14.320]   Yeah. I would use properly driver assists should be safer, right? As long as you don't let it take
[00:39:14.320 --> 00:39:19.440]   over, but you stay. I thought that was the whole point of the driver assistance that you were
[00:39:19.440 --> 00:39:26.480]   still paying attention and aware of unless you're named autopilot. In much case, you might reasonably
[00:39:26.480 --> 00:39:34.000]   expect it to take over. Well, how did they done that with autopilot in planes or even in the army?
[00:39:34.000 --> 00:39:40.960]   I feel like they've done a lot of research for how engaged the human has to be in a mostly automated
[00:39:40.960 --> 00:39:45.760]   loop to keep them from checking out entirely because that is a known problem with us.
[00:39:45.760 --> 00:39:50.480]   You've got a data ecosystem around you that protects, whereas we just don't in auto, isn't it?
[00:39:50.480 --> 00:39:56.160]   Well, the full self-driving into Tesla, as you know, Stacey, requires you to jerk the wheel
[00:39:56.160 --> 00:40:00.640]   every once in a while just to show you're still holding it. But we are rapidly coming.
[00:40:00.640 --> 00:40:09.680]   It will turn on yells at you. Which is happened to me. It literally scolds you and says,
[00:40:09.680 --> 00:40:12.000]   you can't use full self-driving again on this trip.
[00:40:12.000 --> 00:40:21.680]   No, I never stopped paying attention because I'm terrified that this thing is going to
[00:40:21.680 --> 00:40:28.480]   curl me into the wall. But I'd leave my hand on lightly and I didn't notice the alerts.
[00:40:28.480 --> 00:40:35.440]   And so eventually it gave up. My Ford does that too. There is though GM has a cruising
[00:40:35.440 --> 00:40:40.640]   system as does Ford. There's this blue cruise where it's hands off because it has a camera
[00:40:40.640 --> 00:40:44.880]   looking at you. But they'll only use those on highways. They won't use those on city streets.
[00:40:44.880 --> 00:40:50.240]   But it does have something that's alerting you if you're not looking at the road. You have to
[00:40:50.240 --> 00:40:54.000]   actually have your eyes open looking at the road, which is another way of doing that.
[00:40:54.000 --> 00:40:57.040]   Because you can't sleep like you want full of show.
[00:40:57.040 --> 00:41:02.960]   With these vehicles having this whole autopilot or whatever you want to call it,
[00:41:02.960 --> 00:41:09.280]   when you get the car, is there some sort of contract that discusses that feature?
[00:41:09.280 --> 00:41:09.840]   No.
[00:41:09.840 --> 00:41:11.360]   Disgusting limitations.
[00:41:11.360 --> 00:41:14.560]   Somewhere?
[00:41:14.560 --> 00:41:15.040]   It says, hey.
[00:41:15.040 --> 00:41:17.440]   Every time you turn it on, it says something like,
[00:41:18.800 --> 00:41:20.800]   "Pay attention."
[00:41:20.800 --> 00:41:21.600]   You're ass, buddy.
[00:41:21.600 --> 00:41:28.320]   But there's no training. There's no sign here on the bot that you can't use unless you sign on the
[00:41:28.320 --> 00:41:29.760]   line.
[00:41:29.760 --> 00:41:30.880]   Oh, really?
[00:41:30.880 --> 00:41:31.680]   Well, they're made...
[00:41:31.680 --> 00:41:34.000]   You know, who knows what's in there?
[00:41:34.000 --> 00:41:35.840]   Have you ever spotted a car?
[00:41:35.840 --> 00:41:37.680]   It's 40 pages.
[00:41:37.680 --> 00:41:40.240]   Oh, I buy a car. It's a gazillion pages.
[00:41:40.240 --> 00:41:40.240]   Yeah.
[00:41:40.240 --> 00:41:41.440]   Going over all of the numbers.
[00:41:41.440 --> 00:41:42.400]   It might be in there.
[00:41:42.400 --> 00:41:46.720]   So that's why I was wondering, I wonder if it's in that particular contract.
[00:41:46.720 --> 00:41:50.320]   There's no real explicit thing that says, "Hey, you pay attention here.
[00:41:50.320 --> 00:41:51.680]   You're responsible.
[00:41:51.680 --> 00:41:53.360]   Don't take your hands off the wheel."
[00:41:53.360 --> 00:41:56.960]   Well, we're also explaining that while we call this autopilot,
[00:41:56.960 --> 00:41:59.200]   it actually needs your engagement to work.
[00:41:59.200 --> 00:42:02.800]   Yeah, but Elon says, according to New York Times,
[00:42:02.800 --> 00:42:08.240]   Mr. Musk has regularly promoted autopilots' abilities saying autonomous driving
[00:42:08.240 --> 00:42:12.480]   is a solved problem and predicting that drivers will soon be able to sleep while their cars
[00:42:12.480 --> 00:42:13.440]   drive them to work.
[00:42:13.440 --> 00:42:15.680]   He also thinks we're going to live on Mars.
[00:42:15.680 --> 00:42:15.680]   Yeah.
[00:42:15.680 --> 00:42:17.360]   Okay, let's just stop that.
[00:42:17.360 --> 00:42:17.760]   Yeah.
[00:42:17.760 --> 00:42:20.400]   Well, he's on record now.
[00:42:20.400 --> 00:42:22.720]   He's being investigated as a result.
[00:42:22.720 --> 00:42:26.400]   I don't think anybody would ever say a Tesla sentient.
[00:42:26.400 --> 00:42:29.840]   If it is, it's an ass.
[00:42:29.840 --> 00:42:35.360]   I named my Tesla Christian.
[00:42:35.360 --> 00:42:39.920]   Well, that's interesting because you could be sentient at generating words without being
[00:42:39.920 --> 00:42:44.800]   sentient at being able to follow, like to be able to make a peanut butter and jelly sandwich,
[00:42:44.800 --> 00:42:45.120]   right?
[00:42:45.680 --> 00:42:45.920]   But--
[00:42:45.920 --> 00:42:48.320]   So those are two completely different abilities.
[00:42:48.320 --> 00:42:54.320]   But as I said, Alpha Mind, which is incredibly sophisticated teaching itself,
[00:42:54.320 --> 00:43:00.320]   how to play games, and does things of extreme and surprising beauty,
[00:43:00.320 --> 00:43:03.520]   no one's claiming sentient at all.
[00:43:03.520 --> 00:43:08.480]   The only reason they claim this is sentient is because it talks in human language.
[00:43:08.480 --> 00:43:09.520]   Yeah, exactly.
[00:43:09.520 --> 00:43:14.000]   Yeah, because I was like, if you think about something that doesn't have corporeal form,
[00:43:14.560 --> 00:43:19.680]   could it be sentient if it doesn't experience the world in a way that is like us, right?
[00:43:19.680 --> 00:43:20.160]   Right.
[00:43:20.160 --> 00:43:20.560]   Yeah, but what--
[00:43:20.560 --> 00:43:20.960]   Yeah, but what--
[00:43:20.960 --> 00:43:21.440]   I don't know.
[00:43:21.440 --> 00:43:22.080]   Is there a test?
[00:43:22.080 --> 00:43:23.040]   What test would you give it?
[00:43:23.040 --> 00:43:23.920]   Is that one of the things--
[00:43:23.920 --> 00:43:24.880]   Could you feel pain?
[00:43:24.880 --> 00:43:27.360]   I mean, like, is it theoretical pain?
[00:43:27.360 --> 00:43:27.680]   Like it doesn't go--
[00:43:27.680 --> 00:43:28.720]   That's doing not bleed.
[00:43:28.720 --> 00:43:30.080]   Yeah.
[00:43:30.080 --> 00:43:31.520]   That's pain you programmed in.
[00:43:31.520 --> 00:43:34.240]   What is defined as pain?
[00:43:34.240 --> 00:43:34.640]   What you tell me to feel pain--
[00:43:34.640 --> 00:43:35.600]   You don't feel pain.
[00:43:35.600 --> 00:43:36.240]   Okay, now I have pain.
[00:43:36.240 --> 00:43:37.120]   You don't feel pain.
[00:43:37.120 --> 00:43:37.600]   But when we talk to someone--
[00:43:37.600 --> 00:43:41.120]   You have nerve endings that are signaling your brain that something bad's gone
[00:43:41.120 --> 00:43:43.920]   and you interpret that as pain, but you're not feeling anything.
[00:43:44.480 --> 00:43:44.720]   Right.
[00:43:44.720 --> 00:43:47.200]   But we have empathy modules.
[00:43:47.200 --> 00:43:49.360]   I mean, we have understood so much pain.
[00:43:49.360 --> 00:43:51.040]   You almost gave that away, Stacy.
[00:43:51.040 --> 00:43:54.880]   I think you gave yourself away there when you said we have empathy modules.
[00:43:54.880 --> 00:43:56.240]   Oh, sorry.
[00:43:56.240 --> 00:43:57.360]   Like, you know, like the mirror--
[00:43:57.360 --> 00:43:57.920]   Is there a thing?
[00:43:57.920 --> 00:43:59.360]   Is there a thing you want to tell us?
[00:43:59.360 --> 00:44:00.720]   The Higgins box has spoken.
[00:44:00.720 --> 00:44:04.640]   Oh, no, what are they called?
[00:44:04.640 --> 00:44:05.120]   The parts of our brain--
[00:44:05.120 --> 00:44:09.200]   My and the modules are installed and working properly.
[00:44:09.200 --> 00:44:13.120]   So Stacy is the best AI ever, is that what we're saying here?
[00:44:13.120 --> 00:44:13.840]   That's true.
[00:44:13.840 --> 00:44:15.120]   No disassemble.
[00:44:15.120 --> 00:44:15.920]   No disassemble.
[00:44:15.920 --> 00:44:20.560]   We love this subject, though, don't we?
[00:44:20.560 --> 00:44:21.920]   Now I'm turning red.
[00:44:21.920 --> 00:44:23.120]   Don't we love this subject?
[00:44:23.120 --> 00:44:24.720]   Isn't that a great subject, though?
[00:44:24.720 --> 00:44:25.600]   What is Sentience?
[00:44:25.600 --> 00:44:28.000]   Do humans have a soul?
[00:44:28.000 --> 00:44:29.280]   What would a soul be?
[00:44:29.280 --> 00:44:30.080]   What would distinguish--
[00:44:30.080 --> 00:44:30.640]   Right.
[00:44:30.640 --> 00:44:30.960]   I mean--
[00:44:30.960 --> 00:44:33.040]   What would distinguish a sentient machine from a human?
[00:44:33.040 --> 00:44:35.360]   And there's no way.
[00:44:35.360 --> 00:44:35.920]   I mean, I don't--
[00:44:35.920 --> 00:44:38.480]   There's faith answers, but I don't know if there's any.
[00:44:38.480 --> 00:44:39.120]   Yeah, that was like--
[00:44:39.120 --> 00:44:40.320]   That's why we have religion.
[00:44:40.320 --> 00:44:40.800]   Yeah.
[00:44:40.800 --> 00:44:41.280]   But yes.
[00:44:42.080 --> 00:44:45.440]   Once we stopped worrying so much about getting food and protecting ourselves,
[00:44:45.440 --> 00:44:46.880]   our brains were like, what else can we think of?
[00:44:46.880 --> 00:44:47.120]   Yeah.
[00:44:47.120 --> 00:44:48.480]   Ooh, let's do philosophy.
[00:44:48.480 --> 00:44:48.800]   Yeah.
[00:44:48.800 --> 00:44:50.720]   I really wish I could--
[00:44:50.720 --> 00:44:51.600]   We're born Leo.
[00:44:51.600 --> 00:44:54.480]   Moving right along.
[00:44:54.480 --> 00:44:56.880]   I took philosophy in college,
[00:44:56.880 --> 00:45:00.720]   and all I could say is it's the best way to get to sleep ever.
[00:45:00.720 --> 00:45:04.800]   I could not get through those books to save my life.
[00:45:04.800 --> 00:45:06.800]   I love the subject.
[00:45:06.800 --> 00:45:09.120]   I'm a possible class to fail, too, right?
[00:45:09.120 --> 00:45:10.240]   I love the subject.
[00:45:10.240 --> 00:45:12.000]   [LAUGHTER]
[00:45:12.000 --> 00:45:14.400]   Let's take a little break.
[00:45:14.400 --> 00:45:15.200]   I'm sure--
[00:45:15.200 --> 00:45:15.840]   Way to go, sir.
[00:45:15.840 --> 00:45:16.240]   Way to go.
[00:45:16.240 --> 00:45:18.000]   I'm sleeping.
[00:45:18.000 --> 00:45:19.120]   You know I'm sleeping.
[00:45:19.120 --> 00:45:20.320]   I ate a pig earlier.
[00:45:20.320 --> 00:45:21.760]   Oh, yeah.
[00:45:21.760 --> 00:45:21.760]   Yeah.
[00:45:21.760 --> 00:45:22.640]   It was a choice.
[00:45:22.640 --> 00:45:26.880]   It was an octopus of pig, and I went for the more sentient.
[00:45:26.880 --> 00:45:30.320]   The sandwich I had was literate the pig and the pear.
[00:45:30.320 --> 00:45:30.960]   Yeah.
[00:45:30.960 --> 00:45:31.520]   Yeah.
[00:45:31.520 --> 00:45:31.680]   Yeah.
[00:45:31.680 --> 00:45:32.720]   It was--
[00:45:32.720 --> 00:45:34.720]   It actually is putting me out.
[00:45:34.720 --> 00:45:35.040]   It was--
[00:45:35.040 --> 00:45:38.080]   Yeah, you were--
[00:45:38.080 --> 00:45:39.040]   It was a great--
[00:45:39.040 --> 00:45:39.920]   You were at a meat coma.
[00:45:39.920 --> 00:45:41.680]   It's losing sense right now.
[00:45:41.680 --> 00:45:42.640]   It's exhausting.
[00:45:42.640 --> 00:45:44.960]   It's exhausting.
[00:45:44.960 --> 00:45:46.560]   I read somebody--
[00:45:46.560 --> 00:45:48.640]   Where did I read this?
[00:45:48.640 --> 00:45:53.680]   Somebody said maybe it was a New Yorker or somewhere in New York Times.
[00:45:53.680 --> 00:45:56.240]   You know where you ask a scientist,
[00:45:56.240 --> 00:45:58.560]   "How long before I could put my brain in a jar?"
[00:45:58.560 --> 00:46:02.320]   You know, like in science fiction,
[00:46:02.320 --> 00:46:03.120]   we're like, "I don't need--"
[00:46:03.120 --> 00:46:04.560]   I was like, "I can do that for you today."
[00:46:04.560 --> 00:46:06.160]   I don't need a bottle.
[00:46:06.160 --> 00:46:06.960]   A body.
[00:46:06.960 --> 00:46:07.840]   I do need a bottle.
[00:46:07.840 --> 00:46:08.720]   I don't need a body.
[00:46:08.720 --> 00:46:09.760]   I just need--
[00:46:09.760 --> 00:46:11.920]   You know, like to be hooked up so I could be a thinking,
[00:46:11.920 --> 00:46:13.440]   sentient thing.
[00:46:13.440 --> 00:46:17.520]   And the guy, we said,
[00:46:17.520 --> 00:46:19.520]   "Let me explain why that's not going to happen."
[00:46:19.520 --> 00:46:22.880]   It was really good.
[00:46:22.880 --> 00:46:23.440]   It was--
[00:46:23.440 --> 00:46:29.680]   We have apparently mapped a small portion of some rodent's brain.
[00:46:29.680 --> 00:46:31.360]   But in order to do it,
[00:46:31.360 --> 00:46:33.440]   you had to do thin sections of the brain.
[00:46:33.440 --> 00:46:37.280]   Thousands and thousands of thin sections of the brain.
[00:46:37.280 --> 00:46:39.680]   And he said, "And even then, you know, I mean,
[00:46:39.680 --> 00:46:42.640]   is it-- is what we have thinking?
[00:46:42.640 --> 00:46:45.280]   No, it doesn't think because we sliced it up."
[00:46:45.280 --> 00:46:47.280]   It's just reacting.
[00:46:47.280 --> 00:46:48.480]   It's not even doing that.
[00:46:48.480 --> 00:46:50.160]   It's just, you know, anyway.
[00:46:50.160 --> 00:46:51.680]   Well, if you like that topic,
[00:46:51.680 --> 00:46:52.720]   I have a book to recommend to you.
[00:46:52.720 --> 00:46:55.920]   I just finished The Candy House by Jennifer Egan,
[00:46:55.920 --> 00:46:57.840]   who won a Pulitzer for her last moment.
[00:46:57.840 --> 00:46:59.040]   Her name is familiar.
[00:46:59.040 --> 00:47:01.440]   She did a visit from the Goon Squad.
[00:47:01.440 --> 00:47:02.000]   Oh, yeah.
[00:47:02.000 --> 00:47:02.800]   Yeah.
[00:47:02.800 --> 00:47:04.000]   A candy house.
[00:47:04.000 --> 00:47:06.000]   So what's-- it's about brains in a jar?
[00:47:06.000 --> 00:47:11.280]   So part of it is the ability to download your memories.
[00:47:11.280 --> 00:47:12.480]   I find that fascinating.
[00:47:12.480 --> 00:47:13.600]   Things that come out of that.
[00:47:13.600 --> 00:47:17.120]   And so it's not a moral panic novel, I'm glad to say.
[00:47:17.120 --> 00:47:24.000]   But it does explore tons of questions about memory and authority
[00:47:24.000 --> 00:47:25.760]   and other things around technology.
[00:47:25.760 --> 00:47:28.480]   Considered part of the series of visit from the Goon Squad.
[00:47:28.480 --> 00:47:28.640]   So--
[00:47:28.640 --> 00:47:30.000]   Yeah, no, I see it.
[00:47:30.000 --> 00:47:31.200]   All right, maybe--
[00:47:31.200 --> 00:47:32.320]   Maybe goon squad the--
[00:47:32.320 --> 00:47:33.680]   I'm adding this to--
[00:47:33.680 --> 00:47:34.560]   I did.
[00:47:34.560 --> 00:47:35.520]   Wishless.
[00:47:35.520 --> 00:47:36.800]   I haven't read that yet.
[00:47:36.800 --> 00:47:37.360]   It was good.
[00:47:37.360 --> 00:47:38.000]   I enjoyed it.
[00:47:38.000 --> 00:47:40.160]   Oh, yeah.
[00:47:40.160 --> 00:47:40.400]   They're--
[00:47:40.400 --> 00:47:43.120]   All right, I'll read this.
[00:47:43.120 --> 00:47:43.840]   Maybe I'll read it before--
[00:47:43.840 --> 00:47:46.640]   Why don't we do The Candy House as--
[00:47:46.640 --> 00:47:47.440]   I read--
[00:47:47.440 --> 00:47:48.640]   Eligible for the book club.
[00:47:48.640 --> 00:47:51.760]   I read The Keep, which is the only book of hers I read,
[00:47:51.760 --> 00:47:53.520]   but she's a great author.
[00:47:53.520 --> 00:47:54.240]   I thought it was a real--
[00:47:54.240 --> 00:47:56.080]   A visit from the Goon Squad is very good.
[00:47:56.080 --> 00:47:56.480]   All right.
[00:47:56.480 --> 00:47:58.480]   I did enjoy that.
[00:47:58.480 --> 00:48:00.400]   I guess you should read that first then.
[00:48:00.400 --> 00:48:03.520]   Well, I already read it, so yeah, it's a bit too late.
[00:48:03.520 --> 00:48:04.480]   You read it backwards.
[00:48:05.200 --> 00:48:06.240]   Yeah, that's--
[00:48:06.240 --> 00:48:07.360]   How I kind of do things.
[00:48:07.360 --> 00:48:13.840]   Anyway, let's take a break.
[00:48:13.840 --> 00:48:15.360]   Come back more to talk about.
[00:48:15.360 --> 00:48:17.360]   I'm sure we'll find something else.
[00:48:17.360 --> 00:48:17.840]   But--
[00:48:17.840 --> 00:48:21.680]   Ant says you have a fair--
[00:48:21.680 --> 00:48:23.120]   You are like Lambda.
[00:48:23.120 --> 00:48:24.000]   We are like Lambda.
[00:48:24.000 --> 00:48:24.800]   Just ask us a question,
[00:48:24.800 --> 00:48:27.200]   and we will spew forth until you stop us.
[00:48:27.200 --> 00:48:31.680]   Burke says, "Gigastase said honesty to 70 plus percent,
[00:48:31.680 --> 00:48:34.000]   plus or minus 0.05, if you will.
[00:48:34.800 --> 00:48:35.840]   And we shall return."
[00:48:35.840 --> 00:48:36.800]   Say what?
[00:48:36.800 --> 00:48:39.440]   We don't pretend you didn't hear that command.
[00:48:39.440 --> 00:48:40.720]   Oh, oh, I see it.
[00:48:40.720 --> 00:48:41.520]   I see.
[00:48:41.520 --> 00:48:41.920]   Oh.
[00:48:41.920 --> 00:48:48.640]   Set craving for waffles to 9.9.
[00:48:48.640 --> 00:48:50.640]   Pretty much yes.
[00:48:50.640 --> 00:48:55.600]   I like that my honesty is set to 70 percent.
[00:48:55.600 --> 00:48:58.160]   Yeah, that's all we can handle, to be honest.
[00:48:58.160 --> 00:48:59.120]   Is--
[00:48:59.120 --> 00:48:59.600]   Yeah.
[00:48:59.600 --> 00:49:00.080]   Yeah.
[00:49:00.080 --> 00:49:03.760]   Well, yeah, I think Stacy's honesty should be lower than most people.
[00:49:03.760 --> 00:49:04.320]   Yeah.
[00:49:04.320 --> 00:49:06.320]   Yeah, my honesty module is pretty up there.
[00:49:06.320 --> 00:49:06.800]   You're right.
[00:49:06.800 --> 00:49:07.440]   That's good.
[00:49:07.440 --> 00:49:08.800]   You can ask my poor husband.
[00:49:08.800 --> 00:49:10.000]   No, that's fantastic.
[00:49:10.000 --> 00:49:12.000]   That's fantastic.
[00:49:12.000 --> 00:49:13.760]   I think it's fantastic.
[00:49:13.760 --> 00:49:16.640]   Our show today brought to you by Nureva,
[00:49:16.640 --> 00:49:17.840]   and you are EVA,
[00:49:17.840 --> 00:49:20.480]   complicated and costly.
[00:49:20.480 --> 00:49:24.320]   Those are two words that describe the state of audio conferencing
[00:49:24.320 --> 00:49:28.400]   for larger spaces, and they have for a long time.
[00:49:28.400 --> 00:49:29.120]   I'm always--
[00:49:29.120 --> 00:49:32.000]   You know, I've gone to these big corporate offices
[00:49:32.000 --> 00:49:33.200]   and looked at their conference rooms,
[00:49:33.200 --> 00:49:35.520]   and they've explained how it's set up,
[00:49:35.520 --> 00:49:38.640]   and how many tens of thousands of dollars it costs.
[00:49:38.640 --> 00:49:40.960]   And I'm always flabbergasted.
[00:49:40.960 --> 00:49:43.200]   Flabbergasted is what they pay.
[00:49:43.200 --> 00:49:45.280]   And of course, it's got to be even more these days,
[00:49:45.280 --> 00:49:48.000]   because half your staff is at home,
[00:49:48.000 --> 00:49:49.600]   half your staff is in the office.
[00:49:49.600 --> 00:49:53.440]   Hybrid work means conferencing is even more important than ever before.
[00:49:53.440 --> 00:49:55.760]   And you don't want the people who are at home to feel left out.
[00:49:55.760 --> 00:49:57.360]   They should be able to hear what's going on.
[00:49:57.360 --> 00:50:01.280]   But if you're choosing one of those traditional systems,
[00:50:01.280 --> 00:50:03.120]   just get ready to write a big check,
[00:50:03.120 --> 00:50:06.320]   not to mention difficult design software,
[00:50:06.320 --> 00:50:09.760]   selecting from a dizzying array of mics and speakers and DSPs,
[00:50:09.760 --> 00:50:13.600]   and wires everywhere and outside technicians coming in for the install.
[00:50:13.600 --> 00:50:18.000]   Oh my gosh, and it could take your room down for ages to install it,
[00:50:18.000 --> 00:50:21.120]   and then you can't use it until it's calibrated,
[00:50:21.120 --> 00:50:24.400]   and as they've got to come in constantly and continually calibrate it,
[00:50:24.400 --> 00:50:27.840]   the industry was definitely primed for some kind of transformation,
[00:50:27.840 --> 00:50:31.440]   some kind of revolution, and that's Nareva.
[00:50:31.440 --> 00:50:37.760]   Nareva invented the revolutionary microphone mist technology.
[00:50:37.760 --> 00:50:43.360]   Their patented technology means that with a simple integrated microphone and speaker bar,
[00:50:43.360 --> 00:50:45.280]   if you got a really big room you could put in two,
[00:50:45.280 --> 00:50:49.200]   we'll put thousands of virtual microphones in the room.
[00:50:49.200 --> 00:50:51.680]   There are no dead zones.
[00:50:51.680 --> 00:50:55.760]   Everyone everywhere in the room can be heard clearly no matter where they are,
[00:50:55.760 --> 00:50:59.440]   no matter how they're facing, no matter how they're social distancing,
[00:50:59.440 --> 00:51:04.160]   and there are no mics to sanitize, and then people who are participating,
[00:51:04.160 --> 00:51:08.000]   whether they're in the class or the meeting or they're at home,
[00:51:08.000 --> 00:51:12.880]   can just hear what's going on, they can move and talk naturally in the space.
[00:51:12.880 --> 00:51:17.680]   It's amazing, and thanks to continuous auto calibration,
[00:51:17.680 --> 00:51:20.800]   your rooms are instantly and always ready with optimized audio.
[00:51:20.800 --> 00:51:22.480]   You don't need an outside technician.
[00:51:22.480 --> 00:51:25.760]   In fact, your IT department will love it because with the Nareva console,
[00:51:25.760 --> 00:51:31.200]   you have the power to monitor, manage, and adjust those Nareva systems from anywhere.
[00:51:31.200 --> 00:51:32.320]   You don't even have to be on-prem.
[00:51:32.320 --> 00:51:35.920]   You certainly don't have to go from room to room to room to do it.
[00:51:35.920 --> 00:51:37.200]   You could do the install yourself.
[00:51:37.200 --> 00:51:40.160]   There's a little video of this guy installing it.
[00:51:40.160 --> 00:51:42.400]   It's just if you've ever installed the sound bars just like that.
[00:51:42.400 --> 00:51:49.840]   30-minute do it yourself job, and big savings on time and cost compared to those traditional systems.
[00:51:49.840 --> 00:51:51.280]   Nareva's really great.
[00:51:52.240 --> 00:51:56.400]   So ask yourself, do you want to go with the costly and complicated
[00:51:56.400 --> 00:51:58.960]   traditional system?
[00:51:58.960 --> 00:52:00.640]   No, that's complex-pensive.
[00:52:00.640 --> 00:52:04.720]   Maybe you want to make the leap to the simple and economical,
[00:52:04.720 --> 00:52:07.840]   true, full room coverage of Nareva.
[00:52:07.840 --> 00:52:09.680]   Now that's easy economical.
[00:52:09.680 --> 00:52:12.720]   Learn more at Nareva and you are EVA,
[00:52:12.720 --> 00:52:15.360]   noreva.com.
[00:52:15.360 --> 00:52:17.760]   Great audio, simplified.
[00:52:17.760 --> 00:52:22.000]   Just in time, frankly, just in time.
[00:52:22.000 --> 00:52:23.280]   For the hybrid workspace.
[00:52:23.280 --> 00:52:26.560]   Learn more at noreva.com.
[00:52:26.560 --> 00:52:30.720]   Elon, speaking of Elon, he's going to be talking to Twitter employees for the first time this week.
[00:52:30.720 --> 00:52:32.560]   Oh, that's going to be fun.
[00:52:32.560 --> 00:52:35.120]   Oh, man, I'd like to be a fly on the wall for that.
[00:52:35.120 --> 00:52:38.800]   Will he be behind like bulletproof glass?
[00:52:38.800 --> 00:52:44.720]   Thursday, there's an all-hands meeting.
[00:52:46.880 --> 00:52:53.440]   CEO Parag Agrawal and Elon Musk will address the staff.
[00:52:53.440 --> 00:52:57.040]   They'll be able to submit questions and advances according to the Wall Street Journal.
[00:52:57.040 --> 00:53:03.120]   Musk's deal to buy Twitter is actually still full speed of head.
[00:53:03.120 --> 00:53:06.080]   In fact, I don't know if let me look at the current price,
[00:53:06.080 --> 00:53:10.240]   but the Twitter stock jumped because Elon has a,
[00:53:10.240 --> 00:53:12.000]   yachts, well, jumped as a-
[00:53:12.000 --> 00:53:14.240]   Because he's been quiet?
[00:53:14.240 --> 00:53:14.480]   No.
[00:53:15.360 --> 00:53:19.520]   No, because he got even more financing, so it's gone up a little bit.
[00:53:19.520 --> 00:53:22.720]   It's still kind of laggard at $37.99.
[00:53:22.720 --> 00:53:26.960]   Remember, Elon is supposedly offering $54.20.
[00:53:26.960 --> 00:53:28.960]   So clearly-
[00:53:28.960 --> 00:53:31.760]   What was the only problem I needed for some reason?
[00:53:31.760 --> 00:53:33.120]   Yeah, obligated.
[00:53:33.120 --> 00:53:36.720]   So the majority of the stock market says, "Yeah, right, that'll never happen."
[00:53:36.720 --> 00:53:40.800]   I mean, that's a significant low.
[00:53:40.800 --> 00:53:44.560]   In fact, the 52-week high was $73.
[00:53:45.200 --> 00:53:50.720]   But in the last six months, Twitter's never gone above $51.
[00:53:50.720 --> 00:53:55.040]   So, all right, $51.70 on April 25th.
[00:53:55.040 --> 00:53:57.120]   So it never did get to $52.40.
[00:53:57.120 --> 00:54:00.960]   So the market's never shown any confidence that Elon's going to actually get it.
[00:54:00.960 --> 00:54:01.360]   I don't know.
[00:54:01.360 --> 00:54:02.080]   I think he might.
[00:54:02.080 --> 00:54:03.440]   Might.
[00:54:03.440 --> 00:54:06.320]   You know.
[00:54:06.320 --> 00:54:06.880]   I hope not.
[00:54:06.880 --> 00:54:08.320]   Oh, God, I hope not.
[00:54:08.320 --> 00:54:08.640]   Really?
[00:54:08.640 --> 00:54:11.840]   So what could he do?
[00:54:11.840 --> 00:54:14.640]   I mean, people who use Twitter will still use it, won't they?
[00:54:14.640 --> 00:54:16.640]   Depends.
[00:54:16.640 --> 00:54:16.880]   Yeah.
[00:54:16.880 --> 00:54:18.480]   I mean, it's not-
[00:54:18.480 --> 00:54:19.360]   To a certain extent.
[00:54:19.360 --> 00:54:23.040]   I mean, if enough people get on there and start behaving like jerks
[00:54:23.040 --> 00:54:24.080]   and the people you don't-
[00:54:24.080 --> 00:54:26.320]   Like, I mean, basically it's like,
[00:54:26.320 --> 00:54:28.720]   "Oh, you'll go as long as it's fun,
[00:54:28.720 --> 00:54:30.880]   and then if it stops being fun or interesting,
[00:54:30.880 --> 00:54:33.680]   because those people have left, you leave."
[00:54:33.680 --> 00:54:33.840]   Yeah.
[00:54:33.840 --> 00:54:38.320]   Well, he'd be on the site for this all hands,
[00:54:38.320 --> 00:54:39.200]   considering his-
[00:54:39.200 --> 00:54:39.840]   Oh.
[00:54:39.840 --> 00:54:41.040]   I don't know.
[00:54:41.040 --> 00:54:44.400]   Message to his staff about bringing their behind to the office.
[00:54:44.400 --> 00:54:44.960]   Well, yeah.
[00:54:44.960 --> 00:54:46.080]   Well, that's a good question.
[00:54:46.080 --> 00:54:46.240]   It's a good question.
[00:54:46.240 --> 00:54:47.200]   In person for this-
[00:54:47.200 --> 00:54:47.680]   No, I don't think so.
[00:54:47.680 --> 00:54:48.800]   Is he going to do it virtually?
[00:54:48.800 --> 00:54:49.360]   That's a good question.
[00:54:49.360 --> 00:54:50.560]   That's a really good question.
[00:54:50.560 --> 00:54:54.160]   Imagine the troll and he'd get-
[00:54:54.160 --> 00:54:56.240]   Well, I can promise you,
[00:54:56.240 --> 00:55:00.240]   hours afterwards, they'll be recordings and stuff.
[00:55:00.240 --> 00:55:00.720]   Oh, yeah.
[00:55:00.720 --> 00:55:01.200]   Oh, yeah.
[00:55:01.200 --> 00:55:01.840]   Oh, yeah.
[00:55:01.840 --> 00:55:02.320]   Yeah.
[00:55:02.320 --> 00:55:07.040]   So I think it'll be very interesting to see what he says.
[00:55:07.760 --> 00:55:11.680]   So I try to be a good citizen on Twitter, and when I get new followers,
[00:55:11.680 --> 00:55:15.920]   and it's Jane, 473672, looking for a mate.
[00:55:15.920 --> 00:55:16.640]   Yeah.
[00:55:16.640 --> 00:55:18.000]   I reported a spam, right?
[00:55:18.000 --> 00:55:18.480]   Yeah, good.
[00:55:18.480 --> 00:55:20.240]   And on the web app, it's easy to do,
[00:55:20.240 --> 00:55:21.520]   and I figured it's not a good citizen.
[00:55:21.520 --> 00:55:23.520]   They make it go through five steps to it, but I do it.
[00:55:23.520 --> 00:55:26.960]   Now on the mobile app, oh my god, it's nearly impossible.
[00:55:26.960 --> 00:55:28.000]   It's a pain, bro.
[00:55:28.000 --> 00:55:30.000]   But what do you know, huh?
[00:55:30.000 --> 00:55:31.440]   What's your problem with this?
[00:55:31.440 --> 00:55:32.480]   Oh.
[00:55:32.480 --> 00:55:33.520]   It is a pain.
[00:55:33.520 --> 00:55:34.640]   Yeah.
[00:55:35.360 --> 00:55:41.040]   I mean, I get that they're saying, look, we're doing our best to moderate this.
[00:55:41.040 --> 00:55:42.560]   This isn't very easy.
[00:55:42.560 --> 00:55:48.160]   So bear with this, if you will, but it still shouldn't be that many clicks
[00:55:48.160 --> 00:55:54.240]   to report something that is clearly like pornographic or clearly racist or harassment
[00:55:54.240 --> 00:55:57.520]   towards meat individual that's reporting it.
[00:55:57.520 --> 00:56:00.320]   You're trying to help Twitter, Ant.
[00:56:00.320 --> 00:56:02.720]   Trying to help clean the part.
[00:56:02.720 --> 00:56:03.680]   You know.
[00:56:03.680 --> 00:56:04.000]   Yeah.
[00:56:04.000 --> 00:56:10.800]   We're getting there, I'm just looking forward.
[00:56:10.800 --> 00:56:11.520]   What? Oh, sorry.
[00:56:11.520 --> 00:56:14.720]   Oh, I was like, all of a sudden everybody got quiet.
[00:56:14.720 --> 00:56:15.840]   I was reading Twitter.
[00:56:15.840 --> 00:56:17.200]   I was reading Twitter.
[00:56:17.200 --> 00:56:21.600]   Elon also says this September, they're going to show off the first model of their new robot.
[00:56:21.600 --> 00:56:25.680]   I find that hard to believe.
[00:56:25.680 --> 00:56:28.240]   This is the robot last August at A.I. Days
[00:56:30.000 --> 00:56:33.360]   that was actually a person in a robot costume dancing.
[00:56:33.360 --> 00:56:34.160]   And remember that?
[00:56:34.160 --> 00:56:36.400]   Oh, this is the, yeah, this is the Twitter.
[00:56:36.400 --> 00:56:37.040]   Okay.
[00:56:37.040 --> 00:56:39.120]   I mean, the Tesla robot.
[00:56:39.120 --> 00:56:39.680]   Tesla robot.
[00:56:39.680 --> 00:56:46.240]   I love the Virgil's headline says, "Elon S says Tesla's fake robot might be ready by September."
[00:56:46.240 --> 00:56:50.320]   Well done.
[00:56:50.320 --> 00:56:50.640]   Nice.
[00:56:50.640 --> 00:56:58.240]   Tesla's also planning a layoff about 10% of its workforce.
[00:56:58.800 --> 00:57:00.160]   But he's not alone in that.
[00:57:00.160 --> 00:57:02.480]   That seems to be a lot of 10% everywhere.
[00:57:02.480 --> 00:57:04.000]   It's the heaven at the last week of so.
[00:57:04.000 --> 00:57:08.000]   There were a lot of those crypto places too.
[00:57:08.000 --> 00:57:10.000]   I want to say Coinbase is another one.
[00:57:10.000 --> 00:57:13.200]   Coinbase laid off 1,100 employees by,
[00:57:13.200 --> 00:57:14.080]   here's how you found out.
[00:57:14.080 --> 00:57:15.600]   Your email stopped working.
[00:57:15.600 --> 00:57:17.680]   Oh, man.
[00:57:17.680 --> 00:57:23.040]   If you are affected, you will get email in your personal email address.
[00:57:23.040 --> 00:57:24.960]   Your personal address.
[00:57:24.960 --> 00:57:25.440]   Yeah.
[00:57:25.440 --> 00:57:27.200]   Your personal address.
[00:57:27.200 --> 00:57:29.280]   That's got to work out well, yeah.
[00:57:29.280 --> 00:57:37.600]   Well, this isn't really an Arbale-Wick, but I think it's an interesting question.
[00:57:37.600 --> 00:57:40.560]   Why are tech stocks doing so poorly now?
[00:57:40.560 --> 00:57:44.080]   And six months ago, they were the golden stocks, right?
[00:57:44.080 --> 00:57:49.520]   Well, you could be worried about the FTC getting kind of in Congress.
[00:57:49.520 --> 00:57:49.920]   Can you get that?
[00:57:49.920 --> 00:57:54.240]   I'm kind of, well, I also think, you know, if everything's going to,
[00:57:55.120 --> 00:57:59.440]   if you've had your rose-colored glasses on and you thought everything was going up and up,
[00:57:59.440 --> 00:58:03.200]   and you recognize that this was probably not like a fundamentals thing.
[00:58:03.200 --> 00:58:04.640]   It was kind of like a hype thing.
[00:58:04.640 --> 00:58:05.520]   Yeah.
[00:58:05.520 --> 00:58:06.560]   And the hype guys.
[00:58:06.560 --> 00:58:07.360]   Everything's getting sold off.
[00:58:07.360 --> 00:58:07.760]   Yeah.
[00:58:07.760 --> 00:58:08.960]   No, that's a good point.
[00:58:08.960 --> 00:58:13.600]   I think it's the COVID dividend to those stocks is devalued.
[00:58:13.600 --> 00:58:15.040]   I think the whole market is devalued.
[00:58:15.040 --> 00:58:18.560]   We're in Bearland because of inflation and stuff.
[00:58:18.560 --> 00:58:23.600]   And so my broker, I talked to him like once a year,
[00:58:23.600 --> 00:58:24.960]   he said, "It'd be something."
[00:58:24.960 --> 00:58:25.680]   And I asked him.
[00:58:25.680 --> 00:58:34.800]   And he said, "Any stock that has a high P and low profit is going to get hit hard right now?"
[00:58:34.800 --> 00:58:36.320]   Because reality is sitting in.
[00:58:36.320 --> 00:58:36.560]   Yeah.
[00:58:36.560 --> 00:58:37.040]   Exactly.
[00:58:37.040 --> 00:58:37.440]   Yeah.
[00:58:37.440 --> 00:58:39.360]   That's not rocket science.
[00:58:39.360 --> 00:58:40.000]   Right.
[00:58:40.000 --> 00:58:40.800]   No, exactly.
[00:58:40.800 --> 00:58:41.680]   You're going to go to blue chips.
[00:58:41.680 --> 00:58:43.040]   You're going to go to safe harbors.
[00:58:43.040 --> 00:58:46.160]   And so the tech stocks were flying high.
[00:58:46.160 --> 00:58:50.640]   But I think, you know, to my mind, Amazon and Google are still good companies.
[00:58:50.640 --> 00:58:52.240]   Twitter, we don't know what the heck's going to happen.
[00:58:52.240 --> 00:58:54.240]   Facebook, I just, that I don't know.
[00:58:54.240 --> 00:58:58.800]   I'm just glad I put all my money into Bitcoin before it was too late.
[00:58:58.800 --> 00:58:59.280]   Yeah.
[00:58:59.280 --> 00:58:59.840]   And I lost it.
[00:58:59.840 --> 00:59:01.840]   Yeah.
[00:59:01.840 --> 00:59:03.760]   And you know where those Bitcoin are right now.
[00:59:03.760 --> 00:59:05.520]   Yeah, $22,000.
[00:59:05.520 --> 00:59:07.760]   Look at that graph.
[00:59:07.760 --> 00:59:08.640]   There is a graph.
[00:59:08.640 --> 00:59:10.640]   That's the one month graph.
[00:59:10.640 --> 00:59:16.240]   I saw something from Dan Patterson this morning where he mentions it's like
[00:59:16.240 --> 00:59:18.720]   down 70% in the last eight months.
[00:59:18.720 --> 00:59:23.360]   That's the 25% in the last week crypto crypto market anyway.
[00:59:23.360 --> 00:59:23.600]   Yeah.
[00:59:23.600 --> 00:59:26.480]   That's just 70% in the last eight months.
[00:59:26.480 --> 00:59:26.560]   Yeah.
[00:59:26.560 --> 00:59:27.040]   You believe it.
[00:59:27.040 --> 00:59:27.840]   Unbelievable.
[00:59:27.840 --> 00:59:34.880]   Here is Jack Dorsey's presentation for web five.
[00:59:34.880 --> 00:59:38.400]   You know, web two plus web three equals web five.
[00:59:38.400 --> 00:59:40.080]   The next web.
[00:59:40.080 --> 00:59:43.200]   When I heard about this, I thought, oh, you're joking, right?
[00:59:43.200 --> 00:59:44.080]   This is a joke.
[00:59:44.080 --> 00:59:46.320]   I don't think it is.
[00:59:46.320 --> 00:59:47.200]   No, it's not joking.
[00:59:47.200 --> 00:59:48.400]   I tweeted and he answered.
[00:59:48.400 --> 00:59:50.960]   I said, what is the relationship of this to blue sky?
[00:59:50.960 --> 00:59:54.400]   And he answered that blue sky can be built on top of this.
[00:59:54.400 --> 00:59:54.640]   Yeah.
[00:59:54.640 --> 00:59:58.640]   Because the tag line, web five is the decentralized web platform.
[00:59:58.640 --> 01:00:01.120]   Of course, blue sky is decentralized Twitter.
[01:00:01.120 --> 01:00:03.760]   So I should go look at this.
[01:00:03.760 --> 01:00:04.560]   I have not seen this.
[01:00:04.560 --> 01:00:05.440]   It is on.
[01:00:05.440 --> 01:00:08.080]   I put it up there because I wanted to hear what you guys thought of this.
[01:00:08.080 --> 01:00:10.240]   And any BS detection you have.
[01:00:10.240 --> 01:00:12.240]   Well, that's not saying it's not fair.
[01:00:12.240 --> 01:00:14.160]   What interests you?
[01:00:14.160 --> 01:00:16.080]   Because I can't fully figure it out.
[01:00:16.080 --> 01:00:16.960]   It's about identity.
[01:00:16.960 --> 01:00:18.640]   It's about distributed identity.
[01:00:18.640 --> 01:00:22.400]   And there's some interesting slides here about confirming identity.
[01:00:22.400 --> 01:00:23.840]   Oh, that would be line.
[01:00:23.840 --> 01:00:25.200]   Then 25.
[01:00:25.200 --> 01:00:28.720]   Buff and bot.
[01:00:28.720 --> 01:00:31.040]   Gig and bot.
[01:00:31.040 --> 01:00:32.000]   Gig and bot.
[01:00:32.000 --> 01:00:34.640]   Gig and bot.
[01:00:34.640 --> 01:00:35.360]   Gig and bot.
[01:00:35.360 --> 01:00:38.720]   I go for the gigabyte person.
[01:00:38.720 --> 01:00:38.880]   Yeah.
[01:00:38.880 --> 01:00:39.760]   If you want to go.
[01:00:39.760 --> 01:00:43.120]   Have you been called a hundred times before?
[01:00:44.880 --> 01:00:48.080]   No, but when I was in school, they called me Higg and bot.
[01:00:48.080 --> 01:00:50.400]   That's the difference.
[01:00:50.400 --> 01:00:51.760]   Huge Higg and bottom.
[01:00:51.760 --> 01:00:55.440]   That's the difference between then and now.
[01:00:55.440 --> 01:00:58.960]   Now you are a bot.
[01:00:58.960 --> 01:00:59.760]   Not a bot.
[01:00:59.760 --> 01:01:01.040]   We're going to talk web five.
[01:01:01.040 --> 01:01:01.920]   That's what we're going to talk.
[01:01:01.920 --> 01:01:03.120]   Evolving the web.
[01:01:03.120 --> 01:01:03.520]   Okay.
[01:01:03.520 --> 01:01:07.040]   Today in the current web model, his presentation says,
[01:01:07.040 --> 01:01:11.200]   people are users who do not own their own data or identity.
[01:01:11.200 --> 01:01:11.680]   Right?
[01:01:11.680 --> 01:01:14.160]   They're given accounts by companies and their data is held captive
[01:01:14.160 --> 01:01:15.200]   in app silos.
[01:01:15.200 --> 01:01:16.880]   That's clearly going to work.
[01:01:16.880 --> 01:01:17.440]   That's a problem.
[01:01:17.440 --> 01:01:22.800]   The web we want to create a new class of decentralized apps and protocols
[01:01:22.800 --> 01:01:25.200]   that put individuals at the center.
[01:01:25.200 --> 01:01:29.520]   We must empower them with self-owned identity and restore control over the data.
[01:01:29.520 --> 01:01:31.520]   You know, I mean, this is everybody saying this.
[01:01:31.520 --> 01:01:33.840]   Tim Berners-Lee, who invented the worldwide web.
[01:01:33.840 --> 01:01:36.160]   So what is he going to build it?
[01:01:36.160 --> 01:01:37.200]   Well, that's the question is.
[01:01:37.200 --> 01:01:38.080]   Well, you can't build it.
[01:01:38.080 --> 01:01:38.880]   So you can't.
[01:01:38.880 --> 01:01:41.760]   The problem is people are stupid and lazy.
[01:01:41.760 --> 01:01:45.360]   And we don't want to actually have the technical capabilities to do
[01:01:45.360 --> 01:01:47.920]   and manage our own identity in a way that can be
[01:01:47.920 --> 01:01:49.520]   multi-factor to cross lots of places.
[01:01:49.520 --> 01:01:50.400]   Nobody wants to make a server.
[01:01:50.400 --> 01:01:53.920]   Companies don't want to make it, they don't want to make it easy for us
[01:01:53.920 --> 01:01:59.120]   because they can monetize and trap us by using the centralized identity as a way to
[01:01:59.120 --> 01:02:03.360]   to track, like to offer services and to track us and all that.
[01:02:03.360 --> 01:02:11.280]   So this is basically like, oh, give me some sort of philosophy about like work
[01:02:11.280 --> 01:02:14.080]   in people in rights.
[01:02:14.080 --> 01:02:15.680]   I mean, I don't know.
[01:02:15.680 --> 01:02:17.840]   I had Jack to the rescue.
[01:02:17.840 --> 01:02:20.000]   It's socialism.
[01:02:20.000 --> 01:02:20.880]   Is that what you're saying?
[01:02:20.880 --> 01:02:21.680]   Stacey Higginwald.
[01:02:21.680 --> 01:02:23.200]   No, it's more like a tragedy of the.
[01:02:23.200 --> 01:02:25.440]   It's not a tragedy of the commons.
[01:02:25.440 --> 01:02:30.000]   It's like, it's like basically like, people want to get value for nothing
[01:02:30.000 --> 01:02:33.440]   and companies want to extract all the value they can for people.
[01:02:33.440 --> 01:02:34.960]   There's not really a medium ground.
[01:02:34.960 --> 01:02:35.520]   Right.
[01:02:35.520 --> 01:02:38.160]   There's no reason for them to make it easy for us.
[01:02:38.160 --> 01:02:38.480]   Right.
[01:02:39.200 --> 01:02:43.520]   And in fact, I would imagine companies would see this as a great opportunity because,
[01:02:43.520 --> 01:02:49.600]   as you pointed out, if you have a unique identifier with all your data and it,
[01:02:49.600 --> 01:02:52.240]   well, companies can say, hey, great, that makes it a lot easier for us.
[01:02:52.240 --> 01:02:52.640]   Thank you.
[01:02:52.640 --> 01:02:52.880]   Just,
[01:02:52.880 --> 01:02:53.440]   Just,
[01:02:53.440 --> 01:02:59.040]   You were thinking that, but we built the web, which was perfect for things like this
[01:02:59.040 --> 01:02:59.760]   in a lot of ways.
[01:02:59.760 --> 01:03:05.600]   Like you look at like things like domain registries and URLs.
[01:03:05.600 --> 01:03:07.840]   I mean, that made it easy to get places on the web.
[01:03:07.840 --> 01:03:11.760]   And it was a fairly accessible system for many people.
[01:03:11.760 --> 01:03:13.120]   It's not accessible for everyone.
[01:03:13.120 --> 01:03:18.080]   Think about like how well that worked, but then we all wanted to create these walled gardens.
[01:03:18.080 --> 01:03:21.520]   I mean, it's like, whatever that forever tension is called.
[01:03:21.520 --> 01:03:23.840]   This slide nine.
[01:03:23.840 --> 01:03:26.800]   Yeah, I have it up right now.
[01:03:26.800 --> 01:03:30.960]   This is, I think this is blue sky.
[01:03:30.960 --> 01:03:33.840]   I think this is kind of what blue sky is saying.
[01:03:33.840 --> 01:03:36.720]   And blue sky might be a Twitter implementation of this
[01:03:37.280 --> 01:03:40.560]   concept, but this is what I think he wants to do, this decentralized.
[01:03:40.560 --> 01:03:43.920]   And a lot of people thought a lot about this.
[01:03:43.920 --> 01:03:49.840]   Somebody's obviously put a lot of thought into this, but until there is a proof of concept
[01:03:49.840 --> 01:03:55.040]   application, I'm not, you know, where's it still you're looking to trust an identity.
[01:03:55.040 --> 01:03:57.680]   You need a trusted view of identity, right?
[01:03:57.680 --> 01:04:01.440]   And without a centralized authority for that, because that would be.
[01:04:01.440 --> 01:04:02.320]   Yeah, well, we have that.
[01:04:02.320 --> 01:04:04.080]   It's called public key crypto.
[01:04:05.280 --> 01:04:07.280]   PKI does it for a lot of things.
[01:04:07.280 --> 01:04:12.320]   So we kind of have that kind of thing.
[01:04:12.320 --> 01:04:14.960]   But Stacy's point, major pain in the butt.
[01:04:14.960 --> 01:04:16.400]   Yeah.
[01:04:16.400 --> 01:04:20.080]   And even more like, well, what's the incentive?
[01:04:20.080 --> 01:04:24.000]   What's the, why would Amazon or Facebook or Google do this?
[01:04:24.000 --> 01:04:26.240]   I mean, they're going to say, yes, sure.
[01:04:26.240 --> 01:04:27.040]   Go ahead.
[01:04:27.040 --> 01:04:27.680]   Fine.
[01:04:27.680 --> 01:04:27.920]   Yeah.
[01:04:27.920 --> 01:04:31.120]   And if you want to visit us, you're going to handle over your IT.
[01:04:31.120 --> 01:04:32.160]   So we know you are.
[01:04:32.160 --> 01:04:34.000]   Right.
[01:04:34.800 --> 01:04:35.200]   Do you?
[01:04:35.200 --> 01:04:35.520]   Okay.
[01:04:35.520 --> 01:04:41.120]   So this is kind of silly, but looking at this presentation, I want to,
[01:04:41.120 --> 01:04:44.480]   where should I put a file?
[01:04:44.480 --> 01:04:48.480]   A friend of mine wrote this and this, this presentation just brings it all home.
[01:04:48.480 --> 01:04:52.320]   Put it in the chat on the Google Doc.
[01:04:52.320 --> 01:04:54.400]   Chat on the Google.
[01:04:54.400 --> 01:04:55.680]   Oh, no, I just started to meet.
[01:04:55.680 --> 01:04:56.000]   Hold on.
[01:04:56.000 --> 01:04:59.360]   Don't worry.
[01:04:59.360 --> 01:05:00.720]   Soon that's going to be a duo.
[01:05:00.720 --> 01:05:02.400]   All right.
[01:05:02.400 --> 01:05:06.240]   So episode, where does it, where does it go on the top?
[01:05:06.240 --> 01:05:07.600]   No, on the bottom here.
[01:05:07.600 --> 01:05:11.200]   Oh, I just stuck it.
[01:05:11.200 --> 01:05:12.240]   No, undo.
[01:05:12.240 --> 01:05:14.160]   I can't work the internet.
[01:05:14.160 --> 01:05:15.920]   I said, yeah, but what's the internet?
[01:05:15.920 --> 01:05:16.960]   Log, Stacy?
[01:05:16.960 --> 01:05:19.120]   Put it, put it in the IRC or?
[01:05:19.120 --> 01:05:20.720]   I'm going to put it in line.
[01:05:20.720 --> 01:05:22.800]   And then put it in line 28.
[01:05:22.800 --> 01:05:24.720]   Line 46.
[01:05:24.720 --> 01:05:26.320]   You'll have to excuse us.
[01:05:26.320 --> 01:05:28.400]   This technology stuff is hard.
[01:05:28.400 --> 01:05:32.160]   Stasis is like, oh, really?
[01:05:32.160 --> 01:05:33.600]   I don't know how it works.
[01:05:33.600 --> 01:05:34.320]   But go ahead.
[01:05:34.320 --> 01:05:37.360]   Let's just weigh in on Jack's Web 5.
[01:05:37.360 --> 01:05:39.040]   Exactly.
[01:05:39.040 --> 01:05:41.440]   I have a opinion.
[01:05:41.440 --> 01:05:42.000]   I have a opinion.
[01:05:42.000 --> 01:05:42.400]   It's where it is.
[01:05:42.400 --> 01:05:43.120]   Okay.
[01:05:43.120 --> 01:05:49.040]   But my friend who works at an analyst firm, he wrote about what he calls the developer aesthetic,
[01:05:49.040 --> 01:05:51.840]   which is exactly this.
[01:05:51.840 --> 01:05:53.600]   It's like hexagons.
[01:05:53.600 --> 01:05:55.200]   It's got the font for it.
[01:05:55.200 --> 01:05:57.040]   It's basically like trying to appeal.
[01:05:57.040 --> 01:06:01.280]   So, and I was like, so when I saw this, I was like, oh my God,
[01:06:01.280 --> 01:06:02.480]   it's the developer aesthetic.
[01:06:02.480 --> 01:06:03.840]   Yeah, until there's code.
[01:06:03.840 --> 01:06:04.320]   It's totally.
[01:06:04.320 --> 01:06:05.600]   And maybe there is code.
[01:06:05.600 --> 01:06:07.920]   I'm not saying there isn't, but it's hard to judge.
[01:06:07.920 --> 01:06:12.560]   This is a nice slideshow, but it's hard to judge it until there's code, right?
[01:06:12.560 --> 01:06:14.000]   It is.
[01:06:14.000 --> 01:06:15.280]   I mean, well, in just the...
[01:06:15.280 --> 01:06:21.600]   It feels like the tech people, they come at it and they're like,
[01:06:21.600 --> 01:06:23.440]   I have this beautiful vision.
[01:06:23.440 --> 01:06:28.480]   And then they come out and they program it and they come up with protocols and a way to make it
[01:06:28.480 --> 01:06:29.200]   happen.
[01:06:29.200 --> 01:06:34.000]   And then the business people look at that and they say, how can I make that money?
[01:06:34.000 --> 01:06:34.000]   Right.
[01:06:34.000 --> 01:06:34.480]   Right.
[01:06:34.480 --> 01:06:39.760]   And the tech people, you would think they would figure out that that's what happens every time.
[01:06:39.760 --> 01:06:40.880]   But it's like they don't.
[01:06:40.880 --> 01:06:44.000]   And then when the business people come in and start making money, the tech people either do
[01:06:44.000 --> 01:06:45.120]   one of two things.
[01:06:45.120 --> 01:06:50.720]   They jump in and they're like, they sell out and they sell out in the most, like just
[01:06:50.720 --> 01:06:56.640]   Mark a Zuckerbergian way where they're just like continuously making unethical decisions and just
[01:06:57.360 --> 01:07:00.640]   getting away with it and apologizing later if they have to.
[01:07:00.640 --> 01:07:06.720]   Or they just leave completely and become weird people who live in basements and rail from
[01:07:06.720 --> 01:07:12.800]   chats about selling out.
[01:07:12.800 --> 01:07:15.520]   Just on... I'm looking at this slideshow.
[01:07:15.520 --> 01:07:17.760]   I mean, it's really kind of just describing PKI.
[01:07:20.320 --> 01:07:26.640]   PKI or I saw like 20 of these back in like 20, 2008, 2010.
[01:07:26.640 --> 01:07:32.080]   So many, like especially with the rise of like the beginning of the IoT because everybody's like,
[01:07:32.080 --> 01:07:35.840]   oh, you'll carry all your data and your data, like your data profile will talk to the devices
[01:07:35.840 --> 01:07:37.440]   and tell you what you like and what you don't like.
[01:07:37.440 --> 01:07:38.000]   Right.
[01:07:38.000 --> 01:07:40.560]   And it's still the internet we want though.
[01:07:40.560 --> 01:07:41.840]   Oh yeah.
[01:07:41.840 --> 01:07:43.040]   Which is, is there any path to it?
[01:07:43.040 --> 01:07:46.240]   Not until it makes everybody some money.
[01:07:46.240 --> 01:07:48.000]   But we're not going to be free.
[01:07:48.000 --> 01:07:51.200]   It's not... I don't... So here's the problem and this is the problem.
[01:07:51.200 --> 01:07:55.040]   In fact, I thought Jack Dorsey was one of the people who really got that this was the problem,
[01:07:55.040 --> 01:07:59.200]   which is web three. Maybe he does. I mean, that's why he's proposing web five.
[01:07:59.200 --> 01:08:02.960]   Web three looks like it's democratic. It's really controlled by a few companies.
[01:08:02.960 --> 01:08:03.600]   NFTs.
[01:08:03.600 --> 01:08:03.920]   You see these.
[01:08:03.920 --> 01:08:04.800]   Look like they're...
[01:08:04.800 --> 01:08:05.920]   They look like...
[01:08:05.920 --> 01:08:06.960]   You see these exactly.
[01:08:06.960 --> 01:08:06.960]   Yeah.
[01:08:06.960 --> 01:08:11.760]   PCs. They look like they're democratic, but it's... It's not an open C. It doesn't exist.
[01:08:11.760 --> 01:08:15.440]   So it's one of those... You know, I mean, the web looks like it's democratic,
[01:08:15.440 --> 01:08:17.680]   except if it's not on Google, it doesn't exist.
[01:08:17.680 --> 01:08:19.680]   So there are gatekeepers in all of these things.
[01:08:19.680 --> 01:08:22.080]   And I think that that's... I don't know.
[01:08:22.080 --> 01:08:25.200]   I feel like that's going to be a tough one to get around.
[01:08:25.200 --> 01:08:29.440]   This sounds a lot like today's episode of Floss Weekly.
[01:08:29.440 --> 01:08:30.640]   Do they talk about this stuff?
[01:08:30.640 --> 01:08:39.360]   Yeah, Mr. Sam Kern talked a lot about DIDs and Pico and all of this stuff with identification.
[01:08:39.360 --> 01:08:43.040]   Decentralized identification. So check that out.
[01:08:43.040 --> 01:08:43.520]   Yeah.
[01:08:43.520 --> 01:08:44.480]   It's kind of...
[01:08:44.480 --> 01:08:44.560]   It's kind of...
[01:08:44.560 --> 01:08:46.160]   It's 685 today.
[01:08:46.160 --> 01:08:46.800]   Yeah.
[01:08:46.800 --> 01:08:52.960]   I mean, it's a solved thing, but I don't know how it gets implemented globally as the next internet.
[01:08:52.960 --> 01:08:54.480]   These things don't happen kind of that way.
[01:08:54.480 --> 01:08:56.160]   They happen much more.
[01:08:56.160 --> 01:08:58.560]   Yeah. You need like an IEEE standard or something.
[01:08:58.560 --> 01:08:59.040]   Right.
[01:08:59.040 --> 01:08:59.600]   Even that.
[01:08:59.600 --> 01:09:02.560]   How did you get regular people to understand this stuff?
[01:09:02.560 --> 01:09:04.240]   That's a challenge, right?
[01:09:04.240 --> 01:09:10.000]   How do you get them to understand this stuff and get them to want to buy in and make all of the...
[01:09:10.000 --> 01:09:13.360]   I think regular people now understand they want privacy.
[01:09:13.360 --> 01:09:15.280]   But...
[01:09:15.280 --> 01:09:16.560]   I don't think they do.
[01:09:16.560 --> 01:09:18.320]   I think when you tell them...
[01:09:18.320 --> 01:09:19.200]   They want convenience, really.
[01:09:19.200 --> 01:09:20.000]   I will give you privacy.
[01:09:20.000 --> 01:09:22.320]   Yeah, I was like, if I tell you, I will give you privacy,
[01:09:22.320 --> 01:09:25.440]   but I won't be able to give you Google Maps because of that.
[01:09:25.440 --> 01:09:26.000]   They're like...
[01:09:26.000 --> 01:09:26.960]   Oh, sorry.
[01:09:26.960 --> 01:09:28.000]   They're like, "Hecno?
[01:09:28.000 --> 01:09:28.880]   I want Google Maps."
[01:09:28.880 --> 01:09:29.360]   Yeah.
[01:09:29.360 --> 01:09:29.840]   Yeah.
[01:09:29.840 --> 01:09:31.200]   No, I think you're right.
[01:09:31.200 --> 01:09:31.680]   It's like, I'm out.
[01:09:31.680 --> 01:09:33.360]   Yeah, even in Germany, you're right.
[01:09:33.360 --> 01:09:34.000]   You're right.
[01:09:34.000 --> 01:09:34.640]   Yeah.
[01:09:34.640 --> 01:09:36.320]   Convenience, ultimately, is all we care.
[01:09:36.320 --> 01:09:38.320]   Well, so back to Stacy's point.
[01:09:38.320 --> 01:09:40.400]   Can you...
[01:09:40.400 --> 01:09:42.160]   If you're an entrepreneur,
[01:09:42.960 --> 01:09:46.000]   can you see in Jack's map a way to build a business
[01:09:46.000 --> 01:09:50.640]   that survives on its own using this structure
[01:09:50.640 --> 01:09:54.640]   and starts to draw capital in so people can make money doing that?
[01:09:54.640 --> 01:09:58.320]   So, then, I'm saying it's not the average person.
[01:09:58.320 --> 01:10:01.520]   It's the person who builds this into a usable service
[01:10:01.520 --> 01:10:06.240]   of sub-assort, Blue Sky Twitter, whatever, and say, "Yeah, I like that."
[01:10:06.240 --> 01:10:10.720]   No, because nobody wants to be in the picks and shovels.
[01:10:10.720 --> 01:10:12.480]   They want to be in the walled garden.
[01:10:12.480 --> 01:10:14.080]   They want the highest profit margins.
[01:10:14.080 --> 01:10:15.520]   They don't want to build the...
[01:10:15.520 --> 01:10:18.640]   Like, remember how Intel funded all these companies
[01:10:18.640 --> 01:10:20.720]   to help drive the adoption of their chips?
[01:10:20.720 --> 01:10:23.280]   Most people don't have the Intel mentality.
[01:10:23.280 --> 01:10:24.640]   They have the Telco mentality,
[01:10:24.640 --> 01:10:26.080]   which is like, "These are my pipes,
[01:10:26.080 --> 01:10:28.000]   and you're going to pay dearly to use it."
[01:10:28.000 --> 01:10:30.720]   So, you have to decide if you're...
[01:10:30.720 --> 01:10:35.520]   And VCs always want that other model.
[01:10:35.520 --> 01:10:37.520]   So, here's another theory.
[01:10:37.520 --> 01:10:39.840]   Just playing here.
[01:10:41.440 --> 01:10:44.560]   Jack's Web 5 is regulation proof.
[01:10:44.560 --> 01:10:48.720]   In a sense that it's distributed,
[01:10:48.720 --> 01:10:49.280]   it's not proof.
[01:10:49.280 --> 01:10:50.320]   I don't quite mean what I'm saying,
[01:10:50.320 --> 01:10:53.920]   but it's no longer the big company is the big target.
[01:10:53.920 --> 01:10:58.480]   The big place where everything is controlled and we resent that.
[01:10:58.480 --> 01:11:01.280]   Things are happening in a thousand places,
[01:11:01.280 --> 01:11:05.600]   and it'll confuse the hell out of regulators for a long time,
[01:11:05.600 --> 01:11:07.280]   and they won't know how to do it.
[01:11:07.280 --> 01:11:09.680]   And so, you have another permissionless...
[01:11:10.560 --> 01:11:13.120]   ...environment to work in.
[01:11:13.120 --> 01:11:14.480]   Just playing here.
[01:11:14.480 --> 01:11:20.400]   I personally think permissionless environments
[01:11:20.400 --> 01:11:22.960]   in today's society that, again,
[01:11:22.960 --> 01:11:25.360]   I'm going to go with like, end stages,
[01:11:25.360 --> 01:11:29.920]   or the most money-grubbing stage of capitalism
[01:11:29.920 --> 01:11:32.480]   is probably a bad thing for users.
[01:11:32.480 --> 01:11:34.400]   Yeah, as soon as I said it, I knew you were saying.
[01:11:34.400 --> 01:11:37.600]   You're like, "Oh, yeah, here's how my socialist friend..."
[01:11:37.600 --> 01:11:38.240]   I don't wear this.
[01:11:38.240 --> 01:11:39.040]   It's going to go, yeah.
[01:11:39.040 --> 01:11:39.520]   No.
[01:11:39.520 --> 01:11:40.080]   Sorry.
[01:11:40.080 --> 01:11:41.920]   I really lost a lot of faith.
[01:11:41.920 --> 01:11:42.800]   You're right. I don't disagree.
[01:11:42.800 --> 01:11:44.240]   Yeah.
[01:11:44.240 --> 01:11:47.760]   I wanted to be a business reporter,
[01:11:47.760 --> 01:11:49.920]   actually 22 years ago when I started,
[01:11:49.920 --> 01:11:51.040]   I was really excited.
[01:11:51.040 --> 01:11:52.160]   But now...
[01:11:52.160 --> 01:11:52.480]   No.
[01:11:52.480 --> 01:11:55.440]   I don't see the incentive,
[01:11:55.440 --> 01:11:57.760]   yeah, for any business to adopt this, right?
[01:11:57.760 --> 01:11:59.360]   I see why users might want to,
[01:11:59.360 --> 01:12:00.960]   although I think it's way too complicated.
[01:12:00.960 --> 01:12:02.720]   Look, we can't even get rid of passwords.
[01:12:02.720 --> 01:12:04.320]   We're trying to get rid of passwords,
[01:12:04.320 --> 01:12:05.680]   and we can't even get that good done.
[01:12:08.320 --> 01:12:12.640]   Yeah, but Leo, I was writing about this recently.
[01:12:12.640 --> 01:12:15.200]   When I was at Delphi Internet for the horrible month,
[01:12:15.200 --> 01:12:20.560]   and they came in and showed me and others the Netscape browser.
[01:12:20.560 --> 01:12:25.280]   And the nerds got it immediately and said,
[01:12:25.280 --> 01:12:27.440]   "Oh, hell, here we are trying to make a GUI
[01:12:27.440 --> 01:12:30.400]   for a closed network, and that's dead."
[01:12:30.400 --> 01:12:31.360]   They all got it immediately.
[01:12:31.360 --> 01:12:33.600]   The media executives there said,
[01:12:33.600 --> 01:12:34.640]   "No, this is way too complicated.
[01:12:34.640 --> 01:12:35.920]   Nobody's ever going to do this.
[01:12:35.920 --> 01:12:37.120]   What's this HTT?
[01:12:37.120 --> 01:12:38.320]   What's that?
[01:12:38.320 --> 01:12:40.080]   No, no one's ever going to do this.
[01:12:40.080 --> 01:12:41.920]   They're going to stay on AOL forever."
[01:12:41.920 --> 01:12:43.760]   Of course, they're wrong,
[01:12:43.760 --> 01:12:46.720]   because people did develop the web to make it usable.
[01:12:46.720 --> 01:12:51.280]   So, can you imagine this?
[01:12:51.280 --> 01:12:51.920]   Can I imagine this being usable?
[01:12:51.920 --> 01:12:52.320]   Yes.
[01:12:52.320 --> 01:12:54.320]   So they gave that up, though,
[01:12:54.320 --> 01:13:01.360]   when we had a brief moment of unity around HTML and links
[01:13:01.360 --> 01:13:03.200]   and all kinds of things,
[01:13:03.200 --> 01:13:05.440]   and then we were like, "Well, crap, we have to make money."
[01:13:06.640 --> 01:13:07.840]   Advertising is the way.
[01:13:07.840 --> 01:13:09.840]   And then we started getting,
[01:13:09.840 --> 01:13:11.920]   you know, cookies.
[01:13:11.920 --> 01:13:12.480]   It's like beta.
[01:13:12.480 --> 01:13:13.440]   But drop it.
[01:13:13.440 --> 01:13:16.880]   But you know, all the things that the advertisers...
[01:13:16.880 --> 01:13:19.920]   So, again, I'm saying the tech people have
[01:13:19.920 --> 01:13:24.240]   within them to make something that solves the problem that they see.
[01:13:24.240 --> 01:13:27.040]   But if you don't say that the problem that you see is that
[01:13:27.040 --> 01:13:29.760]   eventually all of this gets corrupted by the need to make
[01:13:29.760 --> 01:13:33.920]   gobs of money, like profit, you know...
[01:13:33.920 --> 01:13:34.240]   VC money.
[01:13:34.240 --> 01:13:36.480]   VC money.
[01:13:36.480 --> 01:13:36.880]   VC money.
[01:13:36.880 --> 01:13:39.360]   Then you're not actually solving the whole problem.
[01:13:39.360 --> 01:13:41.680]   You're just solving a little bit of the problem.
[01:13:41.680 --> 01:13:42.640]   Let me ask each of you.
[01:13:42.640 --> 01:13:45.600]   What's the problem you see with the web today?
[01:13:45.600 --> 01:13:52.960]   I think the problem I see in the web is that siloed
[01:13:52.960 --> 01:13:56.080]   and it's controlled by a number of big companies.
[01:13:56.080 --> 01:13:57.600]   The web is a great idea.
[01:13:57.600 --> 01:14:02.880]   The distributed web is such a brilliant and clean and simple idea.
[01:14:02.880 --> 01:14:06.320]   But unfortunately, you know, it's become siloed.
[01:14:07.200 --> 01:14:10.160]   And so there's Twitter, there's Facebook, there's Google,
[01:14:10.160 --> 01:14:12.720]   there's Amazon.
[01:14:12.720 --> 01:14:17.680]   And did you see John Oliver's piece on Sunday on Big Tech?
[01:14:17.680 --> 01:14:18.560]   No, I know just to you.
[01:14:18.560 --> 01:14:19.040]   Yeah.
[01:14:19.040 --> 01:14:20.480]   It was interesting.
[01:14:20.480 --> 01:14:26.400]   I would say it's data extraction without any sort of governance or...
[01:14:26.400 --> 01:14:28.720]   not even regulation ethics, I would just say.
[01:14:28.720 --> 01:14:29.040]   Right.
[01:14:29.040 --> 01:14:29.920]   Yeah.
[01:14:29.920 --> 01:14:30.320]   Yeah.
[01:14:30.320 --> 01:14:32.720]   That would be my biggest issue with it.
[01:14:32.720 --> 01:14:33.360]   Because I do.
[01:14:33.360 --> 01:14:34.240]   I agree with you, Leo.
[01:14:34.240 --> 01:14:35.120]   It is wonderful.
[01:14:35.120 --> 01:14:38.080]   And I don't think being siloed is the worst part about it.
[01:14:38.080 --> 01:14:41.760]   I think it's the fact that people don't understand what all they're giving away when they...
[01:14:41.760 --> 01:14:42.720]   Yeah, no, I agree.
[01:14:42.720 --> 01:14:43.280]   Yeah.
[01:14:43.280 --> 01:14:44.800]   Because it isn't really siloed.
[01:14:44.800 --> 01:14:47.680]   You can have Twitter and Mastodon and Facebook.
[01:14:47.680 --> 01:14:50.880]   And you know, you can move around, you're not stuck in anyone spot.
[01:14:50.880 --> 01:14:51.600]   It's a shame.
[01:14:51.600 --> 01:14:56.560]   It would be nice if information would flow a little more freely through these walls.
[01:14:56.560 --> 01:14:57.760]   But you know...
[01:14:57.760 --> 01:15:03.280]   You see, that's what I feel is an advantage and disadvantage of it.
[01:15:03.280 --> 01:15:08.640]   It's just how free things can be where you can be anonymous.
[01:15:08.640 --> 01:15:13.440]   Or you can be, you know, who you are, actual show who you are out there.
[01:15:13.440 --> 01:15:19.680]   But at the same time, being anonymous could be quite problematic at times for certain people.
[01:15:19.680 --> 01:15:24.880]   Or being anonymous could be an advantage for certain people, depending on the situation.
[01:15:24.880 --> 01:15:26.240]   So it's...
[01:15:26.240 --> 01:15:31.040]   I love being able to have the internet and share information.
[01:15:31.040 --> 01:15:37.680]   But sometimes it's too much bad information that's getting shared that ends up being the gospel and it shouldn't have been.
[01:15:37.680 --> 01:15:43.040]   On the balance though, I agree with you, Jeff, that the internet is unbalanced, very positive.
[01:15:43.040 --> 01:15:44.160]   And still, it remains so...
[01:15:44.160 --> 01:15:45.520]   Mm-hmm. Yeah, it is.
[01:15:45.520 --> 01:15:49.200]   I think a problem, if I'm going to play my own game, because I didn't have an answer when I asked it.
[01:15:49.200 --> 01:15:54.400]   I think my answer would be the attention-based economy,
[01:15:54.400 --> 01:15:58.480]   which is a product of media imparted into the net,
[01:15:58.480 --> 01:16:01.840]   which leads to clickbait and data and everything.
[01:16:01.840 --> 01:16:03.440]   It's not a value-based economy.
[01:16:03.440 --> 01:16:06.000]   And selling your attention.
[01:16:06.000 --> 01:16:08.400]   Because they're not selling your data, they're selling your attention.
[01:16:08.400 --> 01:16:10.320]   And to do that-
[01:16:10.320 --> 01:16:11.280]   Good point.
[01:16:11.280 --> 01:16:19.200]  - they trick you and they grab your data and they have horrible experiences and they silo.
[01:16:19.200 --> 01:16:20.240]   Is it possible-
[01:16:20.240 --> 01:16:23.280]   Though the net is self-healing in that regard.
[01:16:23.280 --> 01:16:27.680]   That, you know, like Adam Smith's invisible hand, you know, fixes markets,
[01:16:28.560 --> 01:16:31.920]   Wait, wait, but we already know that the invisible hand has not worked out.
[01:16:31.920 --> 01:16:32.320]   Well...
[01:16:32.320 --> 01:16:34.080]   No, it hasn't.
[01:16:34.080 --> 01:16:38.240]   Because of like regulatory capture, the governments have turned into a weak
[01:16:38.240 --> 01:16:39.760]   enforcer of the invisible hand.
[01:16:39.760 --> 01:16:40.000]   Right.
[01:16:40.000 --> 01:16:41.040]   Right.
[01:16:41.040 --> 01:16:45.040]   His theory was that if you act in your own self-interest,
[01:16:45.040 --> 01:16:46.640]   magically everything works out.
[01:16:46.640 --> 01:16:49.360]   But I think that in the on the web anyway,
[01:16:49.360 --> 01:16:56.640]   things like, you know, this attention-driven, influencer-driven, clickbait-driven thing,
[01:16:56.640 --> 01:16:59.680]   I think that goes away after a while because people get tired of it.
[01:16:59.680 --> 01:17:00.560]   It's more-
[01:17:00.560 --> 01:17:02.720]   I think the web and the internet in general is more-
[01:17:02.720 --> 01:17:03.280]   It's like life.
[01:17:03.280 --> 01:17:06.320]   It ebbs and flows, it comes and goes.
[01:17:06.320 --> 01:17:10.000]   When bad things happen, they, you know, they will happen.
[01:17:10.000 --> 01:17:12.720]   And then they will go away and-
[01:17:12.720 --> 01:17:16.880]   I think it's going to be more and more like the ebb and flow of life itself.
[01:17:16.880 --> 01:17:18.880]   Because it really is, isn't it?
[01:17:18.880 --> 01:17:19.440]   I mean, it's-
[01:17:19.440 --> 01:17:20.400]   It's-
[01:17:20.400 --> 01:17:23.440]   It's not separate from how the world is in any way.
[01:17:24.000 --> 01:17:26.560]   And as it becomes less so, it becomes more like the world.
[01:17:26.560 --> 01:17:28.960]   Is that not true?
[01:17:28.960 --> 01:17:35.920]   I don't know if I've seen this ebb and flow quite as much.
[01:17:35.920 --> 01:17:36.480]   Well, we're-
[01:17:36.480 --> 01:17:37.120]   Well, you know, we're-
[01:17:37.120 --> 01:17:38.320]   We're younger, Stacy.
[01:17:38.320 --> 01:17:39.760]   We're fishing the middle of it.
[01:17:39.760 --> 01:17:40.320]   We've been-
[01:17:40.320 --> 01:17:41.840]   We've been ebbing and flowing a lot-
[01:17:41.840 --> 01:17:42.800]   Larter than you have.
[01:17:42.800 --> 01:17:43.120]   But we-
[01:17:43.120 --> 01:17:44.160]   You know, we can't see it,
[01:17:44.160 --> 01:17:44.720]   which is-
[01:17:44.720 --> 01:17:44.880]   I was around you.
[01:17:44.880 --> 01:17:47.040]   It's on a larger scale than we're aware of.
[01:17:47.040 --> 01:17:48.560]   But it happens, I think.
[01:17:48.560 --> 01:17:49.200]   It does happen.
[01:17:52.080 --> 01:17:54.080]   I put around for the whole of the internet.
[01:17:54.080 --> 01:17:55.280]   Yes.
[01:17:55.280 --> 01:17:55.920]   But-
[01:17:55.920 --> 01:18:00.800]   Well, you wouldn't say that it's going in one direction and one direction only.
[01:18:00.800 --> 01:18:01.920]   And that's-
[01:18:01.920 --> 01:18:01.920]   No.
[01:18:01.920 --> 01:18:03.600]   That's all that's going to happen.
[01:18:03.600 --> 01:18:04.960]   We don't know what's going to happen.
[01:18:04.960 --> 01:18:05.360]   So let me-
[01:18:05.360 --> 01:18:06.320]   Let me give you an example.
[01:18:06.320 --> 01:18:06.960]   Go ahead, Stacy.
[01:18:06.960 --> 01:18:07.680]   Sorry.
[01:18:07.680 --> 01:18:08.800]   Oh, no, go in.
[01:18:08.800 --> 01:18:09.440]   No, no, you go.
[01:18:09.440 --> 01:18:09.840]   Then I'll go.
[01:18:09.840 --> 01:18:11.600]   No, no, you go.
[01:18:11.600 --> 01:18:15.280]   Now I feel like I'm guilty.
[01:18:15.280 --> 01:18:15.680]   Cheers.
[01:18:15.680 --> 01:18:16.320]   I will say-
[01:18:16.320 --> 01:18:19.200]   You were talking about the whole click-baity,
[01:18:19.200 --> 01:18:20.000]   attention thing.
[01:18:20.480 --> 01:18:20.880]   Um,
[01:18:20.880 --> 01:18:23.040]   as a content creator,
[01:18:23.040 --> 01:18:26.800]   I believe we are all guilty of it to an extent.
[01:18:26.800 --> 01:18:28.640]   Because there are moments where-
[01:18:28.640 --> 01:18:30.240]   We have to be able to-
[01:18:30.240 --> 01:18:31.440]   to get attention to say,
[01:18:31.440 --> 01:18:33.280]   "Hey, check out this piece of art."
[01:18:33.280 --> 01:18:34.560]   You've got to sell yourself.
[01:18:34.560 --> 01:18:34.960]   Yeah.
[01:18:34.960 --> 01:18:35.760]   Right.
[01:18:35.760 --> 01:18:37.040]   You know, so that-
[01:18:37.040 --> 01:18:37.920]   But there is a line.
[01:18:37.920 --> 01:18:43.360]   You know, I shouldn't create episode 131 of Hands on Photography.
[01:18:43.360 --> 01:18:47.280]   That's all about frequency separation and say,
[01:18:47.280 --> 01:18:49.280]   "Hey, check out this beautiful model
[01:18:49.280 --> 01:18:50.640]   that I'm working with today."
[01:18:50.640 --> 01:18:51.600]   As the title.
[01:18:51.600 --> 01:18:53.120]   You know, that's just wrong.
[01:18:53.120 --> 01:18:54.960]   The internet's made of people.
[01:18:54.960 --> 01:18:57.440]   It's unreasonable to expect people to-
[01:18:57.440 --> 01:18:58.240]   Just like Lambda.
[01:18:58.240 --> 01:19:00.160]   To be nice all the time.
[01:19:00.160 --> 01:19:00.720]   Or maybe-
[01:19:00.720 --> 01:19:02.160]   I was going to say the same thing.
[01:19:02.160 --> 01:19:04.160]   So-
[01:19:04.160 --> 01:19:06.080]   But just like people,
[01:19:06.080 --> 01:19:07.280]   it comes and goes,
[01:19:07.280 --> 01:19:08.640]   and there's good and there's bad.
[01:19:08.640 --> 01:19:11.520]   And I don't think that it's-
[01:19:11.520 --> 01:19:14.080]   Yeah, we all do some marketing.
[01:19:14.080 --> 01:19:16.400]   And then, you know,
[01:19:16.400 --> 01:19:18.960]   some people think that the way they can market themselves
[01:19:18.960 --> 01:19:21.360]   is to add more value as opposed to
[01:19:21.360 --> 01:19:23.040]   trick you into reading the article.
[01:19:23.040 --> 01:19:25.520]   But there's going to be both kinds,
[01:19:25.520 --> 01:19:27.600]   and neither one's going to go away.
[01:19:27.600 --> 01:19:28.400]   It's people.
[01:19:28.400 --> 01:19:29.680]   It's just the way people are.
[01:19:29.680 --> 01:19:30.800]   And I hope-
[01:19:30.800 --> 01:19:31.760]   They see the people-
[01:19:31.760 --> 01:19:32.240]   People are-
[01:19:32.240 --> 01:19:34.480]   You know, I admit people like junk food.
[01:19:34.480 --> 01:19:35.840]   And just like they like junk food,
[01:19:35.840 --> 01:19:37.120]   they like junk content.
[01:19:37.120 --> 01:19:39.520]   I guess-
[01:19:39.520 --> 01:19:41.520]   You know what, Jeff, you'd be an expert on this.
[01:19:41.520 --> 01:19:42.960]   Because that's the-
[01:19:42.960 --> 01:19:44.000]   This is what people were saying about television.
[01:19:44.000 --> 01:19:45.440]   I mean, work with People Magazine, yes.
[01:19:45.440 --> 01:19:46.480]   I know a lot of content.
[01:19:46.480 --> 01:19:47.520]   But this is what people said about television.
[01:19:47.520 --> 01:19:52.800]   That inevitably, television would pander to the lowest common denominator.
[01:19:52.800 --> 01:19:54.080]   It's what they said about fiction.
[01:19:54.080 --> 01:19:56.800]   Is it the same thing about TikTok as well?
[01:19:56.800 --> 01:19:58.160]   Yeah.
[01:19:58.160 --> 01:19:58.640]   Because-
[01:19:58.640 --> 01:20:02.560]   Granted, there's a lot of ridiculously talent out there
[01:20:02.560 --> 01:20:03.760]   in the TikTok space.
[01:20:03.760 --> 01:20:05.440]   But man, there's-
[01:20:05.440 --> 01:20:09.520]   I get so many random text messages of a TikTok clip
[01:20:09.520 --> 01:20:11.520]   that I'm like, why does this-
[01:20:11.520 --> 01:20:14.080]   Why does this have so many millions of views?
[01:20:14.080 --> 01:20:16.080]   And it makes no sense whatsoever.
[01:20:16.080 --> 01:20:18.960]   Just something mindless of someone getting kicked in the minutes.
[01:20:18.960 --> 01:20:20.800]   So all media does this, right?
[01:20:20.800 --> 01:20:21.680]   Jeff, anytime-
[01:20:21.680 --> 01:20:22.000]   Yeah.
[01:20:22.000 --> 01:20:25.600]   Media has to live by the size of its audience.
[01:20:25.600 --> 01:20:28.720]   I mean, if you're producing opera-
[01:20:28.720 --> 01:20:30.160]   It's the attention-based marketplace.
[01:20:30.160 --> 01:20:30.640]   Right.
[01:20:30.640 --> 01:20:32.160]   Mass media is what's wrong.
[01:20:32.160 --> 01:20:32.640]   Right.
[01:20:32.640 --> 01:20:33.360]   But is it wrong?
[01:20:33.360 --> 01:20:35.440]   I mean, is it inevitably wrong?
[01:20:35.440 --> 01:20:37.040]   Yes, because the idea of the masses-
[01:20:37.040 --> 01:20:38.000]   Did TV-
[01:20:38.000 --> 01:20:39.920]   Did TV become a complete-
[01:20:39.920 --> 01:20:41.600]   And under wasteland, I would submit no.
[01:20:41.600 --> 01:20:42.320]   What-
[01:20:42.320 --> 01:20:42.640]   No.
[01:20:42.640 --> 01:20:44.160]   Well, once it moved past-
[01:20:45.920 --> 01:20:48.080]   I mean, the Golden Age TV was BS.
[01:20:48.080 --> 01:20:49.840]   It was not Golden Age, it was chunk.
[01:20:49.840 --> 01:20:50.560]   The early days.
[01:20:50.560 --> 01:20:52.080]   I think TV's better now than it was then.
[01:20:52.080 --> 01:20:53.520]   It is.
[01:20:53.520 --> 01:20:53.920]   So this-
[01:20:53.920 --> 01:20:54.560]   Not broadcast TV.
[01:20:54.560 --> 01:20:56.000]   Broadcast TV's gone down to the-
[01:20:56.000 --> 01:20:58.880]   But we have-
[01:20:58.880 --> 01:20:59.760]   Well, but that's the point.
[01:20:59.760 --> 01:21:00.080]   We have-
[01:21:00.080 --> 01:21:01.120]   Okay, so that's my-
[01:21:01.120 --> 01:21:02.880]   I guess that's what I'm saying.
[01:21:02.880 --> 01:21:04.000]   Is that-
[01:21:04.000 --> 01:21:04.960]   More choice yielded more follows-
[01:21:04.960 --> 01:21:06.880]   If you follow a single tributary, yes.
[01:21:06.880 --> 01:21:08.960]   But the ultimate river of life-
[01:21:08.960 --> 01:21:09.920]   It's a whole.
[01:21:09.920 --> 01:21:10.560]   The whole-
[01:21:10.560 --> 01:21:11.040]   Yes, absolutely great.
[01:21:11.040 --> 01:21:11.600]   Absolutely great.
[01:21:11.600 --> 01:21:14.160]   Is not going to go in one bad direction.
[01:21:14.160 --> 01:21:15.280]   In fact, I mean, what go in one-
[01:21:15.840 --> 01:21:17.600]   positive direction in the long run-
[01:21:17.600 --> 01:21:19.680]   Because people are going to ultimately want-
[01:21:19.680 --> 01:21:21.680]   Value in the long run.
[01:21:21.680 --> 01:21:22.480]   Better.
[01:21:22.480 --> 01:21:23.840]   So let me ask this question to Stacy.
[01:21:23.840 --> 01:21:24.480]   Because I want-
[01:21:24.480 --> 01:21:25.040]   I want-
[01:21:25.040 --> 01:21:26.640]   You compare your perspective to your daughter.
[01:21:26.640 --> 01:21:29.360]   When I talked to class this last term
[01:21:29.360 --> 01:21:31.280]   and redesigned the internet with Doug Rushkoff,
[01:21:31.280 --> 01:21:33.680]   I realized-
[01:21:33.680 --> 01:21:36.320]   Seven-eighths the way through the term,
[01:21:36.320 --> 01:21:37.040]   so too late.
[01:21:37.040 --> 01:21:40.160]   That I was expecting the students to-
[01:21:40.160 --> 01:21:41.440]   Compare and contrast,
[01:21:41.440 --> 01:21:43.200]   you know, kind of dreams for the internet.
[01:21:43.200 --> 01:21:45.440]   But they all came on the internet
[01:21:45.440 --> 01:21:47.840]   after it was already all walled gardens.
[01:21:47.840 --> 01:21:49.840]   They weren't there at the beginning of blogs.
[01:21:49.840 --> 01:21:53.200]   They weren't there at the beginning of these things.
[01:21:53.200 --> 01:21:55.600]   And so they accepted the internet as it is today,
[01:21:55.600 --> 01:21:58.560]   and it was hard for them to dream of an internet.
[01:21:58.560 --> 01:21:59.680]   So yes, to Dean Stacy,
[01:21:59.680 --> 01:22:01.680]   you have been on since the beginning of the internet,
[01:22:01.680 --> 01:22:03.200]   and you've seen all that.
[01:22:03.200 --> 01:22:05.760]   Your daughter only knows an internet of the last
[01:22:05.760 --> 01:22:07.200]   in years.
[01:22:07.200 --> 01:22:09.920]   And do you think she sees a very different internet
[01:22:09.920 --> 01:22:11.200]   and its possibilities than you do?
[01:22:13.200 --> 01:22:17.520]   Yes, and she treats it differently because of that.
[01:22:17.520 --> 01:22:17.920]   Good boy.
[01:22:17.920 --> 01:22:20.320]   She treats it much-
[01:22:20.320 --> 01:22:22.720]   cynically is probably-
[01:22:22.720 --> 01:22:24.800]   but she treats it as what it is.
[01:22:24.800 --> 01:22:27.360]   She doesn't have any idealized version of the internet
[01:22:27.360 --> 01:22:30.160]   of free people sharing ideas and making the world a better place.
[01:22:30.160 --> 01:22:32.000]   She just sees it as a place of commerce.
[01:22:32.000 --> 01:22:33.760]   Well, that hurts.
[01:22:33.760 --> 01:22:34.320]   Also-
[01:22:34.320 --> 01:22:35.120]   Interesting.
[01:22:35.120 --> 01:22:36.000]   No way, no way.
[01:22:36.000 --> 01:22:36.960]   No way.
[01:22:36.960 --> 01:22:38.960]   You've said before she doesn't watch TV.
[01:22:38.960 --> 01:22:40.480]   Does she get content from the internet?
[01:22:40.480 --> 01:22:42.800]   She gets content from the internet.
[01:22:42.800 --> 01:22:45.120]   She's actually currently wearing some YouTube merch.
[01:22:45.120 --> 01:22:48.160]   So there is some valuable stuff on there.
[01:22:48.160 --> 01:22:50.320]   It's not a wasteland of merchandising.
[01:22:50.320 --> 01:22:51.440]   Yeah, so it's not-
[01:22:51.440 --> 01:22:53.440]   and I don't mean of it as like this.
[01:22:53.440 --> 01:22:58.320]   But for her, it is like how probably I viewed television,
[01:22:58.320 --> 01:22:59.920]   which is it's an established thing.
[01:22:59.920 --> 01:23:02.240]   It has an established business model,
[01:23:02.240 --> 01:23:03.760]   and you can expect certain-
[01:23:03.760 --> 01:23:05.920]   you can expect it to behave in certain ways.
[01:23:05.920 --> 01:23:08.320]   But it's not all bad.
[01:23:08.320 --> 01:23:09.680]   It's a guess my point, right?
[01:23:09.680 --> 01:23:10.720]   Right, right.
[01:23:10.720 --> 01:23:12.560]   No, she doesn't think it's all bad.
[01:23:12.560 --> 01:23:15.840]   But she has no-
[01:23:15.840 --> 01:23:17.600]   Illusions.
[01:23:17.600 --> 01:23:20.080]   Illusions about it being this sort.
[01:23:20.080 --> 01:23:22.160]   When I bring up things like,
[01:23:22.160 --> 01:23:25.120]   "Oh, well, before Twitter,"
[01:23:25.120 --> 01:23:27.200]   you know, before Twitter, sorry.
[01:23:27.200 --> 01:23:29.360]   Twitter, for example,
[01:23:29.360 --> 01:23:29.760]   she's like,
[01:23:29.760 --> 01:23:32.880]   she thinks my obsession with Twitter is really weird.
[01:23:32.880 --> 01:23:34.000]   She's right, by the way.
[01:23:34.000 --> 01:23:34.240]   But-
[01:23:34.240 --> 01:23:34.720]   She-
[01:23:34.720 --> 01:23:36.240]   She may not be-
[01:23:36.240 --> 01:23:37.360]   She's like,
[01:23:37.360 --> 01:23:39.040]   "Why do you talk to these people?
[01:23:39.040 --> 01:23:40.400]   What does she give you?"
[01:23:40.400 --> 01:23:44.000]   She doesn't have the illusion that some of us old-timers have,
[01:23:44.000 --> 01:23:45.520]   that the internet was going to change the world.
[01:23:45.520 --> 01:23:46.960]   And I said, "It's just a thing for her."
[01:23:46.960 --> 01:23:48.560]   No, it's just part of life.
[01:23:48.560 --> 01:23:49.760]   It's just part of life.
[01:23:49.760 --> 01:23:51.040]   And there's good stuff in there,
[01:23:51.040 --> 01:23:51.680]   as well as bad.
[01:23:51.680 --> 01:23:53.520]   She's well aware of all the crap.
[01:23:53.520 --> 01:23:54.400]   But there's also-
[01:23:54.400 --> 01:23:56.640]   obviously, that's where she's getting content from.
[01:23:56.640 --> 01:23:59.200]   There's good stuff there too, yes?
[01:23:59.200 --> 01:24:01.200]   Yeah, yeah.
[01:24:01.200 --> 01:24:03.200]   But she's also aware that there is stuff there
[01:24:03.200 --> 01:24:05.600]   that you could not find in other places.
[01:24:05.600 --> 01:24:07.440]   Like, again, the rule of like,
[01:24:07.440 --> 01:24:08.880]   you cannot unsee something.
[01:24:09.520 --> 01:24:10.720]   We have been hammering that in
[01:24:10.720 --> 01:24:12.320]   since she was like two or three,
[01:24:12.320 --> 01:24:16.800]   simply because there are no guardrails on the internet.
[01:24:16.800 --> 01:24:20.560]   Well, I would submit that that's true in the world.
[01:24:20.560 --> 01:24:23.600]   The difference is that you have been able to live
[01:24:23.600 --> 01:24:27.040]   in a walled garden at a physical space,
[01:24:27.040 --> 01:24:29.520]   where there isn't war,
[01:24:29.520 --> 01:24:31.520]   there isn't famine, there isn't poverty.
[01:24:31.520 --> 01:24:32.000]   You could.
[01:24:32.000 --> 01:24:34.720]   You could argue that the privilege that we have-
[01:24:34.720 --> 01:24:35.680]   You have privilege.
[01:24:35.680 --> 01:24:37.200]   So she's not going to go around the corner
[01:24:37.200 --> 01:24:39.040]   and see something she can't unsee.
[01:24:39.040 --> 01:24:43.040]   The internet's bringing the whole world into her privilege space.
[01:24:43.040 --> 01:24:44.800]   So yeah, just like-
[01:24:44.800 --> 01:24:46.640]   And that's both a positive and a negative,
[01:24:46.640 --> 01:24:47.200]   because it's-
[01:24:47.200 --> 01:24:47.600]   Yes, yes.
[01:24:47.600 --> 01:24:48.560]   Look at something-
[01:24:48.560 --> 01:24:49.680]   Is that a the nest though?
[01:24:49.680 --> 01:24:51.840]   You're right, at certain age,
[01:24:51.840 --> 01:24:54.480]   under a certain age, you want to keep her away from that.
[01:24:54.480 --> 01:24:55.440]   But that's the world.
[01:24:55.440 --> 01:24:59.040]   And her little nest of privilege-
[01:24:59.040 --> 01:25:02.560]   You could see it in ways like,
[01:25:02.560 --> 01:25:06.240]   I remember the first time I read Tony Morrison's "Beloved"
[01:25:06.240 --> 01:25:07.200]   in high school.
[01:25:07.200 --> 01:25:08.560]   It was an assigned book for us.
[01:25:08.560 --> 01:25:11.280]   And I read that thing and oh my god,
[01:25:11.280 --> 01:25:18.560]   like that was my introduction to like the actual evils of slavery.
[01:25:18.560 --> 01:25:20.640]   Having been taught in Texas schools,
[01:25:20.640 --> 01:25:22.160]   that it was an economic rights issue.
[01:25:22.160 --> 01:25:22.880]   I was like,
[01:25:22.880 --> 01:25:24.480]   "We does not compute."
[01:25:24.480 --> 01:25:27.280]   And that brought me to a place of questioning.
[01:25:27.280 --> 01:25:31.040]   And that was an example of my privilege being
[01:25:31.040 --> 01:25:33.520]   like the privilege of ignoring all the bad things
[01:25:33.520 --> 01:25:36.080]   that have happened to the world that showed that to me.
[01:25:36.080 --> 01:25:36.800]   So I think-
[01:25:36.800 --> 01:25:38.880]   And that's a good thing.
[01:25:38.880 --> 01:25:44.080]   Right, that's a better way perhaps to be introduced to something
[01:25:44.080 --> 01:25:47.520]   like that, than maybe some of the- actually,
[01:25:47.520 --> 01:25:49.200]   I don't know if that's-
[01:25:49.200 --> 01:25:49.520]   You know-
[01:25:49.520 --> 01:25:52.080]   I was like some of the nicer things on Twitter, or TikTok,
[01:25:52.080 --> 01:25:52.800]   maybe is it better-
[01:25:52.800 --> 01:25:53.920]   Or sycophatic-
[01:25:53.920 --> 01:25:56.080]   You know what, Lambda,
[01:25:56.080 --> 01:25:57.360]   telling you what you want to know about the-
[01:25:57.360 --> 01:25:58.080]   Does she watch-
[01:25:58.080 --> 01:25:58.640]   Does she watch-
[01:25:58.640 --> 01:25:58.640]   Does she watch-
[01:25:58.640 --> 01:25:58.640]   Does she watch-
[01:25:58.640 --> 01:25:59.680]   Does she watch-
[01:25:59.680 --> 01:26:00.080]   Does she watch-
[01:26:00.080 --> 01:26:01.280]   TV news?
[01:26:01.280 --> 01:26:02.160]   No, probably not.
[01:26:02.160 --> 01:26:02.640]   No.
[01:26:02.640 --> 01:26:03.200]   I hope not.
[01:26:03.200 --> 01:26:05.840]   Okay, except that there's things happening.
[01:26:05.840 --> 01:26:06.960]   In Evaldi, Texas,
[01:26:06.960 --> 01:26:08.640]   that is real.
[01:26:08.640 --> 01:26:10.320]   That's not made up.
[01:26:10.320 --> 01:26:11.680]   That's important.
[01:26:11.680 --> 01:26:13.680]   But she sheltered from it.
[01:26:13.680 --> 01:26:15.840]   And I think that you cannot
[01:26:15.840 --> 01:26:18.960]   castigate the source of that information,
[01:26:18.960 --> 01:26:20.720]   saying you see there's bad stuff,
[01:26:20.720 --> 01:26:21.920]   because that's not-
[01:26:21.920 --> 01:26:22.960]   That's the real stuff.
[01:26:22.960 --> 01:26:27.440]   But I would want her not to see the pictures of dead children.
[01:26:27.440 --> 01:26:28.640]   Well, I don't want to see them either.
[01:26:28.640 --> 01:26:29.520]   But maybe-
[01:26:29.520 --> 01:26:29.840]   Right.
[01:26:29.840 --> 01:26:30.640]   Maybe we need to.
[01:26:30.640 --> 01:26:33.280]   Maybe you and I need to.
[01:26:33.280 --> 01:26:33.840]   I don't know-
[01:26:33.840 --> 01:26:34.880]   Like I know that-
[01:26:34.880 --> 01:26:35.920]   She's a child.
[01:26:35.920 --> 01:26:37.120]   She's a child.
[01:26:37.120 --> 01:26:38.160]   So you're right.
[01:26:38.160 --> 01:26:39.280]   I mean, there's different things.
[01:26:39.280 --> 01:26:39.680]   I'm like-
[01:26:39.680 --> 01:26:43.280]   But eliminating the issue of-
[01:26:43.280 --> 01:26:44.560]   Somebody's too young for it.
[01:26:44.560 --> 01:26:46.320]   Any adult,
[01:26:46.320 --> 01:26:48.240]   the internet is introducing them
[01:26:48.240 --> 01:26:53.040]   to the broad variety of things in the world,
[01:26:53.040 --> 01:26:54.320]   much of which is horrific.
[01:26:54.320 --> 01:26:55.200]   But-
[01:26:55.200 --> 01:26:56.000]   But it's also-
[01:26:56.000 --> 01:26:58.080]   That's the nest of privilege
[01:26:58.080 --> 01:26:59.120]   that isn't real,
[01:26:59.120 --> 01:27:01.200]   that it's just you're lucky enough to be living in.
[01:27:01.200 --> 01:27:03.120]   But at the same time,
[01:27:03.760 --> 01:27:04.480]   yes.
[01:27:04.480 --> 01:27:07.840]   But it's also introducing people to conspiracy theories
[01:27:07.840 --> 01:27:12.240]   and those same photos being weaponized in the opposite-
[01:27:12.240 --> 01:27:13.280]   Like this could never happen.
[01:27:13.280 --> 01:27:14.320]   This is clearly just-
[01:27:14.320 --> 01:27:15.200]   Well then, one-
[01:27:15.200 --> 01:27:15.840]   But that's right.
[01:27:15.840 --> 01:27:16.400]   I'm absurd.
[01:27:16.400 --> 01:27:18.640]   But that's an additional data point that you go-
[01:27:18.640 --> 01:27:21.120]   And there are people in the world
[01:27:21.120 --> 01:27:22.960]   who think the earth is flat.
[01:27:22.960 --> 01:27:23.360]   I mean,
[01:27:23.360 --> 01:27:26.240]   that doesn't mean you're going to fall into that hole.
[01:27:26.240 --> 01:27:28.720]   The less you're exposed to that,
[01:27:28.720 --> 01:27:30.160]   the more likely you might fall into that hole.
[01:27:30.160 --> 01:27:31.680]   You exercise your brain
[01:27:31.680 --> 01:27:33.440]   by being able to learn how to say,
[01:27:33.440 --> 01:27:34.800]   "Wow, that's BS."
[01:27:34.800 --> 01:27:36.160]   And I'm smart enough to know it
[01:27:36.160 --> 01:27:37.360]   because I got educated.
[01:27:37.360 --> 01:27:38.640]   I don't think the ever-
[01:27:38.640 --> 01:27:41.440]   Ignorance is ever going to be bliss.
[01:27:41.440 --> 01:27:43.120]   In the law-
[01:27:43.120 --> 01:27:44.480]   Or not it is bliss.
[01:27:44.480 --> 01:27:46.400]   But it's not going to be realistic
[01:27:46.400 --> 01:27:48.400]   or a good thing for society.
[01:27:48.400 --> 01:27:50.160]   So I'm just arguing that-
[01:27:50.160 --> 01:27:51.680]   Yeah, there's plenty of bad stuff.
[01:27:51.680 --> 01:27:52.960]   But on the-
[01:27:52.960 --> 01:27:53.680]   I'm with-
[01:27:53.680 --> 01:27:55.440]   Unfortunately, I'm arguing Jeff's point.
[01:27:55.440 --> 01:27:58.160]   I'm enjoying this very much.
[01:27:58.160 --> 01:27:59.280]   The day has come.
[01:27:59.280 --> 01:27:59.680]   Go.
[01:27:59.680 --> 01:28:00.400]   Keep going.
[01:28:00.400 --> 01:28:00.800]   Yeah.
[01:28:00.800 --> 01:28:01.280]   I know.
[01:28:01.280 --> 01:28:02.640]   We got you there, Leo.
[01:28:02.640 --> 01:28:03.840]   But I've always thought that.
[01:28:03.840 --> 01:28:05.280]   See, I've always thought that.
[01:28:05.280 --> 01:28:07.200]   But at the same time as I've always thought that,
[01:28:07.200 --> 01:28:10.400]   I also think it's really important to be wide-eyed
[01:28:10.400 --> 01:28:12.000]   at the-
[01:28:12.000 --> 01:28:12.800]   You know,
[01:28:12.800 --> 01:28:17.040]   what's also going on with companies like Google and Facebook and Amazon
[01:28:17.040 --> 01:28:18.320]   and their predatory ways.
[01:28:18.320 --> 01:28:20.800]   Yeah, I have to get a lot better at-
[01:28:20.800 --> 01:28:23.200]   The show has helped me do that.
[01:28:23.200 --> 01:28:26.480]   At looking at the risks
[01:28:26.480 --> 01:28:28.640]   and grappling with the risks
[01:28:28.640 --> 01:28:32.240]   and arguing that the mistake of the early internet
[01:28:32.240 --> 01:28:33.600]   was built without guardrails
[01:28:33.600 --> 01:28:34.720]   because it was too optimistic
[01:28:34.720 --> 01:28:36.160]   and it didn't see where the risks were.
[01:28:36.160 --> 01:28:39.440]   But too many guardrails has fooled us too.
[01:28:39.440 --> 01:28:42.640]   Well, I think now it's gone beyond that conversation.
[01:28:42.640 --> 01:28:44.480]   It's the world, right?
[01:28:44.480 --> 01:28:45.680]   It's what's out there.
[01:28:45.680 --> 01:28:46.960]   And it's-
[01:28:46.960 --> 01:28:49.040]   Well, it's amplified in certain bad people.
[01:28:49.040 --> 01:28:50.160]   Yeah, yeah.
[01:28:50.160 --> 01:28:51.040]   Other good course.
[01:28:51.040 --> 01:28:54.160]   Yeah, I mean, I think watching television news
[01:28:54.160 --> 01:28:56.480]   is a horrific, horrible, terrible,
[01:28:56.480 --> 01:28:59.920]   no good, very bad idea for adults as well as kids.
[01:28:59.920 --> 01:29:01.120]   Well, here's an interesting stat.
[01:29:01.120 --> 01:29:03.040]   But how are you going to get the news otherwise, right?
[01:29:03.040 --> 01:29:03.600]   How are you going to-
[01:29:03.600 --> 01:29:05.200]   Reuters Institute for the Study of Journalism
[01:29:05.200 --> 01:29:06.320]   just came out with this annual report,
[01:29:06.320 --> 01:29:06.960]   which is very good.
[01:29:06.960 --> 01:29:08.480]   It's down in my numbers,
[01:29:08.480 --> 01:29:10.480]   but it's too wonky for that.
[01:29:10.480 --> 01:29:13.440]   But it said that 19% of America-
[01:29:13.440 --> 01:29:14.800]   I could be wrong on this, I've asked them
[01:29:14.800 --> 01:29:16.240]   because there was not clear what it was,
[01:29:16.240 --> 01:29:17.600]   but 19% of America
[01:29:17.600 --> 01:29:19.440]   that subscribed to a news source.
[01:29:19.440 --> 01:29:21.440]   I saw that on your Twitter, yeah.
[01:29:21.440 --> 01:29:22.400]   And then it said that-
[01:29:22.400 --> 01:29:23.440]   Pay for it.
[01:29:23.440 --> 01:29:24.480]   Of the subscribers.
[01:29:24.480 --> 01:29:27.280]   27% paid for local news.
[01:29:27.280 --> 01:29:27.760]   Yeah.
[01:29:27.760 --> 01:29:28.320]   Which would-
[01:29:28.320 --> 01:29:29.280]   If that's the case,
[01:29:29.280 --> 01:29:32.800]   then that's 5% of America
[01:29:32.800 --> 01:29:34.240]   is paying for local news.
[01:29:34.240 --> 01:29:36.240]   And so we come up with all local news is going to save us.
[01:29:36.240 --> 01:29:37.120]   Well, at that point,
[01:29:37.120 --> 01:29:39.920]   no, this is a tiny little niche product now.
[01:29:39.920 --> 01:29:41.440]   It's nothing.
[01:29:41.440 --> 01:29:42.800]   Well, but look at-
[01:29:42.800 --> 01:29:44.480]   But when we have a club twit,
[01:29:44.480 --> 01:29:46.400]   fewer-
[01:29:46.400 --> 01:29:47.520]   Less than half of 1%
[01:29:47.520 --> 01:29:49.600]   pay club twit.
[01:29:49.600 --> 01:29:52.800]   PBS subscribers 4%, right?
[01:29:52.800 --> 01:29:53.440]   That's normal.
[01:29:53.440 --> 01:29:56.320]   Participation in things like that is normally a small-
[01:29:56.320 --> 01:29:59.440]   Well, yeah, but what I'm saying is that if that's the only-
[01:29:59.440 --> 01:30:01.680]   But all of twit is free,
[01:30:01.680 --> 01:30:03.120]   except for club twit.
[01:30:03.120 --> 01:30:03.680]   Oh, you're-
[01:30:03.680 --> 01:30:03.680]   Yeah, you're-
[01:30:03.680 --> 01:30:04.000]   Not all-
[01:30:04.000 --> 01:30:05.040]   I'm saying about pay walls.
[01:30:05.040 --> 01:30:05.920]   Yeah, yeah, yeah, yeah.
[01:30:05.920 --> 01:30:06.960]   Yeah.
[01:30:06.960 --> 01:30:08.720]   And so what is that doing to democracy?
[01:30:08.720 --> 01:30:10.240]   Well, that's why pay walls are a bad thing, right?
[01:30:10.240 --> 01:30:10.640]   That's why pay walls are a bad thing, right?
[01:30:10.640 --> 01:30:12.000]   That's where you shouldn't do the soloists in the choir.
[01:30:12.000 --> 01:30:12.800]   Yeah.
[01:30:12.800 --> 01:30:13.840]   Now, not just the choir.
[01:30:13.840 --> 01:30:14.400]   Yeah.
[01:30:14.400 --> 01:30:16.160]   And so to your point,
[01:30:16.160 --> 01:30:16.960]   what's the world-
[01:30:16.960 --> 01:30:18.400]   what's the Velton-Showong-
[01:30:18.400 --> 01:30:20.880]   that you, German word of the day?
[01:30:20.880 --> 01:30:21.840]   That you-
[01:30:21.840 --> 01:30:24.720]   that you get if you watch TV with local news,
[01:30:24.720 --> 01:30:26.320]   if you only watch TV news,
[01:30:26.320 --> 01:30:26.880]   if you only do that-
[01:30:26.880 --> 01:30:29.360]   That's getting back to when you ask me what's wrong with the internet.
[01:30:29.360 --> 01:30:30.800]   That's getting back to the silos.
[01:30:30.800 --> 01:30:31.280]   Silos.
[01:30:31.280 --> 01:30:32.720]   Yeah, the free-
[01:30:32.720 --> 01:30:34.320]   The freer the flow of information,
[01:30:34.320 --> 01:30:36.720]   the more varied the better, I think.
[01:30:36.720 --> 01:30:39.040]   But the argument of a lot of research,
[01:30:39.040 --> 01:30:40.800]   and then now, and this is-
[01:30:40.800 --> 01:30:42.960]   I mentioned this book by Axel Brunz,
[01:30:42.960 --> 01:30:46.160]   is the fallacy of the filter bubble and the echo chamber,
[01:30:46.160 --> 01:30:50.640]   is that in fact we get bombarded with a lot more now on the internet and social.
[01:30:50.640 --> 01:30:52.320]   That's what pisses off people.
[01:30:52.320 --> 01:30:53.920]   That's what says, "I don't like those people,
[01:30:53.920 --> 01:30:55.120]   and now they're in front of my face,
[01:30:55.120 --> 01:30:56.080]   and I'm going to get angrier."
[01:30:56.080 --> 01:31:00.240]   And that it's not that we're sheltered from things,
[01:31:00.240 --> 01:31:02.000]   that in fact we're seeing more of the world,
[01:31:02.000 --> 01:31:03.440]   to your point, Leo.
[01:31:03.440 --> 01:31:06.000]   And that that's what brings out people's fears and animus,
[01:31:06.000 --> 01:31:08.080]   because they're not ready for it.
[01:31:08.080 --> 01:31:11.040]   We haven't been exposed to it in the Walpert Cronkite world.
[01:31:11.040 --> 01:31:12.960]   And Stacy, as a good parent,
[01:31:12.960 --> 01:31:14.400]   you're not-
[01:31:14.400 --> 01:31:14.720]   I-
[01:31:14.720 --> 01:31:14.960]   you're-
[01:31:14.960 --> 01:31:16.400]   you know, initially when they're young,
[01:31:16.400 --> 01:31:17.760]   maybe you're shielding their eyes,
[01:31:17.760 --> 01:31:18.160]   but-
[01:31:18.160 --> 01:31:19.600]   but more important,
[01:31:19.600 --> 01:31:23.600]   more valuable is helping them look at it in a way that is informing to them,
[01:31:23.600 --> 01:31:28.080]   and helps them understand what they're seeing critically.
[01:31:28.080 --> 01:31:29.120]   Right?
[01:31:29.120 --> 01:31:29.920]   Yes, but you have-
[01:31:29.920 --> 01:31:30.320]   this-
[01:31:30.320 --> 01:31:35.040]   right now there's a fine line I would say between doing that and really depressing the hell out of them.
[01:31:35.040 --> 01:31:35.440]   I agree.
[01:31:35.440 --> 01:31:36.160]   Yeah, yeah.
[01:31:36.160 --> 01:31:36.400]   Yeah.
[01:31:36.400 --> 01:31:36.960]   So-
[01:31:36.960 --> 01:31:37.440]   I agree.
[01:31:37.440 --> 01:31:38.720]   Especially these days.
[01:31:38.720 --> 01:31:39.280]   Special.
[01:31:39.280 --> 01:31:41.600]   Well, and it's not just true for kids, it's true for all of us.
[01:31:41.600 --> 01:31:42.400]   Oh, exactly.
[01:31:42.400 --> 01:31:42.720]   Yeah.
[01:31:42.720 --> 01:31:43.760]   But-
[01:31:43.760 --> 01:31:44.720]   but you don't-
[01:31:44.720 --> 01:31:45.440]   so-
[01:31:45.440 --> 01:31:46.880]   you know, I always have this-
[01:31:46.880 --> 01:31:50.320]   discussion in my mind about video games or social media with young people.
[01:31:50.320 --> 01:31:52.080]   If you-
[01:31:52.080 --> 01:31:53.440]   if you keep them away from it,
[01:31:53.440 --> 01:31:55.760]   then when they grow up and they're suddenly exposed to it,
[01:31:55.760 --> 01:31:56.320]   they have had-
[01:31:56.320 --> 01:31:58.480]   they have no defenses against it at all.
[01:31:58.480 --> 01:31:59.040]   Well, I was just-
[01:31:59.040 --> 01:32:00.160]   so you gotta-
[01:32:00.160 --> 01:32:00.560]   some-
[01:32:00.560 --> 01:32:02.080]   some exposure is kind of important.
[01:32:02.080 --> 01:32:04.800]   We don't talk about this very much,
[01:32:04.800 --> 01:32:07.200]   but the internet and some of the tools,
[01:32:07.200 --> 01:32:09.040]   like Google Docs or Neat,
[01:32:09.040 --> 01:32:09.840]   or things like that,
[01:32:09.840 --> 01:32:13.120]   have actually made it very easy to collaborate on activism-
[01:32:13.120 --> 01:32:13.440]   good.
[01:32:13.440 --> 01:32:13.920]   For kids-
[01:32:13.920 --> 01:32:14.480]   right.
[01:32:14.480 --> 01:32:15.600]   in a way that-
[01:32:15.600 --> 01:32:17.840]   I don't- we- we really don't talk about it on this show,
[01:32:17.840 --> 01:32:18.560]   but-
[01:32:18.560 --> 01:32:19.040]   you know,
[01:32:19.040 --> 01:32:20.720]   the types of movements-
[01:32:20.720 --> 01:32:22.960]   like if you look at what happened post-Parkland,
[01:32:22.960 --> 01:32:25.840]   if you look at what happens with like
[01:32:25.840 --> 01:32:29.600]   political activism around climate change among the youths-
[01:32:29.600 --> 01:32:35.040]   there's a lot happening in the internet has made that much more accessible.
[01:32:35.040 --> 01:32:36.640]   It's been easier to find those people.
[01:32:36.640 --> 01:32:38.400]   It's easier to collaborate
[01:32:38.400 --> 01:32:39.920]   and to actually take action.
[01:32:39.920 --> 01:32:43.280]   Like, I know my daughter signed up on some Google Docs to actually call
[01:32:44.160 --> 01:32:46.400]   the local law enforcement local-
[01:32:46.400 --> 01:32:48.800]   Congress-
[01:32:48.800 --> 01:32:53.040]   the local state Congress on some legislation she felt
[01:32:53.040 --> 01:32:53.520]   was important,
[01:32:53.520 --> 01:32:54.320]   but I-
[01:32:54.320 --> 01:32:58.320]   it is also hard to find all the nuances around some of those things.
[01:32:58.320 --> 01:33:02.480]   So like she was against a particular dam here,
[01:33:02.480 --> 01:33:04.240]   but we had to talk about like,
[01:33:04.240 --> 01:33:06.400]   well, is protecting salmon?
[01:33:06.400 --> 01:33:07.600]   Is it worth, you know,
[01:33:07.600 --> 01:33:09.280]   depriving some indigenous people-
[01:33:09.280 --> 01:33:10.480]   what their fishing rights?
[01:33:10.480 --> 01:33:11.840]   Because in this case it would.
[01:33:11.840 --> 01:33:13.440]   And yeah.
[01:33:14.240 --> 01:33:16.320]   Then we got into the whole world sucks in this complex.
[01:33:16.320 --> 01:33:19.040]   It's hard because there isn't an obvious like,
[01:33:19.040 --> 01:33:22.320]   I guess in some in some distant
[01:33:22.320 --> 01:33:24.480]   nostalgic past, you could say,
[01:33:24.480 --> 01:33:25.760]   well, there's right and there's wrong.
[01:33:25.760 --> 01:33:28.400]   And here's the past and it's all very straightforward.
[01:33:28.400 --> 01:33:30.160]   But we don't live in that kind of world anymore.
[01:33:30.160 --> 01:33:31.760]   And there's a lot of grays and it's hard to know.
[01:33:31.760 --> 01:33:33.760]   I don't know what you tell kids.
[01:33:33.760 --> 01:33:35.600]   You just prepare them as best you can.
[01:33:35.600 --> 01:33:36.240]   I-
[01:33:36.240 --> 01:33:39.520]   I think that I used to say that my parents did me a disservice
[01:33:39.520 --> 01:33:41.440]   and it's the same disservice that I do my own children.
[01:33:42.240 --> 01:33:46.320]   Is I think protecting them from how many a-holes there are in the world.
[01:33:46.320 --> 01:33:50.960]   And that that's going to be unusual when you see that out in the world.
[01:33:50.960 --> 01:33:53.200]   And that and that and that most people are all good.
[01:33:53.200 --> 01:33:57.040]   And most people I think are, but how do you deal with
[01:33:57.040 --> 01:33:59.920]   the difficult people?
[01:33:59.920 --> 01:34:00.240]   Right.
[01:34:00.240 --> 01:34:06.000]   Well, my mom told me it was all of you, so I was well prepared.
[01:34:10.240 --> 01:34:11.840]   Except me, right? Because my-
[01:34:11.840 --> 01:34:13.280]   So that's Stacy's.
[01:34:13.280 --> 01:34:14.240]   That's great.
[01:34:14.240 --> 01:34:17.040]   But the rest of them are horrible.
[01:34:17.040 --> 01:34:19.440]   Jerks.
[01:34:19.440 --> 01:34:22.720]   No, I was raised and it took me a while to realize it's in a very kind of
[01:34:22.720 --> 01:34:26.640]   snobbish fashion that we are somehow better than almost everybody else.
[01:34:26.640 --> 01:34:30.880]   Was that the academe too?
[01:34:30.880 --> 01:34:31.920]   It's probably the academe.
[01:34:31.920 --> 01:34:32.800]   Yeah, it was probably-
[01:34:32.800 --> 01:34:34.560]   My father's a professor and he's probably like,
[01:34:34.560 --> 01:34:36.960]   yeah, well, you know, those people are not too bright.
[01:34:37.520 --> 01:34:40.320]   But yeah, it was very much the case.
[01:34:40.320 --> 01:34:41.600]   Did you ever-
[01:34:41.600 --> 01:34:42.560]   I never asked you-
[01:34:42.560 --> 01:34:43.200]   I never asked you-
[01:34:43.200 --> 01:34:43.600]   Go ahead.
[01:34:43.600 --> 01:34:44.160]   You go ahead.
[01:34:44.160 --> 01:34:49.840]   You were saying your parents shielded you from the jackasses of the world.
[01:34:49.840 --> 01:34:53.360]   And you were done pretty much have done the same-
[01:34:53.360 --> 01:34:55.360]   had done the same with your children.
[01:34:55.360 --> 01:34:58.800]   Is it because you weren't attacked this often?
[01:34:58.800 --> 01:35:00.480]   Or why was that?
[01:35:00.480 --> 01:35:05.840]   Because I look at my household and I've been straight up with every single person
[01:35:05.840 --> 01:35:08.080]   and say, "Hey, this person is a jerk.
[01:35:08.080 --> 01:35:08.960]   This person is a jerk.
[01:35:08.960 --> 01:35:09.760]   This person is great.
[01:35:09.760 --> 01:35:10.640]   This person is awesome."
[01:35:10.640 --> 01:35:15.760]   So, and so forth, because of my personal experience and I want them to see what it feels
[01:35:15.760 --> 01:35:17.520]   like on both sides of that spectrum.
[01:35:17.520 --> 01:35:19.440]   I don't know, man.
[01:35:19.440 --> 01:35:20.000]   It's a good question.
[01:35:20.000 --> 01:35:23.920]   But I think you also- we've talked about this before and you also had to
[01:35:23.920 --> 01:35:26.480]   tell your kids about risks that they would face.
[01:35:26.480 --> 01:35:27.440]   Yeah.
[01:35:27.440 --> 01:35:31.040]   And I faced fewer risks because I'm pretty much-
[01:35:31.040 --> 01:35:31.440]   Oh, yeah.
[01:35:31.440 --> 01:35:32.240]   Yeah.
[01:35:32.240 --> 01:35:34.160]   Did you tell your kids that job?
[01:35:35.120 --> 01:35:35.600]   Not enough.
[01:35:35.600 --> 01:35:36.880]   Not enough.
[01:35:36.880 --> 01:35:36.880]   Oh.
[01:35:36.880 --> 01:35:38.880]   Because we talked about that a lot.
[01:35:38.880 --> 01:35:39.520]   Yeah.
[01:35:39.520 --> 01:35:42.720]   Well, we did later, but not enough early on, I would say.
[01:35:42.720 --> 01:35:46.000]   I also think back- I mean, you know, there's-
[01:35:46.000 --> 01:35:48.640]   And I'm not judging you, by the way.
[01:35:48.640 --> 01:35:49.120]   Just just-
[01:35:49.120 --> 01:35:49.520]   No, I don't.
[01:35:49.520 --> 01:35:49.920]   I don't.
[01:35:49.920 --> 01:35:49.920]   I don't.
[01:35:49.920 --> 01:35:49.920]   I don't.
[01:35:49.920 --> 01:35:50.720]   I say that's cool.
[01:35:50.720 --> 01:35:51.280]   You're laughing.
[01:35:51.280 --> 01:35:53.840]   You're going to laugh when I'm about to say I'm actually shy.
[01:35:53.840 --> 01:35:56.400]   But I'm the outgoing one of my family.
[01:35:56.400 --> 01:35:57.680]   There are several reasons to my family.
[01:35:57.680 --> 01:36:01.840]   I don't bring in to anything because they don't like a lot of social stuff.
[01:36:01.840 --> 01:36:07.120]   And I look back on my mother who was scared to death of people as I now realize.
[01:36:07.120 --> 01:36:13.840]   And I didn't understand social anxiety in my parents or in my family now,
[01:36:13.840 --> 01:36:14.800]   or in people around me.
[01:36:14.800 --> 01:36:18.720]   And I think that's an interesting problem too, is that there's reasons to be anxious about people.
[01:36:18.720 --> 01:36:20.880]   There's a holes out there.
[01:36:20.880 --> 01:36:24.960]   All of this is insight you gain with age, I think, Jeff.
[01:36:24.960 --> 01:36:25.440]   Yes?
[01:36:25.440 --> 01:36:26.000]   Absolutely.
[01:36:26.000 --> 01:36:26.320]   Yeah.
[01:36:26.320 --> 01:36:27.840]   I've noticed that for myself.
[01:36:27.840 --> 01:36:30.640]   One hopes that you become wise is a strong word,
[01:36:30.640 --> 01:36:32.400]   but you become a little smarter as you get.
[01:36:32.400 --> 01:36:33.440]   But you're always a good person.
[01:36:33.440 --> 01:36:34.880]   Should we have our kids better for that?
[01:36:34.880 --> 01:36:35.280]   I don't.
[01:36:35.280 --> 01:36:38.240]   You know, when you have kids, you're young and dumb.
[01:36:38.240 --> 01:36:38.800]   Yeah.
[01:36:38.800 --> 01:36:41.920]   And you know, I think we all do the best we can.
[01:36:41.920 --> 01:36:42.720]   Not an answer.
[01:36:42.720 --> 01:36:43.120]   Yeah.
[01:36:43.120 --> 01:36:44.160]   But it's not good enough.
[01:36:44.160 --> 01:36:45.360]   Ever.
[01:36:45.360 --> 01:36:46.960]   Oh boy, I screwed up a lot.
[01:36:46.960 --> 01:36:47.520]   Yes.
[01:36:47.520 --> 01:36:51.040]   Everybody, if you're honest as a parent,
[01:36:51.040 --> 01:36:53.120]   probably has those moments where you go away.
[01:36:53.120 --> 01:36:54.880]   I could have done that a little better.
[01:36:54.880 --> 01:36:56.320]   That was cool.
[01:36:56.320 --> 01:36:56.720]   Oh yeah.
[01:36:56.720 --> 01:36:57.840]   That was a bad scene.
[01:36:57.840 --> 01:36:58.720]   That was a bad scene.
[01:36:58.720 --> 01:37:00.960]   Sometimes I feel a little weirdo, bless her heart.
[01:37:00.960 --> 01:37:01.760]   Yeah.
[01:37:01.760 --> 01:37:03.680]   Hey, let's take a little break.
[01:37:03.680 --> 01:37:07.280]   I got to hit boys, you know, for up 10 years.
[01:37:07.280 --> 01:37:09.600]   And now I have a little girl to help raise, boy.
[01:37:09.600 --> 01:37:11.120]   Who bless her heart?
[01:37:11.120 --> 01:37:13.760]   Different techniques are required.
[01:37:13.760 --> 01:37:15.280]   Clearly.
[01:37:15.280 --> 01:37:17.360]   You can't call her a hard head for one thing.
[01:37:17.360 --> 01:37:19.840]   I don't know about a little weirdo.
[01:37:19.840 --> 01:37:20.640]   I'm like, oh.
[01:37:20.640 --> 01:37:21.840]   A little weirdo.
[01:37:21.840 --> 01:37:22.480]   That's okay.
[01:37:22.480 --> 01:37:25.680]   And say with love, that's what that.
[01:37:25.680 --> 01:37:26.560]   That's what it is.
[01:37:26.560 --> 01:37:27.520]   I did hear the love.
[01:37:27.520 --> 01:37:28.240]   Yeah.
[01:37:28.240 --> 01:37:28.800]   Yes, I did.
[01:37:28.800 --> 01:37:29.840]   It's not the words.
[01:37:29.840 --> 01:37:31.440]   It's what's behind the words.
[01:37:31.440 --> 01:37:34.240]   Our show today brought to you by HPE,
[01:37:34.240 --> 01:37:35.920]   Hewlett Packard Enterprise GreenLake.
[01:37:35.920 --> 01:37:39.360]   Orchestrated by the experts at CDW.
[01:37:39.360 --> 01:37:41.760]   Yesterday's infrastructure is not going to help you
[01:37:41.760 --> 01:37:44.400]   meet the demands of tomorrow's innovation.
[01:37:44.400 --> 01:37:49.680]   And now, more than ever, you have to quickly adapt to change.
[01:37:49.680 --> 01:37:52.400]   The consumption-oriented model of the public cloud
[01:37:52.400 --> 01:37:53.760]   is revolutionized.
[01:37:53.760 --> 01:37:56.240]   How organizations think about infrastructure.
[01:37:56.240 --> 01:37:58.560]   Organizations are now demanding the speed and agility
[01:37:58.560 --> 01:38:00.560]   of a public cloud experience.
[01:38:00.560 --> 01:38:02.640]   But many workloads remain on-prem.
[01:38:02.640 --> 01:38:06.800]   For a lot of reasons, complicated legacy applications,
[01:38:06.800 --> 01:38:08.960]   data sovereignty, compliance.
[01:38:08.960 --> 01:38:13.520]   The seriousness of migrating sensitive data.
[01:38:13.520 --> 01:38:17.040]   And of course, IT is caught right in the middle of all this,
[01:38:17.040 --> 01:38:18.640]   trying to satisfy the demand,
[01:38:18.640 --> 01:38:20.800]   the growth of their organization,
[01:38:20.800 --> 01:38:23.840]   struggling to manage disparate data.
[01:38:23.840 --> 01:38:26.000]   And of course, it's always the case.
[01:38:26.000 --> 01:38:27.760]   You got to do it all at budget.
[01:38:27.760 --> 01:38:31.600]   HPE, GreenLake, and CDW can help.
[01:38:31.600 --> 01:38:34.560]   They can provide a seamless, scalable cloud experience
[01:38:34.560 --> 01:38:36.400]   across your entire organization
[01:38:36.400 --> 01:38:38.720]   that you can easily manage from anywhere.
[01:38:38.720 --> 01:38:42.080]   No matter where your data and applications live on-prem,
[01:38:42.080 --> 01:38:44.720]   public and private clouds, or all three,
[01:38:44.720 --> 01:38:49.200]   trust HPE and CDW to automate your processes
[01:38:49.200 --> 01:38:51.920]   so that you can focus on the next big thing.
[01:38:51.920 --> 01:38:54.720]   CDW can help you get the most out of your unique
[01:38:54.720 --> 01:38:56.160]   and complex data needs
[01:38:56.160 --> 01:38:58.160]   by assessing your organization's needs
[01:38:58.160 --> 01:39:01.200]   and designing and implementing a tailored HPE,
[01:39:01.200 --> 01:39:02.800]   GreenLake solution
[01:39:02.800 --> 01:39:04.400]   to modernize your business
[01:39:04.400 --> 01:39:06.560]   and meet your digital transformation goals.
[01:39:06.560 --> 01:39:09.840]   Now, once implemented CDW also provides ongoing management,
[01:39:09.840 --> 01:39:13.120]   freeing up your staff to focus on the fun stuff,
[01:39:13.120 --> 01:39:14.320]   on innovation.
[01:39:14.320 --> 01:39:16.720]   Let CDW do the hard work.
[01:39:16.720 --> 01:39:18.320]   Seamless cloud experience,
[01:39:18.320 --> 01:39:21.360]   of course, across all your organization's apps and data,
[01:39:21.360 --> 01:39:24.080]   thanks to the as-a-service model that meets you at the edge,
[01:39:24.080 --> 01:39:27.360]   you get scalability, instant increase in capacity
[01:39:27.360 --> 01:39:28.320]   when you need it,
[01:39:28.320 --> 01:39:32.240]   so you've got the flexibility to match your growth and meet demand.
[01:39:32.240 --> 01:39:35.600]   And of course, streamlined management of everything,
[01:39:35.600 --> 01:39:39.120]   all your data and applications with a single platform,
[01:39:39.120 --> 01:39:41.120]   providing simplified operations,
[01:39:41.120 --> 01:39:42.880]   accessibility from everywhere,
[01:39:42.880 --> 01:39:46.480]   and again, giving you back time to focus on innovation.
[01:39:46.480 --> 01:39:49.520]   CDW not only helps assess an organization's needs
[01:39:49.520 --> 01:39:52.640]   and build a unique HPE, GreenLake solution,
[01:39:52.640 --> 01:39:54.080]   they also remain an active partner
[01:39:54.080 --> 01:39:56.720]   and provide support throughout the management process.
[01:39:56.720 --> 01:39:58.640]   They're there when you need them.
[01:39:58.640 --> 01:40:01.760]   CDW experts also bring decades of experience
[01:40:01.760 --> 01:40:03.280]   designing, orchestrating,
[01:40:03.280 --> 01:40:06.240]   and managing strategic cloud solutions
[01:40:06.240 --> 01:40:09.520]   that help you unleash the full potential of your investments,
[01:40:09.520 --> 01:40:13.920]   giving you back time to, yes, innovate and dream bigger.
[01:40:13.920 --> 01:40:15.760]   For a seamless cloud experience,
[01:40:15.760 --> 01:40:19.040]   trust HPE and IT orchestration
[01:40:19.040 --> 01:40:21.760]   by CDW people who get it.
[01:40:21.760 --> 01:40:23.440]   Get it?
[01:40:23.440 --> 01:40:27.760]   Learn more at CDW.com/HPE.
[01:40:27.760 --> 01:40:31.440]   That's CDW.com/HPE.
[01:40:31.440 --> 01:40:35.760]   We thank CDW and HPE for their support of this week and Google.
[01:40:35.760 --> 01:40:38.000]   We have, you know what,
[01:40:38.000 --> 01:40:40.560]   this has been such a great philosophical conversation.
[01:40:40.560 --> 01:40:41.360]   We haven't really covered-
[01:40:41.360 --> 01:40:42.640]   This is the perfect kind of twig discussion.
[01:40:42.640 --> 01:40:43.840]   Any of the news, yeah.
[01:40:43.840 --> 01:40:48.560]   Meta, weirdly, has hit the brakes, apparently,
[01:40:48.560 --> 01:40:51.520]   according to Wired, on hardware,
[01:40:51.520 --> 01:40:55.200]   Portal AR Glasses, other hardware?
[01:40:55.200 --> 01:40:56.800]   Consumer hardware, especially.
[01:40:56.800 --> 01:40:59.040]   Portal is going to stay for business.
[01:40:59.040 --> 01:41:00.000]   I wonder, you know-
[01:41:00.000 --> 01:41:02.960]   I mean, Microsoft's done the same thing with HoloLens.
[01:41:02.960 --> 01:41:05.440]   It's become an enterprise product, perhaps,
[01:41:05.440 --> 01:41:06.080]   a product-
[01:41:06.080 --> 01:41:06.640]   Glass.
[01:41:06.640 --> 01:41:07.600]   Glass, same thing.
[01:41:07.600 --> 01:41:08.160]   Cool.
[01:41:08.160 --> 01:41:08.400]   Yeah.
[01:41:08.400 --> 01:41:12.640]   Microsoft's HoloLens though, the guy who let it left,
[01:41:12.640 --> 01:41:14.320]   and they're not sure-
[01:41:14.320 --> 01:41:17.360]   They split up the divisions into two different divisions.
[01:41:17.360 --> 01:41:18.480]   You're right, the army's not-
[01:41:18.480 --> 01:41:18.880]   Happy?
[01:41:18.880 --> 01:41:20.800]   Yeah, so-
[01:41:20.800 --> 01:41:22.560]   I don't know how HoloLens is going to go.
[01:41:22.560 --> 01:41:24.720]   If there's a commonality,
[01:41:24.720 --> 01:41:28.880]   it's that maybe the metaverse isn't all that, right?
[01:41:28.880 --> 01:41:31.120]   And I can't tell-
[01:41:31.120 --> 01:41:32.800]   You know, like, I feel like forever,
[01:41:32.800 --> 01:41:38.080]   I've always been on the front edge of being excited about different tech things.
[01:41:38.080 --> 01:41:40.160]   So like cloud computing, IoT,
[01:41:40.160 --> 01:41:43.680]   and I'd be like telling people why this is a big deal and explaining it.
[01:41:43.680 --> 01:41:45.440]   I don't feel that way about the metaverse,
[01:41:45.440 --> 01:41:48.000]   but I still feel like everybody's pushed in the metaverse on us,
[01:41:48.000 --> 01:41:49.920]   and I truly don't get it.
[01:41:49.920 --> 01:41:52.960]   So I don't know if I'm behind,
[01:41:52.960 --> 01:41:55.680]   or if there's just not a compelling anything.
[01:41:55.680 --> 01:41:56.560]   I don't-
[01:41:56.560 --> 01:41:58.800]   Meta NFTs, I think.
[01:41:58.800 --> 01:41:59.520]   Well, because I think-
[01:41:59.520 --> 01:42:01.840]   I'll try this theory out.
[01:42:01.840 --> 01:42:04.240]   And sorry, it's another Gutenberg moment,
[01:42:04.240 --> 01:42:08.080]   but it took time for technology to become boring,
[01:42:08.080 --> 01:42:11.360]   and then what people did with it became interesting.
[01:42:11.360 --> 01:42:14.400]   And I think there's this hunger by Silicon Valley by the technologists.
[01:42:14.400 --> 01:42:15.760]   No, we got to make it more interesting again.
[01:42:15.760 --> 01:42:16.800]   We got to make it more interesting.
[01:42:16.800 --> 01:42:18.560]   And I don't think that's what's going to be interesting.
[01:42:18.560 --> 01:42:20.720]   I think what's going to be interesting is the-
[01:42:20.720 --> 01:42:24.720]   is Cervantes making the modern novel and Montaigne the essay,
[01:42:24.720 --> 01:42:26.720]   and the guy in Strasbourg, the newspaper,
[01:42:26.720 --> 01:42:28.560]   and what are people invent with this stuff?
[01:42:28.560 --> 01:42:32.880]   But remember, these are corporations driven by shareholders
[01:42:32.880 --> 01:42:36.800]   who want to see growth, and so from their point of view,
[01:42:36.800 --> 01:42:37.920]   they're not thinking,
[01:42:37.920 --> 01:42:40.720]   "Oh, let's see how technology is more sensitive to the punch."
[01:42:40.720 --> 01:42:41.760]   Oh, I'm trying to pass them.
[01:42:41.760 --> 01:42:42.560]   Way past them.
[01:42:42.560 --> 01:42:43.040]   Oh, I agree.
[01:42:43.040 --> 01:42:44.080]   Because all their-
[01:42:44.080 --> 01:42:45.120]   Their only job is-
[01:42:45.120 --> 01:42:46.880]   What's the next big thing?
[01:42:46.880 --> 01:42:48.960]   And how can we make sure we're there?
[01:42:48.960 --> 01:42:50.800]   And they, of course, are faced with the innovators,
[01:42:50.800 --> 01:42:52.880]   dilemmas, historically hard for incumbents
[01:42:52.880 --> 01:42:54.480]   to get the next big thing.
[01:42:54.480 --> 01:42:57.680]   Apple, 52% of its revenue comes from the iPhone.
[01:42:57.680 --> 01:43:01.040]   How do you find something better than that?
[01:43:01.040 --> 01:43:02.480]   You know, how do you meet the-
[01:43:02.480 --> 01:43:04.640]   Well, I don't care about those companies.
[01:43:04.640 --> 01:43:05.840]   Oh, no, no, I'm saying that.
[01:43:05.840 --> 01:43:10.080]   But that's why the tech is doing what it's doing,
[01:43:10.080 --> 01:43:13.840]   is they're not thinking about some broader cultural.
[01:43:13.840 --> 01:43:15.920]   They're going to say, "Where's our Montaigne?"
[01:43:15.920 --> 01:43:18.480]   But Jack's trying to go back
[01:43:18.480 --> 01:43:21.440]   and recapture something that was before,
[01:43:21.440 --> 01:43:22.240]   in terms of the ability-
[01:43:22.240 --> 01:43:24.880]   And that ain't going to happen either, right?
[01:43:24.880 --> 01:43:25.600]   I don't know.
[01:43:25.600 --> 01:43:26.160]   I don't know.
[01:43:26.160 --> 01:43:32.160]   And then as far as you go, Stacey, I agree with you.
[01:43:32.160 --> 01:43:33.200]   This is always-
[01:43:33.200 --> 01:43:35.440]   This is the tech reporter's dilemma,
[01:43:35.440 --> 01:43:38.080]   is what I'm seeing really all that,
[01:43:38.080 --> 01:43:39.840]   or is it just more BS?
[01:43:39.840 --> 01:43:40.880]   When Charlie Rose,
[01:43:40.880 --> 01:43:44.640]   when Jeff Bezos 10 years ago opens the door and says,
[01:43:44.640 --> 01:43:47.440]   "See, Amazon's going to be doing drones."
[01:43:47.440 --> 01:43:51.040]   And Charlie Rose goes, "Wow, that's amazing."
[01:43:51.040 --> 01:43:54.000]   At the time we all mocked him,
[01:43:54.000 --> 01:43:56.400]   we all said, "Oh, Charlie, you fell for it."
[01:43:56.400 --> 01:44:01.680]   And yet, this week, Amazon is starting drone deliveries
[01:44:01.680 --> 01:44:02.880]   in a bunch more cities.
[01:44:02.880 --> 01:44:05.840]   It took them a longer time than they thought it would.
[01:44:05.840 --> 01:44:06.960]   Yeah, it's been a little while.
[01:44:06.960 --> 01:44:07.760]   Yeah, it's so short.
[01:44:07.760 --> 01:44:09.280]   Are we still skeptical about it though,
[01:44:09.280 --> 01:44:11.680]   or are we now saying, "Well, maybe they were right."
[01:44:11.680 --> 01:44:12.880]   Maybe Charlie wasn't so wrong.
[01:44:12.880 --> 01:44:15.440]   Yeah, drone delivery has a place.
[01:44:15.440 --> 01:44:17.200]   But I felt like we could see that.
[01:44:17.200 --> 01:44:18.880]   I don't know.
[01:44:18.880 --> 01:44:20.640]   A lot of us made fun of Charlie Rose.
[01:44:20.640 --> 01:44:23.680]   Like, Jeff pulled the wool over your eyes.
[01:44:23.680 --> 01:44:26.000]   And it may be actually that he did,
[01:44:26.000 --> 01:44:27.520]   because that was a long time ago.
[01:44:27.520 --> 01:44:32.080]   It seems like it was sold as something mainstream.
[01:44:32.080 --> 01:44:34.960]   Yeah, like, just like Elon's saying,
[01:44:34.960 --> 01:44:37.280]   next year, you won't ever-
[01:44:37.280 --> 01:44:37.920]   Dancing Robot, yeah.
[01:44:37.920 --> 01:44:38.560]   Yeah.
[01:44:38.560 --> 01:44:41.200]   But like what Ms. Stacey said is,
[01:44:41.200 --> 01:44:43.600]   yeah, there is a place for drone delivery.
[01:44:43.600 --> 01:44:46.000]   But I didn't like how it seems like,
[01:44:46.000 --> 01:44:49.200]   if I go to order a pair of sneakers or new tripod
[01:44:49.200 --> 01:44:50.560]   that I ordered today,
[01:44:50.560 --> 01:44:52.560]   I should be able to get this in the same day
[01:44:52.560 --> 01:44:54.240]   if I hit Deliver by a Drone.
[01:44:54.240 --> 01:44:55.680]   That's not possible everywhere.
[01:44:55.680 --> 01:44:57.360]   That's only in certain areas.
[01:44:57.360 --> 01:44:59.120]   And it's not for everything.
[01:44:59.120 --> 01:45:01.760]   You know, a drone can't carry everything you order.
[01:45:01.760 --> 01:45:03.920]   Well, it's in Lockford, California.
[01:45:03.920 --> 01:45:06.000]   I don't even know where it's-
[01:45:06.000 --> 01:45:06.560]   Where?
[01:45:06.560 --> 01:45:07.760]   What a lock of bird.
[01:45:07.760 --> 01:45:09.680]   The company announced,
[01:45:09.680 --> 01:45:10.880]   Amazon announced it's going to launch,
[01:45:10.880 --> 01:45:12.720]   it's inaugural drone delivery service
[01:45:12.720 --> 01:45:15.280]   in the town of Lockford, California.
[01:45:15.280 --> 01:45:17.600]   Later this year, after it receives the green light
[01:45:17.600 --> 01:45:19.520]   from the FAA, obviously.
[01:45:19.520 --> 01:45:20.560]   Can't do it.
[01:45:20.560 --> 01:45:22.560]   Of course, there is nothing around it, man.
[01:45:22.560 --> 01:45:23.760]   Yeah, yeah.
[01:45:23.760 --> 01:45:24.480]   Well, that's- Where is it?
[01:45:24.480 --> 01:45:25.360]   Are you looking at it?
[01:45:25.360 --> 01:45:27.120]   I don't know where- I've never even heard of it.
[01:45:27.120 --> 01:45:28.480]   A little north of Stockton.
[01:45:28.480 --> 01:45:29.360]   Oh, okay.
[01:45:29.360 --> 01:45:31.280]   Lockford residents will be the first to receive
[01:45:31.280 --> 01:45:34.400]   prime delivery packages delivered to their backyards
[01:45:34.400 --> 01:45:35.280]   via drone.
[01:45:36.000 --> 01:45:37.280]   Not everything, obviously.
[01:45:37.280 --> 01:45:38.320]   It can't be every-
[01:45:38.320 --> 01:45:40.240]   You're not going to get that inflatable pool.
[01:45:40.240 --> 01:45:43.120]   But thousands of items will be eligible for drone delivery.
[01:45:43.120 --> 01:45:45.840]   There will be rate restrictions,
[01:45:45.840 --> 01:45:47.760]   because most of- Most drones can't do it.
[01:45:47.760 --> 01:45:48.480]   Thank you.
[01:45:48.480 --> 01:45:49.680]   Drone can only carry so much.
[01:45:49.680 --> 01:45:50.080]   Right.
[01:45:50.080 --> 01:45:50.800]   Two pounds.
[01:45:50.800 --> 01:45:53.600]   Amazon says it's developing technology
[01:45:53.600 --> 01:45:55.600]   to improve obstacle avoidance,
[01:45:55.600 --> 01:45:56.720]   both on the ground and the air.
[01:45:56.720 --> 01:46:01.520]   It's working on enabling its drones to operate B-V-L-O-S.
[01:46:03.120 --> 01:46:05.760]   B-V-L-O-S, beyond the visual line of sight.
[01:46:05.760 --> 01:46:10.000]   Well, I would hope so, otherwise the pilot's going-
[01:46:10.000 --> 01:46:10.080]   Let's go.
[01:46:10.080 --> 01:46:11.120]   You're going delivered-
[01:46:11.120 --> 01:46:13.920]   Well, put it over there.
[01:46:13.920 --> 01:46:14.080]   Put it over there.
[01:46:14.080 --> 01:46:15.280]   Put it over there.
[01:46:15.280 --> 01:46:18.720]   So maybe, you know what?
[01:46:18.720 --> 01:46:21.040]   Maybe it's still BS, right?
[01:46:21.040 --> 01:46:22.240]   3,500 people.
[01:46:22.240 --> 01:46:22.880]   Yeah.
[01:46:22.880 --> 01:46:24.960]   78 per-
[01:46:24.960 --> 01:46:25.920]   Yeah, it's a small-scale-
[01:46:25.920 --> 01:46:27.280]   Yeah.
[01:46:27.280 --> 01:46:28.320]   That's the perfect pilot.
[01:46:28.320 --> 01:46:29.120]   I also think-
[01:46:29.120 --> 01:46:29.920]   City.
[01:46:29.920 --> 01:46:30.640]   Yeah.
[01:46:30.640 --> 01:46:32.160]   I mean, this is a pilot.
[01:46:32.160 --> 01:46:34.800]   And I also think drone delivery is not going to be,
[01:46:34.800 --> 01:46:36.400]   obviously, for everything, right?
[01:46:36.400 --> 01:46:37.760]   I think part of the mistake,
[01:46:37.760 --> 01:46:40.160]   technology, or people when we talk about technology,
[01:46:40.160 --> 01:46:41.600]   we often think we're like,
[01:46:41.600 --> 01:46:43.520]   "Oh, this will solve all of our problems."
[01:46:43.520 --> 01:46:45.760]   When in reality, it will solve a subset of our problems.
[01:46:45.760 --> 01:46:47.520]   And the trade-offs, we've got to understand.
[01:46:47.520 --> 01:46:48.320]   So it's not-
[01:46:48.320 --> 01:46:50.400]   It's sold that way, though.
[01:46:50.400 --> 01:46:51.920]   A lot of the stuff today is just-
[01:46:51.920 --> 01:46:52.320]   Oh, yeah.
[01:46:52.320 --> 01:46:53.040]   No, I get it.
[01:46:53.040 --> 01:46:53.360]   Yeah.
[01:46:53.360 --> 01:46:54.800]   Answer to your biggest problem.
[01:46:54.800 --> 01:46:56.000]   You know, how many-
[01:46:56.000 --> 01:46:58.160]   And I know you're in the IoT space,
[01:46:58.160 --> 01:47:00.000]   but how many things in IoT
[01:47:00.000 --> 01:47:03.760]   was set to just make our lives so much better
[01:47:03.760 --> 01:47:05.440]   by putting it on our mobile device
[01:47:05.440 --> 01:47:06.480]   and controlling it there,
[01:47:06.480 --> 01:47:08.880]   instead of flipping the switch that's on the wall,
[01:47:08.880 --> 01:47:09.520]   right when you want it.
[01:47:09.520 --> 01:47:11.600]   This is from an article in The Verge,
[01:47:11.600 --> 01:47:13.360]   2013, nine years ago.
[01:47:13.360 --> 01:47:20.160]   Amazon's Jeff Bezos showed off his company's latest creation
[01:47:20.160 --> 01:47:21.920]   to Charlie Rosen's 60 Minutes drones
[01:47:21.920 --> 01:47:24.080]   that can deliver packages nine years ago,
[01:47:24.080 --> 01:47:26.400]   up to five pounds in your house in less than half an hour.
[01:47:26.400 --> 01:47:29.440]   Part of a program called Amazon Prime Air,
[01:47:30.240 --> 01:47:32.640]   a drone sits at the end of a conveyor belt
[01:47:32.640 --> 01:47:35.520]   waiting to pick up a package nine years later.
[01:47:35.520 --> 01:47:37.360]   They still haven't delivered that back.
[01:47:37.360 --> 01:47:41.520]   Bezos says 86% of Amazon's packages are under five pounds
[01:47:41.520 --> 01:47:44.320]   and carry them up to 10 miles from the fulfillment center.
[01:47:44.320 --> 01:47:47.520]   As soon as Amazon can work out their regulations
[01:47:47.520 --> 01:47:50.160]   and figure out how to prevent your packages
[01:47:50.160 --> 01:47:52.240]   from being dropped on your head from above.
[01:47:52.240 --> 01:47:54.880]   Nine years ago.
[01:47:54.880 --> 01:47:56.480]   This is-
[01:47:56.480 --> 01:47:57.760]   That's called R&D.
[01:47:57.760 --> 01:47:58.240]   Yeah.
[01:47:58.240 --> 01:47:58.960]   Yeah.
[01:47:58.960 --> 01:48:01.920]   This is why our job is hard, by the way,
[01:48:01.920 --> 01:48:04.000]   because we see all this stuff.
[01:48:04.000 --> 01:48:05.280]   I think it's why our job is awesome.
[01:48:05.280 --> 01:48:06.000]   It's why it's fun.
[01:48:06.000 --> 01:48:06.480]   Yeah.
[01:48:06.480 --> 01:48:09.760]   I mean, you get to see this, you get to say,
[01:48:09.760 --> 01:48:12.880]   "Oh, what are the implications of this?"
[01:48:12.880 --> 01:48:15.520]   Because the companies aren't thinking of like,
[01:48:15.520 --> 01:48:16.960]   that sort of thing.
[01:48:16.960 --> 01:48:18.480]   Like, what are the implications to wild?
[01:48:18.480 --> 01:48:19.280]   No, that's our job.
[01:48:19.280 --> 01:48:19.840]   What's your job?
[01:48:19.840 --> 01:48:20.400]   You're running around.
[01:48:20.400 --> 01:48:21.280]   Yeah, that's our job.
[01:48:21.280 --> 01:48:21.920]   I don't know.
[01:48:21.920 --> 01:48:23.920]   And also a big part, I think of my job,
[01:48:23.920 --> 01:48:31.200]   anyways, is telling people whether this is real or not to waste your time.
[01:48:31.200 --> 01:48:33.040]   As best I can.
[01:48:33.040 --> 01:48:33.840]   It's hard to do.
[01:48:33.840 --> 01:48:35.280]   And FTS.
[01:48:35.280 --> 01:48:36.800]   NFTs and bickens.
[01:48:36.800 --> 01:48:42.720]   By the way, now add to the Warren Buffett says all this crypto's crap.
[01:48:42.720 --> 01:48:46.640]   Now add Bill Gates' name to the list of people who are saying,
[01:48:46.640 --> 01:48:47.920]   "Don't..."
[01:48:47.920 --> 01:48:51.280]   Now, you might say these guys are all old dudes.
[01:48:51.280 --> 01:48:52.000]   Are they saying it?
[01:48:52.000 --> 01:48:54.800]   Because right now when it's down 70% percent.
[01:48:54.800 --> 01:48:55.760]   Well, it's easy now, isn't it?
[01:48:55.760 --> 01:48:58.080]   But I've been saying it for a while.
[01:48:58.080 --> 01:49:04.320]   Bill Gates says crypto and NFTs are a sham.
[01:49:04.320 --> 01:49:07.200]   He says...
[01:49:07.200 --> 01:49:07.840]   He's a little word.
[01:49:07.840 --> 01:49:08.240]   Yeah.
[01:49:08.240 --> 01:49:13.200]   These digital assets are 100% based on the greater fool theory.
[01:49:13.200 --> 01:49:15.360]   He told the Tech Crunch conference yesterday.
[01:49:15.360 --> 01:49:18.560]   He's been watching Twi.
[01:49:18.560 --> 01:49:21.440]   He added he's not long or short on crypto.
[01:49:21.440 --> 01:49:26.240]   And he mocked Boardape NFTs joking that expensive digital images of monkeys
[01:49:26.240 --> 01:49:28.480]   will, yeah, improve the world immensely.
[01:49:28.480 --> 01:49:33.440]   He prefers old-fashioned investing because he's an old-fashioned kind of guy.
[01:49:33.440 --> 01:49:40.080]   I'm used to asset classes like a farm where they have output or a company where they make products.
[01:49:40.080 --> 01:49:45.680]   And Mr. Gates, check out @proed.com/prints.
[01:49:45.680 --> 01:49:46.720]   Get some prints.
[01:49:46.720 --> 01:49:48.160]   Get some NFTs.
[01:49:48.160 --> 01:49:49.440]   What do you think about this though?
[01:49:49.440 --> 01:49:54.640]   And Jay-Z has created a Bitcoin academy.
[01:49:54.640 --> 01:49:56.480]   How about for timing, huh?
[01:49:56.480 --> 01:49:59.440]   For kids in the Marcy houses, public projects.
[01:49:59.440 --> 01:50:02.720]   I think this is probably misguided.
[01:50:02.720 --> 01:50:05.840]   This is the projects in Brooklyn where Jay-Z grew up.
[01:50:05.840 --> 01:50:10.400]   Free classes to residents, also devices and data plans.
[01:50:10.400 --> 01:50:15.440]   Children 5 to 17 will be eligible for enrollment in crypto kids camp,
[01:50:15.440 --> 01:50:17.680]   featuring classes on crypto.
[01:50:17.680 --> 01:50:21.600]   What do you think is so exciting about it?
[01:50:21.600 --> 01:50:25.360]   I have no problem with him providing knowledge about it,
[01:50:25.360 --> 01:50:29.040]   considering how many people still have no idea what Bitcoin is.
[01:50:29.040 --> 01:50:31.520]   Some people think you can pick up a Bitcoin.
[01:50:31.520 --> 01:50:32.000]   No, no.
[01:50:32.000 --> 01:50:37.120]   Instructors will quote, "Teach you about Bitcoin, how it works, why you should care,
[01:50:37.120 --> 01:50:39.360]   and how to build your own financial future."
[01:50:39.360 --> 01:50:41.600]   Yeah, I don't know about that part.
[01:50:41.600 --> 01:50:43.040]   That's a little...
[01:50:43.040 --> 01:50:44.880]   Yeah, I don't know about that part.
[01:50:44.880 --> 01:50:46.160]   The other part I'm fine with.
[01:50:46.800 --> 01:50:49.200]   I think...
[01:50:49.200 --> 01:50:51.440]   I mean, look, I don't think Jay-Z is a bad guy,
[01:50:51.440 --> 01:50:53.680]   and I think he's probably trying to do the right thing.
[01:50:53.680 --> 01:50:56.400]   That timing.
[01:50:56.400 --> 01:50:59.520]   Yeah, well, the program's mission statement reads,
[01:50:59.520 --> 01:51:02.320]   "The vision for Bitcoin is that it doesn't have barriers,
[01:51:02.320 --> 01:51:05.360]   but lack of access to financial education is a barrier.
[01:51:05.360 --> 01:51:07.920]   It's still hard to use for everyday necessities,
[01:51:07.920 --> 01:51:10.160]   plus people need devices and data plans.
[01:51:10.160 --> 01:51:12.400]   This program aims to provide education
[01:51:12.400 --> 01:51:15.440]   and power the community with knowledge and get rid of some of the barriers.
[01:51:16.000 --> 01:51:19.920]   So that residents can learn more about Bitcoin specifically in finance in general."
[01:51:19.920 --> 01:51:21.920]   I mean, I think in principle, maybe this is okay,
[01:51:21.920 --> 01:51:26.880]   especially if they learn about Bitcoin, stay the hell away from the volatility of...
[01:51:26.880 --> 01:51:32.320]   Yeah, I'm totally fine with that just educating those youth in that area
[01:51:32.320 --> 01:51:35.440]   that normally wouldn't have access to that information.
[01:51:35.440 --> 01:51:36.400]   I'm totally fine with that.
[01:51:36.400 --> 01:51:37.840]   Well, and what are the promises...
[01:51:37.840 --> 01:51:40.720]   What if it's a lost leader for financial education?
[01:51:40.720 --> 01:51:43.120]   Like these kids are excited about crypto.
[01:51:43.120 --> 01:51:43.600]   That's right.
[01:51:43.600 --> 01:51:46.240]   I mean, it could be...
[01:51:46.240 --> 01:51:48.720]   One of the things that sucks people into Bitcoin is this notion of...
[01:51:48.720 --> 01:51:51.440]   we're underserved communities.
[01:51:51.440 --> 01:51:53.920]   A housing project in Brooklyn,
[01:51:53.920 --> 01:51:59.040]   cities in Africa, big banks don't want to be there.
[01:51:59.040 --> 01:52:01.360]   We're redlined out of existence.
[01:52:01.360 --> 01:52:05.760]   So maybe there is an alternative financial system that we can participate in.
[01:52:05.760 --> 01:52:10.640]   And I think that I understand the desire to do that.
[01:52:10.640 --> 01:52:12.160]   I just hope they don't lose their shirts.
[01:52:12.880 --> 01:52:13.680]   Correct.
[01:52:13.680 --> 01:52:14.880]   That's where I am.
[01:52:14.880 --> 01:52:18.960]   Just teach the financial responsibility part of this as well.
[01:52:18.960 --> 01:52:19.200]   Yeah.
[01:52:19.200 --> 01:52:26.240]   Vice says, "For poor people excluded from the traditional financial system,
[01:52:26.240 --> 01:52:32.000]   throwing them to a new one full of bubbles, fraud, exploitation, and volatility won't
[01:52:32.000 --> 01:52:34.880]   teach them much. It'll just be old wine and new skins."
[01:52:34.880 --> 01:52:38.240]   It is if he is teaching that information.
[01:52:38.240 --> 01:52:39.040]   Yeah.
[01:52:39.040 --> 01:52:40.080]   It depends what they say.
[01:52:40.080 --> 01:52:40.640]   Yeah.
[01:52:40.640 --> 01:52:41.440]   All right.
[01:52:41.440 --> 01:52:46.160]   Same as when the class, "Hey kids, let's make believe we're buying 10 stocks."
[01:52:46.160 --> 01:52:47.120]   My kid went through that.
[01:52:47.120 --> 01:52:51.360]   And you learn the dangers of the stock market, but that's also how the world works.
[01:52:51.360 --> 01:52:51.840]   Yeah.
[01:52:51.840 --> 01:52:52.400]   Yeah.
[01:52:52.400 --> 01:52:53.760]   Bitcoin isn't how the world works yet.
[01:52:53.760 --> 01:52:53.920]   Yeah.
[01:52:53.920 --> 01:52:57.520]   Qualcomm is one of battle against the EU.
[01:52:57.520 --> 01:53:00.960]   We hear a lot about the EU and regulations of Big Tech.
[01:53:00.960 --> 01:53:07.120]   The EU find Qualcomm nearly a billion dollars, actually a billion euros.
[01:53:07.120 --> 01:53:09.200]   So more than a billion dollars.
[01:53:09.200 --> 01:53:10.240]   Barely these days.
[01:53:10.240 --> 01:53:10.480]   Yeah.
[01:53:10.480 --> 01:53:13.280]   This was four years ago.
[01:53:13.280 --> 01:53:22.160]   In 2018, the EC said, "Quall campaign billions of dollars to Apple from 2011 to 2016 to get them
[01:53:22.160 --> 01:53:27.280]   to use only its chips and iPhones and iPads to block out rivals like Intel."
[01:53:27.280 --> 01:53:33.280]   Now, the reason this might be important is this is a court, the general court,
[01:53:33.280 --> 01:53:39.440]   your second highest, annulled the finding, and faulted Margarita Vistiger,
[01:53:39.440 --> 01:53:43.280]   the EU competition enforcer over the handling of the case.
[01:53:43.280 --> 01:53:44.240]   Wow.
[01:53:44.240 --> 01:53:51.520]   A number of procedural irregularities affected Qualcomm's rights of defense and invalidate
[01:53:51.520 --> 01:53:53.360]   the commission's analysis of the conduct.
[01:53:53.360 --> 01:53:55.280]   So yeah, no kidding.
[01:53:55.280 --> 01:53:58.320]   This is a big blow to Margarita Vistiger.
[01:53:58.320 --> 01:53:59.840]   Big blow.
[01:53:59.840 --> 01:54:02.000]   I wonder what the procedure abnormalities were.
[01:54:02.880 --> 01:54:05.040]   Yeah, they don't be very boring.
[01:54:05.040 --> 01:54:07.200]   This story from Reuters is not more suspicious.
[01:54:07.200 --> 01:54:09.120]   I love procedural issues.
[01:54:09.120 --> 01:54:09.120]   Yeah.
[01:54:09.120 --> 01:54:12.160]   Well, can you get on that and give us a report?
[01:54:12.160 --> 01:54:17.440]   You know I was the person and model you in who understood all the rules of like
[01:54:17.440 --> 01:54:20.960]   parliamentary procedure and figured out how to use that to my advantage.
[01:54:20.960 --> 01:54:22.880]   Kind of the Roberts rules of orders.
[01:54:22.880 --> 01:54:23.280]   Yeah.
[01:54:23.280 --> 01:54:24.720]   You are out of order.
[01:54:24.720 --> 01:54:24.960]   Exactly.
[01:54:24.960 --> 01:54:26.240]   Out of order.
[01:54:28.800 --> 01:54:34.000]   You are you having a heart attack?
[01:54:34.000 --> 01:54:41.840]   Oh, yeah.
[01:54:41.840 --> 01:54:42.400]   Oh, yeah.
[01:54:42.400 --> 01:54:46.320]   This is this this is the second dead air that has caught me up like what is happening?
[01:54:46.320 --> 01:54:52.160]   You're talking you're talking about this wonderful guy John Burckow,
[01:54:52.160 --> 01:54:54.000]   who is no longer the speaker.
[01:54:54.000 --> 01:54:55.280]   I won't hear the speaker, but.
[01:54:55.280 --> 01:54:56.400]   He was so great.
[01:54:57.120 --> 01:54:58.880]   Mr Angus Brendan McNeil.
[01:54:58.880 --> 01:55:00.880]   Calm yourself.
[01:55:00.880 --> 01:55:05.600]   You know your cheeky chappy, but you're also an exceptionally noisy one.
[01:55:05.600 --> 01:55:09.360]   This is the I mean this is parliament.
[01:55:09.360 --> 01:55:10.640]   It was the start this order.
[01:55:10.640 --> 01:55:10.960]   Yeah.
[01:55:10.960 --> 01:55:11.920]   Are you wait you want order?
[01:55:11.920 --> 01:55:12.720]   Yeah.
[01:55:12.720 --> 01:55:13.040]   Yeah.
[01:55:13.040 --> 01:55:20.240]   I must order the honorable gentleman to withdraw immediately from the house
[01:55:20.240 --> 01:55:22.640]   for the remainder of this day's sitting.
[01:55:22.640 --> 01:55:26.400]   This is intolerable behavior as far as the public.
[01:55:26.400 --> 01:55:27.760]   No, it's not funny.
[01:55:27.760 --> 01:55:31.680]   That sticker on the subject of Brexit happens to be
[01:55:31.680 --> 01:55:37.120]   affixed to or in the windscreen of my wife's car.
[01:55:37.120 --> 01:55:38.240]   Yes.
[01:55:38.240 --> 01:55:44.000]   And I'm sure the honorable gentleman wouldn't suggest for one moment.
[01:55:44.000 --> 01:55:45.760]   Why don't we have somebody like this?
[01:55:45.760 --> 01:55:53.120]   Maybe Stacey would you volunteer to go to Washington DC and say order.
[01:55:55.520 --> 01:55:58.000]   Yes, I volunteer and you want to back me up.
[01:55:58.000 --> 01:55:59.760]   Well, you'd be so good.
[01:55:59.760 --> 01:56:00.880]   You'd be so good.
[01:56:00.880 --> 01:56:03.520]   Wow.
[01:56:03.520 --> 01:56:04.720]   The truth module is up high.
[01:56:04.720 --> 01:56:05.120]   Yeah.
[01:56:05.120 --> 01:56:05.760]   Yeah.
[01:56:05.760 --> 01:56:06.160]   Yeah.
[01:56:06.160 --> 01:56:08.000]   You have to set it to 80 or 90 percent.
[01:56:08.000 --> 01:56:10.640]   Well, actually, you're actually a part of it.
[01:56:10.640 --> 01:56:13.040]   As long as I can call someone a cheeky chappy.
[01:56:13.040 --> 01:56:13.760]   Chicky chappy.
[01:56:13.760 --> 01:56:15.280]   You may be a cheeky chappy.
[01:56:15.280 --> 01:56:17.680]   You're a cheeky chappy.
[01:56:17.680 --> 01:56:18.880]   This is a damn place here.
[01:56:18.880 --> 01:56:21.840]   You mentioned Douglas Rushkoff.
[01:56:21.840 --> 01:56:25.040]   I just got a you probably got a two Jeffs since you know the guy.
[01:56:26.000 --> 01:56:27.680]   I got an early copy of his new book,
[01:56:27.680 --> 01:56:30.400]   which I cannot wait to read Survival of the Richest.
[01:56:30.400 --> 01:56:34.720]   Escape fantasies of the tech billionaires.
[01:56:34.720 --> 01:56:37.520]   This is what the billionaires are planning.
[01:56:37.520 --> 01:56:41.840]   Not just Mars, but Island bunkers, the metaverse,
[01:56:41.840 --> 01:56:48.400]   and decommissioned nuclear missile silos.
[01:56:48.400 --> 01:56:50.880]   And Douglas and I got along wonderfully teaching class together,
[01:56:50.880 --> 01:56:52.640]   even though we do see the world so much differently.
[01:56:52.640 --> 01:56:55.040]   Yeah. I look forward to reading this.
[01:56:55.040 --> 01:56:56.880]   I just got it and I can't wait to read it.
[01:56:56.880 --> 01:56:58.560]   You said that's an early release.
[01:56:58.560 --> 01:56:59.600]   You can get them on the show.
[01:56:59.600 --> 01:57:01.120]   It's not out yet, but you could pre-order it.
[01:57:01.120 --> 01:57:03.600]   Yeah, we will.
[01:57:03.600 --> 01:57:04.160]   You know what?
[01:57:04.160 --> 01:57:04.880]   Well, let me read it.
[01:57:04.880 --> 01:57:05.680]   If it's good, we will.
[01:57:05.680 --> 01:57:06.720]   Okay.
[01:57:06.720 --> 01:57:08.320]   But I am sure it's good.
[01:57:08.320 --> 01:57:10.160]   He's on team human after all.
[01:57:10.160 --> 01:57:11.840]   Yes.
[01:57:11.840 --> 01:57:12.880]   Douglas Rushkoff.
[01:57:12.880 --> 01:57:13.440]   Well, you see him.
[01:57:13.440 --> 01:57:15.440]   What's his media critic?
[01:57:15.440 --> 01:57:16.480]   Is he a professor?
[01:57:16.480 --> 01:57:17.120]   What is his?
[01:57:17.120 --> 01:57:17.840]   Yeah, he's a professor.
[01:57:17.840 --> 01:57:19.360]   He doesn't have a doctorate like me.
[01:57:19.360 --> 01:57:22.320]   As I don't have a doctorate either, but he's.
[01:57:22.320 --> 01:57:23.920]   Yeah, I was going to say, wait a minute.
[01:57:23.920 --> 01:57:26.320]   I should have been calling you Dr. Jarvis.
[01:57:26.320 --> 01:57:27.520]   Oh, no, no, no, no.
[01:57:27.520 --> 01:57:31.360]   I was just in the University of Virginia and they had the bios.
[01:57:31.360 --> 01:57:33.520]   And so everybody's doctor, doctor, doctor, doctor.
[01:57:33.520 --> 01:57:36.080]   I had to be very clear to say, "Ah, not me."
[01:57:36.080 --> 01:57:38.960]   Well, my father, who has a PhD,
[01:57:38.960 --> 01:57:44.080]   got a very young age, I think it was like 23 or 24 from Columbia,
[01:57:44.080 --> 01:57:46.240]   in paleontology.
[01:57:46.240 --> 01:57:49.680]   Do you know what is the topic of his dissertation?
[01:57:49.680 --> 01:57:50.080]   Yes.
[01:57:50.080 --> 01:57:52.000]   Ancient marine ecosystems.
[01:57:52.800 --> 01:57:55.120]   But he refused.
[01:57:55.120 --> 01:57:56.640]   He said, "Do not call me doctor."
[01:57:56.640 --> 01:57:59.760]   That's, I'm not a medical doctor.
[01:57:59.760 --> 01:58:00.240]   He didn't.
[01:58:00.240 --> 01:58:03.040]   Oh, there's a reverse snobbery going on.
[01:58:03.040 --> 01:58:05.600]   You don't call me doctor until he needed
[01:58:05.600 --> 01:58:06.880]   to restaurant reservations.
[01:58:06.880 --> 01:58:08.400]   Then it was always Dr. LaPorte.
[01:58:08.400 --> 01:58:12.880]   Every time it works.
[01:58:12.880 --> 01:58:14.160]   We got to take a little break.
[01:58:14.160 --> 01:58:15.120]   Professor LaPorte?
[01:58:15.120 --> 01:58:18.080]   Professor does not even back then,
[01:58:18.080 --> 01:58:19.840]   you know, remember, Adley Stevens.
[01:58:19.840 --> 01:58:21.840]   Doesn't get you dinner reservations.
[01:58:21.840 --> 01:58:22.240]   It doesn't get you dinner reservations.
[01:58:22.240 --> 01:58:23.440]   Doctor might, yeah.
[01:58:23.440 --> 01:58:27.680]   Let us, because it is soon to be time.
[01:58:27.680 --> 01:58:28.640]   Oh, wait a minute though.
[01:58:28.640 --> 01:58:30.560]   One TikTok.
[01:58:30.560 --> 01:58:35.760]   Hey, I just want to know that y'all all,
[01:58:35.760 --> 01:58:38.720]   y'all all have full circle on TikTok.
[01:58:38.720 --> 01:58:39.520]   Y'all?
[01:58:39.520 --> 01:58:39.840]   Ant?
[01:58:39.840 --> 01:58:42.880]   This is a Chipotle ad for queso.
[01:58:42.880 --> 01:58:45.360]   This is the ultimate twig.
[01:58:45.360 --> 01:58:47.200]   It's got queso.
[01:58:47.200 --> 01:58:48.000]   It's got Chipotle.
[01:58:48.000 --> 01:58:48.880]   And TikTok.
[01:58:48.880 --> 01:58:49.840]   And TikTok.
[01:58:49.840 --> 01:58:50.400]   Here we go.
[01:58:50.400 --> 01:58:51.680]   I just sent a queso blanco,
[01:58:51.680 --> 01:58:52.880]   close that hot salsa,
[01:58:52.880 --> 01:58:54.720]   queso blanco, shake it up, enjoy.
[01:58:54.720 --> 01:58:57.040]   And queso blanco is free every Monday in June
[01:58:57.040 --> 01:58:58.080]   with an entree purchase.
[01:58:58.080 --> 01:58:58.640]   What?
[01:58:58.640 --> 01:59:00.640]   It was coqueso 22 and the Chipotle ad for online.
[01:59:00.640 --> 01:59:02.480]   But doesn't Chipotle queso,
[01:59:02.480 --> 01:59:03.600]   isn't it disgusting?
[01:59:03.600 --> 01:59:04.720]   They actually got better.
[01:59:04.720 --> 01:59:05.200]   Oh, okay.
[01:59:05.200 --> 01:59:06.000]   It was awful.
[01:59:06.000 --> 01:59:06.400]   It started.
[01:59:06.400 --> 01:59:07.200]   It started. It was awful.
[01:59:07.200 --> 01:59:07.440]   Okay.
[01:59:07.440 --> 01:59:08.880]   It was gritty.
[01:59:08.880 --> 01:59:09.440]   It was really.
[01:59:09.440 --> 01:59:10.080]   It was gritty.
[01:59:10.080 --> 01:59:10.640]   Exactly.
[01:59:10.640 --> 01:59:11.040]   Gritty.
[01:59:11.040 --> 01:59:11.280]   Yes.
[01:59:11.280 --> 01:59:13.040]   It was like chalk.
[01:59:13.040 --> 01:59:14.000]   It was melted chalk.
[01:59:15.280 --> 01:59:19.360]   It was just a thing.
[01:59:19.360 --> 01:59:21.680]   It was just a thing you do.
[01:59:21.680 --> 01:59:24.720]   It's like why one does.
[01:59:24.720 --> 01:59:25.680]   Or.
[01:59:25.680 --> 01:59:27.120]   Plus we remain bitter that Chipotle
[01:59:27.120 --> 01:59:28.240]   had never bought an ad on the show.
[01:59:28.240 --> 01:59:29.840]   We give them tons of publicity.
[01:59:29.840 --> 01:59:31.280]   Oh, you know, early on.
[01:59:31.280 --> 01:59:33.360]   Well, most of it was about how bad their queso was.
[01:59:33.360 --> 01:59:34.640]   So early on.
[01:59:34.640 --> 01:59:38.160]   When we were doing the giz was with Dick D. Bartolo,
[01:59:38.160 --> 01:59:39.360]   we could never get a sponsor.
[01:59:39.360 --> 01:59:41.040]   So I started doing fake ads
[01:59:41.040 --> 01:59:43.600]   for tostinos pizza rolls.
[01:59:44.480 --> 01:59:45.440]   For longest time.
[01:59:45.440 --> 01:59:47.840]   And then I found out they're not tostinos.
[01:59:47.840 --> 01:59:49.280]   They're tostinos.
[01:59:49.280 --> 01:59:49.760]   Toesos.
[01:59:49.760 --> 01:59:51.200]   So I basically blew it.
[01:59:51.200 --> 01:59:56.880]   And now a word from Stacums.
[01:59:56.880 --> 01:59:59.360]   No, let's do the Google change law.
[01:59:59.360 --> 02:00:02.720]   Ah, ah, did you hear that?
[02:00:02.720 --> 02:00:05.040]   Google change law.
[02:00:05.040 --> 02:00:06.720]   Get Q in the middle of him laughing.
[02:00:06.720 --> 02:00:08.880]   Ah, I love it.
[02:00:08.880 --> 02:00:10.080]   What?
[02:00:10.080 --> 02:00:10.560]   He saw.
[02:00:10.560 --> 02:00:11.520]   What is it Jason?
[02:00:11.520 --> 02:00:12.000]   What is it?
[02:00:12.640 --> 02:00:13.200]   What?
[02:00:13.200 --> 02:00:15.280]   We're not seeing the tricaster.
[02:00:15.280 --> 02:00:19.440]   Ah, now does that mean I have to start the show over?
[02:00:19.440 --> 02:00:19.920]   No.
[02:00:19.920 --> 02:00:20.480]   Thank God.
[02:00:20.480 --> 02:00:26.800]   It's just the extra value for the twins.
[02:00:26.800 --> 02:00:28.000]   It's just the stream.
[02:00:28.000 --> 02:00:29.200]   It's not the recording.
[02:00:29.200 --> 02:00:29.840]   OK.
[02:00:29.840 --> 02:00:32.880]   Let's do that again.
[02:00:32.880 --> 02:00:34.480]   And now, ladies and gentlemen,
[02:00:34.480 --> 02:00:36.320]   time for the Google.
[02:00:36.320 --> 02:00:37.120]   Well, you know.
[02:00:37.120 --> 02:00:37.840]   Well, you know,
[02:00:37.840 --> 02:00:39.280]   I'm not going to do that again.
[02:00:39.280 --> 02:00:42.560]   And now, it's time for the Google change law.
[02:00:42.560 --> 02:00:44.080]   [MUSIC PLAYING]
[02:00:44.080 --> 02:00:46.080]   [SCREAMING]
[02:00:46.080 --> 02:00:48.000]   Doesn't he say change law in there?
[02:00:48.000 --> 02:00:49.040]   It's the wrong one.
[02:00:49.040 --> 02:00:53.280]   [LAUGHTER]
[02:00:53.280 --> 02:00:56.960]   And now, it's time for the Google change law.
[02:00:56.960 --> 02:00:57.920]   [MUSIC PLAYING]
[02:00:57.920 --> 02:00:59.520]   The Google change law.
[02:00:59.520 --> 02:01:01.040]   Oh, there we go.
[02:01:01.040 --> 02:01:03.680]   [MUSIC PLAYING]
[02:01:03.680 --> 02:01:07.600]   So if you are a newspaper or a blogger,
[02:01:07.600 --> 02:01:11.840]   you probably take corrections pretty seriously.
[02:01:11.840 --> 02:01:16.480]   YouTube has never had a way for YouTubers to correct their video.
[02:01:16.480 --> 02:01:21.280]   So there's a new feature just added to YouTube called Corrections.
[02:01:21.280 --> 02:01:24.720]   You can easily add more obvious corrections.
[02:01:24.720 --> 02:01:28.320]   I see YouTube videos all the time.
[02:01:28.320 --> 02:01:30.240]   And it's the audio or what are you doing?
[02:01:30.240 --> 02:01:30.880]   How do you-- what?
[02:01:30.880 --> 02:01:31.040]   I don't--
[02:01:31.040 --> 02:01:34.240]   They're info cards that you add to the video.
[02:01:34.240 --> 02:01:36.080]   This is wrong what you're seeing in your brain.
[02:01:36.080 --> 02:01:38.160]   What I'm saying is completely wrong.
[02:01:38.160 --> 02:01:42.160]   Actually, it's weird because you have a pop up that says few corrections.
[02:01:42.160 --> 02:01:44.880]   How many YouTubers are going to use this, really?
[02:01:44.880 --> 02:01:45.880]   Oh, like three.
[02:01:45.880 --> 02:01:46.880]   Yeah.
[02:01:46.880 --> 02:01:47.880]   I was wrong.
[02:01:47.880 --> 02:01:48.240]   Have you used it?
[02:01:48.240 --> 02:01:49.920]   Well, I see that a lot.
[02:01:49.920 --> 02:01:50.760]   I'll watch videos.
[02:01:50.760 --> 02:01:53.800]   And I see it a lot where the guy says something--
[02:01:53.800 --> 02:01:54.240]   I do it.
[02:01:54.240 --> 02:01:54.960]   Everybody does it.
[02:01:54.960 --> 02:01:57.000]   If you're doing it and you're doing it fast, there's something wrong.
[02:01:57.000 --> 02:01:58.160]   It's 64 gigs of RAM.
[02:01:58.160 --> 02:01:59.320]   And it's only 32.
[02:01:59.320 --> 02:02:00.800]   And they'll put a little caption on this.
[02:02:00.800 --> 02:02:02.120]   It was only 32.
[02:02:02.120 --> 02:02:02.600]   I made it.
[02:02:02.600 --> 02:02:03.600]   That's what most people do.
[02:02:03.600 --> 02:02:04.840]   Yeah, that's normal.
[02:02:04.840 --> 02:02:09.320]   Actually, I'm going to use that because I've got to take my videos I did two years ago
[02:02:09.320 --> 02:02:10.760]   for the incoming class.
[02:02:10.760 --> 02:02:13.120]   There's some things that want to change so I can just use it to update.
[02:02:13.120 --> 02:02:14.080]   You don't have to be a correction.
[02:02:14.080 --> 02:02:14.120]   Right?
[02:02:14.120 --> 02:02:15.120]   It could also be an update.
[02:02:15.120 --> 02:02:16.320]   That's a good question.
[02:02:16.320 --> 02:02:19.440]   If it says correction, I think it says corrections.
[02:02:19.440 --> 02:02:19.880]   Let's see.
[02:02:19.880 --> 02:02:21.240]   Here's the full instructions.
[02:02:21.240 --> 02:02:29.440]   Wow.
[02:02:29.440 --> 02:02:30.040]   It's complicated.
[02:02:30.040 --> 02:02:30.920]   OK, I read them all.
[02:02:30.920 --> 02:02:32.240]   YouTube, you've got to watch the video.
[02:02:32.240 --> 02:02:33.240]   You've got to watch the video.
[02:02:33.240 --> 02:02:34.920]   I don't know.
[02:02:34.920 --> 02:02:39.320]   Google Chrome's on device machine learning.
[02:02:39.320 --> 02:02:40.680]   You know, they love saying that.
[02:02:40.680 --> 02:02:41.520]   Is sentient?
[02:02:41.520 --> 02:02:42.160]   No, no.
[02:02:42.160 --> 02:02:43.880]   Sorry.
[02:02:43.880 --> 02:02:46.520]   My phone is alive.
[02:02:46.520 --> 02:02:47.520]   Oh, no.
[02:02:47.520 --> 02:02:48.440]   It shocks.
[02:02:48.440 --> 02:02:48.920]   Will it?
[02:02:48.920 --> 02:02:54.360]   It says, or they say, it blocks noisy notification prompts.
[02:02:54.360 --> 02:02:57.000]   So they've always had built in-- not always, but they've for a long time,
[02:02:57.000 --> 02:03:01.240]   they have a built in phishing detection that tells you if the site you're going to--
[02:03:01.240 --> 02:03:02.000]   you've seen it.
[02:03:02.000 --> 02:03:03.160]   It pops up and says, this is a--
[02:03:03.160 --> 02:03:06.040]   it's a scam, right?
[02:03:06.040 --> 02:03:11.280]   Now, in Chrome 102, it's going to use machine learning that runs entirely
[02:03:11.280 --> 02:03:12.400]   within the browser--
[02:03:12.400 --> 02:03:15.160]   no Google data going back to Google--
[02:03:15.160 --> 02:03:20.200]   that will help identify websites that make unsolicited permission requests
[02:03:20.200 --> 02:03:22.640]   for notifications and silences.
[02:03:22.640 --> 02:03:26.160]   Now, the first thing I do in Chrome and Firefox as far in every browser
[02:03:26.160 --> 02:03:29.120]   is I say, block requests for notifications.
[02:03:29.120 --> 02:03:31.320]   I don't like it when websites do notify.
[02:03:31.320 --> 02:03:33.280]   So you could block that universally?
[02:03:33.280 --> 02:03:34.560]   Yes.
[02:03:34.560 --> 02:03:35.560]   Oh.
[02:03:35.560 --> 02:03:36.680]   It's such a nice feature.
[02:03:36.680 --> 02:03:37.240]   Awesome.
[02:03:37.240 --> 02:03:43.320]   Well, I think I do in Firefox, and I'm pretty sure I remember doing it in Chrome.
[02:03:43.320 --> 02:03:44.160]   It's the first thing I do.
[02:03:44.160 --> 02:03:44.640]   You can do it, Chrome.
[02:03:44.640 --> 02:03:45.000]   Yeah.
[02:03:45.000 --> 02:03:45.680]   I do it.
[02:03:45.680 --> 02:03:46.440]   Yeah.
[02:03:46.440 --> 02:03:49.720]   By this buried, because it's in the content section.
[02:03:49.720 --> 02:03:51.680]   So it's not-- you have to really look for it.
[02:03:51.680 --> 02:03:55.200]   But if you Google it, it's really-- yeah, you should totally do it.
[02:03:55.200 --> 02:03:55.800]   It's easy to do.
[02:03:55.800 --> 02:03:56.320]   Oh, cool.
[02:03:56.320 --> 02:03:57.920]   Oh, because they drive me crazy.
[02:03:57.920 --> 02:04:01.240]   It's the first thing I do when I set up a browser is I turn off a block
[02:04:01.240 --> 02:04:04.240]   notifications from all sites, and I block the request to save passwords,
[02:04:04.240 --> 02:04:05.440]   because I use a password manager.
[02:04:05.440 --> 02:04:06.960]   I don't want that.
[02:04:06.960 --> 02:04:08.120]   So there's little annoyances.
[02:04:08.120 --> 02:04:11.000]   Anyway, Google says that you're not even going to have to change that setting anymore,
[02:04:11.000 --> 02:04:15.440]   because we'll now just know to further improve the browsing experience.
[02:04:15.440 --> 02:04:18.840]   We've also evolved how people are interacting with web notifications.
[02:04:18.840 --> 02:04:29.320]   So in effect, I guess it's possible to sneakily say, turn on notifications
[02:04:29.320 --> 02:04:30.880]   without your permission or something.
[02:04:30.880 --> 02:04:33.280]   So they're going to block that.
[02:04:33.280 --> 02:04:35.280]   Tell people browse the web with minimal interruption.
[02:04:35.280 --> 02:04:39.440]   Chrome predicts when permission props are unlikely to be granted.
[02:04:39.440 --> 02:04:40.680]   Oh, I get it.
[02:04:40.680 --> 02:04:42.400]   Oh, and they see the script.
[02:04:42.400 --> 02:04:47.600]   You don't want-- no, you don't want notifications, and they silence the prompts.
[02:04:47.600 --> 02:04:48.960]   OK.
[02:04:48.960 --> 02:04:51.320]   I guess.
[02:04:51.320 --> 02:04:53.280]   I guess.
[02:04:53.280 --> 02:04:53.680]   Anyway--
[02:04:53.680 --> 02:04:54.680]   Do you have to pay extra?
[02:04:54.680 --> 02:04:55.240]   No.
[02:04:55.240 --> 02:04:56.480]   To get your prompts booked.
[02:04:56.480 --> 02:04:58.840]   Oh, just a self-temptive.
[02:04:58.840 --> 02:05:00.480]   Yeah, how does it know it's undesirable?
[02:05:00.480 --> 02:05:01.480]   Is it Yelp?
[02:05:01.480 --> 02:05:02.480]   Yeah.
[02:05:02.480 --> 02:05:03.480]   [LAUGHTER]
[02:05:03.480 --> 02:05:04.480]   Exactly.
[02:05:04.480 --> 02:05:07.480]   You know, we know you don't want those Yelp notifications.
[02:05:07.480 --> 02:05:09.480]   Well, in fact, I don't.
[02:05:09.480 --> 02:05:10.480]   Yeah.
[02:05:10.480 --> 02:05:13.880]   Google Maps now shows total prices on Android and iOS.
[02:05:13.880 --> 02:05:18.080]   So you'll know ahead of time that you've got to pay some tolls.
[02:05:18.080 --> 02:05:19.080]   That's nice.
[02:05:19.080 --> 02:05:21.880]   Do you pay a lot of tolls, don't you, Jeff, in New Jersey?
[02:05:21.880 --> 02:05:22.880]   Oh, yeah.
[02:05:22.880 --> 02:05:23.880]   Oh, yeah.
[02:05:23.880 --> 02:05:24.880]   It's the same.
[02:05:24.880 --> 02:05:25.880]   We pay a lot of tolls, too.
[02:05:25.880 --> 02:05:26.880]   Really?
[02:05:26.880 --> 02:05:27.880]   We only--
[02:05:27.880 --> 02:05:28.880]   Yeah, I eat the bridges.
[02:05:28.880 --> 02:05:29.880]   It's the bridges.
[02:05:29.880 --> 02:05:30.880]   Just--
[02:05:30.880 --> 02:05:31.880]   Yeah.
[02:05:31.880 --> 02:05:32.880]   Well, it's the turn-like.
[02:05:32.880 --> 02:05:34.880]   No, two for me.
[02:05:34.880 --> 02:05:36.360]   There's Richmond Bridge, too.
[02:05:36.360 --> 02:05:37.880]   Yeah, Richmond and Golden Gate Bridge.
[02:05:37.880 --> 02:05:38.880]   Yeah.
[02:05:38.880 --> 02:05:40.880]   And Bay Bridge has a toll, too.
[02:05:40.880 --> 02:05:48.600]   Shortcuts replacing files and folders stored in multiple locations on Google Drive.
[02:05:48.600 --> 02:05:50.880]   I thought you always had shortcuts.
[02:05:50.880 --> 02:05:51.880]   I didn't understand this.
[02:05:51.880 --> 02:05:52.880]   That's why I put it in there.
[02:05:52.880 --> 02:05:54.880]   I thought you'd explain it to me.
[02:05:54.880 --> 02:05:57.480]   Jeff, I don't know what this is about.
[02:05:57.480 --> 02:05:58.480]   Leo figured it out for me.
[02:05:58.480 --> 02:06:02.480]   Now, it's easier for you to find organized files while keeping the original version.
[02:06:02.480 --> 02:06:04.480]   Well, sure, it's going to do it automatically.
[02:06:04.480 --> 02:06:06.280]   It's going to just do it to you on a given date.
[02:06:06.280 --> 02:06:09.400]   See, this is-- Windows does this, which I don't like.
[02:06:09.400 --> 02:06:14.160]   Windows looks like it's a folder for my documents, but it's just-- it's a saved search.
[02:06:14.160 --> 02:06:16.160]   It's not what you think it is.
[02:06:16.160 --> 02:06:19.560]   So the replacement process will start in 2022.
[02:06:19.560 --> 02:06:24.000]   You'll get a notification in Google Drive a few weeks before your replacement starts.
[02:06:24.000 --> 02:06:25.000]   Yeah.
[02:06:25.000 --> 02:06:26.000]   Wait a minute.
[02:06:26.000 --> 02:06:27.000]   What?
[02:06:27.000 --> 02:06:30.560]   This is a replace all but one location of files and folders that are currently involved.
[02:06:30.560 --> 02:06:32.880]   This is to save them storage.
[02:06:32.880 --> 02:06:33.880]   If you have not--
[02:06:33.880 --> 02:06:37.240]   If you have the same file on multiple locations, they're just going to go suck them all in
[02:06:37.240 --> 02:06:38.240]   a one.
[02:06:38.240 --> 02:06:39.240]   How much can that be?
[02:06:39.240 --> 02:06:40.240]   It's Google.
[02:06:40.240 --> 02:06:41.240]   Well, but there's billions of people.
[02:06:41.240 --> 02:06:42.240]   Yeah.
[02:06:42.240 --> 02:06:43.240]   That's true.
[02:06:43.240 --> 02:06:44.240]   Right across all of us.
[02:06:44.240 --> 02:06:46.640]   The replacement decision will be based on original file and folder ownership will also
[02:06:46.640 --> 02:06:49.320]   consider access and activity on other folders.
[02:06:49.320 --> 02:06:51.480]   That's so they know which one is the original.
[02:06:51.480 --> 02:06:53.280]   At the time of replacement-- oh, so, yes.
[02:06:53.280 --> 02:06:55.000]   So it's automatic.
[02:06:55.000 --> 02:07:00.280]   So they're going to replace duplicate documents in your Google Drive with shortcuts.
[02:07:00.280 --> 02:07:04.120]   What if I put a document in another place so I can change it?
[02:07:04.120 --> 02:07:05.840]   If it's changed, they won't do that.
[02:07:05.840 --> 02:07:08.000]   No, no, no, no, I'm saying something different.
[02:07:08.000 --> 02:07:09.000]   I get ready to--
[02:07:09.000 --> 02:07:10.000]   Tomorrow.
[02:07:10.000 --> 02:07:11.000]   I haven't changed it yet.
[02:07:11.000 --> 02:07:12.000]   I'm going to change it.
[02:07:12.000 --> 02:07:13.000]   Right.
[02:07:13.000 --> 02:07:16.600]   And then I start changing document two, thinking I'm changing only document two, but now I'm
[02:07:16.600 --> 02:07:17.600]   changing document one.
[02:07:17.600 --> 02:07:18.600]   I don't know.
[02:07:18.600 --> 02:07:19.600]   In the railroad.
[02:07:19.600 --> 02:07:20.600]   We'll have to see.
[02:07:20.600 --> 02:07:21.600]   Uh oh, yeah.
[02:07:21.600 --> 02:07:22.600]   Right.
[02:07:22.600 --> 02:07:23.600]   Right.
[02:07:23.600 --> 02:07:25.040]   Google lists.
[02:07:25.040 --> 02:07:27.040]   This is from chromeunboxed.com.
[02:07:27.040 --> 02:07:28.040]   Google lists.
[02:07:28.040 --> 02:07:32.120]   They're recommended third party apps for Google Workspace.
[02:07:32.120 --> 02:07:33.120]   Okay.
[02:07:33.120 --> 02:07:34.120]   Great.
[02:07:34.120 --> 02:07:35.120]   Well, I also put this in here because I didn't--
[02:07:35.120 --> 02:07:36.120]   That's what I thought.
[02:07:36.120 --> 02:07:37.120]   They say they do it every year.
[02:07:37.120 --> 02:07:38.120]   Okay.
[02:07:38.120 --> 02:07:39.120]   But I don't know.
[02:07:39.120 --> 02:07:40.120]   I've never seen it before.
[02:07:40.120 --> 02:07:41.120]   It's just the apps.
[02:07:41.120 --> 02:07:44.560]   You know, when you say I want an app.
[02:07:44.560 --> 02:07:46.040]   But they only recommend 12 of them.
[02:07:46.040 --> 02:07:47.040]   I didn't know them as a--
[02:07:47.040 --> 02:07:49.080]   Yes, those are the special apps.
[02:07:49.080 --> 02:07:51.040]   The male meteor.
[02:07:51.040 --> 02:07:52.040]   Calendar invites.
[02:07:52.040 --> 02:07:57.640]   It changed a little bit to be more modern and more clear.
[02:07:57.640 --> 02:08:04.160]   That's a good thing because these calendar invites are terrible, terrible.
[02:08:04.160 --> 02:08:07.240]   The problem I have, I don't know what this happens to you, is that people want to have
[02:08:07.240 --> 02:08:08.760]   a zoom meeting.
[02:08:08.760 --> 02:08:12.400]   You put the name in it, it says make it a Google Meet meeting and then make it a zoom
[02:08:12.400 --> 02:08:16.160]   meeting and then they don't erase the make it a Google Meet meeting and nobody knows
[02:08:16.160 --> 02:08:17.160]   where to meet.
[02:08:17.160 --> 02:08:22.000]   That's because Google automatically populates it because they're anti-trust.
[02:08:22.000 --> 02:08:23.000]   Yeah.
[02:08:23.000 --> 02:08:24.000]   So here's the--
[02:08:24.000 --> 02:08:25.000]   Because they're anti-trust.
[02:08:25.000 --> 02:08:27.000]   Here's the old invite.
[02:08:27.000 --> 02:08:28.000]   Blah, blah, blah.
[02:08:28.000 --> 02:08:29.000]   You recognize this.
[02:08:29.000 --> 02:08:30.000]   Oh, yeah.
[02:08:30.000 --> 02:08:31.000]   Oh, yeah.
[02:08:31.000 --> 02:08:34.080]   And then the new one is going to be like this.
[02:08:34.080 --> 02:08:36.080]   Do that again?
[02:08:36.080 --> 02:08:37.080]   Go back?
[02:08:37.080 --> 02:08:38.440]   It's not like a match of matter.
[02:08:38.440 --> 02:08:40.240]   No, it's not like a match of matter.
[02:08:40.240 --> 02:08:41.240]   Not helpful.
[02:08:41.240 --> 02:08:42.240]   Well, it is better.
[02:08:42.240 --> 02:08:44.240]   Oh, it's much better.
[02:08:44.240 --> 02:08:47.360]   Figuring out where your meeting is is nicer.
[02:08:47.360 --> 02:08:52.080]   Yeah, see, a nice big button says join with Google Meet.
[02:08:52.080 --> 02:08:54.080]   I hope I can still do zoom through the--
[02:08:54.080 --> 02:08:55.080]   That's one thing.
[02:08:55.080 --> 02:08:56.480]   We've had problems in the past.
[02:08:56.480 --> 02:09:00.360]   I think it's still happening where if you make a zoom meeting with Google, it also ends
[02:09:00.360 --> 02:09:02.720]   a meeting, a meet ID.
[02:09:02.720 --> 02:09:03.720]   And then there's always--
[02:09:03.720 --> 02:09:04.720]   Yes, that's what I'm saying.
[02:09:04.720 --> 02:09:05.720]   --the creator has to kill them.
[02:09:05.720 --> 02:09:06.720]   That's why the app was complaining.
[02:09:06.720 --> 02:09:12.200]   That's why I just said, you have to kill-- you have to explicitly kill that happens to
[02:09:12.200 --> 02:09:13.200]   you too.
[02:09:13.200 --> 02:09:14.200]   Oh, OK.
[02:09:14.200 --> 02:09:15.200]   We understand.
[02:09:15.200 --> 02:09:16.200]   They make it automatically.
[02:09:16.200 --> 02:09:19.720]   They kill it both are there and meet is above zoom and everybody's confused.
[02:09:19.720 --> 02:09:23.640]   Today is the last day you can use Google Talk.
[02:09:23.640 --> 02:09:24.640]   What?
[02:09:24.640 --> 02:09:30.600]   It's still operational, but tomorrow it's over for Google Talk also known as GChat.
[02:09:30.600 --> 02:09:33.320]   This is what you were talking about, Stacey.
[02:09:33.320 --> 02:09:34.320]   Yes.
[02:09:34.320 --> 02:09:39.040]   You were supposed to be using Hangouts, and then they killed Hangouts.
[02:09:39.040 --> 02:09:40.040]   Me.
[02:09:40.040 --> 02:09:43.160]   So you were supposed to be using Meet or Chat.
[02:09:43.160 --> 02:09:44.160]   I think it's Chat.
[02:09:44.160 --> 02:09:46.040]   You use all of them over my life.
[02:09:46.040 --> 02:09:47.040]   Yes.
[02:09:47.040 --> 02:09:48.560]   And so I didn't know GTalk was still available.
[02:09:48.560 --> 02:09:49.560]   I am amazed.
[02:09:49.560 --> 02:09:53.520]   Hangouts is kind of still hanging around, but it's Google Chat.
[02:09:53.520 --> 02:09:56.840]   I think it would be kill Hangouts, because mine's all chat now.
[02:09:56.840 --> 02:09:58.040]   I don't have Hangouts as much.
[02:09:58.040 --> 02:10:00.240]   They rebranded the-- yeah, yeah, yeah.
[02:10:00.240 --> 02:10:01.240]   I have to try this.
[02:10:01.240 --> 02:10:03.080]   I'm getting this irritating thing.
[02:10:03.080 --> 02:10:05.160]   Sorry, Stacey, go ahead.
[02:10:05.160 --> 02:10:06.160]   No, no.
[02:10:06.160 --> 02:10:07.720]   He's very excited about it.
[02:10:07.720 --> 02:10:08.720]   I know.
[02:10:08.720 --> 02:10:09.720]   I was like, oh, my.
[02:10:09.720 --> 02:10:13.160]   When I reboot the machine, it comes on and says, we're changing.
[02:10:13.160 --> 02:10:15.520]   We're eliminating this and that and that and that.
[02:10:15.520 --> 02:10:17.960]   So get rid of this and start this.
[02:10:17.960 --> 02:10:22.960]   And then I click on it and it goes to 404 every damn time now.
[02:10:22.960 --> 02:10:24.520]   Well, that sucks.
[02:10:24.520 --> 02:10:25.680]   Yeah, it does.
[02:10:25.680 --> 02:10:27.680]   All right, one more thing and then it's waffle time.
[02:10:27.680 --> 02:10:31.080]   Well, no, it's a pick time and just bit.
[02:10:31.080 --> 02:10:35.480]   I want to talk about the Floppotron 3.0.
[02:10:35.480 --> 02:10:37.640]   Oh, and that's the Google change log.
[02:10:37.640 --> 02:10:43.440]   And now, ladies and gentlemen.
[02:10:43.440 --> 02:10:45.240]   You're out of order.
[02:10:45.240 --> 02:10:48.960]   It's time for the Floppotron 3.0.
[02:10:48.960 --> 02:10:55.400]   This guy, this crazy guy put together a musical instrument
[02:10:55.400 --> 02:11:02.760]   made out of hundreds of old floppy disks, the drives.
[02:11:02.760 --> 02:11:04.600]   Because, as you probably know, they
[02:11:04.600 --> 02:11:08.080]   make a little whining sound, which he can, the stepper motor,
[02:11:08.080 --> 02:11:12.000]   control to get a specific frequency.
[02:11:12.000 --> 02:11:15.320]   The problem is the floppies tend to sound kind of bassy.
[02:11:15.320 --> 02:11:19.920]   So he's also using various other things,
[02:11:19.920 --> 02:11:26.160]   like flatbed scanners for the higher tones.
[02:11:26.160 --> 02:11:33.160]   Would you like to hear the Floppotron 3.0 in action?
[02:11:33.160 --> 02:11:34.320]   Here it is on YouTube.
[02:11:34.320 --> 02:11:35.320]   Yes.
[02:11:35.320 --> 02:11:37.800]   [MUSIC PLAYING]
[02:11:37.800 --> 02:11:40.440]   A little longer than a few minutes later.
[02:11:40.440 --> 02:11:43.920]   [MUSIC PLAYING]
[02:11:43.920 --> 02:11:47.560]   One, the Timothy.
[02:11:47.560 --> 02:11:49.240]   That was 2016.
[02:11:49.240 --> 02:11:51.000]   This is 2022.
[02:11:51.000 --> 02:11:53.960]   The new Floppotron.
[02:11:53.960 --> 02:11:55.880]   The size of this sucker.
[02:11:55.880 --> 02:11:57.040]   Jesus.
[02:11:57.040 --> 02:11:58.960]   Wow.
[02:11:58.960 --> 02:12:02.280]   This guy ever gets dates.
[02:12:02.280 --> 02:12:04.080]   You want to come over and listen to some music?
[02:12:04.080 --> 02:12:06.080]   [LAUGHTER]
[02:12:06.080 --> 02:12:09.080]   [MUSIC PLAYING]
[02:12:09.080 --> 02:12:12.520]   [MUSIC PLAYING]
[02:12:12.520 --> 02:12:13.360]   This is pretty great.
[02:12:13.360 --> 02:12:14.360]   I got a rock.
[02:12:14.360 --> 02:12:16.360]   [MUSIC PLAYING]
[02:12:16.360 --> 02:12:18.360]   [MUSIC PLAYING]
[02:12:18.360 --> 02:12:20.360]   [LAUGHTER]
[02:12:20.360 --> 02:12:22.360]   This is better than TikTok.
[02:12:22.360 --> 02:12:24.000]   Thank you for watching.
[02:12:24.000 --> 02:12:26.440]   That's scanners.
[02:12:26.440 --> 02:12:28.520]   512 floppy disk drives.
[02:12:28.520 --> 02:12:30.520]   [MUSIC PLAYING]
[02:12:30.520 --> 02:12:32.520]   [MUSIC PLAYING]
[02:12:32.520 --> 02:12:34.520]   [MUSIC PLAYING]
[02:12:34.520 --> 02:12:36.920]   Four flatbed scanners.
[02:12:36.920 --> 02:12:38.560]   Unbelievable.
[02:12:38.560 --> 02:12:39.600]   Doing the low end.
[02:12:39.600 --> 02:12:40.920]   I mean, the high end.
[02:12:40.920 --> 02:12:42.320]   Those are hard drive steppers.
[02:12:42.320 --> 02:12:44.000]   16 of those.
[02:12:44.000 --> 02:12:45.400]   Doing the kind of percussion.
[02:12:45.400 --> 02:12:47.400]   [MUSIC PLAYING]
[02:12:47.400 --> 02:12:54.800]   He's even got custom interfaces.
[02:12:54.800 --> 02:12:58.640]   It's a Floppotron 3.0 PSU module.
[02:12:58.640 --> 02:13:00.640]   [MUSIC PLAYING]
[02:13:00.640 --> 02:13:01.760]   Power consumption.
[02:13:01.760 --> 02:13:04.760]   300 watts peak.
[02:13:04.760 --> 02:13:05.280]   No.
[02:13:05.280 --> 02:13:08.800]   300 watts average 1.2 kilowatts peak.
[02:13:08.800 --> 02:13:12.240]   There is, you saw that emergency stop button.
[02:13:12.240 --> 02:13:15.160]   When the wife comes in.
[02:13:15.160 --> 02:13:16.160]   Wow.
[02:13:16.160 --> 02:13:18.160]   [MUSIC PLAYING]
[02:13:18.160 --> 02:13:20.920]   He went from, does this guy get dates to being married?
[02:13:20.920 --> 02:13:21.440]   All in one makes him joking.
[02:13:21.440 --> 02:13:22.040]   I don't know.
[02:13:22.040 --> 02:13:23.040]   I'm thinking that.
[02:13:23.040 --> 02:13:24.040]   I'm joking.
[02:13:24.040 --> 02:13:26.040]   [LAUGHTER]
[02:13:26.040 --> 02:13:31.720]   This is really mind-boggling.
[02:13:31.720 --> 02:13:32.520]   Press it.
[02:13:32.520 --> 02:13:33.040]   Yeah.
[02:13:33.040 --> 02:13:35.640]   I said I'm up with Mr. Bert.
[02:13:35.640 --> 02:13:36.640]   Sing a little bit.
[02:13:36.640 --> 02:13:38.440]   I love people that do stuff like this.
[02:13:38.440 --> 02:13:41.480]   What does he do for a living?
[02:13:41.480 --> 02:13:42.040]   I don't know.
[02:13:42.040 --> 02:13:43.040]   Let me see.
[02:13:43.040 --> 02:13:45.040]   [MUSIC PLAYING]
[02:13:45.040 --> 02:13:52.080]   [LAUGHTER]
[02:13:52.080 --> 02:13:53.560]   We're so easy to abuse.
[02:13:53.560 --> 02:13:56.960]   With great power comes a great energy bill, he says.
[02:13:56.960 --> 02:13:57.480]   [LAUGHTER]
[02:13:57.480 --> 02:13:59.080]   Oh, boy.
[02:13:59.080 --> 02:14:00.320]   Oh, boy.
[02:14:00.320 --> 02:14:02.280]   That's a long article.
[02:14:02.280 --> 02:14:06.120]   Silent.org.pl is Polish.
[02:14:06.120 --> 02:14:08.320]   The next step of making videos of the new setup,
[02:14:08.320 --> 02:14:11.760]   some minor software bugs to fix, some software to add,
[02:14:11.760 --> 02:14:14.280]   like end stops or scanners.
[02:14:14.280 --> 02:14:15.760]   And he wants to do some new instruments
[02:14:15.760 --> 02:14:18.640]   at .map.mp3x printer, maybe some automated lighting.
[02:14:18.640 --> 02:14:20.320]   This is Stacy on IOT.
[02:14:20.320 --> 02:14:22.680]   This is you, Stacy.
[02:14:22.680 --> 02:14:24.640]   This is where you're headed.
[02:14:24.640 --> 02:14:26.040]   If you continue down this road--
[02:14:26.040 --> 02:14:27.360]   Yet I heard Doted.
[02:14:27.360 --> 02:14:28.800]   This dark road that I'm on.
[02:14:28.800 --> 02:14:31.160]   [LAUGHTER]
[02:14:31.160 --> 02:14:32.320]   Wow.
[02:14:32.320 --> 02:14:37.800]   The flopotron3.0, silent.org.pl.
[02:14:37.800 --> 02:14:39.440]   All right, final ad, then we're picks,
[02:14:39.440 --> 02:14:42.720]   and then it's waffle time.
[02:14:42.720 --> 02:14:44.280]   You know, one of the things I did
[02:14:44.280 --> 02:14:46.360]   love about termination shock, which we're going to talk about
[02:14:46.360 --> 02:14:48.680]   tomorrow on Stacy's book club, is there
[02:14:48.680 --> 02:14:53.440]   is a mention of the Stroopwaffle in the book.
[02:14:53.440 --> 02:14:54.840]   They have a lovely meal.
[02:14:54.840 --> 02:14:58.240]   And at the end, the dessert, he says, of course,
[02:14:58.240 --> 02:14:59.680]   a Stroopwaffle.
[02:14:59.680 --> 02:15:03.400]   So that involves the queen of the Netherlands.
[02:15:03.400 --> 02:15:04.000]   Exactly.
[02:15:04.000 --> 02:15:07.360]   Saskia loves those Stroopwaffles.
[02:15:07.360 --> 02:15:09.560]   This week in Google is brought to you quite literally
[02:15:09.560 --> 02:15:10.440]   by Cash Fly.
[02:15:10.440 --> 02:15:14.640]   Now, we don't have 512 hard drives making music,
[02:15:14.640 --> 02:15:16.840]   but we do have Cash Fly.
[02:15:16.840 --> 02:15:19.680]   Our content delivery network-- we've
[02:15:19.680 --> 02:15:21.200]   been using them for more than a decade.
[02:15:21.200 --> 02:15:26.360]   They really came to our rescue 10 years ago, more than that now.
[02:15:26.360 --> 02:15:28.680]   I think 12 years ago, when we were struggling
[02:15:28.680 --> 02:15:30.920]   with all the downloads we had to give,
[02:15:30.920 --> 02:15:34.640]   and then we went to video and, oh, it's just a nightmare,
[02:15:34.640 --> 02:15:36.960]   Cash Fly's the best.
[02:15:36.960 --> 02:15:39.680]   50 points of presence all over the world,
[02:15:39.680 --> 02:15:43.360]   which means your content is delivered close to your customers.
[02:15:43.360 --> 02:15:45.280]   Outperforms local CDNs.
[02:15:45.280 --> 02:15:49.160]   In fact, it was so much faster than our 30% faster
[02:15:49.160 --> 02:15:50.320]   than other CDNs.
[02:15:50.320 --> 02:15:56.520]   10 times faster than our old HTTP method was a revolution.
[02:15:56.520 --> 02:15:59.200]   And I bet you never say to yourself, gosh,
[02:15:59.200 --> 02:16:01.800]   it's slow downloading this Twitter podcast.
[02:16:01.800 --> 02:16:03.720]   It's not thanks to Cash Fly.
[02:16:03.720 --> 02:16:07.400]   Now, Cash Fly does low latency, ultra low latency,
[02:16:07.400 --> 02:16:10.360]   video streaming, deliver your video with Cash Fly,
[02:16:10.360 --> 02:16:12.200]   the best throughput and global reach,
[02:16:12.200 --> 02:16:14.360]   making your content infinitely scalable.
[02:16:14.360 --> 02:16:17.480]   You can go live in hours, not days, and get this.
[02:16:17.480 --> 02:16:20.640]   Sub 1 second latency.
[02:16:20.640 --> 02:16:22.000]   Less than a second.
[02:16:22.000 --> 02:16:23.920]   Did your unreliable web RTC solution
[02:16:23.920 --> 02:16:26.320]   for their WebSocket live video workflow?
[02:16:26.320 --> 02:16:29.680]   Scalable to millions of users.
[02:16:29.680 --> 02:16:32.560]   You should also check out Cash Fly's storage optimization
[02:16:32.560 --> 02:16:33.320]   system.
[02:16:33.320 --> 02:16:36.400]   We've been kind of using an unannounced version
[02:16:36.400 --> 02:16:37.760]   of this for a long time.
[02:16:37.760 --> 02:16:40.480]   We put our content on Cash Fly's servers,
[02:16:40.480 --> 02:16:42.680]   which takes load off the origin servers,
[02:16:42.680 --> 02:16:45.800]   reduces your S3 bills, and increases your cash hit
[02:16:45.800 --> 02:16:48.160]   ratio to 100%.
[02:16:48.160 --> 02:16:52.800]   Ask about SOS, Cash Fly's storage optimization system,
[02:16:52.800 --> 02:16:56.280]   and with Cash Fly's elite managed packages.
[02:16:56.280 --> 02:16:59.880]   You get fully managed CDN solutions, VIP treatment,
[02:16:59.880 --> 02:17:03.560]   24/7 support, response times in less than an hour.
[02:17:03.560 --> 02:17:07.440]   And I could tell you, the Cash Fly team is fantastic.
[02:17:07.440 --> 02:17:09.400]   If there's ever a problem, they solve it.
[02:17:09.400 --> 02:17:11.280]   And by the way, there's never been a problem.
[02:17:11.280 --> 02:17:12.840]   They're incredible.
[02:17:12.840 --> 02:17:13.520]   So what do you get?
[02:17:13.520 --> 02:17:15.360]   Ultra low latency, video streaming,
[02:17:15.360 --> 02:17:17.320]   more than a million concurrent users,
[02:17:17.320 --> 02:17:21.560]   lightning fast, gaming, zero lag, zero glitches, zero outages,
[02:17:21.560 --> 02:17:24.720]   mobile content optimization that offers automatic
[02:17:24.720 --> 02:17:26.480]   and simple image optimization.
[02:17:26.480 --> 02:17:29.160]   So your site loads faster on any device.
[02:17:29.160 --> 02:17:31.520]   And with those multiple CDNs, you're
[02:17:31.520 --> 02:17:36.200]   getting short routes, minimum performance glitches,
[02:17:36.200 --> 02:17:42.440]   and 100% availability in the last 12 months, 100% availability.
[02:17:42.440 --> 02:17:44.920]   A 98% cash hit ratio as well.
[02:17:44.920 --> 02:17:49.640]   Best of all, Cash Fly has a 24/7, 365 priority support.
[02:17:49.640 --> 02:17:51.680]   So you know, they'll always be there for you when you need them.
[02:17:51.680 --> 02:17:52.680]   We love Cash Fly.
[02:17:52.680 --> 02:17:56.280]   We really are so grateful for all that they've done for us.
[02:17:56.280 --> 02:17:58.080]   They've put us on the air and kept us on the air.
[02:17:58.080 --> 02:18:00.440]   Learn more at cashfly.com.
[02:18:00.440 --> 02:18:04.200]   C-A-C-H-E-F-L-Y.com.
[02:18:04.200 --> 02:18:05.280]   Thank you, Cash Fly.
[02:18:05.280 --> 02:18:07.040]   Thank you, thank you, thank you.
[02:18:07.040 --> 02:18:11.440]   The bottom of my heart, cashfly.com.
[02:18:11.440 --> 02:18:15.360]   Stacey, Stacey, you probably have 100 things
[02:18:15.360 --> 02:18:17.040]   you want to talk about since you've
[02:18:17.040 --> 02:18:18.840]   been gone so much lately.
[02:18:18.840 --> 02:18:21.280]   What do you have for us?
[02:18:21.280 --> 02:18:23.080]   Nothing, absolutely nothing.
[02:18:23.080 --> 02:18:24.520]   A Stroop Waffle.
[02:18:24.520 --> 02:18:27.680]   She's got a Stroop Waffle for us.
[02:18:27.680 --> 02:18:30.760]   No, I have-- OK, so Father's Day is this weekend.
[02:18:30.760 --> 02:18:31.440]   That's in point, yeah.
[02:18:31.440 --> 02:18:34.120]   Oh, crap, yeah.
[02:18:34.120 --> 02:18:35.040]   So--
[02:18:35.040 --> 02:18:35.880]   I have a father.
[02:18:35.880 --> 02:18:37.280]   I better get going, yeah.
[02:18:37.280 --> 02:18:39.200]   Good to know.
[02:18:39.200 --> 02:18:42.000]   I bought this, and I just got--
[02:18:42.000 --> 02:18:45.320]   so my husband actually got this for me for Mother's Day,
[02:18:45.320 --> 02:18:46.520]   and I'm just going to tell you all.
[02:18:46.520 --> 02:18:48.440]   I am really spoiled.
[02:18:48.440 --> 02:18:49.400]   Just know that.
[02:18:49.400 --> 02:18:53.120]   So this is my new Kindle 8.
[02:18:53.120 --> 02:18:55.640]   The reason I got it is because it's waterproof,
[02:18:55.640 --> 02:18:57.640]   and I wanted to read books in my hot tub.
[02:18:57.640 --> 02:18:58.800]   Oh, that's a nice--
[02:18:58.800 --> 02:19:00.800]   Oh, great.
[02:19:00.800 --> 02:19:03.280]   No, that's a great thing.
[02:19:03.280 --> 02:19:05.080]   The wall guarded the privilege.
[02:19:05.080 --> 02:19:07.080]   Flex on them, flex on them.
[02:19:07.080 --> 02:19:08.840]   I know, y'all.
[02:19:08.840 --> 02:19:10.840]   I am high.
[02:19:10.840 --> 02:19:12.760]   Just ask.
[02:19:12.760 --> 02:19:15.880]   I'm a simple woman who needs some very fancy things.
[02:19:15.880 --> 02:19:18.840]   [LAUGHTER]
[02:19:18.840 --> 02:19:22.200]   But my old Kindle was perfectly fine,
[02:19:22.200 --> 02:19:24.840]   and so my husband got it for me.
[02:19:24.840 --> 02:19:25.680]   Oh.
[02:19:25.680 --> 02:19:27.000]   It actually does make a nice gift.
[02:19:27.000 --> 02:19:28.240]   But I was like, I can't take this.
[02:19:28.240 --> 02:19:29.160]   This Kindle is fine.
[02:19:29.160 --> 02:19:30.720]   I've got to wait till this one breaks,
[02:19:30.720 --> 02:19:32.640]   or I leave an airplane because I did that.
[02:19:32.640 --> 02:19:33.680]   Is this a paper white?
[02:19:33.680 --> 02:19:35.360]   In a laces, what kind of--
[02:19:35.360 --> 02:19:36.440]   It is a paper white.
[02:19:36.440 --> 02:19:38.400]   It's the eighth generation paper white.
[02:19:38.400 --> 02:19:40.240]   OK.
[02:19:40.240 --> 02:19:43.360]   So I actually ended up-- my daughter's Kindle broke
[02:19:43.360 --> 02:19:44.480]   while she was quarantining.
[02:19:44.480 --> 02:19:46.840]   So I gave her my Kindle, and then--
[02:19:46.840 --> 02:19:47.840]   Oh, likely.
[02:19:47.840 --> 02:19:50.160]   You used this one.
[02:19:50.160 --> 02:19:52.400]   What happened to the book reader I gave you?
[02:19:52.400 --> 02:19:53.240]   Did that--
[02:19:53.240 --> 02:19:55.760]   [LAUGHTER]
[02:19:55.760 --> 02:19:57.560]   It's over there.
[02:19:57.560 --> 02:19:59.560]   I couldn't read it in the hot tub, Leo.
[02:19:59.560 --> 02:20:00.320]   Oh, no.
[02:20:00.320 --> 02:20:01.760]   That's a good point.
[02:20:01.760 --> 02:20:03.960]   But so I finally opened it because I really--
[02:20:03.960 --> 02:20:07.440]   Or she did read it in the hot tub, but it's a goner.
[02:20:07.440 --> 02:20:08.320]   Yeah, it did work.
[02:20:08.320 --> 02:20:13.240]   But so now, I tested it because one of the things I was
[02:20:13.240 --> 02:20:16.120]   able to do in Puerto Rico, we were in the hotel,
[02:20:16.120 --> 02:20:19.240]   had a pool, and it had little lounge chairs in the pool.
[02:20:19.240 --> 02:20:21.400]   And so I could read in the lounge chair in the pool.
[02:20:21.400 --> 02:20:21.920]   Oh.
[02:20:21.920 --> 02:20:22.720]   It worked.
[02:20:22.720 --> 02:20:24.120]   So this is truly waterproof.
[02:20:24.120 --> 02:20:26.040]   I love the paper whites.
[02:20:26.040 --> 02:20:27.640]   The screen is so crisp.
[02:20:27.640 --> 02:20:29.120]   It's really good.
[02:20:29.120 --> 02:20:29.880]   It's really nice.
[02:20:29.880 --> 02:20:31.040]   Yeah, I don't know if-- here.
[02:20:31.040 --> 02:20:32.200]   Let's show you my--
[02:20:32.200 --> 02:20:36.280]   Is that the signature edition?
[02:20:36.280 --> 02:20:39.920]   With that lock screen ads, or do you get the lock screen ads?
[02:20:39.920 --> 02:20:40.880]   Oh, I get the lock screen.
[02:20:40.880 --> 02:20:41.800]   I love the lock screen.
[02:20:41.800 --> 02:20:42.200]   I'm not going to lie.
[02:20:42.200 --> 02:20:42.200]   I know.
[02:20:42.200 --> 02:20:43.440]   It doesn't bother me at all.
[02:20:43.440 --> 02:20:44.000]   Yeah.
[02:20:44.000 --> 02:20:44.360]   Yeah.
[02:20:44.360 --> 02:20:48.120]   My husband and I are like, romantic brothers, or whatever.
[02:20:48.120 --> 02:20:49.120]   It's like something--
[02:20:49.120 --> 02:20:50.120]   It's usually books.
[02:20:50.120 --> 02:20:51.480]   Yeah.
[02:20:51.480 --> 02:20:52.720]   Oh, look how nice that looks.
[02:20:52.720 --> 02:20:53.720]   Very crisp.
[02:20:53.720 --> 02:20:55.520]   That is so--
[02:20:55.520 --> 02:20:58.320]   Oh, is that LA Miss Doll's book?
[02:20:58.320 --> 02:20:59.880]   Yes, it is.
[02:20:59.880 --> 02:21:00.320]   Who is--
[02:21:00.320 --> 02:21:00.720]   Who is--
[02:21:00.720 --> 02:21:01.320]   Who recognizes--
[02:21:01.320 --> 02:21:01.840]   How?
[02:21:01.840 --> 02:21:03.560]   How did you recognize that?
[02:21:03.560 --> 02:21:05.640]   Give me your book and two words for a sentence.
[02:21:05.640 --> 02:21:08.560]   I named that book in two words.
[02:21:08.560 --> 02:21:11.080]   Our constitution is not good.
[02:21:11.080 --> 02:21:12.560]   Wow.
[02:21:12.560 --> 02:21:13.080]   Amazing.
[02:21:13.080 --> 02:21:13.840]   It's excellent.
[02:21:13.840 --> 02:21:16.960]   And I highly recommend the audiobook version,
[02:21:16.960 --> 02:21:19.160]   because it's LA in full performance.
[02:21:19.160 --> 02:21:20.640]   What's the name of it?
[02:21:20.640 --> 02:21:21.320]   It's great.
[02:21:21.320 --> 02:21:23.320]   Is he-- you have it right in front of you.
[02:21:23.320 --> 02:21:24.440]   Hold on.
[02:21:24.440 --> 02:21:26.360]   But I don't have the title in front of you.
[02:21:26.360 --> 02:21:29.200]   Allow me to retort a black guy's guide to the constitution.
[02:21:29.200 --> 02:21:33.920]   I think it's in my-- yes, I think it's already in my wish list.
[02:21:33.920 --> 02:21:34.640]   I will add it.
[02:21:34.640 --> 02:21:35.120]   No, it's not.
[02:21:35.120 --> 02:21:36.000]   I think you'll like it too, Anne.
[02:21:36.000 --> 02:21:37.800]   Yeah, because you've recommended that before.
[02:21:37.800 --> 02:21:38.320]   Yeah.
[02:21:38.320 --> 02:21:38.840]   It's not directly--
[02:21:38.840 --> 02:21:38.840]   It's not directly--
[02:21:38.840 --> 02:21:39.240]   You recommend it.
[02:21:39.240 --> 02:21:39.840]   --to run.
[02:21:39.840 --> 02:21:39.840]   It's not--
[02:21:39.840 --> 02:21:40.440]   --to run.
[02:21:40.440 --> 02:21:40.840]   Yeah.
[02:21:40.840 --> 02:21:42.040]   Wishless.
[02:21:42.040 --> 02:21:47.040]   I just got the John von Neumann book that you recommended before.
[02:21:47.040 --> 02:21:49.480]   And I'm really enjoying it so far.
[02:21:49.480 --> 02:21:50.760]   Very interesting.
[02:21:50.760 --> 02:21:52.960]   Did I recommend Cloud Kooku Land, do you guys?
[02:21:52.960 --> 02:21:54.800]   It's a little touch of sci-fi.
[02:21:54.800 --> 02:21:55.760]   Yes, you did.
[02:21:55.760 --> 02:21:58.040]   Did you like it at all?
[02:21:58.040 --> 02:21:59.040]   Well, you didn't--
[02:21:59.040 --> 02:22:00.160]   Oh, boy.
[02:22:00.160 --> 02:22:03.160]   It was really distressing to me that I was just like--
[02:22:03.160 --> 02:22:05.440]   and maybe it's because I was like quarantining
[02:22:05.440 --> 02:22:06.800]   and like too much was going--
[02:22:06.800 --> 02:22:08.080]   but I just could not get into it.
[02:22:08.080 --> 02:22:09.760]   And I wanted to so bad.
[02:22:09.760 --> 02:22:10.240]   I'll try--
[02:22:10.240 --> 02:22:13.280]   it took me a while to get into it once I did I was glad.
[02:22:13.280 --> 02:22:13.880]   I was like--
[02:22:13.880 --> 02:22:15.200]   Dear God.
[02:22:15.200 --> 02:22:17.760]   What was Jeff thinking?
[02:22:17.760 --> 02:22:18.880]   No, I mean, everybody--
[02:22:18.880 --> 02:22:20.040]   I mean, like everybody was--
[02:22:20.040 --> 02:22:20.680]   Everybody's talking about it.
[02:22:20.680 --> 02:22:21.640]   I went through--
[02:22:21.640 --> 02:22:23.080]   I got five books in a row.
[02:22:23.080 --> 02:22:24.080]   I couldn't stand.
[02:22:24.080 --> 02:22:24.880]   It's awful.
[02:22:24.880 --> 02:22:26.120]   It's awful when that happens.
[02:22:26.120 --> 02:22:26.960]   It really is.
[02:22:26.960 --> 02:22:28.480]   Yeah.
[02:22:28.480 --> 02:22:30.640]   All right, well, I'm going to add Cloud Kooku Land.
[02:22:30.640 --> 02:22:32.840]   And maybe I can hate it along with you.
[02:22:32.840 --> 02:22:33.240]   I don't know.
[02:22:33.240 --> 02:22:34.680]   I'll see.
[02:22:34.680 --> 02:22:36.320]   I mean, again, I probably should--
[02:22:36.320 --> 02:22:38.240]   Oh, this is Anthony Dorr.
[02:22:38.240 --> 02:22:38.740]   Yeah.
[02:22:38.740 --> 02:22:40.500]   This is also my wish list already.
[02:22:40.500 --> 02:22:41.520]   Yeah, yeah, yeah.
[02:22:41.520 --> 02:22:46.580]   I guess I watched too many dad gum TV reruns because it
[02:22:46.580 --> 02:22:49.560]   amazes me how often y'all able to read these books.
[02:22:49.560 --> 02:22:50.760]   We forced ourselves.
[02:22:50.760 --> 02:22:51.440]   We could tell ourselves.
[02:22:51.440 --> 02:22:53.740]   I do have a nice, a new graphic novel
[02:22:53.740 --> 02:22:54.760]   if anyone's into those.
[02:22:54.760 --> 02:22:56.720]   Monstras.
[02:22:56.720 --> 02:22:57.960]   Looks beautiful.
[02:22:57.960 --> 02:22:58.720]   That's beautiful art.
[02:22:58.720 --> 02:22:59.720]   It is beautiful.
[02:22:59.720 --> 02:23:00.020]   Yeah.
[02:23:00.020 --> 02:23:00.980]   Check out the art here.
[02:23:00.980 --> 02:23:02.000]   Let me find a good art.
[02:23:02.000 --> 02:23:03.580]   Nice.
[02:23:03.580 --> 02:23:04.660]   There's lots of good.
[02:23:04.660 --> 02:23:05.320]   I'm trying to find--
[02:23:05.320 --> 02:23:07.440]   Do you let your daughter read those or no?
[02:23:07.440 --> 02:23:08.500]   Oh, yeah.
[02:23:08.500 --> 02:23:09.340]   Yeah.
[02:23:09.340 --> 02:23:10.440]   That's part childhood.
[02:23:10.440 --> 02:23:11.940]   Oh, it did.
[02:23:11.940 --> 02:23:12.680]   Oh, that's beautiful.
[02:23:12.680 --> 02:23:15.080]   Wow.
[02:23:15.080 --> 02:23:16.380]   That looks like Elden Ring.
[02:23:16.380 --> 02:23:18.220]   That's beautiful.
[02:23:18.220 --> 02:23:19.320]   Yeah, it's really nice.
[02:23:19.320 --> 02:23:20.420]   What is Elden Ring?
[02:23:20.420 --> 02:23:21.080]   It's a video game.
[02:23:21.080 --> 02:23:21.980]   I see it going by.
[02:23:21.980 --> 02:23:22.880]   It's a video game.
[02:23:22.880 --> 02:23:23.520]   Yeah.
[02:23:23.520 --> 02:23:24.520]   It's a very, very different--
[02:23:24.520 --> 02:23:26.820]   Blue scale, not the gray scale.
[02:23:26.820 --> 02:23:28.460]   Oh, that's pretty.
[02:23:28.460 --> 02:23:29.460]   Yeah.
[02:23:29.460 --> 02:23:30.280]   Which volume is that?
[02:23:30.280 --> 02:23:31.080]   There's six of them.
[02:23:31.080 --> 02:23:32.300]   This is volume one.
[02:23:32.300 --> 02:23:33.160]   Volume one.
[02:23:33.160 --> 02:23:34.860]   All right.
[02:23:34.860 --> 02:23:37.520]   Jeff Jarvis, give me a number.
[02:23:37.520 --> 02:23:40.600]   Unless you did this on Windows Weekly,
[02:23:40.600 --> 02:23:43.360]   I wonder whether we should start with a moment of silence.
[02:23:43.360 --> 02:23:44.280]   Oh, it's sad.
[02:23:44.280 --> 02:23:45.360]   It's more.
[02:23:45.360 --> 02:23:46.880]   So sad.
[02:23:46.880 --> 02:23:48.880]   It was going to take over the world.
[02:23:48.880 --> 02:23:49.880]   And then--
[02:23:49.880 --> 02:23:50.880]   She's priced.
[02:23:50.880 --> 02:23:54.080]   [LAUGHTER]
[02:23:54.080 --> 02:23:56.860]   When he wakes up, he really wakes up.
[02:23:56.860 --> 02:23:58.600]   Yes, 27 years.
[02:23:58.600 --> 02:23:59.960]   It's gone forever.
[02:23:59.960 --> 02:24:02.320]   Today's the last day of Internet Explorer.
[02:24:02.320 --> 02:24:04.520]   But not only we talked about it,
[02:24:04.520 --> 02:24:08.280]   the funniest article in the Wall Street Journal--
[02:24:08.280 --> 02:24:10.040]   you're going to like this, Jeff,
[02:24:10.040 --> 02:24:11.600]   because it's clearly--
[02:24:11.600 --> 02:24:15.440]   they had quotes with people who use Internet Explorer.
[02:24:15.440 --> 02:24:17.480]   They never used Internet Explorer.
[02:24:17.480 --> 02:24:19.400]   It's clearly--
[02:24:19.400 --> 02:24:23.520]   these are people who merely wanted
[02:24:23.520 --> 02:24:26.520]   to get in the Wall Street Journal, I think.
[02:24:26.520 --> 02:24:28.840]   I'm still trying to process it, said,
[02:24:28.840 --> 02:24:34.800]   Sam Malma Longa, a 31-year-old Polynesian dance instructor
[02:24:34.800 --> 02:24:35.880]   in Eula, Texas.
[02:24:35.880 --> 02:24:37.560]   This sounds like a parody, doesn't it?
[02:24:37.560 --> 02:24:39.920]   Who is attached to Internet Explorer?
[02:24:39.920 --> 02:24:41.360]   I've used it for so long.
[02:24:41.360 --> 02:24:43.800]   It's the first thing I get on my laptop.
[02:24:43.800 --> 02:24:47.480]   Well, sorry to tell you, Sam, it comes with every version
[02:24:47.480 --> 02:24:48.200]   of Windows.
[02:24:48.200 --> 02:24:50.600]   You don't have to get it.
[02:24:50.600 --> 02:24:52.680]   He's making this up.
[02:24:52.680 --> 02:24:53.680]   He's making this up.
[02:24:53.680 --> 02:24:55.000]   Maybe he's thinking of Chrome.
[02:24:55.000 --> 02:24:55.500]   Yes.
[02:24:55.500 --> 02:24:56.520]   He's thinking of Chrome.
[02:24:56.520 --> 02:24:57.160]   He's thinking of Chrome.
[02:24:57.160 --> 02:24:58.600]   That's exactly right.
[02:24:58.600 --> 02:25:03.280]   It's a 22-year-old Tiktoker from Petersburg, Ontario.
[02:25:03.280 --> 02:25:07.400]   Matt Linkard says, it's sad to see it at Go for me, Internet
[02:25:07.400 --> 02:25:10.120]   Explorer, provided a portal to a world of endless information
[02:25:10.120 --> 02:25:11.360]   in computer games.
[02:25:11.360 --> 02:25:12.080]   Oh, my lord.
[02:25:12.080 --> 02:25:13.040]   And porn.
[02:25:13.040 --> 02:25:14.600]   But he's not going to say it.
[02:25:14.600 --> 02:25:18.640]   Joni Casey, this is really hard to believe,
[02:25:18.640 --> 02:25:21.560]   a 67-year-old retiree in Ellicott City, Maryland,
[02:25:21.560 --> 02:25:23.960]   says the Internet Explorer is one of the three
[02:25:23.960 --> 02:25:26.800]   that pop up on my cell phone.
[02:25:26.800 --> 02:25:28.680]   No.
[02:25:28.680 --> 02:25:30.120]   No, it's not.
[02:25:30.120 --> 02:25:31.280]   It's not.
[02:25:31.280 --> 02:25:33.480]   Joni, she's thinking of Chrome.
[02:25:33.480 --> 02:25:34.200]   I guarantee you.
[02:25:34.200 --> 02:25:38.680]   Unbelievable.
[02:25:38.680 --> 02:25:41.440]   The journal, you know, they--
[02:25:41.440 --> 02:25:42.520]   so how does this work, Jeff?
[02:25:42.520 --> 02:25:46.760]   How do you get somebody to--
[02:25:46.760 --> 02:25:48.000]   like, I need--
[02:25:48.000 --> 02:25:49.000]   you put a-- is there some--
[02:25:49.000 --> 02:25:50.320]   These days you probably go on Twitter.
[02:25:50.320 --> 02:25:52.080]   Go on Twitter, so I see somebody who
[02:25:52.080 --> 02:25:53.280]   remembers in an Explorer.
[02:25:53.280 --> 02:25:53.800]   On my day.
[02:25:53.800 --> 02:25:55.240]   In my day, I'd have to call around.
[02:25:55.240 --> 02:25:56.600]   I think in the Wall Street Journal,
[02:25:56.600 --> 02:25:58.640]   you just walk around the newsroom.
[02:25:58.640 --> 02:26:00.680]   Well, where's the DAGM editor?
[02:26:00.680 --> 02:26:01.680]   Cell phone.
[02:26:01.680 --> 02:26:02.360]   Come on.
[02:26:02.360 --> 02:26:05.760]   One of the three that pops up on your cell phone.
[02:26:05.760 --> 02:26:09.800]   What kind of cell phone does she have?
[02:26:09.800 --> 02:26:10.760]   You flip it open.
[02:26:10.760 --> 02:26:16.720]   But my favorite thing, if I can find it here,
[02:26:16.720 --> 02:26:18.800]   I bookmarked it.
[02:26:18.800 --> 02:26:21.200]   This is a tribute to Internet Explorer website
[02:26:21.200 --> 02:26:26.120]   called lagplorer.tms.sx.
[02:26:26.120 --> 02:26:28.160]   This will bring back some memories.
[02:26:28.160 --> 02:26:30.640]   Internet Explorer is a cat problem.
[02:26:30.640 --> 02:26:32.400]   It needs to close.
[02:26:32.400 --> 02:26:35.960]   We're sorry for the inconvenience.
[02:26:35.960 --> 02:26:39.440]   Oh, man, that makes my skin crawl to this day.
[02:26:39.440 --> 02:26:41.440]   Oh, man.
[02:26:41.440 --> 02:26:42.440]   [LAUGHTER]
[02:26:42.440 --> 02:26:43.200]   Explain that.
[02:26:43.200 --> 02:26:44.440]   Explain this to the audio companies.
[02:26:44.440 --> 02:26:45.520]   You're listening and you're not seeing.
[02:26:45.520 --> 02:26:48.320]   But this happened every-- it's funny when I show these people
[02:26:48.320 --> 02:26:50.000]   everybody recognizes it.
[02:26:50.000 --> 02:26:50.440]   Oh, yeah.
[02:26:50.440 --> 02:26:52.800]   Internet Explorer would crash.
[02:26:52.800 --> 02:26:55.440]   And then the pop-up that would say something's gone wrong.
[02:26:55.440 --> 02:26:56.800]   [LAUGHTER]
[02:26:56.800 --> 02:26:59.800]   Would paint itself on the screen and you could paint with it
[02:26:59.800 --> 02:27:02.160]   because you could just drag it around
[02:27:02.160 --> 02:27:04.320]   and there'd be a million of them.
[02:27:04.320 --> 02:27:05.840]   A million of them.
[02:27:05.840 --> 02:27:09.080]   Lagplorer.tms.sx, if you want it.
[02:27:09.080 --> 02:27:10.440]   Oh, that's hilarious.
[02:27:10.440 --> 02:27:12.400]   That's gold, sir.
[02:27:12.400 --> 02:27:14.400]   That is gold.
[02:27:14.400 --> 02:27:17.160]   Sometimes it's the simplest things in life.
[02:27:17.160 --> 02:27:20.160]   Maybe that's what she means when she says she has three browsers
[02:27:20.160 --> 02:27:22.720]   that pop up on her screen.
[02:27:22.720 --> 02:27:23.520]   I don't know.
[02:27:23.520 --> 02:27:25.160]   She said three, not 300.
[02:27:25.160 --> 02:27:27.480]   That was awesome.
[02:27:27.480 --> 02:27:30.720]   Mr. Ant, what's your-- oh, I know what your thing is.
[02:27:30.720 --> 02:27:32.600]   Yeah.
[02:27:32.600 --> 02:27:35.440]   Yesterday, I believe it was yesterday at the time recording
[02:27:35.440 --> 02:27:40.920]   this show, Adobe announced some updates for Lightroom.
[02:27:40.920 --> 02:27:43.760]   And that's Lightroom Classic Lightroom on your mobile as well
[02:27:43.760 --> 02:27:44.360]   as the--
[02:27:44.360 --> 02:27:45.160]   No, Cloud Lightroom.
[02:27:45.160 --> 02:27:47.560]   --you know, I got bad news for you.
[02:27:47.560 --> 02:27:49.520]   This is not Lightroom Classic.
[02:27:49.520 --> 02:27:53.240]   Video editing is only in the new Lightroom.
[02:27:53.240 --> 02:27:53.960]   Well, hold on.
[02:27:53.960 --> 02:27:55.160]   You didn't let me finish.
[02:27:55.160 --> 02:27:57.440]   I'm so bummed.
[02:27:57.440 --> 02:27:57.940]   OK.
[02:27:57.940 --> 02:27:59.360]   Yeah, let me finish.
[02:27:59.360 --> 02:28:02.760]   They added-- they have a lot of different updates out there.
[02:28:02.760 --> 02:28:06.840]   One, which includes video editing for Lightroom Classic.
[02:28:06.840 --> 02:28:10.520]   And you've been able to import your videos
[02:28:10.520 --> 02:28:13.680]   into Lightroom Classic for, I don't know, how many years now.
[02:28:13.680 --> 02:28:15.240]   Never really found a use for it.
[02:28:15.240 --> 02:28:17.240]   But if you want to, you can.
[02:28:17.240 --> 02:28:20.440]   And do a little bit of trimming and some retouching
[02:28:20.440 --> 02:28:22.040]   of your videos, just like you would
[02:28:22.040 --> 02:28:25.240]   if you were on your iPhone and you shot something real quick.
[02:28:25.240 --> 02:28:28.560]   And it gives you the little quote unquote edit function.
[02:28:28.560 --> 02:28:30.360]   It's going to be very similar to that.
[02:28:30.360 --> 02:28:32.800]   But they also added some other AI stuff in there,
[02:28:32.800 --> 02:28:35.280]   such as the mask and features.
[02:28:35.280 --> 02:28:39.720]   You can now do a mask selection, copy that selection,
[02:28:39.720 --> 02:28:43.680]   and paste those same parameters into a similar shot
[02:28:43.680 --> 02:28:47.160]   without having to recreate your mask and recreate
[02:28:47.160 --> 02:28:48.240]   all of your settings and stuff.
[02:28:48.240 --> 02:28:50.480]   Just more AI trying to speed up some things.
[02:28:50.480 --> 02:28:52.360]   I want you to verify this for me, because I
[02:28:52.360 --> 02:28:53.960]   read the Engadget story, which said,
[02:28:53.960 --> 02:28:59.000]   "The new feature only applies to Lightroom, not Lightroom Classic."
[02:28:59.000 --> 02:29:01.360]   And I only use Lightroom Classic.
[02:29:01.360 --> 02:29:03.080]   I won't go near Lightroom CC.
[02:29:03.080 --> 02:29:04.960]   It's horrible.
[02:29:04.960 --> 02:29:06.520]   I don't have a problem with Lightroom CC,
[02:29:06.520 --> 02:29:08.200]   but I'm a Lightroom Classic person.
[02:29:08.200 --> 02:29:09.320]   Can you see if this works?
[02:29:09.320 --> 02:29:12.760]   Because according to Engadget, it does not apply to Lightroom.
[02:29:12.760 --> 02:29:14.560]   I mean, the Lightroom Classic.
[02:29:14.560 --> 02:29:17.040]   I will double check, because I haven't launched Lightroom
[02:29:17.040 --> 02:29:17.880]   today.
[02:29:17.880 --> 02:29:18.320]   They say--
[02:29:18.320 --> 02:29:19.240]   Update Lightroom Classic.
[02:29:19.240 --> 02:29:21.400]   The Classic has supported limited video editing
[02:29:21.400 --> 02:29:24.440]   and trimming in library mode, but doesn't support advanced
[02:29:24.440 --> 02:29:27.360]   editing in the develop module.
[02:29:27.360 --> 02:29:30.520]   But I'm not going to use CC.
[02:29:30.520 --> 02:29:32.760]   Some of the other things do appear in Lightroom Classic,
[02:29:32.760 --> 02:29:33.400]   so--
[02:29:33.400 --> 02:29:34.480]   The other stuff, yes.
[02:29:34.480 --> 02:29:36.960]   I will launch it sometime tonight.
[02:29:36.960 --> 02:29:37.360]   But it up--
[02:29:37.360 --> 02:29:39.520]   I guess I'm not going to do much video editing
[02:29:39.520 --> 02:29:40.440]   in Lightroom, I guess.
[02:29:40.440 --> 02:29:42.000]   You better not.
[02:29:42.000 --> 02:29:44.080]   Here's the use case for it.
[02:29:44.080 --> 02:29:45.360]   I take a lot of pictures.
[02:29:45.360 --> 02:29:48.520]   But when I go to visit my mom in a couple of weeks,
[02:29:48.520 --> 02:29:50.760]   I won't be here, by the way, two weeks from now,
[02:29:50.760 --> 02:29:52.520]   because Jeff's going to host the show,
[02:29:52.520 --> 02:29:56.040]   because I'm going to be in Rhode Island, see my mom.
[02:29:56.040 --> 02:29:58.480]   But when I shoot pictures, I also every once in a while
[02:29:58.480 --> 02:30:01.560]   record like three or four seconds of video, just a little bit.
[02:30:01.560 --> 02:30:03.520]   Because then I like to do a slideshow with some motion
[02:30:03.520 --> 02:30:05.200]   and sound in it.
[02:30:05.200 --> 02:30:07.520]   But it would be nice to just process it all in Lightroom
[02:30:07.520 --> 02:30:10.840]   Classic and trim and stuff like that, maybe color correct,
[02:30:10.840 --> 02:30:12.800]   so that I don't have to export it out to an editor
[02:30:12.800 --> 02:30:14.880]   and then import it back in and stuff like that.
[02:30:14.880 --> 02:30:15.440]   That's all.
[02:30:15.440 --> 02:30:17.280]   I just be nice to have some simple features.
[02:30:17.280 --> 02:30:19.520]   And I put editing in--
[02:30:19.520 --> 02:30:22.360]   sorry if it scratched my eye-- edit in air quotes,
[02:30:22.360 --> 02:30:23.320]   because it's not--
[02:30:23.320 --> 02:30:25.240]   It's not like you're going to create a whole scene.
[02:30:25.240 --> 02:30:27.000]   But I could trim it up and color correct it,
[02:30:27.000 --> 02:30:28.040]   which would be most important.
[02:30:28.040 --> 02:30:28.600]   Yeah.
[02:30:28.600 --> 02:30:29.560]   Right.
[02:30:29.560 --> 02:30:30.080]   Cool.
[02:30:30.080 --> 02:30:32.640]   That's the Adobe updates.
[02:30:32.640 --> 02:30:36.600]   And lastly, I follow Binro Tripods, actually,
[02:30:36.600 --> 02:30:39.280]   use their tripods here and there.
[02:30:39.280 --> 02:30:42.560]   And they have a closeout on one of their tripod models.
[02:30:42.560 --> 02:30:45.640]   So it includes a video ahead.
[02:30:45.640 --> 02:30:47.640]   So a while supplies last.
[02:30:47.640 --> 02:30:48.640]   Check it out.
[02:30:48.640 --> 02:30:49.640]   Check it out.
[02:30:49.640 --> 02:30:52.480]   For like $150.
[02:30:52.480 --> 02:30:53.480]   What is that model?
[02:30:53.480 --> 02:30:55.680]   It's the S6 Pro video head and it's
[02:30:55.680 --> 02:30:57.400]   on the aluminum tripod legs.
[02:30:57.400 --> 02:31:00.800]   And for $150, you can't beat that.
[02:31:00.800 --> 02:31:01.880]   Yes, nice.
[02:31:01.880 --> 02:31:04.920]   It's an older model, but hey, it's Binro,
[02:31:04.920 --> 02:31:06.480]   so it's going to be a good quality.
[02:31:06.480 --> 02:31:08.640]   It's not a cheap tripod, which we've
[02:31:08.640 --> 02:31:09.800]   talked about on hands-off photography.
[02:31:09.800 --> 02:31:12.000]   I don't usually do pics, but this thing
[02:31:12.000 --> 02:31:14.320]   has been driving me crazy.
[02:31:14.320 --> 02:31:14.920]   I love it.
[02:31:14.920 --> 02:31:15.720]   It's so much fun.
[02:31:15.720 --> 02:31:18.080]   And it's also kind of not so much fun.
[02:31:18.080 --> 02:31:20.880]   It's a new plugin.
[02:31:20.880 --> 02:31:22.920]   Oh, it's an install over here.
[02:31:22.920 --> 02:31:23.800]   Oh, here it is.
[02:31:23.800 --> 02:31:25.440]   Called Toucan.
[02:31:25.440 --> 02:31:26.760]   Do you know about this?
[02:31:26.760 --> 02:31:27.600]   Toucan.
[02:31:27.600 --> 02:31:28.840]   So it's language learning.
[02:31:28.840 --> 02:31:30.160]   I'm going to turn it on.
[02:31:30.160 --> 02:31:32.240]   And the way it works-- I have it set for Spanish,
[02:31:32.240 --> 02:31:34.400]   but they have a variety of languages--
[02:31:34.400 --> 02:31:36.600]   Spanish, French, German, Japanese, Korean, Chinese,
[02:31:36.600 --> 02:31:40.800]   Italian, Arabic, Hindi, Portuguese, and Hebrew.
[02:31:40.800 --> 02:31:42.840]   I'm going to use a Spanish.
[02:31:42.840 --> 02:31:46.680]   When you turn it on, you just leave it running.
[02:31:46.680 --> 02:31:48.040]   It's a Firefox plugin.
[02:31:48.040 --> 02:31:50.160]   I think there's a Chrome plugin as well.
[02:31:50.160 --> 02:31:51.240]   You just leave it running.
[02:31:51.240 --> 02:31:53.680]   And then it changes words everywhere
[02:31:53.680 --> 02:31:58.240]   on the web at random intervals into Spanish.
[02:31:58.240 --> 02:32:01.740]   So here's a tweet from AOC, perhaps a banning
[02:32:01.740 --> 02:32:03.440]   youth employment school fundos.
[02:32:03.440 --> 02:32:04.320]   What?
[02:32:04.320 --> 02:32:06.360]   That's Spanish for funding.
[02:32:06.360 --> 02:32:08.240]   And then you have a little audio.
[02:32:08.240 --> 02:32:10.760]   If you didn't have stupid dark mode on,
[02:32:10.760 --> 02:32:12.360]   you could actually see the box.
[02:32:12.360 --> 02:32:13.960]   I'll just tell you this.
[02:32:13.960 --> 02:32:15.520]   I'll do it on-- I don't know.
[02:32:15.520 --> 02:32:17.480]   We're going to mute Mr. Jarvis.
[02:32:17.480 --> 02:32:17.960]   Yeah, I know.
[02:32:17.960 --> 02:32:18.560]   He's terrible.
[02:32:18.560 --> 02:32:19.560]   I know.
[02:32:19.560 --> 02:32:20.640]   He's evil person.
[02:32:20.640 --> 02:32:23.840]   So here's another article.
[02:32:23.840 --> 02:32:26.800]   This is on a gadget in light mode.
[02:32:26.800 --> 02:32:30.760]   So it's still about unmeas away.
[02:32:30.760 --> 02:32:31.640]   Unmeas.
[02:32:31.640 --> 02:32:34.880]   So I'm going to learn from afar the device appears
[02:32:34.880 --> 02:32:37.760]   to be unamezcla, a blend.
[02:32:37.760 --> 02:32:38.760]   Unamezcla.
[02:32:38.760 --> 02:32:39.960]   Unamezcla.
[02:32:39.960 --> 02:32:41.800]   It's Castilian Spanish, by the way.
[02:32:41.800 --> 02:32:44.680]   Cin embargo, however, Cin embargo.
[02:32:44.680 --> 02:32:46.000]   Cin embargo.
[02:32:46.000 --> 02:32:47.160]   Isn't that great?
[02:32:47.160 --> 02:32:49.800]   So without really much effort, you're slowly
[02:32:49.800 --> 02:32:54.640]   getting Spanish vocabulary into your--
[02:32:54.640 --> 02:32:56.920]   A full cup bokeh to dragon?
[02:32:56.920 --> 02:32:59.040]   A bokeh to dragon.
[02:32:59.040 --> 02:32:59.960]   Yes.
[02:32:59.960 --> 02:33:00.400]   I love that.
[02:33:00.400 --> 02:33:01.400]   Bokeh to dragon.
[02:33:01.400 --> 02:33:04.560]   Bokeh to dragon, aka snap to dragon.
[02:33:04.560 --> 02:33:06.040]   Isn't that great?
[02:33:06.040 --> 02:33:07.840]   That's hysterical.
[02:33:07.840 --> 02:33:10.160]   What are the few tricks up its manga?
[02:33:10.160 --> 02:33:11.240]   That's pretty cool.
[02:33:11.240 --> 02:33:11.880]   Isn't that neat?
[02:33:11.880 --> 02:33:13.280]   It's called Japan.
[02:33:13.280 --> 02:33:17.520]   There's a complete free version, which does pretty much
[02:33:17.520 --> 02:33:18.000]   everything.
[02:33:18.000 --> 02:33:19.080]   It does everything I just showed you.
[02:33:19.080 --> 02:33:21.720]   I actually ended up paying because I thought, oh, this is
[02:33:21.720 --> 02:33:22.000]   great.
[02:33:22.000 --> 02:33:25.560]   I'll use this everywhere.
[02:33:25.560 --> 02:33:27.520]   And then you can always turn it off.
[02:33:27.520 --> 02:33:29.160]   You just hit control and then it turns off.
[02:33:29.160 --> 02:33:32.200]   So if you don't want a Spanish mixed into your
[02:33:32.200 --> 02:33:34.640]   ingatchet, you can do that.
[02:33:34.640 --> 02:33:35.480]   But isn't that cool?
[02:33:35.480 --> 02:33:37.200]   Isn't that clever?
[02:33:37.200 --> 02:33:37.680]   Unmezcla.
[02:33:37.680 --> 02:33:38.640]   Yeah.
[02:33:38.640 --> 02:33:43.280]   Toucan. And I think the website is get toucan.com.
[02:33:43.280 --> 02:33:47.080]   Let me just check with you.
[02:33:47.080 --> 02:33:47.560]   OK.
[02:33:47.560 --> 02:33:49.840]   Nope, that's not it.
[02:33:49.840 --> 02:33:50.520]   What is it?
[02:33:50.520 --> 02:33:53.640]   I should hear a yawn misty.
[02:33:53.640 --> 02:33:54.440]   I know.
[02:33:54.440 --> 02:33:55.400]   I was--
[02:33:55.400 --> 02:33:56.600]   Is that a little hint?
[02:33:56.600 --> 02:33:58.240]   I think I'm still--
[02:33:58.240 --> 02:34:00.280]   I don't have long COVID, but I am definitely not--
[02:34:00.280 --> 02:34:02.080]   I hope you don't have long COVID.
[02:34:02.080 --> 02:34:03.520]   By the way, I think you're--
[02:34:03.520 --> 02:34:05.960]   How many kids have tried to trip to Padcovid?
[02:34:05.960 --> 02:34:07.040]   How are there other kids there?
[02:34:07.040 --> 02:34:08.280]   Literally one.
[02:34:08.280 --> 02:34:09.280]   Oh, wow.
[02:34:09.280 --> 02:34:10.440]   How did she get it?
[02:34:10.440 --> 02:34:11.440]   Do you think?
[02:34:11.440 --> 02:34:14.200]   She got it from the--
[02:34:14.200 --> 02:34:16.920]   what's the person who-- the tour leader.
[02:34:16.920 --> 02:34:18.000]   Oh, man.
[02:34:18.000 --> 02:34:22.880]   And ironically, she wore a mask religiously.
[02:34:22.880 --> 02:34:25.440]   There's a picture that was captioned like,
[02:34:25.440 --> 02:34:28.920]   smiles after the airport, masks down so you can see their faces.
[02:34:28.920 --> 02:34:32.280]   And my child's the only one with their mask on.
[02:34:32.280 --> 02:34:33.360]   Wow.
[02:34:33.360 --> 02:34:34.400]   Wow.
[02:34:34.400 --> 02:34:39.400]   Toucan is joined Toucan, T-O-U-C-A, and join toucan.com.
[02:34:39.400 --> 02:34:41.040]   Not get Toucan joined Toucan.
[02:34:41.040 --> 02:34:41.560]   Well, I--
[02:34:41.560 --> 02:34:43.000]   But she's feeling better now, right?
[02:34:43.000 --> 02:34:43.960]   She came home.
[02:34:43.960 --> 02:34:44.400]   Yeah, yeah.
[02:34:44.400 --> 02:34:45.400]   She's better.
[02:34:45.400 --> 02:34:47.080]   We made it home finally.
[02:34:47.080 --> 02:34:48.560]   Whoo, travel is crazy.
[02:34:48.560 --> 02:34:49.560]   Yeah.
[02:34:49.560 --> 02:34:51.960]   So I love Puerto Rico.
[02:34:51.960 --> 02:34:55.240]   I want to get sick in Puerto Rico.
[02:34:55.240 --> 02:34:57.720]   Actually, I want my daughter to get sick in Puerto Rico.
[02:34:57.720 --> 02:34:59.320]   So actually, she's in Portugal right now.
[02:34:59.320 --> 02:35:00.400]   So I--
[02:35:00.400 --> 02:35:01.080]   Oh.
[02:35:01.080 --> 02:35:02.440]   I know that might also work.
[02:35:02.440 --> 02:35:03.640]   It could happen.
[02:35:03.640 --> 02:35:06.440]   I'm just crossing my fingers.
[02:35:06.440 --> 02:35:07.680]   Stacey Higginbotham--
[02:35:07.680 --> 02:35:08.240]   No, I'm not.
[02:35:08.240 --> 02:35:09.040]   Wow.
[02:35:09.040 --> 02:35:10.440]   I don't want to get sick.
[02:35:10.440 --> 02:35:11.840]   I'm like--
[02:35:11.840 --> 02:35:14.480]   Like, hopefully she doesn't watch this, so--
[02:35:14.480 --> 02:35:16.160]   Some Father's Day for you.
[02:35:16.160 --> 02:35:16.600]   Yeah.
[02:35:16.600 --> 02:35:18.960]   Oh, I get to go to Portugal.
[02:35:18.960 --> 02:35:22.240]   Stacey Higginbotham is at StaceyOnIOT.com.
[02:35:22.240 --> 02:35:23.320]   Sign up for a newsletter.
[02:35:23.320 --> 02:35:25.040]   It's free, and it's great.
[02:35:25.040 --> 02:35:26.760]   Check out the events.
[02:35:26.760 --> 02:35:29.720]   And of course, the IOT podcast she does with.
[02:35:29.720 --> 02:35:31.080]   Kevin Tofel.
[02:35:31.080 --> 02:35:34.800]   Did you do it from San Juan?
[02:35:34.800 --> 02:35:36.440]   Yes, man.
[02:35:36.440 --> 02:35:38.240]   And that was-- that show was snakebit.
[02:35:38.240 --> 02:35:42.920]   I had, like, someone hammering and the upstairs.
[02:35:42.920 --> 02:35:44.680]   Well, that's OK.
[02:35:44.680 --> 02:35:45.720]   It's a journey.
[02:35:45.720 --> 02:35:46.240]   It's a good show.
[02:35:46.240 --> 02:35:47.600]   We apologize.
[02:35:47.600 --> 02:35:49.040]   It's worth it.
[02:35:49.040 --> 02:35:50.160]   Thank you, Stacey.
[02:35:50.160 --> 02:35:51.360]   I appreciate your being back.
[02:35:51.360 --> 02:35:53.400]   You'll be here next week, right?
[02:35:53.400 --> 02:35:54.040]   It will.
[02:35:54.040 --> 02:35:56.040]   I'm going to be gone for two weeks, though, in July.
[02:35:56.040 --> 02:35:57.160]   Just setting your expectations.
[02:35:57.160 --> 02:35:57.760]   Oh, me too.
[02:35:57.760 --> 02:35:58.920]   Well, I'm going to be gone.
[02:35:58.920 --> 02:36:00.120]   Yeah, for a week.
[02:36:00.120 --> 02:36:01.160]   Because we're doing the--
[02:36:01.160 --> 02:36:03.400]   took crews in the middle of July.
[02:36:03.400 --> 02:36:06.400]   Jeff Jarvis is, of course, the Leonard Town Professor
[02:36:06.400 --> 02:36:10.240]   for journalistic innovation at the Craig Newmark Graduate
[02:36:10.240 --> 02:36:14.480]   School of Journalism at the City University of New York.
[02:36:14.480 --> 02:36:15.880]   Buzzmachine.com is his blog.
[02:36:15.880 --> 02:36:17.280]   He also writes at Medium.
[02:36:17.280 --> 02:36:20.560]   He's at Jeff Jarvis on Twitter, and a very active tweeter.
[02:36:20.560 --> 02:36:21.360]   So you really should--
[02:36:21.360 --> 02:36:23.600]   I mean, if you want to follow somebody who's got a lot to say--
[02:36:23.600 --> 02:36:25.640]   No matter what's going on, I'm tweeting on the show,
[02:36:25.640 --> 02:36:26.880]   off the show, off the screen.
[02:36:26.880 --> 02:36:27.720]   No, that's good.
[02:36:27.720 --> 02:36:29.880]   Jeff has free opinions.
[02:36:29.880 --> 02:36:33.080]   You're the kind of guy I want to follow on Twitter,
[02:36:33.080 --> 02:36:33.680]   to be honest.
[02:36:33.680 --> 02:36:36.400]   And you're also full of good information
[02:36:36.400 --> 02:36:38.280]   about other people to follow.
[02:36:38.280 --> 02:36:39.120]   And bigger.
[02:36:39.120 --> 02:36:41.240]   You might want to get a new banner image, though.
[02:36:41.240 --> 02:36:44.120]   Actually, I thought I did a little bit better.
[02:36:44.120 --> 02:36:44.880]   That's kind of--
[02:36:44.880 --> 02:36:46.320]   Whoever looks at their own banner.
[02:36:46.320 --> 02:36:47.680]   Yeah, no, that's true.
[02:36:47.680 --> 02:36:50.040]   That's true.
[02:36:50.040 --> 02:36:52.840]   And Mr. Aunt Pruitt, hands-on photography,
[02:36:52.840 --> 02:36:54.760]   twit.tv/hop.
[02:36:54.760 --> 02:36:57.120]   What's coming up on hop?
[02:36:57.120 --> 02:36:58.280]   No.
[02:36:58.280 --> 02:36:58.720]   Dang it.
[02:36:58.720 --> 02:36:59.280]   Oh, yeah.
[02:36:59.280 --> 02:37:01.920]   It's coming back to our video.
[02:37:01.920 --> 02:37:02.640]   Just have a look.
[02:37:02.640 --> 02:37:03.160]   It's too young to forget.
[02:37:03.160 --> 02:37:04.120]   A blank.
[02:37:04.120 --> 02:37:06.840]   We're going back to our video editing module.
[02:37:06.840 --> 02:37:09.920]   We're just going to continue to ramp up
[02:37:09.920 --> 02:37:14.000]   how you can create captivating videos, other than just
[02:37:14.000 --> 02:37:15.960]   recording them with your phone and sticking them out there.
[02:37:15.960 --> 02:37:18.480]   So we're going to talk a little bit more polished this week.
[02:37:18.480 --> 02:37:21.000]   And I will see you in the club tomorrow morning, 9 AM
[02:37:21.000 --> 02:37:22.880]   Pacific, 12 noon Eastern.
[02:37:22.880 --> 02:37:24.360]   You too, Stacy.
[02:37:24.360 --> 02:37:25.680]   It's the Book Club.
[02:37:25.680 --> 02:37:26.400]   I can't wait.
[02:37:26.400 --> 02:37:28.600]   I'm going to be doing a determination shock by Neil
[02:37:28.600 --> 02:37:29.960]   Stevens' book this month.
[02:37:29.960 --> 02:37:31.960]   But if you go to the Book Club, you
[02:37:31.960 --> 02:37:34.280]   can choose the book for next time.
[02:37:34.280 --> 02:37:36.440]   We already know what Stacy's favorite is.
[02:37:36.440 --> 02:37:38.400]   So make sure you show up at the Book Club.
[02:37:38.400 --> 02:37:40.560]   Otherwise, if you can't make it, of course,
[02:37:40.560 --> 02:37:43.280]   it goes on the Twit Plus feed after the fact.
[02:37:43.280 --> 02:37:46.760]   I think I'll be the only one who loved that book.
[02:37:46.760 --> 02:37:47.880]   Oh, I'll just see what--
[02:37:47.880 --> 02:37:49.200]   Bert said--
[02:37:49.200 --> 02:37:49.720]   John, John--
[02:37:49.720 --> 02:37:50.600]   It's a Bert for John.
[02:37:50.600 --> 02:37:51.200]   John, I'm sorry.
[02:37:51.200 --> 02:37:52.200]   John liked it.
[02:37:52.200 --> 02:37:53.200]   Yeah.
[02:37:53.200 --> 02:37:53.700]   Yeah.
[02:37:53.700 --> 02:37:55.680]   Well, we'll see what the members say.
[02:37:55.680 --> 02:37:56.520]   It'll be interesting.
[02:37:56.520 --> 02:37:57.040]   They read it.
[02:37:57.040 --> 02:37:58.120]   Yeah.
[02:37:58.120 --> 02:38:00.760]   Please join us, Stacy's Book Club, tomorrow morning,
[02:38:00.760 --> 02:38:02.680]   at 9 AM Pacific.
[02:38:02.680 --> 02:38:04.880]   Thanks for joining us right here, everybody.
[02:38:04.880 --> 02:38:08.560]   We do Twit every Wednesday, 2 PM Pacific, 5 PM Eastern,
[02:38:08.560 --> 02:38:10.880]   2100 UTC.
[02:38:10.880 --> 02:38:14.560]   You can join us and watch us live if you want it live.twit.tv
[02:38:14.560 --> 02:38:16.560]   or listen, there's an audio stream as well.
[02:38:16.560 --> 02:38:18.600]   Chat with us at IRC.twit.tv.
[02:38:18.600 --> 02:38:21.200]   If you're a club Twit member, there's a Discord chat as well.
[02:38:21.200 --> 02:38:22.280]   Ongoing.
[02:38:22.280 --> 02:38:24.640]   After the fact on demand versions of the show available
[02:38:24.640 --> 02:38:28.240]   at twit.tv/twig, there's also a YouTube channel.
[02:38:28.240 --> 02:38:30.080]   You can catch it there.
[02:38:30.080 --> 02:38:32.400]   Or subscribe in your favorite podcast player,
[02:38:32.400 --> 02:38:33.680]   and that way you'll get it automatically.
[02:38:33.680 --> 02:38:36.360]   The minute it is available.
[02:38:36.360 --> 02:38:38.360]   Thank you, everybody, for watching.
[02:38:38.360 --> 02:38:39.080]   We'll see you next time.
[02:38:39.080 --> 02:38:41.680]   We need--
[02:38:41.680 --> 02:38:41.880]   What?
[02:38:41.880 --> 02:38:46.000]   We need new art that has ant in it for our front art.
[02:38:46.000 --> 02:38:47.360]   None of it has art.
[02:38:47.360 --> 02:38:48.840]   The new art has been OK.
[02:38:48.840 --> 02:38:53.400]   So just to warn you.
[02:38:53.400 --> 02:38:58.240]   The new artwork does have four hands in it.
[02:38:58.240 --> 02:39:00.640]   Oh, no, no, not our podcast art.
[02:39:00.640 --> 02:39:01.080]   Oh.
[02:39:01.080 --> 02:39:02.480]   I'm talking about it.
[02:39:02.480 --> 02:39:02.920]   There's no--
[02:39:02.920 --> 02:39:05.000]   Oh, yeah, we got to fix that.
[02:39:05.000 --> 02:39:06.200]   Jerry's got to come in here.
[02:39:06.200 --> 02:39:08.520]   Next time you're in studio ant,
[02:39:08.520 --> 02:39:11.000]   ant because of COVID has been doing it from the home.
[02:39:11.000 --> 02:39:11.760]   Oh, yeah.
[02:39:11.760 --> 02:39:12.880]   No, I just want to put it--
[02:39:12.880 --> 02:39:14.200]   Let's get new pictures.
[02:39:14.200 --> 02:39:15.240]   No, you're absolutely right on the one.
[02:39:15.240 --> 02:39:17.360]   It took me like two years to get on that ant.
[02:39:17.360 --> 02:39:17.920]   So don't worry.
[02:39:17.920 --> 02:39:19.720]   Yeah, yeah, yeah.
[02:39:19.720 --> 02:39:21.800]   But you were starting to warn us.
[02:39:21.800 --> 02:39:25.120]   Hey, we're going in the year two now, so we're right on time.
[02:39:25.120 --> 02:39:26.200]   So, yeah.
[02:39:26.200 --> 02:39:27.000]   Do you know who's just--
[02:39:27.000 --> 02:39:28.040]   It's like it's time to bring it up.
[02:39:28.040 --> 02:39:29.840]   Do you know who Saul Bass is?
[02:39:29.840 --> 02:39:31.400]   Is that Ringa Bell?
[02:39:31.400 --> 02:39:33.360]   He was a very famous graphic designer.
[02:39:33.360 --> 02:39:37.040]   Did many posters, including "The Man with the Golden Arm,"
[02:39:37.040 --> 02:39:37.760]   you remember?
[02:39:37.760 --> 02:39:38.040]   Right.
[02:39:38.040 --> 02:39:38.560]   I remember that.
[02:39:38.560 --> 02:39:39.880]   Yeah.
[02:39:39.880 --> 02:39:41.880]   Well, it's kind of what our album art looks like.
[02:39:41.880 --> 02:39:45.600]   [LAUGHTER]
[02:39:45.600 --> 02:39:47.720]   You'll see what I mean.
[02:39:47.720 --> 02:39:48.560]   OK.
[02:39:48.560 --> 02:39:49.880]   In a couple of weeks.
[02:39:49.880 --> 02:39:50.680]   Thank you, everybody.
[02:39:50.680 --> 02:39:51.680]   John.
[02:39:51.680 --> 02:39:52.560]   We'll see you next time.
[02:39:52.560 --> 02:39:53.480]   And this week in Google.
[02:39:53.480 --> 02:39:54.880]   Bye-bye.
[02:39:54.880 --> 02:39:56.160]   Well, that is the end now.
[02:39:56.160 --> 02:39:57.040]   We didn't end before.
[02:39:57.040 --> 02:39:57.760]   We ended again.
[02:39:57.760 --> 02:39:58.120]   No.
[02:39:58.120 --> 02:39:59.800]   I never said goodbye.
[02:39:59.800 --> 02:40:00.600]   Oh, I guess.
[02:40:00.600 --> 02:40:02.040]   Sorry, that's because I interrupted.
[02:40:02.040 --> 02:40:02.560]   I'm sorry.
[02:40:02.560 --> 02:40:04.040]   Stop by the Hickenbot.
[02:40:04.040 --> 02:40:06.040]   I've been relegated to a shelf.
[02:40:06.040 --> 02:40:07.400]   I'm so sorry.
[02:40:07.400 --> 02:40:09.920]   Oh, the Jeff photo.
[02:40:09.920 --> 02:40:11.760]   Nobody puts Jeff in the corner.
[02:40:11.760 --> 02:40:13.800]   Oh, buddy.
[02:40:13.800 --> 02:40:16.200]   Don't miss all about Android every week.
[02:40:16.200 --> 02:40:19.200]   We talk about the latest news, hardware, apps,
[02:40:19.200 --> 02:40:22.040]   and now all the developer-y goodness happening
[02:40:22.040 --> 02:40:23.680]   in the Android ecosystem.
[02:40:23.680 --> 02:40:27.000]   I'm Jason Howell, also joined by Ron Richards, Florence Ion,
[02:40:27.000 --> 02:40:29.120]   and our newest co-host on the panel,
[02:40:29.120 --> 02:40:32.160]   when Toudao, who brings her developer chops--
[02:40:32.160 --> 02:40:33.080]   really great stuff.
[02:40:33.080 --> 02:40:36.120]   We also invite people from all over the Android ecosystem
[02:40:36.120 --> 02:40:39.480]   to talk about this mobile platform we love so much.
[02:40:39.480 --> 02:40:43.480]   Join us every Tuesday, all about Android, on twit.tv.
[02:40:43.480 --> 02:40:46.920]   [MUSIC PLAYING]
[02:40:46.920 --> 02:40:50.280]   [MUSIC PLAYING]
[02:40:50.280 --> 02:40:52.860]   (upbeat music)
[02:40:52.860 --> 02:40:55.780]   [MUSIC]

