;FFMETADATA1
title=The Hobbit Menu
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=226
genre=Podcast
comment=http://www.twit.tv/twig
copyright=These netcasts are released under a Creative Commons Attribution Non-Commercial Share-Alike license. TWiT and TWiT Logo are registered trademarks of Leo Laporte
publisher=TWiT
date=2013
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.000]   It's time for Twink this week in Google Gina Trippani's last show of 2013.
[00:00:05.000 --> 00:00:10.000]   She's moving to Brooklyn and somehow, I don't know, maybe she's going in a Conestoga wagon or something.
[00:00:10.000 --> 00:00:13.000]   We won't be able to talk to her until January.
[00:00:13.000 --> 00:00:18.000]   But her last show, Jeff Jarvis, we're going to get into it on the NSA on spying,
[00:00:18.000 --> 00:00:22.000]   and whether it's okay to blackmail foreign agents over their porn habits.
[00:00:22.000 --> 00:00:24.000]   It's coming up next.
[00:00:24.000 --> 00:00:25.000]   Twink.
[00:00:28.000 --> 00:00:30.000]   NetCasts you love.
[00:00:30.000 --> 00:00:32.000]   From people you trust.
[00:00:32.000 --> 00:00:37.000]   This is Twink.
[00:00:37.000 --> 00:00:45.000]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:45.000 --> 00:00:56.000]   This is Twink, this week in Google, Episode 226, recorded November 27, 2013.
[00:00:56.000 --> 00:00:58.000]   The Hobbit Menu.
[00:00:58.000 --> 00:01:04.000]   This week at Google is brought to you by Shutterstock.com with over 28 million high-quality stock photos,
[00:01:04.000 --> 00:01:07.000]   illustrations, vectors and video clips.
[00:01:07.000 --> 00:01:10.000]   Shutterstock helps you take your creative project to the next level.
[00:01:10.000 --> 00:01:18.000]   For 25% off your new account, go to Shutterstock.com and use the offer code Twig, 1113.
[00:01:18.000 --> 00:01:25.000]   It's time for Twink this week in Google to show the covers of Google, the cloud, the Twitterverse,
[00:01:25.000 --> 00:01:27.000]   and the YouTube channel.
[00:01:27.000 --> 00:01:29.000]   We're going to be doing something new about this.
[00:01:29.000 --> 00:01:31.000]   We're going to be doing something new about this.
[00:01:31.000 --> 00:01:33.000]   We're going to be doing something new about this.
[00:01:33.000 --> 00:01:35.000]   We're going to be doing something new about this.
[00:01:35.000 --> 00:01:37.000]   We're going to be doing something new about this.
[00:01:37.000 --> 00:01:39.000]   We're going to be doing something new about this.
[00:01:39.000 --> 00:01:41.000]   We're going to be doing something new about this.
[00:01:41.000 --> 00:01:43.000]   We're going to be doing something new about this.
[00:01:43.000 --> 00:01:45.000]   We're going to be doing something new about this.
[00:01:45.000 --> 00:01:47.000]   We're going to be doing something new about this.
[00:01:47.000 --> 00:01:49.000]   We're going to be doing something new about this.
[00:01:49.000 --> 00:01:51.000]   We're going to be doing something new about this.
[00:01:51.000 --> 00:01:53.000]   We're going to be doing something new about this.
[00:01:53.000 --> 00:01:55.000]   We're going to be doing something new about this.
[00:01:55.000 --> 00:01:57.000]   We're going to be doing something new about this.
[00:01:57.000 --> 00:01:59.000]   We're going to be doing something new about this.
[00:01:59.000 --> 00:02:01.000]   We're going to be doing something new about this.
[00:02:01.000 --> 00:02:03.000]   We're going to be doing something new about this.
[00:02:03.000 --> 00:02:05.000]   We're going to be doing something new about this.
[00:02:05.000 --> 00:02:07.000]   We're going to be doing something new about this.
[00:02:07.000 --> 00:02:09.000]   We're going to be doing something new about this.
[00:02:09.000 --> 00:02:11.000]   We're going to be doing something new about this.
[00:02:11.000 --> 00:02:13.000]   We're going to be doing something new about this.
[00:02:13.000 --> 00:02:15.000]   We're going to be doing something new about this.
[00:02:15.000 --> 00:02:17.000]   We're going to be doing something new about this.
[00:02:17.000 --> 00:02:19.000]   We're going to be doing something new about this.
[00:02:19.000 --> 00:02:21.000]   We're going to be doing something new about this.
[00:02:21.000 --> 00:02:23.000]   We're going to be doing something new about this.
[00:02:23.000 --> 00:02:25.000]   We're going to be doing something new about this.
[00:02:25.000 --> 00:02:27.000]   We're going to be doing something new about this.
[00:02:27.000 --> 00:02:29.000]   We're going to be doing something new about this.
[00:02:29.000 --> 00:02:31.000]   We're going to be doing something new about this.
[00:02:31.000 --> 00:02:33.000]   We're going to be doing something new about this.
[00:02:33.000 --> 00:02:35.000]   We're going to be doing something new about this.
[00:02:35.000 --> 00:02:37.000]   We're going to be doing something new about this.
[00:02:37.000 --> 00:02:39.000]   We're going to be doing something new about this.
[00:02:39.000 --> 00:02:41.000]   We're going to be doing something new about this.
[00:02:41.000 --> 00:02:43.000]   We're going to be doing something new about this.
[00:02:43.000 --> 00:02:45.000]   We're going to be doing something new about this.
[00:02:45.000 --> 00:02:47.000]   We're going to be doing something new about this.
[00:02:47.000 --> 00:02:49.000]   We're going to be doing something new about this.
[00:02:49.000 --> 00:02:51.000]   We're going to be doing something new about this.
[00:02:51.000 --> 00:02:53.000]   We're going to be doing something new about this.
[00:02:53.000 --> 00:02:55.000]   We're going to be doing something new about this.
[00:02:55.000 --> 00:02:57.000]   We're going to be doing something new about this.
[00:02:57.000 --> 00:02:59.000]   We're going to be doing something new about this.
[00:02:59.000 --> 00:03:01.000]   We're going to be doing something new about this.
[00:03:01.000 --> 00:03:03.000]   We're going to be doing something new about this.
[00:03:03.000 --> 00:03:05.000]   We're going to be doing something new about this.
[00:03:05.000 --> 00:03:07.000]   We're going to be doing something new about this.
[00:03:07.000 --> 00:03:09.000]   We're going to be doing something new about this.
[00:03:09.000 --> 00:03:11.000]   We're going to be doing something new about this.
[00:03:11.000 --> 00:03:13.000]   We're going to be doing something new about this.
[00:03:13.000 --> 00:03:15.000]   We're going to be doing something new about this.
[00:03:15.000 --> 00:03:17.000]   We're going to be doing something new about this.
[00:03:17.000 --> 00:03:19.000]   We're going to be doing something new about this.
[00:03:19.000 --> 00:03:21.000]   We're going to be doing something new about this.
[00:03:21.000 --> 00:03:23.000]   We're going to be doing something new about this.
[00:03:23.000 --> 00:03:25.000]   We're going to be doing something new about this.
[00:03:25.000 --> 00:03:27.000]   We're going to be doing something new about this.
[00:03:27.000 --> 00:03:29.000]   We're going to be doing something new about this.
[00:03:29.000 --> 00:03:31.000]   We're going to be doing something new about this.
[00:03:31.000 --> 00:03:33.000]   We're going to be doing something new about this.
[00:03:33.000 --> 00:03:35.000]   We're going to be doing something new about this.
[00:03:35.000 --> 00:03:37.000]   We're going to be doing something new about this.
[00:03:37.000 --> 00:03:39.000]   We're going to be doing something new about this.
[00:03:39.000 --> 00:03:41.000]   We're going to be doing something new about this.
[00:03:41.000 --> 00:03:43.000]   We're going to be doing something new about this.
[00:03:43.000 --> 00:03:45.000]   We're going to be doing something new about this.
[00:03:45.000 --> 00:03:47.000]   We're going to be doing something new about this.
[00:03:47.000 --> 00:03:49.000]   We're going to be doing something new about this.
[00:03:49.000 --> 00:03:51.000]   We're going to be doing something new about this.
[00:03:51.000 --> 00:03:53.000]   We're going to be doing something new about this.
[00:03:53.000 --> 00:03:55.000]   We're going to be doing something new about this.
[00:03:55.000 --> 00:03:57.000]   We're going to be doing something new about this.
[00:03:57.000 --> 00:03:59.000]   We're going to be doing something new about this.
[00:03:59.000 --> 00:04:01.000]   We're going to be doing something new about this.
[00:04:01.000 --> 00:04:03.000]   We're going to be doing something new about this.
[00:04:03.000 --> 00:04:05.000]   We're going to be doing something new about this.
[00:04:05.000 --> 00:04:07.000]   We're going to be doing something new about this.
[00:04:07.000 --> 00:04:09.000]   We're going to be doing something new about this.
[00:04:09.000 --> 00:04:11.000]   We're going to be doing something new about this.
[00:04:11.000 --> 00:04:13.000]   We're going to be doing something new about this.
[00:04:13.000 --> 00:04:15.000]   We're going to be doing something new about this.
[00:04:15.000 --> 00:04:17.000]   We're going to be doing something new about this.
[00:04:17.000 --> 00:04:19.000]   We're going to be doing something new about this.
[00:04:19.000 --> 00:04:21.000]   We're going to be doing something new about this.
[00:04:21.000 --> 00:04:23.000]   We're going to be doing something new about this.
[00:04:23.000 --> 00:04:25.000]   We're going to be doing something new about this.
[00:04:25.000 --> 00:04:27.000]   We're going to be doing something new about this.
[00:04:27.000 --> 00:04:29.000]   We're going to be doing something new about this.
[00:04:29.000 --> 00:04:31.000]   We're going to be doing something new about this.
[00:04:31.000 --> 00:04:33.000]   We're going to be doing something new about this.
[00:04:33.000 --> 00:04:35.000]   We're going to be doing something new about this.
[00:04:35.000 --> 00:04:37.000]   We're going to be doing something new about this.
[00:04:37.000 --> 00:04:39.000]   We're going to be doing something new about this.
[00:04:39.000 --> 00:04:41.000]   We're going to be doing something new about this.
[00:04:41.000 --> 00:04:42.000]   We're going to be doing something new about this.
[00:04:42.000 --> 00:04:44.000]   We're going to be doing something new about this.
[00:04:44.000 --> 00:04:46.000]   We're going to be doing something new about this.
[00:04:46.000 --> 00:04:48.000]   We're going to be doing something new about this.
[00:04:48.000 --> 00:04:50.000]   We're going to be doing something new about this.
[00:04:50.000 --> 00:04:52.000]   We're going to be doing something new about this.
[00:04:52.000 --> 00:04:54.000]   We're going to be doing something new about this.
[00:04:54.000 --> 00:04:56.000]   We're going to be doing something new about this.
[00:04:56.000 --> 00:04:58.000]   We're going to be doing something new about this.
[00:04:58.000 --> 00:05:00.000]   We're going to be doing something new about this.
[00:05:00.000 --> 00:05:02.000]   We're going to be doing something new about this.
[00:05:02.000 --> 00:05:04.000]   We're going to be doing something new about this.
[00:05:04.000 --> 00:05:06.000]   We're going to be doing something new about this.
[00:05:06.000 --> 00:05:08.000]   We're going to be doing something new about this.
[00:05:08.000 --> 00:05:10.000]   We're going to be doing something new about this.
[00:05:10.000 --> 00:05:11.000]   We're going to be doing something new about this.
[00:05:11.000 --> 00:05:14.000]   It's a good thing I did that because I've been behind doing that.
[00:05:14.000 --> 00:05:15.000]   All right.
[00:05:15.000 --> 00:05:17.000]   You've learned.
[00:05:17.000 --> 00:05:19.000]   You did your penance.
[00:05:19.000 --> 00:05:20.000]   Yeah.
[00:05:20.000 --> 00:05:21.000]   Your password penance.
[00:05:21.000 --> 00:05:23.000]   You're going to be doing more than X is 7 right now.
[00:05:23.000 --> 00:05:31.000]   Ironically, the carriers shot down the plan to put kill switches on hand sets.
[00:05:31.000 --> 00:05:33.000]   Yeah, we're just ridiculous, isn't it?
[00:05:33.000 --> 00:05:36.000]   I mean, this is a nice thing.
[00:05:36.000 --> 00:05:40.000]   San Francisco Attorney General George Gaskone tweeted Tuesday saying that
[00:05:40.000 --> 00:05:46.000]   major carriers shot down plans for a remote kill switch for smartphones that was developed
[00:05:46.000 --> 00:05:49.000]   by Gaskone along with Samsung.
[00:05:49.000 --> 00:05:54.000]   I'm sorry, it was developed by Samsung Gaskone and New York Attorney General Eric Schneiderman
[00:05:54.000 --> 00:05:58.000]   have been trying to get manufacturers to add that to all smartphones so that service could
[00:05:58.000 --> 00:06:02.000]   be shut off when a phone is stolen.
[00:06:02.000 --> 00:06:07.000]   And the conspiracy theory about why they did this, which is cynical and nasty, but hey,
[00:06:07.000 --> 00:06:11.000]   it's phone companies is they would lose a fortune in insurance.
[00:06:11.000 --> 00:06:13.000]   That doesn't make sense.
[00:06:13.000 --> 00:06:15.000]   That's what one's theory said.
[00:06:15.000 --> 00:06:16.000]   I have no idea.
[00:06:16.000 --> 00:06:17.000]   It's speculative anyway.
[00:06:17.000 --> 00:06:18.000]   They lose it.
[00:06:18.000 --> 00:06:19.000]   Fortune.
[00:06:19.000 --> 00:06:26.000]   Because phone carriers phone carriers sell insurance for lost and broken phones.
[00:06:26.000 --> 00:06:27.000]   Okay.
[00:06:27.000 --> 00:06:29.000]   And if you buy that, I guess, huh?
[00:06:29.000 --> 00:06:30.000]   Okay.
[00:06:30.000 --> 00:06:31.000]   Gotcha.
[00:06:31.000 --> 00:06:35.000]   I don't know how that if you lose your phone, you still lose your phone.
[00:06:35.000 --> 00:06:38.000]   Breaking it doesn't make any impact on that.
[00:06:38.000 --> 00:06:39.000]   Yes.
[00:06:39.000 --> 00:06:42.000]   The thing I you know, they're going to you're going to buy a new phone.
[00:06:42.000 --> 00:06:44.000]   I know I really don't buy the conspiracy theory.
[00:06:44.000 --> 00:06:49.320]   However, the phone companies have have left themselves wide open to it by not explaining
[00:06:49.320 --> 00:06:50.320]   themselves.
[00:06:50.320 --> 00:06:54.000]   But I thought there I think their attitude is what that didn't happen.
[00:06:54.000 --> 00:06:57.000]   They're not admitting to us now for not not proving this.
[00:06:57.000 --> 00:06:58.000]   I don't understand.
[00:06:58.000 --> 00:07:02.000]   Oh, I know what it is.
[00:07:02.000 --> 00:07:03.000]   I know what it is.
[00:07:03.000 --> 00:07:04.000]   I know here was the full argument.
[00:07:04.000 --> 00:07:13.960]   The whole argument was that if there was no value in stealing phones, then because phones
[00:07:13.960 --> 00:07:16.440]   are stolen all the time now, right?
[00:07:16.440 --> 00:07:19.640]   iPhones are like candy de thieves.
[00:07:19.640 --> 00:07:23.080]   And so if they're if suddenly now it's not worth it to steal phones anymore, then it's
[00:07:23.080 --> 00:07:25.360]   not worth it to buy phones.
[00:07:25.360 --> 00:07:28.760]   The case that you get it stolen by insurance from the case you don't buy that you don't
[00:07:28.760 --> 00:07:31.160]   buy a new phone to replace the phone that got stolen.
[00:07:31.160 --> 00:07:32.720]   I don't buy it.
[00:07:32.720 --> 00:07:33.720]   I don't know.
[00:07:33.720 --> 00:07:35.600]   That makes no sense.
[00:07:35.600 --> 00:07:36.600]   I think there's other reasons.
[00:07:36.600 --> 00:07:42.720]   And it may merely be that they don't want the burden of having to kill phones.
[00:07:42.720 --> 00:07:45.360]   And if they fail it, there may be liability involved.
[00:07:45.360 --> 00:07:49.720]   And I just I think they don't they don't want that in their hands.
[00:07:49.720 --> 00:07:54.320]   They don't want to have to validate that the kill request came from the actual customer.
[00:07:54.320 --> 00:07:57.440]   I just I think they I bet you anything.
[00:07:57.440 --> 00:07:58.920]   First of all, they're not admitting that this happened.
[00:07:58.920 --> 00:08:03.640]   And the only reason we think it happened is because this guy tweeted it.
[00:08:03.640 --> 00:08:05.160]   And so we don't even know that it happened.
[00:08:05.160 --> 00:08:07.280]   But let's assume it did.
[00:08:07.280 --> 00:08:10.160]   The phone company's not admitting to it.
[00:08:10.160 --> 00:08:14.440]   The word from 18th to your Verizon or T-Mobile, but I would bet their lawyer said, Oh, no,
[00:08:14.440 --> 00:08:15.960]   no, you don't want that liability.
[00:08:15.960 --> 00:08:19.280]   Hey, look what I got in the mail.
[00:08:19.280 --> 00:08:20.280]   Can you read that?
[00:08:20.280 --> 00:08:23.040]   It says meet your Google wallet card.
[00:08:23.040 --> 00:08:24.320]   What did you sign in right away?
[00:08:24.320 --> 00:08:26.640]   I did it on the show last week.
[00:08:26.640 --> 00:08:27.640]   That's right.
[00:08:27.640 --> 00:08:28.640]   You did you guys.
[00:08:28.640 --> 00:08:34.440]   log and it said. Six to 10 weeks, rebel number with me. So no, here it is. And it's an interesting
[00:08:34.440 --> 00:08:37.600]   card. I'm not going to show it to you because I don't want like I will show you the front,
[00:08:37.600 --> 00:08:44.660]   because the numbers on the back says activate your car with Google Wallet app or wallet.google.com
[00:08:44.660 --> 00:08:49.460]   and it has this really cool logo. But it is it's a credit card number. Right? Or you
[00:08:49.460 --> 00:08:57.280]   could show it now. It's a master card debit card. And it debits right out of my, well,
[00:08:57.280 --> 00:09:02.360]   wallet balance. Yeah. So how do you put money into the wallet balance? I don't think you
[00:09:02.360 --> 00:09:06.160]   have to. I don't know. The wall is tied to a credit card. Oh, do I have to transfer
[00:09:06.160 --> 00:09:10.960]   money over? Yeah, it deducts from you from what I understand it deducts from your balance
[00:09:10.960 --> 00:09:15.320]   and you add to your balance from whatever whatever other cards or bank accounts you have linked
[00:09:15.320 --> 00:09:20.680]   to your wallet account. That's that's why it's a yes, this wall card mail, November 26th
[00:09:20.680 --> 00:09:26.360]   estimated delivery December 1st. No, I got it. I got it now. And in the last four digits
[00:09:26.360 --> 00:09:30.240]   if you have credit cards, I don't I don't understand that. It's just one more cool thing
[00:09:30.240 --> 00:09:36.360]   to have. Now I want to Google card. Sure. Spend your wallet balance. Make purchases and
[00:09:36.360 --> 00:09:41.800]   get cash from ATMs all funded by your wallet balance. Yeah, by your wallet. Right. It doesn't
[00:09:41.800 --> 00:09:45.000]   make sense. Right? Like at least let's still like with PayPal, you know, I've got my bank
[00:09:45.000 --> 00:09:49.240]   account link to it. When I buy something through PayPal, PayPal that automatically debits that
[00:09:49.240 --> 00:09:53.160]   from my bank account. And I would kind of want Google Wallet to behave the same way.
[00:09:53.160 --> 00:10:00.160]   I don't want to have to manually sort of fill my balance before I use it. So this is what
[00:10:00.160 --> 00:10:05.280]   they're you know, they're saying your balance is zero. But I can add money to my balance
[00:10:05.280 --> 00:10:09.760]   $20 and there's and normally that'd be a fee. This is how I guess how they make some money.
[00:10:09.760 --> 00:10:15.080]   Oh, I see. So I'm reading a story now on USA Today. I didn't put on the run on yet.
[00:10:15.080 --> 00:10:19.120]   I guess it could. It says that this is why they kind of why they had people aren't using
[00:10:19.120 --> 00:10:25.920]   wallet because there's all kinds of yes butts. Right. That you have to have a Google Android
[00:10:25.920 --> 00:10:29.320]   phone. It has to come with that FC, blah, blah, blah. If you have an iPhone app, you
[00:10:29.320 --> 00:10:34.760]   can't use it to tap, blah, blah, blah. Or over you have to be customer of sprint or to
[00:10:34.760 --> 00:10:41.000]   the smaller competitors to get past all of the above hurdles you can have Google Wallet.
[00:10:41.000 --> 00:10:47.880]   It gives you micro payment capabilities in effect. Oh, right? And then because you're
[00:10:47.880 --> 00:10:51.840]   buying a chunk of stuff from Google, they wave the fees. Okay. I'm going to add a hundred
[00:10:51.840 --> 00:10:54.960]   bucks to my card. Now I have a hundred bucks on this card. Plus you don't have to worry
[00:10:54.960 --> 00:10:58.440]   about losing it or the number. Right. I think it's more it feels more secure.
[00:10:58.440 --> 00:11:03.600]   Right. I mean, I feel like it's something I would give my kid, you know, like I've got
[00:11:03.600 --> 00:11:09.920]   here's a couple hundred bucks on this card, you know, use it until. So now your money will
[00:11:09.920 --> 00:11:15.360]   be available in less than an hour. So it's not an instant instant instant. Yeah. That's
[00:11:15.360 --> 00:11:19.480]   another. But it's funny. Like, I mean, I love Google and I trust Google and they have,
[00:11:19.480 --> 00:11:23.280]   you know, they have my tax information because they pay me to the Play Store for my app sales
[00:11:23.280 --> 00:11:27.880]   and I have my bank information. They have everything. But if it comes transferring money,
[00:11:27.880 --> 00:11:33.000]   like I'm still going to reach for Square or maybe even PayPal when I actually have to
[00:11:33.000 --> 00:11:38.400]   before I even think of wallet. And I think it's just a branding thing or what? But I mean,
[00:11:38.400 --> 00:11:43.160]   you can attach money to Gmail now. You've got the card. You don't need NFC on your device
[00:11:43.160 --> 00:11:49.080]   to use the wallet app anymore. Yeah, I don't know. I don't know what the psychological
[00:11:49.080 --> 00:11:53.080]   block is for me anyway. I just I just don't use it now. Ron on all that Android. I'm like
[00:11:53.080 --> 00:11:57.280]   oh, it's on all of that. And he loves it. He loves it and goes into the, you know, into
[00:11:57.280 --> 00:12:03.560]   the gas station place and taps and pays for his Snapple before a week. I love that. Yeah.
[00:12:03.560 --> 00:12:07.320]   And I just I just haven't I think it just takes a lot to change habits.
[00:12:07.320 --> 00:12:18.120]   Laura, let me understand. It's just another credit card really or another debit card.
[00:12:18.120 --> 00:12:23.800]   Yeah. Although it is a master card. So I guess, you know, I just okay, you know what the real
[00:12:23.800 --> 00:12:28.840]   reason is now Google has one more way to monitor what I'm buying. Yeah. I mean, the more I use
[00:12:28.840 --> 00:12:33.640]   this, what they'd love is for me to make every transaction through Google wallet in some
[00:12:33.640 --> 00:12:37.920]   form of fashion. And they're going to have to give me well, I guess I can slaughter my
[00:12:37.920 --> 00:12:40.880]   points because because it's still going to go to my credit card. My credit card gives
[00:12:40.880 --> 00:12:45.520]   me the points. Yeah. So I just got a hundred points from America Express. Well, okay. So
[00:12:45.520 --> 00:12:50.400]   I stay there. Well, you're right. Right. Right. So now my master card, my personal
[00:12:50.400 --> 00:12:54.120]   master card in got to know what I buy everything I buy in the world is from Google. Oh, I like
[00:12:54.120 --> 00:13:00.200]   that. So it's a transfer of knowledge from your mastercard to Google. That's interesting.
[00:13:00.200 --> 00:13:04.680]   I don't know what that all means. I wasn't thinking about it in terms of data. I was thinking
[00:13:04.680 --> 00:13:07.480]   of it in terms of like, well, Google just wants me to buy stuff from the place door.
[00:13:07.480 --> 00:13:11.560]   It comes to Google. You always have to think of it in terms of data, Gina. Yeah, right.
[00:13:11.560 --> 00:13:17.120]   Oh, you're absolutely right. Absolutely right. So now if I use tap to pay Google knows, if
[00:13:17.120 --> 00:13:22.880]   I use my Google card, Google knows if online I use my wallet, Google knows, and all that
[00:13:22.880 --> 00:13:29.600]   information is useful to them. It happened yesterday. We were talking on one of the shows
[00:13:29.600 --> 00:13:36.000]   on Mac break weekly about hearing aids. And I did a search for hearing aids. And within
[00:13:36.000 --> 00:13:40.680]   seconds, then the next side I went to had a big Google ad for hearing aids. Sure. That's
[00:13:40.680 --> 00:13:47.200]   the whole essence of one. What do they call it now this week? Does that bother people
[00:13:47.200 --> 00:13:53.960]   tracking? I mean, to me, that's like, okay, that makes sense. That let me say it this
[00:13:53.960 --> 00:13:58.440]   way because I know about this people, but let me say it this way. If that's all it was,
[00:13:58.440 --> 00:14:02.320]   if you knew for sure that it didn't go any farther than that, that all it did was put
[00:14:02.320 --> 00:14:08.640]   an ad up because you looked for something. Would that be okay with you? Or is it the
[00:14:08.640 --> 00:14:12.560]   presumption that there's something more going on? Well, you know what bothers me about
[00:14:12.560 --> 00:14:18.760]   it is I look for a motel room in Oshkosh and eight weeks later, it's still telling me
[00:14:18.760 --> 00:14:25.520]   about Dan Motel rooms in Oshkosh. I bought it right. Well, that is because yeah, that's
[00:14:25.520 --> 00:14:29.800]   because it's not it's an imperfect science just as recommendations on Amazon are imperfect.
[00:14:29.800 --> 00:14:37.440]   They always recommend stuff I already bought. If I buy a hearing aid with my Google wallet
[00:14:37.440 --> 00:14:45.440]   card, will that stop the ads? Might be. Oh, wouldn't that be cool? Because that's that's
[00:14:45.440 --> 00:14:50.120]   the wall called stop advertising. As soon as I purchase it, you can stop showing me ads
[00:14:50.120 --> 00:14:55.040]   for this. See if Google's really smart, that's not they get all the signals and they start
[00:14:55.040 --> 00:14:59.760]   to put together a picture of not only do you look for it, but he bought it. Okay, stop showing
[00:14:59.760 --> 00:15:04.800]   the ads. I don't know what I think Amazon mean, there have been times when I've searched
[00:15:04.800 --> 00:15:09.160]   looking on Amazon, then I bought it. And the question is, do I still get the ads for that?
[00:15:09.160 --> 00:15:14.640]   I think you do. I don't think it's smart enough. Somebody in the chat room said, you
[00:15:14.640 --> 00:15:18.120]   know what should really happen is now they should give you a hearing aid battery ads.
[00:15:18.120 --> 00:15:22.000]   Yeah, because they know you bought it. I mean, if all of this is because they aren't good
[00:15:22.000 --> 00:15:27.200]   enough yet. You know, it's funny. It's not that adds up bother me. I I that's don't bother
[00:15:27.200 --> 00:15:30.440]   me. I feel like a lot of times they actually do create a better experience. You know, when
[00:15:30.440 --> 00:15:35.080]   I if I get information I didn't have before, the thing that bothers me is the tracking,
[00:15:35.080 --> 00:15:39.760]   the retention, the sort of long term memory and the type of things that Google knows about
[00:15:39.760 --> 00:15:44.680]   me and aggregate that I don't know. And the fact that I don't have a view into that data
[00:15:44.680 --> 00:15:50.600]   and I don't have a way to like to change it or to see I want to see me the way that Google
[00:15:50.600 --> 00:15:53.720]   sees me. You know, it's part of the reason why I'm building think up. But like even like
[00:15:53.720 --> 00:15:56.600]   at one point I went back to Amazon, you know, I've been shopping in Amazon for 15 years
[00:15:56.600 --> 00:16:00.200]   or whatever, whatever. I was looking at purchases that I made, you know, 15 years ago, which
[00:16:00.200 --> 00:16:04.080]   I don't even remember. And it was just, you know, because I have a short memory. I mean,
[00:16:04.080 --> 00:16:07.520]   human, right? But computers don't they have a long memory and can store tons of data,
[00:16:07.520 --> 00:16:12.000]   a lot more data than I can. And it was a moment where I was like, Oh, and at least on Amazon
[00:16:12.000 --> 00:16:16.640]   I can say this was a gift. I already own this. I thought this was somebody else. You know,
[00:16:16.640 --> 00:16:21.640]   I can I can say like, this is me, this isn't me or whatever. But there's not really any
[00:16:21.640 --> 00:16:25.440]   way to do that with Google. And so it's the retention and it's the aggregate sort of insights
[00:16:25.440 --> 00:16:28.840]   that Google has. And that all happens in the dark. And I know that that's happening in
[00:16:28.840 --> 00:16:32.280]   the dark, right? Like I can't do anything about it. And I don't really get a view into
[00:16:32.280 --> 00:16:37.120]   it. Take out doesn't it wouldn't be hard. It wouldn't be hard to add that to your Google
[00:16:37.120 --> 00:16:42.440]   dashboard and Google would gain value from you. If you told Google, never mind that.
[00:16:42.440 --> 00:16:45.400]   The same way you came with Amazon, Amazon am is the gold standard here. And Amazon did
[00:16:45.400 --> 00:16:50.360]   it and lighten self interest because every time my standard joke in conferences, I say
[00:16:50.360 --> 00:16:53.680]   that when I buy my daughter a Taylor Swift album, the next morning I feel like a dirty
[00:16:53.680 --> 00:16:59.640]   old man on Amazon. And right. And Amazon is like, is it a chance to pick? I feel like
[00:16:59.640 --> 00:17:11.320]   a gangster on Amazon. Because all the signs my son buys. So Vince surf, who is Google's
[00:17:11.320 --> 00:17:19.000]   chief internet evangelist speaking to the federal trade commission was asked about privacy.
[00:17:19.000 --> 00:17:23.960]   This is you know, I love it, by the way, super smart guy. This is an Eric Schmidt moment.
[00:17:23.960 --> 00:17:30.600]   It's it's the problem is engineers tell the truth. The truth is. And sometimes they don't
[00:17:30.600 --> 00:17:36.040]   finesse it and he probably could have finessed this better. He says, you know, I grew up in
[00:17:36.040 --> 00:17:40.000]   a small town of 3000 people where there was no privacy. Everybody knows what everybody's
[00:17:40.000 --> 00:17:47.720]   doing. Privacy may actually be an anomaly. Technology created the notion of privacy. It's
[00:17:47.720 --> 00:17:51.600]   the industrial revolution and the growth of urban concentrations that led to a sense
[00:17:51.600 --> 00:17:57.520]   of anonymity. Right. He says, I don't want you to go away thinking I'm shallow about
[00:17:57.520 --> 00:18:04.280]   it. But overall, it will be increasingly difficult for us to achieve privacy. Our social behavior
[00:18:04.280 --> 00:18:06.960]   is also quite damaging with regard to privacy.
[00:18:06.960 --> 00:18:12.280]   This is going to come back to haunt them. Yeah, he could have said, you know, let's put privacy
[00:18:12.280 --> 00:18:19.000]   in perspective privacy is a relatively recent invention of society. We also have benefits
[00:18:19.000 --> 00:18:26.280]   from sharing and should have dodged. You're by the way, the king of public parts. So this
[00:18:26.280 --> 00:18:31.080]   you've you've had a lot of time to think about how to address this. He says the right thing
[00:18:31.080 --> 00:18:38.000]   towards the end. The technology we use today has far out raced our social intuition, our
[00:18:38.000 --> 00:18:43.400]   headlights. We need to develop social conventions that are more respectful of people's privacy.
[00:18:43.400 --> 00:18:48.080]   That's what he should have said. Yes. Just left it at that. But he's an engineer and he
[00:18:48.080 --> 00:18:52.680]   said he's and he's a professor and he wanted to say, well, let me explain historically.
[00:18:52.680 --> 00:18:58.880]   The other course is the privacy now is not just about companies. It's about government.
[00:18:58.880 --> 00:19:07.840]   I chaired as they put it in the UK or MC, the Guardian all day event on Wednesday last week.
[00:19:07.840 --> 00:19:13.880]   That's why I wasn't here. And so I ran a panel with the amazing Yochai Bekler and Rebecca
[00:19:13.880 --> 00:19:19.640]   McKinnon and Senator Bob, former Senator Bob Kerry. And he started off on the on the rant
[00:19:19.640 --> 00:19:25.960]   of, well, I just trust companies a lot more than I just try government and I just said,
[00:19:25.960 --> 00:19:33.360]   whoa, whoa, no, I have a transaction with Google and I can leave Google and I didn't
[00:19:33.360 --> 00:19:38.360]   know this stuff was going on with the NSA. Wrong.
[00:19:38.360 --> 00:19:44.800]   I think that also I would point out that the kind of invasion of privacy in a 3000 person
[00:19:44.800 --> 00:19:51.080]   town is not what we're talking about. Yeah, right. Yeah, they kind of know what you're
[00:19:51.080 --> 00:19:57.040]   up to, but it's not like they have a camera in your bathroom. Right. Right. One argument
[00:19:57.040 --> 00:20:02.480]   that I used in public parts was, I guess an author named Mark Giroud, I think it was,
[00:20:02.480 --> 00:20:08.400]   said that privacy was really an invention of the technology known as the hallway. That
[00:20:08.400 --> 00:20:11.640]   before the hallway, you would go through room to room with people's businesses no matter
[00:20:11.640 --> 00:20:15.080]   what they were doing, stupid and whatever it was. That's interesting. You had to.
[00:20:15.080 --> 00:20:19.280]   People do that. When was the hallway invented? There was a year. I forget what it was. There
[00:20:19.280 --> 00:20:22.720]   was a time period where we leave what? But it said that yes, the one hallways and then
[00:20:22.720 --> 00:20:27.880]   and then in the case of British aristocracy, the back stairs. When the school. Okay, Google
[00:20:27.880 --> 00:20:34.600]   different stairs. When was the hallway invented? I knew that this would be a good theme for
[00:20:34.600 --> 00:20:41.240]   the show. The 18th century. How about that? According to homeowners hub, which is known
[00:20:41.240 --> 00:20:47.600]   for its scholarly expertise. Then let go back to the search results. That was the first
[00:20:47.600 --> 00:20:54.040]   result. And now the internet's dead. So I can't go back. Okay. Here's wiki answers. Know
[00:20:54.040 --> 00:21:03.360]   your meme. The inventor's secret. Who invented the hallway? Chocchoc.com. I don't know if
[00:21:03.360 --> 00:21:05.840]   there really is. We're going to find the definitive answer here.
[00:21:05.840 --> 00:21:10.560]   Got it. So it's so interesting. You know, when we were looking for houses in Brooklyn,
[00:21:10.560 --> 00:21:15.760]   like in like old, you know, old Brooklyn houses, the one that we bought is like 1910. But there's
[00:21:15.760 --> 00:21:19.600]   this like railroad style where it's just, you know, rooms stacked on one another. And so
[00:21:19.600 --> 00:21:23.200]   many of you have to walk through the rooms. That's very common. Yeah. And there are mid
[00:21:23.200 --> 00:21:27.280]   middle rooms that didn't have windows. And it was just like, ah, you know, but then,
[00:21:27.280 --> 00:21:33.720]   you know, you have the one with a really small hallway around around most of the rooms. But
[00:21:33.720 --> 00:21:37.320]   that's so interesting. Somebody saying the Romans had hallways. I don't know. I'm thinking
[00:21:37.320 --> 00:21:43.440]   back to Ephesus. I don't think there are any hallways. The revolutionary invention of
[00:21:43.440 --> 00:21:49.040]   back stairs, back stairs is huge. There's a late 17th century separated servants from employers
[00:21:49.040 --> 00:21:55.360]   according to historian Mark Giroud reading a certain book that I know. Oh, yeah. The use of
[00:21:55.360 --> 00:22:03.280]   interior halls in the 18th century enabled rooms to be closed to traffic. The closet created as a
[00:22:03.280 --> 00:22:10.160]   place to lock up one's stuff later became a retreat for what we would call privacy. In the 19th
[00:22:10.160 --> 00:22:15.680]   century, Richard Senate says Londoners joined clubs, not to socialize as we do today, but to sit in
[00:22:15.680 --> 00:22:21.440]   silence away from the hustle of the city. At the same time, Daniel J. Sola points out,
[00:22:21.440 --> 00:22:26.880]   notice how well I attribute all my stuff. Very well. Work shifted from farms to factories and
[00:22:26.880 --> 00:22:35.680]   offices, train the home at last into a family's retreat. Thus England says Philippe Erez became
[00:22:35.680 --> 00:22:42.960]   quote the birthplace of privacy. Wow, you, this book, everybody should read public parts.
[00:22:42.960 --> 00:22:47.600]   My God, Vince, would you send a copy to Vince, sir? Actually, I actually was thinking about that.
[00:22:47.600 --> 00:22:53.360]   I need to. Yes. I think you just say, you know, just, you're right. Here's my other favorite part is
[00:22:53.360 --> 00:23:01.600]   this in in 1516. So Thomas Moore argued, I feel like I'm Steve now on the reading some long
[00:23:01.600 --> 00:23:07.760]   normal security show argued in his novel, Utopia that your Delix society is a transparent society
[00:23:07.760 --> 00:23:11.600]   with the eyes of everyone upon them. People have no choice but to do their customary worker to
[00:23:11.600 --> 00:23:15.920]   enjoy their pastimes, which are not dishonorable. In Moore's time, everyone worked on the under
[00:23:15.920 --> 00:23:20.880]   the gaze of everyone else. Public business was conducted out of private homes. The cobbler made
[00:23:20.880 --> 00:23:26.080]   his shoes there. The ale house was a house. Privacy in the modern sense was not expected,
[00:23:26.080 --> 00:23:29.280]   even among the rich who were surrounded by servants and homes whose layouts required
[00:23:29.280 --> 00:23:35.040]   residents to walk through rooms to get other rooms. And then I think that's fascinating.
[00:23:35.040 --> 00:23:39.360]   It is. We have learned here. We have learned something.
[00:23:39.360 --> 00:23:46.560]   Vince came from a strange town where they had no halls.
[00:23:46.560 --> 00:23:55.200]   I think this is really a private and that, you know, that's something we go off and have to
[00:23:55.200 --> 00:23:59.040]   point out is that privacy is not enshrined in the Constitution. It's not even mentioned in the
[00:23:59.040 --> 00:24:03.200]   United States Constitution, but that makes sense. They didn't have they didn't have always.
[00:24:03.200 --> 00:24:05.280]   They didn't even know.
[00:24:05.280 --> 00:24:11.440]   Doesn't doesn't the word privy come from the root privacy?
[00:24:11.440 --> 00:24:16.720]   Yes. And the other thing that fascinates me is that the reason in England that
[00:24:16.720 --> 00:24:27.200]   public schools are actually in American jargon private schools is because public schools in England
[00:24:27.200 --> 00:24:32.960]   yeah, but just to say rich entry schools were for public people, the children of public people,
[00:24:32.960 --> 00:24:40.240]   people of privilege. Ah, interesting. Somebody said the fourth amendment implies privacy.
[00:24:40.240 --> 00:24:44.240]   That's the argument that was made better. It had to be written in from 1890s when
[00:24:44.240 --> 00:24:49.920]   Brandeis and Warren wrote their famous essay about privacy. They were seeking to find privacy
[00:24:49.920 --> 00:24:55.280]   in law and they had to find a rationale of it around. Isn't that amazing? But as you say,
[00:24:55.280 --> 00:24:59.920]   there was no specific mention of privacy. Boy, get me started and see what happens.
[00:24:59.920 --> 00:25:05.360]   And yet if you're a child pornographer, Google could go through your Picasa images.
[00:25:05.360 --> 00:25:10.000]   Well, and not only that, if you're a child pornographer and somebody who has an Arab name,
[00:25:10.000 --> 00:25:13.840]   the NSA is going to go through them. Yeah. We got two pornography stories today.
[00:25:13.840 --> 00:25:19.920]   Yeah, a child pornography bust reported by CBS Sacramento is raising questions about Google's
[00:25:19.920 --> 00:25:24.640]   access to consumer data according to the verge. This struck me as a red hearing because because
[00:25:24.640 --> 00:25:30.960]   all the Google did was the government put watermarks into child porn stuff and Google
[00:25:30.960 --> 00:25:35.280]   reported back where they found that stuff. I don't see a problem without
[00:25:35.280 --> 00:25:40.560]   Google says it does not search images indiscriminately, performs automated searches
[00:25:40.560 --> 00:25:45.120]   looking for specific image fingerprints that have been flagged by law enforcement.
[00:25:45.120 --> 00:25:49.760]   And then forward suspicious activity of the National Center for Missing and Exploited Children,
[00:25:49.760 --> 00:25:53.920]   which then passes into law enforcement. I don't know the slightest problem with that.
[00:25:54.720 --> 00:25:55.680]   Oh, I think that's OK.
[00:25:55.680 --> 00:26:00.080]   Knowing that we can say like, hey, Google, show me pictures of cats. You know,
[00:26:00.080 --> 00:26:04.720]   show me my pictures of cats and show me my friends pictures of birds. And like, I mean,
[00:26:04.720 --> 00:26:08.880]   we know that Google has very long searching capabilities of this,
[00:26:08.880 --> 00:26:12.640]   searching for particular watermarks. I don't have a problem with that.
[00:26:12.640 --> 00:26:12.960]   No.
[00:26:12.960 --> 00:26:18.000]   And I'm, you know, knowing what the technology could do, I'm not surprised by that at all.
[00:26:18.000 --> 00:26:20.960]   I don't know why privacy advocates would be surprised by that.
[00:26:20.960 --> 00:26:26.400]   Because it's a knee-jerk response now. Any story angle now on this verges generally,
[00:26:26.400 --> 00:26:30.160]   I think superb, but in this case, they went off the mark.
[00:26:30.160 --> 00:26:35.280]   Well, the what people get nervous, they go, oh, well, if they could do that for
[00:26:35.280 --> 00:26:39.040]   pornography, they could do it. If I start looking at a lot of cat pictures,
[00:26:39.040 --> 00:26:43.680]   suddenly they could do that. Of course, technologically, they can do anything.
[00:26:43.680 --> 00:26:46.800]   But that's what you don't limit it. Why we have live in a world of law,
[00:26:47.520 --> 00:26:51.600]   where some things are illegal or illegal. And as soon as they make searching for cat pictures
[00:26:51.600 --> 00:26:54.960]   illegal, that's when I move to France. Oh, please, medical.
[00:26:54.960 --> 00:26:57.600]   You and Sarah Lane.
[00:26:57.600 --> 00:27:02.080]   Sarah Lane, just the thought of us. She's already there.
[00:27:02.080 --> 00:27:06.000]   So what was the other pornography story? I didn't see that.
[00:27:06.000 --> 00:27:11.840]   Uh, somewhere under NSA, the NSA, according to, if you got a Huffington Post today, the
[00:27:11.840 --> 00:27:16.080]   lead story, Glenn Greenwald writing for everybody on earth now Huffington Post.
[00:27:16.080 --> 00:27:20.320]   Oh, he's ready for the Huffington Post now? Today. What the heck? Wait for today.
[00:27:20.320 --> 00:27:26.080]   I've been opposed to Maro eBay. So there they were the NSA has been going up for the porn habits
[00:27:26.080 --> 00:27:31.680]   of, um, it's even strong to say suspected terrorists, but there were people who had no
[00:27:31.680 --> 00:27:36.960]   necessarily suspicion, but, but, um, they had a plan to use that to embarrass them because
[00:27:36.960 --> 00:27:40.320]   they're not okay. And one of them was an American citizen.
[00:27:40.320 --> 00:27:44.640]   That's not okay. Did you find the story on Huffington? Yeah. Yeah.
[00:27:44.640 --> 00:27:51.920]   Top secret document reveals NSA spied on porn habits as part of plan to discredit radicalized
[00:27:51.920 --> 00:27:57.600]   radicalizers. Yes. But you know, that's, um, well, they had to search hard for that picture.
[00:27:57.600 --> 00:28:01.920]   What picture would we put up that would say porn, but not say porn?
[00:28:01.920 --> 00:28:08.400]   What is the, what is the keyword for this? Um, let's see, the document provided by
[00:28:08.400 --> 00:28:13.040]   an anti-six targets, all Muslims as exemplars of how personal vulnerabilities can,
[00:28:13.040 --> 00:28:19.600]   but this is, well, no, isn't this what we expect? This is called spy work. Uh, if, I mean, they're,
[00:28:19.600 --> 00:28:24.320]   they're foreign nationals. We suspect them of being bad guys. Don't we use whatever tools we can to
[00:28:24.320 --> 00:28:30.560]   undermine them to figure out where their weaknesses are. Uh, if they're homosexuals and they live
[00:28:30.560 --> 00:28:36.400]   in a country where homosexuality is illegal, would we not use that to try to turn them? I mean,
[00:28:36.400 --> 00:28:42.560]   I read a lot of spy novels, you can tell. Um, that seems like, I'm not, I mean, that seems like
[00:28:42.560 --> 00:28:48.880]   what we do. If they're not, if they're not, uh, accused of or suspected of an actual crime.
[00:28:48.880 --> 00:28:52.080]   Well, but that's, if they're, if that's if they're in the United States,
[00:28:52.080 --> 00:28:56.960]   but this is presumably not in the U S. Well, but that's, that's another point is we've got to get
[00:28:56.960 --> 00:29:01.600]   past this idea that that, uh, unless you're an American citizen, you have no rights whatsoever
[00:29:01.600 --> 00:29:07.840]   under the trawl of the American government that can't live. I can't. Well, but okay. Um,
[00:29:08.960 --> 00:29:12.880]   but okay. But isn't this how you pursue a, uh, uh, uh, uh, uh,
[00:29:12.880 --> 00:29:18.480]   intelligence against foreign, uh, governments that are enemies? I mean, isn't this a long-standing
[00:29:18.480 --> 00:29:21.760]   tradition of this? Well, I mean, it's right. Every time you said, okay, just now, I thought
[00:29:21.760 --> 00:29:28.400]   you were going to say Google. Okay, Google. What's wrong with spying? Is it okay to do this? Um,
[00:29:28.400 --> 00:29:32.720]   it matters if they're foreign Nash. I agree. Look, if I'm a foreign national, I'm going to be
[00:29:32.720 --> 00:29:38.560]   pissed off, but, but, but, but I mean, maybe I read too many spy novels, but that's what kind of
[00:29:38.560 --> 00:29:43.120]   the basis of all this stuff is. You subvert the foreign nationals that you think are threats.
[00:29:43.120 --> 00:29:48.640]   Right. So it's an economic problem very soon. That means that the laws that are attempted in
[00:29:48.640 --> 00:29:54.400]   your, an EU and Brazil to say you may not store any data or pass any data from our citizens
[00:29:54.400 --> 00:29:58.480]   through the U S because the damn NSA is going to come across it. That's going to have a big impact
[00:29:58.480 --> 00:30:06.400]   on, um, our cloud business here on the cloud show. Well, okay. But, uh, if you don't try to
[00:30:07.280 --> 00:30:13.280]   take down terrorists and other bad guys that could have a even more bigger, bigger problem than our
[00:30:13.280 --> 00:30:18.160]   cloud business. If you think the guy is a terrorist, well, that's what they're doing. I mean, uh, so
[00:30:18.160 --> 00:30:22.640]   now the, the only reason, the only thing at all that comes up as illegal in this is that one of the
[00:30:22.640 --> 00:30:27.840]   people was identified as a U S person, uh, and people who are either citizens or residents in the
[00:30:27.840 --> 00:30:32.960]   United States do have better protection, supposedly against an estate surveillance. You don't think
[00:30:32.960 --> 00:30:42.720]   that foreign people should have, people should have, uh, due process? No, none. What do you mean?
[00:30:42.720 --> 00:30:52.160]   By who? The special court that protects foreign people? Well, whom? By whom? Well, then, then,
[00:30:52.160 --> 00:30:58.640]   we don't have to have a, to do a wiretap of a Saudi national, they suspect of financing terrorism.
[00:30:58.640 --> 00:31:02.880]   There is no court you go to to say, can we do a wiretap? That's, that's, uh,
[00:31:02.880 --> 00:31:09.760]   cloak and dagger work. It's surveillance. We've never had any over, I mean, there's the, you know,
[00:31:09.760 --> 00:31:14.560]   there's the house committee on intelligence that does oversight, but there's never been any,
[00:31:14.560 --> 00:31:18.720]   you know, court that says, well, or any, any, any law that says you need a warrant to do that.
[00:31:18.720 --> 00:31:28.000]   That's what we do. Right? I mean, I, I understand, look, if I were foreign, if national, I'd be
[00:31:28.000 --> 00:31:30.560]   pissed. If I'm, if I'm Angela Merkel.
[00:31:30.560 --> 00:31:38.560]   Yeah. What are you looking at me for? I'm just wondering there's not,
[00:31:38.560 --> 00:31:42.720]   do you, is that reprehensible? Gina has no opinion. Gina, what do you think?
[00:31:42.720 --> 00:31:48.880]   I think we're all just global citizens. I don't know. I, I, I, you're a bunch of liberal lefties.
[00:31:48.880 --> 00:31:53.520]   No, we are. Look, I, I have trouble with the distinction between citizens and
[00:31:53.520 --> 00:31:56.960]   foreigns. Everybody's foreign somewhere. I, you know, I don't know. I, I look, I'm not, I'm not
[00:31:56.960 --> 00:32:00.080]   arguing with the way that it's done. I'm honestly, I don't know enough about the way that it's done
[00:32:00.080 --> 00:32:06.000]   to, to really comment intelligently. You don't read spy novels. I don't read my novels. No, no,
[00:32:06.000 --> 00:32:10.640]   watch homeland. You ever watch a homeland? I do, I do love homeland. I'm not watching the
[00:32:10.640 --> 00:32:17.280]   current season, but I did love, okay. Well, last season, okay. So, yeah, I mean, wait a
[00:32:17.280 --> 00:32:21.840]   minute. Why aren't you watching the current season? I don't, I, for, for stupid reasons,
[00:32:21.840 --> 00:32:25.840]   I won't go into, I just, I don't have show time and I haven't had time. I'm getting a little sick
[00:32:25.840 --> 00:32:31.360]   of Carrie's cry face myself. Yeah. And I, and I heard the season wasn't so good and I was.
[00:32:31.360 --> 00:32:34.880]   It's good. No, it's great. Are you kidding? There's a massive twist that will blow your.
[00:32:34.880 --> 00:32:40.160]   Oh, okay. I heard that it wasn't as good as season one and I sort of, I love season one,
[00:32:40.160 --> 00:32:42.480]   but I think there's a, I think they're actually, this is good.
[00:32:42.480 --> 00:32:48.800]   They, whoever told you that hadn't, hadn't gotten that far in the, the first five episodes,
[00:32:48.800 --> 00:32:52.880]   something happens in six that nullifies what you've seen. Well, don't.
[00:32:52.880 --> 00:32:54.080]   Oh, okay. Okay.
[00:32:54.080 --> 00:33:00.000]   But I'm just saying that we, this is a long standing tradition. All nations,
[00:33:00.000 --> 00:33:05.120]   if they identify somebody as hostile to the nation, will pursue any means necessary,
[00:33:05.120 --> 00:33:08.880]   including murdering somebody in Mexico City with a hammer.
[00:33:08.880 --> 00:33:13.360]   If they're, you know, if they're far enough. I guess here's the point. Here's the point.
[00:33:13.360 --> 00:33:18.000]   Here's the point. Would you agree, Leo, that it's wrong for the government to do corporate espionage?
[00:33:18.960 --> 00:33:23.360]   Yes, it needs to be in, it needs to be in the defense of the United States.
[00:33:23.360 --> 00:33:26.560]   Okay. So there has to be some establishment of some.
[00:33:26.560 --> 00:33:28.480]   And there is, but it's not.
[00:33:28.480 --> 00:33:29.040]   Suspicious.
[00:33:29.040 --> 00:33:35.600]   It's not what we think of as US citizens, we don't have the right to a fair trial and a jury of your peers.
[00:33:35.600 --> 00:33:39.680]   That there's oversight by the House Foreign Intelligence Committee and various committees.
[00:33:39.680 --> 00:33:43.200]   But when I, when I interviewed Kerry on, on the Guardian panel,
[00:33:44.400 --> 00:33:48.080]   videos up somewhere online, uh, you know, he said that
[00:33:48.080 --> 00:33:53.040]   congressional oversight isn't that good because he, there hasn't been a single subpoena from,
[00:33:53.040 --> 00:33:53.920]   I agree.
[00:33:53.920 --> 00:33:57.040]   Yeah, I mean, maybe, maybe we need to make it better.
[00:33:57.040 --> 00:34:02.720]   However, um, this is just, this is always gone on. This has gone on since 1777.
[00:34:02.720 --> 00:34:07.520]   Um, and nation, all nations do it. They pursue their national interest.
[00:34:07.520 --> 00:34:14.080]   Um, maybe we, you know, and then we make, we make conventions like the Geneva accords
[00:34:14.800 --> 00:34:18.960]   so that because there's this issue of, you know, blowback.
[00:34:18.960 --> 00:34:22.720]   So we all have kind of some restraint because you, you don't want,
[00:34:22.720 --> 00:34:25.760]   you don't want it to become untrammeled.
[00:34:25.760 --> 00:34:29.440]   Then you really have a problem because we need to have international commerce.
[00:34:29.440 --> 00:34:31.680]   But I think that there's a longstanding tradition of doing this.
[00:34:31.680 --> 00:34:37.120]   Maybe this should change, but I think that's a very big conversation that nobody's proposing.
[00:34:37.120 --> 00:34:40.240]   Yeah, that's a big conversation. But, but the, the internet changes things a little bit.
[00:34:40.240 --> 00:34:46.240]   Like the, the, you know, if, if people in nations other than America and the US are using US services
[00:34:46.240 --> 00:34:50.800]   and what, what that means, like I get that global commerce has been around for a long time, right?
[00:34:50.800 --> 00:34:53.840]   But the internet hasn't been around for that long a time.
[00:34:53.840 --> 00:34:58.880]   So I guess things are changing and very much more, this conversation would never have happened
[00:34:58.880 --> 00:35:03.920]   50 years ago, very much more than ever. We see ourselves as citizens of the world, not as of the US.
[00:35:03.920 --> 00:35:06.400]   You would have never had this conversation 50 years ago.
[00:35:07.600 --> 00:35:10.960]   We're the, we're America. We have the right to defend ourselves. What are you talking about?
[00:35:10.960 --> 00:35:13.680]   Right. Those Ruskies would blow us up in a heartbeat.
[00:35:13.680 --> 00:35:18.560]   You do anything you need to to subvert them, including check up on their porno.
[00:35:18.560 --> 00:35:23.760]   Man, if you could get, how many novels, how many movies, how many plays, how many books have
[00:35:23.760 --> 00:35:28.400]   been written about the idea of figuring out somebody's gay and then blackmailing them.
[00:35:28.400 --> 00:35:30.240]   Yep. Mm-hmm.
[00:35:30.240 --> 00:35:36.480]   I mean, that's, this is a longstanding tradition. Now, I agree. Maybe, maybe this isn't how we want
[00:35:36.480 --> 00:35:41.520]   to proceed, but that conversation we're still far from having. I'm go, go to, go to Washington
[00:35:41.520 --> 00:35:45.200]   and say, Hey, you know, maybe we should be nicer to those bad guys and see how that goes.
[00:35:45.200 --> 00:35:48.080]   And we shouldn't spy on the terrorist porno.
[00:35:48.080 --> 00:35:54.480]   Well, we won't get into this, but I also have a really hard time with the whole good guy,
[00:35:54.480 --> 00:35:58.320]   bad guy dichotomy. That's never, that's never really, I agree, but you're a heavy,
[00:35:58.320 --> 00:36:02.480]   we're not always the good guys. No, God, no. And that's, you know, and see, but this is,
[00:36:02.480 --> 00:36:06.320]   no one is this is, we are now, but that's, yeah, of course, and I agree with you.
[00:36:06.880 --> 00:36:11.440]   I'm, I'm, by the way, I'm arguing a little more strongly than I might feel, but I think this is the,
[00:36:11.440 --> 00:36:15.920]   it's, it's unrealistic to think, Oh, we're going to be nice to people.
[00:36:15.920 --> 00:36:22.560]   Let's, let's extend the same rights and privileges U.S. citizens have to foreign nationals. I don't
[00:36:22.560 --> 00:36:28.800]   think that's going to happen. However, we didn't have Google 50 years ago and Google's a U.S. company.
[00:36:30.000 --> 00:36:36.000]   So if we found out that France was spying on all of us, we'd be pissed.
[00:36:36.000 --> 00:36:41.920]   Just as they are pissed at us. We all use what is, that's the Chinese social scene,
[00:36:41.920 --> 00:36:49.360]   a waybo is the Chinese Twitter. I mean, we all use that. And China used it to surveil us.
[00:36:49.360 --> 00:36:54.240]   And here's the, here's the funny things, you know, I, I, some Chinese people,
[00:36:54.240 --> 00:36:58.880]   journalists tried to get me to set up a waybo account and I said, kind of out of protest,
[00:36:58.880 --> 00:37:03.760]   no, not as long as the government is doing what it is in China. I'm not going to use it. Right. And
[00:37:03.760 --> 00:37:12.000]   then the NSA happened. And you saw that your hat. It's actually a really fascinating question
[00:37:12.000 --> 00:37:17.680]   because I don't know, it may not be practicable, but I mean, wouldn't it be great if we lived in a
[00:37:17.680 --> 00:37:23.200]   world where every person's rights were and privacy were somehow protected? But what would you do?
[00:37:23.200 --> 00:37:31.360]   Would you have a world court where you go and say, I want to wire tap Osama bin Laden? May I,
[00:37:31.360 --> 00:37:35.680]   sir, please, sir, may I? One thing you have is encryption of the yin-yang.
[00:37:35.680 --> 00:37:42.160]   Well, and that's by the way. Okay. So this goes back to this very fundamental conversation that I
[00:37:42.160 --> 00:37:47.520]   think we've talked about before. I know we haven't security now, which is as soon as you say,
[00:37:49.120 --> 00:37:56.560]   as we did after 9/11, never again, then you have to get then then that gives permission to the
[00:37:56.560 --> 00:38:02.240]   NSA and everybody else to go for total information awareness because that is the argument. And on
[00:38:02.240 --> 00:38:07.440]   the panel at the Guardian, Yokai Bengler, who's a brilliant, brilliant legal mind,
[00:38:07.440 --> 00:38:14.640]   love it, Harvard, Berkman Center, said he argued, Bob Kerry said, we have this threat, we have
[00:38:14.640 --> 00:38:20.320]   Muhammad-Adah sitting there doing this and that, we have to be able to stop him. And Yokai came back
[00:38:20.320 --> 00:38:26.640]   and said, no, the threat is overblown. And we've used that threat as a basis to trample rights.
[00:38:26.640 --> 00:38:34.720]   And he said, I grew up in Israel and one gets used to certain things. And I wonder whether the
[00:38:34.720 --> 00:38:39.360]   NSA overreach is such that Americans will start to say even even Americans will start to say that.
[00:38:40.720 --> 00:38:46.000]   I think at some point you have to presume that people like our Secretary of State and our President
[00:38:46.000 --> 00:38:53.760]   are men of goodwill who will pursue our interests judiciously and not use this to blackmail innocence.
[00:38:53.760 --> 00:39:00.240]   Then you know, I think the problem with the problem is the problem is it's not just what they use in
[00:39:00.240 --> 00:39:05.520]   the end to blackmail. It's that along the way they try to gather every dot they can. They have to.
[00:39:05.520 --> 00:39:10.560]   Basic fact, no, they don't have to. They don't have to. Getting more dots doesn't make connecting
[00:39:10.560 --> 00:39:15.520]   the dots easier. It probably makes the connection to the dots harder. The spies by their nature are
[00:39:15.520 --> 00:39:21.680]   going to gather everything they possibly can. The job of the politicians is to exercise oversight
[00:39:21.680 --> 00:39:27.520]   on them to limit what they do and protect rights. That's the balance that should be struck here.
[00:39:27.520 --> 00:39:33.280]   And that balance is struck through transparency, sufficient that the citizens can get mad at the
[00:39:33.280 --> 00:39:37.600]   executive branch. Transparency is sufficient in courts that we know what they're actually doing.
[00:39:37.600 --> 00:39:44.400]   Transparency to Congress so that they can represent our interests in oversight.
[00:39:44.400 --> 00:39:48.080]   This guardian panel, what it came to know is all oversight is falling apart efficiently.
[00:39:48.080 --> 00:39:52.640]   The executive branch hasn't done bup-kiss. Congress isn't really doing anything.
[00:39:52.640 --> 00:39:58.160]   Feinstein only got mad when Angela Merkel's phone got bugged. The courts are a secret.
[00:39:58.160 --> 00:40:01.120]   We don't know anything. The journalists are now afraid that their sources are going to be
[00:40:01.120 --> 00:40:07.520]   compromised. At the end of the day, Leo, the only oversight that can work now is the oversight
[00:40:07.520 --> 00:40:12.480]   coming from the conscience of an individual. Just to say a whistleblower. Edward Snowden.
[00:40:12.480 --> 00:40:17.120]   Right. And they put a headline in the Guardian, I don't like on this piece that I wrote about this,
[00:40:17.120 --> 00:40:23.760]   that it was about Snowden, traitor, or, that wasn't the discussion. The discussion, I think,
[00:40:23.760 --> 00:40:27.440]   is this notion of conscience. And to me, it comes down then, not just,
[00:40:27.440 --> 00:40:32.560]   whistleblowing his conscience after the fact. Hey, this bad thing happened. What we also need
[00:40:32.560 --> 00:40:38.240]   is conscience before the fact. So, was there no, I'm going to go ahead and slime them because
[00:40:38.240 --> 00:40:42.800]   they're gassed to be the ones who did it. But, you know, a caveat here alleged in rumors and
[00:40:42.800 --> 00:40:50.080]   but if level three was the company that allowed the NSA to tap in to the communications,
[00:40:50.080 --> 00:40:55.040]   not only on the internet as a whole, but also among Google and Yahoo servers,
[00:40:55.040 --> 00:41:00.960]   was there no employee of level three who said, hmm, should we do that? Is that evil?
[00:41:00.960 --> 00:41:04.240]   Oh, I doubt it worked that way. I'm sure they got a national security letter. I don't,
[00:41:04.240 --> 00:41:08.720]   I don't think I think that a lot of this telecom stuff is happening very voluntarily.
[00:41:08.720 --> 00:41:11.520]   They didn't need to get letters for a lot of this stuff. Yeah. But I think they,
[00:41:11.520 --> 00:41:17.680]   well, I'm. And even then, even then, so you have the Google employees who, when they found out that
[00:41:17.680 --> 00:41:22.080]   this tapping had been done, to internal and they, both of them said, FU to Google.
[00:41:23.280 --> 00:41:29.280]   If one of those employees had been ordered to do something, all right, what does that employee do?
[00:41:29.280 --> 00:41:35.120]   Now, at some level, they try to refuse. They try to tell their bosses, we can't do this.
[00:41:35.120 --> 00:41:40.480]   Yes, if they go and try to whistle blow, they're violating the security letters,
[00:41:40.480 --> 00:41:46.000]   see why earlier efforts, larger efforts in official sanctions or official oversight do not work.
[00:41:46.000 --> 00:41:52.960]   But we as a, as a country, as a society have to end up depending upon the conscience
[00:41:52.960 --> 00:41:57.920]   of a few geeks. The NSA is the largest employer in America of mathematicians.
[00:41:57.920 --> 00:42:05.760]   Do mathematicians have a Hippocratic oath? Do they, do they, do they use their power for good or ill?
[00:42:05.760 --> 00:42:11.520]   Do they even watch Spider-Man, you know, you got at some point, you got to say, in technologies,
[00:42:11.520 --> 00:42:17.360]   technologists have power at their hands. And I harken back and I did it, maybe it's overkill
[00:42:17.360 --> 00:42:20.960]   because it's this, we're not talking about the A-bomb here, but I harken back to the letter,
[00:42:20.960 --> 00:42:26.800]   the slissard letter signed by, I think, 153 scientists out of the Manhattan Project,
[00:42:26.800 --> 00:42:33.120]   begging Harry Truman not to use the atomic bomb over Japan after Germany had surrendered.
[00:42:33.120 --> 00:42:37.280]   And the bosses held that letter up until the bomb was ready to go.
[00:42:37.280 --> 00:42:43.280]   It's not having to be made up of this. So, because debate equals doing nothing, right?
[00:42:43.280 --> 00:42:50.640]   In that case, I think the perception was if we, if we, if we,
[00:42:50.640 --> 00:42:54.240]   if we start to question it, we just got to go. Nobody's going to, this is never going to happen
[00:42:54.240 --> 00:42:57.520]   if you start to think about it. Let's just do it. That was what it was.
[00:42:57.520 --> 00:43:03.680]   You know, there, okay, this is where, I hate to bring a TV show back into the game,
[00:43:03.680 --> 00:43:08.880]   but this is where the, this is where the most current episode of Homeland is kind of germane
[00:43:08.880 --> 00:43:14.080]   because there is a battle within the CIA between two different points of view, the old school
[00:43:14.080 --> 00:43:20.320]   point of view, which is you plant, you plant, you, you subvert foreign intelligence officers,
[00:43:20.320 --> 00:43:27.600]   you plant people in, in, in, you know, in country, you do careful intelligence work to gather information,
[00:43:27.600 --> 00:43:30.320]   or you just send a drone over and you blow the, blow them all the hell up.
[00:43:30.320 --> 00:43:35.840]   And it's a lot cheaper and easier and might even be, appear to be more effective, at least
[00:43:35.840 --> 00:43:39.920]   initially, just to blow them all up. And in fact, it looks like what we're doing, you know,
[00:43:39.920 --> 00:43:45.840]   with our intelligence efforts failed, we couldn't coordinate our various intelligence agencies.
[00:43:45.840 --> 00:43:49.840]   The information that they knew about 9/11 and every other attack on the US soil
[00:43:49.840 --> 00:43:54.960]   was there, but just never coordinated enough to do anything about it. At least those anyway.
[00:43:54.960 --> 00:43:59.600]   So there's blown up sentiment, sent a drone in no human, no American lives are at risk.
[00:43:59.600 --> 00:44:06.880]   And it's a blunt weapon, but it works. It seems like that's what's going on right now, right?
[00:44:07.520 --> 00:44:11.840]   Robot war. And that's what that too. I mean, that's pretty reprehensible.
[00:44:11.840 --> 00:44:19.840]   So I'd rather they, if it means going and looking at their porn habits and, and blackmailing them
[00:44:19.840 --> 00:44:25.040]   versus sending a drone over and killing a bunch of innocent civilians along with your target.
[00:44:25.040 --> 00:44:29.280]   If they're suspected terrorists, Leo, I don't disagree. The issue here is they didn't stop
[00:44:29.280 --> 00:44:32.960]   at collecting data on suspected terrorists. They collected it on all of us.
[00:44:32.960 --> 00:44:35.920]   Well, not in this case. Not in this case.
[00:44:36.400 --> 00:44:45.200]   I they got our porn. They got they got all over. They've recreated our not that many of us here have
[00:44:45.200 --> 00:44:51.840]   porn. They've got a right. I actually, whoa, what are you saying here? What do you say?
[00:44:51.840 --> 00:44:55.760]   No, no, I just I just funny because you're the guy that's always like, Hey, you know what?
[00:44:55.760 --> 00:44:59.680]   Like everybody does all these things. Like we should we should lift a taboo and that's the
[00:44:59.680 --> 00:45:04.320]   problem. Like the problem is that it's a big deal for anybody to have porn. Anyway, I said in
[00:45:04.320 --> 00:45:07.920]   public parts that, okay, you know, yes, I'm looking for an, you know, what's the big deal.
[00:45:07.920 --> 00:45:11.040]   And some interview once I forget where it was, some four country made a big deal and say,
[00:45:11.040 --> 00:45:15.760]   well, he even confessed to looking at porn. Like you don't. It's like it's such a
[00:45:15.760 --> 00:45:22.880]   revelatory confession. Anyway, okay. And I say, okay, this is this is the Snowden identify six
[00:45:22.880 --> 00:45:32.640]   targets, all Muslims. Presumably, I don't know, but presumably foreign nationals who were radicalized
[00:45:32.640 --> 00:45:38.640]   and considered dangerous. That is Leo, if there if there is some reason to go after them, I don't
[00:45:38.640 --> 00:45:45.360]   think anyone disagrees. But the issue of blanket surveillance is that it is not selected. It is
[00:45:45.360 --> 00:45:51.120]   every they're going after jihadists. In this case, but you don't have evidence that they're going
[00:45:51.120 --> 00:45:58.800]   after you. They are collecting data on me. Yes, they have they have photos of every letter sent
[00:45:58.800 --> 00:46:04.480]   to everyone in America. They have metadata on me, my phone calls, my emails, the NSA says,
[00:46:04.480 --> 00:46:09.920]   wherever you are, the NSA's database is store information about your political views, your medical
[00:46:09.920 --> 00:46:14.720]   history, your internet relations and your activity online, according to the deputy legal
[00:46:14.720 --> 00:46:21.360]   director of the ACLU. The NSA says this personal information won't be abused. But these documents
[00:46:21.360 --> 00:46:28.560]   show the NSA probably defines abuse very narrowly. But my question, and it's somewhat
[00:46:28.560 --> 00:46:36.160]   of a it's somewhat of a rhetorical question. But my question is, well, what do you propose?
[00:46:36.160 --> 00:46:43.120]   You can't after the fact say, well, you know, let's find out what he did last month or last year.
[00:46:43.120 --> 00:46:49.760]   You have to collect it now. It isn't it isn't it reasonable for the NSA say, look, we're just
[00:46:49.760 --> 00:46:54.560]   going to collect everything. We won't look at it until we have reason. And then we'll look at it.
[00:46:54.560 --> 00:46:59.440]   And but unless we have it now, we won't have it in six months when we need to find out.
[00:46:59.440 --> 00:47:04.800]   You have a standard that defines what we do to Americans. Let's just start. And they also say
[00:47:04.800 --> 00:47:09.360]   that they don't break the law with regard to Americans. I mean, whether you believe that or
[00:47:09.360 --> 00:47:12.720]   not, I don't know, you know, they should be held to the law. I'm not saying they shouldn't.
[00:47:12.720 --> 00:47:15.840]   But here's here's the here's the it sounds like people are offended when they do things that are
[00:47:15.840 --> 00:47:21.680]   lawful as well. At this point, yes, because it goes over over the bounds of what we think. And
[00:47:21.680 --> 00:47:28.160]   that's what that's the argument we're having. Start here. Wire tapping phones was done
[00:47:28.160 --> 00:47:34.960]   really nearly until the public said, no, you want to, well, that's creepy. You can't do that.
[00:47:34.960 --> 00:47:39.280]   And then warrants were required for wiretaps. In other words, you had to be selective in what
[00:47:39.280 --> 00:47:45.680]   you went after. That's the principle of work here is that there must be some measure of selectivity
[00:47:45.680 --> 00:47:50.720]   in who you're going after. And if you choose, if you could move to general Alexander's movement
[00:47:50.720 --> 00:47:53.840]   of collect at all, then you were going to collect information on all of us. Now,
[00:47:53.840 --> 00:47:58.720]   your argument against me could be I go with I learned the lesson from Dana Boyd that we should
[00:47:58.720 --> 00:48:03.120]   we should worry about the use of data over the collection of data that limiting knowledge is a
[00:48:03.120 --> 00:48:06.800]   dangerous thing. So you could use that back on me and say Jarvis, they're just collecting
[00:48:06.800 --> 00:48:13.200]   information. Then the question becomes, do you really trust them to do that? Do you trust the
[00:48:13.200 --> 00:48:18.320]   government to just collect all of his information on all of us for the rare case that's the fundamental
[00:48:18.320 --> 00:48:23.520]   question? And I but Stephen Baker, who is one time general counsel for the NSA and the top
[00:48:23.520 --> 00:48:27.440]   homeland security official named Bush administration, Huff post quotes him saying,
[00:48:27.440 --> 00:48:32.240]   the idea of using potentially embarrassing information to undermine undermine targets is a
[00:48:32.240 --> 00:48:36.400]   sound one quote. If people are engaged in trying to recruit folks to kill Americans and we can
[00:48:36.400 --> 00:48:41.680]   discredit them, we ought to on the whole, it's fairer and maybe more humane than bombing a target,
[00:48:41.680 --> 00:48:47.040]   describing the tactic is dropping the truth on them. He says, any system can be abused,
[00:48:47.040 --> 00:48:52.400]   but fears of the policy drifting to, for instance, domestic political opponents don't justify
[00:48:52.400 --> 00:48:57.600]   rejecting it. He says, quote, on that ground, you could question almost any tactic we use in a war.
[00:48:57.600 --> 00:49:02.960]   And at some point you have to say we're counting on our officials to know the difference. Now,
[00:49:02.960 --> 00:49:07.520]   you're saying we don't trust them. So I'm saying, well, what's the alternative? Who's going to defend
[00:49:07.520 --> 00:49:14.800]   you? Who do you trust to defend you? Transparency and oversight is the is the fix is to say that
[00:49:14.800 --> 00:49:18.720]   if they are reasonable people going after reasonable ends, but they're not going to tell you because
[00:49:18.720 --> 00:49:23.440]   they can't tell you Jeff Jarvis, because that would make it public. This is an interesting thing.
[00:49:23.440 --> 00:49:28.560]   And that would disarm them. So they need to tell somebody in secret, but to an extent,
[00:49:28.560 --> 00:49:34.640]   we're not things that are very interesting things that Kerry said to me, was that he actually
[00:49:34.640 --> 00:49:39.040]   argues that more intelligence data should be crowd sourced. There are comes to knowing,
[00:49:39.040 --> 00:49:43.760]   for example, what's going on in Syria, you get a lot more out of Brown Moses. The guy who sits in his
[00:49:43.760 --> 00:49:47.520]   room in London and looks at videos and looks at things and understands where the weapons are,
[00:49:47.520 --> 00:49:52.960]   then you get out of the spokes doing it. By the way, the guy you just quoted Stuart Baker,
[00:49:52.960 --> 00:49:58.560]   he debated a Russ Berger, and Geneen Gibson of The Guardian a few weeks ago. And when I asked a
[00:49:58.560 --> 00:50:02.320]   question, I said I was Jeff Jarvis, and he said, Oh, I know you. I watched twig. What?
[00:50:02.320 --> 00:50:08.240]   Hi, Stuart. Hi, Stuart. Hi, Stuart. Stuart would be happy to be on the show. He's saying the same thing
[00:50:08.240 --> 00:50:13.040]   I am. And I think that we have, I asked Stuart if he would be on the show and he said, yes, good.
[00:50:13.680 --> 00:50:16.800]   I mean, I'm trying to make the same points he is, which is,
[00:50:16.800 --> 00:50:21.920]   okay, so, okay, so clearly we can't make it public because it won't, it'll
[00:50:21.920 --> 00:50:28.880]   mean that the tool is no longer useful. So we need to reveal it to your trusted proxy, Jeff Jarvis.
[00:50:28.880 --> 00:50:35.600]   Who is that? It should be. You remember Congress, right? Yeah, but Congress
[00:50:35.600 --> 00:50:41.120]   doesn't know much what's going on. And when Warren, Warren, Wyden did know what's going on,
[00:50:41.120 --> 00:50:44.400]   he had to hit to his freers. He wasn't allowed to say anything. He wasn't to have any means where
[00:50:44.400 --> 00:50:49.920]   the oversight was effective. We're in the camp because if he tells us what's going on, it will make
[00:50:49.920 --> 00:50:56.160]   it unusable. I do recognize the need for some secrecy to make that porn stuff you're arguing
[00:50:56.160 --> 00:51:01.440]   about unusable. What he's trying to make unusable is the is the broad blanket collection. He asked
[00:51:01.440 --> 00:51:06.640]   straightforward intelligence officials before Congress whether they were collecting data on
[00:51:06.640 --> 00:51:09.920]   millions of the bus because he knew they were and they lied and they lied. I understand.
[00:51:09.920 --> 00:51:14.480]   They lied. But they lied. Understand why? Understand why General Clapper lied.
[00:51:14.480 --> 00:51:22.160]   He lied. He believed whether this is true or not, he lied in our interest, in our best interest.
[00:51:22.160 --> 00:51:29.040]   He felt it was better. The Politburo comrade? Well, I'm just saying we you got it. You got to
[00:51:29.040 --> 00:51:36.880]   give us an alternative so that we can pursue some policy to protect ourselves. So what is that
[00:51:36.880 --> 00:51:41.120]   alternative? You have a Congress you don't trust. Okay. Well, then who?
[00:51:41.120 --> 00:51:46.160]   That's exactly the problem. I'm raising it. I have courts that are gagged. I have
[00:51:46.160 --> 00:51:50.480]   Congress people who are gagged. I have an executive branch that's going too far. I'm
[00:51:50.480 --> 00:51:55.520]   journalists who are now afraid of talking to sources because their contacts are all known.
[00:51:55.520 --> 00:52:02.400]   That's why I'm saying that at the end of the day, are are. It's not unusual for intelligence
[00:52:02.400 --> 00:52:08.000]   officials to lie in public hearings. They do it all the time. They have to. It's a public hearing.
[00:52:08.000 --> 00:52:13.520]   If I lay the law to that he committed perjury. I understand. But I and he understood what he
[00:52:13.520 --> 00:52:20.480]   was doing. I'm sure. But I think he also felt like he had to protect. I mean, it was it was
[00:52:20.480 --> 00:52:24.720]   an end up futile because of Edward Snowden, but he felt, look, I'm not justifying it.
[00:52:24.720 --> 00:52:29.600]   But I'm trying to argue the other. We rarely hear that other point of view, which is, look,
[00:52:29.600 --> 00:52:34.160]   you want to be protected against terrorists. This is what we need to do to protect against
[00:52:34.160 --> 00:52:38.960]   terrorists. What can we do right there? That's that's the argument that I think Yolkei would make.
[00:52:38.960 --> 00:52:46.400]   Who's to say that we do need to do that? That's that's the challenge that needs to be made,
[00:52:46.400 --> 00:52:50.400]   is that the fact that the internet allows you to collect more and more and more means you should
[00:52:50.400 --> 00:52:54.000]   collect more and more and more because you can collect more more. You agree to the premise that
[00:52:54.000 --> 00:52:59.520]   there are people outside and inside the United States who would like to do us harm. I saw
[00:52:59.520 --> 00:53:05.280]   their handy work firsthand. Yes. What do you propose as a means to
[00:53:05.280 --> 00:53:12.800]   ideally ferret them out and keep them from doing us harm or maybe ideally would be to
[00:53:12.800 --> 00:53:18.240]   eliminate them, but let's say to ferret them out and keep us from doing harm. How do we do that?
[00:53:18.240 --> 00:53:24.960]   I think that you set some principles that we live by and that courts and Congress can check
[00:53:25.680 --> 00:53:29.600]   and that one of those principles that I think is what comes out of the NSA argument is that
[00:53:29.600 --> 00:53:35.600]   collecting everything on everyone is not acceptable, that you must have some targeted basis of your
[00:53:35.600 --> 00:53:44.000]   collection. The problem is that the problem is that I think that I would guess and we'll maybe
[00:53:44.000 --> 00:53:49.360]   get steward on it and you can explain this. That doesn't work. And because we have the means to do
[00:53:49.360 --> 00:53:55.360]   total information on awareness, we need to pursue that with what they believe I'm sure is proper
[00:53:55.360 --> 00:53:58.560]   safeguards. We're not we're going to put that we're going to seal it. It's sealed. It's vaulted
[00:53:58.560 --> 00:54:04.000]   until we have a person of interest. And now thank goodness we've been collecting all this
[00:54:04.000 --> 00:54:08.080]   information. We can go through it in regards in light of that person of interest, collect the
[00:54:08.080 --> 00:54:14.640]   information we need to take that person out. And it feels like the opposite of innocent
[00:54:14.640 --> 00:54:18.480]   and so proven guilty. You know what I mean? I understand that, but I think it's also
[00:54:18.480 --> 00:54:22.400]   practical and pragmatic. Let me bring it to technical terms, both of you.
[00:54:23.040 --> 00:54:28.640]   Do you think it was justified for the NSA to tap into the internal server connections between
[00:54:28.640 --> 00:54:34.080]   among Google servers? Well, see, there's two answers to that. One, it's reprehensible and I hate it.
[00:54:34.080 --> 00:54:39.840]   Two, I think we gave them permission. In fact, we gave them a mandate to do that when we said,
[00:54:39.840 --> 00:54:42.880]   hey, protect us. Jana?
[00:54:42.880 --> 00:54:45.600]   That's crazy.
[00:54:49.200 --> 00:54:53.680]   I do not give anybody permission to collect everything that I do online.
[00:54:53.680 --> 00:54:56.320]   No, you gave you but you when you say protect me.
[00:54:56.320 --> 00:55:03.200]   Okay, so you didn't say protect me with any means necessary. You said protect me,
[00:55:03.200 --> 00:55:06.560]   but I don't want you to do anything that I think is bad. And what if they come back to you say,
[00:55:06.560 --> 00:55:09.920]   but wait a minute, we can't protect you unless we do these particular things.
[00:55:09.920 --> 00:55:14.720]   No, I'm saying I'm an American citizen citizen, protect me. And I'm assuming that that means
[00:55:14.720 --> 00:55:18.800]   within the system that we've set up, right? And within the system that we set up, you have to have
[00:55:18.800 --> 00:55:22.320]   just cause to start somebody's stuff. That's, I mean,
[00:55:22.320 --> 00:55:24.960]   this is all legal. Yeah.
[00:55:24.960 --> 00:55:30.000]   Right. It's all legally in secret, right? Because everyone's
[00:55:30.000 --> 00:55:35.760]   it's legal. I mean, you know, we should have voted out the guy, the sons of bitches that
[00:55:35.760 --> 00:55:39.680]   passed the Patriot Act, maybe. Well, even the author of the Patriot Act says he wants to change
[00:55:39.680 --> 00:55:43.760]   it now. I know, but it's the law. It's the general question for you. I want to come back to this
[00:55:43.760 --> 00:55:49.200]   notion of the conscience of the individual. So you're both working for level three.
[00:55:49.200 --> 00:55:58.400]   And you're given the little box that the good pastor on the Twit Network
[00:55:58.400 --> 00:56:04.240]   showed us. You said, hook this up into these cables. What cables are those? They're the ones in
[00:56:04.240 --> 00:56:09.440]   Google. Those fiber optic ones over there. What do you do? You do it because it's the it's a
[00:56:11.600 --> 00:56:18.080]   legally, it's a representative of it's a people. It's a representative of the people. What do you
[00:56:18.080 --> 00:56:23.760]   do? I mean, you know, I like to think that I would say, no, I'm not doing this. What if what
[00:56:23.760 --> 00:56:29.520]   if you found out what if they were the thought that by saying no, that you would actually enable
[00:56:29.520 --> 00:56:36.080]   an attack on the that's that's what Yokai argues is bogus. Well, we don't know though. We don't
[00:56:36.080 --> 00:56:41.360]   know that we never can. So what you're justifying is the police state because there's no
[00:56:41.360 --> 00:56:46.080]   break. There's no end. They will collect everything on everyone in case anything could be valuable.
[00:56:46.080 --> 00:56:50.560]   Right. And then what they do, and then what they do is they say, we're going to collect everything,
[00:56:50.560 --> 00:56:54.800]   but we are going to follow strict rules. And they believe they are doing this, by the way,
[00:56:54.800 --> 00:56:59.520]   strict rules about what we do with that information. We're going to we promise we're going to protect
[00:56:59.520 --> 00:57:04.640]   US citizens. But when it comes down to we've got a person of interest, we need to have that prior
[00:57:04.640 --> 00:57:09.920]   data. We need to because otherwise, what law enforcement is very concerned about is they're
[00:57:09.920 --> 00:57:15.760]   peering into a that there is a dark net that they cannot see and encryption. They cannot see into
[00:57:15.760 --> 00:57:20.400]   what is going on. And frankly, I got to think that what Google is doing by encrypting,
[00:57:20.400 --> 00:57:25.360]   it's it's a level three traffic and Yahoo's now doing it. And everybody else is going to start
[00:57:25.360 --> 00:57:29.440]   doing that is going to scare law enforcement to say, Oh my God, now we're really screwed.
[00:57:29.440 --> 00:57:34.240]   We can't find any of this information. Then what do we do? What do they do before the telephone?
[00:57:34.240 --> 00:57:39.440]   Well, I would argue that just as the internet has given them unprecedented ability to collect
[00:57:39.440 --> 00:57:43.200]   this information. It's also given the bad guys technology, given the bad guys unprecedented
[00:57:43.200 --> 00:57:49.200]   information ability to attack us on our shores. So it was a it was a different you can't compare
[00:57:49.200 --> 00:57:57.360]   this to the days of muskets. We have it's a very different world. And I feel like I feel like
[00:57:57.360 --> 00:58:07.120]   these people are acting in their mind honorably. And I feel like we it the onus is a little bit
[00:58:07.120 --> 00:58:14.000]   on them, but it's also a little bit on us. Because I do believe we are asking them to protect us.
[00:58:14.000 --> 00:58:21.360]   And that we are concerned that there's a threat. And I think there is some onus on us to to
[00:58:21.360 --> 00:58:26.720]   propose a system that works instead of just going, No, you can't collect information.
[00:58:26.720 --> 00:58:33.120]   Because that may not be workable. If it turned out, I mean, let's this is completely hypothetical,
[00:58:33.120 --> 00:58:38.480]   but turned out that there was, you know, if we don't collect this information,
[00:58:38.480 --> 00:58:41.280]   there'll be another 911. Which would you choose?
[00:58:41.280 --> 00:58:51.680]   That's a reductionist, but it's hypothetical. I'm saying it's hypothetical.
[00:58:51.680 --> 00:58:54.560]   Yeah, I mean, there's a huge set of assumptions there.
[00:58:54.560 --> 00:58:58.160]   It's hypothetical that all of his collection has stopped.
[00:58:59.040 --> 00:59:03.440]   Yeah, the arguments that I've made that argument to, I mean, I understand that. But
[00:59:03.440 --> 00:59:08.080]   I'm trying to make the argument on the other side, just so that it's a little more balanced. I mean,
[00:59:08.080 --> 00:59:19.120]   what if it what if it what if it does stop this stuff? Would you be willing to allow well supervised
[00:59:19.120 --> 00:59:23.280]   gathering of information? Ah, but you won't get back to supervised. The problem is, that's my
[00:59:23.280 --> 00:59:26.640]   point of question. That's what you're asking for. No supervision. Okay, so let's get the
[00:59:26.640 --> 00:59:31.200]   middle of my time working and then it's okay from abuse. If we have the oversight, is it okay then?
[00:59:31.200 --> 00:59:38.000]   That super that oversight, well, it is they ain't oversight. If the oversight can't say no.
[00:59:38.000 --> 00:59:42.080]   Okay, but let's say let's say we have a perfect system and we have oversight, then is it okay?
[00:59:42.080 --> 00:59:48.080]   If the oversight is able to say what goes too far and we trust those people to say what's
[00:59:48.080 --> 00:59:52.240]   too far, yes, that's the point of oversight. How about you, Gina? Is it okay for them to spy on
[00:59:52.240 --> 00:59:58.160]   you if there's oversight? The my point is they're not going to spy on you if the oversight is
[00:59:58.160 --> 01:00:02.800]   proper. Why not? Because you're assuming that there's no that I'm not a person of interest,
[01:00:02.800 --> 01:00:08.880]   right? For whatever reason or that I am a person like, am I okay with it if they have a good reason?
[01:00:08.880 --> 01:00:13.200]   Yes. Am I okay with you know, my K with it. They don't have a good reason? No. That's that's what
[01:00:13.200 --> 01:00:17.280]   we're saying. Yeah, exactly. And you know, I understand also and somebody saying, well, that
[01:00:17.280 --> 01:00:21.760]   just justifies torture. No, I mean, the thing is you have to think broadly torture doesn't work
[01:00:21.760 --> 01:00:25.280]   because it causes more problems and it solves and the information gets not useful.
[01:00:25.280 --> 01:00:29.280]   Drones don't work because it radicalizes people and creates more terrorism. There are things that
[01:00:29.280 --> 01:00:33.440]   you can't do because they don't work. But what if there's something that works like finding out
[01:00:33.440 --> 01:00:38.720]   some radicals into porn and using it to discredit him? What if that works? Is that so bad?
[01:00:38.720 --> 01:00:42.880]   It's just a good story.
[01:00:46.480 --> 01:00:53.840]   Porn is the cat. It's my story. It's hard. This is hard stuff. I think it's very hard stuff.
[01:00:53.840 --> 01:00:58.160]   It is. It is. It is. It's fascinating. I don't think I don't know if the chatroom is if you're not.
[01:00:58.160 --> 01:01:02.720]   Are they? No, no, I think this is something we all have to as US citizens and citizens of the
[01:01:02.720 --> 01:01:06.560]   world, we all have to have this conversation. It's a hard conversation. And I think also the
[01:01:06.560 --> 01:01:11.040]   issue there's an economic issue here is can anyone trust America and American companies anymore?
[01:01:11.040 --> 01:01:15.840]   And can they trust our governance of the internet, which let's be honest, we really have.
[01:01:16.560 --> 01:01:25.520]   That is going to face us square on real fast. And I think that cloud companies are going to lose
[01:01:25.520 --> 01:01:34.720]   business. Cloud technology is going to lose trust. America is going to lose trust. And America is
[01:01:34.720 --> 01:01:39.920]   going to lose power over the internet to some people who really are bad guys. And that's because
[01:01:40.960 --> 01:01:46.480]   we're overboard. There's abuse here and they've used sauna oversight. That's the issue.
[01:01:46.480 --> 01:01:54.640]   And now what's new with Android? To be clear, any culture where someone looking at porn is such
[01:01:54.640 --> 01:02:00.800]   a shaming thing that I'm totally in favor of shaming anybody about porn. I use that example of
[01:02:00.800 --> 01:02:08.000]   homosexuality because that is the best argument for making same sex marriage legal. Then you
[01:02:08.000 --> 01:02:14.800]   can't be blackmailed for it. That was the primary argument I made in public parts is that
[01:02:14.800 --> 01:02:20.480]   publicness disarms stigma. Publicness makes these connections, transparency does these things.
[01:02:20.480 --> 01:02:25.200]   And that's why you're right. Obviously, you can't make all spying transparent, but that's what I
[01:02:25.200 --> 01:02:31.440]   was amazed when Bob Kerry argued that a lot more intelligence should be open and transparent and
[01:02:31.440 --> 01:02:36.320]   crowd sourced because it's transparent anyway. And they just try to make it secret then.
[01:02:36.880 --> 01:02:43.120]   I talked to Wolfowitz once at a reception in Washington. Some people will now growl at me for
[01:02:43.120 --> 01:02:47.600]   even talking to him. And it was about blogs in the early days of blogs and the Iraqi blogs.
[01:02:47.600 --> 01:02:50.480]   And he said that he yelled at us and told us people and said, I get up in the morning and I read
[01:02:50.480 --> 01:02:54.240]   these Iraqi blogs. I know more about what's going on in the country than you're telling me.
[01:02:54.240 --> 01:02:54.480]   Right.
[01:02:54.480 --> 01:03:03.200]   I think that unfortunately, here's what I would law before, something a little simpler.
[01:03:04.560 --> 01:03:08.720]   Unfortunately, these conversations have become so polarized and so tied to people's beliefs,
[01:03:08.720 --> 01:03:16.880]   political beliefs, that we don't hear the other person. And it really is just kind of futile
[01:03:16.880 --> 01:03:21.760]   beating our heads against each other. And if we really are going to solve these questions,
[01:03:21.760 --> 01:03:26.080]   very tough questions, we need to listen, we need to hear. And I don't think we should assume that
[01:03:26.080 --> 01:03:30.000]   people are acting from bad intentions. I think, in fact,
[01:03:30.000 --> 01:03:34.560]   no, I think the spies are doing what they, I don't blame the spies actually. I mean,
[01:03:34.560 --> 01:03:38.720]   I do blame at some level. I don't blame the spies who were spies. They're doing,
[01:03:38.720 --> 01:03:42.800]   they're going to go to the limits of what they can do. I blame those who we expect to set the limits.
[01:03:42.800 --> 01:03:48.880]   That's the issue. But then I am asking the next question, which I think is an important one,
[01:03:48.880 --> 01:03:53.360]   especially for folks like us who follow technology, is that at the end of the day,
[01:03:53.920 --> 01:04:03.040]   a technologist doing one codd's turn in this machine has an ethical responsibility. And I
[01:04:03.040 --> 01:04:08.160]   mean, rise up before doctors have a Hippocratic oath. Journalists, even though we don't always
[01:04:08.160 --> 01:04:14.720]   follow it, we think we have ethical oaths and statements. Other fields, academics believe that
[01:04:14.720 --> 01:04:19.840]   they have statements about about their ethics as a trade, as a craft, as a profession.
[01:04:20.560 --> 01:04:27.600]   Do technologies, do programmers, do system administrators, do mathematicians, talk about
[01:04:27.600 --> 01:04:32.880]   ethics. After the financial crash, we got all upset that business schools weren't teaching
[01:04:32.880 --> 01:04:36.880]   ethics to business people who have power. Well, technologists have more power than business people.
[01:04:36.880 --> 01:04:43.680]   Do technologists have these discussions about ethics? Are there professional organizations
[01:04:43.680 --> 01:04:49.120]   that are trying to examine these ethics? Are there these kinds of discussions among the technologists
[01:04:49.120 --> 01:04:57.360]   who are the ones who turn these switches? I gave an interview for a, since, unfortunately,
[01:04:57.360 --> 01:05:05.280]   submarine movie about hackers in I think it's 2003, in which I said the hackers will be the
[01:05:05.280 --> 01:05:11.120]   freedom fighters in the years to come, that it won't be weapons that we will need to maintain
[01:05:11.120 --> 01:05:14.400]   our knowledge and our use of technology so that we can fight for freedom.
[01:05:15.120 --> 01:05:21.280]   So there's the problem is that then that's an anarchistic statement because rather than
[01:05:21.280 --> 01:05:27.920]   trusting the institutions, rather than trusting the employed legitimate technologists, what we're
[01:05:27.920 --> 01:05:32.480]   saying is, and this is not even I've had with Eugene, is that the hacking culture is we'll just
[01:05:32.480 --> 01:05:38.240]   go around it and that's okay. Where what I'm looking for is the Talmud of techies.
[01:05:38.240 --> 01:05:44.880]   Right, right. Well, look, there's, I mean, I think about ethics a lot. I try to think about it. I
[01:05:44.880 --> 01:05:50.800]   talk about it a lot. I think that some coders do and some coders don't. It depends. But you know,
[01:05:50.800 --> 01:05:54.320]   there's the person turning the screw and there's the person telling them to turn the screw and
[01:05:54.320 --> 01:05:59.440]   then there's the boss telling the manager. I mean, that the responsibility lays certainly on the
[01:05:59.440 --> 01:06:04.240]   shoulders of the person who's crimping the ethernet cord and plugging it in and typing the code,
[01:06:04.240 --> 01:06:09.520]   but also the their bosses and their bosses. So I don't think that you can rest it squarely on
[01:06:09.520 --> 01:06:13.840]   the shoulders of the technologists. Like if the actual technologists like had stopped and said,
[01:06:13.840 --> 01:06:18.400]   I'm not doing this, I'm walking out because there are cogs, most likely they would just be replaced
[01:06:18.400 --> 01:06:22.080]   or they would lose their job. I mean, Snowden sacrificed his entire life.
[01:06:22.080 --> 01:06:27.600]   Yeah, well, he'd say what he said. I mean, he sacrificed his entire, it wasn't just his job.
[01:06:27.600 --> 01:06:33.040]   You know, he's a Chelsea Manning as well.
[01:06:33.040 --> 01:06:33.760]   Chelsea Manning, yeah.
[01:06:33.760 --> 01:06:36.400]   I'm not trying to say that all the response, it's a good point, Gina. And I want to make
[01:06:36.400 --> 01:06:39.600]   sure I'm clear. I'm not saying that all the responsibility lies in the technologists.
[01:06:39.600 --> 01:06:45.920]   What I'm saying is that is the oversight of last resort at last resort, one honest
[01:06:45.920 --> 01:06:51.840]   person of conscience. What's the brakes on and says, should we do this? Is this evil? Yeah, good.
[01:06:51.840 --> 01:06:56.560]   I mean, I mean, the unfortunate truth is a lot of nerds and I include myself in this a little bit
[01:06:56.560 --> 01:07:01.760]   are a political, right? You know, men who have just sort of decided that the system doesn't make
[01:07:01.760 --> 01:07:05.840]   sense and it deals with messy humans and their emotions and their relationships and therefore
[01:07:05.840 --> 01:07:09.680]   is illogical and therefore I just will not participate in it, right? You know, like,
[01:07:09.680 --> 01:07:14.000]   you have, I think, a lot of that in the tech community not to generalize.
[01:07:14.000 --> 01:07:21.440]   So there's a weakness there. And I think that with the Gov 2.0 movement and some of the more
[01:07:21.440 --> 01:07:26.080]   recent awareness around what's been going on, particularly with Snowden, I think Snowden is
[01:07:26.080 --> 01:07:31.600]   an incredible role model, I think. I think Yeeks are becoming more aware and are thinking more
[01:07:31.600 --> 01:07:35.440]   about their responsibilities. But I think we could go further. I think that I would love to see
[01:07:35.840 --> 01:07:40.400]   why I would love to even have a hand in drafting the Hippocratic oath. I don't know
[01:07:40.400 --> 01:07:43.040]   of any organizations off the top. I'll look into it more.
[01:07:43.040 --> 01:07:46.080]   Dude, dude, I would love to. I would love to write about that and see.
[01:07:46.080 --> 01:07:52.160]   But what ethics do you teach, though? I mean,
[01:07:52.160 --> 01:07:56.880]   journalism school? I mean, no, but I mean, just saying, what do you teach?
[01:07:56.880 --> 01:08:02.480]   I don't even know if there's a clear right or wrong. Well, I'll give you an example of
[01:08:02.480 --> 01:08:04.880]   journalism, but Lord knows it's not an evil. What does don't be evil mean?
[01:08:05.680 --> 01:08:12.560]   Well, in journalism, it's that the reader, the whole argument now about native advertising,
[01:08:12.560 --> 01:08:19.680]   about all this stuff that you see on BuzzFeed and Forbes, where it's the Scientology thing on the
[01:08:19.680 --> 01:08:25.200]   Atlantic, there's a lot of that going around. And last term, my students went to see one of the
[01:08:25.200 --> 01:08:30.000]   practitioners of the stuff and they came back in class. And I realized I would teach an ethics way
[01:08:30.000 --> 01:08:34.000]   too late in the term because they were all kind of struggling. Oh, yeah, it's funny. You need money.
[01:08:34.000 --> 01:08:39.360]   I said, wow, by the way, it was the year it was the week that I was being observed as to whether
[01:08:39.360 --> 01:08:48.400]   around I should become a full professor. And I got it anyway. But the issue is in my field,
[01:08:48.400 --> 01:08:51.440]   for example, right, when it comes to native advertising, the reader must never be confused
[01:08:51.440 --> 01:08:56.880]   about the source of content of information, right? That if there is a vested interest at heart,
[01:08:58.080 --> 01:09:05.520]   then it should be revealed. That's as David Weinberger has said, transparency is the new
[01:09:05.520 --> 01:09:10.000]   objectivity. Right? So that's that's an ethical statement. I think that's a little simpler and
[01:09:10.000 --> 01:09:15.680]   easier problem to solve. I agree. Then the problem of how do we how do we protect ourselves and still
[01:09:15.680 --> 01:09:20.560]   preserve our values? I mean, that's really the fundamental question. How do we protect ourselves
[01:09:20.560 --> 01:09:25.840]   and still preserve the values of our Republic? Yes, our rights. Yes. That's exactly the question.
[01:09:25.840 --> 01:09:31.680]   And what role does an individual have in doing that? You can say, I know what the values of America
[01:09:31.680 --> 01:09:38.880]   are. I mean, that's I think that's well known. But then how do you protect yourself?
[01:09:38.880 --> 01:09:43.920]   And I think it's very difficult. And as you were saying earlier, Leo, anybody can convince
[01:09:43.920 --> 01:09:48.320]   themselves that the thing that they're doing is the right thing. Yeah. And also the other way,
[01:09:48.320 --> 01:09:53.360]   you know, what Bob Kerry said was he said, he couldn't buy the argument that Yochai was making
[01:09:53.360 --> 01:09:58.800]   and I was making about this. And because what he said is that if you have a group working to get
[01:09:58.800 --> 01:10:02.400]   against the threat, you know, kind of wasn't fully buying the threat, but the threat,
[01:10:02.400 --> 01:10:05.680]   one member of that team can't suddenly just say, well, I don't agree with this and I'm going to
[01:10:05.680 --> 01:10:11.680]   reveal everything. And I get his point. Now, Yochai was arguing interestingly for amnesty for Snowden
[01:10:11.680 --> 01:10:16.640]   and that there was a principle to be written there that said that if what was revealed,
[01:10:17.440 --> 01:10:23.760]   caused to change in society, whether through law or through courts and caused a reversal of
[01:10:23.760 --> 01:10:29.040]   something because it was revealed, then that should be seen as of societal value and that you should
[01:10:29.040 --> 01:10:34.640]   have amnesty for your revelation. Kerry wasn't buying that at all because he was saying he was
[01:10:34.640 --> 01:10:38.480]   saying at a practical level, how do you how do you run an intelligence operation? Probably you're
[01:10:38.480 --> 01:10:41.840]   saying, Leo, with the thought that anyone in my team here could turn around and say, yeah,
[01:10:41.840 --> 01:10:48.640]   you're all evil. I mean, that's not how the military works, right? It's chain of command.
[01:10:48.640 --> 01:10:53.920]   You trust your superiors, you do what you see because you can't have a soldier making ethical
[01:10:53.920 --> 01:10:57.520]   decisions on the field of battle. Right, is that a place for independent thinkers?
[01:10:57.520 --> 01:11:02.880]   Let's let's just be you can't doesn't my point is it doesn't work. Well, but you got to work.
[01:11:02.880 --> 01:11:07.600]   There are still lines when Lieutenant Kelly kills the village, right? I agree. There's lines.
[01:11:07.600 --> 01:11:13.040]   Yep. There's a line. There's a line. Now, let's hope the line doesn't have to be so obvious as that.
[01:11:13.040 --> 01:11:20.720]   He was violating by the way. Yes. Hit the rules of his superiors. I mean, he was acting
[01:11:20.720 --> 01:11:28.640]   illegally. So that's not a question. In fact, he well, anyway, let's not you know what? This
[01:11:28.640 --> 01:11:34.560]   is a great conversation. Let's end it here. Because we have a long show and there's more to do.
[01:11:34.560 --> 01:11:38.480]   This was a good conversation. I think it's a great, a great conversation to have.
[01:11:38.480 --> 01:11:41.520]   This is we have a headline like this in a while. I don't know if you're trying to.
[01:11:41.520 --> 01:11:46.080]   You know, you're highly uncomfortable that it's a good conversation. Yeah. And you're right, Leo.
[01:11:46.080 --> 01:11:50.320]   You know, playing devil's advocate and and explaining, you know, the other point of view. I think it's
[01:11:50.320 --> 01:11:55.200]   important for these kinds of debates. Actually, earlier, Jeff, you were talking about crowdsourcing
[01:11:55.200 --> 01:12:02.720]   intelligence kind of related. This is in our rundown. Eric Schmidt said at a at a in a speech
[01:12:02.720 --> 01:12:08.320]   recently that we should drop smartphones on Iran and Iraq. Oh, that'll that'll solve the problem.
[01:12:08.320 --> 01:12:14.640]   That'll do it. I think the people sort of surveil themselves. I think he's yeah, right.
[01:12:14.640 --> 01:12:21.520]   I would hope what he's saying is access to the internet and information changes hearts and minds
[01:12:21.520 --> 01:12:28.240]   or or thoughts. Just penetrate every every country. And it just happens to be good for business too.
[01:12:30.320 --> 01:12:33.840]   I think the dots just occupies their time so much that they don't have any time to build up.
[01:12:33.840 --> 01:12:41.840]   Give them angry birds. Then we'll see. I want to do the Google change log. Let's play the trumpets
[01:12:41.840 --> 01:12:48.640]   and bring in Gina, Japan. Time to see what's new at Google. The Google change log.
[01:12:48.640 --> 01:12:54.720]   What is new? Gina has the story.
[01:12:56.800 --> 01:13:03.280]   Brand new Google Chrome extension enables voice search in your browser. So you install this
[01:13:03.280 --> 01:13:08.320]   Chrome extension. It's by Google. And then you go to Google.com and you can say just like you can
[01:13:08.320 --> 01:13:15.920]   to to your phone or Google now or your Android phone. I guess iOS too. Okay, Google and do and
[01:13:15.920 --> 01:13:20.960]   do searches. So Google said that the new extension is ideal for those preparing a Thanksgiving feast
[01:13:20.960 --> 01:13:27.440]   this week here in the US anyway. This year rather than stopping midway to wash your hands to type
[01:13:27.440 --> 01:13:32.320]   in a search, you can just speak to your laptop. Okay, Google, how many ounces are in one cup.
[01:13:32.320 --> 01:13:37.760]   And voila, the cooking can go on. The extension also supports reminders. You can say, okay, Google
[01:13:37.760 --> 01:13:43.440]   set a timer for 30 minutes. And when you're once installed, you have to give Chrome permission to
[01:13:43.440 --> 01:13:49.200]   access your computer's microphone and then say, okay, Google to activate the hot word. Your microphone
[01:13:49.200 --> 01:13:54.400]   is not always listening when you have this Chrome extension installed. Or at least it yeah, it's
[01:13:54.400 --> 01:13:58.160]   only listening. It listens for okay Google, but it doesn't it doesn't record everything else
[01:13:58.160 --> 01:14:03.920]   related to our earlier conversation. So I think I want to tell them that irritating Gina,
[01:14:03.920 --> 01:14:08.560]   because I had the Google search page up where we were playing before, but then it times out.
[01:14:08.560 --> 01:14:12.400]   So I have to go to the effort then of going to the Google homepage again.
[01:14:12.400 --> 01:14:18.000]   There's no battery issue here. Why I should have the option to set my laptop all the time.
[01:14:18.000 --> 01:14:21.040]   Chrome can always do it. Yes, it's really annoying to have to go to Google.com.
[01:14:21.040 --> 01:14:26.720]   It is. It is. It is. It's a first step. Listen to me. Listen to me. They've just revolutionized
[01:14:26.720 --> 01:14:33.200]   the way we interact with computers and already I'm complaining. I have to go to Google.com.
[01:14:33.200 --> 01:14:38.800]   Then I can talk to it. Okay, Google. How tall is Yao Ming?
[01:14:38.800 --> 01:14:47.600]   It doesn't talk back, which makes me mad. Are you using your sound up? I think so.
[01:14:47.600 --> 01:14:52.160]   Does yours talk? Hold on. Let me see here. No, I don't think it talks. My phone talks.
[01:14:52.160 --> 01:15:00.800]   Okay, Google. How tall is Yao Ming? It doesn't talk back, which makes me mad.
[01:15:00.800 --> 01:15:08.400]   It talks back in my voice. It's awesome. I still had the stream on. Yeah. Do you have
[01:15:08.400 --> 01:15:14.240]   Brian, do you have my audio up? Yeah, I think it doesn't talk back. Is it an option?
[01:15:14.800 --> 01:15:20.720]   Look at those. Let me try it. Everybody's going to try it. All our volumes are turned down because
[01:15:20.720 --> 01:15:27.680]   we don't go to the street. Okay, Google. How tall is Leo Leport? It doesn't know. Oh, that's not
[01:15:27.680 --> 01:15:31.680]   going to be a card. Is it? Because it's right. It's only going to respond with a card. I'm going
[01:15:31.680 --> 01:15:36.560]   to know that. Okay, Google. How many ounces in a cup? That's a good one.
[01:15:36.560 --> 01:15:44.320]   I like that Leo just whispers. It just talks to me. It did talk to you. It did. You guys can
[01:15:44.320 --> 01:15:48.080]   hear it because obviously it was in my my headphone. But it said, yeah, one US cup is eight US.
[01:15:48.080 --> 01:15:53.920]   Oh, okay. So I don't know. Maybe my audio is not up or Google voice search hot word
[01:15:53.920 --> 01:16:00.800]   allowing incognito options. Let me see here. Okay, Google. Yes, it's a stop.
[01:16:00.800 --> 01:16:03.200]   Is Leo Leport for after five minutes.
[01:16:03.200 --> 01:16:08.800]   According to Wikipedia, the important report is an American technology broadcaster.
[01:16:08.800 --> 01:16:14.000]   Okay, good. I like it. The talking back is at least a big part of the functionality.
[01:16:14.000 --> 01:16:20.080]   Yeah. Yeah. Yeah. My phone will do. You can have your computer anywhere and get information.
[01:16:20.080 --> 01:16:24.320]   Now we are we already observed as you as you noted, one one problem, which is that I have
[01:16:24.320 --> 01:16:27.440]   multiple devices that respond. Okay, Google. It gets a little crazy.
[01:16:27.440 --> 01:16:32.960]   I think you should do. I think you should get everybody's devices together that can do this
[01:16:32.960 --> 01:16:38.320]   and do a little a stick about this. I think that can be very funny. Well done. Somebody pointed
[01:16:38.320 --> 01:16:43.840]   out that if you do command T or control T to get a new tab page, that will give you a Google search.
[01:16:43.840 --> 01:16:49.600]   So that's all you really have to do is command T. Okay. I killed that page in Chrome because I
[01:16:49.600 --> 01:16:54.240]   still want to like that page. Oh, that's no good. Oh, yes. You might need pages.
[01:16:54.240 --> 01:16:58.480]   Yes. Now it's going to be visited. Yeah. Yeah. You could change that. That's a setting.
[01:16:58.480 --> 01:17:02.800]   Yeah. Yeah. Yeah. Yeah. But I like the set a reminder for 30 minutes. That's pretty cool.
[01:17:02.800 --> 01:17:07.280]   Really nice. I do coding sprints and like our coding sprints. It was set a timer for 60 minutes
[01:17:07.280 --> 01:17:13.600]   and just code. Hey, Google. Hey, Jarvis. Okay, Jarvis.
[01:17:13.600 --> 01:17:19.040]   That was your computer that got it. That was Chrome. Yeah. Nice.
[01:17:19.040 --> 01:17:22.640]   People have told me that I was a fool to tell them around with the bat that it was Ironman,
[01:17:22.640 --> 01:17:27.360]   that I should have claimed credit. Is it Ironman? No, no. It's just Jarvis.
[01:17:27.920 --> 01:17:35.440]   Yeah. Let's try it. Okay. Jarvis. Okay. Jarvis.
[01:17:35.440 --> 01:17:39.760]   Yeah. It's been too tentative. You're not believing it. Okay. Jeff Jarvis.
[01:17:39.760 --> 01:17:48.320]   What are you doing? How? I don't want to respond. Oh, well, anyway, that's I think this is cool.
[01:17:48.320 --> 01:17:54.240]   I want to see more and I want to see more more more stuff you can say more and more of our device.
[01:17:54.240 --> 01:17:59.920]   This is clearly something that's going to happen, right? Although I don't use it that much. I mean,
[01:17:59.920 --> 01:18:05.680]   I really only use the voice stuff when I'm in the car or with the baby. I mean, I guess that makes
[01:18:05.680 --> 01:18:09.280]   sense. Would you use it more though if we're smarter and we're more interactive and conversation.
[01:18:09.280 --> 01:18:14.240]   I bet you would. It just has to work better. That's what I'm standing in the street being like,
[01:18:14.240 --> 01:18:21.680]   okay, Google. Yeah, you're right. Okay, Twit. Somebody said it works with okay, Twit.
[01:18:21.680 --> 01:18:25.520]   Okay, Twit. Sorry. It doesn't work with okay, Twit.
[01:18:25.520 --> 01:18:36.400]   Okay, keep going. You're not done. All right. I'm not done. HBO go on iOS and Android can now stream
[01:18:36.400 --> 01:18:43.040]   to the Chromecast. This is a big deal. So, so from the
[01:18:43.040 --> 01:18:47.600]   native apps, there's a little Chromecast button inside the native apps. Chromecast can now stream
[01:18:47.600 --> 01:18:54.400]   from Pandora, Hulu plus Netflix and YouTube and now HBO go. So this was long awaited. They
[01:18:54.400 --> 01:18:58.640]   announced that it would be HBO go when the Chromecast was announced several months ago. So,
[01:18:58.640 --> 01:19:01.040]   so that's that's in the latest update for HBO go now.
[01:19:01.040 --> 01:19:06.720]   Google wallets. Another story in the rundown that says Chromecast is now being sold at Walmart.
[01:19:06.720 --> 01:19:10.400]   Yeah, I can get it everywhere. I can start to think that Chromecast can go mainstream,
[01:19:10.400 --> 01:19:14.240]   which is pretty cool. I really think though it has to have more capabilities. You've really got
[01:19:14.240 --> 01:19:18.320]   to add to it. Yeah, everything has to support it. Yeah. Yeah.
[01:19:18.320 --> 01:19:27.120]   Particularly Android on our Chrome on Android, right? Like you can't cast from Chrome on Android,
[01:19:27.120 --> 01:19:31.760]   which makes you crazy. You can cast from Chrome on your. I didn't know that.
[01:19:31.760 --> 01:19:35.600]   Yeah. Yeah. Like if you want to send something from your phone to Chromecast,
[01:19:35.600 --> 01:19:39.120]   unless I'm wrong chatroom, correct me if I'm wrong, but and I just thought it was just because
[01:19:39.120 --> 01:19:45.680]   Chrome on Android doesn't support extensions yet. But anyway, Google Wallet. We talked with the
[01:19:45.680 --> 01:19:51.600]   Google Wallet card last week. Leo just got his. There's an update to the Android app that lets you
[01:19:51.600 --> 01:19:56.560]   add additional cards to your Google Wallet just by photographing them. So you can have a picture
[01:19:56.560 --> 01:20:01.600]   of your credit or debit card and the number of the expiration date will be captured automatically.
[01:20:01.600 --> 01:20:06.480]   I like that. Yeah. Yeah. I mean, they're just trying to do everything they can to make Google
[01:20:06.480 --> 01:20:12.080]   Wallet easy to use. So does it scan, huh? Yeah, right. So really, I should just go through my
[01:20:12.080 --> 01:20:16.480]   wallet and add every everything to it. Snap pictures. Yeah. And that's a that's an update to the
[01:20:16.480 --> 01:20:22.560]   Android Google Wallet app. But would it it wouldn't see the see what that CVC number on the back,
[01:20:22.560 --> 01:20:27.920]   though? I'd have to add that later somehow. Right. I should get my wallet. Keep going. I'm going
[01:20:27.920 --> 01:20:35.520]   to try it. Yeah. Yeah. There is a new refresh interface for Google's account permissions page.
[01:20:35.520 --> 01:20:40.160]   This is the page that lists all the apps that you've signed into Google and given access to your
[01:20:40.160 --> 01:20:48.000]   Google account to it's at security.google.com/settings/security/permissions. It's it's gives it the same
[01:20:48.000 --> 01:20:54.240]   information just lists it kind of kind of nicer, nicer way, big icons. It lets you know what each
[01:20:54.240 --> 01:21:02.000]   app has access to, for example, you know, hangouts or feely or Picasa third party and internal Google
[01:21:02.000 --> 01:21:07.120]   apps. And you can see also, if you click on one of the apps that you've granted access, you can see
[01:21:07.120 --> 01:21:12.320]   when when you gave access and you can revoke access as well. So kind of if you're here in the
[01:21:12.320 --> 01:21:16.480]   in the US kind of a good good holiday weekend activity, just clean up your your permissions.
[01:21:16.480 --> 01:21:22.560]   If there's old stuff in there, you don't use anymore. And finally, Google adds airports,
[01:21:22.560 --> 01:21:29.280]   train and subway stations to Street View in a Google Maps update. So this is on the lat long
[01:21:29.280 --> 01:21:37.120]   blog. Basically, this is about Thanksgiving travel here in the US. You can kind of do a Street View
[01:21:37.120 --> 01:21:44.480]   around train stations like the Waterloo station in the UK or different airports from Tokyo to
[01:21:44.480 --> 01:21:52.560]   Dubai kind of kind of cool. You could check out, I think you just search for stations and you'll be
[01:21:52.560 --> 01:21:57.680]   able to zoom in and and do the Street View. So you can see ahead what's it going to look like when
[01:21:57.680 --> 01:22:01.200]   you reach your destination. And that's all I got.
[01:22:01.200 --> 01:22:09.040]   So I don't see where I can
[01:22:09.040 --> 01:22:15.440]   add a credit card by taking a picture. So maybe I just don't have the latest for
[01:22:15.440 --> 01:22:18.000]   maybe I didn't get the maybe I didn't get the update. Let's see.
[01:22:18.000 --> 01:22:22.000]   Oh, yeah, the least an update to the Android app. Let's look at the Android app.
[01:22:22.000 --> 01:22:27.840]   How they do that. They do the rolled out updates and where would it be? I don't I see my balance
[01:22:27.840 --> 01:22:34.480]   that I put in there. So it looks like looking in the play store. There was an update pushed out
[01:22:34.480 --> 01:22:38.960]   yesterday. I had a debit card just wait a minute. Here's credit and debit cards. Oh, yeah, wait a
[01:22:38.960 --> 01:22:45.840]   minute. Here's a book butch button. Now can I take Oh, look camera button. Maybe I can scan your
[01:22:45.840 --> 01:22:51.600]   credit or debit card. Oh, this is awesome. Let's see. Okay, I put the numbers right over the numbers.
[01:22:51.600 --> 01:22:56.320]   Yes, it got it. You don't even have to take a picture. Oh, okay. So it got the number,
[01:22:56.320 --> 01:22:59.920]   but I still have to put the expiration and the CVC and then all that stuff.
[01:22:59.920 --> 01:23:06.960]   Well, if I just happened to take anybody's credit card from a wallet, I can put it into my wallet.
[01:23:06.960 --> 01:23:12.400]   Yeah. Is that what we're saying here? Yeah. Well, you have to have enough time with it to take
[01:23:12.400 --> 01:23:17.040]   a picture and get these additional information. I just never thought that that boy that seems
[01:23:17.040 --> 01:23:23.200]   like light security. Yeah. Hold on to your credit card. Well, you know, what you don't give your
[01:23:23.200 --> 01:23:28.400]   credit card to people and then have them disappear. I give them all that data. All you need is a
[01:23:28.400 --> 01:23:32.720]   waiter with a with the Google wallet app and every time somebody pays a walks around walks away with
[01:23:32.720 --> 01:23:40.400]   the car. This is it. Okay, I'm going to add that. Add card. The expiration date should be captured
[01:23:40.400 --> 01:23:44.400]   automatically. It mustn't have gotten the read on your mind. I've seen it. Oh, I can give each
[01:23:44.400 --> 01:23:50.240]   card a color too. So you can choose what color you want your card to be. That one should be blue
[01:23:50.240 --> 01:23:58.000]   or maybe gray. Oh, let's make it blue. Wow. That was pretty easy. Where do I get that,
[01:23:58.000 --> 01:24:03.360]   me up? So it's hard to find because basically you have to go into settings and then it says
[01:24:03.360 --> 01:24:07.680]   payment methods, credit and debit cards. And then you then you have a list and then you press the
[01:24:07.680 --> 01:24:11.520]   plus sign. You could show this Brian. It's nothing. There's nothing. I got it. Okay. Then press the plus
[01:24:11.520 --> 01:24:14.240]   sign. I got it. I got a camera icon right there.
[01:24:14.240 --> 01:24:20.080]   So it does normally pick up. Maybe it just couldn't on my card because of the glare or whatever.
[01:24:20.080 --> 01:24:25.200]   I have a United Airlines. I don't want to have a card and no one can ever read the number because
[01:24:25.200 --> 01:24:32.240]   it's boss badly. I bet it wouldn't take that one. This is cool. What else don't you know?
[01:24:32.240 --> 01:24:35.280]   What are the cards? Can I put it? Oh, I'm sorry. We're doing a show. Never mind.
[01:24:36.400 --> 01:24:42.640]   You put in your Starbucks card or does this do loyalty cards? It does. What does limited
[01:24:42.640 --> 01:24:48.080]   loyalty? It's kind of like coin. It's very. I wonder how much of this is in response to coin.
[01:24:48.080 --> 01:24:53.200]   Like we better rush these features along. It happened so quickly. It's hard to think there was
[01:24:53.200 --> 01:24:56.320]   response. I mean, this was the root of this physical card had been around for a while,
[01:24:56.320 --> 01:25:00.160]   but they're definitely doing a push and it seems like coin was too recent for it to just
[01:25:00.160 --> 01:25:05.200]   funny. It's a good prediction. Land at the same time. Eric Spitz had some months ago and we said
[01:25:05.200 --> 01:25:08.400]   it on the show that Google at one point thought of starting a currency and they said, well,
[01:25:08.400 --> 01:25:12.160]   we wouldn't do that because governments don't like that. Now Bitcoin is getting acceptance
[01:25:12.160 --> 01:25:16.240]   and other currencies are coming out. I wouldn't be surprised if we get Google bucks.
[01:25:16.240 --> 01:25:22.960]   I think they should. Google bucks. Every, you know, this is every science fiction novel from the
[01:25:22.960 --> 01:25:31.040]   70s and 80s basically had corporate including the great William Gibson novels like NeuroMancer had
[01:25:31.040 --> 01:25:36.080]   corporate really corporations were the government. Transnationals were in fact that the fact of
[01:25:36.080 --> 01:25:46.640]   governments of the world. That's only a matter of time. Right? No. Google's building flying cars
[01:25:46.640 --> 01:25:52.640]   too. And I think that really means the end times are near. This is a stealth company that maybe
[01:25:52.640 --> 01:25:59.280]   it's right next to Google, maybe even inside Google. It's doesn't look like anything I'd want to drive.
[01:25:59.920 --> 01:26:07.840]   The patent covers personal aircraft. It's assigned to Z Arrow, a stealth company in Mountain View.
[01:26:07.840 --> 01:26:15.760]   And apparently Z Arrow's offices are near Shoreline Lake in Mountain View.
[01:26:15.760 --> 01:26:21.120]   And that's apparently very close. It could very easily be a Google skunks work. I think
[01:26:21.120 --> 01:26:26.080]   I Sergey Brin would like to have a flying car. He probably has some trusted minions working on it.
[01:26:27.120 --> 01:26:31.760]   Okay, dumb question. Isn't a flying car just a little plane? I mean,
[01:26:31.760 --> 01:26:36.800]   it's not just a personal plane. There used to be a sitcom. The only scene with this spectrum,
[01:26:36.800 --> 01:26:42.080]   there was a sitcom about a guy who had a flying car. When it landed, you basically just fold the
[01:26:42.080 --> 01:26:48.000]   wheels up. Do you remember that? Wasn't very long on the on the this is it back in our youth,
[01:26:48.000 --> 01:26:54.080]   sir. They're are they? Supercar? I don't know. There was one on display in Turnal 5, a JFK.
[01:26:54.080 --> 01:26:58.400]   It was like a as a little over a year now, but it was like a personal plane and it kind of
[01:26:58.400 --> 01:27:02.960]   folded up. I mean, it was a little big for a garage. The Jetsons? Is that what you're thinking?
[01:27:02.960 --> 01:27:06.160]   I forget what it was. I actually made a note of it because I was like, when I'm rich,
[01:27:06.160 --> 01:27:11.280]   I'm going to buy one of these because I love flying. So 40 or 50. I feel like it was that much money.
[01:27:11.280 --> 01:27:16.480]   Was there anybody there over 55? Bob Cummings, right? It was a Bob Cummings show.
[01:27:16.480 --> 01:27:25.120]   How right? Cummings. I loved him. Are you two? I don't know. So apparently the author of this,
[01:27:25.120 --> 01:27:35.280]   which was a reporter for the San Francisco Chronicle, Caleb Garling, emailed Google and got a response
[01:27:35.280 --> 01:27:40.000]   from Dr. Crew. Thanks for your interest and note this morning. As you gathered, I am working on
[01:27:40.000 --> 01:27:45.360]   some interesting transportation ideas at an early stage startup in Mountain View near Google,
[01:27:45.360 --> 01:27:49.680]   and other tech companies, but not affiliated with them. The company is in early stages,
[01:27:49.680 --> 01:27:53.280]   still in stealth mode. And we have not been talking to people about our flying car.
[01:27:53.280 --> 01:28:01.680]   Thank you. But I'll let you know. So there. Too bad. I would like a Google flying car.
[01:28:01.680 --> 01:28:07.520]   I would. I would. Yeah. Have you been scruggled?
[01:28:07.520 --> 01:28:14.640]   Oh, man. You probably talked about this last night at the pond stars and all of that.
[01:28:15.200 --> 01:28:20.320]   We didn't. I actually miss this. I'm so sad because like I love pond stars. Like I love that
[01:28:20.320 --> 01:28:24.720]   that show when I went to Vegas, I went to the shop. I was totally fangirl. And then I saw this
[01:28:24.720 --> 01:28:29.600]   ad and I was like, now you hate them. Now it's so misguided. Oh, these guys would do anything for
[01:28:29.600 --> 01:28:34.880]   money. Give me a break. You think the pond stars are in it for the fame and fortune? They're pond
[01:28:34.880 --> 01:28:42.080]   brokers. So if you go to scruggled.com, this is a continuing campaign from Microsoft that is
[01:28:42.080 --> 01:28:46.720]   so stupid and misguided created by Mark Penn, former pollster for here with Clinton,
[01:28:46.720 --> 01:28:53.120]   got fired by the Clinton team in 2008 after she lost a bunch of primaries. He's a political
[01:28:53.120 --> 01:28:58.720]   dirty trickster. Let's face it. Think here's the scruggled on the mail the first one I saw a couple
[01:28:58.720 --> 01:29:04.560]   of days ago. Think Google respects your privacy. Think again, Google goes through every Gmail that
[01:29:04.560 --> 01:29:09.520]   sent or received looking for keywords so they could target Gmail users with paid ads. And there's
[01:29:09.520 --> 01:29:14.000]   no way to opt out of this invasion of your privacy. Jesus.
[01:29:14.000 --> 01:29:19.600]   Tell Google to stop sign the petition. Outlook.com is different. We don't scan the content of your
[01:29:19.600 --> 01:29:24.880]   email to target you with ads because we can't sell any ads. We have no revenue. We do. We do,
[01:29:24.880 --> 01:29:31.120]   however, scan your email for Andy's spam. I love this too. You can buy a mug.
[01:29:31.120 --> 01:29:36.880]   You can go shopping. Oh, that's Google shopping. Let me see where's the store. There it is.
[01:29:36.880 --> 01:29:41.760]   Click the store. Here's the Google mug. There's a scruggled logo hat.
[01:29:41.760 --> 01:29:48.080]   Don't you love the logo of the spider with the chrome? Like it's a black widow,
[01:29:48.080 --> 01:29:51.360]   except instead of an hourglass, it has the chrome logo. The mug says,
[01:29:51.360 --> 01:29:57.120]   keep calm while we steal your data. It's just Jesus. Can they just be subtle a little bit?
[01:29:57.120 --> 01:29:59.680]   This is so is this political dirty tricks?
[01:29:59.680 --> 01:30:04.320]   Is that are they going to argue it's fair use use of the trademark? I know. I thought it was
[01:30:04.320 --> 01:30:08.800]   interesting that they are using the trademark. I guess they have to. I'm watching you.
[01:30:08.800 --> 01:30:13.760]   They are. They're just trying to bait Google and if you used Proctor and Gamble's trademarks
[01:30:13.760 --> 01:30:21.840]   in an attempt to slander them. The New York Times has stopped every parody. It comes out.
[01:30:21.840 --> 01:30:25.200]   I think they're trying to bait Google and they're doing something that Google should.
[01:30:25.200 --> 01:30:31.680]   Oh, come on. This is so stupid. Step into a web. The t-shirt has a spider in it. Don't get
[01:30:31.680 --> 01:30:37.360]   scruggled. I just think, you know, this is what happens when you hire political operatives.
[01:30:37.360 --> 01:30:39.120]   I'll bet the t-shirts sell better than windows.
[01:30:39.120 --> 01:30:47.360]   They probably do. So the latest is the Pawn Stars. This is a hit piece on Chrome,
[01:30:47.360 --> 01:30:54.880]   Google Chromebooks. I got this Google Chromebook as a gift for my mom. I brought it in today.
[01:30:54.880 --> 01:30:58.640]   Hoping to get enough cash out of it to get to Hollywood. Oh, I'm sure that's what mom wanted
[01:30:58.640 --> 01:31:06.000]   you to do with her nice gift is pon it so you can go to Hollywood and become a drug addict.
[01:31:06.000 --> 01:31:12.800]   Live on Sunset Boulevard. Hi, I'm looking to trade in this gift for a ticket to Hollywood.
[01:31:12.800 --> 01:31:15.760]   What makes you think it's worth that much? It's a laptop.
[01:31:15.760 --> 01:31:22.560]   This is the Google Chromebook, a relatively new kind of device. Because Chromebook applications
[01:31:22.560 --> 01:31:28.480]   are web-based, when you're not connected, it's pretty much a brick. That's a major drop-off.
[01:31:29.360 --> 01:31:31.040]   I could do a better fake glasses now. I could do a better fake glasses now.
[01:31:31.040 --> 01:31:35.600]   Built-in applications like office and iTunes that work even when you're offline.
[01:31:35.600 --> 01:31:40.080]   You see this thingy? I'll tell you this, Gina, you could see that these Pawn Stars have really
[01:31:40.080 --> 01:31:44.960]   become television, you know, celebrity. They really understand how the medium works now.
[01:31:44.960 --> 01:31:49.120]   They're very comfortable on camera. I'm impressed even when they're selling out.
[01:31:49.120 --> 01:31:54.800]   He says that as if any computer isn't anything more than really a calculator,
[01:31:54.800 --> 01:31:58.960]   when it's not connected. I mean, with the exception of games that you can play, you know, without being
[01:31:58.960 --> 01:32:03.680]   connected, like, oh, it's just such a terrible, if you're not connected, it might as well be a
[01:32:03.680 --> 01:32:11.600]   brick. It's like, yeah, most computers are. Anyway, you'll laptop. It doesn't have windows or
[01:32:11.600 --> 01:32:15.200]   or on. Wait a minute, it doesn't have windows or office, so it's not a real laptop?
[01:32:15.200 --> 01:32:23.280]   Without Wi-Fi, it doesn't do much at all. Wait a minute, all Chromebooks have Wi-Fi. Oh, I see,
[01:32:23.280 --> 01:32:26.560]   if you don't earn a Wi-Fi access spot, right? It doesn't do much at all.
[01:32:26.560 --> 01:32:30.640]   Right. Also, it doesn't mention the fact that there's data connections from a lot of Chromebooks.
[01:32:30.640 --> 01:32:32.000]   Wait a minute. Like, you can have a Chromebook with data.
[01:32:32.000 --> 01:32:37.280]   Do this again. Googled tracks what you do so they can sell ads. That's what!
[01:32:37.280 --> 01:32:43.520]   Honey, you got a job in Hollywood. You are good!
[01:32:43.520 --> 01:32:44.240]   That's how you get scruggled.
[01:32:44.240 --> 01:32:47.200]   Just raise that eyebrow. What's a scruggled?
[01:32:47.200 --> 01:32:51.440]   Google is always trying to... So that guy, that old guy, he always clueless like that.
[01:32:51.440 --> 01:32:55.360]   Is that part of his stick? Yeah, it's part of his stick. He's like, he's the dad. He's like the
[01:32:55.360 --> 01:32:58.400]   old fashioned dad. That's making more money off your personal information.
[01:32:58.400 --> 01:33:03.360]   This Chromebook hardware makes it even easier for him. Not going to Hollywood, am I?
[01:33:03.360 --> 01:33:06.880]   Not with a Google Chromebook. You might get Romino.
[01:33:06.880 --> 01:33:15.920]   Unfortunately, I can't buy everything, especially when it's not what it appears to be.
[01:33:15.920 --> 01:33:19.520]   And this is not a real laptop. Exactly what it appears to be.
[01:33:19.520 --> 01:33:22.240]   I don't want to get scruggled. It's a good...
[01:33:22.240 --> 01:33:27.520]   Well, I was hoping the guys would take this thing off my hands, but I appreciate how honest they were.
[01:33:27.520 --> 01:33:30.640]   Scruggled. Who knew?
[01:33:30.640 --> 01:33:33.760]   Your mom was scruggled when she gave you that horrible gift.
[01:33:33.760 --> 01:33:38.560]   That's what... I don't like about this. My mom gave me this. Can I pawn it?
[01:33:38.560 --> 01:33:44.080]   Anyway, I think that's a good ad. It's silly.
[01:33:44.800 --> 01:33:51.040]   Leo, I've put the perfectly in the form out of the show. Yeah. Sorry. Sorry, Jeff. Go ahead.
[01:33:51.040 --> 01:33:53.920]   I just put in the rundown. Bob Cummings flying his flying car.
[01:33:53.920 --> 01:33:59.200]   Oh, how exciting. The comments and the rundown. Oh, let me let me click the comments and let's
[01:33:59.200 --> 01:34:09.200]   take a flight. I have old comments here. There we go. Here we go. Oh, look at that.
[01:34:10.800 --> 01:34:15.600]   That's the idea. Take it. The new Bob Cummings show. So much better than the old Bob Cummings show.
[01:34:15.600 --> 01:34:18.720]   No, there wasn't Bob Cummings. That's the last shot of it. You can...
[01:34:18.720 --> 01:34:25.280]   He's driving a flying car. So the difference between a flying car and a plane, the fact that
[01:34:25.280 --> 01:34:33.120]   a flying car has like a wheel. Oh, because yes. And you can fold up the wings. That's what happens.
[01:34:33.120 --> 01:34:35.840]   That's a good question. It really is just a personal plane, isn't it?
[01:34:36.400 --> 01:34:41.360]   It is. Okay. So I found the personal plane. It's the icon icon aircraft. There's this like personal,
[01:34:41.360 --> 01:34:48.080]   it's $190,000, which you know, not cheap. But the taint that the wings actually fold up.
[01:34:48.080 --> 01:34:55.120]   Yeah, there's quite a few things like this. Yeah, sport fly. I don't think really this is a
[01:34:55.120 --> 01:35:04.400]   great idea. I don't want to rush hour above my head. Well, one of the discussions in journalism
[01:35:04.400 --> 01:35:08.640]   circles is that for journalists to start to use drones, the problem is, of course,
[01:35:08.640 --> 01:35:14.480]   that the ones with the carbon wings and rotors and all that, if they fall, if journalists are
[01:35:14.480 --> 01:35:22.320]   bad pilots, they could kill people. Oh, but it makes the CNN race to get home so much more.
[01:35:22.320 --> 01:35:28.320]   Are you seeing this today? The CNN is spending so many of hours of its airtime on a race from
[01:35:28.320 --> 01:35:35.360]   its on of its reporters to get home from New York to DC. They're tweeting. They're just like the
[01:35:35.360 --> 01:35:41.440]   great race. It's like, there's no news today. There clearly isn't no news today. That's what
[01:35:41.440 --> 01:35:48.880]   they need. They need a flying car. That is cool. I am bummed about this. Although I think I'm
[01:35:48.880 --> 01:35:52.800]   understanding it a little bit better. 23 and me. It's the one where you spit into the vial.
[01:35:53.600 --> 01:35:59.600]   And then you get there. You get your genetic analysis. The FDA says,
[01:35:59.600 --> 01:36:05.360]   the vial right here. You have one? Are you going to do it? I will. I bought it for the follow up.
[01:36:05.360 --> 01:36:11.040]   I've already done it once. So, so folks can see you get this kit.
[01:36:11.040 --> 01:36:17.920]   The spit kit. There is a little vial that you spit and spit and spit. You have to do a lot of
[01:36:17.920 --> 01:36:22.320]   spitting. It requires a lot of significant amount of spitting. And then you send it in and then
[01:36:22.320 --> 01:36:26.240]   they analyze your DNA. Now the FDA is saying, as you were saying, you know?
[01:36:26.240 --> 01:36:30.640]   Well, they're concerned. And I think they want 23 and they actually, I think they've suspended
[01:36:30.640 --> 01:36:35.520]   their 23 and me sales right now because they want 20. They've asked them in the past name.
[01:36:35.520 --> 01:36:42.240]   23 and me did not respond to show that their genetic testing is real because their concern is
[01:36:42.240 --> 01:36:48.960]   that people are going to do this. And then based on the results of this 23 and me test,
[01:36:48.960 --> 01:36:55.280]   have their breasts cut off or you know, somehow act in some crazy way because of perceived health
[01:36:55.280 --> 01:36:59.840]   risks. This is my 23 and me. The truth is you get so little information from this.
[01:36:59.840 --> 01:37:08.480]   But they saying, you know, if you have twice, so this is a good example. My risk of coronary
[01:37:08.480 --> 01:37:14.480]   heart disease is 1.24 times higher than the average. Should I be worried about it? My risk of
[01:37:14.480 --> 01:37:20.960]   prostate cancer 2.43 times. But you know why? Because I'm a white male from European ancestry.
[01:37:20.960 --> 01:37:29.120]   So I don't know how useful exfoliation glaucoma 2.9 times more likely, but my risk is still only
[01:37:29.120 --> 01:37:35.920]   2.2%. So their concern, and I think this is not an illegitimate concern, is that people will act
[01:37:35.920 --> 01:37:42.960]   on this as if this is health information. But this is the same issue, I argue, as getting mammograms
[01:37:42.960 --> 01:37:48.400]   and prostate tests that it goes back actually to our security discussion. The knowledge isn't
[01:37:48.400 --> 01:37:53.040]   the problem. The problem is what you do with it. Yes. And that's what some doctors say is,
[01:37:53.040 --> 01:37:55.520]   there's no knowledge, you're still going to have to go to a doctor and the doctor is going to have
[01:37:55.520 --> 01:38:02.480]   to agree to do a procedure. Yeah. But you know, the same issue comes up for these full body scan
[01:38:02.480 --> 01:38:06.000]   places, right? They do a lot of advertising out here. I don't know if they do it back there.
[01:38:06.000 --> 01:38:12.320]   We're not as narcissistic as all of you are in color. You can go in and like this is like a
[01:38:12.320 --> 01:38:17.840]   storefront where they have like MRI machines and CAT scans, and you go in and they and it's like
[01:38:17.840 --> 01:38:22.720]   proactively tell you what's wrong with you. And most physicians, I know say it's a terrible idea.
[01:38:22.720 --> 01:38:27.520]   Not because it's dangerous, but because the you'll because you might get information that you will
[01:38:27.520 --> 01:38:35.920]   act inappropriately on. It's just a bad idea to get these proactive testing. I have a 17% chance
[01:38:35.920 --> 01:38:44.480]   of getting risk of getting gout, but that's less than average. So I don't think these things are
[01:38:44.480 --> 01:38:48.560]   all that useful. I think it's completely legitimate for the FDA to say to 23andMe,
[01:38:48.560 --> 01:38:53.600]   you got to show us, you know, that these are that this is real. This is not just some page you made
[01:38:53.600 --> 01:39:01.920]   up. Right. But others are saying conspiracy theory, the medical industries trying to shut down 23andMe.
[01:39:01.920 --> 01:39:05.680]   You know, at the same, but at the same time, the FDA has not protected us from these these quacks.
[01:39:05.680 --> 01:39:10.560]   We're talking about how all men have low T. Well, I know they should do a better job in some areas.
[01:39:10.560 --> 01:39:15.760]   I agree. Let's go. prostate cancer. Right. But you can't. Yeah. Well, really, is that true?
[01:39:15.760 --> 01:39:18.800]   Yeah. Yeah. It's not rubbing that stuff in my scalp.
[01:39:18.800 --> 01:39:25.280]   No, I'm just kidding. I don't have low T. I got plenty of T, whatever that is.
[01:39:28.560 --> 01:39:34.000]   I think it's probably the case to 20. So you've done it, right, Jeff? Yeah. Yeah. And have you done it,
[01:39:34.000 --> 01:39:37.760]   Gina? I have done it. And, you know, it was one of those things. I was like, Oh, I told me I had
[01:39:37.760 --> 01:39:44.080]   brown hair and, you know, you know, you can look at them. You know, it was like, it was mostly kind
[01:39:44.080 --> 01:39:48.480]   of, yeah, I have wetting your wax. But I think I think I shared with you guys, there's there's
[01:39:48.480 --> 01:39:52.880]   this one chromosome that's an indicator for Parkinson's. And so I got this email. It was like, Hey,
[01:39:52.880 --> 01:39:56.400]   we have this new test that's going to give you this information about whether or not you have
[01:39:56.400 --> 01:39:59.680]   this chromosome. And I think I told you guys, I was like, I'm gonna go work day, you know,
[01:39:59.680 --> 01:40:04.160]   I was like sitting at my desk and I got this email and said, I go to the site, I log in and it kind
[01:40:04.160 --> 01:40:08.880]   of like, Hey, you know, this could be potentially life changing, changing information. And I started
[01:40:08.880 --> 01:40:12.560]   to get really scared. You know, you should really talk to a doc. Like they, they really kind of
[01:40:12.560 --> 01:40:16.880]   tell you that you should talk to an actual doctor. This is an actually an indicator that you're
[01:40:16.880 --> 01:40:22.320]   going to get Parkinson's. But I got really like, nervous. I got kind of worked up and nervous about
[01:40:22.320 --> 01:40:26.880]   it. And so I was like, okay, and then you know, you click on unlock and I didn't have it, right?
[01:40:26.880 --> 01:40:31.520]   And I was like, Oh, you know, that's kind of scary. But this is, but this is what the FDA saying,
[01:40:31.520 --> 01:40:36.320]   right? Test 23 means test is creating chaos with people in the 20s and 30s. Look, like it's,
[01:40:36.320 --> 01:40:40.960]   it's scary, scary to see like, I have a higher risk of breast cancer or heart attack or whatever.
[01:40:40.960 --> 01:40:48.400]   But at the same time, I kind of feel like you can't have a mastectomy or an MRI without going
[01:40:48.400 --> 01:40:53.920]   to a doctor and getting it. Right. I mean, it's, it's not like this is something I could do just on
[01:40:53.920 --> 01:40:59.120]   my own, right? You would talk to your doctor about it. I think all 23andMe has to do is say,
[01:40:59.120 --> 01:41:03.600]   Hey, look, this is what we're doing. This is this. This is the standard. We, this is our legitimate.
[01:41:03.600 --> 01:41:07.840]   And I think the FDA will probably go away. It's not unreasonable for them to ask for this kind of
[01:41:07.840 --> 01:41:14.160]   information. My point is I don't get all that much out of 23andMe. And look, I'm 99.6% European.
[01:41:14.160 --> 01:41:20.560]   Yeah, exactly. So the same thing about me surprise. Now, apparently a white
[01:41:20.560 --> 01:41:26.960]   white supremacist in Europe was 14% African American. So that was, that must have been a surprise.
[01:41:26.960 --> 01:41:35.760]   I don't know what the point 4% unassigned is, but, and I, you know, I would like to, I would like
[01:41:35.760 --> 01:41:39.920]   that there's some oversight saying that's an accurate result, not something they just generated
[01:41:39.920 --> 01:41:45.840]   randomly. And the reason it's a Google story is because Anne, Anne, what, how do you pronounce it?
[01:41:45.840 --> 01:41:53.760]   What's sick? Well, just keep, just keep, who is the co-founder of this is somebody's wife,
[01:41:53.760 --> 01:42:00.640]   Sergey's wife. Well, ex wife, ex wife, right, right. Oh, this is the,
[01:42:00.640 --> 01:42:08.640]   Oh, yeah, or sooner or separated wife or whatever. Some learning. And her sister is the head of
[01:42:08.640 --> 01:42:12.720]   advertising. That's right. She's still a good one. That's real. And their mother,
[01:42:12.720 --> 01:42:17.680]   they met because their mother read it, Larry and Sergey, the garage. And their mother is a
[01:42:17.680 --> 01:42:23.280]   magnificent legendary journalism teacher at Palo Alto. No kidding. Yes. That's neat. She is
[01:42:23.280 --> 01:42:30.800]   magnificent. She's also one of the best Google plazers there is. 23andMe has 15 days to provide
[01:42:30.800 --> 01:42:33.760]   specific actions to address the issues raised by the FDA.
[01:42:37.840 --> 01:42:44.320]   I think this is 94% European. Ah, but also sub-Saharan African. Oh, good for you.
[01:42:44.320 --> 01:42:49.440]   At least you're not African. It's Sicily, you know, yeah, everybody invaded Sicily.
[01:42:49.440 --> 01:42:57.760]   At some point, currently invaded your ancestor as well. Indeed. At the same time. I'm sorry. That
[01:42:57.760 --> 01:43:06.240]   was really rude. It's almost Thanksgiving. I'm throwing all caution to the wind.
[01:43:06.880 --> 01:43:15.760]   Um, I think we are. Oh, no. Someone I know is now retweeting the faces in things. Twitter
[01:43:15.760 --> 01:43:19.520]   account has driving me nuts. Have you seen this one? No, faces in things. Face pics.
[01:43:19.520 --> 01:43:25.440]   Face is pics. Twitter.com/facespix.
[01:43:25.440 --> 01:43:27.440]   Faces pics. P-I-C-S.
[01:43:27.440 --> 01:43:34.640]   Yeah, I see. Yes. And it's just a man. Oh, I don't care. These are people's faces in things.
[01:43:35.200 --> 01:43:39.680]   There's a man in the mountain. Cannot unsee. There is a tired roof.
[01:43:39.680 --> 01:43:45.280]   Exactly. Now I've done it to you. There's a kissing plant. There is a bear in that wood grain.
[01:43:45.280 --> 01:43:49.760]   I don't know what that is. Wow, a couple of these are.
[01:43:49.760 --> 01:43:54.320]   My bowl of ice cream wants to look at my, look into my soul.
[01:43:54.320 --> 01:44:02.960]   The office idiot. Frog in the coffee. That's good. I like that one. Apparently there's a lot of
[01:44:02.960 --> 01:44:09.920]   these. Yeah. Look at the blanket one. You're getting there. It's one below the back.
[01:44:09.920 --> 01:44:17.440]   Unexpected friend. Oh, that's really scary. Yeah. I think this is, as we take more and more
[01:44:17.440 --> 01:44:22.000]   pictures of ourselves in our lives, you can expect to see more of this.
[01:44:22.000 --> 01:44:25.360]   All right. We're sort of, we're sort of biologically wired to see the human face.
[01:44:26.160 --> 01:44:31.600]   Yeah. Well, and apparently it's everywhere. Sometimes fish as well. It is.
[01:44:31.600 --> 01:44:38.320]   Follow. Follow is like, I like this. You don't like this, Jeff. Oh, it drives me nuts.
[01:44:38.320 --> 01:44:40.880]   This bird poop looks like a young woman, doesn't it?
[01:44:40.880 --> 01:44:46.400]   I actually don't see that. I totally do not see that one.
[01:44:46.400 --> 01:44:52.160]   Don't keep looking. You will. And then you'll be sorry. Oh, is there a werewolf in this golf
[01:44:52.160 --> 01:44:57.360]   swing? There's a lot of mud. This barn likes to sing opera.
[01:44:57.360 --> 01:45:06.400]   Some of these are really far fetched. Really, really far fetched. Oh, angry mop.
[01:45:06.400 --> 01:45:11.760]   You're right. We are programmed to see. Yeah. Not in the face.
[01:45:13.920 --> 01:45:23.040]   Ah, put them up. Put them up. I'm crying. I can't stop crying. Oh, wow.
[01:45:23.040 --> 01:45:29.280]   The Edvard Munch screaming. I think that's going over. Circuit box.
[01:45:29.280 --> 01:45:32.240]   Yeah.
[01:45:32.240 --> 01:45:38.960]   Sorry. Is there a Google doodle for Thanksgiving yet?
[01:45:40.720 --> 01:45:47.360]   Did you like there was a great doctor who doodle? You know, I that is one area of
[01:45:47.360 --> 01:45:52.720]   geeked him that I have never I have yet to experience. I actually bought a season. I
[01:45:52.720 --> 01:45:57.360]   couldn't get through one episode of Doctor Who. I couldn't do it. Oh, you've actually tried. I
[01:45:57.360 --> 01:46:02.080]   haven't tried. I haven't tried. But yeah, I don't it's a thing, right? And there's lots of doctors.
[01:46:02.080 --> 01:46:05.920]   I don't know. It's all confusing. It's hard for beginners to even get like they like Marmite
[01:46:05.920 --> 01:46:11.600]   and cricket. What why are we listening to them? I'm just letting you guys go on because I
[01:46:11.600 --> 01:46:15.920]   went on a diatribe on Sunday about Doctor Who and I just don't get it. And I've now been
[01:46:15.920 --> 01:46:21.680]   banished from the geeks Island. Oh, so we're all in agreement here. This is a that never happens
[01:46:21.680 --> 01:46:25.600]   because we have good people love it. People love it. Love it. I know. I understand. I think it's
[01:46:25.600 --> 01:46:33.520]   a never did the Hobbit. I don't know. I like the books a lot. Should read the books to Edda.
[01:46:33.520 --> 01:46:37.600]   You owe it. You owe that much to her. That's what we'll you don't have the movies. No, but the books.
[01:46:37.600 --> 01:46:44.720]   Yeah. I mean, I like the Denny's breakfast menu. That's Hobbit themed. I don't like the.
[01:46:44.720 --> 01:46:50.800]   There is a Hobbit themed Denny's breakfast menu. There is there actually never go to Denny's
[01:46:50.800 --> 01:46:54.960]   happen to have gone while the movie was out. Happens to see the menu. It's it's.
[01:46:54.960 --> 01:47:02.560]   Don't tell him your 9% sub-Saharan African. Well, Gina, did you see google.com/Hobbit?
[01:47:03.520 --> 01:47:11.680]   Ooh. Brand new. Oh, yeah. It's the breakfast menu. That's not Hobbit food. That's Denny's food.
[01:47:11.680 --> 01:47:17.520]   Hobbits don't eat burgers. But there's that one that one thing with the roll has the little
[01:47:17.520 --> 01:47:21.520]   whole cut out with the egg inside. It's got a terrible name, but it's delicious.
[01:47:21.520 --> 01:47:28.960]   Brew pumpkin coffee. That's not no Hobbit. No self-respecting Hobbit would eat that.
[01:47:28.960 --> 01:47:34.880]   That thing right there. That's an egg in a hole. Cheddar bun. It's a go scroll up a little bit.
[01:47:34.880 --> 01:47:39.600]   Let me let's read that. It's amazing. It's the Hobbit whole breakfast. The Hobbit whole breakfast.
[01:47:39.600 --> 01:47:45.520]   Two fried eggs stuck right in the center of a grilled cheddar bun. Oh, that sounds good.
[01:47:45.520 --> 01:47:50.720]   Served with two slices of bacon and crispy ash browns and melted spout of put hair on your toes.
[01:47:50.720 --> 01:47:56.160]   Oh, yeah. That is Hobbit food. I don't know about the rest of it. You must be green on that plate to be seen.
[01:47:56.880 --> 01:48:02.000]   No green. Now Denny's has no vegetables. I worked with Denny's. I was a busboy at Denny's
[01:48:02.000 --> 01:48:06.080]   when I was first starting out in the world in life. And I can vouch for the fact there are no
[01:48:06.080 --> 01:48:14.080]   vegetables. You were at Denny's? I was at Ponderosa Steakhouse. Aww. Aww. Did the waitress share tips with you?
[01:48:14.080 --> 01:48:18.720]   I had a break. It just take at the Ponderosa Steakhouse. Is it google.com/
[01:48:18.720 --> 01:48:24.640]   Hobbit. I don't have anything. Yeah, I get a 404. Look up Hobbit Google Maps.
[01:48:25.360 --> 01:48:32.400]   Oh. Okay, Google. Hobbit Google Maps. Take us to the Shire.
[01:48:32.400 --> 01:48:38.960]   Take us to the Shire. I want to go to the Shire. Right. This is the, this is the
[01:48:38.960 --> 01:48:43.440]   Chrome. Chrome experiment. It's a Chrome experiment. Yeah. So I should do it in Chrome.
[01:48:43.440 --> 01:48:51.200]   Oh, yeah. Okay. Yeah. Geez. I used Safari on the Mac. I don't know why.
[01:48:53.520 --> 01:49:00.960]   Uh, da-da-da-da-da-da. Show about Google, you know. Google Map. I know. I know. I know. I know.
[01:49:00.960 --> 01:49:06.320]   I know. Middle Earth Google Chrome experiment. Blah, blah. How do I get there? I hate it when
[01:49:06.320 --> 01:49:13.760]   they have articles that talk about a web page. Just say, just say, okay, Google. Okay, Google.
[01:49:13.760 --> 01:49:18.800]   Maps. Hobbit Google Maps. There you go. Wow. Look at that. That was easy.
[01:49:18.800 --> 01:49:24.960]   See? See? Rivendale.
[01:49:24.960 --> 01:49:32.320]   Dalgaldar. Oh, yeah. Oh, yeah. Yeah. The, we're at the Trollshaws. Get the hell out of there.
[01:49:32.320 --> 01:49:35.120]   There's the Shire. Let's zoom in on the Shire. Can we?
[01:49:35.120 --> 01:49:42.880]   Oh, you can't go any closer. Is it maybe only where those, those circles are that you can go closer?
[01:49:43.840 --> 01:49:47.840]   Well, clicking that, clicking something, not there. I don't even know what that is. Not even
[01:49:47.840 --> 01:49:52.960]   the book. Rivendale. There we go. Trollshaw. The hell's Trollshaw?
[01:49:52.960 --> 01:49:58.960]   Oh, this is, they made this is in the movie. This is from the movie. Yeah.
[01:49:58.960 --> 01:50:01.120]   It was, uh, yeah.
[01:50:01.120 --> 01:50:06.560]   Is Denny's on the map? They have the Denny's Hobbit menu in it.
[01:50:06.560 --> 01:50:08.560]   [laughter]
[01:50:08.560 --> 01:50:12.560]   I want one of those now. I want Rivendale. Now that's good. That's where Elrond lives.
[01:50:13.200 --> 01:50:14.800]   He was in the, the Jetsons.
[01:50:14.800 --> 01:50:17.440]   Oh, wait.
[01:50:17.440 --> 01:50:19.280]   [laughter]
[01:50:19.280 --> 01:50:22.720]   Oh, I'm getting punchy. I am getting so punchy.
[01:50:22.720 --> 01:50:25.760]   Uh, oh.
[01:50:25.760 --> 01:50:33.600]   Oh, yeah. But see, this is all from the movie. This is just a big promotion for the movie.
[01:50:33.600 --> 01:50:37.600]   Oh, no. That's okay. But I mean, you know, the movie's coming at them. I got tickets.
[01:50:38.240 --> 01:50:44.080]   December 13th. Peter, Peter. Anyway, there was a doctor who, Google, Google doodle.
[01:50:44.080 --> 01:50:48.480]   Yes, it was quite good. But I didn't get it because I, yeah, because I, yeah, neither me,
[01:50:48.480 --> 01:50:55.360]   me neither. I got it. Nothing. Uh, I think we've done every story on this list now.
[01:50:55.360 --> 01:51:01.360]   Anything you guys want to talk about before we finish her up, I haven't done my ad yet. So I'm
[01:51:01.360 --> 01:51:05.120]   going to do an ad. Oh, we have one good. Phew. I was getting nervous there.
[01:51:05.120 --> 01:51:07.120]   [whistles]
[01:51:07.920 --> 01:51:09.920]   I think we're good.
[01:51:09.920 --> 01:51:11.840]   Katie Couric going to Yahoo, do we care?
[01:51:11.840 --> 01:51:15.440]   Well, I actually want to ask you about that because I'm going to do a NPR appearance tomorrow.
[01:51:15.440 --> 01:51:17.280]   So tell me what I should say about it. What do you think about that?
[01:51:17.280 --> 01:51:23.440]   Well, she's, you know, she's old and washed up. So Yahoo got her. Once again, confirming the
[01:51:23.440 --> 01:51:28.720]   notion that somehow internet media is somehow secondary to real media.
[01:51:28.720 --> 01:51:30.560]   Oh, wow.
[01:51:30.560 --> 01:51:36.640]   But internet natives are going to dismiss Katie Couric. They're not going to be interested.
[01:51:36.640 --> 01:51:41.920]   She has not, she's not, she's not, has nothing to say to them. This is Yahoo pursuing the same
[01:51:41.920 --> 01:51:44.640]   silly media strategy they did under Terry Semmel.
[01:51:44.640 --> 01:51:45.760]   Yeah.
[01:51:45.760 --> 01:51:52.320]   In fact, they started talking to Katie Couric back prior to Marissa Meyer to do the same thing.
[01:51:52.320 --> 01:51:57.440]   They couldn't make a deal because this is the problem with mainstream media. They're paid
[01:51:57.440 --> 01:52:06.560]   millions and millions of dollars to do very little. So under Ross Levinson, a couple of years ago,
[01:52:06.560 --> 01:52:11.440]   they were trying to get her over there. I don't know. I, you know, I think this is very,
[01:52:11.440 --> 01:52:18.000]   this is a very common response. It's self-loathing new media.
[01:52:18.000 --> 01:52:20.560]   They don't feel that? Huh? Yeah.
[01:52:20.560 --> 01:52:24.800]   Self-loathing new media.
[01:52:24.800 --> 01:52:25.440]   Yeah.
[01:52:25.440 --> 01:52:28.560]   Yahoo feels like it's more legitimate when it has Katie Couric.
[01:52:28.560 --> 01:52:28.560]   Yeah.
[01:52:28.560 --> 01:52:31.280]   Katie Couric feels like she's more legitimate if she's on the web.
[01:52:31.280 --> 01:52:31.760]   No.
[01:52:31.760 --> 01:52:32.160]   That's what it's.
[01:52:32.160 --> 01:52:36.080]   No, Katie Couric isn't getting work because in mainstream media, when you're
[01:52:36.080 --> 01:52:39.040]   56, you're old and washed up, especially if you're a woman.
[01:52:39.040 --> 01:52:44.960]   So Katie's got a syndicated talk show and their NBC is paying her a lot of money to sit on the
[01:52:44.960 --> 01:52:51.200]   sidelines. So Katie is just taking the money. Yahoo is self-loathing media saying, well,
[01:52:51.200 --> 01:52:55.360]   nobody's ever going to watch new media unless we have some real celebrities.
[01:52:55.360 --> 01:52:59.760]   And I see the other thing going the other way too. I see a lot of new media types
[01:52:59.760 --> 01:53:04.400]   saying, well, you know, you're not really making it unless you're on NBC, ABC or CBS.
[01:53:04.400 --> 01:53:06.480]   And I think that that's really an old fashioned mindset.
[01:53:06.480 --> 01:53:09.680]   So Katie, I don't blame her. She's going for the bucks.
[01:53:09.680 --> 01:53:12.320]   She's probably going to make sure Yahoo is going to give her a lot of money.
[01:53:12.320 --> 01:53:17.440]   But I don't think she I'm sure she jokes about it in private.
[01:53:17.440 --> 01:53:20.480]   Yeah. I'm going to do something in Yahoo.
[01:53:20.480 --> 01:53:24.160]   I used to be I used to be the news anchor or the Tiffany network CBS.
[01:53:24.160 --> 01:53:24.320]   Yeah.
[01:53:24.320 --> 01:53:24.320]   Yeah.
[01:53:24.320 --> 01:53:25.920]   I'm going to Yahoo.
[01:53:25.920 --> 01:53:26.880]   What the hell, right?
[01:53:26.880 --> 01:53:29.200]   It's a it's a.
[01:53:29.200 --> 01:53:30.160]   It's so depressing.
[01:53:30.160 --> 01:53:32.480]   I swear to I, I, I, Katie Couric is badass.
[01:53:32.480 --> 01:53:33.440]   I sort of I love.
[01:53:33.440 --> 01:53:34.640]   I love Katie Couric.
[01:53:34.640 --> 01:53:36.000]   I love Katie Couric too.
[01:53:36.000 --> 01:53:37.280]   I love Katie Couric.
[01:53:37.280 --> 01:53:41.120]   But her talk shows not doing so well, but she's not working.
[01:53:41.120 --> 01:53:43.920]   No, I don't understand. Is the talk show dying because of this?
[01:53:43.920 --> 01:53:52.400]   She says she's going to finish this season of Katie and then then start to work for you.
[01:53:52.400 --> 01:53:54.720]   And it's not clear that the talk shows going to get renewed.
[01:53:54.720 --> 01:53:56.560]   All the teams unlikely that it will get renewed.
[01:53:56.560 --> 01:53:57.920]   And here's the other thing I don't understand about this.
[01:53:57.920 --> 01:54:01.200]   Is this is this intended to be an appointment viewing show?
[01:54:01.200 --> 01:54:03.760]   Oh, it's noon. It's time for Katie on Yahoo.
[01:54:03.760 --> 01:54:08.000]   Well, that's why none of this makes any sense because that's not how people consume
[01:54:08.000 --> 01:54:08.960]   content on the internet.
[01:54:08.960 --> 01:54:12.000]   Yeah, I mean, if that's the case, I don't know.
[01:54:12.000 --> 01:54:12.960]   I don't know if that's the case.
[01:54:12.960 --> 01:54:15.360]   It shows a certain times that people tune in for.
[01:54:15.360 --> 01:54:16.000]   I mean, that's not.
[01:54:16.000 --> 01:54:17.520]   We're doing this Yahoo hired David Pogue.
[01:54:17.520 --> 01:54:19.200]   I'm sure very expensive from the New York Times.
[01:54:19.200 --> 01:54:23.840]   They're trying to get mainstream media types to come over to Yahoo to legitimize whatever
[01:54:23.840 --> 01:54:24.240]   they're up to.
[01:54:24.240 --> 01:54:25.520]   And it's not the first time they've done it.
[01:54:25.520 --> 01:54:26.320]   They do it all the time.
[01:54:26.320 --> 01:54:30.880]   And you know, I'm sure Oprah will be doing a show there too.
[01:54:31.760 --> 01:54:34.160]   Actually, Oprah doesn't need the money.
[01:54:34.160 --> 01:54:35.840]   Well, that didn't stop her.
[01:54:35.840 --> 01:54:36.640]   Right.
[01:54:36.640 --> 01:54:39.040]   She didn't need the money from a long time.
[01:54:39.040 --> 01:54:40.960]   Yeah, it's not about money from Oprah.
[01:54:40.960 --> 01:54:41.280]   Right.
[01:54:41.280 --> 01:54:42.560]   Well, what is it about Oprah?
[01:54:42.560 --> 01:54:42.720]   All right.
[01:54:42.720 --> 01:54:44.720]   But here's when Howard Stern.
[01:54:44.720 --> 01:54:45.600]   Howard and I understand.
[01:54:45.600 --> 01:54:48.560]   And it's looking to renew his contract at Sirius.
[01:54:48.560 --> 01:54:52.000]   I personally introduced his agent to Eric Schmidt.
[01:54:52.000 --> 01:54:59.920]   It's my opinion that something like Howard Schmidt is Howard Stern is a brand and doesn't need Google.
[01:54:59.920 --> 01:55:01.680]   Google needs him more than he needs them.
[01:55:01.680 --> 01:55:04.160]   He should do his own thing.
[01:55:04.160 --> 01:55:08.880]   Yeah, he totally seems like the somebody who could do the Louis CK in the set of his own
[01:55:08.880 --> 01:55:11.280]   website and just have it be powerful as well.
[01:55:11.280 --> 01:55:13.680]   He's got a huge community.
[01:55:13.680 --> 01:55:14.960]   What does he need Google for?
[01:55:14.960 --> 01:55:17.760]   Well, he's always why did he need Sirius?
[01:55:17.760 --> 01:55:18.560]   He's always needed.
[01:55:18.560 --> 01:55:19.680]   He's always worked for some company.
[01:55:19.680 --> 01:55:20.640]   He's a wage slave.
[01:55:20.640 --> 01:55:22.800]   Some people, it's hard for people to get out of that mindset.
[01:55:22.800 --> 01:55:24.320]   And in fact, it's been hard.
[01:55:24.320 --> 01:55:28.560]   It's always hard for us to attract people to twit because people say,
[01:55:28.560 --> 01:55:31.280]   "But I need to work for a real company."
[01:55:31.280 --> 01:55:34.400]   Leo, I would work there in a flash.
[01:55:34.400 --> 01:55:35.760]   I'd rather work for you than anybody.
[01:55:35.760 --> 01:55:44.480]   When you say that, Leo, you mean people who establish names at like...
[01:55:44.480 --> 01:55:45.440]   No, no, no, no, no.
[01:55:45.440 --> 01:55:46.560]   Other of my...
[01:55:46.560 --> 01:55:49.200]   I won't say particular names, but people...
[01:55:49.200 --> 01:55:49.920]   Yeah, you have to say names.
[01:55:49.920 --> 01:55:51.760]   Who are you in New Media?
[01:55:51.760 --> 01:55:54.000]   I don't go after Katie.
[01:55:54.000 --> 01:55:55.120]   The kitty quirks of the world.
[01:55:55.120 --> 01:55:56.160]   I can't afford them.
[01:55:56.160 --> 01:55:57.600]   And it's funny, even...
[01:55:57.600 --> 01:56:00.080]   I mean, we can't pay millions of dollars to anybody.
[01:56:00.080 --> 01:56:01.840]   We can't give them Yahoo money.
[01:56:01.840 --> 01:56:02.960]   So some of it might be that.
[01:56:02.960 --> 01:56:07.760]   It feels like there's got to be tons of indie podcasters who do it for free.
[01:56:07.760 --> 01:56:10.320]   For the love of it, who are good at it,
[01:56:10.320 --> 01:56:12.880]   who would jump at the chance of being at twit.
[01:56:12.880 --> 01:56:13.920]   I don't know.
[01:56:13.920 --> 01:56:14.400]   I don't know.
[01:56:14.400 --> 01:56:14.880]   I don't know.
[01:56:14.880 --> 01:56:18.560]   It's the people with real jobs working at CNET and places like that,
[01:56:18.560 --> 01:56:20.000]   and revision three who have...
[01:56:22.160 --> 01:56:26.160]   See, CBS about CNET, so now I'm working at CBS.
[01:56:26.160 --> 01:56:27.520]   Right.
[01:56:27.520 --> 01:56:28.480]   Discovery, but revision...
[01:56:28.480 --> 01:56:29.360]   I don't know, I'm working at Discovery.
[01:56:29.360 --> 01:56:32.480]   There's this kind of feeling like that's a real media enterprise.
[01:56:32.480 --> 01:56:35.440]   Yeah.
[01:56:35.440 --> 01:56:39.360]   Well, I wrote on the web for years and thought I wasn't really a writer until I published a book.
[01:56:39.360 --> 01:56:40.160]   Yeah, stop that.
[01:56:40.160 --> 01:56:41.280]   And then I watched the book and was like,
[01:56:41.280 --> 01:56:41.520]   this...
[01:56:41.520 --> 01:56:43.920]   I didn't hear from Abak from any reader of the book.
[01:56:43.920 --> 01:56:44.320]   Right.
[01:56:44.320 --> 01:56:44.880]   Like I still...
[01:56:44.880 --> 01:56:45.360]   Right.
[01:56:45.360 --> 01:56:48.720]   You know, I reached so many more people and had so many better conversations on the web.
[01:56:48.720 --> 01:56:50.480]   Like it was stupid of me to romanticize print.
[01:56:51.600 --> 01:56:52.720]   We do that.
[01:56:52.720 --> 01:56:54.080]   We're in this transitional period.
[01:56:54.080 --> 01:56:59.120]   What's funny is that Yahoo has...
[01:56:59.120 --> 01:57:02.080]   I mean, I think Yahoo probably could do better by...
[01:57:02.080 --> 01:57:03.200]   Certainly could appeal to a young...
[01:57:03.200 --> 01:57:05.600]   You know, you want to make money now.
[01:57:05.600 --> 01:57:09.200]   The audience to go after is the people who are watching only the web,
[01:57:09.200 --> 01:57:13.920]   the 18 to 25 year olds, and every advertiser is desperate to get a hold of them.
[01:57:13.920 --> 01:57:15.280]   You know, hire Katie Couric.
[01:57:15.280 --> 01:57:16.800]   Yeah.
[01:57:16.800 --> 01:57:18.800]   Or David Pogue.
[01:57:18.800 --> 01:57:19.760]   Or David Pogue.
[01:57:19.760 --> 01:57:20.640]   You're coming on us.
[01:57:20.640 --> 01:57:24.000]   You cultivate a team of young identifiable people.
[01:57:24.000 --> 01:57:27.920]   But then the other thing that companies like this do is go to YouTube and say,
[01:57:27.920 --> 01:57:32.000]   I'll fill to Franco or whoever's hot, putty pie, whoever's hot on YouTube.
[01:57:32.000 --> 01:57:32.960]   That's the next...
[01:57:32.960 --> 01:57:36.480]   And I think you just have to grow organically myself, but...
[01:57:36.480 --> 01:57:38.240]   I don't know.
[01:57:38.240 --> 01:57:40.080]   But somebody like Howard is a personal brand.
[01:57:40.080 --> 01:57:42.720]   You know, the whole thing nowadays is the personal brand, right?
[01:57:42.720 --> 01:57:43.920]   He's got the brand.
[01:57:43.920 --> 01:57:45.440]   He should do it himself, he doesn't he?
[01:57:45.440 --> 01:57:46.000]   Anybody?
[01:57:46.000 --> 01:57:47.200]   If he doesn't want to...
[01:57:47.200 --> 01:57:49.200]   If he doesn't want to do it on his own...
[01:57:50.080 --> 01:57:51.440]   Yeah, Google be a good partner.
[01:57:51.440 --> 01:57:56.880]   The problem with Google or anybody like that, or Yahoo for this matter,
[01:57:56.880 --> 01:58:02.720]   there's not a real coincidence of Howard's goals with these companies' goals.
[01:58:02.720 --> 01:58:03.840]   They have different goals.
[01:58:03.840 --> 01:58:08.800]   But Howard strikes me as someone who just wants to do radio.
[01:58:08.800 --> 01:58:09.680]   He doesn't want to run a business.
[01:58:09.680 --> 01:58:10.640]   He doesn't want to think about it.
[01:58:10.640 --> 01:58:11.120]   So do I.
[01:58:11.120 --> 01:58:11.840]   He just hires people...
[01:58:11.840 --> 01:58:13.120]   I have why he's on...
[01:58:13.120 --> 01:58:15.760]   Uh, Maris is going to tell him because he says that it's great.
[01:58:15.760 --> 01:58:16.800]   He's not actually in charge of it.
[01:58:16.800 --> 01:58:17.920]   He just goes in and does what he's told.
[01:58:17.920 --> 01:58:18.480]   He's just telling.
[01:58:18.480 --> 01:58:19.040]   He likes that.
[01:58:19.600 --> 01:58:20.400]   Well, you couldn't put...
[01:58:20.400 --> 01:58:21.440]   I mean, I'm the same way.
[01:58:21.440 --> 01:58:23.600]   I don't want to do any business.
[01:58:23.600 --> 01:58:24.880]   I just hire people to do that.
[01:58:24.880 --> 01:58:28.960]   But you make a lot of business decisions, Leo.
[01:58:28.960 --> 01:58:29.520]   I mean...
[01:58:29.520 --> 01:58:31.040]   But I only make creative decisions.
[01:58:31.040 --> 01:58:32.480]   I don't make business decisions.
[01:58:32.480 --> 01:58:33.280]   Really?
[01:58:33.280 --> 01:58:34.400]   Well, I try not to make business.
[01:58:34.400 --> 01:58:35.520]   Yeah, I mean, I'm a little bit involved.
[01:58:35.520 --> 01:58:36.640]   But I mean, really, Lisa,
[01:58:36.640 --> 01:58:39.040]   is the business size...
[01:58:39.040 --> 01:58:41.040]   I only care about the creative decisions,
[01:58:41.040 --> 01:58:42.240]   the content, stuff like that.
[01:58:42.240 --> 01:58:43.520]   Yeah, I mean, also,
[01:58:43.520 --> 01:58:46.000]   I'm sure Howard makes gazillions of dollars a serious.
[01:58:46.000 --> 01:58:47.920]   If they paid him half a billion dollars,
[01:58:47.920 --> 01:58:49.040]   I would have taken the job.
[01:58:49.760 --> 01:58:50.240]   Yeah.
[01:58:50.240 --> 01:58:52.560]   I mean, for just start on your own thing,
[01:58:52.560 --> 01:58:53.200]   I just take the stage...
[01:58:53.200 --> 01:58:54.960]   Well, I suspect they paid a lot of a serious stock
[01:58:54.960 --> 01:58:56.160]   and there's a holder of serious stock.
[01:58:56.160 --> 01:58:57.040]   And whoops.
[01:58:57.040 --> 01:58:58.720]   Whoops.
[01:58:58.720 --> 01:59:00.160]   I mean...
[01:59:00.160 --> 01:59:02.480]   That's a big payday that he got from them.
[01:59:02.480 --> 01:59:05.040]   At least a theoretically big payday.
[01:59:05.040 --> 01:59:06.240]   So...
[01:59:06.240 --> 01:59:07.040]   I was...
[01:59:07.040 --> 01:59:09.440]   When I spoke to Bear Wolfman in Berlin,
[01:59:09.440 --> 01:59:11.200]   on my last trip,
[01:59:11.200 --> 01:59:13.520]   executive from Fremantle was there,
[01:59:13.520 --> 01:59:15.280]   Fremantle with all the reality TV shows.
[01:59:15.280 --> 01:59:17.440]   And I don't think I'm going any way,
[01:59:17.440 --> 01:59:18.240]   because it was...
[01:59:18.240 --> 01:59:20.400]   Tweeted like crazy,
[01:59:20.400 --> 01:59:20.880]   but he said,
[01:59:20.880 --> 01:59:23.200]   and he put up an iPhone and he said,
[01:59:23.200 --> 01:59:25.840]   "What really scares them is the lone creator,
[01:59:25.840 --> 01:59:26.640]   who's creating stuff."
[01:59:26.640 --> 01:59:28.800]   That should scare them enough.
[01:59:28.800 --> 01:59:30.320]   That should scare the smarter them.
[01:59:30.320 --> 01:59:32.000]   That's the world we're entering, right?
[01:59:32.000 --> 01:59:32.320]   Yeah.
[01:59:32.320 --> 01:59:35.040]   I'm struggling with that,
[01:59:35.040 --> 01:59:37.680]   because I don't want Twit to be the Leo Laporte network.
[01:59:37.680 --> 01:59:38.880]   I would like it to be bigger than that.
[01:59:38.880 --> 01:59:40.880]   I mean, it's kind of built on my brand,
[01:59:40.880 --> 01:59:42.560]   but maybe it is.
[01:59:42.560 --> 01:59:43.360]   Maybe that's all it will be.
[01:59:43.360 --> 01:59:43.840]   I don't know.
[01:59:43.840 --> 01:59:45.680]   I don't know.
[01:59:45.680 --> 01:59:47.360]   I would love to make something that...
[01:59:47.360 --> 01:59:50.400]   You know, I'm thinking my model is more like United Artists.
[01:59:50.400 --> 01:59:52.240]   Remember when Charlie Chaplin
[01:59:52.240 --> 01:59:54.560]   and Douglas Fairbanks and Mary Pickford
[01:59:54.560 --> 01:59:56.240]   said, "I don't want to work in the studio system anymore.
[01:59:56.240 --> 01:59:59.200]   Why don't we form our own movie company, United Artists?"
[01:59:59.200 --> 02:00:01.280]   Because we have a brand.
[02:00:01.280 --> 02:00:03.360]   That's kind of...
[02:00:03.360 --> 02:00:06.640]   I don't know how it worked out for them,
[02:00:06.640 --> 02:00:08.240]   but I think it worked out all right.
[02:00:15.200 --> 02:00:16.240]   It's going to have some turkey tomorrow.
[02:00:16.240 --> 02:00:17.920]   No.
[02:00:17.920 --> 02:00:20.960]   Do you do as many Italians do?
[02:00:20.960 --> 02:00:22.720]   Do you have a turkey and pasta?
[02:00:22.720 --> 02:00:24.880]   Well, yeah.
[02:00:24.880 --> 02:00:27.440]   My family's dinner back on the East Coast.
[02:00:27.440 --> 02:00:30.560]   The turkey is the main dish, but there's always pasta.
[02:00:30.560 --> 02:00:31.120]   Yeah.
[02:00:31.120 --> 02:00:32.160]   There's pasta.
[02:00:32.160 --> 02:00:32.160]   There's pasta.
[02:00:32.160 --> 02:00:32.960]   With big meatballs?
[02:00:32.960 --> 02:00:34.000]   Yeah, yeah.
[02:00:34.000 --> 02:00:34.880]   There's pasta and meatballs.
[02:00:34.880 --> 02:00:36.080]   At every meal.
[02:00:36.080 --> 02:00:37.760]   But no, turkey is the...
[02:00:37.760 --> 02:00:38.880]   Every meal?
[02:00:38.880 --> 02:00:42.320]   At every big celebration meal,
[02:00:42.320 --> 02:00:44.000]   there's always some sort of pasta and meatballs.
[02:00:44.960 --> 02:00:45.760]   Damn.
[02:00:45.760 --> 02:00:46.800]   I want to be in your family.
[02:00:46.800 --> 02:00:47.920]   The Jersey Leo.
[02:00:47.920 --> 02:00:50.320]   If you have a restaurant, it has to serve pasta.
[02:00:50.320 --> 02:00:51.280]   I love that.
[02:00:51.280 --> 02:00:51.440]   Yeah.
[02:00:51.440 --> 02:00:52.720]   Love that.
[02:00:52.720 --> 02:00:53.600]   You got to have pasta.
[02:00:53.600 --> 02:00:54.480]   You got to have gravy.
[02:00:54.480 --> 02:00:56.560]   You got to have meatballs.
[02:00:56.560 --> 02:00:59.840]   You got to have a cheese roll stuffed with an egg.
[02:00:59.840 --> 02:01:01.440]   Oh, man.
[02:01:01.440 --> 02:01:02.800]   That cheese roll food.
[02:01:02.800 --> 02:01:04.240]   That cheese roll food.
[02:01:04.240 --> 02:01:07.120]   Our show today brought to you by Shutterstock.com.
[02:01:07.120 --> 02:01:07.760]   Are you?
[02:01:07.760 --> 02:01:09.600]   If you're in a creative enterprise,
[02:01:09.600 --> 02:01:11.840]   maybe you're starting on a new show with Katie Couric.
[02:01:11.840 --> 02:01:15.600]   And you realize that creative, you need images,
[02:01:15.600 --> 02:01:17.920]   you need vector graphics,
[02:01:17.920 --> 02:01:21.600]   you need something to make it gorgeous.
[02:01:21.600 --> 02:01:26.640]   Shutterstock has over 30 million stock photos,
[02:01:26.640 --> 02:01:28.800]   illustrations, vectors and videos.
[02:01:28.800 --> 02:01:30.320]   They always have the number down at the bottom here.
[02:01:30.320 --> 02:01:36.240]   31,380,089 with 205,000 new images this week alone.
[02:01:36.240 --> 02:01:39.440]   The images are fresh.
[02:01:39.440 --> 02:01:42.000]   They are high quality, often from professionals.
[02:01:42.000 --> 02:01:45.760]   Individually reviewed, cleared for use.
[02:01:45.760 --> 02:01:47.840]   And backed by a legal guarantee,
[02:01:47.840 --> 02:01:49.840]   they are royalty free.
[02:01:49.840 --> 02:01:52.240]   You can use them in anything you're up to.
[02:01:52.240 --> 02:01:53.280]   And look at the footage page.
[02:01:53.280 --> 02:01:55.440]   I think people go to the front page
[02:01:55.440 --> 02:01:57.520]   and they sometimes forget to click this footage tab,
[02:01:57.520 --> 02:01:58.880]   footage.shutterstock.com.
[02:01:58.880 --> 02:02:00.400]   Great video as well.
[02:02:00.400 --> 02:02:03.600]   1.4 million royalty free stock videos.
[02:02:03.600 --> 02:02:04.480]   If you create an account,
[02:02:04.480 --> 02:02:06.640]   they're free videos and images every single week.
[02:02:07.600 --> 02:02:09.360]   You can download any image in any size,
[02:02:09.360 --> 02:02:13.440]   pay only one price or choose individual image packs
[02:02:13.440 --> 02:02:15.120]   or do what we do for the best deal.
[02:02:15.120 --> 02:02:16.480]   You get a monthly subscription.
[02:02:16.480 --> 02:02:20.000]   The standard subscription is when we have 25 images a day.
[02:02:20.000 --> 02:02:21.200]   Great for a publication.
[02:02:21.200 --> 02:02:23.440]   Really fantastic.
[02:02:23.440 --> 02:02:25.040]   The search tool is one of the things
[02:02:25.040 --> 02:02:26.320]   that makes Shutterstock so great
[02:02:26.320 --> 02:02:30.240]   because you can find things not only by the noun,
[02:02:30.240 --> 02:02:35.680]   the dancer or the Santa Claus or a turkey,
[02:02:35.680 --> 02:02:37.680]   but you can also add adjectives.
[02:02:37.680 --> 02:02:40.240]   I don't know. I could say angry turkey.
[02:02:40.240 --> 02:02:42.080]   Would you think I'll find anything for angry turkey?
[02:02:42.080 --> 02:02:44.400]   You think they with 30 million images,
[02:02:44.400 --> 02:02:45.840]   there'll be a few angry.
[02:02:45.840 --> 02:02:47.840]   Oh, these turkeys are mad.
[02:02:47.840 --> 02:02:50.080]   That turkey says back off, buster.
[02:02:50.080 --> 02:02:51.200]   That is one.
[02:02:51.200 --> 02:02:54.560]   Who would have thought they'd have angry turkeys?
[02:02:54.560 --> 02:02:56.240]   They have.
[02:02:56.240 --> 02:02:58.720]   The turkeys have a right to be angry.
[02:02:58.720 --> 02:02:59.840]   I'd be angry too.
[02:02:59.840 --> 02:03:01.760]   Is that a turkey or a buzzer?
[02:03:01.760 --> 02:03:05.360]   Here's a turkey that is not so happy.
[02:03:05.360 --> 02:03:05.920]   Look at that.
[02:03:05.920 --> 02:03:08.560]   That's not a turkey.
[02:03:08.560 --> 02:03:09.360]   That's a duck.
[02:03:09.360 --> 02:03:13.600]   Anyway, when the other thing I like about Shutterstock
[02:03:13.600 --> 02:03:15.360]   is you can also narrow it down by color.
[02:03:15.360 --> 02:03:16.720]   If you look at their color wheel,
[02:03:16.720 --> 02:03:20.000]   you say you've got a webpage that has a certain
[02:03:20.000 --> 02:03:20.400]   palette.
[02:03:20.400 --> 02:03:22.000]   You can choose the palette you want.
[02:03:22.000 --> 02:03:26.480]   I also love the light boxes that you can take images
[02:03:26.480 --> 02:03:29.760]   and save them for later use for inspiration for ideas.
[02:03:29.760 --> 02:03:31.120]   Because it's often the case when you're browsing
[02:03:31.120 --> 02:03:32.880]   Shutterstock, you look at for angry turkeys
[02:03:32.880 --> 02:03:34.240]   and you find something like this.
[02:03:35.040 --> 02:03:35.920]   He's a turkey.
[02:03:35.920 --> 02:03:41.280]   And say, you know, later when I want to do a blog post on angry
[02:03:41.280 --> 02:03:43.840]   young man, I could save, I could use that.
[02:03:43.840 --> 02:03:46.240]   Everybody knows when you're blogging,
[02:03:46.240 --> 02:03:49.200]   when you're creating content of any kind,
[02:03:49.200 --> 02:03:50.880]   having an image just brings it to life.
[02:03:50.880 --> 02:03:52.400]   And Shutterstock is a great way to do it.
[02:03:52.400 --> 02:03:57.760]   I don't know what's going on there to do it.
[02:03:57.760 --> 02:04:03.280]   Search for angry turkeys and search for anything else
[02:04:03.280 --> 02:04:04.320]   and have fun.
[02:04:04.320 --> 02:04:06.240]   Make the free account, create the light boxes.
[02:04:06.240 --> 02:04:07.120]   You can share them with friends.
[02:04:07.120 --> 02:04:10.720]   And if you decide to buy 25% off when you use our offer code
[02:04:10.720 --> 02:04:14.720]   twig 1113 twig twig 1113.
[02:04:14.720 --> 02:04:18.560]   It's our your way of letting them know you heard about it on
[02:04:18.560 --> 02:04:21.200]   this week in Google and we appreciate that.
[02:04:21.200 --> 02:04:26.400]   In return, we'll give you 25% off twig 1113.
[02:04:26.400 --> 02:04:31.200]   It is time for Jin Trapani's tip of the week.
[02:04:32.480 --> 02:04:35.680]   So I kind of ripped this one off from all about Android,
[02:04:35.680 --> 02:04:36.880]   but I've been digging it so much.
[02:04:36.880 --> 02:04:40.320]   I showed you earlier, I got KitKat on the HSC1,
[02:04:40.320 --> 02:04:41.920]   which is the Google Play Edition.
[02:04:41.920 --> 02:04:43.920]   And it comes with the stock Android launcher,
[02:04:43.920 --> 02:04:45.920]   but I wanted the Google Experience launcher,
[02:04:45.920 --> 02:04:48.480]   which is the launcher that you get on the Nexus 5.
[02:04:48.480 --> 02:04:51.120]   And it's that you can't get that through the Play Store.
[02:04:51.120 --> 02:04:56.240]   I assume yet, but you can get it if you sideload an APK,
[02:04:56.240 --> 02:05:00.160]   which is a fan droid and that's where the pH fan droid has posted.
[02:05:00.160 --> 02:05:03.040]   So if you Google fan droid, Google Experience launcher,
[02:05:03.040 --> 02:05:06.240]   you can download the APK and you can change your launcher to,
[02:05:06.240 --> 02:05:08.240]   and of course, you can change it back if you want
[02:05:08.240 --> 02:05:09.600]   to the Google Experience launcher,
[02:05:09.600 --> 02:05:12.560]   which gives you that slide to the left to get Google now.
[02:05:12.560 --> 02:05:21.440]   And the translucent app drawer and the tap and hold to get the different home screens
[02:05:21.440 --> 02:05:22.560]   and to get to widgets.
[02:05:22.560 --> 02:05:23.760]   And it's really, really nice.
[02:05:23.760 --> 02:05:25.840]   I have to say, I was saying before in pre-show,
[02:05:25.840 --> 02:05:26.720]   I really like it.
[02:05:26.720 --> 02:05:28.800]   The Google Experience launcher is really clean and pretty
[02:05:28.800 --> 02:05:32.560]   and gives you that translucency and just makes your phone feel really new with KitKat.
[02:05:32.560 --> 02:05:35.120]   I feel like it's like, oh, now I feel like I really have KitKat
[02:05:35.120 --> 02:05:36.640]   because I've got the Google Experience launcher.
[02:05:36.640 --> 02:05:38.000]   So yeah, check that out.
[02:05:38.000 --> 02:05:42.960]   If you've got KitKat and I think you might be able to even install it on Jelly Bean
[02:05:42.960 --> 02:05:44.000]   if I'm not--
[02:05:44.000 --> 02:05:45.440]   Oh, that's interesting.
[02:05:45.440 --> 02:05:48.400]   Yeah, yeah, if I'm not incorrect.
[02:05:48.400 --> 02:05:51.280]   But yeah, check it out, particularly if you have gotten KitKat.
[02:05:51.280 --> 02:05:52.640]   It works on the Nexus 7 as well.
[02:05:52.640 --> 02:05:53.600]   I got it on my first gen.
[02:05:53.600 --> 02:05:55.520]   I got KitKat on my first gen Nexus 7
[02:05:55.520 --> 02:05:58.000]   and also switched it over to the Google Experience launcher.
[02:05:58.000 --> 02:05:59.600]   It looked really nice.
[02:05:59.600 --> 02:06:02.000]   So that doesn't happen automatically when you get KitKat.
[02:06:02.000 --> 02:06:03.280]   You have to choose the new launcher.
[02:06:03.280 --> 02:06:04.240]   That's right.
[02:06:04.240 --> 02:06:04.800]   Okay.
[02:06:04.800 --> 02:06:05.280]   Right.
[02:06:05.280 --> 02:06:09.440]   Well, so when you get KitKat, you get the Google Search update.
[02:06:09.440 --> 02:06:10.960]   But you still don't have the new launcher.
[02:06:10.960 --> 02:06:12.240]   The launcher isn't an option.
[02:06:12.240 --> 02:06:13.760]   It's just the vanilla Android launcher.
[02:06:13.760 --> 02:06:16.640]   So you have to go get that APK, sideload that APK,
[02:06:16.640 --> 02:06:18.640]   and then you get access to the new launcher.
[02:06:18.640 --> 02:06:22.400]   So the launcher only shipped with the Nexus 5.
[02:06:22.400 --> 02:06:28.480]   So have you with KitKat switched over to the new runtime?
[02:06:28.480 --> 02:06:29.920]   No, I didn't.
[02:06:29.920 --> 02:06:33.440]   Not after the horror stories I heard from--
[02:06:33.440 --> 02:06:35.760]   or the horror story I heard from Ron.
[02:06:35.760 --> 02:06:37.120]   I didn't switch over.
[02:06:37.120 --> 02:06:38.240]   You switched over, though.
[02:06:38.240 --> 02:06:38.640]   I did.
[02:06:38.640 --> 02:06:43.120]   I have had no crashes, no problems, no program didn't work.
[02:06:43.120 --> 02:06:46.000]   And was it a noticeable speed up?
[02:06:46.000 --> 02:06:46.720]   I don't know.
[02:06:46.720 --> 02:06:47.760]   It's hard to tell, right?
[02:06:47.760 --> 02:06:48.320]   It's hard to tell.
[02:06:48.320 --> 02:06:49.200]   Unless you're doing a side by side.
[02:06:49.200 --> 02:06:49.680]   Yeah.
[02:06:49.680 --> 02:06:51.600]   You have to turn on developer options
[02:06:51.600 --> 02:06:54.400]   and then you'll see Select Runtime on KitKat.
[02:06:54.400 --> 02:06:56.400]   I did it on my Nexus 5.
[02:06:56.400 --> 02:06:58.640]   I've done it on my Moto X.
[02:06:58.640 --> 02:07:01.280]   Now that my Nexus 7, my 2013 Nexus 7,
[02:07:01.280 --> 02:07:02.160]   just got KitKat.
[02:07:02.160 --> 02:07:03.360]   I'll try it on there, too.
[02:07:03.360 --> 02:07:04.720]   It takes a while because you have to--
[02:07:04.720 --> 02:07:06.320]   it's like installing a new operating system
[02:07:06.320 --> 02:07:08.240]   has to run through all the apps and update them.
[02:07:08.240 --> 02:07:10.000]   But instead of Dalvik,
[02:07:10.000 --> 02:07:14.640]   which is the well-known Java runtime for Android,
[02:07:14.640 --> 02:07:17.360]   you select Art Android Runtime.
[02:07:17.360 --> 02:07:20.400]   And this, everybody thinks, was being created by Google
[02:07:20.400 --> 02:07:23.520]   in case the lawsuit from Oracle went south.
[02:07:23.520 --> 02:07:24.080]   They won.
[02:07:24.080 --> 02:07:24.720]   They prevailed.
[02:07:24.720 --> 02:07:26.320]   But there's still a new runtime.
[02:07:26.320 --> 02:07:28.960]   And I don't know if this is going to be the new runtime later
[02:07:28.960 --> 02:07:29.680]   or what.
[02:07:29.680 --> 02:07:32.880]   But yeah, I mean, that's certainly something
[02:07:32.880 --> 02:07:36.800]   not to do if you value your functionality.
[02:07:36.800 --> 02:07:38.880]   But I haven't had any problems at all.
[02:07:38.880 --> 02:07:40.320]   I'm now a developer.
[02:07:40.320 --> 02:07:41.280]   Android tells me.
[02:07:41.280 --> 02:07:41.920]   I've tapped a little--
[02:07:41.920 --> 02:07:42.960]   You've tapped seven times.
[02:07:42.960 --> 02:07:43.920]   Yep.
[02:07:43.920 --> 02:07:44.960]   Yeah, I haven't--
[02:07:44.960 --> 02:07:46.960]   there's not one app that has it where I think Bronk just
[02:07:46.960 --> 02:07:48.400]   had a bad experience.
[02:07:48.400 --> 02:07:48.880]   He had--
[02:07:48.880 --> 02:07:49.600]   It was WhatsApp.
[02:07:49.600 --> 02:07:49.920]   Yeah.
[02:07:49.920 --> 02:07:50.560]   WhatsApp, I think.
[02:07:50.560 --> 02:07:51.600]   I don't use WhatsApp.
[02:07:51.600 --> 02:07:53.440]   But so I should try it and see.
[02:07:53.440 --> 02:07:56.720]   But everything else I use all works fine.
[02:07:56.720 --> 02:07:58.720]   And I think a little bit better battery life.
[02:07:58.720 --> 02:08:00.320]   That's what I'm looking for.
[02:08:00.320 --> 02:08:01.120]   Right, right.
[02:08:01.120 --> 02:08:01.680]   Mm-hmm.
[02:08:01.680 --> 02:08:03.120]   Approved battery options.
[02:08:03.120 --> 02:08:04.080]   Yeah.
[02:08:04.080 --> 02:08:06.320]   So the Google Experience Launcher you can get if you
[02:08:06.320 --> 02:08:07.440]   know where to get it.
[02:08:07.440 --> 02:08:08.080]   Yeah.
[02:08:08.080 --> 02:08:12.000]   If you download the APK, when you tap the home button,
[02:08:12.000 --> 02:08:13.040]   it'll prompt you.
[02:08:13.040 --> 02:08:14.000]   Hey, you want this launcher.
[02:08:14.000 --> 02:08:15.280]   Just like any other launcher.
[02:08:15.280 --> 02:08:16.400]   Yep, exactly.
[02:08:16.400 --> 02:08:18.400]   And I'll be really surprised if we don't see this
[02:08:18.400 --> 02:08:20.240]   in the Play Store soon.
[02:08:20.240 --> 02:08:20.880]   Yeah.
[02:08:20.880 --> 02:08:22.400]   Seems like they're moving that way.
[02:08:22.400 --> 02:08:22.800]   Yeah.
[02:08:22.800 --> 02:08:23.840]   They did it with a keyboard.
[02:08:23.840 --> 02:08:26.240]   They did it with Google Print, Cloud Print.
[02:08:26.240 --> 02:08:29.040]   I mean, they're slowly putting all of those little
[02:08:29.040 --> 02:08:31.360]   Google apps out there.
[02:08:31.360 --> 02:08:32.000]   Which makes sense.
[02:08:32.000 --> 02:08:32.480]   Exactly.
[02:08:32.480 --> 02:08:32.720]   Yeah.
[02:08:32.720 --> 02:08:32.880]   Yeah.
[02:08:32.880 --> 02:08:33.360]   Yeah.
[02:08:33.360 --> 02:08:33.840]   It's true.
[02:08:33.840 --> 02:08:36.480]   Jeff Jarvis, your number of the week.
[02:08:36.480 --> 02:08:37.840]   Uh, I'm cheating too.
[02:08:37.840 --> 02:08:38.640]   I didn't really have one.
[02:08:38.640 --> 02:08:42.080]   So I put up a link to three new Nexus 5 commercials.
[02:08:42.080 --> 02:08:43.280]   Figuring three and five.
[02:08:43.280 --> 02:08:43.760]   That's number.
[02:08:43.760 --> 02:08:44.720]   So, gee, don't do this.
[02:08:44.720 --> 02:08:45.680]   You can put any one of them.
[02:08:45.680 --> 02:08:46.320]   They're very short.
[02:08:46.320 --> 02:08:47.120]   They're 16 seconds.
[02:08:47.120 --> 02:08:47.840]   You can play one.
[02:08:47.840 --> 02:08:49.840]   I'll play a commercial.
[02:08:49.840 --> 02:08:51.120]   I like playing commercials.
[02:08:51.120 --> 02:08:52.880]   They add production value to the show.
[02:08:52.880 --> 02:08:54.080]   We did it.
[02:08:54.080 --> 02:08:54.800]   We did it.
[02:08:54.800 --> 02:08:56.000]   That's amazing.
[02:08:56.000 --> 02:08:57.600]   They're doing a photosphere.
[02:08:57.600 --> 02:08:58.960]   Champion on the rock.
[02:08:58.960 --> 02:09:01.040]   And the champion on the rock.
[02:09:01.040 --> 02:09:01.600]   That's neat.
[02:09:01.600 --> 02:09:03.120]   Let me hear your kind of beat.
[02:09:03.120 --> 02:09:04.400]   Good ad for photosphere.
[02:09:04.400 --> 02:09:05.280]   Yeah.
[02:09:05.280 --> 02:09:05.440]   All right.
[02:09:05.440 --> 02:09:06.240]   Here's another one.
[02:09:06.240 --> 02:09:07.600]   This is a bunch of people.
[02:09:07.600 --> 02:09:09.200]   Family on the beach.
[02:09:09.200 --> 02:09:15.120]   [CHEERING]
[02:09:15.120 --> 02:09:15.840]   Cute family.
[02:09:16.640 --> 02:09:17.840]   Fix the photo.
[02:09:17.840 --> 02:09:18.880]   [LAUGHTER]
[02:09:18.880 --> 02:09:20.000]   Oh, I didn't even notice.
[02:09:20.000 --> 02:09:21.760]   There was something wrong with it originally.
[02:09:21.760 --> 02:09:23.760]   HDR because the sun's behind them.
[02:09:23.760 --> 02:09:24.720]   Yes.
[02:09:24.720 --> 02:09:26.080]   And so they had HDR.
[02:09:26.080 --> 02:09:26.560]   OK.
[02:09:26.560 --> 02:09:28.000]   I like the Nexus 5, frankly.
[02:09:28.000 --> 02:09:29.440]   I think it's a great phone.
[02:09:29.440 --> 02:09:30.800]   And people complain about the camera.
[02:09:30.800 --> 02:09:31.680]   I like it.
[02:09:31.680 --> 02:09:32.880]   Oh, let's go to a wedding.
[02:09:32.880 --> 02:09:33.680]   That's what's next.
[02:09:33.680 --> 02:09:36.000]   [MUSIC PLAYING]
[02:09:36.000 --> 02:09:36.960]   Grandma Disco.
[02:09:36.960 --> 02:09:38.240]   And she's doing the robot.
[02:09:38.240 --> 02:09:41.200]   Which was big in Grandma's time.
[02:09:41.200 --> 02:09:44.800]   And then it auto-awesome duds.
[02:09:44.800 --> 02:09:45.800]   It's--
[02:09:45.800 --> 02:09:47.760]   [CHEERING]
[02:09:47.760 --> 02:09:48.800]   Love auto-awesome.
[02:09:48.800 --> 02:09:50.800]   All right.
[02:09:50.800 --> 02:09:51.280]   There you go.
[02:09:51.280 --> 02:09:52.160]   Take that, Apple.
[02:09:52.160 --> 02:09:53.440]   [LAUGHTER]
[02:09:53.440 --> 02:09:54.400]   I think Kevin--
[02:09:54.400 --> 02:09:56.400]   Kevin Marks dropped something for us in the--
[02:09:56.400 --> 02:09:57.600]   I saw that.
[02:09:57.600 --> 02:09:58.720]   --the run down.
[02:09:58.720 --> 02:10:00.640]   He has access to the run downs.
[02:10:00.640 --> 02:10:03.120]   He says he got the Nexus 5 bumper,
[02:10:03.120 --> 02:10:05.360]   which is actually misnamed
[02:10:05.360 --> 02:10:07.200]   because it is a full case, right?
[02:10:07.200 --> 02:10:07.680]   It's a problem.
[02:10:07.680 --> 02:10:08.160]   It's a case, yeah.
[02:10:08.160 --> 02:10:10.880]   So let's look at the video on Kevin Marks, Google+,
[02:10:10.880 --> 02:10:14.560]   he says, "It makes it more likely for me to drop
[02:10:14.560 --> 02:10:16.160]   what the--
[02:10:16.160 --> 02:10:18.320]   if slides right off, it's slippery.
[02:10:18.320 --> 02:10:20.400]   It's--
[02:10:20.400 --> 02:10:21.280]   don't buy it."
[02:10:21.280 --> 02:10:22.960]   It makes him--
[02:10:22.960 --> 02:10:24.480]   you know, I like how he has a towel,
[02:10:24.480 --> 02:10:25.520]   just in case it falls.
[02:10:25.520 --> 02:10:26.000]   He doesn't want to.
[02:10:26.000 --> 02:10:28.320]   So he's asking people,
[02:10:28.320 --> 02:10:30.240]   "How do you add friction
[02:10:30.240 --> 02:10:32.720]   to the official Nexus 5 bumper?"
[02:10:32.720 --> 02:10:35.040]   I don't put my phones in cases.
[02:10:35.040 --> 02:10:36.800]   I already there.
[02:10:36.800 --> 02:10:39.360]   Yeah, I stopped as well.
[02:10:39.360 --> 02:10:40.480]   I like to bear back it.
[02:10:40.480 --> 02:10:42.480]   [LAUGHTER]
[02:10:42.480 --> 02:10:44.480]   [LAUGHTER]
[02:10:44.480 --> 02:10:49.920]   I don't really have a tool or anything.
[02:10:49.920 --> 02:10:50.320]   I don't know.
[02:10:50.320 --> 02:10:51.760]   I was lazy enough and came up.
[02:10:51.760 --> 02:10:53.200]   It's cool.
[02:10:53.200 --> 02:10:54.000]   It's Thanksgiving.
[02:10:54.000 --> 02:10:54.560]   It's a problem.
[02:10:54.560 --> 02:10:56.960]   No one has come up with the phone condom.
[02:10:56.960 --> 02:10:57.840]   Now that you mentioned that.
[02:10:57.840 --> 02:11:00.080]   Apple did the phone socks.
[02:11:00.080 --> 02:11:03.680]   Phone socks.
[02:11:03.680 --> 02:11:05.040]   They have--
[02:11:05.040 --> 02:11:06.320]   I--
[02:11:06.320 --> 02:11:08.080]   maybe they're iPod socks.
[02:11:08.080 --> 02:11:09.200]   They weren't very practical.
[02:11:09.200 --> 02:11:10.640]   Oh, right.
[02:11:10.640 --> 02:11:12.160]   The rubber thing that pulled over
[02:11:12.160 --> 02:11:12.800]   and--
[02:11:12.800 --> 02:11:12.960]   Rubber.
[02:11:12.960 --> 02:11:13.760]   They were like socks.
[02:11:13.760 --> 02:11:14.320]   They were knitted.
[02:11:14.320 --> 02:11:14.880]   Were socks.
[02:11:14.880 --> 02:11:15.760]   They were knitted socks.
[02:11:15.760 --> 02:11:16.880]   It's a terrible idea.
[02:11:16.880 --> 02:11:17.360]   It's a terrible idea.
[02:11:17.360 --> 02:11:19.760]   And they had an event in which they announced them.
[02:11:19.760 --> 02:11:24.000]   So these things happen.
[02:11:24.000 --> 02:11:25.440]   If you have not taken our survey,
[02:11:25.440 --> 02:11:26.240]   we'd love you to.
[02:11:26.240 --> 02:11:27.840]   If you go to YouTube--
[02:11:27.840 --> 02:11:28.320]   I'm sorry.
[02:11:28.320 --> 02:11:31.200]   Why is there a YouTube link there?
[02:11:31.200 --> 02:11:34.240]   If you go to twit.tv/survey,
[02:11:34.240 --> 02:11:35.760]   it will help us better understand you,
[02:11:35.760 --> 02:11:36.720]   your interests, your needs.
[02:11:36.720 --> 02:11:37.680]   I know it's a little long,
[02:11:37.680 --> 02:11:38.880]   but we really appreciate
[02:11:38.880 --> 02:11:40.560]   all the people have taken it.
[02:11:40.560 --> 02:11:41.440]   More than--
[02:11:41.440 --> 02:11:41.920]   what is it?
[02:11:41.920 --> 02:11:44.480]   The last count was like almost 20,000 people have taken it now,
[02:11:44.480 --> 02:11:45.280]   17 something.
[02:11:45.280 --> 02:11:47.920]   So let's put it over the top, if you would.
[02:11:47.920 --> 02:11:50.640]   And yes, we will in aggregate,
[02:11:50.640 --> 02:11:53.680]   take some of that data and tell our advertisers things like our
[02:11:53.680 --> 02:11:54.640]   audiences.
[02:11:54.640 --> 02:11:56.240]   Average age is 12 years old,
[02:11:56.240 --> 02:11:59.200]   and their average income is about $15 a year.
[02:11:59.200 --> 02:12:00.960]   Something like that.
[02:12:00.960 --> 02:12:04.400]   7-- Glenn says 17,000 plus have taken it already.
[02:12:04.400 --> 02:12:07.200]   Let us--
[02:12:07.200 --> 02:12:09.760]   let us all go to twit.tv/survey.
[02:12:10.400 --> 02:12:11.520]   Hey, have a great Thanksgiving.
[02:12:11.520 --> 02:12:13.520]   Jeff, what are you doing for Thanksgiving?
[02:12:13.520 --> 02:12:14.800]   Macaroni and cheese.
[02:12:14.800 --> 02:12:16.000]   Home with the whole family.
[02:12:16.000 --> 02:12:16.720]   That's a tradition.
[02:12:16.720 --> 02:12:18.000]   You know turkey?
[02:12:18.000 --> 02:12:18.720]   We don't like turkey.
[02:12:18.720 --> 02:12:19.920]   Oh, that's right.
[02:12:19.920 --> 02:12:20.640]   I remember this.
[02:12:20.640 --> 02:12:22.160]   I remember that you don't do turkey.
[02:12:22.160 --> 02:12:22.480]   I like that.
[02:12:22.480 --> 02:12:24.240]   Do you have a special recipe,
[02:12:24.240 --> 02:12:24.960]   macaroni and cheese,
[02:12:24.960 --> 02:12:26.560]   or are you just using the craft out of the box?
[02:12:26.560 --> 02:12:28.560]   Well, actually, a recipe of mine,
[02:12:28.560 --> 02:12:29.840]   actually, my wife's--
[02:12:29.840 --> 02:12:31.680]   I was once asked by the New York Times
[02:12:31.680 --> 02:12:33.120]   for couch potato recipes
[02:12:33.120 --> 02:12:34.720]   many, many, many years ago.
[02:12:34.720 --> 02:12:36.560]   She helped me make the most obnoxious recipe
[02:12:36.560 --> 02:12:37.120]   you could imagine.
[02:12:37.120 --> 02:12:38.160]   I submitted that,
[02:12:38.160 --> 02:12:39.600]   but no, I don't have a recipe.
[02:12:39.600 --> 02:12:41.520]   She makes something that kids like.
[02:12:41.520 --> 02:12:42.720]   It's kind of comfort food.
[02:12:42.720 --> 02:12:44.400]   I make a bechamel.
[02:12:44.400 --> 02:12:47.280]   And then I put some--
[02:12:47.280 --> 02:12:48.640]   you have to--
[02:12:48.640 --> 02:12:50.160]   you know, maybe a little smoked gouda.
[02:12:50.160 --> 02:12:51.280]   Do you have to choose the cheese,
[02:12:51.280 --> 02:12:52.400]   depending on the audience?
[02:12:52.400 --> 02:12:55.120]   Melt that into the bechamel.
[02:12:55.120 --> 02:12:57.440]   And then you cook--
[02:12:57.440 --> 02:12:58.640]   you can cook some macaroni,
[02:12:58.640 --> 02:13:00.640]   or like, I like those little squiggly little--
[02:13:00.640 --> 02:13:02.080]   Rotini.
[02:13:02.080 --> 02:13:02.640]   Rotini.
[02:13:02.640 --> 02:13:03.440]   And then you--
[02:13:03.440 --> 02:13:04.560]   so you cook those ahead of time.
[02:13:04.560 --> 02:13:06.640]   You pour this bechamel over the rotini,
[02:13:06.640 --> 02:13:08.160]   then some panko on the top now.
[02:13:08.160 --> 02:13:09.360]   If you really-- if you're not--
[02:13:09.360 --> 02:13:10.480]   if you go crazy, it's not--
[02:13:10.480 --> 02:13:11.200]   it's on a guy, I know.
[02:13:11.200 --> 02:13:12.960]   But you could also put a little salt pork
[02:13:12.960 --> 02:13:14.880]   or ham in there, a little bacon.
[02:13:14.880 --> 02:13:16.160]   Just give it some flavor.
[02:13:16.160 --> 02:13:16.800]   Then you bake that.
[02:13:16.800 --> 02:13:17.760]   I didn't have lunch today.
[02:13:17.760 --> 02:13:18.640]   And it's dinner time here.
[02:13:18.640 --> 02:13:19.200]   I'm strong.
[02:13:19.200 --> 02:13:22.400]   [laughter]
[02:13:22.400 --> 02:13:23.920]   Wait, we're here this wiggly.
[02:13:23.920 --> 02:13:26.400]   I'm looking this up now,
[02:13:26.400 --> 02:13:28.800]   because you know the rotini really folds on to the cheese.
[02:13:28.800 --> 02:13:29.120]   Anyway.
[02:13:29.120 --> 02:13:32.640]   I love bechamel.
[02:13:32.640 --> 02:13:35.200]   Thank you, everybody,
[02:13:35.200 --> 02:13:36.800]   for being here.
[02:13:36.800 --> 02:13:39.520]   We do twig on Wednesday afternoons, 1 p.m. Pacific,
[02:13:39.520 --> 02:13:43.920]   4 p.m. Eastern time, 2100 UTC on twit.tv.
[02:13:43.920 --> 02:13:44.880]   And if you join us live,
[02:13:44.880 --> 02:13:47.680]   we would love to see your shining face in our chatroom.
[02:13:47.680 --> 02:13:48.720]   But if you can't,
[02:13:48.720 --> 02:13:50.160]   on-demand audio and video,
[02:13:50.160 --> 02:13:53.920]   always available after the fact of twit.tv/twig,
[02:13:53.920 --> 02:13:57.680]   or wherever fine net casts are aggregated and distributed.
[02:13:57.680 --> 02:13:59.520]   We thank you for being here.
[02:13:59.520 --> 02:14:00.400]   And we'll see you next week.
[02:14:00.400 --> 02:14:02.800]   Have a great Thanksgiving if you're in the US.
[02:14:02.800 --> 02:14:04.880]   If not, just-- it'll be nice,
[02:14:04.880 --> 02:14:06.960]   because Americans will be quiet,
[02:14:06.960 --> 02:14:08.960]   except for the burping for a whole day.
[02:14:08.960 --> 02:14:10.560]   We'll see you next time.
[02:14:10.560 --> 02:14:11.680]   On twig!
[02:14:11.680 --> 02:14:21.680]   [music]

