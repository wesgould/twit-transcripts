;FFMETADATA1
title=Podcast Legend Having Lunch
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-08-25
track=678
language=English
genre=Podcast
comment=Twitter Security, Annie Leibovitz, AI Art
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.000]   It's time for Twig this week at Google Ants here, Stacey, Jeff.
[00:00:04.000 --> 00:00:08.000]   Of course, we're going to talk about the Twitter whistleblower.
[00:00:08.000 --> 00:00:10.000]   How credible are his assertions?
[00:00:10.000 --> 00:00:15.000]   Is there something wrong with Annie Leibovitz's pictures of black people?
[00:00:15.000 --> 00:00:20.000]   An image generator will turn me into art.
[00:00:20.000 --> 00:00:23.000]   It's all coming up next on Twig.
[00:00:23.000 --> 00:00:29.000]   Podcasts you love from people you trust.
[00:00:29.000 --> 00:00:31.000]   This is Twig.
[00:00:31.000 --> 00:00:37.000]   This is Twig.
[00:00:37.000 --> 00:00:44.000]   This week in Google, Episode 678, recorded Wednesday, August 24th, 2022.
[00:00:44.000 --> 00:00:47.000]   Podcast legend having lunch.
[00:00:47.000 --> 00:00:51.000]   This week in Google is brought to you by Melissa.
[00:00:51.000 --> 00:00:57.000]   Poor data quality, can cost organizations an average of $15 million every year.
[00:00:57.000 --> 00:01:00.000]   Make sure your customer contact data is up to date.
[00:01:00.000 --> 00:01:07.000]   Get started today with 1000 records cleaned for free at Melissa.com/twit.
[00:01:07.000 --> 00:01:11.000]   And by IRL, an original podcast from Mozilla.
[00:01:11.000 --> 00:01:16.000]   IRL is a show for people who build AI and people who develop tech policies,
[00:01:16.000 --> 00:01:19.000]   hosted by Bridget Todd this season of IRL.
[00:01:19.000 --> 00:01:21.000]   Looks at AI in real life.
[00:01:21.000 --> 00:01:24.000]   Search for IRL in your podcast player.
[00:01:26.000 --> 00:01:28.000]   It's time for Twig this week in Google.
[00:01:28.000 --> 00:01:32.000]   Let's show a week over the latest news from the Google verse, the Twitter verse, the Facebook verse.
[00:01:32.000 --> 00:01:36.000]   Really kind of the whole verse thing, all of it together.
[00:01:36.000 --> 00:01:39.000]   Stacey is here because it is World Waffle Day.
[00:01:39.000 --> 00:01:42.000]   So Stacey Higginbotham and her Waffles have appeared.
[00:01:42.000 --> 00:01:45.000]   Actually, we have a big story for Stacey coming up.
[00:01:45.000 --> 00:01:47.000]   Stacey on IOT.com.
[00:01:47.000 --> 00:01:50.000]   She's our IOT guru at Gigastacey.
[00:01:50.000 --> 00:01:54.000]   Jeff Jarvis is also here, the Leonard Towne professor for journalistic innovation
[00:01:54.000 --> 00:02:01.000]   at the Craig, who mark graduate school journalism at the City University of New York.
[00:02:01.000 --> 00:02:02.000]   Hello, Jeff.
[00:02:02.000 --> 00:02:03.000]   Hello, hello.
[00:02:03.000 --> 00:02:08.000]   And then as half as good as Stacey, well, her new hairdo, her love.
[00:02:08.000 --> 00:02:09.000]   I like the new two blouse.
[00:02:09.000 --> 00:02:11.000]   I'm trying not to comment.
[00:02:11.000 --> 00:02:13.000]   I don't comment on appearance.
[00:02:13.000 --> 00:02:16.000]   You can say I, you can say I like your hair cut.
[00:02:16.000 --> 00:02:17.000]   Yeah.
[00:02:17.000 --> 00:02:21.000]   I mean, unless you don't say it, then I automatically assume if you don't say anything, I'm like,
[00:02:21.000 --> 00:02:22.000]   oh, he must take my hair cut.
[00:02:22.000 --> 00:02:25.000]   So I don't want to say this is why you can't weirdly.
[00:02:25.000 --> 00:02:28.000]   This is why I'm confused.
[00:02:28.000 --> 00:02:32.000]   Aunt Pruitt is also here with a fabulous haircut.
[00:02:32.000 --> 00:02:34.000]   The best haircut.
[00:02:34.000 --> 00:02:35.000]   Yes, I got it.
[00:02:35.000 --> 00:02:37.000]   All the hairs are cut.
[00:02:37.000 --> 00:02:39.000]   I was inspired by you, Aunt.
[00:02:39.000 --> 00:02:42.000]   I was like, got to get rid of that hair, sucking to closer to the hair.
[00:02:42.000 --> 00:02:43.000]   So was that hard?
[00:02:43.000 --> 00:02:48.000]   Because I know sometimes if you have longer hair, longer hair, cutting it off is hard.
[00:02:48.000 --> 00:02:51.000]   No, I cut, growing my hair out hard.
[00:02:51.000 --> 00:02:52.000]   You like short hair.
[00:02:52.000 --> 00:02:53.000]   You only have short hair.
[00:02:53.000 --> 00:02:55.000]   That's a really good look, though.
[00:02:55.000 --> 00:02:59.000]   It's very, I'm trying to think who it is.
[00:02:59.000 --> 00:03:00.000]   It's a style.
[00:03:00.000 --> 00:03:02.000]   It is a style.
[00:03:02.000 --> 00:03:03.000]   Yeah.
[00:03:03.000 --> 00:03:04.000]   What do they call it?
[00:03:04.000 --> 00:03:05.000]   Bob's?
[00:03:05.000 --> 00:03:06.000]   It's a pixie.
[00:03:06.000 --> 00:03:07.000]   It's a long pixie.
[00:03:07.000 --> 00:03:08.000]   Pixie.
[00:03:08.000 --> 00:03:09.000]   Okay.
[00:03:09.000 --> 00:03:10.000]   I don't know.
[00:03:10.000 --> 00:03:12.000]   I want to say it's still there.
[00:03:12.000 --> 00:03:14.000]   It's very till the Swinton kind of.
[00:03:14.000 --> 00:03:18.000]   As long as you don't say Caitlyn Jenner, because that's kind of how I feel like it.
[00:03:18.000 --> 00:03:19.000]   It is not Caitlyn Jenner.
[00:03:19.000 --> 00:03:20.000]   No.
[00:03:20.000 --> 00:03:21.000]   That's not it at all.
[00:03:21.000 --> 00:03:23.000]   It's till the Swinton kind of.
[00:03:23.000 --> 00:03:29.000]   You don't know who Harry Styles or Olivia Wilde is, but she'd know who Caitlyn Jenner is?
[00:03:29.000 --> 00:03:31.000]   Of course, everybody knows who Caitlyn is.
[00:03:31.000 --> 00:03:32.000]   Of course.
[00:03:32.000 --> 00:03:33.000]   Yes.
[00:03:33.000 --> 00:03:34.000]   Okay.
[00:03:34.000 --> 00:03:35.000]   Undetech news.
[00:03:35.000 --> 00:03:38.000]   This is kind of a little till the Swinton.
[00:03:38.000 --> 00:03:43.000]   It's that it's that sweep over the right brow that kind of gives it a till the Swinton kind
[00:03:43.000 --> 00:03:44.000]   of a feel.
[00:03:44.000 --> 00:03:45.000]   I like it.
[00:03:45.000 --> 00:03:46.000]   That's Tilda right there.
[00:03:46.000 --> 00:03:47.000]   Yeah.
[00:03:47.000 --> 00:03:48.000]   Yeah.
[00:03:48.000 --> 00:03:49.000]   Oh, yeah.
[00:03:49.000 --> 00:03:50.000]   Yeah.
[00:03:50.000 --> 00:03:51.000]   Yeah.
[00:03:51.000 --> 00:03:53.000]   Her sweep and her cheekbones that could cut glasses.
[00:03:53.000 --> 00:03:54.000]   Oh, yeah.
[00:03:54.000 --> 00:03:55.000]   She's gorgeous.
[00:03:55.000 --> 00:03:56.000]   I love her.
[00:03:56.000 --> 00:04:01.240]   There was a good article recently about her and how she has, despite being kind of indie,
[00:04:01.240 --> 00:04:09.160]   become a real star, Oscar, BAFTA, and so forth, three golden globes, even though she's so
[00:04:09.160 --> 00:04:10.680]   into indie, right?
[00:04:10.680 --> 00:04:15.400]   She doesn't do mainstream characters or anything, but she's so good.
[00:04:15.400 --> 00:04:16.400]   This is from The Guardian.
[00:04:16.400 --> 00:04:23.920]   She disappears or goes blank like Garbo, the enigma of Tilda Swinton.
[00:04:23.920 --> 00:04:30.160]   I ended up, I never watched network TV and I was in a hotel last night or night before
[00:04:30.160 --> 00:04:33.640]   last, and I ended up watching the TMZ one hour special.
[00:04:33.640 --> 00:04:34.640]   Oh my God.
[00:04:34.640 --> 00:04:35.640]   Richard Simmons.
[00:04:35.640 --> 00:04:37.440]   And you wonder where this is going to happen to him?
[00:04:37.440 --> 00:04:38.440]   Wait a minute.
[00:04:38.440 --> 00:04:40.640]   Well, at the end, they say that he's Garbo.
[00:04:40.640 --> 00:04:42.600]   He just chose to leave and that's it.
[00:04:42.600 --> 00:04:43.600]   He's okay.
[00:04:43.600 --> 00:04:44.600]   Good.
[00:04:44.600 --> 00:04:45.600]   Good.
[00:04:45.600 --> 00:04:46.600]   They don't know.
[00:04:46.600 --> 00:04:47.600]   They didn't know a damn thing.
[00:04:47.600 --> 00:04:49.240]   He just kind of disappeared.
[00:04:49.240 --> 00:04:51.920]   I've heard the same thing that he's totally fine.
[00:04:51.920 --> 00:04:54.720]   He just wants to sort of be away from everything.
[00:04:54.720 --> 00:04:55.720]   I've heard that too.
[00:04:55.720 --> 00:04:59.560]   But I like the rumor that it was unwillingly was disappeared as opposed to just on his
[00:04:59.560 --> 00:05:00.560]   own.
[00:05:00.560 --> 00:05:01.560]   There was all these rumors.
[00:05:01.560 --> 00:05:08.560]   I just want to tell you when I disappear, it was my choice.
[00:05:08.560 --> 00:05:09.560]   I garboed.
[00:05:09.560 --> 00:05:11.560]   It'll be a garbo.
[00:05:11.560 --> 00:05:12.560]   I'm just saying.
[00:05:12.560 --> 00:05:16.840]   Just wonder whether anybody will look for you.
[00:05:16.840 --> 00:05:17.840]   I know they won't.
[00:05:17.840 --> 00:05:20.320]   That's why I'm telling you at a time.
[00:05:20.320 --> 00:05:24.600]   I'm trying to stir up some interest.
[00:05:24.600 --> 00:05:30.120]   Speaking of stirring up interest, much, much.
[00:05:30.120 --> 00:05:40.480]   So I remember when Twitter hired Mudge, his real name is Peter Zatko as their security
[00:05:40.480 --> 00:05:41.480]   chief.
[00:05:41.480 --> 00:05:46.400]   It's kind of an interesting story because Mudge was widely known as a hacker, kind of a white
[00:05:46.400 --> 00:05:49.120]   hat, good guy hacker.
[00:05:49.120 --> 00:05:55.400]   They hired him in the face of massive security problems, you may remember, breaches and so
[00:05:55.400 --> 00:05:56.400]   forth.
[00:05:56.400 --> 00:05:59.120]   I think a lot of people said, "Yeah, finally they got somebody who knows what he's doing
[00:05:59.120 --> 00:06:00.120]   in there.
[00:06:00.120 --> 00:06:01.120]   This'll be good."
[00:06:01.120 --> 00:06:07.000]   Six months later, he's gone after Jack left or I fired him.
[00:06:07.000 --> 00:06:08.000]   Yeah.
[00:06:08.000 --> 00:06:17.360]   I mean, for not doing anything, I mean, it was kind of fired for cause, it sounded like.
[00:06:17.360 --> 00:06:19.480]   Well both things in this story can be true.
[00:06:19.480 --> 00:06:20.480]   Yeah.
[00:06:20.480 --> 00:06:21.480]   Oh, yeah.
[00:06:21.480 --> 00:06:22.480]   Yeah.
[00:06:22.480 --> 00:06:23.480]   We're going to get there.
[00:06:23.480 --> 00:06:28.640]   I want you to know, though, what led up to it, the backstory.
[00:06:28.640 --> 00:06:30.360]   So he was fired.
[00:06:30.360 --> 00:06:33.000]   It's about six months ago now.
[00:06:33.000 --> 00:06:34.000]   I think I'm sorry.
[00:06:34.000 --> 00:06:36.200]   He was two years at Twitter, I think, and then fired six months ago.
[00:06:36.200 --> 00:06:37.200]   I don't know.
[00:06:37.200 --> 00:06:38.200]   It wasn't long.
[00:06:38.200 --> 00:06:40.240]   It wasn't long.
[00:06:40.240 --> 00:06:49.080]   Six months later, he files what he calls a whistleblower complaint against Twitter.
[00:06:49.080 --> 00:06:50.080]   Now I don't know.
[00:06:50.080 --> 00:06:53.240]   Is it a whistleblower if you don't work for them and they can't, there's no, no.
[00:06:53.240 --> 00:06:57.080]   He used the same, he used the same organization for help that helped out.
[00:06:57.080 --> 00:06:58.320]   Okay.
[00:06:58.320 --> 00:07:01.840]   So it's a, it's a whistleblower complaint.
[00:07:01.840 --> 00:07:07.800]   The Washington Post has the exclusive cause they got the complaint itself.
[00:07:07.800 --> 00:07:13.160]   The most serious accusation coming to the WAPO is that Twitter violated the terms of
[00:07:13.160 --> 00:07:21.080]   a, of a 11 year old consent decree with the FTC who said, you know, you, you, it's a weird
[00:07:21.080 --> 00:07:22.560]   consent decree.
[00:07:22.560 --> 00:07:28.520]   You had for the next 20 years, it said this was in 2010.
[00:07:28.520 --> 00:07:30.720]   So that would have gone through 2030.
[00:07:30.720 --> 00:07:34.640]   You will be barred from misleading consumers.
[00:07:34.640 --> 00:07:37.440]   After 20 years, go ahead.
[00:07:37.440 --> 00:07:42.960]   But for the next 20 years, you'll be barred from misleading consumers about the extent
[00:07:42.960 --> 00:07:50.240]   to which you protect security, privacy, and confidentiality of non-public consumer information.
[00:07:50.240 --> 00:07:55.720]   One of Mudge's complaints or allegations, I guess is what I probably should call them,
[00:07:55.720 --> 00:07:59.800]   is that Twitter collecting phone number, as, as we've kind of known about Facebook was
[00:07:59.800 --> 00:08:04.520]   accused of doing this too, collecting phone numbers and real names and other things.
[00:08:04.520 --> 00:08:08.920]   Ostensibly for account recovery was actually using that to sell ads.
[00:08:08.920 --> 00:08:12.560]   And we knew that from a couple months ago.
[00:08:12.560 --> 00:08:13.560]   Right.
[00:08:13.560 --> 00:08:16.920]   Mudge also, I'm going to call him Mudge instead of Zatko because I think he's better known
[00:08:16.920 --> 00:08:19.640]   as Mudge, right?
[00:08:19.640 --> 00:08:22.680]   He says he's going to make some more cuddly.
[00:08:22.680 --> 00:08:24.680]   I don't know.
[00:08:24.680 --> 00:08:28.520]   Zatko, I know, he thought Zatko doesn't, yeah.
[00:08:28.520 --> 00:08:33.840]   He was an open source programmer, a hacker, a writer, pretty well known in the security
[00:08:33.840 --> 00:08:35.240]   community.
[00:08:35.240 --> 00:08:42.240]   He founded the Cult of the Dead Cow and was in Loft, which did a lot of cracking stuff.
[00:08:42.240 --> 00:08:44.880]   So he was a very well known guy.
[00:08:44.880 --> 00:08:52.480]   He actually, 2010, started to work at DARPA, the Defense Advanced Research Project Administration,
[00:08:52.480 --> 00:08:55.320]   a government position, he oversaw cybersecurity.
[00:08:55.320 --> 00:08:59.360]   He worked for Google after three years in their Advanced Technology and Products Division
[00:08:59.360 --> 00:09:04.520]   in 2020, who was hired as head of security at Twitter.
[00:09:04.520 --> 00:09:10.880]   There's a hilarious picture of Mudge testifying for Congress, and hell, I just lost it.
[00:09:10.880 --> 00:09:13.280]   In '98, yeah, as one of seven was to them.
[00:09:13.280 --> 00:09:16.800]   Yeah, and so is his name, his name text says Mudge next to a guy called Kingpin.
[00:09:16.800 --> 00:09:17.800]   Yeah.
[00:09:17.800 --> 00:09:21.120]   You know, it's just, you could be sure Congress took that seriously.
[00:09:21.120 --> 00:09:22.120]   Yeah.
[00:09:22.120 --> 00:09:23.120]   Yeah.
[00:09:23.120 --> 00:09:25.560]   He met with Bill Clinton at a security summit.
[00:09:25.560 --> 00:09:28.120]   You know, so he was a well respected guy.
[00:09:28.120 --> 00:09:31.280]   He was a division scientist at BBN where the internet was invented.
[00:09:31.280 --> 00:09:34.720]   So he was, you know, he's pretty well known.
[00:09:34.720 --> 00:09:37.960]   He was sought out by Jack Dorsey, who was the CEO at the time to lead the company's
[00:09:37.960 --> 00:09:44.320]   information security approach after a big hack in July of 2020.
[00:09:44.320 --> 00:09:45.480]   You may remember that hack.
[00:09:45.480 --> 00:09:50.560]   A lot of big name accounts were co-opted, right?
[00:09:50.560 --> 00:09:51.560]   Was that the one?
[00:09:51.560 --> 00:09:52.560]   Yeah.
[00:09:52.560 --> 00:09:55.440]   It was the one, I can't remember, but it was all those famous...
[00:09:55.440 --> 00:09:57.840]   Is this the one that led to the...
[00:09:57.840 --> 00:10:03.560]   No, no, it was the Bitcoin one where Apple said we're giving back to our community.
[00:10:03.560 --> 00:10:04.560]   We support Bitcoin.
[00:10:04.560 --> 00:10:05.560]   Right.
[00:10:05.560 --> 00:10:06.560]   Okay.
[00:10:06.560 --> 00:10:09.960]   All Bitcoin sent to the address below will be sent back to you, doubled.
[00:10:09.960 --> 00:10:11.480]   And Elon tweeted it.
[00:10:11.480 --> 00:10:12.480]   Apple tweeted it.
[00:10:12.480 --> 00:10:14.320]   A lot of people tweeted it.
[00:10:14.320 --> 00:10:15.320]   This is a...
[00:10:15.320 --> 00:10:17.080]   That was bad.
[00:10:17.080 --> 00:10:20.760]   Twitter, a few idiots did it, but that'd be more for the yucks that it is.
[00:10:20.760 --> 00:10:21.760]   Yeah, but it was bad.
[00:10:21.760 --> 00:10:27.760]   I mean, Bill Clinton, I think, Barack Obama, Joe Biden, Bill Gates, Jeff Bezos, Mr. Beast,
[00:10:27.760 --> 00:10:34.280]   Michael Bloomberg, Warren Buffett, Floyd Mayweather, Jr., Kim Kardashian, Kanye West, all their
[00:10:34.280 --> 00:10:39.240]   accounts had been compromised and tweeted that Bitcoin thing.
[00:10:39.240 --> 00:10:42.680]   Uber, Cash app, Apple.
[00:10:42.680 --> 00:10:47.800]   Twitter said that 138 accounts were affected, but only 45 were actually tweeted out that
[00:10:47.800 --> 00:10:49.000]   scam.
[00:10:49.000 --> 00:10:55.080]   And as I remember, they think it was because Mudge was able to use slack, not Mudge, the
[00:10:55.080 --> 00:10:56.080]   bad guys.
[00:10:56.080 --> 00:10:59.400]   Mudge came to save them, save the day where he was able to get into a slack account and
[00:10:59.400 --> 00:11:00.400]   get the credential.
[00:11:00.400 --> 00:11:01.400]   Remember we talked about this?
[00:11:01.400 --> 00:11:05.680]   Get the credentials for an administrative account, which allowed them to do this.
[00:11:05.680 --> 00:11:11.440]   Anyway, it was such a black mark that Twitter hired him.
[00:11:11.440 --> 00:11:18.800]   But then when he was let go, it was because he didn't do a good job.
[00:11:18.800 --> 00:11:23.240]   He was causing problems.
[00:11:23.240 --> 00:11:27.480]   Are they public about that or that's come out now in his allegations of...
[00:11:27.480 --> 00:11:29.760]   I don't think it's a good public question.
[00:11:29.760 --> 00:11:30.760]   Why they fired him?
[00:11:30.760 --> 00:11:32.320]   I don't remember that.
[00:11:32.320 --> 00:11:34.320]   Let me see if I can find that story.
[00:11:34.320 --> 00:11:36.960]   Because there were two executives fired at the same time.
[00:11:36.960 --> 00:11:37.960]   Yeah.
[00:11:37.960 --> 00:11:48.480]   Let me just Google Twitter fires Mudge.
[00:11:48.480 --> 00:11:50.680]   All those stories are the current story, unfortunately.
[00:11:50.680 --> 00:11:51.680]   Oh, here it is.
[00:11:51.680 --> 00:11:54.480]   Twitter shakes up its security team.
[00:11:54.480 --> 00:12:00.120]   This was the story we read in January of this year on this show because it's been visited
[00:12:00.120 --> 00:12:01.120]   already.
[00:12:01.120 --> 00:12:05.640]   Peter Zacko had a security no longer at the company.
[00:12:05.640 --> 00:12:07.280]   Twitter confirmed.
[00:12:07.280 --> 00:12:13.000]   Chief security officer, Rinky Sette was also fired following, quote, an assessment of how
[00:12:13.000 --> 00:12:18.240]   the organization was being led and impact on the top priority work.
[00:12:18.240 --> 00:12:23.560]   Let me see if they have anything else.
[00:12:23.560 --> 00:12:31.120]   I think this background information is important because I think you need to understand this,
[00:12:31.120 --> 00:12:32.800]   that he was fired in January.
[00:12:32.800 --> 00:12:42.000]   Six months later, he does this whistleblower thing.
[00:12:42.000 --> 00:12:44.160]   I feel ethically bound, said Zacko.
[00:12:44.160 --> 00:12:46.960]   This is not a light step to take.
[00:12:46.960 --> 00:12:51.560]   He declined to discuss what happened to Twitter except to stand by the formal complaint.
[00:12:51.560 --> 00:12:56.000]   Under SEC whistleblower rules, he's entitled to legal protection against retaliation as
[00:12:56.000 --> 00:12:58.400]   well as potential monetary rewards.
[00:12:58.400 --> 00:13:02.600]   However, he's not guaranteed to be believed.
[00:13:02.600 --> 00:13:06.000]   Everything is alleged and this begins in investigation.
[00:13:06.000 --> 00:13:14.040]   Casey Newton said in the piece that he wrote that he heard people who were budgets fans,
[00:13:14.040 --> 00:13:17.600]   but he said, "I had an equal number of people calling me and telling me, 'Ah, but you can't
[00:13:17.600 --> 00:13:20.400]   use this story and then tell it in a story as much.'"
[00:13:20.400 --> 00:13:21.400]   He has his fans.
[00:13:21.400 --> 00:13:22.400]   He has his detractors.
[00:13:22.400 --> 00:13:25.600]   Don't we all, but you know, we're going to assault.
[00:13:25.600 --> 00:13:35.120]   Hagrowall, the CEO, says, "I want to get all of the whistleblower complaining now and then
[00:13:35.120 --> 00:13:40.120]   we'll talk about the merits of it."
[00:13:40.120 --> 00:13:46.880]   It says that thousands of employees still had wide ranging and poorly tracked internal
[00:13:46.880 --> 00:13:48.600]   access to the core company software.
[00:13:48.600 --> 00:13:54.960]   Remember, that was part of the problem that allowed this hack two years ago, a situation
[00:13:54.960 --> 00:13:58.920]   that for years led to embarrassing hacks, including the commandeering of accounts held
[00:13:58.920 --> 00:14:03.040]   by such high profile users, Elon Musk, Barack Obama, Obama, and Donald Trump.
[00:14:03.040 --> 00:14:06.680]   In addition, the whistleblower document alleges this is all from The Washington Post.
[00:14:06.680 --> 00:14:11.400]   The company prioritized user growth over reducing spam.
[00:14:11.400 --> 00:14:12.480]   Are you surprised?
[00:14:12.480 --> 00:14:15.400]   Though unwanted content made the user experience worse.
[00:14:15.400 --> 00:14:21.480]   He says it's because executives stood to win individual bonuses of as much as $10 million
[00:14:21.480 --> 00:14:24.160]   tied to increases in daily users.
[00:14:24.160 --> 00:14:27.240]   But no rewards for cutting spam.
[00:14:27.240 --> 00:14:33.920]   He says the CEO, Pargagro Alar was lying when he tweeted in May that the company was "strongly
[00:14:33.920 --> 00:14:39.520]   incentivized to detect and remove as much spam as we possibly can."
[00:14:39.520 --> 00:14:44.240]   Zaco told the Post in an interview that his decision to go public was an extension of his
[00:14:44.240 --> 00:14:49.600]   previous work exposing flaws in specific pieces of software.
[00:14:49.600 --> 00:14:54.400]   "I felt ethically bound," said Zaco.
[00:14:54.400 --> 00:14:59.800]   A redacted version of the 84 page finally went to congressional committees.
[00:14:59.800 --> 00:15:00.880]   That's how the Post got it.
[00:15:00.880 --> 00:15:03.600]   They got the redacted version.
[00:15:03.600 --> 00:15:07.680]   Zaco is represented by a nonprofit law firm called WhistleblowerA.
[00:15:07.680 --> 00:15:13.000]   The FTC is also reviewing the allegations.
[00:15:13.000 --> 00:15:16.160]   Stirt up a hornet's nest.
[00:15:16.160 --> 00:15:25.520]   The chiefly, besides the fact that lax security was this whole thing about bots.
[00:15:25.520 --> 00:15:32.160]   This is why I get a little suspicious because apparently, according to the Post, Elon Musk
[00:15:32.160 --> 00:15:42.680]   before the Whistleblower accusations had subpoenaed Zaco to testify in the trial which is coming
[00:15:42.680 --> 00:15:49.720]   up in Delaware in October.
[00:15:49.720 --> 00:15:58.920]   Zaco had accused Twitter of lying about bots to Elon Musk.
[00:15:58.920 --> 00:16:00.440]   To me, that's pretty damn convenient.
[00:16:00.440 --> 00:16:03.800]   Can I explain the mastic?
[00:16:03.800 --> 00:16:04.800]   Yes.
[00:16:04.800 --> 00:16:10.640]   Let's go now to Techter where Mike Masnick's headline is, "Twitter, Whistleblowing Report
[00:16:10.640 --> 00:16:16.680]   Actually Seems to Confirm Twitter's Legal Argument While Pretending to Support Musk's."
[00:16:16.680 --> 00:16:20.080]   Yes, you better explain that.
[00:16:20.080 --> 00:16:30.440]   What Mike explains in a Bible-length post, it's a much, much detail, is that, number one,
[00:16:30.440 --> 00:16:35.520]   this still isn't a factor in the case because, again, let's not forget, Musk bought Twitter
[00:16:35.520 --> 00:16:38.000]   site on the scene.
[00:16:38.000 --> 00:16:39.560]   That rests over this.
[00:16:39.560 --> 00:16:45.880]   Now Musk is out there trying to say, "Oh, but you're lying to the FTC about your spam
[00:16:45.880 --> 00:16:49.400]   bots, your spam accounts."
[00:16:49.400 --> 00:16:50.400]   And then what--
[00:16:50.400 --> 00:16:51.400]   SEC.
[00:16:51.400 --> 00:16:52.400]   Mike--
[00:16:52.400 --> 00:16:53.400]   Not FTC.
[00:16:53.400 --> 00:16:54.400]   SEC.
[00:16:54.400 --> 00:16:55.400]   Thank you.
[00:16:55.400 --> 00:17:02.280]   And then what Mike explains, details, is that what's at an issue here is Twitter's
[00:17:02.280 --> 00:17:09.080]   MDOWS, monetizable daily active users.
[00:17:09.080 --> 00:17:12.480]   And the MDOW is not a view of the entirety of Twitter.
[00:17:12.480 --> 00:17:16.480]   It is-- they're telling the FTC, this is what we're telling advertisers, of the people
[00:17:16.480 --> 00:17:20.800]   who might actually be human and could click on an ad.
[00:17:20.800 --> 00:17:24.920]   Taking out spam, taking out legitimate good bots too because they're not going to click.
[00:17:24.920 --> 00:17:27.200]   This is the monetizable daily active users.
[00:17:27.200 --> 00:17:29.160]   And yes, we've already cleared out the spam.
[00:17:29.160 --> 00:17:30.480]   We've already gone through wall vat.
[00:17:30.480 --> 00:17:32.560]   That number is post spam reduction.
[00:17:32.560 --> 00:17:34.040]   It is post spam reduction.
[00:17:34.040 --> 00:17:38.320]   And importantly, it's important to keep in mind that Mike points out, is that Twitter
[00:17:38.320 --> 00:17:44.080]   and their executives are very motivated to keep that number low because the higher number
[00:17:44.080 --> 00:17:50.080]   was, if they put it too high, the advertisers would say, "Well, your efficacy of your advertising
[00:17:50.080 --> 00:17:53.560]   is crap because the proportion of people clicking is low."
[00:17:53.560 --> 00:17:55.040]   There's two sides to that story.
[00:17:55.040 --> 00:17:58.080]   We deal with that all the time too.
[00:17:58.080 --> 00:18:05.040]   You want to accurately report how many impressions an ad is going to get on your platform.
[00:18:05.040 --> 00:18:09.200]   And you get paid by the impression, right?
[00:18:09.200 --> 00:18:11.120]   But they also look at your--
[00:18:11.120 --> 00:18:12.120]   How efficient.
[00:18:12.120 --> 00:18:13.120]   --at return on investment.
[00:18:13.120 --> 00:18:17.800]   So if your ads are too expensive, regardless, they're going to say, "Well, it costs too much
[00:18:17.800 --> 00:18:19.920]   to get too little result."
[00:18:19.920 --> 00:18:21.440]   So I guess it's true.
[00:18:21.440 --> 00:18:24.960]   There is some incentive to not make that number too high, but there's also incentive to make
[00:18:24.960 --> 00:18:26.720]   it higher because you make more money.
[00:18:26.720 --> 00:18:33.880]   So you want to find the highest number you can get that will not be so high that they
[00:18:33.880 --> 00:18:35.880]   say, "Well, this isn't working."
[00:18:35.880 --> 00:18:39.600]   So what Mike has to do would be to say the actual number, but who knows?
[00:18:39.600 --> 00:18:42.760]   Yeah, the number which is what Mike says they're actually trying to do and have a method.
[00:18:42.760 --> 00:18:45.640]   He says, "Muscle of Hearrs" has done his explains that exact.
[00:18:45.640 --> 00:18:52.280]   "Muscle" confuses the total number of spam bots on the platform with the numbers Twitter
[00:18:52.280 --> 00:18:55.720]   is giving after in its MDOW report, which is where--
[00:18:55.720 --> 00:18:59.640]   What Twitter has warranted is that we've done this.
[00:18:59.640 --> 00:19:08.120]   So what-- I lose Mike's logic a little bit here, but what he emphasizes is that what
[00:19:08.120 --> 00:19:15.120]   Mudge says backs up Musk, actually backs up Twitter's case.
[00:19:15.120 --> 00:19:20.760]   And so he's just saying-- and Mike being Mike bless his heart gets just outraged at
[00:19:20.760 --> 00:19:24.720]   the media coverage because they just gobble up the Musk line on this.
[00:19:24.720 --> 00:19:26.600]   Well, and don't question, don't analyze it.
[00:19:26.600 --> 00:19:27.960]   Yeah, they're not that subtle.
[00:19:27.960 --> 00:19:31.320]   Yeah, they're not not-- they're not educated on stuff though.
[00:19:31.320 --> 00:19:32.800]   And Mike is-- who is for sure.
[00:19:32.800 --> 00:19:38.080]   And we try to be educated and certainly educate our audience on this.
[00:19:38.080 --> 00:19:42.280]   Remember, for the most part, companies don't report anything-- MDOW is something Twitter
[00:19:42.280 --> 00:19:43.280]   made up.
[00:19:43.280 --> 00:19:51.360]   They report monthly active users-- Mao, MAU, and MDOW or monthly daily active users
[00:19:51.360 --> 00:19:55.400]   now-- MDOW is monetizable users.
[00:19:55.400 --> 00:19:59.480]   In other words, a different number, lower number that reflects the actual users that
[00:19:59.480 --> 00:20:02.160]   are seeing your ad that could buy your product.
[00:20:02.160 --> 00:20:05.520]   So in fact, that's much says this in his complaint.
[00:20:05.520 --> 00:20:09.720]   Until 2019, Twitter reported total monthly users, but stopped because the number was
[00:20:09.720 --> 00:20:14.640]   subject to negative swings for a variety of reasons, including--
[00:20:14.640 --> 00:20:16.360]   I'm really about that account.
[00:20:16.360 --> 00:20:17.360]   Yeah.
[00:20:17.360 --> 00:20:18.360]   Yeah.
[00:20:18.360 --> 00:20:23.480]   Twitter announced a new proprietary opaque metric they called monetizable daily active
[00:20:23.480 --> 00:20:27.840]   users to find its valid user accounts that might click through ads and actually buy a
[00:20:27.840 --> 00:20:28.840]   product.
[00:20:28.840 --> 00:20:34.320]   Yeah, that's-- I understand why Twitter would do that.
[00:20:34.320 --> 00:20:36.760]   And as long as advertisers accept it, that's fine.
[00:20:36.760 --> 00:20:42.640]   There's no legal requirement to use monthly active users instead of daily monetize.
[00:20:42.640 --> 00:20:47.720]   But I think Musk is trying to say that Twitter is defrauding people there.
[00:20:47.720 --> 00:20:51.080]   And without the evidence that in fact what's happening is Twitter saying, no, this is our
[00:20:51.080 --> 00:20:52.080]   methodology.
[00:20:52.080 --> 00:20:53.080]   We're explaining this to you many, many times.
[00:20:53.080 --> 00:20:55.120]   And you're choosing to be confused.
[00:20:55.120 --> 00:20:58.600]   And now you're using Mudge to try to enhance that confusion.
[00:20:58.600 --> 00:21:01.320]   But I think when it gets into court, that's not going to stand.
[00:21:01.320 --> 00:21:03.280]   So there's a few possibilities here.
[00:21:03.280 --> 00:21:04.680]   There's a few possibilities here.
[00:21:04.680 --> 00:21:10.680]   One is that Mudge is absolutely up fourth right, honest, and is doing the right thing and
[00:21:10.680 --> 00:21:13.880]   is telling the world what Twitter is really up to and that they're not secure and they're
[00:21:13.880 --> 00:21:15.360]   just pretending to be.
[00:21:15.360 --> 00:21:20.640]   There's also the opposite, which is that he's a disgruntled employee because he was fired
[00:21:20.640 --> 00:21:22.080]   six months ago.
[00:21:22.080 --> 00:21:27.280]   And perhaps sub-born by Elon Musk who thought, "Ah, here's somebody I can use as a witness
[00:21:27.280 --> 00:21:33.000]   in my trial," or perhaps just motivated by other reasons and isn't telling the truth.
[00:21:33.000 --> 00:21:36.200]   And the truth is I don't know and I don't think anybody knows.
[00:21:36.200 --> 00:21:40.760]   Just because he says it so doesn't mean it so is I guess what it's just--
[00:21:40.760 --> 00:21:49.120]   I say he has a documented history at Twitter of being outspoken about security risks.
[00:21:49.120 --> 00:21:50.760]   And some of them are very real.
[00:21:50.760 --> 00:21:55.480]   Like access to the servers are not having running unpatched versions of, was it Linux
[00:21:55.480 --> 00:21:57.480]   on the servers?
[00:21:57.480 --> 00:22:03.040]   Having employees have access to account credentialing stuff that they shouldn't after the hack.
[00:22:03.040 --> 00:22:04.040]   All of that is very real.
[00:22:04.040 --> 00:22:05.040]   And I can see if you are--
[00:22:05.040 --> 00:22:06.040]   Well, is it real?
[00:22:06.040 --> 00:22:07.040]   Do we know it's real?
[00:22:07.040 --> 00:22:10.440]   Okay, if that is the case, then that makes--
[00:22:10.440 --> 00:22:11.440]   We don't know it's real.
[00:22:11.440 --> 00:22:12.440]   It makes sense that he was saying--
[00:22:12.440 --> 00:22:13.440]   It makes sense that he was probably verifiable.
[00:22:13.440 --> 00:22:18.440]   In other words, in an investigation, we could probably find that out.
[00:22:18.440 --> 00:22:20.440]   But if you saw the truth--
[00:22:20.440 --> 00:22:22.440]   Would I be surprised if it were true?
[00:22:22.440 --> 00:22:23.440]   Hold up.
[00:22:23.440 --> 00:22:24.440]   The Twitter was kind of screwed up?
[00:22:24.440 --> 00:22:25.440]   No.
[00:22:25.440 --> 00:22:26.440]   Yeah.
[00:22:26.440 --> 00:22:27.440]   So hold up though.
[00:22:27.440 --> 00:22:33.840]   If you are a security person-- and there's a very real persona that goes into security
[00:22:33.840 --> 00:22:39.280]   and you feel aggrieved and then you're also going up against management who has no interest
[00:22:39.280 --> 00:22:43.040]   in listening or updating things that don't feel like--
[00:22:43.040 --> 00:22:45.440]   Remember, they're in crisis.
[00:22:45.440 --> 00:22:49.320]   Six months ago, they were still in some sort of like, how are we going to make money?
[00:22:49.320 --> 00:22:52.320]   What are we going to do?
[00:22:52.320 --> 00:22:56.360]   So having some security guy come up, guys, guys, we have to do this.
[00:22:56.360 --> 00:22:58.120]   I can see why he got fired.
[00:22:58.120 --> 00:22:59.120]   Right?
[00:22:59.120 --> 00:23:03.480]   I can also see how he is still a person who desperately cares about the security of something
[00:23:03.480 --> 00:23:09.960]   that journalists, activists, et cetera, use every day to try to influence people.
[00:23:09.960 --> 00:23:10.960]   That's scary.
[00:23:10.960 --> 00:23:15.840]   So you think being fired was justifiable because he raised awareness or attempted to
[00:23:15.840 --> 00:23:20.360]   raise awareness to a fireball, but it's understandable?
[00:23:20.360 --> 00:23:23.720]   I think it was probably not diplomatic.
[00:23:23.720 --> 00:23:28.840]   And I say this because I talked to lots of security people who probably, out of all the
[00:23:28.840 --> 00:23:33.040]   security researchers that I talk to and I do talk to a lot, half of them have left a
[00:23:33.040 --> 00:23:40.000]   job because they were frustrated with the way management treated their recommendations.
[00:23:40.000 --> 00:23:45.720]   So not everybody goes and becomes a whistleblower, but a lot of management and a lot of companies
[00:23:45.720 --> 00:23:48.200]   takes what security-- they should.
[00:23:48.200 --> 00:23:51.040]   I mean, security folks are focused on security.
[00:23:51.040 --> 00:23:54.960]   Management's focused on security, profits, roadmaps, and everything else.
[00:23:54.960 --> 00:23:58.560]   But the security persona is like the superhero.
[00:23:58.560 --> 00:24:01.040]   And when the superhero is like, I will save the day.
[00:24:01.040 --> 00:24:02.880]   And they're like, no, thank you.
[00:24:02.880 --> 00:24:05.480]   And they're like, oh, you are wrong and evil.
[00:24:05.480 --> 00:24:06.480]   I will leave.
[00:24:06.480 --> 00:24:07.800]   But a lot of these people do leave.
[00:24:07.800 --> 00:24:11.040]   It's not that uncommon to see-- I mean, it's a common--
[00:24:11.040 --> 00:24:13.800]   They're going to be a little dogmatic, right, Stacey?
[00:24:13.800 --> 00:24:14.800]   Yeah.
[00:24:14.800 --> 00:24:15.800]   This is the right way.
[00:24:15.800 --> 00:24:19.840]   And when it's not, they're offended.
[00:24:19.840 --> 00:24:25.440]   And maybe they should be, but it's hard to compare all-- I mean, it's true in any field,
[00:24:25.440 --> 00:24:26.440]   right?
[00:24:26.440 --> 00:24:31.280]   Teachers hold your man when people don't respect our scholarship time.
[00:24:31.280 --> 00:24:33.480]   Just get mad when somebody understands how hard the job is.
[00:24:33.480 --> 00:24:35.680]   Yeah, we're all in our--
[00:24:35.680 --> 00:24:36.680]   So--
[00:24:36.680 --> 00:24:37.680]   Blinders.
[00:24:37.680 --> 00:24:42.280]   I have-- I mean, given where he's worked, I have no reason to believe that Zako is anything
[00:24:42.280 --> 00:24:46.040]   but an honorable and talented person.
[00:24:46.040 --> 00:24:47.040]   But there are questions.
[00:24:47.040 --> 00:24:49.840]   Why did he wait so long to file his complaint?
[00:24:49.840 --> 00:24:51.280]   He was fired in January.
[00:24:51.280 --> 00:24:52.960]   Maybe he was spooked by Elon.
[00:24:52.960 --> 00:24:54.520]   I mean, think about it this way.
[00:24:54.520 --> 00:24:59.240]   He wasn't secretive according to all the reporting inside Twitter.
[00:24:59.240 --> 00:25:05.360]   He wasn't secretive about the issues he had with management's response to his recommendations.
[00:25:05.360 --> 00:25:08.280]   So it could be that Musk was like, what's called this guy?
[00:25:08.280 --> 00:25:09.680]   Let's see what happens.
[00:25:09.680 --> 00:25:14.920]   And maybe much was like, oh crap, I don't want to be like a behind the scenes influence
[00:25:14.920 --> 00:25:15.920]   kind of thing.
[00:25:15.920 --> 00:25:18.240]   So he just brought everything into the open.
[00:25:18.240 --> 00:25:19.240]   Or another theory.
[00:25:19.240 --> 00:25:20.240]   I don't know.
[00:25:20.240 --> 00:25:23.400]   Another theory is Jack brings in this guy.
[00:25:23.400 --> 00:25:30.400]   The guy messes everything up.
[00:25:30.400 --> 00:25:31.400]   He's not helping.
[00:25:31.400 --> 00:25:33.280]   He's full of crap.
[00:25:33.280 --> 00:25:34.280]   You're out of here.
[00:25:34.280 --> 00:25:35.280]   Now that I'm the boss.
[00:25:35.280 --> 00:25:38.680]   You don't know the dynamics of the organization.
[00:25:38.680 --> 00:25:42.160]   And you have a technology boss in Arderwald.
[00:25:42.160 --> 00:25:46.160]   And I think that matters in this interview with--
[00:25:46.160 --> 00:25:48.600]   Well, Hayden's also calling Agri-Walls Baby ugly.
[00:25:48.600 --> 00:25:51.520]   I mean, if you built the technology infrastructure in this--
[00:25:51.520 --> 00:25:53.320]   Yeah, good points, Stacy.
[00:25:53.320 --> 00:25:54.320]   Good point.
[00:25:54.320 --> 00:25:56.320]   That's where I'm going to be.
[00:25:56.320 --> 00:25:57.320]   I didn't have that.
[00:25:57.320 --> 00:25:58.320]   That's really--
[00:25:58.320 --> 00:26:02.840]   This didn't turn into another episode of Die Heart.
[00:26:02.840 --> 00:26:06.360]   Because that's where it sounds just like one of those movies.
[00:26:06.360 --> 00:26:08.320]   The one with the team on fun.
[00:26:08.320 --> 00:26:15.040]   When CNN-- when Donnie was a great Irish accent, Donnie was sort of on CNN asked him, why did
[00:26:15.040 --> 00:26:16.720]   it take you so long?
[00:26:16.720 --> 00:26:19.240]   He said, this-- Zachos said, this wasn't my first choice.
[00:26:19.240 --> 00:26:21.560]   This wasn't the path I wanted to take.
[00:26:21.560 --> 00:26:23.360]   I exhausted all internal options.
[00:26:23.360 --> 00:26:25.680]   That could explain the six months.
[00:26:25.680 --> 00:26:26.680]   OK.
[00:26:26.680 --> 00:26:28.840]   He tried to get it fixed.
[00:26:28.840 --> 00:26:30.680]   He tried to do it right.
[00:26:30.680 --> 00:26:36.360]   And he finally got frustrated and said, look, I've got to go public because Twitter's not
[00:26:36.360 --> 00:26:39.960]   going to fix this.
[00:26:39.960 --> 00:26:47.840]   So my only caution, it's just my instinct is, all right, let's not necessarily accept
[00:26:47.840 --> 00:26:50.000]   this on that Facebook.
[00:26:50.000 --> 00:26:51.360]   I think you're right now.
[00:26:51.360 --> 00:26:54.000]   I want to see what the results of investigation are.
[00:26:54.000 --> 00:26:56.440]   It is your right, Jeff, completely believable.
[00:26:56.440 --> 00:26:58.440]   It's not incredible.
[00:26:58.440 --> 00:27:04.440]   But that's also not think this is going to have necessarily a legal impact on the case
[00:27:04.440 --> 00:27:05.680]   that everybody's talking about.
[00:27:05.680 --> 00:27:09.880]   No, but it could have a very big legal impact on Twitter because remember, they have an
[00:27:09.880 --> 00:27:10.880]   FTC consent degree.
[00:27:10.880 --> 00:27:11.880]   That's a tough idea.
[00:27:11.880 --> 00:27:12.880]   Yes, true idea.
[00:27:12.880 --> 00:27:13.880]   That's true.
[00:27:13.880 --> 00:27:14.880]   I am.
[00:27:14.880 --> 00:27:19.160]   And by the way, that's where Zacho could make some money because he will get a percentage
[00:27:19.160 --> 00:27:24.160]   of any judgment against Twitter as a reward for being a whistleblower.
[00:27:24.160 --> 00:27:29.800]   So if you're asking what's his incentive, well, it could be because he's a true believer,
[00:27:29.800 --> 00:27:32.080]   but there might be money in it as well.
[00:27:32.080 --> 00:27:34.120]   Mike also has everybody has.
[00:27:34.120 --> 00:27:35.120]   Sorry.
[00:27:35.120 --> 00:27:41.800]   I think I've asked this before, but could Twitter come after must later for defamation?
[00:27:41.800 --> 00:27:44.680]   Well, you could sue anybody for anything.
[00:27:44.680 --> 00:27:45.680]   My friends.
[00:27:45.680 --> 00:27:47.680]   It's hard for companies.
[00:27:47.680 --> 00:27:55.040]   Well, I mean, but the voting booth companies are doing that after Fox.
[00:27:55.040 --> 00:27:56.440]   So yes, they could.
[00:27:56.440 --> 00:28:01.040]   I mean, this is just the first salvo in what could be a very long war.
[00:28:01.040 --> 00:28:02.040]   Long term.
[00:28:02.040 --> 00:28:07.080]   They're hot in the jaws here.
[00:28:07.080 --> 00:28:08.080]   So I like Twitter.
[00:28:08.080 --> 00:28:09.080]   I just like Twitter.
[00:28:09.080 --> 00:28:13.520]   It's just a fun little place where we can talk and say silly things and see pictures.
[00:28:13.520 --> 00:28:15.320]   Well, there's another question.
[00:28:15.320 --> 00:28:16.400]   There's another question.
[00:28:16.400 --> 00:28:19.800]   Does any of this matter and why does it matter?
[00:28:19.800 --> 00:28:24.720]   I mean, it's a complicated company.
[00:28:24.720 --> 00:28:30.000]   There are, I mean, given that Twitter, Twitter, Twitter, Twitter, Twitter, Twitter, Twitter
[00:28:30.000 --> 00:28:31.200]   can move markets.
[00:28:31.200 --> 00:28:32.960]   Tweets can move markets.
[00:28:32.960 --> 00:28:38.560]   So there's a financial incentive to be like, hey, you know, we need to lock this down.
[00:28:38.560 --> 00:28:43.960]   It also, I mean, if we look at June, you know, January 6, you could say that tweets also
[00:28:43.960 --> 00:28:48.360]   can incite political action violence.
[00:28:48.360 --> 00:28:49.880]   How do we want to call that?
[00:28:49.880 --> 00:28:56.400]   So so your argument is that Twitter is central to conversation in the public square.
[00:28:56.400 --> 00:29:01.840]   And so it needs to be secure, reliable, authentic.
[00:29:01.840 --> 00:29:02.840]   We counted it to be that.
[00:29:02.840 --> 00:29:03.840]   I would argue that.
[00:29:03.840 --> 00:29:04.840]   Yeah.
[00:29:04.840 --> 00:29:05.840]   Yeah.
[00:29:05.840 --> 00:29:09.880]   Well, let's also remember back to the days of the fail whale.
[00:29:09.880 --> 00:29:10.880]   Yeah.
[00:29:10.880 --> 00:29:11.880]   I mean, I'm not quite sure.
[00:29:11.880 --> 00:29:12.880]   It's a lot better than you used to be.
[00:29:12.880 --> 00:29:17.360]   I'm convinced that, I mean, I guess, I mean, I can't deny it, Stacey, that Twitter does
[00:29:17.360 --> 00:29:18.600]   have that impact.
[00:29:18.600 --> 00:29:20.720]   You're absolutely right.
[00:29:20.720 --> 00:29:24.760]   To me though, I still see it's kind of as a more frivolous place where people just throw
[00:29:24.760 --> 00:29:25.760]   crap out.
[00:29:25.760 --> 00:29:33.000]   No, there were like the Saudi spies who got the job at Twitter and had access, back-end
[00:29:33.000 --> 00:29:38.440]   access to accounts from activists and then I don't know if they've heard people or what
[00:29:38.440 --> 00:29:39.440]   happened.
[00:29:39.440 --> 00:29:43.440]   They revealed things back and they just got found guilty.
[00:29:43.440 --> 00:29:47.640]   One thing that Mudge points out too is that the company kind of was forced by the Indian
[00:29:47.640 --> 00:29:50.400]   government.
[00:29:50.400 --> 00:29:59.000]   What some say is the dusk of their democracy to hire certain people at Twitter.
[00:29:59.000 --> 00:30:05.640]   And I mean, and India's been going after Twitter as well, trying to cut political dissension.
[00:30:05.640 --> 00:30:08.200]   There's stuck in a lot of ISIS.
[00:30:08.200 --> 00:30:14.360]   Maybe I'm conditioned because I've been on Twitter since I remember the horse...
[00:30:14.360 --> 00:30:16.360]   When you were a bullpop in the person.
[00:30:16.360 --> 00:30:17.360]   The horse, the horse, the e-books.
[00:30:17.360 --> 00:30:18.360]   Yeah, I remember when I was the...
[00:30:18.360 --> 00:30:19.360]   Sure we do.
[00:30:19.360 --> 00:30:25.000]   You know, so Twitter's always been kind of more of a playground for me than the place
[00:30:25.000 --> 00:30:27.000]   where the national dialogue happens.
[00:30:27.000 --> 00:30:34.040]   In fact, I often think that giving too much importance to tweets even from former presidents
[00:30:34.040 --> 00:30:36.960]   is a mistake because they're just tweets.
[00:30:37.040 --> 00:30:41.940]   But you sound like somebody who is ignoring the reality of like well,
[00:30:41.940 --> 00:30:44.880]   just because I think television was a silly place for me.
[00:30:44.880 --> 00:30:47.080]   Turns it's so sponsored, right?
[00:30:47.080 --> 00:30:48.500]   Anytime television, true.
[00:30:48.500 --> 00:30:50.880]   It doesn't have an influence on the discourse, true?
[00:30:50.880 --> 00:30:52.240]   That's not true.
[00:30:52.240 --> 00:30:58.920]   I mean, Twitter today is not what it was when we joined in like '08 or '07 Stacy
[00:30:58.920 --> 00:31:00.320]   playing Newton, Minnow.
[00:31:00.320 --> 00:31:02.920]   Who's Dino.
[00:31:02.920 --> 00:31:05.760]   Who's that.
[00:31:05.760 --> 00:31:06.600]   Dude, Minnow.
[00:31:06.600 --> 00:31:08.760]   The television is a vast, large land.
[00:31:08.760 --> 00:31:10.560]   Old guy is using Minnow.
[00:31:10.560 --> 00:31:17.340]   Former FCC chairman and the guy who said TV was a wasteland, a vast
[00:31:17.340 --> 00:31:18.340]   wasteland.
[00:31:18.340 --> 00:31:21.760]   He was born in 1926.
[00:31:21.760 --> 00:31:24.480]   So he's only 10 years older than me and Jeff.
[00:31:24.480 --> 00:31:29.080]   Say my father had me when it was 10.
[00:31:29.080 --> 00:31:29.840]   No, I'm kidding.
[00:31:29.840 --> 00:31:30.560]   Just kidding.
[00:31:30.560 --> 00:31:31.560]   Just kidding.
[00:31:31.560 --> 00:31:33.840]   Of the FCC in 1961.
[00:31:34.920 --> 00:31:35.420]   Why?
[00:31:35.420 --> 00:31:39.000]   Yes, this was the, but this was the, this was the dawn of tell, this was the say,
[00:31:39.000 --> 00:31:44.120]   Stacy, this was when TV was in his infant age, just like the internet is now.
[00:31:44.120 --> 00:31:47.040]   So he came along and said, it's all screwed up his career of the world.
[00:31:47.040 --> 00:31:48.040]   And everybody listened.
[00:31:48.040 --> 00:31:49.200]   He is still alive.
[00:31:49.200 --> 00:31:50.360]   Said TV was awful.
[00:31:50.360 --> 00:31:51.200]   He's 96.
[00:31:51.200 --> 00:31:53.200]   He was JFK's chairman of the FCC.
[00:31:53.200 --> 00:31:53.800]   He's still alive.
[00:31:53.800 --> 00:31:54.760]   He's still alive.
[00:31:54.760 --> 00:31:55.600]   Barack Obama.
[00:31:55.600 --> 00:31:57.560]   Say it like that.
[00:31:57.560 --> 00:31:58.320]   Well, it's 96.
[00:31:58.320 --> 00:32:02.240]   His kid came after me because I credit, they're criticized that speech on Twitter.
[00:32:02.240 --> 00:32:03.080]   And the kid went after me.
[00:32:03.080 --> 00:32:03.360]   He.
[00:32:03.360 --> 00:32:04.680]   Well, it's a little late.
[00:32:04.680 --> 00:32:08.400]   I criticized that speech that was a long time ago.
[00:32:08.400 --> 00:32:09.800]   I'm a busy guy, Leo.
[00:32:09.800 --> 00:32:10.880]   It has been a while to get to things.
[00:32:10.880 --> 00:32:13.360]   You know, he gave the speech in 1961.
[00:32:13.360 --> 00:32:15.800]   About to start tweet 1968.
[00:32:15.800 --> 00:32:20.200]   50 years later, Jeff Jarvis noticed and tweeted about it.
[00:32:20.200 --> 00:32:26.280]   Anyway, he won the presidential medal of freedom, was given to him by Barack Obama in 2016.
[00:32:26.280 --> 00:32:30.080]   So he is, you actually might be interested in reading the speech to Stacy.
[00:32:30.080 --> 00:32:31.640]   It's a very famous speech.
[00:32:31.640 --> 00:32:32.000]   Yeah.
[00:32:32.000 --> 00:32:34.080]   I'm familiar with the quote.
[00:32:34.120 --> 00:32:36.040]   So I will go check out the speech.
[00:32:36.040 --> 00:32:36.360]   Yeah.
[00:32:36.360 --> 00:32:38.520]   Just like, oh, man.
[00:32:38.520 --> 00:32:41.600]   All right.
[00:32:41.600 --> 00:32:43.600]   Man, you're like Newt Minnow.
[00:32:43.600 --> 00:32:44.600]   Yeah.
[00:32:44.600 --> 00:32:47.000]   You were very good.
[00:32:47.000 --> 00:32:48.440]   Newton, you know, invitation.
[00:32:48.440 --> 00:32:49.640]   I might add Stacy.
[00:32:49.640 --> 00:32:51.080]   You could make a roadshow out of this.
[00:32:51.080 --> 00:32:53.200]   Excellent.
[00:32:53.200 --> 00:32:54.280]   Me and a nine.
[00:32:54.280 --> 00:32:54.920]   How old, Brooke?
[00:32:54.920 --> 00:32:55.240]   Good.
[00:32:55.240 --> 00:32:55.880]   96.
[00:32:55.880 --> 00:32:56.800]   You're quite quaint.
[00:32:56.800 --> 00:32:59.600]   And Stacy, he can both have as new to know.
[00:32:59.600 --> 00:33:00.200]   I am.
[00:33:00.200 --> 00:33:00.920]   I am serious.
[00:33:00.920 --> 00:33:02.080]   So in the question.
[00:33:02.080 --> 00:33:03.640]   Um.
[00:33:03.640 --> 00:33:06.720]   Of how much import we give Twitter.
[00:33:06.720 --> 00:33:08.080]   Uh.
[00:33:08.080 --> 00:33:08.800]   That's a good question.
[00:33:08.800 --> 00:33:10.080]   I don't know.
[00:33:10.080 --> 00:33:11.560]   It has to be important.
[00:33:11.560 --> 00:33:15.520]   Is it OK if people use it differently than it's intended?
[00:33:15.520 --> 00:33:16.440]   No, does that.
[00:33:16.440 --> 00:33:17.320]   It's only important.
[00:33:17.320 --> 00:33:23.200]   Well, because we're if the FTC and the SEC are getting involved in investigation
[00:33:23.200 --> 00:33:28.360]   over whether there are spam bots on this platform or an investigation over whether
[00:33:28.360 --> 00:33:33.600]   it's truly secure, it's it's kind of acting like, well, it's it's really important.
[00:33:33.840 --> 00:33:34.320]   That it.
[00:33:34.320 --> 00:33:38.520]   I mean, people are using, uh, pseudonyms there.
[00:33:38.520 --> 00:33:40.080]   You don't know whose people's real identity.
[00:33:40.080 --> 00:33:41.080]   It's OK.
[00:33:41.080 --> 00:33:41.880]   It's fine.
[00:33:41.880 --> 00:33:43.160]   But that's so.
[00:33:43.160 --> 00:33:47.320]   But you wouldn't say that on the nation's paper of record, the New York
[00:33:47.320 --> 00:33:48.800]   Times, that's all.
[00:33:48.800 --> 00:33:49.920]   That's the point.
[00:33:49.920 --> 00:33:50.320]   It's not.
[00:33:50.320 --> 00:33:52.880]   That's why it's dangerous to call the Internet of medium.
[00:33:52.880 --> 00:33:53.800]   Exactly that.
[00:33:53.800 --> 00:33:56.000]   But exactly because the presumptions that come with that.
[00:33:56.000 --> 00:33:58.400]   But that's my question to limiting to freedom of speech.
[00:33:58.400 --> 00:33:59.000]   I understand.
[00:33:59.000 --> 00:33:59.400]   No, no, no, no.
[00:33:59.400 --> 00:34:00.280]   That's my question.
[00:34:00.280 --> 00:34:01.200]   Is Twitter.
[00:34:01.200 --> 00:34:01.800]   I'm green.
[00:34:01.960 --> 00:34:06.480]   The, the, I don't think it arises to the level of what it doesn't.
[00:34:06.480 --> 00:34:11.080]   It doesn't have to be as credible as the New York Times.
[00:34:11.080 --> 00:34:16.160]   But then it doesn't have to be as perfect with its spam bots and its security.
[00:34:16.160 --> 00:34:22.120]   OK, so then the argument there is was much should much have blown the whistle
[00:34:22.120 --> 00:34:23.520]   on this private company.
[00:34:23.520 --> 00:34:24.160]   No, he should have.
[00:34:24.160 --> 00:34:24.520]   It's not.
[00:34:24.520 --> 00:34:26.520]   It's a public company and he should have.
[00:34:26.520 --> 00:34:31.760]   And I think what we have to do is wait and see what the investigations
[00:34:32.240 --> 00:34:35.040]   prove from the SEC.
[00:34:35.040 --> 00:34:36.600]   You have to report that somebody.
[00:34:36.600 --> 00:34:38.720]   Yeah, we guess what we're doing enough.
[00:34:38.720 --> 00:34:40.240]   We just spent that's what we're doing.
[00:34:40.240 --> 00:34:42.680]   Absolutely.
[00:34:42.680 --> 00:34:50.840]   But, but I do think that the response does matter depending on how important
[00:34:50.840 --> 00:34:55.560]   you think Twitter is to the national conversation and and its role in the national
[00:34:55.560 --> 00:34:56.200]   conversation.
[00:34:56.200 --> 00:34:56.600]   So.
[00:34:58.760 --> 00:35:02.440]   I mean, it sounds like Stacy, you think it is as important as a as a major
[00:35:02.440 --> 00:35:03.920]   journal of facts.
[00:35:03.920 --> 00:35:06.320]   But I think it's its opinion.
[00:35:06.320 --> 00:35:10.520]   I think it's important less because of the newspaper thing and more because
[00:35:10.520 --> 00:35:13.800]   people treat it like a secure infrastructure kind of like you might.
[00:35:13.800 --> 00:35:14.600]   Well, you shouldn't.
[00:35:14.600 --> 00:35:17.120]   I think that's what you shouldn't.
[00:35:17.120 --> 00:35:19.920]   But, you know, people are holding.
[00:35:19.920 --> 00:35:25.600]   I'll give it as much as the Wall Street Journal opinion page.
[00:35:25.600 --> 00:35:26.360]   How about that?
[00:35:26.360 --> 00:35:28.160]   Oh, you're being.
[00:35:28.160 --> 00:35:30.280]   I think it's kind of like that.
[00:35:30.280 --> 00:35:33.160]   It's just an opinion page for everybody.
[00:35:33.160 --> 00:35:35.280]   Except the.
[00:35:35.280 --> 00:35:36.200]   I think it's the most.
[00:35:36.200 --> 00:35:36.720]   There's a little.
[00:35:36.720 --> 00:35:37.680]   Adam stamps.
[00:35:37.680 --> 00:35:41.680]   It's it's it's important enough because it has time stamps.
[00:35:41.680 --> 00:35:42.440]   It's important enough.
[00:35:42.440 --> 00:35:44.880]   There's a record of it and it's not necessary.
[00:35:44.880 --> 00:35:46.200]   But people don't treat it that way.
[00:35:46.200 --> 00:35:47.360]   That's part of the problem.
[00:35:47.360 --> 00:35:50.960]   We have a record of you typing this out and hit and enter and it went to these
[00:35:50.960 --> 00:35:52.040]   servers exactly.
[00:35:52.040 --> 00:35:53.120]   But that's part of the problem.
[00:35:53.120 --> 00:35:56.360]   And is that people act as if it's the water cooler.
[00:35:57.280 --> 00:36:01.000]   But structure acts as if this is government documents.
[00:36:01.000 --> 00:36:03.840]   Stamped and you know,
[00:36:03.840 --> 00:36:05.080]   which we don't care about either.
[00:36:05.080 --> 00:36:05.440]   Yeah.
[00:36:05.440 --> 00:36:05.960]   Yeah.
[00:36:05.960 --> 00:36:09.520]   Well, those are those are the library tweets are in the library of Congress.
[00:36:09.520 --> 00:36:11.000]   Yeah, but they're not.
[00:36:11.000 --> 00:36:13.360]   But so is all in the family, right?
[00:36:13.360 --> 00:36:16.360]   It's there as a it's there as a culture.
[00:36:16.360 --> 00:36:19.680]   It's there is a cultural phenomenon.
[00:36:19.680 --> 00:36:21.840]   Stacey, do you know what all the family is?
[00:36:21.840 --> 00:36:22.680]   I just check it out.
[00:36:22.680 --> 00:36:25.360]   So that's the racist TV show, right?
[00:36:25.600 --> 00:36:27.600]   No, aren't you bunker?
[00:36:27.600 --> 00:36:31.560]   That's the racist character on a show that taught America how racist we are.
[00:36:31.560 --> 00:36:33.160]   Yeah, our team bunker.
[00:36:33.160 --> 00:36:34.160]   OK.
[00:36:34.160 --> 00:36:35.160]   Yeah.
[00:36:35.160 --> 00:36:36.600]   Wait a minute.
[00:36:36.600 --> 00:36:40.000]   Olivia Wilde and Harry Styles share a deep love.
[00:36:40.000 --> 00:36:44.600]   See, this is why Twitter is important.
[00:36:44.600 --> 00:36:46.440]   This is why Twitter is important.
[00:36:46.440 --> 00:36:50.920]   Trending story, folks.
[00:36:50.920 --> 00:36:54.040]   By the way, this is a really good question for you.
[00:36:54.040 --> 00:36:55.200]   Really dumb question for you.
[00:36:55.200 --> 00:36:58.760]   Yeah, on my Twitter homepage, I don't see trending.
[00:36:58.760 --> 00:37:00.040]   Where do I get trending?
[00:37:00.040 --> 00:37:01.160]   It's on the right.
[00:37:01.160 --> 00:37:03.720]   Oh, man, I want your settings on the right hand corner.
[00:37:03.720 --> 00:37:05.760]   Do you don't have a what's happening?
[00:37:05.760 --> 00:37:07.000]   You don't have that.
[00:37:07.000 --> 00:37:08.040]   I don't have trending.
[00:37:08.040 --> 00:37:09.040]   No, well, that's it's not.
[00:37:09.040 --> 00:37:10.440]   They don't know what's happening anymore.
[00:37:10.440 --> 00:37:11.440]   It's what's happening.
[00:37:11.440 --> 00:37:12.120]   They don't.
[00:37:12.120 --> 00:37:12.720]   What's happening?
[00:37:12.720 --> 00:37:13.440]   Oh, yeah.
[00:37:13.440 --> 00:37:14.480]   They are called trending.
[00:37:14.480 --> 00:37:18.440]   They added in the you have what's happening on yours.
[00:37:18.440 --> 00:37:20.000]   Mr. Jarvis.
[00:37:20.000 --> 00:37:20.880]   Yes.
[00:37:20.880 --> 00:37:22.720]   Oh, OK, this used to be called trending.
[00:37:22.720 --> 00:37:23.720]   I don't used to be trending.
[00:37:23.720 --> 00:37:24.680]   Well, it used to be simpler.
[00:37:24.680 --> 00:37:26.920]   It used to be just the topics.
[00:37:26.920 --> 00:37:27.920]   Oh, I see.
[00:37:27.920 --> 00:37:32.840]   Look, look, under your headlines, some are slugged news.
[00:37:32.840 --> 00:37:33.360]   Yes.
[00:37:33.360 --> 00:37:36.440]   And some are slug trending, but Olivia
[00:37:36.440 --> 00:37:38.800]   while breaking your silence on Harry Styles isn't either.
[00:37:38.800 --> 00:37:41.680]   So it's neither news.
[00:37:41.680 --> 00:37:42.640]   It's some are sponsored.
[00:37:42.640 --> 00:37:43.440]   This is true.
[00:37:43.440 --> 00:37:44.600]   And some are sponsored.
[00:37:44.600 --> 00:37:49.200]   That's my biggest complaint about Twitter now is and it seems to be only
[00:37:49.200 --> 00:37:51.160]   on the model.
[00:37:51.160 --> 00:37:52.680]   They got to make their cash.
[00:37:52.680 --> 00:37:54.440]   But wait, wait, is the time for your commercial?
[00:37:54.440 --> 00:37:55.400]   Yeah, we got to do that.
[00:37:55.400 --> 00:37:56.640]   It's only.
[00:37:56.640 --> 00:38:01.320]   So I've decided I can't read Twitter on mobile anymore or off.
[00:38:01.320 --> 00:38:03.160]   You use a third party app because.
[00:38:03.160 --> 00:38:07.360]   And the way they do it promoted is the last thing in the tweet.
[00:38:07.360 --> 00:38:08.720]   So you start reading it.
[00:38:08.720 --> 00:38:09.200]   I know.
[00:38:09.200 --> 00:38:12.640]   Yes, it's a real tweet and it's by the way, like on mobile,
[00:38:12.640 --> 00:38:15.040]   it's like every fourth tweet and then you get.
[00:38:15.040 --> 00:38:17.200]   Oh, they bit you can tell you.
[00:38:17.200 --> 00:38:18.360]   I was the one's a month.
[00:38:18.360 --> 00:38:22.360]   I get full most times I could tell round off about that.
[00:38:22.360 --> 00:38:23.360]   It's fake.
[00:38:23.360 --> 00:38:24.320]   It's really bad.
[00:38:24.320 --> 00:38:28.920]   I tend to just use list and notifications on mobile.
[00:38:28.920 --> 00:38:31.200]   I don't even bother just going to my feed.
[00:38:31.200 --> 00:38:31.840]   Yeah.
[00:38:31.840 --> 00:38:32.840]   Yep.
[00:38:32.840 --> 00:38:36.080]   That bogs me because it's you don't know it's promoted.
[00:38:36.080 --> 00:38:37.320]   I get fooled a lot.
[00:38:37.320 --> 00:38:41.000]   I feel like I feel like mine are so ridiculous that I'm like.
[00:38:41.000 --> 00:38:43.080]   So let me just here's my mobile.
[00:38:43.080 --> 00:38:47.360]   So Mike Masnick, the first tweet, then Louisa real tweet.
[00:38:47.360 --> 00:38:48.800]   President Biden.
[00:38:48.800 --> 00:38:49.760]   I think he's real.
[00:38:50.880 --> 00:38:56.520]   Wept Moser, Laurie Garrett, all real people and then underdog fantasy,
[00:38:56.520 --> 00:38:58.400]   which by the way, blue check.
[00:38:58.400 --> 00:39:02.240]   What would you do if you won $2 million from a fantasy football draft
[00:39:02.240 --> 00:39:03.040]   entered today?
[00:39:03.040 --> 00:39:04.800]   Promoted promoted.
[00:39:04.800 --> 00:39:08.440]   It's just like to pull an outbreak on every news site in America.
[00:39:08.440 --> 00:39:08.960]   Promoted.
[00:39:08.960 --> 00:39:10.800]   Yeah, I guess I should have known that one was promoted.
[00:39:10.800 --> 00:39:11.160]   Let's go.
[00:39:11.160 --> 00:39:16.960]   Cherylin, I fall, clean weather channel promoted, promoted, promoted.
[00:39:16.960 --> 00:39:17.760]   New feature.
[00:39:17.760 --> 00:39:20.160]   Wall Street Journal wastes their money promoting to me, man.
[00:39:20.800 --> 00:39:23.800]   Oh, yeah, they spent so much money sending me stuff.
[00:39:23.800 --> 00:39:26.960]   But you don't see this on the, um, this is what puzzles me.
[00:39:26.960 --> 00:39:28.840]   You don't see it on the website.
[00:39:28.840 --> 00:39:29.680]   Nope.
[00:39:29.680 --> 00:39:30.880]   I see it on the website.
[00:39:30.880 --> 00:39:31.880]   Yes.
[00:39:31.880 --> 00:39:33.160]   I see.
[00:39:33.160 --> 00:39:33.880]   OK, here we go.
[00:39:33.880 --> 00:39:36.640]   Wall Street Journal is promoting.
[00:39:36.640 --> 00:39:40.200]   Oh, UPS business is promoting a Wall Street Journal story.
[00:39:40.200 --> 00:39:42.120]   Oh, maybe because I'm blue.
[00:39:42.120 --> 00:39:42.960]   No, you're blue.
[00:39:42.960 --> 00:39:47.120]   Data stacks, Boing Boing, Kevin Boing, real person.
[00:39:47.120 --> 00:39:49.280]   Apple promoted tweet.
[00:39:49.440 --> 00:39:53.160]   So every four or five tweets on the website, I'm getting promoted stuff.
[00:39:53.160 --> 00:39:55.920]   I don't have the same frequency.
[00:39:55.920 --> 00:39:56.640]   Leo's right.
[00:39:56.640 --> 00:39:59.200]   I got higher frequency than any promoted ones.
[00:39:59.200 --> 00:40:03.880]   Am I see I got that was five and then promoted, then I've got six promoted.
[00:40:03.880 --> 00:40:04.920]   And this is a Doritos.
[00:40:04.920 --> 00:40:07.560]   Are you on your, your, this is on your computer?
[00:40:07.560 --> 00:40:11.000]   Yeah, this is about every five for me on the web.
[00:40:11.000 --> 00:40:12.200]   Why don't you look at me?
[00:40:12.200 --> 00:40:13.760]   Oh, I'm looking at latest tweets.
[00:40:13.760 --> 00:40:14.560]   Are you not looking at live?
[00:40:14.560 --> 00:40:17.240]   Maybe if it's if I look, go back to home.
[00:40:17.560 --> 00:40:19.560]   Oh, maybe it's go back home.
[00:40:19.560 --> 00:40:24.240]   Fetterman, Aaron Rupar, Massaman.
[00:40:24.240 --> 00:40:25.440]   No, I don't have any promoted.
[00:40:25.440 --> 00:40:29.160]   No, I just switched to latest Leo and I get promoted.
[00:40:29.160 --> 00:40:31.240]   I'm am I just lucky?
[00:40:31.240 --> 00:40:34.000]   Because it's terrible and mobile.
[00:40:34.000 --> 00:40:38.000]   Yes, I get this in my mobile and web experiences.
[00:40:38.000 --> 00:40:38.840]   I have not yet.
[00:40:38.840 --> 00:40:40.680]   There's none of these is promoted yet.
[00:40:40.680 --> 00:40:42.840]   These are all real and good.
[00:40:42.840 --> 00:40:44.600]   They know you're worthless to the advertiser.
[00:40:45.840 --> 00:40:47.880]   I am a must be.
[00:40:47.880 --> 00:40:49.480]   You are not an MDOW.
[00:40:49.480 --> 00:40:50.600]   You're not monetizing.
[00:40:50.600 --> 00:40:52.480]   I'm counting seriously.
[00:40:52.480 --> 00:40:54.040]   I have yet to see a promoted tweet.
[00:40:54.040 --> 00:40:55.880]   That's impressive.
[00:40:55.880 --> 00:40:56.960]   I've seen like eight.
[00:40:56.960 --> 00:40:57.480]   Wow.
[00:40:57.480 --> 00:40:57.680]   Yeah.
[00:40:57.680 --> 00:40:59.440]   Well, I'd every five or six for me.
[00:40:59.440 --> 00:40:59.960]   Wait, wait, wait.
[00:40:59.960 --> 00:41:01.200]   What browser are you using?
[00:41:01.200 --> 00:41:01.920]   Is that an impact?
[00:41:01.920 --> 00:41:05.640]   Oh, I'm in breaks.
[00:41:05.640 --> 00:41:06.920]   Oh, that's what's happening.
[00:41:06.920 --> 00:41:07.640]   I'm in Chrome.
[00:41:07.640 --> 00:41:09.680]   I have Firefox with the U block origin.
[00:41:09.680 --> 00:41:10.840]   It's blocking him.
[00:41:10.840 --> 00:41:12.440]   That's pretty sophisticated.
[00:41:12.440 --> 00:41:13.600]   I'm surprised you block origin.
[00:41:13.600 --> 00:41:14.480]   It's smart enough to know.
[00:41:15.000 --> 00:41:16.040]   Well, it says promoted.
[00:41:16.040 --> 00:41:18.680]   Well, that's that's hard to do.
[00:41:18.680 --> 00:41:22.320]   I mean, usually they do it by a web IP address or something.
[00:41:22.320 --> 00:41:23.280]   Oh, that's good.
[00:41:23.280 --> 00:41:24.320]   OK, that's what it is.
[00:41:24.320 --> 00:41:25.320]   Oh, good.
[00:41:25.320 --> 00:41:25.880]   Thank you.
[00:41:25.880 --> 00:41:27.400]   So you're robbing Twitter.
[00:41:27.400 --> 00:41:29.960]   Twitter can't make money because of you.
[00:41:29.960 --> 00:41:30.960]   You bucks a month.
[00:41:30.960 --> 00:41:32.320]   I give my blue.
[00:41:32.320 --> 00:41:34.160]   Are you still in $3?
[00:41:34.160 --> 00:41:35.360]   Yeah, I'm still at $3.
[00:41:35.360 --> 00:41:36.520]   They raised me.
[00:41:36.520 --> 00:41:38.400]   I haven't gone back since I was in five.
[00:41:38.400 --> 00:41:40.720]   You got it because you quit and then you came back.
[00:41:40.720 --> 00:41:42.920]   I quit.
[00:41:42.920 --> 00:41:44.040]   I just my cards.
[00:41:44.040 --> 00:41:45.480]   It's inspired or whatever.
[00:41:45.480 --> 00:41:46.920]   I know it's like a year.
[00:41:46.920 --> 00:41:47.800]   It's the same thing.
[00:41:47.800 --> 00:41:49.600]   I'm still battling that.
[00:41:49.600 --> 00:41:50.920]   I keep getting notifications.
[00:41:50.920 --> 00:41:52.600]   So, oh, yeah, we need to update your car.
[00:41:52.600 --> 00:41:55.280]   And I'm like, she's that's another one because I use
[00:41:55.280 --> 00:41:56.400]   so many burner cards.
[00:41:56.400 --> 00:41:58.240]   All right, I'm sorry.
[00:41:58.240 --> 00:41:58.920]   This is really boring.
[00:41:58.920 --> 00:42:01.720]   You know, one of my favorite new Twitter accounts is random
[00:42:01.720 --> 00:42:02.400]   restaurant.
[00:42:02.400 --> 00:42:06.520]   Understore restaurant underscore bot.
[00:42:06.520 --> 00:42:08.040]   And it really is random.
[00:42:08.040 --> 00:42:09.240]   Sometimes they're pizza huts.
[00:42:09.240 --> 00:42:11.720]   Sometimes they're fancy restaurants.
[00:42:11.720 --> 00:42:13.160]   They're from all around the world.
[00:42:13.160 --> 00:42:14.680]   Four pictures from a random restaurant.
[00:42:14.680 --> 00:42:18.680]   Would you think that it would be good for Twitter to force
[00:42:18.680 --> 00:42:22.040]   everybody that's a bot to have bought in the at the end of the
[00:42:22.040 --> 00:42:22.840]   name kind of?
[00:42:22.840 --> 00:42:24.880]   No, good.
[00:42:24.880 --> 00:42:26.080]   But you're honestly used to that.
[00:42:26.080 --> 00:42:26.960]   Yeah, but that's OK.
[00:42:26.960 --> 00:42:28.280]   Then I would still know it's a bot.
[00:42:28.280 --> 00:42:30.840]   Yeah, I've got the word of the bot.
[00:42:30.840 --> 00:42:32.040]   It's a word that bot.
[00:42:32.040 --> 00:42:33.560]   I never see.
[00:42:33.560 --> 00:42:37.320]   But all I ever see is the actual name name random restaurant.
[00:42:37.320 --> 00:42:38.560]   You really have to look.
[00:42:38.560 --> 00:42:40.880]   Why do I want to follow this?
[00:42:41.440 --> 00:42:44.720]   Why do I want to see random tweets from restaurants that I don't
[00:42:44.720 --> 00:42:46.240]   think I'm listening to Jeff.
[00:42:46.240 --> 00:42:48.400]   You love the quirkiness of the Internet.
[00:42:48.400 --> 00:42:48.840]   Just don't.
[00:42:48.840 --> 00:42:50.280]   Yes, clearly.
[00:42:50.280 --> 00:42:52.360]   Any of those taint unfollowing.
[00:42:52.360 --> 00:42:54.680]   It's going to irritate you, Leo.
[00:42:54.680 --> 00:42:55.360]   Unfollowing.
[00:42:55.360 --> 00:42:57.600]   And then you're going to talk to Jeff about how I do it.
[00:42:57.600 --> 00:42:59.720]   I'm I'm I'm I'm unfollowing right now.
[00:42:59.720 --> 00:43:03.960]   And we're going to have to hear Jeff talk about the virtues of
[00:43:03.960 --> 00:43:06.200]   quirkiness, although I do follow cats with jobs.
[00:43:06.200 --> 00:43:07.520]   So I'm kind of with Jeff.
[00:43:07.520 --> 00:43:08.400]   Wait a minute.
[00:43:08.400 --> 00:43:08.840]   What's that?
[00:43:08.840 --> 00:43:10.720]   Cats with jobs.
[00:43:10.720 --> 00:43:13.720]   Is that related to bodega cats?
[00:43:13.720 --> 00:43:21.520]   No, it's cat.
[00:43:21.520 --> 00:43:22.920]   Cat workers.
[00:43:22.920 --> 00:43:32.280]   OK, Lisa got a new Twitter account for you to follow.
[00:43:32.280 --> 00:43:34.040]   Wow.
[00:43:34.040 --> 00:43:35.840]   Just I feel so boring.
[00:43:35.840 --> 00:43:36.920]   You need it.
[00:43:36.920 --> 00:43:39.600]   I don't follow anything like this.
[00:43:39.600 --> 00:43:40.320]   That's good.
[00:43:40.760 --> 00:43:41.560]   Oh, keep going.
[00:43:41.560 --> 00:43:42.680]   That's good.
[00:43:42.680 --> 00:43:44.080]   Keep going there.
[00:43:44.080 --> 00:43:46.600]   The one cat.
[00:43:46.600 --> 00:43:48.200]   Cat with jobs is adorable.
[00:43:48.200 --> 00:43:49.680]   Cat in the money.
[00:43:49.680 --> 00:43:51.520]   Let's make it rain, kitty cat.
[00:43:51.520 --> 00:43:54.920]   Oh, my God.
[00:43:54.920 --> 00:43:56.080]   OK, you're right.
[00:43:56.080 --> 00:43:56.920]   I'm following this one.
[00:43:56.920 --> 00:43:59.520]   See, this is much better than random restaurant tweets.
[00:43:59.520 --> 00:44:01.480]   Hey, yes, yes, it is.
[00:44:01.480 --> 00:44:02.600]   There's a podcast one.
[00:44:02.600 --> 00:44:04.560]   Did you get the one with a cat with the mic?
[00:44:04.560 --> 00:44:05.120]   Cat with a mic.
[00:44:05.120 --> 00:44:05.960]   Do you do?
[00:44:05.960 --> 00:44:07.080]   Yeah, keep going down.
[00:44:07.080 --> 00:44:08.320]   No, we don't.
[00:44:08.720 --> 00:44:10.400]   Look, keep going to Chloe and Ian.
[00:44:10.400 --> 00:44:10.920]   You'll see it.
[00:44:10.920 --> 00:44:11.840]   You try it.
[00:44:11.840 --> 00:44:13.880]   I just want to thank you for I went too far.
[00:44:13.880 --> 00:44:15.440]   I went to the end of the internet.
[00:44:15.440 --> 00:44:16.400]   I've come to the end.
[00:44:16.400 --> 00:44:18.920]   Oh, you're podcasting cat.
[00:44:18.920 --> 00:44:19.280]   Keep it up.
[00:44:19.280 --> 00:44:20.880]   Yeah, there's a podcasting cat.
[00:44:20.880 --> 00:44:22.800]   They're going to like casting cat.
[00:44:22.800 --> 00:44:23.240]   There you go.
[00:44:23.240 --> 00:44:25.640]   Oh, podcast cat.
[00:44:25.640 --> 00:44:27.040]   Hello, podcast cat.
[00:44:27.040 --> 00:44:28.800]   But cat he's wearing a boat.
[00:44:28.800 --> 00:44:30.280]   I used to wear both ties.
[00:44:30.280 --> 00:44:33.600]   It's a pretty dorky in them.
[00:44:33.600 --> 00:44:36.680]   Actually, clearly, Annie Leibowitz did not take this picture.
[00:44:37.560 --> 00:44:43.000]   We are going to talk about this because I don't even know about this story and
[00:44:43.000 --> 00:44:44.280]   y'all are getting me all.
[00:44:44.280 --> 00:44:48.280]   Let me take a break and then we'll come back and tell you why any Leibowitz who is widely
[00:44:48.280 --> 00:44:52.440]   considered be the greatest portrait photographer of all time sucks.
[00:44:52.440 --> 00:44:54.280]   But first.
[00:44:54.280 --> 00:44:58.320]   Wow.
[00:44:58.320 --> 00:44:58.960]   Am I right?
[00:44:58.960 --> 00:44:59.440]   Am I right?
[00:44:59.440 --> 00:45:00.560]   And it's going to explain.
[00:45:00.560 --> 00:45:03.120]   But first I use bad words.
[00:45:03.120 --> 00:45:04.920]   He used bad words before the show.
[00:45:04.920 --> 00:45:06.440]   He's so incensed by this.
[00:45:07.440 --> 00:45:11.240]   This week in Google is brought to you by our great friends at Melissa.
[00:45:11.240 --> 00:45:17.560]   I love these guys and they do something every business needs because every business has
[00:45:17.560 --> 00:45:23.440]   an address book suppliers, contractors, customers, customers that probably the most
[00:45:23.440 --> 00:45:24.200]   important, right?
[00:45:24.200 --> 00:45:29.520]   And you wouldn't believe how often that data goes bad and poor quality data
[00:45:29.520 --> 00:45:31.280]   cost you money on average.
[00:45:31.840 --> 00:45:35.440]   $15 million a year.
[00:45:35.440 --> 00:45:38.960]   If you're a small to medium sized business, every penny counts the longer
[00:45:38.960 --> 00:45:41.280]   poor quality data stays in your system.
[00:45:41.280 --> 00:45:42.760]   The more losses you can accumulate.
[00:45:42.760 --> 00:45:43.560]   How do you lose money?
[00:45:43.560 --> 00:45:48.160]   Well, setting bills to the wrong people, catalogs to the wrong people.
[00:45:48.160 --> 00:45:55.200]   Calling somebody by the wrong name or saying, Oh, hi, Stacey, you live in Florida, right?
[00:45:55.200 --> 00:45:57.560]   No, that's bad.
[00:45:57.560 --> 00:46:00.240]   In fact, it can really be bad for customer service.
[00:46:00.240 --> 00:46:03.640]   If you address somebody with the wrong name or the wrong address and the customer's
[00:46:03.640 --> 00:46:07.480]   calling, they're already frustrated and you say, Oh, we're going to get right on this.
[00:46:07.480 --> 00:46:08.800]   That's just terrible.
[00:46:08.800 --> 00:46:09.920]   You need Melissa.
[00:46:09.920 --> 00:46:16.160]   Melissa is the address expert, the leading provider of global data quality and address
[00:46:16.160 --> 00:46:18.200]   management solutions.
[00:46:18.200 --> 00:46:22.320]   They have tools that will clean your address book in a variety of ways.
[00:46:22.320 --> 00:46:25.960]   You can process an entire address list for accuracy.
[00:46:25.960 --> 00:46:30.520]   You can add fields that aren't there so you could supplement it.
[00:46:30.520 --> 00:46:33.880]   You can parse and standardize first and last names.
[00:46:33.880 --> 00:46:37.680]   Wouldn't want to call somebody Higginbotham Stacey.
[00:46:37.680 --> 00:46:39.640]   No profiling.
[00:46:39.640 --> 00:46:43.360]   You can address and analyze your data to improve its quality over time.
[00:46:43.360 --> 00:46:45.040]   Get rid of bad email addresses.
[00:46:45.040 --> 00:46:48.400]   Melissa helps remove up to 95% of bad email addresses.
[00:46:48.400 --> 00:46:50.080]   They just they know that's a bad address.
[00:46:50.080 --> 00:46:52.080]   Get rid of it.
[00:46:52.080 --> 00:46:56.680]   And it's so easy to use there are all sorts of ways you can use Melissa on-prem.
[00:46:56.680 --> 00:46:57.840]   There's a SaaS app.
[00:46:57.840 --> 00:46:58.840]   There's an API.
[00:46:58.840 --> 00:47:02.640]   There's a cloud-based data cleansing and enrichment tool.
[00:47:02.640 --> 00:47:06.760]   You upload a file to the on the first tab as you do this on the website.
[00:47:06.760 --> 00:47:08.520]   Paste your data into the second tab.
[00:47:08.520 --> 00:47:09.960]   Select data quality service.
[00:47:09.960 --> 00:47:10.960]   There's a variety of them.
[00:47:10.960 --> 00:47:11.960]   Click next.
[00:47:11.960 --> 00:47:12.960]   Map the input fields.
[00:47:12.960 --> 00:47:13.960]   Select the output fields.
[00:47:13.960 --> 00:47:14.960]   Process your list.
[00:47:14.960 --> 00:47:15.960]   Badaboom.
[00:47:15.960 --> 00:47:16.960]   You're done.
[00:47:16.960 --> 00:47:22.000]   There's a lookups app on iOS and Android for onesie tooezy address names.
[00:47:22.000 --> 00:47:24.680]   Email address searches.
[00:47:24.680 --> 00:47:30.880]   Melissa's flexible deployment options in so many ways allow you to get it the way you
[00:47:30.880 --> 00:47:35.480]   want it depending on your preferences, your business size, your budget.
[00:47:35.480 --> 00:47:40.600]   And please understand Melissa treats the data as solid goal just like you do.
[00:47:40.600 --> 00:47:45.000]   They continually undergo independent security audits to reinforce their commitment to data
[00:47:45.000 --> 00:47:48.120]   security and privacy and of course their compliance.
[00:47:48.120 --> 00:47:52.760]   Talk to HIPAA GDPR compliant so you never have to worry about using Melissa.
[00:47:52.760 --> 00:47:56.560]   Melissa's data quality suite and clean suite speaks for itself once again though named
[00:47:56.560 --> 00:47:59.600]   a leader by G2 in 2022.
[00:47:59.600 --> 00:48:04.920]   Melissa is experienced independent 37 years of data quality expertise.
[00:48:04.920 --> 00:48:10.240]   More than 10,000 businesses know Melissa as the address experts.
[00:48:10.240 --> 00:48:13.920]   And by the way, if you sign up for a service level agreement 24/7, world are announced
[00:48:13.920 --> 00:48:16.040]   support from their global support center.
[00:48:16.040 --> 00:48:19.120]   So make sure your customer contact data is up to date.
[00:48:19.120 --> 00:48:21.720]   It'll save you money, not cost you money, save you money.
[00:48:21.720 --> 00:48:22.720]   Get started today.
[00:48:22.720 --> 00:48:25.720]   1,000 records clean for free.
[00:48:25.720 --> 00:48:28.920]   Go to Melissa.com/trit where I the API to.
[00:48:28.920 --> 00:48:29.920]   You could add it easily.
[00:48:29.920 --> 00:48:34.640]   Add Melissa to your customer service software or your shopping cart.
[00:48:34.640 --> 00:48:37.800]   Eliminate data entry mistakes at the point of entry.
[00:48:37.800 --> 00:48:38.800]   Melissa.com/trit.
[00:48:38.800 --> 00:48:44.920]   Thank you, Melissa, for your support.
[00:48:44.920 --> 00:48:46.160]   I love Annie Leibowitz.
[00:48:46.160 --> 00:48:48.640]   I think her pictures are amazing.
[00:48:48.640 --> 00:48:55.360]   And I never even imagined there was a problem until I read an article about the latest Annie
[00:48:55.360 --> 00:49:01.480]   Leibowitz photo essay in Vogue.
[00:49:01.480 --> 00:49:09.600]   She did pictures of the new Supreme Court Justice and Katanji Brown and apparently didn't
[00:49:09.600 --> 00:49:10.600]   do a good job.
[00:49:10.600 --> 00:49:12.800]   What's the story?
[00:49:12.800 --> 00:49:16.600]   Sorry, I was on mute.
[00:49:16.600 --> 00:49:17.600]   It's okay.
[00:49:17.600 --> 00:49:18.600]   You can swear publicly.
[00:49:18.600 --> 00:49:19.600]   It's fine.
[00:49:19.600 --> 00:49:25.960]   Well, she was nabbed for the job to capture beautiful images of our latest Supreme Court
[00:49:25.960 --> 00:49:33.440]   Justice who just made that gum history because she's a great portrait photographer.
[00:49:33.440 --> 00:49:39.560]   However, apparently she's not that good at shooting portraits of people of color, black
[00:49:39.560 --> 00:49:41.160]   folks in general.
[00:49:41.160 --> 00:49:46.240]   And we've talked about this on the show in the past about Google's technology, especially
[00:49:46.240 --> 00:49:47.240]   with the pixels.
[00:49:47.240 --> 00:49:48.240]   Oh, these are awful.
[00:49:48.240 --> 00:49:49.240]   Yes.
[00:49:49.240 --> 00:49:50.240]   Yes.
[00:49:50.240 --> 00:49:51.240]   These are the receipts.
[00:49:51.240 --> 00:49:53.240]   So my work is done.
[00:49:53.240 --> 00:49:54.240]   All right.
[00:49:54.240 --> 00:49:56.960]   So there's two problems here.
[00:49:56.960 --> 00:50:02.440]   And one is pointed out in this article in The Guardian, which I thought was really good.
[00:50:02.440 --> 00:50:08.840]   Let me give her credit by Teo Barrow, which is the content.
[00:50:08.840 --> 00:50:14.720]   She chose to take pictures of Justice Brown Jackson at the Lincoln Memorial.
[00:50:14.720 --> 00:50:20.480]   And I understand when she says, oh, yeah, you see what the problem is here?
[00:50:20.480 --> 00:50:24.520]   It's the white savior complex all over again.
[00:50:24.520 --> 00:50:26.920]   She's at the foot of Abe Lincoln.
[00:50:26.920 --> 00:50:30.880]   But there's also who is Abe is lit brilliantly.
[00:50:30.880 --> 00:50:33.320]   And she's seen only in shadow in the shadows.
[00:50:33.320 --> 00:50:34.320]   Right.
[00:50:34.320 --> 00:50:38.880]   People with styles is very high contrast.
[00:50:38.880 --> 00:50:40.760]   Which is fine.
[00:50:40.760 --> 00:50:41.760]   Fine.
[00:50:41.760 --> 00:50:42.760]   That's great.
[00:50:42.760 --> 00:50:43.840]   Moody, I get that.
[00:50:43.840 --> 00:50:47.440]   But go back to that first image there.
[00:50:47.440 --> 00:50:48.440]   It's pretty dark.
[00:50:48.440 --> 00:50:49.440]   We're looking at this frame.
[00:50:49.440 --> 00:50:54.080]   We have the Lincoln Memorial back there in the background, which is has its own set
[00:50:54.080 --> 00:50:56.040]   of lighting already hitting it.
[00:50:56.040 --> 00:50:57.720]   So it's going to show up.
[00:50:57.720 --> 00:51:00.600]   I know Miss Annie has her own light kit.
[00:51:00.600 --> 00:51:06.120]   She could have put one little dad gum light even at the foot of our new Supreme Court
[00:51:06.120 --> 00:51:11.040]   Justice there just to light her up because she's in a dark overcoat, a dark shirt.
[00:51:11.040 --> 00:51:15.440]   She's in the dad gum shadows show her as the subject here.
[00:51:15.440 --> 00:51:18.120]   Well, and the expression is not exactly regal.
[00:51:18.120 --> 00:51:24.640]   I mean, something if you look at this picture, you don't get an image of a proud Supreme
[00:51:24.640 --> 00:51:28.680]   Court Justice representing the first black woman on the Supreme Court.
[00:51:28.680 --> 00:51:32.680]   With power and dignity, you get a completely different impression.
[00:51:32.680 --> 00:51:33.680]   Yeah.
[00:51:33.680 --> 00:51:34.680]   So I agree.
[00:51:34.680 --> 00:51:37.120]   Maybe is it possible this is unprocessed?
[00:51:37.120 --> 00:51:39.280]   Like this isn't what's going to show up in the magazine.
[00:51:39.280 --> 00:51:41.080]   This is a leaked.
[00:51:41.080 --> 00:51:43.160]   Well, no, it's leaked by her.
[00:51:43.160 --> 00:51:44.160]   It was.
[00:51:44.160 --> 00:51:45.160]   Oh, it's from Annie.
[00:51:45.160 --> 00:51:48.000]   This is from Annie Leibowitz, her Twitter account.
[00:51:48.000 --> 00:51:49.000]   Oh, never mind.
[00:51:49.000 --> 00:51:50.000]   Okay.
[00:51:50.000 --> 00:51:52.640]   So, she's just a framing of that.
[00:51:52.640 --> 00:51:55.680]   I mean, she's proud of this work.
[00:51:55.680 --> 00:51:57.640]   And you mentioned the processing on that.
[00:51:57.640 --> 00:52:01.880]   She has a team of post processors.
[00:52:01.880 --> 00:52:03.560]   Somebody looked at this beyond her.
[00:52:03.560 --> 00:52:05.840]   You know, there's several eyes that looked at this.
[00:52:05.840 --> 00:52:13.200]   And then it went to the approval staff at Vanity or whoever that vote at Vogue and they
[00:52:13.200 --> 00:52:14.760]   approved the two.
[00:52:14.760 --> 00:52:21.640]   So I'm like, who the hell who thought this was okay to put this historic person in the
[00:52:21.640 --> 00:52:26.000]   shadows of this this photograph and say, Hey, this is just beautiful.
[00:52:26.000 --> 00:52:27.360]   This is what we want to promote.
[00:52:27.360 --> 00:52:31.880]   This is how we want to promote this historic event in our country by putting this black
[00:52:31.880 --> 00:52:34.960]   woman in the shadows at the foot of a white statue.
[00:52:34.960 --> 00:52:39.960]   I didn't like that one bit, especially when I know her work is beautiful.
[00:52:39.960 --> 00:52:43.880]   She shoots a lot of other athletes and a lot of other actors, actresses, celebrities,
[00:52:43.880 --> 00:52:47.400]   or what have you name and just come out just outstanding.
[00:52:47.400 --> 00:52:48.680]   I love her work.
[00:52:48.680 --> 00:52:55.480]   I frickin pay for the masterclass subscription specifically to watch her photography class.
[00:52:55.480 --> 00:52:56.480]   Here's another image.
[00:52:56.480 --> 00:52:57.480]   Right.
[00:52:57.480 --> 00:53:03.720]   Of hers from Vogue of another African American, sorry, black actress, which is fine, I think.
[00:53:03.720 --> 00:53:05.840]   Do you not?
[00:53:05.840 --> 00:53:10.120]   So it feels like she didn't do justice to that.
[00:53:10.120 --> 00:53:12.160]   That one's better than some of the other things, but yeah.
[00:53:12.160 --> 00:53:18.360]   There's also if you go down, there's also photos of the cover she shot of Simone Biles
[00:53:18.360 --> 00:53:20.000]   and Viola Davis.
[00:53:20.000 --> 00:53:22.560]   Another mistake, yes.
[00:53:22.560 --> 00:53:23.560]   Yep.
[00:53:23.560 --> 00:53:24.560]   Yes.
[00:53:24.560 --> 00:53:29.200]   You know, do you think she's racist?
[00:53:29.200 --> 00:53:32.080]   I'm not going to say she's racist.
[00:53:32.080 --> 00:53:34.480]   I'm saying that she can do better.
[00:53:34.480 --> 00:53:37.840]   She's a brilliant photographer.
[00:53:37.840 --> 00:53:41.680]   She can do better than what she did.
[00:53:41.680 --> 00:53:45.160]   Google was spending a lot more effort to show people of color.
[00:53:45.160 --> 00:53:46.160]   Right.
[00:53:46.160 --> 00:53:47.960]   I mean, part of their campaign.
[00:53:47.960 --> 00:53:51.120]   There's a picture of John Favreau and Yoda.
[00:53:51.120 --> 00:53:54.800]   Yoda's much better lit than the white guy.
[00:53:54.800 --> 00:53:55.800]   Maybe she'll.
[00:53:55.800 --> 00:53:57.400]   Okay, that saves her.
[00:53:57.400 --> 00:54:00.200]   Maybe she's got a thing for green guys.
[00:54:00.200 --> 00:54:01.200]   Sure.
[00:54:01.200 --> 00:54:02.200]   Right?
[00:54:02.200 --> 00:54:03.200]   Sure.
[00:54:03.200 --> 00:54:04.440]   But dude, it's not.
[00:54:04.440 --> 00:54:06.480]   So you're not accusing her of racism as well.
[00:54:06.480 --> 00:54:08.320]   I'm not going to accuse her of racism.
[00:54:08.320 --> 00:54:09.320]   I'm seeing.
[00:54:09.320 --> 00:54:11.640]   I know she's a much better photographer than that.
[00:54:11.640 --> 00:54:14.120]   Beautiful cover.
[00:54:14.120 --> 00:54:15.120]   That's great.
[00:54:15.120 --> 00:54:16.120]   Yeah.
[00:54:16.120 --> 00:54:20.000]   So the Viola Davis in Simone Biles.
[00:54:20.000 --> 00:54:22.800]   Those are really, really bad images.
[00:54:22.800 --> 00:54:24.040]   It's not just a great reason.
[00:54:24.040 --> 00:54:25.040]   Yeah.
[00:54:25.040 --> 00:54:26.040]   Could be so much.
[00:54:26.040 --> 00:54:28.680]   We're hearing from a photographer who knows how it should be done.
[00:54:28.680 --> 00:54:29.680]   That's the important.
[00:54:29.680 --> 00:54:34.840]   If you go to that tweet of hers where she's showing off the pictures of the Supreme Court
[00:54:34.840 --> 00:54:40.120]   Justice, the portrait shot where it's up close, even that one, this is easy.
[00:54:40.120 --> 00:54:45.440]   It's just doing a simple filter on her just to bring the light up on her.
[00:54:45.440 --> 00:54:46.440]   It's pretty darn hard.
[00:54:46.440 --> 00:54:50.520]   But she does have a light in this image because there's a soft shadow going right down the
[00:54:50.520 --> 00:54:51.520]   bridge of her nose.
[00:54:51.520 --> 00:54:54.080]   So she does have a light over there.
[00:54:54.080 --> 00:54:55.080]   She chose to make a moody.
[00:54:55.080 --> 00:54:56.800]   But she still would be sort of in the shadows.
[00:54:56.800 --> 00:54:57.800]   It's going moody.
[00:54:57.800 --> 00:55:07.560]   Now we know that a lot of film is a chemistry is designed to look good for white people
[00:55:07.560 --> 00:55:09.120]   and does not look good for black people.
[00:55:09.120 --> 00:55:10.120]   This is a story of the problem.
[00:55:10.120 --> 00:55:11.120]   Right.
[00:55:11.120 --> 00:55:16.360]   This is what Google was talking about in their recent campaign for the Pixel 6 images
[00:55:16.360 --> 00:55:19.040]   getting skin tone right.
[00:55:19.040 --> 00:55:21.240]   Maybe Annie's just using the wrong film.
[00:55:21.240 --> 00:55:23.320]   I don't get it.
[00:55:23.320 --> 00:55:29.280]   My problem is she's not the only person to solve that image.
[00:55:29.280 --> 00:55:31.920]   It went through a bunch of levels of approvals.
[00:55:31.920 --> 00:55:35.160]   If you do a search, there are so many articles.
[00:55:35.160 --> 00:55:36.600]   Annie Leibowitz photos blasted.
[00:55:36.600 --> 00:55:38.400]   Yet again, she can't photograph black women.
[00:55:38.400 --> 00:55:39.760]   That's from Digital Camera World.
[00:55:39.760 --> 00:55:40.760]   Boing, boing.
[00:55:40.760 --> 00:55:41.760]   Yahoo.
[00:55:41.760 --> 00:55:43.560]   And you said a petapixel had a petapixel.
[00:55:43.560 --> 00:55:45.560]   That's where I first saw it.
[00:55:45.560 --> 00:55:46.560]   That's where I first saw it.
[00:55:46.560 --> 00:55:47.560]   That's where I first saw it.
[00:55:47.560 --> 00:55:48.560]   That's where I first saw it.
[00:55:48.560 --> 00:55:49.560]   That's where I first saw it.
[00:55:49.560 --> 00:55:50.560]   That's where I first saw it.
[00:55:50.560 --> 00:55:51.560]   That's where I first saw it.
[00:55:51.560 --> 00:55:52.560]   That's where I first saw it.
[00:55:52.560 --> 00:55:53.560]   That's where I first saw it.
[00:55:53.560 --> 00:55:54.560]   That's where I first saw it.
[00:55:54.560 --> 00:55:55.560]   That's where I first saw it.
[00:55:55.560 --> 00:55:56.560]   That's where I first saw it.
[00:55:56.560 --> 00:55:57.560]   That's where I first saw it.
[00:55:57.560 --> 00:55:58.560]   That's where I first saw it.
[00:55:58.560 --> 00:55:59.560]   That's where I first saw it.
[00:55:59.560 --> 00:56:00.560]   That's where I first saw it.
[00:56:00.560 --> 00:56:01.560]   That's where I first saw it.
[00:56:01.560 --> 00:56:02.560]   That's where I first saw it.
[00:56:02.560 --> 00:56:03.560]   That's where I first saw it.
[00:56:03.560 --> 00:56:04.560]   That's where I first saw it.
[00:56:04.560 --> 00:56:05.560]   That's where I first saw it.
[00:56:05.560 --> 00:56:06.560]   That's where I first saw it.
[00:56:06.560 --> 00:56:07.560]   That's where I first saw it.
[00:56:07.560 --> 00:56:08.560]   That's where I first saw it.
[00:56:08.560 --> 00:56:09.560]   That's where I first saw it.
[00:56:09.560 --> 00:56:10.560]   That's where I first saw it.
[00:56:10.560 --> 00:56:11.560]   That's where I first saw it.
[00:56:11.560 --> 00:56:12.560]   That's where I first saw it.
[00:56:12.560 --> 00:56:13.560]   That's where I first saw it.
[00:56:13.560 --> 00:56:14.560]   That's where I first saw it.
[00:56:14.560 --> 00:56:15.560]   That's where I first saw it.
[00:56:15.560 --> 00:56:16.560]   That's where I first saw it.
[00:56:16.560 --> 00:56:17.560]   That's where I first saw it.
[00:56:17.560 --> 00:56:18.560]   That's where I first saw it.
[00:56:18.560 --> 00:56:19.560]   That's where I first saw it.
[00:56:19.560 --> 00:56:20.560]   That's where I first saw it.
[00:56:20.560 --> 00:56:21.560]   That's where I first saw it.
[00:56:21.560 --> 00:56:22.560]   That's where I first saw it.
[00:56:22.560 --> 00:56:23.560]   That's where I first saw it.
[00:56:23.560 --> 00:56:24.560]   That's where I first saw it.
[00:56:24.560 --> 00:56:25.560]   That's where I first saw it.
[00:56:25.560 --> 00:56:26.560]   That's where I first saw it.
[00:56:26.560 --> 00:56:27.560]   That's where I first saw it.
[00:56:27.560 --> 00:56:28.560]   That's where I first saw it.
[00:56:29.560 --> 00:56:30.560]   That's where I first saw it.
[00:56:30.560 --> 00:56:31.560]   That's where I first saw it.
[00:56:31.560 --> 00:56:32.560]   That's where I first saw it.
[00:56:32.560 --> 00:56:33.560]   That's where I first saw it.
[00:56:33.560 --> 00:56:34.560]   That's where I first saw it.
[00:56:34.560 --> 00:56:35.560]   That's where I first saw it.
[00:56:35.560 --> 00:56:36.560]   That's where I first saw it.
[00:56:36.560 --> 00:56:37.560]   That's where I first saw it.
[00:56:37.560 --> 00:56:38.560]   That's where I first saw it.
[00:56:38.560 --> 00:56:39.560]   That's where I first saw it.
[00:56:39.560 --> 00:56:40.560]   That's where I first saw it.
[00:56:40.560 --> 00:56:41.560]   That's where I first saw it.
[00:56:41.560 --> 00:56:42.560]   That's where I first saw it.
[00:56:42.560 --> 00:56:43.560]   That's where I first saw it.
[00:56:43.560 --> 00:56:44.560]   That's where I first saw it.
[00:56:44.560 --> 00:56:45.560]   That's where I first saw it.
[00:56:45.560 --> 00:56:46.560]   That's where I first saw it.
[00:56:46.560 --> 00:56:47.560]   That's where I first saw it.
[00:56:47.560 --> 00:56:48.560]   That's where I first saw it.
[00:56:48.560 --> 00:56:49.560]   That's where I first saw it.
[00:56:49.560 --> 00:56:50.560]   That's where I first saw it.
[00:56:50.560 --> 00:56:51.560]   That's where I first saw it.
[00:56:52.560 --> 00:56:53.560]   That's where I first saw it.
[00:56:53.560 --> 00:56:54.560]   That's where I first saw it.
[00:56:54.560 --> 00:56:55.560]   That's where I first saw it.
[00:56:55.560 --> 00:56:56.560]   That's where I first saw it.
[00:56:56.560 --> 00:56:57.560]   That's where I first saw it.
[00:56:57.560 --> 00:56:58.560]   That's where I first saw it.
[00:56:58.560 --> 00:56:59.560]   That's where I first saw it.
[00:56:59.560 --> 00:57:00.560]   That's where I first saw it.
[00:57:00.560 --> 00:57:01.560]   That's where I first saw it.
[00:57:01.560 --> 00:57:02.560]   That's where I first saw it.
[00:57:02.560 --> 00:57:03.560]   That's where I first saw it.
[00:57:03.560 --> 00:57:04.560]   That's where I first saw it.
[00:57:04.560 --> 00:57:05.560]   That's where I first saw it.
[00:57:05.560 --> 00:57:06.560]   That's where I first saw it.
[00:57:06.560 --> 00:57:07.560]   That's where I first saw it.
[00:57:07.560 --> 00:57:08.560]   That's where I first saw it.
[00:57:08.560 --> 00:57:09.560]   That's where I first saw it.
[00:57:09.560 --> 00:57:10.560]   That's where I first saw it.
[00:57:10.560 --> 00:57:11.560]   That's where I first saw it.
[00:57:11.560 --> 00:57:12.560]   That's where I first saw it.
[00:57:12.560 --> 00:57:13.560]   That's where I first saw it.
[00:57:13.560 --> 00:57:14.560]   That's where I first saw it.
[00:57:14.560 --> 00:57:15.560]   That's where I first saw it.
[00:57:15.560 --> 00:57:16.560]   That's where I first saw it.
[00:57:16.560 --> 00:57:17.560]   That's where I first saw it.
[00:57:17.560 --> 00:57:18.560]   That's where I first saw it.
[00:57:18.560 --> 00:57:19.560]   That's where I first saw it.
[00:57:19.560 --> 00:57:20.560]   That's where I first saw it.
[00:57:20.560 --> 00:57:21.560]   That's where I first saw it.
[00:57:21.560 --> 00:57:22.560]   That's where I first saw it.
[00:57:22.560 --> 00:57:23.560]   That's where I first saw it.
[00:57:23.560 --> 00:57:24.560]   That's where I first saw it.
[00:57:24.560 --> 00:57:25.560]   That's where I first saw it.
[00:57:25.560 --> 00:57:26.560]   That's where I first saw it.
[00:57:26.560 --> 00:57:27.560]   That's where I first saw it.
[00:57:27.560 --> 00:57:28.560]   That's where I first saw it.
[00:57:28.560 --> 00:57:29.560]   That's where I first saw it.
[00:57:29.560 --> 00:57:30.560]   That's where I first saw it.
[00:57:30.560 --> 00:57:31.560]   That's where I first saw it.
[00:57:31.560 --> 00:57:32.560]   That's where I first saw it.
[00:57:32.560 --> 00:57:33.560]   That's where I first saw it.
[00:57:33.560 --> 00:57:34.560]   That's where I first saw it.
[00:57:34.560 --> 00:57:35.560]   That's where I first saw it.
[00:57:35.560 --> 00:57:36.560]   That's where I first saw it.
[00:57:37.560 --> 00:57:38.560]   That's where I first saw it.
[00:57:38.560 --> 00:57:39.560]   That's where I first saw it.
[00:57:39.560 --> 00:57:40.560]   That's where I first saw it.
[00:57:40.560 --> 00:57:41.560]   That's where I first saw it.
[00:57:41.560 --> 00:57:42.560]   That's where I first saw it.
[00:57:42.560 --> 00:57:43.560]   That's where I first saw it.
[00:57:43.560 --> 00:57:44.560]   That's where I first saw it.
[00:57:44.560 --> 00:57:45.560]   That's where I first saw it.
[00:57:45.560 --> 00:57:46.560]   That's where I first saw it.
[00:57:46.560 --> 00:57:47.560]   That's where I first saw it.
[00:57:47.560 --> 00:57:48.560]   That's where I first saw it.
[00:57:48.560 --> 00:57:49.560]   That's where I first saw it.
[00:57:49.560 --> 00:57:50.560]   That's where I first saw it.
[00:57:50.560 --> 00:57:51.560]   That's where I first saw it.
[00:57:51.560 --> 00:57:52.560]   That's where I first saw it.
[00:57:52.560 --> 00:57:53.560]   That's where I first saw it.
[00:57:53.560 --> 00:57:54.560]   That's where I first saw it.
[00:57:54.560 --> 00:57:55.560]   That's where I first saw it.
[00:57:55.560 --> 00:57:56.560]   That's where I first saw it.
[00:57:56.560 --> 00:57:57.560]   That's where I first saw it.
[00:57:57.560 --> 00:57:58.560]   That's where I first saw it.
[00:57:58.560 --> 00:57:59.560]   That's where I first saw it.
[00:58:00.560 --> 00:58:01.560]   That's where I first saw it.
[00:58:01.560 --> 00:58:02.560]   That's where I first saw it.
[00:58:02.560 --> 00:58:03.560]   That's where I first saw it.
[00:58:03.560 --> 00:58:04.560]   That's where I first saw it.
[00:58:04.560 --> 00:58:05.560]   That's where I first saw it.
[00:58:05.560 --> 00:58:06.560]   That's where I first saw it.
[00:58:06.560 --> 00:58:07.560]   That's where I first saw it.
[00:58:07.560 --> 00:58:08.560]   That's where I first saw it.
[00:58:08.560 --> 00:58:09.560]   That's where I first saw it.
[00:58:09.560 --> 00:58:10.560]   That's where I first saw it.
[00:58:10.560 --> 00:58:11.560]   That's where I first saw it.
[00:58:11.560 --> 00:58:12.560]   That's where I first saw it.
[00:58:12.560 --> 00:58:13.560]   That's where I first saw it.
[00:58:13.560 --> 00:58:14.560]   That's where I first saw it.
[00:58:14.560 --> 00:58:15.560]   That's where I first saw it.
[00:58:15.560 --> 00:58:16.560]   That's where I first saw it.
[00:58:16.560 --> 00:58:17.560]   That's where I first saw it.
[00:58:17.560 --> 00:58:18.560]   That's where I first saw it.
[00:58:18.560 --> 00:58:19.560]   That's where I first saw it.
[00:58:19.560 --> 00:58:20.560]   That's where I first saw it.
[00:58:20.560 --> 00:58:21.560]   That's where I first saw it.
[00:58:21.560 --> 00:58:22.560]   That's where I first saw it.
[00:58:23.560 --> 00:58:24.560]   That's where I first saw it.
[00:58:24.560 --> 00:58:25.560]   That's where I first saw it.
[00:58:25.560 --> 00:58:26.560]   That's where I first saw it.
[00:58:26.560 --> 00:58:27.560]   That's where I first saw it.
[00:58:27.560 --> 00:58:28.560]   That's where I first saw it.
[00:58:28.560 --> 00:58:29.560]   That's where I first saw it.
[00:58:29.560 --> 00:58:30.560]   That's where I first saw it.
[00:58:30.560 --> 00:58:31.560]   That's where I first saw it.
[00:58:31.560 --> 00:58:32.560]   That's where I first saw it.
[00:58:32.560 --> 00:58:33.560]   That's where I first saw it.
[00:58:33.560 --> 00:58:34.560]   That's where I first saw it.
[00:58:34.560 --> 00:58:35.560]   That's where I first saw it.
[00:58:35.560 --> 00:58:36.560]   That's where I first saw it.
[00:58:36.560 --> 00:58:37.560]   That's where I first saw it.
[00:58:37.560 --> 00:58:38.560]   That's where I first saw it.
[00:58:38.560 --> 00:58:39.560]   That's where I first saw it.
[00:58:39.560 --> 00:58:40.560]   That's where I first saw it.
[00:58:40.560 --> 00:58:41.560]   That's where I first saw it.
[00:58:41.560 --> 00:58:42.560]   That's where I first saw it.
[00:58:42.560 --> 00:58:43.560]   That's where I first saw it.
[00:58:43.560 --> 00:58:44.560]   That's where I first saw it.
[00:58:44.560 --> 00:58:45.560]   That's where I first saw it.
[00:58:46.560 --> 00:58:47.560]   That's where I first saw it.
[00:58:47.560 --> 00:58:48.560]   That's where I first saw it.
[00:58:48.560 --> 00:58:49.560]   That's where I first saw it.
[00:58:49.560 --> 00:58:50.560]   That's where I first saw it.
[00:58:50.560 --> 00:58:51.560]   That's where I first saw it.
[00:58:51.560 --> 00:58:52.560]   That's where I first saw it.
[00:58:52.560 --> 00:58:53.560]   That's where I first saw it.
[00:58:53.560 --> 00:58:54.560]   That's where I first saw it.
[00:58:54.560 --> 00:58:55.560]   That's where I first saw it.
[00:58:55.560 --> 00:58:56.560]   That's where I first saw it.
[00:58:56.560 --> 00:58:57.560]   That's where I first saw it.
[00:58:57.560 --> 00:58:58.560]   That's where I first saw it.
[00:58:58.560 --> 00:58:59.560]   That's where I first saw it.
[00:58:59.560 --> 00:59:00.560]   That's where I first saw it.
[00:59:00.560 --> 00:59:01.560]   That's where I first saw it.
[00:59:01.560 --> 00:59:02.560]   That's where I first saw it.
[00:59:02.560 --> 00:59:03.560]   That's where I first saw it.
[00:59:03.560 --> 00:59:04.560]   That's where I first saw it.
[00:59:04.560 --> 00:59:05.560]   That's where I first saw it.
[00:59:05.560 --> 00:59:06.560]   That's where I first saw it.
[00:59:06.560 --> 00:59:07.560]   That's where I first saw it.
[00:59:07.560 --> 00:59:08.560]   That's where I first saw it.
[00:59:08.560 --> 00:59:09.560]   That's where I first saw it.
[00:59:09.560 --> 00:59:10.560]   That's where I first saw it.
[00:59:10.560 --> 00:59:11.560]   That's where I first saw it.
[00:59:11.560 --> 00:59:12.560]   That's where I first saw it.
[00:59:12.560 --> 00:59:13.560]   That's where I first saw it.
[00:59:13.560 --> 00:59:14.560]   That's where I first saw it.
[00:59:14.560 --> 00:59:15.560]   That's where I first saw it.
[00:59:15.560 --> 00:59:16.560]   That's where I first saw it.
[00:59:16.560 --> 00:59:17.560]   That's where I first saw it.
[00:59:17.560 --> 00:59:18.560]   That's where I first saw it.
[00:59:18.560 --> 00:59:19.560]   That's where I first saw it.
[00:59:19.560 --> 00:59:20.560]   That's where I first saw it.
[00:59:20.560 --> 00:59:21.560]   That's where I first saw it.
[00:59:21.560 --> 00:59:22.560]   That's where I first saw it.
[00:59:22.560 --> 00:59:23.560]   That's where I first saw it.
[00:59:23.560 --> 00:59:24.560]   That's where I first saw it.
[00:59:24.560 --> 00:59:25.560]   That's where I first saw it.
[00:59:25.560 --> 00:59:26.560]   That's where I first saw it.
[00:59:26.560 --> 00:59:27.560]   That's where I first saw it.
[00:59:27.560 --> 00:59:28.560]   That's where I first saw it.
[00:59:28.560 --> 00:59:29.560]   That's where I first saw it.
[00:59:29.560 --> 00:59:30.560]   That's where I first saw it.
[00:59:30.560 --> 00:59:31.560]   That's where I first saw it.
[00:59:31.560 --> 00:59:32.560]   That's where I first saw it.
[00:59:32.560 --> 00:59:33.560]   That's where I first saw it.
[00:59:33.560 --> 00:59:34.560]   That's where I first saw it.
[00:59:34.560 --> 00:59:35.560]   That's where I first saw it.
[00:59:35.560 --> 00:59:36.560]   That's where I first saw it.
[00:59:36.560 --> 00:59:37.560]   That's where I first saw it.
[00:59:37.560 --> 00:59:38.560]   That's where I first saw it.
[00:59:38.560 --> 00:59:39.560]   That's where I first saw it.
[00:59:39.560 --> 00:59:40.560]   That's where I first saw it.
[00:59:40.560 --> 00:59:41.560]   That's where I first saw it.
[00:59:41.560 --> 00:59:42.560]   That's where I first saw it.
[00:59:42.560 --> 00:59:43.560]   That's where I first saw it.
[00:59:43.560 --> 00:59:44.560]   That's where I first saw it.
[00:59:44.560 --> 00:59:45.560]   That's where I first saw it.
[00:59:45.560 --> 00:59:46.560]   That's where I first saw it.
[00:59:46.560 --> 00:59:47.560]   That's where I first saw it.
[00:59:47.560 --> 00:59:48.560]   That's where I first saw it.
[00:59:48.560 --> 00:59:49.560]   That's where I first saw it.
[00:59:49.560 --> 00:59:50.560]   That's where I first saw it.
[00:59:50.560 --> 00:59:51.560]   That's where I first saw it.
[00:59:51.560 --> 00:59:52.560]   That's where I first saw it.
[00:59:52.560 --> 00:59:53.560]   That's where I first saw it.
[00:59:53.560 --> 00:59:54.560]   That's where I first saw it.
[00:59:54.560 --> 00:59:55.560]   That's where I first saw it.
[00:59:55.560 --> 00:59:56.560]   That's where I first saw it.
[00:59:56.560 --> 00:59:57.560]   That's where I first saw it.
[00:59:57.560 --> 00:59:58.560]   That's where I first saw it.
[00:59:58.560 --> 00:59:59.560]   That's where I first saw it.
[00:59:59.560 --> 01:00:00.560]   That's where I first saw it.
[01:00:00.560 --> 01:00:01.560]   That's where I first saw it.
[01:00:01.560 --> 01:00:02.560]   That's where I first saw it.
[01:00:02.560 --> 01:00:03.560]   That's where I first saw it.
[01:00:03.560 --> 01:00:04.560]   That's where I first saw it.
[01:00:04.560 --> 01:00:05.560]   That's where I first saw it.
[01:00:05.560 --> 01:00:06.560]   That's where I first saw it.
[01:00:06.560 --> 01:00:07.560]   That's where I first saw it.
[01:00:07.560 --> 01:00:08.560]   That's where I first saw it.
[01:00:08.560 --> 01:00:09.560]   That's where I first saw it.
[01:00:09.560 --> 01:00:10.560]   That's where I first saw it.
[01:00:10.560 --> 01:00:11.560]   That's where I first saw it.
[01:00:11.560 --> 01:00:12.560]   That's where I first saw it.
[01:00:12.560 --> 01:00:13.560]   That's where I first saw it.
[01:00:13.560 --> 01:00:14.560]   That's where I first saw it.
[01:00:14.560 --> 01:00:15.560]   That's where I first saw it.
[01:00:15.560 --> 01:00:16.560]   That's where I first saw it.
[01:00:16.560 --> 01:00:17.560]   That's where I first saw it.
[01:00:17.560 --> 01:00:18.560]   That's where I first saw it.
[01:00:18.560 --> 01:00:19.560]   That's where I first saw it.
[01:00:19.560 --> 01:00:20.560]   That's where I first saw it.
[01:00:20.560 --> 01:00:21.560]   That's where I first saw it.
[01:00:21.560 --> 01:00:22.560]   That's where I first saw it.
[01:00:22.560 --> 01:00:23.560]   That's where I first saw it.
[01:00:23.560 --> 01:00:24.560]   That's where I first saw it.
[01:00:24.560 --> 01:00:25.560]   That's where I first saw it.
[01:00:25.560 --> 01:00:26.560]   That's where I first saw it.
[01:00:26.560 --> 01:00:27.560]   That's where I first saw it.
[01:00:27.560 --> 01:00:28.560]   That's where I first saw it.
[01:00:28.560 --> 01:00:29.560]   That's where I first saw it.
[01:00:29.560 --> 01:00:30.560]   That's where I first saw it.
[01:00:30.560 --> 01:00:31.560]   That's where I first saw it.
[01:00:31.560 --> 01:00:32.560]   That's where I first saw it.
[01:00:32.560 --> 01:00:33.560]   That's where I first saw it.
[01:00:33.560 --> 01:00:34.560]   That's where I first saw it.
[01:00:34.560 --> 01:00:35.560]   That's where I first saw it.
[01:00:35.560 --> 01:00:36.560]   That's where I first saw it.
[01:00:36.560 --> 01:00:37.560]   That's where I first saw it.
[01:00:37.560 --> 01:00:38.560]   That's where I first saw it.
[01:00:38.560 --> 01:00:39.560]   That's where I first saw it.
[01:00:39.560 --> 01:00:40.560]   That's where I first saw it.
[01:00:40.560 --> 01:00:41.560]   That's where I first saw it.
[01:00:41.560 --> 01:00:42.560]   That's where I first saw it.
[01:00:42.560 --> 01:00:43.560]   That's where I first saw it.
[01:00:43.560 --> 01:00:44.560]   That's where I first saw it.
[01:00:44.560 --> 01:00:45.560]   That's where I first saw it.
[01:00:45.560 --> 01:00:46.560]   That's where I first saw it.
[01:00:46.560 --> 01:00:47.560]   That's where I first saw it.
[01:00:47.560 --> 01:00:48.560]   That's where I first saw it.
[01:00:48.560 --> 01:00:49.560]   That's where I first saw it.
[01:00:49.560 --> 01:00:50.560]   That's where I first saw it.
[01:00:50.560 --> 01:00:51.560]   That's where I first saw it.
[01:00:51.560 --> 01:00:52.560]   That's where I first saw it.
[01:00:52.560 --> 01:00:53.560]   That's where I first saw it.
[01:00:53.560 --> 01:00:54.560]   That's where I first saw it.
[01:00:54.560 --> 01:00:55.560]   That's where I first saw it.
[01:00:55.560 --> 01:00:56.560]   That's where I first saw it.
[01:00:56.560 --> 01:00:57.560]   That's where I first saw it.
[01:00:57.560 --> 01:00:58.560]   That's where I first saw it.
[01:00:58.560 --> 01:00:59.560]   That's where I first saw it.
[01:00:59.560 --> 01:01:00.560]   That's where I first saw it.
[01:01:00.560 --> 01:01:01.560]   That's where I first saw it.
[01:01:01.560 --> 01:01:02.560]   That's where I first saw it.
[01:01:02.560 --> 01:01:03.560]   That's where I first saw it.
[01:01:03.560 --> 01:01:04.560]   That's where I first saw it.
[01:01:04.560 --> 01:01:05.560]   That's where I first saw it.
[01:01:05.560 --> 01:01:06.560]   That's where I first saw it.
[01:01:06.560 --> 01:01:07.560]   That's where I first saw it.
[01:01:07.560 --> 01:01:08.560]   That's where I first saw it.
[01:01:08.560 --> 01:01:09.560]   That's where I first saw it.
[01:01:09.560 --> 01:01:10.560]   That's where I first saw it.
[01:01:10.560 --> 01:01:11.560]   That's where I first saw it.
[01:01:11.560 --> 01:01:12.560]   That's where I first saw it.
[01:01:12.560 --> 01:01:13.560]   That's where I first saw it.
[01:01:13.560 --> 01:01:14.560]   That's where I first saw it.
[01:01:14.560 --> 01:01:15.560]   That's where I first saw it.
[01:01:15.560 --> 01:01:16.560]   That's where I first saw it.
[01:01:16.560 --> 01:01:17.560]   That's where I first saw it.
[01:01:17.560 --> 01:01:18.560]   That's where I first saw it.
[01:01:18.560 --> 01:01:19.560]   That's where I first saw it.
[01:01:19.560 --> 01:01:20.560]   That's where I first saw it.
[01:01:20.560 --> 01:01:21.560]   That's where I first saw it.
[01:01:21.560 --> 01:01:22.560]   That's where I first saw it.
[01:01:22.560 --> 01:01:23.560]   That's where I first saw it.
[01:01:23.560 --> 01:01:24.560]   That's where I first saw it.
[01:01:24.560 --> 01:01:25.560]   That's where I first saw it.
[01:01:25.560 --> 01:01:26.560]   That's where I first saw it.
[01:01:26.560 --> 01:01:27.560]   That's where I first saw it.
[01:01:27.560 --> 01:01:28.560]   That's where I first saw it.
[01:01:28.560 --> 01:01:29.560]   That's where I first saw it.
[01:01:29.560 --> 01:01:30.560]   That's where I first saw it.
[01:01:30.560 --> 01:01:31.560]   That's where I first saw it.
[01:01:31.560 --> 01:01:32.560]   That's where I first saw it.
[01:01:32.560 --> 01:01:33.560]   That's where I first saw it.
[01:01:33.560 --> 01:01:34.560]   That's where I first saw it.
[01:01:34.560 --> 01:01:35.560]   That's where I first saw it.
[01:01:35.560 --> 01:01:36.560]   That's where I first saw it.
[01:01:36.560 --> 01:01:37.560]   That's where I first saw it.
[01:01:37.560 --> 01:01:38.560]   That's where I first saw it.
[01:01:38.560 --> 01:01:39.560]   That's where I first saw it.
[01:01:39.560 --> 01:01:40.560]   That's where I first saw it.
[01:01:40.560 --> 01:01:41.560]   That's where I first saw it.
[01:01:41.560 --> 01:01:42.560]   That's where I first saw it.
[01:01:42.560 --> 01:01:43.560]   That's where I first saw it.
[01:01:43.560 --> 01:01:44.560]   That's where I first saw it.
[01:01:44.560 --> 01:01:45.560]   That's where I first saw it.
[01:01:45.560 --> 01:01:46.560]   That's where I first saw it.
[01:01:46.560 --> 01:01:47.560]   That's where I first saw it.
[01:01:47.560 --> 01:01:48.560]   That's where I first saw it.
[01:01:48.560 --> 01:01:49.560]   That's where I first saw it.
[01:01:49.560 --> 01:01:50.560]   That's where I first saw it.
[01:01:50.560 --> 01:01:51.560]   That's where I first saw it.
[01:01:51.560 --> 01:01:52.560]   That's where I first saw it.
[01:01:52.560 --> 01:01:53.560]   That's where I first saw it.
[01:01:53.560 --> 01:01:54.560]   That's where I first saw it.
[01:01:54.560 --> 01:01:55.560]   That's where I first saw it.
[01:01:55.560 --> 01:01:56.560]   That's where I first saw it.
[01:01:56.560 --> 01:01:57.560]   That's where I first saw it.
[01:01:57.560 --> 01:01:58.560]   That's where I first saw it.
[01:01:58.560 --> 01:01:59.560]   That's where I first saw it.
[01:01:59.560 --> 01:02:00.560]   That's where I first saw it.
[01:02:00.560 --> 01:02:01.560]   That's where I first saw it.
[01:02:01.560 --> 01:02:02.560]   That's where I first saw it.
[01:02:02.560 --> 01:02:03.560]   That's where I first saw it.
[01:02:03.560 --> 01:02:04.560]   That's where I first saw it.
[01:02:05.560 --> 01:02:15.220]   There's a good article comparing these AI generated images from, and this is Petapixel,
[01:02:15.220 --> 01:02:21.400]   from Dolly to Stable Diffusion, and the one we were just talking about, which is actually
[01:02:21.400 --> 01:02:24.880]   in a Discord server, mid-journey.
[01:02:24.880 --> 01:02:26.120]   All three work the same way.
[01:02:26.120 --> 01:02:28.200]   You give them a text description of an image.
[01:02:28.200 --> 01:02:30.520]   They generate an image.
[01:02:30.520 --> 01:02:34.400]   Matt Grocoot, who wrote this for Petapixel, did a great job.
[01:02:34.400 --> 01:02:39.560]   He's writing up Fabian Stelzer's research where Stelzer put together images.
[01:02:39.560 --> 01:02:44.400]   It's actually, we should go to the Stelzer's Twitter thread, where he put together images
[01:02:44.400 --> 01:02:47.440]   with the same prompt from all three.
[01:02:47.440 --> 01:02:48.440]   So smart.
[01:02:48.440 --> 01:02:50.440]   That's pretty awesome.
[01:02:50.440 --> 01:02:51.440]   Pretty awesome.
[01:02:51.440 --> 01:02:52.920]   This is really interesting.
[01:02:52.920 --> 01:02:56.760]   Film still, portrait of an old man, wrinkles dignified, look gray silver here.
[01:02:56.760 --> 01:02:57.760]   Peculiar nose.
[01:02:57.760 --> 01:02:59.280]   I'm surprised they didn't get me.
[01:02:59.280 --> 01:03:01.720]   Why is eternal wisdom in the UI?
[01:03:01.720 --> 01:03:05.560]   Incredible lighting and camera, depth of field bokeh, screenshot from Hollywood.
[01:03:05.560 --> 01:03:08.400]   See how Dolly could do with our Supreme Court justice.
[01:03:08.400 --> 01:03:10.840]   Isn't this the differences here?
[01:03:10.840 --> 01:03:14.320]   Who do you think is the better, or is it not even a question of better?
[01:03:14.320 --> 01:03:15.320]   It's all about taste.
[01:03:15.320 --> 01:03:16.320]   That's the point.
[01:03:16.320 --> 01:03:19.240]   I think Dolly's is the most, it could be real.
[01:03:19.240 --> 01:03:21.400]   Well, so could Stable Diffusion.
[01:03:21.400 --> 01:03:23.880]   Mid-journey looks like a cartoon.
[01:03:23.880 --> 01:03:25.400]   Mid-journey looks like a hobby.
[01:03:25.400 --> 01:03:26.400]   It's creativity.
[01:03:26.400 --> 01:03:27.400]   Yeah.
[01:03:27.400 --> 01:03:36.800]   So here's Beatle's Lego set, catalog photograph.
[01:03:36.800 --> 01:03:38.000]   So I tried to get Dolly.
[01:03:38.000 --> 01:03:40.320]   I finally got in to the beta.
[01:03:40.320 --> 01:03:41.320]   Yeah.
[01:03:41.320 --> 01:03:44.200]   And for my book, I thought maybe I can get a book cover out of this.
[01:03:44.200 --> 01:03:46.400]   Gutenberg parenthesis, you think you can figure it out?
[01:03:46.400 --> 01:03:48.200]   It just completely befuddled it.
[01:03:48.200 --> 01:03:49.200]   How pretty impressed.
[01:03:49.200 --> 01:03:50.200]   Wow.
[01:03:50.200 --> 01:03:51.200]   How about this one?
[01:03:51.200 --> 01:03:54.760]   A human anatomical heart made of flowers, bestel, masterpiece.
[01:03:54.760 --> 01:03:57.760]   Now this one I love mid-journey.
[01:03:57.760 --> 01:03:59.000]   Oh, I love mid-journey.
[01:03:59.000 --> 01:04:00.000]   That's the creepy line.
[01:04:00.000 --> 01:04:02.200]   Oh, you think that's creepy?
[01:04:02.200 --> 01:04:04.400]   Oh, I love it.
[01:04:04.400 --> 01:04:07.640]   Which do you like, Stacey?
[01:04:07.640 --> 01:04:15.080]   Well, Dolly, no, I'm like, Stable Diffusion is the silliest.
[01:04:15.080 --> 01:04:20.200]   I like Dolly too, because mid-journey is prettier and it's more iconic.
[01:04:20.200 --> 01:04:21.840]   It looks like real art to me.
[01:04:21.840 --> 01:04:23.760]   That looks like you would see that in here.
[01:04:23.760 --> 01:04:24.760]   What is art, Leo?
[01:04:24.760 --> 01:04:25.760]   I don't know.
[01:04:25.760 --> 01:04:26.760]   What is art?
[01:04:26.760 --> 01:04:27.760]   What is art?
[01:04:27.760 --> 01:04:30.720]   Behind the scenes shooting of the moon landing Hollywood studio 1969.
[01:04:30.720 --> 01:04:33.760]   That's pretty funny.
[01:04:33.760 --> 01:04:34.760]   That's funny.
[01:04:34.760 --> 01:04:35.760]   What's really interesting.
[01:04:35.760 --> 01:04:39.480]   Stable Diffusion looks closer to all the existing moon photos.
[01:04:39.480 --> 01:04:40.480]   Right.
[01:04:40.480 --> 01:04:43.240]   Mid-journey turns into a lot of cinematic stuff.
[01:04:43.240 --> 01:04:46.400]   So should we just do one?
[01:04:46.400 --> 01:04:47.880]   Yeah, go ahead.
[01:04:47.880 --> 01:04:50.000]   Let's just do, I'll go into the newbies room.
[01:04:50.000 --> 01:04:56.400]   Mid-journey, use free to try for a certain number of images and then you can pay for it.
[01:04:56.400 --> 01:05:01.480]   Here's somebody's Spider-Man with wings and a suit of armor standing above his city skyline.
[01:05:01.480 --> 01:05:05.680]   Think of, you think of some phrase that I could type in here.
[01:05:05.680 --> 01:05:08.880]   It takes about a minute, so we'll have to do something else while we're--
[01:05:08.880 --> 01:05:11.960]   Do you give me the same podcast legend?
[01:05:11.960 --> 01:05:17.160]   The Twitter account I was just on is Fabians.eth.
[01:05:17.160 --> 01:05:22.160]   I'm sorry, @FabiansStelzer, S-T-E-L-Z-E-R.
[01:05:22.160 --> 01:05:23.160]   What about--
[01:05:23.160 --> 01:05:25.480]   He's the one who did all of these.
[01:05:25.480 --> 01:05:27.280]   Podcast legend having lunch.
[01:05:27.280 --> 01:05:28.960]   That's good, I like it.
[01:05:28.960 --> 01:05:32.240]   Podcast legend having lunch.
[01:05:32.240 --> 01:05:34.960]   Do you think it does better if you have a name or no?
[01:05:34.960 --> 01:05:36.160]   No, it doesn't.
[01:05:36.160 --> 01:05:37.160]   All right.
[01:05:37.160 --> 01:05:38.680]   No, I didn't have any names in the other ones.
[01:05:38.680 --> 01:05:42.320]   Do it in Dolly, if you would, Jeff, and then we can--
[01:05:42.320 --> 01:05:44.080]   Oh, I've got to get into it.
[01:05:44.080 --> 01:05:45.080]   Oh, I'm waiting.
[01:05:45.080 --> 01:05:50.520]   So it's going to be a while because I think it has to chew on it for a little bit.
[01:05:50.520 --> 01:05:51.520]   Right.
[01:05:51.520 --> 01:05:58.280]   But yeah, what I noticed is that mid-journey tends to have a bit of a cinematic feel to
[01:05:58.280 --> 01:06:01.600]   get a lot of the blue and blue and teal look.
[01:06:01.600 --> 01:06:02.600]   Yeah.
[01:06:02.600 --> 01:06:03.600]   And then it has a bit of--
[01:06:03.600 --> 01:06:04.600]   It's more moody.
[01:06:04.600 --> 01:06:08.040]   Most of them tend to have this soft, gozier blur on it too.
[01:06:08.040 --> 01:06:09.040]   Here's a--
[01:06:09.040 --> 01:06:11.640]   Here's a photo-realistic potato river.
[01:06:11.640 --> 01:06:20.240]   Here's a-- Oh, for you, Kevin King, a megalodon breaking the surface of a huge wave chasing
[01:06:20.240 --> 01:06:21.240]   an orca.
[01:06:21.240 --> 01:06:22.240]   Okay.
[01:06:22.240 --> 01:06:23.240]   What do you want me to say again?
[01:06:23.240 --> 01:06:24.240]   What do we say?
[01:06:24.240 --> 01:06:25.240]   Podcast--
[01:06:25.240 --> 01:06:26.720]   Podcast legend having lunch.
[01:06:26.720 --> 01:06:27.960]   Having lunch.
[01:06:27.960 --> 01:06:29.960]   That seems like it might not be long enough.
[01:06:29.960 --> 01:06:31.520]   Yeah, I may need more.
[01:06:31.520 --> 01:06:32.520]   Proms.
[01:06:32.520 --> 01:06:33.520]   You want me to say?
[01:06:33.520 --> 01:06:35.520]   The style of-- what's this name?
[01:06:35.520 --> 01:06:36.520]   Hooper.
[01:06:36.520 --> 01:06:37.520]   No.
[01:06:37.520 --> 01:06:38.520]   The--
[01:06:38.520 --> 01:06:39.520]   The--
[01:06:39.520 --> 01:06:40.520]   The--
[01:06:40.520 --> 01:06:48.280]   I'll never forget-- I don't want to say this.
[01:06:48.280 --> 01:06:54.240]   I was talking to Lisa Carney here recently.
[01:06:54.240 --> 01:06:57.120]   She's been on my show a time or two.
[01:06:57.120 --> 01:07:00.040]   She's a photo finisher.
[01:07:00.040 --> 01:07:05.360]   Where basically she takes a lot of the images that you see on ABC Netflix, all the popular
[01:07:05.360 --> 01:07:10.440]   TV shows and movies and she puts their art together.
[01:07:10.440 --> 01:07:12.520]   She really talents it.
[01:07:12.520 --> 01:07:20.840]   She taught about this mid-journey and she feared for some of the creators like you did,
[01:07:20.840 --> 01:07:25.160]   Mr. LaPorte, saying that, you know what, people are going to lose jobs because now a
[01:07:25.160 --> 01:07:30.520]   marketing department can just put in these prompts and get what they want out of it.
[01:07:30.520 --> 01:07:34.280]   And then conversely, she said, you know what, but I'm going to be fine because I'm a finisher.
[01:07:34.280 --> 01:07:39.080]   And I'm like, yeah, you're definitely going to be fine because you can go in and totally
[01:07:39.080 --> 01:07:43.640]   redefine the stuff that came out of the prompts, you know?
[01:07:43.640 --> 01:07:45.680]   I just put it in the discord.
[01:07:45.680 --> 01:07:46.680]   OK.
[01:07:46.680 --> 01:07:47.680]   You did?
[01:07:47.680 --> 01:07:48.680]   OK.
[01:07:48.680 --> 01:07:49.680]   This is good to see.
[01:07:49.680 --> 01:07:51.360]   Oh, I'm not in that server anymore.
[01:07:51.360 --> 01:07:52.440]   I left the server.
[01:07:52.440 --> 01:07:53.440]   Yeah.
[01:07:53.440 --> 01:07:54.440]   OK.
[01:07:54.440 --> 01:07:56.640]   If I do this, I will--
[01:07:56.640 --> 01:07:57.640]   Huh.
[01:07:57.640 --> 01:07:59.920]   That's pretty good.
[01:07:59.920 --> 01:08:02.320]   It was at a style or just in general?
[01:08:02.320 --> 01:08:03.520]   I just style.
[01:08:03.520 --> 01:08:07.600]   His lunch is hot dogs and potatoes.
[01:08:07.600 --> 01:08:10.520]   How come we're all bald?
[01:08:10.520 --> 01:08:13.720]   Did you say bald?
[01:08:13.720 --> 01:08:14.720]   No.
[01:08:14.720 --> 01:08:15.720]   Well, we said legend.
[01:08:15.720 --> 01:08:16.720]   Legend.
[01:08:16.720 --> 01:08:17.720]   Legend.
[01:08:17.720 --> 01:08:19.720]   I guess that implies bald.
[01:08:19.720 --> 01:08:20.720]   Oh, old.
[01:08:20.720 --> 01:08:21.720]   I told you.
[01:08:21.720 --> 01:08:24.720]   That's what I got when I said Gutenberg princess.
[01:08:24.720 --> 01:08:25.720]   OK.
[01:08:25.720 --> 01:08:27.720]   Not good.
[01:08:27.720 --> 01:08:28.720]   No.
[01:08:28.720 --> 01:08:31.680]   So it looks like, I don't know what it looks like.
[01:08:31.680 --> 01:08:33.680]   Oh, here it is.
[01:08:33.680 --> 01:08:34.920]   It popped up.
[01:08:34.920 --> 01:08:35.920]   Yeah.
[01:08:35.920 --> 01:08:37.920]   Legend having lunch.
[01:08:37.920 --> 01:08:39.920]   [LAUGHTER]
[01:08:39.920 --> 01:08:44.240]   Oh, it's still working on it.
[01:08:44.240 --> 01:08:45.240]   It's updating.
[01:08:45.240 --> 01:08:46.240]   Wow.
[01:08:46.240 --> 01:08:47.240]   OK.
[01:08:47.240 --> 01:08:49.200]   There's your thumbnail.
[01:08:49.200 --> 01:08:51.360]   That's awesome.
[01:08:51.360 --> 01:08:52.360]   [LAUGHTER]
[01:08:52.360 --> 01:08:54.560]   I think that's art.
[01:08:54.560 --> 01:08:56.960]   That's awesome.
[01:08:56.960 --> 01:08:57.960]   Podcasts.
[01:08:57.960 --> 01:09:00.720]   And you have here.
[01:09:00.720 --> 01:09:03.280]   Legend having lunch.
[01:09:03.280 --> 01:09:05.080]   They're at here.
[01:09:05.080 --> 01:09:06.360]   I have headphones on.
[01:09:06.360 --> 01:09:08.280]   I'm definitely a podcast legend.
[01:09:08.280 --> 01:09:09.280]   Have a lunch.
[01:09:09.280 --> 01:09:10.280]   Oh, yeah.
[01:09:10.280 --> 01:09:11.280]   Isn't that funny?
[01:09:11.280 --> 01:09:12.280]   Dazzle.
[01:09:12.280 --> 01:09:13.280]   That was good.
[01:09:13.280 --> 01:09:15.280]   Where'd it go?
[01:09:15.280 --> 01:09:16.280]   [SINGING]
[01:09:16.280 --> 01:09:18.280]   I lost it.
[01:09:18.280 --> 01:09:19.960]   I lost it.
[01:09:19.960 --> 01:09:21.280]   Come on, podcast.
[01:09:21.280 --> 01:09:24.680]   So people, it looks like they can also refine it.
[01:09:24.680 --> 01:09:28.960]   Like you go that and then do more with that kind of.
[01:09:28.960 --> 01:09:29.960]   Yeah.
[01:09:29.960 --> 01:09:32.760]   Where'd it go?
[01:09:32.760 --> 01:09:33.760]   Shoot.
[01:09:33.760 --> 01:09:34.760]   Shoot.
[01:09:34.760 --> 01:09:35.760]   Shoot.
[01:09:35.760 --> 01:09:36.760]   Shoot.
[01:09:36.760 --> 01:09:37.760]   Shoot.
[01:09:37.760 --> 01:09:38.760]   [SIGHS]
[01:09:38.760 --> 01:09:40.760]   Stanley in heaven.
[01:09:40.760 --> 01:09:41.760]   [LAUGHTER]
[01:09:41.760 --> 01:09:48.760]   There's the Too Easy.
[01:09:48.760 --> 01:09:51.760]   There's a lot of the-- a lot of-- oh, here it is.
[01:09:51.760 --> 01:09:52.760]   OK.
[01:09:52.760 --> 01:09:53.760]   So now, what happens?
[01:09:53.760 --> 01:09:55.560]   What do these do?
[01:09:55.560 --> 01:10:00.000]   You one, you two, you three, you four.
[01:10:00.000 --> 01:10:02.760]   So I'm thinking maybe that's you two.
[01:10:02.760 --> 01:10:09.760]   Redo, you four.
[01:10:09.760 --> 01:10:12.760]   I don't know what these do.
[01:10:12.760 --> 01:10:14.560]   Hmm.
[01:10:14.560 --> 01:10:21.760]   Maybe if I read the instructions, OK, the Bible send you four images.
[01:10:21.760 --> 01:10:25.760]   Click the number buttons underneath to get upscales or variations.
[01:10:25.760 --> 01:10:26.760]   Ah.
[01:10:26.760 --> 01:10:27.760]   Oh, OK.
[01:10:27.760 --> 01:10:31.760]   So-- oh, and then it probably doesn't do them in place.
[01:10:31.760 --> 01:10:36.760]   There's Stanley in heaven.
[01:10:36.760 --> 01:10:39.760]   Oh, your job will start.
[01:10:39.760 --> 01:10:42.760]   Oh, so I gave it a bunch of jobs at once.
[01:10:42.760 --> 01:10:43.760]   Oh, crap.
[01:10:43.760 --> 01:10:44.760]   [LAUGHTER]
[01:10:44.760 --> 01:10:45.760]   Oh, whoops.
[01:10:45.760 --> 01:10:48.760]   Clicking all those things.
[01:10:48.760 --> 01:10:50.760]   Oh, I'm sorry.
[01:10:50.760 --> 01:10:52.760]   They are angry with you right now.
[01:10:52.760 --> 01:10:54.760]   I might have used it in my free trial.
[01:10:54.760 --> 01:10:55.760]   [LAUGHTER]
[01:10:55.760 --> 01:10:58.760]   OK, here's another podcast legend having lunch.
[01:10:58.760 --> 01:11:00.760]   That's all about the lunch.
[01:11:00.760 --> 01:11:01.760]   Yeah.
[01:11:01.760 --> 01:11:03.760]   [LAUGHTER]
[01:11:03.760 --> 01:11:04.760]   Job finish.
[01:11:04.760 --> 01:11:05.760]   See the results.
[01:11:05.760 --> 01:11:09.760]   It just improved the lunch.
[01:11:09.760 --> 01:11:10.760]   I asked it--
[01:11:10.760 --> 01:11:11.760]   Yeah.
[01:11:11.760 --> 01:11:15.760]   So I told it to improve specifically one of those images, which is--
[01:11:15.760 --> 01:11:20.760]   Did Dad just say Chad would Boseman as a white dude?
[01:11:20.760 --> 01:11:24.760]   Yeah, no, just Chad would Boseman.
[01:11:24.760 --> 01:11:25.760]   Which is weird, right?
[01:11:25.760 --> 01:11:26.760]   Very weird.
[01:11:26.760 --> 01:11:27.760]   [LAUGHTER]
[01:11:27.760 --> 01:11:28.760]   Doesn't look like him at all.
[01:11:28.760 --> 01:11:30.760]   It looks like nothing like him.
[01:11:30.760 --> 01:11:31.760]   Yeah.
[01:11:31.760 --> 01:11:32.760]   See, I think you're better off not--
[01:11:32.760 --> 01:11:33.760]   Oh, here it is.
[01:11:33.760 --> 01:11:34.760]   Oh, here we go.
[01:11:34.760 --> 01:11:35.760]   There we go.
[01:11:35.760 --> 01:11:36.760]   There we go.
[01:11:36.760 --> 01:11:37.760]   Oh, my God.
[01:11:37.760 --> 01:11:38.760]   I love it.
[01:11:38.760 --> 01:11:40.760]   That's awesome.
[01:11:40.760 --> 01:11:42.760]   Look at this one.
[01:11:42.760 --> 01:11:44.760]   It looks like me and Joe Rogan having lunch.
[01:11:44.760 --> 01:11:45.760]   Me and Joe Rogan having lunch.
[01:11:45.760 --> 01:11:46.760]   I love that.
[01:11:46.760 --> 01:11:47.760]   [LAUGHTER]
[01:11:47.760 --> 01:11:49.760]   That's hysterical.
[01:11:49.760 --> 01:11:50.760]   [LAUGHTER]
[01:11:50.760 --> 01:11:53.760]   Podcast legend having a lunch.
[01:11:53.760 --> 01:11:56.760]   I'm going to frame this.
[01:11:56.760 --> 01:11:57.760]   That's a good one.
[01:11:57.760 --> 01:11:59.760]   That's freaking awesome.
[01:11:59.760 --> 01:12:01.760]   That's a good one.
[01:12:01.760 --> 01:12:03.760]   That's pretty amazing.
[01:12:03.760 --> 01:12:05.760]   Yeah, print that out.
[01:12:05.760 --> 01:12:06.760]   That's--
[01:12:06.760 --> 01:12:07.760]   No kidding.
[01:12:07.760 --> 01:12:08.760]   [LAUGHTER]
[01:12:08.760 --> 01:12:11.760]   Save that to my camera roll.
[01:12:11.760 --> 01:12:13.760]   Print that out.
[01:12:13.760 --> 01:12:15.760]   That's a good one.
[01:12:15.760 --> 01:12:16.760]   Wow.
[01:12:16.760 --> 01:12:18.760]   I have to go in and play with it at some time.
[01:12:18.760 --> 01:12:20.760]   I haven't had any time to play with it some more.
[01:12:20.760 --> 01:12:21.760]   Yeah.
[01:12:21.760 --> 01:12:24.760]   Here's-- oh, that's good too.
[01:12:24.760 --> 01:12:27.760]   No, it's still bringing him.
[01:12:27.760 --> 01:12:29.760]   Well, I clicked it 20 times.
[01:12:29.760 --> 01:12:31.760]   Oh, right, right.
[01:12:31.760 --> 01:12:32.760]   [LAUGHTER]
[01:12:32.760 --> 01:12:35.760]   Podcast legend having lunch.
[01:12:35.760 --> 01:12:39.760]   [LAUGHTER]
[01:12:39.760 --> 01:12:41.760]   I don't like those as much.
[01:12:41.760 --> 01:12:42.760]   Wow.
[01:12:42.760 --> 01:12:43.760]   Wowie, wowie.
[01:12:43.760 --> 01:12:45.760]   Oh, we really did a lot of lunches.
[01:12:45.760 --> 01:12:47.760]   See, no headphones on those.
[01:12:47.760 --> 01:12:49.760]   More Chadwicks.
[01:12:49.760 --> 01:12:52.760]   Here's a long spear killer hunter whale.
[01:12:52.760 --> 01:12:54.760]   Japanese demons in the forest.
[01:12:54.760 --> 01:12:55.760]   That's cool.
[01:12:55.760 --> 01:12:58.760]   Here's another podcast legend having lunch.
[01:12:58.760 --> 01:13:00.760]   I think this is what I'm going to look like.
[01:13:00.760 --> 01:13:01.760]   This is fun.
[01:13:01.760 --> 01:13:02.760]   Yeah.
[01:13:02.760 --> 01:13:03.760]   Dude.
[01:13:03.760 --> 01:13:04.760]   Dude, these are great.
[01:13:04.760 --> 01:13:08.760]   That's pretty cool.
[01:13:08.760 --> 01:13:14.760]   Yeah, I have to play with it.
[01:13:14.760 --> 01:13:15.760]   I'm impressed with this.
[01:13:15.760 --> 01:13:20.760]   It's free, although I'm going to pay for it because I want to do more.
[01:13:20.760 --> 01:13:22.760]   More, more, more.
[01:13:22.760 --> 01:13:26.760]   I'm going to use these podcast legends having lunch.
[01:13:26.760 --> 01:13:28.760]   [LAUGHTER]
[01:13:28.760 --> 01:13:29.760]   Look at this one.
[01:13:29.760 --> 01:13:30.760]   Yeah, new avatars now.
[01:13:30.760 --> 01:13:31.760]   That is remarkable.
[01:13:31.760 --> 01:13:32.760]   That is remarkable.
[01:13:32.760 --> 01:13:33.760]   Yeah.
[01:13:33.760 --> 01:13:35.760]   That is just awesome.
[01:13:35.760 --> 01:13:41.760]   Very nice.
[01:13:41.760 --> 01:13:42.760]   Oh, okay.
[01:13:42.760 --> 01:13:43.760]   Yes.
[01:13:43.760 --> 01:13:44.760]   Right.
[01:13:44.760 --> 01:13:48.760]   So go back and say that's what the upscale is.
[01:13:48.760 --> 01:13:50.760]   Which one should I upscale this?
[01:13:50.760 --> 01:13:51.760]   Two.
[01:13:51.760 --> 01:13:52.760]   One, two.
[01:13:52.760 --> 01:13:54.760]   Two is nice.
[01:13:54.760 --> 01:13:59.760]   And I count like three.
[01:13:59.760 --> 01:14:01.760]   The one before that one.
[01:14:01.760 --> 01:14:03.760]   Was just food?
[01:14:03.760 --> 01:14:05.760]   No, the other one was the other one.
[01:14:05.760 --> 01:14:08.760]   The one that looked like a new speaker with somebody.
[01:14:08.760 --> 01:14:10.760]   Oh, I loved that.
[01:14:10.760 --> 01:14:11.760]   That Joe Rogan one.
[01:14:11.760 --> 01:14:12.760]   That one was really good.
[01:14:12.760 --> 01:14:13.760]   Yeah, this one.
[01:14:13.760 --> 01:14:14.760]   That one was really good.
[01:14:14.760 --> 01:14:15.760]   Yeah, let's do, that's three.
[01:14:15.760 --> 01:14:17.760]   You three upscale.
[01:14:17.760 --> 01:14:21.760]   You're a job.
[01:14:21.760 --> 01:14:27.760]   Well, shh, shh, shh, shh.
[01:14:27.760 --> 01:14:30.760]   So you can do eight K.
[01:14:30.760 --> 01:14:31.760]   Nice.
[01:14:31.760 --> 01:14:33.760]   It really came out well too.
[01:14:33.760 --> 01:14:35.760]   Let me tell you.
[01:14:35.760 --> 01:14:37.760]   Oh, it's still working.
[01:14:37.760 --> 01:14:39.760]   That was the woman with lips.
[01:14:39.760 --> 01:14:40.760]   [LAUGHTER]
[01:14:40.760 --> 01:14:41.760]   Be careful there.
[01:14:41.760 --> 01:14:44.760]   A lot of Jesus's.
[01:14:44.760 --> 01:14:45.760]   That's pretty cool.
[01:14:45.760 --> 01:14:47.760]   Matrix code in it.
[01:14:47.760 --> 01:14:49.760]   Realistic.
[01:14:49.760 --> 01:14:53.760]   That's pretty cool.
[01:14:53.760 --> 01:14:55.760]   This is one of the three.
[01:14:55.760 --> 01:14:59.760]   Oh, that's a podcast legend having lunch.
[01:14:59.760 --> 01:15:01.760]   Wait a minute.
[01:15:01.760 --> 01:15:03.760]   Oh, yeah, baby.
[01:15:03.760 --> 01:15:04.760]   Yeah, that's high.
[01:15:04.760 --> 01:15:06.760]   That's my future.
[01:15:06.760 --> 01:15:07.760]   That's high.
[01:15:07.760 --> 01:15:09.760]   That's me in a few years.
[01:15:09.760 --> 01:15:12.760]   Wow.
[01:15:12.760 --> 01:15:17.760]   Wow.
[01:15:17.760 --> 01:15:20.760]   Oh, now we can do make variations up scale to max.
[01:15:20.760 --> 01:15:23.760]   Light up scale redo, beta upscale redo.
[01:15:23.760 --> 01:15:28.760]   Oh, so it's kind of settled on.
[01:15:28.760 --> 01:15:29.760]   Good.
[01:15:29.760 --> 01:15:31.760]   Oh, and this is when you liked.
[01:15:31.760 --> 01:15:33.760]   Yeah, but it's still working on.
[01:15:33.760 --> 01:15:35.760]   Yeah.
[01:15:35.760 --> 01:15:38.760]   That's high, though.
[01:15:38.760 --> 01:15:39.760]   77%.
[01:15:39.760 --> 01:15:44.760]   [LAUGHTER]
[01:15:44.760 --> 01:15:46.760]   What the heck?
[01:15:46.760 --> 01:15:48.760]   That's an AI for you.
[01:15:48.760 --> 01:15:51.760]   That's an AI in a nutshell, right?
[01:15:51.760 --> 01:15:56.760]   He's like, he's the lunches, his necktie.
[01:15:56.760 --> 01:15:59.760]   This is just a beautiful work of art.
[01:15:59.760 --> 01:16:03.760]   It's almost done.
[01:16:03.760 --> 01:16:07.760]   Oh, my God.
[01:16:07.760 --> 01:16:09.760]   Dude, he's a good.
[01:16:09.760 --> 01:16:11.760]   Look at that.
[01:16:11.760 --> 01:16:12.760]   He's a good--
[01:16:12.760 --> 01:16:15.760]   I feel like that's my future.
[01:16:15.760 --> 01:16:16.760]   That's me.
[01:16:16.760 --> 01:16:17.760]   You know, it's fascinating.
[01:16:17.760 --> 01:16:20.760]   You see the little logos?
[01:16:20.760 --> 01:16:24.760]   See the color right there on the headphones, on the cans?
[01:16:24.760 --> 01:16:25.760]   Yeah.
[01:16:25.760 --> 01:16:29.760]   So save this one as a thumbnail for the show.
[01:16:29.760 --> 01:16:30.760]   All right.
[01:16:30.760 --> 01:16:33.760]   I think that would be a good thumbnail for the show.
[01:16:33.760 --> 01:16:34.760]   Perfect.
[01:16:34.760 --> 01:16:35.760]   Yeah.
[01:16:35.760 --> 01:16:37.760]   Oh, that's good, too.
[01:16:37.760 --> 01:16:39.760]   I love these.
[01:16:39.760 --> 01:16:40.760]   All right.
[01:16:40.760 --> 01:16:42.760]   Let's go back to the discord, see what else is done here.
[01:16:42.760 --> 01:16:44.760]   Oh, my gosh.
[01:16:44.760 --> 01:16:49.760]   [SOUND OF DISTANT CHATTER]
[01:16:49.760 --> 01:16:52.760]   Oh, more podcast legends.
[01:16:52.760 --> 01:16:54.760]   Look at this.
[01:16:54.760 --> 01:16:56.760]   It says legend.
[01:16:56.760 --> 01:17:00.760]   [LAUGHTER]
[01:17:00.760 --> 01:17:02.760]   That's a great one.
[01:17:02.760 --> 01:17:05.760]   [LAUGHTER]
[01:17:05.760 --> 01:17:07.760]   Oh, I just got that.
[01:17:07.760 --> 01:17:08.760]   It said legend.
[01:17:08.760 --> 01:17:09.760]   Nice.
[01:17:09.760 --> 01:17:12.760]   [LAUGHTER]
[01:17:12.760 --> 01:17:14.760]   Oh, my God.
[01:17:14.760 --> 01:17:16.760]   This is the finished one.
[01:17:16.760 --> 01:17:18.760]   This is the finished one.
[01:17:18.760 --> 01:17:21.760]   Upscaled a max.
[01:17:21.760 --> 01:17:22.760]   Wow.
[01:17:22.760 --> 01:17:24.760]   That's badass.
[01:17:24.760 --> 01:17:25.760]   Yeah.
[01:17:25.760 --> 01:17:27.760]   So this is called mid-journey.
[01:17:27.760 --> 01:17:30.760]   And it's really-- you can actually sign up instantly
[01:17:30.760 --> 01:17:32.760]   and it's just a discord channel.
[01:17:32.760 --> 01:17:33.760]   Mm-hmm.
[01:17:33.760 --> 01:17:35.760]   Oh, my God.
[01:17:35.760 --> 01:17:37.760]   This is still working.
[01:17:37.760 --> 01:17:38.760]   55%.
[01:17:38.760 --> 01:17:40.760]   You see, that almost looked like Doc Serals, too.
[01:17:40.760 --> 01:17:41.760]   It does.
[01:17:41.760 --> 01:17:42.760]   I should set it to Doc.
[01:17:42.760 --> 01:17:43.760]   [LAUGHTER]
[01:17:43.760 --> 01:17:49.760]   [SPEAKING SPANISH]
[01:17:49.760 --> 01:17:50.760]   Wow.
[01:17:50.760 --> 01:17:51.760]   I should send this--
[01:17:51.760 --> 01:17:53.760]   And it's not taken very long either, consider it.
[01:17:53.760 --> 01:17:55.760]   Oh, these are working really fast.
[01:17:55.760 --> 01:17:58.760]   Considering there's also lots of other people using it.
[01:17:58.760 --> 01:17:59.760]   Mm-hmm.
[01:17:59.760 --> 01:18:00.760]   It's remarkable.
[01:18:00.760 --> 01:18:05.760]   94% almost done.
[01:18:05.760 --> 01:18:14.760]   [SPEAKING SPANISH]
[01:18:14.760 --> 01:18:17.760]   Wow.
[01:18:17.760 --> 01:18:18.760]   Wow.
[01:18:18.760 --> 01:18:19.760]   I think these are beautiful, don't you?
[01:18:19.760 --> 01:18:21.760]   I mean, they feel like they're good-- they're art.
[01:18:21.760 --> 01:18:22.760]   They are very nice.
[01:18:22.760 --> 01:18:23.760]   Wow.
[01:18:23.760 --> 01:18:24.760]   Look at these.
[01:18:27.760 --> 01:18:30.760]   There's something going on in that little brain.
[01:18:30.760 --> 01:18:31.760]   Oh, here you go.
[01:18:31.760 --> 01:18:32.760]   Gosh, no.
[01:18:32.760 --> 01:18:33.760]   This is the max.
[01:18:33.760 --> 01:18:35.760]   This one's the max.
[01:18:35.760 --> 01:18:37.760]   [LAUGHTER]
[01:18:37.760 --> 01:18:38.760]   Wow.
[01:18:38.760 --> 01:18:40.760]   I love these.
[01:18:40.760 --> 01:18:43.760]   Wow.
[01:18:43.760 --> 01:18:45.760]   Wow, wee, wow, wee.
[01:18:45.760 --> 01:18:47.760]   I've got a poppy on for some reason.
[01:18:47.760 --> 01:18:49.760]   [LAUGHTER]
[01:18:49.760 --> 01:18:52.760]   Ooh, it's getting sexy there.
[01:18:52.760 --> 01:18:55.760]   And this is the other fully upscaled one.
[01:18:55.760 --> 01:18:56.760]   Mm-hmm.
[01:18:56.760 --> 01:18:58.760]   Which I think this is a painting.
[01:18:58.760 --> 01:18:59.760]   Yeah.
[01:18:59.760 --> 01:19:01.760]   That's up for some print right there, dude.
[01:19:01.760 --> 01:19:03.760]   Oh, unbelievable.
[01:19:03.760 --> 01:19:07.760]   So well done.
[01:19:07.760 --> 01:19:11.760]   [LAUGHTER]
[01:19:11.760 --> 01:19:12.760]   Wow.
[01:19:12.760 --> 01:19:13.760]   All right.
[01:19:13.760 --> 01:19:16.760]   I feel bad for people who are only listening because--
[01:19:16.760 --> 01:19:17.760]   Yes.
[01:19:17.760 --> 01:19:19.760]   Here's Casey Newton's piece.
[01:19:19.760 --> 01:19:20.760]   Oh, it's a great piece.
[01:19:20.760 --> 01:19:22.760]   His first week.
[01:19:22.760 --> 01:19:24.760]   Yeah, it's quite good.
[01:19:24.760 --> 01:19:26.760]   What do you think, Stacey?
[01:19:26.760 --> 01:19:28.760]   You cover AI.
[01:19:28.760 --> 01:19:30.760]   We're going to talk about Claire and the Sun.
[01:19:30.760 --> 01:19:33.760]   What's the meaning of all this?
[01:19:33.760 --> 01:19:39.760]   Is it just a toy or is it going to be some longer-term impact?
[01:19:39.760 --> 01:19:43.760]   Well, so I don't know if AI has its own meaning.
[01:19:43.760 --> 01:19:45.760]   We're going to give it meaning and we use it as a tool.
[01:19:45.760 --> 01:19:48.760]   But if you look at the way Silicon Valley is pushing AI,
[01:19:48.760 --> 01:19:53.760]   they're pushing AI to do things that people used to do better.
[01:19:53.760 --> 01:19:54.760]   Right.
[01:19:54.760 --> 01:19:57.760]   People used to-- what is the cinnamon rolls with boobs?
[01:19:57.760 --> 01:19:58.760]   It's--
[01:19:58.760 --> 01:19:59.760]   No, no.
[01:19:59.760 --> 01:20:00.760]   Googly eyes.
[01:20:00.760 --> 01:20:01.760]   Geez.
[01:20:01.760 --> 01:20:02.760]   Your mind stays sore.
[01:20:02.760 --> 01:20:03.760]   Wait a minute.
[01:20:03.760 --> 01:20:05.760]   Why am I looking at--
[01:20:05.760 --> 01:20:08.760]   The only prompt is "manature cinnamon rolls."
[01:20:08.760 --> 01:20:11.760]   And for some reason, Dolly likes to add googly eyes to things.
[01:20:11.760 --> 01:20:12.760]   Googly eyes.
[01:20:12.760 --> 01:20:16.760]   I mean, I like to add googly eyes to things.
[01:20:16.760 --> 01:20:20.760]   Anyway, I think-- I mean, again, it's a tool.
[01:20:20.760 --> 01:20:25.760]   And right now, all the use cases we're seeing getting funding
[01:20:25.760 --> 01:20:30.760]   and getting a lot of attention has been around taking people out of the equation.
[01:20:30.760 --> 01:20:34.760]   So you look at Amazon Go, I just read about a hotel in New Orleans
[01:20:34.760 --> 01:20:37.760]   that was fully automated.
[01:20:37.760 --> 01:20:45.760]   And part of me is like, I don't think this is smart because once you automate things,
[01:20:45.760 --> 01:20:49.760]   we've already seen how terribly things can go wrong.
[01:20:49.760 --> 01:20:52.760]   And actually, there's a story this week that we're going to talk about Google
[01:20:52.760 --> 01:20:56.760]   not having people in the mix and the ability to call and communicate with people that
[01:20:56.760 --> 01:20:57.760]   cause us problems.
[01:20:57.760 --> 01:21:00.760]   I think we're going to see that whole thing.
[01:21:00.760 --> 01:21:06.760]   When you start giving AI the ability-- replacing humans with AI and giving them the ability
[01:21:06.760 --> 01:21:11.760]   to check you into a hotel or check you out or fire people or hire people,
[01:21:11.760 --> 01:21:16.760]   what you're seeing is people A will adapt to game in the system and B,
[01:21:16.760 --> 01:21:20.760]   people who get caught in the system or the system doesn't serve them,
[01:21:20.760 --> 01:21:24.760]   they're not going to have a recourse and that's going to be a problem.
[01:21:24.760 --> 01:21:27.760]   So I think that's what we're heading, but I think it's a dumb way to go,
[01:21:27.760 --> 01:21:30.760]   and we're going to hit a wall at some point very soon.
[01:21:30.760 --> 01:21:33.760]   But why do some managers argue?
[01:21:33.760 --> 01:21:40.760]   What do some managers argue that having this AI in place can create other higher level,
[01:21:40.760 --> 01:21:41.760]   higher--
[01:21:41.760 --> 01:21:42.760]   They do argue that.
[01:21:42.760 --> 01:21:43.760]   --do jobs, if you will.
[01:21:43.760 --> 01:21:48.760]   That's true, but we don't actually-- I don't think-- that's what they argue.
[01:21:48.760 --> 01:21:53.760]   I don't think that's actually what happens because it doesn't create higher level jobs.
[01:21:53.760 --> 01:21:57.760]   It creates really bad boring jobs for humans.
[01:21:57.760 --> 01:21:58.760]   So like--
[01:21:58.760 --> 01:21:59.760]   Stop yourself.
[01:21:59.760 --> 01:22:03.760]   You'll have people who are doing high level thinking,
[01:22:03.760 --> 01:22:07.760]   and those were not the people who were doing those other jobs necessarily,
[01:22:07.760 --> 01:22:12.760]   but then you'll have people like you are me becoming photo editors for like Dolly
[01:22:12.760 --> 01:22:14.760]   instead of working with people.
[01:22:14.760 --> 01:22:17.760]   So all the fun parts of our job, like working with other creatives,
[01:22:17.760 --> 01:22:21.760]   gets sucked away and we're just entering stuff into a computer and generating images, right?
[01:22:21.760 --> 01:22:23.760]   And then we're the human in that equation.
[01:22:23.760 --> 01:22:24.760]   Yeah.
[01:22:24.760 --> 01:22:26.760]   We're also seeing creativity that we haven't seen.
[01:22:26.760 --> 01:22:30.760]   Kevin Russe, I just put it in line 85, Leo, did a good piece.
[01:22:30.760 --> 01:22:32.760]   I think a very reason to piece.
[01:22:32.760 --> 01:22:34.760]   We need to talk about how good AI is getting.
[01:22:34.760 --> 01:22:39.760]   And you know, with my antennae up, my Martian antennae up, I was thinking, "Uh-oh."
[01:22:39.760 --> 01:22:44.760]   But that's kind of the question I was asking too, because it's getting good.
[01:22:44.760 --> 01:22:45.760]   It's getting good.
[01:22:45.760 --> 01:22:49.760]   GPT-3 is good enough to write, you know, sports stories.
[01:22:49.760 --> 01:22:50.760]   He wrote a book review with it.
[01:22:50.760 --> 01:22:51.760]   Yeah, he wrote a book review.
[01:22:51.760 --> 01:22:52.760]   Oh, GPT-3.
[01:22:52.760 --> 01:22:56.760]   But he also says at the end, and this is why he's so happy, Russe said at the end.
[01:22:56.760 --> 01:22:59.760]   I don't know where to go.
[01:22:59.760 --> 01:23:00.760]   Sorry.
[01:23:00.760 --> 01:23:05.760]   News media need to do a better job of explaining AI progress to non-experts.
[01:23:05.760 --> 01:23:08.760]   Too often journalists that he had bits himself here rely on outdated sci-fi
[01:23:08.760 --> 01:23:11.760]   shorthand to translate what's happening in AI to a general audience.
[01:23:11.760 --> 01:23:15.760]   We sometimes compare large language models to Skynet and Hal 9000 and
[01:23:15.760 --> 01:23:18.760]   Flatten promising machine learning breakthroughs to Panakee.
[01:23:18.760 --> 01:23:20.760]   The robots are coming to headlines.
[01:23:20.760 --> 01:23:23.760]   So I'm really glad he had that perspective here.
[01:23:23.760 --> 01:23:25.760]   And is it exactly what you're saying?
[01:23:25.760 --> 01:23:27.760]   Is that the opportunities are great?
[01:23:27.760 --> 01:23:30.760]   Yes, there's issues that Stacey overminds what you need to talk about.
[01:23:30.760 --> 01:23:33.760]   But wow, it's getting pretty good, pretty fast.
[01:23:33.760 --> 01:23:34.760]   It's cool.
[01:23:34.760 --> 01:23:44.760]   There is, though, a critical point, the singularity, if you will.
[01:23:44.760 --> 01:23:49.760]   Well, but I'm just going to, I'm like, okay.
[01:23:49.760 --> 01:23:50.760]   Right now.
[01:23:50.760 --> 01:23:51.760]   Right now.
[01:23:51.760 --> 01:23:52.760]   Right now.
[01:23:52.760 --> 01:23:53.760]   Right now.
[01:23:53.760 --> 01:23:56.760]   You look at the figure self your whole right now.
[01:23:56.760 --> 01:24:01.760]   You look at this stuff and in almost every case, you can say, yeah, that's an AI.
[01:24:01.760 --> 01:24:04.760]   Certainly, occasionally something will come up in there.
[01:24:04.760 --> 01:24:05.760]   That's really good.
[01:24:05.760 --> 01:24:06.760]   Like that heart.
[01:24:06.760 --> 01:24:08.760]   That could have been a human that did it.
[01:24:08.760 --> 01:24:12.760]   But we know the difference right now.
[01:24:12.760 --> 01:24:14.760]   There's going to be a point.
[01:24:14.760 --> 01:24:16.760]   There's two singularities.
[01:24:16.760 --> 01:24:21.760]   There's the one that Ray Kurzweil talks about where it's indestwing
[01:24:21.760 --> 01:24:22.760]   useable from human.
[01:24:22.760 --> 01:24:26.760]   You just look at a picture or read an article and you can't tell the difference.
[01:24:26.760 --> 01:24:30.760]   A human could have written it or a machine could have written it.
[01:24:30.760 --> 01:24:31.760]   That's one singularity.
[01:24:31.760 --> 01:24:37.760]   Then there's the real singularity, which is when the machines can design themselves, because
[01:24:37.760 --> 01:24:40.760]   that's when the acceleration happens because they can do it iterated a much faster pace
[01:24:40.760 --> 01:24:41.760]   than humans.
[01:24:41.760 --> 01:24:44.760]   I feel like we're not, it's Zeno's paradox.
[01:24:44.760 --> 01:24:47.760]   We're getting closer and closer without actually getting there.
[01:24:47.760 --> 01:24:50.760]   We're dividing the distance to dividing the distance to dividing the distance.
[01:24:50.760 --> 01:24:51.760]   Yeah.
[01:24:51.760 --> 01:24:53.760]   But I feel like we're not going to quite get there.
[01:24:53.760 --> 01:24:54.760]   Good.
[01:24:54.760 --> 01:24:55.760]   Right.
[01:24:55.760 --> 01:24:57.760]   And who says that's the goal?
[01:24:57.760 --> 01:24:59.760]   Who says it's the goal to be the human brain?
[01:24:59.760 --> 01:25:01.760]   Who says it's the goal to be the human brain?
[01:25:01.760 --> 01:25:02.760]   Well, no, but that's the thing people fear.
[01:25:02.760 --> 01:25:04.760]   That's the thing that people fear is the thing.
[01:25:04.760 --> 01:25:06.760]   All because people like you make them fear it.
[01:25:06.760 --> 01:25:08.760]   I'm not making anybody fear it.
[01:25:08.760 --> 01:25:10.760]   Jeff welcomes our AI overlords.
[01:25:10.760 --> 01:25:16.760]   I think what we've neglect to think about is people, when we start thinking about this,
[01:25:16.760 --> 01:25:24.360]   is we think of AI as this non, I'm trying to think, they don't recognize that there are
[01:25:24.360 --> 01:25:29.400]   still people behind the scenes pull in the strings and dictating how it's A developed
[01:25:29.400 --> 01:25:30.400]   and deployed.
[01:25:30.400 --> 01:25:33.120]   And we don't question those people's motives.
[01:25:33.120 --> 01:25:36.320]   Instead, we look at the AI and declare them Skynet.
[01:25:36.320 --> 01:25:42.400]   When in fact, there's a large portion of people who would make a lot of money and gain a lot
[01:25:42.400 --> 01:25:43.400]   of power.
[01:25:43.400 --> 01:25:46.000]   It would be like a new, scary, feudalism.
[01:25:46.000 --> 01:25:49.880]   If we had Skynet, you know, look at what China's doing to the weakers.
[01:25:49.880 --> 01:25:51.080]   That is very much.
[01:25:51.080 --> 01:25:55.560]   So we attribute a lot of that to AI when it's really the people building the AI.
[01:25:55.560 --> 01:26:02.280]   And I think it's a distinction that we really should make because if you don't look at the
[01:26:02.280 --> 01:26:05.760]   puppet masters and you're only looking at the puppets, you're really not seeing what's
[01:26:05.760 --> 01:26:06.760]   going on.
[01:26:06.760 --> 01:26:12.080]   And I know that sounds like super cheesy anarchist or whatever, but.
[01:26:12.080 --> 01:26:18.200]   There's a long history of scientists, scientists and technologists designing things that they
[01:26:18.200 --> 01:26:22.280]   lose control of the atom bomb is a good example.
[01:26:22.280 --> 01:26:25.520]   Well, they knew that was going to be a bad idea.
[01:26:25.520 --> 01:26:29.920]   I mean, when they were developing it, I mean, the base technology that went to it because
[01:26:29.920 --> 01:26:32.400]   they wanted to fight Nazism.
[01:26:32.400 --> 01:26:35.320]   And they thought that was the only way we could do that successfully.
[01:26:35.320 --> 01:26:36.320]   Oh, my God.
[01:26:36.320 --> 01:26:37.320]   There's this great book.
[01:26:37.320 --> 01:26:40.280]   I'm going to tell you what it is as soon as I get there.
[01:26:40.280 --> 01:26:41.280]   Hold on.
[01:26:41.280 --> 01:26:47.080]   No, everyone should read it because it's actually it talks about chemical warfare.
[01:26:47.080 --> 01:26:54.520]   And it's all, no, it's all about chemistry and scientific research and how these scientists,
[01:26:54.520 --> 01:26:56.560]   oh my God, where's the name of the book?
[01:26:56.560 --> 01:27:03.320]   I think it's not unusual that scientists are just treated as pure research and do it because
[01:27:03.320 --> 01:27:04.320]   they can.
[01:27:04.320 --> 01:27:08.640]   And then they, then it happens that it has an impact.
[01:27:08.640 --> 01:27:16.640]   It's called When We Seize to Understand the World by Benjamin Lebattud, L-A-B-A-T-U-T.
[01:27:16.640 --> 01:27:21.800]   It's not a great read in terms of riveting, but it is a really good portrait of a bunch
[01:27:21.800 --> 01:27:22.960]   of different scientists.
[01:27:22.960 --> 01:27:25.080]   And it's fictionalized too.
[01:27:25.080 --> 01:27:30.480]   But it's all based on their actual lives and facts and then the kind of conversations
[01:27:30.480 --> 01:27:32.800]   behind the scenes and their doubts are what fictionalized.
[01:27:32.800 --> 01:27:39.440]   But it talks about the guy who invented chlorine gas for World War I chemical bombs.
[01:27:39.440 --> 01:27:40.440]   It's Habak.
[01:27:40.440 --> 01:27:41.440]   Alexander Grotendek.
[01:27:41.440 --> 01:27:44.440]   And Werner Heisenberg, who is a messenger.
[01:27:44.440 --> 01:27:46.440]   Yeah, interesting.
[01:27:46.440 --> 01:27:49.200]   I see your book and add a book.
[01:27:49.200 --> 01:27:53.840]   Our dear friend David Weinberger's Everyday Chaos is very much about this.
[01:27:53.840 --> 01:27:59.120]   And it's a wonderful book in which he argues that we're hitting a point, this might be
[01:27:59.120 --> 01:28:04.560]   a good point, Leo, where the computers can explain things or predict, I'm sorry, they
[01:28:04.560 --> 01:28:08.120]   can predict things better than we can, but can't explain it.
[01:28:08.120 --> 01:28:09.440]   There is no explanation.
[01:28:09.440 --> 01:28:13.320]   It's just through no end of A/B testing.
[01:28:13.320 --> 01:28:17.560]   It arrives at what is in fact a good prediction and it could be a prediction of what we would
[01:28:17.560 --> 01:28:19.880]   say next as in Lambda.
[01:28:19.880 --> 01:28:21.600]   And there's no explanation of how it got there.
[01:28:21.600 --> 01:28:22.600]   There will be an ex-pilot.
[01:28:22.600 --> 01:28:24.320]   You can't even look inside it to see the rules.
[01:28:24.320 --> 01:28:25.320]   No.
[01:28:25.320 --> 01:28:26.600]   We can't reverse engineer it.
[01:28:26.600 --> 01:28:27.600]   It just does it.
[01:28:27.600 --> 01:28:30.200]   It just does it and it does a good job of it.
[01:28:30.200 --> 01:28:31.200]   And essentially has.
[01:28:31.200 --> 01:28:33.760]   But the myth is that we could ever explain anything.
[01:28:33.760 --> 01:28:34.760]   Right.
[01:28:34.760 --> 01:28:37.600]   Well, yeah, I was going to say we don't know how the brain works.
[01:28:37.600 --> 01:28:38.600]   So, you know.
[01:28:38.600 --> 01:28:39.600]   Right.
[01:28:39.600 --> 01:28:40.600]   Exactly Stacy.
[01:28:40.600 --> 01:28:43.960]   Not knows what comes out of Leo's.
[01:28:43.960 --> 01:28:48.360]   The concern is quite reasonably though.
[01:28:48.360 --> 01:28:52.040]   Well here you are doing it as kind of blue sky research and we're getting closer and
[01:28:52.040 --> 01:29:00.200]   closer to machines that can produce an output that's indistinguishable from human output.
[01:29:00.200 --> 01:29:03.520]   I think you're giving more credit that people are doing this saying like they have the choice
[01:29:03.520 --> 01:29:04.520]   not to.
[01:29:04.520 --> 01:29:05.520]   Nobody has a choice not to.
[01:29:05.520 --> 01:29:09.600]   We're just going to do it just as we do everything else because, you know, we pursue
[01:29:09.600 --> 01:29:11.200]   these things.
[01:29:11.200 --> 01:29:14.000]   And then the fear is, well, what's the consequence of that?
[01:29:14.000 --> 01:29:18.920]   Well, it's also, I think Leo, it's I think what was wrong in the early days of the Internet,
[01:29:18.920 --> 01:29:23.480]   what was wrong about Facebook was that they maybe they couldn't have guessed it, but they
[01:29:23.480 --> 01:29:28.480]   never I think built in the system to say, how could this be misused?
[01:29:28.480 --> 01:29:29.480]   Let us go to the guard really.
[01:29:29.480 --> 01:29:30.760]   That's what Amy Webb says.
[01:29:30.760 --> 01:29:33.080]   You have to you have to think you have to strategize.
[01:29:33.080 --> 01:29:36.080]   You have to think about the consequence of your actions.
[01:29:36.080 --> 01:29:38.560]   We're not real good at that.
[01:29:38.560 --> 01:29:44.480]   Although, although up and although up and I'm a Robert Oppenheimer, as soon as he saw Trinity
[01:29:44.480 --> 01:29:51.960]   said, you know, now what is it now we have our as gods, we have become gods, but Gutenberg
[01:29:51.960 --> 01:29:56.320]   invented mobile type after the Chinese and correct where he instead and couldn't have
[01:29:56.320 --> 01:30:01.440]   predicted 150 years later, we're going to become death the destroyer of worlds.
[01:30:01.440 --> 01:30:03.240]   It's from the Bhagavad Gita.
[01:30:03.240 --> 01:30:04.240]   Sorry.
[01:30:04.240 --> 01:30:13.960]   That deserves to be remembered because he immediately looked at Trinity and said, yeah,
[01:30:13.960 --> 01:30:16.280]   I think all the scientists at the time.
[01:30:16.280 --> 01:30:17.280]   He probably said that.
[01:30:17.280 --> 01:30:18.280]   Yeah, all the scientists at the time.
[01:30:18.280 --> 01:30:25.120]   I remember I just read the wonderful book about a von Neumann, John of von Neumann, and
[01:30:25.120 --> 01:30:28.840]   everybody understood that they were creating something that could end up destroying the
[01:30:28.840 --> 01:30:33.880]   world, but they were so anxious to stop the Nazis and end the war.
[01:30:33.880 --> 01:30:38.920]   And they really thought as did Oppenheimer, the Nazis were going to beat us to it if we
[01:30:38.920 --> 01:30:40.480]   didn't get on to it.
[01:30:40.480 --> 01:30:44.440]   And so we almost had to do it because it was us or them.
[01:30:44.440 --> 01:30:45.920]   It was a self-defense.
[01:30:45.920 --> 01:30:49.560]   All right, I want to take a little break.
[01:30:49.560 --> 01:30:50.560]   Come back.
[01:30:50.560 --> 01:30:56.120]   We will talk about customer support via Python script.
[01:30:56.120 --> 01:30:57.120]   What?
[01:30:57.120 --> 01:31:04.240]   When we were talking as you written all over it.
[01:31:04.240 --> 01:31:07.560]   Our show today brought to you by In Real Life, IRL.
[01:31:07.560 --> 01:31:12.000]   It's an original podcast from Mozilla, a show for, guess what?
[01:31:12.000 --> 01:31:18.080]   People who build AI and the people who develop tech policies to regulate it.
[01:31:18.080 --> 01:31:21.840]   Bridget Todd hosts this and there's a new season just came out.
[01:31:21.840 --> 01:31:24.320]   This one want to mention it to you.
[01:31:24.320 --> 01:31:27.120]   It looks at AI in real life.
[01:31:27.120 --> 01:31:28.800]   Boy, this couldn't be more timely.
[01:31:28.800 --> 01:31:30.640]   Who can AI help?
[01:31:30.640 --> 01:31:31.640]   Who can it harm?
[01:31:31.640 --> 01:31:33.880]   This is season six of IRL.
[01:31:33.880 --> 01:31:39.480]   The show features fascinating conversations with people who are working to build more trustworthy
[01:31:39.480 --> 01:31:40.480]   AI.
[01:31:40.480 --> 01:31:45.640]   So I'll give you some example episodes, really good ones that I enjoyed.
[01:31:45.640 --> 01:31:51.800]   There's an episode about how the world is mapped with AI and what data is missing as
[01:31:51.800 --> 01:31:53.240]   much as is important, right?
[01:31:53.240 --> 01:31:58.560]   As with data is there, you'll hear about the people who are working to fill in those gaps
[01:31:58.560 --> 01:32:00.400]   and to take control of the data.
[01:32:00.400 --> 01:32:05.400]   There's an episode about gig workers who depend on apps for their livelihood.
[01:32:05.400 --> 01:32:11.400]   You know those algorithms in Uber and Lyft and how they're pushing back against the algorithms
[01:32:11.400 --> 01:32:15.760]   that not only control much they get paid, but who they pick up where they go, what they
[01:32:15.760 --> 01:32:18.840]   do, whether they work or not.
[01:32:18.840 --> 01:32:23.440]   And about the workers who are looking to find new ways to gain power over the data so they
[01:32:23.440 --> 01:32:25.560]   can have better working conditions.
[01:32:25.560 --> 01:32:28.800]   The truth is out there, AI from above.
[01:32:28.800 --> 01:32:33.960]   When an algorithm is your boss, their episodes for political junkies about the role of AI
[01:32:33.960 --> 01:32:38.160]   plays when it comes to the spread of misinformation and hate speech around elections.
[01:32:38.160 --> 01:32:42.120]   This is a very good show about a very important topic.
[01:32:42.120 --> 01:32:45.680]   We've been kind of meandering all around it.
[01:32:45.680 --> 01:32:47.680]   This show is all about it.
[01:32:47.680 --> 01:32:51.320]   IRL, who has the power over AI?
[01:32:51.320 --> 01:32:55.520]   I invite you to search for IRL in your podcast player.
[01:32:55.520 --> 01:33:01.160]   We'll put a link in the show notes at twit.tv/twig and many thanks to IRL and the folks at Mozilla
[01:33:01.160 --> 01:33:03.120]   who put this together.
[01:33:03.120 --> 01:33:09.080]   And tip of the hat, by the way, to Brigitte Todd who does such a good job with this.
[01:33:09.080 --> 01:33:10.080]   IRL.
[01:33:10.080 --> 01:33:16.680]   Go to IRLpodcast.org/twit.
[01:33:16.680 --> 01:33:21.920]   We thank him so much for their support of this week in Google.
[01:33:21.920 --> 01:33:25.680]   So this is actually the story you were talking about with lack of support from Google.
[01:33:25.680 --> 01:33:32.080]   A dad innocently took a picture of his naked toddler to send it to the doctor.
[01:33:32.080 --> 01:33:35.560]   This is from Sundays, New York Times.
[01:33:35.560 --> 01:33:37.400]   The doctor asked him to.
[01:33:37.400 --> 01:33:39.400]   He said, "There's a problem here.
[01:33:39.400 --> 01:33:40.400]   Take a picture.
[01:33:40.400 --> 01:33:41.400]   Send it to me."
[01:33:41.400 --> 01:33:43.240]   Kashmir Hills excellent article.
[01:33:43.240 --> 01:33:49.000]   This happened in February of 2021 with help from the photos that doctor diagnosed the
[01:33:49.000 --> 01:33:51.440]   issue prescribed antibiotics.
[01:33:51.440 --> 01:34:05.000]   The kid was great, but two days later Google killed his account accusing him of child sexual
[01:34:05.000 --> 01:34:14.440]   abuse material and referred him to the San Francisco Police Department.
[01:34:14.440 --> 01:34:18.480]   Now the problem is this guy, by the way, ironically, the father, I will leave the name
[01:34:18.480 --> 01:34:24.040]   out, actually works in this field.
[01:34:24.040 --> 01:34:32.440]   He designs programs for detecting problematic material.
[01:34:32.440 --> 01:34:36.720]   He's a software engineer that does an automated tool for taking down video content flagged
[01:34:36.720 --> 01:34:40.960]   by users as problematic.
[01:34:40.960 --> 01:34:46.000]   He filed a form requesting review of Google's decision, explaining, well, there were pictures
[01:34:46.000 --> 01:34:47.080]   for the doctor.
[01:34:47.080 --> 01:34:51.160]   The problem is he took him on his Android phone and the Android device uploaded it to Google
[01:34:51.160 --> 01:34:54.680]   Photos where it was automatically scanned.
[01:34:54.680 --> 01:34:58.320]   But he lost not only his Gmail account, but he was a Google Fi user.
[01:34:58.320 --> 01:35:00.440]   So he lost his Google Fi account.
[01:35:00.440 --> 01:35:05.640]   He lost all the documents in Google Drive, including pictures and documentation of his
[01:35:05.640 --> 01:35:06.880]   son's first years of life.
[01:35:06.880 --> 01:35:12.560]   He had to get a new phone number, a new email, and he couldn't, as a result, get the security
[01:35:12.560 --> 01:35:14.720]   codes he needed to sign into other internet accounts.
[01:35:14.720 --> 01:35:19.000]   So then, as a chain reaction, he's locked out of his digital life.
[01:35:19.000 --> 01:35:25.160]   San Francisco Police Department takes the case, and after an investigation says, "No
[01:35:25.160 --> 01:35:26.160]   problem.
[01:35:26.160 --> 01:35:27.160]   You didn't do anything wrong.
[01:35:27.160 --> 01:35:29.440]   There was no crime here.
[01:35:29.440 --> 01:35:31.360]   No crime occurred."
[01:35:31.360 --> 01:35:32.880]   This was December, by the way, then.
[01:35:32.880 --> 01:35:35.000]   Now we're 10 months later.
[01:35:35.000 --> 01:35:40.400]   He received a Manila envelope in the mail from the SFPD saying, "You've been investigated.
[01:35:40.400 --> 01:35:45.760]   We served search warrants on Google and your internet service provider.
[01:35:45.760 --> 01:35:49.240]   We got your internet searches, your location history, your messages, any documents and
[01:35:49.240 --> 01:35:53.040]   photos he'd stored with Google."
[01:35:53.040 --> 01:35:54.040]   So they saw everything.
[01:35:54.040 --> 01:35:57.560]   They got everything.
[01:35:57.560 --> 01:35:59.040]   And they said, "Case closed.
[01:35:59.040 --> 01:36:02.400]   No crime occurred here."
[01:36:02.400 --> 01:36:08.200]   The father called the police officer, the police investigator, and said, "Can you help
[01:36:08.200 --> 01:36:14.320]   me get Google to give me back my account?"
[01:36:14.320 --> 01:36:17.040]   You have to talk to Google, the officer said.
[01:36:17.040 --> 01:36:19.880]   There's nothing I can do.
[01:36:19.880 --> 01:36:25.840]   So Mark appealed to Google again, gave the police report to Google, saying, "No crime
[01:36:25.840 --> 01:36:32.320]   occurred after getting a notice two months ago that his account was permanently deleted."
[01:36:32.320 --> 01:36:34.240]   Google said, "No."
[01:36:34.240 --> 01:36:35.880]   He investigated suing them.
[01:36:35.880 --> 01:36:38.600]   He said, "That's going to be too expensive."
[01:36:38.600 --> 01:36:46.520]   The irony is, this is the sad coda on the whole thing, the SFPD has all of his Google
[01:36:46.520 --> 01:36:50.560]   account information on a thumb drive because it was part of their search.
[01:36:50.560 --> 01:36:55.800]   He's trying to get a copy because Google won't give it to him.
[01:36:55.800 --> 01:37:03.400]   It's funny, we talked about this on Sunday on Twitter, and I got the distinct impression
[01:37:03.400 --> 01:37:07.120]   that the panelist blamed the father.
[01:37:07.120 --> 01:37:15.760]   Like you should have known better than to take this picture of your son naked.
[01:37:15.760 --> 01:37:16.760]   Your thoughts?
[01:37:16.760 --> 01:37:19.680]   Did definitely signal his digital eggs in one basket.
[01:37:19.680 --> 01:37:20.680]   Yeah.
[01:37:20.680 --> 01:37:22.680]   Well, I know.
[01:37:22.680 --> 01:37:30.920]   Well, I mean, yeah.
[01:37:30.920 --> 01:37:38.640]   I think the fault here is when government can't deal with questions at scale and deputizes
[01:37:38.640 --> 01:37:42.880]   private entities to do their work for them.
[01:37:42.880 --> 01:37:44.600]   Google's in a position here.
[01:37:44.600 --> 01:37:45.600]   Where are you?
[01:37:45.600 --> 01:37:48.000]   Hey, tell us, take on all child porn.
[01:37:48.000 --> 01:37:49.000]   There's no line there.
[01:37:49.000 --> 01:37:50.000]   We're taking down anything.
[01:37:50.000 --> 01:37:56.600]   A naked child, a name-pom picture with Facebook, here we're taking it down because liability,
[01:37:56.600 --> 01:37:59.440]   because the government made us do this and we're doing that.
[01:37:59.440 --> 01:38:05.600]   I just had a meeting of 25 internet researchers at the school, which was a great meeting.
[01:38:05.600 --> 01:38:09.480]   They were talking about what's going to happen now is that regulation is going to
[01:38:09.480 --> 01:38:12.800]   say to the platforms, "You've got to give your data to researchers."
[01:38:12.800 --> 01:38:17.200]   The researchers are saying, "Whoa, there's not enough of us to protect the world."
[01:38:17.200 --> 01:38:21.440]   Once again, you've deputized somebody to do your work for you.
[01:38:21.440 --> 01:38:25.080]   We've got to grapple with that, I think.
[01:38:25.080 --> 01:38:26.720]   I think he has a right to get his account back.
[01:38:26.720 --> 01:38:29.000]   I think we'll do the wrong figure.
[01:38:29.000 --> 01:38:33.920]   This is what happens when you have private entities become your cops.
[01:38:33.920 --> 01:38:35.920]   I'm curious to know the timeline.
[01:38:35.920 --> 01:38:41.160]   When he snapped the photographs, how long was it before Google sent all this information
[01:38:41.160 --> 01:38:45.800]   to law enforcement and law enforcement got in contact with him?
[01:38:45.800 --> 01:38:48.760]   It was pretty quick.
[01:38:48.760 --> 01:38:52.400]   In the next, isn't this what was supposed to happen?
[01:38:52.400 --> 01:38:56.160]   This is a false positive, of course, but isn't this what's supposed to happen when
[01:38:56.160 --> 01:38:58.160]   we're supposed to try to...
[01:38:58.160 --> 01:39:01.720]   Is your family photos?
[01:39:01.720 --> 01:39:03.560]   This is what Apple got in so much trouble for.
[01:39:03.560 --> 01:39:06.360]   Remember, they were going to do this scanning.
[01:39:06.360 --> 01:39:07.360]   Everybody got all upset.
[01:39:07.360 --> 01:39:08.360]   Right.
[01:39:08.360 --> 01:39:09.360]   I forgot about that.
[01:39:09.360 --> 01:39:12.400]   Apple, I should point out, still does it the way that Google does in the cloud.
[01:39:12.400 --> 01:39:14.440]   Apple was going to do it on your phone.
[01:39:14.440 --> 01:39:17.800]   In any event, what is it any of their business?
[01:39:17.800 --> 01:39:22.760]   What pictures I have on my phone or on my cloud?
[01:39:22.760 --> 01:39:30.680]   I mean, it is their business because there is a documented harm and use of digital lockers
[01:39:30.680 --> 01:39:32.680]   to store images of child.
[01:39:32.680 --> 01:39:34.200]   They're servers, right?
[01:39:34.200 --> 01:39:35.200]   Yeah.
[01:39:35.200 --> 01:39:38.120]   So, that makes sense.
[01:39:38.120 --> 01:39:42.920]   But the issue, I mean, to me, the issue here isn't that Google scanned it and flagged
[01:39:42.920 --> 01:39:43.920]   it.
[01:39:43.920 --> 01:39:52.560]   It's that this father had, you know, the police exonerated him and he doesn't have a recourse
[01:39:52.560 --> 01:39:55.720]   to get his information back or to get back on Google.
[01:39:55.720 --> 01:39:58.920]   And I think that is, to me, that's the big issue.
[01:39:58.920 --> 01:40:05.560]   It's, I mean, like taking pictures of kids' penis is a big deal and you shouldn't do
[01:40:05.560 --> 01:40:06.560]   it.
[01:40:06.560 --> 01:40:07.560]   Yeah.
[01:40:07.560 --> 01:40:08.560]   And it doesn't make...
[01:40:08.560 --> 01:40:10.560]   Even if you're doing it for the doctor.
[01:40:10.560 --> 01:40:11.560]   Hold on.
[01:40:11.560 --> 01:40:12.560]   Yeah.
[01:40:12.560 --> 01:40:16.040]   And it's reasonable for a company to deflect that.
[01:40:16.040 --> 01:40:20.520]   And when you say, "Yeah, I took that because of, you know, I was taking a picture for the
[01:40:20.520 --> 01:40:22.280]   doctor," you explain this situation.
[01:40:22.280 --> 01:40:25.360]   But there's no way for him to explain it and get it.
[01:40:25.360 --> 01:40:26.720]   And Google simply doesn't care.
[01:40:26.720 --> 01:40:29.480]   It's not worth their time to hire someone to do that.
[01:40:29.480 --> 01:40:33.480]   But who's happening in crime here because the father took a picture of his son, but
[01:40:33.480 --> 01:40:36.680]   Google disseminated that picture all over the place.
[01:40:36.680 --> 01:40:39.440]   Google sent it to the National Center for Missing Exploited Children.
[01:40:39.440 --> 01:40:40.440]   Google themselves looked at...
[01:40:40.440 --> 01:40:42.520]   Nick, maybe looked at it.
[01:40:42.520 --> 01:40:44.400]   The police department looked at it.
[01:40:44.400 --> 01:40:46.400]   They looked at everything he had.
[01:40:46.400 --> 01:40:48.320]   They went through his entire account.
[01:40:48.320 --> 01:40:51.480]   Who disseminated that picture?
[01:40:51.480 --> 01:40:52.480]   Not the father.
[01:40:52.480 --> 01:40:56.880]   Google did, but it did it based on the rules and mores that are in place right now for
[01:40:56.880 --> 01:40:57.880]   this.
[01:40:57.880 --> 01:40:59.360]   The reason why they gave Google, yes.
[01:40:59.360 --> 01:41:00.360]   Yeah.
[01:41:00.360 --> 01:41:05.080]   So, I mean, I think it's an eye-opening case in the sense that people are like, "Oh, crap.
[01:41:05.080 --> 01:41:06.560]   Google does that?
[01:41:06.560 --> 01:41:08.240]   Holy moly."
[01:41:08.240 --> 01:41:12.040]   But I don't think in this situation that it was wrong.
[01:41:12.040 --> 01:41:13.040]   So did photo-back.
[01:41:13.040 --> 01:41:14.040]   The wrongness in its day.
[01:41:14.040 --> 01:41:15.040]   Right.
[01:41:15.040 --> 01:41:16.040]   Photo back to the exact same thing.
[01:41:16.040 --> 01:41:17.040]   So it was best buy.
[01:41:17.040 --> 01:41:18.040]   Photo back.
[01:41:18.040 --> 01:41:22.920]   I mean, how many pedophiles had to get caught before they stopped taking their stuff up
[01:41:22.920 --> 01:41:24.080]   to the geek squad?
[01:41:24.080 --> 01:41:25.080]   Good board.
[01:41:25.080 --> 01:41:27.320]   I mean...
[01:41:27.320 --> 01:41:35.680]   Google sent a response to Kashmir Hill when she asked about this.
[01:41:35.680 --> 01:41:38.720]   It was kind of one of those...
[01:41:38.720 --> 01:41:39.720]   No response.
[01:41:39.720 --> 01:41:40.720]   One of the people.
[01:41:40.720 --> 01:41:41.720]   Yeah.
[01:41:41.720 --> 01:41:42.720]   No response.
[01:41:42.720 --> 01:41:43.720]   No response.
[01:41:43.720 --> 01:41:44.720]   Let me see if I can...
[01:41:44.720 --> 01:41:47.720]   One of the odds that Google is going to back down given Kashmir Hill's story.
[01:41:47.720 --> 01:41:48.720]   Well, there is an answer.
[01:41:48.720 --> 01:41:49.720]   Yeah.
[01:41:49.720 --> 01:41:50.720]   Yeah.
[01:41:50.720 --> 01:41:53.960]   I wouldn't be surprised if Google at some point then gives him back as a count.
[01:41:53.960 --> 01:41:54.960]   Never mind.
[01:41:54.960 --> 01:41:56.400]   But there...
[01:41:56.400 --> 01:42:01.440]   I think this is why you want end-to-end encryption on the cloud.
[01:42:01.440 --> 01:42:02.440]   Yes.
[01:42:02.440 --> 01:42:04.680]   And this is exactly why governments don't want it.
[01:42:04.680 --> 01:42:12.840]   Google's statement is, in its entirety, child sexual abuse material is abhorrent and we're
[01:42:12.840 --> 01:42:16.680]   committed to preventing the spread of it on our platforms.
[01:42:16.680 --> 01:42:21.120]   So Pretty Patel, who was the home secretary of the United Kingdom, just came out saying,
[01:42:21.120 --> 01:42:24.120]   "Ah, Facebook, let's not go so quick into that...
[01:42:24.120 --> 01:42:27.520]   And then encryption you're doing after the abortion case because that's going to affect
[01:42:27.520 --> 01:42:28.520]   us here.
[01:42:28.520 --> 01:42:30.160]   So maybe, yeah, maybe the problem is not Google.
[01:42:30.160 --> 01:42:33.160]   The problem is government which wants Google to be doing this.
[01:42:33.160 --> 01:42:34.160]   Always.
[01:42:34.160 --> 01:42:35.160]   All of this stuff to be scanned.
[01:42:35.160 --> 01:42:36.160]   Yes.
[01:42:36.160 --> 01:42:37.160]   And doesn't want any people wanting...
[01:42:37.160 --> 01:42:38.160]   I think a lot of people want...
[01:42:38.160 --> 01:42:40.960]   Or just recognize that this is the impact when you do that.
[01:42:40.960 --> 01:42:41.960]   Yeah.
[01:42:41.960 --> 01:42:46.360]   I think a lot of people want us to prevent sexual abuse of minors.
[01:42:46.360 --> 01:42:51.080]   And when you say to them, "Okay, if we do this, that also means we have the ability to see
[01:42:51.080 --> 01:42:53.000]   if you're going to have an abortion.
[01:42:53.000 --> 01:42:56.360]   And if we share this with the police, then why wouldn't we share that with them?"
[01:42:56.360 --> 01:42:57.360]   Well said.
[01:42:57.360 --> 01:42:58.360]   Intended consequences.
[01:42:58.360 --> 01:42:59.360]   Yep.
[01:42:59.360 --> 01:43:04.080]   This is why I'm constantly like, when Jeff says, "Moral Panic, I roll my ass out."
[01:43:04.080 --> 01:43:09.320]   My eyes because you could have both sides and we have to figure out, like, we have to create
[01:43:09.320 --> 01:43:10.320]   rules.
[01:43:10.320 --> 01:43:15.080]   Like, okay, if we say no encryption, then we have to say you can only get access to this
[01:43:15.080 --> 01:43:17.880]   data and see it in this case.
[01:43:17.880 --> 01:43:19.960]   Not whenever we feel like it's a good enough use case.
[01:43:19.960 --> 01:43:25.360]   We have to specify, okay, to prevent child sexual abuse by this mess.
[01:43:25.360 --> 01:43:26.360]   I also...
[01:43:26.360 --> 01:43:27.760]   And then everything else you can't get.
[01:43:27.760 --> 01:43:34.040]   If you are an active child molester and you actively, you know, do this stuff...
[01:43:34.040 --> 01:43:38.160]   You probably are smart enough to encrypt it.
[01:43:38.160 --> 01:43:39.160]   Like a lot of schools...
[01:43:39.160 --> 01:43:40.160]   They're stupid.
[01:43:40.160 --> 01:43:42.400]   Well, like a lot of schools...
[01:43:42.400 --> 01:43:43.400]   Again.
[01:43:43.400 --> 01:43:47.080]   I think this catches the innocent more than it catches the guilty.
[01:43:47.080 --> 01:43:48.800]   I think you're making an assumption.
[01:43:48.800 --> 01:43:54.320]   I mean, again, I will go back to people who brought their photos of praying on children
[01:43:54.320 --> 01:43:57.520]   to photo stores, like, you know, and those who left it on their...
[01:43:57.520 --> 01:43:58.760]   Well, no, I agree.
[01:43:58.760 --> 01:44:00.840]   And they had to learn, but I think they've learned.
[01:44:00.840 --> 01:44:01.840]   Yeah.
[01:44:01.840 --> 01:44:02.840]   I mean, I think that's...
[01:44:02.840 --> 01:44:06.120]   There are those that do encrypt it, but there's plenty that don't.
[01:44:06.120 --> 01:44:09.520]   And there's plenty of like first time or casual user.
[01:44:09.520 --> 01:44:10.520]   I don't know.
[01:44:10.520 --> 01:44:11.760]   I'm not in the mind.
[01:44:11.760 --> 01:44:15.240]   But I think we don't have the data to say that that's true.
[01:44:15.240 --> 01:44:17.400]   So I would be very cautious about saying...
[01:44:17.400 --> 01:44:20.240]   I will give you some data...
[01:44:20.240 --> 01:44:21.720]   Thank you.
[01:44:21.720 --> 01:44:22.720]   About this that maybe...
[01:44:22.720 --> 01:44:23.720]   I don't know.
[01:44:23.720 --> 01:44:31.800]   Maybe this says something in 2021, one whole year, Google filed 600,000 reports of
[01:44:31.800 --> 01:44:37.680]   child abuse material and disabled the accounts of over 270,000 users.
[01:44:37.680 --> 01:44:38.680]   Wow.
[01:44:38.680 --> 01:44:40.680]   Were they child abusers?
[01:44:40.680 --> 01:44:41.680]   Yes.
[01:44:41.680 --> 01:44:42.680]   Are there that many?
[01:44:42.680 --> 01:44:43.840]   I bet most were.
[01:44:43.840 --> 01:44:44.840]   Oh, yeah.
[01:44:44.840 --> 01:44:48.160]   There's a lot of creepy ass old men out there.
[01:44:48.160 --> 01:44:54.480]   So you think that of those 270,000 users, the majority, 90%, 80%, whatever, even if
[01:44:54.480 --> 01:44:57.200]   it's 50%, we're actually criminally...
[01:44:57.200 --> 01:45:01.480]   We would hear more cash, we would have gotten more stories like this one.
[01:45:01.480 --> 01:45:03.240]   Of the edge case.
[01:45:03.240 --> 01:45:09.000]   Well, let's look at the number of sexual predators in the US.
[01:45:09.000 --> 01:45:12.400]   I mean, yes, that's a proxy that it's a rough proxy.
[01:45:12.400 --> 01:45:15.040]   But what are those people called?
[01:45:15.040 --> 01:45:16.040]   When you have to read...
[01:45:16.040 --> 01:45:17.040]   Fenders.
[01:45:17.040 --> 01:45:18.040]   Sex offenders, yeah.
[01:45:18.040 --> 01:45:21.200]   Of course, you're going to Google for the United States.
[01:45:21.200 --> 01:45:24.720]   750,000 registered sex offenders in the United States.
[01:45:24.720 --> 01:45:30.280]   There are approximately 250,000 convicted sex offenders under a criminal justice supervision
[01:45:30.280 --> 01:45:31.280]   in the...
[01:45:31.280 --> 01:45:33.240]   Oh, that's in this particular...
[01:45:33.240 --> 01:45:35.960]   Okay, but that's total.
[01:45:35.960 --> 01:45:39.800]   So 750,000 registered sex offenders in the United States.
[01:45:39.800 --> 01:45:40.800]   Okay.
[01:45:40.800 --> 01:45:41.800]   That includes...
[01:45:41.800 --> 01:45:45.440]   So if Google's attaching 270,000 a year...
[01:45:45.440 --> 01:45:49.560]   And then studies of victims have shown that less than 30% of sex crimes are reported to
[01:45:49.560 --> 01:45:51.080]   law enforcement.
[01:45:51.080 --> 01:45:54.880]   So to get registered, you have to be all the way to conviction.
[01:45:54.880 --> 01:45:57.800]   And we think about how many people are accused.
[01:45:57.800 --> 01:45:58.800]   At least...
[01:45:58.800 --> 01:46:04.200]   These are gross stats.
[01:46:04.200 --> 01:46:06.200]   I don't like it.
[01:46:06.200 --> 01:46:07.200]   Yeah.
[01:46:07.200 --> 01:46:08.200]   Yeah.
[01:46:08.200 --> 01:46:09.200]   So there's a lot of...
[01:46:09.200 --> 01:46:13.080]   I mean, I was just thinking about one in four, one in five women are sexually assaulted.
[01:46:13.080 --> 01:46:16.840]   Again, not child, not child predators.
[01:46:16.840 --> 01:46:18.640]   But that's...
[01:46:18.640 --> 01:46:23.200]   I mean, there's a lot of people out there doing really icky stuff.
[01:46:23.200 --> 01:46:28.160]   What do the panel on Sunday think that he should have done?
[01:46:28.160 --> 01:46:35.680]   Not uploaded, not taken the picture and not had his uploads turned on to Google.
[01:46:35.680 --> 01:46:37.880]   You can't get the doctor.
[01:46:37.880 --> 01:46:38.880]   Your kid's sick.
[01:46:38.880 --> 01:46:39.880]   You should have done the same thing.
[01:46:39.880 --> 01:46:42.520]   They thought he should have taken the picture.
[01:46:42.520 --> 01:46:47.040]   This is the day and age deal where a tele doctor is normal now.
[01:46:47.040 --> 01:46:48.040]   Yeah.
[01:46:48.040 --> 01:46:51.920]   I don't know.
[01:46:51.920 --> 01:46:53.400]   It feels like an overreaction.
[01:46:53.400 --> 01:46:59.640]   It feels like the stranger danger over a reaction we had 20 years ago that has kept our kids
[01:46:59.640 --> 01:47:03.040]   off the streets for the last 20 years.
[01:47:03.040 --> 01:47:04.040]   Not mine.
[01:47:04.040 --> 01:47:06.040]   Get outside, dad, gun it.
[01:47:06.040 --> 01:47:07.920]   Not going to sit in a small day.
[01:47:07.920 --> 01:47:09.920]   Good for you.
[01:47:09.920 --> 01:47:10.920]   Good for you.
[01:47:10.920 --> 01:47:18.560]   Well, I know they're not going to go to the bathroom because there is now a tool to monitor
[01:47:18.560 --> 01:47:22.640]   kids going to the bathroom in a thousand American schools.
[01:47:22.640 --> 01:47:25.160]   It's called e-haul pass.
[01:47:25.160 --> 01:47:28.320]   If your kid takes too long to poop, they're going to be in trouble.
[01:47:28.320 --> 01:47:32.000]   Let me tell you something.
[01:47:32.000 --> 01:47:42.080]   According to Vice, the system has some resemblance to the worker monitoring carried out by Amazon.
[01:47:42.080 --> 01:47:45.800]   Co-founder and national director of the K-12 Security Information Exchange, a nonprofit
[01:47:45.800 --> 01:47:48.040]   focused on cybersecurity for schools told Motherboard.
[01:47:48.040 --> 01:47:52.640]   The product is just the latest and growing number of student surveillance tools designed
[01:47:52.640 --> 01:47:57.120]   to allow school administrators to monitor and control student behavior at scale on and
[01:47:57.120 --> 01:48:00.200]   off campus.
[01:48:00.200 --> 01:48:01.960]   Here again.
[01:48:01.960 --> 01:48:02.960]   You lost my kid?
[01:48:02.960 --> 01:48:07.720]   Didn't you do anything to make sure you knew where my kid was?
[01:48:07.720 --> 01:48:13.200]   The program e-haul pass tracks how long and what time and how often each child goes to
[01:48:13.200 --> 01:48:18.000]   the restroom and store that information on third party servers run by a prophet.
[01:48:18.000 --> 01:48:24.000]   A private for-profit company.
[01:48:24.000 --> 01:48:28.200]   Just imagine the stats they're generating.
[01:48:28.200 --> 01:48:33.480]   It's kind of interesting because I saw someone talk about this and other monitoring software
[01:48:33.480 --> 01:48:41.960]   for kids in schools as a way to de-professionalize teaching and to make it very easy to scale
[01:48:41.960 --> 01:48:51.800]   monitoring children in school to a untrained person who basically just likes to stay at
[01:48:51.800 --> 01:48:52.800]   the schools.
[01:48:52.800 --> 01:48:56.480]   Well, you might make the argument now that the teacher doesn't have to be a police officer
[01:48:56.480 --> 01:48:59.400]   the teacher can teach, can focus on teaching.
[01:48:59.400 --> 01:49:01.920]   Okay, all teachers have to be an authority figure.
[01:49:01.920 --> 01:49:02.920]   Yeah.
[01:49:02.920 --> 01:49:05.320]   Students request a pass through the software and the teacher then approves it.
[01:49:05.320 --> 01:49:10.640]   The tool promises "hall omniscience" with the ability to always know who has a pass
[01:49:10.640 --> 01:49:13.160]   and who doesn't without asking the student.
[01:49:13.160 --> 01:49:20.600]   The e-haul pass website explicitly talks about stopping meetups of students mentioning vandalism
[01:49:20.600 --> 01:49:27.440]   and TikTok challenges which is the real problem in today's schools.
[01:49:27.440 --> 01:49:32.320]   Yeah, but then there's also a flip side, a potential good flip side on this and I hate
[01:49:32.320 --> 01:49:36.960]   bringing this up but with all of this active shooting stuff that's another data point that
[01:49:36.960 --> 01:49:37.960]   we can have.
[01:49:37.960 --> 01:49:38.960]   We know where all the kids are.
[01:49:38.960 --> 01:49:39.960]   Yeah.
[01:49:39.960 --> 01:49:40.960]   That's a good point.
[01:49:40.960 --> 01:49:41.960]   Good point.
[01:49:41.960 --> 01:49:44.280]   Let's see the world we live in.
[01:49:44.280 --> 01:49:45.280]   Did you?
[01:49:45.280 --> 01:49:49.520]   I feel better about it if they use that information to help the children but you know that's not
[01:49:49.520 --> 01:49:50.520]   necessarily.
[01:49:50.520 --> 01:49:51.520]   Yep.
[01:49:51.520 --> 01:49:58.840]   Did you get a celebrity spam in your Facebook this morning?
[01:49:58.840 --> 01:49:59.840]   What's Facebook?
[01:49:59.840 --> 01:50:01.680]   I wouldn't have noticed.
[01:50:01.680 --> 01:50:02.680]   Facebook.
[01:50:02.680 --> 01:50:05.240]   Yes, your dog did.
[01:50:05.240 --> 01:50:09.360]   Facebook users are reporting celebrity spam is flooding their feeds.
[01:50:09.360 --> 01:50:14.000]   Facebook says earlier today a configuration change caused some people to have trouble
[01:50:14.000 --> 01:50:16.200]   with their Facebook feed.
[01:50:16.200 --> 01:50:21.480]   We resolve the issues quickly as possible.
[01:50:21.480 --> 01:50:26.040]   Hundreds of Facebook users reporting a strange glitch in which hundreds, hundreds, hundreds
[01:50:26.040 --> 01:50:31.840]   of feeds show posts of people commenting on celebrity pages even if they don't follow
[01:50:31.840 --> 01:50:34.080]   the person leaving the comment or the celebrity.
[01:50:34.080 --> 01:50:35.080]   All right.
[01:50:35.080 --> 01:50:36.080]   Yeah.
[01:50:36.080 --> 01:50:37.800]   I don't know how widespread it was.
[01:50:37.800 --> 01:50:38.800]   As of Wednesday morning.
[01:50:38.800 --> 01:50:40.680]   What is a story about this?
[01:50:40.680 --> 01:50:42.680]   Well wait a minute.
[01:50:42.680 --> 01:50:49.120]   Down detector which detracts all this stuff reported thousands of issues related to Facebook.
[01:50:49.120 --> 01:50:53.720]   81% of complaints related to the websites feed.
[01:50:53.720 --> 01:50:57.400]   45% of users reported issues with the Facebook feed.
[01:50:57.400 --> 01:50:59.040]   I don't know.
[01:50:59.040 --> 01:51:00.040]   Yeah, I don't know.
[01:51:00.040 --> 01:51:01.040]   We don't know.
[01:51:01.040 --> 01:51:02.120]   How could you ever know how many it is?
[01:51:02.120 --> 01:51:03.120]   It might be millions.
[01:51:03.120 --> 01:51:04.120]   It might be billions.
[01:51:04.120 --> 01:51:05.120]   We don't know.
[01:51:05.120 --> 01:51:06.120]   That's why I'm asking celebrities.
[01:51:06.120 --> 01:51:07.120]   Too many celebrities.
[01:51:07.120 --> 01:51:08.120]   Oh my God.
[01:51:08.120 --> 01:51:09.120]   Oh my God.
[01:51:09.120 --> 01:51:10.120]   Oh my God.
[01:51:10.120 --> 01:51:18.000]   Facebook is offering a and apparently will be accepted a $37.5 million settlement over
[01:51:18.000 --> 01:51:21.000]   tracking like Google.
[01:51:21.000 --> 01:51:24.680]   This is a similar settlement with Google.
[01:51:24.680 --> 01:51:30.520]   Facebook continue to track user locations even after they switched off location services.
[01:51:30.520 --> 01:51:35.880]   When location data not available the app used IP addresses to get a general location for
[01:51:35.880 --> 01:51:38.960]   the purpose of serving ads for local businesses.
[01:51:38.960 --> 01:51:43.720]   This violates California law as well as breaching the company's own privacy policy.
[01:51:43.720 --> 01:51:49.040]   So preliminary settlement filed Monday in San Francisco federal court judge has still
[01:51:49.040 --> 01:51:51.200]   to approve it.
[01:51:51.200 --> 01:52:00.200]   37.5 million take about a third off for the log firm divided by three and a half billion
[01:52:00.200 --> 01:52:01.200]   users.
[01:52:01.200 --> 01:52:03.200]   You're going to get, I don't know what you're going to get.
[01:52:03.200 --> 01:52:04.880]   You're going to get a postage stamp in the mail.
[01:52:04.880 --> 01:52:09.040]   I don't know.
[01:52:09.040 --> 01:52:17.000]   I think even though you'll probably get very little if anything in the suit it does probably
[01:52:17.000 --> 01:52:21.160]   have some effect on the behavior of these companies I would hope.
[01:52:21.160 --> 01:52:27.760]   Just like complaining about the metaverse avatar of Mark Zuckerberg got him to redesign
[01:52:27.760 --> 01:52:29.760]   it.
[01:52:29.760 --> 01:52:36.760]   Did you see Mark so excited about Horizon World's major updates to Horizon and avatar graphics
[01:52:36.760 --> 01:52:37.760]   coming soon?
[01:52:37.760 --> 01:52:40.400]   Oh, this is the way.
[01:52:40.400 --> 01:52:41.400]   I'm sorry.
[01:52:41.400 --> 01:52:42.400]   This is the good one.
[01:52:42.400 --> 01:52:45.480]   That's the new one.
[01:52:45.480 --> 01:52:47.040]   That's the good one.
[01:52:47.040 --> 01:52:48.520]   Oh, whoops.
[01:52:48.520 --> 01:52:49.780]   I'm sorry.
[01:52:49.780 --> 01:52:51.680]   That's the fixed one.
[01:52:51.680 --> 01:52:52.760]   Still looks pretty bad.
[01:52:52.760 --> 01:52:53.760]   The early one was even.
[01:52:53.760 --> 01:52:55.280]   Does he have legs yet?
[01:52:55.280 --> 01:52:56.280]   Does he grow legs?
[01:52:56.280 --> 01:52:57.280]   No, I don't know.
[01:52:57.280 --> 01:52:59.320]   I don't think they have legs.
[01:52:59.320 --> 01:53:00.320]   It did look a little bit.
[01:53:00.320 --> 01:53:02.000]   I can't find the original.
[01:53:02.000 --> 01:53:07.240]   Well, if you look up just to Zuck ridiculous video.
[01:53:07.240 --> 01:53:08.240]   I know.
[01:53:08.240 --> 01:53:09.240]   There's a lot.
[01:53:09.240 --> 01:53:19.760]   But I do have this statuary from the Los Angeles County Museum of Art LACMA, a piece
[01:53:19.760 --> 01:53:25.400]   in the archive of the world art and imagination in Spanish, America, 1500 to 1800 exhibit, a
[01:53:25.400 --> 01:53:30.920]   sculpture of the baby Jesus looks surprisingly like the baby Mark.
[01:53:30.920 --> 01:53:34.560]   See Zuckerberg is Jesus.
[01:53:34.560 --> 01:53:36.320]   Zuckerberg is a Messiah.
[01:53:36.320 --> 01:53:37.320]   Jesus.
[01:53:37.320 --> 01:53:40.000]   Oh, I'll bring this up for Jeff since he's a TikTok fan.
[01:53:40.000 --> 01:53:41.600]   Have you ever seen the TikTok account?
[01:53:41.600 --> 01:53:43.400]   Has this artist ever seen a baby?
[01:53:43.400 --> 01:53:44.400]   Oh, yes.
[01:53:44.400 --> 01:53:45.400]   Yes.
[01:53:45.400 --> 01:53:46.400]   It's wonderful.
[01:53:46.400 --> 01:53:47.400]   All right.
[01:53:47.400 --> 01:53:48.400]   Yes.
[01:53:48.400 --> 01:53:52.600]   So that feels like something has this artist ever seen a baby because that is a clear example.
[01:53:52.600 --> 01:53:53.600]   No.
[01:53:53.600 --> 01:53:54.600]   Wait a minute.
[01:53:54.600 --> 01:53:56.800]   Oh, no, no, no, no, turn off the audio.
[01:53:56.800 --> 01:53:59.400]   Please has this artist.
[01:53:59.400 --> 01:54:01.040]   Oh, what's what the heck?
[01:54:01.040 --> 01:54:02.040]   How did you get that?
[01:54:02.040 --> 01:54:04.000]   Leo, what do you look at?
[01:54:04.000 --> 01:54:07.000]   No, it's first ever seen Jesus.
[01:54:07.000 --> 01:54:10.640]   Oh, there's also has this artist ever seen a cat.
[01:54:10.640 --> 01:54:17.640]   So so after you've plumbed the depths of has have this artist ever seen a baby?
[01:54:17.640 --> 01:54:19.600]   Come on.
[01:54:19.600 --> 01:54:21.240]   I guess there's more than one.
[01:54:21.240 --> 01:54:22.240]   I don't know.
[01:54:22.240 --> 01:54:23.680]   Yeah, he does a series.
[01:54:23.680 --> 01:54:25.600]   That's the account.
[01:54:25.600 --> 01:54:26.600]   I don't know.
[01:54:26.600 --> 01:54:27.600]   I do.
[01:54:27.600 --> 01:54:28.600]   Yeah, you don't have to watch it.
[01:54:28.600 --> 01:54:30.680]   I just was trying to relate to Jeff through throwing his Twitter.
[01:54:30.680 --> 01:54:31.680]   Oh, yes you do.
[01:54:31.680 --> 01:54:32.680]   Yes.
[01:54:32.680 --> 01:54:33.680]   That's funny.
[01:54:33.680 --> 01:54:36.680]   It's a good one.
[01:54:36.680 --> 01:54:38.680]   What the heck?
[01:54:38.680 --> 01:54:39.680]   I don't know.
[01:54:39.680 --> 01:54:40.680]   I'm sorry.
[01:54:40.680 --> 01:54:41.680]   What account are you on?
[01:54:41.680 --> 01:54:43.500]   You just don't know how to watch a TikTok.
[01:54:43.500 --> 01:54:47.960]   How do you use this TikTok thing?
[01:54:47.960 --> 01:54:50.360]   Hello, TikTok.
[01:54:50.360 --> 01:54:52.760]   Play me.
[01:54:52.760 --> 01:54:54.760]   A song.
[01:54:54.760 --> 01:54:59.840]   Oh, there's a YouTube version too.
[01:54:59.840 --> 01:55:00.840]   So there you go.
[01:55:00.840 --> 01:55:01.840]   We'll take it off.
[01:55:01.840 --> 01:55:04.040]   If you just Google, is this artist ever seen a baby?
[01:55:04.040 --> 01:55:10.280]   Here's the before and after there on the left, the original mark on the right, the
[01:55:10.280 --> 01:55:12.080]   moon approved Mark Zuckerberg.
[01:55:12.080 --> 01:55:14.040]   It's a little better, but a little bit.
[01:55:14.040 --> 01:55:15.400]   He got more shading.
[01:55:15.400 --> 01:55:17.320]   He should just get a better haircut.
[01:55:17.320 --> 01:55:19.440]   And let's let's be honest about it.
[01:55:19.440 --> 01:55:23.040]   The Caesar cut is over, man.
[01:55:23.040 --> 01:55:25.160]   YouTube, rightly.
[01:55:25.160 --> 01:55:27.640]   Does he have a Caesar cut?
[01:55:27.640 --> 01:55:30.800]   I don't know if it's a Caesar or not.
[01:55:30.800 --> 01:55:32.480]   It's cut pretty close to his head.
[01:55:32.480 --> 01:55:34.040]   Does he have bangs?
[01:55:34.040 --> 01:55:35.680]   I don't even know what a Caesar cut is.
[01:55:35.680 --> 01:55:37.360]   It's the short bangs across the front.
[01:55:37.360 --> 01:55:42.680]   I mean, maybe I can't know what is fluttering to any head shape.
[01:55:42.680 --> 01:55:43.680]   Well, it's better.
[01:55:43.680 --> 01:55:45.520]   I mean, I don't know.
[01:55:45.520 --> 01:55:46.520]   No, I'm not.
[01:55:46.520 --> 01:55:47.520]   I'm not talking about Andrew.
[01:55:47.520 --> 01:55:48.520]   I haven't seen a picture of him.
[01:55:48.520 --> 01:55:51.320]   I think your haircut's spectacular.
[01:55:51.320 --> 01:55:54.760]   How bad could his be?
[01:55:54.760 --> 01:55:58.240]   YouTube removes videos of Tesla fans.
[01:55:58.240 --> 01:55:59.240]   Wait, there's Andrew.
[01:55:59.240 --> 01:56:00.240]   No, it's not a Caesar.
[01:56:00.240 --> 01:56:01.240]   That's a nice haircut.
[01:56:01.240 --> 01:56:03.240]   What a good looking fella.
[01:56:03.240 --> 01:56:05.400]   I mean, I like him.
[01:56:05.400 --> 01:56:10.200]   Doesn't look anything like Mark Zuckerberg, not one bit.
[01:56:10.200 --> 01:56:12.680]   I just thought it was that close to the head haircut.
[01:56:12.680 --> 01:56:13.680]   No, no, no, no.
[01:56:13.680 --> 01:56:14.680]   That's just a short haircut.
[01:56:14.680 --> 01:56:15.680]   Now you're going to make her.
[01:56:15.680 --> 01:56:16.680]   Now you're going to make her.
[01:56:16.680 --> 01:56:18.480]   She's going to go to all her friends and say, can you think of those?
[01:56:18.480 --> 01:56:19.480]   It's like Mark Zuckerberg.
[01:56:19.480 --> 01:56:20.480]   I don't like this.
[01:56:20.480 --> 01:56:21.480]   Hey, hey, hey.
[01:56:21.480 --> 01:56:28.220]   YouTube removes video of Tesla fans using actual children to test whether the full self-driving
[01:56:28.220 --> 01:56:29.720]   software stops or children.
[01:56:29.720 --> 01:56:31.680]   Follow the commercial that we talked about last week.
[01:56:31.680 --> 01:56:36.680]   Yeah, where they ran into mannequins, but don't use actual children.
[01:56:36.680 --> 01:56:43.600]   Okay, so some people are just, I'm sorry, anything for reviews, I guess.
[01:56:43.600 --> 01:56:45.680]   They were dumb before the internet.
[01:56:45.680 --> 01:56:46.680]   That's right.
[01:56:46.680 --> 01:56:49.280]   They're not to not make them dumb.
[01:56:49.280 --> 01:56:50.280]   Here's a happy story.
[01:56:50.280 --> 01:56:56.040]   Wait a minute, let me find one.
[01:56:56.040 --> 01:56:57.480]   National Waffle Day.
[01:56:57.480 --> 01:56:59.080]   It's National Waffle Day.
[01:56:59.080 --> 01:57:03.560]   There you go.
[01:57:03.560 --> 01:57:05.520]   There's a new movie about the history of Blackberry.
[01:57:05.520 --> 01:57:07.720]   I think this will be good.
[01:57:07.720 --> 01:57:14.760]   The story of Doom's smartphone company, Jay, a Barrick, Berrukle, and Glenn Howarton,
[01:57:14.760 --> 01:57:16.840]   starring as Mike.
[01:57:16.840 --> 01:57:17.840]   Yeah, movie.
[01:57:17.840 --> 01:57:18.840]   Really?
[01:57:18.840 --> 01:57:22.880]   Yeah, it's done.
[01:57:22.880 --> 01:57:24.040]   They're finishing up production.
[01:57:24.040 --> 01:57:30.120]   I think it's going to show at the, I think, Toronto Film Festival soon.
[01:57:30.120 --> 01:57:32.680]   Did Canada fund this movie?
[01:57:32.680 --> 01:57:34.880]   Yeah, they should have.
[01:57:34.880 --> 01:57:37.400]   Could be about the Osborne 1.
[01:57:37.400 --> 01:57:38.880]   It's wrapped production.
[01:57:38.880 --> 01:57:42.600]   What is the drama in the Blackberry story, though?
[01:57:42.600 --> 01:57:44.720]   It failed.
[01:57:44.720 --> 01:57:47.040]   It had so much potential.
[01:57:47.040 --> 01:57:50.960]   It still exists serving up embedded software through Q and X.
[01:57:50.960 --> 01:57:51.960]   It's in cars.
[01:57:51.960 --> 01:57:52.960]   It is, although.
[01:57:52.960 --> 01:57:53.960]   Yeah, that's right.
[01:57:53.960 --> 01:57:57.880]   Although it's not really about Blackberry, the, you know, pivoted Blackberry.
[01:57:57.880 --> 01:57:59.800]   It's about Blackberry, the phone company.
[01:57:59.800 --> 01:58:04.520]   And in fact, just this month, Blackberry turned off the servers that would let any Blackberry
[01:58:04.520 --> 01:58:05.720]   phone work.
[01:58:05.720 --> 01:58:09.720]   They no longer, no matter which Blackberry have it, no matter it no longer works.
[01:58:09.720 --> 01:58:10.720]   Oh, BBS?
[01:58:10.720 --> 01:58:11.720]   Blackberry Per.
[01:58:11.720 --> 01:58:12.720]   What was it?
[01:58:12.720 --> 01:58:13.720]   BBM, Blackberry Messenger?
[01:58:13.720 --> 01:58:14.720]   Is that the thing?
[01:58:14.720 --> 01:58:15.720]   Yeah, BBM.
[01:58:15.720 --> 01:58:16.720]   Not just that.
[01:58:16.720 --> 01:58:17.720]   Your phone even won't work.
[01:58:17.720 --> 01:58:18.720]   Oh, wow.
[01:58:18.720 --> 01:58:21.720]   I bought the life story of, of Adam Osborne.
[01:58:21.720 --> 01:58:23.560]   I'm going to do that.
[01:58:23.560 --> 01:58:24.720]   That would be boring.
[01:58:24.720 --> 01:58:30.120]   The movie is, the movie is based on a 2015 bestseller called, I think you might remember
[01:58:30.120 --> 01:58:37.000]   this, "Losing the Signal," the untold story behind the extraordinary rise and spectacular
[01:58:37.000 --> 01:58:40.080]   fall of Blackberry.
[01:58:40.080 --> 01:58:43.160]   By two reporters at the Toronto Globe and Mail.
[01:58:43.160 --> 01:58:47.640]   It is a very Canadian story.
[01:58:47.640 --> 01:58:48.640]   Canada content, man.
[01:58:48.640 --> 01:58:51.040]   You got to fill the airwaves with Canada content.
[01:58:51.040 --> 01:58:53.840]   I do have a story.
[01:58:53.840 --> 01:58:57.840]   Normally this would be in the change log, but I'm going to, I'm not going to put it in
[01:58:57.840 --> 01:59:00.760]   the change log because I want Stacy to comment on it.
[01:59:00.760 --> 01:59:05.160]   Google has turned off, has shut down its IoT core services.
[01:59:05.160 --> 01:59:07.280]   We talked about this last week.
[01:59:07.280 --> 01:59:08.280]   We talked about this last week.
[01:59:08.280 --> 01:59:09.280]   Okay, never mind.
[01:59:09.280 --> 01:59:11.720]   Let's forget it.
[01:59:11.720 --> 01:59:16.480]   I put it in the change log last week, which is where it belonged.
[01:59:16.480 --> 01:59:17.480]   Sorry.
[01:59:17.480 --> 01:59:18.480]   Sorry.
[01:59:18.480 --> 01:59:19.480]   Thank you.
[01:59:19.480 --> 01:59:22.400]   We talked about it in Windows Weekly and I thought, oh, I'll definitely ask Stacy about
[01:59:22.400 --> 01:59:26.400]   this, but forgetting that I had already definitely asked Stacy about it.
[01:59:26.400 --> 01:59:31.520]   We have a lot of the line you could talk about.
[01:59:31.520 --> 01:59:34.640]   You could talk about the company that's going to make call center people sound more white
[01:59:34.640 --> 01:59:36.360]   American using AI.
[01:59:36.360 --> 01:59:37.360]   What?
[01:59:37.360 --> 01:59:40.360]   There's more bad AI stuff in here.
[01:59:40.360 --> 01:59:41.360]   Yes.
[01:59:41.360 --> 01:59:42.360]   I'm in this chat.
[01:59:42.360 --> 01:59:44.680]   I am in Bloomington, Indiana.
[01:59:44.680 --> 01:59:45.680]   Watch it.
[01:59:45.680 --> 01:59:48.360]   Can I help you?
[01:59:48.360 --> 01:59:51.280]   Let's see if they can make me sound more American.
[01:59:51.280 --> 01:59:52.480]   Oh, my.
[01:59:52.480 --> 01:59:59.600]   They try to, when everybody's having a proper fit about this, they try to argue that, no,
[01:59:59.600 --> 02:00:03.160]   this is going to create more job opportunities because, you know, the bigger that America
[02:00:03.160 --> 02:00:05.080]   is, well, well, it's really true.
[02:00:05.080 --> 02:00:08.240]   I mean, nobody wants to say it out loud, but I hear from people all the time saying,
[02:00:08.240 --> 02:00:14.960]   yeah, I got a customer service rep, you know, you know, where, you know, and they weren't
[02:00:14.960 --> 02:00:15.960]   American.
[02:00:15.960 --> 02:00:16.960]   They weren't American.
[02:00:16.960 --> 02:00:18.960]   That's normally what I've heard.
[02:00:18.960 --> 02:00:22.720]   Honestly, customer service reps are bad no matter where they're coming from.
[02:00:22.720 --> 02:00:23.720]   Okay.
[02:00:23.720 --> 02:00:24.720]   It's not the reps.
[02:00:24.720 --> 02:00:26.880]   It's the ability that they're not trained.
[02:00:26.880 --> 02:00:27.880]   They're given a note.
[02:00:27.880 --> 02:00:33.480]   Well, they don't have the power either to solve a problem.
[02:00:33.480 --> 02:00:39.040]   We don't want to say accents are a problem because you have one says president Marty
[02:00:39.040 --> 02:00:40.040]   Serum.
[02:00:40.040 --> 02:00:42.760]   They're only a problem because they cause bias and they cause, I don't think they cause
[02:00:42.760 --> 02:00:47.320]   bias, but they are definitely affected by bias.
[02:00:47.320 --> 02:00:48.320]   Yes.
[02:00:48.320 --> 02:00:49.320]   All right.
[02:00:49.320 --> 02:00:50.320]   That's a standard.
[02:00:50.320 --> 02:00:51.320]   Do you cringe?
[02:00:51.320 --> 02:00:59.840]   Do any of you cringe when you have the call customer service and it's someone that's not
[02:00:59.840 --> 02:01:02.360]   American, I presume no because yellow.
[02:01:02.360 --> 02:01:04.240]   I always ask them, where are you?
[02:01:04.240 --> 02:01:05.640]   What's the weather like?
[02:01:05.640 --> 02:01:06.640]   I talk to them.
[02:01:06.640 --> 02:01:08.480]   I'm afraid they can't tell us though.
[02:01:08.480 --> 02:01:11.280]   Well, I've never had anybody say I can't tell you.
[02:01:11.280 --> 02:01:12.280]   Yeah.
[02:01:12.280 --> 02:01:14.080]   Most of if they make something up, that's fine too.
[02:01:14.080 --> 02:01:15.080]   Yeah.
[02:01:15.080 --> 02:01:18.120]   I mean, people who are located here in the US also have accents.
[02:01:18.120 --> 02:01:19.120]   Yeah.
[02:01:19.120 --> 02:01:21.320]   No, it's a dumb, it's a dumb prejudice.
[02:01:21.320 --> 02:01:22.320]   We all agree.
[02:01:22.320 --> 02:01:23.320]   Yeah.
[02:01:23.320 --> 02:01:24.320]   Yeah.
[02:01:24.320 --> 02:01:25.320]   Yeah.
[02:01:25.320 --> 02:01:32.320]   Oh, do you know what a nim cell is?
[02:01:32.320 --> 02:01:33.320]   A nim?
[02:01:33.320 --> 02:01:34.320]   A nim cell?
[02:01:34.320 --> 02:01:35.320]   A nim cell?
[02:01:35.320 --> 02:01:39.280]   You're a nim cell and you're a nim cell and you're a nim cell.
[02:01:39.280 --> 02:01:43.480]   You're a niche internet micro celebrity Taylor and Oran.
[02:01:43.480 --> 02:01:46.840]   I think I was a bore with Debbie's celebrity myself.
[02:01:46.840 --> 02:01:55.440]   Not a micro Taylor Lawrence's new neologism for niche celebrities.
[02:01:55.440 --> 02:02:01.760]   As online fame fragments, nim cells represent a growing fraction of the attention economy
[02:02:01.760 --> 02:02:03.520]   she writes in the Washington Post.
[02:02:03.520 --> 02:02:05.200]   When did she go to the Washington Post?
[02:02:05.200 --> 02:02:06.840]   Oh, she went there some primogars.
[02:02:06.840 --> 02:02:10.440]   I guess she gets nasty times as much as she should.
[02:02:10.440 --> 02:02:11.440]   Yeah.
[02:02:11.440 --> 02:02:12.440]   Yeah.
[02:02:12.440 --> 02:02:13.440]   Yeah.
[02:02:13.440 --> 02:02:16.600]   This is a picture of a nim cell case you wondered what they look like.
[02:02:16.600 --> 02:02:19.880]   That's Bryce Wollman.
[02:02:19.880 --> 02:02:26.000]   I guess he's got a, he's a six foot six, 25 year old operating nurse.
[02:02:26.000 --> 02:02:29.920]   It's his cult following online that's made him a breakout star.
[02:02:29.920 --> 02:02:35.840]   He tweets jokes and missives of missives about life to his 5,000 followers from the handle
[02:02:35.840 --> 02:02:42.760]   the big and sexy 70 talks about driving his Chevy Tahoe with a DVD player often dresses
[02:02:42.760 --> 02:02:44.280]   in bright clothing.
[02:02:44.280 --> 02:02:49.440]   So 5,000 followers is a small follower count, but that's still in the real world.
[02:02:49.440 --> 02:02:51.640]   5,000 people following your head every day.
[02:02:51.640 --> 02:02:52.640]   That'd be noticed.
[02:02:52.640 --> 02:02:53.640]   Yeah.
[02:02:53.640 --> 02:02:54.640]   You know what they want?
[02:02:54.640 --> 02:02:57.720]   The average circulation of a daily newspaper in America was before the steam powered press.
[02:02:57.720 --> 02:02:58.720]   What?
[02:02:58.720 --> 02:02:59.720]   4,000.
[02:02:59.720 --> 02:03:00.720]   So there you go.
[02:03:00.720 --> 02:03:01.720]   It was a sub stack newsletter.
[02:03:01.720 --> 02:03:02.720]   Yeah.
[02:03:02.720 --> 02:03:08.200]   So that's, I mean, are our YouTubers considered this term nim cell?
[02:03:08.200 --> 02:03:12.240]   Well, I don't know what, I mean, I don't know if nim cell is in white, white, spring
[02:03:12.240 --> 02:03:15.840]   use, TikTok and YouTube stars count.
[02:03:15.840 --> 02:03:23.400]   I, you know, I mean, Henry has 2,1 million followers, but I would say he's a nim cell.
[02:03:23.400 --> 02:03:26.040]   I would say, because I can tell you right now, I'm listening to it.
[02:03:26.040 --> 02:03:29.240]   I know Hank and Hank is killing it, but I'm.
[02:03:29.240 --> 02:03:30.240]   He knows who he is.
[02:03:30.240 --> 02:03:31.240]   He knows who he is.
[02:03:31.240 --> 02:03:32.240]   He knows who he is.
[02:03:32.240 --> 02:03:33.240]   And my mom has no idea who he is.
[02:03:33.240 --> 02:03:34.240]   Right.
[02:03:34.240 --> 02:03:35.240]   Right.
[02:03:35.240 --> 02:03:36.240]   But to his, to his fan base, they.
[02:03:36.240 --> 02:03:37.240]   He's a celebrity.
[02:03:37.240 --> 02:03:38.240]   Yeah.
[02:03:38.240 --> 02:03:39.240]   Absolutely.
[02:03:39.240 --> 02:03:40.240]   Absolutely.
[02:03:40.240 --> 02:03:42.240]   In fact, he recently.
[02:03:42.240 --> 02:03:44.080]   But makes nim cell.
[02:03:44.080 --> 02:03:53.400]   He recently performed at, at outside lands, the big conference, the big music festival.
[02:03:53.400 --> 02:03:54.400]   What?
[02:03:54.400 --> 02:03:55.400]   Yes.
[02:03:55.400 --> 02:03:56.400]   He cooked to the music festival.
[02:03:56.400 --> 02:03:57.400]   Yeah.
[02:03:57.400 --> 02:03:58.400]   Look, here's a picture.
[02:03:58.400 --> 02:03:58.400]   Wow.
[02:03:58.400 --> 02:04:03.160]   He's performing at outside lands with his huge audience watching him make sandwiches.
[02:04:03.160 --> 02:04:04.160]   Oh my.
[02:04:04.160 --> 02:04:06.000]   Oh, that's amazing.
[02:04:06.000 --> 02:04:08.040]   So I guess that's more than a nim cell.
[02:04:08.040 --> 02:04:09.880]   I don't know what that is.
[02:04:09.880 --> 02:04:10.880]   Grooveys.
[02:04:10.880 --> 02:04:15.600]   That's by the way, that's Phil Rosenthal from somebody feed Phil who is like his MC and
[02:04:15.600 --> 02:04:17.480]   his daughter, Lily Rosenthal.
[02:04:17.480 --> 02:04:21.040]   He's making chopped cheese and chicken parm at outside lands.
[02:04:21.040 --> 02:04:27.400]   Oh, it's a pretty unusual DJ set.
[02:04:27.400 --> 02:04:28.400]   Yeah.
[02:04:28.400 --> 02:04:37.280]   It looks like it does look like a DJ except he's got a induction burner instead of a
[02:04:37.280 --> 02:04:38.280]   turntable.
[02:04:38.280 --> 02:04:39.280]   No turntables.
[02:04:39.280 --> 02:04:40.280]   Yeah.
[02:04:40.280 --> 02:04:41.280]   This is hysterical.
[02:04:41.280 --> 02:04:46.080]   It's actually kind of cool how the like having induction burners.
[02:04:46.080 --> 02:04:47.560]   Yeah.
[02:04:47.560 --> 02:04:49.560]   Because that like makes cooking right.
[02:04:49.560 --> 02:04:50.560]   He can cook anywhere.
[02:04:50.560 --> 02:04:51.560]   Yeah.
[02:04:51.560 --> 02:04:52.560]   Yeah.
[02:04:52.560 --> 02:04:54.320]   You could plug it into your Tesla and bring it camping.
[02:04:54.320 --> 02:04:55.560]   I hadn't really thought about that.
[02:04:55.560 --> 02:04:57.200]   I'm like, oh, he does it.
[02:04:57.200 --> 02:04:58.400]   He uses them all the time.
[02:04:58.400 --> 02:04:59.400]   Yeah.
[02:04:59.400 --> 02:05:02.600]   But I think there might be video too of that.
[02:05:02.600 --> 02:05:06.880]   I want to see the food.
[02:05:06.880 --> 02:05:08.040]   I want to eat the food.
[02:05:08.040 --> 02:05:15.000]   I found out that he's been cooking near nearby but not bringing me the leftovers.
[02:05:15.000 --> 02:05:17.880]   He brings it to a young, I told you this, a young lady.
[02:05:17.880 --> 02:05:18.880]   Yeah.
[02:05:18.880 --> 02:05:19.880]   Yeah.
[02:05:19.880 --> 02:05:21.880]   Oh, lady.
[02:05:21.880 --> 02:05:23.080]   Uh huh.
[02:05:23.080 --> 02:05:24.080]   So yeah, we eat.
[02:05:24.080 --> 02:05:26.320]   The way to a woman's heart is through her stomach.
[02:05:26.320 --> 02:05:27.320]   All the time.
[02:05:27.320 --> 02:05:29.320]   Here we go.
[02:05:29.320 --> 02:05:31.280]   Here's a, uh, mine too.
[02:05:31.280 --> 02:05:32.680]   Here's a short of him.
[02:05:32.680 --> 02:05:33.680]   Yeah.
[02:05:33.680 --> 02:05:36.960]   Listen to that crowd cheering.
[02:05:36.960 --> 02:05:37.960]   Hahaha.
[02:05:37.960 --> 02:05:38.960]   Hahaha.
[02:05:38.960 --> 02:05:39.960]   Hahaha.
[02:05:39.960 --> 02:05:40.960]   Take the tank.
[02:05:40.960 --> 02:05:41.960]   Take the tank.
[02:05:41.960 --> 02:05:45.760]   It's cooking for him in the Gastromagic stage.
[02:05:45.760 --> 02:05:47.760]   Excuse me, sir.
[02:05:47.760 --> 02:05:50.240]   Oh, that's somebody else's stuff.
[02:05:50.240 --> 02:05:54.320]   I hate this is, this is the new YouTube shorts.
[02:05:54.320 --> 02:05:56.320]   It's just like TikTok.
[02:05:56.320 --> 02:05:57.320]   Yeah.
[02:05:57.320 --> 02:05:58.320]   Everything's just like TikTok.
[02:05:58.320 --> 02:06:00.320]   Wow, wow, wow, wow, wow.
[02:06:00.320 --> 02:06:03.320]   Except YouTube podcast too.
[02:06:03.320 --> 02:06:04.320]   Yeah.
[02:06:04.320 --> 02:06:08.320]   Is YouTube really going to go really trying to promote podcasting?
[02:06:08.320 --> 02:06:10.600]   Uh, yeah, they have a podcast channel.
[02:06:10.600 --> 02:06:11.600]   Yeah.
[02:06:11.600 --> 02:06:13.400]   Uh, they're not promoting us.
[02:06:13.400 --> 02:06:14.600]   Well, they aren't.
[02:06:14.600 --> 02:06:15.600]   No.
[02:06:15.600 --> 02:06:16.600]   Darn them.
[02:06:16.600 --> 02:06:18.640]   Well, I don't know what that even means.
[02:06:18.640 --> 02:06:21.040]   Uh, nobody goes to YouTube for podcasts.
[02:06:21.040 --> 02:06:23.400]   Although NPR just put a ton of podcasts on YouTube.
[02:06:23.400 --> 02:06:25.640]   I may be wrong on that.
[02:06:25.640 --> 02:06:26.640]   Oh.
[02:06:26.640 --> 02:06:29.360]   I have a Google photo question.
[02:06:29.360 --> 02:06:30.360]   Yes.
[02:06:30.360 --> 02:06:33.160]   Ant or anyone.
[02:06:33.160 --> 02:06:34.160]   Sorry.
[02:06:34.160 --> 02:06:37.160]   I felt like we were devolving there, but I have a real question for you.
[02:06:37.160 --> 02:06:43.360]   Twenty shows, including Up First and Throughline are now available on the YouTube podcast channel
[02:06:43.360 --> 02:06:44.360]   from NPR.
[02:06:44.360 --> 02:06:45.360]   Okay.
[02:06:45.360 --> 02:06:47.280]   So I must be real.
[02:06:47.280 --> 02:06:52.720]   You know how in camera in the camera, Google will be like, okay, use portrait mode and you're
[02:06:52.720 --> 02:06:54.320]   like, all right, fine, I'll use it.
[02:06:54.320 --> 02:06:58.440]   And you use it and then it does this post processing of whatever image you took.
[02:06:58.440 --> 02:07:02.080]   How can you make it not post process?
[02:07:02.080 --> 02:07:06.080]   There should be another image next to that one that's not processed.
[02:07:06.080 --> 02:07:10.040]   It allows you to use either one in Google photos.
[02:07:10.040 --> 02:07:11.040]   I never.
[02:07:11.040 --> 02:07:12.040]   All right.
[02:07:12.040 --> 02:07:15.600]   So if I see the photo and I go into this, is it the snow?
[02:07:15.600 --> 02:07:22.160]   Because at the setting hamburger menu or the little menu dots, I got nothing and it's driving
[02:07:22.160 --> 02:07:26.600]   me nuts because like, you know, I have to do a portrait.
[02:07:26.600 --> 02:07:28.960]   Boy, yeah, this is fun podcasting.
[02:07:28.960 --> 02:07:30.720]   Portrait of the.
[02:07:30.720 --> 02:07:31.720]   Oh, yeah.
[02:07:31.720 --> 02:07:32.720]   Sorry.
[02:07:32.720 --> 02:07:33.720]   You guys wanted to take over the show.
[02:07:33.720 --> 02:07:34.720]   I am watching my hands.
[02:07:34.720 --> 02:07:37.360]   No, I was like, this is a real question.
[02:07:37.360 --> 02:07:38.360]   Nothing to do with this.
[02:07:38.360 --> 02:07:39.360]   Oh, wow.
[02:07:39.360 --> 02:07:40.360]   It didn't.
[02:07:40.360 --> 02:07:41.360]   Yeah.
[02:07:41.360 --> 02:07:44.960]   Normally is right next to it or maybe it's under Google photos under that directory.
[02:07:44.960 --> 02:07:46.960]   Because I've seen it before.
[02:07:46.960 --> 02:07:52.080]   It gives you two options where one is the process version and one is the standard.
[02:07:52.080 --> 02:07:53.080]   Oh, snap.
[02:07:53.080 --> 02:07:56.760]   I think it's taken away my standard because like, I have all this hyperpigmentation and
[02:07:56.760 --> 02:08:01.800]   Google just loves to emphasize it when it comes into like, it's like, Freckles sort
[02:08:01.800 --> 02:08:05.120]   of normal and then it's like, Freckles, what's hyperpigmentation?
[02:08:05.120 --> 02:08:07.040]   Oh, Freckles hyperpigmentation.
[02:08:07.040 --> 02:08:12.200]   No, I have hyperpigmentation from I have melasma on my face.
[02:08:12.200 --> 02:08:14.600]   Oh, I never see it.
[02:08:14.600 --> 02:08:16.360]   Do you cover it up?
[02:08:16.360 --> 02:08:19.960]   I cover it some, but not, I don't want to wear a lot of makeup.
[02:08:19.960 --> 02:08:20.960]   Oh, you look great.
[02:08:20.960 --> 02:08:21.960]   I don't, huh.
[02:08:21.960 --> 02:08:28.960]   Well, either way, when I take a photo, it's not like, Vitiglia or something like that.
[02:08:28.960 --> 02:08:29.960]   Oh, okay.
[02:08:29.960 --> 02:08:30.960]   For the LIGO?
[02:08:30.960 --> 02:08:31.960]   For the LIGO.
[02:08:31.960 --> 02:08:33.360]   I do not see that in there.
[02:08:33.360 --> 02:08:34.360]   Like I used to.
[02:08:34.360 --> 02:08:36.840]   I just looked into all of these folders and it's not there.
[02:08:36.840 --> 02:08:38.560]   I wonder why they took that out.
[02:08:38.560 --> 02:08:39.560]   All right.
[02:08:39.560 --> 02:08:40.560]   Well, then.
[02:08:40.560 --> 02:08:45.360]   Because sometimes I do it and have the depth of field off and, you know, it'll blur out
[02:08:45.360 --> 02:08:48.320]   part of your eye for some reason, you know, stuff like that.
[02:08:48.320 --> 02:08:50.160]   Yeah, it'll just get weird.
[02:08:50.160 --> 02:08:51.160]   Yeah.
[02:08:51.160 --> 02:08:59.880]   By the way, Stacy, I'm going to tell you, you can probably trade in that Google Flip 3 for
[02:08:59.880 --> 02:09:03.320]   almost 100% credit for Google Flip 4.
[02:09:03.320 --> 02:09:04.920]   Samsung Flip 4.
[02:09:04.920 --> 02:09:05.920]   Samsung.
[02:09:05.920 --> 02:09:08.360]   Samsung is being very generous with their trade-ins.
[02:09:08.360 --> 02:09:10.800]   So go look and see what you can get for the trade-in.
[02:09:10.800 --> 02:09:16.600]   And that way you could get the new Flip 4 and the gift that keeps on giving.
[02:09:16.600 --> 02:09:17.600]   Yeah.
[02:09:17.600 --> 02:09:18.600]   I know.
[02:09:18.600 --> 02:09:19.600]   Thanks, Uncle Leo.
[02:09:19.600 --> 02:09:24.400]   And I've seen people say, "Wow, I'm getting a huge amount for a trade-in."
[02:09:24.400 --> 02:09:26.040]   They do that when they ask new phones.
[02:09:26.040 --> 02:09:31.840]   Apparently selling very well, almost a million Galaxy Fold and Flip phones sold in the first
[02:09:31.840 --> 02:09:32.840]   day.
[02:09:32.840 --> 02:09:33.840]   Nice.
[02:09:33.840 --> 02:09:34.840]   Yeah.
[02:09:34.840 --> 02:09:35.840]   Good for them.
[02:09:35.840 --> 02:09:36.840]   I know it's great.
[02:09:36.840 --> 02:09:37.840]   It works in my little delicate pause.
[02:09:37.840 --> 02:09:38.840]   Yeah.
[02:09:38.840 --> 02:09:39.840]   I'm not crazy about the Fold.
[02:09:39.840 --> 02:09:43.040]   I think it's too big, but I thought the Flip was just right.
[02:09:43.040 --> 02:09:44.520]   And good reports on the Flip 4.
[02:09:44.520 --> 02:09:48.560]   You might want to do that upgrade because I guess the little screen on the front can
[02:09:48.560 --> 02:09:50.680]   run apps now.
[02:09:50.680 --> 02:09:53.360]   So it can do a lot more stuff.
[02:09:53.360 --> 02:09:55.360]   And that's the Google Change Log.
[02:09:55.360 --> 02:09:57.960]   It's not the Change Log.
[02:09:57.960 --> 02:09:58.960]   Oh crap.
[02:09:58.960 --> 02:09:59.960]   Oh crap.
[02:09:59.960 --> 02:10:03.160]   Your AI is not working very well.
[02:10:03.160 --> 02:10:05.520]   Oh, jag-nav-it.
[02:10:05.520 --> 02:10:10.160]   YouTube is getting split-screen mode and shorts are coming to televisions.
[02:10:10.160 --> 02:10:11.320]   Page 2.
[02:10:11.320 --> 02:10:13.680]   Apple Music 4 for Android.
[02:10:13.680 --> 02:10:19.960]   Now in data with iOS 16 features and Apple classical mention.
[02:10:19.960 --> 02:10:20.960]   Page 3.
[02:10:20.960 --> 02:10:25.360]   YouTube.com podcast goes live as simple explore page.
[02:10:25.360 --> 02:10:26.880]   We already talked about that.
[02:10:26.880 --> 02:10:28.520]   And page 4.
[02:10:28.520 --> 02:10:34.040]   Google Doc, Google Box.
[02:10:34.040 --> 02:10:36.040]   Google Docs.
[02:10:36.040 --> 02:10:37.480]   Google G-dom bop.
[02:10:37.480 --> 02:10:41.240]   Google Docs Update helps everyone stay on task.
[02:10:41.240 --> 02:10:46.240]   Google Docs.
[02:10:46.240 --> 02:10:48.720]   Google Docs say it was good stuff in the change log.
[02:10:48.720 --> 02:10:49.720]   It's not good stuff.
[02:10:49.720 --> 02:10:50.720]   It said there was some things.
[02:10:50.720 --> 02:10:53.080]   Google Docs has a new tasks feature.
[02:10:53.080 --> 02:10:57.760]   So you can rolling out August 17.
[02:10:57.760 --> 02:11:02.360]   Those under schedule release will see it on August 31.
[02:11:02.360 --> 02:11:04.040]   So you can add to do's.
[02:11:04.040 --> 02:11:05.040]   That's kind of cool.
[02:11:05.040 --> 02:11:08.800]   So that would be for a shared document where you want to delegate.
[02:11:08.800 --> 02:11:10.160]   You could add some things.
[02:11:10.160 --> 02:11:14.000]   Or if you're writing a report, you can put your deadlines in there.
[02:11:14.000 --> 02:11:15.320]   That's cool.
[02:11:15.320 --> 02:11:17.200]   Right there in the doc.
[02:11:17.200 --> 02:11:21.400]   And that's, believe it or not, the Google change log.
[02:11:21.400 --> 02:11:23.120]   Sort of change log ever.
[02:11:23.120 --> 02:11:24.120]   Wow.
[02:11:24.120 --> 02:11:25.120]   Man, was that good.
[02:11:25.120 --> 02:11:26.120]   Wow.
[02:11:26.120 --> 02:11:27.120]   I'm a part of history.
[02:11:27.120 --> 02:11:29.760]   Did you see Instagram is now copying be real as well?
[02:11:29.760 --> 02:11:30.760]   Allegedly.
[02:11:30.760 --> 02:11:31.760]   Of course they are.
[02:11:31.760 --> 02:11:32.960]   Working on it.
[02:11:32.960 --> 02:11:34.800]   It's an internal prototype.
[02:11:34.800 --> 02:11:40.120]   But the same idea where you'll get a pop up saying, "Okay, now take a picture of you
[02:11:40.120 --> 02:11:44.000]   and your surroundings and it'll go on your Insta.
[02:11:44.000 --> 02:11:45.320]   It's so close to be real.
[02:11:45.320 --> 02:11:50.120]   It's like they're not even pretending that coming up with new features.
[02:11:50.120 --> 02:11:52.840]   Just copying them."
[02:11:52.840 --> 02:11:56.000]   Once you've got the audience, all you have to do is just copy the features you think
[02:11:56.000 --> 02:11:57.000]   they want.
[02:11:57.000 --> 02:11:58.000]   Yeah.
[02:11:58.000 --> 02:12:01.680]   I just disappointed that Instagram and I keep saying this is no longer Instagram.
[02:12:01.680 --> 02:12:03.440]   But I guess it's doing well.
[02:12:03.440 --> 02:12:06.240]   It is very much like TikTok now when you go to Instagram.
[02:12:06.240 --> 02:12:10.800]   You're looking more videos, more videos.
[02:12:10.800 --> 02:12:11.800]   Yep.
[02:12:11.800 --> 02:12:14.600]   I've complained about it in the past.
[02:12:14.600 --> 02:12:19.280]   But again, I'm someone that I don't just always shoot photographs.
[02:12:19.280 --> 02:12:24.040]   So I'll stick videos out there and they definitely get more attention.
[02:12:24.040 --> 02:12:28.240]   I wish my photographs got more attention, but it is what it is.
[02:12:28.240 --> 02:12:32.320]   I just feel for the photographers that are out there that their focus is.
[02:12:32.320 --> 02:12:35.320]   You're doing my boyfriend's daughter's room.
[02:12:35.320 --> 02:12:37.200]   Oh, there's no bad voice.
[02:12:37.200 --> 02:12:39.760]   That's the TikTok voice.
[02:12:39.760 --> 02:12:41.920]   I was going to hold up.
[02:12:41.920 --> 02:12:44.920]   This character, oh, I got to do the puzzle piece.
[02:12:44.920 --> 02:12:45.920]   Hold on.
[02:12:45.920 --> 02:12:47.280]   Make sure I'm not a bot.
[02:12:47.280 --> 02:12:48.280]   Thank you.
[02:12:48.280 --> 02:12:49.280]   Yeah.
[02:12:49.280 --> 02:12:51.600]   This guy, Efen Mika.
[02:12:51.600 --> 02:12:52.600]   You want to see him?
[02:12:52.600 --> 02:12:53.600]   Five.
[02:12:53.600 --> 02:12:54.600]   Five.
[02:12:54.600 --> 02:12:55.600]   That's Efen Mika, right?
[02:12:55.600 --> 02:12:59.680]   He is a virtual character on TikTok.
[02:12:59.680 --> 02:13:05.280]   Just signed a big record deal with Capitol Records.
[02:13:05.280 --> 02:13:07.880]   He has over 10 million followers on TikTok.
[02:13:07.880 --> 02:13:09.160]   Talk about an incel.
[02:13:09.160 --> 02:13:10.160]   I'm sorry, not incel.
[02:13:10.160 --> 02:13:13.720]   He has just canceled this contract.
[02:13:13.720 --> 02:13:14.720]   One incel.
[02:13:14.720 --> 02:13:16.720]   They just canceled it?
[02:13:16.720 --> 02:13:17.720]   Yep.
[02:13:17.720 --> 02:13:19.160]   And then he's a racist.
[02:13:19.160 --> 02:13:20.160]   Oh, no.
[02:13:20.160 --> 02:13:25.360]   This story from August 12th is already out of date.
[02:13:25.360 --> 02:13:26.360]   Right out of date.
[02:13:26.360 --> 02:13:27.360]   Yep.
[02:13:27.360 --> 02:13:31.360]   And now right after that, I put that in there because right after that Capitol came
[02:13:31.360 --> 02:13:34.800]   back and said, yeah, we're going to sever ties.
[02:13:34.800 --> 02:13:35.800]   Yes, I see.
[02:13:35.800 --> 02:13:36.960]   I saw the first one.
[02:13:36.960 --> 02:13:41.120]   I didn't see your follow up from dad gum.
[02:13:41.120 --> 02:13:46.720]   Capitol Records server ties with AR rapper Efen, sever ties with AR rapper F and Mika
[02:13:46.720 --> 02:13:48.600]   apologize to the black community.
[02:13:48.600 --> 02:13:49.600]   Great.
[02:13:49.600 --> 02:13:52.480]   You should have maybe looked a little closer.
[02:13:52.480 --> 02:13:54.520]   Capital to what he was doing.
[02:13:54.520 --> 02:13:56.320]   You know, well, but that's the thing.
[02:13:56.320 --> 02:13:58.120]   Was he racist?
[02:13:58.120 --> 02:14:06.560]   Okay, the F in stands for factory new, which is a company and it's basically a couple
[02:14:06.560 --> 02:14:12.840]   white dudes working AI to create music.
[02:14:12.840 --> 02:14:16.720]   And they created some some music that people apparently like.
[02:14:16.720 --> 02:14:23.000]   You got 10 million followers and they did something right on that standpoint.
[02:14:23.000 --> 02:14:27.640]   But unfortunately it used a lot of racial slurs and put some interesting graphics up
[02:14:27.640 --> 02:14:29.520]   that just was good.
[02:14:29.520 --> 02:14:32.520]   I was pretty I was stopped before playing his videos.
[02:14:32.520 --> 02:14:34.880]   Yeah, it's pretty crap.
[02:14:34.880 --> 02:14:36.400]   Tastic.
[02:14:36.400 --> 02:14:42.320]   But again, Capitol was just trying to jump on the whatever that ticket was and try to,
[02:14:42.320 --> 02:14:43.840]   you know, capitalize on it.
[02:14:43.840 --> 02:14:47.720]   No pun intended because that's what they all do.
[02:14:47.720 --> 02:14:51.200]   My beef with this and this was brought to my attention on Twitter.
[02:14:51.200 --> 02:14:53.120]   My lady name is andrana.
[02:14:53.120 --> 02:14:54.320]   I believe that's how you say her name.
[02:14:54.320 --> 02:14:57.280]   She brought it to my attention and I dug into it a little bit.
[02:14:57.280 --> 02:15:04.440]   But my problem is that AI had some data to scrape to figure out what it is that people
[02:15:04.440 --> 02:15:09.280]   are liking and willing to pay for.
[02:15:09.280 --> 02:15:13.080]   I don't like the fact that is a couple white guys throwing out a bunch of racial slurs,
[02:15:13.080 --> 02:15:17.840]   but I also don't like the fact that we are we have created the data that's got all of
[02:15:17.840 --> 02:15:24.720]   those racial slurs in it that made it acceptable to be played as records, you know.
[02:15:24.720 --> 02:15:29.720]   I know y'all have some some have some white privilege, but I apparently have black privilege
[02:15:29.720 --> 02:15:34.160]   where I'm supposed to be able to throw around that inward, right?
[02:15:34.160 --> 02:15:37.240]   But you, but not white guys have you ever heard me say that?
[02:15:37.240 --> 02:15:40.720]   No, you never hear me say it.
[02:15:40.720 --> 02:15:42.720]   No, I refuse to say that word.
[02:15:42.720 --> 02:15:47.160]   I don't give a I don't give a crap about it being a term of endearment and things like
[02:15:47.160 --> 02:15:48.160]   that.
[02:15:48.160 --> 02:15:49.240]   So I'm not going to say it.
[02:15:49.240 --> 02:15:56.000]   So I have a problem with that being a part of the data that we put in there to allow it
[02:15:56.000 --> 02:16:01.120]   to be scraped to be put out there in music and say, Oh, okay, let's go ahead and capitalize
[02:16:01.120 --> 02:16:03.560]   off of it.
[02:16:03.560 --> 02:16:09.800]   I don't think that F and Mika has 31 or whatever is 10 million followers because of
[02:16:09.800 --> 02:16:12.800]   the music as much as like I'm looking at these.
[02:16:12.800 --> 02:16:17.840]   They're just kind of their typical TikTok videos, funny videos like a PS five that turns
[02:16:17.840 --> 02:16:20.160]   out to be a coffee machine.
[02:16:20.160 --> 02:16:26.080]   I mean, you know, it's all but that's all brand is all brand man.
[02:16:26.080 --> 02:16:32.560]   Put your get you an album and build your TikTok army and have that TikTok army come after your
[02:16:32.560 --> 02:16:36.040]   your music and your other art and just continue to grow.
[02:16:36.040 --> 02:16:38.920]   And that's what they did.
[02:16:38.920 --> 02:16:43.040]   It would have been different if it weren't white guys doing it for me.
[02:16:43.040 --> 02:16:47.280]   No, because I don't it's just not my thing.
[02:16:47.280 --> 02:16:48.280]   I don't have that.
[02:16:48.280 --> 02:16:50.640]   I don't have that appropriation to say, Well, I'm white.
[02:16:50.640 --> 02:16:54.160]   I'm an appropriate black culture to make a lot of money on TikTok.
[02:16:54.160 --> 02:16:55.160]   Yep.
[02:16:55.160 --> 02:16:56.160]   Right.
[02:16:56.160 --> 02:16:57.680]   That's that's, you know, I'm not thrilled about that.
[02:16:57.680 --> 02:17:02.240]   I don't like either side of it and someone I believe was the same person they mentioned
[02:17:02.240 --> 02:17:07.640]   on Twitter is like, well, we know that you are a little bit more conservative than other
[02:17:07.640 --> 02:17:08.880]   black folks that we know.
[02:17:08.880 --> 02:17:11.600]   Well, yeah, because I'm, you know, I'm not a conservative.
[02:17:11.600 --> 02:17:12.440]   No, I'm not a liberal.
[02:17:12.440 --> 02:17:15.120]   I'm right there in the middle of it.
[02:17:15.120 --> 02:17:19.520]   There's a couple of stuff that both sides say that I totally disagree with and there's
[02:17:19.520 --> 02:17:21.960]   things that they say that I do agree with.
[02:17:21.960 --> 02:17:24.680]   And on this one, I'm right in the middle of the road.
[02:17:24.680 --> 02:17:29.960]   I'm fine with AI being used to help make art better, but I'm not fine with this AI that
[02:17:29.960 --> 02:17:36.120]   is totally in races and just putting this crap out.
[02:17:36.120 --> 02:17:37.120]   I'm not cool with that.
[02:17:37.120 --> 02:17:39.160]   It's very, it's kind of humorous.
[02:17:39.160 --> 02:17:42.560]   The Capitol records paid so little attention that you said, well, 10 million followers
[02:17:42.560 --> 02:17:44.200]   signed them to a deal.
[02:17:44.200 --> 02:17:45.200]   But that's the thing.
[02:17:45.200 --> 02:17:46.720]   That's the music industry, right?
[02:17:46.720 --> 02:17:47.720]   Yeah.
[02:17:47.720 --> 02:17:48.720]   It's always been that way.
[02:17:48.720 --> 02:17:49.720]   Yeah.
[02:17:49.720 --> 02:17:50.720]   Okay.
[02:17:50.720 --> 02:17:51.720]   So on so it's hot.
[02:17:51.720 --> 02:17:52.720]   We don't know who they are, but people are listening.
[02:17:52.720 --> 02:17:55.520]   Let's get them before somebody else do so we can capitalize on it and make more money
[02:17:55.520 --> 02:17:56.520]   for ourselves.
[02:17:56.520 --> 02:18:00.480]   We don't give a damn what he has a billion views on TikTok or it has a billion views
[02:18:00.480 --> 02:18:01.480]   on TikTok.
[02:18:01.480 --> 02:18:08.120]   It's the number one quote virtual being on the platform and his debut major label single
[02:18:08.120 --> 02:18:10.040]   Florida water.
[02:18:10.040 --> 02:18:11.040]   Yeah.
[02:18:11.040 --> 02:18:12.040]   Came out.
[02:18:12.040 --> 02:18:17.040]   That's another rapper that is currently under, I believe, under investigation or even, you
[02:18:17.040 --> 02:18:19.000]   got incarcerated because of racketeering.
[02:18:19.000 --> 02:18:20.000]   Oh, great.
[02:18:20.000 --> 02:18:25.160]   Well, well, well, but the record companies don't care if it sells records.
[02:18:25.160 --> 02:18:26.160]   They don't care.
[02:18:26.160 --> 02:18:27.160]   That's the thing.
[02:18:27.160 --> 02:18:28.160]   Yeah.
[02:18:28.160 --> 02:18:29.160]   No.
[02:18:29.160 --> 02:18:30.160]   All right.
[02:18:30.160 --> 02:18:36.560]   Let's take a little tiny break because it is National Waffle Day.
[02:18:36.560 --> 02:18:40.360]   And when we come back, not month, it's not, it's not a waffle month.
[02:18:40.360 --> 02:18:41.360]   It's only a day.
[02:18:41.360 --> 02:18:45.680]   It is National Waffle Day in the pig and bottom household.
[02:18:45.680 --> 02:18:48.280]   It'd be good if we were National Stroop Waffle Day.
[02:18:48.280 --> 02:18:55.440]   Actually, whoever told us that, oh, I see in Denmark, they celebrate March 25th.
[02:18:55.440 --> 02:19:00.680]   But in the US, August 24th is National Waffle Day.
[02:19:00.680 --> 02:19:01.680]   Whoo.
[02:19:01.680 --> 02:19:02.680]   Whoo.
[02:19:02.680 --> 02:19:04.440]   I do like waffles.
[02:19:04.440 --> 02:19:06.480]   Belgium, Hong Kong, Stroop Waffle.
[02:19:06.480 --> 02:19:10.760]   So that's an international house of pancakes conspiracy.
[02:19:10.760 --> 02:19:13.360]   Let's see the history of National Waffle Day.
[02:19:13.360 --> 02:19:15.880]   There's a waffle house, Jeff.
[02:19:15.880 --> 02:19:17.360]   Oh, there is that.
[02:19:17.360 --> 02:19:18.360]   That's true.
[02:19:18.360 --> 02:19:19.360]   Yes.
[02:19:19.360 --> 02:19:22.120]   But I'm saying it's the idea of Waffle House.
[02:19:22.120 --> 02:19:23.920]   It's the pancake people who responded.
[02:19:23.920 --> 02:19:28.800]   Waffle Day is celebrated in several European countries on March 25th because it's the first
[02:19:28.800 --> 02:19:31.320]   day of spring or roughly so.
[02:19:31.320 --> 02:19:34.760]   I don't think of waffles as a spring food.
[02:19:34.760 --> 02:19:35.760]   Do you stay?
[02:19:35.760 --> 02:19:39.120]   I know every day, but this is also international.
[02:19:39.120 --> 02:19:40.920]   This is international.
[02:19:40.920 --> 02:19:41.920]   This is international.
[02:19:41.920 --> 02:19:42.920]   What is this?
[02:19:42.920 --> 02:19:43.920]   This is international.
[02:19:43.920 --> 02:19:45.680]   It's like a weekend breakfast.
[02:19:45.680 --> 02:19:46.680]   International.
[02:19:46.680 --> 02:19:47.680]   I'm sorry.
[02:19:47.680 --> 02:19:48.680]   You tried to tell us something.
[02:19:48.680 --> 02:19:49.680]   Nothing important.
[02:19:49.680 --> 02:19:52.680]   This is International Strange Music Day.
[02:19:52.680 --> 02:19:54.240]   Yup.
[02:19:54.240 --> 02:19:55.400]   And National Danny Day.
[02:19:55.400 --> 02:19:57.240]   Oh, they just play F in Mika.
[02:19:57.240 --> 02:20:01.600]   If your name is Danny, it's your day today.
[02:20:01.600 --> 02:20:02.800]   National Danny Day.
[02:20:02.800 --> 02:20:03.800]   And you'll know.
[02:20:03.800 --> 02:20:04.800]   Holy Danny.
[02:20:04.800 --> 02:20:06.400]   Yeah.
[02:20:06.400 --> 02:20:10.080]   We could do a John Cage rendition of "Oh Danny Boy."
[02:20:10.080 --> 02:20:13.640]   Danny Boy, the pipes, the pipes.
[02:20:13.640 --> 02:20:22.160]   Now, now, after that fine break, we should get your thing of the week, Stacey Higginbotham.
[02:20:22.160 --> 02:20:26.120]   So once again, actually, we did a post on the site on why there are no things of the
[02:20:26.120 --> 02:20:27.120]   week.
[02:20:27.120 --> 02:20:28.120]   There's just nothing interesting in this part.
[02:20:28.120 --> 02:20:29.120]   I'm sorry.
[02:20:29.120 --> 02:20:33.080]   You actually felt so guilty that you wrote a blog post.
[02:20:33.080 --> 02:20:34.280]   Kevin wrote the blog post.
[02:20:34.280 --> 02:20:37.360]   I just-- we were like, hey, do you want to review this?
[02:20:37.360 --> 02:20:39.240]   And I'm like, no.
[02:20:39.240 --> 02:20:41.320]   And basically-- so there it is.
[02:20:41.320 --> 02:20:42.320]   And gratuitous.
[02:20:42.320 --> 02:20:43.320]   There's norm.
[02:20:43.320 --> 02:20:44.320]   Norm.
[02:20:44.320 --> 02:20:45.320]   Norm.
[02:20:45.320 --> 02:20:46.320]   Norm.
[02:20:46.320 --> 02:20:47.320]   Norm.
[02:20:47.320 --> 02:20:51.240]   Why the once exciting smart home is now boring.
[02:20:51.240 --> 02:20:52.240]   Can I tell you a secret?
[02:20:52.240 --> 02:20:53.240]   Basically it's--
[02:20:53.240 --> 02:20:55.240]   I've been bored by it for years.
[02:20:55.240 --> 02:20:56.240]   Y'know, hush.
[02:20:56.240 --> 02:20:57.240]   Anyway.
[02:20:57.240 --> 02:20:58.240]   But--
[02:20:58.240 --> 02:20:59.240]   Because it's--
[02:20:59.240 --> 02:21:00.240]   Because it's--
[02:21:00.240 --> 02:21:02.720]   Of his first smart home hub from 2010.
[02:21:02.720 --> 02:21:06.200]   No, that's actually-- oh, that is his first smart home.
[02:21:06.200 --> 02:21:07.200]   Boring even then.
[02:21:07.200 --> 02:21:08.680]   That's what the caption says.
[02:21:08.680 --> 02:21:09.680]   Boring even then.
[02:21:09.680 --> 02:21:10.680]   Yeah.
[02:21:10.680 --> 02:21:12.880]   There is an Amazon Echo.
[02:21:12.880 --> 02:21:13.880]   Boring.
[02:21:13.880 --> 02:21:14.880]   [LAUGHTER]
[02:21:14.880 --> 02:21:16.880]   And I even tried to bring up--
[02:21:16.880 --> 02:21:18.880]   This is where it starts getting exciting.
[02:21:18.880 --> 02:21:19.880]   What's this?
[02:21:19.880 --> 02:21:20.880]   Oh, basically--
[02:21:20.880 --> 02:21:21.880]   A tab.
[02:21:21.880 --> 02:21:26.560]   Our thesis here is a device-centric smart home is boring.
[02:21:26.560 --> 02:21:27.560]   Yeah.
[02:21:27.560 --> 02:21:28.560]   Right?
[02:21:28.560 --> 02:21:29.560]   The smart home is not a device-centric.
[02:21:29.560 --> 02:21:30.560]   It's experience-centric.
[02:21:30.560 --> 02:21:32.680]   Anyway, that's not what I want to tell you about this.
[02:21:32.680 --> 02:21:34.600]   I have a thing for y'all.
[02:21:34.600 --> 02:21:35.600]   All right.
[02:21:35.600 --> 02:21:38.760]   Y'all want an audiobook or do y'all want books?
[02:21:38.760 --> 02:21:39.760]   Any book.
[02:21:39.760 --> 02:21:40.760]   This one y'all want.
[02:21:40.760 --> 02:21:41.760]   Any book could be good.
[02:21:41.760 --> 02:21:42.760]   Either is fine.
[02:21:42.760 --> 02:21:43.760]   Doesn't have to be audio.
[02:21:43.760 --> 02:21:44.760]   Could be any book.
[02:21:44.760 --> 02:21:45.760]   All right.
[02:21:45.760 --> 02:21:46.760]   Well, I'm listening to the audiobook.
[02:21:46.760 --> 02:21:47.760]   So I'm going to-- and I think it's a great audiobook.
[02:21:47.760 --> 02:21:51.600]   So I'm going to tell you, it's Razorblade Tears by Simon Crosby.
[02:21:51.600 --> 02:21:52.600]   Is it Simon Crosby?
[02:21:52.600 --> 02:21:53.600]   No, Simon Crosby is one of my sources.
[02:21:53.600 --> 02:21:54.600]   Hold on.
[02:21:54.600 --> 02:21:55.600]   Let me tell you who made it.
[02:21:55.600 --> 02:21:56.600]   This could be the next--
[02:21:56.600 --> 02:21:57.600]   Essay Cosby.
[02:21:57.600 --> 02:21:59.760]   This could be the next Stacey's book club.
[02:21:59.760 --> 02:22:01.360]   We don't know.
[02:22:01.360 --> 02:22:02.720]   It's not science fiction.
[02:22:02.720 --> 02:22:07.400]   OK, this guy actually wrote one of my favorite books of last year.
[02:22:07.400 --> 02:22:10.600]   It was a heist set in the south.
[02:22:10.600 --> 02:22:12.640]   This one, Blacktop Wasteland.
[02:22:12.640 --> 02:22:17.760]   This listening to this book, I am just like, my God, when are we going to get this as a
[02:22:17.760 --> 02:22:18.760]   movie?
[02:22:18.760 --> 02:22:22.040]   It is so good and so vivid.
[02:22:22.040 --> 02:22:23.640]   And I love it.
[02:22:23.640 --> 02:22:25.240]   It is the story.
[02:22:25.240 --> 02:22:29.000]   Well, it-- how would I describe it?
[02:22:29.000 --> 02:22:32.600]   It's two old men go searching for justice after their sons are murdered.
[02:22:32.600 --> 02:22:35.440]   Are they named Leo and Jeff?
[02:22:35.440 --> 02:22:36.440]   No.
[02:22:36.440 --> 02:22:37.440]   Isaiah and Derek.
[02:22:37.440 --> 02:22:41.680]   So it's a Black man and a white man, both from--
[02:22:41.680 --> 02:22:43.840]   Are they named Leo and Anne?
[02:22:43.840 --> 02:22:44.840]   No.
[02:22:44.840 --> 02:22:45.840]   Nope.
[02:22:45.840 --> 02:22:46.840]   Nope.
[02:22:46.840 --> 02:22:47.840]   No.
[02:22:47.840 --> 02:22:51.880]   Is I be interested in reading that book?
[02:22:51.880 --> 02:22:53.880]   But it deals with--
[02:22:53.880 --> 02:22:54.880]   She keeps trying, man.
[02:22:54.880 --> 02:22:55.880]   She persists.
[02:22:55.880 --> 02:22:56.880]   I know.
[02:22:56.880 --> 02:22:57.880]   Nevertheless.
[02:22:57.880 --> 02:22:58.880]   She persists.
[02:22:58.880 --> 02:22:59.880]   She persists.
[02:22:59.880 --> 02:23:03.400]   Anyway, it's great.
[02:23:03.400 --> 02:23:04.600]   It's got action.
[02:23:04.600 --> 02:23:09.800]   It's got reckoning over race, reckoning over rights for gay people.
[02:23:09.800 --> 02:23:12.280]   It's got people coming to terms with who they are.
[02:23:12.280 --> 02:23:13.280]   It's got class.
[02:23:13.280 --> 02:23:15.200]   It's got how we treat ex-cons.
[02:23:15.200 --> 02:23:17.920]   It's just a great story.
[02:23:17.920 --> 02:23:18.920]   I love it.
[02:23:18.920 --> 02:23:20.120]   And my wish list.
[02:23:20.120 --> 02:23:24.360]   Adding it to my wish list because I used that like that yesterday but unfortunately.
[02:23:24.360 --> 02:23:25.360]   But nice.
[02:23:25.360 --> 02:23:26.360]   I really get--
[02:23:26.360 --> 02:23:27.360]   Really?
[02:23:27.360 --> 02:23:28.360]   --its in this book.
[02:23:28.360 --> 02:23:29.360]   I did.
[02:23:29.360 --> 02:23:30.360]   Recommendation.
[02:23:30.360 --> 02:23:31.360]   How'd you know?
[02:23:31.360 --> 02:23:32.360]   You're right.
[02:23:32.360 --> 02:23:38.040]   I got Steve-- see, the problem is Steve gets a day jump on Twig and I downloaded his
[02:23:38.040 --> 02:23:39.040]   book instead.
[02:23:39.040 --> 02:23:40.040]   I'm sorry.
[02:23:40.040 --> 02:23:43.640]   But I will put next month Razorblade to the singularity--
[02:23:43.640 --> 02:23:45.000]   I buy the extra credit.
[02:23:45.000 --> 02:23:46.000]   I buy the extra credit.
[02:23:46.000 --> 02:23:47.000]   Why an extra credit?
[02:23:47.000 --> 02:23:48.000]   How hardly anything.
[02:23:48.000 --> 02:23:49.000]   It's way less than this.
[02:23:49.000 --> 02:23:50.000]   I know.
[02:23:50.000 --> 02:23:51.000]   But I already have two books a month.
[02:23:51.000 --> 02:23:53.000]   I can't read as many as I have.
[02:23:53.000 --> 02:23:56.000]   Oh, should I re-show Will It Waffles?
[02:23:56.000 --> 02:23:58.680]   It's at his National Waffle Day like four or five years ago.
[02:23:58.680 --> 02:23:59.680]   Really?
[02:23:59.680 --> 02:24:00.680]   I did.
[02:24:00.680 --> 02:24:03.800]   That was my thing for the last time we had Waffle Day on this show.
[02:24:03.800 --> 02:24:04.800]   Oh, we've done this before.
[02:24:04.800 --> 02:24:05.800]   Which is my Waffle cookbook.
[02:24:05.800 --> 02:24:06.800]   Oh.
[02:24:06.800 --> 02:24:07.800]   Yeah.
[02:24:07.800 --> 02:24:08.800]   It's a Waffle.
[02:24:08.800 --> 02:24:09.800]   It's a great cookbook.
[02:24:09.800 --> 02:24:13.240]   But is it like-- does it try to waffle things that are not normally waffled?
[02:24:13.240 --> 02:24:14.240]   Yes.
[02:24:14.240 --> 02:24:20.400]   And the best recipe is hash browns with pepper and rosemary and you pop that in the waffle
[02:24:20.400 --> 02:24:21.400]   iron.
[02:24:21.400 --> 02:24:22.400]   Oh.
[02:24:22.400 --> 02:24:23.400]   So yummy.
[02:24:23.400 --> 02:24:25.600]   I have an underused waffle iron.
[02:24:25.600 --> 02:24:26.600]   Oh my god.
[02:24:26.600 --> 02:24:28.320]   Will The Waffle is the best?
[02:24:28.320 --> 02:24:29.320]   Have you not waffled?
[02:24:29.320 --> 02:24:31.040]   Calamari's salad and a waffle iron?
[02:24:31.040 --> 02:24:32.520]   Well, well, well, well.
[02:24:32.520 --> 02:24:34.520]   Well, iron is just a heater.
[02:24:34.520 --> 02:24:35.520]   It's just cooking.
[02:24:35.520 --> 02:24:36.520]   It's just like a--
[02:24:36.520 --> 02:24:37.520]   Yeah.
[02:24:37.520 --> 02:24:38.520]   And shaping it.
[02:24:38.520 --> 02:24:39.520]   Yeah.
[02:24:39.520 --> 02:24:41.520]   You know, that kind of thing about should probably make my grilled cheese and the waffle
[02:24:41.520 --> 02:24:42.520]   iron shouldn't I?
[02:24:42.520 --> 02:24:43.760]   Oh my god, you 100% should.
[02:24:43.760 --> 02:24:44.760]   Yeah.
[02:24:44.760 --> 02:24:45.760]   Yeah.
[02:24:45.760 --> 02:24:46.760]   Yes.
[02:24:46.760 --> 02:24:47.760]   Darn.
[02:24:47.760 --> 02:24:48.760]   Okay, good.
[02:24:48.760 --> 02:24:49.760]   Will It Waffle?
[02:24:49.760 --> 02:24:50.760]   Yes, it will.
[02:24:50.760 --> 02:24:54.520]   53 irresistible and unexpected recipes to make in a waffle iron.
[02:24:54.520 --> 02:24:56.920]   Now, Stacey, a point of order.
[02:24:56.920 --> 02:24:59.200]   I don't have one of them square waffle irons.
[02:24:59.200 --> 02:25:01.040]   I have a round Belgian waffle iron.
[02:25:01.040 --> 02:25:02.040]   Is that okay?
[02:25:02.040 --> 02:25:03.040]   Yeah, that's what I--
[02:25:03.040 --> 02:25:04.040]   You're still going to be fine.
[02:25:04.040 --> 02:25:05.240]   It won't come out square.
[02:25:05.240 --> 02:25:06.920]   That's the information you need to know.
[02:25:06.920 --> 02:25:07.920]   And you know what?
[02:25:07.920 --> 02:25:09.680]   I'll throw out another thing.
[02:25:09.680 --> 02:25:12.040]   The little dash mini waffle irons.
[02:25:12.040 --> 02:25:13.720]   Also in honor of National Waffle Day.
[02:25:13.720 --> 02:25:16.240]   They're like $10.
[02:25:16.240 --> 02:25:17.400]   Get one.
[02:25:17.400 --> 02:25:18.920]   They're delightful.
[02:25:18.920 --> 02:25:23.360]   And if you-- then you can keep a little waffle batter in your fridge and just pour it out,
[02:25:23.360 --> 02:25:24.360]   look at yourself, a little waffle.
[02:25:24.360 --> 02:25:27.200]   You can make a waffle sandwich with an egg in the morning.
[02:25:27.200 --> 02:25:28.680]   You should put that in your office there.
[02:25:28.680 --> 02:25:30.720]   Oh, and it comes in many colors.
[02:25:30.720 --> 02:25:33.120]   Aqua-- oh, not just colors, shapes.
[02:25:33.120 --> 02:25:34.120]   There's a bunny shape.
[02:25:34.120 --> 02:25:36.880]   Oh, you can have a skull shaped waffle.
[02:25:36.880 --> 02:25:37.880]   I see hearts.
[02:25:37.880 --> 02:25:42.680]   I feel like if I had a kid going away to college, I might get them this.
[02:25:42.680 --> 02:25:43.680]   A pumpkin.
[02:25:43.680 --> 02:25:44.680]   You're allowed to, Stacey.
[02:25:44.680 --> 02:25:47.360]   Oh, you can have a waffle iron, surely.
[02:25:47.360 --> 02:25:50.160]   I'd actually get my kid an induction burner, personally.
[02:25:50.160 --> 02:25:51.160]   Yeah, that's safe.
[02:25:51.160 --> 02:25:52.160]   All right.
[02:25:52.160 --> 02:25:53.160]   It is.
[02:25:53.160 --> 02:25:56.160]   It-- when you don't have something on it, it's all-- I mean, it turns out--
[02:25:56.160 --> 02:25:57.160]   So when you say many--
[02:25:57.160 --> 02:25:58.160]   --it's much safer than a hot plate.
[02:25:58.160 --> 02:25:59.160]   These are men.
[02:25:59.160 --> 02:26:00.160]   They're four inches.
[02:26:00.160 --> 02:26:01.160]   They're teensy.
[02:26:01.160 --> 02:26:02.160]   Yeah, they're this--
[02:26:02.160 --> 02:26:03.160]   Teeny, weeny.
[02:26:03.160 --> 02:26:04.160]   Teeny, weeny.
[02:26:04.160 --> 02:26:05.160]   You could make strip waffles.
[02:26:05.160 --> 02:26:06.160]   They're not this size.
[02:26:06.160 --> 02:26:07.160]   They're smaller.
[02:26:07.160 --> 02:26:08.160]   Oh, my God.
[02:26:08.160 --> 02:26:09.160]   Let go that egg.
[02:26:09.160 --> 02:26:11.160]   And there's a thumbnail.
[02:26:11.160 --> 02:26:12.160]   Oh, yeah.
[02:26:12.160 --> 02:26:13.160]   There's my model.
[02:26:13.160 --> 02:26:14.160]   Oh, yeah.
[02:26:14.160 --> 02:26:23.120]   So razor blade tears, a hard couple-- our novel, rather, on Amazon or Audible.
[02:26:23.120 --> 02:26:24.120]   Will it waffle?
[02:26:24.120 --> 02:26:25.120]   A waffle cookbook.
[02:26:25.120 --> 02:26:26.120]   It's a coffee.
[02:26:26.120 --> 02:26:29.120]   And the little mini waffle irons.
[02:26:29.120 --> 02:26:30.120]   There you go.
[02:26:30.120 --> 02:26:31.120]   Sorry, y'all.
[02:26:31.120 --> 02:26:33.120]   I just got so enthusiastic about waffle day.
[02:26:33.120 --> 02:26:34.120]   It's waffle day.
[02:26:34.120 --> 02:26:35.120]   I respect that.
[02:26:35.120 --> 02:26:36.120]   I respect that.
[02:26:36.120 --> 02:26:39.120]   Jeff Jarvis, your number of the week.
[02:26:39.120 --> 02:26:45.120]   Well, I want to mention just quickly that Cranky, which is a wonderful news site in Australia,
[02:26:45.120 --> 02:26:52.120]   said that the Murdochs were unindedical conspirators to Trump in January 6th.
[02:26:52.120 --> 02:26:55.120]   And Lachlan Murdoch threatened to sue them.
[02:26:55.120 --> 02:26:57.120]   It went back and forth and back and forth and back and forth.
[02:26:57.120 --> 02:27:01.120]   And finally, Cranky did a full page-- or a quarter page at The New York Times saying, "Come
[02:27:01.120 --> 02:27:02.120]   after us."
[02:27:02.120 --> 02:27:04.120]   And indeed, he did.
[02:27:04.120 --> 02:27:11.120]   He's suing Lachlan Murdoch as suing Cranky because Cranky said that the Murdochs were
[02:27:11.120 --> 02:27:12.120]   unindedical conspirators.
[02:27:12.120 --> 02:27:16.120]   I'm not going to go to Australia because now I'll get sued for repeating this.
[02:27:16.120 --> 02:27:20.120]   Cranky-- so in the US, of course, they'd be in the clear.
[02:27:20.120 --> 02:27:24.120]   But the problem is the Australian libel laws are off.
[02:27:24.120 --> 02:27:25.120]   It's Australian.
[02:27:25.120 --> 02:27:28.120]   And so they may, in fact, be liable.
[02:27:28.120 --> 02:27:30.120]   But they're going to fight it.
[02:27:30.120 --> 02:27:31.120]   And well, they should.
[02:27:31.120 --> 02:27:34.120]   Peter Frey, who follows the editor, is very good.
[02:27:34.120 --> 02:27:36.120]   So I want to mention that.
[02:27:36.120 --> 02:27:38.120]   It's also a great name for a newspaper.
[02:27:38.120 --> 02:27:39.120]   Cranky is perfect.
[02:27:39.120 --> 02:27:40.120]   Right.
[02:27:40.120 --> 02:27:41.120]   Cranky, mate.
[02:27:41.120 --> 02:27:49.120]   Ben Evans had some fun with the fact that The New York Times put a blog post on explaining
[02:27:49.120 --> 02:27:51.120]   how they use AI to get you to subscribe.
[02:27:51.120 --> 02:27:55.120]   This is the same newspaper that, of course, decryse using dark methods.
[02:27:55.120 --> 02:27:57.120]   Sorry, I can't admit this.
[02:27:57.120 --> 02:28:00.120]   To get you to do things.
[02:28:00.120 --> 02:28:04.120]   Look at the subscription funnel.
[02:28:04.120 --> 02:28:07.120]   I hate seeing slides, but it's funnels.
[02:28:07.120 --> 02:28:08.120]   Oh, God.
[02:28:08.120 --> 02:28:09.120]   I've seen so many.
[02:28:09.120 --> 02:28:16.120]   And then finally, according to Nielsen, for the first time, streaming has now certain
[02:28:16.120 --> 02:28:19.120]   usage has surpassed both broadcast and cable.
[02:28:19.120 --> 02:28:20.120]   Isn't that interesting?
[02:28:20.120 --> 02:28:22.120]   It's just Bremen World.
[02:28:22.120 --> 02:28:23.120]   Yep.
[02:28:23.120 --> 02:28:28.120]   The largest and Netflix number one by far, which is interesting.
[02:28:28.120 --> 02:28:32.120]   Then YouTube, Hulu, Prime, Disney, and HBO Max.
[02:28:32.120 --> 02:28:36.120]   Netflix has 8% of the market, HBO Max 1%.
[02:28:36.120 --> 02:28:40.120]   And then there are a lot of little other guys to make up 10%.
[02:28:40.120 --> 02:28:43.120]   So that, I thought, was very interesting.
[02:28:43.120 --> 02:28:49.120]   Well, Mr. Jarvis, I know this is the picks of the week and stuff, but we sort of saw it
[02:28:49.120 --> 02:28:54.120]   as coming, but was there anything the cable company could have done to prevent this?
[02:28:54.120 --> 02:28:55.120]   I don't think so.
[02:28:55.120 --> 02:28:57.120]   No, they have their own streaming channel.
[02:28:57.120 --> 02:29:02.120]   So you got HBO Max versus that, but it's really cord cutting and all that.
[02:29:02.120 --> 02:29:03.120]   I don't know.
[02:29:03.120 --> 02:29:04.120]   I don't know.
[02:29:04.120 --> 02:29:06.120]   Oh, this is just the way things are going.
[02:29:06.120 --> 02:29:07.120]   The progression.
[02:29:07.120 --> 02:29:09.120]   But the streaming is a pain in the bot.
[02:29:09.120 --> 02:29:10.120]   Do your boys--
[02:29:10.120 --> 02:29:11.120]   How so?
[02:29:11.120 --> 02:29:12.120]   How is it a pain?
[02:29:12.120 --> 02:29:14.120]   You literally pick up a phone.
[02:29:14.120 --> 02:29:15.120]   I'm an old guy.
[02:29:15.120 --> 02:29:18.120]   I'll put the clicker here.
[02:29:18.120 --> 02:29:20.120]   And I'm so amazing.
[02:29:20.120 --> 02:29:21.120]   We don't have a wire anymore.
[02:29:21.120 --> 02:29:23.120]   What's on Channel 4?
[02:29:23.120 --> 02:29:25.120]   You're watching a TV on a TV.
[02:29:25.120 --> 02:29:26.120]   Yeah.
[02:29:26.120 --> 02:29:27.120]   Don't watch it on a TV.
[02:29:27.120 --> 02:29:28.120]   Watch it on an iPad.
[02:29:28.120 --> 02:29:30.120]   A little market research.
[02:29:30.120 --> 02:29:31.120]   Do your boys--
[02:29:31.120 --> 02:29:35.120]   Do they watch primarily YouTube, I'm going to guess?
[02:29:35.120 --> 02:29:36.120]   Yes.
[02:29:36.120 --> 02:29:37.120]   YouTube.
[02:29:37.120 --> 02:29:38.120]   I do.
[02:29:38.120 --> 02:29:42.120]   YouTube and the popular service of Disney Plus.
[02:29:42.120 --> 02:29:43.120]   Oh, it's interesting.
[02:29:43.120 --> 02:29:44.120]   Those are the things.
[02:29:44.120 --> 02:29:46.120]   They dig Marvel and stuff.
[02:29:46.120 --> 02:29:48.120]   They dig all of those little different series.
[02:29:48.120 --> 02:29:51.120]   But they don't watch broadcast television.
[02:29:51.120 --> 02:29:53.120]   Not often.
[02:29:53.120 --> 02:29:57.120]   Only if they catch the parents watching reruns of something or other.
[02:29:57.120 --> 02:30:01.120]   But most of the time, they're watching a streaming service like Disney Plus.
[02:30:01.120 --> 02:30:05.120]   Yeah, you know what my kids said about broadcast television?
[02:30:05.120 --> 02:30:06.120]   What?
[02:30:06.120 --> 02:30:09.120]   I feel so sorry for my friends who have TV.
[02:30:09.120 --> 02:30:11.120]   They have to watch what's on the radio.
[02:30:11.120 --> 02:30:12.120]   Yeah.
[02:30:12.120 --> 02:30:18.120]   And even with that said, Miss Stacy, even-- I don't watch broadcast unless it's--
[02:30:18.120 --> 02:30:21.120]   I go to Hulu to watch reruns of old shows.
[02:30:21.120 --> 02:30:22.120]   I'm not even watching it live.
[02:30:22.120 --> 02:30:26.120]   The only time I'm watching it live is this weekend with the four season.
[02:30:26.120 --> 02:30:28.120]   I never watch it.
[02:30:28.120 --> 02:30:32.120]   And I'm going to-- don't forget, I am a former TV critic.
[02:30:32.120 --> 02:30:34.120]   And I heard.
[02:30:34.120 --> 02:30:36.120]   Yeah.
[02:30:36.120 --> 02:30:38.120]   And that's how I just-- some of it's just so off.
[02:30:38.120 --> 02:30:41.120]   That's how I end up seeing TMZ on Richard Simmons.
[02:30:41.120 --> 02:30:42.120]   An hour.
[02:30:42.120 --> 02:30:43.120]   An hour of that.
[02:30:43.120 --> 02:30:45.120]   Broadcast TV has just gone to the drag.
[02:30:45.120 --> 02:30:47.120]   This is nothing on it anymore.
[02:30:47.120 --> 02:30:51.120]   There is an assumption, though, that you have high speed internet.
[02:30:51.120 --> 02:30:52.120]   OK.
[02:30:52.120 --> 02:30:55.120]   You're not going to watch streaming if you don't have high speed internet.
[02:30:55.120 --> 02:30:58.120]   What is the penetration of high speed internet in the US, I wonder?
[02:30:58.120 --> 02:31:00.120]   Oh, I got a question.
[02:31:00.120 --> 02:31:01.120]   What is-- here we go.
[02:31:01.120 --> 02:31:03.120]   This is from-- let's see.
[02:31:03.120 --> 02:31:05.120]   Well, you can also watch it on your phone.
[02:31:05.120 --> 02:31:07.120]   Don't forget, you got the phone companies are saying--
[02:31:07.120 --> 02:31:08.120]   Yeah, yeah.
[02:31:08.120 --> 02:31:09.120]   Services are included.
[02:31:09.120 --> 02:31:13.120]   Watching the phone isn't very satisfying, though, is it?
[02:31:13.120 --> 02:31:18.120]   Nope, but everything is pushing us to create content for people to watch on the phones.
[02:31:18.120 --> 02:31:19.120]   Remember, quitting?
[02:31:19.120 --> 02:31:21.120]   Yeah, I like watching stuff on my phone.
[02:31:21.120 --> 02:31:23.120]   I like watching stuff on the iPad.
[02:31:23.120 --> 02:31:25.120]   We only watch-- we only have one TV.
[02:31:25.120 --> 02:31:26.120]   We only watch it.
[02:31:26.120 --> 02:31:28.120]   We watch the Sandman.
[02:31:28.120 --> 02:31:32.120]   And that's only because my daughter and I were watching it together.
[02:31:32.120 --> 02:31:33.120]   Yeah.
[02:31:33.120 --> 02:31:34.120]   No, there's just a bunch of--
[02:31:34.120 --> 02:31:36.120]   Oh, here's an interesting picture.
[02:31:36.120 --> 02:31:39.120]   This is from last year, Apple and Cider.
[02:31:39.120 --> 02:31:49.120]   Those blue counties are areas where fewer than 15% of the people are using the internet
[02:31:49.120 --> 02:31:52.120]   at 25 megabits or-- Wow.
[02:31:52.120 --> 02:31:53.120]   Or better?
[02:31:53.120 --> 02:31:56.120]   Yeah, some of those places, though, only have like six people.
[02:31:56.120 --> 02:31:57.120]   Yeah.
[02:31:57.120 --> 02:32:00.120]   They're mostly rural counties, absolutely.
[02:32:00.120 --> 02:32:03.120]   Although, much of Alaska.
[02:32:03.120 --> 02:32:08.120]   So, the percentage of people using the internet, 25 megabits are above per county.
[02:32:08.120 --> 02:32:12.120]   Less than 15% have broadband in the blue counties.
[02:32:12.120 --> 02:32:14.120]   And the gray counties is more than 15%.
[02:32:14.120 --> 02:32:17.120]   I'd like to see some other stats.
[02:32:17.120 --> 02:32:21.120]   But that's-- the blue counties here aren't watching streaming.
[02:32:21.120 --> 02:32:23.120]   I could tell you that.
[02:32:23.120 --> 02:32:29.120]   85% of people in these counties don't have enough internet bandwidth to watch streaming.
[02:32:29.120 --> 02:32:31.120]   They're just waiting on football season every time.
[02:32:31.120 --> 02:32:34.120]   They have enough to watch it on their phone, just not on their position.
[02:32:34.120 --> 02:32:35.120]   Probably.
[02:32:35.120 --> 02:32:39.040]   Well, I mean, yeah, if you're that rural, you may not have-- I mean, not even have high
[02:32:39.040 --> 02:32:41.120]   speed telephone.
[02:32:41.120 --> 02:32:45.120]   Anyway, Aunt Pruitt has a need.
[02:32:45.120 --> 02:32:47.120]   He has a need for something.
[02:32:47.120 --> 02:32:48.120]   I needed a laughs.
[02:32:48.120 --> 02:32:49.120]   These last couple of days.
[02:32:49.120 --> 02:32:51.120]   And I got some.
[02:32:51.120 --> 02:32:52.120]   I got some.
[02:32:52.120 --> 02:32:58.680]   The first one comes from-- actually, we just mentioned her regarding that F&Mika Twitter
[02:32:58.680 --> 02:32:59.680]   thing.
[02:32:59.680 --> 02:33:05.560]   But I wanted to put this out here for you because she had that same drone that you bought that
[02:33:05.560 --> 02:33:09.680]   I tried to tell you not to buy.
[02:33:09.680 --> 02:33:12.040]   You did try-- you tried hard.
[02:33:12.040 --> 02:33:13.040]   I follow Adriana.
[02:33:13.040 --> 02:33:14.280]   I really like her.
[02:33:14.280 --> 02:33:17.400]   In fact, she was the person who kind of introduced me to Black Twitter.
[02:33:17.400 --> 02:33:18.680]   I asked her, who should I follow?
[02:33:18.680 --> 02:33:19.680]   She told me you'd have followed.
[02:33:19.680 --> 02:33:20.680]   She's great.
[02:33:20.680 --> 02:33:21.680]   Oh, she is great.
[02:33:21.680 --> 02:33:25.760]   And that thread, though, she shares the thread of using that pixie drone.
[02:33:25.760 --> 02:33:27.320]   And it was such a great experience.
[02:33:27.320 --> 02:33:29.400]   And they pretty much should have made her an affiliate.
[02:33:29.400 --> 02:33:31.520]   She said, I disobeyed your product warnings.
[02:33:31.520 --> 02:33:37.040]   Let me tell you, is exactly the disaster your product managers figured it would be.
[02:33:37.040 --> 02:33:40.600]   Is this going to be a pixie drone example here?
[02:33:40.600 --> 02:33:44.400]   Let's see.
[02:33:44.400 --> 02:33:45.400]   And that's fine.
[02:33:45.400 --> 02:33:46.400]   And that's fine.
[02:33:46.400 --> 02:33:49.040]   That's what it's supposed to do.
[02:33:49.040 --> 02:33:54.480]   But further down in that thread, there's an interesting incident, which there are no
[02:33:54.480 --> 02:33:59.760]   pictures, but the pixie drone gets stuck in someone's hair.
[02:33:59.760 --> 02:34:00.760]   That's not good.
[02:34:00.760 --> 02:34:01.760]   Look at this.
[02:34:01.760 --> 02:34:02.760]   This is the orbit.
[02:34:02.760 --> 02:34:03.760]   That looks great.
[02:34:03.760 --> 02:34:04.760]   That's the orbit it goes around.
[02:34:04.760 --> 02:34:06.200]   And it wasn't just anyone's hair.
[02:34:06.200 --> 02:34:08.360]   It was a very fancy person.
[02:34:08.360 --> 02:34:09.840]   Well, it wasn't Adriana's hair.
[02:34:09.840 --> 02:34:11.240]   I could tell you that.
[02:34:11.240 --> 02:34:12.760]   Let's go down and see.
[02:34:12.760 --> 02:34:14.160]   There's no pictures of that.
[02:34:14.160 --> 02:34:15.800]   So she's using it a lot.
[02:34:15.800 --> 02:34:17.680]   I'm going to bring mine in next time.
[02:34:17.680 --> 02:34:19.920]   It's just sitting on my desk.
[02:34:19.920 --> 02:34:28.000]   This is the discontinued selfie drone that I bought for a fairly significant expense.
[02:34:28.000 --> 02:34:30.000]   It looks like she's getting the value out of it.
[02:34:30.000 --> 02:34:31.560]   She did pretty well with it.
[02:34:31.560 --> 02:34:36.920]   But again, at the very end, she talks about, whoops, it is thing ended up in someone's
[02:34:36.920 --> 02:34:38.720]   hair, like literally tangled.
[02:34:38.720 --> 02:34:45.360]   Like holy crap, I can't get this out of her hair.
[02:34:45.360 --> 02:34:47.080]   Well I wouldn't do it in a large group of people.
[02:34:47.080 --> 02:34:51.240]   I could do it somewhere where-- because the drone is going to run into people.
[02:34:51.240 --> 02:34:54.320]   And propellers got wrapped up in her hair in knots.
[02:34:54.320 --> 02:34:55.320]   And it-- yeah.
[02:34:55.320 --> 02:34:58.800]   I should have laughed at it when I was--
[02:34:58.800 --> 02:35:00.360]   Oh, it's going to hit this guy.
[02:35:00.360 --> 02:35:01.360]   Watch out.
[02:35:01.360 --> 02:35:02.360]   Watch out.
[02:35:02.360 --> 02:35:03.360]   No!
[02:35:03.360 --> 02:35:04.360]   Oh, that's it.
[02:35:04.360 --> 02:35:05.360]   That's it.
[02:35:05.360 --> 02:35:06.600]   She wasn't paying attention.
[02:35:06.600 --> 02:35:08.440]   She walked right into the thing.
[02:35:08.440 --> 02:35:13.200]   But this is-- you know what, Adriana, you probably shouldn't be using it at an event
[02:35:13.200 --> 02:35:14.760]   with people wandering around, right?
[02:35:14.760 --> 02:35:15.760]   Especially old--
[02:35:15.760 --> 02:35:17.680]   She's looking at the guy.
[02:35:17.680 --> 02:35:19.080]   She's looking at the guy.
[02:35:19.080 --> 02:35:21.600]   Here comes the drone named right for-- oh!
[02:35:21.600 --> 02:35:23.760]   Oh, I look.
[02:35:23.760 --> 02:35:24.760]   Oh.
[02:35:24.760 --> 02:35:25.760]   Oh.
[02:35:25.760 --> 02:35:26.760]   Her name was Kelly.
[02:35:26.760 --> 02:35:30.520]   She was incredibly patient as I attempted to disentangle the pixie propellers from her
[02:35:30.520 --> 02:35:34.440]   hair with the least amount of hair breakage.
[02:35:34.440 --> 02:35:37.640]   The friend was a lot less patient and said, "You just have to break it."
[02:35:37.640 --> 02:35:41.280]   And snapped one of the propeller guards right off the pixie.
[02:35:41.280 --> 02:35:42.280]   Yeah.
[02:35:42.280 --> 02:35:44.160]   Just wrong.
[02:35:44.160 --> 02:35:47.080]   If you ask me, I think she wanted to break it because she said, "This is a really dangerous
[02:35:47.080 --> 02:35:49.760]   thing to be using in a place with this many people."
[02:35:49.760 --> 02:35:53.680]   But I guess I don't have to tell you that now.
[02:35:53.680 --> 02:35:54.680]   Ha, ha, ha.
[02:35:54.680 --> 02:35:57.120]   Yeah, that woman sounded like a piece of work.
[02:35:57.120 --> 02:35:59.760]   Little belligerent.
[02:35:59.760 --> 02:36:00.760]   Little belligerent.
[02:36:00.760 --> 02:36:01.760]   I love that thread.
[02:36:01.760 --> 02:36:02.760]   That thread made my day.
[02:36:02.760 --> 02:36:03.760]   Wow.
[02:36:03.760 --> 02:36:06.600]   37-- 31 tweet thread by Adriana Langston.
[02:36:06.600 --> 02:36:07.600]   Nice.
[02:36:07.600 --> 02:36:08.600]   Absolute--
[02:36:08.600 --> 02:36:09.600]   It's a good thread.
[02:36:09.600 --> 02:36:10.600]   Don't let the 31 tweets fool you.
[02:36:10.600 --> 02:36:11.600]   It's worth it.
[02:36:11.600 --> 02:36:12.600]   It's an absolute classic.
[02:36:12.600 --> 02:36:13.600]   It's very funny.
[02:36:13.600 --> 02:36:14.600]   It's a gem.
[02:36:14.600 --> 02:36:15.600]   Yeah.
[02:36:15.600 --> 02:36:16.600]   And she's great.
[02:36:16.600 --> 02:36:18.040]   I follow her on Twitter too.
[02:36:18.040 --> 02:36:20.040]   Yeah, I love her.
[02:36:20.040 --> 02:36:22.520]   And she's always tweeting at us saying stuff.
[02:36:22.520 --> 02:36:23.520]   Mm-hmm.
[02:36:23.520 --> 02:36:24.520]   Yep.
[02:36:24.520 --> 02:36:26.480]   You said you had something else that would make you smile?
[02:36:26.480 --> 02:36:27.480]   Yeah.
[02:36:27.480 --> 02:36:30.960]   Number two is one of my favorite comedians, Mr. Alonzo Bowden.
[02:36:30.960 --> 02:36:31.960]   If you skip to 37--
[02:36:31.960 --> 02:36:35.120]   I love Alonzo Bowden.
[02:36:35.120 --> 02:36:40.560]   This bit is just-- this wasn't televised.
[02:36:40.560 --> 02:36:42.960]   This was just done strictly for YouTube.
[02:36:42.960 --> 02:36:45.320]   And it turned out my sound if you would.
[02:36:45.320 --> 02:36:46.840]   Oh, no, I turned it off.
[02:36:46.840 --> 02:36:47.840]   Never mind.
[02:36:47.840 --> 02:36:48.840]   Yeah.
[02:36:48.840 --> 02:36:49.840]   I sorry.
[02:36:49.840 --> 02:36:50.840]   Here we go.
[02:36:50.840 --> 02:36:51.840]   Here we go.
[02:36:51.840 --> 02:36:52.840]   Talk show.
[02:36:52.840 --> 02:36:56.160]   On average, women use three times as many words a day as men.
[02:36:56.160 --> 02:36:58.440]   Because women speak in detail.
[02:36:58.440 --> 02:37:01.360]   And it's three times as many words a day.
[02:37:01.360 --> 02:37:02.360]   Men just run.
[02:37:02.360 --> 02:37:04.480]   So what about a week?
[02:37:04.480 --> 02:37:05.960]   Month.
[02:37:05.960 --> 02:37:07.360]   Year.
[02:37:07.360 --> 02:37:09.160]   Or years.
[02:37:09.160 --> 02:37:12.760]   There are men who've been married in excess of 20 years.
[02:37:12.760 --> 02:37:16.400]   How many words have they had to listen to?
[02:37:16.400 --> 02:37:19.160]   It's mind-boggling.
[02:37:19.160 --> 02:37:20.160]   And you could tell.
[02:37:20.160 --> 02:37:24.760]   You could tell the old veterans because they have a filter built in.
[02:37:24.760 --> 02:37:27.680]   And they don't listen to all the words.
[02:37:27.680 --> 02:37:29.680]   They only hear their assignment.
[02:37:29.680 --> 02:37:32.680]   She'll just be talking.
[02:37:32.680 --> 02:37:36.040]   The grocery store.
[02:37:36.040 --> 02:37:37.040]   Milph.
[02:37:37.040 --> 02:37:38.040]   Mm-hmm.
[02:37:38.040 --> 02:37:39.040]   Cereal.
[02:37:39.040 --> 02:37:40.040]   No, no, no.
[02:37:40.040 --> 02:37:45.720]   I'm going to go to the grocery store and get some cereal and milk on the way back.
[02:37:45.720 --> 02:37:48.000]   I'm going to get gas in the car.
[02:37:48.000 --> 02:37:49.520]   And she's like, "I'm glad you were listening."
[02:37:49.520 --> 02:37:51.360]   And he's like, "Oh yeah, I was listening."
[02:37:51.360 --> 02:37:52.520]   That's so true.
[02:37:52.520 --> 02:37:54.240]   I totally do that.
[02:37:54.240 --> 02:37:56.240]   I live that every day.
[02:37:56.240 --> 02:37:57.240]   Every day.
[02:37:57.240 --> 02:37:58.240]   Every day.
[02:37:58.240 --> 02:38:00.000]   So it is Andrew, I hear.
[02:38:00.000 --> 02:38:04.400]   Andrew speaks more than I do.
[02:38:04.400 --> 02:38:05.400]   That's interesting.
[02:38:05.400 --> 02:38:09.720]   That's if we ask Andrew that would he say that's true?
[02:38:09.720 --> 02:38:11.040]   He probably would.
[02:38:11.040 --> 02:38:15.280]   I mean, they're at Wednesdays after I've talked to you all all day.
[02:38:15.280 --> 02:38:17.080]   You have nothing to say.
[02:38:17.080 --> 02:38:18.760]   I go and read a book.
[02:38:18.760 --> 02:38:23.520]   That's my excuse too, is that well, honey, I've been talking all day.
[02:38:23.520 --> 02:38:27.320]   I don't want to talk anymore.
[02:38:27.320 --> 02:38:28.320]   I don't.
[02:38:28.320 --> 02:38:29.840]   I think he's pretty accurate.
[02:38:29.840 --> 02:38:31.080]   I'll just say that.
[02:38:31.080 --> 02:38:32.360]   I don't mind it.
[02:38:32.360 --> 02:38:33.360]   It's a fact.
[02:38:33.360 --> 02:38:34.720]   One more thing.
[02:38:34.720 --> 02:38:37.560]   I just thought about this.
[02:38:37.560 --> 02:38:40.840]   Mr. Jarvis, I've been watching musicals here recently.
[02:38:40.840 --> 02:38:41.840]   What?
[02:38:41.840 --> 02:38:42.840]   What?
[02:38:42.840 --> 02:38:47.320]   One in particular, I just stuck it in our scene.
[02:38:47.320 --> 02:38:50.040]   And in our discord.
[02:38:50.040 --> 02:39:01.560]   Been watching that one in particular because somebody has now been nabbed as Sandy Cheeks.
[02:39:01.560 --> 02:39:03.760]   He's going to be in Sun.
[02:39:03.760 --> 02:39:05.560]   He or she?
[02:39:05.560 --> 02:39:06.560]   She.
[02:39:06.560 --> 02:39:08.960]   The point Pruitt is going to be in a play.
[02:39:08.960 --> 02:39:09.960]   Wow.
[02:39:09.960 --> 02:39:14.960]   She's going to be Sandy Cheeks in the SpongeBob musical.
[02:39:14.960 --> 02:39:15.960]   Yes.
[02:39:15.960 --> 02:39:16.960]   I love it.
[02:39:16.960 --> 02:39:17.960]   That is correct.
[02:39:17.960 --> 02:39:18.960]   This is justice.
[02:39:18.960 --> 02:39:19.960]   This is.
[02:39:19.960 --> 02:39:23.400]   I don't share a lot of personal private stuff because the whole world ain't got to know.
[02:39:23.400 --> 02:39:24.760]   But this was pretty big.
[02:39:24.760 --> 02:39:25.960]   I'm quite proud of her.
[02:39:25.960 --> 02:39:26.960]   She's excited.
[02:39:26.960 --> 02:39:27.960]   She wanted to get into check.
[02:39:27.960 --> 02:39:28.960]   She's going to do so well.
[02:39:28.960 --> 02:39:29.960]   Does she sing?
[02:39:29.960 --> 02:39:31.360]   Yes, she can sing.
[02:39:31.360 --> 02:39:34.640]   We're going to see this.
[02:39:34.640 --> 02:39:37.960]   The whole company is coming to see this.
[02:39:37.960 --> 02:39:42.960]   We just got to call a couple hours ago.
[02:39:42.960 --> 02:39:46.600]   Ashley was right before she got the call that she was in.
[02:39:46.600 --> 02:39:51.400]   And yeah, she sort of been on her grind trying to work on this craft because she's like,
[02:39:51.400 --> 02:39:52.400]   I should act.
[02:39:52.400 --> 02:39:56.640]   I should be an actress and say, hey, we'll get to work, do the work.
[02:39:56.640 --> 02:39:57.640]   And she has.
[02:39:57.640 --> 02:40:01.880]   And she has the option of two different plays actually.
[02:40:01.880 --> 02:40:02.880]   And that one is.
[02:40:02.880 --> 02:40:05.280]   That one is the one she wanted the most.
[02:40:05.280 --> 02:40:06.280]   Okay.
[02:40:06.280 --> 02:40:07.280]   A musical.
[02:40:07.280 --> 02:40:13.040]   See, you're going to be singing cat songs before we know it.
[02:40:13.040 --> 02:40:20.120]   Santa Rosa Junior College Theater Arts November 18 through December 4.
[02:40:20.120 --> 02:40:25.880]   Everybody needs to come to Sonoma County to see SpongeBob the musical featuring Queen
[02:40:25.880 --> 02:40:28.480]   Pruitt as Sandy Cheeks.
[02:40:28.480 --> 02:40:29.480]   Sandy Cheeks.
[02:40:29.480 --> 02:40:33.680]   All of Bikini Bottom is going to celebrate.
[02:40:33.680 --> 02:40:34.680]   She told me about this.
[02:40:34.680 --> 02:40:35.680]   That's so great.
[02:40:35.680 --> 02:40:40.760]   She's like, well, then we don't like musicals, but you'd have to go.
[02:40:40.760 --> 02:40:41.760]   And I'm like, yeah, I know.
[02:40:41.760 --> 02:40:42.760]   Oh, forget, go.
[02:40:42.760 --> 02:40:45.920]   She's going to be rehearsing all around the house for the next three months.
[02:40:45.920 --> 02:40:47.480]   Oh, dude, you don't know.
[02:40:47.480 --> 02:40:52.760]   I've been watching this musical for the last three days with our one and two night.
[02:40:52.760 --> 02:40:54.760]   Son of a mother.
[02:40:54.760 --> 02:40:55.760]   Justice.
[02:40:55.760 --> 02:41:00.760]   We got a new show for you.
[02:41:00.760 --> 02:41:03.760]   We call it Host Hands On Showtunes.
[02:41:03.760 --> 02:41:06.760]   You're going to be the host of Host.
[02:41:06.760 --> 02:41:08.760]   It's going to be so exciting.
[02:41:08.760 --> 02:41:09.760]   Yeah.
[02:41:09.760 --> 02:41:10.760]   Nope.
[02:41:10.760 --> 02:41:11.760]   Nope.
[02:41:11.760 --> 02:41:12.760]   No.
[02:41:12.760 --> 02:41:13.760]   Mebrieee.
[02:41:13.760 --> 02:41:16.760]   Actually, Leo and I will come on and sing our favorite songs.
[02:41:16.760 --> 02:41:17.760]   How's that, Leo?
[02:41:17.760 --> 02:41:18.760]   Oh, no.
[02:41:18.760 --> 02:41:19.760]   Yeah.
[02:41:19.760 --> 02:41:20.760]   No.
[02:41:20.760 --> 02:41:21.760]   No.
[02:41:21.760 --> 02:41:22.760]   No.
[02:41:22.760 --> 02:41:23.760]   No.
[02:41:23.760 --> 02:41:24.760]   No.
[02:41:24.760 --> 02:41:31.320]   I was just doing Samwa Samwa Tizai. I'm far too new
[02:41:31.320 --> 02:41:34.080]   Jeremy please kill the tricaster
[02:41:34.080 --> 02:41:40.600]   I wonder if he was doing Camelot do Camelot that was that's from Camelot
[02:41:40.600 --> 02:41:48.180]   I was doing lights in the rubberhoulet golay thing, but before that I was doing I wonder what the king is doing tonight
[02:41:48.180 --> 02:41:50.020]   I was doing the Richard Burton
[02:41:50.960 --> 02:41:58.780]   I'll save that for another day. I won't get a lot of skinless moths on that in her journeys and so forth
[02:41:58.780 --> 02:42:03.720]   So I'll let you all know how it all goes and what not. She's pretty pumped. I think she's gonna do well
[02:42:03.720 --> 02:42:10.120]   She pretty much knows all the songs and lines already even though they haven't had rehearsal. That's exciting
[02:42:10.120 --> 02:42:16.360]   That's great. No, that's a big part. That's one of the you know the one of the big characters in in the bikini bottom
[02:42:16.360 --> 02:42:19.460]   SpongeBob Patrick Sandy and Squidward
[02:42:20.200 --> 02:42:22.200]   There's a big four right there
[02:42:22.200 --> 02:42:32.320]   I can't move reason I see ants expression is priceless. He seems totally worn over the thought of SpongeBob. Oh, yeah, you nailed it
[02:42:32.320 --> 02:42:34.320]   It's true. It's true
[02:42:34.320 --> 02:42:43.500]   It's the God's truth that's a proof at hands-on photography very exciting interesting controversial episode this week all about any
[02:42:43.500 --> 02:42:45.500]   Leibovitz that'll be very interesting
[02:42:45.760 --> 02:42:53.160]   Of course, he's our sir community manager in the club twit and tomorrow morning at the ungodly hour
[02:42:53.160 --> 02:43:01.200]   Nine a was not a name and your day off. It's Leo's day off and stay in
[02:43:01.200 --> 02:43:04.720]   And walking
[02:43:04.720 --> 02:43:09.720]   We'll be discussing Clara and the Sun. I still have six hours to go
[02:43:09.720 --> 02:43:11.720]   So
[02:43:11.960 --> 02:43:14.840]   Really tired because I'm gonna stay up all night reading this damn thing
[02:43:14.840 --> 02:43:17.000]   But it's actually a great book
[02:43:17.000 --> 02:43:23.240]   We'll discuss it and the book club and if you're not there at 9 a.m. Because you're sleeping like a normal human
[02:43:23.240 --> 02:43:29.640]   It'll be on demand also on the twit plus feed. Yeah, I love this book club though Stacy. Thank you for letting me come
[02:43:29.640 --> 02:43:35.240]   I'm kind of horny then I appreciate it. Yeah, well if you keep complaining, I'm just gonna say no
[02:43:39.720 --> 02:43:42.680]   No complainers no complainers
[02:43:42.680 --> 02:43:47.440]   All right, all right. I love it 9 a.m. Go get up bright and early
[02:43:47.440 --> 02:43:54.260]   Got some good books. I've got some good books for the next two. Yeah. Yeah, I love your recommendations
[02:43:54.260 --> 02:44:00.800]   So that's for club to it members of course twit.tv/club to it seven bucks a month
[02:44:00.800 --> 02:44:05.160]   It's month to month. So, you know, you could just buy it once if you want to just do that
[02:44:05.160 --> 02:44:09.480]   But please please don't join it
[02:44:10.440 --> 02:44:15.240]   I'm gonna bring some waffles. That's what I'll do. I'll bring waffles tomorrow. That'll kill you. Yeah, yeah
[02:44:15.240 --> 02:44:18.720]   Then I'll go into diabetic shock and that'll be the end of the show
[02:44:18.720 --> 02:44:23.920]   I'm gonna zoom in. I'm not coming in. No, no, I'm zooming with my waffler
[02:44:23.920 --> 02:44:31.960]   Stacy Egan boss of his PJs. Will you be in your PJs? I knew it is here wasn't even brushed the last time it was a bit rough
[02:44:31.960 --> 02:44:36.640]   We don't judge
[02:44:36.640 --> 02:44:38.640]   But it was not the polished
[02:44:38.640 --> 02:44:43.680]   His hair was all of his head his glasses were crooked. I mean boy
[02:44:43.680 --> 02:44:47.280]   Haggart
[02:44:47.280 --> 02:44:52.760]   Alright, I'm gonna I'm gonna up my game tomorrow
[02:44:52.760 --> 02:44:58.840]   Stacy Egan-Botham Stacy on IOT comm is her website
[02:44:58.840 --> 02:45:06.320]   Subscribe to the free newsletter check out the events and of course you must listen every week to this IOT podcast that Stacy does
[02:45:06.480 --> 02:45:08.480]   with Kevin Tufal
[02:45:08.480 --> 02:45:10.960]   And it's not boring even
[02:45:10.960 --> 02:45:23.800]   Jeff Jarvis he is the director of the townite Center for entrepreneurial journalism at the
[02:45:23.800 --> 02:45:33.680]   Graduate School of Journalism at the City University of New York and
[02:45:35.520 --> 02:45:38.280]   Buzz machine calm a good friend. Thank you very much
[02:45:38.280 --> 02:45:41.280]   Google shareholder former TV guide critic
[02:45:41.280 --> 02:45:44.960]   Frank Sinatra called him a bum Ray crock called him a nickel millionaire
[02:45:44.960 --> 02:45:48.280]   Murphy Brown called him a bottomless pit of eight
[02:45:48.280 --> 02:45:50.400]   Formally
[02:45:50.400 --> 02:45:55.200]   Formerly one of San Francisco's 100 most eligible bachelors. He used to live in tomato fields
[02:45:55.200 --> 02:46:02.840]   He is a human DDoS and the L. Hefei to Taco Bell does not take criticism. Well is trapped in the jungle gym of life
[02:46:03.120 --> 02:46:11.200]   He's an online man a certified big guy disaster in the grocery store the author of all about email all about Andra's email of the week
[02:46:11.200 --> 02:46:14.480]   23rd 2021
[02:46:14.480 --> 02:46:21.280]   Almost a Jersey mall mogul former cover boy for your favorite pizza menu mr. Jeff. Oh
[02:46:21.280 --> 02:46:24.000]   Dude
[02:46:29.440 --> 02:46:34.680]   Dominated what did you add on you add it on something there pizza menu cover boy was added on oh I see right okay good
[02:46:34.680 --> 02:46:39.640]   The last minute well as you as you tell us more about you see four things about my fascinating life
[02:46:39.640 --> 02:46:44.880]   We add them to the end of the and once you laminated just kind of hard so we just got to write it
[02:46:44.880 --> 02:46:52.920]   Sharpie we do twig every Wednesday round about 2 p.m. Pacific 5 p.m. Eastern 2100 UTC
[02:46:52.920 --> 02:46:57.040]   You can watch it live at live dot twit TV or listen live. There's a live stream
[02:46:57.520 --> 02:47:03.680]   For audio and video you can also chat with us at irc.twit.tv if your club member course
[02:47:03.680 --> 02:47:09.320]   The discord is also a good place to go for chatting after the fact on demand versions that show are available at
[02:47:09.320 --> 02:47:17.160]   The website twit.tv/twig there's a YouTube channel dedicated to it and you can of course
[02:47:17.160 --> 02:47:24.400]   Subscribe in your favorite podcast client and that way you'll get it automatically the minute it's available
[02:47:25.240 --> 02:47:30.600]   And if you do have a podcast client that allows you to leave reviews, please give us a five-star review that would be much
[02:47:30.600 --> 02:47:33.240]   appreciated
[02:47:33.240 --> 02:47:35.240]   Thank you all for being here
[02:47:35.240 --> 02:47:39.720]   See you next time on this week in Google bye-bye
[02:47:39.720 --> 02:47:46.280]   Hey, I'm Rod Pyle editor of Ad Astra magazine and each week
[02:47:46.280 --> 02:47:53.200]   I'm joined by Tariq Malek the editor-in-chief over at space.com in our new this week in space podcast every Friday
[02:47:53.200 --> 02:47:56.760]   Tariq and I take a deep dive into the stories that define the new space age
[02:47:56.760 --> 02:48:00.400]   What's NASA up to when will Americans once again set foot on the moon?
[02:48:00.400 --> 02:48:04.000]   And how about those samples from the Perseverance rover when are those coming on?
[02:48:04.000 --> 02:48:10.860]   What the heck is Elon Musk done now in addition to all the latest and greatest in space exploration will take an occasional look at
[02:48:10.860 --> 02:48:16.760]   bits of space flight history that you probably never heard of and all with an eye towards having a good time along the way
[02:48:16.760 --> 02:48:19.160]   Check us out in your favorite podcatcher
[02:48:19.160 --> 02:48:21.160]   You
[02:48:21.160 --> 02:48:29.520]   [ Music ]

