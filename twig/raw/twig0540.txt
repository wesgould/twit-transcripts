;FFMETADATA1
title=Our Best of 2019
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=540
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:07.000]   Hey everybody! Happy Holidays, Leo Laporte here with the last this week in Google of 2019.
[00:00:07.000 --> 00:00:12.000]   And as we do traditionally, we've decided to take some of the best moments from 2019
[00:00:12.000 --> 00:00:17.000]   and put it into a year and best of. That's what's next.
[00:00:17.000 --> 00:00:21.000]   Podcasts you love.
[00:00:21.000 --> 00:00:23.000]   From people you trust.
[00:00:23.000 --> 00:00:26.000]   This is Detroit.
[00:00:26.000 --> 00:00:32.000]   This is Twig. This Week in Google.
[00:00:32.000 --> 00:00:37.000]   Episode 540 for December 25th, 2019.
[00:00:37.000 --> 00:00:40.000]   The year's best.
[00:00:40.000 --> 00:00:45.000]   As I say at the beginning of every show, this week at Google isn't just about Google.
[00:00:45.000 --> 00:00:49.000]   It's really about the Google verse which includes Facebook and Twitter,
[00:00:49.000 --> 00:00:52.000]   online journalism, blogging, everything online.
[00:00:52.000 --> 00:00:54.000]   Of course there's a lot of Google in it,
[00:00:54.000 --> 00:00:57.000]   but sometimes there's some Facebook in it too.
[00:00:57.000 --> 00:01:03.000]   And actually frankly Facebook often makes some of the best and most interesting conversations
[00:01:03.000 --> 00:01:04.000]   on this week in Google.
[00:01:04.000 --> 00:01:11.000]   Going way back to January when we talked about Facebook's 10 year photo challenge,
[00:01:11.000 --> 00:01:18.000]   which by the way resurfaced every few months on Facebook all through the year 2019.
[00:01:18.000 --> 00:01:21.000]   Is it a threat or just something fun?
[00:01:21.000 --> 00:01:24.000]   Oh, I was going to use this as my number.
[00:01:24.000 --> 00:01:25.000]   I was going to use this one.
[00:01:25.000 --> 00:01:26.000]   I was going to put it up here.
[00:01:26.000 --> 00:01:28.000]   Oh yeah, I got something to say about this one.
[00:01:28.000 --> 00:01:29.000]   Oh yeah.
[00:01:29.000 --> 00:01:32.000]   That woman speaking is exactly how I think about everything.
[00:01:32.000 --> 00:01:35.000]   You're going to read surveillance capitalism Stacy.
[00:01:35.000 --> 00:01:41.000]   It's a great but 625 pages, but read it and then we'll debate it and you have it read it yet.
[00:01:41.000 --> 00:01:42.000]   We talk about that story.
[00:01:42.000 --> 00:01:43.000]   Can we talk about that story?
[00:01:43.000 --> 00:01:44.000]   Let's do the 10 year meeting.
[00:01:44.000 --> 00:01:50.000]   So this is the Facebook 10 year challenge where pleat and utter bull.
[00:01:50.000 --> 00:01:51.000]   It is moral panic.
[00:01:51.000 --> 00:01:52.000]   Great example.
[00:01:52.000 --> 00:01:53.000]   Oh yeah.
[00:01:53.000 --> 00:01:54.000]   Great example.
[00:01:54.000 --> 00:01:57.000]   If you use social panic or technical panic, I'm going to give you some pack.
[00:01:57.000 --> 00:01:58.000]   Some moral panic.
[00:01:58.000 --> 00:01:59.000]   I'm going to give you some facts.
[00:01:59.000 --> 00:02:00.000]   Let me give you the story first.
[00:02:00.000 --> 00:02:02.000]   Cato Neil writing in Wired.
[00:02:02.000 --> 00:02:07.000]   If you use social media, you've probably noticed a trend across Facebook, Instagram and Twitter
[00:02:07.000 --> 00:02:12.000]   of people posting their then and now profile pictures mostly from 10 years ago and this
[00:02:12.000 --> 00:02:13.000]   year.
[00:02:13.000 --> 00:02:17.000]   She said I put out a flippant tweet.
[00:02:17.000 --> 00:02:20.880]   Me 10 years ago probably would have played along with the profile picture aging meme going
[00:02:20.880 --> 00:02:27.160]   around me now ponders how all this data could be mined to train facial recognition algorithms
[00:02:27.160 --> 00:02:30.920]   on age progression and age recognition.
[00:02:30.920 --> 00:02:32.160]   I agree with her.
[00:02:32.160 --> 00:02:33.800]   That's exactly what this is all about.
[00:02:33.800 --> 00:02:35.600]   You disagree, Jeff?
[00:02:35.600 --> 00:02:38.120]   So yes, let me so I will be on through this today.
[00:02:38.120 --> 00:02:44.440]   So number one, this is absurd because two years ago Google showed that they can they can
[00:02:44.440 --> 00:02:49.240]   compare the same person in photos from infancy to current.
[00:02:49.240 --> 00:02:51.000]   It's been done.
[00:02:51.000 --> 00:02:55.280]   Number two, this would be a really crappy data set.
[00:02:55.280 --> 00:02:59.000]   Facebook also has data of people who are identified from ages to ages.
[00:02:59.000 --> 00:03:00.800]   This is pure paranoia.
[00:03:00.800 --> 00:03:07.920]   So I went in and I have a conversation with her and her editor, Nick Thompson on Twitter.
[00:03:07.920 --> 00:03:13.440]   And I said those things and I said this is kind of ridiculous.
[00:03:13.440 --> 00:03:20.880]   Nick came back and said, so then the response was, well, Facebook is doing this for money.
[00:03:20.880 --> 00:03:23.640]   Well Facebook by the way says we didn't come up with this.
[00:03:23.640 --> 00:03:25.400]   They didn't do this.
[00:03:25.400 --> 00:03:30.440]   And so it's yeah, it's just moral panic because an example of the kind of paranoia that is
[00:03:30.440 --> 00:03:31.440]   now bred.
[00:03:31.440 --> 00:03:33.000]   A simple little thing comes up.
[00:03:33.000 --> 00:03:34.000]   People have pictures from 10 years.
[00:03:34.000 --> 00:03:35.000]   This is cute.
[00:03:35.000 --> 00:03:36.000]   This is fine.
[00:03:36.000 --> 00:03:37.720]   A few years ago we all would have joined in and said fine.
[00:03:37.720 --> 00:03:43.200]   So now there is this presumption of some horrible motive and some horrible conspiracy.
[00:03:43.200 --> 00:03:44.720]   It's a conspiracy theorizing.
[00:03:44.720 --> 00:03:50.680]   It is moral panic that's trying to blame everything and find an anxious motive for everything that
[00:03:50.680 --> 00:03:52.320]   anyone sees on technology.
[00:03:52.320 --> 00:03:54.120]   And this is the danger, my friends.
[00:03:54.120 --> 00:04:00.280]   The danger is this kind of crap, this kind of fake news will lead to regulation of the
[00:04:00.280 --> 00:04:02.240]   Internet for us all.
[00:04:02.240 --> 00:04:05.840]   Holy cow, you could not be more wrong.
[00:04:05.840 --> 00:04:07.440]   I had a fit.
[00:04:07.440 --> 00:04:09.960]   Well Stacy, the story is just wrong.
[00:04:09.960 --> 00:04:12.000]   The story is just wrong on every count.
[00:04:12.000 --> 00:04:19.480]   Okay, her thinking on this topic is what I am identifying with.
[00:04:19.480 --> 00:04:21.160]   Not the entire story.
[00:04:21.160 --> 00:04:24.960]   But I would say this is actually a positive.
[00:04:24.960 --> 00:04:32.240]   This is where we want to get people when they are about to use technology to ask themselves,
[00:04:32.240 --> 00:04:35.760]   is it worth the exchange of data that I'm about?
[00:04:35.760 --> 00:04:37.680]   What is going to come of this?
[00:04:37.680 --> 00:04:40.080]   I think that is actually so smart.
[00:04:40.080 --> 00:04:41.840]   It's like, tell me the harm.
[00:04:41.840 --> 00:04:48.120]   It's like, hold on, it is like teaching people about interest rates, which have a purpose,
[00:04:48.120 --> 00:04:49.920]   but also can cause people harm.
[00:04:49.920 --> 00:04:55.440]   I'm not saying usually, I'm just saying interest rates associated with credit cards, right?
[00:04:55.440 --> 00:05:00.720]   Now the other part of this is she's not saying it's evil or there's harm.
[00:05:00.720 --> 00:05:06.080]   She's just wondering if it's going to be used for image progression.
[00:05:06.080 --> 00:05:08.520]   And Google has shown that they can do this.
[00:05:08.520 --> 00:05:10.600]   Facebook might need that data.
[00:05:10.600 --> 00:05:13.320]   Another company that puts these out might need that data.
[00:05:13.320 --> 00:05:20.280]   And this sort of data has been collected in the past through fun, cutesy apps and used
[00:05:20.280 --> 00:05:22.680]   for exactly that purpose.
[00:05:22.680 --> 00:05:27.320]   So it has a basis, in fact, to wonder about it.
[00:05:27.320 --> 00:05:34.080]   So her boss, well, like a lot, Nick Thompson, I'm trying to find the tweet right now.
[00:05:34.080 --> 00:05:35.800]   This is what got me going.
[00:05:35.800 --> 00:05:36.800]   Okay, what?
[00:05:36.800 --> 00:05:38.800]   Yeah, cause you're really upset.
[00:05:38.800 --> 00:05:42.840]   I'm very, I was very upset about this because it, because it is dangerous.
[00:05:42.840 --> 00:05:44.560]   And I want to, I want to read his tweet.
[00:05:44.560 --> 00:05:46.360]   And again, I have immense respect for Nick.
[00:05:46.360 --> 00:05:47.560]   He's an excellent editor.
[00:05:47.560 --> 00:05:48.560]   Wired, he under him.
[00:05:48.560 --> 00:05:50.960]   Wired's been just great.
[00:05:50.960 --> 00:05:51.960]   And he came back and we had it.
[00:05:51.960 --> 00:05:54.520]   There was an exchange and he came back and said what I said was fair.
[00:05:54.520 --> 00:05:56.600]   Now I'm trying to find it because I'm between.
[00:05:56.600 --> 00:06:02.560]   I feel like, of course, this is relatively benign, but I don't think it's a bad thing
[00:06:02.560 --> 00:06:07.960]   to alert people to the fact that this kind of data is something these companies want.
[00:06:07.960 --> 00:06:11.560]   And you should be aware that you might be giving them this kind of information.
[00:06:11.560 --> 00:06:13.040]   Like my people are not aware.
[00:06:13.040 --> 00:06:17.200]   I just want to give you one more data point, which is a study that just came out from the
[00:06:17.200 --> 00:06:24.720]   Pew Research Center, three quarters of Facebook users, 74% in their survey did not know the
[00:06:24.720 --> 00:06:30.520]   social networking behemoth, according to TechCrunch, maintains a list of their interest
[00:06:30.520 --> 00:06:35.480]   and traits to target with a map with ads, 74% of the Facebook users.
[00:06:35.480 --> 00:06:37.760]   Do they know that the New York Times does that?
[00:06:37.760 --> 00:06:38.760]   Wired does that?
[00:06:38.760 --> 00:06:39.760]   And Time Maker does that?
[00:06:39.760 --> 00:06:43.000]   If they don't think Facebook does it, they certainly don't think anybody else does it.
[00:06:43.000 --> 00:06:44.000]   So they don't know where they can be.
[00:06:44.000 --> 00:06:48.520]   And they only discovered this when researchers directed them to view their Facebook ad preferences
[00:06:48.520 --> 00:06:49.520]   page.
[00:06:49.520 --> 00:06:55.080]   A majority, 51% said they were uncomfortable with Facebook compiling the information.
[00:06:55.080 --> 00:06:59.880]   27% said the ad preference listing Facebook had generated was inaccurate.
[00:06:59.880 --> 00:07:05.560]   88% of polled users had some material generated for them on the ad preferences page.
[00:07:05.560 --> 00:07:10.000]   So the big number, the takeaway is three quarters of the Facebook user survey, and I
[00:07:10.000 --> 00:07:13.880]   think is probably accurate, Pew's very good on these, three quarters didn't even know
[00:07:13.880 --> 00:07:18.760]   that Facebook targets them with ads based on what they give Facebook.
[00:07:18.760 --> 00:07:21.360]   We go back to the, I found the tweet.
[00:07:21.360 --> 00:07:26.360]   So this stops again in, hold on a second, in regard to this, it's of, I think, a value
[00:07:26.360 --> 00:07:30.240]   to say, hey, you know, this 10 year progression thing, here's the kind of thing companies might
[00:07:30.240 --> 00:07:32.640]   want to do with that.
[00:07:32.640 --> 00:07:36.040]   So Nick Thompson, again, wire editor, respectable immensely, I think he's great.
[00:07:36.040 --> 00:07:37.040]   Right.
[00:07:37.040 --> 00:07:40.040]   His tweet said this, let's say you wanted to train a facial recognition algorithm on
[00:07:40.040 --> 00:07:41.040]   aging.
[00:07:41.040 --> 00:07:42.040]   What would you do?
[00:07:42.040 --> 00:07:44.560]   Maybe start a meme like 10 year challenge.
[00:07:44.560 --> 00:07:45.560]   Period.
[00:07:45.560 --> 00:07:47.680]   That is fake news.
[00:07:47.680 --> 00:07:49.240]   That is misleading.
[00:07:49.240 --> 00:07:50.240]   That is line.
[00:07:50.240 --> 00:07:52.280]   So I come back and I say utterly unnecessary.
[00:07:52.280 --> 00:07:55.960]   Google photos two years ago demonstrated that it already tracks people back to infancy
[00:07:55.960 --> 00:07:57.560]   by their images.
[00:07:57.560 --> 00:08:00.160]   Facebook has a better repository of people tagged over time.
[00:08:00.160 --> 00:08:04.760]   There need not be an nefarious motive to everything social you see.
[00:08:04.760 --> 00:08:07.080]   Somebody else came in and said, well, that's not really the article says, and I came back
[00:08:07.080 --> 00:08:10.600]   and I said, yeah, but his tweet said this and I wanted a contrary tweet.
[00:08:10.600 --> 00:08:12.480]   Nick came back and said fair.
[00:08:12.480 --> 00:08:15.320]   And Facebook has now responded that they didn't start.
[00:08:15.320 --> 00:08:19.080]   Also they could add as the article states that it's not entirely clear that having better
[00:08:19.080 --> 00:08:22.280]   facial recognition aging AI does anyone harm.
[00:08:22.280 --> 00:08:23.280]   Okay.
[00:08:23.280 --> 00:08:24.280]   We had a story.
[00:08:24.280 --> 00:08:30.720]   No, you didn't because if you read to the second paragraph of the story, she says my
[00:08:30.720 --> 00:08:35.240]   intent wasn't to claim that the meme is inherently dangerous.
[00:08:35.240 --> 00:08:36.240]   What happens?
[00:08:36.240 --> 00:08:42.840]   What happened when her own editor tweeted this Stacy and we know people don't read up.
[00:08:42.840 --> 00:08:45.120]   However, it's however, it is.
[00:08:45.120 --> 00:08:46.120]   It's fine.
[00:08:46.120 --> 00:08:48.840]   Moral panic because this is trying to panic people about something.
[00:08:48.840 --> 00:08:49.840]   No, you're getting it true.
[00:08:49.840 --> 00:08:50.840]   Jeff, Jeff, you're wrong.
[00:08:50.840 --> 00:08:54.240]   You're wrong because we've seen Microsoft do this with that aging thing.
[00:08:54.240 --> 00:08:56.960]   This is the kind of data companies do want.
[00:08:56.960 --> 00:08:58.840]   They have actively pursued.
[00:08:58.840 --> 00:09:00.520]   Google did it, as you said.
[00:09:00.520 --> 00:09:04.560]   And just because it happened once doesn't mean other companies aren't attempting it,
[00:09:04.560 --> 00:09:07.000]   and there isn't the need for a larger data set.
[00:09:07.000 --> 00:09:09.200]   We know Google does this all the time.
[00:09:09.200 --> 00:09:12.520]   They created this would be the group for a data set you could imagine.
[00:09:12.520 --> 00:09:13.520]   No, no, no, no, no, no.
[00:09:13.520 --> 00:09:14.520]   Okay.
[00:09:14.520 --> 00:09:15.520]   Okay.
[00:09:15.520 --> 00:09:16.520]   Okay.
[00:09:16.520 --> 00:09:24.160]   The point isn't that the point of this article isn't that it's happening this way.
[00:09:24.160 --> 00:09:30.720]   The point of the article is that people should ask these questions.
[00:09:30.720 --> 00:09:33.440]   Jeff, you're missing the forest for the trees.
[00:09:33.440 --> 00:09:37.320]   You're focusing on the precise data that she's talking about.
[00:09:37.320 --> 00:09:38.320]   But really the largest point is-
[00:09:38.320 --> 00:09:41.280]   Because it's trying to get people panicky and concerned about something that is now
[00:09:41.280 --> 00:09:42.280]   working to be-
[00:09:42.280 --> 00:09:46.040]   No, it's trying to get people aware that they're digging their data away.
[00:09:46.040 --> 00:09:47.040]   It is trying to-
[00:09:47.040 --> 00:09:49.040]   A very fun little game that a few years ago, everybody was-
[00:09:49.040 --> 00:09:50.840]   No, it isn't a fun little game.
[00:09:50.840 --> 00:09:51.840]   That's like saying-
[00:09:51.840 --> 00:09:52.840]   Yes it is.
[00:09:52.840 --> 00:09:53.840]   Yes it is.
[00:09:53.840 --> 00:09:54.840]   It is just a fun little game.
[00:09:54.840 --> 00:09:56.840]   It is gathering so companies use these games.
[00:09:56.840 --> 00:09:58.320]   That's what the report he says.
[00:09:58.320 --> 00:09:59.400]   It's just a fun little game.
[00:09:59.400 --> 00:10:06.120]   It's like a few years ago, which Game of Thrones character quiz on Facebook?
[00:10:06.120 --> 00:10:07.800]   That's just a fun little game.
[00:10:07.800 --> 00:10:09.160]   What could possibly go wrong?
[00:10:09.160 --> 00:10:12.000]   Which Game of Thrones character are you?
[00:10:12.000 --> 00:10:13.560]   Which Simpsons character are you?
[00:10:13.560 --> 00:10:14.960]   There's no harm to doing that.
[00:10:14.960 --> 00:10:16.640]   We know now there is.
[00:10:16.640 --> 00:10:17.640]   Jeff?
[00:10:17.640 --> 00:10:19.680]   Yes, that was the basis of the Cambridge Analytica.
[00:10:19.680 --> 00:10:20.680]   Wait, no, there is.
[00:10:20.680 --> 00:10:25.080]   This does a speculative paranoid what if there didn't exist?
[00:10:25.080 --> 00:10:26.480]   Yes, this is wrong.
[00:10:26.480 --> 00:10:27.480]   We don't know.
[00:10:27.480 --> 00:10:28.680]   First of all, you don't know that.
[00:10:28.680 --> 00:10:29.680]   You don't know that.
[00:10:29.680 --> 00:10:31.680]   You don't know what the company-
[00:10:31.680 --> 00:10:32.680]   You don't know what the company started.
[00:10:32.680 --> 00:10:33.680]   You don't know what the company started.
[00:10:33.680 --> 00:10:34.680]   Wait, wait, wait.
[00:10:34.680 --> 00:10:35.680]   Facebook's not going on what?
[00:10:35.680 --> 00:10:36.680]   The Nick Thompson-
[00:10:36.680 --> 00:10:37.680]   The Facebook's not going to lie?
[00:10:37.680 --> 00:10:39.680]   I think Nick Thompson, the editor of-
[00:10:39.680 --> 00:10:43.240]   Have you not been paying attention?
[00:10:43.240 --> 00:10:45.320]   Facebook lies all the time.
[00:10:45.320 --> 00:10:50.640]   Jeff, it feels like your concern is making you-
[00:10:50.640 --> 00:10:53.640]   You see this as something way-
[00:10:53.640 --> 00:10:56.760]   You're just too sensitive to this.
[00:10:56.760 --> 00:10:57.760]   This is like-
[00:10:57.760 --> 00:10:58.760]   Is that something-
[00:10:58.760 --> 00:10:59.760]   Is that something to protect and free speech on the Internet?
[00:10:59.760 --> 00:11:00.760]   Yeah, I am.
[00:11:00.760 --> 00:11:02.320]   Well, I think that's overblown too.
[00:11:02.320 --> 00:11:03.320]   I don't think that-
[00:11:03.320 --> 00:11:04.320]   Yeah, that's my point.
[00:11:04.320 --> 00:11:05.320]   This is overblown.
[00:11:05.320 --> 00:11:06.320]   This is panicking everybody.
[00:11:06.320 --> 00:11:10.260]   I'll stand here on this technology show to defend the Internet against this kind of moral
[00:11:10.260 --> 00:11:11.260]   panic and idiocy.
[00:11:11.260 --> 00:11:18.920]   One of the saddest moments in 2019 for us at this week in Google was the official end
[00:11:18.920 --> 00:11:24.440]   of Google+, which frankly I think was the best social network.
[00:11:24.440 --> 00:11:26.920]   So, April is Google-
[00:11:26.920 --> 00:11:30.280]   Mike is of course the last Google+ user.
[00:11:30.280 --> 00:11:32.680]   Oh, April 3rd, isn't it?
[00:11:32.680 --> 00:11:34.680]   Is it April 3rd or second?
[00:11:34.680 --> 00:11:35.680]   Oh, it's coming up.
[00:11:35.680 --> 00:11:38.720]   Yeah, I just got a thing saying, "Save all my photos.
[00:11:38.720 --> 00:11:41.840]   Are you saving all your photos between Google+, and Flickr?
[00:11:41.840 --> 00:11:43.680]   And what are you going to do, man?
[00:11:43.680 --> 00:11:44.680]   Oh, man.
[00:11:44.680 --> 00:11:46.600]   I already have all my photos.
[00:11:46.600 --> 00:11:49.560]   And there's multiple efforts to-
[00:11:49.560 --> 00:11:51.680]   So Google has its own process for downloading.
[00:11:51.680 --> 00:11:53.600]   I downloaded all my posts.
[00:11:53.600 --> 00:11:54.600]   And some of them are-
[00:11:54.600 --> 00:11:56.760]   I put a lot of work into them and they're very evergreen.
[00:11:56.760 --> 00:12:01.800]   So I'm probably going to repost them on my blog.
[00:12:01.800 --> 00:12:03.760]   But we do have-
[00:12:03.760 --> 00:12:10.360]   I mean, like I said, I wrote in a Google, you know, Fast Company piece a couple of months
[00:12:10.360 --> 00:12:12.460]   ago.
[00:12:12.460 --> 00:12:17.900]   Most of what was great about Google+ was killed in 2014.
[00:12:17.900 --> 00:12:21.700]   And so it's kind of been a shadow of itself for the last few years.
[00:12:21.700 --> 00:12:26.860]   So it's at this point no big loss to a certain extent.
[00:12:26.860 --> 00:12:29.340]   That's heresy in some circles.
[00:12:29.340 --> 00:12:38.220]   But the problem is a lot of passionate Google+ users felt like this is the one social network
[00:12:38.220 --> 00:12:39.220]   I want to use.
[00:12:39.220 --> 00:12:43.940]   It's the only one I want to use and all these smart people that I've gotten to know really
[00:12:43.940 --> 00:12:47.940]   well are here and we have these great conversations and everything was great.
[00:12:47.940 --> 00:12:50.700]   And now nobody knows where to go.
[00:12:50.700 --> 00:12:53.460]   Facebook's off the table because their horrible.
[00:12:53.460 --> 00:12:57.500]   Twitter is annoying to a lot of people.
[00:12:57.500 --> 00:12:58.500]   And where else are you going to go?
[00:12:58.500 --> 00:13:02.180]   So people are going to, you know, they're going to plus Bora and they're going to WeMe
[00:13:02.180 --> 00:13:05.940]   and they're going to all these social networks nobody's heard of.
[00:13:05.940 --> 00:13:10.580]   What's happening is that the Google+ community is going off to 20 different social networks.
[00:13:10.580 --> 00:13:12.740]   And so it's a diaspora.
[00:13:12.740 --> 00:13:13.900]   But it's no long.
[00:13:13.900 --> 00:13:14.900]   That means it's gone.
[00:13:14.900 --> 00:13:16.700]   It's being destroyed.
[00:13:16.700 --> 00:13:22.220]   And I actually blame Google for this because they did create something of value.
[00:13:22.220 --> 00:13:24.300]   They really pitched it hard.
[00:13:24.300 --> 00:13:25.860]   A lot of people went all in.
[00:13:25.860 --> 00:13:31.540]   I mean Mike, you know, maybe it was ill advised to put these full length articles on Google+
[00:13:31.540 --> 00:13:33.500]   and nowhere else.
[00:13:33.500 --> 00:13:37.580]   But you did it because you believed in it and you really liked the conversations there.
[00:13:37.580 --> 00:13:39.380]   And how could Google just let that go?
[00:13:39.380 --> 00:13:42.020]   And this is why people no longer trust Google.
[00:13:42.020 --> 00:13:43.020]   They see this.
[00:13:43.020 --> 00:13:44.020]   Okay.
[00:13:44.020 --> 00:13:47.020]   This feels like, and this drives me nuts.
[00:13:47.020 --> 00:13:51.020]   I mean, the web is no, it's no different than the rest of our lives.
[00:13:51.020 --> 00:13:54.900]   There are places I like to go and talk to people and hang out and I spend my time.
[00:13:54.900 --> 00:13:56.060]   I put my effort in.
[00:13:56.060 --> 00:14:00.260]   I've worked at publications that have gone under, for example, there are coffee shops
[00:14:00.260 --> 00:14:04.540]   where I hang out with my friends and they are no longer around or someone moves.
[00:14:04.540 --> 00:14:06.660]   The world moves on.
[00:14:06.660 --> 00:14:11.540]   And I know this feels really mean, but I'm kind of like, you know, it didn't work for
[00:14:11.540 --> 00:14:12.540]   them.
[00:14:12.540 --> 00:14:13.540]   They shut it down.
[00:14:13.540 --> 00:14:14.540]   Okay.
[00:14:14.540 --> 00:14:17.540]   I, if you want your own stuff.
[00:14:17.540 --> 00:14:19.820]   You clearly weren't a Google+ lover.
[00:14:19.820 --> 00:14:20.980]   Well, I wasn't.
[00:14:20.980 --> 00:14:22.580]   But I've loved other things.
[00:14:22.580 --> 00:14:25.780]   Like, I mean, all of my giga ohm stuff, only half of it exists.
[00:14:25.780 --> 00:14:28.660]   I've lost my entire podcast catalog from giga ohm.
[00:14:28.660 --> 00:14:29.660]   And I'm like, eh.
[00:14:29.660 --> 00:14:30.660]   All right.
[00:14:30.660 --> 00:14:31.660]   That's life.
[00:14:31.660 --> 00:14:32.660]   You know, it's life.
[00:14:32.660 --> 00:14:35.100]   I've got, I'll tell you why it hurts.
[00:14:35.100 --> 00:14:41.100]   I mean, it hurts, but just, but I'll tell you why we need something like that.
[00:14:41.100 --> 00:14:42.340]   Do we though?
[00:14:42.340 --> 00:14:43.340]   Yeah, we do.
[00:14:43.340 --> 00:14:44.340]   A part of it.
[00:14:44.340 --> 00:14:45.340]   No, because then we stagnate.
[00:14:45.340 --> 00:14:48.940]   I mean, then you go and create other communities, you take your ideas that you learn from other
[00:14:48.940 --> 00:14:53.740]   people and then we, I mean, think about it in the physical world, you know, it's
[00:14:53.740 --> 00:14:55.260]   no way in the physical world.
[00:14:55.260 --> 00:14:58.220]   If there were, if there were things, there's towns.
[00:14:58.220 --> 00:15:01.740]   Yeah, it's like a town is disappeared.
[00:15:01.740 --> 00:15:02.740]   That's a big deal.
[00:15:02.740 --> 00:15:03.740]   Right.
[00:15:03.740 --> 00:15:04.980]   But those people left that town.
[00:15:04.980 --> 00:15:06.660]   I'm not saying it's not a big deal.
[00:15:06.660 --> 00:15:11.500]   I'm saying these things happen in life and you just have to accept it and put you down
[00:15:11.500 --> 00:15:14.500]   your roots somewhere else and do it all over again.
[00:15:14.500 --> 00:15:17.460]   And that's what people, that's life, I guess.
[00:15:17.460 --> 00:15:18.460]   I just.
[00:15:18.460 --> 00:15:21.020]   I understand what you're saying.
[00:15:21.020 --> 00:15:25.460]   The, the thing that has to be said though is that Google plus the original version of
[00:15:25.460 --> 00:15:31.900]   2011 to 2014 to a lesser extent afterwards was a really, really special social network.
[00:15:31.900 --> 00:15:34.980]   I mean, it was really every social network for some reason.
[00:15:34.980 --> 00:15:39.500]   I, and I certainly have no idea why this is has a certain site that has a certain feel
[00:15:39.500 --> 00:15:45.020]   to it, a certain tone, a certain, you know, Instagram is so different from Twitter or
[00:15:45.020 --> 00:15:48.300]   Facebook, even though it's owned by Facebook.
[00:15:48.300 --> 00:15:52.620]   And the, the essential nature of Google plus, there were things that were achieved on that
[00:15:52.620 --> 00:15:56.620]   social network that you just are unheard of on other social networks.
[00:15:56.620 --> 00:16:03.460]   One of them is lots and lots and lots of people actually turned strangers into close friends
[00:16:03.460 --> 00:16:05.300]   in real life.
[00:16:05.300 --> 00:16:06.300]   And I'm one of those people.
[00:16:06.300 --> 00:16:15.380]   I have 15, 20 people who I go visit and who I know personally and who I didn't know until
[00:16:15.380 --> 00:16:18.100]   we started following them on Google plus you learn you have a lot in common.
[00:16:18.100 --> 00:16:21.220]   You have a lot, you know, you, you gang up on trolls together.
[00:16:21.220 --> 00:16:22.220]   You do all the stuff.
[00:16:22.220 --> 00:16:23.740]   You do hangouts.
[00:16:23.740 --> 00:16:26.340]   And then they're great friends at people in Belgium.
[00:16:26.340 --> 00:16:34.140]   And like I went and stayed with a great friend that I met on Google plus in Switzerland.
[00:16:34.140 --> 00:16:36.820]   You don't hear, you don't hear by that kind of stuff on Facebook.
[00:16:36.820 --> 00:16:42.340]   Basically what you're doing is you're kind of hoovering up all of the friends and acquaintances
[00:16:42.340 --> 00:16:47.180]   and coworkers you've ever had and maintaining some superficial connection with them on Google
[00:16:47.180 --> 00:16:51.580]   plus you're like everybody was a stranger because you're, you know, nobody's family
[00:16:51.580 --> 00:16:54.580]   was really to speak of.
[00:16:54.580 --> 00:16:56.580]   And you make these great friends.
[00:16:56.580 --> 00:16:58.660]   I don't know why that was the case.
[00:16:58.660 --> 00:17:03.900]   We used to have these things called hurls hangouts in real life, essentially meetups.
[00:17:03.900 --> 00:17:10.340]   And I've never heard of a social network that was so conducive to real world friendships.
[00:17:10.340 --> 00:17:13.900]   People had in the early days of Twitter and it stopped soon.
[00:17:13.900 --> 00:17:18.740]   But in the early days of Twitter there were tweetups and it was very much like that.
[00:17:18.740 --> 00:17:23.380]   But Twitter became mass appeal and that ruined it.
[00:17:23.380 --> 00:17:28.140]   And I think one of the reasons Google plus was great is because it wasn't very successful.
[00:17:28.140 --> 00:17:29.140]   Exactly.
[00:17:29.140 --> 00:17:31.260]   The people who were there were great.
[00:17:31.260 --> 00:17:37.340]   But I, what is, what are the, are there lessons to be learned from this?
[00:17:37.340 --> 00:17:40.220]   Google can't do social no matter how much.
[00:17:40.220 --> 00:17:41.340]   I would submit.
[00:17:41.340 --> 00:17:43.460]   I would submit I will not.
[00:17:43.460 --> 00:17:51.460]   I think we need to have a, a, a, I don't think we can trust any company to do this right.
[00:17:51.460 --> 00:17:54.300]   Get a WordPress blog and do it yourself.
[00:17:54.300 --> 00:17:57.500]   Well, but that's not social ultimately.
[00:17:57.500 --> 00:17:58.660]   It's only a little bit social.
[00:17:58.660 --> 00:18:03.180]   I mean, honestly, and it's been tried and this is so I know my solution isn't a good
[00:18:03.180 --> 00:18:04.180]   solution.
[00:18:04.180 --> 00:18:12.420]   But we need a federated open network that, and you know, I, we had hopes that, you know,
[00:18:12.420 --> 00:18:19.300]   identical than status net than mastodon would be that they have the right backbone.
[00:18:19.300 --> 00:18:22.900]   But somehow those communities never gelled properly.
[00:18:22.900 --> 00:18:28.820]   But that's how it has to be because if a company is in charge of it, all sorts A, they
[00:18:28.820 --> 00:18:29.820]   could kill it.
[00:18:29.820 --> 00:18:34.700]   B, they start monetizing Instagram was a great site, a great social network until they put
[00:18:34.700 --> 00:18:39.620]   an ad every ninth post and they started adding all these features like stories and the influencer.
[00:18:39.620 --> 00:18:43.740]   As soon as the influencers arrived, Instagram went way downhill.
[00:18:43.740 --> 00:18:45.380]   What about something like Pinterest?
[00:18:45.380 --> 00:18:47.260]   Yeah, maybe Pinterest is the answer.
[00:18:47.260 --> 00:18:48.260]   I don't know.
[00:18:48.260 --> 00:18:50.180]   I mean, it's, it's a high quality.
[00:18:50.180 --> 00:18:51.660]   Is it very social?
[00:18:51.660 --> 00:18:52.820]   I'm trying to, I mean, like.
[00:18:52.820 --> 00:18:55.660]   Pinterest feels more like people put stuff there, but that doesn't feel like there's
[00:18:55.660 --> 00:18:56.660]   interaction.
[00:18:56.660 --> 00:18:57.660]   Yeah.
[00:18:57.660 --> 00:18:58.660]   Yeah.
[00:18:58.660 --> 00:18:59.660]   Right.
[00:18:59.660 --> 00:19:03.580]   That's where people interact, become cesspits.
[00:19:03.580 --> 00:19:05.660]   That's not, that's not a crazy idea.
[00:19:05.660 --> 00:19:10.140]   I keep, I keep wishing mastodon would succeed because from a technical point of view, it's
[00:19:10.140 --> 00:19:11.140]   the right thing.
[00:19:11.140 --> 00:19:12.780]   Nobody owns it.
[00:19:12.780 --> 00:19:14.140]   It's federated.
[00:19:14.140 --> 00:19:18.660]   You can create your own server, but the communities that have been created on Mastodon, I haven't
[00:19:18.660 --> 00:19:21.140]   found one that really attracts me at this point.
[00:19:21.140 --> 00:19:22.140]   Yeah.
[00:19:22.140 --> 00:19:27.860]   The problem is that every social network is either a failure, in which case is not worth
[00:19:27.860 --> 00:19:30.500]   using because no it's on it.
[00:19:30.500 --> 00:19:35.540]   Or if it's successful, they will ruin it in order to squeeze every nickel out of it.
[00:19:35.540 --> 00:19:37.860]   That's why it can't be commercial.
[00:19:37.860 --> 00:19:41.220]   That, that, that's, that's the only two social networks we've ever seen.
[00:19:41.220 --> 00:19:44.260]   Google plus was different for a while.
[00:19:44.260 --> 00:19:50.180]   I mean, people say it wasn't, it wasn't popular and I, I mean, it's way more popular than
[00:19:50.180 --> 00:19:53.940]   lots of other social networks that have emerged since that people think are popular.
[00:19:53.940 --> 00:19:54.940]   Yeah.
[00:19:54.940 --> 00:19:59.860]   There's great features though, like unlike Mastodon, which really is a Twitter clone,
[00:19:59.860 --> 00:20:02.700]   it was a threaded conversation clone.
[00:20:02.700 --> 00:20:04.500]   It was great with images.
[00:20:04.500 --> 00:20:09.180]   People like Trey Radcliffe showed that it became a fabulous place to put your images.
[00:20:09.180 --> 00:20:14.260]   Google mismanaged it almost from day one, establishing this crazy circle system that
[00:20:14.260 --> 00:20:16.380]   no one wanted to maintain.
[00:20:16.380 --> 00:20:21.860]   And then, you know, I mean, they just, they, they killed it death by a thousand cuts.
[00:20:21.860 --> 00:20:26.020]   And it's, it's tragic and it's a really good example of why Google or any company shouldn't
[00:20:26.020 --> 00:20:28.900]   be in charge of social and yet no one's done any.
[00:20:28.900 --> 00:20:30.140]   I wish they, you know what?
[00:20:30.140 --> 00:20:31.780]   Google open source.
[00:20:31.780 --> 00:20:32.780]   Yeah.
[00:20:32.780 --> 00:20:35.500]   Slack is, Slack for many people's turned into that.
[00:20:35.500 --> 00:20:36.500]   Yeah.
[00:20:36.500 --> 00:20:37.500]   Yeah.
[00:20:37.500 --> 00:20:38.500]   I mean, maybe that's it.
[00:20:38.500 --> 00:20:39.860]   And, and you know what?
[00:20:39.860 --> 00:20:45.460]   I would say we had a, so the former giga-omers, we actually had, we, we took our Slack channel
[00:20:45.460 --> 00:20:48.260]   right at the end of the show or at the end of the company.
[00:20:48.260 --> 00:20:49.260]   You still do it.
[00:20:49.260 --> 00:20:50.260]   It really still is alive.
[00:20:50.260 --> 00:20:55.380]   And for the year afterwards, we actually even paid for our own private Slack.
[00:20:55.380 --> 00:20:58.500]   And we did it because we had a lot of value in it.
[00:20:58.500 --> 00:20:59.500]   And I don't know.
[00:20:59.500 --> 00:21:02.260]   Slack's such a pig as an application.
[00:21:02.260 --> 00:21:03.700]   It's hard to support it.
[00:21:03.700 --> 00:21:04.700]   Oh.
[00:21:04.700 --> 00:21:07.340]   There's no, no, it's just a terrible.
[00:21:07.340 --> 00:21:11.940]   It's, I mean, it's written in an electron and it's just big and heavy.
[00:21:11.940 --> 00:21:15.660]   And it needs, I don't know what it, I don't, it's not bad.
[00:21:15.660 --> 00:21:20.660]   I mean, I know people are doing it.
[00:21:20.660 --> 00:21:24.020]   Discord is another very popular social network.
[00:21:24.020 --> 00:21:25.020]   Can I propose?
[00:21:25.020 --> 00:21:26.820]   I have to say something about it.
[00:21:26.820 --> 00:21:27.820]   Yes.
[00:21:27.820 --> 00:21:29.460]   Let me just say this before you do.
[00:21:29.460 --> 00:21:35.020]   Which is Google, please open source Google+, and let us take it back.
[00:21:35.020 --> 00:21:39.220]   It is early to talk about this because it was only announced yesterday, but it is to
[00:21:39.220 --> 00:21:44.660]   me maybe the most interesting and could be most important story of the year.
[00:21:44.660 --> 00:21:46.420]   Wait, wait, can we guess?
[00:21:46.420 --> 00:21:47.420]   Yeah.
[00:21:47.420 --> 00:21:50.220]   One, two, three, Libra.
[00:21:50.220 --> 00:21:51.220]   Libra.
[00:21:51.220 --> 00:21:52.220]   Yes.
[00:21:52.220 --> 00:21:53.220]   Yeah.
[00:21:53.220 --> 00:21:56.340]   Facebook's cryptocurrency.
[00:21:56.340 --> 00:22:01.300]   And already, France has had all the ones out.
[00:22:01.300 --> 00:22:02.300]   Big boy.
[00:22:02.300 --> 00:22:04.580]   You're not so fast.
[00:22:04.580 --> 00:22:12.580]   Mark Zuckerberg has has the chair of the House Financial Services Committee.
[00:22:12.580 --> 00:22:16.660]   This is going to be a very Maxine waters.
[00:22:16.660 --> 00:22:19.980]   This is going to be a very interesting story.
[00:22:19.980 --> 00:22:23.100]   It's not going to happen till next year.
[00:22:23.100 --> 00:22:27.860]   And it takes some understanding, you know, and a little bit of research to really get
[00:22:27.860 --> 00:22:30.100]   what's going on.
[00:22:30.100 --> 00:22:35.540]   I'm going to refer because I think they are the smartest guys out there to the Wired
[00:22:35.540 --> 00:22:44.620]   article by Stephen Levy and Gregory Barber, the ambitious plans behind Facebook's cryptocurrency
[00:22:44.620 --> 00:22:49.300]   Libra.
[00:22:49.300 --> 00:22:52.340]   The deal is complicated.
[00:22:52.340 --> 00:22:58.540]   First of all, maybe it's not even quite appropriate to call it Facebook's because as Facebook
[00:22:58.540 --> 00:23:07.860]   will point out, Libra is a nonprofit out of Switzerland, global coin, the Libra Association.
[00:23:07.860 --> 00:23:11.060]   They're the Switzerland of digital coinage.
[00:23:11.060 --> 00:23:13.860]   The technology is open source.
[00:23:13.860 --> 00:23:19.220]   The control of the blockchain will be out of Switzerland.
[00:23:19.220 --> 00:23:24.260]   The Libra Association will consist of up to 100 founding members of which Facebook is
[00:23:24.260 --> 00:23:25.580]   just one.
[00:23:25.580 --> 00:23:32.220]   Each of them will put in $10 million to fund the association and receive interest earned
[00:23:32.220 --> 00:23:34.220]   off the reserve.
[00:23:34.220 --> 00:23:36.940]   It's an NGO in other words.
[00:23:36.940 --> 00:23:39.740]   Actually, no, I'm sorry.
[00:23:39.740 --> 00:23:42.300]   I have some skepticism about this part, but keep going.
[00:23:42.300 --> 00:23:43.540]   There will be NGO members.
[00:23:43.540 --> 00:23:46.220]   Libra is not an NGO, but it is a nonprofit.
[00:23:46.220 --> 00:23:47.740]   Each member will be empowered.
[00:23:47.740 --> 00:23:53.500]   I guess it is ultimately a non-governmental organization though in the most truest sense
[00:23:53.500 --> 00:24:00.420]   of the word because, and this is what scares banks and scares governments, it will be a
[00:24:00.420 --> 00:24:08.980]   global currency, not pegged to the US dollar, but tied to the dollar, the yen, the euro.
[00:24:08.980 --> 00:24:13.740]   Can I throw out a challenge just down for just a second, just as the discussion continues?
[00:24:13.740 --> 00:24:18.300]   Is to think about this as a Facebook word, not mention it all, right?
[00:24:18.300 --> 00:24:21.900]   To take that angle out of it and say, does this make sense?
[00:24:21.900 --> 00:24:22.900]   Is this a good thing for people?
[00:24:22.900 --> 00:24:23.900]   Is this a bad thing for people?
[00:24:23.900 --> 00:24:26.780]   Then we can add Facebook back in and there's perfectly legitimate things to talk about
[00:24:26.780 --> 00:24:28.980]   there is whether they're not there a proper--
[00:24:28.980 --> 00:24:30.300]   You're right, because Facebook kind of--
[00:24:30.300 --> 00:24:31.300]   Wait, tell us--
[00:24:31.300 --> 00:24:32.300]   --tills it.
[00:24:32.300 --> 00:24:33.300]   If you say goodbye--
[00:24:33.300 --> 00:24:34.300]   Why do you do that?
[00:24:34.300 --> 00:24:36.700]   I just want to think about the theory of this too.
[00:24:36.700 --> 00:24:37.700]   Go ahead, Stacy.
[00:24:37.700 --> 00:24:39.180]   Okay, so we totally can't.
[00:24:39.180 --> 00:24:44.460]   But before you do that, I want to bring up the fact that I have in covering technology.
[00:24:44.460 --> 00:24:46.540]   I'm sure you guys have seen this too.
[00:24:46.540 --> 00:24:53.660]   There are tons of independent standard settings boards or organizations that have been created
[00:24:53.660 --> 00:24:57.140]   but are yet dominated by one member.
[00:24:57.140 --> 00:25:02.460]   I think it's important when we're talking about this to understand how much Facebook
[00:25:02.460 --> 00:25:03.820]   has here.
[00:25:03.820 --> 00:25:10.100]   It does say that it eventually will step back, but we have no guarantees there.
[00:25:10.100 --> 00:25:14.940]   There's a lot of decision-making that happens in these organizations.
[00:25:14.940 --> 00:25:21.620]   It's entirely possible for an organization with 100 different groups involved to still
[00:25:21.620 --> 00:25:23.900]   be dominated by one or two major players.
[00:25:23.900 --> 00:25:24.900]   That's all I want to say.
[00:25:24.900 --> 00:25:25.900]   Fred Wilson.
[00:25:25.900 --> 00:25:26.900]   Yes, you're right.
[00:25:26.900 --> 00:25:34.540]   Maybe great investor Fred Wilson says, "Think of Facebook as the Satoshi of Bitcoin, the
[00:25:34.540 --> 00:25:37.980]   creator of Bitcoin and LibraCoin like Bitcoin."
[00:25:37.980 --> 00:25:41.660]   In other words, Satoshi released it into the world.
[00:25:41.660 --> 00:25:45.340]   It came up with technology and is open sourcing it.
[00:25:45.340 --> 00:25:55.260]   I think it's also important to say that Facebook understands the value of this not being Facebook.
[00:25:55.260 --> 00:25:59.940]   Some of the articles out there have described it as "Zuckbox," that's the phrase I like,
[00:25:59.940 --> 00:26:02.860]   and "Facecoin."
[00:26:02.860 --> 00:26:06.220]   If it's dead in the water, says Marcus.
[00:26:06.220 --> 00:26:08.860]   I'm not sure who Marcus is.
[00:26:08.860 --> 00:26:09.860]   That is David.
[00:26:09.860 --> 00:26:10.860]   Is it David Marcus?
[00:26:10.860 --> 00:26:13.260]   Is he Facebook executive who came up with this?
[00:26:13.260 --> 00:26:14.260]   Yes.
[00:26:14.260 --> 00:26:17.460]   He was president of PayPal.
[00:26:17.460 --> 00:26:19.060]   Here's how the Levy article begins.
[00:26:19.060 --> 00:26:23.500]   Near the end of 2017 on a Dominican Republic beach with his family, Facebook executive David
[00:26:23.500 --> 00:26:28.140]   Marcus wrestled with the question he'd been pondering since his previous job as president
[00:26:28.140 --> 00:26:35.420]   of PayPal, "How would you build the Internet of money, a friction-free global digital currency
[00:26:35.420 --> 00:26:39.620]   would be a boon for many people with mobile phones but no access to banking?"
[00:26:39.620 --> 00:26:44.500]   Which also coincides with Facebook's commercial interest because that's the other billion
[00:26:44.500 --> 00:26:47.300]   or the next billion Facebook people.
[00:26:47.300 --> 00:26:51.620]   Who better to develop something like this, Marcus wondered, than Facebook with its global
[00:26:51.620 --> 00:26:53.700]   reach and massive user base.
[00:26:53.700 --> 00:26:57.340]   He was at the time, had a Facebook messenger.
[00:26:57.340 --> 00:27:03.020]   He texted Zuckerberg saying, "It's time to talk about creating a cryptocurrency."
[00:27:03.020 --> 00:27:05.540]   He wrote a memo that laid out his ideas.
[00:27:05.540 --> 00:27:10.580]   Mark quickly endorsed the plan saying the approach synced with his ideas.
[00:27:10.580 --> 00:27:15.220]   They were thinking about this for a long time and with messenger, Apple has kind of an in-eye
[00:27:15.220 --> 00:27:17.220]   messages payment using...
[00:27:17.220 --> 00:27:18.220]   Well, it's one way.
[00:27:18.220 --> 00:27:20.220]   It shows the path.
[00:27:20.220 --> 00:27:21.220]   Right.
[00:27:21.220 --> 00:27:24.220]   I mean, the Chinese model that you can do everything.
[00:27:24.220 --> 00:27:25.220]   It's a catch-up.
[00:27:25.220 --> 00:27:26.220]   Not wallet.
[00:27:26.220 --> 00:27:27.220]   We check.
[00:27:27.220 --> 00:27:28.220]   We check.
[00:27:28.220 --> 00:27:29.220]   Thank you.
[00:27:29.220 --> 00:27:30.220]   We check.
[00:27:30.220 --> 00:27:31.220]   But this is more than that.
[00:27:31.220 --> 00:27:34.980]   I think it would be a currency that messenger would run on, Facebook messenger would run
[00:27:34.980 --> 00:27:35.980]   on.
[00:27:35.980 --> 00:27:36.980]   But listen to it.
[00:27:36.980 --> 00:27:40.900]   I mean, so to your point, Jeff, what if we leave out Facebook, but we mentioned the
[00:27:40.900 --> 00:27:50.300]   other partners, Visa, MasterCard, PayPal, Coinbase, Stripe, VCs like Andres and Horowitz
[00:27:50.300 --> 00:27:56.260]   and Thrive, Kiva, which is that micro-lending nonprofit, the NGO.
[00:27:56.260 --> 00:27:59.300]   Even though this will not be a lending platform, let's be quick to add.
[00:27:59.300 --> 00:28:03.020]   But maybe the lending would occur in global coin is the idea.
[00:28:03.020 --> 00:28:10.220]   Women's World Banking, Vodafone, eBay, Lyft, Uber, Spotify.
[00:28:10.220 --> 00:28:12.620]   These are all partners in this.
[00:28:12.620 --> 00:28:17.780]   No banks, of course, because if you're going to pick one victim of this, it would be banks.
[00:28:17.780 --> 00:28:21.140]   Well, the other one is not in there and it's not a victim.
[00:28:21.140 --> 00:28:23.700]   It's somebody who victimizes customers around the world.
[00:28:23.700 --> 00:28:25.060]   The world is Western Union.
[00:28:25.060 --> 00:28:26.220]   Oh, yeah.
[00:28:26.220 --> 00:28:28.180]   This would totally replace that.
[00:28:28.180 --> 00:28:29.860]   This is that's one key to this.
[00:28:29.860 --> 00:28:37.260]   I noticed M-Pesa isn't in there either, which is the African payment platform that is basically
[00:28:37.260 --> 00:28:42.500]   widely used in lieu of banks, but Stripe is in there.
[00:28:42.500 --> 00:28:43.820]   PayU is in there.
[00:28:43.820 --> 00:28:44.900]   PayPal is in there.
[00:28:44.900 --> 00:28:50.780]   So there's an interesting group, MercyCore, Ribbit Capital, Union Square Ventures.
[00:28:50.780 --> 00:28:52.780]   It's a really interesting group.
[00:28:52.780 --> 00:28:53.780]   So you think-
[00:28:53.780 --> 00:28:54.780]   So you were trying to get it.
[00:28:54.780 --> 00:28:58.860]   Yeah, so there's a couple of things about this that we should look at.
[00:28:58.860 --> 00:29:04.940]   One is there's a lot of value in understanding what people are buying, what people are spending
[00:29:04.940 --> 00:29:11.980]   money on, and how money flows around the world and seeing that in almost real time.
[00:29:11.980 --> 00:29:16.500]   It looks like, and again, I don't know the math behind this, but unlike something like
[00:29:16.500 --> 00:29:21.900]   Hyperledger, it looks like the blockchain process behind this, the math behind this,
[00:29:21.900 --> 00:29:24.260]   will make that sort of information public.
[00:29:24.260 --> 00:29:29.540]   So a company like Stripe, Spotify, even B-Send MasterCard, who actually already have access
[00:29:29.540 --> 00:29:34.460]   to this data, it behooves them to have something like this where they can see what people are
[00:29:34.460 --> 00:29:35.460]   buying.
[00:29:35.460 --> 00:29:36.460]   Two.
[00:29:36.460 --> 00:29:40.460]   Facebook says that none of the information would go back to Facebook.
[00:29:40.460 --> 00:29:44.060]   Right, I'm not just worried about Facebook.
[00:29:44.060 --> 00:29:47.740]   I'm curious what it means to make transactions public.
[00:29:47.740 --> 00:29:54.220]   So I've talked to people in the FBI about digital currencies, and they love this because
[00:29:54.220 --> 00:29:58.220]   you can find out who owns what and see what's happening.
[00:29:58.220 --> 00:30:02.020]   So there's a lot of information and data being available.
[00:30:02.020 --> 00:30:06.060]   Like if we're all excited about search data because that shows intent, this is like the
[00:30:06.060 --> 00:30:07.860]   next best thing.
[00:30:07.860 --> 00:30:13.260]   I'm sad to say that Mark didn't take my advice and call him Zuck Bucks, and actually
[00:30:13.260 --> 00:30:15.140]   Libra's facing a lot more headwinds.
[00:30:15.140 --> 00:30:19.180]   I wonder if we'll ever see Libra cryptocurrency.
[00:30:19.180 --> 00:30:21.300]   That'll be a story for 2020.
[00:30:21.300 --> 00:30:28.820]   A story that is continuing throughout 2019 and into 2020 is China continuously stepping
[00:30:28.820 --> 00:30:32.620]   up their use of technology to surveil people.
[00:30:32.620 --> 00:30:36.940]   It actually really came to our attention this past July.
[00:30:36.940 --> 00:30:44.900]   Speaking of malware, the Chinese border patrol, if you cross into the border near the Xinjiang
[00:30:44.900 --> 00:30:51.260]   region, which is, I think it's in the Western area where the local Muslim past pop-up.
[00:30:51.260 --> 00:30:53.660]   The population, the Weagers are being repressed.
[00:30:53.660 --> 00:31:00.140]   If you cross in, you will be forced to install malware on your phone that downloads your
[00:31:00.140 --> 00:31:06.140]   text messages, calendar entries, and phone logs, scans the device looking for 70,000
[00:31:06.140 --> 00:31:10.900]   different apps and stays on the device.
[00:31:10.900 --> 00:31:14.300]   Motherboard got a copy of the malware.
[00:31:14.300 --> 00:31:21.340]   In fact, I guess this is a German newspaper, Sudhdoytzeitung.
[00:31:21.340 --> 00:31:27.260]   They send a reporter who crossed the border had the same malware installed on their phone.
[00:31:27.260 --> 00:31:32.140]   In fact, if you want, you can download the malware from Motherboard.
[00:31:32.140 --> 00:31:35.100]   Why don't you install it on your phone and let get back to us?
[00:31:35.100 --> 00:31:36.580]   How does that get?
[00:31:36.580 --> 00:31:40.460]   The border crossing is from Kyrgyzstan into China.
[00:31:40.460 --> 00:31:42.820]   I guess the theory is, "Well, what are you doing over here?
[00:31:42.820 --> 00:31:46.700]   What are you doing here?"
[00:31:46.700 --> 00:31:51.280]   I think though, it's just, I'm really worried I'm going to be going in the fall to the Middle
[00:31:51.280 --> 00:31:59.140]   East, to both Israel and Jordan and Oman and the United Arab Emirates.
[00:31:59.140 --> 00:32:02.660]   I'm actually trying to figure out what my strategy is going to be, both for going into
[00:32:02.660 --> 00:32:12.340]   those countries, going from Israel into Dubai, and then to top it all off, coming home.
[00:32:12.340 --> 00:32:13.340]   By six burners.
[00:32:13.340 --> 00:32:16.340]   I think I have to have a burner for every country.
[00:32:16.340 --> 00:32:17.340]   I have two backshorts.
[00:32:17.340 --> 00:32:18.340]   Oh man.
[00:32:18.340 --> 00:32:21.340]   Sure you get one for the US too.
[00:32:21.340 --> 00:32:22.340]   Oh, jeez.
[00:32:22.340 --> 00:32:23.340]   Yeah.
[00:32:23.340 --> 00:32:24.340]   Jeez.
[00:32:24.340 --> 00:32:30.340]   By the way, when I went to Israel not too long ago, instead of stamps, they put two little,
[00:32:30.340 --> 00:32:36.580]   they just give you two little very not-pasted cards, an entry card, an exit card.
[00:32:36.580 --> 00:32:39.220]   And you have to have it when you go out so they can take it.
[00:32:39.220 --> 00:32:40.820]   But then you can throw it away.
[00:32:40.820 --> 00:32:41.820]   Right.
[00:32:41.820 --> 00:32:42.820]   I know nothing.
[00:32:42.820 --> 00:32:43.820]   Russia does that too.
[00:32:43.820 --> 00:32:46.180]   Oh, so that there's no evidence that you were in Israel.
[00:32:46.180 --> 00:32:47.180]   Okay.
[00:32:47.180 --> 00:32:48.180]   That's the point.
[00:32:48.180 --> 00:32:49.180]   Clever.
[00:32:49.180 --> 00:32:54.060]   I wasn't here for this next story, but Google got a lot of heat for this.
[00:32:54.060 --> 00:32:56.020]   I don't know if they deserved to.
[00:32:56.020 --> 00:33:04.180]   Jason Hals, Stacy Higginbotham, Kevin Tofel, is your face worth five bucks to Google?
[00:33:04.180 --> 00:33:08.940]   Did you hear the story about someone who apparently was propositioned five dollars for
[00:33:08.940 --> 00:33:13.340]   a face scan by Google employee?
[00:33:13.340 --> 00:33:16.460]   So apparently your face is only worth five dollars.
[00:33:16.460 --> 00:33:18.300]   And I guess that kind of seems- That's actually quite a bit.
[00:33:18.300 --> 00:33:19.300]   Is it?
[00:33:19.300 --> 00:33:24.260]   I feel like it seems low, but- You know, they did a study probably in 20, either 2012 or
[00:33:24.260 --> 00:33:29.540]   2014 where they bought people's social security number for a cookie.
[00:33:29.540 --> 00:33:30.540]   So- Okay.
[00:33:30.540 --> 00:33:35.540]   I feel like- By comparison- Wait, what flavor?
[00:33:35.540 --> 00:33:36.740]   It was a chocolate chip cookie.
[00:33:36.740 --> 00:33:37.740]   Okay.
[00:33:37.740 --> 00:33:38.740]   That's a problem.
[00:33:38.740 --> 00:33:39.740]   It's a guarantee.
[00:33:39.740 --> 00:33:40.740]   All right.
[00:33:40.740 --> 00:33:41.740]   That's a good trade.
[00:33:41.740 --> 00:33:42.740]   It's a good trade.
[00:33:42.740 --> 00:33:48.820]   That's more of a barter as opposed to a dollar figure, but I feel that five dollars for a
[00:33:48.820 --> 00:33:50.460]   face is fair.
[00:33:50.460 --> 00:33:55.940]   What did strike me is the fact that this guy was like- And it's not five dollars cash.
[00:33:55.940 --> 00:33:58.060]   It's five dollars at Starbucks or Amazon, I believe.
[00:33:58.060 --> 00:33:59.060]   Yeah.
[00:33:59.060 --> 00:34:00.060]   With an Amazon?
[00:34:00.060 --> 00:34:01.060]   Gift card.
[00:34:01.060 --> 00:34:02.060]   Yes.
[00:34:02.060 --> 00:34:03.060]   Amazon or Starbucks, right?
[00:34:03.060 --> 00:34:04.060]   Yeah.
[00:34:04.060 --> 00:34:07.580]   Which I thought was hilarious because, you know, Amazon probably could use some faces
[00:34:07.580 --> 00:34:08.580]   themselves.
[00:34:08.580 --> 00:34:09.580]   That's true.
[00:34:09.580 --> 00:34:10.580]   Shh.
[00:34:10.580 --> 00:34:17.860]   Someone notable that they're not giving out their own Play Store gift cards, but also,
[00:34:17.860 --> 00:34:20.300]   that makes a whole lot of sense that they wouldn't do that.
[00:34:20.300 --> 00:34:22.300]   Can I buy a-
[00:34:22.300 --> 00:34:24.300]   Here's the thing now.
[00:34:24.300 --> 00:34:29.620]   I mean, five dollars aside, if this person or anybody offers five dollars for Google
[00:34:29.620 --> 00:34:36.940]   to do that, does Google still have the right to scan their face in other photos and such?
[00:34:36.940 --> 00:34:39.100]   Like, are they getting higher than the five bucks?
[00:34:39.100 --> 00:34:42.700]   Are they getting any extra privacy over their data?
[00:34:42.700 --> 00:34:43.700]   That's a really good question.
[00:34:43.700 --> 00:34:48.620]   Like, I have to imagine- So what happened, at least in the ZDNet story, and I think there
[00:34:48.620 --> 00:34:53.140]   was one on nine to five Google, another person that, you know, they had a report of this
[00:34:53.140 --> 00:34:55.380]   happening, but they were sitting in the park.
[00:34:55.380 --> 00:34:59.260]   Man walks up to them and offers five dollars to scan their face.
[00:34:59.260 --> 00:35:00.980]   They say, "I work for Google.
[00:35:00.980 --> 00:35:06.300]   We're collecting data to improve the next generation of facial recognition phone unlocking."
[00:35:06.300 --> 00:35:13.100]   So, reasonably specific as far as what they're saying that they're using it for, who knows
[00:35:13.100 --> 00:35:17.780]   if they have more and more uses for this face scan than just that, but that's what they're
[00:35:17.780 --> 00:35:18.780]   telling people.
[00:35:18.780 --> 00:35:23.900]   And then they were given a phone in a case that made it difficult to discern what the
[00:35:23.900 --> 00:35:25.140]   phone was.
[00:35:25.140 --> 00:35:30.220]   Many people are positing that it might be the Pixel 4, because there might be a sensor
[00:35:30.220 --> 00:35:35.620]   on the Pixel 4 that would facilitate face scanning in order to unlock.
[00:35:35.620 --> 00:35:40.900]   And then you use the phone to take a selfie at different angles to get the different kind
[00:35:40.900 --> 00:35:46.740]   of direction shots on your face, and then you're handed this five dollar Amazon and Starbucks
[00:35:46.740 --> 00:35:47.740]   card.
[00:35:47.740 --> 00:35:52.500]   And I have to imagine if you're agreeing to this, you must either sign something or maybe
[00:35:52.500 --> 00:35:55.580]   there's something in the app that you're taking the selfie with.
[00:35:55.580 --> 00:35:57.700]   They did give him a contract.
[00:35:57.700 --> 00:35:59.460]   He just didn't read it.
[00:35:59.460 --> 00:36:00.460]   Oh.
[00:36:00.460 --> 00:36:02.060]   Which is unfortunate because- That's too bad.
[00:36:02.060 --> 00:36:06.780]   I'm like, for $5, I would like to see what you're giving over.
[00:36:06.780 --> 00:36:07.780]   Yes.
[00:36:07.780 --> 00:36:10.860]   Well, I mean, he's writing about his friend who's an engineer and who is probably like,
[00:36:10.860 --> 00:36:12.380]   "Yeah, I'll give him my face."
[00:36:12.380 --> 00:36:13.380]   That's not ever.
[00:36:13.380 --> 00:36:14.540]   Yeah, probably one of those.
[00:36:14.540 --> 00:36:16.140]   They've already got everything.
[00:36:16.140 --> 00:36:17.660]   So what's my face?
[00:36:17.660 --> 00:36:22.660]   Yeah, his exact quote was, "Yeah, well, I wanted to see what they'd ask me to do."
[00:36:22.660 --> 00:36:23.660]   Exactly.
[00:36:23.660 --> 00:36:25.260]   I don't care what they're going to do with it.
[00:36:25.260 --> 00:36:27.460]   I just want to know what they want me to do.
[00:36:27.460 --> 00:36:28.460]   Curiosity.
[00:36:28.460 --> 00:36:29.460]   Sure.
[00:36:29.460 --> 00:36:33.220]   I mean, without seeing the contract, we don't know much more.
[00:36:33.220 --> 00:36:39.780]   I'm actually wondering if this wasn't a face scan, but a face scam in that maybe this wasn't
[00:36:39.780 --> 00:36:40.780]   Google at all.
[00:36:40.780 --> 00:36:42.900]   I- Because that's a really good question.
[00:36:42.900 --> 00:36:43.900]   Yeah.
[00:36:43.900 --> 00:36:48.820]   I'm thinking, why would Google have to go out and do this when they have between contractors
[00:36:48.820 --> 00:36:54.460]   and regular employees, 80, 90,000 faces to scan already?
[00:36:54.460 --> 00:36:58.700]   Well, I don't really see the point of them going out in the public and doing this.
[00:36:58.700 --> 00:37:05.020]   Because they don't have is someone playing with their phone trying to unlock it.
[00:37:05.020 --> 00:37:06.020]   That's what this was for.
[00:37:06.020 --> 00:37:07.660]   This was- I'm with Jason.
[00:37:07.660 --> 00:37:11.660]   I think this was testing the Pixel 4's unlock feature.
[00:37:11.660 --> 00:37:16.220]   And so they needed someone to be able to kind of look at it from various angles.
[00:37:16.220 --> 00:37:17.900]   Yeah, that could well be.
[00:37:17.900 --> 00:37:22.780]   I mean, when I was at Google, most of the people I worked with actually carried iPhones.
[00:37:22.780 --> 00:37:26.100]   So if you wanted like a new user group, you got it there.
[00:37:26.100 --> 00:37:28.900]   Yes, trust me, did they have it?
[00:37:28.900 --> 00:37:30.340]   I don't know.
[00:37:30.340 --> 00:37:34.220]   Yeah, and I don't know that I read anything about them actually.
[00:37:34.220 --> 00:37:38.540]   Like in this report, there was nothing in there that said that he was actually unlocking
[00:37:38.540 --> 00:37:41.340]   the phone with his face.
[00:37:41.340 --> 00:37:44.420]   They were just scanning it for multiple directions.
[00:37:44.420 --> 00:37:46.580]   I just wasn't sure if- What did Google say?
[00:37:46.580 --> 00:37:48.260]   They were actually using the Pixel 4.
[00:37:48.260 --> 00:37:50.820]   No, they're just- Yeah, and I'm not sure that we've heard Google say anything.
[00:37:50.820 --> 00:37:51.820]   This guy.
[00:37:51.820 --> 00:37:53.620]   If it was a scam, they probably would have by now.
[00:37:53.620 --> 00:37:54.620]   Right?
[00:37:54.620 --> 00:37:56.500]   It was an assumption.
[00:37:56.500 --> 00:38:00.420]   I naturally ask Google what it would be doing, but the company didn't immediately respond.
[00:38:00.420 --> 00:38:01.420]   Okay.
[00:38:01.420 --> 00:38:02.420]   Okay.
[00:38:02.420 --> 00:38:03.420]   Let's call Google.
[00:38:03.420 --> 00:38:04.620]   Let's see what they have to say right now.
[00:38:04.620 --> 00:38:05.620]   Right now.
[00:38:05.620 --> 00:38:09.540]   I wish, you know, that would be so awesome if we had a hotline like a bat phone to the
[00:38:09.540 --> 00:38:10.540]   Google PR team.
[00:38:10.540 --> 00:38:12.140]   It'd probably be the PR team.
[00:38:12.140 --> 00:38:16.140]   But if we had an actual Google person who would talk to us, I'd be even better.
[00:38:16.140 --> 00:38:20.300]   We would do it like three, maybe four times before we realized they always give us the
[00:38:20.300 --> 00:38:21.300]   same answer.
[00:38:21.300 --> 00:38:23.940]   I think that's more effective actually.
[00:38:23.940 --> 00:38:28.940]   Because, you know, okay, we have nothing to announce at this time.
[00:38:28.940 --> 00:38:29.940]   Yes, exactly.
[00:38:29.940 --> 00:38:30.940]   We have nothing to announce at this time.
[00:38:30.940 --> 00:38:31.940]   Okay.
[00:38:31.940 --> 00:38:32.940]   Well, that bit is done.
[00:38:32.940 --> 00:38:33.940]   I guess we're done with that one.
[00:38:33.940 --> 00:38:38.100]   So, Google's been doing some work in secret.
[00:38:38.100 --> 00:38:43.500]   NASA, which was a partner in this work, inadvertently published a paper which they almost immediately
[00:38:43.500 --> 00:38:49.220]   unpublished, but before too long, people were able to get a copy of it.
[00:38:49.220 --> 00:38:50.220]   Of course.
[00:38:50.220 --> 00:38:56.220]   Google research paper was titled Quantum Supremacy using a programmable superconducting
[00:38:56.220 --> 00:38:57.980]   processor.
[00:38:57.980 --> 00:39:03.140]   Quantum Supremacy is a term used by quantum computing folks.
[00:39:03.140 --> 00:39:09.140]   In fact, I'll point you to a really good and completely impenetrable blog post by the
[00:39:09.140 --> 00:39:13.380]   guy who coined the term, Scott Aronson.
[00:39:13.380 --> 00:39:15.340]   It's actually really great.
[00:39:15.340 --> 00:39:19.820]   Scott Supreme Quantum Supremacy FAQ.
[00:39:19.820 --> 00:39:24.140]   He talks about what quantum supremacy is, but I will attempt in my own way to define it.
[00:39:24.140 --> 00:39:28.780]   It is essentially a proof that quantum computing works.
[00:39:28.780 --> 00:39:37.060]   If a quantum computer can do something significantly faster than a Turing machine can do, then
[00:39:37.060 --> 00:39:38.220]   that's quantum supremacy.
[00:39:38.220 --> 00:39:45.340]   And in fact, in this paper, it was asserted that the Google quantum computer was able
[00:39:45.340 --> 00:39:53.540]   to solve a problem 50,000 times faster than the world's fastest traditional supercomputer.
[00:39:53.540 --> 00:39:55.860]   That's amazing.
[00:39:55.860 --> 00:39:58.500]   It's not a task you would want.
[00:39:58.500 --> 00:40:03.780]   It's not solving a problem that anyone cares about, but it's merely a demonstration that
[00:40:03.780 --> 00:40:05.540]   you could do it.
[00:40:05.540 --> 00:40:07.060]   It's a proof of concept.
[00:40:07.060 --> 00:40:12.220]   It's kind of a random number thing, but it was able to do it according to the paper in
[00:40:12.220 --> 00:40:16.700]   three minutes and 20 seconds, what would take the world's fastest supercomputer summit
[00:40:16.700 --> 00:40:22.420]   10,000 years?
[00:40:22.420 --> 00:40:30.420]   But if you read the blog by Scott Aronson, you'll realize that it's more complicated than that.
[00:40:30.420 --> 00:40:32.180]   It's more complicated than that.
[00:40:32.180 --> 00:40:36.580]   If you really care about quantum supremacy, it's probably worth reading this.
[00:40:36.580 --> 00:40:42.900]   He basically confirms the post saying, "Yeah, I was involved in this.
[00:40:42.900 --> 00:40:45.420]   I've been trying not to tell anybody for a long time.
[00:40:45.420 --> 00:40:46.580]   It's been a big secret."
[00:40:46.580 --> 00:40:52.700]   Google pulled the article because they want to make a big splash when they announce it.
[00:40:52.700 --> 00:40:57.540]   Probably would behoove everybody to understand a little bit more about what this means.
[00:40:57.540 --> 00:41:02.020]   It does not mean that a quantum computer is around the corner.
[00:41:02.020 --> 00:41:07.260]   By the way, he says, "I did not create the quantum supremacy phrase that was coined by
[00:41:07.260 --> 00:41:13.500]   John Preskle in 2012, but these widely consider the guy who first..."
[00:41:13.500 --> 00:41:17.460]   Anyway, that establishes his Bonafide for this article.
[00:41:17.460 --> 00:41:22.580]   He talks about what the problem solved was likely to be and what it means, and it doesn't
[00:41:22.580 --> 00:41:25.980]   mean a lot of things.
[00:41:25.980 --> 00:41:27.340]   This is important.
[00:41:27.340 --> 00:41:32.380]   It might in fact be the kind of thing that you'll look back and say, "Oh, I remember."
[00:41:32.380 --> 00:41:35.580]   Back in 2019, when we first...
[00:41:35.580 --> 00:41:39.380]   He likens it, Aaron's likens it to Kitty Hawk.
[00:41:39.380 --> 00:41:45.460]   The first flight of the Wright brothers, which by the way didn't become public for months,
[00:41:45.460 --> 00:41:50.100]   certainly didn't imply that we'd be able to fly across the country or across around
[00:41:50.100 --> 00:41:52.100]   the world in this airplane.
[00:41:52.100 --> 00:41:53.100]   It was just the happiest.
[00:41:53.100 --> 00:41:59.700]   But it was the first time humans flew for any length, any appreciable distance.
[00:41:59.700 --> 00:42:03.260]   That's kind of what this is.
[00:42:03.260 --> 00:42:06.700]   So what does it mean to Bitcoin?
[00:42:06.700 --> 00:42:09.340]   Well, that's an interesting question.
[00:42:09.340 --> 00:42:10.380]   Some people brought this up.
[00:42:10.380 --> 00:42:12.540]   I don't know if it means anything for Bitcoin.
[00:42:12.540 --> 00:42:20.380]   It would mean maybe if you could create a Bitcoin miner out of a quantum computer, which
[00:42:20.380 --> 00:42:22.740]   by the way, that's years and years off.
[00:42:22.740 --> 00:42:26.860]   You could perhaps make us some money, but that's not likely.
[00:42:26.860 --> 00:42:30.020]   Remember these quantum computers have to be super cooled.
[00:42:30.020 --> 00:42:32.460]   They're very expensive to run.
[00:42:32.460 --> 00:42:37.380]   They're very difficult to create and they're incredibly unstable.
[00:42:37.380 --> 00:42:40.060]   So it's not a practical solution.
[00:42:40.060 --> 00:42:41.540]   Sounds like the perfect computer.
[00:42:41.540 --> 00:42:42.540]   Yeah.
[00:42:42.540 --> 00:42:43.540]   Just what you want.
[00:42:43.540 --> 00:42:49.540]   If you're a good, you're an evil, rich monster, what are you doing?
[00:42:49.540 --> 00:42:50.540]   Nothing.
[00:42:50.540 --> 00:42:52.220]   It doesn't mean anything.
[00:42:52.220 --> 00:42:55.740]   It's not worth the trouble even for even evil, rich monsters.
[00:42:55.740 --> 00:43:00.260]   Not unless you've got the research means of NASA and Google.
[00:43:00.260 --> 00:43:01.260]   No.
[00:43:01.260 --> 00:43:03.380]   But it's fascinating.
[00:43:03.380 --> 00:43:06.340]   And anybody who watches our shows and his interest in this kind of stuff probably really
[00:43:06.340 --> 00:43:10.860]   should read Scott Aronson's blog, his blog is scottaronson.com.
[00:43:10.860 --> 00:43:14.740]   S-C-O-T-T-A-A-R-O-N-S-O-N.com.
[00:43:14.740 --> 00:43:18.420]   But you could also search for quantum supremacy FAQ.
[00:43:18.420 --> 00:43:19.580]   You'd probably find it already done.
[00:43:19.580 --> 00:43:20.860]   I doubt there's more than one.
[00:43:21.860 --> 00:43:23.820]   Here's another one I wasn't here for.
[00:43:23.820 --> 00:43:27.780]   But I think probably it's the highlight of the year.
[00:43:27.780 --> 00:43:31.740]   Why does Jeff Jarvis hate dark mode so much?
[00:43:31.740 --> 00:43:33.100]   Dark mode is for suckers.
[00:43:33.100 --> 00:43:34.100]   Okay.
[00:43:34.100 --> 00:43:35.380]   I'm so happy you brought this up.
[00:43:35.380 --> 00:43:37.820]   I was wondering if it would end up making it in here.
[00:43:37.820 --> 00:43:38.820]   I'm curious to know.
[00:43:38.820 --> 00:43:41.300]   We're starting with it because I'm vindicated.
[00:43:41.300 --> 00:43:42.300]   Okay.
[00:43:42.300 --> 00:43:44.540]   His modo says it's all BS.
[00:43:44.540 --> 00:43:45.540]   Okay.
[00:43:45.540 --> 00:43:47.540]   This guy on his modo says it's all BS.
[00:43:47.540 --> 00:43:48.540]   Yeah.
[00:43:48.540 --> 00:43:49.540]   This guy says it's good enough for me.
[00:43:49.540 --> 00:43:51.540]   I'm not going to go to my research.
[00:43:51.540 --> 00:43:56.660]   Adam Eggst at tidbits explains in his Austin detail that that accepted extraordinary situations.
[00:43:56.660 --> 00:43:59.540]   Dark mode is not easy on the eyes in any way.
[00:43:59.540 --> 00:44:00.540]   Well, how is that?
[00:44:00.540 --> 00:44:04.620]   Well, the words Samantha Cole reports that researchers found that people with astigmatism
[00:44:04.620 --> 00:44:09.180]   light text on dark backgrounds aggravates the condition making text harder to read.
[00:44:09.180 --> 00:44:14.460]   What's more as Suzanne Mayer, a university of Passau researcher said, who conducted multiple
[00:44:14.460 --> 00:44:20.340]   studies and found which design screws with her brain in all the studies, participants
[00:44:20.340 --> 00:44:26.140]   were better performing in the positive polarity condition, otherwise known as not dark mode.
[00:44:26.140 --> 00:44:32.220]   They detected more errors and read faster when dark text was presented on light background
[00:44:32.220 --> 00:44:34.940]   rather than the reverse.
[00:44:34.940 --> 00:44:38.980]   So I just want this on the record that I'm not insane.
[00:44:38.980 --> 00:44:41.420]   You need a hatred for dark mode.
[00:44:41.420 --> 00:44:42.420]   You hate it.
[00:44:42.420 --> 00:44:43.420]   I'm not done.
[00:44:43.420 --> 00:44:46.300]   We're sticking with light mode.
[00:44:46.300 --> 00:44:47.300]   Light mode is the way to go.
[00:44:47.300 --> 00:44:48.900]   It's the way God made.
[00:44:48.900 --> 00:44:51.980]   Oh, you know, John, could you just kill the lights in here?
[00:44:51.980 --> 00:44:54.060]   It's too bright for me right now.
[00:44:54.060 --> 00:44:55.060]   It's far too.
[00:44:55.060 --> 00:44:56.060]   Thank you.
[00:44:56.060 --> 00:44:57.060]   This is what we need to do.
[00:44:57.060 --> 00:44:58.060]   Thank you.
[00:44:58.060 --> 00:44:59.060]   Oh, there we go.
[00:44:59.060 --> 00:45:05.180]   So I know if that's submit, I'll just throw out the two times that I have really like I
[00:45:05.180 --> 00:45:10.060]   like dark mode on Twitter where reading comprehension doesn't really matter as much because it's
[00:45:10.060 --> 00:45:11.060]   very short.
[00:45:11.060 --> 00:45:12.060]   Yeah.
[00:45:12.060 --> 00:45:18.540]   And so like it when I am in a professional setting or like a conference setting and I'm
[00:45:18.540 --> 00:45:22.780]   trying to type without everybody else being really aware of what I'm typing and that I'm
[00:45:22.780 --> 00:45:25.700]   typing and dark mode is awesome for that.
[00:45:25.700 --> 00:45:26.700]   Yeah.
[00:45:26.700 --> 00:45:28.780]   So those are my pro dark mode moments.
[00:45:28.780 --> 00:45:33.380]   I appreciate dark mode when I am in a dark environment.
[00:45:33.380 --> 00:45:39.380]   Like I think that that is an a very honest like application of why dark mode can be very
[00:45:39.380 --> 00:45:40.380]   useful.
[00:45:40.380 --> 00:45:43.780]   In a dark environment, like if I pick up my phone off the bedside table or whatever,
[00:45:43.780 --> 00:45:45.180]   it's dark in the room and I look at it.
[00:45:45.180 --> 00:45:46.180]   It's a bright white.
[00:45:46.180 --> 00:45:48.380]   Like that does irritate my eyes.
[00:45:48.380 --> 00:45:49.380]   Shoxi.
[00:45:49.380 --> 00:45:53.540]   I mean, it's not like, discomfortable or uncomfortable, not discomfortable.
[00:45:53.540 --> 00:45:55.580]   It's the word.
[00:45:55.580 --> 00:45:59.540]   But at the same time, like a darker mode is more pleasing.
[00:45:59.540 --> 00:46:01.380]   So I can prefer it.
[00:46:01.380 --> 00:46:02.380]   Yeah.
[00:46:02.380 --> 00:46:03.380]   Your eyes are smart.
[00:46:03.380 --> 00:46:04.380]   They figure it out.
[00:46:04.380 --> 00:46:05.380]   I'm sure.
[00:46:05.380 --> 00:46:09.020]   So when I have a migraine, dark mode is the only way I can get online.
[00:46:09.020 --> 00:46:11.420]   I'm like, hold out the migraine card, aren't I?
[00:46:11.420 --> 00:46:12.420]   Yeah.
[00:46:12.420 --> 00:46:13.420]   Well, it's a migraine.
[00:46:13.420 --> 00:46:14.420]   Geez.
[00:46:14.420 --> 00:46:15.420]   You're the cranky old man.
[00:46:15.420 --> 00:46:16.420]   Come on.
[00:46:16.420 --> 00:46:17.420]   I got to do the migraine.
[00:46:17.420 --> 00:46:19.100]   You see, it's a thing.
[00:46:19.100 --> 00:46:25.900]   I battle with this and I'm one of those people that see better at nighttime than I do in
[00:46:25.900 --> 00:46:26.900]   the daytime.
[00:46:26.900 --> 00:46:29.900]   And it's been that way for about as long as I can remember.
[00:46:29.900 --> 00:46:34.020]   As a matter of fact, just within the last hour, I was walking the hallway and I had
[00:46:34.020 --> 00:46:37.660]   all my sunglasses and I had to think, oh, take your sunglasses off.
[00:46:37.660 --> 00:46:41.740]   Oh, no, no, no, no, no, no, no, I want you to do the show in the sunglasses.
[00:46:41.740 --> 00:46:42.740]   This is your new look.
[00:46:42.740 --> 00:46:43.740]   Oh, okay.
[00:46:43.740 --> 00:46:44.740]   Oh, yeah.
[00:46:44.740 --> 00:46:45.740]   I would.
[00:46:45.740 --> 00:46:47.580]   But I really do open your eyes.
[00:46:47.580 --> 00:46:49.580]   You're the Adam Riddjors.
[00:46:49.580 --> 00:46:55.740]   I do get better use out of the dark mode on my eyes because of the sensitivity.
[00:46:55.740 --> 00:46:59.900]   I like having a better contrast when I look at the screen and heck, even sitting in the
[00:46:59.900 --> 00:47:02.060]   office with Mr. Howell here.
[00:47:02.060 --> 00:47:03.060]   Yeah.
[00:47:03.060 --> 00:47:05.780]   When I get here before them most of the time.
[00:47:05.780 --> 00:47:06.780]   And the office is dark.
[00:47:06.780 --> 00:47:09.380]   Now I have on one little lamp because that's enough to me.
[00:47:09.380 --> 00:47:11.460]   You look at your dark mode with sunglasses on.
[00:47:11.460 --> 00:47:12.780]   Are you double dark mode?
[00:47:12.780 --> 00:47:14.980]   I have super duper dark mode.
[00:47:14.980 --> 00:47:17.820]   It's not on purpose, but I have.
[00:47:17.820 --> 00:47:18.820]   All right.
[00:47:18.820 --> 00:47:19.820]   Here's the next question.
[00:47:19.820 --> 00:47:21.820]   So when you're on a plane.
[00:47:21.820 --> 00:47:23.540]   So I'm going to say there's a lot of fight going on now.
[00:47:23.540 --> 00:47:27.620]   There's a lot of fighting going on these days on the rates on planes.
[00:47:27.620 --> 00:47:31.020]   And when I go on the plane because I'm global services, I bow down before me.
[00:47:31.020 --> 00:47:32.300]   I get on the plane before the pilot.
[00:47:32.300 --> 00:47:33.300]   That's all I've got.
[00:47:33.300 --> 00:47:34.300]   Right.
[00:47:34.300 --> 00:47:36.300]   I go on the seat on the aisle.
[00:47:36.300 --> 00:47:38.220]   I always raise the shades.
[00:47:38.220 --> 00:47:39.660]   I want the shades up.
[00:47:39.660 --> 00:47:41.100]   I can't stand for people to shades down.
[00:47:41.100 --> 00:47:43.780]   Are you a shades up or shades down person on plane?
[00:47:43.780 --> 00:47:44.780]   Good question.
[00:47:44.780 --> 00:47:45.780]   Good question.
[00:47:45.780 --> 00:47:47.260]   There's a good reason for this though.
[00:47:47.260 --> 00:47:50.500]   I am a shades up for that.
[00:47:50.500 --> 00:47:53.220]   But that's because I am a aviation nerd.
[00:47:53.220 --> 00:47:54.780]   I love airplanes.
[00:47:54.780 --> 00:47:57.460]   So I'm always looking out with my sunglasses.
[00:47:57.460 --> 00:47:58.460]   I'm not.
[00:47:58.460 --> 00:47:59.460]   Shades.
[00:47:59.460 --> 00:48:00.460]   Yeah.
[00:48:00.460 --> 00:48:01.460]   I'm a shades down.
[00:48:01.460 --> 00:48:02.620]   Jeff, I can't believe it.
[00:48:02.620 --> 00:48:03.620]   We don't agree on anything.
[00:48:03.620 --> 00:48:04.620]   Shocking prize.
[00:48:04.620 --> 00:48:10.660]   You know, again, you know, for me, it mirrors my dark mode rational.
[00:48:10.660 --> 00:48:12.340]   Like, it depends on the time of day.
[00:48:12.340 --> 00:48:14.060]   If it's daytime, I'll want the shades up.
[00:48:14.060 --> 00:48:18.700]   If it's nighttime or if it's like early morning, that's probably the more applicable time.
[00:48:18.700 --> 00:48:23.420]   Like if I, you know, travel to the airport and it's like 530 in the morning, I have to
[00:48:23.420 --> 00:48:26.740]   catch a flight or whatever and the sun's going to come up and I do want to sleep a little
[00:48:26.740 --> 00:48:27.740]   bit.
[00:48:27.740 --> 00:48:28.740]   Sure.
[00:48:28.740 --> 00:48:29.740]   Shades down.
[00:48:29.740 --> 00:48:30.740]   That's fine.
[00:48:30.740 --> 00:48:35.180]   I guess that I wear my glasses just sparingly.
[00:48:35.180 --> 00:48:38.580]   And it's clearly helped my eyes over the years because I don't have to wear them all the
[00:48:38.580 --> 00:48:39.580]   time the way most people.
[00:48:39.580 --> 00:48:40.580]   Oh, it's vanity.
[00:48:40.580 --> 00:48:41.580]   It's pure vanity.
[00:48:41.580 --> 00:48:44.580]   No, no, no, I don't wear my glasses all the time.
[00:48:44.580 --> 00:48:46.580]   Don't deserve it.
[00:48:46.580 --> 00:48:50.300]   And it's because I hate having a filter between me and people I'm talking to.
[00:48:50.300 --> 00:48:51.300]   Yeah.
[00:48:51.300 --> 00:48:53.500]   I feel like I don't know why.
[00:48:53.500 --> 00:48:54.500]   I just, I don't know.
[00:48:54.500 --> 00:48:57.980]   I can't walk to the bathroom from bed without glasses off.
[00:48:57.980 --> 00:49:00.580]   I can't talk to anything without glasses off.
[00:49:00.580 --> 00:49:04.460]   I'm all, yeah, my eyes are probably about as bad as yours.
[00:49:04.460 --> 00:49:07.820]   It sounds like I'm sorry to hear that.
[00:49:07.820 --> 00:49:08.820]   Yeah.
[00:49:08.820 --> 00:49:09.820]   Yeah.
[00:49:09.820 --> 00:49:10.820]   It's all right.
[00:49:10.820 --> 00:49:11.820]   Well, I just want to on the record here.
[00:49:11.820 --> 00:49:12.820]   It's good.
[00:49:12.820 --> 00:49:13.820]   This is.
[00:49:13.820 --> 00:49:15.820]   Dark mode is for sucker.
[00:49:15.820 --> 00:49:17.300]   And you have to tell it says it.
[00:49:17.300 --> 00:49:18.900]   So we should all heed what he says.
[00:49:18.900 --> 00:49:21.540]   That said, I never quote his motto, but now they're right.
[00:49:21.540 --> 00:49:22.540]   Yeah.
[00:49:22.540 --> 00:49:25.780]   And I mean, a large part of what he's saying here is that it's, it's a trend like dark
[00:49:25.780 --> 00:49:30.740]   mode is a trend, therefore people like it because people like it.
[00:49:30.740 --> 00:49:33.300]   And I mean, yeah, there's probably a little bit of truth to that actually.
[00:49:33.300 --> 00:49:35.580]   No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.
[00:49:35.580 --> 00:49:38.340]   I know that I like it for reasons outside of that too.
[00:49:38.340 --> 00:49:40.540]   I think it's certain times a day for me.
[00:49:40.540 --> 00:49:42.700]   It's more aesthetically pleasing to my eyes.
[00:49:42.700 --> 00:49:44.060]   Yeah, I agree with that.
[00:49:44.060 --> 00:49:48.540]   There are people that are following the trend because they've heard about it, but again,
[00:49:48.540 --> 00:49:51.700]   there's people like me is just for comfort, you know?
[00:49:51.700 --> 00:49:52.700]   Yeah, comfort.
[00:49:52.700 --> 00:49:53.700]   Yeah.
[00:49:53.700 --> 00:49:55.700]   So I think that's the way I think about the battery savings and everything.
[00:49:55.700 --> 00:49:59.780]   Andrew makes the point like if you really added it up, you're probably not saving that
[00:49:59.780 --> 00:50:00.780]   much.
[00:50:00.780 --> 00:50:02.700]   A lot of dark mode is not true dark anyways.
[00:50:02.700 --> 00:50:06.700]   It's like a shade of gray and you're still burning battery and you do that.
[00:50:06.700 --> 00:50:08.100]   So what are you really saving?
[00:50:08.100 --> 00:50:11.300]   And you have to be on the, you know, an AMOLED display so that you have to have the right
[00:50:11.300 --> 00:50:14.180]   display technology for that to even matter.
[00:50:14.180 --> 00:50:17.380]   So anyways, I don't know if you like dark mode.
[00:50:17.380 --> 00:50:19.540]   Now you have the option at the end of the day.
[00:50:19.540 --> 00:50:21.020]   Isn't it nice to have options?
[00:50:21.020 --> 00:50:22.020]   Yes.
[00:50:22.020 --> 00:50:23.020]   No, it is.
[00:50:23.020 --> 00:50:24.020]   All right.
[00:50:24.020 --> 00:50:26.740]   Happy we've got it.
[00:50:26.740 --> 00:50:31.140]   I'm happy we got the most important story out of the way at the very top.
[00:50:31.140 --> 00:50:34.700]   But I'm not having, I'm not having Pixel 4 in me.
[00:50:34.700 --> 00:50:38.540]   No, I'm just, I just, I'm making my 3A XL.
[00:50:38.540 --> 00:50:39.780]   I'm very happy with it.
[00:50:39.780 --> 00:50:42.940]   I just, facial recognition.
[00:50:42.940 --> 00:50:48.300]   Have you been, have you been paying attention to some of the reviews that seem to be dropping
[00:50:48.300 --> 00:50:49.300]   like crazy right now?
[00:50:49.300 --> 00:50:52.820]   No, what I know you're going to tell me about them.
[00:50:52.820 --> 00:50:56.340]   Yeah, they don't like, I find myself being thrilled.
[00:50:56.340 --> 00:50:58.620]   Less than excited.
[00:50:58.620 --> 00:51:02.580]   And I think one of the big disappointments for me that I'm hearing from multiple reviewers,
[00:51:02.580 --> 00:51:04.060]   almost all of them is just battery.
[00:51:04.060 --> 00:51:05.060]   Yeah.
[00:51:05.060 --> 00:51:08.860]   Battery I feel like is just, that's just one of those things on an $800 to $900 device.
[00:51:08.860 --> 00:51:09.860]   Yeah, the nail battery.
[00:51:09.860 --> 00:51:12.100]   And if you're not, you're doing something wrong.
[00:51:12.100 --> 00:51:15.700]   But the 90 Hertz display, I'm sure it looks great.
[00:51:15.700 --> 00:51:17.860]   But is that really necessary?
[00:51:17.860 --> 00:51:21.260]   Do we really need 90 Hertz on our phones?
[00:51:21.260 --> 00:51:23.260]   Necessary no.
[00:51:23.260 --> 00:51:24.260]   I would say.
[00:51:24.260 --> 00:51:25.820]   No, but they put it in there.
[00:51:25.820 --> 00:51:27.660]   It doesn't even work all the time.
[00:51:27.660 --> 00:51:31.660]   They put it in there and then they turned it off because they know it's going to just
[00:51:31.660 --> 00:51:33.820]   kill the already small battery.
[00:51:33.820 --> 00:51:38.500]   Well, the weird thing is it only works when your screen is on full brightness or like
[00:51:38.500 --> 00:51:40.860]   a love 75% brightness.
[00:51:40.860 --> 00:51:41.860]   Right.
[00:51:41.860 --> 00:51:46.380]   So if like you really have to make that choice, like I want no battery life.
[00:51:46.380 --> 00:51:47.380]   Mm hmm.
[00:51:47.380 --> 00:51:49.660]   It's the anti dark mode.
[00:51:49.660 --> 00:51:51.980]   Yes, it is the anti dark mode.
[00:51:51.980 --> 00:51:52.980]   Oh no.
[00:51:52.980 --> 00:51:53.980]   It's the bright mode.
[00:51:53.980 --> 00:51:56.340]   Mr. Jarvis, you said you wouldn't cite it.
[00:51:56.340 --> 00:51:59.140]   But if you were a pixel two on her, would you be a site?
[00:51:59.140 --> 00:52:00.140]   That'd be very different.
[00:52:00.140 --> 00:52:01.140]   Very different.
[00:52:01.140 --> 00:52:02.140]   That makes sense.
[00:52:02.140 --> 00:52:03.140]   Yeah.
[00:52:03.140 --> 00:52:04.140]   You'd be excited to just upgrade to something new.
[00:52:04.140 --> 00:52:05.700]   It's been a couple of years.
[00:52:05.700 --> 00:52:06.780]   That's what I had before.
[00:52:06.780 --> 00:52:07.780]   I had the pixel two.
[00:52:07.780 --> 00:52:08.780]   Yeah.
[00:52:08.780 --> 00:52:10.620]   And now I have the pixel three.
[00:52:10.620 --> 00:52:13.220]   Three a and the three a is where we have to.
[00:52:13.220 --> 00:52:14.820]   Do you miss the squeeze?
[00:52:14.820 --> 00:52:15.820]   I miss the squeeze.
[00:52:15.820 --> 00:52:16.820]   That's what I was going to ask.
[00:52:16.820 --> 00:52:17.820]   I just need the squeeze.
[00:52:17.820 --> 00:52:20.420]   Stacy, you should try squeezing your three a.
[00:52:20.420 --> 00:52:22.340]   It doesn't work.
[00:52:22.340 --> 00:52:23.340]   Squeeze the bottom.
[00:52:23.340 --> 00:52:25.780]   Oh my god it worked.
[00:52:25.780 --> 00:52:29.380]   Oh, that's perfect.
[00:52:29.380 --> 00:52:30.380]   Oh, yeah.
[00:52:30.380 --> 00:52:34.780]   I've been squeezing this thing for, I guess I need to adjust it.
[00:52:34.780 --> 00:52:35.780]   It's only on the bottom.
[00:52:35.780 --> 00:52:36.780]   It's only the bottom half.
[00:52:36.780 --> 00:52:38.620]   Oh, so it's not the side.
[00:52:38.620 --> 00:52:39.620]   Okay.
[00:52:39.620 --> 00:52:40.620]   No, no, not the bottom bottom.
[00:52:40.620 --> 00:52:41.620]   It's the bottom half.
[00:52:41.620 --> 00:52:43.780]   Yeah, it's the bottom half of the side.
[00:52:43.780 --> 00:52:44.780]   Yeah.
[00:52:44.780 --> 00:52:47.420]   So I've been squeezing it kind of at the midline.
[00:52:47.420 --> 00:52:48.420]   So it doesn't do it.
[00:52:48.420 --> 00:52:49.420]   Oh, it did it there.
[00:52:49.420 --> 00:52:50.420]   Oh, carsten.
[00:52:50.420 --> 00:52:55.700]   You weren't choking the phone.
[00:52:55.700 --> 00:52:57.700]   There are sensitivity settings.
[00:52:57.700 --> 00:52:58.700]   Hug the phone.
[00:52:58.700 --> 00:52:59.700]   I know.
[00:52:59.700 --> 00:53:02.060]   That's why I'll booty squeeze.
[00:53:02.060 --> 00:53:07.140]   I was sad because I really use that a lot in my day to day phone interactions.
[00:53:07.140 --> 00:53:12.420]   I was like, I kept squeezing it and it was like, it's like she just hugged your phone.
[00:53:12.420 --> 00:53:13.420]   Right.
[00:53:13.420 --> 00:53:14.420]   Just hug it.
[00:53:14.420 --> 00:53:15.420]   Don't choke it.
[00:53:15.420 --> 00:53:17.300]   I never, I disable that feature.
[00:53:17.300 --> 00:53:18.300]   I do too.
[00:53:18.300 --> 00:53:19.300]   Yeah.
[00:53:19.300 --> 00:53:24.780]   I just found like no matter what sensitivity I had it set to, it would fire when I didn't
[00:53:24.780 --> 00:53:25.780]   want it to.
[00:53:25.780 --> 00:53:26.780]   Same here.
[00:53:26.780 --> 00:53:28.300]   It's like, stop it.
[00:53:28.300 --> 00:53:29.820]   I don't need you all the time, assistant.
[00:53:29.820 --> 00:53:30.820]   I don't need you right now.
[00:53:30.820 --> 00:53:31.820]   All right.
[00:53:31.820 --> 00:53:32.820]   Same here.
[00:53:32.820 --> 00:53:37.100]   I think after five years of carpal tunnel, maybe my hand grip strength is so weak that
[00:53:37.100 --> 00:53:39.580]   I never unintentionally squeezed it.
[00:53:39.580 --> 00:53:40.580]   Oh, bleh.
[00:53:40.580 --> 00:53:42.900]   Apparently I can't even squeeze it when it's there.
[00:53:42.900 --> 00:53:48.180]   But like, it's so great because I hate talking to my phone because I set off everything in
[00:53:48.180 --> 00:53:49.380]   the house.
[00:53:49.380 --> 00:53:52.820]   And so the squeeze is just like, okay.
[00:53:52.820 --> 00:53:57.940]   I can just imagine 100 devices saying yes, Stacy, yes, Stacy, yes, Stacy.
[00:53:57.940 --> 00:54:04.180]   They also just added the drag from the bottom right corner.
[00:54:04.180 --> 00:54:07.660]   Yeah, in order to launch the assistant.
[00:54:07.660 --> 00:54:10.140]   I think I have that on my swipe up on it.
[00:54:10.140 --> 00:54:11.140]   Oh, yeah.
[00:54:11.140 --> 00:54:13.860]   I can't show that if you really want to see it.
[00:54:13.860 --> 00:54:17.460]   Yeah, so you have to kind of go in a diagonal from the bottom.
[00:54:17.460 --> 00:54:18.460]   Yep.
[00:54:18.460 --> 00:54:19.660]   And that does your system.
[00:54:19.660 --> 00:54:20.660]   Okay.
[00:54:20.660 --> 00:54:25.660]   I never really got into the gesture UI on my phone.
[00:54:25.660 --> 00:54:27.140]   I just like it.
[00:54:27.140 --> 00:54:28.300]   Old school.
[00:54:28.300 --> 00:54:30.180]   And so this is the pixel.
[00:54:30.180 --> 00:54:31.180]   This is two?
[00:54:31.180 --> 00:54:32.180]   This is the two.
[00:54:32.180 --> 00:54:33.180]   This is the two.
[00:54:33.180 --> 00:54:34.180]   Okay.
[00:54:34.180 --> 00:54:36.500]   But everybody at home uses the gesture interface.
[00:54:36.500 --> 00:54:37.500]   I turn it off.
[00:54:37.500 --> 00:54:38.500]   Okay.
[00:54:38.500 --> 00:54:39.860]   I just like using an old stock button.
[00:54:39.860 --> 00:54:44.660]   And on the four, which you will be getting at some point soon to here too, you do have
[00:54:44.660 --> 00:54:48.620]   the ability to opt for the three button versus the gesture.
[00:54:48.620 --> 00:54:49.620]   So that's still there.
[00:54:49.620 --> 00:54:52.740]   I'm not savvy enough for gestures just yet in the new gen.
[00:54:52.740 --> 00:54:56.420]   But you won't get if you do that, you won't get the super assistant.
[00:54:56.420 --> 00:54:57.820]   Yeah, right.
[00:54:57.820 --> 00:54:59.340]   So that's kind of crazy, right?
[00:54:59.340 --> 00:55:05.580]   Like right now there was news that at least with the pixel for at launch, if you have,
[00:55:05.580 --> 00:55:11.060]   if you don't have gestures activated, then you can't access assistant.
[00:55:11.060 --> 00:55:12.060]   Makes no sense.
[00:55:12.060 --> 00:55:14.900]   That makes absolutely no sense.
[00:55:14.900 --> 00:55:17.980]   You can't, you can still get assistant, but you don't get.
[00:55:17.980 --> 00:55:19.620]   But not super assistant.
[00:55:19.620 --> 00:55:20.620]   Awesome.
[00:55:20.620 --> 00:55:21.620]   Super assistant.
[00:55:21.620 --> 00:55:23.620]   The one is the continuum conversational version.
[00:55:23.620 --> 00:55:24.620]   Is that the top?
[00:55:24.620 --> 00:55:25.620]   Let's do it like what?
[00:55:25.620 --> 00:55:28.020]   If it can bring the one like you can't do the other.
[00:55:28.020 --> 00:55:29.420]   What is super assistant?
[00:55:29.420 --> 00:55:31.420]   I feel so lost.
[00:55:31.420 --> 00:55:33.420]   You get continuing conversations.
[00:55:33.420 --> 00:55:34.580]   You get something else.
[00:55:34.580 --> 00:55:35.580]   I don't know.
[00:55:35.580 --> 00:55:36.580]   They talked about it at the.
[00:55:36.580 --> 00:55:38.580]   So a lot more contextual.
[00:55:38.580 --> 00:55:39.580]   At the.
[00:55:39.580 --> 00:55:40.580]   That's more residence.
[00:55:40.580 --> 00:55:41.940]   Residence on the device.
[00:55:41.940 --> 00:55:42.940]   Right.
[00:55:42.940 --> 00:55:43.940]   Yes, exactly.
[00:55:43.940 --> 00:55:44.940]   More of its on device.
[00:55:44.940 --> 00:55:46.180]   You get a little bit of the fast response.
[00:55:46.180 --> 00:55:47.180]   So what?
[00:55:47.180 --> 00:55:48.980]   It like reverts to old dumb assistant.
[00:55:48.980 --> 00:55:51.300]   Like, oh, you're not using.
[00:55:51.300 --> 00:55:55.220]   You just got the foot and not the butler.
[00:55:55.220 --> 00:55:56.220]   Yes.
[00:55:56.220 --> 00:56:00.620]   So this is a trend I think we're going to see more and more, especially as some people
[00:56:00.620 --> 00:56:01.940]   have G Suite either.
[00:56:01.940 --> 00:56:02.940]   Right.
[00:56:02.940 --> 00:56:04.940]   So yes, but go cycle one.
[00:56:04.940 --> 00:56:05.940]   That's for you, Jeff.
[00:56:05.940 --> 00:56:06.940]   Yeah.
[00:56:06.940 --> 00:56:10.540]   Not only that, like you can't be you can't have it on your device at all.
[00:56:10.540 --> 00:56:14.300]   It's not that you can't have G Suite account as your primary account.
[00:56:14.300 --> 00:56:15.620]   It just can't be there at all.
[00:56:15.620 --> 00:56:18.460]   It can't be there at all, which is just what's cool.
[00:56:18.460 --> 00:56:19.460]   Weird.
[00:56:19.460 --> 00:56:20.940]   These are the people who pay you.
[00:56:20.940 --> 00:56:22.380]   That is so weird.
[00:56:22.380 --> 00:56:23.380]   Yeah.
[00:56:23.380 --> 00:56:28.140]   So it's probably to do with like the the limitations around G Suite are mostly about
[00:56:28.140 --> 00:56:32.940]   things like privacy and parsing the data.
[00:56:32.940 --> 00:56:37.540]   So it could be the way that Google's handling that data is not compliant with what they've
[00:56:37.540 --> 00:56:41.900]   told their professional customers who want their data to be kept secure.
[00:56:41.900 --> 00:56:43.620]   It's not for folks like you and me, Jeff.
[00:56:43.620 --> 00:56:46.260]   It's for folks like corporations.
[00:56:46.260 --> 00:56:48.420]   If that makes sense, I know, but they're Google.
[00:56:48.420 --> 00:56:50.020]   They have the quantum supremacy.
[00:56:50.020 --> 00:56:51.020]   They could fix this.
[00:56:51.020 --> 00:56:52.020]   Yes.
[00:56:52.020 --> 00:56:54.540]   I want so badly like I recommend to everyone.
[00:56:54.540 --> 00:56:55.540]   They're like, should I do G Suite?
[00:56:55.540 --> 00:56:57.100]   And I'm like, are you a Google user?
[00:56:57.100 --> 00:56:58.100]   No, don't.
[00:56:58.100 --> 00:57:01.540]   There's no point in it.
[00:57:01.540 --> 00:57:06.660]   What other option would you suggest for a professional who wants to have like a small
[00:57:06.660 --> 00:57:08.220]   but high end productivity suite?
[00:57:08.220 --> 00:57:10.500]   Please don't say Microsoft.
[00:57:10.500 --> 00:57:13.380]   Yeah, that's about it.
[00:57:13.380 --> 00:57:14.860]   Good question.
[00:57:14.860 --> 00:57:15.860]   That's about it.
[00:57:15.860 --> 00:57:16.860]   It's about it.
[00:57:16.860 --> 00:57:20.820]   You're like, be if you're on a Chromebook and you want to have your own domain.
[00:57:20.820 --> 00:57:21.820]   Yeah.
[00:57:21.820 --> 00:57:23.340]   That's about it.
[00:57:23.340 --> 00:57:25.940]   I can't think of anything else.
[00:57:25.940 --> 00:57:27.860]   That would be seamless day to day.
[00:57:27.860 --> 00:57:33.180]   It's just kind of baffling to me how the G Suite over the years is always the punching
[00:57:33.180 --> 00:57:34.700]   bag for stuff like this.
[00:57:34.700 --> 00:57:35.700]   Never fixed this.
[00:57:35.700 --> 00:57:37.860]   Like continually the punching bag.
[00:57:37.860 --> 00:57:40.300]   It never gets any better for the G Suite users.
[00:57:40.300 --> 00:57:41.980]   Meanwhile, they're the ones that are paying.
[00:57:41.980 --> 00:57:44.740]   And I know the chatroom is off the same rope.
[00:57:44.740 --> 00:57:45.740]   Yeah.
[00:57:45.740 --> 00:57:51.340]   Yeah, that's, I mean, that's really the only other option, which is, I'm sorry, I'm not
[00:57:51.340 --> 00:57:52.340]   doing it.
[00:57:52.340 --> 00:57:55.420]   I'm just being cool.
[00:57:55.420 --> 00:58:02.980]   So I mean, and that's not all that's missing from the four RCS support, which the Pixel
[00:58:02.980 --> 00:58:03.980]   3 has.
[00:58:03.980 --> 00:58:04.980]   What?
[00:58:04.980 --> 00:58:09.300]   The Pixel 4 is not is not unsupported on T-Mobile and Verizon at launch, even though
[00:58:09.300 --> 00:58:10.580]   the 3 is supported.
[00:58:10.580 --> 00:58:17.900]   I didn't quite understand that story because I thought it also put more on us on the particular
[00:58:17.900 --> 00:58:20.380]   carriers, not necessarily the device.
[00:58:20.380 --> 00:58:22.380]   So it's more on Google?
[00:58:22.380 --> 00:58:24.740]   No, it's about the carrier.
[00:58:24.740 --> 00:58:25.740]   It is about the carriers.
[00:58:25.740 --> 00:58:26.740]   Absolutely.
[00:58:26.740 --> 00:58:32.700]   Because the headlines that I've been seeing have read as if this is Google's fault.
[00:58:32.700 --> 00:58:35.500]   They screwed up and didn't make this available.
[00:58:35.500 --> 00:58:40.660]   I mean, I always thought it mattered most on the network, not necessarily on the car.
[00:58:40.660 --> 00:58:44.860]   You could say that if this was a big priority for Google, a big enough priority for Google,
[00:58:44.860 --> 00:58:50.380]   Google would have made whatever relationship or management that they needed to do with
[00:58:50.380 --> 00:58:51.700]   the carriers to make sure.
[00:58:51.700 --> 00:58:55.980]   So the carriers, they could say this is not a device that's going to sell in the billion.
[00:58:55.980 --> 00:58:56.980]   So who cares?
[00:58:56.980 --> 00:58:58.620]   So why do they care?
[00:58:58.620 --> 00:59:00.220]   Yeah, there's that.
[00:59:00.220 --> 00:59:08.100]   I mean, which is unfortunate for Google's next big market, our messaging push, you know,
[00:59:08.100 --> 00:59:12.620]   because RCS was yet another messaging solution that was going to change everything.
[00:59:12.620 --> 00:59:14.620]   Ooh, who's storing his stuff?
[00:59:14.620 --> 00:59:17.860]   It's on the plus side, you guys.
[00:59:17.860 --> 00:59:22.140]   It's been what, six months or so since Google's last launch to messaging protocol.
[00:59:22.140 --> 00:59:23.140]   That's true.
[00:59:23.140 --> 00:59:24.140]   You know.
[00:59:24.140 --> 00:59:25.140]   Wait a minute.
[00:59:25.140 --> 00:59:26.140]   Look at the right side.
[00:59:26.140 --> 00:59:27.140]   I'm in light mode today.
[00:59:27.140 --> 00:59:30.660]   We know I'm in dark mode, never mind.
[00:59:30.660 --> 00:59:33.580]   We can automatically switch between light and dark mode throughout the course of this
[00:59:33.580 --> 00:59:34.580]   episode.
[00:59:34.580 --> 00:59:36.700]   We'll put you back into dark mode for this.
[00:59:36.700 --> 00:59:41.540]   Dual band GPS is hardware that's supported on the Pixel 4, but it's not activated at
[00:59:41.540 --> 00:59:42.540]   launch.
[00:59:42.540 --> 00:59:46.300]   So that's another thing that Google is kind of holding off and say, "Ah, somewhere down
[00:59:46.300 --> 00:59:48.260]   the line will activate it."
[00:59:48.260 --> 00:59:53.580]   Just point after point after point, it seems like there's all of these asterisks involved
[00:59:53.580 --> 00:59:55.780]   with the Pixel 4.
[00:59:55.780 --> 01:00:00.260]   And I feel like in years past, yes, people poo poo the next device and then they get
[01:00:00.260 --> 01:00:01.900]   in like, "Oh, well, actually it's not that bad."
[01:00:01.900 --> 01:00:05.900]   And maybe I'll find myself there too, but I'm just getting kind of disheartened by a
[01:00:05.900 --> 01:00:08.060]   lot of these things right now.
[01:00:08.060 --> 01:00:10.500]   And maybe that's because I just don't have it in my hands yet.
[01:00:10.500 --> 01:00:11.860]   Well, that's the difference.
[01:00:11.860 --> 01:00:15.660]   You need to see what it works for you and your personal experience on it.
[01:00:15.660 --> 01:00:22.060]   All of these reviews that have been out there the last couple of days have been pretty downing.
[01:00:22.060 --> 01:00:26.340]   And I get it for the most part, but I don't want to just put all of my stock in those
[01:00:26.340 --> 01:00:32.860]   just yet because like, for example, MKBHD as big as he is, he has a different perspective
[01:00:32.860 --> 01:00:37.900]   when it comes to these devices and a use case.
[01:00:37.900 --> 01:00:43.500]   I remember him making a comment right at the end of his video, basically saying, "Most
[01:00:43.500 --> 01:00:46.660]   people, I wouldn't recommend this phone for."
[01:00:46.660 --> 01:00:51.660]   And so I wanted to ask him, "How do you define most people and what is the phone that you
[01:00:51.660 --> 01:00:52.660]   want to recommend?"
[01:00:52.660 --> 01:00:53.660]   What is the cut off?
[01:00:53.660 --> 01:00:54.660]   Yeah.
[01:00:54.660 --> 01:00:55.660]   You know, is it the iPhone?
[01:00:55.660 --> 01:00:56.660]   Okay.
[01:00:56.660 --> 01:00:57.660]   Why?
[01:00:57.660 --> 01:00:58.660]   Is it the Galaxy Note?
[01:00:58.660 --> 01:00:59.660]   Okay.
[01:00:59.660 --> 01:01:00.660]   Why?
[01:01:00.660 --> 01:01:01.660]   Sure.
[01:01:01.660 --> 01:01:05.660]   You know, but I'm coming from someone who has the reach and the respect in the industry,
[01:01:05.660 --> 01:01:09.980]   you know, as MKBHD does.
[01:01:09.980 --> 01:01:12.380]   And he's kind of been a pretty big fan.
[01:01:12.380 --> 01:01:13.380]   Yeah, that's a pixel phone.
[01:01:13.380 --> 01:01:14.380]   He has.
[01:01:14.380 --> 01:01:15.380]   He has.
[01:01:15.380 --> 01:01:18.820]   So for him to say, like, "I have a hard time recommending this for everyone."
[01:01:18.820 --> 01:01:19.820]   That's kind of a big deal.
[01:01:19.820 --> 01:01:20.820]   That's a big deal.
[01:01:20.820 --> 01:01:21.820]   No, my mind.
[01:01:21.820 --> 01:01:27.580]   And see when he went into the rant about the 90 Hertz display, and then you top that
[01:01:27.580 --> 01:01:32.940]   off by saying, "Most people, do you really think most people know what 90 Hertz on a display
[01:01:32.940 --> 01:01:33.940]   is?"
[01:01:33.940 --> 01:01:38.580]   Most people probably aren't even going to know that the 90 Hertz display is going on
[01:01:38.580 --> 01:01:39.580]   or going off.
[01:01:39.580 --> 01:01:40.580]   Right.
[01:01:40.580 --> 01:01:42.580]   The brightness setting, whatever that is.
[01:01:42.580 --> 01:01:43.580]   Right.
[01:01:43.580 --> 01:01:46.860]   So that's why I'm like, "Why wouldn't he recommend it for most people?"
[01:01:46.860 --> 01:01:48.940]   You know, just for little comments like that.
[01:01:48.940 --> 01:01:51.740]   But I'm not taking the dig at you, Mr. Brownlee.
[01:01:51.740 --> 01:01:55.060]   I'm just curious if you can get this message.
[01:01:55.060 --> 01:01:59.340]   I just want to say, to wrap this up, I love my Pixel 4.
[01:01:59.340 --> 01:02:02.060]   I think it got a lot of heat undeservedly.
[01:02:02.060 --> 01:02:03.420]   I think it's a great little phone.
[01:02:03.420 --> 01:02:09.020]   And as of just a few weeks ago, Ghoul's announced they're going to continually add new features
[01:02:09.020 --> 01:02:10.620]   with monthly updates.
[01:02:10.620 --> 01:02:17.300]   So I have to say, as much as we've been dissing the Pixel 4, I'm pretty happy with it.
[01:02:17.300 --> 01:02:22.340]   But that's just one of the many stories we'll be covering next year on this week in Google.
[01:02:22.340 --> 01:02:24.980]   We do it every Wednesday, and I hope you'll stop by.
[01:02:24.980 --> 01:02:29.180]   On behalf of Jeff Jarvis at Stacey Higabotham, I'm Leo LaPorte.
[01:02:29.180 --> 01:02:36.380]   Have a wonderful holiday, and here's to a very pleasant, peaceful, and googly 2020.
[01:02:36.380 --> 01:02:38.020]   We'll see you next year.
[01:02:38.020 --> 01:02:38.380]   Bye-bye.
[01:02:38.380 --> 01:02:48.380]   [MUSIC]
[01:02:48.380 --> 01:02:50.960]   (bell dinging)

