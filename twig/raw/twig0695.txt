;FFMETADATA1
title=The Airing of Grievances
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-12-22
track=695
language=English
genre=Podcast
comment=Point-E, Public Domain day 2023, IoT and consent, John Carmack
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.080]   It's time for Twig this week in Google our last episode of 2022.
[00:00:06.080 --> 00:00:08.240]   We've got the Festivus poll already.
[00:00:08.240 --> 00:00:14.240]   We're going to celebrate the advent of winter and talk about, well, everything except Elon.
[00:00:14.240 --> 00:00:17.360]   I feel sorry for him. If I mention that it's coming up next.
[00:00:17.360 --> 00:00:20.720]   This week in Google with Stacy and Jeff stay here.
[00:00:20.720 --> 00:00:24.720]   Podcasts you love.
[00:00:24.720 --> 00:00:26.400]   From people you trust.
[00:00:26.400 --> 00:00:29.200]   This is Twig.
[00:00:29.200 --> 00:00:43.680]   This week in Google, episode 695 recorded Wednesday, December 21st, 2022.
[00:00:43.680 --> 00:00:45.360]   The airing of grievances.
[00:00:45.360 --> 00:00:49.840]   This episode of This Week in Google is brought to you by Fastmail.
[00:00:49.840 --> 00:00:55.520]   Reclaim your privacy, boost productivity and make email yours with Fastmail.
[00:00:55.520 --> 00:01:00.160]   Try it free for 30 days. Fastmail.com/twit.
[00:01:00.160 --> 00:01:08.240]   Fastmail is also giving Twit listeners a 15% discount on the first year when you sign up today at fastmail.com/twit.
[00:01:08.240 --> 00:01:13.360]   Thanks for listening to this show as an ad supported network.
[00:01:13.360 --> 00:01:18.560]   We are always looking for new partners with products and services that will benefit our
[00:01:18.560 --> 00:01:21.760]   qualified audience. Are you ready to grow your business?
[00:01:21.760 --> 00:01:26.000]   Reach out to advertise at twit.tv and launch your campaign now.
[00:01:26.000 --> 00:01:33.200]   It's time for Twig this week in Google. The show where we talk about Google,
[00:01:33.200 --> 00:01:38.720]   Twitter, Facebook, Elon, Twitter, Facebook, Google and all of the other stuff.
[00:01:38.720 --> 00:01:43.600]   Stacy Higginbotham is here but really we're on a short leash today.
[00:01:43.600 --> 00:01:47.920]   At any moment she can just throw up her hands and leave.
[00:01:47.920 --> 00:01:50.000]   Or just throw up.
[00:01:50.000 --> 00:01:50.880]   Or just throw up.
[00:01:50.880 --> 00:01:51.760]   Oh either way.
[00:01:51.760 --> 00:01:52.320]   Either way.
[00:01:52.320 --> 00:01:55.120]   So thrilled to see you. Last show of the year Stacy.
[00:01:55.120 --> 00:02:00.080]   Your long nightmare is almost over. Two weeks off, we'll be gone next week.
[00:02:00.080 --> 00:02:04.960]   We'll do the best of and then we'll be back January 4th and it starts all over again.
[00:02:04.960 --> 00:02:10.240]   Jeff Jarvis is here. He's the Leonard Taop Professor for journalistic innovation at the Craig Newmark
[00:02:10.240 --> 00:02:15.520]   Graduate School of Journalism at the City University of New York. Hello Mr. Jarvis.
[00:02:15.520 --> 00:02:17.280]   Hello, boss. How are you?
[00:02:17.280 --> 00:02:20.880]   Great. Jeff's going to be with us for our special holiday Christmas
[00:02:20.880 --> 00:02:29.200]   Twit. I brought in all the old timers. So it's you, Steve Gibson, Paul Farat and Doc Serals.
[00:02:29.200 --> 00:02:33.200]   Old white men yelling at Christmas. It's going to be a great episode.
[00:02:33.200 --> 00:02:34.640]   Paul's grumpier than I knew.
[00:02:34.640 --> 00:02:35.920]   Oh yeah, he's super grumpy.
[00:02:35.920 --> 00:02:37.840]   I don't realize quite.
[00:02:37.840 --> 00:02:41.520]   He's funny grumpy. It's good grumpy. It's amusing and entertaining grumpy.
[00:02:41.520 --> 00:02:42.560]   You know, he's always cheerful.
[00:02:42.560 --> 00:02:44.400]   He comes out differently to Windows Weekly.
[00:02:44.400 --> 00:02:45.520]   Yeah, no it doesn't.
[00:02:45.520 --> 00:02:49.440]   No it doesn't. You know, he's always cheerful.
[00:02:49.440 --> 00:02:52.640]   I just don't understand what he's grumpy about on Windows Weekly. I guess that's the difference.
[00:02:52.640 --> 00:02:54.720]   Yeah, that's because he uses Windows. He'd be grumpy too.
[00:02:54.720 --> 00:03:01.200]   Mr. Ann Pruitt is here. He is ready. He's got a cigar jammed in his mouth.
[00:03:01.200 --> 00:03:05.680]   He's got his hat jammed on his head and he's ready.
[00:03:05.680 --> 00:03:09.280]   It's supposed to be a hat, but it's actually just a stocking.
[00:03:09.280 --> 00:03:12.160]   Oh, somebody sent you a stocking full of cigars, which is pretty cool.
[00:03:12.720 --> 00:03:16.560]   You know, if you had a little gaffers tape, you could just detach that to your head.
[00:03:16.560 --> 00:03:18.640]   Oh, speaking of gaffers tape.
[00:03:18.640 --> 00:03:20.640]   Look, there is a roll.
[00:03:20.640 --> 00:03:22.640]   Now that's my ducks.
[00:03:22.640 --> 00:03:22.880]   Oh my gosh, Timmy.
[00:03:22.880 --> 00:03:23.760]   How's it around?
[00:03:23.760 --> 00:03:26.000]   Don't use always gaffers tape on my desk.
[00:03:26.000 --> 00:03:27.520]   Gaffers tape is the greatest stuff.
[00:03:27.520 --> 00:03:28.880]   Just maybe we'll figure this out.
[00:03:28.880 --> 00:03:30.240]   Just put it right in the middle of the ground, Gabe.
[00:03:30.240 --> 00:03:30.560]   Yeah.
[00:03:30.560 --> 00:03:36.560]   So we lead off with this terrible news from the subreddit.
[00:03:36.560 --> 00:03:40.880]   Data is beautiful. They like to do graphs about data.
[00:03:40.880 --> 00:03:47.040]   They do all sorts of stuff. This is life expectancy by occupation for people with a
[00:03:47.040 --> 00:03:49.760]   Wikipedia page. Let me make this a little bit bigger.
[00:03:49.760 --> 00:03:53.760]   Of course, right at the top, longest life expectancy billionaires,
[00:03:53.760 --> 00:03:55.920]   because they don't have to deal with the day-to-day crap.
[00:03:55.920 --> 00:03:57.760]   The rest of us do, right?
[00:03:57.760 --> 00:04:03.600]   Then biochemists, I don't know why, conservationists, geneticists, neuroscientists, archivists.
[00:04:03.600 --> 00:04:06.720]   Well, they're all cheating doing something to their genes and their biology.
[00:04:06.720 --> 00:04:11.440]   Maybe you need Ist in your entitled to sociologist.
[00:04:11.440 --> 00:04:12.800]   They don't do it if you're a sociologist.
[00:04:12.800 --> 00:04:17.680]   Statisticians, psychologists, yeah, this is all people who don't do anything.
[00:04:17.680 --> 00:04:19.280]   Philanthropists, psychoanalysts, what are they doing?
[00:04:19.280 --> 00:04:21.360]   They sit there and listen to you.
[00:04:21.360 --> 00:04:22.240]   They sit on a couch.
[00:04:22.240 --> 00:04:23.200]   Business women.
[00:04:23.200 --> 00:04:28.880]   There is, as far as I think the other was no business men category.
[00:04:28.880 --> 00:04:30.720]   That's because the men is understood.
[00:04:30.720 --> 00:04:31.440]   It's understood.
[00:04:31.440 --> 00:04:33.200]   This is the, that's what makes you unique.
[00:04:33.200 --> 00:04:36.000]   Meteorologist, microbiologist.
[00:04:36.000 --> 00:04:37.600]   Now we're going to go to the very bottom.
[00:04:37.600 --> 00:04:42.000]   These are the people who have short life expectancy.
[00:04:42.000 --> 00:04:44.240]   As you might expect, drummers right on the top there.
[00:04:44.240 --> 00:04:46.720]   Nationalists?
[00:04:46.720 --> 00:04:47.520]   Nationalists.
[00:04:47.520 --> 00:04:50.080]   Explorers.
[00:04:50.080 --> 00:04:51.200]   This is on Wikipedia.
[00:04:51.200 --> 00:04:52.080]   That makes sense.
[00:04:52.080 --> 00:04:52.800]   Yeah, explorers.
[00:04:52.800 --> 00:04:54.960]   Yeah, of course they run into lions and stuff.
[00:04:54.960 --> 00:04:56.480]   Dissidence, they don't do well.
[00:04:56.480 --> 00:04:57.920]   Soldiers,
[00:04:57.920 --> 00:05:03.120]   guitarists, which, this surprised me because I thought drummers certainly would go faster than
[00:05:03.120 --> 00:05:04.400]   guitarists, but no.
[00:05:04.400 --> 00:05:05.920]   No, it's not very far apart.
[00:05:05.920 --> 00:05:07.040]   The drugs are about to say.
[00:05:07.040 --> 00:05:08.320]   Spies.
[00:05:08.320 --> 00:05:11.040]   That's a bad habit.
[00:05:11.040 --> 00:05:12.480]   Spies, they go fast.
[00:05:12.480 --> 00:05:14.080]   You know, spies is probably less.
[00:05:14.080 --> 00:05:15.520]   They just don't get on Wikipedia.
[00:05:15.520 --> 00:05:16.400]   Ah, they're good.
[00:05:16.400 --> 00:05:16.960]   They probably die on the earth.
[00:05:16.960 --> 00:05:18.080]   These are the bad spies.
[00:05:18.080 --> 00:05:18.880]   They should say that.
[00:05:18.880 --> 00:05:19.200]   Bad spies.
[00:05:19.200 --> 00:05:20.000]   These are the bad spies.
[00:05:20.000 --> 00:05:21.360]   These are the ones that got caught.
[00:05:21.360 --> 00:05:23.040]   Princesses.
[00:05:23.040 --> 00:05:23.920]   They're very low.
[00:05:23.920 --> 00:05:27.200]   But I think princesses probably is skewed a little bit by Diana, right?
[00:05:27.200 --> 00:05:30.240]   Keyboard is worse than guitarist, and drummers.
[00:05:31.120 --> 00:05:33.120]   But you know, who's worse than keyboardist, princes.
[00:05:33.120 --> 00:05:35.760]   And then royalty in general.
[00:05:35.760 --> 00:05:36.480]   Very poor.
[00:05:36.480 --> 00:05:36.960]   Very poor.
[00:05:36.960 --> 00:05:39.360]   Revolutionaries.
[00:05:39.360 --> 00:05:40.960]   And then, wait a minute, what's that?
[00:05:40.960 --> 00:05:41.920]   Podcasters?
[00:05:41.920 --> 00:05:43.440]   Oh no.
[00:05:43.440 --> 00:05:44.240]   Oh no.
[00:05:44.240 --> 00:05:48.000]   We're, the only people who have lived your time already.
[00:05:48.000 --> 00:05:52.400]   The only people who have worked life expected to see are kickboxers and wrappers.
[00:05:52.400 --> 00:05:54.160]   Oh my gosh.
[00:05:54.160 --> 00:05:56.880]   So what's the actual median age of the four of us?
[00:05:57.760 --> 00:06:00.960]   We're way, you know, I mean, look, it looked like most podcasters.
[00:06:00.960 --> 00:06:01.440]   Push it up.
[00:06:01.440 --> 00:06:02.960]   Don't even get to 48.
[00:06:02.960 --> 00:06:05.920]   So yeah, obviously you and I are.
[00:06:05.920 --> 00:06:06.640]   44.
[00:06:06.640 --> 00:06:08.400]   So watch out, Stacey.
[00:06:08.400 --> 00:06:09.920]   I'm in trouble.
[00:06:09.920 --> 00:06:10.640]   I'm in trouble.
[00:06:10.640 --> 00:06:12.640]   You should switch your career to billionaires quickly as you can.
[00:06:12.640 --> 00:06:15.280]   Well, I, you know, technically I'm a journalist.
[00:06:15.280 --> 00:06:15.760]   I'm a journalist.
[00:06:15.760 --> 00:06:16.160]   Yeah.
[00:06:16.160 --> 00:06:17.840]   Our journalists aren't even on this list.
[00:06:17.840 --> 00:06:19.120]   We aren't even on here.
[00:06:19.120 --> 00:06:19.280]   No.
[00:06:19.280 --> 00:06:22.080]   Why are journalists not?
[00:06:22.080 --> 00:06:22.960]   Maybe you're in the middle.
[00:06:22.960 --> 00:06:24.240]   You're in the middle somewhere.
[00:06:24.240 --> 00:06:24.560]   Yeah.
[00:06:24.560 --> 00:06:25.120]   Have to be.
[00:06:25.120 --> 00:06:25.520]   We're more profitable.
[00:06:25.520 --> 00:06:27.120]   Journalists have wikis.
[00:06:27.120 --> 00:06:27.360]   Yeah.
[00:06:27.360 --> 00:06:28.000]   Some of them do.
[00:06:28.000 --> 00:06:29.440]   No, you see the dot dot dot.
[00:06:29.440 --> 00:06:32.400]   That's all the, that's the fly over life.
[00:06:32.400 --> 00:06:33.200]   Expectancies.
[00:06:33.200 --> 00:06:33.440]   Yeah.
[00:06:33.440 --> 00:06:35.920]   That's why people don't matter.
[00:06:35.920 --> 00:06:37.440]   They're gone.
[00:06:37.440 --> 00:06:38.000]   We don't care.
[00:06:38.000 --> 00:06:39.040]   They're journaling.
[00:06:39.040 --> 00:06:40.240]   Can you believe podcasters?
[00:06:40.240 --> 00:06:43.440]   I saw this and this is not made up.
[00:06:43.440 --> 00:06:45.840]   You would think this is, but no, this is data is beautiful.
[00:06:45.840 --> 00:06:47.120]   These are data scientists.
[00:06:47.120 --> 00:06:52.320]   They scrape the data from Wikipedia and podcasters are right down there at the bottom.
[00:06:52.320 --> 00:06:56.160]   But I wonder what is this based on?
[00:06:56.160 --> 00:06:56.800]   Is this like?
[00:06:56.800 --> 00:06:57.760]   Who is skewing us?
[00:06:57.760 --> 00:06:58.800]   Who are the famous?
[00:06:58.800 --> 00:06:59.920]   Is it violence?
[00:06:59.920 --> 00:07:00.880]   Why?
[00:07:00.880 --> 00:07:01.280]   Why?
[00:07:01.280 --> 00:07:02.720]   You're saying we're right.
[00:07:02.720 --> 00:07:03.120]   Yeah.
[00:07:03.120 --> 00:07:03.680]   Right.
[00:07:03.680 --> 00:07:06.240]   Well, there's a lot of comedians who are podcasters.
[00:07:06.240 --> 00:07:06.640]   No, they don't.
[00:07:06.640 --> 00:07:08.240]   Those people can do a lot of drugs.
[00:07:08.240 --> 00:07:10.320]   They're usually very sad, unfortunately.
[00:07:10.320 --> 00:07:11.920]   It's a good thing we're webcasters.
[00:07:11.920 --> 00:07:13.680]   Netcasters.
[00:07:13.680 --> 00:07:14.240]   Nesters.
[00:07:14.240 --> 00:07:14.720]   Nesters.
[00:07:14.720 --> 00:07:15.280]   Netcasters.
[00:07:15.280 --> 00:07:15.760]   Oh, nesters.
[00:07:15.760 --> 00:07:16.320]   Get it.
[00:07:16.320 --> 00:07:18.240]   I messed up the punchline.
[00:07:18.240 --> 00:07:18.720]   Jesus.
[00:07:18.720 --> 00:07:22.960]   It's probably, there's probably a good reason for this.
[00:07:22.960 --> 00:07:25.600]   You know, it's a newer profession.
[00:07:26.400 --> 00:07:30.000]   Anybody who's died and it died young because, you know,
[00:07:30.000 --> 00:07:32.880]   how long could you've been a podcaster for or something?
[00:07:32.880 --> 00:07:37.360]   Well, I want to know YouTubers because those people do some crazy stuff.
[00:07:37.360 --> 00:07:38.080]   Yeah.
[00:07:38.080 --> 00:07:38.400]   Yeah.
[00:07:38.400 --> 00:07:41.120]   Did you see the article about Logan Paul?
[00:07:41.120 --> 00:07:43.040]   Scammer?
[00:07:43.040 --> 00:07:44.320]   He does.
[00:07:44.320 --> 00:07:50.080]   It's a, he, you know, he's a, Jeff, you know, who Logan Paul is because you follow all this
[00:07:50.080 --> 00:07:51.280]   fairly con stuff.
[00:07:51.280 --> 00:07:52.640]   Yeah, some, but not much.
[00:07:52.640 --> 00:07:53.360]   I don't look like a pod.
[00:07:53.360 --> 00:07:54.640]   I'll mix him up with his brother.
[00:07:54.640 --> 00:07:55.680]   Yeah.
[00:07:56.560 --> 00:07:57.600]   Logan Paul, Logan?
[00:07:57.600 --> 00:07:58.560]   No.
[00:07:58.560 --> 00:08:02.160]   The other Paul, but Logan Paul,
[00:08:02.160 --> 00:08:12.160]   does, you know, like he, I think he's the one who said the mattress on fire.
[00:08:12.160 --> 00:08:14.000]   They were all living in a room.
[00:08:14.000 --> 00:08:14.480]   Like Stunt.
[00:08:14.480 --> 00:08:15.120]   Videos.
[00:08:15.120 --> 00:08:15.440]   Yeah.
[00:08:15.440 --> 00:08:15.840]   It's stunt.
[00:08:15.840 --> 00:08:16.240]   Videos.
[00:08:16.240 --> 00:08:16.640]   It's stunt.
[00:08:16.640 --> 00:08:17.200]   Yeah.
[00:08:17.200 --> 00:08:17.520]   Yeah.
[00:08:17.520 --> 00:08:18.080]   It's stunt.
[00:08:18.080 --> 00:08:19.760]   And, uh,
[00:08:19.760 --> 00:08:20.960]   Apparently, wrestler.
[00:08:20.960 --> 00:08:22.160]   Here it is.
[00:08:22.160 --> 00:08:26.240]   How influencers hype crypto without disclosing their financial ties.
[00:08:26.240 --> 00:08:27.520]   Logan Paul's one of them.
[00:08:27.520 --> 00:08:30.000]   It's like, I don't know who took this picture.
[00:08:30.000 --> 00:08:31.200]   That's picture of me.
[00:08:31.200 --> 00:08:33.120]   That, that, that, that,
[00:08:33.120 --> 00:08:37.360]   Logan Paul, a 27 year old boxer and social media influencer.
[00:08:37.360 --> 00:08:40.560]   Uh, but then I'm really curious who this guy is.
[00:08:40.560 --> 00:08:41.520]   Is, is he a journalist?
[00:08:41.520 --> 00:08:44.640]   Maybe he's one of those short lived podcasters you hear about.
[00:08:44.640 --> 00:08:46.560]   It's a strangest picture.
[00:08:46.560 --> 00:08:47.920]   It looks like they're on wall street.
[00:08:47.920 --> 00:08:48.640]   They're just talking.
[00:08:48.640 --> 00:08:49.360]   That's what happens.
[00:08:49.360 --> 00:08:49.360]   Yeah.
[00:08:49.360 --> 00:08:50.000]   You rear end.
[00:08:51.120 --> 00:08:52.480]   It's very strange.
[00:08:52.480 --> 00:08:54.640]   Anyway, uh, enough of that.
[00:08:54.640 --> 00:08:56.640]   We don't need to go into Logan Paul.
[00:08:56.640 --> 00:08:59.920]   Uh, but you have to get another NFT rug pull, which he swore.
[00:08:59.920 --> 00:09:00.960]   This is not a rug pull.
[00:09:00.960 --> 00:09:02.400]   But, but you know what?
[00:09:02.400 --> 00:09:05.120]   NFTs at this point, I mean, look at the Trump NFTs.
[00:09:05.120 --> 00:09:05.920]   It's obvious.
[00:09:05.920 --> 00:09:07.680]   It's, they're just scared.
[00:09:07.680 --> 00:09:09.680]   They're grifts all of them at this point, right?
[00:09:09.680 --> 00:09:15.040]   I was out to say that's, that's something to give you a reason to give them money.
[00:09:15.040 --> 00:09:15.360]   Yeah.
[00:09:15.360 --> 00:09:16.080]   That's all it is.
[00:09:16.080 --> 00:09:16.720]   That's all it is.
[00:09:16.720 --> 00:09:17.280]   That's all it is.
[00:09:17.280 --> 00:09:18.160]   Money laundering.
[00:09:18.160 --> 00:09:18.400]   Yeah.
[00:09:19.840 --> 00:09:21.840]   So Stacy put a story in here.
[00:09:21.840 --> 00:09:22.640]   So we're going to do it.
[00:09:22.640 --> 00:09:26.560]   Stacy is fascinated by artificial intelligence as we all are.
[00:09:26.560 --> 00:09:30.960]   Stable diffusion, uh, chat GPT, Dali too.
[00:09:30.960 --> 00:09:31.760]   Well, there's a new one.
[00:09:31.760 --> 00:09:36.880]   The folks who gave you chat GPT have now done a new AI.
[00:09:36.880 --> 00:09:38.720]   This from open AI point E.
[00:09:38.720 --> 00:09:43.440]   An AI that generates three D models.
[00:09:43.440 --> 00:09:45.440]   Okay.
[00:09:45.440 --> 00:09:48.400]   This isn't actually my story, but this is an interesting story.
[00:09:48.400 --> 00:09:48.560]   No.
[00:09:48.560 --> 00:09:48.960]   All right.
[00:09:49.440 --> 00:09:50.400]   I thought it, wait a minute.
[00:09:50.400 --> 00:09:51.040]   I'm like,
[00:09:51.040 --> 00:09:52.960]   Oh, the one above the Roomba.
[00:09:52.960 --> 00:09:53.520]   Okay.
[00:09:53.520 --> 00:09:54.640]   We got some good stories.
[00:09:54.640 --> 00:09:55.040]   It's okay.
[00:09:55.040 --> 00:09:57.120]   We could, we can talk about point E.
[00:09:57.120 --> 00:09:59.440]   I mean, the room kind of ties to the MSG stuff.
[00:09:59.440 --> 00:09:59.600]   Yeah.
[00:09:59.600 --> 00:10:00.000]   We'll get.
[00:10:00.000 --> 00:10:00.160]   Yeah.
[00:10:00.160 --> 00:10:02.080]   The MSG story really pissed me off.
[00:10:02.080 --> 00:10:03.360]   But anyway, we'll get to that in a second.
[00:10:03.360 --> 00:10:10.720]   Um, so I, I think you're going to see now suddenly this explosion of AI applied to everything.
[00:10:10.720 --> 00:10:12.800]   3D models.
[00:10:12.800 --> 00:10:16.000]   Let's talk about the recursive nature of all this.
[00:10:16.000 --> 00:10:21.600]   Cause I'm fascinated by what GPT chat can do and can't do.
[00:10:21.600 --> 00:10:28.000]   And the limitations of some of the like stable diffusion,
[00:10:28.000 --> 00:10:31.280]   Lindsay, all of these, um, because, you know,
[00:10:31.280 --> 00:10:34.480]   basically because something is trained on all this existing data.
[00:10:34.480 --> 00:10:38.720]   And it's pretty much regurgitating bits of that.
[00:10:38.720 --> 00:10:41.680]   The argument is like, oh, does it create something new?
[00:10:41.680 --> 00:10:42.160]   I don't know.
[00:10:43.120 --> 00:10:49.840]   It does. But how do we create truly new things if we're building on top of this?
[00:10:49.840 --> 00:10:52.800]   And this kind of gets to the artist's rights, but I'm, I'm really just like,
[00:10:52.800 --> 00:11:00.640]   fascinated by again, like, if we start generating all of this and create so much
[00:11:00.640 --> 00:11:04.240]   copy that is AI generated or art that's age I generated.
[00:11:04.240 --> 00:11:07.760]   Does it become a word?
[00:11:07.760 --> 00:11:10.960]   I think the world becomes less valuable over time.
[00:11:10.960 --> 00:11:12.000]   More nonsensical.
[00:11:12.000 --> 00:11:14.800]   Well, I think when you don't put anything new into the system.
[00:11:14.800 --> 00:11:15.440]   Oh, just too.
[00:11:15.440 --> 00:11:19.120]   There was a story out last week that said that we're going to run out of data
[00:11:19.120 --> 00:11:20.720]   to train the AI, which I think is absurd.
[00:11:20.720 --> 00:11:23.280]   Um, um, but I think, I think you're right.
[00:11:23.280 --> 00:11:26.720]   Stacy, I think I was writing a post today that's not done, but, but I think that
[00:11:26.720 --> 00:11:29.040]   content becomes just simply less valuable.
[00:11:29.040 --> 00:11:32.320]   Cause anybody can make content because you can go on a machine and make content.
[00:11:32.320 --> 00:11:34.560]   It becomes whatever to your point.
[00:11:34.560 --> 00:11:35.600]   Good content.
[00:11:35.600 --> 00:11:36.320]   Good content.
[00:11:36.320 --> 00:11:36.560]   Okay.
[00:11:36.560 --> 00:11:36.880]   Sorry.
[00:11:36.880 --> 00:11:37.760]   Becomes more value.
[00:11:37.760 --> 00:11:38.000]   Or art.
[00:11:38.000 --> 00:11:39.120]   I don't know.
[00:11:39.120 --> 00:11:39.840]   What is it?
[00:11:39.840 --> 00:11:40.400]   I don't know.
[00:11:40.400 --> 00:11:41.040]   We'll see.
[00:11:41.040 --> 00:11:41.280]   Yeah.
[00:11:42.240 --> 00:11:47.920]   But, but, but the other thing I reread this stochastic pirates parrots or stochastic pirates.
[00:11:47.920 --> 00:11:48.240]   I like that.
[00:11:48.240 --> 00:11:49.200]   I really like that.
[00:11:49.200 --> 00:11:50.000]   I really like that.
[00:11:50.000 --> 00:11:56.160]   And you can see there's a generational problem is it declines in quality as it gets
[00:11:56.160 --> 00:11:57.920]   parsed through many generations.
[00:11:57.920 --> 00:12:00.480]   Parrots turned into pirates.
[00:12:00.480 --> 00:12:05.360]   Pirates, but the other, the other, I liked our cast experience is the one that Tim Nick Gibrew
[00:12:05.360 --> 00:12:05.600]   and.
[00:12:05.600 --> 00:12:07.200]   Gibrew and what's her name?
[00:12:07.200 --> 00:12:07.680]   Williams.
[00:12:07.680 --> 00:12:08.320]   Don't forget her name.
[00:12:08.320 --> 00:12:08.880]   I forgot her name.
[00:12:08.880 --> 00:12:09.120]   No.
[00:12:09.120 --> 00:12:11.040]   We'll get her name anyway.
[00:12:11.040 --> 00:12:18.000]   But, but, but, but there's, to me, I said this to somebody today on a master done because
[00:12:18.000 --> 00:12:22.240]   Stacy, to your point, I think we've got to know what brand of AI we're talking to.
[00:12:22.240 --> 00:12:28.000]   By this, I mean, it's one that reflects the world as it is on the internet with all its biases
[00:12:28.000 --> 00:12:30.240]   and limitations and exclusions.
[00:12:30.240 --> 00:12:30.720]   Yes.
[00:12:30.720 --> 00:12:36.880]   Or it's one that is trained in a curatorial manner with limitations.
[00:12:36.880 --> 00:12:38.080]   Oh, we only gave it the stuff.
[00:12:38.080 --> 00:12:39.760]   So we make sure we get better stuff.
[00:12:39.760 --> 00:12:40.480]   Which one is it?
[00:12:40.480 --> 00:12:44.880]   I think the reflective is not necessarily bad to the extent that it says this is the
[00:12:44.880 --> 00:12:45.760]   world you made, folks.
[00:12:45.760 --> 00:12:47.200]   Blame yourselves.
[00:12:47.200 --> 00:12:50.160]   The curatorial is difficult.
[00:12:50.160 --> 00:12:56.160]   And we're going to see different, I think, efforts of understanding which AI we're talking to.
[00:12:56.160 --> 00:12:58.160]   That's a good point.
[00:12:58.160 --> 00:13:03.600]   And I would say on the curatorial side, there's no perfect AI, right?
[00:13:03.600 --> 00:13:03.920]   Yeah.
[00:13:03.920 --> 00:13:08.160]   I mean, so like, it's kind of like when you visit another country, you just accept that they do
[00:13:08.160 --> 00:13:09.520]   things differently here, right?
[00:13:09.520 --> 00:13:12.400]   Some people don't like visiting countries because that's what happens.
[00:13:12.400 --> 00:13:17.200]   So I think every AI model is going, depending on who trained it and what values were ascribed
[00:13:17.200 --> 00:13:24.320]   to what it's doing, every AI model is going to have their own little biases or cultures.
[00:13:24.320 --> 00:13:28.800]   And if we stop calling it, I mean, we probably should keep calling it biases because right now,
[00:13:28.800 --> 00:13:30.720]   there are lots of actual harmful things.
[00:13:30.720 --> 00:13:33.920]   But if we start calling it culture, how does that change?
[00:13:33.920 --> 00:13:35.280]   How we might think about it?
[00:13:35.760 --> 00:13:43.280]   Because I could build an AI that, recognizing that I built an AI that is harmful to women or
[00:13:43.280 --> 00:13:47.120]   black people, I can try to adjust that.
[00:13:47.120 --> 00:13:53.920]   And then I should say, though, that this is where this is the culture I'm trying to build.
[00:13:53.920 --> 00:13:54.400]   I don't know.
[00:13:54.400 --> 00:13:55.680]   I'm just trying to think about like--
[00:13:55.680 --> 00:13:55.680]   Yeah.
[00:13:55.680 --> 00:13:56.800]   So it's just a course for you.
[00:13:56.800 --> 00:14:01.360]   Because people think-- and this drives me absolutely bonkers.
[00:14:01.360 --> 00:14:08.720]   People think because it is technology, because it's AI or data-driven, that it's neutral.
[00:14:08.720 --> 00:14:11.280]   And it's not, not just because we have an assembly--
[00:14:11.280 --> 00:14:11.840]   Or accurate.
[00:14:11.840 --> 00:14:12.480]   --six to it.
[00:14:12.480 --> 00:14:13.680]   Yeah, or accurate.
[00:14:13.680 --> 00:14:14.480]   That's exactly--
[00:14:14.480 --> 00:14:14.480]   Right.
[00:14:14.480 --> 00:14:20.320]   --the point of stochastic parrots is that we're going to ascribe more value to it because it's
[00:14:20.320 --> 00:14:24.320]   somehow a computer generated and it's a machine.
[00:14:24.320 --> 00:14:26.640]   And we shouldn't.
[00:14:26.640 --> 00:14:29.040]   Right, Jeff, am I summarizing that accurately?
[00:14:29.040 --> 00:14:29.040]   Right.
[00:14:29.040 --> 00:14:29.600]   Yeah.
[00:14:29.600 --> 00:14:30.240]   Yeah.
[00:14:30.240 --> 00:14:37.440]   Well, it brings the limitations of biases and exclusions of the world with it.
[00:14:37.440 --> 00:14:44.240]   And that there's an ethical responsibility to then deal with that, to Stacy's point, though.
[00:14:44.240 --> 00:14:49.280]   But how do you get your hands around it enough to say, OK, I can guarantee this is not biased
[00:14:49.280 --> 00:14:50.160]   against women.
[00:14:50.160 --> 00:14:51.280]   Well, what does that mean?
[00:14:51.280 --> 00:14:55.520]   How do you guarantee that-- is that a skewed view of the world?
[00:14:55.520 --> 00:14:56.080]   Is that--
[00:14:56.080 --> 00:14:57.120]   How do you define women?
[00:14:57.120 --> 00:14:57.600]   Yeah.
[00:14:57.600 --> 00:14:58.640]   Is it built by Turks?
[00:14:58.640 --> 00:14:59.200]   There you go.
[00:14:59.200 --> 00:14:59.600]   There you go.
[00:14:59.600 --> 00:15:01.040]   [LAUGHTER]
[00:15:01.040 --> 00:15:04.960]   Well, so by the way, Margaret Schmitchell is the author.
[00:15:04.960 --> 00:15:10.800]   The other author, along with Tim Nick Gebru and Angelina Macmillan Major and Emily Bender.
[00:15:10.800 --> 00:15:11.680]   They wrote this.
[00:15:11.680 --> 00:15:17.520]   This was the piece that got Margaret Mitchell and Tim Nick Gebru fired from Google because
[00:15:17.520 --> 00:15:20.720]   this is-- Google said, well, what the--
[00:15:20.720 --> 00:15:21.680]   This is what we're doing.
[00:15:21.680 --> 00:15:22.640]   That's so--
[00:15:22.640 --> 00:15:22.640]   So--
[00:15:22.640 --> 00:15:26.080]   Emily Bender is very good on Mastodon.
[00:15:27.120 --> 00:15:27.920]   She and--
[00:15:27.920 --> 00:15:28.640]   Oh, I have to follow her.
[00:15:28.640 --> 00:15:29.280]   Both of them, Mastodon.
[00:15:29.280 --> 00:15:29.920]   Oh, I have to follow her.
[00:15:29.920 --> 00:15:30.800]   Bender's very good.
[00:15:30.800 --> 00:15:39.120]   She talks a lot about bad media coverage of AI going overboard and losing their breath about
[00:15:39.120 --> 00:15:40.480]   how wonderful it is and amazing it is.
[00:15:40.480 --> 00:15:44.960]   It is-- let's be realistic about what it can do, which is Stacy's point, I think.
[00:15:44.960 --> 00:15:50.880]   They conclude, we have identified a wide variety of costs and risks associated with the rush
[00:15:50.880 --> 00:15:54.320]   for ever larger language models, including environmental costs.
[00:15:54.320 --> 00:15:55.280]   We know, for instance, Chate--
[00:15:55.280 --> 00:15:56.320]   That's a big flavor.
[00:15:56.320 --> 00:15:58.720]   Sam Altman says, burns $3 million a day.
[00:15:58.720 --> 00:16:00.400]   And that's, of course, almost all electricity.
[00:16:00.400 --> 00:16:03.520]   You know, it's a very expensive thing to do.
[00:16:03.520 --> 00:16:08.000]   And by the way, the environmental costs are born typically by those not benefiting from
[00:16:08.000 --> 00:16:12.800]   the resulting technology, financial costs, which in turn erect barriers to entry,
[00:16:12.800 --> 00:16:16.960]   limiting who can contribute to this research area, which languages can benefit from the
[00:16:16.960 --> 00:16:18.160]   most advanced techniques.
[00:16:18.160 --> 00:16:19.760]   You see a lot of it's in English.
[00:16:19.760 --> 00:16:21.840]   Probably a lot of it in Chinese as well, right?
[00:16:21.840 --> 00:16:27.280]   Opportunity costs as researchers pour away effort from directions requiring less resources.
[00:16:27.280 --> 00:16:32.240]   And then, of course, the risk of substantial harm, stereotyping, denigration,
[00:16:32.240 --> 00:16:38.400]   increases in extremist ideology, wrongful arrest, which we've seen, of course, with face
[00:16:38.400 --> 00:16:39.040]   recognition.
[00:16:39.040 --> 00:16:41.440]   We're going to get to that particular subject in a second.
[00:16:41.440 --> 00:16:49.200]   So they say NLP researchers should carefully weigh those risks while pursuing this research
[00:16:49.200 --> 00:16:52.000]   direction and consider whether the benefit's outweigh the risks.
[00:16:52.000 --> 00:16:53.440]   I think that's a very appropriate thing.
[00:16:53.440 --> 00:16:59.440]   That's exactly why you hire Tim Nick Gebru and Margaret Mitchell at Google to be ethical AI,
[00:16:59.440 --> 00:17:02.720]   to be the people saying, hey, be careful here.
[00:17:02.720 --> 00:17:05.840]   And as soon as they did it, Google said, well, no, you don't want to be careful.
[00:17:05.840 --> 00:17:07.520]   Well, it's not just be careful.
[00:17:07.520 --> 00:17:13.600]   I think the messaging that we have against them is kind of like, it portrays them as kind of like
[00:17:13.600 --> 00:17:16.400]   prim or schoolmarmish, like, oh, ethics, oh, weakness.
[00:17:16.400 --> 00:17:18.720]   I think the message here isn't just be careful.
[00:17:18.720 --> 00:17:22.320]   It is this is not value neutral.
[00:17:22.320 --> 00:17:25.520]   And that has big implications positive and negative.
[00:17:25.520 --> 00:17:33.440]   I think Google and all the companies, they cynically want to sell you this concept of technology
[00:17:33.440 --> 00:17:34.480]   as a solution.
[00:17:34.480 --> 00:17:36.240]   And you know who has bought into it?
[00:17:36.240 --> 00:17:39.040]   And this is driving me nuts because it affects our privacy.
[00:17:39.040 --> 00:17:43.280]   The government, the government is like really bought into this.
[00:17:43.280 --> 00:17:46.800]   And by the government, I mean federal government, I mean your municipal governments who are like,
[00:17:46.800 --> 00:17:52.960]   anybody, someone says data driven, they're using it as an excuse to avoid making hard policy
[00:17:52.960 --> 00:17:53.920]   decisions.
[00:17:53.920 --> 00:17:55.120]   Exactly.
[00:17:55.120 --> 00:17:58.960]   That's another important part of this stochastic parrot's paper.
[00:17:58.960 --> 00:18:05.440]   We note the risks associated with synthetic but seemingly coherent text are deeply connected to
[00:18:05.440 --> 00:18:11.680]   the fact that such synthetic text can enter into conversations without any person or entity
[00:18:11.680 --> 00:18:17.600]   being accountable for it. This accountability both involves responsibility for truthfulness
[00:18:17.600 --> 00:18:22.720]   and is important in situating meaning because and this is where people go well,
[00:18:22.720 --> 00:18:23.680]   it's got to be true.
[00:18:23.680 --> 00:18:25.200]   It's data driven, right?
[00:18:25.200 --> 00:18:26.720]   And you're right.
[00:18:26.720 --> 00:18:28.880]   That's how people avoid responsibility.
[00:18:28.880 --> 00:18:31.440]   Well, your city council says, well, we've got the data.
[00:18:31.440 --> 00:18:32.800]   You know, the data told us this.
[00:18:32.800 --> 00:18:34.160]   It's not our responsibility.
[00:18:34.160 --> 00:18:37.920]   Well, that's the other part about this is that is an LP.
[00:18:38.960 --> 00:18:43.040]   I said this again and again on the show, the one I've learned about it is that it only predicts
[00:18:43.040 --> 00:18:46.400]   the next best word, the next word that is best.
[00:18:46.400 --> 00:18:47.840]   It does not deal with facts.
[00:18:47.840 --> 00:18:49.520]   Well, it doesn't understand content at all.
[00:18:49.520 --> 00:18:50.160]   Really, right?
[00:18:50.160 --> 00:18:51.520]   Because it's not content at all.
[00:18:51.520 --> 00:18:58.000]   What is going to sound right to the human ear based on what we produce in the past?
[00:18:58.000 --> 00:18:59.360]   That's all it does.
[00:18:59.360 --> 00:19:01.680]   So there's a story I put in the rundown, which I'll ignore anyway.
[00:19:01.680 --> 00:19:02.480]   Don't want to change subject.
[00:19:02.480 --> 00:19:03.440]   So hold on a second.
[00:19:03.440 --> 00:19:04.080]   One more thought.
[00:19:04.080 --> 00:19:05.120]   No, it's the same subject.
[00:19:05.120 --> 00:19:05.920]   No, no, but one more thought.
[00:19:05.920 --> 00:19:08.480]   I just wanted to respond to what Stacy's original point.
[00:19:08.480 --> 00:19:10.080]   Okay, which I think was well taken.
[00:19:10.080 --> 00:19:14.240]   All content is generated by AI who's going to generate the new stuff.
[00:19:14.240 --> 00:19:18.720]   And I do think there's a point to be made there, Stacy,
[00:19:18.720 --> 00:19:23.040]   but I remember my English teacher in high school saying there's only three stories.
[00:19:23.040 --> 00:19:26.000]   You have to know your Homer, you have to know your Bible,
[00:19:26.000 --> 00:19:27.520]   and you have to know your Shakespeare.
[00:19:27.520 --> 00:19:28.880]   Then you know every story.
[00:19:28.880 --> 00:19:36.000]   So all creativity is based on not all, but a lot of it's better.
[00:19:36.000 --> 00:19:36.960]   We've added another story.
[00:19:37.600 --> 00:19:41.120]   We've added so man versus man, man versus nature.
[00:19:41.120 --> 00:19:42.640]   Oh, now we have man versus machine.
[00:19:42.640 --> 00:19:44.240]   Yeah, man versus machine.
[00:19:44.240 --> 00:19:45.280]   What's the other one?
[00:19:45.280 --> 00:19:48.160]   Man versus man, man nature, man versus the gods.
[00:19:48.160 --> 00:19:48.560]   What?
[00:19:48.560 --> 00:19:50.160]   No, we had that all along.
[00:19:50.160 --> 00:19:51.040]   That was the first one.
[00:19:51.040 --> 00:19:52.080]   Oh, man versus himself.
[00:19:52.080 --> 00:19:55.760]   himself, other man, nature.
[00:19:55.760 --> 00:19:58.080]   And now we've added machines.
[00:19:58.080 --> 00:20:00.240]   I would argue maybe society.
[00:20:00.240 --> 00:20:04.160]   I mean, if you look at a lot of like kind of the literature,
[00:20:04.160 --> 00:20:05.360]   I don't know about society.
[00:20:05.360 --> 00:20:07.600]   Anyway, I would say we've added more.
[00:20:07.600 --> 00:20:08.160]   That's kind of man versus myself, right?
[00:20:08.160 --> 00:20:10.640]   I'm going to tell Mr. Leonard.
[00:20:10.640 --> 00:20:12.560]   Mr. Leonard, we got two more.
[00:20:12.560 --> 00:20:18.640]   And you know, the other flip side is like there are only 41 stories and Shakespeare's
[00:20:18.640 --> 00:20:19.360]   written all of them.
[00:20:19.360 --> 00:20:19.600]   Right.
[00:20:19.600 --> 00:20:21.600]   So I don't, I mean, but you're right.
[00:20:21.600 --> 00:20:25.040]   I mean, and that's what that's a good question though, is where does where's the new stuff come from?
[00:20:25.040 --> 00:20:26.000]   Where's this?
[00:20:26.000 --> 00:20:29.360]   Well, but here's the here's the here's I said this week of the show last week.
[00:20:29.360 --> 00:20:33.280]   That when Montane did his essays, oh, history moment,
[00:20:34.400 --> 00:20:37.680]   he set the bar for joining the public conversation as being a writer.
[00:20:37.680 --> 00:20:45.280]   And what's freaking out professors and teachers about about chat GPT is we'll never know whether
[00:20:45.280 --> 00:20:46.560]   the student really wrote it or not.
[00:20:46.560 --> 00:20:51.760]   And I've seen creative teachers have ideas about, well, maybe you have the student judge
[00:20:51.760 --> 00:20:56.640]   the chat GPT and look into its logic and look into its facts and decide whether there's any good or not.
[00:20:56.640 --> 00:20:59.360]   But the point is they do with Wikipedia is special privilege.
[00:20:59.360 --> 00:21:02.960]   They say whether he is a starting point, not your, not your essay.
[00:21:02.960 --> 00:21:03.600]   Right.
[00:21:03.600 --> 00:21:09.200]   But the point becomes that the privilege that all of us here have as writers kind of goes away.
[00:21:09.200 --> 00:21:10.480]   It's not so special anymore.
[00:21:10.480 --> 00:21:13.760]   And so where does the value lie then?
[00:21:13.760 --> 00:21:22.080]   I think it lies in thought and conversation and lived experience and unique ways to say things.
[00:21:22.080 --> 00:21:25.040]   And it potentially opens that up and says content is commodity.
[00:21:25.040 --> 00:21:30.400]   Looking at hot takes, all the hot takes on Atlantic and New York Times columnists
[00:21:30.400 --> 00:21:31.200]   and all over.
[00:21:31.200 --> 00:21:32.640]   We're over done with hot takes.
[00:21:32.640 --> 00:21:33.840]   It's Twitter. It's come out of time.
[00:21:33.840 --> 00:21:35.760]   Yeah, it's Twitter.
[00:21:35.760 --> 00:21:41.200]   So there's something else, Leo, that I don't think it's a new kind of content.
[00:21:41.200 --> 00:21:47.040]   I think it's more around, in my view, it's always about recapturing conversation.
[00:21:47.040 --> 00:21:48.640]   It's about connections with people.
[00:21:48.640 --> 00:21:52.000]   It's about valuing lived experiences that were not part of mass media.
[00:21:52.000 --> 00:21:54.080]   And it's still a page from your book.
[00:21:54.080 --> 00:21:56.400]   I mean, you've always said the internet is too early.
[00:21:56.400 --> 00:21:57.440]   It's so new.
[00:21:57.440 --> 00:21:58.960]   AI is even newer.
[00:21:58.960 --> 00:22:01.440]   So we don't yet know how it's going to be incorporated.
[00:22:01.440 --> 00:22:07.680]   I think as a chess player, I think back 20 years ago when the first computers beat the world champion
[00:22:07.680 --> 00:22:12.720]   and now computers are regularly accepted to be better than any human ever could be by a long shot.
[00:22:12.720 --> 00:22:15.680]   And at the time, I think a lot of us said, well, that's it for chess.
[00:22:15.680 --> 00:22:19.680]   No, humans going to ever want to play a game that the computer could beat him every single time.
[00:22:19.680 --> 00:22:21.360]   But it actually hasn't.
[00:22:21.360 --> 00:22:24.640]   And a couple of things that have happened, I think will happen with AI as well.
[00:22:24.640 --> 00:22:28.960]   A lot of modern chess is a collaboration between humans and computers.
[00:22:29.760 --> 00:22:34.640]   So they'll use computers to do analyses on a position, but the humans will add this human element to it.
[00:22:34.640 --> 00:22:42.960]   And there's still a very active and exciting and entertaining process of watching humans compete
[00:22:42.960 --> 00:22:46.960]   against each other. We try really hard to keep the computer out of those competitions, but in
[00:22:46.960 --> 00:22:50.000]   preparation, they're very much used.
[00:22:50.000 --> 00:22:55.360]   As a journalist, I think some of the stories that most journalists hate writing, like,
[00:22:56.240 --> 00:23:00.000]   oh, I think the city council meeting.
[00:23:00.000 --> 00:23:06.320]   Yeah. Well, you know, I actually think sports, financial reports, I think city council meetings
[00:23:06.320 --> 00:23:13.040]   are worth getting like, that's where your experience can come into play and chat GPT probably will fail.
[00:23:13.040 --> 00:23:17.920]   You could write chat GPT could write the base. Here's what happened as it does with sports and
[00:23:17.920 --> 00:23:20.720]   finance. The human is the one who has to understand what happened.
[00:23:20.720 --> 00:23:25.600]   Right. Exactly. And provide context like, hey, this happened here.
[00:23:25.600 --> 00:23:30.480]   This person has, and I think, I mean, Jeff, I, you're sweet for saying that I'm a writer,
[00:23:30.480 --> 00:23:35.760]   but I'm not. I think that's where reporting, we went like after like Hunter S. Thompson and Joe
[00:23:35.760 --> 00:23:40.160]   Dydian and all these like luminaries of like feature writing came out. Everyone was like,
[00:23:40.160 --> 00:23:47.120]   Rick Bragg, everyone was like, I am a writer. Yes. But I think there's a lot of value in journalism.
[00:23:47.120 --> 00:23:52.640]   And I think that will come back out and we can, we can kind of create that.
[00:23:52.640 --> 00:23:58.000]   Degree that happened in chess. We value human chess games higher. Now, those are.
[00:23:58.000 --> 00:24:04.080]   I want to hear ants on this in terms of the second ago, how chess is.
[00:24:04.080 --> 00:24:10.480]   Okay. Okay. Do not watch. I know. Just forget what I said, but
[00:24:10.480 --> 00:24:18.080]   got a interesting hat taped to his. You were talking about the human element versus the
[00:24:18.080 --> 00:24:24.800]   computer element inside of a chess match. And me as an outsider looking in and into the game of
[00:24:24.800 --> 00:24:30.960]   chess, I'm to the point where it seems like all chess is just a calculated. It is. That's why
[00:24:30.960 --> 00:24:36.880]   computers have it. It doesn't seem like there's any type of human element in there versus say,
[00:24:36.880 --> 00:24:44.800]   like the poker table. Yeah. Because it's because poker is very calculating, but at the same time,
[00:24:44.800 --> 00:24:49.920]   a wink of an eye or a twitch or something is that human element that can change a hand.
[00:24:49.920 --> 00:24:54.560]   And chess is at the same way at all. Because I just see it as just data.
[00:24:54.560 --> 00:25:00.240]   Yeah. I mean, at its root, it is data. And actually, I guess you could say it's almost like a
[00:25:00.240 --> 00:25:05.280]   competition between humans and a calculator who can calculate the largest division.
[00:25:05.280 --> 00:25:09.840]   And obviously, a calculator is always going to win. Humans can only do so much. That's
[00:25:09.840 --> 00:25:14.320]   that same thing with chess, although there's a certain amount of intuition. But yeah, although
[00:25:14.320 --> 00:25:18.400]   some chess players say, you know, he's psyching me out the way he's looking, the way he's acting,
[00:25:18.400 --> 00:25:23.280]   and they're certainly okay. So they do have that. Well, but you know what, the difference is the
[00:25:23.280 --> 00:25:29.840]   car just like the cards speak. Unlike poker, you see the whole position. There's no,
[00:25:29.840 --> 00:25:37.040]   there's no mystery going on. So I think psychology has a lesser, there's no bluffing in chess.
[00:25:37.680 --> 00:25:40.720]   No bluffing, right? You can act like you won, but if you haven't,
[00:25:40.720 --> 00:25:46.480]   you can freak something out. I know your queen is still there, dude. So that's
[00:25:46.480 --> 00:25:52.960]   exactly one of the problems we talked about was with lensa and AI, and there was a good
[00:25:52.960 --> 00:25:59.200]   wired article about this making sexist images of women not. Yeah. But this is correctable. And
[00:25:59.200 --> 00:26:04.560]   I just wanted to show you, I had a week ago, generated a bunch of pictures of my wife,
[00:26:05.120 --> 00:26:10.320]   some of which I can't show you. They're not safe for work. But I just did the same thing,
[00:26:10.320 --> 00:26:15.920]   same pictures. And then now it's doing Christmas pictures, by the way, and they're fantastic.
[00:26:15.920 --> 00:26:23.280]   They're much better. And there is, and there's no cleavage. There's none of the weirdness
[00:26:23.280 --> 00:26:29.680]   that they did to Lisa. These are the she will be much happier, although she did save some of those.
[00:26:29.680 --> 00:26:33.600]   She put a few of them off a lot. Yeah. That's about it.
[00:26:33.600 --> 00:26:38.960]   I got what it was. I pissed off Sam on Twitter, because I said that it looked like it was run by,
[00:26:38.960 --> 00:26:44.880]   you know, a horny teenage boy. Yeah. It was creepy. Just just look at it. And obviously they've
[00:26:44.880 --> 00:26:49.760]   they've looked into the public. They did something. They just switch. This is the curation point.
[00:26:49.760 --> 00:26:55.760]   Exactly. You curate the data in, but you also limit the data out. There must be a setting.
[00:26:55.760 --> 00:27:00.080]   Well, angelic setting. Yeah, there's some there's something going on.
[00:27:01.680 --> 00:27:05.680]   This is like Elvin Ferry Princess. Yeah, there's different categories.
[00:27:05.680 --> 00:27:10.560]   So and I wanted to do this because I love Lord of the Rings. But these are beautiful. These are
[00:27:10.560 --> 00:27:15.760]   these are like paintings. They're really good. And so that's an example of a human going,
[00:27:15.760 --> 00:27:20.320]   yeah, we better fix that and changing the parameters and making something that's really
[00:27:20.320 --> 00:27:27.040]   fantastic. I'm trying to get to Ant on this. And as a photographer and an artist, and especially
[00:27:27.040 --> 00:27:31.360]   a portrait person, how do you portrait person? I wasn't really trying to say a portrait artist.
[00:27:32.160 --> 00:27:39.200]   How do you look at this in a few ways? Both the competitive angle of being able to illustrate
[00:27:39.200 --> 00:27:44.880]   profiles with this, but they also the input side of it that surely some of the portraits you've
[00:27:44.880 --> 00:27:50.960]   done somewhere are in here teaching the machine. How do you look at this? Yeah.
[00:27:50.960 --> 00:27:56.720]   When if there's something scrappable, like I believe we mentioned this last week,
[00:27:56.720 --> 00:28:05.120]   fine art America. That's where a lot of my prints are coming from. That's public. And
[00:28:05.120 --> 00:28:12.320]   if an AI wants to scrape that to try to train itself, it clearly can because it's public and
[00:28:12.320 --> 00:28:18.480]   not something that I can just take away unless I physically take it down, if you will. But when it
[00:28:18.480 --> 00:28:24.480]   comes to just doing portraits, I still don't believe this AI is going to take me out or take
[00:28:24.480 --> 00:28:29.600]   any other photographers out because there's a certain human connection that's involved in that
[00:28:29.600 --> 00:28:36.640]   portrait session that a computer can't replicate. Yeah, these prompts are you better be really,
[00:28:36.640 --> 00:28:40.480]   really good at your prompts to make them look good because you can just put in, I need a portrait
[00:28:40.480 --> 00:28:44.400]   of somebody sitting on the beach at sunset and it'll put something out there and it may not look
[00:28:44.400 --> 00:28:47.920]   that good. And then you start adding in all of these additional parameters. And next thing you
[00:28:47.920 --> 00:28:51.520]   know, an hour later, you're still moving the sliders around in this AI and it still isn't quite
[00:28:51.520 --> 00:28:54.080]   right when you could have just hired a photographer to do it. Yeah.
[00:28:54.080 --> 00:29:02.480]   I recently followed a woman who is a graphic artist who said she's pulling all of her public
[00:29:02.480 --> 00:29:07.920]   stuff off because... And that's fair because it's feeding the algorithms.
[00:29:07.920 --> 00:29:12.560]   Yeah. Well, what's interesting is when you upload those photos of Lisa, those are
[00:29:12.560 --> 00:29:16.160]   become part of the training data, right? Yeah, all of a sudden, huh? So, yeah.
[00:29:16.960 --> 00:29:22.160]   Now, what's interesting is if Ant, like, let's say Ant took some amazing photos of me and I uploaded
[00:29:22.160 --> 00:29:28.400]   them, then by my actions, Ant has lost control of his ability to pull his art from there, which...
[00:29:28.400 --> 00:29:34.800]   Yeah. I mean... You know, and I found this interesting the other day I was playing
[00:29:34.800 --> 00:29:40.640]   in stable diffusion. You remember when we talked about podcasts having lunch or podcast or having
[00:29:40.640 --> 00:29:45.680]   lunch as a... Yeah, that was a mid-journey thing we did when we first started doing this. Yeah.
[00:29:45.680 --> 00:29:50.080]   Right. So, I pretty much did the same thing in stable diffusion. I had the prompt in front of me
[00:29:50.080 --> 00:29:54.640]   and it said, "It's a podcast or having lunch in his style of an animation." Go on effect.
[00:29:54.640 --> 00:30:01.040]   And the result fascinated me. This is what it came up with. Oh, I like it. First of all,
[00:30:01.040 --> 00:30:06.320]   it wasn't a guy anymore. Exactly. Yeah. It's like, "Huh, what triggered that?" Yeah.
[00:30:07.360 --> 00:30:15.040]   Interesting. So, something's going on with these generators because they see that a lot of...
[00:30:15.040 --> 00:30:19.120]   there's a lot of diversity out there when it comes to something like podcasts. And now,
[00:30:19.120 --> 00:30:22.880]   there are some other things from the diversity standpoint that still needs a little work, but
[00:30:22.880 --> 00:30:30.480]   who knows? It's getting a little better. Which is really all you can ask with this sort of stuff,
[00:30:30.480 --> 00:30:36.960]   is that when you put this stuff up out there, you explain as much as you can what you're trying to
[00:30:36.960 --> 00:30:42.880]   do. And then you accept constructive criticism and then you adjust. I mean, it's basically like
[00:30:42.880 --> 00:30:49.120]   how to be a decent person if you think about it. Yeah. Same things you teach your daughter.
[00:30:49.120 --> 00:30:54.400]   Right. Moving on. Let's talk about, since we were talking about facial recognition,
[00:30:54.400 --> 00:31:00.880]   we'll get to your story in a second, but we should set it up with this story from Madison Square
[00:31:01.600 --> 00:31:07.680]   Garden. I think probably a lot of people saw it. It got a lot of coverage. Mom was bringing her
[00:31:07.680 --> 00:31:15.200]   daughter to Radio City Music Hall the weekend after Thanksgiving. It was a Girl Scout field trip
[00:31:15.200 --> 00:31:20.080]   to see the Christmas show with the rockets and everything. But while her daughter,
[00:31:20.080 --> 00:31:23.280]   other members of the Girl Scout troop and their mothers got to go to the show,
[00:31:24.400 --> 00:31:31.360]   this mother Kelly Conlon was not allowed in. Face recognition caught her as she entered.
[00:31:31.360 --> 00:31:38.880]   Security guards approached her as she got into the lobby. She said, I heard them say,
[00:31:38.880 --> 00:31:44.960]   woman with long dark hair and gray scarf. She was asked her name, asked to produce identification.
[00:31:44.960 --> 00:31:53.600]   They said, face recognition picked you up. She says she posed no threat, but the guards kicked her out.
[00:31:54.400 --> 00:32:00.240]   With the explanation, they knew she was an attorney. They knew my name before I told them.
[00:32:00.240 --> 00:32:05.120]   They knew the firm I was associated with before I told them. They told me I was not allowed to be
[00:32:05.120 --> 00:32:11.760]   there. MSG entertainment, which owns Madison Square Garden, was apparently a billion company with
[00:32:11.760 --> 00:32:17.520]   a billion owners. Huge. We're going to get to that. She works for a law firm,
[00:32:17.520 --> 00:32:22.720]   David Saperstein and Solomon, which has been involved in a personal injury lawsuit against a
[00:32:22.720 --> 00:32:28.560]   restaurant venue owned by MSG. She says, I don't practice in New York. I'm not an attorney that
[00:32:28.560 --> 00:32:35.040]   works on any cases against MSG, but all attorneys from her firmware band and they used face recognition
[00:32:35.040 --> 00:32:41.040]   to recognize those attorneys and block them from entering. That's the right, isn't it?
[00:32:41.040 --> 00:32:46.880]   Hey, it's part of the investigation.
[00:32:49.440 --> 00:32:55.520]   The whole the whole team says one of her partners is a pretext for doing collective punishment
[00:32:55.520 --> 00:32:59.760]   on adversaries who would dare sue MSG in their multi-billion dollar.
[00:32:59.760 --> 00:33:04.640]   What's wrong with this is the human pieces. The dolens, I worked with them and I had cable
[00:33:04.640 --> 00:33:09.360]   vision for a while and I worked on one of their cable networks here in New Jersey.
[00:33:09.360 --> 00:33:13.520]   There's an awful, awful company. It's terrible company.
[00:33:13.520 --> 00:33:16.080]   It's a policy. It's probably a reflection on them.
[00:33:16.080 --> 00:33:19.920]   Yeah. You will never see the rockettes again, Jeff.
[00:33:19.920 --> 00:33:23.360]   But speaking of which, that is the most boring thing.
[00:33:23.360 --> 00:33:27.600]   It's a tradition. Knock it off. They have cameras too.
[00:33:27.600 --> 00:33:34.480]   I was just a mom taking my daughter to see a Christmas show. She's told NBC New York.
[00:33:34.480 --> 00:33:38.320]   I did wait outside. It was embarrassing. It was mortifying. Of course it was.
[00:33:38.320 --> 00:33:42.080]   Well, her daughter was inside. She was outside. That's just awful.
[00:33:42.080 --> 00:33:46.800]   Well, the thing is if this was, why wasn't she privy to this information?
[00:33:46.800 --> 00:33:49.920]   Is it because she's too low on the totem pole? Is that deferred?
[00:33:49.920 --> 00:33:56.560]   MSG says we told that law firm twice. They would not be allowed in that they knew what the policy was.
[00:33:56.560 --> 00:34:02.720]   They said she ought to know. She's not going to come in. By the way, they are now
[00:34:02.720 --> 00:34:09.600]   challenging them because MSG has a liquor license, which requires them to admit members of the public
[00:34:09.600 --> 00:34:13.120]   unless there are people who would be disruptive or constitute a security threat.
[00:34:13.120 --> 00:34:17.280]   They may actually have grandpa to get them.
[00:34:17.280 --> 00:34:26.320]   Yeah, there's a lot that's covered under like, hey, it's a private business. We refuse the
[00:34:26.320 --> 00:34:30.800]   right to serve people. We have the right to refuse service.
[00:34:30.800 --> 00:34:39.440]   But there's also the government. There's a public interest in most members of the public.
[00:34:39.440 --> 00:34:43.760]   Being allowed to go to places that they expect to be allowed.
[00:34:43.760 --> 00:34:49.600]   This is a classic issue of public rights versus private rights.
[00:34:49.600 --> 00:35:00.160]   The new angle here is the ability to enforce it using against more people using this kind of
[00:35:00.160 --> 00:35:05.600]   technology to actually limit them from coming in. Now you're saying you can actually,
[00:35:07.360 --> 00:35:10.480]   because usually if you're going to, what's the word I'm looking for,
[00:35:10.480 --> 00:35:15.520]   discriminate against an individual person, you've had actual interactions with them.
[00:35:15.520 --> 00:35:20.480]   There are James Corden and you're whatever restaurant he got banned from.
[00:35:20.480 --> 00:35:22.960]   Right. You know, and there's a reason.
[00:35:22.960 --> 00:35:25.840]   He apologized, by the way, and it's been allowed back.
[00:35:25.840 --> 00:35:31.440]   They're not charged in a court law, but they're like, well, he's behaved badly.
[00:35:31.440 --> 00:35:34.080]   Yeah, you behave like a jerk. It's their right.
[00:35:35.280 --> 00:35:37.200]   But she wasn't behaving like a jerk, I guess.
[00:35:37.200 --> 00:35:42.640]   But here's the issue. Their contention could be that anybody who behaves like a jerk,
[00:35:42.640 --> 00:35:46.160]   like someone who's a jerk to them on Twitter, does that mean they shouldn't,
[00:35:46.160 --> 00:35:50.240]   you know, now that you have easy access to this imagery and facial recognition,
[00:35:50.240 --> 00:35:56.320]   at what level do you, and this is a public private interest kind of claim.
[00:35:56.320 --> 00:35:58.480]   And so I think this should go all the way to the courts.
[00:35:58.480 --> 00:36:04.240]   And I really want to see what happens here, because this is exactly the sort of governmental
[00:36:04.240 --> 00:36:05.840]   policy we need to start thinking about.
[00:36:05.840 --> 00:36:07.360]   What if you got to remember we have a
[00:36:07.360 --> 00:36:08.880]   some most killing accounts on Twitter?
[00:36:08.880 --> 00:36:14.160]   You remember we have a Supreme Court that said it's okay not to bake a cake for a gay couple or.
[00:36:14.160 --> 00:36:20.960]   Yes, but I mean, we still had, I don't love our current Supreme Court,
[00:36:20.960 --> 00:36:24.720]   but this is what we're set up to do. So we should probably do it.
[00:36:24.720 --> 00:36:28.560]   And knowing that this sort of thing is going to come up a lot in the next few years,
[00:36:28.560 --> 00:36:30.320]   maybe we should think about our court.
[00:36:30.320 --> 00:36:34.400]   Think about it. What if the Dolan's decide, and we don't like podcasters.
[00:36:34.400 --> 00:36:37.840]   We're just going to block all podcasters.
[00:36:37.840 --> 00:36:39.840]   I mean, they own the-
[00:36:39.840 --> 00:36:42.880]   So then we get to the question of a place of public accommodation.
[00:36:42.880 --> 00:36:43.680]   Right.
[00:36:43.680 --> 00:36:48.480]   And which is a restaurant, which they can do, but is there a point in which it's no,
[00:36:48.480 --> 00:36:50.320]   this is truly public you have to let people in?
[00:36:50.320 --> 00:36:52.560]   The same question will come with Twitter.
[00:36:52.560 --> 00:36:58.160]   The same issues get raised with most killing accounts of journals there.
[00:36:59.200 --> 00:37:02.480]   And we get our op-it arms and I'm op-it arms about it because he's a jerk.
[00:37:02.480 --> 00:37:06.880]   But he had the right to do it. He could throw people off Twitter. He owns it now.
[00:37:06.880 --> 00:37:09.760]   I feel like a physical place.
[00:37:09.760 --> 00:37:16.000]   Somehow seems like it should be more open than say a social network,
[00:37:16.000 --> 00:37:20.480]   but I guess that's just an old-fashioned point of view that if Twitter is no different than
[00:37:20.480 --> 00:37:21.280]   Madison Square Garden.
[00:37:21.280 --> 00:37:24.560]   Well, I don't want Twitter to be named a place of public accommodation that must be open,
[00:37:24.560 --> 00:37:27.440]   because that would force you. That's what the Republicans want to do.
[00:37:27.440 --> 00:37:31.520]   The Republicans want to say, you must carry our notches speech.
[00:37:31.520 --> 00:37:33.360]   You are not allowed to moderate.
[00:37:33.360 --> 00:37:35.280]   You do not have a freedom of expression around.
[00:37:35.280 --> 00:37:39.840]   Is it different if the Dolan's decided we're not going to have Donald Trump
[00:37:39.840 --> 00:37:41.760]   into the Radio City music hall?
[00:37:41.760 --> 00:37:45.040]   Is that different from Twitter saying we're not going to allow Donald Trump to post on Twitter?
[00:37:45.040 --> 00:37:46.720]   That's the question.
[00:37:46.720 --> 00:37:47.600]   That's exactly the question.
[00:37:47.600 --> 00:37:53.920]   Twitter is less of a, I mean, the government has not said that there is no rules around
[00:37:53.920 --> 00:37:57.120]   social media. We have not said that everyone has a right to be on social media.
[00:37:57.840 --> 00:38:05.920]   The government has, through liquor licenses, there is a recognition in our laws that people
[00:38:05.920 --> 00:38:10.720]   are generally allowed into places where the public can gather, right?
[00:38:10.720 --> 00:38:14.720]   Whether it's a coffee shop, whether, I mean, yes, you can charge tickets.
[00:38:14.720 --> 00:38:19.760]   You can, but we have established laws around that.
[00:38:19.760 --> 00:38:28.960]   And so to be kicked out of that requires one, you have to do something bad or you have to
[00:38:28.960 --> 00:38:30.640]   violate a policy.
[00:38:30.640 --> 00:38:38.560]   And it's done not through some mechanism across a large place.
[00:38:38.560 --> 00:38:41.840]   It's done at an individual level as you try to walk in the door.
[00:38:41.840 --> 00:38:49.600]   Now, if this person, I suppose, would it make it better if everybody who is doing security at
[00:38:49.600 --> 00:38:52.640]   Madison Square Garden had a list of photos of all of the lawyers?
[00:38:52.640 --> 00:39:00.240]   And then they were like, okay, like the ease at which you can do this makes it a little more
[00:39:00.240 --> 00:39:01.280]   concerning, I guess.
[00:39:01.280 --> 00:39:10.480]   Then there's the case of scale AI, which is, this is the story you put in, which is really
[00:39:10.480 --> 00:39:19.520]   an interesting story. Scale AI is a contractor in Venezuela, or they hire people
[00:39:19.520 --> 00:39:27.200]   in Venezuela, probably in other places around the world, to review and label audio photos and
[00:39:27.200 --> 00:39:29.840]   video data for training artificial intelligence.
[00:39:29.840 --> 00:39:31.680]   Same thing, right?
[00:39:31.680 --> 00:39:33.440]   They gather it from wherever they can.
[00:39:33.440 --> 00:39:42.560]   In the fall of 2020, according to the MIT technology review, gig workers in Venezuela
[00:39:42.560 --> 00:39:47.200]   posted a series of images to online forums where they gathered to talk shop.
[00:39:47.200 --> 00:39:53.360]   These photos were mundane, if sometimes intimate household scenes captured from low angles,
[00:39:53.360 --> 00:39:56.720]   including some you really wouldn't want shared on the internet.
[00:39:56.720 --> 00:40:03.120]   In one particularly revealing shot, a young woman in a lavender t-shirt sits on the toilet.
[00:40:03.120 --> 00:40:05.200]   Her shorts pulled down the mid thigh.
[00:40:05.200 --> 00:40:09.920]   The images are not taken by a person, but by a Roomba.
[00:40:09.920 --> 00:40:16.720]   By development versions of iRobots, Roomba J7 series vacuum, sent to scale AI,
[00:40:17.680 --> 00:40:18.400]   to be...
[00:40:18.400 --> 00:40:19.680]   Wait, wait, wait, wait, development versions.
[00:40:19.680 --> 00:40:21.520]   So this was not a customer who just bought it.
[00:40:21.520 --> 00:40:22.560]   It was a special version.
[00:40:22.560 --> 00:40:23.680]   Unclear.
[00:40:23.680 --> 00:40:28.480]   It's a customer who opts in, I believe, and I have not confirmed this.
[00:40:28.480 --> 00:40:31.360]   This is a customer who opts into, like, beta.
[00:40:31.360 --> 00:40:31.920]   Let me tell you.
[00:40:31.920 --> 00:40:32.400]   Yeah.
[00:40:32.400 --> 00:40:32.480]   Oh.
[00:40:32.480 --> 00:40:35.040]   Well, it's like when you have...
[00:40:35.040 --> 00:40:36.640]   It's stupid.
[00:40:36.640 --> 00:40:37.600]   Like, stupid.
[00:40:37.600 --> 00:40:38.960]   So, as well as it back.
[00:40:38.960 --> 00:40:40.560]   So let me give you the story.
[00:40:40.560 --> 00:40:44.320]   It says, "All of them came from special development robots with hardware and software
[00:40:44.320 --> 00:40:50.080]   modifications that are not and never were present on iRobot Consumer products for purchase,"
[00:40:50.080 --> 00:40:51.040]   according to the company.
[00:40:51.040 --> 00:40:54.240]   "They were given to paid collectors,"
[00:40:54.240 --> 00:40:57.760]   oh, I guess, information collectors.
[00:40:57.760 --> 00:40:58.400]   Data collectors.
[00:40:58.400 --> 00:41:00.560]   "And employees who signed written and acknowledged."
[00:41:00.560 --> 00:41:01.520]   Not acknowledging.
[00:41:01.520 --> 00:41:02.080]   Not acknowledging collectors.
[00:41:02.080 --> 00:41:03.280]   Right.
[00:41:03.280 --> 00:41:07.120]   "Ignoring they were sending data streams, including video back to the company for training
[00:41:07.120 --> 00:41:12.080]   purposes. According to iRobot, the devices were labeled with a bright green sticker that
[00:41:12.080 --> 00:41:17.440]   read video recording and progress. And it was up to those paid data collectors to remove
[00:41:17.440 --> 00:41:21.600]   anything they deemed sensitive from any space the robot operates in, including."
[00:41:21.600 --> 00:41:23.520]   So they kind of knew what they were getting into here.
[00:41:23.520 --> 00:41:23.680]   Yeah.
[00:41:23.680 --> 00:41:30.320]   You had a robot that was looking at you and you had a responsibility to monitor it.
[00:41:30.320 --> 00:41:32.960]   By the way, MIT published the photos.
[00:41:32.960 --> 00:41:35.200]   So, if you're curious.
[00:41:35.200 --> 00:41:37.680]   That's going a little far.
[00:41:37.680 --> 00:41:38.880]   Okay.
[00:41:38.880 --> 00:41:39.520]   Well, hold on.
[00:41:39.520 --> 00:41:43.520]   So the reason I put this in here wasn't actually, they kind of, to me, they buried the lead.
[00:41:43.520 --> 00:41:45.440]   And I understand why they buried the lead.
[00:41:45.440 --> 00:41:49.360]   But here's the big story that we should be talking about, which is
[00:41:49.360 --> 00:41:51.600]   it's somewhere down in the story.
[00:41:51.600 --> 00:41:56.480]   Together, the images reveal an entire data supply chain in new points where personal
[00:41:56.480 --> 00:41:59.920]   information can leak out that few consumers are even aware of.
[00:42:00.640 --> 00:42:07.520]   And this comes back to things that we talked about with Amazon and data utterances.
[00:42:07.520 --> 00:42:12.640]   Remember when everybody was upset because Amazon, Apple, and Google sent actual utterances to
[00:42:12.640 --> 00:42:17.200]   people for annotation to say like, "Hey, is this right?
[00:42:17.200 --> 00:42:19.520]   Did we do what we were supposed to do so the AI could get better?"
[00:42:19.520 --> 00:42:24.080]   So I think the issue here is twofold.
[00:42:24.080 --> 00:42:29.200]   One, consumers need to understand what sensors are on their devices and pay attention to it.
[00:42:29.200 --> 00:42:30.400]   Like we have a little bit of a risk.
[00:42:30.400 --> 00:42:34.320]   But like if someone says, "Hey, this has a camera on it," maybe you don't put it in your bathroom.
[00:42:34.320 --> 00:42:39.440]   But two, companies have, well, I mean-
[00:42:39.440 --> 00:42:41.600]   Yeah, but if it's a vacuum center that's wandering around you,
[00:42:41.600 --> 00:42:42.560]   well, it's doing his job.
[00:42:42.560 --> 00:42:46.160]   Well, and the other thing is the person who brought this into the house may not be the girl.
[00:42:46.160 --> 00:42:51.280]   This is why I always tell everybody in my household the sensors on every device I bring in.
[00:42:51.280 --> 00:42:52.720]   You don't always wait.
[00:42:52.720 --> 00:42:54.240]   MIT has a technology-
[00:42:54.240 --> 00:42:54.880]   There's a technology issue within there.
[00:42:54.880 --> 00:42:59.360]   Timothy Techinger, if you have a picture of a nine-year-old boy who couldn't legally give consent.
[00:42:59.360 --> 00:43:03.440]   His picture was in that data set.
[00:43:03.440 --> 00:43:13.040]   Yeah, and the other thing is, I think I robot handled this well in the sense that they publicly
[00:43:13.040 --> 00:43:15.040]   said, "Hey, we're going to fire a scale AI."
[00:43:15.040 --> 00:43:18.480]   They obviously didn't handle this data correctly.
[00:43:18.480 --> 00:43:28.080]   But companies need to probably implement better ways to handle personal consumer data and
[00:43:28.080 --> 00:43:32.880]   ensure that they're contractors, which of course they're going to be using, does this correctly,
[00:43:32.880 --> 00:43:36.080]   which is something bigger companies are more likely to do than startups.
[00:43:36.080 --> 00:43:40.000]   Like startups are all willy-nilly with your data, like look at Uber and their god mode and
[00:43:40.000 --> 00:43:42.240]   early-bring camera things.
[00:43:42.240 --> 00:43:46.000]   But it's not like our robot has just hopped on to the scene.
[00:43:46.000 --> 00:43:47.600]   They've been around a while, they know.
[00:43:47.600 --> 00:43:52.560]   Well, but they clearly had this company, the people who work at this company,
[00:43:52.560 --> 00:43:56.240]   violated their existing contract terms with iRobot.
[00:43:56.240 --> 00:43:57.280]   That's why they fire them.
[00:43:57.280 --> 00:44:01.840]   Now, I could say something like, "Hey, when a Roomba goes into a bathroom,
[00:44:01.840 --> 00:44:03.680]   automatically turn off the camera."
[00:44:03.680 --> 00:44:04.560]   That's something they could do.
[00:44:04.560 --> 00:44:11.680]   They could also say, "Hey, no phones and no internet connections
[00:44:11.680 --> 00:44:14.400]   when someone is trying to annotate data.
[00:44:14.400 --> 00:44:17.120]   That way this data is harder to get out."
[00:44:17.120 --> 00:44:17.920]   Right?
[00:44:17.920 --> 00:44:20.000]   But that's a lot.
[00:44:20.000 --> 00:44:25.280]   So these are things like debates will probably be happening inside companies.
[00:44:25.280 --> 00:44:31.200]   Yeah, and of course, the other messages to all of our listeners,
[00:44:31.200 --> 00:44:35.440]   you aren't being paranoid when you're saying no cameras in the house,
[00:44:35.440 --> 00:44:39.760]   that it is not unreasonable that sometimes humans see those images.
[00:44:39.760 --> 00:44:43.520]   And you don't really know and you don't really control it.
[00:44:44.320 --> 00:44:48.080]   And yeah, Roomba and the scale AI fired those people,
[00:44:48.080 --> 00:44:50.560]   but people were looking at those images.
[00:44:50.560 --> 00:44:54.080]   It doesn't help that we have
[00:44:54.080 --> 00:44:59.840]   not amaze in our houses and then the media go out and do a story about it.
[00:44:59.840 --> 00:45:01.840]   They're all in the New York Post just in this today.
[00:45:01.840 --> 00:45:03.520]   They're almost into you all the time.
[00:45:03.520 --> 00:45:05.920]   There's no gradation to-
[00:45:05.920 --> 00:45:09.200]   Yeah, that's a good point on the other side,
[00:45:09.200 --> 00:45:12.800]   which is just don't assume that everything is doing this.
[00:45:13.360 --> 00:45:15.840]   You got to be somewhat intelligent.
[00:45:15.840 --> 00:45:17.440]   On the other hand, it's kind of hard to know.
[00:45:17.440 --> 00:45:24.240]   Well, and so here, here y'all, I'd like set you up for this.
[00:45:24.240 --> 00:45:30.240]   Here's where I think the US government could help because right now they're working on a
[00:45:30.240 --> 00:45:31.360]   cybersecurity label.
[00:45:31.360 --> 00:45:35.920]   The focus is on both cybersecurity and it was going to be on privacy.
[00:45:35.920 --> 00:45:38.880]   They brought people in to talk about privacy and then the government basically said,
[00:45:38.880 --> 00:45:41.040]   "I don't know about privacy.
[00:45:41.040 --> 00:45:42.880]   That's real hard.
[00:45:42.880 --> 00:45:44.000]   We don't want to make that call."
[00:45:44.000 --> 00:45:49.040]   This is a place where we could, as part of the cybersecurity label,
[00:45:49.040 --> 00:45:52.160]   that should come out in spring of next year,
[00:45:52.160 --> 00:45:55.280]   they should place data on what sensors are on a device.
[00:45:55.280 --> 00:45:56.640]   Does this have a camera?
[00:45:56.640 --> 00:46:00.400]   Like you don't necessarily expect your robotic vacuum to have a camera, for example.
[00:46:00.400 --> 00:46:01.200]   Does it light up?
[00:46:01.200 --> 00:46:02.400]   Maybe you should make it light up.
[00:46:02.400 --> 00:46:09.360]   Making it more aware or making consumers more aware of what's on their devices has
[00:46:10.000 --> 00:46:13.600]   importance. That could actually happen.
[00:46:13.600 --> 00:46:15.120]   It's not right now going to happen.
[00:46:15.120 --> 00:46:16.880]   That's not where the government's interest is.
[00:46:16.880 --> 00:46:21.040]   But if we complain enough about it, it's possible that they would do it.
[00:46:21.040 --> 00:46:23.760]   Here's Stacey's article, Stacey on IOT.com.
[00:46:23.760 --> 00:46:28.000]   We should talk about consent in IOT.
[00:46:28.000 --> 00:46:33.520]   Of course, that is very appropriate with the news that UFI
[00:46:35.040 --> 00:46:41.280]   was in effect lying that their images were not sent off over the internet from their cameras.
[00:46:41.280 --> 00:46:44.240]   I have nine rules for consent.
[00:46:44.240 --> 00:46:45.600]   They're all down there.
[00:46:45.600 --> 00:46:46.720]   That's a lot of rules.
[00:46:46.720 --> 00:46:47.760]   You couldn't come up with ten?
[00:46:47.760 --> 00:46:49.360]   You couldn't come up with an even more.
[00:46:49.360 --> 00:46:50.800]   I'm a really bad--
[00:46:50.800 --> 00:46:52.320]   I'm a really bad--
[00:46:52.320 --> 00:46:53.200]   Let's get another rule.
[00:46:53.200 --> 00:46:54.560]   You're bad at Linkbait, kid.
[00:46:54.560 --> 00:46:55.840]   You got to say--
[00:46:55.840 --> 00:46:56.320]   I really am.
[00:46:56.320 --> 00:46:58.720]   The title of this headline should be
[00:46:58.720 --> 00:47:03.040]   "Ten Things You Need to Know About IOT Privacy."
[00:47:03.040 --> 00:47:03.920]   Yes.
[00:47:04.720 --> 00:47:07.760]   Ten rules to protect you and your family from horrible--
[00:47:07.760 --> 00:47:11.520]   It's a list of co-stacey and you didn't even go to the list of co-stacey's.
[00:47:11.520 --> 00:47:13.040]   And it's so long.
[00:47:13.040 --> 00:47:17.280]   My husband's like, "My God, Stacey, just put these things in there."
[00:47:17.280 --> 00:47:18.320]   I'm like, "Oh, I'm bad at it."
[00:47:18.320 --> 00:47:20.800]   Provide transparency about your data collection practice.
[00:47:20.800 --> 00:47:21.680]   Let's look at this.
[00:47:21.680 --> 00:47:24.400]   Provide transparency around the sensors inside the device.
[00:47:24.400 --> 00:47:25.760]   Yeah, if there's a camera in there,
[00:47:25.760 --> 00:47:28.080]   you should say there's a camera in there and a microphone.
[00:47:28.080 --> 00:47:32.000]   Protect the user's data through encryption, at rest, and in motion.
[00:47:32.000 --> 00:47:33.040]   Absolutely.
[00:47:33.040 --> 00:47:35.840]   Promote safe data practices with partners.
[00:47:35.840 --> 00:47:36.240]   Absolutely.
[00:47:36.240 --> 00:47:38.560]   That's part of the problem with this scale AI, right?
[00:47:38.560 --> 00:47:42.800]   Develop a clear practice around the use of data after a merger acquisition.
[00:47:42.800 --> 00:47:44.400]   Big story there, right?
[00:47:44.400 --> 00:47:49.200]   Look at that owl cam that I have, the dash cam,
[00:47:49.200 --> 00:47:52.160]   which went bankrupt, closed their doors.
[00:47:52.160 --> 00:47:53.200]   Somebody bought them.
[00:47:53.200 --> 00:47:56.720]   I guess they got all the videos of me singing in my car,
[00:47:56.720 --> 00:47:59.760]   picking my nose, mooning the other drivers.
[00:47:59.760 --> 00:48:00.480]   It's all there.
[00:48:01.360 --> 00:48:03.360]   Develop and explain your data.
[00:48:03.360 --> 00:48:07.360]   Develop and explain your data deletion policy
[00:48:07.360 --> 00:48:10.160]   and give consumers a chance to delete their data.
[00:48:10.160 --> 00:48:10.880]   Absolutely.
[00:48:10.880 --> 00:48:13.600]   GDPR kind of requires that, right?
[00:48:13.600 --> 00:48:13.840]   Yes.
[00:48:13.840 --> 00:48:14.960]   Promise users-
[00:48:14.960 --> 00:48:16.400]   Yeah, I wrote this in 2020.
[00:48:16.400 --> 00:48:17.360]   Oh, really?
[00:48:17.360 --> 00:48:18.640]   Oh, you're a prescient.
[00:48:18.640 --> 00:48:20.960]   You are so ahead of the curve.
[00:48:20.960 --> 00:48:24.480]   Promise users the device will work for X number of years.
[00:48:24.480 --> 00:48:26.320]   God, we know that story.
[00:48:26.320 --> 00:48:27.440]   Look at Wink.
[00:48:27.440 --> 00:48:30.080]   Patch devices and owl.
[00:48:30.080 --> 00:48:31.280]   Owl stopped working.
[00:48:31.280 --> 00:48:33.840]   Patch devices in the wake of new vulnerabilities.
[00:48:33.840 --> 00:48:36.000]   Again, if you could,
[00:48:36.000 --> 00:48:38.800]   anchor, sold that camera,
[00:48:38.800 --> 00:48:40.320]   I'm sorry, wise,
[00:48:40.320 --> 00:48:42.640]   sold that camera that they couldn't patch.
[00:48:42.640 --> 00:48:44.240]   They couldn't patch.
[00:48:44.240 --> 00:48:48.400]   Push users to ask for consent from others in their environment.
[00:48:48.400 --> 00:48:49.760]   Yeah, especially if you have kids.
[00:48:49.760 --> 00:48:52.800]   These are good, Stacey.
[00:48:52.800 --> 00:48:54.160]   It's very good.
[00:48:54.160 --> 00:48:54.960]   These are really good.
[00:48:54.960 --> 00:48:57.520]   There's a reason I do what I do.
[00:48:57.520 --> 00:48:58.400]   Yes, you're doing it.
[00:48:58.400 --> 00:49:00.480]   How do I find it on your site?
[00:49:00.480 --> 00:49:02.480]   Well, I'm going to actually use that for something I'm working on.
[00:49:02.480 --> 00:49:04.480]   Stacey on IoT consent.
[00:49:04.480 --> 00:49:05.040]   Or consent.
[00:49:05.040 --> 00:49:06.480]   But you know what, Jeff,
[00:49:06.480 --> 00:49:08.800]   I have a PowerPoint all about consent.
[00:49:08.800 --> 00:49:10.560]   So I can share-
[00:49:10.560 --> 00:49:11.920]   Can you seriously send that to me?
[00:49:11.920 --> 00:49:12.240]   Yes.
[00:49:12.240 --> 00:49:12.880]   Is it called-
[00:49:12.880 --> 00:49:13.200]   Yeah, I sent it to us.
[00:49:13.200 --> 00:49:14.160]   No means no.
[00:49:14.160 --> 00:49:17.600]   No, it probably is called like-
[00:49:17.600 --> 00:49:20.000]   Ten things you need, you know.
[00:49:20.000 --> 00:49:20.960]   You should ask.
[00:49:20.960 --> 00:49:22.880]   That one only has seven rules, I think.
[00:49:22.880 --> 00:49:24.000]   But yeah.
[00:49:24.000 --> 00:49:24.800]   Oh, Stacey.
[00:49:24.800 --> 00:49:25.920]   Five or ten.
[00:49:25.920 --> 00:49:27.280]   Not nine.
[00:49:27.280 --> 00:49:28.240]   Four three.
[00:49:28.240 --> 00:49:29.040]   Three's good, too.
[00:49:29.040 --> 00:49:29.840]   Peace.
[00:49:29.840 --> 00:49:30.720]   If you want to be really-
[00:49:30.720 --> 00:49:31.200]   It's fine.
[00:49:31.200 --> 00:49:32.160]   Three's fine.
[00:49:32.160 --> 00:49:33.520]   Or let me ask Stacey one more question.
[00:49:33.520 --> 00:49:35.520]   It can be one thing,
[00:49:35.520 --> 00:49:42.000]   but what is the mutual obligation from the purchaser or the user?
[00:49:42.000 --> 00:49:43.600]   What should we expect of them?
[00:49:43.600 --> 00:49:45.120]   Anything?
[00:49:45.120 --> 00:49:47.600]   The purchaser to the user?
[00:49:47.600 --> 00:49:48.560]   No, no, the purchaser.
[00:49:48.560 --> 00:49:50.400]   The person who buys the IoT.
[00:49:50.400 --> 00:49:50.880]   Yeah.
[00:49:50.880 --> 00:49:52.560]   You buy and put this stuff in your house.
[00:49:52.560 --> 00:49:53.200]   Do you have-
[00:49:53.200 --> 00:49:53.520]   So what-
[00:49:53.520 --> 00:49:54.480]   Do you have an obligation?
[00:49:54.480 --> 00:49:55.760]   Is there something we should expect to use?
[00:49:55.760 --> 00:49:56.720]   Yes, I have an obligation.
[00:49:56.720 --> 00:49:58.800]   That's like, I think it's rule number eight.
[00:49:58.800 --> 00:49:59.360]   It's-
[00:49:59.360 --> 00:50:02.160]   If you had read the articles, Jeff, you would know rule eight.
[00:50:02.160 --> 00:50:05.360]   No, no, no, no, no, but it's push users.
[00:50:05.360 --> 00:50:09.760]   So my job when I buy a connected device is to understand what it does,
[00:50:09.760 --> 00:50:12.000]   and then to tell everybody in my environment what it is.
[00:50:12.000 --> 00:50:12.000]   Okay.
[00:50:12.000 --> 00:50:12.480]   Okay.
[00:50:12.480 --> 00:50:12.960]   But-
[00:50:12.960 --> 00:50:15.200]   And my husband and I actually fight about this sometimes.
[00:50:15.200 --> 00:50:18.560]   Like when we put cameras up when we're gone and like the housekeepers are going to come,
[00:50:18.560 --> 00:50:21.680]   he's like, I want to tell them.
[00:50:21.680 --> 00:50:24.880]   I want to say, hey, by the way, we put up cameras because we're out of town.
[00:50:24.880 --> 00:50:27.200]   And he's like, no, I don't want them to know they're cameras.
[00:50:27.200 --> 00:50:30.320]   I was like, if your end goal is for them not to steal our stuff,
[00:50:30.320 --> 00:50:32.240]   telling them there are cameras is going to be fine.
[00:50:32.240 --> 00:50:33.040]   Who won?
[00:50:33.040 --> 00:50:34.960]   Um, I did.
[00:50:34.960 --> 00:50:35.840]   Of course, she's-
[00:50:35.840 --> 00:50:37.040]   Come on, really?
[00:50:37.040 --> 00:50:37.520]   It was-
[00:50:37.520 --> 00:50:38.960]   Well, it was something like this.
[00:50:38.960 --> 00:50:39.360]   I mean,
[00:50:39.360 --> 00:50:44.480]   he wins a lot of arguments, but if I really care,
[00:50:44.480 --> 00:50:47.280]   my caring about something almost always-
[00:50:47.280 --> 00:50:47.760]   Oh, we know.
[00:50:47.760 --> 00:50:48.400]   I care more than he does.
[00:50:48.400 --> 00:50:49.040]   Stacy, we know.
[00:50:49.040 --> 00:50:49.280]   Yeah.
[00:50:49.280 --> 00:50:50.240]   No, we know how to-
[00:50:50.240 --> 00:50:51.680]   We know how to get games played.
[00:50:51.680 --> 00:50:53.760]   Stacy cares, folks.
[00:50:53.760 --> 00:50:54.800]   Stacy wears-
[00:50:56.480 --> 00:51:02.080]   The new champion Instagram photo goes to-
[00:51:02.080 --> 00:51:04.400]   And I don't think this is spoiler anymore-
[00:51:04.400 --> 00:51:08.640]   The new champions of the world and Lionel Messi.
[00:51:08.640 --> 00:51:11.680]   Campiones del Mundo!
[00:51:11.680 --> 00:51:14.480]   Oh, of course it was his-
[00:51:14.480 --> 00:51:18.880]   It was penalty kick that won the game for Argentina.
[00:51:18.880 --> 00:51:24.960]   And it is now the holder of the new record for most Instagram likes.
[00:51:24.960 --> 00:51:26.160]   Or into-
[00:51:26.160 --> 00:51:28.960]   70.7 million likes.
[00:51:28.960 --> 00:51:30.800]   70 for-
[00:51:30.800 --> 00:51:31.760]   Wait, after this story?
[00:51:31.760 --> 00:51:34.400]   But I mean, it is the number one sport on the planet.
[00:51:34.400 --> 00:51:34.800]   Yeah.
[00:51:34.800 --> 00:51:36.080]   Yeah, 3.5 billion people-
[00:51:36.080 --> 00:51:36.960]   Do you watch the game?
[00:51:36.960 --> 00:51:38.240]   No.
[00:51:38.240 --> 00:51:38.800]   I watch it-
[00:51:38.800 --> 00:51:39.440]   I watch the-
[00:51:39.440 --> 00:51:39.920]   I watch the-
[00:51:39.920 --> 00:51:39.920]   I watch the-
[00:51:39.920 --> 00:51:41.040]   I watch the last part.
[00:51:41.040 --> 00:51:42.480]   I watch the whole thing.
[00:51:42.480 --> 00:51:42.880]   It was-
[00:51:42.880 --> 00:51:44.320]   It was unbelievable.
[00:51:44.320 --> 00:51:45.200]   I really enjoyed it.
[00:51:45.200 --> 00:51:46.400]   I don't know from football.
[00:51:46.400 --> 00:51:47.360]   I watched the whole thing.
[00:51:47.360 --> 00:51:48.480]   I was amazed.
[00:51:48.480 --> 00:51:49.520]   Yeah, it was pretty good.
[00:51:49.520 --> 00:51:50.080]   Really?
[00:51:50.080 --> 00:51:52.800]   See, all I got out of it is now I understand why football's
[00:51:52.800 --> 00:51:53.920]   number one in the US.
[00:51:54.880 --> 00:51:57.360]   I was so fast there in the last-
[00:51:57.360 --> 00:51:58.560]   It was a very exciting-
[00:51:58.560 --> 00:51:58.720]   You had a very exciting-
[00:51:58.720 --> 00:52:00.160]   Ten minutes of regulation.
[00:52:00.160 --> 00:52:01.920]   Everything was happening so fast.
[00:52:01.920 --> 00:52:02.640]   Yeah, yeah.
[00:52:02.640 --> 00:52:03.440]   It was pretty cool.
[00:52:03.440 --> 00:52:05.520]   Yeah, I tuned in towards the end and it was still tied.
[00:52:05.520 --> 00:52:06.800]   It was very exciting.
[00:52:06.800 --> 00:52:07.360]   Yeah.
[00:52:07.360 --> 00:52:07.840]   Yeah.
[00:52:07.840 --> 00:52:09.360]   I want to show you a really neat thread.
[00:52:09.360 --> 00:52:10.560]   At least I'll put it in the-
[00:52:10.560 --> 00:52:11.760]   By the way, the post-
[00:52:11.760 --> 00:52:13.920]   the previous record holder-
[00:52:13.920 --> 00:52:15.520]   a stock photo of an egg.
[00:52:15.520 --> 00:52:19.200]   So don't get all excited.
[00:52:19.200 --> 00:52:20.080]   Lionel Messi-
[00:52:20.080 --> 00:52:21.120]   Oh, I remember this.
[00:52:21.120 --> 00:52:21.760]   I-
[00:52:21.760 --> 00:52:23.200]   This was an ask.
[00:52:23.200 --> 00:52:24.560]   Okay, after this, can we do an ad?
[00:52:24.560 --> 00:52:25.680]   Because you told me this is how-
[00:52:25.680 --> 00:52:26.960]   we measure how long the shows are.
[00:52:26.960 --> 00:52:27.920]   Yeah, could I do an ad right now?
[00:52:27.920 --> 00:52:28.800]   Oh.
[00:52:28.800 --> 00:52:34.320]   This picture of an egg has 58 million likes on Instagram.
[00:52:34.320 --> 00:52:39.680]   Beating Kylie Jenner's 18 million world record.
[00:52:39.680 --> 00:52:40.400]   Thank God.
[00:52:40.400 --> 00:52:43.920]   Messi has now got 70 million.
[00:52:43.920 --> 00:52:44.880]   That's all I can say.
[00:52:44.880 --> 00:52:46.240]   I didn't realize his age.
[00:52:46.240 --> 00:52:47.760]   I thought he was a little bit younger.
[00:52:47.760 --> 00:52:48.160]   Oh, yeah.
[00:52:48.160 --> 00:52:49.040]   He's the old man.
[00:52:49.040 --> 00:52:49.920]   Yeah.
[00:52:49.920 --> 00:52:51.200]   He's like Tom Brady, man.
[00:52:51.200 --> 00:52:51.600]   Yeah.
[00:52:51.600 --> 00:52:52.880]   For some reason, I thought he was a little
[00:52:52.880 --> 00:52:57.360]   younger, so I was like why everybody just giving it. Making it such a big deal. Oh yeah, probably
[00:52:57.360 --> 00:53:03.560]   his last chance. Yeah, my men and I being going to show off ski is a columnist for the
[00:53:03.560 --> 00:53:08.360]   New Yorker writing about Latino matters. And she wrote two columns about messy in the New
[00:53:08.360 --> 00:53:14.880]   Yorker and explaining. And she's also Argentinian explaining the meaning of this to Argentina
[00:53:14.880 --> 00:53:22.020]   and people and and his momating to give back to the country, it was just phenomenal to
[00:53:22.020 --> 00:53:27.440]   be. He has 404 million followers on Instagram by the way. So he's underperforming. He could
[00:53:27.440 --> 00:53:35.900]   have he should have more. The number one person on Instagram is his old rival Cristiano Ronaldo
[00:53:35.900 --> 00:53:42.620]   520 followers, but Messi's got the World Cup. That's what not always already retired.
[00:53:42.620 --> 00:53:50.180]   Yeah, yeah, I think. Yeah, that is on the way up and back is amazing hat trick. But
[00:53:50.180 --> 00:53:55.740]   also I put up a thread online. 85 would you know, we can't because Miss Stacy has said
[00:53:55.740 --> 00:53:59.540]   it's waffle time. This is when you shake your head no and I hear the bell and then all
[00:53:59.540 --> 00:54:07.160]   that all spots. No, Jeff, no, it's the opposite of Pavlov. It's the no food free. I wore this
[00:54:07.160 --> 00:54:12.620]   bell earlier on Windows weekly and Bert came running in with a giant wrench saying crush
[00:54:12.620 --> 00:54:19.340]   the bell. Is that the show title? I am not crushing my bell for anyone. I like they
[00:54:19.340 --> 00:54:23.340]   made it came in with the tool. He didn't just like say this is a problem. He was like this
[00:54:23.340 --> 00:54:27.700]   a problem. Here's how we solve it. All solution. This is giant. I love that.
[00:54:27.700 --> 00:54:36.540]   Oh, scissors might have done it. I'm not crushing my bell for anyone. Or actually we
[00:54:36.540 --> 00:54:40.460]   have a sponsor for the show and I want to welcome them because this is a company I've
[00:54:40.460 --> 00:54:44.860]   been working with for I think a decade and I've been dying to get them on the air because
[00:54:44.860 --> 00:54:51.420]   I've been recommending them for free for years. I've all one of my number one precepts these
[00:54:51.420 --> 00:55:00.340]   days is if you care about email, you should be paying for it. Free email is is worth every
[00:55:00.340 --> 00:55:05.700]   penny you're paying for it in terms of support in terms of quality on and on and on. And
[00:55:05.700 --> 00:55:09.380]   when you're if you're on Gmail and you lose your account, good luck getting it back. It's
[00:55:09.380 --> 00:55:18.140]   all I can say. That's why I've used fast mail for years. It's why I'm a huge fast mail fan.
[00:55:18.140 --> 00:55:21.020]   There's another reason you don't want to do free email with free email. You pay with your
[00:55:21.020 --> 00:55:28.420]   privacy for over 20 years now. Fast mail has been a leader in email and email privacy at
[00:55:28.420 --> 00:55:33.460]   fast mail. Your data stays yours with better productivity features and it's not expensive
[00:55:33.460 --> 00:55:39.340]   as little as $3 a month. $3 a month is worth paying if email is important to you to your
[00:55:39.340 --> 00:55:47.380]   business. If you care about your email, you need fast mail and all I love the features
[00:55:47.380 --> 00:55:54.060]   on fast mail. Certainly privacy is a big deal. Your personal data is stored in the US. I
[00:55:54.060 --> 00:55:59.060]   want to make sure you understand that and it's fully GDPR compliant. It's kept away
[00:55:59.060 --> 00:56:05.020]   from third parties because fast mail's business is you. So they don't have to sell your data
[00:56:05.020 --> 00:56:11.220]   to anybody else. Plus they give you the best spam filters. If you're a real geek, you'll
[00:56:11.220 --> 00:56:17.220]   really appreciate the implementation of iMap that fast mail is actually a leader in the
[00:56:17.220 --> 00:56:22.900]   open source community in iMap contributing back to Cyrus, their server and to a lot of
[00:56:22.900 --> 00:56:32.700]   technologies. They lead the way frankly. They also use SIV. You can do more automated email
[00:56:32.700 --> 00:56:38.100]   filtering for spam but I love having absolute control with my SIV rules. They have fantastic
[00:56:38.100 --> 00:56:45.300]   filtering rules. It is the best email. Now with fast mail, you can use masked email so
[00:56:45.300 --> 00:56:49.500]   you don't have to use your real address when you sign up for something. In fact, you can
[00:56:49.500 --> 00:56:54.660]   create unlimited aliases. One of the things I think a lot of people use fast mail don't
[00:56:54.660 --> 00:57:01.100]   know about is the fast mail allows you to host your domain at fast mail. I wouldn't put
[00:57:01.100 --> 00:57:09.940]   a website there but with fast mail I have maybe, I don't know, 15, a dozen email services,
[00:57:09.940 --> 00:57:15.300]   email names, all of them hosted at fast mail. One of the reasons I like to use fast mail
[00:57:15.300 --> 00:57:23.140]   for that, let's pick one here. Leo.fm for instance. That's my website. So I use their
[00:57:23.140 --> 00:57:28.740]   DNS but one of the reasons I also like to use it is because now when email is sent to
[00:57:28.740 --> 00:57:38.100]   Leo.fm, it is using all of the most important strong email authentication technologies. So
[00:57:38.100 --> 00:57:46.500]   it's fast mail automatically does all of that. D-Kim and SPF and D-Mark. So that means more
[00:57:46.500 --> 00:57:51.900]   mail is going to get through, your mail is authenticated. It also means you have unlimited
[00:57:51.900 --> 00:57:59.220]   anything that's sent to any address at Leo.fm comes into my inbox. So when I sign up for
[00:57:59.220 --> 00:58:05.100]   something, I sign up with a company's name. I can use random names. It means I can be
[00:58:05.100 --> 00:58:10.060]   more secure. In fact, it works with our sponsor Bitwarden. You can create random names for
[00:58:10.060 --> 00:58:15.140]   every single account which is like having two passwords and Bitwarden will keep track
[00:58:15.140 --> 00:58:20.100]   of it. But it's not just about privacy and security. You can customize your workflow.
[00:58:20.100 --> 00:58:24.980]   They have a fantastic web mail client. One of the best I've ever used but it also works
[00:58:24.980 --> 00:58:31.060]   with every other desktop mail client out looking. I use mail made. I use Apple mail.
[00:58:31.060 --> 00:58:35.540]   I use a whole variety of stuff. It all works great with fast mail. But I think you'll like
[00:58:35.540 --> 00:58:40.740]   the web version as well. You can customize your workflow with colors, custom swipes right
[00:58:40.740 --> 00:58:46.020]   and left. You got night mode. You don't have to use that, Jeff. They got day mode for Jeff.
[00:58:46.020 --> 00:58:53.180]   You can organize your inbox with scheduled send which is fantastic. Send something a week
[00:58:53.180 --> 00:58:57.940]   from now. You can snooze. You have folders. You have labels and folders. If you like
[00:58:57.940 --> 00:59:03.020]   Gmail's labels, you've got that but you also have folders, real iMap style folders, search
[00:59:03.020 --> 00:59:07.700]   bar, fantastic search. I never have to worry about finding old messages. I can just search
[00:59:07.700 --> 00:59:12.100]   from very quickly and easily. You keep track of all of your important details in your life.
[00:59:12.100 --> 00:59:16.420]   By the way, fast mail, because it's a real iMap server, it also supports contacts. It
[00:59:16.420 --> 00:59:22.780]   also supports calendars. One of my fast mail tips that I really love is I add anybody that
[00:59:22.780 --> 00:59:28.660]   I want to get mail from to my contacts. It's my address book. Why not? Fast mail, I can
[00:59:28.660 --> 00:59:33.420]   set up a filter so that if you're in my contact list and you send me an email, it goes right
[00:59:33.420 --> 00:59:39.700]   into the important folder. I never miss email from people I know. That's by itself the single
[00:59:39.700 --> 00:59:48.300]   most useful filter ever. You can send and receive emails from any domain you own. I have aliases
[00:59:48.300 --> 00:59:54.700]   for all my domains. You can manage multiple email addresses in one space, works with Bitwarden,
[00:59:54.700 --> 00:59:58.820]   works with one password so you can create unique passwords and unique emails for every
[00:59:58.820 --> 01:00:03.380]   account. Desktop mobile, what else can I tell you about this? When you download the
[01:00:03.380 --> 01:00:07.220]   fast mail app, you'll get the most out of your email. In fact, it's actually the best
[01:00:07.220 --> 01:00:12.860]   iOS and Android app. If you use fast mail, use their app because it's fantastic. I can
[01:00:12.860 --> 01:00:17.580]   go on and on. I'm such a fan of fast mail and I have 10 years of pent up wanting to tell
[01:00:17.580 --> 01:00:22.580]   you about fast mail. I'm so glad they're a new advertiser. I look forward to advertising
[01:00:22.580 --> 01:00:27.660]   all next year with fast mail. They have a US-based support team. They're really good.
[01:00:27.660 --> 01:00:34.860]   Email experts. You always wanted an email administrator who really knew what was going
[01:00:34.860 --> 01:00:38.540]   on and could explain it to you and you could say, "Can I do this?" They go, "Oh, yes.
[01:00:38.540 --> 01:00:43.500]   Very easy. Let me set it up." I love that. The fast mail team believes in working for
[01:00:43.500 --> 01:00:51.220]   customers as people to be cared for, not products to be exploited. Advertisers layer left out,
[01:00:51.220 --> 01:00:57.140]   putting you in your privacy at center. I use fast mail. I love fast mail. It's fast. It's
[01:00:57.140 --> 01:01:02.300]   easy to use. It cares about privacy. It's got all the tools I want. If you're a more advanced
[01:01:02.300 --> 01:01:10.300]   user, trust me. The sky's the limit I use fast mail to the hilt. It's just the best. You'll
[01:01:10.300 --> 01:01:14.420]   never lose anything. It's easy to download your old data if you want, import it into
[01:01:14.420 --> 01:01:19.580]   your new fast mail inbox and vice versa. If you ever decide to leave fast mail, don't
[01:01:19.580 --> 01:01:25.460]   worry. It's not a trap. You can get everything out of it. They're constantly working on new
[01:01:25.460 --> 01:01:32.180]   internet standards. Always open, by the way, always standards-based. Frankly, other email
[01:01:32.180 --> 01:01:41.180]   providers are following fast mail's lead. This new year, new you, new email, please. Do
[01:01:41.180 --> 01:01:46.900]   yourself a favor. Check out fast mail. Reclaim your privacy. Boost your productivity. Make
[01:01:46.900 --> 01:01:52.420]   email yours with fast mail. Just ask the geeks in your life, the nerds in your life. You heard
[01:01:52.420 --> 01:01:57.020]   of fast mail? I guarantee you. They'll all say, "Oh, yeah, that's the best. It's the best."
[01:01:57.020 --> 01:02:05.380]   Try it free for 30 days. Fastmail.com/twit. F-A-S-T-M-A-I-L.com/twit. And I love this. We
[01:02:05.380 --> 01:02:10.580]   were able to arrange with them a 15% discount on your whole first year. But you've got to
[01:02:10.580 --> 01:02:17.820]   sign up now and you've got to use that address. Fastmail.com/twit. I've been begging these
[01:02:17.820 --> 01:02:24.980]   guys, "Let me tell the world about you. I am such a fan. Fastmail.com/twit." We welcome
[01:02:24.980 --> 01:02:30.940]   to our show to Twit. I look forward to a great year telling you all about the features. I
[01:02:30.940 --> 01:02:36.220]   said, "Can I do a fast mail tip of the day?" And they said, "Yeah, yeah. You do whatever
[01:02:36.220 --> 01:02:41.220]   you want. So you're going to be hearing a lot more about fast mail as if you hadn't
[01:02:41.220 --> 01:02:50.180]   already." Let's see. Looks like... I want to give Mr. Ant Pruitt a moment of grief. Yes.
[01:02:50.180 --> 01:02:56.420]   Because last week... Yes. He watched... No funny hats going to help Ant. Last week, you
[01:02:56.420 --> 01:03:02.860]   gave me grief on the show for not being here. I heard you. I saw that. And you earned it.
[01:03:02.860 --> 01:03:08.860]   Did you not earn that grief? I'm here all the time. Ant. Ant. He's got a tape now. So
[01:03:08.860 --> 01:03:13.980]   it's coming down the side of his head like you're a one-eared basset hound. That is not...
[01:03:13.980 --> 01:03:20.540]   That is not a good... That is not a good look, man. I'm just saying. Okay, I'll adjust it.
[01:03:20.540 --> 01:03:28.300]   Is it better? Yeah, sure. Now you've got a basset hound with a big tongue. Oh, Lord.
[01:03:28.300 --> 01:03:34.380]   Oh, wait. No, that's where your actual hair is. Don't do that. It's not a tape. Never
[01:03:34.380 --> 01:03:40.620]   okay. I was like, oh. So are you mad at Ant? Because what did he say? I don't even remember.
[01:03:40.620 --> 01:03:44.380]   He said something about you. He said, oh, Jeff Jarvis is near. He says, he's here all the time.
[01:03:44.380 --> 01:03:48.780]   He's not here this week. I'm here all the time. One week, I miss. One week. Leo goes off on
[01:03:48.780 --> 01:03:55.100]   vacations. You go off on vacation. I have not your ones for my job. And you give me crap for it.
[01:03:55.100 --> 01:04:01.900]   Like I said, you earned it. Yeah. You got to be a low person to yell on a one-eared basset hound.
[01:04:01.900 --> 01:04:06.460]   That's all I'm saying. I just want to prove I watched the episode, which was very good.
[01:04:06.460 --> 01:04:09.900]   I was pissed. I wasn't there for the way we missed you. And I have to say, this year,
[01:04:09.900 --> 01:04:13.020]   you've been here a lot more than in past years where you would go to Davos.
[01:04:13.020 --> 01:04:21.980]   Stacy. It's been Stacy. Yes. Let's say it. A-O-A-W-O. But Stacy is. Oh, it's like A-O-L.
[01:04:21.980 --> 01:04:29.740]   People. You're right. Stacy is A-O-L. You've got Stacy. No. I know that Stacy,
[01:04:29.740 --> 01:04:35.740]   because she actually works for a living unlike Jeff and me. Hey. Hey. Has to go and do things.
[01:04:35.740 --> 01:04:41.020]   And you go to conferences. You do things. So that's fine. We're just glad to have you whenever we can.
[01:04:41.020 --> 01:04:45.900]   People were saying, is Stacy leaving the show? And I just want to reassure you. No, you're not
[01:04:45.900 --> 01:04:50.700]   leaving the show. Is that true, Stacy? No, I'm not leaving the show. I love you. Yes. I'm not
[01:04:50.700 --> 01:04:58.140]   leaving the show. Yes. I'm not leaving the show. No, I have no bananas. All right. Now we've,
[01:04:58.140 --> 01:05:02.700]   everybody's gotten a heat now. Things off their channels. I will not be here next week.
[01:05:02.700 --> 01:05:08.220]   No one will be here next week. No one, oh, you won't be here in two weeks on January 4th.
[01:05:08.220 --> 01:05:12.540]   You're going to CES. Yeah. Maybe you could just report in or something.
[01:05:12.540 --> 01:05:18.460]   No. Not during like any CES schedules. I mean, like maybe if I can,
[01:05:18.460 --> 01:05:22.780]   I'll check. Maybe I can do a walk in like on my phone. I'll be like,
[01:05:22.780 --> 01:05:26.140]   Yeah, just do a statement. Call it. You know what? If you did a walk in talking,
[01:05:26.140 --> 01:05:29.180]   actually, that would help us. That'd be really cool. Just give us.
[01:05:29.180 --> 01:05:35.100]   Test it out. Just remember the show floor is an open on the fourth. Oh, oh, yeah. I know.
[01:05:35.100 --> 01:05:40.620]   Well, it's open on Wednesday, isn't it? That's it normally is. No. Okay. Here's,
[01:05:40.620 --> 01:05:46.300]   here's what they shifted it because New Year's Eve is on Sunday. Okay. So it doesn't start till
[01:05:46.300 --> 01:05:52.540]   Thursday the fifth. Yeah, I know those three days. And there's nothing going on at this hour is
[01:05:52.540 --> 01:05:56.940]   there because you could normally you might be able to go to show stoppers the night before a
[01:05:56.940 --> 01:06:02.300]   PEPCOM or well, because I'm going to at two o'clock, I'm going to be at Samsung's press conference
[01:06:02.300 --> 01:06:07.180]   or Samsung's keynote. Basically it's a press or keynote where they go all crazy and Samsung-y.
[01:06:07.180 --> 01:06:09.660]   So after that, I could probably like dial in and be like,
[01:06:09.660 --> 01:06:15.420]   supported from the Samsung. It was it's like, I don't know, but I was watching,
[01:06:15.420 --> 01:06:21.740]   probably with CNN during an election night. And the reporter was at the post election party and
[01:06:21.740 --> 01:06:26.780]   the candidate was speaking. The reporter out there back to the candidate is trying to be heard
[01:06:26.780 --> 01:06:33.260]   over the loud noises of cheering. I mean, you could do it like that. I'm at the party.
[01:06:33.260 --> 01:06:38.940]   You couldn't even hear them. I'll find I'll find a better lanyard mic. Maybe I should say that was
[01:06:38.940 --> 01:06:43.660]   like someone send me a recommendation and I'll pick it up. I will send you. I'm here at the Samsung
[01:06:43.660 --> 01:06:49.660]   show. It is usual there, but I probably have a few laying around. I don't know. Because, well,
[01:06:49.660 --> 01:06:54.620]   I mean, I can just order it. I just don't know. Like y'all y'all do this for a living. So y'all
[01:06:54.620 --> 01:07:01.820]   know what's good. What else what else is in the news we should it's public domain day every every
[01:07:01.820 --> 01:07:07.900]   year on the 1st of January. Duke University, the Center for the Study of Public Domain tells us
[01:07:07.900 --> 01:07:17.980]   what stuff is going into the public domain. January 1st works from 1927 and earlier, of course,
[01:07:17.980 --> 01:07:21.340]   we'll be open to all and actually there's we're starting to get to the era where there's some
[01:07:21.340 --> 01:07:29.100]   really good stuff now that you know, Disney is not extending copyright mouth well into the future.
[01:07:29.100 --> 01:07:35.580]   Yes, we have no bananas. Yes, we have no bananas. Let's see to the lighthouse,
[01:07:35.580 --> 01:07:41.020]   that classic Virginia wolf. Oh, this is the one I'm excited about all of sure much of Sherlock
[01:07:41.020 --> 01:07:46.300]   Holmes. The last of Sherlock Holmes the last of it. So yeah, I've been thinking I might be kind of
[01:07:46.300 --> 01:07:51.420]   fun to do like a reading of all of the works of Sherlock Holmes since you'll still get taken
[01:07:51.420 --> 01:07:57.100]   down for it. I guarantee you will I riot. You think I will try it. We'll see. Will it
[01:07:57.100 --> 01:08:04.220]   gather? A a mill now we are six. Creative Winnie the pooh, Thornton Wilders, the Bridge of San
[01:08:04.220 --> 01:08:10.540]   Louis Ray, Aaron is Timingways men without women. It's the Jeff Jarvis story. Will he?
[01:08:12.140 --> 01:08:21.340]   No, Jeff and Leo story. Well, I'm sorry. Agatha Christie, Edith Wharton. What did I do to you?
[01:08:21.340 --> 01:08:26.300]   Herman has a steppin wolf. I love steppin wolf, but I think that is me back to my my
[01:08:26.300 --> 01:08:33.100]   yeah, psycho curious teen years. Yeah, I think the English translation probably still is in
[01:08:33.100 --> 01:08:39.180]   in the copyright because it was probably subsequent to the German publication. Same thing with Kafka's
[01:08:39.180 --> 01:08:45.740]   America Marcel Proust, the final of installment of De la Reisseurs de de Tompelcue.
[01:08:45.740 --> 01:08:49.500]   Oh, I was hoping you'd say it in front. Oh, wait. Yeah. Nice.
[01:08:49.500 --> 01:08:58.860]   Movies metropolis, the jazz singer, the first that was the first talkie. Yo, I'd seen nothing yet. Wings
[01:08:58.860 --> 01:09:03.980]   Academy Award winner, the larger Hitchcock's first movie.
[01:09:05.420 --> 01:09:09.420]   King of Kings with the Sussleby to Bill. Oh boy. That was an early thing is it's
[01:09:09.420 --> 01:09:16.620]   1927. So these are pretty early movies, right? Yeah. Musical compositions, the best things
[01:09:16.620 --> 01:09:22.940]   in life are free. Now that's true. Ice cream, you scream. We all scream for ice cream.
[01:09:22.940 --> 01:09:29.420]   Putting on the ritz. We can sing that funny face. Wait, wait, wait, Howard Johnson? Like
[01:09:29.420 --> 01:09:34.780]   really Howard Johnson? Yeah. He did like the Howard Johnson. I wondered, do you think it is about
[01:09:34.780 --> 01:09:42.620]   ice cream? They were famous for 28 flavors. Maybe can't help loving that man and old man
[01:09:42.620 --> 01:09:46.700]   river from the musical showboat. That's a different time, folks.
[01:09:46.700 --> 01:09:54.060]   Bessie Smith, Louis Armstrong, Duke Ellington. A lot of stuff coming into public domain.
[01:09:54.060 --> 01:10:01.340]   You want to hear ice cream, you scream, we all scream for ice cream. That's not public domain yet.
[01:10:02.540 --> 01:10:05.660]   Not public domain yet. No, this Howard Johnson was a song lyricist.
[01:10:05.660 --> 01:10:09.340]   Oh, okay. Unducted into the songwriter's hall of favor, 1970. Okay.
[01:10:09.340 --> 01:10:15.420]   Are you going to do? Are you going to do my, you can play my favorite copyright story of the
[01:10:15.420 --> 01:10:20.540]   week of the year. What's that? I want to run down. What's that? Well, it's first, I want to do a quiz.
[01:10:20.540 --> 01:10:28.060]   And Stacey, as the youngins here, you know, Tom Lehrer. Oh, I love this story. Negative.
[01:10:28.060 --> 01:10:34.780]   Tom Lehrer. Oh, at the bottom, Leo, I put up three direct links to three songs,
[01:10:34.780 --> 01:10:42.460]   one of which I sang in the 10th grade in talent shop. So Lehrer who was, was until his retirement
[01:10:42.460 --> 01:10:45.980]   of math professor at the University of California, Santa Cruz, just down the road. Oh, I thought it was
[01:10:45.980 --> 01:10:49.660]   in my tea. Well, I think he made it a bit, but he ended his career at UCSC.
[01:10:50.540 --> 01:10:59.020]   Wrote so many great parody songs before Weird Al. Oh, why did he hear? Yeah. Alan Sherman.
[01:10:59.020 --> 01:11:06.700]   There was in the 50s, this guy named Tom Lehrer. And he has put all of his music,
[01:11:06.700 --> 01:11:13.260]   including the sheet music, into public domain. He's in his 90s now.
[01:11:13.900 --> 01:11:22.460]   '93, I think. Yeah. And I think it's kind of a way of, I don't know, saying, here, you know,
[01:11:22.460 --> 01:11:28.540]   I'm done. You can, you can have these. I'm sure he'd first sell them. It's great. But, you know,
[01:11:28.540 --> 01:11:34.620]   yeah, he's wonderful. The most famous is probably Vatican Rag, line 124. Shall I play that? I'll play
[01:11:34.620 --> 01:11:39.660]   a little bit of it. Well, I think so. We can. We know we can now. He has on his website,
[01:11:39.660 --> 01:11:45.260]   audio, also lyrics sheet music for download. He says, get them now, because I don't know how
[01:11:45.260 --> 01:11:55.580]   long the site will be up. And there's often a live version and a studio version. Do you have a,
[01:11:55.580 --> 01:11:59.180]   I would do the live version. All right. I'll play the live version. Another big news story of the
[01:11:59.180 --> 01:12:05.500]   year concerned the Ecumenical Council in Rome known as Vatican II. This is how old this is.
[01:12:05.500 --> 01:12:10.300]   Among the things they did in an attempt to make the church more commercial
[01:12:10.300 --> 01:12:18.140]   was to use the vernacular and two portions of the mass to replace Latin and to widen somewhat
[01:12:18.140 --> 01:12:22.860]   the range of music permissible in the liturgy. But I feel that if they really want to sell a product
[01:12:22.860 --> 01:12:31.500]   in this secular age, what they ought to do is to redo some of the liturgical music in popular
[01:12:31.500 --> 01:12:40.620]   song forms. I have a modest example here. It's called the Vatican Rag.
[01:12:58.060 --> 01:13:04.700]   I actually, one of my favorites is, of course, his, and it's really kind of a tour to force
[01:13:04.700 --> 01:13:10.060]   his periodic table of the elements. That's amazing. I'll play, I'll play a little bit of this.
[01:13:10.060 --> 01:13:18.780]   He also has released the Aristotle version of that. So first, this is Tom Lair singing the
[01:13:18.780 --> 01:13:24.620]   periodic table of the elements to the tune of I Am the very model of a modern major general.
[01:13:25.180 --> 01:13:34.140]   [Music]
[01:13:34.140 --> 01:13:37.820]   There's an anemone arsenic aluminum selenium and hydrogen and oxygen and nitrogen and
[01:13:37.820 --> 01:13:42.300]   radium and nickel diemium, neptunium, germanium and iron amoreseum, ruthenium, uranium,
[01:13:42.300 --> 01:13:46.460]   europium, zeconium, rutecium, vanadium and lanthanum and osmium and ostatine and radium
[01:13:46.460 --> 01:13:51.820]   and gold protectinium and indium and gallium and iodine and thorium and fulium and phallium.
[01:13:52.780 --> 01:13:59.580]   Amazing. He memorized that and does it all. But he probably should have done the Aristotle version.
[01:13:59.580 --> 01:14:03.180]   It's a little a little bit easier going back a little bit in history. Let me see if I can
[01:14:03.180 --> 01:14:07.020]   find that. It wasn't on the site, was it, John? We had to find it on YouTube.
[01:14:07.020 --> 01:14:12.700]   Let me see if I can find the YouTube version. Well, now you're gonna get taken down.
[01:14:12.700 --> 01:14:21.420]   Maybe, maybe not. This is Tom Lair, the kind of the original, the historic version of this song.
[01:14:22.140 --> 01:14:25.580]   Thank you. You may be interested to know that there is an older, much earlier version of that
[01:14:25.580 --> 01:14:29.340]   song, which is due to Aristotle and which goes like this.
[01:14:29.340 --> 01:14:39.340]   There's earth and air and fire and water. It's a little shorter. You can probably,
[01:14:39.340 --> 01:14:48.380]   everybody could memorize that. Done. That's a new math. So, I wonder what a pollution rate
[01:14:48.380 --> 01:14:53.980]   song. So many raised ones. So, that, okay, as a youngin, I will say this reminds me of
[01:14:53.980 --> 01:15:02.940]   Yako from Animaniacs. He had a periodic table song. He probably was done. Just copying Tom Lair.
[01:15:02.940 --> 01:15:08.700]   So, it's a similar, like, I'm sure if anyone in the chat is like, I can't play that. I'm sure
[01:15:08.700 --> 01:15:13.020]   they're like, oh, yeah. One of brothers would get us. Yeah. They would. They would come after
[01:15:13.020 --> 01:15:18.300]   this is the one that Mr. Jeff Jarvis sang in his 10th grade talent show. Are you ready?
[01:15:18.300 --> 01:15:24.860]   Time was when an American about to go abroad would be warned by his friends or the guidebooks,
[01:15:24.860 --> 01:15:29.660]   not to drink the water. But times have changed and now a foreigner coming to this country
[01:15:29.660 --> 01:15:31.180]   might be offered the following advice.
[01:15:37.740 --> 01:15:45.340]   If you visit American city, you will find it very very pretty. Just two things of which you must be
[01:15:45.340 --> 01:15:52.940]   with. Drink the water and don't worry. Oh, my God. Were you a radical Jeff in your youth? Is that
[01:15:52.940 --> 01:16:02.300]   Oh, yeah. I organized a moratorium march in my high school. I was a pure ale in, I'm gonna
[01:16:02.300 --> 01:16:09.100]   break here. Very rare. I do that. I know. In the eighth grade, the girls were told they could not wear
[01:16:09.100 --> 01:16:16.780]   pants. And I pushed them to have a protest wearing pants. And they all got called to the auditorium
[01:16:16.780 --> 01:16:25.420]   to get sent home. And I called in the ACLU and won. Wow. You had the ACLU on speed dial when you
[01:16:25.420 --> 01:16:30.060]   were in eighth grade. I went to the phone. I went to the phone booth. So my Republican father was
[01:16:30.060 --> 01:16:35.900]   not necessarily happy with me. And I had the number and I picked in a lot of times. And I called the
[01:16:35.900 --> 01:16:40.780]   ACLU and told them what was happening. And that scared the administration and the girls could wear
[01:16:40.780 --> 01:16:47.740]   their pants. Baller today, the boys would just wear dresses and just be like, I mean, no, I mean,
[01:16:47.740 --> 01:16:55.500]   both are good. Yeah, right. I always realized I knew from a young age that I could never run for
[01:16:55.500 --> 01:17:02.780]   president because back in 1972, when I was a high school sophomore, I marched in a protest against
[01:17:02.780 --> 01:17:08.620]   the Vietnam War. And there was a picture of me on the very front page, full color picture of me on
[01:17:08.620 --> 01:17:15.660]   the very front page of the Santa Cruz Sentinel, waving a Vietcong flag. So at that point, I figured
[01:17:15.660 --> 01:17:22.780]   there goes my political career. It's just a matter of opposition research away from from falling apart.
[01:17:23.980 --> 01:17:32.700]   You would Jade Fodck. John Federman, speaking of left wing politics, TikTok
[01:17:32.700 --> 01:17:39.020]   whisperer article in the New York Times, Federman, of course, won as Democratic Senator from Pennsylvania
[01:17:39.020 --> 01:17:46.060]   flipping that seat. And it's because of TikTok, which the Republicans want to ban everywhere.
[01:17:46.060 --> 01:17:50.940]   I wonder if that's why you think it cost us a seat.
[01:17:52.220 --> 01:17:58.860]   You know, it's funny at this point, I think actually Twitter might be more of a threat to our political
[01:17:58.860 --> 01:18:07.180]   politics than TikTok to me. Hell yeah. Under bomb. I guess we can do our brief, very brief,
[01:18:07.180 --> 01:18:12.940]   I promise you, Elon rundown. We did a song for the musk moment.
[01:18:12.940 --> 01:18:16.620]   The musk moment. It's a musky moment in Memphis.
[01:18:17.820 --> 01:18:22.540]   And everybody pays for the club. They don't get that in the show. No, I think it must free show.
[01:18:22.540 --> 01:18:25.820]   Yeah. Oh, you know what? That would really put the membership through the roof, wouldn't it?
[01:18:25.820 --> 01:18:27.740]   Promise you. I'm a member.
[01:18:27.740 --> 01:18:36.540]   Okay. First of all, details on the stalker incident that that tipped Elon over the, I think,
[01:18:36.540 --> 01:18:46.460]   honestly, that's what tipped Elon over fully into madness. He whoever, well, he got, he heard
[01:18:47.100 --> 01:18:55.260]   that a stalker had attacked his child ex. Good news. The child was alone. No parents with this.
[01:18:55.260 --> 01:19:01.820]   What is he? Two year old, but he had his security detail with him. And so he was safe.
[01:19:01.820 --> 01:19:09.420]   But Elon said, you see, you see, when you tweet my jet location, you're given away assassination
[01:19:09.420 --> 01:19:12.860]   coordinates. That was the term he used. And he's miles away from your home.
[01:19:12.860 --> 01:19:18.620]   You see what happens. Well, now we know from the south Pasadena police that actually it wasn't
[01:19:18.620 --> 01:19:25.260]   anywhere. It was far from the airport. And the suspect, the police are investigating in this
[01:19:25.260 --> 01:19:31.420]   entire incident is Elon Musk's security guy, the driver of the car, who apparently pulled his gun
[01:19:31.420 --> 01:19:36.700]   and threatened a random stranger at the gas station. So we'll wait and see what happens there. But
[01:19:36.700 --> 01:19:41.020]   that, and I don't, you know what? Elon probably didn't know what happened. He just got a report
[01:19:41.020 --> 01:19:45.820]   back. Yeah, there was an incident. And so I'm going to give him the benefit of the doubt on that.
[01:19:45.820 --> 01:19:49.740]   At that point, he immediately shut down. And we've talked about this. He shut down the,
[01:19:49.740 --> 01:19:56.780]   the Elon's jet account move that guy, Jack Sweeney, Florida college student moved everything to
[01:19:56.780 --> 01:20:03.020]   mast it on Instagram and Facebook. And this is where your troubles began because people started
[01:20:03.020 --> 01:20:11.340]   tweeting that. And Elon said, no, you can't, you can't tweet that. And he said, no more tweeting.
[01:20:11.340 --> 01:20:17.180]   He took, he made it every time you go to a mastodon page, it says it's, it's, it's malicious. It's
[01:20:17.180 --> 01:20:23.180]   malware, which I find offensive to it. So either that is not malware. It also said that it's failed
[01:20:23.180 --> 01:20:29.660]   to send the tweet. If you're on mobile, yeah, I will block the tweet. And then he took down
[01:20:29.660 --> 01:20:33.180]   journalists. We've reported all this. Now, the first thing they understand is the journalists say,
[01:20:33.180 --> 01:20:37.340]   we're still down. You may see our accounts on Twitter, but we cannot tweet.
[01:20:37.340 --> 01:20:45.580]   So, oh, no. So you know, I saw stories that they were back. Elon said they were back. But they say,
[01:20:45.580 --> 01:20:48.940]   there's there's shot there. We're shot with the opposite of a shadow banning there. There's
[01:20:48.940 --> 01:20:52.060]   there's shadow pinned. Yeah, they can't write. Well, it's kind of what a shadow
[01:20:52.060 --> 01:20:56.380]   ban was supposed to be, although this isn't actually what Twitter did, but that you would,
[01:20:56.380 --> 01:20:59.740]   no, they can't even write, but they can't even send something. Shadow ban is you right and you
[01:20:59.740 --> 01:21:04.300]   don't know that nobody knows nobody sees it. Yeah. A tree that fell in the forest.
[01:21:04.300 --> 01:21:09.580]   Then of course, the latest, I think that can you now can you now put a link to mastodon on your
[01:21:09.580 --> 01:21:16.220]   account? I think you can. Yes, you can. Okay, can. Okay. The join mastodon account is still
[01:21:16.220 --> 01:21:21.900]   put up a general link. Okay, join mastodon account is back that was blocked. So that's good.
[01:21:22.540 --> 01:21:27.740]   I did enjoy the article. The editorial written. I can't remember where was it Forbes, where the
[01:21:27.740 --> 01:21:32.700]   writer said that John Mastodon, the founder of a social media network had been banned.
[01:21:32.700 --> 01:21:37.900]   They're creating a ton of memes about John Mastodon. That's hilarious.
[01:21:37.900 --> 01:21:44.860]   I was a conservative writer who thought that it was when John joined mastodon was taken down
[01:21:44.860 --> 01:21:49.900]   that it was a guy named John Mastodon who was the founder of Mastodon. So it was a whole
[01:21:49.900 --> 01:21:54.140]   singularology was cool. Yeah, but we had a lot of fun with it on the
[01:21:54.140 --> 01:22:02.060]   last. Then Elon put up a part me. Go ahead. Go ahead. Then Elon put up a poll saying,
[01:22:02.060 --> 01:22:07.900]   well, should I step down as CEO? I obviously stung by a series of, you know, events that really
[01:22:07.900 --> 01:22:12.140]   must have hurt him to the core and I'll be honest with you. I'm starting to feel a little bit bad
[01:22:12.140 --> 01:22:19.740]   for him. Paul went against him 57 to 43. Yes, you should step down. Has he stepped down yet?
[01:22:19.740 --> 01:22:26.700]   No, he said that when they find some schmuck who'll take the job, he will. But let's not forget that
[01:22:26.700 --> 01:22:33.180]   he also testified that he was going to because of Tesla. He's been sued by Tesla shareholders.
[01:22:33.180 --> 01:22:39.980]   Every word is that the Saudis said stop this crap and leave and the Saudis, which are the number
[01:22:39.980 --> 01:22:45.580]   one investor in Twitter. He will end up owning Twitter as he goes bankrupt.
[01:22:45.580 --> 01:22:50.940]   Yeah. And by the way, that's the other thing he said is he says, I will resign as CEO as soon as
[01:22:50.940 --> 01:22:55.900]   I find someone foolish enough to take the job. Okay, but he'll still be the owner. And then he
[01:22:55.900 --> 01:23:01.580]   also said after that, I'll just run the software and servers team. So I'm not sure exactly what
[01:23:01.580 --> 01:23:08.460]   he's giving up. But I guess he won't have the CEO on his business card. He probably still
[01:23:08.460 --> 01:23:15.420]   have chief Twitter on his business card. All right. Is that the is that the full sum of the week in
[01:23:15.420 --> 01:23:19.660]   Musk this week in Musk? The full Muskie? Pretty close. Yeah. Yeah. Is there anything?
[01:23:19.660 --> 01:23:24.540]   I am feeling sorry for you. Oh, no, no, no. He's launching blue again for the eight dollars,
[01:23:24.540 --> 01:23:31.180]   right? But he's charging $11 if you're on an iPhone. Oh, because of the 30%? Yeah,
[01:23:31.180 --> 01:23:35.980]   is that 30%? I thought Apple didn't let you do that. They don't. Well, he's Elon Musk.
[01:23:35.980 --> 01:23:40.460]   Actually, they kind of do. You just can't tell people about it. You can't say on your app,
[01:23:40.460 --> 01:23:47.020]   it's cheaper if you go buy it from the website. So you can't do that. People do that. Spotify,
[01:23:47.020 --> 01:23:53.980]   does that frankly? When he announced the whole, you can't put this link in that link and
[01:23:53.980 --> 01:24:02.940]   the other social networks out there. Did you are here any pushback about truth? Was it truth
[01:24:02.940 --> 01:24:07.740]   that social or whatever it's called? Because that was on that list too. I just heard pushback about
[01:24:07.740 --> 01:24:12.940]   sharing Instagram and Mastodon. Well, they also banded links to Facebook and Instagram and
[01:24:12.940 --> 01:24:18.060]   the TikTok was not on the list. Yeah. And Linktree and the other. Yeah, Linktree,
[01:24:18.060 --> 01:24:24.060]   which is a big deal for creators because very often you'll have a list of all your stuff on a
[01:24:24.060 --> 01:24:28.380]   link aggregate site. They blocked that as well. I guess that's all back. Yeah. I'm feeling sorry
[01:24:28.380 --> 01:24:34.700]   for him because I think I do feel sorry for him. I think he fell into this Jesus.
[01:24:34.700 --> 01:24:43.260]   Okay, you can pity someone for being wrong, but when they have that much power and no one else
[01:24:43.260 --> 01:24:49.020]   has taken them out, you have to like, I could be like, man, that man is having some issues.
[01:24:49.020 --> 01:24:53.420]   Yeah, that's why I feel sorry for some of these issues. Well, yeah, I don't feel sorry for that.
[01:24:53.420 --> 01:24:59.100]   I mean, I just become our issues. Yeah, he's on that pole of his about the whole step in there.
[01:24:59.100 --> 01:25:02.860]   I didn't even see it because I haven't met on Twitter. Sure, I did. Of course, I wonder like,
[01:25:02.860 --> 01:25:10.540]   what was the point of any of us normal folks that have no stake in Twitter or no employment
[01:25:10.540 --> 01:25:15.260]   tires, anything like that? What was the point of us voting on that as if it would actually matter?
[01:25:15.260 --> 01:25:20.700]   You know, it he's going to go somewhere when he's ready to go somewhere. He still owns the
[01:25:20.700 --> 01:25:27.100]   diagram company. And I think that was just feared. So why are we, I shouldn't say we because I didn't
[01:25:27.100 --> 01:25:33.500]   vote on it, but why are people still feeding into this person's egos beyond? He's yeah, he's a
[01:25:33.500 --> 01:25:38.620]   narcissist and we should probably follow the right rules for dealing with narcissists and we are not
[01:25:38.620 --> 01:25:46.300]   which is to ignore them. Right. Yeah. It's hard to ignore him if you love Twitter because, you know,
[01:25:47.660 --> 01:25:51.740]   that's a segue into nothing. Yeah, there's nothing more to talk about. I could give you something.
[01:25:51.740 --> 01:25:56.860]   It goes over. No, I just feel like we could come back to that story that you launched with the 3D
[01:25:56.860 --> 01:26:03.820]   GPT. Oh, that was it. Is there more to say about that? No, I don't know. Okay, fine.
[01:26:03.820 --> 01:26:11.100]   I like this tweet. Congratulations to the inevitable next CEO of Twitter. Ladies gentlemen,
[01:26:11.100 --> 01:26:17.100]   I give you Marissa Meyer. Marissa Meyer. That was my question to you all. Who would be good
[01:26:17.100 --> 01:26:23.500]   candidate for this. This is a job. Yeah, Jason and do it in a heartbeat. Oh, there, Elon is surrounded
[01:26:23.500 --> 01:26:28.300]   himself with a lot of sick fans who would say, Oh, yeah, I'll do it. Elon, I'll do it. Jason asked
[01:26:28.300 --> 01:26:33.980]   to do it. I think David Sax is a more likely candidate, to be honest, or cave on, uh,
[01:26:33.980 --> 01:26:39.740]   Bekapur, who is apparently also in the inner circle, right? He's a former, uh, Twitter.
[01:26:40.460 --> 01:26:47.660]   Fired by, uh, Prague. Right. Um, well, I'm not saying who would likely get it because they're
[01:26:47.660 --> 01:26:52.780]   in the inner circle. I'm saying who would be an ideal person to, well, cave on to be good. He's
[01:26:52.780 --> 01:26:58.940]   the former head of product at Twitter. Um, okay, but he had an authority. You'd be a fool to take
[01:26:58.940 --> 01:27:04.140]   the job. Mm hmm. Absolute fool. Yeah, the person would take it would be more, but I have to say,
[01:27:04.140 --> 01:27:09.980]   when you read tweets from David Sax and others, Elon's mother, even tweeting in support of Elon,
[01:27:10.460 --> 01:27:16.300]   uh, I mean, there are people who love Elon who would gladly do it. It's his mom. Well, his mom
[01:27:16.300 --> 01:27:23.500]   would be a good CEO. She's a fashion model. Easy. No, so there's no one on the outside.
[01:27:23.500 --> 01:27:28.620]   Um, nobody on the outside. No, it's got a, I would be the CEO of Twitter. Would you,
[01:27:28.620 --> 01:27:33.980]   you would you do it? I would go in and do it. I love Twitter. Well, I don't know. I'd have to go
[01:27:33.980 --> 01:27:38.380]   learn all about everything that's happening. And then I'd figure something out. And I'd probably
[01:27:38.380 --> 01:27:44.300]   ask lots of smart people. Yeah. Hey, should the next leader be someone that is, is, is super
[01:27:44.300 --> 01:27:49.660]   technical or someone that is just pragmatic? No, because Twitter product is in technology at all.
[01:27:49.660 --> 01:27:55.740]   Twitter's product is, is, is the, is the people, the, the culture, all of that. That's the product.
[01:27:55.740 --> 01:27:59.020]   That's the thing. Elon is, because, because Musk comes in and the first thing he started
[01:27:59.020 --> 01:28:03.820]   talking about is I need to see your last handful of commits. He misunderstood what he bought. He's
[01:28:03.820 --> 01:28:09.420]   a part of that. Or he did understand it, but he was trying to cut costs dramatically, which he,
[01:28:09.420 --> 01:28:12.780]   by the way, he says we're going bankrupt. We only have a billion dollars runway left.
[01:28:12.780 --> 01:28:20.140]   Uh, and I won't, you know, I mean, if you love Twitter, I mean, this is sad. It's the end of
[01:28:20.140 --> 01:28:25.100]   Twitter. I don't see any way around it, right? Some people think it'll come back, I guess.
[01:28:25.100 --> 01:28:30.380]   If you, what if you got somebody good in there? The problem is even before even on Twitter was
[01:28:30.380 --> 01:28:35.660]   failing, but that's what it's been failing for years. Twitter is a business has been failing
[01:28:35.660 --> 01:28:40.700]   for years. So it, I mean, in some of the things should have been done, but they probably shouldn't
[01:28:40.700 --> 01:28:46.380]   have been done quite so publicly and for, for the lols, I guess, is kind of the feeling like,
[01:28:46.380 --> 01:28:51.900]   yeah, did he need to reduce staff? Probably. That's why I asked you, should it be someone from the
[01:28:51.900 --> 01:28:57.580]   outside of the industry to come in and take over? Somebody that's, that can just write the
[01:28:57.580 --> 01:29:01.340]   ship if you will. Let me ask you to, ask you just a process. Let me try this on you. Because I think
[01:29:01.340 --> 01:29:05.500]   we're getting stumbled because Elon's still there as to loans it. Let's say the company has gone
[01:29:05.500 --> 01:29:11.660]   bankrupt. Private equity has bought it from the, from the debtors, uh, sane company.
[01:29:11.660 --> 01:29:18.060]   Then your question is, whom do they hire? Yeah. Yeah. Who could run it without Musk? That's a
[01:29:18.060 --> 01:29:23.900]   different question. Different question. Yeah. Well, there's an entire stable of CEOs who do like
[01:29:23.900 --> 01:29:30.380]   bankruptcy, like who do that job? And that's a very different job than the people that bank hires
[01:29:30.380 --> 01:29:35.980]   for that are not the same people you would hire if you were running a venture back. John Jay Ray,
[01:29:35.980 --> 01:29:42.860]   the third, the guy who's in receipt, you know, in charge of FTX is, you know, bankruptcy. He'd be
[01:29:42.860 --> 01:29:51.100]   good picking up the pieces of FTX. I say we get Laurie from Raviga. Who's looking at this?
[01:29:51.100 --> 01:29:59.420]   Laurie from Sabine. Oh, yeah. Laurie from Raviga. Yeah. Yeah. Yeah. Who's the guy who got into the car,
[01:29:59.420 --> 01:30:05.980]   got put in a box and got sent off to an island. That was that, the Gavin Belson. No, no, that was his,
[01:30:05.980 --> 01:30:13.500]   his, his, his, good history reference there. I wasn't talking elbow, but yeah, that's good. I like it.
[01:30:13.500 --> 01:30:18.940]   You mean they had a room like the piano box? Remember the guy at the beginning of Silicon Valley
[01:30:18.940 --> 01:30:22.620]   who had that car that was so thin, he could drive death, then it could drive through.
[01:30:22.620 --> 01:30:30.860]   Not really. Who was that? What was anyway? You get the idea. It doesn't matter. All right,
[01:30:30.860 --> 01:30:36.860]   we're done with Musk. Sorry for the. That was the must. I just want to say, I'm now actually
[01:30:36.860 --> 01:30:40.300]   starting to feel bad for the guy because it's just not going to you. You said it five times.
[01:30:40.300 --> 01:30:45.260]   I'm sorry. It's not going well. No, it's not going well. Well, he did it to himself and he's a
[01:30:45.260 --> 01:30:49.660]   schmuck and he's bringing in races. He's bringing in, no, no, there's no.
[01:30:49.660 --> 01:30:53.740]   I'm sorry. Hold on. Hold on. Hold on. Hold on. Hold on. Hold on. What do you mean?
[01:30:53.740 --> 01:30:58.460]   All right, Mr. Jarvis, he's bringing in races. Oh, he's bringing in the worst of the worst who
[01:30:58.460 --> 01:31:02.460]   were taken off of Twitter for good reason. And they're back. They're terrible people saying
[01:31:02.460 --> 01:31:07.660]   terrible things. No, he's, he's. I don't think any of them ever left quite frankly.
[01:31:07.660 --> 01:31:13.260]   Well, some of them are there. And I mean, maybe they had private. I've seen the same old
[01:31:13.260 --> 01:31:18.700]   crap show from from day one, whether he was there or not, honestly. I don't think that's
[01:31:18.700 --> 01:31:24.220]   got anything to do with Musk. That's the platform. No, he did. He literally unbanned about
[01:31:24.220 --> 01:31:28.860]   literally did unbansing people, but there were still just as many of my
[01:31:28.860 --> 01:31:34.940]   fellow guys out there and bots that he complained about prior to buying that are still just doing
[01:31:34.940 --> 01:31:44.940]   their their their gods work, if you will. He is now using his pulpit to attempt to shoot down the
[01:31:44.940 --> 01:31:50.460]   reconciliation $1.7 trillion reconciliation act that Congress will almost certainly pass.
[01:31:50.460 --> 01:31:55.820]   But he's had a poll. He's got a poll. He said, by the way, for in future, only Twitter,
[01:31:55.820 --> 01:32:01.180]   blue people will be allowed to vote, which of course, is very much like a poll tax, I think,
[01:32:01.180 --> 01:32:08.700]   you know, where you you paid a vote. We've tried that, I think. Anyway, I don't care. I honestly
[01:32:08.700 --> 01:32:14.220]   don't care. I feel bad for him. He's troubled. I just want you to know I feel that I told you I
[01:32:14.220 --> 01:32:21.500]   feel bad for the guy. I feel bad for the guy. Hey, everybody, it's Leo LaPorte, the founder
[01:32:21.500 --> 01:32:28.060]   and host of many of the Twitch podcasts. I don't normally talk to you about advertising,
[01:32:28.060 --> 01:32:33.020]   but I want to take a moment to do that right now. Our mission statement at Twitter,
[01:32:33.020 --> 01:32:39.500]   we're dedicated to building a highly engaged community of tech enthusiasts. That's our audience
[01:32:39.500 --> 01:32:44.540]   and you, I guess, since you're listening, by offering them the knowledge they need to understand
[01:32:44.540 --> 01:32:51.180]   and use technology in today's world. To do that, we also create partnerships with trusted brands
[01:32:51.180 --> 01:32:56.940]   and make important introductions between them and our audience. That's how we finance our podcasts,
[01:32:56.940 --> 01:33:01.660]   but it's also, and our audience tells us this all the time, a part of the service we offer,
[01:33:01.660 --> 01:33:06.860]   it's a valued bit of information for our audience members. They want to know about
[01:33:06.860 --> 01:33:15.820]   great brands like yours. So can we help you by introducing you to our highly qualified audience?
[01:33:15.820 --> 01:33:21.260]   And why do you get a lot with advertising on the Twitch podcasts? Partnering with Twitch
[01:33:21.260 --> 01:33:26.380]   means you're going to get, if I may say so, humbly, the gold standard in podcast advertising.
[01:33:26.380 --> 01:33:30.700]   And we throw in a lot of valuable services. You get a full service continuity team,
[01:33:30.700 --> 01:33:35.580]   supporting everything from copywriting to graphic design. I don't think anybody else
[01:33:35.580 --> 01:33:41.100]   does this or does this as well as we do. You get ads that are embedded in our content that are
[01:33:41.100 --> 01:33:46.540]   unique every time. I read them, our hosts read them. We always over deliver on impressions.
[01:33:46.540 --> 01:33:53.340]   And frankly, we're here to talk about your product. So we really give our listeners a
[01:33:53.340 --> 01:33:59.420]   great introduction to what you offer. We've got onboarding services, ad tech with pod sites,
[01:33:59.420 --> 01:34:04.300]   that's free for direct clients. We give you a lot of reporting so you know who saw your
[01:34:04.300 --> 01:34:08.860]   advertisement. You'll even know how many responded by going to your website.
[01:34:08.860 --> 01:34:14.140]   We'll also give you courtesy commercials that you can share across social media and landing pages.
[01:34:14.140 --> 01:34:19.020]   We think these are really valuable. People like me and our other hosts talking about your products
[01:34:19.020 --> 01:34:24.780]   sincerely and informationally. Those are incredibly valuable. You also get other free goodies,
[01:34:24.780 --> 01:34:29.580]   mentions in our weekly newsletter that sent out to thousands of fans. We give bonus ads
[01:34:29.580 --> 01:34:35.580]   to people who buy a significant amount of advertising. You'll get social media promotion too.
[01:34:35.580 --> 01:34:39.660]   But let me tell you, we are looking for an advertising partner that's going to be with
[01:34:39.660 --> 01:34:45.740]   us long term. Visit twitter.tv/advertise. Check out our partner testimonials. Tim Broome,
[01:34:45.740 --> 01:34:53.820]   founder of ITProTV. They started ITProTV in 2013, immediately started advertising with us and grew
[01:34:53.820 --> 01:35:01.820]   that company to a really amazing success. Hundreds of thousands of ongoing customers.
[01:35:01.820 --> 01:35:06.220]   They've been on our network for more than 10 years and they say, and I'll quote Tim,
[01:35:06.220 --> 01:35:10.060]   we would not be where we are today without the Twit Network. That's just one example.
[01:35:10.060 --> 01:35:16.300]   Mark McCurry, who's the CEO of Authentic, he was actually one of the first people to buy ads
[01:35:16.300 --> 01:35:21.900]   on our network. He's been with us for 16 years. He said, and I'm quoting, "The feedback from
[01:35:21.900 --> 01:35:27.500]   many advertisers over those 16 years across a range of product categories is that if ads and
[01:35:27.500 --> 01:35:33.420]   podcasts are going to work for a brand, they're going to work on Twit shows. I'm proud to say
[01:35:33.420 --> 01:35:39.980]   that the ads we do over deliver, they work really well because they're honest, they have integrity,
[01:35:39.980 --> 01:35:45.020]   our audience trusts us and we say, this is a great product. They believe it. They listen.
[01:35:45.020 --> 01:35:50.140]   Our listeners are highly intelligent. They're heavily engaged. They're tech savvy.
[01:35:50.140 --> 01:35:55.660]   They're dedicated to our network. That's partly because we only work with high integrity partners
[01:35:55.660 --> 01:36:03.340]   that we have thoroughly and personally vetted. I approve every single advertiser on the network.
[01:36:03.340 --> 01:36:07.740]   If you're ready to elevate your brand and you've got a great product, I want you to reach out to us.
[01:36:08.300 --> 01:36:15.340]   Advertise at twit.tv. I want you to break out of the advertising norm, grow your brand
[01:36:15.340 --> 01:36:24.380]   with host red authentic ads on twit.tv. Visit twit.tv/advertise for more details or email us,
[01:36:24.380 --> 01:36:29.900]   advertise at twit.tv if you're ready to launch your campaign now. People are dreaming about him.
[01:36:29.900 --> 01:36:33.500]   Move on. Move on. Move on. More Musk.
[01:36:33.500 --> 01:36:40.460]   People are dreaming about him. According to Bloomberg,
[01:36:40.460 --> 01:36:49.580]   there are people having bad dreams. A lot of them about Elon Musk. Let me see if I can find this
[01:36:49.580 --> 01:36:55.340]   story. Are you yawning? Is that a passive aggressive?
[01:36:56.300 --> 01:37:02.540]   According to psychologists who specialize in treating people suffering from nightmares and
[01:37:02.540 --> 01:37:08.140]   other sleep disorders, nightly visitations from Elon Musk are happening a lot these days.
[01:37:08.140 --> 01:37:13.340]   Such shared dream occurrences which often happen during periods of instability offer a broader
[01:37:13.340 --> 01:37:18.300]   insight into our collective psyche. When someone is in the press, it triggers a lot of dream images
[01:37:18.300 --> 01:37:23.100]   said gale the lane of Florida based clinical psychologist and dream analyst. A lot of people
[01:37:23.100 --> 01:37:27.820]   are thinking about Elon Musk. He demonstrates the human lust for power, play and creativity.
[01:37:27.820 --> 01:37:32.460]   He's also disappointed a lot of people. A search through Twitter confirms Musk is turning up
[01:37:32.460 --> 01:37:37.180]   constantly in people subconscious right now, just as he is dominating.
[01:37:37.180 --> 01:37:43.340]   Like yours this week in Google. Ex-Google contractors, settles lawsuit. This was,
[01:37:43.340 --> 01:37:47.420]   I thought, kind of interesting. Are you familiar with the fellowship of friends?
[01:37:48.700 --> 01:37:58.860]   Not Quakers. It's a obscure religious cult that apparently has had a lot of influence inside Google.
[01:37:58.860 --> 01:38:08.780]   This is the stuff of one part of conspiracy. A former video producer for Google has settled a
[01:38:08.780 --> 01:38:15.740]   lawsuit claiming he was fired after he complained that a religious sect had gained a foothold
[01:38:16.300 --> 01:38:21.900]   inside a business unit of the company. The Google developer studios which makes those videos
[01:38:21.900 --> 01:38:25.100]   showcasing the company's technologies. We've seen them. We've shown them.
[01:38:25.100 --> 01:38:31.420]   The suit claims the leader of the business unit, Peter Lubbers, a longtime member of the fellowship
[01:38:31.420 --> 01:38:36.860]   of friends, hired many of the religious groups members onto the team as contractors, helped some
[01:38:36.860 --> 01:38:41.660]   advance to full-time positions and gave work to many others when staffing company conferences
[01:38:41.660 --> 01:38:48.540]   and parties. Google confirmed the suit had been resolved and also confirmed that Mr. Lubbers
[01:38:48.540 --> 01:38:52.780]   was no longer employed by Google, although it declined to explain his departure.
[01:38:52.780 --> 01:38:57.340]   Well, it's like Landmark and Keller Williams.
[01:38:57.340 --> 01:39:02.220]   You know what it's like? I didn't realize this, but the Kelly company.
[01:39:02.220 --> 01:39:08.220]   You know, remember Kelly Girls? They don't call him that anymore, but Jeff and I remember it.
[01:39:08.220 --> 01:39:18.060]   Kelly's services, which is a well-known temporary company, was sued. Similarly,
[01:39:18.060 --> 01:39:22.620]   a former Kelly employee claimed the company failed a promoter because unlike many co-workers,
[01:39:22.620 --> 01:39:28.140]   she was not a member of the fellowship. The court awarded her $6.5 million in damages.
[01:39:28.140 --> 01:39:36.940]   Lubbers came from Kelly's services. Now, honestly, this cult is not huge. The fellowship of friends
[01:39:36.940 --> 01:39:42.300]   described itself as available to anyone interested in pursuing the spiritual work of awakening.
[01:39:42.300 --> 01:39:48.860]   This sounds good to me. 1,500 members around the world, 500 in and around Oregon House,
[01:39:48.860 --> 01:39:52.540]   California, which is... I was going to say this sounds so California.
[01:39:52.540 --> 01:39:54.140]   It's very California. Yeah.
[01:39:54.140 --> 01:40:00.220]   You know, I don't think that they were like they had some weird religious thing that they were
[01:40:00.220 --> 01:40:04.620]   trying to do with Google's video services. He was just hiring hard his friends. People he knew.
[01:40:04.620 --> 01:40:09.340]   Yeah. Anyway, it's a weird story. New York Times covering it.
[01:40:09.340 --> 01:40:14.940]   Did I mention Elon Musk is seeking new investors for Twitter? Had I mentioned that?
[01:40:14.940 --> 01:40:18.780]   What do you think of Elon, Leo? I feel sorry for him.
[01:40:18.780 --> 01:40:24.300]   What have I said? You got paid me too. You're going to pay the $8, aren't you?
[01:40:24.300 --> 01:40:26.940]   No, no, no, no. I don't feel that sorry for him. I just feel bad.
[01:40:26.940 --> 01:40:34.220]   The poor guys, I feel bad for for E as well. Yay. I feel bad. I mean, you know,
[01:40:34.220 --> 01:40:38.220]   they're going through something and it's going through it in public. It's hard. It's hard.
[01:40:38.220 --> 01:40:43.660]   And I think he genuinely thinks that people are trying to kill him. I do now realize that's why
[01:40:43.660 --> 01:40:49.580]   he doesn't have a house, right? I do feel sorry for him if he's, you know, concerned about his
[01:40:49.580 --> 01:40:53.820]   security for his family. He's worried about blaming him. I'm empathetic about that.
[01:40:53.820 --> 01:40:58.780]   Somebody other crap that... I mean, narcissists do turn to paranoia.
[01:40:58.780 --> 01:41:02.940]   I mean, if you think the whole world revolves around you, then... Exactly.
[01:41:03.740 --> 01:41:07.740]   Well, the whole show revolves around me. I can't speak for the world.
[01:41:07.740 --> 01:41:11.980]   That is true. We know it's not a democracy.
[01:41:11.980 --> 01:41:19.180]   When I quit my radio job, which my last show was on Sunday, I was very proud because somebody
[01:41:19.180 --> 01:41:23.020]   wrote a little blog post about how they visited the cottage and stuff. It was a very nice blog post.
[01:41:23.020 --> 01:41:24.940]   And it got on Hacker News.
[01:41:24.940 --> 01:41:31.500]   Oh, really? Yeah. Unfortunately, the title said something like in memoriam
[01:41:32.060 --> 01:41:36.460]   for Leo Laporte. Oh, no. And people thought I died. Oh, no.
[01:41:36.460 --> 01:41:41.100]   Which was kind of cool because I could see what people would say if I...
[01:41:41.100 --> 01:41:43.900]   Yeah, you got a Mark Twain moment. I got a Mark Twain moment.
[01:41:43.900 --> 01:41:47.340]   Oh, please. Yeah. I did. And I immediately said, "You know what? I'm not dead." And
[01:41:47.340 --> 01:41:52.220]   to reassure them. But people said some very... There were some mean people,
[01:41:52.220 --> 01:41:55.660]   some said some mean things. And there were some people said I was too big for my britches,
[01:41:55.660 --> 01:41:59.580]   things like that. So I know when I died that there will be some people who say nice things,
[01:41:59.580 --> 01:42:04.620]   there will be some people who say mean things. But I feel sorry for Elon Musk. I'm just saying that.
[01:42:04.620 --> 01:42:10.380]   John Carmack is leaving Meta after more than eight years. He's a contractor
[01:42:10.380 --> 01:42:17.020]   at Meta. But he is widely loved as the creator of Doom, a brilliant programmer
[01:42:17.020 --> 01:42:25.660]   who then started to work with Palmer Lucky at Oculus when Oculus was acquired by Meta. He moved
[01:42:25.660 --> 01:42:33.180]   over to Meta than Facebook. And he posted a post on Facebook in which he said,
[01:42:33.180 --> 01:42:40.220]   "I have to leave." He said Meta, which is in the midst of transitioning, as you know,
[01:42:40.220 --> 01:42:45.180]   from a social networking company to one focused on the Metaverse, was operating at, quote,
[01:42:45.180 --> 01:42:50.060]   "half the effectiveness" has, quote, "a ridiculous amount of people and resources,
[01:42:50.060 --> 01:42:56.860]   but we constantly self-sabotage and squander effort. It has been a struggle for me. I have
[01:42:56.860 --> 01:43:00.780]   a voice at the highest levels here, so it feels like I should be able to move things,
[01:43:00.780 --> 01:43:03.500]   but I'm evidently not persuasive enough and quit."
[01:43:03.500 --> 01:43:15.420]   He was Mark's right hand guy on VR, I guess. Although, according to the time, sometimes
[01:43:15.420 --> 01:43:19.980]   a dissenting voice about how the effort was going, he became known for internal posts that
[01:43:19.980 --> 01:43:25.180]   criticized the decision-making and direction set forth by Mr. Zuckerberg and Andrew Bosworth,
[01:43:25.180 --> 01:43:29.020]   Meta's chief technology officer, Carmack was a consultant part of him.
[01:43:29.020 --> 01:43:34.300]   Is his stance against this whole Metaverse push?
[01:43:34.300 --> 01:43:40.620]   You look like a chef. It was my sash, it was my crazies at the top of his head.
[01:43:40.620 --> 01:43:44.300]   That hair transplant didn't work out so well there.
[01:43:44.300 --> 01:43:46.140]   It works so good.
[01:43:46.140 --> 01:43:50.780]   It's a whole show should be about ants.
[01:43:50.780 --> 01:43:54.540]   Where's Ant's tape at the moment?
[01:43:54.540 --> 01:43:57.740]   All right.
[01:43:57.740 --> 01:44:05.580]   Oh, but I didn't hear the answer. I don't think about it.
[01:44:05.580 --> 01:44:06.620]   What was your question again?
[01:44:06.620 --> 01:44:09.260]   Did you feel sorry for Elon Musk?
[01:44:10.780 --> 01:44:16.460]   That he's against the Metaverse push? Or does he have something to do?
[01:44:16.460 --> 01:44:19.500]   So I have a theory, but I don't know if this is true.
[01:44:19.500 --> 01:44:21.020]   I think we were talking about this in a bit.
[01:44:21.020 --> 01:44:27.980]   Notice that he was all about VR, right?
[01:44:27.980 --> 01:44:30.460]   He was saying the company's not doing well with VR.
[01:44:30.460 --> 01:44:32.940]   The Quest Pro should have come out years ago.
[01:44:32.940 --> 01:44:39.660]   I think Metaverse, I don't know. Is it VR or is it AR the future?
[01:44:39.660 --> 01:44:44.780]   I think I'm wondering if he doesn't like that direction.
[01:44:44.780 --> 01:44:48.700]   Those RTs, they hate compromise.
[01:44:48.700 --> 01:44:51.420]   And AR is the ultimate compromise.
[01:44:51.420 --> 01:44:54.860]   It is, but it's going to be the only way VR has a shot, if you ask me.
[01:44:54.860 --> 01:44:59.420]   Hey, speaking of Instagram, they have finally, Instagram hacking.
[01:44:59.420 --> 01:45:03.180]   It's always, God, it was one of the things that calls all the time on the radio show
[01:45:03.180 --> 01:45:06.540]   for people who lost their accounts. Lisa lost several of her accounts.
[01:45:08.140 --> 01:45:11.740]   Instagram has, I don't know how well it will work, but it's a response anyway.
[01:45:11.740 --> 01:45:16.940]   They're adding new ways for tools to recover their accounts after being hacked.
[01:45:16.940 --> 01:45:19.660]   Instagram.com/hacked.
[01:45:19.660 --> 01:45:24.380]   They'll have a feature for reporting and trying to resolve account access problems.
[01:45:24.380 --> 01:45:29.900]   What would happen if Meta pulls a Google and says, you know what,
[01:45:29.900 --> 01:45:32.460]   Agamir, you better put two FA on these accounts.
[01:45:32.460 --> 01:45:38.620]   You should, you can. Google still doesn't make people do they?
[01:45:38.620 --> 01:45:44.060]   I mean, they have made some thought it forced it to your email and stuff.
[01:45:44.060 --> 01:45:46.060]   Okay. I thought they forced it.
[01:45:46.060 --> 01:45:54.220]   Everyone should force it, especially if, well, especially for services that people find valuable.
[01:45:54.220 --> 01:46:00.620]   Because it's not convenient, but my God, y'all, you just got to do it.
[01:46:01.100 --> 01:46:05.180]   Yeah. What if Meta first forces people to do this?
[01:46:05.180 --> 01:46:11.180]   I wonder what their hurdle would be because Instagram users are more like,
[01:46:11.180 --> 01:46:15.580]   quote, unquote, normal people, not a bunch of tech nerds.
[01:46:15.580 --> 01:46:17.020]   I feel like it would educate them.
[01:46:17.020 --> 01:46:22.940]   I would hope considering some of the phone calls and messages I've gotten in the past about,
[01:46:22.940 --> 01:46:25.340]   I can't get into them that account or I was hacked.
[01:46:25.340 --> 01:46:30.860]   And I'm like, well, and you know, if they did, like, so I remember when Fortnite did their
[01:46:30.860 --> 01:46:34.540]   special dance for people, like you could unlock a special thing.
[01:46:34.540 --> 01:46:40.380]   Mailchimp, when I did two factor on Mailchimp, they offered me a 10% discount.
[01:46:40.380 --> 01:46:44.380]   Instagram could offer like a compelling feature or something like a filter.
[01:46:44.380 --> 01:46:46.540]   I don't know, just something.
[01:46:46.540 --> 01:46:51.580]   And so enabled two factor and then get some, pay some money to some influencers to use that
[01:46:51.580 --> 01:46:52.940]   filter and then everybody would do it.
[01:46:52.940 --> 01:46:56.460]   I mean, it's not, it's not hard to gamify two factor.
[01:46:56.460 --> 01:47:02.780]   I'm washing and post.
[01:47:02.780 --> 01:47:03.820]   We talked about Google.
[01:47:03.820 --> 01:47:06.300]   They're done.
[01:47:06.300 --> 01:47:10.220]   Wait, y'all, y'all, I got to be real.
[01:47:10.220 --> 01:47:11.020]   Be real, man.
[01:47:11.020 --> 01:47:16.460]   I'm going to fade in like soon.
[01:47:16.460 --> 01:47:17.100]   Great. We're almost done.
[01:47:17.100 --> 01:47:17.980]   I'm not trying to threaten y'all.
[01:47:17.980 --> 01:47:18.460]   No, we're done.
[01:47:18.460 --> 01:47:18.940]   Okay.
[01:47:18.940 --> 01:47:20.460]   I'm like, I'm just like, oh, man.
[01:47:21.020 --> 01:47:21.900]   Look at her eyes.
[01:47:21.900 --> 01:47:23.340]   Look at her eyes.
[01:47:23.340 --> 01:47:24.460]   This will perk you up.
[01:47:24.460 --> 01:47:25.420]   It's been a hard day.
[01:47:25.420 --> 01:47:26.620]   This will perk you up.
[01:47:26.620 --> 01:47:28.540]   Festivus is what Thursday?
[01:47:28.540 --> 01:47:30.460]   Today's Festivus.
[01:47:30.460 --> 01:47:31.900]   Happy-
[01:47:31.900 --> 01:47:33.100]   It's also the solstice.
[01:47:33.100 --> 01:47:33.980]   Happy Festivus.
[01:47:33.980 --> 01:47:36.540]   Here's a bear rubbing himself up against the Festivus bowl.
[01:47:36.540 --> 01:47:39.420]   And you don't like the things I put up on the rundown.
[01:47:39.420 --> 01:47:40.460]   Happy Festivus.
[01:47:40.460 --> 01:47:42.300]   Are you grateful for that?
[01:47:42.300 --> 01:47:44.860]   Are you going to rub yourself up on that Festivus bowl?
[01:47:44.860 --> 01:47:45.660]   I am just-
[01:47:45.660 --> 01:47:47.420]   I need to leave before that happens.
[01:47:47.420 --> 01:47:47.820]   Please don't.
[01:47:47.820 --> 01:47:48.460]   Please don't.
[01:47:49.100 --> 01:47:51.180]   The airing of grievances will commence.
[01:47:51.180 --> 01:47:55.340]   Festivus is day and solstice today.
[01:47:55.340 --> 01:47:56.540]   So, so happy winter.
[01:47:56.540 --> 01:47:58.540]   What time?
[01:47:58.540 --> 01:47:59.100]   See winter.
[01:47:59.100 --> 01:48:01.660]   147 was solstice today.
[01:48:01.660 --> 01:48:03.340]   Three hours ago.
[01:48:03.340 --> 01:48:03.900]   One time zone.
[01:48:03.900 --> 01:48:05.500]   Wow. Happy winter.
[01:48:05.500 --> 01:48:07.900]   It felt like winter for the whole week.
[01:48:07.900 --> 01:48:08.540]   So, finally.
[01:48:08.540 --> 01:48:09.100]   Sure did.
[01:48:09.100 --> 01:48:10.700]   It's dark here.
[01:48:10.700 --> 01:48:12.380]   It's going to be eight degrees here tomorrow.
[01:48:12.380 --> 01:48:15.580]   Did I show you the bear rubbing up against the Festivus bowl?
[01:48:15.580 --> 01:48:16.780]   You're welcome.
[01:48:16.780 --> 01:48:17.660]   Watch that again.
[01:48:17.660 --> 01:48:18.060]   Thank you.
[01:48:18.060 --> 01:48:18.620]   Thank you.
[01:48:18.620 --> 01:48:19.420]   Thank you.
[01:48:19.420 --> 01:48:24.380]   Now we know what the Festivus bowl is for.
[01:48:24.380 --> 01:48:26.620]   And this comes from being on Mastodon.
[01:48:26.620 --> 01:48:29.580]   By the way, Washington Post is going to open.
[01:48:29.580 --> 01:48:30.460]   No segue.
[01:48:30.460 --> 01:48:32.220]   Well, Mastodon is to update.
[01:48:32.220 --> 01:48:33.740]   Update, update, update.
[01:48:33.740 --> 01:48:36.220]   Jeremy Bowers says they're considering it.
[01:48:36.220 --> 01:48:38.460]   I think he probably got ahead of his bosses a little bit.
[01:48:38.460 --> 01:48:39.020]   I think they-
[01:48:39.020 --> 01:48:42.060]   I'm going to bet they probably will, but he wasn't announcementable.
[01:48:42.060 --> 01:48:42.620]   They're doing other-
[01:48:42.620 --> 01:48:43.340]   What's the case-
[01:48:43.340 --> 01:48:48.300]   What's the case, Jeff, for a media outlet to have their own Mastodon service?
[01:48:48.300 --> 01:48:50.140]   You couldn't do that on Twitter.
[01:48:50.140 --> 01:48:53.020]   In Twitter, everybody had to post on Twitter in one place.
[01:48:53.020 --> 01:48:56.700]   And one of the things people miss from Twitter is that centralized place where you get all your news.
[01:48:56.700 --> 01:48:58.380]   So there's arguments against it.
[01:48:58.380 --> 01:49:01.820]   There's arguments that say, "Well, no, the journalist should have their own place wherever they are,
[01:49:01.820 --> 01:49:07.420]   but one, it can take portability of one's identity and social graph elsewhere if you quit the place.
[01:49:07.420 --> 01:49:09.580]   And so you own your own identity, which is important."
[01:49:09.580 --> 01:49:16.460]   I think that for the place to share the load, as you do, Leo, by having Twitter not social,
[01:49:16.460 --> 01:49:19.020]   you're sharing the load in a federated world.
[01:49:19.020 --> 01:49:21.820]   And media outlets are going to benefit from Mastodon.
[01:49:21.820 --> 01:49:22.940]   They should join in.
[01:49:22.940 --> 01:49:25.980]   The posters, to be the other things you'd be proud of,
[01:49:25.980 --> 01:49:29.100]   they are going to put Reldotme on all of their authors' pages.
[01:49:29.100 --> 01:49:32.140]   So authors at The Washington Post can verify through The Washington Post.
[01:49:32.140 --> 01:49:33.180]   Anyway, so you added-
[01:49:33.180 --> 01:49:33.740]   So you added-
[01:49:33.740 --> 01:49:34.220]   So you added-
[01:49:34.220 --> 01:49:35.980]   My Mastodon instance or some other-
[01:49:35.980 --> 01:49:36.460]   Exactly.
[01:49:36.460 --> 01:49:39.900]   They'd have a green box next to their name and their Washington Post link saying-
[01:49:39.900 --> 01:49:41.180]   So Jeremy is-
[01:49:41.180 --> 01:49:41.660]   So Jeremy is-
[01:49:41.660 --> 01:49:42.220]   Yeah.
[01:49:42.220 --> 01:49:42.780]   Is on-
[01:49:43.820 --> 01:49:45.820]   Jordan on host, but he can put Reldotme on-
[01:49:45.820 --> 01:49:47.260]   Post to say, "I am really that."
[01:49:47.260 --> 01:49:47.740]   Yeah.
[01:49:47.740 --> 01:49:55.900]   And then they've also already put a Mastodon button on the reporter's pages.
[01:49:55.900 --> 01:49:58.620]   So when you go to their page-
[01:49:58.620 --> 01:49:59.180]   Wow.
[01:49:59.180 --> 01:50:02.700]   Now, what I really want, but I think it's probably technically impossible,
[01:50:02.700 --> 01:50:06.140]   you tell me, I would love for next to the share on Twitter,
[01:50:06.140 --> 01:50:07.660]   share on Facebook, etc.
[01:50:07.660 --> 01:50:09.340]   to be a share on Mastodon button.
[01:50:09.340 --> 01:50:13.420]   Problem is, of course, if I come along as a reader and I click on that,
[01:50:13.420 --> 01:50:15.180]   the Post doesn't know what instance I'm on.
[01:50:15.180 --> 01:50:15.980]   Yeah.
[01:50:15.980 --> 01:50:18.700]   Would it well go to Mastodon-
[01:50:18.700 --> 01:50:19.260]   It was so-
[01:50:19.260 --> 01:50:22.780]   It couldn't then go to Mastodon.social versus Twitter.
[01:50:22.780 --> 01:50:24.380]   Because it doesn't know what the reader's on.
[01:50:24.380 --> 01:50:27.500]   But I really want that share button there,
[01:50:27.500 --> 01:50:29.740]   and I wanted to, you know, displace Twitter.
[01:50:29.740 --> 01:50:31.340]   I think there's probably a way to figure it out.
[01:50:31.340 --> 01:50:33.980]   I'm not sure what I-
[01:50:33.980 --> 01:50:35.980]   So you're saying, okay, I'm reading-
[01:50:35.980 --> 01:50:38.620]   Well, unfortunately-
[01:50:38.620 --> 01:50:39.980]   You're reading Washington Post's story.
[01:50:39.980 --> 01:50:40.780]   This is a story-
[01:50:40.780 --> 01:50:43.020]   She's got email, she's got Twitter, she's got Insta-
[01:50:43.020 --> 01:50:43.580]   There's also-
[01:50:43.580 --> 01:50:44.460]   She's got RSS.
[01:50:44.460 --> 01:50:45.420]   She's got a lot of guys who got-
[01:50:45.420 --> 01:50:46.940]   Go to Taylor Lawrence.
[01:50:46.940 --> 01:50:47.820]   Oh, Taylor Lawrence.
[01:50:47.820 --> 01:50:48.540]   Okay, there you go.
[01:50:48.540 --> 01:50:49.340]   That's a good-
[01:50:49.340 --> 01:50:50.620]   Well, the other one who got-
[01:50:50.620 --> 01:50:51.660]   True Harwell-
[01:50:51.660 --> 01:50:51.900]   Yes.
[01:50:51.900 --> 01:50:52.540]   Yeah.
[01:50:52.540 --> 01:50:55.420]   So if I search for-
[01:50:55.420 --> 01:50:57.180]   Let's search for Taylor.
[01:50:57.180 --> 01:51:00.540]   Let me see the other guy who was on there.
[01:51:00.540 --> 01:51:01.660]   Was that True Harwell?
[01:51:01.660 --> 01:51:03.100]   Yes, True Harwell.
[01:51:03.100 --> 01:51:03.420]   Thank you.
[01:51:03.420 --> 01:51:03.740]   Yes.
[01:51:03.740 --> 01:51:04.220]   Yeah.
[01:51:04.220 --> 01:51:05.180]   Look at his page.
[01:51:05.180 --> 01:51:08.620]   So first, I'll go to Taylor because I can't change.
[01:51:08.620 --> 01:51:11.980]   She doesn't have Mastodon on here either.
[01:51:11.980 --> 01:51:12.700]   So they need to-
[01:51:12.700 --> 01:51:13.820]   Well, go to Drew.
[01:51:13.820 --> 01:51:14.380]   Go to Drew.
[01:51:14.380 --> 01:51:15.020]   Maybe it's in the article?
[01:51:15.020 --> 01:51:16.460]   You think it's at the end of the article?
[01:51:16.460 --> 01:51:17.340]   No, no, no, I don't think so.
[01:51:17.340 --> 01:51:18.220]   No, no, no, no,
[01:51:18.220 --> 01:51:19.500]   it's from the author page.
[01:51:19.500 --> 01:51:20.300]   Go to Drew.
[01:51:20.300 --> 01:51:21.180]   Can't you go to Drew?
[01:51:21.180 --> 01:51:21.980]   Yeah, I can do it at Drew.
[01:51:21.980 --> 01:51:22.620]   Just be patient.
[01:51:22.620 --> 01:51:24.060]   Slow down, buddy.
[01:51:24.060 --> 01:51:26.700]   I only got one pair of hands here.
[01:51:26.700 --> 01:51:28.220]   Drew-
[01:51:28.220 --> 01:51:32.700]   Because he was one of the booted off folk.
[01:51:32.700 --> 01:51:34.380]   I think he's still not allowed to post.
[01:51:34.380 --> 01:51:36.060]   I think he's one of those.
[01:51:36.060 --> 01:51:36.940]   Okay.
[01:51:36.940 --> 01:51:38.700]   Musk blamed a Twitter account for-
[01:51:38.700 --> 01:51:41.900]   Okay, so yeah, this is the story he did with Taylor.
[01:51:41.900 --> 01:51:43.500]   The guy who was author page.
[01:51:43.500 --> 01:51:44.380]   The guy who was named.
[01:51:44.380 --> 01:51:45.900]   Now I'm going to click his name.
[01:51:45.900 --> 01:51:47.020]   And there's his Mastodon.
[01:51:47.020 --> 01:51:48.460]   There's a Mastodon button.
[01:51:48.460 --> 01:51:50.460]   That's going to go to his Mastodon profile.
[01:51:50.460 --> 01:51:53.820]   Yeah, so this is one of the problems with Mastodon.
[01:51:53.820 --> 01:51:54.780]   That's a set of drugs.
[01:51:54.780 --> 01:51:58.140]   There is a follow, but this is the problem.
[01:51:58.140 --> 01:51:59.740]   Unless you're on that server,
[01:51:59.740 --> 01:52:01.740]   this is a two-step process.
[01:52:01.740 --> 01:52:03.100]   You have to copy that link.
[01:52:03.100 --> 01:52:04.700]   Those are really neat plugging in.
[01:52:04.700 --> 01:52:07.500]   And then you have to go to your server.
[01:52:07.500 --> 01:52:08.620]   If you log into your server,
[01:52:08.620 --> 01:52:10.860]   you have to paste that link in here.
[01:52:10.860 --> 01:52:13.340]   Hit return and then it will search for it.
[01:52:13.340 --> 01:52:14.540]   And I'm already following him,
[01:52:14.540 --> 01:52:16.620]   but then you will see his account.
[01:52:16.620 --> 01:52:18.060]   And you could follow it from there.
[01:52:18.060 --> 01:52:21.740]   There's a nice Mastodon plugin that the Sunjake sent me to,
[01:52:21.740 --> 01:52:24.780]   where it will automatically do that for you.
[01:52:24.780 --> 01:52:25.580]   Yeah, this is actually-
[01:52:25.580 --> 01:52:26.140]   Here's a Google.
[01:52:26.140 --> 01:52:27.180]   There's a Google tie into that.
[01:52:27.180 --> 01:52:28.220]   That's the guy who did Inbox.
[01:52:28.220 --> 01:52:32.460]   So he wrote a little JavaScript plugin for it,
[01:52:32.460 --> 01:52:35.820]   Chrome that Nysons Mastodon.
[01:52:35.820 --> 01:52:39.740]   That is one of the problems with Mastodon is it's tricky to follow.
[01:52:39.740 --> 01:52:40.460]   But you can do it.
[01:52:40.460 --> 01:52:41.500]   Here's the other question.
[01:52:41.500 --> 01:52:42.780]   I'm on a story.
[01:52:42.780 --> 01:52:45.100]   I'm on a Washington Post story.
[01:52:45.100 --> 01:52:46.460]   I want to, with one click,
[01:52:46.460 --> 01:52:47.580]   share it on Mastodon.
[01:52:47.580 --> 01:52:49.980]   I want to go to Mastodon and have the link go there
[01:52:49.980 --> 01:52:51.580]   and populate with a card.
[01:52:51.580 --> 01:52:52.700]   And it adds up to the post.
[01:52:52.700 --> 01:52:53.820]   That's up to the post.
[01:52:53.820 --> 01:52:54.300]   That's up to the post.
[01:52:54.300 --> 01:52:54.780]   Pass.
[01:52:54.780 --> 01:52:55.420]   Well, no, no, no.
[01:52:55.420 --> 01:52:56.620]   I don't think it is because the problem is-
[01:52:56.620 --> 01:52:57.340]   Yes, I can tell you it is.
[01:52:57.340 --> 01:52:59.340]   Well, here's the question.
[01:52:59.340 --> 01:53:01.580]   How would it know if you and I would do the same thing,
[01:53:01.580 --> 01:53:03.020]   how would it know that for you,
[01:53:03.020 --> 01:53:04.700]   you've got to go to Twit.net social for me.
[01:53:04.700 --> 01:53:05.740]   You've got to go Mastodon social.
[01:53:05.740 --> 01:53:06.220]   Watch.
[01:53:06.220 --> 01:53:06.780]   Okay, good.
[01:53:06.780 --> 01:53:07.820]   You want to see?
[01:53:07.820 --> 01:53:10.780]   Here I'm on Advent of Code.
[01:53:10.780 --> 01:53:13.900]   And I want to share my results.
[01:53:13.900 --> 01:53:14.940]   So let me go back to-
[01:53:14.940 --> 01:53:17.020]   I solved this puzzle.
[01:53:17.020 --> 01:53:17.980]   So at the bottom,
[01:53:17.980 --> 01:53:21.180]   where it would say share on Twitter,
[01:53:21.180 --> 01:53:23.340]   which in fact it does.
[01:53:23.340 --> 01:53:25.900]   But I can also share on Mastodon, right?
[01:53:25.900 --> 01:53:27.420]   Oh, so do that.
[01:53:27.420 --> 01:53:27.740]   Okay.
[01:53:27.740 --> 01:53:28.620]   Click that.
[01:53:28.620 --> 01:53:31.340]   Now it says, well, where do you live on Mastodon?
[01:53:31.340 --> 01:53:31.740]   Okay.
[01:53:31.740 --> 01:53:32.460]   Twit.net social.
[01:53:32.460 --> 01:53:34.620]   That's the one hard part.
[01:53:34.620 --> 01:53:36.940]   And now here's a pre-filled post.
[01:53:36.940 --> 01:53:37.820]   And I can look at that.
[01:53:37.820 --> 01:53:38.940]   Okay, I'm going to reveal that.
[01:53:38.940 --> 01:53:39.980]   It's what you want.
[01:53:39.980 --> 01:53:41.900]   So it is possible too,
[01:53:41.900 --> 01:53:43.340]   because Advent of Code,
[01:53:43.340 --> 01:53:45.500]   which is the great Eric Wastel,
[01:53:45.500 --> 01:53:47.020]   he does an amazing job with this.
[01:53:47.020 --> 01:53:48.460]   That's pretty cool.
[01:53:48.460 --> 01:53:51.500]   He, you know, this is the coding puzzles
[01:53:51.500 --> 01:53:55.180]   that a lot of us on the Twit Discord are doing right now.
[01:53:55.180 --> 01:53:56.380]   You can see I'm way behind.
[01:53:56.380 --> 01:53:57.580]   I'm 10 days behind now.
[01:53:57.580 --> 01:54:01.660]   But that's because I gave up on trying to do it all in real time.
[01:54:01.660 --> 01:54:03.820]   But yeah, so it is doable.
[01:54:03.820 --> 01:54:04.700]   It's totally doable.
[01:54:04.700 --> 01:54:05.180]   Okay.
[01:54:05.180 --> 01:54:07.260]   I think that that's, there's that extra step.
[01:54:07.260 --> 01:54:08.620]   You have to say which,
[01:54:08.620 --> 01:54:09.740]   see, that's the weird thing.
[01:54:09.740 --> 01:54:11.820]   People go, well, Twitter is Twitter, right.
[01:54:11.820 --> 01:54:13.340]   So you're going to have to enter Twitter.com.
[01:54:13.340 --> 01:54:15.900]   Mastodon's not everybody's on a different server.
[01:54:15.900 --> 01:54:17.980]   But I can follow anybody on any other server.
[01:54:17.980 --> 01:54:20.060]   The follow is really the biggest problem right now.
[01:54:20.060 --> 01:54:20.540]   Right.
[01:54:20.540 --> 01:54:21.180]   Well, what I really want,
[01:54:21.180 --> 01:54:22.940]   I just want to stay on the share thing for a second.
[01:54:22.940 --> 01:54:25.420]   I like the cards on Twitter.
[01:54:25.420 --> 01:54:28.700]   And I would love it if it would populate the line.
[01:54:28.700 --> 01:54:29.660]   That's a web standard.
[01:54:29.660 --> 01:54:30.460]   That's a link.
[01:54:30.460 --> 01:54:31.340]   That's a card.
[01:54:31.340 --> 01:54:33.580]   And the card should come with,
[01:54:33.580 --> 01:54:35.580]   because it's Mastodon standard, Alt text.
[01:54:35.580 --> 01:54:36.700]   Alt text.
[01:54:36.700 --> 01:54:37.180]   Yeah.
[01:54:37.180 --> 01:54:38.140]   Then it goes to Mastodon.
[01:54:38.140 --> 01:54:39.260]   That's all a web standard.
[01:54:39.260 --> 01:54:41.820]   Like retweet the article with a comment on top.
[01:54:41.820 --> 01:54:44.540]   Remember Twitter used to do that, right?
[01:54:44.540 --> 01:54:45.900]   Doesn't it?
[01:54:45.900 --> 01:54:47.740]   That's a web standard.
[01:54:47.740 --> 01:54:50.860]   It has nothing to do with Mastodon or Twitter.
[01:54:50.860 --> 01:54:52.140]   Mastodon will have to,
[01:54:52.140 --> 01:54:54.300]   they have to properly form.
[01:54:54.300 --> 01:54:55.340]   Material up exactly.
[01:54:55.340 --> 01:54:56.220]   Right, exactly.
[01:54:56.220 --> 01:54:57.420]   They have to properly format the share.
[01:54:57.420 --> 01:54:58.220]   And as you can see,
[01:54:58.220 --> 01:54:59.340]   But the card is a standard.
[01:54:59.340 --> 01:55:01.180]   Advent of code has done that.
[01:55:02.220 --> 01:55:04.700]   And so, yeah, you can totally do that.
[01:55:04.700 --> 01:55:05.820]   That's not a hard thing to do.
[01:55:05.820 --> 01:55:13.020]   But I agree, Mastodon is not as easy as Twitter.
[01:55:13.020 --> 01:55:13.820]   It's not Twitter.
[01:55:13.820 --> 01:55:15.340]   It's decentralized.
[01:55:15.340 --> 01:55:17.500]   To me, that's, it's strength.
[01:55:17.500 --> 01:55:18.060]   That's all right.
[01:55:18.060 --> 01:55:19.340]   Because no one can buy it.
[01:55:19.340 --> 01:55:19.900]   No.
[01:55:19.900 --> 01:55:23.260]   By the way, the EU is creating its own Mastodon instances,
[01:55:23.260 --> 01:55:25.100]   which I think is fabulous.
[01:55:25.100 --> 01:55:26.300]   I think you're going to see,
[01:55:26.300 --> 01:55:27.980]   that's one of the opportunities, I think,
[01:55:27.980 --> 01:55:29.180]   because it's decentralized,
[01:55:29.180 --> 01:55:32.060]   is that a media company can have its own server.
[01:55:32.620 --> 01:55:33.980]   For its own people.
[01:55:33.980 --> 01:55:34.780]   A government entity.
[01:55:34.780 --> 01:55:36.380]   Or an entity as well.
[01:55:36.380 --> 01:55:37.020]   Yeah.
[01:55:37.020 --> 01:55:37.260]   Right.
[01:55:37.260 --> 01:55:40.220]   If you, unless, I think SFBA is one, right.
[01:55:40.220 --> 01:55:41.820]   And the San Francisco standard,
[01:55:41.820 --> 01:55:43.340]   the head of that call being said,
[01:55:43.340 --> 01:55:45.180]   should I start an instance?
[01:55:45.180 --> 01:55:48.220]   I said, you just kind of want to do some moderation.
[01:55:48.220 --> 01:55:48.780]   Right.
[01:55:48.780 --> 01:55:50.140]   Bozilla is creating one.
[01:55:50.140 --> 01:55:52.780]   Vivaldi, the browser is creating one.
[01:55:52.780 --> 01:55:53.740]   I think this is going to,
[01:55:53.740 --> 01:55:56.460]   I think this, these young guys at Mastodon.
[01:55:56.460 --> 01:55:58.860]   This has legs.
[01:55:58.860 --> 01:55:59.500]   And his legs.
[01:55:59.500 --> 01:56:00.140]   The other thing.
[01:56:00.140 --> 01:56:00.620]   The other thing.
[01:56:00.620 --> 01:56:06.060]   The EU is also investing money into various Fediverse apps.
[01:56:06.060 --> 01:56:09.100]   Pixel Fed and in others.
[01:56:09.100 --> 01:56:11.100]   So they're so happy that something came out
[01:56:11.100 --> 01:56:12.700]   from some place, from Germany,
[01:56:12.700 --> 01:56:14.460]   and someplace other than Silicon Valley.
[01:56:14.460 --> 01:56:16.940]   They're pointing their money up,
[01:56:16.940 --> 01:56:18.540]   which I've said for years and years and years.
[01:56:18.540 --> 01:56:20.460]   Stop whining that you don't have Google,
[01:56:20.460 --> 01:56:22.940]   invest in your own startups.
[01:56:22.940 --> 01:56:23.820]   Figure it out yourself.
[01:56:23.820 --> 01:56:24.700]   And so this is good.
[01:56:24.700 --> 01:56:25.500]   More competition.
[01:56:25.500 --> 01:56:26.620]   It's a wonderful thing.
[01:56:26.620 --> 01:56:26.940]   Business.
[01:56:26.940 --> 01:56:29.740]   Rather than regulating invested innovation.
[01:56:29.740 --> 01:56:33.020]   Let's wrap this beautiful puppy up in with a put a bow on it.
[01:56:33.020 --> 01:56:35.260]   It's the last show of the year for me.
[01:56:35.260 --> 01:56:36.860]   And for you all.
[01:56:36.860 --> 01:56:37.660]   Bring your bow.
[01:56:37.660 --> 01:56:42.060]   And we again, just a reminder,
[01:56:42.060 --> 01:56:43.100]   won't be here next week,
[01:56:43.100 --> 01:56:45.900]   but we will have a wonderful best of episode.
[01:56:45.900 --> 01:56:47.020]   Stop.
[01:56:47.020 --> 01:56:47.820]   I looked at the rundown.
[01:56:47.820 --> 01:56:48.540]   Oh, yeah, it's fun.
[01:56:48.540 --> 01:56:51.660]   It's some really fun stuff from all year long.
[01:56:51.660 --> 01:56:54.620]   And then we'll be back January 4th with the very first,
[01:56:54.620 --> 01:56:57.500]   this week in Google of the New Year.
[01:56:57.500 --> 01:56:58.700]   Stacey Higginbotham,
[01:56:58.700 --> 01:57:00.220]   so nice to have you.
[01:57:00.220 --> 01:57:02.620]   What is your thing this week?
[01:57:02.620 --> 01:57:06.220]   Well, it's kind of,
[01:57:06.220 --> 01:57:09.420]   this is not something I would recommend,
[01:57:09.420 --> 01:57:11.340]   but I also bought it for something.
[01:57:11.340 --> 01:57:12.460]   I'm just going to tell you about it
[01:57:12.460 --> 01:57:15.420]   because it's a device and you might see it and wonder how's that.
[01:57:15.420 --> 01:57:16.060]   Okay.
[01:57:16.060 --> 01:57:19.020]   This is the Theragon Smart Goggles.
[01:57:19.020 --> 01:57:21.020]   Now I have a Theragon which I love,
[01:57:21.020 --> 01:57:22.700]   but I didn't know you could put it on your face.
[01:57:22.700 --> 01:57:23.900]   Yeah.
[01:57:23.900 --> 01:57:24.220]   Yes.
[01:57:24.220 --> 01:57:26.060]   This is a Theragon for your face.
[01:57:26.060 --> 01:57:26.460]   Hold on.
[01:57:26.460 --> 01:57:27.100]   Oh, let go.
[01:57:27.980 --> 01:57:32.300]   So the Theragon that I have is a pneumatic device that pounds my muscles.
[01:57:32.300 --> 01:57:34.860]   Does this pound your face?
[01:57:34.860 --> 01:57:35.900]   It does.
[01:57:35.900 --> 01:57:37.900]   All right.
[01:57:37.900 --> 01:57:39.340]   Not as hard, I presume.
[01:57:39.340 --> 01:57:40.700]   No.
[01:57:40.700 --> 01:57:42.300]   So what this, what this is.
[01:57:42.300 --> 01:57:43.580]   Yeah, I wouldn't want to percuss my cheekbones.
[01:57:43.580 --> 01:57:44.060]   That would hurt.
[01:57:44.060 --> 01:57:45.740]   Go ahead.
[01:57:45.740 --> 01:57:47.180]   So is this for migraines or?
[01:57:47.180 --> 01:57:47.660]   Tell me about this.
[01:57:47.660 --> 01:57:49.900]   So I bought it for migraines.
[01:57:49.900 --> 01:57:50.460]   Here's what it is.
[01:57:50.460 --> 01:57:53.580]   For $199, there is a 60-day return policy,
[01:57:53.580 --> 01:57:55.180]   which is why I did buy it.
[01:57:56.460 --> 01:58:00.380]   It is a pair of goggles that you wear.
[01:58:00.380 --> 01:58:04.380]   It has a sensor on the inside where the, there it is.
[01:58:04.380 --> 01:58:06.300]   Oh, sorry.
[01:58:06.300 --> 01:58:07.500]   I'm covering up the sensor.
[01:58:07.500 --> 01:58:09.980]   That is supposed to match the vibrations.
[01:58:09.980 --> 01:58:11.820]   So it vibrates, it heats,
[01:58:11.820 --> 01:58:15.020]   and it has these little like Chatsu knobs that knob.
[01:58:15.020 --> 01:58:20.860]   And the sensor matches the vibrations to your heart rate.
[01:58:20.860 --> 01:58:24.940]   The idea is this will relax you quickly and put you to sleep.
[01:58:24.940 --> 01:58:25.980]   I'm going to sneeze in a second.
[01:58:25.980 --> 01:58:27.740]   Also you'd wear it going to bed.
[01:58:27.740 --> 01:58:31.100]   You wear it to go to bed or whatever you want to.
[01:58:31.100 --> 01:58:33.340]   They have three settings that are pre-programmed.
[01:58:33.340 --> 01:58:33.900]   One is-
[01:58:33.900 --> 01:58:36.780]   That's my favorite setting.
[01:58:36.780 --> 01:58:38.460]   Chudas, can I have that setting?
[01:58:38.460 --> 01:58:39.420]   The Hachut setting.
[01:58:39.420 --> 01:58:41.660]   Sorry.
[01:58:41.660 --> 01:58:44.700]   Never apologize for your sneeze.
[01:58:44.700 --> 01:58:46.620]   It's a focus setting, which is like-
[01:58:46.620 --> 01:58:47.660]   You could go on, you could do it on stage.
[01:58:47.660 --> 01:58:49.740]   If you want to focus-
[01:58:49.740 --> 01:58:51.260]   Oh, there it is.
[01:58:51.260 --> 01:58:52.860]   Look, there's the relax setting,
[01:58:52.860 --> 01:58:53.980]   which is just vibrations.
[01:58:53.980 --> 01:58:56.460]   And then there's the sleep setting, which is vibrations and heat.
[01:58:56.460 --> 01:58:59.580]   That's good.
[01:58:59.580 --> 01:59:00.940]   Yeah, that does.
[01:59:00.940 --> 01:59:01.660]   It-
[01:59:01.660 --> 01:59:04.940]   Here's the verdict after a week of using it.
[01:59:04.940 --> 01:59:06.540]   I got it because I thought maybe it would help
[01:59:06.540 --> 01:59:08.060]   when I feel a migraine coming on.
[01:59:08.060 --> 01:59:09.980]   Not in the throes of one, obviously.
[01:59:09.980 --> 01:59:11.340]   It didn't work for that.
[01:59:11.340 --> 01:59:13.180]   And what I found was the percussive stuff.
[01:59:13.180 --> 01:59:14.540]   It percusses right here.
[01:59:14.540 --> 01:59:16.540]   Under your eyes,
[01:59:16.540 --> 01:59:19.740]   it percusses kind of in your temple area around your eyes.
[01:59:19.740 --> 01:59:21.340]   And then in the center.
[01:59:21.340 --> 01:59:22.780]   It's a little off center for me.
[01:59:22.780 --> 01:59:24.380]   So when it tried to percuss here,
[01:59:24.380 --> 01:59:25.900]   like not much happened,
[01:59:25.900 --> 01:59:27.260]   but that could also just be Botox.
[01:59:27.260 --> 01:59:28.060]   I don't know.
[01:59:28.060 --> 01:59:28.780]   I couldn't feel it.
[01:59:28.780 --> 01:59:29.580]   It's a very bad-
[01:59:29.580 --> 01:59:33.260]   Does Botox make you numb?
[01:59:33.260 --> 01:59:34.780]   No, no.
[01:59:34.780 --> 01:59:36.460]   If you walk up to me and do this, I can feel it.
[01:59:36.460 --> 01:59:36.940]   Oh, okay.
[01:59:36.940 --> 01:59:37.580]   I'm just teasing.
[01:59:37.580 --> 01:59:39.900]   I'm just having fun.
[01:59:39.900 --> 01:59:41.740]   Ask a friend.
[01:59:41.740 --> 01:59:43.340]   Yeah.
[01:59:43.340 --> 01:59:47.980]   It does not make you numb.
[01:59:47.980 --> 01:59:49.660]   Um, anyway, so
[01:59:51.820 --> 01:59:53.740]   the headache stuff did not work for me.
[01:59:53.740 --> 01:59:55.660]   The relaxation stuff was actually nice.
[01:59:55.660 --> 01:59:57.500]   And like the heat on your eyes,
[01:59:57.500 --> 01:59:59.900]   like you know how sometimes you'll rub your hands together
[01:59:59.900 --> 02:00:01.100]   and you'll do this in your eyes.
[02:00:01.100 --> 02:00:02.620]   You stick it to your eyes and then they relax.
[02:00:02.620 --> 02:00:04.060]   Okay.
[02:00:04.060 --> 02:00:04.780]   For people listening,
[02:00:04.780 --> 02:00:07.580]   I just rubbed my palms together and stuck my palms to my eyes.
[02:00:07.580 --> 02:00:09.980]   I was taught to do that by my optometrist.
[02:00:09.980 --> 02:00:10.220]   Yeah.
[02:00:10.220 --> 02:00:11.100]   Yeah.
[02:00:11.100 --> 02:00:11.900]   So this is like that.
[02:00:11.900 --> 02:00:13.340]   And it's like 15 minutes of that.
[02:00:13.340 --> 02:00:14.380]   Um,
[02:00:14.380 --> 02:00:16.060]   This is really good.
[02:00:16.060 --> 02:00:16.540]   Exactly.
[02:00:16.540 --> 02:00:17.740]   Yeah, it's very relaxing.
[02:00:17.740 --> 02:00:18.780]   That's nice.
[02:00:18.780 --> 02:00:19.660]   The percussion,
[02:00:19.660 --> 02:00:21.900]   it just kind of like squeezes in.
[02:00:21.900 --> 02:00:22.620]   Oh, good.
[02:00:22.620 --> 02:00:23.980]   It doesn't tap tap tap.
[02:00:23.980 --> 02:00:25.580]   No, it just,
[02:00:25.580 --> 02:00:27.100]   and it's like a wave pattern.
[02:00:27.100 --> 02:00:29.020]   So it squeezes in at your temples,
[02:00:29.020 --> 02:00:29.980]   that it unsqueezes,
[02:00:29.980 --> 02:00:31.980]   that it squeezes in it under your eyes,
[02:00:31.980 --> 02:00:32.780]   and top of your cheeks,
[02:00:32.780 --> 02:00:34.060]   and then it unsqueezes.
[02:00:34.060 --> 02:00:35.420]   And then it unsqueezes in the middle
[02:00:35.420 --> 02:00:36.460]   and it unsqueezes.
[02:00:36.460 --> 02:00:37.900]   I kind of wanted some like,
[02:00:37.900 --> 02:00:39.740]   Shiatsu rubbing action happening.
[02:00:39.740 --> 02:00:41.820]   That felt like it'd be nicer,
[02:00:41.820 --> 02:00:42.620]   but it was fine.
[02:00:42.620 --> 02:00:46.140]   Um, and then the vibrations are also fine.
[02:00:46.140 --> 02:00:47.500]   The heat was probably the nicest thing.
[02:00:47.500 --> 02:00:48.220]   So if that's your thing,
[02:00:48.220 --> 02:00:49.980]   just buy like a $20 heating thing.
[02:00:49.980 --> 02:00:51.580]   Yeah, you could put one in a microwave
[02:00:51.580 --> 02:00:52.780]   that warms it up in the.
[02:00:52.780 --> 02:00:52.940]   Yeah.
[02:00:52.940 --> 02:00:52.940]   Yeah.
[02:00:52.940 --> 02:00:52.940]   Yeah.
[02:00:52.940 --> 02:00:52.940]   Yeah.
[02:00:52.940 --> 02:00:56.460]   But if, if you see this,
[02:00:56.460 --> 02:00:57.740]   and you've got $200,
[02:00:57.740 --> 02:00:59.340]   and you're like, maybe some people on Reddit
[02:00:59.340 --> 02:01:01.020]   said it did help their headaches.
[02:01:01.020 --> 02:01:03.820]   I mean, there is a 60 day return policy.
[02:01:03.820 --> 02:01:05.820]   Unfortunately,
[02:01:05.820 --> 02:01:09.180]   you can't get it till January 24th on Amazon.
[02:01:09.180 --> 02:01:12.140]   Because I thought this would be a nice Christmas gift, actually.
[02:01:12.140 --> 02:01:14.060]   Uh, you know,
[02:01:14.060 --> 02:01:16.700]   I don't know how I would react if someone gave me this for Christmas.
[02:01:16.700 --> 02:01:17.660]   Oh, it's thoughtful.
[02:01:17.660 --> 02:01:19.820]   It's like, you know, I care about you.
[02:01:19.820 --> 02:01:21.100]   Like, give you a headache.
[02:01:21.100 --> 02:01:21.660]   Use this.
[02:01:21.660 --> 02:01:22.140]   Yeah.
[02:01:22.140 --> 02:01:23.100]   He says to Lisa.
[02:01:23.100 --> 02:01:23.260]   Yeah.
[02:01:23.260 --> 02:01:24.860]   You don't think this would be good for Lisa.
[02:01:24.860 --> 02:01:29.500]   We already have the, the Theragon that pounds your thighs.
[02:01:29.500 --> 02:01:29.820]   Yeah.
[02:01:29.820 --> 02:01:31.900]   Oh, in my child, I should say.
[02:01:31.900 --> 02:01:32.700]   I love them too.
[02:01:32.700 --> 02:01:32.860]   Yeah.
[02:01:32.860 --> 02:01:34.460]   I put the, oh, I love them too.
[02:01:34.460 --> 02:01:34.700]   Yeah.
[02:01:34.700 --> 02:01:36.220]   Um, the sensor,
[02:01:36.220 --> 02:01:38.940]   they, my child said the sensor,
[02:01:38.940 --> 02:01:41.260]   it has a little red light because it's tracking your heart.
[02:01:41.260 --> 02:01:42.220]   Wait, using LEDs.
[02:01:42.220 --> 02:01:42.380]   Oh.
[02:01:42.380 --> 02:01:45.660]   They said that they found that distracting.
[02:01:45.660 --> 02:01:46.540]   I did not notice it.
[02:01:46.540 --> 02:01:48.140]   I was like, you're supposed to close your eyes.
[02:01:48.140 --> 02:01:49.740]   And they're like, I did mom.
[02:01:49.740 --> 02:01:52.540]   Wait a minute.
[02:01:52.540 --> 02:01:53.900]   You have more than one child?
[02:01:53.900 --> 02:01:55.660]   No, I just have one.
[02:01:55.660 --> 02:01:56.140]   Oh, okay.
[02:01:56.140 --> 02:01:58.060]   Anyway, that's the deal.
[02:01:58.060 --> 02:01:59.980]   And they, yeah.
[02:01:59.980 --> 02:02:00.940]   Yeah, these look great.
[02:02:00.940 --> 02:02:01.340]   Thank you.
[02:02:01.340 --> 02:02:03.180]   Now go away.
[02:02:03.180 --> 02:02:04.540]   Go relax.
[02:02:04.540 --> 02:02:05.900]   Take an app.
[02:02:05.900 --> 02:02:07.180]   Have a wonderful holiday.
[02:02:07.180 --> 02:02:08.940]   Merry Christmas.
[02:02:08.940 --> 02:02:10.460]   Are you doing anything fun for Christmas?
[02:02:10.460 --> 02:02:13.500]   Um, we're sleeping here.
[02:02:13.500 --> 02:02:14.300]   Oh, you know what?
[02:02:14.300 --> 02:02:15.660]   I have sleep.
[02:02:15.660 --> 02:02:17.020]   I have a snoopy.
[02:02:17.020 --> 02:02:22.540]   Do we get taken down if I play it?
[02:02:22.540 --> 02:02:24.620]   Uh, go ahead play it.
[02:02:24.620 --> 02:02:25.260]   I don't care.
[02:02:25.260 --> 02:02:26.140]   Screw YouTube.
[02:02:26.140 --> 02:02:27.900]   All right, here you go.
[02:02:27.900 --> 02:02:31.500]   Oh, he dances.
[02:02:31.500 --> 02:02:32.540]   Look at him.
[02:02:32.540 --> 02:02:33.260]   Oh.
[02:02:33.260 --> 02:02:34.700]   You know what?
[02:02:34.700 --> 02:02:35.820]   That sounds so crappy.
[02:02:35.820 --> 02:02:37.900]   I can't imagine us getting taken down.
[02:02:37.900 --> 02:02:41.420]   If we get taken down by the Vince Coraldi estate.
[02:02:41.420 --> 02:02:43.500]   Oh, that's awesome.
[02:02:44.140 --> 02:02:47.740]   Do they have a little woodstock bird
[02:02:47.740 --> 02:02:48.780]   that can dance with him?
[02:02:48.780 --> 02:02:50.220]   Because that would be really cute.
[02:02:50.220 --> 02:02:51.500]   Oh, that would be amazing.
[02:02:51.500 --> 02:02:54.060]   No, that's that we in my house,
[02:02:54.060 --> 02:02:55.740]   the rule is when that plays
[02:02:55.740 --> 02:02:57.420]   and anyone can play it at any time.
[02:02:57.420 --> 02:02:57.980]   Yeah.
[02:02:57.980 --> 02:02:59.340]   We all have to stop and dance.
[02:02:59.340 --> 02:03:00.220]   Oh, I love it.
[02:03:00.220 --> 02:03:00.860]   Very effective.
[02:03:00.860 --> 02:03:01.900]   I want to move with you.
[02:03:01.900 --> 02:03:03.100]   I want to move in.
[02:03:03.100 --> 02:03:03.900]   I want to dance.
[02:03:03.900 --> 02:03:06.060]   Stays you have a great Christmas.
[02:03:06.060 --> 02:03:06.540]   Take care.
[02:03:06.540 --> 02:03:07.580]   Y'all too.
[02:03:07.580 --> 02:03:08.060]   Thank you.
[02:03:08.060 --> 02:03:11.820]   Jeff Jarvis, number of the week.
[02:03:11.820 --> 02:03:12.540]   Ooh.
[02:03:12.540 --> 02:03:13.980]   Well, I'm going to ask you this question.
[02:03:13.980 --> 02:03:16.140]   So the New York Times did one of those.
[02:03:16.140 --> 02:03:17.900]   We don't understand the rest of the country stories.
[02:03:17.900 --> 02:03:20.300]   And I wanted to see from the from you guys who actually know.
[02:03:20.300 --> 02:03:24.300]   They declared that San Francisco streets are empty.
[02:03:24.300 --> 02:03:25.900]   The new Du Bois.
[02:03:25.900 --> 02:03:27.340]   I decided I said that.
[02:03:27.340 --> 02:03:28.460]   It's like I'm a new Du Bois.
[02:03:28.460 --> 02:03:30.940]   I was in Du Bois once and there was nothing there.
[02:03:30.940 --> 02:03:32.220]   Nothing.
[02:03:32.220 --> 02:03:33.100]   No one was on the other.
[02:03:33.100 --> 02:03:34.540]   That's because they're all in tunnels
[02:03:34.540 --> 02:03:36.140]   underneath the ground and above.
[02:03:36.140 --> 02:03:36.940]   That's cold.
[02:03:36.940 --> 02:03:37.260]   Street.
[02:03:37.260 --> 02:03:39.820]   So is that is that an accurate view?
[02:03:39.820 --> 02:03:41.100]   I haven't been there in a while.
[02:03:41.100 --> 02:03:41.740]   I'll say.
[02:03:41.740 --> 02:03:42.380]   It's accurate.
[02:03:42.380 --> 02:03:43.340]   Well, maybe that's why.
[02:03:43.340 --> 02:03:43.980]   It's accurate.
[02:03:43.980 --> 02:03:45.180]   It says Benito.
[02:03:45.180 --> 02:03:46.460]   Do you live in the city, Benito?
[02:03:46.460 --> 02:03:47.980]   He lives in the mission.
[02:03:47.980 --> 02:03:49.580]   And it's just got really quiet now.
[02:03:49.580 --> 02:03:51.100]   Downtown.
[02:03:51.100 --> 02:03:51.660]   You know why?
[02:03:51.660 --> 02:03:52.140]   Wow.
[02:03:52.140 --> 02:03:54.220]   It's scary down there.
[02:03:54.220 --> 02:03:55.820]   People are pooping on the streets.
[02:03:55.820 --> 02:03:56.860]   They're yelling at you.
[02:03:56.860 --> 02:03:58.060]   They're breaking into cars.
[02:03:58.060 --> 02:03:59.260]   This is a financial statement.
[02:03:59.260 --> 02:03:59.500]   I think they.
[02:03:59.500 --> 02:04:01.740]   I think they.
[02:04:01.740 --> 02:04:02.940]   Yeah, that's the other problem.
[02:04:02.940 --> 02:04:03.420]   There's no.
[02:04:03.420 --> 02:04:05.340]   Benito's saying there's nobody working there anymore.
[02:04:05.340 --> 02:04:05.900]   I'm only working.
[02:04:05.900 --> 02:04:06.860]   That's the real issue.
[02:04:06.860 --> 02:04:08.620]   But I think it's also as a tourist.
[02:04:08.620 --> 02:04:09.580]   I don't want to go there.
[02:04:09.580 --> 02:04:12.300]   Maybe I'll go to the outskirts of town.
[02:04:12.300 --> 02:04:14.060]   But.
[02:04:14.060 --> 02:04:16.460]   I think San Francisco always did brilliantly
[02:04:16.460 --> 02:04:18.460]   was that it gettalized the tourists.
[02:04:18.460 --> 02:04:18.620]   Right.
[02:04:18.620 --> 02:04:20.220]   No, San Francisco never goes to Georgia
[02:04:20.220 --> 02:04:20.780]   and LA Square.
[02:04:20.780 --> 02:04:21.180]   No.
[02:04:21.180 --> 02:04:22.460]   No, or a Pier 39.
[02:04:22.460 --> 02:04:23.020]   Of course.
[02:04:23.020 --> 02:04:24.220]   I think that's where San Francisco.
[02:04:24.220 --> 02:04:24.780]   Right.
[02:04:24.780 --> 02:04:25.900]   Yeah, they have the Irish coffee
[02:04:25.900 --> 02:04:27.740]   and they think they're living in the San Francisco life.
[02:04:27.740 --> 02:04:28.620]   And it's brilliant.
[02:04:28.620 --> 02:04:28.860]   Right.
[02:04:28.860 --> 02:04:31.100]   It pushes them off in the corner.
[02:04:31.100 --> 02:04:31.980]   Oh, Fisherman's Wharf.
[02:04:31.980 --> 02:04:33.420]   Yes, you'll love that.
[02:04:33.420 --> 02:04:33.740]   Yeah.
[02:04:33.740 --> 02:04:35.340]   Yeah, we all go there all the time.
[02:04:35.340 --> 02:04:35.740]   Wait, wait.
[02:04:35.740 --> 02:04:36.780]   Yeah, I went there once.
[02:04:36.780 --> 02:04:38.140]   Wait, he's like, huh.
[02:04:38.140 --> 02:04:38.620]   Okay.
[02:04:38.620 --> 02:04:39.980]   I went there to see the seals.
[02:04:39.980 --> 02:04:40.380]   No.
[02:04:40.380 --> 02:04:41.020]   Are the seals?
[02:04:41.020 --> 02:04:41.420]   What are those?
[02:04:41.420 --> 02:04:42.060]   Pier 39.
[02:04:42.060 --> 02:04:42.220]   Yeah.
[02:04:42.220 --> 02:04:42.700]   Fisherman's Wharf.
[02:04:42.700 --> 02:04:43.180]   Yeah.
[02:04:43.180 --> 02:04:43.740]   Yeah, yeah.
[02:04:43.740 --> 02:04:44.540]   Yeah, that was lovely.
[02:04:44.540 --> 02:04:44.700]   Yeah.
[02:04:44.700 --> 02:04:47.020]   Or you didn't go there to buy a hat or something.
[02:04:47.020 --> 02:04:47.820]   Or a piece of art.
[02:04:47.820 --> 02:04:50.060]   They sell art.
[02:04:50.060 --> 02:04:51.340]   No, I wouldn't say bad art.
[02:04:51.340 --> 02:04:52.060]   Yeah.
[02:04:52.060 --> 02:04:54.940]   No, but you could get Cotton Candy and write the
[02:04:54.940 --> 02:04:56.300]   her merry grand stuff.
[02:04:56.300 --> 02:04:57.020]   I think it's all right.
[02:04:57.020 --> 02:04:57.340]   Yeah.
[02:04:57.340 --> 02:04:57.660]   Yeah.
[02:04:57.660 --> 02:04:58.620]   It was a merry-go-round.
[02:04:58.620 --> 02:04:59.100]   Oh, yeah.
[02:04:59.100 --> 02:05:01.100]   On Pier 39.
[02:05:01.100 --> 02:05:01.420]   Yeah.
[02:05:01.420 --> 02:05:02.700]   Next time.
[02:05:02.700 --> 02:05:04.300]   When you're 39, it's like the worst.
[02:05:04.300 --> 02:05:06.140]   It's a little crowded.
[02:05:06.140 --> 02:05:08.860]   Lisa and Michael went there during COVID.
[02:05:08.860 --> 02:05:10.300]   It was so terrifying.
[02:05:10.300 --> 02:05:12.860]   It was so scary, but it was crowded.
[02:05:12.860 --> 02:05:13.820]   It was very crowded.
[02:05:13.820 --> 02:05:15.180]   So I have a feeling they're talking.
[02:05:15.180 --> 02:05:15.340]   Yeah.
[02:05:15.340 --> 02:05:17.180]   This is the downtown area, right?
[02:05:17.180 --> 02:05:17.580]   Yeah.
[02:05:17.580 --> 02:05:17.980]   Yeah.
[02:05:17.980 --> 02:05:18.140]   Yeah.
[02:05:18.140 --> 02:05:19.020]   This is financial history.
[02:05:19.020 --> 02:05:21.340]   Benino, this is good.
[02:05:21.340 --> 02:05:23.820]   You live in Des Moines now.
[02:05:23.820 --> 02:05:27.420]   The mission is probably still pretty vibrant.
[02:05:27.420 --> 02:05:28.380]   I think the mission.
[02:05:28.380 --> 02:05:29.500]   Yeah, the mission's kicking.
[02:05:29.500 --> 02:05:30.540]   Yeah, I love the mission.
[02:05:30.540 --> 02:05:32.620]   All right.
[02:05:32.620 --> 02:05:33.100]   There you go.
[02:05:33.100 --> 02:05:34.380]   There's a number zero.
[02:05:35.340 --> 02:05:35.660]   Uh...
[02:05:35.660 --> 02:05:38.940]   And Pruitt, pick of the week.
[02:05:38.940 --> 02:05:41.420]   I got two years there.
[02:05:41.420 --> 02:05:42.940]   Some holiday deals.
[02:05:42.940 --> 02:05:46.700]   Sony has some holiday deals going on some of the camera bodies.
[02:05:46.700 --> 02:05:51.420]   And I like to show the $10,000 Sony camera on Mac.
[02:05:51.420 --> 02:05:52.620]   Break weekly yesterday.
[02:05:52.620 --> 02:05:53.100]   Oh, man.
[02:05:53.100 --> 02:05:53.660]   The Venice.
[02:05:53.660 --> 02:05:56.540]   The new PTZ camera.
[02:05:56.540 --> 02:05:57.660]   Oh, the PTZ.
[02:05:57.660 --> 02:05:58.220]   Yeah.
[02:05:58.220 --> 02:06:01.740]   I love my sonies.
[02:06:01.740 --> 02:06:02.220]   Yeah.
[02:06:02.220 --> 02:06:06.300]   These are a couple years old, so they're really, really inexpensive right now.
[02:06:06.300 --> 02:06:07.580]   One of them is a point and shoot.
[02:06:07.580 --> 02:06:08.860]   The ZV-1 is like $700.
[02:06:08.860 --> 02:06:09.900]   This is the vlogger camera.
[02:06:09.900 --> 02:06:10.380]   Right.
[02:06:10.380 --> 02:06:11.740]   I've wanted this camera.
[02:06:11.740 --> 02:06:12.780]   Yeah, it was pretty solid.
[02:06:12.780 --> 02:06:13.020]   Yeah.
[02:06:13.020 --> 02:06:17.740]   And then there's the A7C, which is a four-frame camera,
[02:06:17.740 --> 02:06:19.580]   but it's really, really tiny.
[02:06:19.580 --> 02:06:22.140]   So if there's somebody that's wanting to upgrade the four-frame,
[02:06:22.140 --> 02:06:25.580]   I think it's a pretty nice camera.
[02:06:25.580 --> 02:06:28.060]   It's not my cup of tea because it's so dadgum small,
[02:06:28.060 --> 02:06:31.740]   but you still get a really nice Sony sensor on it.
[02:06:32.220 --> 02:06:34.460]   I think it was about $1,500 on 16.
[02:06:34.460 --> 02:06:36.300]   I can't make much money, but it was a
[02:06:36.300 --> 02:06:38.380]   18-count on your website.
[02:06:38.380 --> 02:06:38.860]   18.
[02:06:38.860 --> 02:06:40.620]   What is a full-frame camera?
[02:06:40.620 --> 02:06:41.900]   Oh, Stacey.
[02:06:41.900 --> 02:06:45.420]   The sensor is larger in a four-frames,
[02:06:45.420 --> 02:06:48.460]   essentially like a 35 millimeter.
[02:06:48.460 --> 02:06:49.980]   Six sizes of a day you have.
[02:06:49.980 --> 02:06:51.260]   35-500 piece of film.
[02:06:51.260 --> 02:06:53.180]   The bigger the sensor, the more light it can gather,
[02:06:53.180 --> 02:06:54.700]   the better the more light you can capture.
[02:06:54.700 --> 02:06:55.580]   Right.
[02:06:55.580 --> 02:06:56.940]   So the faster you can--
[02:06:56.940 --> 02:06:58.620]   Yeah, and also more pixels.
[02:06:58.620 --> 02:06:59.260]   Yeah.
[02:06:59.260 --> 02:07:00.060]   More pixels.
[02:07:01.020 --> 02:07:02.060]   Harder pixels.
[02:07:02.060 --> 02:07:03.900]   You know?
[02:07:03.900 --> 02:07:05.660]   Pro so almost a four-frame.
[02:07:05.660 --> 02:07:09.340]   Four-frame doesn't look the same as a 48 megapixel iPhone.
[02:07:09.340 --> 02:07:10.140]   It's not the same.
[02:07:10.140 --> 02:07:10.300]   Right.
[02:07:10.300 --> 02:07:11.260]   Just doesn't look the same.
[02:07:11.260 --> 02:07:12.060]   Right.
[02:07:12.060 --> 02:07:14.300]   You're a very exciting shooter, though, right?
[02:07:14.300 --> 02:07:15.100]   More than--
[02:07:15.100 --> 02:07:15.820]   Yes.
[02:07:15.820 --> 02:07:16.460]   I like that.
[02:07:16.460 --> 02:07:16.860]   I do.
[02:07:16.860 --> 02:07:18.380]   I have a lot of Sony gear.
[02:07:18.380 --> 02:07:19.100]   Sony's nice.
[02:07:19.100 --> 02:07:19.420]   Yeah.
[02:07:19.420 --> 02:07:20.220]   Sony's nice.
[02:07:20.220 --> 02:07:23.980]   Spider checker, this reminded me,
[02:07:23.980 --> 02:07:27.420]   I needed to get another one because Biscuit destroyed mine.
[02:07:27.420 --> 02:07:29.500]   Like an idiot, I had mine out on the floor,
[02:07:29.500 --> 02:07:30.700]   and he got a hold of it.
[02:07:30.700 --> 02:07:33.180]   So folks that are in photography,
[02:07:33.180 --> 02:07:34.700]   get yourself a color checker,
[02:07:34.700 --> 02:07:38.780]   because I'm seeing a lot of weird skin tones out there
[02:07:38.780 --> 02:07:40.620]   with some people's photography,
[02:07:40.620 --> 02:07:42.700]   and yeah, get yourself a color checker.
[02:07:42.700 --> 02:07:45.420]   So this is like a card you hold up the first shot,
[02:07:45.420 --> 02:07:46.540]   and then that way you can do color.
[02:07:46.540 --> 02:07:48.620]   Well, I have a great card I carry with me.
[02:07:48.620 --> 02:07:49.740]   Great cards will work, too.
[02:07:49.740 --> 02:07:51.260]   And this actually will--
[02:07:51.260 --> 02:07:53.740]   I believe it has a great card on the backside of it.
[02:07:53.740 --> 02:07:56.060]   Works very well.
[02:07:56.060 --> 02:07:59.420]   You can install the data color software with your--
[02:07:59.420 --> 02:08:02.220]   well, not software plugging in with your editing software,
[02:08:02.220 --> 02:08:03.900]   and it'll help do all the calibration
[02:08:03.900 --> 02:08:04.940]   and get you squared away.
[02:08:04.940 --> 02:08:05.980]   Oh, so that's nice.
[02:08:05.980 --> 02:08:07.100]   So this works with its spider.
[02:08:07.100 --> 02:08:09.340]   So yeah, spider makes one of those things
[02:08:09.340 --> 02:08:10.860]   you suck onto your screen.
[02:08:10.860 --> 02:08:12.220]   Yeah, you can put those--
[02:08:12.220 --> 02:08:14.300]   Yeah, I have one of those, too, somewhere over here.
[02:08:14.300 --> 02:08:15.900]   Oh, so this can't be too expensive.
[02:08:15.900 --> 02:08:16.380]   This is--
[02:08:16.380 --> 02:08:17.900]   No, it's like colors.
[02:08:17.900 --> 02:08:18.780]   Yeah, I think it's--
[02:08:18.780 --> 02:08:20.380]   Well, it comes with this little case,
[02:08:20.380 --> 02:08:21.420]   little protective case.
[02:08:21.420 --> 02:08:23.420]   It's like, have you seen the Pantone color palette?
[02:08:23.420 --> 02:08:24.700]   Oh, that's expensive.
[02:08:24.700 --> 02:08:26.620]   This one, I think it's--
[02:08:26.620 --> 02:08:29.580]   I think it's now marked down to $80 for the holidays,
[02:08:29.580 --> 02:08:30.460]   something like that.
[02:08:30.460 --> 02:08:30.540]   Cool.
[02:08:30.540 --> 02:08:33.980]   So spiderfromdatacolor.com.
[02:08:33.980 --> 02:08:34.300]   Yeah.
[02:08:34.300 --> 02:08:37.260]   And I want to shout out, lastly,
[02:08:37.260 --> 02:08:40.700]   the local kids that are signing on National Signing Day.
[02:08:40.700 --> 02:08:43.340]   I've been-- I've had the pleasure of watching
[02:08:43.340 --> 02:08:45.900]   Cy Vadjowali for the last three years,
[02:08:45.900 --> 02:08:48.620]   play ball here at Rancho Ocatati,
[02:08:48.620 --> 02:08:52.540]   who officially signed today with the Cal Bears.
[02:08:52.540 --> 02:08:53.180]   Yeah.
[02:08:53.180 --> 02:08:54.700]   The Cali Bears to play football.
[02:08:54.700 --> 02:08:56.780]   So congratulations, young man.
[02:08:56.780 --> 02:08:59.100]   And everybody else around the country
[02:08:59.100 --> 02:09:00.780]   getting an extra opportunity to--
[02:09:00.780 --> 02:09:02.700]   You caught that ball with his thighs.
[02:09:02.700 --> 02:09:03.980]   That was a catch.
[02:09:03.980 --> 02:09:07.260]   Yeah, they caught it incomplete.
[02:09:07.260 --> 02:09:09.980]   And I remember thinking, I thought it was a catch.
[02:09:09.980 --> 02:09:10.540]   It caught that.
[02:09:10.540 --> 02:09:12.140]   You can catch up all with your thighs.
[02:09:12.140 --> 02:09:12.700]   Check that tape.
[02:09:12.700 --> 02:09:12.940]   Yeah.
[02:09:12.940 --> 02:09:14.940]   Yeah, it controls it.
[02:09:14.940 --> 02:09:16.780]   Hit the ground.
[02:09:16.780 --> 02:09:17.340]   Hit the ground.
[02:09:17.340 --> 02:09:18.380]   He always did, but you know.
[02:09:18.380 --> 02:09:20.300]   That's a catch.
[02:09:20.300 --> 02:09:20.700]   It's a catch.
[02:09:20.700 --> 02:09:22.300]   But they wrote it incomplete because there's
[02:09:22.300 --> 02:09:24.300]   no instant replay in high school.
[02:09:24.300 --> 02:09:25.100]   Yeah.
[02:09:25.100 --> 02:09:26.780]   Today's a big day for a lot of kids
[02:09:26.780 --> 02:09:30.300]   getting an extra opportunity to further their education
[02:09:30.300 --> 02:09:32.540]   and get to play football at the next level.
[02:09:32.540 --> 02:09:33.020]   So they get--
[02:09:33.020 --> 02:09:34.540]   Another person present.
[02:09:34.540 --> 02:09:37.660]   This early in December, what college they're going to be playing for.
[02:09:37.660 --> 02:09:38.220]   This is early.
[02:09:38.220 --> 02:09:38.860]   This is early.
[02:09:38.860 --> 02:09:40.060]   This is early signing day.
[02:09:40.060 --> 02:09:41.580]   There'll be another signing day,
[02:09:41.580 --> 02:09:43.660]   usually in February-ish.
[02:09:43.660 --> 02:09:44.220]   OK.
[02:09:44.220 --> 02:09:44.940]   Yeah.
[02:09:44.940 --> 02:09:45.340]   For people to--
[02:09:45.340 --> 02:09:47.020]   So this is people got early acceptance.
[02:09:47.020 --> 02:09:48.380]   Right.
[02:09:48.380 --> 02:09:48.860]   Yeah.
[02:09:48.860 --> 02:09:49.660]   How exciting.
[02:09:49.660 --> 02:09:50.380]   Cal's a big--
[02:09:50.380 --> 02:09:50.860]   I found the--
[02:09:50.860 --> 02:09:51.420]   Football power.
[02:09:51.420 --> 02:09:53.340]   --the hat you should get in for Christmas on Fridays.
[02:09:53.340 --> 02:09:53.900]   Oh, good.
[02:09:53.900 --> 02:09:54.940]   Where is that?
[02:09:54.940 --> 02:09:56.140]   Line 86.
[02:09:56.140 --> 02:09:59.660]   That's 86 because you have such a sad effort with this hat.
[02:09:59.660 --> 02:10:00.860]   This is the new--
[02:10:00.860 --> 02:10:01.340]   [BELL RINGING]
[02:10:01.340 --> 02:10:03.020]   Then we're going to get you this hat, Ants.
[02:10:03.020 --> 02:10:05.420]   So you'll be ready for--
[02:10:05.420 --> 02:10:06.460]   [LAUGHTER]
[02:10:06.460 --> 02:10:07.420]   Oh, wow.
[02:10:07.420 --> 02:10:08.860]   [LAUGHTER]
[02:10:08.860 --> 02:10:10.620]   I actually might buy that for myself.
[02:10:10.620 --> 02:10:11.340]   That's pretty--
[02:10:11.340 --> 02:10:11.980]   I don't know.
[02:10:11.980 --> 02:10:12.620]   I think so.
[02:10:12.620 --> 02:10:13.980]   My hands are a lot of them.
[02:10:13.980 --> 02:10:14.940]   There's a lot of them.
[02:10:14.940 --> 02:10:15.260]   Yeah.
[02:10:15.260 --> 02:10:17.980]   You should see that hat Andy was wearing a Mac break weekly
[02:10:17.980 --> 02:10:18.380]   yesterday.
[02:10:18.380 --> 02:10:20.140]   It was almost as dopey as this.
[02:10:20.140 --> 02:10:22.620]   [LAUGHTER]
[02:10:22.620 --> 02:10:23.980]   This takes the cake.
[02:10:23.980 --> 02:10:25.660]   Can we buy him a loot, too?
[02:10:25.660 --> 02:10:26.180]   Yeah.
[02:10:26.180 --> 02:10:26.700]   [LAUGHTER]
[02:10:26.700 --> 02:10:28.220]   It's awesome.
[02:10:28.220 --> 02:10:28.700]   It's the--
[02:10:28.700 --> 02:10:29.820]   Go to the store.
[02:10:29.820 --> 02:10:30.620]   The medieval is--
[02:10:30.620 --> 02:10:32.460]   Shap out Hall.
[02:10:32.460 --> 02:10:33.900]   All he needs is a fife.
[02:10:33.900 --> 02:10:35.740]   Oh, he has a whole bunch of medieval--
[02:10:35.740 --> 02:10:37.260]   [LAUGHTER]
[02:10:37.260 --> 02:10:37.580]   Yeah.
[02:10:37.580 --> 02:10:38.300]   --hat crap.
[02:10:38.300 --> 02:10:40.620]   [LAUGHTER]
[02:10:40.620 --> 02:10:41.820]   That's so good.
[02:10:41.820 --> 02:10:43.660]   I like how serious he looks, even though--
[02:10:43.660 --> 02:10:45.180]   [LAUGHTER]
[02:10:45.180 --> 02:10:45.660]   --even though--
[02:10:45.660 --> 02:10:47.180]   [LAUGHTER]
[02:10:47.180 --> 02:10:49.020]   Oh, you can twist it up that--
[02:10:49.020 --> 02:10:49.740]   Look at that.
[02:10:49.740 --> 02:10:50.540]   It's a turban.
[02:10:50.540 --> 02:10:51.580]   It's a hat.
[02:10:51.580 --> 02:10:53.020]   So what is that--
[02:10:53.020 --> 02:10:53.820]   --the colors.
[02:10:53.820 --> 02:10:54.780]   --this is stuff--
[02:10:54.780 --> 02:10:55.260]   Oh.
[02:10:55.260 --> 02:10:56.780]   --like at the bottom, is that like--
[02:10:56.780 --> 02:10:57.500]   A little trim.
[02:10:57.500 --> 02:10:57.980]   --for a napkin.
[02:10:57.980 --> 02:10:58.540]   What is that?
[02:10:58.540 --> 02:10:59.580]   It's an napkin.
[02:10:59.580 --> 02:11:00.140]   It's a little napkin.
[02:11:00.140 --> 02:11:02.940]   No, the long strandy thing.
[02:11:02.940 --> 02:11:04.860]   What is that for?
[02:11:04.860 --> 02:11:06.700]   When you have a net-smoked turkey leg,
[02:11:06.700 --> 02:11:08.060]   you need to wipe your lips with something.
[02:11:08.060 --> 02:11:09.580]   Toss it over your shoulders.
[02:11:09.580 --> 02:11:10.540]   [LAUGHTER]
[02:11:10.540 --> 02:11:13.900]   People know you're a bard.
[02:11:13.900 --> 02:11:17.180]   Yeah, I'm just curious what the function of that was.
[02:11:17.180 --> 02:11:17.660]   OK.
[02:11:17.660 --> 02:11:18.460]   I sure had one.
[02:11:18.460 --> 02:11:19.500]   Oh, absolutely.
[02:11:19.500 --> 02:11:21.260]   Everybody-- probably a scarf, right?
[02:11:21.260 --> 02:11:23.340]   Everybody forget you saw this because I'm
[02:11:23.340 --> 02:11:25.340]   going to wear it next year.
[02:11:25.340 --> 02:11:28.860]   I don't want Jeff to get any credit at all.
[02:11:28.860 --> 02:11:29.900]   That is awesome.
[02:11:29.900 --> 02:11:31.500]   Absolutely hilarious.
[02:11:31.500 --> 02:11:32.540]   Oh, wait a minute.
[02:11:32.540 --> 02:11:35.100]   You have to sew this yourself?
[02:11:35.100 --> 02:11:35.900]   Oh, no.
[02:11:35.900 --> 02:11:36.700]   Yeah, I'm out.
[02:11:36.700 --> 02:11:39.220]   I'm out.
[02:11:39.220 --> 02:11:41.020]   Oh, man.
[02:11:41.020 --> 02:11:42.860]   You got to sew it yourself.
[02:11:42.860 --> 02:11:44.060]   Wow.
[02:11:44.060 --> 02:11:46.940]   Industrial manual or-- oh, I guess that's them.
[02:11:46.940 --> 02:11:48.860]   Maybe they're doing it.
[02:11:48.860 --> 02:11:49.740]   No, no, no.
[02:11:49.740 --> 02:11:53.100]   Do you want it to be so authentic that you do it yourself?
[02:11:53.100 --> 02:11:54.620]   That they do their own so--
[02:11:54.620 --> 02:11:55.660]   Hand-sewn.
[02:11:55.660 --> 02:11:57.500]   Surely they explain that.
[02:11:57.500 --> 02:11:57.980]   Oh, see.
[02:11:57.980 --> 02:11:58.940]   Yeah, probably some.
[02:11:58.940 --> 02:12:01.580]   Oh, I see.
[02:12:01.580 --> 02:12:02.900]   You don't need it to be authentic.
[02:12:02.900 --> 02:12:03.780]   No.
[02:12:03.780 --> 02:12:05.060]   No.
[02:12:05.060 --> 02:12:08.140]   There's all sorts of-- oh, Renaissance medieval elven king
[02:12:08.140 --> 02:12:10.940]   princes men's--
[02:12:10.940 --> 02:12:13.260]   circulate you put on your head.
[02:12:13.260 --> 02:12:14.500]   That's cute.
[02:12:14.500 --> 02:12:16.140]   That's a men's?
[02:12:16.140 --> 02:12:17.140]   I'm a good.
[02:12:17.140 --> 02:12:17.980]   Oh, yeah.
[02:12:17.980 --> 02:12:18.540]   That's a--
[02:12:18.540 --> 02:12:21.100]   Feels like a pretty generic thing, you know, like men will.
[02:12:21.100 --> 02:12:23.860]   Honestly, any head could wear that.
[02:12:23.860 --> 02:12:29.300]   But heavy rests the head that lies the crown on the thing.
[02:12:29.300 --> 02:12:30.660]   So there.
[02:12:30.660 --> 02:12:33.460]   Ladies and gentlemen, we've come to the concluding moments
[02:12:33.460 --> 02:12:35.820]   of this fine program.
[02:12:35.820 --> 02:12:38.900]   We thank you so much for joining us.
[02:12:38.900 --> 02:12:41.460]   We encourage you to join us again when we come back.
[02:12:41.460 --> 02:12:44.020]   Of course, next week of very nice best of--
[02:12:44.020 --> 02:12:45.020]   We'll be back to you.
[02:12:45.020 --> 02:12:45.780]   Funny stuff, love.
[02:12:45.780 --> 02:12:46.780]   So funny stuff.
[02:12:46.780 --> 02:12:47.100]   Funny stuff.
[02:12:47.100 --> 02:12:48.460]   Funny stuff.
[02:12:48.460 --> 02:12:49.460]   Speaking of funny stuff.
[02:12:49.460 --> 02:12:49.900]   More authentic.
[02:12:49.900 --> 02:12:51.220]   Lots of more authentic.
[02:12:51.220 --> 02:12:53.700]   That's Jeff Jarvis, the director of the Townite Center
[02:12:53.700 --> 02:12:57.620]   for entrepreneurial journalism at the Craig Newmark Graduate
[02:12:57.620 --> 02:13:00.780]   School of Journalism at the City University.
[02:13:00.780 --> 02:13:02.140]   And Googly Eyes.
[02:13:02.140 --> 02:13:03.940]   Hilarity with Googly Eyes.
[02:13:03.940 --> 02:13:05.900]   Is there Googly Eyes in the special--
[02:13:05.900 --> 02:13:06.460]   It's a rundown.
[02:13:06.460 --> 02:13:07.380]   It's there, yeah.
[02:13:07.380 --> 02:13:07.980]   Yeah.
[02:13:07.980 --> 02:13:08.740]   How exciting.
[02:13:08.740 --> 02:13:11.900]   And my Robo-Burger adventure.
[02:13:11.900 --> 02:13:13.700]   Yes, that was a classic.
[02:13:13.700 --> 02:13:16.900]   Jeff Jarvis covered with Greece.
[02:13:16.900 --> 02:13:18.220]   Stacey, thank you so much.
[02:13:18.220 --> 02:13:19.940]   Stacey on IOT.com.
[02:13:19.940 --> 02:13:20.820]   It's a live station.
[02:13:20.820 --> 02:13:22.740]   You must read all her stuff.
[02:13:22.740 --> 02:13:26.980]   Join, subscribe, and of course get her podcast,
[02:13:26.980 --> 02:13:31.340]   the IOT podcast with Kevin Tofel, who is also unmasted up.
[02:13:31.340 --> 02:13:34.300]   And we will see you not next time.
[02:13:34.300 --> 02:13:37.460]   But maybe we will, if you can call us from CES,
[02:13:37.460 --> 02:13:38.420]   but otherwise--
[02:13:38.420 --> 02:13:43.220]   Yeah, Aunt sent me some mics, but Jejosafetz, $200 a piece.
[02:13:43.220 --> 02:13:44.620]   We can send you a good mike.
[02:13:44.620 --> 02:13:45.140]   We can send you.
[02:13:45.140 --> 02:13:46.140]   They work.
[02:13:46.140 --> 02:13:48.660]   They work for $200.
[02:13:48.660 --> 02:13:51.020]   They better do more than what they better massage my face.
[02:13:51.020 --> 02:13:52.700]   A good mike's worth of.
[02:13:52.700 --> 02:13:54.500]   It's weight and gold.
[02:13:54.500 --> 02:13:59.740]   @proit, twit.tv/hop, hands-on-tography.
[02:13:59.740 --> 02:14:01.980]   He's not wearing a fish on his head, kids.
[02:14:01.980 --> 02:14:04.100]   That's his tongue.
[02:14:04.100 --> 02:14:08.140]   You can also buy his prints at @proit.com/prints.
[02:14:08.140 --> 02:14:11.460]   And he's the community manager at the best place,
[02:14:11.460 --> 02:14:13.220]   the happiest place on Earth.
[02:14:13.220 --> 02:14:14.940]   No, I'm not talking Disneyland.
[02:14:14.940 --> 02:14:20.980]   I am talking our beautiful, wonderful club twit.
[02:14:20.980 --> 02:14:24.300]   If you're not a member, you really ought to be one.
[02:14:24.300 --> 02:14:26.060]   Oh, he got a message on.
[02:14:26.060 --> 02:14:28.780]   A lot of folks said, I'm a member of this club
[02:14:28.780 --> 02:14:30.020]   because I love twit.
[02:14:30.020 --> 02:14:32.180]   And I'm going to care about the commercials.
[02:14:32.180 --> 02:14:33.020]   I'm not going to the commercials.
[02:14:33.020 --> 02:14:34.380]   I just want to support it.
[02:14:34.380 --> 02:14:35.740]   Here's Aunt's next hat.
[02:14:35.740 --> 02:14:36.260]   Meems.
[02:14:36.260 --> 02:14:40.140]   [LAUGHTER]
[02:14:40.140 --> 02:14:41.100]   That's all in the club.
[02:14:41.100 --> 02:14:41.780]   Now, what do you get?
[02:14:41.780 --> 02:14:43.740]   You get to add free versions of all the shows
[02:14:43.740 --> 02:14:45.300]   because you're giving us $7 a month,
[02:14:45.300 --> 02:14:48.740]   so we don't need to foist ads upon you.
[02:14:48.740 --> 02:14:50.060]   No tracking, no ads.
[02:14:50.060 --> 02:14:52.420]   You even get shows that we don't put out in public like hands-on
[02:14:52.420 --> 02:14:54.740]   windows with Paul Thorett, Hands-on Macintosh,
[02:14:54.740 --> 02:14:56.740]   with Michael Sargent, the entitled Linux Show.
[02:14:56.740 --> 02:14:59.180]   All those special events that Aunt Prute puts together
[02:14:59.180 --> 02:15:00.500]   as our community manager.
[02:15:00.500 --> 02:15:03.860]   In fact, the next event is January 12th, Stacy's Book Club
[02:15:03.860 --> 02:15:06.380]   Project Hail Mary.
[02:15:06.380 --> 02:15:07.380]   We're going to do an inside twist.
[02:15:07.380 --> 02:15:08.900]   And Fleishman was awesome.
[02:15:08.900 --> 02:15:12.020]   Fleishman, that was so much fun last week.
[02:15:12.020 --> 02:15:13.140]   Yeah, Glenn Fleishman.
[02:15:13.140 --> 02:15:16.260]   So that's on the Twit Plus feed, if you're a club member.
[02:15:16.260 --> 02:15:18.540]   All of this stuff gets on the Twit Plus feed when
[02:15:18.540 --> 02:15:21.140]   two towers going to be in February.
[02:15:21.140 --> 02:15:24.700]   Lisa and I will do an inside Twit on January 19th.
[02:15:24.700 --> 02:15:26.580]   But you got to be a club member.
[02:15:26.580 --> 02:15:29.980]   Now, another good thing to know is there's $7 a month,
[02:15:29.980 --> 02:15:33.420]   but you can also get an annual membership, pay once every year.
[02:15:33.420 --> 02:15:35.060]   And that's a good gift.
[02:15:35.060 --> 02:15:40.740]   I'm just saying, twit.tv/club, twit to find out more.
[02:15:40.740 --> 02:15:42.100]   The club really makes a difference.
[02:15:42.100 --> 02:15:45.580]   We really probably wouldn't be doing the Mastodon
[02:15:45.580 --> 02:15:47.180]   without the club because the Mastodon's
[02:15:47.180 --> 02:15:50.940]   getting pretty expensive, same with our forums, our IRC.
[02:15:50.940 --> 02:15:52.980]   You use Mastodon host?
[02:15:52.980 --> 02:15:54.180]   I do.
[02:15:54.180 --> 02:15:55.340]   He's fantastic.
[02:15:55.340 --> 02:15:57.300]   Hugo, the guy who runs it.
[02:15:57.300 --> 02:15:58.060]   And all of our data--
[02:15:58.060 --> 02:16:00.380]   Some Jake is talking to him, yeah.
[02:16:00.380 --> 02:16:00.860]   Good.
[02:16:00.860 --> 02:16:03.260]   Well, Jake's running his own instance I saw.
[02:16:03.260 --> 02:16:03.740]   Yeah, he does.
[02:16:03.740 --> 02:16:04.060]   Yeah.
[02:16:04.060 --> 02:16:07.060]   And Hugo knows a lot about--
[02:16:07.060 --> 02:16:12.580]   When the onslaught began, he shut down new subscribers,
[02:16:12.580 --> 02:16:14.660]   new customers because he couldn't support--
[02:16:14.660 --> 02:16:15.660]   we want to make sure he was--
[02:16:15.660 --> 02:16:16.620]   He also says he wants to serve--
[02:16:16.620 --> 02:16:20.300]   he wants to handle no more than 25% of instances.
[02:16:20.300 --> 02:16:20.660]   Yes.
[02:16:20.660 --> 02:16:23.620]   Because he doesn't want another monolithic being there.
[02:16:23.620 --> 02:16:23.980]   I love you.
[02:16:23.980 --> 02:16:24.460]   Way to go.
[02:16:24.460 --> 02:16:24.980]   Failure.
[02:16:24.980 --> 02:16:25.860]   I love Hugo.
[02:16:25.860 --> 02:16:28.020]   He's been fantastic.
[02:16:28.020 --> 02:16:31.300]   Yeah, it makes it very easy to have your own instance.
[02:16:31.300 --> 02:16:34.980]   I don't plug it because I don't think he wants new customers.
[02:16:34.980 --> 02:16:36.180]   He's not opening up again yet.
[02:16:36.180 --> 02:16:36.900]   Yeah.
[02:16:36.900 --> 02:16:40.380]   Masto.host is the best way to host Mastodon.
[02:16:40.380 --> 02:16:41.380]   He's been really good.
[02:16:41.380 --> 02:16:42.820]   Well, it's not hugely expensive.
[02:16:42.820 --> 02:16:44.940]   It's $350 a month right now.
[02:16:44.940 --> 02:16:45.780]   And I'm really only--
[02:16:45.780 --> 02:16:47.460]   How many users are you up to now?
[02:16:47.460 --> 02:16:49.140]   More than $5,000.
[02:16:49.140 --> 02:16:49.980]   Wow.
[02:16:49.980 --> 02:16:50.500]   Wow.
[02:16:50.500 --> 02:16:51.020]   If it gets much bigger--
[02:16:51.020 --> 02:16:52.060]   I thought somebody was two.
[02:16:52.060 --> 02:16:52.980]   Yeah.
[02:16:52.980 --> 02:16:57.540]   If it gets much bigger, it's going to probably
[02:16:57.540 --> 02:17:02.460]   have to go to another tier, kind of a custom tier.
[02:17:02.460 --> 02:17:04.020]   If you want to join--
[02:17:04.020 --> 02:17:06.900]   Anybody's listening to this should and can.
[02:17:06.900 --> 02:17:08.500]   But tell your friends if they want to join.
[02:17:08.500 --> 02:17:09.780]   It's really for Twit listeners.
[02:17:09.780 --> 02:17:12.260]   Not Club Twit, but just anybody listens to Twit.
[02:17:12.260 --> 02:17:14.580]   So say I listened to Twitter.
[02:17:14.580 --> 02:17:17.620]   I'm here to follow Jeff Jarvis or Leo or Ann or anything
[02:17:17.620 --> 02:17:18.860]   like that will get you in.
[02:17:18.860 --> 02:17:20.460]   If you just say, I'm getting off of Twitter,
[02:17:20.460 --> 02:17:23.060]   I'm not going to prove your application,
[02:17:23.060 --> 02:17:25.260]   because I just don't want to overrun by people who
[02:17:25.260 --> 02:17:26.780]   aren't Twit listeners.
[02:17:26.780 --> 02:17:28.140]   It's got to be a tight community.
[02:17:28.140 --> 02:17:29.260]   It's the joy of the community.
[02:17:29.260 --> 02:17:29.760]   Yeah.
[02:17:29.760 --> 02:17:32.940]   But there are 5,000 plus instances.
[02:17:32.940 --> 02:17:35.500]   There's plenty of places you can go.
[02:17:35.500 --> 02:17:36.500]   Don't join--
[02:17:36.500 --> 02:17:39.060]   I see that I have 1,000 people following me over here.
[02:17:39.060 --> 02:17:39.900]   I'm like, wow.
[02:17:39.900 --> 02:17:43.300]   Don't join the big instances, is my suggestion.
[02:17:43.300 --> 02:17:43.740]   Like I did.
[02:17:43.740 --> 02:17:45.300]   You're unmested on that social.
[02:17:45.300 --> 02:17:47.620]   That's half a million people.
[02:17:47.620 --> 02:17:48.460]   And--
[02:17:48.460 --> 02:17:50.540]   So the local feed is useless to me.
[02:17:50.540 --> 02:17:51.660]   Yeah.
[02:17:51.660 --> 02:17:55.020]   Go somewhere where it's people that you know or--
[02:17:55.020 --> 02:17:57.060]   there's a San Francisco Bay Area instance.
[02:17:57.060 --> 02:17:58.060]   For instance, go somewhere.
[02:17:58.060 --> 02:17:59.140]   It's geographic.
[02:17:59.140 --> 02:18:00.300]   It's FBA.
[02:18:00.300 --> 02:18:01.300]   Social.
[02:18:01.300 --> 02:18:04.060]   Go somewhere for migraine sufferers, whatever it is.
[02:18:04.060 --> 02:18:06.220]   That's actually a pretty quiet instance.
[02:18:06.220 --> 02:18:08.020]   But anyway, there's instances for every--
[02:18:08.020 --> 02:18:10.660]   Stop shouting.
[02:18:10.660 --> 02:18:12.020]   Would you all be quiet?
[02:18:12.020 --> 02:18:13.020]   Everybody.
[02:18:13.020 --> 02:18:14.020]   Yes.
[02:18:14.020 --> 02:18:14.520]   It's--
[02:18:14.520 --> 02:18:15.340]   I have to say--
[02:18:15.340 --> 02:18:16.780]   For the wind.
[02:18:16.780 --> 02:18:17.700]   A dark mode for all.
[02:18:17.700 --> 02:18:21.020]   I always had problems with Twitter fights,
[02:18:21.020 --> 02:18:23.460]   and it always made me kind of queasy.
[02:18:23.460 --> 02:18:24.660]   Mass, and it's just--
[02:18:24.660 --> 02:18:25.620]   I love it.
[02:18:25.620 --> 02:18:26.060]   I've just--
[02:18:26.060 --> 02:18:26.740]   Really?
[02:18:26.740 --> 02:18:27.900]   There's good information.
[02:18:27.900 --> 02:18:29.420]   I should have listened to you a long ago.
[02:18:29.420 --> 02:18:29.860]   Yeah.
[02:18:29.860 --> 02:18:31.140]   Well, it wasn't like this long.
[02:18:31.140 --> 02:18:32.140]   It wasn't like this.
[02:18:32.140 --> 02:18:33.500]   It was like the migraine one.
[02:18:33.500 --> 02:18:35.900]   It was very quiet.
[02:18:35.900 --> 02:18:39.540]   But it's got a little more interesting now.
[02:18:39.540 --> 02:18:41.500]   And follow people.
[02:18:41.500 --> 02:18:43.900]   There are a number of good tools
[02:18:43.900 --> 02:18:48.060]   for using to follow people over from Twitter.
[02:18:48.060 --> 02:18:51.140]   Actually, Paul Thorett came up with a really nice one
[02:18:51.140 --> 02:18:53.660]   that Willow Remus had mentioned unmasted on.
[02:18:53.660 --> 02:18:55.820]   Let me see if I can find the name.
[02:18:55.820 --> 02:18:58.460]   Because what it does is it goes out,
[02:18:58.460 --> 02:19:00.660]   scrapes all the people off to Twitter,
[02:19:00.660 --> 02:19:02.500]   and you could follow them on your instance
[02:19:02.500 --> 02:19:03.260]   with one click of the book.
[02:19:03.260 --> 02:19:04.580]   Move to Don.org.
[02:19:04.580 --> 02:19:05.580]   Move to Don.
[02:19:05.580 --> 02:19:06.860]   Move to Don.org.
[02:19:06.860 --> 02:19:07.340]   Yeah.
[02:19:07.340 --> 02:19:07.940]   It's brilliant.
[02:19:07.940 --> 02:19:09.660]   Yeah, he's great.
[02:19:09.660 --> 02:19:11.180]   You have to sign into your Twitter account
[02:19:11.180 --> 02:19:12.180]   and your Mastodon account.
[02:19:12.180 --> 02:19:13.780]   So create a Mastodon account.
[02:19:13.780 --> 02:19:14.340]   First--
[02:19:14.340 --> 02:19:15.900]   There are five people who have mentioned
[02:19:15.900 --> 02:19:20.220]   a Mastodon address in their profile or a pinned tweet.
[02:19:20.220 --> 02:19:22.180]   And unlike the others, which only give you
[02:19:22.180 --> 02:19:26.700]   a CSV of everyone, this lets you pick one off at a time
[02:19:26.700 --> 02:19:28.220]   of who you want to follow on Mastodon.
[02:19:28.220 --> 02:19:28.860]   Yeah.
[02:19:28.860 --> 02:19:30.580]   And it does a really good job.
[02:19:30.580 --> 02:19:31.420]   I am.
[02:19:31.420 --> 02:19:31.900]   It's really slick.
[02:19:31.900 --> 02:19:34.060]   If you are feeling like, oh, I'm on Mastodon,
[02:19:34.060 --> 02:19:37.500]   but nobody I know is here, just go through the process.
[02:19:37.500 --> 02:19:39.460]   This gives you a starter kit of buddies.
[02:19:39.460 --> 02:19:40.260]   Yeah.
[02:19:40.260 --> 02:19:41.300]   It scrapes it through.
[02:19:41.300 --> 02:19:43.380]   And you could see there's a lot of people.
[02:19:43.380 --> 02:19:44.660]   I'm following most of these.
[02:19:44.660 --> 02:19:47.700]   You can sort it by when they joined Mastodon,
[02:19:47.700 --> 02:19:50.820]   how long they've been on, or when they've most recently
[02:19:50.820 --> 02:19:51.900]   been active.
[02:19:51.900 --> 02:19:56.460]   So it gives you a really good idea of who you're going to follow.
[02:19:56.460 --> 02:19:59.540]   As you can see, I've followed everybody on here.
[02:19:59.540 --> 02:20:00.540]   Here's somebody.
[02:20:00.540 --> 02:20:01.540]   James Crickland.
[02:20:01.540 --> 02:20:02.540]   Yep.
[02:20:02.540 --> 02:20:03.540]   Kind of follow you.
[02:20:03.540 --> 02:20:04.540]   James Podcaster.
[02:20:04.540 --> 02:20:08.540]   Connie Guglielmo, a regular on our shows, the editor and chief
[02:20:08.540 --> 02:20:09.540]   of CNET.
[02:20:09.540 --> 02:20:12.540]   I thought I already followed Adrianne Langston.
[02:20:12.540 --> 02:20:13.540]   So maybe this--
[02:20:13.540 --> 02:20:15.540]   But sometimes it does that sometimes.
[02:20:15.540 --> 02:20:17.540]   It shows me places that I thought I'd follow.
[02:20:17.540 --> 02:20:19.020]   Might be an alternate account.
[02:20:19.020 --> 02:20:21.100]   Some people have multiple accounts, that kind of thing.
[02:20:21.100 --> 02:20:22.540]   Anyway, it's very handy.
[02:20:22.540 --> 02:20:24.540]   And you can really bring every-- there's Kevin Tofol.
[02:20:24.540 --> 02:20:26.900]   You get the name of the guy to give him credit,
[02:20:26.900 --> 02:20:28.060]   but it's good.
[02:20:28.060 --> 02:20:29.060]   Yeah.
[02:20:29.060 --> 02:20:31.740]   So move to don.org.
[02:20:31.740 --> 02:20:34.300]   Our Mastodon instance is twit.social.
[02:20:34.300 --> 02:20:36.460]   If you want to support it, we don't take direct donations,
[02:20:36.460 --> 02:20:37.460]   but join ClubTwit.
[02:20:37.460 --> 02:20:38.460]   You'll be supporting it.
[02:20:38.460 --> 02:20:39.460]   Yes.
[02:20:39.460 --> 02:20:40.460]   Twit.tv/clubtwit.
[02:20:40.460 --> 02:20:41.620]   Thank you, everybody.
[02:20:41.620 --> 02:20:43.420]   I hope you have a wonderful holiday.
[02:20:43.420 --> 02:20:45.260]   Enjoy your Festivus.
[02:20:45.260 --> 02:20:49.940]   Don't get too angry in the airing of grievances, but let him out.
[02:20:49.940 --> 02:20:50.940]   Let it out.
[02:20:50.940 --> 02:20:54.420]   You'll feel much better, much better.
[02:20:54.420 --> 02:20:55.420]   And then have a waffle.
[02:20:55.420 --> 02:20:56.420]   And then have a waffle.
[02:20:56.420 --> 02:20:57.420]   And life is good.
[02:20:57.420 --> 02:21:01.100]   Christmas Eve, Hanukkah-- this is the third night, I think,
[02:21:01.100 --> 02:21:02.500]   of fourth night of Hanukkah.
[02:21:02.500 --> 02:21:03.420]   Kwanzaa.
[02:21:03.420 --> 02:21:05.500]   Kwanzaa.
[02:21:05.500 --> 02:21:07.460]   Or just celebrate the solstice.
[02:21:07.460 --> 02:21:09.980]   But whatever it is that you do at this time, this dark time
[02:21:09.980 --> 02:21:13.660]   of year, make sure there's lots of light, love, and family
[02:21:13.660 --> 02:21:14.620]   all around you.
[02:21:14.620 --> 02:21:15.460]   And cookies.
[02:21:15.460 --> 02:21:17.060]   And cookies and waffles.
[02:21:17.060 --> 02:21:17.540]   Yes.
[02:21:17.540 --> 02:21:18.420]   Or donuts.
[02:21:18.420 --> 02:21:21.340]   And have a happy new year, too.
[02:21:21.340 --> 02:21:23.700]   And we'll all be back next year.
[02:21:23.700 --> 02:21:24.580]   Thanks for joining us.
[02:21:24.580 --> 02:21:26.820]   It's been great being with you this year.
[02:21:26.820 --> 02:21:28.420]   And see you next time on Twig.
[02:21:28.420 --> 02:21:29.100]   Thanks, everybody.
[02:21:29.100 --> 02:21:30.820]   Bye, my friends.
[02:21:30.820 --> 02:21:31.740]   Bye.
[02:21:31.740 --> 02:21:32.740]   Bye.
[02:21:32.740 --> 02:21:33.380]   Bye, young.
[02:21:33.380 --> 02:21:35.820]   Don't miss all about Android every week.
[02:21:35.820 --> 02:21:38.780]   We talk about the latest news, hardware, apps,
[02:21:38.780 --> 02:21:40.940]   and now all the developer-y goodness
[02:21:40.940 --> 02:21:43.260]   happening in the Android ecosystem.
[02:21:43.260 --> 02:21:46.100]   I'm Jason Howell, also joined by Ron Richards, Florence
[02:21:46.100 --> 02:21:48.700]   Ion, and our newest co-host on the panel,
[02:21:48.700 --> 02:21:51.740]   When To Dao Who Brings Her Developer Chops.
[02:21:51.740 --> 02:21:52.700]   Really great stuff.
[02:21:52.700 --> 02:21:55.740]   We also invite people from all over the Android ecosystem
[02:21:55.740 --> 02:21:59.060]   to talk about this mobile platform we love so much.
[02:21:59.060 --> 02:22:02.540]   Join us every Tuesday, all about Android, on twit.tv.
[02:22:02.540 --> 02:22:05.540]   [MUSIC PLAYING]
[02:22:05.540 --> 02:22:14.020]   I am ready.
[02:22:14.020 --> 02:22:16.780]   I've got my festive hat on.
[02:22:16.780 --> 02:22:18.340]   I've got my--
[02:22:18.340 --> 02:22:19.460]   Look at that jacket.
[02:22:19.460 --> 02:22:23.100]   I've got my festive jacket on.
[02:22:23.100 --> 02:22:23.740]   Wow.
[02:22:23.740 --> 02:22:26.060]   [LAUGHTER]
[02:22:26.060 --> 02:22:29.060]   Doesn't really fit very well.
[02:22:29.060 --> 02:22:31.420]   I've got to be honest, that jacket
[02:22:31.420 --> 02:22:33.540]   may incite another migraine.
[02:22:33.540 --> 02:22:35.620]   I'm just going to be real, real, real.
[02:22:35.620 --> 02:22:38.940]   It is a-- it might be in me.
[02:22:38.940 --> 02:22:39.700]   All right, let's do it.
[02:22:39.700 --> 02:22:40.340]   Let's get going.
[02:22:40.340 --> 02:22:41.380]   Sorry, we're so late.
[02:22:41.380 --> 02:22:42.940]   I apologize.
[02:22:42.940 --> 02:22:45.660]   But this is the last show of the year.
[02:22:45.660 --> 02:22:46.460]   Wait a minute.
[02:22:46.460 --> 02:22:50.300]   What does my hat say?
[02:22:50.300 --> 02:22:51.060]   Catgirl.
[02:22:51.060 --> 02:22:52.020]   What does it say?
[02:22:52.020 --> 02:22:53.020]   Catgirl.
[02:22:53.020 --> 02:22:56.260]   I thought this was a Christmas hat.
[02:22:56.260 --> 02:22:57.580]   What did you grab?
[02:22:57.580 --> 02:22:59.380]   I thought it was a MAGA hat, because it's
[02:22:59.380 --> 02:23:00.980]   like camouflage and red.
[02:23:00.980 --> 02:23:01.460]   So who is that?
[02:23:01.460 --> 02:23:02.420]   Let's put on a Christmas hat.
[02:23:02.420 --> 02:23:04.420]   Oh, boy.
[02:23:04.420 --> 02:23:05.340]   Wait a minute.
[02:23:05.340 --> 02:23:06.980]   Here we go.
[02:23:06.980 --> 02:23:09.740]   It's not a Christmas hat.
[02:23:09.740 --> 02:23:10.740]   I don't know what it is.
[02:23:10.740 --> 02:23:11.100]   It's a wardrobe.
[02:23:11.100 --> 02:23:14.300]   Maybe Antnows, because it's got an "N" on it.
[02:23:14.300 --> 02:23:16.060]   That looks like Nebraska.
[02:23:16.060 --> 02:23:17.460]   Am I a corn husker?
[02:23:17.460 --> 02:23:19.780]   Looks like Nebraska's in.
[02:23:19.780 --> 02:23:20.820]   That red.
[02:23:20.820 --> 02:23:22.260]   I'm a corn husker.
[02:23:22.260 --> 02:23:22.780]   All right.
[02:23:22.780 --> 02:23:23.780]   Well, I got some other ones.
[02:23:23.780 --> 02:23:24.140]   Wait a minute.
[02:23:24.140 --> 02:23:25.140]   This is-- oh, this is good.
[02:23:25.140 --> 02:23:27.300]   This is a Santa hat.
[02:23:27.300 --> 02:23:28.300]   There you go.
[02:23:28.300 --> 02:23:29.140]   Yes, you got a Santa.
[02:23:29.140 --> 02:23:29.940]   Y'all make up.
[02:23:29.940 --> 02:23:31.780]   Like my stocking does.
[02:23:31.780 --> 02:23:34.500]   That's not going to work.
[02:23:34.500 --> 02:23:38.220]   It's a good thing I brought a few of them out.
[02:23:38.220 --> 02:23:39.180]   There you go.
[02:23:39.180 --> 02:23:40.020]   You're crown.
[02:23:40.020 --> 02:23:40.860]   I don't know what the hell.
[02:23:40.860 --> 02:23:44.340]   I mean, are we doing Alice in Wonderland after this?
[02:23:44.340 --> 02:23:45.980]   Yeah, I'm not sure what that is.
[02:23:45.980 --> 02:23:47.940]   That's not--
[02:23:47.940 --> 02:23:49.940]   He just has a whole bunch of hats just sitting there.
[02:23:49.940 --> 02:23:50.460]   Yeah.
[02:23:50.460 --> 02:23:51.140]   Oh, I do.
[02:23:51.140 --> 02:23:52.220]   Oh, this is it.
[02:23:52.220 --> 02:23:54.620]   I got it now.
[02:23:54.620 --> 02:23:56.060]   There you go.
[02:23:56.060 --> 02:23:56.900]   That's what we'll do.
[02:23:56.900 --> 02:23:58.100]   The camera's too low.
[02:23:58.100 --> 02:23:59.100]   That'll stay out.
[02:23:59.100 --> 02:24:01.260]   Come back to the rock top.
[02:24:01.260 --> 02:24:02.260]   You can put your finger on top.
[02:24:02.260 --> 02:24:03.260]   You can put your finger on top.
[02:24:03.260 --> 02:24:04.100]   You can put your finger on top.
[02:24:04.100 --> 02:24:06.100]   You can put your finger on top.
[02:24:06.100 --> 02:24:07.780]   [LAUGHTER]
[02:24:07.780 --> 02:24:08.700]   Oh, yeah.
[02:24:08.700 --> 02:24:11.100]   It's like I had the cup on my head.
[02:24:11.100 --> 02:24:13.980]   I don't know what this fascination is with Santa's lower
[02:24:13.980 --> 02:24:16.140]   half, but--
[02:24:16.140 --> 02:24:20.940]   This is gold.
[02:24:20.940 --> 02:24:21.940]   I don't know what this is.
[02:24:21.940 --> 02:24:27.500]   It was funny as people tune in to watch the show.
[02:24:27.500 --> 02:24:28.060]   I don't know.
[02:24:28.060 --> 02:24:29.300]   I don't know why.
[02:24:29.300 --> 02:24:31.460]   Yeah, I got a message to me.
[02:24:31.460 --> 02:24:34.140]   Watch-- it says, I stop in the middle of whatever I'm
[02:24:34.140 --> 02:24:35.700]   listening to when I get my twig.
[02:24:35.700 --> 02:24:39.020]   All right.
[02:24:39.020 --> 02:24:40.020]   Y'all.
[02:24:40.020 --> 02:24:43.860]   That's not good either, actually.
[02:24:43.860 --> 02:24:44.620]   Although--
[02:24:44.620 --> 02:24:45.780]   Hilarious.
[02:24:45.780 --> 02:24:46.620]   When I wore this--
[02:24:46.620 --> 02:24:49.220]   You can't put it out of your head in.
[02:24:49.220 --> 02:24:51.620]   I think Bird came running in with a giant wrench
[02:24:51.620 --> 02:24:55.820]   and said, you've got to cut off that ball.
[02:24:55.820 --> 02:24:57.700]   Oh, that's right.
[02:24:57.700 --> 02:24:58.780]   That bell is annoying.
[02:24:58.780 --> 02:24:59.380]   Is that--
[02:24:59.380 --> 02:24:59.380]   Is that--
[02:24:59.380 --> 02:25:00.380]   Well, drive--
[02:25:00.380 --> 02:25:04.020]   Is that going to give you another migraine?
[02:25:04.020 --> 02:25:06.700]   No, because if I feel anything coming on, I'll just duck
[02:25:06.700 --> 02:25:07.220]   and go.
[02:25:07.220 --> 02:25:08.860]   I'm out of here.
[02:25:08.860 --> 02:25:11.540]   I won't do this.
[02:25:11.540 --> 02:25:12.180]   I promise.
[02:25:12.180 --> 02:25:12.940]   I just--
[02:25:12.940 --> 02:25:14.260]   Oh, he's going to shake his head at me.
[02:25:14.260 --> 02:25:14.660]   I know.
[02:25:14.660 --> 02:25:16.220]   No.

