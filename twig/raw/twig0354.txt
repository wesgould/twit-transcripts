;FFMETADATA1
title=Bacon Explosion
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=354
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2016
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.400]   It's time for Twig. This week in Google, our newest host joins us. Stacey Higginbotham.
[00:00:06.400 --> 00:00:11.400]   Along with Matthew Ingram from Fortune Magazine, Erin Nukem. He's got a cool new chip for us.
[00:00:11.400 --> 00:00:18.000]   We'll talk about the latest IoT device from Google and some big changes at Twitter.
[00:00:18.000 --> 00:00:21.000]   It's all ahead on Twig.
[00:00:21.000 --> 00:00:24.000]   Netcast you love.
[00:00:24.000 --> 00:00:27.000]   From people you trust.
[00:00:27.000 --> 00:00:32.000]   This is Twig.
[00:00:32.000 --> 00:00:37.000]   Bandwidth for this week in Google is provided by cash fly.
[00:00:37.000 --> 00:00:44.000]   C-A-C-H-E-F-L-Y dot com.
[00:00:44.000 --> 00:00:52.000]   This is Twig. This week in Google, Episode 354, recorded Wednesday, May 25, 2016.
[00:00:52.000 --> 00:00:54.000]   Bacon Explosion.
[00:00:54.000 --> 00:00:58.000]   This week in Google is brought to you by Zip Recruiter. Are you hiring?
[00:00:58.000 --> 00:01:04.000]   With ZipRecruiter.com you could post to more than 100+ job sites, including social networks, all with a single click.
[00:01:04.000 --> 00:01:07.000]   Screen, rate and hire the right candidates fast.
[00:01:07.000 --> 00:01:13.000]   Try ZipRecruiter with a free four day trial now at ziprecruiter.com/twig.
[00:01:13.000 --> 00:01:22.000]   And by Blue Apron. Blue Apron will send you fresh, high quality ingredients to cook delicious meals with simple step by step instructions right to your door.
[00:01:22.000 --> 00:01:28.000]   See what's on the menu this week and get your first two meals free with free shipping by going to Blue Apron.com/twig.
[00:01:28.000 --> 00:01:31.000]   That's Blue Apron.com/twig.
[00:01:31.000 --> 00:01:41.000]   And by the Ring Video Doorbell with Ring, you can see and talk to anyone at your door from anywhere in the world using your smartphone.
[00:01:41.000 --> 00:01:43.000]   It's like color ID for your home.
[00:01:43.000 --> 00:01:49.000]   Right now get free expedited FedEx shipping when you go to Ring.com/twig.
[00:01:49.000 --> 00:02:00.000]   It's time for Twig this week in Google to show a week over the latest news from the Google verse and the Facebook cloud and the Dropbox and the Twitter and all that jazz.
[00:02:00.000 --> 00:02:08.000]   Jeff Jarvis has the week off. He's traveling. But you know what? We've run in a pretty good team today sitting next to me Aaron Newcomb.
[00:02:08.000 --> 00:02:10.000]   Yes. From the Benetia Maker Space.
[00:02:10.000 --> 00:02:12.000]   That's right. And NetApp. And NetApp.
[00:02:12.000 --> 00:02:16.000]   A regular on Floss Weekly and all about Android and on this show too. It's nice to see you again Aaron.
[00:02:16.000 --> 00:02:17.000]   Yeah, glad to be here.
[00:02:17.000 --> 00:02:19.000]   Always glad to be on Twig.
[00:02:19.000 --> 00:02:22.000]   You're on New... I keep forgetting you've been on the new screen savers like three or four times.
[00:02:22.000 --> 00:02:23.000]   I have a couple times, yes.
[00:02:23.000 --> 00:02:26.000]   So you're a regular around here. We love having you.
[00:02:26.000 --> 00:02:29.000]   Also regular on this show Matthew Ingram.
[00:02:29.000 --> 00:02:32.000]   Really thrilled to have him from Fortune.com.
[00:02:32.000 --> 00:02:34.000]   Matthew with 1T.
[00:02:34.000 --> 00:02:37.000]   All the way up from...
[00:02:37.000 --> 00:02:39.000]   Visiting us from a Tamwa.
[00:02:39.000 --> 00:02:40.000]   No.
[00:02:40.000 --> 00:02:41.000]   Ottawa.
[00:02:41.000 --> 00:02:42.000]   No.
[00:02:42.000 --> 00:02:43.000]   Toronto.
[00:02:43.000 --> 00:02:44.000]   That's it.
[00:02:44.000 --> 00:02:45.000]   Right.
[00:02:45.000 --> 00:02:47.000]   Great names.
[00:02:47.000 --> 00:02:48.000]   Nice to see you.
[00:02:48.000 --> 00:02:53.000]   He's using Linux and Skype on Linux today which will be an adventure.
[00:02:53.000 --> 00:02:54.000]   Ubuntu?
[00:02:54.000 --> 00:02:56.000]   Yeah.
[00:02:56.000 --> 00:02:57.000]   Ubuntu.
[00:02:57.000 --> 00:02:58.000]   Mint.
[00:02:58.000 --> 00:02:59.000]   Linux Mint.
[00:02:59.000 --> 00:03:00.000]   Yeah.
[00:03:00.000 --> 00:03:01.000]   Oh, use Mint.
[00:03:01.000 --> 00:03:04.000]   I decided I reverted to Debian. I went all the way back.
[00:03:04.000 --> 00:03:05.000]   Yeah.
[00:03:05.000 --> 00:03:06.000]   That works.
[00:03:06.000 --> 00:03:07.000]   Debian's cool.
[00:03:07.000 --> 00:03:13.000]   Although there's a standing joke in the Reddit forms for Linux that if you're an arch user,
[00:03:13.000 --> 00:03:16.000]   how long does it take before you... people find out, you know, like two minutes?
[00:03:16.000 --> 00:03:17.000]   Everybody, you have the nice place.
[00:03:17.000 --> 00:03:18.000]   Hey, look who's here.
[00:03:18.000 --> 00:03:20.000]   Now, I'm really excited about this.
[00:03:20.000 --> 00:03:24.000]   Stacey Higginbotham is here from staceyoniot.com.
[00:03:24.000 --> 00:03:26.000]   Your longtime colleague of Matthew, right?
[00:03:26.000 --> 00:03:28.000]   You said you worked together on two jobs.
[00:03:28.000 --> 00:03:29.000]   Mm-hmm.
[00:03:29.000 --> 00:03:31.000]   Yes, a fortune and a gig of home.
[00:03:31.000 --> 00:03:32.000]   Wow.
[00:03:32.000 --> 00:03:36.000]   Were you managing editor at Gig of Home at the end?
[00:03:36.000 --> 00:03:39.000]   I was managing editor for like six months at Gig of Home.
[00:03:39.000 --> 00:03:40.000]   Yeah.
[00:03:40.000 --> 00:03:42.000]   And then I gave it over to Tom Krasit.
[00:03:42.000 --> 00:03:43.000]   Yeah, no fun.
[00:03:43.000 --> 00:03:44.000]   It is terrible.
[00:03:44.000 --> 00:03:45.000]   Yeah.
[00:03:45.000 --> 00:03:49.000]   And then fortune and then you decided, what's all this work stuff about?
[00:03:49.000 --> 00:03:52.000]   I'm going to become a podcaster.
[00:03:52.000 --> 00:03:55.000]   I test gadgets full time and be a podcaster.
[00:03:55.000 --> 00:03:57.000]   What are you nuts?
[00:03:57.000 --> 00:03:59.000]   I'm actually still writing.
[00:03:59.000 --> 00:04:06.000]   Thrilled to say that Stacey's agreed to make a regular stop on her weekly schedule because
[00:04:06.000 --> 00:04:11.000]   ever since Gina Trepani left, we've been looking for a suitable replacement and it's not been
[00:04:11.000 --> 00:04:12.000]   easy.
[00:04:12.000 --> 00:04:16.000]   And of course, these guys come a lot, but they have actual jobs.
[00:04:16.000 --> 00:04:18.000]   So we need somebody who, it's an unusual combination.
[00:04:18.000 --> 00:04:19.000]   Somebody unemployed.
[00:04:19.000 --> 00:04:20.000]   [laughter]
[00:04:20.000 --> 00:04:22.000]   I'm just teasing you.
[00:04:22.000 --> 00:04:24.000]   I'm really glad.
[00:04:24.000 --> 00:04:25.000]   I have a job.
[00:04:25.000 --> 00:04:26.000]   You have a job.
[00:04:26.000 --> 00:04:27.000]   She's busy.
[00:04:27.000 --> 00:04:32.000]   She's, as most self-employed folks are probably busier than an employed person, really.
[00:04:32.000 --> 00:04:35.000]   Yeah, I wouldn't go that far.
[00:04:35.000 --> 00:04:37.000]   Anyway, welcome.
[00:04:37.000 --> 00:04:39.000]   We're really thrilled to have you.
[00:04:39.000 --> 00:04:41.000]   Nice to meet you.
[00:04:41.000 --> 00:04:42.000]   Yeah.
[00:04:42.000 --> 00:04:44.000]   It's really great.
[00:04:44.000 --> 00:04:50.360]   So none of you have been here since Google I/O. We covered it, of course, kind of did
[00:04:50.360 --> 00:04:55.560]   the quick coverage because our show was yesterday, last week, right after the keynote.
[00:04:55.560 --> 00:04:57.280]   And then of course, there were more sessions.
[00:04:57.280 --> 00:05:01.000]   We learned more about, for instance, something we didn't know about at the time.
[00:05:01.000 --> 00:05:03.000]   So maybe we'll start with that.
[00:05:03.000 --> 00:05:04.000]   Android on Chrome.
[00:05:04.000 --> 00:05:05.000]   Yeah.
[00:05:05.000 --> 00:05:06.000]   Huge.
[00:05:06.000 --> 00:05:07.000]   Huge.
[00:05:07.000 --> 00:05:08.000]   All right.
[00:05:08.000 --> 00:05:10.000]   Why is it so huge?
[00:05:10.000 --> 00:05:11.000]   Aaron?
[00:05:11.000 --> 00:05:13.000]   Because they're native apps.
[00:05:13.000 --> 00:05:16.720]   Again, we always have this debate on this show, right, whether Chrome OS or Android is
[00:05:16.720 --> 00:05:17.720]   better.
[00:05:17.720 --> 00:05:19.000]   Now you have the best of both worlds.
[00:05:19.000 --> 00:05:23.080]   So you can run your Chrome OS and then you can, oh, there's this app I really want to
[00:05:23.080 --> 00:05:24.080]   run.
[00:05:24.080 --> 00:05:27.080]   And not only does it run, but it runs and it runs natively.
[00:05:27.080 --> 00:05:30.360]   If you go out and look at some of the verge coverage on this, you'll see they demoed a
[00:05:30.360 --> 00:05:33.000]   little bit of this of apps running in Chrome.
[00:05:33.000 --> 00:05:39.000]   They did full-size Gmail and full-size couple of full-size apps running on the Pixel and
[00:05:39.000 --> 00:05:40.800]   it looks great and it performs great.
[00:05:40.800 --> 00:05:42.280]   And now you don't have to make a choice.
[00:05:42.280 --> 00:05:46.120]   You can just say, oh, yeah, I want that app and I want it to run here, but I want all
[00:05:46.120 --> 00:05:49.000]   the native functionality that I have in Chrome OS and you've got both.
[00:05:49.000 --> 00:05:50.840]   So I think it's really fantastic.
[00:05:50.840 --> 00:05:56.840]   And I think eventually as we head towards that merge, that eventual merge that's going
[00:05:56.840 --> 00:06:00.080]   to happen, I think between Chrome OS and Android, this is one step towards that.
[00:06:00.080 --> 00:06:02.280]   So I'm really excited to see it.
[00:06:02.280 --> 00:06:05.320]   What's interesting is it's running.
[00:06:05.320 --> 00:06:09.320]   They had, in fact, if you watch the seminar, it's on YouTube.
[00:06:09.320 --> 00:06:13.480]   You can watch the talk, which was on Thursday.
[00:06:13.480 --> 00:06:17.240]   They had tried other ways of doing this, including something called Android Runtime
[00:06:17.240 --> 00:06:20.840]   for Chrome, which they decided not to use.
[00:06:20.840 --> 00:06:23.200]   They said, you know, this didn't work too well.
[00:06:23.200 --> 00:06:25.440]   So they're doing containers, basically, now, right?
[00:06:25.440 --> 00:06:28.200]   Docker-style containers.
[00:06:28.200 --> 00:06:32.720]   I think the most important upshot of that is if you're a developer, you don't have to
[00:06:32.720 --> 00:06:34.640]   change your app at all.
[00:06:34.640 --> 00:06:36.120]   Right.
[00:06:36.120 --> 00:06:41.880]   So that means immediately, and this is going to start on people who have, there's three
[00:06:41.880 --> 00:06:43.080]   models.
[00:06:43.080 --> 00:06:48.280]   If you're in the developer channel on your Chromebook start next month, I immediately
[00:06:48.280 --> 00:06:50.480]   put my Chromebook pixel in the developer channel.
[00:06:50.480 --> 00:06:53.320]   So I'll let you know when we get it, but that should start in June.
[00:06:53.320 --> 00:06:57.200]   And then they expect it to roll out Chrome OS wide with some exceptions, some of the
[00:06:57.200 --> 00:07:02.640]   older Chromebooks, I think, don't do this in September.
[00:07:02.640 --> 00:07:11.680]   Here is the demo from Google I/O, which unfortunately didn't go too well because they
[00:07:11.680 --> 00:07:23.040]   didn't have internet access, but she demoed how she could use Skype on Linux failed, huh?
[00:07:23.040 --> 00:07:25.040]   That didn't take long.
[00:07:25.040 --> 00:07:28.320]   We're going to get Matt to boot into Windows.
[00:07:28.320 --> 00:07:30.040]   That's not, by the way, Linux's fault.
[00:07:30.040 --> 00:07:31.840]   That's Skype's microsustval.
[00:07:31.840 --> 00:07:38.240]   So she's using Photoshop Mix on a Chromebook to do Photoshop-style editing.
[00:07:38.240 --> 00:07:40.800]   We'll go ahead and cut out that.
[00:07:40.800 --> 00:07:42.200]   We can zoom in.
[00:07:42.200 --> 00:07:45.920]   Will the app support, if I guess if you have a pixel they'll support touch.
[00:07:45.920 --> 00:07:46.920]   Right.
[00:07:46.920 --> 00:07:47.920]   Yep, they do.
[00:07:47.920 --> 00:07:52.560]   In fact, they were showing that on the video that I saw, he was actually squeezing, pinching,
[00:07:52.560 --> 00:07:59.000]   zooming, and touching to move the cursor around as he wanted to enter text and things like
[00:07:59.000 --> 00:08:00.000]   that.
[00:08:00.000 --> 00:08:01.640]   So it's very, very cool.
[00:08:01.640 --> 00:08:02.640]   Very cool.
[00:08:02.640 --> 00:08:07.160]   They did say that if you wanted to optimize for this as a developer, you might want to
[00:08:07.160 --> 00:08:08.320]   look at layouts, right?
[00:08:08.320 --> 00:08:10.360]   Because that's the only issue.
[00:08:10.360 --> 00:08:13.720]   Stacey, what do you think this means for Chrome OS?
[00:08:13.720 --> 00:08:18.080]   Does this mean Chrome OS suddenly is going to, it's already very successful in education.
[00:08:18.080 --> 00:08:19.680]   It's going to go mainstream?
[00:08:19.680 --> 00:08:26.040]   Yeah, I would think that you could put this on, I mean, I didn't pay too much attention
[00:08:26.040 --> 00:08:28.320]   to this because I was more focused on the device side.
[00:08:28.320 --> 00:08:31.240]   Yeah, we'll get to the home because that's your thing.
[00:08:31.240 --> 00:08:32.240]   I know.
[00:08:32.240 --> 00:08:33.240]   Home and chips.
[00:08:33.240 --> 00:08:34.240]   That's where I was excited.
[00:08:34.240 --> 00:08:42.440]   But this gives me more reason to buy a Chromebook for my daughter because right now she's got
[00:08:42.440 --> 00:08:43.760]   a MacBook.
[00:08:43.760 --> 00:08:46.760]   But it's also, it could change the way people develop.
[00:08:46.760 --> 00:08:52.320]   I mean, as these things start moving together, it gives you a reason to touch on a PC, but
[00:08:52.320 --> 00:08:55.040]   a laptop form factor, perhaps.
[00:08:55.040 --> 00:08:56.040]   I don't know.
[00:08:56.040 --> 00:08:58.520]   This is just me thinking ahead and getting all crazy here.
[00:08:58.520 --> 00:09:00.240]   You know, schools will probably turn it off.
[00:09:00.240 --> 00:09:03.480]   So we'll sum enterprises because they don't want kids playing Minecraft.
[00:09:03.480 --> 00:09:05.440]   That's one of the advantages of a Chromebook.
[00:09:05.440 --> 00:09:06.440]   Here's my question.
[00:09:06.440 --> 00:09:08.640]   Maybe you could answer this, Aaron.
[00:09:08.640 --> 00:09:14.040]   My sense is because of the way they're doing this with containers, even though they're
[00:09:14.040 --> 00:09:19.240]   talking about Chrome OS and Chromebooks, I think this could probably work on Windows
[00:09:19.240 --> 00:09:20.240]   and Mac as well.
[00:09:20.240 --> 00:09:22.800]   I could do this in any Chrome browser.
[00:09:22.800 --> 00:09:24.040]   You should be able to.
[00:09:24.040 --> 00:09:25.040]   Absolutely.
[00:09:25.040 --> 00:09:28.760]   They may artificially restrict that initially, but I don't.
[00:09:28.760 --> 00:09:30.520]   I mean, why not?
[00:09:30.520 --> 00:09:35.200]   And that means Chrome is also, and this is really good for Google because the more you
[00:09:35.200 --> 00:09:36.960]   use Chrome, the happier they are.
[00:09:36.960 --> 00:09:39.560]   The more you separate yourself from the underlying OS, the better.
[00:09:39.560 --> 00:09:41.560]   As far as they're concerned.
[00:09:41.560 --> 00:09:42.560]   Yeah.
[00:09:42.560 --> 00:09:43.560]   Unless it's Android.
[00:09:43.560 --> 00:09:48.320]   But all the signals that really Google makes money from, of course, is all the data, the
[00:09:48.320 --> 00:09:52.320]   signals that you send them as you use Chrome browsing the web.
[00:09:52.320 --> 00:09:57.120]   This makes Chrome an incredibly valuable product, more so than any browser.
[00:09:57.120 --> 00:09:58.200]   It's already dominant.
[00:09:58.200 --> 00:10:00.760]   It's already beaten IE.
[00:10:00.760 --> 00:10:02.560]   Here's a-- this is interesting.
[00:10:02.560 --> 00:10:05.200]   This is a game, fairly challenging game.
[00:10:05.200 --> 00:10:09.440]   But because you're running on a Chromebook, you have a GPU much better than any Android
[00:10:09.440 --> 00:10:10.360]   phone.
[00:10:10.360 --> 00:10:14.000]   So you can run games.
[00:10:14.000 --> 00:10:16.400]   It's going to be very interesting to see.
[00:10:16.400 --> 00:10:20.480]   The one thing that's hard to get used to, and it's not a problem on the Pixel.
[00:10:20.480 --> 00:10:24.240]   By the way, she's making a purchase with the store.
[00:10:24.240 --> 00:10:25.240]   Wait, store.
[00:10:25.240 --> 00:10:26.240]   Absolutely.
[00:10:26.240 --> 00:10:27.240]   For her wallet.
[00:10:27.240 --> 00:10:28.240]   Everything works.
[00:10:28.240 --> 00:10:29.240]   It all works.
[00:10:29.240 --> 00:10:30.960]   With Pixel, you've got the touch interface right there on the screen.
[00:10:30.960 --> 00:10:32.360]   So that's great.
[00:10:32.360 --> 00:10:36.480]   One problem that I found, because I run Android on the desktop a lot, is that you have to--
[00:10:36.480 --> 00:10:40.320]   if you don't have a touch interface, then you have to translate in your iPhone.
[00:10:40.320 --> 00:10:43.280]   You have to have a touch interface to run your head between mouse touches.
[00:10:43.280 --> 00:10:44.280]   Like, oh, yeah, this isn't a double click.
[00:10:44.280 --> 00:10:45.280]   It's single click.
[00:10:45.280 --> 00:10:47.280]   Things like that, you have to translate in your head.
[00:10:47.280 --> 00:10:50.480]   So depending on where this goes, whether it goes more towards a desktop or whether it'll
[00:10:50.480 --> 00:10:56.160]   be a requirement to have a touch interface to run this, we'll just have to wait and see.
[00:10:56.160 --> 00:11:00.160]   Why did they say why they were restricting it to only some Chromebooks?
[00:11:00.160 --> 00:11:03.280]   Is it a CPU problem, a RAM problem?
[00:11:03.280 --> 00:11:04.280]   I didn't hear.
[00:11:04.280 --> 00:11:05.280]   I didn't hear a word.
[00:11:05.280 --> 00:11:06.280]   Yeah.
[00:11:06.280 --> 00:11:07.280]   Yeah.
[00:11:07.280 --> 00:11:08.280]   All right.
[00:11:08.280 --> 00:11:09.920]   So that's one of the big things announced.
[00:11:09.920 --> 00:11:16.320]   That might have the biggest short-term impact, but they also announced-- before I even talk
[00:11:16.320 --> 00:11:19.840]   about Google Home, the underlying technology, which is--
[00:11:19.840 --> 00:11:20.840]   Oh.
[00:11:20.840 --> 00:11:21.840]   Yes, they say--
[00:11:21.840 --> 00:11:22.840]   Very exciting.
[00:11:22.840 --> 00:11:24.760]   I'm so excited.
[00:11:24.760 --> 00:11:32.000]   The home is just one interface to a new way, kind of ambient search, a new way of thinking
[00:11:32.000 --> 00:11:35.320]   about what Google does.
[00:11:35.320 --> 00:11:37.400]   So tell us about that.
[00:11:37.400 --> 00:11:38.400]   Google Assistant?
[00:11:38.400 --> 00:11:39.400]   Yeah.
[00:11:39.400 --> 00:11:43.560]   That's the technical thing that's going on.
[00:11:43.560 --> 00:11:44.560]   That's what they call it.
[00:11:44.560 --> 00:11:49.720]   So the subtext for the whole keynote, from my perspective, was they're bringing AI into
[00:11:49.720 --> 00:11:52.520]   everything, and they're finally talking about it.
[00:11:52.520 --> 00:11:58.400]   And we could see this from Google now was a wonderful way to start that.
[00:11:58.400 --> 00:12:01.880]   Kevin and I have talked for, it feels like decades, but it's only years about bringing
[00:12:01.880 --> 00:12:04.680]   context when you need it and where you need it.
[00:12:04.680 --> 00:12:05.680]   Kevin Tofel.
[00:12:05.680 --> 00:12:07.480]   But it never quite lived up to--
[00:12:07.480 --> 00:12:08.480]   It did not.
[00:12:08.480 --> 00:12:09.480]   It did not--
[00:12:09.480 --> 00:12:10.640]   It's promised, right?
[00:12:10.640 --> 00:12:14.520]   It was cool, but not like, wow, cool.
[00:12:14.520 --> 00:12:19.680]   And I think what Google had to figure out, and what I'm still really curious about, is
[00:12:19.680 --> 00:12:24.160]   Google saw that you need context to give people information when and where they need
[00:12:24.160 --> 00:12:25.880]   it, because we have no patience.
[00:12:25.880 --> 00:12:29.280]   But they couldn't figure out how to do it in a way that preserved the way they make
[00:12:29.280 --> 00:12:30.280]   money.
[00:12:30.280 --> 00:12:31.280]   Yeah.
[00:12:31.280 --> 00:12:34.240]   And I'm still waiting on this, because if you look at--
[00:12:34.240 --> 00:12:37.800]   You ask Google for a movie tonight.
[00:12:37.800 --> 00:12:44.720]   They're like, oh, I think the demo that Sundar showed was, I think he asked for a movie.
[00:12:44.720 --> 00:12:47.720]   They showed him three options playing near him.
[00:12:47.720 --> 00:12:49.720]   He was like family movie.
[00:12:49.720 --> 00:12:52.800]   Then it brought up the Jungle Book and others, and he's like the Jungle Book.
[00:12:52.800 --> 00:12:55.280]   And then it went to-- was it Fandango and bought--
[00:12:55.280 --> 00:12:56.280]   Yeah.
[00:12:56.280 --> 00:12:57.280]   --forts to the jungle?
[00:12:57.280 --> 00:13:02.160]   I mean, it all happened in the bot format that everyone's so excited about.
[00:13:02.160 --> 00:13:08.240]   But I'm still like, who did the deals on the back end to pay Google for that?
[00:13:08.240 --> 00:13:18.600]   Well, so even more than just bringing machine learning and AI out in just the more foreground,
[00:13:18.600 --> 00:13:25.920]   it also strikes me that what's going on is that-- and we were talking on Windows weekly
[00:13:25.920 --> 00:13:32.080]   earlier that Microsoft's doing this a little bit with Bing-- that Google wants to be your
[00:13:32.080 --> 00:13:33.680]   assistant in general.
[00:13:33.680 --> 00:13:35.440]   Like your intermediary.
[00:13:35.440 --> 00:13:40.120]   So that-- and this was the instant app part of it too, which is the very end of the keynote,
[00:13:40.120 --> 00:13:41.920]   and I think got underplayed.
[00:13:41.920 --> 00:13:46.080]   The idea that right now we're in this weird siloed environment.
[00:13:46.080 --> 00:13:48.640]   You want to add the capability to your phone.
[00:13:48.640 --> 00:13:50.520]   You have to download and install an app.
[00:13:50.520 --> 00:13:53.160]   You have to run the app on the iPhone.
[00:13:53.160 --> 00:13:54.520]   They don't even really talk to each other.
[00:13:54.520 --> 00:13:57.200]   They're somewhat more so on Android, but not a lot.
[00:13:57.200 --> 00:13:59.480]   So there's these kind of siloed experiences.
[00:13:59.480 --> 00:14:04.120]   If I want to book a table, I go to open table phone, book a room, I go to hotels.
[00:14:04.120 --> 00:14:09.320]   You have to-- and so what I got the sense that I thought was really the most powerful
[00:14:09.320 --> 00:14:13.680]   thing, and this is what you're saying with assistant, I think, is that Google is going
[00:14:13.680 --> 00:14:16.200]   to be our intermediary to the rest of the world.
[00:14:16.200 --> 00:14:21.200]   You just ask us, and we'll contact-- I don't know if they have to make a bus deal, because
[00:14:21.200 --> 00:14:23.200]   instant apps means we'll load the app.
[00:14:23.200 --> 00:14:27.480]   The app has to be customized in a modular fashion, but we'll load the part of the app
[00:14:27.480 --> 00:14:29.440]   that we need, and we'll do it through the app.
[00:14:29.440 --> 00:14:31.800]   And I think that this is very interesting.
[00:14:31.800 --> 00:14:35.720]   It means that Google puts itself between me and this is--
[00:14:35.720 --> 00:14:36.720]   You and everything.
[00:14:36.720 --> 00:14:38.280]   You and everything.
[00:14:38.280 --> 00:14:40.800]   But isn't that what we want?
[00:14:40.800 --> 00:14:43.880]   Don't we want a smart assistant that we just talked to and it does what we want?
[00:14:43.880 --> 00:14:46.520]   We don't want to think about how you book a table.
[00:14:46.520 --> 00:14:47.680]   I just want that table.
[00:14:47.680 --> 00:14:50.160]   Book be a table for two tomorrow night.
[00:14:50.160 --> 00:14:51.800]   I don't need to open table.
[00:14:51.800 --> 00:14:53.280]   What is that?
[00:14:53.280 --> 00:14:55.120]   I totally agree with you.
[00:14:55.120 --> 00:15:00.680]   I think the big questions, though, are how is this going to contribute money to Google?
[00:15:00.680 --> 00:15:04.840]   And then the way other companies are doing it, like Facebook's doing it with Facebook
[00:15:04.840 --> 00:15:09.640]   M, we've got to assume that Apple's going to do something.
[00:15:09.640 --> 00:15:15.560]   And then you've got Microsoft, Cortana, that whole play there.
[00:15:15.560 --> 00:15:21.200]   Everyone wants to be your personal assistant, but I don't know if people are ready to give
[00:15:21.200 --> 00:15:25.880]   all of their-- right now it's based on hardware, your OS, your platform you're on.
[00:15:25.880 --> 00:15:30.080]   I don't know if people are ready for kind of giving all of that to one vendor.
[00:15:30.080 --> 00:15:31.520]   Does that make sense?
[00:15:31.520 --> 00:15:35.880]   Well, you mean open table or you mean users?
[00:15:35.880 --> 00:15:36.880]   Users.
[00:15:36.880 --> 00:15:39.440]   I always think about the consumer first or the user first.
[00:15:39.440 --> 00:15:41.160]   I mean, I see how Google makes money, of course.
[00:15:41.160 --> 00:15:45.360]   It's going to take a penny from every transaction and they're perfectly happy.
[00:15:45.360 --> 00:15:46.360]   Wow.
[00:15:46.360 --> 00:15:47.360]   Right.
[00:15:47.360 --> 00:15:50.680]   And even they don't even need to do that because right now they make a ton of money every
[00:15:50.680 --> 00:15:51.680]   time you search.
[00:15:51.680 --> 00:15:54.880]   Every time you-- they put ads up there.
[00:15:54.880 --> 00:15:59.240]   Well, they want to get rid of the big blue-- the list of 10 blue links, right?
[00:15:59.240 --> 00:16:00.240]   That was the--
[00:16:00.240 --> 00:16:02.080]   I think-- yeah.
[00:16:02.080 --> 00:16:04.440]   So you're saying what do we put there instead of ads?
[00:16:04.440 --> 00:16:05.440]   Right.
[00:16:05.440 --> 00:16:07.440]   How do you make 40 or 60 billion?
[00:16:07.440 --> 00:16:08.440]   Right.
[00:16:08.440 --> 00:16:11.760]   So how you monetize is one thing and how much do you trust Google is the other, right?
[00:16:11.760 --> 00:16:16.120]   If they're sitting in between you and what you want to do and Google only contracts with
[00:16:16.120 --> 00:16:21.440]   open table and they don't contract with ABC reservations to make those reservations,
[00:16:21.440 --> 00:16:26.320]   then you may not realize it, but you're being stifled in your ability to choose what you
[00:16:26.320 --> 00:16:27.320]   want to do.
[00:16:27.320 --> 00:16:30.840]   So I think that a lot of it has to do with just how much do I trust Google or Apple or
[00:16:30.840 --> 00:16:34.400]   Microsoft or whoever comes down the pike.
[00:16:34.400 --> 00:16:38.280]   And it's another reason why these things really should be open sourced and Google's
[00:16:38.280 --> 00:16:46.040]   taken some pretty big steps towards that in open sourcing their processor, the TPU, the
[00:16:46.040 --> 00:16:47.040]   Tensor Processing Unit.
[00:16:47.040 --> 00:16:48.040]   Wait, wait, wait.
[00:16:48.040 --> 00:16:50.960]   They did not open source the Tensor Processing Unit.
[00:16:50.960 --> 00:16:52.760]   That's not true.
[00:16:52.760 --> 00:16:56.800]   They will let other people run on it, but they didn't give-- they won't even talk about
[00:16:56.800 --> 00:16:57.800]   how that--
[00:16:57.800 --> 00:16:58.800]   You're kicking the code.
[00:16:58.800 --> 00:16:59.800]   I can't get the block diagrams.
[00:16:59.800 --> 00:17:00.800]   I can't get them.
[00:17:00.800 --> 00:17:01.800]   Yeah.
[00:17:01.800 --> 00:17:02.800]   You can't get into the code.
[00:17:02.800 --> 00:17:03.800]   They did open source TensorFlow.
[00:17:03.800 --> 00:17:04.800]   Oh, they did it.
[00:17:04.800 --> 00:17:05.800]   Yeah.
[00:17:05.800 --> 00:17:06.800]   Yeah.
[00:17:06.800 --> 00:17:07.800]   That's what I was talking about.
[00:17:07.800 --> 00:17:08.800]   Yeah.
[00:17:08.800 --> 00:17:09.800]   Oh, OK.
[00:17:09.800 --> 00:17:10.800]   Yeah.
[00:17:10.800 --> 00:17:11.800]   Not the processor.
[00:17:11.800 --> 00:17:12.800]   Not the processor.
[00:17:12.800 --> 00:17:13.800]   You're right.
[00:17:13.800 --> 00:17:14.800]   It's the TensorFlow software that you can use, but anybody can use it.
[00:17:14.800 --> 00:17:20.560]   It's more than other companies have done in terms of making this available in an open
[00:17:20.560 --> 00:17:21.560]   way.
[00:17:21.560 --> 00:17:26.640]   And so I think that, for my money, out of all the companies, I'd probably trust Google
[00:17:26.640 --> 00:17:30.000]   the most to be that intermediary between me and what I want to do.
[00:17:30.000 --> 00:17:31.000]   But who knows?
[00:17:31.000 --> 00:17:32.560]   It only takes one bad move on their part.
[00:17:32.560 --> 00:17:33.720]   You raised an interesting point.
[00:17:33.720 --> 00:17:37.760]   They will become a big player in a decider of who's successful.
[00:17:37.760 --> 00:17:42.040]   You asked for a movie, and they decide to put it on voodoo and on Netflix.
[00:17:42.040 --> 00:17:43.200]   That's to Netflix detriment.
[00:17:43.200 --> 00:17:46.960]   So it almost becomes a net neutrality issue, doesn't it?
[00:17:46.960 --> 00:17:49.440]   Because they're no longer an agnostic provider.
[00:17:49.440 --> 00:17:50.440]   Yeah.
[00:17:50.440 --> 00:17:51.760]   They're like, oh, we provide movies.
[00:17:51.760 --> 00:17:55.440]   So here's what they want from us.
[00:17:55.440 --> 00:17:57.000]   They offer you a bunch of choices.
[00:17:57.000 --> 00:18:00.040]   I mean, when I look for a movie today, I can see it on a bunch of things.
[00:18:00.040 --> 00:18:05.120]   And I don't think Google will get rid of that because it would put it in a position.
[00:18:05.120 --> 00:18:06.840]   I think it's probably smart enough.
[00:18:06.840 --> 00:18:08.480]   It knows it doesn't want to be there.
[00:18:08.480 --> 00:18:13.160]   That's a challenge, though, because for-- so I want to say, book me a table.
[00:18:13.160 --> 00:18:17.720]   At my favorite restaurant, I don't want to say, please use open table to book me a table at my--
[00:18:17.720 --> 00:18:22.080]   And that's one of the things wrong, by the way, with the Alexa is the skills are very specific.
[00:18:22.080 --> 00:18:24.280]   And so you have to learn a new syntax.
[00:18:24.280 --> 00:18:30.120]   So I don't want it to be-- I mean, as a user, don't I want to just say, book me a table or show me a movie?
[00:18:30.120 --> 00:18:32.320]   What if Google-- it knows what's on your phone.
[00:18:32.320 --> 00:18:37.440]   If you've got an Android phone or it knows your apps, you could also-- it could say, oh, she's got open table.
[00:18:37.440 --> 00:18:38.600]   She's got that app on her phone.
[00:18:38.600 --> 00:18:41.360]   Or her emails are all from open table.
[00:18:41.360 --> 00:18:42.360]   Let's pull that in.
[00:18:42.360 --> 00:18:43.360]   That would be great.
[00:18:43.360 --> 00:18:45.560]   That becomes a default reference.
[00:18:45.560 --> 00:18:46.560]   Yeah.
[00:18:46.560 --> 00:18:50.080]   So, I mean, it has enough knowledge that it could do that.
[00:18:50.080 --> 00:18:51.160]   Did we lose Matthew?
[00:18:51.160 --> 00:18:52.960]   I see you've pulled his monitor out.
[00:18:52.960 --> 00:18:54.680]   He's-- he had a power outage.
[00:18:54.680 --> 00:18:56.280]   He had a power outage.
[00:18:56.280 --> 00:18:58.160]   Oh, Matthew, I'm sorry.
[00:18:58.160 --> 00:19:01.120]   Yeah, you know, there was a big outage in Seattle on the other side of the country.
[00:19:01.120 --> 00:19:02.120]   OK.
[00:19:02.120 --> 00:19:02.640]   Good.
[00:19:02.640 --> 00:19:03.360]   Well, sorry.
[00:19:03.360 --> 00:19:06.160]   I was looking forward to him explaining Twitter's changes to me.
[00:19:06.160 --> 00:19:07.560]   I know.
[00:19:07.560 --> 00:19:10.960]   Well, we'll agree that if the power comes back, we'll get him back on.
[00:19:10.960 --> 00:19:14.360]   Not much we can do about a power outage, unfortunately.
[00:19:14.360 --> 00:19:15.560]   OK, good.
[00:19:15.560 --> 00:19:18.360]   Well, I'm so glad, though, that we got Stacey and Aaron.
[00:19:18.360 --> 00:19:20.360]   So I think, frankly, it's--
[00:19:20.360 --> 00:19:21.360]   It's cozy.
[00:19:21.360 --> 00:19:22.360]   We'll go survive.
[00:19:22.360 --> 00:19:22.360]   Carry on.
[00:19:22.360 --> 00:19:23.360]   We'll survive.
[00:19:23.360 --> 00:19:28.560]   OK, let's talk about the hardware than the Google Home hardware, which looks, of course,
[00:19:28.560 --> 00:19:32.760]   like a Me Too product compared to Amazon's Echo.
[00:19:32.760 --> 00:19:33.760]   Is it Stacey?
[00:19:33.760 --> 00:19:35.760]   I mean, this is the thing you cover most.
[00:19:35.760 --> 00:19:39.760]   This is-- this was the thing I was most excited about.
[00:19:39.760 --> 00:19:44.040]   Well, first of all, we don't know what it's going to be real.
[00:19:44.040 --> 00:19:46.040]   So they haven't even announced availability.
[00:19:46.040 --> 00:19:47.040]   So I want to get there.
[00:19:47.040 --> 00:19:50.560]   The rumors I've heard are that it may be sooner than we think that they're already in manufacture,
[00:19:50.560 --> 00:19:51.560]   right?
[00:19:51.560 --> 00:19:54.960]   I think they-- I can tell you that they've got beta units.
[00:19:54.960 --> 00:19:55.960]   Right.
[00:19:55.960 --> 00:20:00.560]   But again, so we're basing all this on the video and what we saw.
[00:20:00.560 --> 00:20:01.560]   Right.
[00:20:01.560 --> 00:20:04.400]   Like, we don't even know what the trigger phrase is.
[00:20:04.400 --> 00:20:07.080]   We saw one-- at one point, somebody said, "Hey, Google," and it worked.
[00:20:07.080 --> 00:20:10.720]   I think it was, "Hey, Google," which makes sense because they wouldn't want to activate
[00:20:10.720 --> 00:20:11.720]   your phone.
[00:20:11.720 --> 00:20:12.720]   Right.
[00:20:12.720 --> 00:20:13.720]   So you wouldn't--
[00:20:13.720 --> 00:20:17.120]   It's a problem now with the mode with the Android Wear is that I say, "OK, you know who
[00:20:17.120 --> 00:20:19.720]   ended both my phone and my watch wake up."
[00:20:19.720 --> 00:20:20.720]   Exactly.
[00:20:20.720 --> 00:20:22.320]   So imagine you have that third device.
[00:20:22.320 --> 00:20:27.440]   So the two things that are, like, different is, well, I guess there's three.
[00:20:27.440 --> 00:20:29.640]   One, it looks different, and you can choose your design, which--
[00:20:29.640 --> 00:20:30.640]   It's pretty.
[00:20:30.640 --> 00:20:31.640]   It's much prettier.
[00:20:31.640 --> 00:20:32.640]   Yeah.
[00:20:32.640 --> 00:20:33.640]   Hi.
[00:20:33.640 --> 00:20:34.640]   I guess it's prettier.
[00:20:34.640 --> 00:20:35.640]   I have my house--
[00:20:35.640 --> 00:20:36.640]   You don't think so.
[00:20:36.640 --> 00:20:40.920]   So many of these, like, white, funky-looking, glowing LED devices, it's ridiculous.
[00:20:40.920 --> 00:20:44.000]   But it's better looking than the Echo, isn't it?
[00:20:44.000 --> 00:20:45.000]   Yes, it is.
[00:20:45.000 --> 00:20:47.760]   Echo is kind of almost 2001 model with scary.
[00:20:47.760 --> 00:20:48.760]   Yeah.
[00:20:48.760 --> 00:20:49.760]   It's because it's black.
[00:20:49.760 --> 00:20:50.760]   Right.
[00:20:50.760 --> 00:20:51.760]   It definitely looks like an issue.
[00:20:51.760 --> 00:20:52.760]   It's really--
[00:20:52.760 --> 00:20:53.760]   Right.
[00:20:53.760 --> 00:20:54.760]   Yeah.
[00:20:54.760 --> 00:21:00.120]   So the two functionalities that are cool is it's using Chromecast, that capability.
[00:21:00.120 --> 00:21:02.760]   So you can multicast, which is like what you can do on the server.
[00:21:02.760 --> 00:21:03.760]   Yeah.
[00:21:03.760 --> 00:21:04.760]   Yeah.
[00:21:04.760 --> 00:21:05.760]   The Echo doesn't do that.
[00:21:05.760 --> 00:21:11.640]   The other thing in the video is showed a tie-in to the contextual information we were talking
[00:21:11.640 --> 00:21:12.640]   about.
[00:21:12.640 --> 00:21:19.280]   And the woman in the video said, "Hey, Google, I'm listening," impromptu with a beep.
[00:21:19.280 --> 00:21:22.360]   And Google was like, "Yo, I think your flight was delayed.
[00:21:22.360 --> 00:21:23.440]   Your flight's going to be delayed."
[00:21:23.440 --> 00:21:24.440]   But imagine--
[00:21:24.440 --> 00:21:25.440]   I love notifications.
[00:21:25.440 --> 00:21:26.440]   Yes.
[00:21:26.440 --> 00:21:27.440]   Yes.
[00:21:27.440 --> 00:21:29.280]   Although you can see how people would totally abuse that, right?
[00:21:29.280 --> 00:21:30.280]   Yeah.
[00:21:30.280 --> 00:21:31.280]   You can be beep-enched all the time.
[00:21:31.280 --> 00:21:33.120]   But it'd be great.
[00:21:33.120 --> 00:21:38.000]   Like, "Hey, leave five minutes earlier for your appointment, so you're not late."
[00:21:38.000 --> 00:21:39.000]   Thanks.
[00:21:39.000 --> 00:21:43.920]   Well, we've raised in both these past two topics this whole issue of the user has to
[00:21:43.920 --> 00:21:45.840]   have control.
[00:21:45.840 --> 00:21:48.680]   This is the problem because you want to make it very simple.
[00:21:48.680 --> 00:21:51.320]   You don't want to have a long list of checkboxes.
[00:21:51.320 --> 00:21:54.880]   Like, "I want notifications from this app but not ad app."
[00:21:54.880 --> 00:21:57.840]   But ultimately, you're going to have to do something like that.
[00:21:57.840 --> 00:21:59.320]   You know, it may just be--
[00:21:59.320 --> 00:22:00.480]   Voice might be the boon here.
[00:22:00.480 --> 00:22:03.440]   It may be just, "Hey, I have a notification from Tripit.
[00:22:03.440 --> 00:22:06.320]   Do you want to hear these in future?"
[00:22:06.320 --> 00:22:08.000]   Something like that, right?
[00:22:08.000 --> 00:22:09.360]   Or maybe you just, for now--
[00:22:09.360 --> 00:22:13.920]   I mean, in Google now, it pops up the card and you're like, "Do you care about this?"
[00:22:13.920 --> 00:22:15.120]   It does ask you, right?
[00:22:15.120 --> 00:22:16.120]   Yeah.
[00:22:16.120 --> 00:22:17.120]   Oh.
[00:22:17.120 --> 00:22:18.120]   Right.
[00:22:18.120 --> 00:22:20.480]   So maybe that's the sort of thing we'll get initially.
[00:22:20.480 --> 00:22:24.440]   And I think context will matter a lot.
[00:22:24.440 --> 00:22:28.600]   And if you have this in your home, you're going to be giving Google--
[00:22:28.600 --> 00:22:30.440]   It's kind of like, think about a triangulation, right?
[00:22:30.440 --> 00:22:34.680]   But say you have this, you have an on hub, you have your phone that runs on Android.
[00:22:34.680 --> 00:22:37.960]   They can pinpoint where you are really accurately and start giving you.
[00:22:37.960 --> 00:22:39.360]   And maybe even who else is with you.
[00:22:39.360 --> 00:22:40.360]   I mean, it's creepy.
[00:22:40.360 --> 00:22:41.360]   I know.
[00:22:41.360 --> 00:22:43.360]   I love this.
[00:22:43.360 --> 00:22:45.120]   Google knows all this stuff already.
[00:22:45.120 --> 00:22:46.120]   Right.
[00:22:46.120 --> 00:22:51.880]   So if they surface that information to give you better products, that's kind of really compelling.
[00:22:51.880 --> 00:22:52.880]   I agree.
[00:22:52.880 --> 00:22:54.360]   I don't know if this is the size.
[00:22:54.360 --> 00:22:55.560]   It feels like it's roughly the size.
[00:22:55.560 --> 00:22:56.800]   It's definitely the inspiration.
[00:22:56.800 --> 00:23:00.480]   This is a Glade Air freshener.
[00:23:00.480 --> 00:23:03.600]   If I take the label off, I bet I could have fooled you.
[00:23:03.600 --> 00:23:05.280]   It said, oh, you know what, Stacey?
[00:23:05.280 --> 00:23:06.960]   You mentioned they're sending these out for beta.
[00:23:06.960 --> 00:23:09.080]   I did get mine the other day.
[00:23:09.080 --> 00:23:10.080]   What do you think?
[00:23:10.080 --> 00:23:11.080]   I'd be so jealous.
[00:23:11.080 --> 00:23:13.680]   I'd be like, dang it.
[00:23:13.680 --> 00:23:16.400]   And it smells so good.
[00:23:16.400 --> 00:23:20.840]   I was just about to say that the thing looks like an aromatherapy product in the pictures
[00:23:20.840 --> 00:23:21.840]   that they have.
[00:23:21.840 --> 00:23:22.840]   Ooh, smells wonderful.
[00:23:22.840 --> 00:23:26.040]   I love the smell of Google in the morning.
[00:23:26.040 --> 00:23:29.600]   I ordered and then supposed to, I think it's going to come in the next six months.
[00:23:29.600 --> 00:23:30.880]   Something called a G-Bode.
[00:23:30.880 --> 00:23:31.880]   You guys know about this.
[00:23:31.880 --> 00:23:32.880]   This is Cynthia Bishier.
[00:23:32.880 --> 00:23:33.880]   I mean, it goes off.
[00:23:33.880 --> 00:23:34.880]   Yeah.
[00:23:34.880 --> 00:23:38.240]   Cynthia is a robotics guru and has been for some time.
[00:23:38.240 --> 00:23:44.480]   And she announced this and had a crowdfunding drive for it, which I contributed to.
[00:23:44.480 --> 00:23:45.880]   I think it's been two years.
[00:23:45.880 --> 00:23:49.080]   And it's very similar to all these other products.
[00:23:49.080 --> 00:23:52.120]   The only difference is it has a screen on it.
[00:23:52.120 --> 00:23:53.120]   What?
[00:23:53.120 --> 00:23:54.920]   So it can look at you.
[00:23:54.920 --> 00:23:59.480]   Here's your show at Carsten because you're missing the G-Bode in action.
[00:23:59.480 --> 00:24:00.480]   Wow.
[00:24:00.480 --> 00:24:01.480]   So and it could follow you around.
[00:24:01.480 --> 00:24:03.840]   You can ask it to show stuff.
[00:24:03.840 --> 00:24:06.360]   The problem is this is great.
[00:24:06.360 --> 00:24:08.280]   And it really should be tied into Google.
[00:24:08.280 --> 00:24:09.280]   Yeah.
[00:24:09.280 --> 00:24:10.280]   Right?
[00:24:10.280 --> 00:24:12.520]   Don't you want the, it's got a camera.
[00:24:12.520 --> 00:24:14.880]   Don't you want the Google intelligence in it?
[00:24:14.880 --> 00:24:15.880]   Right?
[00:24:15.880 --> 00:24:17.680]   So that would be cool except for the camera.
[00:24:17.680 --> 00:24:19.800]   I mean, cameras freak people out.
[00:24:19.800 --> 00:24:20.800]   Look at this thing.
[00:24:20.800 --> 00:24:21.800]   Oh, I don't know.
[00:24:21.800 --> 00:24:22.800]   Really?
[00:24:22.800 --> 00:24:23.800]   That's scary.
[00:24:23.800 --> 00:24:24.800]   Yes.
[00:24:24.800 --> 00:24:28.840]   What you do in your home, I mean, I have an Alexa in my bedroom, for example.
[00:24:28.840 --> 00:24:29.840]   Me too.
[00:24:29.840 --> 00:24:30.840]   I know.
[00:24:30.840 --> 00:24:32.760]   But too, I don't care.
[00:24:32.760 --> 00:24:33.760]   Oh.
[00:24:33.760 --> 00:24:34.760]   I know.
[00:24:34.760 --> 00:24:37.360]   I've seen you jump on the mattress.
[00:24:37.360 --> 00:24:41.840]   Yeah, you've seen a lot more of me than a, you know, so, you know, this is a little creepy,
[00:24:41.840 --> 00:24:48.640]   a girl in the tent talking to her little pet robot and it's reading to her.
[00:24:48.640 --> 00:24:49.880]   This Cynthia is very good.
[00:24:49.880 --> 00:24:52.160]   She makes robots that are very appealing.
[00:24:52.160 --> 00:24:54.200]   Oh, my daughter would love this.
[00:24:54.200 --> 00:24:55.800]   Yeah, I think this is interesting.
[00:24:55.800 --> 00:25:00.920]   Anyway, I'll get one about the same time probably as I'll get a Google home.
[00:25:00.920 --> 00:25:01.920]   You're right.
[00:25:01.920 --> 00:25:02.920]   The camera is an interesting slice.
[00:25:02.920 --> 00:25:04.360]   It's very different.
[00:25:04.360 --> 00:25:05.520]   You could use it by the way.
[00:25:05.520 --> 00:25:07.960]   You notice they're using it like Skype.
[00:25:07.960 --> 00:25:10.800]   So you can, you, it could be a telepresence unit.
[00:25:10.800 --> 00:25:12.360]   Dad is on a business trip.
[00:25:12.360 --> 00:25:17.160]   He's watching the family pizza and they can see him while he sees them.
[00:25:17.160 --> 00:25:19.720]   It becomes his little telepresence unit.
[00:25:19.720 --> 00:25:20.720]   There.
[00:25:20.720 --> 00:25:22.720]   I don't know.
[00:25:22.720 --> 00:25:24.160]   Is this a true cre- is this cross the creepy one?
[00:25:24.160 --> 00:25:27.040]   To me, it's just a little creepy.
[00:25:27.040 --> 00:25:28.560]   I don't have cameras in my house though.
[00:25:28.560 --> 00:25:32.760]   So I test them out and then I take them, I ship them back out.
[00:25:32.760 --> 00:25:33.760]   This is what happens though.
[00:25:33.760 --> 00:25:37.560]   This is the interesting thing we talk about this a lot is that there are a large, large
[00:25:37.560 --> 00:25:43.800]   portion of people really find this disturbing and would like to shut this down.
[00:25:43.800 --> 00:25:45.000]   And that's my problem.
[00:25:45.000 --> 00:25:48.760]   I want, I want to be able to choose this as an opportunity for me.
[00:25:48.760 --> 00:25:49.960]   I don't have any problem.
[00:25:49.960 --> 00:25:52.160]   If Stacy, you don't want to have a camera in your house.
[00:25:52.160 --> 00:25:58.960]   But I hope that people's privacy concerns won't put hobble this kind of stuff.
[00:25:58.960 --> 00:26:01.760]   Because I think ultimately this could be really great.
[00:26:01.760 --> 00:26:08.080]   I think it's be, I mean, imagine if you're disabled or like a shut-in, shut-ins probably
[00:26:08.080 --> 00:26:10.880]   the wrong, like elderly and your family can't visit you.
[00:26:10.880 --> 00:26:15.760]   Something like this would be amazing because it acts as like a companion, just like it
[00:26:15.760 --> 00:26:16.760]   did for that kid.
[00:26:16.760 --> 00:26:18.000]   But- Wait a minute.
[00:26:18.000 --> 00:26:19.080]   You haven't seen the best of it.
[00:26:19.080 --> 00:26:20.320]   Show this part.
[00:26:20.320 --> 00:26:26.320]   So sometimes what happens is dad can't be home and he wants to say good night to this
[00:26:26.320 --> 00:26:27.320]   little girl.
[00:26:27.320 --> 00:26:29.480]   So the G-Bow will do it for you.
[00:26:29.480 --> 00:26:30.480]   Good night.
[00:26:30.480 --> 00:26:31.480]   Oh, see that?
[00:26:31.480 --> 00:26:32.480]   Enough you.
[00:26:32.480 --> 00:26:33.480]   That's just wrong.
[00:26:33.480 --> 00:26:35.480]   Yeah, that part's a little creepy.
[00:26:35.480 --> 00:26:36.480]   I can't wait.
[00:26:36.480 --> 00:26:37.480]   Look.
[00:26:37.480 --> 00:26:38.480]   Do you not know?
[00:26:38.480 --> 00:26:43.600]   No, you're not getting audio from me, Carsten.
[00:26:43.600 --> 00:26:44.600]   You are.
[00:26:44.600 --> 00:26:45.600]   Okay.
[00:26:45.600 --> 00:26:48.000]   So I'm just curious what the G-Bow said.
[00:26:48.000 --> 00:26:52.000]   I would love you forever.
[00:26:52.000 --> 00:26:55.000]   Thank you all.
[00:26:55.000 --> 00:26:58.400]   Oh, G-Bow.
[00:26:58.400 --> 00:26:59.400]   I would love you forever.
[00:26:59.400 --> 00:27:00.400]   And then it winks.
[00:27:00.400 --> 00:27:01.400]   What's up with that?
[00:27:01.400 --> 00:27:04.360]   It's- Well, that's what Rishirz does.
[00:27:04.360 --> 00:27:06.120]   I was excited about this for a minute.
[00:27:06.120 --> 00:27:07.120]   Now it's- Now it's-
[00:27:07.120 --> 00:27:08.120]   She's-
[00:27:08.120 --> 00:27:15.360]   And I think a lot of roboticists are experimenting with the idea of making this more companion
[00:27:15.360 --> 00:27:16.360]   line, right?
[00:27:16.360 --> 00:27:17.360]   Right.
[00:27:17.360 --> 00:27:18.360]   This could be a challenge.
[00:27:18.360 --> 00:27:19.360]   Right.
[00:27:19.360 --> 00:27:20.360]   This could be a challenge.
[00:27:20.360 --> 00:27:22.560]   I mean, I- Oh, go on.
[00:27:22.560 --> 00:27:23.560]   No, no, don't.
[00:27:23.560 --> 00:27:24.560]   You're too polite.
[00:27:24.560 --> 00:27:26.560]   You're going to learn not to be so polite, Stacey.
[00:27:26.560 --> 00:27:28.400]   It may be.
[00:27:28.400 --> 00:27:33.400]   So I love the fact that the echo, you know, orients itself to whomever is speaking to
[00:27:33.400 --> 00:27:34.400]   it.
[00:27:34.400 --> 00:27:35.880]   I think that's a really nice, friendly thing.
[00:27:35.880 --> 00:27:39.800]   But when you look at like the teddy bears that will like look at the person as a way
[00:27:39.800 --> 00:27:40.800]   to indicate that they're-
[00:27:40.800 --> 00:27:41.800]   I've been screaming.
[00:27:41.800 --> 00:27:42.800]   Those things are-
[00:27:42.800 --> 00:27:43.800]   Yeah.
[00:27:43.800 --> 00:27:49.600]   So I think we've got a long way to go and that kind of crossed over a line for me.
[00:27:49.600 --> 00:27:51.040]   Especially interacts with children.
[00:27:51.040 --> 00:27:55.600]   I met Cynthia at a food camp and she brought a small, you know, two or three foot tall
[00:27:55.600 --> 00:28:01.000]   robot that was humanoid but not human that walked around.
[00:28:01.000 --> 00:28:08.080]   And I watched her have the robot go up to a kid, roughly the same size, and watch the
[00:28:08.080 --> 00:28:09.080]   kid interact.
[00:28:09.080 --> 00:28:11.000]   The kid was not at all disturbed by it.
[00:28:11.000 --> 00:28:12.840]   Oh, was it the now robot?
[00:28:12.840 --> 00:28:13.840]   I think it's any-
[00:28:13.840 --> 00:28:14.840]   Probably.
[00:28:14.840 --> 00:28:15.840]   Probably.
[00:28:15.840 --> 00:28:16.840]   That thing is adorable.
[00:28:16.840 --> 00:28:17.840]   It was like now.
[00:28:17.840 --> 00:28:18.840]   It looks just like now, yeah.
[00:28:18.840 --> 00:28:19.840]   Yeah.
[00:28:19.840 --> 00:28:22.440]   My daughter saving up the $8,000 it takes to buy that thing.
[00:28:22.440 --> 00:28:27.800]   So maybe it's just us, maybe- it's by that it's Brazil, not for sheers.
[00:28:27.800 --> 00:28:28.880]   I keep saying her name wrong.
[00:28:28.880 --> 00:28:33.600]   She's from MIT Media Lab and there's a great TED talk.
[00:28:33.600 --> 00:28:40.160]   She's really a fairly prominent roboticist, professor of media arts and sciences at MIT
[00:28:40.160 --> 00:28:42.280]   Media Lab.
[00:28:42.280 --> 00:28:44.960]   And the creator of this Gebo.
[00:28:44.960 --> 00:28:47.080]   I mean, I'm glad they're experimenting, right?
[00:28:47.080 --> 00:28:48.960]   They're seeing, well, what can people tolerate?
[00:28:48.960 --> 00:28:50.080]   What is the creepy line?
[00:28:50.080 --> 00:28:51.080]   Where is it?
[00:28:51.080 --> 00:28:52.080]   But I think it's interesting.
[00:28:52.080 --> 00:28:55.200]   It's telling, in fact, that it's not something in our genetics.
[00:28:55.200 --> 00:28:58.720]   I think a kid looking at Polar Express was creeped out.
[00:28:58.720 --> 00:29:01.400]   Were your kids creeped out by Polar Express?
[00:29:01.400 --> 00:29:02.400]   She never saw it because-
[00:29:02.400 --> 00:29:03.400]   It was creepy.
[00:29:03.400 --> 00:29:04.400]   It was eyes.
[00:29:04.400 --> 00:29:05.400]   Yeah.
[00:29:05.400 --> 00:29:06.400]   That would haunt her nightmares.
[00:29:06.400 --> 00:29:08.400]   That was the uncanny line, Valley, right?
[00:29:08.400 --> 00:29:12.600]   But maybe kids aren't creeped out by semi-humanoid robots like now.
[00:29:12.600 --> 00:29:13.600]   Yeah.
[00:29:13.600 --> 00:29:17.760]   And they didn't grow up watching all the '80s sci-fi Terminator stuff like we did too.
[00:29:17.760 --> 00:29:19.520]   We've been propagating that.
[00:29:19.520 --> 00:29:25.320]   We've been programmed to be fearful of all of this stuff, I think, a little bit over time.
[00:29:25.320 --> 00:29:26.640]   Yeah.
[00:29:26.640 --> 00:29:31.600]   I'd rather, frankly, have a robot that comes from MIT Media Lab than now that comes from
[00:29:31.600 --> 00:29:36.440]   SoftBank, a giant Japanese multinational.
[00:29:36.440 --> 00:29:39.240]   It just makes me more nervous to have a big corporation behind this.
[00:29:39.240 --> 00:29:40.720]   And maybe that's the solution.
[00:29:40.720 --> 00:29:42.680]   Have you seen it dance to Macarena?
[00:29:42.680 --> 00:29:44.400]   Because it's adorable.
[00:29:44.400 --> 00:29:45.400]   Aww.
[00:29:45.400 --> 00:29:46.800]   Yeah, there's the now.
[00:29:46.800 --> 00:29:51.400]   We had someone come and give us a demonstration of this at our makerspace.
[00:29:51.400 --> 00:29:52.400]   It was pretty cool.
[00:29:52.400 --> 00:29:54.240]   It was fun to interact with.
[00:29:54.240 --> 00:29:57.080]   When it didn't work, it was a little weird because it kept asking questions.
[00:29:57.080 --> 00:29:58.080]   What do you want to do?
[00:29:58.080 --> 00:30:00.000]   What do you want to do?
[00:30:00.000 --> 00:30:01.440]   But when it was working, it was pretty cool.
[00:30:01.440 --> 00:30:02.760]   And the kids loved it.
[00:30:02.760 --> 00:30:08.800]   They talk about now with autistic children and how autistic children can really relate
[00:30:08.800 --> 00:30:15.400]   in a way that's much more comfortable than relating to humans, for instance.
[00:30:15.400 --> 00:30:16.960]   It is expensive, though.
[00:30:16.960 --> 00:30:23.560]   Yeah, it's not, you know, it's, I'm sure, you know what the Geebo was expensive, but not
[00:30:23.560 --> 00:30:24.560]   prohibitively.
[00:30:24.560 --> 00:30:25.560]   Is it?
[00:30:25.560 --> 00:30:26.560]   I think it was 500.
[00:30:26.560 --> 00:30:27.560]   Oh, I know.
[00:30:27.560 --> 00:30:28.560]   It was 2000.
[00:30:28.560 --> 00:30:29.560]   Oh, I know.
[00:30:29.560 --> 00:30:30.560]   I might be now.
[00:30:30.560 --> 00:30:31.560]   I'm trying to remember.
[00:30:31.560 --> 00:30:37.760]   I feel like if I'm going to invest in a true robot, I want it to have arms so it can actually
[00:30:37.760 --> 00:30:42.840]   do something for me, like full clothes or empty the dishwasher, those kind of like
[00:30:42.840 --> 00:30:45.320]   menial tasks that I'm not into.
[00:30:45.320 --> 00:30:47.600]   And I haven't seen that yet.
[00:30:47.600 --> 00:30:48.600]   Yeah.
[00:30:48.600 --> 00:30:49.960]   Everything else I can do with my various kinds.
[00:30:49.960 --> 00:30:54.160]   Well, that's when you knew two legs and arms, right?
[00:30:54.160 --> 00:31:00.200]   They raised over $2 million, $4.99 in the Indiegogo campaign, the develop edition with
[00:31:00.200 --> 00:31:02.320]   an SDK is $5.99.
[00:31:02.320 --> 00:31:04.240]   But I think that sounds like it might be more open.
[00:31:04.240 --> 00:31:06.000]   Yeah, if it has an SDK.
[00:31:06.000 --> 00:31:07.000]   Yeah.
[00:31:07.000 --> 00:31:08.680]   Should maybe that's what Google should do.
[00:31:08.680 --> 00:31:09.920]   Then it could do Google lookups.
[00:31:09.920 --> 00:31:12.840]   If Google, yeah, Google home had an SDK.
[00:31:12.840 --> 00:31:13.840]   Yes, it does.
[00:31:13.840 --> 00:31:14.840]   It needs an API or something.
[00:31:14.840 --> 00:31:15.840]   Needs an API.
[00:31:15.840 --> 00:31:17.360]   So that you can, because I'd love to use it.
[00:31:17.360 --> 00:31:22.320]   I've got a camera monitoring my, I think we're going to talk about something like this
[00:31:22.320 --> 00:31:26.440]   in a minute, but I have a camera monitoring my front entryway for when people come up.
[00:31:26.440 --> 00:31:29.520]   So when the UPS guy comes, takes a picture and then it sends me a notification.
[00:31:29.520 --> 00:31:33.760]   Nice guy was here, but it would be cool to interface that with either Google Home or
[00:31:33.760 --> 00:31:35.080]   some other product.
[00:31:35.080 --> 00:31:39.400]   I'd love to have that be the way that I get my notifications or have that be a consistent
[00:31:39.400 --> 00:31:40.400]   way to get notifications.
[00:31:40.400 --> 00:31:42.680]   Because right now it's like a bunch of different apps.
[00:31:42.680 --> 00:31:43.920]   There's like five or six different apps.
[00:31:43.920 --> 00:31:45.200]   You can use to do this.
[00:31:45.200 --> 00:31:47.200]   None of them integrate exactly the same way every time.
[00:31:47.200 --> 00:31:49.800]   So it'd be nice if there was a consistent way and say, look, put this product in your
[00:31:49.800 --> 00:31:54.240]   home, Google Home, and then when you can send notifications to it from other things, and
[00:31:54.240 --> 00:31:56.480]   then it would just automatically do whatever your preferences were.
[00:31:56.480 --> 00:31:57.480]   Send me an email.
[00:31:57.480 --> 00:31:59.280]   Send me a, what text, whatever.
[00:31:59.280 --> 00:32:04.760]   Google has Brillo and Weave, and I was really disappointed they didn't talk about that.
[00:32:04.760 --> 00:32:09.800]   But their version of Weave, not Nes version of Weave, is going to be a device to device
[00:32:09.800 --> 00:32:11.280]   communications protocol.
[00:32:11.280 --> 00:32:17.600]   So it's possible that if you had devices running Weave, that could happen.
[00:32:17.600 --> 00:32:21.880]   And it could become, I mean, that's kind of everyone's hope is that unified, the grand
[00:32:21.880 --> 00:32:23.920]   unification theory for Smart Home.
[00:32:23.920 --> 00:32:24.920]   I do.
[00:32:24.920 --> 00:32:26.920]   I'm going to go with you now though on the open thing.
[00:32:26.920 --> 00:32:30.800]   We talked about this when you're on the new screensaver, open platforms for home automation.
[00:32:30.800 --> 00:32:33.440]   It's the only hope.
[00:32:33.440 --> 00:32:38.160]   For security reasons, for interoperability reasons, for privacy reasons, you just got
[00:32:38.160 --> 00:32:39.160]   to be open.
[00:32:39.160 --> 00:32:43.240]   And also think about all the applications and think about all the applications we can't
[00:32:43.240 --> 00:32:44.240]   think about.
[00:32:44.240 --> 00:32:46.800]   I know that's a strange way to put it, but there's a lot of other applications that we're
[00:32:46.800 --> 00:32:47.800]   not talking about.
[00:32:47.800 --> 00:32:52.520]   But somebody may want to develop an application for home automation that nobody else is working
[00:32:52.520 --> 00:32:53.520]   on.
[00:32:53.520 --> 00:32:56.280]   And then of course, then you can go out and write that thing.
[00:32:56.280 --> 00:33:00.080]   And then maybe 10 or 20 or 100 other people want to use that too.
[00:33:00.080 --> 00:33:04.680]   And so to meet in my mind, it's the only way to go right now to develop home automation
[00:33:04.680 --> 00:33:10.760]   is to have an open platform, open source software that can be developed by anybody.
[00:33:10.760 --> 00:33:11.760]   I'm with you.
[00:33:11.760 --> 00:33:13.160]   So I disagreeing with you.
[00:33:13.160 --> 00:33:14.520]   Oh, oh, shock.
[00:33:14.520 --> 00:33:21.440]   Oh, and it's because I believe this, I really do agree with you in philosophy and spirit.
[00:33:21.440 --> 00:33:28.480]   But having, and that's why I actually loved smart things forever, but it's too hard for
[00:33:28.480 --> 00:33:29.480]   users.
[00:33:29.480 --> 00:33:34.920]   These long tail, awesome use cases that everyone loves to think about in their nerd world, they're
[00:33:34.920 --> 00:33:37.040]   not what people want when they buy the smart home.
[00:33:37.040 --> 00:33:40.120]   And they don't want to spend a lot of time tweaking it and dealing with it.
[00:33:40.120 --> 00:33:44.880]   So something like Nest that works with Nest that I already put everything together for
[00:33:44.880 --> 00:33:48.880]   you is probably how it's going to end up.
[00:33:48.880 --> 00:33:50.640]   And I don't like it anymore than you guys do.
[00:33:50.640 --> 00:33:51.640]   Right.
[00:33:51.640 --> 00:33:52.640]   No, that's an excellent point.
[00:33:52.640 --> 00:33:53.640]   Those two things can coexist though.
[00:33:53.640 --> 00:33:54.960]   Those two things can absolutely coexist.
[00:33:54.960 --> 00:33:55.960]   I mean, think about it.
[00:33:55.960 --> 00:33:56.960]   You don't go out.
[00:33:56.960 --> 00:33:57.960]   You don't pick up your Android phone.
[00:33:57.960 --> 00:34:00.800]   And every time you pick it up, tweak the Linux kernel, right?
[00:34:00.800 --> 00:34:03.640]   But the Linux kernel enables Android and it's open source.
[00:34:03.640 --> 00:34:04.880]   You don't have to worry about that.
[00:34:04.880 --> 00:34:06.400]   It took 10 years, but it took 10 years.
[00:34:06.400 --> 00:34:09.320]   But I mean, you can write things around that and sell products.
[00:34:09.320 --> 00:34:12.680]   It doesn't always have to be the geeky, hard way to do things.
[00:34:12.680 --> 00:34:17.920]   Google could totally do Nest and base it on an open source platform and have it work and
[00:34:17.920 --> 00:34:21.560]   have it be that platform that you're talking about where someone just puts it on the wall
[00:34:21.560 --> 00:34:23.840]   and plugs the power in and they don't do anything else.
[00:34:23.840 --> 00:34:25.360]   It totally could be that.
[00:34:25.360 --> 00:34:28.880]   But if it's open source, then you can also go out and build on top of that.
[00:34:28.880 --> 00:34:31.520]   And so I think both are possible with open source.
[00:34:31.520 --> 00:34:36.320]   I agree with you, Stacey, that consumers just want to turn it on and it works.
[00:34:36.320 --> 00:34:41.760]   But one of the reasons the Nest stopped selling is it did, but that's all it did.
[00:34:41.760 --> 00:34:43.600]   It didn't work very easily with anything else.
[00:34:43.600 --> 00:34:47.240]   And so it didn't have the utility that they had hoped it would have.
[00:34:47.240 --> 00:34:51.600]   It was ultimately, I think the Nest was ultimately a disappointment.
[00:34:51.600 --> 00:34:53.240]   And right?
[00:34:53.240 --> 00:34:57.440]   So it was easy, but it didn't do what we wanted it to do.
[00:34:57.440 --> 00:35:01.200]   And it also didn't always work very well.
[00:35:01.200 --> 00:35:02.200]   Right.
[00:35:02.200 --> 00:35:03.200]   I mean, people people.
[00:35:03.200 --> 00:35:04.200]   People, people, people, people, people, people, people.
[00:35:04.200 --> 00:35:06.200]   You're heat off in the middle of the night after it went out.
[00:35:06.200 --> 00:35:07.440]   I had always turned it on.
[00:35:07.440 --> 00:35:09.680]   I was like, oh, stop.
[00:35:09.680 --> 00:35:13.160]   So yeah, those kind of things confused people.
[00:35:13.160 --> 00:35:17.720]   And I think the market for $250 thermostats was saturated.
[00:35:17.720 --> 00:35:18.720]   Yeah.
[00:35:18.720 --> 00:35:19.960]   I have two nests.
[00:35:19.960 --> 00:35:21.800]   And when we moved, I didn't put them in again.
[00:35:21.800 --> 00:35:24.760]   It was like, yeah, it wasn't.
[00:35:24.760 --> 00:35:27.480]   I'd already done the big hurdle of buying it.
[00:35:27.480 --> 00:35:29.800]   And but it still wasn't worth a little hurdle of installing it.
[00:35:29.800 --> 00:35:31.160]   It's like, you know what?
[00:35:31.160 --> 00:35:33.320]   This programmable thermostats just fine.
[00:35:33.320 --> 00:35:38.360]   I tell it, turn the heat on at 6 AM, turn it off at 10 AM, turn it back on at 5 PM, turn
[00:35:38.360 --> 00:35:40.160]   it off at 10 PM.
[00:35:40.160 --> 00:35:41.160]   Works.
[00:35:41.160 --> 00:35:42.360]   Here's the temperature.
[00:35:42.360 --> 00:35:43.360]   Keep it at that temperature.
[00:35:43.360 --> 00:35:44.960]   The rest of the time, keep it cold.
[00:35:44.960 --> 00:35:50.160]   What I hope we're getting to is like the world without thermostats, where your temperature
[00:35:50.160 --> 00:35:52.680]   is monitored in each individual room.
[00:35:52.680 --> 00:35:53.680]   It knows where you are.
[00:35:53.680 --> 00:35:55.320]   And then it's like, boom.
[00:35:55.320 --> 00:35:57.000]   That is the big problem with my thermostat.
[00:35:57.000 --> 00:36:00.560]   It was also the problem with a nest, which is it heats the hall.
[00:36:00.560 --> 00:36:04.840]   And it doesn't know how hot I am in the bedroom, but the hall is perfect.
[00:36:04.840 --> 00:36:06.320]   And my bedroom is 100 degrees.
[00:36:06.320 --> 00:36:07.320]   Right.
[00:36:07.320 --> 00:36:08.320]   And you need an Ecobee.
[00:36:08.320 --> 00:36:12.360]   Yeah, the Ecobee has like a remote sensor, which is what you should do.
[00:36:12.360 --> 00:36:13.360]   Yeah.
[00:36:13.360 --> 00:36:18.000]   But ironically, that's, hey, oh my God, look who showed up.
[00:36:18.000 --> 00:36:19.000]   Your power went out.
[00:36:19.000 --> 00:36:21.680]   Matthew, oh, now we're having the same audio issue.
[00:36:21.680 --> 00:36:23.040]   Tell you what, work on his audio issue.
[00:36:23.040 --> 00:36:24.040]   I'm going to take a break.
[00:36:24.040 --> 00:36:25.280]   Matthew Ingram is back.
[00:36:25.280 --> 00:36:26.760]   Fortune.com.
[00:36:26.760 --> 00:36:31.080]   Stacey Higginbotham is here, our new regular on the show.
[00:36:31.080 --> 00:36:33.040]   So happy to say that.
[00:36:33.040 --> 00:36:34.480]   And Aaron Newcomb too.
[00:36:34.480 --> 00:36:37.320]   And both of you guys can come back every day if you want to.
[00:36:37.320 --> 00:36:39.320]   For sure today, brought to you by zippercruder.com.
[00:36:39.320 --> 00:36:42.720]   I'm going to, here's my thesis.
[00:36:42.720 --> 00:36:48.680]   If you're hiring, your goal is to reach the most possible people.
[00:36:48.680 --> 00:36:53.360]   And here's why I say that because somewhere out there is just the right employee and you
[00:36:53.360 --> 00:36:54.520]   want to reach them.
[00:36:54.520 --> 00:36:56.200]   Maybe there's 10 people like that.
[00:36:56.200 --> 00:36:59.920]   Maybe there's a thousand, but you've got to get to wherever they are.
[00:36:59.920 --> 00:37:05.640]   And hiring the right person for your job, your opening is a really big deal.
[00:37:05.640 --> 00:37:06.920]   That can make or break a company.
[00:37:06.920 --> 00:37:13.040]   Trust me, I know the right person makes your company the wrong person can bring the whole
[00:37:13.040 --> 00:37:14.200]   thing down.
[00:37:14.200 --> 00:37:18.840]   Your company is made of people and getting the right person in the right job is so critical.
[00:37:18.840 --> 00:37:23.400]   Zip recruiter makes it easy because instead of just posting on, you know, onesie, two
[00:37:23.400 --> 00:37:27.560]   z job boards and Craigslist, maybe this posts everywhere.
[00:37:27.560 --> 00:37:33.240]   A hundred plus job sites plus Craigslist plus Twitter plus Facebook plus LinkedIn plus everywhere.
[00:37:33.240 --> 00:37:37.680]   So you're going to get many, many people seeing your posting.
[00:37:37.680 --> 00:37:41.920]   They'll also immediately match it to their more than six million current resumes in their
[00:37:41.920 --> 00:37:42.920]   own database.
[00:37:42.920 --> 00:37:44.240]   So that's awesome.
[00:37:44.240 --> 00:37:47.680]   That means, you know, you're going to get candidates almost immediately.
[00:37:47.680 --> 00:37:51.720]   Now I know some of you are saying, well, that's the last thing I want as a million applicants.
[00:37:51.720 --> 00:37:54.240]   I don't, they're going to, my phone's going to ring off the hook.
[00:37:54.240 --> 00:37:55.440]   No, this is the beauty part.
[00:37:55.440 --> 00:37:57.640]   It doesn't go to your phone or your inbox.
[00:37:57.640 --> 00:38:01.880]   It goes to the zip recruiter interface where you can very easily screen out the ones that
[00:38:01.880 --> 00:38:08.120]   just don't make it pick, rank the rest and pick the right person fast.
[00:38:08.120 --> 00:38:11.960]   You could search by skills, by location, by work experience.
[00:38:11.960 --> 00:38:17.640]   They're going to deliver the best, most relevant candidates based on what you're looking for.
[00:38:17.640 --> 00:38:19.400]   Plus they offer optimized pages.
[00:38:19.400 --> 00:38:23.040]   So for your candidates to look at the look great on any screen that there are a lot of
[00:38:23.040 --> 00:38:24.880]   people now who apply over mobile.
[00:38:24.880 --> 00:38:26.960]   I mean, that's how they're, that's their computer.
[00:38:26.960 --> 00:38:31.840]   They can do this over their, on their mobile phones, find candidates in any city, any industry,
[00:38:31.840 --> 00:38:35.880]   nationwide, no more juggling emails, no more answering calls.
[00:38:35.880 --> 00:38:38.600]   Just find the right person fast.
[00:38:38.600 --> 00:38:44.080]   No wonder more than 800,000 businesses now use zipper cruder, including us.
[00:38:44.080 --> 00:38:47.200]   They've been trusted by many hundreds of the Fortune 500.
[00:38:47.200 --> 00:38:49.040]   You should try them free.
[00:38:49.040 --> 00:38:55.360]   ziprecruiter.com/twig ziprecruiter.com/twig.
[00:38:55.360 --> 00:38:58.440]   G four days free.
[00:38:58.440 --> 00:39:00.760]   ziprecruiter.com/twig.
[00:39:00.760 --> 00:39:03.520]   I think we got Matthew McIngham back.
[00:39:03.520 --> 00:39:04.520]   Are you there?
[00:39:04.520 --> 00:39:05.520]   Can you hear me?
[00:39:05.520 --> 00:39:06.520]   Okay.
[00:39:06.520 --> 00:39:07.520]   Yeah, your power went out.
[00:39:07.520 --> 00:39:08.520]   It went out twice actually.
[00:39:08.520 --> 00:39:14.480]   When I once and came back and I was up and I told Karsten, okay, you can call me again.
[00:39:14.480 --> 00:39:17.800]   And then just literally as I typed that, it went out again.
[00:39:17.800 --> 00:39:18.800]   It wasn't just you though.
[00:39:18.800 --> 00:39:19.800]   Was your neighborhood?
[00:39:19.800 --> 00:39:22.560]   Hard to say.
[00:39:22.560 --> 00:39:25.120]   They've been working on the power in our neighborhood.
[00:39:25.120 --> 00:39:26.120]   And so I think.
[00:39:26.120 --> 00:39:30.160]   Wasn't because you have too many NIST thermostats and echoes.
[00:39:30.160 --> 00:39:31.160]   I don't have any.
[00:39:31.160 --> 00:39:33.360]   So you didn't miss much.
[00:39:33.360 --> 00:39:38.760]   We were talking about the new kind of way of thinking about things that Google has introduced.
[00:39:38.760 --> 00:39:44.400]   Their idea of an assistant as opposed to the 10 blue links and the Google home device.
[00:39:44.400 --> 00:39:47.960]   Google home is really interesting, I think.
[00:39:47.960 --> 00:39:53.640]   It's something I was, I think I said this to you when Amazon came out with the Echo.
[00:39:53.640 --> 00:39:58.000]   I thought this is the perfect, this should be something that Google does.
[00:39:58.000 --> 00:40:02.400]   I mean, if it's already, if you're already pretty integrated with Google, it just seems
[00:40:02.400 --> 00:40:03.920]   like a natural for them.
[00:40:03.920 --> 00:40:06.840]   Google now trying to suggest things to you.
[00:40:06.840 --> 00:40:10.640]   And so I'm eager to see what it's like in sort of real life.
[00:40:10.640 --> 00:40:14.280]   You know what I do though with the Echo that I won't be able to do is order stuff from
[00:40:14.280 --> 00:40:15.880]   my Amazon.
[00:40:15.880 --> 00:40:20.960]   And it's really kind of insanely, I've gotten kind of hooked on it.
[00:40:20.960 --> 00:40:22.520]   Like I'm standing there.
[00:40:22.520 --> 00:40:25.120]   It's the morning I'm brushing my teeth and I go, oh, I'm almost out of deodorant.
[00:40:25.120 --> 00:40:28.520]   And I can literally say Echo, order me some more deodorant.
[00:40:28.520 --> 00:40:29.760]   It knows what I bought last time.
[00:40:29.760 --> 00:40:30.760]   It says, oh, you mean this?
[00:40:30.760 --> 00:40:32.640]   And I said, yes, what's your key?
[00:40:32.640 --> 00:40:33.640]   What's your pin?
[00:40:33.640 --> 00:40:34.640]   I enter it.
[00:40:34.640 --> 00:40:35.640]   I don't enter it.
[00:40:35.640 --> 00:40:36.640]   I say it.
[00:40:36.640 --> 00:40:37.640]   And it's ordered.
[00:40:37.640 --> 00:40:40.480]   And I get it two days from now that I don't know if Google's going to, I guess they could
[00:40:40.480 --> 00:40:41.480]   make a do you.
[00:40:41.480 --> 00:40:42.480]   And that's really powerful.
[00:40:42.480 --> 00:40:43.480]   It's amazing.
[00:40:43.480 --> 00:40:45.600]   You're, yeah, you can get a dash button.
[00:40:45.600 --> 00:40:46.600]   Yeah, you can get.
[00:40:46.600 --> 00:40:47.600]   So I order stuff now.
[00:40:47.600 --> 00:40:48.920]   I'll say, oh, I'm out of batteries.
[00:40:48.920 --> 00:40:50.120]   Google get me some more bad or echo.
[00:40:50.120 --> 00:40:51.720]   I give me some more batteries.
[00:40:51.720 --> 00:40:54.720]   That is a remarkable transformation.
[00:40:54.720 --> 00:40:58.640]   You know, what happens if those of us who covered technology, we really can see this
[00:40:58.640 --> 00:41:00.080]   happen all the time.
[00:41:00.080 --> 00:41:05.560]   And those of you who are enthusiasts who use technology, but we kind of, we get, we take
[00:41:05.560 --> 00:41:06.840]   it for granted.
[00:41:06.840 --> 00:41:10.000]   But our lives have changed phenomenally with some of these technologies.
[00:41:10.000 --> 00:41:13.880]   The fact that I have to go to the library to find out when Jimmy Stewart was born, I could
[00:41:13.880 --> 00:41:17.880]   just literally seconds from now find that information is remarkable.
[00:41:17.880 --> 00:41:18.360]   And this is a
[00:41:18.360 --> 00:41:20.560]   According comparable step.
[00:41:20.560 --> 00:41:25.200]   According to the information, Apple is working on something very similar.
[00:41:25.200 --> 00:41:26.920]   Can they do it as good?
[00:41:26.920 --> 00:41:27.920]   Can they do it as well?
[00:41:27.920 --> 00:41:28.920]   Well, that's a good question.
[00:41:28.920 --> 00:41:32.880]   And it feels to me, I don't know about what's these two things, but it feels to me as though
[00:41:32.880 --> 00:41:36.640]   what's really going to be important for lots of people or for these things to go broad
[00:41:36.640 --> 00:41:40.760]   is, is for them to be as extensible as possible.
[00:41:40.760 --> 00:41:44.240]   So to allow you to connect to as many different things as possible.
[00:41:44.240 --> 00:41:46.080]   Exactly what we were talking about, by the way.
[00:41:46.080 --> 00:41:50.160]   Yeah, if Apple only allows you to connect to Apple things, that's great.
[00:41:50.160 --> 00:41:54.600]   If you want to live inside their ecosystem, but lots of people don't.
[00:41:54.600 --> 00:41:57.440]   So they're going to make it easy to connect to other things.
[00:41:57.440 --> 00:41:59.840]   That's not an Apple culture.
[00:41:59.840 --> 00:42:01.240]   That's not a do, right?
[00:42:01.240 --> 00:42:03.200]   No, Google's pretty good.
[00:42:03.200 --> 00:42:04.200]   Usually.
[00:42:04.200 --> 00:42:07.920]   Google's good at it and Apple's been bad just on the infrastructure side too.
[00:42:07.920 --> 00:42:08.920]   True.
[00:42:08.920 --> 00:42:16.000]   Like so you're kind of like, but I will say this whole conversation that we're having
[00:42:16.000 --> 00:42:19.520]   right now makes me feel so bad for Sonos.
[00:42:19.520 --> 00:42:20.680]   I know.
[00:42:20.680 --> 00:42:25.280]   I said that six months ago, I said, well, it's nice knowing you.
[00:42:25.280 --> 00:42:29.280]   Because they've got Chromecast audio built into the home.
[00:42:29.280 --> 00:42:30.280]   This could be amazing.
[00:42:30.280 --> 00:42:31.280]   On the Google.
[00:42:31.280 --> 00:42:32.280]   Yeah.
[00:42:32.280 --> 00:42:33.880]   But okay, so here's what I would like though.
[00:42:33.880 --> 00:42:40.560]   How just visualize this world with the Sonos, with the mic, and it connects back to whatever
[00:42:40.560 --> 00:42:41.560]   you want.
[00:42:41.560 --> 00:42:48.720]   It could call it could run, I'll say her name, Alexa voice services, or it could run, you
[00:42:48.720 --> 00:42:49.720]   know, Google's.
[00:42:49.720 --> 00:42:54.920]   And if you visualize that world, that's like an open world because.
[00:42:54.920 --> 00:42:57.840]   Can they do that?
[00:42:57.840 --> 00:42:59.360]   Well they'd have to make the heels, right?
[00:42:59.360 --> 00:43:00.360]   Yeah.
[00:43:00.360 --> 00:43:01.360]   What is so is right now?
[00:43:01.360 --> 00:43:03.480]   It's just a speaker system.
[00:43:03.480 --> 00:43:05.120]   The five has a mic, doesn't it?
[00:43:05.120 --> 00:43:07.520]   Yeah, it has a mic, but not turned on.
[00:43:07.520 --> 00:43:12.320]   And actually one of my problems with Sonos in the beginning, apart from the price was
[00:43:12.320 --> 00:43:16.000]   that they weren't that good at being open to other things.
[00:43:16.000 --> 00:43:17.000]   So they weren't.
[00:43:17.000 --> 00:43:19.080]   And they soft felt like a closed ecosystem.
[00:43:19.080 --> 00:43:20.600]   Soft was awful.
[00:43:20.600 --> 00:43:21.600]   And yet I love it.
[00:43:21.600 --> 00:43:25.680]   And I have Sonos in every room because it is the best way to do that.
[00:43:25.680 --> 00:43:27.080]   But those days are dying.
[00:43:27.080 --> 00:43:33.120]   Right now I have an Amazon dot Bluetooth to a nice speaker system nicer than the Sonos,
[00:43:33.120 --> 00:43:34.280]   frankly.
[00:43:34.280 --> 00:43:38.640]   And that's how I listen and I haven't used the Sonos in the bedroom in a long time.
[00:43:38.640 --> 00:43:42.840]   And if you can have a Chromecast audio, you know, then you can use whatever speaker you
[00:43:42.840 --> 00:43:43.840]   want.
[00:43:43.840 --> 00:43:45.760]   I bought these speakers I bought from JBL.
[00:43:45.760 --> 00:43:50.880]   They do AirPlay, Chromecast, they do everything, every protocol Bluetooth.
[00:43:50.880 --> 00:43:55.160]   So you can have, I thought this is going to what I need is like it does DLNA.
[00:43:55.160 --> 00:43:57.680]   I could do any input.
[00:43:57.680 --> 00:43:58.680]   That's what you want, right?
[00:43:58.680 --> 00:44:00.680]   You want that kind of openness and that kind of flexibility.
[00:44:00.680 --> 00:44:01.680]   Exactly.
[00:44:01.680 --> 00:44:02.680]   Sonos could be.
[00:44:02.680 --> 00:44:09.040]   I mean, what they've done is giving you a way to talk to your house and hopefully a way
[00:44:09.040 --> 00:44:13.320]   for your house to talk back to you or the web, the Internet.
[00:44:13.320 --> 00:44:16.400]   And it just feels like such a huge missed opportunity.
[00:44:16.400 --> 00:44:17.400]   Makes me sad.
[00:44:17.400 --> 00:44:21.880]   And I guess I'm afraid, I mean, I'm afraid that Apple is going to have one that's just
[00:44:21.880 --> 00:44:26.760]   Apple and Amazon is going to focus on trying to get you to order more stuff from Amazon.
[00:44:26.760 --> 00:44:27.760]   And you know what?
[00:44:27.760 --> 00:44:31.880]   I resent this sort of siloification of everything.
[00:44:31.880 --> 00:44:39.640]   I mean, I'm upset enough about Facebook and that sort of thing sort of controlling what
[00:44:39.640 --> 00:44:40.640]   we do.
[00:44:40.640 --> 00:44:46.480]   And I don't want another walled garden in a different hardware format.
[00:44:46.480 --> 00:44:56.280]   I should point out though, I think we assume that Apple and Google and Microsoft and maybe
[00:44:56.280 --> 00:44:59.400]   Amazon have an inside track because they've been working on this.
[00:44:59.400 --> 00:45:01.440]   But don't forget this hound app.
[00:45:01.440 --> 00:45:07.440]   I mean, these guys out of nowhere made something that frankly is everybody's good, maybe better.
[00:45:07.440 --> 00:45:09.560]   It's better than Siri for sure.
[00:45:09.560 --> 00:45:12.120]   And they by the way, they are open.
[00:45:12.120 --> 00:45:13.120]   You can add.
[00:45:13.120 --> 00:45:14.800]   Yeah, that's a good point.
[00:45:14.800 --> 00:45:17.920]   You could add hound, for instance, to your Sonos.
[00:45:17.920 --> 00:45:22.560]   And if Sonos is smart, they're looking at that.
[00:45:22.560 --> 00:45:25.200]   And I wonder to be honest, I don't know enough about it.
[00:45:25.200 --> 00:45:26.200]   Maybe Stacey does.
[00:45:26.200 --> 00:45:35.320]   I got to wonder whether AI, like real sort of an assistant, virtual assistant that actually
[00:45:35.320 --> 00:45:40.880]   works in a way that's kind of intuitive for people is not something you can just throw
[00:45:40.880 --> 00:45:43.600]   a huge amount of math horsepower at.
[00:45:43.600 --> 00:45:44.680]   Or do you know what I mean?
[00:45:44.680 --> 00:45:48.360]   I think there's something ineffable.
[00:45:48.360 --> 00:45:53.480]   There's something sort of hard to define about what makes those things work well.
[00:45:53.480 --> 00:45:57.840]   And so maybe it's open to anyone to solve that problem.
[00:45:57.840 --> 00:46:02.520]   Maybe it isn't a problem that has to do with processing power and how many PhDs you have.
[00:46:02.520 --> 00:46:05.040]   One thing that's key is big data though.
[00:46:05.040 --> 00:46:06.040]   True.
[00:46:06.040 --> 00:46:07.040]   You have to have-
[00:46:07.040 --> 00:46:08.040]   No, definitely.
[00:46:08.040 --> 00:46:09.040]   You need those input.
[00:46:09.040 --> 00:46:10.040]   Yeah.
[00:46:10.040 --> 00:46:12.160]   There's a startup called Josh AI that does this.
[00:46:12.160 --> 00:46:14.960]   But- Oh, there are a lot of them.
[00:46:14.960 --> 00:46:15.960]   There's VIV.A.I.
[00:46:15.960 --> 00:46:16.960]   VIV.A.I.
[00:46:16.960 --> 00:46:17.960]   Yeah.
[00:46:17.960 --> 00:46:23.440]   Well, VIV is- VIV's AI is more about connecting things without having-
[00:46:23.440 --> 00:46:29.120]   The developers have to do a lot of that grunt work of linking APIs together.
[00:46:29.120 --> 00:46:34.080]   But what I was going to say is if you guys saw the movie Herr, what I thought was worth
[00:46:34.080 --> 00:46:38.640]   noting is the guy bought a software program and he loaded it on his-
[00:46:38.640 --> 00:46:43.400]   I think his desktop and his phone or whatever.
[00:46:43.400 --> 00:46:45.960]   And what if that's the kind of the way this works?
[00:46:45.960 --> 00:46:48.080]   Because that's totally possible.
[00:46:48.080 --> 00:46:51.960]   It just needs to the software would link it back to the cloud and tell it where to go.
[00:46:51.960 --> 00:46:53.160]   I don't know.
[00:46:53.160 --> 00:46:59.680]   This is the knowledge navigator that originally was conceived of by Alan Kay when he-
[00:46:59.680 --> 00:47:01.040]   And then brought it- he brought it to Apple.
[00:47:01.040 --> 00:47:05.120]   And Apple, under Scully, made a knowledge navigator video.
[00:47:05.120 --> 00:47:08.400]   And the whole idea of the knowledge navigator was-
[00:47:08.400 --> 00:47:10.800]   It's kind of silly.
[00:47:10.800 --> 00:47:12.160]   You have this-
[00:47:12.160 --> 00:47:14.240]   What's the main on deforestation?
[00:47:14.240 --> 00:47:15.240]   Assistant.
[00:47:15.240 --> 00:47:16.360]   This digital assistant.
[00:47:16.360 --> 00:47:19.120]   Who knows it's on- in this case it's on it.
[00:47:19.120 --> 00:47:21.920]   Remember, this was- this was 30 years ago.
[00:47:21.920 --> 00:47:25.200]   So this is a MacBook, but they didn't have MacBooks at the time.
[00:47:25.200 --> 00:47:28.040]   They call it a Dine-a-Book.
[00:47:28.040 --> 00:47:31.840]   And the idea of this whole thing was that it was an intermediate.
[00:47:31.840 --> 00:47:35.240]   It didn't have to have any knowledge or data about you.
[00:47:35.240 --> 00:47:37.240]   What it would learn I guess.
[00:47:37.240 --> 00:47:42.080]   But it was- it had knowledge about- I guess what you need is knowledge about the outside
[00:47:42.080 --> 00:47:43.240]   world, right?
[00:47:43.240 --> 00:47:46.200]   So it goes- it's an agent.
[00:47:46.200 --> 00:47:49.640]   If it has enough knowledge about the outside world, it doesn't need to know much about
[00:47:49.640 --> 00:47:50.640]   anything else.
[00:47:50.640 --> 00:47:53.560]   And I think, you know, obviously big data helps.
[00:47:53.560 --> 00:47:59.000]   But I'm kind of thinking about the way that I would interact with and have interacted
[00:47:59.000 --> 00:48:01.640]   with sort of virtual assistants.
[00:48:01.640 --> 00:48:07.680]   The value of doing that and the value that it brings to you is in the interaction and
[00:48:07.680 --> 00:48:10.160]   the way it sort of responds.
[00:48:10.160 --> 00:48:13.600]   And I think those things are all open.
[00:48:13.600 --> 00:48:18.680]   If it can just go find the information you want and arrange it on the Internet, it doesn't
[00:48:18.680 --> 00:48:19.680]   need that much.
[00:48:19.680 --> 00:48:25.000]   I guess what I would say is we're not making a distinction between two distinct entities.
[00:48:25.000 --> 00:48:28.560]   There's machine learning and there's artificial intelligence.
[00:48:28.560 --> 00:48:31.680]   And I think we conflate them, but I don't think they're exactly the same.
[00:48:31.680 --> 00:48:35.480]   So the idea- and I bought a book on this.
[00:48:35.480 --> 00:48:37.240]   After Google I always said, "I better learn about this."
[00:48:37.240 --> 00:48:41.760]   I bought a book that put me- it literally made me unconscious after one chapter is called
[00:48:41.760 --> 00:48:43.440]   "Python Machine Learning."
[00:48:43.440 --> 00:48:44.680]   It's a good book.
[00:48:44.680 --> 00:48:45.680]   You want to borrow?
[00:48:45.680 --> 00:48:46.680]   Sounds good.
[00:48:46.680 --> 00:48:47.680]   It's a great way to sleep.
[00:48:47.680 --> 00:48:49.200]   It's better than anything, better than Sominax.
[00:48:49.200 --> 00:48:50.200]   No, it's amazing.
[00:48:50.200 --> 00:48:55.000]   It was 7 p.m. and I was out.
[00:48:55.000 --> 00:48:58.120]   Because of these- and it said it's the easiest way.
[00:48:58.120 --> 00:48:59.280]   This is- oh, you're going to get this.
[00:48:59.280 --> 00:49:00.800]   It's so simple.
[00:49:00.800 --> 00:49:07.560]   Machine learning is about throwing data at algorithms and the algorithms learning a variety
[00:49:07.560 --> 00:49:08.720]   of different skills.
[00:49:08.720 --> 00:49:13.600]   One thing I did learn from the first chapter is sometimes it's the ability to discriminate
[00:49:13.600 --> 00:49:16.840]   between things, sometimes facial recognition.
[00:49:16.840 --> 00:49:18.360]   That's machine learning.
[00:49:18.360 --> 00:49:22.000]   But there's artificial intelligence which may involve machine learning, but it's the idea
[00:49:22.000 --> 00:49:28.120]   that- and that's more like this agent, where it is- it has understandings of the outside
[00:49:28.120 --> 00:49:33.160]   world and it can do things on your behalf.
[00:49:33.160 --> 00:49:35.080]   So one needs big data for sure.
[00:49:35.080 --> 00:49:40.800]   I guess you need big data to develop an AI, but it doesn't have to be Google style.
[00:49:40.800 --> 00:49:41.800]   Look at Hound.
[00:49:41.800 --> 00:49:43.600]   Hound is really, really good.
[00:49:43.600 --> 00:49:47.240]   Clearly, they don't have the data set that Google has.
[00:49:47.240 --> 00:49:51.120]   So none of these things have actual AI.
[00:49:51.120 --> 00:49:55.400]   What they've done is- it's- it's- it's- the way you're defining it is-
[00:49:55.400 --> 00:49:56.400]   It's tricking.
[00:49:56.400 --> 00:49:58.040]   I mean, it's not even tricking you.
[00:49:58.040 --> 00:49:59.360]   I mean, it's pretty- I don't know.
[00:49:59.360 --> 00:50:02.680]   I think it's pretty freaking awesome.
[00:50:02.680 --> 00:50:09.960]   But there's a couple ways that it's, I guess you would say tricking you with this because
[00:50:09.960 --> 00:50:14.880]   you're doing so much with speech to- I mean with talking to something which is what we're
[00:50:14.880 --> 00:50:15.880]   talking about.
[00:50:15.880 --> 00:50:20.400]   So here's got to figure out, A, it's got to take whatever you've just said, turn it into
[00:50:20.400 --> 00:50:25.720]   actual text, then figure out what to do with that text, which is another step.
[00:50:25.720 --> 00:50:28.600]   And then, you know, from there it's got to actually go off.
[00:50:28.600 --> 00:50:33.760]   VIV is cool because when it goes off to actually implement something, it's helping automate
[00:50:33.760 --> 00:50:37.080]   that process.
[00:50:37.080 --> 00:50:41.920]   So in the way it responds back to you is still crazy programmed.
[00:50:41.920 --> 00:50:44.680]   I mean, we're not going to get AI's with personality.
[00:50:44.680 --> 00:50:45.680]   Right.
[00:50:45.680 --> 00:50:48.920]   Well, and I'm trying to find the article I read.
[00:50:48.920 --> 00:50:53.240]   I think it was in Medium by an AI researcher who said you'd be surprised at how many times
[00:50:53.240 --> 00:50:56.800]   what looks like AI has a human involved.
[00:50:56.800 --> 00:50:58.440]   Yeah, like Facebook.
[00:50:58.440 --> 00:50:59.440]   Yeah, Facebook.
[00:50:59.440 --> 00:51:03.760]   Yeah, they actually, I find it fascinating that they want to kind of pretend that it's
[00:51:03.760 --> 00:51:08.280]   a machine, but which is the opposite of what you would think they would want to do.
[00:51:08.280 --> 00:51:14.960]   Even Facebook's trending topics, it turns out happened to have human intervention.
[00:51:14.960 --> 00:51:15.960]   It was cute.
[00:51:15.960 --> 00:51:19.080]   Computers, frankly, are not that smart.
[00:51:19.080 --> 00:51:24.640]   Did you see the, there's an article in Aon magazine that is all about how your brain is
[00:51:24.640 --> 00:51:27.720]   not an, your brain is not a computer.
[00:51:27.720 --> 00:51:30.480]   If you haven't read it, it goes, check it out.
[00:51:30.480 --> 00:51:31.480]   It's amazing.
[00:51:31.480 --> 00:51:34.280]   Jeff Hawkins has been saying this for a long time.
[00:51:34.280 --> 00:51:35.280]   Right.
[00:51:35.280 --> 00:51:39.280]   The founder of Palm, the guy who created graffiti, he's now, you know, as a neuroscientist who's
[00:51:39.280 --> 00:51:43.320]   now working in Nemento on brain-like chips.
[00:51:43.320 --> 00:51:44.800]   And that's what he's always says.
[00:51:44.800 --> 00:51:45.800]   Our brains are slow.
[00:51:45.800 --> 00:51:46.800]   Yeah.
[00:51:46.800 --> 00:51:49.080]   Here's me move at the speed of light.
[00:51:49.080 --> 00:51:53.640]   Our brains much, much slower than that, but they're massively parallel, like unbelievably
[00:51:53.640 --> 00:51:54.640]   parallel.
[00:51:54.640 --> 00:51:55.640]   And this.
[00:51:55.640 --> 00:51:57.040]   They're also go ahead.
[00:51:57.040 --> 00:52:00.240]   They're also not representational the way a computer is.
[00:52:00.240 --> 00:52:01.240]   Exactly.
[00:52:01.240 --> 00:52:02.240]   Exactly.
[00:52:02.240 --> 00:52:03.240]   Exactly.
[00:52:03.240 --> 00:52:05.800]   And that's why, and that's why computers wind up brute forcing a lot of stuff.
[00:52:05.800 --> 00:52:12.920]   You think about the way computers play chess, real, like human chess players making leaps
[00:52:12.920 --> 00:52:18.720]   of sort of logic and they see patterns without even recognizing them.
[00:52:18.720 --> 00:52:24.760]   They jump steps and computers have to go through every single one of those steps, which
[00:52:24.760 --> 00:52:27.200]   doesn't always get you to where you want to go.
[00:52:27.200 --> 00:52:31.080]   The essay you were talking about is right, Robert Epstein, the empty brain.
[00:52:31.080 --> 00:52:34.640]   Your brain does not process information, retrieve knowledge or store memories.
[00:52:34.640 --> 00:52:40.120]   In short, your brain is not a computer that's from this month's A Young Magazine.
[00:52:40.120 --> 00:52:41.120]   Yeah.
[00:52:41.120 --> 00:52:42.320]   Very interesting.
[00:52:42.320 --> 00:52:44.120]   He's a research psychologist.
[00:52:44.120 --> 00:52:50.080]   Now I have to say a lot of people like to think that because we do.
[00:52:50.080 --> 00:52:55.760]   And if you ask Ray Kurzweil, he'll say, well, it doesn't really matter how it gets there.
[00:52:55.760 --> 00:52:57.280]   That's what I mean by tricking you, by the way.
[00:52:57.280 --> 00:53:00.440]   I mean, computers don't do division.
[00:53:00.440 --> 00:53:04.320]   They just look like they can do division because they can subtract really fast, right?
[00:53:04.320 --> 00:53:07.200]   So as long as you get there, it doesn't matter.
[00:53:07.200 --> 00:53:08.200]   Yeah.
[00:53:08.200 --> 00:53:11.760]   Kurzweil's point was if it's indistinguishable from a human mentality, does it matter that
[00:53:11.760 --> 00:53:12.760]   it doesn't work the same way?
[00:53:12.760 --> 00:53:15.320]   If it plays chess better than a human, does it matter?
[00:53:15.320 --> 00:53:17.440]   It's not playing it the same way a human plays it.
[00:53:17.440 --> 00:53:18.440]   I don't know.
[00:53:18.440 --> 00:53:22.680]   But I think if you're thinking about human assistance or sorry, virtual assistance and
[00:53:22.680 --> 00:53:28.040]   what you might want them to do, some things a computer is going to be really good at,
[00:53:28.040 --> 00:53:30.160]   like finding and getting information.
[00:53:30.160 --> 00:53:31.160]   Yes.
[00:53:31.160 --> 00:53:35.920]   Understanding nuance or your tone of voice or your emotion or terrible.
[00:53:35.920 --> 00:53:36.920]   Do you know what I mean?
[00:53:36.920 --> 00:53:38.440]   Yeah, they're terrible at that.
[00:53:38.440 --> 00:53:39.440]   We're nowhere near.
[00:53:39.440 --> 00:53:42.000]   I mean, even human beings are terrible at that.
[00:53:42.000 --> 00:53:46.840]   Well, but a two year old is much better at recognizing mommy's mad than the Alexa could
[00:53:46.840 --> 00:53:47.840]   ever be.
[00:53:47.840 --> 00:53:50.920]   Although they do have an advantage.
[00:53:50.920 --> 00:53:55.080]   So here's a creepy, like this is one of the things that I think about Facebook actually
[00:53:55.080 --> 00:53:56.640]   did it with him.
[00:53:56.640 --> 00:53:59.360]   Imagine telling your personal assistant, your virtual assistant.
[00:53:59.360 --> 00:54:02.600]   Oh, hey, say hello to my dog.
[00:54:02.600 --> 00:54:03.600]   Hi, Sam.
[00:54:03.600 --> 00:54:06.400]   Oh, that is my UPS delivery.
[00:54:06.400 --> 00:54:07.400]   I love that.
[00:54:07.400 --> 00:54:08.960]   See, who needs a ring doorbell?
[00:54:08.960 --> 00:54:09.960]   You got Sam.
[00:54:09.960 --> 00:54:11.760]   Yeah, I've got, yeah.
[00:54:11.760 --> 00:54:14.280]   The sound doorbell.
[00:54:14.280 --> 00:54:16.920]   I have Aussie does exactly the same thing.
[00:54:16.920 --> 00:54:17.920]   No, I know.
[00:54:17.920 --> 00:54:20.880]   Hopefully she'll stop.
[00:54:20.880 --> 00:54:21.880]   I may have to go.
[00:54:21.880 --> 00:54:23.760]   You might have to see what UPS brought.
[00:54:23.760 --> 00:54:24.760]   That's just an excuse.
[00:54:24.760 --> 00:54:25.840]   I know that's what you want to do.
[00:54:25.840 --> 00:54:26.840]   There you go.
[00:54:26.840 --> 00:54:30.800]   No, I have a caller that vibrates for that's the shut up.
[00:54:30.800 --> 00:54:31.800]   Really?
[00:54:31.800 --> 00:54:32.800]   It's not a shock, I hope.
[00:54:32.800 --> 00:54:34.000]   It is not a shock.
[00:54:34.000 --> 00:54:35.800]   It can shock, but we don't shock.
[00:54:35.800 --> 00:54:37.520]   We just buzz vibrate.
[00:54:37.520 --> 00:54:41.400]   Does she go, oh, that feels good or does she stop barking?
[00:54:41.400 --> 00:54:42.720]   No, she stops barking.
[00:54:42.720 --> 00:54:44.720]   She's like, what's happening?
[00:54:44.720 --> 00:54:48.120]   We use a Coke can with some pennies.
[00:54:48.120 --> 00:54:49.120]   Yeah, yeah, yeah.
[00:54:49.120 --> 00:54:50.120]   That works.
[00:54:50.120 --> 00:54:51.920]   That works like dog's.
[00:54:51.920 --> 00:54:55.280]   We tried that and she was louder.
[00:54:55.280 --> 00:54:56.280]   Oh, wow.
[00:54:56.280 --> 00:54:59.920]   Yeah, she did not care for that, but told us.
[00:54:59.920 --> 00:55:03.520]   Can you get those colors for humans or?
[00:55:03.520 --> 00:55:05.520]   So I tried it on myself before I put it on her.
[00:55:05.520 --> 00:55:06.520]   I tried it on myself.
[00:55:06.520 --> 00:55:07.520]   Good for you.
[00:55:07.520 --> 00:55:08.520]   You are really humane.
[00:55:08.520 --> 00:55:09.520]   Yeah.
[00:55:09.520 --> 00:55:10.520]   It's painful.
[00:55:10.520 --> 00:55:11.520]   It's like the path.
[00:55:11.520 --> 00:55:13.520]   You tried the food as well?
[00:55:13.520 --> 00:55:16.320]   I have eaten a dog biscuit.
[00:55:16.320 --> 00:55:17.720]   Didn't we all do those dares when we were kids?
[00:55:17.720 --> 00:55:19.120]   I've eaten a dog biscuit.
[00:55:19.120 --> 00:55:20.120]   Yeah, recently.
[00:55:20.120 --> 00:55:22.120]   Her dog biscuit is great.
[00:55:22.120 --> 00:55:23.720]   It's better than mine.
[00:55:23.720 --> 00:55:24.720]   Probably.
[00:55:24.720 --> 00:55:27.120]   Okay, I have no idea what we were talking about.
[00:55:27.120 --> 00:55:28.120]   Oh, it's so good too.
[00:55:28.120 --> 00:55:29.520]   I gosh darn it.
[00:55:29.520 --> 00:55:30.520]   But she learning.
[00:55:30.520 --> 00:55:32.520]   Oh, Virgil says, oh, here's the example.
[00:55:32.520 --> 00:55:33.920]   Buying a gift.
[00:55:33.920 --> 00:55:34.920]   Yes.
[00:55:34.920 --> 00:55:35.920]   Well-faced.
[00:55:35.920 --> 00:55:37.000]   Like, oh, it's my wife's birthday.
[00:55:37.000 --> 00:55:39.040]   Buy her a gift.
[00:55:39.040 --> 00:55:43.960]   If you've got Facebook and you've got people involved, you know, that don't know the person,
[00:55:43.960 --> 00:55:45.560]   yeah, it could be iffy.
[00:55:45.560 --> 00:55:50.960]   But if you had like Google and Google like new or email address, this is totally unethical.
[00:55:50.960 --> 00:55:57.000]   But those assistants could actually generate some really interesting products and or gifts.
[00:55:57.000 --> 00:55:58.000]   I don't know.
[00:55:58.000 --> 00:55:59.640]   I'm thinking about like.
[00:55:59.640 --> 00:56:00.640]   True.
[00:56:00.640 --> 00:56:01.640]   Go on.
[00:56:01.640 --> 00:56:02.640]   Amazon could do that easily.
[00:56:02.640 --> 00:56:07.080]   It's one of the things you're talking about with Aloe because of course Aloe incorporates
[00:56:07.080 --> 00:56:10.240]   those canned answers that inbox has.
[00:56:10.240 --> 00:56:13.960]   And we're thinking, we'll be really nice if Aloe had a switch to say, you know what?
[00:56:13.960 --> 00:56:15.600]   I don't want to talk to this person.
[00:56:15.600 --> 00:56:20.120]   Could you just, whenever they chat me respond with a suitable reply?
[00:56:20.120 --> 00:56:24.600]   And then what if that person did the same thing to you and those bots would just continue
[00:56:24.600 --> 00:56:26.320]   to talk to each other forever?
[00:56:26.320 --> 00:56:29.240]   That would be kind of awesome.
[00:56:29.240 --> 00:56:33.240]   I think it was a great video.
[00:56:33.240 --> 00:56:34.520]   Future of the social internet.
[00:56:34.520 --> 00:56:37.360]   It sounds like a Philip K. Dick novel actually.
[00:56:37.360 --> 00:56:38.360]   We all know that.
[00:56:38.360 --> 00:56:39.360]   Let's take a break.
[00:56:39.360 --> 00:56:42.240]   You could go see what Sam was all upset about and we're going to take a break and come back
[00:56:42.240 --> 00:56:43.240]   with more.
[00:56:43.240 --> 00:56:44.240]   You know what?
[00:56:44.240 --> 00:56:45.240]   It could have been.
[00:56:45.240 --> 00:56:46.240]   Could have been your blue apron delivery.
[00:56:46.240 --> 00:56:47.560]   Oh, man, I love blue apron.
[00:56:47.560 --> 00:56:49.160]   We got to get Stacey a blue apron box.
[00:56:49.160 --> 00:56:52.800]   I bet she did get to blue apron's named after the little blue aprons.
[00:56:52.800 --> 00:56:58.600]   The apprentices at the court on blue wear, the up and coming young chefs.
[00:56:58.600 --> 00:57:03.320]   It's not a cooking school, but it kind of makes you really enjoy cooking.
[00:57:03.320 --> 00:57:06.240]   The idea is, you know, end of the day you've been working hard.
[00:57:06.240 --> 00:57:07.320]   It's five o'clock.
[00:57:07.320 --> 00:57:10.600]   The last thing you want to do is think about, what am I going to make for dinner?
[00:57:10.600 --> 00:57:13.480]   And then I got to go grocery shopping and pick up the ingredients.
[00:57:13.480 --> 00:57:15.480]   Then I got to go home and cook it.
[00:57:15.480 --> 00:57:20.680]   That's why we all end up at fast food restaurants or, you know, Applebee's.
[00:57:20.680 --> 00:57:22.880]   Now this is so much better.
[00:57:22.880 --> 00:57:25.240]   Imagine instead you go home and they're on your doorstep.
[00:57:25.240 --> 00:57:26.680]   It's your blue apron box.
[00:57:26.680 --> 00:57:27.680]   Oh, I forgot.
[00:57:27.680 --> 00:57:29.840]   I got the blue apron box came.
[00:57:29.840 --> 00:57:36.720]   It's refrigerated inside delicious, fresh ingredients from local farms, sustainably
[00:57:36.720 --> 00:57:43.120]   sourced seafood or humanely raised beef, chicken, free-range chicken, naturally raised pork,
[00:57:43.120 --> 00:57:46.240]   regenerative farming practice produce.
[00:57:46.240 --> 00:57:49.400]   And it's refrigerated, never frozen.
[00:57:49.400 --> 00:57:54.280]   And each one has exactly the right ingredients and the right proportion to make the exact
[00:57:54.280 --> 00:57:55.280]   meal.
[00:57:55.280 --> 00:57:56.280]   So this is the beauty part.
[00:57:56.280 --> 00:57:57.280]   You get this card.
[00:57:57.280 --> 00:57:58.600]   It's a recipe in there.
[00:57:58.600 --> 00:58:02.000]   And you know, if it, if it called recipe calls for a clove of garlic, you get a clove
[00:58:02.000 --> 00:58:03.000]   of garlic.
[00:58:03.000 --> 00:58:06.320]   If it calls for three ounces of soy sauce, you get three ounces.
[00:58:06.320 --> 00:58:08.360]   So there's no waste.
[00:58:08.360 --> 00:58:10.960]   You always have exactly what you need.
[00:58:10.960 --> 00:58:11.960]   You're going to make meals.
[00:58:11.960 --> 00:58:15.400]   You just won't believe they're so good.
[00:58:15.400 --> 00:58:18.600]   And in a way, there are meals you probably wouldn't have tried except that you've got
[00:58:18.600 --> 00:58:19.600]   all the ingredients.
[00:58:19.600 --> 00:58:20.600]   You've got the card.
[00:58:20.600 --> 00:58:23.840]   There's even videos in the blue apron site if you have questions about a technique.
[00:58:23.840 --> 00:58:26.540]   And all this for less than $10 a serving.
[00:58:26.540 --> 00:58:30.200]   Each meal can be prepared in 40 minutes or less.
[00:58:30.200 --> 00:58:34.000]   They're roughly, you know, 200 to 400 calories are very helpful.
[00:58:34.000 --> 00:58:35.680]   They're delicious.
[00:58:35.680 --> 00:58:37.160]   They're delicious.
[00:58:37.160 --> 00:58:40.160]   And you're going to use ingredients perhaps you've never tried before.
[00:58:40.160 --> 00:58:41.520]   That's one of the things I love about blue apron.
[00:58:41.520 --> 00:58:42.520]   We found a new bok choy.
[00:58:42.520 --> 00:58:43.520]   I had never tried.
[00:58:43.520 --> 00:58:44.520]   And now it's all all eat.
[00:58:44.520 --> 00:58:45.520]   It's so good.
[00:58:45.520 --> 00:58:48.160]   Oh, heirloom tomatoes this time of year.
[00:58:48.160 --> 00:58:52.720]   We're getting a lot of those cooked meals like crispy cod and cabbage slaw tacos with
[00:58:52.720 --> 00:58:55.700]   pepita pineapple and avocado salsa.
[00:58:55.700 --> 00:59:00.300]   I don't know what pepita is, but after you do it, you'll know pan seared pork chops with
[00:59:00.300 --> 00:59:03.260]   two cheese mashed potatoes and sauteed spinach.
[00:59:03.260 --> 00:59:06.020]   That sounds like something Matthew would like.
[00:59:06.020 --> 00:59:10.300]   I don't know why I see you as a pan seared pork chop with two cheese mashed potatoes and
[00:59:10.300 --> 00:59:11.300]   sauteed.
[00:59:11.300 --> 00:59:12.300]   Totally.
[00:59:12.300 --> 00:59:13.300]   Yeah.
[00:59:13.300 --> 00:59:14.300]   Yeah, wouldn't you?
[00:59:14.300 --> 00:59:16.700]   How about I'm getting hungry just I know I hate these ads because I get my mouth starts
[00:59:16.700 --> 00:59:17.700]   watering.
[00:59:17.700 --> 00:59:23.520]   I got these muffleda style grilled cheese sandwiches with baby romaine and pistachio salad.
[00:59:23.520 --> 00:59:26.320]   See, this is like, this is like stuff you wouldn't think of.
[00:59:26.320 --> 00:59:29.920]   But once you've done it, now you know, they even show wine pairings.
[00:59:29.920 --> 00:59:30.920]   Great for date night.
[00:59:30.920 --> 00:59:35.280]   They have family plans too with with a kid friendly ingredients.
[00:59:35.280 --> 00:59:38.360]   What a great way to get your kids into cooking blue apron.com.
[00:59:38.360 --> 00:59:39.360]   Here's a deal.
[00:59:39.360 --> 00:59:41.640]   You go there right now blue apron.com/twit.
[00:59:41.640 --> 00:59:44.400]   You're going to get your first two meals free.
[00:59:44.400 --> 00:59:48.940]   So I like it when we do this because that way you can try without any risk.
[00:59:48.940 --> 00:59:51.980]   See if you like it blue apron.com/twit.
[00:59:51.980 --> 00:59:55.540]   They delivered 99% of the continent on the United States.
[00:59:55.540 --> 01:00:05.340]   Don't know if they go to Juneau or SAG way, but you could ask blue apron.com/twit.
[01:00:05.340 --> 01:00:10.820]   I think the only way to really do these things is for you to ship some of it to all of us.
[01:00:10.820 --> 01:00:11.820]   I will.
[01:00:11.820 --> 01:00:12.820]   And we will change it.
[01:00:12.820 --> 01:00:13.820]   I don't know the views.
[01:00:13.820 --> 01:00:14.820]   I don't know about Canada.
[01:00:14.820 --> 01:00:15.820]   What?
[01:00:15.820 --> 01:00:18.100]   Let me find out.
[01:00:18.100 --> 01:00:21.760]   It said continental you have in states, but Stacy, you're going to get a box.
[01:00:21.760 --> 01:00:24.080]   And Matthew, if I can get you a box, I would.
[01:00:24.080 --> 01:00:25.680]   Otherwise come to my house.
[01:00:25.680 --> 01:00:26.680]   I'll go.
[01:00:26.680 --> 01:00:27.680]   Okay.
[01:00:27.680 --> 01:00:28.680]   Come visit me, Matthew.
[01:00:28.680 --> 01:00:29.680]   Yeah, exactly.
[01:00:29.680 --> 01:00:30.680]   All right.
[01:00:30.680 --> 01:00:31.680]   Where I don't even know where you're located, Stacy.
[01:00:31.680 --> 01:00:32.680]   See, it's a big mystery.
[01:00:32.680 --> 01:00:33.680]   I'm in Austin, Texas.
[01:00:33.680 --> 01:00:34.680]   Austin.
[01:00:34.680 --> 01:00:35.680]   The internet.
[01:00:35.680 --> 01:00:36.680]   Austin lives on the internet.
[01:00:36.680 --> 01:00:39.000]   Oh, you've got Austin's got food heaven.
[01:00:39.000 --> 01:00:40.000]   You got barbecue.
[01:00:40.000 --> 01:00:41.520]   You got queso.
[01:00:41.520 --> 01:00:43.800]   I want to go there so that I can live on queso.
[01:00:43.800 --> 01:00:44.800]   Not for long.
[01:00:44.800 --> 01:00:45.800]   Not for long.
[01:00:45.800 --> 01:00:47.800]   I can live a short time.
[01:00:47.800 --> 01:00:49.340]   I can live on barbecue.
[01:00:49.340 --> 01:00:51.580]   What's your favorite?
[01:00:51.580 --> 01:00:53.420]   Which one is salt liquor?
[01:00:53.420 --> 01:00:55.420]   Oh, no.
[01:00:55.420 --> 01:00:56.980]   It depends on the type.
[01:00:56.980 --> 01:00:57.980]   Okay.
[01:00:57.980 --> 01:01:02.340]   So pork ribs, movers, beef, franklin's.
[01:01:02.340 --> 01:01:05.340]   You got to go to Franklin's, even though the line is super crazy long.
[01:01:05.340 --> 01:01:06.860]   Franklin's world famous.
[01:01:06.860 --> 01:01:07.860]   Yeah.
[01:01:07.860 --> 01:01:08.860]   Yeah.
[01:01:08.860 --> 01:01:12.780]   Who was it was in the line and was like some celebrity and I think it was like Jay-Z.
[01:01:12.780 --> 01:01:13.780]   Oh, yeah.
[01:01:13.780 --> 01:01:14.780]   That's right.
[01:01:14.780 --> 01:01:15.780]   Yeah.
[01:01:15.780 --> 01:01:16.780]   I think that's right.
[01:01:16.780 --> 01:01:17.780]   You're not going to make me wait in line.
[01:01:17.780 --> 01:01:18.780]   Is it?
[01:01:18.780 --> 01:01:19.780]   Yeah.
[01:01:19.780 --> 01:01:20.780]   What about the president?
[01:01:20.780 --> 01:01:22.700]   Did my brisket, sorry.
[01:01:22.700 --> 01:01:23.700]   Brisket is the beef.
[01:01:23.700 --> 01:01:24.700]   Sorry.
[01:01:24.700 --> 01:01:25.700]   Okay.
[01:01:25.700 --> 01:01:27.540]   And it's got to be you can't go with lean brisket.
[01:01:27.540 --> 01:01:28.540]   No, Jesus.
[01:01:28.540 --> 01:01:29.540]   No.
[01:01:29.540 --> 01:01:30.540]   It's got to be.
[01:01:30.540 --> 01:01:31.540]   It's a fat that makes it good.
[01:01:31.540 --> 01:01:32.540]   Makes it taste.
[01:01:32.540 --> 01:01:33.540]   Yeah.
[01:01:33.540 --> 01:01:34.540]   So, okay.
[01:01:34.540 --> 01:01:36.060]   So is iron horse in that list?
[01:01:36.060 --> 01:01:37.060]   No.
[01:01:37.060 --> 01:01:38.660]   None of the places I've been to.
[01:01:38.660 --> 01:01:39.900]   I heard horses good.
[01:01:39.900 --> 01:01:40.900]   It's not bad.
[01:01:40.900 --> 01:01:41.980]   And it's very authentic.
[01:01:41.980 --> 01:01:42.980]   I want to go.
[01:01:42.980 --> 01:01:44.860]   I'm going to go to Franklin next time while getting in line.
[01:01:44.860 --> 01:01:46.420]   So Stacey, I'm going to come.
[01:01:46.420 --> 01:01:49.020]   You could take me to one of those and then you can come.
[01:01:49.020 --> 01:01:51.620]   We'll go to smoke me play some Montreal.
[01:01:51.620 --> 01:01:54.300]   We'll have some fatty brisket.
[01:01:54.300 --> 01:01:56.700]   Oh, look at that brisket.
[01:01:56.700 --> 01:01:57.700]   So good.
[01:01:57.700 --> 01:01:58.700]   Oh, man.
[01:01:58.700 --> 01:01:59.700]   It's almost dinner time.
[01:01:59.700 --> 01:02:00.700]   That's amazing.
[01:02:00.700 --> 01:02:01.700]   That's fantastic.
[01:02:01.700 --> 01:02:02.700]   This is so mean.
[01:02:02.700 --> 01:02:03.700]   There's the line.
[01:02:03.700 --> 01:02:05.220]   I think we went to Franklin's.
[01:02:05.220 --> 01:02:06.900]   I think I remember waiting in this line.
[01:02:06.900 --> 01:02:07.900]   Yeah.
[01:02:07.900 --> 01:02:08.900]   Oh, man.
[01:02:08.900 --> 01:02:09.900]   Oh, man.
[01:02:09.900 --> 01:02:14.900]   Oh, that's so mean.
[01:02:14.900 --> 01:02:16.620]   That's just mean.
[01:02:16.620 --> 01:02:17.620]   We're all drooling now.
[01:02:17.620 --> 01:02:18.620]   I know.
[01:02:18.620 --> 01:02:19.620]   We've got some Montreal.
[01:02:19.620 --> 01:02:20.620]   You've got to go to the shorts.
[01:02:20.620 --> 01:02:23.420]   Shores is supposed to be great for the smoked meat and poutine.
[01:02:23.420 --> 01:02:24.420]   It's mind-boggling.
[01:02:24.420 --> 01:02:25.420]   Yeah.
[01:02:25.420 --> 01:02:26.420]   Smoked meat is mind-boggling.
[01:02:26.420 --> 01:02:27.420]   Let's go up.
[01:02:27.420 --> 01:02:29.220]   Let's maybe take this show on the road.
[01:02:29.220 --> 01:02:31.220]   Yeah, there we go.
[01:02:31.220 --> 01:02:34.620]   They've been running their briskets for like six weeks or something.
[01:02:34.620 --> 01:02:36.500]   Do you have a kid you said?
[01:02:36.500 --> 01:02:37.500]   This weekend barbecue.
[01:02:37.500 --> 01:02:40.940]   Yeah, I'll host that.
[01:02:40.940 --> 01:02:45.700]   You know, something we do at the makerspace every once in a while is we make our own bacon.
[01:02:45.700 --> 01:02:46.940]   Oh, that's fun.
[01:02:46.940 --> 01:02:47.940]   Yeah.
[01:02:47.940 --> 01:02:48.940]   Hey, you hear it?
[01:02:48.940 --> 01:02:49.940]   You smoke it.
[01:02:49.940 --> 01:02:51.740]   I just read a big article on salting.
[01:02:51.740 --> 01:02:52.740]   Yep.
[01:02:52.740 --> 01:02:53.740]   Yep.
[01:02:53.740 --> 01:02:54.740]   You got to cure it.
[01:02:54.740 --> 01:02:56.940]   Have you ever had the bacon explosion?
[01:02:56.940 --> 01:03:01.380]   Like, God, I don't know if I want this question.
[01:03:01.380 --> 01:03:06.700]   It's where you take sausage and you roll it out.
[01:03:06.700 --> 01:03:15.020]   You put bacon, more sausage meat bacon, then you roll it up and wrap it in bacon.
[01:03:15.020 --> 01:03:20.860]   So you've got basically a log of sausage meat with a woven wrap of bacon.
[01:03:20.860 --> 01:03:21.860]   Wow.
[01:03:21.860 --> 01:03:23.420]   And then barbecue.
[01:03:23.420 --> 01:03:25.260]   Does it explode or is that just?
[01:03:25.260 --> 01:03:27.620]   No, it's just so we launch bacon.
[01:03:27.620 --> 01:03:29.620]   So much bacon sausage.
[01:03:29.620 --> 01:03:31.620]   So then you explode.
[01:03:31.620 --> 01:03:33.540]   This is becoming the hipster rage now.
[01:03:33.540 --> 01:03:35.220]   This was the New York Times last week.
[01:03:35.220 --> 01:03:39.420]   Just to add salt, how to make bacon and pancetta at home.
[01:03:39.420 --> 01:03:40.420]   And--
[01:03:40.420 --> 01:03:42.580]   And pancetta is so good.
[01:03:42.580 --> 01:03:43.580]   Yes.
[01:03:43.580 --> 01:03:44.580]   Ah.
[01:03:44.580 --> 01:03:45.580]   Look at it.
[01:03:45.580 --> 01:03:48.180]   In this fireplace, prosciutto hanging--
[01:03:48.180 --> 01:03:49.180]   Yeah.
[01:03:49.180 --> 01:03:50.180]   [LAUGHTER]
[01:03:50.180 --> 01:03:54.820]   Stacey, you know, Kevin made his own--
[01:03:54.820 --> 01:03:56.740]   Oh, yeah.
[01:03:56.740 --> 01:03:57.740]   You can--
[01:03:57.740 --> 01:03:58.740]   I'm going to do this kitchen.
[01:03:58.740 --> 01:04:01.860]   Oh, Michael Rolman, I think.
[01:04:01.860 --> 01:04:04.540]   Is that the guy who does it?
[01:04:04.540 --> 01:04:09.500]   I think he has a bunch of instructions on how to do your own meat.
[01:04:09.500 --> 01:04:11.060]   But anyway, cure your own meat.
[01:04:11.060 --> 01:04:12.060]   That was a big thing.
[01:04:12.060 --> 01:04:13.060]   Yeah.
[01:04:13.060 --> 01:04:16.660]   Well, this article in the New York Times is going to bring it all back.
[01:04:16.660 --> 01:04:17.660]   This was last week.
[01:04:17.660 --> 01:04:20.340]   And I think they recommended some--
[01:04:20.340 --> 01:04:26.140]   charcut-- oh, this is the book-- charcuterie, the crown of salting, smoking, and curing.
[01:04:26.140 --> 01:04:27.140]   Mm-hmm.
[01:04:27.140 --> 01:04:28.140]   Mm-hmm.
[01:04:28.140 --> 01:04:29.140]   Mm-hmm.
[01:04:29.140 --> 01:04:30.540]   Michael Rolman, very good.
[01:04:30.540 --> 01:04:31.540]   There you go.
[01:04:31.540 --> 01:04:32.540]   Yeah.
[01:04:32.540 --> 01:04:37.580]   My friend Kevin Fitchard, who used to be a chef before he became a tech reporter, and
[01:04:37.580 --> 01:04:42.300]   now he's like an analyst or marketing person somewhere.
[01:04:42.300 --> 01:04:44.500]   He actually cooked his way through that book.
[01:04:44.500 --> 01:04:46.820]   Oh, is he dead now?
[01:04:46.820 --> 01:04:47.820]   No.
[01:04:47.820 --> 01:04:48.820]   [LAUGHTER]
[01:04:48.820 --> 01:04:49.820]   No.
[01:04:49.820 --> 01:04:52.460]   He knows a lot of good things, though.
[01:04:52.460 --> 01:04:55.340]   This would be so dangerous for me to buy this book.
[01:04:55.340 --> 01:04:56.940]   I'm so tempted, but--
[01:04:56.940 --> 01:04:57.940]   No.
[01:04:57.940 --> 01:05:01.340]   I think I need a book about salting.
[01:05:01.340 --> 01:05:02.940]   You could test some technology.
[01:05:02.940 --> 01:05:05.380]   There's like digital thermometers you need, timers.
[01:05:05.380 --> 01:05:07.620]   You could set up all kinds of stuff.
[01:05:07.620 --> 01:05:08.620]   True.
[01:05:08.620 --> 01:05:09.620]   [SNIFFS]
[01:05:09.620 --> 01:05:11.780]   There's got to be a connected smoker.
[01:05:11.780 --> 01:05:12.780]   There has to be one.
[01:05:12.780 --> 01:05:13.780]   Oh, no.
[01:05:13.780 --> 01:05:14.780]   That's a perfect one.
[01:05:14.780 --> 01:05:17.900]   Yeah, one of the-- one of the guys-- I think you can order products online, actually.
[01:05:17.900 --> 01:05:23.180]   One of the guys that comes to our maker space built his own controller for barbecue, specifically
[01:05:23.180 --> 01:05:24.180]   for barbecue.
[01:05:24.180 --> 01:05:25.180]   I hear that, so--
[01:05:25.180 --> 01:05:30.180]   Because you hook this thing up, and it actually-- it manually adjusts the heat to make sure
[01:05:30.180 --> 01:05:31.620]   that you're getting a consistent temperature.
[01:05:31.620 --> 01:05:33.820]   You don't have to come back and babysit it.
[01:05:33.820 --> 01:05:34.820]   Nice.
[01:05:34.820 --> 01:05:35.820]   Yeah.
[01:05:35.820 --> 01:05:36.820]   It's like a sous vide for barbecuing.
[01:05:36.820 --> 01:05:37.820]   Yeah.
[01:05:37.820 --> 01:05:40.180]   Yeah, Charbroil apparently launched a Wi-Fi connected smoker.
[01:05:40.180 --> 01:05:41.180]   Yeah.
[01:05:41.180 --> 01:05:44.060]   You guys are so awful.
[01:05:44.060 --> 01:05:47.620]   By the way, I hear that that's one of the modules a project era is going to offer.
[01:05:47.620 --> 01:05:48.620]   Mm-hmm.
[01:05:48.620 --> 01:05:49.620]   Is the--
[01:05:49.620 --> 01:05:50.620]   Yes.
[01:05:50.620 --> 01:05:51.620]   --spoker module.
[01:05:51.620 --> 01:05:52.620]   No, I'm lying.
[01:05:52.620 --> 01:05:53.620]   I'm just trying to get this back on track.
[01:05:53.620 --> 01:05:54.620]   We can build one of those with my project.
[01:05:54.620 --> 01:05:56.020]   I have to go get something to eat.
[01:05:56.020 --> 01:05:57.020]   [LAUGHTER]
[01:05:57.020 --> 01:05:58.020]   Really?
[01:05:58.020 --> 01:05:59.020]   I'm super excited about it.
[01:05:59.020 --> 01:06:00.020]   No, no, I'm just kidding.
[01:06:00.020 --> 01:06:01.740]   I am less than excited about RS.
[01:06:01.740 --> 01:06:02.900]   So you're going to have to convince me.
[01:06:02.900 --> 01:06:03.900]   The idea is--
[01:06:03.900 --> 01:06:04.900]   Sure.
[01:06:04.900 --> 01:06:08.940]   --and they did decide not to have the phone modules be one of the modules.
[01:06:08.940 --> 01:06:10.780]   They'll be add-on modules.
[01:06:10.780 --> 01:06:15.380]   It just feels like something that's going to be fall apart in my hand.
[01:06:15.380 --> 01:06:17.700]   Tell me why you're excited about this, Stacy.
[01:06:17.700 --> 01:06:23.700]   I'm excited because I love having everything on my smartphone, but I'm also sad because
[01:06:23.700 --> 01:06:27.940]   like the camera quality isn't as good or you want to-- like the idea of swapping stuff
[01:06:27.940 --> 01:06:30.180]   out and putting like a glucose monitor or--
[01:06:30.180 --> 01:06:31.180]   Right.
[01:06:31.180 --> 01:06:33.060]   --all of that's super exciting to me.
[01:06:33.060 --> 01:06:37.020]   And maybe that's just because I'm the type of person who wants to have like lots of crazy
[01:06:37.020 --> 01:06:38.020]   gadgets.
[01:06:38.020 --> 01:06:41.700]   Well, I-- you know, actually it probably would save me money because I just buy new
[01:06:41.700 --> 01:06:42.860]   phones all the time.
[01:06:42.860 --> 01:06:44.940]   This way I just buy a new module, right?
[01:06:44.940 --> 01:06:46.740]   Yeah, I have a budget.
[01:06:46.740 --> 01:06:47.740]   [LAUGHTER]
[01:06:47.740 --> 01:06:54.980]   I'm going to go out on a limb and say no one is ever going to use these phones.
[01:06:54.980 --> 01:06:57.220]   I mean, not no one, but--
[01:06:57.220 --> 01:06:58.220]   So this was a--
[01:06:58.220 --> 01:06:59.580]   This is specifically speaking, no one.
[01:06:59.580 --> 01:07:04.940]   Part of the Advanced Technology Group that Google acquired when they bought Motorola,
[01:07:04.940 --> 01:07:12.460]   when they sold Motorola Lenovo, they kept Project R around and they talked about it
[01:07:12.460 --> 01:07:16.540]   at Google I/O this year.
[01:07:16.540 --> 01:07:19.340]   For a while they were saying we're going to sell it in Puerto Rico, I thought that was
[01:07:19.340 --> 01:07:21.140]   like a month's ago.
[01:07:21.140 --> 01:07:22.140]   They never did.
[01:07:22.140 --> 01:07:23.140]   So--
[01:07:23.140 --> 01:07:25.580]   So does it appear to anywhere?
[01:07:25.580 --> 01:07:28.100]   Are there any units that people are actually using?
[01:07:28.100 --> 01:07:29.100]   Good question.
[01:07:29.100 --> 01:07:30.100]   Are there?
[01:07:30.100 --> 01:07:31.100]   I don't know.
[01:07:31.100 --> 01:07:32.100]   I think developers are getting theirs.
[01:07:32.100 --> 01:07:39.580]   It feels like a great project with a great rationale that I really want to have happened
[01:07:39.580 --> 01:07:41.540]   that's never-- that's not going to happen.
[01:07:41.540 --> 01:07:42.540]   Yeah.
[01:07:42.540 --> 01:07:45.300]   Well, wait, because they brought-- because Rick, is it Osterlo?
[01:07:45.300 --> 01:07:46.300]   I didn't--
[01:07:46.300 --> 01:07:47.300]   Osterlo came back.
[01:07:47.300 --> 01:07:50.100]   Yeah, he's going to be in charge of hardware now at Google, which is good news.
[01:07:50.100 --> 01:07:51.100]   He's great.
[01:07:51.100 --> 01:07:53.460]   He's associated with this project too.
[01:07:53.460 --> 01:07:57.300]   So this is a smart man who actually knows how to build hardware.
[01:07:57.300 --> 01:08:01.940]   So maybe that's why I'm like, this could be real.
[01:08:01.940 --> 01:08:03.580]   I'm with both of you.
[01:08:03.580 --> 01:08:04.740]   I'm going to agree with both of you.
[01:08:04.740 --> 01:08:09.220]   And the one hand, I'm thrilled to see innovation in smartphones because we're kind of at peak
[01:08:09.220 --> 01:08:10.660]   smartphone right now, right?
[01:08:10.660 --> 01:08:12.860]   There's not-- nobody's thinking of anything new to do.
[01:08:12.860 --> 01:08:13.860]   Right.
[01:08:13.860 --> 01:08:15.300]   We're just more slabs of glass.
[01:08:15.300 --> 01:08:19.260]   But I also agree with Matthew that-- and we have seen modular stuff before.
[01:08:19.260 --> 01:08:20.420]   Remember the visor?
[01:08:20.420 --> 01:08:21.420]   Mm-hmm.
[01:08:21.420 --> 01:08:25.740]   With the visor modules or whatever they call those.
[01:08:25.740 --> 01:08:27.540]   It seems a little geeky.
[01:08:27.540 --> 01:08:29.580]   It also seems like it's kind of ugly.
[01:08:29.580 --> 01:08:30.580]   Right.
[01:08:30.580 --> 01:08:31.580]   [LAUGHS]
[01:08:31.580 --> 01:08:35.580]   I mean, it's the same people who buy a Raspberry Pi might buy it.
[01:08:35.580 --> 01:08:36.580]   Yeah.
[01:08:36.580 --> 01:08:38.900]   So I think it's going to be an interesting niche.
[01:08:38.900 --> 01:08:43.380]   The thing that I'm hopeful for is that maybe things will come out of it.
[01:08:43.380 --> 01:08:46.620]   So lots of people won't use these phones.
[01:08:46.620 --> 01:08:52.060]   Maybe things will come out of the-- as they're developed or people come up with new modules,
[01:08:52.060 --> 01:08:57.740]   maybe we'll find interesting things that can go beyond just this and actually have an
[01:08:57.740 --> 01:09:00.740]   impact on real phones that regular people use.
[01:09:00.740 --> 01:09:04.100]   I see LG G5 selling because that's kind of modular.
[01:09:04.100 --> 01:09:09.580]   I mean, you have a battery module, you have an audio module, and you have a camera module.
[01:09:09.580 --> 01:09:11.220]   Is that taken the world by storm?
[01:09:11.220 --> 01:09:13.660]   I don't think so.
[01:09:13.660 --> 01:09:14.660]   People don't want more complexity.
[01:09:14.660 --> 01:09:15.660]   Not that I know of.
[01:09:15.660 --> 01:09:16.660]   I just want that.
[01:09:16.660 --> 01:09:20.660]   Well, and the problem-- like, I'm like, you, Leo, when I watched the ad, half of me is
[01:09:20.660 --> 01:09:23.060]   thinking, wow, that looks really awesome.
[01:09:23.060 --> 01:09:27.580]   And half of me is thinking, I'm going to leave the module on one and my other pants,
[01:09:27.580 --> 01:09:29.380]   or it's going to fall out of my shirt pocket.
[01:09:29.380 --> 01:09:30.380]   It's going to fall apart with your hand.
[01:09:30.380 --> 01:09:33.740]   And then I'll have to fall in with a hole in it that doesn't do anything.
[01:09:33.740 --> 01:09:34.740]   [LAUGHS]
[01:09:34.740 --> 01:09:35.740]   [LAUGHS]
[01:09:35.740 --> 01:09:39.580]   It was one thing back in the day when you couldn't make a choice.
[01:09:39.580 --> 01:09:41.620]   I'm thinking, like, computers, things like that.
[01:09:41.620 --> 01:09:45.620]   I mean, you had to choose, do I want the network card, or do I want the-- whatever
[01:09:45.620 --> 01:09:47.180]   it was at the time, the fire wire card.
[01:09:47.180 --> 01:09:48.180]   Because I don't have room for both.
[01:09:48.180 --> 01:09:50.580]   So it has to be modular, because I have to put one thing into the other.
[01:09:50.580 --> 01:09:51.580]   Right.
[01:09:51.580 --> 01:09:52.580]   But now phones have everything.
[01:09:52.580 --> 01:09:53.580]   There's everything in the world.
[01:09:53.580 --> 01:09:54.580]   And computers have everything.
[01:09:54.580 --> 01:09:55.580]   Yeah.
[01:09:55.580 --> 01:09:57.580]   We were in the position where you could put the slots.
[01:09:57.580 --> 01:09:58.580]   Yep.
[01:09:58.580 --> 01:09:59.580]   You had to go with different cards.
[01:09:59.580 --> 01:10:02.060]   And there were standins so that you could make a card.
[01:10:02.060 --> 01:10:03.060]   Yeah.
[01:10:03.060 --> 01:10:07.860]   And that's-- I mean, I guess you could still buy a tower case with an ATX bus or something.
[01:10:07.860 --> 01:10:08.860]   Yeah.
[01:10:08.860 --> 01:10:09.860]   For gamers.
[01:10:09.860 --> 01:10:10.860]   It's for gamers.
[01:10:10.860 --> 01:10:11.860]   It's not--
[01:10:11.860 --> 01:10:12.860]   For gamers, right.
[01:10:12.860 --> 01:10:13.860]   Or 3D designers.
[01:10:13.860 --> 01:10:16.580]   I think most people buy a computer completely un-evident.
[01:10:16.580 --> 01:10:18.340]   And they never do anything to it.
[01:10:18.340 --> 01:10:19.340]   Right.
[01:10:19.340 --> 01:10:20.340]   Yeah.
[01:10:20.340 --> 01:10:23.380]   What if you get something cool, like the new me, OK?
[01:10:23.380 --> 01:10:29.140]   So if you're familiar with the new me sensor concept, they're building a sensor for glucose
[01:10:29.140 --> 01:10:30.220]   monitoring.
[01:10:30.220 --> 01:10:33.460]   And this is its own little thingy.
[01:10:33.460 --> 01:10:34.460]   It's a little triangle shape.
[01:10:34.460 --> 01:10:36.420]   Again, you stick your food in it.
[01:10:36.420 --> 01:10:41.140]   But imagine if you have these-- imagine if you could put that in a phone, and then you
[01:10:41.140 --> 01:10:45.100]   give that to your kid when they go out to school, or you put it in their phone for
[01:10:45.100 --> 01:10:48.860]   them so they can test their food for whatever they're allergic to.
[01:10:48.860 --> 01:10:56.220]   I just think it makes an interesting platform to test out some of the cool stuff we're developing
[01:10:56.220 --> 01:11:00.700]   with advanced sensors and, I don't know, other technology.
[01:11:00.700 --> 01:11:05.420]   I love the idea of a science lab in a box with all the sensors.
[01:11:05.420 --> 01:11:10.180]   And you could-- I mean, you'd be like Mr. Spock or Bones or something.
[01:11:10.180 --> 01:11:11.540]   This is the NEMA.
[01:11:11.540 --> 01:11:12.540]   This is an NIMA.
[01:11:12.540 --> 01:11:13.540]   Oh, yeah.
[01:11:13.540 --> 01:11:16.220]   We actually demoed this on the new screen savers.
[01:11:16.220 --> 01:11:19.020]   Scott Jung, our medical guy, had it.
[01:11:19.020 --> 01:11:21.740]   The one he had could detect gluten.
[01:11:21.740 --> 01:11:24.660]   Gluten was the only one-- everyone's talking about peanuts, but it doesn't do that yet,
[01:11:24.660 --> 01:11:25.660]   I don't think.
[01:11:25.660 --> 01:11:26.660]   Yeah.
[01:11:26.660 --> 01:11:30.020]   But why couldn't you just have that connect to your phone through some other means?
[01:11:30.020 --> 01:11:31.860]   Like why have a module?
[01:11:31.860 --> 01:11:33.340]   Why not just have a connect to your phone?
[01:11:33.340 --> 01:11:34.340]   Well, exactly.
[01:11:34.340 --> 01:11:35.340]   So that's kind of what a project are.
[01:11:35.340 --> 01:11:37.900]   We might have a new NEMA module, right?
[01:11:37.900 --> 01:11:43.180]   Yeah, I can't-- I cannot think of anything more disgusting than sitting with somebody who
[01:11:43.180 --> 01:11:46.220]   takes a bite of food off their plate and shows it on their phone.
[01:11:46.220 --> 01:11:47.220]   [LAUGHTER]
[01:11:47.220 --> 01:11:50.020]   That's really, really weird.
[01:11:50.020 --> 01:11:54.140]   So it kind of reminded me, I don't know if you guys ever saw these.
[01:11:54.140 --> 01:11:56.820]   They were Lapka, Lapka.
[01:11:56.820 --> 01:11:58.780]   Airbnb actually bought the company.
[01:11:58.780 --> 01:12:01.220]   There's these little-- and I think I threw mine away.
[01:12:01.220 --> 01:12:04.820]   It was these highly designed little sensor cubes.
[01:12:04.820 --> 01:12:11.700]   They tested for-- like, pesticides in food.
[01:12:11.700 --> 01:12:16.700]   There was one for testing Wi-Fi and one for-- I don't remember.
[01:12:16.700 --> 01:12:20.780]   And you literally-- they had prongs and you would stick the food one into an apple or
[01:12:20.780 --> 01:12:22.460]   something and it would test that.
[01:12:22.460 --> 01:12:26.220]   And so what I saw the aura, that's immediately what I thought of because these were designed
[01:12:26.220 --> 01:12:27.540]   to be-- there they are.
[01:12:27.540 --> 01:12:28.540]   Yeah.
[01:12:28.540 --> 01:12:29.540]   Right, wait, wait, wait.
[01:12:29.540 --> 01:12:30.540]   Airbnb bought this company.
[01:12:30.540 --> 01:12:31.540]   Yeah, I don't know why.
[01:12:31.540 --> 01:12:32.540]   That's so weird.
[01:12:32.540 --> 01:12:35.020]   It's so weird.
[01:12:35.020 --> 01:12:36.020]   Why did they buy--
[01:12:36.020 --> 01:12:37.020]   What the--
[01:12:37.020 --> 01:12:38.020]   [LAUGHTER]
[01:12:38.020 --> 01:12:40.540]   They said-- it's an acquire.
[01:12:40.540 --> 01:12:41.540]   They bought the team.
[01:12:41.540 --> 01:12:43.260]   Oh, the aqua, I think.
[01:12:43.260 --> 01:12:44.260]   Yeah.
[01:12:44.260 --> 01:12:45.420]   Think about like, yeah, crazy.
[01:12:45.420 --> 01:12:46.420]   This was so fetishistic.
[01:12:46.420 --> 01:12:47.420]   This is like--
[01:12:47.420 --> 01:12:48.700]   It also was-- it had wood, right?
[01:12:48.700 --> 01:12:49.700]   I mean, it was like--
[01:12:49.700 --> 01:12:50.700]   Yeah.
[01:12:50.700 --> 01:12:51.700]   Yeah.
[01:12:51.700 --> 01:12:52.700]   Weird.
[01:12:52.700 --> 01:12:53.900]   Concept sensor nodes.
[01:12:53.900 --> 01:12:57.060]   They were created, by the way, for the-- for Project Aura originally.
[01:12:57.060 --> 01:12:58.060]   Were they?
[01:12:58.060 --> 01:12:59.060]   Yeah.
[01:12:59.060 --> 01:13:00.620]   Oh, no, what are they reminding me of them?
[01:13:00.620 --> 01:13:01.620]   Yeah.
[01:13:01.620 --> 01:13:03.740]   They were wooden and ceramic blocks.
[01:13:03.740 --> 01:13:05.660]   Wooden and radiation.
[01:13:05.660 --> 01:13:07.420]   Humidity.
[01:13:07.420 --> 01:13:08.660]   Wow.
[01:13:08.660 --> 01:13:09.660]   Airbnb says--
[01:13:09.660 --> 01:13:10.820]   It's really clever.
[01:13:10.820 --> 01:13:11.740]   Yeah.
[01:13:11.740 --> 01:13:12.740]   It's Russian.
[01:13:12.740 --> 01:13:14.340]   Like, a really clever, well-designed thing
[01:13:14.340 --> 01:13:16.100]   that no one will ever use.
[01:13:16.100 --> 01:13:17.100]   Right.
[01:13:17.100 --> 01:13:20.380]   So maybe that's what Aura is, and that's why it's bad that it
[01:13:20.380 --> 01:13:21.380]   reminds me of.
[01:13:21.380 --> 01:13:23.020]   So it's like a Shin-du-goo.
[01:13:23.020 --> 01:13:24.980]   Remember that, Leo?
[01:13:24.980 --> 01:13:25.780]   Yeah, Shin-du-goo.
[01:13:25.780 --> 01:13:29.740]   We actually now have people in our chat room named Shin-du-goo.
[01:13:29.740 --> 01:13:32.620]   And didn't we do-- like, we did a whole Shin-du-goo
[01:13:32.620 --> 01:13:35.740]   for iPad episode, the--
[01:13:35.740 --> 01:13:38.220]   something that is not exactly useful, but somehow not
[01:13:38.220 --> 01:13:39.260]   altogether useless.
[01:13:39.260 --> 01:13:42.980]   Like, the example Matthew gave is of the helmet with a suction
[01:13:42.980 --> 01:13:45.380]   cup on the back, so when you're on the Tokyo subway,
[01:13:45.380 --> 01:13:47.980]   you just glue yourself to the window.
[01:13:47.980 --> 01:13:48.700]   So you don't--
[01:13:48.700 --> 01:13:49.860]   Your head doesn't--
[01:13:49.860 --> 01:13:52.900]   You fall over when you fall asleep.
[01:13:52.900 --> 01:13:54.940]   Yeah, those are Shin-du-goo, I would say.
[01:13:54.940 --> 01:13:56.220]   Those-- those--
[01:13:56.220 --> 01:13:56.980]   That's sense of--
[01:13:56.980 --> 01:13:58.740]   They're beautiful and kind of useful,
[01:13:58.740 --> 01:14:00.700]   but also unnecessary.
[01:14:00.700 --> 01:14:01.780]   Kind of?
[01:14:01.780 --> 01:14:04.860]   Like, do you really need to know what level of radiation
[01:14:04.860 --> 01:14:06.500]   there is in the--
[01:14:06.500 --> 01:14:07.860]   If you were Japanese, you might.
[01:14:07.860 --> 01:14:08.340]   You might.
[01:14:08.340 --> 01:14:08.860]   Yeah, you might.
[01:14:08.860 --> 01:14:09.360]   Yeah.
[01:14:09.360 --> 01:14:10.180]   You're a Fukushima.
[01:14:10.180 --> 01:14:12.460]   And you would want a wood and ceramic.
[01:14:12.460 --> 01:14:14.700]   Beautiful device to do that.
[01:14:14.700 --> 01:14:16.460]   Or a Geiger counter, whichever.
[01:14:16.460 --> 01:14:18.620]   [LAUGHTER]
[01:14:18.620 --> 01:14:20.700]   Oh, we're going to have fun with you.
[01:14:20.700 --> 01:14:23.940]   There's a new module right there.
[01:14:23.940 --> 01:14:26.860]   I just have to point out that Stacey--
[01:14:26.860 --> 01:14:30.140]   I can't think of a single gadget of any kind
[01:14:30.140 --> 01:14:33.660]   that Stacey has not wanted and/or loved.
[01:14:33.660 --> 01:14:34.500]   Because--
[01:14:34.500 --> 01:14:35.940]   Is that unfair, Stacey?
[01:14:35.940 --> 01:14:37.940]   No, I have-- I actually brought on the show
[01:14:37.940 --> 01:14:40.940]   a non-working gadget to warn people about it.
[01:14:40.940 --> 01:14:42.460]   Oh, you're going to show that later.
[01:14:42.460 --> 01:14:43.580]   That'll be your pick of the week.
[01:14:43.580 --> 01:14:44.380]   Yeah, that's--
[01:14:44.380 --> 01:14:46.860]   Well, I was going to ask the rules for that.
[01:14:46.860 --> 01:14:47.060]   Because--
[01:14:47.060 --> 01:14:48.340]   There's no rules.
[01:14:48.340 --> 01:14:48.900]   OK.
[01:14:48.900 --> 01:14:51.660]   You get to make your own rules.
[01:14:51.660 --> 01:14:52.660]   We have a contributor--
[01:14:52.660 --> 01:14:53.660]   No rules.
[01:14:53.660 --> 01:14:55.820]   --that brings a heavy metal album every time.
[01:14:55.820 --> 01:14:57.740]   [LAUGHTER]
[01:14:57.740 --> 01:14:58.900]   So--
[01:14:58.900 --> 01:14:59.420]   What--
[01:14:59.420 --> 01:15:01.220]   It's just-- you know what?
[01:15:01.220 --> 01:15:03.220]   I think our audience, one of their favorite parts
[01:15:03.220 --> 01:15:06.740]   of all the shows is like the picks of the week.
[01:15:06.740 --> 01:15:08.100]   I call it the back of the book.
[01:15:08.100 --> 01:15:09.660]   Because remember in magazines, it always
[01:15:09.660 --> 01:15:12.180]   have the weird columns in the back.
[01:15:12.180 --> 01:15:13.900]   Like, you know, the weird stuff.
[01:15:13.900 --> 01:15:16.020]   This is-- that's our weird stuff.
[01:15:16.020 --> 01:15:18.660]   Well, I thought you meant the ads for the C monkeys.
[01:15:18.660 --> 01:15:23.380]   Yeah, C monkeys, X-ray glasses, and chindulos.
[01:15:23.380 --> 01:15:27.060]   So closing arguments have been presented.
[01:15:27.060 --> 01:15:30.420]   And as far as I know, the jury's still out, right?
[01:15:30.420 --> 01:15:32.020]   Google versus Oracle.
[01:15:32.020 --> 01:15:33.380]   Yeah.
[01:15:33.380 --> 01:15:33.820]   Oh, yeah.
[01:15:33.820 --> 01:15:36.260]   They thought they'd come in sooner than this, I think.
[01:15:36.260 --> 01:15:38.420]   We've had somebody on yesterday on TNT,
[01:15:38.420 --> 01:15:40.540]   so we should know soon.
[01:15:40.540 --> 01:15:44.220]   So closing remarks were Monday.
[01:15:44.220 --> 01:15:48.980]   Of course, Oracle is suing Google for $9.3 billion,
[01:15:48.980 --> 01:15:53.340]   saying they violated-- this is so weird.
[01:15:53.340 --> 01:15:57.500]   Oracle's copyright of the Java API.
[01:15:57.500 --> 01:16:00.260]   No, they didn't copy the code.
[01:16:00.260 --> 01:16:06.660]   No, they copied-- they used the copyrighted APIs.
[01:16:06.660 --> 01:16:11.140]   And the jury of 10 people with no technical experience
[01:16:11.140 --> 01:16:14.420]   are now challenged to figure out--
[01:16:14.420 --> 01:16:14.940]   Yeah.
[01:16:14.940 --> 01:16:15.380]   --what.
[01:16:15.380 --> 01:16:19.180]   It's just the worst possible combination of things.
[01:16:19.180 --> 01:16:20.820]   But it's a fair use case.
[01:16:20.820 --> 01:16:21.460]   That's the funny thing.
[01:16:21.460 --> 01:16:22.700]   It's not a technology case.
[01:16:22.700 --> 01:16:24.660]   It's a fair use case.
[01:16:24.660 --> 01:16:27.660]   So Google's defense is, oh, yeah, we
[01:16:27.660 --> 01:16:30.620]   used those APIs to make Android.
[01:16:30.620 --> 01:16:33.700]   But it was fair use, because it was a transformative work,
[01:16:33.700 --> 01:16:36.700]   and it didn't cost Oracle any money.
[01:16:36.700 --> 01:16:37.220]   Right.
[01:16:37.220 --> 01:16:39.140]   It wasn't even Oracle at the time.
[01:16:39.140 --> 01:16:39.620]   It was a sign.
[01:16:39.620 --> 01:16:40.140]   They would give it away.
[01:16:40.140 --> 01:16:41.540]   They would give it a great case.
[01:16:41.540 --> 01:16:44.100]   I mean, if you look at the other cases that Google has
[01:16:44.100 --> 01:16:48.380]   won unfair use, they've been way more in a gray area than this.
[01:16:48.380 --> 01:16:51.620]   They've been-- if you look at the perfect 10,
[01:16:51.620 --> 01:16:56.820]   I think it was the thumbnails that Google used for photos.
[01:16:56.820 --> 01:16:59.060]   The court ruled that that was transformative simply
[01:16:59.060 --> 01:17:02.060]   because it allowed people to search them in a way that
[01:17:02.060 --> 01:17:03.500]   wasn't possible before.
[01:17:03.500 --> 01:17:04.180]   I don't know.
[01:17:04.180 --> 01:17:08.220]   You clearly haven't seen Oracle's slide presented to the jury
[01:17:08.220 --> 01:17:12.620]   in the closing arguments showing the scale heavily
[01:17:12.620 --> 01:17:14.020]   balanced in Oracle's favor.
[01:17:14.020 --> 01:17:15.860]   OK, I'm convinced you're right.
[01:17:15.860 --> 01:17:16.860]   This is hilarious.
[01:17:16.860 --> 01:17:19.660]   This is what you show people who are not technical.
[01:17:19.660 --> 01:17:22.220]   Unlike leans to one side, I'm totally convinced.
[01:17:22.220 --> 01:17:23.820]   I used to work in Oracle marketing,
[01:17:23.820 --> 01:17:25.700]   and this came right out of marketing.
[01:17:25.700 --> 01:17:27.540]   It wasn't the legal department that put this together.
[01:17:27.540 --> 01:17:27.860]   Yeah.
[01:17:27.860 --> 01:17:29.220]   No, it was the marketing.
[01:17:29.220 --> 01:17:30.860]   It's a joke.
[01:17:30.860 --> 01:17:33.980]   My fear is this whole case is kind of a train wreck.
[01:17:33.980 --> 01:17:36.340]   You've got a jury that doesn't know anything
[01:17:36.340 --> 01:17:40.340]   about APIs or technology, and/or fair use,
[01:17:40.340 --> 01:17:43.780]   and they're trying to determine whether these things that they
[01:17:43.780 --> 01:17:48.100]   don't understand qualify under some clause and copyright
[01:17:48.100 --> 01:17:50.940]   that they likewise don't understand.
[01:17:50.940 --> 01:17:54.220]   So there is no hope of a ruling that makes any sense.
[01:17:54.220 --> 01:17:55.660]   Well, it was interesting that they brought in--
[01:17:55.660 --> 01:17:56.100]   That's what all cases are.
[01:17:56.100 --> 01:17:58.180]   It was interesting that they brought in Jonathan Schwartz,
[01:17:58.180 --> 01:17:59.980]   too, to testify.
[01:17:59.980 --> 01:18:02.740]   Form the last CEO at Sun, who sold it to Oracle.
[01:18:02.740 --> 01:18:05.980]   And he just had a big deal on Google's side,
[01:18:05.980 --> 01:18:08.780]   on Google's behalf, which is really, really interesting.
[01:18:08.780 --> 01:18:12.100]   This was actually a very big topic when I was at Sun.
[01:18:12.100 --> 01:18:13.500]   I can't believe Google's doing this.
[01:18:13.500 --> 01:18:14.540]   They're stomping all over us.
[01:18:14.540 --> 01:18:15.060]   Blah, blah, blah.
[01:18:15.060 --> 01:18:16.740]   This was the cafeteria talk, right?
[01:18:16.740 --> 01:18:17.500]   James Gosling.
[01:18:17.500 --> 01:18:18.500]   We had him on the show.
[01:18:18.500 --> 01:18:20.460]   James Gosling, who wrote Java, said, yeah,
[01:18:20.460 --> 01:18:22.380]   we thought it was slimy.
[01:18:22.380 --> 01:18:25.140]   What is it, the nine lines of code, which
[01:18:25.140 --> 01:18:28.180]   was really array-bounds checking, which anybody could write.
[01:18:28.180 --> 01:18:29.300]   Probably would look pretty similar.
[01:18:29.300 --> 01:18:33.020]   But anyway, Gosling said, no, we looked at Google's Java code,
[01:18:33.020 --> 01:18:36.220]   and we felt like it looked a lot like what we wrote.
[01:18:36.220 --> 01:18:43.260]   But neither Jonathan or Gosling thought it was so worth.
[01:18:43.260 --> 01:18:44.380]   That's exactly the point.
[01:18:44.380 --> 01:18:45.220]   That's exactly the point.
[01:18:45.220 --> 01:18:47.980]   Everybody at Sun at the time was very disappointed
[01:18:47.980 --> 01:18:50.340]   that we weren't going to get any money out of this, basically.
[01:18:50.340 --> 01:18:53.500]   But they chose not to sue them.
[01:18:53.500 --> 01:18:56.900]   They felt like they didn't have a leg to stand on in terms
[01:18:56.900 --> 01:18:58.220]   of taking this to court.
[01:18:58.220 --> 01:19:00.180]   And then, of course, Oracle buys Sun,
[01:19:00.180 --> 01:19:02.500]   and the attitude is different at Oracle.
[01:19:02.500 --> 01:19:06.100]   Gosling said he was explaining Java to Oracle,
[01:19:06.100 --> 01:19:09.780]   and he saw the Oracle lawyers' eyes light up.
[01:19:09.780 --> 01:19:13.780]   So it may well have been part of the reason Oracle bought Sun,
[01:19:13.780 --> 01:19:16.100]   as they saw some IP that they maybe could go after Google.
[01:19:16.100 --> 01:19:17.220]   Oh, absolutely was.
[01:19:17.220 --> 01:19:21.500]   But the other thing is that if they win this judgment,
[01:19:21.500 --> 01:19:26.140]   that's $2 billion more than they paid for Sun microsystems.
[01:19:26.140 --> 01:19:28.780]   And Java's just one small part of what they got from Sun.
[01:19:28.780 --> 01:19:30.260]   So it's very interesting.
[01:19:30.260 --> 01:19:34.020]   The judge has already ruled, believe it or not, in Google's favor.
[01:19:34.020 --> 01:19:39.700]   The judge said, no, you can't copyright APIs.
[01:19:39.700 --> 01:19:42.140]   And he believed that Google had done this
[01:19:42.140 --> 01:19:44.140]   in a clean room environment.
[01:19:44.140 --> 01:19:45.940]   Of course, Oracle appealed.
[01:19:45.940 --> 01:19:48.060]   And it didn't go to where it would have normally gone,
[01:19:48.060 --> 01:19:50.140]   which is the Ninth Circuit Court of Appeals, who might well
[01:19:50.140 --> 01:19:51.860]   have continued to rule in Google's favor.
[01:19:51.860 --> 01:19:53.700]   But instead, it went to the federal court
[01:19:53.700 --> 01:19:56.220]   because it involved patent law.
[01:19:56.220 --> 01:19:57.940]   And there's just this little loophole that
[01:19:57.940 --> 01:19:59.420]   means it had to go to federal court.
[01:19:59.420 --> 01:20:02.220]   The federal court has always had a little bit more trouble
[01:20:02.220 --> 01:20:04.060]   with cases like this.
[01:20:04.060 --> 01:20:08.300]   And they overturned Judge Alsop's ruling.
[01:20:08.300 --> 01:20:13.980]   Then the Supreme Court declined to rule.
[01:20:13.980 --> 01:20:16.660]   The federal court said, you can copyright an API.
[01:20:16.660 --> 01:20:20.020]   So by the way, that's decided unless the Supreme Court takes it
[01:20:20.020 --> 01:20:21.780]   back up, but that's decided.
[01:20:21.780 --> 01:20:25.260]   So as soon as they were told where they could do that,
[01:20:25.260 --> 01:20:28.020]   Oracle asserted its copyright on the APIs.
[01:20:28.020 --> 01:20:30.980]   And then the Supreme Court said, all right, now you've
[01:20:30.980 --> 01:20:33.100]   got to go back to Judge Alsop's court
[01:20:33.100 --> 01:20:38.420]   and figure out whether Google violated a copyright.
[01:20:38.420 --> 01:20:39.460]   And that's where we stand today.
[01:20:39.460 --> 01:20:41.100]   It's just very strange.
[01:20:41.100 --> 01:20:46.780]   And the problem is fair use is in no way a simple concept.
[01:20:46.780 --> 01:20:51.300]   I mean, it's got four incredibly complicated factors
[01:20:51.300 --> 01:20:55.380]   that courts have to consider in every fair use case.
[01:20:55.380 --> 01:20:57.700]   Opinions are all over the map.
[01:20:57.700 --> 01:20:58.740]   Is it transformative?
[01:20:58.740 --> 01:20:59.780]   Is it not?
[01:20:59.780 --> 01:21:02.740]   Is the use justified?
[01:21:02.740 --> 01:21:06.500]   Is that how much of the finished work did it use?
[01:21:06.500 --> 01:21:09.820]   Is it going to affect the market for that?
[01:21:09.820 --> 01:21:12.980]   I mean, these are even the term transformative
[01:21:12.980 --> 01:21:15.140]   is hard to define.
[01:21:15.140 --> 01:21:17.580]   Well, I have more faith that a jury can decide that
[01:21:17.580 --> 01:21:22.140]   than they could decide whether Google stole Sun's code
[01:21:22.140 --> 01:21:26.140]   or whether APIs are copyrightable.
[01:21:26.140 --> 01:21:29.980]   Those things really are opaque to a normal, non-technical
[01:21:29.980 --> 01:21:31.020]   person.
[01:21:31.020 --> 01:21:34.140]   At least fair use is something that you could presumably
[01:21:34.140 --> 01:21:36.820]   apply common sense to.
[01:21:36.820 --> 01:21:38.940]   Who knows what will happen though?
[01:21:38.940 --> 01:21:39.860]   We'll find out.
[01:21:39.860 --> 01:21:40.620]   We'll keep an eye on that.
[01:21:40.620 --> 01:21:42.180]   And I'm sure they'll report at the minute it happens.
[01:21:42.180 --> 01:21:45.340]   We'll break in if they come back with a verdict.
[01:21:45.340 --> 01:21:46.540]   But we're still waiting.
[01:21:46.540 --> 01:21:48.140]   They've been now-- how long?
[01:21:48.140 --> 01:21:49.100]   Two days, three days.
[01:21:49.100 --> 01:21:52.340]   They've been thinking about since Monday.
[01:21:52.340 --> 01:21:55.980]   And the judge apparently joked to the jury,
[01:21:55.980 --> 01:21:59.140]   don't look up what an API is.
[01:21:59.140 --> 01:22:00.860]   I don't know if he was joking.
[01:22:00.860 --> 01:22:02.980]   Don't try to understand what an API is.
[01:22:02.980 --> 01:22:05.820]   He's the judge who famously taught himself Java
[01:22:05.820 --> 01:22:08.820]   for the previous case.
[01:22:08.820 --> 01:22:10.260]   Although he might have some standing,
[01:22:10.260 --> 01:22:14.380]   he has a bachelor's in mathematics from Mississippi State.
[01:22:14.380 --> 01:22:18.820]   So he's not completely ignorant of technical stuff.
[01:22:18.820 --> 01:22:20.140]   It was from 1967.
[01:22:20.140 --> 01:22:22.060]   So I doubt he used it to computer at the time,
[01:22:22.060 --> 01:22:24.300]   but not a personal computer anyway.
[01:22:24.300 --> 01:22:27.020]   I heard he just taught himself JavaScript.
[01:22:27.020 --> 01:22:28.500]   Oh, no, really?
[01:22:28.500 --> 01:22:29.140]   Are you kidding?
[01:22:29.140 --> 01:22:31.460]   No, I'm just kidding.
[01:22:31.460 --> 01:22:35.220]   He did say, I taught myself Java so I could talk.
[01:22:35.220 --> 01:22:37.020]   Sure he did.
[01:22:37.020 --> 01:22:38.620]   I think he-- isn't he the guy who said,
[01:22:38.620 --> 01:22:40.260]   and I wrote an array-bounds checker,
[01:22:40.260 --> 01:22:43.340]   and it wasn't that hard.
[01:22:43.340 --> 01:22:44.020]   Nine lines--
[01:22:44.020 --> 01:22:45.500]   Don't we all?
[01:22:45.500 --> 01:22:46.820]   Who hasn't done it?
[01:22:46.820 --> 01:22:48.220]   Who hasn't done it?
[01:22:48.220 --> 01:22:50.380]   I've done it in e-max-lessp.
[01:22:50.380 --> 01:22:51.500]   So you could.
[01:22:51.500 --> 01:22:52.060]   You could.
[01:22:52.060 --> 01:22:53.060]   Absolutely.
[01:22:53.060 --> 01:22:55.020]   Fewer lines of code, probably.
[01:22:55.020 --> 01:22:57.900]   I did it recursively.
[01:22:57.900 --> 01:23:00.740]   I did it with no pen.
[01:23:00.740 --> 01:23:01.820]   I'm sorry.
[01:23:01.820 --> 01:23:03.140]   They're a joke.
[01:23:03.140 --> 01:23:06.740]   Foxconn is replacing 60,000 factory workers.
[01:23:06.740 --> 01:23:08.180]   Foxconn, of course, builds the iPhone.
[01:23:08.180 --> 01:23:11.020]   The build is also a lot of other technology stuff.
[01:23:11.020 --> 01:23:15.660]   They're replacing those workers with robots.
[01:23:15.660 --> 01:23:19.140]   They say, no, nobody's going to lose their job.
[01:23:19.140 --> 01:23:19.660]   Maybe not.
[01:23:19.660 --> 01:23:25.940]   Maybe business is so good that according
[01:23:25.940 --> 01:23:29.300]   to a government official speaking to the South China
[01:23:29.300 --> 01:23:32.700]   Morning Post, one factory-- one Foxconn factory
[01:23:32.700 --> 01:23:36.060]   has reduced employee strength from 110,000 to 50,000
[01:23:36.060 --> 01:23:40.860]   thanks to the introduction of robots.
[01:23:40.860 --> 01:23:43.180]   Actually, this is about China investing in robots
[01:23:43.180 --> 01:23:44.820]   as much as it is Foxconn.
[01:23:44.820 --> 01:23:46.340]   The other thing that's happening, of course,
[01:23:46.340 --> 01:23:48.700]   is Foxconn's building plants outside of China.
[01:23:48.700 --> 01:23:50.300]   They built a plant in Brazil, and now they're
[01:23:50.300 --> 01:23:53.180]   building a plant in India.
[01:23:53.180 --> 01:23:54.820]   And by the way, they're the ones who
[01:23:54.820 --> 01:23:59.620]   bought the Nokia candy bar phone business from Microsoft.
[01:23:59.620 --> 01:24:02.900]   Speaking of Microsoft, I think it's
[01:24:02.900 --> 01:24:04.900]   kind of over for Windows phone.
[01:24:04.900 --> 01:24:05.900]   I don't know.
[01:24:05.900 --> 01:24:07.780]   I don't want to be premature.
[01:24:07.780 --> 01:24:08.460]   You think so?
[01:24:08.460 --> 01:24:12.060]   Maybe they sold the feature phone business to Foxconn
[01:24:12.060 --> 01:24:13.660]   last week this week.
[01:24:13.660 --> 01:24:17.580]   They laid off 1,850 people, most of them in Finland,
[01:24:17.580 --> 01:24:20.220]   to streamline the smartphone business,
[01:24:20.220 --> 01:24:23.740]   and that nearly $1 billion charge for that--
[01:24:23.740 --> 01:24:26.660]   mostly in severance, and so forth.
[01:24:26.660 --> 01:24:29.540]   I think that it's pretty clear if Microsoft continues
[01:24:29.540 --> 01:24:31.940]   in the phone business, it won't be--
[01:24:31.940 --> 01:24:33.580]   to compete with iOS and Android, it'll
[01:24:33.580 --> 01:24:36.580]   be some sort of enterprise play.
[01:24:36.580 --> 01:24:38.380]   How much have they spent on that thing?
[01:24:38.380 --> 01:24:39.580]   Do you think?
[01:24:39.580 --> 01:24:42.660]   Well, they bought Nokia for $7.6 billion.
[01:24:42.660 --> 01:24:46.540]   They took a write off of how much $5, $6.5 billion,
[01:24:46.540 --> 01:24:48.060]   something like that.
[01:24:48.060 --> 01:24:50.540]   I think they've written off the entire metal thing.
[01:24:50.540 --> 01:24:51.620]   Yeah, that with that.
[01:24:51.620 --> 01:24:52.740]   $950 million.
[01:24:52.740 --> 01:24:55.100]   Yeah, I think it's pretty much--
[01:24:55.100 --> 01:24:56.940]   Wow.
[01:24:56.940 --> 01:25:01.100]   What's kind of ironic is that the current CEO, Satya Nadella,
[01:25:01.100 --> 01:25:04.300]   was dead set against it before he was CEO.
[01:25:04.300 --> 01:25:07.460]   This was Balmer's acquisition, and he didn't think
[01:25:07.460 --> 01:25:09.260]   it was a good idea, and maybe he was proven right.
[01:25:09.260 --> 01:25:10.100]   And he was right.
[01:25:10.100 --> 01:25:11.540]   Maybe he was.
[01:25:11.540 --> 01:25:13.220]   You're going to have to explain what this means to me,
[01:25:13.220 --> 01:25:14.340]   Aaron Nukem.
[01:25:14.340 --> 01:25:18.980]   Google wants to add the Raspberry Pi 3 to AOSP.
[01:25:18.980 --> 01:25:20.420]   Oh, cool.
[01:25:20.420 --> 01:25:21.300]   What the hell?
[01:25:21.300 --> 01:25:22.700]   Does it mean it's going to be an Android?
[01:25:22.700 --> 01:25:24.180]   Yeah, you'll be able to run Android on it.
[01:25:24.180 --> 01:25:24.680]   Yeah.
[01:25:24.680 --> 01:25:25.660]   Yeah, that's awesome.
[01:25:25.660 --> 01:25:26.180]   That's great.
[01:25:26.180 --> 01:25:27.300]   You did not see that one.
[01:25:27.300 --> 01:25:28.380]   I'm excited all of a sudden.
[01:25:28.380 --> 01:25:29.300]   Woo.
[01:25:29.300 --> 01:25:30.060]   That is interesting.
[01:25:30.060 --> 01:25:34.500]   Right now, Pies typically will mostly run Linux, Debian,
[01:25:34.500 --> 01:25:36.180]   a special version called Raspbian.
[01:25:36.180 --> 01:25:39.420]   There are other operating systems people sometimes run on it.
[01:25:39.420 --> 01:25:41.340]   And you can run Android on--
[01:25:41.340 --> 01:25:43.820]   it's very difficult to run Android on Raspberry Pi now,
[01:25:43.820 --> 01:25:44.820]   because the drivers aren't there.
[01:25:44.820 --> 01:25:45.820]   And that's what this does.
[01:25:45.820 --> 01:25:47.900]   It has the drivers that they need to actually run this
[01:25:47.900 --> 01:25:50.260]   on the Broadcom chip that they use.
[01:25:50.260 --> 01:25:53.660]   So this is going to be really, really exciting.
[01:25:53.660 --> 01:25:56.660]   So for a long time, in fact, I think maybe last time
[01:25:56.660 --> 01:25:59.100]   I was on the new screen savers, we talked a little bit about this.
[01:25:59.100 --> 01:26:02.380]   I use some of the knockoff Raspberry Pi clones,
[01:26:02.380 --> 01:26:05.260]   like O'Droid.
[01:26:05.260 --> 01:26:05.900]   I see right.
[01:26:05.900 --> 01:26:07.300]   And now, see, right.
[01:26:07.300 --> 01:26:09.380]   And those types of things that use the hard kernel--
[01:26:09.380 --> 01:26:16.860]   the all-winner type chipsets, because the Android--
[01:26:16.860 --> 01:26:19.100]   Android already runs on those chipsets.
[01:26:19.100 --> 01:26:20.540]   So it makes it really easy to run.
[01:26:20.540 --> 01:26:21.700]   And things run natively.
[01:26:21.700 --> 01:26:22.740]   There's native drivers.
[01:26:22.740 --> 01:26:25.540]   There's support for the Mali GPUs that are built into them,
[01:26:25.540 --> 01:26:26.300]   and so forth.
[01:26:26.300 --> 01:26:29.500]   But there hasn't been support for it on the Raspberry Pi.
[01:26:29.500 --> 01:26:31.860]   People would love to run Android on Raspberry Pi now.
[01:26:31.860 --> 01:26:33.540]   It looks like they're going to be able to.
[01:26:33.540 --> 01:26:38.860]   Well, someday, currently, the Google Git commit
[01:26:38.860 --> 01:26:40.900]   has no-- it's an empty repository.
[01:26:40.900 --> 01:26:42.860]   But the repository is there.
[01:26:42.860 --> 01:26:44.300]   They've created the repository.
[01:26:44.300 --> 01:26:45.460]   That's the news.
[01:26:45.460 --> 01:26:46.500]   We have--
[01:26:46.500 --> 01:26:47.580]   We've got a repository.
[01:26:47.580 --> 01:26:49.060]   We have created a repository.
[01:26:49.060 --> 01:26:52.020]   We haven't actually put anything in it.
[01:26:52.020 --> 01:26:54.340]   But any day now--
[01:26:54.340 --> 01:26:56.500]   I didn't know Google had a Git.
[01:26:56.500 --> 01:26:59.420]   Is that compete with GitHub, Google Git?
[01:26:59.420 --> 01:27:00.060]   I guess I'm just--
[01:27:00.060 --> 01:27:01.060]   Google code.
[01:27:01.060 --> 01:27:02.220]   Yeah, well, they killed code.
[01:27:02.220 --> 01:27:03.220]   They did, didn't they?
[01:27:03.220 --> 01:27:04.220]   Yeah.
[01:27:04.220 --> 01:27:09.420]   90 days of Android sales beat almost nine months
[01:27:09.420 --> 01:27:13.340]   of Windows 10 sales.
[01:27:13.340 --> 01:27:15.620]   Maybe that's Windows.
[01:27:15.620 --> 01:27:19.060]   No, Windows 10.
[01:27:19.060 --> 01:27:21.860]   So Microsoft, of course, all happy,
[01:27:21.860 --> 01:27:23.580]   because they've sold 300 million,
[01:27:23.580 --> 01:27:25.060]   or there didn't sell any of them,
[01:27:25.060 --> 01:27:26.020]   because they gave them away.
[01:27:26.020 --> 01:27:30.540]   But there are 300 million Windows 10 active users.
[01:27:30.540 --> 01:27:36.260]   But that took nine months to get there, almost 10 or 11
[01:27:36.260 --> 01:27:36.780]   soon.
[01:27:36.780 --> 01:27:38.300]   It's July 29.
[01:27:38.300 --> 01:27:40.060]   It was the anniversary day.
[01:27:40.060 --> 01:27:44.900]   In just the last quarter, 350 million smartphones
[01:27:44.900 --> 01:27:48.660]   were sold 293 million of them ran Android.
[01:27:48.660 --> 01:27:50.460]   2.4 million ran Windows phone.
[01:27:50.460 --> 01:27:51.460]   I'm sorry, I shouldn't laugh.
[01:27:51.460 --> 01:27:54.420]   That's mean.
[01:27:54.420 --> 01:27:57.220]   And those of you who live in Canada
[01:27:57.220 --> 01:28:02.860]   have been there before with BlackBerry.
[01:28:02.860 --> 01:28:04.180]   I'm sure you blanked that out.
[01:28:04.180 --> 01:28:08.020]   What is this one?
[01:28:08.020 --> 01:28:12.900]   Google says, you're going to be able to not use passwords
[01:28:12.900 --> 01:28:14.660]   anymore on Android.
[01:28:14.660 --> 01:28:15.900]   Yes, I thought that was interesting.
[01:28:15.900 --> 01:28:17.820]   We can tell who you are.
[01:28:17.820 --> 01:28:19.780]   So you don't have to tell us.
[01:28:19.780 --> 01:28:21.060]   Trust scores.
[01:28:21.060 --> 01:28:22.900]   Authentication, of course, is the really
[01:28:22.900 --> 01:28:25.100]   the biggest challenge out there.
[01:28:25.100 --> 01:28:27.500]   How do you prove you are who you say you are?
[01:28:27.500 --> 01:28:29.700]   And it's why, all of a sudden, in the last 20 years,
[01:28:29.700 --> 01:28:33.260]   all of us have had acquired hundreds of passwords,
[01:28:33.260 --> 01:28:36.060]   or in my case, one, monkey 123.
[01:28:36.060 --> 01:28:38.380]   And have to, every time you go somewhere,
[01:28:38.380 --> 01:28:41.020]   do something, remember your password.
[01:28:41.020 --> 01:28:42.260]   Terrible system.
[01:28:42.260 --> 01:28:43.220]   Everybody agrees.
[01:28:43.220 --> 01:28:45.660]   And people have tried to come up with some solution.
[01:28:45.660 --> 01:28:50.420]   Google says, you're not going to need to log into Android apps
[01:28:50.420 --> 01:28:52.140]   by next year.
[01:28:52.140 --> 01:28:53.860]   They talked about this at I/O.
[01:28:53.860 --> 01:28:57.420]   There's something they call the Trust API.
[01:28:57.420 --> 01:29:00.740]   And what they'll do is they'll look at things
[01:29:00.740 --> 01:29:06.500]   like how fast you type and how you talk and the devices you've
[01:29:06.500 --> 01:29:09.260]   logged into Wi-Fi and Bluetooth.
[01:29:09.260 --> 01:29:12.540]   And we can create an effect like a super cookie
[01:29:12.540 --> 01:29:18.580]   that identifies you uniquely and is pretty reliable.
[01:29:18.580 --> 01:29:20.700]   And--
[01:29:20.700 --> 01:29:22.540]   It's like using metadata to--
[01:29:22.540 --> 01:29:22.900]   Yeah.
[01:29:22.900 --> 01:29:24.460]   --this is why metadata is so powerful.
[01:29:24.460 --> 01:29:27.100]   So now they're finally using it for good.
[01:29:27.100 --> 01:29:28.740]   People have used this--
[01:29:28.740 --> 01:29:30.940]   that's where the term super cookie came from.
[01:29:30.940 --> 01:29:37.660]   They've used this to create cookies that identify you uniquely.
[01:29:37.660 --> 01:29:39.580]   Actually, Google probably thought about it and said,
[01:29:39.580 --> 01:29:42.020]   that could also be used instead of a password.
[01:29:42.020 --> 01:29:43.300]   How reliable is it?
[01:29:43.300 --> 01:29:45.540]   Apparently, they're going to do a scoring system
[01:29:45.540 --> 01:29:49.500]   so that they kind of know if it's a high quality identification
[01:29:49.500 --> 01:29:53.380]   or maybe it's a little dicey.
[01:29:53.380 --> 01:29:54.500]   So they're not going to have launch it.
[01:29:54.500 --> 01:29:56.340]   Roll it out to my bank account first.
[01:29:56.340 --> 01:29:57.460]   Yeah, maybe not.
[01:29:57.460 --> 01:29:59.060]   I like this idea, though.
[01:29:59.060 --> 01:29:59.580]   I love it.
[01:29:59.580 --> 01:30:00.620]   This is going to use--
[01:30:00.620 --> 01:30:02.780]   they're going to use--
[01:30:02.780 --> 01:30:04.380]   like for games.
[01:30:04.380 --> 01:30:05.340]   Yeah.
[01:30:05.340 --> 01:30:06.740]   It'll be relatively low.
[01:30:06.740 --> 01:30:08.980]   Yeah, when you start to play a game, the Google Games thing
[01:30:08.980 --> 01:30:10.380]   won't say, OK, log in now.
[01:30:10.380 --> 01:30:11.980]   I don't know. It's you.
[01:30:11.980 --> 01:30:13.580]   Yeah, I could see that being Randy.
[01:30:13.580 --> 01:30:15.820]   My Xbox One does that with face recognition.
[01:30:15.820 --> 01:30:17.900]   When I sit down, it says, hello, Leo.
[01:30:17.900 --> 01:30:23.820]   The weird thing is that my wife's son, my stepson,
[01:30:23.820 --> 01:30:25.500]   looks a lot like his mother.
[01:30:25.500 --> 01:30:29.180]   So every time Lisa sits down, it says, hello, Michael.
[01:30:29.180 --> 01:30:30.740]   So it's not perfect.
[01:30:30.740 --> 01:30:32.140]   Needs a bit of work.
[01:30:32.140 --> 01:30:34.340]   They need to retrain that model.
[01:30:34.340 --> 01:30:36.340]   But I think it's the combination of the things
[01:30:36.340 --> 01:30:38.100]   that they're talking about, the vocal inflections,
[01:30:38.100 --> 01:30:40.020]   the facial recognition, and the proximity.
[01:30:40.020 --> 01:30:41.700]   It's not just so much information.
[01:30:41.700 --> 01:30:44.180]   It's when you combine those three different vectors,
[01:30:44.180 --> 01:30:47.380]   you get a fairly-- with a fairly high degree,
[01:30:47.380 --> 01:30:50.460]   you can get a unique match on that.
[01:30:50.460 --> 01:30:52.460]   And you're going to have to be--
[01:30:52.460 --> 01:30:54.860]   it's going to have to be mission impossible, basically.
[01:30:54.860 --> 01:30:56.020]   Tom Cruise with like--
[01:30:56.020 --> 01:30:57.460]   The rubble is here.
[01:30:57.460 --> 01:30:57.820]   Right.
[01:30:57.820 --> 01:31:01.700]   And imitating your voice in order to get by that thing.
[01:31:01.700 --> 01:31:03.140]   Well, that's biometrics.
[01:31:03.140 --> 01:31:05.660]   I think biometrics is pretty good to begin with.
[01:31:05.660 --> 01:31:07.660]   Although Apple drives me crazy, because even though they
[01:31:07.660 --> 01:31:11.340]   know my fingerprint, they constantly ask me for a password.
[01:31:11.340 --> 01:31:12.500]   The iPhone we found out now.
[01:31:12.500 --> 01:31:13.100]   Yeah.
[01:31:13.100 --> 01:31:16.860]   If you don't use your iPhone for what was it?
[01:31:16.860 --> 01:31:18.020]   Was it eight hours?
[01:31:18.020 --> 01:31:19.220]   I'd ask you for the carrier.
[01:31:19.220 --> 01:31:20.380]   It asks you for your password.
[01:31:20.380 --> 01:31:25.300]   It's like, dude, you have my freaking fingerprint.
[01:31:25.300 --> 01:31:27.660]   Is that better than my password?
[01:31:27.660 --> 01:31:28.740]   Isn't it?
[01:31:28.740 --> 01:31:29.180]   Maybe not.
[01:31:29.180 --> 01:31:30.300]   But we're getting there, though.
[01:31:30.300 --> 01:31:33.260]   I remember with my iPhone, every time I installed something,
[01:31:33.260 --> 01:31:35.340]   every time I tried to download an app,
[01:31:35.340 --> 01:31:38.700]   every time it asked me for my app store password.
[01:31:38.700 --> 01:31:40.300]   It was such a pain, right?
[01:31:40.300 --> 01:31:41.220]   Could you not?
[01:31:41.220 --> 01:31:41.740]   Yeah.
[01:31:41.740 --> 01:31:44.100]   Just remember it from 15 seconds ago.
[01:31:44.100 --> 01:31:45.900]   But that's what's frustrating for me, because now we
[01:31:45.900 --> 01:31:48.300]   do have a fingerprint, and we have a way of actively
[01:31:48.300 --> 01:31:50.420]   authenticating, and it still wants the password.
[01:31:50.420 --> 01:31:51.020]   It's like, come on.
[01:31:51.020 --> 01:31:53.300]   I think they're afraid if you haven't used it for a while,
[01:31:53.300 --> 01:31:53.700]   they're afraid of--
[01:31:53.700 --> 01:31:54.620]   That's exactly right.
[01:31:54.620 --> 01:31:55.980]   You know, someone else has gotten it.
[01:31:55.980 --> 01:31:57.300]   Stole your finger and your phone.
[01:31:57.300 --> 01:31:59.780]   Cut off your finger.
[01:31:59.780 --> 01:32:02.980]   Samsung's newest tablet will scan your iris.
[01:32:02.980 --> 01:32:04.820]   They're actually aiming it not at consumers,
[01:32:04.820 --> 01:32:07.780]   but at business and mostly government.
[01:32:07.780 --> 01:32:12.340]   So an iris scanner, it's in the Indian market only right now,
[01:32:12.340 --> 01:32:17.660]   has dual eye scanners to an actual iris scanner.
[01:32:17.660 --> 01:32:19.300]   You know, Windows has that hello feature,
[01:32:19.300 --> 01:32:21.300]   but I don't think it's as quite--
[01:32:21.300 --> 01:32:21.900]   Maybe it is.
[01:32:21.900 --> 01:32:24.140]   That's pretty sophisticated iris scanning.
[01:32:24.140 --> 01:32:27.020]   Yeah, no kidding.
[01:32:27.020 --> 01:32:28.260]   Actually, I guess no.
[01:32:28.260 --> 01:32:31.140]   I guess Windows Hello does use iris as well as
[01:32:31.140 --> 01:32:33.980]   fingerprint scanning and facial recognition.
[01:32:33.980 --> 01:32:36.980]   Because I can look at my laptop, my Surface Book, and it
[01:32:36.980 --> 01:32:38.100]   says, oh, hi, Leo.
[01:32:38.100 --> 01:32:40.740]   Sometimes it says, I have no idea who you are.
[01:32:40.740 --> 01:32:43.580]   I hope it's better than the Nexus machines,
[01:32:43.580 --> 01:32:46.220]   the global entry type machines that we have.
[01:32:46.220 --> 01:32:47.660]   Do they not work well?
[01:32:47.660 --> 01:32:49.940]   Well, so they scan your iris.
[01:32:49.940 --> 01:32:50.260]   Right.
[01:32:50.260 --> 01:32:51.020]   I've seen people do it.
[01:32:51.020 --> 01:32:54.220]   Nine times out of 10, it says, please move back.
[01:32:54.220 --> 01:32:55.340]   Please move forward.
[01:32:55.340 --> 01:32:56.300]   Please move back.
[01:32:56.300 --> 01:32:56.900]   Please try again.
[01:32:56.900 --> 01:32:57.940]   No, Windows Hello does that.
[01:32:57.940 --> 01:32:58.340]   15 times.
[01:32:58.340 --> 01:32:59.540]   No, that's what Windows Hello does.
[01:32:59.540 --> 01:33:00.860]   It does the same exact thing.
[01:33:00.860 --> 01:33:01.740]   It drives me nuts.
[01:33:01.740 --> 01:33:02.340]   Sit forward.
[01:33:02.340 --> 01:33:03.220]   Sit back.
[01:33:03.220 --> 01:33:04.420]   Please move back.
[01:33:04.420 --> 01:33:06.260]   You have to be exactly the focal length, I guess.
[01:33:06.260 --> 01:33:06.780]   Yeah.
[01:33:06.780 --> 01:33:07.300]   Yeah.
[01:33:07.300 --> 01:33:11.860]   I don't know if this is good or bad,
[01:33:11.860 --> 01:33:14.540]   but puppy cams are coming to Facebook Live.
[01:33:14.540 --> 01:33:15.420]   It's bad.
[01:33:15.420 --> 01:33:18.300]   And you won't be limited to 90 minutes anymore.
[01:33:18.300 --> 01:33:19.460]   You can stream--
[01:33:19.460 --> 01:33:20.340]   Unlimited.
[01:33:20.340 --> 01:33:22.500]   Forever.
[01:33:22.500 --> 01:33:23.340]   Wow.
[01:33:23.340 --> 01:33:24.220]   Oh, I don't know.
[01:33:24.220 --> 01:33:24.700]   Why not?
[01:33:24.700 --> 01:33:26.860]   I don't know.
[01:33:26.860 --> 01:33:29.420]   Because it's not just going to be puppies, obviously.
[01:33:29.420 --> 01:33:32.660]   It's going to be exploding watermelons and stuff like that.
[01:33:32.660 --> 01:33:33.660]   Crot shot.
[01:33:33.660 --> 01:33:34.660]   But it won't be born.
[01:33:34.660 --> 01:33:36.100]   It'll burn over and over.
[01:33:36.100 --> 01:33:38.180]   No.
[01:33:38.180 --> 01:33:40.220]   So that's sort of a plus.
[01:33:40.220 --> 01:33:42.940]   It's true.
[01:33:42.940 --> 01:33:44.540]   Among other new features, the ability
[01:33:44.540 --> 01:33:50.020]   to look at an engagement graph so you can skip
[01:33:50.020 --> 01:33:53.900]   to the most exciting parts of live videos.
[01:33:53.900 --> 01:33:54.500]   The end.
[01:33:54.500 --> 01:33:56.020]   [LAUGHTER]
[01:33:56.020 --> 01:33:57.940]   [LAUGHTER]
[01:33:57.940 --> 01:33:59.860]   So we're going to give you an infinite amount of time,
[01:33:59.860 --> 01:34:02.460]   but we're going to be like, oh, hey, by the way,
[01:34:02.460 --> 01:34:03.460]   you can skip to right here.
[01:34:03.460 --> 01:34:04.540]   Skip to the scoring part.
[01:34:04.540 --> 01:34:07.180]   You might want to jump to this part.
[01:34:07.180 --> 01:34:08.340]   What?
[01:34:08.340 --> 01:34:10.500]   That's actually kind of an interesting idea.
[01:34:10.500 --> 01:34:13.660]   Lots of people were engaged at this point.
[01:34:13.660 --> 01:34:14.980]   Actually, that could be really useful.
[01:34:14.980 --> 01:34:15.820]   That's really interesting.
[01:34:15.820 --> 01:34:17.740]   I should do that with Game of Thrones.
[01:34:17.740 --> 01:34:19.940]   I have a friend who does--
[01:34:19.940 --> 01:34:21.940]   it's an app called-- or it's a service called,
[01:34:21.940 --> 01:34:23.580]   are you watching this?
[01:34:23.580 --> 01:34:25.780]   Are you-- yeah.
[01:34:25.780 --> 01:34:27.980]   And they mind data on the Twitter
[01:34:27.980 --> 01:34:29.940]   timeline for sporting events.
[01:34:29.940 --> 01:34:31.260]   And they would let you know.
[01:34:31.260 --> 01:34:33.700]   They would send you a notification.
[01:34:33.700 --> 01:34:36.660]   What a team that you followed their game got good
[01:34:36.660 --> 01:34:37.500]   so you could tune in.
[01:34:37.500 --> 01:34:38.260]   Oh, I love that.
[01:34:38.260 --> 01:34:38.940]   Interesting.
[01:34:38.940 --> 01:34:39.900]   That's a good idea.
[01:34:39.900 --> 01:34:40.300]   Why isn't Twitter?
[01:34:40.300 --> 01:34:42.100]   Yeah, I know.
[01:34:42.100 --> 01:34:43.100]   Wait, what?
[01:34:43.100 --> 01:34:45.580]   Why doesn't Twitter do that?
[01:34:45.580 --> 01:34:46.420]   I don't know.
[01:34:46.420 --> 01:34:47.660]   What would be super cool--
[01:34:47.660 --> 01:34:50.620]   this is something that FCC is doing with their cable card--
[01:34:50.620 --> 01:34:51.340]   not cable card.
[01:34:51.340 --> 01:34:52.180]   Oh, god.
[01:34:52.180 --> 01:34:53.660]   Opening up the cable.
[01:34:53.660 --> 01:34:55.220]   Please don't mention that.
[01:34:55.220 --> 01:34:57.380]   Sorry.
[01:34:57.380 --> 01:34:59.660]   Things like that could be amazing because right now,
[01:34:59.660 --> 01:35:02.980]   I think I'm not a sports fan, so I'm lost here.
[01:35:02.980 --> 01:35:05.340]   But you could actually think of it
[01:35:05.340 --> 01:35:08.620]   if you had open-- if you treated your cable box,
[01:35:08.620 --> 01:35:10.420]   like you did the internet, you could actually
[01:35:10.420 --> 01:35:13.420]   just send people directly to the game, which would be freaking
[01:35:13.420 --> 01:35:14.380]   awesome.
[01:35:14.380 --> 01:35:14.900]   Anyway.
[01:35:14.900 --> 01:35:17.060]   Just walk in the room and it knows what you want to see.
[01:35:17.060 --> 01:35:18.140]   Yeah.
[01:35:18.140 --> 01:35:20.820]   Oh, now we're going back to crazy context and AI.
[01:35:20.820 --> 01:35:21.420]   I love it.
[01:35:21.420 --> 01:35:21.940]   Go circle.
[01:35:21.940 --> 01:35:23.860]   I want it.
[01:35:23.860 --> 01:35:25.260]   So we're glad you're back, Matthew,
[01:35:25.260 --> 01:35:28.100]   because Stacy wanted you to explain what Twitter's changes
[01:35:28.100 --> 01:35:29.580]   mean.
[01:35:29.580 --> 01:35:30.220]   Oh, yeah.
[01:35:30.220 --> 01:35:30.940]   Help me, Matthew.
[01:35:30.940 --> 01:35:31.980]   See the expert on this?
[01:35:31.980 --> 01:35:34.060]   Is that the deal?
[01:35:34.060 --> 01:35:38.420]   Yeah, Twitter-- I mean, this is your kind of live on there.
[01:35:38.420 --> 01:35:40.580]   So no more Twitter canoes.
[01:35:40.580 --> 01:35:41.500]   It is part of my beat.
[01:35:41.500 --> 01:35:41.860]   No, Matthew.
[01:35:41.860 --> 01:35:43.500]   Are you tweeting right now, Matthew?
[01:35:43.500 --> 01:35:44.140]   I am.
[01:35:44.140 --> 01:35:46.660]   Actually, infinite Twitter canoes now.
[01:35:46.660 --> 01:35:47.100]   Right.
[01:35:47.100 --> 01:35:49.940]   So the Twitter canoes can be unlimited.
[01:35:49.940 --> 01:35:52.580]   You can add up to 50 people, I think.
[01:35:52.580 --> 01:35:59.940]   So depending on where you fall on using Twitter
[01:35:59.940 --> 01:36:02.860]   and whether you've been harassed, for example,
[01:36:02.860 --> 01:36:06.860]   you would either see these changes as good or the worst
[01:36:06.860 --> 01:36:07.580]   thing ever.
[01:36:07.580 --> 01:36:11.620]   So I saw probably 50/50 in my stream people saying, hey,
[01:36:11.620 --> 01:36:14.580]   this sounds great because handles
[01:36:14.580 --> 01:36:17.140]   won't be counted against the character limit.
[01:36:17.140 --> 01:36:19.460]   Images and other embeds won't be counted
[01:36:19.460 --> 01:36:20.900]   against the character limit.
[01:36:20.900 --> 01:36:21.820]   So that's good, right?
[01:36:21.820 --> 01:36:23.140]   You get more room.
[01:36:23.140 --> 01:36:26.300]   But then anyone who's ever been at mentioned
[01:36:26.300 --> 01:36:32.900]   into a giant fight with social justice warriors or whatever
[01:36:32.900 --> 01:36:38.020]   and then been copied on every mention and every retweet
[01:36:38.020 --> 01:36:42.460]   knows that that can be a hugely annoying problem.
[01:36:42.460 --> 01:36:45.260]   So Twitter's made it easier for people
[01:36:45.260 --> 01:36:48.460]   to tweet without worrying about using up space.
[01:36:48.460 --> 01:36:53.740]   But at the same time, it's enabled probably more harassment.
[01:36:53.740 --> 01:36:57.740]   So in a way, this is Twitter in a nutshell.
[01:36:57.740 --> 01:37:00.820]   Everything it does is simultaneously good, really good
[01:37:00.820 --> 01:37:02.620]   and also incredibly horrible.
[01:37:02.620 --> 01:37:09.380]   So the only thing that really--
[01:37:09.380 --> 01:37:11.180]   I mean, really, the bottom line, the only thing that counts
[01:37:11.180 --> 01:37:15.420]   against 140 characters is the stuff you say.
[01:37:15.420 --> 01:37:15.860]   Yeah.
[01:37:15.860 --> 01:37:16.380]   You're--
[01:37:16.380 --> 01:37:17.380]   Yeah.
[01:37:17.380 --> 01:37:20.820]   We haven't gotten to the part about the period before that had
[01:37:20.820 --> 01:37:21.260]   symbol.
[01:37:21.260 --> 01:37:22.300]   Do you want me to get into that?
[01:37:22.300 --> 01:37:24.740]   They're changing that?
[01:37:24.740 --> 01:37:25.140]   What?
[01:37:25.140 --> 01:37:25.660]   Yes.
[01:37:25.660 --> 01:37:26.140]   So--
[01:37:26.140 --> 01:37:26.900]   What?
[01:37:26.900 --> 01:37:28.780]   It took us years to figure that out.
[01:37:28.780 --> 01:37:29.140]   Right.
[01:37:29.140 --> 01:37:31.500]   A thing that a lot of people didn't even know exists--
[01:37:31.500 --> 01:37:33.620]   I think some people still haven't figured it out.
[01:37:33.620 --> 01:37:34.700]   It's now going away.
[01:37:34.700 --> 01:37:38.900]   So that's good in a sense that lots of people
[01:37:38.900 --> 01:37:41.260]   can now live their lives without having
[01:37:41.260 --> 01:37:43.540]   to figure out how it works.
[01:37:43.540 --> 01:37:44.260]   But--
[01:37:44.260 --> 01:37:44.780]   OK.
[01:37:44.780 --> 01:37:45.460]   Let me explain.
[01:37:45.460 --> 01:37:46.460]   Let me explain.
[01:37:46.460 --> 01:37:47.460]   So--
[01:37:47.460 --> 01:37:48.220]   And I think you're right.
[01:37:48.220 --> 01:37:51.220]   I think a lot of people-- I've taught so many people
[01:37:51.220 --> 01:37:55.020]   that when you just at reply somebody, at Matthew I--
[01:37:55.020 --> 01:37:57.820]   hey, that was great-- the only people who see it
[01:37:57.820 --> 01:37:59.980]   are people who follow both of us, right?
[01:37:59.980 --> 01:38:00.660]   Both of us.
[01:38:00.660 --> 01:38:02.020]   Right.
[01:38:02.020 --> 01:38:05.060]   So if you want to reply to somebody,
[01:38:05.060 --> 01:38:07.780]   but you want all of your followers to see it,
[01:38:07.780 --> 01:38:10.100]   you can't begin it with an at-sign.
[01:38:10.100 --> 01:38:12.580]   So the thing that people just made up--
[01:38:12.580 --> 01:38:13.940]   I mean, Twitter never said to do this,
[01:38:13.940 --> 01:38:16.180]   but the thing that people made up is you put a dot.
[01:38:16.180 --> 01:38:19.460]   And then at Matthew I, now I'm both responding to you
[01:38:19.460 --> 01:38:21.460]   and everybody in my--
[01:38:21.460 --> 01:38:23.380]   who follows me can see it.
[01:38:23.380 --> 01:38:25.820]   That's not going to work anymore.
[01:38:25.820 --> 01:38:27.660]   No, you won't have to do that.
[01:38:27.660 --> 01:38:29.580]   So in effect, if you--
[01:38:29.580 --> 01:38:32.220]   so there's two things, which are-- which
[01:38:32.220 --> 01:38:35.340]   make it actually more confusing rather than less.
[01:38:35.340 --> 01:38:37.900]   Or the way it was described made it more confusing.
[01:38:37.900 --> 01:38:41.220]   So if you start a tweet with an at--
[01:38:41.220 --> 01:38:41.740]   Yeah.
[01:38:41.740 --> 01:38:43.300]   --everyone in your timeline can see it.
[01:38:43.300 --> 01:38:45.340]   So they're basically turning that off.
[01:38:45.340 --> 01:38:45.860]   Right.
[01:38:45.860 --> 01:38:46.860]   And it was only--
[01:38:46.860 --> 01:38:48.340]   See, that was important because it meant
[01:38:48.340 --> 01:38:50.340]   you could have a conversation with that being--
[01:38:50.340 --> 01:38:50.860]   Exactly.
[01:38:50.860 --> 01:38:52.260]   --you and I could talk back and forth
[01:38:52.260 --> 01:38:53.940]   without annoying everybody else.
[01:38:53.940 --> 01:38:58.340]   So this is another example of where it's good because it
[01:38:58.340 --> 01:39:01.460]   was a dumb thing that people didn't understand
[01:39:01.460 --> 01:39:03.700]   and it confused people.
[01:39:03.700 --> 01:39:07.540]   But the change actually can make something worse
[01:39:07.540 --> 01:39:10.460]   because things that originally would have been just
[01:39:10.460 --> 01:39:13.100]   between you and somebody who will now or could now
[01:39:13.100 --> 01:39:14.940]   be seen by everyone.
[01:39:14.940 --> 01:39:17.180]   You may not want that.
[01:39:17.180 --> 01:39:21.900]   My understanding is replies will still only be seen by--
[01:39:21.900 --> 01:39:27.020]   so if you reply to a tweet and it starts with an at,
[01:39:27.020 --> 01:39:28.580]   then it's only seen by--
[01:39:28.580 --> 01:39:29.300]   I get it.
[01:39:29.300 --> 01:39:29.940]   --of your followers.
[01:39:29.940 --> 01:39:32.420]   So if you create a new tweet that starts with an at--
[01:39:32.420 --> 01:39:33.140]   Exactly.
[01:39:33.140 --> 01:39:34.020]   Then everyone will see it.
[01:39:34.020 --> 01:39:34.820]   Oh, actually, that sounds--
[01:39:34.820 --> 01:39:35.940]   Oh, that-- OK, that's good.
[01:39:35.940 --> 01:39:35.940]   Yeah.
[01:39:35.940 --> 01:39:36.460]   Sorry.
[01:39:36.460 --> 01:39:36.980]   It does.
[01:39:36.980 --> 01:39:36.980]   Right.
[01:39:36.980 --> 01:39:37.500]   Right.
[01:39:37.500 --> 01:39:37.740]   See?
[01:39:37.740 --> 01:39:38.620]   It sounds good.
[01:39:38.620 --> 01:39:39.780]   Yeah.
[01:39:39.780 --> 01:39:40.060]   But--
[01:39:40.060 --> 01:39:41.020]   OK, I can handle that.
[01:39:41.020 --> 01:39:43.220]   The time it took me to explain all that.
[01:39:43.220 --> 01:39:44.160]   Well, that's what-- because I've
[01:39:44.160 --> 01:39:45.300]   been reading up with that for like--
[01:39:45.300 --> 01:39:46.620]   Imagine having this conversation
[01:39:46.620 --> 01:39:47.740]   with a normal human being.
[01:39:47.740 --> 01:39:48.580]   But it's more intuitive.
[01:39:48.580 --> 01:39:50.340]   You shouldn't even have to have the conversation now.
[01:39:50.340 --> 01:39:52.220]   Because it'll behave kind of like you would expect.
[01:39:52.220 --> 01:39:53.020]   Like you would expect.
[01:39:53.020 --> 01:39:53.780]   Like you would expect.
[01:39:53.780 --> 01:39:54.300]   Yeah.
[01:39:54.300 --> 01:39:57.620]   If it's a reply, then only you and I'll see it.
[01:39:57.620 --> 01:40:00.460]   We're not even getting into the whole quote tweet
[01:40:00.460 --> 01:40:03.340]   versus native retweeting.
[01:40:03.340 --> 01:40:04.300]   If they got rid of the--
[01:40:04.300 --> 01:40:05.540]   Need a whole show for that.
[01:40:05.540 --> 01:40:08.500]   They got rid of long ago what most people wanted,
[01:40:08.500 --> 01:40:12.940]   which was a retweet that quotes the original tweet
[01:40:12.940 --> 01:40:15.480]   and then allows you to add your own thoughts.
[01:40:15.480 --> 01:40:16.600]   No, they have that.
[01:40:16.600 --> 01:40:17.880]   No, that's still there.
[01:40:17.880 --> 01:40:19.120]   That's the quote tweet button.
[01:40:19.120 --> 01:40:19.800]   But it's actually--
[01:40:19.800 --> 01:40:22.400]   How do you get a quote tweet?
[01:40:22.400 --> 01:40:22.720]   Well--
[01:40:22.720 --> 01:40:23.240]   Maybe--
[01:40:23.240 --> 01:40:23.640]   I'll just quote--
[01:40:23.640 --> 01:40:25.040]   What do you use for Twitter?
[01:40:25.040 --> 01:40:26.240]   What app do you use for Twitter?
[01:40:26.240 --> 01:40:26.680]   Do you use--
[01:40:26.680 --> 01:40:30.120]   I'm using-- OK, I'm on the Twitter homepage right now.
[01:40:30.120 --> 01:40:32.720]   Use the web.
[01:40:32.720 --> 01:40:34.800]   That's what Twitter wants you to use, Matthew.
[01:40:34.800 --> 01:40:35.800]   Don't you know?
[01:40:35.800 --> 01:40:36.300]   OK.
[01:40:36.300 --> 01:40:36.800]   All right.
[01:40:36.800 --> 01:40:36.960]   All right.
[01:40:36.960 --> 01:40:39.120]   So I see a retweet button.
[01:40:39.120 --> 01:40:40.600]   So you should be able to--
[01:40:40.600 --> 01:40:42.760]   when you go to retweet--
[01:40:42.760 --> 01:40:43.660]   Oh, here it is.
[01:40:43.660 --> 01:40:44.380]   No, copy.
[01:40:44.380 --> 01:40:44.880]   No.
[01:40:44.880 --> 01:40:47.380]   So there's an added comment at the top.
[01:40:47.380 --> 01:40:50.140]   Yeah, if you click on retweet, click it.
[01:40:50.140 --> 01:40:50.460]   Yeah.
[01:40:50.460 --> 01:40:53.820]   Oh, they have added that back.
[01:40:53.820 --> 01:40:54.300]   They have--
[01:40:54.300 --> 01:40:55.580]   That's been there for a long time.
[01:40:55.580 --> 01:40:56.500]   Oh.
[01:40:56.500 --> 01:40:57.500]   Sorry.
[01:40:57.500 --> 01:40:58.980]   Because I remember when it just went--
[01:40:58.980 --> 01:40:59.900]   it just went out.
[01:40:59.900 --> 01:41:00.740]   It was like, that's it.
[01:41:00.740 --> 01:41:01.900]   That's all you got.
[01:41:01.900 --> 01:41:02.780]   The native retweet.
[01:41:02.780 --> 01:41:06.620]   That was Twitter's biggest-- that was Twitter's change.
[01:41:06.620 --> 01:41:08.860]   So they backed off on that sometime previously
[01:41:08.860 --> 01:41:09.660]   and I just didn't notice.
[01:41:09.660 --> 01:41:10.260]   OK.
[01:41:10.260 --> 01:41:12.220]   Unfortunately, the quoting has actually
[01:41:12.220 --> 01:41:14.620]   caused even more problems.
[01:41:14.620 --> 01:41:17.980]   So I have been yelled at by people,
[01:41:17.980 --> 01:41:20.500]   including a prominent Twitter investor,
[01:41:20.500 --> 01:41:23.900]   for what he called witch hunt retweets,
[01:41:23.900 --> 01:41:28.180]   where you quote someone's tweet and then say something
[01:41:28.180 --> 01:41:30.180]   snarky about it.
[01:41:30.180 --> 01:41:32.860]   This is apparently bad behavior.
[01:41:32.860 --> 01:41:35.780]   You used to be able to like, the reason they did that
[01:41:35.780 --> 01:41:39.340]   is because people were changing people's original tweets
[01:41:39.340 --> 01:41:42.100]   and not necessarily doing whatever was it,
[01:41:42.100 --> 01:41:44.860]   MT, QTR, I don't know.
[01:41:44.860 --> 01:41:48.060]   Whatever language we were supposed to do to indicate.
[01:41:48.060 --> 01:41:49.940]   Yes, MT.
[01:41:49.940 --> 01:41:51.860]   Which again, was invented by users.
[01:41:51.860 --> 01:41:53.060]   Yeah.
[01:41:53.060 --> 01:41:56.180]   All the good stuff was invented by users, frankly.
[01:41:56.180 --> 01:41:57.620]   Well, and unfortunately, it feels
[01:41:57.620 --> 01:42:00.340]   like the things that users invented
[01:42:00.340 --> 01:42:03.500]   to solve a problem that actually worked
[01:42:03.500 --> 01:42:06.980]   have been replaced by things that are official
[01:42:06.980 --> 01:42:07.860]   but don't work well.
[01:42:07.860 --> 01:42:10.460]   In other words, Twitter is better idea.
[01:42:10.460 --> 01:42:11.940]   Yeah.
[01:42:11.940 --> 01:42:13.980]   [LAUGHING]
[01:42:13.980 --> 01:42:15.980]   We need it this week in Twitter.
[01:42:15.980 --> 01:42:17.660]   No, I don't--
[01:42:17.660 --> 01:42:18.740]   No.
[01:42:18.740 --> 01:42:19.740]   No.
[01:42:19.740 --> 01:42:21.620]   Could it be 140 seconds?
[01:42:21.620 --> 01:42:24.500]   Yeah, I don't.
[01:42:24.500 --> 01:42:26.860]   But Twitter is, if there were anything that I
[01:42:26.860 --> 01:42:29.260]   have had a love/hate relationship with, it would be Twitter.
[01:42:29.260 --> 01:42:29.620]   Yep.
[01:42:29.620 --> 01:42:32.420]   That is the epitome of a love/hate relationship.
[01:42:32.420 --> 01:42:34.660]   I wrote a whole post about that, how I have a love/hate
[01:42:34.660 --> 01:42:35.220]   relationship.
[01:42:35.220 --> 01:42:36.620]   I'm on it all the time.
[01:42:36.620 --> 01:42:37.820]   I use it for work.
[01:42:37.820 --> 01:42:39.660]   It's a fantastic thing.
[01:42:39.660 --> 01:42:43.660]   It's probably changed my life more than any other--
[01:42:43.660 --> 01:42:45.620]   certainly social technology that I can think of,
[01:42:45.620 --> 01:42:46.980]   including Facebook.
[01:42:46.980 --> 01:42:50.020]   And yet, it's so frustrating.
[01:42:50.020 --> 01:42:51.300]   I have a theory.
[01:42:51.300 --> 01:42:52.220]   Can you--
[01:42:52.220 --> 01:42:52.740]   You all--
[01:42:52.740 --> 01:42:54.300]   That's just the service, but the company.
[01:42:54.300 --> 01:42:56.660]   Yeah, well, the company's ridiculous.
[01:42:56.660 --> 01:42:58.340]   That's part of the fun of it.
[01:42:58.340 --> 01:43:00.420]   But here's my theory.
[01:43:00.420 --> 01:43:03.380]   I think, OK, you tell me where I'm wrong here.
[01:43:03.380 --> 01:43:09.620]   If you just watched your timeline, not the @ replies to you,
[01:43:09.620 --> 01:43:11.780]   but if you just looked at the home,
[01:43:11.780 --> 01:43:13.220]   that's when you love it, right?
[01:43:13.220 --> 01:43:15.140]   Because it's a great continuous stream of what's
[01:43:15.140 --> 01:43:17.820]   happening right now.
[01:43:17.820 --> 01:43:20.820]   Where we get into the deeper waters
[01:43:20.820 --> 01:43:22.900]   is where you look at notifications
[01:43:22.900 --> 01:43:25.220]   and what people are saying to you.
[01:43:25.220 --> 01:43:28.460]   Now, admittedly, that's the conversation part of Twitter.
[01:43:28.460 --> 01:43:32.580]   And so I'm throwing out the baby with the bath water.
[01:43:32.580 --> 01:43:34.220]   But I find Twitter wonderful.
[01:43:34.220 --> 01:43:38.020]   If I just look at the timeline stream, the main home stream--
[01:43:38.020 --> 01:43:38.700]   No, you're right.
[01:43:38.700 --> 01:43:41.380]   And they do two completely different things.
[01:43:41.380 --> 01:43:44.940]   In a way, Twitter is several different, arguably useful,
[01:43:44.940 --> 01:43:48.260]   services kind of merged into one.
[01:43:48.260 --> 01:43:50.220]   And so the notifications on the conversation
[01:43:50.220 --> 01:43:52.420]   is one of those things.
[01:43:52.420 --> 01:43:57.580]   And the real-time stream of information is another.
[01:43:57.580 --> 01:44:05.580]   But I don't think what makes Twitter so useful and frustrating
[01:44:05.580 --> 01:44:07.300]   is the way those things work together.
[01:44:07.300 --> 01:44:10.500]   To me, if you had one--
[01:44:10.500 --> 01:44:14.420]   if you had either of those all by themselves,
[01:44:14.420 --> 01:44:15.660]   they might be less irritating.
[01:44:15.660 --> 01:44:19.340]   But I'm not convinced they would be as sort of magical
[01:44:19.340 --> 01:44:22.900]   or produce the kinds of things that Twitter does.
[01:44:22.900 --> 01:44:25.180]   But less frustrating, probably.
[01:44:25.180 --> 01:44:28.620]   Moving along, I noticed on your having a deep conversation
[01:44:28.620 --> 01:44:32.300]   on Twitter, of all things, with Jason Calicanis
[01:44:32.300 --> 01:44:36.060]   about the sexiest story of the week.
[01:44:36.060 --> 01:44:38.660]   This broken originally with a blind item--
[01:44:38.660 --> 01:44:41.220]   I think it was in the New York Times--
[01:44:41.220 --> 01:44:45.820]   saying that some people, including Dan Denton of Gawker--
[01:44:45.820 --> 01:44:47.140]   Denton.
[01:44:47.140 --> 01:44:47.660]   Denton.
[01:44:47.660 --> 01:44:49.140]   I know I say his name wrong because I
[01:44:49.140 --> 01:44:51.020]   don't want to give many publicity.
[01:44:51.020 --> 01:44:54.380]   So Freddie Mercury of the Gawker--
[01:44:54.380 --> 01:44:55.220]   No, all right.
[01:44:55.220 --> 01:44:57.940]   Yeah, Nick Denton.
[01:44:57.940 --> 01:45:00.380]   I really have a problem saying his name.
[01:45:00.380 --> 01:45:03.380]   It kind of sticks in my mouth.
[01:45:03.380 --> 01:45:07.020]   Or maybe it's my craw more properly.
[01:45:07.020 --> 01:45:11.060]   Nick Denton, in an interview with Nick Denton,
[01:45:11.060 --> 01:45:14.060]   the New York Times, I think it was, said he thought--
[01:45:14.060 --> 01:45:15.620]   and maybe it's true-- that somebody's
[01:45:15.620 --> 01:45:19.260]   funding Hulk Hogan's lawsuits against Gawker--
[01:45:19.260 --> 01:45:23.260]   Hogan, of course, won $140 million lawsuit for Gawker
[01:45:23.260 --> 01:45:25.420]   showing his sex tape.
[01:45:25.420 --> 01:45:27.300]   There's a long, stupid story there.
[01:45:27.300 --> 01:45:30.060]   And of course, those of us in the media
[01:45:30.060 --> 01:45:32.380]   are very conflicted about this because on the one hand,
[01:45:32.380 --> 01:45:33.900]   freedom of the press, blah, blah, blah,
[01:45:33.900 --> 01:45:36.980]   but on the other hand, Nick Denton is Satan.
[01:45:36.980 --> 01:45:38.500]   So--
[01:45:38.500 --> 01:45:40.100]   You know, when the problem is--
[01:45:40.100 --> 01:45:41.260]   Or one of his lieutenant.
[01:45:41.260 --> 01:45:43.020]   At least one of his actual minions.
[01:45:43.020 --> 01:45:43.980]   Yeah.
[01:45:43.980 --> 01:45:45.820]   So I don't know how you feel about Nick Denton,
[01:45:45.820 --> 01:45:47.860]   but there's no love loss.
[01:45:47.860 --> 01:45:48.700]   I like him.
[01:45:48.700 --> 01:45:49.860]   I'm sure he's very likable.
[01:45:49.860 --> 01:45:50.420]   I have no--
[01:45:50.420 --> 01:45:51.180]   He's extremely like him.
[01:45:51.180 --> 01:45:52.940]   Interesting meeting him.
[01:45:52.940 --> 01:45:55.460]   But the latest is-- and this comes from a deal book--
[01:45:55.460 --> 01:46:00.780]   that, in fact, that Silicon Valley financier is Peter Teal.
[01:46:00.780 --> 01:46:01.300]   The guy--
[01:46:01.300 --> 01:46:02.260]   I see Forbes broke that.
[01:46:02.260 --> 01:46:03.980]   Forbes did break it.
[01:46:03.980 --> 01:46:04.740]   And then Andrew--
[01:46:04.740 --> 01:46:05.220]   And now--
[01:46:05.220 --> 01:46:06.820]   And now Andrew Russ Sorkin is saying it.
[01:46:06.820 --> 01:46:07.460]   Confirmed it.
[01:46:07.460 --> 01:46:07.820]   Confirmed.
[01:46:07.820 --> 01:46:10.100]   Because he was one who wrote the original blind item.
[01:46:10.100 --> 01:46:11.740]   And I have a feeling-- I don't know this,
[01:46:11.740 --> 01:46:15.980]   but I have a feeling that Peter Teal confirmed that.
[01:46:15.980 --> 01:46:17.460]   Yeah, it sure seems to be the except--
[01:46:17.460 --> 01:46:20.100]   Because that's just the kind of thing Peter Teal would do.
[01:46:20.100 --> 01:46:21.540]   And it makes a lot of sense.
[01:46:21.540 --> 01:46:23.940]   Because Teal was outed when he was still
[01:46:23.940 --> 01:46:25.460]   closeted by Gawker.
[01:46:25.460 --> 01:46:27.500]   I mean, that's enough to make you angry.
[01:46:27.500 --> 01:46:29.820]   Teal, of course, the financier who--
[01:46:29.820 --> 01:46:32.140]   if you watched the social network movie,
[01:46:32.140 --> 01:46:36.220]   and you know, was the first to give Facebook any venture
[01:46:36.220 --> 01:46:37.340]   funding, made quite a bit of money.
[01:46:37.340 --> 01:46:40.620]   And that's a very, very well-known venture capitalist in--
[01:46:40.620 --> 01:46:42.140]   Created a thing called PayPal.
[01:46:42.140 --> 01:46:43.940]   You've heard of PayPal before?
[01:46:43.940 --> 01:46:44.820]   Yeah.
[01:46:44.820 --> 01:46:47.540]   He, Elon, that-- the gang.
[01:46:47.540 --> 01:46:48.540]   So--
[01:46:48.540 --> 01:46:49.860]   I think they call him Amafia.
[01:46:49.860 --> 01:46:51.060]   The PayPal mafia.
[01:46:51.060 --> 01:46:52.700]   Yeah.
[01:46:52.700 --> 01:46:56.300]   So you're going back and forth with Jason Calicanis,
[01:46:56.300 --> 01:47:00.020]   and Calicanis is saying, what?
[01:47:00.020 --> 01:47:00.980]   This is fine.
[01:47:00.980 --> 01:47:02.900]   Peter Teal should give Hulk Hogan money
[01:47:02.900 --> 01:47:04.900]   to pursue his lawsuit and others.
[01:47:04.900 --> 01:47:07.780]   Let's bring down Gawker.
[01:47:07.780 --> 01:47:10.380]   Yeah, he wasn't exactly saying that.
[01:47:10.380 --> 01:47:10.660]   But--
[01:47:10.660 --> 01:47:12.820]   It's Twitter, so I can misquote him.
[01:47:12.820 --> 01:47:13.820]   Yeah.
[01:47:13.820 --> 01:47:18.060]   Certainly, lots of people in my stream were saying that.
[01:47:18.060 --> 01:47:21.980]   And so it's a popular viewpoint.
[01:47:21.980 --> 01:47:25.380]   I mean, the point I was trying to make--
[01:47:25.380 --> 01:47:28.780]   and I actually got into it a bit with Keith Rabois,
[01:47:28.780 --> 01:47:30.940]   I think is how you pronounce his name--
[01:47:30.940 --> 01:47:34.780]   who's also a PayPal mafia member.
[01:47:34.780 --> 01:47:38.100]   The point I was trying to make is that if the First Amendment
[01:47:38.100 --> 01:47:43.820]   means anything, it's supposed to protect speech and the press,
[01:47:43.820 --> 01:47:45.740]   even when it's offensive.
[01:47:45.740 --> 01:47:48.860]   If it only protected things that we agreed with,
[01:47:48.860 --> 01:47:51.660]   written by people that we like, then it
[01:47:51.660 --> 01:47:53.820]   wouldn't function properly.
[01:47:53.820 --> 01:47:57.020]   So if you remember Jerry Falwell and Hustler Magazine,
[01:47:57.020 --> 01:47:58.620]   one of famous First Amendment case,
[01:47:58.620 --> 01:48:04.260]   even though Larry Flint was a flaming trash fire
[01:48:04.260 --> 01:48:07.460]   of a human being, and Hustler was a horrible, horrible,
[01:48:07.460 --> 01:48:10.700]   worse than Gawker times 1,000.
[01:48:10.700 --> 01:48:13.740]   And Jerry Falwell was also kind of a horrible human being.
[01:48:13.740 --> 01:48:19.580]   So you had-- but the point was that a satire of Falwell
[01:48:19.580 --> 01:48:21.180]   was protected by the First Amendment,
[01:48:21.180 --> 01:48:25.060]   even though it was incredibly offensive and horrible,
[01:48:25.060 --> 01:48:29.380]   in all kinds of obscene ways.
[01:48:29.380 --> 01:48:32.700]   And so my point is not--
[01:48:32.700 --> 01:48:37.860]   maybe Nick Denton is not the media person we
[01:48:37.860 --> 01:48:40.820]   would like to hold up as a symbol of everything
[01:48:40.820 --> 01:48:43.020]   that journalists stands for.
[01:48:43.020 --> 01:48:47.820]   But that's not where First Amendment cases,
[01:48:47.820 --> 01:48:49.060]   where the rubber hits the road.
[01:48:49.060 --> 01:48:52.220]   It's where the person is unpleasant.
[01:48:52.220 --> 01:48:54.100]   The thing they wrote is offensive.
[01:48:54.100 --> 01:48:57.180]   And the person who's criticizing it kind of has a point.
[01:48:57.180 --> 01:48:58.460]   Yeah, you can have that point of view
[01:48:58.460 --> 01:49:01.020]   and still root for Peter Thiel.
[01:49:01.020 --> 01:49:01.620]   Can you?
[01:49:01.620 --> 01:49:02.460]   Yeah.
[01:49:02.460 --> 01:49:03.140]   Absolutely.
[01:49:03.140 --> 01:49:04.180]   I don't think so.
[01:49:04.180 --> 01:49:05.540]   He's trying to create a chill--
[01:49:05.540 --> 01:49:08.140]   I mean, it's the whole idea of a chilling effect
[01:49:08.140 --> 01:49:10.140]   on how people report.
[01:49:10.140 --> 01:49:10.620]   Exactly.
[01:49:10.620 --> 01:49:13.260]   Well, there's certain things that should be chilled.
[01:49:13.260 --> 01:49:14.260]   No.
[01:49:14.260 --> 01:49:14.900]   Well--
[01:49:14.900 --> 01:49:15.660]   No?
[01:49:15.660 --> 01:49:16.100]   Really?
[01:49:16.100 --> 01:49:17.740]   But who decides what those things are?
[01:49:17.740 --> 01:49:18.740]   We have lost it.
[01:49:18.740 --> 01:49:21.140]   Should Peter Thiel decide what those things are?
[01:49:21.140 --> 01:49:22.060]   Oh, the court should.
[01:49:22.060 --> 01:49:25.700]   Peter Thiel could make it very painful for Nick Denton.
[01:49:25.700 --> 01:49:26.020]   Right.
[01:49:26.020 --> 01:49:28.660]   So let's say if the case succeeds,
[01:49:28.660 --> 01:49:30.820]   Peter Thiel will have bankrupted Denton.
[01:49:30.820 --> 01:49:31.100]   No.
[01:49:31.100 --> 01:49:33.380]   And even if the case fails, he's done
[01:49:33.380 --> 01:49:35.300]   what he wants to do, which is cost debt in millions
[01:49:35.300 --> 01:49:36.220]   and millions of dollars.
[01:49:36.220 --> 01:49:38.100]   Denton had to go out and get an investor
[01:49:38.100 --> 01:49:39.580]   to pay his legal costs.
[01:49:39.580 --> 01:49:41.780]   So there's some pain there.
[01:49:41.780 --> 01:49:42.580]   Right.
[01:49:42.580 --> 01:49:45.180]   So if Peter Thiel-- let's say he takes exception
[01:49:45.180 --> 01:49:48.500]   to something someone else writes about Silicon Valley
[01:49:48.500 --> 01:49:49.940]   or one of these investments--
[01:49:49.940 --> 01:49:52.100]   he's got enough money to crush them as well.
[01:49:52.100 --> 01:49:52.620]   I have to do it.
[01:49:52.620 --> 01:49:54.980]   So he's using the court system for personal vendetta.
[01:49:54.980 --> 01:49:55.860]   No, I have to agree with you.
[01:49:55.860 --> 01:49:57.220]   It's like Charlie Ergud.
[01:49:57.220 --> 01:49:59.060]   I still read and for him.
[01:49:59.060 --> 01:50:01.660]   And look, we already have another example.
[01:50:01.660 --> 01:50:04.100]   A billionaire sued Mother Jones.
[01:50:04.100 --> 01:50:05.980]   Luckily, he lost.
[01:50:05.980 --> 01:50:08.820]   But I think if a billionaire succeeds,
[01:50:08.820 --> 01:50:11.140]   other billionaires will be like, hey, that's a good idea.
[01:50:11.140 --> 01:50:14.260]   I should sue this guy who wrote something I don't like.
[01:50:14.260 --> 01:50:16.060]   It's not against the law for him to do this stuff.
[01:50:16.060 --> 01:50:16.560]   Right.
[01:50:16.560 --> 01:50:17.300]   That's what I asked.
[01:50:17.300 --> 01:50:18.940]   Just to answer that, I mean, it's not against the law.
[01:50:18.940 --> 01:50:21.500]   So you can go ahead and give money,
[01:50:21.500 --> 01:50:22.620]   pay for someone's court fees.
[01:50:22.620 --> 01:50:24.620]   I mean, Donald Trump's promising to do that all the time
[01:50:24.620 --> 01:50:27.500]   with the people that get into trouble at his rallies
[01:50:27.500 --> 01:50:28.500]   and stuff, right?
[01:50:28.500 --> 01:50:30.220]   I mean, anybody can say, oh, yeah, here,
[01:50:30.220 --> 01:50:31.340]   I'll pay for your leave money.
[01:50:31.340 --> 01:50:33.580]   And there's nothing wrong with that either.
[01:50:33.580 --> 01:50:38.260]   But what's disturbing is that Peter Thiel did this in secret.
[01:50:38.260 --> 01:50:41.140]   So if he had just come out and said,
[01:50:41.140 --> 01:50:42.580]   I'm against Nick Denton and Gawker.
[01:50:42.580 --> 01:50:44.620]   I'm going to crush this company.
[01:50:44.620 --> 01:50:48.140]   I'm going to fund anybody's lawsuit who's suing them.
[01:50:48.140 --> 01:50:50.460]   I would actually feel better about that.
[01:50:50.460 --> 01:50:53.260]   And as Simon Owens pointed out, as a coder
[01:50:53.260 --> 01:50:55.260]   to your conversation with Jason Calicanis,
[01:50:55.260 --> 01:50:58.020]   plenty of examples of mainstream media companies
[01:50:58.020 --> 01:51:01.860]   not running a story out of fear of a litigious billionaire.
[01:51:01.860 --> 01:51:03.500]   We wouldn't want that.
[01:51:03.500 --> 01:51:05.860]   And that is a chilling effect, right?
[01:51:05.860 --> 01:51:08.500]   Si.
[01:51:08.500 --> 01:51:10.220]   Can I just assassinate Nick Denton?
[01:51:10.220 --> 01:51:12.060]   Would that be OK?
[01:51:12.060 --> 01:51:13.900]   Pretty sure that's against the law, man.
[01:51:13.900 --> 01:51:16.820]   That's a different court case.
[01:51:16.820 --> 01:51:18.500]   Might be worth it.
[01:51:18.500 --> 01:51:19.020]   All right, let's--
[01:51:19.020 --> 01:51:24.660]   You don't have to like him, Leo.
[01:51:24.660 --> 01:51:25.900]   No one's asking you to like him.
[01:51:25.900 --> 01:51:29.140]   Well, you've been his victim twice.
[01:51:29.140 --> 01:51:33.900]   And you might-- the problem is I completely agree.
[01:51:33.900 --> 01:51:38.260]   But it does change the tenor of the conversation a little bit.
[01:51:38.260 --> 01:51:42.380]   And the problem is that the video clip of Hulk Hogan is--
[01:51:42.380 --> 01:51:44.020]   I said this to Keith.
[01:51:44.020 --> 01:51:48.140]   It's-- it is right on the line of things that--
[01:51:48.140 --> 01:51:50.780]   Well, for instance, what if he'd done a written description
[01:51:50.780 --> 01:51:51.500]   of the video?
[01:51:51.500 --> 01:51:55.540]   It would have accomplished the same journalistic benefit.
[01:51:55.540 --> 01:51:56.580]   They did.
[01:51:56.580 --> 01:51:58.780]   And they included a nine second clip.
[01:51:58.780 --> 01:52:01.180]   Well, did they have to include the nine second clip?
[01:52:01.180 --> 01:52:01.860]   No.
[01:52:01.860 --> 01:52:03.780]   So they probably didn't.
[01:52:03.780 --> 01:52:06.100]   But that's-- that isn't a judgment.
[01:52:06.100 --> 01:52:06.700]   No, I know.
[01:52:06.700 --> 01:52:08.140]   They still have the right to do it.
[01:52:08.140 --> 01:52:08.740]   That's the right to do it.
[01:52:08.740 --> 01:52:09.220]   Right.
[01:52:09.220 --> 01:52:18.660]   I know.
[01:52:18.660 --> 01:52:21.060]   It's a stupid thing to get upset about.
[01:52:21.060 --> 01:52:22.420]   It's actually a great-- you know what?
[01:52:22.420 --> 01:52:24.780]   It's a great thing to talk about because it really
[01:52:24.780 --> 01:52:26.940]   asserts the issue of free speech.
[01:52:26.940 --> 01:52:27.620]   And you're right.
[01:52:27.620 --> 01:52:29.180]   There's no point in defending free speech if it's
[01:52:29.180 --> 01:52:30.300]   speech you agree with.
[01:52:30.300 --> 01:52:30.820]   That's meaningless.
[01:52:30.820 --> 01:52:32.220]   And I'm trying to think of who said it,
[01:52:32.220 --> 01:52:35.300]   the First Amendment experts said, you know,
[01:52:35.300 --> 01:52:39.820]   we never get the First Amendment case that we want, the one
[01:52:39.820 --> 01:52:42.020]   where we can defend 100%--
[01:52:42.020 --> 01:52:43.340]   So is Nazis or something.
[01:52:43.340 --> 01:52:46.140]   Shining light, and everyone loves them.
[01:52:46.140 --> 01:52:49.220]   And the person who's fighting them is clearly evil.
[01:52:49.220 --> 01:52:51.580]   You just never get that.
[01:52:51.580 --> 01:52:52.700]   No, it's meaningless unless you're
[01:52:52.700 --> 01:52:55.780]   willing to represent and defend speech that you hate.
[01:53:01.780 --> 01:53:03.700]   Let's take a break.
[01:53:03.700 --> 01:53:04.460]   Oh, this is fun.
[01:53:04.460 --> 01:53:09.580]   Stacey Higginbotham is here from StaceyOnIOT.com.
[01:53:09.580 --> 01:53:11.500]   You have a podcast with Kevin Tofel.
[01:53:11.500 --> 01:53:17.460]   We love Kevin, too, at StaceyOnIOT.com.
[01:53:17.460 --> 01:53:21.780]   Still have the old gigastacey handle I see on the Twitter.
[01:53:21.780 --> 01:53:22.500]   That's me?
[01:53:22.500 --> 01:53:23.340]   Why change it?
[01:53:23.340 --> 01:53:25.140]   I can't-- the white change something so good.
[01:53:25.140 --> 01:53:27.340]   It's such a good name.
[01:53:27.340 --> 01:53:28.780]   It's such a good name.
[01:53:28.780 --> 01:53:29.500]   Aaron Newcomb here.
[01:53:29.500 --> 01:53:32.340]   He's going to show us one of his latest projects.
[01:53:32.340 --> 01:53:36.060]   Aaron from NetApp, but also the Benisha Maker Space.
[01:53:36.060 --> 01:53:36.500]   That's right.
[01:53:36.500 --> 01:53:38.820]   Inspiring and empowering all ages.
[01:53:38.820 --> 01:53:39.740]   We're in my shirt today.
[01:53:39.740 --> 01:53:40.380]   We're in his shirt.
[01:53:40.380 --> 01:53:41.140]   Representing.
[01:53:41.140 --> 01:53:42.380]   Always wear your shirt.
[01:53:42.380 --> 01:53:43.780]   I didn't wear my shirt over the weekend.
[01:53:43.780 --> 01:53:45.100]   It was a big mistake.
[01:53:45.100 --> 01:53:47.100]   And Matthew Ingram is here.
[01:53:47.100 --> 01:53:50.860]   He is with fortune.com.
[01:53:50.860 --> 01:53:52.780]   Our show today brought to you by something Aaron's
[01:53:52.780 --> 01:53:55.220]   going to make himself for you.
[01:53:55.220 --> 01:53:55.780]   Yes.
[01:53:55.780 --> 01:53:56.980]   This is the easy way.
[01:53:56.980 --> 01:53:59.100]   You don't have to be a maker to use the Ring Video
[01:53:59.100 --> 01:53:59.940]   Doorbell.
[01:53:59.940 --> 01:54:02.540]   I mean, I installed it and I'm no maker.
[01:54:02.540 --> 01:54:05.660]   It's an easy thing to install, but it changes your experience
[01:54:05.660 --> 01:54:07.980]   of your doorbell.
[01:54:07.980 --> 01:54:09.940]   Everywhere ever lived, the doorbells
[01:54:09.940 --> 01:54:12.460]   would kind of this funky thing hanging off the door jam.
[01:54:12.460 --> 01:54:14.100]   And it goes, donk, donk.
[01:54:14.100 --> 01:54:15.500]   The chimes never really work.
[01:54:15.500 --> 01:54:16.940]   So I was ready for an upgrade.
[01:54:16.940 --> 01:54:18.180]   I always miss the doorbell.
[01:54:18.180 --> 01:54:21.300]   And I like to know when the UPS guy comes looking forward
[01:54:21.300 --> 01:54:22.900]   to my presence.
[01:54:22.900 --> 01:54:24.900]   Then I found out about the Ring Video Doorbell.
[01:54:24.900 --> 01:54:27.940]   This easily installed doorbell takes over from your old doorbell.
[01:54:27.940 --> 01:54:30.540]   Wired or not, by the way, because it has a big old lithium
[01:54:30.540 --> 01:54:31.260]   and I have battery.
[01:54:31.260 --> 01:54:34.540]   And it can go full year on one charge.
[01:54:34.540 --> 01:54:36.380]   And it was-- but I had wired.
[01:54:36.380 --> 01:54:37.420]   So it was an easy thing to do.
[01:54:37.420 --> 01:54:38.940]   I unscrewed my old doorbell.
[01:54:38.940 --> 01:54:40.340]   Happy to see it go.
[01:54:40.340 --> 01:54:42.180]   So I had drips of paint on it from the last time
[01:54:42.180 --> 01:54:43.940]   the house was painted.
[01:54:43.940 --> 01:54:46.140]   Took those two wires coming out of the door jam,
[01:54:46.140 --> 01:54:48.740]   attached to this very attractive Ring Video Doorbell.
[01:54:48.740 --> 01:54:52.300]   Using the tools they provide, including a level and the drill
[01:54:52.300 --> 01:54:53.980]   bit-- everything they-- you don't need a drill bit,
[01:54:53.980 --> 01:54:56.860]   probably, but they provide one.
[01:54:56.860 --> 01:54:57.980]   Because I didn't have to drill.
[01:54:57.980 --> 01:54:58.740]   I was very simple.
[01:54:58.740 --> 01:55:00.980]   I just unscrewed some things, screwed something else back on.
[01:55:00.980 --> 01:55:03.140]   There now, all of a sudden, I've got a camera
[01:55:03.140 --> 01:55:05.940]   where my doorbell is, and a microphone, and a speaker,
[01:55:05.940 --> 01:55:08.340]   and a motion detector.
[01:55:08.340 --> 01:55:11.660]   And this thing, I could show you on my smartphone.
[01:55:11.660 --> 01:55:14.180]   When the doorbell rings, I hear it no matter where I am.
[01:55:14.180 --> 01:55:15.580]   I don't have to-- not only in the house,
[01:55:15.580 --> 01:55:17.460]   but I mean, I'm at work and I can hear a ring.
[01:55:17.460 --> 01:55:19.580]   I can answer it from anywhere over the internet.
[01:55:19.580 --> 01:55:21.620]   It's tied to the Wi-Fi, which means
[01:55:21.620 --> 01:55:23.980]   if somebody comes to the door that I don't like the looks of,
[01:55:23.980 --> 01:55:25.180]   I could say, hey, I'm in the bathroom.
[01:55:25.180 --> 01:55:27.420]   I'm going to write out with my shotgun.
[01:55:27.420 --> 01:55:29.860]   That usually is sufficient.
[01:55:29.860 --> 01:55:32.100]   You know that over 95% of home break-ins
[01:55:32.100 --> 01:55:34.260]   happen during the day.
[01:55:34.260 --> 01:55:36.500]   And burglars almost always start by ringing your doorbell
[01:55:36.500 --> 01:55:38.020]   to see if you're home.
[01:55:38.020 --> 01:55:39.500]   Then they go around back and break in.
[01:55:39.500 --> 01:55:41.740]   Well, now you'll not only have crystal clear HD video
[01:55:41.740 --> 01:55:44.500]   of them ringing your doorbell, but you can scare them off.
[01:55:44.500 --> 01:55:46.300]   Nick Denton, I got a gun.
[01:55:46.300 --> 01:55:47.740]   Get out of here.
[01:55:47.740 --> 01:55:51.020]   You don't go through my garbage anymore.
[01:55:51.020 --> 01:55:53.500]   Stop it.
[01:55:53.500 --> 01:55:55.060]   It's so useful.
[01:55:55.060 --> 01:55:56.260]   It also detects motion.
[01:55:56.260 --> 01:55:58.100]   So you can see people just walking by.
[01:55:58.100 --> 01:56:00.900]   You could set the-- how sensitive the motion detection is.
[01:56:00.900 --> 01:56:02.020]   I love this thing.
[01:56:02.020 --> 01:56:03.660]   I am so happy I got it.
[01:56:03.660 --> 01:56:06.020]   In fact, so happy that we've even given it to family members.
[01:56:06.020 --> 01:56:09.580]   I mean, parents, family, friends, lots of people
[01:56:09.580 --> 01:56:11.260]   really love the Ring Video Doorbell.
[01:56:11.260 --> 01:56:12.580]   Why don't you take a look at it?
[01:56:12.580 --> 01:56:15.900]   Go to ring.com/twig.
[01:56:15.900 --> 01:56:18.580]   And you can get free expedited shipping $199.
[01:56:18.580 --> 01:56:20.300]   It's well worth it.
[01:56:20.300 --> 01:56:22.580]   Makes a really good gift too.
[01:56:22.580 --> 01:56:25.180]   And free expedited shipping for twig viewers
[01:56:25.180 --> 01:56:28.460]   when you go to ring.com/twig.
[01:56:28.460 --> 01:56:30.820]   Sure, you could build your own Clue G. Little thing
[01:56:30.820 --> 01:56:32.620]   hanging off your door jam.
[01:56:32.620 --> 01:56:33.180]   Like me.
[01:56:33.180 --> 01:56:35.100]   You might even want to do that.
[01:56:35.100 --> 01:56:38.180]   But for those of you who don't, there's always ring.
[01:56:38.180 --> 01:56:42.140]   Ring.com/twig.
[01:56:42.140 --> 01:56:43.020]   Time magazine.
[01:56:43.020 --> 01:56:44.580]   I mean, USA Today, rather.
[01:56:44.580 --> 01:56:46.380]   Oh, Time Magazine and USA Today.
[01:56:46.380 --> 01:56:50.020]   Maybe one of their top 10 gadgets last year.
[01:56:50.020 --> 01:56:55.020]   And Richard Branson invested $28 million in it.
[01:56:55.020 --> 01:56:58.020]   Because he saw somebody use it at his island retreat
[01:56:58.020 --> 01:56:59.540]   and said, what's that?
[01:56:59.540 --> 01:57:00.380]   What's a Ring Doorbell?
[01:57:00.380 --> 01:57:02.380]   Oh, I'd like to buy that.
[01:57:02.380 --> 01:57:04.660]   But when Richard Branson says I'd like to buy that,
[01:57:04.660 --> 01:57:07.020]   he's not talking 200 bucks.
[01:57:07.020 --> 01:57:08.700]   Ring.com/twig.
[01:57:08.700 --> 01:57:09.980]   All right.
[01:57:09.980 --> 01:57:12.540]   Let's start.
[01:57:12.540 --> 01:57:15.300]   Stacy's got something that doesn't work.
[01:57:15.300 --> 01:57:17.700]   Yes, I also set up something that does work.
[01:57:17.700 --> 01:57:19.620]   So you can take the pick, which you want.
[01:57:19.620 --> 01:57:21.060]   No, do both.
[01:57:21.060 --> 01:57:22.020]   All right.
[01:57:22.020 --> 01:57:23.500]   So this is my thing that doesn't work.
[01:57:23.500 --> 01:57:26.940]   This is the Pebble B. Pebble B Stone.
[01:57:26.940 --> 01:57:28.620]   This is a tracker.
[01:57:28.620 --> 01:57:30.780]   And let me pull--
[01:57:30.780 --> 01:57:34.340]   I was going to pull up my app for you so you could see it.
[01:57:34.340 --> 01:57:37.980]   And it's got a soft, programmable button on the back.
[01:57:37.980 --> 01:57:41.260]   And if I press it once, it will find my phone for me
[01:57:41.260 --> 01:57:45.060]   by doing an alarm, even if my phone volume is turned off.
[01:57:45.060 --> 01:57:48.380]   And if you do a long press, you could do things like have
[01:57:48.380 --> 01:57:51.700]   to talk to your phone like it'll take a picture.
[01:57:51.700 --> 01:57:54.300]   It's supposed to work with if this than that.
[01:57:54.300 --> 01:57:56.260]   But I haven't gotten that to work.
[01:57:56.260 --> 01:57:58.860]   So I set it up trying to turn my hue lights on and off
[01:57:58.860 --> 01:57:59.820]   with the long press.
[01:57:59.820 --> 01:58:00.540]   And that didn't work.
[01:58:00.540 --> 01:58:03.220]   And then I set it up with the WeMo to turn different light
[01:58:03.220 --> 01:58:04.180]   off and on.
[01:58:04.180 --> 01:58:05.340]   And that didn't work.
[01:58:05.340 --> 01:58:08.660]   So it's $30.
[01:58:08.660 --> 01:58:11.980]   I really don't want to recommend it because so far,
[01:58:11.980 --> 01:58:13.580]   I haven't gotten the functionality that I thought
[01:58:13.580 --> 01:58:14.580]   was so awesome to work.
[01:58:14.580 --> 01:58:16.620]   But there you have it.
[01:58:16.620 --> 01:58:18.540]   But the alarm thing works.
[01:58:18.540 --> 01:58:19.740]   Yeah, you want to hear it?
[01:58:19.740 --> 01:58:20.220]   Yeah, sure.
[01:58:20.220 --> 01:58:22.020]   Here we go.
[01:58:22.020 --> 01:58:23.740]   I just pressed it.
[01:58:23.740 --> 01:58:27.260]   [SCREAMING]
[01:58:27.260 --> 01:58:30.540]   And all the other things, I can set a text.
[01:58:30.540 --> 01:58:32.700]   If I wanted to set it up to do an emergency text
[01:58:32.700 --> 01:58:35.740]   from this thing, if my phone's in my pocket,
[01:58:35.740 --> 01:58:36.780]   I mean, it's Bluetooth.
[01:58:36.780 --> 01:58:38.860]   So it needs to be near your phone.
[01:58:38.860 --> 01:58:42.140]   But I could be like, help, I'm being attacked.
[01:58:42.140 --> 01:58:44.700]   I would make that the short press not the long press, though.
[01:58:44.700 --> 01:58:46.580]   Help me, help me.
[01:58:46.580 --> 01:58:48.820]   So that sort of works.
[01:58:48.820 --> 01:58:49.820]   It does work.
[01:58:49.820 --> 01:58:50.820]   It does work.
[01:58:50.820 --> 01:58:52.340]   Yeah, part of it.
[01:58:52.340 --> 01:58:55.620]   Because I'm a person who likes to play with my house,
[01:58:55.620 --> 01:58:58.580]   I was really keen on the whole button thing.
[01:58:58.580 --> 01:59:00.100]   So disappointing.
[01:59:00.100 --> 01:59:01.620]   Maybe I'll figure out what's going on.
[01:59:01.620 --> 01:59:03.540]   It happens a lot, doesn't it, though, right?
[01:59:03.540 --> 01:59:05.540]   Things kind of 90% work.
[01:59:05.540 --> 01:59:06.540]   It kind of works.
[01:59:06.540 --> 01:59:08.540]   It kind of works.
[01:59:08.540 --> 01:59:09.700]   So you could be--
[01:59:09.700 --> 01:59:11.260]   That's what you need us for.
[01:59:11.260 --> 01:59:13.180]   Could it be if this, then that?
[01:59:13.180 --> 01:59:14.260]   Could it be that integration?
[01:59:14.260 --> 01:59:16.900]   So it does trigger to ifd.
[01:59:16.900 --> 01:59:18.660]   And then it goes away.
[01:59:18.660 --> 01:59:20.060]   And so it could be that.
[01:59:20.060 --> 01:59:22.620]   But my other if this, then that integrations with you
[01:59:22.620 --> 01:59:23.580]   and wemo work.
[01:59:23.580 --> 01:59:29.420]   So it could see this is where things get lost in the cloud.
[01:59:29.420 --> 01:59:30.540]   So where they--
[01:59:30.540 --> 01:59:33.980]   You could also just take one of the Amazon dash buttons,
[01:59:33.980 --> 01:59:35.740]   one of the guys that are maker spaces, hack that,
[01:59:35.740 --> 01:59:36.740]   so that it works.
[01:59:36.740 --> 01:59:37.900]   They've actually opened it up.
[01:59:37.900 --> 01:59:38.400]   Yeah.
[01:59:38.400 --> 01:59:39.460]   And they've all been--
[01:59:39.460 --> 01:59:40.300]   You can--
[01:59:40.300 --> 01:59:41.380]   That's $20.
[01:59:41.380 --> 01:59:43.340]   Yeah, it's kind of expensive for a button.
[01:59:43.340 --> 01:59:44.340]   It's $20.
[01:59:44.340 --> 01:59:45.340]   Yeah, I've got--
[01:59:45.340 --> 01:59:47.180]   And it does never replaceable battery,
[01:59:47.180 --> 01:59:48.860]   or does the new one have a replaceable battery?
[01:59:48.860 --> 01:59:49.620]   It does not.
[01:59:49.620 --> 01:59:50.120]   Yeah.
[01:59:50.120 --> 01:59:50.620]   It still does.
[01:59:50.620 --> 01:59:52.100]   Thousand presses.
[01:59:52.100 --> 01:59:54.060]   So you can make a duetling.
[01:59:54.060 --> 01:59:54.660]   They opened up the--
[01:59:54.660 --> 01:59:55.300]   Yes.
[01:59:55.300 --> 01:59:57.940]   Yeah, you can buy a kind of all-purpose dash.
[01:59:57.940 --> 01:59:59.540]   Right.
[01:59:59.540 --> 02:00:00.460]   Yeah.
[02:00:00.460 --> 02:00:01.060]   You can make it.
[02:00:01.060 --> 02:00:03.460]   So that's cool.
[02:00:03.460 --> 02:00:05.260]   OK, so the thing that--
[02:00:05.260 --> 02:00:07.540]   I've talked about this before, so this may not be new,
[02:00:07.540 --> 02:00:10.700]   but I'm running an app called Roger, not the Evil Telecom
[02:00:10.700 --> 02:00:11.860]   Company.
[02:00:11.860 --> 02:00:13.300]   It's Get Roger.
[02:00:13.300 --> 02:00:15.020]   It's a walkie-talkie app.
[02:00:15.020 --> 02:00:19.340]   But what's cool about it is I set it up to talk to my Echo.
[02:00:19.340 --> 02:00:20.300]   I don't know.
[02:00:20.300 --> 02:00:22.380]   I'm terrible focusing.
[02:00:22.380 --> 02:00:23.740]   Just where?
[02:00:23.740 --> 02:00:24.700]   So here we go.
[02:00:24.700 --> 02:00:29.700]   I screwed that up.
[02:00:29.700 --> 02:00:31.860]   Cover your Echo's ears.
[02:00:31.860 --> 02:00:33.820]   Yes.
[02:00:33.820 --> 02:00:38.500]   Alexa, turn on master bedroom.
[02:00:38.500 --> 02:00:41.300]   Can everyone have one that says just turn on master?
[02:00:41.300 --> 02:00:42.340]   Did you see that happen?
[02:00:42.340 --> 02:00:43.340]   Yeah.
[02:00:43.340 --> 02:00:44.540]   Me?
[02:00:44.540 --> 02:00:47.580]   So you don't have to buy the $30 remote.
[02:00:47.580 --> 02:00:49.420]   You can control it from your phone.
[02:00:49.420 --> 02:00:51.500]   Is that your we-mo doing that?
[02:00:51.500 --> 02:00:53.100]   That's actually a hue.
[02:00:53.100 --> 02:00:53.940]   That's hue doing that.
[02:00:53.940 --> 02:00:54.380]   OK.
[02:00:54.380 --> 02:00:55.620]   It's hue through the Echo.
[02:00:55.620 --> 02:00:57.380]   So it's talking to my actual--
[02:00:57.380 --> 02:00:58.460]   Yeah.
[02:00:58.460 --> 02:01:00.300]   So yeah, and that's free.
[02:01:00.300 --> 02:01:00.780]   Yay!
[02:01:00.780 --> 02:01:02.780]   Need a walkie-talkie.
[02:01:02.780 --> 02:01:05.340]   Free if you have a $200 Amazon Echo.
[02:01:05.340 --> 02:01:06.900]   And the hue lights.
[02:01:06.900 --> 02:01:09.060]   And the $60 light hue lights.
[02:01:09.060 --> 02:01:10.540]   But then it's free.
[02:01:10.540 --> 02:01:14.260]   It saves you the $30 extra remote.
[02:01:14.260 --> 02:01:15.500]   No, I like it.
[02:01:15.500 --> 02:01:16.220]   That's another thing.
[02:01:16.220 --> 02:01:18.860]   I have a lot of hue lights that I never screwed back in when
[02:01:18.860 --> 02:01:19.660]   we moved.
[02:01:19.660 --> 02:01:20.660]   It's like I got a box.
[02:01:20.660 --> 02:01:24.980]   I got the whole SmartThings kit, the hue lights, the nests.
[02:01:24.980 --> 02:01:28.540]   And I just got-- I don't really--
[02:01:28.540 --> 02:01:31.140]   it's cool.
[02:01:31.140 --> 02:01:31.980]   I don't really need it.
[02:01:31.980 --> 02:01:33.780]   And like it enough to reinstall everything.
[02:01:33.780 --> 02:01:34.260]   I guess it's--
[02:01:34.260 --> 02:01:34.540]   Nobody's yet.
[02:01:34.540 --> 02:01:35.100]   I feel bad.
[02:01:35.100 --> 02:01:36.300]   It needs the trouble.
[02:01:36.300 --> 02:01:37.140]   Nobody needs that.
[02:01:37.140 --> 02:01:38.820]   Now this you need.
[02:01:38.820 --> 02:01:40.300]   This is a new from Logitech.
[02:01:40.300 --> 02:01:41.540]   It's called their ZeroTouch.
[02:01:41.540 --> 02:01:43.660]   And I just saw it.
[02:01:43.660 --> 02:01:45.700]   Like, maybe I saw it.
[02:01:45.700 --> 02:01:46.700]   Now this isn't it.
[02:01:46.700 --> 02:01:52.300]   That's the dash Amazon Dash Internet of Things version.
[02:01:52.300 --> 02:01:56.060]   The ZeroTouch is kind of an interesting idea.
[02:01:56.060 --> 02:01:59.700]   And I'm not sure really why I like it, but I do.
[02:01:59.700 --> 02:02:00.540]   So here's the thing.
[02:02:00.540 --> 02:02:04.340]   It's a little dotty kind of thing that there's two of them.
[02:02:04.340 --> 02:02:06.660]   There's one that you can put on your dash.
[02:02:06.660 --> 02:02:10.660]   And there's one that I bought that you could put in a vent.
[02:02:10.660 --> 02:02:13.100]   And if you-- Carson show me, because I've got it right here.
[02:02:13.100 --> 02:02:15.940]   So it's a big magnet is what it is.
[02:02:15.940 --> 02:02:17.740]   I'm thinking an RF--
[02:02:17.740 --> 02:02:18.500]   I don't know what's in it.
[02:02:18.500 --> 02:02:19.700]   Maybe RFID or Bluetooth.
[02:02:19.700 --> 02:02:21.060]   I don't know.
[02:02:21.060 --> 02:02:23.740]   Then you put-- and they offer in the box,
[02:02:23.740 --> 02:02:27.340]   they offer two different kinds of metal plates, one big round one.
[02:02:27.340 --> 02:02:28.020]   And then this one.
[02:02:28.020 --> 02:02:30.180]   And these are just magnetic plates, right?
[02:02:30.180 --> 02:02:32.620]   And you're supposed to put them not anywhere near where
[02:02:32.620 --> 02:02:35.780]   your charging or your NFC would be.
[02:02:35.780 --> 02:02:37.340]   So I put it down at the bottom of the phone.
[02:02:37.340 --> 02:02:41.500]   And it's a good, powerful magnet.
[02:02:41.500 --> 02:02:42.620]   Do you have to glue it on there?
[02:02:42.620 --> 02:02:43.340]   No.
[02:02:43.340 --> 02:02:46.020]   Because this goes right on my vent.
[02:02:46.020 --> 02:02:47.260]   I push it on a vent.
[02:02:47.260 --> 02:02:48.020]   You could glue it.
[02:02:48.020 --> 02:02:48.620]   OK.
[02:02:48.620 --> 02:02:50.180]   You had to glue the magnet to the back, right?
[02:02:50.180 --> 02:02:51.660]   Yeah, you had to glue the magnets back.
[02:02:51.660 --> 02:02:52.180]   But you can peel it.
[02:02:52.180 --> 02:02:53.100]   Yeah, sorry, that's what I meant.
[02:02:53.100 --> 02:02:55.700]   So did you notice that as soon as I attached it,
[02:02:55.700 --> 02:02:58.780]   I'll do it again, the Logitech-- you have an app on here.
[02:02:58.780 --> 02:03:01.380]   The Logitech ZeroTouch app woke up.
[02:03:01.380 --> 02:03:02.700]   And then went away.
[02:03:02.700 --> 02:03:06.380]   Now, Android only right now, if you have Android,
[02:03:06.380 --> 02:03:09.180]   you know that most newer versions of Android, you could say,
[02:03:09.180 --> 02:03:11.020]   oh, it automatically be unlocked when
[02:03:11.020 --> 02:03:12.900]   you're paired to this device.
[02:03:12.900 --> 02:03:14.100]   So that's set up.
[02:03:14.100 --> 02:03:16.660]   So now, if I wave my hand in front-- this whole idea
[02:03:16.660 --> 02:03:18.060]   is you don't have to touch it.
[02:03:18.060 --> 02:03:21.060]   You're going to wave your hand in front of these.
[02:03:21.060 --> 02:03:23.140]   Did you hear it said, hey, Leo, what's up?
[02:03:23.140 --> 02:03:25.020]   And I can now command it.
[02:03:25.020 --> 02:03:28.020]   It can be tied to your Bluetooth so it can come through your speakers.
[02:03:28.020 --> 02:03:30.940]   It does a lot of different things.
[02:03:30.940 --> 02:03:34.420]   So the obvious immediately, obvious ones are navigation,
[02:03:34.420 --> 02:03:37.380]   and you could tie it to your Google Maps or Waze.
[02:03:37.380 --> 02:03:40.980]   It'll launch music and Spotify or Pandora.
[02:03:40.980 --> 02:03:43.980]   When text messages come in, it will say, oh, you just
[02:03:43.980 --> 02:03:45.500]   got a text from Lisa.
[02:03:45.500 --> 02:03:48.020]   It'll read you the text to say you want to reply.
[02:03:48.020 --> 02:03:49.420]   All the stuff that you would kind of
[02:03:49.420 --> 02:03:53.100]   do with their variety of different tools that will do this.
[02:03:53.100 --> 02:03:56.900]   Apps that will do this kind of would do with OK Google.
[02:03:56.900 --> 02:03:58.580]   But it does it kind of automatically,
[02:03:58.580 --> 02:04:00.380]   just because you wave your hand in front of it
[02:04:00.380 --> 02:04:02.140]   while you're driving.
[02:04:02.140 --> 02:04:04.220]   And it also will send a glimpse.
[02:04:04.220 --> 02:04:06.500]   So I could say, I'm sending a glimpse to Lisa.
[02:04:06.500 --> 02:04:07.260]   I'm running late.
[02:04:07.260 --> 02:04:10.340]   And it will send the little text message with a map on it
[02:04:10.340 --> 02:04:13.180]   that's updated automatically about where I am.
[02:04:13.180 --> 02:04:16.940]   You can send text messages and emails to just the same as you
[02:04:16.940 --> 02:04:18.260]   would on OK Google.
[02:04:18.260 --> 02:04:19.620]   Send a text to Lisa.
[02:04:19.620 --> 02:04:21.180]   I'm running late.
[02:04:21.180 --> 02:04:23.820]   It does a lot of different stuff.
[02:04:23.820 --> 02:04:25.700]   And what I like is it's just kind of--
[02:04:25.700 --> 02:04:31.900]   it's really a little easy $60 do-hickey that attaches your phone
[02:04:31.900 --> 02:04:33.220]   to your dash.
[02:04:33.220 --> 02:04:36.620]   But then, because of the software, I really like that.
[02:04:36.620 --> 02:04:39.740]   I just feel like this is a really cool thing, $59.99
[02:04:39.740 --> 02:04:42.940]   for the AirVent version, $79.99 for the one that
[02:04:42.940 --> 02:04:45.140]   glues to your dashboard.
[02:04:45.140 --> 02:04:46.580]   And actually, I didn't get that kind.
[02:04:46.580 --> 02:04:50.140]   So I don't know exactly how it works.
[02:04:50.140 --> 02:04:54.860]   And so unfortunately, because iOS doesn't allow you to do
[02:04:54.860 --> 02:04:57.300]   this kind of integration, it has to be Android.
[02:04:57.300 --> 02:04:58.540]   Hands-free calling as well.
[02:04:58.540 --> 02:05:00.980]   I didn't mention that.
[02:05:00.980 --> 02:05:02.940]   And you can launch arbitrary apps as well.
[02:05:02.940 --> 02:05:05.140]   So you can say launch the weather app, or Yahoo,
[02:05:05.140 --> 02:05:06.420]   weather, whatever.
[02:05:06.420 --> 02:05:09.820]   So it's kind of-- I guess it's pretty much what OK Google
[02:05:09.820 --> 02:05:12.260]   will do.
[02:05:12.260 --> 02:05:13.860]   But touchless.
[02:05:13.860 --> 02:05:18.260]   And you don't have to shout, OK Google over the car.
[02:05:18.260 --> 02:05:19.980]   I was going to ask, may I have you tried this?
[02:05:19.980 --> 02:05:20.820]   Well, as you're driving?
[02:05:20.820 --> 02:05:21.180]   Yeah.
[02:05:21.180 --> 02:05:24.180]   And how well does it recognize your voice with the car--
[02:05:24.180 --> 02:05:25.300]   Very engine background and all that.
[02:05:25.300 --> 02:05:25.660]   Very well.
[02:05:25.660 --> 02:05:27.900]   And one of the advantages, of course,
[02:05:27.900 --> 02:05:30.660]   is it's kind of closer to your mouth than anything else
[02:05:30.660 --> 02:05:33.900]   is because you put it on an event right in front of you.
[02:05:33.900 --> 02:05:36.140]   And it's actually the vents are a pretty easy place
[02:05:36.140 --> 02:05:37.140]   to put this thing.
[02:05:37.140 --> 02:05:38.820]   This thing, it doesn't mar anything.
[02:05:38.820 --> 02:05:40.020]   It just clips right on.
[02:05:40.020 --> 02:05:41.620]   Nick, keep your phone cool.
[02:05:41.620 --> 02:05:44.220]   But you have to glue something to your phone, though.
[02:05:44.220 --> 02:05:45.300]   Yeah, not that bad.
[02:05:45.300 --> 02:05:47.300]   It's a little metal sticker.
[02:05:47.300 --> 02:05:48.420]   And I can pry it off.
[02:05:48.420 --> 02:05:49.980]   It's not going to--
[02:05:49.980 --> 02:05:52.740]   my bigger problem is I change phones so often.
[02:05:52.740 --> 02:05:54.100]   I only have two of these.
[02:05:54.100 --> 02:05:55.380]   I don't know where I'll get another one.
[02:05:55.380 --> 02:05:59.940]   So it's not going to take long before I can't use it anymore.
[02:05:59.940 --> 02:06:00.460]   It also--
[02:06:00.460 --> 02:06:02.940]   So it's doing battery life.
[02:06:02.940 --> 02:06:06.740]   It does have a battery in the doohickey.
[02:06:06.740 --> 02:06:08.820]   And I don't know if it's replaceable.
[02:06:08.820 --> 02:06:11.340]   Actually, I should have found that out.
[02:06:11.340 --> 02:06:12.780]   I would guess you could pry this off.
[02:06:12.780 --> 02:06:14.620]   And it's one of those little diamonds--
[02:06:14.620 --> 02:06:17.140]   Yeah, I'm sure it's got to be a super long life.
[02:06:17.140 --> 02:06:18.340]   It's going to be-- it's not doing long.
[02:06:18.340 --> 02:06:19.780]   Because it's not doing anything.
[02:06:19.780 --> 02:06:21.260]   I meant the battery on your phone.
[02:06:21.260 --> 02:06:23.660]   So running the app and having it pay attention.
[02:06:23.660 --> 02:06:24.820]   Oh, I haven't noticed that.
[02:06:24.820 --> 02:06:26.500]   But of course, one of the nice things about doing this
[02:06:26.500 --> 02:06:29.820]   in the car is you could always connect it to the charger
[02:06:29.820 --> 02:06:30.340]   if you wanted to do it.
[02:06:30.340 --> 02:06:31.340]   Oh, yeah.
[02:06:31.340 --> 02:06:38.100]   It's not something you don't already have.
[02:06:38.100 --> 02:06:41.060]   You can always say, OK, Google and do most of that stuff.
[02:06:41.060 --> 02:06:44.740]   But it's kind of so transparent and easy.
[02:06:44.740 --> 02:06:46.100]   I really like this.
[02:06:46.100 --> 02:06:49.340]   Android 4.4 or later, it does work with a Bluetooth system
[02:06:49.340 --> 02:06:51.660]   in most cars.
[02:06:51.660 --> 02:06:52.780]   But you don't have to have one.
[02:06:52.780 --> 02:06:56.860]   Your phone will make all the noise it needs to make.
[02:06:56.860 --> 02:06:59.620]   And it's pretty cool.
[02:06:59.620 --> 02:07:04.340]   Zero touch from Logitech.
[02:07:04.340 --> 02:07:05.300]   Matthew, what do you got for?
[02:07:05.300 --> 02:07:06.980]   Is there anything?
[02:07:06.980 --> 02:07:08.740]   Have you met a album?
[02:07:08.740 --> 02:07:10.380]   I do not have a heavy metal album.
[02:07:10.380 --> 02:07:13.700]   But I was thinking about a number because I know Jeff--
[02:07:13.700 --> 02:07:15.060]   We ought to do a number, yeah.
[02:07:15.060 --> 02:07:15.820]   Has a number.
[02:07:15.820 --> 02:07:23.500]   So here's the number, 146 million, 380,598.
[02:07:23.500 --> 02:07:24.700]   What's that?
[02:07:24.700 --> 02:07:30.460]   Those are the views of the woman who got the Chewbacca mask.
[02:07:30.460 --> 02:07:31.180]   How many?
[02:07:31.180 --> 02:07:31.660]   How many?
[02:07:31.660 --> 02:07:33.180]   146 million?
[02:07:33.180 --> 02:07:37.980]   146.4 million right now.
[02:07:37.980 --> 02:07:39.860]   I assume everyone has seen it.
[02:07:39.860 --> 02:07:41.500]   Yes.
[02:07:41.500 --> 02:07:43.140]   I find this fascinating.
[02:07:43.140 --> 02:07:45.180]   So now she's gone to Facebook.
[02:07:45.180 --> 02:07:46.460]   She met Mark Zuckerberg.
[02:07:46.460 --> 02:07:47.420]   She's been on Ellen.
[02:07:47.420 --> 02:07:48.340]   She's been on Ellen.
[02:07:48.340 --> 02:07:49.260]   She's been on the TV show.
[02:07:49.260 --> 02:07:50.220]   Oh, Chewbacca was there.
[02:07:50.220 --> 02:07:50.780]   She's met Chewbacca.
[02:07:50.780 --> 02:07:52.780]   Well, someone in a suit.
[02:07:52.780 --> 02:07:54.580]   Peter Mayhew, the original Chewbacca.
[02:07:54.580 --> 02:07:54.980]   Oh, wow.
[02:07:54.980 --> 02:07:56.460]   No, he mentioned her.
[02:07:56.460 --> 02:07:57.820]   So he's seen the video.
[02:07:57.820 --> 02:07:58.820]   Wow.
[02:07:58.820 --> 02:08:01.060]   I just find it fascinating that--
[02:08:01.060 --> 02:08:01.860]   I mean, what was it?
[02:08:01.860 --> 02:08:03.500]   Like two days ago or something?
[02:08:03.500 --> 02:08:07.540]   So it's gone from kind of woman in car streaming
[02:08:07.540 --> 02:08:11.020]   to what she presumes that are just her friends
[02:08:11.020 --> 02:08:14.940]   to global Facebook Live phenomenon.
[02:08:14.940 --> 02:08:17.220]   I just find that fascinating.
[02:08:17.220 --> 02:08:19.220]   I mean, it's such a huge--
[02:08:19.220 --> 02:08:21.100]   and the thing that I like about it--
[02:08:21.100 --> 02:08:24.740]   so as someone mentioned, it's kind of--
[02:08:24.740 --> 02:08:26.100]   it makes you feel a little weird
[02:08:26.100 --> 02:08:28.700]   because she's kind of marketing for Disney
[02:08:28.700 --> 02:08:30.500]   and she's kind of marketing for Facebook Live.
[02:08:30.500 --> 02:08:31.860]   But that's not why she did it.
[02:08:31.860 --> 02:08:34.020]   She just did it because she enjoyed the mask.
[02:08:34.020 --> 02:08:38.180]   I like the fact that she's just so goddamn happy about that.
[02:08:38.180 --> 02:08:39.540]   It's the best laugh ever.
[02:08:39.540 --> 02:08:44.340]   She is so overjoyed and it is so infectious.
[02:08:44.340 --> 02:08:48.100]   It's-- and so I don't mind that she's kind of been, you know,
[02:08:48.100 --> 02:08:49.660]   turned into this media star.
[02:08:49.660 --> 02:08:50.460]   And now she's everywhere.
[02:08:50.460 --> 02:08:52.820]   And Facebook is using it to market live.
[02:08:52.820 --> 02:08:55.140]   And Disney's using it to market whatever.
[02:08:55.140 --> 02:08:56.820]   I'm sure I will get sick of it because it's
[02:08:56.820 --> 02:08:58.060]   going to become annoying.
[02:08:58.060 --> 02:09:03.140]   But I'm not-- but her and the video was just so genuine.
[02:09:03.140 --> 02:09:04.180]   I've found it fascinating.
[02:09:04.180 --> 02:09:07.540]   And obviously, that's what people are reacting to, I think.
[02:09:07.540 --> 02:09:08.340]   Don't play it.
[02:09:08.340 --> 02:09:09.940]   I'm not going to play it.
[02:09:09.940 --> 02:09:10.940]   I was tempted to--
[02:09:10.940 --> 02:09:11.940]   I'm not going to.
[02:09:11.940 --> 02:09:14.060]   I'm sitting here looking at it with hovering over there.
[02:09:14.060 --> 02:09:14.860]   Have you not seen it?
[02:09:14.860 --> 02:09:15.860]   I haven't seen it yet.
[02:09:15.860 --> 02:09:16.340]   Oh, wow.
[02:09:16.340 --> 02:09:16.940]   It's hilarious.
[02:09:16.940 --> 02:09:19.340]   Make that 146 million in one.
[02:09:19.340 --> 02:09:20.460]   She makes you laugh.
[02:09:20.460 --> 02:09:22.460]   Like she's just got a great laugh about that.
[02:09:22.460 --> 02:09:24.540]   Just a beautiful, rich laugh.
[02:09:24.540 --> 02:09:27.260]   And it's authentic, which really is what makes it.
[02:09:27.260 --> 02:09:29.220]   And Cole's the department store that
[02:09:29.220 --> 02:09:31.700]   sold it that she cleverly mentioned
[02:09:31.700 --> 02:09:37.020]   has given her some money and a bunch of masks for her kids.
[02:09:37.020 --> 02:09:39.780]   And of course, Hasbro is very happy because they sold the mask
[02:09:39.780 --> 02:09:39.980]   out.
[02:09:39.980 --> 02:09:43.100]   You can buy them now on eBay for only four times
[02:09:43.100 --> 02:09:43.940]   the face value.
[02:09:43.940 --> 02:09:44.940]   Right.
[02:09:44.940 --> 02:09:48.380]   So of course, it's going to get completely overdone.
[02:09:48.380 --> 02:09:51.100]   And it's going to get perverted in some way.
[02:09:51.100 --> 02:09:53.740]   And then people are going to try and recreate it
[02:09:53.740 --> 02:09:56.300]   with all these dumb knockoffs.
[02:09:56.300 --> 02:10:00.260]   But I found it fascinating just how--
[02:10:00.260 --> 02:10:03.580]   because obviously, something about her resonates with people.
[02:10:03.580 --> 02:10:06.140]   In about 10 years, you'll be able to write that piece,
[02:10:06.140 --> 02:10:08.180]   that think piece, whatever happened to the Chewbacca
[02:10:08.180 --> 02:10:12.740]   mom and how she went through some real depression afterwards.
[02:10:12.740 --> 02:10:14.940]   But she's back on her feet again.
[02:10:14.940 --> 02:10:15.740]   Mary's broke up.
[02:10:15.740 --> 02:10:17.780]   She's got a new movie out of it.
[02:10:17.780 --> 02:10:18.280]   Exactly.
[02:10:18.280 --> 02:10:19.860]   I'm not kidding.
[02:10:19.860 --> 02:10:22.460]   It's the lifestyle piece.
[02:10:22.460 --> 02:10:23.660]   Hey, I forgot.
[02:10:23.660 --> 02:10:25.140]   We usually do do numbers with Jeff.
[02:10:25.140 --> 02:10:26.820]   And I did have some numbers I wanted to do.
[02:10:26.820 --> 02:10:29.780]   First of all, Patrick De La Hante has told me--
[02:10:29.780 --> 02:10:31.620]   he's our programmer and house programmer--
[02:10:31.620 --> 02:10:34.180]   that he checked with the API.
[02:10:34.180 --> 02:10:38.420]   And there are now 535 days, right?
[02:10:38.420 --> 02:10:39.700]   Days, right?
[02:10:39.700 --> 02:10:40.740]   Worth of content.
[02:10:40.740 --> 02:10:43.660]   If you started listening to every podcast we've made since day
[02:10:43.660 --> 02:10:48.900]   one, you would be listening for nearly two years, 535 days,
[02:10:48.900 --> 02:10:49.940]   nonstop.
[02:10:49.940 --> 02:10:53.340]   And this, which I was very happy to see from PodTrack, which
[02:10:53.340 --> 02:10:55.780]   does our audience metrics, that we
[02:10:55.780 --> 02:11:00.980]   are the number six podcast in the country with 1 million
[02:11:00.980 --> 02:11:04.540]   unique listeners every month and a total 6.5 million downloads
[02:11:04.540 --> 02:11:05.900]   every month.
[02:11:05.900 --> 02:11:07.420]   Of course, NPR is number one.
[02:11:07.420 --> 02:11:10.340]   This American Life and Serial, number two, WNYC Studios,
[02:11:10.340 --> 02:11:13.820]   number three, House Stuff Works, number four, CBS, number five,
[02:11:13.820 --> 02:11:15.060]   we're number six.
[02:11:15.060 --> 02:11:16.900]   And good on the Moths and Roman Mars,
[02:11:16.900 --> 02:11:20.740]   because they've got great numbers and one podcast.
[02:11:20.740 --> 02:11:24.980]   So 1 million uniques for one podcast, that's a lot better.
[02:11:24.980 --> 02:11:29.460]   Hey, Leo, aren't you part of this new PRX thing--
[02:11:29.460 --> 02:11:29.660]   What's that?
[02:11:29.660 --> 02:11:31.380]   --radio public?
[02:11:31.380 --> 02:11:32.060]   I don't know what it is.
[02:11:32.060 --> 02:11:33.620]   Radio public, have you heard of that?
[02:11:33.620 --> 02:11:34.660]   I'm pretty sure--
[02:11:34.660 --> 02:11:35.140]   We might well be.
[02:11:35.140 --> 02:11:36.260]   --that's been around for a while.
[02:11:36.260 --> 02:11:37.420]   I wrote about it.
[02:11:37.420 --> 02:11:41.660]   So it's a spinoff from PRX, which is the distributor
[02:11:41.660 --> 02:11:44.620]   for this American Life and a bunch of other public radio
[02:11:44.620 --> 02:11:45.780]   programs.
[02:11:45.780 --> 02:11:51.060]   It's a nonprofit, basically distribution entity.
[02:11:51.060 --> 02:11:54.620]   So it has spun off a new thing called Radio Public.
[02:11:54.620 --> 02:12:01.220]   And they're trying to create a kind of radio app or service
[02:12:01.220 --> 02:12:07.340]   that will let you listen to anything, not just public radio.
[02:12:07.660 --> 02:12:10.340]   But any form of audio, so podcasts--
[02:12:10.340 --> 02:12:12.380]   It's an app.
[02:12:12.380 --> 02:12:13.820]   Yeah.
[02:12:13.820 --> 02:12:16.620]   App/service.
[02:12:16.620 --> 02:12:17.420]   We may well be.
[02:12:17.420 --> 02:12:19.860]   I mean, our license allows anybody who wants to do that.
[02:12:19.860 --> 02:12:23.700]   Do that as long as they don't take out our hats
[02:12:23.700 --> 02:12:24.980]   or try to cash in.
[02:12:24.980 --> 02:12:28.100]   It's online at radiopublic.com.
[02:12:28.100 --> 02:12:29.300]   We make listening to podcasts better
[02:12:29.300 --> 02:12:32.380]   by listening to the people who make them.
[02:12:32.380 --> 02:12:32.980]   Interesting.
[02:12:32.980 --> 02:12:35.260]   --public benefit corporation.
[02:12:35.260 --> 02:12:36.860]   I could have sworn they mentioned you.
[02:12:36.860 --> 02:12:40.220]   It just seems like more of a power grab from the NPR mafia.
[02:12:40.220 --> 02:12:41.220]   But--
[02:12:41.220 --> 02:12:43.140]   [LAUGHTER]
[02:12:43.140 --> 02:12:43.660]   Hopefully not.
[02:12:43.660 --> 02:12:44.620]   I mean, hopefully this is--
[02:12:44.620 --> 02:12:45.860]   I think they're like the invented podcast.
[02:12:45.860 --> 02:12:46.860]   I mean, hopefully not.
[02:12:46.860 --> 02:12:47.340]   I agree.
[02:12:47.340 --> 02:12:48.660]   Totally agree on that.
[02:12:48.660 --> 02:12:53.500]   But the public radio model is basically broken, I think.
[02:12:53.500 --> 02:12:54.300]   Yeah, I agree.
[02:12:54.300 --> 02:12:57.020]   And so maybe this is a way to get back to something
[02:12:57.020 --> 02:12:58.500]   that actually is in the public benefit.
[02:12:58.500 --> 02:12:59.260]   This is radiopilot.
[02:12:59.260 --> 02:13:00.180]   I'm going to look into this.
[02:13:00.180 --> 02:13:00.860]   This is interesting.
[02:13:00.860 --> 02:13:02.260]   I shall as well.
[02:13:02.260 --> 02:13:04.980]   And one interesting thing is that they--
[02:13:04.980 --> 02:13:06.500]   I think it's interesting that they
[02:13:06.500 --> 02:13:09.380]   decided to create a separate company.
[02:13:09.380 --> 02:13:13.420]   So radiopublic is a separate entity.
[02:13:13.420 --> 02:13:19.660]   It's actually for profit, but it's a public benefit corporation.
[02:13:19.660 --> 02:13:24.860]   So they've incorporated it as whatever the schedule is
[02:13:24.860 --> 02:13:26.780]   to be a public benefit corporation.
[02:13:26.780 --> 02:13:30.020]   And they're funded--
[02:13:30.020 --> 02:13:33.020]   so they got funding from Clache, I think,
[02:13:33.020 --> 02:13:37.980]   the Knight Foundation, Graham Holdings, Matter Ventures.
[02:13:37.980 --> 02:13:39.100]   I just think it's an interesting--
[02:13:39.100 --> 02:13:40.260]   No, very interesting.
[02:13:40.260 --> 02:13:44.060]   I don't really know at all what it means.
[02:13:44.060 --> 02:13:46.220]   Is it just another podcast app and directory?
[02:13:46.220 --> 02:13:50.980]   I mean, iTunes was used to be dominant, which was both good
[02:13:50.980 --> 02:13:52.340]   and bad for podcasting.
[02:13:52.340 --> 02:13:53.820]   They haven't been in a long time.
[02:13:53.820 --> 02:13:56.940]   Only about 60% to 60% of our downloads go through iTunes.
[02:13:56.940 --> 02:13:59.060]   Now it's going down every month.
[02:13:59.060 --> 02:14:01.140]   But I would welcome another directory.
[02:14:01.140 --> 02:14:03.740]   I don't know if that's going to change the world.
[02:14:03.740 --> 02:14:04.740]   What I hope it is--
[02:14:04.740 --> 02:14:06.540]   What I hope they're doing is I hope
[02:14:06.540 --> 02:14:11.060]   that they're bringing some of the locally produced shows
[02:14:11.060 --> 02:14:12.780]   to a more national audience.
[02:14:12.780 --> 02:14:13.300]   Exactly.
[02:14:13.300 --> 02:14:15.260]   I had never heard of Forum until we moved out here
[02:14:15.260 --> 02:14:15.900]   to San Francisco.
[02:14:15.900 --> 02:14:16.900]   Yeah, it's a great show.
[02:14:16.900 --> 02:14:19.060]   It's on here at KQED every morning.
[02:14:19.060 --> 02:14:20.420]   And now I love to listen to it.
[02:14:20.420 --> 02:14:23.300]   But when I was back east, I had no idea that it was even there.
[02:14:23.300 --> 02:14:24.780]   Maybe that's the idea, too.
[02:14:24.780 --> 02:14:25.700]   And I'd be all forward.
[02:14:25.700 --> 02:14:26.700]   There's a lot of great stuff out there
[02:14:26.700 --> 02:14:28.860]   that if they could make it easily--
[02:14:28.860 --> 02:14:33.500]   find it easy on here and make a good way for people
[02:14:33.500 --> 02:14:35.620]   to find new things that, hey, here's
[02:14:35.620 --> 02:14:39.580]   this podcast or public radio show out of Texas somewhere.
[02:14:39.580 --> 02:14:41.740]   And it's really interesting.
[02:14:41.740 --> 02:14:44.020]   And now I have an easy way to listen to it as well.
[02:14:44.020 --> 02:14:45.340]   I think that would be really cool.
[02:14:45.340 --> 02:14:48.500]   Because it's hard to find stuff that's not already
[02:14:48.500 --> 02:14:50.740]   in your podcasting app or whatever.
[02:14:50.740 --> 02:14:52.140]   It's just hard to find that stuff.
[02:14:52.140 --> 02:14:53.860]   And I think that is what they're going to try and do.
[02:14:53.860 --> 02:14:55.820]   At least my understanding of what they wanted to do
[02:14:55.820 --> 02:14:59.180]   is to make it easier to find those things,
[02:14:59.180 --> 02:15:01.580]   to have recommendation engines, for example,
[02:15:01.580 --> 02:15:05.580]   or to just sort of sort through the massive amounts
[02:15:05.580 --> 02:15:08.700]   of audio type content out there and find things
[02:15:08.700 --> 02:15:10.980]   that you might like listening to.
[02:15:10.980 --> 02:15:12.820]   I shall keep my eye peeled.
[02:15:12.820 --> 02:15:16.420]   I'll be interested to see what they're doing.
[02:15:16.420 --> 02:15:19.180]   And now, ladies and gentlemen, it's time to make--
[02:15:19.180 --> 02:15:19.660]   That's right.
[02:15:19.660 --> 02:15:20.340]   Time for a project.
[02:15:20.340 --> 02:15:22.780]   Aaron Nukem has-- he always comes in with great projects.
[02:15:22.780 --> 02:15:25.500]   I don't know how you'd have time to do this.
[02:15:25.500 --> 02:15:26.340]   I don't either.
[02:15:26.340 --> 02:15:27.180]   You're doing the world tonight?
[02:15:27.180 --> 02:15:30.580]   Yeah, late nights and taking breaks from other things.
[02:15:30.580 --> 02:15:31.300]   I mean, it's great.
[02:15:31.300 --> 02:15:31.900]   It's hard to do.
[02:15:31.900 --> 02:15:32.620]   Very hard to do.
[02:15:32.620 --> 02:15:33.820]   It's very time consuming.
[02:15:33.820 --> 02:15:35.980]   My wife will tell you.
[02:15:35.980 --> 02:15:38.340]   Yeah, so what I brought today is--
[02:15:38.340 --> 02:15:41.300]   we talk a lot about Raspberry Pi.
[02:15:41.300 --> 02:15:43.060]   Talk a lot about Arduino.
[02:15:43.060 --> 02:15:45.900]   But there's a third platform that over the past couple
[02:15:45.900 --> 02:15:50.580]   of years has really started to gain ground with makers.
[02:15:50.580 --> 02:15:54.100]   And that is the ESP8266.
[02:15:54.100 --> 02:15:56.100]   It doesn't have a catchy name, which is part of the problem
[02:15:56.100 --> 02:15:57.740]   with the platform, in my opinion.
[02:15:57.740 --> 02:15:59.820]   But it makes you sound like a real geek.
[02:15:59.820 --> 02:16:02.220]   Yeah, do you have the ESP8266?
[02:16:02.220 --> 02:16:02.980]   Oh, yeah.
[02:16:02.980 --> 02:16:04.140]   That's a great Wi-Fi watch.
[02:16:04.140 --> 02:16:05.140]   That's a great sound.
[02:16:05.140 --> 02:16:08.940]   I love the sock on that.
[02:16:08.940 --> 02:16:10.820]   But it is the ESP8266.
[02:16:10.820 --> 02:16:13.780]   What sets us apart from an Arduino, for example,
[02:16:13.780 --> 02:16:15.780]   it is a microcontroller like an Arduino.
[02:16:15.780 --> 02:16:18.260]   But it has built-in Wi-Fi.
[02:16:18.260 --> 02:16:20.380]   So there's a lot that you can do with this.
[02:16:20.380 --> 02:16:20.900]   And it's cheap.
[02:16:20.900 --> 02:16:21.500]   It's five bucks.
[02:16:21.500 --> 02:16:22.260]   Very cheap.
[02:16:22.260 --> 02:16:22.940]   Very cheap.
[02:16:22.940 --> 02:16:24.420]   So you can get it on eBay.
[02:16:24.420 --> 02:16:25.500]   I put a link in the rundown.
[02:16:25.500 --> 02:16:29.340]   You can get it for $3.99 on eBay from China right now.
[02:16:29.340 --> 02:16:32.140]   And that's for the developer board, not even the chip itself.
[02:16:32.140 --> 02:16:35.100]   So the chip you can buy for about $2.50.
[02:16:35.100 --> 02:16:36.820]   But if you want the developer board, which breaks out
[02:16:36.820 --> 02:16:39.420]   all the connections, the GPIO and all that,
[02:16:39.420 --> 02:16:42.140]   is right now it's $3.99 on eBay.
[02:16:42.140 --> 02:16:43.740]   So let me show you what it looks like.
[02:16:43.740 --> 02:16:45.580]   And I'll give you a comparison to--
[02:16:45.580 --> 02:16:46.980]   so here it is right here.
[02:16:46.980 --> 02:16:48.980]   This is the developer board.
[02:16:48.980 --> 02:16:51.620]   And you'll notice this right here--
[02:16:51.620 --> 02:16:52.780]   I've got to go the other way.
[02:16:52.780 --> 02:16:54.820]   This right here, this silver thing,
[02:16:54.820 --> 02:16:58.380]   and this is the Wi-Fi antenna, this is the actual module.
[02:16:58.380 --> 02:17:01.500]   All the rest of this is actually the dev board, which
[02:17:01.500 --> 02:17:05.260]   gives you the ability, for example, to plug into your computer
[02:17:05.260 --> 02:17:06.940]   and program it and things like that.
[02:17:06.940 --> 02:17:08.500]   So it comes with a USB interface,
[02:17:08.500 --> 02:17:09.540]   but the rest is like GPIO.
[02:17:09.540 --> 02:17:11.260]   If you get the dev board, yeah.
[02:17:11.260 --> 02:17:12.540]   You get the USB interface.
[02:17:12.540 --> 02:17:13.740]   You can plug it in, like I said.
[02:17:13.740 --> 02:17:16.140]   And it also comes with a three volt power regulator,
[02:17:16.140 --> 02:17:17.260]   because it's a three volt board.
[02:17:17.260 --> 02:17:19.940]   And sometimes you're using five volt, things like that.
[02:17:19.940 --> 02:17:21.420]   So it's worth it to get the dev board
[02:17:21.420 --> 02:17:23.020]   if you just want to play around with it.
[02:17:23.020 --> 02:17:25.900]   But just in a size comparison here, I can hold this up.
[02:17:25.900 --> 02:17:28.700]   This is a Raspberry Pi--
[02:17:28.700 --> 02:17:29.780]   Look how much bigger, yeah.
[02:17:29.780 --> 02:17:32.380]   Raspberry Pi version 2, I just happen to have that--
[02:17:32.380 --> 02:17:33.580]   It's about half the size of that.
[02:17:33.580 --> 02:17:37.420]   And this is about half the size in the actual module that
[02:17:37.420 --> 02:17:40.180]   runs this thing is about the size of the Broadcom chip.
[02:17:40.180 --> 02:17:42.820]   It's less than one inch by one inch in diameter.
[02:17:42.820 --> 02:17:44.820]   So you just say, OK, that's cool.
[02:17:44.820 --> 02:17:45.300]   It's small.
[02:17:45.300 --> 02:17:45.620]   It's cheap.
[02:17:45.620 --> 02:17:46.660]   What can you do with it?
[02:17:46.660 --> 02:17:47.620]   There's all kinds of things.
[02:17:47.620 --> 02:17:50.460]   Once you add the ability to be able to talk over Wi-Fi,
[02:17:50.460 --> 02:17:52.540]   there's all kinds of things you can do with it.
[02:17:52.540 --> 02:17:53.460]   Also, it's low power.
[02:17:53.460 --> 02:17:56.140]   So you can run it off of just about any power source.
[02:17:56.140 --> 02:17:58.260]   It'll run for days.
[02:17:58.260 --> 02:18:01.540]   Somebody ran this thing for 17 days on one of those coin cell
[02:18:01.540 --> 02:18:03.420]   batteries.
[02:18:03.420 --> 02:18:04.900]   And you can do things like this.
[02:18:04.900 --> 02:18:09.180]   What I did was a quick and dirty little project,
[02:18:09.180 --> 02:18:12.620]   since this thing has Wi-Fi and I've programmed it
[02:18:12.620 --> 02:18:16.820]   to automatically connect to various hotspots around.
[02:18:16.820 --> 02:18:18.780]   One of them happens to be here in the brick house.
[02:18:18.780 --> 02:18:20.300]   And all it's doing is it's telling me
[02:18:20.300 --> 02:18:22.580]   what the signal strength of the Wi-Fi is.
[02:18:22.580 --> 02:18:24.420]   So what I can do is if I wanted to,
[02:18:24.420 --> 02:18:26.420]   I could walk around the studio right now
[02:18:26.420 --> 02:18:30.100]   and tell you where the signal is coming in, the clearest,
[02:18:30.100 --> 02:18:31.660]   and where it has problems.
[02:18:31.660 --> 02:18:33.860]   And it'll also automatically switch to the strongest
[02:18:33.860 --> 02:18:34.340]   signal.
[02:18:34.340 --> 02:18:37.860]   If I get below a certain DB level on this,
[02:18:37.860 --> 02:18:42.940]   it'll tell me and it'll switch to the next best Wi-Fi
[02:18:42.940 --> 02:18:45.420]   access point that it can find.
[02:18:45.420 --> 02:18:46.980]   So it's really easy.
[02:18:46.980 --> 02:18:48.340]   Another really good thing about this
[02:18:48.340 --> 02:18:50.660]   is the people that developed code for it
[02:18:50.660 --> 02:18:52.860]   have made it compatible with Arduino.
[02:18:52.860 --> 02:18:55.100]   So with the Arduino IDE--
[02:18:55.100 --> 02:18:56.260]   What do you program it with?
[02:18:56.260 --> 02:18:57.540]   With the Arduino IDE.
[02:18:57.540 --> 02:18:59.180]   It's a C, can you see plus plus?
[02:18:59.180 --> 02:18:59.740]   C plus plus.
[02:18:59.740 --> 02:19:01.340]   C plus plus and the Arduino IDE.
[02:19:01.340 --> 02:19:04.780]   But what that also gives you is access to all the Arduino--
[02:19:04.780 --> 02:19:07.420]   not all, but most of the Arduino libraries.
[02:19:07.420 --> 02:19:09.260]   So for example, on this, what I used
[02:19:09.260 --> 02:19:12.540]   to program my little OLED display here
[02:19:12.540 --> 02:19:15.700]   is the Adafruit graphics library,
[02:19:15.700 --> 02:19:18.100]   because that was already existed for Arduino.
[02:19:18.100 --> 02:19:21.140]   And I was able to use that to do all the graphics
[02:19:21.140 --> 02:19:23.380]   on this nice little display here.
[02:19:23.380 --> 02:19:24.780]   So you get a lot of compatibility
[02:19:24.780 --> 02:19:26.500]   by using the Arduino IDE.
[02:19:26.500 --> 02:19:28.380]   And it's really easy to code that way.
[02:19:28.380 --> 02:19:33.020]   And you just upload your code to the board and away you go.
[02:19:33.020 --> 02:19:34.100]   So I would highly recommend it.
[02:19:34.100 --> 02:19:34.980]   There's a lot of cool things.
[02:19:34.980 --> 02:19:37.220]   What started me down this path was
[02:19:37.220 --> 02:19:40.500]   I wanted to make a light sensor to put out in the garden
[02:19:40.500 --> 02:19:42.700]   to figure out how much light I get in various areas
[02:19:42.700 --> 02:19:44.700]   of the garden to know what kind of plants to put there
[02:19:44.700 --> 02:19:46.820]   and how much to water them and all that kind of stuff.
[02:19:46.820 --> 02:19:47.860]   I was like, boy, it'd be really nice
[02:19:47.860 --> 02:19:50.540]   if I just had a little thing that had built in Wi-Fi.
[02:19:50.540 --> 02:19:52.460]   And I could just set it here and it would automatically
[02:19:52.460 --> 02:19:55.220]   upload the data to my server somewhere
[02:19:55.220 --> 02:19:58.020]   and tell me I could go at the end of the day, go look at it.
[02:19:58.020 --> 02:19:59.900]   And so I bought these to use for that.
[02:19:59.900 --> 02:20:02.220]   Now I'm thinking of all kinds of other uses.
[02:20:02.220 --> 02:20:05.900]   So for example, I think it would be fun if I had one of these
[02:20:05.900 --> 02:20:09.260]   and I was wearing it, and then I gave everyone else one
[02:20:09.260 --> 02:20:11.260]   and it showed me the signal strength relative
[02:20:11.260 --> 02:20:13.980]   to the one that I had set up as an access point
[02:20:13.980 --> 02:20:14.900]   that I was wearing.
[02:20:14.900 --> 02:20:19.980]   You could kind of do a game, right, where you had to find me.
[02:20:19.980 --> 02:20:22.020]   And you could use that to see if you were getting hotter
[02:20:22.020 --> 02:20:24.460]   or colder, if you were getting close to where I was hiding.
[02:20:24.460 --> 02:20:25.900]   - Oh, it's fun.
[02:20:25.900 --> 02:20:27.940]   - I think it would be really fun.
[02:20:27.940 --> 02:20:29.580]   There's all kinds of other things you can do with it
[02:20:29.580 --> 02:20:31.460]   because it's Wi-Fi enabled.
[02:20:31.460 --> 02:20:34.700]   And so if anyone out there has thought about using
[02:20:34.700 --> 02:20:37.420]   an Arduino, but they were longing for a platform
[02:20:37.420 --> 02:20:41.220]   that they could connect to their Wi-Fi on really easily,
[02:20:41.220 --> 02:20:42.700]   I would definitely recommend picking this up
[02:20:42.700 --> 02:20:43.620]   and checking it out.
[02:20:43.620 --> 02:20:46.420]   There's a lot of good information that I've put in
[02:20:46.420 --> 02:20:48.340]   that should be in the show notes for this show.
[02:20:48.340 --> 02:20:50.060]   So you can find out how to use it,
[02:20:50.060 --> 02:20:52.500]   how to get your Arduino IDE set up to run it.
[02:20:52.500 --> 02:20:54.380]   And there's a lot of great tutorials out there online.
[02:20:54.380 --> 02:20:56.340]   If you just do a Google search, you'll find all sorts of stuff.
[02:20:56.340 --> 02:20:57.900]   - And I love this.
[02:20:57.900 --> 02:21:01.460]   It supports the Hayes Modem AT command set.
[02:21:01.460 --> 02:21:03.020]   - Yes.
[02:21:03.020 --> 02:21:03.860]   (laughing)
[02:21:03.860 --> 02:21:04.700]   - Natively, natively.
[02:21:04.700 --> 02:21:05.540]   - This is great.
[02:21:05.540 --> 02:21:06.380]   - AT-T-T baby.
[02:21:06.380 --> 02:21:09.180]   - When you first get this thing, that's the default firmware
[02:21:09.180 --> 02:21:10.020]   that it has on that.
[02:21:10.020 --> 02:21:11.620]   It lets you connect with the serial port
[02:21:11.620 --> 02:21:13.900]   and send AT commands to it.
[02:21:13.900 --> 02:21:16.460]   So you can send AT commands to find out, for example,
[02:21:16.460 --> 02:21:19.220]   what local access points this thing can see.
[02:21:19.220 --> 02:21:21.620]   - And so even though you're using AT commands,
[02:21:21.620 --> 02:21:22.580]   you're talking to Wi-Fi.
[02:21:22.580 --> 02:21:24.740]   - You can still talk to Wi-Fi with those AT commands.
[02:21:24.740 --> 02:21:25.580]   Absolutely.
[02:21:25.580 --> 02:21:26.540]   - I think that's kind of cool.
[02:21:26.540 --> 02:21:28.500]   We'll talk about a copy-rated API.
[02:21:28.500 --> 02:21:29.340]   - Yep.
[02:21:29.340 --> 02:21:30.780]   - It was, by the way, the AT command set.
[02:21:30.780 --> 02:21:32.900]   - Oh, it was copyrighted, no one.
[02:21:32.900 --> 02:21:34.860]   - Apparently they weren't able to keep a lid on it.
[02:21:34.860 --> 02:21:36.340]   - Yeah, I guess speaking of copyright.
[02:21:36.340 --> 02:21:37.260]   - Larry Ellison.
[02:21:37.260 --> 02:21:38.100]   - Guess not.
[02:21:38.100 --> 02:21:38.940]   - Yeah.
[02:21:38.940 --> 02:21:39.780]   - But it's a lot of fun.
[02:21:39.780 --> 02:21:41.540]   It's something to tinker with.
[02:21:41.540 --> 02:21:45.180]   - I expect a flood of these kind of microcontrollers
[02:21:45.180 --> 02:21:47.020]   in all sorts of specialty areas.
[02:21:47.020 --> 02:21:47.860]   - Yeah.
[02:21:47.860 --> 02:21:48.700]   - This is gonna be a flood.
[02:21:48.700 --> 02:21:50.500]   - This really enables the internet of things, right?
[02:21:50.500 --> 02:21:53.900]   Because again, if I was to break off this module
[02:21:53.900 --> 02:21:56.060]   off of the development board,
[02:21:56.060 --> 02:21:56.900]   - It's tiny.
[02:21:56.900 --> 02:21:57.740]   - It's so tiny.
[02:21:57.740 --> 02:22:00.900]   - Yeah, it's about as big as my thumb.
[02:22:00.900 --> 02:22:01.740]   It's almost--
[02:22:01.740 --> 02:22:02.660]   - Well, here's a picture on the,
[02:22:02.660 --> 02:22:04.700]   there's a picture without the dev board.
[02:22:04.700 --> 02:22:05.540]   - Oh, there you go.
[02:22:05.540 --> 02:22:06.700]   - I mean, it is a chip.
[02:22:06.700 --> 02:22:08.900]   - Check at that in comparison to--
[02:22:08.900 --> 02:22:09.740]   - It's a--
[02:22:09.740 --> 02:22:10.580]   - The battery.
[02:22:10.580 --> 02:22:11.420]   I mean, it's super small.
[02:22:11.420 --> 02:22:14.340]   And so you can put that in where you put in a light switch
[02:22:14.340 --> 02:22:15.180]   behind a light switch,
[02:22:15.180 --> 02:22:16.020]   you could put it--
[02:22:16.020 --> 02:22:16.860]   - Very.
[02:22:16.860 --> 02:22:18.260]   - Underneath the table at Starbucks.
[02:22:18.260 --> 02:22:19.500]   - So does it have--
[02:22:19.500 --> 02:22:21.300]   - So non-volatile memory,
[02:22:21.300 --> 02:22:23.820]   you can store the program in or--
[02:22:23.820 --> 02:22:26.340]   - The program will stay resident, yeah, in the ROM.
[02:22:26.340 --> 02:22:27.380]   So you don't have to worry.
[02:22:27.380 --> 02:22:28.780]   Once you've uploaded it, it'll stay there.
[02:22:28.780 --> 02:22:29.740]   And just like in Arduino,
[02:22:29.740 --> 02:22:31.420]   you can unplug it, plug it in,
[02:22:31.420 --> 02:22:34.260]   a little bit more difficult to do that with the Raspberry Pi
[02:22:34.260 --> 02:22:35.820]   because it doesn't write as fast.
[02:22:35.820 --> 02:22:36.660]   - Right.
[02:22:36.660 --> 02:22:37.740]   - So, but this, yeah,
[02:22:37.740 --> 02:22:40.580]   you can plug it in, unplug it, stick it somewhere.
[02:22:40.580 --> 02:22:43.820]   - So you get the dashboard program it,
[02:22:43.820 --> 02:22:47.260]   but then how do you program the little one?
[02:22:47.260 --> 02:22:48.660]   - So the little one, you'd have to do
[02:22:48.660 --> 02:22:49.860]   kind of what they're doing there is
[02:22:49.860 --> 02:22:52.060]   you'd have to hook up an FTDI programmer,
[02:22:52.060 --> 02:22:54.620]   some sort of a programmer that hooked up to it.
[02:22:54.620 --> 02:22:56.620]   That's why I say it's good.
[02:22:56.620 --> 02:22:57.700]   If you get the development board,
[02:22:57.700 --> 02:22:58.540]   then you can play with that.
[02:22:58.540 --> 02:22:59.700]   - At the very least, start there.
[02:22:59.700 --> 02:23:00.780]   - Start with the development board
[02:23:00.780 --> 02:23:02.220]   'cause it's easy to hook up to the computer,
[02:23:02.220 --> 02:23:03.340]   it shows up as a comp board,
[02:23:03.340 --> 02:23:05.340]   you don't have to worry about buying an extra programmer
[02:23:05.340 --> 02:23:06.940]   to get your program to run on it.
[02:23:06.940 --> 02:23:09.100]   But even if you do, it's really not that hard.
[02:23:09.100 --> 02:23:11.740]   - Nice.
[02:23:11.740 --> 02:23:12.580]   - It's cool.
[02:23:12.580 --> 02:23:14.220]   I've been having a lot of fun with it.
[02:23:14.220 --> 02:23:17.020]   Between the graphics library, the OLED screen
[02:23:17.020 --> 02:23:19.140]   and the wifi built into it,
[02:23:19.140 --> 02:23:20.180]   I've been having a lot of fun with it
[02:23:20.180 --> 02:23:21.540]   over the past couple of weeks.
[02:23:21.540 --> 02:23:23.780]   - We were talking before on the screen savers
[02:23:23.780 --> 02:23:26.340]   and I know you and Stacy before the show,
[02:23:26.340 --> 02:23:29.380]   we're talking about the OpenHab software
[02:23:29.380 --> 02:23:34.380]   that would let you kind of create an open IoT environment
[02:23:35.700 --> 02:23:37.620]   where other things could talk to it.
[02:23:37.620 --> 02:23:39.500]   And I think this is gonna--
[02:23:39.500 --> 02:23:41.460]   - And this totally could be one of those devices.
[02:23:41.460 --> 02:23:43.740]   You could absolutely integrate this with OpenHab.
[02:23:43.740 --> 02:23:44.580]   - Yeah.
[02:23:44.580 --> 02:23:48.020]   - Well, it's been kind of an IoT show.
[02:23:48.020 --> 02:23:48.860]   Thank you, Aaron.
[02:23:48.860 --> 02:23:49.700]   - You're welcome.
[02:23:49.700 --> 02:23:51.340]   - Actually, Leo, I just wanted to say
[02:23:51.340 --> 02:23:53.660]   I got two things I wrote confused.
[02:23:53.660 --> 02:23:57.220]   So it wasn't the PRX radio thing
[02:23:57.220 --> 02:23:58.940]   that you were involved with.
[02:23:58.940 --> 02:24:01.340]   It was the BitTorrent Live.
[02:24:01.340 --> 02:24:03.580]   - Yes, we are in the launch product
[02:24:03.580 --> 02:24:05.100]   for Apple TV and BitTorrent Live.
[02:24:05.100 --> 02:24:08.540]   We've been playing with that and talking to them for ages.
[02:24:08.540 --> 02:24:11.980]   I used BitTorrent in the earliest days of the podcast
[02:24:11.980 --> 02:24:13.900]   because we didn't have any bandwidth,
[02:24:13.900 --> 02:24:16.700]   we could afford bandwidth so I had actually asked our audience,
[02:24:16.700 --> 02:24:18.860]   would you mind seating the show?
[02:24:18.860 --> 02:24:19.700]   Did you?
[02:24:19.700 --> 02:24:20.540]   - Yeah, thank you.
[02:24:20.540 --> 02:24:21.980]   - Yeah, episode six or something.
[02:24:21.980 --> 02:24:22.820]   I was like, yeah, I'll do that.
[02:24:22.820 --> 02:24:24.460]   - It was a free-weight distribution method.
[02:24:24.460 --> 02:24:26.020]   - So I've been using BitTorrent forever
[02:24:26.020 --> 02:24:27.700]   and BitTorrent Live we used for a little while
[02:24:27.700 --> 02:24:30.220]   and now they have an app on the Apple TV
[02:24:30.220 --> 02:24:33.140]   and we're on it, which is pretty awesome.
[02:24:33.140 --> 02:24:34.060]   - Interesting.
[02:24:34.060 --> 02:24:37.460]   One of the 10 or 11 networks that they launched with.
[02:24:37.460 --> 02:24:41.300]   So that means another way to watch us on the Apple TV.
[02:24:41.300 --> 02:24:42.460]   Stacy, what do you wanna plug?
[02:24:42.460 --> 02:24:45.580]   Stacy on IOT.com, right?
[02:24:45.580 --> 02:24:46.580]   - Sure.
[02:24:46.580 --> 02:24:47.420]   - Anything else?
[02:24:47.420 --> 02:24:49.140]   - That's my newsletter.
[02:24:49.140 --> 02:24:51.180]   IOTpodcast.com is the podcast.
[02:24:51.180 --> 02:24:52.020]   - Got it.
[02:24:52.020 --> 02:24:56.460]   And Gigga Stacy on the Twitter.
[02:24:56.460 --> 02:24:58.700]   Thank you for putting off your dinner a little bit.
[02:24:58.700 --> 02:25:00.260]   We appreciate it.
[02:25:00.260 --> 02:25:02.980]   - Yeah, I'm waiting for, how long is that gonna take
[02:25:02.980 --> 02:25:04.020]   to arrive, the Blue Apron?
[02:25:04.020 --> 02:25:04.860]   - Blue Apron.
[02:25:04.860 --> 02:25:05.700]   - It's on its way.
[02:25:05.700 --> 02:25:06.540]   Yeah, the box is on its way.
[02:25:06.540 --> 02:25:07.900]   It should be there any minute now.
[02:25:07.900 --> 02:25:08.740]   - Exactly.
[02:25:08.740 --> 02:25:10.300]   - We're using Amazon drones.
[02:25:10.300 --> 02:25:12.220]   The latest drone technology.
[02:25:12.220 --> 02:25:14.540]   - I'll drive to Buffalo and pick mine up for the border.
[02:25:14.540 --> 02:25:15.380]   - That's what you'll have to do.
[02:25:15.380 --> 02:25:16.380]   Get a mail drive to Buffalo.
[02:25:16.380 --> 02:25:17.780]   That's what you need.
[02:25:17.780 --> 02:25:19.020]   So great to have you, Stacy.
[02:25:19.020 --> 02:25:19.860]   Welcome to the show.
[02:25:19.860 --> 02:25:22.420]   We're thrilled to have you.
[02:25:22.420 --> 02:25:26.100]   Any time, it's just great.
[02:25:26.100 --> 02:25:27.060]   Well, we see you next week.
[02:25:27.060 --> 02:25:28.540]   You're gonna be around next week.
[02:25:28.540 --> 02:25:29.620]   - I think so, yeah.
[02:25:29.620 --> 02:25:30.460]   - Good.
[02:25:30.460 --> 02:25:33.300]   I'll introduce you to the excitement that is Jeff Jarvis.
[02:25:33.300 --> 02:25:36.100]   - I know the excitement that is Jeff Jarvis.
[02:25:36.100 --> 02:25:37.700]   - Maybe we can get Dan Denton on.
[02:25:37.700 --> 02:25:38.540]   I mean, Nick Denton.
[02:25:38.540 --> 02:25:40.940]   That'd be kind of fun.
[02:25:40.940 --> 02:25:42.580]   Aaron, thank you for being here.
[02:25:42.580 --> 02:25:45.300]   Where can people find more about your projects?
[02:25:45.300 --> 02:25:47.820]   Or do you have the Benisha Makerspace must have a website?
[02:25:47.820 --> 02:25:48.660]   - We do.
[02:25:48.660 --> 02:25:50.100]   BenishaMakerspace.org for sure.
[02:25:50.100 --> 02:25:51.460]   If you're in the Bay Area, come check us out.
[02:25:51.460 --> 02:25:52.980]   We have people coming up from San Francisco
[02:25:52.980 --> 02:25:54.540]   to take our welding classes.
[02:25:54.540 --> 02:25:55.900]   They're coming down from Petaluma
[02:25:55.900 --> 02:25:57.620]   to do some of our programming classes
[02:25:57.620 --> 02:25:59.260]   and our Duino Raspberry Pi Knights.
[02:25:59.260 --> 02:26:01.420]   So definitely check us out if you're in the Bay Area.
[02:26:01.420 --> 02:26:04.860]   If not, I'm at Aaron Newcomb and plus Aaron Newcomb Online.
[02:26:04.860 --> 02:26:06.340]   You can follow me there.
[02:26:06.340 --> 02:26:08.700]   And I'm on the Twitch shows all over the place, right?
[02:26:08.700 --> 02:26:10.700]   So I can just tune into Floss Weekly,
[02:26:10.700 --> 02:26:11.980]   tune into this week in Google.
[02:26:11.980 --> 02:26:12.820]   - The morning of the matter.
[02:26:12.820 --> 02:26:14.220]   - All about Android.
[02:26:14.220 --> 02:26:15.780]   So any of the shows, yeah.
[02:26:15.780 --> 02:26:16.980]   I pop up all over the place.
[02:26:16.980 --> 02:26:17.820]   - Thank you, sir.
[02:26:17.820 --> 02:26:20.620]   Matthew Ingram, love having you on Fortune.com.
[02:26:20.620 --> 02:26:26.340]   @Matthewi1T, M-A-T-H-E-W-I on Twitter.
[02:26:26.340 --> 02:26:29.700]   Anything you wanna promote or plug or plug?
[02:26:29.700 --> 02:26:31.940]   Gotta give everybody a chance.
[02:26:31.940 --> 02:26:34.140]   - Just everything I write.
[02:26:34.140 --> 02:26:35.340]   - Everything I write.
[02:26:35.340 --> 02:26:36.580]   - Read it all right to the end.
[02:26:36.580 --> 02:26:38.900]   - Every word he takes.
[02:26:38.900 --> 02:26:40.340]   - Share it with your fans. - Click on the ads.
[02:26:40.340 --> 02:26:42.180]   - Click on the ads. - Yes, please click on the ad.
[02:26:42.180 --> 02:26:43.020]   - God knows.
[02:26:43.020 --> 02:26:45.340]   Don't use ad vloggers, kids. - Please don't run an ad vlog.
[02:26:45.340 --> 02:26:48.260]   (laughing)
[02:26:48.260 --> 02:26:50.100]   - I only use it for the show, honest.
[02:26:50.100 --> 02:26:51.140]   At home, I don't use it.
[02:26:51.140 --> 02:26:52.980]   (laughing)
[02:26:52.980 --> 02:26:54.060]   Thank you all for joining us.
[02:26:54.060 --> 02:26:56.340]   We do this week at Google every Wednesday, 130 Pacific,
[02:26:56.340 --> 02:26:58.460]   430 Eastern, 2030 UTC.
[02:26:58.460 --> 02:27:00.100]   Watch us live, will ya?
[02:27:00.100 --> 02:27:02.100]   Or come and visit us in the studio.
[02:27:02.100 --> 02:27:03.780]   It's always nice to have a live studio audience.
[02:27:03.780 --> 02:27:05.700]   You can email tickets@twit.tv.
[02:27:05.700 --> 02:27:06.900]   We'll put a chair out for you.
[02:27:06.900 --> 02:27:10.820]   Don't forget those world tour teas are available,
[02:27:10.820 --> 02:27:12.940]   but only for a couple more weeks
[02:27:12.940 --> 02:27:17.940]   at teespring.com/twit, T-W-E-S-P-R-I-N-G.com/twit.
[02:27:17.940 --> 02:27:21.580]   $20, it goes to a good cause.
[02:27:21.580 --> 02:27:23.660]   The people who sew the t-shirt.
[02:27:23.660 --> 02:27:27.460]   Teespring.com/twit for a limited time only.
[02:27:27.460 --> 02:27:29.620]   Thanks for being here, we'll see you next time.
[02:27:29.620 --> 02:27:30.460]   On Twig.
[02:27:30.460 --> 02:27:33.040]   (upbeat music)
[02:27:33.040 --> 02:27:35.620]   (upbeat music)
[02:27:35.620 --> 02:27:38.200]   (upbeat music)
[02:27:38.200 --> 02:27:40.220]   [Music]

