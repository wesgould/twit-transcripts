;FFMETADATA1
title=Sparkle Vamps
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=470
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:01.340]   It's time for Twig this week.
[00:00:01.340 --> 00:00:03.120]   In Google, Jeff has the week off.
[00:00:03.120 --> 00:00:06.140]   Joan Donovan sits in for him from Data and Society.net.
[00:00:06.140 --> 00:00:08.820]   Of course, Stacy Higginbotham will be here as well.
[00:00:08.820 --> 00:00:10.080]   So will the changelog.
[00:00:10.080 --> 00:00:11.600]   It's triumphant return.
[00:00:11.600 --> 00:00:15.360]   Some interesting new tips and tricks from Google.
[00:00:15.360 --> 00:00:17.520]   We'll talk about Facebook and Twitter
[00:00:17.520 --> 00:00:19.440]   and why I'm leaving the mall.
[00:00:19.440 --> 00:00:22.400]   It's all coming up next on Twig.
[00:00:22.400 --> 00:00:28.120]   Netcast you love.
[00:00:28.120 --> 00:00:29.560]   From people you trust.
[00:00:29.560 --> 00:00:34.920]   This is Twig.
[00:00:34.920 --> 00:00:40.720]   This is Twig.
[00:00:40.720 --> 00:00:44.200]   This week in Google, episode 470, recorded Wednesday,
[00:00:44.200 --> 00:00:46.960]   August 22, 2018.
[00:00:46.960 --> 00:00:49.040]   Sparkle Vamps.
[00:00:49.040 --> 00:00:52.040]   This week in Google is brought to you by DigitalOcean,
[00:00:52.040 --> 00:00:55.280]   the easiest cloud platform to deploy, manage, and scale
[00:00:55.280 --> 00:00:59.720]   applications over 150,000 businesses, rely on DigitalOcean
[00:00:59.720 --> 00:01:02.920]   to remove infrastructure friction and deliver industry
[00:01:02.920 --> 00:01:04.520]   leading price performance.
[00:01:04.520 --> 00:01:10.720]   Sign up today and receive a free $100 credit at dio.co/twig.
[00:01:10.720 --> 00:01:14.680]   And by LastPass, secure every password protected entry point
[00:01:14.680 --> 00:01:15.920]   to your business.
[00:01:15.920 --> 00:01:19.920]   Join over 33,000 businesses and start managing and securing
[00:01:19.920 --> 00:01:21.560]   your company's passwords today.
[00:01:21.560 --> 00:01:24.920]   Learn more at lastpass.com/twig.
[00:01:25.640 --> 00:01:28.680]   And by Rocket Mortgage by Quick and Loans.
[00:01:28.680 --> 00:01:30.920]   Introducing Rate Shield Approval.
[00:01:30.920 --> 00:01:32.600]   If you're in the market to buy a home.
[00:01:32.600 --> 00:01:35.480]   Rate Shield Approval locks up your rate for up to 90 days
[00:01:35.480 --> 00:01:36.440]   while you shop.
[00:01:36.440 --> 00:01:38.160]   It's a real game changer.
[00:01:38.160 --> 00:01:43.760]   Learn more and get started at rocketmortgage.com/twig.
[00:01:43.760 --> 00:01:45.280]   It's time for Twig this week in Google,
[00:01:45.280 --> 00:01:48.840]   the show where we talk about the latest from the Google
[00:01:48.840 --> 00:01:53.040]   verse, which means it's really anything and everything.
[00:01:53.040 --> 00:01:55.880]   Stacey Hagenbotham is here from Stacey on IoT.
[00:01:55.880 --> 00:01:59.040]   She likes to cover IoT and hardware.
[00:01:59.040 --> 00:02:00.360]   But everything else too.
[00:02:00.360 --> 00:02:01.840]   Hi Stacey.
[00:02:01.840 --> 00:02:02.800]   Hello Leo.
[00:02:02.800 --> 00:02:04.160]   Good to see you.
[00:02:04.160 --> 00:02:08.920]   And Joan Donovan's here from Data and Society.
[00:02:08.920 --> 00:02:11.560]   So great to have Boston Joan.
[00:02:11.560 --> 00:02:12.800]   In fact, we were talking about you.
[00:02:12.800 --> 00:02:17.560]   We were using your name in vain because we were talking
[00:02:17.560 --> 00:02:20.000]   about a study that just you guys just put out
[00:02:20.000 --> 00:02:23.680]   data in society and I forgot what it was about.
[00:02:23.680 --> 00:02:29.400]   Now there's new studies all the time.
[00:02:29.400 --> 00:02:33.600]   Was it, yeah, give me a hint.
[00:02:33.600 --> 00:02:36.440]   Say it again, Carson, because Alex Jones.
[00:02:36.440 --> 00:02:37.880]   Oh, we were talking about--
[00:02:37.880 --> 00:02:38.800]   Oh, yeah.
[00:02:38.800 --> 00:02:41.240]   Yeah, and about how--
[00:02:41.240 --> 00:02:42.160]   actually, it was great.
[00:02:42.160 --> 00:02:45.400]   It was the article that started off with the Rockwell
[00:02:45.400 --> 00:02:48.160]   and the Nazis in the '60s.
[00:02:48.160 --> 00:02:49.160]   And we were talking about--
[00:02:49.160 --> 00:02:49.640]   Oh, yeah.
[00:02:49.640 --> 00:02:53.920]   Yeah, how he knew very well that his story wouldn't really
[00:02:53.920 --> 00:02:55.560]   get any traction by itself.
[00:02:55.560 --> 00:02:58.600]   But if you could get the mainstream media to pick it up,
[00:02:58.600 --> 00:03:01.120]   it would amplify it.
[00:03:01.120 --> 00:03:01.880]   Yeah, yeah.
[00:03:01.880 --> 00:03:05.960]   Now that has been sort of the vein of my existence
[00:03:05.960 --> 00:03:09.200]   for the last year, has been tracking what happened.
[00:03:09.200 --> 00:03:11.080]   After that, the right rally, there
[00:03:11.080 --> 00:03:15.480]   was lots and lots of action from platform companies
[00:03:15.480 --> 00:03:17.840]   to remove accounts, especially ones
[00:03:17.840 --> 00:03:20.960]   associated with known white supremacists,
[00:03:20.960 --> 00:03:23.880]   white nationalists that organized that rally.
[00:03:23.880 --> 00:03:27.520]   And overwhelmingly, what we find is that, yeah,
[00:03:27.520 --> 00:03:30.680]   if platforms don't give them the tools to coordinate,
[00:03:30.680 --> 00:03:34.280]   they have a really hard time gaining audience.
[00:03:34.280 --> 00:03:37.520]   And so it'll be interesting to see the fallout
[00:03:37.520 --> 00:03:43.440]   from the partial no-platforming of Alex Jones.
[00:03:43.440 --> 00:03:45.560]   But I don't know what's going to happen.
[00:03:45.560 --> 00:03:47.840]   I mean, he's still got stuff on Twitter.
[00:03:47.840 --> 00:03:51.120]   There's still plenty of content on YouTube.
[00:03:51.120 --> 00:03:53.280]   It's really hard.
[00:03:53.280 --> 00:03:55.880]   The adage that the internet is forever
[00:03:55.880 --> 00:03:59.920]   is still sort of true in this strange way.
[00:03:59.920 --> 00:04:03.440]   The internet, if you're a dedicated hunter of information,
[00:04:03.440 --> 00:04:06.440]   you probably can find it on the internet.
[00:04:06.440 --> 00:04:09.800]   And I think his die-hard followers
[00:04:09.800 --> 00:04:12.200]   are dedicated hunters of information.
[00:04:12.200 --> 00:04:14.360]   Although there are probably plenty who will be like,
[00:04:14.360 --> 00:04:16.360]   eh, OK.
[00:04:16.360 --> 00:04:20.160]   Yeah, I do think making it harder to link to inflammatory stuff
[00:04:20.160 --> 00:04:24.880]   that he has written or said is probably not a bad thing.
[00:04:24.880 --> 00:04:29.720]   And I think the biggest concern that I've had about this
[00:04:29.720 --> 00:04:33.240]   for the last year is really about content recommendation
[00:04:33.240 --> 00:04:34.200]   systems, right?
[00:04:34.200 --> 00:04:36.960]   So how does it end up getting served to people
[00:04:36.960 --> 00:04:39.920]   that are looking for information?
[00:04:39.920 --> 00:04:43.520]   He's probably got the most content online
[00:04:43.520 --> 00:04:45.080]   about Sandy Hook.
[00:04:45.080 --> 00:04:47.960]   And so if you're looking for information about what happened
[00:04:47.960 --> 00:04:50.320]   or even searching for information about,
[00:04:50.320 --> 00:04:53.800]   should I rent an apartment in that town,
[00:04:53.800 --> 00:04:57.600]   you end up being served in for war's content.
[00:04:57.600 --> 00:05:01.240]   So there's recommendation and search
[00:05:01.240 --> 00:05:06.240]   is something that we need to think a lot more about as we
[00:05:06.240 --> 00:05:10.360]   start to restructure information online.
[00:05:10.360 --> 00:05:16.080]   Was it your article that suggested quarantining?
[00:05:16.080 --> 00:05:19.000]   Yeah, so that's one of the things that Dana Boyd and I
[00:05:19.000 --> 00:05:21.400]   have been looking at for the last year,
[00:05:21.400 --> 00:05:25.360]   is if we go back into the '60s and '70s
[00:05:25.360 --> 00:05:29.680]   and understand what was the major shift in journalism
[00:05:29.680 --> 00:05:33.840]   that happened related to coverage of the KKK
[00:05:33.840 --> 00:05:37.320]   or the American Nazi Party, which is Rockwell's,
[00:05:37.320 --> 00:05:40.320]   the head of that.
[00:05:40.320 --> 00:05:45.760]   What you see is that media thought a lot about strategies
[00:05:45.760 --> 00:05:50.640]   for ensuring that they don't get coverage through violence.
[00:05:50.640 --> 00:05:55.320]   And so they would talk about quarantining.
[00:05:55.320 --> 00:05:58.480]   Journalists would talk about it as well as civil rights
[00:05:58.480 --> 00:06:02.320]   organizers would talk about quarantining certain extremist
[00:06:02.320 --> 00:06:09.760]   ideas or rallies so that it doesn't turn into a big public
[00:06:09.760 --> 00:06:11.000]   discussion.
[00:06:11.000 --> 00:06:16.440]   And what was particular about the KKK at that time
[00:06:16.440 --> 00:06:22.480]   was also that they were using really,
[00:06:22.480 --> 00:06:28.320]   like with cross-burnings and their parades and things,
[00:06:28.320 --> 00:06:30.360]   they really wanted to look intimidating.
[00:06:30.360 --> 00:06:33.600]   And so they often used fire as part of the imagery
[00:06:33.600 --> 00:06:35.760]   of their movements.
[00:06:35.760 --> 00:06:40.200]   But part of it was strategic to get front page coverage
[00:06:40.200 --> 00:06:46.800]   and so that they would end up in everybody's view.
[00:06:46.800 --> 00:06:51.440]   And so the use of optics, we saw directly translate
[00:06:51.440 --> 00:06:54.640]   into the organizing of that Unite the Right rally
[00:06:54.640 --> 00:06:55.720]   on that Friday night.
[00:06:55.720 --> 00:06:56.720]   The Tiki torches.
[00:06:56.720 --> 00:06:58.040]   Using those torches, exactly.
[00:06:58.040 --> 00:06:59.520]   This is-- Holy cow.
[00:06:59.520 --> 00:07:03.160]   Straight from the normal Lincoln Rockwell playbook.
[00:07:03.160 --> 00:07:03.760]   Exactly.
[00:07:03.760 --> 00:07:08.920]   And so those-- the notion of optics and how do you get wrong
[00:07:08.920 --> 00:07:09.920]   rock?
[00:07:09.920 --> 00:07:10.920]   Rock rock.
[00:07:10.920 --> 00:07:11.920]   Rockwell.
[00:07:11.920 --> 00:07:14.400]   I can't-- there's Lincoln in there somewhere.
[00:07:14.400 --> 00:07:15.400]   George?
[00:07:15.400 --> 00:07:15.920]   Yeah.
[00:07:15.920 --> 00:07:16.920]   George.
[00:07:16.920 --> 00:07:17.800]   You know what's great?
[00:07:17.800 --> 00:07:18.880]   We've forgotten his name.
[00:07:18.880 --> 00:07:19.600]   Awesome.
[00:07:19.600 --> 00:07:20.600]   I know, right?
[00:07:20.600 --> 00:07:21.840]   George Lincoln Rockwell.
[00:07:21.840 --> 00:07:22.760]   Sorry to remember it.
[00:07:22.760 --> 00:07:26.400]   Just the idea of spreading contents, especially
[00:07:26.400 --> 00:07:30.840]   through the images that they want to present about themselves.
[00:07:30.840 --> 00:07:33.200]   We're seeing these deep presidences with what
[00:07:33.200 --> 00:07:35.880]   we're going through now where lots of people
[00:07:35.880 --> 00:07:37.360]   are using social media.
[00:07:37.360 --> 00:07:39.880]   We're spreading images like this.
[00:07:39.880 --> 00:07:42.480]   And these images are really about two things.
[00:07:42.480 --> 00:07:43.600]   One is recruitment.
[00:07:43.600 --> 00:07:47.720]   That is, they want to look really strong and powerful and cool.
[00:07:47.720 --> 00:07:51.520]   And then the other thing is about spreading terror mayhem,
[00:07:51.520 --> 00:07:52.000]   right?
[00:07:52.000 --> 00:07:57.600]   And making people feel as if they are more powerful.
[00:07:57.600 --> 00:08:02.360]   So we are looking at what it might think--
[00:08:02.360 --> 00:08:07.640]   we're thinking through how platforms could adopt what we
[00:08:07.640 --> 00:08:11.800]   might call strategic amplification so that when
[00:08:11.800 --> 00:08:14.160]   events like this happen, they aren't
[00:08:14.160 --> 00:08:18.480]   susceptible to spreading ideologies or the images
[00:08:18.480 --> 00:08:22.480]   that these white supremacists really want to share.
[00:08:22.480 --> 00:08:27.960]   But rather are doing more responsible and more
[00:08:27.960 --> 00:08:31.720]   ethical recommendation.
[00:08:31.720 --> 00:08:32.080]   Got it.
[00:08:32.080 --> 00:08:34.680]   So turn the cameras off the flaming
[00:08:34.680 --> 00:08:37.800]   crosses and back to the counter protesters who
[00:08:37.800 --> 00:08:40.960]   might be holding up signs saying, hey, we love everybody
[00:08:40.960 --> 00:08:43.680]   or nice people doing nice things.
[00:08:43.680 --> 00:08:45.480]   It was almost something that bothered me.
[00:08:45.480 --> 00:08:50.280]   Way back to my college years about a certain category
[00:08:50.280 --> 00:08:52.840]   of propagandists.
[00:08:52.840 --> 00:08:54.640]   And we see them a lot more now.
[00:08:54.640 --> 00:08:57.040]   They tend to be on the right, but not always.
[00:08:57.040 --> 00:09:01.880]   They do a lot of, I'm joking, wink, wink stuff, dog
[00:09:01.880 --> 00:09:04.280]   whistling kind of, but in a joking way.
[00:09:04.280 --> 00:09:06.600]   I'm just joking so that if anybody confronts them
[00:09:06.600 --> 00:09:10.400]   on their evil rhetoric, their Nazi rhetoric, they just, oh,
[00:09:10.400 --> 00:09:12.840]   no, you can't you take a joke?
[00:09:12.840 --> 00:09:15.200]   You got no sense of humor.
[00:09:15.200 --> 00:09:17.840]   Yeah, and that's one of-- we have a report coming out.
[00:09:17.840 --> 00:09:21.800]   Hopefully next month we're still waiting on some of the editing
[00:09:21.800 --> 00:09:23.480]   and whatnot to come back.
[00:09:23.480 --> 00:09:27.240]   But it's about the uses of irony in these spaces.
[00:09:27.240 --> 00:09:33.760]   So this moment where you'll see tried and true neo-Nazis
[00:09:33.760 --> 00:09:36.880]   recruiting on social media platforms.
[00:09:36.880 --> 00:09:40.160]   And if they get caught, they move into this space of, oh,
[00:09:40.160 --> 00:09:41.280]   it's just parody.
[00:09:41.280 --> 00:09:42.600]   Oh, I'm being--
[00:09:42.600 --> 00:09:43.560]   I'm joking around.
[00:09:43.560 --> 00:09:45.960]   You can't take a joke.
[00:09:45.960 --> 00:09:48.160]   That's what something awful in 4chan
[00:09:48.160 --> 00:09:50.200]   and cert to some degree, right?
[00:09:50.200 --> 00:09:52.160]   It's a very common trope.
[00:09:52.160 --> 00:09:53.000]   Oh, come on.
[00:09:53.000 --> 00:09:54.000]   Yeah.
[00:09:54.000 --> 00:09:56.320]   This is where the lulls.
[00:09:56.320 --> 00:10:01.720]   Yeah, and that kind of hedging is difficult
[00:10:01.720 --> 00:10:05.760]   if you're running a content moderation protocol,
[00:10:05.760 --> 00:10:07.840]   because at the same time, you don't
[00:10:07.840 --> 00:10:10.000]   want to over moderate content and you
[00:10:10.000 --> 00:10:11.880]   want to have room for humor.
[00:10:11.880 --> 00:10:19.640]   But we see that ironic hedge is really running cover
[00:10:19.640 --> 00:10:25.840]   for getting out of strikes or bans on accounts that
[00:10:25.840 --> 00:10:28.400]   are spreading this not just content.
[00:10:28.400 --> 00:10:31.440]   And you'll see there's one very particular meme
[00:10:31.440 --> 00:10:35.120]   I'm thinking of right now, which is the Hitler did nothing
[00:10:35.120 --> 00:10:35.760]   wrong.
[00:10:35.760 --> 00:10:40.000]   And so you'll see this on all of these other spaces.
[00:10:40.000 --> 00:10:41.440]   And it's become a joke.
[00:10:41.440 --> 00:10:47.320]   And even in some of the subtext of conversations online
[00:10:47.320 --> 00:10:50.960]   with other people, they'll make reference to this meme.
[00:10:50.960 --> 00:10:52.760]   Maybe they'll substitute, not Hitler.
[00:10:52.760 --> 00:10:56.160]   But you'll see people tweeting, Alex Jones did nothing wrong.
[00:10:56.160 --> 00:10:58.880]   And it's a wink and a nod to this other meme
[00:10:58.880 --> 00:11:03.160]   that is done out of irony and jest.
[00:11:03.160 --> 00:11:07.720]   But also useful to the recruitment
[00:11:07.720 --> 00:11:10.080]   by some of these white nationalists online.
[00:11:10.080 --> 00:11:12.200]   This is why I've just wiped--
[00:11:12.200 --> 00:11:15.160]   washed my hands of social media.
[00:11:15.160 --> 00:11:16.160]   So thank you.
[00:11:16.160 --> 00:11:18.400]   I really tried to fight this for the longest time
[00:11:18.400 --> 00:11:19.840]   and say, no, there's good stuff there.
[00:11:19.840 --> 00:11:22.840]   Stacey, you've always said, there's real value in it.
[00:11:22.840 --> 00:11:24.200]   I hear that a lot from people.
[00:11:24.200 --> 00:11:27.960]   Despite the pollution, there's something good in there.
[00:11:27.960 --> 00:11:30.240]   But I'm just tired of drinking from that stream.
[00:11:30.240 --> 00:11:32.200]   There's too much pollution in this.
[00:11:32.200 --> 00:11:37.680]   So this is beyond the issues of white supremacy
[00:11:37.680 --> 00:11:40.640]   and doing it for the lulls aspect of it.
[00:11:40.640 --> 00:11:43.960]   This is something that I know women face all the time
[00:11:43.960 --> 00:11:48.120]   just in counteracting sexist language and behavior.
[00:11:48.120 --> 00:11:52.400]   So I'm thinking about, Joan, from a, I guess,
[00:11:52.400 --> 00:11:56.520]   a strategic point of view as a platform, as a media company.
[00:11:56.520 --> 00:11:58.240]   Because there is a subset.
[00:11:58.240 --> 00:12:00.800]   I remember myself in my teen years
[00:12:00.800 --> 00:12:03.560]   because teenagers are all a bunch of jerks.
[00:12:03.560 --> 00:12:07.640]   We're like, ha, ha, ha, ha, ha, ha, we're so cutting edge.
[00:12:07.640 --> 00:12:12.640]   And so I'm trying to think about how you both recognize that
[00:12:12.640 --> 00:12:16.240]   and adapt to it because that is how some of these people
[00:12:16.240 --> 00:12:17.200]   do get sucked in.
[00:12:17.200 --> 00:12:19.640]   At some point in time, you are I,
[00:12:19.640 --> 00:12:22.160]   not that I joked about Hitler or Nazis,
[00:12:22.160 --> 00:12:26.560]   but you do have a blacker sense of humor, perhaps.
[00:12:26.560 --> 00:12:29.880]   And so I don't know.
[00:12:29.880 --> 00:12:33.400]   Like, how do you engage those people or not engage those people
[00:12:33.400 --> 00:12:35.640]   and help them not be recruited further in?
[00:12:35.640 --> 00:12:37.120]   I don't know.
[00:12:37.120 --> 00:12:37.960]   There's a lot of questions.
[00:12:37.960 --> 00:12:39.040]   Yeah.
[00:12:39.040 --> 00:12:42.640]   No, and I think that there's no one solution
[00:12:42.640 --> 00:12:45.480]   that we can just adopt and say, oh, this is the method
[00:12:45.480 --> 00:12:46.560]   that's going to work, right?
[00:12:46.560 --> 00:12:51.680]   So there are groups of people that are dedicated to engaging
[00:12:51.680 --> 00:12:57.080]   with people who post much, much more violence rhetoric online.
[00:12:57.080 --> 00:13:01.600]   And in a one-to-one scenario, we'll
[00:13:01.600 --> 00:13:04.760]   try to understand their motivations.
[00:13:04.760 --> 00:13:07.640]   So this is especially done with people
[00:13:07.640 --> 00:13:12.000]   who are posting more anti-immigrant sentiments.
[00:13:12.000 --> 00:13:15.120]   Then there's other de-escalation techniques
[00:13:15.120 --> 00:13:19.320]   where people will go and redirect the content online.
[00:13:19.320 --> 00:13:21.600]   So people will be talking about one thing,
[00:13:21.600 --> 00:13:26.800]   and then people might intervene by spamming the thread
[00:13:26.800 --> 00:13:30.080]   to get people talking about something else,
[00:13:30.080 --> 00:13:34.440]   especially if things start to get too dark or too
[00:13:34.440 --> 00:13:35.680]   serious.
[00:13:35.680 --> 00:13:38.600]   But one of the things that I think we've been wholly unprepared
[00:13:38.600 --> 00:13:42.880]   for is understanding the relationship across all
[00:13:42.880 --> 00:13:49.880]   of these platforms and where content on something like YouTube
[00:13:49.880 --> 00:13:52.080]   is being gamed, but not on YouTube.
[00:13:52.080 --> 00:13:57.960]   It's being gamed in off-platform chat rooms or message
[00:13:57.960 --> 00:13:59.840]   boards where they're saying, oh, I
[00:13:59.840 --> 00:14:02.280]   noticed that YouTube is starting to moderate
[00:14:02.280 --> 00:14:05.240]   for the use of X, Y, and Z speech.
[00:14:05.240 --> 00:14:08.200]   So let's change how we talk about these people
[00:14:08.200 --> 00:14:11.200]   to use this other dog whistle.
[00:14:11.200 --> 00:14:13.840]   And so they're constantly updating their terminology
[00:14:13.840 --> 00:14:19.120]   and updating through other platforms
[00:14:19.120 --> 00:14:21.760]   ways of getting around the terms of service
[00:14:21.760 --> 00:14:26.800]   on much larger platforms that are
[00:14:26.800 --> 00:14:29.680]   capable of doing content moderation.
[00:14:29.680 --> 00:14:33.200]   But we see that the gaming of systems usually
[00:14:33.200 --> 00:14:38.640]   is happening in spaces that have a lot less moderation than YouTube,
[00:14:38.640 --> 00:14:42.000]   Facebook, or Twitter, and then Facebook and Twitter and YouTube
[00:14:42.000 --> 00:14:46.120]   are really the spaces where these techniques are deployed.
[00:14:46.120 --> 00:14:47.080]   OK.
[00:14:47.080 --> 00:14:49.120]   I think it's kind of funny mentioning the gaming
[00:14:49.120 --> 00:14:52.080]   of the system because my daughter, who is--
[00:14:52.080 --> 00:14:54.000]   she just turned 12--
[00:14:54.000 --> 00:14:56.440]   she plays an online game called Star Stables,
[00:14:56.440 --> 00:15:00.440]   and they're not allowed to talk about a lot of things.
[00:15:00.440 --> 00:15:02.920]   As a means to protect kids who are playing it.
[00:15:02.920 --> 00:15:06.160]   But it is hilarious how the kids get around it.
[00:15:06.160 --> 00:15:08.240]   And this is innocuous.
[00:15:08.240 --> 00:15:12.120]   My child is not being a jerk coming.
[00:15:12.120 --> 00:15:14.040]   It's an exciting online game where
[00:15:14.040 --> 00:15:18.720]   adventures, horses, and mysteries are waiting to be explored.
[00:15:18.720 --> 00:15:19.720]   Yes.
[00:15:19.720 --> 00:15:24.320]   But the point is, I think that's as light as kids
[00:15:24.320 --> 00:15:27.800]   trying to get around that so they can make friendships on this game
[00:15:27.800 --> 00:15:29.520]   or talk to each other about things
[00:15:29.520 --> 00:15:33.720]   that the moderators don't want them to talk about on Global Chat.
[00:15:33.720 --> 00:15:36.160]   All the way to censorship in China.
[00:15:36.160 --> 00:15:38.640]   So this is just people.
[00:15:38.640 --> 00:15:41.040]   But I appreciate--
[00:15:41.040 --> 00:15:42.480]   I don't know, Joan.
[00:15:42.480 --> 00:15:43.720]   It feels like you've got to be--
[00:15:43.720 --> 00:15:44.920]   is it tilting at Windmills?
[00:15:44.920 --> 00:15:45.520]   I don't know.
[00:15:45.520 --> 00:15:46.320]   I'm just like, oh.
[00:15:46.320 --> 00:15:46.920]   Yeah.
[00:15:46.920 --> 00:15:49.560]   Well, the way that we do our research, I think,
[00:15:49.560 --> 00:15:54.960]   is really about bringing to light these different techniques
[00:15:54.960 --> 00:16:00.040]   and understanding where design decisions might make a difference,
[00:16:00.040 --> 00:16:04.520]   but also understanding where this is a broad social problem.
[00:16:04.520 --> 00:16:09.800]   And so we don't want to put all the blame on platforms and technology
[00:16:09.800 --> 00:16:12.200]   companies because we do know that there
[00:16:12.200 --> 00:16:18.160]   are social networks of real people involved that are doing--
[00:16:18.160 --> 00:16:23.440]   they're doing quite a bit of ingenuity to get around these systems.
[00:16:23.440 --> 00:16:30.080]   And I think it speaks to a longer history that we have here where
[00:16:30.080 --> 00:16:34.600]   now we have had the internet in people's homes for long enough
[00:16:34.600 --> 00:16:38.680]   that now that I'm seeing journalism is tilted towards reporting
[00:16:38.680 --> 00:16:39.680]   what's online.
[00:16:39.680 --> 00:16:42.560]   And we're seeing less and less reporting about what's
[00:16:42.560 --> 00:16:44.840]   happening in meat space.
[00:16:44.840 --> 00:16:46.200]   And so it's been interesting to see
[00:16:46.200 --> 00:16:51.640]   that shift where the online world is becoming more and more real
[00:16:51.640 --> 00:16:57.080]   as we lose less and less of a hold and a grip on what's
[00:16:57.080 --> 00:16:59.160]   happening offline.
[00:16:59.160 --> 00:17:03.400]   I don't know if it's more real, but it is an easier thing to cover.
[00:17:03.400 --> 00:17:06.880]   And one of the challenges of local journalism
[00:17:06.880 --> 00:17:09.280]   is that you can only get, however many people
[00:17:09.280 --> 00:17:13.760]   care about your local issues to read it in an era of scale.
[00:17:13.760 --> 00:17:17.400]   Talking about something online matters to everyone
[00:17:17.400 --> 00:17:20.400]   who's online, which is all the possible clicks.
[00:17:20.400 --> 00:17:23.160]   So I think that is one of the challenges.
[00:17:23.160 --> 00:17:24.920]   And I will give you so much money, Joan,
[00:17:24.920 --> 00:17:27.720]   if you never say the word "meanspace" again.
[00:17:27.720 --> 00:17:29.080]   It is disgusting.
[00:17:29.080 --> 00:17:30.840]   It's making me want to be a vegetarian,
[00:17:30.840 --> 00:17:32.680]   but I understand what you're saying.
[00:17:32.680 --> 00:17:36.560]   It's because I can't in good faith use the word offline anymore.
[00:17:36.560 --> 00:17:38.560]   I mean, I've got some phone.
[00:17:38.560 --> 00:17:40.360]   I can see.
[00:17:40.360 --> 00:17:42.960]   I was watching TV last night.
[00:17:42.960 --> 00:17:46.640]   And there were successive ads, several, that began and end
[00:17:46.640 --> 00:17:48.360]   with people looking at their smartphones.
[00:17:48.360 --> 00:17:51.200]   And what I realized is it wasn't a coincidence.
[00:17:51.200 --> 00:17:54.000]   It's just so much part of the way life is that everybody's
[00:17:54.000 --> 00:17:55.840]   always looking at the smartphone that if you're
[00:17:55.840 --> 00:17:57.880]   going to represent it in an advertisement,
[00:17:57.880 --> 00:17:59.280]   if you're going to represent people,
[00:17:59.280 --> 00:18:00.480]   they're going to be looking at their phone.
[00:18:00.480 --> 00:18:03.000]   It had nothing to do with smartphones.
[00:18:03.000 --> 00:18:04.480]   That's what they were doing.
[00:18:04.480 --> 00:18:06.600]   So meanwhile, your smartphone is tracking you.
[00:18:06.600 --> 00:18:09.200]   We talked about this last week, AP,
[00:18:09.200 --> 00:18:12.640]   discovered that even when your location history is turned off,
[00:18:12.640 --> 00:18:14.520]   if you launch certain apps, Google
[00:18:14.520 --> 00:18:17.760]   will keep track of where you are.
[00:18:17.760 --> 00:18:21.200]   I got in a little battle yesterday with Steve Gibson,
[00:18:21.200 --> 00:18:23.760]   who says, well, they really shouldn't.
[00:18:23.760 --> 00:18:28.280]   And it's complicated because there are other settings as well.
[00:18:28.280 --> 00:18:30.560]   And Google does kind of hide these settings.
[00:18:30.560 --> 00:18:32.960]   But they do warn you when location history has turned off
[00:18:32.960 --> 00:18:35.760]   that you still will be sending location to Google.
[00:18:35.760 --> 00:18:38.640]   Any time you do searches, it says,
[00:18:38.640 --> 00:18:41.720]   some location data may be saved as part of your activity
[00:18:41.720 --> 00:18:44.000]   and other services like Search and Maps.
[00:18:44.000 --> 00:18:46.720]   Actually, that's the new language that Google stuck
[00:18:46.720 --> 00:18:48.120]   in there last Friday.
[00:18:48.120 --> 00:18:50.560]   Yeah, I was going to say, I haven't ever seen that.
[00:18:50.560 --> 00:18:54.240]   And I turn my location off all the time.
[00:18:54.240 --> 00:18:55.560]   That's different.
[00:18:55.560 --> 00:18:59.280]   So you're turning location off or location history off?
[00:18:59.280 --> 00:19:01.440]   I turn location history off.
[00:19:01.440 --> 00:19:03.160]   And sometimes I actually--
[00:19:03.160 --> 00:19:06.080]   well, now I don't think I can turn the GPS off unless I go
[00:19:06.080 --> 00:19:07.880]   into airplane mode.
[00:19:07.880 --> 00:19:10.240]   But I used to be able to turn all of my location off.
[00:19:10.240 --> 00:19:13.840]   I used to be a setting that you could turn off location.
[00:19:13.840 --> 00:19:17.280]   I wish, like, I'll be honest, my mom no longer
[00:19:17.280 --> 00:19:19.760]   takes her phone with her when she leaves to go places
[00:19:19.760 --> 00:19:21.280]   unless she--
[00:19:21.280 --> 00:19:23.800]   Unless how are we going to find her if she does that?
[00:19:23.800 --> 00:19:26.680]   So my favorite thing is she brings it to places
[00:19:26.680 --> 00:19:30.120]   if she has an online coupon she needs to show the store clerk.
[00:19:30.120 --> 00:19:31.240]   So then she'll bring her phone.
[00:19:31.240 --> 00:19:34.400]   But the rest of the time, she's like, no phone.
[00:19:34.400 --> 00:19:37.360]   Well, now Google's being sued by a San Diego man,
[00:19:37.360 --> 00:19:41.000]   activists in Washington are urging the FTC to--
[00:19:41.000 --> 00:19:43.160]   and I think this is probably appropriate-- investigate
[00:19:43.160 --> 00:19:46.640]   whether the company is in breach of its consent
[00:19:46.640 --> 00:19:49.200]   decree from 2011.
[00:19:49.200 --> 00:19:52.880]   The lawsuit was filed on Friday in San Francisco
[00:19:52.880 --> 00:19:56.360]   for representing a guy named Napoleon Patasil,
[00:19:56.360 --> 00:19:59.280]   argued that Google is violating the California Invasion
[00:19:59.280 --> 00:20:03.160]   of Privacy Act and the state's constitutional right
[00:20:03.160 --> 00:20:03.880]   to privacy.
[00:20:03.880 --> 00:20:06.480]   They're trying to get class action sued on this.
[00:20:06.480 --> 00:20:09.600]   Epic, we talked about the Electronic Privacy Information
[00:20:09.600 --> 00:20:10.360]   Center.
[00:20:10.360 --> 00:20:12.600]   A sternly worded, according to ours,
[00:20:12.600 --> 00:20:16.160]   technical three page letter to the FTC.
[00:20:16.160 --> 00:20:19.280]   This is a tricky one because it's clear
[00:20:19.280 --> 00:20:22.360]   that Google didn't really want to make it too easy
[00:20:22.360 --> 00:20:24.440]   to turn off location.
[00:20:24.440 --> 00:20:25.840]   On the other hand, it's not hard.
[00:20:25.840 --> 00:20:27.840]   It's not-- I don't think it's a stretch to say, well,
[00:20:27.840 --> 00:20:30.720]   when you use maps and search, Google's--
[00:20:30.720 --> 00:20:33.960]   it's helpful for Google to save that information
[00:20:33.960 --> 00:20:36.400]   so you can find your way back so that your search
[00:20:36.400 --> 00:20:38.640]   is location-based.
[00:20:38.640 --> 00:20:40.560]   But ultimately, it's in their commercial interest
[00:20:40.560 --> 00:20:42.280]   that they save your location information.
[00:20:42.280 --> 00:20:45.520]   So I think this is a good thing that this came to the surface
[00:20:45.520 --> 00:20:51.480]   in that Google probably will modify its settings and verbiage
[00:20:51.480 --> 00:20:55.040]   to reflect really what should be reflected, which
[00:20:55.040 --> 00:20:58.040]   is how to do this and whether you've actually done it.
[00:20:58.040 --> 00:21:02.040]   Yes.
[00:21:02.040 --> 00:21:04.960]   Again, I will say for a company that
[00:21:04.960 --> 00:21:08.600]   wants to suck up as much data as it does.
[00:21:08.600 --> 00:21:11.520]   And I will give it full credit for using that data
[00:21:11.520 --> 00:21:15.040]   to make a lot a hugely better experience for me.
[00:21:15.040 --> 00:21:15.880]   I love it.
[00:21:15.880 --> 00:21:20.000]   But I'm kind of like, Google, I'm giving you all this.
[00:21:20.000 --> 00:21:21.920]   Let's work on the trust here.
[00:21:21.920 --> 00:21:24.320]   Yes, that seems fair.
[00:21:24.320 --> 00:21:26.480]   I'll give it to you if you say, hey,
[00:21:26.480 --> 00:21:27.640]   this is what I'm going to do with it.
[00:21:27.640 --> 00:21:30.240]   When you're going to get all cagey with me, I'm like, oh.
[00:21:30.240 --> 00:21:30.760]   Well--
[00:21:30.760 --> 00:21:31.360]   Oh, I don't know.
[00:21:31.360 --> 00:21:35.680]   And here is an example of why you might not
[00:21:35.680 --> 00:21:38.720]   want Google to always keep all of the information
[00:21:38.720 --> 00:21:39.560]   about where you are.
[00:21:39.560 --> 00:21:43.560]   Google's fighting this, but nevertheless,
[00:21:43.560 --> 00:21:47.520]   it's concerning the FBI investigating
[00:21:47.520 --> 00:21:51.960]   a spate of armed robberies across Portland, Maine.
[00:21:51.960 --> 00:21:54.120]   Made an unprecedented request of Google.
[00:21:54.120 --> 00:21:57.520]   This is back in March, but it's coming forward now.
[00:21:57.520 --> 00:22:01.280]   The feds said, we would like the information
[00:22:01.280 --> 00:22:05.960]   of everybody who uses a Google phone or uses Google services,
[00:22:05.960 --> 00:22:08.200]   who was within the vicinity of at least two
[00:22:08.200 --> 00:22:12.320]   of these nine robberies within the 30-minute time
[00:22:12.320 --> 00:22:14.680]   frames of the robberies.
[00:22:14.680 --> 00:22:16.920]   The space covered--
[00:22:16.920 --> 00:22:18.680]   I don't know why it's in hectares, but it is,
[00:22:18.680 --> 00:22:21.960]   and I can't convert, but it's a large area, 45 hectares,
[00:22:21.960 --> 00:22:24.880]   includes anyone with an Android or iPhone using Google's
[00:22:24.880 --> 00:22:26.400]   tools.
[00:22:26.400 --> 00:22:27.800]   And then they said, not only would
[00:22:27.800 --> 00:22:29.680]   want a list of everybody who was in that area--
[00:22:29.680 --> 00:22:31.120]   I mean, this is a phishing expert.
[00:22:31.120 --> 00:22:34.480]   They don't even know that the suspects were carrying phones.
[00:22:34.480 --> 00:22:36.720]   This is exactly what a phishing--
[00:22:36.720 --> 00:22:38.960]   the definition of a phishing expedition.
[00:22:38.960 --> 00:22:41.880]   Well, let's just find out everybody who was in that area.
[00:22:41.880 --> 00:22:44.680]   They wanted to know of those people
[00:22:44.680 --> 00:22:47.200]   in that area within that 30-minute time frame,
[00:22:47.200 --> 00:22:51.840]   full names, addresses, Google account activity,
[00:22:51.840 --> 00:22:56.040]   all affected users' historical locations.
[00:22:56.040 --> 00:22:58.240]   Google did not provide the information,
[00:22:58.240 --> 00:23:00.280]   but the fact is they probably had it, right?
[00:23:00.280 --> 00:23:02.160]   And we know they had it.
[00:23:02.160 --> 00:23:05.280]   And by the way, this happened in March,
[00:23:05.280 --> 00:23:07.120]   and they did find their suspect in the end
[00:23:07.120 --> 00:23:09.600]   without Google's assistance.
[00:23:09.600 --> 00:23:10.440]   But it's--
[00:23:10.440 --> 00:23:12.400]   There was-- extremely concerning.
[00:23:12.400 --> 00:23:18.160]   We just had a Supreme Court case on this, and Lord help me.
[00:23:18.160 --> 00:23:20.160]   We didn't-- I know, and I can feel you know the details.
[00:23:20.160 --> 00:23:23.480]   It didn't, unfortunately, limit it
[00:23:23.480 --> 00:23:25.200]   as much as we would have liked.
[00:23:25.200 --> 00:23:26.720]   Oh.
[00:23:26.720 --> 00:23:29.840]   We-- I thought-- and this was all about the guy who'd
[00:23:29.840 --> 00:23:31.680]   robbed a number of convenience stores.
[00:23:31.680 --> 00:23:32.600]   Yes.
[00:23:32.600 --> 00:23:33.440]   And--
[00:23:33.440 --> 00:23:34.280]   Baltimore?
[00:23:34.280 --> 00:23:34.840]   In Baltimore.
[00:23:34.840 --> 00:23:35.600]   Yeah, somewhere.
[00:23:35.600 --> 00:23:40.880]   And they had ended up getting his location history
[00:23:40.880 --> 00:23:45.040]   for a long period of time without using only a pen register.
[00:23:45.040 --> 00:23:45.320]   And--
[00:23:45.320 --> 00:23:45.680]   Right.
[00:23:45.680 --> 00:23:47.200]   And by the way, I've been corrected,
[00:23:47.200 --> 00:23:48.520]   at least in the state of California,
[00:23:48.520 --> 00:23:51.120]   a pen register does now require a warrant.
[00:23:51.120 --> 00:23:53.080]   And I'm sure this request of Google
[00:23:53.080 --> 00:23:54.560]   required a warrant of the FBI.
[00:23:54.560 --> 00:23:56.960]   That's the sad thing, is I think the court approved it.
[00:23:56.960 --> 00:24:01.320]   In any event, the real concern was about these warrantless
[00:24:01.320 --> 00:24:02.720]   location searches.
[00:24:02.720 --> 00:24:04.640]   And the Supreme Court decision essentially
[00:24:04.640 --> 00:24:06.920]   said what they did wrong in that case
[00:24:06.920 --> 00:24:09.360]   was they collected too much information,
[00:24:09.360 --> 00:24:12.960]   that they were getting days of information.
[00:24:12.960 --> 00:24:16.920]   But they did-- but the opinion implied
[00:24:16.920 --> 00:24:19.240]   that a day or two would be fine.
[00:24:19.240 --> 00:24:21.720]   It's the days of information that
[00:24:21.720 --> 00:24:26.240]   exposed all sorts of other information about the suspect.
[00:24:26.240 --> 00:24:28.520]   And that was a violation of his rights.
[00:24:28.520 --> 00:24:32.200]   So it did not in any way really limit--
[00:24:32.200 --> 00:24:34.480]   in fact, didn't limit this specific kind of search.
[00:24:34.480 --> 00:24:37.440]   Because all they're saying is, we just want 30 minutes.
[00:24:37.440 --> 00:24:39.280]   So what would be interesting, perhaps,
[00:24:39.280 --> 00:24:41.880]   is if in this phishing expedition,
[00:24:41.880 --> 00:24:44.480]   if I knew I happened to be there and I
[00:24:44.480 --> 00:24:46.560]   was at some sort of dicey location,
[00:24:46.560 --> 00:24:49.240]   maybe I was at a Planned Parenthood getting an abortion,
[00:24:49.240 --> 00:24:51.880]   or I was at a sex shop or something like that,
[00:24:51.880 --> 00:24:54.280]   I could count countersuit.
[00:24:54.280 --> 00:24:58.880]   I could argue that this is violating my privacy.
[00:24:58.880 --> 00:24:59.920]   I don't know.
[00:24:59.920 --> 00:25:01.480]   But it would be tough, because you'd
[00:25:01.480 --> 00:25:03.280]   have to have known about it.
[00:25:03.280 --> 00:25:04.360]   That's part of the problem.
[00:25:04.360 --> 00:25:07.560]   The ACLU said it's unlikely the average user of Google services
[00:25:07.560 --> 00:25:12.680]   would know such government searches were even possible.
[00:25:12.680 --> 00:25:15.120]   And another attorney said, the warrant
[00:25:15.120 --> 00:25:17.720]   amounts to a completely indiscriminate search
[00:25:17.720 --> 00:25:19.000]   of a large group of people.
[00:25:19.000 --> 00:25:21.720]   This is from an article in Forbes.
[00:25:21.720 --> 00:25:23.480]   Even though they limited the search to users
[00:25:23.480 --> 00:25:25.600]   who'd been at the two of the locations within certain time
[00:25:25.600 --> 00:25:27.560]   frames, that is a general search.
[00:25:27.560 --> 00:25:31.840]   It is prohibited under the Constitution.
[00:25:31.840 --> 00:25:32.760]   And it should be.
[00:25:32.760 --> 00:25:33.680]   I mean, it should be.
[00:25:33.680 --> 00:25:36.480]   You can't rummage around in people's stuff
[00:25:36.480 --> 00:25:39.560]   hoping to find illegal material.
[00:25:39.560 --> 00:25:41.680]   You have to have probable cause.
[00:25:41.680 --> 00:25:44.000]   I wonder, though, if there is a way
[00:25:44.000 --> 00:25:46.880]   that they could have gotten this kind of data
[00:25:46.880 --> 00:25:50.480]   by purchasing data from these companies.
[00:25:50.480 --> 00:25:51.400]   It's not luxurious.
[00:25:51.400 --> 00:25:52.320]   People are like, if you--
[00:25:52.320 --> 00:25:54.160]   Well, that's that pen register thing.
[00:25:54.160 --> 00:25:58.360]   So I had a law enforcement guy in a couple of days ago.
[00:25:58.360 --> 00:26:01.720]   He said, I think you misunderstand, at least in California,
[00:26:01.720 --> 00:26:03.280]   how pen registers are handled.
[00:26:03.280 --> 00:26:05.160]   They are not warrantless searches.
[00:26:05.160 --> 00:26:08.080]   So I had for a long-- and they were until a few years ago.
[00:26:08.080 --> 00:26:09.600]   And that's where I was confused.
[00:26:09.600 --> 00:26:10.960]   The law changed.
[00:26:10.960 --> 00:26:13.160]   In California, I don't know where else.
[00:26:13.160 --> 00:26:15.440]   A few years ago, that you couldn't just
[00:26:15.440 --> 00:26:18.080]   ask the phone company for location information.
[00:26:18.080 --> 00:26:21.320]   You had to get a warrant to do that.
[00:26:21.320 --> 00:26:22.520]   So nevertheless--
[00:26:22.520 --> 00:26:24.880]   But I'm going to show that we should
[00:26:24.880 --> 00:26:28.800]   have some kind of system of transparency
[00:26:28.800 --> 00:26:32.080]   and auditing where these companies have to,
[00:26:32.080 --> 00:26:35.800]   either every six months, every year, disclose when
[00:26:35.800 --> 00:26:39.000]   and how they have shared your data with others,
[00:26:39.000 --> 00:26:43.520]   be it law enforcement or whatever companies they've
[00:26:43.520 --> 00:26:45.280]   sold your data to.
[00:26:45.280 --> 00:26:48.040]   I've looked at in Facebook and on Twitter now,
[00:26:48.040 --> 00:26:51.440]   you can look at the ads that are targeted to you
[00:26:51.440 --> 00:26:53.440]   and find out why they're targeted to you
[00:26:53.440 --> 00:26:57.960]   and what specific metadata categories they were using
[00:26:57.960 --> 00:26:59.080]   to find you.
[00:26:59.080 --> 00:27:03.840]   But to me, this system like this, if it is going to go through
[00:27:03.840 --> 00:27:07.800]   and there's going to be little we can do to stop these companies
[00:27:07.800 --> 00:27:11.320]   from sharing data between other institutions, organizations,
[00:27:11.320 --> 00:27:15.000]   selling it, that there should be a system, at least,
[00:27:15.000 --> 00:27:17.000]   in the very, very least, where there's
[00:27:17.000 --> 00:27:21.040]   disclosure of when, where, why, and how,
[00:27:21.040 --> 00:27:23.040]   and what data was shared.
[00:27:23.040 --> 00:27:29.960]   The FBI agent in justifying the need for the warrant
[00:27:29.960 --> 00:27:31.720]   wrote, quote, "It would identify which
[00:27:31.720 --> 00:27:35.040]   cellular devices were near two or more of the locations
[00:27:35.040 --> 00:27:36.800]   where the robberies occurred at the date and time
[00:27:36.800 --> 00:27:39.000]   the robberies occurred and may assist law enforcement
[00:27:39.000 --> 00:27:41.680]   determining which persons were present or involved
[00:27:41.680 --> 00:27:44.320]   with the robberies under investigation."
[00:27:44.320 --> 00:27:47.280]   I, you know, it was signed off by a warrant,
[00:27:47.280 --> 00:27:48.520]   signed off by a judge.
[00:27:48.520 --> 00:27:51.520]   The judge also sent a gag order to Google,
[00:27:51.520 --> 00:27:54.440]   which is the only reason we only are learning about this now
[00:27:54.440 --> 00:27:58.160]   because they were required to keep the warrant secret for 180
[00:27:58.160 --> 00:28:00.000]   days for six months.
[00:28:00.000 --> 00:28:03.280]   So that's why we're learning now about it.
[00:28:03.280 --> 00:28:05.600]   Google didn't do it.
[00:28:05.600 --> 00:28:08.480]   They were expected to return the information April 19.
[00:28:08.480 --> 00:28:09.120]   They did not.
[00:28:09.120 --> 00:28:12.080]   The FBI filed a motion to extend the time it had to get the data.
[00:28:12.080 --> 00:28:13.400]   A judge granted it.
[00:28:13.400 --> 00:28:14.680]   That's really what's scaring me.
[00:28:14.680 --> 00:28:17.040]   More than Google collecting the information
[00:28:17.040 --> 00:28:19.840]   that judges are granting these warrants.
[00:28:19.840 --> 00:28:21.440]   And I think that's not the end of it.
[00:28:21.440 --> 00:28:24.040]   Now that law enforcement knows that that treasure
[00:28:24.040 --> 00:28:26.120]   trove of information exists, they're
[00:28:26.120 --> 00:28:28.520]   going to go after it more and more.
[00:28:28.520 --> 00:28:29.840]   And you can get judges to--
[00:28:29.840 --> 00:28:31.400]   I mean, there are crazy judges.
[00:28:31.400 --> 00:28:34.160]   I mean, I live in Texas where we elect our judges.
[00:28:34.160 --> 00:28:38.680]   So we have those who are often--
[00:28:38.680 --> 00:28:39.520]   Politicized.
[00:28:39.520 --> 00:28:42.880]   --they're like, yes, I will give all those crazy data,
[00:28:42.880 --> 00:28:45.360]   especially on those illegals and crazy--
[00:28:45.360 --> 00:28:47.520]   Well, but I've had debates with normal,
[00:28:47.520 --> 00:28:50.000]   completely reasonable people who say, no,
[00:28:50.000 --> 00:28:51.800]   if you can help law enforcement, Google should,
[00:28:51.800 --> 00:28:52.880]   Apple should.
[00:28:52.880 --> 00:28:53.880]   You can help law enforcement.
[00:28:53.880 --> 00:28:54.880]   It's hard to explain to them.
[00:28:54.880 --> 00:28:55.520]   Not in this way.
[00:28:55.520 --> 00:28:57.280]   Well, it's hard to explain to people
[00:28:57.280 --> 00:28:59.080]   why this is a bad thing.
[00:28:59.080 --> 00:29:02.640]   In the case of the encrypted iPhone and the San Bernardino
[00:29:02.640 --> 00:29:05.600]   issue, it's hard to explain to people that, well, yeah,
[00:29:05.600 --> 00:29:07.760]   but if you get the government a backdoor,
[00:29:07.760 --> 00:29:11.000]   then the risk of others, hackers and bad guys
[00:29:11.000 --> 00:29:14.320]   getting access through that backdoor, is very high.
[00:29:14.320 --> 00:29:17.560]   Have they not read their history?
[00:29:17.560 --> 00:29:18.560]   I'm not.
[00:29:18.560 --> 00:29:19.640]   I just don't understand.
[00:29:19.640 --> 00:29:20.680]   But there's no-- well, here's what
[00:29:20.680 --> 00:29:22.400]   there isn't any history of.
[00:29:22.400 --> 00:29:27.040]   There's never been a data collection device as good as this.
[00:29:27.040 --> 00:29:27.560]   No.
[00:29:27.560 --> 00:29:32.280]   These smartphones are a law enforcement officer's dream come
[00:29:32.280 --> 00:29:34.240]   true.
[00:29:34.240 --> 00:29:36.760]   And this is where we're going to really face a battle,
[00:29:36.760 --> 00:29:39.000]   I think, going forward between people
[00:29:39.000 --> 00:29:43.080]   and say, but we want to give police every weapon possible
[00:29:43.080 --> 00:29:45.120]   to fight crime.
[00:29:45.120 --> 00:29:48.520]   And it turns out that everybody's
[00:29:48.520 --> 00:29:50.240]   carrying a surveillance device all the time.
[00:29:50.240 --> 00:29:53.080]   So why wouldn't you give police access to that?
[00:29:53.080 --> 00:29:54.600]   And this is not the end of this.
[00:29:54.600 --> 00:29:55.560]   This is going to be.
[00:29:55.560 --> 00:29:58.320]   Now Google, to their credit-- and I want to point this out
[00:29:58.320 --> 00:29:59.280]   as well--
[00:29:59.280 --> 00:30:02.840]   deny, deny, deny, deny, they deny four motions.
[00:30:02.840 --> 00:30:06.400]   Eventually, the feds gave up.
[00:30:06.400 --> 00:30:10.120]   Well, so is there-- what would be an interesting app
[00:30:10.120 --> 00:30:13.240]   is something that would generate false information
[00:30:13.240 --> 00:30:14.400]   about where someone is.
[00:30:14.400 --> 00:30:17.840]   So services that you would run, maybe they're VPNs,
[00:30:17.840 --> 00:30:22.480]   that you run your data through that send basically fake data.
[00:30:22.480 --> 00:30:23.880]   Is that even technically possible?
[00:30:23.880 --> 00:30:25.880]   Sure.
[00:30:25.880 --> 00:30:26.880]   So that--
[00:30:26.880 --> 00:30:29.120]   But here's my real point.
[00:30:29.120 --> 00:30:31.640]   Criminals, like good criminals, there's
[00:30:31.640 --> 00:30:32.920]   a lot of stupid criminals.
[00:30:32.920 --> 00:30:35.440]   This stuff only works out in stupid criminals.
[00:30:35.440 --> 00:30:37.120]   Smart criminals are not going to carry--
[00:30:37.120 --> 00:30:39.240]   they're going to carry burner phones that don't identify them.
[00:30:39.240 --> 00:30:41.120]   They're not going to carry--
[00:30:41.120 --> 00:30:45.480]   not going to carry my iPhone into my robbery.
[00:30:45.480 --> 00:30:47.680]   They're going to learn how to obfuscate their data.
[00:30:47.680 --> 00:30:50.560]   If tools like that exist, they see they're going to use them.
[00:30:50.560 --> 00:30:54.920]   The rest of us are going to say, well, I have nothing to hide.
[00:30:54.920 --> 00:30:57.400]   And we're the ones who are vulnerable, but not the criminals.
[00:30:57.400 --> 00:30:59.560]   The criminals are going to get away with it.
[00:30:59.560 --> 00:31:04.560]   It's normal people who are going to get screwed.
[00:31:04.560 --> 00:31:07.160]   And frankly, if you get a government that
[00:31:07.160 --> 00:31:09.920]   is willing to use this against its populace,
[00:31:09.920 --> 00:31:11.800]   and we're seeing this more and more all over the world
[00:31:11.800 --> 00:31:14.600]   in Turkey and China and all over the world,
[00:31:14.600 --> 00:31:18.760]   I really fear for the future.
[00:31:18.760 --> 00:31:22.200]   I feel like we're already living in it.
[00:31:22.200 --> 00:31:25.680]   That's why I killed my Twitter, Facebook, Instagram account
[00:31:25.680 --> 00:31:26.240]   today.
[00:31:26.240 --> 00:31:28.080]   Your Twitter, Facebook, and Instagram account
[00:31:28.080 --> 00:31:29.080]   are not the culprit, too.
[00:31:29.080 --> 00:31:30.800]   The culprits are things like--
[00:31:30.800 --> 00:31:33.560]   I'm not-- you can't have my phone.
[00:31:33.560 --> 00:31:34.360]   OK.
[00:31:34.360 --> 00:31:36.160]   You can't have my phone.
[00:31:36.160 --> 00:31:38.560]   I mean, as much as I may mask my mother.
[00:31:38.560 --> 00:31:40.960]   It's kind of putting my head in the sand, isn't it?
[00:31:40.960 --> 00:31:42.360]   I'm an ostrich.
[00:31:42.360 --> 00:31:44.840]   You have a point.
[00:31:44.840 --> 00:31:46.640]   So OK.
[00:31:46.640 --> 00:31:47.840]   I'm taking my phone.
[00:31:47.840 --> 00:31:50.240]   I'm just not tweeting about it.
[00:31:50.240 --> 00:31:52.680]   But where does the intervention need to happen?
[00:31:52.680 --> 00:31:56.800]   Is the intervention that the apps are collecting all this data?
[00:31:56.800 --> 00:31:59.520]   And so the infrastructure is built in such a way
[00:31:59.520 --> 00:32:05.480]   that you can't run appropriate search without knowing
[00:32:05.480 --> 00:32:07.240]   where people are located.
[00:32:07.240 --> 00:32:10.200]   That's definitely true for maps.
[00:32:10.200 --> 00:32:15.040]   But I'm like, if we were to try to tackle this issue
[00:32:15.040 --> 00:32:18.120]   or to harness it in some way, is the idea
[00:32:18.120 --> 00:32:20.240]   to change the hardware and the phone
[00:32:20.240 --> 00:32:25.680]   or to make the operating system easier for users
[00:32:25.680 --> 00:32:30.240]   to customize so that they can turn off geolocation.
[00:32:30.240 --> 00:32:32.160]   So your carrier to--
[00:32:32.160 --> 00:32:33.840]   whatever you ping--
[00:32:33.840 --> 00:32:37.320]   so your phone to work has to ping a cell tower.
[00:32:37.320 --> 00:32:39.200]   And that cell tower has a physical location
[00:32:39.200 --> 00:32:42.200]   based on where you are because of physics, basically.
[00:32:42.200 --> 00:32:45.280]   It has to send data packets over the air,
[00:32:45.280 --> 00:32:47.840]   and they can only go so far.
[00:32:47.840 --> 00:32:51.760]   So your cell company-- so the company who's
[00:32:51.760 --> 00:32:52.400]   handling the phone--
[00:32:52.400 --> 00:32:53.800]   They know where you are every minute.
[00:32:53.800 --> 00:32:55.240]   They know exactly where you are.
[00:32:55.240 --> 00:32:58.200]   And then there's other layers.
[00:32:58.200 --> 00:33:00.800]   I mean, so the cell guys know where you are.
[00:33:00.800 --> 00:33:04.440]   There have been rules about how they can use that data.
[00:33:04.440 --> 00:33:07.480]   Those rules are somewhat in flux now.
[00:33:07.480 --> 00:33:10.440]   On the Google and those sites, those guys also--
[00:33:10.440 --> 00:33:12.240]   they were like, oh, that's beautiful data.
[00:33:12.240 --> 00:33:13.400]   I must have that data.
[00:33:13.400 --> 00:33:16.640]   So they built services that make it compelling for you
[00:33:16.640 --> 00:33:17.840]   to give it to them.
[00:33:17.840 --> 00:33:20.600]   And they built these services so you
[00:33:20.600 --> 00:33:23.600]   will use their platforms.
[00:33:23.600 --> 00:33:27.560]   It sounds nefarious when I say it, but it's not nefarious.
[00:33:27.560 --> 00:33:29.400]   They are offering a vital service.
[00:33:29.400 --> 00:33:31.800]   But in return, they're taking that data
[00:33:31.800 --> 00:33:34.880]   and trying to glean insights from it.
[00:33:34.880 --> 00:33:37.000]   And then there's these random other companies,
[00:33:37.000 --> 00:33:42.160]   like Euclid and others, that have come around looking
[00:33:42.160 --> 00:33:45.880]   for things like when your cell phone is trying
[00:33:45.880 --> 00:33:49.720]   to attach to a Wi-Fi network, it broadcasts information,
[00:33:49.720 --> 00:33:54.280]   and it goes through previous networks that it's joined.
[00:33:54.280 --> 00:33:56.640]   It kind of creates a fingerprint that says, hey,
[00:33:56.640 --> 00:33:58.400]   this is this person.
[00:33:58.400 --> 00:34:00.920]   They don't know that I'm Stacey Higamoth unnecessarily,
[00:34:00.920 --> 00:34:04.240]   but they know that they have seen this phone before.
[00:34:04.240 --> 00:34:06.600]   And so there's companies that do that without your consent,
[00:34:06.600 --> 00:34:07.520]   without anything.
[00:34:07.520 --> 00:34:14.640]   So there's many layers to this, an infrastructure perspective.
[00:34:14.640 --> 00:34:17.680]   I just want to point out, thank you, Pran in the chat room,
[00:34:17.680 --> 00:34:20.440]   to prove my point that if you're a crook,
[00:34:20.440 --> 00:34:24.120]   you know about this and you avoid using this guy.
[00:34:24.120 --> 00:34:26.240]   They called him the governor.
[00:34:26.240 --> 00:34:27.320]   He's 76.
[00:34:27.320 --> 00:34:30.720]   He's the oldest Hatton Garden burglar.
[00:34:30.720 --> 00:34:34.920]   He traveled to the burglary, the $14 million jewel heist,
[00:34:34.920 --> 00:34:40.120]   on a bus, but he didn't use his own senior oyster
[00:34:40.120 --> 00:34:41.720]   past to travel on the bus.
[00:34:41.720 --> 00:34:43.680]   He used somebody else's.
[00:34:43.680 --> 00:34:44.920]   (laughs)
[00:34:44.920 --> 00:34:49.920]   So just even this guy knows, don't use your own transit.
[00:34:49.920 --> 00:34:55.040]   But cars to get on the bus, if you're gonna rob--
[00:34:55.040 --> 00:34:55.880]   - If you're good.
[00:34:55.880 --> 00:34:57.120]   - Do a jewel heif.
[00:34:57.120 --> 00:34:57.960]   Right?
[00:34:57.960 --> 00:34:59.320]   - If you're gonna escape from the law and run,
[00:34:59.320 --> 00:35:02.560]   you should swap out your toll tags with somebody else's.
[00:35:02.560 --> 00:35:04.960]   - That's another one, those toll tags, man.
[00:35:04.960 --> 00:35:06.640]   They know exactly when you're crossing the bridge,
[00:35:06.640 --> 00:35:08.040]   when you're crossing the bus.
[00:35:08.040 --> 00:35:10.160]   So the guy who eventually got caught
[00:35:10.160 --> 00:35:14.200]   for these main robberies, it's kind of interesting.
[00:35:14.200 --> 00:35:19.200]   They caught him because they found footprints
[00:35:19.200 --> 00:35:22.960]   on the snow at the crime scenes.
[00:35:22.960 --> 00:35:27.920]   And he had lost his shoe, and they found the shoe,
[00:35:27.920 --> 00:35:31.480]   and they took DNA samples, and they matched his DNA samples.
[00:35:31.480 --> 00:35:36.240]   But they also had easy past toll records for his work truck,
[00:35:36.240 --> 00:35:38.360]   historical cell phone location.
[00:35:38.360 --> 00:35:42.320]   He ended up pleading guilty, but his attorney said,
[00:35:42.320 --> 00:35:44.040]   "Hey, if he had gone to trial,
[00:35:44.040 --> 00:35:46.560]   "if he hadn't pled guilty, and we'd gone to trial,
[00:35:46.560 --> 00:35:50.760]   "we would have brought up this overarching,
[00:35:50.760 --> 00:35:55.360]   "overbroad warrant, and we think it would have helped us
[00:35:55.360 --> 00:35:56.200]   "win the case."
[00:35:56.200 --> 00:35:59.560]   So there's a blowback to something like this.
[00:35:59.560 --> 00:36:02.120]   When an investigation violates the constitution,
[00:36:02.120 --> 00:36:04.960]   the evidence can't be used to trial,
[00:36:04.960 --> 00:36:08.240]   which often results in dismissals of the charges.
[00:36:08.240 --> 00:36:11.040]   So maybe the FBI ought to think twice
[00:36:11.040 --> 00:36:16.040]   about some of these overreaching warrants.
[00:36:16.040 --> 00:36:20.280]   But Google's still in the heat because they have
[00:36:20.280 --> 00:36:22.320]   that information, they've fought it on this one,
[00:36:22.320 --> 00:36:26.440]   but they know where you are all the time.
[00:36:26.440 --> 00:36:32.240]   Let's see, we're gonna do the change log a little bit.
[00:36:32.240 --> 00:36:33.520]   I'm bringing it back.
[00:36:33.520 --> 00:36:37.200]   - Oh, with the cool graphics?
[00:36:37.200 --> 00:36:39.040]   - Yeah, and everything, and the trumpets.
[00:36:39.040 --> 00:36:40.480]   (laughs)
[00:36:40.480 --> 00:36:43.640]   - I think we investigated the theme I wanted to use last week,
[00:36:43.640 --> 00:36:45.920]   and I think we investigated and determined
[00:36:45.920 --> 00:36:47.400]   that we couldn't use it, that we don't have--
[00:36:47.400 --> 00:36:49.600]   - Oh, the tiger something rag.
[00:36:49.600 --> 00:36:50.600]   - Tiger rag, yeah.
[00:36:50.600 --> 00:36:51.440]   - Yes.
[00:36:51.440 --> 00:36:52.280]   - Ooh. - Ooh.
[00:36:52.280 --> 00:36:53.120]   - Tiger rag.
[00:36:53.120 --> 00:36:57.640]   Where did Elon Musk Instagram account go?
[00:36:57.640 --> 00:36:59.480]   That's my next question.
[00:36:59.480 --> 00:37:00.320]   Why don't we return?
[00:37:00.320 --> 00:37:01.640]   - He had an Instagram account?
[00:37:01.640 --> 00:37:02.840]   - Yeah. - Of course he did.
[00:37:02.840 --> 00:37:05.160]   - And was it a Zellia Banks?
[00:37:05.160 --> 00:37:07.400]   It was Instagramming.
[00:37:07.400 --> 00:37:09.800]   He's holding me prisoner into the house?
[00:37:09.800 --> 00:37:11.160]   - Yes, yes.
[00:37:11.160 --> 00:37:15.440]   I don't even wanna understand what's happening there.
[00:37:15.440 --> 00:37:17.760]   I see the headlines and I'm like, "No."
[00:37:17.760 --> 00:37:19.080]   - Yep. - No.
[00:37:19.080 --> 00:37:21.520]   - I think the next thing for me is to stop watching
[00:37:21.520 --> 00:37:22.680]   24-hour news.
[00:37:22.680 --> 00:37:25.320]   That's just about as Twitter.
[00:37:25.320 --> 00:37:28.720]   - Pretty sure that my desire not to know anything about that
[00:37:28.720 --> 00:37:31.120]   is the biggest signifier that I am a nerd.
[00:37:31.120 --> 00:37:34.120]   Just like, "Oh, look, there's a new article about risk five.
[00:37:34.120 --> 00:37:34.960]   Let me read that instead."
[00:37:34.960 --> 00:37:35.960]   - Much more interesting.
[00:37:35.960 --> 00:37:39.160]   Let's look at the arm roadmap when we come back.
[00:37:39.160 --> 00:37:41.080]   - First, a word.
[00:37:41.080 --> 00:37:42.520]   Actually, I bet you have some things to say
[00:37:42.520 --> 00:37:44.120]   about the arm roadmap.
[00:37:44.120 --> 00:37:45.200]   - Oh, you know I do.
[00:37:45.200 --> 00:37:46.840]   - I thought you might.
[00:37:46.840 --> 00:37:50.320]   Stacey Higginbotham, StaceyOnIOT.com.
[00:37:50.320 --> 00:37:54.560]   Joan Donovan from datasociety.net.
[00:37:54.560 --> 00:37:56.400]   We will continue with this week in Google.
[00:37:56.400 --> 00:37:58.640]   Jeff's odd assignment.
[00:37:58.640 --> 00:38:00.320]   He's in Argentina right now.
[00:38:00.320 --> 00:38:02.520]   Lucky dog.
[00:38:02.520 --> 00:38:06.000]   Our show today brought to you by DigitalOcean.
[00:38:06.000 --> 00:38:07.640]   I love DigitalOcean.
[00:38:07.640 --> 00:38:12.640]   If you're a coder, if you want to deploy an application
[00:38:12.640 --> 00:38:17.520]   on the net, there is no easier way than DigitalOcean.
[00:38:17.520 --> 00:38:18.520]   I use it all the time.
[00:38:18.520 --> 00:38:19.920]   It's a fun way for me.
[00:38:19.920 --> 00:38:22.000]   Let me log into my DigitalOcean account.
[00:38:22.000 --> 00:38:25.680]   It's a fun way for me to try new web technologies
[00:38:25.680 --> 00:38:27.360]   very affordably.
[00:38:27.360 --> 00:38:31.920]   It's the easiest cloud platform to deploy, manage
[00:38:31.920 --> 00:38:33.600]   and scale applications.
[00:38:33.600 --> 00:38:35.240]   They call them droplets.
[00:38:35.240 --> 00:38:37.880]   Like droplets in the ocean, right?
[00:38:37.880 --> 00:38:40.800]   I'll show you how it's easy it is to create a droplet.
[00:38:40.800 --> 00:38:42.360]   Droplets are virtual machines.
[00:38:42.360 --> 00:38:44.880]   They're scalable compute platforms.
[00:38:44.880 --> 00:38:48.560]   When you create a droplet, you choose either an operating
[00:38:48.560 --> 00:38:50.880]   system, and you see we have Ubuntu, free BSD,
[00:38:50.880 --> 00:38:55.880]   Fedora, Debian, CentOS, or choose a container,
[00:38:55.880 --> 00:38:59.240]   CoreOS, Fedora, Atomic, RancherOS,
[00:38:59.240 --> 00:39:02.200]   or a one click app.
[00:39:02.200 --> 00:39:06.360]   This course, Django, Docker, GitLab, Ghost, Doku, LAMP.
[00:39:06.360 --> 00:39:09.120]   Let's say I wanted to set up a ghost blog.
[00:39:09.120 --> 00:39:10.560]   Press that button.
[00:39:10.560 --> 00:39:11.640]   How big?
[00:39:11.640 --> 00:39:13.560]   Well, you know, I'm just beginning here.
[00:39:13.560 --> 00:39:14.760]   I don't want to spend too much money,
[00:39:14.760 --> 00:39:19.560]   so let's do the .007 cents an hour tier.
[00:39:19.560 --> 00:39:22.960]   Gives me a CPU, a gigabyte of memory, 25 SSD,
[00:39:22.960 --> 00:39:25.440]   a terabyte of transfer, a lot of transfer.
[00:39:25.440 --> 00:39:27.200]   You can enable backups if you want,
[00:39:27.200 --> 00:39:28.880]   add block storage, you even can choose
[00:39:28.880 --> 00:39:30.440]   the data center region.
[00:39:30.440 --> 00:39:32.120]   I always choose San Francisco too.
[00:39:32.120 --> 00:39:34.320]   I don't know why, but you can use New York,
[00:39:34.320 --> 00:39:36.280]   Amsterdam, San Francisco, Singapore, London,
[00:39:36.280 --> 00:39:38.360]   Frankfurt, Toronto, Bangalore.
[00:39:38.360 --> 00:39:39.560]   Lots of other options.
[00:39:39.560 --> 00:39:44.560]   I always use SSH because I have uploaded my SSH keys
[00:39:44.560 --> 00:39:48.600]   to DigitalOcean, makes it easy for me to log in.
[00:39:48.600 --> 00:39:50.640]   So now I've got a ghost server.
[00:39:50.640 --> 00:39:53.440]   This is the host name, although I can change it
[00:39:53.440 --> 00:39:55.680]   to Leo's Ghost.
[00:39:55.680 --> 00:39:57.320]   Great Caesar's Ghost.
[00:39:57.320 --> 00:39:58.160]   How about that?
[00:39:58.160 --> 00:40:00.760]   Great Leo's Ghost.
[00:40:01.760 --> 00:40:04.240]   I can add tags, I can say which project,
[00:40:04.240 --> 00:40:05.080]   and I can create.
[00:40:05.080 --> 00:40:08.240]   Now watch, within a minute, that server will be
[00:40:08.240 --> 00:40:11.040]   provisioned and ready to use.
[00:40:11.040 --> 00:40:12.920]   You could choose from standard to CPU's,
[00:40:12.920 --> 00:40:16.040]   optimized droplets, customized as I just did.
[00:40:16.040 --> 00:40:18.040]   Easy to use control panel and API,
[00:40:18.040 --> 00:40:19.680]   let's developers spend more time coding,
[00:40:19.680 --> 00:40:21.880]   less time managing their infrastructure.
[00:40:21.880 --> 00:40:23.960]   You can access the compute resources you need
[00:40:23.960 --> 00:40:26.760]   very affordably, save up to 55%
[00:40:26.760 --> 00:40:29.040]   compared to other cloud providers.
[00:40:29.040 --> 00:40:31.720]   I eat a great way for you to try,
[00:40:31.720 --> 00:40:34.320]   but you can still scale it up and make it
[00:40:34.320 --> 00:40:36.240]   a full production server if you need to.
[00:40:36.240 --> 00:40:38.680]   Keep it on this because it's almost done
[00:40:38.680 --> 00:40:40.880]   creating my droplet, that's how fast it is.
[00:40:40.880 --> 00:40:42.240]   You always know what you'll pay per month
[00:40:42.240 --> 00:40:45.600]   with flat pricing structure across all data center regions.
[00:40:45.600 --> 00:40:49.200]   Great Leo's Ghost is up and running.
[00:40:49.200 --> 00:40:50.280]   How about that?
[00:40:50.280 --> 00:40:52.120]   That's how fast and easy it is.
[00:40:52.120 --> 00:40:57.120]   $5 a month, $99.99% uptime SLA, $4.9,
[00:40:58.560 --> 00:41:00.400]   cloud firewalls, monitoring and learning,
[00:41:00.400 --> 00:41:02.640]   full DNS management, global data centers,
[00:41:02.640 --> 00:41:07.360]   enterprise grade SSDs, they're fast, easy to use APIs.
[00:41:07.360 --> 00:41:11.280]   Look, it's no wonder more than 150,000 businesses,
[00:41:11.280 --> 00:41:13.440]   including many of the world's fastest growing
[00:41:13.440 --> 00:41:15.320]   startups rely on digital ocean,
[00:41:15.320 --> 00:41:16.840]   to remove infrastructure, friction
[00:41:16.840 --> 00:41:18.880]   and deliver industry leading price performance.
[00:41:18.880 --> 00:41:20.680]   You know, I would love this Kevin Tofill,
[00:41:20.680 --> 00:41:23.120]   who set up a Raspberry Pi circuit program in Python.
[00:41:23.120 --> 00:41:26.960]   Just do this, set up an Ubuntu server, a digital ocean.
[00:41:27.960 --> 00:41:29.200]   And it's affordable.
[00:41:29.200 --> 00:41:34.200]   Sign up today, you'll get a free, get this, $100 credit.
[00:41:34.200 --> 00:41:41.280]   Go to dio.co/twig, dio.co/twig for a free $100 credit.
[00:41:41.280 --> 00:41:46.680]   Digital ocean makes it so easy.
[00:41:46.680 --> 00:41:48.680]   I just love these guys.
[00:41:48.680 --> 00:41:52.760]   Digital ocean, dio.co/twig,
[00:41:52.760 --> 00:41:55.680]   and you'll get a free $100 credit.
[00:41:55.680 --> 00:41:57.320]   One of the reasons I love digital ocean
[00:41:57.320 --> 00:41:59.920]   is no matter what you want to do online,
[00:41:59.920 --> 00:42:01.800]   there is a tutorial.
[00:42:01.800 --> 00:42:06.400]   You just, if you Google, I want to set up, you know, anything.
[00:42:06.400 --> 00:42:11.240]   I want to set up, you know, WordPress instance,
[00:42:11.240 --> 00:42:12.440]   a digital ocean, just Google it,
[00:42:12.440 --> 00:42:15.120]   and there'll be a whole tutorial walking right through it.
[00:42:15.120 --> 00:42:16.160]   It's awesome.
[00:42:16.160 --> 00:42:20.640]   Digital ocean, dio.co/twig, and we thank them.
[00:42:20.640 --> 00:42:23.640]   But their support, they've been with us for some time
[00:42:23.640 --> 00:42:25.480]   of this week in Google.
[00:42:27.080 --> 00:42:28.840]   I haven't talked about Facebook in so long.
[00:42:28.840 --> 00:42:30.960]   In fact, I wonder if getting rid of my Facebook
[00:42:30.960 --> 00:42:34.280]   maybe will impair my ability to cover Facebook.
[00:42:34.280 --> 00:42:37.600]   Fortunately, you guys are going to stay on Facebook, right?
[00:42:37.600 --> 00:42:40.440]   So I don't have to worry about that.
[00:42:40.440 --> 00:42:42.040]   - Me? I'm not on Facebook, really.
[00:42:42.040 --> 00:42:43.360]   (laughs)
[00:42:43.360 --> 00:42:44.720]   - Shh, sorry.
[00:42:44.720 --> 00:42:46.720]   - It's not on Facebook, right?
[00:42:46.720 --> 00:42:49.360]   Well, we know one person who will.
[00:42:49.360 --> 00:42:52.200]   Jeff Jarvis, I bet Joan, I bet you're not either, are you?
[00:42:52.200 --> 00:42:54.480]   - What, on Facebook?
[00:42:54.480 --> 00:42:55.880]   - Yeah. - I'm on Facebook.
[00:42:55.880 --> 00:43:00.880]   But I have, well, I mean, it's hard to figure it out.
[00:43:00.880 --> 00:43:04.280]   I have, no, I just have, I have real and fake accounts.
[00:43:04.280 --> 00:43:06.920]   - Ah, I thought I'd say no more, say no more.
[00:43:06.920 --> 00:43:08.400]   - I'll say no more, but yeah.
[00:43:08.400 --> 00:43:10.600]   - I think Cisco is an art to a blind man.
[00:43:10.600 --> 00:43:13.560]   - You know, there's ways in which you can enjoy
[00:43:13.560 --> 00:43:16.360]   these platforms without being publicly available
[00:43:16.360 --> 00:43:18.680]   to everyone for everything.
[00:43:18.680 --> 00:43:24.200]   So I am an avid fan of having multiple accounts
[00:43:24.200 --> 00:43:25.320]   for multiple things.
[00:43:25.320 --> 00:43:29.200]   And yeah, and so, yeah, and the other thing is,
[00:43:29.200 --> 00:43:32.320]   is that is just kind of the habit I learned with
[00:43:32.320 --> 00:43:34.080]   when I first started AOL, right?
[00:43:34.080 --> 00:43:36.840]   We all had, you had access to five screen names,
[00:43:36.840 --> 00:43:39.120]   so I had five screen names.
[00:43:39.120 --> 00:43:40.200]   - Right. - And so, yeah.
[00:43:40.200 --> 00:43:42.160]   - You come from that era. - Yeah.
[00:43:42.160 --> 00:43:45.680]   - Yeah, I even segmented some of my Netflix stuff
[00:43:45.680 --> 00:43:48.560]   so that, you know, if I wanna watch, you know,
[00:43:48.560 --> 00:43:51.960]   like documentaries and things that I wanna be recommended
[00:43:51.960 --> 00:43:56.320]   that I have a persona on there that has different
[00:43:56.320 --> 00:43:58.920]   of content. - You are paranoid.
[00:43:58.920 --> 00:44:00.360]   - I'm not so much paranoid.
[00:44:00.360 --> 00:44:04.120]   It's just that the recommendation systems can be so terrible
[00:44:04.120 --> 00:44:07.000]   if you have, this is something that a friend of mine
[00:44:07.000 --> 00:44:09.240]   complained about, I wanted to borrow his Hulu
[00:44:09.240 --> 00:44:10.240]   to see if I liked it.
[00:44:10.240 --> 00:44:13.720]   And he said, no, you're gonna mess up all my recommendations.
[00:44:13.720 --> 00:44:14.560]   (laughing)
[00:44:14.560 --> 00:44:15.560]   - Kind of true, you know. - That's so mad
[00:44:15.560 --> 00:44:17.160]   when Hulu asked me for it, like,
[00:44:17.160 --> 00:44:19.400]   Hulu used to just be like, you would hang out on it
[00:44:19.400 --> 00:44:20.240]   and then suddenly it was like--
[00:44:20.240 --> 00:44:21.440]   - Oh, now they ask you for a lot, yeah.
[00:44:21.440 --> 00:44:24.240]   - It's like, can we find out who you are, what do you like?
[00:44:24.240 --> 00:44:25.600]   And I'm like, I'm not taking a quiz
[00:44:25.600 --> 00:44:27.120]   to watch a freaking TV show.
[00:44:27.120 --> 00:44:28.560]   - So what's wrong with the app? - TV is supposed to be,
[00:44:28.560 --> 00:44:29.680]   like, brain dead. - Yeah.
[00:44:29.680 --> 00:44:32.160]   - You figure it out. - Yeah.
[00:44:32.160 --> 00:44:34.080]   - I'm giving you signals, you figure it out.
[00:44:34.080 --> 00:44:36.600]   - Yeah. (laughing)
[00:44:36.600 --> 00:44:38.640]   - Well, he didn't want me to mess up, you know,
[00:44:38.640 --> 00:44:42.080]   he's really into, you know, early Saturday Night Live
[00:44:42.080 --> 00:44:44.240]   and different sketch comedy stuff.
[00:44:44.240 --> 00:44:46.240]   And he was like, no, you're gonna get in there.
[00:44:46.240 --> 00:44:48.280]   You're gonna watch some, like, you know, cooking show
[00:44:48.280 --> 00:44:49.520]   or you're gonna watch this other thing.
[00:44:49.520 --> 00:44:51.720]   And it's gonna, it'll never forget that.
[00:44:51.720 --> 00:44:53.560]   - Oh, it's true.
[00:44:53.560 --> 00:44:54.400]   - Yeah.
[00:44:54.400 --> 00:44:56.080]   - And so-- - You should have a guest mode.
[00:44:56.080 --> 00:44:57.720]   Oh, no, they don't want you to do that
[00:44:57.720 --> 00:45:00.480]   because they don't want you to share the password, yeah.
[00:45:00.480 --> 00:45:03.280]   - I mean, I'm in favor of a delete button, right?
[00:45:03.280 --> 00:45:06.240]   There should be a way to hit reset on some of these
[00:45:06.240 --> 00:45:10.600]   where, you know, you're able to just start again, you know?
[00:45:10.600 --> 00:45:12.880]   It used to be that you could erase your history
[00:45:12.880 --> 00:45:16.240]   and your cookies and get, you know, a fresh install
[00:45:16.240 --> 00:45:18.520]   or something and you could start over.
[00:45:18.520 --> 00:45:22.600]   But a lot of these apps, they just retain, retain, retain.
[00:45:22.600 --> 00:45:25.040]   - And it's unfortunate because there are like,
[00:45:25.040 --> 00:45:30.560]   man, some of my things have like me down this weird rabbit hole
[00:45:30.560 --> 00:45:34.960]   and I'm like, okay, granted, I did watch that once
[00:45:34.960 --> 00:45:37.840]   when I was recovering from like wisdom, teaser, three.
[00:45:37.840 --> 00:45:39.200]   You know what this is not-- - You know what this is not.
[00:45:39.200 --> 00:45:40.360]   - This is not who I am.
[00:45:40.360 --> 00:45:44.280]   Please stop giving me this.
[00:45:45.560 --> 00:45:50.200]   - Yeah, you blame every James Franco recommendation I get
[00:45:50.200 --> 00:45:53.440]   on a sick day out of the office.
[00:45:53.440 --> 00:45:56.200]   - I watched "Twilight" y'all and let me tell you.
[00:45:56.200 --> 00:45:58.040]   - Okay, yeah, you did.
[00:45:58.040 --> 00:45:59.000]   - We are done.
[00:45:59.000 --> 00:46:02.360]   I mean, I was like, I gotta see what the sparkle vamps are about
[00:46:02.360 --> 00:46:04.920]   and I saw it and I was like, oh yeah,
[00:46:04.920 --> 00:46:07.520]   I'd watch the heck out of these if I were drunk.
[00:46:07.520 --> 00:46:11.160]   But I really don't want that forever influencing
[00:46:11.160 --> 00:46:13.480]   my choices, you know?
[00:46:13.480 --> 00:46:17.760]   - Well, even though I have two computers,
[00:46:17.760 --> 00:46:20.600]   one that I use at home, one that I use for work stuff,
[00:46:20.600 --> 00:46:23.480]   because of the nature of what I research
[00:46:23.480 --> 00:46:26.240]   and because I can't just sign out of all of my accounts
[00:46:26.240 --> 00:46:31.400]   on my work computer, my Amazon and my YouTube
[00:46:31.400 --> 00:46:35.160]   are perpetually infected with nonsense.
[00:46:35.160 --> 00:46:39.360]   And so I cannot hand my phone to a young person
[00:46:39.360 --> 00:46:41.880]   who's like, let me watch videos on YouTube.
[00:46:41.880 --> 00:46:43.640]   I'm like, nope, because I don't know
[00:46:43.640 --> 00:46:46.320]   what you're gonna get recommended.
[00:46:46.320 --> 00:46:49.600]   So, you know, I have learned my lesson,
[00:46:49.600 --> 00:46:53.880]   but there's really no way to stop,
[00:46:53.880 --> 00:46:56.320]   at least as far as I can tell, those two apps
[00:46:56.320 --> 00:47:00.160]   from like really, you know,
[00:47:00.160 --> 00:47:01.920]   bunching all of your interests
[00:47:01.920 --> 00:47:05.200]   and keeping with the recommendations.
[00:47:05.200 --> 00:47:08.400]   - Like, you know, come to stake a bit.
[00:47:08.400 --> 00:47:09.600]   This is really true.
[00:47:09.600 --> 00:47:14.280]   I am actually reluctant to watch stuff on Netflix.
[00:47:14.280 --> 00:47:18.480]   You know, like, I actually, it's like my mom is watching.
[00:47:18.480 --> 00:47:20.520]   - Dachty stuff? - Yeah.
[00:47:20.520 --> 00:47:22.640]   It's like, oh, I better not watch that because--
[00:47:22.640 --> 00:47:23.960]   - I need an incognito mode.
[00:47:23.960 --> 00:47:25.600]   - It's gonna then pop up.
[00:47:25.600 --> 00:47:29.360]   And the worst thing is if you use your Apple TV,
[00:47:29.360 --> 00:47:33.360]   then the Apple TV app blasts everything you've just watched
[00:47:33.360 --> 00:47:35.120]   across in a banner, across the screen,
[00:47:35.120 --> 00:47:37.240]   as soon as you turn it on.
[00:47:37.240 --> 00:47:39.000]   - Well, yeah, this is how I get in trouble.
[00:47:39.000 --> 00:47:42.240]   So like, you know how couples have this battle about like--
[00:47:42.240 --> 00:47:43.760]   - You shouldn't be watching without--
[00:47:43.760 --> 00:47:45.800]   - I watched, yeah, something without my--
[00:47:45.800 --> 00:47:46.800]   - Don't you do it.
[00:47:46.800 --> 00:47:47.640]   Don't do it.
[00:47:47.640 --> 00:47:48.480]   - Don't do it.
[00:47:48.480 --> 00:47:51.000]   - And yeah, he found out.
[00:47:51.000 --> 00:47:52.960]   And it was grim, you guys.
[00:47:52.960 --> 00:47:55.640]   I mean, it really tested our 16 year marriage.
[00:47:55.640 --> 00:47:59.200]   - How dare you watch Better Call Saul without me?
[00:47:59.200 --> 00:48:00.720]   - Yeah, that's not easy.
[00:48:00.720 --> 00:48:01.560]   - It's true.
[00:48:01.560 --> 00:48:03.800]   And like now with like, chromecasting,
[00:48:03.800 --> 00:48:07.200]   so my daughter is constantly like checking on my phone,
[00:48:07.200 --> 00:48:08.400]   you know, just to see stuff.
[00:48:08.400 --> 00:48:09.640]   Like what you were saying, Joan.
[00:48:09.640 --> 00:48:14.240]   And 90% of my videos are corgis and, you know, weird gadgets.
[00:48:14.240 --> 00:48:15.080]   So we're fine.
[00:48:15.080 --> 00:48:17.480]   But I can totally see that someone in your position
[00:48:17.480 --> 00:48:20.240]   would be like, yeah, I don't know what you're seeing.
[00:48:20.240 --> 00:48:21.400]   - Joan has to watch info.
[00:48:21.400 --> 00:48:22.240]   - I mean, I'm sitting here reading videos.
[00:48:22.240 --> 00:48:23.560]   - Right, yeah.
[00:48:23.560 --> 00:48:24.720]   - I do, yeah.
[00:48:24.720 --> 00:48:27.800]   And that's, I mean, that is just not even the worst of it.
[00:48:27.800 --> 00:48:30.760]   But what's been really hard, I think,
[00:48:30.760 --> 00:48:33.680]   is something that is sort of tests the boundaries
[00:48:33.680 --> 00:48:37.000]   with my wife is I will order a bunch of stuff
[00:48:37.000 --> 00:48:41.000]   related to history and, you know, books online
[00:48:41.000 --> 00:48:43.280]   about neo-Nazis and white supremacists.
[00:48:43.280 --> 00:48:46.080]   And then it starts to, you know, our Amazon
[00:48:46.080 --> 00:48:48.640]   starts to have this particular flavor to it
[00:48:48.640 --> 00:48:49.640]   in the recommendations.
[00:48:49.640 --> 00:48:50.480]   - Yeah.
[00:48:50.480 --> 00:48:52.640]   - It's just like, do not do this on our home account.
[00:48:52.640 --> 00:48:54.040]   - Yeah, I don't share,
[00:48:54.040 --> 00:48:57.440]   Lisa and I will not share Amazon accounts for that reason.
[00:48:57.440 --> 00:48:59.080]   - But the thing is the prime.
[00:48:59.080 --> 00:49:00.680]   You don't wanna pay twice for shipping.
[00:49:00.680 --> 00:49:02.440]   - Oh, that's a good point.
[00:49:02.440 --> 00:49:03.520]   Oh, that's a good point.
[00:49:03.520 --> 00:49:06.080]   - Well, we're paying-- - We pay, we're gonna ship it.
[00:49:06.080 --> 00:49:07.800]   - No, 'cause you could have your own account
[00:49:07.800 --> 00:49:09.440]   and be in a family with your wife.
[00:49:09.440 --> 00:49:10.960]   So you could just-- - Oh.
[00:49:10.960 --> 00:49:12.960]   - 'Cause my husband and I don't share accounts,
[00:49:12.960 --> 00:49:15.160]   but he's part of our family, so he gets it.
[00:49:15.160 --> 00:49:17.000]   He has his own account, but he--
[00:49:17.000 --> 00:49:19.240]   - You both get the prime? - He's also part of, yeah.
[00:49:19.240 --> 00:49:20.360]   - Oh, that's cool.
[00:49:20.360 --> 00:49:22.680]   - Okay, I do know that. - It's not an idea.
[00:49:22.680 --> 00:49:23.840]   It's a feature. - I do.
[00:49:23.840 --> 00:49:26.960]   - So, and we actually can share our Kindle titles too.
[00:49:26.960 --> 00:49:29.040]   So like, when he buys something,
[00:49:29.040 --> 00:49:32.000]   this is actually something I wish didn't always happen.
[00:49:32.000 --> 00:49:34.920]   But when he buys something on Amazon,
[00:49:34.920 --> 00:49:36.360]   if it's something that's shareable,
[00:49:36.360 --> 00:49:38.240]   his Kindle titles show up on my Kindle,
[00:49:38.240 --> 00:49:39.760]   and I can download them.
[00:49:39.760 --> 00:49:42.920]   - Lisa has so polluted my audible stream.
[00:49:42.920 --> 00:49:44.840]   (laughing)
[00:49:44.840 --> 00:49:46.480]   I just wanna show you.
[00:49:46.480 --> 00:49:50.400]   So, stranger in a strange land, that's me,
[00:49:50.400 --> 00:49:52.440]   White Fang, that was a freebie,
[00:49:52.440 --> 00:49:54.560]   Jack London Valley of Genius,
[00:49:54.560 --> 00:49:59.560]   Bourdain, Stephen King, good stuff, right?
[00:49:59.560 --> 00:50:02.520]   Then James Comey, that's hers,
[00:50:02.520 --> 00:50:04.520]   never split the difference negotiating
[00:50:04.520 --> 00:50:06.240]   as if your life depended on it.
[00:50:06.240 --> 00:50:08.280]   Carl Hyacen, the plant paradox,
[00:50:08.280 --> 00:50:10.440]   hidden dangers and healthy foods.
[00:50:10.440 --> 00:50:12.160]   And then back-- - Carl Hyacen is awesome.
[00:50:12.160 --> 00:50:13.120]   - I love Carl Hyacen.
[00:50:13.120 --> 00:50:15.520]   Actually, I have a whole bunch of Carl Hyacen
[00:50:15.520 --> 00:50:17.280]   earlier onto my feed.
[00:50:17.280 --> 00:50:20.160]   But it's funny because the mix,
[00:50:20.160 --> 00:50:23.400]   it's completely different people.
[00:50:23.400 --> 00:50:26.320]   She's, it's all mixed into my feed.
[00:50:26.320 --> 00:50:28.640]   - I saw the Autonomous, the book I recommended
[00:50:28.640 --> 00:50:31.000]   a couple of months ago. - And we interviewed the author,
[00:50:31.000 --> 00:50:33.320]   thanks to you, Analene Newitz.
[00:50:33.320 --> 00:50:34.440]   That's a great book.
[00:50:34.440 --> 00:50:36.200]   - It is. - Oh, but you see,
[00:50:36.200 --> 00:50:37.960]   now it's giving it away, it says five hours
[00:50:37.960 --> 00:50:41.440]   and 22 minutes left. (laughs)
[00:50:41.440 --> 00:50:44.360]   A lot of my books have stuff left.
[00:50:44.360 --> 00:50:46.720]   I, you know, it's for a long time.
[00:50:46.720 --> 00:50:49.240]   I've always had this problem with physical books,
[00:50:49.240 --> 00:50:51.520]   like my bedside table, it's dangerous.
[00:50:51.520 --> 00:50:53.600]   It could fall on you and hurt you.
[00:50:53.600 --> 00:50:55.560]   'Cause I always have a lot of reading materials.
[00:50:55.560 --> 00:50:56.920]   But for a long time with Audible, it was like,
[00:50:56.920 --> 00:51:00.520]   no one book, finish the book, and you do the next book.
[00:51:00.520 --> 00:51:02.640]   But it's too easy to just to dip in.
[00:51:03.560 --> 00:51:06.920]   And so I have 43 hours left on Shogun.
[00:51:06.920 --> 00:51:08.840]   (laughs)
[00:51:08.840 --> 00:51:10.240]   - I can't dip.
[00:51:10.240 --> 00:51:11.800]   I am full of version. - You know the dipper?
[00:51:11.800 --> 00:51:13.240]   - And then out, I can't do it.
[00:51:13.240 --> 00:51:14.880]   - Not a dipper.
[00:51:14.880 --> 00:51:16.960]   - It's, you know, I even read the New Yorker
[00:51:16.960 --> 00:51:19.040]   cover to cover. - What?
[00:51:19.040 --> 00:51:21.680]   - I know, it is, it, I'm like a completus too,
[00:51:21.680 --> 00:51:22.680]   it's terrible. - That's bad.
[00:51:22.680 --> 00:51:24.560]   - I've got a stack right now.
[00:51:24.560 --> 00:51:26.720]   - I just, unless it's a baseball article,
[00:51:26.720 --> 00:51:29.720]   or God help me another fly fishing article.
[00:51:29.720 --> 00:51:31.040]   - Yeah. - But other than that,
[00:51:31.040 --> 00:51:33.040]   I'm all in.
[00:51:33.040 --> 00:51:34.520]   - That's so funny.
[00:51:34.520 --> 00:51:36.400]   - Yeah, that was actually, that hurt my New Yorker's
[00:51:36.400 --> 00:51:38.800]   subscription, 'cause I couldn't be completed.
[00:51:38.800 --> 00:51:41.880]   And so then I said, well, I'm just not gonna get it.
[00:51:41.880 --> 00:51:43.280]   'Cause I can't complete it.
[00:51:43.280 --> 00:51:46.120]   Facebook is being asked, well, let me ask you this,
[00:51:46.120 --> 00:51:47.920]   I really wanna know your opinion.
[00:51:47.920 --> 00:51:52.720]   Facebook's artificial intelligence lab, right there.
[00:51:52.720 --> 00:51:56.360]   Those four words bother me, is working with New York
[00:51:56.360 --> 00:51:58.720]   University's medical school to make MRI exams
[00:51:58.720 --> 00:52:03.720]   10 times faster, NYU is giving an anonymous, they say,
[00:52:03.720 --> 00:52:08.200]   data set of 10,000 MRI's, with as many as three million
[00:52:08.200 --> 00:52:11.380]   images of knees, brains, and livers, to Facebook.
[00:52:11.380 --> 00:52:17.400]   So, Facebook shouldn't have an artificial intelligence lab.
[00:52:17.400 --> 00:52:18.680]   First of all. - But no, it should.
[00:52:18.680 --> 00:52:19.960]   - Give it to Google. - Okay, why?
[00:52:19.960 --> 00:52:23.280]   - Give it to somebody who cares about the world.
[00:52:23.280 --> 00:52:26.960]   Don't give it to the worst spy agency in the history
[00:52:26.960 --> 00:52:29.320]   of mankind. It's like saying, we're gonna give it
[00:52:29.320 --> 00:52:31.360]   to the KGB, you don't mind, do you?
[00:52:31.360 --> 00:52:34.960]   - Okay, well, maybe they could have given it,
[00:52:34.960 --> 00:52:36.960]   maybe they could have made it an open data set.
[00:52:36.960 --> 00:52:38.920]   That would have probably been better.
[00:52:38.920 --> 00:52:42.960]   But, yeah, I mean, it's just anonymous in its picture.
[00:52:42.960 --> 00:52:46.960]   - I don't think Facebook should be in the AI business.
[00:52:46.960 --> 00:52:50.000]   And they say, we wanna make MRI's faster.
[00:52:50.000 --> 00:52:52.760]   What does Facebook care if MRIs are faster?
[00:52:52.760 --> 00:52:53.840]   They don't care about that.
[00:52:53.840 --> 00:52:56.920]   That's not what they're, that's not their goal.
[00:52:56.920 --> 00:53:00.080]   They're trying to test different models.
[00:53:00.080 --> 00:53:03.520]   But testing models for the benefit of humanity
[00:53:03.520 --> 00:53:07.240]   or against anonymous data sets makes other things
[00:53:07.240 --> 00:53:09.560]   more powerful. I don't think that's...
[00:53:09.560 --> 00:53:10.720]   - Yeah, this is what... - I think it'd be,
[00:53:10.720 --> 00:53:12.160]   this is what... - They should have made it open.
[00:53:12.160 --> 00:53:15.600]   - This is what the AI research group at Facebook said.
[00:53:15.600 --> 00:53:18.360]   They started to talk about this with NYU last year
[00:53:18.360 --> 00:53:22.400]   because Facebook's AI team wanted to work on something
[00:53:22.400 --> 00:53:27.400]   with real world benefits even as it performs basic research.
[00:53:27.400 --> 00:53:30.600]   - Well, think about it.
[00:53:30.600 --> 00:53:33.760]   If you're trying, AI researchers, man,
[00:53:33.760 --> 00:53:35.520]   you've got a couple ways to get them.
[00:53:35.520 --> 00:53:39.040]   You can get them notoriety by giving them great data sets
[00:53:39.040 --> 00:53:41.640]   and let them publish awesome papers that people love.
[00:53:41.640 --> 00:53:43.200]   You can pay them a lot of money
[00:53:43.200 --> 00:53:45.080]   and give them fabulous benefits.
[00:53:45.080 --> 00:53:48.360]   So Facebook, everybody can pay them lots of money
[00:53:48.360 --> 00:53:49.640]   and give them fabulous benefits.
[00:53:49.640 --> 00:53:51.680]   So Facebook just wants them to be able to also
[00:53:51.680 --> 00:53:53.560]   like cure liver cancer.
[00:53:53.560 --> 00:53:55.160]   That's a nice way to recruit people.
[00:53:55.160 --> 00:53:56.200]   - It's not, yes.
[00:53:56.200 --> 00:53:58.240]   It's not really though of Facebook's business model
[00:53:58.240 --> 00:53:59.760]   cure liver cancer.
[00:53:59.760 --> 00:54:01.960]   - No, it's not, but it's their business model
[00:54:01.960 --> 00:54:04.480]   to have happy AI researchers that wanna work for them.
[00:54:04.480 --> 00:54:05.360]   - That's true.
[00:54:05.360 --> 00:54:10.000]   But I don't know. - There's also a big division,
[00:54:10.000 --> 00:54:12.040]   I don't know how big it is.
[00:54:12.040 --> 00:54:12.880]   I'll take that back.
[00:54:12.880 --> 00:54:15.160]   As a sociologist, I shouldn't be making estimates
[00:54:15.160 --> 00:54:17.760]   about size, but within Microsoft,
[00:54:17.760 --> 00:54:21.600]   there's a research division related to AI
[00:54:21.600 --> 00:54:24.200]   and genetics and machine learning.
[00:54:24.200 --> 00:54:28.480]   And so I can see that part of this is probably related
[00:54:28.480 --> 00:54:32.920]   to a tax write off that has to do with developing
[00:54:32.920 --> 00:54:37.480]   this kind of good for the world data.
[00:54:37.480 --> 00:54:41.240]   Like when you make that much money as an organization,
[00:54:41.240 --> 00:54:46.240]   you can really spend on any kind of lucrative,
[00:54:46.240 --> 00:54:51.560]   lab based research, whether or not you intend
[00:54:51.560 --> 00:54:54.680]   on implementing it or using it.
[00:54:54.680 --> 00:54:58.160]   That's what worries me is that you give privileged access
[00:54:58.160 --> 00:55:03.480]   to this group that doesn't have any plans to do anything
[00:55:03.480 --> 00:55:05.000]   good for society.
[00:55:05.000 --> 00:55:05.840]   - Oh, yeah.
[00:55:05.840 --> 00:55:07.200]   I mean, and the other thing is,
[00:55:07.200 --> 00:55:10.520]   is there's lots of computing power and lots of great
[00:55:10.520 --> 00:55:15.880]   researchers in public universities that could really do
[00:55:15.880 --> 00:55:17.640]   some wonderful stuff with this.
[00:55:17.640 --> 00:55:22.200]   And at NYU itself, so I'm wondering where
[00:55:22.200 --> 00:55:23.920]   and how the partnership came together
[00:55:23.920 --> 00:55:28.920]   and why Facebook becomes the best group for the job
[00:55:28.920 --> 00:55:36.120]   when we have so many other medical facilities
[00:55:36.120 --> 00:55:42.000]   that could really use that kind of data as well as,
[00:55:42.000 --> 00:55:47.040]   they can produce different and more interesting outcomes.
[00:55:47.680 --> 00:55:52.680]   By having it done in a way that it can be implemented.
[00:55:52.680 --> 00:55:56.680]   I wonder what the implementation strategy is
[00:55:56.680 --> 00:55:59.840]   or how the machines will eventually change.
[00:55:59.840 --> 00:56:02.560]   - I guess it's silly of me to worry about Facebook
[00:56:02.560 --> 00:56:04.760]   when Google's doing the same thing
[00:56:04.760 --> 00:56:07.400]   and they're just as interested in personal information
[00:56:07.400 --> 00:56:08.240]   and stuff.
[00:56:08.240 --> 00:56:11.880]   - Well, and what if, like just bear with me here,
[00:56:11.880 --> 00:56:16.000]   but old people, we're getting up there in the world, right?
[00:56:16.000 --> 00:56:19.440]   There's a lot of us and there's apparently
[00:56:19.440 --> 00:56:21.040]   not a lot of young people on Facebook.
[00:56:21.040 --> 00:56:24.480]   So maybe this is a potential red news source.
[00:56:24.480 --> 00:56:25.960]   I'm not saying it's, but you know,
[00:56:25.960 --> 00:56:29.120]   you don't close those avenues off if you have access to them.
[00:56:29.120 --> 00:56:29.960]   - Yeah.
[00:56:29.960 --> 00:56:32.280]   - You're like, oh, okay, we'll work.
[00:56:32.280 --> 00:56:35.160]   We can kill lots of birds with this one stone.
[00:56:35.160 --> 00:56:38.360]   - It's good, I'm getting all my Facebook hate out now
[00:56:38.360 --> 00:56:39.560]   before it just gets back.
[00:56:39.560 --> 00:56:42.560]   - I mean, it would make a very interesting science
[00:56:42.560 --> 00:56:46.280]   competition though around AI, machine learning,
[00:56:46.280 --> 00:56:49.040]   computer vision, all those, you know,
[00:56:49.040 --> 00:56:50.840]   image modeling, you know,
[00:56:50.840 --> 00:56:54.040]   and if, you know, if we're gonna go full,
[00:56:54.040 --> 00:56:57.000]   you know, full capitalism here,
[00:56:57.000 --> 00:56:58.880]   you know, that kind of competition
[00:56:58.880 --> 00:57:02.260]   is what's gonna spur development and science
[00:57:02.260 --> 00:57:07.120]   not by siloing off the data into, you know,
[00:57:07.120 --> 00:57:09.880]   the already biggest, one of the biggest companies
[00:57:09.880 --> 00:57:14.360]   in the world that doesn't always make
[00:57:14.360 --> 00:57:16.720]   the best assessments and judgments.
[00:57:16.720 --> 00:57:20.760]   - It bothers me in the same way that it bothered me
[00:57:20.760 --> 00:57:23.000]   when research on the human genome was done
[00:57:23.000 --> 00:57:27.040]   by both commercial and nonprofit organizations.
[00:57:27.040 --> 00:57:30.200]   I kind of, but I guess it's unrealistic.
[00:57:30.200 --> 00:57:31.800]   - Our government is not funding science.
[00:57:31.800 --> 00:57:34.440]   - Yeah, it's unrealistic to suspect AI research
[00:57:34.440 --> 00:57:36.080]   to be done by nonprofits.
[00:57:36.080 --> 00:57:38.080]   - Yeah.
[00:57:38.080 --> 00:57:40.600]   - And that's, I'm gonna say,
[00:57:40.600 --> 00:57:41.180]   the
[00:57:41.180 --> 00:57:42.560]   - I'm just gonna see no project, you know,
[00:57:42.560 --> 00:57:44.720]   the way that that's ended up, you know,
[00:57:44.720 --> 00:57:47.520]   we're still, talk about data surveillance, you know,
[00:57:47.520 --> 00:57:49.720]   we're still struggling with, you know,
[00:57:49.720 --> 00:57:52.240]   corporations like 23andMe providing
[00:57:52.240 --> 00:57:56.440]   base level data to police and governments.
[00:57:56.440 --> 00:58:00.640]   And so, yeah, there's a whole other host of privacy concerns
[00:58:00.640 --> 00:58:04.400]   that are about, you know, bio material.
[00:58:04.400 --> 00:58:07.400]   - Yeah, I mean, it ended up,
[00:58:07.400 --> 00:58:11.520]   if we had relied on government or nonprofit
[00:58:11.520 --> 00:58:14.960]   to analyze the genome, we would be still waiting
[00:58:14.960 --> 00:58:18.160]   because it was Craig Vettner's commercial venture
[00:58:18.160 --> 00:58:20.800]   that ended up doing it fastest.
[00:58:20.800 --> 00:58:23.360]   But it just, but I don't wanna see people
[00:58:23.360 --> 00:58:24.800]   patent parts of the genome.
[00:58:24.800 --> 00:58:26.240]   And that's one thing that's happening.
[00:58:26.240 --> 00:58:29.920]   I just, I don't know.
[00:58:29.920 --> 00:58:33.160]   Maybe it's unrealistic of me, I guess, to expect.
[00:58:33.160 --> 00:58:34.880]   - It's not, it's just socialistic of you.
[00:58:34.880 --> 00:58:35.960]   - It's socialistic of me.
[00:58:35.960 --> 00:58:38.880]   And I'm a socialist, what can I say?
[00:58:38.880 --> 00:58:40.640]   - Well, I can't hear those are still,
[00:58:40.640 --> 00:58:42.600]   you're trained to think that's a bad word.
[00:58:42.600 --> 00:58:43.840]   - No, it's not a bad word.
[00:58:43.840 --> 00:58:46.920]   I don't want the US government to own all businesses,
[00:58:46.920 --> 00:58:49.400]   not that kind of, not government socialism, but--
[00:58:49.400 --> 00:58:50.480]   - Right.
[00:58:50.480 --> 00:58:53.840]   - I think a little society should go in there a little bit.
[00:58:53.840 --> 00:58:57.000]   We should have parity with big businesses.
[00:58:57.000 --> 00:59:00.800]   Google says, hey, it's not just Lenovo,
[00:59:00.800 --> 00:59:03.600]   we're gonna make a display equipped AI speaker
[00:59:03.600 --> 00:59:05.440]   before the holidays.
[00:59:05.440 --> 00:59:07.040]   I think this is so weird.
[00:59:07.040 --> 00:59:09.040]   - Why?
[00:59:09.040 --> 00:59:14.880]   - I mean, Amazon and other companies have kind of,
[00:59:14.880 --> 00:59:18.600]   as they've got, let other companies into this realm,
[00:59:18.600 --> 00:59:20.840]   they've limited some of the functionality.
[00:59:20.840 --> 00:59:22.760]   So unlike--
[00:59:22.760 --> 00:59:27.000]   - Is the smart display, the Lenovo Smart Display Limited,
[00:59:27.000 --> 00:59:27.840]   compared to it?
[00:59:27.840 --> 00:59:28.680]   - I don't know.
[00:59:28.680 --> 00:59:31.760]   So like, well, there is no equivalent to Google.
[00:59:31.760 --> 00:59:33.000]   - Right.
[00:59:33.000 --> 00:59:33.840]   - No.
[00:59:33.840 --> 00:59:36.320]   - It doesn't have Google, it does everything
[00:59:36.320 --> 00:59:38.640]   a Google home would do.
[00:59:38.640 --> 00:59:39.720]   - Right, well, that's what I'm saying,
[00:59:39.720 --> 00:59:43.280]   but if Amazon, when I buy lights,
[00:59:43.280 --> 00:59:44.480]   which is a terrible example,
[00:59:44.480 --> 00:59:49.120]   but some of the other products that use the Amazon Echo,
[00:59:49.120 --> 00:59:51.240]   they don't do things like play Spotify, for example.
[00:59:51.240 --> 00:59:54.760]   - No, I know, that's so nice with the Echo built in.
[00:59:54.760 --> 00:59:58.760]   With Echo built in is very limited.
[00:59:58.760 --> 01:00:01.160]   I can't even ask it for the weather.
[01:00:01.160 --> 01:00:03.480]   I mean, it's very limited.
[01:00:03.480 --> 01:00:05.200]   - So that's what I'm concerned about.
[01:00:05.200 --> 01:00:08.400]   I'm like, oh, so Google's gonna get in this game.
[01:00:08.400 --> 01:00:11.800]   What is it going to change about possibly Lenovo
[01:00:11.800 --> 01:00:13.240]   or future devices?
[01:00:13.240 --> 01:00:15.360]   - So when they announced this at I/O,
[01:00:15.360 --> 01:00:18.120]   they said that it wasn't just Lenovo.
[01:00:18.120 --> 01:00:21.360]   JBL, LG, and Sony would all be making
[01:00:21.360 --> 01:00:25.640]   screen based smart display platforms.
[01:00:25.640 --> 01:00:29.120]   - And I saw JBLs at CES.
[01:00:29.120 --> 01:00:30.000]   Did we see Sony's?
[01:00:30.000 --> 01:00:30.840]   I can't remember.
[01:00:30.840 --> 01:00:34.880]   I mean, yes, some of those are, we saw at CES.
[01:00:34.880 --> 01:00:37.440]   - Lenovo was the only compelling one, as I remember.
[01:00:37.440 --> 01:00:39.480]   - Yeah, Lenovo was the one that we were all like,
[01:00:39.480 --> 01:00:40.560]   ooh, that's pretty good.
[01:00:40.560 --> 01:00:43.160]   - A lot of people even around here have,
[01:00:43.160 --> 01:00:44.760]   I still have held off.
[01:00:44.760 --> 01:00:47.400]   - I wanted the cost code deal,
[01:00:47.400 --> 01:00:50.040]   and now that I know that people got it at Costco for 200,
[01:00:50.040 --> 01:00:51.600]   I really resound like heck and two people.
[01:00:51.600 --> 01:00:53.120]   - I know.
[01:00:53.120 --> 01:00:54.720]   - Like, oh, man.
[01:00:54.720 --> 01:00:58.280]   Joan over here is like, okay,
[01:00:58.280 --> 01:01:00.800]   were we not just talking about misuse of data?
[01:01:00.800 --> 01:01:01.640]   - And stuff.
[01:01:01.640 --> 01:01:02.480]   - I'm trying to mix it up.
[01:01:02.480 --> 01:01:03.320]   - No, I'm trying.
[01:01:03.320 --> 01:01:05.480]   - I'm trying to gadget society.
[01:01:05.480 --> 01:01:08.840]   - I am all for interesting gadgets.
[01:01:08.840 --> 01:01:11.080]   Last time I was on, I think we talked a bit about
[01:01:11.080 --> 01:01:13.080]   video cameras and the nest,
[01:01:13.080 --> 01:01:14.840]   and now I'm seeing them everywhere.
[01:01:14.840 --> 01:01:15.680]   - Everywhere.
[01:01:15.680 --> 01:01:17.640]   - I am not tuned to them,
[01:01:17.640 --> 01:01:20.520]   but they're outside of all the buildings in New York City,
[01:01:20.520 --> 01:01:24.000]   you know, and they're just really small and discreet.
[01:01:24.000 --> 01:01:25.360]   - Or maybe hidden.
[01:01:25.360 --> 01:01:27.560]   - But now I just feel, yeah.
[01:01:27.560 --> 01:01:28.560]   - The Googles.
[01:01:28.560 --> 01:01:32.160]   - The Lenovo screen is nice because unlike the Amazon Echo
[01:01:32.160 --> 01:01:35.360]   devices, it has a physical cover on the camera,
[01:01:35.360 --> 01:01:37.200]   which called me out school.
[01:01:37.200 --> 01:01:39.760]   I'm just like, now I feel secure.
[01:01:39.760 --> 01:01:43.760]   - 'Cause I have that little spot, the Amazon spot Echo.
[01:01:43.760 --> 01:01:45.800]   And Lisa won't let me put it in the bedroom,
[01:01:45.800 --> 01:01:46.960]   even though it's a perfect alarm clock.
[01:01:46.960 --> 01:01:49.760]   - Yeah, I wouldn't let you put it in the bedroom either.
[01:01:49.760 --> 01:01:53.280]   Not just because, so I talked to a security firm,
[01:01:53.280 --> 01:01:54.440]   and this freaked me out.
[01:01:54.440 --> 01:01:55.280]   - It's naked in one of it,
[01:01:55.280 --> 01:01:57.000]   just to see what will happen.
[01:01:57.000 --> 01:01:59.320]   - They said that they actually dealt with
[01:01:59.320 --> 01:02:01.600]   an internet camera company,
[01:02:01.600 --> 01:02:04.760]   whose staff was looking at videos from the customer.
[01:02:04.760 --> 01:02:07.920]   So they actually, they were touting this as like a win
[01:02:07.920 --> 01:02:09.520]   for their customer, but I'm like,
[01:02:09.520 --> 01:02:12.840]   actually that is a huge loss for everybody
[01:02:12.840 --> 01:02:13.840]   who wants to buy this price.
[01:02:13.840 --> 01:02:15.440]   - They didn't say what the brand it was, did they?
[01:02:15.440 --> 01:02:16.680]   - They did not say the brand.
[01:02:16.680 --> 01:02:18.600]   I was like, look, I'll just, yeah.
[01:02:18.600 --> 01:02:21.000]   - I don't think Amazon's gonna let that happen.
[01:02:21.000 --> 01:02:23.000]   But who knows?
[01:02:23.000 --> 01:02:24.560]   That's why I dance naked in front of mine.
[01:02:24.560 --> 01:02:26.080]   That'll stop them.
[01:02:26.080 --> 01:02:27.960]   - Oh, my God, they didn't see that.
[01:02:27.960 --> 01:02:28.800]   - Oh!
[01:02:28.800 --> 01:02:31.880]   - You know, I used to think I researched terrorism,
[01:02:31.880 --> 01:02:34.080]   but I thought I just--
[01:02:34.080 --> 01:02:36.000]   - That's really terrorism.
[01:02:36.000 --> 01:02:37.840]   - This is one kind of extremism.
[01:02:37.840 --> 01:02:39.720]   I think I'm gonna have to moderate.
[01:02:39.720 --> 01:02:42.160]   - So when I set up last night,
[01:02:42.160 --> 01:02:44.360]   a camera to monitor our cat door.
[01:02:44.360 --> 01:02:46.080]   - Oh my goodness.
[01:02:46.080 --> 01:02:47.920]   - One of these cheap, wise--
[01:02:47.920 --> 01:02:48.760]   - Is it a wise?
[01:02:48.760 --> 01:02:49.920]   - Yeah, wise pan cam.
[01:02:49.920 --> 01:02:51.760]   They're 30 bucks, why not?
[01:02:51.760 --> 01:02:54.480]   So it's plugged in, 'cause we were trying to figure out
[01:02:54.480 --> 01:02:57.000]   if which cats were coming and what animals are coming in.
[01:02:57.000 --> 01:02:58.120]   So now I get a notification
[01:02:58.120 --> 01:02:59.320]   whether every animal comes in.
[01:02:59.320 --> 01:03:02.200]   And the camera pans and follows the animal.
[01:03:02.200 --> 01:03:04.760]   But then about midnight, I should show you.
[01:03:04.760 --> 01:03:07.840]   One of our cats discovered it, an idiot.
[01:03:07.840 --> 01:03:10.520]   - She had to beat it.
[01:03:10.520 --> 01:03:11.360]   - She tried.
[01:03:11.360 --> 01:03:13.040]   - But him kids are bad for cats.
[01:03:13.040 --> 01:03:20.520]   - Yeah, 'cause the pan cams goes zzzz zzzz.
[01:03:20.520 --> 01:03:24.040]   - Yeah, they moved. - So I'm like--
[01:03:24.040 --> 01:03:25.960]   - Is this a new kind of mouse?
[01:03:25.960 --> 01:03:27.360]   What is this?
[01:03:27.360 --> 01:03:29.360]   What is this in the house?
[01:03:29.360 --> 01:03:30.960]   Here, I don't know if you could see this,
[01:03:30.960 --> 01:03:32.840]   but the cat found it.
[01:03:32.840 --> 01:03:34.320]   And it went dark.
[01:03:34.320 --> 01:03:35.160]   Did you see that?
[01:03:35.160 --> 01:03:36.320]   Let me do that again.
[01:03:36.320 --> 01:03:37.320]   Let me do that again.
[01:03:37.320 --> 01:03:38.640]   The cat found it.
[01:03:38.640 --> 01:03:39.480]   Midnight.
[01:03:40.680 --> 01:03:43.200]   Then she got out of the way.
[01:03:43.200 --> 01:03:44.360]   But that's--
[01:03:44.360 --> 01:03:47.280]   - I think that's a good use for camera, a cat door cam.
[01:03:47.280 --> 01:03:49.800]   - We've put our wise camera in front of the fish tank
[01:03:49.800 --> 01:03:51.840]   when we travel so we could see if any fish die.
[01:03:51.840 --> 01:03:53.160]   - See?
[01:03:53.160 --> 01:03:56.040]   - We have also, I actually thought about putting it
[01:03:56.040 --> 01:03:59.280]   in front of the dog door because I don't think my dog
[01:03:59.280 --> 01:04:01.640]   goes to the bathroom in the middle of the night.
[01:04:01.640 --> 01:04:03.920]   And I'm like really concerned about this.
[01:04:03.920 --> 01:04:06.480]   So these are the sort of neurotic things I have.
[01:04:06.480 --> 01:04:07.400]   I know.
[01:04:07.400 --> 01:04:08.240]   - Wait a minute.
[01:04:08.240 --> 01:04:09.080]   - 'Cause I'm trying to run it.
[01:04:09.080 --> 01:04:11.160]   Want your dog to go to the bathroom in the middle of the night
[01:04:11.160 --> 01:04:12.000]   or not?
[01:04:12.000 --> 01:04:14.800]   - Like, I don't think she goes to the bathroom
[01:04:14.800 --> 01:04:16.360]   after like six o'clock at night
[01:04:16.360 --> 01:04:18.160]   until like seven the next morning.
[01:04:18.160 --> 01:04:19.480]   - You know what, Nieh, do you why?
[01:04:19.480 --> 01:04:22.240]   I think that's completely reasonable.
[01:04:22.240 --> 01:04:24.320]   - I don't wanna know this much about your bathroom house,
[01:04:24.320 --> 01:04:26.360]   but I feel it's weird for my dog.
[01:04:26.360 --> 01:04:28.720]   - I don't know.
[01:04:28.720 --> 01:04:29.640]   It's nighttime.
[01:04:29.640 --> 01:04:32.240]   The dog goes to sleep, wakes up in the morning.
[01:04:32.240 --> 01:04:34.920]   Like any normal dog goes outside.
[01:04:34.920 --> 01:04:37.760]   Seems reasonable.
[01:04:37.760 --> 01:04:39.920]   Anyway, but now you know, right?
[01:04:39.920 --> 01:04:41.480]   - Well, yeah, this is the plan.
[01:04:41.480 --> 01:04:42.960]   I'm like, oh, okay.
[01:04:42.960 --> 01:04:45.000]   - That's what happens, Joan, when these cameras
[01:04:45.000 --> 01:04:48.200]   are so cheap, which they are now, $20 for the static camera.
[01:04:48.200 --> 01:04:52.080]   Well, there's no reason not to have them everywhere.
[01:04:52.080 --> 01:04:54.040]   - Well, I told someone you had to put it in their garage
[01:04:54.040 --> 01:04:56.000]   'cause they were like, I need to open closed garage door
[01:04:56.000 --> 01:04:56.840]   sensor.
[01:04:56.840 --> 01:04:58.160]   I was like, actually 20 bucks.
[01:04:58.160 --> 01:04:59.520]   Just pop that sucker in there.
[01:04:59.520 --> 01:05:01.600]   If it's right there, your door's open.
[01:05:01.600 --> 01:05:04.240]   - I don't know.
[01:05:04.240 --> 01:05:07.600]   I'm looking at these screens on these smart speakers
[01:05:07.600 --> 01:05:10.360]   and I'm just like, why don't you just plug a speaker
[01:05:10.360 --> 01:05:12.880]   into your old iPad?
[01:05:12.880 --> 01:05:14.480]   And here you go.
[01:05:14.480 --> 01:05:17.720]   - So some people do their, you can use your,
[01:05:17.720 --> 01:05:22.720]   it's the Kindle Fire tablets as an echo show, basically.
[01:05:22.720 --> 01:05:26.560]   So that is something that can happen.
[01:05:26.560 --> 01:05:30.920]   - You can't have this Lenovo, you can have it.
[01:05:30.920 --> 01:05:34.640]   You can have it monitor any of the cameras too.
[01:05:34.640 --> 01:05:38.520]   So it's like a multi-camera interface.
[01:05:38.520 --> 01:05:41.880]   - And you can find it 'cause it's like Leo doesn't want
[01:05:41.880 --> 01:05:44.080]   to be on Facebook 'cause he's afraid of other people
[01:05:44.080 --> 01:05:46.760]   watching him and now he's like, I can watch myself.
[01:05:46.760 --> 01:05:49.000]   - No, I am not afraid of other people watching.
[01:05:49.000 --> 01:05:51.080]   - I can stand to make in front of his spy pam.
[01:05:51.080 --> 01:05:53.040]   - The decision not to be on social media
[01:05:53.040 --> 01:05:54.600]   is not because they're spying on me
[01:05:54.600 --> 01:05:57.320]   because I just don't wanna participate.
[01:05:57.320 --> 01:05:58.160]   - Okay.
[01:05:58.160 --> 01:05:59.560]   - That's separate 'cause it makes me feel bad.
[01:05:59.560 --> 01:06:01.120]   I don't like the content.
[01:06:01.120 --> 01:06:02.280]   It makes me feel bad.
[01:06:02.280 --> 01:06:03.640]   Actually, that's all I need to say.
[01:06:03.640 --> 01:06:05.520]   It makes me feel bad.
[01:06:05.520 --> 01:06:06.720]   So I don't wanna, it's just bad.
[01:06:06.720 --> 01:06:08.040]   I noticed I feel much happier
[01:06:08.040 --> 01:06:09.600]   when I'm not looking at that crap.
[01:06:09.600 --> 01:06:12.160]   Even Instagram of late.
[01:06:12.160 --> 01:06:15.360]   So that's why I just shuttered those accounts.
[01:06:15.360 --> 01:06:20.360]   I don't like Facebook 'cause I do feel like they're a very,
[01:06:20.360 --> 01:06:23.080]   and I know Jeff hates it when I say this,
[01:06:23.080 --> 01:06:27.580]   but I really feel like they're an evil company.
[01:06:27.580 --> 01:06:31.320]   Maybe that's wrong, but I feel all the evidence is
[01:06:32.760 --> 01:06:36.200]   that Jason Calicanis was here on Twitter the other day.
[01:06:36.200 --> 01:06:37.160]   He knows these guys.
[01:06:37.160 --> 01:06:39.320]   He said, "Oh yeah, they're evil."
[01:06:39.320 --> 01:06:40.840]   Oh, okay.
[01:06:40.840 --> 01:06:44.480]   He said, "Their whole deal is do what they want,
[01:06:44.480 --> 01:06:45.320]   do whatever they want.
[01:06:45.320 --> 01:06:46.480]   They get caught, they apologize,
[01:06:46.480 --> 01:06:47.720]   and then they go on doing it."
[01:06:47.720 --> 01:06:50.240]   And there's a lot of evidence that that's true.
[01:06:50.240 --> 01:06:55.320]   So that's why I don't wanna give them a 10,000 MRI records
[01:06:55.320 --> 01:06:57.640]   for NYU, and I think that's a poor choice
[01:06:57.640 --> 01:06:58.720]   for NYU to make.
[01:06:58.720 --> 01:07:00.480]   Yeah, they're anonymized.
[01:07:00.480 --> 01:07:04.360]   But as we all know, anonymous is not really that anonymous.
[01:07:04.360 --> 01:07:06.340]   Where has been, but it hasn't, okay,
[01:07:06.340 --> 01:07:09.760]   didn't Mark Zuckerberg and his wife donate a lot of money
[01:07:09.760 --> 01:07:12.320]   to solve some disease or something?
[01:07:12.320 --> 01:07:15.040]   Oh, in that case, let me join Facebook up,
[01:07:15.040 --> 01:07:18.120]   like John Thomas gonna log right in and...
[01:07:18.120 --> 01:07:20.000]   Who cares?
[01:07:20.000 --> 01:07:22.200]   No, I'm just saying they may have established
[01:07:22.200 --> 01:07:23.200]   a link through John Thomas.
[01:07:23.200 --> 01:07:25.000]   Oh, through NYU, I see, yeah, yeah, yeah.
[01:07:25.000 --> 01:07:29.280]   But I don't remember who they donated to or how that worked.
[01:07:29.280 --> 01:07:33.080]   Happiness, happiness, happiness, happiness.
[01:07:33.080 --> 01:07:34.680]   Happiness.
[01:07:34.680 --> 01:07:36.960]   Would you like some happiness?
[01:07:36.960 --> 01:07:39.320]   I'm looking, do you have some happiness?
[01:07:39.320 --> 01:07:43.720]   I do, 'cause hot chips, woo, woo.
[01:07:43.720 --> 01:07:45.920]   Oh, hot chips is here.
[01:07:45.920 --> 01:07:48.400]   It was this week, and they talked about,
[01:07:48.400 --> 01:07:49.760]   this is actually Google related.
[01:07:49.760 --> 01:07:51.920]   So you get a two for one, Stacey nerds out on chips,
[01:07:51.920 --> 01:07:53.240]   and it's Google related.
[01:07:53.240 --> 01:07:54.240]   Yay.
[01:07:54.240 --> 01:07:55.760]   They did the Pixel Visual Core.
[01:07:55.760 --> 01:07:59.160]   They broke it down for people at the event.
[01:07:59.160 --> 01:08:01.040]   So this is their custom...
[01:08:01.040 --> 01:08:02.640]   Was this the Google Next event?
[01:08:02.640 --> 01:08:04.240]   What event was this?
[01:08:04.240 --> 01:08:05.880]   No, this was hot chips.
[01:08:05.880 --> 01:08:07.600]   It's called hot chips.
[01:08:07.600 --> 01:08:08.880]   Do you not know about hot chips?
[01:08:08.880 --> 01:08:13.280]   Only the most exciting event of the season?
[01:08:13.280 --> 01:08:16.120]   The only hot chips I'm interested in come with KSO.
[01:08:16.120 --> 01:08:19.760]   It is a annual chip conference.
[01:08:19.760 --> 01:08:22.760]   It's a symposium on high performance chips.
[01:08:22.760 --> 01:08:23.600]   It's amazing.
[01:08:23.600 --> 01:08:26.760]   Sponsored by the IEEE Technical Committee,
[01:08:26.760 --> 01:08:29.320]   microprocessors and microcomputers.
[01:08:29.320 --> 01:08:30.680]   They know how to throw a party.
[01:08:30.680 --> 01:08:32.400]   Let me tell you.
[01:08:32.400 --> 01:08:34.400]   Okay, I guarantee you, like,
[01:08:34.400 --> 01:08:36.440]   a good third of your audience is probably like,
[01:08:36.440 --> 01:08:37.280]   "Yeah, I just--"
[01:08:37.280 --> 01:08:38.120]   No, they are.
[01:08:38.120 --> 01:08:38.960]   No, they already are.
[01:08:38.960 --> 01:08:39.800]   Let me read guys, woo.
[01:08:39.800 --> 01:08:41.120]   The chatroom already said,
[01:08:41.120 --> 01:08:42.120]   scooter X and the chatroom,
[01:08:42.120 --> 01:08:44.440]   and I said, "You should be talking about hot chips."
[01:08:44.440 --> 01:08:48.520]   So tell me what you learned at hot chips this year.
[01:08:48.520 --> 01:08:52.800]   Well, I did not go because I don't officially cover chips
[01:08:52.800 --> 01:08:54.200]   anymore, but I get a lot of the briefings.
[01:08:54.200 --> 01:08:56.960]   So, but I didn't go to this.
[01:08:56.960 --> 01:08:58.720]   So I was reading about the Google one.
[01:08:58.720 --> 01:09:03.720]   So everybody launches both like fancy innovations
[01:09:03.720 --> 01:09:05.200]   and making their server chips better,
[01:09:05.200 --> 01:09:06.760]   design new architectures.
[01:09:06.760 --> 01:09:08.280]   And then you get a lot of researchers
[01:09:08.280 --> 01:09:12.680]   talking about like crazy wacky cool stuff that they can do.
[01:09:12.680 --> 01:09:16.440]   So Google talked about their Pixel Visual Core chip
[01:09:16.440 --> 01:09:17.920]   that they designed.
[01:09:17.920 --> 01:09:19.520]   And this was,
[01:09:21.640 --> 01:09:24.600]   they taught, this was what they used in--
[01:09:24.600 --> 01:09:26.200]   This isn't the Pixel 2.
[01:09:26.200 --> 01:09:28.440]   Yes, I was like, the phone.
[01:09:28.440 --> 01:09:29.600]   I'm currently using this guy.
[01:09:29.600 --> 01:09:32.360]   I'm holding up here, this guy.
[01:09:32.360 --> 01:09:34.840]   This is what makes this crappy little lens,
[01:09:34.840 --> 01:09:37.320]   single lens camera looks better
[01:09:37.320 --> 01:09:40.360]   than any other camera phone on the market.
[01:09:40.360 --> 01:09:43.280]   Yeah, so they did this whole talk about how they decided,
[01:09:43.280 --> 01:09:46.360]   everyone's like, "Oh, you know, use a dedicated chip."
[01:09:46.360 --> 01:09:49.440]   They were like, "Oh, CPUs didn't work, GPUs didn't work."
[01:09:49.440 --> 01:09:51.240]   So we designed our own ASIC.
[01:09:51.240 --> 01:09:54.920]   And they baked the algorithms that they wanted to use in,
[01:09:54.920 --> 01:09:58.120]   but that's really hard because algorithms change
[01:09:58.120 --> 01:09:59.600]   all the time.
[01:09:59.600 --> 01:10:03.640]   So they basically talked, it was really nerdy on like,
[01:10:03.640 --> 01:10:06.720]   how they, like the trade-offs and engineering compromises.
[01:10:06.720 --> 01:10:08.160]   So is this field programmable?
[01:10:08.160 --> 01:10:09.720]   I mean, can you update?
[01:10:09.720 --> 01:10:12.000]   No. No, it is not an FPGA.
[01:10:12.000 --> 01:10:13.200]   It's an ASIC.
[01:10:13.200 --> 01:10:18.200]   So they design it and then it's set in, not stone, silicon.
[01:10:18.760 --> 01:10:22.280]   Ha ha, sand, it's set in sand.
[01:10:22.280 --> 01:10:23.840]   Well, now I'm looking at this slide though.
[01:10:23.840 --> 01:10:25.960]   Maybe I got confused by this slide.
[01:10:25.960 --> 01:10:29.200]   So here's the CPU, which is fully programmable.
[01:10:29.200 --> 01:10:32.360]   Here's an ASIC, which is fully not programmable.
[01:10:32.360 --> 01:10:34.760]   The right side of the slide is performance.
[01:10:34.760 --> 01:10:37.520]   So ASICs are much higher performance
[01:10:37.520 --> 01:10:41.400]   and CPU is a much higher energy per operation.
[01:10:41.400 --> 01:10:46.400]   They say this is a PVC, the pixel visual course, IPU,
[01:10:48.440 --> 01:10:50.680]   which they say is not an ASIC,
[01:10:50.680 --> 01:10:53.560]   but a programmable image processing unit.
[01:10:53.560 --> 01:10:55.800]   So it is programmable.
[01:10:55.800 --> 01:10:58.680]   Oh, crap, you're right.
[01:10:58.680 --> 01:11:00.320]   It's in, but that slide does say that.
[01:11:00.320 --> 01:11:03.280]   That is not the takeaway I got for this at all.
[01:11:03.280 --> 01:11:04.560]   Well, you gotta look at the slides,
[01:11:04.560 --> 01:11:05.920]   'cause I look at the pictures.
[01:11:05.920 --> 01:11:08.760]   I can't understand the text, so I look at the pictures.
[01:11:08.760 --> 01:11:09.680]   (laughs)
[01:11:09.680 --> 01:11:12.440]   This is actually a non-text live blog of it.
[01:11:12.440 --> 01:11:13.720]   But I thought that was interesting
[01:11:13.720 --> 01:11:15.800]   and that is in response to what you said,
[01:11:15.800 --> 01:11:20.800]   which is it's very hard to bake the algorithms in
[01:11:20.800 --> 01:11:23.360]   as you're designing this, 'cause things change.
[01:11:23.360 --> 01:11:25.440]   Yeah, but this isn't an FPGA.
[01:11:25.440 --> 01:11:28.640]   This is not quite an FPGA either.
[01:11:28.640 --> 01:11:30.520]   So-- Right, that's why I'm like--
[01:11:30.520 --> 01:11:33.800]   Yeah, it's a, so the high level program module model
[01:11:33.800 --> 01:11:37.800]   is halide, which is actually interestingly a program,
[01:11:37.800 --> 01:11:39.760]   kind of a camera program, but I think that's a coincidence.
[01:11:39.760 --> 01:11:42.560]   Domain specific language for image processing.
[01:11:42.560 --> 01:11:46.040]   The IPU supports a subset of the halide language.
[01:11:46.040 --> 01:11:48.880]   It's not floating points, it's very simple language.
[01:11:48.880 --> 01:11:50.800]   And they're sad about the loading point.
[01:11:50.800 --> 01:11:52.280]   And they're floating point.
[01:11:52.280 --> 01:11:56.040]   Halide back-end generates kernels and all API calls,
[01:11:56.040 --> 01:11:58.200]   proprietary API for resource allocation
[01:11:58.200 --> 01:11:59.760]   and execution control.
[01:11:59.760 --> 01:12:00.680]   So that's interesting.
[01:12:00.680 --> 01:12:05.680]   So it is, like a CPU in that you can write software to it.
[01:12:05.680 --> 01:12:09.440]   Well, yeah, all of this, I mean,
[01:12:09.440 --> 01:12:11.600]   it's a piece of silicon, it's a chip.
[01:12:11.600 --> 01:12:14.320]   But Na6 has got a more dedicated than that, right?
[01:12:14.320 --> 01:12:18.000]   It is, but you do write, I mean, you write software for it.
[01:12:18.000 --> 01:12:20.720]   Or you, yeah, I could write a glue code around it, I guess.
[01:12:20.720 --> 01:12:23.960]   But it's a done deal.
[01:12:23.960 --> 01:12:26.320]   Well, now I need to call Google and talk to them.
[01:12:26.320 --> 01:12:27.960]   All right, well, we won't try to talk about it.
[01:12:27.960 --> 01:12:30.320]   Most Na6 do stencil operations, the value of a pixel
[01:12:30.320 --> 01:12:32.400]   is a function of a pixel of the pixels routed.
[01:12:32.400 --> 01:12:35.000]   Computer's over a pixel array to calculate a new pixel
[01:12:35.000 --> 01:12:35.960]   requires a lot of data.
[01:12:35.960 --> 01:12:37.040]   See, this is why I don't read the text.
[01:12:37.040 --> 01:12:38.400]   Let me look at the pictures.
[01:12:38.400 --> 01:12:39.240]   Hard way.
[01:12:39.240 --> 01:12:40.880]   (laughing)
[01:12:40.880 --> 01:12:42.720]   I'm just gonna, I'm gonna talk to the Google guys.
[01:12:42.720 --> 01:12:43.560]   So I'll figure it out.
[01:12:43.560 --> 01:12:46.000]   It's interesting, it has its own RAM.
[01:12:46.000 --> 01:12:48.440]   It's DDR4 RAM.
[01:12:48.440 --> 01:12:51.880]   Well, yeah, 'cause it's, you have to have access to,
[01:12:51.880 --> 01:12:53.840]   like you have to have access to memory
[01:12:53.840 --> 01:12:55.760]   so you can optimize really quickly.
[01:12:55.760 --> 01:12:58.200]   That is, that is.
[01:12:58.200 --> 01:13:00.320]   It's got eight cores.
[01:13:00.320 --> 01:13:01.840]   Wow.
[01:13:01.840 --> 01:13:03.520]   Wow, this is a sophisticated thing.
[01:13:03.520 --> 01:13:06.800]   Did they talk about, 'cause one of the things
[01:13:06.800 --> 01:13:09.080]   we knew about the Pixel Visual Core,
[01:13:09.080 --> 01:13:10.880]   actually we didn't know about it.
[01:13:10.880 --> 01:13:14.520]   They didn't tell us until after they started selling the phone.
[01:13:14.520 --> 01:13:16.440]   And it wasn't even enabled at first.
[01:13:16.440 --> 01:13:18.600]   You suspected they were doing something, yes.
[01:13:18.600 --> 01:13:23.080]   Oh yeah, yes, it wasn't enabled and now I guess it is.
[01:13:23.080 --> 01:13:27.040]   But the other thing that was there is ARM announced their,
[01:13:27.040 --> 01:13:29.600]   what I think of as their CPU roadmap.
[01:13:29.600 --> 01:13:30.680]   Yeah.
[01:13:30.680 --> 01:13:32.360]   This was fascinating too.
[01:13:32.360 --> 01:13:35.160]   Yeah, 'cause that gets them,
[01:13:35.160 --> 01:13:37.280]   they're really gunning for the laptop market here
[01:13:37.280 --> 01:13:38.360]   and we didn't talk about it.
[01:13:38.360 --> 01:13:40.000]   Kevin and I didn't talk about it on the show,
[01:13:40.000 --> 01:13:41.480]   like our show, IoT show,
[01:13:41.480 --> 01:13:44.440]   because it wasn't really an IoT news thing.
[01:13:44.440 --> 01:13:46.920]   But he's super pumped about it for Chromebooks,
[01:13:46.920 --> 01:13:51.800]   although it's basically what they're saying is,
[01:13:51.800 --> 01:13:54.440]   we are gonna, in the next two years,
[01:13:54.440 --> 01:13:56.400]   we will be desktop class.
[01:13:56.400 --> 01:13:59.640]   I don't think they're saying we'll be desktop.
[01:13:59.640 --> 01:14:03.520]   Yeah, they're saying we'll be the equivalent of an i5,
[01:14:03.520 --> 01:14:07.240]   sorry, I don't believe that they're gonna be desktop class.
[01:14:07.400 --> 01:14:09.600]   You don't think so, huh?
[01:14:09.600 --> 01:14:11.160]   Wouldn't that be interesting?
[01:14:11.160 --> 01:14:13.320]   Don't get me wrong, but that's...
[01:14:13.320 --> 01:14:18.320]   The Cortex-A76 is 10 nanometer and 7 nanometer.
[01:14:18.320 --> 01:14:19.200]   Yeah.
[01:14:19.200 --> 01:14:20.720]   They'll be followed by 7 nanometer
[01:14:20.720 --> 01:14:23.720]   and 5 nanometer chips by 2020,
[01:14:23.720 --> 01:14:25.960]   they'll be into 5 nanometers.
[01:14:25.960 --> 01:14:26.800]   Don't panic.
[01:14:26.800 --> 01:14:29.120]   What the what, what the what?
[01:14:29.120 --> 01:14:32.720]   Our roadmap of client CPUs has been designed
[01:14:32.720 --> 01:14:36.840]   to take advantage of the disruptive innovation 5G.
[01:14:37.320 --> 01:14:38.800]   We'll bring to client devices.
[01:14:38.800 --> 01:14:42.200]   I think this is what ARMzaming at is an always on
[01:14:42.200 --> 01:14:45.760]   all day battery life,
[01:14:45.760 --> 01:14:48.840]   maybe not top of the line processor performance,
[01:14:48.840 --> 01:14:52.040]   but good enough desktop class performance
[01:14:52.040 --> 01:14:56.440]   to give you a laptop that in the next couple of years,
[01:14:56.440 --> 01:14:59.680]   that will be a completely different thing, right?
[01:14:59.680 --> 01:15:02.480]   They say to break through the dominance of X86,
[01:15:02.480 --> 01:15:03.960]   aka until an AMD,
[01:15:03.960 --> 01:15:06.800]   and gauge substantial market share in Windows laptops
[01:15:06.800 --> 01:15:09.280]   and Chromebooks over the next five years.
[01:15:09.280 --> 01:15:10.120]   Right.
[01:15:10.120 --> 01:15:11.080]   That's why when you say desktop,
[01:15:11.080 --> 01:15:13.280]   I think of like higher end gaming computers.
[01:15:13.280 --> 01:15:14.320]   No, no, no, no, no, no, no, no.
[01:15:14.320 --> 01:15:16.160]   And I'm like, they're not gonna get you there.
[01:15:16.160 --> 01:15:17.840]   They're gonna get you to laptop performance.
[01:15:17.840 --> 01:15:19.720]   Okay, laptop's good though.
[01:15:19.720 --> 01:15:24.480]   I have an ARM based Windows laptop, the HP NVX2,
[01:15:24.480 --> 01:15:27.400]   and it's unusably slow, it's an 835 based.
[01:15:27.400 --> 01:15:29.360]   And it's just, it's not, you can use it.
[01:15:29.360 --> 01:15:30.800]   I shouldn't say unusable.
[01:15:30.800 --> 01:15:32.960]   It's just, you gotta have patience.
[01:15:32.960 --> 01:15:35.640]   But if they could get that,
[01:15:35.640 --> 01:15:37.080]   and this is what they say next year,
[01:15:37.080 --> 01:15:41.200]   they'll get that to the equivalent of a 2013 I5.
[01:15:41.200 --> 01:15:42.160]   That's fine.
[01:15:42.160 --> 01:15:46.760]   Especially if you have all day battery life.
[01:15:46.760 --> 01:15:48.240]   When's 5G happening?
[01:15:48.240 --> 01:15:50.320]   I keep asking this every week.
[01:15:50.320 --> 01:15:52.120]   Goodness, who knows?
[01:15:52.120 --> 01:15:54.960]   It's already happened, Leo.
[01:15:54.960 --> 01:15:56.240]   What did you blink and miss it?
[01:15:56.240 --> 01:15:57.080]   I missed it.
[01:15:57.080 --> 01:16:01.040]   The smartphone should be out next year, 2019.
[01:16:01.040 --> 01:16:03.720]   Smartphones that are capable of using 5G.
[01:16:04.920 --> 01:16:07.680]   I can't remember, I think some of them are going to be,
[01:16:07.680 --> 01:16:08.520]   sorry.
[01:16:08.520 --> 01:16:10.720]   No, the phones actually will be out.
[01:16:10.720 --> 01:16:12.920]   Modules will be out first, phones later.
[01:16:12.920 --> 01:16:16.880]   Yeah, we have a, a Moto X phone that they say
[01:16:16.880 --> 01:16:21.560]   they'll have a 5G Moto Mod 4 in a year, right?
[01:16:21.560 --> 01:16:24.040]   But will the carriers be there?
[01:16:24.040 --> 01:16:27.440]   The carriers plan to be there in some markets
[01:16:27.440 --> 01:16:31.840]   with actual 5G.
[01:16:31.840 --> 01:16:32.680]   Yes.
[01:16:32.680 --> 01:16:33.680]   Wow. Yes, they will.
[01:16:34.680 --> 01:16:36.680]   That is a true statement.
[01:16:36.680 --> 01:16:37.680]   All right.
[01:16:37.680 --> 01:16:41.680]   One more story and then we'll take a break and the change log
[01:16:41.680 --> 01:16:43.680]   because there are a few things that Google has announced
[01:16:43.680 --> 01:16:45.680]   and I know you're interested.
[01:16:45.680 --> 01:16:47.680]   Was there anything else from Hotchips you were excited about
[01:16:47.680 --> 01:16:49.680]   besides the arm room map and the pixel?
[01:16:49.680 --> 01:16:53.680]   No, Microsoft showed off their so-per-stuff again.
[01:16:53.680 --> 01:16:55.680]   Which I was kind of...
[01:16:55.680 --> 01:16:56.680]   They're what stuff?
[01:16:56.680 --> 01:16:58.680]   They're, they're IoT security stuff.
[01:16:58.680 --> 01:16:59.680]   Oh, yeah.
[01:16:59.680 --> 01:17:00.680]   We've talked about it in the past.
[01:17:00.680 --> 01:17:01.680]   They keep talking about it like it's a new thing
[01:17:01.680 --> 01:17:04.680]   and I, I'm still like, Microsoft, what are you doing?
[01:17:04.680 --> 01:17:07.680]   They still talk about HoloLens like it's a new thing.
[01:17:07.680 --> 01:17:09.680]   Oh, HoloLens was nice.
[01:17:09.680 --> 01:17:11.680]   I want to take a little break.
[01:17:11.680 --> 01:17:15.680]   If you are using passwords that you generate in your head
[01:17:15.680 --> 01:17:19.680]   or worse using the same password over and over,
[01:17:19.680 --> 01:17:22.680]   you need to know about the best password manager ever made,
[01:17:22.680 --> 01:17:23.680]   LastPass.
[01:17:23.680 --> 01:17:26.680]   And if you're a business that trusts employees with passwords,
[01:17:26.680 --> 01:17:30.680]   you really need to know about the best last password manager
[01:17:30.680 --> 01:17:33.680]   ever made, LastPass.
[01:17:33.680 --> 01:17:38.680]   According to a survey, 32% of employees share passwords
[01:17:38.680 --> 01:17:39.680]   with others.
[01:17:39.680 --> 01:17:43.680]   In our case, it was one employee, an engineering manager,
[01:17:43.680 --> 01:17:46.680]   who shared all the passwords with others on an open public
[01:17:46.680 --> 01:17:47.680]   website.
[01:17:47.680 --> 01:17:53.680]   That's when we immediately installed LastPass Enterprise.
[01:17:53.680 --> 01:17:57.680]   It has saved our bacon more times than I can even think.
[01:17:57.680 --> 01:18:01.680]   It makes it convenient to share passwords without sharing
[01:18:01.680 --> 01:18:03.680]   the keys to the kingdom.
[01:18:03.680 --> 01:18:06.680]   In fact, I can share a password, say, to our QuickBooks,
[01:18:06.680 --> 01:18:09.680]   to our bookkeeping, and the person who gets it doesn't even
[01:18:09.680 --> 01:18:10.680]   get the password.
[01:18:10.680 --> 01:18:13.680]   They just get the ability to log in until I revoke it at any time.
[01:18:13.680 --> 01:18:16.680]   So if an employee leaves or doesn't need access,
[01:18:16.680 --> 01:18:17.680]   I can take it away.
[01:18:17.680 --> 01:18:21.680]   In fact, you could configure over 100 policies, access security
[01:18:21.680 --> 01:18:23.680]   reports, create shared folders.
[01:18:23.680 --> 01:18:26.680]   We have, for instance, an ops folder that our DevOps team has
[01:18:26.680 --> 01:18:29.680]   access to, but nobody else does.
[01:18:29.680 --> 01:18:33.680]   You could set master password requirements so that they have
[01:18:33.680 --> 01:18:36.680]   to, you know, they can't use change me one, two, three as their
[01:18:36.680 --> 01:18:38.680]   password for the LastPass.
[01:18:38.680 --> 01:18:40.680]   We require two factor.
[01:18:40.680 --> 01:18:41.680]   That's nice too.
[01:18:41.680 --> 01:18:45.680]   In fact, LastPass has its own authenticator app, which makes
[01:18:45.680 --> 01:18:46.680]   it really easy.
[01:18:46.680 --> 01:18:49.680]   An authentication, a verification button pops up on an employee's
[01:18:49.680 --> 01:18:53.680]   phone when they try to log in to say, you know, hey, are you
[01:18:53.680 --> 01:18:54.680]   you?
[01:18:54.680 --> 01:18:57.680]   Make sure they're the only ones with access to their accounts.
[01:18:57.680 --> 01:19:01.680]   We give as a benefit of employee, everybody who works here not
[01:19:01.680 --> 01:19:05.680]   only uses the LastPass Enterprise, of course, at work, but they
[01:19:05.680 --> 01:19:08.680]   get their own LastPass personal for home.
[01:19:08.680 --> 01:19:10.680]   So that's nice too.
[01:19:10.680 --> 01:19:13.680]   Even if credentials might be compromised through phishing
[01:19:13.680 --> 01:19:17.680]   attacks or malware, having two factor on keeps outsiders out.
[01:19:17.680 --> 01:19:20.680]   LastPass's password generator makes it easy to use or create
[01:19:20.680 --> 01:19:23.680]   in a unique, random password your employees don't have to
[01:19:23.680 --> 01:19:25.680]   remember or write down.
[01:19:25.680 --> 01:19:28.680]   And if you use Microsoft Active Directory, here's a great
[01:19:28.680 --> 01:19:29.680]   benefit.
[01:19:29.680 --> 01:19:31.680]   You can use it to log on to your LastPass.
[01:19:31.680 --> 01:19:34.680]   So now they really only have one password to remember to get
[01:19:34.680 --> 01:19:35.680]   into everything.
[01:19:35.680 --> 01:19:38.680]   Data is encrypted and decrypted at the device level.
[01:19:38.680 --> 01:19:41.680]   Data stored in the vault is kept secret even from LastPass.
[01:19:41.680 --> 01:19:43.680]   They don't have access to it either.
[01:19:43.680 --> 01:19:46.680]   And I know that's true because Steve Gibson has seen the code
[01:19:46.680 --> 01:19:49.680]   and he's given it his seal of approval.
[01:19:49.680 --> 01:19:52.680]   From easy onboarding to password auto fill, LastPass makes it
[01:19:52.680 --> 01:19:55.680]   easy for businesses to take control of passwords and
[01:19:55.680 --> 01:19:57.680]   reduce the threat of breach.
[01:19:57.680 --> 01:20:01.680]   I know that as an individual you're using it by now, right?
[01:20:01.680 --> 01:20:03.680]   But get it for your work too.
[01:20:03.680 --> 01:20:05.680]   LastPass Premium for personal use, LastPass Families for the
[01:20:05.680 --> 01:20:07.680]   entire family, that's what we use at home.
[01:20:07.680 --> 01:20:10.680]   LastPass Teams for teams of 50 or fewer.
[01:20:10.680 --> 01:20:12.680]   There are so many great features.
[01:20:12.680 --> 01:20:15.680]   The LastPass Families has an emergency access feature where
[01:20:15.680 --> 01:20:20.680]   you can designate, I designate my wife if something should happen
[01:20:20.680 --> 01:20:23.680]   to me, she would get access to the password vault.
[01:20:23.680 --> 01:20:26.680]   You see so many, you hear so many stories about people who pass
[01:20:26.680 --> 01:20:30.680]   away and their next if kin have no access to the bank, it
[01:20:30.680 --> 01:20:31.680]   counts anything.
[01:20:31.680 --> 01:20:34.680]   Makes it very difficult to resolve your estate.
[01:20:34.680 --> 01:20:37.680]   So I actually gave access to my daughter and Lisa, but they
[01:20:37.680 --> 01:20:39.680]   don't have access to my passwords yet.
[01:20:39.680 --> 01:20:41.680]   If something should happen to me, they request it.
[01:20:41.680 --> 01:20:43.680]   LastPass checks with me.
[01:20:43.680 --> 01:20:44.680]   I don't respond.
[01:20:44.680 --> 01:20:46.680]   I said so if I don't respond within a week, but you can set
[01:20:46.680 --> 01:20:50.680]   the timeframe, then they pass along the keys so that Lisa can
[01:20:50.680 --> 01:20:53.680]   get in there and settle my accounts.
[01:20:53.680 --> 01:20:56.680]   I don't need a debt certificate.
[01:20:56.680 --> 01:21:00.680]   No, no, it's a dead man switch, which I think is better,
[01:21:00.680 --> 01:21:01.680]   frankly.
[01:21:01.680 --> 01:21:06.680]   But I think that that really works well because I could say,
[01:21:06.680 --> 01:21:09.680]   well, I have a month or I have a week or I have a day to respond.
[01:21:09.680 --> 01:21:10.680]   And they say they email.
[01:21:10.680 --> 01:21:15.680]   Unless I just don't respond.
[01:21:15.680 --> 01:21:17.680]   So I think that that's a really good way to do it.
[01:21:17.680 --> 01:21:19.680]   And you would only do it with somebody you really trust.
[01:21:19.680 --> 01:21:22.680]   Obviously your attorney or your family members, that kind of
[01:21:22.680 --> 01:21:23.680]   thing.
[01:21:23.680 --> 01:21:25.680]   So if the government picks you up and takes you away, but
[01:21:25.680 --> 01:21:27.680]   you're still alive.
[01:21:27.680 --> 01:21:30.680]   She might then want access to my accounts, right?
[01:21:30.680 --> 01:21:31.680]   Exactly.
[01:21:31.680 --> 01:21:32.680]   Right?
[01:21:32.680 --> 01:21:33.680]   Yeah.
[01:21:33.680 --> 01:21:34.680]   All right.
[01:21:34.680 --> 01:21:35.680]   He's not dead yet.
[01:21:35.680 --> 01:21:36.680]   But he's not available.
[01:21:36.680 --> 01:21:38.680]   He's not around.
[01:21:38.680 --> 01:21:43.680]   And I just want to watch Netflix.
[01:21:43.680 --> 01:21:48.680]   More than 13 million people, including me for the last 10,
[01:21:48.680 --> 01:21:50.680]   you know, last past is 10 years old this July.
[01:21:50.680 --> 01:21:53.680]   It's the first part of a program I install on any new device.
[01:21:53.680 --> 01:21:56.680]   I installed Chrome and then I install LastPass.
[01:21:56.680 --> 01:21:58.680]   You got to have it.
[01:21:58.680 --> 01:22:00.680]   The number one most preferred password manager, 13 million
[01:22:00.680 --> 01:22:04.680]   individual users, 33,000 business users.
[01:22:04.680 --> 01:22:10.680]   LastPass.com/twit, it really is a very, very important.
[01:22:10.680 --> 01:22:13.680]   LastPass.com/twit.
[01:22:13.680 --> 01:22:14.680]   Facebook.
[01:22:14.680 --> 01:22:16.680]   Okay, Joan, I want to know what you think about this.
[01:22:16.680 --> 01:22:20.680]   Facebook, back to Facebook, is rating the trustworthiness of its
[01:22:20.680 --> 01:22:22.680]   users.
[01:22:22.680 --> 01:22:24.680]   They say on a scale from zero to one, but that could just as well
[01:22:24.680 --> 01:22:25.680]   be zero to one hundred, right?
[01:22:25.680 --> 01:22:26.680]   It doesn't really matter.
[01:22:26.680 --> 01:22:28.680]   The scale is.
[01:22:28.680 --> 01:22:33.680]   So they're assessing your reputation as part of an effort
[01:22:33.680 --> 01:22:34.680]   against fake news.
[01:22:34.680 --> 01:22:38.680]   So what Facebook says it'll do is look at the stories you post
[01:22:38.680 --> 01:22:44.680]   and if they're true, you will get a higher rating.
[01:22:44.680 --> 01:22:50.680]   This, yeah, it's, I don't know, I don't know where they're
[01:22:50.680 --> 01:22:53.680]   getting these kinds of recommendations from, right?
[01:22:53.680 --> 01:22:58.680]   Because, you know, I could post only things that are
[01:22:58.680 --> 01:23:02.680]   satirical and be the funniest person in someone's feed because
[01:23:02.680 --> 01:23:06.680]   I post things and my comments are, look at this, it's
[01:23:06.680 --> 01:23:07.680]   ridiculous, right?
[01:23:07.680 --> 01:23:11.680]   And then, so it's just, to me, it seems like there's all these
[01:23:11.680 --> 01:23:15.680]   moments around, you know, thinking about how fake news is
[01:23:15.680 --> 01:23:19.680]   going to be used, going into the midterms or the 2020s.
[01:23:19.680 --> 01:23:24.680]   And so, but in this instance, right, pinning it on users, right?
[01:23:24.680 --> 01:23:28.680]   As if the users are the ones that are responsible for vetting
[01:23:28.680 --> 01:23:33.680]   all of the content and then only posting what is absolutely
[01:23:33.680 --> 01:23:34.680]   true.
[01:23:34.680 --> 01:23:37.680]   It just seems like another way that they're shifting
[01:23:37.680 --> 01:23:43.680]   responsibility for the system that they've built, right?
[01:23:43.680 --> 01:23:45.680]   We'll solve it with code.
[01:23:45.680 --> 01:23:46.680]   That's their attitude.
[01:23:46.680 --> 01:23:47.680]   We'll solve it with code.
[01:23:47.680 --> 01:23:52.680]   And like, who wants only, you know, people who share true
[01:23:52.680 --> 01:23:54.680]   things in their feed anyway, right?
[01:23:54.680 --> 01:23:56.680]   That stuff isn't interesting.
[01:23:56.680 --> 01:24:00.680]   You know, like, I try to want to read a dictionary and
[01:24:00.680 --> 01:24:02.680]   encyclopedia, sure, I will.
[01:24:02.680 --> 01:24:06.680]   But I think, you know, one of the things that our research,
[01:24:06.680 --> 01:24:10.680]   we've learned over the, this last year and a half or so of
[01:24:10.680 --> 01:24:14.680]   dealing with these stories is that people know their false.
[01:24:14.680 --> 01:24:18.680]   They share them anyway because either they cohere to a certain
[01:24:18.680 --> 01:24:22.680]   political belief or they think it's funny or it's outlandish.
[01:24:22.680 --> 01:24:25.680]   And, which is why I think they should really focus on the
[01:24:25.680 --> 01:24:29.680]   extremist stuff, the like, conspiracy stuff and the hate
[01:24:29.680 --> 01:24:34.680]   speech and stop trying to, you know, stop trying to walk through
[01:24:34.680 --> 01:24:36.680]   this gray zone.
[01:24:36.680 --> 01:24:39.680]   And I wonder too, is this really only for people that are
[01:24:39.680 --> 01:24:44.680]   influencers or is it about, you know, like people who have pages?
[01:24:44.680 --> 01:24:47.680]   I mean, we all have a, they limit the amount of friends you can
[01:24:47.680 --> 01:24:48.680]   have.
[01:24:48.680 --> 01:24:50.680]   You can only have 5,000 friends.
[01:24:50.680 --> 01:24:55.680]   And so that's not a ton of impact per individual user.
[01:24:55.680 --> 01:25:00.680]   So I wonder where or how they're going to apply this to those
[01:25:00.680 --> 01:25:04.680]   places where we see this stuff really proliferate, which is in
[01:25:04.680 --> 01:25:06.680]   groups and on pages.
[01:25:06.680 --> 01:25:10.680]   And so, you know, I just feel like blaming the user just isn't,
[01:25:10.680 --> 01:25:13.680]   it's not going to go very far.
[01:25:13.680 --> 01:25:15.680]   And that is every tech company strategy though.
[01:25:15.680 --> 01:25:16.680]   Privacy, blame the user.
[01:25:16.680 --> 01:25:18.680]   Security, blame the user.
[01:25:18.680 --> 01:25:20.680]   You ought to, you ought to have known better.
[01:25:20.680 --> 01:25:23.680]   So the Washington Post interviewed Tessa Lyons, who they
[01:25:23.680 --> 01:25:27.680]   identify as the product manager who is in charge of fighting
[01:25:27.680 --> 01:25:28.680]   misinformation.
[01:25:28.680 --> 01:25:31.680]   And as she says, this came about because, you know, they had
[01:25:31.680 --> 01:25:35.680]   this reporting system, right, where as you're, as you're reading
[01:25:35.680 --> 01:25:39.680]   a post, this started a couple of years ago, you could, there was a
[01:25:39.680 --> 01:25:42.680]   tab in the upper right hand corner that said, is this, you know,
[01:25:42.680 --> 01:25:44.680]   problematic content.
[01:25:44.680 --> 01:25:48.680]   But Lyons says she soon realized many people were reporting
[01:25:48.680 --> 01:25:51.680]   posts as false, not because they were false because they didn't
[01:25:51.680 --> 01:25:52.680]   agree with it.
[01:25:52.680 --> 01:25:57.680]   And so they, that said, then they had to step back and say, okay,
[01:25:57.680 --> 01:26:00.680]   now we got to figure out who we're getting the signal from and
[01:26:00.680 --> 01:26:03.680]   whether we can trust them when they say it's false.
[01:26:03.680 --> 01:26:06.680]   So they're trying to assess whether the people who are flagging
[01:26:06.680 --> 01:26:10.680]   posts as false are themselves trustworthy.
[01:26:10.680 --> 01:26:15.680]   Instead of just looking to see if the, so the idea is they don't
[01:26:15.680 --> 01:26:18.680]   want as many, they don't want to have to get as many articles that
[01:26:18.680 --> 01:26:20.680]   they then have to look at.
[01:26:20.680 --> 01:26:22.680]   She says, I like to make the joke.
[01:26:22.680 --> 01:26:24.680]   If people only reported things that were false, this job would be
[01:26:24.680 --> 01:26:25.680]   easy.
[01:26:25.680 --> 01:26:29.680]   People often report things they just disagree with.
[01:26:29.680 --> 01:26:34.680]   So now they're, that's why they're giving you a trust score so
[01:26:34.680 --> 01:26:37.680]   that if you're trustworthy and you say it's false, they're, they're
[01:26:37.680 --> 01:26:39.680]   going to take a look at it.
[01:26:39.680 --> 01:26:44.680]   We've seen similar systems like this deployed around, you know,
[01:26:44.680 --> 01:26:45.680]   trusted flaggers.
[01:26:45.680 --> 01:26:48.680]   It's hard to do.
[01:26:48.680 --> 01:26:50.680]   It's hard to do.
[01:26:50.680 --> 01:26:54.680]   And also, you know, people flag for multiple different kinds of
[01:26:54.680 --> 01:26:55.680]   reasons.
[01:26:55.680 --> 01:26:59.680]   And yeah, I would say that, you know, sure, I disagree with things
[01:26:59.680 --> 01:27:01.680]   and some things are partially true.
[01:27:01.680 --> 01:27:04.680]   That's the other thing that's been so difficult about talking
[01:27:04.680 --> 01:27:08.680]   with these companies about conspiracy theories, which is there's
[01:27:08.680 --> 01:27:13.680]   always elements of the theory that are true, that are verifiable,
[01:27:13.680 --> 01:27:17.680]   that circulate with the stuff that is false.
[01:27:17.680 --> 01:27:21.680]   You know, and it's sort of like decoding, you know, it's like
[01:27:21.680 --> 01:27:24.680]   living in the X files in some instances where we're like, yeah,
[01:27:24.680 --> 01:27:27.680]   there are clues and there are things that we can point to and
[01:27:27.680 --> 01:27:32.680]   say, yeah, this thing happened, but maybe the grays didn't land
[01:27:32.680 --> 01:27:34.680]   specifically in New Hampshire.
[01:27:34.680 --> 01:27:39.680]   So, you know, it's just, it's hard to understand why the user
[01:27:39.680 --> 01:27:44.680]   then would become the thing that they want to assess other
[01:27:44.680 --> 01:27:49.680]   than to say that what it'll help them do is downrank certain
[01:27:49.680 --> 01:27:55.680]   people's content so that everybody's uncle Larry, who every
[01:27:55.680 --> 01:27:59.680]   family already knows is untrustable, is then seen as
[01:27:59.680 --> 01:28:02.680]   untrustable by the algorithm and therefore their content doesn't
[01:28:02.680 --> 01:28:07.680]   get seen or shared beyond their own page.
[01:28:07.680 --> 01:28:09.680]   And so I think that--
[01:28:09.680 --> 01:28:11.680]   It's like a self-driving car.
[01:28:11.680 --> 01:28:14.680]   It would be so much easier if they just weren't any humans
[01:28:14.680 --> 01:28:15.680]   around.
[01:28:15.680 --> 01:28:19.680]   Yeah, and that's the thing is though, is they depend so much.
[01:28:19.680 --> 01:28:22.680]   You have to understand these systems are really like, they're
[01:28:22.680 --> 01:28:24.680]   like libraries without books, right?
[01:28:24.680 --> 01:28:26.680]   They're just shelves for things.
[01:28:26.680 --> 01:28:28.680]   That's what they've built is infrastructure.
[01:28:28.680 --> 01:28:32.680]   And so without the users putting things on the shelves and
[01:28:32.680 --> 01:28:38.680]   cheering them and renting them out, there's nothing there, right?
[01:28:38.680 --> 01:28:39.680]   Yeah.
[01:28:39.680 --> 01:28:41.680]   That's a very good analogy.
[01:28:41.680 --> 01:28:43.680]   I get that, yeah.
[01:28:43.680 --> 01:28:49.680]   Yeah, and so ultimately they need to have some measure of good
[01:28:49.680 --> 01:28:56.680]   patronage, but, you know, rating every single user based on what
[01:28:56.680 --> 01:28:58.680]   they flag.
[01:28:58.680 --> 01:29:02.680]   You know, ultimately I think that they're--
[01:29:02.680 --> 01:29:05.680]   One of the things that I think we're going to see over the next
[01:29:05.680 --> 01:29:09.680]   ten years is that we're going to see less and less user-generated
[01:29:09.680 --> 01:29:12.680]   content on these platforms and more and more.
[01:29:12.680 --> 01:29:14.680]   You know, you're allowed to circulate things that have been
[01:29:14.680 --> 01:29:16.680]   verified and vetted.
[01:29:16.680 --> 01:29:21.680]   Which will then open up new markets for things, right?
[01:29:21.680 --> 01:29:23.680]   You'll see people turning.
[01:29:23.680 --> 01:29:26.680]   You can see the temptation if you built this library with just
[01:29:26.680 --> 01:29:27.680]   shelves.
[01:29:27.680 --> 01:29:30.680]   The first thing you're going to want is come on in, everybody,
[01:29:30.680 --> 01:29:31.680]   just fill it up.
[01:29:31.680 --> 01:29:32.680]   Yeah.
[01:29:32.680 --> 01:29:34.680]   We don't care what you put in there.
[01:29:34.680 --> 01:29:35.680]   Just put it in there.
[01:29:35.680 --> 01:29:38.680]   And I think it's not unreasonable if I put myself in
[01:29:38.680 --> 01:29:41.680]   Mark Zuckerberg's shoes to say, well, you know, put everything
[01:29:41.680 --> 01:29:42.680]   in there.
[01:29:42.680 --> 01:29:45.680]   And it's up to the user to figure out if it's good or bad.
[01:29:45.680 --> 01:29:47.680]   That's not our job.
[01:29:47.680 --> 01:29:52.680]   But then you get all this heat on you because your system inadvertently
[01:29:52.680 --> 01:29:55.680]   allowed all sorts of bad things to happen.
[01:29:55.680 --> 01:29:57.680]   I don't know what it is.
[01:29:57.680 --> 01:30:00.680]   But so now they're saying, well, now we want a police was on
[01:30:00.680 --> 01:30:01.680]   the shelves.
[01:30:01.680 --> 01:30:06.680]   But it's hard to retroactively do that.
[01:30:06.680 --> 01:30:07.680]   Yeah.
[01:30:07.680 --> 01:30:10.680]   And that's where I think also the pace of technology is an
[01:30:10.680 --> 01:30:14.680]   interesting question, which is that we moved so quickly from
[01:30:14.680 --> 01:30:21.680]   being able to upload one picture, one song, one ten second video
[01:30:21.680 --> 01:30:29.680]   to just full blown streaming HD content from people's bedroom.
[01:30:29.680 --> 01:30:34.680]   And in that massive technological change, there was very little
[01:30:34.680 --> 01:30:38.680]   regulation, there was very little foresight, there was very
[01:30:38.680 --> 01:30:40.680]   little, you know,
[01:30:40.680 --> 01:30:41.680]   It's turned on the closet.
[01:30:41.680 --> 01:30:42.680]   Yeah.
[01:30:42.680 --> 01:30:43.680]   So come on.
[01:30:43.680 --> 01:30:47.680]   And so every, you know, now everyone isn't just, you know,
[01:30:47.680 --> 01:30:51.680]   their own printing press, they're their own, you know, author.
[01:30:51.680 --> 01:30:55.680]   And it's really hard because people now have shifted into thinking
[01:30:55.680 --> 01:30:59.680]   of themselves as, you know, citizen journalists and documentarians.
[01:30:59.680 --> 01:31:01.680]   That's a tear that you.
[01:31:01.680 --> 01:31:02.680]   Professional positions.
[01:31:02.680 --> 01:31:08.680]   And so everybody feels a right to the Internet and they feel a
[01:31:08.680 --> 01:31:12.680]   right to broadcast and they feel a right to amplification.
[01:31:12.680 --> 01:31:17.680]   But the downstream effects of that on democracy, for instance,
[01:31:17.680 --> 01:31:21.680]   are, you know, that's what we're seeing here is that when everybody
[01:31:21.680 --> 01:31:25.680]   can do political advertising, then it becomes really hard to
[01:31:25.680 --> 01:31:28.680]   assess what the issues are and who to trust.
[01:31:28.680 --> 01:31:34.680]   This is a really interesting conundrum.
[01:31:34.680 --> 01:31:35.680]   Yeah.
[01:31:35.680 --> 01:31:38.680]   No, that's what you get into research, Leo.
[01:31:38.680 --> 01:31:39.680]   There's plenty to do.
[01:31:39.680 --> 01:31:43.680]   I feel that I fear that any solution that you apply to this is
[01:31:43.680 --> 01:31:47.680]   going to have a worse outcome than the problem itself.
[01:31:47.680 --> 01:31:50.680]   I don't know because I, you know, I still really like the Internet.
[01:31:50.680 --> 01:31:55.680]   There's still things that are useful on web pages and, and like
[01:31:55.680 --> 01:31:58.680]   even just the ability to be able to, you know, talk to you and
[01:31:58.680 --> 01:31:59.680]   stay see today.
[01:31:59.680 --> 01:32:04.680]   I mean, these are all positive, I think, outcomes of like having
[01:32:04.680 --> 01:32:08.680]   access to these technologies, but there has to be a better
[01:32:08.680 --> 01:32:09.680]   governance model.
[01:32:09.680 --> 01:32:11.680]   There has to be some management.
[01:32:11.680 --> 01:32:16.680]   There has to be, you know, when we can't reduce it all to technology.
[01:32:16.680 --> 01:32:19.680]   No, I don't deny the value of it.
[01:32:19.680 --> 01:32:21.680]   It's certainly worth saving.
[01:32:21.680 --> 01:32:26.680]   But I, when, as soon as you say, there has to be some governance,
[01:32:26.680 --> 01:32:29.680]   that's very difficult to do.
[01:32:29.680 --> 01:32:30.680]   Oh, yeah.
[01:32:30.680 --> 01:32:31.680]   People freak out.
[01:32:31.680 --> 01:32:35.680]   And I don't, and I don't know how you do it without actually
[01:32:35.680 --> 01:32:39.680]   creating a worse monster is all I'm saying.
[01:32:39.680 --> 01:32:40.680]   Yeah.
[01:32:40.680 --> 01:32:41.680]   We just talked about that.
[01:32:41.680 --> 01:32:44.680]   If you do this, this trust thing, well, then there's all these
[01:32:44.680 --> 01:32:45.680]   other issues that come up.
[01:32:45.680 --> 01:32:49.680]   Part of the problem is that we set up the Internet without really
[01:32:49.680 --> 01:32:51.680]   considering the consequences.
[01:32:51.680 --> 01:32:53.680]   Now we've got consequences.
[01:32:53.680 --> 01:32:56.680]   So now we're rushing to set up some solution without considering
[01:32:56.680 --> 01:33:00.680]   the consequences, which is going to give us worse consequences.
[01:33:00.680 --> 01:33:01.680]   Yeah.
[01:33:01.680 --> 01:33:06.680]   And, but that's part of why you would want to, you know, bracket some
[01:33:06.680 --> 01:33:10.680]   of these concerns and work on things.
[01:33:10.680 --> 01:33:14.680]   You know, some, some of these are easier to solve than others.
[01:33:14.680 --> 01:33:17.680]   Like for instance, the ad tech problem that we've identified and
[01:33:17.680 --> 01:33:21.680]   we know that, you know, bad actors online have been using
[01:33:21.680 --> 01:33:25.680]   advertising technology in order to force audiences to see
[01:33:25.680 --> 01:33:26.680]   disinformation.
[01:33:26.680 --> 01:33:31.680]   That can be solved, but the revenue cost to these platform
[01:33:31.680 --> 01:33:34.680]   companies would be enormous because they would have to check
[01:33:34.680 --> 01:33:35.680]   every ad.
[01:33:35.680 --> 01:33:39.680]   You know, so there are bits and pieces of this that can be
[01:33:39.680 --> 01:33:42.680]   dealt with and can be moderated.
[01:33:42.680 --> 01:33:48.680]   But there are other parts of it that just feel so overwhelming
[01:33:48.680 --> 01:33:53.680]   when, you know, when you, when you take it all as, as, you know,
[01:33:53.680 --> 01:33:54.680]   one big problem.
[01:33:54.680 --> 01:33:58.680]   I, I'm, I'm for disaggregating things and then also starting to
[01:33:58.680 --> 01:34:02.680]   understand where there needs to be nuance and then where there's,
[01:34:02.680 --> 01:34:09.680]   you know, room for these platform companies to use some of their
[01:34:09.680 --> 01:34:15.680]   revenue in order to either bolster local journalism or to invest
[01:34:15.680 --> 01:34:21.680]   in more robust content moderation or even teach people how to use
[01:34:21.680 --> 01:34:25.680]   these systems and, you know, some of the things that I've seen
[01:34:25.680 --> 01:34:30.680]   where people do pro, share, you know, copious amounts of fake
[01:34:30.680 --> 01:34:32.680]   news has to do with it.
[01:34:32.680 --> 01:34:35.680]   They, they don't think anybody else is seeing what they're
[01:34:35.680 --> 01:34:36.680]   posting.
[01:34:36.680 --> 01:34:38.680]   And so they just keep posting more and more and more things
[01:34:38.680 --> 01:34:40.680]   because nobody's engaging with it.
[01:34:40.680 --> 01:34:46.680]   And so even learning how to use these platforms and having limits
[01:34:46.680 --> 01:34:48.680]   would be useful for some.
[01:34:48.680 --> 01:34:51.680]   Let's take a break.
[01:34:51.680 --> 01:34:52.680]   I'm exhausted.
[01:34:52.680 --> 01:34:53.680]   Michael.
[01:34:53.680 --> 01:34:54.680]   Okay.
[01:34:54.680 --> 01:34:58.680]   It just often seems so daunting to me that I just, I just want to
[01:34:58.680 --> 01:35:00.680]   move to an island and give up.
[01:35:00.680 --> 01:35:01.680]   I know we can't.
[01:35:01.680 --> 01:35:02.680]   No, you can't.
[01:35:02.680 --> 01:35:07.680]   Well, you can't go without your, you know, your display speaker
[01:35:07.680 --> 01:35:10.680]   and you don't want to go there.
[01:35:10.680 --> 01:35:11.680]   Come on.
[01:35:11.680 --> 01:35:15.680]   You still need access to movies and good, good to be.
[01:35:15.680 --> 01:35:18.680]   Completely unfeasible to say, let's just not mess with it all
[01:35:18.680 --> 01:35:20.680]   and just let's see what happens.
[01:35:20.680 --> 01:35:23.680]   Or if we already know what happens.
[01:35:23.680 --> 01:35:26.680]   I think, yeah, I mean, it's a funny question, right?
[01:35:26.680 --> 01:35:32.680]   I mean, eventually, like you would imagine that it, that what
[01:35:32.680 --> 01:35:35.680]   I see happening is actually what you've done, which is that
[01:35:35.680 --> 01:35:37.680]   good people leave, right?
[01:35:37.680 --> 01:35:41.680]   And so you end up like the bad forces out the good, you know,
[01:35:41.680 --> 01:35:46.680]   we've seen that over and over again in social networks online.
[01:35:46.680 --> 01:35:50.680]   If you don't tend to your garden, the weeds will push out all
[01:35:50.680 --> 01:35:53.680]   the flowers and all you'll have is crap.
[01:35:53.680 --> 01:35:54.680]   Yeah.
[01:35:54.680 --> 01:36:00.680]   And this, I think we've seen and shed other social media as we've,
[01:36:00.680 --> 01:36:03.680]   you know, moved through different cycles of the Internet.
[01:36:03.680 --> 01:36:07.680]   And I think that you're one of many people that are having this
[01:36:07.680 --> 01:36:08.680]   reckoning.
[01:36:08.680 --> 01:36:10.680]   I think this is happening right now.
[01:36:10.680 --> 01:36:11.680]   I think it's happening.
[01:36:11.680 --> 01:36:13.680]   I think there's an earthquake going on right now.
[01:36:13.680 --> 01:36:15.680]   And then people are moving out.
[01:36:15.680 --> 01:36:17.680]   Yeah, they definitely are.
[01:36:17.680 --> 01:36:20.680]   And they're not just leaving, though, but they're also, you know,
[01:36:20.680 --> 01:36:22.680]   a lot of people are trying new things.
[01:36:22.680 --> 01:36:24.680]   They're trying smaller systems.
[01:36:24.680 --> 01:36:26.680]   They're more interested in privacy.
[01:36:26.680 --> 01:36:29.680]   They're more interested in things that disappear online.
[01:36:29.680 --> 01:36:33.680]   They're much more cognizant of what it means to put up information
[01:36:33.680 --> 01:36:34.680]   online.
[01:36:34.680 --> 01:36:37.680]   You know, younger people are definitely at the forefront of this.
[01:36:37.680 --> 01:36:40.680]   I'm using Mastodon more.
[01:36:40.680 --> 01:36:41.680]   Yeah.
[01:36:41.680 --> 01:36:42.680]   Are you really interested?
[01:36:42.680 --> 01:36:43.680]   Okay.
[01:36:43.680 --> 01:36:45.680]   I was going to ask, because why did that story?
[01:36:45.680 --> 01:36:48.680]   And I was like, oh, I know that we talked about it for a while ago.
[01:36:48.680 --> 01:36:51.680]   Yeah, I have an account lying around it.
[01:36:51.680 --> 01:36:53.680]   So actually, it's one account lying around.
[01:36:53.680 --> 01:36:55.680]   It was on an instance that is gone.
[01:36:55.680 --> 01:36:59.680]   So fortunately, I had a backup account on Mastodon.social,
[01:36:59.680 --> 01:37:02.680]   which you can't join now, I think, because it's just so big.
[01:37:02.680 --> 01:37:04.680]   Let me log in.
[01:37:04.680 --> 01:37:06.680]   I don't know if I can log in quickly here.
[01:37:06.680 --> 01:37:15.680]   My suspicion is it's only good because nobody's using it.
[01:37:15.680 --> 01:37:19.680]   And that as soon as it becomes as popular as Twitter, it will have
[01:37:19.680 --> 01:37:21.680]   all the problems Twitter has.
[01:37:21.680 --> 01:37:23.680]   Although it's structurally somewhat different.
[01:37:23.680 --> 01:37:25.680]   It's a federated social network.
[01:37:25.680 --> 01:37:27.680]   So there are many, many different servers.
[01:37:27.680 --> 01:37:31.680]   And whoever runs the server is responsible for the rules.
[01:37:31.680 --> 01:37:34.680]   You can say, well, I don't have any Nazis online.
[01:37:34.680 --> 01:37:36.680]   In fact, most of them say that.
[01:37:36.680 --> 01:37:38.680]   There are probably Nazi instances.
[01:37:38.680 --> 01:37:40.680]   I was about to say somewhere.
[01:37:40.680 --> 01:37:41.680]   Well, they probably are.
[01:37:41.680 --> 01:37:45.680]   But what happens is the other instances don't federate with them.
[01:37:45.680 --> 01:37:51.680]   So it effectively, it allows it, but it keeps it out of my feed,
[01:37:51.680 --> 01:37:53.680]   which is, you know.
[01:37:53.680 --> 01:37:59.680]   If they do that in secret, is that?
[01:37:59.680 --> 01:38:00.680]   Well, that's the other thing.
[01:38:00.680 --> 01:38:03.680]   We've talked about this before that I always thought you lift the rock
[01:38:03.680 --> 01:38:05.680]   up and you let sunlight disinfect.
[01:38:05.680 --> 01:38:07.680]   And that's how you cure this stuff.
[01:38:07.680 --> 01:38:09.680]   But it's become clear.
[01:38:09.680 --> 01:38:13.680]   And thanks to researchers like Joan, that in fact,
[01:38:13.680 --> 01:38:14.680]   sunlight does not disinfect.
[01:38:14.680 --> 01:38:17.680]   It just spreads the word.
[01:38:17.680 --> 01:38:21.680]   Yeah, it's definitely something that we've, you know,
[01:38:21.680 --> 01:38:25.680]   when people are operating bot accounts and, you know,
[01:38:25.680 --> 01:38:29.680]   using social media amplification tools and SEO strategies,
[01:38:29.680 --> 01:38:33.680]   they're just so much louder than any other rational voice.
[01:38:33.680 --> 01:38:36.680]   And so it's not the same relationship as you would have to
[01:38:36.680 --> 01:38:40.680]   speech in, say, a public space.
[01:38:40.680 --> 01:38:43.680]   So there is a lot to be, and there's also a lot to be gained from
[01:38:43.680 --> 01:38:46.680]   thinking about the design of Mastodon itself, which kind of
[01:38:46.680 --> 01:38:51.680]   echoes the early Internet around decentralization,
[01:38:51.680 --> 01:38:56.680]   people, you know, have an issue based interests where they, you know,
[01:38:56.680 --> 01:39:00.680]   that the name of the chat room or the name of the service describes
[01:39:00.680 --> 01:39:03.680]   the thing that you're interested in or the subculture.
[01:39:03.680 --> 01:39:07.680]   Well, you know, I'll give you a really interesting example that
[01:39:07.680 --> 01:39:09.680]   this might be a little controversial.
[01:39:09.680 --> 01:39:17.680]   But sex workers were forced off by FOSTA and SESTA public forums.
[01:39:17.680 --> 01:39:23.680]   But they say we have a right to, it's in our interest and it's in
[01:39:23.680 --> 01:39:27.680]   our safety to have a place where we can be and find clients and
[01:39:27.680 --> 01:39:28.680]   clients can find us.
[01:39:28.680 --> 01:39:32.680]   So there is a sex worker Mastodon.
[01:39:32.680 --> 01:39:35.680]   Now, if I don't want to see that, it's a trivial thing for me to say,
[01:39:35.680 --> 01:39:37.680]   well, I just don't want to see any traffic from there.
[01:39:37.680 --> 01:39:41.680]   So it solves the problem, I think, in a really interesting way,
[01:39:41.680 --> 01:39:45.680]   because it can exist some Mastodon instances confederate to it,
[01:39:45.680 --> 01:39:48.680]   but you don't have to see it if you don't want to.
[01:39:48.680 --> 01:39:52.680]   It has very good controls for that kind of thing.
[01:39:52.680 --> 01:39:56.680]   It doesn't allow quoted retweets.
[01:39:56.680 --> 01:39:59.680]   It does allow retweets, they call it boosts.
[01:39:59.680 --> 01:40:02.680]   And I think retweets are part of the problem on Twitter because they
[01:40:02.680 --> 01:40:04.680]   amplify the outrage.
[01:40:04.680 --> 01:40:10.680]   I'm not, you know, I think it's a really interesting experiment because
[01:40:10.680 --> 01:40:13.680]   is it possible to design structurally something that will be less
[01:40:13.680 --> 01:40:17.680]   prone to the problems that Facebook and Twitter and others are prone to?
[01:40:17.680 --> 01:40:20.680]   Or does everything in the long run just become like Twitter?
[01:40:20.680 --> 01:40:21.680]   I don't know.
[01:40:21.680 --> 01:40:24.680]   If you look at it, it looks just like Twitter.
[01:40:24.680 --> 01:40:27.680]   So what you're seeing here, this is the instance I'm on, or actually,
[01:40:27.680 --> 01:40:28.680]   no, this is my feed.
[01:40:28.680 --> 01:40:30.680]   So these are people I'm following.
[01:40:30.680 --> 01:40:33.680]   This is the instance I'm on the local timeline.
[01:40:33.680 --> 01:40:36.680]   So this is everything going on on that server.
[01:40:36.680 --> 01:40:40.680]   What I don't have turned on, I'll turn it on briefly, is the federated
[01:40:40.680 --> 01:40:41.680]   timeline.
[01:40:41.680 --> 01:40:42.680]   That's everything.
[01:40:42.680 --> 01:40:47.680]   And so there's a little more stuff on here that you might not want to see.
[01:40:47.680 --> 01:40:50.680]   Notice there's another feature which I think is really, really
[01:40:50.680 --> 01:40:51.680]   interesting.
[01:40:51.680 --> 01:40:53.680]   Sensitive content.
[01:40:53.680 --> 01:40:58.680]   And there's a, when you post something, when you toot, as they call it,
[01:40:58.680 --> 01:41:04.680]   you can press the CW button, which is a content warning.
[01:41:04.680 --> 01:41:05.680]   And you can write your warning.
[01:41:05.680 --> 01:41:10.680]   I posted, for instance, a toot about the meat meal, meat-based meal I
[01:41:10.680 --> 01:41:11.680]   made yesterday.
[01:41:11.680 --> 01:41:16.680]   And I, in my content, I hid the content and I put warning, you know,
[01:41:16.680 --> 01:41:19.680]   non-vegetarian content, your meat.
[01:41:19.680 --> 01:41:26.680]   So if somebody's bothered by that, if people responsibly use it,
[01:41:26.680 --> 01:41:28.680]   that also has a very nice impact.
[01:41:28.680 --> 01:41:29.680]   There's very little-
[01:41:29.680 --> 01:41:31.680]   Is it that that was a responsible use?
[01:41:31.680 --> 01:41:32.680]   I'm just curious.
[01:41:32.680 --> 01:41:33.680]   I think it was.
[01:41:33.680 --> 01:41:36.680]   I put cooking, calling meat so that, because I was going to talk about
[01:41:36.680 --> 01:41:38.680]   ways to cook an animal.
[01:41:38.680 --> 01:41:41.680]   And I wanted to be sensitive to the fact that some people may not want to
[01:41:41.680 --> 01:41:42.680]   see that.
[01:41:42.680 --> 01:41:45.680]   And there is right now anyway, culturally, and, and mast it on that
[01:41:45.680 --> 01:41:46.680]   kind of thing.
[01:41:46.680 --> 01:41:49.680]   People, if they talk about Twitter, for instance, that becomes a content
[01:41:49.680 --> 01:41:50.680]   warning.
[01:41:50.680 --> 01:41:52.680]   We're going to talk about bird sight.
[01:41:52.680 --> 01:41:57.680]   If, if it's anything that could be triggering, if, you know, I think
[01:41:57.680 --> 01:41:58.680]   that's really interesting.
[01:41:58.680 --> 01:42:01.680]   People are very thoughtful about the other people on there.
[01:42:01.680 --> 01:42:05.680]   And then because it's federated, the run-it version runs the instance
[01:42:05.680 --> 01:42:07.680]   really gets to set whatever rules they want.
[01:42:07.680 --> 01:42:12.680]   And there's also then that generates a culture, right, that one hopes
[01:42:12.680 --> 01:42:15.680]   persists.
[01:42:15.680 --> 01:42:16.680]   I don't know.
[01:42:16.680 --> 01:42:20.680]   I, I, I think it's a very, we've talked about it before.
[01:42:20.680 --> 01:42:21.680]   It was very hot for a while.
[01:42:21.680 --> 01:42:22.680]   It's kind of gone down.
[01:42:22.680 --> 01:42:26.680]   Oddly enough, without much publicity, it's coming back now.
[01:42:26.680 --> 01:42:29.680]   And I think that that, what I sense is that's people fleeing
[01:42:29.680 --> 01:42:30.680]   Twitter.
[01:42:30.680 --> 01:42:31.680]   Yeah.
[01:42:31.680 --> 01:42:34.680]   And the other thing that's probably going to be the tail of the tape is
[01:42:34.680 --> 01:42:39.680]   how good the mobile app is and how much the mobile app can incorporate
[01:42:39.680 --> 01:42:42.680]   the user features that people want.
[01:42:42.680 --> 01:42:43.680]   Well, here's an interesting thing.
[01:42:43.680 --> 01:42:47.680]   It's, it's, it's, it's, there is no official mobile app.
[01:42:47.680 --> 01:42:51.680]   All the mobile, there are many third party mobile apps because it's got
[01:42:51.680 --> 01:42:52.680]   an API.
[01:42:52.680 --> 01:42:56.680]   So you can choose a mobile app that lets you do what you want to do.
[01:42:56.680 --> 01:42:58.680]   Different mobile apps have different features.
[01:42:58.680 --> 01:42:59.680]   Mm-hmm.
[01:42:59.680 --> 01:43:01.680]   Isn't that interesting?
[01:43:01.680 --> 01:43:02.680]   Yeah.
[01:43:02.680 --> 01:43:04.680]   I mean, this might be the wave of the future, right?
[01:43:04.680 --> 01:43:07.680]   I mean, around podcasts and things, you know, it's been really
[01:43:07.680 --> 01:43:12.680]   helpful to have multiple different podcast apps to be able to run, you know,
[01:43:12.680 --> 01:43:16.680]   whatever way in which you want to download and sort your content.
[01:43:16.680 --> 01:43:20.680]   And so it might just be that you might use one instance, you might use
[01:43:20.680 --> 01:43:24.680]   one app for Mastodon for, you know, your sex work channels.
[01:43:24.680 --> 01:43:28.680]   And then you've got another app running where you have other channels.
[01:43:28.680 --> 01:43:29.680]   Right.
[01:43:29.680 --> 01:43:36.680]   You're actually is a, the one I use on iOS, the app that I use on iOS has
[01:43:36.680 --> 01:43:38.680]   exactly that feature.
[01:43:38.680 --> 01:43:39.680]   Let me see what it's called.
[01:43:39.680 --> 01:43:41.680]   It's called a toot don.
[01:43:41.680 --> 01:43:45.680]   And you can say, I want to follow this instance.
[01:43:45.680 --> 01:43:48.680]   You can actually have a timeline that's just the instance, that instance.
[01:43:48.680 --> 01:43:51.680]   So if you wanted to follow Nazis specifically, you could.
[01:43:51.680 --> 01:43:52.680]   Yeah.
[01:43:52.680 --> 01:43:54.680]   And if you didn't want to, you wouldn't have to ever.
[01:43:54.680 --> 01:43:55.680]   You could block it.
[01:43:55.680 --> 01:44:00.680]   So I'm, so this is an example, isn't it?
[01:44:00.680 --> 01:44:04.680]   But I think it's very hard to retrofit Twitter or Facebook.
[01:44:04.680 --> 01:44:06.680]   I mean, that's exactly what these companies are facing.
[01:44:06.680 --> 01:44:08.680]   It's very hard to retrofit.
[01:44:08.680 --> 01:44:11.680]   But I think if you thought and you looked and carefully thought about it,
[01:44:11.680 --> 01:44:16.680]   read all of Jones postings on data and society, and then said, well,
[01:44:16.680 --> 01:44:18.680]   what could we make that might solve some of those problems?
[01:44:18.680 --> 01:44:21.680]   I think you, it might be possible to create something that didn't have
[01:44:21.680 --> 01:44:22.680]   those same problems.
[01:44:22.680 --> 01:44:25.680]   I hope that that's the case.
[01:44:25.680 --> 01:44:26.680]   Yeah.
[01:44:26.680 --> 01:44:30.680]   I think you're, I mean, you're the best user, like, you know, test case in this
[01:44:30.680 --> 01:44:34.680]   an area because, you know, so much about how the things work, but also
[01:44:34.680 --> 01:44:40.680]   you're invested in having a stream of information that mirrors the way
[01:44:40.680 --> 01:44:42.680]   that you want to live in this world, right?
[01:44:42.680 --> 01:44:43.680]   Right.
[01:44:43.680 --> 01:44:44.680]   And so many of them are.
[01:44:44.680 --> 01:44:45.680]   I do.
[01:44:45.680 --> 01:44:47.680]   There is valuable stuff in the sewage of Twitter, and I would love to get
[01:44:47.680 --> 01:44:48.680]   that over here.
[01:44:48.680 --> 01:44:49.680]   Mm hmm.
[01:44:49.680 --> 01:44:53.680]   I, I, I despair on being able to create any sort of sewage treatment
[01:44:53.680 --> 01:44:54.680]   system on Twitter.
[01:44:54.680 --> 01:44:56.680]   That, that's not going to happen, right?
[01:44:56.680 --> 01:45:01.680]   They're trying.
[01:45:01.680 --> 01:45:04.680]   In the public health conversation, public or healthy conversations.
[01:45:04.680 --> 01:45:06.680]   Do you feel like they are?
[01:45:06.680 --> 01:45:10.680]   Or that, or that, I mean, I mean, there's a lot of research being done.
[01:45:10.680 --> 01:45:13.680]   I, I'll be interested to see how it shows up in features.
[01:45:13.680 --> 01:45:17.680]   You know, there's a lot of this talk about shadow banning and what that
[01:45:17.680 --> 01:45:18.680]   actually looks like.
[01:45:18.680 --> 01:45:22.680]   And you've seen some modifications to the replies so that replies that
[01:45:22.680 --> 01:45:26.680]   have certain words in them are hidden behind.
[01:45:26.680 --> 01:45:29.680]   You know, you have to click on this show me more button.
[01:45:29.680 --> 01:45:34.680]   So there, there's a sort of, you know, there's a concerted attempt
[01:45:34.680 --> 01:45:36.680]   to change the interface.
[01:45:36.680 --> 01:45:42.680]   But I mean, so much of what is terrible is just stuff we already know
[01:45:42.680 --> 01:45:43.680]   about.
[01:45:43.680 --> 01:45:44.680]   Right.
[01:45:44.680 --> 01:45:45.680]   Right.
[01:45:45.680 --> 01:45:49.680]   So unless they're willing to not just moderate content, but also think
[01:45:49.680 --> 01:45:53.680]   about certain users differently or not treat celebrities.
[01:45:53.680 --> 01:45:54.680]   Badly.
[01:45:54.680 --> 01:45:59.680]   You know, differently, you know, like, then, you know, things could
[01:45:59.680 --> 01:46:00.680]   change.
[01:46:00.680 --> 01:46:05.680]   I was so sad when I read the New York Times editorial by the woman
[01:46:05.680 --> 01:46:07.680]   who was in Star Wars.
[01:46:07.680 --> 01:46:08.680]   I can't remember.
[01:46:08.680 --> 01:46:09.680]   Oh, yeah.
[01:46:09.680 --> 01:46:13.680]   And how she, she had to basically flee social media because she was
[01:46:13.680 --> 01:46:16.680]   so mistreated because she was an Asian woman.
[01:46:16.680 --> 01:46:19.680]   Yeah.
[01:46:19.680 --> 01:46:24.680]   I mean, harassment online has been, you know, at the same moment
[01:46:24.680 --> 01:46:29.680]   that we're seeing women really step into the four of different
[01:46:29.680 --> 01:46:34.680]   industries becoming CEOs, like stepping into male dominated
[01:46:34.680 --> 01:46:35.680]   markets around gaming.
[01:46:35.680 --> 01:46:39.680]   We're seeing more and more harassment online.
[01:46:39.680 --> 01:46:43.680]   The top, it's happening in public, right?
[01:46:43.680 --> 01:46:47.680]   You know, it used to be you'd get harassed and show up in your
[01:46:47.680 --> 01:46:50.680]   email or maybe your voicemail would get attacked.
[01:46:50.680 --> 01:46:54.680]   But now it's just flagrant and it's so visible.
[01:46:54.680 --> 01:46:59.680]   And part of that is the strategy of letting other people know that
[01:46:59.680 --> 01:47:04.680]   follow that person, that, you know, that the harassment is happening
[01:47:04.680 --> 01:47:07.680]   and that if they were to speak up, they would get harassed too.
[01:47:07.680 --> 01:47:08.680]   Yeah.
[01:47:08.680 --> 01:47:11.680]   And so, you know, that public, the public harassment and the public
[01:47:11.680 --> 01:47:14.680]   shaming is something that these platform companies, especially
[01:47:14.680 --> 01:47:21.680]   Twitter, need to act quickly, but also very decisively, right?
[01:47:21.680 --> 01:47:24.680]   And it would make people think twice if they thought they were
[01:47:24.680 --> 01:47:28.680]   going to lose their account forever or that it was going to be
[01:47:28.680 --> 01:47:34.680]   harder and harder for them to, you know, stay online or, you know,
[01:47:34.680 --> 01:47:37.680]   if there were cross platform policies where you didn't just
[01:47:37.680 --> 01:47:38.680]   lose your Twitter account.
[01:47:38.680 --> 01:47:40.680]   Maybe you lost your medium account as well or you lost your
[01:47:40.680 --> 01:47:45.680]   YouTube channel and you wouldn't keep on doing this.
[01:47:45.680 --> 01:47:47.680]   You wouldn't behave in this way.
[01:47:47.680 --> 01:47:49.680]   If you want to understand this a little better, read the
[01:47:49.680 --> 01:47:51.680]   New York Times opinion piece by Kelly Marie Tran.
[01:47:51.680 --> 01:47:55.680]   I won't be marginalized by online harassment.
[01:47:55.680 --> 01:47:58.680]   She talks about her experience.
[01:47:58.680 --> 01:48:01.680]   She says, you might know at the end, she says, you might know me as
[01:48:01.680 --> 01:48:02.680]   Kelly.
[01:48:02.680 --> 01:48:04.680]   I am the first woman of color to have a leading role in a Star Wars
[01:48:04.680 --> 01:48:05.680]   movie.
[01:48:05.680 --> 01:48:08.680]   I am the first Asian woman to appear on the cover of Vanity Fair.
[01:48:08.680 --> 01:48:12.680]   My real name is Loanne and I am just getting started.
[01:48:12.680 --> 01:48:15.680]   And that's really what she's saying is, I'm not going to let
[01:48:15.680 --> 01:48:16.680]   this keep me down.
[01:48:16.680 --> 01:48:17.680]   Good for her.
[01:48:17.680 --> 01:48:21.680]   But the fact that she even has to say this is sad.
[01:48:21.680 --> 01:48:29.680]   Do the platforms, are they in touch with you and Dana and reading
[01:48:29.680 --> 01:48:30.680]   what you're saying?
[01:48:30.680 --> 01:48:33.680]   I mean, are they open to what you have to say?
[01:48:33.680 --> 01:48:37.680]   Yeah, we have different researchers that have gone out to
[01:48:37.680 --> 01:48:43.680]   myself included, to talk with product design people, policy
[01:48:43.680 --> 01:48:44.680]   people.
[01:48:44.680 --> 01:48:45.680]   Good.
[01:48:45.680 --> 01:48:48.680]   We often, if we do see an anxious problem, we don't wait to
[01:48:48.680 --> 01:48:49.680]   research it.
[01:48:49.680 --> 01:48:53.680]   We reach out directly to them and say, hey, we see this is
[01:48:53.680 --> 01:48:54.680]   happening.
[01:48:54.680 --> 01:48:56.680]   I hope you can see it too.
[01:48:56.680 --> 01:49:01.680]   What's interesting actually is the camaraderie around researchers
[01:49:01.680 --> 01:49:06.680]   within these companies and researchers that exist in my world
[01:49:06.680 --> 01:49:12.680]   as well as other academics that are at Harvard and Stanford as
[01:49:12.680 --> 01:49:15.680]   well as UT Austin.
[01:49:15.680 --> 01:49:19.680]   So, like for instance, the Social Science Research Council has
[01:49:19.680 --> 01:49:24.680]   a new initiative to help with Facebook recruiting university
[01:49:24.680 --> 01:49:29.680]   researchers to do research with this really big bucket of
[01:49:29.680 --> 01:49:32.680]   Facebook data that they're releasing through this Social
[01:49:32.680 --> 01:49:37.680]   Science Research Council that academics can then apply to do
[01:49:37.680 --> 01:49:38.680]   research on.
[01:49:38.680 --> 01:49:43.680]   So normally that kind of data is kept in-house and the researchers
[01:49:43.680 --> 01:49:46.680]   at these companies will be the ones to analyze it.
[01:49:46.680 --> 01:49:50.680]   But we see more and more that these companies realize that
[01:49:50.680 --> 01:49:57.680]   they're not going to attract the talent and the rigor of academics
[01:49:57.680 --> 01:50:01.680]   that work in universities or work in research institutes like
[01:50:01.680 --> 01:50:05.680]   myself because we don't want to design products.
[01:50:05.680 --> 01:50:10.680]   Our interests actually lie in understanding how these products
[01:50:10.680 --> 01:50:15.680]   influence elections, how they impact society.
[01:50:15.680 --> 01:50:21.680]   And so we do have quite a few different meetups and gatherings
[01:50:21.680 --> 01:50:24.680]   and to talk through these problems.
[01:50:24.680 --> 01:50:28.680]   But it's from our vantage point, there are design decisions that
[01:50:28.680 --> 01:50:34.680]   can be made that haven't been made that we would like for
[01:50:34.680 --> 01:50:36.680]   platform companies to implement.
[01:50:36.680 --> 01:50:41.680]   But ultimately we do see that they reach out, they listen,
[01:50:41.680 --> 01:50:46.680]   and when we release reports they definitely react and let us
[01:50:46.680 --> 01:50:51.680]   know what they understand about the report and then they
[01:50:51.680 --> 01:50:54.680]   obviously always have questions.
[01:50:54.680 --> 01:51:00.680]   So that's been a different, you know, the mood of that research
[01:51:00.680 --> 01:51:03.680]   has definitely changed over the last year.
[01:51:03.680 --> 01:51:08.680]   It used to be that they were taking interest in fake news and
[01:51:08.680 --> 01:51:11.680]   thinking about it more like a problem of spam and how do we get
[01:51:11.680 --> 01:51:16.680]   spam off of the websites and then as it evolved into, oh, this
[01:51:16.680 --> 01:51:23.680]   was a concerted effort by foreign, you know, medium
[01:51:23.680 --> 01:51:26.680]   manipulators to manipulate elections.
[01:51:26.680 --> 01:51:31.680]   The tenor around the seriousness of what was at stake changed.
[01:51:31.680 --> 01:51:38.680]   And we've had some really important conversations with
[01:51:38.680 --> 01:51:42.680]   Twitter, Facebook, Google.
[01:51:42.680 --> 01:51:46.680]   And so I, you know, to be honest with you, I wouldn't stay in
[01:51:46.680 --> 01:51:50.680]   the field if I felt like we weren't getting somewhere.
[01:51:50.680 --> 01:51:55.680]   It would, you know, it would, it does sometimes feel like we're
[01:51:55.680 --> 01:51:59.680]   like beating a brick wall and we're like, what are we trying to
[01:51:59.680 --> 01:52:00.680]   do here?
[01:52:00.680 --> 01:52:08.680]   But we are getting, we are getting, I think, closer to
[01:52:08.680 --> 01:52:11.680]   understanding.
[01:52:11.680 --> 01:52:16.680]   And we're learning, I think from our vantage point as researchers
[01:52:16.680 --> 01:52:20.680]   have been thinking about how complex the problem looks from the
[01:52:20.680 --> 01:52:22.680]   inside of these companies.
[01:52:22.680 --> 01:52:25.680]   And I think that the companies are learning from us that there are
[01:52:25.680 --> 01:52:29.680]   different ways in which they can engage and think about doing
[01:52:29.680 --> 01:52:35.680]   research, which hopefully will end up in their design decisions.
[01:52:35.680 --> 01:52:37.680]   I want to take a little break.
[01:52:37.680 --> 01:52:40.680]   We will come back and get the change log.
[01:52:40.680 --> 01:52:45.680]   All the new stuff happening on Google.
[01:52:45.680 --> 01:52:48.680]   So, first, I'll show you today brought to you by Rocket Mortgage.
[01:52:48.680 --> 01:52:51.680]   Your new home will be brought to you by Rocket Mortgage if you
[01:52:51.680 --> 01:52:53.680]   pay attention closely to the woods.
[01:52:53.680 --> 01:52:56.680]   I am about to speak Rocket Mortgage from quick and loans, the
[01:52:56.680 --> 01:52:58.680]   best lender in the country.
[01:52:58.680 --> 01:53:01.680]   Number one in customer satisfaction for eight years in a row
[01:53:01.680 --> 01:53:03.680]   according to JD Power.
[01:53:03.680 --> 01:53:06.680]   Number one, that probably is why they're also, as of December,
[01:53:06.680 --> 01:53:10.680]   the number one largest in terms of volume, mortgage lender in the
[01:53:10.680 --> 01:53:11.680]   country.
[01:53:11.680 --> 01:53:13.680]   They listen, they think about their customers and they decided to
[01:53:13.680 --> 01:53:15.680]   do that in the country.
[01:53:15.680 --> 01:53:17.680]   They're going to be able to do that in the country.
[01:53:17.680 --> 01:53:19.680]   They're going to be able to do that in the country.
[01:53:19.680 --> 01:53:22.680]   They're going to be able to do that in the country.
[01:53:22.680 --> 01:53:24.680]   They're going to be able to do that in the country.
[01:53:24.680 --> 01:53:27.680]   They're going to be able to do that in the country.
[01:53:27.680 --> 01:53:30.680]   They're going to be able to do that in the country.
[01:53:30.680 --> 01:53:32.680]   So, let me walk you through it step by step.
[01:53:32.680 --> 01:53:34.680]   First, on your phone, you log into your account.
[01:53:34.680 --> 01:53:35.680]   Go to rocketmortgage.com/twigs.
[01:53:35.680 --> 01:53:37.680]   Set that account up so you'll be ready.
[01:53:37.680 --> 01:53:39.680]   You'll log into your account.
[01:53:39.680 --> 01:53:43.680]   You answer a few simple questions.
[01:53:43.680 --> 01:53:45.680]   You give them permission to get your data.
[01:53:45.680 --> 01:53:47.680]   You don't have to go get it yourself.
[01:53:47.680 --> 01:53:48.680]   You don't have to go to the attic.
[01:53:48.680 --> 01:53:49.680]   You don't have to call your previous employers or anything.
[01:53:49.680 --> 01:53:51.680]   They get everything they need to basically figure out what
[01:53:51.680 --> 01:53:54.680]   you're good for, what, what, and then they give you all the
[01:53:54.680 --> 01:53:55.680]   loans for which you qualify.
[01:53:55.680 --> 01:53:57.680]   You choose the rate, the term, and everything.
[01:53:57.680 --> 01:53:58.680]   That's step one.
[01:53:58.680 --> 01:54:02.680]   Step two, 24 hours later, you get verified approval.
[01:54:02.680 --> 01:54:05.680]   A loan officer goes over the information, verifies income
[01:54:05.680 --> 01:54:08.680]   assets and credit, and they give you verified approval.
[01:54:08.680 --> 01:54:10.680]   You now have the strength of a cash buyer.
[01:54:10.680 --> 01:54:13.680]   You get that verified approval letter, which you can take
[01:54:13.680 --> 01:54:15.680]   with you when you make an offer.
[01:54:15.680 --> 01:54:17.680]   You go right to the front of the line because you're good for it.
[01:54:17.680 --> 01:54:19.680]   You're verified.
[01:54:19.680 --> 01:54:21.680]   You got the money.
[01:54:21.680 --> 01:54:25.680]   And once you're verified, step three, rate shield approval,
[01:54:25.680 --> 01:54:28.680]   all new and exclusive, and frankly, we didn't need this last year.
[01:54:28.680 --> 01:54:29.680]   Interest rates were flat.
[01:54:29.680 --> 01:54:31.680]   They've been going up, haven't they?
[01:54:31.680 --> 01:54:34.680]   And that means there's stress.
[01:54:34.680 --> 01:54:37.680]   That means there's like, we got to buy this house.
[01:54:37.680 --> 01:54:40.680]   It could cost us a lot more tomorrow if the rate goes up.
[01:54:40.680 --> 01:54:43.680]   No, with rate shield approval, they lock up your rate for up to
[01:54:43.680 --> 01:54:44.680]   90 days.
[01:54:44.680 --> 01:54:45.680]   You have three months to look.
[01:54:45.680 --> 01:54:48.680]   Your rate cannot go up.
[01:54:48.680 --> 01:54:51.680]   If your rate goes up, your rate stays the same.
[01:54:51.680 --> 01:54:54.680]   If rates go up, your rate stays, if they go down, your rate goes
[01:54:54.680 --> 01:54:56.680]   down, either way you win.
[01:54:56.680 --> 01:54:59.680]   This is why they're the best, quick and loans in rocket
[01:54:59.680 --> 01:55:00.680]   mortgage.
[01:55:00.680 --> 01:55:04.680]   All you have to do right now, go to rocketmortgage.com/twegangetstarted.
[01:55:04.680 --> 01:55:07.680]   Rate shield approvals only valid on certain 30-year purchase
[01:55:07.680 --> 01:55:10.680]   transactions, additional conditions, or exclusions may apply
[01:55:10.680 --> 01:55:13.680]   based on quick and loans data in comparison to public data
[01:55:13.680 --> 01:55:14.680]   records.
[01:55:14.680 --> 01:55:16.680]   Equal housing lender, licensed in all 50 states, and MLS
[01:55:16.680 --> 01:55:19.680]   consumer access.org number 30, 30.
[01:55:19.680 --> 01:55:21.680]   You don't have to remember all that.
[01:55:21.680 --> 01:55:22.680]   Just remember this.
[01:55:22.680 --> 01:55:25.680]   Rocketmortgage.com/twegan.
[01:55:25.680 --> 01:55:28.680]   We thank quick and loans and rocket mortgage for their
[01:55:28.680 --> 01:55:32.680]   support of this week in Google.
[01:55:32.680 --> 01:55:36.680]   And now get the Bugles Ready, Carsten, because it's time
[01:55:36.680 --> 01:55:40.680]   for the Google Change Log.
[01:55:40.680 --> 01:55:42.680]   The Google Change Log.
[01:55:42.680 --> 01:55:43.680]   You missed it.
[01:55:43.680 --> 01:55:44.680]   You demanded it.
[01:55:44.680 --> 01:55:46.680]   Back by popular demand.
[01:55:46.680 --> 01:55:48.680]   Leo's Salad.
[01:55:48.680 --> 01:55:50.680]   It's the Change Log.
[01:55:50.680 --> 01:55:52.680]   New stuff from Google.
[01:55:52.680 --> 01:55:56.680]   Interesting article in Google's keyword blog about new ways
[01:55:56.680 --> 01:55:58.680]   search is helping you.
[01:55:58.680 --> 01:56:01.680]   Imagine you're remodeling your kitchen.
[01:56:01.680 --> 01:56:04.680]   I think Stacy's always remodeling your kitchen.
[01:56:04.680 --> 01:56:07.680]   And you want to know about how quartz compares to granite.
[01:56:07.680 --> 01:56:11.680]   You could enter a search term, quartz vs granite.
[01:56:11.680 --> 01:56:12.680]   And it would give you these.
[01:56:12.680 --> 01:56:14.680]   This is a new search box.
[01:56:14.680 --> 01:56:15.680]   Look at this.
[01:56:15.680 --> 01:56:19.680]   Cost benefits weight durability.
[01:56:19.680 --> 01:56:22.680]   When you search for something, you'll see a panel with set of
[01:56:22.680 --> 01:56:24.680]   relevant subtopics to explore.
[01:56:24.680 --> 01:56:28.680]   Another example, if you search emergency fund, you'll get a
[01:56:28.680 --> 01:56:30.680]   quick view of information that relates to the recommended
[01:56:30.680 --> 01:56:32.680]   size, purpose, and importance of an emergency fund.
[01:56:32.680 --> 01:56:33.680]   Wait a minute.
[01:56:33.680 --> 01:56:34.680]   I got to try that.
[01:56:34.680 --> 01:56:39.680]   Emergency fund.
[01:56:39.680 --> 01:56:40.680]   No.
[01:56:40.680 --> 01:56:41.680]   Maybe it's not on yet.
[01:56:41.680 --> 01:56:43.680]   Or maybe I just have Google set.
[01:56:43.680 --> 01:56:44.680]   Maybe you can't spell.
[01:56:44.680 --> 01:56:45.680]   Did I spell it wrong?
[01:56:45.680 --> 01:56:47.680]   No, you did.
[01:56:47.680 --> 01:56:49.680]   I frequently spell things wrong.
[01:56:49.680 --> 01:56:52.680]   I am terrible at spelling and Google is constantly like,
[01:56:52.680 --> 01:56:54.680]   did you perhaps mean this?
[01:56:54.680 --> 01:56:57.680]   That's the single biggest use at Google these days for me is
[01:56:57.680 --> 01:56:59.680]   spell check.
[01:56:59.680 --> 01:57:02.680]   This is a new feature in search, but that's not the only
[01:57:02.680 --> 01:57:03.680]   thing.
[01:57:03.680 --> 01:57:06.680]   Are you ready for some good news?
[01:57:06.680 --> 01:57:10.680]   Now you can ask your Google home for good news.
[01:57:10.680 --> 01:57:14.680]   You say Google, tell me something good.
[01:57:14.680 --> 01:57:16.680]   Mine doesn't work on that.
[01:57:16.680 --> 01:57:18.680]   Well, I think some of the things we talk about on the
[01:57:18.680 --> 01:57:20.680]   change log, we should give you a disclaimer.
[01:57:20.680 --> 01:57:22.680]   Some of the things mentioned in the chain log are not
[01:57:22.680 --> 01:57:25.680]   available yet, available yet, but we'll be soon.
[01:57:25.680 --> 01:57:30.680]   So tell me something good.
[01:57:30.680 --> 01:57:32.680]   Tell me something good is a new experimental feature.
[01:57:32.680 --> 01:57:33.680]   Maybe that's why you don't get it.
[01:57:33.680 --> 01:57:37.680]   For assistant users in the US, it delivers your daily dose of
[01:57:37.680 --> 01:57:38.680]   good news.
[01:57:38.680 --> 01:57:41.680]   Just say, hey, goog, tell me something good to receive a
[01:57:41.680 --> 01:57:43.680]   brief news summary about people who are solving problems for
[01:57:43.680 --> 01:57:46.680]   our communities and the world.
[01:57:46.680 --> 01:57:51.680]   It comes from the solutions journalism network, a non-partisan
[01:57:51.680 --> 01:57:56.680]   nonprofit.
[01:57:56.680 --> 01:57:57.680]   I don't know.
[01:57:57.680 --> 01:57:59.680]   Trump associates going to jail.
[01:57:59.680 --> 01:58:00.680]   It's not going to be in there.
[01:58:00.680 --> 01:58:03.680]   It's a nonpartisan nonprofit dedicated to spreading the
[01:58:03.680 --> 01:58:05.680]   practice of solutions journalism.
[01:58:05.680 --> 01:58:08.680]   I didn't even know that was a practice.
[01:58:08.680 --> 01:58:11.680]   But if you feel bad, ask Google to tell you something good.
[01:58:11.680 --> 01:58:14.680]   I've been using the Apple Watch for some time with the
[01:58:14.680 --> 01:58:15.680]   activity rings.
[01:58:15.680 --> 01:58:17.680]   Now Google's doing it.
[01:58:17.680 --> 01:58:20.680]   The new Google Fit has, well, they're not rings.
[01:58:20.680 --> 01:58:21.680]   They're octagons.
[01:58:21.680 --> 01:58:23.680]   But it's the same idea.
[01:58:23.680 --> 01:58:25.680]   The activity rings.
[01:58:25.680 --> 01:58:28.680]   They don't have the stand ring, but they have move minutes and
[01:58:28.680 --> 01:58:29.680]   heart points.
[01:58:29.680 --> 01:58:34.680]   It's an all new Google Fit, also an Android Wear notice.
[01:58:34.680 --> 01:58:36.680]   Don't expect this on the iPhone.
[01:58:36.680 --> 01:58:39.680]   I don't know if you get these new features yet on the iPhone, but
[01:58:39.680 --> 01:58:42.680]   the Google Fit on Android and on the Android Wear.
[01:58:42.680 --> 01:58:45.680]   You get move minutes for all your activity.
[01:58:45.680 --> 01:58:49.680]   You get heart points when you get your heart pumping harder.
[01:58:49.680 --> 01:58:51.680]   So do some kickboxing.
[01:58:51.680 --> 01:58:55.680]   I think this is a good thing, yes?
[01:58:55.680 --> 01:58:56.680]   I guess so.
[01:58:56.680 --> 01:59:00.680]   I mean, Android Wear was so crappy, I just went with Fitbit.
[01:59:00.680 --> 01:59:04.680]   And I don't see myself going back to it anytime soon.
[01:59:04.680 --> 01:59:07.680]   And then, okay, I'm going to do a couple.
[01:59:07.680 --> 01:59:09.680]   What do you think, Stacey?
[01:59:09.680 --> 01:59:13.680]   Should the change log includes things that might be happening
[01:59:13.680 --> 01:59:17.680]   or are probably going to happen but haven't happened yet?
[01:59:17.680 --> 01:59:19.680]   Or should it only be things that are happening?
[01:59:19.680 --> 01:59:26.680]   For instance, have you seen the pictures of the new -- let me
[01:59:26.680 --> 01:59:31.680]   see if I can find it -- Pixel 3 XL in Russian?
[01:59:31.680 --> 01:59:34.680]   So a change log is actually things that have changed, so I
[01:59:34.680 --> 01:59:36.680]   think you should keep it that way.
[01:59:36.680 --> 01:59:37.680]   Damn it.
[01:59:37.680 --> 01:59:40.680]   But we can talk about that outside of the change log.
[01:59:40.680 --> 01:59:42.680]   Okay, I'll save that.
[01:59:42.680 --> 01:59:44.680]   They probably don't want me to mention the experimental
[01:59:44.680 --> 01:59:48.680]   podcast app, Shortwave, because that's not out yet either.
[01:59:48.680 --> 01:59:50.680]   I'll save those for after the change log.
[01:59:50.680 --> 01:59:55.680]   So a brief, a truncated, but I think sufficiently interesting
[01:59:55.680 --> 01:59:57.680]   Google change log is worth.
[01:59:57.680 --> 01:59:58.680]   Thank you.
[01:59:58.680 --> 02:00:00.680]   And God bless.
[02:00:00.680 --> 02:00:03.680]   So now we'll mention the Pixel 3.
[02:00:03.680 --> 02:00:05.680]   You use a Pixel 2, right?
[02:00:05.680 --> 02:00:06.680]   Stacey?
[02:00:06.680 --> 02:00:07.680]   I do.
[02:00:07.680 --> 02:00:08.680]   Yeah.
[02:00:08.680 --> 02:00:09.680]   The smaller, not the XL.
[02:00:09.680 --> 02:00:12.680]   So somebody in Russia has -- this is an XL, obviously.
[02:00:12.680 --> 02:00:17.680]   And it's very similar, except -- look, there's a notch on nuts.
[02:00:17.680 --> 02:00:20.680]   No one knows when this is going to come out.
[02:00:20.680 --> 02:00:24.680]   The rumor is October, but I have to tell you, with pie out now,
[02:00:24.680 --> 02:00:31.680]   and all of these leaks of, you know, packaged Pixel 3s, I wonder --
[02:00:31.680 --> 02:00:33.680]   Who were they said in these two?
[02:00:33.680 --> 02:00:35.680]   I don't know.
[02:00:35.680 --> 02:00:37.680]   How were they getting out into the world?
[02:00:37.680 --> 02:00:41.680]   That was my -- and can I just tell you, pie is confusing me.
[02:00:41.680 --> 02:00:42.680]   Is it?
[02:00:42.680 --> 02:00:43.680]   You don't like pie?
[02:00:43.680 --> 02:00:47.680]   I don't -- I used to get the weather.
[02:00:47.680 --> 02:00:48.680]   Now I don't get the weather.
[02:00:48.680 --> 02:00:52.680]   I actually followed this lovely log post that told me all the ways
[02:00:52.680 --> 02:00:54.680]   I could get the weather, and I tried to troubleshoot that way.
[02:00:54.680 --> 02:00:56.680]   You're talking about the widget.
[02:00:56.680 --> 02:00:58.680]   There is no weather widget.
[02:00:58.680 --> 02:01:00.680]   It used to be a weather widget.
[02:01:00.680 --> 02:01:01.680]   There is -- there's a widget.
[02:01:01.680 --> 02:01:04.680]   So if you use the -- let me turn up the widget.
[02:01:04.680 --> 02:01:05.680]   I'll look for the widget.
[02:01:05.680 --> 02:01:10.680]   If you use the default Pixel launcher at the top here,
[02:01:10.680 --> 02:01:13.680]   you get this changing -- you can't really see it very well, can you?
[02:01:13.680 --> 02:01:16.680]   This changing widget, mine says six minutes to home,
[02:01:16.680 --> 02:01:20.680]   because it knows I'm almost done, no delays, moderate traffic.
[02:01:20.680 --> 02:01:23.680]   Then it gives me a little sun and 76 degrees.
[02:01:23.680 --> 02:01:26.680]   I can tap that, and the weather pops up.
[02:01:26.680 --> 02:01:29.680]   Or if I text six minutes to home, it shows me the commute.
[02:01:29.680 --> 02:01:30.680]   Right.
[02:01:30.680 --> 02:01:31.680]   So my --
[02:01:31.680 --> 02:01:32.680]   Don't you have that?
[02:01:32.680 --> 02:01:33.680]   My son went away.
[02:01:33.680 --> 02:01:34.680]   No.
[02:01:34.680 --> 02:01:35.680]   Your son went away.
[02:01:35.680 --> 02:01:37.680]   My son went away, and then it came back briefly.
[02:01:37.680 --> 02:01:38.680]   Well, no, it does.
[02:01:38.680 --> 02:01:40.680]   It's not going to be a good way again.
[02:01:40.680 --> 02:01:42.680]   That's what's confusing about this.
[02:01:42.680 --> 02:01:44.680]   It changes all the time.
[02:01:44.680 --> 02:01:46.680]   It doesn't give me any other information.
[02:01:46.680 --> 02:01:47.680]   Sometimes I get calendared.
[02:01:47.680 --> 02:01:49.680]   Like if I had an appointment, it would show me a calendar.
[02:01:49.680 --> 02:01:51.680]   Yes, it does give me the calendars.
[02:01:51.680 --> 02:01:52.680]   But you don't get the weather.
[02:01:52.680 --> 02:01:53.680]   I don't get the weather anymore.
[02:01:53.680 --> 02:01:55.680]   I used to get the weather, and I loved getting the weather.
[02:01:55.680 --> 02:01:57.680]   I love the weather, too.
[02:01:57.680 --> 02:01:58.680]   Why?
[02:01:58.680 --> 02:02:02.680]   And then the other thing is, I'm constantly putting my phone
[02:02:02.680 --> 02:02:03.680]   into do not disturb.
[02:02:03.680 --> 02:02:06.680]   And when I use the nifty little --
[02:02:06.680 --> 02:02:07.680]   It's a rocker.
[02:02:07.680 --> 02:02:08.680]   Yeah?
[02:02:08.680 --> 02:02:09.680]   The rocker to turn it off.
[02:02:09.680 --> 02:02:10.680]   Yeah?
[02:02:10.680 --> 02:02:11.680]   It doesn't actually turn it off.
[02:02:11.680 --> 02:02:12.680]   I have to turn it off.
[02:02:12.680 --> 02:02:13.680]   Oh, there is --
[02:02:13.680 --> 02:02:16.680]   I notice there are a lot of people complaining because this old do
[02:02:16.680 --> 02:02:17.680]   not disturb thing.
[02:02:17.680 --> 02:02:18.680]   Yeah.
[02:02:18.680 --> 02:02:20.680]   You have to go here for that.
[02:02:20.680 --> 02:02:21.680]   The rocker --
[02:02:21.680 --> 02:02:23.680]   So you never want to touch that.
[02:02:23.680 --> 02:02:27.680]   The rocker is just the three different states of the bell
[02:02:27.680 --> 02:02:30.680]   and the level of the media.
[02:02:30.680 --> 02:02:33.680]   If you touch the gear, you'll see more about --
[02:02:33.680 --> 02:02:35.680]   You'll get an idea of what you're getting.
[02:02:35.680 --> 02:02:36.680]   That's not do not disturb.
[02:02:36.680 --> 02:02:41.680]   You have to go up here and turn that do not disturb icon offer
[02:02:41.680 --> 02:02:42.680]   on still.
[02:02:42.680 --> 02:02:44.680]   How about the gestures?
[02:02:44.680 --> 02:02:46.680]   Some people have been complaining about this.
[02:02:46.680 --> 02:02:49.680]   Instead of the three buttons at the bottom, we now have a pill.
[02:02:49.680 --> 02:02:51.680]   Swipe up all the way.
[02:02:51.680 --> 02:02:53.680]   You get apps.
[02:02:53.680 --> 02:02:55.680]   Swipe up halfway.
[02:02:55.680 --> 02:02:58.680]   Oh, you have to turn the pill on.
[02:02:58.680 --> 02:02:59.680]   Oh.
[02:02:59.680 --> 02:03:01.680]   So let me help you turning on the pill.
[02:03:01.680 --> 02:03:03.680]   That's in settings.
[02:03:03.680 --> 02:03:05.680]   Why am I on the show again?
[02:03:05.680 --> 02:03:09.680]   No, I think I showed Jeff how to do this last week, but you were
[02:03:09.680 --> 02:03:11.680]   apparently asleep.
[02:03:11.680 --> 02:03:12.680]   It's possible.
[02:03:12.680 --> 02:03:16.680]   They don't, for some reason, they don't turn it on, which I think
[02:03:16.680 --> 02:03:19.680]   they should because I think it's a big part of it, right?
[02:03:19.680 --> 02:03:20.680]   I can Google this.
[02:03:20.680 --> 02:03:22.680]   You don't actually have to show me.
[02:03:22.680 --> 02:03:24.680]   We have other things we can do.
[02:03:24.680 --> 02:03:28.680]   If you type in a home in a search, you can --
[02:03:28.680 --> 02:03:30.680]   I don't know.
[02:03:30.680 --> 02:03:31.680]   Screw it.
[02:03:31.680 --> 02:03:33.680]   Yeah, just figure it out.
[02:03:33.680 --> 02:03:35.680]   I can Google this.
[02:03:35.680 --> 02:03:37.680]   I am adapting to this.
[02:03:37.680 --> 02:03:38.680]   Okay, story two.
[02:03:38.680 --> 02:03:41.680]   And then we're going to take a break and get -- because everybody has
[02:03:41.680 --> 02:03:42.680]   to go home.
[02:03:42.680 --> 02:03:43.680]   We've been going on way too long.
[02:03:43.680 --> 02:03:45.680]   Google, according -- no, not your fault.
[02:03:45.680 --> 02:03:47.680]   It's John's fault.
[02:03:47.680 --> 02:03:48.680]   Google -- no, just teasing.
[02:03:48.680 --> 02:03:49.680]   My fault.
[02:03:49.680 --> 02:03:52.680]   It's always -- anything that goes wrong in this show is my fault
[02:03:52.680 --> 02:03:54.680]   because I'm your MC.
[02:03:54.680 --> 02:03:56.680]   I'm your master of ceremonies.
[02:03:56.680 --> 02:03:59.680]   Google is developing an experimental podcast app called
[02:03:59.680 --> 02:04:00.680]   Shortwave.
[02:04:00.680 --> 02:04:02.680]   This is according to the Verge.
[02:04:02.680 --> 02:04:04.680]   They're the trademark filing they found.
[02:04:04.680 --> 02:04:08.680]   "Reached by the Verge, a Google spokesperson emphasized --
[02:04:08.680 --> 02:04:11.680]   confirmed it, emphasized the app is being developed within the
[02:04:11.680 --> 02:04:15.680]   company's area at 120 incubator, not related to existing Google
[02:04:15.680 --> 02:04:16.680]   projects."
[02:04:16.680 --> 02:04:20.680]   "One of the many projects we're working on within area 120
[02:04:20.680 --> 02:04:24.680]   is Shortwave, which helps users discover and consume spoken word
[02:04:24.680 --> 02:04:25.680]   audio in new ways."
[02:04:25.680 --> 02:04:29.680]   It's a very early experiment, so aren't there?
[02:04:29.680 --> 02:04:32.680]   There aren't many details to share right now.
[02:04:32.680 --> 02:04:35.680]   What do you think transcriptions may be?
[02:04:35.680 --> 02:04:38.680]   I can't talk about it.
[02:04:38.680 --> 02:04:44.680]   Stacey Denise Higginbotham.
[02:04:44.680 --> 02:04:46.680]   I'm making up your middle name because I don't --
[02:04:46.680 --> 02:04:47.680]   You are.
[02:04:47.680 --> 02:04:48.680]   I'm like, interesting.
[02:04:48.680 --> 02:04:49.680]   Is Denise okay?
[02:04:49.680 --> 02:04:50.680]   Well, you're Denise.
[02:04:50.680 --> 02:04:51.680]   Your brother's Denise.
[02:04:51.680 --> 02:04:52.680]   Yeah.
[02:04:52.680 --> 02:04:53.680]   Yeah.
[02:04:53.680 --> 02:04:55.680]   [ Laughter ]
[02:04:55.680 --> 02:04:56.680]   [ Laughter ]
[02:04:56.680 --> 02:04:59.680]   The best guess you can give is Marie.
[02:04:59.680 --> 02:05:00.680]   What?
[02:05:00.680 --> 02:05:01.680]   Right.
[02:05:01.680 --> 02:05:04.680]   There's a lot of Marie's for middle, you know.
[02:05:04.680 --> 02:05:06.680]   Stacey Marie Higginbotham.
[02:05:06.680 --> 02:05:07.680]   That does have a ring to it.
[02:05:07.680 --> 02:05:08.680]   No.
[02:05:08.680 --> 02:05:09.680]   It's just everybody's.
[02:05:09.680 --> 02:05:10.680]   It's close.
[02:05:10.680 --> 02:05:11.680]   It's close.
[02:05:11.680 --> 02:05:12.680]   Yeah.
[02:05:12.680 --> 02:05:13.680]   That's my mom's middle name.
[02:05:13.680 --> 02:05:15.680]   My mom's first name is my middle name.
[02:05:15.680 --> 02:05:17.680]   Joan Donovan, you are so smart.
[02:05:17.680 --> 02:05:18.680]   Joan, so good.
[02:05:18.680 --> 02:05:21.680]   Well, if you ever need to socially engineer your way into
[02:05:21.680 --> 02:05:24.680]   something, just remember it's a woman's name.
[02:05:24.680 --> 02:05:27.680]   Marie might be the way to go.
[02:05:27.680 --> 02:05:28.680]   Wow.
[02:05:28.680 --> 02:05:29.680]   Fascinating.
[02:05:29.680 --> 02:05:34.680]   So, Joan Donovan, data and society, datasociety.net,
[02:05:34.680 --> 02:05:36.680]   your pick of the week.
[02:05:36.680 --> 02:05:40.680]   So, this is actually something you might like, Leo.
[02:05:40.680 --> 02:05:41.680]   I have it.
[02:05:41.680 --> 02:05:42.680]   I interviewed Siva.
[02:05:42.680 --> 02:05:43.680]   I love it.
[02:05:43.680 --> 02:05:44.680]   Okay.
[02:05:44.680 --> 02:05:47.680]   So, this book, I just started it and I'm loving it.
[02:05:47.680 --> 02:05:49.680]   And so, I'm hoping that the --
[02:05:49.680 --> 02:05:50.680]   I think that this is --
[02:05:50.680 --> 02:05:51.680]   I think this is --
[02:05:51.680 --> 02:05:52.680]   This is --
[02:05:52.680 --> 02:05:53.680]   Here it is.
[02:05:53.680 --> 02:05:55.680]   This is why I'm not on Facebook anymore.
[02:05:55.680 --> 02:05:56.680]   Frankly.
[02:05:56.680 --> 02:05:57.680]   Yes.
[02:05:57.680 --> 02:06:01.680]   How Facebook disconnects us and undermines democracy.
[02:06:01.680 --> 02:06:03.680]   That was the seed.
[02:06:03.680 --> 02:06:05.680]   And Siva's great.
[02:06:05.680 --> 02:06:06.680]   Yeah, he's on this weekend.
[02:06:06.680 --> 02:06:08.680]   No, I'm sorry, a triangulation episode.
[02:06:08.680 --> 02:06:09.680]   Mm-hmm.
[02:06:09.680 --> 02:06:10.680]   Yeah.
[02:06:10.680 --> 02:06:14.680]   And then I have one other one, which is another book about the
[02:06:14.680 --> 02:06:17.680]   Internet, custodians of the Internet, Tarleton Gillespie,
[02:06:17.680 --> 02:06:18.680]   who I don't know about.
[02:06:18.680 --> 02:06:19.680]   King that recently.
[02:06:19.680 --> 02:06:24.680]   And I read a pre-print of this a few months back and was really
[02:06:24.680 --> 02:06:25.680]   impressed.
[02:06:25.680 --> 02:06:29.680]   And so, I'm going to now take my pen to the paper and really
[02:06:29.680 --> 02:06:30.680]   dig in.
[02:06:30.680 --> 02:06:34.680]   But I think these two books right now couldn't be more timely
[02:06:34.680 --> 02:06:38.680]   thinking about content moderation, thinking about these platforms
[02:06:38.680 --> 02:06:41.680]   and, you know, how they're shaping our lives.
[02:06:41.680 --> 02:06:45.680]   By the way, Gillespie is a principal researcher at Microsoft
[02:06:45.680 --> 02:06:48.680]   Research, New England, and an affiliated associate professor
[02:06:48.680 --> 02:06:49.680]   at Cornell.
[02:06:49.680 --> 02:06:51.680]   It's published by the Yale University Press.
[02:06:51.680 --> 02:06:54.680]   And it's exactly what we were just talking about.
[02:06:54.680 --> 02:06:55.680]   Mm-hmm.
[02:06:55.680 --> 02:06:58.680]   Platforms, content, moderation, and the hidden decisions that
[02:06:58.680 --> 02:07:01.680]   shape social media, custodians of the Internet.
[02:07:01.680 --> 02:07:03.680]   Put that on our list, Karsten.
[02:07:03.680 --> 02:07:05.680]   That looks like a good one.
[02:07:05.680 --> 02:07:07.680]   Stacy Higginbotham.
[02:07:07.680 --> 02:07:10.680]   Okay, you guys, y'all are going to hate me, but I'm under
[02:07:10.680 --> 02:07:14.680]   bar go for a bunch of devices that break like the next couple
[02:07:14.680 --> 02:07:15.680]   weeks.
[02:07:15.680 --> 02:07:17.680]   Clearly for something new from Google.
[02:07:17.680 --> 02:07:18.680]   Oh.
[02:07:18.680 --> 02:07:23.680]   So, being under embargo for all these crazy things, I had to
[02:07:23.680 --> 02:07:28.680]   reach into the old book world again, sorry for the device
[02:07:28.680 --> 02:07:30.680]   lovers out there.
[02:07:30.680 --> 02:07:34.680]   But this is actually an old book that I saw because of an
[02:07:34.680 --> 02:07:37.680]   Axios article citing Bill Gates.
[02:07:37.680 --> 02:07:42.680]   So it was Bill Gates in his blog talking about this.
[02:07:42.680 --> 02:07:46.680]   So the book is called Capital, sorry, Capitalism Without
[02:07:46.680 --> 02:07:47.680]   Capital.
[02:07:47.680 --> 02:07:48.680]   Oh.
[02:07:48.680 --> 02:07:50.680]   And it's the idea, and I'm interested in this because, you
[02:07:50.680 --> 02:07:53.680]   know, everything I talk about is basically being sold as a
[02:07:53.680 --> 02:07:54.680]   service.
[02:07:54.680 --> 02:07:57.680]   So the idea is for out much of history when you make a
[02:07:57.680 --> 02:08:03.680]   product, your as demand increases, oh, I can never, this is
[02:08:03.680 --> 02:08:04.680]   why I'm terrible with economics.
[02:08:04.680 --> 02:08:07.680]   I can't read charts at the same time and talk.
[02:08:07.680 --> 02:08:12.680]   So the idea is that as demand increases prices drop until you
[02:08:12.680 --> 02:08:16.680]   reach some sort of magic equilibrium, what happens with
[02:08:16.680 --> 02:08:20.680]   software is your cost of producing goods doesn't really
[02:08:20.680 --> 02:08:24.680]   mesh necessarily with how much you can charge for them.
[02:08:24.680 --> 02:08:28.680]   And so there's this whole area of the economy that we're not
[02:08:28.680 --> 02:08:31.680]   really dealing, that we're not really talking about.
[02:08:31.680 --> 02:08:34.680]   So I got this book so I could learn more about this because I
[02:08:34.680 --> 02:08:38.680]   think that's a lot of the old rules, no longer apply to the
[02:08:38.680 --> 02:08:42.680]   new way of technology, the new way of being.
[02:08:42.680 --> 02:08:43.680]   Yeah.
[02:08:43.680 --> 02:08:47.680]   And like, you know, I talk about this when I was in Germany
[02:08:47.680 --> 02:08:51.680]   earlier this year at this Bosch event, they have actually an
[02:08:51.680 --> 02:08:54.680]   entire lab dedicated to business models for the service
[02:08:54.680 --> 02:08:55.680]   economy.
[02:08:55.680 --> 02:08:56.680]   Wow.
[02:08:56.680 --> 02:08:59.680]   So I think that's a lot of the things that we're going to
[02:08:59.680 --> 02:09:04.680]   talk about today, and entire lab dedicated to business models
[02:09:04.680 --> 02:09:08.680]   for the service economy, which is kind of crazy to think about,
[02:09:08.680 --> 02:09:09.680]   but also kind of awesome.
[02:09:09.680 --> 02:09:12.680]   I keep trying to talk to people in that lab, but I don't speak
[02:09:12.680 --> 02:09:14.680]   German, so maybe I should get Jeff over there.
[02:09:14.680 --> 02:09:15.680]   So yeah.
[02:09:15.680 --> 02:09:16.680]   Very nice.
[02:09:16.680 --> 02:09:17.680]   The book.
[02:09:17.680 --> 02:09:22.680]   Capitalism without capital, the rise of the intangible economy
[02:09:22.680 --> 02:09:25.680]   written by Jonathan Haskell.
[02:09:25.680 --> 02:09:30.680]   I don't have a pick, but I have a reminder that we've had some
[02:09:30.680 --> 02:09:33.680]   very good triangulations over the last few weeks.
[02:09:33.680 --> 02:09:35.680]   Last two weeks were Saul's.
[02:09:35.680 --> 02:09:36.680]   I'm sorry.
[02:09:36.680 --> 02:09:38.680]   That's what Steve Jobs calls him.
[02:09:38.680 --> 02:09:42.680]   Sal Sequoian with some great anecdotes about Steve Jobs.
[02:09:42.680 --> 02:09:44.680]   He's the guy who was in charge of AppleScript and an
[02:09:44.680 --> 02:09:47.680]   automator at Apple till he was let go a couple of years ago,
[02:09:47.680 --> 02:09:51.680]   and he has some very pungent things to say and some great demos
[02:09:51.680 --> 02:09:52.680]   too.
[02:09:52.680 --> 02:09:57.680]   This week, 3 p.m. Friday, 6 p.m. Eastern, we're going to talk
[02:09:57.680 --> 02:10:03.680]   to on triangulation to Bill Atkinson, who is really the heart
[02:10:03.680 --> 02:10:07.680]   and soul of the Macintosh, the Lisa, and General Magic.
[02:10:07.680 --> 02:10:09.680]   He's going to bring some toys and props and tools, and we're
[02:10:09.680 --> 02:10:12.680]   going to talk about Apple, but we'll also talk about General
[02:10:12.680 --> 02:10:13.680]   Magic.
[02:10:13.680 --> 02:10:15.680]   This is going to be a lot of fun.
[02:10:15.680 --> 02:10:16.680]   Bill's always a great interview.
[02:10:16.680 --> 02:10:18.680]   That's this Friday, 3 p.m.
[02:10:18.680 --> 02:10:19.680]   Pacific, 6 p.m.
[02:10:19.680 --> 02:10:20.680]   Eastern time.
[02:10:20.680 --> 02:10:25.680]   The following week, Ken Cascenda will be our guest,
[02:10:25.680 --> 02:10:27.680]   Megan Moroni interviewed him last week.
[02:10:27.680 --> 02:10:30.680]   He was one of the designers of the iPhone and talks about the
[02:10:30.680 --> 02:10:35.680]   culture at Apple and how it was designing the iPhone in the
[02:10:35.680 --> 02:10:37.680]   very beginnings.
[02:10:37.680 --> 02:10:40.680]   So we have some really good triangulations coming up, and
[02:10:40.680 --> 02:10:43.680]   those come out for download Friday afternoon, but you can
[02:10:43.680 --> 02:10:47.680]   watch us do it live usually 3 p.m.
[02:10:47.680 --> 02:10:49.680]   Pacific time on Friday.
[02:10:49.680 --> 02:10:50.680]   Thank you Stacey.
[02:10:50.680 --> 02:10:56.680]   Higginbothamstacey on IOT.com at the gigastacey on the
[02:10:56.680 --> 02:10:57.680]   Twitter still.
[02:10:57.680 --> 02:11:02.680]   I haven't listed my Twitter handle in years, so I guess I
[02:11:02.680 --> 02:11:03.680]   don't have to say anything.
[02:11:03.680 --> 02:11:06.680]   That's one of the best places to find me, because I actually
[02:11:06.680 --> 02:11:08.680]   check that most every day.
[02:11:08.680 --> 02:11:11.680]   Whereas Facebook, I think you get an automated message that
[02:11:11.680 --> 02:11:13.680]   says, "I'm not very good at Facebook.
[02:11:13.680 --> 02:11:15.680]   I do check it.
[02:11:15.680 --> 02:11:18.680]   I'm not on it all the time."
[02:11:18.680 --> 02:11:21.680]   Yeah, I've slowly been eliminating always of reaching me.
[02:11:21.680 --> 02:11:23.680]   I don't read email.
[02:11:23.680 --> 02:11:26.680]   I don't accept tweets.
[02:11:26.680 --> 02:11:29.680]   I don't want to talk to you, so don't try.
[02:11:29.680 --> 02:11:31.680]   Joan Donovan, I love talking to you.
[02:11:31.680 --> 02:11:34.680]   Data, society.net.
[02:11:34.680 --> 02:11:37.680]   She's a researcher there, and as you can tell, talks a lot
[02:11:37.680 --> 02:11:42.680]   about fake news, platforms, social media.
[02:11:42.680 --> 02:11:43.680]   It's always a pleasure.
[02:11:43.680 --> 02:11:47.680]   Anything you want to plug at Boston Joan on Twitter?
[02:11:47.680 --> 02:11:51.680]   No, it's just keep on internet.
[02:11:51.680 --> 02:11:52.680]   That's about it.
[02:11:52.680 --> 02:11:53.680]   Keep on the internet.
[02:11:53.680 --> 02:11:54.680]   Keep on--
[02:11:54.680 --> 02:11:55.680]   Stay online.
[02:11:55.680 --> 02:11:56.680]   Keep on internet.
[02:11:56.680 --> 02:11:57.680]   I'll do it.
[02:11:57.680 --> 02:11:58.680]   I'll do it.
[02:11:58.680 --> 02:11:59.680]   I'll do it.
[02:11:59.680 --> 02:12:03.680]   I'm not-- yeah, now you've made me a believer.
[02:12:03.680 --> 02:12:04.680]   I'm going to open it back up.
[02:12:04.680 --> 02:12:07.680]   I haven't been on Massadon a few months, but I think I need to
[02:12:07.680 --> 02:12:08.680]   try.
[02:12:08.680 --> 02:12:10.680]   I'd be really curious what you think about it.
[02:12:10.680 --> 02:12:12.680]   You're a good person.
[02:12:12.680 --> 02:12:13.680]   I think too.
[02:12:13.680 --> 02:12:14.680]   You're a good person, Joan.
[02:12:14.680 --> 02:12:15.680]   Well, you're just a good person.
[02:12:15.680 --> 02:12:17.680]   Yeah, let's just leave it at that.
[02:12:17.680 --> 02:12:18.680]   Show's over.
[02:12:18.680 --> 02:12:19.680]   Show's over.
[02:12:19.680 --> 02:12:21.680]   She's a good person.
[02:12:21.680 --> 02:12:22.680]   Thank you all for being here.
[02:12:22.680 --> 02:12:25.680]   We do this week in Google every Wednesday, 130, Pacific,
[02:12:25.680 --> 02:12:28.680]   430, Eastern, 20, 30 UTC.
[02:12:28.680 --> 02:12:30.680]   You can watch us live at twit.tv/live.
[02:12:30.680 --> 02:12:31.680]   You can join us in the studio.
[02:12:31.680 --> 02:12:33.680]   Email tickets at twit.tv if you're going to be in
[02:12:33.680 --> 02:12:36.680]   Petaluma, California anytime soon.
[02:12:36.680 --> 02:12:38.680]   But do email us because sometimes I'm not here.
[02:12:38.680 --> 02:12:40.680]   Sometimes the door is locked.
[02:12:40.680 --> 02:12:43.680]   It's just a good idea to check ahead and make sure we're here.
[02:12:43.680 --> 02:12:47.680]   You can also listen on demand to anything or watch.
[02:12:47.680 --> 02:12:50.680]   We make audio and video of all our shows available at twit.tv.
[02:12:50.680 --> 02:12:54.680]   In this case, twit.tv/twig.
[02:12:54.680 --> 02:12:56.680]   Show comes out Wednesday afternoon after the live taping.
[02:12:56.680 --> 02:12:59.680]   Usually takes us an hour or two to get it out maybe a little bit longer.
[02:12:59.680 --> 02:13:02.680]   You could subscribe to just find your favorite podcaster.
[02:13:02.680 --> 02:13:04.680]   That way you don't have to worry podcast application.
[02:13:04.680 --> 02:13:07.680]   You don't have to worry about schedules.
[02:13:07.680 --> 02:13:10.680]   It'll just appear on your phone when it's ready and you'll be able to listen.
[02:13:10.680 --> 02:13:12.680]   A minute it's available.
[02:13:12.680 --> 02:13:14.680]   Thank you so much for being here.
[02:13:14.680 --> 02:13:17.680]   And I'll see you next time on This Week in Kugel.
[02:13:17.680 --> 02:13:18.680]   Bye-bye.
[02:13:18.680 --> 02:13:28.680]   [Music]

