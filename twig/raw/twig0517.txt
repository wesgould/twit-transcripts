;FFMETADATA1
title=...And His Last Name was Higginbotham
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=517
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.840]   It's time for Twig this week in Google, Jeff and Stacey are here. There is a lot to talk about,
[00:00:05.840 --> 00:00:12.640]   including the amazing kind of revolution at VidCon. It's all about TikTok, baby. And by the way,
[00:00:12.640 --> 00:00:18.240]   is the next president going to be a former YouTube influencer, Google, Apple, Amazon, and
[00:00:18.240 --> 00:00:25.920]   Facebook, all head to Congress. We'll talk about that testimony and the coverage and a baby named
[00:00:25.920 --> 00:00:34.240]   Google. It's all coming up next on Twig. Netcast you love from people you trust.
[00:00:34.240 --> 00:00:48.880]   This is Twig. This is Twig. This week in Google, episode 517, recorded Wednesday, July 17th,
[00:00:48.880 --> 00:00:56.080]   2019. And his last name was Higginbotham. This week in Google is brought to you by
[00:00:56.080 --> 00:01:03.120]   Worldwide Technology. WWT's Advanced Technology Center is like no other testing and research lab
[00:01:03.120 --> 00:01:08.160]   with more than half a billion dollars of equipment, including great OEMs like NetApp.
[00:01:08.160 --> 00:01:13.840]   And it's virtual so you can access it 24/7. To learn more and get insights into all that it offers,
[00:01:13.840 --> 00:01:21.600]   go to www.t.com/twit. It's time for Twig this week in Google to show where we cover the latest
[00:01:21.600 --> 00:01:26.800]   of the Google verse, which includes the Facebook verse, the Twitter verse, the thing of verse,
[00:01:26.800 --> 00:01:34.720]   and the Stacey Higginbotham verse. Hello Stacey from stacey.net.com and the iotpodcast.com.
[00:01:34.720 --> 00:01:40.880]   Kevin was great last week, filled in for you. But it's we miss you Kevin. We always miss you
[00:01:40.880 --> 00:01:48.960]   when you're not here. Yeah. I had my Botox yesterday. So we should be okay when Stacey gets the Botox.
[00:01:48.960 --> 00:01:57.040]   This is not cosmetic. She does not need in any way shape or form cosmetic help. This is for your
[00:01:57.040 --> 00:02:03.200]   headaches, but it works. It does. It works really well. It's amazing. I just can't. I can't. Go ahead.
[00:02:03.200 --> 00:02:07.760]   Go ahead. Go ahead. Go ahead. Go ahead. Try. Try. That's all I can do. I can't wrinkle it. I'm
[00:02:07.760 --> 00:02:14.160]   trying so hard. No, stop. Don't. I don't want you to get a headache. Also with us at Jeff Jarvis,
[00:02:14.160 --> 00:02:20.240]   he is a headache. Buzz machines.com, professor of journalism at the townite graduate center for
[00:02:20.240 --> 00:02:25.520]   journalism at the city university. New York. Always a pleasure. Jeff, happy birthday. He's not
[00:02:25.520 --> 00:02:30.880]   going to like it. He doesn't want me to say it. But he's a he celebrated his birthday this week. Yeah.
[00:02:31.840 --> 00:02:40.480]   Yeah. You're older and wiser and grumpy. Look at you. Okay, Jeff. I'm very grumpy.
[00:02:40.480 --> 00:02:47.600]   Very great. Jeff. Jeff. Jeff's been playing with face app, which is that. I'm going to send it
[00:02:47.600 --> 00:02:52.400]   to Carson. Send it to Carson so he can show it. That's that crazy app. You want to show yours first?
[00:02:52.400 --> 00:02:56.880]   Yeah. Mine's going to be funnier. No, mine. Mine. Look, I look so old. I guarantee you.
[00:02:57.600 --> 00:03:03.040]   Look how old I look in that. It's horrible. That's terrifying. Terrifying. So,
[00:03:03.040 --> 00:03:07.920]   no, you want to see the. I don't want to look. I don't want to spoil yours, Jeff, because mine
[00:03:07.920 --> 00:03:12.800]   looks so old. I want to see yours first. Yours is going to run as it be worse. I guarantee it.
[00:03:12.800 --> 00:03:17.600]   All right. I'll show you mine. And then you show me yours. Okay. This is what face app, which is a
[00:03:17.600 --> 00:03:25.520]   viral app. This is the this went viral about eight months ago or something. And now I just come back.
[00:03:25.520 --> 00:03:28.400]   I guess they have new features. But one of the features is it'll aid you.
[00:03:28.400 --> 00:03:34.240]   Look at that. Waddle. It's so gross. The waddle is the one thing I actually have.
[00:03:34.240 --> 00:03:37.760]   But thanks very much, Stacy. No, no. It's all about the waddle.
[00:03:37.760 --> 00:03:45.600]   I think I look distinguished. Actually, I look like Freddy Krueger for crying out loud.
[00:03:45.600 --> 00:03:50.160]   That's okay. Wait a minute. Jeff's worse. Jeff's yours is worse.
[00:03:51.280 --> 00:03:56.560]   How could yours be worse? All right. Carson's got to pull it up here. I think it's a pretty cool app.
[00:03:56.560 --> 00:04:00.880]   I mean, I don't know if I will look like that's exactly what you're going to look like.
[00:04:00.880 --> 00:04:03.920]   That's what I think you're going to look like. Thank you. Thank you.
[00:04:03.920 --> 00:04:09.120]   It's ancient mariner. Although your I your glasses are being swallowed by your face at the
[00:04:09.120 --> 00:04:16.480]   swim. So I think you know, they've been on for 40 years. Jeff, the wrinkles are growing around.
[00:04:16.480 --> 00:04:22.400]   Jeff, this is about 15 years from now. Jeff, I got better for you. Shut up for both of us.
[00:04:22.400 --> 00:04:28.880]   I'm a couple of years behind you. That's all. I think I think now here's the thing.
[00:04:28.880 --> 00:04:32.000]   At what point do we stop doing video in these podcasts?
[00:04:32.000 --> 00:04:37.840]   What do you think Jeff? Stacy, you got to do this too. You have face up yet? I want to see
[00:04:37.840 --> 00:04:44.080]   what Stacy looks like. I didn't do it because I don't do those knowing that they use my photograph
[00:04:44.080 --> 00:04:47.120]   for all kinds of things. Well, and that it's funny because that is the story.
[00:04:47.120 --> 00:04:53.040]   You now have lost the rights to your face, to that photo. You now no longer have copyright.
[00:04:53.040 --> 00:05:01.760]   So the way this works is you upload it to the cloud because they have to do the editing in the
[00:05:01.760 --> 00:05:12.080]   cloud. But this app comes from Russia and the apps creators are harvesting metadata.
[00:05:13.040 --> 00:05:19.280]   Close research, this is from the Verge. Close research suggests face app isn't doing anything
[00:05:19.280 --> 00:05:26.480]   particularly unusual in its code or its network traffic. However, they do have your picture now.
[00:05:26.480 --> 00:05:32.960]   That's the issue. An index executive and CEO of the Russian company that created the app,
[00:05:32.960 --> 00:05:38.720]   Yarslav Gonsharov, said that photos uploaded the app are stored in the company's servers to
[00:05:38.720 --> 00:05:43.520]   save bandwidth if several filters are applied, but they will get deleted not too long after.
[00:05:43.520 --> 00:05:50.560]   And because of GDPR, I would imagine, according to TechCrunch, face app says you can request
[00:05:50.560 --> 00:05:55.600]   that your face be removed from our server. However, if you do, you'll get a message saying the
[00:05:55.600 --> 00:06:00.960]   team's currently overloaded, but users can send the request through settings, support, report,
[00:06:00.960 --> 00:06:08.640]   a bug with the word privacy in the subject line. But they're in Russia. So, you know,
[00:06:08.640 --> 00:06:15.840]   but everybody's got my face, right? Yeah, exactly. Well, yes. In all of these apps,
[00:06:15.840 --> 00:06:21.280]   I mean, when you looked at Microsoft, was it wasn't hot or not, Microsoft had one where you did your
[00:06:21.280 --> 00:06:26.400]   face. Same thing, they predict your age. Yeah. In fact, Microsoft came out of Microsoft research.
[00:06:26.400 --> 00:06:32.640]   That was so they could get face data. Right. And Google did a very similar effort with a couple
[00:06:32.640 --> 00:06:41.520]   of their apps, actually. They had the art face app thing. So, so this is very common. It's just,
[00:06:41.520 --> 00:06:46.160]   I don't, I don't like participating in them. Yeah. That makes sense. You just don't want to
[00:06:46.160 --> 00:06:51.040]   imagine yourself with wrinkles as you have none. You're physically incapable of wrinkly now.
[00:06:51.040 --> 00:06:54.480]   Do you think face app would detect the Botox and say, well, you're actually going to look
[00:06:54.480 --> 00:07:00.880]   exactly the same? Well, only my forehead. I'm gonna, I don't have, I don't have Botox anywhere else.
[00:07:00.880 --> 00:07:05.120]   So, okay. See, look, there's wrinkles right by my eyes. I'm smiling. It's perfect.
[00:07:05.120 --> 00:07:12.960]   Perfect. You're perfect. This is the same issue with TikTok, which is a Chinese-based company that
[00:07:12.960 --> 00:07:21.760]   kids all over the world are uploading videos to crazily. In fact, now, Jeff, you and I wanted
[00:07:21.760 --> 00:07:24.240]   to go to VidCon, but didn't. Right.
[00:07:24.240 --> 00:07:30.640]   Apparently, this is from an Atlantic article, Taylor Lawrence, saying TikTok stars are preparing to
[00:07:30.640 --> 00:07:38.160]   take over the internet at the VidCon Party. YouTube hosted its annual party, influencers,
[00:07:38.160 --> 00:07:44.880]   industry professionals, sipping beer, eating vegetable skewers as DJs blasted pop hits.
[00:07:44.880 --> 00:07:50.480]   Taylor writes the party felt like a generic corporate-sponsored event, probably because the
[00:07:50.480 --> 00:07:57.040]   real party was in 1.9 miles down the road at a private event hosted by TikTok. It was a private
[00:07:57.040 --> 00:08:03.360]   event for creators only, no marketers, no fans, but it was by 815, 15 minutes out of the party
[00:08:03.360 --> 00:08:09.120]   started at the Bulmore lanes in LA. It was so packed that hundreds of creators were left standing
[00:08:09.120 --> 00:08:17.760]   outside. Sam Co, he has 300,000 followers, arrived 15 minutes early by 10. I'm sorry,
[00:08:17.760 --> 00:08:22.880]   she was still trying to get in. I'm annoyed we drove all the way here from Santa Monica. Oh,
[00:08:22.880 --> 00:08:30.240]   aw, I see other huge creators here too, and they're still waiting in line. So, the party,
[00:08:30.240 --> 00:08:37.840]   by the way, they loaded this is gummy bears on the bowling alley, seems like a bad idea.
[00:08:41.840 --> 00:08:48.160]   People are already saying that TikTok is the next big thing. And if VidCon, which really was
[00:08:48.160 --> 00:08:55.520]   created by YouTubers to highlight YouTubers, is taken over by TikTok, I mean, can the world be far
[00:08:55.520 --> 00:09:02.400]   behind? But also goes to say, no platform is forever. Yeah, that's a good point.
[00:09:02.400 --> 00:09:08.160]   I've got a story on the rundown saying Google searches is passÃ© now, there's all these other
[00:09:08.160 --> 00:09:12.320]   searches out there, all this fear that they're all going to take over the world and own it forever.
[00:09:12.320 --> 00:09:16.880]   So far, I'll actually stay tuned for my tip though, because I have a tip at the end.
[00:09:16.880 --> 00:09:22.480]   I discovered a little something and I'm going to tip for you that will help you if you wish to
[00:09:22.480 --> 00:09:29.200]   wean yourself from the Goog. I want you to say, we do get new platforms, they just get purchased by
[00:09:29.200 --> 00:09:35.840]   the bigger players. So think about WhatsApp and Instagram. TikTok is not for sale, however.
[00:09:35.840 --> 00:09:43.760]   Fact. But Musically was most of a game musically. Right. And not Yandex. What is the name of the
[00:09:43.760 --> 00:09:48.320]   Chinese company? I do. Is it Baidu who owns? Oh, no. They're owned by Tencent.
[00:09:48.320 --> 00:09:52.640]   They are owned by Tencent. I think so. They spent, get ready for this,
[00:09:52.640 --> 00:10:01.680]   $1 billion on advertising last year, $1 billion. And they paid one influencer, a million dollars
[00:10:01.680 --> 00:10:07.840]   for a 15 second video. They're actively wooing YouTube's creators. First quarter of 2019,
[00:10:07.840 --> 00:10:12.560]   the third most installed app worldwide, right behind WhatsApp and Facebook messenger,
[00:10:12.560 --> 00:10:19.440]   1.2 billion monthly users globally. That's bigger than Instagram. And coming up on YouTube,
[00:10:19.440 --> 00:10:26.080]   which is 1.9 and Facebook itself, of course, more than 2 billion. TikTok is big, but this is the
[00:10:26.080 --> 00:10:31.040]   point every video you create on TikTok is sent to China. Sure. You want to hear some
[00:10:31.040 --> 00:10:35.920]   talk for a second and look at some? Sure. Go ahead. Something funny while I fire up TikTok.
[00:10:35.920 --> 00:10:43.200]   So my daughter is a, she is young. She turns 13 in a month. And her favorite thing to do on
[00:10:43.200 --> 00:10:50.080]   TikTok is to watch recreations of popular old vines. It is mind blowing.
[00:10:50.080 --> 00:10:53.280]   Wow. How meta. Because frankly, TikTok is vine.
[00:10:53.280 --> 00:10:59.040]   It is vine. And she just, like, she speaks only in memes. And it is her, like,
[00:10:59.840 --> 00:11:04.240]   I should have her come on the show and do a guide to what teens think in the meme scene.
[00:11:04.240 --> 00:11:09.520]   But yeah, it's pretty wild. And there are some fun TikToks.
[00:11:09.520 --> 00:11:17.760]   No, it's just as vine was a great platform for people to make what was a six second videos
[00:11:17.760 --> 00:11:23.040]   that were hysterical in many cases. And these people were propelled to fame and fortune.
[00:11:23.040 --> 00:11:28.720]   You got a Twitter bot vine and killed it. You got to think Twitter is looking at TikTok saying,
[00:11:28.800 --> 00:11:39.200]   Hmm. What have we done? I can't show TikTok videos from TikTok.
[00:11:39.200 --> 00:11:43.680]   Yeah, you can. Well, I do it on my phone. Go to trending. Go to trending.
[00:11:43.680 --> 00:11:47.920]   TikTok. Oh, go to, oh, I see it's on the website. If I go to trending hashtags.
[00:11:47.920 --> 00:11:52.560]   Yeah. Okay. So let's turn on some sound and see what these, see what these guys.
[00:11:52.560 --> 00:11:58.080]   It's Marvin Howard. Oh, the catty. Oh, she's me.
[00:11:58.080 --> 00:12:03.760]   So these are influencers talking about influencers.
[00:12:03.760 --> 00:12:04.960]   A VidCon.
[00:12:04.960 --> 00:12:08.960]   It's over. Oh, what's up Dakota? Let's go.
[00:12:08.960 --> 00:12:12.800]   That's, that's, I guess, good. If you know who those people are,
[00:12:12.800 --> 00:12:15.600]   how about this one? This is a kid.
[00:12:18.000 --> 00:12:22.960]   Because remember the part of this was originally to, as musically to lip sync to existing music.
[00:12:22.960 --> 00:12:27.760]   So they're eating cotton candy and the guys in the thing with a mask and
[00:12:27.760 --> 00:12:35.920]   there's too much base. My amp can't handle it. I don't, I don't know what that is. How about
[00:12:35.920 --> 00:12:38.320]   what? Oh, Mer's glue. Oh, Mer's glue.
[00:12:38.320 --> 00:12:41.920]   Elmer's glue color changing glue. Ladies and gentlemen, I give you here's some sandals.
[00:12:41.920 --> 00:12:46.160]   We squirt color changing glue on the sandals and sprinkle them.
[00:12:46.800 --> 00:12:48.640]   That was exciting with sprinkles.
[00:12:48.640 --> 00:12:54.080]   And now you have candy shoes. They're terribly uncomfortable to wear.
[00:12:54.080 --> 00:12:55.280]   Oh, so painful.
[00:12:55.280 --> 00:13:02.000]   What if you just put paper in the bottom and then did it be so much better?
[00:13:02.000 --> 00:13:10.640]   There is a wonderful subreddit on Reddit called DIYed spilt why and that belongs on DIY.
[00:13:12.800 --> 00:13:17.600]   Then there's, then there's just people singing songs. There's little hoodie.
[00:13:17.600 --> 00:13:30.400]   I see why this can speak to the young's. So your daughter likes it.
[00:13:30.400 --> 00:13:35.040]   She doesn't like, she only likes the recreation of the vines.
[00:13:35.040 --> 00:13:36.240]   Oh, that's all she watches.
[00:13:36.240 --> 00:13:38.080]   So she doesn't like, that's all she watches.
[00:13:39.360 --> 00:13:42.400]   And why I don't know. Let's see what hashtag pets.
[00:13:42.400 --> 00:13:45.280]   The death of vine left a huge hole in many, many people's.
[00:13:45.280 --> 00:13:48.160]   Many people's lives. They were bereft.
[00:13:48.160 --> 00:13:51.040]   She thinks she thinks a lot of them are a little too
[00:13:51.040 --> 00:13:57.280]   mean or risque or stupid in a not fun way.
[00:13:57.280 --> 00:13:57.920]   Yeah.
[00:13:57.920 --> 00:14:00.000]   She's kind of an absurdist though.
[00:14:00.000 --> 00:14:02.480]   Here's from the hashtag pets are people.
[00:14:02.480 --> 00:14:06.880]   A vine has been viewed more than 1.3 million times.
[00:14:06.880 --> 00:14:09.040]   Did I say vine? I meant TikTok.
[00:14:09.760 --> 00:14:12.160]   It's just a cat sitting there.
[00:14:12.160 --> 00:14:16.720]   Very fat cat. 1.2 million views.
[00:14:16.720 --> 00:14:18.000]   How long?
[00:14:18.000 --> 00:14:20.240]   There's a big investment in watching. Let's put it that way though.
[00:14:20.240 --> 00:14:22.160]   Well, that's the point, right? You're sitting on your phone.
[00:14:22.160 --> 00:14:23.680]   You're waiting in line at the DMV.
[00:14:23.680 --> 00:14:25.840]   It's just another meme thing.
[00:14:25.840 --> 00:14:27.760]   I see kids doing this a lot.
[00:14:27.760 --> 00:14:29.120]   Teenagers doing this a lot.
[00:14:29.120 --> 00:14:30.480]   They pass around memes.
[00:14:30.480 --> 00:14:35.040]   We're at dinner and Michael and his friends.
[00:14:35.040 --> 00:14:40.000]   He's 16 are handing their phone around and looking at a meme and laughing.
[00:14:40.000 --> 00:14:42.320]   They're not that funny.
[00:14:42.320 --> 00:14:47.760]   I'm thinking this is really degrading the human's experience.
[00:14:47.760 --> 00:14:53.040]   Instead of having a conversation, it's a meme-based conversation.
[00:14:53.040 --> 00:14:53.920]   It's very strange.
[00:14:53.920 --> 00:14:55.600]   Let's see.
[00:14:55.600 --> 00:15:00.000]   I don't want this to become the old talking about those crazy kids.
[00:15:00.000 --> 00:15:02.400]   I'm sure this is great.
[00:15:02.400 --> 00:15:03.280]   It's a new one.
[00:15:03.280 --> 00:15:04.800]   We're as old as our pictures, Leo.
[00:15:04.800 --> 00:15:06.640]   That's pretty damn old.
[00:15:06.640 --> 00:15:07.440]   Yeah.
[00:15:07.440 --> 00:15:08.480]   That's pretty.
[00:15:08.480 --> 00:15:12.880]   So, did you let your daughter do TikToks knowing that her image is now being sent to China?
[00:15:12.880 --> 00:15:14.800]   She doesn't want to do TikToks.
[00:15:14.800 --> 00:15:20.320]   She's not allowed and she doesn't want to be actually online doing these things.
[00:15:20.320 --> 00:15:22.080]   So I don't have to worry about that for now.
[00:15:22.080 --> 00:15:24.800]   I'll be interested because she's going to a new school.
[00:15:24.800 --> 00:15:28.240]   Her old school had a no smartphone at all policy and this new school does.
[00:15:29.440 --> 00:15:30.480]   And so we'll see.
[00:15:30.480 --> 00:15:35.680]   And all the parents who were in the class, we all got together and did a no smartphone kind of pledge.
[00:15:35.680 --> 00:15:37.440]   So we'll see.
[00:15:37.440 --> 00:15:41.760]   My son makes a lot of memes, but he doesn't actually like to appear in them.
[00:15:41.760 --> 00:15:42.560]   He just likes to...
[00:15:42.560 --> 00:15:43.920]   Yeah.
[00:15:43.920 --> 00:15:46.080]   She makes lots of memes.
[00:15:46.080 --> 00:15:49.200]   He likes to get copyright take down notices, basically.
[00:15:49.200 --> 00:15:52.560]   So, do you use a meme generator or does he do it by hand?
[00:15:52.560 --> 00:15:54.000]   No, he just edits by hand.
[00:15:54.000 --> 00:15:54.720]   By hand.
[00:15:54.720 --> 00:15:56.400]   And how about your daughter, Stacy?
[00:15:56.400 --> 00:15:59.120]   She edits her own.
[00:15:59.120 --> 00:16:00.400]   So, in a way, this is good.
[00:16:00.400 --> 00:16:01.120]   You're learning skills.
[00:16:01.120 --> 00:16:04.160]   You're learning comedy, writing, pithy comedy, writing.
[00:16:04.160 --> 00:16:05.440]   You're learning photo shopping.
[00:16:05.440 --> 00:16:07.040]   Rurody.
[00:16:07.040 --> 00:16:07.760]   Yeah, brevity.
[00:16:07.760 --> 00:16:10.720]   I mean, I think it's...
[00:16:10.720 --> 00:16:11.680]   Yeah, I think it's not...
[00:16:11.680 --> 00:16:13.200]   I mean, there's some skills in that.
[00:16:13.200 --> 00:16:15.120]   You'll be great at PowerPoint presentations.
[00:16:15.120 --> 00:16:16.640]   Oh, goody.
[00:16:16.640 --> 00:16:20.320]   Yay, PowerPoint.
[00:16:20.320 --> 00:16:27.360]   I mean, I spent from the age of probably 17 or 18 till now,
[00:16:28.240 --> 00:16:29.440]   learning this medium.
[00:16:29.440 --> 00:16:31.680]   But this is a...
[00:16:31.680 --> 00:16:32.880]   I don't know if it's a dying medium.
[00:16:32.880 --> 00:16:35.200]   Thanks to podcasting, it's not Hallelujah.
[00:16:35.200 --> 00:16:38.480]   But that's what you do.
[00:16:38.480 --> 00:16:40.560]   You start young and you're learning a new medium.
[00:16:40.560 --> 00:16:44.320]   Do you think in 20, 30 years,
[00:16:44.320 --> 00:16:46.400]   well, I'll just be looking at memes all the time.
[00:16:46.400 --> 00:16:49.520]   They're nice.
[00:16:49.520 --> 00:16:52.320]   I mean, so people make fun of emojis.
[00:16:52.320 --> 00:16:53.440]   People make fun of gifts.
[00:16:53.440 --> 00:16:57.520]   But I do think they're a really good, quick way to get an emotion
[00:16:57.520 --> 00:16:59.360]   or point across that's kind of fun.
[00:16:59.360 --> 00:17:03.440]   And I think we'll have the same sort of thing with TikTok
[00:17:03.440 --> 00:17:07.040]   or short form videos that are not gifts.
[00:17:07.040 --> 00:17:11.600]   I mean, they're ways to share an emotion or a mood or how you're feeling.
[00:17:11.600 --> 00:17:15.040]   Yeah, I mean, in a way, emojis themselves are a new way of communicating, right?
[00:17:15.040 --> 00:17:20.000]   They're a non-verbal communication that a lot of young people
[00:17:20.000 --> 00:17:22.640]   are able to use quite adeptly to say a lot of things.
[00:17:24.320 --> 00:17:29.280]   So I'm fascinated about the, I mentioned last week,
[00:17:29.280 --> 00:17:32.640]   I'm reading this book about dictionary wars with Webster,
[00:17:32.640 --> 00:17:34.080]   who turns out to be a bit of a jerk.
[00:17:34.080 --> 00:17:34.640]   Oh yeah.
[00:17:34.640 --> 00:17:36.400]   Oh yeah, so surprised.
[00:17:36.400 --> 00:17:39.280]   Don't spell it that way.
[00:17:39.280 --> 00:17:43.440]   But the desire for standardization,
[00:17:43.440 --> 00:17:46.800]   Caxton, who was the first major publisher,
[00:17:46.800 --> 00:17:50.160]   Printer in the UK wanted to standardize the language.
[00:17:50.160 --> 00:17:52.800]   Webster wanted to standardize American as the language.
[00:17:53.520 --> 00:17:57.200]   Johnson, there was once you could with printing, sorry guys, good moment.
[00:17:57.200 --> 00:18:01.120]   But now what the internet does is free us up again.
[00:18:01.120 --> 00:18:04.720]   Although I'll come back into our alphabets and our expression.
[00:18:04.720 --> 00:18:05.920]   I don't know if that's a great thing,
[00:18:05.920 --> 00:18:12.400]   because if you read 18th century manuscripts, it's crazy.
[00:18:12.400 --> 00:18:14.000]   Talk about non-standard.
[00:18:14.000 --> 00:18:19.360]   They don't Shakespeare, they spelled his name five different ways, his own name.
[00:18:20.560 --> 00:18:24.080]   So I don't think standard, I don't know, maybe it's because I grew up in a world
[00:18:24.080 --> 00:18:25.920]   where spelling was standardized.
[00:18:25.920 --> 00:18:29.360]   Was it Webster who said, let's not use the British O-U-R?
[00:18:29.360 --> 00:18:33.120]   Oh yeah, Webster had all kinds of, he was very controversial.
[00:18:33.120 --> 00:18:36.640]   He went for a lot of, sorry, he didn't put that.
[00:18:36.640 --> 00:18:40.560]   He took out the U in color, because we should have an American way
[00:18:40.560 --> 00:18:42.400]   to take out all kinds of other things.
[00:18:42.400 --> 00:18:47.760]   To the Z in the S, in organized and apologized.
[00:18:47.760 --> 00:18:48.800]   Oh, that's right, yeah, yeah.
[00:18:50.000 --> 00:18:52.800]   Yeah, organized and spelled with a Z, a Z.
[00:18:52.800 --> 00:18:55.280]   But a lot of those things didn't last in the UK.
[00:18:55.280 --> 00:18:59.040]   Webster's spellings.
[00:18:59.040 --> 00:19:03.440]   Two, now we have somebody from Portsmouth in England in the studio.
[00:19:03.440 --> 00:19:06.320]   When you look at the bizarre American spellings of color
[00:19:06.320 --> 00:19:10.640]   and organized, do you think, how illiterate these colonies are?
[00:19:10.640 --> 00:19:14.800]   I'm a software programmer, so I have a doctor in the American.
[00:19:14.800 --> 00:19:18.080]   You have, that's, oh by the way, there's another interesting story,
[00:19:18.080 --> 00:19:23.680]   Americanizations because of coding have become universal.
[00:19:23.680 --> 00:19:29.840]   There's also interesting misspellings that have become universal.
[00:19:29.840 --> 00:19:36.800]   In the HT, ML spec or HTTP spec, there's a referrer, right?
[00:19:36.800 --> 00:19:39.600]   Which has been historically misspelled with one R.
[00:19:39.600 --> 00:19:43.280]   And I think it's one, is it one R or one F?
[00:19:43.280 --> 00:19:46.320]   Anyway, it's misspelled, but because that's how it's spelled,
[00:19:47.440 --> 00:19:48.720]   you have to spell it that way.
[00:19:48.720 --> 00:19:51.120]   And that's, I think, it's fascinating.
[00:19:51.120 --> 00:19:54.640]   I'm reading the book you recommended, Drexler's.
[00:19:54.640 --> 00:19:55.680]   Was Drexler's English?
[00:19:55.680 --> 00:19:56.160]   It's really--
[00:19:56.160 --> 00:19:57.120]   The dryers English.
[00:19:57.120 --> 00:19:58.640]   Dreyers English, wonderful.
[00:19:58.640 --> 00:19:59.360]   Isn't it good?
[00:19:59.360 --> 00:19:59.760]   Yeah.
[00:19:59.760 --> 00:20:02.880]   And I'm listening, on Audible, they have the great courses.
[00:20:02.880 --> 00:20:05.520]   And I'm listening to a course in the English language
[00:20:05.520 --> 00:20:06.960]   and how it's changed.
[00:20:06.960 --> 00:20:10.000]   And it's interesting, there's grammatic cessation,
[00:20:10.000 --> 00:20:13.360]   there's syntax change, and it's a very--
[00:20:13.360 --> 00:20:14.720]   Saturation fights.
[00:20:14.720 --> 00:20:18.400]   The words silly used to mean saintly.
[00:20:18.400 --> 00:20:24.560]   And so there's Dreyers English, an utterly correct guide
[00:20:24.560 --> 00:20:27.760]   to clarity and style, which actually is very--
[00:20:27.760 --> 00:20:29.040]   It's actually very fun and loose.
[00:20:29.040 --> 00:20:30.640]   He's not one of those.
[00:20:30.640 --> 00:20:35.520]   Yeah, is it sharp and funny as per the little blur?
[00:20:35.520 --> 00:20:36.000]   Yes, it is.
[00:20:36.000 --> 00:20:37.440]   Yes, it is.
[00:20:37.440 --> 00:20:38.640]   And he uses--
[00:20:38.640 --> 00:20:40.400]   And he believes in the Oxford comma.
[00:20:40.400 --> 00:20:41.360]   God bless him.
[00:20:41.360 --> 00:20:41.840]   Yeah.
[00:20:41.840 --> 00:20:44.080]   He says there's no reason not to use the Oxford comma.
[00:20:45.040 --> 00:20:46.480]   So he got rid of--
[00:20:46.480 --> 00:20:46.880]   Or does he--
[00:20:46.880 --> 00:20:48.720]   As he calls it, by the way, the serial comma.
[00:20:48.720 --> 00:20:50.720]   It is not the Oxford comma.
[00:20:50.720 --> 00:20:51.200]   That's right.
[00:20:51.200 --> 00:20:52.480]   It's the Oxford serial comma.
[00:20:52.480 --> 00:20:54.160]   He got rid of--
[00:20:54.160 --> 00:20:57.840]   A Webster got rid of redundant letters like the E from Fure,
[00:20:57.840 --> 00:21:02.640]   the K from Frallocken Music, the U from Artor, Endeavour and Terror.
[00:21:02.640 --> 00:21:04.800]   He wanted to get rid of--
[00:21:04.800 --> 00:21:07.440]   He switched the RE, the ER with meter--
[00:21:07.440 --> 00:21:08.240]   Theater.
[00:21:08.240 --> 00:21:08.880]   Sopter.
[00:21:08.880 --> 00:21:10.400]   Yeah, theater.
[00:21:10.400 --> 00:21:10.400]   Theater.
[00:21:12.160 --> 00:21:16.400]   He switched the CE to CE on Expanse, Defense, Pretense.
[00:21:16.400 --> 00:21:17.840]   But what he's done now is he's given--
[00:21:17.840 --> 00:21:19.520]   Ready to come down on whiskey.
[00:21:19.520 --> 00:21:21.360]   Who's the comma?
[00:21:21.360 --> 00:21:22.480]   Who's the comma?
[00:21:22.480 --> 00:21:27.760]   What he's done is given Americans the opportunity to be very pretentious
[00:21:27.760 --> 00:21:29.120]   and spell O-U-R.
[00:21:29.120 --> 00:21:31.200]   Yeah.
[00:21:31.200 --> 00:21:33.920]   And theater-- I know people who literally spell theater RE.
[00:21:33.920 --> 00:21:34.880]   The theater.
[00:21:34.880 --> 00:21:35.600]   The theater.
[00:21:35.600 --> 00:21:36.320]   The theater.
[00:21:36.320 --> 00:21:37.120]   I'm in the theater.
[00:21:37.120 --> 00:21:37.440]   The theater.
[00:21:39.360 --> 00:21:42.000]   And so really all he's done is given us the Opportunity Act.
[00:21:42.000 --> 00:21:43.760]   But it was--
[00:21:43.760 --> 00:21:46.000]   At the moment, it was very much about
[00:21:46.000 --> 00:21:48.000]   breaking away from Britain culturally.
[00:21:48.000 --> 00:21:48.560]   Yeah.
[00:21:48.560 --> 00:21:49.680]   And trying to come through your own language.
[00:21:49.680 --> 00:21:53.120]   So what I'm arguing is that the internet now has the opportunity for people to break away
[00:21:53.120 --> 00:21:57.360]   from the orthodoxy and do so not just with the letters we had,
[00:21:57.360 --> 00:21:59.760]   but with this whole new world of visual expression.
[00:21:59.760 --> 00:22:06.000]   It will be very interesting to see how the language changes
[00:22:06.000 --> 00:22:08.560]   and the rate of change and how that's changed by the internet.
[00:22:08.560 --> 00:22:12.240]   I would imagine it would be significant, right?
[00:22:12.240 --> 00:22:14.240]   Stacey, here's a question for you.
[00:22:14.240 --> 00:22:21.680]   How much language is unique to her daughter and her age that you have to ask what it means
[00:22:21.680 --> 00:22:23.040]   or you have to figure it out?
[00:22:23.040 --> 00:22:23.840]   Oh boy.
[00:22:23.840 --> 00:22:25.680]   Words, phrases, expressions.
[00:22:25.680 --> 00:22:26.400]   That's slang.
[00:22:26.400 --> 00:22:27.120]   That's happened all.
[00:22:27.120 --> 00:22:27.680]   That's--
[00:22:27.680 --> 00:22:28.160]   That's--
[00:22:28.160 --> 00:22:29.600]   Every generation is a slang.
[00:22:29.600 --> 00:22:30.560]   There's also kind of a secret language.
[00:22:30.560 --> 00:22:32.000]   And do you use the word "eat"?
[00:22:32.000 --> 00:22:36.160]   So "eat"-- they make fun of you if you use the word "eat."
[00:22:36.160 --> 00:22:36.160]   Yeah.
[00:22:36.160 --> 00:22:36.720]   Okay.
[00:22:36.720 --> 00:22:40.080]   I said on "fleet" the other day and they said, oh, that's so 2016.
[00:22:40.080 --> 00:22:40.320]   Yeah.
[00:22:40.320 --> 00:22:42.160]   So everything goes faster.
[00:22:42.160 --> 00:22:48.480]   I will say that I pick up just being on Twitter anything I need to know.
[00:22:48.480 --> 00:22:50.160]   So I had one who was talking.
[00:22:50.160 --> 00:22:52.800]   She's like, oh, I low-key something something.
[00:22:52.800 --> 00:22:54.560]   And I was like, well, I high-key.
[00:22:54.560 --> 00:22:55.360]   Da-da-da-da.
[00:22:55.360 --> 00:22:57.360]   And she was like, oh my god, you were the coolest mom ever.
[00:22:57.360 --> 00:22:59.440]   And I was like, thank you, Twitter.
[00:22:59.440 --> 00:23:00.560]   The Twitter.
[00:23:00.560 --> 00:23:01.120]   Oh, thank you.
[00:23:01.120 --> 00:23:02.960]   I'm on "fleet."
[00:23:02.960 --> 00:23:03.520]   No.
[00:23:03.520 --> 00:23:03.760]   No.
[00:23:05.920 --> 00:23:08.320]   But it was like, you know--
[00:23:08.320 --> 00:23:09.360]   I used Reddit.
[00:23:09.360 --> 00:23:09.840]   It was a moment.
[00:23:09.840 --> 00:23:10.400]   It was the same way.
[00:23:10.400 --> 00:23:12.160]   Reddit's the-- yeah, I used to write it that way.
[00:23:12.160 --> 00:23:16.720]   So linguists talk-- this is a great, by the way, really good lecture.
[00:23:16.720 --> 00:23:19.440]   I'll see if I can find it before the end of the show.
[00:23:19.440 --> 00:23:26.160]   But he talks about how language changes semantic bleaching, morphological reduction,
[00:23:26.160 --> 00:23:29.520]   phonetic erosion-- this is from the Wikipedia--
[00:23:29.520 --> 00:23:31.920]   obligatory-fication.
[00:23:32.960 --> 00:23:37.120]   That's when the use of linguistic structures becomes increasingly more obligatory
[00:23:37.120 --> 00:23:40.320]   in the process of grammaticalization.
[00:23:40.320 --> 00:23:44.880]   And actually, this Wikipedia article will be a great way
[00:23:44.880 --> 00:23:46.480]   if you wanted to know more about it.
[00:23:46.480 --> 00:23:53.120]   He talks about-- in French, I don't know, are we getting really too obscure here?
[00:23:53.120 --> 00:23:59.440]   You know, if you say-- if I want to say he doesn't walk, I say,
[00:23:59.440 --> 00:24:01.920]   Ilannomarche-pah, right?
[00:24:01.920 --> 00:24:03.520]   That's-- and there's a double negative--
[00:24:03.520 --> 00:24:04.720]   So it's never made any logic.
[00:24:04.720 --> 00:24:06.080]   Logically, I could never figure that out.
[00:24:06.080 --> 00:24:07.280]   Well, it used to be--
[00:24:07.280 --> 00:24:09.440]   In French, it used to be Ilannomarche.
[00:24:09.440 --> 00:24:11.360]   There was no pa.
[00:24:11.360 --> 00:24:13.040]   Well, so where the heck did the pa come from?
[00:24:13.040 --> 00:24:13.600]   This is no sense.
[00:24:13.600 --> 00:24:14.800]   So I think this is fascinating.
[00:24:14.800 --> 00:24:15.840]   I apologize if you don't.
[00:24:15.840 --> 00:24:17.040]   I'll do this quick.
[00:24:17.040 --> 00:24:18.000]   Pa, I mean, step.
[00:24:18.000 --> 00:24:24.160]   And so, as you probably know, in English, we'll often say, oh, she wouldn't drink a drop.
[00:24:24.160 --> 00:24:27.760]   He wouldn't walk a step as a emphatic way of saying,
[00:24:27.760 --> 00:24:29.840]   not only will he walk, he won't even walk a step.
[00:24:29.840 --> 00:24:31.360]   Same thing in French.
[00:24:31.360 --> 00:24:34.160]   Ilannomarche-pah, he wouldn't walk a step.
[00:24:34.160 --> 00:24:38.880]   But what happened is pa became absorbed into the negative of the language.
[00:24:38.880 --> 00:24:42.000]   So then now, pa is added to any negative.
[00:24:42.000 --> 00:24:46.400]   It was originally in a way of making more emphatic.
[00:24:46.400 --> 00:24:50.560]   Ha, just for that one verb, that's crazy.
[00:24:50.560 --> 00:24:52.800]   It started with that one verb and it extended to all the rest.
[00:24:52.800 --> 00:24:53.440]   Because--
[00:24:53.440 --> 00:24:54.640]   Well, and they do--
[00:24:54.640 --> 00:24:55.280]   Is that cool?
[00:24:55.280 --> 00:24:55.840]   Is that cool?
[00:24:55.840 --> 00:24:56.640]   Like in the Fecchoyah.
[00:24:56.640 --> 00:24:59.280]   Yeah. Yeah. I will. I won't do anything.
[00:24:59.280 --> 00:24:59.600]   Yeah.
[00:24:59.600 --> 00:25:00.880]   OK.
[00:25:00.880 --> 00:25:01.360]   Are he doesn't want to--
[00:25:01.360 --> 00:25:05.360]   One of the most amazing things going to English confuses the world is the meaningless do,
[00:25:05.360 --> 00:25:09.360]   how we add that verb into things for no reason whatsoever.
[00:25:09.360 --> 00:25:09.840]   Right.
[00:25:09.840 --> 00:25:10.320]   But without it--
[00:25:10.320 --> 00:25:10.880]   But without it.
[00:25:10.880 --> 00:25:11.440]   --a great verb.
[00:25:11.440 --> 00:25:15.520]   Well, there were no prepositions originally.
[00:25:15.520 --> 00:25:17.200]   How did a word--
[00:25:17.200 --> 00:25:23.840]   If you think about it, when Adam named the things, everything had a name, but it was a thing.
[00:25:24.880 --> 00:25:27.120]   What's a preposition have to do with anything?
[00:25:27.120 --> 00:25:28.480]   What does do have to do anything?
[00:25:28.480 --> 00:25:29.200]   What does it do?
[00:25:29.200 --> 00:25:31.440]   What does it do?
[00:25:31.440 --> 00:25:33.120]   What are you talking about the meaningless do?
[00:25:33.120 --> 00:25:36.080]   Give us an example of the meaningless do.
[00:25:36.080 --> 00:25:40.640]   Do you walk?
[00:25:40.640 --> 00:25:41.440]   Carson help me.
[00:25:41.440 --> 00:25:42.160]   Do you walk?
[00:25:42.160 --> 00:25:43.360]   Do you walk?
[00:25:43.360 --> 00:25:44.400]   Yeah.
[00:25:44.400 --> 00:25:45.440]   Instead of--
[00:25:45.440 --> 00:25:46.080]   Walk you?
[00:25:46.080 --> 00:25:46.480]   Walk you?
[00:25:46.480 --> 00:25:48.720]   Do you walk?
[00:25:48.720 --> 00:25:51.200]   The do doesn't really make any sense.
[00:25:51.200 --> 00:25:52.400]   It's not needed.
[00:25:52.400 --> 00:25:53.360]   It's probably another--
[00:25:53.360 --> 00:25:54.160]   If you go to Spanish--
[00:25:54.160 --> 00:25:55.600]   Oh, okay.
[00:25:55.600 --> 00:25:56.160]   I got you.
[00:25:56.160 --> 00:25:56.560]   Yes.
[00:25:56.560 --> 00:25:57.280]   Yeah, in French it's--
[00:25:57.280 --> 00:25:58.320]   I do the laundry.
[00:25:58.320 --> 00:25:59.680]   Masha vu.
[00:25:59.680 --> 00:26:00.240]   Right.
[00:26:00.240 --> 00:26:00.560]   It's--
[00:26:00.560 --> 00:26:00.880]   It's--
[00:26:00.880 --> 00:26:02.880]   Can replace any verb.
[00:26:02.880 --> 00:26:03.440]   I launder.
[00:26:03.440 --> 00:26:06.240]   Instead of I launder, I do the laundry.
[00:26:06.240 --> 00:26:06.960]   You add a--
[00:26:06.960 --> 00:26:08.240]   Add due to a noun.
[00:26:08.240 --> 00:26:10.480]   Well, you French, though, they have fey, and they do like--
[00:26:10.480 --> 00:26:11.840]   Which is like do, yeah.
[00:26:11.840 --> 00:26:13.120]   Which is kind of like do.
[00:26:13.120 --> 00:26:15.120]   And I imagine there's a Spanish--
[00:26:15.120 --> 00:26:15.440]   Anyway.
[00:26:15.440 --> 00:26:16.080]   No, do--
[00:26:16.080 --> 00:26:17.520]   Do English is a special--
[00:26:17.520 --> 00:26:18.080]   Yeah.
[00:26:18.080 --> 00:26:18.560]   Do--
[00:26:18.560 --> 00:26:19.120]   It's a special--
[00:26:19.120 --> 00:26:19.680]   --the weird thing.
[00:26:19.680 --> 00:26:19.680]   It's a special--
[00:26:19.680 --> 00:26:20.720]   --the meaningless do, isn't it?
[00:26:20.720 --> 00:26:22.240]   So here's where I got that from.
[00:26:22.240 --> 00:26:25.360]   Really-- Leo, if you like all the stuff, you like this book.
[00:26:25.360 --> 00:26:28.400]   Our magnificent bastard tongue, the untold history
[00:26:28.400 --> 00:26:29.680]   of English by John McWhorter.
[00:26:29.680 --> 00:26:32.560]   I love-- I love linguistics.
[00:26:32.560 --> 00:26:33.040]   To me, it's--
[00:26:33.040 --> 00:26:34.080]   And this one is really good.
[00:26:34.080 --> 00:26:34.720]   --kindlessly fascinating.
[00:26:34.720 --> 00:26:36.080]   It's from--
[00:26:36.080 --> 00:26:37.520]   A few years ago.
[00:26:37.520 --> 00:26:39.520]   I had-- I was just going through my audible--
[00:26:39.520 --> 00:26:40.160]   Find it.
[00:26:40.160 --> 00:26:42.240]   It's like one of the first lip books I listened to on Audible.
[00:26:42.240 --> 00:26:43.840]   And it's really good.
[00:26:43.840 --> 00:26:45.120]   I will write it down.
[00:26:45.120 --> 00:26:45.760]   I do.
[00:26:45.760 --> 00:26:48.240]   I enjoy these kinds of things.
[00:26:48.240 --> 00:26:50.400]   But we are moving to a less verbal world.
[00:26:50.400 --> 00:26:52.320]   I remember Trey Radcliffe, the photographer, telling me,
[00:26:52.320 --> 00:26:55.920]   thanks to Instagram, he thought that the young people today
[00:26:55.920 --> 00:26:59.040]   were growing up with a much better visual vocabulary--
[00:26:59.040 --> 00:26:59.440]   Oh, yeah.
[00:26:59.440 --> 00:27:00.560]   --than previous generations.
[00:27:00.560 --> 00:27:02.400]   I think he'd grant that that's probably true.
[00:27:02.400 --> 00:27:04.080]   And emojis, same thing.
[00:27:04.080 --> 00:27:08.160]   Apple and Google showing off new emojis.
[00:27:08.160 --> 00:27:13.840]   And it's interesting how emojis reflect cultural changes.
[00:27:13.840 --> 00:27:16.480]   So these new emojis are more inclusive.
[00:27:16.480 --> 00:27:19.840]   There's a prosthetic arm, a girl, an orgy, and a wheelchair.
[00:27:20.720 --> 00:27:24.000]   There's a seeing eye dog, a man with a cane.
[00:27:24.000 --> 00:27:27.200]   What's going on there?
[00:27:27.200 --> 00:27:28.640]   You open it up there.
[00:27:28.640 --> 00:27:31.280]   He's opening up-- So--
[00:27:31.280 --> 00:27:32.320]   Oh, you got a new hat?
[00:27:32.320 --> 00:27:34.320]   Is it an emoji hat?
[00:27:34.320 --> 00:27:35.440]   Is it a like girl?
[00:27:35.440 --> 00:27:38.880]   Is that a Buddha-judge hat?
[00:27:38.880 --> 00:27:39.360]   What is it?
[00:27:39.360 --> 00:27:40.160]   I don't know what it is.
[00:27:40.160 --> 00:27:40.800]   Mclewen.
[00:27:40.800 --> 00:27:42.960]   Oh, Mclewen hat.
[00:27:42.960 --> 00:27:43.840]   So wait a minute.
[00:27:43.840 --> 00:27:45.120]   Mclewen hat.
[00:27:45.120 --> 00:27:45.440]   Wait a minute.
[00:27:45.440 --> 00:27:47.600]   Matthew Ingram got a major award in all you--
[00:27:47.600 --> 00:27:47.920]   How's that?
[00:27:47.920 --> 00:27:48.400]   How's that?
[00:27:48.400 --> 00:27:48.640]   How's the Matthew's picture?
[00:27:48.640 --> 00:27:49.440]   Isn't on it.
[00:27:49.440 --> 00:27:51.200]   All you got was that lousy hat.
[00:27:51.200 --> 00:27:51.920]   Well, because here's--
[00:27:51.920 --> 00:27:55.280]   When I was at the Media Ecology Association
[00:27:55.280 --> 00:27:56.320]   where Matthew got that award,
[00:27:56.320 --> 00:27:59.680]   I saw a guy with a marshal Mclewen hat,
[00:27:59.680 --> 00:28:01.200]   and I was all jealous.
[00:28:01.200 --> 00:28:02.400]   And I said, "Ooh, how do I get this?"
[00:28:02.400 --> 00:28:05.280]   Well, actually, there's a high school in Toronto.
[00:28:05.280 --> 00:28:06.320]   Marshal Mclewen High School.
[00:28:06.320 --> 00:28:06.800]   Hi.
[00:28:06.800 --> 00:28:08.320]   So I got myself--
[00:28:08.320 --> 00:28:09.040]   He's a rebel.
[00:28:09.040 --> 00:28:11.200]   You're the marshal Mclewen Rebels.
[00:28:11.200 --> 00:28:13.120]   That is the greatest t-shirt ever.
[00:28:13.120 --> 00:28:14.240]   Is that the best?
[00:28:14.240 --> 00:28:15.280]   Please tell the word--
[00:28:15.280 --> 00:28:15.600]   Is that the best?
[00:28:15.600 --> 00:28:16.240]   I just got it.
[00:28:16.240 --> 00:28:17.200]   Oh my gosh.
[00:28:17.200 --> 00:28:19.280]   The marshal Mclewen Reg Rebels.
[00:28:19.280 --> 00:28:21.440]   Send me an email with the information
[00:28:21.440 --> 00:28:22.320]   how to get that t-shirt.
[00:28:22.320 --> 00:28:22.880]   We are going to--
[00:28:22.880 --> 00:28:24.800]   So I'm now just a customer--
[00:28:24.800 --> 00:28:26.160]   It's a marshal Mclewen High.
[00:28:26.160 --> 00:28:27.360]   Just in fact, we're going to get--
[00:28:27.360 --> 00:28:29.840]   We're going to get marshal Mclewen
[00:28:29.840 --> 00:28:31.520]   Rebels t-shirts for everybody.
[00:28:31.520 --> 00:28:32.880]   You think you need a marshal Mclewen
[00:28:32.880 --> 00:28:33.520]   and a wrap t-shirt?
[00:28:33.520 --> 00:28:34.960]   This will be our new t-shirt.
[00:28:34.960 --> 00:28:35.920]   So I didn't know what to do,
[00:28:35.920 --> 00:28:38.800]   so he was born in 1911,
[00:28:38.800 --> 00:28:39.920]   so I put an 11 on there, I think.
[00:28:39.920 --> 00:28:41.840]   But I should have put a four on there,
[00:28:41.840 --> 00:28:44.480]   because he was all about tetraads and the fours.
[00:28:44.480 --> 00:28:46.080]   But you could put your own number on it.
[00:28:46.080 --> 00:28:46.480]   Here's one.
[00:28:46.480 --> 00:28:46.960]   I don't know.
[00:28:46.960 --> 00:28:49.120]   This might be a mixed message.
[00:28:49.120 --> 00:28:51.760]   Property of marshal Mclewen Rebels.
[00:28:51.760 --> 00:28:54.000]   Might not be--
[00:28:54.000 --> 00:28:54.960]   You found it quickly.
[00:28:54.960 --> 00:28:55.520]   Yeah, not.
[00:28:55.520 --> 00:28:58.640]   Well, it's available in the marshal Mclewen
[00:28:58.640 --> 00:29:00.160]   Catholic Secondary School.
[00:29:00.160 --> 00:29:00.560]   The Rebels--
[00:29:00.560 --> 00:29:01.040]   Yeah.
[00:29:01.040 --> 00:29:02.240]   How about this one?
[00:29:02.240 --> 00:29:04.320]   The reason they're just seeing all these fans they have.
[00:29:04.320 --> 00:29:05.440]   Carson, you're going to want this one.
[00:29:05.440 --> 00:29:08.560]   It says, "Go fight when marshal Mclewen."
[00:29:08.560 --> 00:29:11.040]   [LAUGHTER]
[00:29:11.040 --> 00:29:13.040]   That's a t-shirt.
[00:29:13.040 --> 00:29:15.600]   That's a t-shirt.
[00:29:15.600 --> 00:29:18.720]   Or maybe just something a little more
[00:29:18.720 --> 00:29:19.520]   understated.
[00:29:19.520 --> 00:29:20.480]   Yeah.
[00:29:20.480 --> 00:29:22.240]   That's--
[00:29:22.240 --> 00:29:23.760]   And they have hats, as you can see.
[00:29:23.760 --> 00:29:24.960]   You can do the hats.
[00:29:24.960 --> 00:29:25.920]   You do lots of things.
[00:29:25.920 --> 00:29:27.440]   Wow.
[00:29:27.440 --> 00:29:27.920]   Rizzome priced.
[00:29:27.920 --> 00:29:29.040]   I'm very proud of this.
[00:29:29.040 --> 00:29:31.040]   And you chose 11 because why?
[00:29:31.040 --> 00:29:32.880]   I think it was born in 1911.
[00:29:32.880 --> 00:29:33.520]   I didn't--
[00:29:33.520 --> 00:29:34.800]   You wouldn't let you do it without a number.
[00:29:34.800 --> 00:29:35.920]   You should have chose since '65.
[00:29:35.920 --> 00:29:36.240]   Oh, what number?
[00:29:36.240 --> 00:29:36.800]   Oh, what number?
[00:29:36.800 --> 00:29:39.440]   What's the Gutenberg number?
[00:29:39.440 --> 00:29:40.720]   14 something?
[00:29:40.720 --> 00:29:41.760]   14, 15.
[00:29:41.760 --> 00:29:42.560]   Oh, you could--
[00:29:42.560 --> 00:29:45.200]   Marshal property marshal Mclewen Rebels, 14, 50.
[00:29:45.200 --> 00:29:48.240]   I think it's only as if you were
[00:29:48.240 --> 00:29:48.880]   on the team.
[00:29:48.880 --> 00:29:50.080]   I think it only goes to two digits.
[00:29:50.080 --> 00:29:51.120]   Here's 2019.
[00:29:51.120 --> 00:29:53.440]   Here's XXL.
[00:29:53.440 --> 00:29:56.160]   Or just--
[00:29:56.160 --> 00:29:56.560]   You know what?
[00:29:56.560 --> 00:29:58.400]   Do it in big, bold, block letters.
[00:29:58.400 --> 00:30:00.880]   Marshall Mclewen Rebels.
[00:30:00.880 --> 00:30:01.920]   That is awesome.
[00:30:01.920 --> 00:30:03.600]   And what color should it be?
[00:30:03.600 --> 00:30:04.640]   They have many colors.
[00:30:04.640 --> 00:30:06.240]   And then--
[00:30:06.240 --> 00:30:08.480]   You could go to hats too.
[00:30:08.480 --> 00:30:08.960]   They have hats.
[00:30:08.960 --> 00:30:10.400]   They have other things as well.
[00:30:10.400 --> 00:30:11.920]   That is so hysterical.
[00:30:11.920 --> 00:30:12.720]   I'm going to buy these.
[00:30:12.720 --> 00:30:13.280]   It's the best.
[00:30:13.280 --> 00:30:13.920]   Yeah.
[00:30:13.920 --> 00:30:14.800]   Isn't it the best?
[00:30:14.800 --> 00:30:15.440]   Yeah.
[00:30:15.440 --> 00:30:16.880]   Oh, I forgot the guys team.
[00:30:16.880 --> 00:30:17.840]   I'm sorry, even drawer.
[00:30:17.840 --> 00:30:19.520]   Whoever was, thank you for this.
[00:30:19.520 --> 00:30:21.120]   What size are you, Carsten?
[00:30:21.120 --> 00:30:21.680]   Large?
[00:30:21.680 --> 00:30:22.880]   I am large.
[00:30:22.880 --> 00:30:23.200]   All right.
[00:30:23.200 --> 00:30:24.480]   Let's get--
[00:30:24.480 --> 00:30:26.480]   John, you're large, right?
[00:30:26.480 --> 00:30:28.320]   1x.
[00:30:28.320 --> 00:30:32.160]   Let's get two larges and two XLs.
[00:30:32.160 --> 00:30:35.280]   All right, I'm ready.
[00:30:35.280 --> 00:30:36.160]   Wait, but not that--
[00:30:36.160 --> 00:30:37.120]   that design?
[00:30:37.120 --> 00:30:37.360]   Yes.
[00:30:37.360 --> 00:30:38.640]   You don't want the cooler.
[00:30:38.640 --> 00:30:39.520]   That's pretty cool.
[00:30:39.520 --> 00:30:41.840]   I like that design.
[00:30:41.840 --> 00:30:43.200]   It's subtle.
[00:30:43.200 --> 00:30:44.000]   It's understated.
[00:30:44.000 --> 00:30:44.400]   They're--
[00:30:44.400 --> 00:30:45.520]   It's a gray T-shirt,
[00:30:45.520 --> 00:30:47.360]   so it's like an athletic T-shirt.
[00:30:47.360 --> 00:30:48.880]   In bright green letters at the top,
[00:30:48.880 --> 00:30:50.240]   it says Marshall McLuhan.
[00:30:50.240 --> 00:30:52.160]   Underneath it and slightly bolder letters,
[00:30:52.160 --> 00:30:53.440]   in blue, it says rebels.
[00:30:53.440 --> 00:30:57.280]   And I like the possibility that it might say
[00:30:57.280 --> 00:30:58.880]   Marshall McLuhan rebels.
[00:30:58.880 --> 00:30:59.360]   Rebels?
[00:30:59.360 --> 00:31:00.240]   Oh, yeah.
[00:31:00.240 --> 00:31:00.640]   Oh, yeah.
[00:31:00.640 --> 00:31:01.600]   Oh, yeah.
[00:31:01.600 --> 00:31:02.480]   Oh.
[00:31:02.480 --> 00:31:03.040]   Nice.
[00:31:03.040 --> 00:31:03.840]   Oh.
[00:31:03.840 --> 00:31:08.480]   As the medium as the message was the medium as the mess age,
[00:31:08.480 --> 00:31:09.440]   and medium as massage,
[00:31:09.440 --> 00:31:11.520]   he loved medium as the mass age.
[00:31:11.520 --> 00:31:12.560]   He loved mums.
[00:31:12.560 --> 00:31:13.600]   He loved mums.
[00:31:13.600 --> 00:31:16.640]   So McLuhan.
[00:31:16.640 --> 00:31:18.960]   Then there's a little long path.
[00:31:18.960 --> 00:31:21.040]   But McLuhan would say that we're passing from
[00:31:21.040 --> 00:31:23.760]   textual back to morality.
[00:31:23.760 --> 00:31:26.000]   We passed from morality.
[00:31:26.000 --> 00:31:26.640]   Yes.
[00:31:26.640 --> 00:31:27.920]   Textual back to morality.
[00:31:27.920 --> 00:31:29.920]   And even though we're talking about images here,
[00:31:29.920 --> 00:31:31.200]   it's a form of morality.
[00:31:31.200 --> 00:31:33.040]   It's a form of conversation.
[00:31:33.040 --> 00:31:35.280]   Really?
[00:31:35.280 --> 00:31:36.320]   Yeah, you got to ask.
[00:31:36.320 --> 00:31:37.600]   This is an issue I have,
[00:31:37.600 --> 00:31:39.680]   because I think about the oral culture,
[00:31:39.680 --> 00:31:40.480]   and then I think about
[00:31:40.480 --> 00:31:42.080]   text-based culture,
[00:31:42.080 --> 00:31:43.520]   and it feels like things like emojis
[00:31:43.520 --> 00:31:44.720]   feel more text-based.
[00:31:45.760 --> 00:31:47.360]   Gifts and vines feel more.
[00:31:47.360 --> 00:31:50.560]   I mean, this is really academic,
[00:31:50.560 --> 00:31:51.520]   but I'm like,
[00:31:51.520 --> 00:31:52.640]   where do we draw the line?
[00:31:52.640 --> 00:31:53.760]   Especially since
[00:31:53.760 --> 00:31:57.840]   it's all kind of flowing together.
[00:31:57.840 --> 00:31:59.920]   To me, here's the difference I think, Stacey.
[00:31:59.920 --> 00:32:01.760]   And then this is why I agree with you,
[00:32:01.760 --> 00:32:03.200]   because I struggle the same thing.
[00:32:03.200 --> 00:32:07.600]   Text-based means the current alphabet of 26.
[00:32:07.600 --> 00:32:10.480]   So emoji are emoji being the plural.
[00:32:10.480 --> 00:32:12.960]   Emoji are a new alphabet.
[00:32:12.960 --> 00:32:14.880]   Yeah, but I mean,
[00:32:14.880 --> 00:32:16.160]   you've got other alphabets.
[00:32:16.160 --> 00:32:19.360]   Go ahead.
[00:32:19.360 --> 00:32:21.360]   Well, you've got other alphabets.
[00:32:21.360 --> 00:32:24.000]   So is it just your phonetic alphabet?
[00:32:24.000 --> 00:32:25.360]   Lexicon's better than an alphabet.
[00:32:25.360 --> 00:32:26.320]   It's not an alphabet,
[00:32:26.320 --> 00:32:27.280]   because it's not a letter.
[00:32:27.280 --> 00:32:28.560]   It's a lexicon.
[00:32:28.560 --> 00:32:31.280]   It's a pictogram.
[00:32:31.280 --> 00:32:32.800]   It's pictograms.
[00:32:32.800 --> 00:32:34.400]   Well, because I'm just thinking about,
[00:32:34.400 --> 00:32:35.680]   if I go into Sweden,
[00:32:35.680 --> 00:32:37.360]   they've got more letters.
[00:32:37.360 --> 00:32:38.480]   If I go to Russia,
[00:32:38.480 --> 00:32:40.160]   they've got completely different letters.
[00:32:40.160 --> 00:32:44.160]   But they're all textual.
[00:32:45.120 --> 00:32:47.680]   Yes, but the emoji are words, not letters.
[00:32:47.680 --> 00:32:50.480]   Okay.
[00:32:50.480 --> 00:32:52.240]   That's how I can get behind that.
[00:32:52.240 --> 00:32:55.440]   So that's why you're saying that's
[00:32:55.440 --> 00:32:59.520]   so you're arguing that Lexicon is really a dictionary of words,
[00:32:59.520 --> 00:33:01.040]   vocabulary.
[00:33:01.040 --> 00:33:03.680]   So does emoji are vocabulary?
[00:33:03.680 --> 00:33:03.920]   Yes.
[00:33:03.920 --> 00:33:05.520]   Okay.
[00:33:05.520 --> 00:33:07.040]   Thanks, Carsten.
[00:33:07.040 --> 00:33:08.720]   I knew I needed the Harvard brain in here.
[00:33:08.720 --> 00:33:10.400]   I was just trying to clear up this,
[00:33:10.400 --> 00:33:11.280]   clear up things a little.
[00:33:12.880 --> 00:33:15.760]   Meanwhile, Marshall McLuhan High School wants me to create an account.
[00:33:15.760 --> 00:33:18.480]   Always.
[00:33:18.480 --> 00:33:22.880]   So this is slowing me down just a slightest bit.
[00:33:22.880 --> 00:33:24.560]   It's okay.
[00:33:24.560 --> 00:33:26.800]   We're nowhere near the show at this point.
[00:33:26.800 --> 00:33:27.120]   Yeah.
[00:33:27.120 --> 00:33:29.440]   The show has drifted way off.
[00:33:29.440 --> 00:33:32.560]   I tried to get back on line with emojis,
[00:33:32.560 --> 00:33:34.160]   but that didn't go well.
[00:33:34.160 --> 00:33:35.120]   Just pushed us even farther.
[00:33:35.120 --> 00:33:36.080]   First to see the farther.
[00:33:36.080 --> 00:33:39.200]   Facebook, $5 billion fine.
[00:33:39.200 --> 00:33:40.320]   That sounds like a big fine,
[00:33:40.320 --> 00:33:41.760]   $5 billion with a B.
[00:33:42.320 --> 00:33:43.680]   That's the largest fine ever.
[00:33:43.680 --> 00:33:45.120]   It's the largest FTC's ever.
[00:33:45.120 --> 00:33:46.640]   It's the largest FTC's fine ever.
[00:33:46.640 --> 00:33:50.720]   And it's almost a quarter of their annual profits.
[00:33:50.720 --> 00:33:55.600]   There's only one little fly in the ointment
[00:33:55.600 --> 00:33:58.080]   that as soon as this fine was announced,
[00:33:58.080 --> 00:33:59.760]   Facebook stock went up $6 billion.
[00:33:59.760 --> 00:34:02.400]   Meaning this fine,
[00:34:02.400 --> 00:34:06.320]   make that smiling guy, Mark Zuckerberg a little richer.
[00:34:06.320 --> 00:34:09.120]   They don't own all that stock and they don't cash in all that stock.
[00:34:09.120 --> 00:34:10.240]   But Mark owns a bit.
[00:34:11.200 --> 00:34:11.920]   Yeah.
[00:34:11.920 --> 00:34:12.800]   They don't cash it in.
[00:34:12.800 --> 00:34:15.200]   But there was a big jump in the stock price,
[00:34:15.200 --> 00:34:17.360]   which just shows that the stock market--
[00:34:17.360 --> 00:34:18.080]   Here's the point.
[00:34:18.080 --> 00:34:19.360]   The stock market feels like,
[00:34:19.360 --> 00:34:20.320]   oh, this is good news.
[00:34:20.320 --> 00:34:22.320]   Yeah, I got to move it back.
[00:34:22.320 --> 00:34:25.760]   Of course, the stock market is constantly doing crazy stuff.
[00:34:25.760 --> 00:34:27.360]   Yeah, it doesn't mean anything.
[00:34:27.360 --> 00:34:29.200]   But okay, so it's fine.
[00:34:29.200 --> 00:34:32.480]   I mean, look, if you said I'm going to take a quarter of your profits,
[00:34:32.480 --> 00:34:34.960]   I'd say, what do you think you are, the IRS?
[00:34:34.960 --> 00:34:36.480]   No, I would say that hurts.
[00:34:36.480 --> 00:34:38.160]   Right?
[00:34:38.720 --> 00:34:41.600]   Plus, when you have in Europe,
[00:34:41.600 --> 00:34:46.400]   on your GDPR, you can go up to 4% of gross.
[00:34:46.400 --> 00:34:48.880]   And the problem is that if every country starts doing this,
[00:34:48.880 --> 00:34:50.640]   you end up getting to 100%.
[00:34:50.640 --> 00:34:51.200]   Yeah.
[00:34:51.200 --> 00:34:53.520]   Good point.
[00:34:53.520 --> 00:34:57.680]   Nevertheless, a lot of people, including Neelai Patel,
[00:34:57.680 --> 00:34:59.120]   writing in the Verge said,
[00:34:59.120 --> 00:35:00.720]   this is a slap on the wrist.
[00:35:00.720 --> 00:35:02.320]   You see that?
[00:35:02.320 --> 00:35:03.200]   But that's part of the--
[00:35:03.200 --> 00:35:05.680]   Neelai is always so cranky, though.
[00:35:05.680 --> 00:35:07.760]   That's why we like him.
[00:35:07.760 --> 00:35:10.960]   Yeah, and it's the Damasine conversion.
[00:35:10.960 --> 00:35:11.680]   The people who--
[00:35:11.680 --> 00:35:14.160]   Oh, no, suddenly text awful.
[00:35:14.160 --> 00:35:15.600]   It's never bad.
[00:35:15.600 --> 00:35:17.040]   The punishment is never bad enough.
[00:35:17.040 --> 00:35:19.280]   I'm tired of that.
[00:35:19.280 --> 00:35:20.320]   Because it's dangerous.
[00:35:20.320 --> 00:35:21.840]   Here's what happened yesterday.
[00:35:21.840 --> 00:35:24.960]   Let's talk about the Senate Judiciary Committee.
[00:35:24.960 --> 00:35:26.880]   Was it the Senate of the House?
[00:35:26.880 --> 00:35:28.000]   Senate.
[00:35:28.000 --> 00:35:28.320]   Okay.
[00:35:28.320 --> 00:35:29.520]   It was Cruz.
[00:35:29.520 --> 00:35:30.800]   That's right.
[00:35:30.800 --> 00:35:31.280]   That's right.
[00:35:31.280 --> 00:35:32.320]   And Maisie Herono.
[00:35:32.320 --> 00:35:33.040]   Yep.
[00:35:33.040 --> 00:35:34.560]   Maisie, to my mind, the good sign.
[00:35:34.560 --> 00:35:35.440]   Google, Apple.
[00:35:35.440 --> 00:35:36.400]   The bad of the bouncing.
[00:35:36.400 --> 00:35:38.720]   And who else were called to testify?
[00:35:38.720 --> 00:35:41.760]   Actually, this is a series of hearings.
[00:35:41.760 --> 00:35:42.800]   This particular hearing--
[00:35:42.800 --> 00:35:43.920]   Oh, well, on Amazon and Facebook.
[00:35:43.920 --> 00:35:45.120]   Facebook, on graph course.
[00:35:45.120 --> 00:35:47.600]   This was a series of hearings.
[00:35:47.600 --> 00:35:51.680]   And this one particularly was focused on trust, antitrust.
[00:35:51.680 --> 00:35:53.440]   And they said,
[00:35:53.440 --> 00:35:58.400]   "How do we promote innovation in this country with tech being
[00:35:58.400 --> 00:36:01.040]   so strong?"
[00:36:04.480 --> 00:36:07.920]   They also were speaking in the House, right?
[00:36:07.920 --> 00:36:09.920]   Was there another hearing in the House?
[00:36:09.920 --> 00:36:11.440]   There were four separate hearings.
[00:36:11.440 --> 00:36:12.640]   Okay, there were a lot of them.
[00:36:12.640 --> 00:36:14.000]   There were many.
[00:36:14.000 --> 00:36:14.560]   Yeah.
[00:36:14.560 --> 00:36:17.360]   Turn off the room mic because it's a little echo-y.
[00:36:17.360 --> 00:36:22.800]   All of the executives who testified in the House
[00:36:22.800 --> 00:36:26.240]   said, "We face competition at every term,
[00:36:26.240 --> 00:36:28.960]   especially with WeChat and TikTok
[00:36:28.960 --> 00:36:30.960]   eating into our global market share."
[00:36:33.760 --> 00:36:35.920]   But they did the thing that Mark Zuckerberg did,
[00:36:35.920 --> 00:36:38.480]   which when asked questions he didn't know the answer to,
[00:36:38.480 --> 00:36:40.720]   he said, "Well, let me ask my team and I'll get back to you."
[00:36:40.720 --> 00:36:42.480]   It's frustrating.
[00:36:42.480 --> 00:36:45.840]   It's definitely frustrating, said freshman representative Lucy
[00:36:45.840 --> 00:36:49.600]   McBath of Georgia, which definitely means we're asking the right kinds of questions,
[00:36:49.600 --> 00:36:51.520]   and we recognize this is an ongoing issue.
[00:36:51.520 --> 00:36:55.600]   In the Senate,
[00:36:55.600 --> 00:37:02.480]   maybe it was a little different because it seemed like they were competing agendas,
[00:37:02.480 --> 00:37:03.040]   right, Jeff?
[00:37:04.000 --> 00:37:05.920]   Yeah, so the right,
[00:37:05.920 --> 00:37:08.960]   so on the one hand you have people going after the platform saying,
[00:37:08.960 --> 00:37:10.960]   "Take down extremist content."
[00:37:10.960 --> 00:37:14.960]   And then with no sense of irony whatsoever,
[00:37:14.960 --> 00:37:17.360]   the right is complaining that censorship of them,
[00:37:17.360 --> 00:37:21.600]   which to me kind of admits that they are doing extremist content,
[00:37:21.600 --> 00:37:22.560]   but we'll leave that to the side.
[00:37:22.560 --> 00:37:25.200]   So now they're arguing that
[00:37:25.200 --> 00:37:30.080]   so Ted Cruz was going after them,
[00:37:31.920 --> 00:37:35.360]   a presentation he'd gotten out of Google that he thought was damning,
[00:37:35.360 --> 00:37:39.680]   and so on Maisie Veronoe, who's the ranking member on the other side,
[00:37:39.680 --> 00:37:42.640]   someone I haven't watched the testimony yet,
[00:37:42.640 --> 00:37:46.000]   but someone I know who I respect a lot who watched it said that she was doing a better job
[00:37:46.000 --> 00:37:47.840]   of defending Google than Google's guy was.
[00:37:47.840 --> 00:37:55.040]   But where you end up here pretty soon is you end up with the potential of the government
[00:37:55.040 --> 00:38:01.520]   basically trying to legislate forcing the platforms to run certain content.
[00:38:02.480 --> 00:38:06.320]   You have things like PragerU, who also testified,
[00:38:06.320 --> 00:38:11.280]   having gone from Trump's own little White House party for the extremists,
[00:38:11.280 --> 00:38:12.880]   they go over to Congress then.
[00:38:12.880 --> 00:38:17.280]   And so they're trying, you've got this pincer movement going on,
[00:38:17.280 --> 00:38:21.040]   where from the left it's, "You're too big, we're going to break you up,"
[00:38:21.040 --> 00:38:23.840]   and you got privacy, and then from the right it's,
[00:38:23.840 --> 00:38:26.960]   you're censoring us, and you're not letting us speak,
[00:38:27.840 --> 00:38:31.760]   because other people, the left, are saying get the bad speech down,
[00:38:31.760 --> 00:38:35.840]   but we don't think our speech is bad, and we're all going to attack you now,
[00:38:35.840 --> 00:38:41.600]   and what's going to suffer is the internet, and that's what worries me,
[00:38:41.600 --> 00:38:42.560]   that's what worries me greatly.
[00:38:42.560 --> 00:38:46.320]   They also asked some good questions. In fact, I was very impressed
[00:38:46.320 --> 00:38:49.120]   by some of these people.
[00:38:49.120 --> 00:38:51.600]   Primilla Jayapal, who's a Democrat from Washington,
[00:38:51.600 --> 00:38:54.800]   asked the Amazon General Counsel.
[00:38:56.240 --> 00:38:59.840]   She said, "When people sell products on your site, do you track which products
[00:38:59.840 --> 00:39:03.440]   are the most successful? And then do you sometimes create a product
[00:39:03.440 --> 00:39:06.240]   to compete with that product?" We've heard, of course, that Amazon does that.
[00:39:06.240 --> 00:39:08.800]   In fact, I think there's pretty good evidence that they do something like that.
[00:39:08.800 --> 00:39:11.760]   Oh, sure, that's one of their virtuous cycles.
[00:39:11.760 --> 00:39:14.320]   Amazon, well, here's Amazon's response.
[00:39:14.320 --> 00:39:18.080]   We do not use individual seller data in order to compete with them.
[00:39:18.080 --> 00:39:19.920]   Seller data.
[00:39:19.920 --> 00:39:23.280]   Yeah, so, Primilla Jayapal said, "You do connect,"
[00:39:23.280 --> 00:39:26.720]   oh, no, I'm sorry, this is David Chiciline of Rhode Island.
[00:39:26.720 --> 00:39:29.840]   "Do you do collect enormous data about what products are popular,
[00:39:29.840 --> 00:39:32.640]   what's selling and where they're selling? You're saying,
[00:39:32.640 --> 00:39:36.080]   you don't use that in any way to promote Amazon products,
[00:39:36.080 --> 00:39:37.680]   and I remind you, you're under oath."
[00:39:37.680 --> 00:39:42.160]   Sutton replied, "We use data to serve our customers."
[00:39:42.160 --> 00:39:47.760]   We used, that sounds like a little weasel, right?
[00:39:47.760 --> 00:39:49.920]   Yep, that's the squirming out of it.
[00:39:50.560 --> 00:39:52.880]   Well, I will say this about that.
[00:39:52.880 --> 00:39:54.800]   We use data to serve our customers.
[00:39:54.800 --> 00:39:57.200]   Depends on your definition of data.
[00:39:57.200 --> 00:39:59.280]   Depends on your definition of serve.
[00:39:59.280 --> 00:40:01.680]   Depends on your definition of customers.
[00:40:01.680 --> 00:40:04.240]   I'll serve you right.
[00:40:04.240 --> 00:40:11.920]   I mean, Amazon does think of the individual buyer.
[00:40:11.920 --> 00:40:15.200]   It doesn't think about society, it doesn't think about externalities,
[00:40:15.200 --> 00:40:17.680]   and it thinks about maximizing its profits
[00:40:18.480 --> 00:40:20.800]   by optimizing the buyer experience.
[00:40:20.800 --> 00:40:22.320]   You might say that that's their job.
[00:40:22.320 --> 00:40:25.280]   Yeah, well, I mean, that is their mission.
[00:40:25.280 --> 00:40:26.560]   They talk about it all the time.
[00:40:26.560 --> 00:40:26.720]   Yeah.
[00:40:26.720 --> 00:40:28.800]   That's day one, blah, blah, blah.
[00:40:28.800 --> 00:40:33.120]   So it makes sense to say, "Hey, I'm going to look at data,
[00:40:33.120 --> 00:40:35.920]   either in aggregate or what people are buying for them,
[00:40:35.920 --> 00:40:40.560]   and show them that. And because we want to make money, too,
[00:40:40.560 --> 00:40:42.400]   I'm going to use that and show them my stuff."
[00:40:42.400 --> 00:40:48.320]   And the consumer gets maybe the best price for this.
[00:40:48.320 --> 00:40:49.520]   Which they want.
[00:40:49.520 --> 00:40:52.000]   They can find other stuff if they work harder.
[00:40:52.000 --> 00:40:54.800]   It's just a question of how far do we want to let
[00:40:54.800 --> 00:40:58.880]   those systemic advantages accrue to Amazon?
[00:40:58.880 --> 00:41:03.040]   We live in a country where there are systemic advantages
[00:41:03.040 --> 00:41:06.160]   that have accrued over time to whole swaths of people.
[00:41:06.160 --> 00:41:10.960]   So having it happen to companies, it feels very much like something
[00:41:10.960 --> 00:41:13.920]   we don't know how to talk about,
[00:41:13.920 --> 00:41:16.320]   and we don't know how to think about it regularly.
[00:41:17.040 --> 00:41:22.080]   So I think it's part of a macro challenge we have here, just in general.
[00:41:22.080 --> 00:41:25.680]   Fox News had a headline that would be,
[00:41:25.680 --> 00:41:29.200]   "I would think worthy of the Ball War Litten Contest."
[00:41:29.200 --> 00:41:33.360]   Google VP grilled in hearing over alleged bias against conservatives,
[00:41:33.360 --> 00:41:36.480]   as slain reporter's father calls for regulation.
[00:41:36.480 --> 00:41:40.160]   I got a little whiplash on that one.
[00:41:40.160 --> 00:41:41.360]   Okay.
[00:41:41.360 --> 00:41:43.760]   Republic, you also make a on Tuesday,
[00:41:43.760 --> 00:41:46.640]   criticized Google during the Senate Judiciary Committee hearing,
[00:41:47.520 --> 00:41:52.080]   over allegations of bias and censorship of conservative groups
[00:41:52.080 --> 00:41:53.200]   in the tech giants platforms.
[00:41:53.200 --> 00:41:55.520]   That actually wasn't what the hearing was about, but okay.
[00:41:55.520 --> 00:41:57.680]   Also during the hearing, the father of Virginia reporter,
[00:41:57.680 --> 00:41:59.920]   Alison Parker, who was shot and killed in August,
[00:41:59.920 --> 00:42:03.280]   while conducting an interview on live television,
[00:42:03.280 --> 00:42:06.560]   called on Congress to further regulate Google and other tech giants.
[00:42:06.560 --> 00:42:10.240]   The problem I can't sink.
[00:42:10.240 --> 00:42:10.960]   Yeah.
[00:42:10.960 --> 00:42:16.320]   The problem apparently is that the killer uploaded videos of the murder to YouTube,
[00:42:17.280 --> 00:42:18.800]   and of course he did.
[00:42:18.800 --> 00:42:23.040]   We all saw them, but I think YouTube probably pulled them down as quickly as it could.
[00:42:23.040 --> 00:42:27.440]   Ted Cruz, Google's control over what people hear,
[00:42:27.440 --> 00:42:30.240]   watch, read and say is unprecedented.
[00:42:30.240 --> 00:42:33.360]   Google can and often does control our discourse.
[00:42:33.360 --> 00:42:39.120]   The American people are subject to overt censorship and covert manipulation by Google's algorithm.
[00:42:39.120 --> 00:42:43.040]   Again, let's look at the, let's, you know,
[00:42:43.040 --> 00:42:47.520]   yeah, that's what we have people screaming to take down hate speech and take down
[00:42:47.520 --> 00:42:49.200]   extremist speech and take down everything.
[00:42:49.200 --> 00:42:52.400]   And then we have them saying, when you take it down, you're hurting us.
[00:42:52.400 --> 00:42:57.120]   Now note also, I went looking for coverage of this hearing.
[00:42:57.120 --> 00:43:04.400]   And according to Google, except for one, to my mind, badly done NPR story,
[00:43:04.400 --> 00:43:06.960]   every single bit of coverage was on far right media.
[00:43:06.960 --> 00:43:08.320]   Wait, Congress and farther right.
[00:43:08.320 --> 00:43:10.000]   I covered it yesterday.
[00:43:10.000 --> 00:43:10.720]   Yeah.
[00:43:10.720 --> 00:43:12.000]   But that's it.
[00:43:12.000 --> 00:43:12.640]   Yeah.
[00:43:12.640 --> 00:43:17.840]   So partly because in a way, I feel like the media,
[00:43:17.840 --> 00:43:20.880]   does it need to cover this?
[00:43:20.880 --> 00:43:22.640]   This seems like a dog and pony show.
[00:43:22.640 --> 00:43:25.840]   It is, but we got to know what they're doing.
[00:43:25.840 --> 00:43:31.280]   They're trying to present a worldview here that's going to have a major impact that is being
[00:43:31.280 --> 00:43:34.560]   believed by 40% of America.
[00:43:34.560 --> 00:43:39.360]   And, you know, every time I'm on MSNBC, I begged them to start a feature called,
[00:43:39.360 --> 00:43:41.120]   "We watch Fox News so you don't have to."
[00:43:41.120 --> 00:43:44.240]   Because it's important to cover it as a political actor.
[00:43:44.240 --> 00:43:49.840]   I said somewhat smartass, but what I mean it, in a sense that I don't know what they
[00:43:49.840 --> 00:43:53.200]   do cover, what was my father's see, and I don't know what they don't cover.
[00:43:53.200 --> 00:43:57.920]   And, you know, I want to compare that then to what Left Media is doing,
[00:43:57.920 --> 00:43:59.920]   which is the media I watch on MSNBC.
[00:43:59.920 --> 00:44:02.800]   And we need to see the different worldviews people are getting.
[00:44:02.800 --> 00:44:08.320]   And we have a lot of talk about how there's bifurcation in social media,
[00:44:08.320 --> 00:44:11.280]   but there's much bigger to my mind in media.
[00:44:11.280 --> 00:44:12.960]   By the way, there's an excellent study I'm writing about.
[00:44:12.960 --> 00:44:13.920]   I'll talk about it more next week.
[00:44:13.920 --> 00:44:20.320]   Research named Alex Bruns from Queensland, Australia, who I saw last week in Madrid,
[00:44:20.320 --> 00:44:27.360]   who did a study arguing that there's basically no such thing as filter bubbles and echo chambers.
[00:44:27.360 --> 00:44:31.280]   But those have become the working metaphors for all the discussion of where we are.
[00:44:31.280 --> 00:44:37.680]   And we're far too much taking assumptions and myths in this world where we're headed.
[00:44:37.680 --> 00:44:42.000]   So we've got to look also at the larger ecosystem of media and where media are in this.
[00:44:42.000 --> 00:44:43.200]   We don't really see that.
[00:44:43.200 --> 00:44:46.640]   So this is the case where there was a hearing, there's a huge outcry,
[00:44:46.640 --> 00:44:48.800]   but only in one half of the media world.
[00:44:48.800 --> 00:44:50.720]   The half is even over it.
[00:44:50.720 --> 00:44:52.880]   But it's not at all in the other media world.
[00:44:52.880 --> 00:44:53.280]   Yeah.
[00:44:53.280 --> 00:44:56.240]   Well, the other media world was worried about some other things going on.
[00:44:56.240 --> 00:44:57.120]   Oh, for the fact.
[00:44:57.120 --> 00:44:57.600]   No, you can't.
[00:44:57.600 --> 00:44:59.200]   Now, not true that.
[00:44:59.200 --> 00:45:00.640]   A little more concern about that.
[00:45:00.640 --> 00:45:03.440]   And the first media world was trying to ignore what was going on in the media world.
[00:45:03.440 --> 00:45:07.440]   Yeah, it was just fascinating is if you watch
[00:45:07.440 --> 00:45:10.480]   as I try to do Fox for a while and then watch MSNBC for a while,
[00:45:10.480 --> 00:45:13.280]   it's almost as if they're reporting on different countries.
[00:45:13.280 --> 00:45:13.920]   Oh, god.
[00:45:13.920 --> 00:45:16.320]   It's really a disconnect.
[00:45:16.320 --> 00:45:20.240]   Meanwhile, when the four congress women testified, I turned to Fox.
[00:45:20.240 --> 00:45:20.560]   Yeah.
[00:45:20.560 --> 00:45:22.480]   And they went on about how this was Antifa.
[00:45:22.480 --> 00:45:24.720]   Antifa, how are you pronouncing it?
[00:45:24.720 --> 00:45:24.960]   Right?
[00:45:24.960 --> 00:45:27.760]   So it was a very different world view, shall we say?
[00:45:27.760 --> 00:45:33.680]   Meanwhile, today Dave Marcus is testifying in front of the House Financial Services Committee
[00:45:34.240 --> 00:45:36.960]   about Facebook's cryptocurrency Libra.
[00:45:36.960 --> 00:45:43.280]   And certainly members of Congress, there are members of Congress and definitely President Trump
[00:45:43.280 --> 00:45:49.680]   and definitely Treasury Secretary Mnuchin who think Facebook should halt development of Libra.
[00:45:49.680 --> 00:45:56.640]   Marcus, however, wasn't he was a little equivocal about agreeing to a moratorium.
[00:45:56.640 --> 00:46:01.280]   He said, we're going to take the time to get this right.
[00:46:02.640 --> 00:46:04.640]   We will not launch until we first
[00:46:04.640 --> 00:46:07.680]   address our concerns, but we're going ahead with it is pretty much the end.
[00:46:07.680 --> 00:46:09.440]   Well, what did you expect him to say?
[00:46:09.440 --> 00:46:10.800]   You're right.
[00:46:10.800 --> 00:46:11.520]   Oh, thanks.
[00:46:11.520 --> 00:46:12.000]   Never mind.
[00:46:12.000 --> 00:46:12.320]   Yeah.
[00:46:12.320 --> 00:46:12.800]   Yeah.
[00:46:12.800 --> 00:46:16.720]   And it's a it's a form of prior restraint.
[00:46:16.720 --> 00:46:21.120]   Um, you know, you can regulate it anytime you want.
[00:46:21.120 --> 00:46:24.000]   You can you can stop and but see what it is first.
[00:46:24.000 --> 00:46:24.240]   Yeah.
[00:46:24.240 --> 00:46:26.880]   Or pass along now.
[00:46:26.880 --> 00:46:27.600]   You can do that.
[00:46:27.600 --> 00:46:29.360]   Or yeah, but you don't even know what it is.
[00:46:29.360 --> 00:46:31.760]   Well, I think one of the concerns.
[00:46:32.480 --> 00:46:37.600]   Here's Maxine Waters, Facebook's proposed entry into financial services is all the more troubling
[00:46:37.600 --> 00:46:39.760]   because it is already harmed.
[00:46:39.760 --> 00:46:44.000]   Facebook has already harmed vast numbers of people on a scale similar to Wells Fargo
[00:46:44.000 --> 00:46:47.680]   and demonstrated a pattern of failing to keep consumer data private on a scale similar to
[00:46:47.680 --> 00:46:48.080]   that.
[00:46:48.080 --> 00:46:51.760]   I'm sorry, but that's that's that's ridiculous.
[00:46:51.760 --> 00:46:55.360]   Facebook says it's going to be a Swiss consortium.
[00:46:55.360 --> 00:46:58.800]   It'll be regulated by the Swiss, not the US.
[00:46:58.800 --> 00:47:01.840]   It's other companies, many other companies involved in this.
[00:47:01.840 --> 00:47:05.440]   There's also a concern about its use for money laundering.
[00:47:05.440 --> 00:47:07.920]   Uh, all cryptocurrency has that.
[00:47:07.920 --> 00:47:08.480]   That's a legitimate.
[00:47:08.480 --> 00:47:08.960]   Sure.
[00:47:08.960 --> 00:47:09.200]   Yeah.
[00:47:09.200 --> 00:47:16.800]   So, uh, there, uh, Maxine Waters did circulate a draft bill that would block it.
[00:47:16.800 --> 00:47:22.560]   Legally borrowing all large tech platforms from providing financial services.
[00:47:22.560 --> 00:47:28.240]   Uh, Senator Brian Schatz, a Democrat from Hawaii has circulated a similar bill in the Senate.
[00:47:28.800 --> 00:47:31.360]   That would outlaw reserve backed digital currency.
[00:47:31.360 --> 00:47:32.960]   Libra is reserve backed.
[00:47:32.960 --> 00:47:36.880]   Uh, so maybe, maybe they will make a lot.
[00:47:36.880 --> 00:47:37.920]   Maybe there ought to be a law.
[00:47:37.920 --> 00:47:38.400]   I don't know.
[00:47:38.400 --> 00:47:46.240]   I had to say there's serious concerns about this.
[00:47:46.240 --> 00:47:47.280]   Legitimately.
[00:47:47.280 --> 00:47:51.200]   There should be, if not federal oversight, at least some sort of auditability
[00:47:51.200 --> 00:47:57.360]   of the process that is conducted by a third party that isn't involved.
[00:47:58.240 --> 00:47:58.400]   Yes.
[00:47:58.400 --> 00:48:04.400]   And should Facebook, which already, or Google or Amazon, or Apple, I guess,
[00:48:04.400 --> 00:48:07.040]   and Apple's credit card is rumored to be coming out any day now.
[00:48:07.040 --> 00:48:12.000]   Uh, should these companies be told in no uncertain, uh, words?
[00:48:12.000 --> 00:48:13.920]   You should not be in financial services.
[00:48:13.920 --> 00:48:16.240]   You've got, you've got plenty of fish to fry.
[00:48:16.240 --> 00:48:21.360]   Let's not extend your monopoly into an entire new sector.
[00:48:21.360 --> 00:48:22.640]   I think that that's reasonable.
[00:48:22.640 --> 00:48:23.600]   That's antitrust.
[00:48:24.400 --> 00:48:26.480]   Um, not no, not under American law.
[00:48:26.480 --> 00:48:28.000]   It's not, not if they don't abuse it.
[00:48:28.000 --> 00:48:33.040]   They can go into, into things and using your monopoly to enter new markets
[00:48:33.040 --> 00:48:35.120]   is exactly what antitrust law is about.
[00:48:35.120 --> 00:48:36.800]   It's not a monopoly.
[00:48:36.800 --> 00:48:39.920]   Under legal definition, it's not a monopoly.
[00:48:39.920 --> 00:48:41.120]   It's a, well, that's true.
[00:48:41.120 --> 00:48:42.480]   We got a really, that's true.
[00:48:42.480 --> 00:48:44.320]   We got to really understand what monopoly is.
[00:48:44.320 --> 00:48:44.560]   Yeah.
[00:48:44.560 --> 00:48:48.320]   Using the fact that you're very, very large is not,
[00:48:48.320 --> 00:48:49.440]   it's not saying.
[00:48:49.440 --> 00:48:50.000]   No, I know.
[00:48:50.000 --> 00:48:54.080]   It's a small company can use its clout to get into a new market.
[00:48:54.080 --> 00:48:56.720]   A medium and a large company can use its clout to get into market.
[00:48:56.720 --> 00:48:58.000]   Here's what I would argue you may not.
[00:48:58.000 --> 00:48:59.760]   Here's the problem.
[00:48:59.760 --> 00:49:03.920]   The, the, the, the pardon me, moral panic we have is aimed at a few specific companies.
[00:49:03.920 --> 00:49:06.640]   Instead say if you're concerned about currency,
[00:49:06.640 --> 00:49:08.400]   because Facebook's not the only one that's going to develop currencies.
[00:49:08.400 --> 00:49:08.960]   All those are are too.
[00:49:08.960 --> 00:49:10.080]   If you're concerned about them,
[00:49:10.080 --> 00:49:12.880]   then figure out what you want to do in regulation of the currencies and those
[00:49:12.880 --> 00:49:16.480]   activities as opposed to saying, if Facebook doesn't, we hate it.
[00:49:16.480 --> 00:49:19.360]   That's, that's not a saying regulatory review.
[00:49:19.360 --> 00:49:26.560]   It's a little disingenuous because Facebook has a history of pushing the envelope on privacy,
[00:49:26.560 --> 00:49:27.840]   data sharing it.
[00:49:27.840 --> 00:49:31.120]   So it's not like this is an innocent or a neutral actor here.
[00:49:31.120 --> 00:49:36.640]   It is a, it is someone who has a bad history coming forth and saying,
[00:49:36.640 --> 00:49:40.160]   yeah, I'm going to, I'm going to get into this.
[00:49:40.160 --> 00:49:43.680]   It's like, if you knew the town drunk, we're going to, you know, maybe they're
[00:49:43.680 --> 00:49:46.800]   normal businesses, a new stand operator and now they're like, you know what?
[00:49:47.440 --> 00:49:50.960]   I'm going to get into the rental car business or the chauffeuring business or
[00:49:50.960 --> 00:49:52.000]   I'm going to loan you money.
[00:49:52.000 --> 00:49:56.640]   You might be like, hmm, I don't know if you're the right company to do that.
[00:49:56.640 --> 00:49:58.720]   I think we can agree there's cause for concern.
[00:49:58.720 --> 00:50:03.920]   It may not be a trust, any trust issue because of the definition of trust.
[00:50:03.920 --> 00:50:07.840]   Although one could say Google has pretty much of a monopoly on search.
[00:50:07.840 --> 00:50:12.160]   We could also see that the market uses new searches.
[00:50:12.160 --> 00:50:13.680]   Okay. Well, hold on.
[00:50:13.680 --> 00:50:17.840]   You could also see the market respond. So I don't think portal is selling very well.
[00:50:17.840 --> 00:50:20.640]   The Facebook portal, they're, they're cameras.
[00:50:20.640 --> 00:50:20.880]   Yeah.
[00:50:20.880 --> 00:50:22.480]   And I think one of the.
[00:50:22.480 --> 00:50:24.960]   Which sales on it for a time day.
[00:50:24.960 --> 00:50:26.320]   Oh, really?
[00:50:26.320 --> 00:50:28.320]   So it was one of the big sellers at prime.
[00:50:28.320 --> 00:50:32.080]   I don't know if it sold well, but it was cheap as hell.
[00:50:32.080 --> 00:50:35.440]   That is not a sign that it sold well.
[00:50:35.440 --> 00:50:37.600]   That is a sign it was not selling well.
[00:50:37.600 --> 00:50:42.560]   And by the way, somebody who spent like $1,200 on two Facebook portals,
[00:50:42.560 --> 00:50:44.000]   I am not happy about that.
[00:50:44.000 --> 00:50:50.080]   So it might be that the market makes the decision not to use Facebook's currency.
[00:50:50.080 --> 00:50:52.640]   And maybe what you need to look for is Facebook.
[00:50:52.640 --> 00:50:59.600]   Pushing it at the expense of other currencies or using its position to force people to use this
[00:50:59.600 --> 00:51:00.720]   particular currency.
[00:51:00.720 --> 00:51:05.040]   And that's that might be antitrust.
[00:51:05.040 --> 00:51:06.000]   It's very hard to say.
[00:51:08.800 --> 00:51:14.320]   I mean, we see this already with like ISPs and they're like discounting, you know, usage data
[00:51:14.320 --> 00:51:21.840]   caps, you know, for certain things like, let's say AT&T and saying, yeah, I think it's sufficient.
[00:51:21.840 --> 00:51:22.960]   Your case is sufficient.
[00:51:22.960 --> 00:51:26.960]   Facebook is the town drunk and they're trying to get in finance.
[00:51:26.960 --> 00:51:32.160]   And regardless of whether they're monopoly or not, they probably shouldn't be running the banking
[00:51:32.160 --> 00:51:32.560]   sector.
[00:51:32.560 --> 00:51:36.720]   So regulate the banking generically.
[00:51:36.720 --> 00:51:39.680]   The problem I have with these tax bills too, these tax laws in Europe, we're going to go
[00:51:39.680 --> 00:51:40.400]   after one company.
[00:51:40.400 --> 00:51:42.720]   That's that's prima-facing, not fair.
[00:51:42.720 --> 00:51:46.800]   You have a tax bill that gives equal treatment to companies.
[00:51:46.800 --> 00:51:50.960]   You have a banking regulation that is equal, which is just smarter regulation too, because
[00:51:50.960 --> 00:51:54.640]   who knows, along could come TikTok and decide it wants to be a bank.
[00:51:54.640 --> 00:51:55.040]   Yes.
[00:51:55.040 --> 00:51:56.000]   And that's right.
[00:51:56.000 --> 00:52:00.240]   It's a private venue to regulate banking as a generic activity.
[00:52:00.240 --> 00:52:00.720]   Fine.
[00:52:00.720 --> 00:52:00.960]   Okay.
[00:52:00.960 --> 00:52:06.240]   However, however, however, I think what's really happening here is a challenge to
[00:52:06.800 --> 00:52:08.240]   government and nations.
[00:52:08.240 --> 00:52:10.160]   Wars are fought without weapons and armies.
[00:52:10.160 --> 00:52:13.040]   They're fought with data and kids in Moldavia.
[00:52:13.040 --> 00:52:19.920]   And currencies are going to be started, have been started by entities other than governments.
[00:52:19.920 --> 00:52:24.720]   There's a larger challenge to government here, and I'm not sure the government's going to win
[00:52:24.720 --> 00:52:27.840]   in some of these wars, whether it's Facebook or anybody else.
[00:52:27.840 --> 00:52:29.600]   Well, yeah, they-
[00:52:29.600 --> 00:52:32.480]   That's a fair point, but that is kind of a different point.
[00:52:32.480 --> 00:52:35.280]   They may not win, but I think it is their battle to play.
[00:52:35.280 --> 00:52:36.800]   It's definitely their battle to take.
[00:52:36.800 --> 00:52:38.560]   But I'm saying then go about it wisely.
[00:52:38.560 --> 00:52:39.840]   Don't go about it emotionally.
[00:52:39.840 --> 00:52:44.800]   I was in Madrid last week and I had to panel people questioning me,
[00:52:44.800 --> 00:52:46.800]   and one of them was, "Well, what about breaking them up?"
[00:52:46.800 --> 00:52:50.000]   And I said, "That's emotional. Tell me why. It's pure emotion."
[00:52:50.000 --> 00:52:55.600]   And a lot of this discussion is emotional, and that's the danger.
[00:52:55.600 --> 00:52:56.400]   The old evidence.
[00:52:56.400 --> 00:53:03.120]   There is a case to be made that these companies, as they get bigger and bigger,
[00:53:03.120 --> 00:53:06.480]   and put more of their fingers and more pies, and gather more data about us,
[00:53:06.480 --> 00:53:12.000]   become more and more difficult to disentangle from the American economy.
[00:53:12.000 --> 00:53:13.840]   So maybe not breaking them up.
[00:53:13.840 --> 00:53:15.760]   Here's a proposal from Robert Epstein.
[00:53:15.760 --> 00:53:17.360]   I'm sure you hate this too.
[00:53:17.360 --> 00:53:19.600]   This is in Bloomberg Business Week.
[00:53:19.600 --> 00:53:23.920]   To break Google's monopoly on search, don't dismantle it.
[00:53:23.920 --> 00:53:26.400]   Make its index public.
[00:53:26.400 --> 00:53:30.240]   Give other companies access to the Google index.
[00:53:30.240 --> 00:53:31.280]   You could have an API.
[00:53:31.280 --> 00:53:32.560]   In fact, they do have an API.
[00:53:32.560 --> 00:53:34.080]   They just don't let anybody use it.
[00:53:34.080 --> 00:53:39.920]   So then other search engines with other approaches
[00:53:39.920 --> 00:53:43.600]   could emerge that would provide some competition.
[00:53:43.600 --> 00:53:44.560]   What about that idea?
[00:53:44.560 --> 00:53:50.880]   Again, it's a specific attack on a specific company's specific funders.
[00:53:50.880 --> 00:53:53.200]   There's no other company in this realm.
[00:53:53.200 --> 00:53:54.480]   Let's go to that other story.
[00:53:54.480 --> 00:53:55.760]   Let's go to that other story.
[00:53:55.760 --> 00:53:58.400]   There are others.
[00:53:58.400 --> 00:53:59.280]   There is Bing.
[00:54:00.080 --> 00:54:04.000]   Oh, come on. Bing is incredible as to how the functionality is put up.
[00:54:04.000 --> 00:54:05.200]   5% of the market.
[00:54:05.200 --> 00:54:07.360]   Well, because the market can speak.
[00:54:07.360 --> 00:54:10.720]   So what's to say that people prefer Google?
[00:54:10.720 --> 00:54:11.920]   Here's what Epstein says.
[00:54:11.920 --> 00:54:15.120]   There's precedent for this both in law and Google's business practices.
[00:54:15.120 --> 00:54:18.560]   When private ownership of essential resources and services,
[00:54:18.560 --> 00:54:22.000]   water, electricity, telecommunications, and so on,
[00:54:22.000 --> 00:54:24.000]   no longer serves the public interest,
[00:54:24.000 --> 00:54:26.160]   governments often step in to control them.
[00:54:26.720 --> 00:54:33.200]   The 1956 consent decree AT&T, the US forced this is before the breakup,
[00:54:33.200 --> 00:54:35.840]   AT&T to share its patents with other AT&T's.
[00:54:35.840 --> 00:54:37.840]   Well, AT&T was a monopoly.
[00:54:37.840 --> 00:54:38.720]   Let's be clear about that.
[00:54:38.720 --> 00:54:39.200]   Come on.
[00:54:39.200 --> 00:54:41.920]   Google, I don't care what straw,
[00:54:41.920 --> 00:54:44.480]   straw, drug search engines you pull up,
[00:54:44.480 --> 00:54:47.600]   Google completely dominates in search.
[00:54:47.600 --> 00:54:49.760]   But search is now so important.
[00:54:49.760 --> 00:54:51.680]   Compared to the other things that go on.
[00:54:51.680 --> 00:54:52.480]   That's the other thing.
[00:54:52.480 --> 00:54:54.080]   I would say for most people,
[00:54:54.080 --> 00:54:55.440]   Google is the internet.
[00:54:55.440 --> 00:54:57.200]   That's why the right to be forgotten
[00:54:57.200 --> 00:54:59.920]   texts Google, because if it's not in Google's index,
[00:54:59.920 --> 00:55:01.360]   nobody's going to see that page.
[00:55:01.360 --> 00:55:02.640]   Google is the internet.
[00:55:02.640 --> 00:55:10.000]   Line, roll us into me, go to line 30 in the rundown.
[00:55:10.000 --> 00:55:11.760]   You have the BBC with an accent.
[00:55:11.760 --> 00:55:17.200]   Google search isn't everything.
[00:55:17.200 --> 00:55:19.920]   It's just almost anything.
[00:55:19.920 --> 00:55:23.920]   This is a Mark Ward technology correspondent writing.
[00:55:24.560 --> 00:55:31.760]   Just simply arguing that this function that we thought was all
[00:55:31.760 --> 00:55:33.760]   encompassing does happen a lot of other ways.
[00:55:33.760 --> 00:55:38.640]   Yes, but Google is getting their hands into all of this as well.
[00:55:38.640 --> 00:55:44.400]   They have their home and voice and they use lots of their data that they only they gather
[00:55:44.400 --> 00:55:47.280]   to make it better.
[00:55:47.280 --> 00:55:50.400]   And I want to welcome my new master.
[00:55:53.280 --> 00:55:56.080]   This is not a compelling, I mean, it's an opinion piece.
[00:55:56.080 --> 00:55:58.080]   It's not a reportage.
[00:55:58.080 --> 00:56:00.080]   Oh, no, that's emotion.
[00:56:00.080 --> 00:56:02.560]   That's not facts and data.
[00:56:02.560 --> 00:56:06.000]   And honestly, it doesn't provide me with it.
[00:56:06.000 --> 00:56:08.080]   It just says, you know, somebody else could come along.
[00:56:08.080 --> 00:56:13.200]   As has happened in the past, Microsoft was going to take over the world.
[00:56:13.200 --> 00:56:14.800]   Microsoft was the big evil.
[00:56:14.800 --> 00:56:16.480]   There was no being Microsoft ever.
[00:56:20.000 --> 00:56:24.240]   Once I don't see a story in the Guardian 2009, somebody put it from a decade ago.
[00:56:24.240 --> 00:56:29.440]   No one's ever going to honest to God, Guardian, no one's ever going to unseat WhatsApp.
[00:56:29.440 --> 00:56:32.960]   I don't want to.
[00:56:32.960 --> 00:56:36.480]   I mean, I mean, no, my space, my space.
[00:56:36.480 --> 00:56:36.880]   Yeah.
[00:56:36.880 --> 00:56:39.600]   There was a different time.
[00:56:39.600 --> 00:56:42.240]   My space had up to 25 million users.
[00:56:42.240 --> 00:56:45.600]   I mean, this Google is so dominating.
[00:56:45.600 --> 00:56:52.080]   The Google index, the crown jewel of Google, has been so long cultivated that it's hard to
[00:56:52.080 --> 00:56:53.840]   imagine another company coming.
[00:56:53.840 --> 00:56:56.080]   Look at, I mean, we talk a lot about DuckDuckGo.
[00:56:56.080 --> 00:56:57.760]   That's an awesome thing.
[00:56:57.760 --> 00:57:00.480]   The index is not the value.
[00:57:00.480 --> 00:57:02.800]   The index, fine.
[00:57:02.800 --> 00:57:06.000]   So we save you the effort and expense of scraping the web.
[00:57:06.000 --> 00:57:08.720]   Well, that's considerable, but that's not where the brilliance of Google is.
[00:57:08.720 --> 00:57:11.520]   Well, good.
[00:57:11.520 --> 00:57:12.960]   Then you don't mind if we open it up.
[00:57:15.520 --> 00:57:18.960]   And so we're going to end up with another crappy web search.
[00:57:18.960 --> 00:57:20.240]   No, it won't be crappy to be.
[00:57:20.240 --> 00:57:25.680]   Well, no, I'll tell you after my pick of the week, because I learned from this article that
[00:57:25.680 --> 00:57:31.600]   Google does offer one and only one company in the world full access to the API.
[00:57:31.600 --> 00:57:33.200]   It's a company called StartPage.
[00:57:33.200 --> 00:57:37.920]   And Google does not get information back from StartPage.
[00:57:37.920 --> 00:57:43.760]   The Google search, if I search for Jeff Jarvis, this will be the equivalent result of an incognito
[00:57:43.760 --> 00:57:46.160]   search using Google because it is the API.
[00:57:46.160 --> 00:57:49.280]   It is the Google, but notice anonymous view.
[00:57:49.280 --> 00:57:51.280]   Google lets them do this.
[00:57:51.280 --> 00:57:56.640]   According to this article, I'm not sure I buy this.
[00:57:56.640 --> 00:58:00.400]   Google allows this kind of sharing with a chosen few, most notably a small but ingenious
[00:58:00.400 --> 00:58:02.800]   company called StartPage in the Netherlands.
[00:58:02.800 --> 00:58:08.880]   In 2009, Google granted StartPage access to its index and return for fees generated by ads
[00:58:08.880 --> 00:58:11.280]   placed near the StartPage search results.
[00:58:11.280 --> 00:58:13.680]   I actually don't see any ads.
[00:58:13.680 --> 00:58:21.680]   What I don't see and I love is the YouTube slide show, the Google AMP slide show.
[00:58:21.680 --> 00:58:24.480]   There's a Wikipedia link, which is great.
[00:58:24.480 --> 00:58:26.240]   So there is a knowledge bar.
[00:58:26.240 --> 00:58:28.320]   You can make this your default search engine.
[00:58:28.320 --> 00:58:32.800]   It is my default search engine in Firefox.
[00:58:32.800 --> 00:58:38.800]   I think it gives me, I mean, would you agree these are good search results for your name, Jeff?
[00:58:38.800 --> 00:58:39.360]   Yeah.
[00:58:39.360 --> 00:58:41.680]   And Google deciding to do this.
[00:58:42.400 --> 00:58:43.680]   I have no objection to Google.
[00:58:43.680 --> 00:58:44.160]   Bravo.
[00:58:44.160 --> 00:58:44.640]   Bravo.
[00:58:44.640 --> 00:58:50.080]   But government mandating it because they happen to have a Jones about Google particularly,
[00:58:50.080 --> 00:58:54.080]   I find troubling as precedent.
[00:58:54.080 --> 00:58:58.880]   I kind of like to see them mandate it because then we could see other companies like StartPage.
[00:58:58.880 --> 00:59:00.160]   Under what legal time?
[00:59:00.160 --> 00:59:02.320]   I worry that StartPage at any point.
[00:59:02.320 --> 00:59:04.640]   But what legal doctrine?
[00:59:04.640 --> 00:59:09.680]   I just gave you water, electricity, it's not a monopoly.
[00:59:09.680 --> 00:59:12.400]   Where it is monopoly, Leo, where it is, closed-room monopoly.
[00:59:12.400 --> 00:59:14.960]   This does not require it, by the way, it's advertising.
[00:59:14.960 --> 00:59:17.840]   This does not require it to be a monopoly.
[00:59:17.840 --> 00:59:21.600]   It's merely saying it's in the national, there's no law, you make the law.
[00:59:21.600 --> 00:59:25.760]   It's in the national interest that we, it's a little socialistic.
[00:59:25.760 --> 00:59:28.720]   Under what there has to be some legal doctrine in place.
[00:59:28.720 --> 00:59:29.520]   Well, they exist.
[00:59:29.520 --> 00:59:30.880]   That's doing this for a reason.
[00:59:30.880 --> 00:59:31.360]   It exists.
[00:59:31.360 --> 00:59:34.720]   Your water company is not a prime company.
[00:59:34.720 --> 00:59:35.680]   It's a really bad.
[00:59:35.680 --> 00:59:38.560]   Your water company is not a private company.
[00:59:38.560 --> 00:59:39.360]   It is run by.
[00:59:39.360 --> 00:59:41.840]   Don't give me the utility thing.
[00:59:41.840 --> 00:59:43.280]   That's so, you know, why?
[00:59:43.280 --> 00:59:48.160]   Because that's going to, that's that leads straight into the White House and Ted Cruz
[00:59:48.160 --> 00:59:52.160]   saying we're not going to force you to publish things you don't want to publish.
[00:59:52.160 --> 00:59:54.880]   Because you don't think Google is an internet utility?
[00:59:54.880 --> 00:59:56.800]   No, absolutely not.
[00:59:56.800 --> 00:59:59.280]   Absolutely not.
[00:59:59.280 --> 01:00:02.160]   I think it's.
[01:00:02.160 --> 01:00:03.920]   If we ever close the utility.
[01:00:03.920 --> 01:00:04.480]   Watch out.
[01:00:04.480 --> 01:00:08.240]   Watch out to what the Donald Trump's do to it.
[01:00:08.880 --> 01:00:12.400]   Watch out to what others in other countries judge.
[01:00:12.400 --> 01:00:13.680]   I think that's a straw dog.
[01:00:13.680 --> 01:00:16.160]   I think we could fight that battle when that time comes.
[01:00:16.160 --> 01:00:18.160]   I don't think it's at all unreasonable to say.
[01:00:18.160 --> 01:00:20.720]   They're there yesterday in the Senate Judiciary Committee.
[01:00:20.720 --> 01:00:26.800]   The challenge always with regulation is it's going to come from the parties in charge.
[01:00:26.800 --> 01:00:31.040]   And presumably if, I mean, we used to have, and I'm not being
[01:00:31.040 --> 01:00:34.000]   trite when I say we used to have a Congress that worked,
[01:00:34.000 --> 01:00:37.680]   but we used to actually have people coming from both sides to create.
[01:00:38.480 --> 01:00:39.520]   Reasonable legislation.
[01:00:39.520 --> 01:00:40.800]   230 came that way, right.
[01:00:40.800 --> 01:00:46.880]   But I will say to say that we should not
[01:00:46.880 --> 01:00:52.160]   regulate something because we are afraid of what the party in charge will do
[01:00:52.160 --> 01:00:56.960]   is just giving up and saying our democracy is clearly broken and we should just give up down.
[01:00:56.960 --> 01:01:01.680]   So we either have to say, okay, it's broken.
[01:01:01.680 --> 01:01:02.640]   I hope it fix.
[01:01:02.640 --> 01:01:06.560]   I hope we can get a fix and I hope our government can step in when we need it to.
[01:01:07.520 --> 01:01:12.880]   To regulate problematic industries or we just say, okay, I'm moving to
[01:01:12.880 --> 01:01:14.240]   pick your country.
[01:01:14.240 --> 01:01:17.120]   Well, I just think throwing up your hands and saying, it's too,
[01:01:17.120 --> 01:01:18.720]   it's impossible to regulate these guys.
[01:01:18.720 --> 01:01:20.720]   So let's just let them go and see what happens.
[01:01:20.720 --> 01:01:21.760]   I'm not saying that.
[01:01:21.760 --> 01:01:26.480]   I have, I have put forward regulatory regimes that I do believe in.
[01:01:26.480 --> 01:01:32.160]   I believe we're more flexible, but I believe we're more negotiable that are based on principle,
[01:01:32.160 --> 01:01:37.120]   that are not based on momentary hatred or moral panic.
[01:01:37.440 --> 01:01:40.160]   So I'm not saying let's do that then, but let's do something.
[01:01:40.160 --> 01:01:43.040]   Let's talk about something else.
[01:01:43.040 --> 01:01:45.520]   Y'all, y'all are yelling so much today.
[01:01:45.520 --> 01:01:48.480]   Oh man, we're overriding the Botox.
[01:01:48.480 --> 01:01:50.960]   How about the baby named after Google?
[01:01:50.960 --> 01:01:52.960]   That'll cheer you up.
[01:01:52.960 --> 01:01:54.160]   And nice.
[01:01:54.160 --> 01:01:55.360]   What did nice Google do?
[01:01:55.360 --> 01:01:56.800]   What did nice Google do?
[01:01:56.800 --> 01:02:03.840]   What did a baby boy in Indonesia given the name Google got a goody bag from Google after
[01:02:03.840 --> 01:02:06.000]   the story of his name went viral.
[01:02:06.000 --> 01:02:11.040]   Ella Karina, the mother of the baby, did not want to list every item given to her son,
[01:02:11.040 --> 01:02:13.520]   but there are a few items.
[01:02:13.520 --> 01:02:18.640]   Google Indonesia sent the items, including a baby jumper branded with a company's logo.
[01:02:18.640 --> 01:02:22.880]   Oh, at first Karina said she didn't agree with her company.
[01:02:22.880 --> 01:02:28.400]   Her husband's idea to name the son after the company, but she got became more accepting of
[01:02:28.400 --> 01:02:32.560]   the notion after the positive responses they received.
[01:02:32.560 --> 01:02:33.760]   We'll get free stuff.
[01:02:33.760 --> 01:02:34.960]   Little baby Google.
[01:02:35.600 --> 01:02:42.400]   That kid's going to have such a nightmare life just because of things like hey,
[01:02:42.400 --> 01:02:47.440]   I don't know what it may be an Indonesian Google means something good.
[01:02:47.440 --> 01:02:49.840]   Maybe it means chocolate cake.
[01:02:49.840 --> 01:02:51.200]   I think it's actually a fun night.
[01:02:51.200 --> 01:02:52.080]   Google.
[01:02:52.080 --> 01:02:53.520]   It's not a terrible name.
[01:02:53.520 --> 01:02:54.240]   Great baby name.
[01:02:54.240 --> 01:02:55.520]   Every baby named.
[01:02:55.520 --> 01:02:57.120]   Everyone's leaving their kids Alexa anymore.
[01:02:57.120 --> 01:02:59.120]   So what's the little kid let's play a game here.
[01:02:59.120 --> 01:03:03.360]   What tech brands would be okay as names without associations?
[01:03:03.360 --> 01:03:03.600]   Bing?
[01:03:03.600 --> 01:03:04.640]   Yeah.
[01:03:04.640 --> 01:03:05.120]   Okay.
[01:03:05.120 --> 01:03:05.920]   Bing Crosby.
[01:03:05.920 --> 01:03:07.200]   Uber.
[01:03:07.200 --> 01:03:07.920]   Chandler Bing.
[01:03:07.920 --> 01:03:09.120]   Chandler Bing.
[01:03:09.120 --> 01:03:10.080]   Uber.
[01:03:10.080 --> 01:03:12.160]   Do I just land Uber?
[01:03:12.160 --> 01:03:12.720]   All us.
[01:03:12.720 --> 01:03:13.760]   A little tough there.
[01:03:13.760 --> 01:03:13.920]   No.
[01:03:13.920 --> 01:03:16.560]   Is it Uber soft?
[01:03:16.560 --> 01:03:16.880]   No.
[01:03:16.880 --> 01:03:20.800]   That poor kid if it's a boy bad, bad, bad.
[01:03:20.800 --> 01:03:27.520]   Face there is a kid named Facebook after the Arab Spring.
[01:03:27.520 --> 01:03:29.040]   That's not great.
[01:03:29.040 --> 01:03:29.760]   Yeah.
[01:03:29.760 --> 01:03:33.120]   I'm wondering if I think was a kid in Egypt was named Facebook because it's
[01:03:33.120 --> 01:03:34.320]   Yo Twitter.
[01:03:34.320 --> 01:03:35.920]   Twitter is a good Twitter.
[01:03:35.920 --> 01:03:37.360]   Twitter is a good name.
[01:03:37.360 --> 01:03:38.000]   Twitter.
[01:03:38.000 --> 01:03:39.120]   By the way, I like it.
[01:03:39.120 --> 01:03:42.400]   This is where this is where in while we're on the subject.
[01:03:42.400 --> 01:03:44.320]   We're trying to lighten things up.
[01:03:44.320 --> 01:03:45.920]   Do you have the new Twitter look?
[01:03:45.920 --> 01:03:46.480]   Do you like it?
[01:03:46.480 --> 01:03:48.480]   I've had it for a while.
[01:03:48.480 --> 01:03:51.040]   I like it, but mine said dark moats so Jeff would hate it.
[01:03:51.040 --> 01:03:53.680]   I know.
[01:03:53.680 --> 01:03:54.240]   I'm sorry.
[01:03:54.240 --> 01:03:55.840]   So I keep checking.
[01:03:55.840 --> 01:03:56.480]   I don't know why.
[01:03:56.480 --> 01:03:59.200]   I honestly don't know why I have such a visual reaction.
[01:03:59.200 --> 01:04:00.480]   I guess dark moat but I do.
[01:04:00.480 --> 01:04:00.960]   It's earnest.
[01:04:00.960 --> 01:04:02.240]   It is different.
[01:04:02.240 --> 01:04:05.840]   And so I had a hard time with it for the first probably three days.
[01:04:05.840 --> 01:04:09.200]   And I was like, but I forced myself to do it because it's easy on the eyes.
[01:04:09.200 --> 01:04:13.760]   Because I forced myself to like embrace something for a week or so.
[01:04:13.760 --> 01:04:16.160]   And then I'm like, yeah, it's still not me or.
[01:04:16.160 --> 01:04:16.720]   Oh, okay.
[01:04:16.720 --> 01:04:17.600]   Join us on the call.
[01:04:17.600 --> 01:04:18.720]   And it is easier.
[01:04:18.720 --> 01:04:19.600]   Dark moat.
[01:04:19.600 --> 01:04:21.520]   Come to us.
[01:04:21.520 --> 01:04:23.120]   Oh, I'm going to sneeze.
[01:04:23.120 --> 01:04:24.960]   Sneize to us.
[01:04:24.960 --> 01:04:25.840]   All right.
[01:04:25.840 --> 01:04:27.040]   I'm going to log into Twitter.
[01:04:27.040 --> 01:04:28.560]   I've been checking.
[01:04:28.560 --> 01:04:29.360]   Lisa got it.
[01:04:30.320 --> 01:04:32.080]   It's slowly rolling out this week.
[01:04:32.080 --> 01:04:34.640]   And Lisa got it.
[01:04:34.640 --> 01:04:35.760]   But yes, bless you.
[01:04:35.760 --> 01:04:36.960]   But I have.
[01:04:36.960 --> 01:04:38.160]   I don't know if I've got it yet.
[01:04:38.160 --> 01:04:42.320]   I just hope Twitter doesn't destroy tweet deck because as far as I'm concerned,
[01:04:42.320 --> 01:04:43.440]   that's the way.
[01:04:43.440 --> 01:04:45.040]   That's the only way to use Twitter.
[01:04:45.040 --> 01:04:45.920]   Nope.
[01:04:45.920 --> 01:04:47.040]   I don't have it yet.
[01:04:47.040 --> 01:04:50.080]   I still have the whole new Twitter is coming.
[01:04:50.080 --> 01:04:51.680]   What does that look like?
[01:04:51.680 --> 01:04:53.520]   Tweet deck.
[01:04:53.520 --> 01:04:54.960]   Oh, no.
[01:04:54.960 --> 01:04:55.840]   No, I know a tweet deck.
[01:04:55.840 --> 01:04:56.320]   Okay.
[01:04:56.320 --> 01:04:56.800]   That's.
[01:04:56.800 --> 01:04:57.520]   Do you have it?
[01:04:57.520 --> 01:04:58.560]   Carson on that machine?
[01:04:58.560 --> 01:04:59.520]   Can you forget?
[01:04:59.520 --> 01:05:00.720]   Can you see the new Twitter?
[01:05:00.720 --> 01:05:00.720]   I do.
[01:05:00.720 --> 01:05:02.240]   I've forgotten all about old Twitter.
[01:05:02.240 --> 01:05:05.920]   So this everything's on the left now, right?
[01:05:05.920 --> 01:05:08.160]   And then they put lists over there, which is nice.
[01:05:08.160 --> 01:05:09.440]   I'm glad they brought lists back.
[01:05:09.440 --> 01:05:12.320]   Okay.
[01:05:12.320 --> 01:05:14.560]   They do keep.
[01:05:14.560 --> 01:05:16.720]   Well, I think they were already switching your time.
[01:05:16.720 --> 01:05:19.120]   So you know what they didn't do?
[01:05:19.120 --> 01:05:20.960]   I don't know if they should.
[01:05:20.960 --> 01:05:24.400]   I remember Twitter is now putting when a major political figure,
[01:05:24.400 --> 01:05:26.000]   somebody with over a hundred thousand followers,
[01:05:26.000 --> 01:05:27.840]   says something that violates the rules.
[01:05:28.720 --> 01:05:29.840]   I've never seen that.
[01:05:29.840 --> 01:05:30.800]   I've never seen it yet.
[01:05:30.800 --> 01:05:36.640]   Some people thought that the Donald Trump's go back to home where you came from,
[01:05:36.640 --> 01:05:41.120]   to the US citizens members of the House of Representatives,
[01:05:41.120 --> 01:05:44.480]   that that may be deserved that thing, but they didn't do that.
[01:05:44.480 --> 01:05:47.840]   I go back to what happened on Capitol Hill just yesterday,
[01:05:47.840 --> 01:05:50.640]   where the right wing is screaming that there be.
[01:05:50.640 --> 01:05:52.080]   Twitter is being sensitive.
[01:05:52.080 --> 01:05:53.840]   Twitter is saying, well, they're not scared.
[01:05:53.840 --> 01:05:56.320]   Yeah.
[01:05:56.320 --> 01:05:57.520]   You think that says prudent?
[01:05:57.520 --> 01:05:58.320]   Yeah.
[01:05:58.320 --> 01:06:02.000]   Well, it's interesting.
[01:06:02.000 --> 01:06:07.920]   One point is it the moral obligation of a company to do things like that?
[01:06:07.920 --> 01:06:10.320]   At what point do you understand and say,
[01:06:10.320 --> 01:06:12.240]   well, if you're going to make a rule, you're going to have to do it.
[01:06:12.240 --> 01:06:13.760]   I don't know.
[01:06:13.760 --> 01:06:14.400]   I don't care.
[01:06:14.400 --> 01:06:14.960]   It's Twitter.
[01:06:14.960 --> 01:06:15.520]   It don't cares.
[01:06:15.520 --> 01:06:18.160]   Here's the New York Times.
[01:06:18.160 --> 01:06:19.280]   I don't know if I agree with this.
[01:06:19.280 --> 01:06:20.880]   This is another VidCon story.
[01:06:20.880 --> 01:06:27.360]   The New York Times, Kevin Russe, who I like a lot and this very smart tech reporter
[01:06:27.360 --> 01:06:29.200]   don't scoff at the influencers.
[01:06:29.200 --> 01:06:31.440]   They're taking over the world.
[01:06:31.440 --> 01:06:36.960]   He says that all these TikTokers and YouTubers, they're going to be in charge.
[01:06:36.960 --> 01:06:41.280]   They're going to be in charge, which I don't know.
[01:06:41.280 --> 01:06:47.120]   Maybe he says, well, first of all, being an influencer is a tough job.
[01:06:47.120 --> 01:06:50.240]   These people are essentially one person startups.
[01:06:50.240 --> 01:06:55.520]   The best of them can spot trends, experiment relentlessly with new platforms and formats,
[01:06:55.520 --> 01:06:57.600]   build an authentic connection with an audience,
[01:06:57.600 --> 01:06:59.920]   play close attention to their channel analytics,
[01:06:59.920 --> 01:07:03.200]   figure out how to distinguish themselves in a crowded media environment,
[01:07:03.200 --> 01:07:05.840]   all while churning out a constant stream of new content.
[01:07:05.840 --> 01:07:08.880]   What better qualifications for President of the United States?
[01:07:08.880 --> 01:07:09.600]   I ask you.
[01:07:09.600 --> 01:07:16.880]   He says these social media guys, he says, just look at AOC, basically,
[01:07:17.840 --> 01:07:25.600]   she is the opening wedge of young social media aware, literate people.
[01:07:25.600 --> 01:07:28.480]   Influencer culture, he writes, is already an established force.
[01:07:28.480 --> 01:07:33.120]   A generation of direct to consumer brands were built using the tools and tactics of
[01:07:33.120 --> 01:07:38.960]   social media like glossier, the influence beloved beauty company that raised $100 million at a
[01:07:38.960 --> 01:07:40.720]   billion valuation or away.
[01:07:40.720 --> 01:07:46.400]   The luggage startup, whose ubiquitous Instagram ads helped it reach a valuation of $1.4 billion,
[01:07:46.400 --> 01:07:47.280]   by the way.
[01:07:47.280 --> 01:07:48.880]   Jeez, podcast.
[01:07:48.880 --> 01:07:51.360]   We did podcast ads for way two, but I don't know if we helped them.
[01:07:51.360 --> 01:07:54.080]   No, I had them on one of my podcasts.
[01:07:54.080 --> 01:07:54.560]   Very well.
[01:07:54.560 --> 01:08:01.760]   Natalie Zadeh, you may know her as Natalie's outlet, 10 million subscribers on YouTube,
[01:08:01.760 --> 01:08:07.680]   a wave of influencers who treated their online brand building as a business rather than a fun hobby.
[01:08:07.680 --> 01:08:15.760]   So what if instead, okay, well, no, okay, that kills that point. Never mind.
[01:08:15.760 --> 01:08:16.960]   Go back, keep going.
[01:08:16.960 --> 01:08:21.760]   So for instance, four years ago, she came to VidCon, but she was a marketing student,
[01:08:21.760 --> 01:08:23.920]   and less than 7,000 subscribers.
[01:08:23.920 --> 01:08:27.920]   She decided to study her favorite YouTubers, figure out what made it work,
[01:08:27.920 --> 01:08:30.080]   tested videos in multiple genres.
[01:08:30.080 --> 01:08:34.480]   She said she was watching people like Michelle Fon that were building legacies out of honestly,
[01:08:34.480 --> 01:08:36.240]   just being really relatable online.
[01:08:36.240 --> 01:08:38.480]   It was always an explanation.
[01:08:38.480 --> 01:08:41.440]   Which company's just simply can't learn the value of relationships?
[01:08:41.440 --> 01:08:41.760]   Yeah.
[01:08:41.760 --> 01:08:42.640]   A trust.
[01:08:42.640 --> 01:08:43.760]   Oh my gosh.
[01:08:44.880 --> 01:08:47.600]   Yeah, I was going to say, what if this is about sincerity?
[01:08:47.600 --> 01:08:51.040]   So maybe I should say, what if it's about sincerity in gaming the algorithms?
[01:08:51.040 --> 01:08:51.520]   But like,
[01:08:51.520 --> 01:08:56.640]   You do have to wonder why Brian Williams is still on MSNBC after all this time.
[01:08:56.640 --> 01:09:00.880]   I mean, honest to God, this guy is not a network anchor.
[01:09:00.880 --> 01:09:01.600]   He's terrible.
[01:09:01.600 --> 01:09:05.120]   Never did you get from from fun to?
[01:09:05.120 --> 01:09:09.040]   Well, I'm just saying you're, I'm saying mainstream media does seem to have missed the point
[01:09:09.040 --> 01:09:13.200]   that integrity and relatability and authenticness are important.
[01:09:13.760 --> 01:09:19.600]   They think that I sat in meetings at NBC and they just, this is before the controversy with him.
[01:09:19.600 --> 01:09:23.200]   And it was all, they were all built around Brian, Brian, Brian, Brian, Brian, Brian.
[01:09:23.200 --> 01:09:26.240]   This is their view of the relationship is a celebrity.
[01:09:26.240 --> 01:09:30.640]   And they don't understand that Michelle Fon and bigger and huge.
[01:09:30.640 --> 01:09:35.040]   The Green Brothers and name them.
[01:09:35.040 --> 01:09:35.360]   Yeah.
[01:09:35.360 --> 01:09:36.800]   Of an entirely different relationship.
[01:09:36.800 --> 01:09:38.320]   I think we get full, I get full too.
[01:09:38.320 --> 01:09:39.680]   I'm a mainstream media guy.
[01:09:39.680 --> 01:09:43.440]   We get fooled by the logins of the world.
[01:09:43.440 --> 01:09:45.840]   And think, Oh God, that stuff on YouTube, that's crap.
[01:09:45.840 --> 01:09:47.440]   That's terrible content.
[01:09:47.440 --> 01:09:48.960]   I don't know why it's just the kids.
[01:09:48.960 --> 01:09:49.840]   They'll grow out of it.
[01:09:49.840 --> 01:09:53.920]   I think that's the big phrase that they use in the 30 Rock every day.
[01:09:53.920 --> 01:09:54.960]   They'll grow out of it.
[01:09:54.960 --> 01:09:56.720]   And then they'll come watch Brian Williams.
[01:09:56.720 --> 01:09:58.960]   Don't read newspapers.
[01:09:58.960 --> 01:10:00.480]   This is why I love VidCon.
[01:10:00.480 --> 01:10:03.040]   This is exactly why I so love VidCon.
[01:10:03.040 --> 01:10:06.960]   It's because it forces you and I tell every media person I meet and every TV person,
[01:10:06.960 --> 01:10:11.120]   especially I meet, they don't listen to me, to go to VidCon.
[01:10:11.120 --> 01:10:13.760]   Because you will see it entirely different relationships.
[01:10:13.760 --> 01:10:19.520]   And can't we really say that Donald Trump got elected president in great to a great deal
[01:10:19.520 --> 01:10:20.480]   because he did that.
[01:10:20.480 --> 01:10:24.400]   He knew exactly how to reach out and build his platform.
[01:10:24.400 --> 01:10:26.080]   No, no, I'm discreet.
[01:10:26.080 --> 01:10:29.680]   What he did is he used that platform to manipulate dumb old big media.
[01:10:29.680 --> 01:10:31.520]   That's true.
[01:10:31.520 --> 01:10:32.320]   They were complicit.
[01:10:32.320 --> 01:10:33.440]   And that's the secret to what he did.
[01:10:33.440 --> 01:10:34.880]   He didn't really build the relationship.
[01:10:34.880 --> 01:10:36.640]   He built the relationship and the rallies.
[01:10:36.640 --> 01:10:37.840]   I think he did.
[01:10:37.840 --> 01:10:39.520]   But the relationship was a screaming relationship.
[01:10:39.520 --> 01:10:41.680]   He has an amazing relationship with his base.
[01:10:41.680 --> 01:10:45.520]   As he said himself, he literally could go out on Fifth Avenue,
[01:10:45.520 --> 01:10:47.680]   shoot somebody and his base would go, "Yay, Donald."
[01:10:47.680 --> 01:10:50.720]   It doesn't matter what he does.
[01:10:50.720 --> 01:10:55.840]   That is an example of an incredible relationship with that 30% of American population.
[01:10:55.840 --> 01:11:01.120]   What all he does is shout back to them what they want to hear.
[01:11:01.120 --> 01:11:01.680]   Well, it works.
[01:11:01.680 --> 01:11:05.840]   I'm not saying it's good.
[01:11:05.840 --> 01:11:06.880]   It works.
[01:11:08.160 --> 01:11:09.600]   Yeah, but I think it's a different analysis.
[01:11:09.600 --> 01:11:11.680]   It's popular.
[01:11:11.680 --> 01:11:12.640]   A little regular news here.
[01:11:12.640 --> 01:11:13.360]   We've always had populism.
[01:11:13.360 --> 01:11:15.120]   And by the way, Kevin Ruiz points out.
[01:11:15.120 --> 01:11:21.040]   We've always had this with talk show hosts and movie stars and Davos attendees.
[01:11:21.040 --> 01:11:22.080]   I don't know why he's throwing that in.
[01:11:22.080 --> 01:11:23.200]   Maybe a shot at you, Jeff.
[01:11:23.200 --> 01:11:24.000]   I don't know.
[01:11:24.000 --> 01:11:26.560]   But we have always had this populism.
[01:11:26.560 --> 01:11:29.120]   You go back to Huey Long.
[01:11:29.120 --> 01:11:33.040]   Huey Long spoke to his people, right?
[01:11:34.640 --> 01:11:36.000]   7-A-Rola.
[01:11:36.000 --> 01:11:37.200]   7-A-Rola.
[01:11:37.200 --> 01:11:39.840]   Yeah, if you want to go a little further back.
[01:11:39.840 --> 01:11:44.800]   So a little bit of a breeze here.
[01:11:44.800 --> 01:11:45.520]   The bad priest, 7-A-Rola.
[01:11:45.520 --> 01:11:46.400]   Yes, breaking news.
[01:11:46.400 --> 01:11:53.600]   Bloomberg reporting, the Trump now is talking about intervening in the DoD
[01:11:53.600 --> 01:11:55.760]   cloud contract with Amazon or Microsoft.
[01:11:55.760 --> 01:11:57.840]   Oh, yeah, he's really upset about that.
[01:11:57.840 --> 01:12:00.400]   He doesn't want them to award the-
[01:12:00.400 --> 01:12:01.920]   He doesn't want Jeff Bezos to do the product.
[01:12:01.920 --> 01:12:05.040]   I think the Jedi contract, I think the 10 billion-
[01:12:05.040 --> 01:12:06.800]   Yeah, I put it up there on the rundown.
[01:12:06.800 --> 01:12:10.800]   There, he's also saying we should investigate Google's relationship to China.
[01:12:10.800 --> 01:12:16.560]   Because Peter Thiel said Google should be investigated for as a traitor.
[01:12:16.560 --> 01:12:20.160]   Did you see Dan Primak?
[01:12:20.160 --> 01:12:23.920]   Do you get his newsletter subscription, the Axios one?
[01:12:23.920 --> 01:12:27.520]   Where he was like, Peter, are you sure you're not a vampire?
[01:12:27.520 --> 01:12:30.240]   Can you deny that you are a vampire?
[01:12:30.240 --> 01:12:31.600]   And oh, it was just so-
[01:12:31.600 --> 01:12:32.720]   Very hard to prove the negative.
[01:12:32.720 --> 01:12:33.200]   Salty.
[01:12:33.200 --> 01:12:34.560]   Questions?
[01:12:34.560 --> 01:12:38.800]   Questions for Peter Thiel after his accusations.
[01:12:38.800 --> 01:12:41.840]   So a few questions, Mr. Thiel.
[01:12:41.840 --> 01:12:43.680]   Have you short-chorted Google stock?
[01:12:43.680 --> 01:12:49.360]   By the way, Thiel is one of the big investors and founders of Palantir.
[01:12:49.360 --> 01:12:53.840]   Is Palantir currently competing with Google for a major US government contract?
[01:12:53.840 --> 01:12:54.720]   Oh, so good.
[01:12:54.720 --> 01:12:56.480]   You've said on the record you're not a vampire.
[01:12:56.480 --> 01:12:58.960]   Are you able to provide independent verification this claim?
[01:12:58.960 --> 01:13:01.040]   Because were you a vampire?
[01:13:01.040 --> 01:13:02.960]   It would pose a national security risk.
[01:13:02.960 --> 01:13:08.000]   Is it because you're actually a vampire that you deny being a vampire?
[01:13:08.000 --> 01:13:10.560]   And would that not be seemingly treasonous?
[01:13:10.560 --> 01:13:13.520]   In the generally accepted societal punishment is death.
[01:13:13.520 --> 01:13:16.160]   I'll be it by wooden stake in this case.
[01:13:16.160 --> 01:13:19.520]   One shot of Axios is going to get put out of business.
[01:13:19.520 --> 01:13:20.560]   Yeah.
[01:13:20.560 --> 01:13:27.280]   You know, the only reason I don't fear so much is because the Donald says a lot of things
[01:13:27.280 --> 01:13:28.720]   and does very few of them.
[01:13:28.720 --> 01:13:33.040]   So admittedly, he's the leader of the free world.
[01:13:33.040 --> 01:13:37.600]   And others do things on his behalf, including his cabinet members,
[01:13:37.600 --> 01:13:41.360]   like rolling back certain EPA restrictions among other things.
[01:13:41.360 --> 01:13:43.600]   But I honestly think that's the real agenda.
[01:13:43.600 --> 01:13:48.880]   That the stuff, all the noise at the top is to distract you from the real agenda,
[01:13:48.880 --> 01:13:53.200]   which is getting done very effectively by appointing judges, by removing restrictions
[01:13:53.200 --> 01:13:58.000]   and regulations that doesn't get the headlines because the Donalds getting all the attention.
[01:13:58.000 --> 01:13:59.680]   So over here, look at this.
[01:13:59.680 --> 01:14:02.320]   I think let's investigate.
[01:14:02.320 --> 01:14:02.720]   Go, go.
[01:14:02.720 --> 01:14:07.280]   Meanwhile, we don't need any emissions restrictions.
[01:14:07.280 --> 01:14:09.360]   Those are silly, silly.
[01:14:09.360 --> 01:14:13.360]   That's personally my personal view of it all.
[01:14:13.360 --> 01:14:18.320]   Twitch, I'm going to apologize right now for eating.
[01:14:18.320 --> 01:14:18.960]   Please.
[01:14:18.960 --> 01:14:22.880]   One must keep one's blood sugar at a reasonable level.
[01:14:23.760 --> 01:14:26.640]   So this is before you were on the pre-talk, Stacey.
[01:14:26.640 --> 01:14:29.600]   I wonder, have you bought the how much is it here?
[01:14:29.600 --> 01:14:32.000]   $275 toaster?
[01:14:32.000 --> 01:14:35.440]   No, because I have the.
[01:14:35.440 --> 01:14:37.440]   Stick up the tune of oven.
[01:14:37.440 --> 01:14:38.640]   Her stuff's even more.
[01:14:38.640 --> 01:14:40.720]   But this is particular for.
[01:14:40.720 --> 01:14:45.440]   It only makes one slice at a time because that's better.
[01:14:45.440 --> 01:14:47.600]   Does it have a bidet?
[01:14:47.600 --> 01:14:49.840]   Does it have a bidet?
[01:14:49.840 --> 01:14:52.160]   It washes your butt and makes toast.
[01:14:52.880 --> 01:14:55.760]   I will say my toast does taste better in the June oven.
[01:14:55.760 --> 01:14:57.040]   So I could imagine a.
[01:14:57.040 --> 01:15:00.800]   Teddicated novel toaster.
[01:15:00.800 --> 01:15:01.840]   This is from.
[01:15:01.840 --> 01:15:04.480]   This is from Mitsubishi Electric.
[01:15:04.480 --> 01:15:04.960]   Look at it.
[01:15:04.960 --> 01:15:06.480]   Let's watch the video.
[01:15:06.480 --> 01:15:07.360]   Let's watch the video.
[01:15:07.360 --> 01:15:08.720]   I'm going to show you the video here.
[01:15:08.720 --> 01:15:09.600]   Yeah.
[01:15:09.600 --> 01:15:11.200]   That's not it.
[01:15:11.200 --> 01:15:13.360]   That's Texas toast.
[01:15:13.360 --> 01:15:16.800]   What they sell in Japan a lot.
[01:15:16.800 --> 01:15:18.240]   Texas toast is good.
[01:15:18.240 --> 01:15:21.440]   Okay, I'm going to skip the taste test.
[01:15:22.160 --> 01:15:22.960]   Yeah, all right.
[01:15:22.960 --> 01:15:23.520]   Shut up.
[01:15:23.520 --> 01:15:24.640]   This is what it looks like.
[01:15:24.640 --> 01:15:28.080]   It looks like a grilled cheese maker.
[01:15:28.080 --> 01:15:29.040]   Does look like grilled cheese.
[01:15:29.040 --> 01:15:30.480]   Yeah, because you put the lid down.
[01:15:30.480 --> 01:15:32.480]   This is the.
[01:15:32.480 --> 01:15:36.640]   It costs so much because it's a completely new structure.
[01:15:36.640 --> 01:15:39.520]   You put butter on the thing.
[01:15:39.520 --> 01:15:40.880]   It really is a grill.
[01:15:40.880 --> 01:15:42.400]   This is, you know what?
[01:15:42.400 --> 01:15:44.160]   If I were George Foreman, I'd be pissed.
[01:15:44.160 --> 01:15:47.920]   Is it George Foreman grill?
[01:15:47.920 --> 01:15:52.000]   It's a freaking George $270 George Foreman grill.
[01:15:52.000 --> 01:15:52.320]   Okay.
[01:15:52.320 --> 01:15:56.080]   Yeah, no, I wouldn't buy that.
[01:15:56.080 --> 01:15:58.560]   I did just buy something really cool.
[01:15:58.560 --> 01:16:00.400]   And I can't wait to tell you guys about it in a week.
[01:16:00.400 --> 01:16:01.040]   Whenever I was.
[01:16:01.040 --> 01:16:05.280]   Is it the Whirlpool trash composter?
[01:16:05.280 --> 01:16:08.960]   No, but does that automatically tell me which goes where?
[01:16:08.960 --> 01:16:12.800]   Also, I am composting now and I hit it.
[01:16:12.800 --> 01:16:14.000]   No, that's why you need this.
[01:16:14.000 --> 01:16:14.880]   To solve that problem.
[01:16:14.880 --> 01:16:15.120]   Yeah.
[01:16:15.120 --> 01:16:16.960]   We need the new zero food recycler.
[01:16:16.960 --> 01:16:18.400]   They're putting it on indigo go.
[01:16:18.400 --> 01:16:22.320]   I wanted that since it was announced at CES forever ago.
[01:16:22.320 --> 01:16:22.640]   Yeah.
[01:16:22.640 --> 01:16:24.160]   Well, wait, that it's coming up.
[01:16:24.160 --> 01:16:24.640]   No, no, go back up.
[01:16:24.640 --> 01:16:26.480]   That date line is 2017.
[01:16:26.480 --> 01:16:29.120]   Okay, but but but but but but it is now on indigo go.
[01:16:29.120 --> 01:16:29.920]   Let me find it here.
[01:16:29.920 --> 01:16:30.320]   Oh, okay.
[01:16:30.320 --> 01:16:31.360]   I was like, wait.
[01:16:31.360 --> 01:16:32.160]   Oh, you're right.
[01:16:32.160 --> 01:16:33.280]   They announced it two years ago.
[01:16:33.280 --> 01:16:33.840]   Go go.
[01:16:33.840 --> 01:16:36.160]   Well, it's the it's Whirlpools like.
[01:16:36.160 --> 01:16:38.720]   Yeah, GE does stuff on indigo go.
[01:16:38.720 --> 01:16:39.520]   What's that GE thing?
[01:16:39.520 --> 01:16:39.920]   That's right.
[01:16:39.920 --> 01:16:40.320]   That's right.
[01:16:40.320 --> 01:16:41.120]   It's W Labs.
[01:16:41.120 --> 01:16:42.560]   They call it's Whirlpools kind of.
[01:16:42.560 --> 01:16:43.920]   What are you eating?
[01:16:43.920 --> 01:16:44.720]   Stating tank.
[01:16:46.320 --> 01:16:47.840]   It's like cheesecake.
[01:16:47.840 --> 01:16:49.280]   Okay, but when you're done with it,
[01:16:49.280 --> 01:16:50.960]   okay, here's what you do.
[01:16:50.960 --> 01:16:52.160]   Flavor what flavor?
[01:16:52.160 --> 01:16:53.280]   Here's what you do.
[01:16:53.280 --> 01:16:54.080]   Cherry.
[01:16:54.080 --> 01:16:54.720]   You can buy it.
[01:16:54.720 --> 01:16:56.080]   You can buy it now, by the way.
[01:16:56.080 --> 01:16:59.440]   So let me just show you how it works.
[01:16:59.440 --> 01:17:02.000]   It's only $1,199.
[01:17:02.000 --> 01:17:05.520]   That doesn't include the pods that you have to buy.
[01:17:05.520 --> 01:17:06.720]   You end up with dirt.
[01:17:06.720 --> 01:17:07.200]   Yeah.
[01:17:07.200 --> 01:17:09.120]   So what it is, it's a little it's a little thing
[01:17:09.120 --> 01:17:12.400]   that sits next to your counter.
[01:17:12.400 --> 01:17:13.200]   Yeah, it's a big thing.
[01:17:13.200 --> 01:17:14.320]   It's bigger than your trash can.
[01:17:15.360 --> 01:17:16.720]   You put anything in there.
[01:17:16.720 --> 01:17:18.800]   Food, scraps, everything.
[01:17:18.800 --> 01:17:20.480]   Meat, potatoes, milk.
[01:17:20.480 --> 01:17:21.520]   It doesn't matter.
[01:17:21.520 --> 01:17:22.800]   Put all your scraps in there.
[01:17:22.800 --> 01:17:23.360]   Now, wait a minute.
[01:17:23.360 --> 01:17:27.520]   I got to point out that while your scraps are in there
[01:17:27.520 --> 01:17:31.040]   and rotting, it's in your kitchen.
[01:17:31.040 --> 01:17:34.720]   So they do have a special filter system
[01:17:34.720 --> 01:17:37.200]   to try to get rid of the odors, but I wonder.
[01:17:37.200 --> 01:17:37.440]   That's true.
[01:17:37.440 --> 01:17:38.160]   That's...
[01:17:38.160 --> 01:17:38.800]   Okay, but wait a minute.
[01:17:38.800 --> 01:17:41.280]   Now it's going to take 3 and 1/2 kilograms,
[01:17:41.280 --> 01:17:44.800]   7 pounds of food stuff, churn it up,
[01:17:44.800 --> 01:17:46.160]   and then when it's done, look,
[01:17:46.160 --> 01:17:50.480]   you get this beautiful mulch you can feed your microgreens with.
[01:17:50.480 --> 01:17:57.360]   Why is it not using the heat generated by this to...
[01:17:57.360 --> 01:17:58.640]   Well, because it's doing it in 20...
[01:17:58.640 --> 01:18:01.280]   It does it in 24 hours, so you not really get...
[01:18:01.280 --> 01:18:05.200]   And you have to buy a special pod that you put in.
[01:18:05.200 --> 01:18:08.000]   They show it briefly, but they don't really emphasize it.
[01:18:08.000 --> 01:18:09.520]   Oh, it has worms in it?
[01:18:09.520 --> 01:18:09.920]   Is that...
[01:18:09.920 --> 01:18:10.880]   No, I wish.
[01:18:10.880 --> 01:18:11.840]   Is that the side version under the sink?
[01:18:11.840 --> 01:18:12.640]   Worms are slow.
[01:18:12.640 --> 01:18:13.280]   How do you do it with worms?
[01:18:13.280 --> 01:18:15.520]   No, the pod has chemicals in it.
[01:18:15.520 --> 01:18:17.520]   Oh, well, that seems...
[01:18:17.520 --> 01:18:19.440]   And it speeds up the process.
[01:18:19.440 --> 01:18:23.040]   That seems like a recurring business model, but that's about it.
[01:18:23.040 --> 01:18:23.760]   This is the...
[01:18:23.760 --> 01:18:23.760]   This is the...
[01:18:23.760 --> 01:18:24.800]   ...fyingly short.
[01:18:24.800 --> 01:18:26.960]   This is the juicero of composting.
[01:18:26.960 --> 01:18:33.520]   So you need the zero filter,
[01:18:33.520 --> 01:18:37.920]   because it stinks, because it's rotting food in your kitchen,
[01:18:37.920 --> 01:18:39.120]   and then you also...
[01:18:39.120 --> 01:18:41.120]   And I'm just assuming, and maybe it doesn't.
[01:18:41.120 --> 01:18:45.760]   And then you also need for $13.99, the zero additive.
[01:18:45.760 --> 01:18:50.640]   It's made from something called Quar, C-O-I-R, and baking soda.
[01:18:50.640 --> 01:18:51.440]   Quar is...
[01:18:51.440 --> 01:18:53.200]   Oh, Quar is like hemp or roost.
[01:18:53.200 --> 01:18:53.760]   Yeah, coconut.
[01:18:53.760 --> 01:18:55.040]   It's made from coconut husk.
[01:18:55.040 --> 01:18:58.160]   The zero additive pack is made from paper design,
[01:18:58.160 --> 01:18:59.920]   so the entire additive, including the packaging,
[01:18:59.920 --> 01:19:01.040]   can be placed in the device.
[01:19:01.040 --> 01:19:02.320]   You get...
[01:19:02.320 --> 01:19:03.040]   Oh, I'm sorry.
[01:19:03.040 --> 01:19:03.360]   You know what?
[01:19:03.360 --> 01:19:03.840]   It's only...
[01:19:03.840 --> 01:19:05.120]   It's 14 bucks for four of them.
[01:19:05.120 --> 01:19:06.400]   So it's only a few bucks a week.
[01:19:06.400 --> 01:19:10.560]   However, the zero food recycler team
[01:19:10.560 --> 01:19:11.360]   has been notified.
[01:19:11.360 --> 01:19:13.280]   A number of fraudulent retail websites
[01:19:13.280 --> 01:19:14.720]   have been targeting consumers
[01:19:14.720 --> 01:19:16.720]   across social media platforms.
[01:19:16.720 --> 01:19:20.880]   Please make sure you get the genuine zero food recycler
[01:19:20.880 --> 01:19:24.160]   as part of the W What Labs Indiegogo pre-order campaign
[01:19:24.160 --> 01:19:26.640]   at wlabsinnovations.com.
[01:19:26.640 --> 01:19:30.880]   So...
[01:19:30.880 --> 01:19:32.560]   I don't know why Google decided...
[01:19:32.560 --> 01:19:33.840]   Serious decided to take a...
[01:19:33.840 --> 01:19:34.320]   Oh!
[01:19:34.320 --> 01:19:34.320]   To take...
[01:19:34.320 --> 01:19:35.120]   Make a note of that.
[01:19:35.120 --> 01:19:36.640]   False positive.
[01:19:36.640 --> 01:19:38.560]   I would say...
[01:19:38.560 --> 01:19:40.240]   What was I going to say?
[01:19:40.240 --> 01:19:40.720]   Oh!
[01:19:40.720 --> 01:19:42.000]   I have a composter.
[01:19:42.000 --> 01:19:43.520]   I won't with the OXO composter,
[01:19:43.520 --> 01:19:46.720]   but the popular compost design for those counterthings
[01:19:46.720 --> 01:19:48.960]   does include a charcoal filter for the smell,
[01:19:48.960 --> 01:19:50.240]   because they do smell gross.
[01:19:50.240 --> 01:19:51.040]   Yeah, that's what...
[01:19:51.040 --> 01:19:52.880]   I mean, I got a lot of stuff on...
[01:19:52.880 --> 01:19:55.280]   By the way, they also have a smart oven.
[01:19:55.280 --> 01:19:59.680]   Which looks a lot like a June.
[01:19:59.680 --> 01:20:02.560]   Oh, Mitsubishi?
[01:20:02.560 --> 01:20:04.960]   No, W Labs, which is Westinghouse.
[01:20:04.960 --> 01:20:05.440]   Whirlpools?
[01:20:05.440 --> 01:20:06.320]   Oh, yeah!
[01:20:06.320 --> 01:20:09.840]   They launched that at CES this last year.
[01:20:09.840 --> 01:20:10.560]   Yeah.
[01:20:10.560 --> 01:20:12.080]   Smart oven, huh?
[01:20:12.080 --> 01:20:13.920]   Wonder what their smart oven costs.
[01:20:13.920 --> 01:20:17.200]   It has an integration with Yumly.
[01:20:17.200 --> 01:20:17.680]   Seven-yumly?
[01:20:17.680 --> 01:20:18.160]   With what?
[01:20:18.160 --> 01:20:18.720]   Yumly?
[01:20:18.720 --> 01:20:19.600]   Oh, I love Yumly.
[01:20:19.600 --> 01:20:21.760]   Did they buy Yumly?
[01:20:21.760 --> 01:20:22.800]   I think they bought Yumly.
[01:20:22.800 --> 01:20:23.280]   Wow!
[01:20:23.280 --> 01:20:26.720]   Whirlpools got some money, honey.
[01:20:26.720 --> 01:20:27.280]   Oh!
[01:20:27.280 --> 01:20:28.000]   You know what?
[01:20:28.000 --> 01:20:29.680]   Do any of you have a jewel cooker?
[01:20:29.680 --> 01:20:32.400]   A jewel-souvide machine?
[01:20:32.400 --> 01:20:34.000]   No.
[01:20:34.000 --> 01:20:35.280]   No.
[01:20:35.280 --> 01:20:36.000]   Well, Breville,
[01:20:36.000 --> 01:20:38.560]   which actually does make you a smart oven.
[01:20:38.560 --> 01:20:39.120]   Breville's great.
[01:20:39.120 --> 01:20:41.920]   They purchased Chef Steps, the company behind the jewel.
[01:20:41.920 --> 01:20:44.960]   And they purchased the assets,
[01:20:44.960 --> 01:20:46.560]   which means it was a fire sale.
[01:20:46.560 --> 01:20:46.960]   Oh!
[01:20:46.960 --> 01:20:48.080]   So, oh!
[01:20:48.080 --> 01:20:48.080]   Oh!
[01:20:48.080 --> 01:20:49.840]   So, jeweled was not a...
[01:20:49.840 --> 01:20:52.880]   I feel like I have a jewel on order.
[01:20:52.880 --> 01:20:55.520]   I feel like you do.
[01:20:55.520 --> 01:20:58.720]   I feel like I ordered that a couple of years ago.
[01:20:58.720 --> 01:21:00.320]   Oh, that's not good.
[01:21:00.320 --> 01:21:03.440]   Well, I was like, I have a jewel, but...
[01:21:03.440 --> 01:21:06.400]   Oh, well, I know we're getting back.
[01:21:06.400 --> 01:21:08.160]   So, jeweled is kind of the wave of food.
[01:21:08.160 --> 01:21:09.360]   Let's get back on track.
[01:21:09.360 --> 01:21:10.000]   Oh, yeah, yeah.
[01:21:10.000 --> 01:21:10.640]   No more cooking.
[01:21:10.640 --> 01:21:13.280]   Let's talk about something really important.
[01:21:13.280 --> 01:21:14.640]   Prime Day,
[01:21:14.640 --> 01:21:18.160]   surpassed Black Friday and Cyber Monday,
[01:21:18.160 --> 01:21:22.640]   combined the biggest shopping event in history.
[01:21:22.640 --> 01:21:25.360]   How many workers walked out?
[01:21:25.360 --> 01:21:27.520]   And did it impact this?
[01:21:27.520 --> 01:21:30.800]   Because that was going to be the big boycott, right?
[01:21:30.800 --> 01:21:33.280]   I intentionally did not buy anything on Amazon
[01:21:33.280 --> 01:21:37.200]   to show support for the walkout.
[01:21:37.200 --> 01:21:39.200]   I didn't buy anything on Amazon,
[01:21:39.200 --> 01:21:40.480]   because I just didn't need anything.
[01:21:40.480 --> 01:21:43.040]   So, the big sellers in the US...
[01:21:43.040 --> 01:21:44.480]   The big seller page didn't find much.
[01:21:44.480 --> 01:21:46.640]   Yeah, it's weird.
[01:21:46.640 --> 01:21:47.760]   Look at the big sellers.
[01:21:47.760 --> 01:21:53.200]   It was the life straw personal water filter.
[01:21:53.200 --> 01:21:54.880]   That was everywhere.
[01:21:54.880 --> 01:21:55.680]   That was like...
[01:21:55.680 --> 01:21:57.360]   They pushed this like crazy.
[01:21:57.360 --> 01:22:01.200]   This is the thing I really yelled at Chad Johnson for using,
[01:22:01.200 --> 01:22:02.720]   right, on the gizwiz.
[01:22:02.720 --> 01:22:06.160]   They brought in some scummy swan water.
[01:22:07.040 --> 01:22:08.400]   And he drank it.
[01:22:08.400 --> 01:22:09.440]   And I said,
[01:22:09.440 --> 01:22:12.320]   "If you died, I'd be responsible.
[01:22:12.320 --> 01:22:13.440]   Please don't do that."
[01:22:13.440 --> 01:22:15.680]   What do you think you are?
[01:22:15.680 --> 01:22:16.800]   A YouTube star?
[01:22:16.800 --> 01:22:17.920]   Did he die?
[01:22:17.920 --> 01:22:18.960]   No, he didn't.
[01:22:18.960 --> 01:22:19.360]   See?
[01:22:19.360 --> 01:22:20.240]   Did he get sick?
[01:22:20.240 --> 01:22:22.640]   Go eat Tide Pods on your time, Chad Johnson.
[01:22:22.640 --> 01:22:24.880]   No, but I just was worried.
[01:22:24.880 --> 01:22:25.760]   Like, we're liable
[01:22:25.760 --> 01:22:28.720]   if an employee decides to drink swamp water
[01:22:28.720 --> 01:22:30.240]   through the life straw and he gets sick.
[01:22:30.240 --> 01:22:31.520]   But he didn't.
[01:22:31.520 --> 01:22:35.760]   It makes disgusting water drinkable.
[01:22:36.640 --> 01:22:37.360]   Notable.
[01:22:37.360 --> 01:22:38.480]   But maybe not-
[01:22:38.480 --> 01:22:40.400]   There's a bit of a privilege thing here.
[01:22:40.400 --> 01:22:42.240]   Why don't we buy those to send to countries
[01:22:42.240 --> 01:22:44.640]   where they can't get drinkable water?
[01:22:44.640 --> 01:22:46.080]   Well, it was used in Haiti
[01:22:46.080 --> 01:22:49.360]   after the earthquake, Rwanda, 2015.
[01:22:49.360 --> 01:22:53.200]   And apparently can be commonly found in the rucksacks
[01:22:53.200 --> 01:22:55.280]   of many a member of the US military.
[01:22:55.280 --> 01:22:58.560]   It's the kind of thing I'll tell you why it sold well.
[01:22:58.560 --> 01:22:59.600]   It was $10.
[01:22:59.600 --> 01:23:01.360]   And this kind of thing, he thought,
[01:23:01.360 --> 01:23:03.760]   "Well, I'm not going to drink any bad water,
[01:23:03.760 --> 01:23:06.480]   but if I ever had to, it'd be really good that I had the straw."
[01:23:06.480 --> 01:23:07.760]   That's good to have in your go bag.
[01:23:07.760 --> 01:23:08.720]   You put it in your go bag.
[01:23:08.720 --> 01:23:10.640]   I guarantee you that's what-
[01:23:10.640 --> 01:23:12.480]   I have a little light, a sterile light,
[01:23:12.480 --> 01:23:15.680]   that's for sterilizing water so you can drink it.
[01:23:15.680 --> 01:23:17.360]   I've never used it, but I got it.
[01:23:17.360 --> 01:23:21.440]   Number two, the Instapot.
[01:23:21.440 --> 01:23:22.400]   It will never die.
[01:23:22.400 --> 01:23:25.440]   Man, who doesn't have an Instapot?
[01:23:25.440 --> 01:23:27.360]   This is what's so confusing to me.
[01:23:27.360 --> 01:23:29.600]   Three hundred-
[01:23:29.600 --> 01:23:33.680]   I still have not bought one because no one has convinced me why I need it.
[01:23:33.680 --> 01:23:34.720]   You don't need an Instapot.
[01:23:34.720 --> 01:23:35.760]   You don't need it.
[01:23:35.760 --> 01:23:37.360]   I don't love mine, but-
[01:23:37.360 --> 01:23:39.360]   Three hundred thousand Instapots.
[01:23:39.360 --> 01:23:42.320]   Oh, actually, wait a minute.
[01:23:42.320 --> 01:23:42.800]   I take it back.
[01:23:42.800 --> 01:23:43.760]   They said that was last year.
[01:23:43.760 --> 01:23:45.200]   They probably sold more this year.
[01:23:45.200 --> 01:23:48.880]   And then weirdly, I guess because they were 50% off,
[01:23:48.880 --> 01:23:51.040]   the number three seller, 23 and me,
[01:23:51.040 --> 01:23:53.120]   and Ancestry DNA DNA kits.
[01:23:53.120 --> 01:23:55.600]   Yep.
[01:23:55.600 --> 01:23:58.720]   So I found-
[01:23:58.720 --> 01:24:00.000]   Did I tell you my story about this?
[01:24:01.280 --> 01:24:02.640]   This is-
[01:24:02.640 --> 01:24:07.520]   So my grandfather was illegitimate.
[01:24:07.520 --> 01:24:12.080]   And in West Virginia, back in the Haller,
[01:24:12.080 --> 01:24:15.040]   he was raised by his grandmother as if his grandmother
[01:24:15.040 --> 01:24:16.480]   were in his mother to the town.
[01:24:16.480 --> 01:24:21.040]   And his mother was, by all appearances, his sister.
[01:24:21.040 --> 01:24:22.800]   But now-
[01:24:22.800 --> 01:24:26.320]   No, you took the black and he's fighting on the wall.
[01:24:26.320 --> 01:24:27.920]   So-
[01:24:27.920 --> 01:24:28.240]   No.
[01:24:28.240 --> 01:24:29.440]   I don't know who-
[01:24:29.440 --> 01:24:30.960]   My name probably isn't Jarvis.
[01:24:30.960 --> 01:24:32.000]   What?
[01:24:32.000 --> 01:24:34.000]   Given genealogy.
[01:24:34.000 --> 01:24:34.800]   Right? So we've been-
[01:24:34.800 --> 01:24:35.600]   The kids-
[01:24:35.600 --> 01:24:36.400]   We were visiting by father.
[01:24:36.400 --> 01:24:38.080]   We went crazy on genealogy from-
[01:24:38.080 --> 01:24:38.720]   Because I found-
[01:24:38.720 --> 01:24:40.800]   Because 23 and me came to me and said,
[01:24:40.800 --> 01:24:44.320]   "You have a second cousin who shares grandparents-
[01:24:44.320 --> 01:24:49.040]   Who's from the town where the guy-
[01:24:49.040 --> 01:24:52.240]   Knocked up my great grandmother-
[01:24:52.240 --> 01:24:54.560]   Was from Buffalo, New York.
[01:24:54.560 --> 01:24:55.680]   What was his last name?
[01:24:55.680 --> 01:24:58.320]   My mom's from Buffalo.
[01:24:58.320 --> 01:24:59.520]   I forget right now.
[01:24:59.520 --> 01:25:01.280]   Cousin, my long-lost cousin-
[01:25:01.280 --> 01:25:03.600]   His last name was Higginbotham.
[01:25:03.600 --> 01:25:04.320]   Related to me.
[01:25:04.320 --> 01:25:08.080]   That was about the best punchline ever.
[01:25:08.080 --> 01:25:11.600]   And his last name was Higginbotham.
[01:25:11.600 --> 01:25:15.520]   That's not my mom's last name though.
[01:25:15.520 --> 01:25:16.320]   That was your mom's-
[01:25:16.320 --> 01:25:17.040]   That's a good question.
[01:25:17.040 --> 01:25:18.320]   By violating your security-
[01:25:18.320 --> 01:25:19.520]   Don't say your mother's name.
[01:25:19.520 --> 01:25:20.080]   My security.
[01:25:20.080 --> 01:25:22.160]   Because yes, they still-
[01:25:22.160 --> 01:25:24.000]   They still ask that question.
[01:25:24.000 --> 01:25:25.440]   I always yell at them when they do.
[01:25:25.440 --> 01:25:28.400]   And that man's name was Higginbotham.
[01:25:28.400 --> 01:25:29.840]   Let's-
[01:25:29.840 --> 01:25:33.280]   I want to just take a real quick break with our sponsor
[01:25:33.280 --> 01:25:34.640]   and then we will get back to more.
[01:25:34.640 --> 01:25:37.360]   We'll do some fun stuff.
[01:25:37.360 --> 01:25:38.640]   We'll do some fun stuff.
[01:25:38.640 --> 01:25:39.600]   We could talk about Google.
[01:25:39.600 --> 01:25:41.840]   I think we have.
[01:25:41.840 --> 01:25:42.640]   We could talk about-
[01:25:42.640 --> 01:25:43.680]   Did we talk about-
[01:25:43.680 --> 01:25:45.280]   The people who are leaving Google.
[01:25:45.280 --> 01:25:46.080]   All the people who-
[01:25:46.080 --> 01:25:48.720]   Oh, I just saw that story.
[01:25:48.720 --> 01:25:49.280]   Yeah.
[01:25:53.600 --> 01:25:57.840]   Meanwhile, our show today brought to you by worldwide technology.
[01:25:57.840 --> 01:26:00.720]   WWT, I think a lot of you,
[01:26:00.720 --> 01:26:03.840]   especially if you're an enterprise know about WWT.
[01:26:03.840 --> 01:26:06.000]   But do you know about their Advanced Technology Center?
[01:26:06.000 --> 01:26:07.360]   This thing is amazing.
[01:26:07.360 --> 01:26:09.280]   They started building about 10 years ago.
[01:26:09.280 --> 01:26:11.760]   Because they realized when you're selling enterprise technologies,
[01:26:11.760 --> 01:26:14.240]   it's never just in a vacuum.
[01:26:14.240 --> 01:26:17.520]   It has to integrate with a variety of other technologies.
[01:26:17.520 --> 01:26:18.560]   You need to test it.
[01:26:18.560 --> 01:26:22.400]   They really wanted to bring down the time from-
[01:26:22.400 --> 01:26:25.200]   figuring out what you want to buy to getting in place.
[01:26:25.200 --> 01:26:26.640]   The ATC does that.
[01:26:26.640 --> 01:26:30.720]   Now half a billion dollars of equipment in this amazing lab.
[01:26:30.720 --> 01:26:32.960]   From hundreds of OEMs.
[01:26:32.960 --> 01:26:37.440]   I mean, all the Biggies, Cisco, NetApp, VMware.
[01:26:37.440 --> 01:26:38.640]   But not just the Biggies.
[01:26:38.640 --> 01:26:39.840]   They've got disruptors to like-
[01:26:39.840 --> 01:26:41.680]   Tanium and Equinix and Expanse.
[01:26:41.680 --> 01:26:44.480]   WWT is a trusted partner.
[01:26:44.480 --> 01:26:49.520]   When it comes time for you to expand your portfolio,
[01:26:49.520 --> 01:26:52.640]   to add to your security, to move to the cloud,
[01:26:52.640 --> 01:26:55.840]   WWT is there for you.
[01:26:55.840 --> 01:26:57.920]   And we'll be there with you over the years.
[01:26:57.920 --> 01:27:01.600]   Many of their customers are there for years for over a decade.
[01:27:01.600 --> 01:27:04.080]   They know WWT is where they can go to.
[01:27:04.080 --> 01:27:07.440]   They get the answers they need to make sure their business runs.
[01:27:07.440 --> 01:27:08.160]   Right.
[01:27:08.160 --> 01:27:09.440]   So much information.
[01:27:09.440 --> 01:27:13.120]   So many tools.
[01:27:13.120 --> 01:27:14.160]   Such smart people.
[01:27:14.160 --> 01:27:18.400]   In fact, the WWT engineers work in these labs every day themselves.
[01:27:18.400 --> 01:27:21.600]   Beta testing, new equipment building, reference architectures.
[01:27:21.600 --> 01:27:24.800]   Helping clients in business, in government,
[01:27:24.800 --> 01:27:28.640]   set up custom integrations so that you can make decisions.
[01:27:28.640 --> 01:27:32.720]   You can see the results and frankly, with less investment on your part,
[01:27:32.720 --> 01:27:34.160]   they've already made the investment.
[01:27:34.160 --> 01:27:37.680]   What's cool about the Advanced Technology Center,
[01:27:37.680 --> 01:27:40.640]   it's an incubator for IT innovation.
[01:27:40.640 --> 01:27:41.920]   And you can use it.
[01:27:41.920 --> 01:27:43.280]   You can use it.
[01:27:43.280 --> 01:27:44.880]   Hundreds of on demand.
[01:27:44.880 --> 01:27:46.960]   Integrated Solutions Labs.
[01:27:46.960 --> 01:27:48.800]   Representing the newest advances and things.
[01:27:48.800 --> 01:27:52.080]   Everything from flash storage to software to find networking,
[01:27:52.080 --> 01:27:55.840]   network automation, end point security architecture.
[01:27:55.840 --> 01:27:59.200]   Stuff that you're looking at right now that you're trying to solve.
[01:27:59.200 --> 01:28:01.600]   And you shouldn't try to do it on your own.
[01:28:01.600 --> 01:28:05.280]   WWT and the Advanced Technology Center is there to help you.
[01:28:05.280 --> 01:28:07.680]   You can learn about products before you launch them.
[01:28:07.680 --> 01:28:11.040]   You can do exactly what the engineers do.
[01:28:11.040 --> 01:28:15.760]   Spin up proofs of concepts and pilots using their built-in sandbox.
[01:28:16.720 --> 01:28:20.720]   So you can select the right solution for you with confidence.
[01:28:20.720 --> 01:28:22.240]   And they're there.
[01:28:22.240 --> 01:28:23.600]   They're engineers are there to help you.
[01:28:23.600 --> 01:28:26.800]   Reduce concept time from months to weeks.
[01:28:26.800 --> 01:28:29.040]   That increases the speed to market.
[01:28:29.040 --> 01:28:31.520]   They call it lab as a service.
[01:28:31.520 --> 01:28:32.800]   I love this idea.
[01:28:32.800 --> 01:28:36.160]   It is within the ATC, a dedicated lab space you can use
[01:28:36.160 --> 01:28:39.520]   to perform programmatic testing using any of these,
[01:28:39.520 --> 01:28:41.920]   this half a billion dollars worth of gear,
[01:28:41.920 --> 01:28:43.920]   this vast technology ecosystem.
[01:28:43.920 --> 01:28:45.520]   And it's virtual.
[01:28:45.520 --> 01:28:46.400]   So you can use it.
[01:28:46.400 --> 01:28:47.440]   You don't have to go to St. Louis.
[01:28:47.440 --> 01:28:50.960]   You can use it anywhere you are 24/7.
[01:28:50.960 --> 01:28:53.040]   Later this summer,
[01:28:53.040 --> 01:28:55.360]   WWT is going to launch a new digital platform,
[01:28:55.360 --> 01:28:58.880]   which will give you hands-on access to more than 200 lab environments.
[01:28:58.880 --> 01:29:00.000]   This is going to be even better.
[01:29:00.000 --> 01:29:02.080]   And if you want early access to that,
[01:29:02.080 --> 01:29:04.880]   you can go right now to www.wbt.com/twit
[01:29:04.880 --> 01:29:06.480]   and sign up for pre-launch access.
[01:29:06.480 --> 01:29:11.120]   WWT, World Wide Technology, simplifies the complex.
[01:29:11.120 --> 01:29:14.320]   So remember that www.wbt.com/twit.
[01:29:14.320 --> 01:29:17.040]   I know people as I've been talking about this over the last few weeks
[01:29:17.040 --> 01:29:18.400]   are very excited about this.
[01:29:18.400 --> 01:29:20.800]   This is what we need.
[01:29:20.800 --> 01:29:23.440]   This used to be something that,
[01:29:23.440 --> 01:29:26.560]   you know, back in the days of PC Magazine and the Great Tech Magazines,
[01:29:26.560 --> 01:29:28.960]   this is, I remember they had a great lab in Foster City,
[01:29:28.960 --> 01:29:30.560]   this kind of thing they used to do.
[01:29:30.560 --> 01:29:32.080]   No one can afford to do this anymore.
[01:29:32.080 --> 01:29:35.200]   WWT can half a billion dollars worth of gear.
[01:29:35.200 --> 01:29:39.120]   And a lot of that's thanks to their great OEM partners like NetApp.
[01:29:39.120 --> 01:29:43.040]   WWT, delivering business and technology outcomes around the world.
[01:29:43.040 --> 01:29:44.720]   WWT.com/twit.
[01:29:44.720 --> 01:29:46.800]   Thank you, WWT, for your support.
[01:29:46.800 --> 01:29:49.520]   We're already talking about going out and visiting that lab next year.
[01:29:49.520 --> 01:29:50.800]   I want to see it.
[01:29:50.800 --> 01:29:54.560]   WWT.com/twit.
[01:29:54.560 --> 01:30:03.360]   So, what was it that we were going to talk about?
[01:30:03.360 --> 01:30:03.840]   I forgot.
[01:30:05.360 --> 01:30:08.400]   Google, the workers, mostly women, right?
[01:30:08.400 --> 01:30:13.280]   Or are they all women who started the walkout?
[01:30:13.280 --> 01:30:15.120]   Most of them have now left.
[01:30:15.120 --> 01:30:17.520]   Most is a bit strong, four out of seven.
[01:30:17.520 --> 01:30:18.640]   Well, that's more than half.
[01:30:18.640 --> 01:30:19.200]   That is the truth.
[01:30:19.200 --> 01:30:19.680]   It's the truth.
[01:30:19.680 --> 01:30:19.680]   It's the truth.
[01:30:19.680 --> 01:30:20.480]   It's more than half.
[01:30:20.480 --> 01:30:21.600]   It's technically accurate.
[01:30:21.600 --> 01:30:25.520]   The most notable, of course, is Meredith Whitaker.
[01:30:25.520 --> 01:30:29.280]   She was, you know, not, she was not a run-of-the-mill employee.
[01:30:29.280 --> 01:30:30.880]   She ran Google's Open Research.
[01:30:30.880 --> 01:30:34.880]   Eric Anderson, who was a great, great person.
[01:30:34.880 --> 01:30:37.280]   At Google News Lab left.
[01:30:37.280 --> 01:30:40.960]   Whitaker, you may remember a couple of months ago, claimed that Google had retaliated her.
[01:30:40.960 --> 01:30:47.040]   She was working on AI ethics through AI Now, a research institute she co-founded that has
[01:30:47.040 --> 01:30:48.800]   received funding from Google.
[01:30:48.800 --> 01:30:53.440]   She'd already been told that her AI ethics work was no longer a fit for the cloud division.
[01:30:53.440 --> 01:30:57.520]   She wrote in a farewell note posted last week on Medium.
[01:30:57.520 --> 01:31:00.400]   It's clear Google isn't a place where I can continue this work.
[01:31:00.400 --> 01:31:02.960]   She left, but she did on her way out.
[01:31:02.960 --> 01:31:08.320]   Herge employees to unionize, protect whistleblowers, and insist on transparency around the technology
[01:31:08.320 --> 01:31:09.840]   they're building and how it will be used.
[01:31:09.840 --> 01:31:14.640]   You know, there's so many companies where the workers don't have that power, but in a company
[01:31:14.640 --> 01:31:22.160]   like Google, where your line workers, our PhDs, are very valuable engineers and developers.
[01:31:22.160 --> 01:31:23.760]   Not easy to replace.
[01:31:23.760 --> 01:31:28.000]   Rightly so, I think they have a voice in the work that they're doing.
[01:31:28.000 --> 01:31:29.680]   She had been at Google 13 years.
[01:31:32.320 --> 01:31:33.840]   So, I think that that's significant.
[01:31:33.840 --> 01:31:37.760]   I don't know, I have no inside knowledge.
[01:31:37.760 --> 01:31:38.240]   I don't know.
[01:31:38.240 --> 01:31:46.080]   You know, Google says she was told she could continue to working on AI ethics in her personal
[01:31:46.080 --> 01:31:48.560]   time, but her official role of the company was open source work.
[01:31:48.560 --> 01:31:53.680]   So, did you ever read the story about how Google doesn't fire anyone?
[01:31:53.680 --> 01:31:56.720]   They just like shuns them off to irrelevancy?
[01:31:56.720 --> 01:31:58.960]   I saw it on Silicon Valley, the TV show.
[01:31:59.680 --> 01:32:01.680]   Well, there is that, but there is an article.
[01:32:01.680 --> 01:32:05.440]   Google would take a guy who they didn't want anymore, but couldn't fire.
[01:32:05.440 --> 01:32:08.640]   And they just sent him to the roof where he could just sit around all day.
[01:32:08.640 --> 01:32:12.400]   And he got up there and there were a bunch of people in the roof said,
[01:32:12.400 --> 01:32:16.240]   "Yeah, we'd collect our paychecks and we enjoy the free food."
[01:32:16.240 --> 01:32:20.240]   There was a story about, yes, that is that.
[01:32:20.240 --> 01:32:21.680]   Apparently they do it in Japan.
[01:32:21.680 --> 01:32:22.720]   It's common in Japan.
[01:32:22.720 --> 01:32:28.720]   It was about people with stock options waiting for the divest, I think.
[01:32:29.600 --> 01:32:30.160]   Yeah.
[01:32:30.160 --> 01:32:32.080]   Sure, because you're not going to leave.
[01:32:32.080 --> 01:32:33.120]   Either way, not just look.
[01:32:33.120 --> 01:32:36.560]   You've got three more years to vest that valuable stock options.
[01:32:36.560 --> 01:32:37.520]   You're never going to quit.
[01:32:37.520 --> 01:32:39.280]   I'll go make compost for you, yeah.
[01:32:39.280 --> 01:32:43.360]   In California, at least, and I bet you this is the case in general,
[01:32:43.360 --> 01:32:45.760]   but in California, at least, it's hard to fire an employee.
[01:32:45.760 --> 01:32:49.280]   Oh, in Texas, it's very easy.
[01:32:49.280 --> 01:32:52.400]   Actually, wait a minute, California is in that wheel state,
[01:32:52.400 --> 01:32:53.520]   so maybe it isn't so hard.
[01:32:53.520 --> 01:32:54.000]   I don't know.
[01:32:54.000 --> 01:32:55.520]   That's what I thought.
[01:32:55.520 --> 01:32:56.480]   I find it difficult.
[01:32:58.320 --> 01:33:01.200]   Maybe Larry just doesn't have the...
[01:33:01.200 --> 01:33:02.880]   I just can't fire him.
[01:33:02.880 --> 01:33:04.800]   Yes, people who do that for him.
[01:33:04.800 --> 01:33:07.120]   Here's Google's statement to Wired.
[01:33:07.120 --> 01:33:12.080]   Meredith Whitaker has decided to resign in Google's open source program management team.
[01:33:12.080 --> 01:33:16.160]   Since 2016, Meredith spent most of her time on work for NYU's,
[01:33:16.160 --> 01:33:19.200]   by the way, NYU's in AI now institute.
[01:33:19.200 --> 01:33:22.320]   And we'll leave the company to focus on this work.
[01:33:22.320 --> 01:33:25.600]   Google will continue to work with policymakers, academics,
[01:33:25.600 --> 01:33:27.680]   the tech community, other leaders from across industries
[01:33:27.680 --> 01:33:29.520]   as we tackle these important issues,
[01:33:29.520 --> 01:33:33.600]   in addition to providing transparency into how we're putting our AI principles into action.
[01:33:33.600 --> 01:33:38.480]   It kind of misses the point, I think, but her last day at Google was Monday.
[01:33:38.480 --> 01:33:43.920]   Celio Neil Hart, the former global head of trust and transparency marketing at YouTube ads,
[01:33:43.920 --> 01:33:46.080]   left to go to Pinterest.
[01:33:46.080 --> 01:33:52.480]   Claire Stapleton, a former marketing manager YouTube left in June.
[01:33:54.240 --> 01:34:00.400]   So, yeah, honestly, if they're not happy, they should go back where they came from.
[01:34:00.400 --> 01:34:00.960]   Oh, wait a minute.
[01:34:00.960 --> 01:34:01.360]   I'm sorry.
[01:34:01.360 --> 01:34:06.080]   Where is the boxing glove when you need it?
[01:34:06.080 --> 01:34:06.720]   How's Stacy?
[01:34:06.720 --> 01:34:08.160]   Wow, that was just...
[01:34:08.160 --> 01:34:13.360]   No, but truthfully, if you're not happy, the company, you should leave.
[01:34:13.360 --> 01:34:19.040]   But what if you had been happy until you spoke out and the company made you so miserable
[01:34:19.040 --> 01:34:20.400]   that now you had no other option?
[01:34:20.400 --> 01:34:20.960]   It's different.
[01:34:20.960 --> 01:34:21.760]   It's different, yeah.
[01:34:23.680 --> 01:34:26.640]   Which might be happening here.
[01:34:26.640 --> 01:34:32.000]   Google says it did investigate the retaliation claims from Stapleton and Whitaker found no
[01:34:32.000 --> 01:34:33.280]   evidence of retaliation.
[01:34:33.280 --> 01:34:40.000]   Erica Anderson, former head of news ecosystem for Google News Lab left in January to join Vox Media.
[01:34:40.000 --> 01:34:44.480]   Only three organizers employed at Google just eight months after the protest.
[01:34:44.480 --> 01:34:49.680]   The remaining organizers decided to decline to comment or did not respond.
[01:34:52.000 --> 01:34:53.600]   So, what was it?
[01:34:53.600 --> 01:34:55.280]   Seven, three or only three are left.
[01:34:55.280 --> 01:34:57.280]   That's more than half.
[01:34:57.280 --> 01:35:00.800]   I don't know.
[01:35:00.800 --> 01:35:03.680]   You know, this feels like an employment dispute.
[01:35:03.680 --> 01:35:05.520]   I don't know if I can weigh in on this.
[01:35:05.520 --> 01:35:07.040]   I'll let you do that.
[01:35:07.040 --> 01:35:09.920]   Because why?
[01:35:09.920 --> 01:35:11.760]   Because I don't have anything to say.
[01:35:11.760 --> 01:35:13.040]   Oh, okay.
[01:35:13.040 --> 01:35:14.240]   I was like, I don't understand.
[01:35:14.240 --> 01:35:18.480]   Well, I just think I'm not surprised by this.
[01:35:18.480 --> 01:35:21.680]   I think it's important to note it and be like, hey, that's not awesome.
[01:35:22.320 --> 01:35:28.160]   Um, and I wanted to bring up something that I'm sure y'all talked about.
[01:35:28.160 --> 01:35:29.440]   Did y'all talk about it last week?
[01:35:29.440 --> 01:35:29.760]   Yeah.
[01:35:29.760 --> 01:35:32.240]   The Google works listening on the okay Google.
[01:35:32.240 --> 01:35:33.760]   No, let's talk about that.
[01:35:33.760 --> 01:35:35.520]   Google left and said, well, of course they're listening.
[01:35:35.520 --> 01:35:37.280]   Well, how do you think we do quality assurance?
[01:35:37.280 --> 01:35:39.280]   That's me saying that.
[01:35:39.280 --> 01:35:40.000]   Oh, but yeah.
[01:35:40.000 --> 01:35:41.040]   But I mean, honestly.
[01:35:41.040 --> 01:35:47.840]   So what actually in we talked about this on our show, Kevin and I today,
[01:35:47.840 --> 01:35:53.120]   but what actually struck me about this was, and I knew that this had happened, one,
[01:35:53.120 --> 01:35:58.880]   is I thought Google actually had a pro a procedure where they anonymize or modulate
[01:35:58.880 --> 01:36:00.720]   the voices so you couldn't tell who was speaking.
[01:36:00.720 --> 01:36:01.520]   Oh, that's a good idea.
[01:36:01.520 --> 01:36:03.440]   But apparently they don't have that.
[01:36:03.440 --> 01:36:07.840]   I don't know why I thought that I read it somewhere that was wrong, apparently.
[01:36:07.840 --> 01:36:11.440]   But the other thing is the false positive rate here is pretty high.
[01:36:11.440 --> 01:36:17.760]   So this company of VRT, VNT, V something T, they said they had over a thousand
[01:36:17.760 --> 01:36:25.920]   utterances, recordings of utterances, and there was 153 false positives where people didn't say
[01:36:25.920 --> 01:36:28.560]   Google or okay or hey or anything like that.
[01:36:28.560 --> 01:36:29.360]   Right.
[01:36:29.360 --> 01:36:33.920]   And as a result, they got there were private conversations and all sorts of stuff.
[01:36:33.920 --> 01:36:40.960]   VRT is a Flemish Belgian news organization that was given the information, leaked the
[01:36:40.960 --> 01:36:45.360]   information from one of their language reviewers of some Dutch audio data.
[01:36:45.360 --> 01:36:46.880]   And yeah, that's interesting, isn't it?
[01:36:46.880 --> 01:36:48.160]   What was it? What percentage was it?
[01:36:48.160 --> 01:36:51.520]   It turns out, well, okay, we don't know the base number.
[01:36:51.520 --> 01:36:59.360]   It's more than a thousand recordings, but of those more than a thousand, 153 of them were
[01:36:59.360 --> 01:37:01.440]   something like 15, at least 15%.
[01:37:01.440 --> 01:37:02.960]   Yeah, at least 15.
[01:37:02.960 --> 01:37:05.440]   Well, at most it would be 15%.
[01:37:05.440 --> 01:37:06.400]   Okay.
[01:37:06.400 --> 01:37:07.360]   Right. At most, 15%.
[01:37:07.360 --> 01:37:08.080]   Right.
[01:37:08.080 --> 01:37:12.160]   But yeah, but it's still, I mean, that's a crazy high amount.
[01:37:12.160 --> 01:37:16.160]   That's not certainty at a place where I would feel comfortable
[01:37:17.040 --> 01:37:22.480]   I mean, I'm, I have a Google in my office, but I've always unplugged it when I'm like
[01:37:22.480 --> 01:37:24.560]   talking about sensitive things with people.
[01:37:24.560 --> 01:37:30.400]   But there's a couple of things I want you actually go over and unplug your your
[01:37:30.400 --> 01:37:32.880]   assistance before you have serious conversations.
[01:37:32.880 --> 01:37:38.880]   Well, before I have conversations about like with a source or something, if someone's talking.
[01:37:38.880 --> 01:37:39.920]   Yeah, as a journalist.
[01:37:39.920 --> 01:37:40.240]   Yeah.
[01:37:40.240 --> 01:37:40.640]   Okay.
[01:37:40.640 --> 01:37:41.760]   Oh, absolutely.
[01:37:41.760 --> 01:37:41.920]   Yeah.
[01:37:41.920 --> 01:37:42.400]   Yeah.
[01:37:42.400 --> 01:37:48.480]   And I don't have like, I don't have any like of my smart speakers on, although I guess if you were
[01:37:48.480 --> 01:37:52.640]   leaking it on my phone that I'm talking to you on, you could hear it.
[01:37:52.640 --> 01:37:56.080]   Yeah, you have a microphone in your pocket all the time.
[01:37:56.080 --> 01:38:01.280]   Here's Google's one of a part of Google's blog post language experts only review around
[01:38:01.280 --> 01:38:05.440]   0.2% of all audio snippets.
[01:38:05.440 --> 01:38:09.840]   Actually, that's a fairly, that's a surprisingly large number, honestly.
[01:38:11.120 --> 01:38:14.480]   That means that's what is 20 out of every thousand.
[01:38:14.480 --> 01:38:16.880]   Snippets are reviewed.
[01:38:16.880 --> 01:38:21.280]   Audio snippets are not associated with user accounts as part of the review process.
[01:38:21.280 --> 01:38:25.840]   And and this is to your point, Stacy reviewers are directed not to transcribe
[01:38:25.840 --> 01:38:28.400]   background conversations or other noises.
[01:38:28.400 --> 01:38:32.240]   And only to transcribe snippets that are directed to Google.
[01:38:32.240 --> 01:38:33.840]   If they don't hear, okay, goog.
[01:38:33.840 --> 01:38:35.440]   They're not supposed to pay.
[01:38:35.440 --> 01:38:38.080]   They're supposed to say, oh, I can't listen to that move on Stacy.
[01:38:38.080 --> 01:38:39.280]   Hey, no mind.
[01:38:39.280 --> 01:38:39.600]   Yes.
[01:38:40.320 --> 01:38:44.640]   It occurs to me that yes, I get false positives to right.
[01:38:44.640 --> 01:38:45.440]   I'll be talking.
[01:38:45.440 --> 01:38:46.400]   We all do, right?
[01:38:46.400 --> 01:38:53.280]   But in every case, which is not one of them, it comes to life.
[01:38:53.280 --> 01:38:55.200]   It comes to life.
[01:38:55.200 --> 01:38:57.360]   And so you know when you've triggered a false positive.
[01:38:57.360 --> 01:39:02.320]   No, because you can turn off, you shouldn't, but you can turn off the blue.
[01:39:02.320 --> 01:39:06.640]   And in fact, I've done that on my Echo show because it's annoying.
[01:39:06.640 --> 01:39:11.120]   And so the only way I know is there's a little blue light on my show because it's got a screen.
[01:39:11.120 --> 01:39:14.240]   If it weren't as if they didn't have a screen, then maybe there'd be a ring.
[01:39:14.240 --> 01:39:17.520]   But if you weren't looking at it, there's no audio.
[01:39:17.520 --> 01:39:18.000]   I see.
[01:39:18.000 --> 01:39:18.240]   Okay.
[01:39:18.240 --> 01:39:18.800]   All right.
[01:39:18.800 --> 01:39:20.560]   I've done this with now.
[01:39:20.560 --> 01:39:22.960]   Now I'm worrying that I shouldn't have.
[01:39:22.960 --> 01:39:25.680]   I've done this with the turn those back on.
[01:39:25.680 --> 01:39:27.280]   Turn those back on Echo in my bedroom.
[01:39:27.280 --> 01:39:27.760]   Yeah.
[01:39:27.760 --> 01:39:28.480]   Because I don't like that.
[01:39:28.480 --> 01:39:29.840]   It's annoying.
[01:39:29.840 --> 01:39:31.280]   In fact, they probably make it annoying.
[01:39:31.280 --> 01:39:34.800]   Oh, you see, there was for me is I only do I have the screen.
[01:39:34.800 --> 01:39:37.040]   I don't do any screenless assistant.
[01:39:37.040 --> 01:39:41.440]   But this, even during the show, I read your system.
[01:39:41.440 --> 01:39:42.160]   I use as a screen.
[01:39:42.160 --> 01:39:44.240]   I should just point out just minutes ago,
[01:39:44.240 --> 01:39:50.480]   Siri had a full page of transcribed conversation because it had accidentally been triggered.
[01:39:50.480 --> 01:39:53.040]   I didn't say a Siri, but it had actually been triggered.
[01:39:53.040 --> 01:39:56.400]   And this whole screen was full.
[01:39:56.400 --> 01:39:58.000]   Now it did make that thing at the end.
[01:39:58.000 --> 01:40:03.360]   That's how I knew that little bit at the end, but still, I think they're listening.
[01:40:03.360 --> 01:40:04.400]   We should, you know what?
[01:40:04.400 --> 01:40:06.880]   Remember in the old days, mom and dad would say,
[01:40:06.880 --> 01:40:09.920]   little pictures have big ears.
[01:40:09.920 --> 01:40:12.320]   Little what?
[01:40:12.320 --> 01:40:13.600]   I think that's talking about kids.
[01:40:13.600 --> 01:40:14.080]   Yes.
[01:40:14.080 --> 01:40:16.240]   No, no, that's not talking about voice assistants.
[01:40:16.240 --> 01:40:17.680]   This was 50 years ago.
[01:40:17.680 --> 01:40:18.560]   I'm just saying.
[01:40:18.560 --> 01:40:25.120]   Now we have to say, little voice assistants have big ears.
[01:40:25.120 --> 01:40:28.080]   Yes.
[01:40:28.080 --> 01:40:29.520]   I know.
[01:40:29.520 --> 01:40:33.200]   Google Assistant only sends audio to Google after your device detects you're interacting
[01:40:33.200 --> 01:40:38.960]   with the assistant or thinks you're interacting by saying, hey, Google are physically triggering
[01:40:38.960 --> 01:40:43.920]   the Google Assistant a clear indicator such as the flashing dots on top of a Google home or an
[01:40:43.920 --> 01:40:48.480]   on screen indicator on your Android device will activate anytime the Google device is communicating
[01:40:48.480 --> 01:40:50.320]   with Google in order to fulfill your request.
[01:40:50.320 --> 01:40:56.640]   Rarely devices that have the Google Assistant built in may experience what we call a false accept.
[01:40:56.640 --> 01:40:58.320]   It's not so rare.
[01:40:58.320 --> 01:41:02.160]   It's 15% of this particular sample size rare.
[01:41:02.160 --> 01:41:03.200]   That's why.
[01:41:03.200 --> 01:41:08.240]   I want us to stop talking about this like, ah, like here's the details.
[01:41:08.240 --> 01:41:12.000]   I want us to start talking about how we fix this or how we make these.
[01:41:12.000 --> 01:41:14.800]   There is one and only one good.
[01:41:14.800 --> 01:41:16.160]   There's one easy solution.
[01:41:16.160 --> 01:41:17.440]   It pisses me off.
[01:41:17.440 --> 01:41:21.520]   The first assistant that does this will suddenly become the market leader.
[01:41:22.320 --> 01:41:28.080]   Let us change the trigger phrase to something long and unusual.
[01:41:28.080 --> 01:41:28.480]   Please.
[01:41:28.480 --> 01:41:31.200]   My old Moto X phone, it was helping me.
[01:41:31.200 --> 01:41:33.200]   Hey, you call a fraudulent to get the deal of notions.
[01:41:33.200 --> 01:41:35.200]   My old Moto X phone was help me.
[01:41:35.200 --> 01:41:36.880]   And this is how I know they can do it.
[01:41:36.880 --> 01:41:38.240]   They did it five years ago.
[01:41:38.240 --> 01:41:40.560]   Help me, Obi Wan Kenobi, you're my only hope.
[01:41:40.560 --> 01:41:45.840]   If I said that, it would trigger that never had false positives.
[01:41:45.840 --> 01:41:50.800]   The reason you don't do that is because sometimes in the middle of a conversation or
[01:41:50.800 --> 01:41:54.320]   argument, you want to say, help me Obi Wan Kenobi, you're my only hope.
[01:41:54.320 --> 01:41:55.280]   He wants to show off.
[01:41:55.280 --> 01:41:57.840]   He wants to show that off in every conversation.
[01:41:57.840 --> 01:42:00.400]   No, I'm just right, Carson.
[01:42:00.400 --> 01:42:05.520]   And I tell you what, if Echo does it or Google does it, whoever does it first,
[01:42:05.520 --> 01:42:09.760]   and it's not a hard thing to do, this would be so wonderful.
[01:42:09.760 --> 01:42:13.680]   It would be immediately the best selling device because everybody agrees with me on this.
[01:42:13.680 --> 01:42:17.440]   The only reason I can think they don't do this is because it's expensive.
[01:42:17.440 --> 01:42:18.560]   You have to have more memory.
[01:42:18.560 --> 01:42:20.400]   You have to have more processor.
[01:42:20.400 --> 01:42:24.800]   If phone could do it because the phone's got a lot of intelligence, why these phones don't is beyond me.
[01:42:24.800 --> 01:42:33.120]   But a small cheap Echo daughter, Google Home Hub, maybe that's too expensive.
[01:42:33.120 --> 01:42:38.640]   But look, add the five bucks because even if it costs five bucks more, everybody will buy that.
[01:42:38.640 --> 01:42:39.840]   I don't know why they haven't done that.
[01:42:39.840 --> 01:42:40.720]   That drives me nuts.
[01:42:40.720 --> 01:42:44.080]   Okay.
[01:42:44.080 --> 01:42:46.160]   Well, I have different ideas.
[01:42:47.600 --> 01:42:52.080]   It would also make it so we don't have to always say, hey, Shlomo,
[01:42:52.080 --> 01:42:56.720]   hey, Guillermo, Alio,
[01:42:56.720 --> 01:43:01.040]   children, children could be named the A word again.
[01:43:01.040 --> 01:43:03.280]   Children could be named Ali HexÃ© again.
[01:43:03.280 --> 01:43:05.760]   It would please set the free.
[01:43:05.760 --> 01:43:11.760]   Have we seen any, you know, those every year, those, you know, names, kids have,
[01:43:11.760 --> 01:43:15.680]   as they're, have you ever seen anything that says, oh, yeah, that word has gone up or down?
[01:43:15.680 --> 01:43:16.080]   Oh, yeah.
[01:43:16.080 --> 01:43:16.880]   Yeah.
[01:43:16.880 --> 01:43:18.160]   Social security.
[01:43:18.160 --> 01:43:18.480]   Yeah.
[01:43:18.480 --> 01:43:19.360]   It went down.
[01:43:19.360 --> 01:43:22.400]   I'll go to Wolfram Alpha.
[01:43:22.400 --> 01:43:27.120]   Because, you know, I love one of the things I love about Wolfram Alpha is you can put your name in
[01:43:27.120 --> 01:43:30.160]   and you can see the incidence of using that name.
[01:43:30.160 --> 01:43:34.880]   So let me just say the name A-L-E-X-A ranks 90th.
[01:43:34.880 --> 01:43:36.000]   Yeah.
[01:43:36.000 --> 01:43:36.720]   Look at the graph.
[01:43:36.720 --> 01:43:40.400]   Can I zoom in on that graph?
[01:43:40.400 --> 01:43:42.160]   Let me see if I can zoom in on that graph a little bit.
[01:43:43.040 --> 01:43:52.560]   So see, it was going up until it was about 2008 and the sudden precipitous decline in people naming
[01:43:52.560 --> 01:43:56.160]   their children A-L-E-X-A.
[01:43:56.160 --> 01:44:00.800]   There are 85,000 people named Ali HexÃ© today.
[01:44:00.800 --> 01:44:04.560]   But, oh, a lot of four-year-olds.
[01:44:04.560 --> 01:44:08.880]   Expected current age distribution, the Youngs.
[01:44:10.000 --> 01:44:12.320]   Here are some famous people named Ali HexÃ©.
[01:44:12.320 --> 01:44:17.040]   But if you look at that graph, that drop-off is pretty precipitous.
[01:44:17.040 --> 01:44:18.320]   Something must have caused that.
[01:44:18.320 --> 01:44:19.760]   Isn't that interesting?
[01:44:19.760 --> 01:44:20.320]   Yeah.
[01:44:20.320 --> 01:44:22.560]   The next web did a story.
[01:44:22.560 --> 01:44:22.960]   Let's see.
[01:44:22.960 --> 01:44:25.600]   How old is five weeks ago?
[01:44:25.600 --> 01:44:28.080]   And they've got graphs on Madam A.
[01:44:28.080 --> 01:44:32.240]   They've got graphs on Bixby, which is hilarious because Bixby actually goes up.
[01:44:32.240 --> 01:44:34.400]   Nobody names their kid Bixby.
[01:44:34.400 --> 01:44:36.160]   And if you do, you should be punished.
[01:44:36.160 --> 01:44:37.440]   It went from four to five.
[01:44:37.440 --> 01:44:38.000]   Yes.
[01:44:38.000 --> 01:44:38.400]   Yeah.
[01:44:38.400 --> 01:44:40.160]   It's like zero to ten.
[01:44:40.160 --> 01:44:41.200]   Is there a Y-L?
[01:44:41.200 --> 01:44:44.640]   But we know there's at least one kid in Indonesia named Google.
[01:44:44.640 --> 01:44:44.880]   So...
[01:44:44.880 --> 01:44:46.720]   Yes.
[01:44:46.720 --> 01:44:48.640]   So in series, same decline.
[01:44:48.640 --> 01:44:49.280]   Cortana.
[01:44:49.280 --> 01:44:50.720]   No.
[01:44:50.720 --> 01:44:55.520]   It peaked in 2015, apparently.
[01:44:55.520 --> 01:44:56.000]   What happened?
[01:44:56.000 --> 01:44:56.640]   Was that when...
[01:44:56.640 --> 01:44:59.760]   I think it's right about when all this stuff started appearing.
[01:44:59.760 --> 01:45:00.480]   Yeah.
[01:45:00.480 --> 01:45:01.200]   Oh, I was what?
[01:45:01.200 --> 01:45:03.840]   Because there's a Cortana named after a video game character?
[01:45:03.840 --> 01:45:04.160]   Yeah.
[01:45:04.160 --> 01:45:06.400]   It's a character in Halo, yeah.
[01:45:06.400 --> 01:45:06.800]   Halo.
[01:45:06.800 --> 01:45:08.400]   It's actually the same character.
[01:45:08.400 --> 01:45:11.600]   It's a virtual assistant female virtual.
[01:45:11.600 --> 01:45:12.640]   It's like Jarvis.
[01:45:12.640 --> 01:45:13.280]   It's like Jarvis.
[01:45:13.280 --> 01:45:14.000]   Only female.
[01:45:14.000 --> 01:45:14.400]   Yeah.
[01:45:14.400 --> 01:45:16.240]   With three syllables.
[01:45:16.240 --> 01:45:18.480]   Okay.
[01:45:18.480 --> 01:45:18.960]   Okay.
[01:45:18.960 --> 01:45:19.280]   Good.
[01:45:19.280 --> 01:45:21.120]   Thank you for ending this on a up note.
[01:45:21.120 --> 01:45:22.240]   Anything else we should...
[01:45:22.240 --> 01:45:24.160]   Oh, let's see here.
[01:45:24.160 --> 01:45:27.600]   Did we miss anything you wanted to talk about, Jeff or Stacy?
[01:45:27.600 --> 01:45:28.000]   We got the...
[01:45:28.000 --> 01:45:31.840]   I love the Japanese toast drum, happy.
[01:45:31.840 --> 01:45:33.680]   Facebook's new finger cams.
[01:45:33.680 --> 01:45:35.360]   What about...
[01:45:36.320 --> 01:45:37.200]   That's not real.
[01:45:37.200 --> 01:45:37.760]   Oh, good.
[01:45:37.760 --> 01:45:38.080]   Okay.
[01:45:38.080 --> 01:45:38.640]   It's a pat.
[01:45:38.640 --> 01:45:39.200]   It's a pat.
[01:45:39.200 --> 01:45:39.440]   It's a pat.
[01:45:39.440 --> 01:45:40.240]   It's me and they're going to do it.
[01:45:40.240 --> 01:45:40.640]   Okay.
[01:45:40.640 --> 01:45:41.840]   And so he goes berserk about it.
[01:45:41.840 --> 01:45:43.600]   He's going to put wires in your head.
[01:45:43.600 --> 01:45:44.240]   No!
[01:45:44.240 --> 01:45:44.720]   No!
[01:45:44.720 --> 01:45:45.040]   Neural link.
[01:45:45.040 --> 01:45:45.200]   No!
[01:45:45.200 --> 01:45:45.200]   No!
[01:45:45.200 --> 01:45:45.920]   We got it to that one.
[01:45:45.920 --> 01:45:46.160]   No!
[01:45:46.160 --> 01:45:46.800]   Yeah, we got it to that one.
[01:45:46.800 --> 01:45:47.200]   So...
[01:45:47.200 --> 01:45:48.080]   That was last night.
[01:45:48.080 --> 01:45:48.880]   That was last night.
[01:45:48.880 --> 01:45:49.840]   I know!
[01:45:49.840 --> 01:45:50.960]   It was trending on Twitter.
[01:45:50.960 --> 01:45:52.160]   I said, what is this?
[01:45:52.160 --> 01:45:54.240]   They had a live event, which I didn't watch.
[01:45:54.240 --> 01:45:56.720]   I watched the last half of it.
[01:45:56.720 --> 01:46:00.800]   The first half my wife made us go out to the park and go pottering.
[01:46:00.800 --> 01:46:01.280]   But...
[01:46:01.280 --> 01:46:01.920]   Good for you.
[01:46:01.920 --> 01:46:08.480]   Good for Kimmy.
[01:46:08.480 --> 01:46:10.080]   We HPW you fans.
[01:46:10.080 --> 01:46:11.120]   We call it pottering.
[01:46:11.120 --> 01:46:13.360]   I thought it was mispronounced "puttering."
[01:46:13.360 --> 01:46:13.760]   No.
[01:46:13.760 --> 01:46:14.480]   Well, it is.
[01:46:14.480 --> 01:46:15.600]   It's equivalent of puttering.
[01:46:15.600 --> 01:46:16.640]   I need the putter in the park.
[01:46:16.640 --> 01:46:18.080]   It's equivalent of puttering.
[01:46:18.080 --> 01:46:20.640]   And because there's a big event going on now,
[01:46:20.640 --> 01:46:23.680]   we're all out and about all of us putterers.
[01:46:23.680 --> 01:46:27.120]   Elon Musk's Neural Link.
[01:46:27.120 --> 01:46:30.400]   This is basically Elon reading too much science fiction,
[01:46:30.400 --> 01:46:32.400]   but I am all for it.
[01:46:32.400 --> 01:46:37.920]   The idea is why are we wearing silly glasses or in-ear monitors
[01:46:37.920 --> 01:46:40.800]   when we could have wires directed into our brain?
[01:46:40.800 --> 01:46:46.240]   And who better to bore a hole into our heads than Elon Musk?
[01:46:46.240 --> 01:46:46.560]   Yeah.
[01:46:46.560 --> 01:46:50.400]   The first big advance is flexible threads, which are less likely.
[01:46:50.400 --> 01:46:54.480]   No, not highly unlikely, but less likely to damage the brain
[01:46:54.480 --> 01:46:57.840]   than the current materials used in brain machine interfaces.
[01:46:57.840 --> 01:47:00.160]   These threads also create the possibility.
[01:47:00.160 --> 01:47:01.200]   There might be some fender benders.
[01:47:01.200 --> 01:47:03.840]   There might be less likely.
[01:47:03.840 --> 01:47:05.120]   Threads also create the possibility
[01:47:05.120 --> 01:47:06.960]   transferring a higher volume of data
[01:47:06.960 --> 01:47:11.520]   as many as 3,072 electrodes per array
[01:47:11.520 --> 01:47:13.760]   distributed across 96 threads.
[01:47:13.760 --> 01:47:16.080]   They are 4 to 6 micrometers in width.
[01:47:16.080 --> 01:47:17.600]   That's thinner than a human hair.
[01:47:17.600 --> 01:47:20.720]   You know, think of it as acupuncture for the brain.
[01:47:20.720 --> 01:47:22.320]   You first, Elon.
[01:47:22.320 --> 01:47:24.800]   They are looking for volunteers.
[01:47:24.800 --> 01:47:27.600]   Early experiments will be done with neuroscientists at Stanford.
[01:47:28.240 --> 01:47:28.800]   We hope...
[01:47:28.800 --> 01:47:31.360]   We need you to describe the picture of the rat
[01:47:31.360 --> 01:47:33.200]   because they've tried it on rats.
[01:47:33.200 --> 01:47:36.080]   Oh, this isn't the story.
[01:47:36.080 --> 01:47:38.080]   There's a fabulous photo.
[01:47:38.080 --> 01:47:39.120]   I think it's on the verge.
[01:47:39.120 --> 01:47:41.200]   There's the rat with a USB-C interface.
[01:47:41.200 --> 01:47:42.560]   Good.
[01:47:42.560 --> 01:47:43.920]   God, that is the worst.
[01:47:43.920 --> 01:47:45.920]   That does seem a little cruel.
[01:47:45.920 --> 01:47:47.680]   Doesn't it seem well?
[01:47:47.680 --> 01:47:50.640]   Well, yeah, at least it's not micro-USB.
[01:47:50.640 --> 01:47:51.600]   That would be cruel.
[01:47:51.600 --> 01:47:52.960]   At least it's not moderate, don't they?
[01:47:52.960 --> 01:47:53.520]   It would be cruel.
[01:47:54.640 --> 01:47:58.240]   So they said by the end of the year,
[01:47:58.240 --> 01:47:59.520]   they're going to have a human.
[01:47:59.520 --> 01:48:03.840]   We hope to have a human patient by the end of next year.
[01:48:03.840 --> 01:48:08.240]   Mosk revealed results that the rest of the team
[01:48:08.240 --> 01:48:09.200]   hadn't realized he would.
[01:48:09.200 --> 01:48:12.400]   A monkey has been able to control a computer with its brain.
[01:48:12.400 --> 01:48:15.520]   You know what?
[01:48:15.520 --> 01:48:18.400]   I'm hoping in my lifetime I could put it on a little funny hat
[01:48:18.400 --> 01:48:20.560]   with a USB-C interface.
[01:48:22.880 --> 01:48:25.920]   I can lie in bed and be stimulated.
[01:48:25.920 --> 01:48:28.320]   Are doing that, sweet.
[01:48:28.320 --> 01:48:29.280]   Can you rephrase that?
[01:48:29.280 --> 01:48:30.640]   Magnetic resonance.
[01:48:30.640 --> 01:48:34.000]   The don't that are non-intrusive.
[01:48:34.000 --> 01:48:37.600]   Ultimately.
[01:48:37.600 --> 01:48:38.560]   You don't just put on a funny hat.
[01:48:38.560 --> 01:48:42.960]   You have this robot that looks like a combination of a,
[01:48:42.960 --> 01:48:45.680]   what was it, a sewing machine and a microscope
[01:48:45.680 --> 01:48:50.080]   that injects these 96 filaments into your brain.
[01:48:50.720 --> 01:48:52.560]   Because he also invented the robot.
[01:48:52.560 --> 01:48:56.080]   And if you look at a picture of that, it is horrifying.
[01:48:56.080 --> 01:48:56.320]   Yeah.
[01:48:56.320 --> 01:48:59.360]   It's not going to be suddenly neural ink.
[01:48:59.360 --> 01:49:02.000]   We'll have this neural license start taking over people's brain.
[01:49:02.000 --> 01:49:05.840]   Musk said, "Ultimately, we want to achieve a symbiosis
[01:49:05.840 --> 01:49:07.760]   with artificial intelligence."
[01:49:07.760 --> 01:49:10.480]   Because even in a benign scenario,
[01:49:10.480 --> 01:49:12.320]   humans are going to be left behind.
[01:49:12.320 --> 01:49:16.880]   So what we want to create is emerging with AI.
[01:49:16.880 --> 01:49:19.280]   He said, "We're already a brain in a vat."
[01:49:20.160 --> 01:49:21.280]   That vats are skull.
[01:49:21.280 --> 01:49:25.920]   So what we want to do is make a connection into the vat.
[01:49:25.920 --> 01:49:31.600]   What's the thing that he has been arguing is so scary and awful?
[01:49:31.600 --> 01:49:33.520]   Well, he's kind of his attitude.
[01:49:33.520 --> 01:49:35.600]   And I think Elon's using a little too much something.
[01:49:35.600 --> 01:49:39.920]   I think his attitude is, these are the neural-length threads.
[01:49:39.920 --> 01:49:46.480]   His attitude is, if the AI is taking over, at least we can be part of it.
[01:49:50.000 --> 01:49:52.080]   What's he having with the lunch so I don't have it?
[01:49:52.080 --> 01:49:55.920]   Is there, well, I think the kids call it Molly.
[01:49:55.920 --> 01:50:02.240]   To combat the problem, the company has developed the aforementioned neurological robot
[01:50:02.240 --> 01:50:06.720]   capable of inserting six threads a minute automatically.
[01:50:06.720 --> 01:50:09.360]   I want to see the picture.
[01:50:09.360 --> 01:50:11.200]   Where's the picture of the robot?
[01:50:11.200 --> 01:50:11.920]   Here it is.
[01:50:11.920 --> 01:50:12.400]   There.
[01:50:12.400 --> 01:50:13.920]   It's not really a robot though.
[01:50:13.920 --> 01:50:15.360]   It's not walking around.
[01:50:15.360 --> 01:50:18.640]   You have that's going to hang out on your head.
[01:50:18.640 --> 01:50:19.440]   Let me.
[01:50:19.440 --> 01:50:19.840]   Let me.
[01:50:19.840 --> 01:50:20.240]   So cool.
[01:50:20.240 --> 01:50:23.280]   I had a robot like that hang out my groin for surgery.
[01:50:23.280 --> 01:50:23.600]   Really?
[01:50:23.600 --> 01:50:25.440]   Yeah.
[01:50:25.440 --> 01:50:29.040]   Is that what an arthroscopic surgery looks like?
[01:50:29.040 --> 01:50:30.560]   That's what it is now, it's robotic.
[01:50:30.560 --> 01:50:31.760]   Yeah, it's robotic.
[01:50:31.760 --> 01:50:33.760]   So I'm going to just illustrate this.
[01:50:33.760 --> 01:50:36.560]   Draw a picture of Karsten's head right there.
[01:50:36.560 --> 01:50:38.960]   Where's the bow tie?
[01:50:38.960 --> 01:50:40.560]   And oh yeah, we need a bow tie.
[01:50:40.560 --> 01:50:40.960]   All right.
[01:50:40.960 --> 01:50:42.560]   Bow tie for orientation.
[01:50:42.560 --> 01:50:44.080]   Yep, just for orientation.
[01:50:44.080 --> 01:50:45.120]   There's Karsten's head.
[01:50:45.120 --> 01:50:48.160]   His bow tie is usually smaller than his head.
[01:50:48.160 --> 01:50:48.480]   You know what?
[01:50:48.480 --> 01:50:51.360]   I should probably make the threads be a different color, shouldn't I?
[01:50:51.360 --> 01:50:52.720]   Yeah.
[01:50:52.720 --> 01:50:53.920]   Yeah, let's make the threads.
[01:50:53.920 --> 01:51:00.960]   Oh, this is perfect.
[01:51:00.960 --> 01:51:07.440]   I would, when I look like that picture from FaceApp,
[01:51:07.440 --> 01:51:08.400]   I will get this done.
[01:51:08.400 --> 01:51:09.840]   Because why not?
[01:51:09.840 --> 01:51:12.400]   You got nothing left to lose.
[01:51:12.400 --> 01:51:13.760]   They could put it in my wattle.
[01:51:17.840 --> 01:51:19.600]   So Karsten, you think this is a good idea?
[01:51:19.600 --> 01:51:24.000]   I think this is important and needs to be done.
[01:51:24.000 --> 01:51:25.200]   Yes, we've got to do it.
[01:51:25.200 --> 01:51:27.440]   There's going to be, you're going to, you know,
[01:51:27.440 --> 01:51:29.600]   to make an omelet.
[01:51:29.600 --> 01:51:30.320]   Is it important Karsten?
[01:51:30.320 --> 01:51:31.760]   You need to break a few eggs.
[01:51:31.760 --> 01:51:35.680]   So is it important because of the looming threat of AI,
[01:51:35.680 --> 01:51:38.640]   or is it important because it's people who are paralyzed
[01:51:38.640 --> 01:51:39.680]   and otherwise in capacity?
[01:51:39.680 --> 01:51:42.720]   There are several reasons why it is important.
[01:51:42.720 --> 01:51:47.280]   It is important because people are blind and deaf.
[01:51:47.280 --> 01:51:48.000]   Yeah.
[01:51:48.000 --> 01:51:54.240]   It is also important because movies shown through this would be really cool.
[01:51:54.240 --> 01:51:58.720]   I think, imagine, I think this would be great.
[01:51:58.720 --> 01:52:01.360]   Because so far, here's the problem.
[01:52:01.360 --> 01:52:04.240]   And this is, you know, this is the stuff of science fiction.
[01:52:04.240 --> 01:52:07.440]   We're in the matrix that connect to Thing, the back of your head.
[01:52:07.440 --> 01:52:11.520]   This, the human machine interface is very poor right now.
[01:52:11.520 --> 01:52:14.320]   Speech is the closest thing we've got to a natural interface.
[01:52:14.320 --> 01:52:15.360]   Typing is not it.
[01:52:16.400 --> 01:52:20.080]   And, you know, and wearables like watches or air pods
[01:52:20.080 --> 01:52:21.520]   are these silly glasses.
[01:52:21.520 --> 01:52:24.400]   They're really primitive.
[01:52:24.400 --> 01:52:26.720]   I think we need, we're going to need, if it's going to happen,
[01:52:26.720 --> 01:52:29.680]   we need this kind of, you know, interface.
[01:52:29.680 --> 01:52:33.520]   So yeah, and you know what, I think within my lifetime,
[01:52:33.520 --> 01:52:34.400]   we will perfect this.
[01:52:34.400 --> 01:52:36.800]   Next 20 or 30 years we will perfect this.
[01:52:36.800 --> 01:52:38.160]   And then I predict you'll become,
[01:52:38.160 --> 01:52:40.320]   whatever company does it, you'll be complaining about them.
[01:52:40.320 --> 01:52:42.400]   Well, yeah.
[01:52:44.720 --> 01:52:47.600]   They've got a monopoly on the neural link on my head.
[01:52:47.600 --> 01:52:49.840]   I have such a headache.
[01:52:49.840 --> 01:52:51.680]   You won't need Botox.
[01:52:51.680 --> 01:52:52.000]   Yeah.
[01:52:52.000 --> 01:52:53.040]   You won't need Botox.
[01:52:53.040 --> 01:52:53.760]   Maybe.
[01:52:53.760 --> 01:52:54.320]   You may not.
[01:52:54.320 --> 01:52:55.040]   That's true.
[01:52:55.040 --> 01:52:55.840]   You may not.
[01:52:55.840 --> 01:52:56.560]   Yeah.
[01:52:56.560 --> 01:52:56.800]   Yeah.
[01:52:56.800 --> 01:52:59.360]   All right.
[01:52:59.360 --> 01:53:01.440]   Neural almost all done.
[01:53:01.440 --> 01:53:02.240]   And we're done.
[01:53:02.240 --> 01:53:03.120]   You're right.
[01:53:03.120 --> 01:53:03.840]   This is what we need.
[01:53:03.840 --> 01:53:06.240]   We need every head in the country so we can just calm,
[01:53:06.240 --> 01:53:07.040]   angry people down.
[01:53:07.040 --> 01:53:07.520]   Push a button.
[01:53:07.520 --> 01:53:11.120]   The president, instead of having the nuclear codes,
[01:53:11.120 --> 01:53:12.480]   he'll have the neural link codes.
[01:53:12.480 --> 01:53:13.680]   Oh, God.
[01:53:13.680 --> 01:53:14.960]   Flip a switch and we all go,
[01:53:14.960 --> 01:53:20.000]   so the funniest thing, I didn't watch this,
[01:53:20.000 --> 01:53:21.920]   but according to the Verge,
[01:53:21.920 --> 01:53:23.520]   the chief surgeon,
[01:53:23.520 --> 01:53:25.920]   when he came out to do his presentation,
[01:53:25.920 --> 01:53:27.040]   was dressed in scrubs.
[01:53:27.040 --> 01:53:28.400]   Like he was like, just out.
[01:53:28.400 --> 01:53:29.520]   I'm just out of the OR.
[01:53:29.520 --> 01:53:35.280]   Little bit of theater in Elon's soul.
[01:53:35.280 --> 01:53:36.000]   Little bit of it.
[01:53:36.000 --> 01:53:39.040]   Let's get our picks of the weekend.
[01:53:39.040 --> 01:53:39.520]   We can.
[01:53:39.520 --> 01:53:41.040]   Are we not going to do a, you know what?
[01:53:41.040 --> 01:53:42.560]   A what?
[01:53:42.560 --> 01:53:44.560]   The Google change.
[01:53:44.560 --> 01:53:45.440]   Oh crap.
[01:53:45.440 --> 01:53:47.280]   Again with a Google change log.
[01:53:47.280 --> 01:53:48.480]   Play the trumpets.
[01:53:48.480 --> 01:53:48.960]   No, no.
[01:53:48.960 --> 01:53:50.640]   I said I would.
[01:53:50.640 --> 01:53:52.480]   Carsten has prepared it.
[01:53:52.480 --> 01:53:54.400]   The Google change log.
[01:53:54.400 --> 01:53:55.920]   The trumpeters are ready.
[01:53:55.920 --> 01:53:58.400]   The trumpets will sound.
[01:53:58.400 --> 01:54:00.560]   They've been smoking in the back for a while.
[01:54:00.560 --> 01:54:02.240]   How about this?
[01:54:02.240 --> 01:54:03.440]   Google, this is neat.
[01:54:03.440 --> 01:54:04.240]   Arts and culture,
[01:54:04.240 --> 01:54:05.680]   they do some really cool stuff.
[01:54:05.680 --> 01:54:07.920]   They have made an AR gallery
[01:54:07.920 --> 01:54:09.840]   that you can use that celebrates color,
[01:54:09.840 --> 01:54:10.640]   the art of color.
[01:54:10.640 --> 01:54:12.400]   You can wander through the gallery.
[01:54:12.400 --> 01:54:15.120]   It's a pocket gal.
[01:54:15.120 --> 01:54:17.520]   You need the application pocket gallery.
[01:54:17.520 --> 01:54:19.280]   And you put it on your phone.
[01:54:19.280 --> 01:54:19.840]   Is it Rothko's?
[01:54:19.840 --> 01:54:22.720]   I don't know.
[01:54:22.720 --> 01:54:25.600]   You can actually put these things.
[01:54:25.600 --> 01:54:27.280]   This is, we're seeing a lot of this
[01:54:27.280 --> 01:54:30.960]   in honor of the Apollo 11 launch this week.
[01:54:30.960 --> 01:54:33.840]   NASA has put a lunar lander on your desk.
[01:54:33.840 --> 01:54:35.360]   Oh.
[01:54:35.360 --> 01:54:36.720]   Whistler's mother.
[01:54:36.720 --> 01:54:39.680]   Oh, you know, they say they've got
[01:54:39.680 --> 01:54:41.760]   Rembrandt's masterpiece, The Night Watch,
[01:54:41.760 --> 01:54:44.960]   which I have seen in Amsterdam.
[01:54:44.960 --> 01:54:47.440]   If you get a chance to go
[01:54:47.440 --> 01:54:52.880]   to the Rijksmuseum and go to the Rembrandt gallery,
[01:54:52.880 --> 01:54:54.480]   The Night Watch is at the end.
[01:54:54.480 --> 01:54:55.760]   It is massive.
[01:54:55.760 --> 01:54:58.880]   And that is worth going to Amsterdam,
[01:54:58.880 --> 01:54:59.440]   just to see.
[01:54:59.440 --> 01:55:00.960]   But now you can do it in your phone.
[01:55:00.960 --> 01:55:02.880]   I can't imagine it'll be quite the same.
[01:55:02.880 --> 01:55:06.800]   But not everybody can get there.
[01:55:06.800 --> 01:55:10.560]   And it's just an amazing painter.
[01:55:10.560 --> 01:55:13.280]   Speaking of AR, did you see the Stranger Things
[01:55:13.280 --> 01:55:14.240]   at the New York Times,
[01:55:14.240 --> 01:55:16.880]   where you can go visit the Star Court Mall
[01:55:16.880 --> 01:55:19.680]   and it becomes to life?
[01:55:19.680 --> 01:55:22.160]   That means you need to print newspaper.
[01:55:22.160 --> 01:55:22.720]   What's that?
[01:55:22.720 --> 01:55:26.080]   Yeah, you know, it's kind of a mismatch.
[01:55:26.080 --> 01:55:27.760]   It's a technology mismatch.
[01:55:27.760 --> 01:55:28.080]   Yeah.
[01:55:28.080 --> 01:55:28.400]   Yeah.
[01:55:28.400 --> 01:55:30.160]   Have you watched Stranger Things yet?
[01:55:30.160 --> 01:55:31.120]   I started.
[01:55:31.120 --> 01:55:32.800]   I haven't.
[01:55:32.800 --> 01:55:33.920]   Not Capillane.
[01:55:33.920 --> 01:55:35.120]   Not Capillane.
[01:55:35.120 --> 01:55:38.640]   Maybe the '80s weren't like a big thing for me.
[01:55:38.640 --> 01:55:39.040]   I don't know.
[01:55:39.040 --> 01:55:41.520]   That was my childhood.
[01:55:41.520 --> 01:55:43.440]   So I'm going to all reserve judgment.
[01:55:43.440 --> 01:55:45.200]   Did you like it, Kirsten?
[01:55:45.200 --> 01:55:48.640]   I did not actually like the '80s
[01:55:48.640 --> 01:55:49.920]   by moving through them.
[01:55:49.920 --> 01:55:52.560]   Nobody lives in the '80s like the '80s.
[01:55:52.560 --> 01:55:54.480]   I like looking back upon them.
[01:55:54.480 --> 01:55:56.000]   That mall is kind of cool.
[01:55:56.000 --> 01:55:58.880]   Is that old, disused mall that they rebuilt
[01:55:58.880 --> 01:56:00.000]   for shooting the show?
[01:56:00.000 --> 01:56:05.040]   And as a result, they put in old movies,
[01:56:05.040 --> 01:56:06.640]   theaters, they had a game.
[01:56:06.640 --> 01:56:08.560]   There was a game store they didn't end up using.
[01:56:08.560 --> 01:56:10.480]   It's kind of cool.
[01:56:10.480 --> 01:56:16.160]   Google tweeted because that's how Google communicates now.
[01:56:16.160 --> 01:56:19.680]   Over the next couple of weeks, we're rolling out a redesign
[01:56:19.680 --> 01:56:22.560]   news tab and search on desktop that refresh design
[01:56:22.560 --> 01:56:25.040]   makes publisher names more prominent.
[01:56:25.040 --> 01:56:26.240]   I wonder why.
[01:56:26.240 --> 01:56:29.200]   And organize these articles more clearly to help you find the news
[01:56:29.200 --> 01:56:33.040]   you need to which immediately somebody replies,
[01:56:33.040 --> 01:56:34.320]   is there a way to revert it back?
[01:56:35.200 --> 01:56:36.720]   [laughter]
[01:56:36.720 --> 01:56:37.920]   Have we seen this?
[01:56:37.920 --> 01:56:38.640]   Have you seen it?
[01:56:38.640 --> 01:56:39.280]   Yeah.
[01:56:39.280 --> 01:56:42.080]   So the problem is when they changed Google
[01:56:42.080 --> 01:56:42.800]   and it was the last time,
[01:56:42.800 --> 01:56:45.040]   it became useless for me for researching the show.
[01:56:45.040 --> 01:56:47.200]   So what I have to do is go to Google,
[01:56:47.200 --> 01:56:48.160]   search for the word Google,
[01:56:48.160 --> 01:56:49.680]   which brings me to feel like an idiot,
[01:56:49.680 --> 01:56:50.960]   and then grow the news tab there.
[01:56:50.960 --> 01:56:52.000]   Oh.
[01:56:52.000 --> 01:56:54.480]   And the advantage of that was that well,
[01:56:54.480 --> 01:56:55.440]   but it's not so good anymore.
[01:56:55.440 --> 01:56:57.600]   The advantage of that was that it had the old view.
[01:56:57.600 --> 01:56:59.760]   Here's one version of the story and here's other versions.
[01:56:59.760 --> 01:57:01.040]   They've taken all that away.
[01:57:01.040 --> 01:57:02.480]   So now it's just pretty boxes.
[01:57:03.440 --> 01:57:05.120]   And it's not as useful.
[01:57:05.120 --> 01:57:05.680]   Oh.
[01:57:05.680 --> 01:57:06.880]   That's pretty boxes now.
[01:57:06.880 --> 01:57:08.240]   That's the pretty boxes.
[01:57:08.240 --> 01:57:10.320]   It's useful, but not as useful.
[01:57:10.320 --> 01:57:10.640]   Yeah.
[01:57:10.640 --> 01:57:12.320]   Oh.
[01:57:12.320 --> 01:57:12.640]   All right.
[01:57:12.640 --> 01:57:14.800]   And Google, of course,
[01:57:14.800 --> 01:57:18.400]   leads with Fox News because,
[01:57:18.400 --> 01:57:20.720]   hey, I don't want anybody to shut me down.
[01:57:20.720 --> 01:57:24.000]   I actually might be a search result for me.
[01:57:24.000 --> 01:57:26.240]   I don't know if you get the same result.
[01:57:26.240 --> 01:57:28.000]   It's also a current story.
[01:57:28.000 --> 01:57:28.400]   Yeah.
[01:57:28.400 --> 01:57:30.000]   That's how I do research every week for the show.
[01:57:30.000 --> 01:57:30.640]   Oh, smart.
[01:57:30.640 --> 01:57:32.720]   Do you know about the tennis game?
[01:57:33.440 --> 01:57:34.640]   Hidden in Google?
[01:57:34.640 --> 01:57:35.920]   I lost with one ball.
[01:57:35.920 --> 01:57:39.280]   I know you're a serious tennis player.
[01:57:39.280 --> 01:57:40.720]   No.
[01:57:40.720 --> 01:57:43.040]   So it's in honor of Wimbledon,
[01:57:43.040 --> 01:57:44.960]   which, by the way,
[01:57:44.960 --> 01:57:46.480]   Federer Jocovitch, what a...
[01:57:46.480 --> 01:57:46.880]   She's...
[01:57:46.880 --> 01:57:47.440]   What a...
[01:57:47.440 --> 01:57:50.240]   We are all huge Federer fans,
[01:57:50.240 --> 01:57:51.280]   so it broke our hearts,
[01:57:51.280 --> 01:57:52.800]   but what a amazing platform.
[01:57:52.800 --> 01:57:54.240]   You know, nobody won that match.
[01:57:54.240 --> 01:57:55.440]   That was a tie.
[01:57:55.440 --> 01:57:56.080]   Oh.
[01:57:56.080 --> 01:57:56.640]   That was amazing.
[01:57:56.640 --> 01:57:57.280]   Was it ever?
[01:57:57.280 --> 01:57:57.520]   Yeah.
[01:57:57.520 --> 01:57:59.040]   So, okay.
[01:57:59.040 --> 01:58:01.280]   So if you search for Wimbledon scores,
[01:58:02.160 --> 01:58:03.760]   where's the tennis?
[01:58:03.760 --> 01:58:06.320]   Is it at the bottom?
[01:58:06.320 --> 01:58:09.120]   I don't see it.
[01:58:09.120 --> 01:58:10.400]   Let me go back and look.
[01:58:10.400 --> 01:58:12.080]   Look at the article again.
[01:58:12.080 --> 01:58:15.280]   You go to the score for your Wimbledon scores,
[01:58:15.280 --> 01:58:16.960]   and then there's a bar at the top.
[01:58:16.960 --> 01:58:18.640]   You scroll it far to the left.
[01:58:18.640 --> 01:58:21.040]   And there's a little ball there at the right.
[01:58:21.040 --> 01:58:21.360]   Ah.
[01:58:21.360 --> 01:58:23.680]   See, I didn't...
[01:58:23.680 --> 01:58:24.800]   So, I was a score.
[01:58:24.800 --> 01:58:26.000]   All the way to the left.
[01:58:26.000 --> 01:58:27.360]   Oh, I wasn't scrolling.
[01:58:27.360 --> 01:58:27.600]   It's all there, right?
[01:58:27.600 --> 01:58:28.080]   There it is.
[01:58:28.080 --> 01:58:28.720]   There's a green ball.
[01:58:28.720 --> 01:58:29.920]   What's that ball doing there?
[01:58:29.920 --> 01:58:31.360]   Oh, I'll tell you what it's doing there.
[01:58:31.360 --> 01:58:34.400]   It's time to play tennis.
[01:58:34.400 --> 01:58:35.440]   Wait, let me turn my side.
[01:58:35.440 --> 01:58:36.160]   Right and left keys.
[01:58:36.160 --> 01:58:37.440]   Okay.
[01:58:37.440 --> 01:58:38.800]   Okay, I'm ready.
[01:58:38.800 --> 01:58:42.240]   Is there sound?
[01:58:42.240 --> 01:58:46.720]   Oh, this is pong.
[01:58:46.720 --> 01:58:48.800]   That's pong, baby.
[01:58:48.800 --> 01:58:50.240]   Boop.
[01:58:50.240 --> 01:58:54.080]   And I'm just...
[01:58:54.080 --> 01:58:54.720]   It's over.
[01:58:54.720 --> 01:58:55.600]   And I'm just as bad.
[01:58:55.600 --> 01:58:58.960]   No, it's not over yet.
[01:58:58.960 --> 01:58:59.280]   Can't.
[01:58:59.920 --> 01:59:02.720]   Oh, oh, this is like Federer Djokovic.
[01:59:02.720 --> 01:59:04.240]   Okay, never mind.
[01:59:04.240 --> 01:59:07.200]   The first Google messages beta.
[01:59:07.200 --> 01:59:10.320]   4.7 crashes instantly on start,
[01:59:10.320 --> 01:59:11.680]   just a word of warning,
[01:59:11.680 --> 01:59:12.720]   if you're doing the beta.
[01:59:12.720 --> 01:59:16.640]   And Alphabet's wing
[01:59:16.640 --> 01:59:21.920]   launches an app to manage air traffic for drones.
[01:59:21.920 --> 01:59:27.040]   This is the wing air management system.
[01:59:27.040 --> 01:59:29.040]   A wing is an offshoot of Alphabet.
[01:59:29.040 --> 01:59:32.800]   And the app is called OpenSky.
[01:59:32.800 --> 01:59:36.400]   It's been approved to manage drone flights in Australia,
[01:59:36.400 --> 01:59:37.280]   where it's free.
[01:59:37.280 --> 01:59:38.560]   So if you're not down under,
[01:59:38.560 --> 01:59:42.400]   maybe you won't get a chance to use this.
[01:59:42.400 --> 01:59:43.840]   Everyone,
[01:59:43.840 --> 01:59:49.120]   it says empowering everyone to safely access the sky.
[01:59:49.120 --> 01:59:50.400]   Flying is complex.
[01:59:50.400 --> 01:59:52.240]   Through automation and data,
[01:59:52.240 --> 01:59:53.520]   our OpenSky platform
[01:59:53.520 --> 01:59:55.360]   empowers you to take flight with confidence.
[01:59:56.080 --> 01:59:58.800]   Having crashed many, many, many drones.
[01:59:58.800 --> 02:00:04.000]   Many, many, many, many, many drones.
[02:00:04.000 --> 02:00:06.640]   I am all in on this.
[02:00:06.640 --> 02:00:09.040]   It's approved by CASA in Australia.
[02:00:09.040 --> 02:00:13.040]   So when you open it up and you agree to the terms of service,
[02:00:13.040 --> 02:00:14.400]   as I am doing right now,
[02:00:14.400 --> 02:00:17.520]   blah, blah, blah, blah, boy, that's a long term to service.
[02:00:17.520 --> 02:00:19.120]   I should have read that probably.
[02:00:19.120 --> 02:00:21.680]   You'll get a choice of recreational, commercial,
[02:00:21.680 --> 02:00:24.400]   or REOC.
[02:00:24.400 --> 02:00:25.520]   Those are licensed drones.
[02:00:25.520 --> 02:00:28.480]   I'm going to do recreational, flying for fun or hobby.
[02:00:28.480 --> 02:00:32.720]   You see it's only got the map only works in Australia.
[02:00:32.720 --> 02:00:35.280]   There's no information for anywhere else.
[02:00:35.280 --> 02:00:37.840]   They could tell you what you can do.
[02:00:37.840 --> 02:00:41.280]   Here's some checklist.
[02:00:41.280 --> 02:00:42.480]   Okay.
[02:00:42.480 --> 02:00:44.160]   That's cool.
[02:00:44.160 --> 02:00:46.720]   Does this control your drone in some way?
[02:00:46.720 --> 02:00:49.680]   I think it's like a flight plan.
[02:00:49.680 --> 02:00:51.840]   It helps you know what's around, I guess.
[02:00:51.840 --> 02:00:55.040]   We did the emoji.
[02:00:55.040 --> 02:00:56.320]   Oh, one more.
[02:00:56.320 --> 02:00:58.640]   Google Maps now has, I think this is good,
[02:00:58.640 --> 02:00:59.840]   bike sharing stations.
[02:00:59.840 --> 02:01:02.720]   Bike sharing was big.
[02:01:02.720 --> 02:01:05.440]   And is it still big in New York City with a city bike?
[02:01:05.440 --> 02:01:06.560]   People still do that, Jeff.
[02:01:06.560 --> 02:01:08.160]   Oh, yeah.
[02:01:08.160 --> 02:01:12.240]   So last April, city bikes started appearing on Google Maps.
[02:01:12.240 --> 02:01:18.000]   Now, Asia, America, Europe, real time bike sharing data
[02:01:18.000 --> 02:01:23.360]   from a partnership with ITO world to be able to locate the newest,
[02:01:23.360 --> 02:01:26.880]   I'm sorry, the nearest bike sharing station in 23 cities,
[02:01:26.880 --> 02:01:30.640]   Barcelona, Berlin, Brussels, Budapest, Chicago, Dublin, Hamburg, Helsinki,
[02:01:30.640 --> 02:01:37.280]   Cauchon, London, Los Angeles, Lyon, Madrid, Mexico City, Montreal,
[02:01:37.280 --> 02:01:41.120]   New Taipei, City, Rio de Janeiro, the San Francisco Bay Area,
[02:01:41.120 --> 02:01:44.720]   yay, San Paolo, Toronto, Vienna, Warsaw, and Zurich.
[02:01:44.720 --> 02:01:48.480]   You will search for specific bike share service.
[02:01:48.480 --> 02:01:53.040]   So you start with the service, but I'll show you a carousel of services.
[02:01:53.040 --> 02:01:57.040]   And then I'll tell you, the app, if you use Lime or any of the other services,
[02:01:57.040 --> 02:01:59.680]   the app will show you where a nearby bike is.
[02:01:59.680 --> 02:02:02.160]   So this isn't, you know, you can use the,
[02:02:02.160 --> 02:02:04.080]   you probably will use the app because in most cases,
[02:02:04.080 --> 02:02:07.840]   these bikes you use the app to check them out, to unlock them.
[02:02:07.840 --> 02:02:09.520]   So that's nice.
[02:02:09.520 --> 02:02:10.080]   It's good to have.
[02:02:10.080 --> 02:02:12.800]   That's not a quick question, Lee, all one more story.
[02:02:12.800 --> 02:02:13.280]   Yeah.
[02:02:13.280 --> 02:02:16.560]   The Nielsen podcast monitoring story.
[02:02:16.560 --> 02:02:16.880]   Yes.
[02:02:16.880 --> 02:02:19.360]   There's actually two big stories in podcasting.
[02:02:19.360 --> 02:02:24.320]   One, a rumor from Bloomberg that Apple is going to start funding original podcasts.
[02:02:24.320 --> 02:02:27.840]   And it's Mark Gurman who's very good.
[02:02:27.840 --> 02:02:29.680]   So I think it's probably accurate.
[02:02:29.680 --> 02:02:33.760]   That would be exclusive to Apple's podcast service.
[02:02:33.760 --> 02:02:37.760]   They have a podcast app now on both Macintosh and on phones.
[02:02:37.760 --> 02:02:44.720]   And this to me is exactly what I would expect from Apple.
[02:02:44.720 --> 02:02:45.920]   It's what Spotify is doing.
[02:02:45.920 --> 02:02:48.000]   They're treating podcasts like television shows.
[02:02:48.000 --> 02:02:53.840]   And it's because of serial and shows like that that have seasons and big rollouts.
[02:02:53.840 --> 02:02:56.480]   And I guess Apple will give them big budgets.
[02:02:56.480 --> 02:03:01.360]   And even though they're not video, that's kind of a television model.
[02:03:01.360 --> 02:03:04.080]   You know, it's a must see TV model.
[02:03:04.080 --> 02:03:04.400]   Oh, yeah.
[02:03:04.400 --> 02:03:06.080]   I'm watching serial.
[02:03:06.080 --> 02:03:08.080]   I have a got to see episode two, episode three.
[02:03:08.080 --> 02:03:08.880]   We're more.
[02:03:08.880 --> 02:03:10.640]   We're really our model is radio.
[02:03:10.640 --> 02:03:12.000]   This is like a radio show.
[02:03:12.000 --> 02:03:13.040]   It's on every week, right?
[02:03:13.040 --> 02:03:15.680]   And that's always been more my model.
[02:03:16.160 --> 02:03:19.200]   And I think almost all podcasts are on that radio model.
[02:03:19.200 --> 02:03:21.200]   Makes sense though that Apple given their background,
[02:03:21.200 --> 02:03:22.480]   they're going to treat this like Netflix,
[02:03:22.480 --> 02:03:24.560]   like they do their own television operations.
[02:03:24.560 --> 02:03:29.440]   We're going to give Sony $100,000 to produce a podcast.
[02:03:29.440 --> 02:03:33.120]   You know, and we'll tell them what it is, the James Corden show or whatever.
[02:03:33.120 --> 02:03:36.880]   So I expect that my biggest concern about Apple doing this,
[02:03:36.880 --> 02:03:41.920]   you might remember a year ago, Apple added podcast metrics.
[02:03:41.920 --> 02:03:43.600]   This is how it's going to tie into Nielsen.
[02:03:43.600 --> 02:03:46.000]   Added podcasts metrics to its podcast app.
[02:03:46.000 --> 02:03:49.360]   And so podcasters can get those metrics, but so can advertisers.
[02:03:49.360 --> 02:03:54.000]   The problem is it's limited to people who used Apple's podcast apps or iTunes to get podcasts.
[02:03:54.000 --> 02:03:56.960]   So and listen, by the way, you have to listen in the app.
[02:03:56.960 --> 02:04:01.360]   So it's kind of limited in its value to us because that isn't even a majority of our listeners.
[02:04:01.360 --> 02:04:06.480]   If Apple decides to like really double down on this, advertisers
[02:04:06.480 --> 02:04:09.920]   could put pressure on us to use those metrics.
[02:04:09.920 --> 02:04:11.840]   And that's kind of the answer to the second story,
[02:04:11.840 --> 02:04:15.200]   which is that Nielsen, which is the big television ratings company.
[02:04:15.200 --> 02:04:19.280]   And with Scarborough, they do radio ratings as well.
[02:04:19.280 --> 02:04:24.720]   They're going to, it's really, you know, the only way I could measure how you listen to this show is
[02:04:24.720 --> 02:04:28.960]   if I had the app that you're listening to, then I would get that data back from the app.
[02:04:28.960 --> 02:04:34.320]   But our audience listens on a huge number of apps as I want them to, including Spotify and
[02:04:34.320 --> 02:04:37.360]   Apple's iTunes. I want you to listen wherever you want.
[02:04:37.360 --> 02:04:44.000]   And old fashioned RSS and download it on your computer or listen on Amazon Echo.
[02:04:44.000 --> 02:04:48.880]   I think increasingly Echo and Google Assistant listens are going to become a big part of our
[02:04:48.880 --> 02:04:55.600]   downloads because it's so easy. You can just say, you know, Echo play the play that this
[02:04:55.600 --> 02:05:00.960]   week in Google podcast and it'll play it. So Nielsen realizing that says, well, we're just going to
[02:05:00.960 --> 02:05:06.240]   call people. They already do this. This isn't, you know, a big addition.
[02:05:06.240 --> 02:05:08.960]   The problem is it's not like you're doing three networks.
[02:05:08.960 --> 02:05:13.680]   The sample could not possibly be large enough to find this show.
[02:05:13.680 --> 02:05:19.680]   Yeah. The odds of finding somebody who listens to this show through sampling.
[02:05:19.680 --> 02:05:22.720]   Well, and there will be huge pressure. There already is pressure on us.
[02:05:22.720 --> 02:05:28.960]   Nielsen has already kind of signed up iHeart podcast network, cadence 13, mid-roll,
[02:05:28.960 --> 02:05:35.440]   westwood one in Cabana. And by the way, all of those do their advertising. In fact,
[02:05:35.440 --> 02:05:42.400]   some of them are agencies. So it's going to put pressure on us from advertisers saying,
[02:05:42.400 --> 02:05:44.480]   well, mid-roll has that information. Why don't you?
[02:05:44.480 --> 02:05:53.120]   This is, you know what? This is part of a medium becoming mature, but it kind of squeezes
[02:05:53.120 --> 02:05:59.280]   people like us. The little guys and the new guys won't be seen. They'll not exist.
[02:05:59.280 --> 02:06:04.400]   Yeah. It's making it mainstream media. But that's what's happened to podcasting.
[02:06:05.200 --> 02:06:11.040]   It's no longer a hobbyist thing. You can still do it as a hobby as long as you don't need to
[02:06:11.040 --> 02:06:14.240]   make any money at it. But if you decided you wanted to make a business out of it,
[02:06:14.240 --> 02:06:20.320]   you're going to have to become a player. Right. That's sad. YouTube will continue.
[02:06:20.320 --> 02:06:24.880]   That's a good place for people to go. There will be places that people can make stuff.
[02:06:24.880 --> 02:06:30.480]   You know, Anchor, which was bought by Spotify, but Anchor.fm facilitates people making shows.
[02:06:31.040 --> 02:06:35.280]   Couldn't be easier. If you've got a smartphone, you can make a podcast. And Anchor gets
[02:06:35.280 --> 02:06:40.800]   numbers and they sell advertising for you and all that stuff too, so you can monetize it.
[02:06:40.800 --> 02:06:46.800]   It's just the way it's, you know, it's curious your thoughts. Yeah. You know,
[02:06:46.800 --> 02:06:52.960]   I have no idea what the future is going to go. We're going in August, we're going to the big
[02:06:52.960 --> 02:06:59.200]   podcast movement show, which is the last trade show for podcasts. It's in Orlando. Lisa and I
[02:06:59.200 --> 02:07:02.720]   are going to go. We're going to bring some of our staff. All the agencies we know are going to be
[02:07:02.720 --> 02:07:06.560]   there. I think it's going to be a very interesting time because on the one hand, it's time for a
[02:07:06.560 --> 02:07:12.480]   victory lap podcast now. They're hot, right? It's a victory lap for podcasting. This is a true
[02:07:12.480 --> 02:07:17.360]   medium. And frankly, I think is on part noticed, by the way, a couple of those companies I mentioned
[02:07:17.360 --> 02:07:24.320]   are radio companies. It is now radio companies, Spotify, Apple, they're all saying, oh, you know,
[02:07:24.320 --> 02:07:30.000]   this is the new medium. So that's really good in terms of the stature of podcasting.
[02:07:30.000 --> 02:07:36.720]   The downside is it also, you know, I don't know what the amateur podcast or I don't know what that
[02:07:36.720 --> 02:07:41.280]   what that's going to do to them. We've been very lucky. We've kind of found a way to survive
[02:07:41.280 --> 02:07:47.200]   over the last 15 years. It may be, you know, maybe that our path is to acquisition by one of
[02:07:47.200 --> 02:07:50.720]   these companies. I wouldn't be surprised if in the long run, that's what ends up happening. It's,
[02:07:50.720 --> 02:07:55.280]   you know, it's just, it's kind of what happens. I think you for one welcome your new master.
[02:07:55.280 --> 02:08:00.880]   Well, if I got it, I welcome by new corporate overlords. Yeah. But I need to work for a while.
[02:08:00.880 --> 02:08:06.960]   The whole reason I did. Twit 15 years ago, was so that I would be the corporate overlord. So I didn't
[02:08:06.960 --> 02:08:11.760]   have to, I didn't have to modify. And you can tell if you listen to any of our shows that go on and on
[02:08:11.760 --> 02:08:17.840]   and on. They're completely discursive. No corporate parent would have allowed us to spend half an
[02:08:17.840 --> 02:08:23.120]   hour talking about language changes at the beginning of the show. But a lot of toast or toast.
[02:08:23.120 --> 02:08:30.320]   So how to take angle toasterly has at least had a taking. That's a commerce opportunity. So there,
[02:08:30.320 --> 02:08:35.280]   there you go. You shouldn't have asked me that's slow to stand. All right. Hey, this was a lot of
[02:08:35.280 --> 02:08:41.280]   fun. Stacy's mad. Stacy. I'm not mad. I am getting hungry though. I'll be honest. I can tell.
[02:08:41.280 --> 02:08:52.720]   You see what I have here? Oh, Stacy's thing. Yes. This is. And tell us about this. This is what?
[02:08:52.720 --> 02:09:01.360]   These are the wise bulbs. These are a package of four bulbs made by. Okay. Now. See the lights
[02:09:01.360 --> 02:09:08.960]   above me and these lights here now you say out loud as loud as you can. Okay, Google turn off wise.
[02:09:09.760 --> 02:09:15.120]   Are they already off or are you off? Turn them off. Okay.
[02:09:15.120 --> 02:09:23.760]   Okay. You want them on or off? They're off now. You can turn off now. Do you want to turn on
[02:09:23.760 --> 02:09:30.240]   the lights or do you want them wise? Turn on the lights. Okay. Okay, Google turn on the lights.
[02:09:33.440 --> 02:09:39.600]   Okay. Turning on three lights. Yay. That was a long way to go for nothing.
[02:09:39.600 --> 02:09:49.440]   Okay, Google set the lights to warm. Sorry. I don't understand. Okay, Google make the lights cool.
[02:09:49.440 --> 02:09:56.720]   Sorry. I'm not sure how to help with that. She's so nice. What am I saying wrong, Carsten?
[02:09:56.720 --> 02:10:01.680]   Um, I don't remember what the syntax was. Anyway, you get the idea. You, because these are white,
[02:10:01.680 --> 02:10:05.200]   right? These are like hues, but they're not colored, right? Stacey, the wise bulbs.
[02:10:05.200 --> 02:10:11.840]   Right. They're tunable and they're dimmable. So, uh, tunable means they can turn from, I think
[02:10:11.840 --> 02:10:19.600]   it's 27 to 65, 2700 to 6500 Kelvin in the color temperature. Yeah. So super warm to super cool.
[02:10:19.600 --> 02:10:26.880]   And then they are, uh, dimmable, but they do not work in a dimmer switch. So do not put them on a
[02:10:26.880 --> 02:10:34.960]   dimmer switch, but you can dim them from the app or from Google or Madam A. And the catch here is
[02:10:34.960 --> 02:10:43.920]   that these are four Wi-Fi bulbs that you can buy for 29.99. Oh, what a deal. After shipping is like
[02:10:43.920 --> 02:10:51.440]   41 shipping in taxes, like 4170 or something. So, but that's, so, I mean, that's not, there's
[02:10:51.440 --> 02:10:56.960]   cheaper LED bulbs, but they're cheaper than the hue, right? They're cheaper than any Wi-Fi.
[02:10:56.960 --> 02:11:03.920]   So any Wi-Fi bulb that is both tunable and dimmable and just white, that is the cheapest price. I've
[02:11:03.920 --> 02:11:11.680]   seen. Okay. Here we go. Okay, Google set lights to cool white. You got it changing three lights to
[02:11:11.680 --> 02:11:18.000]   cool white. They look exactly the same, but trust me, they're not here. Let's see if you can tell
[02:11:18.000 --> 02:11:23.680]   the difference because I'll, I'll turn mine to, uh, let's see. Your camera might sense it better.
[02:11:23.680 --> 02:11:27.360]   We have so many lights in here that overwhelm these that are not going to look any different.
[02:11:27.360 --> 02:11:35.200]   Okay. This is the coolest setting. I'm getting more. I can tell you looking at the light,
[02:11:35.200 --> 02:11:40.560]   it's definitely noticeable. So cool is like daylight and warm is like sunset.
[02:11:40.560 --> 02:11:46.320]   And so they're like, if you wanted to wake up or pay attention, you'd set it to a cooler temperature,
[02:11:46.320 --> 02:11:50.000]   a bluer temperature. And if you want it to be more restful, you can you dim these?
[02:11:50.000 --> 02:11:56.400]   You can see, I just turned mine to the oranges setting. Okay. Here, I will dim it. Here I am. This
[02:11:56.400 --> 02:12:03.840]   is a 50%. And then I could turn out all my lights. You should get these instead of the hue,
[02:12:03.840 --> 02:12:09.840]   not the, obviously the hue color, but the hue white bulbs. So here's my, here's my take on these.
[02:12:09.840 --> 02:12:15.120]   You should buy these if you want to play with smart lights. I actually just did a post in my
[02:12:15.120 --> 02:12:19.360]   newsletter last week about how I think, why is this smart company to watch?
[02:12:19.360 --> 02:12:24.960]   Smart home company to watch because I totally, I'm glitching all over. It's amazing.
[02:12:24.960 --> 02:12:30.480]   Because it's so cheap, you can play with this stuff without really risking a lot.
[02:12:30.480 --> 02:12:36.160]   The lights, you don't actually have to have the camera for it to work. If you do have the camera.
[02:12:36.160 --> 02:12:41.120]   Let me ask you that. Does this need a hub? Because why sells a hub? You don't need a
[02:12:41.120 --> 02:12:46.880]   camera in a bridge, but you don't need that for the lights. Oh, okay. So that's another reason to get
[02:12:46.880 --> 02:12:51.760]   it. You just put these in your bulbs and if then I have a wise app on my phone, I can control
[02:12:51.760 --> 02:12:57.760]   them or I can use it says it works with both Google and Amazon Zeko. It does and it works with
[02:12:57.760 --> 02:13:04.240]   IFTT. We'll, so one of the things I like about the hue bulbs, once I have the hue hub on there,
[02:13:04.240 --> 02:13:08.960]   is I can tell Amazon to look for the hue hub. And if they're all on the same network, it'll see it.
[02:13:08.960 --> 02:13:13.840]   And then I'll be able to control it from that point on. How do I get Amazon or Google to find
[02:13:13.840 --> 02:13:19.680]   these? Does it just see them? It just, when you link it, you set it up so you link the accounts.
[02:13:19.680 --> 02:13:24.240]   And then with Google, it just found them with Amazon. I did a device discovery.
[02:13:24.240 --> 02:13:28.880]   Yeah. So you have to tell a search. And I have controls here, which is kind of nice.
[02:13:28.880 --> 02:13:33.840]   I can turn them on and off from the app. And would that be the case over the internet?
[02:13:33.840 --> 02:13:37.680]   Or do I have to be on the same network as the bulbs to turn them on and off?
[02:13:37.680 --> 02:13:41.760]   No, you can remotely control them because they are, again, Wi-Fi.
[02:13:41.760 --> 02:13:44.960]   Nice. So yay. And then let's see if this, I set mine up.
[02:13:44.960 --> 02:13:49.680]   I'm going to, I got, I think, wisest. Oh, no, that's the right sensor. I was showing you what
[02:13:49.680 --> 02:13:53.040]   happened with the sensor. Oh, it has a sensor because I love that with my Philips.
[02:13:53.040 --> 02:14:00.480]   So I bought the Wysense kit and I'm using it open and close right now. And that's 20 bucks.
[02:14:00.480 --> 02:14:06.000]   It's 1999. For that, you do need the camera. But here it is. Ready? This is how fast it works.
[02:14:06.880 --> 02:14:15.120]   This is me. Oh, this is really slow right now. Oh, oh, it was so slow that time. Wow.
[02:14:15.120 --> 02:14:16.640]   But normally it's pretty quick.
[02:14:16.640 --> 02:14:22.160]   Normally it's faster, but that was like a good, what, seven seconds of latency, five seconds of
[02:14:22.160 --> 02:14:29.680]   latency. I didn't count. So this is cool. So Stacey's review is on the web at staceyoniot.com.
[02:14:29.680 --> 02:14:35.280]   I think, honestly, I think I'm going to stock up on these. I think that's, that's really great.
[02:14:36.240 --> 02:14:42.160]   Um, the, there are a couple things. One, the scheduling is a mess.
[02:14:42.160 --> 02:14:46.800]   There, the scheduling is not where you expect it to be. So if you do buy these and you want
[02:14:46.800 --> 02:14:51.840]   to schedule them for a time, know that you have to go to the shortcuts, not the bulb itself.
[02:14:51.840 --> 02:14:54.880]   Okay. The bulb itself has weird timing. I don't know why they did it.
[02:14:54.880 --> 02:14:59.600]   They'll fix all that. Wyses, but this is, this is Wyses story, which is they get this stuff
[02:14:59.600 --> 02:15:04.400]   out really cheap and then they've incrementally improve it. They just added people recognition of
[02:15:04.400 --> 02:15:10.640]   their $20 cameras. I know what the company called X and R dot AI. And I think I may have introduced
[02:15:10.640 --> 02:15:16.000]   them, but I'm not 100% sure. You should get a finder's fee. No, no, I'm just happy that that's
[02:15:16.000 --> 02:15:21.680]   a, I'm, I, this is a Seattle company, right? Uh huh. Both of them are. I, I feel like they've
[02:15:21.680 --> 02:15:28.080]   really done a great job of making this stuff very, very accessible, at least price wise.
[02:15:28.080 --> 02:15:32.160]   And the, and the features are great. And the cameras, there's no subscription fee.
[02:15:32.800 --> 02:15:36.320]   So this is, these are, these are really desirable, I think products.
[02:15:36.320 --> 02:15:41.200]   And record locally if you want, if you, if you want to, you can actually not have it,
[02:15:41.200 --> 02:15:45.440]   like the sensors won't work without the internet and neither would these bulbs,
[02:15:45.440 --> 02:15:48.880]   they won't work remotely or there'd be no point in having them. But the camera,
[02:15:48.880 --> 02:15:52.160]   you can pop an SD card in and you don't have to have anything go to the internet.
[02:15:52.160 --> 02:15:57.760]   Yeah. I love that. Uh, it does not use five gigs. It uses 2.4 only.
[02:15:59.680 --> 02:16:04.560]   And then it has a proprietary protocol for the sensors. Oh, they're not using a standard.
[02:16:04.560 --> 02:16:11.920]   It's a sub gigahertz thing. They're doing it because range and power consumption. Okay.
[02:16:11.920 --> 02:16:16.720]   But at that price, you can, somebody you can play with, you know what I'd love to see is LED
[02:16:16.720 --> 02:16:22.480]   strips and stuff like that, the additional accessories. I would like to see an outdoor camera.
[02:16:22.480 --> 02:16:26.960]   Outdoor camera, that's just right. I just bought an outdoor housing for my camera.
[02:16:26.960 --> 02:16:32.960]   So we'll see their Home Depot just put a wise smart plug to for 20 bucks on their website.
[02:16:32.960 --> 02:16:37.280]   Oh man. That's really, as a possible product leak.
[02:16:37.280 --> 02:16:44.240]   Pretty good. So thank you, Stacy. And thanks to Anthony Nielsen, our producer who
[02:16:44.240 --> 02:16:47.920]   set this up so we could show you in studio what you're doing.
[02:16:47.920 --> 02:16:49.440]   And I screwed it up. I'm sorry.
[02:16:49.440 --> 02:16:53.680]   Oh no, you did great. I think these are really cool. And you know,
[02:16:53.680 --> 02:16:57.920]   LED bulbs, these are these last forever, right? I mean, they're really good.
[02:16:57.920 --> 02:17:01.680]   Should last a really long time. Some of them do. Some of them don't.
[02:17:01.680 --> 02:17:05.600]   You know, it's funny. Sometimes they do burn out. Every time I change a light bulb in the house,
[02:17:05.600 --> 02:17:09.360]   I tell Lisa, well, this is the last time I'll be changing this one.
[02:17:09.360 --> 02:17:15.520]   My biggest issue is my radios fail more often than the light bulb.
[02:17:15.520 --> 02:17:18.800]   Oh, interesting. Maybe that's what's going on. Yeah.
[02:17:20.160 --> 02:17:27.680]   Jeff Jarvis, a number of the week. Hold on. I'm just two of them here. Google made 3,200
[02:17:27.680 --> 02:17:35.360]   changes to search last year. Wow. This from search engine land. Always a great resource.
[02:17:35.360 --> 02:17:40.560]   Oh, yes. Versus in 2010, they made 350 to 400 changes.
[02:17:40.560 --> 02:17:43.600]   All right. Back to the days of Matt cuts.
[02:17:43.600 --> 02:17:47.360]   This was a big one compared to so it's just all kinds of things that they do.
[02:17:47.360 --> 02:17:52.640]   And again, this goes to the notion that show us your algorithm. Well, okay. You want all the
[02:17:52.640 --> 02:17:54.960]   additions to the APIs. I'll ask.
[02:17:54.960 --> 02:18:04.400]   My pick of the week I already told you about, which is start page, which gives you Google results
[02:18:04.400 --> 02:18:09.280]   without giving Google any information. And it's easy to install if you're using Firefox.
[02:18:09.280 --> 02:18:15.280]   I've moved from Brave because I wanted a private browser, but I think Firefox is really on the
[02:18:15.280 --> 02:18:21.840]   forefront now of this, including the ability to use start page in the search bar as a search
[02:18:21.840 --> 02:18:26.240]   default search engine. It's very easy to configure. If you go to start page.com and you've got
[02:18:26.240 --> 02:18:30.320]   Firefox. And there's one other thing I want to mention. Steve Gibson has been talking a lot about
[02:18:30.320 --> 02:18:37.440]   DNS over HTTPS. It's at the very bottom of the network settings and the general settings of
[02:18:37.440 --> 02:18:45.120]   Firefox. If you enable DNS over HTTPS and use CloudFlare, what will happen is it'll
[02:18:45.120 --> 02:18:53.280]   use CloudFlare's DNS, not your ISP's DNS. Your ISP, every time you use, go out and look up a site,
[02:18:53.280 --> 02:18:57.840]   your ISP gets the information about what sites you visit. And as many ISPs sell that information
[02:18:57.840 --> 02:19:02.400]   to marketers and other people, and maybe that's something they don't deserve. So they don't like
[02:19:02.400 --> 02:19:09.120]   it. In fact, the British Internet Society or whatever said that Mozilla was the enemy of the year,
[02:19:09.120 --> 02:19:15.760]   or they nominated them because they were enabling DNS over HTTPS secure DNS,
[02:19:15.760 --> 02:19:20.720]   because it would make it hard for the British authorities to filter sites. Another good reason
[02:19:20.720 --> 02:19:27.360]   to use it. I am starting to really feel like Mozilla with its sync, with its features, with its
[02:19:27.360 --> 02:19:34.240]   privacy is my choice for a browser. If you want to block trackers, cookies, crypto miners,
[02:19:34.240 --> 02:19:39.920]   and fingerprinters, all the features that a privacy advocate would want are in here.
[02:19:39.920 --> 02:19:45.840]   And I'm starting to think with Brave, with its cryptocurrency and some of the things it's doing,
[02:19:45.840 --> 02:19:50.080]   maybe I maybe want to go with somebody who really has no axe to grind. And that is, of course,
[02:19:50.080 --> 02:19:57.120]   Firefox. So Firefox plus start page, that's my new default for searching the interface. Start page,
[02:19:58.320 --> 02:20:05.680]   searching the internet, start page.com. Thank you, Stacy. Go eat. Thank you, Jeff. Go do whatever
[02:20:05.680 --> 02:20:11.360]   it is people of your age do. I think I'm sorry we missed the early bird dinner at Denny's, but
[02:20:11.360 --> 02:20:18.960]   you're so cruel. So I'm like, so many years behind him, I can do that. It's not what you talk about
[02:20:18.960 --> 02:20:27.040]   today. And I asked for the senior discount. But do show that picture one more time of the age.
[02:20:27.040 --> 02:20:31.360]   Oh, thanks a lot. No, I love you, Jeff. You know that.
[02:20:31.360 --> 02:20:36.400]   Wise and Jeff. Happy birthday, Jeff. You're getting younger now, by the way. It starts
[02:20:36.400 --> 02:20:42.400]   counting backwards now. Jeff Jarvis is a professor at journalism at CUNY. He writes at buzzmachine.com.
[02:20:42.400 --> 02:20:47.280]   He's on the Twitter at Jeff Jarvis. And he's a great guy. If you ever get a chance to say hi to
[02:20:47.280 --> 02:20:52.800]   him in an airport, please do. You do because people after they say, "Hey, I just saw Jeff
[02:20:52.800 --> 02:20:55.520]   Jarvis. Why don't you say hi? Say hi." Yeah. It's okay.
[02:20:55.520 --> 02:21:00.240]   Like say hi. Well, if you're thrilled, say hi. He'll punch you in the nose. So it's perfect.
[02:21:00.240 --> 02:21:08.000]   Former TV guide critic and our man about town, Mr. Jeff Jarvis, and in English, such an inspiration.
[02:21:08.000 --> 02:21:12.320]   Thank you for dryers English. That's so good. And I'm really good. Tell me that other book again.
[02:21:12.320 --> 02:21:16.160]   I really want to read it. Oh, hell. I forget it. Sorry. I'll listen back to the show.
[02:21:16.160 --> 02:21:23.760]   I was like, I want to read, oh, hell. John McWhorter. Okay. MCWHORTER.
[02:21:23.760 --> 02:21:30.320]   Excellent. I love stuff about this like this. He's an American academic in linguist at Columbia.
[02:21:30.320 --> 02:21:38.880]   Thank you, Stacy. We love you too. Stacy on IOT.com is our website, the IOT podcast with Kevin Tofel.
[02:21:38.880 --> 02:21:42.880]   And of course, Stacy's great newsletter, a free newsletter you must subscribe to.
[02:21:44.000 --> 02:21:52.000]   Always a pleasure to have you on the show. Please come back. We do this week in Google every
[02:21:52.000 --> 02:21:58.160]   Wednesday, 130 Pacific, 430 Eastern, 2030 UTC. You can watch live. Actually, you can join us live.
[02:21:58.160 --> 02:22:02.000]   We had some nice members of the studio audience visiting today. All you have to do is email
[02:22:02.000 --> 02:22:09.440]   tickets at Twit.TV, right, Roberta? Tickets at Twit.TV. You don't have to wear a Twit hat,
[02:22:09.440 --> 02:22:13.600]   but you could wear a Marshall McLuhan rebels t-shirt. That would be welcome.
[02:22:14.240 --> 02:22:18.960]   If you want to watch live in our stream, we have a stream for you, Twit.TV/live, audio and video.
[02:22:18.960 --> 02:22:22.560]   If you do that, the chat room is a place to be hanging out with other people watching live at
[02:22:22.560 --> 02:22:30.800]   irc.twit.tv. On demand versions of the show, still available. No salesman will call from Nielsen.
[02:22:30.800 --> 02:22:37.440]   Just go to twit.tv/twig or fire up your favorite podcast application and subscribe. I don't care
[02:22:37.440 --> 02:22:41.920]   which one you use. We're not spying on you. We just want you to get it. The minute it's available.
[02:22:41.920 --> 02:22:55.360]   Thanks for listening. We'll see you next time on This Week in Google. Bye.
[02:22:55.360 --> 02:22:57.380]   (sighs)

