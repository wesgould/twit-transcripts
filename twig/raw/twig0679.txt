;FFMETADATA1
title=Conglomeratized!
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt, Mike Masnick
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-09-01
track=679
language=English
genre=Podcast
comment=Age Appropriate Design, Snap layoffs, Twitter Circle, Animal Translators
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.600]   It's time for Twig this week in Google Jeff Jarvis is here Stacey Higginbotham and
[00:00:05.600 --> 00:00:09.760]   Pruitt also Mike Masnick of Techdert. He's going to join us to talk about three
[00:00:09.760 --> 00:00:14.420]   new California laws that are just awful for the internet. We'll talk about why
[00:00:14.420 --> 00:00:22.680]   exercise bicycle usage is down 23% this year. The voice of New Jersey's Twitter
[00:00:22.680 --> 00:00:28.680]   is moving to Washington DC, the White House to be exact, and how you too can
[00:00:28.680 --> 00:00:34.560]   talk to naked mole rats. It's all coming up next on Twig.
[00:00:34.560 --> 00:00:49.560]   Podcasts you love from people you trust. This is Twig. This week in Google
[00:00:49.560 --> 00:00:58.800]   Episode 679 recorded Wednesday August 31st 2022, conglomerateized. This week in
[00:00:58.800 --> 00:01:05.320]   Google is brought to you by HPE GreenLake orchestrated by the experts at CDW who
[00:01:05.320 --> 00:01:10.240]   can help you consolidate and manage all your data in one flexible edge to cloud
[00:01:10.240 --> 00:01:19.760]   platform to scale and innovate. Learn more at CDW.com/HPE and by ClickUp the
[00:01:19.760 --> 00:01:25.560]   productivity platform that will save you one day a week on work guaranteed. Use
[00:01:25.560 --> 00:01:31.440]   the code Twig to get 15% off ClickUp's massive unlimited plan for a year meaning
[00:01:31.440 --> 00:01:35.600]   you could start reclaiming your time for under $5 a month. Sign up today at
[00:01:35.600 --> 00:01:42.960]   ClickUp.com. Hurry this offer in soon. And by New Relic use the data platform
[00:01:42.960 --> 00:01:46.680]   made for the curious. Right now you can get access to the whole New Relic
[00:01:46.680 --> 00:01:51.320]   platform and 100 gigabytes of data per month free forever. No credit card
[00:01:51.320 --> 00:01:59.720]   required. Sign up at New Relic.com/Twig. It's time for Twig this week in Google
[00:01:59.720 --> 00:02:03.280]   the show we cover the latest from the Googleverse. There's some Twitter news
[00:02:03.280 --> 00:02:08.600]   this week. There is some Facebook news this week but the top story this week
[00:02:08.600 --> 00:02:12.960]   comes from California. First let me introduce our panel of course Stacy Higginbotham
[00:02:12.960 --> 00:02:24.400]   from StacyOnIOT.com at Gigastacy. Great to see you. Welcome. Thank you. Also with us it is
[00:02:24.400 --> 00:02:32.200]   Jeff Jarvis. He is the the the lettered top professor for journalistic innovation
[00:02:32.200 --> 00:02:38.600]   at the Craig Newmark Graduate School of Maryland. I'm at the City University of
[00:02:38.600 --> 00:02:44.800]   New York. Hello. Hello. Hello. Hello. Earlier we were we were trying to persuade
[00:02:44.800 --> 00:02:51.280]   Queen Pruitt to sing your intro as Sandy Cheeks but she wouldn't do it.
[00:02:51.280 --> 00:02:56.760]   Pruitt is here and maybe he'll do it. Hi Ann. That would be no sir. That'll be a
[00:02:56.760 --> 00:03:03.880]   no. Do you sing it all? That would be no sir. I hear you have a lot of sponge
[00:03:03.880 --> 00:03:12.440]   Bob around the house so. Congratulations. It never ends. It's musical this musical
[00:03:12.440 --> 00:03:18.720]   that every day. I said that our big news of the week comes from California and to
[00:03:18.720 --> 00:03:25.320]   bring us that news it's always great to welcome Mike Masnick Techdirt.com. Mike you
[00:03:25.320 --> 00:03:31.920]   have been on a tear this week before I'd say that talk about that though let me
[00:03:31.920 --> 00:03:37.440]   say as Stacy did before the show began happy 25th anniversary. Thank you.
[00:03:37.440 --> 00:03:43.340]   Unbelievable. We've been doing Techdirt for 25 years. That's longer than
[00:03:43.340 --> 00:03:51.580]   you quit. That's amazing. August of 97. Yeah. That's old. I'm very old. Even
[00:03:51.580 --> 00:03:55.980]   more tech TV practically before there was an internet. I mean 97. No. That's when the
[00:03:55.980 --> 00:04:00.820]   internet was still young and wonderful and fresh. Were you straight out of school when
[00:04:00.820 --> 00:04:05.940]   you started it or what? I started it when I was in school. I was I was in the process
[00:04:05.940 --> 00:04:10.940]   of graduating and I started it in order to try and help find a job. I didn't realize
[00:04:10.940 --> 00:04:16.540]   it would become my job. You made your job. Well done. Bravo. What did you major in Mike?
[00:04:16.540 --> 00:04:21.020]   Well that was I was I was in business school. I was getting an MBA at that point.
[00:04:21.020 --> 00:04:27.700]   Okay. So that's why you probably had to get a real job. I was looking for a real
[00:04:27.700 --> 00:04:32.220]   job and I was trying to show enthusiasm for the technology industry. You could
[00:04:32.220 --> 00:04:35.580]   have been working at McKinsey or something so it's good. You could you
[00:04:35.580 --> 00:04:42.660]   saved from that. Yeah. You could have made a lot of money. Oh man. I know. Don't remind
[00:04:42.660 --> 00:04:49.380]   me of that part. Yes. I definitely I definitely probably make significantly less than almost
[00:04:49.380 --> 00:04:55.580]   all of my classmates. So it's not that's not how we measure. You have more soul left.
[00:04:55.580 --> 00:05:03.060]   No. Not how we measure success in life. Congratulations. Tech Dirt is a must read. Even if you weren't
[00:05:03.060 --> 00:05:07.220]   here we would have I believe I counted five stories in our rundown. Oh I had them in there
[00:05:07.220 --> 00:05:12.140]   too. Yeah. Oh yeah. Yeah. But the big story comes from Monday night when the California
[00:05:12.140 --> 00:05:21.380]   Senate passed three horrible bills. And now the only question is will the governor make
[00:05:21.380 --> 00:05:30.060]   them law. Let's start with AB 2273 which passed 33 to nothing in the Senate. It had already
[00:05:30.060 --> 00:05:35.700]   gotten through the house. The age appropriate desire or the assembly as they call it here.
[00:05:35.700 --> 00:05:42.180]   The age appropriate design code. Yeah. Okay. The idea is I think laudable and that's probably
[00:05:42.180 --> 00:05:46.100]   why it passed 33 to not. No it's not. No it's not. No it's not. Protect the children. You
[00:05:46.100 --> 00:05:50.460]   don't believe in protecting the children. Let's explain what it is before we moral
[00:05:50.460 --> 00:06:00.220]   panics. Sure. Sure. So I would say the concept behind it is laudable. The idea that you know
[00:06:00.220 --> 00:06:04.660]   the internet in some ways there are children on there and occasionally children can wander
[00:06:04.660 --> 00:06:10.340]   into areas that are not safe for them. And that is a legitimate concern right in terms
[00:06:10.340 --> 00:06:17.380]   of in terms of how children and the internet interact. Now in the US we have federal law
[00:06:17.380 --> 00:06:22.540]   that tries to deal with some of that right. We have COPPA the what is it children's online
[00:06:22.540 --> 00:06:27.820]   privacy protection act which has some rules for sites that are directed at those under
[00:06:27.820 --> 00:06:34.420]   13. What California is doing is really sort of trying to completely change the way the
[00:06:34.420 --> 00:06:39.980]   internet works. And you know I think effectively they're sort of trying to turn into Disneyland
[00:06:39.980 --> 00:06:47.020]   where everything has to be appropriate for children at all times. That's not quite what
[00:06:47.020 --> 00:06:51.700]   the bill says and there's some some language in there that makes it pretend that it does
[00:06:51.700 --> 00:06:55.500]   not apply so broadly but it but it really does. If you want I can start to go through
[00:06:55.500 --> 00:07:01.340]   the list of problems this so long it would take up more than your entire show. Let me
[00:07:01.340 --> 00:07:10.820]   show the short version that Assemblywoman Buffy Wicks published and my college classmate
[00:07:10.820 --> 00:07:16.980]   Roger McNamee endorsed. He said the big platforms are using disinformation to try to stop
[00:07:16.980 --> 00:07:25.340]   it. The California age appropriate design codes how it works companies must stop selling
[00:07:25.340 --> 00:07:32.460]   kids personal information. Okay that's fair COPPA does say that as well right. Set all
[00:07:32.460 --> 00:07:37.420]   default settings to the most private. That sounds all right except it's not just sites
[00:07:37.420 --> 00:07:46.500]   meant to be for 18 year under 18s by the way kids is under 18 not under 13 as in COPPA.
[00:07:46.500 --> 00:07:50.940]   It means all sites that an 18 year old might visit including mine which is everything including
[00:07:50.940 --> 00:07:58.580]   our IRC including our forums including Tech Dirt dot com. Design kids experience based
[00:07:58.580 --> 00:08:06.340]   on their estimated age. Wait a minute estimated age there's already something interesting.
[00:08:06.340 --> 00:08:14.380]   We're supposed to estimate how old the person is visiting our site. Yes with near certainty
[00:08:14.380 --> 00:08:18.580]   there are some requirements that you have to estimate the age of people who are visiting
[00:08:18.580 --> 00:08:24.100]   your site and it has to be fairly accurate under the law which the only way you can do
[00:08:24.100 --> 00:08:29.740]   that is with age verification. I can ask them but then they might lie as they do as one
[00:08:29.740 --> 00:08:34.980]   does. As one does. Right well we already have with COPPA right. I mean so there are lots
[00:08:34.980 --> 00:08:40.580]   of sites that say you can't access it under 13 and that's specifically because of the
[00:08:40.580 --> 00:08:44.620]   rules of COPPA which is for sites targeting those under the age of 13 and the way we've
[00:08:44.620 --> 00:08:49.300]   dealt with that is that basically every site says no one under 13 is supposed to use this
[00:08:49.300 --> 00:08:54.260]   site and then what happens is every kid lies and every parent teaches their kid to lie
[00:08:54.260 --> 00:08:59.180]   and you put in a date that makes them over 13 and I don't know that it's necessarily
[00:08:59.180 --> 00:09:02.060]   a good idea that we're teaching. Well maybe it depends on how you look at it. It might
[00:09:02.060 --> 00:09:06.980]   be a good idea that we're teaching kids to lie about who they are on the internet. That's
[00:09:06.980 --> 00:09:12.420]   one of those perverse consequences many of us kind of have but would it be enough for
[00:09:12.420 --> 00:09:16.860]   me just to put up a sign everywhere we are saying you must be 18. I hate to do this because
[00:09:16.860 --> 00:09:21.580]   we have a lot of smart high school kids who listen to our stuff. You must be 18 to visit
[00:09:21.580 --> 00:09:27.180]   this site. That will not work under this law because the law is not for sites that are
[00:09:27.180 --> 00:09:33.860]   targeted at people under the age of 18. It's likely to be accessed by someone under the
[00:09:33.860 --> 00:09:40.220]   age of 18. Again there's multiple problems there immediately. So my.
[00:09:40.220 --> 00:09:44.980]   This is a measurement cause is what you're saying. It's just judgment cause. It's not
[00:09:44.980 --> 00:09:51.640]   even judgment's cause. If you make a mistake on that then you're in trouble. So likely
[00:09:51.640 --> 00:09:57.180]   to be accessed by, if you think high school students might access your website then suddenly
[00:09:57.180 --> 00:10:01.220]   you have to be concerned and you probably have to do something to try and get in compliance
[00:10:01.220 --> 00:10:06.660]   with this law. Now you said well this means that we're going to have to do face recognition
[00:10:06.660 --> 00:10:13.060]   to determine whether you're 18. That's not what I said. That's not what I said.
[00:10:13.060 --> 00:10:21.580]   But that is what this group the age verification association I forget providers association
[00:10:21.580 --> 00:10:26.940]   told me right. So you know I started to look into it because because of the requirement
[00:10:26.940 --> 00:10:31.620]   that you have to accurately estimate the age of people on your site. You know the only
[00:10:31.620 --> 00:10:36.780]   way to do that is with different age verification tools. Now there is a company that has a near
[00:10:36.780 --> 00:10:42.420]   monopoly on the age verification tools and that is MindGeek which is the parent company
[00:10:42.420 --> 00:10:47.580]   of Pornhub. So they're very good at this by the way.
[00:10:47.580 --> 00:10:55.260]   So one solution here is that every website is going to have to give data including like
[00:10:55.260 --> 00:11:01.540]   you know login data or facial recognition data to the company behind Pornhub. I don't
[00:11:01.540 --> 00:11:12.380]   quite see how that helps the privacy of children across California. Now so this group this age
[00:11:12.380 --> 00:11:17.820]   verification trade association reached out to me to say oh no no no don't worry about
[00:11:17.820 --> 00:11:24.580]   that. We have a better solution than the Pornhub solution. Oh thank God. It just involves when
[00:11:24.580 --> 00:11:31.460]   you visit a website we will need to scan your face and use our special AI to determine
[00:11:31.460 --> 00:11:36.660]   whether you are 17 or 18 years old and you know and then we'll decide whether or not
[00:11:36.660 --> 00:11:41.700]   you can visit these websites. So every website in according to this like these people who
[00:11:41.700 --> 00:11:47.060]   are providing the tools and who are supporting this law all every website has to do is install
[00:11:47.060 --> 00:11:53.100]   that these companies systems to scan faces use AI which of course never makes mistakes
[00:11:53.100 --> 00:11:58.580]   and can't be tricked and it'll be fine. So people point out like well you know they're
[00:11:58.580 --> 00:12:01.820]   going to be ways around that everyone's going to be holding up you know that picture of Jeff
[00:12:01.820 --> 00:12:11.180]   Jarvis that you have on the desk for every baby face. Hey under 18. Yeah. And so they
[00:12:11.180 --> 00:12:17.020]   said don't worry about that we have a solution to that. If it's if they're unsure they may
[00:12:17.020 --> 00:12:22.100]   have to do a live miss test in which they will make you record a video in which you have
[00:12:22.100 --> 00:12:27.260]   to say phrases that they will present to you to prove that you're a live human being there.
[00:12:27.260 --> 00:12:32.100]   That's like it's like a hostage video just to visit websites in California or you could
[00:12:32.100 --> 00:12:37.460]   do what they propose in the UK and just go to a local pub and say here's my driver's license
[00:12:37.460 --> 00:12:44.140]   I'm under 18 am I okay. It's over eight. Oh either way we got to know I don't care what age
[00:12:44.140 --> 00:12:48.740]   you are I need to know your exact age so everybody must go to the bar. Yeah everybody must go to
[00:12:48.740 --> 00:12:56.700]   the pub. Wait so the goal of this is I guess I am a little unclear on I know they're trying
[00:12:56.700 --> 00:13:02.020]   to do this. Like the children. I get it. Okay I get protect the children but like is it to
[00:13:02.020 --> 00:13:06.340]   protect them from going to sites with objectionable stuff is it for us not to collect their data.
[00:13:06.340 --> 00:13:12.500]   Like I now I know the age of the person on my site. What is that right. So yeah that's that's
[00:13:12.500 --> 00:13:16.140]   a good question and it's amazing how much stuff you have to get to before you even get to kind
[00:13:16.140 --> 00:13:21.100]   of the meat of the law which is equally problematic. So basically if your site is likely to be accessed
[00:13:21.100 --> 00:13:26.460]   by anyone under the age of 18 which is you know again almost every site then there are
[00:13:26.460 --> 00:13:30.540]   a bunch of requirements that you have and the biggest one and the most it's it's kind
[00:13:30.540 --> 00:13:38.020]   of ridiculous is that every feature for every feature on your website you have to create
[00:13:38.020 --> 00:13:44.980]   a cash I'm suddenly blanking on what the TP I a data protection impact assessment I think
[00:13:44.980 --> 00:13:51.980]   is yeah something like like the environmental impact report right for every single feature
[00:13:51.980 --> 00:13:56.580]   and how it affects kids right and there's a whole bunch of categories that of things
[00:13:56.580 --> 00:14:03.220]   that it must include in terms of you know the likelihood that a child will see harmful
[00:14:03.220 --> 00:14:09.020]   content on your website is one of those things. So you know and this raises questions too
[00:14:09.020 --> 00:14:13.780]   because harmful content lots of harmful content is clearly protected under the First Amendment
[00:14:13.780 --> 00:14:20.260]   you know there is perhaps strong language on some websites perhaps mine is that harmful
[00:14:20.260 --> 00:14:25.180]   harmful is not defined in the bill so we don't exactly know. Just like the UK just like the
[00:14:25.180 --> 00:14:30.780]   UK. Well in fact it is a UK Baroness that is that wrote this bill and it is used almost
[00:14:30.780 --> 00:14:36.140]   entirely whole cloth from Baroness is a nice touch Mike by the way a very nice touch.
[00:14:36.140 --> 00:14:43.340]   She directed one one the Bridget Jones diaries movie and then she says gave up her career
[00:14:43.340 --> 00:14:50.980]   in Hollywood to protect the children. Yeah she made a documentary about kids you know
[00:14:50.980 --> 00:14:57.900]   on their phones and decided that you know that documentary taught her about how evil
[00:14:57.900 --> 00:15:02.900]   the internet and phones were and so she's sort of dedicated her life now to destroying
[00:15:02.900 --> 00:15:08.260]   the internet. She's making a cheesy public service announcements like this play my sound.
[00:15:08.260 --> 00:15:14.940]   Oh I turned it off hold on. Wait a minute what are we tired of? I mean go back I gotta
[00:15:14.940 --> 00:15:19.620]   hear what you're tired of. Well people my generation have felt the negative consequences
[00:15:19.620 --> 00:15:28.900]   of being unregulated online. You mean like my chat room? Yeah. Right. Designed for teens.
[00:15:28.900 --> 00:15:34.420]   No wait a minute. Oh it's time for the internet to be designed with kids teens and young people
[00:15:34.420 --> 00:15:40.660]   in mind. Designed for teens. By the way that means my privacy policy has to be written
[00:15:40.660 --> 00:15:48.740]   for teens. Yes. That means I have to do a DPIA on every feature to see a conduct a risk
[00:15:48.740 --> 00:15:55.340]   assessment of how it uses kids data. Make it easy for kids to report privacy concerns.
[00:15:55.340 --> 00:15:59.300]   Let kids know when they're being monitored or tracked. Provide all privacy notices and
[00:15:59.300 --> 00:16:05.100]   clear note a clear language that young users can understand. How young? By the way how
[00:16:05.100 --> 00:16:08.340]   young is that? Actually that would be okay. But wait a minute. I think everybody would
[00:16:08.340 --> 00:16:15.900]   have five year olds. What? The how old is based on that because you're now tracking everyone's
[00:16:15.900 --> 00:16:20.700]   age and you will have a sense of what age groups are visiting your site. You have to
[00:16:20.700 --> 00:16:25.660]   direct it towards the youngest of that crew. So as you determine that information then
[00:16:25.660 --> 00:16:30.380]   you have to rewrite your policies. And Stacey's right. There are ideas in there that are
[00:16:30.380 --> 00:16:35.820]   good ideas in general that websites should do. The question is how do you enforcing that
[00:16:35.820 --> 00:16:45.660]   legally is creating a real difficulty under this law and there's a whole bunch of liability
[00:16:45.660 --> 00:16:49.980]   problems with the way it's structured. I think there are a lot of things that websites
[00:16:49.980 --> 00:16:55.060]   could do to be better about the fact that sometimes children access their websites.
[00:16:55.060 --> 00:17:02.020]   But this is really designed to effectively wipe out any website that that couldn't be
[00:17:02.020 --> 00:17:07.140]   visited by a child. It's really designed to make the entire web so that it's appropriate
[00:17:07.140 --> 00:17:13.020]   for children. And that takes away a lot of what is great about the internet.
[00:17:13.020 --> 00:17:18.460]   So when you complained about this they said, "Well don't worry because the California
[00:17:18.460 --> 00:17:26.100]   Attorney General decides when to enforce this." So they're not going to do it for you.
[00:17:26.100 --> 00:17:33.940]   I would hope not. But even that is just kind of a weird risk. I criticize the California
[00:17:33.940 --> 00:17:38.300]   Attorney General and lots of things. If I keep doing that then suddenly he become interested
[00:17:38.300 --> 00:17:44.060]   in researching this. And the way the law works is not only you have to create these
[00:17:44.060 --> 00:17:49.340]   DPIA's for every feature. So we have search. So I have to hire a privacy lawyer to do
[00:17:49.340 --> 00:17:54.860]   an impact assessment of the search on tech dirt, comments, sharing, all the different
[00:17:54.860 --> 00:17:59.460]   features that we have. Every single one has to have a DPIA written. And because there's
[00:17:59.460 --> 00:18:03.820]   legal liability associated with it you really have to hire a lawyer to do this. Then you
[00:18:03.820 --> 00:18:08.740]   have to store them. And at any point the Attorney General of California can demand that you
[00:18:08.740 --> 00:18:14.980]   produce them. And you have three business days. It was short of them that but they were nice.
[00:18:14.980 --> 00:18:17.940]   And they made it something that's 48 hours.
[00:18:17.940 --> 00:18:22.340]   It was originally 48 hours and now it's three business days. So you get a little bit of
[00:18:22.340 --> 00:18:29.660]   extra space. And you have to produce other states. You have to produce all of those DPIA's
[00:18:29.660 --> 00:18:35.460]   to the Attorney General. And with the DPIA's if you determine in any of those DPIA's that
[00:18:35.460 --> 00:18:41.580]   a child might come across harmful content or there are a few other categories of things,
[00:18:41.580 --> 00:18:46.540]   you'd have to come up with a, I think they say, timed plan to mitigate this before a child
[00:18:46.540 --> 00:18:53.100]   may access it. And so if I hire a lawyer to go through the website and say, well, if somebody
[00:18:53.100 --> 00:18:59.940]   does a search, if we're doing a control on a DPIA on our search function, and they may
[00:18:59.940 --> 00:19:04.940]   come across swear words. Is that harmful? Well, it depends on the age. I would think
[00:19:04.940 --> 00:19:09.700]   it might be harmful for some some children. They of course don't define harm. No, harm
[00:19:09.700 --> 00:19:14.940]   is not defined in the bill. So there's like fair use. Larry Lessick says it's right to
[00:19:14.940 --> 00:19:20.060]   hire an attorney. This is another this is right to hire. This is not even a right to hire.
[00:19:20.060 --> 00:19:24.580]   This is a requirement to hire an attorney because you have to go through all these things and
[00:19:24.580 --> 00:19:28.940]   you have to have them ready. Because at any moment the Attorney General could demand them
[00:19:28.940 --> 00:19:33.180]   and you have 72 hours to produce them for everything on your site, including the plan
[00:19:33.180 --> 00:19:41.980]   on how you will minimize or get rid of that potential harm. And so that's an impossible
[00:19:41.980 --> 00:19:45.420]   thing to comply with for most sites. I mean, I'm sure like, you know, the biggest sites
[00:19:45.420 --> 00:19:49.620]   in the world, I'm sure Google and Facebook and whatever will have plans to deal with this.
[00:19:49.620 --> 00:19:55.460]   But for everyone else, what are they going to do? So this is a company. Hold on just a
[00:19:55.460 --> 00:20:02.060]   second. Okay, go ahead. Okay, you go. Well, I'm just want to clarify. This says for companies
[00:20:02.060 --> 00:20:07.740]   with 25 or million more an annual gross revenue. So or buy or sell personal information of
[00:20:07.740 --> 00:20:13.660]   100,000 or more users or derive 50% of annual revenue for selling or sharing consumers personal
[00:20:13.660 --> 00:20:18.300]   information. I think that lets me off the hook. It lets you off the hook, doesn't it?
[00:20:18.300 --> 00:20:21.660]   Not quite. So I thought so too. And lots of people point us out because it's because
[00:20:21.660 --> 00:20:29.900]   the law. The law is included in the CPR, which is the California privacy regulation, whatever
[00:20:29.900 --> 00:20:35.740]   thing, the California privacy law that came about a few years ago. So it becomes a part
[00:20:35.740 --> 00:20:40.300]   of that. So it only applies to companies that are applied there. Now, so most of that seems
[00:20:40.300 --> 00:20:45.260]   to exclude you. The thing is you only have to qualify for one of those things, not all
[00:20:45.260 --> 00:20:49.100]   three of those things that you listed, the 25 million excludes lots of people. But that
[00:20:49.100 --> 00:20:56.300]   second one says if you if you buy, sell or share information on 100,000 users, again,
[00:20:56.300 --> 00:21:02.220]   I would think that should exclude you and I because in theory, I don't buy, sell or share,
[00:21:02.220 --> 00:21:08.140]   except when you start to look at the definition of buy, sell and share, which basically appears
[00:21:08.140 --> 00:21:12.940]   to include any sort of third party service that you have on your website.
[00:21:12.940 --> 00:21:18.780]   So I have advertising. If you have advertising on your website, you end more than 100,000
[00:21:18.780 --> 00:21:27.660]   annual users, then you buy, sell or share information on over 100,000 people and therefore you qualify
[00:21:27.660 --> 00:21:33.580]   under the law. So both you and I, congratulations, qualify under the law. Yes. All right, Stacy,
[00:21:33.580 --> 00:21:38.780]   your turn. Sorry. I just wanted to look at that and say, Oh, well, that's only for big companies.
[00:21:38.780 --> 00:21:45.580]   Can you bracket off your site? If you're doing the age verification, can you just say no 18 year olds
[00:21:45.580 --> 00:21:50.220]   and then they have to lie to get on? Because I could also see this kind of
[00:21:50.220 --> 00:21:54.860]   well, people do with not but it doesn't work this way. Right. So I know it doesn't work, but
[00:21:54.860 --> 00:22:00.540]   with with with copper, you could in theory get away with that with this because it's likely to
[00:22:00.540 --> 00:22:06.380]   be accessed by you putting a disclaimer is not enough. If they are still likely to visit your site,
[00:22:06.380 --> 00:22:12.620]   then you could be in trouble. No, it affects you too, probably Stacy. You have more than 100,000
[00:22:12.620 --> 00:22:18.860]   visitors all year, right? Oh, yeah. And you would sell advertising, yes. I do, but I don't sell
[00:22:18.860 --> 00:22:24.380]   any information on my I don't either. But apparently that doesn't matter. Well, the definition of sell
[00:22:24.380 --> 00:22:31.020]   is much more broad than the word sell implies. She's not in California, but she doesn't visit the
[00:22:31.020 --> 00:22:35.020]   state of California and get locked up as she goes. Wait a minute. She's fine. But what is her
[00:22:35.020 --> 00:22:39.340]   liability? She has users in California, right? I know that but I'm saying I'm asking how do they
[00:22:39.340 --> 00:22:45.180]   come after her? That I don't I don't know how it will directly apply to companies outside of
[00:22:45.180 --> 00:22:52.380]   California. But I don't know. I mean that you would have to hire a privacy layer to to figure it
[00:22:52.380 --> 00:22:56.540]   out. So that might be an out then it's might only affect California companies, which is you and me.
[00:22:56.540 --> 00:23:02.700]   Well, we are California companies or in theory companies that that have users in California.
[00:23:02.700 --> 00:23:07.740]   So in theory, Stacy, if you want to block away, block out all of your California users. And then
[00:23:07.740 --> 00:23:13.020]   we get back to the whole issue of sort of a much more fragmented internet. You know, there are already
[00:23:13.020 --> 00:23:17.660]   companies that don't allow people in the EU to visit it because they don't want to deal with the
[00:23:17.660 --> 00:23:22.540]   GDPR. But now imagine that you have you have this in California. And that's not even getting into
[00:23:22.540 --> 00:23:28.460]   like one, you know, assuming that this goes through and assuming that that nobody challenges it or
[00:23:28.460 --> 00:23:32.940]   that if it is challenged, somehow it's found to be constitutional. Now you have a blueprint for
[00:23:32.940 --> 00:23:37.820]   every other state to pass their laws. And some of those laws might be contradictory. And in fact,
[00:23:37.820 --> 00:23:43.420]   like right now there are state privacy laws like Illinois is a big one. They have a state privacy
[00:23:43.420 --> 00:23:49.980]   law that I think the age verification requirements of the California law probably violate they violate
[00:23:49.980 --> 00:23:56.460]   Illinois privacy regulation. So I don't know, you know, do we just start blocking states one by one?
[00:23:56.460 --> 00:24:02.140]   You know, I don't know. So there's also this attorney's general. So like,
[00:24:02.700 --> 00:24:09.500]   in Texas, harmful content could be, you know, a boy, LGBTQIA plus abortion or abortion information.
[00:24:09.500 --> 00:24:14.060]   Or gay information. So exactly. Right. So they can they can pass law. So other states will likely
[00:24:14.060 --> 00:24:19.500]   pass laws using nearly identical language. But because it's all within the determination of the
[00:24:19.500 --> 00:24:24.460]   attorney general, yes, you're right. So Texas could pass this exact law and determine that all sorts
[00:24:24.460 --> 00:24:29.660]   of content is harmful and go after websites for how could liberal content there? Literally.
[00:24:30.460 --> 00:24:35.820]   Yes. How close is this to the law that be been Baroness beeping Kidrin got passed in the UK?
[00:24:35.820 --> 00:24:42.380]   Because they have this law too. Right. So the UK design code, which sounds the same and has
[00:24:42.380 --> 00:24:47.580]   basically the same name is not the same. Oh, right. And there are some some key differences. The first
[00:24:47.580 --> 00:24:54.940]   thing is that it's not really a regulation by itself. It is basically instructions to the regulators
[00:24:54.940 --> 00:25:00.540]   of the UK data data protection. It's not off com. It's the the ICO.
[00:25:00.540 --> 00:25:07.820]   Because it's harmful stuff is going to be off com, right? This. Yes. But that's separate.
[00:25:07.820 --> 00:25:11.100]   Well, let me get to that in a second. That's important. Yes.
[00:25:11.100 --> 00:25:17.900]   Jump to head. So the UK, you know, when they were still part of the EU,
[00:25:17.900 --> 00:25:23.420]   they put in place laws to match the GDPR. So they have their own sort of version of the GDPR.
[00:25:23.420 --> 00:25:28.620]   Now that they've, you know, cleave themselves away from the EU, they still have their own data
[00:25:28.620 --> 00:25:37.820]   protection regime. The design code rules are basically an instruction for the regulators of
[00:25:37.820 --> 00:25:45.420]   the data protection on how to how to apply it. And what's interesting is that the Baroness,
[00:25:45.420 --> 00:25:50.940]   her organization has submitted, I think there was something like 250 complaints about websites in
[00:25:50.940 --> 00:25:56.860]   the UK that they say are not complying with the design code. So far, as of like a week and a half
[00:25:56.860 --> 00:26:01.420]   ago, which was the last that there was a report on this, the regulator has gone after zero of
[00:26:01.420 --> 00:26:07.660]   those companies. However, so that there's been no enforcement, which is again, it's entirely up
[00:26:07.660 --> 00:26:13.660]   to the regulator. They have the data protection regulator in the UK. However, every time that
[00:26:13.660 --> 00:26:21.580]   any website makes any change these days to, you know, have better privacy rules or better
[00:26:21.580 --> 00:26:27.020]   functions for teenagers and children, her organization takes credit for it and claims they're only
[00:26:27.020 --> 00:26:32.860]   doing it because of the design. Just like your classmate, just like the parents teacher council,
[00:26:32.860 --> 00:26:38.300]   just like all these moral entrepreneurs, as they're called, who make a living out of doing this.
[00:26:38.300 --> 00:26:44.780]   The irony is, by the way, Mike, Mike, this is enough to make me want to go just drink
[00:26:44.780 --> 00:26:49.180]   hemlock and go off the edge of the earth. This is only the first three bad laws.
[00:26:49.180 --> 00:26:51.980]   Mike talked about it. I know we only have half an hour of questions.
[00:26:51.980 --> 00:26:53.260]   This is just the first one.
[00:26:53.260 --> 00:27:00.940]   This is the worst one, I'll say. But the thing that I'll add is, Jeff, you brought up the
[00:27:00.940 --> 00:27:06.140]   online safety bill, which is the other bill that is now making its way through the UK.
[00:27:06.140 --> 00:27:14.220]   And the same person, Beeban Kidron, who basically helped write the design code in both the UK
[00:27:14.220 --> 00:27:18.460]   and in California, has now said that that is her focus is getting that law passed and that will
[00:27:18.460 --> 00:27:24.460]   create a whole bunch of other ridiculous problems. And it appears that her goal is to export it.
[00:27:24.460 --> 00:27:30.220]   And in fact, I would argue that the California version of the design code is actually much closer
[00:27:30.220 --> 00:27:37.580]   to the online safety bill in the UK than it is to the actual design code, the age-appropriate
[00:27:37.580 --> 00:27:42.700]   design code in the UK, because there's actually an enforcement mechanism in the California one.
[00:27:42.700 --> 00:27:47.660]   It's not just up to the regulator to decide, but here it actually goes to the attorney general,
[00:27:47.660 --> 00:27:53.260]   and they have the ability to find companies for a variety of different failures under this bill.
[00:27:54.780 --> 00:28:00.140]   It still has to be, hold on, let me, I got to move it along. This bill has to still be
[00:28:00.140 --> 00:28:04.940]   assigned by Governor Newsom. Seems likely he will since he's running for president, apparently,
[00:28:04.940 --> 00:28:10.460]   and he doesn't want anybody to say you're not in favor of kids. It doesn't go in effect till next
[00:28:10.460 --> 00:28:16.140]   year. What do you think? It's actually a year and a half. It doesn't go into effect until 2024,
[00:28:16.140 --> 00:28:21.820]   the summer of 2024. I plan to just ignore it. And if they go after me, go out of business.
[00:28:21.820 --> 00:28:25.100]   What are you thinking, Mike? I think that's what most companies are going to do.
[00:28:25.100 --> 00:28:31.260]   There's no way to comply. And even if you tried to comply, it would be so ridiculously expensive
[00:28:31.260 --> 00:28:35.980]   that I think most companies that are small companies and mid-sized companies that qualify
[00:28:35.980 --> 00:28:40.140]   under the law will simply ignore it and hope that the attorney general never decides to go after
[00:28:40.140 --> 00:28:45.420]   them, because that's the only way you can deal with it realistically. And that seems like a bad
[00:28:45.420 --> 00:28:51.100]   situation, because what good is a law if most of the people it applies to are going to ignore it,
[00:28:51.100 --> 00:28:53.340]   but that's what California wants to do right now.
[00:28:53.340 --> 00:28:56.300]   The next bill,
[00:28:56.300 --> 00:29:05.100]   AB 587. Wait, there's more. This one we can do a little faster because we already know about this
[00:29:05.100 --> 00:29:10.940]   bill because it's effectively the same as the ones Texas and Florida passed and which were
[00:29:10.940 --> 00:29:18.380]   over turn, not overturned, but prevented blocked by the courts because it's a violation of the
[00:29:18.380 --> 00:29:22.620]   First Amendment. Tell me what AB 587 does.
[00:29:22.620 --> 00:29:30.540]   So 587, again, just like 2273, which is presented in this sort of noble fashion that sounds good,
[00:29:30.540 --> 00:29:35.740]   587 is technically about transparency. And the idea here is that it requires
[00:29:35.740 --> 00:29:41.180]   companies, social media companies to be transparent about their policies, have them published where
[00:29:41.180 --> 00:29:45.260]   people can read them and they have to file them with the government and they have to sort of live
[00:29:45.260 --> 00:29:50.540]   up to what is in their content moderation policies.
[00:29:50.540 --> 00:29:53.260]   That seems fair. That's mild.
[00:29:53.260 --> 00:29:57.740]   Yeah, it seems fair. And in theory, there's some stuff that is good about it.
[00:29:57.740 --> 00:30:04.220]   But the reality is, again, I keep trying to explain this. Transparency itself is a good thing.
[00:30:04.220 --> 00:30:08.380]   And I'm super supportive and I want more companies to be more transparent and I often push
[00:30:08.380 --> 00:30:14.220]   companies to be more transparent. Mandated transparency begins to lead into a whole bunch of problems.
[00:30:14.220 --> 00:30:19.260]   And so, again, I don't have that much time to go through all the problems.
[00:30:19.260 --> 00:30:24.860]   I can do it in one sense. It's a roadmap for companies to game moderation.
[00:30:24.860 --> 00:30:30.460]   Yes, because you are filing these things and you have to live up to them and you can only
[00:30:30.460 --> 00:30:37.580]   change them every so often, people who don't run websites where they have to deal with the public
[00:30:37.580 --> 00:30:42.860]   or with people posting stuff on their websites do not realize how many bad actors there are with
[00:30:42.860 --> 00:30:46.940]   malicious intent who just want to mess with you. Stay way ahead of you.
[00:30:46.940 --> 00:30:51.900]   And it might just be as simple as spammers, right? That's the sort of most obvious one.
[00:30:51.900 --> 00:30:56.140]   You have people who are more malicious in other ways, but spammers, right? Spammers want to take
[00:30:56.140 --> 00:30:59.660]   advantage of your system to promote whatever it is that they're promoting. And they are looking
[00:30:59.660 --> 00:31:04.220]   for any edge that they can have. Now, under this law, you are basically telling them,
[00:31:04.220 --> 00:31:09.100]   here's exactly the rules for how to spam us and we will not be able to stop it because
[00:31:09.100 --> 00:31:13.260]   to make a change, we sort of have to notify everyone that we're making a change. And there's a period
[00:31:13.260 --> 00:31:18.700]   of time under which we can't make that change. Therefore, you have freedom to spam our site and
[00:31:18.700 --> 00:31:25.580]   we cannot take it down for this long. Past 31 to 3, waiting to sign to be signed.
[00:31:25.580 --> 00:31:28.220]   Cousin to show us your algorithm, Google. Yes.
[00:31:28.220 --> 00:31:37.340]   Finally. And this one's a little smaller. We can get through this one a little quicker. AB2879.
[00:31:37.900 --> 00:31:39.740]   Who is it against cyberbullying?
[00:31:39.740 --> 00:31:47.660]   Right. Again, it's another one of these. It's framed. It's cyberbullying. It's a bad thing, right?
[00:31:47.660 --> 00:31:52.300]   We should try and stop cyberbullying. But again, there are problems with this and that it basically
[00:31:52.300 --> 00:31:57.420]   is blaming the companies, if anyone on their platform, cyberbullies someone else, right?
[00:31:57.420 --> 00:32:03.260]   Now, we do want companies to try and stop that if companies are aware of people using their platforms
[00:32:03.260 --> 00:32:07.500]   to cyberbullies someone else. It's a good thing for them to try and do something about that to
[00:32:07.500 --> 00:32:14.540]   prevent that, to prevent those kinds of things. But this bill effectively pins the blame on the
[00:32:14.540 --> 00:32:20.940]   company that if cyberbullying happens on their platform, that they are going to have liability
[00:32:20.940 --> 00:32:25.020]   for that. And so the end result of that. Sorry. Go ahead.
[00:32:25.020 --> 00:32:32.620]   With this one, 230 come into play anywhere. In theory, right. In theory, 230 should preempt
[00:32:32.620 --> 00:32:37.100]   this bill. I mean, in theory, 230 might preempt some of the other bills too. It's a little trick
[00:32:37.100 --> 00:32:41.100]   here with that. But with this one, it seems fairly clear that 230 should preempt it.
[00:32:41.100 --> 00:32:46.220]   But again, so far we've seen with Florida and with Texas and with a number of other states that
[00:32:46.220 --> 00:32:50.140]   are preparing similar bills, they are kind of banking on the fact that the courts are suddenly
[00:32:50.140 --> 00:32:55.740]   like a little bit afraid to rely on section 230 and are looking to cut back section 230.
[00:32:55.740 --> 00:33:00.220]   So to some extent, I think this is an attempt to have this law in place in case section 230
[00:33:00.220 --> 00:33:04.940]   disappears and they can sort of jump right in and, you know, and, and, and, and, and,
[00:33:04.940 --> 00:33:09.020]   like a trigger for liability. It's, it's a fact that I mean, I don't think anyone
[00:33:09.020 --> 00:33:12.700]   supporting it would say that directly, but that seems to be the way that this is playing out.
[00:33:12.700 --> 00:33:16.460]   Soon as 230 is gone, everybody's liable.
[00:33:16.460 --> 00:33:20.300]   Yeah. Can I ask a quick question? Yeah. I know you're trying to, but, um,
[00:33:20.300 --> 00:33:25.260]   so you've talked about companies your size and the possibility of this. What about the big
[00:33:25.260 --> 00:33:30.540]   companies? What about Twitter and Facebook and Google? How do these laws, if you're there and,
[00:33:30.540 --> 00:33:36.140]   and you have any kind of effort to comply? What does that look like? I think, I think it will
[00:33:36.140 --> 00:33:40.860]   be ridiculously difficult. I think that they're, they're just going to be in trouble and I don't,
[00:33:40.860 --> 00:33:46.060]   I don't know, you know, and, and the, you know, these companies have really not
[00:33:46.060 --> 00:33:51.260]   taken any stand on these bills at all. I think, you know, for the obvious reason that they don't,
[00:33:51.260 --> 00:33:54.620]   they don't want the headline, you know, you know, exactly what would happen if they had spoken
[00:33:54.620 --> 00:33:58.700]   out against any of these bills is that the New York Times and the Washington Post would have huge
[00:33:58.700 --> 00:34:03.980]   giant headlines about, you know, Google is trying to allow cyber
[00:34:03.980 --> 00:34:10.940]   whatever, whatever it would be. You know, and so there's, there's been effectively no pushback on,
[00:34:10.940 --> 00:34:16.220]   on any of these bills, which is why we must defend the internet. That's why, you know, that's,
[00:34:16.220 --> 00:34:20.540]   this is why folks, I get panicky about moral panic is because we got to defend the freedoms
[00:34:20.540 --> 00:34:27.660]   from there from this kind of stupidity. Yeah. Yeah. So, you know, I mean, what, what happens
[00:34:27.660 --> 00:34:31.020]   realistically, again, they're sort of like, what, what, what has to happen under the law? What
[00:34:31.020 --> 00:34:36.700]   happens realistically is my guess is that, you know, basically these companies work with the
[00:34:36.700 --> 00:34:40.620]   Attorney General and, you know, try and sort of come to some sort of agreement on we're doing
[00:34:40.620 --> 00:34:45.500]   this, this and this, you know, like don't go after us. Now I don't remember exactly where
[00:34:45.500 --> 00:34:51.820]   some of these bills turned out because there were, there were late amendments on, on some of them
[00:34:51.820 --> 00:34:57.020]   in terms of where the enforcement was and who can enforce it. So some of these bills originally
[00:34:57.020 --> 00:35:00.460]   had a private right of action. There was another bill that actually didn't get this far, which was
[00:35:00.460 --> 00:35:06.220]   good, which was the, the online addiction bill, which it said like, come, come on,
[00:35:06.220 --> 00:35:12.460]   do liable. That one did not go through that one got dropped. And so that one was, you know, if,
[00:35:12.460 --> 00:35:19.260]   if a web site or, you know, internet service, a dicks, you know, a dicks a child, then
[00:35:19.260 --> 00:35:23.580]   originally parents could sue. So all parents would have to do and say,
[00:35:23.580 --> 00:35:30.380]   is hey, my kid is addicted to Instagram. I'm suing. That was a problem. They took away the,
[00:35:30.380 --> 00:35:34.860]   the private right of action for the parents to sue, but then they left it up to any local
[00:35:34.860 --> 00:35:40.060]   official so that it was, you know, it could be the Attorney General, but could also be like a
[00:35:40.060 --> 00:35:44.940]   local prosecutor who might be, you know, aiming for higher office and what better thing to do
[00:35:44.940 --> 00:35:50.460]   than to get a headline or you're taking on the big bad internet companies. I think that,
[00:35:50.460 --> 00:35:54.700]   that may have gotten limited more and more on some of the other bills. I don't remember,
[00:35:54.700 --> 00:35:58.700]   because we had like all of these bills happening at once and there were a few last
[00:35:58.700 --> 00:36:03.020]   minute changes. So I don't know exactly what all of the last minute amendments were on,
[00:36:03.020 --> 00:36:08.380]   on all of them. So it might not be that anyone can, I think all of the private rights of action
[00:36:08.380 --> 00:36:15.020]   were removed. But there is a question of like, which, which, you know, law enforcement prosecutors
[00:36:15.020 --> 00:36:19.340]   can, can make use of these laws. And I'm not sure where that came out in the end.
[00:36:19.340 --> 00:36:27.500]   It, I'm just looking at the final text of the 20, 20, 20, 73, the age appropriate design code.
[00:36:27.500 --> 00:36:31.900]   And it looks like that one's just the attorney general. It's just the AG. Yeah. Yeah. And it says
[00:36:31.900 --> 00:36:38.060]   there, it would create a California Children's Data Protection Working Group. Yes. To determine
[00:36:38.060 --> 00:36:45.100]   best practices for implementations. So the, the, the nicest interpretation of this is it,
[00:36:45.100 --> 00:36:48.940]   it allows the legislator to legislate you to create a committee and
[00:36:48.940 --> 00:36:55.500]   great. Like that, that part, like, nice, sure. I mean, it would be great if there were some best
[00:36:55.500 --> 00:37:00.620]   practices, but the fear is with best practices, of course, is that like, if any, if any new company
[00:37:00.620 --> 00:37:04.540]   comes along and wants to do something different, maybe they have a better way, but it's not considered
[00:37:04.540 --> 00:37:08.780]   one of the best practices, then you're playing with fire, right? You know, then, then you're, you're
[00:37:08.780 --> 00:37:13.980]   basically just asking for the attorney general to come after you. And so best practices could be good
[00:37:13.980 --> 00:37:17.740]   in that there's, you could learn something interesting. You could get sites to do stuff, but, but it
[00:37:17.740 --> 00:37:22.460]   takes away the ability for experimentation. And maybe there are better ways to do things. And again,
[00:37:22.460 --> 00:37:27.020]   you know, a lot of the problems that people are discussing and the challenges and the risks are
[00:37:27.020 --> 00:37:32.860]   things that are highly dynamic. They're changing. The bad actors are constantly adjusting. And as a
[00:37:32.860 --> 00:37:37.020]   website, you have to be able to adjust as well. And a lot of these don't really allow for that
[00:37:37.020 --> 00:37:40.780]   kind of adjustment. They assume there's this kind of static world. And there's like, things that
[00:37:40.780 --> 00:37:44.940]   are good for kids, things that are bad for kids. And you have to only do the things that are good.
[00:37:44.940 --> 00:37:51.740]   And as you say, this on at techter.com, the bills are fundamentally flawed, written by people who do
[00:37:51.740 --> 00:37:59.100]   not understand how the internet works at all. I recommend people go to techter.com, read a Mike's
[00:37:59.100 --> 00:38:07.340]   very scathing article. There are lots of fact filled and filled lots of links. If you want to know
[00:38:07.340 --> 00:38:12.140]   more, Mike has a lot to do. He's a busy guy because well, the fight has just begun.
[00:38:12.140 --> 00:38:15.660]   25 years you've been doing this. Are you getting tired?
[00:38:15.660 --> 00:38:23.980]   I'm a little tired this week. But, but, but in general, I still enjoy very much what I'm doing.
[00:38:23.980 --> 00:38:27.740]   I wish that I were not the only one raising the alarm about these particular bills.
[00:38:27.740 --> 00:38:33.260]   Amen, brother. But it's an important time to let's plug also. It's an important time to join tech
[00:38:33.260 --> 00:38:40.380]   to sign up. What do you call it? Join or subscribe or sign up? We have a Patreon.
[00:38:40.380 --> 00:38:44.300]   If people like to use Patreon, we also have a thing called the insider program.
[00:38:44.300 --> 00:38:50.380]   Patreon and the insider program are both sort of similar ways of subscribing and supporting us.
[00:38:50.380 --> 00:38:55.260]   And that includes, by the way, we're going to announce the details very soon. We're having an
[00:38:55.260 --> 00:39:00.460]   online get together next week to celebrate the 25th anniversary. And that is open for people
[00:39:00.460 --> 00:39:03.740]   who subscribe. That's correct. So we'll be sending out more details on that.
[00:39:03.740 --> 00:39:08.620]   See you there. Techter.com. Become a friend of Techter. Support them on Patreon.
[00:39:08.620 --> 00:39:14.780]   Techter is our friend. Their card game, the collected all card game by their t-shirts.
[00:39:14.780 --> 00:39:19.580]   Use their VPN. Do everything you can because Mike is doing God's work. And,
[00:39:19.580 --> 00:39:23.740]   Mike, I really appreciate it. We're going to, what we're going to do is take this segment,
[00:39:23.740 --> 00:39:28.860]   turn it into a twit bit so we can disseminate it as widely as possible beyond this show.
[00:39:28.860 --> 00:39:33.500]   Because I think not only do Californians need to know about this, but I think
[00:39:33.500 --> 00:39:37.900]   the whole world needs to understand that this is a trend and it's very likely other states will
[00:39:37.900 --> 00:39:42.460]   follow suit. So this is something to be aware of. Thank you, Mike, for giving us your time. I
[00:39:42.460 --> 00:39:46.940]   know you're busy. I really appreciate it. No, thanks for having me. Give me a chance to
[00:39:46.940 --> 00:39:52.380]   talk about this and raise the alarm and hopefully get some more people aware about it because,
[00:39:52.380 --> 00:39:57.500]   you know, not enough people are talking about it. Absolutely right. Thank you, Mike. Take care.
[00:39:57.500 --> 00:40:01.580]   Take care. All right, we still have lots to talk about. This is a very full week,
[00:40:01.580 --> 00:40:06.860]   but I did want to get Mike on. We will take a break, come back with our regularly scheduled
[00:40:06.860 --> 00:40:12.380]   programming in just a moment. Ant Pruitt, Stacey Higginbotham, Jeff Jarvis,
[00:40:12.380 --> 00:40:16.940]   goes right in congressmen. I don't know. Do something. All right.
[00:40:16.940 --> 00:40:23.980]   Get mad. Get mad. You know, you know, I just realized what Twitter blew was six bucks a month,
[00:40:23.980 --> 00:40:26.860]   right? That's what it jumped. It was three to five. Three to five.
[00:40:26.860 --> 00:40:32.220]   Oh, three to five. Okay. The insider with with Tech Dirt is five dollars a month.
[00:40:32.220 --> 00:40:38.380]   Well, much better than Twitter blue. Yeah, much better. Much better than Twitter blue. Yeah.
[00:40:38.380 --> 00:40:47.740]   By this, not that. All right, let's take a little break. Our show today brought to you by HPE
[00:40:47.740 --> 00:40:55.260]   GreenLake orchestrated by the experts at CDW. Those helpful folks at CDW understand that your
[00:40:55.260 --> 00:41:00.860]   organization needs simple management over its big data, but with some needing to keep their
[00:41:00.860 --> 00:41:06.700]   workloads on prem for, you know, organizational or compliance requirements, it might feel challenging
[00:41:06.700 --> 00:41:12.700]   to organize and optimize your data. Oh, I got some good news. That's where CDW can help your
[00:41:12.700 --> 00:41:18.380]   organization by consolidating and managing all your data in one flexible unified experience
[00:41:18.380 --> 00:41:24.940]   with the HPE GreenLake Edge to Cloud platform. The experience you get with HPE GreenLake is unique
[00:41:24.940 --> 00:41:30.540]   because no matter where your data or applications live, you can free up energy and resources
[00:41:30.540 --> 00:41:35.180]   with automated processes and streamlined management. And we can all use some more
[00:41:35.180 --> 00:41:40.460]   streamlining these days. Am I right? Not only that HPE GreenLake creates a seamless cloud experience
[00:41:40.460 --> 00:41:45.420]   among multiple data environments, thanks to the as a service model that meets your remote workforce
[00:41:45.420 --> 00:41:50.700]   where they live at the edge. And with unrivaled scalability, you'll see an instant increase in
[00:41:50.700 --> 00:41:57.100]   capacity, allowing for greater flexibility and something we all like accelerated business growth.
[00:41:57.100 --> 00:42:02.780]   So your team can tackle bigger priorities like, you know, innovating, winning. When you need to
[00:42:02.780 --> 00:42:11.020]   get more out of your technology, HPE makes data transformation possible and CDW makes it powerful.
[00:42:11.020 --> 00:42:19.340]   Learn more at CDW.com/HPE. Thank you, CDW for supporting this week in Google and you support
[00:42:19.340 --> 00:42:28.380]   us when you go to that address CDW.com/HPE. All right, I'm sorry I had to beat you all up because I
[00:42:28.380 --> 00:42:32.220]   didn't want to keep Mike too long and I want to get out of democracy. You are the dictator.
[00:42:32.220 --> 00:42:39.900]   And it is not a democracy, dang it. No, this is authoritarian podcast. I didn't even ask him about
[00:42:39.900 --> 00:42:46.140]   Senator Amy Klobuchars. Oh, that because I know this would have steam coming at your ears,
[00:42:46.140 --> 00:42:49.580]   Jeff Jarvison. Jarvis.
[00:42:49.580 --> 00:42:56.860]   Amy Klobuchars. Yeah, they changed my name at Ellis Island. Yeah, Jarvison. It was Jarvison.
[00:42:56.860 --> 00:43:04.140]   And David Sisolini from my home state and my senator. And your senator,
[00:43:04.140 --> 00:43:09.660]   Cory Booker. Cory Booker. I am I am writing a letter to Cory Booker this weekend.
[00:43:09.660 --> 00:43:18.300]   I'm good. Three good Democratic members of Congress, but they are back with the JCP,
[00:43:18.300 --> 00:43:23.020]   the Journalism Competition and Preservation Act. Look, who doesn't want competition and
[00:43:23.020 --> 00:43:27.020]   preservation among journalists for the kids for the kids? For the kids.
[00:43:27.020 --> 00:43:32.380]   This is much. I would say for the democracy. For the democracy, actually, absolutely right.
[00:43:32.380 --> 00:43:37.260]   For the newspaper hedge fund owners, actually. Well, this is the funny thing. So, you know,
[00:43:37.260 --> 00:43:44.460]   this is a new bill updated. The big change is it no longer applies to big news organizations.
[00:43:44.460 --> 00:43:50.460]   The idea of it's a link tax. So Google, Facebook, anybody who links to snippets.
[00:43:50.460 --> 00:43:54.380]   It's a link blackmail more like it. Well, this is what's funny about it. Yeah,
[00:43:54.380 --> 00:44:02.940]   because it's the Rupert Murdoch Australia thing. It's it says they can demand a fee from these big
[00:44:02.940 --> 00:44:09.580]   tech companies for linking to them. Right. But she's she's smart. She's smart. She said, well,
[00:44:09.580 --> 00:44:15.740]   you know what? Let's exempt Google and Facebook and let's I mean, not not not I'm sorry, not
[00:44:15.740 --> 00:44:20.220]   exempt them. Let's exempt the New York Times, the Washington Post, the Fox, the big organization.
[00:44:20.220 --> 00:44:25.100]   They don't need the money. Let's make it for the little guy. Anybody with under 1500 employees.
[00:44:25.100 --> 00:44:29.660]   Right, Jeff, that's good. That supports local journalism. Small
[00:44:29.660 --> 00:44:35.820]   business. Budge blogs biting at your ankles. I was going to say is Gannett in her home district
[00:44:35.820 --> 00:44:42.060]   or in her own state. Well, what's so but what's what Mike points out and tech dirt is all this
[00:44:42.060 --> 00:44:49.260]   bill basically says is buy up large newspapers, cut down under 1500 employees and step three profit.
[00:44:49.260 --> 00:44:56.460]   You know, just she says she's basically saying you get free money as long as you fire enough
[00:44:56.460 --> 00:45:02.860]   people first fire enough people. Jeff, come on. Now you do want to support small journalism
[00:45:02.860 --> 00:45:09.180]   organizations. I know you do. This is just this is just ridiculous. This is this is the worst of
[00:45:09.180 --> 00:45:17.020]   my our field, my field. I'll say it myself. Journalism should be independent of a government
[00:45:17.020 --> 00:45:21.260]   and not lobbying to government. And that's what this is from. This is the the newspapers
[00:45:21.260 --> 00:45:26.140]   association. This is this is the trade associations have turned into lobbying organizations.
[00:45:26.140 --> 00:45:31.500]   Purely and they're going after this. They've been going after this removal of antitrust
[00:45:31.500 --> 00:45:37.100]   so that they can all gang together, which is a little ironic because they're claiming
[00:45:37.100 --> 00:45:42.460]   antitrust everywhere else in the world. And they're trying to have backshish and blackmail against
[00:45:42.460 --> 00:45:47.900]   the big companies. And the interesting thing is I just recommended a book to to I had a bunch of
[00:45:49.500 --> 00:45:56.540]   internet researchers in the school last Monday, Tuesday. And as we were talking there, I we got
[00:45:56.540 --> 00:46:00.380]   into a weird conversation. I mentioned a book that I love called Media at War by Gwyneth
[00:46:00.380 --> 00:46:06.220]   Jackaway. You're not going to find it at any bookstore that's out of print. But it's about how
[00:46:06.220 --> 00:46:12.860]   the newspaper industry fought the entrance of radio. And it's the exact same game plan here
[00:46:13.660 --> 00:46:21.020]   to use their clout to cash in with government and to try to disadvantage their new competitors
[00:46:21.020 --> 00:46:25.900]   because they don't know what they're doing. It's disgusting. It's wrong. It's protectionist.
[00:46:25.900 --> 00:46:30.300]   It doesn't help news. It doesn't help democracy. It's BS.
[00:46:30.300 --> 00:46:36.460]   Well, and then Mike points out there's an even greater hazard because it allows a smaller
[00:46:36.460 --> 00:46:44.620]   organization to band together to form antitrust part. Yeah. Without violating trust laws.
[00:46:44.620 --> 00:46:49.260]   Not at a line, but while the antitrust band together to go to Google and Facebook and Apple and
[00:46:49.260 --> 00:46:56.780]   Microsoft and say we want some money. And you if you aren't carrying our journals, you must carry
[00:46:56.780 --> 00:47:04.860]   them. So as Mike points out, this is essentially giving disinformation brokers the right to aggregate
[00:47:05.420 --> 00:47:11.180]   and go to Google and say, well, you the bill says platforms cannot refuse to link.
[00:47:11.180 --> 00:47:16.620]   And it's compelled speech is not free speech. And the right wing is using just that. They're
[00:47:16.620 --> 00:47:22.460]   trying to say you can't take down our tweets. You can't take down our stuff at the same time.
[00:47:22.460 --> 00:47:28.140]   And we're going to get to something later with about true social, I suspect, where the platforms
[00:47:28.140 --> 00:47:32.460]   end up in a bind. The left is saying take down all the junk. The right says, that's our junk.
[00:47:32.460 --> 00:47:37.740]   You're tucked down. And compelled speech is not free speech. Yeah.
[00:47:37.740 --> 00:47:43.980]   It's he says it's an end run around anti trust law copyright law and common carry law.
[00:47:43.980 --> 00:47:46.540]   Yep. Just to get Google Apple,
[00:47:46.540 --> 00:47:49.260]   Amazon and Microsoft to give us some money for linking to us.
[00:47:49.260 --> 00:47:55.100]   Once again, like the California law, setting terrible precedent. Yeah. It's awful. It's just
[00:47:55.100 --> 00:48:01.020]   awful. Why? Sure is not my I will hurt for about a week to.
[00:48:02.060 --> 00:48:05.740]   I'm it's I really like her. But and he asked the question at the end.
[00:48:05.740 --> 00:48:10.780]   I used to just used to think Klobuchar was ignorant about how the internet worked. But now,
[00:48:10.780 --> 00:48:14.380]   I'm beginning to realize she's deliberately seeking to destroy it. So that's my question is,
[00:48:14.380 --> 00:48:19.420]   what what gives here? Is it is it the people who just don't have thing to do?
[00:48:19.420 --> 00:48:23.580]   It's good. It's good for Paul. It's good for voting to get the internet's evil and it's terrible
[00:48:23.580 --> 00:48:26.860]   and it's turning apart democracy. That's awful. And we have to kill it. We have to get rid of it
[00:48:26.860 --> 00:48:33.420]   just like every new medium before. And it's a certain age group of those politicians doing all of that
[00:48:33.420 --> 00:48:43.420]   too. A lot of it. Yep. Yep. So nobody, nobody lost an election by by banning the internet in effect.
[00:48:43.420 --> 00:48:47.980]   That's how that's a nobody won one by by protecting the internet and protecting speech.
[00:48:47.980 --> 00:48:52.140]   It's it's not a popular thing to protect speech. You end up protecting pornography. You end up
[00:48:52.140 --> 00:49:00.460]   protecting Nazis and Skokie. The First Amendment is not a popular piece of law of constitutional law,
[00:49:00.460 --> 00:49:06.060]   but it's vital to the democracy and it needs our legislators to stand up for it instead of
[00:49:06.060 --> 00:49:11.820]   trying to cut through it. And and it's very much in peril. Jeff Kossaf who wrote the
[00:49:11.820 --> 00:49:15.900]   wonderful 26 words that created the internet along with the United States of Anonymous and
[00:49:15.900 --> 00:49:21.260]   as a new book coming out soon is also very worried about what's going to happen with Sullivan
[00:49:21.260 --> 00:49:26.940]   in the Supreme Court, which is part of the First Amendment. And so bit by bit piece by piece,
[00:49:26.940 --> 00:49:32.060]   they're going after the most fundamental right in America from both sides because the internet
[00:49:32.060 --> 00:49:36.620]   is now the bad guy. Why is this a problem? Why do I find a problem with a moral panic?
[00:49:36.620 --> 00:49:41.900]   This is what I'm seeing as you're making me feel guilty now. I feel bad that all this time.
[00:49:41.900 --> 00:49:45.900]   It's good. I was a bit critical of the big of the tech giants when I should have been.
[00:49:45.900 --> 00:49:53.740]   Well, so I'm not that no, it's just waiting the tech giants with the internet. And I think
[00:49:53.740 --> 00:49:59.580]   that's the easy shorthand. And I hear you saying this a lot with Jeff, Jeff, like for example,
[00:49:59.580 --> 00:50:04.300]   talking about like content laws, you're like, we have to protect the internet. I would argue
[00:50:04.300 --> 00:50:10.220]   that we just have to protect the communication medium that we're predominantly using today
[00:50:10.220 --> 00:50:14.860]   instead of being like, ah, the internet, like it's a special snowflake. It's not anymore. It's
[00:50:15.740 --> 00:50:19.660]   it's an established method by which we communicate with everyone. It's our
[00:50:19.660 --> 00:50:26.060]   it's our telephone network. It's our newspaper. It's all of this. And I don't know if it's a
[00:50:26.060 --> 00:50:32.620]   function of like nomenclature, but it's definitely we we keep associating the internet with,
[00:50:32.620 --> 00:50:38.860]   you know, the fang stocks, for example, or the internet with ad tech. And it is all these things,
[00:50:38.860 --> 00:50:44.460]   but it's also more and we've got to figure out how to divorce that or talk about it in a way that
[00:50:45.580 --> 00:50:51.100]   well, I might I grow says, Stacy. And I think one of the questions that the way I look at it,
[00:50:51.100 --> 00:50:55.740]   and I just had one of the professors who in my group of researchers last week,
[00:50:55.740 --> 00:51:01.100]   school be for this, but I'll still argue it, is that the current proprietors of the net and they
[00:51:01.100 --> 00:51:04.780]   are the net's proprietors. Let's be honest, they're running the infrastructure. They're running the
[00:51:04.780 --> 00:51:09.980]   functions we use, but they're not forever. Well, and my point is that's there forever.
[00:51:09.980 --> 00:51:15.900]   The re there as big a threat to the to the true real little internet as as government is and
[00:51:15.900 --> 00:51:21.260]   and everybody. I think no, I think they are they are a threat because they totally
[00:51:21.260 --> 00:51:29.180]   they've let ad tech run rampant over it. And they they have not been good stewards of exactly.
[00:51:29.180 --> 00:51:33.500]   I guess what the internet was capable. And this and this California law is a good example,
[00:51:33.500 --> 00:51:37.740]   because they can easily adhere to this law. So this is something you talk about, Jeff,
[00:51:37.740 --> 00:51:43.660]   with regulatory capture. They don't mind this law, but it's harmful to this. I'm a little
[00:51:43.660 --> 00:51:50.140]   internet guy, your little internet guy to Stacy. This is harmful to us.
[00:51:50.140 --> 00:51:57.500]   Actually, I wonder if what I see happening. I mean, let's be real. This happened with GDPR,
[00:51:57.500 --> 00:52:03.340]   and we were like, Oh, it's the world. What happened is we ended up, you know, in my case,
[00:52:03.340 --> 00:52:07.980]   it was Mailchimp and other people it was WordPress. Other services will come up and they'll be like,
[00:52:07.980 --> 00:52:11.980]   here, we're going to handle this for you. And yes, you'll end up paying them and they'll do it. But
[00:52:11.980 --> 00:52:15.740]   that's realistically how something like this gets played out. Sorry.
[00:52:15.740 --> 00:52:20.700]   I hope so. But I think the other piece of this is what you're really complaining about, Leo,
[00:52:20.700 --> 00:52:26.220]   is the outcroppings of an intention based economy, which was invented by media media still depends
[00:52:26.220 --> 00:52:32.140]   on it. It's still media. And that that's what infected the internet. And that's what infected
[00:52:32.140 --> 00:52:38.940]   the internet of going for scale and taking our data and abusing this with all kinds of junkie
[00:52:38.940 --> 00:52:43.820]   web pages and content. What we have to do, what we have to encourage is the invention of the
[00:52:43.820 --> 00:52:48.780]   next generations. And as you're going to hear me, I'm going to do my Gutenberg moments because,
[00:52:48.780 --> 00:52:57.020]   by the way, folks, here's the manuscript going in. Yeah, yeah, 446 pages.
[00:53:01.100 --> 00:53:04.380]   The Gutenberg parentheses coming out next June from Bloomsbury.
[00:53:04.380 --> 00:53:10.300]   I just forgot what I was going to say because of that little exciting moment.
[00:53:10.300 --> 00:53:18.220]   I think what you're saying is we have to let the next generation of the internet. And this is my
[00:53:18.220 --> 00:53:25.420]   hope is that it doesn't really, none of this really matters because the internet is so vital
[00:53:25.420 --> 00:53:31.660]   and so disparate and so large that it can't really be shut down by these behemoths of government.
[00:53:31.660 --> 00:53:35.580]   But we can end up with a lot of bad stuff. We will. And I have history.
[00:53:35.580 --> 00:53:39.420]   I think it's going to swim in the interstices of these laws and these graphs.
[00:53:39.420 --> 00:53:45.020]   We've got to recognize the reflex to control the reflex for censorship, for banning,
[00:53:45.020 --> 00:53:50.460]   protectionist legislation that happened with print, it happened with radio, it happened with TV.
[00:53:50.460 --> 00:53:59.100]   Or as Tim Wu said, the instinct to agglomerate, the instinct to control is really about
[00:53:59.100 --> 00:54:03.660]   finding all these little things and making them one big thing to monopolize.
[00:54:03.660 --> 00:54:08.220]   Yeah, fine. That's the economy. That's capitalism.
[00:54:08.220 --> 00:54:10.780]   That's what it is. It's not just to monopolize things.
[00:54:10.780 --> 00:54:16.220]   It was too much on that. It's also to make it easier for people to access and make things
[00:54:16.220 --> 00:54:22.540]   more accessible to people. So think about the perfect example of this was the Yahoo homepage
[00:54:22.540 --> 00:54:30.060]   pre-Google and then Google. That can glamoratize the internet in a way that means it's supposed to
[00:54:30.060 --> 00:54:34.460]   be glamoratized. Well, I'm just like a pleasure.
[00:54:34.460 --> 00:54:40.460]   You remember? We want de-compobitorization. That's what we want.
[00:54:41.420 --> 00:54:47.580]   But you don't, not as a user. You remember having to like, "Let's see what's on the internet. I'll go
[00:54:47.580 --> 00:54:53.580]   to my Yahoo! Cool side of the day. Cool side of the day. Oh, let's look at finance. Let's look at
[00:54:53.580 --> 00:54:59.740]   some news. I mean, so, yeah. It is going to be right. It's not just about money. It's also about
[00:54:59.740 --> 00:55:05.900]   ease of use. And that's a constant, I mean, look at Apple versus Android. That's what that's about.
[00:55:05.900 --> 00:55:12.220]   Here's what makes me optimistic is that, well, I look at things like what Corey
[00:55:12.220 --> 00:55:18.060]   Doctorow calls the largest consumer boycott of all time, which is the widespread use of ad blockers.
[00:55:18.060 --> 00:55:24.860]   I think humans are going to act in their own interest. And this a glamor is an
[00:55:24.860 --> 00:55:34.860]   agglomerization, which is what business wants. Ben Thompson calls it his aggregation theory.
[00:55:34.860 --> 00:55:40.060]   It's what business wants because it's more profitable. It's what Corey Doctorow calls
[00:55:40.060 --> 00:55:44.860]   choke point capitalism, where you have to give a dollar every time you want to do anything on
[00:55:44.860 --> 00:55:50.060]   the internet. You have to give a dollar to Google. That's the tendency there. But humans are the
[00:55:50.060 --> 00:55:55.580]   opposite. And I have a feeling, for instance, that podcasting is going to get eaten alive
[00:55:55.580 --> 00:56:04.060]   by big companies, by aggregation, and that pretty soon it'll be Spotify, iHeart, Amazon,
[00:56:04.060 --> 00:56:08.860]   Apple, a handful of companies that really make all the money in podcasting and all the little
[00:56:08.860 --> 00:56:14.220]   independent companies like mine will be gone. Or worse, Theo, that podcast, like blogging,
[00:56:14.220 --> 00:56:19.020]   the New York Times just like blogging. And they took over blogs, and then they missed blogs.
[00:56:19.020 --> 00:56:24.700]   And now, you know, try to find blogs in RSS. It's tougher. So do they do they ruin podcasting
[00:56:24.700 --> 00:56:30.620]   in the process? Exactly. But just as podcasting took over when blogs kind of went away, I think
[00:56:30.620 --> 00:56:35.500]   this is where I'm agreeing with you, Jeff. I know it's hard to see, but I'm kind of, I think
[00:56:35.500 --> 00:56:43.260]   the tendency is in the right direction, regardless of this tendency towards aggregation.
[00:56:44.220 --> 00:56:49.340]   That in the long run, innovation happens at a smaller scale. And we'll always win because people
[00:56:49.340 --> 00:56:55.740]   are people. Yeah, I hope that's to me the only opposite. Now, what role does like the FTC have
[00:56:55.740 --> 00:57:02.140]   in buying up these? Because we will say, well, hold up, they're ineffective. One second. So
[00:57:02.140 --> 00:57:10.380]   looking at looking at innovation, think about Facebook buying WhatsApp or Instagram when they
[00:57:10.380 --> 00:57:16.220]   saw new forms of content sharing and social media kind of developing so they could crush it or
[00:57:16.220 --> 00:57:23.500]   incorporate it. The FTC is now looking in their rules for like evaluating antitrust arguments.
[00:57:23.500 --> 00:57:27.500]   They're looking at things like, Hey, are you buying up future innovation? We did a whole thing on
[00:57:27.500 --> 00:57:31.900]   Amazon buying iRobot about the future. In the short term, that is a government action.
[00:57:31.900 --> 00:57:37.980]   Yes, but unfortunately, the government changes every four years in the winds of politics.
[00:57:37.980 --> 00:57:42.940]   That's actually fortunate. We actually that's that's part of like democracy. No, I know.
[00:57:42.940 --> 00:57:49.100]   It's a peaceful transfer. I support it. But that's why you're not going to get a monolithic FTC.
[00:57:49.100 --> 00:57:53.420]   Every four years, new chairman, new ideas. The last one goes out the window.
[00:57:53.420 --> 00:57:59.020]   That's why we have to have good laws. I mean, so you've got your laws, the laws
[00:57:59.020 --> 00:58:09.900]   then dictate the regulations. The agencies haven't always been so politicized.
[00:58:09.900 --> 00:58:16.620]   Yes, they have. I mean, they always have. Yes, they always have. The commissioners may have been
[00:58:16.620 --> 00:58:23.020]   politicized, but the actual like decision making, like if I look at the FTC, the FTC,
[00:58:23.020 --> 00:58:28.060]   all of those agencies, they have a whole layer of staff and experts that are actually trying
[00:58:28.060 --> 00:58:33.340]   to solve this. And then they have the commissioners who are like deciding on some cases and making
[00:58:33.340 --> 00:58:39.260]   statements and evaluating decision like a direction, but trying to solve. She's playing
[00:58:39.260 --> 00:58:46.220]   new middle again this week. I think I'm going to have to say government is in the same boat as
[00:58:46.220 --> 00:58:54.060]   big tech, all the all the aggregators, the power mongers, the big guys, and we're just going to have
[00:58:54.060 --> 00:59:00.060]   be the little minnows or noots that swims through the gaps because
[00:59:00.060 --> 00:59:07.420]   and even more than ever before, the winds of politics blow, hither and yawn.
[00:59:07.420 --> 00:59:13.980]   There's no policy that will survive to a new Congress. None.
[00:59:13.980 --> 00:59:20.780]   You could argue that historically our innovation has been around technology. What if our innovation
[00:59:20.780 --> 00:59:25.260]   is now moving into a political sphere? So our innovation then becomes about,
[00:59:25.260 --> 00:59:32.300]   and this is what private equity has done for decades is kind of this regulatory arbitrage.
[00:59:32.300 --> 00:59:37.020]   So it could be that our innovation is just going to move from like cool stuff and physics to like
[00:59:37.020 --> 00:59:41.660]   really boring stuff in lawyers. If I had a choice, that would suck. Yes. Thank you.
[00:59:41.660 --> 00:59:47.260]   You just you just said what I was going to say. If I have a choice, I'd rather have innovation
[00:59:47.260 --> 00:59:50.540]   occur with the innovators, not with the members of Congress and the lawyers.
[00:59:50.540 --> 00:59:56.460]   Yeah, but who's it's hard and expensive and Congress no longer funds it.
[00:59:56.460 --> 01:00:00.860]   And meanwhile, Congress is putting all this money into these programs and these companies
[01:00:00.860 --> 01:00:05.020]   are like, Oh my God, if I could just play this regulatory arbitrage, I could get billions
[01:00:05.020 --> 01:00:09.260]   as opposed to going, I feel like politics kind of got us into this by
[01:00:10.220 --> 01:00:16.940]   reallocating money from basic research funding to like weird programs and such. I don't know.
[01:00:16.940 --> 01:00:23.340]   It's hard. I feel so my bastic just tweeted. If you want to see me getting increasingly
[01:00:23.340 --> 01:00:26.700]   exasperated and frustrated, watch the first 45 minutes of today's this week.
[01:00:26.700 --> 01:00:32.940]   And as I said, we're going to make it a good cause folks. We're going to make it.
[01:00:32.940 --> 01:00:38.140]   It's with it. I'm trying to be optimistic, Stacey. If I thought that the future
[01:00:38.780 --> 01:00:47.260]   are future dependent on right thinking members of Congress and agency administrators and
[01:00:47.260 --> 01:00:53.340]   staff, I would be very depressed. I think that I think in the long run, our hope, our best hope
[01:00:53.340 --> 01:00:59.340]   is with individual action. And yes, a lot of the actual action will not get you anywhere. That
[01:00:59.340 --> 01:01:06.060]   is such a lie. People are sold on this because we're taught to believe that I'm serious. This is
[01:01:06.060 --> 01:01:08.700]   this is like, I'll argue with you in a minute. Keep going.
[01:01:08.700 --> 01:01:18.620]   Okay. It is a lie to make us powerless. And I say this, we need, we need collective action, one,
[01:01:18.620 --> 01:01:25.900]   as as people. But we also need to get up and find politicians and people who actually can speak to
[01:01:25.900 --> 01:01:31.580]   us because this lie about individual action, if you look at what's happening at a political level,
[01:01:31.580 --> 01:01:36.380]   at a corporate level, there is no way that is a lie that they tell us to keep us.
[01:01:36.380 --> 01:01:41.980]   Oh, I agree with you. I think we should stay politically active and aware and alert. And we
[01:01:41.980 --> 01:01:45.820]   should write members of Congress and all that stuff. I'm not saying give up on that. And these
[01:01:45.820 --> 01:01:50.460]   people will die. I know. And new people will come in. And then new people will happen.
[01:01:50.460 --> 01:01:54.860]   Yeah. No, I'm not saying don't do that. And I believe in collective action. But honestly,
[01:01:54.860 --> 01:01:57.740]   where does innovation come? Not from not from collective action.
[01:01:59.180 --> 01:02:04.380]   You cannot have innovation at the scale that we need. You cannot have physics innovation.
[01:02:04.380 --> 01:02:09.900]   And that's part of the problem. At an individual or even a tiny company level,
[01:02:09.900 --> 01:02:16.140]   you need big bucks to make inroads into material science and to solve things like climate change.
[01:02:16.140 --> 01:02:22.700]   You cannot sit there in hope that some billionaire is going to be like, oh, hell yeah, Mars, that's
[01:02:22.700 --> 01:02:27.820]   the future. And because then you get people going to fricking Mars instead of researching
[01:02:27.820 --> 01:02:33.260]   useful. Well, I think Elon Musk is no longer our best best hope, but but in the early days.
[01:02:33.260 --> 01:02:40.700]   That philosophy is what gives him the that that that but you're talking about today's like
[01:02:40.700 --> 01:02:47.660]   Elon and don't today's Elon is a is a horrible person. But because Elon wanted to become today's
[01:02:47.660 --> 01:02:52.380]   Elon, he did innovate. He did do some amazing things. And I think that's over and over again.
[01:02:52.380 --> 01:02:56.620]   You see that. Did he? Yes. He did it as a team, not just him.
[01:02:56.620 --> 01:03:01.020]   No, of course he created a team. He created a team and yes, he got government subsidies.
[01:03:01.020 --> 01:03:06.620]   Absolutely. Nevertheless, it was his vision to do tests, you know, to move Tesla. He didn't
[01:03:06.620 --> 01:03:10.700]   create Tesla, but he moved Tesla forward his vision. He moves SpaceX forward with his vision.
[01:03:10.700 --> 01:03:15.260]   Now he's reaping the reward of it, which makes him a complete ass. But
[01:03:15.260 --> 01:03:22.780]   but early Elon was a mover and shaker. And I think that you could say that in a lot of early
[01:03:22.780 --> 01:03:31.260]   Henry Ford transformed the world later Henry Ford, not so nice. So that's his that this is that.
[01:03:31.260 --> 01:03:36.220]   I think this is the way of the world. But I do think that individual action
[01:03:36.220 --> 01:03:42.460]   that eventually becomes collective action, but it starts with the individual. You know, you got
[01:03:42.460 --> 01:03:49.020]   a, you know, you got enjoyed him on that. Yeah. So in fact, I just wrote this paragraph.
[01:03:49.020 --> 01:03:52.860]   The last paragraph I wrote the book just today, where I was trying to grapple with just that.
[01:03:52.860 --> 01:03:58.700]   And Stacy, you know, I'll pull Stacy on Stacy. You know, I think you're right, but I'm also going
[01:03:58.700 --> 01:04:04.380]   to disagree with you that I think individual action matters to the extent that everything we
[01:04:04.380 --> 01:04:09.340]   choose to do on the internet, everything we say and don't say everybody that we argue with or
[01:04:09.340 --> 01:04:14.620]   don't everybody we shun has an impact in the sense that it is a process of setting norms.
[01:04:14.620 --> 01:04:19.980]   And it's slow and it's maybe frustrating, but we have a responsibility. If we share a whole
[01:04:19.980 --> 01:04:23.660]   bunch of junk on the internet, we're making the internet a worse place. If we share our full
[01:04:23.660 --> 01:04:27.260]   wonderful stuff on the internet, we're making the internet a better place. So we have that role,
[01:04:27.260 --> 01:04:32.780]   but that role is in fact, you're right, very circumscribed, very limited. And how does society
[01:04:32.780 --> 01:04:38.060]   make changes? It may change us through institutions, but institutions aren't just laws or governments
[01:04:38.060 --> 01:04:46.620]   or bodies. They're also norms at large. They're also larger views of how we see the society. So
[01:04:46.620 --> 01:04:51.260]   things like free and fair elections as an institution that we stand behind until it gets attacked,
[01:04:51.260 --> 01:04:56.460]   or the First Amendment as institution we stand to mind until it gets attacked. And so then we
[01:04:56.460 --> 01:05:01.100]   have a responsibility as a society to protect those institutions, to update them, to build new ones,
[01:05:01.100 --> 01:05:05.900]   whatever that might be. So it gets fought out at a bigger level of institutions, but we contribute
[01:05:05.900 --> 01:05:09.180]   to that with our individual actions, which means we have individual responsibility.
[01:05:09.180 --> 01:05:13.020]   Here's how I would put it. There are no norms without norm array.
[01:05:13.020 --> 01:05:16.780]   Am I right? Leave it to Leo.
[01:05:16.780 --> 01:05:22.300]   If you don't get up on the table and you say union now, then you can have collective action.
[01:05:22.300 --> 01:05:28.700]   But it starts with an individual starts with an idea. So, and I think that the big problem is
[01:05:28.700 --> 01:05:34.460]   looking to government or Google or Facebook or the California legislature for solutions.
[01:05:34.460 --> 01:05:37.500]   That's not where they start. It starts with norm array on the table.
[01:05:37.500 --> 01:05:41.900]   And by the way, I just said that so we'd have a title.
[01:05:41.900 --> 01:05:50.140]   I was hoping for the compartmental decision or whatever the heck it was.
[01:05:50.140 --> 01:05:51.820]   Deconglomeration.
[01:05:51.820 --> 01:05:53.740]   Conglomerization.
[01:05:53.740 --> 01:05:58.460]   Norms start with norm array. That's the one I'm voting for anyway.
[01:05:58.460 --> 01:06:01.180]   Well, it's not a democracy.
[01:06:01.180 --> 01:06:01.740]   It's not a democracy.
[01:06:01.740 --> 01:06:02.060]   It's a norm about ready.
[01:06:02.060 --> 01:06:02.940]   You have the only vote.
[01:06:02.940 --> 01:06:06.140]   Wait a minute. Who's a normal race? See how quickly they forget?
[01:06:06.140 --> 01:06:06.540]   Wow.
[01:06:06.540 --> 01:06:07.980]   Wow.
[01:06:07.980 --> 01:06:08.940]   Okay.
[01:06:08.940 --> 01:06:11.340]   Mobile history coming here. You asked for it, Stacey.
[01:06:11.340 --> 01:06:13.420]   It's a film?
[01:06:13.420 --> 01:06:15.820]   Yeah, but it was based on a person.
[01:06:15.820 --> 01:06:16.380]   A person.
[01:06:16.380 --> 01:06:16.940]   For a living party.
[01:06:16.940 --> 01:06:20.860]   It was based on a person who was a union organizer.
[01:06:20.860 --> 01:06:21.820]   By Sally Field.
[01:06:21.820 --> 01:06:22.860]   Oh, yeah.
[01:06:22.860 --> 01:06:27.660]   I played a union organizer once in high school. It was our UIL play.
[01:06:27.660 --> 01:06:28.220]   Yeah.
[01:06:28.220 --> 01:06:34.460]   I would really wish I could put the video of norm array standing up on the table,
[01:06:34.460 --> 01:06:36.780]   but I do want this show to survive YouTube.
[01:06:36.780 --> 01:06:38.380]   Yeah, plus not do that.
[01:06:38.380 --> 01:06:38.540]   Yeah.
[01:06:38.540 --> 01:06:39.100]   Yeah.
[01:06:39.100 --> 01:06:40.140]   I won't.
[01:06:40.140 --> 01:06:44.940]   All I have to say is you like me.
[01:06:44.940 --> 01:06:46.220]   You really like me.
[01:06:46.220 --> 01:06:47.340]   And again,
[01:06:47.340 --> 01:06:48.540]   Oh, Sally, that's fresh.
[01:06:48.540 --> 01:06:49.020]   Yep.
[01:06:49.020 --> 01:06:50.860]   No, it's from her acceptance speech.
[01:06:50.860 --> 01:06:51.340]   Something speech.
[01:06:51.340 --> 01:06:54.140]   For winning the Academy Award for playing norm array.
[01:06:54.140 --> 01:06:55.740]   Oh, Sally Field.
[01:06:55.740 --> 01:06:56.300]   Sally Field.
[01:06:56.300 --> 01:06:57.340]   Sally Field.
[01:06:57.820 --> 01:06:59.180]   You probably don't think of true.
[01:06:59.180 --> 01:07:00.060]   No, she also.
[01:07:00.060 --> 01:07:01.020]   I was a flying nun.
[01:07:01.020 --> 01:07:01.500]   I was a flying nun.
[01:07:01.500 --> 01:07:03.180]   Before my time.
[01:07:03.180 --> 01:07:04.780]   All before.
[01:07:04.780 --> 01:07:07.500]   You can play the flying nun without sound Leo.
[01:07:07.500 --> 01:07:09.420]   Does anybody remember the flying nun?
[01:07:09.420 --> 01:07:11.420]   Well, we're old enough to.
[01:07:11.420 --> 01:07:13.100]   You don't remember the flying nun.
[01:07:13.100 --> 01:07:14.540]   Oh my God.
[01:07:14.540 --> 01:07:15.500]   I know you do.
[01:07:15.500 --> 01:07:16.140]   You're an old.
[01:07:16.140 --> 01:07:16.220]   Yeah.
[01:07:16.220 --> 01:07:16.540]   Okay.
[01:07:16.540 --> 01:07:17.180]   On an old farm.
[01:07:17.180 --> 01:07:20.940]   How could people have forgotten the flying nun?
[01:07:20.940 --> 01:07:21.340]   Welcome.
[01:07:21.340 --> 01:07:21.740]   They were.
[01:07:21.740 --> 01:07:22.140]   They were.
[01:07:22.140 --> 01:07:22.380]   They were.
[01:07:22.380 --> 01:07:23.660]   You know,
[01:07:23.660 --> 01:07:24.220]   Zycoats.
[01:07:24.220 --> 01:07:27.020]   Again, I think, and I think this is intentional.
[01:07:27.740 --> 01:07:29.980]   I was going to play the flying nun thing,
[01:07:29.980 --> 01:07:32.700]   but the F key on my keyboard popped up.
[01:07:32.700 --> 01:07:35.660]   And all I could do is the lying nun.
[01:07:35.660 --> 01:07:39.580]   So, and I don't think I want to play that clip.
[01:07:39.580 --> 01:07:41.020]   Are you serious?
[01:07:41.020 --> 01:07:43.580]   This is the universe saying Leo.
[01:07:43.580 --> 01:07:45.820]   So you don't use it the F-words to protect the children.
[01:07:45.820 --> 01:07:46.860]   It's time to move on.
[01:07:46.860 --> 01:07:47.260]   Do it.
[01:07:47.260 --> 01:07:48.620]   Wow.
[01:07:48.620 --> 01:07:53.340]   Do we want to talk about the Twitter,
[01:07:53.340 --> 01:07:54.940]   the Twitter whistleblower,
[01:07:54.940 --> 01:07:55.980]   which we just move on?
[01:07:55.980 --> 01:07:56.540]   Oh.
[01:07:56.540 --> 01:07:57.020]   Oh.
[01:07:57.020 --> 01:07:58.060]   We talked about it last week.
[01:07:58.060 --> 01:07:59.340]   Yeah, we talked about it last week.
[01:07:59.340 --> 01:08:01.980]   Well, now, now Musk is trying to delay the case.
[01:08:01.980 --> 01:08:04.780]   You know, it's clear part of his thing.
[01:08:04.780 --> 01:08:06.140]   And bottom line,
[01:08:06.140 --> 01:08:08.940]   Elon was always looking for a way to get out of this.
[01:08:08.940 --> 01:08:11.100]   He realized early on, whoops.
[01:08:11.100 --> 01:08:15.340]   And he's just been ever since grasping at any straw
[01:08:15.340 --> 01:08:16.620]   so that he could get out of this.
[01:08:16.620 --> 01:08:17.740]   So now we know.
[01:08:17.740 --> 01:08:20.460]   But, but it looks like he can, right?
[01:08:20.460 --> 01:08:20.940]   With.
[01:08:20.940 --> 01:08:22.780]   Ah, well, you know, the.
[01:08:22.780 --> 01:08:23.580]   Oh, no, no, no.
[01:08:23.580 --> 01:08:24.460]   It's Stacy.
[01:08:24.460 --> 01:08:25.580]   Stacy will know about this.
[01:08:25.580 --> 01:08:27.900]   The key is M-A-E.
[01:08:27.900 --> 01:08:31.020]   M-A-E?
[01:08:31.020 --> 01:08:31.340]   Right.
[01:08:31.340 --> 01:08:33.900]   I'm turning right to stage V.
[01:08:33.900 --> 01:08:37.340]   Oh, that a little made up algorithm that they.
[01:08:37.340 --> 01:08:38.620]   No, no, no, no.
[01:08:38.620 --> 01:08:39.340]   Not algorithm.
[01:08:39.340 --> 01:08:43.180]   It's a monthly something engagement.
[01:08:43.180 --> 01:08:46.860]   A monetizable D.
[01:08:46.860 --> 01:08:48.940]   No, you guys suck.
[01:08:48.940 --> 01:08:50.060]   You psych so bad.
[01:08:50.060 --> 01:08:50.860]   Not MDow.
[01:08:50.860 --> 01:08:54.540]   No, no, it's a material adverse.
[01:08:55.420 --> 01:08:56.140]   Oh, oh.
[01:08:56.140 --> 01:08:56.700]   Event.
[01:08:56.700 --> 01:08:57.180]   Event.
[01:08:57.180 --> 01:08:57.980]   Oh, okay.
[01:08:57.980 --> 01:08:59.500]   This is why you're not involved.
[01:08:59.500 --> 01:09:00.940]   This is why you're not involved.
[01:09:00.940 --> 01:09:01.740]   This is why you're not involved.
[01:09:01.740 --> 01:09:03.820]   The thing that gets him out of that contract.
[01:09:03.820 --> 01:09:07.340]   With no uncertainty is an M-A-E.
[01:09:07.340 --> 01:09:10.140]   If if there's a material adverse effect,
[01:09:10.140 --> 01:09:15.500]   then the bid he made for Twitter can be abrogated because it doesn't,
[01:09:15.500 --> 01:09:18.140]   you know, you know, if Twitter suddenly says,
[01:09:18.140 --> 01:09:20.780]   oh, it turns out we stole all our code.
[01:09:20.780 --> 01:09:23.180]   And we have to go out of business.
[01:09:23.180 --> 01:09:23.740]   That's an M-A-E.
[01:09:23.740 --> 01:09:26.780]   It's like when HP bought autonomy.
[01:09:26.780 --> 01:09:27.420]   Exactly.
[01:09:27.420 --> 01:09:30.540]   Then autonomy was like, I cooked my books for however many years.
[01:09:30.540 --> 01:09:31.820]   Yeah, we made it all up.
[01:09:31.820 --> 01:09:35.820]   So there is some thinking that Zacco's revelations
[01:09:35.820 --> 01:09:39.900]   qualify as a material adverse event.
[01:09:39.900 --> 01:09:43.500]   And so Elon's now out of off the hook.
[01:09:43.500 --> 01:09:44.540]   Whether Zacco is--
[01:09:44.540 --> 01:09:47.500]   Well, but that's determined by a court or a lawyer.
[01:09:47.500 --> 01:09:48.220]   Yeah, yeah, yeah.
[01:09:48.220 --> 01:09:48.700]   It's not like--
[01:09:48.700 --> 01:09:51.420]   Do I know where a court of chanceery has to say so.
[01:09:51.420 --> 01:09:53.420]   But it's just you can make a strong case.
[01:09:53.420 --> 01:09:53.980]   You can drop--
[01:09:53.980 --> 01:09:56.700]   If Mosk solicited him to do this.
[01:09:56.700 --> 01:09:57.340]   Yeah, he did.
[01:09:57.340 --> 01:09:57.820]   I don't know.
[01:09:57.820 --> 01:09:58.220]   I think that's--
[01:09:58.220 --> 01:09:59.260]   Oh, that was dodgy.
[01:09:59.260 --> 01:10:00.060]   He got to prove that.
[01:10:00.060 --> 01:10:01.260]   Yeah, you have to prove that.
[01:10:01.260 --> 01:10:03.100]   But I think that--
[01:10:03.100 --> 01:10:03.820]   Discovery.
[01:10:03.820 --> 01:10:05.740]   Notice that Elon is now throwing out--
[01:10:05.740 --> 01:10:09.340]   He's now throwing out the spam bots.
[01:10:09.340 --> 01:10:11.420]   He's now throwing out the MDOWs.
[01:10:11.420 --> 01:10:13.820]   He says, let's step the pose this guy,
[01:10:13.820 --> 01:10:15.900]   because I think we've got an M-A here.
[01:10:15.900 --> 01:10:16.940]   That's what I think.
[01:10:16.940 --> 01:10:19.260]   Yeah, it's the force majeure of like--
[01:10:19.260 --> 01:10:19.740]   It is.
[01:10:19.740 --> 01:10:20.700]   Yeah, an act of God.
[01:10:20.700 --> 01:10:21.420]   Yep.
[01:10:21.420 --> 01:10:22.620]   An act of Zacko.
[01:10:22.620 --> 01:10:24.700]   An act of much, actually, is what it is.
[01:10:24.700 --> 01:10:25.340]   Act of much.
[01:10:25.340 --> 01:10:26.380]   An act of much.
[01:10:26.380 --> 01:10:28.780]   Are you all going to set up Twitter circles?
[01:10:28.780 --> 01:10:31.740]   Of course, majeure.
[01:10:31.740 --> 01:10:32.540]   Oh, my God.
[01:10:32.540 --> 01:10:33.180]   [LAUGHTER]
[01:10:33.180 --> 01:10:34.540]   Force majeure!
[01:10:34.540 --> 01:10:35.820]   Oh, shoot.
[01:10:35.820 --> 01:10:36.780]   Oh, gee.
[01:10:36.780 --> 01:10:37.900]   You just won the--
[01:10:37.900 --> 01:10:38.060]   Okay, okay.
[01:10:38.060 --> 01:10:39.020]   You're just still not the title.
[01:10:39.020 --> 01:10:40.220]   You just won the internet.
[01:10:40.220 --> 01:10:41.500]   No, no, that's pretty darn good.
[01:10:41.500 --> 01:10:43.820]   Like that one down, a force majeure.
[01:10:43.820 --> 01:10:44.780]   [LAUGHTER]
[01:10:44.780 --> 01:10:45.980]   I'll see you in a bit.
[01:10:45.980 --> 01:10:47.660]   Talk about some more.
[01:10:47.660 --> 01:10:50.300]   Oh, on that note, she sticks it up here.
[01:10:50.300 --> 01:10:50.780]   Yes.
[01:10:50.780 --> 01:10:51.740]   That's a mic drop.
[01:10:51.740 --> 01:10:53.660]   Just so you know that--
[01:10:53.660 --> 01:10:54.700]   I did not do this.
[01:10:54.700 --> 01:10:56.300]   She was going to--
[01:10:56.300 --> 01:10:57.660]   Stacy has to go get her kit.
[01:10:57.660 --> 01:10:59.100]   She was going to do this anyway.
[01:10:59.100 --> 01:11:00.460]   We all knew about it.
[01:11:00.460 --> 01:11:01.420]   It's not because of me.
[01:11:01.420 --> 01:11:02.140]   Don't blame me.
[01:11:02.140 --> 01:11:04.220]   No, it was a mic drop.
[01:11:04.220 --> 01:11:05.020]   I think that's right.
[01:11:05.020 --> 01:11:05.420]   It was--
[01:11:05.420 --> 01:11:05.980]   How much is that?
[01:11:05.980 --> 01:11:06.380]   How much is that?
[01:11:06.380 --> 01:11:06.540]   It's a huge time of the year.
[01:11:06.540 --> 01:11:09.100]   Twitter circle.
[01:11:09.100 --> 01:11:10.220]   I like this idea.
[01:11:10.220 --> 01:11:12.220]   You could pick up to 150 people
[01:11:12.220 --> 01:11:15.420]   that is your circle and tweet only to them
[01:11:15.420 --> 01:11:17.740]   and only they can respond and only they can read.
[01:11:17.740 --> 01:11:18.700]   Yeah.
[01:11:19.580 --> 01:11:20.700]   And it's a good idea.
[01:11:20.700 --> 01:11:23.020]   It makes Twitter a social network again.
[01:11:23.020 --> 01:11:24.300]   We've been talking about how
[01:11:24.300 --> 01:11:25.900]   the world is moving away from social
[01:11:25.900 --> 01:11:27.420]   towards creators.
[01:11:27.420 --> 01:11:29.420]   That's pretty good, right?
[01:11:29.420 --> 01:11:30.940]   Yeah, that's a good idea.
[01:11:30.940 --> 01:11:34.460]   I'm not saying I will do this, at least--
[01:11:34.460 --> 01:11:35.180]   You know why?
[01:11:35.180 --> 01:11:36.300]   I don't see the reason to,
[01:11:36.300 --> 01:11:37.740]   but the only downside,
[01:11:37.740 --> 01:11:38.860]   you've got to make this list.
[01:11:38.860 --> 01:11:40.860]   Yeah, that's a pain in the butt.
[01:11:40.860 --> 01:11:42.380]   It's such a pain in the ass.
[01:11:42.380 --> 01:11:43.340]   Who's in my circle?
[01:11:43.340 --> 01:11:44.460]   Who's not in my circle?
[01:11:44.460 --> 01:11:45.100]   Well, remember what?
[01:11:45.100 --> 01:11:46.540]   That wasn't the early days of Facebook
[01:11:46.540 --> 01:11:48.380]   when you created groups of people.
[01:11:48.380 --> 01:11:48.620]   Right?
[01:11:48.620 --> 01:11:49.100]   Remember that?
[01:11:49.100 --> 01:11:49.980]   Yeah, it failed.
[01:11:49.980 --> 01:11:50.780]   Same thing with--
[01:11:50.780 --> 01:11:51.580]   Visibly, right?
[01:11:51.580 --> 01:11:52.220]   Visibly, right?
[01:11:52.220 --> 01:11:52.780]   Yeah.
[01:11:52.780 --> 01:11:53.180]   Right?
[01:11:53.180 --> 01:11:53.340]   Yeah.
[01:11:53.340 --> 01:11:53.740]   Circles.
[01:11:53.740 --> 01:11:54.700]   In fact, that was the term--
[01:11:54.700 --> 01:11:54.860]   Circles.
[01:11:54.860 --> 01:11:55.500]   They used.
[01:11:55.500 --> 01:11:56.540]   Yeah.
[01:11:56.540 --> 01:11:59.020]   You know, the one thing missing,
[01:11:59.020 --> 01:12:00.060]   if you could monetize,
[01:12:00.060 --> 01:12:00.700]   if you could say,
[01:12:00.700 --> 01:12:03.340]   I want to--
[01:12:03.340 --> 01:12:05.980]   If you pay me 100 bucks a month,
[01:12:05.980 --> 01:12:07.100]   you can be in my circle.
[01:12:07.100 --> 01:12:08.940]   I was thinking about this,
[01:12:08.940 --> 01:12:11.020]   and I will tweet to you and you alone
[01:12:11.020 --> 01:12:13.340]   the most important,
[01:12:13.340 --> 01:12:15.180]   relevant stories of the day,
[01:12:15.180 --> 01:12:16.380]   every single day.
[01:12:16.380 --> 01:12:18.060]   Doesn't this seem like Twitter already
[01:12:18.060 --> 01:12:19.100]   has something like that
[01:12:19.100 --> 01:12:20.700]   in their monetization plan?
[01:12:20.700 --> 01:12:22.540]   Well, they were thinking of--
[01:12:22.540 --> 01:12:23.500]   Like subscribers.
[01:12:23.500 --> 01:12:24.620]   They were thinking of doing--
[01:12:24.620 --> 01:12:25.180]   Some kind of a describe tweet,
[01:12:25.180 --> 01:12:25.580]   so some kind of a tag scene.
[01:12:25.580 --> 01:12:26.460]   Twitter only fans.
[01:12:26.460 --> 01:12:27.260]   Did you see that?
[01:12:27.260 --> 01:12:29.100]   Super follows.
[01:12:29.100 --> 01:12:29.660]   That's what I was thinking about.
[01:12:29.660 --> 01:12:30.460]   Super follows.
[01:12:30.460 --> 01:12:30.860]   Yeah.
[01:12:30.860 --> 01:12:31.260]   Twitter--
[01:12:31.260 --> 01:12:35.900]   According to internal documents,
[01:12:35.900 --> 01:12:38.140]   in the spring of this year,
[01:12:38.140 --> 01:12:38.940]   Twitter considered
[01:12:38.940 --> 01:12:42.060]   making a radical change to its platform.
[01:12:42.060 --> 01:12:44.060]   This is a scoop from the Verge.
[01:12:44.060 --> 01:12:47.260]   After years of quietly allowing adult content,
[01:12:47.260 --> 01:12:49.020]   the company would monetize it,
[01:12:49.020 --> 01:12:50.460]   give adult content creators
[01:12:50.460 --> 01:12:50.860]   e.b.
[01:12:50.860 --> 01:12:54.220]   Ability to Sell only fan style paid subscriptions
[01:12:54.220 --> 01:12:57.420]   with Twitter keeping some of the revenue.
[01:12:57.420 --> 01:13:00.860]   Now, you can imagine the
[01:13:00.860 --> 01:13:02.460]   fewer that would have created.
[01:13:02.460 --> 01:13:05.660]   Maybe we could thank Elon for that not happening.
[01:13:05.660 --> 01:13:09.020]   The Verge says,
[01:13:09.020 --> 01:13:11.500]   "Before the final go ahead to launch,
[01:13:11.500 --> 01:13:13.980]   Twitter convened 84 employees
[01:13:13.980 --> 01:13:15.500]   to form a red team
[01:13:15.500 --> 01:13:17.740]   to pressure test the decision
[01:13:17.740 --> 01:13:18.860]   to allow adult creators
[01:13:18.860 --> 01:13:20.780]   to monetize on the platform
[01:13:20.780 --> 01:13:22.220]   by specifically focusing on
[01:13:22.220 --> 01:13:24.380]   what it would look like for Twitter to do this
[01:13:24.380 --> 01:13:26.220]   safely and responsibly."
[01:13:26.220 --> 01:13:28.540]   They knew that they would lose advertising support,
[01:13:28.540 --> 01:13:30.220]   but they figured they'd make more money.
[01:13:30.220 --> 01:13:31.900]   Make it up and they're not
[01:13:31.900 --> 01:13:32.540]   an additional--
[01:13:32.540 --> 01:13:34.060]   The advertising wasn't working so well.
[01:13:34.060 --> 01:13:34.620]   Yeah.
[01:13:34.620 --> 01:13:38.940]   I mean, only fans is projecting
[01:13:38.940 --> 01:13:41.580]   2.5 billion in revenue this year,
[01:13:41.580 --> 01:13:43.100]   so it's not a bad direction.
[01:13:43.900 --> 01:13:45.820]   The red team said, though,
[01:13:45.820 --> 01:13:47.180]   they gave-- they brought the bad news.
[01:13:47.180 --> 01:13:52.460]   Among everything else we can't figure out,
[01:13:52.460 --> 01:13:53.500]   we can't figure out
[01:13:53.500 --> 01:13:55.660]   if it's-- if there's non-consensual nudity
[01:13:55.660 --> 01:13:57.020]   or CSAM on our site.
[01:13:57.020 --> 01:13:59.660]   And since we can't verify that,
[01:13:59.660 --> 01:14:03.580]   we can't verify creators and consumers
[01:14:03.580 --> 01:14:05.340]   of adult content are of legal age,
[01:14:05.340 --> 01:14:06.860]   we can't do this.
[01:14:06.860 --> 01:14:10.460]   Saved by the bell on site.
[01:14:10.460 --> 01:14:11.980]   Saved by Elon Musk,
[01:14:11.980 --> 01:14:15.180]   who then literally two weeks before
[01:14:15.180 --> 01:14:17.180]   said, "I'll buy the company,"
[01:14:17.180 --> 01:14:18.380]   then they immediately said,
[01:14:18.380 --> 01:14:19.500]   "Okay, never mind.
[01:14:19.500 --> 01:14:21.740]   Forget we even-- forget we even mentioned that."
[01:14:21.740 --> 01:14:22.860]   Put your clothes back on.
[01:14:22.860 --> 01:14:23.660]   Put your clothes back.
[01:14:23.660 --> 01:14:29.980]   Did we talk last week about Jack Dorsey's regret that Twitter--
[01:14:29.980 --> 01:14:30.700]   No, we did not.
[01:14:30.700 --> 01:14:33.340]   --that he made the biggest issue
[01:14:33.340 --> 01:14:36.300]   in my biggest regret he tweeted when asked,
[01:14:36.300 --> 01:14:37.900]   "What's your biggest regret?"
[01:14:37.900 --> 01:14:41.180]   You might imagine Jack Dorsey having many regrets.
[01:14:41.980 --> 01:14:43.980]   But his biggest regret was kind of surprising,
[01:14:43.980 --> 01:14:45.980]   is that Twitter became a company,
[01:14:45.980 --> 01:14:48.220]   a company which made--
[01:14:48.220 --> 01:14:49.180]   He kind of said this before
[01:14:49.180 --> 01:14:51.740]   where a very centralization of the net
[01:14:51.740 --> 01:14:53.100]   was the problem, but he regretted that.
[01:14:53.100 --> 01:14:55.500]   But now we got down to it
[01:14:55.500 --> 01:14:56.940]   to say that it should not be a company,
[01:14:56.940 --> 01:14:58.140]   it should be instead.
[01:14:58.140 --> 01:14:59.740]   He said it should be a protocol.
[01:14:59.740 --> 01:15:00.700]   A protocol.
[01:15:00.700 --> 01:15:03.180]   Definitely can't be owned by a state or a company.
[01:15:03.180 --> 01:15:07.100]   And he says, "It's becoming clear to me every day."
[01:15:07.100 --> 01:15:10.140]   Now that I got my billion dollars--
[01:15:10.140 --> 01:15:10.940]   Yeah.
[01:15:10.940 --> 01:15:11.420]   Yeah.
[01:15:11.420 --> 01:15:15.340]   And now that I've been subpoenaed into the case, by the way.
[01:15:15.340 --> 01:15:17.980]   Well, I agree with him.
[01:15:17.980 --> 01:15:24.780]   In fact, Kevin Marks said, "Yeah, that was exactly our idea."
[01:15:24.780 --> 01:15:27.420]   Kevin tweeted, "We were treating it as a protocol
[01:15:27.420 --> 01:15:32.140]   right up until Chirp when Twitter rug pulled that Chirp
[01:15:32.140 --> 01:15:33.660]   was a third party, I think a third party--
[01:15:33.660 --> 01:15:34.940]   Oh, yeah, I remember Chirp.
[01:15:34.940 --> 01:15:35.980]   Twitter client.
[01:15:35.980 --> 01:15:39.340]   "And killed all third party clients and most apps."
[01:15:40.220 --> 01:15:42.140]   That was the bad mistake right there.
[01:15:42.140 --> 01:15:43.020]   That was the cliff.
[01:15:43.020 --> 01:15:46.060]   Twitter decided, "No, we don't want you to--
[01:15:46.060 --> 01:15:48.860]   in fact, your friend Bill Gross."
[01:15:48.860 --> 01:15:49.580]   Bill Gross.
[01:15:49.580 --> 01:15:51.100]   Tell us the Bill Gross story.
[01:15:51.100 --> 01:15:53.740]   So Bill was starting all kinds of companies
[01:15:53.740 --> 01:15:55.980]   and great things around Twitter,
[01:15:55.980 --> 01:15:57.020]   because he believed in Twitter,
[01:15:57.020 --> 01:15:57.740]   I thought it was great.
[01:15:57.740 --> 01:16:00.380]   And tweeted it.
[01:16:00.380 --> 01:16:02.860]   Treated it like a protocol.
[01:16:02.860 --> 01:16:06.940]   Because they had APIs, just like Facebook in the early days,
[01:16:06.940 --> 01:16:09.500]   valued developers too, because they grew on them,
[01:16:09.500 --> 01:16:11.820]   and then threw them off the boat.
[01:16:11.820 --> 01:16:14.860]   And so Bill started very companies.
[01:16:14.860 --> 01:16:18.060]   I actually invested in one, made that investment rest in peace,
[01:16:18.060 --> 01:16:18.300]   I think.
[01:16:18.300 --> 01:16:20.140]   No, actually it's a restaurant, I think.
[01:16:20.140 --> 01:16:22.540]   But it morphed and pivoted into about a hundred different things.
[01:16:22.540 --> 01:16:25.100]   He started advertising companies,
[01:16:25.100 --> 01:16:26.940]   Twitter, other things.
[01:16:26.940 --> 01:16:29.580]   Remember, the Twitter search started not at Twitter,
[01:16:29.580 --> 01:16:31.100]   but was bought by Twitter.
[01:16:31.100 --> 01:16:32.940]   So then he was going to buy Tweet Deck.
[01:16:32.940 --> 01:16:33.420]   Right.
[01:16:33.420 --> 01:16:35.500]   And that's when Twitter swept in and said,
[01:16:35.500 --> 01:16:37.340]   "No, no, no, nobody can have any APIs.
[01:16:37.340 --> 01:16:38.940]   We're going to buy the only one that's out there at Tweet Deck,
[01:16:38.940 --> 01:16:40.220]   and nobody else can do this now."
[01:16:40.220 --> 01:16:41.820]   Here from the Wayback Machine,
[01:16:41.820 --> 01:16:47.820]   this is also from Kevin's tweet a link to a Twitter tool store,
[01:16:47.820 --> 01:16:53.580]   there were all these third party tools that you could use.
[01:16:53.580 --> 01:16:54.300]   You remember these?
[01:16:54.300 --> 01:16:55.340]   Memory.
[01:16:55.340 --> 01:16:57.500]   Yeah, Tweet Deck.
[01:16:57.500 --> 01:17:01.500]   Yeah, but you could use with Twitter to make Twitter better.
[01:17:01.500 --> 01:17:03.740]   And oh, that would be so great.
[01:17:03.740 --> 01:17:06.940]   So in other words, Jack, you killed that idea.
[01:17:06.940 --> 01:17:09.580]   Maybe now he regrets.
[01:17:09.580 --> 01:17:10.460]   You regret such?
[01:17:10.460 --> 01:17:10.700]   Yeah.
[01:17:10.700 --> 01:17:12.860]   Now that he has--
[01:17:12.860 --> 01:17:13.660]   So he talks.
[01:17:13.660 --> 01:17:14.060]   Oh, money.
[01:17:14.060 --> 01:17:14.620]   Letting talks.
[01:17:14.620 --> 01:17:15.260]   Come on.
[01:17:15.260 --> 01:17:19.740]   In the, well, boards talk too, and shareholders talk,
[01:17:19.740 --> 01:17:21.740]   and investors talk, and VCs talk.
[01:17:21.740 --> 01:17:24.460]   It's true, once you go public, it's hard for you to say,
[01:17:24.460 --> 01:17:25.820]   "Oh, yeah, we're not going to make any money,
[01:17:25.820 --> 01:17:27.340]   we're just going to be a protocol."
[01:17:27.340 --> 01:17:28.140]   You can't do that.
[01:17:28.140 --> 01:17:30.140]   Well, well, who did?
[01:17:30.140 --> 01:17:30.540]   Who did?
[01:17:30.540 --> 01:17:31.740]   Who?
[01:17:31.740 --> 01:17:32.220]   WordPress.
[01:17:33.100 --> 01:17:34.780]   God bless Matt Mullenwig.
[01:17:34.780 --> 01:17:38.860]   Matt Mullenwig, and I got called by Polaris VCs
[01:17:38.860 --> 01:17:40.140]   when they were going to invest in WordPress,
[01:17:40.140 --> 01:17:42.140]   and they said, "We're just trying to figure out
[01:17:42.140 --> 01:17:45.900]   this crazy idea he has to have an open source piece,
[01:17:45.900 --> 01:17:46.860]   and then build on top of it."
[01:17:46.860 --> 01:17:48.380]   And I said, "It's brilliant.
[01:17:48.380 --> 01:17:49.180]   It's amazing."
[01:17:49.180 --> 01:17:53.100]   And it shows how six apart went wrong
[01:17:53.100 --> 01:17:54.780]   by trying to control everything,
[01:17:54.780 --> 01:17:56.220]   and that's when they fell apart,
[01:17:56.220 --> 01:17:58.060]   is when they said, "No, no, no, you can't have that many customers
[01:17:58.060 --> 01:17:59.500]   because you can't compete with us."
[01:17:59.500 --> 01:18:02.700]   And WordPress said, "No, no, no, we're going to enable competitors
[01:18:02.700 --> 01:18:05.980]   so we can grow, so we can get more open source coding."
[01:18:05.980 --> 01:18:08.140]   And now is a third of the pages on the Internet,
[01:18:08.140 --> 01:18:09.180]   so we're trying to serve WordPress.
[01:18:09.180 --> 01:18:10.860]   It worked beautifully.
[01:18:10.860 --> 01:18:11.580]   Maybe there was a great--
[01:18:11.580 --> 01:18:11.980]   Maybe there was a great--
[01:18:11.980 --> 01:18:12.940]   Maybe Jack could have done it.
[01:18:12.940 --> 01:18:13.980]   I think that's the model.
[01:18:13.980 --> 01:18:18.780]   I think WordPress very quietly shows us a model of what can be.
[01:18:18.780 --> 01:18:19.820]   I've always loved Matt.
[01:18:19.820 --> 01:18:21.580]   Matt is brilliant.
[01:18:21.580 --> 01:18:22.540]   Matt Mullenwig, yep.
[01:18:22.540 --> 01:18:25.100]   And my son is applying for a job there right now,
[01:18:25.100 --> 01:18:26.460]   so I'm not saying that because of that, Matt.
[01:18:26.460 --> 01:18:27.180]   But no, good.
[01:18:27.180 --> 01:18:30.780]   And as you know, WordPress was for a long time a sponsor.
[01:18:31.980 --> 01:18:33.340]   Matt's always been a big supporter.
[01:18:33.340 --> 01:18:36.620]   I've known Matt since WordPress started, I think.
[01:18:36.620 --> 01:18:41.820]   I think we interviewed him on "Tryingulation."
[01:18:41.820 --> 01:18:43.100]   Can we get him on, Twig?
[01:18:43.100 --> 01:18:43.340]   Yeah.
[01:18:43.340 --> 01:18:44.380]   It'd be great.
[01:18:44.380 --> 01:18:44.860]   Let's try.
[01:18:44.860 --> 01:18:45.260]   I love that.
[01:18:45.260 --> 01:18:45.980]   It's a great idea.
[01:18:45.980 --> 01:18:46.380]   Yeah.
[01:18:46.380 --> 01:18:49.580]   I think just to talk about the Twitter right now,
[01:18:49.580 --> 01:18:50.380]   it would be fascinating.
[01:18:50.380 --> 01:18:53.900]   Now, the way, Matt could be the one to buy Twitter
[01:18:53.900 --> 01:18:54.940]   and save us from all this.
[01:18:54.940 --> 01:18:58.140]   I don't think he's got $44 billion.
[01:18:58.140 --> 01:18:59.980]   Well, it's not going to be $44 billion.
[01:19:00.940 --> 01:19:02.300]   Whatever's going to happen is going to happen.
[01:19:02.300 --> 01:19:04.060]   And the price is going to come way down.
[01:19:04.060 --> 01:19:05.740]   And then somebody's going to rescue them at that price.
[01:19:05.740 --> 01:19:09.580]   Well, that's an interesting idea.
[01:19:09.580 --> 01:19:10.060]   Yeah.
[01:19:10.060 --> 01:19:10.540]   Yeah.
[01:19:10.540 --> 01:19:11.980]   The purchase is not going to happen.
[01:19:11.980 --> 01:19:13.980]   Either Musk is going to be able to back out
[01:19:13.980 --> 01:19:16.460]   because of the MBBA or whatever use it was,
[01:19:16.460 --> 01:19:17.980]   the force majority,
[01:19:17.980 --> 01:19:19.500]   Madrid.
[01:19:19.500 --> 01:19:23.900]   Or he gets out but has to settle
[01:19:23.900 --> 01:19:25.100]   and they get a few billion bucks,
[01:19:25.100 --> 01:19:25.980]   but they still,
[01:19:25.980 --> 01:19:27.820]   but still the price is going to go down.
[01:19:27.820 --> 01:19:29.740]   Matt was a guest in 2011.
[01:19:29.740 --> 01:19:33.020]   On this weekend tech number 291 titled eggs,
[01:19:33.020 --> 01:19:34.140]   the silent killer.
[01:19:34.140 --> 01:19:35.500]   I don't know.
[01:19:35.500 --> 01:19:38.060]   And you know who else was on that show?
[01:19:38.060 --> 01:19:39.660]   Joshua Topolsky,
[01:19:39.660 --> 01:19:41.420]   John C. Devorek and Gina Smith.
[01:19:41.420 --> 01:19:43.100]   That was an old timer.
[01:19:43.100 --> 01:19:43.900]   Topolsky.
[01:19:43.900 --> 01:19:45.580]   Yeah, Topolsky now.
[01:19:45.580 --> 01:19:45.980]   He was,
[01:19:45.980 --> 01:19:49.740]   he's so he founded a gadget, right?
[01:19:49.740 --> 01:19:53.100]   And then yeah, I think this is my next after they left.
[01:19:53.100 --> 01:19:55.100]   My play just just a little,
[01:19:55.100 --> 01:19:56.780]   because you can't take yourself down.
[01:19:57.900 --> 01:19:59.900]   I could, I could certainly look like.
[01:19:59.900 --> 01:20:00.460]   Let me see if,
[01:20:00.460 --> 01:20:02.780]   here's, uh, here's John.
[01:20:02.780 --> 01:20:03.900]   Wow.
[01:20:03.900 --> 01:20:04.860]   Let me, uh,
[01:20:04.860 --> 01:20:06.300]   this is in the old, uh,
[01:20:06.300 --> 01:20:08.380]   the old cottage cottage.
[01:20:08.380 --> 01:20:10.940]   There's Gina, there's John wearing a mouse cap.
[01:20:10.940 --> 01:20:11.500]   Sounds Gina Smith.
[01:20:11.500 --> 01:20:11.900]   Yeah.
[01:20:11.900 --> 01:20:13.340]   And he made me say.
[01:20:13.340 --> 01:20:14.220]   Now where's Matt?
[01:20:14.220 --> 01:20:14.780]   You wouldn't be.
[01:20:14.780 --> 01:20:15.660]   No, there's Joshua.
[01:20:15.660 --> 01:20:16.380]   Joshua Topolsky.
[01:20:16.380 --> 01:20:17.260]   There's Gina.
[01:20:17.260 --> 01:20:18.860]   There's Devorek.
[01:20:18.860 --> 01:20:20.060]   No, it's exactly.
[01:20:20.060 --> 01:20:20.540]   I don't know.
[01:20:20.540 --> 01:20:22.060]   Maybe Matt's.
[01:20:22.060 --> 01:20:23.420]   There's a great podcast he got.
[01:20:23.420 --> 01:20:24.380]   He's got Dan too.
[01:20:24.380 --> 01:20:25.420]   But he should listen to it.
[01:20:25.420 --> 01:20:25.980]   It is not.
[01:20:25.980 --> 01:20:27.500]   He's a man a few words, Matt.
[01:20:27.500 --> 01:20:27.740]   Came.
[01:20:27.740 --> 01:20:28.300]   There he is.
[01:20:28.300 --> 01:20:28.940]   There he is.
[01:20:28.940 --> 01:20:29.420]   There he is.
[01:20:29.420 --> 01:20:33.580]   And that it was perhaps tied to a blog
[01:20:33.580 --> 01:20:35.100]   that the Chinese government.
[01:20:35.100 --> 01:20:35.740]   He wants,
[01:20:35.740 --> 01:20:36.860]   I don't know why Matt was on.
[01:20:36.860 --> 01:20:37.740]   Was he there as a,
[01:20:37.740 --> 01:20:38.700]   oh look, we're drinking.
[01:20:38.700 --> 01:20:40.780]   He's a drinking friend.
[01:20:40.780 --> 01:20:43.740]   Yeah, let's give him back all of his belongings.
[01:20:43.740 --> 01:20:45.020]   Let's get Matt Mullenweg.
[01:20:45.020 --> 01:20:45.900]   Yeah, so we're adding.
[01:20:45.900 --> 01:20:49.740]   This is when I was doing it all myself.
[01:20:49.740 --> 01:20:50.780]   Even I think editing.
[01:20:50.780 --> 01:20:51.580]   Yeah, I know it's amazing.
[01:20:51.580 --> 01:20:52.140]   You could do that.
[01:20:52.140 --> 01:20:53.260]   Do you know each other?
[01:20:53.260 --> 01:20:53.820]   We do now.
[01:20:53.820 --> 01:20:54.940]   Would you like a glass of wine?
[01:20:54.940 --> 01:20:56.700]   I've known Matt before he had
[01:20:56.700 --> 01:20:57.500]   crazy beer.
[01:20:57.500 --> 01:20:58.220]   I like the beard.
[01:20:58.220 --> 01:20:59.260]   Yeah, he's good.
[01:20:59.260 --> 01:21:00.460]   He's brand new to work.
[01:21:00.460 --> 01:21:01.340]   He's brand new.
[01:21:01.340 --> 01:21:02.460]   This is yesterday.
[01:21:02.460 --> 01:21:03.020]   It's actually cool.
[01:21:03.020 --> 01:21:03.900]   Always cry to you.
[01:21:03.900 --> 01:21:04.380]   Oh yeah.
[01:21:04.380 --> 01:21:05.660]   No, we're talking about Matt Mullen.
[01:21:05.660 --> 01:21:06.860]   All right, let's not talk about
[01:21:06.860 --> 01:21:07.820]   a porn anymore.
[01:21:07.820 --> 01:21:09.100]   We got to stay these back.
[01:21:09.100 --> 01:21:12.620]   So, let's take a break.
[01:21:12.620 --> 01:21:17.820]   This week at Google brought to you by ClickUp.
[01:21:17.820 --> 01:21:19.980]   Oh, I love ClickUp.
[01:21:19.980 --> 01:21:21.420]   Imagine if you could get,
[01:21:21.420 --> 01:21:23.420]   in your work week,
[01:21:23.420 --> 01:21:26.300]   one day off extra every week.
[01:21:27.260 --> 01:21:27.900]   What would you do?
[01:21:27.900 --> 01:21:30.300]   I would just stay in bed all day.
[01:21:30.300 --> 01:21:31.980]   Maybe you'd want to cook
[01:21:31.980 --> 01:21:33.980]   healthy meals or work on that novel.
[01:21:33.980 --> 01:21:36.860]   Just watch some good reality TV.
[01:21:36.860 --> 01:21:39.420]   That's what ClickUp gives you.
[01:21:39.420 --> 01:21:40.860]   The productivity platform
[01:21:40.860 --> 01:21:44.300]   that promises to save you one day
[01:21:44.300 --> 01:21:45.500]   of work a week.
[01:21:45.500 --> 01:21:46.140]   Guaranteed.
[01:21:46.140 --> 01:21:48.300]   ClickUp was started.
[01:21:48.300 --> 01:21:49.740]   Again, this is my theory.
[01:21:49.740 --> 01:21:52.620]   Some great developers said,
[01:21:52.620 --> 01:21:53.900]   I want to scratch my own itch.
[01:21:55.020 --> 01:21:57.180]   Decided productivity is broken.
[01:21:57.180 --> 01:21:59.180]   There are too many tools to keep track of,
[01:21:59.180 --> 01:22:01.980]   too many things in entirely separate ecosystems.
[01:22:01.980 --> 01:22:03.980]   There's got to be a better way
[01:22:03.980 --> 01:22:05.660]   to get through the daily hustle.
[01:22:05.660 --> 01:22:07.180]   And ClickUp was born.
[01:22:07.180 --> 01:22:10.220]   One tool to house all your tasks,
[01:22:10.220 --> 01:22:11.020]   all your projects,
[01:22:11.020 --> 01:22:11.660]   all your docs,
[01:22:11.660 --> 01:22:13.260]   goals, spreadsheets, everything.
[01:22:13.260 --> 01:22:17.100]   Built for teams from one person to a thousand or more,
[01:22:17.100 --> 01:22:19.580]   people ClickUp is packed with features
[01:22:19.580 --> 01:22:21.020]   and customization options
[01:22:21.020 --> 01:22:23.020]   that no other productivity tool has.
[01:22:23.020 --> 01:22:25.020]   So you can work however you like to work.
[01:22:25.020 --> 01:22:27.740]   You know, ClickUp comes out of the box,
[01:22:27.740 --> 01:22:28.700]   ready to go.
[01:22:28.700 --> 01:22:30.700]   Of course, it's completely customizable.
[01:22:30.700 --> 01:22:32.700]   I mean, you could do anything you want with it.
[01:22:32.700 --> 01:22:34.700]   And then they also have presets
[01:22:34.700 --> 01:22:37.260]   like product for various jobs,
[01:22:37.260 --> 01:22:40.540]   like project management or engineering or sales.
[01:22:40.540 --> 01:22:41.420]   If you're in HR,
[01:22:41.420 --> 01:22:43.260]   there's an HR module marketing.
[01:22:43.260 --> 01:22:46.300]   Easy to use solutions to create a more efficient
[01:22:46.300 --> 01:22:48.460]   work environment.
[01:22:48.460 --> 01:22:49.420]   And it brings all your,
[01:22:49.420 --> 01:22:51.260]   you don't have to abandon your existing tools,
[01:22:51.260 --> 01:22:52.300]   your Zoom, your Slack.
[01:22:52.940 --> 01:22:55.660]   Your Zapier, all of that just goes right into it.
[01:22:55.660 --> 01:22:57.500]   I just think this is brilliant.
[01:22:57.500 --> 01:23:01.180]   Join more than 800,000 highly productive teams
[01:23:01.180 --> 01:23:03.660]   using ClickUp today.
[01:23:03.660 --> 01:23:05.660]   That's a lot 800,000.
[01:23:05.660 --> 01:23:09.660]   Use the code Twig when you go to ClickUp.com
[01:23:09.660 --> 01:23:13.340]   and you'll get 15% off ClickUp's massive
[01:23:13.340 --> 01:23:15.020]   unlimited plan for a year.
[01:23:15.020 --> 01:23:18.220]   So you can start reclaiming that day a week
[01:23:18.220 --> 01:23:20.540]   for under $5 a month.
[01:23:21.420 --> 01:23:22.700]   That's pretty awesome.
[01:23:22.700 --> 01:23:24.220]   Good extra day every week.
[01:23:24.220 --> 01:23:25.580]   Sign up today.
[01:23:25.580 --> 01:23:29.340]   ClickUp.com, use the code TWIG.
[01:23:29.340 --> 01:23:33.020]   15% off gets it down to under $5 a month.
[01:23:33.020 --> 01:23:34.620]   Click up CLICK.
[01:23:34.620 --> 01:23:38.380]   Click up UPUP.com.
[01:23:38.380 --> 01:23:41.980]   Thank you ClickUp for supporting this week in Google.
[01:23:41.980 --> 01:23:42.700]   We love these guys.
[01:23:42.700 --> 01:23:43.580]   It's a great product.
[01:23:43.580 --> 01:23:45.420]   Did you get your daughter?
[01:23:45.420 --> 01:23:46.460]   You're all good?
[01:23:46.460 --> 01:23:50.460]   Nobody can tell Stacy what we were talking about
[01:23:50.460 --> 01:23:51.260]   watching this one.
[01:23:51.260 --> 01:23:52.700]   No, you can't beat it out of me.
[01:23:52.700 --> 01:23:55.820]   Thank goodness for timestamps.
[01:23:55.820 --> 01:23:59.980]   Thank goodness for editors.
[01:23:59.980 --> 01:24:04.620]   So bad news today this morning for SNAP.
[01:24:04.620 --> 01:24:08.380]   During the pandemic, they went crazy.
[01:24:08.380 --> 01:24:09.980]   The parent company of Snapchat.
[01:24:09.980 --> 01:24:14.380]   Hiring more than Amazon,
[01:24:14.380 --> 01:24:16.860]   Metapentress, Twitter, Alphabet, Microsoft, Apple.
[01:24:18.540 --> 01:24:21.740]   Almost doubled in size during the pandemic.
[01:24:21.740 --> 01:24:25.500]   Now they've decided today they announced to shrink
[01:24:25.500 --> 01:24:29.660]   20% of their workforce that will save them half a billion dollars a year.
[01:24:29.660 --> 01:24:36.220]   SNAP had assumed, according to Evan Spiegel,
[01:24:36.220 --> 01:24:38.540]   the CEO, a higher rate of revenue growth.
[01:24:38.540 --> 01:24:40.140]   And that's not what they're seeing.
[01:24:40.140 --> 01:24:42.140]   Now I blame Facebook.
[01:24:42.140 --> 01:24:46.380]   I really do for, but they're still growing 8%
[01:24:46.380 --> 01:24:48.460]   third quarter revenue growth.
[01:24:48.460 --> 01:24:49.980]   That's the slowest ever though.
[01:24:49.980 --> 01:24:53.340]   You blame Facebook and not TikTok and XSS.
[01:24:53.340 --> 01:24:54.540]   Oh, maybe it is TikTok.
[01:24:54.540 --> 01:24:57.420]   SNAP is pulling an Instagram now.
[01:24:57.420 --> 01:24:59.340]   They're adding TikTok like features, right?
[01:24:59.340 --> 01:25:01.740]   They're even everybody's doing be real now too.
[01:25:01.740 --> 01:25:02.300]   I love that.
[01:25:02.300 --> 01:25:03.340]   Oh yeah, that's right.
[01:25:03.340 --> 01:25:04.140]   SNAP just enough.
[01:25:04.140 --> 01:25:07.180]   They're going to have front camera, back camera snaps.
[01:25:07.180 --> 01:25:11.420]   They killed, I bought stupidly the Pixie,
[01:25:11.420 --> 01:25:12.860]   which was their selfie drone.
[01:25:12.860 --> 01:25:13.820]   I know, stop laughing.
[01:25:13.820 --> 01:25:14.860]   You want to told you not to buy?
[01:25:14.860 --> 01:25:16.940]   You told me not to buy 250 bucks.
[01:25:16.940 --> 01:25:19.180]   They literally killed it like a week later.
[01:25:19.180 --> 01:25:22.140]   It's like, well, thank you for the money.
[01:25:22.140 --> 01:25:22.620]   Bye bye.
[01:25:22.620 --> 01:25:26.300]   Could you give it back?
[01:25:26.300 --> 01:25:27.580]   I'm going to try.
[01:25:27.580 --> 01:25:30.860]   Hey, I try to get my Google class back.
[01:25:30.860 --> 01:25:31.180]   Yeah.
[01:25:31.180 --> 01:25:34.140]   No, it's a schmuck tax.
[01:25:34.140 --> 01:25:34.620]   Okay.
[01:25:34.620 --> 01:25:35.420]   Schmuck tax.
[01:25:35.420 --> 01:25:36.620]   It's schmuck.
[01:25:36.620 --> 01:25:37.580]   Here's another title.
[01:25:37.580 --> 01:25:39.340]   There's no title.
[01:25:39.340 --> 01:25:40.780]   I'm paying the schmuck tax.
[01:25:40.780 --> 01:25:41.660]   So many titles.
[01:25:41.660 --> 01:25:43.500]   You're just going to cut this up into five shows,
[01:25:43.500 --> 01:25:44.540]   so we have five titles.
[01:25:44.540 --> 01:25:46.540]   [laughter]
[01:25:46.540 --> 01:25:48.620]   New Jersey.
[01:25:48.620 --> 01:25:50.860]   Very happy.
[01:25:50.860 --> 01:25:52.060]   Remember the Twitter account?
[01:25:52.060 --> 01:25:54.460]   Wow, this is great.
[01:25:54.460 --> 01:25:56.220]   The New Jersey Twitter account,
[01:25:56.220 --> 01:25:57.580]   the one with attitude,
[01:25:57.580 --> 01:26:00.460]   the official Twitter of the Garden State.
[01:26:00.460 --> 01:26:01.820]   Yeah.
[01:26:01.820 --> 01:26:03.660]   Look at that.
[01:26:03.660 --> 01:26:04.620]   Look at the banner.
[01:26:04.620 --> 01:26:06.220]   It's the hand.
[01:26:06.220 --> 01:26:09.500]   What was the best one?
[01:26:09.500 --> 01:26:10.540]   Was it about Christy?
[01:26:10.540 --> 01:26:11.340]   It was your mother.
[01:26:11.340 --> 01:26:13.820]   Who let you have an account?
[01:26:13.820 --> 01:26:14.380]   Your mother.
[01:26:14.380 --> 01:26:17.260]   [laughter]
[01:26:17.260 --> 01:26:18.220]   Jersey Corn.
[01:26:18.220 --> 01:26:19.260]   Jersey Corn.
[01:26:19.260 --> 01:26:21.020]   I can't imagine a more beautiful thing.
[01:26:21.020 --> 01:26:23.740]   And then it's got Tony Soprano saying,
[01:26:23.740 --> 01:26:25.260]   "This was all cornfields here."
[01:26:25.260 --> 01:26:27.420]   Go back, go back like--
[01:26:27.420 --> 01:26:29.100]   I have to go back years practically.
[01:26:29.100 --> 01:26:30.140]   Well, no, no, just go back.
[01:26:30.140 --> 01:26:32.220]   Go back a few months when she was still there.
[01:26:32.220 --> 01:26:33.260]   Oh, is she gone already?
[01:26:33.260 --> 01:26:34.220]   Oh, she--
[01:26:34.220 --> 01:26:35.180]   No, you don't know this.
[01:26:35.180 --> 01:26:36.060]   Oh!
[01:26:36.060 --> 01:26:36.860]   She went to the White House.
[01:26:36.860 --> 01:26:37.420]   Well, I know.
[01:26:37.420 --> 01:26:39.020]   That's the story, but I didn't really--
[01:26:39.020 --> 01:26:39.660]   And?
[01:26:39.660 --> 01:26:40.540]   And?
[01:26:40.540 --> 01:26:42.460]   So in the White House this week,
[01:26:42.460 --> 01:26:44.140]   the way he knows who did this.
[01:26:44.140 --> 01:26:45.660]   But the White House suddenly,
[01:26:45.660 --> 01:26:46.860]   their Twitter account got--
[01:26:46.860 --> 01:26:47.660]   Oh!
[01:26:47.660 --> 01:26:48.700]   --got that too.
[01:26:48.700 --> 01:26:49.180]   Oh!
[01:26:49.180 --> 01:26:49.980]   When they--
[01:26:49.980 --> 01:26:50.460]   When they--
[01:26:50.460 --> 01:26:52.700]   People complained about the student loans,
[01:26:52.700 --> 01:26:54.460]   pointed out all the PPP loans.
[01:26:54.460 --> 01:26:55.980]   Oh, I remember all these Republicans.
[01:26:55.980 --> 01:26:57.100]   All the rich guys got.
[01:26:57.100 --> 01:26:57.820]   And all that--
[01:26:57.820 --> 01:26:59.180]   That the former--
[01:26:59.180 --> 01:27:00.860]   The governor's former tweeter now
[01:27:00.860 --> 01:27:02.860]   in the White House just did a little smiley.
[01:27:02.860 --> 01:27:06.140]   So Megan Cohen, who was the voice of New Jersey,
[01:27:06.140 --> 01:27:08.060]   now at the White House.
[01:27:08.060 --> 01:27:09.180]   And you think she's tweeting?
[01:27:10.460 --> 01:27:12.620]   I mean, presumed, but we don't know.
[01:27:12.620 --> 01:27:13.660]   She was the--
[01:27:13.660 --> 01:27:13.740]   Yes.
[01:27:13.740 --> 01:27:15.180]   She departed last week.
[01:27:15.180 --> 01:27:16.540]   So at the end of July,
[01:27:16.540 --> 01:27:19.740]   as the social media director
[01:27:19.740 --> 01:27:22.780]   for the state of New Jersey,
[01:27:22.780 --> 01:27:24.300]   actually she was a--
[01:27:24.300 --> 01:27:27.180]   Phil Murphy's intern
[01:27:27.180 --> 01:27:28.060]   for a long time.
[01:27:28.060 --> 01:27:28.540]   Right.
[01:27:28.540 --> 01:27:29.180]   Right.
[01:27:29.180 --> 01:27:31.900]   And now she's at the Biden administration
[01:27:31.900 --> 01:27:34.540]   as joining the Office of Digital Strategy
[01:27:34.540 --> 01:27:36.860]   as Deputy Director of Platforms.
[01:27:36.860 --> 01:27:37.980]   Is that just code for--
[01:27:37.980 --> 01:27:39.020]   [LAUGHTER]
[01:27:39.020 --> 01:27:39.500]   Twitter?
[01:27:40.220 --> 01:27:40.780]   Yeah.
[01:27:40.780 --> 01:27:41.100]   I think so.
[01:27:41.100 --> 01:27:41.580]   I think so.
[01:27:41.580 --> 01:27:42.300]   I hope so.
[01:27:42.300 --> 01:27:42.940]   Yeah.
[01:27:42.940 --> 01:27:46.940]   So for instance--
[01:27:46.940 --> 01:27:48.300]   and this is the one you're talking about--
[01:27:48.300 --> 01:27:50.300]   somebody--
[01:27:50.300 --> 01:27:51.420]   she tweeted,
[01:27:51.420 --> 01:27:53.740]   "Who lets New Jersey have a Twitter
[01:27:53.740 --> 01:27:57.660]   tweeted someone with 88 followers
[01:27:57.660 --> 01:27:58.940]   to which she replied,
[01:27:58.940 --> 01:27:59.580]   'Your mom.'
[01:27:59.580 --> 01:28:01.580]   [LAUGHTER]
[01:28:01.580 --> 01:28:04.300]   And got half a million likes and 85,000.
[01:28:04.300 --> 01:28:05.820]   Retweets.
[01:28:05.820 --> 01:28:09.740]   439,000 followers as a result for New Jersey.
[01:28:09.740 --> 01:28:10.380]   He's official.
[01:28:10.380 --> 01:28:12.460]   Twitter account.
[01:28:12.460 --> 01:28:13.420]   Governor Murphy says,
[01:28:13.420 --> 01:28:15.660]   "Megancoin has been an incredibly
[01:28:15.660 --> 01:28:16.780]   valuable member of our team
[01:28:16.780 --> 01:28:18.060]   and our humor and will be greatly
[01:28:18.060 --> 01:28:18.780]   missed in our office."
[01:28:18.780 --> 01:28:19.100]   I bet.
[01:28:19.100 --> 01:28:20.620]   Her passion for our state
[01:28:20.620 --> 01:28:23.260]   and fierce defense of central Jersey.
[01:28:23.260 --> 01:28:24.780]   Why central?
[01:28:24.780 --> 01:28:26.460]   Just central?
[01:28:26.460 --> 01:28:29.260]   Well, see, that's a fight we have all the time.
[01:28:29.260 --> 01:28:30.540]   There are those who say there's only
[01:28:30.540 --> 01:28:31.980]   North Jersey and South Jersey.
[01:28:31.980 --> 01:28:33.740]   There are those of us who live in central Jersey
[01:28:33.740 --> 01:28:35.180]   saying, "No, there is also central Jersey,"
[01:28:35.180 --> 01:28:35.900]   which is where I am,
[01:28:35.900 --> 01:28:37.100]   and it exists, damn it.
[01:28:37.100 --> 01:28:37.900]   OK.
[01:28:37.900 --> 01:28:39.180]   But there are fights about that.
[01:28:39.180 --> 01:28:40.300]   Her defense appears--
[01:28:40.300 --> 01:28:41.420]   Has their theme,
[01:28:41.420 --> 01:28:42.620]   it exists, damn it.
[01:28:42.620 --> 01:28:43.340]   It exists, damn it.
[01:28:43.340 --> 01:28:45.580]   Governor--
[01:28:45.580 --> 01:28:46.220]   The governor said,
[01:28:46.220 --> 01:28:49.340]   "I wish her the best at the White House."
[01:28:49.340 --> 01:28:49.820]   She--
[01:28:49.820 --> 01:28:52.060]   You know, she's going to be a little hampered
[01:28:52.060 --> 01:28:53.420]   because as in New Jersey,
[01:28:53.420 --> 01:28:54.780]   Twitter,
[01:28:54.780 --> 01:28:58.140]   she's had some great New Jersey icons to tweet.
[01:28:58.140 --> 01:28:59.580]   The Sopranos.
[01:28:59.580 --> 01:29:01.340]   I just showed you Tony Soprano.
[01:29:01.340 --> 01:29:03.500]   The Boss, Bon Jovi.
[01:29:03.500 --> 01:29:05.020]   They're all Americans, Leo.
[01:29:05.020 --> 01:29:06.700]   She's picked fights with other states,
[01:29:06.700 --> 01:29:08.780]   especially in defensive New Jersey's pizza
[01:29:08.780 --> 01:29:09.740]   industry.
[01:29:09.740 --> 01:29:11.180]   You might disagree, Jeff.
[01:29:11.180 --> 01:29:14.780]   She's treated the Taylor Ham versus the
[01:29:14.780 --> 01:29:16.220]   pork-roll war fairly.
[01:29:16.220 --> 01:29:17.180]   Big fight.
[01:29:17.180 --> 01:29:17.900]   Big.
[01:29:17.900 --> 01:29:20.060]   Even though she knows the real name is Taylor Ham.
[01:29:20.060 --> 01:29:22.380]   I don't know.
[01:29:22.380 --> 01:29:23.020]   These are all--
[01:29:23.020 --> 01:29:24.300]   These are all Jersey-isms.
[01:29:24.300 --> 01:29:29.340]   Ben Ginkoin will go down in your Jersey history
[01:29:29.340 --> 01:29:31.980]   as the creator and operator of @njgov.
[01:29:31.980 --> 01:29:33.660]   Social accounts that brought hundreds of thousands
[01:29:33.660 --> 01:29:34.540]   of New Jerseyans
[01:29:34.540 --> 01:29:37.020]   in with jokes and memes,
[01:29:37.020 --> 01:29:38.540]   only to follow them up with critical
[01:29:38.540 --> 01:29:39.740]   information about everything from
[01:29:39.740 --> 01:29:41.980]   winter storms to the COVID-19 panic.
[01:29:41.980 --> 01:29:43.500]   I'm sorry, pandemic.
[01:29:43.500 --> 01:29:45.740]   So I hope she tweets for them.
[01:29:45.740 --> 01:29:47.740]   She is--
[01:29:47.740 --> 01:29:50.220]   And this is, of course, from the New Jersey Globe.
[01:29:50.220 --> 01:29:52.940]   Hence the central Jersey snark.
[01:29:52.940 --> 01:29:55.260]   She's the first Livingston High School graduate
[01:29:55.260 --> 01:29:59.900]   to take the Statehouse route to the White House.
[01:29:59.900 --> 01:30:01.740]   Since Robert H. Grady
[01:30:01.740 --> 01:30:06.460]   became the Associate Director of OMB under George H.W. Bush.
[01:30:07.500 --> 01:30:08.620]   So there you go.
[01:30:08.620 --> 01:30:12.860]   Another Livingston native in 2016, Chris Christie.
[01:30:12.860 --> 01:30:14.220]   He did something.
[01:30:14.220 --> 01:30:16.140]   I don't remember what it was, but yeah.
[01:30:16.140 --> 01:30:17.980]   So that's great.
[01:30:17.980 --> 01:30:22.540]   Do you think that she's managing the POTUS account now?
[01:30:22.540 --> 01:30:23.580]   That's why were they hiring her?
[01:30:23.580 --> 01:30:23.980]   Yeah.
[01:30:23.980 --> 01:30:25.580]   I mean, they have input into it.
[01:30:25.580 --> 01:30:29.820]   I'm sure that the other people have a lot of say,
[01:30:29.820 --> 01:30:31.740]   but she's very creative.
[01:30:31.740 --> 01:30:32.700]   Do you stay--
[01:30:32.700 --> 01:30:33.100]   Do you stay--
[01:30:33.100 --> 01:30:33.580]   Do you stay--
[01:30:33.580 --> 01:30:35.100]   Have a Peloton?
[01:30:35.100 --> 01:30:35.980]   Seems like you would.
[01:30:35.980 --> 01:30:37.980]   You're muted.
[01:30:37.980 --> 01:30:39.020]   No, I do not.
[01:30:39.020 --> 01:30:40.460]   You don't.
[01:30:40.460 --> 01:30:45.740]   So I can't blame you for Peloton's financial difficulties.
[01:30:45.740 --> 01:30:48.620]   I didn't buy one, so you could blame me for that.
[01:30:48.620 --> 01:30:50.860]   Well, I bought one, but I have to say,
[01:30:50.860 --> 01:30:51.900]   I am responsible.
[01:30:51.900 --> 01:30:54.540]   During the first six months of 2022,
[01:30:54.540 --> 01:30:58.860]   the average amount of data streamed to at-home fitness bikes
[01:30:58.860 --> 01:31:03.580]   were down 23% compared to a year ago.
[01:31:03.580 --> 01:31:06.060]   People who stopped riding their fitness bikes,
[01:31:06.060 --> 01:31:07.340]   and I think Peloton's--
[01:31:07.340 --> 01:31:08.380]   There's a shock.
[01:31:08.380 --> 01:31:10.060]   Number one, internet connected.
[01:31:10.060 --> 01:31:11.580]   This comes from Plume, by the way.
[01:31:11.580 --> 01:31:13.740]   They were able to--
[01:31:13.740 --> 01:31:16.220]   They want to be the app Annie of Wi-Fi devices.
[01:31:16.220 --> 01:31:20.620]   I mean, is it all through Plume mesh networks?
[01:31:20.620 --> 01:31:21.660]   I guess it must be, right?
[01:31:21.660 --> 01:31:22.300]   Yeah.
[01:31:22.300 --> 01:31:23.660]   Yeah, they're pulling the--
[01:31:23.660 --> 01:31:24.780]   Because if you think about it,
[01:31:24.780 --> 01:31:26.220]   they've got the deal with Comcast.
[01:31:26.220 --> 01:31:28.620]   So if you've got a network extender with Comcast,
[01:31:28.620 --> 01:31:30.940]   it's a Plume, and then they've got a deal with Samsung.
[01:31:30.940 --> 01:31:32.220]   Oh, so they're all over.
[01:31:33.260 --> 01:31:34.620]   So their software's running all over.
[01:31:34.620 --> 01:31:38.460]   Fitness bikes were the number one contraction.
[01:31:38.460 --> 01:31:41.500]   Number two, followed by media players,
[01:31:41.500 --> 01:31:43.020]   Blu-ray players, iPods.
[01:31:43.020 --> 01:31:45.020]   iPods, really?
[01:31:45.020 --> 01:31:45.900]   iPods?
[01:31:45.900 --> 01:31:47.020]   Blu-ray players.
[01:31:47.020 --> 01:31:47.740]   What?
[01:31:47.740 --> 01:31:49.660]   Yeah, I bet there was a decline.
[01:31:49.660 --> 01:31:50.540]   VCRs, way down.
[01:31:50.540 --> 01:31:53.020]   That was a five-pold, and one of their media devices back.
[01:31:53.020 --> 01:31:59.420]   The 14% decline for those PCs down 7% year over year.
[01:31:59.420 --> 01:32:02.220]   I think that's also a move to mobile, right?
[01:32:02.860 --> 01:32:06.380]   Smart TVs saw an increase of 34%.
[01:32:06.380 --> 01:32:10.540]   Smart speakers, 27% year over year.
[01:32:10.540 --> 01:32:12.140]   It's a big growth for those.
[01:32:12.140 --> 01:32:13.260]   Good.
[01:32:13.260 --> 01:32:18.460]   I think Plume measures this data through its own mesh routers,
[01:32:18.460 --> 01:32:21.100]   as well as third-party hardware running its software.
[01:32:21.100 --> 01:32:24.140]   And they have partnerships with ISPs like Comcast,
[01:32:24.140 --> 01:32:25.580]   Jarter, and Vodafone.
[01:32:25.580 --> 01:32:27.980]   So 41 million homes worldwide.
[01:32:27.980 --> 01:32:28.780]   That's pretty good.
[01:32:28.780 --> 01:32:31.180]   Those are going to be useful stats, I think.
[01:32:32.140 --> 01:32:34.300]   Oh, 41 million homes.
[01:32:34.300 --> 01:32:35.020]   Yeah, okay.
[01:32:35.020 --> 01:32:36.300]   That's a good number.
[01:32:36.300 --> 01:32:41.100]   Remember, I mean, Nielsen and Arbitra are like a thousand homes,
[01:32:41.100 --> 01:32:42.460]   you know, for TV ratings.
[01:32:42.460 --> 01:32:43.980]   Yeah, you're right.
[01:32:43.980 --> 01:32:47.900]   Yeah, 41 million seems like a good start anyway.
[01:32:47.900 --> 01:32:52.940]   Speaking of which, Comcast and Charter,
[01:32:52.940 --> 01:32:55.420]   this is from Fast Company,
[01:32:55.420 --> 01:32:58.540]   face a grim new reality actual competition.
[01:33:00.220 --> 01:33:02.780]   And the competition, you like it?
[01:33:02.780 --> 01:33:03.820]   You happy?
[01:33:03.820 --> 01:33:05.980]   Fireless 5G, baby.
[01:33:05.980 --> 01:33:07.420]   I knew this was going to happen.
[01:33:07.420 --> 01:33:08.620]   I was so excited.
[01:33:08.620 --> 01:33:09.740]   We saw it with them.
[01:33:09.740 --> 01:33:14.060]   Are they quoting Bruce Lakeman's stats from a couple of weeks ago?
[01:33:14.060 --> 01:33:16.220]   I'm sure they are.
[01:33:16.220 --> 01:33:17.100]   I don't.
[01:33:17.100 --> 01:33:18.220]   I'm just looking through it.
[01:33:18.220 --> 01:33:22.460]   Basically, their contention is that T-Mobile and Verizon's
[01:33:22.460 --> 01:33:26.620]   residential 5G services are growing fast enough to...
[01:33:26.620 --> 01:33:28.300]   And we talked about this a couple of weeks ago to...
[01:33:28.300 --> 01:33:29.820]   Because your daughter uses it, right?
[01:33:29.820 --> 01:33:33.340]   My daughter uses Verizon and it's 25 bucks a month,
[01:33:33.340 --> 01:33:38.220]   less than a third what Comcast would charge her for better performance.
[01:33:38.220 --> 01:33:41.020]   It's like 150 megabits down, 30 up.
[01:33:41.020 --> 01:33:41.660]   It's great.
[01:33:41.660 --> 01:33:42.220]   Yeah.
[01:33:42.220 --> 01:33:45.020]   Because now that your mileage may vary.
[01:33:45.020 --> 01:33:46.460]   She's near the highway,
[01:33:46.460 --> 01:33:48.860]   which means I think she's near a Verizon cell tower
[01:33:48.860 --> 01:33:52.620]   that isn't maybe too jammed or has a lot of...
[01:33:52.620 --> 01:33:55.660]   I mean, I'm sure it depends a lot on the quality of this service.
[01:33:55.660 --> 01:33:57.020]   Well, the wireless 5G,
[01:33:57.020 --> 01:33:58.460]   it's not just one tower.
[01:33:58.460 --> 01:34:00.300]   They actually have them all over the place.
[01:34:00.300 --> 01:34:05.020]   There's more cell towers as it were acting to provide the signal.
[01:34:05.020 --> 01:34:05.260]   Yeah.
[01:34:05.260 --> 01:34:08.540]   Verizon, Stacey, where you live,
[01:34:08.540 --> 01:34:10.140]   I'm going to guess because it's Sylvan.
[01:34:10.140 --> 01:34:12.380]   It probably wouldn't work well for you.
[01:34:12.380 --> 01:34:12.940]   True or wrong?
[01:34:12.940 --> 01:34:17.180]   It's not because it's Sylvan.
[01:34:17.180 --> 01:34:19.980]   It's because we have nimbies who won't let cell towers.
[01:34:19.980 --> 01:34:20.380]   That's what we're saying.
[01:34:20.380 --> 01:34:21.420]   It's rich and Sylvan.
[01:34:21.420 --> 01:34:21.740]   Yes.
[01:34:21.740 --> 01:34:23.580]   We don't want to un-sill-vage.
[01:34:23.580 --> 01:34:25.900]   It's the Sylvan nimbies you got to watch out for.
[01:34:25.900 --> 01:34:26.620]   Yeah.
[01:34:26.620 --> 01:34:27.580]   It's a Sylvan nimby.
[01:34:27.580 --> 01:34:30.140]   There we have pointer ears and little furry toes.
[01:34:30.140 --> 01:34:33.100]   Actually, so apparently our city council
[01:34:33.100 --> 01:34:37.500]   tried to get a tower for AT&T on the island,
[01:34:37.500 --> 01:34:40.860]   like an additional tower in AT&T,
[01:34:40.860 --> 01:34:43.260]   or maybe it was Verizon said it just wasn't worth their money.
[01:34:43.260 --> 01:34:46.780]   So how's your cell coverage?
[01:34:46.780 --> 01:34:49.020]   Yeah, I was going to ask, who do you use and how is your coverage?
[01:34:49.020 --> 01:34:52.540]   Timo is probably the best one on the island.
[01:34:52.540 --> 01:34:55.020]   I still use Verizon, but it's not great.
[01:34:55.980 --> 01:34:56.620]   So they do have towers.
[01:34:56.620 --> 01:34:57.900]   And there are parts of the island where...
[01:34:57.900 --> 01:34:59.340]   Yeah, they have it.
[01:34:59.340 --> 01:35:00.540]   There is a tower.
[01:35:00.540 --> 01:35:03.900]   A tower.
[01:35:03.900 --> 01:35:07.740]   Well, you're going to need to go to the satellite then.
[01:35:07.740 --> 01:35:09.580]   God, no.
[01:35:09.580 --> 01:35:11.260]   There's too many trees here for satellite.
[01:35:11.260 --> 01:35:13.500]   Well, so much for that then.
[01:35:13.500 --> 01:35:16.460]   Let me just do a little research here.
[01:35:16.460 --> 01:35:21.500]   The total square mileage of your island is 65 square miles.
[01:35:22.380 --> 01:35:24.380]   The history of the population...
[01:35:24.380 --> 01:35:25.340]   I'm so obsessed with this island.
[01:35:25.340 --> 01:35:29.900]   The population is around 25,000.
[01:35:29.900 --> 01:35:34.940]   So yeah, I think one tower should do for per each, right?
[01:35:34.940 --> 01:35:37.580]   Well, but no, because you got to remember,
[01:35:37.580 --> 01:35:40.220]   we're surrounded by water and water is terrible for wireless.
[01:35:40.220 --> 01:35:40.940]   Oh, is that true?
[01:35:40.940 --> 01:35:41.660]   Oh, really?
[01:35:41.660 --> 01:35:42.940]   Water mountains?
[01:35:42.940 --> 01:35:43.980]   I didn't know that.
[01:35:43.980 --> 01:35:44.460]   Oh, yeah.
[01:35:44.460 --> 01:35:45.500]   Yeah.
[01:35:45.500 --> 01:35:47.660]   Spectrum attenuates in water.
[01:35:47.660 --> 01:35:48.060]   Oh, yeah.
[01:35:48.060 --> 01:35:50.060]   But so if you're in a boat, it's not going to be good.
[01:35:50.060 --> 01:35:53.500]   But if you're on the island itself, it's not going to have to be good, is it?
[01:35:53.500 --> 01:35:55.180]   Well, but we get some of our signals.
[01:35:55.180 --> 01:36:10.940]   Like some people on one side of the island that is close to
[01:36:10.940 --> 01:36:11.900]   network because of that.
[01:36:11.900 --> 01:36:17.020]   That's federal airspace, water, and low, low grade mountains.
[01:36:17.020 --> 01:36:17.660]   So who do you?
[01:36:17.660 --> 01:36:19.500]   And CIA strange things going on.
[01:36:20.380 --> 01:36:21.580]   Well, that's the federal airspace.
[01:36:21.580 --> 01:36:24.140]   Let's just call federal craziness.
[01:36:24.140 --> 01:36:28.780]   So Stacy, I got a weird one for you that I wanted to hear your
[01:36:28.780 --> 01:36:29.180]   fusics.
[01:36:29.180 --> 01:36:29.980]   I don't understand this.
[01:36:29.980 --> 01:36:31.340]   And it's down a law, maybe.
[01:36:31.340 --> 01:36:35.420]   AWS offering private 5G for offices.
[01:36:35.420 --> 01:36:36.860]   I don't understand.
[01:36:36.860 --> 01:36:40.220]   Build your own private mobile network.
[01:36:40.220 --> 01:36:42.300]   Okay.
[01:36:42.300 --> 01:36:43.900]   Okay.
[01:36:43.900 --> 01:36:45.580]   So Stacy says, okay, I got a dummy.
[01:36:45.580 --> 01:36:48.140]   No, no, no, I'm just like, I'm like, oh, okay.
[01:36:49.020 --> 01:36:53.740]   So basically there are, okay, there's two kinds of private networks.
[01:36:53.740 --> 01:36:57.500]   One, there was this whole auction a couple of years ago where you could actually buy
[01:36:57.500 --> 01:37:00.220]   spectrum as a business or school or something.
[01:37:00.220 --> 01:37:05.980]   And some companies did, like John Deere bought their own spectrum to have a private network.
[01:37:05.980 --> 01:37:07.900]   I'm actually going to go visit it at the end of September.
[01:37:07.900 --> 01:37:09.580]   Does it nationwide or is it just a certain?
[01:37:09.580 --> 01:37:10.460]   Nope.
[01:37:10.460 --> 01:37:12.220]   It's only in near their factory.
[01:37:12.220 --> 01:37:13.340]   So they've got a 5G.
[01:37:13.340 --> 01:37:17.660]   So they have their own cell network for the factory, basically.
[01:37:17.660 --> 01:37:21.580]   They have their own spectrum that they bought to run their own cell network.
[01:37:21.580 --> 01:37:23.740]   So that's one type of private network.
[01:37:23.740 --> 01:37:26.540]   And like Chevron has some spectrum of their own.
[01:37:26.540 --> 01:37:28.460]   MIT, I think has some.
[01:37:28.460 --> 01:37:32.780]   Fewer companies bought it than expected, but so that's one type.
[01:37:32.780 --> 01:37:34.380]   That is not what AWS is doing.
[01:37:34.380 --> 01:37:42.140]   There is also company like AT&T provides a private 5G network
[01:37:42.140 --> 01:37:45.340]   where you operate your own cell equipment.
[01:37:45.340 --> 01:37:47.100]   But it's on their network.
[01:37:47.100 --> 01:37:49.420]   But it's on AT&T's network.
[01:37:49.420 --> 01:37:56.300]   Yeah, we have a femto cell at TWIT because T-Mobile, we don't have any T-Mobile.
[01:37:56.300 --> 01:38:00.220]   So we have a femto cell connects to our internet network.
[01:38:00.220 --> 01:38:02.220]   But this is running your own.
[01:38:02.220 --> 01:38:07.580]   Yeah, but this is your own equipment in your own tower, but it's running on their spectrum.
[01:38:07.580 --> 01:38:11.340]   So it's like, if you had a femto cell plus a server that's running a core network.
[01:38:12.060 --> 01:38:17.500]   Does it use your internet landline or does it use their 5G cell network?
[01:38:17.500 --> 01:38:20.220]   It would use, I mean, it could use either, I guess.
[01:38:20.220 --> 01:38:25.740]   But so with the AWS, the weird thing about AWS is, and I'm not sure about this,
[01:38:25.740 --> 01:38:30.940]   because AWS and Microsoft is both partnered with AT&T and Verizon.
[01:38:30.940 --> 01:38:34.380]   So all of those companies have their own private 5G options.
[01:38:34.380 --> 01:38:38.460]   So you're running, I think you're running data from.
[01:38:39.660 --> 01:38:44.940]   It uses this closest AWS servers, like, because they've got their own central
[01:38:44.940 --> 01:38:47.820]   office and you're running that data there to the cloud.
[01:38:47.820 --> 01:38:50.060]   But it uses the CBRS spectrum.
[01:38:50.060 --> 01:38:51.100]   So it's not using it.
[01:38:51.100 --> 01:38:51.420]   Okay, it does.
[01:38:51.420 --> 01:38:52.620]   Okay.
[01:38:52.620 --> 01:38:55.100]   So then the CBRS spectrum is private spectrum.
[01:38:55.100 --> 01:38:57.180]   So AT&T did buy some CBRS.
[01:38:57.180 --> 01:39:01.580]   So the CBRS spectrum is like what John Deere bought in that.
[01:39:01.580 --> 01:39:01.980]   So that's.
[01:39:01.980 --> 01:39:04.300]   That's .5 to 3.7 gigahertz.
[01:39:05.500 --> 01:39:09.020]   And it's intended for exactly this, right?
[01:39:09.020 --> 01:39:14.940]   Building private LTE networks or public networks if you're a city or a government, right?
[01:39:14.940 --> 01:39:15.900]   Yes.
[01:39:15.900 --> 01:39:23.100]   So in that CB band, despite its name, it has nothing to do with citizens band.
[01:39:23.100 --> 01:39:23.420]   No.
[01:39:23.420 --> 01:39:23.740]   Okay.
[01:39:23.740 --> 01:39:27.740]   It's in a 3.5 gigahertz band.
[01:39:27.740 --> 01:39:29.660]   Breaker, breaker, breaker, breaker.
[01:39:29.660 --> 01:39:32.540]   The incumbents.
[01:39:32.540 --> 01:39:32.860]   Okay.
[01:39:32.860 --> 01:39:34.380]   So then are the names?
[01:39:34.380 --> 01:39:34.780]   So.
[01:39:34.780 --> 01:39:38.300]   Yes, but they're okay with it.
[01:39:38.300 --> 01:39:39.260]   You're sharing it.
[01:39:39.260 --> 01:39:41.500]   We've got a long way towards sharing spectrum.
[01:39:41.500 --> 01:39:41.660]   Right.
[01:39:41.660 --> 01:39:43.100]   We've gotten really good at that now.
[01:39:43.100 --> 01:39:43.340]   Right.
[01:39:43.340 --> 01:39:45.100]   But yeah.
[01:39:45.100 --> 01:39:45.340]   Okay.
[01:39:45.340 --> 01:39:50.220]   So then that's a dedicated spectrum that AT&T owns that's in the CBRS band.
[01:39:50.220 --> 01:39:54.140]   So they're not running the same AT&T network that you or I might be on.
[01:39:54.140 --> 01:39:55.500]   It's your own dedicated network.
[01:39:55.500 --> 01:39:56.860]   Or in this case, AWS.
[01:39:56.860 --> 01:39:58.460]   AWS not AT&T.
[01:39:58.460 --> 01:40:01.100]   Is it AT&T?
[01:40:01.100 --> 01:40:01.500]   It says,
[01:40:01.500 --> 01:40:05.900]   "80BS private 5G runs on AWS managed infrastructure."
[01:40:05.900 --> 01:40:08.380]   So.
[01:40:08.380 --> 01:40:08.700]   Yeah.
[01:40:08.700 --> 01:40:11.820]   But it might be that might be AT&T spectrum as opposed to.
[01:40:11.820 --> 01:40:12.940]   Yeah.
[01:40:12.940 --> 01:40:16.220]   Because AWS has a deal with both AT&T and Verizon,
[01:40:16.220 --> 01:40:18.540]   four private 5G networks.
[01:40:18.540 --> 01:40:18.620]   Right.
[01:40:18.620 --> 01:40:20.620]   So why do you do this?
[01:40:20.620 --> 01:40:20.620]   So I would have to.
[01:40:20.620 --> 01:40:22.620]   So you do this because you want.
[01:40:22.620 --> 01:40:23.020]   Do this because you want.
[01:40:23.020 --> 01:40:24.700]   No, no.
[01:40:24.700 --> 01:40:28.540]   You do it because you want low latency.
[01:40:28.540 --> 01:40:32.540]   You want to control the quality of the bandwidth, basically.
[01:40:32.540 --> 01:40:36.140]   So you want to say only my factory robots can be on this.
[01:40:36.140 --> 01:40:37.660]   This could be a first security reasons.
[01:40:37.660 --> 01:40:39.820]   It could be because you don't want other traffic getting.
[01:40:39.820 --> 01:40:41.500]   It's not for your staff's phones.
[01:40:41.500 --> 01:40:43.020]   No, no, no.
[01:40:43.020 --> 01:40:47.180]   It's for like industrial use cases where like.
[01:40:47.180 --> 01:40:49.660]   I like Stacey's robot rotation.
[01:40:49.660 --> 01:40:50.540]   That was very good Stacey.
[01:40:50.540 --> 01:40:50.940]   I like that.
[01:40:50.940 --> 01:40:51.420]   That was good.
[01:40:51.420 --> 01:40:54.380]   We used to have a JFDAN actually.
[01:40:56.300 --> 01:40:59.820]   I believe this is also my monster and Stacey angry Stacey hungry.
[01:40:59.820 --> 01:41:00.540]   I think it is.
[01:41:00.540 --> 01:41:04.060]   But you have the firm rest here as opposed to the floppy rest.
[01:41:04.060 --> 01:41:04.940]   As opposed to the flight.
[01:41:04.940 --> 01:41:05.340]   It's true.
[01:41:05.340 --> 01:41:06.780]   It was a variety of performance.
[01:41:06.780 --> 01:41:07.340]   It's very good.
[01:41:07.340 --> 01:41:08.380]   Well, actually.
[01:41:08.380 --> 01:41:09.820]   The risks are very agile.
[01:41:09.820 --> 01:41:14.380]   That brings me to another private spectrum for.
[01:41:14.380 --> 01:41:15.900]   And this is another thing you're an expert in.
[01:41:15.900 --> 01:41:21.900]   The CV2X applications, the vehicle to vehicle and vehicle to everything else.
[01:41:21.900 --> 01:41:25.820]   The National Transportation Safety Board just wrote a letter to the FCC.
[01:41:26.700 --> 01:41:29.340]   Saying, give us this spectrum.
[01:41:29.340 --> 01:41:34.220]   They left 30 megahertz of bandwidth for these vehicle to vehicle communications.
[01:41:34.220 --> 01:41:37.340]   And they haven't done anything.
[01:41:37.340 --> 01:41:39.100]   They've saved the spectrum.
[01:41:39.100 --> 01:41:41.580]   But despite keen interest, I'm quoting ours,
[01:41:41.580 --> 01:41:46.060]   technical from some automakers in industry groups like the Association of State Highway and
[01:41:46.060 --> 01:41:50.940]   Transportation Officials and the Intelligent Transportation Society of America.
[01:41:50.940 --> 01:41:52.140]   It hasn't been deployed.
[01:41:53.420 --> 01:42:01.260]   In fact, the FCC took some of that and reallocated it to Wi-Fi, giving it to Wi-Fi 6E, I think.
[01:42:01.260 --> 01:42:07.900]   Yeah. Well, that's because the Trump administration way back at the very beginning, like 2016,
[01:42:07.900 --> 01:42:15.660]   they declined the Department of Transportation, like declined to make rules or there was some sort
[01:42:15.660 --> 01:42:17.900]   of weird governmental decision.
[01:42:17.900 --> 01:42:20.300]   And I don't remember exactly what it was, but I could look it up.
[01:42:20.300 --> 01:42:27.020]   So since 1995, the NTSB has been lobbying for a wireless collision avoidance technology.
[01:42:27.020 --> 01:42:27.420]   Wow.
[01:42:27.420 --> 01:42:27.980]   Cars.
[01:42:27.980 --> 01:42:28.380]   Nice.
[01:42:28.380 --> 01:42:28.380]   Nice.
[01:42:28.380 --> 01:42:28.940]   Wow.
[01:42:28.940 --> 01:42:31.420]   Cars talking to each other saying, no, you go, Alphonse.
[01:42:31.420 --> 01:42:32.700]   No, after you, Alphonse.
[01:42:32.700 --> 01:42:32.860]   But--
[01:42:32.860 --> 01:42:34.380]   Watch out.
[01:42:34.380 --> 01:42:35.500]   Here comes Alphonse.
[01:42:35.500 --> 01:42:35.820]   Yeah.
[01:42:35.820 --> 01:42:38.060]   That-- it would be huge.
[01:42:38.060 --> 01:42:44.540]   The NTSB said that connected vehicle technology would reduce the ever escalating carnage
[01:42:45.580 --> 01:42:52.380]   on US roads. They urged the FCC to make sure Wi-Fi devices don't encroach on the remaining 30
[01:42:52.380 --> 01:42:57.420]   megahertz. The good news is 30 megahertz is not as small as that might sound.
[01:42:57.420 --> 01:42:58.860]   It's not a huge amount of spectrum.
[01:42:58.860 --> 01:43:04.620]   But they do it in Europe with 40 megahertz for V2X.
[01:43:04.620 --> 01:43:08.220]   And this year, apparently, a million V2X cars will be sold in Europe.
[01:43:08.220 --> 01:43:11.820]   It'll be in the $10 million range in the next couple of years.
[01:43:12.860 --> 01:43:15.820]   So they're going to have it in Europe. Will we have it here?
[01:43:15.820 --> 01:43:16.060]   Well--
[01:43:16.060 --> 01:43:21.500]   Yeah. So it was the Trump administration back in 2017.
[01:43:21.500 --> 01:43:27.980]   So all the way back in 2017, they said, we're not going to mess with this right now.
[01:43:27.980 --> 01:43:32.540]   And it was part of a--
[01:43:32.540 --> 01:43:33.420]   I'm just reading this--
[01:43:33.420 --> 01:43:37.980]   GM dabbled with a vehicle to vehicle in the 2017 Cadillac CTS.
[01:43:38.940 --> 01:43:44.620]   In 2018, Toyota publicly committed to deployment in all new cars for the 2021 model year.
[01:43:44.620 --> 01:43:52.220]   But shortly thereafter, the Trump FCC said the spectrum is under consideration for reallocation.
[01:43:52.220 --> 01:43:54.300]   So Toyota backed off.
[01:43:54.300 --> 01:43:56.780]   Yeah, they stopped doing it because they were--
[01:43:56.780 --> 01:44:00.940]   the Obama administration had said, hey, we're going to go forward with this.
[01:44:00.940 --> 01:44:03.660]   And then very early on in Trump, they said no.
[01:44:03.660 --> 01:44:08.140]   And so then they were like-- the car makers were like, well, never mind.
[01:44:08.700 --> 01:44:15.660]   The NTSB traffic fatalities last year in the US, 42,915 people died.
[01:44:15.660 --> 01:44:21.020]   The NTSB documented scenarios where vehicle to vehicle would have saved lives.
[01:44:21.020 --> 01:44:24.860]   I think we need this. And I think it's time to move on this.
[01:44:24.860 --> 01:44:29.100]   Well, so here's-- I mean, there's a lot of issues there.
[01:44:29.100 --> 01:44:32.620]   There's everyone's favorite, which is spectrum sharing.
[01:44:32.620 --> 01:44:37.020]   Like, OK, how do we make sure this doesn't interfere with anything nearby?
[01:44:37.020 --> 01:44:39.580]   Then there's like, OK, now we've got cars with this.
[01:44:39.580 --> 01:44:42.220]   How-- what kind of protocols are we going to make it?
[01:44:42.220 --> 01:44:43.660]   Like, how are they going to communicate?
[01:44:43.660 --> 01:44:47.740]   How do you deal with-- and then there's also these ambitious,
[01:44:47.740 --> 01:44:50.780]   not just vehicle to vehicle, but like vehicle to infrastructure.
[01:44:50.780 --> 01:44:53.660]   So how is that going to work?
[01:44:53.660 --> 01:44:55.660]   What are the standards that we're going to be putting place?
[01:44:55.660 --> 01:44:57.580]   And then Qualcomm was fighting with--
[01:44:57.580 --> 01:44:59.660]   I don't know who they were fighting with.
[01:44:59.660 --> 01:45:03.260]   Anyway, it's been kind of a mess.
[01:45:03.260 --> 01:45:08.140]   As it always is when you're trying to lay out some sort of regulatory standard,
[01:45:08.140 --> 01:45:11.100]   everybody wants it to go their way, basically.
[01:45:11.100 --> 01:45:13.900]   And I know you're a big fan of Dr. Duolittle.
[01:45:13.900 --> 01:45:19.260]   I wonder if you could do a little rendition of "He Can Talk to the Animals to the Animals."
[01:45:19.260 --> 01:45:19.660]   Just sing it.
[01:45:19.660 --> 01:45:21.660]   Why do you keep trying to get me to do music?
[01:45:21.660 --> 01:45:24.860]   Because I know it makes you crazy.
[01:45:24.860 --> 01:45:26.060]   You're crazy to do a duet.
[01:45:26.060 --> 01:45:29.180]   I don't even know what you're singing.
[01:45:30.460 --> 01:45:32.860]   Don't you remember Dr. Duolittle?
[01:45:32.860 --> 01:45:33.340]   Dr. Duolittle.
[01:45:33.340 --> 01:45:33.820]   Rex Harrison.
[01:45:33.820 --> 01:45:35.180]   Is it like the Eddie Murphy?
[01:45:35.180 --> 01:45:35.820]   No, no.
[01:45:35.820 --> 01:45:36.620]   Well, I don't know.
[01:45:36.620 --> 01:45:37.020]   No, no.
[01:45:37.020 --> 01:45:38.700]   Rex Harrison was Dr. Duolittle.
[01:45:38.700 --> 01:45:41.100]   And he sang, and the theme was "He Can Talk to the Animals."
[01:45:41.100 --> 01:45:43.500]   We do no references after 1965, Stacy.
[01:45:43.500 --> 01:45:47.180]   Rex Harrison listened, "My Fair Lady."
[01:45:47.180 --> 01:45:48.060]   Yes, he was.
[01:45:48.060 --> 01:45:52.780]   He was also in other things, believe it or not, including Dr. Duolittle.
[01:45:52.780 --> 01:45:53.180]   Duolittle.
[01:45:53.180 --> 01:45:56.860]   Anyway, there was a law lead into a OK story.
[01:45:56.860 --> 01:45:58.460]   It's a great story.
[01:45:58.460 --> 01:46:02.860]   If you think talking to naked mole rats is a great thing,
[01:46:02.860 --> 01:46:03.900]   it's a great story.
[01:46:03.900 --> 01:46:06.460]   You know how they're doing it?
[01:46:06.460 --> 01:46:06.940]   AI.
[01:46:06.940 --> 01:46:07.260]   AI.
[01:46:07.260 --> 01:46:07.740]   AI.
[01:46:07.740 --> 01:46:08.860]   Isn't that cool?
[01:46:08.860 --> 01:46:09.420]   Of course.
[01:46:09.420 --> 01:46:10.380]   New York Times.
[01:46:10.380 --> 01:46:12.780]   Tory called the animal-
[01:46:12.780 --> 01:46:13.900]   I'm going to get an NFT.
[01:46:13.900 --> 01:46:15.100]   Animal translators.
[01:46:15.100 --> 01:46:17.740]   Scientists are using machine learning to eavesdrop
[01:46:17.740 --> 01:46:20.860]   on naked mole rats, fruit bats, crows, and whales.
[01:46:20.860 --> 01:46:23.660]   And even the hope is to communicate back.
[01:46:23.660 --> 01:46:26.940]   Now, I'm going to play you some naked mole rats.
[01:46:26.940 --> 01:46:27.820]   Would you?
[01:46:27.820 --> 01:46:28.940]   [chuckles]
[01:46:28.940 --> 01:46:29.660]   Oh, boy.
[01:46:29.660 --> 01:46:30.380]   They're hitting you.
[01:46:30.380 --> 01:46:32.300]   OK, listen.
[01:46:32.300 --> 01:46:32.940]   See here that?
[01:46:32.940 --> 01:46:33.900]   So.
[01:46:33.900 --> 01:46:34.380]   Mm-hmm.
[01:46:34.380 --> 01:46:38.860]   So it turns out that that's a conversation.
[01:46:38.860 --> 01:46:42.380]   When two mole rats meet in a dark tunnel,
[01:46:42.380 --> 01:46:45.660]   they exchange a standard salutation.
[01:46:45.660 --> 01:46:46.460]   They make a shaw--
[01:46:46.460 --> 01:46:47.180]   This is what you heard.
[01:46:47.180 --> 01:46:51.660]   A soft chirp, then a repeating soft chirp.
[01:46:51.660 --> 01:46:53.740]   They have a little conversation.
[01:46:54.300 --> 01:46:56.780]   And according to somebody,
[01:46:56.780 --> 01:46:58.380]   Alison Barker, a neuroscientist--
[01:46:58.380 --> 01:46:59.420]   How much spectrum do they do that on?
[01:46:59.420 --> 01:46:59.980]   It's pretty high.
[01:46:59.980 --> 01:47:00.780]   You want to hear it again?
[01:47:00.780 --> 01:47:01.580]   [whistles]
[01:47:01.580 --> 01:47:03.660]   I'm just a big, big, little spectrum.
[01:47:03.660 --> 01:47:04.220]   Oh, I get it.
[01:47:04.220 --> 01:47:06.220]   Uh, I don't know.
[01:47:06.220 --> 01:47:07.980]   Alison Barker is neuroscientist
[01:47:07.980 --> 01:47:10.620]   Max Planck Institute for Brain Research.
[01:47:10.620 --> 01:47:12.860]   It's in Germany, so you know it's good.
[01:47:12.860 --> 01:47:14.700]   Said they have a little conversation.
[01:47:14.700 --> 01:47:17.100]   Hidden in this everyday exchange
[01:47:17.100 --> 01:47:18.700]   is a wealth of social information.
[01:47:18.700 --> 01:47:21.820]   They discovered when they used machine learning algorithms
[01:47:21.820 --> 01:47:29.260]   to analyze 36,000 soft chirps recorded in seven mole rat colonies,
[01:47:29.260 --> 01:47:33.500]   not only did each mole rat have its unique sound,
[01:47:33.500 --> 01:47:37.180]   each colony had its own dialect,
[01:47:37.180 --> 01:47:44.140]   which was passed down culturally from generation to generation.
[01:47:44.140 --> 01:47:45.820]   Further--
[01:47:45.820 --> 01:47:47.100]   Horses do that.
[01:47:47.100 --> 01:47:47.980]   Horses do that.
[01:47:47.980 --> 01:47:49.500]   There's no organs.
[01:47:49.500 --> 01:47:50.700]   Killer whales.
[01:47:51.660 --> 01:47:53.340]   There was a whole Netflix document.
[01:47:53.340 --> 01:47:55.820]   Was it Netflix documentary or Apple documentary?
[01:47:55.820 --> 01:47:56.220]   Probably.
[01:47:56.220 --> 01:47:59.420]   The ones in Norway have their own culture and dialect.
[01:47:59.420 --> 01:47:59.740]   Sure.
[01:47:59.740 --> 01:48:01.420]   That's fascinating.
[01:48:01.420 --> 01:48:03.260]   Here, even with--
[01:48:03.260 --> 01:48:04.540]   Get ready for--
[01:48:04.540 --> 01:48:06.140]   Put your fascination hat on.
[01:48:06.140 --> 01:48:08.780]   During times of social instability,
[01:48:08.780 --> 01:48:13.340]   such as in the weeks after a colony's queen was violently deposed,
[01:48:13.340 --> 01:48:16.860]   the cohesive dialects fell apart.
[01:48:16.860 --> 01:48:18.540]   When a new queen began to reign,
[01:48:18.540 --> 01:48:20.780]   a new dialect appeared to take hold.
[01:48:20.780 --> 01:48:23.260]   Now what would you say?
[01:48:23.260 --> 01:48:26.460]   Dr. Barker says the greeting call,
[01:48:26.460 --> 01:48:29.020]   which I thought was going to be pretty basic,
[01:48:29.020 --> 01:48:31.420]   turned out to be incredibly complicated.
[01:48:31.420 --> 01:48:35.180]   I think it's cool that they can use machine learning.
[01:48:35.180 --> 01:48:35.580]   It makes sense.
[01:48:35.580 --> 01:48:37.020]   Machine learning, what is it?
[01:48:37.020 --> 01:48:39.660]   You apply a data set, you collect a lot of information,
[01:48:39.660 --> 01:48:44.300]   and then you try to create an algorithm to detect patterns.
[01:48:44.300 --> 01:48:47.980]   They could more rats are going to take over with the machine together.
[01:48:47.980 --> 01:48:49.180]   No, because we can talk to them.
[01:48:49.180 --> 01:48:50.060]   Anick about that.
[01:48:50.060 --> 01:48:50.940]   Don't.
[01:48:50.940 --> 01:48:51.660]   Say don't.
[01:48:51.660 --> 01:48:52.780]   They're not your friends.
[01:48:52.780 --> 01:48:54.300]   We're your friends.
[01:48:54.300 --> 01:48:57.660]   We can give you a little tiny volrat clothing.
[01:48:57.660 --> 01:48:59.900]   Pants, shirts, little hats.
[01:48:59.900 --> 01:49:01.260]   Hats.
[01:49:01.260 --> 01:49:01.500]   Yep.
[01:49:01.500 --> 01:49:05.580]   Giving rise to voice assistants that recognize speech,
[01:49:05.580 --> 01:49:08.780]   transcription software that turns volrat
[01:49:08.780 --> 01:49:14.300]   chirps into text and digital tools that translate between human languages.
[01:49:14.300 --> 01:49:15.260]   So you may--
[01:49:15.260 --> 01:49:18.220]   not only are we hearing-- understanding for the first time what they're saying,
[01:49:18.220 --> 01:49:20.940]   we can maybe even talk to them.
[01:49:20.940 --> 01:49:25.580]   They can use machine learning algorithms to identify when squeaking mice are stressed,
[01:49:25.580 --> 01:49:27.660]   or why fruit bats are shouting.
[01:49:27.660 --> 01:49:31.180]   You would too, if you were a fruit bat.
[01:49:31.180 --> 01:49:35.260]   They want to make a Google translate for alphabets.
[01:49:35.260 --> 01:49:40.700]   This is Dana Rice, who is an expert on dolphin cognition and communication at Hunter College.
[01:49:40.700 --> 01:49:42.140]   I think this is pretty cool.
[01:49:43.500 --> 01:49:44.300]   You seem--
[01:49:44.300 --> 01:49:44.300]   Yeah.
[01:49:44.300 --> 01:49:45.500]   --stacy to poo poo it.
[01:49:45.500 --> 01:49:47.420]   No, I'm not poo poo.
[01:49:47.420 --> 01:49:49.580]   It's just-- it's not the first time this has happened.
[01:49:49.580 --> 01:49:57.100]   Oh, I'm excited that we might actually be able to talk to the animals, like Dr. Duolittle.
[01:49:57.100 --> 01:50:00.380]   So, yo, this is the one you know about, I think.
[01:50:00.380 --> 01:50:01.580]   Maybe not.
[01:50:01.580 --> 01:50:03.100]   That's the up-your-way.
[01:50:03.100 --> 01:50:04.700]   Researchers at the University of Washington
[01:50:04.700 --> 01:50:07.980]   use machine learning to develop software called Deep Squeak
[01:50:07.980 --> 01:50:12.860]   that can automatically detect, analyze, and categorize the ultrasonic vocalizations
[01:50:12.860 --> 01:50:13.500]   of rodents.
[01:50:13.500 --> 01:50:15.740]   Oh, wow.
[01:50:15.740 --> 01:50:21.020]   Deep Squeak has now been repurposed for other species, including lemurs and whales,
[01:50:21.020 --> 01:50:22.540]   while other teams have--
[01:50:22.540 --> 01:50:24.060]   Can they do it for cats and dogs?
[01:50:24.060 --> 01:50:25.180]   Yes, they can.
[01:50:25.180 --> 01:50:28.860]   Can they do it for cows and chickens?
[01:50:28.860 --> 01:50:29.340]   Anything.
[01:50:29.340 --> 01:50:30.860]   So, an E.P.
[01:50:30.860 --> 01:50:33.900]   Don't eat me.
[01:50:33.900 --> 01:50:35.420]   That would be bad if they said that.
[01:50:35.420 --> 01:50:37.740]   You must stop killing us.
[01:50:37.740 --> 01:50:41.020]   I mean, do we want to know what the--
[01:50:41.020 --> 01:50:43.100]   I mean, think about if you can talk to a whale or--
[01:50:43.100 --> 01:50:44.060]   Well, I know.
[01:50:44.060 --> 01:50:49.260]   Did you know that bats are pugilistic, frequently quarreling in their crowded colonies?
[01:50:49.260 --> 01:50:52.540]   And the vast majority of their vocalizations are aggressive?
[01:50:52.540 --> 01:50:55.660]   Basically, they're pushing each other.
[01:50:55.660 --> 01:50:58.540]   Imagine a big stadium and everyone wants to find a seat.
[01:50:58.540 --> 01:50:59.820]   Said Dr.--
[01:50:59.820 --> 01:51:00.540]   They're hooligans!
[01:51:00.540 --> 01:51:01.500]   They're hooligans!
[01:51:01.500 --> 01:51:04.460]   That makes perfect sense.
[01:51:04.460 --> 01:51:09.980]   Anyway, all right.
[01:51:09.980 --> 01:51:11.660]   I just-- I feel like--
[01:51:11.660 --> 01:51:12.620]   Here's some fruit bats.
[01:51:12.620 --> 01:51:15.580]   See, they're punching.
[01:51:15.580 --> 01:51:16.300]   I don't want those.
[01:51:16.300 --> 01:51:16.940]   That's crashes my brain.
[01:51:16.940 --> 01:51:18.700]   They're cute, actually, but no, they're not.
[01:51:18.700 --> 01:51:19.100]   I don't know.
[01:51:19.100 --> 01:51:21.500]   Bats are adorable.
[01:51:21.500 --> 01:51:23.260]   They are actually.
[01:51:23.260 --> 01:51:24.140]   They're really weird.
[01:51:24.140 --> 01:51:25.580]   You've got the cutest little faces.
[01:51:25.580 --> 01:51:26.940]   This is like fur--
[01:51:26.940 --> 01:51:28.060]   Stacey-- Stacey--
[01:51:28.060 --> 01:51:30.140]   Stacey found cute birds, by the way.
[01:51:30.140 --> 01:51:31.100]   Or did you just Stacey?
[01:51:31.100 --> 01:51:32.620]   I did.
[01:51:32.620 --> 01:51:34.620]   I said-- I said Jeff has this--
[01:51:34.620 --> 01:51:36.780]   He has my permission to play this TikTok video.
[01:51:36.780 --> 01:51:37.660]   I thought you were on the phone.
[01:51:37.660 --> 01:51:38.460]   Because I was like--
[01:51:38.460 --> 01:51:39.820]   Oh, man.
[01:51:39.820 --> 01:51:40.540]   On the rundown.
[01:51:40.540 --> 01:51:41.900]   Oh, I should've known this before then.
[01:51:41.900 --> 01:51:43.180]   OK, I thought you were going to put on the rundown.
[01:51:43.180 --> 01:51:44.620]   Oh, I never put--
[01:51:44.620 --> 01:51:45.740]   It's Stacey.
[01:51:45.740 --> 01:51:48.220]   Maybe it's--
[01:51:48.220 --> 01:51:49.100]   It's actually DM.
[01:51:49.100 --> 01:51:54.780]   You haven't been posting in the TikTok corner, sir.
[01:51:54.780 --> 01:51:56.220]   There's no TikTok corner today.
[01:51:56.220 --> 01:51:57.260]   I banned it.
[01:51:57.260 --> 01:51:58.700]   Where is--
[01:51:58.700 --> 01:52:00.300]   Where is TikTok corner?
[01:52:00.300 --> 01:52:00.780]   I don't see any--
[01:52:00.780 --> 01:52:03.820]   It starts online 55.
[01:52:03.820 --> 01:52:04.700]   Oh, shoot.
[01:52:04.700 --> 01:52:06.300]   Just found a line.
[01:52:06.300 --> 01:52:06.940]   There is no--
[01:52:06.940 --> 01:52:09.260]   I'm sorry, I put it above line.
[01:52:09.260 --> 01:52:10.140]   You stuck it in.
[01:52:10.140 --> 01:52:10.620]   Put it above.
[01:52:10.620 --> 01:52:11.900]   Oh, you squirre some--
[01:52:11.900 --> 01:52:12.700]   You were supposed to rule it.
[01:52:12.700 --> 01:52:14.140]   This is why I don't do anything on the--
[01:52:14.140 --> 01:52:15.260]   I'm like, no, not this.
[01:52:15.260 --> 01:52:16.860]   And by the way, just watch.
[01:52:16.860 --> 01:52:19.580]   Look, this account is private.
[01:52:19.580 --> 01:52:21.420]   Ah, I'm--
[01:52:21.420 --> 01:52:25.260]   Oh, they must--
[01:52:25.260 --> 01:52:28.060]   Maybe they went viral and they didn't want to be viral.
[01:52:28.060 --> 01:52:29.740]   It was woo pancakes.
[01:52:29.740 --> 01:52:31.580]   And they do--
[01:52:31.580 --> 01:52:31.980]   It was--
[01:52:31.980 --> 01:52:35.980]   They were teaching a crow how to play the guitar.
[01:52:36.460 --> 01:52:38.060]   They gave their--
[01:52:38.060 --> 01:52:39.500]   They strum their guitar with a guitar that they gave it.
[01:52:39.500 --> 01:52:40.060]   She made it.
[01:52:40.060 --> 01:52:41.100]   She made it private.
[01:52:41.100 --> 01:52:43.100]   Crow is a pretty awesome note.
[01:52:43.100 --> 01:52:43.820]   Good for her.
[01:52:43.820 --> 01:52:45.820]   She had crows in line.
[01:52:45.820 --> 01:52:47.340]   You could go into the next two lines.
[01:52:47.340 --> 01:52:49.180]   She only had 526 followers.
[01:52:49.180 --> 01:52:50.860]   We were about to put her on the map.
[01:52:50.860 --> 01:52:52.300]   And instead--
[01:52:52.300 --> 01:52:54.220]   No, I think she might already have been on the map.
[01:52:54.220 --> 01:52:54.540]   I think--
[01:52:54.540 --> 01:52:57.180]   Because usually when people show up on my 4U page,
[01:52:57.180 --> 01:52:58.140]   they're going viral.
[01:52:58.140 --> 01:52:58.860]   Ah, yeah.
[01:52:58.860 --> 01:53:00.700]   She didn't want all that attention.
[01:53:00.700 --> 01:53:01.900]   So I've got one on the next line.
[01:53:01.900 --> 01:53:02.060]   My baby.
[01:53:02.060 --> 01:53:02.620]   56.
[01:53:02.620 --> 01:53:04.300]   Because we did a bird--
[01:53:04.300 --> 01:53:04.780]   This is a tweet.
[01:53:05.420 --> 01:53:06.380]   Well, it's OK.
[01:53:06.380 --> 01:53:06.380]   It's--
[01:53:06.380 --> 01:53:06.860]   OK.
[01:53:06.860 --> 01:53:06.860]   OK.
[01:53:06.860 --> 01:53:08.380]   Everybody's just saying it was a bird.
[01:53:08.380 --> 01:53:11.580]   Ah, this tweet, a bird telling a hedgehog to hurry
[01:53:11.580 --> 01:53:12.700]   because it's dangerous.
[01:53:12.700 --> 01:53:15.660]   And let's turn on the sound so we can hear this.
[01:53:15.660 --> 01:53:17.020]   Well, I don't think so.
[01:53:17.020 --> 01:53:18.540]   Here, we're just going to watch it.
[01:53:18.540 --> 01:53:19.180]   I think the bird is--
[01:53:19.180 --> 01:53:20.940]   I think the bird's trying to eat the hedgehog.
[01:53:20.940 --> 01:53:22.140]   No, no, no, look.
[01:53:22.140 --> 01:53:23.100]   It's the bird saying it's--
[01:53:23.100 --> 01:53:24.060]   No, he's definitely--
[01:53:24.060 --> 01:53:26.300]   No, you're anthropomorphizing.
[01:53:26.300 --> 01:53:26.540]   Yes, he is.
[01:53:26.540 --> 01:53:28.700]   The bird wants to eat the hedgehog.
[01:53:28.700 --> 01:53:32.460]   And he says, "Get out of the road so I can eat you."
[01:53:32.460 --> 01:53:32.940]   Hedgehog.
[01:53:33.580 --> 01:53:35.180]   Bird's like, why is this--
[01:53:35.180 --> 01:53:35.180]   Why is this--
[01:53:35.180 --> 01:53:37.180]   Why is this lunch moving?
[01:53:37.180 --> 01:53:38.620]   Why is he pecking it?
[01:53:38.620 --> 01:53:39.820]   Crows are smart, right?
[01:53:39.820 --> 01:53:41.660]   Yeah, crows are very smart.
[01:53:41.660 --> 01:53:41.980]   It's pretty awesome.
[01:53:41.980 --> 01:53:42.460]   Yeah, right.
[01:53:42.460 --> 01:53:42.620]   Yeah.
[01:53:42.620 --> 01:53:47.500]   I think Charlie Jane Anders or Annalise Newitz
[01:53:47.500 --> 01:53:52.300]   actually had a short story about using AI to understand crows
[01:53:52.300 --> 01:53:58.940]   and then turning them into delivery birds for hacked biologics.
[01:53:58.940 --> 01:53:59.980]   It was a good short story.
[01:53:59.980 --> 01:54:01.180]   Here is a parrot.
[01:54:02.700 --> 01:54:04.300]   This is something you don't ever want.
[01:54:04.300 --> 01:54:06.380]   This is something you don't ever want.
[01:54:06.380 --> 01:54:08.220]   This is a parrot who has learned to bark.
[01:54:08.220 --> 01:54:11.340]   And as a result, bark's at the dog.
[01:54:11.340 --> 01:54:13.660]   It's a cockatoo.
[01:54:13.660 --> 01:54:15.820]   Where are you doing this?
[01:54:15.820 --> 01:54:16.380]   Cracking them.
[01:54:16.380 --> 01:54:19.740]   The cockatoo is barking at the dog.
[01:54:19.740 --> 01:54:22.060]   The poor dog.
[01:54:22.060 --> 01:54:26.620]   And the dog is just looking who taught this bird to talk.
[01:54:26.620 --> 01:54:30.220]   Machine learning to talk to her.
[01:54:30.220 --> 01:54:30.780]   Machine learning to talk to her.
[01:54:30.780 --> 01:54:31.260]   To talk to her.
[01:54:31.260 --> 01:54:31.260]   To talk to her.
[01:54:31.260 --> 01:54:31.820]   That's who.
[01:54:32.700 --> 01:54:34.140]   You want to give them a number?
[01:54:34.140 --> 01:54:35.260]   Well, we're in Tic-Tac Corner.
[01:54:35.260 --> 01:54:37.260]   I'm going to give you an amazing one.
[01:54:37.260 --> 01:54:37.980]   Amazing.
[01:54:37.980 --> 01:54:39.900]   The grumpy chef.
[01:54:39.900 --> 01:54:41.660]   Let me see which one.
[01:54:41.660 --> 01:54:42.060]   Let me see which one.
[01:54:42.060 --> 01:54:42.780]   This is one.
[01:54:42.780 --> 01:54:43.980]   One 30.
[01:54:43.980 --> 01:54:44.860]   I'm hoping we see her.
[01:54:44.860 --> 01:54:45.740]   Let me see which one it is.
[01:54:45.740 --> 01:54:47.180]   Oh, this is one of your picks.
[01:54:47.180 --> 01:54:47.660]   One 30.
[01:54:47.660 --> 01:54:48.460]   Oh, okay.
[01:54:48.460 --> 01:54:49.660]   Well, you're giving a pick.
[01:54:49.660 --> 01:54:50.060]   Okay.
[01:54:50.060 --> 01:54:50.700]   Very nice.
[01:54:50.700 --> 01:54:51.020]   Oh, no.
[01:54:51.020 --> 01:54:51.420]   Actually, no.
[01:54:51.420 --> 01:54:52.620]   I'm one 31.
[01:54:52.620 --> 01:54:53.740]   Oh, I love this guy.
[01:54:53.740 --> 01:54:55.020]   I follow him all the time.
[01:54:55.020 --> 01:54:57.340]   In fact, I just saw this tic-tac.
[01:54:57.340 --> 01:54:57.900]   Kind of funny.
[01:54:57.900 --> 01:55:01.660]   He normally, he's there to kiss.
[01:55:01.660 --> 01:55:03.980]   To diss horrible cooking videos,
[01:55:03.980 --> 01:55:06.620]   of which there are many on tic-tac.
[01:55:06.620 --> 01:55:07.900]   But in this case,
[01:55:07.900 --> 01:55:12.540]   it's a chocolatier who he cannot cannot hear enough.
[01:55:12.540 --> 01:55:13.820]   Just listen.
[01:55:13.820 --> 01:55:17.020]   All right.
[01:55:17.020 --> 01:55:19.660]   It's time for some more chocolate self-loathing
[01:55:19.660 --> 01:55:21.420]   while I watch this genius at work.
[01:55:21.420 --> 01:55:25.820]   Get the f*** out of here.
[01:55:25.820 --> 01:55:30.860]   He's making a safe and it works.
[01:55:31.740 --> 01:55:35.020]   What kind of evil genius is this?
[01:55:35.020 --> 01:55:35.740]   Oh, you know what?
[01:55:35.740 --> 01:55:38.060]   Oh, you think you're slick.
[01:55:38.060 --> 01:55:38.940]   I know what you're doing.
[01:55:38.940 --> 01:55:43.660]   You're practicing for an actual bank robbery off site.
[01:55:43.660 --> 01:55:44.780]   Like Ocean's 11.
[01:55:44.780 --> 01:55:47.340]   That did he.
[01:55:47.340 --> 01:55:48.620]   He made a chocolate safe.
[01:55:48.620 --> 01:55:49.980]   That actually works.
[01:55:49.980 --> 01:55:51.420]   Now what?
[01:55:51.420 --> 01:55:55.340]   Caramel?
[01:55:55.340 --> 01:55:56.140]   Caramel.
[01:55:56.140 --> 01:55:58.060]   Like even the caramel just looks perfect.
[01:55:58.940 --> 01:56:00.140]   He does a lot of smearing.
[01:56:00.140 --> 01:56:01.100]   There you go.
[01:56:01.100 --> 01:56:02.300]   That's great.
[01:56:02.300 --> 01:56:02.780]   Yeah.
[01:56:02.780 --> 01:56:03.420]   Oh, no.
[01:56:03.420 --> 01:56:04.220]   He's the one with it.
[01:56:04.220 --> 01:56:05.100]   Donton vacation and...
[01:56:05.100 --> 01:56:07.420]   Fully functioning safe.
[01:56:07.420 --> 01:56:08.700]   Gold bars inside.
[01:56:08.700 --> 01:56:10.220]   Dad sauce.
[01:56:10.220 --> 01:56:11.100]   15 out of 10.
[01:56:11.100 --> 01:56:13.180]   I would eat it, but I wouldn't want to because there's a gorgeous...
[01:56:13.180 --> 01:56:15.020]   That sauce.
[01:56:15.020 --> 01:56:16.780]   Just play the next one too.
[01:56:16.780 --> 01:56:17.180]   The next one.
[01:56:17.180 --> 01:56:17.820]   Yeah.
[01:56:17.820 --> 01:56:18.940]   He can't hate on now.
[01:56:18.940 --> 01:56:19.820]   He's even better at that.
[01:56:19.820 --> 01:56:20.300]   No, no, no.
[01:56:20.300 --> 01:56:21.820]   Back on the rundown.
[01:56:21.820 --> 01:56:23.100]   Oh, okay.
[01:56:23.100 --> 01:56:23.660]   Next one down.
[01:56:23.660 --> 01:56:24.220]   Yeah, okay.
[01:56:24.220 --> 01:56:25.580]   He's even brilliant.
[01:56:25.580 --> 01:56:28.460]   Is this the one where he's making the connect for?
[01:56:28.460 --> 01:56:29.100]   Yes.
[01:56:29.100 --> 01:56:29.580]   Yes.
[01:56:29.580 --> 01:56:31.260]   Thanks for the spoiler, yes.
[01:56:31.260 --> 01:56:31.660]   Okay.
[01:56:31.660 --> 01:56:32.300]   Ruin it.
[01:56:32.300 --> 01:56:35.260]   This is where he got his turn to take over the whole f*** world.
[01:56:35.260 --> 01:56:37.100]   And you know what?
[01:56:37.100 --> 01:56:37.660]   I'd let him.
[01:56:37.660 --> 01:56:39.660]   This is definitely not...
[01:56:39.660 --> 01:56:40.060]   Stop.
[01:56:40.060 --> 01:56:41.340]   Is this a connect for?
[01:56:41.340 --> 01:56:42.140]   Younger kids.
[01:56:42.140 --> 01:56:44.540]   We're going until 20...
[01:56:44.540 --> 01:56:44.940]   Four.
[01:56:44.940 --> 01:56:45.500]   Wait, dude.
[01:56:45.500 --> 01:56:51.180]   I'm going into some fights off of connect four back in the day.
[01:56:51.180 --> 01:56:53.020]   Wow.
[01:56:57.100 --> 01:56:58.140]   This is all chocolate.
[01:56:58.140 --> 01:56:59.900]   Are they each going to have a small bit?
[01:56:59.900 --> 01:57:00.220]   Yep.
[01:57:00.220 --> 01:57:00.780]   Because that's...
[01:57:00.780 --> 01:57:03.740]   That's some balling.
[01:57:03.740 --> 01:57:05.740]   Jeez.
[01:57:05.740 --> 01:57:08.860]   They're going to have to get the the bloop machine out for this one.
[01:57:08.860 --> 01:57:12.860]   The in frequency and just the attention is more till it's just the next level.
[01:57:12.860 --> 01:57:16.540]   Also the fact that this guy has like alien equipment to cut chocolate with.
[01:57:16.540 --> 01:57:17.180]   And like...
[01:57:17.180 --> 01:57:18.380]   Got C and C's.
[01:57:18.380 --> 01:57:19.660]   Unreal, dude.
[01:57:19.660 --> 01:57:23.740]   That's the three in a row.
[01:57:23.740 --> 01:57:25.340]   Go for one more.
[01:57:25.340 --> 01:57:26.220]   Go for it.
[01:57:26.220 --> 01:57:26.860]   Connect four.
[01:57:26.860 --> 01:57:27.420]   Ten out of ten.
[01:57:27.420 --> 01:57:29.980]   I love it.
[01:57:29.980 --> 01:57:31.500]   How much he hates it.
[01:57:31.500 --> 01:57:32.780]   When they're good, right?
[01:57:32.780 --> 01:57:33.020]   Yeah.
[01:57:33.020 --> 01:57:37.260]   Yeah, I see that guy a lot.
[01:57:37.260 --> 01:57:38.300]   Oh, never seen it.
[01:57:38.300 --> 01:57:39.100]   I love the chef.
[01:57:39.100 --> 01:57:40.940]   But then the chef liking something.
[01:57:40.940 --> 01:57:41.980]   It's unusual.
[01:57:41.980 --> 01:57:43.020]   The chef is great.
[01:57:43.020 --> 01:57:46.540]   So John is traveling with Umfries McGee.
[01:57:46.540 --> 01:57:47.980]   So he did not hear any of those.
[01:57:47.980 --> 01:57:48.940]   Super fan.
[01:57:48.940 --> 01:57:50.380]   He's a super fan.
[01:57:50.380 --> 01:57:53.340]   Did you hear how many Roger Waters shows he's going to?
[01:57:53.340 --> 01:57:54.060]   Like...
[01:57:54.060 --> 01:57:54.380]   Yeah.
[01:57:54.380 --> 01:57:57.660]   And colluding one on a Tuesday in Sacramento.
[01:57:57.660 --> 01:57:59.100]   Yeah, we almost went to that one.
[01:57:59.100 --> 01:58:00.060]   Are you going to that one?
[01:58:00.060 --> 01:58:01.340]   No, sir.
[01:58:01.340 --> 01:58:03.900]   He moved us to Friday in San Francisco.
[01:58:03.900 --> 01:58:06.060]   We're going on the 23rd to see Roger Waters.
[01:58:06.060 --> 01:58:07.900]   Ah.
[01:58:07.900 --> 01:58:09.820]   No interesting, but I love his passion.
[01:58:09.820 --> 01:58:12.460]   I just love how much he enjoys his history.
[01:58:12.460 --> 01:58:15.420]   Yeah, he's created a little Roger Waters shrine out here in the hall.
[01:58:15.420 --> 01:58:15.740]   Yeah.
[01:58:15.740 --> 01:58:16.700]   I got an Instagram.
[01:58:16.700 --> 01:58:19.260]   It's a good series.
[01:58:19.260 --> 01:58:20.380]   Yeah, no, it really is.
[01:58:20.380 --> 01:58:22.780]   It's five feet by three feet.
[01:58:22.780 --> 01:58:24.620]   It should make a TikTok out of it.
[01:58:24.620 --> 01:58:25.740]   What's his apartment like?
[01:58:25.740 --> 01:58:26.700]   What is his home like?
[01:58:26.700 --> 01:58:27.180]   That's it.
[01:58:27.180 --> 01:58:29.740]   This is shrines?
[01:58:29.740 --> 01:58:30.220]   Yeah, no.
[01:58:30.220 --> 01:58:33.980]   In a couple of MacBooks and monitors and where...
[01:58:33.980 --> 01:58:37.180]   Yeah, he has a cockpit with a bunch of monitors.
[01:58:37.180 --> 01:58:40.380]   It's like a jerk.
[01:58:40.380 --> 01:58:40.540]   It's awesome.
[01:58:40.540 --> 01:58:41.180]   Paradise.
[01:58:41.180 --> 01:58:41.900]   It's awesome.
[01:58:41.900 --> 01:58:42.780]   It's a big character.
[01:58:42.780 --> 01:58:43.820]   He feels such a great character.
[01:58:43.820 --> 01:58:47.100]   When he's been with us since almost the beginning,
[01:58:47.100 --> 01:58:48.300]   he retired.
[01:58:48.300 --> 01:58:52.460]   He was an IT guy at Cal State and retired.
[01:58:52.460 --> 01:58:55.340]   Moved next door to the studio, the old cottage.
[01:58:55.340 --> 01:58:58.460]   Because he was such a fan of the show, he just wanted to live next door.
[01:58:58.460 --> 01:59:01.900]   And he would come over every day.
[01:59:01.900 --> 01:59:03.260]   Did you think he was a stalker?
[01:59:03.260 --> 01:59:05.100]   Did you think he was a stalker?
[01:59:05.100 --> 01:59:08.380]   No, I never got a creepy vibe from John.
[01:59:08.380 --> 01:59:09.180]   Okay.
[01:59:09.180 --> 01:59:12.940]   Finally, Lisa says, well, we've got to do something with this guy.
[01:59:12.940 --> 01:59:13.740]   Why did he...
[01:59:13.740 --> 01:59:17.900]   He could be there on Saturday when you're doing the radio show and get you salads and stuff.
[01:59:17.900 --> 01:59:21.260]   He slowly works his way up from that to...
[01:59:21.260 --> 01:59:22.860]   He runs our whole studio.
[01:59:22.860 --> 01:59:25.180]   He's the only guy who has everything works in here.
[01:59:25.180 --> 01:59:26.620]   Everything.
[01:59:26.620 --> 01:59:29.020]   We are at a loss if there's no John.
[01:59:29.020 --> 01:59:31.820]   Yeah, it's kind of an amazing story.
[01:59:31.820 --> 01:59:34.380]   He has a pension from Cal State.
[01:59:34.380 --> 01:59:35.420]   Yeah.
[01:59:35.420 --> 01:59:38.700]   He doesn't really need to work, but he likes doing it.
[01:59:38.700 --> 01:59:39.100]   Thank you.
[01:59:39.100 --> 01:59:40.860]   Thank you, John.
[01:59:40.860 --> 01:59:42.780]   Love you, Jammer Bee.
[01:59:42.780 --> 01:59:43.580]   Love you, Jammer Bee.
[01:59:43.580 --> 01:59:44.060]   Yeah.
[01:59:44.060 --> 01:59:44.700]   Let's do the...
[01:59:44.700 --> 01:59:47.820]   By the way, Benito is the new Jammer Bee.
[01:59:47.820 --> 01:59:49.660]   He's going there.
[01:59:50.540 --> 01:59:51.020]   No.
[01:59:51.020 --> 01:59:54.300]   Let's do the Google change log, Benito.
[01:59:54.300 --> 01:59:57.580]   The Google change log.
[01:59:57.580 --> 02:00:03.180]   It's a change, but it's also a transition.
[02:00:03.180 --> 02:00:09.340]   Robert Kinsell, who has been at YouTube as their chief business officer, is leaving
[02:00:09.340 --> 02:00:12.300]   to be... spend more time with his money.
[02:00:12.300 --> 02:00:15.820]   He has been there 12 years as a YouTube exec.
[02:00:15.820 --> 02:00:17.660]   He really...
[02:00:17.660 --> 02:00:19.420]   I mean, if you think about it, he's been there
[02:00:19.420 --> 02:00:25.580]   since what 2011 or 2010, that means he really put YouTube on the map.
[02:00:25.580 --> 02:00:31.020]   He'll be leaving early next year to start the next chapter in his career.
[02:00:31.020 --> 02:00:31.580]   Do you know him?
[02:00:31.580 --> 02:00:32.940]   Yeah.
[02:00:32.940 --> 02:00:34.540]   His name comes up a lot.
[02:00:34.540 --> 02:00:35.580]   A Davos, of course.
[02:00:35.580 --> 02:00:39.660]   He was kind of the biz dev guy, right?
[02:00:39.660 --> 02:00:40.220]   That's kind of...
[02:00:40.220 --> 02:00:45.660]   Every company has to have a person who establishes relationships, maintains them.
[02:00:46.220 --> 02:00:52.380]   I also saw a story that Susan Wojiske had talked to Elon way back in the day about being number
[02:00:52.380 --> 02:00:53.260]   two at Tesla.
[02:00:53.260 --> 02:00:54.460]   Didn't.
[02:00:54.460 --> 02:00:56.140]   I wonder if he would have taken over if she had to.
[02:00:56.140 --> 02:00:56.860]   Interesting.
[02:00:56.860 --> 02:00:58.540]   Yeah, he was kind of always next in line.
[02:00:58.540 --> 02:01:02.860]   What else?
[02:01:02.860 --> 02:01:04.140]   Change log.
[02:01:04.140 --> 02:01:13.180]   Google search maps will identify confirmed veterans' hospitals and abortion providers.
[02:01:13.180 --> 02:01:15.580]   That's an interesting combination.
[02:01:16.220 --> 02:01:19.100]   Veteran hospitals or abortion clinics near me.
[02:01:19.100 --> 02:01:21.180]   Actually, this is the...
[02:01:21.180 --> 02:01:23.100]   Veterans' hospitals provide abortions to veterans?
[02:01:23.100 --> 02:01:23.340]   No.
[02:01:23.340 --> 02:01:24.460]   Do they?
[02:01:24.460 --> 02:01:24.780]   No.
[02:01:24.780 --> 02:01:25.020]   Do they?
[02:01:25.020 --> 02:01:25.420]   I don't know.
[02:01:25.420 --> 02:01:25.820]   They don't know.
[02:01:25.820 --> 02:01:26.380]   Federal facilities.
[02:01:26.380 --> 02:01:27.340]   They're hospitals.
[02:01:27.340 --> 02:01:27.980]   Yeah, I don't know.
[02:01:27.980 --> 02:01:29.980]   I bet you Congress doesn't let them.
[02:01:29.980 --> 02:01:31.820]   I bet you bet you.
[02:01:31.820 --> 02:01:38.220]   For example, when you look up veteran hospitals near me,
[02:01:38.220 --> 02:01:42.380]   a veteran hospital chip with a check mark will appear in only places
[02:01:42.380 --> 02:01:47.100]   that have been confirmed by Google to offer those services will get listed.
[02:01:47.100 --> 02:01:51.420]   The reason this is a problem with abortion clinics is there a lot of anti-abortion
[02:01:51.420 --> 02:01:56.540]   "clinics" that will show up on the maps, but they don't in fact offer abortions.
[02:01:56.540 --> 02:01:58.780]   They may not even have medical personnel.
[02:01:58.780 --> 02:02:00.380]   Oh, the pregnancy crisis centers.
[02:02:00.380 --> 02:02:00.780]   Yes.
[02:02:00.780 --> 02:02:06.300]   So they will now say whether a place provides abortions.
[02:02:06.300 --> 02:02:09.020]   You might also see might not provide abortions.
[02:02:11.100 --> 02:02:14.620]   So the button will also appear for mental health clinics,
[02:02:14.620 --> 02:02:16.540]   physiotherapy centers, and travel clinics.
[02:02:16.540 --> 02:02:17.980]   What's a travel clinic?
[02:02:17.980 --> 02:02:18.940]   Traveling physician?
[02:02:18.940 --> 02:02:19.980]   Maybe.
[02:02:19.980 --> 02:02:20.540]   I don't know.
[02:02:20.540 --> 02:02:21.020]   Yeah.
[02:02:21.020 --> 02:02:21.420]   Oh, no.
[02:02:21.420 --> 02:02:24.780]   So Google actually calls...
[02:02:24.780 --> 02:02:28.060]   Does a number of ways to verify, including calling the businesses,
[02:02:28.060 --> 02:02:31.980]   to see if they do what they say they do.
[02:02:31.980 --> 02:02:32.620]   So that's good.
[02:02:32.620 --> 02:02:36.940]   There is a new made for fit accessory line,
[02:02:36.940 --> 02:02:40.780]   including a pixel charging stand for the Sense 2,
[02:02:40.780 --> 02:02:42.300]   Versa 4, and Charge 5.
[02:02:42.300 --> 02:02:43.180]   That's ridiculous.
[02:02:43.180 --> 02:02:46.300]   Charge your phone, charge your headphones, and charge your...
[02:02:46.300 --> 02:02:48.380]   Well, Apple has a bunch of these, right?
[02:02:48.380 --> 02:02:50.060]   So it's a pair of...
[02:02:50.060 --> 02:02:50.620]   Parity.
[02:02:50.620 --> 02:02:54.620]   Lisa has a charging stand for her iPhone and Apple Watch,
[02:02:54.620 --> 02:02:55.980]   and it's a good idea, actually.
[02:02:55.980 --> 02:02:56.620]   Totally useful.
[02:02:56.620 --> 02:02:56.780]   Yeah.
[02:02:56.780 --> 02:02:57.660]   Yeah.
[02:02:57.660 --> 02:03:01.260]   I'm really wondering when Google's going to do its event.
[02:03:01.260 --> 02:03:02.780]   Apple's event is a week from today.
[02:03:02.780 --> 02:03:04.380]   Oh.
[02:03:04.380 --> 02:03:04.700]   Yeah.
[02:03:04.700 --> 02:03:05.580]   I'm wondering when...
[02:03:05.580 --> 02:03:06.940]   I, by the way, I apologize.
[02:03:06.940 --> 02:03:08.860]   I will not be here next week for this week in Google.
[02:03:08.860 --> 02:03:09.900]   Little programming now.
[02:03:09.900 --> 02:03:11.180]   You leave again?
[02:03:11.180 --> 02:03:12.860]   Okay.
[02:03:12.860 --> 02:03:16.780]   I have to go see Joan Jett, poison,
[02:03:16.780 --> 02:03:19.420]   Motley Crue, and Def Leppard.
[02:03:19.420 --> 02:03:22.140]   And the show starts at 4.30.
[02:03:22.140 --> 02:03:22.620]   So I'll be...
[02:03:22.620 --> 02:03:22.940]   This is...
[02:03:22.940 --> 02:03:24.540]   Look at Stacy's face.
[02:03:24.540 --> 02:03:26.540]   And this is not a JAML-B session.
[02:03:26.540 --> 02:03:29.180]   This is a Lisa LePorte.
[02:03:29.180 --> 02:03:29.660]   Lisa.
[02:03:29.660 --> 02:03:30.780]   This is a Lisa, yeah.
[02:03:30.780 --> 02:03:33.180]   This is something I am doing as a good husband.
[02:03:35.180 --> 02:03:36.460]   You were volen-told.
[02:03:36.460 --> 02:03:37.420]   I was volen-told.
[02:03:37.420 --> 02:03:37.980]   I like that.
[02:03:37.980 --> 02:03:38.700]   That's a good word.
[02:03:38.700 --> 02:03:39.260]   That's nice.
[02:03:39.260 --> 02:03:39.740]   That's nice.
[02:03:39.740 --> 02:03:40.700]   That's a very good word.
[02:03:40.700 --> 02:03:43.420]   Queen Pruitt said, "I had to go."
[02:03:43.420 --> 02:03:47.100]   So you're going to be either on your phone the whole time.
[02:03:47.100 --> 02:03:50.860]   I don't think you can sit at a Motley Crue concert
[02:03:50.860 --> 02:03:51.820]   and look at your phone.
[02:03:51.820 --> 02:03:52.700]   I don't think that's a lie.
[02:03:52.700 --> 02:03:54.380]   I fell asleep at a Def Leppard concert.
[02:03:54.380 --> 02:03:54.860]   Did you?
[02:03:54.860 --> 02:03:55.340]   You know.
[02:03:55.340 --> 02:03:55.900]   Really?
[02:03:55.900 --> 02:03:56.060]   Yeah.
[02:03:56.060 --> 02:03:56.620]   We were...
[02:03:56.620 --> 02:03:57.260]   Wow.
[02:03:57.260 --> 02:03:59.820]   I was pregnant if it helps, but yeah.
[02:03:59.820 --> 02:04:00.140]   Okay.
[02:04:00.140 --> 02:04:04.060]   Oh, so your little child was brought up to the beautiful tunes...
[02:04:04.060 --> 02:04:06.060]   Thegol sit-toes of the song.
[02:04:06.060 --> 02:04:07.100]   The song's so chuggy.
[02:04:07.100 --> 02:04:08.460]   Oh, me.
[02:04:08.460 --> 02:04:11.740]   Oh, that was terrible.
[02:04:11.740 --> 02:04:12.460]   Wow, y'all.
[02:04:12.460 --> 02:04:12.940]   Awesome.
[02:04:12.940 --> 02:04:14.220]   Sugar on that baby.
[02:04:14.220 --> 02:04:17.260]   That's hysterical.
[02:04:17.260 --> 02:04:18.460]   That's a st-
[02:04:18.460 --> 02:04:19.180]   Wow.
[02:04:19.180 --> 02:04:21.340]   Google Meet is getting a push to talk
[02:04:21.340 --> 02:04:24.540]   with a space bar on Mute Shortcut.
[02:04:24.540 --> 02:04:26.860]   You know, that's a good idea.
[02:04:26.860 --> 02:04:29.740]   I had that on other applications where,
[02:04:29.740 --> 02:04:31.980]   you know, you have to click Control D to Mute
[02:04:31.980 --> 02:04:33.340]   or whatever, click a button.
[02:04:33.340 --> 02:04:35.660]   Having the space bar push to talk is great.
[02:04:35.660 --> 02:04:36.940]   I think Jitzy used to do that.
[02:04:36.940 --> 02:04:37.500]   Yeah.
[02:04:37.500 --> 02:04:38.300]   I really liked that.
[02:04:38.300 --> 02:04:38.860]   Zoom does it.
[02:04:38.860 --> 02:04:39.420]   Zoom does it.
[02:04:39.420 --> 02:04:40.460]   Okay, that's where I've seen it.
[02:04:40.460 --> 02:04:45.900]   Also, Google made it so it's dedicated
[02:04:45.900 --> 02:04:47.020]   Mute hardware devices.
[02:04:47.020 --> 02:04:48.380]   I didn't even know there were such a thing.
[02:04:48.380 --> 02:04:50.780]   Would only have Hot Word Detection
[02:04:50.780 --> 02:04:52.700]   enabled when a device is not in a meeting.
[02:04:52.700 --> 02:04:57.660]   So that's a good idea because you don't want it
[02:04:57.660 --> 02:05:01.100]   to suddenly wake up and play poor some sugar on me
[02:05:01.100 --> 02:05:03.820]   during your important business meeting.
[02:05:03.820 --> 02:05:08.940]   This will avoid accidental activation of the assistant during calls.
[02:05:08.940 --> 02:05:12.460]   That would be pretty funny.
[02:05:12.460 --> 02:05:18.140]   How to use emojis in Google Docs.
[02:05:18.140 --> 02:05:19.660]   It just got easier.
[02:05:19.660 --> 02:05:21.260]   Don't seen it.
[02:05:21.260 --> 02:05:21.820]   Oh, no.
[02:05:21.820 --> 02:05:22.460]   Don't.
[02:05:22.460 --> 02:05:26.300]   In fact, you could transcribe Shakespeare in emojis.
[02:05:27.980 --> 02:05:32.700]   If you want, just type at followed by the text
[02:05:32.700 --> 02:05:36.300]   like smile or dog in a dropdown menu will appear along.
[02:05:36.300 --> 02:05:37.180]   That's good.
[02:05:37.180 --> 02:05:40.220]   So at trigger the emoji insertion.
[02:05:40.220 --> 02:05:44.940]   This is what I feel like I'm supposed to be the old guy on the pound.
[02:05:44.940 --> 02:05:46.380]   Yeah, you are the old fart here.
[02:05:46.380 --> 02:05:48.460]   This isn't this a document.
[02:05:48.460 --> 02:05:51.260]   No, I don't need logos in that.
[02:05:51.260 --> 02:05:52.780]   What's wrong with you?
[02:05:52.780 --> 02:05:55.660]   Emojis are the best way to communicate.
[02:05:55.660 --> 02:05:57.340]   It's the new alphabet.
[02:05:57.820 --> 02:05:58.300]   All right.
[02:05:58.300 --> 02:06:02.940]   Okay, won't send you any smiley faces anytime soon.
[02:06:02.940 --> 02:06:06.220]   Of course, now everyone's going to you know, and everyone.
[02:06:06.220 --> 02:06:10.220]   How can you be the discord community manager?
[02:06:10.220 --> 02:06:11.100]   Yeah, you're.
[02:06:11.100 --> 02:06:13.100]   And that's exactly what I was going to say.
[02:06:13.100 --> 02:06:13.900]   It was like, how do you do it?
[02:06:13.900 --> 02:06:15.500]   That is a document.
[02:06:15.500 --> 02:06:17.500]   This is we're not talking about social.
[02:06:17.500 --> 02:06:18.620]   If I'm, oh, okay.
[02:06:18.620 --> 02:06:19.980]   I'm trying to do a document.
[02:06:19.980 --> 02:06:20.940]   I get it.
[02:06:20.940 --> 02:06:21.260]   Okay.
[02:06:21.260 --> 02:06:23.180]   Okay, that makes sense.
[02:06:23.180 --> 02:06:26.460]   Let me get a resume and somebody's got emojis on it.
[02:06:26.460 --> 02:06:28.060]   Well, so here's the deal though.
[02:06:28.060 --> 02:06:31.580]   Like sometimes when I'm editing, I'll be like,
[02:06:31.580 --> 02:06:36.220]   it's nice when you're editing someone's doc to be able to convey some tone.
[02:06:36.220 --> 02:06:43.340]   Because sometimes it's kind of like, you're telling someone that it's not right.
[02:06:43.340 --> 02:06:43.820]   Yeah.
[02:06:43.820 --> 02:06:46.940]   And if you could tell it to them nicely, that would be helpful.
[02:06:46.940 --> 02:06:48.380]   Oh boy.
[02:06:48.380 --> 02:06:49.900]   Android.
[02:06:49.900 --> 02:06:51.500]   You'd make a fire coach.
[02:06:51.500 --> 02:06:53.420]   Probably face me.
[02:06:53.420 --> 02:06:55.580]   Oh, no, I'd make a great coach.
[02:06:55.580 --> 02:06:56.940]   You'd make a horror coach.
[02:06:56.940 --> 02:06:57.900]   I have no problem.
[02:06:57.900 --> 02:06:58.460]   People.
[02:06:58.460 --> 02:07:04.140]   You're hearing that this is me being a woman and so many people being like,
[02:07:04.140 --> 02:07:05.420]   you're so brisk.
[02:07:05.420 --> 02:07:06.620]   You're so terse.
[02:07:06.620 --> 02:07:09.260]   And I'm like, I'm just telling you how it is people.
[02:07:09.260 --> 02:07:09.660]   Right.
[02:07:09.660 --> 02:07:11.660]   You don't need an emoji for that.
[02:07:11.660 --> 02:07:13.260]   Just put a period.
[02:07:13.260 --> 02:07:16.220]   I do because, oh no, periods are very bad.
[02:07:16.220 --> 02:07:17.180]   They're very aggressive.
[02:07:17.180 --> 02:07:19.340]   Gosh.
[02:07:19.340 --> 02:07:23.500]   You talked to Gen Z about periods.
[02:07:23.500 --> 02:07:25.260]   They are just like, why are you mad at me?
[02:07:25.260 --> 02:07:27.180]   Oh boy.
[02:07:27.180 --> 02:07:27.420]   Yeah.
[02:07:27.420 --> 02:07:31.740]   Android 13, everybody upgraded now on their pixels.
[02:07:31.740 --> 02:07:33.500]   Well, you wouldn't have heard of it.
[02:07:33.500 --> 02:07:34.780]   If you un-upgrade it.
[02:07:34.780 --> 02:07:35.020]   Yeah.
[02:07:35.020 --> 02:07:35.580]   Update it.
[02:07:35.580 --> 02:07:36.460]   It's exactly the same.
[02:07:36.460 --> 02:07:37.420]   Yep.
[02:07:37.420 --> 02:07:38.220]   What's the difference?
[02:07:38.220 --> 02:07:38.700]   I don't know.
[02:07:38.700 --> 02:07:40.380]   It's a minor update.
[02:07:40.380 --> 02:07:42.460]   Now we prepare for Android 14.
[02:07:42.460 --> 02:07:44.620]   Wow.
[02:07:44.620 --> 02:07:51.740]   Today they dropped the Pixel 4 and 4xL from the Android 13 beta program.
[02:07:52.860 --> 02:07:58.300]   So I don't, you know, 14 won't be till early next year.
[02:07:58.300 --> 02:07:59.980]   So don't, you can relax a little bit.
[02:07:59.980 --> 02:08:03.500]   Presumably the Pixel 7 and 7 will be,
[02:08:03.500 --> 02:08:05.580]   Pro will be added to the Android beta program.
[02:08:05.580 --> 02:08:07.100]   And that becomes available.
[02:08:07.100 --> 02:08:11.820]   And of course, I think Pixel 6, yes, and Pixel 5 even,
[02:08:11.820 --> 02:08:16.460]   will be for the quarterly platform release of Android 13.
[02:08:16.460 --> 02:08:19.740]   Every year, this time of year,
[02:08:20.540 --> 02:08:23.420]   my Nest doorbell starts playing spooky songs.
[02:08:23.420 --> 02:08:27.980]   Instead of Ding Dong, for fall of 2022,
[02:08:27.980 --> 02:08:31.420]   Google's Nest Doorbell will have three different holiday ringtones,
[02:08:31.420 --> 02:08:32.300]   including a new one.
[02:08:32.300 --> 02:08:35.100]   It starts in just a few days.
[02:08:35.100 --> 02:08:38.380]   You'll be able to turn on an October Fest theme.
[02:08:38.380 --> 02:08:41.180]   Oh, as your ringtone.
[02:08:41.180 --> 02:08:41.740]   Yeah.
[02:08:41.740 --> 02:08:42.300]   Oh, pop.
[02:08:42.300 --> 02:08:42.860]   Oh, pop.
[02:08:42.860 --> 02:08:43.260]   Oh, pop.
[02:08:43.260 --> 02:08:43.500]   Oh, bah, bah.
[02:08:43.500 --> 02:08:44.460]   And then.
[02:08:44.460 --> 02:08:47.340]   That ringtone will be available through October 5th,
[02:08:47.340 --> 02:08:50.460]   which is actually the dates of October 5th 2022 in Munich.
[02:08:50.460 --> 02:08:55.100]   It's mainly in September, just from our brain nose.
[02:08:55.100 --> 02:08:55.500]   Yeah.
[02:08:55.500 --> 02:08:57.820]   It's September 17th through October 3rd.
[02:08:57.820 --> 02:08:59.900]   So if you get there October 5th and you go,
[02:08:59.900 --> 02:09:01.020]   "Well, it's October."
[02:09:01.020 --> 02:09:01.260]   No.
[02:09:01.260 --> 02:09:01.900]   Here's the beer.
[02:09:01.900 --> 02:09:02.060]   Nope, no.
[02:09:02.060 --> 02:09:02.700]   Where's the beer?
[02:09:02.700 --> 02:09:06.940]   He says, the designer of this song says,
[02:09:06.940 --> 02:09:09.980]   "I listened to and analyzed the musical arrangements and styles
[02:09:09.980 --> 02:09:12.380]   of a variety of traditional polka songs
[02:09:12.380 --> 02:09:13.500]   that I composed a simple--
[02:09:13.500 --> 02:09:14.300]   Oh boy.
[02:09:14.300 --> 02:09:14.860]   Ow.
[02:09:14.860 --> 02:09:17.820]   That I composed a simple original melody on the piano
[02:09:17.820 --> 02:09:19.500]   at a rather slow tempo,
[02:09:19.500 --> 02:09:22.220]   along with separate harmony and bass lines using audio production
[02:09:22.220 --> 02:09:24.700]   software replaced each of the piano tracks
[02:09:24.700 --> 02:09:27.740]   with a digital sample of a traditional German accordion
[02:09:27.740 --> 02:09:29.660]   and sped up the tempo.
[02:09:29.660 --> 02:09:30.700]   Yes, that's on here?
[02:09:30.700 --> 02:09:31.980]   So it's doorbell friendly.
[02:09:31.980 --> 02:09:36.380]   Ah, did he put it?
[02:09:36.380 --> 02:09:36.860]   Let me see.
[02:09:36.860 --> 02:09:40.220]   Let me see if he put it on the keyword blog.
[02:09:40.220 --> 02:09:41.100]   I think he didn't.
[02:09:41.100 --> 02:09:44.220]   I think you're supposed to just be pleasantly surprised
[02:09:44.220 --> 02:09:46.780]   when it happens.
[02:09:47.660 --> 02:09:50.860]   Um, let's see, doorbell.
[02:09:50.860 --> 02:09:53.980]   It's good it doesn't have an F in it.
[02:09:53.980 --> 02:09:56.780]   I can type it.
[02:09:56.780 --> 02:09:57.980]   [laughter]
[02:09:57.980 --> 02:10:01.260]   Oh, I forgot about that.
[02:10:01.260 --> 02:10:01.740]   That quickly.
[02:10:01.740 --> 02:10:07.900]   Learn how we fine tune the Nest doorbell ringtones.
[02:10:07.900 --> 02:10:09.580]   Wow.
[02:10:09.580 --> 02:10:10.700]   Oh, here we go.
[02:10:10.700 --> 02:10:11.180]   Here's a--
[02:10:11.180 --> 02:10:11.740]   Okay.
[02:10:11.740 --> 02:10:15.260]   Here's one for Halloween.
[02:10:15.260 --> 02:10:17.340]   This is going to be a witch's cackle.
[02:10:17.340 --> 02:10:24.700]   We'll scare the cat and that's nice.
[02:10:24.700 --> 02:10:25.980]   Here's Thanksgiving gobble.
[02:10:25.980 --> 02:10:31.660]   What's funny is they do this and they don't tell you.
[02:10:31.660 --> 02:10:32.780]   So all of a sudden--
[02:10:32.780 --> 02:10:33.260]   Oh, really?
[02:10:33.260 --> 02:10:34.220]   I remember last year.
[02:10:34.220 --> 02:10:35.500]   Yeah, well, I'm on mine.
[02:10:35.500 --> 02:10:38.380]   I guess I have it whatever enabled, but yeah.
[02:10:38.380 --> 02:10:40.620]   So last year I was like, why is my doorbell?
[02:10:40.620 --> 02:10:42.620]   Sounds so spooky.
[02:10:42.620 --> 02:10:43.020]   Here we go.
[02:10:43.020 --> 02:10:44.380]   You ready for the October fest?
[02:10:44.380 --> 02:10:49.420]   [music]
[02:10:49.420 --> 02:10:51.020]   Okay, no, that's a winner.
[02:10:51.020 --> 02:10:53.660]   I would have that ball ear.
[02:10:53.660 --> 02:10:55.340]   Yeah, here he is.
[02:10:55.340 --> 02:10:56.460]   Yeah, about what he is.
[02:10:56.460 --> 02:10:57.660]   He's doing it.
[02:10:57.660 --> 02:11:00.300]   This is the guy, my home recordings to--
[02:11:00.300 --> 02:11:01.420]   Oh, playing a cat and Sam.
[02:11:01.420 --> 02:11:02.140]   One more time.
[02:11:02.140 --> 02:11:03.180]   Oh, I got one today.
[02:11:03.180 --> 02:11:08.380]   That's a good doorbell.
[02:11:08.380 --> 02:11:10.380]   So I don't get visitors.
[02:11:10.380 --> 02:11:11.820]   This sells--
[02:11:11.820 --> 02:11:12.620]   Do you have an--
[02:11:12.620 --> 02:11:13.900]   Do you have an esterbell?
[02:11:14.860 --> 02:11:15.740]   No, that's right.
[02:11:15.740 --> 02:11:16.620]   I don't.
[02:11:16.620 --> 02:11:17.260]   I like it.
[02:11:17.260 --> 02:11:20.460]   It does Christmas songs too in Christmas.
[02:11:20.460 --> 02:11:23.980]   And that's when I first became aware of it several Christmases ago.
[02:11:23.980 --> 02:11:26.940]   I said, did my doorbell just do horse clip clops?
[02:11:26.940 --> 02:11:29.980]   And a winner--
[02:11:29.980 --> 02:11:31.260]   You heard a Clyde's deal.
[02:11:31.260 --> 02:11:34.380]   What the hell?
[02:11:34.380 --> 02:11:36.220]   You heard Clyde's deal?
[02:11:36.220 --> 02:11:41.260]   No, it was like a Christmas, you know, what a Christmas sleighbell thing.
[02:11:41.260 --> 02:11:41.500]   Yeah.
[02:11:42.300 --> 02:11:43.100]   Yay.
[02:11:43.100 --> 02:11:48.620]   Anyway, so that's something to look forward to.
[02:11:48.620 --> 02:11:50.460]   A horrible Michael Winslow impression.
[02:11:50.460 --> 02:11:55.340]   The mouth that roared.
[02:11:55.340 --> 02:12:00.460]   Watch material you and Android UIs interpreted in more physical ways.
[02:12:00.460 --> 02:12:06.780]   Design shared a video collage of animated experiments
[02:12:06.780 --> 02:12:09.500]   inspired by material design.
[02:12:09.500 --> 02:12:12.620]   This-- if this doesn't make you want Android 14, I don't know what will.
[02:12:12.620 --> 02:12:13.980]   Not.
[02:12:13.980 --> 02:12:16.620]   Why do they--
[02:12:16.620 --> 02:12:17.180]   How much--
[02:12:17.180 --> 02:12:17.180]   Who--
[02:12:17.180 --> 02:12:17.660]   Who--
[02:12:17.660 --> 02:12:20.380]   Who did this and how much money did he get paid to do it?
[02:12:20.380 --> 02:12:22.540]   This is crazy.
[02:12:22.540 --> 02:12:26.620]   All right, I want to see the computer they used to do this.
[02:12:26.620 --> 02:12:30.300]   Yeah, well, no, somebody spent hours animating this day's weeks.
[02:12:30.300 --> 02:12:31.980]   AI.
[02:12:31.980 --> 02:12:32.860]   Sure, it's all AI.
[02:12:32.860 --> 02:12:34.700]   Did you see--
[02:12:34.700 --> 02:12:36.780]   Okay, that's the Google change log.
[02:12:39.420 --> 02:12:40.460]   Oh, not--
[02:12:40.460 --> 02:12:41.740]   Sorry, Benito.
[02:12:41.740 --> 02:12:42.380]   Sorry, Benito.
[02:12:42.380 --> 02:12:44.860]   But Benito's poor wrist is shaking.
[02:12:44.860 --> 02:12:46.380]   Sorry, I made a jump.
[02:12:46.380 --> 02:12:48.780]   Did you see John Oliver on Sunday?
[02:12:48.780 --> 02:12:56.300]   Because remember, we spent all day Wednesday last week making silly pictures using AI, right?
[02:12:56.300 --> 02:12:57.740]   Those pictures were great.
[02:12:57.740 --> 02:13:00.300]   Podcast host eats lunch.
[02:13:00.300 --> 02:13:01.980]   Heavenless.
[02:13:01.980 --> 02:13:03.020]   So that Sunday--
[02:13:03.020 --> 02:13:04.060]   Host has legend.
[02:13:04.060 --> 02:13:04.700]   I--
[02:13:04.700 --> 02:13:05.180]   Was it?
[02:13:05.180 --> 02:13:05.580]   I don't know.
[02:13:05.580 --> 02:13:06.860]   Yeah, podcast legend has--
[02:13:06.860 --> 02:13:07.420]   Heavenless.
[02:13:07.420 --> 02:13:07.900]   J.
[02:13:07.900 --> 02:13:08.460]   Legend.
[02:13:08.460 --> 02:13:08.940]   Legend.
[02:13:08.940 --> 02:13:08.940]   Legend.
[02:13:08.940 --> 02:13:13.900]   Uh, that Sunday, John Oliver did a whole thing on--
[02:13:13.900 --> 02:13:13.900]   Uh--
[02:13:13.900 --> 02:13:16.780]   Dolly?
[02:13:16.780 --> 02:13:18.860]   No, he did the same one we did,
[02:13:18.860 --> 02:13:20.540]   I can't remember what his name was, but--
[02:13:20.540 --> 02:13:21.660]   And then he did a whole bunch of--
[02:13:21.660 --> 02:13:21.660]   In turn.
[02:13:21.660 --> 02:13:24.220]   Was it mid--
[02:13:24.220 --> 02:13:25.420]   Is that the name of it mid-turning?
[02:13:25.420 --> 02:13:26.460]   Yeah, yeah, yeah, yeah, yeah.
[02:13:26.460 --> 02:13:32.140]   And then he did a bunch of John Oliver things, right?
[02:13:32.140 --> 02:13:36.540]   And so I thought, well, that's pretty cool.
[02:13:37.260 --> 02:13:40.620]   There's-- this is really becoming the next big thing.
[02:13:40.620 --> 02:13:41.980]   Then I found--
[02:13:41.980 --> 02:13:43.580]   Let me see if I can find it again.
[02:13:43.580 --> 02:13:48.060]   I would like to say when I introduced it months ago, you poo-pooed me.
[02:13:48.060 --> 02:13:49.100]   Well, because it was terrible.
[02:13:49.100 --> 02:13:52.460]   But it's got really good.
[02:13:52.460 --> 02:13:53.980]   What exactly did he say, Mrs. Stacey?
[02:13:53.980 --> 02:13:54.620]   What did he say?
[02:13:54.620 --> 02:13:57.660]   He was-- we'd have to roll the tape,
[02:13:57.660 --> 02:14:00.220]   but he was basically like, this is stupid.
[02:14:00.220 --> 02:14:00.620]   What?
[02:14:00.620 --> 02:14:03.100]   This is-- you tell the computer to make something and it does?
[02:14:03.100 --> 02:14:03.580]   Hm.
[02:14:03.580 --> 02:14:05.660]   Well, I didn't know how good these things would.
[02:14:05.660 --> 02:14:09.420]   And I think a lot of it is they've gotten better and better and better, right?
[02:14:09.420 --> 02:14:11.900]   And there's a new--
[02:14:11.900 --> 02:14:14.460]   after you do your John Oliver, there was a new thing that they just started doing.
[02:14:14.460 --> 02:14:16.140]   A new trick Dolly has.
[02:14:16.140 --> 02:14:16.780]   What's that?
[02:14:16.780 --> 02:14:19.020]   Online 73.
[02:14:19.020 --> 02:14:21.900]   You can finish a painting.
[02:14:21.900 --> 02:14:26.940]   Ah, yes, that's one story I had.
[02:14:26.940 --> 02:14:28.700]   I thought I bookmarked this search engine,
[02:14:28.700 --> 02:14:31.500]   because I really wanted to play with it and search for--
[02:14:31.500 --> 02:14:32.940]   For the crop market place below that,
[02:14:32.940 --> 02:14:34.780]   how is it to go across all of them?
[02:14:34.780 --> 02:14:36.540]   Yeah, maybe that's the one I'm--
[02:14:36.540 --> 02:14:37.340]   What it is.
[02:14:37.340 --> 02:14:39.980]   What line again?
[02:14:39.980 --> 02:14:41.260]   The 74.
[02:14:41.260 --> 02:14:45.100]   Dolly adds out painting.
[02:14:45.100 --> 02:14:47.340]   Yes, I think maybe it's the--
[02:14:47.340 --> 02:14:48.780]   Oh, look how pretty that is.
[02:14:48.780 --> 02:14:50.380]   Okay, I take it back, Stacey.
[02:14:50.380 --> 02:14:51.100]   I was wrong.
[02:14:51.100 --> 02:14:52.540]   Outstanding.
[02:14:52.540 --> 02:14:52.860]   So hold on.
[02:14:52.860 --> 02:14:54.060]   Now, look at this.
[02:14:54.060 --> 02:14:54.540]   Look at this.
[02:14:54.540 --> 02:14:55.980]   Here's the girl with the paint--
[02:14:55.980 --> 02:14:56.460]   The--
[02:14:56.460 --> 02:14:57.500]   Perliering, yeah?
[02:14:57.500 --> 02:14:58.380]   The Perliering.
[02:14:58.380 --> 02:14:59.260]   Go the time lapse.
[02:14:59.260 --> 02:15:00.700]   I'll talk--
[02:15:00.700 --> 02:15:01.580]   And then--
[02:15:01.580 --> 02:15:03.020]   No, no, no, no, no, no.
[02:15:03.020 --> 02:15:03.820]   On the right--
[02:15:03.820 --> 02:15:04.940]   Oh, these are the preparedness.
[02:15:04.940 --> 02:15:07.900]   So it's generating the rest of the painting.
[02:15:07.900 --> 02:15:10.700]   You can pick what works, and then it'll keep going.
[02:15:10.700 --> 02:15:12.540]   And so it's showing her room now.
[02:15:12.540 --> 02:15:13.740]   Yeah.
[02:15:13.740 --> 02:15:16.140]   That's pretty wild.
[02:15:16.140 --> 02:15:17.820]   That's pretty cool.
[02:15:17.820 --> 02:15:21.500]   I do feel like this is gonna be, at some point,
[02:15:21.500 --> 02:15:26.780]   good enough that illustrators can just use this, right?
[02:15:26.780 --> 02:15:28.620]   There is--
[02:15:28.620 --> 02:15:30.700]   This is another thing that I bookmarked.
[02:15:32.700 --> 02:15:35.820]   The prompt marketplace below that
[02:15:35.820 --> 02:15:37.580]   is a place where you can--
[02:15:37.580 --> 02:15:38.860]   Prompt is--
[02:15:38.860 --> 02:15:39.420]   Because people--
[02:15:39.420 --> 02:15:43.980]   The art of making the right prompt is now what matters.
[02:15:43.980 --> 02:15:45.260]   And so people will share--
[02:15:45.260 --> 02:15:45.820]   Ah, okay.
[02:15:45.820 --> 02:15:48.540]   The prompts we create across the various services.
[02:15:48.540 --> 02:15:51.980]   This is not the one I was thinking of, though.
[02:15:51.980 --> 02:15:54.940]   Let me see if I can find this search tool, because
[02:15:54.940 --> 02:16:02.220]   the idea was you could see all of the things people have done,
[02:16:03.100 --> 02:16:05.660]   but searched by prompt, right?
[02:16:05.660 --> 02:16:07.580]   That's what the prompt thing is, I think.
[02:16:07.580 --> 02:16:12.380]   If you go dig into it, you're just dubious,
[02:16:12.380 --> 02:16:14.620]   'cause I put it on my part of the rundown,
[02:16:14.620 --> 02:16:15.900]   not your part of the rundown.
[02:16:15.900 --> 02:16:17.340]   It doesn't look like the one I was--
[02:16:17.340 --> 02:16:20.620]   Oh, it's not the one I want, so it's just no good.
[02:16:20.620 --> 02:16:21.100]   No.
[02:16:21.100 --> 02:16:22.780]   This one is even more fun.
[02:16:22.780 --> 02:16:24.700]   Well, let me look at the prompt.
[02:16:24.700 --> 02:16:25.980]   Everything you have is more fun.
[02:16:25.980 --> 02:16:27.420]   It was more fun.
[02:16:27.420 --> 02:16:29.180]   So where do I enter this search here?
[02:16:29.180 --> 02:16:31.260]   I don't know.
[02:16:31.260 --> 02:16:32.860]   Find out yourself, Grumpy.
[02:16:32.860 --> 02:16:33.980]   Okay, find a prompt.
[02:16:33.980 --> 02:16:35.100]   Oh my gosh, you guys.
[02:16:35.100 --> 02:16:38.300]   Like, who are those two old men on the mother's side?
[02:16:38.300 --> 02:16:40.620]   Yes, see, this didn't work.
[02:16:40.620 --> 02:16:41.420]   Stand by.
[02:16:41.420 --> 02:16:41.980]   So--
[02:16:41.980 --> 02:16:42.860]   Thank you.
[02:16:42.860 --> 02:16:43.820]   Waldorf and Stenler.
[02:16:43.820 --> 02:16:44.780]   Waldorf and Stenler.
[02:16:44.780 --> 02:16:46.540]   I wish I had bookmarked this.
[02:16:46.540 --> 02:16:47.340]   I thought I did.
[02:16:47.340 --> 02:16:51.420]   Oh, well, not worth spending a whole lot of time on.
[02:16:51.420 --> 02:16:53.260]   But I just like the idea that I could--
[02:16:53.260 --> 02:16:54.620]   Anyway, watch John Oliver.
[02:16:54.620 --> 02:16:57.660]   What was it next week tonight?
[02:16:57.660 --> 02:16:59.420]   Is that what it's called this week tonight?
[02:16:59.420 --> 02:16:59.980]   Last week.
[02:16:59.980 --> 02:17:00.460]   Last week.
[02:17:00.460 --> 02:17:01.260]   Last week tonight.
[02:17:01.260 --> 02:17:06.940]   He did, I thought, a very good job of playing with it.
[02:17:06.940 --> 02:17:07.820]   Well, because he--
[02:17:07.820 --> 02:17:11.660]   I can't play it, obviously, so you have to watch it.
[02:17:11.660 --> 02:17:13.420]   But he started doing some--
[02:17:13.420 --> 02:17:16.940]   He noticed that when you looked at--
[02:17:16.940 --> 02:17:17.900]   What was it called again?
[02:17:17.900 --> 02:17:20.140]   The one we used--
[02:17:20.140 --> 02:17:20.140]   Midjourney.
[02:17:20.140 --> 02:17:20.620]   Midjourney.
[02:17:20.620 --> 02:17:23.900]   And you search for late night hosts.
[02:17:23.900 --> 02:17:26.940]   Like, you know, Jimmy Fallon, there's 20 prompts.
[02:17:26.940 --> 02:17:30.220]   And, you know, Conan O'Brien, there's five.
[02:17:30.220 --> 02:17:32.380]   He had 483 prompts.
[02:17:32.380 --> 02:17:34.060]   And then he started looking.
[02:17:34.060 --> 02:17:38.140]   And there's a couple of people who really like doing a lot of John Oliver stuff.
[02:17:38.140 --> 02:17:44.860]   And eventually he came up with this John Oliver and a cabbage story.
[02:17:44.860 --> 02:17:47.740]   Nice little eagle boost, right?
[02:17:47.740 --> 02:17:52.380]   It segues to him marrying a cabbage in real life and then eating it.
[02:17:52.380 --> 02:17:55.260]   It's hallucinogenic.
[02:17:55.260 --> 02:17:58.860]   I can't describe it, but--
[02:17:58.860 --> 02:18:00.780]   I'll watch it right after I watch Hard Knocks.
[02:18:00.780 --> 02:18:02.700]   What's Hard Knocks?
[02:18:02.700 --> 02:18:05.340]   It's the NFL training camp.
[02:18:05.340 --> 02:18:06.460]   Oh, I like that show.
[02:18:06.460 --> 02:18:08.940]   Aren't they doing soccer this year?
[02:18:08.940 --> 02:18:11.580]   Oh, no, that's another one.
[02:18:11.580 --> 02:18:13.740]   That's the Amazon Prime one, I think they're doing soccer.
[02:18:13.740 --> 02:18:16.300]   Well, you know what else is premiering this week?
[02:18:16.300 --> 02:18:18.620]   The Lord of the Rings on Amazon on Friday.
[02:18:18.620 --> 02:18:20.860]   Are we excited about that?
[02:18:20.860 --> 02:18:25.340]   Or Love and Thunder comes out on September 8th.
[02:18:25.340 --> 02:18:27.260]   What's Love and Thunder?
[02:18:27.260 --> 02:18:28.300]   Thor.
[02:18:28.300 --> 02:18:29.820]   It's the Thor movie.
[02:18:29.820 --> 02:18:33.500]   Oh, it's something like the Marvel Cinematic Universe.
[02:18:33.500 --> 02:18:35.020]   Yeah.
[02:18:35.020 --> 02:18:39.260]   I don't follow that because I never read those.
[02:18:39.260 --> 02:18:42.140]   The comic books?
[02:18:42.140 --> 02:18:43.100]   It's a comic book.
[02:18:43.100 --> 02:18:44.860]   You don't have to read the source material.
[02:18:44.860 --> 02:18:47.260]   Hard it is all into that stuff.
[02:18:47.260 --> 02:18:52.220]   Anyway, I've been watching the Lord of the Rings one.
[02:18:52.220 --> 02:18:53.340]   It's pretty good.
[02:18:53.340 --> 02:18:57.900]   [SINGING]
[02:18:57.900 --> 02:18:59.500]   OK, I think we're done.
[02:18:59.500 --> 02:19:03.500]   I think it's time to do our last ad and then get you
[02:19:03.500 --> 02:19:08.380]   to do your picks of the week if you would be willing, shall we?
[02:19:08.380 --> 02:19:10.060]   I have a thing this week, y'all.
[02:19:10.060 --> 02:19:10.540]   Yay!
[02:19:10.540 --> 02:19:11.340]   It's going to be so great.
[02:19:11.340 --> 02:19:13.740]   Stacey's thing just around the corner.
[02:19:13.740 --> 02:19:15.580]   But first, a word from New Relic.
[02:19:15.580 --> 02:19:17.100]   Love New Relic.
[02:19:17.100 --> 02:19:19.580]   Developers, we love you.
[02:19:19.580 --> 02:19:21.820]   You're some of the most curious people
[02:19:21.820 --> 02:19:23.420]   to first explore the newest tech.
[02:19:23.420 --> 02:19:25.500]   Why do you know how things work?
[02:19:25.500 --> 02:19:26.380]   Why they work?
[02:19:27.020 --> 02:19:28.780]   You actually read the manuals, right?
[02:19:28.780 --> 02:19:32.460]   That's why so many engineers turn to New Relic.
[02:19:32.460 --> 02:19:35.900]   Because New Relic will tell you exactly what's going on
[02:19:35.900 --> 02:19:39.340]   about the things you build.
[02:19:39.340 --> 02:19:42.620]   Shows what's really happening in your software life cycle.
[02:19:42.620 --> 02:19:45.740]   One place to see the data from your entire stack.
[02:19:45.740 --> 02:19:48.300]   So you don't have to look into 16 different tools
[02:19:48.300 --> 02:19:49.900]   and make those connections manually.
[02:19:49.900 --> 02:19:52.220]   New Relic pin points issues.
[02:19:52.220 --> 02:19:53.660]   They even do it down to the line of code.
[02:19:53.660 --> 02:19:56.620]   So you know if there is a problem, why they're happening,
[02:19:56.620 --> 02:19:59.900]   you could fix it, recommit, resolve it quickly.
[02:19:59.900 --> 02:20:04.620]   That's why more than 14,000 companies use New Relic.
[02:20:04.620 --> 02:20:06.380]   When teams come together around data,
[02:20:06.380 --> 02:20:09.420]   New Relic lets you triage problems,
[02:20:09.420 --> 02:20:11.020]   be confident in decisions,
[02:20:11.020 --> 02:20:13.260]   and reduce the time needed to implement resolutions
[02:20:13.260 --> 02:20:15.340]   using data, not opinions.
[02:20:15.340 --> 02:20:19.420]   Use the data platform made for the curious access,
[02:20:19.420 --> 02:20:21.020]   the whole New Relic platform.
[02:20:21.020 --> 02:20:26.140]   And 100 gigabytes of data free every month, forever!
[02:20:26.860 --> 02:20:28.460]   No credit card required.
[02:20:28.460 --> 02:20:31.180]   Sign up at New Relic.com/twig
[02:20:31.180 --> 02:20:36.380]   anyw, re-l-i-c.com/twig
[02:20:36.380 --> 02:20:37.020]   New Relic.
[02:20:37.020 --> 02:20:39.500]   Love you New Relic, New Relic.
[02:20:39.500 --> 02:20:43.500]   .com/twigets-for the engineer in all of us.
[02:20:43.500 --> 02:20:47.340]   Now, Stacy's got a thing!
[02:20:47.340 --> 02:20:54.300]   Y'all, okay, you're no I'm obsessed with buttons.
[02:20:54.300 --> 02:20:54.940]   Love 'em.
[02:20:54.940 --> 02:20:55.580]   Yup.
[02:20:55.580 --> 02:20:58.620]   And I just picked up a button.
[02:20:58.620 --> 02:21:00.220]   It's a big button.
[02:21:00.220 --> 02:21:04.300]   It is a it's the tap dial switch from Phillips Hue.
[02:21:04.300 --> 02:21:05.660]   Oh, I've seen this.
[02:21:05.660 --> 02:21:07.180]   I want to see this show me.
[02:21:07.180 --> 02:21:09.980]   Well, I let me show you.
[02:21:09.980 --> 02:21:12.380]   Okay, so this is a magnet.
[02:21:12.380 --> 02:21:14.380]   This is so I can mount it to a wall if I want,
[02:21:14.380 --> 02:21:16.300]   but I never mount these two walls.
[02:21:16.300 --> 02:21:17.260]   Comes in white and black.
[02:21:17.260 --> 02:21:18.460]   It's $49.95.
[02:21:18.460 --> 02:21:20.940]   This is the dial.
[02:21:20.940 --> 02:21:23.580]   It dials.
[02:21:24.540 --> 02:21:25.420]   Dye-ling.
[02:21:25.420 --> 02:21:27.740]   So they've had switches before.
[02:21:27.740 --> 02:21:29.980]   I think I've had a few switches.
[02:21:29.980 --> 02:21:29.980]   Oh, y'all.
[02:21:29.980 --> 02:21:30.460]   Hold on.
[02:21:30.460 --> 02:21:31.020]   Yeah.
[02:21:31.020 --> 02:21:32.620]   Oh, you know I've got this for you.
[02:21:32.620 --> 02:21:33.420]   All right, you ready?
[02:21:33.420 --> 02:21:33.900]   Yeah.
[02:21:33.900 --> 02:21:34.460]   Gen one.
[02:21:34.460 --> 02:21:35.420]   Ah!
[02:21:35.420 --> 02:21:36.380]   That's the Hue tap.
[02:21:36.380 --> 02:21:37.420]   Yes, the Hue tap.
[02:21:37.420 --> 02:21:37.660]   Okay.
[02:21:37.660 --> 02:21:40.140]   This is a high Z-Tie-Zo.
[02:21:40.140 --> 02:21:43.340]   This is so look at the thickness.
[02:21:43.340 --> 02:21:45.340]   This does this one does not spin.
[02:21:45.340 --> 02:21:46.300]   All right.
[02:21:46.300 --> 02:21:49.740]   And because I am a completist in all things,
[02:21:49.740 --> 02:21:52.060]   here is the Hue button,
[02:21:52.060 --> 02:21:55.020]   which the middle one is a $29.90.
[02:21:55.020 --> 02:21:56.300]   And it only does one thing.
[02:21:56.300 --> 02:21:57.260]   It's on or off, right?
[02:21:57.260 --> 02:21:57.740]   It doesn't.
[02:21:57.740 --> 02:21:59.020]   It actually does.
[02:21:59.020 --> 02:22:00.780]   It's a one tap.
[02:22:00.780 --> 02:22:01.180]   Oh.
[02:22:01.180 --> 02:22:01.660]   Double tap.
[02:22:01.660 --> 02:22:02.140]   Multi-tap.
[02:22:02.140 --> 02:22:02.620]   Triple tap.
[02:22:02.620 --> 02:22:02.940]   Okay.
[02:22:02.940 --> 02:22:03.020]   Okay.
[02:22:03.020 --> 02:22:03.660]   So three things.
[02:22:03.660 --> 02:22:07.020]   This has four taps and they're nice.
[02:22:07.020 --> 02:22:07.980]   Hue's always done this.
[02:22:07.980 --> 02:22:12.460]   This is actually, so these,
[02:22:12.460 --> 02:22:14.300]   I don't know if you can see the dots
[02:22:14.300 --> 02:22:15.260]   because there's little dots.
[02:22:15.260 --> 02:22:16.300]   It's like rail dots on them.
[02:22:16.300 --> 02:22:17.660]   One, two, three, and four.
[02:22:17.660 --> 02:22:18.860]   Well, this one isn't rail.
[02:22:18.860 --> 02:22:20.620]   This is just like on there.
[02:22:20.620 --> 02:22:21.580]   So you can tell the difference,
[02:22:21.580 --> 02:22:23.100]   but this one is actually in Braille.
[02:22:23.100 --> 02:22:24.780]   So they're really getting,
[02:22:24.780 --> 02:22:26.380]   each iteration is getting better.
[02:22:26.380 --> 02:22:28.860]   But it's not just the taps.
[02:22:28.860 --> 02:22:30.300]   You can also turn, right?
[02:22:30.300 --> 02:22:31.340]   Yeah.
[02:22:31.340 --> 02:22:32.620]   So this is the dial.
[02:22:32.620 --> 02:22:35.660]   So this one adds the dimming capabilities.
[02:22:35.660 --> 02:22:37.420]   So this one, I only get the--
[02:22:37.420 --> 02:22:38.460]   Oh, second tap.
[02:22:38.460 --> 02:22:39.020]   Oh, sorry.
[02:22:39.020 --> 02:22:39.980]   I didn't realize.
[02:22:39.980 --> 02:22:41.180]   And you can turn.
[02:22:41.180 --> 02:22:41.900]   Well, yeah, I have the--
[02:22:41.900 --> 02:22:43.340]   So these are my scenes.
[02:22:43.340 --> 02:22:45.820]   So there's four scenes.
[02:22:45.820 --> 02:22:48.540]   Four scenes I can use.
[02:22:48.540 --> 02:22:51.340]   And then there's dialing up and down.
[02:22:51.340 --> 02:22:53.500]   And the dial is always just vouch.
[02:22:53.500 --> 02:22:54.300]   The buttons are--
[02:22:54.300 --> 02:22:57.340]   Doesn't that confuse you?
[02:22:57.340 --> 02:22:59.260]   No, the dial is independent of the buttons.
[02:22:59.260 --> 02:23:00.220]   So--
[02:23:00.220 --> 02:23:00.780]   The buttons are--
[02:23:00.780 --> 02:23:01.980]   The buttons stay where they are.
[02:23:01.980 --> 02:23:02.300]   Dial--
[02:23:02.300 --> 02:23:03.340]   It's a demonstration problem,
[02:23:03.340 --> 02:23:04.460]   not a dial problem, okay.
[02:23:04.460 --> 02:23:05.180]   Yeah.
[02:23:05.180 --> 02:23:05.740]   This is good.
[02:23:05.740 --> 02:23:06.780]   And it doesn't need wires.
[02:23:06.780 --> 02:23:07.420]   It's wireless.
[02:23:07.420 --> 02:23:08.780]   It's wireless.
[02:23:08.780 --> 02:23:09.420]   It's Zigbee.
[02:23:09.420 --> 02:23:12.940]   So it's Huzigbee, I should confirm.
[02:23:12.940 --> 02:23:13.740]   So yeah, this is--
[02:23:13.740 --> 02:23:15.100]   And it's heavy.
[02:23:15.100 --> 02:23:15.580]   Huz does--
[02:23:15.580 --> 02:23:17.020]   It's because the magnet in it
[02:23:17.020 --> 02:23:18.220]   for attaching it to the base--
[02:23:18.220 --> 02:23:19.020]   Heavy's good.
[02:23:19.020 --> 02:23:19.980]   It doesn't slide around.
[02:23:19.980 --> 02:23:20.540]   Heavy's good.
[02:23:20.540 --> 02:23:21.660]   Like for instance,
[02:23:21.660 --> 02:23:24.460]   I would like to have this on the side table
[02:23:24.460 --> 02:23:26.460]   in the living room where we watch TV
[02:23:26.460 --> 02:23:29.500]   so I could manipulate that without getting up.
[02:23:29.500 --> 02:23:30.940]   You know, I could just press a button,
[02:23:30.940 --> 02:23:32.300]   say, okay, it's movie night.
[02:23:32.300 --> 02:23:34.700]   That would be cool.
[02:23:34.700 --> 02:23:35.180]   Yeah.
[02:23:35.180 --> 02:23:35.660]   Yeah.
[02:23:35.660 --> 02:23:37.820]   Yeah, I keep this on my bedside table lamp.
[02:23:37.820 --> 02:23:38.860]   It just sits right.
[02:23:38.860 --> 02:23:39.420]   Yeah.
[02:23:39.420 --> 02:23:39.900]   It's like--
[02:23:39.900 --> 02:23:40.860]   Night time.
[02:23:40.860 --> 02:23:42.060]   Oh, that's got a magnet too.
[02:23:42.060 --> 02:23:43.020]   That's neat.
[02:23:43.020 --> 02:23:43.660]   Yeah.
[02:23:43.660 --> 02:23:44.700]   Very cool.
[02:23:44.700 --> 02:23:46.860]   How much is the new dials?
[02:23:46.860 --> 02:23:48.300]   Hue dials.
[02:23:48.300 --> 02:23:49.900]   $49.95.
[02:23:49.900 --> 02:23:50.860]   Hue ain't cheap.
[02:23:50.860 --> 02:23:52.220]   You have to--
[02:23:52.220 --> 02:23:54.380]   I presume you have to have a Hue hub and Hue lights.
[02:23:54.380 --> 02:23:57.740]   You have to have a Hue hub and lights.
[02:23:57.740 --> 02:23:59.660]   I guess you could use it with a Hue hub.
[02:23:59.660 --> 02:24:02.060]   I don't--
[02:24:02.060 --> 02:24:04.860]   I mean, you could jack around and go with like a Samsung,
[02:24:04.860 --> 02:24:06.060]   smart things Hue.
[02:24:06.060 --> 02:24:07.660]   Right.
[02:24:07.660 --> 02:24:08.700]   But I don't know why you would.
[02:24:08.700 --> 02:24:10.860]   And it is battery powered,
[02:24:10.860 --> 02:24:13.580]   so it's like AAA or AA batteries you put in there.
[02:24:13.580 --> 02:24:15.740]   Yeah, this looks like--
[02:24:15.740 --> 02:24:16.380]   Let me--
[02:24:16.380 --> 02:24:16.860]   A little bat.
[02:24:16.860 --> 02:24:17.980]   So it's not rechargeable.
[02:24:17.980 --> 02:24:18.860]   You put batteries in.
[02:24:18.860 --> 02:24:22.220]   Stacy has a screwdriver.
[02:24:22.220 --> 02:24:22.940]   Oh my goodness.
[02:24:22.940 --> 02:24:23.580]   It's the ready.
[02:24:23.580 --> 02:24:23.820]   Yes.
[02:24:23.820 --> 02:24:26.140]   Yeah, much respect.
[02:24:26.140 --> 02:24:26.300]   Get it out.
[02:24:26.300 --> 02:24:27.020]   Much respect.
[02:24:27.020 --> 02:24:31.580]   It's like, I can't get it out right now because--
[02:24:31.580 --> 02:24:33.020]   Yeah, that's the whole tool belt there.
[02:24:33.020 --> 02:24:33.900]   We're not seeing it.
[02:24:33.900 --> 02:24:37.020]   Night is almost as cool as my bottle opener at the desk.
[02:24:37.020 --> 02:24:38.220]   That's much respect.
[02:24:38.220 --> 02:24:42.860]   I have nine Hue lights,
[02:24:42.860 --> 02:24:44.460]   but all I do is I just turn them off on.
[02:24:44.460 --> 02:24:46.380]   Oh, that's a nice white-down sauce.
[02:24:46.380 --> 02:24:46.700]   What is that?
[02:24:46.700 --> 02:24:47.180]   A pipe?
[02:24:47.180 --> 02:24:48.060]   What do you got?
[02:24:48.060 --> 02:24:48.860]   I'm here to show them.
[02:24:48.860 --> 02:24:49.740]   Pipe in there.
[02:24:49.740 --> 02:24:51.020]   What is that?
[02:24:51.020 --> 02:24:52.380]   Oh, this is a candle snuffer.
[02:24:52.380 --> 02:24:53.500]   Oh, OK, sure.
[02:24:53.500 --> 02:24:55.660]   Sure, that's what that is.
[02:24:55.660 --> 02:24:56.700]   It's got a little antenna.
[02:24:56.700 --> 02:24:57.420]   Every one should have.
[02:24:57.420 --> 02:24:57.660]   What?
[02:24:57.660 --> 02:24:58.780]   You have a toolbox.
[02:24:58.780 --> 02:25:00.380]   It's a candle snuffer.
[02:25:00.380 --> 02:25:00.860]   It is--
[02:25:00.860 --> 02:25:01.260]   Do you--
[02:25:01.260 --> 02:25:02.780]   Jeff, you'll appreciate this one.
[02:25:02.780 --> 02:25:03.340]   What is that?
[02:25:03.340 --> 02:25:04.380]   A rectal thermometer?
[02:25:04.380 --> 02:25:05.020]   What is that?
[02:25:05.020 --> 02:25:06.380]   No, it's a little--
[02:25:06.380 --> 02:25:07.180]   It's a thermometer.
[02:25:07.180 --> 02:25:11.420]   It's a centimeter pica and inch.
[02:25:11.420 --> 02:25:12.380]   Oh, pica's.
[02:25:12.380 --> 02:25:13.740]   Yeah.
[02:25:13.740 --> 02:25:15.180]   And scissors.
[02:25:15.180 --> 02:25:16.940]   But you use that all the time.
[02:25:16.940 --> 02:25:19.340]   Got a little knife in here.
[02:25:19.340 --> 02:25:20.860]   Which do you use more than the
[02:25:20.860 --> 02:25:23.660]   candle snuffer or the pica inch measure?
[02:25:23.660 --> 02:25:26.220]   OK, I use the candle snuffer a lot.
[02:25:26.220 --> 02:25:28.780]   How big is this candle?
[02:25:28.780 --> 02:25:29.580]   Let's find out.
[02:25:29.580 --> 02:25:32.540]   It's a five pica.
[02:25:32.540 --> 02:25:33.740]   It's just right over the wick.
[02:25:33.740 --> 02:25:34.220]   It's right over the wick.
[02:25:34.220 --> 02:25:34.460]   Yeah.
[02:25:34.460 --> 02:25:35.500]   Five pica.
[02:25:35.500 --> 02:25:36.700]   Yeah, that's a big--
[02:25:36.700 --> 02:25:38.380]   That's a big candle.
[02:25:38.380 --> 02:25:40.540]   Mr. Jeff--
[02:25:40.540 --> 02:25:41.580]   Oh, I found it.
[02:25:41.580 --> 02:25:42.940]   Oh, it's in my--
[02:25:42.940 --> 02:25:44.380]   We were all trying to tell you.
[02:25:44.380 --> 02:25:45.740]   It's in my pic of the week.
[02:25:45.740 --> 02:25:46.860]   I'm trying to tell you in chat.
[02:25:46.860 --> 02:25:47.900]   It's in my pic of the week.
[02:25:47.900 --> 02:25:50.620]   Lexica, L-E-X-I-C-A dot art.
[02:25:50.620 --> 02:25:52.700]   So if you search for--
[02:25:52.700 --> 02:25:53.820]   This is what I wanted to do.
[02:25:53.820 --> 02:25:56.220]   John Oliver apparently--
[02:25:56.220 --> 02:25:56.940]   It's fun.
[02:25:56.940 --> 02:25:59.900]   And this searches all the stable diffusion images.
[02:25:59.900 --> 02:26:00.300]   OK.
[02:26:00.300 --> 02:26:02.060]   It will provide--
[02:26:02.060 --> 02:26:04.940]   Apparently, John--
[02:26:04.940 --> 02:26:07.420]   As you know, John Oliver is quite the fan of Adam Driver.
[02:26:07.420 --> 02:26:11.580]   So this one is a portrait of John Oliver
[02:26:11.580 --> 02:26:13.020]   standing next to Adam Driver.
[02:26:13.900 --> 02:26:16.460]   And the Carpathian army.
[02:26:16.460 --> 02:26:18.300]   Here is--
[02:26:18.300 --> 02:26:22.460]   I think these are hysterical.
[02:26:22.460 --> 02:26:23.900]   Their noses are so similar, right?
[02:26:23.900 --> 02:26:24.220]   Yeah.
[02:26:24.220 --> 02:26:29.580]   I have to say, OK, Stacey, I was wrong when I dismissed
[02:26:29.580 --> 02:26:33.020]   the idea of these AI-generated art.
[02:26:33.020 --> 02:26:34.140]   This is amazing.
[02:26:34.140 --> 02:26:35.980]   Amazing stuff.
[02:26:35.980 --> 02:26:36.620]   Yes.
[02:26:36.620 --> 02:26:38.620]   This is stable diffusion, not dolly.
[02:26:38.620 --> 02:26:40.540]   This one is stable diffusion.
[02:26:40.540 --> 02:26:41.820]   You can install yourself, right?
[02:26:41.820 --> 02:26:44.060]   Is that the one you can put on your own computer?
[02:26:44.060 --> 02:26:48.220]   You don't have to pay anything for it or anything like that.
[02:26:48.220 --> 02:26:49.340]   These are quite--
[02:26:49.340 --> 02:26:52.300]   That is--
[02:26:52.300 --> 02:26:53.020]   Stressing.
[02:26:53.020 --> 02:26:54.060]   They're very--
[02:26:54.060 --> 02:26:54.460]   Yeah.
[02:26:54.460 --> 02:26:57.100]   A portrait of John Oliver standing next to Adam Driver.
[02:26:57.100 --> 02:26:59.980]   Stoic, military uniform, fantasy, intricate, beautiful,
[02:26:59.980 --> 02:27:01.580]   highly detailed charcoal,
[02:27:01.580 --> 02:27:03.740]   centered dark, smoky digital painting,
[02:27:03.740 --> 02:27:05.980]   art station, concept art smooth,
[02:27:05.980 --> 02:27:09.100]   sharp focus, illustration art by Greg Grootkowski.
[02:27:09.100 --> 02:27:13.420]   And I guess that gave stable diffusion enough information
[02:27:13.420 --> 02:27:16.620]   to really get detailed on these.
[02:27:16.620 --> 02:27:20.620]   And unlike dolly, it doesn't seem to resist
[02:27:20.620 --> 02:27:24.060]   putting real people's faces in, right?
[02:27:24.060 --> 02:27:24.860]   Like that does--
[02:27:24.860 --> 02:27:27.740]   You could tell it's Adam Driver and John Oliver.
[02:27:27.740 --> 02:27:32.860]   Anyway, so if you're looking for ways without,
[02:27:32.860 --> 02:27:34.540]   you know, having to figure out what the--
[02:27:34.540 --> 02:27:35.340]   Give me a prompt.
[02:27:35.340 --> 02:27:36.780]   We can try one.
[02:27:36.780 --> 02:27:37.100]   Five million.
[02:27:37.100 --> 02:27:37.500]   Pretty much.
[02:27:37.500 --> 02:27:38.060]   Pretty much.
[02:27:38.460 --> 02:27:38.940]   What is it?
[02:27:38.940 --> 02:27:39.660]   Printing press.
[02:27:39.660 --> 02:27:43.100]   Now, normally you'd add a lot more features,
[02:27:43.100 --> 02:27:44.460]   but this search will search for--
[02:27:44.460 --> 02:27:45.820]   In this case, you'll see what--
[02:27:45.820 --> 02:27:47.420]   You're part of somebody else's search, right?
[02:27:47.420 --> 02:27:48.140]   Exactly.
[02:27:48.140 --> 02:27:50.700]   So a beautiful woodcut print--
[02:27:50.700 --> 02:27:54.140]   Ooh, that's interesting.
[02:27:54.140 --> 02:27:55.900]   Here's a printing press.
[02:27:55.900 --> 02:27:58.620]   The invention of the printing press using spaghetti.
[02:27:58.620 --> 02:28:02.220]   Minds come up with this.
[02:28:02.220 --> 02:28:05.500]   Small details, intricate,
[02:28:05.500 --> 02:28:08.220]   Canon 80 millimeters, cinematic lighting,
[02:28:08.220 --> 02:28:10.380]   West Anderson film, Kodakrome.
[02:28:10.380 --> 02:28:13.660]   Here it is, the invention of the printing press using spaghetti.
[02:28:13.660 --> 02:28:14.140]   Wow.
[02:28:14.140 --> 02:28:18.220]   Okay, infinite laughs.
[02:28:18.220 --> 02:28:20.060]   That's interesting.
[02:28:20.060 --> 02:28:22.460]   Here's Joe Biden foaming at the mouth in the oval office
[02:28:22.460 --> 02:28:25.100]   when a 3D printed AR-15 is shown to him.
[02:28:25.100 --> 02:28:26.060]   Wow.
[02:28:26.060 --> 02:28:27.660]   Okay, that's a little strange.
[02:28:27.660 --> 02:28:30.700]   Hyper-realistic humanoid robot.
[02:28:30.700 --> 02:28:32.860]   A lot of Joe Biden here.
[02:28:32.860 --> 02:28:34.620]   Joe Biden propaganda poster.
[02:28:37.980 --> 02:28:39.260]   Search for how to do it.
[02:28:39.260 --> 02:28:39.980]   Jack Bench.
[02:28:39.980 --> 02:28:41.980]   Benching press.
[02:28:41.980 --> 02:28:42.780]   Oh, there's press.
[02:28:42.780 --> 02:28:45.420]   See, press got in there with Arnold Schwarzenegger body.
[02:28:45.420 --> 02:28:46.540]   It is kind of creepy.
[02:28:46.540 --> 02:28:49.180]   Oh, put it in mole rat.
[02:28:49.180 --> 02:28:51.340]   Naked, it's mole rat.
[02:28:51.340 --> 02:28:52.300]   All right, let's do it.
[02:28:52.300 --> 02:28:53.340]   Just roll mole rat.
[02:28:53.340 --> 02:28:54.220]   Mole rat is enough.
[02:28:54.220 --> 02:28:56.060]   'Cause they're closed.
[02:28:56.060 --> 02:28:57.740]   You don't want to put naked in these things.
[02:28:57.740 --> 02:29:01.580]   Ah, a human mole rat dressed in 1700s royal attire.
[02:29:01.580 --> 02:29:04.300]   They're not naked mole rats.
[02:29:04.300 --> 02:29:05.100]   You know what?
[02:29:05.100 --> 02:29:06.540]   Can we get tardigrades in here?
[02:29:07.180 --> 02:29:08.540]   Oh, why not?
[02:29:08.540 --> 02:29:09.180]   Why not?
[02:29:09.180 --> 02:29:11.980]   323.
[02:29:11.980 --> 02:29:13.500]   Oh, baby.
[02:29:13.500 --> 02:29:14.540]   Getting creepy now.
[02:29:14.540 --> 02:29:17.660]   All kinds of tardigrade action.
[02:29:17.660 --> 02:29:21.580]   Anyway, that's l-e-x-i-c-a.art.
[02:29:21.580 --> 02:29:26.860]   It's a search engine for AI generated illustration,
[02:29:26.860 --> 02:29:30.220]   which I, you know, and I think it is.
[02:29:30.220 --> 02:29:31.900]   It's, I don't think it's gotten better,
[02:29:31.900 --> 02:29:34.060]   but I think humans have gotten better at figuring out
[02:29:34.060 --> 02:29:36.540]   how to get it to do their bidding.
[02:29:36.540 --> 02:29:37.260]   So the phrase-
[02:29:37.260 --> 02:29:38.140]   Well, that's a sense of-
[02:29:38.140 --> 02:29:40.220]   What happens, right?
[02:29:40.220 --> 02:29:40.940]   Yeah, right.
[02:29:40.940 --> 02:29:43.340]   Very interesting.
[02:29:43.340 --> 02:29:45.420]   That's the stable diffusion stuff.
[02:29:45.420 --> 02:29:46.380]   Symbiosis.
[02:29:46.380 --> 02:29:48.780]   Human machine symbiosis.
[02:29:48.780 --> 02:29:51.340]   Jeff, pick.
[02:29:51.340 --> 02:29:53.020]   Oh, well, my turn, okay.
[02:29:53.020 --> 02:29:54.780]   Of the week or number of the week from you.
[02:29:54.780 --> 02:29:55.980]   Uh, well, whatever.
[02:29:55.980 --> 02:29:57.740]   Well, so I just want to mention real quickly,
[02:29:57.740 --> 02:29:58.940]   I've talked about Crikey,
[02:29:58.940 --> 02:30:01.740]   the small Australian news site that is being sued by the
[02:30:01.740 --> 02:30:03.340]   dared lot one merback to sue them.
[02:30:03.340 --> 02:30:04.460]   They haven't go fund me.
[02:30:04.460 --> 02:30:04.780]   Good.
[02:30:04.780 --> 02:30:07.420]   Um, and they've raised almost half a million dollars.
[02:30:07.420 --> 02:30:09.180]   And they're trying to raise three billion dollars,
[02:30:09.180 --> 02:30:10.060]   or was it five million?
[02:30:10.060 --> 02:30:12.460]   Cause it's not going to be cheap.
[02:30:12.460 --> 02:30:12.780]   Yeah.
[02:30:12.780 --> 02:30:15.420]   Um, so Taylor runs, we love Taylor runs on the show.
[02:30:15.420 --> 02:30:16.620]   We should get her on, by the way.
[02:30:16.620 --> 02:30:18.460]   Uh, I'm like, do we know how?
[02:30:18.460 --> 02:30:19.020]   Do you know her?
[02:30:19.020 --> 02:30:20.300]   Do you have context?
[02:30:20.300 --> 02:30:21.980]   I feel like she would just blow me off.
[02:30:21.980 --> 02:30:23.020]   I don't know.
[02:30:23.020 --> 02:30:24.620]   She might not come in this tech.
[02:30:24.620 --> 02:30:25.980]   I would love to get her on.
[02:30:25.980 --> 02:30:26.780]   Are you kidding?
[02:30:26.780 --> 02:30:27.420]   I'll DM her.
[02:30:27.420 --> 02:30:28.220]   I'll DM him, please.
[02:30:28.220 --> 02:30:30.860]   She'd probably treat you with more respect than she would me.
[02:30:30.860 --> 02:30:33.020]   Probably not, but that's okay.
[02:30:33.020 --> 02:30:33.420]   All right.
[02:30:33.420 --> 02:30:38.060]   Um, so poor, poor Taylor had to cover the, um, crypto woodstock,
[02:30:38.060 --> 02:30:40.220]   which I see as the crypto Bohemian Grove.
[02:30:40.220 --> 02:30:42.700]   Where was it?
[02:30:42.700 --> 02:30:46.620]   It was there in, in a camp there in California, Idlewild,
[02:30:46.620 --> 02:30:49.580]   um, California.
[02:30:49.580 --> 02:30:51.100]   Now I've been to the Bohemian Grove.
[02:30:51.100 --> 02:30:53.340]   That was quite fun, but I don't know.
[02:30:53.340 --> 02:30:56.940]   And you know, by the way, I gave a lecture on what the internet was in the year
[02:30:56.940 --> 02:30:58.540]   and the early 90s to the Bohemian Grove.
[02:30:58.540 --> 02:30:59.020]   Wow.
[02:30:59.020 --> 02:30:59.340]   Yeah.
[02:30:59.340 --> 02:30:59.580]   Oh wow.
[02:30:59.580 --> 02:31:01.020]   Were they all naked there?
[02:31:01.020 --> 02:31:01.420]   No.
[02:31:01.420 --> 02:31:02.460]   You know, I think it's over.
[02:31:02.460 --> 02:31:03.500]   George Schultz was there.
[02:31:03.500 --> 02:31:09.100]   Uh, he and his cronies have a, had a cabin way high up away from everybody else.
[02:31:09.100 --> 02:31:11.580]   It was Bastille Day, it was July 14th.
[02:31:11.580 --> 02:31:18.860]   So a bunch of drunken bohemian club was originally a San Francisco men's club
[02:31:18.860 --> 02:31:22.540]   that was for bohemians, for artists, for actors, for performers,
[02:31:22.540 --> 02:31:26.220]   eventually became a very wealthy men's club in San Francisco.
[02:31:26.220 --> 02:31:31.020]   But what was interesting is they would invite actors and performers
[02:31:31.020 --> 02:31:32.780]   and they would let them be members for free.
[02:31:32.780 --> 02:31:36.700]   So they would put on shows for the very rich wealthy men.
[02:31:36.700 --> 02:31:40.460]   And they have a beautiful wooded grove up the road apiece.
[02:31:40.460 --> 02:31:41.740]   They also have a downtown club.
[02:31:41.740 --> 02:31:46.300]   So many years ago, a member invited me and, uh,
[02:31:46.300 --> 02:31:48.140]   it was Bastille Day.
[02:31:48.140 --> 02:31:53.340]   So there were a bunch of drunken billionaires playing the Marseille as
[02:31:53.340 --> 02:31:54.380]   marching around.
[02:31:54.380 --> 02:31:57.260]   I did pee on a tree because you're supposed to.
[02:31:57.260 --> 02:31:58.140]   That's part of the deal.
[02:31:58.140 --> 02:31:58.140]   Oh, still.
[02:31:58.140 --> 02:32:00.540]   Uh, and then they, it was actually,
[02:32:00.540 --> 02:32:01.580]   it was, you know what it really was?
[02:32:01.580 --> 02:32:03.900]   It's what Burning Man would be after a hundred years.
[02:32:03.900 --> 02:32:07.180]   Yeah, seriously.
[02:32:07.180 --> 02:32:09.020]   That's, that's something to put in Lexa.
[02:32:09.020 --> 02:32:09.580]   Lexa.
[02:32:09.580 --> 02:32:10.060]   Exactly.
[02:32:10.060 --> 02:32:10.700]   I was thinking that.
[02:32:10.700 --> 02:32:11.740]   Exactly.
[02:32:11.740 --> 02:32:12.380]   Very good.
[02:32:12.380 --> 02:32:13.740]   What Burning Man are there?
[02:32:13.740 --> 02:32:17.500]   Because they have, uh, at the beginning, they have a play that they do
[02:32:17.500 --> 02:32:22.700]   at the end of care or maybe it's at the end, they do, they have the end of cares
[02:32:22.700 --> 02:32:23.660]   and they burn things.
[02:32:23.660 --> 02:32:26.780]   Anyway, so this was for NFT people.
[02:32:26.780 --> 02:32:31.900]   Here's a, here's a member of the feminist rock group Pussy Riot
[02:32:31.900 --> 02:32:34.380]   talking about using crypto to raise money for Ukraine.
[02:32:34.380 --> 02:32:36.060]   Okay.
[02:32:36.060 --> 02:32:39.260]   Um, I'm fair.
[02:32:39.260 --> 02:32:41.740]   I'm really skeptical about all of this.
[02:32:41.740 --> 02:32:42.780]   Yeah.
[02:32:42.780 --> 02:32:43.260]   So am I.
[02:32:43.260 --> 02:32:46.060]   So then the next one, and I think he's a friend of both of ours,
[02:32:46.060 --> 02:32:48.140]   but Loeek.
[02:32:48.140 --> 02:32:49.100]   Loeek lemur.
[02:32:49.100 --> 02:32:50.140]   We were just talking about it.
[02:32:50.140 --> 02:32:50.700]   We were just talking about it.
[02:32:50.700 --> 02:32:51.100]   Oh yeah.
[02:32:51.100 --> 02:32:52.620]   So now he has another event.
[02:32:52.620 --> 02:32:56.060]   And another baby on the way, by the way.
[02:32:56.060 --> 02:32:57.580]   Another one really cheats.
[02:32:57.580 --> 02:32:58.140]   Yeah.
[02:32:58.140 --> 02:32:59.260]   Is he 50 years old?
[02:32:59.260 --> 02:33:05.820]   He, uh, he, uh, invited, uh, Lisa and, and, and I to Loeb a couple of years in Paris.
[02:33:05.820 --> 02:33:08.220]   It was really fun about 10 years ago.
[02:33:08.220 --> 02:33:09.020]   I love Loeek.
[02:33:09.020 --> 02:33:11.260]   Uh, but then he got a character.
[02:33:11.260 --> 02:33:12.700]   Yeah, I guess a little strange.
[02:33:12.700 --> 02:33:15.340]   He did an, he went, he joined the ayahuasca tribe.
[02:33:15.340 --> 02:33:16.380]   Yeah.
[02:33:16.380 --> 02:33:17.900]   And so this is, this is that.
[02:33:17.900 --> 02:33:25.660]   So he's now starting powah here, which is the combination of, um, web three,
[02:33:26.140 --> 02:33:28.380]   and ancient wisdom.
[02:33:28.380 --> 02:33:32.940]   My partner, my De Linna Saptori, also spent time with me in the forest,
[02:33:32.940 --> 02:33:38.620]   which is where we had the idea of powah, deeply immersed with our indigenous friends,
[02:33:38.620 --> 02:33:39.580]   the Ashken-keem-
[02:33:39.580 --> 02:33:41.180]   Wait a video or get the actual accent.
[02:33:41.180 --> 02:33:41.580]   I know.
[02:33:41.580 --> 02:33:42.780]   I can't because-
[02:33:42.780 --> 02:33:43.180]   Yeah.
[02:33:43.180 --> 02:33:43.340]   No.
[02:33:43.340 --> 02:33:44.060]   No.
[02:33:44.060 --> 02:33:44.060]   No.
[02:33:44.060 --> 02:33:46.620]   I also met Lucy Barin-Kova there.
[02:33:46.620 --> 02:33:54.620]   He introduced me and, uh, and the whole, the web group, uh, to, uh, President Sarkozy.
[02:33:54.620 --> 02:33:58.300]   We went to the presidential palace in Paris, got to meet Sarkozy.
[02:33:58.300 --> 02:33:59.340]   Very well connected.
[02:33:59.340 --> 02:33:59.900]   Oh.
[02:33:59.900 --> 02:34:03.420]   Didn't he, did he have something to do with seismic science?
[02:34:03.420 --> 02:34:03.420]   Yes.
[02:34:03.420 --> 02:34:04.300]   That was, that was him.
[02:34:04.300 --> 02:34:05.100]   That was seismic.
[02:34:05.100 --> 02:34:05.260]   Yeah.
[02:34:05.260 --> 02:34:05.980]   Okay.
[02:34:05.980 --> 02:34:09.740]   It was a very early kind of, um, Instagram-ish kind of thing.
[02:34:09.740 --> 02:34:10.060]   Yeah.
[02:34:10.060 --> 02:34:12.940]   So this is web three AI metaverse.
[02:34:12.940 --> 02:34:13.340]   Okay.
[02:34:13.340 --> 02:34:15.660]   Then conscious entrepreneurship.
[02:34:15.660 --> 02:34:16.700]   I don't know what that is.
[02:34:16.700 --> 02:34:16.940]   Good.
[02:34:16.940 --> 02:34:20.060]   And science of consciousness, then indigenous wisdom,
[02:34:20.060 --> 02:34:22.140]   then plant medicines and psychedelics.
[02:34:22.140 --> 02:34:23.820]   But where do I take the ayahuasca?
[02:34:23.820 --> 02:34:26.220]   I want to, I want to, where's the ayahuasca booth?
[02:34:26.220 --> 02:34:29.740]   Oh, get your tickets and a collector NFT.
[02:34:29.740 --> 02:34:30.220]   Of course.
[02:34:30.220 --> 02:34:30.780]   Of course.
[02:34:30.780 --> 02:34:31.180]   Notch.
[02:34:31.180 --> 02:34:32.460]   You know what?
[02:34:32.460 --> 02:34:33.420]   I love vote the week.
[02:34:33.420 --> 02:34:35.260]   And if he's, if he's into this, that's-
[02:34:35.260 --> 02:34:35.900]   God bless him.
[02:34:35.900 --> 02:34:36.460]   God bless him.
[02:34:36.460 --> 02:34:37.420]   Congratulations.
[02:34:37.420 --> 02:34:40.540]   Um, I will watch this video.
[02:34:40.540 --> 02:34:42.140]   I still can't quite figure out what it's about.
[02:34:42.140 --> 02:34:44.380]   In prizes from reading that newsletter myself.
[02:34:44.380 --> 02:34:47.100]   And what's your thing of the week?
[02:34:47.100 --> 02:34:51.180]   My thing in a week, I didn't really have anything
[02:34:51.180 --> 02:34:53.180]   because I hadn't had time to play with some stuffs.
[02:34:53.180 --> 02:34:57.820]   But I brought up this, this football season is here video
[02:34:57.820 --> 02:34:59.980]   because we talked about it so excited.
[02:34:59.980 --> 02:35:01.020]   Floss weekly.
[02:35:01.020 --> 02:35:02.620]   I read it for some football.
[02:35:02.620 --> 02:35:05.100]   And if you skip to 8'10, I just thought it'd be funny
[02:35:05.100 --> 02:35:08.380]   because we did have a chat with Queen Pruitt pre-show.
[02:35:08.380 --> 02:35:11.340]   And this is just her, this new highlight.
[02:35:11.340 --> 02:35:14.460]   Is this at a Clemson game?
[02:35:14.460 --> 02:35:14.940]   Or what is this?
[02:35:14.940 --> 02:35:16.940]   This is at a former Clemson tailgate.
[02:35:16.940 --> 02:35:19.580]   Is she trying to celebrate like the ballplayers?
[02:35:19.580 --> 02:35:21.020]   Dude, I don't even say, you know what?
[02:35:21.020 --> 02:35:22.220]   That would mean just yet.
[02:35:23.180 --> 02:35:24.780]   But watch the chest bump.
[02:35:24.780 --> 02:35:25.740]   Yeah.
[02:35:25.740 --> 02:35:29.980]   That's why they're not to do it with you.
[02:35:29.980 --> 02:35:31.420]   She says, "I know I won't fall."
[02:35:31.420 --> 02:35:32.700]   So you're gonna knock her right over.
[02:35:32.700 --> 02:35:34.380]   Are you, she can do it with you?
[02:35:34.380 --> 02:35:35.740]   Yep.
[02:35:35.740 --> 02:35:36.780]   Oh, no, no, no, no.
[02:35:36.780 --> 02:35:37.980]   You're gonna knock her right over.
[02:35:37.980 --> 02:35:38.380]   Boom!
[02:35:38.380 --> 02:35:40.220]   Oh, geez.
[02:35:40.220 --> 02:35:41.740]   That's me.
[02:35:41.740 --> 02:35:44.620]   Hey, hey, I did nothing.
[02:35:44.620 --> 02:35:50.940]   Well, now we know, now we know the answer to the question.
[02:35:50.940 --> 02:35:51.580]   What happens?
[02:35:52.220 --> 02:35:53.420]   When they're immovable.
[02:35:53.420 --> 02:35:56.220]   What is it?
[02:35:56.220 --> 02:35:58.300]   Fableable Force meets as something after.
[02:35:58.300 --> 02:35:59.420]   Irresistible objects?
[02:35:59.420 --> 02:35:59.980]   Yes.
[02:35:59.980 --> 02:36:02.300]   I don't know if I'm irresistible.
[02:36:02.300 --> 02:36:02.300]   Yes.
[02:36:02.300 --> 02:36:05.740]   Wow.
[02:36:05.740 --> 02:36:07.740]   That just catered to my mind.
[02:36:07.740 --> 02:36:08.380]   I love it.
[02:36:08.380 --> 02:36:12.700]   I wanted to give a shout out because I had an honor and pleasure
[02:36:12.700 --> 02:36:16.540]   of shooting Miss Natalie Conklin and Miss Shayna Harding.
[02:36:16.540 --> 02:36:20.300]   They are the owners of Guderian lines.
[02:36:20.300 --> 02:36:21.740]   The images you did for them.
[02:36:21.740 --> 02:36:22.300]   In Napa.
[02:36:22.300 --> 02:36:25.260]   Yeah, these are in, um, Miss Natalie is a...
[02:36:25.260 --> 02:36:25.820]   They're vintage.
[02:36:25.820 --> 02:36:27.740]   Big-time Olympic gold medalist.
[02:36:27.740 --> 02:36:28.540]   Oh.
[02:36:28.540 --> 02:36:31.980]   And she's from here and it was, I really appreciate them, uh...
[02:36:31.980 --> 02:36:32.620]   What is she?
[02:36:32.620 --> 02:36:33.180]   What is she?
[02:36:33.180 --> 02:36:34.460]   What is she, uh, golden?
[02:36:34.460 --> 02:36:36.060]   What was swimming?
[02:36:36.060 --> 02:36:36.460]   Swimming.
[02:36:36.460 --> 02:36:39.820]   She was in the, she swam in the year when Michael Phelps was swimming.
[02:36:39.820 --> 02:36:40.700]   He was blowing up.
[02:36:40.700 --> 02:36:42.220]   She was doing just the same.
[02:36:42.220 --> 02:36:44.860]   Quietly in the background, of course.
[02:36:44.860 --> 02:36:47.980]   Doing everything he did backward and in heels.
[02:36:47.980 --> 02:36:49.340]   But who gets the credit?
[02:36:49.340 --> 02:36:49.980]   I tell you.
[02:36:49.980 --> 02:36:50.780]   Nice.
[02:36:50.780 --> 02:36:51.420]   Pretty much.
[02:36:51.420 --> 02:36:52.380]   I've seen these pictures.
[02:36:52.380 --> 02:36:53.580]   They're beautiful.
[02:36:53.580 --> 02:36:53.900]   I love this.
[02:36:53.900 --> 02:36:54.700]   Thank you, sir.
[02:36:54.700 --> 02:36:56.060]   Guderian lines.
[02:36:56.060 --> 02:36:57.660]   G-A-D-E-R lines.
[02:36:57.660 --> 02:37:02.380]   I'm trying to get more in local business here, sir.
[02:37:02.380 --> 02:37:03.580]   Here's Ant at work.
[02:37:03.580 --> 02:37:05.500]   And I bet they...
[02:37:05.500 --> 02:37:06.060]   Show the show.
[02:37:06.060 --> 02:37:06.780]   Yeah, there you go.
[02:37:06.780 --> 02:37:08.700]   I bet they had a lot of fun with you.
[02:37:08.700 --> 02:37:09.660]   Yeah, it was a lot of fun.
[02:37:09.660 --> 02:37:10.060]   Yeah.
[02:37:10.060 --> 02:37:13.740]   Way up and, um, right outside of Calistoga.
[02:37:13.740 --> 02:37:15.340]   Yeah, it's beautiful.
[02:37:15.340 --> 02:37:16.940]   It's a very nice, yeah.
[02:37:16.940 --> 02:37:18.220]   I'll give it a little heart myself.
[02:37:19.020 --> 02:37:19.740]   Thank you, sir.
[02:37:19.740 --> 02:37:20.780]   And you're in lastly.
[02:37:20.780 --> 02:37:21.500]   It's a nice print.
[02:37:21.500 --> 02:37:23.340]   Yeah, I got a new print up.
[02:37:23.340 --> 02:37:24.380]   I finally put it up.
[02:37:24.380 --> 02:37:25.020]   Oh, that's pretty.
[02:37:25.020 --> 02:37:26.700]   If you're interested in astrophotography,
[02:37:26.700 --> 02:37:27.580]   I got a print for you.
[02:37:27.580 --> 02:37:29.340]   Antpriot.com/print.
[02:37:29.340 --> 02:37:31.340]   So you can get it on a pillow.
[02:37:31.340 --> 02:37:32.220]   I can get it on a pillow.
[02:37:32.220 --> 02:37:32.780]   Or a pillow.
[02:37:32.780 --> 02:37:34.620]   Look at that.
[02:37:34.620 --> 02:37:35.740]   That is a pretty pillow.
[02:37:35.740 --> 02:37:36.380]   There you go.
[02:37:36.380 --> 02:37:37.820]   I'd sleep well on that pillow.
[02:37:37.820 --> 02:37:41.740]   Is the host of Hands-On Photography?
[02:37:41.740 --> 02:37:43.500]   What are you going to do this week?
[02:37:43.500 --> 02:37:47.100]   This week, I'm sitting down with a...
[02:37:48.460 --> 02:37:49.980]   And actually, he's a Twit fan.
[02:37:49.980 --> 02:37:51.740]   His name is Zach Sedewall.
[02:37:51.740 --> 02:37:54.140]   And we're going to talk about, um,
[02:37:54.140 --> 02:37:56.860]   brand photography and getting more into
[02:37:56.860 --> 02:37:59.740]   working with brands because he's a former
[02:37:59.740 --> 02:38:01.980]   creative director for a couple of different brands.
[02:38:01.980 --> 02:38:04.700]   And wanted to get his insight to help share
[02:38:04.700 --> 02:38:06.780]   with the community because people are
[02:38:06.780 --> 02:38:09.580]   taking photos and they're always asking,
[02:38:09.580 --> 02:38:11.260]   "Hey, well, how do I start my business?"
[02:38:11.260 --> 02:38:12.860]   Well, this is some of the tips.
[02:38:12.860 --> 02:38:16.780]   Ant will also be hosting a fireside chat
[02:38:16.780 --> 02:38:19.580]   in a couple of weeks with our club Twit members.
[02:38:19.580 --> 02:38:23.260]   On September 22nd and 9am Pacific.
[02:38:23.260 --> 02:38:23.980]   Club Twit.
[02:38:23.980 --> 02:38:26.220]   Ants, of course, the community manager there.
[02:38:26.220 --> 02:38:29.100]   That is our little membership club that we do.
[02:38:29.100 --> 02:38:32.460]   Just seven bucks a month to join Club Twit.
[02:38:32.460 --> 02:38:36.620]   All you have to do is go to twit.tv/clubtwit.
[02:38:36.620 --> 02:38:37.580]   Sign up.
[02:38:37.580 --> 02:38:41.580]   We use memberful, so it's very easy to do.
[02:38:41.580 --> 02:38:43.420]   There's a yearly plan as well.
[02:38:43.420 --> 02:38:47.500]   You get free, uh, ad-free versions.
[02:38:47.500 --> 02:38:48.140]   Well, they're not free.
[02:38:48.140 --> 02:38:48.860]   They're seven bucks a month.
[02:38:48.860 --> 02:38:51.260]   You get ad-free versions of all the shows.
[02:38:51.260 --> 02:38:54.700]   You get access to the Discord, which is fantastic.
[02:38:54.700 --> 02:38:57.420]   Really great place to converse.
[02:38:57.420 --> 02:38:59.980]   And look, they came up with a lexica search
[02:38:59.980 --> 02:39:02.220]   for a Elon Musk as Captain America.
[02:39:02.220 --> 02:39:02.780]   I love that.
[02:39:02.780 --> 02:39:05.660]   And you also get the Twit Plus feed,
[02:39:05.660 --> 02:39:07.580]   which includes shows that we do not offer
[02:39:07.580 --> 02:39:10.060]   in public because they're brand new.
[02:39:10.060 --> 02:39:12.620]   This is kind of our workshop is in the club
[02:39:12.620 --> 02:39:15.980]   for shows that don't have the audience yet to have advertising.
[02:39:15.980 --> 02:39:17.740]   So the club subsidizes them.
[02:39:17.740 --> 02:39:19.580]   Hands on windows with Paul Therat.
[02:39:19.580 --> 02:39:22.140]   Hands on Macintosh with Michael Sargent,
[02:39:22.140 --> 02:39:25.260]   the untitled Linux show with Jonathan Bennett,
[02:39:25.260 --> 02:39:28.140]   the Giz Fizz with Dick D. Bartolo, Stacey's book club
[02:39:28.140 --> 02:39:30.540]   with Stacy Higginbotham right here.
[02:39:30.540 --> 02:39:31.500]   Did you decide yet?
[02:39:31.500 --> 02:39:33.420]   Do we decide what the book's going to be next time?
[02:39:33.420 --> 02:39:36.060]   Uh, no, we will close the vote this weekend.
[02:39:36.060 --> 02:39:37.580]   Cause uh-
[02:39:37.580 --> 02:39:38.780]   Oh, but if you want to-
[02:39:38.780 --> 02:39:40.060]   Well, it says till 9/10.
[02:39:41.260 --> 02:39:41.980]   Okay, next week.
[02:39:41.980 --> 02:39:42.780]   The vote closes now.
[02:39:42.780 --> 02:39:43.820]   Next weekend, sorry.
[02:39:43.820 --> 02:39:44.380]   Yeah, yeah.
[02:39:44.380 --> 02:39:44.780]   Next weekend.
[02:39:44.780 --> 02:39:45.980]   That's awesome.
[02:39:45.980 --> 02:39:47.180]   Uh, I really-
[02:39:47.180 --> 02:39:48.540]   Go vote for your favorite book.
[02:39:48.540 --> 02:39:52.300]   Yeah, I really enjoyed the last book,
[02:39:52.300 --> 02:39:54.620]   the Clara and the Sun.
[02:39:54.620 --> 02:39:58.620]   Our, uh, our new choices are ancillary justice,
[02:39:58.620 --> 02:40:03.180]   the long way to a small angry planet, or wayward.
[02:40:03.180 --> 02:40:04.540]   But if you remember the club,
[02:40:04.540 --> 02:40:05.820]   go to the book club section,
[02:40:05.820 --> 02:40:09.180]   a club to it in the discord and vote.
[02:40:10.460 --> 02:40:11.900]   Right now it's very close.
[02:40:11.900 --> 02:40:14.700]   It could go any way, any way in either direction.
[02:40:14.700 --> 02:40:15.020]   Yeah.
[02:40:15.020 --> 02:40:22.620]   Um, and you also get our internal thanks and gratitude,
[02:40:22.620 --> 02:40:25.580]   cause it helps us keep all of the shows,
[02:40:25.580 --> 02:40:26.780]   uh, a flow of injury-
[02:40:26.780 --> 02:40:26.860]   Indeed.
[02:40:26.860 --> 02:40:27.980]   difficult times.
[02:40:27.980 --> 02:40:29.500]   Twit.t.t/club to it,
[02:40:29.500 --> 02:40:30.380]   seven bucks a month.
[02:40:30.380 --> 02:40:32.700]   Uh, it's a, it's fun too.
[02:40:32.700 --> 02:40:33.980]   It's a really fun place to be.
[02:40:33.980 --> 02:40:39.180]   Um, Jeff Jarvis, ladies, gentlemen,
[02:40:39.180 --> 02:40:40.780]   Frank Sinatra called him a bum.
[02:40:40.780 --> 02:40:43.900]   But he's also the director of the townite center for
[02:40:43.900 --> 02:40:46.780]   entrepreneurial journalism at the Craig Newman graduate
[02:40:46.780 --> 02:40:49.660]   school of journalism, the city university, York.
[02:40:49.660 --> 02:40:52.140]   Buzzmachine.com.
[02:40:52.140 --> 02:40:54.700]   He's everybody's-
[02:40:54.700 --> 02:40:56.220]   I don't think that'll ever get old.
[02:40:56.220 --> 02:40:58.300]   Everybody's favorite nickel millionaire.
[02:40:58.300 --> 02:41:01.580]   Well, once I have a, um, pre-order for the book
[02:41:01.580 --> 02:41:02.860]   up, which won't be until November,
[02:41:02.860 --> 02:41:04.300]   then that's getting rid of all that stuff.
[02:41:04.300 --> 02:41:05.660]   I'm gonna be gonna be pumping the book.
[02:41:05.660 --> 02:41:06.860]   Just put the, uh, just put the book.
[02:41:07.660 --> 02:41:09.420]   Yeah, you could screw Craig Newmark.
[02:41:09.420 --> 02:41:10.460]   I got a book.
[02:41:10.460 --> 02:41:11.580]   I got a book to say.
[02:41:11.580 --> 02:41:12.700]   Hi Craig.
[02:41:12.700 --> 02:41:13.740]   I got to sell this book.
[02:41:13.740 --> 02:41:14.140]   Just kidding.
[02:41:14.140 --> 02:41:14.940]   We're just kidding.
[02:41:14.940 --> 02:41:15.820]   Craig, we love you.
[02:41:15.820 --> 02:41:16.780]   We love you.
[02:41:16.780 --> 02:41:17.260]   We love you.
[02:41:17.260 --> 02:41:20.380]   You're the Matt Mullenweg of, uh, classified ads.
[02:41:20.380 --> 02:41:21.340]   Yeah.
[02:41:21.340 --> 02:41:23.580]   You probably wouldn't appreciate that either.
[02:41:23.580 --> 02:41:25.020]   That's kind of true though, isn't it?
[02:41:25.020 --> 02:41:25.660]   Actually, it is.
[02:41:25.660 --> 02:41:26.060]   So true.
[02:41:26.060 --> 02:41:26.460]   Yeah.
[02:41:26.460 --> 02:41:26.780]   Yeah.
[02:41:26.780 --> 02:41:27.580]   Open platform.
[02:41:27.580 --> 02:41:27.900]   Yeah.
[02:41:27.900 --> 02:41:27.900]   Yeah.
[02:41:27.900 --> 02:41:27.900]   Yeah.
[02:41:27.900 --> 02:41:28.380]   Yeah.
[02:41:28.380 --> 02:41:32.140]   Stacy Eigenbotham's at StacyOnIOT.com.
[02:41:32.140 --> 02:41:32.940]   Wake up, Stacy.
[02:41:32.940 --> 02:41:33.740]   She-
[02:41:33.740 --> 02:41:36.620]   I was playing with my dials.
[02:41:36.620 --> 02:41:38.220]   Oh, pushing the buttons.
[02:41:38.220 --> 02:41:39.260]   Got to push the buttons.
[02:41:39.260 --> 02:41:42.620]   If you go there, you can get her newsletter.
[02:41:42.620 --> 02:41:43.180]   That's free.
[02:41:43.180 --> 02:41:44.460]   You can listen to their podcast.
[02:41:44.460 --> 02:41:46.380]   Stacy and Kevin Tofoldo, great show.
[02:41:46.380 --> 02:41:47.500]   The IOT podcast.
[02:41:47.500 --> 02:41:51.580]   She's also got events and fun things going on.
[02:41:51.580 --> 02:41:55.740]   StacyOnIOT.com.
[02:41:55.740 --> 02:41:58.300]   We do this week at Google every Wednesday,
[02:41:58.300 --> 02:42:01.820]   round two Pacific five Eastern 2100 UTC.
[02:42:01.820 --> 02:42:04.060]   The live stream is at live.tuit.tv.
[02:42:04.060 --> 02:42:05.100]   That goes day and night,
[02:42:05.100 --> 02:42:07.820]   but that's when we'll be doing this week in Google.
[02:42:07.820 --> 02:42:11.180]   If you're watching live chat live in the public chat
[02:42:11.180 --> 02:42:13.900]   remierc.tuit.tv or in the club Twitch chat
[02:42:13.900 --> 02:42:14.780]   remon discord.
[02:42:14.780 --> 02:42:16.700]   After the fact, you can get the show from
[02:42:16.700 --> 02:42:18.140]   twit.tv/twig.
[02:42:18.140 --> 02:42:20.460]   That's free.
[02:42:20.460 --> 02:42:21.740]   You can also get it on YouTube.
[02:42:21.740 --> 02:42:24.140]   There's a dedicated channel for this week in Google
[02:42:24.140 --> 02:42:26.060]   or subscribing your favorite podcast client,
[02:42:26.060 --> 02:42:27.180]   audio or video.
[02:42:27.180 --> 02:42:30.380]   Pick one and get it done.
[02:42:30.380 --> 02:42:32.060]   So you can listen later in the week.
[02:42:32.060 --> 02:42:34.220]   Thanks everybody for joining us.
[02:42:34.220 --> 02:42:35.100]   Bye bye everybody.
[02:42:35.100 --> 02:42:37.740]   We'll see you next week on this week in Google.
[02:42:37.740 --> 02:42:39.100]   So long.
[02:42:39.100 --> 02:42:43.020]   Hey, I'm Rod Pyle, editor of Ad Astor magazine.
[02:42:43.020 --> 02:42:45.100]   And each week I'm joined by Tariq Malek,
[02:42:45.100 --> 02:42:47.820]   the editor in chief over at space.com.
[02:42:47.820 --> 02:42:49.980]   In our new this week in space podcast,
[02:42:49.980 --> 02:42:52.780]   every Friday Tariq and I take a deep dive into the stories
[02:42:52.780 --> 02:42:54.300]   that define the new space age.
[02:42:54.300 --> 02:42:55.420]   What's NASA up to?
[02:42:55.420 --> 02:42:58.140]   When will Americans once again set foot on the moon?
[02:42:58.140 --> 02:43:00.300]   And how about those samples from the Perseverance rover?
[02:43:00.300 --> 02:43:01.740]   One of those coming home.
[02:43:01.740 --> 02:43:03.980]   What the heck is Elon Musk done now?
[02:43:03.980 --> 02:43:06.700]   In addition to all the latest and greatest in space exploration,
[02:43:06.700 --> 02:43:09.660]   we'll take an occasional look at bits of space flight history
[02:43:09.660 --> 02:43:11.420]   that you probably never heard of.
[02:43:11.420 --> 02:43:14.140]   And all with an eye towards having a good time along the way.
[02:43:14.140 --> 02:43:19.900]   Check us out in your favorite podcatcher.
[02:43:19.900 --> 02:43:25.900]   [ Music ]
[02:43:25.900 --> 02:43:28.580]   (bell ringing)

