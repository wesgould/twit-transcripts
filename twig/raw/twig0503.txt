;FFMETADATA1
title=Get Off Stacey's Lawn!
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=503
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:02.720]   Coming up on this week in Google, I'm Jason Helfilling in for Leo LaPorte.
[00:00:02.720 --> 00:00:05.200]   Also Stacy Higginbotham and Jeff Jarvis are here.
[00:00:05.200 --> 00:00:07.160]   We're going to talk about Google Cloud Next.
[00:00:07.160 --> 00:00:08.880]   Stacy's going to break it down for us.
[00:00:08.880 --> 00:00:13.560]   Also Larry and Sergey, M.I.A. at the weekly Friday meetings at Google.
[00:00:13.560 --> 00:00:15.360]   Does this mean something bigger?
[00:00:15.360 --> 00:00:18.360]   G Suite and Google Home getting some updates.
[00:00:18.360 --> 00:00:21.800]   Google's Pixel 3A phone, a bunch of leaks there.
[00:00:21.800 --> 00:00:25.960]   And then we spend some time talking about how the Australian and UK
[00:00:25.960 --> 00:00:29.840]   governments are really tamping down on free speech, on social media.
[00:00:29.840 --> 00:00:31.320]   There's a lot of change happening.
[00:00:31.320 --> 00:00:33.600]   We'll discuss it next on this week in Google.
[00:00:33.600 --> 00:00:38.080]   Netcast you love.
[00:00:38.080 --> 00:00:39.520]   From people you trust.
[00:00:39.520 --> 00:00:44.960]   This is Twig.
[00:00:44.960 --> 00:00:52.200]   This is Twig.
[00:00:52.200 --> 00:00:58.480]   This week in Google, episode 503 recorded on Wednesday, April 10, 2019.
[00:00:58.480 --> 00:01:00.920]   Get off Stacy's lawn.
[00:01:00.920 --> 00:01:03.760]   This episode of This Week in Google is brought to you by CapTera.
[00:01:03.760 --> 00:01:07.680]   Find the right tools to make an informed software decision for your business.
[00:01:07.680 --> 00:01:12.480]   Visit capterasfreewebsite@captera.com/twig.
[00:01:12.480 --> 00:01:16.840]   And by ExpressVPN, protect your online privacy with one click.
[00:01:16.840 --> 00:01:18.360]   Yes, it's that easy.
[00:01:18.360 --> 00:01:25.160]   For three extra months free with a one-year package, go to expressvpn.com/twig.
[00:01:25.160 --> 00:01:30.200]   And by Thousand Eyes, companies that run in the cloud rely on Thousand Eyes.
[00:01:30.200 --> 00:01:35.120]   It's the place they go first to see, understand, and improve the digital experience
[00:01:35.120 --> 00:01:38.120]   of their cloud-based applications and services.
[00:01:38.120 --> 00:01:42.440]   Do the cloud write and improve services for your customers and employees today.
[00:01:42.440 --> 00:01:46.240]   Visit Thousand Eyes.com/twig.
[00:01:46.240 --> 00:01:49.120]   It's time for Twig this week in Google.
[00:01:49.120 --> 00:01:51.840]   And my oh my, there's a lot of Google this week.
[00:01:51.840 --> 00:01:57.040]   I'm Jason Howell, I'm filling in for Leo, who I believe is in Hawaii, maybe sipping cocktails
[00:01:57.040 --> 00:01:59.120]   on the beach.
[00:01:59.120 --> 00:02:00.600]   I hope he's on the beach anyways.
[00:02:00.600 --> 00:02:04.080]   If he's in Hawaii and he's not on the beach, he's Hawaiian wrong.
[00:02:04.080 --> 00:02:07.520]   Joining us today is Jeff Jarvis.
[00:02:07.520 --> 00:02:11.840]   I was given a sheet of paper, so I must read it, the Leonard Toe Professor for journalistic
[00:02:11.840 --> 00:02:15.720]   innovation at the Craig Newmark Graduate School of Journalism at the City University of New
[00:02:15.720 --> 00:02:21.440]   York and the Director of the Toe Knight Center for Entrepreneurial Journalism at the Craig
[00:02:21.440 --> 00:02:24.680]   Newmark Graduate School of Journalism at the City University of New York.
[00:02:24.680 --> 00:02:27.440]   I feel like I should be better at reading that faster.
[00:02:27.440 --> 00:02:28.440]   Who?
[00:02:28.440 --> 00:02:29.840]   No, he did a good job.
[00:02:29.840 --> 00:02:31.840]   He did a very good job.
[00:02:31.840 --> 00:02:34.680]   So I'm here to Stacey, I want to tell you where I am, but go ahead and introduce Stacey.
[00:02:34.680 --> 00:02:40.200]   Okay, also joining us, of course, Stacey Higumotham, creator of the IoT podcast, an awesome person
[00:02:40.200 --> 00:02:41.360]   of this week in Google.
[00:02:41.360 --> 00:02:42.360]   How are you Stacey?
[00:02:42.360 --> 00:02:43.520]   I am well, thank you.
[00:02:43.520 --> 00:02:45.240]   Great to have you here.
[00:02:45.240 --> 00:02:47.600]   So here we have a special guest.
[00:02:47.600 --> 00:02:48.600]   What's your first question?
[00:02:48.600 --> 00:02:50.600]   Well, I'm not a big Rochester.
[00:02:50.600 --> 00:02:51.600]   I land this morning.
[00:02:51.600 --> 00:02:53.840]   It's freaking snowing, of course.
[00:02:53.840 --> 00:02:55.240]   It's Rochester.
[00:02:55.240 --> 00:02:58.800]   And so I'm in the hotel and of course the Wi-Fi is awful because it's a hotel.
[00:02:58.800 --> 00:03:04.440]   So I go walking down the street and what do I see with this lovely coworking space called
[00:03:04.440 --> 00:03:07.760]   Metro Co-work right here in downtown Rochester.
[00:03:07.760 --> 00:03:12.040]   And then I'm in the conference room doing very long calls and I come out for a brief
[00:03:12.040 --> 00:03:15.680]   and I see someone who says, "Are you Jeff?"
[00:03:15.680 --> 00:03:16.680]   So here's a fan.
[00:03:16.680 --> 00:03:17.680]   Hello.
[00:03:17.680 --> 00:03:23.520]   I'm going to hand off the headset.
[00:03:23.520 --> 00:03:24.520]   Hello.
[00:03:24.520 --> 00:03:25.520]   Welcome to this week in Google.
[00:03:25.520 --> 00:03:26.520]   How are you?
[00:03:26.520 --> 00:03:27.520]   What's your name?
[00:03:27.520 --> 00:03:28.520]   My name is Dibia.
[00:03:28.520 --> 00:03:29.520]   Dibia Mystery.
[00:03:29.520 --> 00:03:31.680]   I'm just a software engineer out here in Rochester.
[00:03:31.680 --> 00:03:33.240]   Don't sell yourself short.
[00:03:33.240 --> 00:03:34.600]   Just a software engineer.
[00:03:34.600 --> 00:03:36.360]   You're our kind of people.
[00:03:36.360 --> 00:03:38.320]   That's great.
[00:03:38.320 --> 00:03:39.320]   It's great to meet you.
[00:03:39.320 --> 00:03:43.600]   And I'm sure you made Jeff made his day by recognizing him.
[00:03:43.600 --> 00:03:45.760]   He lives for moments like that.
[00:03:45.760 --> 00:03:46.760]   Yes.
[00:03:46.760 --> 00:03:53.200]   He's a first time seeing my people celebrity out in Oakland.
[00:03:53.200 --> 00:03:54.520]   So it's been great.
[00:03:54.520 --> 00:03:59.680]   It's really nice to meet you.
[00:03:59.680 --> 00:04:04.720]   And I'm sure you can download and keep that in his safekeeping.
[00:04:04.720 --> 00:04:05.720]   That's awesome.
[00:04:05.720 --> 00:04:06.720]   It really is.
[00:04:06.720 --> 00:04:07.720]   It's kind of great.
[00:04:07.720 --> 00:04:13.120]   I've done it in Munich, in Sydney, in all kinds of places.
[00:04:13.120 --> 00:04:14.680]   I've walked into the street.
[00:04:14.680 --> 00:04:16.760]   I don't just have to use Stacy in the UJ.
[00:04:16.760 --> 00:04:18.560]   But people say, are you?
[00:04:18.560 --> 00:04:19.560]   Oh, twig.
[00:04:19.560 --> 00:04:22.640]   And then we do the Leo hand shake.
[00:04:22.640 --> 00:04:24.800]   And it's pretty wonderful.
[00:04:24.800 --> 00:04:28.520]   So I just hear the Rochester open the door and they say, twig.
[00:04:28.520 --> 00:04:30.320]   So it's pretty cool.
[00:04:30.320 --> 00:04:32.680]   Does it ever take you by surprise though when that happens?
[00:04:32.680 --> 00:04:34.320]   Because you're like not in that mindset.
[00:04:34.320 --> 00:04:37.080]   And you're like, oh, I got to get into that mindset.
[00:04:37.080 --> 00:04:41.640]   You know, walking down a street in London and some guy, I think, oh my God, I want to
[00:04:41.640 --> 00:04:45.520]   not pay somebody, what do I do?
[00:04:45.520 --> 00:04:46.520]   It's nice.
[00:04:46.520 --> 00:04:51.360]   I can say it doesn't happen to me very often to answer your comment from a few minutes.
[00:04:51.360 --> 00:04:52.360]   That's why.
[00:04:52.360 --> 00:04:53.360]   Yeah.
[00:04:53.360 --> 00:04:54.640]   So it happened to me once.
[00:04:54.640 --> 00:04:59.160]   And I was at a Google event and the person in line turned around was like, Stacy, Stacy
[00:04:59.160 --> 00:05:00.240]   Higginbotham from twig.
[00:05:00.240 --> 00:05:04.000]   And I was like, hello.
[00:05:04.000 --> 00:05:05.000]   Do I know you?
[00:05:05.000 --> 00:05:07.040]   No, but I know everything about you through the show.
[00:05:07.040 --> 00:05:11.440]   That's the weird thing is when people do talk to you, they've heard you talk for
[00:05:11.440 --> 00:05:13.200]   hours on hours on hours.
[00:05:13.200 --> 00:05:16.960]   And so they know all of these weird, you know, they know things that you said three years
[00:05:16.960 --> 00:05:21.120]   ago that was just like a throwaway comment that you made.
[00:05:21.120 --> 00:05:24.840]   Meanwhile, in their minds, that's become, you know, part of their construct of who you
[00:05:24.840 --> 00:05:26.800]   are outside of the show.
[00:05:26.800 --> 00:05:30.480]   And so, you know, I've had that instance where they bring up some sort of topic.
[00:05:30.480 --> 00:05:32.720]   It's like, I have no idea what you're talking about, but yes.
[00:05:32.720 --> 00:05:33.720]   Right.
[00:05:33.720 --> 00:05:34.720]   But they know things.
[00:05:34.720 --> 00:05:35.720]   So these are right.
[00:05:35.720 --> 00:05:37.720]   People are about three years old.
[00:05:37.720 --> 00:05:42.040]   Maybe 10 years or now called ambient intimacy.
[00:05:42.040 --> 00:05:44.840]   That's because you follow someone in social, you follow them online.
[00:05:44.840 --> 00:05:47.840]   You feel like you know the word next time you see them, you know it all.
[00:05:47.840 --> 00:05:49.040]   Yep.
[00:05:49.040 --> 00:05:50.240]   That sounds about right.
[00:05:50.240 --> 00:05:51.720]   Sounds about right.
[00:05:51.720 --> 00:05:52.720]   That's awesome.
[00:05:52.720 --> 00:05:53.720]   Cool.
[00:05:53.720 --> 00:05:55.560]   Well, should we talk Google?
[00:05:55.560 --> 00:05:59.560]   Talk about some Google stuff because I think there's a lot going on.
[00:05:59.560 --> 00:06:04.720]   Let's start with this top story only because it was one of those things that happened that
[00:06:04.720 --> 00:06:07.520]   just just felt very Google to me.
[00:06:07.520 --> 00:06:13.080]   One week there was Google announcing its new AI ethics board.
[00:06:13.080 --> 00:06:16.400]   The next it was flat out canceled.
[00:06:16.400 --> 00:06:20.120]   Google decided to end it and go back to the drawing board as they said.
[00:06:20.120 --> 00:06:26.360]   And I'm sure this was talked about last week on the show before it was canceled when it
[00:06:26.360 --> 00:06:28.320]   was still the new thing.
[00:06:28.320 --> 00:06:29.520]   None of us were here last week.
[00:06:29.520 --> 00:06:30.520]   So, right?
[00:06:30.520 --> 00:06:31.520]   Right.
[00:06:31.520 --> 00:06:37.480]   So basically it was one kind of controversy or questionable, you know, character.
[00:06:37.480 --> 00:06:41.360]   After another first Alessandro at Quisti.
[00:06:41.360 --> 00:06:44.400]   I hope I'm pronouncing his last name properly.
[00:06:44.400 --> 00:06:51.800]   He bowed out initially saying that this wasn't the right way for him to be involved in tackling
[00:06:51.800 --> 00:06:53.920]   the ethical issues of AI.
[00:06:53.920 --> 00:06:55.960]   But that was just the beginning.
[00:06:55.960 --> 00:06:56.960]   K.
[00:06:56.960 --> 00:06:59.080]   Cole James came under fire, a significant scrutiny.
[00:06:59.080 --> 00:07:03.720]   There were employees at Google as well as a whole bunch of other people over comments
[00:07:03.720 --> 00:07:05.320]   about trans.
[00:07:05.320 --> 00:07:10.560]   The trans community also heritage foundations, climate change stance of which K is a member
[00:07:10.560 --> 00:07:13.720]   of the heritage foundation.
[00:07:13.720 --> 00:07:14.720]   Is it D.N.
[00:07:14.720 --> 00:07:19.040]   Or Diane Gubbins came under fire for his ties to drones.
[00:07:19.040 --> 00:07:25.280]   And of course Google has a storied history in the recent year of a lot of backlash around
[00:07:25.280 --> 00:07:29.480]   drones and government involvement and all that kind of stuff.
[00:07:29.480 --> 00:07:31.440]   It's just one thing after another.
[00:07:31.440 --> 00:07:37.720]   And we actually interviewed the author of this Vox article Kelsey Piper on Tech News
[00:07:37.720 --> 00:07:39.680]   Weekly last week.
[00:07:39.680 --> 00:07:47.160]   And Kelsey, you know, basically her position on this is that ethics boards for AI totally,
[00:07:47.160 --> 00:07:50.840]   you know, it's great that companies feel the need to do this.
[00:07:50.840 --> 00:07:51.840]   It needs to be done.
[00:07:51.840 --> 00:07:56.240]   We need to be looking at the ethical aspects of AI at these early stages.
[00:07:56.240 --> 00:08:01.160]   But from what she sees in their efforts in companies like Google doing this, they're
[00:08:01.160 --> 00:08:04.760]   not necessarily doing it through the guise of let's improve AI.
[00:08:04.760 --> 00:08:07.960]   It really feels more like a publicity stunt feels more like.
[00:08:07.960 --> 00:08:13.040]   But let's cover our rear ends, try and avoid regulation.
[00:08:13.040 --> 00:08:16.200]   And so maybe do it for the right reasons instead of the wrong reasons.
[00:08:16.200 --> 00:08:18.960]   Then you won't cancel your effort a week later.
[00:08:18.960 --> 00:08:23.160]   Oh, what do you guys think?
[00:08:23.160 --> 00:08:27.720]   Well, I, you know, you wonder about so Facebook is going to set up.
[00:08:27.720 --> 00:08:28.720]   It's the pre-court.
[00:08:28.720 --> 00:08:33.960]   And what I hear now is they're going to groups trying to understand how to go to groups,
[00:08:33.960 --> 00:08:36.920]   how to go to groups, how to nominate people, how to create it.
[00:08:36.920 --> 00:08:38.440]   It's you never going to be safe.
[00:08:38.440 --> 00:08:39.440]   You should not.
[00:08:39.440 --> 00:08:41.640]   You're going to do something and you're going to have somebody on there and somebody's going
[00:08:41.640 --> 00:08:42.640]   to disagree with them.
[00:08:42.640 --> 00:08:43.640]   And that's the world now.
[00:08:43.640 --> 00:08:44.640]   Yeah.
[00:08:44.640 --> 00:08:49.560]   And if you have a real AI ethics board that's trying to actually hash out meaty issues,
[00:08:49.560 --> 00:08:52.760]   there's going to be people with varying disagreements.
[00:08:52.760 --> 00:08:57.880]   And it won't matter as long as their contributions are valuable, much like you might hire engineers
[00:08:57.880 --> 00:09:01.600]   that kind of have differing opinions on things.
[00:09:01.600 --> 00:09:05.560]   But if you're just creating this like board just for optical purposes, then sure, it makes
[00:09:05.560 --> 00:09:09.760]   sense that you would be like, oh, not everyone's perfect.
[00:09:09.760 --> 00:09:13.880]   And I would say we don't, companies can do this.
[00:09:13.880 --> 00:09:14.880]   It's fine.
[00:09:14.880 --> 00:09:18.520]   I feel like, yes, they should probably have people asking the hard questions, who should
[00:09:18.520 --> 00:09:20.520]   probably just have a culture of that.
[00:09:20.520 --> 00:09:25.400]   But we really do need to have an independent body.
[00:09:25.400 --> 00:09:30.880]   Maybe it's like an NAB, maybe it's a governmental body, but we really do need to have, and companies
[00:09:30.880 --> 00:09:33.440]   need to put their representatives on it.
[00:09:33.440 --> 00:09:35.840]   And we need to have representatives from academia.
[00:09:35.840 --> 00:09:44.320]   So having these ethics boards captive to a commercial interest is kind of like letting
[00:09:44.320 --> 00:09:47.840]   your cat watch your chickens, cat skill chickens, right?
[00:09:47.840 --> 00:09:48.840]   Yeah.
[00:09:48.840 --> 00:09:49.840]   I think so.
[00:09:49.840 --> 00:09:52.440]   So yeah, I've never seen it firsthand, but I assume.
[00:09:52.440 --> 00:09:54.200]   Okay, letting a fox guard your head.
[00:09:54.200 --> 00:09:56.880]   I've also never seen a box.
[00:09:56.880 --> 00:10:02.520]   Well, I was going to go with a cat doing something else.
[00:10:02.520 --> 00:10:06.280]   And then I was like, ah, scrit chickens.
[00:10:06.280 --> 00:10:07.840]   Cat mouse doesn't matter.
[00:10:07.840 --> 00:10:10.320]   We know where we know where you're coming from.
[00:10:10.320 --> 00:10:11.320]   Not a good idea.
[00:10:11.320 --> 00:10:16.800]   Yeah, it's a bad idea, but it is good to take a take a close, solid look at this, because
[00:10:16.800 --> 00:10:21.240]   we keep seeing time and time again, you know, the projecting into the future as far as the
[00:10:21.240 --> 00:10:25.640]   direction of AI and how much influence it's such an early stage.
[00:10:25.640 --> 00:10:29.520]   It feels like it has in everything in the world of technology right now.
[00:10:29.520 --> 00:10:34.160]   It just feels as infused by the potential of AI and the capabilities of it.
[00:10:34.160 --> 00:10:36.560]   So who knows where that leads 10 years from now?
[00:10:36.560 --> 00:10:41.360]   We should be looking at it now for sure to really understand kind of how there can be,
[00:10:41.360 --> 00:10:46.560]   you know, how it can be abused, both from an intentional standpoint and an unintentional
[00:10:46.560 --> 00:10:47.560]   standpoint.
[00:10:47.560 --> 00:10:48.560]   So the efforts are good.
[00:10:48.560 --> 00:10:51.560]   I feel like I support that.
[00:10:51.560 --> 00:11:00.120]   But and at this time and place in technology companies totally under the gun, under the
[00:11:00.120 --> 00:11:04.440]   microscope being severely scrutinized right now.
[00:11:04.440 --> 00:11:09.720]   It's just a weird time to have this sort of flip-flop, you know?
[00:11:09.720 --> 00:11:14.400]   I mean, it doesn't look very good for Google to have this happen at this point.
[00:11:14.400 --> 00:11:19.600]   I think it just exposes how much window dressing these are.
[00:11:19.600 --> 00:11:25.560]   So how not silly, but just frivolous, frivolous.
[00:11:25.560 --> 00:11:30.720]   So does Google come back out again down the line with another ethics board and say, we're
[00:11:30.720 --> 00:11:31.840]   going to get it right this time?
[00:11:31.840 --> 00:11:35.080]   I mean, or do they go a completely different approach?
[00:11:35.080 --> 00:11:37.360]   Why don't we get like a true industry body?
[00:11:37.360 --> 00:11:39.360]   I mean, I would love to see Google fund.
[00:11:39.360 --> 00:11:44.640]   I mean, Montreal, a lot of the fabulous AI researchers come out of that university or
[00:11:44.640 --> 00:11:48.520]   Carnegie Mellon, they could create and help fund an institute.
[00:11:48.520 --> 00:11:54.160]   Big tech companies could participate or could help fund it, but ultimately it should be
[00:11:54.160 --> 00:11:59.680]   chaired by and held by somebody who ends have people on the board who are not behold into
[00:11:59.680 --> 00:12:00.680]   these companies.
[00:12:00.680 --> 00:12:01.680]   Right.
[00:12:01.680 --> 00:12:02.680]   Which I believe Stacy did that.
[00:12:02.680 --> 00:12:08.600]   I think there's one star at MIT that was funded by the Knight Foundation and a whole
[00:12:08.600 --> 00:12:12.640]   bunch of others that they've already funded something, whether it's the right view or
[00:12:12.640 --> 00:12:13.640]   not.
[00:12:13.640 --> 00:12:14.640]   I don't know.
[00:12:14.640 --> 00:12:15.640]   Okay.
[00:12:15.640 --> 00:12:16.640]   Yeah.
[00:12:16.640 --> 00:12:21.440]   There was the OpenAI group that Sam Altman did and I think it was Elon Musk and that
[00:12:21.440 --> 00:12:23.800]   is turning into a kerfuffle.
[00:12:23.800 --> 00:12:31.160]   But maybe I'm not familiar with the MIT one, but we should have several and it may just
[00:12:31.160 --> 00:12:32.160]   be.
[00:12:32.160 --> 00:12:37.240]   And another thought is because AI creeps into every single industry, you may actually want
[00:12:37.240 --> 00:12:42.520]   to have not the tech companies talking about this, but actually have industry experts.
[00:12:42.520 --> 00:12:47.600]   So have the banking industry come up with their own AI ethics committee and talk about
[00:12:47.600 --> 00:12:49.680]   things like redlining.
[00:12:49.680 --> 00:12:55.000]   And that may actually make more sense because AI itself isn't interesting.
[00:12:55.000 --> 00:12:58.400]   It's the applications and how you tend to use it that cause the problems.
[00:12:58.400 --> 00:12:59.400]   Sure.
[00:12:59.400 --> 00:13:02.440]   And they have the data sources.
[00:13:02.440 --> 00:13:05.680]   And the data sources.
[00:13:05.680 --> 00:13:07.880]   Or they could just call me an all Asian opinion.
[00:13:07.880 --> 00:13:08.880]   Okay.
[00:13:08.880 --> 00:13:09.880]   Well, that's easier.
[00:13:09.880 --> 00:13:12.280]   That's easier for everyone else.
[00:13:12.280 --> 00:13:14.840]   Not maybe not easier for you, but.
[00:13:14.840 --> 00:13:16.320]   We can make it a week.
[00:13:16.320 --> 00:13:18.080]   A feature on the show.
[00:13:18.080 --> 00:13:21.720]   Google asks should we or should we?
[00:13:21.720 --> 00:13:22.720]   Yes.
[00:13:22.720 --> 00:13:24.160]   I think here's a regular feature.
[00:13:24.160 --> 00:13:27.160]   It's basically Stacy gives her opinion.
[00:13:27.160 --> 00:13:32.040]   Whether you ask for it or not.
[00:13:32.040 --> 00:13:34.120]   Should we be concerned?
[00:13:34.120 --> 00:13:38.480]   Should we be worried about the fact that Larry Page and Sergey Brin are not attending the
[00:13:38.480 --> 00:13:43.160]   TGIF monthly meeting or weekly town hall meetings at Google anymore?
[00:13:43.160 --> 00:13:44.680]   They freak out.
[00:13:44.680 --> 00:13:45.680]   Yeah.
[00:13:45.680 --> 00:13:48.320]   Because that would be where they would talk about something like this.
[00:13:48.320 --> 00:13:49.320]   But they're not there.
[00:13:49.320 --> 00:13:53.760]   They're not giving anyone an opportunity to ask them about things like this.
[00:13:53.760 --> 00:13:54.760]   Stacy.
[00:13:54.760 --> 00:13:56.000]   I think we're trying to get.
[00:13:56.000 --> 00:14:01.480]   I guess I looked at this and I was like, I thought they tried to step back a while back.
[00:14:01.480 --> 00:14:03.480]   What are they doing?
[00:14:03.480 --> 00:14:04.480]   Never mind.
[00:14:04.480 --> 00:14:08.560]   I was going to say something horribly inappropriate, but they're building space cars and I don't
[00:14:08.560 --> 00:14:11.640]   know, going to Burning Man.
[00:14:11.640 --> 00:14:12.640]   They're enjoying.
[00:14:12.640 --> 00:14:14.880]   They're also trying to empower a Sundar.
[00:14:14.880 --> 00:14:15.880]   Well, that's true.
[00:14:15.880 --> 00:14:18.440]   They're not the boss at Google anymore.
[00:14:18.440 --> 00:14:19.440]   Sundar is.
[00:14:19.440 --> 00:14:20.440]   Right.
[00:14:20.440 --> 00:14:21.440]   Yeah.
[00:14:21.440 --> 00:14:22.440]   So wouldn't that happen?
[00:14:22.440 --> 00:14:24.600]   This was the shift to alphabet already 2015.
[00:14:24.600 --> 00:14:26.160]   I can't believe it's almost been four years.
[00:14:26.160 --> 00:14:27.600]   I'm going to sell all the time.
[00:14:27.600 --> 00:14:28.600]   It's crazy.
[00:14:28.600 --> 00:14:32.400]   And I still can't help but call it Google.
[00:14:32.400 --> 00:14:35.120]   Four years later, I'm still calling it Google.
[00:14:35.120 --> 00:14:40.240]   But they took that was their goal with that as part of that was to kind of restructure
[00:14:40.240 --> 00:14:41.680]   to step out of it.
[00:14:41.680 --> 00:14:47.960]   Even though after that, they still one of them continued to make it to these TGIF meetings
[00:14:47.960 --> 00:14:49.240]   for like the next three years.
[00:14:49.240 --> 00:14:52.440]   It's just been this year that both of them are M.I.A.
[00:14:52.440 --> 00:14:58.560]   Meanwhile, this has been possibly one of Google's most, you know, controversy filled
[00:14:58.560 --> 00:15:00.280]   with years.
[00:15:00.280 --> 00:15:05.840]   So many, so many things happening from drones, China search, sexual harassment, all these
[00:15:05.840 --> 00:15:09.800]   things happening that I'm sure employees would love to talk to them about.
[00:15:09.800 --> 00:15:15.520]   So, it's easy to see this in combination with that and think, oh, well, isn't it convenient
[00:15:15.520 --> 00:15:20.360]   that they don't have to have to face, you know, their own employees and actually talk
[00:15:20.360 --> 00:15:21.360]   about this stuff.
[00:15:21.360 --> 00:15:26.360]   But maybe it really is just them stepping out of the limelight even further.
[00:15:26.360 --> 00:15:27.560]   This would be a good time personally.
[00:15:27.560 --> 00:15:32.640]   I'd be like, yeah, you know, I think we said three years ago, yeah, we're doing that now
[00:15:32.640 --> 00:15:37.480]   because yeah, we want to be on the beach somewhere.
[00:15:37.480 --> 00:15:38.480]   On Google Island.
[00:15:38.480 --> 00:15:40.840]   Maybe they already live on Google Island.
[00:15:40.840 --> 00:15:43.480]   Oh, that's a good nation.
[00:15:43.480 --> 00:15:45.000]   We don't know it.
[00:15:45.000 --> 00:15:47.600]   They're building Google Island further and deep.
[00:15:47.600 --> 00:15:49.560]   The best way to do it is to live there.
[00:15:49.560 --> 00:15:53.040]   I want to go to there.
[00:15:53.040 --> 00:15:55.840]   There's Google Island.
[00:15:55.840 --> 00:15:57.960]   And then there's Google Island Underground.
[00:15:57.960 --> 00:16:01.280]   That's what they're really working on is their bunkers.
[00:16:01.280 --> 00:16:02.280]   Make sure they're better.
[00:16:02.280 --> 00:16:03.880]   They better be careful.
[00:16:03.880 --> 00:16:06.520]   They might get bored through.
[00:16:06.520 --> 00:16:09.280]   That's true.
[00:16:09.280 --> 00:16:12.400]   Elon Musk has a plan.
[00:16:12.400 --> 00:16:13.640]   Let's see here.
[00:16:13.640 --> 00:16:18.640]   Google drone is that's an April four.
[00:16:18.640 --> 00:16:19.640]   What's that?
[00:16:19.640 --> 00:16:20.640]   No, no, they've been testing.
[00:16:20.640 --> 00:16:23.280]   So the you skipped a story.
[00:16:23.280 --> 00:16:24.280]   No, it's OK.
[00:16:24.280 --> 00:16:25.520]   We don't have to go in order.
[00:16:25.520 --> 00:16:26.520]   There's no order here.
[00:16:26.520 --> 00:16:28.520]   We're here before AM.
[00:16:28.520 --> 00:16:32.600]   The car's wife will be throwing me through her fingers of the table at home.
[00:16:32.600 --> 00:16:33.600]   So no, no, no.
[00:16:33.600 --> 00:16:34.600]   There is no order.
[00:16:34.600 --> 00:16:36.400]   I think drones is great.
[00:16:36.400 --> 00:16:37.400]   We're kind of talking.
[00:16:37.400 --> 00:16:39.760]   Yeah, Project Wing.
[00:16:39.760 --> 00:16:45.760]   Basically the news here is that it's now operating officially in Canberra, Australia.
[00:16:45.760 --> 00:16:51.480]   They've been testing this in a suburb near Canberra for a year.
[00:16:51.480 --> 00:16:54.520]   And now they're kind of rolling it out to like four suburbs.
[00:16:54.520 --> 00:16:55.720]   It's kind of an official thing.
[00:16:55.720 --> 00:16:57.840]   It doesn't local businesses.
[00:16:57.840 --> 00:17:03.960]   Is this the first area that is officially being served by Project Wounds?
[00:17:03.960 --> 00:17:04.960]   I think it is.
[00:17:04.960 --> 00:17:05.960]   I think so.
[00:17:05.960 --> 00:17:09.320]   And I think the April Fool's joke you're talking about kind of relates to this actually
[00:17:09.320 --> 00:17:15.800]   was on April Fool's they did a Amazon drone delivery blimp thing.
[00:17:15.800 --> 00:17:18.440]   How ever.
[00:17:18.440 --> 00:17:22.600]   However, Amazon actually has a patent that was granted.
[00:17:22.600 --> 00:17:27.320]   I think it was like April 2nd for a delivery drone that was attached to basically a hot
[00:17:27.320 --> 00:17:28.560]   air balloon.
[00:17:28.560 --> 00:17:31.440]   So that was an April Fool's joke.
[00:17:31.440 --> 00:17:32.440]   That's real.
[00:17:32.440 --> 00:17:33.440]   It's not.
[00:17:33.440 --> 00:17:34.920]   It's called the Amazon mothership was the April Fool's joke.
[00:17:34.920 --> 00:17:42.560]   But the patent exists in Amazon, I guess, was or is looking at using some kind of quieter
[00:17:42.560 --> 00:17:43.760]   drone.
[00:17:43.760 --> 00:17:44.760]   And this is a big issue.
[00:17:44.760 --> 00:17:49.520]   So what Google found in their Ken Barrett testing was that dogs went nuts.
[00:17:49.520 --> 00:17:50.520]   People hated it.
[00:17:50.520 --> 00:17:54.840]   It was just because drones say are they're really irritating.
[00:17:54.840 --> 00:17:55.840]   Yeah.
[00:17:55.840 --> 00:17:59.400]   Well, the drone pilots.
[00:17:59.400 --> 00:18:02.440]   Yeah, those people can be irritating too.
[00:18:02.440 --> 00:18:07.160]   So this says that wing is they have quieter delivery models.
[00:18:07.160 --> 00:18:08.360]   So they responded to that.
[00:18:08.360 --> 00:18:10.800]   But you know, I don't think they're blimps or hot air balloons.
[00:18:10.800 --> 00:18:12.320]   So I don't know.
[00:18:12.320 --> 00:18:14.360]   There's still going to make noise.
[00:18:14.360 --> 00:18:15.360]   Yeah.
[00:18:15.360 --> 00:18:20.080]   I mean, this is really like drone sound convenient, but I'm like, this is really a convenience
[00:18:20.080 --> 00:18:21.080]   issue.
[00:18:21.080 --> 00:18:25.200]   Same thing with things like you think we're complaining about our sidewalks being like
[00:18:25.200 --> 00:18:29.240]   cluttered with micro mobility with scooters and bikes and everything.
[00:18:29.240 --> 00:18:34.400]   Imagine when we have last mile delivery robots, just like rooming around.
[00:18:34.400 --> 00:18:40.960]   And again, and I hate to say this because I used to not be so pro regulation, but I am
[00:18:40.960 --> 00:18:46.680]   just kind of like, God, tech firms just stop throwing crap out there and annoying people.
[00:18:46.680 --> 00:18:48.840]   You know, and I don't know.
[00:18:48.840 --> 00:18:50.920]   Think about the consequences.
[00:18:50.920 --> 00:18:55.720]   Maybe we don't all have to, I don't know, to everything all at once.
[00:18:55.720 --> 00:18:57.120]   Get off my line.
[00:18:57.120 --> 00:18:58.120]   Slow down.
[00:18:58.120 --> 00:18:59.120]   Quit innovating.
[00:18:59.120 --> 00:19:00.680]   Quit your innovation addiction.
[00:19:00.680 --> 00:19:05.960]   Or just give it, give, give the infrastructure some time to adapt.
[00:19:05.960 --> 00:19:06.960]   Yeah.
[00:19:06.960 --> 00:19:09.240]   Oh, but that's never going to happen.
[00:19:09.240 --> 00:19:10.240]   It will happen.
[00:19:10.240 --> 00:19:11.960]   We can't say that it's never going to happen.
[00:19:11.960 --> 00:19:14.840]   Otherwise we're giving them the keys to our future.
[00:19:14.840 --> 00:19:15.840]   You're right.
[00:19:15.840 --> 00:19:17.840]   That is a really good point.
[00:19:17.840 --> 00:19:22.440]   Stacey, they already got the keys and the lock and the door.
[00:19:22.440 --> 00:19:24.280]   No, they don't.
[00:19:24.280 --> 00:19:25.280]   We can.
[00:19:25.280 --> 00:19:27.640]   I mean, I'm in Austin.
[00:19:27.640 --> 00:19:29.880]   We voted to kick Uber out for a while.
[00:19:29.880 --> 00:19:30.880]   It actually happened.
[00:19:30.880 --> 00:19:34.720]   Yeah, like Austin is normal at any possible definition.
[00:19:34.720 --> 00:19:37.320]   Well, no, but we did bring in scooters.
[00:19:37.320 --> 00:19:39.400]   We actually allowed for limited trials.
[00:19:39.400 --> 00:19:42.280]   We put in, we put in some regulations.
[00:19:42.280 --> 00:19:46.000]   We tested them out around South by and put in more regulations there.
[00:19:46.000 --> 00:19:51.800]   They were basically trying to work out with companies, but not falling victim to their
[00:19:51.800 --> 00:19:53.360]   blackmail tactics.
[00:19:53.360 --> 00:19:57.720]   And I think that's what we need to do later on on the show.
[00:19:57.720 --> 00:20:02.480]   Believe it or not, I can talk about a regulatory regime that I like, which I don't think I've
[00:20:02.480 --> 00:20:04.480]   talked about before.
[00:20:04.480 --> 00:20:06.120]   All right.
[00:20:06.120 --> 00:20:07.120]   Sick.
[00:20:07.120 --> 00:20:08.120]   All right.
[00:20:08.120 --> 00:20:10.360]   First, personal record.
[00:20:10.360 --> 00:20:12.360]   Could be new content, could be old content.
[00:20:12.360 --> 00:20:14.320]   I have this problem with students all the time.
[00:20:14.320 --> 00:20:19.160]   I become told me that if I repeat myself, don't just sit there and say, oh, Jarvis, it's
[00:20:19.160 --> 00:20:20.160]   already good.
[00:20:20.160 --> 00:20:22.520]   You got to tell me that I sent this before.
[00:20:22.520 --> 00:20:23.520]   They do.
[00:20:23.520 --> 00:20:25.920]   As it turns out, you've said a lot of things.
[00:20:25.920 --> 00:20:27.080]   It's hard to keep track of it all.
[00:20:27.080 --> 00:20:28.720]   I only have so many things to say.
[00:20:28.720 --> 00:20:29.720]   Yeah.
[00:20:29.720 --> 00:20:30.720]   Right.
[00:20:30.720 --> 00:20:33.680]   Before you start repeating yourself, that's the only story of my life to.
[00:20:33.680 --> 00:20:35.800]   Have I talked about the way students interact?
[00:20:35.800 --> 00:20:36.800]   Oh, yes.
[00:20:36.800 --> 00:20:38.800]   No, no, you've never mentioned this.
[00:20:38.800 --> 00:20:40.800]   No, of course not.
[00:20:40.800 --> 00:20:41.800]   Never ever.
[00:20:41.800 --> 00:20:42.800]   Carson C.
[00:20:42.800 --> 00:20:43.800]   Carson would know.
[00:20:43.800 --> 00:20:47.480]   Carson, for speaking the truth.
[00:20:47.480 --> 00:20:49.080]   Should we talk about Google Cloud?
[00:20:49.080 --> 00:20:51.120]   Because I know this is today news.
[00:20:51.120 --> 00:20:56.040]   This is yesterday and today, Google Cloud Next 2019 happening.
[00:20:56.040 --> 00:20:57.520]   Why wouldn't we talk about it?
[00:20:57.520 --> 00:20:59.120]   And I sat down to do my homework.
[00:20:59.120 --> 00:21:00.720]   I'm like, all right, well, this is big news.
[00:21:00.720 --> 00:21:02.000]   I better understand this.
[00:21:02.000 --> 00:21:04.040]   And then eight paragraphs in.
[00:21:04.040 --> 00:21:06.600]   And I had birds swirling over my head.
[00:21:06.600 --> 00:21:11.920]   Because while I understand the consumer implications of why it's important for all these things
[00:21:11.920 --> 00:21:16.920]   to be taking place from the in the weeds perspective, I'm completely lost.
[00:21:16.920 --> 00:21:18.600]   So tell me stay safe.
[00:21:18.600 --> 00:21:20.840]   You can buzz in the air.
[00:21:20.840 --> 00:21:24.440]   Join me in the weeds.
[00:21:24.440 --> 00:21:25.600]   Take us into the weeds.
[00:21:25.600 --> 00:21:27.520]   What were you excited to hear about?
[00:21:27.520 --> 00:21:28.520]   Okay.
[00:21:28.520 --> 00:21:34.760]   So we talked about this on today's show because, okay, let me organize my thinking.
[00:21:34.760 --> 00:21:35.760]   All right.
[00:21:35.760 --> 00:21:40.000]   The big news is Google Anthos and the other big news item that I thought was exciting
[00:21:40.000 --> 00:21:43.240]   was Google Cloud Run and they're sort of related.
[00:21:43.240 --> 00:21:47.800]   So Anthos is a new cloud infrastructure platform for Google.
[00:21:47.800 --> 00:21:52.600]   And basically what Google's done here is made this really awesome Jiu Jitsu move where
[00:21:52.600 --> 00:21:55.520]   it has turned its weakness into a strength.
[00:21:55.520 --> 00:22:01.320]   So other cloud providers like Amazon and Microsoft are actually far and away bigger than Google
[00:22:01.320 --> 00:22:04.520]   when it comes to having customers adopt them.
[00:22:04.520 --> 00:22:07.480]   So Google's like, all right, and Google has great tech.
[00:22:07.480 --> 00:22:11.120]   I don't understand that is connect there, but it's there.
[00:22:11.120 --> 00:22:14.720]   Google said, all right, we're not going to be number one or number two.
[00:22:14.720 --> 00:22:21.760]   Let's open everything up and make it so people can run wherever they want, which is awesome.
[00:22:21.760 --> 00:22:24.040]   So what do you mean, wherever?
[00:22:24.040 --> 00:22:26.400]   Well, you can run on Microsoft's cloud.
[00:22:26.400 --> 00:22:27.800]   You can run on Amazon's cloud.
[00:22:27.800 --> 00:22:32.200]   You can run on Google's cloud and you can tie all those applications together.
[00:22:32.200 --> 00:22:33.960]   So that is what Anthos is doing.
[00:22:33.960 --> 00:22:37.000]   Don't the others have to cooperate with that?
[00:22:37.000 --> 00:22:42.760]   No, no, no, it becomes this dashboard where you can pull in all of your data from all
[00:22:42.760 --> 00:22:47.440]   of your applications running in various different clouds and on premise.
[00:22:47.440 --> 00:22:51.720]   And the reason why I was excited about this is because I am an IOT buff and we've got
[00:22:51.720 --> 00:22:56.240]   so many companies who are like, I'm going to do this calculation and stuff on prem, this
[00:22:56.240 --> 00:22:57.760]   I'm going to do in the cloud.
[00:22:57.760 --> 00:23:02.200]   So this is a way to tie it all together using Kubernetes.
[00:23:02.200 --> 00:23:08.760]   And so yay, and Kubernetes is a framework for managing applications across the cloud
[00:23:08.760 --> 00:23:13.760]   and in containers and it's awesome and Google invented a while back and it's open source
[00:23:13.760 --> 00:23:14.760]   and yay.
[00:23:14.760 --> 00:23:15.760]   Okay.
[00:23:15.760 --> 00:23:18.560]   So that's Anthos questions, comments.
[00:23:18.560 --> 00:23:20.200]   I'm excited because you're excited.
[00:23:20.200 --> 00:23:21.200]   It's Stacy.
[00:23:21.200 --> 00:23:24.560]   It is a really big deal.
[00:23:24.560 --> 00:23:28.720]   I mean, hey, interoperability, but that sounds like a great thing.
[00:23:28.720 --> 00:23:30.640]   That sounds like a lot of flexibility.
[00:23:30.640 --> 00:23:32.800]   It's a hard technical challenge.
[00:23:32.800 --> 00:23:34.760]   So there's a lot of questions here.
[00:23:34.760 --> 00:23:38.640]   Like how do you handle security for your applications running in between clouds?
[00:23:38.640 --> 00:23:40.880]   How is Google going to track price changes?
[00:23:40.880 --> 00:23:46.000]   Like there's going to be a lot to dig into here, but at its face, which is all I've got
[00:23:46.000 --> 00:23:50.880]   right now are their blog posts, you know, it's actually really cool.
[00:23:50.880 --> 00:23:55.080]   So cloud run, cloud run gets us into serverless.
[00:23:55.080 --> 00:23:57.040]   Do we remember what serverless is?
[00:23:57.040 --> 00:23:58.040]   Y'all?
[00:23:58.040 --> 00:24:02.360]   I mean, I can imagine it means there's no server.
[00:24:02.360 --> 00:24:03.360]   Yes.
[00:24:03.360 --> 00:24:04.360]   Okay, good.
[00:24:04.360 --> 00:24:10.200]   What's cool about serverless computing and you've got this with Google has this, Amazon
[00:24:10.200 --> 00:24:12.720]   has it, Amazon's is lambda.
[00:24:12.720 --> 00:24:18.120]   So what happens with serverless is you're using a container and that container is going
[00:24:18.120 --> 00:24:22.680]   to contain your application and all the libraries you need to run that application, which means
[00:24:22.680 --> 00:24:27.200]   now you've got this cool little container that you can just shove onto a server in the cloud,
[00:24:27.200 --> 00:24:32.360]   you can shove it down into a server at the edge, you can run it anywhere.
[00:24:32.360 --> 00:24:36.720]   You can actually run them on, I have someone who's running containers on light bulbs, you
[00:24:36.720 --> 00:24:37.720]   guys.
[00:24:37.720 --> 00:24:43.760]   So idea, yeah, you package up your application, you run it on a light bulb, a connected light.
[00:24:43.760 --> 00:24:45.080]   And so it's very cool.
[00:24:45.080 --> 00:24:46.320]   That's why containers are cool.
[00:24:46.320 --> 00:24:48.120]   They're tidy, they run anywhere.
[00:24:48.120 --> 00:24:49.120]   Yay.
[00:24:49.120 --> 00:24:50.120]   Okay.
[00:24:50.120 --> 00:24:51.120]   Nice.
[00:24:51.120 --> 00:24:52.120]   So.
[00:24:52.120 --> 00:24:58.720]   I can't tell you a whole lot more about that.
[00:24:58.720 --> 00:24:59.720]   Okay.
[00:24:59.720 --> 00:25:05.200]   So with cloud run, I can't tell you a whole lot more about that.
[00:25:05.200 --> 00:25:06.200]   Okay.
[00:25:06.200 --> 00:25:11.440]   So with cloud run, I can tell you a lot more about that.
[00:25:11.440 --> 00:25:19.080]   So I can tell you a lot more about that.
[00:25:19.080 --> 00:25:24.760]   Okay. So with cloud run, I kind of went off on a little Kubernetes tangent.
[00:25:24.760 --> 00:25:29.240]   So with cloud run, what you could or with serverless, what you can do is you throw those
[00:25:29.240 --> 00:25:31.920]   up and you run them for only when you need them.
[00:25:31.920 --> 00:25:35.600]   And this is really cool because you don't have to provision an entire server for what
[00:25:35.600 --> 00:25:37.000]   you're trying to do.
[00:25:37.000 --> 00:25:40.320]   You just are like, oh, I need to run it running.
[00:25:40.320 --> 00:25:41.320]   And then it shuts down.
[00:25:41.320 --> 00:25:42.620]   And so you don't pay as much.
[00:25:42.620 --> 00:25:43.840]   This is awesome for IoT.
[00:25:43.840 --> 00:25:45.440]   This is awesome for any functional things.
[00:25:45.440 --> 00:25:50.720]   Like I'm going to turn on an application when someone logs into my website or when someone
[00:25:50.720 --> 00:25:55.160]   signs up for something or clicks through on a ticket, you know, a mobile ticket, it's
[00:25:55.160 --> 00:26:01.560]   going to spin up a quick serverless infrastructure, serverless, a lambda or I guess cloud run.
[00:26:01.560 --> 00:26:06.280]   And then it'll be like, oh, I'm running the application, then I run down and I shut down.
[00:26:06.280 --> 00:26:07.800]   So that's what serverless is.
[00:26:07.800 --> 00:26:08.800]   It's awesome.
[00:26:08.800 --> 00:26:09.800]   It's cheaper.
[00:26:09.800 --> 00:26:10.800]   Yay.
[00:26:10.800 --> 00:26:13.760]   Cloud run is a way to do that.
[00:26:13.760 --> 00:26:19.160]   And you can do it across multiple clouds as well.
[00:26:19.160 --> 00:26:20.920]   So this is really awesome.
[00:26:20.920 --> 00:26:23.000]   It's HTTP driven container.
[00:26:23.000 --> 00:26:30.760]   So your container is going to get an HTTP URL and you basically run that wherever you
[00:26:30.760 --> 00:26:34.240]   want in a really easy way for developers to run it.
[00:26:34.240 --> 00:26:36.960]   So it's so, so, so, so, so, so, yeah.
[00:26:36.960 --> 00:26:37.960]   Okay.
[00:26:37.960 --> 00:26:38.960]   So you're right.
[00:26:38.960 --> 00:26:42.720]   You're pulling a media typey photo from here or there.
[00:26:42.720 --> 00:26:45.560]   You're pulling code from here or there.
[00:26:45.560 --> 00:26:46.560]   Yes.
[00:26:46.560 --> 00:26:47.560]   Yes.
[00:26:47.560 --> 00:26:50.880]   A container is your application and all the codes you need to run it.
[00:26:50.880 --> 00:26:54.640]   So then you just like fling that around wherever you want to run it in a really easy
[00:26:54.640 --> 00:26:55.640]   way.
[00:26:55.640 --> 00:26:57.960]   And it's pretty awesome.
[00:26:57.960 --> 00:27:02.360]   And so that's the super duper cloud stuff for all the super duper nerdy's out there.
[00:27:02.360 --> 00:27:05.640]   And I would love to hear more from y'all if you're a developer about how easy this is
[00:27:05.640 --> 00:27:10.160]   going to be to use or what you're worried about just because I want to know.
[00:27:10.160 --> 00:27:11.160]   Okay.
[00:27:11.160 --> 00:27:13.080]   And now we can talk about G Suite.
[00:27:13.080 --> 00:27:14.720]   G Suite on my.
[00:27:14.720 --> 00:27:15.720]   Okay.
[00:27:15.720 --> 00:27:17.680]   So G Suite.
[00:27:17.680 --> 00:27:18.680]   Yes.
[00:27:18.680 --> 00:27:20.520]   And we will let's see here.
[00:27:20.520 --> 00:27:24.240]   Let's scroll down because I think they ended up in the change like, but the change log is
[00:27:24.240 --> 00:27:25.640]   like huge.
[00:27:25.640 --> 00:27:26.920]   So any of this stuff.
[00:27:26.920 --> 00:27:29.200]   No, no, no, it's okay.
[00:27:29.200 --> 00:27:33.360]   Because there was a bunch of G Suite and this is the stuff that I could connect to a
[00:27:33.360 --> 00:27:34.520]   little bit more.
[00:27:34.520 --> 00:27:38.240]   In my mind, I could attach myself to G Suite a little bit more.
[00:27:38.240 --> 00:27:40.840]   But yes, so G Suite Google Home, right?
[00:27:40.840 --> 00:27:42.840]   Because now there's more assistance.
[00:27:42.840 --> 00:27:44.640]   Yeah, you can put it on a set.
[00:27:44.640 --> 00:27:50.520]   So, and I haven't tried this yet because I'm constantly sad by my Google Home experience.
[00:27:50.520 --> 00:27:53.920]   But yes, theoretically, because I'm too.
[00:27:53.920 --> 00:27:57.160]   Oh, yeah, we should do a whole show on that.
[00:27:57.160 --> 00:27:59.760]   Okay, we can talk about that in a second.
[00:27:59.760 --> 00:28:00.760]   Okay.
[00:28:00.760 --> 00:28:01.760]   Yes.
[00:28:01.760 --> 00:28:07.120]   But yes, theoretically, I could now access my G Suite calendar and data on my Google Assistant
[00:28:07.120 --> 00:28:10.480]   and presumably my Google Home.
[00:28:10.480 --> 00:28:15.560]   So is this when I read this, I immediately thought of you, Jeff, because you have historically
[00:28:15.560 --> 00:28:22.560]   been the one to complain about the fact that you have a G Suite account.
[00:28:22.560 --> 00:28:24.320]   That's your main account, right?
[00:28:24.320 --> 00:28:27.120]   Your Google Apps account, let's say.
[00:28:27.120 --> 00:28:31.440]   And so a lot of these features never hit you or they hit you much, much later.
[00:28:31.440 --> 00:28:33.600]   Is this news that made you happy?
[00:28:33.600 --> 00:28:36.880]   Like does this feel like a temporary, very strange?
[00:28:36.880 --> 00:28:37.880]   I think it's better.
[00:28:37.880 --> 00:28:42.440]   And then so I discovered a new problem this week.
[00:28:42.440 --> 00:28:47.640]   So I finally found out my Chromebook, how I can have, I didn't realize that for one
[00:28:47.640 --> 00:28:50.760]   click I could go from one account to another account, right?
[00:28:50.760 --> 00:28:51.760]   Oh, yeah.
[00:28:51.760 --> 00:28:54.680]   So my college account now is on Google.
[00:28:54.680 --> 00:28:55.680]   Oh, yeah.
[00:28:55.680 --> 00:28:56.680]   Nope.
[00:28:56.680 --> 00:29:00.320]   That administrator says this must be the first account you sign into.
[00:29:00.320 --> 00:29:07.320]   It's just thinking, I'm not going to answer your question, but I'm just more blind again.
[00:29:07.320 --> 00:29:08.320]   No.
[00:29:08.320 --> 00:29:14.040]   I have Jeff's problem and I was excited by this news, but I will say that I have yet to try
[00:29:14.040 --> 00:29:16.320]   it because it's been a busy day.
[00:29:16.320 --> 00:29:18.400]   Yeah, yeah.
[00:29:18.400 --> 00:29:21.200]   But you also are very disappointed with Google Home.
[00:29:21.200 --> 00:29:24.920]   I want to circle back around to that because I've been incredibly disappointed with Google
[00:29:24.920 --> 00:29:25.920]   Home lately.
[00:29:25.920 --> 00:29:28.560]   What's just disappointed you folks?
[00:29:28.560 --> 00:29:31.280]   Well, you go first because you brought it up.
[00:29:31.280 --> 00:29:33.720]   Tell me what's disappointing you Stacey.
[00:29:33.720 --> 00:29:38.800]   So mine is currently unplugged because my family hates it.
[00:29:38.800 --> 00:29:41.520]   Oh, well, there's news right there.
[00:29:41.520 --> 00:29:42.520]   And I think I've talked about this.
[00:29:42.520 --> 00:29:45.360]   The voice match is terrible.
[00:29:45.360 --> 00:29:51.480]   And so I turned that off, but then I have no control over who accesses things in my
[00:29:51.480 --> 00:29:52.480]   house.
[00:29:52.480 --> 00:29:53.720]   So that makes me sad.
[00:29:53.720 --> 00:29:56.320]   And then I also can't access again.
[00:29:56.320 --> 00:30:02.120]   I mean, now I'll be able to access my calendars, but I will only be able to access my calendars.
[00:30:02.120 --> 00:30:05.680]   And I just, yeah.
[00:30:05.680 --> 00:30:11.200]   And oh, and a lot of the functionality that is native, I guess, for lack of a better word
[00:30:11.200 --> 00:30:15.680]   to Amazon is still like a skill on the Google Home.
[00:30:15.680 --> 00:30:22.600]   So I have to still ask various apps to do things, like ask Harmony to turn on the television.
[00:30:22.600 --> 00:30:26.240]   And then this random voice comes on as like turning on television.
[00:30:26.240 --> 00:30:28.040]   You're like, who's a what?
[00:30:28.040 --> 00:30:32.240]   Why are you inside my Google Home?
[00:30:32.240 --> 00:30:34.640]   So that's a disappointment.
[00:30:34.640 --> 00:30:36.800]   And so I'm like, Google, fix this stuff, right?
[00:30:36.800 --> 00:30:40.760]   And they're like, here, have John Legend for a temporary amount of time to be your Google
[00:30:40.760 --> 00:30:41.760]   voice.
[00:30:41.760 --> 00:30:44.680]   And I'm like, I do not care about John Legend.
[00:30:44.680 --> 00:30:47.320]   So that is my frustration.
[00:30:47.320 --> 00:30:51.040]   They just, they hoisted John Legend on to you.
[00:30:51.040 --> 00:30:52.320]   I did not have that happen.
[00:30:52.320 --> 00:30:54.200]   For me, they hoisted him.
[00:30:54.200 --> 00:30:55.800]   I don't think you hoisted him on to be.
[00:30:55.800 --> 00:30:56.800]   Oh, sorry.
[00:30:56.800 --> 00:30:59.280]   Thank you for the correct.
[00:30:59.280 --> 00:31:01.600]   Carson is very appreciative of that too.
[00:31:01.600 --> 00:31:04.760]   That is not the only time I will put my foot in my mouth.
[00:31:04.760 --> 00:31:11.640]   My problems have been just that there was a time way back when Wavy lines on the screen
[00:31:11.640 --> 00:31:17.560]   to denote that we're going into a flashback period here where Google Home worked without
[00:31:17.560 --> 00:31:21.760]   much effort to do pretty much everything I asked it to do.
[00:31:21.760 --> 00:31:23.160]   I knew there were limitations.
[00:31:23.160 --> 00:31:24.640]   I worked within those limitations.
[00:31:24.640 --> 00:31:28.240]   If I asked it to play this song, I played that song, no big deal.
[00:31:28.240 --> 00:31:33.880]   Then I trained, I educated my children on how to use it to play songs as well.
[00:31:33.880 --> 00:31:34.880]   And it always worked.
[00:31:34.880 --> 00:31:39.400]   And it was magical because I had like a five year old kid being able to play whatever
[00:31:39.400 --> 00:31:42.760]   song she wanted to given that there were parental control.
[00:31:42.760 --> 00:31:46.640]   So it was nothing inappropriate, but like she could control it and say, hey, I want to
[00:31:46.640 --> 00:31:49.800]   listen to Katy Perry, blah, blah, blah, or whatever.
[00:31:49.800 --> 00:31:50.800]   And it worked.
[00:31:50.800 --> 00:31:55.840]   Now we're in a stage, I feel like there's been some sort of shift in the past, however
[00:31:55.840 --> 00:32:00.640]   many recent months, two, three, four, five months somewhere in there, where I feel like
[00:32:00.640 --> 00:32:02.680]   we are constantly fighting it.
[00:32:02.680 --> 00:32:05.360]   Like we're doing the same thing we ever did.
[00:32:05.360 --> 00:32:10.000]   But now we get these weird error messages that come back like, okay, I'll play that
[00:32:10.000 --> 00:32:11.000]   song.
[00:32:11.000 --> 00:32:13.880]   And then it'll wait for a second and it'll say, sorry, that playlist is empty.
[00:32:13.880 --> 00:32:15.360]   And you're like, I didn't ask for a playlist.
[00:32:15.360 --> 00:32:18.200]   I asked for that song that I know is in the library and you've played it a million times
[00:32:18.200 --> 00:32:20.600]   when I say this thing, but you're not doing it now and you're not really telling me
[00:32:20.600 --> 00:32:21.600]   why you're not doing it.
[00:32:21.600 --> 00:32:23.760]   You're just making up this thing about a playlist that I don't even know what you're
[00:32:23.760 --> 00:32:25.000]   talking about.
[00:32:25.000 --> 00:32:26.600]   And that's just one example.
[00:32:26.600 --> 00:32:30.960]   And so the more that happens, the more now I'm not the only ones swearing at the Google
[00:32:30.960 --> 00:32:31.960]   home when it doesn't work.
[00:32:31.960 --> 00:32:34.840]   Now my kids are because they're like, Google home is stupid.
[00:32:34.840 --> 00:32:36.560]   I'm like, don't call it stupid.
[00:32:36.560 --> 00:32:37.880]   It can hear you.
[00:32:37.880 --> 00:32:43.280]   Let's call it something nicer because maybe that's why it's not being nice, but it's
[00:32:43.280 --> 00:32:47.120]   just, it's really frustrating and it used to be amazing.
[00:32:47.120 --> 00:32:49.960]   And now like we're just fighting it all the time.
[00:32:49.960 --> 00:32:50.960]   Yeah.
[00:32:50.960 --> 00:32:52.800]   And we've had issues.
[00:32:52.800 --> 00:32:57.320]   They've improved it and then it went away again with the song choices.
[00:32:57.320 --> 00:33:01.440]   So it would constantly, instead of playing the most popular version of a song, it would
[00:33:01.440 --> 00:33:08.600]   play the karaoke version, which, you know, when my daughter asks for a version of a song,
[00:33:08.600 --> 00:33:10.640]   she doesn't want the karaoke version.
[00:33:10.640 --> 00:33:15.880]   That's not, that should never be your default unless you're, unless you're a karaoke bar.
[00:33:15.880 --> 00:33:16.880]   Right.
[00:33:16.880 --> 00:33:20.600]   Yeah, and maybe that has something to do with like parental controls.
[00:33:20.600 --> 00:33:24.880]   Like if you're blocking, you know, if they're, if they're asking for a song that has.
[00:33:24.880 --> 00:33:29.320]   So I don't have parental controls, but I will, I'll, I'll bring this up because this is something
[00:33:29.320 --> 00:33:34.200]   that has been on my mind and I'm very curious for resources on this.
[00:33:34.200 --> 00:33:38.720]   In August, my daughter turns 13, which on the internet makes her an adult.
[00:33:38.720 --> 00:33:39.720]   Yes.
[00:33:39.720 --> 00:33:40.720]   Right.
[00:33:40.720 --> 00:33:48.000]   So all of her, all of our accounts that she has tied into will suddenly become totally
[00:33:48.000 --> 00:33:52.760]   opaque to me and all the parental controls I've set up go instantly away.
[00:33:52.760 --> 00:33:57.520]   And that's on Amazon and that's on Google because 13 is when you're going to be option
[00:33:57.520 --> 00:34:00.760]   to just say, I don't care about the parent.
[00:34:00.760 --> 00:34:01.760]   It's 18 to be.
[00:34:01.760 --> 00:34:03.680]   No force parental controls.
[00:34:03.680 --> 00:34:10.560]   You don't because there's no, I mean, if once you're above 13, so now I'm like,
[00:34:10.560 --> 00:34:14.760]   I mean, I have a good enough relationship with my daughter.
[00:34:14.760 --> 00:34:22.120]   I'm, I'm, you know, I don't, we talk about these things, but, and I don't want to like,
[00:34:22.120 --> 00:34:28.360]   I need some some transition time periods where, you know, like, it's all moving too fast.
[00:34:28.360 --> 00:34:30.360]   Well, it is.
[00:34:30.360 --> 00:34:35.000]   I mean, like basically that day in August, she told it, she becomes a legal adult on
[00:34:35.000 --> 00:34:36.000]   the internet basically.
[00:34:36.000 --> 00:34:37.000]   Yeah.
[00:34:37.000 --> 00:34:42.000]   Right.
[00:34:42.000 --> 00:34:43.960]   So she's, yeah, she's not dating.
[00:34:43.960 --> 00:34:44.960]   I'm not worried about that.
[00:34:44.960 --> 00:34:48.720]   But, but yeah, and there's a lot of stuff on the internet that you, you really shouldn't
[00:34:48.720 --> 00:34:49.960]   have access to.
[00:34:49.960 --> 00:34:50.960]   Yeah.
[00:34:50.960 --> 00:34:55.640]   So yeah, this is my, this is my issue.
[00:34:55.640 --> 00:34:57.640]   You trust her.
[00:34:57.640 --> 00:34:58.640]   I do.
[00:34:58.640 --> 00:35:01.680]   So we have a, you trust her.
[00:35:01.680 --> 00:35:03.960]   And she's going to see some stuff in life anyway.
[00:35:03.960 --> 00:35:06.000]   And has to deal with it.
[00:35:06.000 --> 00:35:12.440]   And the more in a way she sees it now, I could argue maybe, no, no, no, no, no, no, no, no,
[00:35:12.440 --> 00:35:16.880]   I don't mean to see all bad stuff, but when she does come across now and knows to say,
[00:35:16.880 --> 00:35:18.880]   mom, what's that?
[00:35:18.880 --> 00:35:19.880]   Yeah.
[00:35:19.880 --> 00:35:23.960]   Hold on to that relationship while you can.
[00:35:23.960 --> 00:35:24.960]   Right.
[00:35:24.960 --> 00:35:28.120]   You know, that's all I'm saying.
[00:35:28.120 --> 00:35:29.120]   Yes.
[00:35:29.120 --> 00:35:35.120]   But there are things on the internet that developmentally know 13 year olds really appropriate for.
[00:35:35.120 --> 00:35:36.120]   And I'm not.
[00:35:36.120 --> 00:35:37.120]   Absolutely.
[00:35:37.120 --> 00:35:39.840]   And so that's, I'm like, yeah, she can talk to me.
[00:35:39.840 --> 00:35:42.280]   But I mean, there are things on the internet that traumatize me.
[00:35:42.280 --> 00:35:45.960]   And I'm a full blown adult who knows that people suck.
[00:35:45.960 --> 00:35:51.920]   So I don't, anyway, my kids are older, obviously, but I have no parents who said that their
[00:35:51.920 --> 00:35:55.840]   kids kind of said, oh, I know what I don't want to look at.
[00:35:55.840 --> 00:35:56.840]   Mm hmm.
[00:35:56.840 --> 00:35:59.840]   I want to win.
[00:35:59.840 --> 00:36:00.840]   No, I don't want that.
[00:36:00.840 --> 00:36:02.840]   And I think you've risen.
[00:36:02.840 --> 00:36:04.840]   You've risen.
[00:36:04.840 --> 00:36:05.840]   Jesus.
[00:36:05.840 --> 00:36:11.840]   You've raised a great daughter and you trust her and I'm going to bet she's going to be
[00:36:11.840 --> 00:36:12.840]   okay.
[00:36:12.840 --> 00:36:14.840]   Yeah, I'm sure she'll be it.
[00:36:14.840 --> 00:36:16.720]   Yeah, I have confidence in her being okay.
[00:36:16.720 --> 00:36:20.400]   I just want to have a little bit of control over.
[00:36:20.400 --> 00:36:23.960]   I still want to have control over parts of her internet life.
[00:36:23.960 --> 00:36:25.720]   And that is not crazy.
[00:36:25.720 --> 00:36:31.320]   Not all parents are going to agree that 13 is the age that suddenly you give them the
[00:36:31.320 --> 00:36:33.800]   keys to the internet car.
[00:36:33.800 --> 00:36:35.040]   And yeah.
[00:36:35.040 --> 00:36:39.080]   And you know, we all my parenting philosophy on the internet is, you know, I tell her
[00:36:39.080 --> 00:36:41.800]   there are things you can never unsee.
[00:36:41.800 --> 00:36:44.680]   And so choose widely when you click on links.
[00:36:44.680 --> 00:36:45.680]   And that's so far working.
[00:36:45.680 --> 00:36:49.040]   Hey, Jeff, everyone in the chat room is yelling at you because they say you're not on your
[00:36:49.040 --> 00:36:50.040]   headset, Mike.
[00:36:50.040 --> 00:36:51.040]   Yes, I know.
[00:36:51.040 --> 00:36:52.040]   They're not.
[00:36:52.040 --> 00:36:54.040]   They're not really yelling at you.
[00:36:54.040 --> 00:36:55.040]   It's the room.
[00:36:55.040 --> 00:36:58.040]   Yeah, I think it's kind of a boxy room.
[00:36:58.040 --> 00:36:59.040]   Yeah.
[00:36:59.040 --> 00:37:02.960]   If I move this closer, we'll stick a pillow over your head.
[00:37:02.960 --> 00:37:03.960]   Well, there you go.
[00:37:03.960 --> 00:37:06.000]   Now everyone in the chat room can be like, uh huh.
[00:37:06.000 --> 00:37:07.800]   Okay, we've addressed it.
[00:37:07.800 --> 00:37:08.800]   Yes.
[00:37:08.800 --> 00:37:09.800]   All right.
[00:37:09.800 --> 00:37:12.200]   Well, it's okay.
[00:37:12.200 --> 00:37:16.880]   We'd rather have you like this than not have you at all.
[00:37:16.880 --> 00:37:19.320]   So can we talk about the Google Maps money maker?
[00:37:19.320 --> 00:37:23.680]   Because I didn't see it on our rundown, but I saw this story and I was like, I saw
[00:37:23.680 --> 00:37:24.680]   it added.
[00:37:24.680 --> 00:37:26.240]   I didn't see when it was added.
[00:37:26.240 --> 00:37:27.240]   So it's in there.
[00:37:27.240 --> 00:37:30.320]   Let's talk about that after we take this break because we got a thank sponsor and then
[00:37:30.320 --> 00:37:35.720]   we'll come back and we'll talk about maps and making money and other words that start
[00:37:35.720 --> 00:37:36.720]   with them.
[00:37:36.720 --> 00:37:39.040]   This episode of this week in Google is brought to you by capterra.
[00:37:39.040 --> 00:37:45.000]   We've all read some surprising online reviews, uh, whether you're trying to get a sweet deal
[00:37:45.000 --> 00:37:47.800]   on, uh, on something you've been saving for.
[00:37:47.800 --> 00:37:51.520]   If you're trying to find the best happy hour in town, whatever you happen to be looking
[00:37:51.520 --> 00:37:54.960]   for, it's generally a good idea to read the reviews first at this point.
[00:37:54.960 --> 00:38:00.240]   I feel like as internet citizens, we are trained to know that there are reviews of the things
[00:38:00.240 --> 00:38:02.400]   that we might, uh, be interested in looking into.
[00:38:02.400 --> 00:38:04.240]   And that is probably a good idea to look into those.
[00:38:04.240 --> 00:38:08.040]   So why should finding the right software for your business be any different?
[00:38:08.040 --> 00:38:13.320]   I read thousands of real software reviews and find the right software for your business
[00:38:13.320 --> 00:38:16.640]   at capterra.com/twig.
[00:38:16.640 --> 00:38:20.800]   Capterra is the leading free online resource to help you find the best software solution
[00:38:20.800 --> 00:38:21.800]   for your business.
[00:38:21.800 --> 00:38:26.980]   Uh, they have more than 800,000 reviews of products from real software users.
[00:38:26.980 --> 00:38:29.880]   So you can discover everything that you need to make an informed decision.
[00:38:29.880 --> 00:38:36.240]   You can search more than 700 specific categories of software, everything crossing, uh, crossing
[00:38:36.240 --> 00:38:41.240]   all over the place from, uh, project management to email marketing to yoga studio management
[00:38:41.240 --> 00:38:42.240]   software.
[00:38:42.240 --> 00:38:46.520]   I'm sure there's plenty of other very specific categories of apps that you can look at.
[00:38:46.520 --> 00:38:48.760]   And yes, there are 700 is quite a few.
[00:38:48.760 --> 00:38:52.400]   So I could be kind of fun to just see like, Hey, I didn't realize there were so many yoga
[00:38:52.400 --> 00:38:54.920]   studio management software out there.
[00:38:54.920 --> 00:38:58.680]   No matter what kind of software your business needs, capterra makes it easy to discover
[00:38:58.680 --> 00:39:01.120]   the right solution fast.
[00:39:01.120 --> 00:39:05.480]   Join the millions of people who use capterra every month to find the right tools for their
[00:39:05.480 --> 00:39:06.480]   business.
[00:39:06.480 --> 00:39:10.960]   And if you have a business, you know, like spending a little bit of time now to find
[00:39:10.960 --> 00:39:13.480]   the right solutions can save you a lot of time later.
[00:39:13.480 --> 00:39:18.280]   Cause if you, you pick the wrong solution and then you, you take the time to get to know
[00:39:18.280 --> 00:39:22.200]   it, to integrate it and everything and finding out later that it was the poor decision for
[00:39:22.200 --> 00:39:26.800]   whatever reason, uh, at some points that could be too late, but you definitely waste a lot
[00:39:26.800 --> 00:39:28.800]   of time and money in the process.
[00:39:28.800 --> 00:39:31.760]   So check out capterra's free website.
[00:39:31.760 --> 00:39:36.640]   Visit capterra.com/twig and you can find the right tools to make an informed software
[00:39:36.640 --> 00:39:38.400]   decision for your business.
[00:39:38.400 --> 00:39:40.200]   That's capterra.com/twig.
[00:39:40.200 --> 00:39:47.880]   C A P T E R R A dot com/twig capterra software selection.
[00:39:47.880 --> 00:39:48.880]   Simplified.
[00:39:48.880 --> 00:39:50.600]   We thank capterra for their support.
[00:39:50.600 --> 00:39:51.600]   So maps.
[00:39:51.600 --> 00:39:52.600]   Let's see here.
[00:39:52.600 --> 00:39:56.640]   So I didn't get a chance to read this story, but I did see that it was added at the top,
[00:39:56.640 --> 00:40:02.600]   in the top block here, the Bloomberg story that says, gah, Google's big money maker could
[00:40:02.600 --> 00:40:04.240]   be maps on your phone.
[00:40:04.240 --> 00:40:07.920]   I feel like it's been a big money maker for Google for a long time already.
[00:40:07.920 --> 00:40:12.560]   Maps is one of the apps that I use possibly more than almost any other app on my phone.
[00:40:12.560 --> 00:40:14.120]   I use it daily.
[00:40:14.120 --> 00:40:15.520]   So what can you tell us about it?
[00:40:15.520 --> 00:40:18.680]   Yeah, there's two parts to this story that are interesting.
[00:40:18.680 --> 00:40:26.880]   One is as a consumer, we'll start seeing more personalized options for us around me, so
[00:40:26.880 --> 00:40:28.560]   based on our maps and location.
[00:40:28.560 --> 00:40:31.320]   So that was interesting more than what we already see.
[00:40:31.320 --> 00:40:37.800]   So it pointed out that restaurants and places can already buy ads to get the little pin.
[00:40:37.800 --> 00:40:39.320]   I don't know what those are called.
[00:40:39.320 --> 00:40:40.320]   The little pin drop.
[00:40:40.320 --> 00:40:41.320]   Pin drop thing.
[00:40:41.320 --> 00:40:42.320]   Yeah.
[00:40:42.320 --> 00:40:43.320]   I don't know.
[00:40:43.320 --> 00:40:47.520]   And yeah, the thing on the map.
[00:40:47.520 --> 00:40:52.160]   And then so Google's basically pushing forward and monetizing that more.
[00:40:52.160 --> 00:40:59.720]   And so we're going to see things like personalized results, suggestions for us based on where
[00:40:59.720 --> 00:41:01.560]   we've been and what we like.
[00:41:01.560 --> 00:41:03.520]   So here's a vegan restaurant near you.
[00:41:03.520 --> 00:41:07.880]   So we're going to, I'm like, I don't know if I want that, but who knew if I wanted Google
[00:41:07.880 --> 00:41:08.880]   search and all the tools.
[00:41:08.880 --> 00:41:09.880]   I use that all the time.
[00:41:09.880 --> 00:41:10.880]   Just today.
[00:41:10.880 --> 00:41:11.880]   I've been Rochester.
[00:41:11.880 --> 00:41:12.880]   Oh, yes.
[00:41:12.880 --> 00:41:14.520]   No, I use it constantly.
[00:41:14.520 --> 00:41:18.360]   And yeah, advertising could actually be somewhat beneficial there, where you see who's going
[00:41:18.360 --> 00:41:22.560]   to spend the money to get me, who knows what I like, it's targeted.
[00:41:22.560 --> 00:41:23.560]   Could that be beneficial?
[00:41:23.560 --> 00:41:24.560]   I don't know.
[00:41:24.560 --> 00:41:25.680]   So they're going to do more there.
[00:41:25.680 --> 00:41:29.000]   And then the second part of this is for developers who use the map.
[00:41:29.000 --> 00:41:31.920]   So Google has made their maps available to developers.
[00:41:31.920 --> 00:41:34.400]   They've changed the pricing on that.
[00:41:34.400 --> 00:41:37.960]   So it's much, much, much more expensive.
[00:41:37.960 --> 00:41:45.280]   So there is a person in here who's got an app that they use Google Maps to show people
[00:41:45.280 --> 00:41:49.040]   where they can find drugs at pharmacies near them.
[00:41:49.040 --> 00:41:53.640]   And basically before Google changed their pricing plan, this guy's, this guy startup
[00:41:53.640 --> 00:41:56.320]   got 750,000 free map views a month.
[00:41:56.320 --> 00:42:00.040]   It was charged 50 cents for every 1,000 views on top of that.
[00:42:00.040 --> 00:42:03.440]   But now Google is starting to charge after 30,000 views.
[00:42:03.440 --> 00:42:07.720]   And the cost is $7 now per 1,000 views.
[00:42:07.720 --> 00:42:10.080]   So his costs were basically nothing.
[00:42:10.080 --> 00:42:12.720]   And now they're $5,000 a month.
[00:42:12.720 --> 00:42:16.480]   So Google is offering people a credit.
[00:42:16.480 --> 00:42:19.600]   So small developers get a credit for use of their maps.
[00:42:19.600 --> 00:42:21.040]   So a $200 credit.
[00:42:21.040 --> 00:42:27.120]   But this was kind of an interesting side note, which means are we going to see fewer apps
[00:42:27.120 --> 00:42:29.200]   that can take advantage of location?
[00:42:29.200 --> 00:42:30.200]   Yeah.
[00:42:30.200 --> 00:42:31.480]   Well, they switch to other maps.
[00:42:31.480 --> 00:42:32.720]   I don't know.
[00:42:32.720 --> 00:42:37.520]   That's the question that kind of comes to mind for me because maps integration, like
[00:42:37.520 --> 00:42:42.520]   I think I take it for granted because it's so easy for developers to tap into it.
[00:42:42.520 --> 00:42:46.080]   Developers know that if they bring in like a Google map, they're adding instant value
[00:42:46.080 --> 00:42:50.080]   to their app and people will recognize it and respect the quality of it.
[00:42:50.080 --> 00:42:56.720]   But that could potentially rule out kind of the, you know, the kind of lower install
[00:42:56.720 --> 00:43:02.600]   base third party developers or at least discourage them and maybe point them in a different
[00:43:02.600 --> 00:43:04.800]   direction for a map solution.
[00:43:04.800 --> 00:43:07.800]   But I don't know.
[00:43:07.800 --> 00:43:10.840]   I guess we'd have to see how that pans out.
[00:43:10.840 --> 00:43:15.760]   It is kind of interesting that we haven't seen as much monetization around maps now that
[00:43:15.760 --> 00:43:16.760]   I think about it.
[00:43:16.760 --> 00:43:22.160]   I guess I've just always kind of, you know, it's something that I use in a timely valuable
[00:43:22.160 --> 00:43:24.160]   information, right, our location.
[00:43:24.160 --> 00:43:26.560]   Well, put it in a way.
[00:43:26.560 --> 00:43:31.200]   Ways has tried a lot to monetize over the years, you know, Dunkin Donuts ads.
[00:43:31.200 --> 00:43:34.760]   Do you want to change where you're going right now and go to the nearest Dunkin Donuts?
[00:43:34.760 --> 00:43:35.760]   No thanks.
[00:43:35.760 --> 00:43:36.760]   Yeah.
[00:43:36.760 --> 00:43:41.200]   So it never made a lot of sense in ways.
[00:43:41.200 --> 00:43:44.480]   Whereas maps use differently ways you only use in your driving somewhere.
[00:43:44.480 --> 00:43:45.480]   That's it.
[00:43:45.480 --> 00:43:46.480]   Right.
[00:43:46.480 --> 00:43:48.840]   And maps I do when I get to a new city, I say what's around me?
[00:43:48.840 --> 00:43:51.400]   Where's the nearest used bookstore?
[00:43:51.400 --> 00:43:57.880]   I think there's a lot of geographic uses that haven't been plumbed yet.
[00:43:57.880 --> 00:43:58.880]   Mm hmm.
[00:43:58.880 --> 00:43:59.880]   Agree.
[00:43:59.880 --> 00:44:03.000]   I mean, yeah, I was in Helsinki last week or Stockholm last week.
[00:44:03.000 --> 00:44:04.000]   I don't remember.
[00:44:04.000 --> 00:44:05.920]   And is it least owned by Google?
[00:44:05.920 --> 00:44:06.920]   Yep.
[00:44:06.920 --> 00:44:07.920]   Okay.
[00:44:07.920 --> 00:44:08.920]   Yes.
[00:44:08.920 --> 00:44:09.920]   Yes.
[00:44:09.920 --> 00:44:12.120]   And the first thing I did was, yeah, restaurant near me.
[00:44:12.120 --> 00:44:14.840]   And then the second thing I did was, dear God, it's so cold.
[00:44:14.840 --> 00:44:17.360]   How can I find more gloves?
[00:44:17.360 --> 00:44:18.360]   Okay.
[00:44:18.360 --> 00:44:21.600]   Did you wear a restaurant I recommended to you, Stacy?
[00:44:21.600 --> 00:44:24.440]   Did you go to my restaurant I recommended in Stockholm?
[00:44:24.440 --> 00:44:27.920]   Um, it was like a Monday or something and it was closed.
[00:44:27.920 --> 00:44:28.920]   Oh, no.
[00:44:28.920 --> 00:44:29.920]   Was that a time?
[00:44:29.920 --> 00:44:30.920]   I'm sorry.
[00:44:30.920 --> 00:44:31.920]   The time I had it was closed.
[00:44:31.920 --> 00:44:34.600]   I was sad.
[00:44:34.600 --> 00:44:39.680]   Was that okay Jarvis command that pulled up the restaurant recommendation?
[00:44:39.680 --> 00:44:41.560]   That's what we need.
[00:44:41.560 --> 00:44:43.160]   That's what we need.
[00:44:43.160 --> 00:44:46.240]   Okay, Jarvis, I'm traveling to Europe, the following cities.
[00:44:46.240 --> 00:44:48.040]   Tell me where I should eat.
[00:44:48.040 --> 00:44:50.800]   Okay, Jarvis, how do you pronounce like she's checked?
[00:44:50.800 --> 00:44:55.480]   Go to a shop and then Copenhagen, you go to Fiski Bine.
[00:44:55.480 --> 00:44:56.480]   Okay.
[00:44:56.480 --> 00:45:00.320]   Okay, Jarvis, we all needed okay Jarvis in our life.
[00:45:00.320 --> 00:45:01.320]   Yeah.
[00:45:01.320 --> 00:45:05.680]   So speaking of money making off of services, it seems like Google's on a little bit of
[00:45:05.680 --> 00:45:06.680]   a tear.
[00:45:06.680 --> 00:45:11.840]   I mean, obviously Google is an ad company at its core.
[00:45:11.840 --> 00:45:15.280]   That's why it's been as successful as it is.
[00:45:15.280 --> 00:45:19.600]   So bringing ads or some sort of ad integration into maps makes sense.
[00:45:19.600 --> 00:45:23.800]   Now if you now apparently on Android TV, if you have an Android TV device, and this isn't
[00:45:23.800 --> 00:45:28.080]   just like a standalone device, like a shield TV, if you happen to have like a Sony TV with
[00:45:28.080 --> 00:45:35.000]   Android TV built in as it's like core OS, you might start seeing sponsored rows in the
[00:45:35.000 --> 00:45:40.600]   interface, sponsored ads embedded into Android TV.
[00:45:40.600 --> 00:45:45.520]   Also saw news that with the assistant updates that we're seeing, now it's going to start
[00:45:45.520 --> 00:45:50.400]   pulling back results that have sponsored ad results, which that kind of makes a little
[00:45:50.400 --> 00:45:54.800]   bit more sense to me because like when I do a great search, I get these sponsored links
[00:45:54.800 --> 00:46:00.880]   and I don't really think badly about that because a lot of times they are still relevant.
[00:46:00.880 --> 00:46:06.760]   So that makes a little bit of sense with assistant, but clogging up your Android TV space, that
[00:46:06.760 --> 00:46:09.960]   might make a lot of people upset.
[00:46:09.960 --> 00:46:15.360]   And Google Assistant is as long as it's not delivered via voice, I hate ads delivered via
[00:46:15.360 --> 00:46:16.360]   voice.
[00:46:16.360 --> 00:46:18.000]   That's going to be a real tough.
[00:46:18.000 --> 00:46:19.320]   That's going to be is it a Rubicon?
[00:46:19.320 --> 00:46:22.160]   We'll call it a Rubicon or burning the ships kind of moment.
[00:46:22.160 --> 00:46:23.160]   Yeah.
[00:46:23.160 --> 00:46:24.160]   I do.
[00:46:24.160 --> 00:46:27.480]   I don't believe that in the case of assistant, that's what's happening.
[00:46:27.480 --> 00:46:32.800]   I think what it is, is you do a query and it doesn't necessarily know how to answer it
[00:46:32.800 --> 00:46:33.800]   by voice.
[00:46:33.800 --> 00:46:36.480]   So it says here, I found some results for you.
[00:46:36.480 --> 00:46:40.320]   If you've ever done those queries and then it gives you like a search result or whatever,
[00:46:40.320 --> 00:46:44.840]   now it's going to start including sponsored items in that list as well.
[00:46:44.840 --> 00:46:51.600]   So you might see an ad for Expedia, just one example.
[00:46:51.600 --> 00:46:54.120]   I don't know how much that bugs me.
[00:46:54.120 --> 00:46:55.680]   I don't know how much any of this bugs me.
[00:46:55.680 --> 00:47:00.800]   I guess even with the sponsored row on Android TV, I'd probably get used to just not seeing
[00:47:00.800 --> 00:47:02.680]   it anymore, even though it's there.
[00:47:02.680 --> 00:47:09.440]   You know, I had a Kindle where, you know, I paid extra for the Kindle without ads and
[00:47:09.440 --> 00:47:11.040]   then I got a Kindle with ads.
[00:47:11.040 --> 00:47:16.360]   And yeah, I just, I'm actually, I find them amusing because I'm like, Oh, wow, look at
[00:47:16.360 --> 00:47:21.040]   this scurrilous, exciting romance novel that you think I want.
[00:47:21.040 --> 00:47:22.040]   Are they targeted ads?
[00:47:22.040 --> 00:47:25.960]   No, apparently not because my husband gets the same ones.
[00:47:25.960 --> 00:47:28.200]   Well, he may have a secret romance thing.
[00:47:28.200 --> 00:47:29.480]   I don't know.
[00:47:29.480 --> 00:47:31.640]   Who's who's dog is that?
[00:47:31.640 --> 00:47:32.640]   I got to know.
[00:47:32.640 --> 00:47:33.640]   It's my dog.
[00:47:33.640 --> 00:47:34.640]   I'm sorry.
[00:47:34.640 --> 00:47:36.480]   My family is home and my dog is excited.
[00:47:36.480 --> 00:47:37.920]   It's okay.
[00:47:37.920 --> 00:47:38.920]   Your dog is forgiven.
[00:47:38.920 --> 00:47:41.000]   Mom has been ignoring me.
[00:47:41.000 --> 00:47:43.120]   Finally, you've come.
[00:47:43.120 --> 00:47:44.120]   Yeah.
[00:47:44.120 --> 00:47:47.680]   Mom went in this room and shut the door on me so I couldn't be there.
[00:47:47.680 --> 00:47:50.120]   So she talks to herself.
[00:47:50.120 --> 00:47:51.120]   This is all I hear.
[00:47:51.120 --> 00:47:52.120]   I don't hear it.
[00:47:52.120 --> 00:47:53.120]   It's really strange.
[00:47:53.120 --> 00:47:54.120]   I'm worried about mom.
[00:47:54.120 --> 00:47:55.120]   Right.
[00:47:55.120 --> 00:47:56.320]   She's not talking to me.
[00:47:56.320 --> 00:48:01.120]   Are you guys excited about the lower cost pixels?
[00:48:01.120 --> 00:48:05.720]   Apparently the pixel three a is going to happen sometime soon.
[00:48:05.720 --> 00:48:08.440]   There's like all these things that keep happening here for you, Dr. Android.
[00:48:08.440 --> 00:48:10.760]   I still have my pixel two and I'm quite happy with it.
[00:48:10.760 --> 00:48:11.760]   Okay.
[00:48:11.760 --> 00:48:16.480]   Well, I mean, this is still definitely in the kind of roomery category, but things keep
[00:48:16.480 --> 00:48:17.480]   happening.
[00:48:17.480 --> 00:48:18.480]   It's been there a long time.
[00:48:18.480 --> 00:48:23.080]   It has and supposedly it's going to happen sometime, you know, possibly before IO, but
[00:48:23.080 --> 00:48:29.240]   Google very recently last week updated its Google Play Developer Console and in doing
[00:48:29.240 --> 00:48:33.800]   so listed two new devices, codename Bonito and Sargo, which have been the codenames that
[00:48:33.800 --> 00:48:37.920]   we've been hearing for the Pixel 3a and the Pixel 3a XL.
[00:48:37.920 --> 00:48:41.240]   These are mid-range devices as far as we know.
[00:48:41.240 --> 00:48:46.680]   And then they also ended up adding a navigation option on the Play Store to Pixel 3a and
[00:48:46.680 --> 00:48:48.960]   the 3a XL and the Nest Hub Max.
[00:48:48.960 --> 00:48:53.080]   All three of those devices have not been officially announced yet, only to then remove
[00:48:53.080 --> 00:48:54.840]   them, of course, once news actually broke.
[00:48:54.840 --> 00:48:58.480]   So these were accidentally leaked once again by Google.
[00:48:58.480 --> 00:49:01.880]   So we know that they are something they are something that exists.
[00:49:01.880 --> 00:49:07.160]   And then Hiroshi Lockheimer from the Android team has also hinted on Twitter that he has
[00:49:07.160 --> 00:49:11.200]   an unreleased phone that he's taken pictures of, but that he can't share those pictures
[00:49:11.200 --> 00:49:12.680]   because he doesn't want to out the phone.
[00:49:12.680 --> 00:49:17.280]   So definitely going to happen, it's mid-range Pixel device.
[00:49:17.280 --> 00:49:23.040]   So I think it lacks the glass back in lieu of probably more of like a plastic casing if
[00:49:23.040 --> 00:49:25.200]   I'm not mistaken.
[00:49:25.200 --> 00:49:28.240]   Less RAM, four gigs of RAM, a 1080p display.
[00:49:28.240 --> 00:49:29.720]   Four gigs of RAM?
[00:49:29.720 --> 00:49:32.760]   That is nothing.
[00:49:32.760 --> 00:49:34.400]   Remember when that was something.
[00:49:34.400 --> 00:49:41.000]   Now it's competing with like 12 gigs, which is probably overboard, but still.
[00:49:41.000 --> 00:49:47.040]   I'm dragging 670, so it's a lower classification processor.
[00:49:47.040 --> 00:49:48.040]   Yeah.
[00:49:48.040 --> 00:49:50.560]   I don't know, mid-range pixels basically.
[00:49:50.560 --> 00:49:55.800]   And I think I don't know if the strategy with the mid-range Pixel like what that's going
[00:49:55.800 --> 00:49:56.800]   to be.
[00:49:56.800 --> 00:50:00.760]   Is it just to bring more people into the Pixel family or is it to target kind of people?
[00:50:00.760 --> 00:50:02.400]   Well, I saw a story just now.
[00:50:02.400 --> 00:50:04.160]   I don't think it's on the rundown.
[00:50:04.160 --> 00:50:07.600]   The people, Pixel buyers are mainly switching from Samsung.
[00:50:07.600 --> 00:50:09.880]   Yes, that is in the rundown actually.
[00:50:09.880 --> 00:50:12.840]   And that's a good thing to throw in there.
[00:50:12.840 --> 00:50:15.680]   That makes me explain what the strategy is.
[00:50:15.680 --> 00:50:16.680]   Yeah.
[00:50:16.680 --> 00:50:18.280]   Where did I put that?
[00:50:18.280 --> 00:50:21.680]   Yeah, the majority of, gosh, I'm trying to find that.
[00:50:21.680 --> 00:50:24.560]   I know it's in here somewhere.
[00:50:24.560 --> 00:50:30.080]   Basically from what I can remember, the report essentially said that those users coming over
[00:50:30.080 --> 00:50:35.520]   to the Pixel and to the OnePlus 6T aren't necessarily coming from Apple.
[00:50:35.520 --> 00:50:39.640]   Apple side, they aren't coming from iPhones, but they're coming from Samsung devices by
[00:50:39.640 --> 00:50:40.640]   and large.
[00:50:40.640 --> 00:50:46.440]   And of course, you've got pixels apparently for the last quarter of last year.
[00:50:46.440 --> 00:50:52.040]   The Pixel 3 was performed really strongly in Verizon stores.
[00:50:52.040 --> 00:50:56.800]   The OnePlus 6T is the first time that a OnePlus phone has been in a carrier store.
[00:50:56.800 --> 00:50:59.880]   It was in T-Mobile and it did really well in Q4.
[00:50:59.880 --> 00:51:03.560]   And that's all great, but it's just not doing what I think, at least in the terms of the
[00:51:03.560 --> 00:51:09.120]   Pixel, what Google wanted, which was to pull people over from the iPhone back into Android
[00:51:09.120 --> 00:51:10.120]   Lamb.
[00:51:10.120 --> 00:51:11.120]   Here's the numbers.
[00:51:11.120 --> 00:51:12.560]   So this is a verge story.
[00:51:12.560 --> 00:51:14.360]   Counterpoint research.
[00:51:14.360 --> 00:51:20.240]   51% of Pixel 3 buyers were formerly Samsung Galaxy owners, only 18% migrated from iPhones
[00:51:20.240 --> 00:51:24.240]   and 14% from Motorola devices.
[00:51:24.240 --> 00:51:26.580]   There you go.
[00:51:26.580 --> 00:51:29.140]   So who still has Motorola devices?
[00:51:29.140 --> 00:51:34.640]   Well, actually the Moto G series is a pretty popular series.
[00:51:34.640 --> 00:51:36.880]   And that's a, that's like a mid-ranger.
[00:51:36.880 --> 00:51:37.880]   It's a mid-ranger.
[00:51:37.880 --> 00:51:43.520]   But by and large, it's seen as a really good quality low to mid-range device for the, for
[00:51:43.520 --> 00:51:44.680]   the cost.
[00:51:44.680 --> 00:51:49.760]   Because I think they cost somewhere around like 300, 400-ish dollars and they're still,
[00:51:49.760 --> 00:51:52.280]   you know, they, they still do a really good job in that price range.
[00:51:52.280 --> 00:51:54.600]   So that might be part of it.
[00:51:54.600 --> 00:51:55.600]   But you're right.
[00:51:55.600 --> 00:51:58.320]   I think the computer is much more of a role as you used to.
[00:51:58.320 --> 00:52:03.860]   The more I think about the more I think Google getting more range in the pixels is good
[00:52:03.860 --> 00:52:05.320]   news for us Pixel fans.
[00:52:05.320 --> 00:52:06.320]   Yeah.
[00:52:06.320 --> 00:52:07.320]   I would agree.
[00:52:07.320 --> 00:52:08.320]   Yeah.
[00:52:08.320 --> 00:52:12.760]   Because I want to pay a thousand dollars for my new phone.
[00:52:12.760 --> 00:52:17.080]   Or even if I, if I want my thousand dollar phone, I want Google to keep investing in
[00:52:17.080 --> 00:52:22.760]   it so I don't invest in something that's going to go away as Google has seen it do.
[00:52:22.760 --> 00:52:26.600]   There is, there, there's a lot of value there in knowing that if you're going to spend X
[00:52:26.600 --> 00:52:30.160]   amount of dollars on, on your smartphone, you want to know that it's going to get that
[00:52:30.160 --> 00:52:31.160]   ongoing support.
[00:52:31.160 --> 00:52:34.880]   And to me, that's one of the biggest, one of the bigger, you know, kind of key benefits
[00:52:34.880 --> 00:52:37.800]   of buying the Pixel.
[00:52:37.800 --> 00:52:41.880]   It might have a shortcomings, you know, in different places, but you know, Google is
[00:52:41.880 --> 00:52:49.000]   going to be dedicated to it for possibly longer and definitely quicker on a release,
[00:52:49.000 --> 00:52:51.720]   you know, to release basis.
[00:52:51.720 --> 00:52:56.240]   They're going to put those updates out to this phone before most other phones even get
[00:52:56.240 --> 00:52:57.840]   them.
[00:52:57.840 --> 00:53:02.240]   So there's a lot of value there, but a lot of people don't understand that value either.
[00:53:02.240 --> 00:53:06.120]   Like a lot of mainstream users probably don't really even pay attention to that.
[00:53:06.120 --> 00:53:07.920]   Like, yeah, well, it's a Google phone.
[00:53:07.920 --> 00:53:09.200]   That's why I got it.
[00:53:09.200 --> 00:53:10.960]   They don't know why, you know, you know what I mean?
[00:53:10.960 --> 00:53:16.840]   Like they're not as, as clear down to the idea like the update schedule and why that's
[00:53:16.840 --> 00:53:18.840]   why that really matters.
[00:53:18.840 --> 00:53:21.240]   And you know, to be fair, they shouldn't have to be.
[00:53:21.240 --> 00:53:22.240]   No, they shouldn't.
[00:53:22.240 --> 00:53:23.240]   You're right.
[00:53:23.240 --> 00:53:25.040]   That's, that's why Google made a branded phone.
[00:53:25.040 --> 00:53:28.880]   It's so you could be like, you will get the best and we will control and make sure it works.
[00:53:28.880 --> 00:53:29.880]   Yeah.
[00:53:29.880 --> 00:53:31.480]   And that's, that's what people want.
[00:53:31.480 --> 00:53:35.360]   They don't actually want to think about update schedules.
[00:53:35.360 --> 00:53:36.720]   No, they should.
[00:53:36.720 --> 00:53:37.960]   Yeah, you're absolutely right.
[00:53:37.960 --> 00:53:41.240]   They shouldn't have to have to do that.
[00:53:41.240 --> 00:53:43.840]   Let us nerds do that for them.
[00:53:43.840 --> 00:53:44.840]   Exactly.
[00:53:44.840 --> 00:53:49.280]   Although if they want to watch the show and ask questions, we're happy to take them.
[00:53:49.280 --> 00:53:51.120]   Absolutely.
[00:53:51.120 --> 00:53:52.920]   Also, let's see here.
[00:53:52.920 --> 00:53:53.920]   I saw this right before.
[00:53:53.920 --> 00:53:55.720]   I just thought it was kind of interesting.
[00:53:55.720 --> 00:54:01.320]   Apparently Google has created a a, I don't know how to describe this.
[00:54:01.320 --> 00:54:03.760]   It's an art exhibit in Milano.
[00:54:03.760 --> 00:54:08.040]   It's a temporary art exhibit where they're showing three different rooms with different
[00:54:08.040 --> 00:54:09.040]   aesthetics.
[00:54:09.040 --> 00:54:11.080]   One, a aesthetic is essential.
[00:54:11.080 --> 00:54:13.440]   One is vital and one is transformative.
[00:54:13.440 --> 00:54:20.400]   And these rooms are all like high design rooms with like the pixel palette color palette,
[00:54:20.400 --> 00:54:23.240]   kind of the pastel color palettes.
[00:54:23.240 --> 00:54:29.760]   And they, they outfit anyone going through the space with a wearable that Google designed
[00:54:29.760 --> 00:54:32.720]   that has embedded in the bands.
[00:54:32.720 --> 00:54:36.160]   It's especially made wristband that measures biological response.
[00:54:36.160 --> 00:54:42.440]   So stuff like heart activity, breathing rates, skin temperature, skin conductivity and motion
[00:54:42.440 --> 00:54:48.480]   and tracks how you respond to each of the rooms as you're walking through them.
[00:54:48.480 --> 00:54:53.240]   So afterwards, when you're done, you can take a look at the data and see how relaxed
[00:54:53.240 --> 00:54:56.400]   you were in maybe it's a real job.
[00:54:56.400 --> 00:54:58.960]   Yeah, I couldn't quite understand.
[00:54:58.960 --> 00:55:01.320]   I was like, I was trying to find some clues here.
[00:55:01.320 --> 00:55:02.320]   This is perfect.
[00:55:02.320 --> 00:55:07.360]   So Google takes this data when I am walking through, I don't know which different room
[00:55:07.360 --> 00:55:16.560]   is called, but when I'm walking with through the vital room and I am super relaxed and happy,
[00:55:16.560 --> 00:55:23.400]   then that data can be used like, Hey, pottery barn, Stacy is this type of consumer for you.
[00:55:23.400 --> 00:55:28.800]   Oh, yeah, that's the future and it's creepy as I'll get out.
[00:55:28.800 --> 00:55:29.800]   Who knows?
[00:55:29.800 --> 00:55:33.600]   Maybe they're going to take this technology and put it into their pixel watch whenever
[00:55:33.600 --> 00:55:36.120]   that happens, if it ever happens.
[00:55:36.120 --> 00:55:42.760]   Well, like Pfizer and other companies are doing research with lighting and sound along
[00:55:42.760 --> 00:55:48.320]   with wearables to measure stress response in people and they're using it for smoking
[00:55:48.320 --> 00:55:50.200]   cessation programs.
[00:55:50.200 --> 00:55:58.800]   So there is a legitimate need to not need rationale behind understanding how I might
[00:55:58.800 --> 00:56:02.320]   feel about something that is not just because you want to sell me stuff.
[00:56:02.320 --> 00:56:03.320]   Sure, sure.
[00:56:03.320 --> 00:56:05.440]   But I'm minutes to sell me stuff.
[00:56:05.440 --> 00:56:07.600]   At the end of the day, probably so.
[00:56:07.600 --> 00:56:10.440]   And it's a temporary art exhibit.
[00:56:10.440 --> 00:56:13.880]   Most of us, if not all of us, will never see this.
[00:56:13.880 --> 00:56:18.280]   But let's see here.
[00:56:18.280 --> 00:56:22.520]   I'm trying to like get through the Googly stuff before we go out of that.
[00:56:22.520 --> 00:56:25.080]   A YouTube TV price hike coming to YouTube TV.
[00:56:25.080 --> 00:56:28.360]   You remember when it was $35 and it was such a deal.
[00:56:28.360 --> 00:56:34.480]   Now it's going to jump up to $50, $50 a month for Google TV or sorry for YouTube TV.
[00:56:34.480 --> 00:56:38.480]   And that's because they actually just secured a deal with discovery.
[00:56:38.480 --> 00:56:44.480]   So they're going to add the discovery channel, HD TV, Food Network, TLC, ID animal planet,
[00:56:44.480 --> 00:56:46.680]   travel channel and motor trend.
[00:56:46.680 --> 00:56:48.880]   They probably paid a lot for motor trend.
[00:56:48.880 --> 00:56:51.400]   So basically they're recreating the cable bundle.
[00:56:51.400 --> 00:56:52.400]   Exactly.
[00:56:52.400 --> 00:56:54.120]   They've allowed new people.
[00:56:54.120 --> 00:56:55.120]   Yes.
[00:56:55.120 --> 00:56:56.120]   Yeah.
[00:56:56.120 --> 00:57:00.960]   At one point, the over the top idea seemed like a way to save money because you're cutting
[00:57:00.960 --> 00:57:05.840]   the cord, you're getting out of the cable thing and you're just going on the internet
[00:57:05.840 --> 00:57:08.280]   and picking out and choosing whatever.
[00:57:08.280 --> 00:57:13.720]   And yeah, we're definitely drifting away from that now because all of these services,
[00:57:13.720 --> 00:57:19.760]   all of the OTT services are raising their rates, adding more, obviously because users
[00:57:19.760 --> 00:57:21.120]   probably want more.
[00:57:21.120 --> 00:57:23.760]   If you're a subscriber, you probably want access to more channels.
[00:57:23.760 --> 00:57:26.360]   Of course, we got paid for those.
[00:57:26.360 --> 00:57:27.720]   So now you're paying.
[00:57:27.720 --> 00:57:31.880]   I don't know if you're paying exactly what you were paying before, but it's not much
[00:57:31.880 --> 00:57:34.440]   of a savings if there is any savings there anymore.
[00:57:34.440 --> 00:57:36.400]   I don't know.
[00:57:36.400 --> 00:57:40.800]   Do either of you guys have satellite or do you do these services?
[00:57:40.800 --> 00:57:41.800]   No.
[00:57:41.800 --> 00:57:42.800]   We do.
[00:57:42.800 --> 00:57:43.800]   We do.
[00:57:43.800 --> 00:57:49.320]   What are you going to have in Seattle?
[00:57:49.320 --> 00:57:52.280]   We cut the cord along like 2008.
[00:57:52.280 --> 00:57:56.120]   So we have, right now we have a Netflix subscription.
[00:57:56.120 --> 00:57:59.400]   I guess we're Amazon Prime, so we have that.
[00:57:59.400 --> 00:58:05.400]   At this moment in time, we have HBO, but we go, we just subscribe when we want and then
[00:58:05.400 --> 00:58:06.400]   stop.
[00:58:06.400 --> 00:58:07.400]   Yeah.
[00:58:07.400 --> 00:58:12.880]   Same with sling TV for sports for like the playoffs.
[00:58:12.880 --> 00:58:19.880]   If we added all up, especially because my husband gets the hockey in the baseball packages,
[00:58:19.880 --> 00:58:22.760]   it's probably like cable.
[00:58:22.760 --> 00:58:25.680]   But it's not ongoing the entire year.
[00:58:25.680 --> 00:58:26.840]   You jump in when you need it.
[00:58:26.840 --> 00:58:28.320]   You jump out when you don't, right?
[00:58:28.320 --> 00:58:29.320]   Right.
[00:58:29.320 --> 00:58:30.320]   What's your river it's about?
[00:58:30.320 --> 00:58:31.320]   Yeah.
[00:58:31.320 --> 00:58:32.320]   Yeah.
[00:58:32.320 --> 00:58:34.560]   I mean, that's, that's usually how I, how we use these services.
[00:58:34.560 --> 00:58:41.600]   We put our left foot in, we put our left foot out, we get the HBO and then we take it out.
[00:58:41.600 --> 00:58:42.600]   Okay.
[00:58:42.600 --> 00:58:43.600]   I'm sorry.
[00:58:43.600 --> 00:58:44.600]   I'm sorry.
[00:58:44.600 --> 00:58:45.600]   So much shopping.
[00:58:45.600 --> 00:58:46.600]   It's okay.
[00:58:46.600 --> 00:58:50.360]   You've given some, some crafty individual, the ammo they need to do it nice this week
[00:58:50.360 --> 00:58:53.520]   in Google remix and I put the challenge out there.
[00:58:53.520 --> 00:58:55.520]   Somebody's got to take us up on it.
[00:58:55.520 --> 00:58:56.520]   Yeah.
[00:58:56.520 --> 00:58:59.640]   That's the only time that we end up doing these services with something like the Olympics.
[00:58:59.640 --> 00:59:01.280]   The Olympics is a prime example.
[00:59:01.280 --> 00:59:02.720]   The Olympics are happening.
[00:59:02.720 --> 00:59:06.720]   We'll jump in, we'll do a month and inevitably we'll forget to cancel it.
[00:59:06.720 --> 00:59:09.960]   And so then we'll do another month and then we'll actually cancel it.
[00:59:09.960 --> 00:59:12.520]   That's usually how it goes.
[00:59:12.520 --> 00:59:16.880]   Did either of you watch Netflix's Black Mirror, Bandersnatch?
[00:59:16.880 --> 00:59:17.880]   Nope.
[00:59:17.880 --> 00:59:18.880]   Or did you hear about this?
[00:59:18.880 --> 00:59:19.880]   Nope.
[00:59:19.880 --> 00:59:20.880]   I hate choose your own adventure.
[00:59:20.880 --> 00:59:26.160]   See now I, yeah, I have not watched this and I'm kind of leery to because I kind of want,
[00:59:26.160 --> 00:59:29.960]   just want to sit down and watch, have them choose the story for me.
[00:59:29.960 --> 00:59:32.960]   I don't want to have to make decisions.
[00:59:32.960 --> 00:59:34.800]   But I'm a huge Black Mirror fan.
[00:59:34.800 --> 00:59:38.360]   So I'm kind of surprised that I haven't just broken down and watched Bandersnatch because
[00:59:38.360 --> 00:59:42.560]   apparently it's apparently it's really good from what people seem to say.
[00:59:42.560 --> 00:59:47.320]   But it is a choose your own adventure or interactive storytelling because Netflix is
[00:59:47.320 --> 00:59:52.200]   being sued by choose your own adventure right now.
[00:59:52.200 --> 00:59:54.720]   So we should maybe rename it something else.
[00:59:54.720 --> 00:59:57.520]   But apparently YouTube is getting in on this as well.
[00:59:57.520 --> 01:00:02.360]   They're going to start creating some of their own choose your own adventure programs.
[01:00:02.360 --> 01:00:08.920]   They're restructuring their programming staff to make room for this new approach at the
[01:00:08.920 --> 01:00:12.880]   same time that they're cutting some of the cutting costs on some of their content.
[01:00:12.880 --> 01:00:16.080]   So they're focusing more on live events and reality shows.
[01:00:16.080 --> 01:00:18.960]   This seems really expensive though to do choose your own adventure.
[01:00:18.960 --> 01:00:21.360]   So I don't know.
[01:00:21.360 --> 01:00:25.240]   I guess I'm not surprised that YouTube would jump into it if it's a trend and they want
[01:00:25.240 --> 01:00:27.240]   to be at the forefront of the trend.
[01:00:27.240 --> 01:00:30.320]   It feels so weird because TV is such a lean back experience.
[01:00:30.320 --> 01:00:37.840]   So I'm like, I mean, in granted, we're, as I have already illustrated, I am old and want
[01:00:37.840 --> 01:00:39.080]   people to get off my lawn.
[01:00:39.080 --> 01:00:44.560]   So maybe this just isn't appealing to me, but it would be appealing to others.
[01:00:44.560 --> 01:00:45.560]   Yeah.
[01:00:45.560 --> 01:00:46.560]   No people.
[01:00:46.560 --> 01:00:48.880]   Netflix does have another choose your own adventure.
[01:00:48.880 --> 01:00:52.320]   They might have a few others, but one of them that they do have is Minecraft.
[01:00:52.320 --> 01:00:59.400]   It's a Minecraft story that, you know, obviously for kids and you basically navigate your way
[01:00:59.400 --> 01:01:01.240]   through the story with the remote.
[01:01:01.240 --> 01:01:05.120]   And I have watched my kids play that and or watch that play that.
[01:01:05.120 --> 01:01:08.600]   I don't know what you call it when you're watching and playing at the same time.
[01:01:08.600 --> 01:01:10.160]   And they really enjoyed it.
[01:01:10.160 --> 01:01:11.160]   They totally got into it.
[01:01:11.160 --> 01:01:12.160]   They're not even mine.
[01:01:12.160 --> 01:01:16.960]   They're not even Minecraft fans and they enjoyed the process.
[01:01:16.960 --> 01:01:19.280]   Nine and almost six.
[01:01:19.280 --> 01:01:20.280]   Okay.
[01:01:20.280 --> 01:01:23.680]   They're much more into Roblox.
[01:01:23.680 --> 01:01:25.840]   Their jam is Roblox, not Minecraft.
[01:01:25.840 --> 01:01:26.840]   It's similar.
[01:01:26.840 --> 01:01:27.840]   Oh, okay.
[01:01:27.840 --> 01:01:30.360]   I was like, what are Roblox like?
[01:01:30.360 --> 01:01:35.440]   Yeah, Roblox, which is, yeah, it's the same, same, but different kind of.
[01:01:35.440 --> 01:01:37.360]   Got it.
[01:01:37.360 --> 01:01:41.880]   So yeah, maybe maybe we're just outside of the demographic for choose your own adventure
[01:01:41.880 --> 01:01:42.880]   movies.
[01:01:42.880 --> 01:01:44.320]   Or we're just lazy.
[01:01:44.320 --> 01:01:45.800]   It's possible we're lazy.
[01:01:45.800 --> 01:01:46.800]   Yeah.
[01:01:46.800 --> 01:01:47.800]   Yeah.
[01:01:47.800 --> 01:01:48.800]   I mean, I don't watch much TV anyways.
[01:01:48.800 --> 01:01:55.000]   I hardly watch TV nowadays so that when I do, I really just want to put something on
[01:01:55.000 --> 01:01:57.160]   and not be part of the process.
[01:01:57.160 --> 01:02:00.080]   Well, we still, you still want to tour.
[01:02:00.080 --> 01:02:03.880]   You still want somebody to do the work and have vision and you're trying to, part of
[01:02:03.880 --> 01:02:06.680]   what you're doing is you're just kind of sussing out their brain of where they're going
[01:02:06.680 --> 01:02:09.840]   to go and where's this going to end?
[01:02:09.840 --> 01:02:12.160]   And all I'm smart, I figured it out.
[01:02:12.160 --> 01:02:14.480]   And then it takes away half of that fun.
[01:02:14.480 --> 01:02:15.480]   Yeah.
[01:02:15.480 --> 01:02:18.480]   Like ultimately, what I want is the best story.
[01:02:18.480 --> 01:02:20.760]   So great.
[01:02:20.760 --> 01:02:25.160]   Have a mode where I can walk through it if I want to.
[01:02:25.160 --> 01:02:31.400]   Also have a mode where you give me the best story, the best kind of curated walk through
[01:02:31.400 --> 01:02:32.400]   or something.
[01:02:32.400 --> 01:02:33.400]   I don't know how you do that.
[01:02:33.400 --> 01:02:37.960]   But maybe the best story for you is some other story is the idea here.
[01:02:37.960 --> 01:02:41.200]   And I don't like them because I want to see all the stories.
[01:02:41.200 --> 01:02:42.200]   Yeah.
[01:02:42.200 --> 01:02:43.640]   And then I get frustrated.
[01:02:43.640 --> 01:02:46.240]   So that's why like choose your own adventure books.
[01:02:46.240 --> 01:02:51.160]   I hate it as a kid because I had to read them all and because I'm a little OCD and it was
[01:02:51.160 --> 01:02:54.120]   just like very distressing to me.
[01:02:54.120 --> 01:02:57.920]   So would you put a mark on the page next to the thing you would?
[01:02:57.920 --> 01:02:59.400]   I had little little.
[01:02:59.400 --> 01:03:01.120]   I've been here before.
[01:03:01.120 --> 01:03:02.120]   Yeah.
[01:03:02.120 --> 01:03:03.120]   Okay.
[01:03:03.120 --> 01:03:06.040]   This one and then I'd mark off everyone that I've chosen.
[01:03:06.040 --> 01:03:07.040]   Yeah.
[01:03:07.040 --> 01:03:11.240]   It was and sometimes I would just read them cover to cover and just like reassemble them
[01:03:11.240 --> 01:03:12.240]   in my brain.
[01:03:12.240 --> 01:03:13.240]   Whoa.
[01:03:13.240 --> 01:03:14.240]   That's interesting.
[01:03:14.240 --> 01:03:18.920]   Well, if you had to be, if you wanted to read them all, you just had to do that.
[01:03:18.920 --> 01:03:22.200]   If you wanted to be a completist, yeah, you've wanted everything.
[01:03:22.200 --> 01:03:24.240]   I'm a completist.
[01:03:24.240 --> 01:03:27.480]   I think the thing that like I actually liked choose your own adventure, but the thing that
[01:03:27.480 --> 01:03:31.480]   I did not like about them and my kids have read a few of them and I encounter this with
[01:03:31.480 --> 01:03:36.600]   them is that when you go through and you don't make it to the end, like you said, Stacy,
[01:03:36.600 --> 01:03:38.240]   you have to start over.
[01:03:38.240 --> 01:03:42.840]   And so then you know, you do that two or three times and you're reading the same beginning
[01:03:42.840 --> 01:03:43.840]   stuff.
[01:03:43.840 --> 01:03:44.840]   Right.
[01:03:44.840 --> 01:03:50.840]   And tell you until your path goes separate ways, you know, versus where you've been before.
[01:03:50.840 --> 01:03:55.280]   So just it feels very repetitive and not, you know, the enjoyment to windows.
[01:03:55.280 --> 01:03:59.480]   That's why you have to read it all the way all the way through and then you read it.
[01:03:59.480 --> 01:04:00.480]   Yes.
[01:04:00.480 --> 01:04:01.960]   I haven't thought of it like that.
[01:04:01.960 --> 01:04:03.600]   That's a good good idea.
[01:04:03.600 --> 01:04:04.800]   I'm highly efficient.
[01:04:04.800 --> 01:04:05.800]   Yes.
[01:04:05.800 --> 01:04:06.800]   No kidding.
[01:04:06.800 --> 01:04:12.720]   Let's take a break and then when we come back, we can talk a little bit about well, we of
[01:04:12.720 --> 01:04:14.160]   course we've got the change log.
[01:04:14.160 --> 01:04:16.160]   We can also talk a little bit about this.
[01:04:16.160 --> 01:04:17.160]   Government stuff.
[01:04:17.160 --> 01:04:18.320]   Yeah, the government stuff.
[01:04:18.320 --> 01:04:21.560]   I feel like we need to know you need to touch on some of that stuff because there's
[01:04:21.560 --> 01:04:25.320]   a lot of stuff coming to a head and I know you wrote something, Jeff, that we can talk
[01:04:25.320 --> 01:04:26.320]   about as well.
[01:04:26.320 --> 01:04:28.680]   So let's take a break and then we'll jump into some of that.
[01:04:28.680 --> 01:04:33.320]   This episode of this week in Google is brought to you by Express VPN.
[01:04:33.320 --> 01:04:34.320]   Admit it.
[01:04:34.320 --> 01:04:37.520]   You think you think that cyber crime happens to everyone else.
[01:04:37.520 --> 01:04:39.680]   It never happens to you, right?
[01:04:39.680 --> 01:04:41.840]   We all think that all these things happen to other people.
[01:04:41.840 --> 01:04:43.200]   It couldn't possibly happen to us.
[01:04:43.200 --> 01:04:44.600]   It never has after all.
[01:04:44.600 --> 01:04:46.560]   So what do we have to worry about?
[01:04:46.560 --> 01:04:51.320]   No one's going to target my passwords or my credit card details.
[01:04:51.320 --> 01:04:53.000]   You're probably wrong though.
[01:04:53.000 --> 01:04:57.800]   Stealing data from unsuspecting people on public Wi-Fi happens to be one of the simplest
[01:04:57.800 --> 01:05:01.160]   and cheapest ways for hackers to make money.
[01:05:01.160 --> 01:05:06.480]   When you leave your internet connection unencrypted, you might as well be writing your passwords
[01:05:06.480 --> 01:05:10.760]   and your credit card numbers on a huge billboard for everyone to see or at least the hackers
[01:05:10.760 --> 01:05:14.080]   that know how to get to it and see that billboard.
[01:05:14.080 --> 01:05:18.920]   That's why I use Express VPN and I recommend that you do too to protect yourself from cyber
[01:05:18.920 --> 01:05:19.920]   criminals.
[01:05:19.920 --> 01:05:24.960]   Express VPN secures and anonymizes your internet browsing by encrypting your data and hiding
[01:05:24.960 --> 01:05:27.120]   your public IP address.
[01:05:27.120 --> 01:05:29.000]   Express VPN has easy to use apps.
[01:05:29.000 --> 01:05:30.440]   They're super simple to use.
[01:05:30.440 --> 01:05:35.360]   They run seamlessly in the background on your computer, your phone, your tablet.
[01:05:35.360 --> 01:05:38.640]   And turning it on is literally a single click.
[01:05:38.640 --> 01:05:41.840]   It's really nicely integrated into Android.
[01:05:41.840 --> 01:05:46.640]   You see a little VPN icon up in your notification pane.
[01:05:46.640 --> 01:05:48.320]   It's all kind of integrated into the OS.
[01:05:48.320 --> 01:05:51.160]   So it's super simple to activate.
[01:05:51.160 --> 01:05:55.560]   Using Express VPN, you can safely surf on public Wi-Fi and know that you are not being
[01:05:55.560 --> 01:06:00.360]   snooped on or having your personal data stolen, it's all encrypted and protected.
[01:06:00.360 --> 01:06:05.680]   And for less than $7 a month, you can get the same Express VPN protection that I use on
[01:06:05.680 --> 01:06:07.680]   my phone and my laptop.
[01:06:07.680 --> 01:06:12.360]   Express VPN is rated the number one VPN service by TechRadar.
[01:06:12.360 --> 01:06:15.280]   Comes with a 30 day money back guarantee.
[01:06:15.280 --> 01:06:20.360]   You can protect your online activity today and find out how you can get three extra months
[01:06:20.360 --> 01:06:25.160]   free with a one year package at ExpressVPN.com/twig.
[01:06:25.160 --> 01:06:31.240]   That's eXPR ESS VPN.com/twig.
[01:06:31.240 --> 01:06:35.160]   You'll get three extra months free when you do that with a one year package.
[01:06:35.160 --> 01:06:38.600]   Visit ExpressVPN.com/twig to learn more.
[01:06:38.600 --> 01:06:40.960]   Okay, so government stuff.
[01:06:40.960 --> 01:06:49.200]   First of all, house voted, voted of course as expected to, well voted for the Save the
[01:06:49.200 --> 01:06:50.640]   Internet Act.
[01:06:50.640 --> 01:06:53.720]   So now it faces the challenge.
[01:06:53.720 --> 01:07:01.280]   This is of course going to face a lot of pushback from Republicans who lead the Senate.
[01:07:01.280 --> 01:07:05.000]   Mitch McConnell calls it dead on arrival.
[01:07:05.000 --> 01:07:09.920]   Also Trump has basically said that he plans to veto this from what I understand.
[01:07:09.920 --> 01:07:12.320]   What do you think?
[01:07:12.320 --> 01:07:18.720]   I mean, this, so then does that make this more or less just symbolic than anything else?
[01:07:18.720 --> 01:07:26.600]   I mean, if Republicans don't plan on doing anything in the Senate and the president's
[01:07:26.600 --> 01:07:31.160]   going to veto it even if they do, I mean, that's not reason not to do it.
[01:07:31.160 --> 01:07:33.680]   But what where next?
[01:07:33.680 --> 01:07:39.600]   So this is a good, what's it, litmus tests or something for the midterms?
[01:07:39.600 --> 01:07:40.600]   Okay.
[01:07:40.600 --> 01:07:41.600]   Not midterms.
[01:07:41.600 --> 01:07:42.600]   The election.
[01:07:42.600 --> 01:07:44.880]   The opposite of the midterms.
[01:07:44.880 --> 01:07:47.040]   Yeah, the term.
[01:07:47.040 --> 01:07:52.840]   So as that is that star, it'll give people a talking point.
[01:07:52.840 --> 01:07:53.840]   Yeah.
[01:07:53.840 --> 01:08:00.320]   Put it in the spot where we voted for you voted against, there against neutrality.
[01:08:00.320 --> 01:08:02.680]   Now we can have a narrow of our attack ads.
[01:08:02.680 --> 01:08:05.360]   So he voted for or against or whatever.
[01:08:05.360 --> 01:08:06.360]   Yeah.
[01:08:06.360 --> 01:08:10.000]   How much that is, because everything else going on.
[01:08:10.000 --> 01:08:12.880]   Yeah, that's true.
[01:08:12.880 --> 01:08:13.880]   Yeah.
[01:08:13.880 --> 01:08:15.400]   So that happened.
[01:08:15.400 --> 01:08:20.560]   But then there's this kind of larger trend that's happening right now.
[01:08:20.560 --> 01:08:26.960]   In one case, Australia passed a social media law on the heels of the Christchurch massacre
[01:08:26.960 --> 01:08:29.640]   that happened a few weeks prior to this.
[01:08:29.640 --> 01:08:36.320]   They sped through a law that would require social media companies to remove violent content,
[01:08:36.320 --> 01:08:40.920]   things like kidnapping, murders, rape, terror attacks, that sort of stuff.
[01:08:40.920 --> 01:08:46.720]   It doesn't really address hate speech, which was pivotal to what happened at Christchurch.
[01:08:46.720 --> 01:08:51.280]   But Australia, nonetheless, pushed this through very rapidly, I think in like five days for
[01:08:51.280 --> 01:08:53.320]   an inception to passing it through.
[01:08:53.320 --> 01:08:56.280]   The UK released their online harms white paper.
[01:08:56.280 --> 01:08:57.360]   This was published on Monday.
[01:08:57.360 --> 01:09:02.240]   I have right here, I've read almost the entire thing.
[01:09:02.240 --> 01:09:03.240]   Oh, good.
[01:09:03.240 --> 01:09:06.760]   And I've had to be pulled back from suicide a few times.
[01:09:06.760 --> 01:09:07.760]   Oh boy.
[01:09:07.760 --> 01:09:08.760]   They're cleaning the net.
[01:09:08.760 --> 01:09:09.760]   Oh, it's awful.
[01:09:09.760 --> 01:09:10.760]   It's awful.
[01:09:10.760 --> 01:09:14.040]   I wish I had read it so I could act as a nice counterpoint.
[01:09:14.040 --> 01:09:16.640]   Well, so, so, no, I don't think you would stay.
[01:09:16.640 --> 01:09:18.040]   So actually, I don't think you would.
[01:09:18.040 --> 01:09:25.320]   So this is what they're doing is they're going to tell the platforms, you're responsible
[01:09:25.320 --> 01:09:30.320]   for taking down illegal content and illegal behavior.
[01:09:30.320 --> 01:09:33.880]   And then they're going to tell them, well, you also have to take down legal but harmful
[01:09:33.880 --> 01:09:34.880]   behavior.
[01:09:34.880 --> 01:09:35.880]   Right.
[01:09:35.880 --> 01:09:36.880]   What is that?
[01:09:36.880 --> 01:09:37.960]   What is the line there?
[01:09:37.960 --> 01:09:38.960]   That's every-
[01:09:38.960 --> 01:09:40.600]   error for speech.
[01:09:40.600 --> 01:09:43.560]   So for example, they want to outlaw trolling.
[01:09:43.560 --> 01:09:44.560]   Okay.
[01:09:44.560 --> 01:09:45.560]   Define it.
[01:09:45.560 --> 01:09:46.560]   Define it.
[01:09:46.560 --> 01:09:47.560]   No, that means you've defined it.
[01:09:47.560 --> 01:09:49.360]   And if you don't do it right, if you don't define it, we're going to take away a huge
[01:09:49.360 --> 01:09:54.320]   amount of money and we're going to find not only you big company, we're going to find
[01:09:54.320 --> 01:09:55.320]   your executives.
[01:09:55.320 --> 01:10:02.600]   Meanwhile, the other report that was out recently says we're also going to find the programmers.
[01:10:02.600 --> 01:10:03.600]   Disinformation.
[01:10:03.600 --> 01:10:05.560]   That's a fun one.
[01:10:05.560 --> 01:10:10.760]   How do you get rid of legal disinformation?
[01:10:10.760 --> 01:10:12.440]   The things your uncle Joe says to you.
[01:10:12.440 --> 01:10:13.440]   That were legal.
[01:10:13.440 --> 01:10:18.000]   So what's going to happen, obviously, is that the platforms are going to play safe and
[01:10:18.000 --> 01:10:20.520]   pull down anything that has to do with conversation.
[01:10:20.520 --> 01:10:23.240]   So this applies to anyone.
[01:10:23.240 --> 01:10:26.200]   I'll see if I can find it real quickly.
[01:10:26.200 --> 01:10:29.440]   I can't believe I spent my time reading this junk.
[01:10:29.440 --> 01:10:31.400]   You've got to read something.
[01:10:31.400 --> 01:10:32.400]   That's true.
[01:10:32.400 --> 01:10:34.400]   I can find better things.
[01:10:34.400 --> 01:10:39.160]   It's called a duty of care that you should just be right in advance and do this stuff
[01:10:39.160 --> 01:10:40.720]   and figure it out.
[01:10:40.720 --> 01:10:43.000]   Here it is.
[01:10:43.000 --> 01:10:47.560]   The regulatory framework should apply, "Two companies that allow users to share or discover
[01:10:47.560 --> 01:10:51.720]   user-generated content or interact with each other online."
[01:10:51.720 --> 01:10:52.720]   Period.
[01:10:52.720 --> 01:10:55.320]   That's the entirety of the net.
[01:10:55.320 --> 01:10:56.800]   It's conversation.
[01:10:56.800 --> 01:11:00.640]   And any conversation is now going to be regulated.
[01:11:00.640 --> 01:11:05.120]   Meanwhile, they're not sure what to do about private channels.
[01:11:05.120 --> 01:11:09.240]   They're trying to figure that out, which means that the bad guys will just go down into
[01:11:09.240 --> 01:11:10.400]   them.
[01:11:10.400 --> 01:11:19.000]   And it's terrible, terrible, terrible law in the making.
[01:11:19.000 --> 01:11:20.000]   It's really frightening.
[01:11:20.000 --> 01:11:23.800]   Meanwhile, though, Stacey, I'm going to surprise you here.
[01:11:23.800 --> 01:11:24.800]   Unless--
[01:11:24.800 --> 01:11:25.800]   Okay.
[01:11:25.800 --> 01:11:26.800]   --the Internet court before.
[01:11:26.800 --> 01:11:27.800]   Oh, fick it.
[01:11:27.800 --> 01:11:34.800]   Talk about it again, and then we'll-- I'll cut you off if you're getting ready.
[01:11:34.800 --> 01:11:38.200]   Carson's memory is not 100% all the time.
[01:11:38.200 --> 01:11:39.200]   It's like 90%--
[01:11:39.200 --> 01:11:44.200]   I kind of took a lot when Jeff's talking.
[01:11:44.200 --> 01:11:45.200]   Hey!
[01:11:45.200 --> 01:11:46.200]   Hey!
[01:11:46.200 --> 01:11:47.200]   Hey!
[01:11:47.200 --> 01:11:50.200]   I thought we were Goombas.
[01:11:50.200 --> 01:11:51.840]   I thought we were friends.
[01:11:51.840 --> 01:11:54.040]   He's cackling maniacally right now.
[01:11:54.040 --> 01:11:58.680]   So these IV-- could be cruel.
[01:11:58.680 --> 01:12:01.400]   So I think I mentioned this part of it.
[01:12:01.400 --> 01:12:06.960]   I joined a high-level working group, a high-level-- a transatlantic high-level working group
[01:12:06.960 --> 01:12:11.600]   on content moderation and freedom of expression.
[01:12:11.600 --> 01:12:15.600]   And out of this came-- and it's chat about rules, so I can't say who said what.
[01:12:15.600 --> 01:12:20.120]   And I want to in some cases, but they said they don't want to be quoted.
[01:12:20.120 --> 01:12:24.720]   But they're a very smart person proposed Internet courts.
[01:12:24.720 --> 01:12:30.160]   And the argument here is that the platforms are being put in a position-- this is in Germany,
[01:12:30.160 --> 01:12:33.480]   it's true, with this it's true, and Australia it's true-- where they're being told, you go
[01:12:33.480 --> 01:12:37.600]   find the illegal stuff, you decide if it's illegal, and you get rid of it.
[01:12:37.600 --> 01:12:41.880]   Never mind the legal but harmful, but just even illegal stuff, right?
[01:12:41.880 --> 01:12:42.880]   What's illegal?
[01:12:42.880 --> 01:12:46.920]   In Germany, that's a day game, they've got to decide what is manifestly illegal.
[01:12:46.920 --> 01:12:50.240]   And they're owned within 24 hours, or they get sued huge amounts of money.
[01:12:50.240 --> 01:12:52.720]   So what even at the point is that the German authorities said they were taking down too
[01:12:52.720 --> 01:12:53.720]   much.
[01:12:53.720 --> 01:12:56.040]   Well, of course they were, because they were playing safe.
[01:12:56.040 --> 01:13:00.640]   The problem with this is, obviously, that the companies don't know how to do this.
[01:13:00.640 --> 01:13:02.680]   So they're making up as they go.
[01:13:02.680 --> 01:13:04.320]   It's all in secret, there's no due process.
[01:13:04.320 --> 01:13:08.920]   So this very smart person proposed an Internet court, nation to nation.
[01:13:08.920 --> 01:13:15.800]   They were betrayed judges, and that the Internet company, when it came to any dispute, they
[01:13:15.800 --> 01:13:17.080]   weren't sure if it was illegal.
[01:13:17.080 --> 01:13:21.560]   The person who did it said, "No, you're wrong company, it's not illegal," or whatever.
[01:13:21.560 --> 01:13:22.560]   It goes to the court.
[01:13:22.560 --> 01:13:28.120]   And the point of this smart person was that then due process comes back in, number one,
[01:13:28.120 --> 01:13:31.840]   number two, this is doing things in the bright light of public.
[01:13:31.840 --> 01:13:35.720]   And number three, that's where we're supposed to negotiate our legal norms in this world,
[01:13:35.720 --> 01:13:38.920]   is in court in the public with due process.
[01:13:38.920 --> 01:13:45.760]   And so I found that actually appealing because then the government is doing what the government
[01:13:45.760 --> 01:13:46.760]   is supposed to do.
[01:13:46.760 --> 01:13:51.200]   It should be right, very clear laws and have very clear enforcement of them in the courts
[01:13:51.200 --> 01:13:52.200]   with due process.
[01:13:52.200 --> 01:13:58.080]   Now then in terms of the legal but harmful, that should be a matter of the company's own
[01:13:58.080 --> 01:14:03.280]   terms of service with their users, where, for example, Facebook says, "Naked breasts,
[01:14:03.280 --> 01:14:04.280]   no."
[01:14:04.280 --> 01:14:06.280]   I think it's dumb, but they can do that, right?
[01:14:06.280 --> 01:14:08.200]   That's their definition of harmful.
[01:14:08.200 --> 01:14:09.200]   That helps us.
[01:14:09.200 --> 01:14:15.680]   And the way it works in the US under the Federal Trade Commission structure is that any company
[01:14:15.680 --> 01:14:18.360]   can have guarantees they make to their customers.
[01:14:18.360 --> 01:14:22.880]   The Federal Trade Commission doesn't care what those guarantees are, except if you violate
[01:14:22.880 --> 01:14:27.400]   that, if you don't stand up to the covenant you make with your customers, then they come
[01:14:27.400 --> 01:14:28.400]   after you.
[01:14:28.400 --> 01:14:31.240]   In essence, for consumer fraud, you said you do it, you didn't.
[01:14:31.240 --> 01:14:36.720]   So you split that world up where you have variable standards set by platform and you
[01:14:36.720 --> 01:14:38.520]   can decide, "I like this platform.
[01:14:38.520 --> 01:14:42.640]   I don't like this platform because of their rules," whatever, but those are their rules.
[01:14:42.640 --> 01:14:48.080]   And the company is monitored based on whether or not it meets its own rules.
[01:14:48.080 --> 01:14:49.880]   That's an illegal framework.
[01:14:49.880 --> 01:14:53.240]   And in the illegal framework, the company still has to play police because the scale
[01:14:53.240 --> 01:14:54.240]   is too huge.
[01:14:54.240 --> 01:14:55.680]   They still have to find the stuff.
[01:14:55.680 --> 01:15:00.600]   They're only liable once they've been notified something's bad, once they don't act.
[01:15:00.600 --> 01:15:04.120]   And if they have any doubt, now they have a court to go to.
[01:15:04.120 --> 01:15:08.160]   So that's a regulatory regime that starts to make sense to me.
[01:15:08.160 --> 01:15:11.160]   I wrote it up on Medium.
[01:15:11.160 --> 01:15:16.800]   I was with you until the tada, and now I'm like, "No, we're just teasing Jeff."
[01:15:16.800 --> 01:15:18.640]   No, I like it.
[01:15:18.640 --> 01:15:20.240]   I should read your stuff.
[01:15:20.240 --> 01:15:21.240]   I should read your stuff.
[01:15:21.240 --> 01:15:22.240]   It's a super person.
[01:15:22.240 --> 01:15:24.200]   What kind of person?
[01:15:24.200 --> 01:15:26.400]   The tada kind of person.
[01:15:26.400 --> 01:15:27.400]   Oh, totally.
[01:15:27.400 --> 01:15:28.400]   Yes.
[01:15:28.400 --> 01:15:29.400]   So your jealous.
[01:15:29.400 --> 01:15:30.400]   I did.
[01:15:30.400 --> 01:15:31.400]   I did.
[01:15:31.400 --> 01:15:32.400]   I did.
[01:15:32.400 --> 01:15:33.400]   I did.
[01:15:33.400 --> 01:15:34.400]   Truth.
[01:15:34.400 --> 01:15:35.400]   You have the truth.
[01:15:35.400 --> 01:15:37.440]   Jeff beat you to the tada, didn't he?
[01:15:37.440 --> 01:15:41.120]   No, I think it really drove his point home in a very, very clear place.
[01:15:41.120 --> 01:15:45.560]   Anyway, I think it's a very clear place.
[01:15:45.560 --> 01:15:49.000]   So meanwhile, the governments are going to Brazil.
[01:15:49.000 --> 01:15:51.280]   You also have the fake news law.
[01:15:51.280 --> 01:15:52.280]   You have Singapore.
[01:15:52.280 --> 01:15:57.520]   They have a fake news law where you're things that are wrong or illegal.
[01:15:57.520 --> 01:16:01.480]   That's a fast-track to authoritarianism.
[01:16:01.480 --> 01:16:04.200]   And these governments don't know what the F to do.
[01:16:04.200 --> 01:16:07.320]   They already have laws in almost every case.
[01:16:07.320 --> 01:16:08.320]   They're not enforcing them.
[01:16:08.320 --> 01:16:12.080]   They're forcing the companies to enforce them with no clear guidelines.
[01:16:12.080 --> 01:16:13.160]   Companies don't know what to do.
[01:16:13.160 --> 01:16:15.080]   They're messing up.
[01:16:15.080 --> 01:16:16.080]   This is a mess.
[01:16:16.080 --> 01:16:17.920]   I completely agree.
[01:16:17.920 --> 01:16:22.040]   But the way that the way that if you read this online harm report, it's just amazing.
[01:16:22.040 --> 01:16:26.080]   They're just flailing over themselves saying, if we don't like it, you shouldn't have it.
[01:16:26.080 --> 01:16:27.080]   You bad companies.
[01:16:27.080 --> 01:16:28.080]   We're going to blame you for it.
[01:16:28.080 --> 01:16:30.240]   And by the way, one more thing.
[01:16:30.240 --> 01:16:32.160]   Only once so far, I haven't finished every word.
[01:16:32.160 --> 01:16:38.240]   But only once did I see a reference of taking the actual illegal actor to the authorities.
[01:16:38.240 --> 01:16:39.240]   Right?
[01:16:39.240 --> 01:16:44.000]   So constantly, the platforms are being blamed for everything that's being done on them.
[01:16:44.000 --> 01:16:52.960]   And if someone is harassing someone, if someone is spewing terrorists or race, eat, betrayal,
[01:16:52.960 --> 01:16:57.840]   and if the country wants to say that's illegal, then what if the person who spews it?
[01:16:57.840 --> 01:16:59.960]   Forget where it was spewed.
[01:16:59.960 --> 01:17:00.960]   Go to the spewer.
[01:17:00.960 --> 01:17:01.960]   Sorry.
[01:17:01.960 --> 01:17:04.720]   Well, it really seems to me when you put...
[01:17:04.720 --> 01:17:06.240]   Okay, wait a minute.
[01:17:06.240 --> 01:17:08.160]   The more I think about that, yeah, that is a little weird.
[01:17:08.160 --> 01:17:09.160]   The way...
[01:17:09.160 --> 01:17:10.160]   Wait a minute.
[01:17:10.160 --> 01:17:11.160]   The way...
[01:17:11.160 --> 01:17:21.880]   The way that you put me out, I totally lost my train of thought.
[01:17:21.880 --> 01:17:23.880]   Spewer just derailed me.
[01:17:23.880 --> 01:17:26.640]   That's what you eat.
[01:17:26.640 --> 01:17:29.360]   I need someone to jump in because now I can't.
[01:17:29.360 --> 01:17:34.240]   So the spewer is probably protected by free speech?
[01:17:34.240 --> 01:17:35.800]   Here we go, yes.
[01:17:35.800 --> 01:17:37.400]   Or it can be also...
[01:17:37.400 --> 01:17:39.160]   Well, if we see a lot --
[01:17:39.160 --> 01:17:45.580]   If we see a lot of behavior, what I'm saying is then go after the person who does the illegal
[01:17:45.580 --> 01:17:48.720]   behavior rather than the one who accidentally carries it.
[01:17:48.720 --> 01:17:55.400]   So if you're saying like inciting violence or inciting harm, and are those actual laws
[01:17:55.400 --> 01:17:59.760]   or the interpretations of laws brought forth by the Constitution?
[01:17:59.760 --> 01:18:03.280]   Depends on the country in a place like Germany, right?
[01:18:03.280 --> 01:18:07.520]   It is illegal to deny the Holocaust.
[01:18:07.520 --> 01:18:08.520]   Oh, okay.
[01:18:08.520 --> 01:18:12.240]   So, I'm just being told -- you must take that down.
[01:18:12.240 --> 01:18:14.760]   Okay, that's the law.
[01:18:14.760 --> 01:18:15.760]   Okay.
[01:18:15.760 --> 01:18:22.720]   But then, hardly ever do we hear of a process that goes to the person who actually does that.
[01:18:22.720 --> 01:18:28.360]   The platform is being blamed for the behaviors of the public upon it, and it's expected to
[01:18:28.360 --> 01:18:29.600]   clean up the public.
[01:18:29.600 --> 01:18:36.720]   I was wondering, must it be C this week about all this?
[01:18:36.720 --> 01:18:38.600]   The anchor says, "Come on, Jeff.
[01:18:38.600 --> 01:18:40.000]   Come on, Jeff."
[01:18:40.000 --> 01:18:43.360]   White nationalism, because there was going to be the hearing about this, which we'll talk
[01:18:43.360 --> 01:18:44.760]   about more in a second.
[01:18:44.760 --> 01:18:46.600]   White nationalism.
[01:18:46.600 --> 01:18:50.400]   And I said, in part, and Leo, we got nervous right now, I'm going to say something political.
[01:18:50.400 --> 01:18:53.160]   But part of me, I'm going to say it and just don't at me.
[01:18:53.160 --> 01:18:54.160]   There are white nationalists.
[01:18:54.160 --> 01:18:55.160]   There's a white nationalist in Congress.
[01:18:55.160 --> 01:18:57.520]   There's a few at the White House.
[01:18:57.520 --> 01:18:59.680]   You get rid of all the white nationalists in one line.
[01:18:59.680 --> 01:19:02.000]   You haven't solved our problem in society.
[01:19:02.000 --> 01:19:04.320]   We have a problem with racism in society.
[01:19:04.320 --> 01:19:07.160]   And I think there's this belief that if we could just clean up the internet, everything
[01:19:07.160 --> 01:19:08.840]   is going to be okay again.
[01:19:08.840 --> 01:19:09.840]   No.
[01:19:09.840 --> 01:19:10.840]   Okay, that's fair.
[01:19:10.840 --> 01:19:11.840]   That's really simple.
[01:19:11.840 --> 01:19:12.840]   It's a little bit of a temptation.
[01:19:12.840 --> 01:19:14.760]   It'll be okay again.
[01:19:14.760 --> 01:19:18.320]   So, no, that's not the case.
[01:19:18.320 --> 01:19:22.320]   But what's happening is we're blaming -- it's more a panic, pardon me, as I said it.
[01:19:22.320 --> 01:19:26.440]   We're blaming the technology for all these human behaviors, and we're stopping at the
[01:19:26.440 --> 01:19:27.440]   technology.
[01:19:27.440 --> 01:19:31.400]   And we're not going to the root cause or to the root players.
[01:19:31.400 --> 01:19:37.520]   And so there was a hearing this week of Facebook and Google, white nationalism, and you have
[01:19:37.520 --> 01:19:42.760]   a story in the rundown about what happened to the live feed comments.
[01:19:42.760 --> 01:19:49.560]   They got overtaken by comments along those lines, along those very lines.
[01:19:49.560 --> 01:19:52.360]   They proved -- They came in and said, "You don't like us?
[01:19:52.360 --> 01:19:53.360]   Here we are."
[01:19:53.360 --> 01:19:54.360]   Right.
[01:19:54.360 --> 01:19:57.760]   So, in society right now, we've got bigger problems, and we're not saying that platforms
[01:19:57.760 --> 01:19:58.760]   are blameless.
[01:19:58.760 --> 01:20:00.600]   I'm not saying that they are without responsibility.
[01:20:00.600 --> 01:20:03.480]   I'm not saying that they shouldn't -- yes, should they set their own rules to give our
[01:20:03.480 --> 01:20:04.480]   white nationalism?
[01:20:04.480 --> 01:20:06.400]   Yeah, I actually think so.
[01:20:06.400 --> 01:20:08.400]   That's their choice to do.
[01:20:08.400 --> 01:20:14.000]   But when government tells them they must, that's de facto illegal.
[01:20:14.000 --> 01:20:16.200]   And that has a problem with our first amendment.
[01:20:16.200 --> 01:20:20.960]   Well, so it's the government -- No, and you're right.
[01:20:20.960 --> 01:20:25.560]   And so if you think about the government trying to go after people through the platforms,
[01:20:25.560 --> 01:20:30.640]   though -- so you're right about this being -- it is an effective mirror.
[01:20:30.640 --> 01:20:35.480]   I would say it's both a mirror of the worst of us, our society, but it also does magnify
[01:20:35.480 --> 01:20:38.080]   it, which is kind of the problem.
[01:20:38.080 --> 01:20:41.920]   Yeah, now I'm like, "Why don't we go after?
[01:20:41.920 --> 01:20:47.240]   Is it because Google won't give away identity for people who are -- I mean, do we need rules
[01:20:47.240 --> 01:20:52.840]   that make it acceptable or a consent when you sign up for platforms that in the case
[01:20:52.840 --> 01:21:00.720]   of -- but then you get into this issue that in the case of hate speech or something actually
[01:21:00.720 --> 01:21:04.840]   illegal that you would pass that over to authorities, but then authorities may use that
[01:21:04.840 --> 01:21:06.080]   and overstep their bounds?
[01:21:06.080 --> 01:21:08.280]   I'm just thinking out loud here.
[01:21:08.280 --> 01:21:09.280]   Sure.
[01:21:09.280 --> 01:21:18.400]   And would users use a service that had the ability to analyze their use and determine
[01:21:18.400 --> 01:21:26.000]   whether a user's approach to using that service falls into the good category or the bad category?
[01:21:26.000 --> 01:21:31.760]   Would I as a user -- even if my intention is not bad, would I as a user trust the service
[01:21:31.760 --> 01:21:36.000]   to be able to make that determination and to hand my information out?
[01:21:36.000 --> 01:21:40.440]   So Lindsey Graham -- I'm sorry, I'm back in the echo room because I was too loud out there.
[01:21:40.440 --> 01:21:45.720]   Lindsey Graham accused the platforms this week of the left-wing bias of what they choose
[01:21:45.720 --> 01:21:46.880]   to kill.
[01:21:46.880 --> 01:21:53.040]   The platforms are -- maybe the richest beasts on Earth, they control the world, but give
[01:21:53.040 --> 01:21:54.120]   a little sympathy here.
[01:21:54.120 --> 01:21:57.840]   They're stuck in a no-win position here.
[01:21:57.840 --> 01:21:58.840]   Yeah.
[01:21:58.840 --> 01:22:01.840]   They're like parents with their kids fighting in the backseat.
[01:22:01.840 --> 01:22:03.400]   He's got this.
[01:22:03.400 --> 01:22:05.080]   You're paying -- he is side-done.
[01:22:05.080 --> 01:22:07.080]   Oh, my God.
[01:22:07.080 --> 01:22:08.080]   Yes.
[01:22:08.080 --> 01:22:09.080]   Yes.
[01:22:09.080 --> 01:22:10.080]   Yes.
[01:22:10.080 --> 01:22:13.240]   So we need a more nuanced discussion of this.
[01:22:13.240 --> 01:22:18.200]   We need to go after the bad behaviorists, the viewers.
[01:22:18.200 --> 01:22:21.440]   We need to go after the core cause and roots of this.
[01:22:21.440 --> 01:22:23.320]   We need to do a lot.
[01:22:23.320 --> 01:22:27.800]   And that's what I -- what I used to be -- what I used to be about moral panic, what I'm saying
[01:22:27.800 --> 01:22:35.520]   is it's a simplistic blame game that says the technology is causing all this.
[01:22:35.520 --> 01:22:37.280]   It's much more complex than that.
[01:22:37.280 --> 01:22:42.200]   And a bunch of laws, a bunch of regulations, a bunch of fines against technology companies
[01:22:42.200 --> 01:22:44.720]   aren't going to do it then where are we?
[01:22:44.720 --> 01:22:46.560]   Yeah.
[01:22:46.560 --> 01:22:51.880]   From the perspective of the people who wrote the white paper as one example, from their
[01:22:51.880 --> 01:22:57.040]   perspective, they're thinking you choked this off at the source, right?
[01:22:57.040 --> 01:23:03.160]   It's like you stop the flow of water through the hose at the beginning of that hose by
[01:23:03.160 --> 01:23:07.760]   clenching it off, not thinking about the unintended consequences, of course, just thinking about
[01:23:07.760 --> 01:23:15.960]   how can we make the most effect to this problem right now instead of dragging this out further?
[01:23:15.960 --> 01:23:22.640]   And I can understand the desire to pay attention to this and make it important to do something
[01:23:22.640 --> 01:23:29.000]   now, but yeah, it really feels like kind of that unintended consequence aspect of it
[01:23:29.000 --> 01:23:31.080]   is still blown out.
[01:23:31.080 --> 01:23:32.080]   Yeah.
[01:23:32.080 --> 01:23:33.080]   Some things are illegal.
[01:23:33.080 --> 01:23:34.560]   They should be enforced.
[01:23:34.560 --> 01:23:37.920]   Child porn is the one we all agreed to immediately.
[01:23:37.920 --> 01:23:38.920]   Insiting terrorism.
[01:23:38.920 --> 01:23:43.440]   Yeah, I think probably we'd pretty much agree.
[01:23:43.440 --> 01:23:48.800]   But what I guess the thing is like in the German law, insult.
[01:23:48.800 --> 01:23:55.600]   In this regulation, proposed regulation, disinformation, they're just not thinking this through.
[01:23:55.600 --> 01:23:58.640]   And they're creating things that cannot be enforceable.
[01:23:58.640 --> 01:24:00.320]   And so you cannot win.
[01:24:00.320 --> 01:24:01.320]   Yes.
[01:24:01.320 --> 01:24:05.800]   Meanwhile, Zuckerberg is going for worldwide regulation.
[01:24:05.800 --> 01:24:08.280]   That's what he wrote in his op-ed, which I'm sure was discussed while we were here.
[01:24:08.280 --> 01:24:10.680]   So I don't want to go into depth on it.
[01:24:10.680 --> 01:24:14.880]   But understandably, he's saying, this is a pain dealing with all these countries.
[01:24:14.880 --> 01:24:15.880]   Can't you go with just one?
[01:24:15.880 --> 01:24:16.880]   Well, then we'll end up.
[01:24:16.880 --> 01:24:21.840]   Worldwide, well, yeah, that's never going to work because every culture has different
[01:24:21.840 --> 01:24:23.920]   standards in norms.
[01:24:23.920 --> 01:24:27.560]   And if the platforms say we're going to go with the lowest common denominator of freedom
[01:24:27.560 --> 01:24:32.040]   and the highest water work of regulation, then we're all living in China.
[01:24:32.040 --> 01:24:33.040]   Yeah.
[01:24:33.040 --> 01:24:39.000]   And sometimes I feel like I feel like Mark was being a bit disingenuous there in many
[01:24:39.000 --> 01:24:41.040]   ways because that is never going to happen.
[01:24:41.040 --> 01:24:46.440]   A and by calling for it, you know, right.
[01:24:46.440 --> 01:24:48.440]   It's like calling for.
[01:24:48.440 --> 01:24:49.440]   It's like calling for.
[01:24:49.440 --> 01:24:53.080]   I think he actually would love it if it happened.
[01:24:53.080 --> 01:24:54.080]   Well, yes, yes.
[01:24:54.080 --> 01:24:55.320]   It would be very convenient for him.
[01:24:55.320 --> 01:24:57.320]   But the idea is that that's happened.
[01:24:57.320 --> 01:24:58.320]   Yeah.
[01:24:58.320 --> 01:24:59.320]   Yeah.
[01:24:59.320 --> 01:25:01.320]   And I think that if we...
[01:25:01.320 --> 01:25:04.520]   Oh, this is so hard.
[01:25:04.520 --> 01:25:06.240]   I mean, people are hard.
[01:25:06.240 --> 01:25:07.240]   Social more is...
[01:25:07.240 --> 01:25:12.680]   And we see this and this is something that changes over time, which is both good and bad.
[01:25:12.680 --> 01:25:18.240]   You could call it... if you look at King's arc of moral justice, we've changed a lot
[01:25:18.240 --> 01:25:23.960]   as a country in the last 60 years.
[01:25:23.960 --> 01:25:26.880]   But actually, I guess it's now 70 years.
[01:25:26.880 --> 01:25:28.880]   But we do have further to go.
[01:25:28.880 --> 01:25:30.280]   And other countries are far behind.
[01:25:30.280 --> 01:25:31.560]   And some are way ahead.
[01:25:31.560 --> 01:25:35.120]   And on some issues, we're very progressive on others.
[01:25:35.120 --> 01:25:37.680]   We're incredibly conservative.
[01:25:37.680 --> 01:25:40.440]   So this is just hard.
[01:25:40.440 --> 01:25:45.440]   You mean we're not going to solve it right now on this episode of This Week in Google?
[01:25:45.440 --> 01:25:46.440]   I know.
[01:25:46.440 --> 01:25:47.440]   Or I solve it.
[01:25:47.440 --> 01:25:48.440]   It's on medium.
[01:25:48.440 --> 01:25:49.440]   It's there.
[01:25:49.440 --> 01:25:50.440]   That's it.
[01:25:50.440 --> 01:25:53.120]   That's a good idea.
[01:25:53.120 --> 01:25:54.120]   I like that.
[01:25:54.120 --> 01:25:55.120]   Yeah.
[01:25:55.120 --> 01:25:57.520]   Although courts are subject to politic...
[01:25:57.520 --> 01:25:58.520]   Politic...
[01:25:58.520 --> 01:25:59.520]   Politic...
[01:25:59.520 --> 01:26:00.520]   Politic...
[01:26:00.520 --> 01:26:01.520]   Politic...
[01:26:01.520 --> 01:26:03.600]   I'm not going to be able to say that word.
[01:26:03.600 --> 01:26:07.120]   But yeah, people will use politics in how they...
[01:26:07.120 --> 01:26:08.120]   That's a good question.
[01:26:08.120 --> 01:26:09.120]   Wait, what, Jeff?
[01:26:09.120 --> 01:26:13.120]   I get that one with another shot at Tequila.
[01:26:13.120 --> 01:26:16.120]   Oh, just you wait until my special thing.
[01:26:16.120 --> 01:26:20.360]   We're all drinking shots today.
[01:26:20.360 --> 01:26:22.600]   Should we do the changelog?
[01:26:22.600 --> 01:26:28.920]   The Google changelog!
[01:26:28.920 --> 01:26:30.440]   We got a lot of things on the changelog.
[01:26:30.440 --> 01:26:31.600]   So I'll rattle them off here.
[01:26:31.600 --> 01:26:35.600]   Dropbox showed off a new feature at Google Cloud Next that apparently they had teased
[01:26:35.600 --> 01:26:42.720]   this in March, but integrated Google Docs sheets and slides editing within Dropbox
[01:26:42.720 --> 01:26:43.720]   itself.
[01:26:43.720 --> 01:26:46.280]   And apparently this is something that we're going to see a little bit more of.
[01:26:46.280 --> 01:26:51.880]   This kind of cross collaboration between services.
[01:26:51.880 --> 01:26:52.880]   So you don't have to...
[01:26:52.880 --> 01:26:54.840]   Like the clouds, yeah.
[01:26:54.840 --> 01:26:57.440]   I was going to say, are they going to use containers?
[01:26:57.440 --> 01:26:59.040]   Probably.
[01:26:59.040 --> 01:27:01.640]   Maybe this is the prime example.
[01:27:01.640 --> 01:27:07.960]   So third party add-on support coming to G Suite in coming months as a beta idea here is to
[01:27:07.960 --> 01:27:11.280]   lessen the need to, again, jump between apps when doing things.
[01:27:11.280 --> 01:27:16.480]   So integrating Evernote into the sidebar of Gmail is one example that they showed off.
[01:27:16.480 --> 01:27:21.120]   So you don't need to leave your Gmail in order to go in Evernote where your notes are.
[01:27:21.120 --> 01:27:22.120]   That sort of stuff.
[01:27:22.120 --> 01:27:27.360]   So they've got Partners Box, Evernote, Asana, DocuSign, QuickBooks, 15 of them at launch
[01:27:27.360 --> 01:27:31.480]   and more over time.
[01:27:31.480 --> 01:27:33.600]   We already talked about Google Assistant.
[01:27:33.600 --> 01:27:36.240]   The new security tools for G Suite.
[01:27:36.240 --> 01:27:42.360]   So they have the new security sandbox that will actually execute attachments before letting
[01:27:42.360 --> 01:27:43.400]   you open them.
[01:27:43.400 --> 01:27:47.360]   So they execute those in the sandbox that it can affect you once you open it on your
[01:27:47.360 --> 01:27:48.360]   end.
[01:27:48.360 --> 01:27:53.160]   It can detect what's going on there before it hits your side of things.
[01:27:53.160 --> 01:27:58.040]   Also a new security and alert center for admins, for single security services, best
[01:27:58.040 --> 01:28:02.720]   practices, tools, and addressing threats.
[01:28:02.720 --> 01:28:05.360]   But and also along the security tip, this is kind of cool.
[01:28:05.360 --> 01:28:10.640]   Google announced that Android devices running Android 7 or higher can act as a physical
[01:28:10.640 --> 01:28:13.400]   security key for two-factor authentication.
[01:28:13.400 --> 01:28:17.080]   So you don't need an extra dongle like the Titan security key that they rolled out last
[01:28:17.080 --> 01:28:18.080]   year.
[01:28:18.080 --> 01:28:23.480]   So it's plain how that works versus the late two-factor which I do now.
[01:28:23.480 --> 01:28:25.480]   And then heavy with the dongle.
[01:28:25.480 --> 01:28:26.960]   So how does this?
[01:28:26.960 --> 01:28:29.920]   So if you have a computer, let's say you have a Chromebook, I'm assuming this will work
[01:28:29.920 --> 01:28:33.800]   with a Chromebook that supports Bluetooth, you would connect your phone via Bluetooth
[01:28:33.800 --> 01:28:36.800]   to the Chrome browser to verify your logins.
[01:28:36.800 --> 01:28:40.600]   So it would your phone is connecting wirelessly.
[01:28:40.600 --> 01:28:44.560]   That is your hardware, your hardware connection essentially.
[01:28:44.560 --> 01:28:47.600]   There's been a lot of Bluetooth exploits.
[01:28:47.600 --> 01:28:51.960]   Granted, you do have to be really close within Bluetooth range.
[01:28:51.960 --> 01:28:52.960]   But.
[01:28:52.960 --> 01:28:53.960]   Yeah.
[01:28:53.960 --> 01:28:54.960]   Another option.
[01:28:54.960 --> 01:28:56.680]   Oh, you just said the magic word.
[01:28:56.680 --> 01:28:58.680]   You're screaming when I said that.
[01:28:58.680 --> 01:28:59.680]   Yeah.
[01:28:59.680 --> 01:29:00.680]   I'm so sorry.
[01:29:00.680 --> 01:29:03.920]   My notes were covered with an assistant bubble.
[01:29:03.920 --> 01:29:08.480]   So yes, this is going to work in Gmail, G Suite, Google Cloud, any Google account service
[01:29:08.480 --> 01:29:14.760]   essentially using Fido authentication standard.
[01:29:14.760 --> 01:29:16.200]   Let's see here.
[01:29:16.200 --> 01:29:22.280]   Google Plus for enterprise is well, Google Plus is dead and they needed to do something
[01:29:22.280 --> 01:29:23.760]   with Google Plus for enterprise.
[01:29:23.760 --> 01:29:25.600]   So it got a new name.
[01:29:25.600 --> 01:29:29.320]   In usual Google fashion, it's kind of confusing.
[01:29:29.320 --> 01:29:31.720]   Google Currents is its name.
[01:29:31.720 --> 01:29:36.800]   And that's confusing because Google Currents was something that Google did back in 2011.
[01:29:36.800 --> 01:29:38.960]   It was a magazine app that they released.
[01:29:38.960 --> 01:29:39.960]   So I.
[01:29:39.960 --> 01:29:40.960]   I powered waves.
[01:29:40.960 --> 01:29:41.960]   Yeah.
[01:29:41.960 --> 01:29:42.960]   Right.
[01:29:42.960 --> 01:29:45.440]   See, that's what it should have.
[01:29:45.440 --> 01:29:47.440]   Two years later, they renamed it Google News.
[01:29:47.440 --> 01:29:49.640]   So I guess the name was available again.
[01:29:49.640 --> 01:29:54.200]   And it was long ago enough that you probably aren't still holding on to that.
[01:29:54.200 --> 01:29:59.640]   So that's available today in beta form assistance.
[01:29:59.640 --> 01:30:02.720]   We talked a little bit about the ads coming into assistant.
[01:30:02.720 --> 01:30:04.280]   There are also some new features.
[01:30:04.280 --> 01:30:08.160]   Cards are getting more information dense and more interactive.
[01:30:08.160 --> 01:30:11.400]   So they showed off a number of different examples of this.
[01:30:11.400 --> 01:30:15.720]   Like if you did a search for alphabet stock before you would get just the really basic
[01:30:15.720 --> 01:30:19.120]   like four or five pieces of information about where it's at right now.
[01:30:19.120 --> 01:30:24.320]   And after you get like a full chart where you can select like the performance over a
[01:30:24.320 --> 01:30:29.480]   year or the last month or the last week, a lot more information making their way into
[01:30:29.480 --> 01:30:30.480]   the cards.
[01:30:30.480 --> 01:30:35.600]   Also new tools in assistant like bubble level tip calculator, mortgage calculator, color
[01:30:35.600 --> 01:30:37.640]   picker and some other things.
[01:30:37.640 --> 01:30:41.000]   So making assistant smarter.
[01:30:41.000 --> 01:30:45.760]   And then finally, Google Smart Compose.
[01:30:45.760 --> 01:30:48.800]   Gmail may not have inbox bundles yet.
[01:30:48.800 --> 01:30:52.600]   And yes, I will say this every time there's a new feature added to Gmail until it happens.
[01:30:52.600 --> 01:30:56.080]   But it did bring smart replies into subject lines.
[01:30:56.080 --> 01:31:01.120]   So for example, if you wrote an email and the body said, Hey, happy birthday, Craig.
[01:31:01.120 --> 01:31:03.360]   It's really great that you were born.
[01:31:03.360 --> 01:31:07.880]   Then the subject might, you know, give you the option of saying happy birthday.
[01:31:07.880 --> 01:31:10.000]   So we change it after you've written it?
[01:31:10.000 --> 01:31:11.760]   No, no, no, no, you write the email first.
[01:31:11.760 --> 01:31:12.760]   Right.
[01:31:12.760 --> 01:31:14.520]   If you went in and you write the book.
[01:31:14.520 --> 01:31:17.760]   Who writes their emails without having the subject?
[01:31:17.760 --> 01:31:19.680]   Do you guys write your emails first and not your subject?
[01:31:19.680 --> 01:31:20.680]   No, I don't.
[01:31:20.680 --> 01:31:23.040]   But I know some people who hate doing subject lines.
[01:31:23.040 --> 01:31:24.040]   Oh, yeah.
[01:31:24.040 --> 01:31:25.040]   Yeah.
[01:31:25.040 --> 01:31:26.600]   I don't always spend ever once a while.
[01:31:26.600 --> 01:31:29.080]   I do because I've got like the idea for the email in my head.
[01:31:29.080 --> 01:31:30.840]   I'm just like fired off.
[01:31:30.840 --> 01:31:32.680]   And then I go and do the subject line at the end.
[01:31:32.680 --> 01:31:33.680]   Sure.
[01:31:33.680 --> 01:31:34.680]   But not every time.
[01:31:34.680 --> 01:31:35.680]   Okay.
[01:31:35.680 --> 01:31:40.560]   What you do to find out what's the open rate is on different subject lines, they'll help
[01:31:40.560 --> 01:31:42.800]   you get your memos.
[01:31:42.800 --> 01:31:47.600]   We know for sure that if you put this in your subject line, but the word free, people
[01:31:47.600 --> 01:31:48.600]   are guaranteed open it.
[01:31:48.600 --> 01:31:49.600]   Oh my God.
[01:31:49.600 --> 01:31:52.320]   The pitches that I might get after that.
[01:31:52.320 --> 01:31:53.320]   Yes.
[01:31:53.320 --> 01:31:57.720]   Oh, and that's all I have in the Google change law.
[01:31:57.720 --> 01:32:02.720]   There's probably more, but there's so much more by the person's wife's birthday.
[01:32:02.720 --> 01:32:04.960]   Her name is the last birthday.
[01:32:04.960 --> 01:32:05.960]   Yes.
[01:32:05.960 --> 01:32:09.920]   He's coming up and we're already five minutes past the time that I told her we were definitely
[01:32:09.920 --> 01:32:10.920]   going to be done.
[01:32:10.920 --> 01:32:14.120]   So let's thank the final sponsor and then we will do our tips and tricks and we'll get
[01:32:14.120 --> 01:32:16.120]   Carson to his celebration.
[01:32:16.120 --> 01:32:17.120]   One thing.
[01:32:17.120 --> 01:32:18.120]   One thing.
[01:32:18.120 --> 01:32:19.120]   First, yeah.
[01:32:19.120 --> 01:32:22.920]   Let us pay tribute to the black hole and to the young woman.
[01:32:22.920 --> 01:32:24.880]   Oh, yes.
[01:32:24.880 --> 01:32:26.520]   We cannot leave before we do that.
[01:32:26.520 --> 01:32:27.520]   No, you're right.
[01:32:27.520 --> 01:32:28.840]   You're absolutely right.
[01:32:28.840 --> 01:32:30.920]   Today's a really big day, right?
[01:32:30.920 --> 01:32:36.280]   Celebrating women and STEM because computer scientist Katie Bowman, who got her bachelor's
[01:32:36.280 --> 01:32:37.720]   in 2011.
[01:32:37.720 --> 01:32:42.040]   She was the lead of the development of the first algorithm to actually show a black hole
[01:32:42.040 --> 01:32:43.040]   for the very first time.
[01:32:43.040 --> 01:32:47.200]   Three years ago, she began the development of this algorithm approximately a thousand
[01:32:47.200 --> 01:32:49.480]   disks, five petabytes of data.
[01:32:49.480 --> 01:32:52.920]   And today the image is released.
[01:32:52.920 --> 01:32:58.120]   The first time a black hole has been photographed using the event horizon telescope.
[01:32:58.120 --> 01:32:59.120]   There it is.
[01:32:59.120 --> 01:33:00.120]   That is amazing.
[01:33:00.120 --> 01:33:01.120]   That is so cool.
[01:33:01.120 --> 01:33:02.120]   That is great.
[01:33:02.120 --> 01:33:05.680]   That picture of her, which we're showing on the air.
[01:33:05.680 --> 01:33:07.280]   It's just so wonderful.
[01:33:07.280 --> 01:33:08.720]   I love that.
[01:33:08.720 --> 01:33:12.960]   I love this story and I love that she's being celebrated for this because I'll say
[01:33:12.960 --> 01:33:15.560]   it's a huge, that's a breakthrough right there.
[01:33:15.560 --> 01:33:16.560]   Thank you.
[01:33:16.560 --> 01:33:21.160]   Oh, no, thank you for bringing it up because I really did want to get that in there and
[01:33:21.160 --> 01:33:23.360]   I completely spaced it.
[01:33:23.360 --> 01:33:25.040]   Okay, sorry.
[01:33:25.040 --> 01:33:26.880]   This is a horrible pun.
[01:33:26.880 --> 01:33:30.960]   This episode of This Week in Google is brought to you by a thousand eyes.
[01:33:30.960 --> 01:33:35.520]   Get an immediate and unmatched view of all the networks and dependencies that impact your
[01:33:35.520 --> 01:33:38.880]   users' digital experience.
[01:33:38.880 --> 01:33:41.280]   So let's talk a little bit about cloud adoption.
[01:33:41.280 --> 01:33:43.560]   We've talked about cloud in this episode.
[01:33:43.560 --> 01:33:47.680]   Let's talk about adoption and doing it right because cloud is obviously it's great for
[01:33:47.680 --> 01:33:48.680]   business.
[01:33:48.680 --> 01:33:54.080]   But there is, of course, a trade off when you gain agility, you increase risk and you
[01:33:54.080 --> 01:33:55.200]   lose control.
[01:33:55.200 --> 01:34:00.120]   When your cloud app or service goes down, how do you know what went wrong?
[01:34:00.120 --> 01:34:01.120]   That led to that.
[01:34:01.120 --> 01:34:04.640]   As you scramble to find the root cause of the problem, you are losing revenue.
[01:34:04.640 --> 01:34:07.240]   You're impacting employee productivity.
[01:34:07.240 --> 01:34:09.160]   You're damaging your brand.
[01:34:09.160 --> 01:34:14.400]   You need instant visibility into the entire service delivery path from the cloud to your
[01:34:14.400 --> 01:34:18.320]   end user, including the portions that you don't own or control.
[01:34:18.320 --> 01:34:20.920]   And that's where Thousand Eyes comes in.
[01:34:20.920 --> 01:34:23.400]   It's unlike anything you've seen before.
[01:34:23.400 --> 01:34:29.920]   Thousand Eyes is cloud-based software built to help organizations do the cloud right.
[01:34:29.920 --> 01:34:34.400]   A massive array of vantage points span the global internet, cloud providers, and even
[01:34:34.400 --> 01:34:36.960]   the Wi-Fi in your local coffee shop.
[01:34:36.960 --> 01:34:39.640]   Cold school IT monitoring is passive.
[01:34:39.640 --> 01:34:46.360]   It operates in silos and can only see inside the data center no use in the cloud.
[01:34:46.360 --> 01:34:52.080]   Thousand Eyes, unique path visualization technology extends beyond any boundaries, allowing you
[01:34:52.080 --> 01:34:58.040]   to see, to understand, and improve the experience for all of your apps, your services, and your
[01:34:58.040 --> 01:34:59.520]   websites.
[01:34:59.520 --> 01:35:03.520]   Regain control and ensure the best possible digital experience for your customers and
[01:35:03.520 --> 01:35:04.520]   employees.
[01:35:04.520 --> 01:35:11.280]   After you're in the right cloud, connected to the right SAS instance and see issues before
[01:35:11.280 --> 01:35:15.040]   they impact your customers and employees with Thousand Eyes.
[01:35:15.040 --> 01:35:19.960]   Join the top banks, enterprises, SAS companies, and the world's largest and fastest growing
[01:35:19.960 --> 01:35:25.080]   brands that rely on Thousand Eyes software to do the cloud and to do it right.
[01:35:25.080 --> 01:35:27.120]   Visit thousandeyes.com/twit.
[01:35:27.120 --> 01:35:30.160]   You can see what you've been missing.
[01:35:30.160 --> 01:35:35.960]   You'll get the exclusive ebook on the five cloud migration challenges you shouldn't ignore.
[01:35:35.960 --> 01:35:43.320]   If the cloud is important to you, either today or in your future, you'll want to visit thousandeyes.com/twit.
[01:35:43.320 --> 01:35:46.160]   That's thousandeyes.com/twit.
[01:35:46.160 --> 01:35:50.800]   Thousand Eyes thrive in a connected world and we thank them for their support of this
[01:35:50.800 --> 01:35:52.400]   weekend Google.
[01:35:52.400 --> 01:35:54.040]   All right.
[01:35:54.040 --> 01:35:58.240]   So let's start off with a cocktail, Stacy.
[01:35:58.240 --> 01:35:59.840]   What you got?
[01:35:59.840 --> 01:36:06.400]   This spring and thoughts of people's thoughts turned to alcohol or not.
[01:36:06.400 --> 01:36:08.840]   This is a cooling cocktail.
[01:36:08.840 --> 01:36:13.280]   I really was tacked out and I'll be honest, you guys.
[01:36:13.280 --> 01:36:15.760]   Could be bad because I am moving in the next few weeks.
[01:36:15.760 --> 01:36:18.280]   But this cocktail is called the Verde Lady.
[01:36:18.280 --> 01:36:23.560]   I have never had or heard about it before, but I made it and it is awesome.
[01:36:23.560 --> 01:36:26.640]   It is a gin, chartreuse, and mint cocktail.
[01:36:26.640 --> 01:36:27.640]   That sounds delightful.
[01:36:27.640 --> 01:36:28.640]   It is.
[01:36:28.640 --> 01:36:31.640]   It's erb-y, but it's not super sweet.
[01:36:31.640 --> 01:36:36.360]   If you want something super sweet, you could add more simple, but it's really well balanced.
[01:36:36.360 --> 01:36:42.080]   I would use a gin that is more of a neutral gin, maybe a little sweeter gin.
[01:36:42.080 --> 01:36:44.280]   So the default here would be like Hendrix.
[01:36:44.280 --> 01:36:48.520]   If you wanted to go straight neutral, you could go with like a tank or a.
[01:36:48.520 --> 01:36:54.080]   But more of you would probably go more for a Bombay or Hendrix is my bet because people
[01:36:54.080 --> 01:36:55.680]   tend to like sweeter stuff.
[01:36:55.680 --> 01:36:57.720]   But it's really good and it's really easy to make.
[01:36:57.720 --> 01:37:02.760]   This person also has lovely photographs, but I just thought I'd share that with you.
[01:37:02.760 --> 01:37:06.400]   And once you have chartreuse in your bar, you can make all sorts of awesome drinks such
[01:37:06.400 --> 01:37:08.880]   as the Last Word, which is another one of my favorites.
[01:37:08.880 --> 01:37:09.880]   Yes.
[01:37:09.880 --> 01:37:10.880]   Last Word is so good.
[01:37:10.880 --> 01:37:11.880]   So good.
[01:37:11.880 --> 01:37:12.880]   So much alcohol though.
[01:37:12.880 --> 01:37:13.880]   Got to really watch those.
[01:37:13.880 --> 01:37:15.680]   That's why it's called the Last Word.
[01:37:15.680 --> 01:37:20.800]   I'm like, if you make a Last Word with whiskey instead of gin, it's called the final say.
[01:37:20.800 --> 01:37:23.000]   So there you go.
[01:37:23.000 --> 01:37:24.000]   Thank you.
[01:37:24.000 --> 01:37:25.000]   This is like perfect timing.
[01:37:25.000 --> 01:37:28.400]   I'm throwing a, I hope my wife does not listen to this.
[01:37:28.400 --> 01:37:29.400]   We can Google.
[01:37:29.400 --> 01:37:36.040]   I'm throwing a surprise birthday party cocktail party for her for her birthday next week.
[01:37:36.040 --> 01:37:40.480]   And I need to come up with a couple of like cocktails to make sure we have the right stuff
[01:37:40.480 --> 01:37:41.480]   for.
[01:37:41.480 --> 01:37:42.480]   So this is going on the list.
[01:37:42.480 --> 01:37:44.360]   There you go.
[01:37:44.360 --> 01:37:45.360]   Thank you.
[01:37:45.360 --> 01:37:46.360]   Perfect timing.
[01:37:46.360 --> 01:37:47.920]   It's like you knew.
[01:37:47.920 --> 01:37:48.920]   It's like my birthday.
[01:37:48.920 --> 01:37:52.720]   It's as if your birthday was this month too.
[01:37:52.720 --> 01:37:53.720]   Cool.
[01:37:53.720 --> 01:37:54.720]   Thank you.
[01:37:54.720 --> 01:37:55.720]   Thank you.
[01:37:55.720 --> 01:37:58.440]   Stacey Jeff, what you got for your number?
[01:37:58.440 --> 01:38:00.280]   More is the number more.
[01:38:00.280 --> 01:38:06.600]   So there is now appears to be fairly confirmed reports that the pixel group has new laptops
[01:38:06.600 --> 01:38:09.880]   and tablets in bounds as the verb.
[01:38:09.880 --> 01:38:10.880]   Oh, really?
[01:38:10.880 --> 01:38:11.880]   Yeah.
[01:38:11.880 --> 01:38:14.960]   Because when they, when they lost some people and so on, there was, there was, there was
[01:38:14.960 --> 01:38:19.480]   presumption it was going to be killed, but Google confirmed to the verge that no, it has
[01:38:19.480 --> 01:38:21.200]   more coming this way.
[01:38:21.200 --> 01:38:22.200]   Interesting.
[01:38:22.200 --> 01:38:25.640]   It's very happy, very happy indeed.
[01:38:25.640 --> 01:38:29.360]   It's time for my, how old is the, is the latest pixel book?
[01:38:29.360 --> 01:38:31.800]   Like I think I've had this for two years now.
[01:38:31.800 --> 01:38:32.800]   Well, yeah.
[01:38:32.800 --> 01:38:36.920]   So the pixel slate, which is what I'm using is last year's last October.
[01:38:36.920 --> 01:38:39.320]   And so the pixel book is the October before.
[01:38:39.320 --> 01:38:42.160]   So I'm hoping IO they might announce a few things.
[01:38:42.160 --> 01:38:43.160]   That'd be nice.
[01:38:43.160 --> 01:38:45.160]   Man, I, you know, I wouldn't be surprised.
[01:38:45.160 --> 01:38:49.840]   It would be, it would be an interesting reversal if they had some hardware announcements of
[01:38:49.840 --> 01:38:50.840]   some of some sort.
[01:38:50.840 --> 01:38:54.000]   It would be, you have to do it, but they have it.
[01:38:54.000 --> 01:38:55.000]   Wow.
[01:38:55.000 --> 01:38:56.000]   Yeah.
[01:38:56.000 --> 01:38:57.800]   And I mean, we've got the pixel three, a kind of on the horizon.
[01:38:57.800 --> 01:39:01.400]   There's rumors about a galaxy or a pixel watch.
[01:39:01.400 --> 01:39:02.400]   Who knows?
[01:39:02.400 --> 01:39:03.400]   I don't know.
[01:39:03.400 --> 01:39:05.960]   I think a part of me would be pretty surprised if they had a hardware announcement at IO,
[01:39:05.960 --> 01:39:10.200]   but maybe around that time, you know, maybe they spread it out a little bit, maybe sometime
[01:39:10.200 --> 01:39:12.000]   soon.
[01:39:12.000 --> 01:39:13.000]   So interesting.
[01:39:13.000 --> 01:39:14.560]   Okay.
[01:39:14.560 --> 01:39:16.360]   I hadn't heard that news.
[01:39:16.360 --> 01:39:18.280]   That was news to me.
[01:39:18.280 --> 01:39:24.800]   My pick is an app that I discovered probably about three weeks ago.
[01:39:24.800 --> 01:39:25.800]   I mean, I'd heard of it.
[01:39:25.800 --> 01:39:29.080]   I'd heard of all trails before, but I wanted to find a trail.
[01:39:29.080 --> 01:39:33.080]   I wanted to leave my wife and I had like a date day and I was like, well, why don't
[01:39:33.080 --> 01:39:37.280]   we go for a hike and make it a photo hike or we'll take pictures and, you know, just
[01:39:37.280 --> 01:39:39.400]   hike along, whatever, but I don't know where to take us.
[01:39:39.400 --> 01:39:40.640]   Where are we going to go?
[01:39:40.640 --> 01:39:44.760]   So all trails is a really cool app that I've been kind of giving you.
[01:39:44.760 --> 01:39:45.760]   I've used that.
[01:39:45.760 --> 01:39:46.760]   It is awesome.
[01:39:46.760 --> 01:39:47.760]   Yes.
[01:39:47.760 --> 01:39:49.520]   It is a great app.
[01:39:49.520 --> 01:39:56.000]   Now, so there is a free version or 2999 a year, which 2999 a year, if you plan on using
[01:39:56.000 --> 01:39:57.760]   this regularly is probably a good thing.
[01:39:57.760 --> 01:40:02.000]   It allows you to do offline maps and a number of other, a number of other features.
[01:40:02.000 --> 01:40:03.000]   Yes.
[01:40:03.000 --> 01:40:04.000]   It gives you elevation.
[01:40:04.000 --> 01:40:06.440]   Like I get very excited when I'm hiking a mountain.
[01:40:06.440 --> 01:40:08.640]   It's like you've hiked this high and you're like, oh, yeah.
[01:40:08.640 --> 01:40:09.640]   Yeah.
[01:40:09.640 --> 01:40:11.880]   Yeah, but offline maps are super important.
[01:40:11.880 --> 01:40:13.080]   Yes, absolutely.
[01:40:13.080 --> 01:40:17.740]   So it'll, it'll GPS give you GPS directions to the trailhead because sometimes trails
[01:40:17.740 --> 01:40:20.720]   don't begin, you know, where a road is or whatever.
[01:40:20.720 --> 01:40:25.400]   You can track your route with GPS, view your day, download for offline, of course, follow
[01:40:25.400 --> 01:40:28.600]   other people's new routes or alterations to routes.
[01:40:28.600 --> 01:40:31.480]   So like this is Helen Putnam, outer loop here in Petaluma.
[01:40:31.480 --> 01:40:35.300]   You get a little description gives you these tags as far as like what you could expect
[01:40:35.300 --> 01:40:36.760]   if it's wildflower season.
[01:40:36.760 --> 01:40:39.560]   You know, you can isolate to those tags.
[01:40:39.560 --> 01:40:43.800]   And then it gives you a view on a map of the actual trail.
[01:40:43.800 --> 01:40:47.320]   And you can see other people who have taken these trails and modified them.
[01:40:47.320 --> 01:40:51.940]   So you can choose to follow their modified kind of direction on those trails if they
[01:40:51.940 --> 01:40:54.860]   may be extended it by a couple of miles or whatever.
[01:40:54.860 --> 01:40:56.540]   You get your elevation.
[01:40:56.540 --> 01:40:58.180]   Of course, I could download it.
[01:40:58.180 --> 01:40:59.580]   But I just thought it was really cool.
[01:40:59.580 --> 01:41:03.460]   And of course, like, you know, people will give their reviews and you can do the same.
[01:41:03.460 --> 01:41:08.380]   So I've found this really helpful because here in the Bay Area, it's been very wet.
[01:41:08.380 --> 01:41:12.340]   And so when you go hiking, you run the risk of running into very muddy areas.
[01:41:12.340 --> 01:41:16.900]   And you can see in here, for example, one week ago, four stars hiking.
[01:41:16.900 --> 01:41:20.980]   You know, they might mention, you know, beyond alert for poison, oak and ticks.
[01:41:20.980 --> 01:41:22.740]   But a month ago, it was really muddy.
[01:41:22.740 --> 01:41:25.320]   And I remember that because that's about the time that we went hiking.
[01:41:25.320 --> 01:41:26.320]   It was super muddy.
[01:41:26.320 --> 01:41:28.900]   So you can kind of get check in on it and be like, well, what are the conditions of
[01:41:28.900 --> 01:41:31.020]   this particular trail right now?
[01:41:31.020 --> 01:41:36.780]   And like I'll go into my history and show this was set the 17th we went to on a hike
[01:41:36.780 --> 01:41:38.220]   in Nevada.
[01:41:38.220 --> 01:41:41.300]   And it, you know, it tracked my whole hike.
[01:41:41.300 --> 01:41:44.540]   It also, I took a couple of photos along the way.
[01:41:44.540 --> 01:41:51.420]   So it drops a pinpoint where you take a photo and associates it with that point on the map.
[01:41:51.420 --> 01:41:57.060]   And it's just a cool app for if you want to go hiking or maybe you want to go mountain
[01:41:57.060 --> 01:42:03.860]   biking or running, you can filter for like dog friendly, kid friendly, wheelchair accessible,
[01:42:03.860 --> 01:42:05.500]   all that stuff.
[01:42:05.500 --> 01:42:07.020]   And it's just really useful.
[01:42:07.020 --> 01:42:09.140]   So it's called all trails.
[01:42:09.140 --> 01:42:13.180]   And I'm sure there's an iOS version of this, probably one of those, you know, cross platform
[01:42:13.180 --> 01:42:16.100]   apps, but I really like the Android app a lot.
[01:42:16.100 --> 01:42:18.500]   And you know, get out in the wilderness a little bit.
[01:42:18.500 --> 01:42:21.020]   Why don't you?
[01:42:21.020 --> 01:42:22.020]   But that is it.
[01:42:22.020 --> 01:42:26.900]   I think we've reached the end and yeah, there's a lot of fun.
[01:42:26.900 --> 01:42:29.060]   As usual, I love doing a show with you too.
[01:42:29.060 --> 01:42:31.140]   Thank you so much for letting me crash a party.
[01:42:31.140 --> 01:42:32.140]   Appreciate it.
[01:42:32.140 --> 01:42:33.140]   Likewise.
[01:42:33.140 --> 01:42:37.140]   And I like how you took my cocktail that's bad for you and you countered with something
[01:42:37.140 --> 01:42:38.780]   that's for you.
[01:42:38.780 --> 01:42:39.860]   This is what you do.
[01:42:39.860 --> 01:42:42.460]   Do you do the trail the day before the day after?
[01:42:42.460 --> 01:42:43.980]   What's the proper order?
[01:42:43.980 --> 01:42:47.300]   I would do it before and then have your verdant lady.
[01:42:47.300 --> 01:42:48.300]   That's just me.
[01:42:48.300 --> 01:42:49.300]   All right.
[01:42:49.300 --> 01:42:50.300]   That makes a lot of sense.
[01:42:50.300 --> 01:42:58.180]   Stacy, of course, IOT podcast.com Stacy on IOT.com at gigastacy on Twitter.
[01:42:58.180 --> 01:43:00.180]   Am I missing anything?
[01:43:00.180 --> 01:43:01.180]   Stacy.
[01:43:01.180 --> 01:43:03.020]   No, squeak away.
[01:43:03.020 --> 01:43:06.460]   She get there a little bit of no, it was open.
[01:43:06.460 --> 01:43:07.460]   It was it was not the right.
[01:43:07.460 --> 01:43:10.460]   It was a profit.
[01:43:10.460 --> 01:43:12.900]   Well, Stacy, thank you so much.
[01:43:12.900 --> 01:43:13.900]   Appreciate it.
[01:43:13.900 --> 01:43:16.740]   And you're moving in two weeks, right?
[01:43:16.740 --> 01:43:19.180]   Well, we hopefully close on my birthday.
[01:43:19.180 --> 01:43:21.180]   It's two weeks late on the close.
[01:43:21.180 --> 01:43:24.900]   So next week, if I'm gone, it's because I am packing and I will move on the following
[01:43:24.900 --> 01:43:25.900]   Monday.
[01:43:25.900 --> 01:43:28.580]   Can't wait to see your new studio.
[01:43:28.580 --> 01:43:32.980]   No, I'm moving to Austin and then I'm moving to Seattle a month after that.
[01:43:32.980 --> 01:43:33.980]   It's a two-step.
[01:43:33.980 --> 01:43:35.980]   Texas two-step move.
[01:43:35.980 --> 01:43:36.980]   Okay.
[01:43:36.980 --> 01:43:39.020]   A lot of earthen ladies in my future.
[01:43:39.020 --> 01:43:42.420]   You got a lot of change happening in your life right now.
[01:43:42.420 --> 01:43:44.180]   Stacy change law.
[01:43:44.180 --> 01:43:45.180]   Okay.
[01:43:45.180 --> 01:43:47.100]   Oh, Carson, go, go.
[01:43:47.100 --> 01:43:48.100]   Okay.
[01:43:48.100 --> 01:43:49.100]   Thank you, Stacy.
[01:43:49.100 --> 01:43:50.100]   All right.
[01:43:50.100 --> 01:43:52.260]   And Jeff, always so always so much fun to get the chance to chat with you.
[01:43:52.260 --> 01:43:55.540]   I'm sure I'm going to see you at Google I/O here next month, right?
[01:43:55.540 --> 01:43:56.540]   Yes, exactly.
[01:43:56.540 --> 01:44:02.140]   And I want to thank again, Metro Co-work here at Rochester, who were kind enough to let
[01:44:02.140 --> 01:44:06.260]   me broadcast for the facilities.
[01:44:06.260 --> 01:44:10.260]   Yes, in about every room that exists there because I think you were going to be.
[01:44:10.260 --> 01:44:11.900]   I feel like they actually asked me to come back in here.
[01:44:11.900 --> 01:44:15.900]   So yeah, but I was going to say that.
[01:44:15.900 --> 01:44:17.140]   Jeff, thank you so much.
[01:44:17.140 --> 01:44:24.220]   Buzzmachine.com to follow everything that Jeff is doing as he gallivants around the world.
[01:44:24.220 --> 01:44:29.700]   And a farewell again to Divya Mystery, who was here, who was in a low as a fan, happened
[01:44:29.700 --> 01:44:30.700]   to be just in the room.
[01:44:30.700 --> 01:44:31.700]   So I think it's so cool.
[01:44:31.700 --> 01:44:32.700]   Absolutely.
[01:44:32.700 --> 01:44:33.700]   Love it.
[01:44:33.700 --> 01:44:34.700]   Give you a hug.
[01:44:34.700 --> 01:44:35.700]   Thank you so much.
[01:44:35.700 --> 01:44:43.860]   We do the show every Wednesday, 4 p.m. Eastern, 1 p.m. Pacific, 20, 100, 200 UTC.
[01:44:43.860 --> 01:44:46.660]   You can go to twit.tv/twig.
[01:44:46.660 --> 01:44:48.660]   That's the show page for this week in Google there.
[01:44:48.660 --> 01:44:54.540]   You'll find all of the previous episodes, links to subscribe to audio, to video, pretty
[01:44:54.540 --> 01:44:56.900]   much everything that you need to know is listed there.
[01:44:56.900 --> 01:45:00.020]   Show notes, all that stuff for this week in Google.
[01:45:00.020 --> 01:45:01.580]   I'm just now filling in for Leo.
[01:45:01.580 --> 01:45:03.540]   Leo will be back next week.
[01:45:03.540 --> 01:45:10.860]   So you will see him sitting in his bigger chair, his more impressive chair at this set.
[01:45:10.860 --> 01:45:12.900]   But I look forward to the next time that I can hop on.
[01:45:12.900 --> 01:45:16.620]   Thanks again for welcoming me to the show and allowing me to be a part of it because
[01:45:16.620 --> 01:45:19.380]   I really do have a great time doing the show with these two.
[01:45:19.380 --> 01:45:20.380]   It's just a lot of fun.
[01:45:20.380 --> 01:45:22.340]   That is it for this week.
[01:45:22.340 --> 01:45:23.700]   The dog's saying it's time to go.
[01:45:23.700 --> 01:45:28.620]   So we'll see you next week and have a wonderful week and hope you get to your party, Carsten.
[01:45:28.620 --> 01:45:29.620]   Is it a party?
[01:45:29.620 --> 01:45:31.620]   I don't know if it's a party, but farewell.
[01:45:31.620 --> 01:45:32.620]   Bye.
[01:45:32.620 --> 01:45:42.620]   [MUSIC]

