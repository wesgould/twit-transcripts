;FFMETADATA1
title=Never Hug an Elmo
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=510
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.000]   It's time for Twig this week at Google Matthew Ingram and Jeff Jarvis is here.
[00:00:04.000 --> 00:00:10.000]   We answer some eternal life's eternal questions is Google a white collar sweatshop,
[00:00:10.000 --> 00:00:16.000]   Google's cold fusion experiment fizzles, and Elizabeth Warren's big billboard.
[00:00:16.000 --> 00:00:18.000]   It's all coming up next on Twig.
[00:00:18.000 --> 00:00:24.000]   Netcast you love from people you trust.
[00:00:28.000 --> 00:00:30.000]   This is Twig.
[00:00:30.000 --> 00:00:42.000]   This is Twig. This week in Google, episode 510, recorded Wednesday, May 29th, 2019.
[00:00:42.000 --> 00:00:44.000]   Never hug an Elmo.
[00:00:44.000 --> 00:00:47.000]   This week in Google is brought to you by Captera.
[00:00:47.000 --> 00:00:50.000]   Find the right tools to make an informed software decision for your business.
[00:00:50.000 --> 00:00:53.000]   Captera is software selection, simplify.
[00:00:53.000 --> 00:00:58.000]   Visit Captera's free website at keptera.com/twig.
[00:00:58.000 --> 00:01:03.000]   And by ExpressVPN. Protect your online privacy with one click.
[00:01:03.000 --> 00:01:09.000]   It's that easy. For three extra months free with a one-year package go to expressvpn.com/twig.
[00:01:09.000 --> 00:01:15.000]   And by Grammarly. Grammarly is a communication tool that helps people improve their writing
[00:01:15.000 --> 00:01:18.000]   so it's mistake free, clear, and effective.
[00:01:18.000 --> 00:01:25.000]   Start writing confidently by going to Grammarly.com/twig and get 20% off a Grammarly premium account today.
[00:01:25.000 --> 00:01:30.000]   It's time for Twig. This week in Google to show we cover the Google verse which means everything,
[00:01:30.000 --> 00:01:34.000]   including Facebook, Twitter, and the President's tweets.
[00:01:34.000 --> 00:01:37.000]   Speaking of the President's tweets, here's Jeff Jarvis.
[00:01:37.000 --> 00:01:39.000]   He is a professor.
[00:01:39.000 --> 00:01:43.000]   How did Ari Melber put it last night? He said it perfectly, I thought.
[00:01:43.000 --> 00:01:48.000]   Professor of Journalism at the Craig Newmark Graduate School of Journalism. Something like that.
[00:01:48.000 --> 00:01:50.000]   Yeah, he didn't do the whole...
[00:01:50.000 --> 00:01:51.000]   He got a right.
[00:01:51.000 --> 00:01:52.000]   ...Tow Professor for...
[00:01:52.000 --> 00:01:53.000]   Yeah, I didn't call you...
[00:01:53.000 --> 00:01:54.000]   The whole segment, you know, over.
[00:01:54.000 --> 00:01:58.000]   Yeah, and it didn't call you former TV guide critic either.
[00:01:58.000 --> 00:02:01.000]   That was somebody messing with you in the control room. I swear to God.
[00:02:01.000 --> 00:02:02.000]   Yeah.
[00:02:02.000 --> 00:02:05.000]   Or just... I'm on the list of... Who the hell is this guy? I don't know.
[00:02:05.000 --> 00:02:06.000]   Who did you get to get to the guy?
[00:02:06.000 --> 00:02:07.000]   Yeah, yeah.
[00:02:07.000 --> 00:02:10.000]   Look him up. Google him. What does it say?
[00:02:10.000 --> 00:02:13.000]   Jeff is always welcomebuzzmachine.com. What would Google do?
[00:02:13.000 --> 00:02:16.000]   The books, many of them at Jeff Jarvis on Twitter.
[00:02:16.000 --> 00:02:22.000]   Also with us from the Columbia Journalism Review, where he's Chief Digital Writer, Matthew Ingram.
[00:02:22.000 --> 00:02:28.000]   Great to have you, CJR.org and M-A-T-H-E-W-I on Twitter.
[00:02:28.000 --> 00:02:32.000]   I always get confused, Matthew. What's the... Where do you get in trouble in the UK with the two fingers?
[00:02:32.000 --> 00:02:33.000]   Which way?
[00:02:33.000 --> 00:02:34.000]   Oh, yeah.
[00:02:34.000 --> 00:02:35.000]   So, yeah, the universe...
[00:02:35.000 --> 00:02:36.000]   Churchill first did this, right?
[00:02:36.000 --> 00:02:38.000]   The universe was okay.
[00:02:38.000 --> 00:02:39.000]   No, that's not okay.
[00:02:39.000 --> 00:02:40.000]   That's not okay.
[00:02:40.000 --> 00:02:41.000]   That's not okay.
[00:02:41.000 --> 00:02:42.000]   So, he did this first.
[00:02:42.000 --> 00:02:43.000]   Oh.
[00:02:43.000 --> 00:02:45.000]   And then that means up your kilt.
[00:02:45.000 --> 00:02:46.000]   Yeah.
[00:02:46.000 --> 00:02:53.500]   And then one of... This was the best part of the darkest hour was when the Secretary's
[00:02:53.500 --> 00:02:59.000]   titles up to him and says, "Mr. Churchill, I do not think that means what you think it means."
[00:02:59.000 --> 00:03:01.000]   And from then on, he did it that way.
[00:03:01.000 --> 00:03:02.000]   Yeah.
[00:03:02.000 --> 00:03:08.000]   V for victory. We were doing V for victory backwards for those just listening. Backwards is wrong.
[00:03:08.000 --> 00:03:09.000]   Forward says right.
[00:03:09.000 --> 00:03:10.000]   Your hand shatters.
[00:03:10.000 --> 00:03:12.000]   The peace sign. You know the peace sign.
[00:03:12.000 --> 00:03:13.000]   No, yeah.
[00:03:13.000 --> 00:03:19.280]   Hey, I want to... Before we get into the Google stuff, today is the 100th anniversary of a
[00:03:19.280 --> 00:03:27.360]   very seminal event in world history. In 1919, on this day, there was a total eclipse of the
[00:03:27.360 --> 00:03:33.120]   sun. But the reason it's important in world history is because observations made during
[00:03:33.120 --> 00:03:38.120]   that total eclipse confirmed Einstein's theory of relativity.
[00:03:38.120 --> 00:03:39.120]   Oh.
[00:03:39.120 --> 00:03:40.120]   Phantom famous.
[00:03:40.120 --> 00:03:42.120]   And it literally made him famous.
[00:03:42.120 --> 00:03:43.120]   Yep.
[00:03:43.120 --> 00:03:45.120]   Because until then it was like, "Oh, yeah, sure."
[00:03:45.120 --> 00:03:47.120]   He was a patent clerk.
[00:03:47.120 --> 00:03:48.120]   Yeah.
[00:03:48.120 --> 00:03:56.240]   Oh, that owl. He's always saying crazy things. But imagine, I mean, 1919, they proved that
[00:03:56.240 --> 00:04:02.480]   based on measurements from two different places on the earth of the eclipse, they proved that
[00:04:02.480 --> 00:04:03.480]   relativity was correct.
[00:04:03.480 --> 00:04:05.480]   It was the bending of the light.
[00:04:05.480 --> 00:04:06.480]   Yep.
[00:04:06.480 --> 00:04:07.480]   Light bending.
[00:04:07.480 --> 00:04:08.480]   Pretty neat.
[00:04:08.480 --> 00:04:09.480]   Yeah.
[00:04:09.480 --> 00:04:14.320]   Show that graph, Karsten, a little bit. Keep going. There you go. So the apparent location
[00:04:14.320 --> 00:04:18.240]   of the star was different than the actual location of the star. Because the sun bent
[00:04:18.240 --> 00:04:22.600]   the light, you wouldn't have been able to see that if you hadn't had the sun obscured,
[00:04:22.600 --> 00:04:25.440]   which of course, in its corona, the light.
[00:04:25.440 --> 00:04:27.360]   Jeff, you have a reference to the relativity?
[00:04:27.360 --> 00:04:32.080]   Well, he predicted light bending. He predicted that gravity would have been light.
[00:04:32.080 --> 00:04:33.080]   It would have been the light.
[00:04:33.080 --> 00:04:34.080]   Mm-hmm.
[00:04:34.080 --> 00:04:36.320]   And everybody said, "Come on, owl."
[00:04:36.320 --> 00:04:37.320]   That's crazy.
[00:04:37.320 --> 00:04:38.320]   That's crazy.
[00:04:38.320 --> 00:04:40.320]   That's crazy, talk.
[00:04:40.320 --> 00:04:43.080]   Weird owl, Einstein, they call them up to that pit route.
[00:04:43.080 --> 00:04:45.080]   You saw that one, right, Jeff?
[00:04:45.080 --> 00:04:48.200]   You remember that?
[00:04:48.200 --> 00:04:51.600]   I think you probably wrote it up in whatever the...
[00:04:51.600 --> 00:04:54.840]   The Tribune, the New York Herald Tribune.
[00:04:54.840 --> 00:05:00.280]   All right. Now we get on to the modern times. A hundred years later, nobody doubts Einstein,
[00:05:00.280 --> 00:05:06.600]   but everybody doubts global warming. Meanwhile, tornadoes ravage the Midwest. And you in
[00:05:06.600 --> 00:05:08.400]   New York, well, I don't know if you got it in New Jersey.
[00:05:08.400 --> 00:05:11.760]   It's coming through in about four minutes, according to Darkskine.
[00:05:11.760 --> 00:05:13.560]   We're going to get an extorb.
[00:05:13.560 --> 00:05:18.200]   So first time I can remember New York City getting a tornado warning.
[00:05:18.200 --> 00:05:22.440]   And we just did Windows, finished Windows Weekly, Paul Therat, who's in the lower Macungee
[00:05:22.440 --> 00:05:27.640]   Valley of New Jersey near Allentown, was on a standing alert during the entire show that
[00:05:27.640 --> 00:05:28.880]   a tornado could pass through.
[00:05:28.880 --> 00:05:29.880]   Really? Yeah.
[00:05:29.880 --> 00:05:30.880]   Wow.
[00:05:30.880 --> 00:05:33.880]   And of course, many died in Alabama yesterday.
[00:05:33.880 --> 00:05:34.880]   Oh, it was a lot.
[00:05:34.880 --> 00:05:35.880]   The jury devastated.
[00:05:35.880 --> 00:05:45.160]   Did you see the clip of the weather man who was checking social media and people were
[00:05:45.160 --> 00:05:46.160]   complaining that...
[00:05:46.160 --> 00:05:47.160]   They were pissed.
[00:05:47.160 --> 00:05:48.160]   They said, "Don't...
[00:05:48.160 --> 00:05:49.160]   We're watching basketball."
[00:05:49.160 --> 00:05:50.160]   The Bachelorette.
[00:05:50.160 --> 00:05:51.160]   Oh, it was the Bachelorette.
[00:05:51.160 --> 00:05:52.160]   Bachelorette.
[00:05:52.160 --> 00:05:53.160]   Yeah.
[00:05:53.160 --> 00:05:54.160]   And he...
[00:05:54.160 --> 00:05:55.760]   So he was checking social media.
[00:05:55.760 --> 00:05:57.360]   He did a split screen, right?
[00:05:57.360 --> 00:05:58.840]   No, that was the other guy.
[00:05:58.840 --> 00:06:01.160]   Oh, this was a true report for that.
[00:06:01.160 --> 00:06:05.000]   But he said he was checking social media and he said, "We're getting complaints.
[00:06:05.000 --> 00:06:06.160]   Can't we go back to the show?"
[00:06:06.160 --> 00:06:07.160]   And he just lost it.
[00:06:07.160 --> 00:06:09.280]   He was like, "No, we can't go back to the show.
[00:06:09.280 --> 00:06:10.280]   People are going to die."
[00:06:10.280 --> 00:06:11.280]   The same as what?
[00:06:11.280 --> 00:06:12.280]   The same as what?
[00:06:12.280 --> 00:06:13.280]   The tornado.
[00:06:13.280 --> 00:06:14.280]   But this was the show some other time.
[00:06:14.280 --> 00:06:15.280]   This was Paul's reaction too, as well...
[00:06:15.280 --> 00:06:16.280]   I don't know.
[00:06:16.280 --> 00:06:17.280]   I'm going to keep you in the show.
[00:06:17.280 --> 00:06:19.040]   I'm saying, "Paul, go to the basement."
[00:06:19.040 --> 00:06:21.000]   Whether when you snap when you live...
[00:06:21.000 --> 00:06:25.600]   When you live in those places, maybe you get used to it.
[00:06:25.600 --> 00:06:27.160]   And so you think, "So what?"
[00:06:27.160 --> 00:06:28.640]   It's not that big a deal.
[00:06:28.640 --> 00:06:31.160]   Probably nine times out of ten, you get a tornado warning.
[00:06:31.160 --> 00:06:32.160]   Nothing happens.
[00:06:32.160 --> 00:06:33.800]   It's pretty good video, by the way.
[00:06:33.800 --> 00:06:34.800]   Shall we play that?
[00:06:34.800 --> 00:06:36.160]   Yeah, yeah, it's fun.
[00:06:36.160 --> 00:06:37.160]   Let's roll the clip.
[00:06:37.160 --> 00:06:38.160]   He goes up.
[00:06:38.160 --> 00:06:39.160]   Carsten.
[00:06:39.160 --> 00:06:40.160]   Can you do that?
[00:06:40.160 --> 00:06:43.120]   Do you have the capability or should I do it?
[00:06:43.120 --> 00:06:45.840]   Something is technically Twitter failed us.
[00:06:45.840 --> 00:06:47.080]   That's not a good sign.
[00:06:47.080 --> 00:06:48.080]   Twitter has failed us.
[00:06:48.080 --> 00:06:51.520]   I haven't seen a Twitter error message in a long time.
[00:06:51.520 --> 00:06:52.520]   That's...
[00:06:52.520 --> 00:06:55.320]   Yeah, where's the whale?
[00:06:55.320 --> 00:06:57.720]   But that's our friend.
[00:06:57.720 --> 00:06:58.720]   Ming, what's her name?
[00:06:58.720 --> 00:06:59.680]   I forgot what her name is.
[00:06:59.680 --> 00:07:02.760]   But I have one of her illustrations.
[00:07:02.760 --> 00:07:05.240]   She was the one who created the fail whale.
[00:07:05.240 --> 00:07:06.240]   Yeah.
[00:07:06.240 --> 00:07:08.960]   This heavy rain is between you and your tomato.
[00:07:08.960 --> 00:07:10.760]   That's why this was a dangerous situation tonight.
[00:07:10.760 --> 00:07:11.760]   It's dark.
[00:07:11.760 --> 00:07:12.760]   It's also rainy.
[00:07:12.760 --> 00:07:13.760]   I was just taking social media.
[00:07:13.760 --> 00:07:15.640]   We have viewers complaining already.
[00:07:15.640 --> 00:07:16.800]   Just go back to the show.
[00:07:16.800 --> 00:07:18.800]   No, we're not going back to the show, folks.
[00:07:18.800 --> 00:07:21.160]   This is a dangerous situation, okay?
[00:07:21.160 --> 00:07:22.160]   It's nice to see you.
[00:07:22.160 --> 00:07:23.640]   It's not just your neighborhood.
[00:07:23.640 --> 00:07:26.600]   I'm sick and tired of people complaining about this.
[00:07:26.600 --> 00:07:30.440]   Our job here is to keep people safe and that is what we're going to do.
[00:07:30.440 --> 00:07:33.240]   Some of you complain that this is all about my ego.
[00:07:33.240 --> 00:07:34.240]   Stop!
[00:07:34.240 --> 00:07:35.800]   Okay, just stop right now.
[00:07:35.800 --> 00:07:36.800]   It's not.
[00:07:36.800 --> 00:07:37.800]   Twitter gets to everybody eventually.
[00:07:37.800 --> 00:07:38.800]   I've done with you people.
[00:07:38.800 --> 00:07:39.800]   I really am.
[00:07:39.800 --> 00:07:41.440]   This is for the week now.
[00:07:41.440 --> 00:07:42.440]   Yeah.
[00:07:42.440 --> 00:07:43.440]   Sit, dangerous situation here.
[00:07:43.440 --> 00:07:44.440]   ABC 13, WSC T.
[00:07:44.440 --> 00:07:45.440]   I'm sorry I did that.
[00:07:45.440 --> 00:07:46.440]   I'm just...
[00:07:46.440 --> 00:07:51.080]   It just really bothers me that we have people that don't care about other people's safety
[00:07:51.080 --> 00:07:52.080]   around here.
[00:07:52.080 --> 00:07:53.080]   That's just ridiculous.
[00:07:53.080 --> 00:07:55.080]   New tornado warning here.
[00:07:55.080 --> 00:07:56.080]   Wow.
[00:07:56.080 --> 00:07:57.080]   People were done.
[00:07:57.080 --> 00:07:58.080]   People were done.
[00:07:58.080 --> 00:07:59.080]   It was serious.
[00:07:59.080 --> 00:08:00.080]   Very serious.
[00:08:00.080 --> 00:08:01.080]   Yeah.
[00:08:01.080 --> 00:08:03.080]   But there was another weatherman, Bachelorette.
[00:08:03.080 --> 00:08:08.720]   I'm trying to remember where it was, but he was actually fired for breaking into a football
[00:08:08.720 --> 00:08:11.320]   game with a weather alert.
[00:08:11.320 --> 00:08:12.320]   Wow.
[00:08:12.320 --> 00:08:13.320]   Yeah.
[00:08:13.320 --> 00:08:16.280]   Well, Jeff and I are old enough to remember the Heidi incident.
[00:08:16.280 --> 00:08:17.280]   Oh yeah?
[00:08:17.280 --> 00:08:18.280]   Heidi.
[00:08:18.280 --> 00:08:22.080]   That changed television forever.
[00:08:22.080 --> 00:08:23.080]   Mm-hmm.
[00:08:23.080 --> 00:08:24.080]   Mm-hmm.
[00:08:24.080 --> 00:08:27.760]   Trying to say that with a serious face, but it did.
[00:08:27.760 --> 00:08:31.760]   The networks had scheduled a running of Heidi during...
[00:08:31.760 --> 00:08:33.800]   Was it a Raiders game?
[00:08:33.800 --> 00:08:37.600]   Like the Swiss, you know, cattle and bird rotors.
[00:08:37.600 --> 00:08:38.600]   Jets Raiders, 1968.
[00:08:38.600 --> 00:08:41.280]   You know, Jets fans are really mean.
[00:08:41.280 --> 00:08:46.000]   In the middle of the game, 1968.
[00:08:46.000 --> 00:08:48.080]   In the middle of the game...
[00:08:48.080 --> 00:08:49.080]   Heidi comes on.
[00:08:49.080 --> 00:08:52.520]   They're down jets.
[00:08:52.520 --> 00:08:54.520]   And they just break away.
[00:08:54.520 --> 00:09:00.960]   Let me jump ahead.
[00:09:00.960 --> 00:09:01.880]   Well, wait a minute.
[00:09:01.880 --> 00:09:03.520]   This is just the game.
[00:09:03.520 --> 00:09:05.440]   Oh, I get it.
[00:09:05.440 --> 00:09:09.680]   This happened after the network started broadcasting Heidi.
[00:09:09.680 --> 00:09:10.680]   Oh, right.
[00:09:10.680 --> 00:09:11.680]   That was a gap.
[00:09:11.680 --> 00:09:16.040]   So the Raiders score, but nobody saw it because they were watching some growing up.
[00:09:16.040 --> 00:09:18.040]   Nobody had internet to get wild.
[00:09:18.040 --> 00:09:20.040]   No other way of knowing.
[00:09:20.040 --> 00:09:21.760]   Absolutely the spirit.
[00:09:21.760 --> 00:09:23.520]   So people were furious.
[00:09:23.520 --> 00:09:25.040]   Oh, right.
[00:09:25.040 --> 00:09:27.400]   This was an AFL championship game.
[00:09:27.400 --> 00:09:29.280]   And of all things, Heidi.
[00:09:29.280 --> 00:09:33.320]   And then somebody in the YouTube comments says, "What the hell?
[00:09:33.320 --> 00:09:41.800]   I came here to watch Heidi."
[00:09:41.800 --> 00:09:47.480]   But after that, the networks were very loath to preempt anything for anything.
[00:09:47.480 --> 00:09:51.720]   The Heidi game, the world famous Heidi game.
[00:09:51.720 --> 00:09:52.920]   Wow.
[00:09:52.920 --> 00:09:54.440]   I was six, I think.
[00:09:54.440 --> 00:09:55.440]   I missed that.
[00:09:55.440 --> 00:09:56.440]   Yeah.
[00:09:56.440 --> 00:09:57.440]   I don't stop that.
[00:09:57.440 --> 00:09:59.720]   He's just warding it over us.
[00:09:59.720 --> 00:10:00.720]   Just stop it.
[00:10:00.720 --> 00:10:02.680]   You guys born in the 70s.
[00:10:02.680 --> 00:10:03.680]   The 60s.
[00:10:03.680 --> 00:10:04.680]   Whatever.
[00:10:04.680 --> 00:10:05.680]   What-evs.
[00:10:05.680 --> 00:10:07.680]   All right.
[00:10:07.680 --> 00:10:11.880]   Moving on, I'm sure there's actually some Google news in here.
[00:10:11.880 --> 00:10:12.880]   Somewhere.
[00:10:12.880 --> 00:10:13.880]   Google's gold.
[00:10:13.880 --> 00:10:17.400]   As long as we're talking science, Google had a...
[00:10:17.400 --> 00:10:18.880]   They spent a pittance on it.
[00:10:18.880 --> 00:10:21.440]   A cold fusion project.
[00:10:21.440 --> 00:10:27.600]   Spent 10 million, which is nothing in the world of cold fusion.
[00:10:27.600 --> 00:10:32.600]   They found no evidence whatsoever that fusion is possible at a real temperature.
[00:10:32.600 --> 00:10:33.600]   Pretty conclusive.
[00:10:33.600 --> 00:10:35.200]   It seems like they came up.
[00:10:35.200 --> 00:10:36.960]   Why would they do that?
[00:10:36.960 --> 00:10:37.960]   Yeah.
[00:10:37.960 --> 00:10:44.600]   In 2015, because there was research, somebody claimed a pair of sinus claims they...
[00:10:44.600 --> 00:10:45.600]   They were mainly pawns and...
[00:10:45.600 --> 00:10:48.040]   It's got a guy named Fleishman, I think.
[00:10:48.040 --> 00:10:49.040]   Oh, yeah.
[00:10:49.040 --> 00:10:50.040]   Fleishman member.
[00:10:50.040 --> 00:10:51.040]   Wow.
[00:10:51.040 --> 00:10:52.040]   You do have a good memory.
[00:10:52.040 --> 00:10:53.040]   1989.
[00:10:53.040 --> 00:10:54.040]   There's a news conference.
[00:10:54.040 --> 00:10:55.040]   I remember it.
[00:10:55.040 --> 00:10:56.040]   And they claim one at least.
[00:10:56.040 --> 00:10:58.240]   You were one, three years old than Matthew?
[00:10:58.240 --> 00:10:59.240]   Yeah.
[00:10:59.240 --> 00:11:01.400]   But he was interested in science by that time.
[00:11:01.400 --> 00:11:05.680]   It said we've solved the energy problems for the world.
[00:11:05.680 --> 00:11:07.640]   It's cold fusion is going to work.
[00:11:07.640 --> 00:11:09.680]   It worked in the lab.
[00:11:09.680 --> 00:11:11.320]   And then it was just completely repudiated.
[00:11:11.320 --> 00:11:18.320]   I was wondering what happened to those guys.
[00:11:18.320 --> 00:11:20.400]   Turn out to be a pawn scheme.
[00:11:20.400 --> 00:11:21.400]   A pawn scheme.
[00:11:21.400 --> 00:11:22.400]   Oh.
[00:11:22.400 --> 00:11:23.400]   Oh.
[00:11:23.400 --> 00:11:24.400]   So...
[00:11:24.400 --> 00:11:27.400]   Well, this is just basically rule fact checking is all.
[00:11:27.400 --> 00:11:28.400]   Yeah.
[00:11:28.400 --> 00:11:31.960]   Oddly, because it was debunked almost immediately.
[00:11:31.960 --> 00:11:35.960]   It wasn't like, oh, they're still wondering, can it be done?
[00:11:35.960 --> 00:11:40.880]   But they awarded $10 million, almost like an X price, to 30 researchers in labs around
[00:11:40.880 --> 00:11:45.720]   researchers published a dozen individual papers in Monday's nature.
[00:11:45.720 --> 00:11:52.640]   They sum up the findings of Google's investigation saying, no evidence whatsoever.
[00:11:52.640 --> 00:11:56.440]   But nature in an editorial said Google's investment wasn't for not.
[00:11:56.440 --> 00:12:01.960]   The project has produced materials, tools, and insights such as calorimeters that operate
[00:12:01.960 --> 00:12:07.520]   reliably under extreme conditions and techniques for producing and characterizing highly hybrided
[00:12:07.520 --> 00:12:12.360]   materials that could benefit other areas of energy and fusion research.
[00:12:12.360 --> 00:12:17.400]   But whether the spinoff benefits alone justify continued efforts in investment in pursuit
[00:12:17.400 --> 00:12:19.880]   of a probable pipe dream is another matter.
[00:12:19.880 --> 00:12:20.880]   Opinions are split.
[00:12:20.880 --> 00:12:26.200]   Now, kudos for Google, but $10 million to Google is nothing.
[00:12:26.200 --> 00:12:27.200]   Yeah.
[00:12:27.200 --> 00:12:31.240]   And I guess when you think about it, if they had actually found evidence, that would be
[00:12:31.240 --> 00:12:32.400]   pretty huge for Google.
[00:12:32.400 --> 00:12:34.200]   I mean, their power needs are massive.
[00:12:34.200 --> 00:12:35.200]   Oh, yeah.
[00:12:35.200 --> 00:12:36.200]   Oh, you think they actually...
[00:12:36.200 --> 00:12:37.200]   That was the heck.
[00:12:37.200 --> 00:12:38.200]   The reason is that...
[00:12:38.200 --> 00:12:43.240]   Maybe we could use cold fusion in our server room.
[00:12:43.240 --> 00:12:46.680]   Okay.
[00:12:46.680 --> 00:12:50.720]   There was a bit of a tempest in a teapot yesterday.
[00:12:50.720 --> 00:13:00.440]   We saw that Google's new YouTube was apparently blocking Microsoft's new chromium-based edge.
[00:13:00.440 --> 00:13:06.000]   And people found the offending string that said, if it's edge, don't play.
[00:13:06.000 --> 00:13:08.160]   Say your browser's not compliant.
[00:13:08.160 --> 00:13:13.200]   However, they were able to change the user age and in edge and get YouTube to work.
[00:13:13.200 --> 00:13:15.080]   So there was no technical reason.
[00:13:15.080 --> 00:13:20.640]   It apparently was a mistake because Google reversed the code and everything.
[00:13:20.640 --> 00:13:22.480]   And it does now work fine.
[00:13:22.480 --> 00:13:26.840]   So the question, a lot of people blame Google saying, "See, we told you Google would never
[00:13:26.840 --> 00:13:29.840]   allow Microsoft to base edge on chromium."
[00:13:29.840 --> 00:13:30.840]   No.
[00:13:30.840 --> 00:13:31.840]   No.
[00:13:31.840 --> 00:13:33.800]   Even Paul Therat said, "No, that's crazy.
[00:13:33.800 --> 00:13:34.800]   That was a mistake."
[00:13:34.800 --> 00:13:39.000]   It seemed like even if you were going to do that, you wouldn't be that obvious about
[00:13:39.000 --> 00:13:40.000]   it.
[00:13:40.000 --> 00:13:43.120]   You wouldn't just put a line in saying, "Don't play if it's edge."
[00:13:43.120 --> 00:13:44.120]   Right.
[00:13:44.120 --> 00:13:45.120]   It was kind of blatant.
[00:13:45.120 --> 00:13:46.120]   Yeah.
[00:13:46.120 --> 00:13:47.120]   That's pretty obvious.
[00:13:47.120 --> 00:13:48.960]   Easy to find, even.
[00:13:48.960 --> 00:13:51.320]   It was an unobscured JavaScript.
[00:13:51.320 --> 00:13:52.720]   So, okay.
[00:13:52.720 --> 00:13:56.800]   Big article in The New York Times about Google's use of contract employees.
[00:13:56.800 --> 00:14:02.480]   I wanted you to address this a little bit, Jeff, because it's been my experience.
[00:14:02.480 --> 00:14:05.800]   This is very common practice in all Silicon Valley companies.
[00:14:05.800 --> 00:14:09.920]   Microsoft doesn't, and many companies don't hire employees.
[00:14:09.920 --> 00:14:16.360]   They hire firms who hire employees, and those contract employees save them a lot of money.
[00:14:16.360 --> 00:14:19.600]   Although, this seems pretty egregious.
[00:14:19.600 --> 00:14:24.600]   About half of Google's employment is contract employees.
[00:14:24.600 --> 00:14:25.600]   Yeah.
[00:14:25.600 --> 00:14:26.600]   A few things about this.
[00:14:26.600 --> 00:14:27.600]   One is others do it.
[00:14:27.600 --> 00:14:28.600]   Right?
[00:14:28.600 --> 00:14:29.600]   Connie Nast did it.
[00:14:29.600 --> 00:14:31.280]   Vox has a union now.
[00:14:31.280 --> 00:14:34.520]   And there's a confuffle today about how they're hiring freelancers.
[00:14:34.520 --> 00:14:43.680]   The thing I don't know about the Google story is, if it's the person delivering your groceries,
[00:14:43.680 --> 00:14:49.440]   should that be the same permanence as a software engineer at Google?
[00:14:49.440 --> 00:14:54.560]   If it's the person who's answering customer service, and I don't mean to degrade their
[00:14:54.560 --> 00:15:00.280]   work, but to a Google, they're more interchangeable.
[00:15:00.280 --> 00:15:03.920]   Yeah, this doesn't sound much about saving money.
[00:15:03.920 --> 00:15:06.560]   It's about potentially what the relationship is.
[00:15:06.560 --> 00:15:10.920]   Now, there was a few years ago, security guards on campus were not Google employees.
[00:15:10.920 --> 00:15:14.160]   They were hired outside, and there was a crow-fuffle, and they were brought on staff.
[00:15:14.160 --> 00:15:19.480]   Well, let me just say something about that, because we have an armed guard, a security
[00:15:19.480 --> 00:15:20.480]   guard.
[00:15:20.480 --> 00:15:21.480]   Right.
[00:15:21.480 --> 00:15:22.480]   Mo, who's great, you met him.
[00:15:22.480 --> 00:15:23.480]   Yeah.
[00:15:23.480 --> 00:15:24.480]   Really great guy.
[00:15:24.480 --> 00:15:25.480]   Former Marine.
[00:15:25.480 --> 00:15:29.520]   Well-trained in the use of firearms, I might add.
[00:15:29.520 --> 00:15:32.840]   But we can't hire him, because our insurance would go through the roof, because he has
[00:15:32.840 --> 00:15:33.840]   a firearm.
[00:15:33.840 --> 00:15:34.840]   Right.
[00:15:34.840 --> 00:15:36.400]   And so we hire a security company.
[00:15:36.400 --> 00:15:38.360]   He works for the security company.
[00:15:38.360 --> 00:15:42.000]   I would like to have it in a Twit Cop uniform.
[00:15:42.000 --> 00:15:43.440]   I want a Twit Cop.
[00:15:43.440 --> 00:15:44.440]   I do.
[00:15:44.440 --> 00:15:45.440]   Yeah.
[00:15:45.440 --> 00:15:46.440]   That's a good idea.
[00:15:46.440 --> 00:15:48.040]   Maybe we should hire him just for that.
[00:15:48.040 --> 00:15:50.000]   I mean, it's to me.
[00:15:50.000 --> 00:15:51.000]   But that makes sense.
[00:15:51.000 --> 00:15:54.880]   But if I look at the numbers, this is the New York Times got an internal document, and
[00:15:54.880 --> 00:15:57.360]   it said, as of March, Google had 121,000.
[00:15:57.360 --> 00:15:58.360]   It's a hundred twenty-one thousand.
[00:15:58.360 --> 00:16:00.040]   Yeah, it's more than fifty percent.
[00:16:00.040 --> 00:16:06.200]   121,000 temps and contractors, this is globally, compared to a hundred two thousand full-time
[00:16:06.200 --> 00:16:07.200]   employees.
[00:16:07.200 --> 00:16:09.720]   So it's got to be more than just the security guard out front.
[00:16:09.720 --> 00:16:12.560]   Do you remember what Kevin did for them?
[00:16:12.560 --> 00:16:13.560]   I can't remember what he did.
[00:16:13.560 --> 00:16:16.360]   Kevin, Kevin Tofel was not an employee.
[00:16:16.360 --> 00:16:20.760]   And Kevin pointed out that when you're not an employee, it's a very weird position if
[00:16:20.760 --> 00:16:25.240]   you're in the Googleplex, because you can't eat in some of the cafeterias.
[00:16:25.240 --> 00:16:27.880]   You can't come to the all-hands meetings.
[00:16:27.880 --> 00:16:30.120]   Your access card only gets you into certain buildings.
[00:16:30.120 --> 00:16:32.680]   You're really a second-class citizen.
[00:16:32.680 --> 00:16:39.600]   So the question, if they have that many, I could see, like you said, the occasional
[00:16:39.600 --> 00:16:45.200]   position or maybe a few people in a division, but that's over fifty percent of their entire
[00:16:45.200 --> 00:16:46.200]   staff.
[00:16:46.200 --> 00:16:47.200]   Seems like a lot.
[00:16:47.200 --> 00:16:48.200]   And they don't get any benefits.
[00:16:48.200 --> 00:16:49.200]   They get paid less.
[00:16:49.200 --> 00:16:52.760]   I mean, that seems pretty egregious to me.
[00:16:52.760 --> 00:16:55.320]   What do you think, Jeff?
[00:16:55.320 --> 00:17:01.040]   Well, again, I go to a place like Connie Nast, where I work here.
[00:17:01.040 --> 00:17:03.760]   And the majority of the people who did work for Connie Nast were freelancers.
[00:17:03.760 --> 00:17:06.240]   I'm not saying that's right.
[00:17:06.240 --> 00:17:12.160]   And indeed, U.S. law is now such that I know when I try to hire people now, it is really
[00:17:12.160 --> 00:17:14.960]   hard to hire somebody not as an employee.
[00:17:14.960 --> 00:17:20.600]   I end up having to bring people on to staff for a four-week gig.
[00:17:20.600 --> 00:17:22.000]   Bring one on staff.
[00:17:22.000 --> 00:17:24.000]   I've got to pay benefits to the university.
[00:17:24.000 --> 00:17:29.560]   I've got to do all that stuff because you can't come on as a consultant or freelancer unless
[00:17:29.560 --> 00:17:31.560]   you have a business doing that.
[00:17:31.560 --> 00:17:37.480]   You can prove it with a client list and business cards and stationary and all kinds of stuff.
[00:17:37.480 --> 00:17:38.920]   That's an IRS regulation.
[00:17:38.920 --> 00:17:43.000]   The IRS has for years been cracking down on this practice because a lot of businesses
[00:17:43.000 --> 00:17:44.240]   avoided benefits.
[00:17:44.240 --> 00:17:46.000]   That was the chief reason to do it.
[00:17:46.000 --> 00:17:49.920]   So this is by hiring temps or contract employees.
[00:17:49.920 --> 00:17:55.600]   The New York Times story quotes someone saying you can save $100,000 a year per job.
[00:17:55.600 --> 00:17:57.440]   That depends what you're paying them.
[00:17:57.440 --> 00:18:00.480]   So you've got 121,000?
[00:18:00.480 --> 00:18:01.480]   That's a lot of money.
[00:18:01.480 --> 00:18:02.480]   Contractors.
[00:18:02.480 --> 00:18:03.580]   That's a lot of money.
[00:18:03.580 --> 00:18:10.200]   This reminds me a lot of the Dutch sandwich and the Irish wrap around or whatever those
[00:18:10.200 --> 00:18:11.360]   tax maneuvers are.
[00:18:11.360 --> 00:18:18.520]   They're not illegal that they're taking advantages of loophole in the law to make more money.
[00:18:18.520 --> 00:18:25.280]   And I guess on the one hand, especially publicly held corporations have a fiduciary responsibility
[00:18:25.280 --> 00:18:28.400]   of their shareholders to take advantage of these loopholes.
[00:18:28.400 --> 00:18:34.240]   Since they are legal, my tax attorney always used to say, I don't, we don't use them anymore.
[00:18:34.240 --> 00:18:44.720]   Leo, pay no, not a penny less than you owe and not a penny more than you should or whatever.
[00:18:44.720 --> 00:18:47.520]   The idea is if there's a loophole and you don't take advantage of it, you're not doing
[00:18:47.520 --> 00:18:48.960]   your job.
[00:18:48.960 --> 00:18:51.400]   So I'm sure Google would defend this in the same way.
[00:18:51.400 --> 00:18:56.880]   Look, this is legal and that it's not legal then it's not right.
[00:18:56.880 --> 00:18:57.880]   It's not right.
[00:18:57.880 --> 00:19:01.640]   Any more than the Irish runaround is.
[00:19:01.640 --> 00:19:06.080]   But that's not how it works in the United States.
[00:19:06.080 --> 00:19:12.440]   And you know, the thing that bothers me is the reason it's that way is because corporations
[00:19:12.440 --> 00:19:19.000]   spend lots of money on political action committees and lobbyists and elections to keep the laws
[00:19:19.000 --> 00:19:22.680]   in their favor and it's too bad.
[00:19:22.680 --> 00:19:26.360]   Anyway, we use temp employees.
[00:19:26.360 --> 00:19:29.440]   You guys, you guys don't work for Twit.
[00:19:29.440 --> 00:19:33.920]   But when you're in the building, you're an employee.
[00:19:33.920 --> 00:19:37.400]   So I mean, I guess in a way you could say the same thing about Twit, probably half of
[00:19:37.400 --> 00:19:40.680]   our employees are more than half our contractors.
[00:19:40.680 --> 00:19:46.240]   But it's exactly as you said, Jeff, they fit the IRS requirement, which is they do this
[00:19:46.240 --> 00:19:49.400]   for a living with other clients.
[00:19:49.400 --> 00:19:54.720]   That's why we never stop people from having other podcasts that there's generally it's
[00:19:54.720 --> 00:19:57.640]   supposedly the schedule is your own.
[00:19:57.640 --> 00:20:01.240]   You know, you don't you don't you're not told you have to be here at a certain time.
[00:20:01.240 --> 00:20:02.320]   That's the case for you guys.
[00:20:02.320 --> 00:20:05.200]   But on the other hand, when you have to be out of town, you go and stay safe.
[00:20:05.200 --> 00:20:06.200]   You're very forgiving.
[00:20:06.200 --> 00:20:07.200]   Yeah.
[00:20:07.200 --> 00:20:09.560]   Well, I think that's the law.
[00:20:09.560 --> 00:20:12.640]   You think I would let you take time off if I didn't have to?
[00:20:12.640 --> 00:20:14.120]   You could do our business.
[00:20:14.120 --> 00:20:15.600]   You're forgiving because the law.
[00:20:15.600 --> 00:20:19.840]   Well, yeah, actually you don't get paid when you're not here, but that's also a benefit
[00:20:19.840 --> 00:20:21.520]   of a temp or a contractor of a temp.
[00:20:21.520 --> 00:20:23.920]   Yeah, that's the thing is flexible on both sides.
[00:20:23.920 --> 00:20:25.720]   You pay them only when they work.
[00:20:25.720 --> 00:20:34.480]   But it feels to me as though someone can craft a law so that if 52% of your staff are contractors,
[00:20:34.480 --> 00:20:36.920]   that crosses some threshold.
[00:20:36.920 --> 00:20:37.920]   Yeah.
[00:20:37.920 --> 00:20:47.040]   Part of this is exacerbated by the fact that the IRS has been significantly cut back on
[00:20:47.040 --> 00:20:48.720]   its ability to enforce.
[00:20:48.720 --> 00:20:54.200]   They don't have a huge enforcement ability anyway.
[00:20:54.200 --> 00:20:58.760]   So you know, the IRS has essentially been gutted.
[00:20:58.760 --> 00:21:02.960]   So you can get away with it even if you even if it's not strictly legal, you have to get
[00:21:02.960 --> 00:21:03.960]   an interpretation.
[00:21:03.960 --> 00:21:04.960]   You have to get the IRS.
[00:21:04.960 --> 00:21:07.760]   You have to get tax court to say, no, it's not legal.
[00:21:07.760 --> 00:21:09.640]   You could say, well, I believe so.
[00:21:09.640 --> 00:21:12.400]   Well, okay, the tax court says no, but there's I don't know how many judges.
[00:21:12.400 --> 00:21:14.080]   There's eight or nine judges in the tax court.
[00:21:14.080 --> 00:21:16.920]   There's not a lot of them.
[00:21:16.920 --> 00:21:22.360]   The IRS has 9,000 auditors down a third from 2010.
[00:21:22.360 --> 00:21:23.360]   Yeah.
[00:21:23.360 --> 00:21:30.560]   Last time the IRS had fewer than 10,000 revenue agents was 1953 and then the economy was one
[00:21:30.560 --> 00:21:33.840]   seventh of its current size at that time.
[00:21:33.840 --> 00:21:36.680]   And it's still shrinking.
[00:21:36.680 --> 00:21:42.560]   The IRS conducted 675,000 fewer audits in 2017 than it did in 2010, a drop in the audit
[00:21:42.560 --> 00:21:44.040]   rate of 42%.
[00:21:44.040 --> 00:21:49.600]   So that's the other side of this is if it's not enforced, you can you can, I mean, there's
[00:21:49.600 --> 00:21:54.040]   clearly a case to be made, although these are real contractors.
[00:21:54.040 --> 00:21:57.120]   And then if the IRS disagrees, they have to pursue it.
[00:21:57.120 --> 00:22:00.040]   And if they don't have the resources to do so, they won't.
[00:22:00.040 --> 00:22:07.640]   And the New York Times story says that about 10 years ago, it was only a third of employees.
[00:22:07.640 --> 00:22:08.800]   Now it's over half.
[00:22:08.800 --> 00:22:09.800]   Yeah.
[00:22:09.800 --> 00:22:12.440]   Well, but again, the kinds of functions that have been added.
[00:22:12.440 --> 00:22:13.440]   Yeah.
[00:22:13.440 --> 00:22:15.240]   They could be driving way more.
[00:22:15.240 --> 00:22:17.480]   So literally delivering grocery, right?
[00:22:17.480 --> 00:22:20.640]   Or content moderation or right.
[00:22:20.640 --> 00:22:21.640]   Right.
[00:22:21.640 --> 00:22:22.640]   Right.
[00:22:22.640 --> 00:22:25.940]   I was saying with Facebook and their content moderators, they're, you know, reportedly
[00:22:25.940 --> 00:22:33.960]   hired 30,000, I think the vast majority are through con, contracted agencies, companies.
[00:22:33.960 --> 00:22:38.600]   Email messages about workplace security concerns that go to full time staff are often not shared
[00:22:38.600 --> 00:22:41.720]   with contract workers.
[00:22:41.720 --> 00:22:46.420]   In a letter to send her a Pachai temp workers complained the company sent security updates
[00:22:46.420 --> 00:22:52.240]   only to full time employees during that shooting at YouTube's offices last year.
[00:22:52.240 --> 00:22:55.620]   Contractors weren't told there's an active shooter.
[00:22:55.620 --> 00:22:59.000]   They were also under, I wonder if they even had a Google email address.
[00:22:59.000 --> 00:23:02.440]   They probably had their own independent email address.
[00:23:02.440 --> 00:23:06.160]   They weren't on a, you know, a company group.
[00:23:06.160 --> 00:23:07.680]   YouTube says that was an oversight.
[00:23:07.680 --> 00:23:08.680]   Yeah.
[00:23:08.680 --> 00:23:13.840]   Well, it was.
[00:23:13.840 --> 00:23:16.600]   So yeah, I don't know what to make of this.
[00:23:16.600 --> 00:23:21.080]   It's, it wouldn't be, they wouldn't be doing it if it weren't legal or at least could, if
[00:23:21.080 --> 00:23:26.500]   there weren't a reasonable interpretation that it's legal, but since that interpretation
[00:23:26.500 --> 00:23:28.180]   stands unless the IRS is using it.
[00:23:28.180 --> 00:23:29.180]   And there's always a fight about this.
[00:23:29.180 --> 00:23:30.180]   Yeah.
[00:23:30.180 --> 00:23:31.180]   Yeah.
[00:23:31.180 --> 00:23:32.180]   Yeah.
[00:23:32.180 --> 00:23:33.180]   This is a constant battle.
[00:23:33.180 --> 00:23:34.180]   I know.
[00:23:34.180 --> 00:23:35.180]   And we're very careful.
[00:23:35.180 --> 00:23:39.500]   We have, you know, we outsource payroll and HR to a payroll company and they are the ones
[00:23:39.500 --> 00:23:43.900]   who enforce and they have very strict rules and we adhere to, we always try to adhere
[00:23:43.900 --> 00:23:46.220]   to all those rules.
[00:23:46.220 --> 00:23:52.600]   Because nobody wants this vanishing position.
[00:23:52.600 --> 00:23:56.280]   Here's another, the New York Times blowing the lid off Google.
[00:23:56.280 --> 00:24:02.000]   We won't question their motives in all of this, but according to the times, and this
[00:24:02.000 --> 00:24:05.440]   is a great article, but Brian X Chen, who we know well, he's been on our show many times
[00:24:05.440 --> 00:24:10.860]   and Kate Metz, two of two really good tech reporters, a lot of times Google duplex uses
[00:24:10.860 --> 00:24:11.860]   humans.
[00:24:11.860 --> 00:24:14.100]   In fact, maybe more often than not.
[00:24:14.100 --> 00:24:20.100]   So you know that great demonstration where Google duplex made a booking.
[00:24:20.100 --> 00:24:25.420]   Well, here's an example.
[00:24:25.420 --> 00:24:28.940]   So what, what decayed and the Brian did is they went out and they made reservations,
[00:24:28.940 --> 00:24:31.820]   but they went to the restaurant ahead of time because of the New York Times and said,
[00:24:31.820 --> 00:24:33.700]   look, we're going to do this.
[00:24:33.700 --> 00:24:35.180]   We just want to see what happens.
[00:24:35.180 --> 00:24:38.980]   Here's a Google Assistant call with a loud tie kitchen.
[00:24:38.980 --> 00:24:39.980]   Hello.
[00:24:39.980 --> 00:24:43.380]   I'm calling to make a reservation for a client.
[00:24:43.380 --> 00:24:45.620]   I'm calling for Google's call me to your recorders.
[00:24:45.620 --> 00:24:48.580]   Do you have any available for some of their main 19?
[00:24:48.580 --> 00:24:49.580]   Oh, yes, sir.
[00:24:49.580 --> 00:24:54.940]   And yeah, we would love to take your reservation and for how many people, sir?
[00:24:54.940 --> 00:24:56.940]   It's just for a party of two.
[00:24:56.940 --> 00:24:57.940]   I'm sorry.
[00:24:57.940 --> 00:24:59.340]   Can you repeat that?
[00:24:59.340 --> 00:25:01.100]   It's just for a party of two.
[00:25:01.100 --> 00:25:02.100]   That's pretty good, right?
[00:25:02.100 --> 00:25:03.100]   Oh, for a party of two.
[00:25:03.100 --> 00:25:04.100]   Okay.
[00:25:04.100 --> 00:25:06.100]   And your name, sir?
[00:25:06.100 --> 00:25:07.540]   The first name is Katie C-A-D-E.
[00:25:07.540 --> 00:25:08.540]   C-A-D-E, Katie.
[00:25:08.540 --> 00:25:09.540]   I'm sorry.
[00:25:09.540 --> 00:25:10.540]   C-A-D-E.
[00:25:10.540 --> 00:25:15.540]   Yeah, that's correct.
[00:25:15.540 --> 00:25:16.540]   You got it.
[00:25:16.540 --> 00:25:17.540]   Oh, okay.
[00:25:17.540 --> 00:25:18.540]   And Katie, two people for--
[00:25:18.540 --> 00:25:20.540]   It's amazing, isn't it, what AI can do?
[00:25:20.540 --> 00:25:21.540]   No.
[00:25:21.540 --> 00:25:22.540]   Except that's a human.
[00:25:22.540 --> 00:25:23.540]   It's such a guy.
[00:25:23.540 --> 00:25:24.540]   It sounded very real.
[00:25:24.540 --> 00:25:26.900]   Yeah, I'm invented Irish AI yet.
[00:25:26.900 --> 00:25:31.180]   No, no, I think it would use an Irish accent, but in this case, it was a human.
[00:25:31.180 --> 00:25:34.100]   Google later confirmed to our disappointment the caller had been telling the truth.
[00:25:34.100 --> 00:25:36.500]   He was a person working in a call center.
[00:25:36.500 --> 00:25:40.300]   The company said Google said about 25% of calls placed through duplex start with a
[00:25:40.300 --> 00:25:41.300]   human.
[00:25:41.300 --> 00:25:44.460]   And in about 15% of human intervenes.
[00:25:44.460 --> 00:25:46.780]   So somebody's always listening.
[00:25:46.780 --> 00:25:50.340]   We tested duplex for several days, calling more than a dozen restaurants.
[00:25:50.340 --> 00:25:52.940]   Our tests showed a heavy reliance on humans.
[00:25:52.940 --> 00:25:55.220]   That makes sense, though, right?
[00:25:55.220 --> 00:26:00.180]   Out of four successful bookings, three were done by real people.
[00:26:00.180 --> 00:26:05.580]   It did sound good when it was a bot, but so--
[00:26:05.580 --> 00:26:08.500]   Wasn't Facebook-- remember Facebook M when they--
[00:26:08.500 --> 00:26:10.260]   Yeah, that was all humans too, right?
[00:26:10.260 --> 00:26:13.420]   Yeah, except they pretended to be AI.
[00:26:13.420 --> 00:26:18.300]   So they wanted you to think it was AI, but it was actually human beings.
[00:26:18.300 --> 00:26:25.700]   So they did the same in Northern California, and this was the only reservation that used
[00:26:25.700 --> 00:26:28.660]   Google AI all the way through.
[00:26:28.660 --> 00:26:29.660]   Hello, boys.
[00:26:29.660 --> 00:26:31.300]   Hello, my name is Appiah.
[00:26:31.300 --> 00:26:32.300]   Hello.
[00:26:32.300 --> 00:26:33.300]   Hello.
[00:26:33.300 --> 00:26:36.420]   Hi, I'm calling to make a reservation.
[00:26:36.420 --> 00:26:40.220]   I'm Google's automated booking service, so I'll record the call.
[00:26:40.220 --> 00:26:43.660]   Could I book a table for Tuesday the 21st?
[00:26:43.660 --> 00:26:45.220]   OK, hello.
[00:26:45.220 --> 00:26:55.980]   I'd like to make a reservation for a client for Tuesday the 21st.
[00:26:55.980 --> 00:27:00.540]   OK, actually, how many people?
[00:27:00.540 --> 00:27:01.540]   It's for 10 people.
[00:27:01.540 --> 00:27:03.540]   I think the bot's doing pretty well.
[00:27:03.540 --> 00:27:04.540]   OK, one time.
[00:27:04.540 --> 00:27:05.540]   Mm-hmm.
[00:27:05.540 --> 00:27:06.540]   At 7 PM.
[00:27:06.540 --> 00:27:07.540]   7 PM?
[00:27:07.540 --> 00:27:08.540]   OK.
[00:27:08.540 --> 00:27:12.420]   I need a table for 7 PM.
[00:27:12.420 --> 00:27:14.220]   Well, that was a giveaway.
[00:27:14.220 --> 00:27:15.220]   OK, 7 PM?
[00:27:15.220 --> 00:27:16.220]   OK.
[00:27:16.220 --> 00:27:17.220]   And then--
[00:27:17.220 --> 00:27:18.220]   Yeah.
[00:27:18.220 --> 00:27:20.020]   Is there any kids?
[00:27:20.020 --> 00:27:23.660]   I'm actually booking on behalf of a client, so I'm not too sure.
[00:27:23.660 --> 00:27:24.660]   Not too good.
[00:27:24.660 --> 00:27:25.660]   OK.
[00:27:25.660 --> 00:27:26.660]   Good answers.
[00:27:26.660 --> 00:27:27.660]   Pretty good.
[00:27:27.660 --> 00:27:30.100]   OK, so please be on time.
[00:27:30.100 --> 00:27:34.300]   That's 7 PM on Tuesday, right?
[00:27:34.300 --> 00:27:35.300]   Yeah.
[00:27:35.300 --> 00:27:36.300]   OK.
[00:27:36.300 --> 00:27:37.300]   OK.
[00:27:37.300 --> 00:27:41.180]   OK, I can put you in the reservation book.
[00:27:41.180 --> 00:27:43.340]   So, to you on Tuesday.
[00:27:43.340 --> 00:27:46.940]   Oh, would you like the client's name now?
[00:27:46.940 --> 00:27:47.940]   Mm.
[00:27:47.940 --> 00:27:48.940]   Yeah.
[00:27:48.940 --> 00:27:49.940]   Oh.
[00:27:49.940 --> 00:27:50.940]   That was close.
[00:27:50.940 --> 00:27:51.940]   Good luck with that.
[00:27:51.940 --> 00:27:52.940]   Name is Kate.
[00:27:52.940 --> 00:27:53.940]   Yeah.
[00:27:53.940 --> 00:27:54.940]   OK, Kate.
[00:27:54.940 --> 00:27:56.940]   OK, what is the last name?
[00:27:56.940 --> 00:27:58.940]   Last name, meth.
[00:27:58.940 --> 00:27:59.940]   That's OK.
[00:27:59.940 --> 00:28:01.580]   The bot announced Kate's name properly.
[00:28:01.580 --> 00:28:04.580]   The human did not.
[00:28:04.580 --> 00:28:06.780]   So I'm impressed.
[00:28:06.780 --> 00:28:07.780]   Yeah.
[00:28:07.780 --> 00:28:11.620]   The restaurants owner, Mr. Park, said everything was perfect.
[00:28:11.620 --> 00:28:14.860]   It's like a real person talking.
[00:28:14.860 --> 00:28:18.300]   And he was very impressed with how it handled the question about children and the kids
[00:28:18.300 --> 00:28:19.300]   in the party.
[00:28:19.300 --> 00:28:20.300]   It was impressive.
[00:28:20.300 --> 00:28:21.300]   Yeah.
[00:28:21.300 --> 00:28:22.300]   So--
[00:28:22.300 --> 00:28:23.740]   Yeah, this just seemed like a gotch a story.
[00:28:23.740 --> 00:28:32.580]   I think the problem here is that Google put a service out as a test before they had enough
[00:28:32.580 --> 00:28:35.300]   capabilities so they had to self-know people.
[00:28:35.300 --> 00:28:39.740]   I don't think it's a little bit funny, but I don't think it's any kind of shot.
[00:28:39.740 --> 00:28:46.460]   Well, I guess the issue is a lot of times tech companies will offer the moon.
[00:28:46.460 --> 00:28:48.060]   You know, they really want to do the sci-fi.
[00:28:48.060 --> 00:28:49.220]   Isn't this amazing?
[00:28:49.220 --> 00:28:50.220]   Mm-hmm.
[00:28:50.220 --> 00:28:53.860]   And it is always a little disappointing to know that it doesn't really work that well
[00:28:53.860 --> 00:28:57.860]   and that humans have to do like with them and with this.
[00:28:57.860 --> 00:29:03.860]   Neither Google nor Facebook's really hidden this fact, but they also don't mention it.
[00:29:03.860 --> 00:29:07.300]   It's not like Sundar Pachay stood on stage and say, "How about that Google?"
[00:29:07.300 --> 00:29:11.900]   No, by the way, half the time it'll be a human.
[00:29:11.900 --> 00:29:12.900]   But--
[00:29:12.900 --> 00:29:17.780]   Because I think they over-- I think this is the real complaint and I would share this.
[00:29:17.780 --> 00:29:18.780]   They over--
[00:29:18.780 --> 00:29:20.820]   Tech companies tend to oversell what they can do.
[00:29:20.820 --> 00:29:24.260]   I think that's happening now with Tesla and the--
[00:29:24.260 --> 00:29:25.260]   Oh, yeah.
[00:29:25.260 --> 00:29:33.900]   There's been a lot of backlash about how sort of automation is not functional in the way
[00:29:33.900 --> 00:29:36.180]   that Tesla would like you to think it is.
[00:29:36.180 --> 00:29:37.940]   Well, in that story of the one down--
[00:29:37.940 --> 00:29:40.700]   That's bad because it can kill people.
[00:29:40.700 --> 00:29:41.700]   Right.
[00:29:41.700 --> 00:29:42.700]   Right.
[00:29:42.700 --> 00:29:44.860]   That's not a minor detail.
[00:29:44.860 --> 00:29:47.300]   So which story are you referring to?
[00:29:47.300 --> 00:29:51.380]   There was one I saw today that was another rundown saying that the lane change--
[00:29:51.380 --> 00:29:53.020]   Oh, yeah, because it was a report.
[00:29:53.020 --> 00:29:55.220]   It's tested the lane change.
[00:29:55.220 --> 00:30:03.540]   Yeah, so Consumer Reports, who has historically been critical of autopilot-- we should point
[00:30:03.540 --> 00:30:06.180]   this out-- tested this new feature.
[00:30:06.180 --> 00:30:10.340]   So one of the features that Tesla's starting to roll out is the ability right now when
[00:30:10.340 --> 00:30:12.020]   you use autopilot, you really wait.
[00:30:12.020 --> 00:30:15.940]   You can't-- you could sort of do it on a back road, but it certainly wouldn't navigate
[00:30:15.940 --> 00:30:20.020]   through town because it doesn't see stop signs, it doesn't see red lights.
[00:30:20.020 --> 00:30:25.620]   You really use it-- I use it on the highway only when lanes are clear and you just want
[00:30:25.620 --> 00:30:27.980]   it to kind of-- it's basically fancy cruise control.
[00:30:27.980 --> 00:30:28.980]   You just wanted to stay in the lane.
[00:30:28.980 --> 00:30:30.660]   Is your family OK with that?
[00:30:30.660 --> 00:30:31.660]   No.
[00:30:31.660 --> 00:30:32.660]   Or are you doing your alone?
[00:30:32.660 --> 00:30:33.660]   No.
[00:30:33.660 --> 00:30:34.660]   Yeah, I would say.
[00:30:34.660 --> 00:30:35.820]   Lisa does not trust it.
[00:30:35.820 --> 00:30:40.940]   She believes that the car has several times tried to throw her into a guardrail, so she
[00:30:40.940 --> 00:30:42.460]   will not use it.
[00:30:42.460 --> 00:30:44.340]   I certainly don't trust it.
[00:30:44.340 --> 00:30:48.340]   I mean, I keep-- and Tesla does a lot to make sure you're keeping your hands in the
[00:30:48.340 --> 00:30:49.340]   way.
[00:30:49.340 --> 00:30:50.340]   They'll-- they'll-- they'll--
[00:30:50.340 --> 00:30:51.340]   They'll turn it off.
[00:30:51.340 --> 00:30:52.340]   They'll spank you.
[00:30:52.340 --> 00:30:53.340]   And really?
[00:30:53.340 --> 00:30:56.700]   Yeah, well, not literally, but they'll-- but it's like being spanked.
[00:30:56.700 --> 00:30:57.700]   I feel like I'm chest.
[00:30:57.700 --> 00:31:01.820]   I'd like to get a child because if you don't have your hands on the wheel after the third
[00:31:01.820 --> 00:31:05.420]   reminder to do that, it'll go beep beep beep.
[00:31:05.420 --> 00:31:09.500]   It puts a big red thing on your speedometer that says, because you're not keeping your
[00:31:09.500 --> 00:31:13.860]   hands on the wheel, you can-- you may no longer use autopilot until the next ride.
[00:31:13.860 --> 00:31:15.340]   Oh, soup for you.
[00:31:15.340 --> 00:31:19.020]   I got to take a picture of it because it-- I mean, I feel spanked.
[00:31:19.020 --> 00:31:24.460]   I feel seriously chastised by the-- my-- my freaking car.
[00:31:24.460 --> 00:31:26.580]   Gosh darn it.
[00:31:26.580 --> 00:31:28.340]   And my attitude is, I'm such a teenager.
[00:31:28.340 --> 00:31:30.140]   Well, you didn't tell me.
[00:31:30.140 --> 00:31:32.540]   What do you mean?
[00:31:32.540 --> 00:31:35.340]   Anyway, it does-- it does alert you.
[00:31:35.340 --> 00:31:37.660]   It says, put your hands on the wheel, show that you're using it.
[00:31:37.660 --> 00:31:41.260]   I always keep my hands on the wheel, but the way it's set up, which is unfortunate, is
[00:31:41.260 --> 00:31:43.220]   you actually have to torque the wheel a little bit.
[00:31:43.220 --> 00:31:44.820]   It's got torque sensor.
[00:31:44.820 --> 00:31:49.300]   Some other cars like the new Cadillac has a capacitance sensor, so you don't have to
[00:31:49.300 --> 00:31:51.220]   turn the wheel for it to know you're there.
[00:31:51.220 --> 00:31:52.220]   It can feel your hands.
[00:31:52.220 --> 00:31:56.780]   But the Tesla, you have to kind of slightly jiggle the wheel, which is annoying.
[00:31:56.780 --> 00:31:57.780]   It's ridiculous.
[00:31:57.780 --> 00:31:58.780]   So you're sitting here--
[00:31:58.780 --> 00:31:59.780]   It's like a kid.
[00:31:59.780 --> 00:32:02.060]   It's like old movies, like making believe you're driving.
[00:32:02.060 --> 00:32:03.060]   Yeah.
[00:32:03.060 --> 00:32:04.060]   Do it this.
[00:32:04.060 --> 00:32:05.060]   Yeah.
[00:32:05.060 --> 00:32:06.060]   Yeah.
[00:32:06.060 --> 00:32:07.380]   Yeah, you're going like this.
[00:32:07.380 --> 00:32:11.020]   So I do that, but I mean, sometimes you forget, then they'll say, come on, show us your
[00:32:11.020 --> 00:32:16.220]   gear, and then do it three times the second time it beeps at you.
[00:32:16.220 --> 00:32:18.180]   But if you miss that, then it gives you--
[00:32:18.180 --> 00:32:19.180]   And I said several times--
[00:32:19.180 --> 00:32:20.820]   Then you're Elon's voice.
[00:32:20.820 --> 00:32:22.460]   You are not driving.
[00:32:22.460 --> 00:32:28.620]   So in order to get the Holy Grail for Elon, which is not level five, but maybe level
[00:32:28.620 --> 00:32:35.140]   three would be that you could, as you're leaving, turn on autopilot if you have navigation
[00:32:35.140 --> 00:32:37.940]   engaged, and it would get you to where you're going.
[00:32:37.940 --> 00:32:41.020]   So we'd know how to get on the turn left here.
[00:32:41.020 --> 00:32:42.780]   We'd do all that.
[00:32:42.780 --> 00:32:44.580]   That is a long way off.
[00:32:44.580 --> 00:32:49.060]   But a step towards that is they've turned on a feature, not on my car, but on more modern
[00:32:49.060 --> 00:32:52.140]   cars with better hardware.
[00:32:52.140 --> 00:32:56.900]   Normally when you're doing autopilot and you want to lane change, you do the turn signal.
[00:32:56.900 --> 00:32:59.060]   Theoretically, the car checks.
[00:32:59.060 --> 00:33:01.660]   It speeds up a little bit and changes lanes.
[00:33:01.660 --> 00:33:02.900]   It has been my experience.
[00:33:02.900 --> 00:33:04.900]   The car doesn't check well.
[00:33:04.900 --> 00:33:08.340]   So I never do it without looking first and then I do it.
[00:33:08.340 --> 00:33:13.540]   Well, you can now turn it off and say, well, anytime you feel like it, you may change lanes.
[00:33:13.540 --> 00:33:18.860]   Consumer report says, at least several times it cut-- it did it at a bad time.
[00:33:18.860 --> 00:33:21.180]   It cut off the car behind it.
[00:33:21.180 --> 00:33:23.340]   Or it broke state law.
[00:33:23.340 --> 00:33:26.700]   It didn't specify how it broke state law.
[00:33:26.700 --> 00:33:29.300]   But they said it's dangerous.
[00:33:29.300 --> 00:33:30.540]   You shouldn't use this.
[00:33:30.540 --> 00:33:32.380]   And this is not anything new.
[00:33:32.380 --> 00:33:37.220]   I think Tesla, even by calling it autopilot, is pretty--
[00:33:37.220 --> 00:33:44.020]   Yeah, I think there's been a lot of over-marketing or overselling.
[00:33:44.020 --> 00:33:52.700]   He also, last month at an event, said that he thought Tesla owners would be able to make
[00:33:52.700 --> 00:33:59.300]   $30,000 a year by leasing out their Tesla.
[00:33:59.300 --> 00:34:02.020]   He said, but this should happen by next year from our standpoint.
[00:34:02.020 --> 00:34:05.580]   If you fast forward a year, maybe a year and three months, I like how.
[00:34:05.580 --> 00:34:07.140]   See, this is one trick, by the way, kids.
[00:34:07.140 --> 00:34:10.340]   If you want to lie, be very specific.
[00:34:10.340 --> 00:34:13.260]   Okay, a year and three months.
[00:34:13.260 --> 00:34:16.580]   Next year, for sure, we'll have over a million robo taxis on the road.
[00:34:16.580 --> 00:34:18.260]   He said this last month.
[00:34:18.260 --> 00:34:20.460]   The fleet wakes up with an over-the-air update.
[00:34:20.460 --> 00:34:21.460]   That's all it takes.
[00:34:21.460 --> 00:34:23.860]   All we have to do is turn it on.
[00:34:23.860 --> 00:34:29.340]   He says, Tesla owners, we know, because we have all the numbers, only drive their car
[00:34:29.340 --> 00:34:31.380]   two out of the 24 hours of a day.
[00:34:31.380 --> 00:34:35.740]   So we'll have a switch that a Tesla owner can turn on that makes your car a ride-sharing
[00:34:35.740 --> 00:34:39.420]   vehicle that can be hailed.
[00:34:39.420 --> 00:34:43.220]   We'll go pick somebody up, take them to where they want, automatically charge them just
[00:34:43.220 --> 00:34:47.860]   like Uber, and you could make up to $30,000 a year.
[00:34:47.860 --> 00:34:49.580]   So mean to ask you, Paul.
[00:34:49.580 --> 00:34:53.380]   This is not going to happen in a year and three months.
[00:34:53.380 --> 00:35:01.900]   Plus, the sensors and processing power that's on a Tesla versus what Google's putting onto
[00:35:01.900 --> 00:35:04.020]   its cars with LiDAR and all this stuff.
[00:35:04.020 --> 00:35:08.300]   I know Elon makes fun of LiDAR and says it's unnecessary because he doesn't have it.
[00:35:08.300 --> 00:35:14.580]   I cannot believe that he believes there is sufficient technology on an existing Tesla
[00:35:14.580 --> 00:35:16.180]   to be a self-driving car.
[00:35:16.180 --> 00:35:23.100]   Well, in his defense, he has said he's tweeted, "I'm sad that we have to report tweets as
[00:35:23.100 --> 00:35:24.100]   a fact."
[00:35:24.100 --> 00:35:29.020]   But anyway, he's tweeted that he doesn't believe LiDAR is necessary.
[00:35:29.020 --> 00:35:31.100]   That's a very big expense.
[00:35:31.100 --> 00:35:34.260]   He could just do it with cameras and everybody said, "Ha ha!"
[00:35:34.260 --> 00:35:35.980]   Because there's no LiDAR on Tesla.
[00:35:35.980 --> 00:35:40.020]   However, and I wish I'd had this source of the study, but I just saw a study that said,
[00:35:40.020 --> 00:35:42.100]   "No, Elon's right.
[00:35:42.100 --> 00:35:43.980]   Cameras are sufficient.
[00:35:43.980 --> 00:35:46.700]   You don't have to have LiDAR if you have properly configured cameras."
[00:35:46.700 --> 00:35:51.060]   And by the way, Tesla's also have some radar capability.
[00:35:51.060 --> 00:35:52.140]   That's all you need.
[00:35:52.140 --> 00:35:54.180]   You can do full level 5 self-driving.
[00:35:54.180 --> 00:35:55.260]   So you don't have to have LiDAR.
[00:35:55.260 --> 00:35:57.220]   That would save a lot of money.
[00:35:57.220 --> 00:36:00.580]   That one of the things Waymo works really hard to do, and one of the things that they
[00:36:00.580 --> 00:36:05.940]   claimed Corey Lewandowski stole from Waymo when he went to Uber, was this way to do LiDAR
[00:36:05.940 --> 00:36:09.620]   for a tenth of cost because it's about $10,000 or $11,000 for a whole reason.
[00:36:09.620 --> 00:36:10.620]   Is it?
[00:36:10.620 --> 00:36:12.540]   Wave says self-driving cars don't need sensors.
[00:36:12.540 --> 00:36:14.420]   Experts aren't so sure.
[00:36:14.420 --> 00:36:15.420]   No.
[00:36:15.420 --> 00:36:16.420]   Mashable?
[00:36:16.420 --> 00:36:17.420]   No.
[00:36:17.420 --> 00:36:22.140]   It was a...
[00:36:22.140 --> 00:36:25.500]   When I saw the study, it was reliable.
[00:36:25.500 --> 00:36:26.500]   Here it is.
[00:36:26.500 --> 00:36:30.700]   Researchers back non-LIDAR approach to self-driving cars.
[00:36:30.700 --> 00:36:31.700]   This is...
[00:36:31.700 --> 00:36:33.540]   That's just one research opinion.
[00:36:33.540 --> 00:36:35.940]   Well, let me show you.
[00:36:35.940 --> 00:36:40.260]   The researchers found analyzing captured images from a bird's eye view rather than the more
[00:36:40.260 --> 00:36:45.500]   traditional frontal view more than tripled their accuracy, making a stereo camera viable
[00:36:45.500 --> 00:36:49.420]   and low-cost alternative to LiDAR.
[00:36:49.420 --> 00:36:54.900]   This is from Cornell, a paper called "Sudo LiDAR" from visual depth estimation bridging
[00:36:54.900 --> 00:36:57.540]   the gap in 3D object detection for autonomous driving.
[00:36:57.540 --> 00:37:02.860]   So because of that highly academic title, I believe them.
[00:37:02.860 --> 00:37:04.220]   If you say it, the British accent.
[00:37:04.220 --> 00:37:05.820]   It's all that is possible.
[00:37:05.820 --> 00:37:09.580]   Cornell researchers are saying the data they captured from stereo cameras was nearly as
[00:37:09.580 --> 00:37:13.180]   precise as LiDAR.
[00:37:13.180 --> 00:37:18.340]   So I think Elon has a basis for what he said.
[00:37:18.340 --> 00:37:25.020]   However, there's no way in a year, a year and a half, two years, or even five years,
[00:37:25.020 --> 00:37:30.300]   this kind of autonomy, this level of autonomy will be available to the owner of a Tesla
[00:37:30.300 --> 00:37:31.300]   vehicle.
[00:37:31.300 --> 00:37:33.500]   I'm sorry, that just can't possibly be true.
[00:37:33.500 --> 00:37:36.260]   Do you remember "Summon"?
[00:37:36.260 --> 00:37:38.380]   I have "Summon" on my car.
[00:37:38.380 --> 00:37:42.180]   But the description, the initial description was you could be across town.
[00:37:42.180 --> 00:37:43.180]   Yeah, yeah, no.
[00:37:43.180 --> 00:37:47.900]   And then when you get it, they said, "Well, make sure your garage door is open."
[00:37:47.900 --> 00:37:51.980]   And if you do summon, it moves like a mile an hour.
[00:37:51.980 --> 00:37:52.980]   But I've never used it.
[00:37:52.980 --> 00:37:53.980]   I haven't on my car.
[00:37:53.980 --> 00:37:58.940]   I suppose before I turn my Tesla back in in July, I should probably try it once.
[00:37:58.940 --> 00:37:59.940]   But I'm afraid.
[00:37:59.940 --> 00:38:00.940]   I don't want to.
[00:38:00.940 --> 00:38:06.100]   I've seen too many Reddit reports of summon bashing into the garage door, the side of
[00:38:06.100 --> 00:38:08.300]   the wall, or whatever.
[00:38:08.300 --> 00:38:11.540]   You got a 3.10 vehicle, even if it's moving in a mile an hour.
[00:38:11.540 --> 00:38:13.660]   If it hits something, it's going to hit it.
[00:38:13.660 --> 00:38:17.220]   And it might put, you know, a brew, it might bruise you.
[00:38:17.220 --> 00:38:20.380]   Put a dent in it.
[00:38:20.380 --> 00:38:25.940]   By the way, if Tesla owners turn their cars into robo taxis and they kill somebody who's
[00:38:25.940 --> 00:38:33.820]   responsible, he says, "Tep musks as well, we would be.
[00:38:33.820 --> 00:38:35.700]   Tesla would be."
[00:38:35.700 --> 00:38:36.700]   Not the car.
[00:38:36.700 --> 00:38:37.700]   Oh my God, I hope so.
[00:38:37.700 --> 00:38:38.700]   Yeah.
[00:38:38.700 --> 00:38:43.380]   Yes, but the right thing to do is make sure they're very few accidents.
[00:38:43.380 --> 00:38:49.060]   You also at one point said, "We're going to put a feature, a more aggressive lane change
[00:38:49.060 --> 00:38:54.220]   feature in, and there might be some fender benders, but it'll be up to you whether you
[00:38:54.220 --> 00:38:56.220]   turn it on or not."
[00:38:56.220 --> 00:39:02.180]   Maybe a few, what's a few fender benders?
[00:39:02.180 --> 00:39:07.140]   I think we're more likely to get cold fusion next year than we are self-driving Teslas.
[00:39:07.140 --> 00:39:08.780]   But who knows?
[00:39:08.780 --> 00:39:10.420]   You know what?
[00:39:10.420 --> 00:39:17.100]   I want to, since we're bashing Elon, let's praise him for SpaceX and these satellites
[00:39:17.100 --> 00:39:21.980]   that they put up, this was pretty incredible, right?
[00:39:21.980 --> 00:39:29.420]   Although astronomers say Starlink could be a problem because with 12,000 low-Earth orbit
[00:39:29.420 --> 00:39:33.620]   satellites, we're not going to ever be able to do astronomy again.
[00:39:33.620 --> 00:39:36.500]   What's the event that Daniel...
[00:39:36.500 --> 00:39:37.660]   The Kessler Syndrome?
[00:39:37.660 --> 00:39:39.580]   Kessler Syndrome, yes.
[00:39:39.580 --> 00:39:43.540]   So this is an astronomer in Georgia saw this.
[00:39:43.540 --> 00:39:47.380]   Remember, so I just to recap, it was a week and a half ago now, so I figure you all know
[00:39:47.380 --> 00:39:54.740]   that SpaceX launched the first Falcon, was it a nine rocket with 60 Starlink satellites?
[00:39:54.740 --> 00:40:00.580]   Eventually they hope to get 12,000 satellites in low-Earth orbit and provide high-speed
[00:40:00.580 --> 00:40:03.300]   internet to every corner of the globe.
[00:40:03.300 --> 00:40:05.220]   Right after the launch, the satellites deployed.
[00:40:05.220 --> 00:40:10.540]   This is what a Georgia astronomer caught on video.
[00:40:10.540 --> 00:40:12.260]   Those are the satellites.
[00:40:12.260 --> 00:40:16.060]   Now they're not yet fully deployed, so they won't be all in a line like that.
[00:40:16.060 --> 00:40:22.460]   But if you have 12,000 objects that bright in the sky, astronomers are very concerned.
[00:40:22.460 --> 00:40:25.620]   They will only be visible at sunrise and sunset.
[00:40:25.620 --> 00:40:28.260]   Oh, so this is the sun bounce?
[00:40:28.260 --> 00:40:29.900]   Oh, are we?
[00:40:29.900 --> 00:40:30.900]   That makes sense.
[00:40:30.900 --> 00:40:35.860]   And Musk has already said that the rest of them they're going to coat with something non-reflective.
[00:40:35.860 --> 00:40:37.500]   To reduce the reflective?
[00:40:37.500 --> 00:40:40.740]   Maybe the stealth satellites by then.
[00:40:40.740 --> 00:40:42.460]   But it's only when it catches the sun, right?
[00:40:42.460 --> 00:40:46.140]   So in the middle of the night, you'll be able to see stars no problem.
[00:40:46.140 --> 00:40:48.540]   They of course are all autonomous.
[00:40:48.540 --> 00:40:51.660]   By the way, this is where autonomy becomes very important.
[00:40:51.660 --> 00:40:57.460]   And they have crypton jets that they can realign their orbit because it's very important
[00:40:57.460 --> 00:40:59.460]   they're not collide.
[00:40:59.460 --> 00:41:02.460]   It would be a...
[00:41:02.460 --> 00:41:03.900]   It would more than double the number.
[00:41:03.900 --> 00:41:08.420]   I think it triples the number of satellites in orbit around the Earth.
[00:41:08.420 --> 00:41:09.420]   And the Kessler...
[00:41:09.420 --> 00:41:12.500]   There are 2,000 satellites currently in orbit.
[00:41:12.500 --> 00:41:13.500]   So...
[00:41:13.500 --> 00:41:15.500]   It's going to be six times more.
[00:41:15.500 --> 00:41:16.500]   Yes.
[00:41:16.500 --> 00:41:17.500]   That would be $14,000.
[00:41:17.500 --> 00:41:20.820]   And that is one concern is the clutter, just the sheer volume.
[00:41:20.820 --> 00:41:22.940]   That's where the Kessler syndrome is a cause for concern.
[00:41:22.940 --> 00:41:25.820]   This is an astronomer back in the '70s who said...
[00:41:25.820 --> 00:41:29.100]   And there have been a number of science fiction novels based on this.
[00:41:29.100 --> 00:41:35.580]   If you were to have a collision and they would create more space debris, at some point you'd
[00:41:35.580 --> 00:41:37.020]   start having a lot of collisions.
[00:41:37.020 --> 00:41:40.020]   You'd have a kind of exponential increase in the number of collisions.
[00:41:40.020 --> 00:41:48.340]   And pretty soon, the entire sphere would be surrounded by space degree to the degree
[00:41:48.340 --> 00:41:50.700]   that the planet would die.
[00:41:50.700 --> 00:41:54.020]   And if it didn't, the bigger problem would be you could no longer send anything into
[00:41:54.020 --> 00:41:56.340]   space because something would hit it.
[00:41:56.340 --> 00:41:57.340]   Yeah.
[00:41:57.340 --> 00:41:58.340]   So...
[00:41:58.340 --> 00:41:59.580]   Remember the scene in "Wally"?
[00:41:59.580 --> 00:42:00.660]   It's just like that.
[00:42:00.660 --> 00:42:04.260]   There are seven eaves, which is a great Neil Stephen's novel.
[00:42:04.260 --> 00:42:06.260]   Seven eaves, I was just thinking about that.
[00:42:06.260 --> 00:42:07.260]   Yes.
[00:42:07.260 --> 00:42:08.260]   Yeah.
[00:42:08.260 --> 00:42:09.900]   Where the bits of the moon collide and...
[00:42:09.900 --> 00:42:11.220]   That's the Kessler syndrome.
[00:42:11.220 --> 00:42:12.820]   Yeah.
[00:42:12.820 --> 00:42:15.820]   And that brings us to another Neil Stephen's story.
[00:42:15.820 --> 00:42:16.820]   He's got a...
[00:42:16.820 --> 00:42:17.820]   I cannot wait.
[00:42:17.820 --> 00:42:19.460]   I'm already pre-ordered it.
[00:42:19.460 --> 00:42:25.980]   He's got a new novel coming out June 3rd in a couple of days called "Fall" or "Dodge
[00:42:25.980 --> 00:42:29.740]   in Hell."
[00:42:29.740 --> 00:42:31.780]   The hybrid science fantasy...
[00:42:31.780 --> 00:42:36.740]   sci-fi fantasy novel begins in the present day with Richard "Dodge" forthrast, an eccentric
[00:42:36.740 --> 00:42:40.300]   multi-billion-aremaite is forchered in the video game industry.
[00:42:40.300 --> 00:42:44.860]   When a freak accident during a routine medical procedure leaves it brain dead, the family
[00:42:44.860 --> 00:42:48.500]   is left to contend with his request to have his brain preserved until technology exists
[00:42:48.500 --> 00:42:50.700]   to bring him back to life.
[00:42:50.700 --> 00:42:55.140]   The near future world of fall is full of buzzwords and concepts we know.
[00:42:55.140 --> 00:42:59.740]   Augmented reality headsets, next-gen wireless network self-driving vehicles, facial recognition,
[00:42:59.740 --> 00:43:03.620]   quantum computing, blockchain and distributed crypto all feature prominently.
[00:43:03.620 --> 00:43:07.180]   Neil's always been great on that kind of stuff.
[00:43:07.180 --> 00:43:13.460]   In fact, his novel "Snow Crash" kind of presaged the metaverse this whole idea of living in it.
[00:43:13.460 --> 00:43:17.340]   You know he was a consultant to Amazon, right?
[00:43:17.340 --> 00:43:18.340]   To Blue Origin.
[00:43:18.340 --> 00:43:19.340]   Yeah, he still is.
[00:43:19.340 --> 00:43:20.340]   Yeah.
[00:43:20.340 --> 00:43:21.340]   I didn't know that.
[00:43:21.340 --> 00:43:25.100]   Whether he does anything important, I don't know.
[00:43:25.100 --> 00:43:33.340]   So they asked him an interview, "P.C. Magazine asked him, 'How would you describe the current
[00:43:33.340 --> 00:43:37.940]   state of the internet?'
[00:43:37.940 --> 00:43:41.180]   Just in a general sense of its role in our daily lives and where that concept of the
[00:43:41.180 --> 00:43:47.980]   myasma, characters in the novel refer to the myasma.
[00:43:47.980 --> 00:43:49.420]   That's the internet and social media."
[00:43:49.420 --> 00:43:51.980]   He says, "I end up having a pretty dark view of it.
[00:43:51.980 --> 00:43:56.220]   As you can tell from the book, I saw someone recently described social media in its current
[00:43:56.220 --> 00:43:59.660]   state as a doomsday machine and I think that's not far off.
[00:43:59.660 --> 00:44:05.580]   We've turned over our perception of what's real to algorithmically driven systems that
[00:44:05.580 --> 00:44:09.060]   are designed not to have humans in the loop because, well, if humans are in the loop, they're
[00:44:09.060 --> 00:44:13.580]   not scalable and if they're not scalable, they can't make tons and tons of money."
[00:44:13.580 --> 00:44:19.020]   Jeff, is that moral panic?
[00:44:19.020 --> 00:44:23.740]   Moral panic is more when you're blaming your problems on something than that.
[00:44:23.740 --> 00:44:27.660]   Yeah, it's not really when you're anticipating some dystopia.
[00:44:27.660 --> 00:44:29.100]   It's just dystopia.
[00:44:29.100 --> 00:44:30.100]   We kind of do.
[00:44:30.100 --> 00:44:36.860]   I mean, he's kind of right about this that we've optimized the algorithms to such a degree.
[00:44:36.860 --> 00:44:41.620]   Does it count as moral panic if it's right?
[00:44:41.620 --> 00:44:42.700]   Maybe we should be panicking.
[00:44:42.700 --> 00:44:45.820]   That's Karsten, by the way, that voice, that ethereal voice we should exhibit.
[00:44:45.820 --> 00:44:50.820]   Steve Harvard.
[00:44:50.820 --> 00:44:51.300]   Yeah, the voice of Harvard weighing in.
[00:44:51.300 --> 00:44:55.220]   Stephen Singles on to say the result is the situation we see today where no one agrees
[00:44:55.220 --> 00:44:59.700]   on what factual reality is and everyone is driven in the direction of content that's
[00:44:59.700 --> 00:45:05.660]   "more engaging" which almost always means it's more emotional, it's less factually
[00:45:05.660 --> 00:45:07.660]   based, it's less rational.
[00:45:07.660 --> 00:45:08.660]   I think he's right.
[00:45:08.660 --> 00:45:10.660]   Yeah, kind of destructive for a basic sit-at-point.
[00:45:10.660 --> 00:45:17.540]   So number one, that's what media has done for 100 years.
[00:45:17.540 --> 00:45:20.580]   But it's never done it so well.
[00:45:20.580 --> 00:45:22.660]   It's worse now.
[00:45:22.660 --> 00:45:26.180]   It's never done it so well, so it wasn't a problem.
[00:45:26.180 --> 00:45:31.580]   Two, research, and I constantly quote Rasmus Kles Nielsen of the Oxford Reuters Institute
[00:45:31.580 --> 00:45:32.900]   for the Study of Journalism.
[00:45:32.900 --> 00:45:36.740]   Every time somebody says filter bubble, he says, "I'm sorry, can't find any studies
[00:45:36.740 --> 00:45:39.060]   that say there actually is a filter bubble."
[00:45:39.060 --> 00:45:41.020]   And he has plenty of studies that say that it's not.
[00:45:41.020 --> 00:45:45.500]   So this notion that we're sent to this thing that we all already agree with and we dig
[00:45:45.500 --> 00:45:50.980]   into deeper, the research is not to date showing that, not that it couldn't, but so far
[00:45:50.980 --> 00:45:51.980]   it has not.
[00:45:51.980 --> 00:45:55.420]   Now, okay, I take it back, it's more authentic.
[00:45:55.420 --> 00:45:56.420]   Yeah.
[00:45:56.420 --> 00:46:02.620]   But I think I was pointing about the algorithm prioritizing engagement.
[00:46:02.620 --> 00:46:03.620]   Well, that's true.
[00:46:03.620 --> 00:46:04.620]   We know that's true.
[00:46:04.620 --> 00:46:05.620]   I think that's real.
[00:46:05.620 --> 00:46:06.620]   Yeah.
[00:46:06.620 --> 00:46:07.620]   And so...
[00:46:07.620 --> 00:46:08.620]   That's exactly what media has always done.
[00:46:08.620 --> 00:46:14.860]   Right, but this is much, much bigger and faster and stronger than media has ever been.
[00:46:14.860 --> 00:46:18.060]   And saying that's what media has already always done...
[00:46:18.060 --> 00:46:19.300]   It doesn't mean it's good.
[00:46:19.300 --> 00:46:20.780]   It doesn't mean it's good.
[00:46:20.780 --> 00:46:27.940]   Because of cable news, my kids, people look at me funny when I let my kids walk to school.
[00:46:27.940 --> 00:46:29.580]   Yeah, right.
[00:46:29.580 --> 00:46:31.340]   Isn't that you are nuts.
[00:46:31.340 --> 00:46:34.540]   Don't you know there's stranger danger, Karsten?
[00:46:34.540 --> 00:46:35.540]   Yeah.
[00:46:35.540 --> 00:46:44.980]   The World Health Organization on May 25th voted to add gaming disorder as an illness to
[00:46:44.980 --> 00:46:51.180]   their diagnostic manual, the ICD, a behavioral addiction.
[00:46:51.180 --> 00:46:59.100]   According to the World Health Organization, it's not enough to be a video game fan, nor
[00:46:59.100 --> 00:47:02.940]   does it count the number of hours spent playing.
[00:47:02.940 --> 00:47:06.740]   A video game addict is someone with an inability to stop playing even though it interferes
[00:47:06.740 --> 00:47:12.100]   with other areas of one's life, such as family, relationships, school, work, and sleep.
[00:47:12.100 --> 00:47:16.580]   These problems would typically continue for at least a year.
[00:47:16.580 --> 00:47:18.060]   I think you could add Facebook.
[00:47:18.060 --> 00:47:20.580]   Next year it'll be Facebook disorder, right?
[00:47:20.580 --> 00:47:27.940]   Although here's a solution from Joe Pinsker writing in The Atlantic.
[00:47:27.940 --> 00:47:31.300]   And I think this almost makes me want to go back to Facebook.
[00:47:31.300 --> 00:47:34.860]   Now I try to defy the Facebook algorithm.
[00:47:34.860 --> 00:47:36.820]   Social network is predictable and dreary.
[00:47:36.820 --> 00:47:38.660]   My quest to make it random and fun.
[00:47:38.660 --> 00:47:42.620]   He says that because you follow friends and stuff, you end up in, it's not quite a filter
[00:47:42.620 --> 00:47:44.820]   bubble, but it sort of is.
[00:47:44.820 --> 00:47:46.740]   He had a term he used.
[00:47:46.740 --> 00:47:50.020]   I'll see if I can find it.
[00:47:50.020 --> 00:47:54.740]   But basically you make everything kind of the same.
[00:47:54.740 --> 00:48:01.140]   So what he's done, and I didn't know about this, but there's apparently a, I guess it's
[00:48:01.140 --> 00:48:04.820]   a Chrome extension called Noisify, which pop.
[00:48:04.820 --> 00:48:05.700]   It's a randomizer.
[00:48:05.700 --> 00:48:06.660]   It's a randomizer.
[00:48:06.660 --> 00:48:12.420]   It populates, it adds noise to your Facebook activity.
[00:48:12.420 --> 00:48:17.300]   It does random searches.
[00:48:17.300 --> 00:48:22.620]   And then as a result, you'll start getting all sorts of random stuff in your feed, which
[00:48:22.620 --> 00:48:27.180]   she says is, or he says is great.
[00:48:27.180 --> 00:48:29.300]   I actually did that on Twitter as well.
[00:48:29.300 --> 00:48:33.820]   I started following people who were interested in things that were completely different than
[00:48:33.820 --> 00:48:39.220]   the things I was interested in, or people who shared sort of random weird things, because
[00:48:39.220 --> 00:48:44.420]   I wanted to try and randomize the feed instead of seeing the same things all the time.
[00:48:44.420 --> 00:48:48.860]   So we need a new bumper sticker instead of saying subvert the dominant paradigm.
[00:48:48.860 --> 00:48:52.260]   It says, I don't know, subvert the dominant algorithm.
[00:48:52.260 --> 00:48:53.820]   Randomize, yeah.
[00:48:53.820 --> 00:48:54.820]   Randomize your algorithms.
[00:48:54.820 --> 00:48:58.020]   I think this is kind of cool.
[00:48:58.020 --> 00:49:05.500]   The guy who wrote it, Max Hawkins apparently also built a tool that had Uber drop him off
[00:49:05.500 --> 00:49:06.940]   at random locations.
[00:49:06.940 --> 00:49:08.620]   I read that one.
[00:49:08.620 --> 00:49:09.620]   Yeah.
[00:49:09.620 --> 00:49:10.620]   I like that.
[00:49:10.620 --> 00:49:11.620]   Right?
[00:49:11.620 --> 00:49:12.620]   Because we always go to the same places.
[00:49:12.620 --> 00:49:13.620]   I like that.
[00:49:13.620 --> 00:49:18.340]   Then he built a tool that randomly picked Facebook events for him to attend and he would go.
[00:49:18.340 --> 00:49:19.340]   Yeah.
[00:49:19.340 --> 00:49:21.340]   And didn't he randomize Spotify too?
[00:49:21.340 --> 00:49:22.340]   Yep.
[00:49:22.340 --> 00:49:25.900]   He tried listening to music picked randomly by Spotify, wearing clothes bought randomly
[00:49:25.900 --> 00:49:31.020]   on Amazon, growing out random styles of facial hair and arranging phone calls with friends
[00:49:31.020 --> 00:49:33.580]   on randomly subject selected topics.
[00:49:33.580 --> 00:49:34.580]   Oh, and tattoos.
[00:49:34.580 --> 00:49:36.580]   He got a random tattoo.
[00:49:36.580 --> 00:49:38.580]   He got a random tattoo.
[00:49:38.580 --> 00:49:39.580]   Random junk.
[00:49:39.580 --> 00:49:42.500]   Got a random tattoo.
[00:49:42.500 --> 00:49:46.260]   He says, I was really worried it was going to be anime porn or something.
[00:49:46.260 --> 00:49:49.700]   And I'd be stuck with it for the rest of my life, but the computer selection was an abstract
[00:49:49.700 --> 00:49:51.460]   illustration of a parent and child.
[00:49:51.460 --> 00:49:53.020]   I super lucked out.
[00:49:53.020 --> 00:49:55.700]   But he got the tattoo.
[00:49:55.700 --> 00:49:56.700]   Hmm.
[00:49:56.700 --> 00:49:58.340]   I don't know if I go.
[00:49:58.340 --> 00:49:59.580]   Sort of an art project.
[00:49:59.580 --> 00:50:00.580]   We should have done.
[00:50:00.580 --> 00:50:01.580]   Carson, let's try to get the.
[00:50:01.580 --> 00:50:02.580]   Yeah.
[00:50:02.580 --> 00:50:04.940]   Get this guy, Max Hawkins.
[00:50:04.940 --> 00:50:07.580]   I would love to see the tattoo if nothing else.
[00:50:07.580 --> 00:50:09.700]   You can compare tattoos.
[00:50:09.700 --> 00:50:11.540]   My tattoo is not random.
[00:50:11.540 --> 00:50:13.780]   Dropped off randomly in different places.
[00:50:13.780 --> 00:50:14.780]   Yeah.
[00:50:14.780 --> 00:50:16.580]   My bet is is better.
[00:50:16.580 --> 00:50:18.540]   Yeah.
[00:50:18.540 --> 00:50:22.300]   Every time I joined a group, Pinsker writes, Facebook inferred that my new group mates
[00:50:22.300 --> 00:50:27.340]   were people like me, this is the algorithm at work, that I shared their interests.
[00:50:27.340 --> 00:50:34.020]   In other words, rather be them being interested in them because precisely because I do not,
[00:50:34.020 --> 00:50:37.700]   rather than being interested in them precisely because I do not, even in groups, Facebook's
[00:50:37.700 --> 00:50:41.460]   not designed to expand your social horizons.
[00:50:41.460 --> 00:50:45.860]   It's designed to surface things that are relevant to you and your offline peers.
[00:50:45.860 --> 00:50:46.860]   That's right.
[00:50:46.860 --> 00:50:49.500]   You have the rest of the internet for doing all the rest.
[00:50:49.500 --> 00:50:50.500]   Right.
[00:50:50.500 --> 00:50:56.180]   The more one pushes back against personalization, the closer one gets to Max Hawkins unfiltered,
[00:50:56.180 --> 00:50:58.460]   randomized extreme and all of them.
[00:50:58.460 --> 00:51:00.020]   Otherwise known as mass media.
[00:51:00.020 --> 00:51:01.100]   Yes.
[00:51:01.100 --> 00:51:02.100]   Just why not?
[00:51:02.100 --> 00:51:03.340]   Why don't we just go back to three channels?
[00:51:03.340 --> 00:51:04.340]   Why don't we just go back to your post?
[00:51:04.340 --> 00:51:06.900]   Let's just go back to three channels.
[00:51:06.900 --> 00:51:07.900]   Yeah.
[00:51:07.900 --> 00:51:13.580]   There's a reason he says chat roulette doesn't have the same market share as Skype.
[00:51:13.580 --> 00:51:17.020]   My fellow serendipity seekers had started to feel that exhaustion.
[00:51:17.020 --> 00:51:18.980]   It turns out even before I met them.
[00:51:18.980 --> 00:51:21.660]   Randomness is not the only reason.
[00:51:21.660 --> 00:51:22.660]   Yeah.
[00:51:22.660 --> 00:51:29.980]   Finally, he followed like a baby rat's groups and he would just get pictures of baby rats
[00:51:29.980 --> 00:51:35.500]   in his feet, which is a little unsettling, honestly.
[00:51:35.500 --> 00:51:39.780]   The article, "How I Hacked Facebook Ends," it's certainly possible to battle the algorithms
[00:51:39.780 --> 00:51:43.420]   and discover new vistas from which to assess one's life.
[00:51:43.420 --> 00:51:46.220]   But the fight becomes wearying.
[00:51:46.220 --> 00:51:52.780]   After a while, the baby rats start to look like, well, baby rats.
[00:51:52.780 --> 00:51:53.780]   Meanwhile.
[00:51:53.780 --> 00:52:03.500]   Meanwhile, back in the real world, we have a new ability to get true random number generation.
[00:52:03.500 --> 00:52:04.700]   Well see, now that's a big deal.
[00:52:04.700 --> 00:52:06.340]   I'm putting that up right now.
[00:52:06.340 --> 00:52:07.420]   That's a big deal.
[00:52:07.420 --> 00:52:08.860]   So that is a big catch.
[00:52:08.860 --> 00:52:16.580]   Just as some background, almost all computer programming languages have a facility to generate
[00:52:16.580 --> 00:52:18.140]   random numbers for dice throws.
[00:52:18.140 --> 00:52:20.220]   There's all sorts of reasons you would use it.
[00:52:20.220 --> 00:52:24.700]   But in every case, they're called pseudo random number generators because it's very hard
[00:52:24.700 --> 00:52:28.820]   for a computer actually to create something random.
[00:52:28.820 --> 00:52:34.900]   And so there have been lots of attempts to make a real random number generator.
[00:52:34.900 --> 00:52:42.620]   Basically, my favorite is, who are the folks who do 1.1.1.
[00:52:42.620 --> 00:52:43.620]   Cloudflare.
[00:52:43.620 --> 00:52:49.580]   Cloudflare has a random number generator based on lava lamps.
[00:52:49.580 --> 00:52:52.140]   And the reason is, and by the way, it works.
[00:52:52.140 --> 00:52:54.420]   It works on very good at picking random numbers.
[00:52:54.420 --> 00:52:57.220]   Here's a little video from an English guy.
[00:52:57.220 --> 00:52:59.700]   This is a Cloudflare's headquarters in San Francisco.
[00:52:59.700 --> 00:53:06.700]   They have a wall of lava lamps because essentially the balls of lava are chaotic.
[00:53:06.700 --> 00:53:09.780]   So what you need is chaos.
[00:53:09.780 --> 00:53:15.060]   Steve Gibson has talked about using, I think, was a diode to create.
[00:53:15.060 --> 00:53:16.740]   You need noise.
[00:53:16.740 --> 00:53:23.940]   So this is an entropy wall that is actually generating of lava lamps.
[00:53:23.940 --> 00:53:26.660]   So what was the new way quantum mechanics?
[00:53:26.660 --> 00:53:29.380]   Yeah, well, quantum random something.
[00:53:29.380 --> 00:53:30.380]   Quantum entanglement.
[00:53:30.380 --> 00:53:33.260]   You know, everything's better with a little quantum entanglement.
[00:53:33.260 --> 00:53:37.500]   And a little cold fusion while you're at it.
[00:53:37.500 --> 00:53:45.140]   I think quantum entanglement is the blockchain of the next couple of years.
[00:53:45.140 --> 00:53:48.980]   Jeff, you're going to take a break, but when we come back, I want you to talk about this
[00:53:48.980 --> 00:53:51.540]   Facebook workshop you're about to participate in.
[00:53:51.540 --> 00:53:52.540]   Yeah.
[00:53:52.540 --> 00:53:53.540]   Or I was already there.
[00:53:53.540 --> 00:53:54.540]   I wrote it from it.
[00:53:54.540 --> 00:53:55.540]   Good.
[00:53:55.540 --> 00:53:56.540]   I'd love to hear about it.
[00:53:56.540 --> 00:53:57.540]   That's why I was on the show last week.
[00:53:57.540 --> 00:54:02.260]   A report from the front, from the field in just a moment.
[00:54:02.260 --> 00:54:05.620]   But first a word from our sponsor, Capeterra.
[00:54:05.620 --> 00:54:07.460]   We love the Capeterra.
[00:54:07.460 --> 00:54:08.460]   I love it.
[00:54:08.460 --> 00:54:11.780]   Not the least of the reasons I love it is because people call me all the time on the
[00:54:11.780 --> 00:54:15.180]   radio show and say things like, Oh, I run a yoga studio.
[00:54:15.180 --> 00:54:16.180]   What software should I use?
[00:54:16.180 --> 00:54:17.180]   I have to go.
[00:54:17.180 --> 00:54:22.260]   I don't know because business software, that's a whole different kettle of fish.
[00:54:22.260 --> 00:54:25.460]   Line of business software is really important to you in your business, whether you're a
[00:54:25.460 --> 00:54:28.420]   dentist, a veterinarian, a plumber, a yoga studio.
[00:54:28.420 --> 00:54:33.500]   But often we're using old, out of date software.
[00:54:33.500 --> 00:54:37.740]   How many people still run Windows XP and IEA because the software requires it?
[00:54:37.740 --> 00:54:40.700]   Or worse, Joey wrote it.
[00:54:40.700 --> 00:54:44.900]   He was the boss's nephew's first cousin.
[00:54:44.900 --> 00:54:47.180]   Joey has disappeared, but we're still using this software.
[00:54:47.180 --> 00:54:50.980]   No one knows how it works and no one can fix it.
[00:54:50.980 --> 00:54:52.140]   People suffer.
[00:54:52.140 --> 00:54:53.140]   No need to.
[00:54:53.140 --> 00:54:55.340]   Go to capeterra.com.
[00:54:55.340 --> 00:55:02.560]   Capeterra has tens of thousands of business programs in 700 categories.
[00:55:02.560 --> 00:55:08.500]   Some of the big categories like CRM, IT project management, e-commerce, link management tools,
[00:55:08.500 --> 00:55:13.900]   web conferencing, search engine optimization, and some more specific categories like yoga
[00:55:13.900 --> 00:55:15.620]   studio management.
[00:55:15.620 --> 00:55:17.940]   Choose the category you're looking for.
[00:55:17.940 --> 00:55:21.340]   And then you can, first of all, you'll find hundreds of programs in that category, which
[00:55:21.340 --> 00:55:22.340]   is amazing.
[00:55:22.340 --> 00:55:27.660]   You'll go a little further by filtering based on ratings, based on how the program works,
[00:55:27.660 --> 00:55:31.180]   where it's stored, if it's in the cloud or on your hard drive, that kind of stuff.
[00:55:31.180 --> 00:55:35.660]   How many, whether you can make appointments, whether, you know, I mean, how many seats
[00:55:35.660 --> 00:55:37.220]   you can have, all of that stuff.
[00:55:37.220 --> 00:55:38.820]   Put them side by side.
[00:55:38.820 --> 00:55:41.940]   And then here's the best part about capeterra.
[00:55:41.940 --> 00:55:49.220]   900,000 real reviews from real users with 1,000 more every single day.
[00:55:49.220 --> 00:55:54.500]   That means you're going to get up to date reviews from actual users, and there's no better
[00:55:54.500 --> 00:55:57.100]   way to know if a program is right for you.
[00:55:57.100 --> 00:56:00.980]   It is an amazing, amazing website.
[00:56:00.980 --> 00:56:03.140]   And I kind of buried the lead.
[00:56:03.140 --> 00:56:04.780]   You'd pay a lot for that, right?
[00:56:04.780 --> 00:56:05.780]   It's free.
[00:56:05.780 --> 00:56:06.780]   Free.
[00:56:06.780 --> 00:56:07.780]   Absolutely free.
[00:56:07.780 --> 00:56:08.780]   No, not freemium.
[00:56:08.780 --> 00:56:10.940]   No, you know, like, okay, now pay us.
[00:56:10.940 --> 00:56:11.940]   No, none of that.
[00:56:11.940 --> 00:56:12.940]   I hate that one that happens.
[00:56:12.940 --> 00:56:13.940]   Is it free?
[00:56:13.940 --> 00:56:14.940]   And then they want money.
[00:56:14.940 --> 00:56:16.580]   There's no, not a capeterra.
[00:56:16.580 --> 00:56:17.820]   It is absolutely free.
[00:56:17.820 --> 00:56:19.820]   Go to capeterra.com/twig.
[00:56:19.820 --> 00:56:21.820]   C-A-P-T-E-R-R-A.com/twig.
[00:56:21.820 --> 00:56:25.060]   I'm going to ask you a favor, though.
[00:56:25.060 --> 00:56:28.060]   Of course, you could just go to capeterra.com, and I think most people do.
[00:56:28.060 --> 00:56:31.820]   But if you want to help support this show, use just that special URL.
[00:56:31.820 --> 00:56:35.060]   You'll get the same site, but that way they know you saw it here.
[00:56:35.060 --> 00:56:37.820]   Capeterra.com/twig.
[00:56:37.820 --> 00:56:42.340]   Capeterra is software selection simplified.
[00:56:42.340 --> 00:56:47.220]   We thank you so much for making this week in Google possible.
[00:56:47.220 --> 00:56:50.900]   So Jarvis, what was this workshop?
[00:56:50.900 --> 00:56:59.860]   So Facebook has held, I think, five workshops they call them on their oversight board proposal,
[00:56:59.860 --> 00:57:05.060]   which Zuckerberg talked about a few months ago.
[00:57:05.060 --> 00:57:09.540]   Sometimes called their Supreme Court, a matter of appeal, but there's some debate about what
[00:57:09.540 --> 00:57:10.900]   it'll actually look like.
[00:57:10.900 --> 00:57:15.700]   So they brought together, they did, well, I think they've done in Asia, they did them,
[00:57:15.700 --> 00:57:18.260]   the next ones in Berlin, they did this one in New York.
[00:57:18.260 --> 00:57:29.020]   They invited 40 people, including me, academics, lawyers, civil society, critics of theirs
[00:57:29.020 --> 00:57:30.020]   were there.
[00:57:30.020 --> 00:57:32.580]   It was Shadow House rules, so I can't, I'm not supposed to say names.
[00:57:32.580 --> 00:57:33.580]   That's good.
[00:57:33.580 --> 00:57:36.020]   They had people who were unhappy.
[00:57:36.020 --> 00:57:37.020]   Oh, yeah.
[00:57:37.020 --> 00:57:38.020]   Yeah.
[00:57:38.020 --> 00:57:40.300]   People have been publicly critical of Facebook.
[00:57:40.300 --> 00:57:47.100]   And then the idea was to try to sus this notion of the oversight board.
[00:57:47.100 --> 00:57:51.500]   And as these Facebooks want, whatever they do, events, Matthew, were you with their local
[00:57:51.500 --> 00:57:55.580]   event in Denver, right?
[00:57:55.580 --> 00:57:56.580]   Matthew, you're muted.
[00:57:56.580 --> 00:57:57.580]   Is it us?
[00:57:57.580 --> 00:57:58.580]   Matthew.
[00:57:58.580 --> 00:57:59.580]   It's us.
[00:57:59.580 --> 00:58:00.580]   It's us?
[00:58:00.580 --> 00:58:01.580]   No, it's me.
[00:58:01.580 --> 00:58:02.580]   Oh, good.
[00:58:02.580 --> 00:58:03.580]   Thanks.
[00:58:03.580 --> 00:58:04.580]   Yeah.
[00:58:04.580 --> 00:58:08.420]   So they tend to overproduce things and fill up the schedule.
[00:58:08.420 --> 00:58:11.580]   And I think that was the problem with this one.
[00:58:11.580 --> 00:58:22.660]   But they took us through, interestingly, what all they do now to try to moderate content.
[00:58:22.660 --> 00:58:26.140]   And then took us through the oversight board.
[00:58:26.140 --> 00:58:28.220]   They gave us a nice little booklet.
[00:58:28.220 --> 00:58:33.740]   And in here, they've already published this publicly, the draft charter for the oversight
[00:58:33.740 --> 00:58:42.700]   board, and then all of their current community practices, community standards, which amount
[00:58:42.700 --> 00:58:47.620]   to about 40 pages of statutes, basically.
[00:58:47.620 --> 00:58:58.780]   So what they want to do is have as many as 40 people onto a universal board with, they
[00:58:58.780 --> 00:59:08.580]   think three person, like appellate courts, that would rule on things brought to the court.
[00:59:08.580 --> 00:59:11.100]   Clearly, there's an issue of scale here.
[00:59:11.100 --> 00:59:20.660]   Facebook gets a million reports a day of things that it then has 20 offices working in 50 languages,
[00:59:20.660 --> 00:59:24.940]   24/7 around the world, judging these stuff.
[00:59:24.940 --> 00:59:29.420]   And they judge it against the rules that they have.
[00:59:29.420 --> 00:59:37.980]   I read a really good legal paper, and I think the Harvard Law Review, by Kate Klannick, who
[00:59:37.980 --> 00:59:44.740]   really, really smart legal scholar, who quoted somebody else saying that what had happened
[00:59:44.740 --> 00:59:49.980]   with the platform is they went from a standards-based structure to a rules-based structure.
[00:59:49.980 --> 00:59:51.740]   Standards are, that's awful.
[00:59:51.740 --> 00:59:52.740]   That's evil.
[00:59:52.740 --> 00:59:53.740]   We don't like that.
[00:59:53.740 --> 00:59:54.740]   That doesn't fit here.
[00:59:54.740 --> 00:59:55.740]   That's what we see it.
[00:59:55.740 --> 00:59:59.340]   But that's exactly what Pinsker says.
[00:59:59.340 --> 01:00:01.420]   That doesn't scale because that's humans.
[01:00:01.420 --> 01:00:02.420]   Rules.
[01:00:02.420 --> 01:00:03.420]   Scales.
[01:00:03.420 --> 01:00:04.420]   So rules.
[01:00:04.420 --> 01:00:05.420]   Scales.
[01:00:05.420 --> 01:00:07.780]   And then the rules become ridiculous and the enforcement becomes ridiculous and you get
[01:00:07.780 --> 01:00:16.140]   problems like the napalm photo and the child, yes, in danger, yes, without clothes, yes,
[01:00:16.140 --> 01:00:17.140]   off.
[01:00:17.140 --> 01:00:18.300]   So do you still need humans?
[01:00:18.300 --> 01:00:19.300]   Is that the final?
[01:00:19.300 --> 01:00:20.980]   Well, they have humans.
[01:00:20.980 --> 01:00:23.660]   They have people in all these offices doing this.
[01:00:23.660 --> 01:00:26.460]   So they have three legs of the stool.
[01:00:26.460 --> 01:00:29.220]   They have one division sets policy.
[01:00:29.220 --> 01:00:30.300]   One division is product.
[01:00:30.300 --> 01:00:31.740]   They make the tools necessary.
[01:00:31.740 --> 01:00:32.980]   And one division is operations.
[01:00:32.980 --> 01:00:34.820]   They enforce the rules.
[01:00:34.820 --> 01:00:42.380]   And then there are appeals possible now within that structure, but now they're looking at
[01:00:42.380 --> 01:00:46.180]   having an external board.
[01:00:46.180 --> 01:00:49.220]   And the good news is, well, all right, one more thing.
[01:00:49.220 --> 01:00:54.300]   So the board's decisions would be final.
[01:00:54.300 --> 01:00:58.620]   Facebook could not appeal them in turn.
[01:00:58.620 --> 01:01:00.180]   And they can't fire somebody from the board.
[01:01:00.180 --> 01:01:01.180]   I don't know how I can.
[01:01:01.180 --> 01:01:03.460]   So this is the point of the board is have humans.
[01:01:03.460 --> 01:01:06.900]   Well, humans add it, but it's at a mental level.
[01:01:06.900 --> 01:01:10.060]   So then we also ask, we spend a lot of time asking, well, hold on.
[01:01:10.060 --> 01:01:13.900]   Is this just about content takedowns according to the rules you have?
[01:01:13.900 --> 01:01:16.100]   Or is this about having an influence on policy?
[01:01:16.100 --> 01:01:20.980]   Well, they're not going to let an external board make up policy, but they say it would
[01:01:20.980 --> 01:01:22.220]   have an influence on policy.
[01:01:22.220 --> 01:01:27.140]   But what is that influence unknown?
[01:01:27.140 --> 01:01:32.620]   And what struck me is the sincerity of what they do.
[01:01:32.620 --> 01:01:35.980]   You talk to the people who were actually overseeing enforcement of these rules.
[01:01:35.980 --> 01:01:37.460]   They gave us two case studies.
[01:01:37.460 --> 01:01:42.420]   The whole thing was chat about rule, which I presume everybody knows what that is, but
[01:01:42.420 --> 01:01:45.300]   for those who don't, you can take away any knowledge.
[01:01:45.300 --> 01:01:49.540]   You can't quote someone without their explicit permission and you can't say that someone
[01:01:49.540 --> 01:01:53.140]   who was there with other permission.
[01:01:53.140 --> 01:01:54.620]   They did try to get an NDA.
[01:01:54.620 --> 01:01:56.420]   I refused to sign it.
[01:01:56.420 --> 01:02:00.540]   That was going to be only about two case studies they had because those case studies were real
[01:02:00.540 --> 01:02:03.100]   life and they didn't want to get in trouble for privacy.
[01:02:03.100 --> 01:02:05.620]   Instead, they didn't require the NDA in the end.
[01:02:05.620 --> 01:02:06.620]   They just made it off the record.
[01:02:06.620 --> 01:02:07.980]   A great read was off the record.
[01:02:07.980 --> 01:02:10.420]   You wouldn't use anything from those two case studies.
[01:02:10.420 --> 01:02:15.260]   So they took us those case studies and then they had us say what we would think.
[01:02:15.260 --> 01:02:19.020]   And the room was split on both of them.
[01:02:19.020 --> 01:02:20.980]   60, 40 splits.
[01:02:20.980 --> 01:02:24.980]   These are not clear cut and easy decisions.
[01:02:24.980 --> 01:02:26.100]   And so they demonstrated that.
[01:02:26.100 --> 01:02:30.500]   I had to leave to go to Hamburg the night of the second thing.
[01:02:30.500 --> 01:02:33.380]   So I left before the very final discussion.
[01:02:33.380 --> 01:02:41.620]   And in the end, the problem here is, so in the working group I'm part of about internet
[01:02:41.620 --> 01:02:45.860]   regulation, which I've talked about in the show before, the one where we talked about
[01:02:45.860 --> 01:02:50.020]   the notion of starting an internet court to do with matters of legality.
[01:02:50.020 --> 01:02:57.860]   This in essence becomes a court dealing with matters that are legal but undesirable.
[01:02:57.860 --> 01:03:02.020]   And that would be within the company or external to the company, but not in courts.
[01:03:02.020 --> 01:03:10.180]   So you end up having these layered structures of courts trying to enforce rules.
[01:03:10.180 --> 01:03:13.220]   It's all pretty complex.
[01:03:13.220 --> 01:03:15.980]   And the reason is, as you say, Leo, it's scale.
[01:03:15.980 --> 01:03:18.660]   Otherwise you can't have an album or find something.
[01:03:18.660 --> 01:03:20.540]   You can't have staffs across the world.
[01:03:20.540 --> 01:03:24.060]   You know that functions at the level of Facebook.
[01:03:24.060 --> 01:03:25.060]   Still see how it's.
[01:03:25.060 --> 01:03:26.060]   I'm not sure either.
[01:03:26.060 --> 01:03:30.380]   I think what it does, Matthew, is it says that it's not it is more of a spring court
[01:03:30.380 --> 01:03:36.140]   in the sense that it says that they're not going to deal with nitty gritty appeals, but
[01:03:36.140 --> 01:03:38.340]   when they see an issue, they want to rule on.
[01:03:38.340 --> 01:03:44.420]   It's almost like spring court granting certiori that we think that this is off and we want
[01:03:44.420 --> 01:03:45.580]   to examine this.
[01:03:45.580 --> 01:03:48.620]   And we want some more research about the impact of this.
[01:03:48.620 --> 01:03:54.620]   And we want to say, no, that decision is wrong.
[01:03:54.620 --> 01:04:00.020]   You know, I think if you go back a few years, breastfeeding could have come to the court.
[01:04:00.020 --> 01:04:05.020]   You know, naked breasts are against the rules on Facebook period end of case.
[01:04:05.020 --> 01:04:12.380]   Then come and say, breastfeeding or breast cancer, wrong, no, or art.
[01:04:12.380 --> 01:04:13.380]   Oh boy.
[01:04:13.380 --> 01:04:15.060]   And Facebook says, we don't know what to do.
[01:04:15.060 --> 01:04:18.380]   And then there's a court that rules that says no breastfeeding is not what I want to
[01:04:18.380 --> 01:04:19.380]   serve.
[01:04:19.380 --> 01:04:23.380]   Because I'll tell you what, you would end up being the, the escape code for everything
[01:04:23.380 --> 01:04:24.380]   Facebook did.
[01:04:24.380 --> 01:04:28.260]   I thought so too, Leo, I asked a couple of really smart people in the room, one, one
[01:04:28.260 --> 01:04:32.940]   of a major lawyer, one, a kind of civil society person.
[01:04:32.940 --> 01:04:34.940]   I said, would you serve on this?
[01:04:34.940 --> 01:04:39.260]   And they surprisingly, accepting conflicts and anything else, they both said, yeah, they
[01:04:39.260 --> 01:04:40.660]   found the challenge fascinating.
[01:04:40.660 --> 01:04:41.660]   It's a challenge.
[01:04:41.660 --> 01:04:43.300]   Oh, it's a challenge.
[01:04:43.300 --> 01:04:47.100]   But I wonder, I mean, yeah, there, I take it on face value.
[01:04:47.100 --> 01:04:48.780]   They're all very sincere at Facebook and all this.
[01:04:48.780 --> 01:04:52.780]   But in the long run, it's just a way of saying here.
[01:04:52.780 --> 01:04:53.780]   Well, okay.
[01:04:53.780 --> 01:04:57.660]   So there's this big steamy pile of deliciousness.
[01:04:57.660 --> 01:04:58.660]   Right.
[01:04:58.660 --> 01:04:59.660]   Somebody to deal with.
[01:04:59.660 --> 01:05:00.660]   Exactly.
[01:05:00.660 --> 01:05:04.460]   Or yeah, it's either it's either outsourcing problems.
[01:05:04.460 --> 01:05:07.940]   It's a zero effort to get input from outside the place and accountability.
[01:05:07.940 --> 01:05:11.700]   That's a possibility one, possibly two is it's outsourcing the problems and sloughing
[01:05:11.700 --> 01:05:12.700]   them off.
[01:05:12.700 --> 01:05:13.700]   Possibly three.
[01:05:13.700 --> 01:05:14.700]   It's just PR.
[01:05:14.700 --> 01:05:15.700]   It's all three.
[01:05:15.700 --> 01:05:16.700]   I guarantee you it's all three of those things.
[01:05:16.700 --> 01:05:17.700]   Yeah.
[01:05:17.700 --> 01:05:19.700]   It's all depends on the mix.
[01:05:19.700 --> 01:05:20.700]   Yeah.
[01:05:20.700 --> 01:05:24.900]   Now Zuckerberg can say, Hey, if not my fault, we have problem solved.
[01:05:24.900 --> 01:05:27.220]   We asked our panel.
[01:05:27.220 --> 01:05:30.620]   And by the way, that it's interesting they chose such a large panel 40 people.
[01:05:30.620 --> 01:05:34.300]   Well, so you're under the whole damn world.
[01:05:34.300 --> 01:05:37.220]   It's large enough that it will never come to an agreement on.
[01:05:37.220 --> 01:05:42.100]   Well, no, you have three person pet three have three person judges.
[01:05:42.100 --> 01:05:45.340]   Do you want to give given case to how do they get across all of those?
[01:05:45.340 --> 01:05:46.340]   I don't have a tribunal.
[01:05:46.340 --> 01:05:48.980]   But this is also they had to do this to get diversity.
[01:05:48.980 --> 01:05:49.980]   Yeah.
[01:05:49.980 --> 01:05:53.700]   But who do you how who decides the three people is it lava lamps at cloud flare?
[01:05:53.700 --> 01:05:56.340]   They haven't decided that they haven't decided how to do that.
[01:05:56.340 --> 01:05:57.340]   Mark this.
[01:05:57.340 --> 01:05:58.340]   That's an issue.
[01:05:58.340 --> 01:05:59.340]   Mark this.
[01:05:59.340 --> 01:06:02.380]   So it's now meanwhile at the same time.
[01:06:02.380 --> 01:06:04.940]   I don't think we've talked about this show yet.
[01:06:04.940 --> 01:06:09.940]   The French came out with their regulatory proposal, which I don't think we've talked
[01:06:09.940 --> 01:06:10.940]   about the show.
[01:06:10.940 --> 01:06:11.940]   Have we?
[01:06:11.940 --> 01:06:13.780]   No, it's actually very interesting.
[01:06:13.780 --> 01:06:16.540]   And I met the guy who was in charge of it.
[01:06:16.540 --> 01:06:20.700]   And and I like him.
[01:06:20.700 --> 01:06:25.020]   So what the French said was they did an experiment with Facebook where they were embedded in
[01:06:25.020 --> 01:06:29.860]   Facebook for two months acting like a regulator trying to figure out how it would work.
[01:06:29.860 --> 01:06:33.300]   No regulations underlying this trying to understand how it would work.
[01:06:33.300 --> 01:06:39.420]   What they decided was that they did not want to try to regulate what actually appears on
[01:06:39.420 --> 01:06:41.620]   a Facebook.
[01:06:41.620 --> 01:06:48.100]   Instead they want to they want transparency and accountability as to what Facebook warrants
[01:06:48.100 --> 01:06:51.660]   it will do and whether it is effective in doing that or not.
[01:06:51.660 --> 01:06:54.660]   And that's where they hold them accountable, which means that Facebook would have to have
[01:06:54.660 --> 01:07:00.020]   a set of rules that they have a compact a covenant with the public.
[01:07:00.020 --> 01:07:03.540]   Facebook would have to provide data to the regulator so the data could judge what they
[01:07:03.540 --> 01:07:04.540]   do.
[01:07:04.540 --> 01:07:07.140]   And the regulator could say you're not doing a good enough job.
[01:07:07.140 --> 01:07:11.540]   You're not being accountable enough for transparent enough against what you said you would do.
[01:07:11.540 --> 01:07:16.020]   And what it would allow is for each service to have its own standards, which I think is
[01:07:16.020 --> 01:07:17.020]   smart.
[01:07:17.020 --> 01:07:20.180]   You're getting in the UK and in Germany, they're trying to get a universal standard across
[01:07:20.180 --> 01:07:23.220]   all services, which I think is is a fool's errand.
[01:07:23.220 --> 01:07:27.980]   However, the French are only going to deal with the three or four largest services with
[01:07:27.980 --> 01:07:33.540]   Facebook's companies, Google's and Twitter is pretty much and not deal right now with
[01:07:33.540 --> 01:07:34.540]   medium and small size.
[01:07:34.540 --> 01:07:37.060]   Of course, that's where the bad guys are all going to go.
[01:07:37.060 --> 01:07:38.860]   And so they're looking at the scale.
[01:07:38.860 --> 01:07:39.860]   There's the diagram.
[01:07:39.860 --> 01:07:44.020]   This is the chart.
[01:07:44.020 --> 01:07:50.340]   I complimented the person on his PowerPoint skills.
[01:07:50.340 --> 01:07:54.380]   So you have a lot of work now, so one more point than I'll stop, which is a lie, but
[01:07:54.380 --> 01:07:56.020]   I'll promise you this.
[01:07:56.020 --> 01:07:58.820]   Just like Facebook, promise.
[01:07:58.820 --> 01:08:05.580]   In the last meeting of the working group I'm on, what really hit me was a strong argument
[01:08:05.580 --> 01:08:10.940]   that no one should be none of these services should be trying, none of the governments
[01:08:10.940 --> 01:08:13.780]   should be trying to regulate content.
[01:08:13.780 --> 01:08:17.300]   When you do that, you end up with forbidden speech, danger danger.
[01:08:17.300 --> 01:08:25.900]   Number one, number two, from the content alone, you can't necessarily suss out the intent.
[01:08:25.900 --> 01:08:30.380]   And so you have a choice of regulating the actors, the behaviors or the content.
[01:08:30.380 --> 01:08:34.860]   And the content may be evidence in some cases, but the belief of the group that I'm part of,
[01:08:34.860 --> 01:08:40.860]   which includes this French regulator, is that behavior is where you go.
[01:08:40.860 --> 01:08:46.740]   And so, you know, was, because you look at the look at the Nancy Pelosi video, right?
[01:08:46.740 --> 01:08:48.980]   People who are saying they should take it down because it's false.
[01:08:48.980 --> 01:08:50.380]   That's a horrible standard.
[01:08:50.380 --> 01:08:51.380]   That's an awful standard.
[01:08:51.380 --> 01:08:53.260]   You don't want Facebook judging falsity.
[01:08:53.260 --> 01:08:57.820]   And there have been plenty of videos that slowed down Donald Trump to make fun of them
[01:08:57.820 --> 01:09:01.220]   and make him sound drunk, but tons of those.
[01:09:01.220 --> 01:09:04.580]   So is that content to be taken down?
[01:09:04.580 --> 01:09:08.340]   Well, what does that do to the public discussion?
[01:09:08.340 --> 01:09:12.740]   Is it instead of behavior that you say is not acceptable on a platform?
[01:09:12.740 --> 01:09:18.900]   If you're trying to manipulate people, the public discussion, that's hard.
[01:09:18.900 --> 01:09:21.020]   That's just as hard if not harder to do.
[01:09:21.020 --> 01:09:22.500]   That's what you're really trying to do.
[01:09:22.500 --> 01:09:27.100]   And in the end, in the end, all of these efforts are not trying to regulate platforms.
[01:09:27.100 --> 01:09:29.060]   They're not trying to regulate technology.
[01:09:29.060 --> 01:09:34.300]   They're trying to regulate human behavior through an intermediary.
[01:09:34.300 --> 01:09:38.420]   And it's the humans who are causing the problems here, not the technology.
[01:09:38.420 --> 01:09:39.420]   And you've got all these efforts.
[01:09:39.420 --> 01:09:44.540]   And part of the irony of this is very little of this goes to the source.
[01:09:44.540 --> 01:09:48.660]   What's happening now in Europe is that when the platforms take down something that is
[01:09:48.660 --> 01:09:52.820]   illegal, because in Germany, the platform must within 24 hours take down anything that
[01:09:52.820 --> 01:09:54.220]   is manifestly illegal.
[01:09:54.220 --> 01:09:58.100]   But what they're doing is they're taking it down under their community standards.
[01:09:58.100 --> 01:10:02.060]   And they're not reporting the illegal act to authorities.
[01:10:02.060 --> 01:10:07.460]   So all we're doing is regulating the intermediary and never going to the source of the illegal
[01:10:07.460 --> 01:10:08.700]   behavior.
[01:10:08.700 --> 01:10:10.660]   And we're trying to get rid of human behavior.
[01:10:10.660 --> 01:10:13.460]   But again, we're only doing it through the intermediary.
[01:10:13.460 --> 01:10:16.580]   All of which is to say, we ain't there yet.
[01:10:16.580 --> 01:10:18.660]   None of this makes sense.
[01:10:18.660 --> 01:10:19.820]   None of it.
[01:10:19.820 --> 01:10:24.140]   And there are these efforts by government to step in and say, in the case of the UK, we're
[01:10:24.140 --> 01:10:25.980]   going to get rid of illegal content.
[01:10:25.980 --> 01:10:30.300]   We're also going to make you get rid of legal but harmful content.
[01:10:30.300 --> 01:10:33.660]   That makes zero effing sense.
[01:10:33.660 --> 01:10:37.420]   And so you talked earlier, Leo, about how the platform is trying to convince you or the
[01:10:37.420 --> 01:10:40.460]   technology companies are going to credit convince you we can deliver the world.
[01:10:40.460 --> 01:10:43.540]   And all the regulators are trying to convince you, we have come up with the solution.
[01:10:43.540 --> 01:10:45.980]   Nobody has end of rant.
[01:10:45.980 --> 01:10:50.860]   This editorial from the MIT Technology Review, actually it's not editorial.
[01:10:50.860 --> 01:10:52.140]   It's a story.
[01:10:52.140 --> 01:10:57.820]   But it quotes media lab researcher, Hoseynn Dirac, Dirac, Dirac, Dirac, Dirac, Dirac,
[01:10:57.820 --> 01:11:04.300]   also known as Hoder, the, the, well, that's been for that.
[01:11:04.300 --> 01:11:06.020]   Spent six years in jail for blogging.
[01:11:06.020 --> 01:11:07.020]   What?
[01:11:07.020 --> 01:11:08.340]   What's your blog father in Iran?
[01:11:08.340 --> 01:11:09.340]   Oh, in Iran.
[01:11:09.340 --> 01:11:10.340]   Okay.
[01:11:10.340 --> 01:11:11.340]   Thank God.
[01:11:11.340 --> 01:11:12.340]   Not here.
[01:11:12.340 --> 01:11:13.340]   Okay.
[01:11:13.340 --> 01:11:14.340]   Also part Canadian.
[01:11:14.340 --> 01:11:15.340]   He calls it mal information.
[01:11:15.340 --> 01:11:17.260]   In other words, this is like the Pelosi video.
[01:11:17.260 --> 01:11:21.580]   True information that's been subtly manipulated to harm somebody.
[01:11:21.580 --> 01:11:27.220]   I'm not fully happy with his final conclusion, which is he, Hoder agrees it's important
[01:11:27.220 --> 01:11:29.540]   to label videos that have been fabricated in some way.
[01:11:29.540 --> 01:11:32.020]   And by the way, I will give him standing since he was in Iran.
[01:11:32.020 --> 01:11:34.460]   He probably has a lot of experience with this.
[01:11:34.460 --> 01:11:38.980]   But he says, the best solution is nurturing media literacy.
[01:11:38.980 --> 01:11:43.100]   That's where we always come down to is we just need a smarter constituency.
[01:11:43.100 --> 01:11:45.900]   I think the point is resilient.
[01:11:45.900 --> 01:11:46.900]   Yeah.
[01:11:46.900 --> 01:11:47.900]   Sorry.
[01:11:47.900 --> 01:11:53.540]   So Alexios, Alexios Mansarles, who used to run the fact checking network mentions that
[01:11:53.540 --> 01:12:00.500]   piece that the risk is if, if you say Facebook should take down a slightly manipulated video
[01:12:00.500 --> 01:12:06.300]   of Nancy Pelosi, then you really give them free rein to take down just about anything.
[01:12:06.300 --> 01:12:08.740]   If it's technically fake.
[01:12:08.740 --> 01:12:12.340]   Do you think that labeling it as they did was the right thing to do, Matthew?
[01:12:12.340 --> 01:12:14.620]   So my issue was with the labeling.
[01:12:14.620 --> 01:12:19.580]   I think they should have just said, you know, this is a manipulated video.
[01:12:19.580 --> 01:12:26.180]   Instead, they said there's further reporting on this video click here, which no one is
[01:12:26.180 --> 01:12:28.100]   ever going to do in a million years.
[01:12:28.100 --> 01:12:31.100]   But it is kind of up to the media, which they did.
[01:12:31.100 --> 01:12:35.060]   It's kind of, it's funny because we were talking about this before the show.
[01:12:35.060 --> 01:12:39.380]   When you were on Ari Malberg yesterday, I think was it you or Ari who said there were
[01:12:39.380 --> 01:12:43.300]   only 300 people who saw that video on Twitter initially.
[01:12:43.300 --> 01:12:49.540]   That was me because that was, yeah, that was quoting his stone.
[01:12:49.540 --> 01:12:55.580]   But after the media picked it up, of course, everybody saw it on the other hand.
[01:12:55.580 --> 01:12:57.580]   So that's one downside to media coverage.
[01:12:57.580 --> 01:13:01.580]   On the other hand, it was the media that said was quick to say this is manipulated footage.
[01:13:01.580 --> 01:13:04.500]   It was slowed down to make her look drunk.
[01:13:04.500 --> 01:13:07.300]   I think that was probably what needed to be said.
[01:13:07.300 --> 01:13:12.500]   Also, it's worth noting that the other video, the one that Trump retweeted, the one where
[01:13:12.500 --> 01:13:18.260]   they cobbled together bits and pieces of her mumbling so that it made her seem senile.
[01:13:18.260 --> 01:13:20.980]   That was created by Fox News as far as I know.
[01:13:20.980 --> 01:13:23.380]   So that's not Facebook's fault.
[01:13:23.380 --> 01:13:24.380]   Yeah.
[01:13:24.380 --> 01:13:25.660]   And yeah, you can't.
[01:13:25.660 --> 01:13:26.660]   You can't take it down.
[01:13:26.660 --> 01:13:36.060]   I remember Dvorak, when he first got his Amiga video editing, the video toaster, he took
[01:13:36.060 --> 01:13:42.100]   Bill Gates's keynotes from Comdex, edited everything but the gulps because Bill, I guess,
[01:13:42.100 --> 01:13:44.260]   was very audible swallowing.
[01:13:44.260 --> 01:13:45.260]   Nice.
[01:13:45.260 --> 01:13:49.420]   So it was a lot of them all together.
[01:13:49.420 --> 01:13:51.100]   Now that's funny.
[01:13:51.100 --> 01:13:53.980]   It's clearly manipulated.
[01:13:53.980 --> 01:13:57.300]   It smurches Bill Gates if you believe it.
[01:13:57.300 --> 01:13:59.420]   I mean, that's the problem.
[01:13:59.420 --> 01:14:02.140]   But this is also where the intent matters.
[01:14:02.140 --> 01:14:05.140]   The behavior matters, not the content.
[01:14:05.140 --> 01:14:06.140]   Right.
[01:14:06.140 --> 01:14:12.380]   But if you edited a video of Trump to make him sound even dumber, is that should that
[01:14:12.380 --> 01:14:13.380]   be taken down?
[01:14:13.380 --> 01:14:15.180]   MSNBC does that every day.
[01:14:15.180 --> 01:14:16.940]   This is their stock and trade.
[01:14:16.940 --> 01:14:17.940]   Right.
[01:14:17.940 --> 01:14:18.940]   Plus there's satire.
[01:14:18.940 --> 01:14:19.940]   There's.
[01:14:19.940 --> 01:14:20.940]   Yeah.
[01:14:20.940 --> 01:14:21.940]   So 2000, I'm over here.
[01:14:21.940 --> 01:14:26.340]   2016, put under other.
[01:14:26.340 --> 01:14:27.460]   There was a lot of this stuff going on.
[01:14:27.460 --> 01:14:29.900]   So it happened to Trump.
[01:14:29.900 --> 01:14:30.900]   Slow Trump.
[01:14:30.900 --> 01:14:32.900]   There you go.
[01:14:32.900 --> 01:14:33.900]   All right.
[01:14:33.900 --> 01:14:35.900]   Let's go to the tape.
[01:14:35.900 --> 01:14:36.900]   2016.
[01:14:36.900 --> 01:14:37.900]   And there wasn't.
[01:14:37.900 --> 01:14:38.900]   There was an outcry about this.
[01:14:38.900 --> 01:14:39.900]   I remember this.
[01:14:39.900 --> 01:14:40.900]   Yeah.
[01:14:40.900 --> 01:14:48.900]   But you can look at that and no, it's fake, right?
[01:14:48.900 --> 01:14:51.900]   Nobody even knows what a trillion is.
[01:14:51.900 --> 01:14:53.900]   Well, yes and no.
[01:14:53.900 --> 01:14:54.900]   You know what?
[01:14:54.900 --> 01:14:55.900]   It's not that far.
[01:14:55.900 --> 01:14:56.900]   It's so close to you.
[01:14:56.900 --> 01:15:07.900]   Did you ever get so close to a deal or a job that you don't really see it?
[01:15:07.900 --> 01:15:15.060]   I have to say this is for the left as funny as the Nancy Pelosi video is for the right.
[01:15:15.060 --> 01:15:16.660]   It's the same thing.
[01:15:16.660 --> 01:15:20.420]   So there was this outcry where we're heading toward a, what do you call it?
[01:15:20.420 --> 01:15:21.420]   The Majesty.
[01:15:21.420 --> 01:15:22.420]   What do you say that?
[01:15:22.420 --> 01:15:23.420]   Let's say Majesty.
[01:15:23.420 --> 01:15:24.420]   Let's say Majesty.
[01:15:24.420 --> 01:15:28.420]   Where people were basically saying, but it's the third most powerful person in the country
[01:15:28.420 --> 01:15:32.420]   and they're making lies about her.
[01:15:32.420 --> 01:15:33.420]   Yeah, that's democracy.
[01:15:33.420 --> 01:15:34.420]   Welcome.
[01:15:34.420 --> 01:15:35.420]   Welcome.
[01:15:35.420 --> 01:15:37.140]   Welcome to lots of countries are trying to outlaw that.
[01:15:37.140 --> 01:15:41.620]   I mean, yeah, we should remember spitting image, the British puppet show where they had
[01:15:41.620 --> 01:15:44.700]   all the world figures and made massive fun of.
[01:15:44.700 --> 01:15:48.540]   Well, my favorite thing Leo now is honest to God, the British government tried to get
[01:15:48.540 --> 01:15:51.100]   McDonald's to stop serving milkshakes.
[01:15:51.100 --> 01:15:52.100]   What why?
[01:15:52.100 --> 01:15:55.740]   Because the Euroshakes milkshakes at night.
[01:15:55.740 --> 01:15:56.740]   They were throwing.
[01:15:56.740 --> 01:16:01.260]   They honest to God, try to outlaw milkshakes.
[01:16:01.260 --> 01:16:02.260]   That's where we are.
[01:16:02.260 --> 01:16:03.260]   That's the only thing.
[01:16:03.260 --> 01:16:07.900]   Well, they tried to get them places when he was having a rally.
[01:16:07.900 --> 01:16:08.900]   Right.
[01:16:08.900 --> 01:16:09.900]   Never.
[01:16:09.900 --> 01:16:10.900]   Never.
[01:16:10.900 --> 01:16:11.900]   Never.
[01:16:11.900 --> 01:16:12.900]   Not like near the rally.
[01:16:12.900 --> 01:16:15.460]   My actually, my thought on that is let them throw milkshakes.
[01:16:15.460 --> 01:16:16.460]   It could be worse.
[01:16:16.460 --> 01:16:17.460]   Yeah.
[01:16:17.460 --> 01:16:21.860]   So, they're saying they did a tweet say we're still serving milkshakes, then they got in
[01:16:21.860 --> 01:16:22.860]   trouble for it.
[01:16:22.860 --> 01:16:24.820]   So there's a milkshake fricking war.
[01:16:24.820 --> 01:16:30.460]   Vice published a recipe for how to make a good throwing milkshake.
[01:16:30.460 --> 01:16:33.460]   Oh, it's one.
[01:16:33.460 --> 01:16:34.460]   It's thin.
[01:16:34.460 --> 01:16:36.860]   You don't want it too thick, right?
[01:16:36.860 --> 01:16:39.060]   Yeah, you want to do a little bit of a suit.
[01:16:39.060 --> 01:16:40.060]   Yeah.
[01:16:40.060 --> 01:16:41.060]   Yeah.
[01:16:41.060 --> 01:16:42.060]   He yelled at the security people.
[01:16:42.060 --> 01:16:43.060]   They could have stopped this.
[01:16:43.060 --> 01:16:45.260]   This one that by the way, this picture is captured.
[01:16:45.260 --> 01:16:51.140]   This picture is captured Nigel Farage dies of milkshake wounds.
[01:16:51.140 --> 01:16:54.300]   So the motto is lactose intolerance.
[01:16:54.300 --> 01:16:57.460]   Oh, I like it.
[01:16:57.460 --> 01:16:58.460]   You know what?
[01:16:58.460 --> 01:16:59.460]   That's clever.
[01:16:59.460 --> 01:17:00.460]   That's clever.
[01:17:00.460 --> 01:17:01.460]   That's a civil disobedience.
[01:17:01.460 --> 01:17:02.460]   I like it.
[01:17:02.460 --> 01:17:03.460]   It is.
[01:17:03.460 --> 01:17:04.460]   I mean, I wouldn't want to get hit.
[01:17:04.460 --> 01:17:06.420]   I want to endorse anybody doing that.
[01:17:06.420 --> 01:17:07.420]   But yeah.
[01:17:07.420 --> 01:17:09.420]   It depends on the flavor.
[01:17:09.420 --> 01:17:10.420]   Really.
[01:17:10.420 --> 01:17:13.540]   We've got to make people more resilient.
[01:17:13.540 --> 01:17:15.100]   It's now called media literacy.
[01:17:15.100 --> 01:17:17.020]   I don't think that's really the right way to look at it.
[01:17:17.020 --> 01:17:18.420]   That's about how do you do literacy?
[01:17:18.420 --> 01:17:19.420]   Yeah.
[01:17:19.420 --> 01:17:24.820]   But resiliency, if you live in Estonia or you live in Ukraine, you have a lot more resiliency.
[01:17:24.820 --> 01:17:25.900]   You know the people lie.
[01:17:25.900 --> 01:17:27.300]   You know you're going to be dubious.
[01:17:27.300 --> 01:17:28.300]   You know what's going on.
[01:17:28.300 --> 01:17:31.860]   When people say they don't trust social media, I see that's great news.
[01:17:31.860 --> 01:17:33.660]   That means that they're dubious.
[01:17:33.660 --> 01:17:34.660]   That's good.
[01:17:34.660 --> 01:17:35.660]   Good.
[01:17:35.660 --> 01:17:37.100]   And to me, the story about Finland.
[01:17:37.100 --> 01:17:38.100]   No.
[01:17:38.100 --> 01:17:39.100]   Go ahead.
[01:17:39.100 --> 01:17:40.900]   First, Matthew, finish your thought.
[01:17:40.900 --> 01:17:43.380]   I was just going to say the risk is.
[01:17:43.380 --> 01:17:48.340]   I think the point that Hussein and Alexios were making was if we give Facebook the power
[01:17:48.340 --> 01:17:53.940]   to take down things that are quote unquote untrue, it'll start taking down all kinds
[01:17:53.940 --> 01:17:54.940]   of stuff.
[01:17:54.940 --> 01:17:56.940]   It'll start taking down humor and commentary.
[01:17:56.940 --> 01:18:00.140]   I'll point out, but they have the power that we don't need to give them the power.
[01:18:00.140 --> 01:18:01.140]   Right.
[01:18:01.140 --> 01:18:02.140]   They're a private entity.
[01:18:02.140 --> 01:18:03.140]   Cheer them on.
[01:18:03.140 --> 01:18:05.380]   We should not encourage them to do so.
[01:18:05.380 --> 01:18:06.940]   Yeah, I agree.
[01:18:06.940 --> 01:18:08.780]   I mean, that's what everybody says about free speech.
[01:18:08.780 --> 01:18:12.140]   It means nothing to defend free speech unless it's something you don't agree with.
[01:18:12.140 --> 01:18:14.100]   And it's meaningful.
[01:18:14.100 --> 01:18:16.740]   You've got to defend Julian Assange.
[01:18:16.740 --> 01:18:18.220]   Exactly.
[01:18:18.220 --> 01:18:21.700]   You may not like him, but you got to defend him.
[01:18:21.700 --> 01:18:26.420]   And of course, now the US is about to throw the espionage act at him.
[01:18:26.420 --> 01:18:29.140]   And that really does seem to have a chilling effect.
[01:18:29.140 --> 01:18:35.600]   In fact, one media researcher who was it said this said, he's doing exactly what every
[01:18:35.600 --> 01:18:40.820]   investigative reporter in the world has done and to accuse him of espionage and threaten
[01:18:40.820 --> 01:18:44.860]   him with a lifetime in jail for it is a chilling to the first amendment.
[01:18:44.860 --> 01:18:49.180]   And so the government's argument is that he's not a real journalist.
[01:18:49.180 --> 01:18:50.180]   It doesn't count.
[01:18:50.180 --> 01:18:52.060]   Therefore, we can do this to him.
[01:18:52.060 --> 01:18:53.060]   No.
[01:18:53.060 --> 01:18:55.300]   But so now they get to find a real journalist.
[01:18:55.300 --> 01:18:57.780]   And what is the first thing they do in authoritarian states?
[01:18:57.780 --> 01:19:02.380]   They give you a journalism license license because if you've got a license, it can be
[01:19:02.380 --> 01:19:03.380]   revoked.
[01:19:03.380 --> 01:19:04.580]   What are they doing?
[01:19:04.580 --> 01:19:06.220]   Finland, Jeff.
[01:19:06.220 --> 01:19:08.060]   It's on the run down.
[01:19:08.060 --> 01:19:10.420]   But that wasn't the full story I found.
[01:19:10.420 --> 01:19:12.100]   I'm trying to find the actual story.
[01:19:12.100 --> 01:19:15.100]   What Finland teaches about dispelling fake news.
[01:19:15.100 --> 01:19:20.140]   So Finland can teach us many things in many areas.
[01:19:20.140 --> 01:19:24.340]   Finland against fake news.
[01:19:24.340 --> 01:19:25.340]   It's basically media literacy.
[01:19:25.340 --> 01:19:26.940]   I remember reading this piece.
[01:19:26.940 --> 01:19:27.940]   Yes.
[01:19:27.940 --> 01:19:31.220]   In the classroom, Finland has an effective weapon to fight fake news.
[01:19:31.220 --> 01:19:33.980]   Education, the Nordic Nation tops the list of European countries.
[01:19:33.980 --> 01:19:39.260]   Even the most resilient, there's your word, Jeff, to disinformation.
[01:19:39.260 --> 01:19:43.380]   Denmark second Netherlands the third, then Sweden, Estonia, Macedonia, Turkey.
[01:19:43.380 --> 01:19:44.380]   Oh, no.
[01:19:44.380 --> 01:19:45.380]   Sorry.
[01:19:45.380 --> 01:19:48.380]   At the bottom of the chart, Macedonia, Turkey and Albania.
[01:19:48.380 --> 01:19:52.100]   I was going to say that makes a lot more sense.
[01:19:52.100 --> 01:19:58.180]   And they're doing it with high quality education and having more educated people as a prerequisite
[01:19:58.180 --> 01:20:01.500]   for tackling the negative effects of fake news and post truth.
[01:20:01.500 --> 01:20:06.780]   The media literacy index's authors wrote, "While some regulation is necessary, education
[01:20:06.780 --> 01:20:08.860]   seems to be the best all around solution."
[01:20:08.860 --> 01:20:10.620]   Well, I couldn't disagree with that.
[01:20:10.620 --> 01:20:11.620]   Yeah, no, it's fine.
[01:20:11.620 --> 01:20:13.300]   We need to do a better job of education.
[01:20:13.300 --> 01:20:14.300]   I agree.
[01:20:14.300 --> 01:20:17.260]   We're doing a terrible job getting worse all the time in this country.
[01:20:17.260 --> 01:20:19.940]   That makes me very sad.
[01:20:19.940 --> 01:20:22.660]   We're taking money away from schools.
[01:20:22.660 --> 01:20:23.660]   Teachers are poorly paid.
[01:20:23.660 --> 01:20:27.220]   In fact, there's a real problem now with teachers leaving the field because they're
[01:20:27.220 --> 01:20:29.740]   paid so badly.
[01:20:29.740 --> 01:20:33.900]   And I don't think that that's planning for the future in any way.
[01:20:33.900 --> 01:20:42.260]   Let's take a break, then I'll show you the latest billboard in San Francisco.
[01:20:42.260 --> 01:20:44.620]   But first, a word from ExpressVPN.
[01:20:44.620 --> 01:20:48.620]   When you want to protect yourself online, I think you know by now the best way to do
[01:20:48.620 --> 01:20:51.580]   it, it was a virtual private network or a VPN.
[01:20:51.580 --> 01:20:58.100]   VPN runs on your system, whether it's a computer, a laptop, a phone, a tablet, encrypts all
[01:20:58.100 --> 01:21:03.100]   your transactions, all your conversation, everything that's going out of that computer.
[01:21:03.100 --> 01:21:08.020]   From your computer through the open Wi-Fi access point, past your Net Service Provider,
[01:21:08.020 --> 01:21:13.260]   all the way out to the VPN server, which then puts it out in the real world.
[01:21:13.260 --> 01:21:18.260]   So it's very important that you choose a VPN server and a VPN service that does right
[01:21:18.260 --> 01:21:24.740]   by you, that keeps your privacy, that's fast, easy to use, doesn't log, doesn't inject ads.
[01:21:24.740 --> 01:21:26.780]   That's ExpressVPN.
[01:21:26.780 --> 01:21:28.660]   It is the best.
[01:21:28.660 --> 01:21:29.660]   They do not log.
[01:21:29.660 --> 01:21:31.580]   They keep no information about you.
[01:21:31.580 --> 01:21:35.220]   They have servers all over the world so you can emerge under the public internet.
[01:21:35.220 --> 01:21:39.460]   That means no more geographic restrictions.
[01:21:39.460 --> 01:21:41.100]   Choose a server that's near you to be fast.
[01:21:41.100 --> 01:21:45.940]   Choose a server that's in another country to have access to their content, for instance.
[01:21:45.940 --> 01:21:51.100]   And if you're on public Wi-Fi, the simplest way to protect yourself from bad guys and
[01:21:51.100 --> 01:21:55.540]   your privacy is with a VPN from ExpressVPN.
[01:21:55.540 --> 01:22:01.300]   It secures and anonymizes your browsing, encrypts your data, hides your public IP address.
[01:22:01.300 --> 01:22:02.300]   Easy to use too.
[01:22:02.300 --> 01:22:03.780]   They've got apps for every platform.
[01:22:03.780 --> 01:22:07.140]   One button turns it on, one button turns it off.
[01:22:07.140 --> 01:22:11.900]   And it's less than $7 a month with a 30-day money back guarantee.
[01:22:11.900 --> 01:22:14.180]   I hope I've convinced you it's the best VPN out there.
[01:22:14.180 --> 01:22:18.660]   If you want a VPN and you should be using a VPN to protect your security and your privacy,
[01:22:18.660 --> 01:22:20.620]   there's no better than ExpressVPN.
[01:22:20.620 --> 01:22:22.500]   It's the safest way to surf than that.
[01:22:22.500 --> 01:22:29.380]   Find out how you can get three extra months free with a one-year package at expressvpn.com/twig.
[01:22:29.380 --> 01:22:32.900]   ExPR, ESS, spell it out.
[01:22:32.900 --> 01:22:35.540]   ExpressVPN.com/twig.
[01:22:35.540 --> 01:22:38.180]   Three extra months when you buy a one-year package.
[01:22:38.180 --> 01:22:41.820]   That's the best deal at ExpressVPN.com/twig.
[01:22:41.820 --> 01:22:46.580]   Thank you ExpressVPN for a great service, for supporting our privacy and security, and
[01:22:46.580 --> 01:22:48.220]   for supporting this week in Google.
[01:22:48.220 --> 01:22:51.020]   And thank you for using that URL.
[01:22:51.020 --> 01:22:54.820]   It's vpn.com/twig that shows them.
[01:22:54.820 --> 01:22:57.380]   You saw it here first.
[01:22:57.380 --> 01:22:58.540]   Elizabeth Warren's just went...
[01:22:58.540 --> 01:22:59.540]   Sorry?
[01:22:59.540 --> 01:23:02.460]   A little more perspective on the Finns, just one more added note.
[01:23:02.460 --> 01:23:03.460]   Yes.
[01:23:03.460 --> 01:23:06.260]   Wall Street Journal has a story about how they have to teach Finns small talk because they
[01:23:06.260 --> 01:23:07.260]   don't know how to do it.
[01:23:07.260 --> 01:23:09.420]   Yeah, Finns are gonna have a good time.
[01:23:09.420 --> 01:23:10.420]   Notorious.
[01:23:10.420 --> 01:23:11.420]   It's a bunch of...
[01:23:11.420 --> 01:23:12.420]   It's a nation of introverts.
[01:23:12.420 --> 01:23:14.260]   Oh my God, I want to move to Finland.
[01:23:14.260 --> 01:23:15.260]   I know.
[01:23:15.260 --> 01:23:16.260]   I know.
[01:23:16.260 --> 01:23:18.580]   That's why they invented this.
[01:23:18.580 --> 01:23:20.220]   That's why this Ericsson...
[01:23:20.220 --> 01:23:21.660]   I mean, that's why Selfa...
[01:23:21.660 --> 01:23:25.420]   That's why they pretty much started texting because they didn't want to see each other
[01:23:25.420 --> 01:23:27.300]   face to face.
[01:23:27.300 --> 01:23:30.020]   Now, those are all misnomers.
[01:23:30.020 --> 01:23:32.180]   Also cold and dark a lot of the time.
[01:23:32.180 --> 01:23:33.940]   Yeah, but I'll tell you what.
[01:23:33.940 --> 01:23:36.060]   It's the 4th of July on Helsinki.
[01:23:36.060 --> 01:23:37.740]   Man, that's a party.
[01:23:37.740 --> 01:23:38.740]   That is a party.
[01:23:38.740 --> 01:23:40.540]   The land of the midnight sun.
[01:23:40.540 --> 01:23:45.300]   I was in Helsinki in the middle of the summer and it was several times, actually.
[01:23:45.300 --> 01:23:49.380]   And those Finns know how to celebrate the sun.
[01:23:49.380 --> 01:23:54.780]   Elizabeth Warren has put a big old billboard up right in San Francisco's face.
[01:23:54.780 --> 01:23:56.980]   Break up big tech.
[01:23:56.980 --> 01:24:02.220]   Text tech to the short code to join our fight.
[01:24:02.220 --> 01:24:03.820]   It's an interesting place to put the billboard.
[01:24:03.820 --> 01:24:07.620]   It's a 4th in Townsend, which we used to...
[01:24:07.620 --> 01:24:10.180]   Right by where we used to work, Karsten.
[01:24:10.180 --> 01:24:11.980]   Now, but tech TV was down there.
[01:24:11.980 --> 01:24:12.980]   Yeah, that's...
[01:24:12.980 --> 01:24:15.740]   I mean, basically that's where Yahoo's billboard used to be.
[01:24:15.740 --> 01:24:16.740]   Is it?
[01:24:16.740 --> 01:24:17.740]   Do you think it's...
[01:24:17.740 --> 01:24:18.740]   No, it's not.
[01:24:18.740 --> 01:24:23.860]   It's not the old Yahoo billboard because look, in fact, when you look at it now, I think
[01:24:23.860 --> 01:24:27.140]   I'm not going to give you any money if that's where you think a billboard belongs.
[01:24:27.140 --> 01:24:28.140]   Oh, yeah.
[01:24:28.140 --> 01:24:30.500]   Who's this HD Buttercup?
[01:24:30.500 --> 01:24:32.500]   Now that's a billboard.
[01:24:32.500 --> 01:24:36.260]   Anyway, you can't even see it.
[01:24:36.260 --> 01:24:37.260]   Yeah.
[01:24:37.260 --> 01:24:38.260]   It's behind the bush.
[01:24:38.260 --> 01:24:40.900]   And it's facing the wrong way from the bus terminal and...
[01:24:40.900 --> 01:24:42.700]   Well, I can go on and on.
[01:24:42.700 --> 01:24:45.100]   Anyway, that's an interesting idea.
[01:24:45.100 --> 01:24:50.700]   I don't know, it's kind of baiting the lion, you know?
[01:24:50.700 --> 01:24:55.580]   Going to the center of the big tech universe and saying, "You ought to be broken up, but
[01:24:55.580 --> 01:24:57.620]   I bet it works."
[01:24:57.620 --> 01:25:00.020]   Certainly got our attention.
[01:25:00.020 --> 01:25:03.140]   Well, what was...
[01:25:03.140 --> 01:25:09.100]   So when I was in Congress two weeks ago, talking about Section 230...
[01:25:09.100 --> 01:25:11.180]   Wait a minute, what were you doing in Congress?
[01:25:11.180 --> 01:25:12.180]   We didn't hear about this.
[01:25:12.180 --> 01:25:15.220]   I'm asking a group I'm part of on Internet Regulation and had a visit to Congress.
[01:25:15.220 --> 01:25:16.220]   Oh, nice.
[01:25:16.220 --> 01:25:17.220]   It said it.
[01:25:17.220 --> 01:25:19.980]   So I met one senator and some staffers.
[01:25:19.980 --> 01:25:25.940]   And a lot of discussion from one of them about Section 230, which I can talk about more
[01:25:25.940 --> 01:25:32.500]   length in a minute because I just finished a book about it.
[01:25:32.500 --> 01:25:34.500]   Sorry.
[01:25:34.500 --> 01:25:36.740]   But that threw me off completely.
[01:25:36.740 --> 01:25:38.980]   What was I saying?
[01:25:38.980 --> 01:25:41.780]   You were in Congress talking to the senators.
[01:25:41.780 --> 01:25:43.700]   Representatives about...
[01:25:43.700 --> 01:25:48.620]   The fear is the Section 230 is going to die because you now have the right going after
[01:25:48.620 --> 01:25:51.020]   the tech platforms because they're being mean to the conservatives.
[01:25:51.020 --> 01:25:56.020]   So just to explain, Section 230 is the safe harbor protection of the Digital Money and
[01:25:56.020 --> 01:26:01.100]   Copyright Act, which means that platforms are protected against lawsuit.
[01:26:01.100 --> 01:26:04.500]   This is something that in Europe has now been thrown out.
[01:26:04.500 --> 01:26:05.500]   Right.
[01:26:05.500 --> 01:26:06.500]   And so there's a new book out.
[01:26:06.500 --> 01:26:07.500]   That was your football.
[01:26:07.500 --> 01:26:09.500]   The 26 words that created the Internet.
[01:26:09.500 --> 01:26:10.500]   Wow.
[01:26:10.500 --> 01:26:11.500]   Good book.
[01:26:11.500 --> 01:26:12.500]   By Jeff Kossup.
[01:26:12.500 --> 01:26:13.500]   It's a very good book.
[01:26:13.500 --> 01:26:14.500]   Did you read it to Matthew?
[01:26:14.500 --> 01:26:16.500]   I have skimmed it.
[01:26:16.500 --> 01:26:18.220]   I have not read every word.
[01:26:18.220 --> 01:26:21.140]   So he really breaks down Section 230.
[01:26:21.140 --> 01:26:25.460]   The thing that's really been irritating over the last little while is that even some media
[01:26:25.460 --> 01:26:29.260]   less have been getting Section 230 wrong, describing it incorrectly.
[01:26:29.260 --> 01:26:30.260]   Yes.
[01:26:30.260 --> 01:26:31.260]   Oh, I got it wrong.
[01:26:31.260 --> 01:26:32.260]   It's not the DMCA.
[01:26:32.260 --> 01:26:34.020]   It's the Communications Decency Act, first of all.
[01:26:34.020 --> 01:26:35.020]   Yeah.
[01:26:35.020 --> 01:26:36.020]   It's all the path to apologies.
[01:26:36.020 --> 01:26:37.020]   Yeah.
[01:26:37.020 --> 01:26:40.940]   So the computer or user of an interactive computer service shall be treated as a publisher or
[01:26:40.940 --> 01:26:45.380]   speaker of any information provided by another information content provider.
[01:26:45.380 --> 01:26:48.660]   In other words, the 26 words are not responsible.
[01:26:48.660 --> 01:26:51.500]   So it protects them from liability effectively.
[01:26:51.500 --> 01:26:57.940]   But there's this argument that the right in particular has been trying to push that
[01:26:57.940 --> 01:27:01.900]   requires the platforms to be neutral, which it doesn't.
[01:27:01.900 --> 01:27:02.900]   No, not wrong.
[01:27:02.900 --> 01:27:03.900]   It says you're not responsible.
[01:27:03.900 --> 01:27:07.540]   But I've seen multiple articles.
[01:27:07.540 --> 01:27:12.740]   Vox ran one that they had to correct, in which they said, you know, Facebook is really
[01:27:12.740 --> 01:27:16.380]   kind of walking a fine line because it's not being neutral.
[01:27:16.380 --> 01:27:21.940]   And that's, I mean, Section 230 was actually created to allow them to moderate things.
[01:27:21.940 --> 01:27:25.260]   So it's the exact opposite of the standard story.
[01:27:25.260 --> 01:27:28.900]   Ron Wyden who wrote this said what, Jeff?
[01:27:28.900 --> 01:27:33.220]   Exactly what Matthew just said, which is that they provided a shield and a sword with
[01:27:33.220 --> 01:27:37.820]   the expectation they would use both the shield from the liability so that they would allow
[01:27:37.820 --> 01:27:43.940]   conversation, but the sword that they would take responsibility for having a decent conversation.
[01:27:43.940 --> 01:27:44.940]   Wow.
[01:27:44.940 --> 01:27:46.820]   I like Ron Wyden.
[01:27:46.820 --> 01:27:48.500]   That's a very good description.
[01:27:48.500 --> 01:27:50.740]   It's a really last smart thing Congress did.
[01:27:50.740 --> 01:27:51.740]   Yeah.
[01:27:51.740 --> 01:27:52.740]   Yeah.
[01:27:52.740 --> 01:27:58.940]   It also actually said that that would hopefully create room for tools that users could use
[01:27:58.940 --> 01:28:03.580]   to moderate content themselves, which of course we have not seen any of.
[01:28:03.580 --> 01:28:04.580]   But that's the kind of thing.
[01:28:04.580 --> 01:28:06.780]   I think that on Twitter, a little bit of a footnote.
[01:28:06.780 --> 01:28:07.780]   Yeah.
[01:28:07.780 --> 01:28:08.780]   Exactly.
[01:28:08.780 --> 01:28:09.780]   I agree.
[01:28:09.780 --> 01:28:14.620]   So the fear now is that with the right and the left, both going after the platforms left
[01:28:14.620 --> 01:28:20.700]   because they're big evil capitalist companies, right because they think that they're being
[01:28:20.700 --> 01:28:24.940]   me to us and taking a stuff down that no one's going to stand up for 230.
[01:28:24.940 --> 01:28:31.500]   So the recently Fosca Cessa, whatever it was, was a carve out of 230 about child pornography
[01:28:31.500 --> 01:28:35.780]   or child sexual trafficking.
[01:28:35.780 --> 01:28:36.780]   Thank you.
[01:28:36.780 --> 01:28:41.060]   The fear is there'll be a next car route will be about opioids and the next car will be
[01:28:41.060 --> 01:28:45.220]   about bullying and the next car route will be about terrorism and then there'll be nothing
[01:28:45.220 --> 01:28:46.620]   left of 230.
[01:28:46.620 --> 01:28:50.940]   And then what you're going to find is that the platforms that enable conversation, including
[01:28:50.940 --> 01:28:57.340]   by the way, like Wikipedia, places like that, will be so fearful of liability that they
[01:28:57.340 --> 01:29:00.620]   will cut off the conversation that the Internet is amazing.
[01:29:00.620 --> 01:29:06.780]   So that was one of the arguments about Section 230, like Facebook and Google, can do whatever
[01:29:06.780 --> 01:29:07.780]   they want.
[01:29:07.780 --> 01:29:12.700]   They're massive companies, but Section 230 also protects everyone with a blog, everyone
[01:29:12.700 --> 01:29:16.660]   with a website, everyone with a newspaper with a comment section.
[01:29:16.660 --> 01:29:22.180]   This is a great infographic from the Electronic Frontier Foundation and Craig Newmark's Craig
[01:29:22.180 --> 01:29:26.060]   Connects on CDA 230.
[01:29:26.060 --> 01:29:30.620]   It talks about the law, but then it says, "What if there were no CDA 230 to protect online
[01:29:30.620 --> 01:29:32.300]   speech?"
[01:29:32.300 --> 01:29:33.780]   And here's the consequences.
[01:29:33.780 --> 01:29:37.180]   Sites like Huffington Post, Facebook, Twitter, Google+, and Reddit, well, they'll have to
[01:29:37.180 --> 01:29:42.180]   take off the Google+, should be sued every time a user crossed the line.
[01:29:42.180 --> 01:29:45.940]   Innovation would be diminished as new social media platforms and products would face new
[01:29:45.940 --> 01:29:49.060]   legal obstacles to getting started.
[01:29:49.060 --> 01:29:53.260]   Sites would have to pay for lawyers to review content every post.
[01:29:53.260 --> 01:30:00.140]   Every review service, every Yelp or TripAdvisor, or even just a website about restaurant.
[01:30:00.140 --> 01:30:04.060]   Even Internet service providers would have to censor content.
[01:30:04.060 --> 01:30:10.500]   And they do have examples of countries that don't have this protection, Turkey and Thailand,
[01:30:10.500 --> 01:30:12.500]   what the consequences are.
[01:30:12.500 --> 01:30:16.940]   And there actually was a case recently that I was reading about that I'm pretty sure involved
[01:30:16.940 --> 01:30:17.940]   a Yelp review.
[01:30:17.940 --> 01:30:22.700]   I think it was a restaurant or someone who provided a service or a dentist or something.
[01:30:22.700 --> 01:30:27.340]   And they wanted to sue commenters on the site.
[01:30:27.340 --> 01:30:29.860]   That's a classic section 230 case.
[01:30:29.860 --> 01:30:32.940]   Well, let's defend the book.
[01:30:32.940 --> 01:30:36.900]   Once again, the 26 words that created the Internet, Jeff Kossoff, I think it'd be a
[01:30:36.900 --> 01:30:39.900]   great triangulation guest.
[01:30:39.900 --> 01:30:41.100]   Let the Harvard boy know.
[01:30:41.100 --> 01:30:46.100]   Jeff Kossoff, K-O-S-S-E-F-F.
[01:30:46.100 --> 01:30:50.500]   He is an assistant professor at the U.S. Naval Academy's Cyber Science Department.
[01:30:50.500 --> 01:30:51.900]   Wow, interesting.
[01:30:51.900 --> 01:30:53.620]   All right.
[01:30:53.620 --> 01:30:57.340]   I'm all for it.
[01:30:57.340 --> 01:30:58.340]   Make it so.
[01:30:58.340 --> 01:31:01.140]   Yeah, we'll work on getting him.
[01:31:01.140 --> 01:31:02.140]   That's great.
[01:31:02.140 --> 01:31:06.180]   By the way, a little word of warning, Mark Zuckerberg and Cheryl Sandberg.
[01:31:06.180 --> 01:31:11.180]   Do not go visit Matthew Ingram's Lake cabin.
[01:31:11.180 --> 01:31:14.180]   I know the canoeing looks fine, but you can only come.
[01:31:14.180 --> 01:31:15.180]   They can come.
[01:31:15.180 --> 01:31:16.500]   They just can't tell anybody.
[01:31:16.500 --> 01:31:21.700]   They might be arrested because they snubbed international lawmakers again in a committee
[01:31:21.700 --> 01:31:24.380]   hearing in Canada yesterday.
[01:31:24.380 --> 01:31:27.380]   And was there a warrant issued for their arrest?
[01:31:27.380 --> 01:31:28.900]   So there was a subpoena.
[01:31:28.900 --> 01:31:29.900]   Okay.
[01:31:29.900 --> 01:31:31.940]   And then there was a summons.
[01:31:31.940 --> 01:31:35.060]   And so the summons effectively is open ended.
[01:31:35.060 --> 01:31:39.860]   So as soon as they enter the country, the summons takes effect and they are to report
[01:31:39.860 --> 01:31:40.860]   to Parliament.
[01:31:40.860 --> 01:31:41.860]   Wow.
[01:31:41.860 --> 01:31:44.700]   And if they don't, then they can be cited for contempt.
[01:31:44.700 --> 01:31:47.300]   And what happens when you're cited?
[01:31:47.300 --> 01:31:49.780]   What happens when you're cited for contempt?
[01:31:49.780 --> 01:31:50.780]   Absolutely nothing.
[01:31:50.780 --> 01:31:51.780]   Oh, right.
[01:31:51.780 --> 01:31:53.860]   But yeah.
[01:31:53.860 --> 01:31:59.340]   So it sounds a lot worse than it is in practice.
[01:31:59.340 --> 01:32:02.580]   Being Canada, the Parliament will apologize to them.
[01:32:02.580 --> 01:32:03.580]   Sorry.
[01:32:03.580 --> 01:32:05.500]   So technically we could throw them in jail, but that's.
[01:32:05.500 --> 01:32:09.500]   Well, it's not much different from the situation here where people pretty much just say to
[01:32:09.500 --> 01:32:11.500]   Congress and not.
[01:32:11.500 --> 01:32:12.500]   Yeah.
[01:32:12.500 --> 01:32:13.500]   I don't think so.
[01:32:13.500 --> 01:32:14.500]   Not today.
[01:32:14.500 --> 01:32:20.900]   My favorite part was the guy, the co-chair of the committee, the grand committee, it's
[01:32:20.900 --> 01:32:26.620]   called that was having the hearings, the guy that Facebook sent.
[01:32:26.620 --> 01:32:32.460]   The co-chair said, but you're not even in the top 100 top Facebook executives.
[01:32:32.460 --> 01:32:33.460]   What the heck?
[01:32:33.460 --> 01:32:34.460]   They're hurt.
[01:32:34.460 --> 01:32:35.460]   They just hurt.
[01:32:35.460 --> 01:32:38.300]   Well, the committee's name is what's the, do you have the full name?
[01:32:38.300 --> 01:32:39.580]   It's so, it's so grand.
[01:32:39.580 --> 01:32:40.580]   It's so, so important.
[01:32:40.580 --> 01:32:45.620]   The international grand committee on data, privacy and wow, disinformation, I think.
[01:32:45.620 --> 01:32:46.620]   Wow.
[01:32:46.620 --> 01:32:47.620]   The grand committee.
[01:32:47.620 --> 01:32:48.820]   All capitalized, of course.
[01:32:48.820 --> 01:32:49.820]   Yeah.
[01:32:49.820 --> 01:32:50.820]   Because there's multiple countries.
[01:32:50.820 --> 01:32:52.500]   It's not just Canada.
[01:32:52.500 --> 01:32:56.100]   So we've been talking a lot about the Huawei situation.
[01:32:56.100 --> 01:32:57.100]   Mm hmm.
[01:32:57.100 --> 01:32:59.940]   I've been wanting you to explain this to me more.
[01:32:59.940 --> 01:33:01.260]   What should I think of Huawei?
[01:33:01.260 --> 01:33:02.260]   I have one.
[01:33:02.260 --> 01:33:04.820]   I'm concerned.
[01:33:04.820 --> 01:33:06.820]   So that's really the real question.
[01:33:06.820 --> 01:33:10.500]   You know, essentially at this point, Huawei is banned.
[01:33:10.500 --> 01:33:15.740]   Actually, US companies are banned from doing business with Huawei.
[01:33:15.740 --> 01:33:21.500]   It started, of course, it started in 2012 when the Congress advised US companies and
[01:33:21.500 --> 01:33:26.540]   the defense intelligence community, the advised companies.
[01:33:26.540 --> 01:33:29.980]   Be careful about working with several Chinese companies.
[01:33:29.980 --> 01:33:31.980]   Huawei was one of them.
[01:33:31.980 --> 01:33:32.980]   ZTE was a Xiaomi.
[01:33:32.980 --> 01:33:33.980]   I can't remember.
[01:33:33.980 --> 01:33:38.460]   Of course, the drum beat got stronger a few months ago with 5G.
[01:33:38.460 --> 01:33:43.620]   We went around briefing our allies saying, really, you shouldn't use Huawei networking
[01:33:43.620 --> 01:33:44.820]   gear.
[01:33:44.820 --> 01:33:51.580]   And then it really turned up, the heat turned up last week when a number of US companies
[01:33:51.580 --> 01:33:56.900]   were forbidden from doing business with Huawei, meaning Huawei couldn't get parts.
[01:33:56.900 --> 01:34:01.300]   People complied by saying, okay, no more Android for you, no more updates.
[01:34:01.300 --> 01:34:08.020]   There was a 90-day kind of relenting, but it says temporary.
[01:34:08.020 --> 01:34:12.980]   Qualcomm said, okay, no more chips for you than ARM, a UK company said, okay, you can't
[01:34:12.980 --> 01:34:15.060]   use our designs anymore.
[01:34:15.060 --> 01:34:17.900]   Huawei said, well, that's okay.
[01:34:17.900 --> 01:34:19.140]   We've been preparing for this.
[01:34:19.140 --> 01:34:20.140]   We knew this was going to happen.
[01:34:20.140 --> 01:34:21.460]   We saw what happened to ZTE.
[01:34:21.460 --> 01:34:23.820]   We've been stockpiling parts.
[01:34:23.820 --> 01:34:29.180]   They even were forbidden briefly from using SD cards in the phones.
[01:34:29.180 --> 01:34:31.340]   By the way, that changed for some reason.
[01:34:31.340 --> 01:34:33.260]   Oh, yeah, that's right.
[01:34:33.260 --> 01:34:36.780]   Panasonic said no more batteries for you.
[01:34:36.780 --> 01:34:39.420]   So it got really serious.
[01:34:39.420 --> 01:34:44.460]   There's something certain though, because first of all, China has a nuclear option.
[01:34:44.460 --> 01:34:45.460]   They can respond.
[01:34:45.460 --> 01:34:53.220]   In fact, already, you know, JP Morgan said yesterday that because Chinese people are
[01:34:53.220 --> 01:34:59.140]   very protective of Huawei, apparently, that Apple, their analysts said Apple should expect
[01:34:59.140 --> 01:35:06.020]   a severe depletion in sales of iPhones in China because the Chinese people will buy
[01:35:06.020 --> 01:35:09.300]   Huawei phones in defensive Huawei.
[01:35:09.300 --> 01:35:15.340]   And JP Morgan said, probably your market in China will be cut in half Apple.
[01:35:15.340 --> 01:35:17.780]   That is 30% of Apple's iPhone.
[01:35:17.780 --> 01:35:19.780]   So it's a big chunk.
[01:35:19.780 --> 01:35:20.780]   30%.
[01:35:20.780 --> 01:35:23.740]   I'd like it to be more.
[01:35:23.740 --> 01:35:31.740]   Then I saw a story today that the Department of Homeland Security is warning that drones
[01:35:31.740 --> 01:35:36.900]   manufactured by Chinese companies, the DJI drone is probably the most popular drone in
[01:35:36.900 --> 01:35:39.100]   the world I had to run.
[01:35:39.100 --> 01:35:43.220]   Unfortunately, a lot of government agencies apparently are using them.
[01:35:43.220 --> 01:35:47.740]   The Interior Department tracked lava flows that kill away oil companies fly them over
[01:35:47.740 --> 01:35:51.980]   pipelines, electric utilities, use them to inspect transmission lines.
[01:35:51.980 --> 01:35:58.180]   DHS says those drones could be gathering information.
[01:35:58.180 --> 01:36:03.980]   Somebody a little smarter pointed out that, well, you don't really need the drones because
[01:36:03.980 --> 01:36:09.100]   we've got Chinese have satellites over all of this stuff anyway.
[01:36:09.100 --> 01:36:10.700]   And I think this is part of the problem.
[01:36:10.700 --> 01:36:15.540]   The President muddy the waters considerably this week by at first saying, well, it's
[01:36:15.540 --> 01:36:16.780]   a big security problem.
[01:36:16.780 --> 01:36:21.740]   And then in the same sentence saying, but if we can fix our trade problems with China,
[01:36:21.740 --> 01:36:22.740]   all will be forgiven.
[01:36:22.740 --> 01:36:23.740]   Right.
[01:36:23.740 --> 01:36:24.740]   Well, I include while we're-
[01:36:24.740 --> 01:36:27.620]   Is it a trade problem or is it a security problem?
[01:36:27.620 --> 01:36:30.380]   This is extremely unclear.
[01:36:30.380 --> 01:36:31.660]   I don't know what to say.
[01:36:31.660 --> 01:36:37.860]   We've yet to see real for A, I would say this, there's lots of evidence from a lot of sources
[01:36:37.860 --> 01:36:44.460]   going way back that Huawei has always been kind of nasty and highly competitive years
[01:36:44.460 --> 01:36:50.420]   long accusations of thefts, dubious ethics.
[01:36:50.420 --> 01:36:54.460]   Father Robert told us stories about Huawei executives wandering trade show floors with
[01:36:54.460 --> 01:36:59.820]   a camera taking pictures of competitors circuit boards.
[01:36:59.820 --> 01:37:03.580]   But that doesn't rise to the level of-
[01:37:03.580 --> 01:37:04.580]   How about you?
[01:37:04.580 --> 01:37:05.580]   Side-brake.
[01:37:05.580 --> 01:37:06.580]   Yeah.
[01:37:06.580 --> 01:37:07.580]   I mean, well, I don't know.
[01:37:07.580 --> 01:37:17.220]   But this is the real problem is that if you're going to pick one company out of China, you're
[01:37:17.220 --> 01:37:20.100]   going to have problems.
[01:37:20.100 --> 01:37:24.620]   You still have plenty- For instance, Huawei owns and has put in a considerable number
[01:37:24.620 --> 01:37:28.140]   of the sub-undersea cables.
[01:37:28.140 --> 01:37:32.860]   So a lot of data traffic goes through Huawei cables anyway.
[01:37:32.860 --> 01:37:33.860]   Oh.
[01:37:33.860 --> 01:37:36.140]   So there's A, there's not anything-
[01:37:36.140 --> 01:37:37.620]   Anything going into China is going to be-
[01:37:37.620 --> 01:37:38.620]   Yeah.
[01:37:38.620 --> 01:37:39.620]   There's not much you can do.
[01:37:39.620 --> 01:37:41.780]   Well, no, these cables aren't just to China.
[01:37:41.780 --> 01:37:45.420]   They're all over the world.
[01:37:45.420 --> 01:37:49.260]   So you've got a lot of- This is a complicated issue.
[01:37:49.260 --> 01:37:54.340]   We've never been shown evidence that Huawei has engaged in espionage, but it is the case
[01:37:54.340 --> 01:37:58.740]   that the Chinese government has the right to tell Huawei what to do, whether they own
[01:37:58.740 --> 01:38:06.140]   Huawei or not, Huawei says a US blacklist of Huawei will harm billions of consumers,
[01:38:06.140 --> 01:38:07.140]   including Americans.
[01:38:07.140 --> 01:38:11.980]   They had a press conference today.
[01:38:11.980 --> 01:38:12.980]   Talk about this.
[01:38:12.980 --> 01:38:13.980]   So there's-
[01:38:13.980 --> 01:38:16.620]   Huawei's very upset about it, as you might expect.
[01:38:16.620 --> 01:38:20.060]   I just want to know whether I'm going to get updates to my P20 Pro.
[01:38:20.060 --> 01:38:21.060]   No.
[01:38:21.060 --> 01:38:22.060]   Well, as of-
[01:38:22.060 --> 01:38:23.060]   Well, I thought-
[01:38:23.060 --> 01:38:24.060]   I thought the next three months-
[01:38:24.060 --> 01:38:25.060]   I thought the next three months-
[01:38:25.060 --> 01:38:26.060]   Yes, we will.
[01:38:26.060 --> 01:38:27.060]   But then we're out of luck.
[01:38:27.060 --> 01:38:30.260]   If this goes through, you will not.
[01:38:30.260 --> 01:38:31.260]   Google will not update.
[01:38:31.260 --> 01:38:36.980]   A lot of P20 owners are selling their phones at a great discount on eBay, which is too bad
[01:38:36.980 --> 01:38:39.860]   because that and its successor were great phones.
[01:38:39.860 --> 01:38:40.860]   P30 is a great phone.
[01:38:40.860 --> 01:38:41.860]   Yeah, cameras are the real-
[01:38:41.860 --> 01:38:42.860]   Yeah, the main story.
[01:38:42.860 --> 01:38:48.980]   Actually, if you're looking for a graduation gift, it's a great deal on them right now.
[01:38:48.980 --> 01:38:50.900]   They were security updates, but-
[01:38:50.900 --> 01:38:51.900]   Damn.
[01:38:51.900 --> 01:38:52.900]   But-
[01:38:52.900 --> 01:38:58.780]   So there's a Huawei PR guy who's coming to me a couple times over the years, which is
[01:38:58.780 --> 01:39:03.580]   to say, I guess they're having no luck getting the real media so they get their down to me.
[01:39:03.580 --> 01:39:05.940]   And I mentioned this to you, I think about a year and a half, two years ago.
[01:39:05.940 --> 01:39:07.140]   I said, "They want us to come to-
[01:39:07.140 --> 01:39:08.140]   Yep.
[01:39:08.140 --> 01:39:09.140]   " Coverage is something that you said-
[01:39:09.140 --> 01:39:11.300]   Uhh, I said, "Yeah, you're right now."
[01:39:11.300 --> 01:39:14.460]   But that's one of the things they're accused of is-
[01:39:14.460 --> 01:39:18.340]   Oh, and not only that, they would fly journalists all around the world.
[01:39:18.340 --> 01:39:19.340]   Right.
[01:39:19.340 --> 01:39:20.340]   Everybody does that.
[01:39:20.340 --> 01:39:21.340]   So does everybody else.
[01:39:21.340 --> 01:39:22.860]   I refuse it, of course.
[01:39:22.860 --> 01:39:28.740]   It's a matter of ethics, but every company I buy offers from every company to go to Taipei,
[01:39:28.740 --> 01:39:31.540]   to go to their factories to go all over the place.
[01:39:31.540 --> 01:39:34.580]   And yes, we got offers from Huawei in the past.
[01:39:34.580 --> 01:39:37.260]   But I- We've gotten offers from LG in the past.
[01:39:37.260 --> 01:39:38.260]   Everybody.
[01:39:38.260 --> 01:39:39.260]   Everybody.
[01:39:39.260 --> 01:39:42.420]   And they're coming out with their own OS, right?
[01:39:42.420 --> 01:39:43.420]   Well-
[01:39:43.420 --> 01:39:44.420]   Well-
[01:39:44.420 --> 01:39:47.660]   So they could still use the open source version of Android.
[01:39:47.660 --> 01:39:49.380]   They just can't use Google services.
[01:39:49.380 --> 01:39:50.860]   But they don't-
[01:39:50.860 --> 01:39:52.220]   But they don't in China anyway.
[01:39:52.220 --> 01:39:56.140]   So they already have- In China, they have their own store.
[01:39:56.140 --> 01:39:57.140]   They have their own services.
[01:39:57.140 --> 01:39:58.140]   Right.
[01:39:58.140 --> 01:39:59.820]   So they could easily roll those out worldwide.
[01:39:59.820 --> 01:40:00.820]   I don't-
[01:40:00.820 --> 01:40:04.140]   So says their own OS is rolling out later this year.
[01:40:04.140 --> 01:40:07.500]   These same restrictions pretty much put ZTE out of business last year.
[01:40:07.500 --> 01:40:10.820]   I don't think it bodes well for Huawei.
[01:40:10.820 --> 01:40:17.260]   But the real question, and I don't know what the answer is, is this part of a trade war?
[01:40:17.260 --> 01:40:18.260]   Is this even-
[01:40:18.260 --> 01:40:19.260]   Sure phase, I would.
[01:40:19.260 --> 01:40:23.540]   Is it anti-Chinese sentiment or is there a legitimate cause for consumer?
[01:40:23.540 --> 01:40:24.540]   That's what we're trying to figure out.
[01:40:24.540 --> 01:40:26.540]   Huawei does not seem like a very cuddly company.
[01:40:26.540 --> 01:40:29.140]   No, there's a lot of reason to blame them.
[01:40:29.140 --> 01:40:30.140]   There's a lot of reason.
[01:40:30.140 --> 01:40:31.140]   Yeah.
[01:40:31.140 --> 01:40:32.940]   A lot of reason to blame them.
[01:40:32.940 --> 01:40:34.300]   It feels like a trade war to me.
[01:40:34.300 --> 01:40:37.220]   There's been no- Unless they're just keeping it all secret.
[01:40:37.220 --> 01:40:43.580]   There's been no conclusive evidence that Huawei has spied on anyone as far as I can tell.
[01:40:43.580 --> 01:40:48.580]   But we should point out, there's a lot to be said for the fact that Huawei could and might
[01:40:48.580 --> 01:40:51.380]   be ordered to do so by the Chinese government.
[01:40:51.380 --> 01:40:55.060]   I think it's reasonable to say try not to use 5G gear from Huawei, even though that's
[01:40:55.060 --> 01:41:00.020]   the number one company and the best, most affordable choice.
[01:41:00.020 --> 01:41:03.020]   Yeah, I agree on the networking.
[01:41:03.020 --> 01:41:04.020]   On the networking.
[01:41:04.020 --> 01:41:05.020]   Yeah.
[01:41:05.020 --> 01:41:06.580]   But the phones-
[01:41:06.580 --> 01:41:07.580]   Claptops.
[01:41:07.580 --> 01:41:09.380]   Keep them in selling phones to Europe.
[01:41:09.380 --> 01:41:10.380]   Right.
[01:41:10.380 --> 01:41:11.940]   I mean, we don't buy phones from them anyways.
[01:41:11.940 --> 01:41:12.940]   Right.
[01:41:12.940 --> 01:41:14.100]   They're not selling America.
[01:41:14.100 --> 01:41:17.580]   They had planned to do that a year and a half ago and they were cut off by the Commerce
[01:41:17.580 --> 01:41:18.580]   Department.
[01:41:18.580 --> 01:41:25.780]   So are they evil empire or not?
[01:41:25.780 --> 01:41:26.780]   I think it's true.
[01:41:26.780 --> 01:41:28.300]   I think both are true.
[01:41:28.300 --> 01:41:37.180]   I think the administration is using Huawei as a poster child and hopes that by bludgeoning
[01:41:37.180 --> 01:41:40.940]   Huawei- I'm Trump basically said this, hoped that by bludgeoning Huawei they could force
[01:41:40.940 --> 01:41:43.100]   the Chinese government to make a trade deal.
[01:41:43.100 --> 01:41:44.100]   Right.
[01:41:44.100 --> 01:41:45.100]   You said-
[01:41:45.100 --> 01:41:46.580]   And to forget a deal, if they get a deal, Huawei's back in.
[01:41:46.580 --> 01:41:48.340]   It's suddenly all their phones.
[01:41:48.340 --> 01:41:51.300]   Nevertheless, it's completely reasonable to be concerned about Huawei.
[01:41:51.300 --> 01:41:56.020]   The problem I have with that is, it's reasonably concerned with any Chinese company.
[01:41:56.020 --> 01:41:58.220]   All of our stuff has been in China.
[01:41:58.220 --> 01:41:59.220]   It's not a lot of-
[01:41:59.220 --> 01:42:00.220]   No.
[01:42:00.220 --> 01:42:06.900]   I mean, the worst thing we could do at this point would be to cut off American companies
[01:42:06.900 --> 01:42:09.460]   selling to China and Chinese companies selling to us.
[01:42:09.460 --> 01:42:11.020]   Coming up a big wall.
[01:42:11.020 --> 01:42:14.660]   Like 80% of routers are Chinese men.
[01:42:14.660 --> 01:42:15.660]   Yeah.
[01:42:15.660 --> 01:42:17.660]   But we might have a bigger problem.
[01:42:17.660 --> 01:42:25.140]   I mean, Ben Thompson pointed out China took the first shots in the trade war.
[01:42:25.140 --> 01:42:26.940]   It threw out a bunch of US tech firms.
[01:42:26.940 --> 01:42:28.500]   So maybe it's time that we respond.
[01:42:28.500 --> 01:42:34.860]   I mean, look what companies have been thrown out of China, including Facebook, Twitter,
[01:42:34.860 --> 01:42:36.460]   Google, Flickr, Sam, Snapchat.
[01:42:36.460 --> 01:42:37.460]   But that's not new.
[01:42:37.460 --> 01:42:40.660]   No, but that's the point is they've been doing this forever.
[01:42:40.660 --> 01:42:43.020]   It's not just that they're thrown out.
[01:42:43.020 --> 01:42:45.500]   It's being impossible for them to see.
[01:42:45.500 --> 01:42:46.500]   Yeah.
[01:42:46.500 --> 01:42:47.500]   You can't use WhatsApp.
[01:42:47.500 --> 01:42:48.940]   Unless they agree to censorship.
[01:42:48.940 --> 01:42:49.940]   Yeah.
[01:42:49.940 --> 01:42:50.940]   Well, that's, yeah.
[01:42:50.940 --> 01:42:55.060]   But what if they just turn off one of those undersea cables?
[01:42:55.060 --> 01:43:05.380]   Can we maybe send some alternate people in India before we do this though?
[01:43:05.380 --> 01:43:07.980]   I don't know what the answer is.
[01:43:07.980 --> 01:43:13.060]   I just know that this is messy and is unlikely to end.
[01:43:13.060 --> 01:43:16.460]   I think this will all end in tears.
[01:43:16.460 --> 01:43:21.180]   It's unlikely to end well.
[01:43:21.180 --> 01:43:27.180]   China may not have a lot of, and I'm sure the calculus in Washington is China does not
[01:43:27.180 --> 01:43:34.540]   have a lot of ways to respond to this except to buckle and give in.
[01:43:34.540 --> 01:43:38.580]   It's going to, I mean, it's going to cost us a lot of money in the United States.
[01:43:38.580 --> 01:43:44.100]   A lot of stuff's made in China, Apple cannot reasonably move iPhone manufacturer back to
[01:43:44.100 --> 01:43:45.100]   the US.
[01:43:45.100 --> 01:43:46.100]   Maybe it can move it to Brazil.
[01:43:46.100 --> 01:43:47.100]   It already did so.
[01:43:47.100 --> 01:43:48.980]   It has moved some stuff to India.
[01:43:48.980 --> 01:43:53.820]   Because both India and Brazil have strong tariffs that prevent the sale of phones not
[01:43:53.820 --> 01:43:54.900]   made in those countries.
[01:43:54.900 --> 01:43:58.540]   So Fox kind of set up factories in both countries.
[01:43:58.540 --> 01:44:01.540]   Still Chinese company making them.
[01:44:01.540 --> 01:44:02.540]   I don't know.
[01:44:02.540 --> 01:44:04.220]   This is a really complicated story.
[01:44:04.220 --> 01:44:11.260]   And I don't know if anybody knows and whether you should continue to use DJI drones.
[01:44:11.260 --> 01:44:12.260]   I don't know.
[01:44:12.260 --> 01:44:17.180]   It's clear that the DHS is now extending the fight to other Chinese companies.
[01:44:17.180 --> 01:44:21.180]   And by the way, Matthew, how long have you had that phone?
[01:44:21.180 --> 01:44:23.140]   It's about a year now.
[01:44:23.140 --> 01:44:26.180]   When you bought it, did you have any, was there a little voice in the back of your head and
[01:44:26.180 --> 01:44:29.420]   said, oh, Huawei China?
[01:44:29.420 --> 01:44:34.340]   I don't really know because I don't do anything on the phone that I care about.
[01:44:34.340 --> 01:44:36.300]   I just wanted it for the cameras.
[01:44:36.300 --> 01:44:38.340]   I asked Huawei about that.
[01:44:38.340 --> 01:44:42.820]   And the American representative of Huawei, the PR person who's based, I think, up in
[01:44:42.820 --> 01:44:46.700]   Seattle, said, the data is routed through Germany, not China.
[01:44:46.700 --> 01:44:48.780]   So don't worry about that.
[01:44:48.780 --> 01:44:52.540]   But on the other hand, when I set up my P20, the very first question was asked was, is
[01:44:52.540 --> 01:44:56.820]   it okay if we send information back to the home office?
[01:44:56.820 --> 01:44:57.860]   So they didn't even hide it.
[01:44:57.860 --> 01:45:00.660]   I mean, so, but every time it does.
[01:45:00.660 --> 01:45:02.780]   Yeah, I mean, Apple has the same thing.
[01:45:02.780 --> 01:45:03.780]   That was the P20.
[01:45:03.780 --> 01:45:08.260]   Was that the phone that that's that for your location?
[01:45:08.260 --> 01:45:09.260]   Yes.
[01:45:09.260 --> 01:45:10.260]   It wanted to know what my location was.
[01:45:10.260 --> 01:45:13.860]   And is it okay to send it back to the Huawei?
[01:45:13.860 --> 01:45:18.780]   But we just learned, by the way, that the iPhones doing exactly the same thing.
[01:45:18.780 --> 01:45:19.780]   Great article.
[01:45:19.780 --> 01:45:23.220]   Is this the New York Times again?
[01:45:23.220 --> 01:45:25.220]   And they just they hate technology.
[01:45:25.220 --> 01:45:27.820]   No, it was the Washington Post.
[01:45:27.820 --> 01:45:28.820]   The iPhone.
[01:45:28.820 --> 01:45:32.100]   And this was always the problem with the iPhone.
[01:45:32.100 --> 01:45:37.980]   You Apple can say, oh, we take your privacy very seriously.
[01:45:37.980 --> 01:45:43.300]   But if you unless you live in China, well, or unless you don't put it, you know, the
[01:45:43.300 --> 01:45:47.700]   promise is minute you put third party apps on your phone.
[01:45:47.700 --> 01:45:48.820]   Apple can't stop this.
[01:45:48.820 --> 01:45:51.900]   Apple, this is the subtitle.
[01:45:51.900 --> 01:45:53.620]   The article is it's the middle of the night.
[01:45:53.620 --> 01:45:56.300]   Do you know what your iPhone, you know who your iPhone is talking to?
[01:45:56.300 --> 01:45:58.980]   Apple says, what happens in your iPhone stays on your iPhone?
[01:45:58.980 --> 01:46:04.860]   Our privacy experiment showed 5,400 hidden app trackers guzzled our data in a single
[01:46:04.860 --> 01:46:06.820]   week.
[01:46:06.820 --> 01:46:08.620]   Basically any app.
[01:46:08.620 --> 01:46:10.900]   And by the way, some of this is for good reason.
[01:46:10.900 --> 01:46:16.380]   Any app you put on your phone potentially can phone home, including Microsoft's OneDrive
[01:46:16.380 --> 01:46:17.740]   into its mint night.
[01:46:17.740 --> 01:46:19.820]   But all of these are cloud based services.
[01:46:19.820 --> 01:46:22.300]   And the Nike Spotify, the Washington Post.
[01:46:22.300 --> 01:46:23.300]   Oh, yeah.
[01:46:23.300 --> 01:46:24.300]   Even our own app.
[01:46:24.300 --> 01:46:25.300]   That's what they do.
[01:46:25.300 --> 01:46:27.020]   Another channel.
[01:46:27.020 --> 01:46:28.740]   One app, the Crime Alert Citizen.
[01:46:28.740 --> 01:46:32.900]   That one is a really weird app shared personally identifiable information in violation of its
[01:46:32.900 --> 01:46:35.020]   published privacy policy.
[01:46:35.020 --> 01:46:36.580]   Okay.
[01:46:36.580 --> 01:46:39.300]   That's a big ding.
[01:46:39.300 --> 01:46:41.540]   But that's everybody knows that you're signing up.
[01:46:41.540 --> 01:46:45.620]   You even when you when you install the app, and Apple's really good about this.
[01:46:45.620 --> 01:46:48.300]   It says, okay, can I send location information?
[01:46:48.300 --> 01:46:49.300]   You have three choices.
[01:46:49.300 --> 01:46:52.780]   No, only when the app is running all the time.
[01:46:52.780 --> 01:46:57.220]   Even if you say all the time or only when the app's running, Apple will again warn you
[01:46:57.220 --> 01:46:58.740]   a week or two later.
[01:46:58.740 --> 01:47:02.300]   In fact, several times this app is still sending location information.
[01:47:02.300 --> 01:47:03.820]   You want to do that?
[01:47:03.820 --> 01:47:04.820]   So Apple does what it can.
[01:47:04.820 --> 01:47:10.860]   But any third party app on any device is is is this is the potential.
[01:47:10.860 --> 01:47:17.220]   Someone also pointed out that that Washington Post story loads something like 150 trackers
[01:47:17.220 --> 01:47:25.500]   and pixels and sends God knows what kind of data everybody wants your data.
[01:47:25.500 --> 01:47:32.100]   But I think it's this is another case of tech companies promising more than they can deliver.
[01:47:32.100 --> 01:47:36.020]   Yes, everything stays in your phone except everything that doesn't stay in your phone.
[01:47:36.020 --> 01:47:37.620]   And we sign up for that.
[01:47:37.620 --> 01:47:40.420]   What's the point of having one get benefit out of much of it?
[01:47:40.420 --> 01:47:42.620]   And we yeah, what's the point of having one drive?
[01:47:42.620 --> 01:47:44.540]   It's a thank your files.
[01:47:44.540 --> 01:47:45.540]   That's the whole point.
[01:47:45.540 --> 01:47:56.620]   Let's see a couple more stories and we can we can get ready for the the pics and so forth.
[01:47:56.620 --> 01:48:03.140]   Amazon's facial recognition ban taken to the shareholders shareholders voted overwhelmingly
[01:48:03.140 --> 01:48:05.900]   98% to reject.
[01:48:05.900 --> 01:48:08.700]   No, what are you talking about?
[01:48:08.700 --> 01:48:10.820]   We're making some good money on this stuff.
[01:48:10.820 --> 01:48:13.300]   Are you nuts?
[01:48:13.300 --> 01:48:14.300]   You two point four closer.
[01:48:14.300 --> 01:48:17.660]   I have a little little Amazon stock.
[01:48:17.660 --> 01:48:18.660]   Did you vote?
[01:48:18.660 --> 01:48:21.580]   No, no, you get through proxy.
[01:48:21.580 --> 01:48:23.740]   No, I actually don't.
[01:48:23.740 --> 01:48:25.060]   Yeah, no, I didn't.
[01:48:25.060 --> 01:48:26.060]   It's weird.
[01:48:26.060 --> 01:48:29.900]   Usually you get a proxy when says, no, I'll let somebody else vote for me.
[01:48:29.900 --> 01:48:34.100]   Amazon just they just listened to me and they knew what I would vote did it for me.
[01:48:34.100 --> 01:48:35.100]   I know it.
[01:48:35.100 --> 01:48:36.100]   I know what you'd say.
[01:48:36.100 --> 01:48:37.100]   Yeah.
[01:48:37.100 --> 01:48:39.700]   Amazon is up to something interesting.
[01:48:39.700 --> 01:48:49.260]   According to Bloomberg poised to unleash a long-feared purge of small suppliers.
[01:48:49.260 --> 01:48:55.140]   Oh, that could be like a nuclear winter for small.
[01:48:55.140 --> 01:48:57.940]   More than half of the products you buy in Amazon are through the marketplace.
[01:48:57.940 --> 01:49:02.140]   They come from the little guys.
[01:49:02.140 --> 01:49:07.140]   Amazon's concern is apparently over counterfeiting and they have a terrible counterfeiting problem.
[01:49:07.140 --> 01:49:14.940]   I talked to a company that makes printers and they said, you can buy ink cartridges that
[01:49:14.940 --> 01:49:17.940]   have our brand, our label, everything on them.
[01:49:17.940 --> 01:49:18.940]   They're not.
[01:49:18.940 --> 01:49:19.940]   They're counterfeit.
[01:49:19.940 --> 01:49:24.780]   They don't work well and we hear about it because people say, oh, those ink cartridges
[01:49:24.780 --> 01:49:25.780]   were awful.
[01:49:25.780 --> 01:49:28.300]   You sold me and they've complained to Amazon.
[01:49:28.300 --> 01:49:30.620]   There's not much they can do about it.
[01:49:30.620 --> 01:49:32.180]   So maybe this is time.
[01:49:32.180 --> 01:49:37.180]   Two months ago, Amazon halted orders for thousands of suppliers with no explanation.
[01:49:37.180 --> 01:49:39.980]   Panic ensued until the orders quietly resumed weeks later.
[01:49:39.980 --> 01:49:46.260]   Amazon saying it was part of a plan to weed out counterfeit products, but Bloomberg warns
[01:49:46.260 --> 01:49:49.420]   and unfortunately I've reached the end of my free article limit so I don't know what
[01:49:49.420 --> 01:49:53.220]   they're warning.
[01:49:53.220 --> 01:49:57.380]   Maybe one of you is willing to play Bloomberg 35 bucks a month for access to their articles
[01:49:57.380 --> 01:49:58.380]   can tell me.
[01:49:58.380 --> 01:49:59.860]   I just can't bring myself to do it.
[01:49:59.860 --> 01:50:00.860]   You can't do it either.
[01:50:00.860 --> 01:50:02.780]   Well, because they got the terminals.
[01:50:02.780 --> 01:50:05.060]   This is an insanely profitable company.
[01:50:05.060 --> 01:50:06.700]   I got a free membership.
[01:50:06.700 --> 01:50:07.700]   Five bucks.
[01:50:07.700 --> 01:50:08.700]   I do it.
[01:50:08.700 --> 01:50:09.700]   I pay wired.
[01:50:09.700 --> 01:50:13.180]   I pay and Conde Nast is no broke company.
[01:50:13.180 --> 01:50:17.060]   I pay for a lot of content pay the Washington Post and the New York Times Wall Street Journal
[01:50:17.060 --> 01:50:20.980]   even, but I just can't bring myself to give Bloomberg 35 bucks a month.
[01:50:20.980 --> 01:50:21.980]   Sorry, Bloomberg.
[01:50:21.980 --> 01:50:22.980]   Jeez.
[01:50:22.980 --> 01:50:26.580]   Because you're not getting the value of the terminal, obviously, which is two million
[01:50:26.580 --> 01:50:27.580]   dollars a month or whatever.
[01:50:27.580 --> 01:50:28.580]   Right.
[01:50:28.580 --> 01:50:29.580]   That's a lot more expensive.
[01:50:29.580 --> 01:50:31.580]   Which they've done a good job with.
[01:50:31.580 --> 01:50:32.580]   Well, it works.
[01:50:32.580 --> 01:50:35.260]   Everybody thinks they want to pay well now.
[01:50:35.260 --> 01:50:36.260]   Everybody.
[01:50:36.260 --> 01:50:41.820]   And actually people still play Bloomberg for being the first to blow the whistle on the
[01:50:41.820 --> 01:50:47.140]   Chinese supply chain spying with that super micro story, which they have never retracted
[01:50:47.140 --> 01:50:49.140]   but never proven.
[01:50:49.140 --> 01:50:55.180]   Yeah, and they dedicated at least one or two reporters to try and to reprove that story
[01:50:55.180 --> 01:50:57.180]   and nothing has come up since.
[01:50:57.180 --> 01:50:58.180]   I think it's happened.
[01:50:58.180 --> 01:50:59.180]   It's a huge journalism.
[01:50:59.180 --> 01:51:00.540]   You know what I'm thinking now?
[01:51:00.540 --> 01:51:03.140]   Here's a short summary.
[01:51:03.140 --> 01:51:04.140]   Axios.
[01:51:04.140 --> 01:51:06.460]   I love it when people summarize Bloomberg stories.
[01:51:06.460 --> 01:51:07.460]   Thank you.
[01:51:07.460 --> 01:51:09.500]   Thank you, Axios.
[01:51:09.500 --> 01:51:10.980]   Thank you, Axios.
[01:51:10.980 --> 01:51:11.980]   So they're going to stick with Amazon.
[01:51:11.980 --> 01:51:16.660]   I was going to stick with Procter and Gamble, Sony, Lego, but they're going to cut off a
[01:51:16.660 --> 01:51:19.020]   lot of these small guys.
[01:51:19.020 --> 01:51:22.700]   A lot of, I think a lot of business on Amazon, I noticed this.
[01:51:22.700 --> 01:51:27.540]   I'll buy a consumer product, a deodorant or something.
[01:51:27.540 --> 01:51:31.740]   And what I really think is happening is these small guys are going to a big box store.
[01:51:31.740 --> 01:51:33.460]   They're buying a lot of them.
[01:51:33.460 --> 01:51:37.700]   They're breaking them up and then they're selling them on Amazon because a lot of the
[01:51:37.700 --> 01:51:39.300]   stuff I get is kind of like that.
[01:51:39.300 --> 01:51:44.060]   Like they got a better deal and they sold it and they're making whatever, 10 cents on
[01:51:44.060 --> 01:51:48.180]   paying basically for the convenience because I could go to the big box store.
[01:51:48.180 --> 01:51:57.180]   When I was in LA, my Lyft driver, his full time job now besides Lyft is selling fashion
[01:51:57.180 --> 01:51:59.020]   magazines to the rest of the world.
[01:51:59.020 --> 01:52:02.100]   He just goes to the newsstand and buys them.
[01:52:02.100 --> 01:52:04.100]   And it's smart.
[01:52:04.100 --> 01:52:05.100]   Vape stuff.
[01:52:05.100 --> 01:52:10.700]   It's called as Chatham is telling me, Tan Diner in the chat room and says, it's retail arbitrage.
[01:52:10.700 --> 01:52:12.620]   Arbitrage, yeah.
[01:52:12.620 --> 01:52:13.620]   That's pretty good.
[01:52:13.620 --> 01:52:14.740]   Bylo so high.
[01:52:14.740 --> 01:52:17.220]   I don't mind that.
[01:52:17.220 --> 01:52:21.300]   Although often the condition of the things I get is not great.
[01:52:21.300 --> 01:52:24.780]   I don't think Amazon would be hurt too much by getting rid of a lot of those small guys,
[01:52:24.780 --> 01:52:29.020]   but I feel really bad for the small guys.
[01:52:29.020 --> 01:52:32.900]   We had a story on Mac Break Weekly yesterday about a guy.
[01:52:32.900 --> 01:52:37.020]   When Apple did the deal with Amazon to sell, because Apple and Amazon were fighting, but
[01:52:37.020 --> 01:52:41.580]   when they made up and Amazon started selling Apple gear, they got rid of a lot of small
[01:52:41.580 --> 01:52:42.580]   guys selling stuff.
[01:52:42.580 --> 01:52:43.980]   Apple doesn't even sell anymore.
[01:52:43.980 --> 01:52:49.900]   Really old laptops and devices that people were buying for parts or for whatever reason.
[01:52:49.900 --> 01:52:52.260]   These guys are just, they're gone.
[01:52:52.260 --> 01:52:53.260]   They're not on Amazon.
[01:52:53.260 --> 01:52:55.900]   They have to go to eBay and other places to sell it.
[01:52:55.900 --> 01:52:57.540]   And it's hurt their business quite a bit.
[01:52:57.540 --> 01:52:58.540]   Yeah.
[01:52:58.540 --> 01:52:59.620]   I'm a part of it.
[01:52:59.620 --> 01:53:04.660]   The fear of Amazon has been that they would learn the business from others, including big
[01:53:04.660 --> 01:53:07.140]   companies, and then make their own products.
[01:53:07.140 --> 01:53:08.140]   Right.
[01:53:08.140 --> 01:53:09.140]   And they don't need to advertise.
[01:53:09.140 --> 01:53:10.140]   Well, they are doing that.
[01:53:10.140 --> 01:53:11.140]   What a large one they want.
[01:53:11.140 --> 01:53:13.620]   How many hundreds of in-house brands do they have?
[01:53:13.620 --> 01:53:14.620]   Exactly.
[01:53:14.620 --> 01:53:21.340]   So, the cynical view would be that they're using this as an excuse to get rid of those
[01:53:21.340 --> 01:53:22.340]   guys.
[01:53:22.340 --> 01:53:27.460]   Let's take a break and Jeff, I want a number.
[01:53:27.460 --> 01:53:31.860]   I have a number, Matthew, anything you want to pick and show, and I have something really
[01:53:31.860 --> 01:53:32.860]   cool to show.
[01:53:32.860 --> 01:53:33.860]   No change.
[01:53:33.860 --> 01:53:34.860]   One question.
[01:53:34.860 --> 01:53:35.860]   Oh, wait a minute.
[01:53:35.860 --> 01:53:36.860]   Yes.
[01:53:36.860 --> 01:53:37.860]   And then we'll do the change.
[01:53:37.860 --> 01:53:38.860]   Like I'm sorry.
[01:53:38.860 --> 01:53:39.860]   Oh, change.
[01:53:39.860 --> 01:53:40.860]   What's your question?
[01:53:40.860 --> 01:53:45.860]   The question was that really long and recent horror wits.
[01:53:45.860 --> 01:53:47.780]   Why we're investing in podcasts thing.
[01:53:47.780 --> 01:53:49.540]   I just wanted to do your take on it.
[01:53:49.540 --> 01:53:54.700]   Did you find anything there of new or of value or interesting or wrong?
[01:53:54.700 --> 01:53:56.220]   A lot.
[01:53:56.220 --> 01:54:00.620]   Let me pull it up so I can run through it.
[01:54:00.620 --> 01:54:02.580]   Of course, I found it very interesting.
[01:54:02.580 --> 01:54:04.420]   I sent it immediately to Lisa.
[01:54:04.420 --> 01:54:09.420]   I said there might be some graphs in here we could use.
[01:54:09.420 --> 01:54:11.220]   It's a substantively correct.
[01:54:11.220 --> 01:54:12.860]   I don't think they're wrong.
[01:54:12.860 --> 01:54:15.300]   I wish they'd talked to us a little bit.
[01:54:15.300 --> 01:54:20.860]   They talk a lot to, they never mention us.
[01:54:20.860 --> 01:54:25.180]   And we should, there are charts in there we should show up on that we are weirdly not
[01:54:25.180 --> 01:54:26.180]   on.
[01:54:26.180 --> 01:54:27.180]   Let me just be interviewed.
[01:54:27.180 --> 01:54:28.180]   Ben Evans.
[01:54:28.180 --> 01:54:29.180]   No, I know.
[01:54:29.180 --> 01:54:30.180]   I should.
[01:54:30.180 --> 01:54:31.180]   He should.
[01:54:31.180 --> 01:54:34.140]   That's my mistake.
[01:54:34.140 --> 01:54:35.140]   There it is.
[01:54:35.140 --> 01:54:39.900]   Investing in the podcast ecosystem in 2019.
[01:54:39.900 --> 01:54:44.100]   And the good news is they're saying it's a good idea.
[01:54:44.100 --> 01:54:45.460]   It's consumption is rapidly growing.
[01:54:45.460 --> 01:54:48.620]   A quarter of Americans listen weekly.
[01:54:48.620 --> 01:54:52.780]   Podcast listeners are consuming six plus hours a week of podcasts.
[01:54:52.780 --> 01:54:56.460]   Listeners are generally educated, millennials and slightly more than half are male.
[01:54:56.460 --> 01:54:59.780]   The gap is closing.
[01:54:59.780 --> 01:55:01.700]   These are all, everything's very positive.
[01:55:01.700 --> 01:55:02.940]   I wouldn't knock any of this.
[01:55:02.940 --> 01:55:05.620]   Here's a brief history of podcasting, which is essentially.
[01:55:05.620 --> 01:55:07.060]   He's our Dave later though.
[01:55:07.060 --> 01:55:08.940]   Yeah, he's had a lot of stuff.
[01:55:08.940 --> 01:55:12.460]   I mean, I guess Adam Curry should also complain because he's not in there.
[01:55:12.460 --> 01:55:15.780]   I'm not complaining about that.
[01:55:15.780 --> 01:55:23.740]   I do complain about their top podcast list because A, we're not on it in the place where
[01:55:23.740 --> 01:55:24.740]   we should be.
[01:55:24.740 --> 01:55:28.620]   And B, it comes from a company that we work with.
[01:55:28.620 --> 01:55:35.220]   But even the company that given these stats says that this list is missing a lot of this
[01:55:35.220 --> 01:55:38.300]   top podcast snapshot is first they use iTunes.
[01:55:38.300 --> 01:55:42.920]   They do explain that iTunes has nothing to do with a number of listeners just how many
[01:55:42.920 --> 01:55:45.620]   subscriptions you got in the last week or so.
[01:55:45.620 --> 01:55:47.700]   So why even publish this list?
[01:55:47.700 --> 01:55:53.460]   And then they use this list, which is also missing a number of significant networks,
[01:55:53.460 --> 01:55:55.300]   including us.
[01:55:55.300 --> 01:55:56.300]   But anyway.
[01:55:56.300 --> 01:56:02.940]   Either losing the investment opportunities there because it's old places like NPRI,
[01:56:02.940 --> 01:56:03.940]   heart radio.
[01:56:03.940 --> 01:56:05.940]   Yeah, these are all owned by C.
[01:56:05.940 --> 01:56:10.260]   Yeah, these are all owned, I guess, Ball Sparse do Sparse dual sports should get bought right
[01:56:10.260 --> 01:56:11.660]   now.
[01:56:11.660 --> 01:56:12.820]   But essentially it's accurate.
[01:56:12.820 --> 01:56:15.460]   I mean, I think it's a pretty good piece.
[01:56:15.460 --> 01:56:20.540]   It certainly doesn't hurt podcasting that companies like this are talking about it.
[01:56:20.540 --> 01:56:29.220]   This is we are at the peak, I think, of the podcast valuations right now.
[01:56:29.220 --> 01:56:30.220]   So.
[01:56:30.220 --> 01:56:31.620]   Okay, thank you.
[01:56:31.620 --> 01:56:32.620]   So I want to.
[01:56:32.620 --> 01:56:38.660]   I've engaged in investment banking firm, I believe, to investigate these things and
[01:56:38.660 --> 01:56:40.660]   we've talked to some people.
[01:56:40.660 --> 01:56:44.060]   I thought that China, the lessons from China were interesting.
[01:56:44.060 --> 01:56:45.060]   That was.
[01:56:45.060 --> 01:56:46.060]   Oh, okay.
[01:56:46.060 --> 01:56:47.060]   You know what?
[01:56:47.060 --> 01:56:48.060]   You're right.
[01:56:48.060 --> 01:56:49.060]   That was the thing I didn't know anything about.
[01:56:49.060 --> 01:56:52.180]   Apparently podcasting is huge in China.
[01:56:52.180 --> 01:57:00.460]   They've got the Shimalaya FM raised 580 million and is now valued at 3.6 billion.
[01:57:00.460 --> 01:57:01.460]   Yeah.
[01:57:01.460 --> 01:57:05.220]   So this is not a podcast app.
[01:57:05.220 --> 01:57:08.020]   This is more like a like a Spotify for podcasts.
[01:57:08.020 --> 01:57:09.900]   It's three bucks a month.
[01:57:09.900 --> 01:57:17.940]   You get audio books too and 300 top influencer podcasts and courses.
[01:57:17.940 --> 01:57:21.540]   So yeah, this is a very interesting business in China.
[01:57:21.540 --> 01:57:22.540]   It's one of the most.
[01:57:22.540 --> 01:57:23.540]   The books are huge.
[01:57:23.540 --> 01:57:24.540]   Yeah, yeah.
[01:57:24.540 --> 01:57:25.540]   Isn't that great?
[01:57:25.540 --> 01:57:31.260]   They also, yeah, because at least at Harvard said, hey, don't just think of podcast, think
[01:57:31.260 --> 01:57:32.260]   about you.
[01:57:32.260 --> 01:57:33.380]   Well, that's exactly right.
[01:57:33.380 --> 01:57:34.860]   That's why I never liked the word podcast.
[01:57:34.860 --> 01:57:38.860]   In fact, now that Apple is selling the last iPod.
[01:57:38.860 --> 01:57:39.860]   Yeah.
[01:57:39.860 --> 01:57:43.260]   I think it's time maybe to drop that word, but that's whatever.
[01:57:43.260 --> 01:57:44.700]   I have to use it because that's what everybody does.
[01:57:44.700 --> 01:57:45.700]   Well, they have a new iPod.
[01:57:45.700 --> 01:57:47.700]   Yeah, but it's the last iPod guarantee.
[01:57:47.700 --> 01:57:49.180]   Oh, they said they definitely said that.
[01:57:49.180 --> 01:57:50.180]   Oh, they haven't said that.
[01:57:50.180 --> 01:57:51.180]   But they had.
[01:57:51.180 --> 01:57:55.220]   So they just updated the iPod touch for the first time in four years.
[01:57:55.220 --> 01:57:58.180]   You think in four years they're going to make another one?
[01:57:58.180 --> 01:57:59.420]   I think this is the last one.
[01:57:59.420 --> 01:58:02.020]   And the updates were so minimal.
[01:58:02.020 --> 01:58:06.740]   It was like, yeah, it's got an iPhone 7 chip in it.
[01:58:06.740 --> 01:58:07.740]   It's Apple.
[01:58:07.740 --> 01:58:11.620]   Unless they sell millions of them, Apple's not going to update it again.
[01:58:11.620 --> 01:58:14.340]   This is the last iPod.
[01:58:14.340 --> 01:58:20.380]   They do talk about significant exits, major acquisitions.
[01:58:20.380 --> 01:58:25.340]   And that's always interesting to see.
[01:58:25.340 --> 01:58:27.100]   So yeah.
[01:58:27.100 --> 01:58:28.700]   So podcasts are big.
[01:58:28.700 --> 01:58:30.900]   I like to call them netcasts, as everybody knows.
[01:58:30.900 --> 01:58:32.660]   I'm the only one in the whole world.
[01:58:32.660 --> 01:58:33.660]   Yeah.
[01:58:33.660 --> 01:58:34.660]   I didn't work with it.
[01:58:34.660 --> 01:58:35.660]   Oh, sorry.
[01:58:35.660 --> 01:58:36.660]   Yeah.
[01:58:36.660 --> 01:58:39.580]   It doesn't feel like Spotify is trying to corner the market though, in a way.
[01:58:39.580 --> 01:58:40.740]   Yeah, they can't.
[01:58:40.740 --> 01:58:42.700]   Because that's fundamentally against the nature.
[01:58:42.700 --> 01:58:44.860]   It's like saying I'm going to corner the market and blogs.
[01:58:44.860 --> 01:58:45.860]   Right.
[01:58:45.860 --> 01:58:46.860]   Or websites.
[01:58:46.860 --> 01:58:47.860]   Right.
[01:58:47.860 --> 01:58:50.780]   Medium, by the way, there's starting to be a little bit of a, I wonder, do you still like
[01:58:50.780 --> 01:58:53.100]   medium because there's starting to be a little backlash against medium?
[01:58:53.100 --> 01:58:56.700]   I see people, publishers moving away from medium.
[01:58:56.700 --> 01:58:58.300]   Oh, there has been for a while.
[01:58:58.300 --> 01:58:59.300]   Yeah.
[01:58:59.300 --> 01:59:03.180]   Like they roped people in and then there was a backlash and then they wrote a pattern
[01:59:03.180 --> 01:59:04.180]   and subscribed.
[01:59:04.180 --> 01:59:05.180]   Subscribe to them.
[01:59:05.180 --> 01:59:06.180]   Yeah.
[01:59:06.180 --> 01:59:07.180]   Yeah.
[01:59:07.180 --> 01:59:08.740]   I'm not a big fan anymore.
[01:59:08.740 --> 01:59:12.020]   I still use it, but I cross post on my blog.
[01:59:12.020 --> 01:59:14.100]   So as you put it on your blog, that's fine.
[01:59:14.100 --> 01:59:16.500]   I don't have a problem putting it in other places.
[01:59:16.500 --> 01:59:18.380]   I think it's a lovely tool.
[01:59:18.380 --> 01:59:20.140]   I like to write on it.
[01:59:20.140 --> 01:59:21.140]   Right.
[01:59:21.140 --> 01:59:22.140]   Yeah.
[01:59:22.140 --> 01:59:23.140]   It's that's writing tool is great.
[01:59:23.140 --> 01:59:24.140]   That's fair.
[01:59:24.140 --> 01:59:25.140]   Good CMS.
[01:59:25.140 --> 01:59:26.940]   Yeah.
[01:59:26.940 --> 01:59:32.780]   I wanted to be listed in this podcast production companies, but I didn't deem to put us in there.
[01:59:32.780 --> 01:59:35.780]   I didn't mean to bring up something painful.
[01:59:35.780 --> 01:59:38.860]   No, I'm always jealous.
[01:59:38.860 --> 01:59:40.460]   I'm just a jealous person.
[01:59:40.460 --> 01:59:42.300]   Oh, no.
[01:59:42.300 --> 01:59:46.500]   You're a pioneer and the pioneer is going to left behind for the new kids.
[01:59:46.500 --> 01:59:49.300]   Yeah, there is that flavor of the month thing.
[01:59:49.300 --> 01:59:54.380]   Maybe if we took the lava lamp out, that would help people think we're hipping with it.
[01:59:54.380 --> 01:59:57.700]   The pioneers get the arrows and settlers get the linch.
[01:59:57.700 --> 01:59:58.700]   Yeah.
[01:59:58.700 --> 01:59:59.700]   No, I don't mind.
[01:59:59.700 --> 02:00:01.260]   And honestly, this was a very good.
[02:00:01.260 --> 02:00:02.260]   I'm glad.
[02:00:02.260 --> 02:00:05.500]   I mean, I was I hadn't brought it up because I thought, well, it's kind of inside baseball.
[02:00:05.500 --> 02:00:11.860]   But yeah, no, it was a very, I think a very good piece for us and every every podcaster.
[02:00:11.860 --> 02:00:12.860]   All right.
[02:00:12.860 --> 02:00:16.700]   Now we'll take a break.
[02:00:16.700 --> 02:00:22.700]   No, now we'll do the Google change log.
[02:00:22.700 --> 02:00:27.620]   That was a nice, faint, fully incarcerated.
[02:00:27.620 --> 02:00:33.700]   So remember at Google I/O and Google said, you can ask Google what's for dinner and order
[02:00:33.700 --> 02:00:34.700]   and all that.
[02:00:34.700 --> 02:00:37.420]   Now they've turned that on.
[02:00:37.420 --> 02:00:43.420]   French fries, lettuce wraps, mausomond curry, chicken wings and cupcakes.
[02:00:43.420 --> 02:00:49.820]   Now you can use Google search or maps or the assistant to order food from right now.
[02:00:49.820 --> 02:00:52.300]   Store dash postmates delivery.com slice.
[02:00:52.300 --> 02:01:00.980]   Now, zuppler and others coming soon look for the order online button in search and maps
[02:01:00.980 --> 02:01:05.900]   when you're searching for a restaurant or a type of cuisine.
[02:01:05.900 --> 02:01:09.300]   Let the Google assistant handle dinner.
[02:01:09.300 --> 02:01:16.020]   You can also say, Hey, you know who order food from your restaurant here.
[02:01:16.020 --> 02:01:22.780]   I'll show you the menu and if you can get food picked up or delivered from that restaurant,
[02:01:22.780 --> 02:01:24.660]   it'll set it up for you.
[02:01:24.660 --> 02:01:30.580]   It's the end of the line for another Google property that really part of the change log
[02:01:30.580 --> 02:01:35.100]   to right new and stuff going away and dead and dead new and dead.
[02:01:35.100 --> 02:01:36.580]   That we're Canadian.
[02:01:36.580 --> 02:01:41.420]   You know, if if Gina, Trippani were here, I would mention this is the 10th anniversary
[02:01:41.420 --> 02:01:43.020]   of Google wave.
[02:01:43.020 --> 02:01:44.020]   Mm hmm.
[02:01:44.020 --> 02:01:45.020]   It is.
[02:01:45.020 --> 02:01:47.820]   But I can't hurt her like that anymore.
[02:01:47.820 --> 02:01:55.380]   I think she said that when she left the show, you can't hurt me anymore, LaPorte.
[02:01:55.380 --> 02:02:03.060]   Google has also sunsetted YouTube gaming, which I didn't even know existed.
[02:02:03.060 --> 02:02:04.060]   The YouTube game.
[02:02:04.060 --> 02:02:05.060]   Did you know about this?
[02:02:05.060 --> 02:02:06.740]   Carsten, the YouTube gaming app?
[02:02:06.740 --> 02:02:09.260]   Yeah, they were trying to go after Twitch.
[02:02:09.260 --> 02:02:10.260]   Okay.
[02:02:10.260 --> 02:02:11.660]   Oh, so it's for streaming.
[02:02:11.660 --> 02:02:12.660]   It's for streaming.
[02:02:12.660 --> 02:02:13.660]   Oh, I got it.
[02:02:13.660 --> 02:02:18.260]   Shutting down May 30th, it's got to have something to do with Stadia coming.
[02:02:18.260 --> 02:02:23.740]   Oh, I thought that of course maybe and they have a new hub, but I have a feeling that
[02:02:23.740 --> 02:02:26.580]   Stadia is going to replace this.
[02:02:26.580 --> 02:02:31.100]   It's weird though to close something before the replacement is fully ready.
[02:02:31.100 --> 02:02:32.620]   Nobody uses YouTube gaming.
[02:02:32.620 --> 02:02:34.460]   That's why it probably doesn't matter.
[02:02:34.460 --> 02:02:37.820]   Nobody listens to tech now.
[02:02:37.820 --> 02:02:38.820]   Nope.
[02:02:38.820 --> 02:02:47.020]   Uh, Google Assistant gets New York subway arrival times and I can't wait to use this.
[02:02:47.020 --> 02:02:48.900]   Here's the biggest thing.
[02:02:48.900 --> 02:02:52.580]   You're going to be able to pay with Google Pay or Apple Pay.
[02:02:52.580 --> 02:02:54.100]   So I asked Mary Jo Foley.
[02:02:54.100 --> 02:02:57.500]   She said the subways already have these little touch to pay.
[02:02:57.500 --> 02:02:59.740]   Well, the ones I go into.
[02:02:59.740 --> 02:03:02.700]   Well, you got to go to a higher class of subway apparently.
[02:03:02.700 --> 02:03:04.820]   Oh, she's probably on the East side.
[02:03:04.820 --> 02:03:06.300]   Must be on the East side.
[02:03:06.300 --> 02:03:07.300]   Yeah.
[02:03:07.300 --> 02:03:10.740]   On the Upper East side, we've had these for years.
[02:03:10.740 --> 02:03:16.740]   You're just, you're just get the attendant to let you in and he'll bill you later.
[02:03:16.740 --> 02:03:22.140]   Uh, say all aboard.
[02:03:22.140 --> 02:03:24.380]   Next week, those commuting between current, the current video.
[02:03:24.380 --> 02:03:25.380]   Sorry.
[02:03:25.380 --> 02:03:26.380]   Sorry for a second.
[02:03:26.380 --> 02:03:27.380]   The current video that's driving people nuts.
[02:03:27.380 --> 02:03:29.220]   I don't know if you can find it in a flash.
[02:03:29.220 --> 02:03:35.780]   Carsten is a wrapped trying to climb up the, one of the poles and sliding back down.
[02:03:35.780 --> 02:03:37.780]   Well, remember the pizza rat.
[02:03:37.780 --> 02:03:40.140]   Pizza rat has been around for a while.
[02:03:40.140 --> 02:03:44.460]   This is the, now the new, the new star of the New York City subway.
[02:03:44.460 --> 02:03:47.820]   Ladies and gentlemen, I give you subway rat.
[02:03:47.820 --> 02:03:49.020]   It's in a subway car.
[02:03:49.020 --> 02:03:50.860]   So that's kind of creepy.
[02:03:50.860 --> 02:03:54.260]   Meet the pole dancing rat.
[02:03:54.260 --> 02:03:58.700]   New Yorkers are treated to the occasional rat sighting on the subway.
[02:03:58.700 --> 02:04:00.980]   This is the worst shaky cam video ever.
[02:04:00.980 --> 02:04:01.980]   But this is three AM.
[02:04:01.980 --> 02:04:02.980]   I'd be nervous.
[02:04:02.980 --> 02:04:03.980]   I'd be nervous.
[02:04:03.980 --> 02:04:08.180]   Is it the end train, well, is it nerves or is it three AM on the end train?
[02:04:08.180 --> 02:04:09.180]   I don't know.
[02:04:09.180 --> 02:04:12.540]   It's also zooming so you can be as far away as possible.
[02:04:12.540 --> 02:04:14.940]   Yeah, maybe that's it.
[02:04:14.940 --> 02:04:16.220]   You know, those poles are slippery.
[02:04:16.220 --> 02:04:17.220]   A lot of grease.
[02:04:17.220 --> 02:04:18.220]   There we go.
[02:04:18.220 --> 02:04:19.220]   That one.
[02:04:19.220 --> 02:04:20.220]   Yeah, that one.
[02:04:20.220 --> 02:04:21.220]   Here's less shaky kiss.
[02:04:21.220 --> 02:04:22.220]   That was the New York posts edit.
[02:04:22.220 --> 02:04:23.220]   All right.
[02:04:23.220 --> 02:04:24.220]   This is the guy.
[02:04:24.220 --> 02:04:25.220]   He looks cute.
[02:04:25.220 --> 02:04:26.220]   It's cute.
[02:04:26.220 --> 02:04:27.220]   Rats are cute.
[02:04:27.220 --> 02:04:28.220]   So you can also get, I was Google Assistant.
[02:04:28.220 --> 02:04:30.220]   We'll now tell you which cars have the rats.
[02:04:30.220 --> 02:04:31.220]   First rule.
[02:04:31.220 --> 02:04:36.860]   First rule somebody told me of the New York subway, if the car is empty, there's a reason
[02:04:36.860 --> 02:04:38.500]   to do not get on.
[02:04:38.500 --> 02:04:42.340]   You see crowded car, crowded car, empty car, crowded car, crowded car.
[02:04:42.340 --> 02:04:44.500]   Don't go in the empty car.
[02:04:44.500 --> 02:04:46.340]   It's like it happened to find it.
[02:04:46.340 --> 02:04:47.700]   That's a really good tip.
[02:04:47.700 --> 02:04:49.540]   Remember that.
[02:04:49.540 --> 02:04:52.500]   The other the other New York tip is never hug an Elmo.
[02:04:52.500 --> 02:04:53.500]   Oh, God.
[02:04:53.500 --> 02:04:56.740]   Oh, go anywhere near those guys.
[02:04:56.740 --> 02:04:59.100]   Oh, that is those things are disgusting.
[02:04:59.100 --> 02:05:01.540]   You can tell from a distance.
[02:05:01.540 --> 02:05:06.060]   By the way, for those who are not wise of the waves of the Big Apple, if you go down
[02:05:06.060 --> 02:05:10.020]   a Times Square, there are lots of people dressed up in character costumes, including
[02:05:10.020 --> 02:05:12.020]   Sesame Street's Elmo.
[02:05:12.020 --> 02:05:13.020]   But there are many others.
[02:05:13.020 --> 02:05:14.020]   They are not official.
[02:05:14.020 --> 02:05:16.020]   They are a not official.
[02:05:16.020 --> 02:05:17.540]   They've segregated them more and more.
[02:05:17.540 --> 02:05:21.580]   They've slowly moved them kind of out of the way, which is they still have the half naked
[02:05:21.580 --> 02:05:22.580]   people.
[02:05:22.580 --> 02:05:23.580]   They do this.
[02:05:23.580 --> 02:05:24.580]   They do this.
[02:05:24.580 --> 02:05:25.580]   Yeah.
[02:05:25.580 --> 02:05:26.580]   Yeah.
[02:05:26.580 --> 02:05:30.140]   The naked cowboy has many successors.
[02:05:30.140 --> 02:05:31.140]   It wasn't.
[02:05:31.140 --> 02:05:32.940]   There's a statue of Liberty.
[02:05:32.940 --> 02:05:33.940]   He's not naked.
[02:05:33.940 --> 02:05:34.940]   He's wearing underwear.
[02:05:34.940 --> 02:05:35.940]   It's tidy white.
[02:05:35.940 --> 02:05:39.620]   He's so, you know, there's lots of things you can hug.
[02:05:39.620 --> 02:05:41.380]   Don't or not.
[02:05:41.380 --> 02:05:43.420]   Yeah, don't or not.
[02:05:43.420 --> 02:05:50.300]   But I do like the idea of touch to pay coming to the the IRT parent or the MTA.
[02:05:50.300 --> 02:05:54.820]   Apparently it will come first to let me see.
[02:05:54.820 --> 02:06:01.180]   This is the Google Post, the central man, Grand Central in Manhattan and Atlantic Avenue,
[02:06:01.180 --> 02:06:03.260]   Barkley Center in Brooklyn.
[02:06:03.260 --> 02:06:04.260]   That's a pilot.
[02:06:04.260 --> 02:06:08.300]   And then it will roll out to other subway stations and it will work with both Apple
[02:06:08.300 --> 02:06:11.540]   Pay and credit card.
[02:06:11.540 --> 02:06:14.380]   My United credit card still doesn't work with Google Pay.
[02:06:14.380 --> 02:06:15.540]   So mine doesn't either.
[02:06:15.540 --> 02:06:16.540]   That's up to the bank.
[02:06:16.540 --> 02:06:17.540]   It's up to the bank.
[02:06:17.540 --> 02:06:18.540]   Yeah.
[02:06:18.540 --> 02:06:22.900]   TD Canada Trust does not support Google Pay, nor do they have any plans to support Google
[02:06:22.900 --> 02:06:23.900]   Play.
[02:06:23.900 --> 02:06:24.900]   They've asked.
[02:06:24.900 --> 02:06:26.740]   They have us.
[02:06:26.740 --> 02:06:28.300]   They have us online statement.
[02:06:28.300 --> 02:06:31.620]   It says we have no plans to support it.
[02:06:31.620 --> 02:06:32.620]   Period.
[02:06:32.620 --> 02:06:35.580]   So why use their app basically?
[02:06:35.580 --> 02:06:43.740]   While Congress is debating a law to ban loot boxes, Google Play Store has kind of jumped
[02:06:43.740 --> 02:06:44.740]   the gun.
[02:06:44.740 --> 02:06:47.940]   They're going to start requiring games with loot boxes to disclose the odds, which frankly
[02:06:47.940 --> 02:06:49.580]   is all I would ask.
[02:06:49.580 --> 02:06:53.940]   It's not ban them, but if you know the odds, at least you have a chance.
[02:06:53.940 --> 02:06:58.500]   This is also a policy in the Apple App Store.
[02:06:58.500 --> 02:07:05.060]   So loot boxes, which are basically gambling, have long been considered a problem in games,
[02:07:05.060 --> 02:07:06.060]   especially Fortnite.
[02:07:06.060 --> 02:07:09.540]   I think it was when Fortnite really took off, it got people that really got kids.
[02:07:09.540 --> 02:07:11.060]   What are the kids using them?
[02:07:11.060 --> 02:07:12.060]   Yeah, that's the problem.
[02:07:12.060 --> 02:07:13.060]   Kids are gambling.
[02:07:13.060 --> 02:07:15.380]   What's a loot box?
[02:07:15.380 --> 02:07:24.460]   So it's a box that's kind of like one of those, buy the box and see what you get.
[02:07:24.460 --> 02:07:25.460]   Oh, OK.
[02:07:25.460 --> 02:07:26.460]   See what you--
[02:07:26.460 --> 02:07:27.460]   Oh, OK.
[02:07:27.460 --> 02:07:28.460]   Mystery box.
[02:07:28.460 --> 02:07:29.460]   Yeah.
[02:07:29.460 --> 02:07:33.100]   Loot boxes is in a video game, you will get the chance to get a new weapon or a new skin
[02:07:33.100 --> 02:07:34.100]   or something.
[02:07:34.100 --> 02:07:35.100]   Oh, OK.
[02:07:35.100 --> 02:07:37.940]   A loot box is a randomized version of that.
[02:07:37.940 --> 02:07:38.940]   Right.
[02:07:38.940 --> 02:07:41.340]   So you'll get a good gun, you might not.
[02:07:41.340 --> 02:07:45.340]   What's not randomized is the cost of the loot box.
[02:07:45.340 --> 02:07:48.620]   That's just what you get for it.
[02:07:48.620 --> 02:07:49.620]   Kids love that stuff.
[02:07:49.620 --> 02:07:51.780]   You ever go to a gumball machine?
[02:07:51.780 --> 02:07:55.860]   You put in your penny, you don't know what you're going to get.
[02:07:55.860 --> 02:07:57.220]   Yeah, or the claw.
[02:07:57.220 --> 02:07:58.220]   The claw is not randomized.
[02:07:58.220 --> 02:07:59.220]   You don't get anything.
[02:07:59.220 --> 02:08:00.220]   Well, yeah, you don't get anything.
[02:08:00.220 --> 02:08:03.780]   But the claw decides who will stay and who will be.
[02:08:03.780 --> 02:08:04.780]   Nah.
[02:08:04.780 --> 02:08:05.780]   No, you can.
[02:08:05.780 --> 02:08:06.780]   You can game the claw.
[02:08:06.780 --> 02:08:07.780]   You can.
[02:08:07.780 --> 02:08:08.780]   Can you?
[02:08:08.780 --> 02:08:09.780]   Yeah.
[02:08:09.780 --> 02:08:10.780]   Oh, sure.
[02:08:10.780 --> 02:08:11.780]   There are videos that you show to online that--
[02:08:11.780 --> 02:08:13.980]   They have a course on this at Harvard.
[02:08:13.980 --> 02:08:14.980]   Yes.
[02:08:14.980 --> 02:08:15.980]   When I was--
[02:08:15.980 --> 02:08:19.460]   You just watch, you have to watch and see when it pays out.
[02:08:19.460 --> 02:08:20.460]   Yeah.
[02:08:20.460 --> 02:08:23.660]   It is required by law to pay out at a certain rate.
[02:08:23.660 --> 02:08:28.620]   So you just-- you watch and then you go and when it's going to pay out, you go.
[02:08:28.620 --> 02:08:30.740]   You waste your life saying they're watching the claw.
[02:08:30.740 --> 02:08:34.660]   When I was at Elliot House, it was widely known.
[02:08:34.660 --> 02:08:38.860]   The claws were gameable.
[02:08:38.860 --> 02:08:39.620]   Elliot--
[02:08:39.620 --> 02:08:40.620]   Elliot.
[02:08:40.620 --> 02:08:41.620]   Pardon me?
[02:08:41.620 --> 02:08:42.620]   Adams.
[02:08:42.620 --> 02:08:43.620]   Pardon me.
[02:08:43.620 --> 02:08:44.620]   Oh, oh, oh.
[02:08:44.620 --> 02:08:45.740]   As in John Adams?
[02:08:45.740 --> 02:08:46.740]   I didn't--
[02:08:46.740 --> 02:08:48.540]   No, I don't know if what was--
[02:08:48.540 --> 02:08:49.540]   Probably.
[02:08:49.540 --> 02:08:50.540]   Right?
[02:08:50.540 --> 02:08:52.940]   I don't know what Adams it was.
[02:08:52.940 --> 02:08:55.340]   But I know that Elliot House was the enemy.
[02:08:55.340 --> 02:08:56.340]   Yeah.
[02:08:56.340 --> 02:09:00.500]   So don't ever say he was an Elliot House.
[02:09:00.500 --> 02:09:03.660]   Let's see.
[02:09:03.660 --> 02:09:05.060]   Is that the end?
[02:09:05.060 --> 02:09:06.660]   Well, there's a few things.
[02:09:06.660 --> 02:09:09.900]   I'll save-- I have a number and actually a really interesting pick.
[02:09:09.900 --> 02:09:13.060]   Well, I mean of the-- of the-- you know what?
[02:09:13.060 --> 02:09:14.060]   That's--
[02:09:14.060 --> 02:09:19.340]   I'm not going to change the lock.
[02:09:19.340 --> 02:09:23.140]   Show today brought to you by Grammarly.
[02:09:23.140 --> 02:09:28.700]   I don't not-- I am not-- I have always said that I have very good grammar and spelling.
[02:09:28.700 --> 02:09:33.020]   But I do not think of Grammarly as a spell checker or grammar checker.
[02:09:33.020 --> 02:09:35.580]   Despite its name, I think people misunderstood what it is.
[02:09:35.580 --> 02:09:36.580]   Understand what it is.
[02:09:36.580 --> 02:09:41.900]   Grammarly is a communication tool that helps you write more effectively.
[02:09:41.900 --> 02:09:43.660]   That's-- who wouldn't want that, right?
[02:09:43.660 --> 02:09:45.060]   Nobody's perfect.
[02:09:45.060 --> 02:09:49.260]   It's like a little friend who will say, you know, you could-- you could phrase this differently
[02:09:49.260 --> 02:09:54.060]   or that's confusing or maybe the reading level for what you're doing here is a little too
[02:09:54.060 --> 02:09:57.060]   high or a little too low.
[02:09:57.060 --> 02:10:01.020]   Grammarly helps everyone do their best work and you do your best work.
[02:10:01.020 --> 02:10:03.500]   You'll accomplish more of your goals.
[02:10:03.500 --> 02:10:08.580]   Improve your communication at school at work with resumes.
[02:10:08.580 --> 02:10:10.540]   Show your best self through writing.
[02:10:10.540 --> 02:10:12.420]   And Grammarly is everywhere you write.
[02:10:12.420 --> 02:10:16.540]   Chrome Firefox Safari Edge, iOS, Android, Windows, Mac.
[02:10:16.540 --> 02:10:20.060]   They do have a basic product that reviews critical spelling and grammar.
[02:10:20.060 --> 02:10:21.740]   You're certainly worth trying.
[02:10:21.740 --> 02:10:25.300]   But I would suggest Grammarly Premium because not only do you get spelling and grammar,
[02:10:25.300 --> 02:10:32.780]   you get advanced punctuation, structure, style within context, vocabulary suggestions,
[02:10:32.780 --> 02:10:38.100]   conciseness, and a readability score which helps you decide, is this appropriate for
[02:10:38.100 --> 02:10:41.820]   a business proposal, an academic essay or a casual blog post?
[02:10:41.820 --> 02:10:43.780]   What am I doing here?
[02:10:43.780 --> 02:10:49.300]   Stop making email typos on your phone, close more deals at work with your emails, and even
[02:10:49.300 --> 02:10:53.700]   polish your resume to get a great job.
[02:10:53.700 --> 02:10:55.500]   Grammarly.com/twig.
[02:10:55.500 --> 02:10:58.700]   Right now you'll get 20% off Grammarly Premium.
[02:10:58.700 --> 02:11:03.300]   Grammarly.com/twig for 20% off.
[02:11:03.300 --> 02:11:05.740]   Thank you Grammarly.
[02:11:05.740 --> 02:11:07.580]   Thank you Grammarly for supporting this week on Google.
[02:11:07.580 --> 02:11:12.980]   And I thank you Twig listeners for using that URL Grammarly.com/twig.
[02:11:12.980 --> 02:11:20.620]   You'll get a little reward because you will get 20% off the Premium account.
[02:11:20.620 --> 02:11:27.380]   And now ladies and gentlemen, I invite our guest today, Mr. Matthew Ingram, to give us
[02:11:27.380 --> 02:11:30.460]   a pick of the week.
[02:11:30.460 --> 02:11:35.180]   So I picked something from the rundown that I thought was fascinating.
[02:11:35.180 --> 02:11:36.180]   Good.
[02:11:36.180 --> 02:11:37.180]   And we didn't do it.
[02:11:37.180 --> 02:11:38.180]   I don't know if you saw this, but...
[02:11:38.180 --> 02:11:39.180]   I must have known.
[02:11:39.180 --> 02:11:42.460]   I'm pretty sure it was Samsung.
[02:11:42.460 --> 02:11:46.860]   They took individual photos, including a photo of the Mona Lisa.
[02:11:46.860 --> 02:11:47.860]   Oh, isn't this wild?
[02:11:47.860 --> 02:11:52.340]   And showed that they could animate them.
[02:11:52.340 --> 02:11:55.380]   It's mind-bogglingly amazing.
[02:11:55.380 --> 02:12:00.860]   The key is on this, they didn't have a whole video or a bunch of frames in one frame.
[02:12:00.860 --> 02:12:04.460]   And so they effectively animate the face using the existing...
[02:12:04.460 --> 02:12:06.940]   Yeah, scroll down to the Mona Lisa one.
[02:12:06.940 --> 02:12:08.860]   Yeah, it's pretty amazing.
[02:12:08.860 --> 02:12:09.860]   So that's...
[02:12:09.860 --> 02:12:10.860]   Oh!
[02:12:10.860 --> 02:12:11.860]   Oh, yeah.
[02:12:11.860 --> 02:12:13.060]   Oh, look at that.
[02:12:13.060 --> 02:12:17.380]   And actually, depending on which the base truth is always the same, it's Mona Lisa, but
[02:12:17.380 --> 02:12:24.060]   depending on which video they use for the kind of intermediate truth, Mona Lisa could
[02:12:24.060 --> 02:12:25.060]   have different looks.
[02:12:25.060 --> 02:12:26.060]   That's cool.
[02:12:26.060 --> 02:12:27.620]   Yeah, I just thought it was fascinating.
[02:12:27.620 --> 02:12:31.700]   And that's a single photo of a face from one perspective.
[02:12:31.700 --> 02:12:34.700]   I think Mona Lisa would look most like she won on the left.
[02:12:34.700 --> 02:12:36.260]   I'm just saying.
[02:12:36.260 --> 02:12:37.260]   Yeah.
[02:12:37.260 --> 02:12:38.260]   Mona Lisa.
[02:12:38.260 --> 02:12:39.260]   Noramita.
[02:12:39.260 --> 02:12:40.260]   Noramita talks too much.
[02:12:40.260 --> 02:12:41.260]   She's too keen.
[02:12:41.260 --> 02:12:42.260]   She talks too much.
[02:12:42.260 --> 02:12:43.260]   Her eyes are too white.
[02:12:43.260 --> 02:12:45.820]   And the one on the right is nerdy Mona Lisa.
[02:12:45.820 --> 02:12:46.820]   Yeah.
[02:12:46.820 --> 02:12:48.320]   Isn't that funny?
[02:12:48.320 --> 02:12:51.900]   But all three of them could be.
[02:12:51.900 --> 02:12:55.620]   I don't know if that counts as a deep fake or not.
[02:12:55.620 --> 02:12:59.940]   I mean, it's for you the kind of technology you could generate deep fakes with.
[02:12:59.940 --> 02:13:01.340]   Oh, yeah, we're going to say so much.
[02:13:01.340 --> 02:13:03.580]   So I have another one that isn't that is related.
[02:13:03.580 --> 02:13:04.580]   Yes.
[02:13:04.580 --> 02:13:06.580]   I found a video.
[02:13:06.580 --> 02:13:11.660]   I don't know if you can find it, but it was so a guy who specializes in creating effectively
[02:13:11.660 --> 02:13:14.300]   deep fakes.
[02:13:14.300 --> 02:13:20.740]   Morfed Bill Hader's face while he's doing an impression of Robert De Niro to make him
[02:13:20.740 --> 02:13:22.900]   look like Robert De Niro.
[02:13:22.900 --> 02:13:25.540]   And then he did it with Arnold Schwarzenegger as well.
[02:13:25.540 --> 02:13:32.300]   And it was an interview from I think 2005, but his video clip just came out.
[02:13:32.300 --> 02:13:34.540]   Yeah, that's it.
[02:13:34.540 --> 02:13:38.140]   OK, this was on Conan or?
[02:13:38.140 --> 02:13:39.140]   Yeah.
[02:13:39.140 --> 02:13:40.900]   So here's here's Bill Hader on.
[02:13:40.900 --> 02:13:41.900]   Literally just started.
[02:13:41.900 --> 02:13:43.820]   Oh, yeah, he doesn't look this.
[02:13:43.820 --> 02:13:44.820]   He looks young.
[02:13:44.820 --> 02:13:49.580]   Oh, finally, the discrimination.
[02:13:49.580 --> 02:13:50.580]   What is that you?
[02:13:50.580 --> 02:13:51.580]   Is that me?
[02:13:51.580 --> 02:13:52.580]   What is going on?
[02:13:52.580 --> 02:13:53.580]   There it was me.
[02:13:53.580 --> 02:13:54.580]   OK.
[02:13:54.580 --> 02:13:56.020]   Nope, it's you.
[02:13:56.020 --> 02:14:01.180]   You have too many tabs open.
[02:14:01.180 --> 02:14:02.180]   Too many.
[02:14:02.180 --> 02:14:06.540]   You've got the Samsung tab open and the Hader tab.
[02:14:06.540 --> 02:14:08.020]   Like weird things.
[02:14:08.020 --> 02:14:10.500]   See, it just happened.
[02:14:10.500 --> 02:14:11.500]   Wow.
[02:14:11.500 --> 02:14:13.860]   We got some lights.
[02:14:13.860 --> 02:14:17.300]   Wait a minute, go back because that's weird.
[02:14:17.300 --> 02:14:19.380]   That way he looks like De Niro.
[02:14:19.380 --> 02:14:21.740]   Yeah, go back because we got to see Hader.
[02:14:21.740 --> 02:14:22.740]   It's amazing.
[02:14:22.740 --> 02:14:23.740]   Come to that impression.
[02:14:23.740 --> 02:14:28.860]   No, no, I watch him on the acceptance speech on the Emmys for Angels and America.
[02:14:28.860 --> 02:14:29.860]   So he looks like Hader.
[02:14:29.860 --> 02:14:31.860]   I was watching it.
[02:14:31.860 --> 02:14:34.340]   As soon as he starts the impression.
[02:14:34.340 --> 02:14:37.660]   Just totally into the peripheral and weird things.
[02:14:37.660 --> 02:14:39.260]   Oh, just up.
[02:14:39.260 --> 02:14:40.260]   Oh, wow.
[02:14:40.260 --> 02:14:41.260]   Isn't that amazing?
[02:14:41.260 --> 02:14:44.260]   We got some lights.
[02:14:44.260 --> 02:14:47.660]   A couple of cameras.
[02:14:47.660 --> 02:14:49.660]   That's wild.
[02:14:49.660 --> 02:14:51.620]   Yeah, that's wild.
[02:14:51.620 --> 02:14:55.340]   You have to see this is a video for folks listening.
[02:14:55.340 --> 02:14:58.300]   You can Google that, but that is wild.
[02:14:58.300 --> 02:14:59.300]   It's creepy.
[02:14:59.300 --> 02:15:00.300]   Yeah.
[02:15:00.300 --> 02:15:01.860]   And it's almost impossible to tell.
[02:15:01.860 --> 02:15:03.180]   You can't see it.
[02:15:03.180 --> 02:15:04.180]   Yeah.
[02:15:04.180 --> 02:15:06.220]   That's what's scary about all this stuff.
[02:15:06.220 --> 02:15:12.140]   But what we need is a smarter, better educated group of people can.
[02:15:12.140 --> 02:15:13.140]   Fins.
[02:15:13.140 --> 02:15:14.300]   We need a bunch of fins.
[02:15:14.300 --> 02:15:15.300]   The fins.
[02:15:15.300 --> 02:15:17.620]   Jeff, do you have a number?
[02:15:17.620 --> 02:15:19.020]   I do indeed.
[02:15:19.020 --> 02:15:25.780]   So Axios using crowd dangle of Facebook tool looked at the effectiveness of Donald
[02:15:25.780 --> 02:15:28.500]   Chrome's tweets and found they have lost their potency.
[02:15:28.500 --> 02:15:31.100]   I won't say that they're impotent.
[02:15:31.100 --> 02:15:41.020]   But his interaction rate, which is likes and retweets divided by total audience, has gone
[02:15:41.020 --> 02:15:51.900]   from in the election time 0.55% down to 0.16%.
[02:15:51.900 --> 02:15:52.900]   So it was...
[02:15:52.900 --> 02:15:55.500]   Well, does that you think has a number of tweets?
[02:15:55.500 --> 02:15:57.140]   Because his tweet count has gone up.
[02:15:57.140 --> 02:15:58.980]   They've built that in a little bit.
[02:15:58.980 --> 02:16:04.180]   But generally, just the interaction is going down.
[02:16:04.180 --> 02:16:07.300]   I think this is generally true across the board.
[02:16:07.300 --> 02:16:08.300]   I wonder though.
[02:16:08.300 --> 02:16:10.580]   He's not as fascinating to people as he was two years ago.
[02:16:10.580 --> 02:16:13.540]   We've had Twitter's got an algorithm now, right?
[02:16:13.540 --> 02:16:14.540]   Oh.
[02:16:14.540 --> 02:16:15.540]   I mean, are they...
[02:16:15.540 --> 02:16:16.540]   I think so.
[02:16:16.540 --> 02:16:17.860]   That you could say the same thing.
[02:16:17.860 --> 02:16:18.860]   And I wonder...
[02:16:18.860 --> 02:16:19.860]   I'm ranking him.
[02:16:19.860 --> 02:16:20.860]   Here's my question.
[02:16:20.860 --> 02:16:23.500]   I wonder if all of the...
[02:16:23.500 --> 02:16:27.980]   When Trump got elected, there was a huge spike in tech news, not tech news, political news
[02:16:27.980 --> 02:16:31.420]   shows, and especially like the Daily.
[02:16:31.420 --> 02:16:32.420]   The Trump bump.
[02:16:32.420 --> 02:16:33.420]   The Trump bump.
[02:16:33.420 --> 02:16:37.540]   And I'm wondering if that's starting to slow down a little bit because, in fact, I think
[02:16:37.540 --> 02:16:38.540]   it is.
[02:16:38.540 --> 02:16:42.740]   I'm my guess as it is because they're doing a lot less Trump coverage on the Daily for
[02:16:42.740 --> 02:16:43.740]   one thing.
[02:16:43.740 --> 02:16:44.740]   Yeah.
[02:16:44.740 --> 02:16:49.300]   And I also just know that I refused to follow him, so I had a column devoted to him in my
[02:16:49.300 --> 02:16:50.300]   tweet deck.
[02:16:50.300 --> 02:16:53.020]   And I used to go every day.
[02:16:53.020 --> 02:16:56.940]   I was just thinking a couple of days ago before this story came out.
[02:16:56.940 --> 02:17:01.900]   Yeah, I haven't gone to look at the Trump column for like four or five days.
[02:17:01.900 --> 02:17:03.260]   So I think there's a lot of that.
[02:17:03.260 --> 02:17:06.460]   I think we've heard kind of everything he has to say, which is part of this too, is
[02:17:06.460 --> 02:17:08.500]   that you look at the numbers.
[02:17:08.500 --> 02:17:14.800]   Since April 1, no collusion 54 times, no obstruction 30 times, which 20 times, hoax
[02:17:14.800 --> 02:17:15.800]   19 times.
[02:17:15.800 --> 02:17:18.940]   You kind of heard it all.
[02:17:18.940 --> 02:17:24.060]   His pace of tweeting has picked up from 157 times per month during the first six months
[02:17:24.060 --> 02:17:31.860]   to 284 times per month over the last six months.
[02:17:31.860 --> 02:17:41.220]   I just wish Twitter were not the national forum for discussion of serious political matters.
[02:17:41.220 --> 02:17:46.300]   I just think that's the wrong place for these discussions to happen.
[02:17:46.300 --> 02:17:51.780]   Yeah, but what really happens here is that it's media's amplification that makes that
[02:17:51.780 --> 02:17:52.860]   defective.
[02:17:52.860 --> 02:17:56.780]   People who call Trump, Jay Rosen argues that he hates it when people say Trump's brilliant
[02:17:56.780 --> 02:17:58.620]   Twitter and I agree with Jay.
[02:17:58.620 --> 02:17:59.620]   He's not.
[02:17:59.620 --> 02:18:00.620]   BDR chumps.
[02:18:00.620 --> 02:18:12.140]   No, I don't take away credit to the because the Donald's got a real nose for how to get
[02:18:12.140 --> 02:18:13.140]   coverage.
[02:18:13.140 --> 02:18:14.140]   He always has.
[02:18:14.140 --> 02:18:21.140]   He's brilliant at media.
[02:18:21.140 --> 02:18:22.140]   Brilliant.
[02:18:22.140 --> 02:18:24.140]   Brilliant implies intelligence.
[02:18:24.140 --> 02:18:27.620]   He's very good at getting the media to pay attention.
[02:18:27.620 --> 02:18:29.340]   He's very great.
[02:18:29.340 --> 02:18:33.460]   Brilliant implies like there's this thinking process going on.
[02:18:33.460 --> 02:18:36.260]   I don't know if that's the case, but he's very adept.
[02:18:36.260 --> 02:18:39.140]   He knows exactly what buttons to push to get coverage.
[02:18:39.140 --> 02:18:40.780]   Yeah, so that's what I'm saying.
[02:18:40.780 --> 02:18:45.780]   In fact, from now on, every president better because that's clearly the new bar for getting
[02:18:45.780 --> 02:18:46.780]   media coverage.
[02:18:46.780 --> 02:18:54.220]   I always use a Bayon analogy like Trump could say, you know, you like to think you can live
[02:18:54.220 --> 02:18:56.780]   in like fake news in the tabloid media.
[02:18:56.780 --> 02:18:58.180]   I was born in that.
[02:18:58.180 --> 02:19:00.500]   I mean, he was born in that.
[02:19:00.500 --> 02:19:02.380]   He was created out of that.
[02:19:02.380 --> 02:19:03.380]   Yeah.
[02:19:03.380 --> 02:19:05.100]   No, he was.
[02:19:05.100 --> 02:19:10.780]   So and Twitter just made it made a perfect little foil for him.
[02:19:10.780 --> 02:19:11.780]   Yeah.
[02:19:11.780 --> 02:19:15.820]   And it just, you know, the times were perfect.
[02:19:15.820 --> 02:19:21.420]   This is, this is again, I would go back to this great zen up to Ficki piece in the scientific
[02:19:21.420 --> 02:19:27.100]   American about sociological versus psychological and it started with this is why people like
[02:19:27.100 --> 02:19:31.780]   Game of Thrones, but ultimately this is why we have a hard time understanding what's going
[02:19:31.780 --> 02:19:36.660]   on in our world and technology because we look at it from a psychological point of view.
[02:19:36.660 --> 02:19:41.660]   We look at it from the person point, individual individuals like what Mark Zuckerberg did
[02:19:41.660 --> 02:19:44.940]   this or Evan or did that.
[02:19:44.940 --> 02:19:49.780]   But really we should be looking at it from a sociological and she says, that's why you
[02:19:49.780 --> 02:19:53.460]   can't say, well, if you go back in time, kill Hitler with that have with the Nazis, not
[02:19:53.460 --> 02:19:54.460]   have existed.
[02:19:54.460 --> 02:19:58.300]   Of course they would have because it was sociological was a response to economic and
[02:19:58.300 --> 02:20:00.700]   other conditions in Germany.
[02:20:00.700 --> 02:20:04.900]   And I think we'd have a Trump right now, whether his name's Donald or not.
[02:20:04.900 --> 02:20:07.100]   I think it's both.
[02:20:07.100 --> 02:20:09.060]   I think it's also anthropological.
[02:20:09.060 --> 02:20:10.060]   It's about communities.
[02:20:10.060 --> 02:20:12.020]   Oh, yeah, you can't ignore.
[02:20:12.020 --> 02:20:13.020]   It's also psychological.
[02:20:13.020 --> 02:20:14.940]   Yes, you can't ignore psychology.
[02:20:14.940 --> 02:20:15.940]   Sure.
[02:20:15.940 --> 02:20:16.940]   Right.
[02:20:16.940 --> 02:20:19.580]   But, but it's a mistake to the great man theory is a mistake.
[02:20:19.580 --> 02:20:21.380]   I think it's really the point of it.
[02:20:21.380 --> 02:20:27.340]   This is also why I argue that in journalism, we've got a reconceive of journalism from
[02:20:27.340 --> 02:20:28.340]   that basis.
[02:20:28.340 --> 02:20:30.500]   I just had a conversation with an anthropologist.
[02:20:30.500 --> 02:20:33.820]   I want to bring the anthropologist into the journalism school to say, what do we have
[02:20:33.820 --> 02:20:34.820]   communities are getting along?
[02:20:34.820 --> 02:20:37.460]   What do we know about communities from an anthropological structure?
[02:20:37.460 --> 02:20:39.900]   What do we know of the sociologist, what do we know of the sociologist, what do we
[02:20:39.900 --> 02:20:40.900]   know of the...
[02:20:40.900 --> 02:20:41.740]   We just love and I'm seeing it already.
[02:20:41.740 --> 02:20:45.420]   I'm seeing the horse race coverage already on the media.
[02:20:45.420 --> 02:20:50.300]   We just, that's how we like to see it as, you know, it's a horse race and it does such
[02:20:50.300 --> 02:20:55.100]   a disservice to the real serious issues that we have to solve.
[02:20:55.100 --> 02:20:56.100]   It sells newspapers.
[02:20:56.100 --> 02:20:57.100]   Right.
[02:20:57.100 --> 02:20:58.100]   And that's why it works.
[02:20:58.100 --> 02:20:59.100]   Yeah.
[02:20:59.100 --> 02:21:00.100]   I'd expect better.
[02:21:00.100 --> 02:21:04.620]   This is the book I mentioned on the show, sometimes we go, how history is wrong.
[02:21:04.620 --> 02:21:05.620]   I'm reading it.
[02:21:05.620 --> 02:21:06.620]   In neuroscience, by dictionary stories.
[02:21:06.620 --> 02:21:07.620]   It's pretty wonky.
[02:21:07.620 --> 02:21:08.620]   I hope I'll warn you, sufficient.
[02:21:08.620 --> 02:21:09.620]   No, I like it.
[02:21:09.620 --> 02:21:10.620]   It's fascinating.
[02:21:10.620 --> 02:21:12.380]   A lot of books.
[02:21:12.380 --> 02:21:14.300]   And this might be one of them.
[02:21:14.300 --> 02:21:17.700]   Could really be better served with a long article.
[02:21:17.700 --> 02:21:21.740]   Yeah, I would agree.
[02:21:21.740 --> 02:21:23.740]   Because I feel like I kind of got the premise.
[02:21:23.740 --> 02:21:24.740]   I got it.
[02:21:24.740 --> 02:21:26.620]   But you had to prove it to you.
[02:21:26.620 --> 02:21:27.620]   Yeah.
[02:21:27.620 --> 02:21:28.620]   But you know, I got it.
[02:21:28.620 --> 02:21:32.220]   And my post about it, you know, and he said, you got it right.
[02:21:32.220 --> 02:21:34.260]   So I was glad to hear that.
[02:21:34.260 --> 02:21:35.260]   But I kept on.
[02:21:35.260 --> 02:21:38.380]   I needed him to walk me through the process.
[02:21:38.380 --> 02:21:39.380]   Far.
[02:21:39.380 --> 02:21:43.140]   And there needs to be a freemium model to the book.
[02:21:43.140 --> 02:21:47.260]   Far too many books are written that really should have just been nice.
[02:21:47.260 --> 02:21:53.660]   I do agree on some of them, not chefs, obviously, but some of them sort of pad them out because
[02:21:53.660 --> 02:21:58.140]   they think this has got to be really thick or people aren't going to take it seriously.
[02:21:58.140 --> 02:22:01.220]   It's frustrating to me because I do audiobooks.
[02:22:01.220 --> 02:22:06.100]   And unlike paper books, it's not really a scam.
[02:22:06.100 --> 02:22:07.900]   Right.
[02:22:07.900 --> 02:22:11.340]   So if I, you know, book like that, you know, you can say, okay, let me look at the next
[02:22:11.340 --> 02:22:12.340]   chapter.
[02:22:12.340 --> 02:22:13.340]   Okay, let me look at the next chapter.
[02:22:13.340 --> 02:22:14.340]   What is he saying?
[02:22:14.340 --> 02:22:15.340]   What speed do you listen to the audiobooks on?
[02:22:15.340 --> 02:22:16.340]   Normal.
[02:22:16.340 --> 02:22:17.340]   Maybe I should listen.
[02:22:17.340 --> 02:22:18.340]   Real 3.5.
[02:22:18.340 --> 02:22:21.300]   Very, I'm starting to realize.
[02:22:21.300 --> 02:22:22.300]   1.75.
[02:22:22.300 --> 02:22:24.940]   Very few people listen to our podcast at 1.0.
[02:22:24.940 --> 02:22:26.940]   They almost all listen to sped up.
[02:22:26.940 --> 02:22:28.940]   I'm going to listen to it too.
[02:22:28.940 --> 02:22:30.820]   How do people give it up with me?
[02:22:30.820 --> 02:22:33.020]   What speed do you be able to double when I talk at?
[02:22:33.020 --> 02:22:34.660]   Yeah, don't talk like that.
[02:22:34.660 --> 02:22:36.540]   The problem is they come here and they sit in the studio.
[02:22:36.540 --> 02:22:38.260]   We all sound like Nancy Pelosi.
[02:22:38.260 --> 02:22:42.500]   I think we're all drunk because we're talking at normal speed.
[02:22:42.500 --> 02:22:44.540]   This show takes so long.
[02:22:44.540 --> 02:22:45.540]   So damn long.
[02:22:45.540 --> 02:22:46.540]   Let's send it.
[02:22:46.540 --> 02:22:47.540]   Let's send it now.
[02:22:47.540 --> 02:22:49.620]   But I'm going to end it with a number and a pick.
[02:22:49.620 --> 02:22:50.860]   How about that?
[02:22:50.860 --> 02:22:54.980]   Oh, the number is 1.3 million dollars.
[02:22:54.980 --> 02:23:00.700]   That's what this crappy Samsung laptop went for at an art auction because...
[02:23:00.700 --> 02:23:01.700]   Oh, nice.
[02:23:01.700 --> 02:23:03.460]   Oh, man.
[02:23:03.460 --> 02:23:10.460]   It's called the Persistence of Chaos, a cheap Samsung NC10 loaded with the most potent viruses
[02:23:10.460 --> 02:23:16.180]   of all time, the six viruses that cause almost $100 billion in damage.
[02:23:16.180 --> 02:23:22.820]   It was created by artist Guo O. Dung and it was sold by Sotheby's.
[02:23:22.820 --> 02:23:23.820]   Wow.
[02:23:23.820 --> 02:23:24.820]   Wow.
[02:23:24.820 --> 02:23:25.820]   Nice.
[02:23:25.820 --> 02:23:29.700]   Some are a collector for $1.3 million.
[02:23:29.700 --> 02:23:32.140]   Why didn't I think of that?
[02:23:32.140 --> 02:23:35.460]   Honestly, I could have done this.
[02:23:35.460 --> 02:23:36.940]   Isn't that what you say about modern art though?
[02:23:36.940 --> 02:23:40.580]   I could have done that.
[02:23:40.580 --> 02:23:44.980]   And my pick of the week will protect you against those viruses and a lot of other stuff.
[02:23:44.980 --> 02:23:48.540]   Don't you sell an iPod with the 10 most dangerous podcasts on it?
[02:23:48.540 --> 02:23:50.780]   Oh, brilliant.
[02:23:50.780 --> 02:23:53.620]   I'll call them netcast though and really get your attention.
[02:23:53.620 --> 02:23:54.780]   Nobody will buy it, Leo.
[02:23:54.780 --> 02:24:00.060]   So I have to do more research on the provenance of this, but I'm really impressed and I think
[02:24:00.060 --> 02:24:02.060]   it's something people should pay attention to.
[02:24:02.060 --> 02:24:04.660]   In the past, we've recommended services like Open DNS.
[02:24:04.660 --> 02:24:10.300]   You replace your internet service providers DNS numbers with Open DNS.
[02:24:10.300 --> 02:24:14.700]   That means all your DNS queries out to the internet go through them and it means they
[02:24:14.700 --> 02:24:17.980]   can do things like block stuff that's not good and so forth.
[02:24:17.980 --> 02:24:19.780]   It's really an interesting way to do it.
[02:24:19.780 --> 02:24:25.540]   CloudFlare has one, it's 1.1.1.1, it protects your privacy so that you use them instead of
[02:24:25.540 --> 02:24:29.340]   your internet service provider, thereby keeping your internet service provider from learning
[02:24:29.340 --> 02:24:31.820]   about every single thing you're doing on the internet.
[02:24:31.820 --> 02:24:39.180]   This one, they say, combines the privacy of CloudFlare DNS with the ad blocking and malware
[02:24:39.180 --> 02:24:43.420]   blocking of PiWhole, which is a Raspberry Pi project we've talked about.
[02:24:43.420 --> 02:24:45.420]   It's called NextDNS.
[02:24:45.420 --> 02:24:48.420]   It's NextDNS.io.
[02:24:48.420 --> 02:24:49.660]   It's a simple thing to do.
[02:24:49.660 --> 02:24:53.580]   You replace your regular ISP DNS with their numbers.
[02:24:53.580 --> 02:24:56.860]   Actually they support IPv6 too, which makes it really powerful.
[02:24:56.860 --> 02:24:59.340]   Then you can customize it.
[02:24:59.340 --> 02:25:02.940]   You can create a configuration without actually signing up for it.
[02:25:02.940 --> 02:25:05.820]   Let me just show you what you see.
[02:25:05.820 --> 02:25:09.940]   This is how you would set it up and they show how to do it for Windows, Mac, Android,
[02:25:09.940 --> 02:25:14.100]   iOS, or the best way to do it in your router.
[02:25:14.100 --> 02:25:17.740]   They talk about how to do it in a regular router or if you use PFSense, you can use that
[02:25:17.740 --> 02:25:18.740]   as well.
[02:25:18.740 --> 02:25:22.020]   Then they show you what you can do.
[02:25:22.020 --> 02:25:26.420]   You can turn on security, which will block malware, phishing attacks, botnets.
[02:25:26.420 --> 02:25:28.340]   You can turn on ad blocking.
[02:25:28.340 --> 02:25:30.020]   It'll block it at the routers.
[02:25:30.020 --> 02:25:31.500]   That's what PiWhole does.
[02:25:31.500 --> 02:25:33.980]   You can also block porn.
[02:25:33.980 --> 02:25:39.500]   If you really have a problem with social media, you can block all social networking sites,
[02:25:39.500 --> 02:25:42.180]   gambling sites, crypto miners.
[02:25:42.180 --> 02:25:46.420]   That's a big problem with malware that does crypto mining on your system without your
[02:25:46.420 --> 02:25:47.420]   permission.
[02:25:47.420 --> 02:25:49.420]   Take news and click bait.
[02:25:49.420 --> 02:25:52.060]   How do you do that?
[02:25:52.060 --> 02:25:53.060]   Because they're lists.
[02:25:53.060 --> 02:25:54.060]   Here's how you do that.
[02:25:54.060 --> 02:25:55.060]   You can actually go into it.
[02:25:55.060 --> 02:25:56.060]   You can see what lists.
[02:25:56.060 --> 02:25:57.060]   That's dangerous.
[02:25:57.060 --> 02:26:00.020]   Well, don't turn it on then.
[02:26:00.020 --> 02:26:01.540]   You don't have to turn it on.
[02:26:01.540 --> 02:26:03.980]   There's Fox News on there.
[02:26:03.980 --> 02:26:08.460]   What you can do is look at what lists they're using and you can even modify them.
[02:26:08.460 --> 02:26:12.940]   VPNs and proxies, peer-to-peer networks, piracy sites, and gaming sites.
[02:26:12.940 --> 02:26:19.620]   Basically, you're going to use it in a way that perhaps wouldn't make it so complete.
[02:26:19.620 --> 02:26:22.220]   You can look directly at the lists if you want.
[02:26:22.220 --> 02:26:28.380]   Various people create various lists and you can use those instead of those gross switches.
[02:26:28.380 --> 02:26:33.900]   If you want to get smarter, you can actually modify the list and so forth.
[02:26:33.900 --> 02:26:34.900]   There's quite a few.
[02:26:34.900 --> 02:26:37.500]   Or you can just go to the porn list and use all that.
[02:26:37.500 --> 02:26:41.180]   Or, yeah, you can turn it the other way around because you do have the ability to both blacklist
[02:26:41.180 --> 02:26:43.580]   and whitelist sites.
[02:26:43.580 --> 02:26:49.380]   You can also turn off access to a bunch of services, including Fortnite parents.
[02:26:49.380 --> 02:26:51.580]   You have that ability.
[02:26:51.580 --> 02:26:54.540]   Analytics are there to show you how many queries have been blocked.
[02:26:54.540 --> 02:26:58.020]   What I really like is you can create multiple configurations.
[02:26:58.020 --> 02:27:01.900]   You can have a configuration for the kids, a configuration for home, a configuration for
[02:27:01.900 --> 02:27:02.900]   work.
[02:27:02.900 --> 02:27:05.140]   It even works on mobile devices.
[02:27:05.140 --> 02:27:08.340]   I have an Android configuration, an iOS configuration.
[02:27:08.340 --> 02:27:11.140]   I was behind it.
[02:27:11.140 --> 02:27:16.980]   The Chinese government.
[02:27:16.980 --> 02:27:18.300]   I don't know.
[02:27:18.300 --> 02:27:19.860]   That would be a very important question.
[02:27:19.860 --> 02:27:21.820]   Ask before you sign up for it.
[02:27:21.820 --> 02:27:23.500]   I did read up before I recommended.
[02:27:23.500 --> 02:27:29.140]   I did read up on the people behind it and you probably should do so as well.
[02:27:29.140 --> 02:27:32.420]   It says, "Director of Engineering and Netflix."
[02:27:32.420 --> 02:27:33.420]   It's good people.
[02:27:33.420 --> 02:27:36.820]   Former head of mobile at Dailymotion.
[02:27:36.820 --> 02:27:38.980]   It's network gurus.
[02:27:38.980 --> 02:27:40.540]   Guys who understand this stuff.
[02:27:40.540 --> 02:27:45.740]   It's not that hard a thing to do, but it is a hard thing for most users to do.
[02:27:45.740 --> 02:27:49.540]   If you're smart enough to be able to change your DNS settings, this is a very interesting
[02:27:49.540 --> 02:27:50.540]   service.
[02:27:50.540 --> 02:27:51.540]   I've recommended open DNS in the past.
[02:27:51.540 --> 02:27:52.900]   I'm testing next DNS.
[02:27:52.900 --> 02:27:55.820]   I've put it on at home and on my phones.
[02:27:55.820 --> 02:28:00.780]   So far it works and it works imperceptibly and it absolutely will speed up your surfing
[02:28:00.780 --> 02:28:02.540]   in a lot of interesting ways.
[02:28:02.540 --> 02:28:04.660]   The ability to block fake news.
[02:28:04.660 --> 02:28:05.660]   Wow.
[02:28:05.660 --> 02:28:11.460]   I couldn't download this show though.
[02:28:11.460 --> 02:28:15.220]   They should have a filter for long podcasts.
[02:28:15.220 --> 02:28:18.300]   Thank you Matthew Ingram for hanging out with us for the last couple of hours.
[02:28:18.300 --> 02:28:23.220]   Chief Digital Writer at the Columbia Review of Journalism, cjr.org.
[02:28:23.220 --> 02:28:24.220]   He's on the Twitter too.
[02:28:24.220 --> 02:28:25.220]   It's always a pleasure to see you.
[02:28:25.220 --> 02:28:27.900]   It's almost like season, isn't it?
[02:28:27.900 --> 02:28:28.900]   It is almost.
[02:28:28.900 --> 02:28:31.180]   Oh, you're going out to the lake soon.
[02:28:31.180 --> 02:28:32.180]   Oh, yeah.
[02:28:32.180 --> 02:28:35.580]   Oh, I might have to rejoin Instagram just to see your canoe.
[02:28:35.580 --> 02:28:38.180]   I'm going to see Matthew soon.
[02:28:38.180 --> 02:28:39.180]   Oh, yeah.
[02:28:39.180 --> 02:28:42.180]   I have some gelato with this.
[02:28:42.180 --> 02:28:48.900]   No, I'm going to put in at the yeah, exactly at the scene in Toronto.
[02:28:48.900 --> 02:28:51.260]   McLuhan esque.
[02:28:51.260 --> 02:28:52.260]   What's it called?
[02:28:52.260 --> 02:28:54.900]   Matthew, I'm something forgetting the media ecology.
[02:28:54.900 --> 02:28:55.900]   Yeah, ecology.
[02:28:55.900 --> 02:28:56.900]   Wow.
[02:28:56.900 --> 02:28:58.900]   So I'm receiving an award.
[02:28:58.900 --> 02:28:59.900]   Congratulations.
[02:28:59.900 --> 02:29:00.900]   Oh, yeah.
[02:29:00.900 --> 02:29:01.900]   Mazel.
[02:29:01.900 --> 02:29:04.620]   What's the James Carey award?
[02:29:04.620 --> 02:29:05.620]   Nice.
[02:29:05.620 --> 02:29:07.940]   Oh, my worship, James Carey.
[02:29:07.940 --> 02:29:10.940]   Yeah, he's a huge fan.
[02:29:10.940 --> 02:29:12.780]   A theorist, a media theorist.
[02:29:12.780 --> 02:29:14.260]   Funny you should ask.
[02:29:14.260 --> 02:29:15.260]   There you go.
[02:29:15.260 --> 02:29:20.060]   James Carey, Columbia professor, the late James Carey.
[02:29:20.060 --> 02:29:21.060]   Brilliant.
[02:29:21.060 --> 02:29:26.300]   He's the one I quote all the time about how it's conversation, folks.
[02:29:26.300 --> 02:29:28.060]   It's not content.
[02:29:28.060 --> 02:29:29.060]   Exactly.
[02:29:29.060 --> 02:29:30.060]   So you're going to...
[02:29:30.060 --> 02:29:32.660]   So it's only one R.
[02:29:32.660 --> 02:29:33.780]   The two R.
[02:29:33.780 --> 02:29:34.980]   James Carey is different.
[02:29:34.980 --> 02:29:36.420]   Yeah, not the comedian.
[02:29:36.420 --> 02:29:37.420]   That's Jim Carey.
[02:29:37.420 --> 02:29:38.420]   Yeah, yeah.
[02:29:38.420 --> 02:29:39.420]   It's the...
[02:29:39.420 --> 02:29:41.580]   It was the media research award.
[02:29:41.580 --> 02:29:42.980]   Is that what you're getting?
[02:29:42.980 --> 02:29:44.620]   No, so I'm getting the...
[02:29:44.620 --> 02:29:46.340]   No, he's going to be the Carey award.
[02:29:46.340 --> 02:29:47.820]   Just the Carey award, even though...
[02:29:47.820 --> 02:29:50.020]   James Carey award for outstanding journalism.
[02:29:50.020 --> 02:29:51.020]   Wow.
[02:29:51.020 --> 02:29:53.100]   Kevin Kelly, I think, got it.
[02:29:53.100 --> 02:29:55.260]   David Carr, a bunch of great people.
[02:29:55.260 --> 02:29:56.260]   Oh, great company.
[02:29:56.260 --> 02:29:57.260]   Yeah.
[02:29:57.260 --> 02:29:58.260]   Yeah.
[02:29:58.260 --> 02:29:59.260]   Congratulations.
[02:29:59.260 --> 02:30:00.260]   That was a nice...
[02:30:00.260 --> 02:30:01.260]   That's really nice.
[02:30:01.260 --> 02:30:02.260]   Mm-hmm.
[02:30:02.260 --> 02:30:03.260]   Congratulations, Matt.
[02:30:03.260 --> 02:30:04.260]   That's really great.
[02:30:04.260 --> 02:30:05.260]   Yes, Mazel.
[02:30:05.260 --> 02:30:06.260]   Okay.
[02:30:06.260 --> 02:30:10.620]   And despite the Yiddish, I don't think he's Jewish, but that doesn't matter.
[02:30:10.620 --> 02:30:13.700]   You don't have to be Jewish to love rye bread.
[02:30:13.700 --> 02:30:15.700]   Jeff Jarvis, Buzz Machine...
[02:30:15.700 --> 02:30:16.700]   I'm a New Yorker.
[02:30:16.700 --> 02:30:17.700]   He's a New Yorker.
[02:30:17.700 --> 02:30:18.860]   You got a problem with that?
[02:30:18.860 --> 02:30:21.420]   I got no problem with that.
[02:30:21.420 --> 02:30:23.860]   Although now you can use your Apple watch to get on the subway.
[02:30:23.860 --> 02:30:28.660]   I feel like it's not exactly, you know, the tough city it used to be.
[02:30:28.660 --> 02:30:29.660]   Thank you so much.
[02:30:29.660 --> 02:30:31.460]   Wait, do you get the rats?
[02:30:31.460 --> 02:30:32.460]   Please, please, please.
[02:30:32.460 --> 02:30:33.460]   The rats are...
[02:30:33.460 --> 02:30:34.460]   Paul Danson rats.
[02:30:34.460 --> 02:30:35.460]   That's true.
[02:30:35.460 --> 02:30:36.460]   Yep.
[02:30:36.460 --> 02:30:39.540]   Tao professor of journalistic innovation at the Craig Newmark Graduate School of Journalism
[02:30:39.540 --> 02:30:43.140]   at the city university of New York, Buzz Machine dot com at Jeff Jarvis.
[02:30:43.140 --> 02:30:44.860]   What would Google do?
[02:30:44.860 --> 02:30:46.700]   Thank you so much for being here.
[02:30:46.700 --> 02:30:51.060]   And former TV guide critic.
[02:30:51.060 --> 02:30:56.100]   We do this week in Google every Wednesday about 130 Pacific, 430 Eastern, 2030 UTC.
[02:30:56.100 --> 02:30:59.580]   Stop by and watch or listen live at twit.tv/live.
[02:30:59.580 --> 02:31:04.420]   Chat with us if you're watching live at IRC dot twit.tv.
[02:31:04.420 --> 02:31:07.300]   And of course, on-demand versions of the show are available at our website.
[02:31:07.300 --> 02:31:08.660]   That's why it's a netcast.
[02:31:08.660 --> 02:31:10.700]   Twit.tv/twig.
[02:31:10.700 --> 02:31:15.180]   Subscribe in your favorite podcast player and you'll get it automatically every Wednesday
[02:31:15.180 --> 02:31:16.660]   evening.
[02:31:16.660 --> 02:31:17.660]   Thanks so much for listening.
[02:31:17.660 --> 02:31:20.020]   We'll see you next time on this week in Google.
[02:31:20.020 --> 02:31:20.540]   Bye-bye.
[02:31:20.540 --> 02:31:30.540]   [MUSIC]

