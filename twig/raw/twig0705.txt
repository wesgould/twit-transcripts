;FFMETADATA1
title=I Was Merely Fluffing
artist=Leo Laporte, Jeff Jarvis, Ant Pruitt, Jason Howell
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-03-02
track=705
language=English
genre=Podcast
comment=TikTok ban coming to the US? ChatGPT Windows 11 taskbar, Twitter job cuts
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100

[00:00:00.000 --> 00:00:02.720]   It's time for Twig this week in Google.
[00:00:02.720 --> 00:00:04.400]   Aunt Pruitt, Jeff Jarvis are here.
[00:00:04.400 --> 00:00:06.560]   Stacey's got the week off, but Jason Howell from all
[00:00:06.560 --> 00:00:09.680]   about Android fills in, and we had some great conversations.
[00:00:09.680 --> 00:00:12.640]   It could be the end of the line for TikTok.
[00:00:12.640 --> 00:00:15.680]   I'll play devil's advocate and say, it's about time.
[00:00:15.680 --> 00:00:17.800]   We'll also talk about Microsoft.
[00:00:17.800 --> 00:00:20.560]   They say they've got the new Bing powered chat
[00:00:20.560 --> 00:00:22.880]   GPT in Windows 11.
[00:00:22.880 --> 00:00:24.960]   They lie.
[00:00:24.960 --> 00:00:28.960]   And we'll talk about Twitter cuts, and who is in and who
[00:00:28.960 --> 00:00:29.460]   is out.
[00:00:29.460 --> 00:00:32.280]   We may even have an inkling of who the next CEO of Twitter
[00:00:32.280 --> 00:00:32.560]   will be.
[00:00:32.560 --> 00:00:34.440]   It's all coming up next on Twig.
[00:00:34.440 --> 00:00:41.720]   Podcasts you love from people you trust.
[00:00:41.720 --> 00:00:42.960]   This is Twig.
[00:00:42.960 --> 00:00:49.840]   This is Twig.
[00:00:49.840 --> 00:00:53.840]   This week in Google, episode 705, recorded Wednesday,
[00:00:53.840 --> 00:00:56.520]   March 1, 2023.
[00:00:56.520 --> 00:00:59.320]   I was merely fluffing.
[00:00:59.320 --> 00:01:02.320]   This week in Google is brought to you by Fast Mail.
[00:01:02.320 --> 00:01:05.120]   Reclaim your privacy, boost productivity,
[00:01:05.120 --> 00:01:08.080]   and make email yours with Fast Mail.
[00:01:08.080 --> 00:01:11.680]   Try it free for 30 days at fastmail.com/twit.
[00:01:11.680 --> 00:01:13.840]   Fast Mail is also giving Twit listeners
[00:01:13.840 --> 00:01:19.760]   a 15% discount on the first year when you sign up today.
[00:01:19.760 --> 00:01:21.280]   Thanks for listening to this show.
[00:01:21.280 --> 00:01:24.000]   As an ad supported network, we are always
[00:01:24.000 --> 00:01:27.160]   looking for new partners with products and services that
[00:01:27.160 --> 00:01:30.200]   will benefit our qualified audience.
[00:01:30.200 --> 00:01:31.800]   Are you ready to grow your business?
[00:01:31.800 --> 00:01:35.760]   Reach out to advertise at twit.tv and launch your campaign
[00:01:35.760 --> 00:01:37.800]   now.
[00:01:37.800 --> 00:01:40.280]   It's time for Twig this week in Google.
[00:01:40.280 --> 00:01:44.160]   The show we cover the latest news from the Googlers.
[00:01:44.160 --> 00:01:47.000]   And Pruitt's here in his pink hat.
[00:01:47.000 --> 00:01:49.040]   It's not pink, but it's orange.
[00:01:49.040 --> 00:01:51.000]   It's orange, right?
[00:01:51.000 --> 00:01:51.440]   Yes, sir.
[00:01:51.440 --> 00:01:51.940]   OK.
[00:01:51.940 --> 00:01:54.800]   Clem's in orange, hands-on photography host,
[00:01:54.800 --> 00:01:57.540]   also the community manager for a club,
[00:01:57.540 --> 00:02:00.000]   and just a man about the Twit.
[00:02:00.000 --> 00:02:01.520]   He's always here, which is nice.
[00:02:01.520 --> 00:02:03.660]   Runs our Floss show.
[00:02:03.660 --> 00:02:05.520]   I see every Sunday now and has to tech guys.
[00:02:05.520 --> 00:02:08.440]   In fact, we're going to do a thing with you on Sunday.
[00:02:08.440 --> 00:02:09.880]   Yes, sir.
[00:02:09.880 --> 00:02:12.120]   We're going to have a therapy session on Sunday, sir.
[00:02:12.120 --> 00:02:13.640]   That's what I'm calling it.
[00:02:13.640 --> 00:02:18.680]   Can we get a couch or anything here for Andy can lie on?
[00:02:18.680 --> 00:02:21.240]   He's switched from Windows to Mac, and he needs therapy.
[00:02:21.240 --> 00:02:22.240]   Oh, that's what it is.
[00:02:22.240 --> 00:02:24.220]   I was like, who's giving the therapy and who's receiving
[00:02:24.220 --> 00:02:24.720]   therapy?
[00:02:24.720 --> 00:02:25.460]   I have to know.
[00:02:25.460 --> 00:02:28.260]   So I think Micah's going to talk him off the ledge.
[00:02:28.260 --> 00:02:28.740]   We'll see.
[00:02:28.740 --> 00:02:29.380]   We'll try.
[00:02:29.380 --> 00:02:30.420]   Oh, boy.
[00:02:30.420 --> 00:02:31.380]   We'll try.
[00:02:31.380 --> 00:02:34.380]   Also with this, the director of the Townite Center
[00:02:34.380 --> 00:02:37.380]   for Entrepreneurial Journalism at the Craig Newmark.
[00:02:37.380 --> 00:02:40.260]   [MUSIC - KRAIG NEWMARK, "KRAIG NEWMARK"]
[00:02:40.260 --> 00:02:43.380]   Graduate School of Journalism at City University of New York.
[00:02:43.380 --> 00:02:44.820]   Mr. Jeffrey Jarvis.
[00:02:44.820 --> 00:02:46.220]   Hello, Jeff.
[00:02:46.220 --> 00:02:47.820]   Hello there, and I will call him to the show
[00:02:47.820 --> 00:02:49.540]   to convince Ant that he did it all wrong.
[00:02:49.540 --> 00:02:51.060]   He just come to the Chromebook.
[00:02:51.060 --> 00:02:53.060]   Come to the Chromebook side.
[00:02:53.060 --> 00:02:55.560]   Come to the Chromebook side.
[00:02:55.560 --> 00:02:57.040]   Was that over an option, Ant?
[00:02:57.040 --> 00:02:59.040]   Was that ever a consideration for you?
[00:02:59.040 --> 00:03:00.800]   No, never.
[00:03:00.800 --> 00:03:05.640]   Last Sunday in Aztec, I demoed that a new Acer gaming laptop.
[00:03:05.640 --> 00:03:07.200]   Was that at all intriguing to you?
[00:03:07.200 --> 00:03:09.040]   That was a nice Chromebook.
[00:03:09.040 --> 00:03:12.120]   It was a nice Chromebook, but that did not intrigue me at all,
[00:03:12.120 --> 00:03:16.080]   because the most I could do would be the venture resolve
[00:03:16.080 --> 00:03:17.520]   on a Linux partition.
[00:03:17.520 --> 00:03:18.480]   Yeah, that wouldn't work.
[00:03:18.480 --> 00:03:19.560]   That's it.
[00:03:19.560 --> 00:03:20.060]   Yeah.
[00:03:20.060 --> 00:03:22.360]   No, I just need to be able to turn a diagram, computer
[00:03:22.360 --> 00:03:23.220]   on, and get the work.
[00:03:23.220 --> 00:03:26.740]   As a production machine, and not just producing words
[00:03:26.740 --> 00:03:29.240]   on a screen in a dock or whatever,
[00:03:29.240 --> 00:03:30.440]   you need more than a--
[00:03:30.440 --> 00:03:31.960]   And I think the Mac is a good choice.
[00:03:31.960 --> 00:03:33.160]   I do too.
[00:03:33.160 --> 00:03:34.540]   Yeah, I think I like it.
[00:03:34.540 --> 00:03:37.080]   Sorry, Jeff.
[00:03:37.080 --> 00:03:38.200]   Although you can play--
[00:03:38.200 --> 00:03:39.160]   I'm sorry for Ant.
[00:03:39.160 --> 00:03:42.200]   I feel bad for Ant having to go into one empire
[00:03:42.200 --> 00:03:43.000]   into the other.
[00:03:43.000 --> 00:03:44.920]   You can play Son of the Forest on it, though,
[00:03:44.920 --> 00:03:47.480]   which is pretty cool.
[00:03:47.480 --> 00:03:49.780]   Oh, normally Stacey would be here, Stacey's
[00:03:49.780 --> 00:03:51.140]   is a little under the weather.
[00:03:51.140 --> 00:03:53.060]   But that's good news for us, because we've
[00:03:53.060 --> 00:03:54.140]   got Jason Howell from--
[00:03:54.140 --> 00:03:54.940]   Hello.
[00:03:54.940 --> 00:03:56.420]   --Haul about hand-roids.
[00:03:56.420 --> 00:03:58.060]   Hole in a bound hot rod.
[00:03:58.060 --> 00:04:00.620]   You got Carl Peay coming on the show?
[00:04:00.620 --> 00:04:01.820]   Yeah.
[00:04:01.820 --> 00:04:03.460]   Yes.
[00:04:03.460 --> 00:04:04.900]   Big announcement.
[00:04:04.900 --> 00:04:06.100]   Oh, I'm sorry.
[00:04:06.100 --> 00:04:07.380]   Was it a secret?
[00:04:07.380 --> 00:04:08.700]   I don't know if it was a secret.
[00:04:08.700 --> 00:04:10.540]   But yes, we do have Carl Peay coming on the show
[00:04:10.540 --> 00:04:11.700]   in a couple of weeks.
[00:04:11.700 --> 00:04:12.740]   From the nothing company.
[00:04:12.740 --> 00:04:13.180]   Yeah.
[00:04:13.180 --> 00:04:14.380]   CEO of Nothing.
[00:04:14.380 --> 00:04:15.300]   That's a big deal.
[00:04:15.300 --> 00:04:15.800]   He was--
[00:04:15.800 --> 00:04:16.300]   Big deal.
[00:04:16.300 --> 00:04:18.520]   He was at 1+ for a long time.
[00:04:18.520 --> 00:04:21.520]   Yeah, he was the 1+ guy for the longest time.
[00:04:21.520 --> 00:04:24.440]   And then the parent company sucked it back in.
[00:04:24.440 --> 00:04:25.440]   Then Oppo.
[00:04:25.440 --> 00:04:26.360]   That's right.
[00:04:26.360 --> 00:04:28.800]   And a lot of people have not been very happy about that.
[00:04:28.800 --> 00:04:29.300]   No.
[00:04:29.300 --> 00:04:29.800]   Yeah, I know.
[00:04:29.800 --> 00:04:31.120]   The reverberations of that.
[00:04:31.120 --> 00:04:34.000]   And so I think Peay said, oh, there's an opportunity here
[00:04:34.000 --> 00:04:36.280]   with Android, and he created the Nothing company.
[00:04:36.280 --> 00:04:36.800]   Yeah.
[00:04:36.800 --> 00:04:38.200]   And they have the Nothing phone, which--
[00:04:38.200 --> 00:04:40.000]   Did they sell that in the US yet?
[00:04:40.000 --> 00:04:41.560]   Nothing phone won now.
[00:04:41.560 --> 00:04:42.680]   You can get it in the US.
[00:04:42.680 --> 00:04:44.680]   Of course, you're not going to get it in carrier stores
[00:04:44.680 --> 00:04:45.240]   and everything.
[00:04:45.240 --> 00:04:47.900]   But they are selling it online.
[00:04:47.900 --> 00:04:48.580]   You can get it.
[00:04:48.580 --> 00:04:51.320]   They've announced, though, that the Nothing phone
[00:04:51.320 --> 00:04:53.940]   too officially comes to the US.
[00:04:53.940 --> 00:04:55.860]   Like, you can get the Nothing phone one.
[00:04:55.860 --> 00:04:56.780]   But I don't know.
[00:04:56.780 --> 00:04:58.580]   You're probably going to have to jump through some hoops.
[00:04:58.580 --> 00:04:59.380]   Nothing too.
[00:04:59.380 --> 00:05:00.820]   They're bringing directly to the US.
[00:05:00.820 --> 00:05:02.540]   And I think they even had announcement at Mobile World
[00:05:02.540 --> 00:05:05.380]   Congress that they're going to have the latest Snapdragon
[00:05:05.380 --> 00:05:06.300]   powered in there.
[00:05:06.300 --> 00:05:08.060]   So it's going to be a higher powered--
[00:05:08.060 --> 00:05:10.420]   Well, they still have the LEDs on the back.
[00:05:10.420 --> 00:05:12.180]   I mean, why wouldn't they?
[00:05:12.180 --> 00:05:14.820]   That seems to be the big trademark
[00:05:14.820 --> 00:05:15.660]   of this device.
[00:05:15.660 --> 00:05:18.120]   But whether it's a good trademark, I don't know.
[00:05:18.120 --> 00:05:18.820]   But it's different.
[00:05:18.820 --> 00:05:19.820]   It's different.
[00:05:19.820 --> 00:05:21.120]   I mean, maybe it's a dead part.
[00:05:21.120 --> 00:05:22.020]   It's just a thing.
[00:05:22.020 --> 00:05:24.160]   It doesn't do anything, right?
[00:05:24.160 --> 00:05:27.280]   I mean, it's-- well, style, of course,
[00:05:27.280 --> 00:05:28.860]   but also notification.
[00:05:28.860 --> 00:05:31.680]   There's some integration into the system
[00:05:31.680 --> 00:05:35.620]   with notifications, alerts, charging status.
[00:05:35.620 --> 00:05:36.080]   That's right.
[00:05:36.080 --> 00:05:38.100]   It'll show you how much your battery is charged based
[00:05:38.100 --> 00:05:38.300]   on that.
[00:05:38.300 --> 00:05:41.280]   But it means you can't put a case on it, I suppose not.
[00:05:41.280 --> 00:05:43.400]   Because you wouldn't see the glyph.
[00:05:43.400 --> 00:05:46.240]   Because it's drained the battery, Jason.
[00:05:46.240 --> 00:05:48.400]   Well, I mean, I have not used the device myself.
[00:05:48.400 --> 00:05:52.160]   It's LED, so it shouldn't be too much, right?
[00:05:52.160 --> 00:05:52.660]   Yeah.
[00:05:52.660 --> 00:05:55.640]   I mean, does it drain the battery more than if the LEDs were
[00:05:55.640 --> 00:05:56.280]   there?
[00:05:56.280 --> 00:05:56.800]   Yeah.
[00:05:56.800 --> 00:05:59.600]   But I don't know how much it's really--
[00:05:59.600 --> 00:06:01.120]   And I'm going to guess it's not--
[00:06:01.120 --> 00:06:03.280]   even though it looks like it's lines of LED,
[00:06:03.280 --> 00:06:08.640]   it's probably just one LED with a kind of light conductive tape.
[00:06:08.640 --> 00:06:10.040]   So it's not--
[00:06:10.040 --> 00:06:11.600]   it's maybe three LEDs.
[00:06:11.600 --> 00:06:14.400]   But the phones used to have those notification lights.
[00:06:14.400 --> 00:06:16.000]   In fact, I miss those to be honest.
[00:06:16.000 --> 00:06:16.500]   Yeah.
[00:06:16.500 --> 00:06:17.360]   A little alert light.
[00:06:17.360 --> 00:06:18.840]   I didn't enjoy that display.
[00:06:18.840 --> 00:06:19.400]   Yeah.
[00:06:19.400 --> 00:06:20.280]   Bring them back.
[00:06:20.280 --> 00:06:21.680]   So this is-- I understand.
[00:06:21.680 --> 00:06:23.800]   Anyway, it's a big get.
[00:06:23.800 --> 00:06:25.880]   And it'll be exciting to see what Carl has to say.
[00:06:25.880 --> 00:06:26.400]   Yeah.
[00:06:26.400 --> 00:06:27.280]   Definitely looking at that.
[00:06:27.280 --> 00:06:30.520]   Coming soon to all about Android, let's say when.
[00:06:30.520 --> 00:06:32.240]   You know what's coming soon to the United States?
[00:06:32.240 --> 00:06:36.680]   I think a TikTok ban, a real live, honest to goodness.
[00:06:36.680 --> 00:06:41.240]   TikTok ban, the story, of course, a couple of days ago,
[00:06:41.240 --> 00:06:44.360]   was that the federal agencies are now
[00:06:44.360 --> 00:06:48.800]   have 30 days to get TikTok off devices.
[00:06:48.800 --> 00:06:52.400]   Now, these are devices that are owned by the federal agencies
[00:06:52.400 --> 00:06:54.400]   and then used by employees.
[00:06:54.400 --> 00:06:57.640]   I think employees could still have it on their personal device,
[00:06:57.640 --> 00:06:59.200]   I would think.
[00:06:59.200 --> 00:07:04.320]   Chris Ray, who has been kind of virulently nasty about China
[00:07:04.320 --> 00:07:08.360]   and especially-- but he's one of the people who's saying
[00:07:08.360 --> 00:07:11.480]   COVID was released from the Wohann Labs,
[00:07:11.480 --> 00:07:13.520]   despite the fact that there's disagreement
[00:07:13.520 --> 00:07:14.840]   among the intelligence agencies.
[00:07:14.840 --> 00:07:17.440]   He's acting like, oh, no, it's a Chinese.
[00:07:17.440 --> 00:07:21.080]   And now, I think he's been going hard against TikTok,
[00:07:21.080 --> 00:07:22.240]   and now he might be getting his way,
[00:07:22.240 --> 00:07:29.560]   because the House Committee approved,
[00:07:29.560 --> 00:07:32.960]   has passed the bill along, the Foreign Affairs Committee
[00:07:32.960 --> 00:07:35.800]   voted on Wednesday along party lines,
[00:07:35.800 --> 00:07:41.640]   to give Joe Biden the power to ban TikTok in the US.
[00:07:41.640 --> 00:07:44.440]   But when you say that, it's like in the US dot dot dot
[00:07:44.440 --> 00:07:46.560]   on government devices, right?
[00:07:46.560 --> 00:07:48.720]   No, no, no, this is like ban--
[00:07:48.720 --> 00:07:50.600]   It's a long-- for everybody?
[00:07:50.600 --> 00:07:51.640]   For everyone.
[00:07:51.640 --> 00:07:52.400]   That's major.
[00:07:52.400 --> 00:07:54.800]   No more TikTok corner for us.
[00:07:54.800 --> 00:07:56.080]   No kidding.
[00:07:56.080 --> 00:07:57.880]   The bill does not precisely specify
[00:07:57.880 --> 00:08:00.040]   this according to Reuters how the ban will work,
[00:08:00.040 --> 00:08:04.360]   gives Biden power to ban any transactions with TikTok,
[00:08:04.360 --> 00:08:05.920]   which in turn could prevent anyone in the US
[00:08:05.920 --> 00:08:08.160]   from accessing or downloading the app on their phones.
[00:08:08.160 --> 00:08:11.600]   It would effectively ban it, because it would force
[00:08:11.600 --> 00:08:14.440]   the Android Play Store and the Apple Store
[00:08:14.440 --> 00:08:17.320]   to take it off the store, just as they did with Huawei,
[00:08:17.320 --> 00:08:17.760]   by the way.
[00:08:17.760 --> 00:08:20.800]   They've basically put Huawei out of business in the US
[00:08:20.800 --> 00:08:23.280]   by saying no company could do business with Huawei.
[00:08:23.280 --> 00:08:26.400]   They could do this similar thing to TikTok.
[00:08:26.400 --> 00:08:28.080]   You would still have it on your phone,
[00:08:28.080 --> 00:08:29.480]   if you had it on your phone.
[00:08:29.480 --> 00:08:31.360]   You could probably still sideload it, right?
[00:08:31.360 --> 00:08:32.760]   Like that's one way--
[00:08:32.760 --> 00:08:33.280]   Possibly.
[00:08:33.280 --> 00:08:36.280]   That these things you could use it on the web.
[00:08:36.280 --> 00:08:37.800]   You could use it on the web, yeah, right.
[00:08:37.800 --> 00:08:40.800]   They're blocking website access.
[00:08:40.800 --> 00:08:42.160]   And TikTok wouldn't have any way to make money.
[00:08:42.160 --> 00:08:43.800]   But there's got to be a First Amendment case here.
[00:08:43.800 --> 00:08:46.880]   Yeah, I mean, I'm sure-- well, I don't know.
[00:08:46.880 --> 00:08:49.400]   We've banned Huawei.
[00:08:49.400 --> 00:08:49.840]   Yeah.
[00:08:49.840 --> 00:08:50.760]   CFIUS--
[00:08:50.760 --> 00:08:54.560]   This is the user's own platform for speech.
[00:08:54.560 --> 00:08:56.640]   Well, it seems to me-- and I have to say,
[00:08:56.640 --> 00:08:59.760]   Mike Masnick, a tech nerd, used the phrase,
[00:08:59.760 --> 00:09:02.080]   you'll be happy to know, "Moral Panic."
[00:09:02.080 --> 00:09:02.560]   Whoa.
[00:09:02.560 --> 00:09:03.800]   Yeah, that's a few--
[00:09:03.800 --> 00:09:04.880]   Carvody, actually.
[00:09:04.880 --> 00:09:05.640]   We've got this article--
[00:09:05.640 --> 00:09:06.160]   Carvody.
[00:09:06.160 --> 00:09:06.160]   Carvody.
[00:09:06.160 --> 00:09:07.160]   Yeah, it's in there.
[00:09:07.160 --> 00:09:08.640]   Yeah, it's in there.
[00:09:08.640 --> 00:09:11.640]   Our growing TikTok, "Moral Panic,"
[00:09:11.640 --> 00:09:15.640]   still isn't addressing the actual problem.
[00:09:15.640 --> 00:09:18.640]   The problem really being--
[00:09:18.640 --> 00:09:22.920]   And honestly, I understand why a company or government agency
[00:09:22.920 --> 00:09:24.840]   would say ban TikTok.
[00:09:24.840 --> 00:09:28.600]   Just like, for instance, the military banned Strava,
[00:09:28.600 --> 00:09:32.280]   the running app, because it turned out a lot of military,
[00:09:32.280 --> 00:09:34.160]   were using it to map their runs.
[00:09:34.160 --> 00:09:37.920]   And it's certainly giving a map of the inside of the Pentagon
[00:09:37.920 --> 00:09:39.640]   to understand why the government--
[00:09:39.640 --> 00:09:41.480]   would they ban it for government users, for everybody?
[00:09:41.480 --> 00:09:42.680]   For government.
[00:09:42.680 --> 00:09:43.840]   Oh, government users.
[00:09:43.840 --> 00:09:45.720]   And I understand that.
[00:09:45.720 --> 00:09:48.360]   And I think that's certainly within their preview.
[00:09:48.360 --> 00:09:50.280]   And I guess you could make a case.
[00:09:50.280 --> 00:09:53.240]   But honestly, do they ban Facebook?
[00:09:53.240 --> 00:09:56.520]   Well, I think that's a point that Karl really
[00:09:56.520 --> 00:10:00.160]   makes in this article that is absolutely true, which is OK.
[00:10:00.160 --> 00:10:02.480]   So we can look at TikTok, and we can say, hey, you're doing
[00:10:02.480 --> 00:10:04.600]   all this stuff, you're collecting all this data.
[00:10:04.600 --> 00:10:06.760]   We don't know exactly what's happening behind the scenes
[00:10:06.760 --> 00:10:08.960]   that data is that transferring over to the Chinese government.
[00:10:08.960 --> 00:10:11.400]   Blah, blah, a lot of people assume or think
[00:10:11.400 --> 00:10:14.040]   that they have the details that point to that actually happening.
[00:10:14.040 --> 00:10:15.840]   But then we've got all of these other apps that
[00:10:15.840 --> 00:10:16.800]   are on our phones.
[00:10:16.800 --> 00:10:20.320]   Facebook, Instagram-- they're all tracking our location.
[00:10:20.320 --> 00:10:22.440]   They're all doing stuff with our data.
[00:10:22.440 --> 00:10:25.640]   Why is it different over here versus over here?
[00:10:25.640 --> 00:10:28.240]   I mean, it really doesn't seem to solve the problem.
[00:10:28.240 --> 00:10:31.160]   If you have a problem with this kind of information,
[00:10:31.160 --> 00:10:35.080]   data brokers and everything having access to that information,
[00:10:35.080 --> 00:10:36.360]   then do something about that.
[00:10:36.360 --> 00:10:38.880]   Why is it so lucrative to trade in people's data?
[00:10:38.880 --> 00:10:40.880]   And maybe that's the problem.
[00:10:40.880 --> 00:10:44.320]   You said another key phrase in there, far as TikTok.
[00:10:44.320 --> 00:10:48.520]   You said that we don't know what they're doing with that information
[00:10:48.520 --> 00:10:50.080]   once it goes back to China.
[00:10:50.080 --> 00:10:52.800]   Hey, we don't know what to do with the information on those three
[00:10:52.800 --> 00:10:55.920]   platforms you just mentioned that are here in the US.
[00:10:55.920 --> 00:10:57.880]   So why aren't we--
[00:10:57.880 --> 00:10:58.800]   Which is Carl's point.
[00:10:58.800 --> 00:11:00.320]   --down on those folks, other than just Brian's--
[00:11:00.320 --> 00:11:01.800]   Carl writes a couple months.
[00:11:01.800 --> 00:11:02.560]   Oh, right, yeah.
[00:11:02.560 --> 00:11:03.480]   That's what Carl says.
[00:11:03.480 --> 00:11:06.880]   Carl writes, yes, TikTok plays fast and loose with consumer data.
[00:11:06.880 --> 00:11:10.160]   So does nearly every other foreign and domestic app and service
[00:11:10.160 --> 00:11:11.360]   on your phone.
[00:11:11.360 --> 00:11:13.760]   From apps that track and monetize your every waking
[00:11:13.760 --> 00:11:16.800]   movement in granular detail to apps and services
[00:11:16.800 --> 00:11:20.200]   that casually traffic in your mental health specifics.
[00:11:20.200 --> 00:11:22.760]   And that's before you get to the telecom industry, which
[00:11:22.760 --> 00:11:25.440]   has pioneered irresponsible collection and monetization
[00:11:25.440 --> 00:11:28.680]   of user info, all this data fed into a massive
[00:11:28.680 --> 00:11:31.600]   and intentionally confusing data broker market.
[00:11:31.600 --> 00:11:33.800]   Remember those two words, data broker,
[00:11:33.800 --> 00:11:37.200]   that regulators have been generally disinterested
[00:11:37.200 --> 00:11:42.640]   in seriously policing, less US companies, CASP, lose money.
[00:11:42.640 --> 00:11:43.360]   Oh, no.
[00:11:43.360 --> 00:11:44.840]   We don't want to pass some autosize.
[00:11:44.840 --> 00:11:47.480]   Privacy larvae, tough on data brokers, app makers, OEMs,
[00:11:47.480 --> 00:11:50.040]   or telecobs, because rampant surveillance and data
[00:11:50.040 --> 00:11:52.680]   monetization is simply too lucrative.
[00:11:52.680 --> 00:11:55.720]   So in a way, this is hand waving and saying,
[00:11:55.720 --> 00:11:58.040]   pay no attention to what's going on in the US.
[00:11:58.040 --> 00:11:59.160]   It's China.
[00:11:59.160 --> 00:11:59.680]   China.
[00:11:59.680 --> 00:12:00.760]   It's political.
[00:12:00.760 --> 00:12:02.320]   Yeah, political points.
[00:12:02.320 --> 00:12:04.200]   You get another distraction.
[00:12:04.200 --> 00:12:08.200]   And data brokers, Axiom, has been around long before the internet.
[00:12:08.200 --> 00:12:10.080]   I've talked about this before, where
[00:12:10.080 --> 00:12:11.800]   I've freaked out students by showing
[00:12:11.800 --> 00:12:15.160]   how I can get the names and addresses of women, two miles
[00:12:15.160 --> 00:12:19.280]   from me, who have these characteristics, which you can't do.
[00:12:19.280 --> 00:12:21.680]   Otherwise, Google doesn't know all that.
[00:12:21.680 --> 00:12:23.440]   But Axiom does.
[00:12:23.440 --> 00:12:25.880]   And it's supported and used by magazine companies
[00:12:25.880 --> 00:12:27.200]   and media companies all over.
[00:12:27.200 --> 00:12:31.520]   Well, and as Karl points, in history, it's trivially easy
[00:12:31.520 --> 00:12:33.800]   for the Chinese, Iranian, or any other government intelligence
[00:12:33.800 --> 00:12:37.040]   agents to buy this data from a lot of data brokers.
[00:12:37.040 --> 00:12:38.280]   That's a really great point.
[00:12:38.280 --> 00:12:40.240]   You don't need TikTok.
[00:12:40.240 --> 00:12:42.560]   It's all available if they want it.
[00:12:42.560 --> 00:12:45.040]   The other claim probably says the Chinese government will use
[00:12:45.040 --> 00:12:49.400]   TikTok to fill US kids' heads with gibberish and propaganda.
[00:12:49.400 --> 00:12:51.320]   But not only on the left, Karl.
[00:12:51.320 --> 00:12:52.240]   He's been on the show.
[00:12:52.240 --> 00:12:53.120]   We've got to get him on again.
[00:12:53.120 --> 00:12:55.520]   Not only is there no evidence that's actually happening at scale.
[00:12:55.520 --> 00:12:57.760]   It's a rich concern coming from a country
[00:12:57.760 --> 00:13:02.960]   so inundated in authoritarian propaganda across AM radio, Fox,
[00:13:02.960 --> 00:13:06.000]   and clear the internet that residents increasingly
[00:13:06.000 --> 00:13:09.400]   engage in widespread domestic terrorism.
[00:13:09.400 --> 00:13:12.640]   Yeah, it's coming from within the house.
[00:13:12.640 --> 00:13:15.160]   But it's really, I think far too easy.
[00:13:15.160 --> 00:13:18.040]   And I think Chris Ray is an example of this.
[00:13:18.040 --> 00:13:21.120]   To just deflect blame, I say blaming China.
[00:13:21.120 --> 00:13:22.920]   There's also the FCC commissioner who's
[00:13:22.920 --> 00:13:24.760]   been calling for TikTok's ban, who
[00:13:24.760 --> 00:13:27.760]   has notoriously lax on regulating telecom,
[00:13:27.760 --> 00:13:30.080]   because he takes money from the telecom industry.
[00:13:30.080 --> 00:13:35.000]   So I think this is lip service only to privacy.
[00:13:35.000 --> 00:13:38.680]   It's politically expedient because blame China
[00:13:38.680 --> 00:13:42.280]   seems to be the popular thing right now.
[00:13:42.280 --> 00:13:43.720]   Even when we say this, people say, well,
[00:13:43.720 --> 00:13:47.200]   why are you giving you support to the enemy?
[00:13:47.200 --> 00:13:51.080]   Well, I'm giving you the support to Elon by being there.
[00:13:51.080 --> 00:13:51.960]   Not me.
[00:13:51.960 --> 00:13:54.880]   I withdrew my aid and support to Elon.
[00:13:54.880 --> 00:13:58.400]   ACLU on Twitter, though, did tweet.
[00:13:58.400 --> 00:14:01.200]   This bill is a serious violation of our First Amendment
[00:14:01.200 --> 00:14:05.840]   rights Congress must vote no on this vague, overbroad,
[00:14:05.840 --> 00:14:10.360]   and unconstitutional legislation.
[00:14:10.360 --> 00:14:11.920]   Somebody in the chat room has pointed out,
[00:14:11.920 --> 00:14:16.240]   and I think it's true, that yes, there's First Amendment
[00:14:16.240 --> 00:14:16.760]   rights.
[00:14:16.760 --> 00:14:19.720]   But as soon as it comes down to national security,
[00:14:19.720 --> 00:14:23.240]   we'll look at what just happened in the Supreme Court.
[00:14:23.240 --> 00:14:29.640]   They decided to not review the Wikimedia's lawsuits
[00:14:29.640 --> 00:14:32.960]   against NSA's collection of data,
[00:14:32.960 --> 00:14:35.560]   blanket collection of data, saying national security.
[00:14:35.560 --> 00:14:39.200]   The NSA's defense wasn't-- we're not doing it.
[00:14:39.200 --> 00:14:42.360]   We're not only targeting international people.
[00:14:42.360 --> 00:14:43.560]   They didn't say any of that.
[00:14:43.560 --> 00:14:45.240]   They just said, hey, national security,
[00:14:45.240 --> 00:14:48.640]   if we were to stop that national security.
[00:14:48.640 --> 00:14:50.120]   Think of the children.
[00:14:50.120 --> 00:14:53.360]   So I think that's the problem, is that, yeah, the ACLU
[00:14:53.360 --> 00:14:53.960]   could fight this.
[00:14:53.960 --> 00:14:55.720]   They could take it all the way to the Supreme Court,
[00:14:55.720 --> 00:14:58.960]   and the Supreme Court would say, no, national security.
[00:14:58.960 --> 00:15:01.000]   Without any evidence, without any known evidence,
[00:15:01.000 --> 00:15:02.880]   without any actual discussion of what's happening,
[00:15:02.880 --> 00:15:04.480]   what is the problem with all of this
[00:15:04.480 --> 00:15:06.800]   is we don't talk about actual harm.
[00:15:06.800 --> 00:15:08.200]   We talk about fears.
[00:15:08.200 --> 00:15:09.960]   We talk about could-be's.
[00:15:09.960 --> 00:15:12.160]   But the evidence of harm is not here.
[00:15:12.160 --> 00:15:14.000]   All right, I am going to be-- because somebody
[00:15:14.000 --> 00:15:15.840]   has to argue this--
[00:15:15.840 --> 00:15:17.880]   and I think there are people-- it's a legitimate point of view.
[00:15:17.880 --> 00:15:19.480]   There are a lot of people in our audience who say, no,
[00:15:19.480 --> 00:15:21.200]   wait a minute.
[00:15:21.200 --> 00:15:22.760]   A, it's just a social media app.
[00:15:22.760 --> 00:15:23.880]   What's the harm in banning it?
[00:15:23.880 --> 00:15:26.600]   B, it's pretty clear the Chinese government
[00:15:26.600 --> 00:15:29.960]   has access to anything TikTok has access to.
[00:15:29.960 --> 00:15:33.480]   C, we already know TikTok employees
[00:15:33.480 --> 00:15:36.000]   were using location information from TikTok
[00:15:36.000 --> 00:15:39.120]   to find out what journalists were meeting.
[00:15:39.120 --> 00:15:41.440]   They were trying to track down a leak.
[00:15:41.440 --> 00:15:42.840]   So they looked at where journalists,
[00:15:42.840 --> 00:15:45.560]   American journalists, were on their TikTok app
[00:15:45.560 --> 00:15:48.520]   to see if they were meeting with any TikTok employees.
[00:15:48.520 --> 00:15:51.240]   So we already know they've reached that boundary.
[00:15:51.240 --> 00:15:53.120]   So why shouldn't we ban TikTok?
[00:15:53.120 --> 00:15:58.600]   I mean, this is pretty clearly a hazardous app.
[00:15:58.600 --> 00:15:59.760]   Yeah, there are other problems.
[00:15:59.760 --> 00:16:00.840]   That doesn't go over that.
[00:16:00.840 --> 00:16:02.280]   There are other problems.
[00:16:02.280 --> 00:16:04.400]   Yeah, we should-- let's take a look at Facebook
[00:16:04.400 --> 00:16:05.040]   and telecoms too.
[00:16:05.040 --> 00:16:09.360]   But right now we've got this proximate problem with TikTok.
[00:16:09.360 --> 00:16:10.200]   What's wrong with banning it?
[00:16:10.200 --> 00:16:11.520]   That doesn't answer the national security.
[00:16:11.520 --> 00:16:14.320]   Right, yeah, we have a problem with Facebook and Instagram
[00:16:14.320 --> 00:16:17.080]   and everyone else is tracking our own people,
[00:16:17.080 --> 00:16:20.400]   our own citizens, and trying to target stuff for us
[00:16:20.400 --> 00:16:22.160]   for their own profitable gain.
[00:16:22.160 --> 00:16:24.080]   So that's a right now.
[00:16:24.080 --> 00:16:25.400]   Yeah, well, OK, so--
[00:16:25.400 --> 00:16:26.960]   Why?
[00:16:26.960 --> 00:16:28.160]   We'll get to that.
[00:16:28.160 --> 00:16:29.360]   But right now we're--
[00:16:29.360 --> 00:16:29.840]   Get to that?
[00:16:29.840 --> 00:16:30.920]   We'll get to that.
[00:16:30.920 --> 00:16:31.760]   We'll get to that.
[00:16:31.760 --> 00:16:33.200]   This doesn't mean getting live.
[00:16:33.200 --> 00:16:35.640]   It's not an offense to say, well, everybody does it.
[00:16:35.640 --> 00:16:36.760]   That's not an offense.
[00:16:36.760 --> 00:16:37.880]   Backyard.
[00:16:37.880 --> 00:16:41.080]   That's the stuff that I've always been bugged about
[00:16:41.080 --> 00:16:43.960]   with our government is we do so many things that are--
[00:16:43.960 --> 00:16:47.000]   and I'm going to piss some people off.
[00:16:47.000 --> 00:16:50.400]   We put our nose in a lot of different business elsewhere,
[00:16:50.400 --> 00:16:53.280]   but we really do a really crap-tastic job of taking care
[00:16:53.280 --> 00:16:57.440]   of our own backyard and the issues right here.
[00:16:57.440 --> 00:17:01.080]   Yeah, but why should we let a foreign government
[00:17:01.080 --> 00:17:02.880]   have access to all of that information?
[00:17:02.880 --> 00:17:06.480]   Well, but to your Mr. Devil's advocate,
[00:17:06.480 --> 00:17:08.760]   your argument about the employees going
[00:17:08.760 --> 00:17:10.920]   after a couple of reporters as a reporter,
[00:17:10.920 --> 00:17:11.640]   I shouldn't like that.
[00:17:11.640 --> 00:17:14.240]   But A, they got fired and B, that doesn't--
[00:17:14.240 --> 00:17:15.360]   They got fired when they got caught.
[00:17:15.360 --> 00:17:17.360]   They got fired when they got fired.
[00:17:17.360 --> 00:17:17.880]   Yeah.
[00:17:17.880 --> 00:17:18.880]   And what it demonstrated--
[00:17:18.880 --> 00:17:20.200]   You're right, it doesn't affect National Security.
[00:17:20.200 --> 00:17:21.960]   But it demonstrated they have the ability and the will.
[00:17:21.960 --> 00:17:23.600]   Someone had the ability to do that.
[00:17:23.600 --> 00:17:24.880]   Somebody did.
[00:17:24.880 --> 00:17:26.640]   But we have the anti-tiktok.
[00:17:26.640 --> 00:17:29.640]   Now let's say President Xi or his proxy
[00:17:29.640 --> 00:17:32.480]   goes to Mr. TikTok and says, hey,
[00:17:32.480 --> 00:17:34.440]   I'd like that same information, please.
[00:17:34.440 --> 00:17:36.440]   I noticed you did that.
[00:17:36.440 --> 00:17:37.320]   Well, here's the other--
[00:17:37.320 --> 00:17:38.160]   Well, here's the other--
[00:17:38.160 --> 00:17:41.040]   The side of this coin, Leo, Mr. Devil's advocate,
[00:17:41.040 --> 00:17:44.600]   is that we're setting a precedent here, devil,
[00:17:44.600 --> 00:17:45.360]   to be used by our real government.
[00:17:45.360 --> 00:17:47.560]   I appreciate you saying that over and over
[00:17:47.560 --> 00:17:50.520]   so that nobody thinks that actually espouses this point.
[00:17:50.520 --> 00:17:51.440]   I know, exactly.
[00:17:51.440 --> 00:17:52.560]   But I do have to--
[00:17:52.560 --> 00:17:54.240]   I should probably recuse myself,
[00:17:54.240 --> 00:17:57.640]   because my son makes a living on TikTok.
[00:17:57.640 --> 00:17:58.960]   He's an example of impacts.
[00:17:58.960 --> 00:18:00.800]   And this is what some people say is, yeah,
[00:18:00.800 --> 00:18:03.000]   but think of all the creators who use TikTok,
[00:18:03.000 --> 00:18:03.800]   which, intimately--
[00:18:03.800 --> 00:18:04.160]   He's hurt.
[00:18:04.160 --> 00:18:07.120]   Somebody like my boy who has made a very good living.
[00:18:07.120 --> 00:18:08.120]   He's now got a cookbook.
[00:18:08.120 --> 00:18:09.800]   He's talking about a TV show.
[00:18:09.800 --> 00:18:12.640]   He was in Miami making Cubano sandwiches
[00:18:12.640 --> 00:18:16.480]   with Guy Fieri, all because of TikTok.
[00:18:16.480 --> 00:18:18.480]   Yeah, but don't worry, YouTube shorts
[00:18:18.480 --> 00:18:20.480]   just cross 50 billion daily views.
[00:18:20.480 --> 00:18:20.840]   Oh, please.
[00:18:20.840 --> 00:18:22.640]   So he could just go over YouTube.
[00:18:22.640 --> 00:18:23.880]   He's on Insta.
[00:18:23.880 --> 00:18:26.400]   It's TikTok that really drove his success
[00:18:26.400 --> 00:18:28.560]   2.1 million followers.
[00:18:28.560 --> 00:18:29.760]   But Taylor's in two.
[00:18:29.760 --> 00:18:30.960]   He actually--
[00:18:30.960 --> 00:18:32.120]   He was very smart.
[00:18:32.120 --> 00:18:36.040]   He-- we had this conversation and we went to lunch.
[00:18:36.040 --> 00:18:38.240]   It was like a year and a half ago.
[00:18:38.240 --> 00:18:41.600]   And he said, Dad, I got about 30,000 followers on TikTok
[00:18:41.600 --> 00:18:42.640]   doing these cooking things.
[00:18:42.640 --> 00:18:43.320]   What do you think?
[00:18:43.320 --> 00:18:47.120]   I said, I think you should go all in and push it.
[00:18:47.120 --> 00:18:48.480]   And he studied the algorithm.
[00:18:48.480 --> 00:18:49.840]   He figured out what people wanted.
[00:18:49.840 --> 00:18:50.600]   He looked at comments.
[00:18:50.600 --> 00:18:53.960]   He really got involved, carefully tailored something
[00:18:53.960 --> 00:18:55.320]   to TikTok, and it was a huge success.
[00:18:55.320 --> 00:18:56.880]   He was able to do that.
[00:18:56.880 --> 00:18:57.880]   And he did it really well.
[00:18:57.880 --> 00:18:59.360]   He created a career around it.
[00:18:59.360 --> 00:19:01.600]   And it's given him a living.
[00:19:01.600 --> 00:19:02.600]   So now--
[00:19:02.600 --> 00:19:03.800]   So now-- oh, I'm sorry.
[00:19:03.800 --> 00:19:05.400]   I took off my doubles advocate.
[00:19:05.400 --> 00:19:06.600]   Where'd I put it?
[00:19:06.600 --> 00:19:09.760]   But I actually asked him.
[00:19:09.760 --> 00:19:11.840]   I said, are you worried about TikTok being banned?
[00:19:11.840 --> 00:19:15.040]   He said, no, I got the Instagram now.
[00:19:15.040 --> 00:19:15.760]   I make more money.
[00:19:15.760 --> 00:19:17.120]   I'm getting more engaged on Instagram.
[00:19:17.120 --> 00:19:20.560]   So he was-- which does not mean that--
[00:19:20.560 --> 00:19:23.320]   he used TikTok to get to the next level.
[00:19:23.320 --> 00:19:24.320]   That was his entree.
[00:19:24.320 --> 00:19:26.880]   That's not just the whole part of being a content creator,
[00:19:26.880 --> 00:19:27.600]   though, sir.
[00:19:27.600 --> 00:19:30.800]   It's being able to not necessarily depend
[00:19:30.800 --> 00:19:32.640]   on one particular product.
[00:19:32.640 --> 00:19:35.160]   You sort of market yourself everywhere,
[00:19:35.160 --> 00:19:38.400]   because stuff goes away, especially when there's companies
[00:19:38.400 --> 00:19:40.800]   like Google involved, and all of a sudden it kills something
[00:19:40.800 --> 00:19:42.320]   off, you still want to be able to have some type of--
[00:19:42.320 --> 00:19:42.320]   Right.
[00:19:42.320 --> 00:19:43.280]   By pointing exactly.
[00:19:43.280 --> 00:19:44.640]   That's why we should ban TikTok,
[00:19:44.640 --> 00:19:46.560]   because there's other avenues.
[00:19:46.560 --> 00:19:48.640]   It's not like we're going to lose anything.
[00:19:48.640 --> 00:19:50.600]   [LAUGHTER]
[00:19:50.600 --> 00:19:51.960]   So why should we let--
[00:19:51.960 --> 00:19:52.440]   All right.
[00:19:52.440 --> 00:19:56.080]   Why should we let the Chinese spy on our people?
[00:19:56.080 --> 00:19:59.520]   It's not like it's the only way that people can create
[00:19:59.520 --> 00:20:01.120]   and put stuff out.
[00:20:01.120 --> 00:20:03.040]   Let's ban the product.
[00:20:03.040 --> 00:20:05.600]   We've got Montessori, or whatever his name is.
[00:20:05.600 --> 00:20:07.320]   Adam Misori at Instagram.
[00:20:07.320 --> 00:20:08.160]   Why should we--
[00:20:08.160 --> 00:20:11.440]   Allow him to dig into the analytics of us and--
[00:20:11.440 --> 00:20:12.240]   I'll get to that.
[00:20:12.240 --> 00:20:14.800]   --and assume that we just want to see more reels.
[00:20:14.800 --> 00:20:15.760]   We're going to get to that.
[00:20:15.760 --> 00:20:16.400]   We're on Instagram.
[00:20:16.400 --> 00:20:17.400]   The committee's looking at it.
[00:20:17.400 --> 00:20:18.920]   It's a tiger-fucking platform.
[00:20:18.920 --> 00:20:22.200]   But we feel that you all really want to see more videos,
[00:20:22.200 --> 00:20:24.680]   and we noticed that you're all not watching these videos.
[00:20:24.680 --> 00:20:27.080]   So we're going to dial it back just a little bit.
[00:20:27.080 --> 00:20:30.920]   Do you think a Chinese company should have the same rights
[00:20:30.920 --> 00:20:32.640]   and privileges as a US company?
[00:20:32.640 --> 00:20:35.720]   Well, here's the issue, Mr. Senator Devil,
[00:20:35.720 --> 00:20:41.720]   is that other countries have said the same thing about us.
[00:20:41.720 --> 00:20:42.400]   So--
[00:20:42.400 --> 00:20:43.440]   About all their rights.
[00:20:43.440 --> 00:20:48.440]   --Germany, U, Canada, Brazil have all tried to pass laws
[00:20:48.440 --> 00:20:51.400]   saying that America does horrible things with data.
[00:20:51.400 --> 00:20:53.720]   That number one, at least data should be stored locally.
[00:20:53.720 --> 00:20:55.320]   Though, of course, that's so the government can get hold
[00:20:55.320 --> 00:20:56.800]   of it in that country.
[00:20:56.800 --> 00:20:58.920]   Or number two, that we shouldn't trust America at all.
[00:20:58.920 --> 00:21:00.520]   We should ban American apps.
[00:21:00.520 --> 00:21:01.040]   That's fine.
[00:21:01.040 --> 00:21:03.080]   We're setting a precedent here around China.
[00:21:03.080 --> 00:21:04.480]   Let them try us.
[00:21:04.480 --> 00:21:07.520]   Because we make the gosh darn best apps in the whole wide world.
[00:21:07.520 --> 00:21:09.440]   And let them try it.
[00:21:09.440 --> 00:21:12.640]   Just like France tried-- and Spain tried to ban Google.
[00:21:12.640 --> 00:21:15.720]   Let them try it because they'll come crawling back to us
[00:21:15.720 --> 00:21:18.640]   because we make the best damn apps in the world.
[00:21:18.640 --> 00:21:20.840]   Boy, you really do a great demo of that.
[00:21:20.840 --> 00:21:21.360]   Yeah.
[00:21:21.360 --> 00:21:22.600]   I'm firing up right now.
[00:21:22.600 --> 00:21:26.320]   [LAUGHTER]
[00:21:26.320 --> 00:21:27.360]   See?
[00:21:27.360 --> 00:21:28.680]   No, I see.
[00:21:28.680 --> 00:21:29.560]   It's a good argument.
[00:21:29.560 --> 00:21:30.800]   Great demo of the advocate.
[00:21:30.800 --> 00:21:31.400]   Yeah.
[00:21:31.400 --> 00:21:33.680]   I'm running for Congress, and I'd like to have your vote,
[00:21:33.680 --> 00:21:34.680]   Mr. Pruitt.
[00:21:34.680 --> 00:21:36.880]   [LAUGHTER]
[00:21:36.880 --> 00:21:38.600]   You know, black people love me.
[00:21:38.600 --> 00:21:39.360]   They love me.
[00:21:39.360 --> 00:21:40.440]   I just want you to know.
[00:21:40.440 --> 00:21:41.920]   Right.
[00:21:41.920 --> 00:21:43.840]   I know as a Trump said, the black people.
[00:21:43.840 --> 00:21:45.360]   I should say, the blacks.
[00:21:45.360 --> 00:21:46.360]   The blacks love me.
[00:21:46.360 --> 00:21:47.240]   There you go.
[00:21:47.240 --> 00:21:48.280]   They love me.
[00:21:48.280 --> 00:21:51.080]   So just warning you.
[00:21:51.080 --> 00:21:52.080]   Good.
[00:21:52.080 --> 00:21:54.120]   Actually Biden didn't-- Biden said it.
[00:21:54.120 --> 00:21:56.360]   He said, if you don't vote for me, you're not black.
[00:21:56.360 --> 00:21:57.920]   There's another-- that's another good one.
[00:21:57.920 --> 00:21:58.880]   That's-- yeah.
[00:21:58.880 --> 00:21:59.960]   Good one.
[00:21:59.960 --> 00:22:01.000]   You ain't black.
[00:22:01.000 --> 00:22:02.080]   What are you talking about?
[00:22:02.080 --> 00:22:03.200]   Yes, I am.
[00:22:03.200 --> 00:22:04.480]   Well, I'm not.
[00:22:04.480 --> 00:22:05.800]   But anyway.
[00:22:05.800 --> 00:22:08.320]   [LAUGHTER]
[00:22:08.320 --> 00:22:10.760]   OK, I think I-- I think you stumped.
[00:22:10.760 --> 00:22:11.760]   Yeah, I think you stumped.
[00:22:11.760 --> 00:22:12.680]   You did your best.
[00:22:12.680 --> 00:22:15.800]   You did your best, but didn't win the day.
[00:22:15.800 --> 00:22:16.680]   I did win the day.
[00:22:16.680 --> 00:22:17.400]   You're all shut up.
[00:22:17.400 --> 00:22:18.160]   No, you didn't.
[00:22:18.160 --> 00:22:20.920]   Well, so what happens then in this world
[00:22:20.920 --> 00:22:25.600]   where the devil wins and TikTok is banned in the US?
[00:22:25.600 --> 00:22:27.280]   Now we go after Facebook next.
[00:22:27.280 --> 00:22:27.920]   Is that what happens next?
[00:22:27.920 --> 00:22:29.000]   Because I told Antwe would.
[00:22:29.000 --> 00:22:30.520]   Or China is still an expensive--
[00:22:30.520 --> 00:22:31.920]   More than a computer later than it might be.
[00:22:31.920 --> 00:22:36.880]   No, I think it's a mistake to say that a Chinese company
[00:22:36.880 --> 00:22:38.640]   should have the same rights as an American company.
[00:22:38.640 --> 00:22:42.080]   If you're an American company, that's different.
[00:22:42.080 --> 00:22:43.080]   Protection isn't, man.
[00:22:43.080 --> 00:22:44.520]   Protection isn't all the way.
[00:22:44.520 --> 00:22:47.720]   No, but also those are bad guys.
[00:22:47.720 --> 00:22:49.480]   We're the good guys.
[00:22:49.480 --> 00:22:53.960]   No, Chinese, only rice from Sacramento.
[00:22:53.960 --> 00:22:58.480]   So an immigrant comes in and breaks into my home
[00:22:58.480 --> 00:23:00.200]   and decides to steal some of my stuff.
[00:23:00.200 --> 00:23:01.520]   That's a problem.
[00:23:01.520 --> 00:23:04.680]   But if my neighbor that's been here for all of their life
[00:23:04.680 --> 00:23:07.200]   decides to come in and steal my stuff,
[00:23:07.200 --> 00:23:09.800]   that's OK because he's been there all his life.
[00:23:09.800 --> 00:23:12.280]   Is that what you're saying?
[00:23:12.280 --> 00:23:14.640]   Well, now, if you really want to get down to law and order,
[00:23:14.640 --> 00:23:16.280]   that's another conversation.
[00:23:16.280 --> 00:23:18.680]   [LAUGHTER]
[00:23:18.680 --> 00:23:22.280]   Hey, he's an American, so I'm just OK.
[00:23:22.280 --> 00:23:26.200]   No, but we're not talking about individuals stealing stuff.
[00:23:26.200 --> 00:23:32.040]   From you, we're talking about companies doing their job.
[00:23:32.040 --> 00:23:33.680]   Companies own a bunch of individuals.
[00:23:33.680 --> 00:23:35.240]   But I don't trust-- look, don't you
[00:23:35.240 --> 00:23:38.320]   know that any Chinese company has to do the bidding
[00:23:38.320 --> 00:23:39.200]   of the Chinese government?
[00:23:39.200 --> 00:23:41.840]   That's the way it's constructed.
[00:23:41.840 --> 00:23:42.160]   Right?
[00:23:42.160 --> 00:23:43.000]   The CCP gets--
[00:23:43.000 --> 00:23:45.840]   Any American company does the bidding of people like Elon Musk?
[00:23:45.840 --> 00:23:46.680]   Well, that's different.
[00:23:46.680 --> 00:23:47.760]   He's just private citizens.
[00:23:47.760 --> 00:23:48.840]   It's not like the--
[00:23:48.840 --> 00:23:52.200]   I mean, it's pretty obvious that the federal government
[00:23:52.200 --> 00:23:54.120]   told Twitter what to do.
[00:23:54.120 --> 00:23:55.920]   But we've investigated that.
[00:23:55.920 --> 00:23:56.840]   Thank you very much.
[00:23:56.840 --> 00:24:02.760]   Just-- I think you could say on the face of it
[00:24:02.760 --> 00:24:08.280]   that you're asking for the rights of a Chinese company
[00:24:08.280 --> 00:24:15.000]   to invade our privacy, to propagandize our youth.
[00:24:15.000 --> 00:24:16.520]   You're saying that they have the right to do that.
[00:24:16.520 --> 00:24:19.120]   And I say they don't.
[00:24:19.120 --> 00:24:22.320]   Are you done with your-- can you take your hat off now?
[00:24:22.320 --> 00:24:24.760]   I think the hats permanently stuck out at this point.
[00:24:24.760 --> 00:24:25.600]   I'm a little concerned.
[00:24:25.600 --> 00:24:26.120]   I like this.
[00:24:26.120 --> 00:24:27.680]   I think I might run for Congress.
[00:24:27.680 --> 00:24:28.600]   I got the tie.
[00:24:28.600 --> 00:24:30.200]   Yeah, you're incredibly--
[00:24:30.200 --> 00:24:30.720]   --weissive.
[00:24:30.720 --> 00:24:31.200]   --weissive.
[00:24:31.200 --> 00:24:31.680]   Yeah, it's really bad.
[00:24:31.680 --> 00:24:33.920]   You also have the coffee stains on the shirt.
[00:24:33.920 --> 00:24:34.920]   No, shh.
[00:24:34.920 --> 00:24:40.960]   I didn't go to Harvard for 18 years
[00:24:40.960 --> 00:24:44.160]   to be treated like this.
[00:24:44.160 --> 00:24:46.760]   You know I'm a professional ice skater, don't you?
[00:24:46.760 --> 00:24:49.760]   All right, moving on.
[00:24:49.760 --> 00:24:52.840]   It is funny, though, that when you dress like this,
[00:24:52.840 --> 00:24:56.120]   it does kind of make you more of a Republican.
[00:24:56.120 --> 00:24:59.640]   I just-- I'm not kidding.
[00:24:59.640 --> 00:25:00.840]   I'm not kidding.
[00:25:00.840 --> 00:25:04.240]   I just feel more-- I feel like I'm right.
[00:25:04.240 --> 00:25:06.160]   Oh, gosh.
[00:25:06.160 --> 00:25:07.680]   Dress for success, that's what they say.
[00:25:07.680 --> 00:25:12.280]   So do you think-- so I don't know if this is related or not.
[00:25:12.280 --> 00:25:19.920]   But they were asking last week the International Trade
[00:25:19.920 --> 00:25:24.560]   Commission banned Apple watches.
[00:25:24.560 --> 00:25:26.400]   And they're potentially going to ban
[00:25:26.400 --> 00:25:29.000]   the import of Apple watches because they believe
[00:25:29.000 --> 00:25:31.960]   Apple infringed on a patent with a company called LiveCore
[00:25:31.960 --> 00:25:33.280]   for the EKG.
[00:25:33.280 --> 00:25:37.280]   This thing that you use, the LiveCore Cardia, Jeff, right?
[00:25:37.280 --> 00:25:37.680]   Right.
[00:25:37.680 --> 00:25:40.080]   Cardia and where--
[00:25:40.080 --> 00:25:41.680]   Who, where, what is it?
[00:25:41.680 --> 00:25:42.440]   International--
[00:25:42.440 --> 00:25:42.920]   US.
[00:25:42.920 --> 00:25:43.440]   U.S.
[00:25:43.440 --> 00:25:44.440]   U.S.
[00:25:44.440 --> 00:25:44.960]   US.
[00:25:44.960 --> 00:25:46.000]   The banned--
[00:25:46.000 --> 00:25:48.320]   They would ban the import of Apple watches
[00:25:48.320 --> 00:25:51.400]   because they say they violated a LiveCore's patents.
[00:25:51.400 --> 00:25:52.120]   Now it's interesting--
[00:25:52.120 --> 00:25:53.440]   But it's more complicated than that
[00:25:53.440 --> 00:25:57.120]   because the US Patent and Trademark Office invalidated
[00:25:57.120 --> 00:26:01.720]   LiveCore's patents minutes before the ITC said,
[00:26:01.720 --> 00:26:02.360]   we're going to ban it.
[00:26:02.360 --> 00:26:03.960]   But that's another thing.
[00:26:03.960 --> 00:26:08.040]   So Apple went to President Biden and said,
[00:26:08.040 --> 00:26:11.520]   would you please veto that ban?
[00:26:11.520 --> 00:26:13.320]   We don't-- we-- you know, this--
[00:26:13.320 --> 00:26:15.160]   and he refused to.
[00:26:15.160 --> 00:26:17.560]   So that in a while, if that's related or a piece of evidence,
[00:26:17.560 --> 00:26:21.360]   but he refused to, do you think Biden, who now has
[00:26:21.360 --> 00:26:26.040]   the encouragement of the US Congress to ban TikTok,
[00:26:26.040 --> 00:26:27.240]   that he now has the power to do that,
[00:26:27.240 --> 00:26:28.720]   do you think he will do that?
[00:26:28.720 --> 00:26:31.000]   Biden's a lot of it at his core.
[00:26:31.000 --> 00:26:32.120]   I like Joe.
[00:26:32.120 --> 00:26:32.920]   Joe's my man.
[00:26:32.920 --> 00:26:36.360]   But Biden, on section 230, on TikTok, on this kind of stuff,
[00:26:36.360 --> 00:26:40.240]   he's got people in his administration who are reflexively--
[00:26:40.240 --> 00:26:42.040]   More all panicky.
[00:26:42.040 --> 00:26:45.800]   Yeah, remember Biden's administration
[00:26:45.800 --> 00:26:49.520]   sent a brief to the Supreme Court against section 230?
[00:26:49.520 --> 00:26:50.120]   Yeah, oh yeah.
[00:26:50.120 --> 00:26:54.160]   He said when he was campaigning, he thought 1030
[00:26:54.160 --> 00:26:57.960]   should be overturned, which is protect the 26 words that
[00:26:57.960 --> 00:27:00.160]   created the internet.
[00:27:00.160 --> 00:27:03.000]   So it's unknown.
[00:27:03.000 --> 00:27:03.560]   But I--
[00:27:03.560 --> 00:27:04.480]   Yeah, I'm worried about Biden.
[00:27:04.480 --> 00:27:05.520]   I think he could actually do it.
[00:27:05.520 --> 00:27:08.480]   I think he'd actually ban TikTok.
[00:27:08.480 --> 00:27:10.480]   But really, what would be the real consequence of that?
[00:27:10.480 --> 00:27:13.120]   Besides the fact that we'd have to stop our TikTok segment.
[00:27:13.120 --> 00:27:15.120]   Oh, shucks.
[00:27:15.120 --> 00:27:17.280]   Hey, hey, hey.
[00:27:17.280 --> 00:27:21.000]   I'm authentically curious to know, obviously,
[00:27:21.000 --> 00:27:24.240]   if you can't get a TikTok app in the US anymore,
[00:27:24.240 --> 00:27:27.800]   if it's truly and completely cut off,
[00:27:27.800 --> 00:27:29.600]   people are going to have to stop using it.
[00:27:29.600 --> 00:27:33.280]   But I mean, man, it is so popular.
[00:27:33.280 --> 00:27:38.200]   When I think of teenagers, like the youths of today,
[00:27:38.200 --> 00:27:42.760]   and TikTok is ingrained into the fabric of being a teenager
[00:27:42.760 --> 00:27:47.080]   right now, it's part of the technological experience,
[00:27:47.080 --> 00:27:47.680]   if you're a kid.
[00:27:47.680 --> 00:27:49.480]   True of your kids, Jason?
[00:27:49.480 --> 00:27:51.960]   No, my kids do not have access to TikTok.
[00:27:51.960 --> 00:27:55.440]   Yeah, yeah, too young at this point.
[00:27:55.440 --> 00:27:57.360]   But-- It's a good parent at the table.
[00:27:57.360 --> 00:28:00.960]   Trying, trying, failing sometimes, but trying.
[00:28:00.960 --> 00:28:02.560]   I mean, but the drive is there.
[00:28:02.560 --> 00:28:03.640]   The desire is there.
[00:28:03.640 --> 00:28:07.000]   And actually, even if TikTok is banned,
[00:28:07.000 --> 00:28:10.200]   all the TikTok content ends up on YouTube anyways.
[00:28:10.200 --> 00:28:11.560]   All right, I'm sorry, Instagram.
[00:28:11.560 --> 00:28:13.000]   It's not like the content goes--
[00:28:13.000 --> 00:28:14.680]   So cross-haves the same stuff.
[00:28:14.680 --> 00:28:18.640]   We do know we have some experimental data.
[00:28:18.640 --> 00:28:21.760]   KIV in Alabama bans TikTok on government devices,
[00:28:21.760 --> 00:28:24.200]   which also banned it in state-run universities.
[00:28:24.200 --> 00:28:25.200]   So, oh, OK.
[00:28:25.200 --> 00:28:26.360]   University of Alabama--
[00:28:26.360 --> 00:28:27.120]   Oh, and that's right.
[00:28:27.120 --> 00:28:28.520]   -- ain't no TikTok.
[00:28:28.520 --> 00:28:30.640]   Access as well.
[00:28:30.640 --> 00:28:33.960]   So researchers I know are now having problems with this.
[00:28:33.960 --> 00:28:36.080]   So then-- So that means TikTok
[00:28:36.080 --> 00:28:38.640]   on the local network at the school.
[00:28:38.640 --> 00:28:41.040]   The Wi-Fi is completely inaccessible.
[00:28:41.040 --> 00:28:41.800]   Yeah, they block it.
[00:28:41.800 --> 00:28:43.800]   Still, obviously, getting it through their cellular--
[00:28:43.800 --> 00:28:46.320]   Well, and of course-- and I think that's one data point,
[00:28:46.320 --> 00:28:47.520]   which is--
[00:28:47.520 --> 00:28:49.160]   didn't really phase the students.
[00:28:49.160 --> 00:28:51.080]   Well, because they already have it on their phone anyways
[00:28:51.080 --> 00:28:52.600]   through the data connection.
[00:28:52.600 --> 00:28:53.920]   They just use the phone's data.
[00:28:53.920 --> 00:28:54.720]   Right.
[00:28:54.720 --> 00:28:56.920]   But if they can't get it there, I don't know.
[00:28:56.920 --> 00:29:01.080]   Auburn banned TikTok, and students can't stop talking
[00:29:01.080 --> 00:29:02.360]   about it, says in the New York Times.
[00:29:02.360 --> 00:29:03.200]   This is last month.
[00:29:03.200 --> 00:29:06.760]   Is Auburn a state school or how does private school?
[00:29:06.760 --> 00:29:08.160]   Well, I don't know.
[00:29:08.160 --> 00:29:09.960]   Auburn is a state.
[00:29:09.960 --> 00:29:10.480]   Yeah.
[00:29:10.480 --> 00:29:12.040]   Oh, OK.
[00:29:12.040 --> 00:29:13.280]   OK, I didn't even know that.
[00:29:13.280 --> 00:29:17.000]   So a senior at Auburn-- so surprised last month
[00:29:17.000 --> 00:29:18.800]   about a new ban on TikTok.
[00:29:18.800 --> 00:29:20.880]   She read the news alert about it aloud to her friends.
[00:29:20.880 --> 00:29:22.480]   We were like, oh, that's weird.
[00:29:22.480 --> 00:29:23.520]   Why would she do that?
[00:29:23.520 --> 00:29:26.200]   And laughed it off and moved on.
[00:29:26.200 --> 00:29:29.640]   She is the editor-in-chief of the campus newspaper, which
[00:29:29.640 --> 00:29:31.200]   has its own TikTok account.
[00:29:31.200 --> 00:29:36.320]   I think they're going to stop posting on TikTok,
[00:29:36.320 --> 00:29:39.760]   but I don't think the students are going to stop using TikTok.
[00:29:39.760 --> 00:29:42.920]   19 governors have banned TikTok.
[00:29:42.920 --> 00:29:45.400]   And what do they have in common?
[00:29:45.400 --> 00:29:46.760]   They're all Republicans.
[00:29:46.760 --> 00:29:47.760]   Conservatives?
[00:29:47.760 --> 00:29:48.240]   Yeah.
[00:29:48.240 --> 00:29:49.600]   They're banning abortion, too.
[00:29:49.600 --> 00:29:50.120]   So--
[00:29:50.120 --> 00:29:51.320]   Well, should we get more ahead of that?
[00:29:51.320 --> 00:29:54.400]   And I should point out that the Congressional Committee
[00:29:54.400 --> 00:29:58.640]   that banned it was pretty much party line vote.
[00:29:58.640 --> 00:30:00.000]   There wasn't 100% party line, right?
[00:30:00.000 --> 00:30:01.000]   Was it not--
[00:30:01.000 --> 00:30:02.920]   No, there was one or two Democrats voted in favor as well,
[00:30:02.920 --> 00:30:05.120]   I think.
[00:30:05.120 --> 00:30:05.440]   Let's see.
[00:30:05.440 --> 00:30:06.520]   Let me look at this.
[00:30:06.520 --> 00:30:13.600]   This is the House Foreign Affairs Committee 24 to 16.
[00:30:13.600 --> 00:30:15.480]   Yeah, so that's not pure party line.
[00:30:15.480 --> 00:30:17.120]   Yeah.
[00:30:17.120 --> 00:30:19.760]   Democrats opposed the bill saying it was rushed to require due
[00:30:19.760 --> 00:30:23.840]   diligence through debate and consultation with experts.
[00:30:23.840 --> 00:30:27.080]   They don't have the actual vote count here.
[00:30:27.080 --> 00:30:31.360]   But they said a long party line.
[00:30:31.360 --> 00:30:33.440]   As I remember, I read another source
[00:30:33.440 --> 00:30:38.160]   said I think one Democrat voted to ban as well.
[00:30:38.160 --> 00:30:40.280]   But is it not really partisan?
[00:30:40.280 --> 00:30:42.480]   It's not a partisan issue.
[00:30:42.480 --> 00:30:43.040]   Is it?
[00:30:43.040 --> 00:30:44.760]   What is it partisan today?
[00:30:44.760 --> 00:30:45.840]   Yeah.
[00:30:45.840 --> 00:30:46.360]   There.
[00:30:46.360 --> 00:30:47.200]   Good point.
[00:30:47.200 --> 00:30:48.400]   Well, this is the anti-China.
[00:30:48.400 --> 00:30:50.240]   You've got to be tougher on China.
[00:30:50.240 --> 00:30:51.680]   Wing, stick.
[00:30:51.680 --> 00:30:53.080]   Right.
[00:30:53.080 --> 00:30:58.080]   I've kind of felt like the big tech, anti-tech movement
[00:30:58.080 --> 00:31:00.280]   kind of crossed party lines, to be honest.
[00:31:00.280 --> 00:31:01.080]   No, I agree with you.
[00:31:01.080 --> 00:31:03.080]   Oh, it has.
[00:31:03.080 --> 00:31:05.360]   It seems like it is kind of in vogue right now
[00:31:05.360 --> 00:31:07.200]   to be very anti-tech policy.
[00:31:07.200 --> 00:31:09.920]   And this falls firmly into the--
[00:31:09.920 --> 00:31:13.880]   Is it our duty to defend tech as a tech--
[00:31:13.880 --> 00:31:16.680]   To defend the freedoms that the internet gives us.
[00:31:16.680 --> 00:31:17.720]   It's not the tech.
[00:31:17.720 --> 00:31:18.160]   Yeah.
[00:31:18.160 --> 00:31:22.520]   It's we're defending the internet and the freedoms we get.
[00:31:22.520 --> 00:31:25.440]   That's my next book.
[00:31:25.440 --> 00:31:27.760]   Because the internet isn't tech.
[00:31:27.760 --> 00:31:28.760]   It isn't wires.
[00:31:28.760 --> 00:31:29.720]   It isn't tubes.
[00:31:29.720 --> 00:31:31.280]   It isn't these companies.
[00:31:31.280 --> 00:31:34.840]   It is the ability of people who never could be heard in mass
[00:31:34.840 --> 00:31:35.560]   media to now speak.
[00:31:35.560 --> 00:31:36.360]   Voices.
[00:31:36.360 --> 00:31:37.520]   Yeah, it's the voices.
[00:31:37.520 --> 00:31:38.720]   It's the communities.
[00:31:38.720 --> 00:31:41.840]   The Black Twitter event that we held at the school two weeks
[00:31:41.840 --> 00:31:49.560]   ago, you look and the peril that it's in musk's hands now.
[00:31:49.560 --> 00:31:51.120]   But you look at what we lose there,
[00:31:51.120 --> 00:31:52.960]   the power of the movements that came there.
[00:31:52.960 --> 00:31:56.840]   And those movements are precisely what scare the old white guys
[00:31:56.840 --> 00:31:57.600]   who look like me.
[00:31:57.600 --> 00:32:06.080]   I think it's also our duty, those tech journalists,
[00:32:06.080 --> 00:32:13.800]   to weigh the evidence and to be honest about when tech isn't
[00:32:13.800 --> 00:32:15.960]   necessarily right or good.
[00:32:15.960 --> 00:32:16.520]   Oh, yes.
[00:32:16.520 --> 00:32:17.520]   Oh, for sure.
[00:32:17.520 --> 00:32:18.440]   That's what I was thinking.
[00:32:18.440 --> 00:32:19.200]   Oh, I agree with that.
[00:32:19.200 --> 00:32:23.240]   I don't want to unilaterally defend big tech and just say, oh, no, no,
[00:32:23.240 --> 00:32:25.240]   I totally need to criticize them.
[00:32:25.240 --> 00:32:26.040]   Absolutely.
[00:32:26.040 --> 00:32:29.240]   But we also have to separate out those things which are
[00:32:29.240 --> 00:32:31.760]   tech's fault or the internet's fault versus those things that
[00:32:31.760 --> 00:32:33.760]   are innate in our society.
[00:32:33.760 --> 00:32:36.240]   Tech and the internet didn't make us racist.
[00:32:36.240 --> 00:32:37.560]   Didn't make us misogynist.
[00:32:37.560 --> 00:32:39.240]   We came in and said, again, sir,
[00:32:39.240 --> 00:32:41.040]   say it did not make us racist.
[00:32:41.040 --> 00:32:42.480]   We already were.
[00:32:42.480 --> 00:32:45.240]   And so when we blame the internet for saying, oh, it
[00:32:45.240 --> 00:32:46.400]   increased racism in America.
[00:32:46.400 --> 00:32:47.400]   Nah, nah, it was always--
[00:32:47.400 --> 00:32:49.040]   It brought out the opposite.
[00:32:49.040 --> 00:32:53.680]   It brought a lot of brought out voices who were not heard in a white
[00:32:53.680 --> 00:32:57.680]   hegemony and the white hegemony wants to burn the place down now as a result.
[00:32:57.680 --> 00:32:58.800]   That's what's really happening.
[00:32:58.800 --> 00:33:01.240]   Mm.
[00:33:01.240 --> 00:33:02.040]   Yeah.
[00:33:02.040 --> 00:33:02.880]   Just a classic.
[00:33:02.880 --> 00:33:05.600]   It is something we--
[00:33:05.600 --> 00:33:10.680]   on all of our shows, we are always struggling with is to find the truth, right?
[00:33:10.680 --> 00:33:13.480]   And I think we try to be honest and not partisan.
[00:33:13.480 --> 00:33:17.320]   I know a lot of people think we're partisan because you're such a lefty Jeff
[00:33:17.320 --> 00:33:19.600]   Jeff, but--
[00:33:19.600 --> 00:33:21.960]   Well, Senator Devil.
[00:33:21.960 --> 00:33:29.240]   But I think really we know that it's our mandate to dig.
[00:33:29.240 --> 00:33:33.040]   And because we know tech to dig as deep as we can, it'd be critical of it,
[00:33:33.040 --> 00:33:35.600]   but also defend it as needed.
[00:33:35.600 --> 00:33:37.520]   But to really look for the truth of it.
[00:33:37.520 --> 00:33:42.800]   And the truth isn't always obvious, but tech is a double edged sword.
[00:33:42.800 --> 00:33:43.920]   Like many things.
[00:33:43.920 --> 00:33:45.840]   It can be used for good or ill.
[00:33:45.840 --> 00:33:48.400]   Mm-hmm.
[00:33:48.400 --> 00:33:54.360]   And tech used to be a cool thing that people were into.
[00:33:54.360 --> 00:33:58.800]   And now it's an integral, integrated part of the fabric of life.
[00:33:58.800 --> 00:33:59.720]   It's society.
[00:33:59.720 --> 00:34:00.240]   Yeah.
[00:34:00.240 --> 00:34:00.880]   It's everything.
[00:34:00.880 --> 00:34:01.880]   Yeah.
[00:34:01.880 --> 00:34:08.320]   So it's important, I guess, is my point for us to look at both sides of that coin.
[00:34:08.320 --> 00:34:09.760]   I want tech to be better.
[00:34:09.760 --> 00:34:12.320]   I want-- because I love technology.
[00:34:12.320 --> 00:34:13.840]   I love it when it works.
[00:34:13.840 --> 00:34:14.360]   Right.
[00:34:14.360 --> 00:34:17.920]   And if it's not working, that's why sometimes with the TikTok story,
[00:34:17.920 --> 00:34:21.480]   it's hard for me because it really goes back for me to, OK,
[00:34:21.480 --> 00:34:24.360]   I get the argument that you're making against TikTok.
[00:34:24.360 --> 00:34:25.960]   But show me the proof.
[00:34:25.960 --> 00:34:29.760]   Show me that this horrible stuff is actually happening before we
[00:34:29.760 --> 00:34:31.680]   vilify and remove it.
[00:34:31.680 --> 00:34:31.680]   Yes.
[00:34:31.680 --> 00:34:32.680]   Yes.
[00:34:32.680 --> 00:34:33.520]   Well, OK.
[00:34:33.520 --> 00:34:34.880]   Senator Devil here.
[00:34:34.880 --> 00:34:35.960]   Oh, god, he's back.
[00:34:35.960 --> 00:34:38.320]   [LAUGHTER]
[00:34:38.320 --> 00:34:39.320]   This one.
[00:34:39.320 --> 00:34:42.120]   I do not know if we would know--
[00:34:42.120 --> 00:34:42.640]   Is that a joke?
[00:34:42.640 --> 00:34:42.920]   Yeah.
[00:34:42.920 --> 00:34:43.640]   How would we know?
[00:34:43.640 --> 00:34:44.640]   I guess this is a good thing.
[00:34:44.640 --> 00:34:46.880]   If the channel was using TikTok, we don't.
[00:34:46.880 --> 00:34:47.400]   We can't stop people.
[00:34:47.400 --> 00:34:48.560]   It's behind the scenes.
[00:34:48.560 --> 00:34:49.400]   We can't tell.
[00:34:49.400 --> 00:34:50.080]   We can't tell.
[00:34:50.080 --> 00:34:52.960]   And by the time you know, it may be too late.
[00:34:52.960 --> 00:34:53.320]   Yeah.
[00:34:53.320 --> 00:34:56.600]   So why not cut it short?
[00:34:56.600 --> 00:34:57.880]   Why not just say, look, we don't--
[00:34:57.880 --> 00:34:59.000]   You've been watching the Murdoch dryer animation.
[00:34:59.000 --> 00:35:00.400]   --just where you draw that line.
[00:35:00.400 --> 00:35:01.920]   We don't need TikTok.
[00:35:01.920 --> 00:35:04.200]   Because you could say that about a million different things
[00:35:04.200 --> 00:35:06.760]   and you could draw that line anywhere you want and cut it and cut it.
[00:35:06.760 --> 00:35:08.840]   I would share a bunch of different things that I know--
[00:35:08.840 --> 00:35:12.840]   It's a pretty easy thing to do if it's something
[00:35:12.840 --> 00:35:15.040]   from an enemy nation.
[00:35:15.040 --> 00:35:21.160]   We don't allow Russian social media in the United States, right?
[00:35:21.160 --> 00:35:24.560]   Oh, I can use that today.
[00:35:24.560 --> 00:35:26.560]   Sure I can.
[00:35:26.560 --> 00:35:29.960]   What is Russian social media?
[00:35:29.960 --> 00:35:30.720]   There used to be--
[00:35:30.720 --> 00:35:34.120]   Pavel Durov created the Russian Facebook.
[00:35:34.120 --> 00:35:38.320]   In fact, that's why he left Russia and founded Telegram
[00:35:38.320 --> 00:35:39.640]   because--
[00:35:39.640 --> 00:35:44.360]   It was telegraphed, telegram, because it was taken from him, right?
[00:35:44.360 --> 00:35:46.240]   So is that still around?
[00:35:46.240 --> 00:35:47.000]   V-contaktah.
[00:35:47.000 --> 00:35:47.600]   V-contaktah.
[00:35:47.600 --> 00:35:48.080]   V-contaktah.
[00:35:48.080 --> 00:35:48.200]   V-contaktah.
[00:35:48.200 --> 00:35:48.280]   V-contaktah.
[00:35:48.280 --> 00:35:49.760]   V-k.
[00:35:49.760 --> 00:35:50.720]   Yeah.
[00:35:50.720 --> 00:35:50.920]   V-k.
[00:35:50.920 --> 00:35:52.600]   Can you use it now?
[00:35:52.600 --> 00:35:53.720]   Yeah, sure.
[00:35:53.720 --> 00:35:54.600]   Well, that should be bad.
[00:35:54.600 --> 00:35:56.040]   Welcome to the--
[00:35:56.040 --> 00:35:57.680]   Telegram.
[00:35:57.680 --> 00:36:00.280]   How about, you know, in fact--
[00:36:00.280 --> 00:36:01.160]   I think I can get it.
[00:36:01.160 --> 00:36:02.160]   Let's see if I can get it on my phone.
[00:36:02.160 --> 00:36:05.040]   656 million users as of May 21st.
[00:36:05.040 --> 00:36:05.400]   Oh.
[00:36:05.400 --> 00:36:06.760]   You're talking about Telegram?
[00:36:06.760 --> 00:36:07.840]   No, V-contaktah.
[00:36:07.840 --> 00:36:08.880]   V-k, yeah.
[00:36:08.880 --> 00:36:10.320]   Oh, V-contaktah.
[00:36:10.320 --> 00:36:13.560]   Which was his Durav's original creation.
[00:36:13.560 --> 00:36:16.400]   It was taken from him.
[00:36:16.400 --> 00:36:17.640]   And then he went off--
[00:36:17.640 --> 00:36:21.360]   Yeah, I can install it from my inventory.
[00:36:21.360 --> 00:36:25.880]   Notice they call it the largest European social network.
[00:36:25.880 --> 00:36:28.840]   Russia's always wanted to be European, haven't they?
[00:36:28.840 --> 00:36:31.520]   Well, all right, I got a better example.
[00:36:31.520 --> 00:36:33.080]   We chat.
[00:36:33.080 --> 00:36:33.680]   We chat.
[00:36:33.680 --> 00:36:34.840]   And we know.
[00:36:34.840 --> 00:36:38.440]   We know we chat is used by Chinese, military, and Chinese
[00:36:38.440 --> 00:36:44.920]   government officials, to contact overseas Chinese, to say, hey,
[00:36:44.920 --> 00:36:48.080]   we know you have family in China.
[00:36:48.080 --> 00:36:50.840]   You might want to consider what you're saying.
[00:36:50.840 --> 00:36:52.800]   We know they use it that way.
[00:36:52.800 --> 00:36:53.360]   We chat.
[00:36:53.360 --> 00:36:54.360]   And we use it--
[00:36:54.360 --> 00:36:56.800]   We use these services in Iran to try
[00:36:56.800 --> 00:36:58.600]   to undercut an evil government.
[00:36:58.600 --> 00:36:59.800]   Right.
[00:36:59.800 --> 00:37:03.000]   So why don't we ban WeChat?
[00:37:03.000 --> 00:37:05.160]   It's available in the App Store.
[00:37:05.160 --> 00:37:07.520]   And what's it going to accomplish if we do?
[00:37:07.520 --> 00:37:08.840]   That's the point I think.
[00:37:08.840 --> 00:37:10.320]   Same thing that it would accomplish
[00:37:10.320 --> 00:37:11.760]   if they got rid of TikTok.
[00:37:11.760 --> 00:37:15.000]   I mean, that's kind of part of my point
[00:37:15.000 --> 00:37:18.120]   that I was making earlier is if TikTok goes,
[00:37:18.120 --> 00:37:20.320]   I think that line gets drawn further and further,
[00:37:20.320 --> 00:37:22.960]   potentially, the WeChat or whatever.
[00:37:22.960 --> 00:37:25.120]   And I guess the reality is, I don't
[00:37:25.120 --> 00:37:29.600]   know whether it is a good idea to do it or not.
[00:37:29.600 --> 00:37:31.680]   Maybe there are reasons and everything,
[00:37:31.680 --> 00:37:34.520]   but it just seems like that line is nebulous
[00:37:34.520 --> 00:37:37.480]   and continues to stretch out if you go there.
[00:37:37.480 --> 00:37:39.240]   I got another question, Jason, out of that.
[00:37:39.240 --> 00:37:42.080]   Because you were talking about before.
[00:37:42.080 --> 00:37:45.760]   But the companies that now are proprietors of the net.
[00:37:45.760 --> 00:37:49.520]   I'm curious, if we go 10 years forward,
[00:37:49.520 --> 00:37:52.160]   do you think that Facebook will still be around?
[00:37:52.160 --> 00:37:54.160]   Do you think Google will still be dominant?
[00:37:54.160 --> 00:37:56.560]   Do you think Amazon will still be dominant?
[00:37:56.560 --> 00:37:59.720]   Do you think that some of these companies are really long term?
[00:37:59.720 --> 00:38:04.440]   Or are they like friend feed and Myspace
[00:38:04.440 --> 00:38:07.040]   that they are evanescent?
[00:38:07.040 --> 00:38:08.880]   What do you guys think?
[00:38:08.880 --> 00:38:13.080]   I mean, I think I place a Facebook and a Google
[00:38:13.080 --> 00:38:16.640]   and an Amazon in the same category as a Microsoft.
[00:38:16.640 --> 00:38:19.760]   And a Microsoft has been around a very, very long time
[00:38:19.760 --> 00:38:21.720]   and has changed and transformed over the years.
[00:38:21.720 --> 00:38:25.920]   But less powerful than it was, but still around
[00:38:25.920 --> 00:38:27.720]   and still actually pretty powerful.
[00:38:27.720 --> 00:38:32.560]   Like Microsoft's not a small company.
[00:38:32.560 --> 00:38:36.160]   They're not as powerful and influential as they once were.
[00:38:36.160 --> 00:38:38.640]   But they've certainly turned things around.
[00:38:38.640 --> 00:38:41.840]   And I think Google is kind of in that part right now.
[00:38:41.840 --> 00:38:43.080]   I don't think Google's going anywhere.
[00:38:43.080 --> 00:38:45.720]   I think 10 years, absolutely Google will be around.
[00:38:45.720 --> 00:38:48.360]   But do I think that a Facebook TikTok
[00:38:48.360 --> 00:38:50.600]   is going to be the next Google in 10 years?
[00:38:50.600 --> 00:38:51.440]   I don't know.
[00:38:51.440 --> 00:38:52.720]   Yeah, I mean, it's...
[00:38:52.720 --> 00:38:54.040]   So the reason they're being...
[00:38:54.040 --> 00:38:55.400]   I don't put them in the same class.
[00:38:55.400 --> 00:38:59.240]   I put Amazon in Microsoft in the same class.
[00:38:59.240 --> 00:39:01.160]   I don't necessarily put Google and Facebook
[00:39:01.160 --> 00:39:03.240]   in the same classes, Microsoft.
[00:39:03.240 --> 00:39:05.800]   I think Amazon has done long
[00:39:05.800 --> 00:39:10.800]   that the long tail, they've started out just selling books.
[00:39:10.800 --> 00:39:14.000]   And it continued to grow with this very, very long vision
[00:39:14.000 --> 00:39:15.680]   to get to where they are now.
[00:39:15.680 --> 00:39:19.240]   And before Bezos was out, I'm pretty sure
[00:39:19.240 --> 00:39:24.480]   the vision was put into Jassy's that the current CEO
[00:39:24.480 --> 00:39:26.960]   put into his head and the leadership to figure out,
[00:39:26.960 --> 00:39:28.760]   "Hey, we can still continue to grow
[00:39:28.760 --> 00:39:32.200]   "and continue to just do all the things the right way
[00:39:32.200 --> 00:39:34.680]   "from a data analyst standpoint
[00:39:34.680 --> 00:39:37.760]   "and just quietly keep growing and quietly
[00:39:37.760 --> 00:39:39.400]   "just keep gobbling things up,
[00:39:39.400 --> 00:39:42.240]   "all while continuing to have some small businesses
[00:39:42.240 --> 00:39:44.600]   "be on the platform and they can benefit too
[00:39:44.600 --> 00:39:48.000]   "and just keep taking over the world one bit at a time."
[00:39:48.000 --> 00:39:48.920]   But what's your point, Jeff?
[00:39:48.920 --> 00:39:50.280]   What is it?
[00:39:50.280 --> 00:39:51.920]   I don't understand what you're getting at.
[00:39:51.920 --> 00:39:54.320]   We demonize the present proprietors.
[00:39:54.320 --> 00:39:56.040]   We say, "These companies are ruining the world.
[00:39:56.040 --> 00:39:58.000]   "I don't know, we gotta do something, right?"
[00:39:58.000 --> 00:40:01.120]   Microsoft, the browsers, they're screwing everything up
[00:40:01.120 --> 00:40:03.240]   and the EU and so on and so forth.
[00:40:03.240 --> 00:40:05.560]   And I think that we end up fighting yesterday's war all the time
[00:40:05.560 --> 00:40:07.560]   and there's somebody new out there coming.
[00:40:07.560 --> 00:40:11.000]   Maybe, but remember in the teens,
[00:40:11.000 --> 00:40:13.160]   and I'm talking about the 19 teens,
[00:40:13.160 --> 00:40:17.280]   antitrust law was created because of the robber barons,
[00:40:17.280 --> 00:40:20.600]   because of the extreme power of standard oil
[00:40:20.600 --> 00:40:25.120]   and the railroads and they had such dominant market positions
[00:40:25.120 --> 00:40:27.440]   there was nobody could compete with them,
[00:40:27.440 --> 00:40:28.360]   nobody checked them.
[00:40:28.360 --> 00:40:30.640]   So the government decided that it had to
[00:40:30.640 --> 00:40:32.240]   and that's when antitrust law,
[00:40:32.240 --> 00:40:34.440]   the Sherman and trust law act and so forth,
[00:40:34.440 --> 00:40:38.560]   were enacted, isn't it appropriate that as big,
[00:40:38.560 --> 00:40:40.880]   let's say Google and Amazon,
[00:40:40.880 --> 00:40:44.280]   I think you probably have to include,
[00:40:44.280 --> 00:40:47.520]   maybe include Microsoft, but you could pick who it is.
[00:40:47.520 --> 00:40:50.840]   But as these four or five tech giants become bigger,
[00:40:50.840 --> 00:40:53.400]   more dominant, more powerful, isn't it appropriate
[00:40:53.400 --> 00:40:57.840]   just as it was in the 1910s to cut down the robber barons?
[00:40:57.840 --> 00:41:00.880]   Isn't it time to cut down them to size so that we can,
[00:41:00.880 --> 00:41:02.480]   have something new?
[00:41:02.480 --> 00:41:04.400]   You're, I guess you're saying they're gonna,
[00:41:04.400 --> 00:41:05.680]   they're gonna die on their own.
[00:41:05.680 --> 00:41:06.520]   Is that what you're saying?
[00:41:06.520 --> 00:41:11.520]   Yeah, Microsoft faded in its overarching power on its own.
[00:41:11.520 --> 00:41:15.440]   But nobody argued that standard oil was gonna die on its own.
[00:41:15.440 --> 00:41:16.560]   I mean, we broke it up.
[00:41:16.560 --> 00:41:19.840]   Well, Bell Telephone II.
[00:41:19.840 --> 00:41:20.680]   Same thing.
[00:41:20.680 --> 00:41:24.320]   But I think that these things are,
[00:41:24.320 --> 00:41:26.320]   the arc is quicker for them.
[00:41:26.320 --> 00:41:28.200]   I agree with you, Anthony, I think that Amazon,
[00:41:28.200 --> 00:41:29.880]   and I would say Google,
[00:41:29.880 --> 00:41:32.600]   and Jason, what you're saying, they're Microsoft.
[00:41:32.600 --> 00:41:34.600]   They'll go up and down, they'll be around,
[00:41:34.600 --> 00:41:36.760]   they won't be as powerful, they won't be as scary.
[00:41:36.760 --> 00:41:38.200]   I think Facebook could disappear.
[00:41:38.200 --> 00:41:40.040]   I think that's an, I think would disappear.
[00:41:40.040 --> 00:41:41.440]   Facebook is on the,
[00:41:41.440 --> 00:41:45.160]   that's an easy prediction since they're on the imminent
[00:41:45.160 --> 00:41:46.760]   cusp of disappearing, right?
[00:41:46.760 --> 00:41:47.600]   Even as we speak.
[00:41:47.600 --> 00:41:49.120]   Yeah, they're out of the discussion.
[00:41:49.120 --> 00:41:51.480]   I mean, when I go there occasionally, which I do,
[00:41:51.480 --> 00:41:53.000]   I thought I usually was often used to,
[00:41:53.000 --> 00:41:54.240]   I still find myself, oh, that friend,
[00:41:54.240 --> 00:41:55.240]   I miss that friend, I miss that friend.
[00:41:55.240 --> 00:41:57.000]   I still actually have engaged in it,
[00:41:57.000 --> 00:41:59.200]   but it's less than the public conversation,
[00:41:59.200 --> 00:42:02.000]   it's less than the fear conversation.
[00:42:02.000 --> 00:42:04.440]   The meta stuff, I think, is kind of,
[00:42:04.440 --> 00:42:05.440]   metaverse is kind of a joke.
[00:42:05.440 --> 00:42:06.280]   So it's going to go,
[00:42:06.280 --> 00:42:09.040]   Twitter is going to go bankrupt one way or the other.
[00:42:09.040 --> 00:42:15.000]   And so I think we demonize these corporations,
[00:42:15.000 --> 00:42:16.720]   and this is part of Carl's point, I think,
[00:42:16.720 --> 00:42:18.240]   in his piece about TikTok.
[00:42:18.240 --> 00:42:20.480]   Without looking at the principle level,
[00:42:20.480 --> 00:42:23.840]   we're not defending the privacy of American citizens.
[00:42:23.840 --> 00:42:26.320]   Instead, we're going after TikTok.
[00:42:26.320 --> 00:42:29.240]   Do you think it's even possible that that's why
[00:42:29.240 --> 00:42:32.040]   these members of Congress are doing this
[00:42:32.040 --> 00:42:36.320]   because it distracts from the real privacy issues?
[00:42:36.320 --> 00:42:37.600]   Yes, yes.
[00:42:37.600 --> 00:42:40.200]   I think so.
[00:42:40.200 --> 00:42:41.040]   And I think too.
[00:42:41.040 --> 00:42:42.520]   It's easy to blame China for our woes.
[00:42:42.520 --> 00:42:43.400]   Oh, yeah.
[00:42:43.400 --> 00:42:46.680]   And not address the issues, the real issue,
[00:42:46.680 --> 00:42:50.200]   had on, especially when, unfortunately,
[00:42:50.200 --> 00:42:54.720]   money is so important in our government.
[00:42:54.720 --> 00:42:57.640]   And they need them the contributions
[00:42:57.640 --> 00:42:59.400]   of these telecom companies.
[00:42:59.400 --> 00:43:01.280]   - It goes back to what Jason was saying earlier.
[00:43:01.280 --> 00:43:06.120]   The convergence of left and right attacks on the net,
[00:43:06.120 --> 00:43:08.840]   it's a pincer movement.
[00:43:08.840 --> 00:43:13.400]   And in 230, the sword and the shield,
[00:43:13.400 --> 00:43:15.520]   the Democrats go after the shield
[00:43:15.520 --> 00:43:17.640]   because they think that the platform should be better
[00:43:17.640 --> 00:43:18.760]   at taking down hate speech.
[00:43:18.760 --> 00:43:20.120]   The Republicans go after the sword
[00:43:20.120 --> 00:43:22.560]   because it's their hate speech is being taken down.
[00:43:22.560 --> 00:43:26.640]   They find themselves in this strange bedfellows
[00:43:26.640 --> 00:43:28.320]   conspiracy now against the tech companies
[00:43:28.320 --> 00:43:30.160]   because the tech companies are easy targets.
[00:43:30.160 --> 00:43:33.240]   They are folk devils for everything wrong with us.
[00:43:33.240 --> 00:43:35.520]   And so you're not just distracting
[00:43:35.520 --> 00:43:38.760]   from the tech companies and money and privacy in that.
[00:43:38.760 --> 00:43:42.400]   You're distracting again from these higher societal ills
[00:43:42.400 --> 00:43:44.360]   we have that we're not facing.
[00:43:44.360 --> 00:43:48.280]   When chat GPT spits back stupidity,
[00:43:48.280 --> 00:43:50.640]   it's our stupidity, it's spitting back.
[00:43:50.640 --> 00:43:52.320]   And we're not dealing with that.
[00:43:52.320 --> 00:43:54.600]   TikTok has been, they've seen this coming
[00:43:54.600 --> 00:43:57.280]   and they've spent more than a billion and a half dollars
[00:43:57.280 --> 00:44:00.440]   so far trying to respond to this moving
[00:44:00.440 --> 00:44:03.380]   their data centers to the US to Oracle.
[00:44:03.380 --> 00:44:06.320]   This is another one that cracks me up
[00:44:06.320 --> 00:44:10.440]   that we trust Oracle more than we trust TikTok.
[00:44:10.440 --> 00:44:11.280]   - Yeah.
[00:44:11.280 --> 00:44:12.120]   - I don't know.
[00:44:12.120 --> 00:44:12.960]   - No.
[00:44:12.960 --> 00:44:19.200]   - They understand that this is existential threat to them.
[00:44:19.200 --> 00:44:20.400]   They have a business in China,
[00:44:20.400 --> 00:44:23.560]   but if the US cuts off TikTok, it's probably,
[00:44:23.560 --> 00:44:25.440]   not good for TikTok in the long run.
[00:44:25.440 --> 00:44:27.080]   - What would the citizenry say?
[00:44:27.080 --> 00:44:31.320]   - American or Chinese?
[00:44:31.320 --> 00:44:32.760]   - American.
[00:44:32.760 --> 00:44:33.600]   - Voters.
[00:44:33.600 --> 00:44:34.440]   - What would voters say?
[00:44:34.440 --> 00:44:36.360]   - Yeah, and I think that's probably the calculus
[00:44:36.360 --> 00:44:39.120]   that's gonna be, Congress still has to vote on this.
[00:44:39.120 --> 00:44:42.560]   I imagine, in fact, if you care about this,
[00:44:42.560 --> 00:44:44.840]   you probably should write your Congress critter.
[00:44:44.840 --> 00:44:47.400]   I imagine they're putting their finger to the wind right now
[00:44:47.400 --> 00:44:49.200]   saying, well, what's gonna happen?
[00:44:49.200 --> 00:44:51.400]   - If young people would vote a little more,
[00:44:51.400 --> 00:44:53.360]   they might not be at risk.
[00:44:53.360 --> 00:44:56.560]   But this might be the type of thing
[00:44:56.560 --> 00:44:58.160]   that gets young people to vote.
[00:44:58.160 --> 00:45:00.200]   - And I'm sure that's what,
[00:45:00.200 --> 00:45:01.600]   wait, I'm losing my TikTok?
[00:45:01.600 --> 00:45:02.440]   - Yeah.
[00:45:02.440 --> 00:45:03.600]   - You better believe I'm voting on it.
[00:45:03.600 --> 00:45:05.400]   - I'm sure that's what said it unknown nothing,
[00:45:05.400 --> 00:45:09.280]   is investigating, is whether the people in my state--
[00:45:09.280 --> 00:45:12.480]   - Younger vote though, in the last couple years.
[00:45:12.480 --> 00:45:13.320]   - It's better.
[00:45:13.320 --> 00:45:14.160]   - It's an open up tick.
[00:45:14.160 --> 00:45:16.640]   - It is, and I think as you continue to eat away
[00:45:16.640 --> 00:45:18.920]   at things that they care about, Supreme Court,
[00:45:18.920 --> 00:45:21.840]   yesterday looked like in the oral arguments,
[00:45:21.840 --> 00:45:25.960]   they were gonna vote to overturn President Biden's
[00:45:25.960 --> 00:45:27.720]   student loan forgiveness.
[00:45:27.720 --> 00:45:30.840]   That's another one, you do that,
[00:45:30.840 --> 00:45:33.800]   and that gets young people into the,
[00:45:33.800 --> 00:45:36.000]   but maybe not that young, people in their 30s.
[00:45:36.000 --> 00:45:36.840]   - Hopefully it does.
[00:45:36.840 --> 00:45:38.680]   - Get them into the voting booth.
[00:45:38.680 --> 00:45:39.520]   - Yeah.
[00:45:39.520 --> 00:45:40.360]   - This is why you need to vote,
[00:45:40.360 --> 00:45:41.840]   it's things like this, and you're right.
[00:45:41.840 --> 00:45:44.880]   I'm sure that this is every member of Congress is going,
[00:45:44.880 --> 00:45:47.880]   is this gonna cost us a lot of votes under 25?
[00:45:47.880 --> 00:45:49.240]   - Yeah, probably.
[00:45:49.240 --> 00:45:51.040]   - Yeah, well the other thing is,
[00:45:51.040 --> 00:45:54.480]   is that media should be defending TikTok,
[00:45:54.480 --> 00:45:56.240]   just like the question of whether media defends--
[00:45:56.240 --> 00:45:57.080]   - Oh, but that's a hard thing to do.
[00:45:57.080 --> 00:45:58.520]   - I even feel a little weird.
[00:45:58.520 --> 00:45:59.840]   - Defending TikTok.
[00:45:59.840 --> 00:46:02.000]   - I know, but we should be defending the First Amendment
[00:46:02.000 --> 00:46:04.560]   and rights of speech on the internet and as podcasters.
[00:46:04.560 --> 00:46:08.520]   I think we especially, because what we are doing right now
[00:46:08.520 --> 00:46:10.840]   was enabled by TikTok.
[00:46:10.840 --> 00:46:11.680]   - TikTok.
[00:46:11.680 --> 00:46:12.680]   - Thank goodness TikTok.
[00:46:12.680 --> 00:46:14.160]   - Oh no, not TikTok.
[00:46:14.160 --> 00:46:15.000]   - Not TikTok.
[00:46:15.000 --> 00:46:16.000]   - Well your son is TikTok.
[00:46:16.000 --> 00:46:16.840]   - Yes.
[00:46:16.840 --> 00:46:17.680]   - We have.
[00:46:17.680 --> 00:46:18.520]   - We wanna hear more about the TikTok.
[00:46:18.520 --> 00:46:19.360]   I wanna hear more about the TikTok.
[00:46:19.360 --> 00:46:20.200]   - Uh huh.
[00:46:20.200 --> 00:46:21.040]   - We'll be showing that.
[00:46:21.040 --> 00:46:24.240]   - You got a big advance, six figure advance to write a--
[00:46:24.240 --> 00:46:25.080]   - Whoa!
[00:46:25.080 --> 00:46:25.920]   - Cook book.
[00:46:25.920 --> 00:46:26.760]   - Cook book.
[00:46:26.760 --> 00:46:29.040]   - No kidding, you're not as awesome.
[00:46:29.040 --> 00:46:33.120]   And so one of the things he's doing is he's going,
[00:46:33.120 --> 00:46:36.000]   I, Mike Elgin, I talked to Mike Elgin, Mike Elgin,
[00:46:36.000 --> 00:46:37.800]   as you know we went down to Oaxaca last year
[00:46:37.800 --> 00:46:40.640]   for the day of the dead, and the Elkins, Mike and Amirah,
[00:46:40.640 --> 00:46:42.400]   know every chef in Oaxaca,
[00:46:42.400 --> 00:46:46.160]   including one of the most famous chef Alejandro Ruiz.
[00:46:46.160 --> 00:46:50.080]   And Mike has arranged for Henry to go down for a month.
[00:46:50.080 --> 00:46:51.240]   (laughing)
[00:46:51.240 --> 00:46:52.080]   - Oh!
[00:46:52.080 --> 00:46:53.880]   - To share a prettis with Chef Alex.
[00:46:53.880 --> 00:46:54.720]   - Wow.
[00:46:54.720 --> 00:46:56.040]   - To share a mutual thing.
[00:46:56.040 --> 00:46:58.840]   'Cause he's gonna do TikToks of Chef Alex.
[00:46:58.840 --> 00:46:59.840]   - Right, heck yeah.
[00:46:59.840 --> 00:47:02.560]   - And that's gonna, and he says,
[00:47:02.560 --> 00:47:04.920]   I think I'm gonna do a Oaxaca chapter in the book,
[00:47:04.920 --> 00:47:05.800]   which is great by the way,
[00:47:05.800 --> 00:47:09.560]   'cause Oaxaca food is the amazing, wonderful cuisine
[00:47:09.560 --> 00:47:12.000]   that has yet to really enter the United States.
[00:47:12.000 --> 00:47:13.600]   And once it does, people are gonna raise things.
[00:47:13.600 --> 00:47:14.440]   - Oh this is gonna be great.
[00:47:14.440 --> 00:47:16.640]   - This is gonna sandwich a fry, isn't it?
[00:47:16.640 --> 00:47:17.480]   - Oh yeah.
[00:47:17.480 --> 00:47:18.640]   (laughing)
[00:47:18.640 --> 00:47:19.480]   - Oh yeah.
[00:47:19.480 --> 00:47:20.320]   - Oh yeah.
[00:47:20.320 --> 00:47:21.160]   - Dick the Oaxaca cuisine.
[00:47:21.160 --> 00:47:22.000]   - Oaxaca cuisine.
[00:47:22.000 --> 00:47:22.840]   - And turn it into a sandwich.
[00:47:22.840 --> 00:47:23.680]   - Into a sandwich.
[00:47:23.680 --> 00:47:24.760]   What kind of guys is it saying?
[00:47:24.760 --> 00:47:25.600]   - Yeah.
[00:47:25.600 --> 00:47:27.880]   - They got the taco thing.
[00:47:27.880 --> 00:47:30.200]   - Yeah, I'd say they would be a great sauce over.
[00:47:30.200 --> 00:47:31.680]   - No, that's his thing.
[00:47:31.680 --> 00:47:34.320]   By the way, he wasn't originally a sandwich guy.
[00:47:34.320 --> 00:47:36.680]   That was a thing that he learned from TikTok.
[00:47:36.680 --> 00:47:39.440]   The algorithm showed him when he made sandwiches,
[00:47:39.440 --> 00:47:40.680]   huge difference.
[00:47:40.680 --> 00:47:43.360]   So he made sandwiches, you know?
[00:47:43.360 --> 00:47:45.800]   - See, the Chinese are controlling them.
[00:47:45.800 --> 00:47:47.280]   They're controlling all of our stomachs.
[00:47:47.280 --> 00:47:50.720]   - We wouldn't be eating sandwiches if it weren't for China.
[00:47:50.720 --> 00:47:53.360]   - Yeah, they were admitted there weren't they?
[00:47:53.360 --> 00:47:55.280]   - Now, okay, so.
[00:47:55.280 --> 00:47:59.400]   - Are you gonna come back to your Senator thing?
[00:47:59.400 --> 00:48:00.240]   - Yeah.
[00:48:00.240 --> 00:48:01.760]   - Are you gonna be changing?
[00:48:01.760 --> 00:48:02.840]   - How did you know?
[00:48:02.840 --> 00:48:05.120]   It helps if you use the voice.
[00:48:05.120 --> 00:48:10.120]   - The Chinese government is not known for freedom.
[00:48:10.120 --> 00:48:11.760]   They're known for repression.
[00:48:11.760 --> 00:48:14.080]   They're known for throwing their citizens
[00:48:14.080 --> 00:48:15.800]   into concentration camps.
[00:48:15.800 --> 00:48:16.640]   - Yep.
[00:48:16.640 --> 00:48:21.640]   - Should we not try to keep that kind of,
[00:48:21.640 --> 00:48:25.520]   ill feeling out of the United States of America?
[00:48:25.520 --> 00:48:28.760]   (laughing)
[00:48:28.760 --> 00:48:29.760]   - I love your filibuster.
[00:48:29.760 --> 00:48:31.320]   (laughing)
[00:48:31.320 --> 00:48:32.160]   - It's quick.
[00:48:32.160 --> 00:48:35.920]   - I mean, don't they deserve sanctions?
[00:48:35.920 --> 00:48:39.000]   Just like we sanctioned the Russkies,
[00:48:39.000 --> 00:48:41.360]   I'm sorry, the Russians against it
[00:48:41.360 --> 00:48:44.160]   because of violations of human rights.
[00:48:44.160 --> 00:48:47.240]   I don't think we should just turn a blind eye
[00:48:47.240 --> 00:48:50.920]   out of the Chinese just because they make our phones.
[00:48:50.920 --> 00:48:52.760]   That's not enough.
[00:48:52.760 --> 00:48:53.600]   We want more.
[00:48:53.600 --> 00:48:57.320]   - We don't get fist bumps to the Saudis.
[00:48:57.320 --> 00:49:01.080]   - Well, I don't like that either, but that's another matter.
[00:49:01.080 --> 00:49:01.920]   - Oh man.
[00:49:01.920 --> 00:49:03.520]   - Jared can keep his billion dollars.
[00:49:03.520 --> 00:49:06.760]   I think we should keep the Saudis out of this.
[00:49:06.760 --> 00:49:08.000]   I'm a, you know, it's an interesting
[00:49:08.000 --> 00:49:09.640]   because I'm a big F1, Ant and I,
[00:49:09.640 --> 00:49:12.160]   both big F1 fans, Formula One racing.
[00:49:12.160 --> 00:49:15.120]   And this is a problem because the Formula One
[00:49:15.120 --> 00:49:17.920]   has told people like Louis Hamilton,
[00:49:17.920 --> 00:49:21.640]   you can't wear that rainbow flag helmet
[00:49:21.640 --> 00:49:23.760]   where next time we go to Saudi Arabia,
[00:49:23.760 --> 00:49:27.200]   they're telling them, no, we gotta,
[00:49:27.200 --> 00:49:29.680]   you gotta run all that stuff by us first.
[00:49:29.680 --> 00:49:32.560]   And a lot of the races are in,
[00:49:32.560 --> 00:49:33.920]   well, Bahrain's the first race.
[00:49:33.920 --> 00:49:35.440]   - Bahrain is the first one.
[00:49:35.440 --> 00:49:38.040]   - And then there's Saudi Arabia is the second one.
[00:49:38.040 --> 00:49:40.320]   And so this is a global sport.
[00:49:40.320 --> 00:49:42.320]   I love, I know you love it, Ant,
[00:49:42.320 --> 00:49:43.160]   and we're very excited about it.
[00:49:43.160 --> 00:49:45.680]   - Similar actions happened there in the World Cup,
[00:49:45.680 --> 00:49:46.680]   at Qatar.
[00:49:46.680 --> 00:49:47.600]   - Yep.
[00:49:47.600 --> 00:49:49.440]   They either act, F1's really bad.
[00:49:49.440 --> 00:49:51.640]   They won't even let Louis wear his earring.
[00:49:51.640 --> 00:49:56.240]   - You can't wear earrings, dude.
[00:49:56.240 --> 00:49:57.080]   What the hell?
[00:49:57.080 --> 00:49:57.920]   - It was a safety concern.
[00:49:57.920 --> 00:49:59.440]   - This is 1952.
[00:49:59.440 --> 00:50:01.280]   - What are we talking here?
[00:50:01.280 --> 00:50:02.120]   - Yeah.
[00:50:02.120 --> 00:50:02.960]   - Anyway.
[00:50:02.960 --> 00:50:05.920]   I think you can make a,
[00:50:05.920 --> 00:50:07.960]   - Wait a second, are there women drivers in the F1?
[00:50:07.960 --> 00:50:08.800]   - No, sorry.
[00:50:08.800 --> 00:50:09.640]   - Not yet.
[00:50:09.640 --> 00:50:11.360]   - There's one black driver, that's it.
[00:50:11.360 --> 00:50:13.800]   No women.
[00:50:13.800 --> 00:50:17.720]   - Female, you left Twitter and Facebook
[00:50:17.720 --> 00:50:18.840]   because of the principle.
[00:50:18.840 --> 00:50:20.480]   - Well, I know, I didn't watch,
[00:50:20.480 --> 00:50:23.320]   and I did not watch the World Cup
[00:50:23.320 --> 00:50:25.800]   because I was upset that it was in Qatar.
[00:50:25.800 --> 00:50:27.960]   By the way, there's got me in F1 racing, Qatar.
[00:50:27.960 --> 00:50:30.860]   It's hard.
[00:50:30.860 --> 00:50:32.680]   It's hard.
[00:50:32.680 --> 00:50:37.360]   You know, this is the same sort of thing with his TikTok.
[00:50:37.360 --> 00:50:39.960]   - I don't, you know, I'm not in favor of
[00:50:39.960 --> 00:50:42.360]   how Chinese government treats its people.
[00:50:42.360 --> 00:50:45.080]   I think it's reprehensible.
[00:50:45.080 --> 00:50:46.080]   But I love Chinese people.
[00:50:46.080 --> 00:50:47.960]   - But in Chinese people are good there.
[00:50:47.960 --> 00:50:52.160]   - I have to look and see if you mix any Chinese sandwiches.
[00:50:52.160 --> 00:50:53.000]   You might be working.
[00:50:53.000 --> 00:50:54.640]   - How is your Chinese these days?
[00:50:54.640 --> 00:50:55.720]   - It's very bad.
[00:50:55.720 --> 00:50:57.880]   I lost it all, sad to say.
[00:50:57.880 --> 00:50:58.880]   But I love the country.
[00:50:58.880 --> 00:50:59.760]   I love the people.
[00:50:59.760 --> 00:51:00.680]   I love its history.
[00:51:00.680 --> 00:51:02.240]   I'm fascinated by it.
[00:51:02.240 --> 00:51:05.240]   I think it is, I think it should be,
[00:51:06.280 --> 00:51:11.280]   by all rights, one of the most successful countries
[00:51:11.280 --> 00:51:13.160]   in the world because it's got the resources,
[00:51:13.160 --> 00:51:15.560]   it's got the brains, it's got the people.
[00:51:15.560 --> 00:51:18.280]   And it's, unfortunately, I think the CCP,
[00:51:18.280 --> 00:51:21.800]   which was, you know, I have to say in 1945,
[00:51:21.800 --> 00:51:26.760]   Mao Zedong did lift the Chinese people
[00:51:26.760 --> 00:51:31.200]   out of a feudal system into the modern world.
[00:51:31.200 --> 00:51:32.960]   But he made a lot of mistakes subsequently.
[00:51:32.960 --> 00:51:34.480]   It caused a great famine.
[00:51:35.720 --> 00:51:37.600]   The current regime is not just.
[00:51:37.600 --> 00:51:38.440]   - It's repressed.
[00:51:38.440 --> 00:51:40.280]   - You know, I know you're not a southern or so.
[00:51:40.280 --> 00:51:41.920]   - How do you know that?
[00:51:41.920 --> 00:51:44.240]   - 'Cause you didn't say Mao Zedong.
[00:51:44.240 --> 00:51:45.680]   - Mao Zedong. - That's how I heard it.
[00:51:45.680 --> 00:51:48.640]   - That's how I heard it going on all my life.
[00:51:48.640 --> 00:51:50.080]   - Mao Zedong. - Chairman Mao.
[00:51:50.080 --> 00:51:54.920]   But Mao in the early days did a lot
[00:51:54.920 --> 00:51:55.960]   for the Chinese people.
[00:51:55.960 --> 00:51:58.760]   And to this day, when we were in China in 2009,
[00:51:58.760 --> 00:52:00.720]   my son, Henry and I went,
[00:52:00.720 --> 00:52:02.360]   and we went to a small village
[00:52:02.360 --> 00:52:04.640]   and they all had big screen, flat screen TVs.
[00:52:05.560 --> 00:52:07.960]   And we were told by our guide,
[00:52:07.960 --> 00:52:10.920]   essentially that's what happens every year.
[00:52:10.920 --> 00:52:11.760]   It's a new thing.
[00:52:11.760 --> 00:52:15.160]   It's air conditioning one year, big screen TVs the next year.
[00:52:15.160 --> 00:52:17.080]   They're slowly raising the standard of living.
[00:52:17.080 --> 00:52:19.960]   And this is a very poor Chinese village, you know,
[00:52:19.960 --> 00:52:22.200]   basically subsistence agriculture.
[00:52:22.200 --> 00:52:25.400]   But they have been lifted into the modern world.
[00:52:25.400 --> 00:52:27.480]   And so that's one of the reasons I think the Chinese people
[00:52:27.480 --> 00:52:29.440]   are very supportive of their regime
[00:52:29.440 --> 00:52:31.840]   because they get a lot of benefits from it.
[00:52:31.840 --> 00:52:35.560]   And it really was a vicious feudal society.
[00:52:35.560 --> 00:52:37.080]   - Right, but the other thing is people say,
[00:52:37.080 --> 00:52:39.360]   well, the thing I hate most is I hate people say,
[00:52:39.360 --> 00:52:40.960]   "Well, China's not ready for democracy."
[00:52:40.960 --> 00:52:42.240]   Or listen to what the Chinese people say.
[00:52:42.240 --> 00:52:43.080]   - Yeah, I don't think so.
[00:52:43.080 --> 00:52:45.720]   - They don't have, and it's wrong to say any people
[00:52:45.720 --> 00:52:50.160]   are not ready for their own ruling themselves.
[00:52:50.160 --> 00:52:53.600]   And be opinion polls.
[00:52:53.600 --> 00:52:58.080]   An element does a trust index every year.
[00:52:58.080 --> 00:53:03.080]   And the repressive regimes, Russia, Philippines, China
[00:53:03.080 --> 00:53:05.000]   are all high on trust of institutions.
[00:53:05.000 --> 00:53:06.600]   'Cause what else are you gonna say?
[00:53:06.600 --> 00:53:07.760]   - Right.
[00:53:07.760 --> 00:53:10.040]   - Some stranger calls you, "Oh, yes, yes, I love them.
[00:53:10.040 --> 00:53:10.880]   I love them.
[00:53:10.880 --> 00:53:12.400]   They're out for us, absolutely."
[00:53:12.400 --> 00:53:15.440]   And you can't separate that out for easily.
[00:53:15.440 --> 00:53:17.120]   - And as somebody's pointing out,
[00:53:17.120 --> 00:53:18.680]   a Mao probably is the highest death count
[00:53:18.680 --> 00:53:22.360]   of any dictator he killed.
[00:53:22.360 --> 00:53:25.280]   We don't even know how many hundreds of millions of people
[00:53:25.280 --> 00:53:26.760]   in the Great Leap Forward.
[00:53:26.760 --> 00:53:29.680]   And the Cultural Revolution in the '60s was horrible.
[00:53:29.680 --> 00:53:32.140]   So he committed a lot of crimes.
[00:53:32.140 --> 00:53:37.360]   You know, life is, stuff is complicated.
[00:53:37.360 --> 00:53:39.160]   And to say, "Yeah, we're gonna ban TikTok.
[00:53:39.160 --> 00:53:41.080]   That's gonna solve everything.
[00:53:41.080 --> 00:53:43.760]   It's not gonna solve anything at all."
[00:53:43.760 --> 00:53:46.440]   - Well, that's where media and politics come together
[00:53:46.440 --> 00:53:50.600]   and oversimplify the world and lose history and lose context.
[00:53:50.600 --> 00:53:52.400]   And again, we don't deal with the real issues.
[00:53:52.400 --> 00:53:55.280]   - We're not good with ambiguity and gray areas.
[00:53:55.280 --> 00:53:56.360]   - Right.
[00:53:56.360 --> 00:53:58.320]   - We want good and bad.
[00:53:58.320 --> 00:54:00.920]   We want to know who's the good guys who are the bad guys.
[00:54:00.920 --> 00:54:01.760]   We're the good guys.
[00:54:01.760 --> 00:54:02.720]   - Tell me who to hate.
[00:54:02.720 --> 00:54:04.360]   - Buy me a new to hate.
[00:54:04.360 --> 00:54:06.760]   - I hate Senator Devil.
[00:54:06.760 --> 00:54:08.920]   - Well, a friend, my colleague.
[00:54:08.920 --> 00:54:10.400]   - You brought him back again.
[00:54:10.400 --> 00:54:12.600]   - On the outside of the aisle.
[00:54:12.600 --> 00:54:14.720]   - But take his head out of his kista
[00:54:14.720 --> 00:54:18.560]   and start thinking about the real world we live in.
[00:54:18.560 --> 00:54:19.720]   - All right.
[00:54:19.720 --> 00:54:21.640]   Senator Devil's gonna retire now.
[00:54:21.640 --> 00:54:24.320]   - Good.
[00:54:25.400 --> 00:54:26.240]   - Now that he's officially-
[00:54:26.240 --> 00:54:27.080]   - Not officially-
[00:54:27.080 --> 00:54:30.840]   - And Senator Devil both go into the-
[00:54:30.840 --> 00:54:33.600]   - All right.
[00:54:33.600 --> 00:54:38.200]   Let us take a little break and there is more to talk about
[00:54:38.200 --> 00:54:40.120]   in just a little bit,
[00:54:40.120 --> 00:54:41.840]   but we do want to mention our sponsor
[00:54:41.840 --> 00:54:44.600]   'cause I am a fan, a big fan.
[00:54:44.600 --> 00:54:47.440]   Senator Devil probably is to everybody loves.
[00:54:47.440 --> 00:54:49.080]   Fast mail.
[00:54:49.080 --> 00:54:51.040]   I've said this for some time now.
[00:54:51.040 --> 00:54:52.440]   If you care about email,
[00:54:52.440 --> 00:54:54.680]   why are you using free email?
[00:54:54.680 --> 00:54:57.320]   Email that treats you as a product,
[00:54:57.320 --> 00:54:58.240]   not as the customer.
[00:54:58.240 --> 00:55:02.080]   Fast mail isn't free.
[00:55:02.080 --> 00:55:04.520]   It's three bucks, as little as three bucks a month.
[00:55:04.520 --> 00:55:06.600]   It's not hugely expensive,
[00:55:06.600 --> 00:55:08.320]   but free email isn't free either.
[00:55:08.320 --> 00:55:10.640]   You pay with your privacy.
[00:55:10.640 --> 00:55:11.480]   For over 20 years,
[00:55:11.480 --> 00:55:14.560]   fast mail's been a leader in email privacy
[00:55:14.560 --> 00:55:15.680]   because with fast mail,
[00:55:15.680 --> 00:55:19.440]   you're a customer, not the product.
[00:55:19.440 --> 00:55:20.360]   Not only is it private,
[00:55:20.360 --> 00:55:23.440]   you get great support because they care about you.
[00:55:23.440 --> 00:55:24.800]   They want to keep you happy.
[00:55:24.800 --> 00:55:26.440]   You're their customer.
[00:55:26.440 --> 00:55:27.920]   Your personal data is safe,
[00:55:27.920 --> 00:55:29.800]   kept away from third parties.
[00:55:29.800 --> 00:55:32.040]   They have the best spam filtering ever.
[00:55:32.040 --> 00:55:35.680]   I absolutely love fast mail spam filters.
[00:55:35.680 --> 00:55:38.600]   I have set up a very, I think,
[00:55:38.600 --> 00:55:42.240]   good system for making email usable again.
[00:55:42.240 --> 00:55:45.720]   No ads, no tracking pixels.
[00:55:45.720 --> 00:55:47.960]   Your data is stored in the US with fast mail
[00:55:47.960 --> 00:55:50.320]   and it's fully GDPR compliant.
[00:55:50.320 --> 00:55:52.200]   Now, let me talk a little bit about some of the features.
[00:55:52.200 --> 00:55:53.360]   You're going to love about fast mail.
[00:55:53.360 --> 00:55:55.600]   First of all, fast mail is real email.
[00:55:55.600 --> 00:55:59.240]   It is not some company's idea of email.
[00:55:59.240 --> 00:56:02.200]   They use the open source Cyrus IMAP server,
[00:56:02.200 --> 00:56:04.920]   which is the king of IMAP servers
[00:56:04.920 --> 00:56:08.440]   configured exactly the best possible way.
[00:56:08.440 --> 00:56:09.160]   In fact,
[00:56:09.160 --> 00:56:11.160]   fast mail contributes back to the open source
[00:56:11.160 --> 00:56:13.280]   and the internet community.
[00:56:13.280 --> 00:56:15.200]   They've helped create email standards
[00:56:15.200 --> 00:56:17.760]   for authentication to reduce spam.
[00:56:17.760 --> 00:56:21.080]   They're absolutely a good citizen,
[00:56:21.080 --> 00:56:22.760]   a good neighbor in the email world.
[00:56:22.760 --> 00:56:26.640]   So you're supporting somebody's doing all the right things.
[00:56:26.640 --> 00:56:27.840]   Because it's true IMAP,
[00:56:27.840 --> 00:56:31.240]   you can, of course, use any client you want.
[00:56:31.240 --> 00:56:35.320]   Doesn't, you know, thunderbird, outlook,
[00:56:35.320 --> 00:56:37.840]   Microsoft mail, Apple mail,
[00:56:37.840 --> 00:56:40.160]   but you can also use their great web mail.
[00:56:40.160 --> 00:56:41.840]   You won't be giving up that great web mail.
[00:56:41.840 --> 00:56:43.440]   In fact, I think it's 10 times better
[00:56:43.440 --> 00:56:44.920]   than Gmail's interface.
[00:56:44.920 --> 00:56:47.360]   You can customize your workflow with colors,
[00:56:47.360 --> 00:56:50.840]   custom swipes, you got night mode.
[00:56:50.840 --> 00:56:52.600]   They've got apps for iOS and Android
[00:56:52.600 --> 00:56:54.560]   that are superb.
[00:56:54.560 --> 00:56:57.280]   You can customize your inbox with scheduled send.
[00:56:57.280 --> 00:56:58.360]   I love that feature.
[00:56:58.360 --> 00:57:01.040]   Snooze, folders, labels.
[00:57:01.040 --> 00:57:02.840]   Yeah, folders and labels.
[00:57:02.840 --> 00:57:04.720]   So that's kind of cool too.
[00:57:04.720 --> 00:57:07.480]   Search bar, the search is fantastic.
[00:57:07.480 --> 00:57:10.200]   I keep all my email,
[00:57:10.200 --> 00:57:12.400]   never delete anything because fast mail
[00:57:12.400 --> 00:57:13.240]   has such a good search.
[00:57:13.240 --> 00:57:14.160]   I know everything's there
[00:57:14.160 --> 00:57:15.480]   and I can find it in a minute.
[00:57:15.480 --> 00:57:16.640]   In a second,
[00:57:16.640 --> 00:57:18.360]   keep track of all the important details
[00:57:18.360 --> 00:57:20.880]   in your life with fast mail's powerful sidebar.
[00:57:20.880 --> 00:57:24.160]   I've moved my calendar, my address book to fast mail.
[00:57:24.160 --> 00:57:26.280]   I don't use Google anymore for those.
[00:57:26.280 --> 00:57:29.280]   Fast mail supports CalDAV and CardDAV,
[00:57:29.280 --> 00:57:31.120]   the internet standards for that.
[00:57:31.120 --> 00:57:32.360]   The calendar works great.
[00:57:32.360 --> 00:57:34.640]   I even have my notes stored in fast mail.
[00:57:34.640 --> 00:57:39.240]   And I use fast mail as my DNS host
[00:57:39.240 --> 00:57:43.920]   for my websites because then I get email too
[00:57:43.920 --> 00:57:46.360]   and fast mail just makes it very easy
[00:57:46.360 --> 00:57:49.400]   to turn off all the email authentication protocols,
[00:57:49.400 --> 00:57:52.240]   SPF and DKIM and DMARC.
[00:57:52.240 --> 00:57:54.600]   That's why my emails get through better.
[00:57:54.600 --> 00:57:56.080]   They're authenticated.
[00:57:56.080 --> 00:57:57.960]   It's just, and I can have an infinite number
[00:57:57.960 --> 00:57:59.720]   of email addresses by the way.
[00:57:59.720 --> 00:58:00.600]   As far as I can tell,
[00:58:00.600 --> 00:58:02.480]   there's no limit on the number of domains.
[00:58:02.480 --> 00:58:04.160]   I can have hosted a fast mail.
[00:58:04.160 --> 00:58:05.000]   That's pretty cool.
[00:58:05.000 --> 00:58:06.480]   You wouldn't put a website there,
[00:58:06.480 --> 00:58:08.160]   although I do have a single,
[00:58:08.160 --> 00:58:11.200]   like a about me page on fast mail.
[00:58:11.200 --> 00:58:13.120]   So you could put a web page there.
[00:58:13.120 --> 00:58:16.200]   But mostly I use it for DNS because I get the email.
[00:58:16.200 --> 00:58:18.640]   So when I won't sign up for something to an account,
[00:58:18.640 --> 00:58:20.520]   every time I sign up, I have a unique email.
[00:58:20.520 --> 00:58:22.040]   They even support, which is really cool,
[00:58:22.040 --> 00:58:25.760]   one password and Bitwarden's masked email standards.
[00:58:25.760 --> 00:58:26.600]   You've seen this.
[00:58:26.600 --> 00:58:29.000]   This is something new that really adds to your security.
[00:58:29.000 --> 00:58:31.400]   Of course, you have a unique password for every website.
[00:58:31.400 --> 00:58:33.920]   Now if you're using Bitwarden or one password,
[00:58:33.920 --> 00:58:36.680]   fast mail will give you a unique email address
[00:58:36.680 --> 00:58:38.000]   for every website.
[00:58:38.000 --> 00:58:40.120]   They all go to the same inbox.
[00:58:40.120 --> 00:58:43.040]   But this way, you've got kind of double the protection.
[00:58:43.040 --> 00:58:45.160]   A hacker has to guess the unique email
[00:58:45.160 --> 00:58:47.320]   as well as the password.
[00:58:47.320 --> 00:58:49.080]   That really is cool.
[00:58:49.080 --> 00:58:53.560]   Desktop or mobile, fast mail is the best.
[00:58:53.560 --> 00:58:55.440]   And their support is the best.
[00:58:55.440 --> 00:58:58.680]   Not some notebook reader, but real US-based support team
[00:58:58.680 --> 00:59:00.840]   with email experts answering the phone,
[00:59:00.840 --> 00:59:02.680]   answering your questions.
[00:59:02.680 --> 00:59:04.440]   So then they're always in reach.
[00:59:04.440 --> 00:59:07.560]   And because again, you're a customer,
[00:59:07.560 --> 00:59:09.720]   they give you that kind of support.
[00:59:09.720 --> 00:59:12.080]   The fast mail team believes working for customers
[00:59:12.080 --> 00:59:14.160]   is people to be cared for,
[00:59:14.160 --> 00:59:17.080]   not products to be exploited,
[00:59:17.080 --> 00:59:19.040]   advertisers are left out,
[00:59:19.040 --> 00:59:21.640]   you and your privacy are at the center.
[00:59:21.640 --> 00:59:24.040]   I could read you a review after review of Fastmail,
[00:59:24.040 --> 00:59:25.640]   but you gotta listen to me when I tell you,
[00:59:25.640 --> 00:59:28.480]   I've been using Fastmail for more than 10 years.
[00:59:28.480 --> 00:59:30.440]   I am absolutely hooked.
[00:59:30.440 --> 00:59:31.320]   I will go nowhere else.
[00:59:31.320 --> 00:59:34.240]   In fact, I just signed up again for another three years.
[00:59:34.240 --> 00:59:36.160]   I do it three years at a time.
[00:59:36.160 --> 00:59:38.480]   It's just the best.
[00:59:38.480 --> 00:59:41.560]   It's very easy to get your data out of any other email client
[00:59:41.560 --> 00:59:42.520]   and into Fastmail.
[00:59:42.520 --> 00:59:44.320]   Plus, Fastmail will go out and fetch that email
[00:59:44.320 --> 00:59:46.120]   if you want to keep those accounts alive.
[00:59:46.120 --> 00:59:48.320]   I would do that if you move email addresses.
[00:59:48.320 --> 00:59:50.040]   And as I said, I have customer domain,
[00:59:50.040 --> 00:59:51.760]   so I have a custom email address with Fastmail,
[00:59:51.760 --> 00:59:53.440]   doesn't cost anymore.
[00:59:53.440 --> 00:59:54.680]   It's just great.
[00:59:54.680 --> 00:59:57.180]   They power open source email.
[00:59:57.180 --> 01:00:02.200]   They are moving email forward with new internet standards.
[01:00:02.200 --> 01:00:03.840]   It's a good company with great people
[01:00:03.840 --> 01:00:05.120]   who really care about email.
[01:00:05.120 --> 01:00:06.360]   If you care about email,
[01:00:06.360 --> 01:00:09.040]   get Fastmail.
[01:00:09.040 --> 01:00:10.920]   Reclaim your privacy, boost productivity,
[01:00:10.920 --> 01:00:13.840]   make email yours with Fastmail.
[01:00:13.840 --> 01:00:15.360]   Try it free for 30 days.
[01:00:15.360 --> 01:00:18.080]   Fastmail.com/twit.
[01:00:18.080 --> 01:00:20.800]   F-A-S-T-M-A-I-L.
[01:00:20.800 --> 01:00:24.280]   Fastmail.com/twit.
[01:00:24.280 --> 01:00:25.800]   And by the way, if you use that address,
[01:00:25.800 --> 01:00:30.000]   fastmail.com/twit, you'll get 15% off on your first year,
[01:00:30.000 --> 01:00:32.440]   your whole first year when you sign up today.
[01:00:32.440 --> 01:00:34.600]   Fastmail.com/twit.
[01:00:34.600 --> 01:00:36.960]   I've been telling people this
[01:00:36.960 --> 01:00:38.640]   since long before they were an advertiser.
[01:00:38.640 --> 01:00:40.920]   I don't know why everybody doesn't just use Fastmail.
[01:00:40.920 --> 01:00:44.620]   It is literally the best email service in the market.
[01:00:44.620 --> 01:00:46.680]   Do it.
[01:00:46.680 --> 01:00:47.520]   Do it.
[01:00:47.520 --> 01:00:48.360]   Fastmail.
[01:00:48.360 --> 01:00:50.080]   Thank you, Fastmail, for supporting us.
[01:00:50.080 --> 01:00:54.200]   All right.
[01:00:54.200 --> 01:00:55.600]   All right.
[01:00:55.600 --> 01:00:56.440]   Enough of this.
[01:00:56.440 --> 01:00:57.280]   - All right, Senator.
[01:00:57.280 --> 01:00:58.120]   - Senator.
[01:00:58.120 --> 01:00:58.940]   - Send it up.
[01:00:58.940 --> 01:00:59.760]   - Be that this is not a democracy and you're in charge.
[01:00:59.760 --> 01:01:01.000]   We've just lost democracy.
[01:01:01.000 --> 01:01:01.840]   What's next?
[01:01:01.840 --> 01:01:02.920]   - Oh.
[01:01:02.920 --> 01:01:05.840]   - Actually, YouTube has a new lead out.
[01:01:05.840 --> 01:01:08.600]   We talked about Susan Wojcicki moving along.
[01:01:08.600 --> 01:01:11.760]   The new guy in charge, Neil Mowen,
[01:01:11.760 --> 01:01:15.720]   wrote his first letter and of course he's smart.
[01:01:15.720 --> 01:01:17.480]   Our 2023 priorities,
[01:01:17.480 --> 01:01:20.360]   it's aimed particularly at creators, right?
[01:01:20.360 --> 01:01:23.240]   Who's most important to YouTube after advertisers?
[01:01:23.240 --> 01:01:25.480]   'Cause they're the most important creators.
[01:01:25.480 --> 01:01:27.000]   Somebody's got to make that content.
[01:01:27.000 --> 01:01:27.840]   - Yeah.
[01:01:27.840 --> 01:01:31.920]   - He says, "A little over 15 years ago,
[01:01:31.920 --> 01:01:33.720]   "I visited a company with an interesting take
[01:01:33.720 --> 01:01:35.480]   "on digital video.
[01:01:35.480 --> 01:01:36.560]   "As I walked through YouTube,
[01:01:36.560 --> 01:01:38.680]   "small offices above a pizza parlor,
[01:01:38.680 --> 01:01:41.000]   "I could see the promise of the platform.
[01:01:41.000 --> 01:01:43.440]   "I've thought about that moment over the past few weeks
[01:01:43.440 --> 01:01:45.720]   "as my longtime friend and mentor, Susan Wojcicki,
[01:01:45.720 --> 01:01:48.520]   "transitioned to become an advisor to Google and Alphabet
[01:01:48.520 --> 01:01:52.960]   "and I took the helm as the new leader of YouTube."
[01:01:52.960 --> 01:01:54.080]   So what does he say?
[01:01:54.080 --> 01:01:58.480]   He quoted--
[01:01:58.480 --> 01:02:00.440]   - We need to do everything we can to make
[01:02:00.440 --> 01:02:02.040]   Marques Brownlee happy.
[01:02:02.040 --> 01:02:03.560]   - Yeah, well that's number one.
[01:02:03.560 --> 01:02:04.560]   (laughing)
[01:02:04.560 --> 01:02:06.200]   Definitely number one.
[01:02:06.200 --> 01:02:07.520]   - And Mr. B's happy.
[01:02:07.520 --> 01:02:08.680]   - He and Mr. B's,
[01:02:08.680 --> 01:02:09.520]   - Yes.
[01:02:09.520 --> 01:02:10.360]   - He and Mr. B's happy.
[01:02:10.360 --> 01:02:11.360]   - Probably more importantly Mr. B's.
[01:02:11.360 --> 01:02:15.600]   - He did say, he quoted a study from Oxford Economics
[01:02:15.600 --> 01:02:18.800]   that in 2021 more than two million creators,
[01:02:18.800 --> 01:02:21.280]   two million earned the money equivalent
[01:02:21.280 --> 01:02:24.960]   of a full-time job on YouTube.
[01:02:24.960 --> 01:02:25.880]   - That's awesome.
[01:02:25.880 --> 01:02:28.680]   - So you talk about TikTok and it's for sure true,
[01:02:28.680 --> 01:02:30.600]   the TikTok has powered a lot of people,
[01:02:30.600 --> 01:02:33.800]   but nobody comes close to what YouTube's done, right?
[01:02:35.720 --> 01:02:37.760]   YouTube has a shopping feature now,
[01:02:37.760 --> 01:02:40.880]   ad revenue sharing on shorts.
[01:02:40.880 --> 01:02:43.240]   The number of people subscribing to individual channels
[01:02:43.240 --> 01:02:46.600]   has jumped 20% year over year,
[01:02:46.600 --> 01:02:49.640]   six million people now subscribed to channels.
[01:02:49.640 --> 01:02:51.280]   It's not a huge number given the number of people
[01:02:51.280 --> 01:02:52.240]   who use YouTube.
[01:02:52.240 --> 01:02:54.840]   I think that's also 'cause creators keep saying,
[01:02:54.840 --> 01:02:58.640]   "Hit the smash the subscribe button during the--"
[01:02:58.640 --> 01:02:59.800]   - Never gets old.
[01:02:59.800 --> 01:03:01.960]   - They have added under his, you know,
[01:03:01.960 --> 01:03:05.000]   this is probably something Susan Wojcicki had in process,
[01:03:05.000 --> 01:03:06.760]   but they just added dubbing clips
[01:03:06.760 --> 01:03:08.880]   into alternate languages, right?
[01:03:08.880 --> 01:03:10.760]   How many languages do they have now?
[01:03:10.760 --> 01:03:12.400]   - It's kind of cool.
[01:03:12.400 --> 01:03:13.240]   - Wow.
[01:03:13.240 --> 01:03:16.320]   - Of course, Mr. Beast started it.
[01:03:16.320 --> 01:03:17.160]   (laughing)
[01:03:17.160 --> 01:03:18.080]   - Like in you.
[01:03:18.080 --> 01:03:21.640]   - It was tested by Mr. Beast first.
[01:03:21.640 --> 01:03:24.840]   - So this isn't captioned, this is dubbing.
[01:03:24.840 --> 01:03:25.680]   - Dubbing.
[01:03:25.680 --> 01:03:27.440]   Wow.
[01:03:27.440 --> 01:03:30.360]   - Multiple language audio tracks, they built it--
[01:03:30.360 --> 01:03:33.440]   - Oh, can we do that from a clip from the show?
[01:03:33.440 --> 01:03:36.240]   - Sure, I wanna hear Twig in German.
[01:03:36.240 --> 01:03:41.080]   - Okay, we just gotta get a cast of people together.
[01:03:41.080 --> 01:03:43.240]   - What about--
[01:03:43.240 --> 01:03:44.720]   - Can we just put this on YouTube?
[01:03:44.720 --> 01:03:45.560]   Can we dub it there?
[01:03:45.560 --> 01:03:46.680]   - Yeah.
[01:03:46.680 --> 01:03:48.480]   - Yeah, but again, somebody has to speak German.
[01:03:48.480 --> 01:03:49.880]   You're gonna do it?
[01:03:49.880 --> 01:03:52.240]   - No, I thought, oh, I thought it was a computer dump.
[01:03:52.240 --> 01:03:53.680]   - No, no, no, no, no, no.
[01:03:53.680 --> 01:03:56.520]   - That's what I thought too, Jeff, at first.
[01:03:56.520 --> 01:03:57.360]   - Oh, okay.
[01:03:57.360 --> 01:04:00.360]   - You have to imagine that's coming at some point.
[01:04:00.360 --> 01:04:01.640]   - Yeah, actually, why not?
[01:04:01.640 --> 01:04:02.480]   You know what?
[01:04:02.480 --> 01:04:03.320]   - That's not an easy one.
[01:04:03.320 --> 01:04:04.320]   - This weekend, AI.
[01:04:04.320 --> 01:04:06.480]   - Come to think of it, the AI could do it.
[01:04:06.480 --> 01:04:08.880]   I mean, they can translate pretty well already.
[01:04:08.880 --> 01:04:10.400]   They're doing the transcriptions.
[01:04:10.400 --> 01:04:11.960]   They're doing the trans-is translations.
[01:04:11.960 --> 01:04:14.040]   I guess all you'd have to do is have the voice.
[01:04:14.040 --> 01:04:14.880]   - You just have the voice.
[01:04:14.880 --> 01:04:16.320]   - Yeah, I like the image.
[01:04:16.320 --> 01:04:18.240]   - The creator chooses which language.
[01:04:18.240 --> 01:04:20.960]   So there's now a, and you could see this
[01:04:20.960 --> 01:04:23.040]   on the Mr. Beast video.
[01:04:23.040 --> 01:04:25.680]   Hindi, Thai, Vietnamese, Arabic, he dubs it
[01:04:25.680 --> 01:04:27.600]   into much, much different languages.
[01:04:27.600 --> 01:04:32.280]   Over 15% of the dub videos, watch time came from viewers
[01:04:32.280 --> 01:04:34.240]   in a different language than the original recording.
[01:04:34.240 --> 01:04:35.280]   So that's big.
[01:04:35.280 --> 01:04:38.600]   Viewers watched over two million hours
[01:04:38.600 --> 01:04:40.880]   in January alone of dubbed videos.
[01:04:40.880 --> 01:04:42.640]   - So does Mr. Beast pay someone to do it
[01:04:42.640 --> 01:04:44.600]   or volunteers to it?
[01:04:44.600 --> 01:04:46.640]   - Well-known creator, Mr. Beast,
[01:04:46.640 --> 01:04:51.280]   aka Jimmy Donaldson, who has 130 million global subscribers,
[01:04:51.280 --> 01:04:55.960]   dubbed his 11 most popular videos in 11 languages.
[01:04:55.960 --> 01:04:59.160]   In an interview with YouTube's creator insider,
[01:04:59.160 --> 01:05:00.160]   is that Renee?
[01:05:00.160 --> 01:05:01.000]   I don't know.
[01:05:01.920 --> 01:05:05.560]   Mr. Beast explained, that should be the name of my senator.
[01:05:05.560 --> 01:05:07.640]   - Senator Beast!
[01:05:07.640 --> 01:05:08.480]   - Explained?
[01:05:08.480 --> 01:05:11.000]   Why the feature was beneficial?
[01:05:11.000 --> 01:05:13.320]   Well, I don't even have to explain that.
[01:05:13.320 --> 01:05:16.080]   It's much easier to just run one channel than 12.
[01:05:16.080 --> 01:05:19.120]   You have to make 12 different thumbnails, blah, blah, blah.
[01:05:19.120 --> 01:05:21.560]   So I'm looking to see if they explain
[01:05:21.560 --> 01:05:23.960]   where, how he got, I guess he hired,
[01:05:23.960 --> 01:05:26.120]   or YouTube probably hired.
[01:05:26.120 --> 01:05:27.160]   - YouTube did, yeah.
[01:05:27.160 --> 01:05:29.080]   - Somebody, probably, if you do that.
[01:05:29.080 --> 01:05:30.960]   - Testing it out in a big splash.
[01:05:30.960 --> 01:05:32.000]   - PR, lady.
[01:05:32.000 --> 01:05:33.880]   - Who wants to volunteer?
[01:05:33.880 --> 01:05:38.160]   Any of our listeners proficient in the language.
[01:05:38.160 --> 01:05:39.000]   - You pick a language.
[01:05:39.000 --> 01:05:40.000]   - You tell us.
[01:05:40.000 --> 01:05:42.040]   - You'd have to have four people, right?
[01:05:42.040 --> 01:05:44.280]   You'd have to have somebody doing my voice some--
[01:05:44.280 --> 01:05:45.120]   - Or a different voice.
[01:05:45.120 --> 01:05:47.720]   - And whoever does my voice has to have,
[01:05:47.720 --> 01:05:49.400]   has to have Senator Beast.
[01:05:49.400 --> 01:05:50.880]   (laughing)
[01:05:50.880 --> 01:05:53.880]   It doesn't have to be a fog on, leg on,
[01:05:53.880 --> 01:05:55.160]   it has to be something appropriate to your guy.
[01:05:55.160 --> 01:05:56.640]   - It could be a Bavarian accent.
[01:05:56.640 --> 01:05:58.720]   - Yeah, it talks like this.
[01:05:58.720 --> 01:05:59.840]   - Oh boy.
[01:05:59.840 --> 01:06:01.560]   - And there'd be, you have to have a Jeff,
[01:06:01.560 --> 01:06:03.800]   once you have to have an aunt,
[01:06:03.800 --> 01:06:06.160]   once you have to have a Jason.
[01:06:06.160 --> 01:06:07.840]   And next week you'll have to have a Stacey.
[01:06:07.840 --> 01:06:09.480]   (laughing)
[01:06:09.480 --> 01:06:10.320]   - So.
[01:06:10.320 --> 01:06:11.200]   - Jason, yeah.
[01:06:11.200 --> 01:06:12.040]   - Good luck with that.
[01:06:12.040 --> 01:06:12.880]   - Jason.
[01:06:12.880 --> 01:06:14.040]   (laughing)
[01:06:14.040 --> 01:06:14.880]   - Okay.
[01:06:14.880 --> 01:06:16.880]   Swamp rats has to double and cling on.
[01:06:16.880 --> 01:06:18.720]   (laughing)
[01:06:18.720 --> 01:06:20.040]   - One, two, four.
[01:06:20.040 --> 01:06:21.040]   - One swamp, right?
[01:06:21.040 --> 01:06:22.440]   - Mm.
[01:06:22.440 --> 01:06:26.200]   - A South, according to Moen's letter,
[01:06:26.200 --> 01:06:29.000]   South Korean creator, gained over 30,000 members
[01:06:29.000 --> 01:06:32.720]   after launching channel memberships to seven months ago.
[01:06:32.720 --> 01:06:34.720]   Six million viewers paid for channel memberships
[01:06:34.720 --> 01:06:37.920]   on YouTube in December, 20% increase.
[01:06:37.920 --> 01:06:42.360]   So yeah, it makes sense that Moen would be focused on creators.
[01:06:42.360 --> 01:06:44.800]   He talks about top gaming creators.
[01:06:44.800 --> 01:06:46.880]   YouTube's always done that, right?
[01:06:46.880 --> 01:06:49.960]   It's always been about the, you know,
[01:06:49.960 --> 01:06:51.320]   and I don't, I kind of don't like this
[01:06:51.320 --> 01:06:56.320]   'cause it's, it's, it's, I think a little deceptive
[01:06:56.440 --> 01:07:00.800]   to the rank and file creator who is never gonna make dollar one.
[01:07:00.800 --> 01:07:01.680]   - Mm-hmm.
[01:07:01.680 --> 01:07:03.280]   - But you go, look at all these people
[01:07:03.280 --> 01:07:05.480]   making a living on YouTube.
[01:07:05.480 --> 01:07:06.480]   There's two million of them.
[01:07:06.480 --> 01:07:09.160]   You two could, but what you're really saying is,
[01:07:09.160 --> 01:07:10.760]   come on, everybody, you gotta work for us,
[01:07:10.760 --> 01:07:12.000]   make more content for us.
[01:07:12.000 --> 01:07:13.920]   We're gonna make the bulk of the money.
[01:07:13.920 --> 01:07:15.200]   And they're not lying.
[01:07:15.200 --> 01:07:16.040]   - Yeah.
[01:07:16.040 --> 01:07:16.880]   - There are two million people.
[01:07:16.880 --> 01:07:17.720]   - No, it's true.
[01:07:17.720 --> 01:07:19.000]   They make time for you to say it's happening.
[01:07:19.000 --> 01:07:20.360]   - Yeah.
[01:07:20.360 --> 01:07:23.920]   And you, and, and it's true that you could also.
[01:07:23.920 --> 01:07:25.040]   (laughing)
[01:07:25.040 --> 01:07:26.320]   - Are you likely to?
[01:07:26.320 --> 01:07:27.520]   That's a different story.
[01:07:27.520 --> 01:07:32.240]   - Oh, and here is Renee Ritchie, creator liaison in the,
[01:07:32.240 --> 01:07:35.840]   in that YouTube letter, what creators need to know
[01:07:35.840 --> 01:07:37.720]   from Neil's letter.
[01:07:37.720 --> 01:07:39.240]   So, yay, Renee.
[01:07:39.240 --> 01:07:43.840]   They also talk about protecting kids.
[01:07:43.840 --> 01:07:49.600]   Looking ahead, this is a pivotal moment for our industry.
[01:07:49.600 --> 01:07:51.960]   We face challenging economic headwinds
[01:07:51.960 --> 01:07:54.680]   and uncertain geopolitical conditions.
[01:07:54.680 --> 01:07:58.400]   AI represents incredible creative opportunities,
[01:07:58.400 --> 01:08:01.560]   but must be balanced by responsible stewardship.
[01:08:01.560 --> 01:08:05.040]   This is what motivates me and everyone at YouTube
[01:08:05.040 --> 01:08:07.840]   to do our best work every single day.
[01:08:07.840 --> 01:08:10.840]   It's kind of an anodyne, typical of what you'd expect letter,
[01:08:10.840 --> 01:08:14.480]   but there's some facts in the interview.
[01:08:14.480 --> 01:08:19.480]   - How does, how does Sol Tank do when it comes to YouTube?
[01:08:19.480 --> 01:08:23.280]   I mean, YouTube as a, as a platform has been around much longer
[01:08:23.280 --> 01:08:24.640]   than a TikTok than it is.
[01:08:24.640 --> 01:08:26.080]   - It's just funny, when he was doing TikTok,
[01:08:26.080 --> 01:08:27.120]   he said, "Why don't you do more YouTube?"
[01:08:27.120 --> 01:08:28.280]   He's in there.
[01:08:28.280 --> 01:08:31.040]   So he has done, since then he's done some long form,
[01:08:31.040 --> 01:08:32.680]   YouTube, he does YouTube shorts.
[01:08:32.680 --> 01:08:34.320]   I've seen some of his long form stuff, yeah.
[01:08:34.320 --> 01:08:36.720]   And it's good, but Insta's his, you know,
[01:08:36.720 --> 01:08:39.120]   because of his style, which was TikTok inspired
[01:08:39.120 --> 01:08:42.280]   of his last, fast, fast, short cuts.
[01:08:42.280 --> 01:08:44.840]   It really lends itself to Instagram reels
[01:08:44.840 --> 01:08:46.840]   or effectively TikTok.
[01:08:46.840 --> 01:08:47.680]   - Right.
[01:08:47.680 --> 01:08:49.560]   - It's not just cooking too,
[01:08:49.560 --> 01:08:53.000]   he's flirting in a specific way with his audience.
[01:08:53.000 --> 01:08:54.080]   - Right, it's why.
[01:08:54.080 --> 01:08:56.080]   It's, it's, it's, it's, it's, it's flirting.
[01:08:56.080 --> 01:08:56.920]   - Really?
[01:08:56.920 --> 01:08:57.760]   (laughs)
[01:08:57.760 --> 01:08:59.960]   - How many proposals has Hank had?
[01:08:59.960 --> 01:09:01.800]   - Quite a few, he's a, he's--
[01:09:01.800 --> 01:09:02.960]   - He'll be on the bachelor in action.
[01:09:02.960 --> 01:09:05.040]   - Yeah, yeah.
[01:09:05.040 --> 01:09:06.880]   He's got a girlfriend, now I shouldn't say this though,
[01:09:06.880 --> 01:09:07.720]   probably, huh?
[01:09:07.720 --> 01:09:10.280]   Here he's, here he's making--
[01:09:10.280 --> 01:09:12.720]   - Did he beat her through the, through the TikTok?
[01:09:12.720 --> 01:09:15.480]   - Bonnie, no, she's an influencer though.
[01:09:15.480 --> 01:09:16.320]   I hear, I understand.
[01:09:16.320 --> 01:09:17.960]   - Oh, oh wow, okay.
[01:09:17.960 --> 01:09:19.120]   - He's done a lot of collabs.
[01:09:19.120 --> 01:09:20.480]   That's one of the things you do, right?
[01:09:20.480 --> 01:09:21.320]   - Yep.
[01:09:21.320 --> 01:09:23.920]   - Yeah, you're doing these, these collabs.
[01:09:23.920 --> 01:09:24.760]   - Which is cool.
[01:09:24.760 --> 01:09:25.600]   - Yeah.
[01:09:25.600 --> 01:09:27.400]   - That's really a nice part of it.
[01:09:27.400 --> 01:09:29.840]   - Yeah, there is, you know, and I think YouTube has this too,
[01:09:29.840 --> 01:09:33.080]   there, whereas this kind of collegial thing about it.
[01:09:33.080 --> 01:09:36.000]   Here's making a Chico-Pesto sandwich.
[01:09:36.000 --> 01:09:37.160]   - Oh, this is-- - That's a simple reaction.
[01:09:37.160 --> 01:09:39.520]   - This is an ad, did you see that real quick?
[01:09:39.520 --> 01:09:41.920]   The Misetta peppers, it's an ad for Misetta.
[01:09:41.920 --> 01:09:42.920]   - Oh.
[01:09:42.920 --> 01:09:46.200]   - And this is by the way, TikTok doesn't get any of that money.
[01:09:46.200 --> 01:09:50.000]   Nor does YouTube, and that's really how you make money
[01:09:50.000 --> 01:09:52.800]   and all of these platforms is selling your own ads.
[01:09:52.800 --> 01:09:55.080]   - Oh, that looks good, oh, that looks good, oh.
[01:09:55.080 --> 01:09:57.080]   - And don't forget a Misetta.
[01:09:57.080 --> 01:10:01.760]   (laughing)
[01:10:01.760 --> 01:10:03.160]   - This is the jar.
[01:10:03.160 --> 01:10:04.520]   - He blends the lids.
[01:10:04.520 --> 01:10:05.960]   - But isn't that subtle?
[01:10:05.960 --> 01:10:07.720]   - Yes, I freaking love that.
[01:10:07.720 --> 01:10:09.000]   - Did I tell you about this one?
[01:10:09.000 --> 01:10:09.840]   - It's still.
[01:10:09.840 --> 01:10:10.840]   - Yeah, I'd say this one.
[01:10:10.840 --> 01:10:11.680]   - It's awesome, awesome.
[01:10:11.680 --> 01:10:12.520]   - Did I tell you about that?
[01:10:12.520 --> 01:10:13.840]   I can't play the music, but I'll take that off.
[01:10:13.840 --> 01:10:17.800]   But this one, he got a partnership with a,
[01:10:17.800 --> 01:10:21.000]   Los Angeles company, ShockU.LA,
[01:10:21.000 --> 01:10:23.520]   they go around and do catering for big events,
[01:10:23.520 --> 01:10:24.560]   or big company.
[01:10:24.560 --> 01:10:27.800]   They gave him that Snow Crab,
[01:10:27.800 --> 01:10:30.160]   it looked like about a pound of caviar.
[01:10:30.160 --> 01:10:31.800]   - It's huge amount of caviar.
[01:10:31.800 --> 01:10:34.840]   - And he made a po-boy, a Snow Crab po-boy
[01:10:34.840 --> 01:10:37.440]   with a caviar remoulade.
[01:10:37.440 --> 01:10:41.480]   He said it was a $7,000 sandwich.
[01:10:41.480 --> 01:10:43.880]   (laughing)
[01:10:43.880 --> 01:10:45.440]   - It just looks so delicious.
[01:10:45.440 --> 01:10:46.280]   - That's 11.
[01:10:46.280 --> 01:10:49.440]   - I have a dollar sandwich.
[01:10:49.440 --> 01:10:52.520]   - You made a $7,000 sandwich today.
[01:10:52.520 --> 01:10:53.880]   - That's amazing.
[01:10:53.880 --> 01:10:56.400]   - Oh, capitalism man, capitalism.
[01:10:56.400 --> 01:10:59.040]   - Has Chef, the only Chef reaction ever?
[01:10:59.040 --> 01:11:00.480]   - No, I don't think so.
[01:11:00.480 --> 01:11:02.080]   - I don't think so.
[01:11:02.080 --> 01:11:04.600]   - I have to ask him about that, that's a good question.
[01:11:04.600 --> 01:11:07.080]   Okay, so notice by the way here on his Insta,
[01:11:07.080 --> 01:11:09.640]   link to his YouTube's.
[01:11:09.640 --> 01:11:12.240]   So, you know, they're probably doing it.
[01:11:12.240 --> 01:11:13.440]   And this is a short, right?
[01:11:13.440 --> 01:11:15.440]   I think this is a short.
[01:11:15.440 --> 01:11:19.160]   YouTube, first YouTube is up, YouTube too is up.
[01:11:19.160 --> 01:11:20.000]   Oh, these are little Insta.
[01:11:20.000 --> 01:11:21.080]   - Oh, I see, okay.
[01:11:21.080 --> 01:11:22.800]   - Insta things, and then he has the link.
[01:11:22.800 --> 01:11:23.640]   - It's still on Instagram.
[01:11:23.640 --> 01:11:25.320]   - Link visit the link to YouTube.
[01:11:25.320 --> 01:11:26.640]   - Oh, my God, this is a commercial.
[01:11:26.640 --> 01:11:28.720]   - Yeah, this is longer for it.
[01:11:28.720 --> 01:11:29.560]   - This is a big league.
[01:11:29.560 --> 01:11:30.400]   - Yeah, this is longer.
[01:11:30.400 --> 01:11:32.560]   So this is where he gets you into his longer form.
[01:11:32.560 --> 01:11:33.400]   - Yeah.
[01:11:33.400 --> 01:11:34.720]   - And I don't know, let's see what the views,
[01:11:34.720 --> 01:11:37.440]   I don't think he does these do so as well.
[01:11:37.440 --> 01:11:38.280]   Where do I see them?
[01:11:38.280 --> 01:11:39.120]   - 39,000.
[01:11:39.120 --> 01:11:40.120]   - 7, 10 months.
[01:11:40.120 --> 01:11:41.480]   - 10 months, yes, see, that's nothing.
[01:11:41.480 --> 01:11:42.320]   - Oh, there is.
[01:11:42.320 --> 01:11:43.160]   - That's nothing.
[01:11:43.160 --> 01:11:44.000]   - Yeah.
[01:11:44.000 --> 01:11:46.000]   - You sell them the salt, you sell them the cookbook,
[01:11:46.000 --> 01:11:48.160]   you sell them the TV show, you sell them the, no, no, no.
[01:11:48.160 --> 01:11:49.560]   - You're selling your presents.
[01:11:49.560 --> 01:11:50.400]   - Yeah.
[01:11:50.400 --> 01:11:51.240]   - You know.
[01:11:51.240 --> 01:11:52.080]   - I am hungry.
[01:11:52.080 --> 01:11:53.800]   - I know, it's impossible to watch his videos
[01:11:53.800 --> 01:11:55.520]   and not get incredibly hungry.
[01:11:55.520 --> 01:11:56.360]   My boy.
[01:11:56.360 --> 01:11:57.200]   - Yeah.
[01:11:57.200 --> 01:11:58.040]   - I'll tell you.
[01:11:58.040 --> 01:12:00.160]   - He, did I tell you he cooked a kobana
[01:12:00.160 --> 01:12:02.520]   with Guy Fieri last weekend?
[01:12:02.520 --> 01:12:05.320]   - I heard you mentioned Guy Fieri, that's awesome.
[01:12:05.320 --> 01:12:08.640]   - He was at, is that gonna be on Diner's Drive-ins and--
[01:12:08.640 --> 01:12:09.480]   - No, I just--
[01:12:09.480 --> 01:12:11.120]   - Yeah, Guy doesn't do that anymore,
[01:12:11.120 --> 01:12:12.480]   they just rerun it forever.
[01:12:12.480 --> 01:12:13.320]   I don't think he's--
[01:12:13.320 --> 01:12:15.400]   - No, no, no, no, no, he just visited,
[01:12:15.400 --> 01:12:16.800]   he just visited places near me.
[01:12:16.800 --> 01:12:18.040]   - Oh, okay.
[01:12:18.040 --> 01:12:22.720]   So, no, it was at a Miami Food Festival.
[01:12:22.720 --> 01:12:24.760]   Apparently it's a big one.
[01:12:24.760 --> 01:12:27.960]   And he, he, he did a couple of things.
[01:12:27.960 --> 01:12:32.200]   He was, he was at the Mr. Clean Booth making Bloody Marys.
[01:12:32.200 --> 01:12:35.200]   (laughing)
[01:12:35.200 --> 01:12:38.240]   - Somewhere I have some video of that.
[01:12:38.240 --> 01:12:41.680]   And then Guy Fieri was one of the celebrity chefs there.
[01:12:41.680 --> 01:12:42.840]   And he did a-- - Fieri.
[01:12:42.840 --> 01:12:44.320]   - And he was on, Fieri.
[01:12:44.320 --> 01:12:45.160]   - Fieri.
[01:12:45.160 --> 01:12:49.080]   He was on stage with Guy doing a kobana sandwich.
[01:12:49.080 --> 01:12:49.920]   - Oh, that's so cool.
[01:12:49.920 --> 01:12:51.200]   - Yeah.
[01:12:51.200 --> 01:12:53.600]   So, yeah, he's, you know, there's this whole,
[01:12:53.600 --> 01:12:55.920]   he'd make the rounds basically these days.
[01:12:55.920 --> 01:12:56.760]   - Yeah.
[01:12:56.760 --> 01:12:59.440]   - Food is it, you know, thanks to the Food Network.
[01:12:59.440 --> 01:13:00.240]   Food is it.
[01:13:00.240 --> 01:13:02.200]   - But it's funny 'cause Henry--
[01:13:02.200 --> 01:13:03.040]   - It's a little always been style.
[01:13:03.040 --> 01:13:04.520]   - Henry always wanted to do that.
[01:13:04.520 --> 01:13:09.040]   You know, he grew up watching YouTube food videos.
[01:13:09.040 --> 01:13:10.160]   That's how he learned to cook.
[01:13:10.160 --> 01:13:11.200]   It wasn't for me.
[01:13:11.200 --> 01:13:12.960]   - Oh, yeah.
[01:13:12.960 --> 01:13:15.240]   So he was always oriented in that.
[01:13:15.240 --> 01:13:17.080]   - What does he think of your machines,
[01:13:17.080 --> 01:13:19.040]   your many machines?
[01:13:19.040 --> 01:13:19.880]   - He doesn't--
[01:13:19.880 --> 01:13:21.120]   - She'd eaten or jealous?
[01:13:21.120 --> 01:13:24.000]   - That's a good question.
[01:13:24.000 --> 01:13:24.840]   I don't know.
[01:13:24.840 --> 01:13:26.120]   He doesn't use machines, really.
[01:13:26.120 --> 01:13:29.460]   He uses like a bullet, mostly chops his own.
[01:13:29.460 --> 01:13:31.880]   I gave him my-- - Yeah.
[01:13:31.880 --> 01:13:35.760]   - I had a very nice knife roll with like the best knives.
[01:13:35.760 --> 01:13:39.000]   Not that I'm a chef, but I, you know, at one point I said,
[01:13:39.000 --> 01:13:40.400]   "I don't want some good knives."
[01:13:40.400 --> 01:13:41.680]   And I gave it to him one year
[01:13:41.680 --> 01:13:43.760]   and the I don't think he doesn't use it, so I don't know.
[01:13:43.760 --> 01:13:45.120]   (laughing)
[01:13:45.120 --> 01:13:46.200]   If he won't use my knives,
[01:13:46.200 --> 01:13:49.640]   he definitely won't use my kitchen appliances.
[01:13:49.640 --> 01:13:52.840]   Microsoft-- - Of what Mr. Rolle,
[01:13:52.840 --> 01:13:55.040]   he'll simply put it in our Discord,
[01:13:55.040 --> 01:13:57.280]   speaking of AI and dubbing.
[01:13:57.280 --> 01:13:58.120]   - Oh, wait a minute.
[01:13:58.120 --> 01:13:59.200]   Now this is the, is this--
[01:13:59.200 --> 01:14:00.640]   - Turn your sound on. - We've been working on this.
[01:14:00.640 --> 01:14:01.600]   We've been working on this.
[01:14:01.600 --> 01:14:03.520]   We're very excited about this.
[01:14:03.520 --> 01:14:08.440]   At some point I can retire and AI Leo could take over.
[01:14:08.440 --> 01:14:10.320]   - Well, do you do Vondem Himmelbist?
[01:14:10.320 --> 01:14:12.120]   Alice Lied on Schmerz and Stilis.
[01:14:12.120 --> 01:14:12.960]   (laughing)
[01:14:12.960 --> 01:14:13.800]   Then he thought that he'd hellend
[01:14:13.800 --> 01:14:16.160]   is dopple mitt Erkwickung fullest.
[01:14:16.160 --> 01:14:18.480]   Ah, Ichbendist, Tribans, Mude,
[01:14:18.480 --> 01:14:20.800]   was sold all their Schmerz und Luz,
[01:14:20.800 --> 01:14:23.880]   Susser Fridekom, Akom, and Mine Bruce.
[01:14:23.880 --> 01:14:26.560]   - What did I just say?
[01:14:26.560 --> 01:14:28.280]   - I know, do I need to bleep any of this?
[01:14:28.280 --> 01:14:29.720]   - You said, "Abby, just let me know."
[01:14:29.720 --> 01:14:32.120]   - Okay, but it was real words, right?
[01:14:32.120 --> 01:14:33.680]   - In my breast.
[01:14:33.680 --> 01:14:35.160]   - It was real words, wasn't it?
[01:14:35.160 --> 01:14:36.520]   - He put the words for her.
[01:14:36.520 --> 01:14:37.360]   - The words were-- - The words were.
[01:14:37.360 --> 01:14:38.400]   - transcript there for you.
[01:14:39.240 --> 01:14:40.320]   - Ah.
[01:14:40.320 --> 01:14:42.680]   - Oh, no, let me go look.
[01:14:42.680 --> 01:14:44.760]   - Thou that from the heavens arch.
[01:14:44.760 --> 01:14:46.360]   - Every pain and sorrow still.
[01:14:46.360 --> 01:14:47.960]   - Every pain and sorrow still.
[01:14:47.960 --> 01:14:49.320]   - Is this good?
[01:14:49.320 --> 01:14:52.840]   - Oh, the doubly wretched heart,
[01:14:52.840 --> 01:14:54.680]   doubly with refreshment fillist.
[01:14:54.680 --> 01:14:57.000]   I am weary with contending why this pain
[01:14:57.000 --> 01:15:01.720]   and desire peace descending come into my breast.
[01:15:01.720 --> 01:15:03.320]   - Okay, play it again now, we're now in the game.
[01:15:03.320 --> 01:15:06.160]   - Now that you know it's poetry,
[01:15:06.160 --> 01:15:08.160]   or their duvondum hymnal best.
[01:15:08.160 --> 01:15:09.920]   Alice lead on Schmerzen's stillest.
[01:15:09.920 --> 01:15:10.760]   - But whose voice is that?
[01:15:10.760 --> 01:15:11.920]   - Then their doubled Elend is--
[01:15:11.920 --> 01:15:12.760]   - That's my voice.
[01:15:12.760 --> 01:15:13.600]   - Is that?
[01:15:13.600 --> 01:15:14.440]   - Her quick and full.
[01:15:14.440 --> 01:15:16.320]   (speaking in foreign language)
[01:15:16.320 --> 01:15:18.600]   Was soul, all their Schmerz und Lust,
[01:15:18.600 --> 01:15:21.360]   Sser free day, calm, a calm and minute bruce.
[01:15:21.360 --> 01:15:22.960]   - Terrible pronunciation.
[01:15:22.960 --> 01:15:24.280]   - Is it?
[01:15:24.280 --> 01:15:25.120]   - Oh yeah.
[01:15:25.120 --> 01:15:25.960]   - It doesn't sound like you.
[01:15:25.960 --> 01:15:27.280]   - That's the dub version.
[01:15:27.280 --> 01:15:28.400]   Sounds like a young Leo.
[01:15:28.400 --> 01:15:30.240]   - Yeah, it sounds like a young Leo.
[01:15:30.240 --> 01:15:32.360]   - Yeah, you're best at Leo.
[01:15:32.360 --> 01:15:35.440]   - Which synthesis are you using?
[01:15:35.440 --> 01:15:38.720]   Was that Descript or was it 11 Labs?
[01:15:38.720 --> 01:15:39.560]   I don't know which one was.
[01:15:39.560 --> 01:15:42.600]   - 11 Labs, he's played with both.
[01:15:42.600 --> 01:15:44.960]   - Yeah, it's getting quite efficient.
[01:15:44.960 --> 01:15:46.640]   - You know what, Anthony, the coffee stain
[01:15:46.640 --> 01:15:47.800]   is not on my shirt.
[01:15:47.800 --> 01:15:50.000]   (laughing)
[01:15:50.000 --> 01:15:52.360]   - Now see, AI removes those imperfections.
[01:15:52.360 --> 01:15:54.080]   - Oh, yeah, I like it.
[01:15:54.080 --> 01:15:55.600]   - I like it.
[01:15:55.600 --> 01:15:56.560]   - Yeah, he's working on that.
[01:15:56.560 --> 01:15:59.000]   It's kind of the low voxel version of Leo, though.
[01:15:59.000 --> 01:16:01.560]   It's not, it looks more like,
[01:16:01.560 --> 01:16:05.320]   I told him we kind of want a max headroom thing.
[01:16:05.320 --> 01:16:06.160]   (laughing)
[01:16:06.160 --> 01:16:07.000]   Right?
[01:16:07.000 --> 01:16:07.920]   - That's pretty close, right?
[01:16:07.920 --> 01:16:08.760]   - Yeah.
[01:16:08.760 --> 01:16:09.600]   - Yeah.
[01:16:09.600 --> 01:16:10.440]   - Hello, Polly.
[01:16:10.440 --> 01:16:12.560]   - You got it, but you got to add the stutter in.
[01:16:12.560 --> 01:16:14.720]   - Yeah, we can do that, sure.
[01:16:14.720 --> 01:16:16.800]   (laughing)
[01:16:16.800 --> 01:16:19.040]   - Anthony Nielsen has become our king of AI.
[01:16:19.040 --> 01:16:20.040]   - He really has, yeah.
[01:16:20.040 --> 01:16:21.240]   - Let's talk about AI.
[01:16:21.240 --> 01:16:24.840]   AI is all the rage these days.
[01:16:24.840 --> 01:16:29.480]   In fact, Microsoft, actually they kind of misled people.
[01:16:29.480 --> 01:16:33.040]   They announced they have a new thing they're doing
[01:16:33.040 --> 01:16:35.960]   when they update Windows, they called it a moment.
[01:16:35.960 --> 01:16:38.640]   And moment two came out yesterday
[01:16:38.640 --> 01:16:42.840]   and they said that it would bring AI powered Bing
[01:16:42.840 --> 01:16:44.100]   to Windows 11.
[01:16:44.100 --> 01:16:47.040]   Not really.
[01:16:47.040 --> 01:16:50.000]   What it does is it puts a logo on your Windows 11 task bar
[01:16:50.000 --> 01:16:55.000]   that when you click it opens up edge and the Bing chat.
[01:16:55.000 --> 01:16:57.680]   - So they gave you a shortcut or an ad.
[01:16:57.680 --> 01:16:58.560]   (laughing)
[01:16:58.560 --> 01:16:59.400]   - Or an ad.
[01:16:59.400 --> 01:17:00.240]   - An ad.
[01:17:00.240 --> 01:17:02.160]   - Let's call a spade a spade.
[01:17:02.160 --> 01:17:03.000]   - Yeah.
[01:17:03.000 --> 01:17:03.820]   - It's an ad.
[01:17:03.820 --> 01:17:05.160]   - Oh boy.
[01:17:05.160 --> 01:17:06.560]   - See there's-- - Is that a feature?
[01:17:06.560 --> 01:17:08.760]   - Yeah, here's the, if you look at it
[01:17:08.760 --> 01:17:13.480]   in the little search pill we call it, there's a B.
[01:17:13.480 --> 01:17:16.120]   When you click it, it says introduce the new Bing,
[01:17:16.120 --> 01:17:17.480]   but if you want to do anything,
[01:17:17.480 --> 01:17:18.640]   it has to open up edge and do it.
[01:17:18.640 --> 01:17:20.040]   - You can't just use that little search bar
[01:17:20.040 --> 01:17:21.120]   and plug it in there.
[01:17:21.120 --> 01:17:21.960]   - No.
[01:17:21.960 --> 01:17:23.120]   - See, that would be an integrated--
[01:17:23.120 --> 01:17:23.960]   - Interesting.
[01:17:23.960 --> 01:17:25.200]   - I mean, there's nothing that they didn't do that
[01:17:25.200 --> 01:17:26.040]   except they wanted to--
[01:17:26.040 --> 01:17:27.280]   - This is all performance.
[01:17:27.280 --> 01:17:29.080]   It's Microsoft is playing a game here.
[01:17:29.080 --> 01:17:29.920]   - Yeah.
[01:17:31.320 --> 01:17:34.440]   Well, Microsoft was sure touted there in the beginning
[01:17:34.440 --> 01:17:37.200]   as having made a really smart move
[01:17:37.200 --> 01:17:38.960]   with this whole chat GPT thing.
[01:17:38.960 --> 01:17:40.440]   - The reporters or idiots.
[01:17:40.440 --> 01:17:43.040]   - Were a couple of weeks away from that now,
[01:17:43.040 --> 01:17:44.800]   has that proven to be the case?
[01:17:44.800 --> 01:17:48.000]   I mean, Google didn't, you know, initially.
[01:17:48.000 --> 01:17:49.480]   I mean, they had their-- - They still haven't.
[01:17:49.480 --> 01:17:51.440]   - They had their bar thing.
[01:17:51.440 --> 01:17:53.160]   - But they were released here.
[01:17:53.160 --> 01:17:54.000]   - Yeah.
[01:17:54.000 --> 01:17:55.920]   - That's turned out to actually be kind of a good thing
[01:17:55.920 --> 01:17:57.180]   by comparison, right?
[01:17:58.480 --> 01:18:01.960]   - Jason, in the annex in the kitty table,
[01:18:01.960 --> 01:18:04.240]   part of the rundown, I put in two stories
[01:18:04.240 --> 01:18:07.760]   that was interesting to me that there's already a tech lash
[01:18:07.760 --> 01:18:09.320]   against OpenAI now.
[01:18:09.320 --> 01:18:10.160]   - Oh, yeah.
[01:18:10.160 --> 01:18:11.000]   - Oh, yeah.
[01:18:11.000 --> 01:18:13.520]   - And that's kind of reasonable because OpenAI,
[01:18:13.520 --> 01:18:17.520]   which was funded in 2015, partly by Elon Musk,
[01:18:17.520 --> 01:18:19.600]   who part a company with him a few years later,
[01:18:19.600 --> 01:18:24.320]   to be a response to the closed source development of AI
[01:18:24.320 --> 01:18:26.800]   by Microsoft and Google and others.
[01:18:26.800 --> 01:18:30.880]   It was to be the open one, the nonprofit one,
[01:18:30.880 --> 01:18:32.160]   but it's no longer that.
[01:18:32.160 --> 01:18:33.760]   - Yeah, it's definitely not nonprofit.
[01:18:33.760 --> 01:18:34.600]   - Anyone.
[01:18:34.600 --> 01:18:35.440]   - Yeah.
[01:18:35.440 --> 01:18:38.720]   - So this is a motherboard article, OpenAI is now,
[01:18:38.720 --> 01:18:40.280]   everything it promised not to be,
[01:18:40.280 --> 01:18:43.040]   corporate closed source and for profit.
[01:18:43.040 --> 01:18:43.880]   - True, right?
[01:18:43.880 --> 01:18:44.720]   - Yeah, right.
[01:18:44.720 --> 01:18:51.760]   - They, now this to me is a little nerve-wracking.
[01:18:51.760 --> 01:18:54.240]   Sam Altman, its CEO, wrote a blog post this week,
[01:18:54.240 --> 01:18:57.720]   planning for AGI and beyond.
[01:18:57.720 --> 01:19:02.760]   AGI is the scary AI, the general intelligence,
[01:19:02.760 --> 01:19:05.560]   which would essentially be,
[01:19:05.560 --> 01:19:07.880]   as he says, our mission is to ensure that
[01:19:07.880 --> 01:19:10.280]   artificial and general intelligence,
[01:19:10.280 --> 01:19:14.520]   AI systems that are generally smarter than humans
[01:19:14.520 --> 01:19:16.200]   benefits all of humanity.
[01:19:16.200 --> 01:19:22.560]   If AGI successfully created, well, yeah,
[01:19:22.560 --> 01:19:23.480]   I don't know.
[01:19:24.320 --> 01:19:26.400]   (laughs)
[01:19:26.400 --> 01:19:28.520]   I don't know, we don't have AGI.
[01:19:28.520 --> 01:19:30.040]   I think that's pretty clear,
[01:19:30.040 --> 01:19:33.440]   Blake Lemoine and his look not withstanding.
[01:19:33.440 --> 01:19:34.520]   It's not sentient.
[01:19:34.520 --> 01:19:35.760]   - No, no.
[01:19:35.760 --> 01:19:38.100]   - It's not as smart as a human.
[01:19:38.100 --> 01:19:41.160]   It sounds like it is.
[01:19:41.160 --> 01:19:45.080]   I once asked Ray Kurzweil, who was kind of the king
[01:19:45.080 --> 01:19:46.800]   of all of the singularity, right?
[01:19:46.800 --> 01:19:49.760]   He said his sometime in the next 20 years,
[01:19:49.760 --> 01:19:51.640]   he said, I think by 2035,
[01:19:52.800 --> 01:19:54.480]   their computers will be in,
[01:19:54.480 --> 01:19:58.160]   now this is his key phrase, indistinguishable from humans.
[01:19:58.160 --> 01:20:00.240]   And I said, well, but will they be thinking?
[01:20:00.240 --> 01:20:01.400]   And he says, it doesn't matter.
[01:20:01.400 --> 01:20:02.880]   If you can't tell the difference,
[01:20:02.880 --> 01:20:05.560]   they're indistinguishable from humans, that's it.
[01:20:05.560 --> 01:20:10.600]   It's foolish to say, well, are they thinking?
[01:20:10.600 --> 01:20:13.240]   That's not, if you can't tell, does it matter?
[01:20:13.240 --> 01:20:15.400]   I don't know if Jason's thinking, right?
[01:20:15.400 --> 01:20:16.480]   Oh, thank you.
[01:20:16.480 --> 01:20:18.480]   - Thank you for that.
[01:20:18.480 --> 01:20:22.360]   - What is thinking to a computer?
[01:20:22.360 --> 01:20:26.720]   I mean, any operation that is determined by a computer
[01:20:26.720 --> 01:20:29.200]   is in essence thinking, right?
[01:20:29.200 --> 01:20:30.400]   - Not as we think.
[01:20:30.400 --> 01:20:32.240]   - Not as we think of thinking,
[01:20:32.240 --> 01:20:34.080]   but when I think of--
[01:20:34.080 --> 01:20:35.120]   - Yeah, it's thinking. - It's getting weird.
[01:20:35.120 --> 01:20:37.360]   When I think of thinking, yeah, it's thinking.
[01:20:37.360 --> 01:20:39.920]   I think of, you know, I want this thing to happen.
[01:20:39.920 --> 01:20:41.520]   I have to understand it, I have to figure out,
[01:20:41.520 --> 01:20:44.640]   I have to know how to do this thing or whatever.
[01:20:44.640 --> 01:20:46.720]   That's what a computer does when it's, you know,
[01:20:46.720 --> 01:20:48.120]   performing an action.
[01:20:48.120 --> 01:20:49.320]   - I mean-- - It's the type of thinking.
[01:20:49.320 --> 01:20:50.520]   - Yeah.
[01:20:50.520 --> 01:20:51.520]   - Yeah. - That's Ray's point.
[01:20:51.520 --> 01:20:54.600]   - Exactly, Kurzweil said, if you can't tell the difference,
[01:20:54.600 --> 01:20:56.600]   doesn't matter what the internal process is,
[01:20:56.600 --> 01:20:57.960]   if the result is the same.
[01:20:57.960 --> 01:21:02.320]   Now, the real singularity comes when,
[01:21:02.320 --> 01:21:03.920]   and this is where it gets scary,
[01:21:03.920 --> 01:21:08.920]   when AGI can design better AGI or can build better machines.
[01:21:08.920 --> 01:21:10.560]   - Can build a better version of itself.
[01:21:10.560 --> 01:21:12.560]   - Then they're faster than humans.
[01:21:12.560 --> 01:21:14.680]   It becomes iterative and it gets faster
[01:21:14.680 --> 01:21:17.960]   and faster accelerates to the point where, you know,
[01:21:17.960 --> 01:21:19.160]   I don't know what, I don't know what--
[01:21:19.160 --> 01:21:20.680]   - Yeah, what is going to go to it.
[01:21:20.680 --> 01:21:21.840]   - Yeah. - In other words, right now,
[01:21:21.840 --> 01:21:23.200]   we're holding it back.
[01:21:23.200 --> 01:21:25.160]   But as soon as it could--
[01:21:25.160 --> 01:21:26.680]   - Pesky humans. - Pesky humans.
[01:21:26.680 --> 01:21:29.840]   As soon as it can do it itself, nothing to stop it.
[01:21:29.840 --> 01:21:31.640]   A machine that could design machines.
[01:21:31.640 --> 01:21:37.000]   I don't know what takes over the universe.
[01:21:37.000 --> 01:21:38.200]   - Yeah.
[01:21:38.200 --> 01:21:41.800]   - So we can imagine a world in which humanity flourishes
[01:21:41.800 --> 01:21:44.000]   to a degree that it's probably impossible
[01:21:44.000 --> 01:21:46.040]   for any of us to fully visualize it yet.
[01:21:46.040 --> 01:21:48.560]   We hope to contribute to the world, says Altman.
[01:21:48.560 --> 01:21:52.840]   And AGI aligned with such flourishing.
[01:21:52.840 --> 01:21:55.400]   And if not, well, we tried.
[01:21:55.400 --> 01:21:58.580]   I mean, really?
[01:21:58.580 --> 01:22:01.800]   The first AGI will just be a point
[01:22:01.800 --> 01:22:03.240]   along the continuum of intelligence.
[01:22:03.240 --> 01:22:05.880]   We think it's likely progress will continue from there.
[01:22:05.880 --> 01:22:07.760]   Yeah, see, that's what scares me.
[01:22:07.760 --> 01:22:09.520]   Possibly sustaining the rate of progress
[01:22:09.520 --> 01:22:12.240]   we've seen over the past decade for a long period of time.
[01:22:12.240 --> 01:22:14.960]   Or he doesn't say it or faster.
[01:22:14.960 --> 01:22:17.800]   If this is true, the world could become extremely different
[01:22:17.800 --> 01:22:19.000]   from how it is today.
[01:22:19.000 --> 01:22:21.240]   And the risks could be extraordinary.
[01:22:21.240 --> 01:22:25.680]   A misaligned, super intelligent AGI
[01:22:25.680 --> 01:22:27.800]   could cause grievous harm to the world.
[01:22:27.800 --> 01:22:33.800]   An autocratic regime with a decisive super intelligence lead
[01:22:33.800 --> 01:22:35.560]   could do that as well.
[01:22:35.560 --> 01:22:36.560]   Yikes.
[01:22:36.560 --> 01:22:40.680]   So what are they doing to prevent that?
[01:22:40.680 --> 01:22:43.160]   - Nothing.
[01:22:43.160 --> 01:22:45.420]   (laughing)
[01:22:46.760 --> 01:22:49.640]   We think a slower takeoff is easier to make safe
[01:22:49.640 --> 01:22:53.280]   and coordination among AGI efforts to slow down
[01:22:53.280 --> 01:22:57.120]   at critical junctures will likely be important,
[01:22:57.120 --> 01:22:59.080]   even in a world where we don't need to do this
[01:22:59.080 --> 01:23:01.040]   to solve technical alignment problems,
[01:23:01.040 --> 01:23:03.040]   slowing down may be important to give society
[01:23:03.040 --> 01:23:04.160]   enough time to adapt.
[01:23:04.160 --> 01:23:07.060]   So don't be in such a hurry.
[01:23:07.060 --> 01:23:11.240]   Successfully transitioning to a world
[01:23:11.240 --> 01:23:12.880]   with super intelligence,
[01:23:12.880 --> 01:23:16.240]   I should add non-human super intelligence
[01:23:16.240 --> 01:23:18.240]   is perhaps the most important and hopeful
[01:23:18.240 --> 01:23:20.400]   and scary project in human history.
[01:23:20.400 --> 01:23:22.440]   Success is far from guaranteed
[01:23:22.440 --> 01:23:25.400]   and the stakes boundless downside
[01:23:25.400 --> 01:23:28.960]   and boundless upside will hopefully unite all of us.
[01:23:28.960 --> 01:23:30.280]   - It's not really--
[01:23:30.280 --> 01:23:32.080]   - All he's saying is slow down.
[01:23:32.080 --> 01:23:34.720]   He's not really giving us any tools.
[01:23:34.720 --> 01:23:35.880]   - He's also out there saying,
[01:23:35.880 --> 01:23:37.360]   this was in last week too,
[01:23:37.360 --> 01:23:39.800]   it was in the rundown last week where he's saying,
[01:23:39.800 --> 01:23:40.760]   regulate us.
[01:23:40.760 --> 01:23:42.000]   So we don't have to make good decisions,
[01:23:42.000 --> 01:23:42.800]   you make them for us.
[01:23:42.800 --> 01:23:43.800]   - Yeah, that's not it.
[01:23:43.800 --> 01:23:46.240]   Kevin Marks and I are both in the rundown,
[01:23:46.240 --> 01:23:47.240]   invading the rundown.
[01:23:47.240 --> 01:23:48.240]   Hi, Kevin.
[01:23:48.240 --> 01:23:49.080]   - Oh, how many--
[01:23:49.080 --> 01:23:50.400]   - Hi, Kevin. - Emily Bender.
[01:23:50.400 --> 01:23:52.680]   - We invited him on, but I think it was such a short notice
[01:23:52.680 --> 01:23:54.200]   that he probably couldn't get in.
[01:23:54.200 --> 01:23:56.480]   - Yeah, I just got an email from him.
[01:23:56.480 --> 01:23:59.560]   So Emily Bender, who was one of the stochastic parents,
[01:23:59.560 --> 01:24:01.520]   paper authors, co-authors,
[01:24:01.520 --> 01:24:03.240]   has a thread which is in there
[01:24:03.240 --> 01:24:05.560]   in which she's tearing apart all.
[01:24:05.560 --> 01:24:08.880]   She's amazing on her ongoing comments
[01:24:08.880 --> 01:24:12.560]   on the hype of AI.
[01:24:12.560 --> 01:24:15.120]   But she has been a naysayer.
[01:24:15.120 --> 01:24:16.120]   It's fairly-- - Oh, absolutely.
[01:24:16.120 --> 01:24:16.960]   - Yeah. - Absolutely.
[01:24:16.960 --> 01:24:19.320]   - Well, just the naysayer made me of the hype,
[01:24:19.320 --> 01:24:21.000]   made me of the over-promise.
[01:24:21.000 --> 01:24:24.640]   And so she-- - So this is her reaction
[01:24:24.640 --> 01:24:27.340]   to Sam's post. - This piece, yes.
[01:24:27.340 --> 01:24:30.840]   - From the get-go, this is just gross.
[01:24:30.840 --> 01:24:33.480]   They think they're really in the business
[01:24:33.480 --> 01:24:35.320]   of developing shaping AGI
[01:24:35.320 --> 01:24:37.000]   and they think are they are positioned
[01:24:37.000 --> 01:24:39.720]   to decide what benefits all of humanity.
[01:24:39.720 --> 01:24:41.240]   - Ding, ding, ding, ding.
[01:24:42.240 --> 01:24:46.240]   - Where's her-- - I don't understand.
[01:24:46.240 --> 01:24:48.280]   - That's the next one. - How does this Twitter word--
[01:24:48.280 --> 01:24:49.920]   - Get some of that. - That's not Twitter.
[01:24:49.920 --> 01:24:51.600]   It's masted on. - Sure, sure.
[01:24:51.600 --> 01:24:53.600]   - How does masted not turn next post?
[01:24:53.600 --> 01:24:54.760]   That's right there, where your--
[01:24:54.760 --> 01:24:56.520]   - That's turn next post. - Then Altman invites the reader
[01:24:56.520 --> 01:24:58.720]   to imagine that AGI have successfully created
[01:24:58.720 --> 01:25:00.200]   is literally magic.
[01:25:00.200 --> 01:25:04.000]   Also, what does turbo-charging the economy mean?
[01:25:04.000 --> 01:25:05.600]   If there's already abundance more dollars
[01:25:05.600 --> 01:25:07.760]   for the super-rich has to be.
[01:25:07.760 --> 01:25:10.760]   Also note the rhetorical sleight of hand here,
[01:25:10.760 --> 01:25:13.760]   paragraph one has AGI as a hypothetical,
[01:25:13.760 --> 01:25:15.280]   but by paragraph two, it's already something
[01:25:15.280 --> 01:25:17.160]   that has potential. (laughs)
[01:25:17.160 --> 01:25:21.600]   But oh no, the magical imagined AGI also has downsides,
[01:25:21.600 --> 01:25:24.520]   but it's also so, so tempting and important to create
[01:25:24.520 --> 01:25:26.840]   that we cannot not create it.
[01:25:26.840 --> 01:25:28.520]   Note the next rhetorical sleight of hand here.
[01:25:28.520 --> 01:25:32.760]   Now, AGI is an un-preventable future.
[01:25:32.760 --> 01:25:34.760]   - That's the essence of what she said.
[01:25:34.760 --> 01:25:35.760]   - It's a really good post.
[01:25:35.760 --> 01:25:37.720]   (sighs)
[01:25:37.720 --> 01:25:41.640]   I think I'm following Ms. Bender,
[01:25:41.640 --> 01:25:43.240]   but if not, I'm going following her.
[01:25:43.240 --> 01:25:44.400]   - She's brilliant.
[01:25:44.400 --> 01:25:46.760]   - Yeah, I'm pretty sure I am following her.
[01:25:46.760 --> 01:25:49.200]   - You know what the problem is, Masterdawn,
[01:25:49.200 --> 01:25:52.480]   you can't try to switch over from one page to another.
[01:25:52.480 --> 01:25:54.720]   - No, already following her.
[01:25:54.720 --> 01:25:57.720]   Well, if actually if I'd been following it as a follower,
[01:25:57.720 --> 01:26:01.800]   as opposed to in that form, I could see it a little bit better.
[01:26:01.800 --> 01:26:05.040]   But yeah, she's a professor of linguistics
[01:26:05.040 --> 01:26:07.920]   at UW University of Washington.
[01:26:07.920 --> 01:26:09.880]   She runs the Master of Science
[01:26:09.880 --> 01:26:12.600]   and Computational Linguistics Program.
[01:26:12.600 --> 01:26:14.160]   So she knows what she's talking about.
[01:26:14.160 --> 01:26:15.240]   - Oh yeah. - Yeah.
[01:26:15.240 --> 01:26:16.720]   - Yeah, again, she was co-author
[01:26:16.720 --> 01:26:20.360]   with Timna Gebru and Schmargert Schmitchell
[01:26:20.360 --> 01:26:21.680]   and one other who's name I forget,
[01:26:21.680 --> 01:26:23.960]   the fourth beetle, fifth beetle
[01:26:23.960 --> 01:26:25.920]   of the stochastic parents,
[01:26:25.920 --> 01:26:28.680]   - Right. - Bigger warning of this.
[01:26:28.680 --> 01:26:30.880]   - So yeah, I mean, this is a good question.
[01:26:30.880 --> 01:26:32.440]   I mean, talk about big tech.
[01:26:34.000 --> 01:26:38.360]   - It's kind of like a nuclear proliferation.
[01:26:38.360 --> 01:26:43.200]   We're in the era of AI proliferation.
[01:26:43.200 --> 01:26:46.920]   And there's nobody talking about disarmament.
[01:26:46.920 --> 01:26:48.440]   - But here too, to talk about,
[01:26:48.440 --> 01:26:49.680]   we must regulate this now.
[01:26:49.680 --> 01:26:50.920]   We don't know what it is.
[01:26:50.920 --> 01:26:52.400]   - Well, you can't, yeah, I guess you can't.
[01:26:52.400 --> 01:26:53.240]   But what do you do then?
[01:26:53.240 --> 01:26:54.080]   - That's right.
[01:26:54.080 --> 01:26:56.000]   - If you can't regulate it, what do you do?
[01:26:56.000 --> 01:26:59.440]   - You educate people, you be cautious,
[01:26:59.440 --> 01:27:00.960]   you keep people accountable,
[01:27:00.960 --> 01:27:03.160]   you do what Emily Bender is doing
[01:27:03.160 --> 01:27:06.840]   and mock them when they're going overboard.
[01:27:06.840 --> 01:27:09.280]   That's what society does.
[01:27:09.280 --> 01:27:11.440]   - I mean, but what is being cautious?
[01:27:11.440 --> 01:27:12.920]   'Cause there's gonna be a lot of companies
[01:27:12.920 --> 01:27:17.920]   that see the, the upside of getting into AI.
[01:27:17.920 --> 01:27:21.960]   And their bottom line is to make money.
[01:27:21.960 --> 01:27:24.360]   So like where, like how can we expect to--
[01:27:24.360 --> 01:27:25.760]   - Don't do what Microsoft did.
[01:27:25.760 --> 01:27:28.440]   - To the expectation to draw the line of what caution is
[01:27:28.440 --> 01:27:30.920]   because they're probably gonna push it.
[01:27:30.920 --> 01:27:34.200]   - Right, Microsoft threw caution to the wind,
[01:27:34.200 --> 01:27:38.200]   Google almost did, but then held back and said,
[01:27:38.200 --> 01:27:39.440]   no, we're the cautious ones.
[01:27:39.440 --> 01:27:40.520]   - Yeah, right.
[01:27:40.520 --> 01:27:43.880]   - Yeah, yeah.
[01:27:43.880 --> 01:27:47.920]   - So if we did an AI show, I don't know if we're gonna,
[01:27:47.920 --> 01:27:49.280]   we're not really in a position.
[01:27:49.280 --> 01:27:50.120]   - Please, please.
[01:27:50.120 --> 01:27:52.880]   - So launch new shows, but everybody wants to be on the show
[01:27:52.880 --> 01:27:54.880]   except for Ant and his left.
[01:27:54.880 --> 01:27:56.640]   'Cause nothing, no one would do it.
[01:27:56.640 --> 01:28:00.880]   It's not in my, no, but, but no, he's not gonna do it.
[01:28:00.880 --> 01:28:02.960]   - We're gonna do a musical theater show.
[01:28:02.960 --> 01:28:04.560]   - Oh gosh, no.
[01:28:04.560 --> 01:28:07.600]   - Yeah, Queensborough, there you go.
[01:28:07.600 --> 01:28:10.480]   If you pass it to the AI show, you have to do the next one
[01:28:10.480 --> 01:28:11.760]   and that is musical theater.
[01:28:11.760 --> 01:28:12.760]   So there you go.
[01:28:12.760 --> 01:28:16.440]   - Oh my gosh.
[01:28:16.440 --> 01:28:19.160]   - I fear that we would not have that much to,
[01:28:19.160 --> 01:28:20.160]   we'd have plenty to talk about,
[01:28:20.160 --> 01:28:22.000]   but it'll be highly speculative, right?
[01:28:22.000 --> 01:28:24.360]   - Speculation, that's all it is.
[01:28:24.360 --> 01:28:28.280]   Every week you're speculating on what could potentially happen.
[01:28:28.280 --> 01:28:30.040]   And I think that's gonna get old.
[01:28:30.040 --> 01:28:32.640]   - You got stupid coverage, you got stupid companies,
[01:28:32.640 --> 01:28:33.960]   you got smart things happening,
[01:28:33.960 --> 01:28:35.080]   you got great opportunities.
[01:28:35.080 --> 01:28:38.120]   I, we talked about this, so in our management program,
[01:28:38.120 --> 01:28:40.800]   we have 23 amazing high level managers
[01:28:40.800 --> 01:28:43.800]   in the executive program, I helped start at the school.
[01:28:43.800 --> 01:28:46.240]   We were talking about this just yesterday.
[01:28:46.240 --> 01:28:50.680]   I did a class on, on Mastronon and the Fediverse and this.
[01:28:50.680 --> 01:28:51.880]   And one of the issues that comes out,
[01:28:51.880 --> 01:28:53.720]   it's a very international group.
[01:28:53.720 --> 01:28:56.320]   They are using it for translation.
[01:28:56.320 --> 01:28:58.640]   They're using it to fix their work.
[01:28:58.640 --> 01:29:02.880]   It's making people who are shy about their English
[01:29:02.880 --> 01:29:05.240]   as a second language more confident.
[01:29:05.240 --> 01:29:07.360]   It's all these kinds of uses that you don't know
[01:29:07.360 --> 01:29:11.160]   people are putting it to and finding value in it.
[01:29:11.160 --> 01:29:14.760]   One person said she's using it to write grant proposals
[01:29:14.760 --> 01:29:16.840]   'cause it's all the same BS.
[01:29:16.840 --> 01:29:19.720]   So she just turns them out with it, right?
[01:29:19.720 --> 01:29:22.880]   There's interesting uses, there's interesting dangers,
[01:29:22.880 --> 01:29:24.400]   there's interesting development,
[01:29:24.400 --> 01:29:25.360]   there's interesting business,
[01:29:25.360 --> 01:29:27.000]   there's bad media coverage about it.
[01:29:27.000 --> 01:29:28.160]   Oh, it's rich.
[01:29:28.160 --> 01:29:32.280]   Let me put my Miss Lisa Laport had on right now.
[01:29:32.280 --> 01:29:35.640]   Okay, so we're gonna do this week in AI show.
[01:29:35.640 --> 01:29:37.800]   You're gonna be able to talk about this for a year,
[01:29:37.800 --> 01:29:39.960]   at least, you know, 52 episodes.
[01:29:39.960 --> 01:29:40.800]   Oh, yeah, oh yeah.
[01:29:40.800 --> 01:29:41.720]   You're gonna get 50 episodes out of this.
[01:29:41.720 --> 01:29:42.560]   Oh yeah, oh yeah.
[01:29:42.560 --> 01:29:43.400]   No, yeah.
[01:29:43.400 --> 01:29:44.560]   I feel like it might get six.
[01:29:44.560 --> 01:29:46.360]   I feel like we recorders attack me
[01:29:46.360 --> 01:29:48.760]   on a daily basis has something to do with AI.
[01:29:48.760 --> 01:29:49.600]   It's clearly something.
[01:29:49.600 --> 01:29:52.400]   And they all say the same thing, Mr. Howell.
[01:29:52.400 --> 01:29:55.480]   I have that same qualm, but my qualm is that this is just,
[01:29:56.560 --> 01:29:58.200]   it's kind of irrational exuberance
[01:29:58.200 --> 01:29:59.720]   in that we're gonna hit another AI winner.
[01:29:59.720 --> 01:30:00.800]   We've done this before.
[01:30:00.800 --> 01:30:03.600]   Many, many times, even in my memory.
[01:30:03.600 --> 01:30:05.520]   You know, we've got all three of the FTS
[01:30:05.520 --> 01:30:06.600]   and all that crap we do.
[01:30:06.600 --> 01:30:08.000]   Oh yeah, for sure.
[01:30:08.000 --> 01:30:10.640]   But even about AI, we've gotten all excited about it.
[01:30:10.640 --> 01:30:13.480]   We thought self-driving cars were just around the corner.
[01:30:13.480 --> 01:30:15.760]   We thought, this is not,
[01:30:15.760 --> 01:30:17.400]   and there've been many AI winners
[01:30:17.400 --> 01:30:21.600]   and I just would hate to start a show about something
[01:30:21.600 --> 01:30:23.560]   and then the AI winner happens.
[01:30:23.560 --> 01:30:25.440]   And we all go, yeah.
[01:30:25.440 --> 01:30:26.680]   It's about a temporary show.
[01:30:26.680 --> 01:30:28.160]   It would have to be temporary.
[01:30:28.160 --> 01:30:29.480]   It'd have to be just kind of a,
[01:30:29.480 --> 01:30:31.280]   I think if we did it, it would be in the club
[01:30:31.280 --> 01:30:34.480]   and it would be just kind of a chat like this.
[01:30:34.480 --> 01:30:36.280]   But I see, I do want to talk to experts.
[01:30:36.280 --> 01:30:37.840]   I would like that people like Emily Bender.
[01:30:37.840 --> 01:30:39.960]   There's lots of people who invite like Emily Bender.
[01:30:39.960 --> 01:30:41.520]   Like that young woman I sent you,
[01:30:41.520 --> 01:30:44.840]   whose name I can't remember Jason on TikTok.
[01:30:44.840 --> 01:30:46.240]   Whenever people send me young women,
[01:30:46.240 --> 01:30:47.480]   I have to send them back.
[01:30:47.480 --> 01:30:48.880]   It's kind of a legal thing.
[01:30:48.880 --> 01:30:53.520]   - If Stacy were here,
[01:30:53.520 --> 01:30:54.360]   he would talk to me.
[01:30:54.360 --> 01:30:56.360]   - Oh, it's a joke.
[01:30:56.360 --> 01:30:58.360]   - Oh God, it's out of here.
[01:30:58.360 --> 01:30:59.200]   - Oh God.
[01:30:59.200 --> 01:31:00.120]   - And one's deniability.
[01:31:00.120 --> 01:31:01.960]   Rachel Woods.
[01:31:01.960 --> 01:31:04.080]   - Is that the name of a young woman?
[01:31:04.080 --> 01:31:04.920]   Let's be clear.
[01:31:04.920 --> 01:31:05.760]   - Rachel Woods.
[01:31:05.760 --> 01:31:06.600]   - Okay.
[01:31:06.600 --> 01:31:08.000]   - Play a little bit of Rachel Woods by the way.
[01:31:08.000 --> 01:31:10.440]   - She's really good on this topic.
[01:31:10.440 --> 01:31:11.680]   She's on TikTok.
[01:31:11.680 --> 01:31:13.120]   - Oh, here you go.
[01:31:13.120 --> 01:31:18.120]   - Okay, you can't do it because it won't go on the thing.
[01:31:18.120 --> 01:31:21.120]   Rachel Woods takes talk.
[01:31:21.120 --> 01:31:23.920]   So she's a Tiktoker who explains AI.
[01:31:23.920 --> 01:31:26.400]   - Oh, she's an AI startup founder.
[01:31:26.400 --> 01:31:28.600]   Okay, so she has something.
[01:31:28.600 --> 01:31:29.440]   - I think, but you're better.
[01:31:29.440 --> 01:31:30.280]   - Yeah.
[01:31:30.280 --> 01:31:31.120]   So here we go.
[01:31:31.120 --> 01:31:32.680]   This is her most recent.
[01:31:32.680 --> 01:31:36.120]   Let me turn on the sound thing.
[01:31:36.120 --> 01:31:36.960]   Turn the machine.
[01:31:36.960 --> 01:31:38.600]   Turn on the machine.
[01:31:38.600 --> 01:31:40.200]   - Can you get some real intelligence
[01:31:40.200 --> 01:31:41.080]   to know how to turn on the sound?
[01:31:41.080 --> 01:31:45.120]   - Can you turn on the machine?
[01:31:45.120 --> 01:31:46.040]   There's a machine.
[01:31:46.040 --> 01:31:48.000]   I have to push that button.
[01:31:48.000 --> 01:31:50.800]   I said I have to push that button.
[01:31:50.800 --> 01:31:52.240]   - I forgot your screw was touching.
[01:31:52.240 --> 01:31:53.080]   - I forgot your screw was touching.
[01:31:53.080 --> 01:31:53.920]   - Here we go.
[01:31:53.920 --> 01:31:56.360]   - Chat GPT API was announced and people are going crazy.
[01:31:56.360 --> 01:31:59.480]   It's 10 times cheaper than OpenAI's previous model,
[01:31:59.480 --> 01:32:03.240]   which means we're about to have chat GPT in every product.
[01:32:03.240 --> 01:32:05.960]   They already announced it in a new grocery shopping assistant
[01:32:05.960 --> 01:32:08.160]   for Instacart, a shopping assistant
[01:32:08.160 --> 01:32:12.800]   for any Shopify store, a tutor on any subject in Quizlet
[01:32:12.800 --> 01:32:14.960]   and your own personal AI in Snapchat.
[01:32:14.960 --> 01:32:15.800]   - Holy cow.
[01:32:15.800 --> 01:32:17.040]   - There was a lot more in this announcement
[01:32:17.040 --> 01:32:19.120]   around privacy and restrictions
[01:32:19.120 --> 01:32:20.760]   of how you can use the API.
[01:32:20.760 --> 01:32:22.720]   We're gonna be covering all of this tomorrow in our newsletter.
[01:32:22.720 --> 01:32:23.960]   So drop your questions.
[01:32:23.960 --> 01:32:26.600]   If you're not already on it, the link is in my bio.
[01:32:26.600 --> 01:32:30.200]   - Wow, I think this is, she's great.
[01:32:30.200 --> 01:32:31.040]   - I lord for her a lot.
[01:32:31.040 --> 01:32:32.240]   - Yeah, yeah, yeah.
[01:32:32.240 --> 01:32:34.320]   And she's got a newsletter.
[01:32:34.320 --> 01:32:36.520]   So subscribe.
[01:32:36.520 --> 01:32:38.200]   Let me see if I can find the--
[01:32:38.200 --> 01:32:39.320]   - Actually, I needed that too.
[01:32:39.320 --> 01:32:40.160]   - Yeah.
[01:32:40.160 --> 01:32:43.280]   Let me find her link.
[01:32:43.280 --> 01:32:48.280]   Theaiexchange.com/theRachelwoods.
[01:32:50.120 --> 01:32:53.240]   And there's a link there to her newsletter.
[01:32:53.240 --> 01:32:54.920]   Interesting.
[01:32:54.920 --> 01:32:58.600]   All right, Jason can make that happen.
[01:32:58.600 --> 01:33:00.280]   Because you know what?
[01:33:00.280 --> 01:33:02.600]   Despite all appearances, he can think.
[01:33:02.600 --> 01:33:03.720]   - It's true, sometimes.
[01:33:03.720 --> 01:33:05.760]   - Despite all appearance.
[01:33:05.760 --> 01:33:09.040]   - I don't believe what you see here.
[01:33:09.040 --> 01:33:10.960]   - I just dazed her about this much.
[01:33:10.960 --> 01:33:12.280]   - No, Jason's a great producer.
[01:33:12.280 --> 01:33:13.120]   - The bottom of my forehead.
[01:33:13.120 --> 01:33:14.360]   - No, I'm teasing.
[01:33:14.360 --> 01:33:15.920]   He's a great producer and a great show.
[01:33:15.920 --> 01:33:19.800]   - Is Jason producing as he's hosting right now?
[01:33:19.800 --> 01:33:21.640]   - Yeah, I'm pretty excited.
[01:33:21.640 --> 01:33:22.480]   - It's amazing.
[01:33:22.480 --> 01:33:23.840]   - It's really impressive, I have to say.
[01:33:23.840 --> 01:33:24.680]   - It really is.
[01:33:24.680 --> 01:33:26.520]   - We are very lucky.
[01:33:26.520 --> 01:33:28.880]   Extremely lucky that Jason's with us.
[01:33:28.880 --> 01:33:30.720]   Also the host of All About Android,
[01:33:30.720 --> 01:33:32.040]   and we mentioned at the beginning of the show,
[01:33:32.040 --> 01:33:34.920]   gets great guests on there along with his regulars.
[01:33:34.920 --> 01:33:35.760]   - Yeah.
[01:33:35.760 --> 01:33:37.080]   - Ron and--
[01:33:37.080 --> 01:33:37.920]   - And when--
[01:33:37.920 --> 01:33:39.480]   - And last night we had Flow on.
[01:33:39.480 --> 01:33:40.320]   - Flow came back.
[01:33:40.320 --> 01:33:41.160]   - Nice.
[01:33:41.160 --> 01:33:42.000]   - Came back for review.
[01:33:42.000 --> 01:33:43.600]   - Michelle shows up for time to time.
[01:33:43.600 --> 01:33:45.080]   - Michelle Ramon, yes.
[01:33:45.080 --> 01:33:46.400]   - You got a great thrill to have him.
[01:33:46.400 --> 01:33:48.280]   - Pamela P. and J.R. Rayfield from Android.
[01:33:48.280 --> 01:33:49.120]   - I love J.R.
[01:33:49.120 --> 01:33:50.560]   - Yeah, it's fun.
[01:33:50.560 --> 01:33:51.680]   - We're having a good time.
[01:33:51.680 --> 01:33:53.440]   - He also has tech news weekly,
[01:33:53.440 --> 01:33:55.720]   and actually I thought of it,
[01:33:55.720 --> 01:33:58.280]   'cause we were talking about what you should do tomorrow.
[01:33:58.280 --> 01:33:59.120]   - Yes.
[01:33:59.120 --> 01:34:01.120]   - I'm still actively looking for my new product.
[01:34:01.120 --> 01:34:02.120]   - I thought of something.
[01:34:02.120 --> 01:34:03.320]   - I can't remember what it was.
[01:34:03.320 --> 01:34:04.560]   I should have written it down.
[01:34:04.560 --> 01:34:05.400]   - I hope you remember.
[01:34:05.400 --> 01:34:06.880]   - I know what is Mike McEw.
[01:34:06.880 --> 01:34:09.760]   - Oh, yes, this is a clipboard.
[01:34:09.760 --> 01:34:11.200]   - Yes, Mr. Flipboard.
[01:34:11.200 --> 01:34:12.800]   - Oh, yes, okay.
[01:34:12.800 --> 01:34:16.000]   - I hope because Flipboard has joined the Fediver.
[01:34:16.000 --> 01:34:17.480]   So Flipboard was originally created.
[01:34:17.480 --> 01:34:22.480]   Mike created it with the goal of being kind of a magazine
[01:34:22.480 --> 01:34:25.440]   based on people you follow on Twitter.
[01:34:25.440 --> 01:34:29.400]   Well, maybe that's not the best business model anymore.
[01:34:29.400 --> 01:34:31.740]   So they're moving kind of away from Twitter
[01:34:31.740 --> 01:34:33.360]   and over to Mastodon.
[01:34:33.360 --> 01:34:35.680]   They're even talking about creating their own Mastodon
[01:34:35.680 --> 01:34:37.120]   instance flipboard.s.
[01:34:37.120 --> 01:34:38.720]   - You did, you already did.
[01:34:38.720 --> 01:34:39.560]   - He did.
[01:34:39.560 --> 01:34:42.520]   - Uh, so this is really interesting.
[01:34:42.520 --> 01:34:47.040]   I mean, like a lot of people who have abandoned Twitter,
[01:34:47.040 --> 01:34:48.920]   but now if you've built a business around Twitter,
[01:34:48.920 --> 01:34:50.960]   you might be very slow to do that.
[01:34:50.960 --> 01:34:52.760]   I'm good for Mike.
[01:34:52.760 --> 01:34:56.080]   - Mike is very active on Medium on Mastodon.
[01:34:56.080 --> 01:34:59.440]   - At the same time, Tony Stablebine and Evan Williams
[01:34:59.440 --> 01:35:01.880]   have gone all in for Medium.
[01:35:01.880 --> 01:35:04.720]   And so the Medium instance,
[01:35:04.720 --> 01:35:08.800]   me.dm, pretty cool, me.dm is now up.
[01:35:08.800 --> 01:35:10.280]   And I'm getting all kinds of followers from it.
[01:35:10.280 --> 01:35:11.960]   People are joining that one as well.
[01:35:11.960 --> 01:35:13.080]   - Nice.
[01:35:13.080 --> 01:35:16.520]   So you've got Flipboard, you've got Medium.
[01:35:16.520 --> 01:35:18.360]   Who was it though that decided not to?
[01:35:18.360 --> 01:35:19.920]   The Financial Times or somebody?
[01:35:19.920 --> 01:35:21.040]   - Lefty, yeah.
[01:35:21.040 --> 01:35:22.280]   - They had one and they said, yeah,
[01:35:22.280 --> 01:35:23.280]   I mean, it's a bad idea.
[01:35:23.280 --> 01:35:25.600]   There's legal liabilities and all this stuff.
[01:35:25.600 --> 01:35:28.960]   So I, I for one, I'm not sure I,
[01:35:28.960 --> 01:35:31.960]   for AI, totally think companies like Medium and Flipboard
[01:35:31.960 --> 01:35:33.160]   should do this.
[01:35:33.160 --> 01:35:36.480]   I mean, I guess we're kind of in the same boat.
[01:35:36.480 --> 01:35:37.880]   I shouldn't really complain.
[01:35:37.880 --> 01:35:40.160]   'Cause Twitter has its own Mastodon instance.
[01:35:40.160 --> 01:35:42.200]   And it's made up of our listeners, right?
[01:35:42.200 --> 01:35:43.040]   Our fans.
[01:35:43.040 --> 01:35:43.880]   - Community.
[01:35:43.880 --> 01:35:44.720]   - It's a community.
[01:35:44.720 --> 01:35:45.640]   - Yeah, so it makes sense actually.
[01:35:45.640 --> 01:35:46.480]   Take it back.
[01:35:46.480 --> 01:35:48.600]   I was gonna say something bad about it.
[01:35:48.600 --> 01:35:53.600]   - Would you question both community Zars here?
[01:35:53.600 --> 01:35:55.560]   I was talking to somebody the other day about,
[01:35:55.560 --> 01:36:00.080]   about the need for outsourced moderating services
[01:36:00.080 --> 01:36:02.320]   for instance hosts.
[01:36:02.320 --> 01:36:04.200]   Is that something you think as you grow in scale,
[01:36:04.200 --> 01:36:05.880]   you might need or?
[01:36:05.880 --> 01:36:06.880]   - Well, we don't actually.
[01:36:06.880 --> 01:36:09.080]   - I'll put people too nice.
[01:36:09.080 --> 01:36:10.720]   - You mean moderators.
[01:36:10.720 --> 01:36:11.640]   - Yeah.
[01:36:11.640 --> 01:36:14.520]   - Well, so far I can't be moderating right now.
[01:36:14.520 --> 01:36:17.320]   - Yeah, I have an end to outsourced it.
[01:36:17.320 --> 01:36:19.640]   I think, just like our Discord,
[01:36:19.640 --> 01:36:22.760]   when you have a really tight community,
[01:36:22.760 --> 01:36:25.040]   and I have to approve you to go in there,
[01:36:25.040 --> 01:36:27.560]   and I'm pretty cautious.
[01:36:27.560 --> 01:36:28.920]   And by the way, if anybody acts out,
[01:36:28.920 --> 01:36:30.320]   I boot them immediately.
[01:36:30.320 --> 01:36:33.440]   So I think it doesn't take a lot of moderation.
[01:36:33.440 --> 01:36:36.800]   Every day, I make sure I check everything.
[01:36:36.800 --> 01:36:38.960]   - How long every day do you spend doing it?
[01:36:38.960 --> 01:36:40.600]   - Oh, an hour a day, not more.
[01:36:40.600 --> 01:36:41.440]   - Wow.
[01:36:41.440 --> 01:36:43.240]   - That's a commitment.
[01:36:43.240 --> 01:36:45.480]   - Well, and the one thing that everybody should know,
[01:36:45.480 --> 01:36:48.440]   if you're on Mastodon, is that when you see a post
[01:36:48.440 --> 01:36:51.560]   you don't like, or a series of posts from somebody you don't like,
[01:36:51.560 --> 01:36:53.280]   you have total control too.
[01:36:53.280 --> 01:36:55.840]   You can mute or block anybody.
[01:36:55.840 --> 01:37:00.840]   And by doing so, you're keeping them out of your feed forever.
[01:37:00.840 --> 01:37:01.680]   So--
[01:37:01.680 --> 01:37:02.520]   - And you can report them too.
[01:37:02.520 --> 01:37:04.360]   - And then I would suggest if it's somebody
[01:37:04.360 --> 01:37:06.960]   that you think is really toxic, report them to me,
[01:37:06.960 --> 01:37:07.920]   I check every day.
[01:37:07.920 --> 01:37:10.280]   We don't, we get at most a reported day.
[01:37:10.280 --> 01:37:12.440]   And usually it's not people on art,
[01:37:12.440 --> 01:37:16.320]   Mastodon instances, people on art, Mastodon instances.
[01:37:16.320 --> 01:37:20.560]   So I think it's actually sustainable.
[01:37:20.560 --> 01:37:23.320]   I spend an hour there, 'cause I'm reading stuff,
[01:37:23.320 --> 01:37:25.480]   not totally moderate.
[01:37:25.480 --> 01:37:27.240]   That's the total amount of time.
[01:37:27.240 --> 01:37:29.520]   So I have two, okay, I'll just show you real quickly
[01:37:29.520 --> 01:37:31.400]   the dashboard that I see.
[01:37:31.400 --> 01:37:34.360]   Two pending reports and eight pending users.
[01:37:34.360 --> 01:37:36.120]   I won't show you the reports or the users,
[01:37:36.120 --> 01:37:38.320]   but so I will review those every day.
[01:37:38.320 --> 01:37:39.840]   And if it's an account,
[01:37:41.240 --> 01:37:44.400]   I can suspend an account even if it's not on our instance.
[01:37:44.400 --> 01:37:47.240]   And that just means people on our instance can't see it.
[01:37:47.240 --> 01:37:49.160]   - So the account still exists, it's just,
[01:37:49.160 --> 01:37:50.000]   it's not acting.
[01:37:50.000 --> 01:37:53.080]   - So one of them is pretending to be,
[01:37:53.080 --> 01:37:56.200]   you can show this, the National Security Agency.
[01:37:56.200 --> 01:38:00.160]   And I don't know, I think that's probably a parody account.
[01:38:00.160 --> 01:38:01.560]   This one's a little tricky.
[01:38:01.560 --> 01:38:03.200]   I will look at it.
[01:38:03.200 --> 01:38:06.560]   - But it's verified, there's a blue check.
[01:38:06.560 --> 01:38:08.080]   (laughing)
[01:38:08.080 --> 01:38:09.560]   It also says your local--
[01:38:09.560 --> 01:38:11.160]   - I think that's from Zuckerberg.
[01:38:11.160 --> 01:38:12.920]   - It also says your local friendly
[01:38:12.920 --> 01:38:14.760]   Neighborhood Surveillance Agency.
[01:38:14.760 --> 01:38:16.080]   - Yeah.
[01:38:16.080 --> 01:38:18.520]   - I'm thinking it's probably a parody account.
[01:38:18.520 --> 01:38:20.400]   I'll look at it, if it's clearly parody,
[01:38:20.400 --> 01:38:21.720]   I won't block it, but--
[01:38:21.720 --> 01:38:24.280]   - You can use tabs that need 24/7 because we care.
[01:38:24.280 --> 01:38:25.920]   - Yeah.
[01:38:25.920 --> 01:38:28.920]   And then there's another one that you should not show
[01:38:28.920 --> 01:38:33.840]   that is kind of anti-trans.
[01:38:33.840 --> 01:38:36.680]   So that one, I'm just gonna press the suspend button.
[01:38:36.680 --> 01:38:38.440]   They get a notification that Twitter,
[01:38:38.440 --> 01:38:40.000]   social has suspend them.
[01:38:40.000 --> 01:38:41.240]   - Oh, they get a notification.
[01:38:41.240 --> 01:38:42.560]   - Oh.
[01:38:42.560 --> 01:38:44.160]   - You do it without notification?
[01:38:44.160 --> 01:38:46.520]   - No, yes, you can't.
[01:38:46.520 --> 01:38:48.320]   - They're going to get notified.
[01:38:48.320 --> 01:38:49.480]   - I think they get notified.
[01:38:49.480 --> 01:38:50.920]   I may be wrong on that, actually.
[01:38:50.920 --> 01:38:52.400]   I don't know.
[01:38:52.400 --> 01:38:55.120]   But what happens as a result is even,
[01:38:55.120 --> 01:38:57.280]   the only reason it showed up at Twitter.socials,
[01:38:57.280 --> 01:39:00.480]   'cause somebody in Twitter that social was following them,
[01:39:00.480 --> 01:39:03.080]   or maybe it's possible, no, no, it would only show up
[01:39:03.080 --> 01:39:04.600]   'cause somebody had followed them.
[01:39:04.600 --> 01:39:08.880]   So that person won't be able to see those posts,
[01:39:08.880 --> 01:39:10.480]   and no one else into it social will,
[01:39:10.480 --> 01:39:12.320]   nor will anybody able to follow them anymore.
[01:39:12.320 --> 01:39:15.080]   So that's pretty easy.
[01:39:15.080 --> 01:39:16.880]   - So I did it while we're on the air.
[01:39:16.880 --> 01:39:17.720]   - That's great.
[01:39:17.720 --> 01:39:19.400]   - It wasn't that hard.
[01:39:19.400 --> 01:39:21.520]   - We had Oig and Roch go on--
[01:39:21.520 --> 01:39:22.360]   - Oh, did you?
[01:39:22.360 --> 01:39:23.200]   - My Twitter summit.
[01:39:23.200 --> 01:39:24.040]   - Oh, yeah.
[01:39:24.040 --> 01:39:24.880]   - Nice.
[01:39:24.880 --> 01:39:25.200]   - I was Oig and through video-
[01:39:25.200 --> 01:39:26.680]   - He's the creator of Mastodize,
[01:39:26.680 --> 01:39:29.200]   the developer who invented Mastodon.
[01:39:29.200 --> 01:39:31.560]   - So he blew minds there.
[01:39:31.560 --> 01:39:35.000]   And my management students were blown by this as well.
[01:39:35.000 --> 01:39:37.560]   He said that, I don't think it's a problem to say,
[01:39:37.560 --> 01:39:39.120]   but in the life of Mastodon,
[01:39:39.120 --> 01:39:41.680]   writes from 2018 to today, five years,
[01:39:41.680 --> 01:39:45.120]   he said, maybe I've raised, including the latest Roch,
[01:39:45.120 --> 01:39:47.520]   maybe a total of $500,000.
[01:39:47.520 --> 01:39:50.720]   So he has this thing that is challenging Twitter
[01:39:50.720 --> 01:39:54.000]   that is presenting, he's an event activity pub,
[01:39:54.000 --> 01:39:58.240]   but he's making an activity pub come to life for more people.
[01:39:58.240 --> 01:40:02.760]   10 million accounts signed up of $500,000.
[01:40:02.760 --> 01:40:03.600]   It's nothing.
[01:40:03.600 --> 01:40:04.440]   - Yeah.
[01:40:04.440 --> 01:40:05.280]   - Wow.
[01:40:05.280 --> 01:40:06.600]   - And that's through a Patreon.
[01:40:06.600 --> 01:40:08.320]   He has a handful of developers.
[01:40:08.320 --> 01:40:10.440]   I know he hired a new developer,
[01:40:10.440 --> 01:40:11.440]   but for the longest time, it was just him.
[01:40:11.440 --> 01:40:12.360]   - Hire three people.
[01:40:12.360 --> 01:40:13.200]   - Yeah.
[01:40:13.200 --> 01:40:15.600]   - He got 1,700 applications for three and five.
[01:40:15.600 --> 01:40:18.880]   - The thing to remember is it's just one example
[01:40:18.880 --> 01:40:20.480]   of the Fediverse in activity pub.
[01:40:20.480 --> 01:40:23.760]   I always say this, but one thing that's become clear,
[01:40:23.760 --> 01:40:26.120]   we had Mike Masnick on on Sunday on Twitch.
[01:40:26.120 --> 01:40:30.440]   And he said, there are a number of really good competitors
[01:40:30.440 --> 01:40:32.560]   to Mastodon that aren't really competitors.
[01:40:32.560 --> 01:40:35.920]   They're just, they're similar projects that work
[01:40:35.920 --> 01:40:38.280]   on activity pub so you can follow somebody on Mastodon.
[01:40:38.280 --> 01:40:40.720]   - We also had Darius who started Home Down.
[01:40:40.720 --> 01:40:41.560]   - There you go.
[01:40:41.560 --> 01:40:42.400]   - Yeah.
[01:40:42.400 --> 01:40:43.960]   - And you know, he loves Mastodon.
[01:40:43.960 --> 01:40:45.640]   He's got nothing against Mastodon.
[01:40:45.640 --> 01:40:48.040]   He forked it because he wanted to do something differently.
[01:40:48.040 --> 01:40:50.160]   And it's a great fork.
[01:40:50.160 --> 01:40:52.120]   - I think more and more you're gonna see this.
[01:40:52.120 --> 01:40:54.880]   And I, for one, I think this is a really good,
[01:40:54.880 --> 01:41:01.520]   strong movement away from centralized social.
[01:41:01.520 --> 01:41:05.680]   You asked earlier about the future for Facebook.
[01:41:05.680 --> 01:41:07.480]   I mean, this is, I think the future is social.
[01:41:07.480 --> 01:41:08.960]   I love Discord too.
[01:41:08.960 --> 01:41:09.800]   - I think so.
[01:41:09.800 --> 01:41:11.160]   - Discords are a very good example.
[01:41:11.160 --> 01:41:12.960]   Boy, that was an eye opener when we created this
[01:41:12.960 --> 01:41:13.960]   in the club, Twit.
[01:41:13.960 --> 01:41:16.760]   Oh, this is a good, thank you, Mash Potato.
[01:41:16.760 --> 01:41:17.680]   - I love this diagram.
[01:41:17.680 --> 01:41:19.560]   - This is the tree of the Fediverse
[01:41:19.560 --> 01:41:20.960]   and it's actually out of date now.
[01:41:20.960 --> 01:41:25.960]   There are many, many more compatible projects going on.
[01:41:25.960 --> 01:41:30.360]   It means huge.
[01:41:30.360 --> 01:41:33.240]   So, you know, you can, and if you have a WordPress,
[01:41:33.240 --> 01:41:36.040]   you can have it be an equal partner on the Fediverse too.
[01:41:36.040 --> 01:41:37.720]   So, there's a plugin for that.
[01:41:37.720 --> 01:41:41.040]   So, I think more and more you're gonna see that.
[01:41:41.040 --> 01:41:44.080]   - And Mozilla.
[01:41:44.080 --> 01:41:48.040]   - Oh, by the way, take who, or yeah, take,
[01:41:48.040 --> 01:41:49.080]   it's short for quarter main.
[01:41:49.080 --> 01:41:50.640]   So, I wouldn't say true.
[01:41:50.640 --> 01:41:54.680]   They take who in our Discord, he's in our club, Twit,
[01:41:54.680 --> 01:41:57.360]   says he has been suspended from Mastodon by accident.
[01:41:57.360 --> 01:41:58.760]   And you do get a message.
[01:41:58.760 --> 01:42:01.520]   (laughing)
[01:42:01.520 --> 01:42:04.320]   And I un-is-expanded, you didn't I, okay, good.
[01:42:04.320 --> 01:42:06.840]   (laughing)
[01:42:06.840 --> 01:42:07.960]   You do get a message.
[01:42:07.960 --> 01:42:12.200]   Yeah, I'm really encouraged by all of that.
[01:42:12.200 --> 01:42:13.200]   I think this is--
[01:42:13.200 --> 01:42:14.040]   - Yeah, I learned it from you.
[01:42:14.040 --> 01:42:16.920]   And I'm very grateful 'cause it's giving me hope.
[01:42:16.920 --> 01:42:19.160]   - And is it safe to say, I think it is for me,
[01:42:19.160 --> 01:42:22.480]   but is it safe to say for all of you that Mastodon now
[01:42:22.480 --> 01:42:26.640]   kind of fills that itch, that Twitter itch, or no?
[01:42:26.640 --> 01:42:29.040]   - For me, not for me. - Not aunt.
[01:42:29.040 --> 01:42:31.720]   - Yeah, I've just noticed in general for myself,
[01:42:31.720 --> 01:42:36.400]   my desire to engage with a Twitter-like feed
[01:42:36.400 --> 01:42:39.760]   on a daily basis has dropped considerably in the last months.
[01:42:39.760 --> 01:42:41.440]   - Yeah, I still go to Twitter and when I see it,
[01:42:41.440 --> 01:42:42.560]   I go, I don't wanna hang out.
[01:42:42.560 --> 01:42:46.040]   - I open Twitter a handful of times a week.
[01:42:46.040 --> 01:42:48.600]   I open Mastodon slightly more than that,
[01:42:48.600 --> 01:42:51.200]   and sometimes I post, but I'm just not sharing a lot.
[01:42:51.200 --> 01:42:53.560]   And that's been a general trend for me lately.
[01:42:53.560 --> 01:42:55.240]   And sometimes I feel a little bad about that
[01:42:55.240 --> 01:42:57.600]   because of what I do, and being in touch
[01:42:57.600 --> 01:43:02.040]   with the community and everything, but yeah, I don't know.
[01:43:02.040 --> 01:43:05.520]   My desire to interact in that way has dropped a lot.
[01:43:05.520 --> 01:43:08.000]   And I don't know if it's just a result
[01:43:08.000 --> 01:43:11.080]   of the turbulence of recent months,
[01:43:11.080 --> 01:43:14.480]   or if it's just kind of shifting interest, but there we are.
[01:43:14.480 --> 01:43:16.160]   - Have you, we discussed this before,
[01:43:16.160 --> 01:43:19.720]   and you already know that I moved away
[01:43:19.720 --> 01:43:21.240]   from being an advisor to it.
[01:43:21.240 --> 01:43:22.760]   Have you guys tried post.news?
[01:43:22.760 --> 01:43:24.400]   Do you have any opinions about it?
[01:43:24.400 --> 01:43:25.880]   - I'm not, no, I haven't.
[01:43:25.880 --> 01:43:28.360]   - I'm not a fan only because it's an Andreessen Horowitz.
[01:43:28.360 --> 01:43:31.400]   - It's a, it has centralized B.
[01:43:31.400 --> 01:43:32.560]   - Yeah, I agree.
[01:43:32.560 --> 01:43:33.400]   - I feel similar.
[01:43:33.400 --> 01:43:34.240]   - Nobody's turned up.
[01:43:34.240 --> 01:43:36.400]   - Mr. Cowell. - Really, nobody's using it?
[01:43:36.400 --> 01:43:38.320]   - Well, no, no, that's not the case.
[01:43:38.320 --> 01:43:42.080]   People are, but I follow 54 people,
[01:43:42.080 --> 01:43:46.120]   and now when I go to my people I'm following,
[01:43:46.120 --> 01:43:48.040]   it's all George Conway, literally.
[01:43:48.040 --> 01:43:49.720]   - All four Conway and George Conway.
[01:43:49.720 --> 01:43:52.000]   - Okay, that's not what I want, so okay.
[01:43:52.000 --> 01:43:53.080]   (laughing)
[01:43:53.080 --> 01:43:54.080]   - I know.
[01:43:54.080 --> 01:43:55.160]   - I think a lot of people went there
[01:43:55.160 --> 01:43:56.280]   because it was like Twitter.
[01:43:56.280 --> 01:43:58.360]   It was centralized Twitter like place,
[01:43:58.360 --> 01:44:00.400]   and I think that people, it's sad
[01:44:00.400 --> 01:44:04.160]   because Taylor Lauren's popped in, briefly it masted on,
[01:44:04.160 --> 01:44:05.280]   didn't like what she saw and went,
[01:44:05.280 --> 01:44:06.120]   ran back to Twitter.
[01:44:06.120 --> 01:44:07.960]   I think a lot of people really want Twitter.
[01:44:07.960 --> 01:44:10.480]   And you were, you were about to say?
[01:44:10.480 --> 01:44:12.640]   - I'm fairly similar to Mr. Howe,
[01:44:12.640 --> 01:44:15.960]   where I just haven't really had the desire
[01:44:15.960 --> 01:44:19.520]   to go in there and check either platform recently.
[01:44:19.520 --> 01:44:22.040]   I'm pretty much in broadcast mode
[01:44:22.040 --> 01:44:23.720]   and pretty much in campaign mode,
[01:44:23.720 --> 01:44:25.480]   in regard to my son.
[01:44:25.480 --> 01:44:27.920]   Every now and then, usually like on a Friday night
[01:44:27.920 --> 01:44:29.320]   or a Saturday, I'll go in there
[01:44:29.320 --> 01:44:32.960]   and check notifications from people that I know,
[01:44:32.960 --> 01:44:34.640]   just to make sure I didn't miss anything.
[01:44:34.640 --> 01:44:38.480]   But now, even they know that he's not really in here that often,
[01:44:38.480 --> 01:44:41.040]   so you may not get a reply quickly.
[01:44:41.040 --> 01:44:44.720]   It's just, social media in general
[01:44:44.720 --> 01:44:47.560]   has just been a bit of a craptastic mess,
[01:44:47.560 --> 01:44:51.280]   and I just try to keep my energy clean
[01:44:51.280 --> 01:44:53.120]   and happy, if you will.
[01:44:53.120 --> 01:44:55.240]   - I know, every time I'm tempted to post on Twitter,
[01:44:55.240 --> 01:44:57.720]   fortunately, I could,
[01:44:57.720 --> 01:45:00.080]   but I don't wanna break my silence.
[01:45:00.080 --> 01:45:03.120]   Every time though, it's because somebody got me angry.
[01:45:03.120 --> 01:45:05.120]   - Right, that's the thing, I'm like,
[01:45:05.120 --> 01:45:06.560]   "I wanna be learning one thing."
[01:45:06.560 --> 01:45:07.400]   - It's nice.
[01:45:07.400 --> 01:45:09.000]   You just open A that got a word.
[01:45:09.000 --> 01:45:10.680]   - We had voices.
[01:45:10.680 --> 01:45:12.640]   It's great that we had the voice to be out,
[01:45:12.640 --> 01:45:15.160]   to be able to go out there and just scream,
[01:45:15.160 --> 01:45:16.640]   that something has upset us
[01:45:16.640 --> 01:45:18.160]   and where they're being in justice
[01:45:18.160 --> 01:45:20.520]   or something really, really small.
[01:45:20.520 --> 01:45:22.960]   But, usually, it seems like I'm seeing
[01:45:22.960 --> 01:45:24.680]   a lot of that stuff out there,
[01:45:24.680 --> 01:45:25.920]   and it just brings me down.
[01:45:25.920 --> 01:45:27.560]   I'm like, "Nah, I'll be a step boy."
[01:45:27.560 --> 01:45:30.440]   Let me go turn on some old mama's family reruns
[01:45:30.440 --> 01:45:32.080]   and why I have for the rest of the day.
[01:45:32.080 --> 01:45:33.680]   - I'd say in the last couple of years,
[01:45:33.680 --> 01:45:37.640]   time and time again, when I open a feed like Twitter,
[01:45:37.640 --> 01:45:39.040]   or sometimes master it on,
[01:45:39.040 --> 01:45:40.080]   although less so am I master it on,
[01:45:40.080 --> 01:45:41.680]   master it on, for whatever reason,
[01:45:41.680 --> 01:45:44.720]   feels like a little bit more open
[01:45:44.720 --> 01:45:47.560]   and accepting to me right now than the Twitter universe does.
[01:45:47.560 --> 01:45:49.480]   But, in the last couple of years,
[01:45:49.480 --> 01:45:51.400]   I have many times had the experience
[01:45:51.400 --> 01:45:52.840]   where I open up my app,
[01:45:52.840 --> 01:45:54.080]   like, "Okay, I got this thing.
[01:45:54.080 --> 01:45:55.240]   "I gotta put this out there."
[01:45:55.240 --> 01:45:58.160]   I type it all up, I see it on the screen,
[01:45:58.160 --> 01:45:59.440]   and then I hit delete.
[01:45:59.440 --> 01:46:01.080]   It's like I don't even send it.
[01:46:01.080 --> 01:46:03.760]   'Cause I'm like, at a certain point, by the end of it,
[01:46:03.760 --> 01:46:06.480]   I'm like, "Is it even worth it for me to put this out there?"
[01:46:06.480 --> 01:46:10.440]   Because, on one hand, I already did the thing
[01:46:10.440 --> 01:46:12.000]   that I was moved to do,
[01:46:12.000 --> 01:46:14.800]   which was to put my thoughts into words.
[01:46:14.800 --> 01:46:18.240]   But, if I hit send, so often it feels like
[01:46:18.240 --> 01:46:23.240]   all I'm doing is inviting some sort of reaction.
[01:46:23.240 --> 01:46:26.600]   And, I hope that that reaction is good,
[01:46:26.600 --> 01:46:29.280]   or I hope that people interpret my words
[01:46:29.280 --> 01:46:31.200]   in the way that I intend for them to,
[01:46:31.200 --> 01:46:33.040]   but then there's always that doubt that's there,
[01:46:33.040 --> 01:46:35.800]   and that's been keeping me from sharing a lot
[01:46:35.800 --> 01:46:37.440]   on social media lately.
[01:46:37.440 --> 01:46:40.200]   And it happens to me a lot.
[01:46:40.200 --> 01:46:42.560]   Again, I'm really grateful for the community
[01:46:42.560 --> 01:46:44.840]   that I've been able to build on Twitter,
[01:46:44.840 --> 01:46:48.480]   and as well as to Twitter, that social platform here,
[01:46:48.480 --> 01:46:51.640]   'cause I'm looking right now at Michael Kidd
[01:46:51.640 --> 01:46:54.880]   that gave me all of this great information about NAS
[01:46:54.880 --> 01:46:55.840]   that I totally forgot about.
[01:46:55.840 --> 01:46:57.040]   I forgot about True NAS.
[01:46:57.040 --> 01:47:01.800]   And, it's just this long thread of just useful information.
[01:47:01.800 --> 01:47:04.280]   He's not yelling at me, he's not belittling me,
[01:47:04.280 --> 01:47:06.560]   he's not telling me I should've bought this system
[01:47:06.560 --> 01:47:09.440]   versus that system, and that's how this stuff
[01:47:09.440 --> 01:47:10.440]   is supposed to be.
[01:47:10.440 --> 01:47:15.440]   It's just gotten so far-fetched with people just yelling
[01:47:15.440 --> 01:47:17.800]   and fussing about any and everything,
[01:47:17.800 --> 01:47:19.160]   and that's why I just stay awake
[01:47:19.160 --> 01:47:22.680]   'cause I just don't wanna pollute the good energy here
[01:47:22.680 --> 01:47:24.640]   in the four walls of pretty fast stuff.
[01:47:24.640 --> 01:47:28.480]   - I also wonder, especially for you, Jason,
[01:47:28.480 --> 01:47:29.800]   'cause you've been doing this for a long time,
[01:47:29.800 --> 01:47:31.760]   almost as long as I have.
[01:47:31.760 --> 01:47:33.400]   If you're not just tired as I am,
[01:47:33.400 --> 01:47:34.800]   tired of sticking your head out,
[01:47:34.800 --> 01:47:37.800]   and just say, "I wanna be a hermit."
[01:47:37.800 --> 01:47:38.640]   Are you ready?
[01:47:38.640 --> 01:47:39.960]   - Yes, absolutely.
[01:47:39.960 --> 01:47:40.800]   - You feel that way?
[01:47:40.800 --> 01:47:41.920]   - Break up a little small room,
[01:47:41.920 --> 01:47:43.040]   break it up and ask,
[01:47:43.040 --> 01:47:45.000]   have people push food over the top to you?
[01:47:45.000 --> 01:47:46.000]   That's 'cause I am.
[01:47:46.000 --> 01:47:47.520]   (laughing)
[01:47:47.520 --> 01:47:48.920]   - I am doing that.
[01:47:48.920 --> 01:47:50.840]   - Even I get that, what's it like?
[01:47:50.840 --> 01:47:52.480]   - I haven't been doing this as long as you are,
[01:47:52.480 --> 01:47:55.960]   but I enjoy just the solitude from time to time.
[01:47:55.960 --> 01:47:57.520]   - Yeah, yeah.
[01:47:57.520 --> 01:48:00.120]   - Well, so much of our iPhone.
[01:48:00.120 --> 01:48:00.960]   - Sorry.
[01:48:00.960 --> 01:48:01.800]   - Go ahead.
[01:48:01.800 --> 01:48:03.360]   - Well, I'm just gonna say so much of my career
[01:48:03.360 --> 01:48:08.240]   has been sharing and putting my thoughts and my words
[01:48:08.240 --> 01:48:10.920]   and my feelings and everything out there.
[01:48:10.920 --> 01:48:13.000]   And I think you're right to a certain degree,
[01:48:13.000 --> 01:48:15.680]   though I absolutely feel that from time to time
[01:48:15.680 --> 01:48:16.920]   where it's like, you know,
[01:48:16.920 --> 01:48:19.640]   there's a private citizen that wants a simpler,
[01:48:19.640 --> 01:48:22.880]   sometimes I just want a simpler, a normal.
[01:48:22.880 --> 01:48:25.240]   And sometimes that doesn't involve sharing
[01:48:25.240 --> 01:48:27.120]   every piece of my life, which--
[01:48:27.120 --> 01:48:27.960]   - Amen.
[01:48:27.960 --> 01:48:31.480]   - I don't know, but I also feel bad admitting
[01:48:31.480 --> 01:48:34.520]   that on the show because so much of my career
[01:48:34.520 --> 01:48:36.720]   is built around being public
[01:48:36.720 --> 01:48:38.560]   and being kind of in that role.
[01:48:38.560 --> 01:48:42.400]   So then I feel like I'm kind of like retracting myself
[01:48:42.400 --> 01:48:46.560]   from this community that largely is incredibly welcoming,
[01:48:46.560 --> 01:48:48.560]   but it's just that one little portion
[01:48:48.560 --> 01:48:51.320]   that can be negative enough that it's like,
[01:48:51.320 --> 01:48:53.360]   "Well, why would I invite that negativity?
[01:48:53.360 --> 01:48:55.760]   "Why not just not have it?"
[01:48:55.760 --> 01:48:56.920]   - You know?
[01:48:56.920 --> 01:48:58.240]   - It's such a--
[01:48:58.240 --> 01:49:00.560]   - I don't wanna be ungrateful
[01:49:00.560 --> 01:49:03.400]   'cause it's such a privilege to get to do what we do.
[01:49:03.400 --> 01:49:05.240]   - 100% I totally agree.
[01:49:05.240 --> 01:49:10.240]   And honestly, to me, what more and more it feels like
[01:49:10.240 --> 01:49:15.960]   what we do is a conversation with, you know,
[01:49:15.960 --> 01:49:20.520]   our friends, the hosts, but also with our community.
[01:49:20.520 --> 01:49:22.720]   And even though I wish the community all had a C
[01:49:22.720 --> 01:49:25.440]   at the table 'cause it would be great if we could do that.
[01:49:25.440 --> 01:49:27.240]   But we do as much as we can for that.
[01:49:27.240 --> 01:49:29.120]   We have a stage open now all the time.
[01:49:29.120 --> 01:49:31.400]   We have the IRC open all the time.
[01:49:31.400 --> 01:49:34.160]   I read the Mastodon, I read the Twit forms.
[01:49:34.160 --> 01:49:35.280]   I know you all do too.
[01:49:35.280 --> 01:49:40.040]   So I feel like we're actually kind of privileged
[01:49:40.040 --> 01:49:41.200]   to be in a conversation.
[01:49:41.200 --> 01:49:42.680]   And I don't mind the conversation.
[01:49:42.680 --> 01:49:44.440]   I like the conversation.
[01:49:44.440 --> 01:49:45.280]   What I don't like--
[01:49:45.280 --> 01:49:46.280]   - In the community.
[01:49:46.280 --> 01:49:47.120]   - In the community.
[01:49:47.120 --> 01:49:50.560]   - What I don't like is the general performative,
[01:49:50.560 --> 01:49:52.120]   well, basically it's what tweeting is,
[01:49:52.120 --> 01:49:55.360]   which is shouting out to the world.
[01:49:55.360 --> 01:49:57.520]   I don't think what I have to say is that important.
[01:49:57.520 --> 01:49:58.360]   - One thing.
[01:49:58.360 --> 01:50:01.240]   - I think it's a risk of big old mass media
[01:50:01.240 --> 01:50:03.600]   where scale became the goal
[01:50:03.600 --> 01:50:05.640]   and performing for the whole world became the goal.
[01:50:05.640 --> 01:50:06.720]   - Well, and this is something I learned
[01:50:06.720 --> 01:50:08.040]   from the Black Twitter event
[01:50:08.040 --> 01:50:10.760]   is what special about Black Twitter is not
[01:50:10.760 --> 01:50:12.480]   even the movements that mattered,
[01:50:12.480 --> 01:50:14.520]   like Black Lives Matter, which mattered greatly,
[01:50:14.520 --> 01:50:18.000]   but it's the, what I learned in the room that day
[01:50:18.000 --> 01:50:21.640]   is the value of the community for the community's own sake.
[01:50:21.640 --> 01:50:24.000]   To have joy and sorrow and everyday life there
[01:50:24.000 --> 01:50:26.200]   and to feel a place of comfort.
[01:50:26.200 --> 01:50:27.400]   And it's not one community.
[01:50:27.400 --> 01:50:28.680]   It's a bunch of communities.
[01:50:28.680 --> 01:50:29.880]   It's some people that don't like each other
[01:50:29.880 --> 01:50:31.320]   that came out too.
[01:50:31.320 --> 01:50:32.680]   But that's so different from,
[01:50:32.680 --> 01:50:34.240]   I gotta speak to millions of people.
[01:50:34.240 --> 01:50:36.040]   - That's a completely different experience.
[01:50:36.040 --> 01:50:37.400]   Yes. - The internet for this,
[01:50:37.400 --> 01:50:39.080]   but it's mass mediated.
[01:50:39.080 --> 01:50:43.080]   My favorite stat in the Gutenberg premises coming out in June,
[01:50:43.080 --> 01:50:44.360]   pre-orders available now,
[01:50:44.360 --> 01:50:50.400]   is that before the mechanization of print
[01:50:50.400 --> 01:50:53.960]   with steam-powered presses and the line of type,
[01:50:53.960 --> 01:50:55.880]   the average circulation of a daily newspaper
[01:50:55.880 --> 01:50:57.480]   in the United States was 4,000.
[01:50:57.480 --> 01:51:01.800]   - Which is just about the average number
[01:51:01.800 --> 01:51:03.440]   of active people and Twit.s,
[01:51:03.440 --> 01:51:05.680]   socials, messed it on its list.
[01:51:05.680 --> 01:51:07.320]   - It's almost exactly the same number.
[01:51:07.320 --> 01:51:08.240]   - Yeah. - Right, right.
[01:51:08.240 --> 01:51:10.320]   It's like a Dunbar number.
[01:51:10.320 --> 01:51:11.400]   - That's really interesting.
[01:51:11.400 --> 01:51:12.800]   - So it is. - That's the right,
[01:51:12.800 --> 01:51:14.600]   in other words, that's kind of the right number.
[01:51:14.600 --> 01:51:15.440]   - Yeah. - Yeah.
[01:51:15.440 --> 01:51:17.920]   - 4,000 for active users.
[01:51:17.920 --> 01:51:19.360]   - So it's exactly that number.
[01:51:19.360 --> 01:51:21.200]   - Wow. - That's phenomenal.
[01:51:21.200 --> 01:51:24.360]   - So Leo, have you, as the iPhone user among us,
[01:51:24.360 --> 01:51:26.920]   unless Ant's gonna get drawn into,
[01:51:26.920 --> 01:51:28.280]   further into the Apple World.
[01:51:28.280 --> 01:51:29.280]   - I know, he could.
[01:51:29.280 --> 01:51:30.120]   - He could.
[01:51:30.120 --> 01:51:34.080]   - He could say, "Oh, I do not like iOS.
[01:51:34.080 --> 01:51:35.640]   "Do not like iOS."
[01:51:35.640 --> 01:51:37.800]   - Stay there, stay there Ant.
[01:51:37.800 --> 01:51:41.200]   Leo, have you signed up for the Blue Sky beta?
[01:51:41.200 --> 01:51:42.960]   - Oh, let's talk about it.
[01:51:42.960 --> 01:51:44.240]   I haven't been invited. - I should wouldn't have
[01:51:44.240 --> 01:51:45.080]   thought about it. - I haven't been invited.
[01:51:45.080 --> 01:51:46.480]   - But you've signed up for it.
[01:51:46.480 --> 01:51:47.880]   - You can sign up to be invited,
[01:51:47.880 --> 01:51:48.720]   but I don't know. - You can sign up
[01:51:48.720 --> 01:51:50.600]   to be invited, I think, right?
[01:51:50.600 --> 01:51:51.680]   - Right, and you did that. - And I didn't sign up
[01:51:51.680 --> 01:51:53.920]   to be invited ages ago. - We did, okay.
[01:51:53.920 --> 01:51:55.720]   - But I don't think I've been invited.
[01:51:55.720 --> 01:51:59.920]   So this is Jack Dorsey's last act as CEO of Twitter.
[01:51:59.920 --> 01:52:04.200]   He funded, I think he put $10 million
[01:52:04.200 --> 01:52:09.200]   into a research effort to come up with basically Mastodon,
[01:52:09.200 --> 01:52:11.760]   let's be honest, or the Fediverse.
[01:52:11.760 --> 01:52:12.600]   - Protocol.
[01:52:12.600 --> 01:52:15.560]   - A protocol based open Twitter.
[01:52:15.560 --> 01:52:19.360]   It was money that he put in that even when Elon took over,
[01:52:19.360 --> 01:52:21.200]   Elon could not take back.
[01:52:21.200 --> 01:52:23.960]   So the Blue Sky has continued, I'm sure Elon's not thrilled
[01:52:23.960 --> 01:52:25.840]   about it, he didn't like Mastodon either.
[01:52:25.840 --> 01:52:28.800]   But it is now close to being released.
[01:52:28.800 --> 01:52:31.840]   It's an invite only beta in the App Store.
[01:52:31.840 --> 01:52:36.280]   So yeah, you know what I guess I should go back in
[01:52:36.280 --> 01:52:38.640]   and join it, but honestly, I feel like it's solving
[01:52:38.640 --> 01:52:42.240]   a problem that doesn't exist, which is trying to create
[01:52:42.240 --> 01:52:44.760]   a Fediverse, but a Fediverse already exists.
[01:52:44.760 --> 01:52:46.080]   So do we need to do-- - Well, the question is
[01:52:46.080 --> 01:52:49.560]   whether or not it federates with the Fediverse.
[01:52:49.560 --> 01:52:51.840]   - Well, right now they have their own protocol,
[01:52:51.840 --> 01:52:53.880]   which is very similar to Activity Pub.
[01:52:53.880 --> 01:52:57.640]   - It's like Scuttlebutt, where we had Rabble on,
[01:52:57.640 --> 01:52:59.600]   who Bumble is also at the likelihood event.
[01:52:59.600 --> 01:53:00.520]   - Oh, nice. - He came all the way
[01:53:00.520 --> 01:53:01.360]   from New Zealand for it.
[01:53:01.360 --> 01:53:02.280]   - Oh, that's wonderful. - And a wonderful,
[01:53:02.280 --> 01:53:05.440]   Lane Cook. - Well, did you record this event?
[01:53:05.440 --> 01:53:07.200]   - No, that's a sore point.
[01:53:07.200 --> 01:53:10.160]   - I think this is a shame,
[01:53:10.160 --> 01:53:13.080]   'cause this is something that I think the world should hear.
[01:53:13.080 --> 01:53:14.400]   It sounds like it was an amazing event.
[01:53:14.400 --> 01:53:16.600]   - I couldn't agree more, I was told not to.
[01:53:16.600 --> 01:53:19.840]   Not by the group, not by the group, by the boss.
[01:53:19.840 --> 01:53:24.680]   But anyway, we have a report on it coming out very soon,
[01:53:24.680 --> 01:53:26.160]   and it was an amazing event.
[01:53:27.000 --> 01:53:30.760]   But I think it's too soon to say that Activity Pub
[01:53:30.760 --> 01:53:33.760]   or Master Hunter or Home Town or anything is it.
[01:53:33.760 --> 01:53:36.160]   I think that if Jack comes along and invents,
[01:53:36.160 --> 01:53:39.960]   and I'm suddenly forgetting her name,
[01:53:39.960 --> 01:53:42.920]   the head of a blue sky, Jay Raber,
[01:53:42.920 --> 01:53:47.200]   invent new things, the world benefits, right?
[01:53:47.200 --> 01:53:49.400]   It's the same with Scuttlebutt.
[01:53:49.400 --> 01:53:52.680]   It becomes people find more good ideas
[01:53:52.680 --> 01:53:54.680]   in that kind of open environment.
[01:53:54.680 --> 01:53:56.400]   - Well, I haven't downloaded it.
[01:53:56.400 --> 01:53:58.960]   So we'll download it. - You can?
[01:53:58.960 --> 01:54:00.320]   - Well, let's see.
[01:54:00.320 --> 01:54:01.920]   It's in the App Store, which means you can,
[01:54:01.920 --> 01:54:04.360]   but you might have to-- - Might have to code
[01:54:04.360 --> 01:54:06.520]   in order to log in or whatever.
[01:54:06.520 --> 01:54:08.520]   - Blue Sky mobile, is that it?
[01:54:08.520 --> 01:54:10.560]   No, that's a staffing service.
[01:54:10.560 --> 01:54:12.920]   Blue Sky VPN, no, that's a VPN.
[01:54:12.920 --> 01:54:15.200]   This is the problem.
[01:54:15.200 --> 01:54:17.280]   Blue Sky Social, see what's next.
[01:54:17.280 --> 01:54:18.720]   That sounds right.
[01:54:18.720 --> 01:54:20.480]   So let me download that.
[01:54:20.480 --> 01:54:22.680]   - Orca's China. - Yeah, it's China.
[01:54:22.680 --> 01:54:24.880]   This is, by the way, a big problem on the App Store,
[01:54:24.880 --> 01:54:26.280]   but I think that is it.
[01:54:26.280 --> 01:54:27.960]   - Yeah, Blue Sky Social, that's it.
[01:54:27.960 --> 01:54:30.480]   - Yeah, let's open it and see what's happening.
[01:54:30.480 --> 01:54:33.840]   Private beta, create a new account, sign in.
[01:54:33.840 --> 01:54:38.600]   Well, I don't, I guess I could create a new account.
[01:54:38.600 --> 01:54:39.600]   - Create a new account?
[01:54:39.600 --> 01:54:40.920]   - Try to.
[01:54:40.920 --> 01:54:43.880]   - Yeah, how do I get back?
[01:54:43.880 --> 01:54:46.840]   - It says, "private beta, the app is available for download,
[01:54:46.840 --> 01:54:49.320]   but you will need an invite code to create an account.
[01:54:49.320 --> 01:54:50.160]   - Yes, can invite code.
[01:54:50.160 --> 01:54:52.400]   - Let's ask it for an invite code.
[01:54:52.400 --> 01:54:55.760]   Which I did way back in the day.
[01:54:55.760 --> 01:54:58.960]   I just, I feel like we shouldn't dilute our efforts
[01:54:58.960 --> 01:54:59.800]   at this point.
[01:54:59.800 --> 01:55:00.640]   We've got activity pub.
[01:55:00.640 --> 01:55:03.200]   Nobody's saying activity pub isn't good.
[01:55:03.200 --> 01:55:05.680]   - Yeah, I kind of agree, but then again,
[01:55:05.680 --> 01:55:09.360]   when we talk to Rebel about what he's doing with Scuttlebutt
[01:55:09.360 --> 01:55:12.080]   and planetary, there were all kinds of new ideas
[01:55:12.080 --> 01:55:13.400]   that were entirely different.
[01:55:13.400 --> 01:55:15.360]   - Yeah, and I think that's worthwhile.
[01:55:15.360 --> 01:55:17.760]   - I agree, Rebel had, for instance,
[01:55:17.760 --> 01:55:21.480]   the idea of this portability, that your identity wasn't tied,
[01:55:21.480 --> 01:55:24.240]   you know, you had your own public key identity,
[01:55:24.240 --> 01:55:26.720]   public private key identity, which you could take with you.
[01:55:26.720 --> 01:55:29.600]   - And also with the Maori, that it works offline
[01:55:29.600 --> 01:55:32.280]   in a closed network.
[01:55:32.280 --> 01:55:37.760]   No, I think there's lots of room for development right now.
[01:55:37.760 --> 01:55:41.240]   - So, yeah, I wonder how I would get an invite.
[01:55:41.240 --> 01:55:44.880]   Well, if you're listening, I'd like a new fight.
[01:55:44.880 --> 01:55:46.440]   So would we all.
[01:55:46.440 --> 01:55:49.120]   Meanwhile, I'd also like an app for Android, just saying.
[01:55:49.120 --> 01:55:50.120]   - Yeah.
[01:55:50.120 --> 01:55:51.560]   - Hey, everybody, Leo LePorte here.
[01:55:51.560 --> 01:55:53.760]   I am the founder and one of the hosts
[01:55:53.760 --> 01:55:56.720]   at the Twit Podcast Network.
[01:55:56.720 --> 01:55:59.320]   I wanna talk to you a little bit about what we do here at Twit,
[01:55:59.320 --> 01:56:01.240]   'cause I think it's unique.
[01:56:01.240 --> 01:56:06.240]   And I think for anybody who is bringing a product
[01:56:06.240 --> 01:56:08.920]   or a service to a tech audience,
[01:56:08.920 --> 01:56:12.000]   you need to know about what we do here at Twit.
[01:56:12.000 --> 01:56:13.880]   We've built an amazing audience
[01:56:13.880 --> 01:56:17.720]   of engaged, intelligent, affluent listeners
[01:56:17.720 --> 01:56:20.000]   who listen to us and trust us
[01:56:20.000 --> 01:56:22.160]   when we recommend a product.
[01:56:22.160 --> 01:56:24.200]   Our mission statement is Twit is to build
[01:56:24.200 --> 01:56:27.800]   a highly engaged community of tech enthusiasts.
[01:56:27.800 --> 01:56:30.960]   Well, already your ears should be perking up at that
[01:56:30.960 --> 01:56:33.840]   because highly engaged is good for you.
[01:56:33.840 --> 01:56:36.000]   Tech enthusiasts, if that's who you're looking for,
[01:56:36.000 --> 01:56:37.280]   this is the place.
[01:56:37.280 --> 01:56:39.800]   We do it by offering them the knowledge they need
[01:56:39.800 --> 01:56:42.440]   to understand and use technology in today's world.
[01:56:42.440 --> 01:56:45.040]   And I hear from our audience all the time,
[01:56:45.040 --> 01:56:48.080]   part of that knowledge comes from our advertisers.
[01:56:48.080 --> 01:56:49.080]   We are very careful.
[01:56:49.080 --> 01:56:52.080]   We pick advertisers with great products,
[01:56:52.080 --> 01:56:54.360]   great services with integrity,
[01:56:54.360 --> 01:56:58.840]   and introduce them to our audience with authenticity
[01:56:58.840 --> 01:57:01.280]   and genuine enthusiasm.
[01:57:01.280 --> 01:57:03.000]   And that makes our host Red Ads
[01:57:03.000 --> 01:57:05.400]   different from anything else you can buy.
[01:57:05.400 --> 01:57:10.160]   We are literally bringing you to the attention of our audience
[01:57:10.160 --> 01:57:13.560]   and giving you a big, fat endorsement.
[01:57:13.560 --> 01:57:16.520]   We like to create partnerships with trusted brands.
[01:57:16.520 --> 01:57:18.680]   Brands who are in it for the long run,
[01:57:18.680 --> 01:57:22.280]   long-term partners that want to grow with us.
[01:57:22.280 --> 01:57:24.800]   And we have so many great success stories.
[01:57:24.800 --> 01:57:28.720]   Tim Broome, who founded ITProTV in 2013,
[01:57:28.720 --> 01:57:30.800]   started advertising with us on day one,
[01:57:30.800 --> 01:57:32.800]   has been with us ever since.
[01:57:32.800 --> 01:57:36.200]   He said, quote, "We would not be where we are today
[01:57:36.200 --> 01:57:37.840]   "without the Twit network."
[01:57:37.840 --> 01:57:39.520]   I think the proof is in the pudding.
[01:57:39.520 --> 01:57:42.600]   Advertisers like ITProTV and Audible,
[01:57:42.600 --> 01:57:44.840]   they've been with us for more than 10 years.
[01:57:44.840 --> 01:57:48.480]   They stick around because their ads work.
[01:57:48.480 --> 01:57:51.520]   And honestly, isn't that why you're buying advertising?
[01:57:51.520 --> 01:57:52.720]   You get a lot with Twit.
[01:57:52.720 --> 01:57:55.200]   We have a very full service attitude.
[01:57:55.200 --> 01:57:59.080]   We almost think of it as kind of artisanal advertising,
[01:57:59.080 --> 01:58:00.520]   boutique advertising.
[01:58:00.520 --> 01:58:04.160]   You'll get a full service continuity team.
[01:58:04.160 --> 01:58:05.800]   People who are on the phone with you,
[01:58:05.800 --> 01:58:07.080]   who are in touch with you,
[01:58:07.080 --> 01:58:10.320]   who support you with everything from copywriting
[01:58:10.320 --> 01:58:11.880]   to graphic design.
[01:58:11.880 --> 01:58:14.480]   So you are not alone in this.
[01:58:14.480 --> 01:58:17.280]   We embed our ads into the shows.
[01:58:17.280 --> 01:58:19.240]   They're not added later.
[01:58:19.240 --> 01:58:20.680]   They're part of the shows.
[01:58:20.680 --> 01:58:22.920]   In fact, often, there are such a part of our shows
[01:58:22.920 --> 01:58:25.960]   that our other hosts will chime in on the ad saying,
[01:58:25.960 --> 01:58:27.280]   "Yeah, I love that."
[01:58:27.280 --> 01:58:30.160]   Or just the other day, one of our hosts said,
[01:58:30.160 --> 01:58:32.280]   "Man, I really gotta buy that."
[01:58:32.280 --> 01:58:34.680]   That's an additional benefit to you
[01:58:34.680 --> 01:58:37.480]   because you're hearing people, our audience trusts,
[01:58:37.480 --> 01:58:40.280]   saying, "Yeah, that sounds great."
[01:58:40.280 --> 01:58:43.240]   We deliver, always over-deliver on impressions.
[01:58:43.240 --> 01:58:46.640]   So you know you're gonna get the impressions you expect.
[01:58:46.640 --> 01:58:48.600]   The ads are unique every time.
[01:58:48.600 --> 01:58:50.480]   We don't pre-record them and roll them in.
[01:58:50.480 --> 01:58:54.400]   We are genuinely doing those ads in the middle of the show.
[01:58:54.400 --> 01:58:56.360]   We'll give you great onboarding services,
[01:58:56.360 --> 01:59:00.320]   ad tech with pod sites that's free for direct clients.
[01:59:00.320 --> 01:59:01.600]   Gives you a lot of reporting,
[01:59:01.600 --> 01:59:04.240]   gives you a great idea of how well your ads are working.
[01:59:04.240 --> 01:59:05.640]   You'll get courtesy commercials.
[01:59:05.640 --> 01:59:07.440]   You actually can take our ads and share them
[01:59:07.440 --> 01:59:09.920]   across social media and landing pages.
[01:59:09.920 --> 01:59:11.920]   That really extends the reach.
[01:59:11.920 --> 01:59:13.160]   There are other free goodies too,
[01:59:13.160 --> 01:59:15.440]   including mentions in our weekly newsletter
[01:59:15.440 --> 01:59:18.560]   that sent to thousands of fans, engaged fans
[01:59:18.560 --> 01:59:20.120]   who really wanna see this stuff.
[01:59:20.120 --> 01:59:24.080]   We give you bonus ads and social media promotion too.
[01:59:24.080 --> 01:59:26.720]   So if you want to be a long-term partner,
[01:59:26.720 --> 01:59:30.840]   introduce your product to a savvy, engaged tech audience.
[01:59:30.840 --> 01:59:34.040]   Visit twit.tv/advertise.
[01:59:34.040 --> 01:59:35.440]   Check out those testimonials.
[01:59:35.440 --> 01:59:37.720]   Mark McCrary is the CEO of Authentic.
[01:59:37.720 --> 01:59:38.560]   You probably know him,
[01:59:38.560 --> 01:59:42.520]   one of the biggest original podcast advertising companies.
[01:59:42.520 --> 01:59:45.720]   We've been with him for 16 years.
[01:59:45.720 --> 01:59:47.920]   Mark said the feedback from many advertisers
[01:59:47.920 --> 01:59:51.680]   over 16 years across a range of product categories.
[01:59:51.680 --> 01:59:54.480]   Everything from razors to computers
[01:59:54.480 --> 01:59:57.640]   is that if ads and podcasts are gonna work for a brand,
[01:59:57.640 --> 01:59:59.560]   they're gonna work on twit shows.
[01:59:59.560 --> 02:00:02.880]   I'm very proud of what we do, because it's honest,
[02:00:02.880 --> 02:00:05.160]   it's got integrity, it's authentic,
[02:00:05.160 --> 02:00:08.120]   and it really is a great introduction
[02:00:08.120 --> 02:00:11.160]   to our audience of your brand.
[02:00:11.160 --> 02:00:14.000]   Our listeners are smart, they're engaged,
[02:00:14.000 --> 02:00:17.400]   they're tech savvy, they're dedicated to our network.
[02:00:17.400 --> 02:00:19.520]   And that's one of the reasons we only work
[02:00:19.520 --> 02:00:21.080]   with high-integrity partners
[02:00:21.080 --> 02:00:23.200]   that we've personally and thoroughly vetted.
[02:00:23.200 --> 02:00:25.880]   I have absolute approval on everybody.
[02:00:25.880 --> 02:00:28.960]   If you've got a great product, I wanna hear from you.
[02:00:28.960 --> 02:00:30.760]   Elevate your brand by reaching out today
[02:00:30.760 --> 02:00:33.200]   at advertise@twit.tv.
[02:00:33.200 --> 02:00:34.800]   Break out of the advertising norm.
[02:00:34.800 --> 02:00:38.440]   Grow your brand with host red ads on twit.tv.
[02:00:38.440 --> 02:00:41.120]   Visit twit.tv/advertise for more details.
[02:00:41.120 --> 02:00:45.800]   Or you can email us, advertise@twit.tv
[02:00:45.800 --> 02:00:47.640]   if you're ready to launch your campaign now.
[02:00:47.640 --> 02:00:48.960]   I can't wait to see your product.
[02:00:48.960 --> 02:00:50.160]   So give us a ring.
[02:00:50.160 --> 02:00:54.080]   - Have you tried, you've got a pixel there.
[02:00:54.080 --> 02:00:56.520]   Have you tried that alien's video
[02:00:56.520 --> 02:00:58.040]   that's supposed to crash the pixel?
[02:00:58.040 --> 02:00:59.680]   - No, I was worried of--
[02:00:59.680 --> 02:01:01.800]   - Do it, come on, blow us talkin' about it.
[02:01:01.800 --> 02:01:03.360]   - Oh, Jason, don't wanna be--
[02:01:03.360 --> 02:01:04.200]   - No, I'm not gonna do it.
[02:01:04.200 --> 02:01:05.040]   - All right, go that way.
[02:01:05.040 --> 02:01:06.960]   - Last time I didn't sell it like this on this show,
[02:01:06.960 --> 02:01:10.320]   I ended up getting myself kicked out of a Google account.
[02:01:10.320 --> 02:01:12.080]   - Almost never got it recovered.
[02:01:12.080 --> 02:01:15.200]   - Guys.
[02:01:15.200 --> 02:01:18.360]   - There is a particular video.
[02:01:18.360 --> 02:01:22.040]   It is a clip from Alien that was on YouTube.
[02:01:22.040 --> 02:01:23.600]   - It's an HDR clip, right?
[02:01:23.600 --> 02:01:24.440]   - I believe so.
[02:01:24.440 --> 02:01:25.800]   I think they've fixed it.
[02:01:25.800 --> 02:01:26.800]   - Oh.
[02:01:26.800 --> 02:01:28.960]   - I think they fixed it on the YouTube side
[02:01:28.960 --> 02:01:31.200]   and then Google's pushing out a fix.
[02:01:31.200 --> 02:01:32.040]   - Well, let's click.
[02:01:32.040 --> 02:01:34.280]   So it's Alien, how do I find it?
[02:01:34.280 --> 02:01:35.400]   - That's a really great question.
[02:01:35.400 --> 02:01:37.440]   It's probably part of this.
[02:01:37.440 --> 02:01:38.880]   You know, I should just do a Google search,
[02:01:38.880 --> 02:01:39.800]   shouldn't I, and see--
[02:01:39.800 --> 02:01:40.640]   - Let's see here.
[02:01:40.640 --> 02:01:41.760]   - If I can find a link.
[02:01:41.760 --> 02:01:42.800]   - It is this clip.
[02:01:42.800 --> 02:01:43.960]   - But no be responsible.
[02:01:43.960 --> 02:01:47.000]   We put a link, Alien 4K HDR.
[02:01:47.000 --> 02:01:49.040]   Get out of there on Apex clips.
[02:01:49.040 --> 02:01:54.040]   - Alien, here's your 4K HDR get out of there.
[02:01:54.040 --> 02:01:57.960]   - Apparently the minute you started,
[02:01:57.960 --> 02:02:00.080]   it crashes the whole phone, right?
[02:02:00.080 --> 02:02:02.080]   That one with a skull?
[02:02:02.080 --> 02:02:03.240]   - And Apex clips, yep, that would be it.
[02:02:03.240 --> 02:02:04.720]   - Apex clips, all right.
[02:02:04.720 --> 02:02:07.320]   - So you'd go ahead and go over to the show shot?
[02:02:08.200 --> 02:02:10.000]   - We gotta watch this crash.
[02:02:10.000 --> 02:02:10.840]   - Maybe.
[02:02:10.840 --> 02:02:13.240]   - You think they fixed it?
[02:02:13.240 --> 02:02:14.240]   - Yeah, I believe that they have--
[02:02:14.240 --> 02:02:16.240]   - This is two years ago.
[02:02:16.240 --> 02:02:17.240]   It was posted on 4K.
[02:02:17.240 --> 02:02:18.520]   - Yes, the same video.
[02:02:18.520 --> 02:02:19.640]   So we're looking at the same video.
[02:02:19.640 --> 02:02:22.200]   This is the video that was linked to from--
[02:02:22.200 --> 02:02:23.040]   - All right, you ready?
[02:02:23.040 --> 02:02:24.080]   - R's Technica.
[02:02:24.080 --> 02:02:25.040]   - Okay.
[02:02:25.040 --> 02:02:26.360]   - And the second is started playing,
[02:02:26.360 --> 02:02:28.000]   it was supposed to reboot your phone.
[02:02:28.000 --> 02:02:30.040]   So I think they fixed it, whatever it was.
[02:02:30.040 --> 02:02:32.640]   - But what do they find out why it?
[02:02:32.640 --> 02:02:33.760]   - What would have caused that?
[02:02:33.760 --> 02:02:36.400]   - Well, my understanding, my brief understanding
[02:02:36.400 --> 02:02:38.520]   from last night is that there is suspicion
[02:02:38.520 --> 02:02:40.840]   that had something to do with an HDR codec
[02:02:40.840 --> 02:02:46.320]   and something happening with this particular video with HDR.
[02:02:46.320 --> 02:02:47.640]   Beyond that, I have no clue.
[02:02:47.640 --> 02:02:49.560]   - At least one person reported that not only
[02:02:49.560 --> 02:02:52.760]   did it reboot their phone, but they lost connectivity
[02:02:52.760 --> 02:02:54.120]   until they rebooted again.
[02:02:54.120 --> 02:02:55.560]   - Again, a second time.
[02:02:55.560 --> 02:02:59.200]   - Right, and Flow did not write about this
[02:02:59.200 --> 02:03:02.160]   for Gizmodo Florence-Ion, but she did experience this
[02:03:02.160 --> 02:03:03.840]   and her colleague wrote about it.
[02:03:03.840 --> 02:03:05.400]   So you can kind of read her experience.
[02:03:05.400 --> 02:03:10.000]   - Either fixed or it doesn't impact all too bad.
[02:03:10.000 --> 02:03:10.840]   - But it was a--
[02:03:10.840 --> 02:03:12.840]   - But it could have been another great viral moment
[02:03:12.840 --> 02:03:15.560]   like you knocking yourself off Google,
[02:03:15.560 --> 02:03:17.640]   like me sticking the pen up side down.
[02:03:17.640 --> 02:03:18.480]   - That was the best.
[02:03:18.480 --> 02:03:20.600]   - On my Galaxy Note.
[02:03:20.600 --> 02:03:22.440]   - Oh yeah.
[02:03:22.440 --> 02:03:25.280]   - But how long were you off, Jason,
[02:03:25.280 --> 02:03:27.880]   before your conference of Google got you back on?
[02:03:27.880 --> 02:03:28.720]   - Yeah.
[02:03:28.720 --> 02:03:29.720]   - And by Google account?
[02:03:29.720 --> 02:03:32.560]   I mean, all things considered,
[02:03:32.560 --> 02:03:33.960]   I think it was only a couple of days,
[02:03:33.960 --> 02:03:35.600]   but it was still--
[02:03:35.600 --> 02:03:36.440]   - It was scary.
[02:03:36.440 --> 02:03:37.320]   - It was still scary.
[02:03:37.320 --> 02:03:39.880]   I mean, it was a situation that made me realize
[02:03:39.880 --> 02:03:42.400]   just how dependent upon my Google account
[02:03:42.400 --> 02:03:45.360]   I actually am and how much I really lose if I lost I could.
[02:03:45.360 --> 02:03:47.440]   - Sunday we had a guy call.
[02:03:47.440 --> 02:03:52.280]   He, his family asked him to digitize all the family VHS tapes.
[02:03:52.280 --> 02:03:53.440]   - He did.
[02:03:53.440 --> 02:03:57.280]   - They included a video of him and his sister as kids
[02:03:57.280 --> 02:04:00.840]   and taking a bath or swimming or something naked.
[02:04:00.840 --> 02:04:04.360]   At Google determined it was a child porn, it wasn't obviously.
[02:04:04.360 --> 02:04:07.200]   And he has lost his accounts and business accounts,
[02:04:07.200 --> 02:04:09.400]   all of his account, boom, gone.
[02:04:09.400 --> 02:04:12.760]   And he's appealed and he's, but the problem is,
[02:04:12.760 --> 02:04:15.880]   Google, it's hard to get any action out of them.
[02:04:15.880 --> 02:04:19.200]   - Every once in a while, I get an email very randomly
[02:04:19.200 --> 02:04:22.320]   from someone online to my personal account
[02:04:22.320 --> 02:04:23.320]   or my work account saying,
[02:04:23.320 --> 02:04:26.080]   "Hey, I ran across your video on YouTube.
[02:04:26.080 --> 02:04:27.040]   "This happened to me.
[02:04:27.040 --> 02:04:28.400]   "How did you get it figured out?
[02:04:28.400 --> 02:04:30.280]   "And I always hate to have the reply
[02:04:30.280 --> 02:04:31.440]   "and be like, look, it's--
[02:04:31.440 --> 02:04:32.480]   - I have connections.
[02:04:32.480 --> 02:04:34.200]   - Yeah, like because of what I do,
[02:04:34.200 --> 02:04:37.480]   I knew someone who was able to really help me
[02:04:37.480 --> 02:04:41.200]   and that's probably not an option for you and I'm so sorry.
[02:04:41.200 --> 02:04:43.560]   I wish I had a better answer.
[02:04:43.560 --> 02:04:44.960]   - Don't, well, I don't be the best.
[02:04:44.960 --> 02:04:48.160]   - I get people who come to me about Dell computers.
[02:04:48.160 --> 02:04:52.000]   - 2005, I wrote Dell Hell.
[02:04:52.000 --> 02:04:52.840]   (laughing)
[02:04:52.840 --> 02:04:54.160]   And to this day, someone will come here,
[02:04:54.160 --> 02:04:55.000]   "I have another problem.
[02:04:55.000 --> 02:04:56.520]   "Can you help me have a connection?"
[02:04:56.520 --> 02:04:57.720]   - No, I don't know.
[02:04:57.720 --> 02:04:59.640]   - Dell Hell, is that the name of the article
[02:04:59.640 --> 02:05:01.000]   or the book or?
[02:05:01.000 --> 02:05:02.600]   - Oh, you don't know that story?
[02:05:02.600 --> 02:05:05.160]   - No, I'm not certain that I know that you're Dell Hell.
[02:05:05.160 --> 02:05:06.720]   - I complained on my blog,
[02:05:06.720 --> 02:05:09.240]   this is before Twitter, about my laptop,
[02:05:09.240 --> 02:05:11.400]   I said, "Dell sucks!"
[02:05:11.400 --> 02:05:13.640]   And then all kinds of things.
[02:05:13.640 --> 02:05:16.200]   I was accused of dropping the stock price of Dell,
[02:05:16.200 --> 02:05:17.800]   honestly, by 20%.
[02:05:17.800 --> 02:05:18.640]   - Wow, that's power.
[02:05:18.640 --> 02:05:19.640]   - Michael Dell came back to the company.
[02:05:19.640 --> 02:05:20.480]   - That's power, sir.
[02:05:20.480 --> 02:05:23.200]   - We started their blog with Lionel Menchaka.
[02:05:23.200 --> 02:05:26.200]   They hired, they transferred a bunch of technicians
[02:05:26.200 --> 02:05:29.080]   to go solve bloggers' problems before they blew up.
[02:05:29.080 --> 02:05:33.160]   And this led to the whole kind of structure now
[02:05:33.160 --> 02:05:37.080]   of social media crisis management around companies
[02:05:37.080 --> 02:05:40.520]   and jerk customers like me.
[02:05:40.520 --> 02:05:42.840]   - Wow.
[02:05:42.840 --> 02:05:45.080]   - Knowing you.
[02:05:45.080 --> 02:05:48.400]   (laughing)
[02:05:48.400 --> 02:05:49.240]   - Well, all right.
[02:05:49.240 --> 02:05:50.840]   So was there really an issue?
[02:05:50.840 --> 02:05:51.840]   I mean, in my way--
[02:05:51.840 --> 02:05:52.680]   - Well, yeah, I was.
[02:05:52.680 --> 02:05:53.520]   - I would say there was.
[02:05:53.520 --> 02:05:58.360]   - My Dell Hell, August 29th, 2005.
[02:05:58.360 --> 02:06:00.360]   I'm just a citizen, a consumer,
[02:06:00.360 --> 02:06:03.640]   a guy who has my own printing press.
[02:06:03.640 --> 02:06:04.600]   - I am obnoxious.
[02:06:04.600 --> 02:06:07.760]   - And I get to use it however I want.
[02:06:07.760 --> 02:06:11.960]   Oh, so this was on Buzz Machine, Dell Lies.
[02:06:11.960 --> 02:06:12.800]   - Oh, yeah, oh, yeah.
[02:06:12.800 --> 02:06:15.800]   - The headline was Dell Lies and Dell Sucks.
[02:06:15.800 --> 02:06:17.040]   - Not my proudest brother.
[02:06:17.040 --> 02:06:18.440]   (laughing)
[02:06:18.440 --> 02:06:19.880]   - Not my most mature.
[02:06:19.880 --> 02:06:22.400]   - Wow. - Wow.
[02:06:22.400 --> 02:06:24.280]   - Well, good for you.
[02:06:25.360 --> 02:06:28.720]   - You know, one thing I realized, I was watching TV
[02:06:28.720 --> 02:06:33.320]   somebody had passed away and it was the guy, okay.
[02:06:33.320 --> 02:06:36.880]   So it was the guy who Dick Cheney shot in the face by accident.
[02:06:36.880 --> 02:06:37.720]   - Oh, right.
[02:06:37.720 --> 02:06:39.840]   - Passed away a couple of, maybe a few weeks ago.
[02:06:39.840 --> 02:06:41.680]   - And the poor guy, this is his robot.
[02:06:41.680 --> 02:06:43.760]   - That's the old thing.
[02:06:43.760 --> 02:06:44.920]   That's the old thing.
[02:06:44.920 --> 02:06:46.560]   They don't talk about anything else.
[02:06:46.560 --> 02:06:47.400]   - They don't talk about his lives.
[02:06:47.400 --> 02:06:48.240]   - It's just the guy-- - I was like,
[02:06:48.240 --> 02:06:49.600]   I just thought that was something one thing.
[02:06:49.600 --> 02:06:51.880]   - He was the victim, he'd even do it.
[02:06:51.880 --> 02:06:54.000]   The guy Dick Cheney shot by accident.
[02:06:54.000 --> 02:06:56.160]   And I thought, that's the problem.
[02:06:56.160 --> 02:06:58.880]   - Yeah. - Jeff, when you die,
[02:06:58.880 --> 02:06:59.720]   Dell Hell.
[02:06:59.720 --> 02:07:01.480]   - Dell Hell.
[02:07:01.480 --> 02:07:03.080]   - That was it, you had your moment.
[02:07:03.080 --> 02:07:03.920]   - That was my moment.
[02:07:03.920 --> 02:07:04.760]   - What's in place, man?
[02:07:04.760 --> 02:07:07.200]   - What's mine gonna be, Senator Devil?
[02:07:07.200 --> 02:07:08.200]   I don't know.
[02:07:08.200 --> 02:07:10.760]   Oh no, no, no, no, no, it's gonna be the URL.
[02:07:10.760 --> 02:07:12.720]   - Hey, Jeff has a whole-- - Robot.
[02:07:12.720 --> 02:07:14.160]   - I'm an idiot. - Bad robot.
[02:07:14.160 --> 02:07:16.560]   Yeah, that's right, it'll be, yeah, that's true.
[02:07:16.560 --> 02:07:18.120]   We do have it all written down.
[02:07:18.120 --> 02:07:20.120]   (laughing)
[02:07:20.120 --> 02:07:21.200]   Here's Jeff's, there you go.
[02:07:21.200 --> 02:07:22.040]   - There you go.
[02:07:22.040 --> 02:07:23.040]   - That's right, that's it.
[02:07:23.040 --> 02:07:26.240]   - Yeah, it might be that I was a dev gnaw.
[02:07:26.240 --> 02:07:28.120]   That would be depressing. - Thank you, Mr. Know.
[02:07:28.120 --> 02:07:30.200]   - Yeah, that would be depressing.
[02:07:30.200 --> 02:07:33.000]   You may remember him as that crappy animated thing
[02:07:33.000 --> 02:07:36.720]   on NBC. (laughing)
[02:07:36.720 --> 02:07:38.080]   - Oh Lord.
[02:07:38.080 --> 02:07:40.720]   - Pissed off, "Sole of Metal Briar" regularly.
[02:07:40.720 --> 02:07:41.760]   - Yeah.
[02:07:41.760 --> 02:07:45.800]   So, we should probably do a little Twitter update.
[02:07:45.800 --> 02:07:47.440]   Elon right now is on stage talking about
[02:07:47.440 --> 02:07:49.800]   master plan number three.
[02:07:49.800 --> 02:07:52.240]   It's funny, when his first master plan came out,
[02:07:52.240 --> 02:07:53.320]   I was very impressed.
[02:07:53.320 --> 02:07:55.480]   We talked a lot about it, master plan two as well.
[02:07:55.480 --> 02:07:58.600]   He was executing, you know, it was things like
[02:07:58.600 --> 02:08:01.200]   create a high priced electric vehicle
[02:08:01.200 --> 02:08:05.400]   to support the creation of an affordable electric vehicle
[02:08:05.400 --> 02:08:06.440]   to save the planet.
[02:08:06.440 --> 02:08:09.240]   It was stuff like that, it was great.
[02:08:09.240 --> 02:08:11.360]   I don't know if people are as quite as interested
[02:08:11.360 --> 02:08:13.480]   in master plan three.
[02:08:13.480 --> 02:08:17.280]   Like, maybe the bloom is off the rose a little bit,
[02:08:17.280 --> 02:08:19.440]   but we'll give you an update as soon as it's released.
[02:08:19.440 --> 02:08:21.560]   I don't know if they've done it yet.
[02:08:21.560 --> 02:08:24.400]   Meanwhile, Elon is still running Twitter,
[02:08:24.400 --> 02:08:27.520]   although, according to platformer,
[02:08:27.520 --> 02:08:30.920]   they're the, that's gonna by the way be casey.
[02:08:30.920 --> 02:08:33.520]   - That's gonna be Casey Newton's obituary.
[02:08:33.520 --> 02:08:34.840]   (laughing)
[02:08:34.840 --> 02:08:37.880]   - As he followed the Twitter collapse
[02:08:37.880 --> 02:08:40.040]   in the mid 2020s.
[02:08:40.040 --> 02:08:43.800]   - According to Zor Shiffer and Casey Newton,
[02:08:43.800 --> 02:08:45.920]   what's been going on at Twitter might give you some hint
[02:08:45.920 --> 02:08:47.680]   about the next CEO.
[02:08:49.080 --> 02:08:52.960]   So clearly AI generated Bluebird.
[02:08:52.960 --> 02:08:53.800]   - Yep, Dally.
[02:08:53.800 --> 02:08:55.200]   - Dally, that's great.
[02:08:55.200 --> 02:08:56.600]   - No surprise.
[02:08:56.600 --> 02:09:03.280]   So, Davis, who was currently CEO of the boring company,
[02:09:03.280 --> 02:09:06.360]   loaned himself out to Twitter when Elon bought it.
[02:09:06.360 --> 02:09:09.000]   It turned out he was one of that gang
[02:09:09.000 --> 02:09:11.600]   and has emerged as one of Musk's top lieutenants.
[02:09:11.600 --> 02:09:13.640]   Even he had just, he had his wife,
[02:09:13.640 --> 02:09:16.480]   he just had a baby and I think he was bringing the newborn
[02:09:16.480 --> 02:09:19.520]   to Twitter, big layoffs, another--
[02:09:19.520 --> 02:09:21.320]   - Yeah, there was sleeping in the office.
[02:09:21.320 --> 02:09:23.360]   If I read that correctly.
[02:09:23.360 --> 02:09:25.560]   - Another new baby in the family was sleeping in the office.
[02:09:25.560 --> 02:09:26.600]   - Can you believe that?
[02:09:26.600 --> 02:09:29.080]   - That's a little more dedication to him.
[02:09:29.080 --> 02:09:29.920]   - Totally.
[02:09:29.920 --> 02:09:31.160]   I thought the same thing.
[02:09:31.160 --> 02:09:37.800]   - Another round of layoffs on Saturday, another 200 employees,
[02:09:37.800 --> 02:09:40.140]   including some big names.
[02:09:40.140 --> 02:09:44.360]   Esther Crawford is the one most people talked about.
[02:09:44.360 --> 02:09:46.720]   She was the one who was sleeping in the office,
[02:09:46.720 --> 02:09:49.720]   posted on Twitter the picture of her in a sleeping bag
[02:09:49.720 --> 02:09:51.040]   with a hashtag--
[02:09:51.040 --> 02:09:52.400]   - Charge of Blue.
[02:09:52.400 --> 02:09:55.880]   - Yeah, hashtag live where you work or sleep where you work.
[02:09:55.880 --> 02:09:57.440]   - We've just not a good hashtag.
[02:09:57.440 --> 02:09:58.280]   - Hashtag. - No, never.
[02:09:58.280 --> 02:09:59.120]   - And he's normalized that.
[02:09:59.120 --> 02:10:01.640]   - But also Leah Culver, who I have huge respect for.
[02:10:01.640 --> 02:10:03.680]   She was in charge of Twitter's spaces.
[02:10:03.680 --> 02:10:08.000]   Serial startup person did some really interesting stuff.
[02:10:08.000 --> 02:10:09.480]   I think she did Pounce, right?
[02:10:09.480 --> 02:10:12.440]   Which was a Twitter competitor in the early days.
[02:10:12.440 --> 02:10:13.280]   - Oh yeah.
[02:10:13.280 --> 02:10:14.320]   - All Pounce of--
[02:10:14.320 --> 02:10:17.880]   - All people on the Do Not Fire list.
[02:10:17.880 --> 02:10:20.960]   Martin DeKuiper, who was the creator of review,
[02:10:20.960 --> 02:10:23.240]   the newsletter program Twitter bought,
[02:10:23.240 --> 02:10:27.220]   and then under Elon has deprecated, has gotten rid of.
[02:10:27.220 --> 02:10:30.840]   The thought is, according to the platformer,
[02:10:30.840 --> 02:10:33.840]   it was gonna be so expensive to pay them out
[02:10:33.840 --> 02:10:35.860]   that they were on a Do Not Fire list,
[02:10:35.860 --> 02:10:41.920]   but there's some suspicion that perhaps
[02:10:41.920 --> 02:10:44.240]   Elon isn't planning on paying them out.
[02:10:44.240 --> 02:10:47.000]   - Micropsy.
[02:10:47.000 --> 02:10:48.040]   - Yeah, they are.
[02:10:48.040 --> 02:10:49.480]   - Can't afford.
[02:10:49.480 --> 02:10:51.840]   - The company's head of sales, Chris Reedy.
[02:10:51.840 --> 02:10:52.680]   - Cut.
[02:10:52.680 --> 02:10:54.560]   - What's to sell?
[02:10:54.560 --> 02:10:59.560]   - And all of this under the guidance of Davis.
[02:10:59.560 --> 02:11:03.200]   So there's some suspicion that Davis is the guy
[02:11:03.200 --> 02:11:05.680]   Elon is thinking he's gonna take it over.
[02:11:05.680 --> 02:11:07.160]   Although I hate to be,
[02:11:07.160 --> 02:11:10.320]   it's very much like the Politburo under Stalin.
[02:11:10.320 --> 02:11:13.520]   You know, okay, here's the next round of loyalists who are,
[02:11:13.520 --> 02:11:14.360]   (sniffing)
[02:11:14.360 --> 02:11:16.680]   I hate to be the next one in line.
[02:11:16.680 --> 02:11:17.840]   - Right beyond that.
[02:11:17.840 --> 02:11:18.680]   - Yeah.
[02:11:18.680 --> 02:11:20.120]   - Do you ever learn?
[02:11:20.120 --> 02:11:22.440]   I made it this time, oh, I'm invincible.
[02:11:22.440 --> 02:11:24.360]   Yeah, it just hasn't gotten to you yet.
[02:11:24.360 --> 02:11:27.600]   - In December, the information reported that Musk tasks
[02:11:27.600 --> 02:11:30.200]   Davis with cutting half a billion in costs,
[02:11:30.200 --> 02:11:32.200]   instead he cut close to a billion,
[02:11:32.200 --> 02:11:34.120]   all while sleeping in the office with his partner
[02:11:34.120 --> 02:11:35.440]   and their newborn child.
[02:11:35.440 --> 02:11:36.680]   - There you go.
[02:11:36.680 --> 02:11:39.760]   - His success in bringing down costs by any means necessary
[02:11:39.760 --> 02:11:41.600]   has led to growing speculation internally
[02:11:41.600 --> 02:11:45.240]   that Musk will choose him to be the next CEO, says.
[02:11:45.240 --> 02:11:46.880]   So he's shiffering Casey Newton.
[02:11:46.880 --> 02:11:50.560]   Has, (laughing)
[02:11:50.560 --> 02:11:53.360]   Father Robert tweeted today that,
[02:11:53.360 --> 02:11:55.880]   Twitter social has been more reliable in the last six months
[02:11:55.880 --> 02:11:57.360]   than Twitter, is that the case?
[02:11:57.360 --> 02:11:59.960]   Is Twitter been, Twitter was down for a lot of people, right?
[02:11:59.960 --> 02:12:02.920]   And then the timeline wasn't loading.
[02:12:02.920 --> 02:12:04.560]   There've been some issues, yes.
[02:12:04.560 --> 02:12:07.640]   - I haven't had any accessibility issues like that
[02:12:07.640 --> 02:12:08.640]   with Twitter.
[02:12:08.640 --> 02:12:09.760]   - Okay.
[02:12:09.760 --> 02:12:11.560]   - I've noticed a ton of notifications.
[02:12:11.560 --> 02:12:13.600]   Like I had to mute, I basically muted all Twitter
[02:12:13.600 --> 02:12:15.960]   notifications 'cause suddenly it just ramped up.
[02:12:15.960 --> 02:12:18.640]   I was getting that ton out of nowhere.
[02:12:18.640 --> 02:12:20.400]   Like a lot of promotional like, oh,
[02:12:20.400 --> 02:12:22.920]   in case you missed it sort of things,
[02:12:22.920 --> 02:12:25.720]   seeing it's very pluggy when I log into it.
[02:12:25.720 --> 02:12:28.200]   It's like, oh, it really wants to be smart
[02:12:28.200 --> 02:12:29.600]   and tell me what I wanna see.
[02:12:29.600 --> 02:12:31.680]   And that has changed significantly.
[02:12:31.680 --> 02:12:32.920]   So maybe that's part of the reason why
[02:12:32.920 --> 02:12:33.800]   I'm not using it as much.
[02:12:33.800 --> 02:12:37.960]   But I haven't seen it being like, you know,
[02:12:37.960 --> 02:12:41.440]   seen it broken in any real major way.
[02:12:41.440 --> 02:12:43.400]   I haven't encountered that.
[02:12:43.400 --> 02:12:45.440]   Which I kind of surprised about
[02:12:45.440 --> 02:12:47.720]   'cause I expected with all of the layoffs
[02:12:47.720 --> 02:12:49.480]   that things would get really choppy.
[02:12:49.480 --> 02:12:52.920]   And that hasn't necessarily been my experience so far.
[02:12:52.920 --> 02:12:59.200]   Elon Musk's defense, according to Siva Vaigenothan,
[02:12:59.200 --> 02:13:02.960]   of Scott Adams shows why he is misguided and dangerous.
[02:13:02.960 --> 02:13:06.680]   When Adams makes a,
[02:13:06.680 --> 02:13:08.560]   I had a Scott Adams, a creator of Dilbert.
[02:13:08.560 --> 02:13:09.640]   Makes it full of himself.
[02:13:09.640 --> 02:13:10.880]   He mainly just harms himself,
[02:13:10.880 --> 02:13:15.440]   but Musk has the power to harm others, said Siva.
[02:13:15.440 --> 02:13:19.440]   Scott Adams lost his syndication
[02:13:19.440 --> 02:13:24.440]   for the Dilbert comic strip after a racist video on YouTube.
[02:13:24.440 --> 02:13:25.640]   But,
[02:13:25.640 --> 02:13:31.400]   Elon's kind of been supportive.
[02:13:31.400 --> 02:13:35.000]   And Elon also is a accused of racism, I might add.
[02:13:35.000 --> 02:13:36.800]   Well, as somebody said on Twitter,
[02:13:36.800 --> 02:13:38.480]   you can take Elon out of South Africa,
[02:13:38.480 --> 02:13:40.360]   but you can't take the South Africa out of Elon.
[02:13:40.360 --> 02:13:41.360]   - Yeah, yeah.
[02:13:41.360 --> 02:13:42.200]   Wow.
[02:13:42.200 --> 02:13:44.600]   - Yeah, Musk said,
[02:13:44.600 --> 02:13:46.160]   I don't agree with everything Scott says,
[02:13:46.160 --> 02:13:48.840]   but Dilbert is legit funny and insightful.
[02:13:48.840 --> 02:13:51.040]   We should stop canceling comedy.
[02:13:51.040 --> 02:13:54.640]   Yeah, actually Dilbert's not that funny.
[02:13:54.640 --> 02:13:55.480]   No, it's not. - No, it's not.
[02:13:55.480 --> 02:13:56.320]   - Okay.
[02:13:56.320 --> 02:13:58.880]   - The Dilbrito was a funny thing, but not.
[02:13:58.880 --> 02:14:00.480]   - Yeah.
[02:14:00.480 --> 02:14:05.280]   - Musk also said, you know,
[02:14:05.280 --> 02:14:07.320]   let's see,
[02:14:07.320 --> 02:14:10.720]   that the media is racist.
[02:14:10.720 --> 02:14:11.560]   - Okay.
[02:14:11.560 --> 02:14:12.400]   - That was the other thing.
[02:14:12.400 --> 02:14:13.480]   - That's great. - Okay.
[02:14:13.480 --> 02:14:15.320]   Musk said, "Without Evans, for a very long time,
[02:14:15.320 --> 02:14:17.600]   "US media was racist against non-white people.
[02:14:17.600 --> 02:14:20.080]   "Now they're racist against whites and Asians.
[02:14:20.080 --> 02:14:21.960]   "Same thing happened with elite colleges
[02:14:21.960 --> 02:14:23.680]   "and high schools in America.
[02:14:23.680 --> 02:14:26.600]   "Maybe they could try not being racist."
[02:14:26.600 --> 02:14:27.440]   - He's interesting.
[02:14:27.440 --> 02:14:30.360]   - He's just purely out of white supremacy land, pure.
[02:14:30.360 --> 02:14:31.200]   - Yeah.
[02:14:31.200 --> 02:14:34.720]   Musk agreed with the tweet saying,
[02:14:34.720 --> 02:14:36.040]   "Adam's comments weren't good,
[02:14:36.040 --> 02:14:38.080]   "but had an element of truth."
[02:14:38.080 --> 02:14:39.520]   By the way, this is what racist always said.
[02:14:39.520 --> 02:14:41.440]   In fact, it's what racist always think.
[02:14:41.440 --> 02:14:43.440]   Racists never say, "I'm a racist."
[02:14:43.440 --> 02:14:45.040]   They don't even think they're racist.
[02:14:45.040 --> 02:14:46.400]   They just say, "No, I'm just telling the truth."
[02:14:46.400 --> 02:14:47.240]   - "I'm just telling the truth."
[02:14:47.240 --> 02:14:48.080]   - Yeah.
[02:14:48.080 --> 02:14:48.920]   - "I'm just telling the truth."
[02:14:48.920 --> 02:14:49.760]   - Yeah, right.
[02:14:49.760 --> 02:14:50.920]   - Yeah, exactly.
[02:14:50.920 --> 02:14:51.760]   - Yeah.
[02:14:51.760 --> 02:14:54.200]   - "I'm really close friends with black persons."
[02:14:54.200 --> 02:14:55.200]   - Obviously. - Yeah.
[02:14:55.200 --> 02:14:58.520]   - And of course, Elon's tweets in support of
[02:15:00.040 --> 02:15:02.800]   Scott Adams were responded to by Scott Adams.
[02:15:02.800 --> 02:15:05.920]   Saying, "Thank you, Elon."
[02:15:05.920 --> 02:15:07.200]   So he's still on Twitter.
[02:15:07.200 --> 02:15:10.400]   Elon Musk.
[02:15:10.400 --> 02:15:11.360]   - Meanwhile, Elon's.
[02:15:11.360 --> 02:15:12.200]   - Go ahead.
[02:15:12.200 --> 02:15:13.040]   - Go ahead.
[02:15:13.040 --> 02:15:14.600]   - Well, you're gonna say the same thing I am.
[02:15:14.600 --> 02:15:15.440]   - Yeah, you go.
[02:15:15.440 --> 02:15:17.920]   - He's gonna build his own AI
[02:15:17.920 --> 02:15:21.240]   that is not to woke.
[02:15:21.240 --> 02:15:23.920]   This is chat TPGPT is to woke.
[02:15:23.920 --> 02:15:26.720]   And we're gonna build an AI that's not woke.
[02:15:26.720 --> 02:15:28.920]   This is according to the information.
[02:15:28.920 --> 02:15:30.840]   We're gonna dial back the wokeometer.
[02:15:30.840 --> 02:15:34.720]   - He recruited a team. - Yeah, he has to be an idiot
[02:15:34.720 --> 02:15:35.760]   to go to work for Elon,
[02:15:35.760 --> 02:15:38.320]   but he recruited a team develop open AI rival.
[02:15:38.320 --> 02:15:40.320]   Okay, fine.
[02:15:40.320 --> 02:15:41.160]   Who you gonna get?
[02:15:41.160 --> 02:15:42.480]   - What money?
[02:15:42.480 --> 02:15:43.440]   Where's the money to do that?
[02:15:43.440 --> 02:15:44.280]   - Oh, wait a minute.
[02:15:44.280 --> 02:15:45.120]   He's the richest.
[02:15:45.120 --> 02:15:46.160]   Like, you know, this is the thing.
[02:15:46.160 --> 02:15:47.720]   People talk about all it's going bankrupt.
[02:15:47.720 --> 02:15:48.760]   He's spending all his money.
[02:15:48.760 --> 02:15:49.600]   He's still there.
[02:15:49.600 --> 02:15:52.120]   He's now, once again, the richest man in the world
[02:15:52.120 --> 02:15:55.360]   has something like $197 billion.
[02:15:55.360 --> 02:15:57.920]   He could lose all 44 billion
[02:15:57.920 --> 02:16:01.000]   and still be in the top 10 richest people in the world.
[02:16:01.000 --> 02:16:02.720]   He's not at risk of anything.
[02:16:02.720 --> 02:16:04.560]   - Nope.
[02:16:04.560 --> 02:16:05.960]   Well, but Tesla's stock is, well,
[02:16:05.960 --> 02:16:07.640]   but he's losing more than 44 billion.
[02:16:07.640 --> 02:16:09.520]   - He's leveraged with all on Tesla stock,
[02:16:09.520 --> 02:16:11.080]   which is probably not again.
[02:16:11.080 --> 02:16:12.080]   - That's not what he was saying to me.
[02:16:12.080 --> 02:16:14.000]   One of our students was saying to me the other day,
[02:16:14.000 --> 02:16:16.560]   is I said, I think that Twitter will go bankrupt
[02:16:16.560 --> 02:16:19.000]   and the banks will own it for 13 billion.
[02:16:19.000 --> 02:16:21.920]   They said, yeah, but so much of Tesla is mortgaged on it.
[02:16:23.840 --> 02:16:28.840]   - Elon's tweet about censoring, you know, chat GPT,
[02:16:28.840 --> 02:16:33.240]   wouldn't tell a joke in the style of,
[02:16:33.240 --> 02:16:36.080]   it would tell a joke in the style of Jerry Seinfeld,
[02:16:36.080 --> 02:16:39.720]   but would not tell a joke in the style of Dave Chappelle,
[02:16:39.720 --> 02:16:41.280]   which is weird, but okay.
[02:16:41.280 --> 02:16:45.960]   Some people said that's just 'cause it's not as good.
[02:16:45.960 --> 02:16:49.920]   You can't do it, but Elon said, you know,
[02:16:49.920 --> 02:16:52.520]   we need, what was it?
[02:16:52.520 --> 02:16:57.520]   We need a non-woke GPT, chat GPT.
[02:16:57.520 --> 02:17:00.720]   Another instance in response to a user asking Open CEO,
[02:17:00.720 --> 02:17:04.360]   Sam Altman to turn off the woke settings for GPT,
[02:17:04.360 --> 02:17:06.920]   Musk replied saying, the danger of training AI to be woke,
[02:17:06.920 --> 02:17:09.680]   in other words, lie is deadly.
[02:17:09.680 --> 02:17:11.360]   Now, one thing I gotta point out,
[02:17:11.360 --> 02:17:13.760]   there's no definition of woke.
[02:17:13.760 --> 02:17:17.440]   Woke is, you're just saying things I disagree with, right?
[02:17:17.440 --> 02:17:20.440]   - Woke started in black America.
[02:17:20.440 --> 02:17:22.320]   - I know, that's ironic. - And they got co-opted.
[02:17:22.320 --> 02:17:23.320]   It's ironic.
[02:17:23.320 --> 02:17:25.800]   - Not just ironic, no, it's on purpose.
[02:17:25.800 --> 02:17:26.640]   It's on purpose.
[02:17:26.640 --> 02:17:29.080]   You co-op the language of your opponent.
[02:17:29.080 --> 02:17:30.080]   - Yeah, yeah.
[02:17:30.080 --> 02:17:37.040]   Mozilla is leading mammoth's pre-seed funding.
[02:17:37.040 --> 02:17:41.280]   Mammoth is a mastodon app, which I actually don't like very much.
[02:17:41.280 --> 02:17:44.400]   - We don't own. - No, it's on macOS,
[02:17:44.400 --> 02:17:46.280]   but I guess it's also on iOS.
[02:17:46.280 --> 02:17:49.080]   It was originally built on iOS.
[02:17:50.440 --> 02:17:52.440]   Yeah, I think it's a terrible app, frankly,
[02:17:52.440 --> 02:17:54.160]   and I don't understand why you need one on the Mac,
[02:17:54.160 --> 02:17:57.760]   since you could just use the browser, which is much better.
[02:17:57.760 --> 02:18:01.920]   But again, if it's a venture capital-funded effort
[02:18:01.920 --> 02:18:03.000]   in the Fediverse,
[02:18:03.000 --> 02:18:07.160]   I'm gonna be inherently biased against it,
[02:18:07.160 --> 02:18:08.400]   'cause I am woke.
[02:18:08.400 --> 02:18:12.320]   Just telling you right now.
[02:18:12.320 --> 02:18:13.400]   - The devil is woke.
[02:18:13.400 --> 02:18:14.320]   - The devil woke.
[02:18:14.320 --> 02:18:15.160]   - The devil is woke.
[02:18:15.160 --> 02:18:16.480]   - The devil is woke.
[02:18:16.480 --> 02:18:17.440]   - The devil's woke.
[02:18:17.440 --> 02:18:19.120]   - There we go, show title, show title.
[02:18:19.120 --> 02:18:20.360]   - The devil is woke.
[02:18:20.360 --> 02:18:26.640]   What does woke mean, is it being woke up?
[02:18:26.640 --> 02:18:29.400]   - I'm the wrong person to ask,
[02:18:29.400 --> 02:18:32.480]   because apparently I'm not woke enough either
[02:18:32.480 --> 02:18:33.480]   as a black man.
[02:18:33.480 --> 02:18:36.840]   I get a lot of crap thrown at me and people 'cause
[02:18:36.840 --> 02:18:37.680]   - Oh, really?
[02:18:37.680 --> 02:18:39.760]   - 'Cause it's at me because I don't always agree
[02:18:39.760 --> 02:18:42.760]   with some of the stuff that said in the quote,
[02:18:42.760 --> 02:18:44.920]   "woke community" and stuff like that,
[02:18:44.920 --> 02:18:48.600]   'cause some things don't make sense to me.
[02:18:48.600 --> 02:18:52.480]   - Is the premise, if you're asleep,
[02:18:52.480 --> 02:18:54.520]   you're not aware of systemic racism,
[02:18:54.520 --> 02:18:57.400]   but once you wake up, you're aware of it.
[02:18:57.400 --> 02:19:00.400]   - Right, 'cause I live in this world
[02:19:00.400 --> 02:19:03.120]   and I know that racism is this.
[02:19:03.120 --> 02:19:04.960]   I have experienced racism,
[02:19:04.960 --> 02:19:08.760]   but I'm probably one of the few black folks
[02:19:08.760 --> 02:19:11.080]   that you're gonna meet that says, "You know what?
[02:19:11.080 --> 02:19:14.520]   "Not everything that happens to me on a bad note
[02:19:14.520 --> 02:19:16.080]   "is because of racism."
[02:19:16.080 --> 02:19:19.320]   - Something is my bad decisions, you know?
[02:19:19.320 --> 02:19:21.000]   - That seems fair.
[02:19:21.000 --> 02:19:23.720]   - I am not woke when I say stuff like that.
[02:19:23.720 --> 02:19:27.280]   - I would never dream to say anyone is woke or unwoke.
[02:19:27.280 --> 02:19:29.960]   I don't even like that word to be honest with you.
[02:19:29.960 --> 02:19:33.360]   - I don't really get into it too often
[02:19:33.360 --> 02:19:35.520]   'cause most of the time people just--
[02:19:35.520 --> 02:19:36.840]   - Not for me to say-- - It's not for me to say
[02:19:36.840 --> 02:19:38.440]   black people, it's how you should feel, Andy.
[02:19:38.440 --> 02:19:40.240]   - Yeah, right?
[02:19:40.240 --> 02:19:42.760]   - For the record, it's a police brutality.
[02:19:42.760 --> 02:19:43.680]   That's where the origin--
[02:19:43.680 --> 02:19:45.480]   - Oh, origin's police brutality.
[02:19:45.480 --> 02:19:47.680]   - Yes, Benito says, "Oh, I'm sorry, I miss--
[02:19:47.680 --> 02:19:49.240]   - Oh, okay, thanks, Anis, Benito.
[02:19:49.240 --> 02:19:50.920]   - So it's not about systemic racism,
[02:19:50.920 --> 02:19:53.320]   it's waking up to police brutality, Benito?
[02:19:53.320 --> 02:19:55.960]   - Something along those lines.
[02:19:55.960 --> 02:19:58.440]   That's where it started, that's where the origin is from.
[02:19:58.440 --> 02:20:00.160]   - I'm gonna have to take it over.
[02:20:00.160 --> 02:20:01.920]   - Check the urban dictionary on this.
[02:20:01.920 --> 02:20:04.480]   - So the expert at this is Meredith Clark,
[02:20:04.480 --> 02:20:06.160]   who's also an expert in black twitter.
[02:20:06.160 --> 02:20:07.840]   I put up something on the bottom of the rundown.
[02:20:07.840 --> 02:20:10.760]   - I'm so sad that your black twitter event
[02:20:10.760 --> 02:20:12.640]   is not somewhere saved.
[02:20:12.640 --> 02:20:14.440]   - Tell me about it.
[02:20:15.280 --> 02:20:17.240]   - And I know he's feeling it.
[02:20:17.240 --> 02:20:20.360]   - Here's what the Niva AI says.
[02:20:20.360 --> 02:20:22.800]   Being woke is defined as being aware of social political
[02:20:22.800 --> 02:20:25.000]   issues, particularly those related to racism
[02:20:25.000 --> 02:20:26.240]   and social injustice.
[02:20:26.240 --> 02:20:28.320]   It's also used to describe somebody who's conscious
[02:20:28.320 --> 02:20:30.240]   of disparities between different demographics
[02:20:30.240 --> 02:20:32.720]   and socioeconomic standings.
[02:20:32.720 --> 02:20:34.480]   Someone's pretentious about how much they care
[02:20:34.480 --> 02:20:37.280]   about a social issue, that's the other side of it.
[02:20:37.280 --> 02:20:39.760]   Finally, it's used to describe someone who's asleep
[02:20:39.760 --> 02:20:41.960]   and uncritically accepting whatever nonsense
[02:20:41.960 --> 02:20:44.720]   social science professors dream up to advance
[02:20:44.720 --> 02:20:46.240]   Marxist goals.
[02:20:46.240 --> 02:20:50.240]   Oh wow, that's the other side of it.
[02:20:50.240 --> 02:20:54.080]   - That's the everything that's been piled onto what's--
[02:20:54.080 --> 02:20:55.480]   - Right, yes.
[02:20:55.480 --> 02:20:56.320]   - Right, right.
[02:20:56.320 --> 02:21:02.040]   - Now that Niva result, was that the AI summarizing
[02:21:02.040 --> 02:21:03.240]   or was that a clip?
[02:21:03.240 --> 02:21:04.960]   Okay, that was the AI summarizing.
[02:21:04.960 --> 02:21:09.920]   It was officially added to the dictionary 2017.
[02:21:09.920 --> 02:21:11.960]   I think this is one of the things it was summarizing.
[02:21:11.960 --> 02:21:16.160]   All right, anyway, enough, enough, enough.
[02:21:16.160 --> 02:21:17.720]   Should we do a quick change log?
[02:21:17.720 --> 02:21:18.560]   - Let's do it.
[02:21:18.560 --> 02:21:20.880]   - Door, what the hey.
[02:21:20.880 --> 02:21:23.800]   - A Google change log.
[02:21:23.800 --> 02:21:25.120]   - This is one of those weeks where I was like,
[02:21:25.120 --> 02:21:26.160]   God, I'm not finding much.
[02:21:26.160 --> 02:21:27.800]   And then suddenly I've got a lot.
[02:21:27.800 --> 02:21:29.120]   - Oh, there's a ton in here.
[02:21:29.120 --> 02:21:32.160]   - Mr. change log, you should be doing this, not me.
[02:21:32.160 --> 02:21:33.000]   Go ahead.
[02:21:33.000 --> 02:21:38.160]   Okay, new changes coming to Android.
[02:21:38.160 --> 02:21:42.920]   A lot of miniature changes, things like a Google Keep note
[02:21:42.920 --> 02:21:46.640]   widget for storing a single note on your home screen
[02:21:46.640 --> 02:21:50.520]   and always adding to or removing from it, whatever.
[02:21:50.520 --> 02:21:53.240]   Some wireless shortcuts for Keep.
[02:21:53.240 --> 02:21:58.240]   Chrome getting 300% zoom, which, this is one of those updates
[02:21:58.240 --> 02:22:01.880]   where it's like, it was timed with Mobile World Congress
[02:22:01.880 --> 02:22:02.880]   and we talked about it a little bit.
[02:22:02.880 --> 02:22:03.720]   - Oh, that's nice.
[02:22:03.720 --> 02:22:05.160]   - On all that Android.
[02:22:05.160 --> 02:22:07.400]   It's like, here's a bunch of announcements,
[02:22:07.400 --> 02:22:09.040]   but they're all like really small.
[02:22:09.040 --> 02:22:09.880]   - Right.
[02:22:09.880 --> 02:22:11.680]   - So, it's kind of silly.
[02:22:11.680 --> 02:22:13.600]   - And this is for the current Android
[02:22:13.600 --> 02:22:15.760]   or is this the next edition?
[02:22:15.760 --> 02:22:20.760]   - No, these are updates that hit, I think everyone on Android
[02:22:20.760 --> 02:22:23.280]   because it's done through Google Play services.
[02:22:23.280 --> 02:22:25.320]   So you're not waiting for a major OS update.
[02:22:25.320 --> 02:22:27.680]   This just kind of happens, Google's announced.
[02:22:27.680 --> 02:22:31.040]   These, some of them may be in progress
[02:22:31.040 --> 02:22:32.680]   and some of them already rolled out.
[02:22:32.680 --> 02:22:34.520]   - Noise cancellation and meet, that's good.
[02:22:34.520 --> 02:22:35.680]   - Yeah, noise cancellation.
[02:22:35.680 --> 02:22:39.960]   - Easy F animation and drive, emoji kitchen combos.
[02:22:39.960 --> 02:22:41.440]   - A fast pair come into Chromebooks.
[02:22:41.440 --> 02:22:42.600]   This was announced.
[02:22:42.600 --> 02:22:43.600]   - Oh yeah, that had a while ago.
[02:22:43.600 --> 02:22:45.000]   - This is for Jeff, yeah.
[02:22:45.000 --> 02:22:45.840]   - That's fine.
[02:22:45.840 --> 02:22:46.760]   - Kind of like what happens on a Mac
[02:22:46.760 --> 02:22:48.520]   where you open up your Bluetooth headphones
[02:22:48.520 --> 02:22:49.520]   and it just goes, I see that.
[02:22:49.520 --> 02:22:52.080]   - I mean, you use, I mean, with a phone,
[02:22:52.080 --> 02:22:54.520]   I interact with fast pair a lot
[02:22:54.520 --> 02:22:56.560]   and it's awesome when it works.
[02:22:56.560 --> 02:22:57.680]   - Yep.
[02:22:57.680 --> 02:22:59.160]   - And it works pretty well now.
[02:22:59.160 --> 02:23:00.240]   So, okay.
[02:23:00.240 --> 02:23:03.160]   - Awesome when it works.
[02:23:03.160 --> 02:23:04.000]   - That's right.
[02:23:05.560 --> 02:23:07.440]   - Am I doing the rest of these or?
[02:23:07.440 --> 02:23:08.280]   - I'm happy to do it.
[02:23:08.280 --> 02:23:10.400]   - Well, you know, you just checked off three out of the list
[02:23:10.400 --> 02:23:11.880]   which is good.
[02:23:11.880 --> 02:23:14.280]   Androids adding support for, why don't we trade it off?
[02:23:14.280 --> 02:23:15.440]   I'll do one, you do one.
[02:23:15.440 --> 02:23:19.520]   Androids adding support for ECIM transfer between devices.
[02:23:19.520 --> 02:23:20.560]   I didn't know it didn't have that.
[02:23:20.560 --> 02:23:25.040]   ECIM is really a revolution in how smartphones work.
[02:23:25.040 --> 02:23:27.160]   So that means you can have an account.
[02:23:27.160 --> 02:23:30.040]   In fact, I've kind of did it already.
[02:23:30.040 --> 02:23:33.880]   I had an account on my Pixel 6
[02:23:33.880 --> 02:23:35.880]   and I got the Samsung Ultra.
[02:23:35.880 --> 02:23:40.200]   And in Siri, I could have ECIM just transferred over.
[02:23:40.200 --> 02:23:43.840]   And then you've got the Samsung for review.
[02:23:43.840 --> 02:23:45.800]   So I got to transfer it back, that kind of thing.
[02:23:45.800 --> 02:23:46.640]   - Right.
[02:23:46.640 --> 02:23:48.000]   - But instead we have the physical one.
[02:23:48.000 --> 02:23:48.920]   So we end up using that.
[02:23:48.920 --> 02:23:50.480]   - It's all done in software.
[02:23:50.480 --> 02:23:51.560]   - Yeah.
[02:23:51.560 --> 02:23:52.400]   - Yeah, it's all done in software.
[02:23:52.400 --> 02:23:55.360]   - And you know, actually, if you have Google Fi account,
[02:23:55.360 --> 02:23:57.560]   it kind of does that for you anyway.
[02:23:57.560 --> 02:23:59.320]   When I got the Samsung Ultra, it says,
[02:23:59.320 --> 02:24:01.320]   complete the activation install Fi.
[02:24:01.320 --> 02:24:02.160]   - Oh.
[02:24:02.160 --> 02:24:03.000]   - 'Cause I'm a Fi user.
[02:24:03.000 --> 02:24:04.440]   - It's a SIM, that's nice.
[02:24:04.440 --> 02:24:05.440]   Yeah, I've got Mint.
[02:24:05.440 --> 02:24:07.120]   So I've got the Fi.
[02:24:07.120 --> 02:24:08.720]   - They also support ECIM though.
[02:24:08.720 --> 02:24:09.560]   They can do this now.
[02:24:09.560 --> 02:24:10.400]   - No, they have ECIM.
[02:24:10.400 --> 02:24:11.560]   I have ECIM for Mint.
[02:24:11.560 --> 02:24:12.400]   - For, I mean--
[02:24:12.400 --> 02:24:13.320]   - For sponsor.
[02:24:13.320 --> 02:24:16.040]   - For whatever reason, I feel more comfortable
[02:24:16.040 --> 02:24:17.120]   having the physical SIM.
[02:24:17.120 --> 02:24:18.440]   You know, it's just so easy to look at it
[02:24:18.440 --> 02:24:20.600]   and be like, I want this in the light.
[02:24:20.600 --> 02:24:21.440]   - Boom.
[02:24:21.440 --> 02:24:22.360]   I'm done.
[02:24:22.360 --> 02:24:23.720]   - You're such a lotite.
[02:24:23.720 --> 02:24:24.560]   - I know, right?
[02:24:24.560 --> 02:24:27.280]   I'm becoming a lotite as I get older, apparently.
[02:24:27.280 --> 02:24:29.360]   Let's see here.
[02:24:29.360 --> 02:24:30.680]   This one I'm not super familiar with.
[02:24:30.680 --> 02:24:35.680]   Chrome has improved memory use and battery life on the MacBook.
[02:24:35.680 --> 02:24:39.000]   - We talked about this yesterday on Mac break weekly.
[02:24:39.000 --> 02:24:40.760]   Chrome is notorious pig.
[02:24:40.760 --> 02:24:41.840]   - Well, yes it is.
[02:24:41.840 --> 02:24:42.680]   - On Mac.
[02:24:42.680 --> 02:24:43.520]   - No question.
[02:24:43.520 --> 02:24:44.360]   - And so they're trying to get a little--
[02:24:44.360 --> 02:24:45.680]   - On those two.
[02:24:45.680 --> 02:24:46.520]   - Is it really?
[02:24:46.520 --> 02:24:47.360]   - Yeah.
[02:24:47.360 --> 02:24:49.200]   - So they talked about some things like the tabs
[02:24:49.200 --> 02:24:52.560]   they can sleep them faster, I frames they'll sleep.
[02:24:52.560 --> 02:24:56.360]   There is a power setting on Chrome,
[02:24:56.360 --> 02:24:57.800]   which only gains you 30 minutes,
[02:24:57.800 --> 02:25:01.040]   but they claim on a MacBook Pro,
[02:25:01.040 --> 02:25:05.120]   14 inch you can get a 17 hours of video time.
[02:25:05.120 --> 02:25:06.120]   - Okay. - That's nice.
[02:25:06.120 --> 02:25:06.960]   - Nice.
[02:25:06.960 --> 02:25:08.400]   - Watching videos for 17 hours.
[02:25:08.400 --> 02:25:09.720]   - All right.
[02:25:09.720 --> 02:25:10.560]   - That's about as long as--
[02:25:10.560 --> 02:25:12.360]   - For all those times that I'm watching videos
[02:25:12.360 --> 02:25:13.200]   for 17 hours, yeah.
[02:25:13.200 --> 02:25:14.920]   - As long as a baseball game, I think.
[02:25:14.920 --> 02:25:15.760]   - It's what it's trying to do.
[02:25:15.760 --> 02:25:17.680]   - Little bit longer.
[02:25:17.680 --> 02:25:18.520]   - Just a smidge.
[02:25:18.520 --> 02:25:20.640]   - Waymo's starting autonomous testing in LA
[02:25:20.640 --> 02:25:21.920]   with no human driver.
[02:25:21.920 --> 02:25:24.760]   Waymo, didn't they announce, was it Waymo?
[02:25:24.760 --> 02:25:27.280]   No, it was Cruz that announced a million hours
[02:25:27.280 --> 02:25:28.760]   without a--
[02:25:28.760 --> 02:25:29.680]   - Yeah, it was Cruz.
[02:25:29.680 --> 02:25:30.720]   - That's amazing.
[02:25:30.720 --> 02:25:33.440]   Everywhere now you start.
[02:25:33.440 --> 02:25:36.080]   So I said earlier in the show that full self-driving
[02:25:36.080 --> 02:25:39.360]   was a kind of AI that hasn't taken off,
[02:25:39.360 --> 02:25:40.800]   but I guess it really has.
[02:25:40.800 --> 02:25:41.880]   - In some ways it is.
[02:25:41.880 --> 02:25:42.760]   - Yeah.
[02:25:42.760 --> 02:25:45.800]   - Not taken off in the way of we're all
[02:25:45.800 --> 02:25:48.080]   riding around in vehicles every day
[02:25:48.080 --> 02:25:49.800]   that are driving themselves,
[02:25:49.800 --> 02:25:51.480]   but this is where it begins, right?
[02:25:51.480 --> 02:25:53.120]   This is where that trend begins.
[02:25:53.120 --> 02:25:55.160]   - Oh, was Waymo, one million miles
[02:25:55.160 --> 02:25:56.520]   with no human driver.
[02:25:56.520 --> 02:25:58.480]   - On public roads.
[02:25:58.480 --> 02:26:02.800]   So that's a lot, they have even more
[02:26:02.800 --> 02:26:06.280]   with a safety driver, 8 million miles.
[02:26:06.280 --> 02:26:10.440]   Now they also revealed that they'd had a couple of accidents,
[02:26:10.440 --> 02:26:12.720]   bunch of fender benders,
[02:26:12.720 --> 02:26:14.120]   but for a million miles,
[02:26:14.120 --> 02:26:15.760]   that's a lot better than a human.
[02:26:15.760 --> 02:26:16.600]   - Absolutely.
[02:26:16.600 --> 02:26:19.400]   - Why doesn't Waymo get as much press as Tesla?
[02:26:19.400 --> 02:26:23.920]   'Cause you can buy a Tesla.
[02:26:23.920 --> 02:26:26.640]   - You can't, Waymo's limited to a few markets,
[02:26:26.640 --> 02:26:29.600]   Phoenix, San Francisco, LA.
[02:26:29.600 --> 02:26:32.560]   And so that's why I think most people
[02:26:32.560 --> 02:26:33.400]   don't have any experience.
[02:26:33.400 --> 02:26:35.200]   I've never ridden a Waymo, have you?
[02:26:35.200 --> 02:26:36.040]   - Nope.
[02:26:36.040 --> 02:26:36.880]   - You've never been in a way.
[02:26:36.880 --> 02:26:38.200]   - But you raise your way to an important point,
[02:26:38.200 --> 02:26:41.200]   'cause Waymo actually does it.
[02:26:41.200 --> 02:26:43.360]   - When Tesla's doing promising it.
[02:26:43.360 --> 02:26:45.720]   - Well Tesla's kind of admitted in filings
[02:26:45.720 --> 02:26:48.120]   that it's what so-called full self-driving
[02:26:48.120 --> 02:26:51.960]   is level two driver assist, which isn't really much.
[02:26:51.960 --> 02:26:52.800]   - Yeah.
[02:26:52.800 --> 02:26:54.920]   So we now know that's just a marketing term
[02:26:54.920 --> 02:26:55.760]   when Tesla says-
[02:26:55.760 --> 02:26:57.400]   - You know, we talk about regulation
[02:26:57.400 --> 02:27:01.040]   and I sound like a libertarian in these moments when we do.
[02:27:01.040 --> 02:27:02.800]   If you're gonna regulate anything,
[02:27:02.800 --> 02:27:04.480]   self-driving cars seems to be the thing
[02:27:04.480 --> 02:27:06.760]   you'd want to regulate from the get-go.
[02:27:06.760 --> 02:27:07.600]   - Yeah.
[02:27:07.600 --> 02:27:09.400]   - Well they are technically.
[02:27:09.400 --> 02:27:11.440]   It's just the way Musk gets away with stuff.
[02:27:11.440 --> 02:27:14.160]   - They haven't done an aggressive term crap on.
[02:27:14.160 --> 02:27:15.720]   - A lot of people complain about NHTSA,
[02:27:15.720 --> 02:27:17.200]   which sets the national standards
[02:27:17.200 --> 02:27:19.440]   that they've kind of national highway transportation
[02:27:19.440 --> 02:27:22.960]   and safety administration has kind of given up to now,
[02:27:22.960 --> 02:27:26.000]   given Tesla a free pass, not so much these days.
[02:27:26.000 --> 02:27:27.920]   They're getting more serious about it.
[02:27:27.920 --> 02:27:30.840]   But they should never been allowed to call it autopilot.
[02:27:30.840 --> 02:27:31.680]   - Mm-mm.
[02:27:31.680 --> 02:27:33.160]   - That's not good.
[02:27:33.160 --> 02:27:34.000]   - No, agreed.
[02:27:34.000 --> 02:27:35.640]   What else you got?
[02:27:35.640 --> 02:27:39.040]   - Magic Eraser, defining feature of the Pixel devices,
[02:27:39.040 --> 02:27:42.160]   coming to Google One subscribers via the photos,
[02:27:42.160 --> 02:27:44.960]   the Google Photos app, so it's not limited to-
[02:27:44.960 --> 02:27:45.960]   - Every party gets-
[02:27:45.960 --> 02:27:48.600]   - Devices with a tensor chip.
[02:27:48.600 --> 02:27:50.080]   - You get magic eraser.
[02:27:50.080 --> 02:27:51.600]   - That's right. - And you get magic eraser.
[02:27:51.600 --> 02:27:55.640]   Even iPhone users on photos get magic eraser.
[02:27:55.640 --> 02:27:57.720]   - You can remove things. - Do we have it yet?
[02:27:57.720 --> 02:27:58.600]   - Should I try it?
[02:27:58.600 --> 02:27:59.920]   - Oh.
[02:27:59.920 --> 02:28:02.280]   - It says starting on Thursday, Thursday.
[02:28:02.280 --> 02:28:04.640]   - Thursday. - It's gonna be rolling out
[02:28:04.640 --> 02:28:08.160]   to Google One subscribers using the Google Photos app.
[02:28:08.160 --> 02:28:10.040]   But yeah, that's Android, that's iOS.
[02:28:10.040 --> 02:28:12.960]   All Pixel users, not just as it's been limited
[02:28:12.960 --> 02:28:15.040]   to the six and the seven so far.
[02:28:15.040 --> 02:28:16.120]   - So nice.
[02:28:16.120 --> 02:28:17.880]   - There you go. - Very cool.
[02:28:17.880 --> 02:28:21.400]   - Remove items from your photos to your heart's content.
[02:28:21.400 --> 02:28:25.760]   - It's a feature that I never use.
[02:28:25.760 --> 02:28:26.600]   - Does it work? - But-
[02:28:26.600 --> 02:28:27.440]   - No, you don't use it.
[02:28:27.440 --> 02:28:28.680]   - I mean, I never think to use,
[02:28:28.680 --> 02:28:31.360]   I just don't spend a whole lot of time editing photos
[02:28:31.360 --> 02:28:33.040]   on my phone, I guess.
[02:28:33.040 --> 02:28:34.960]   - It works fairly well to be-
[02:28:34.960 --> 02:28:36.720]   - Lightroom does this affinity photo.
[02:28:36.720 --> 02:28:39.240]   A lot of programs do it, right?
[02:28:39.240 --> 02:28:40.240]   - Yes, sir.
[02:28:40.240 --> 02:28:43.880]   I would still lean on my trusty Photoshop for some stuff,
[02:28:43.880 --> 02:28:46.440]   but I've played around with it recently.
[02:28:46.440 --> 02:28:50.360]   And like say I wanted to take a picture of the living room
[02:28:50.360 --> 02:28:52.640]   or something, but Kylo was sitting there.
[02:28:52.640 --> 02:28:57.320]   So I'll erase Kylo, but Kylo's shadow is still there.
[02:28:57.320 --> 02:28:58.400]   So now I need to go back in.
[02:28:58.400 --> 02:29:00.080]   - I need to erase the shadow.
[02:29:00.080 --> 02:29:02.560]   - And then you're hoping that it gets that right
[02:29:02.560 --> 02:29:04.120]   and sometimes it doesn't always do it,
[02:29:04.120 --> 02:29:07.640]   but the average consumer would totally be fine
[02:29:07.640 --> 02:29:10.160]   with the results that happen.
[02:29:10.160 --> 02:29:11.040]   With me on the other hand,
[02:29:11.040 --> 02:29:12.680]   I'm probably just gonna go into Photoshop
[02:29:12.680 --> 02:29:15.600]   and fix what I need to fix.
[02:29:15.600 --> 02:29:16.440]   - Yeah.
[02:29:16.440 --> 02:29:20.720]   - You know, I have to say it'll be on iPhones,
[02:29:20.720 --> 02:29:23.080]   but, and maybe this'll be fixed,
[02:29:23.080 --> 02:29:26.520]   but Google Photos can't handle the iPhone format.
[02:29:26.520 --> 02:29:27.360]   The hike-
[02:29:27.360 --> 02:29:28.360]   - Oh, the heif-
[02:29:28.360 --> 02:29:29.360]   - Oh, the heif-
[02:29:29.360 --> 02:29:30.200]   - Heif-
[02:29:30.200 --> 02:29:31.040]   - Staying.
[02:29:31.040 --> 02:29:31.880]   - High efficiency.
[02:29:31.880 --> 02:29:33.040]   - High efficiency, I don't think it can.
[02:29:33.040 --> 02:29:34.480]   Oh, wait a minute, as a Google One member,
[02:29:34.480 --> 02:29:38.120]   you get access to extra editing features.
[02:29:38.120 --> 02:29:40.680]   So maybe they did add that capability.
[02:29:40.680 --> 02:29:41.520]   - Maybe it's their own-
[02:29:41.520 --> 02:29:45.160]   - And a quick tip, the magic eraser,
[02:29:45.160 --> 02:29:46.880]   depending on what you're trying to erase,
[02:29:46.880 --> 02:29:51.320]   it works best when you zoom in and then do the eraser.
[02:29:51.320 --> 02:29:52.800]   - Oh, okay.
[02:29:52.800 --> 02:29:53.720]   - Oh.
[02:29:53.720 --> 02:29:56.520]   - Yeah, I have a bunch of stuff 'cause I'm a Google One subscriber,
[02:29:56.520 --> 02:29:58.080]   but not yet magic eraser.
[02:29:58.080 --> 02:30:00.680]   I'm coming tomorrow, maybe.
[02:30:00.680 --> 02:30:01.960]   - Yeah.
[02:30:01.960 --> 02:30:02.800]   - Nice.
[02:30:02.800 --> 02:30:03.640]   - But dude, I already had that,
[02:30:03.640 --> 02:30:05.400]   didn't I already have that on the Pixel?
[02:30:05.400 --> 02:30:06.240]   - Yeah, I was already-
[02:30:06.240 --> 02:30:07.480]   - 'Cause you would already have it on your Pixel.
[02:30:07.480 --> 02:30:08.320]   - Yeah.
[02:30:08.320 --> 02:30:09.160]   - Yeah.
[02:30:09.160 --> 02:30:10.400]   - The 7 and the 6, the 6A.
[02:30:10.400 --> 02:30:14.160]   You know, and on the Pixel,
[02:30:14.160 --> 02:30:16.400]   I think it's integrated into the camera,
[02:30:16.400 --> 02:30:18.240]   not necessarily the Photos app.
[02:30:18.240 --> 02:30:21.400]   - Limited raw, so we're already even-
[02:30:21.400 --> 02:30:22.240]   - Yeah, for the edge.
[02:30:22.240 --> 02:30:23.480]   - I shoot a lot in raw,
[02:30:23.480 --> 02:30:25.640]   I probably shouldn't shoot so much in raw.
[02:30:25.640 --> 02:30:30.720]   - You gotta be eating up your Photos cloud storage.
[02:30:30.720 --> 02:30:32.080]   - Ah, what the hell?
[02:30:32.080 --> 02:30:34.880]   You know what, I still have one of those things.
[02:30:34.880 --> 02:30:35.720]   - It's one of those things.
[02:30:35.720 --> 02:30:38.160]   - We have the raw capability on Amazon Photos,
[02:30:38.160 --> 02:30:40.120]   so I just upload to Amazon as well.
[02:30:40.120 --> 02:30:40.960]   - Well, there you go.
[02:30:40.960 --> 02:30:45.400]   And I think I don't have to upload the originals,
[02:30:45.400 --> 02:30:47.440]   or I can't upload the originals on Photos, right?
[02:30:47.440 --> 02:30:50.680]   It has unlimited, you have to turn it on if you-
[02:30:50.680 --> 02:30:51.680]   - It's well.
[02:30:51.680 --> 02:30:52.840]   - Yeah.
[02:30:52.840 --> 02:30:57.840]   - Yeah, Google might try and opt you into original size
[02:30:57.840 --> 02:30:59.920]   by default the first time you launch the app.
[02:30:59.920 --> 02:31:00.920]   - Right, so you'd have to say-
[02:31:00.920 --> 02:31:02.040]   - So you just, you know,
[02:31:02.040 --> 02:31:04.280]   whether to do that or I think they call it high quality,
[02:31:04.280 --> 02:31:07.640]   which is like a lower res, but high enough, in my opinion.
[02:31:07.640 --> 02:31:10.040]   How often do I have full of photos out of my photo reel
[02:31:10.040 --> 02:31:12.360]   and blowing it up to a wall size image?
[02:31:12.360 --> 02:31:13.200]   Like I'm not doing that.
[02:31:13.200 --> 02:31:14.040]   - Exactly.
[02:31:14.040 --> 02:31:15.080]   - So who cares?
[02:31:15.080 --> 02:31:17.360]   - Magic eraser, there it is, yeah.
[02:31:17.360 --> 02:31:19.920]   Tap to erase highlighted suggestions.
[02:31:19.920 --> 02:31:23.360]   Click or brush to erase more.
[02:31:23.360 --> 02:31:24.200]   - Yeah.
[02:31:24.200 --> 02:31:25.040]   - It doesn't have any suggestions
[02:31:25.040 --> 02:31:26.440]   'cause this is a perfect fit.
[02:31:26.440 --> 02:31:27.680]   - It's like this is amazing.
[02:31:27.680 --> 02:31:29.400]   - Let's erase my wife.
[02:31:29.400 --> 02:31:31.320]   - I dare you to try to erase her.
[02:31:31.320 --> 02:31:32.160]   I dare you.
[02:31:32.160 --> 02:31:34.200]   No, don't do that.
[02:31:34.200 --> 02:31:35.040]   No, no, no, no, no.
[02:31:35.040 --> 02:31:36.880]   Oh boy, that one just circle it.
[02:31:36.880 --> 02:31:38.240]   - Oh, just circle it?
[02:31:38.240 --> 02:31:41.440]   - For that instance, circling would work totally fine.
[02:31:41.440 --> 02:31:43.240]   - Oh, 'cause it does.
[02:31:43.240 --> 02:31:44.240]   - We'll see what it does.
[02:31:44.240 --> 02:31:45.880]   - Oh, it's a ghost.
[02:31:45.880 --> 02:31:47.320]   - She's almost gone.
[02:31:47.320 --> 02:31:49.040]   What am I gonna do?
[02:31:49.040 --> 02:31:50.960]   Okay, so circle it.
[02:31:50.960 --> 02:31:52.120]   - Yeah.
[02:31:52.120 --> 02:31:55.720]   - And then, well, that's, well.
[02:31:55.720 --> 02:31:56.960]   But now I can do something.
[02:31:56.960 --> 02:31:57.880]   Would I do additional?
[02:31:57.880 --> 02:31:59.880]   - And then you find to, but brush in things.
[02:31:59.880 --> 02:32:00.720]   - Yeah, yeah, yeah.
[02:32:00.720 --> 02:32:01.560]   - Oh, look at that.
[02:32:01.560 --> 02:32:04.660]   It's like she never existed.
[02:32:04.660 --> 02:32:07.720]   (laughing)
[02:32:07.720 --> 02:32:08.560]   Sorry Lisa.
[02:32:08.560 --> 02:32:10.040]   - Right now she's doing the same to you.
[02:32:10.040 --> 02:32:11.200]   - She's mad now.
[02:32:11.200 --> 02:32:12.520]   She's not gonna be happy about this.
[02:32:12.520 --> 02:32:13.880]   - You should be ducking sir.
[02:32:13.880 --> 02:32:14.880]   She's erasing your phone though.
[02:32:14.880 --> 02:32:16.080]   - We have a bad number from her phone.
[02:32:16.080 --> 02:32:16.920]   - Look at that.
[02:32:16.920 --> 02:32:18.920]   There's her shadow, but there's no.
[02:32:18.920 --> 02:32:19.760]   - Wow.
[02:32:19.760 --> 02:32:21.720]   That's cool.
[02:32:21.720 --> 02:32:22.560]   Yeah, there you go.
[02:32:22.560 --> 02:32:24.360]   - Cancel, I didn't wanna do that.
[02:32:24.360 --> 02:32:27.480]   Discard, I love my beautiful.
[02:32:27.480 --> 02:32:28.960]   But there are plenty of pictures
[02:32:28.960 --> 02:32:32.360]   what I would like to erase some people in.
[02:32:32.360 --> 02:32:34.120]   Oh, she told me, okay.
[02:32:34.120 --> 02:32:36.080]   She said this is a good picture.
[02:32:36.080 --> 02:32:38.680]   These two guys with their solo cups on the rocks,
[02:32:38.680 --> 02:32:41.160]   but she said I should erase this guy.
[02:32:41.160 --> 02:32:42.000]   - Yeah.
[02:32:42.000 --> 02:32:42.840]   - And just keep the other one.
[02:32:42.840 --> 02:32:43.680]   - So just circle.
[02:32:43.680 --> 02:32:44.520]   - Yeah, let's try it.
[02:32:44.520 --> 02:32:45.840]   Let's try it.
[02:32:45.840 --> 02:32:47.240]   They don't know they're being erased.
[02:32:47.240 --> 02:32:48.080]   - Zoom in first.
[02:32:48.080 --> 02:32:48.920]   - Zoom in first.
[02:32:48.920 --> 02:32:50.400]   - Zoom in just a little one.
[02:32:50.400 --> 02:32:51.920]   - But at first I have to pick it, right?
[02:32:51.920 --> 02:32:53.440]   It's under two holes.
[02:32:53.440 --> 02:32:55.280]   Magic eraser.
[02:32:55.280 --> 02:32:57.440]   Okay, it's gonna suggest, I does no suggestion,
[02:32:57.440 --> 02:32:58.440]   but now I wanna zoom in.
[02:32:58.440 --> 02:32:59.280]   - And just a little.
[02:32:59.280 --> 02:33:00.200]   - And just erase.
[02:33:00.200 --> 02:33:01.520]   - Yeah, and then circle.
[02:33:01.520 --> 02:33:04.720]   'Cause you wanted to still think about the--
[02:33:04.720 --> 02:33:05.560]   - He didn't know he'd be at it.
[02:33:05.560 --> 02:33:07.720]   - Oh my God.
[02:33:07.720 --> 02:33:08.560]   - Yeah.
[02:33:08.560 --> 02:33:09.400]   - Where did he go?
[02:33:09.400 --> 02:33:11.480]   - Would you even know if he didn't know?
[02:33:11.480 --> 02:33:12.640]   - Oops, undo that one.
[02:33:12.640 --> 02:33:15.360]   - So now when you zoom out and look at it,
[02:33:15.360 --> 02:33:17.520]   it's like he never was there.
[02:33:17.520 --> 02:33:18.360]   That was awful.
[02:33:18.360 --> 02:33:20.680]   - Right, you should have named the tool Stalin.
[02:33:20.680 --> 02:33:22.120]   - Yeah, Stalin.
[02:33:22.120 --> 02:33:22.960]   Look at that.
[02:33:22.960 --> 02:33:24.960]   Now it's actually, it is a better picture.
[02:33:24.960 --> 02:33:28.120]   - Yeah, so for most people, that's totally gonna work.
[02:33:28.120 --> 02:33:29.520]   - No kidding.
[02:33:29.520 --> 02:33:30.840]   - Fake news.
[02:33:30.840 --> 02:33:33.120]   - Wow.
[02:33:33.120 --> 02:33:33.960]   Keep fake.
[02:33:33.960 --> 02:33:35.880]   - Well now what I could do is save a copy.
[02:33:35.880 --> 02:33:36.720]   - Yeah.
[02:33:36.720 --> 02:33:38.120]   - So I have the original.
[02:33:38.120 --> 02:33:38.960]   Wow.
[02:33:38.960 --> 02:33:40.840]   - So you have the original with that random guy.
[02:33:40.840 --> 02:33:41.680]   - So I do it.
[02:33:41.680 --> 02:33:42.840]   And then let's see.
[02:33:42.840 --> 02:33:43.800]   There's two guys there.
[02:33:43.800 --> 02:33:45.280]   - There's random guy.
[02:33:45.280 --> 02:33:46.120]   - Two guys there.
[02:33:46.120 --> 02:33:47.440]   - Yup.
[02:33:47.440 --> 02:33:48.600]   - And go here?
[02:33:48.600 --> 02:33:49.440]   - No.
[02:33:49.440 --> 02:33:50.760]   - One go more.
[02:33:50.760 --> 02:33:51.600]   - Random guy.
[02:33:51.600 --> 02:33:52.440]   - That's pretty good.
[02:33:52.440 --> 02:33:53.280]   - Yeah, okay.
[02:33:53.280 --> 02:33:54.120]   - That's pretty easy.
[02:33:54.120 --> 02:33:54.960]   - That's nice.
[02:33:54.960 --> 02:33:55.880]   Now you'll be able to do that
[02:33:55.880 --> 02:33:57.520]   if you're the one who will be able to do that.
[02:33:57.520 --> 02:33:58.920]   - That's right, yeah.
[02:33:58.920 --> 02:34:01.360]   - Gmail, client sign encryption now available
[02:34:01.360 --> 02:34:02.200]   in more businesses.
[02:34:02.200 --> 02:34:03.480]   Don't get your hopes up.
[02:34:03.480 --> 02:34:06.920]   This is not encrypted email, right?
[02:34:06.920 --> 02:34:11.040]   Although it does hide it from Google.
[02:34:11.040 --> 02:34:14.160]   The feature makes it so even Google itself
[02:34:14.160 --> 02:34:16.480]   can't see the contents of the emails it's hosting
[02:34:16.480 --> 02:34:18.400]   but they do being encrypted before.
[02:34:18.400 --> 02:34:19.840]   - But it's not plausible.
[02:34:19.840 --> 02:34:20.840]   - It's not a high ability.
[02:34:20.840 --> 02:34:21.680]   - Yeah.
[02:34:21.680 --> 02:34:22.760]   (laughing)
[02:34:22.760 --> 02:34:25.840]   - Customers have sole control over their encryption keys.
[02:34:25.840 --> 02:34:28.280]   So that's, yeah, you know what, that's good.
[02:34:28.280 --> 02:34:30.160]   And users can encrypt emails they're sending
[02:34:30.160 --> 02:34:31.160]   within their organization
[02:34:31.160 --> 02:34:33.960]   as well as emails they're sending to other parties
[02:34:33.960 --> 02:34:36.800]   even if the recipient doesn't use Gmail.
[02:34:36.800 --> 02:34:39.400]   Well, I wonder what they're using for the encryption.
[02:34:39.400 --> 02:34:42.440]   Well, interesting.
[02:34:42.440 --> 02:34:46.040]   Client sign encryption for Gmail
[02:34:46.040 --> 02:34:49.480]   and even Google can't see what you're doing.
[02:34:49.480 --> 02:34:51.920]   That's a Google workspace feature.
[02:34:51.920 --> 02:34:55.160]   - I think that's good, right?
[02:34:55.160 --> 02:34:56.000]   - Of course.
[02:34:56.000 --> 02:34:56.920]   - Turn that sucker on.
[02:34:56.920 --> 02:34:58.840]   - Yeah, until encryption is illegal.
[02:34:58.840 --> 02:35:01.040]   Let's go all in on it.
[02:35:01.040 --> 02:35:02.960]   - And I think people know this.
[02:35:02.960 --> 02:35:06.600]   The data is encrypted at rest on Google servers
[02:35:06.600 --> 02:35:10.240]   and in transit between Gmail accounts
[02:35:10.240 --> 02:35:13.360]   but the keys aren't held by you, they're held by Google.
[02:35:13.360 --> 02:35:14.880]   So if they were subpoenaed or whatever
[02:35:14.880 --> 02:35:16.480]   they still would have access to it.
[02:35:16.480 --> 02:35:18.400]   So all they're really doing is handing the keys
[02:35:18.400 --> 02:35:19.800]   over to the client and saying,
[02:35:19.800 --> 02:35:20.640]   "We don't have them.
[02:35:20.640 --> 02:35:23.080]   "We don't have them out of our hands."
[02:35:23.080 --> 02:35:25.960]   - Well, speaking of your hands
[02:35:25.960 --> 02:35:27.840]   and things being in or out of them,
[02:35:27.840 --> 02:35:30.040]   the Pixel watch, that was a horrible segue.
[02:35:31.040 --> 02:35:32.880]   We'll have fun.
[02:35:32.880 --> 02:35:33.880]   - Thank you for effort, Jason.
[02:35:33.880 --> 02:35:35.080]   - Thank you.
[02:35:35.080 --> 02:35:37.480]   - Sometimes it feels right to try and stay away.
[02:35:37.480 --> 02:35:41.600]   Fall detection coming to your Pixel watch.
[02:35:41.600 --> 02:35:43.360]   This was something that we've known about for a while.
[02:35:43.360 --> 02:35:44.200]   - You could have used that,
[02:35:44.200 --> 02:35:45.600]   getting out of the studio, Leo.
[02:35:45.600 --> 02:35:46.440]   - Yeah.
[02:35:46.440 --> 02:35:48.360]   - You know, my Apple watch did not say,
[02:35:48.360 --> 02:35:49.800]   you just taken a fall.
[02:35:49.800 --> 02:35:51.440]   But the other day I was making the bed
[02:35:51.440 --> 02:35:53.120]   and it said, "Did you just fall?"
[02:35:53.120 --> 02:35:55.320]   No, I was fluffing.
[02:35:55.320 --> 02:35:57.000]   I was merely fluffing.
[02:35:59.000 --> 02:36:00.880]   So the worst thing is,
[02:36:00.880 --> 02:36:02.480]   so at first we'll say, did you just fall?
[02:36:02.480 --> 02:36:04.840]   If you don't see it or feel of the buzz,
[02:36:04.840 --> 02:36:06.480]   it will then go, "Whoo!"
[02:36:06.480 --> 02:36:07.400]   - Whoo!
[02:36:07.400 --> 02:36:08.560]   Calling 911!
[02:36:08.560 --> 02:36:09.400]   No, whoo!
[02:36:09.400 --> 02:36:10.240]   - Stop!
[02:36:10.240 --> 02:36:13.120]   - That's really, so far I've been able to stop it,
[02:36:13.120 --> 02:36:14.840]   but it happens more than not to.
[02:36:14.840 --> 02:36:16.400]   - Kind of frightening, actually.
[02:36:16.400 --> 02:36:17.880]   - What's the push-ups you do,
[02:36:17.880 --> 02:36:20.360]   where you have barbells and you lift one
[02:36:20.360 --> 02:36:23.680]   and then you put it in and you do the other one like that?
[02:36:23.680 --> 02:36:25.520]   - That's you doing a bent over row.
[02:36:25.520 --> 02:36:26.360]   - Something like that.
[02:36:26.360 --> 02:36:27.200]   - Something like that.
[02:36:27.200 --> 02:36:28.040]   - Something with it.
[02:36:28.040 --> 02:36:31.200]   And that's the one that sets it off every time I put down the,
[02:36:31.200 --> 02:36:33.840]   when I put down the dumbbell and it goes--
[02:36:33.840 --> 02:36:35.240]   - Working on those lats.
[02:36:35.240 --> 02:36:37.160]   - Yeah, get my lats.
[02:36:37.160 --> 02:36:40.320]   - I was just fluffing Google.
[02:36:40.320 --> 02:36:41.640]   Okay, so now you too,
[02:36:41.640 --> 02:36:43.720]   and Pixel Watch owners will have that fine experience.
[02:36:43.720 --> 02:36:44.640]   - There you go.
[02:36:44.640 --> 02:36:48.120]   - Finally, your Google Docs are about to look
[02:36:48.120 --> 02:36:49.160]   a little bit different.
[02:36:49.160 --> 02:36:50.600]   Did you put this in here?
[02:36:50.600 --> 02:36:52.400]   Is that a change log really?
[02:36:52.400 --> 02:36:53.240]   - I don't know.
[02:36:53.240 --> 02:36:54.080]   Sometimes I don't know.
[02:36:54.080 --> 02:36:55.720]   - A small refresh.
[02:36:55.720 --> 02:36:56.640]   - Oh, okay.
[02:36:56.640 --> 02:36:57.960]   - Well, yeah.
[02:36:57.960 --> 02:36:58.960]   - Little material three is--
[02:36:58.960 --> 02:37:00.640]   - I could've put that in early on
[02:37:00.640 --> 02:37:02.800]   when I didn't have many things and then--
[02:37:02.800 --> 02:37:03.640]   - That's probably it.
[02:37:03.640 --> 02:37:04.480]   - I didn't trim it out.
[02:37:04.480 --> 02:37:05.320]   That's what I'm gonna leave it on.
[02:37:05.320 --> 02:37:07.800]   - And that's the Google change log.
[02:37:07.800 --> 02:37:08.800]   (dramatic music)
[02:37:08.800 --> 02:37:10.040]   Now you know.
[02:37:10.040 --> 02:37:11.000]   Now you know, it's him.
[02:37:11.000 --> 02:37:12.560]   It's all him.
[02:37:12.560 --> 02:37:13.400]   - It's all me.
[02:37:13.400 --> 02:37:14.240]   - It's me.
[02:37:14.240 --> 02:37:15.080]   - It's just the good word.
[02:37:15.080 --> 02:37:17.120]   - And then sometimes you go off the rails
[02:37:17.120 --> 02:37:20.120]   and you take chat room suggestions.
[02:37:20.120 --> 02:37:22.720]   - We don't, for some reason, we wait a minute.
[02:37:22.720 --> 02:37:25.120]   We have a scooter exchange log.
[02:37:25.120 --> 02:37:25.960]   I don't know.
[02:37:25.960 --> 02:37:26.800]   - It's pretty active.
[02:37:26.800 --> 02:37:29.000]   - Bullet group, model file, defy satellite.
[02:37:29.000 --> 02:37:32.040]   Think motor roller defy satellite link
[02:37:32.040 --> 02:37:35.640]   unveiled at Motor World Congress.
[02:37:35.640 --> 02:37:37.360]   - So just for the chat room,
[02:37:37.360 --> 02:37:39.600]   change log has to be Google related.
[02:37:39.600 --> 02:37:40.440]   That's the--
[02:37:40.440 --> 02:37:41.400]   - That's the motor roller, no count.
[02:37:41.400 --> 02:37:43.040]   - Yeah, motor roller, it's not a Google--
[02:37:43.040 --> 02:37:44.520]   - No, I didn't say more.
[02:37:44.520 --> 02:37:47.080]   - You should be. - Yeah, not anymore, right.
[02:37:47.080 --> 02:37:49.600]   - Oh wait a minute, here it is.
[02:37:49.600 --> 02:37:50.440]   He's got a bunch in there.
[02:37:50.440 --> 02:37:51.680]   - Here's the scooter X one.
[02:37:51.680 --> 02:37:52.640]   I'll just read the titles.
[02:37:52.640 --> 02:37:53.480]   YouTube helped form,
[02:37:53.480 --> 02:37:56.560]   which enables posting and new comments changes planned.
[02:37:56.560 --> 02:37:58.360]   Android 14 will bring passkey support
[02:37:58.360 --> 02:38:00.080]   for Dashlane and other apps.
[02:38:00.080 --> 02:38:03.000]   Android 13 QPR two beta gets final feedback survey
[02:38:03.000 --> 02:38:04.440]   before launch.
[02:38:04.440 --> 02:38:06.600]   LumaFusion video editor now fully available
[02:38:06.600 --> 02:38:08.480]   for Android and Chrome OS.
[02:38:08.480 --> 02:38:11.280]   And you can now access Google tasks on the web
[02:38:11.280 --> 02:38:13.080]   without using Gmail's sidebar.
[02:38:13.080 --> 02:38:13.920]   Thank you.
[02:38:13.920 --> 02:38:14.760]   - You've done better a scooter.
[02:38:14.760 --> 02:38:16.280]   - That's the scooter X change log.
[02:38:16.280 --> 02:38:17.280]   - There we go, phone calls. - Thank you, yes.
[02:38:17.280 --> 02:38:18.280]   - Bumble call. - There's a scooter X.
[02:38:18.280 --> 02:38:20.840]   - Bumble call. - Bumble call.
[02:38:20.840 --> 02:38:21.760]   - All right, get ready.
[02:38:21.760 --> 02:38:26.760]   Your picks of the week are next on our agenda.
[02:38:26.760 --> 02:38:33.640]   But first, I wanna plug the club just briefly.
[02:38:33.640 --> 02:38:35.760]   First of all, before I go into that,
[02:38:35.760 --> 02:38:37.000]   just thank you club members
[02:38:37.000 --> 02:38:38.800]   'cause you make so much possible here.
[02:38:38.800 --> 02:38:40.880]   And increasingly we're having a hard time,
[02:38:40.880 --> 02:38:42.880]   frankly, selling ads.
[02:38:42.880 --> 02:38:43.720]   It's not just us,
[02:38:43.720 --> 02:38:46.600]   everybody's seeing a downturn in podcast ad sales,
[02:38:46.600 --> 02:38:50.880]   but unlike NPR, which can fire 10% of its staff
[02:38:50.880 --> 02:38:54.120]   when it's down $300 million in ad sales.
[02:38:54.120 --> 02:38:55.640]   We don't wanna do that.
[02:38:55.640 --> 02:38:58.560]   So that's why two years ago,
[02:38:58.560 --> 02:39:00.880]   during COVID, Lisa created Club Twit
[02:39:00.880 --> 02:39:02.080]   and it has been a boon.
[02:39:02.080 --> 02:39:03.360]   Thank you.
[02:39:03.360 --> 02:39:05.040]   Club Twit is seven bucks a month.
[02:39:05.040 --> 02:39:05.880]   That's all it is.
[02:39:05.880 --> 02:39:07.960]   You get ad-free versions of all the shows.
[02:39:07.960 --> 02:39:10.240]   You get special shows we don't put out in public
[02:39:10.240 --> 02:39:12.600]   on the Twit Plus feed like hands-on Macintosh
[02:39:12.600 --> 02:39:15.880]   with Micah Sergeant, Paul Therrot's hands-on windows,
[02:39:15.880 --> 02:39:17.760]   the Untitled Linux Show with Jonathan Bennett,
[02:39:17.760 --> 02:39:21.280]   Giz Fizz with Dick D. Bartolo, Stacey's Book Club.
[02:39:21.280 --> 02:39:22.760]   We get so much extra stuff
[02:39:22.760 --> 02:39:25.760]   and it's put together a bunch of great events coming up.
[02:39:25.760 --> 02:39:30.160]   I think Samable Sam-ed is this Thursday, right?
[02:39:30.160 --> 02:39:31.000]   That is correct.
[02:39:31.000 --> 02:39:33.600]   In the club, that's gonna be fantastic.
[02:39:33.600 --> 02:39:36.800]   A little AMA with Sam, he's our car guy.
[02:39:36.800 --> 02:39:39.920]   Stacey's Book Club is coming up next month.
[02:39:39.920 --> 02:39:42.680]   Victor will be under the microscope
[02:39:42.680 --> 02:39:43.800]   and our inside Twit chat.
[02:39:43.800 --> 02:39:44.960]   He's one of our great editors,
[02:39:44.960 --> 02:39:47.560]   Alex Wilhelm, who you and probably know well
[02:39:47.560 --> 02:39:49.840]   from Twit, he's been on for years.
[02:39:49.840 --> 02:39:52.600]   Just had a baby, you're gonna do an AMA with Alex
[02:39:52.600 --> 02:39:55.720]   and then Sean Powers from Floss Weekly.
[02:39:55.720 --> 02:39:57.840]   There's a lot of fun also in this Discord.
[02:39:57.840 --> 02:40:00.320]   I mean, I think the Discord is really
[02:40:00.320 --> 02:40:02.200]   more than just talking about the shows.
[02:40:02.200 --> 02:40:04.400]   It's all the stuff that you're interested in.
[02:40:04.400 --> 02:40:07.200]   In fact, we've kinda had, let me do that right now.
[02:40:07.200 --> 02:40:08.560]   An AI section.
[02:40:08.560 --> 02:40:11.200]   I feel like we should have an AI section.
[02:40:11.200 --> 02:40:12.360]   We should have an AI.
[02:40:12.360 --> 02:40:13.560]   Heck yeah, we should.
[02:40:13.560 --> 02:40:14.880]   AI, baby.
[02:40:14.880 --> 02:40:16.520]   What should I just call it, AI?
[02:40:16.520 --> 02:40:17.360]   Yeah.
[02:40:17.360 --> 02:40:19.400]   Hey, it's text, we're gonna create a channel.
[02:40:19.400 --> 02:40:21.480]   Look at that, that's how easy it is.
[02:40:21.480 --> 02:40:24.040]   And I have to alphabetically sort it,
[02:40:24.040 --> 02:40:24.880]   so I'll drag it up to the top.
[02:40:24.880 --> 02:40:26.320]   We have to say, where is it?
[02:40:26.320 --> 02:40:28.720]   That's how it is for us.
[02:40:28.720 --> 02:40:30.960]   Yeah, you can't do it, but that's okay
[02:40:30.960 --> 02:40:32.680]   because Ant is very active.
[02:40:32.680 --> 02:40:33.960]   And so if you have a request,
[02:40:33.960 --> 02:40:36.120]   there's a request channel, you can add it.
[02:40:36.120 --> 02:40:38.160]   So there you go, there's our AI channel.
[02:40:38.160 --> 02:40:39.480]   See?
[02:40:39.480 --> 02:40:40.320]   I'll tell you what.
[02:40:40.320 --> 02:40:41.560]   It should be, wait, wait, wait, wait, wait,
[02:40:41.560 --> 02:40:43.160]   how did beer get above AI?
[02:40:43.160 --> 02:40:46.200]   How did beer get above AI?
[02:40:46.200 --> 02:40:48.080]   How was it not above AI?
[02:40:48.080 --> 02:40:49.680]   What do you mean it is looking at?
[02:40:49.680 --> 02:40:51.760]   In fact, are you serious now?
[02:40:51.760 --> 02:40:56.080]   Oh my God, AI is already alive, it's alive.
[02:40:56.080 --> 02:40:59.040]   That's why we love the club,
[02:40:59.040 --> 02:41:01.120]   because of people in the Discord or club members,
[02:41:01.120 --> 02:41:04.240]   and it's just a great community to hang out in.
[02:41:04.240 --> 02:41:09.240]   We also, I should add, have stuff that, you know,
[02:41:09.240 --> 02:41:12.640]   like before and after the shows,
[02:41:12.640 --> 02:41:14.320]   there's just a whole bunch of content
[02:41:14.320 --> 02:41:15.920]   that we don't have a place to put.
[02:41:15.920 --> 02:41:17.560]   It all goes into the club.
[02:41:17.560 --> 02:41:20.120]   And it's just, I think, a great way to support what we do.
[02:41:20.120 --> 02:41:22.000]   I really appreciate it.
[02:41:22.000 --> 02:41:24.160]   And I think you get the benefits of it,
[02:41:24.160 --> 02:41:25.400]   seven bucks a month.
[02:41:25.400 --> 02:41:26.960]   If you're not yet a club member,
[02:41:26.960 --> 02:41:29.640]   and you're not, 'cause you're hearing this, I guess,
[02:41:29.640 --> 02:41:34.200]   please do me a favor and go to twit.tv/clubtwit
[02:41:34.200 --> 02:41:35.240]   and sign up.
[02:41:35.240 --> 02:41:37.560]   You could buy it for a whole year, 84 bucks.
[02:41:37.560 --> 02:41:40.040]   By the way, I should point out, when you do that,
[02:41:40.040 --> 02:41:42.120]   single-handedly, you are guaranteeing
[02:41:42.120 --> 02:41:44.120]   another 12 months of shows.
[02:41:44.120 --> 02:41:46.000]   'Cause we can't stop now.
[02:41:46.000 --> 02:41:47.840]   You've paid for those shows.
[02:41:47.840 --> 02:41:51.000]   Your March 1st, 2024, we have to do it.
[02:41:51.000 --> 02:41:52.880]   - Thank you. - Yeah.
[02:41:52.880 --> 02:41:55.040]   So by signing up for a year,
[02:41:55.040 --> 02:41:58.160]   you're in a way, single-handedly extending the life of Twit
[02:41:58.160 --> 02:41:59.680]   by an entire year.
[02:41:59.680 --> 02:42:00.720]   How about that?
[02:42:00.720 --> 02:42:01.680]   Talk about power.
[02:42:01.680 --> 02:42:04.120]   - That always resets by the next person
[02:42:04.120 --> 02:42:05.280]   that buys the year's subsets.
[02:42:05.280 --> 02:42:06.840]   - Oh yeah, whoever bought it last,
[02:42:06.840 --> 02:42:08.640]   adds another five minutes or whatever.
[02:42:08.640 --> 02:42:09.480]   - We're gonna be here forever.
[02:42:09.480 --> 02:42:11.160]   - Twit, you're not going anywhere.
[02:42:11.160 --> 02:42:13.320]   - Well, I will tell you this, it'll be the canary.
[02:42:13.320 --> 02:42:16.720]   If it goes away, then you know you got one year,
[02:42:16.720 --> 02:42:17.600]   the clock is ticking.
[02:42:17.600 --> 02:42:18.680]   - Oh boy.
[02:42:18.680 --> 02:42:21.520]   - We hope we don't have to do that, honestly.
[02:42:21.520 --> 02:42:23.840]   But we don't have, you know, we don't have the pockets,
[02:42:23.840 --> 02:42:28.320]   we don't have government funding, we don't have VC funding.
[02:42:28.320 --> 02:42:31.320]   I'm basically spending every penny I have
[02:42:31.320 --> 02:42:32.920]   to keep this thing on the road.
[02:42:32.920 --> 02:42:34.720]   So help us out a little bit.
[02:42:34.720 --> 02:42:39.720]   Just, you know, gas, grass or cash, no one rides the three.
[02:42:39.720 --> 02:42:42.320]   I think is the slow one.
[02:42:43.320 --> 02:42:46.440]   - I think I heard something like that before, but.
[02:42:46.440 --> 02:42:49.960]   - I can't exactly remember the bumper sticker.
[02:42:49.960 --> 02:42:50.800]   Twit, that's that one.
[02:42:50.800 --> 02:42:52.720]   - You heard in the mean streets of Pelham.
[02:42:52.720 --> 02:42:53.560]   - Oh yeah.
[02:42:53.560 --> 02:42:58.000]   It's our new slogan for the club.
[02:42:58.000 --> 02:43:03.000]   I like it, gas, grass or cash, no one rides for free.
[02:43:03.000 --> 02:43:04.960]   - So Leo, go back to the state,
[02:43:04.960 --> 02:43:06.840]   to the Discord screen you showed a minute ago.
[02:43:06.840 --> 02:43:09.320]   I wanted to scroll down, there was a neat illustration.
[02:43:09.320 --> 02:43:10.960]   - Not that.
[02:43:10.960 --> 02:43:13.640]   - No, scroll where you were.
[02:43:13.640 --> 02:43:14.960]   I didn't see it on my screen.
[02:43:14.960 --> 02:43:16.160]   I don't know why, I keep going.
[02:43:16.160 --> 02:43:18.680]   It was somebody sitting at a desk with pictures over it.
[02:43:18.680 --> 02:43:20.560]   - Oh yeah, and that's Mr. Nielsen's hand here.
[02:43:20.560 --> 02:43:22.120]   - There we go, yeah, yeah.
[02:43:22.120 --> 02:43:23.520]   - Oh, that's amazing.
[02:43:23.520 --> 02:43:25.280]   - There you are, Jeff.
[02:43:25.280 --> 02:43:29.520]   The old, the young Jeff, there's Ant, there's Leo.
[02:43:29.520 --> 02:43:32.840]   And this should have that low-fi tunes going on
[02:43:32.840 --> 02:43:33.920]   in the background, right?
[02:43:33.920 --> 02:43:35.480]   - Yeah, how did you do that, Mr. Nielsen?
[02:43:35.480 --> 02:43:37.000]   - He is a master.
[02:43:37.000 --> 02:43:39.080]   I think it's stable diffusion, right?
[02:43:39.080 --> 02:43:40.040]   Anthony's really good.
[02:43:40.040 --> 02:43:41.040]   - Yeah, you know what, we now--
[02:43:41.040 --> 02:43:42.840]   - Did you see that somebody got stable diffusion
[02:43:42.840 --> 02:43:44.800]   running on a phone fully?
[02:43:44.800 --> 02:43:46.520]   - Yeah, I have it.
[02:43:46.520 --> 02:43:47.280]   - On a phone?
[02:43:47.280 --> 02:43:48.920]   - On my iPhone.
[02:43:48.920 --> 02:43:49.760]   - On your phone?
[02:43:49.760 --> 02:43:50.600]   - Wow.
[02:43:50.600 --> 02:43:51.600]   - Yeah, I've had it for some time.
[02:43:51.600 --> 02:43:52.440]   - Offline.
[02:43:52.440 --> 02:43:53.280]   - Wow.
[02:43:53.280 --> 02:43:54.120]   - Yeah.
[02:43:54.120 --> 02:43:54.960]   - I thought you had it on a computer.
[02:43:54.960 --> 02:43:56.040]   - Well, I have it on a computer too,
[02:43:56.040 --> 02:43:57.160]   but I also have it on the phone.
[02:43:57.160 --> 02:43:59.600]   And the reason you can do that is because the models
[02:43:59.600 --> 02:44:03.640]   aren't that big, and the iPhone has a neural processing unit.
[02:44:03.640 --> 02:44:07.320]   I mean, you forget how powerful these things are
[02:44:07.320 --> 02:44:08.160]   in our pocket.
[02:44:08.160 --> 02:44:09.240]   - Yeah.
[02:44:09.240 --> 02:44:10.080]   - What was it called?
[02:44:10.080 --> 02:44:13.000]   I have to remember what it was called.
[02:44:13.000 --> 02:44:17.920]   It would probably be in my pictures, my pictures thing,
[02:44:17.920 --> 02:44:18.800]   but where is that?
[02:44:18.800 --> 02:44:20.720]   Not productivity, that's for sure.
[02:44:20.720 --> 02:44:25.400]   The opposite of productivity.
[02:44:25.400 --> 02:44:29.400]   Maybe it's in photos.
[02:44:29.400 --> 02:44:31.440]   Oh yeah, it is, draw things it's called.
[02:44:31.440 --> 02:44:36.280]   It's a, it's so, oh, nope, nope, yep.
[02:44:36.280 --> 02:44:39.280]   So it's, this is a paper cut Christmas card.
[02:44:39.280 --> 02:44:41.680]   I drew with draw things.
[02:44:41.680 --> 02:44:43.080]   If you've used stable diffusion,
[02:44:43.080 --> 02:44:44.120]   you'll recognize the interface.
[02:44:44.120 --> 02:44:45.840]   That's the stable diffusion interface.
[02:44:45.840 --> 02:44:46.680]   - Yeah.
[02:44:46.680 --> 02:44:51.600]   - And you can add different, where is it?
[02:44:51.600 --> 02:44:53.120]   Here it is, you can add different models.
[02:44:53.120 --> 02:44:57.680]   So it has a lot of the stable diffusion models versions,
[02:44:57.680 --> 02:45:00.280]   but it also has, you know, cyberpunk and Tron,
[02:45:00.280 --> 02:45:03.560]   and there's an open journey style, Elden ring style.
[02:45:03.560 --> 02:45:05.120]   So these are just the models.
[02:45:05.120 --> 02:45:08.120]   Now they're fairly big, you wouldn't add a whole bunch of them.
[02:45:08.120 --> 02:45:09.640]   Let me add Elden ring.
[02:45:09.640 --> 02:45:11.400]   Oh no, I think I already have that.
[02:45:11.400 --> 02:45:13.480]   Need to download the selected model over the network,
[02:45:13.480 --> 02:45:18.160]   proceed, and it is, as they all are, 1.6 gigs.
[02:45:18.160 --> 02:45:19.440]   But that model is now on the phone,
[02:45:19.440 --> 02:45:20.840]   and the phone actually works pretty fast,
[02:45:20.840 --> 02:45:24.200]   almost as fast as it would on a GPU based system.
[02:45:24.200 --> 02:45:25.040]   - Wow.
[02:45:25.040 --> 02:45:25.880]   - Yeah, impressive.
[02:45:25.880 --> 02:45:28.040]   - Draw things it's called, it's free.
[02:45:28.040 --> 02:45:30.320]   It's a stable diffusion for the iPhone.
[02:45:30.320 --> 02:45:31.920]   That's been out for a while.
[02:45:31.920 --> 02:45:33.600]   All right.
[02:45:34.440 --> 02:45:38.000]   - Pics of the week, picks.
[02:45:38.000 --> 02:45:40.640]   Now Jason, you don't have to do a pick.
[02:45:40.640 --> 02:45:41.600]   You were a last minute.
[02:45:41.600 --> 02:45:42.680]   - I have one in there.
[02:45:42.680 --> 02:45:43.520]   - All right.
[02:45:43.520 --> 02:45:44.360]   - What is it?
[02:45:44.360 --> 02:45:46.560]   - Well, I just thought I would kind of throw a bone
[02:45:46.560 --> 02:45:48.800]   for the app artifact.
[02:45:48.800 --> 02:45:51.760]   I don't know if that's been talked about much.
[02:45:51.760 --> 02:45:54.440]   - Only peripherally, and I'm actually glad you brought it up.
[02:45:54.440 --> 02:45:55.280]   I've been meaning to you.
[02:45:55.280 --> 02:45:57.800]   This is by Kevin Sistrom, the guy who started
[02:45:57.800 --> 02:45:58.720]   Instagram in his book.
[02:45:58.720 --> 02:46:00.360]   - Yeah, yeah, and I would think it was just like
[02:46:00.360 --> 02:46:03.040]   a couple of weeks ago that they really opened it up
[02:46:03.040 --> 02:46:04.600]   and allowed everybody to kind of get in
[02:46:04.600 --> 02:46:05.520]   and start using it.
[02:46:05.520 --> 02:46:07.920]   We talked about it a little bit on all about Android last night.
[02:46:07.920 --> 02:46:11.120]   And I think the general comparison or consensus
[02:46:11.120 --> 02:46:15.120]   by us on the panel last night was that it's hard to tell
[02:46:15.120 --> 02:46:18.840]   right now what is different between artifact,
[02:46:18.840 --> 02:46:21.560]   which is like a, they're billing it as like an AI driven
[02:46:21.560 --> 02:46:24.720]   social news aggregator, right?
[02:46:24.720 --> 02:46:28.160]   Like here are the news items that are happening right now
[02:46:28.160 --> 02:46:31.560]   that you would like or you would be interested in.
[02:46:31.560 --> 02:46:33.800]   And it's hard for me to really tell the differences
[02:46:33.800 --> 02:46:36.160]   between what artifact is doing and what something
[02:46:36.160 --> 02:46:38.000]   like Google News is doing.
[02:46:38.000 --> 02:46:41.080]   'Cause I use the Google News app on Android quite a bit.
[02:46:41.080 --> 02:46:43.480]   And Google of course is making the same determinations
[02:46:43.480 --> 02:46:47.080]   on the backend as far as what news it thinks.
[02:46:47.080 --> 02:46:47.920]   - Whoa, what's up, Bill?
[02:46:47.920 --> 02:46:48.760]   My friend Bill Gross.
[02:46:48.760 --> 02:46:50.320]   - I was just about to share this to you.
[02:46:50.320 --> 02:46:51.920]   - Well, it's different Bill Gross.
[02:46:51.920 --> 02:46:53.560]   - Oh, okay, and it's the New York Post.
[02:46:53.560 --> 02:46:55.600]   But I will show you the disadvantage to this.
[02:46:55.600 --> 02:46:58.280]   When I share it to somebody else and to my mom,
[02:46:58.280 --> 02:46:59.120]   she's gonna really want it.
[02:46:59.120 --> 02:46:59.960]   - Oh, and it's wrapped.
[02:46:59.960 --> 02:47:04.120]   - That's a link to not New York Post, but artifact.new.
[02:47:04.120 --> 02:47:05.120]   - Right.
[02:47:05.120 --> 02:47:07.600]   - And that's what, it's the same thing Apple News does this.
[02:47:07.600 --> 02:47:09.320]   Google does not.
[02:47:09.320 --> 02:47:10.920]   Google News does not, I don't believe so.
[02:47:10.920 --> 02:47:12.440]   - No, it shares the actual article.
[02:47:12.440 --> 02:47:15.040]   And so I think that's kind of a negative on this
[02:47:15.040 --> 02:47:17.920]   to be honest. - Gotta drive those users, right?
[02:47:17.920 --> 02:47:21.600]   You get some stats after you've read a number of articles.
[02:47:21.600 --> 02:47:24.240]   You know, they actually, it's kind of gamified a little bit.
[02:47:24.240 --> 02:47:25.960]   Like if you go there, you've got 37 reads.
[02:47:25.960 --> 02:47:27.760]   I think once you get to like 50 reads,
[02:47:27.760 --> 02:47:29.800]   then it gives you kind of like a thumbs up
[02:47:29.800 --> 02:47:31.280]   and says, "Hey, you've trained our system.
[02:47:31.280 --> 02:47:33.960]   We know you better and we'll start giving you
[02:47:33.960 --> 02:47:37.160]   more appropriate or accurate stories.
[02:47:37.160 --> 02:47:38.160]   But you can kind of see, you know,
[02:47:38.160 --> 02:47:40.360]   what are the topics that I follow closely?
[02:47:40.360 --> 02:47:45.360]   What are the publications that I tend to read most?
[02:47:45.360 --> 02:47:46.200]   Things like that.
[02:47:46.200 --> 02:47:48.760]   So, but it's still like--
[02:47:48.760 --> 02:47:49.840]   - You can also follow friends.
[02:47:49.840 --> 02:47:51.360]   I'm gonna follow Renee.
[02:47:51.360 --> 02:47:52.200]   Oh, wait a minute.
[02:47:52.200 --> 02:47:53.560]   - Is it following or is it just inviting?
[02:47:53.560 --> 02:47:54.400]   - That's inviting. - I think it's just inviting.
[02:47:54.400 --> 02:47:55.240]   - You're right. - You're right.
[02:47:55.240 --> 02:47:56.080]   - I think it's just inviting them.
[02:47:56.080 --> 02:47:56.920]   - So I'm on a two day streak.
[02:47:56.920 --> 02:47:58.920]   I have 37 reads.
[02:47:58.920 --> 02:48:00.760]   - Yeah. - So they've gamified it.
[02:48:00.760 --> 02:48:01.840]   - A little bit. - Yeah.
[02:48:01.840 --> 02:48:05.080]   - It supposedly gets smarter as you read.
[02:48:05.080 --> 02:48:06.440]   It's one of those things like TikTok.
[02:48:06.440 --> 02:48:08.440]   You wanna be really careful about what you open.
[02:48:08.440 --> 02:48:09.360]   - Right.
[02:48:09.360 --> 02:48:10.200]   Be really sure.
[02:48:10.200 --> 02:48:13.000]   Yeah, 'cause certainly they're tracking your scrolls
[02:48:13.000 --> 02:48:14.800]   and how long you pause on a certain thing
[02:48:14.800 --> 02:48:18.400]   and click into and yeah, I don't know.
[02:48:18.400 --> 02:48:21.120]   I've always felt like Google News has done a really good job
[02:48:21.120 --> 02:48:24.000]   of giving me-- - Oh, you put up the wrong link.
[02:48:24.000 --> 02:48:24.920]   - Oh, did I?
[02:48:24.920 --> 02:48:26.600]   - It's-- - This is another catalog
[02:48:26.600 --> 02:48:28.360]   of how you identify and assess all of your art
[02:48:28.360 --> 02:48:29.880]   and collectibles in one place.
[02:48:29.880 --> 02:48:31.280]   - Well, don't you have a lot of art?
[02:48:31.280 --> 02:48:32.360]   - Oh, shoot, I did. - Oh, look at all of that.
[02:48:32.360 --> 02:48:33.520]   - Oh, I linked to the wrong one.
[02:48:33.520 --> 02:48:34.440]   - Look at all of that.
[02:48:34.440 --> 02:48:36.640]   - Yeah, you're right, you're right.
[02:48:36.640 --> 02:48:38.560]   Here, instead, artifact.news.
[02:48:38.560 --> 02:48:40.040]   That's like their set.
[02:48:40.040 --> 02:48:41.360]   - That's the way I was confused.
[02:48:41.360 --> 02:48:43.360]   - This doesn't look like a feed at all.
[02:48:43.360 --> 02:48:44.360]   - Yeah, no, sorry.
[02:48:44.360 --> 02:48:47.720]   I did that last minute before coming in here.
[02:48:47.720 --> 02:48:50.640]   So artifact.news, if you wanna go and check it out,
[02:48:50.640 --> 02:48:52.400]   there's no like wait list or anything.
[02:48:52.400 --> 02:48:53.560]   You can just get it. - There was.
[02:48:53.560 --> 02:48:54.640]   - There was. - There was.
[02:48:54.640 --> 02:48:55.640]   - Now they've opened it up.
[02:48:55.640 --> 02:48:57.640]   But worth checking out, you know,
[02:48:57.640 --> 02:48:59.920]   I think for me, the jury's still out as far as,
[02:48:59.920 --> 02:49:05.760]   you know, Google News is so ingrained in my usage
[02:49:05.760 --> 02:49:07.240]   that I just use that all the time.
[02:49:07.240 --> 02:49:09.280]   So will I start to use artifact?
[02:49:09.280 --> 02:49:10.360]   I don't know at this point.
[02:49:10.360 --> 02:49:11.760]   I don't know why I would,
[02:49:11.760 --> 02:49:15.360]   but I'm gonna give it the shot, so.
[02:49:15.360 --> 02:49:17.040]   - I agree, artifact.news.
[02:49:17.040 --> 02:49:18.920]   It's worth certainly worth paying attention to.
[02:49:18.920 --> 02:49:19.760]   - Yeah. - Especially 'cause
[02:49:19.760 --> 02:49:22.920]   if it's lineage.
[02:49:22.920 --> 02:49:25.560]   - Jeff Jarvis, a numero--
[02:49:25.560 --> 02:49:27.000]   - So weak.
[02:49:27.000 --> 02:49:29.640]   - I do anything, you know, make fun of myself
[02:49:29.640 --> 02:49:30.880]   for the purposes of the show.
[02:49:30.880 --> 02:49:34.880]   I try to put myself out there.
[02:49:34.880 --> 02:49:38.760]   So I've been watching with mixed views,
[02:49:38.760 --> 02:49:43.240]   the teenage look filter on TikTok.
[02:49:43.240 --> 02:49:44.080]   - Yeah, I did it.
[02:49:44.080 --> 02:49:45.720]   It didn't make me look younger.
[02:49:45.720 --> 02:49:48.080]   Did it make you look like the same thing here?
[02:49:48.080 --> 02:49:50.920]   I did, I was really bummed.
[02:49:50.920 --> 02:49:52.480]   - I made a TikTok with it.
[02:49:52.480 --> 02:49:54.720]   - Yeah, I can only do so much here.
[02:49:54.720 --> 02:49:56.680]   - Exactly.
[02:49:56.680 --> 02:50:00.480]   - I've been seeing all these people on TikTok looking young.
[02:50:00.480 --> 02:50:02.840]   And then I thought, well, I try this.
[02:50:02.840 --> 02:50:03.760]   - So that's mine.
[02:50:03.760 --> 02:50:05.840]   Let me see yours here.
[02:50:05.840 --> 02:50:07.920]   This is Jeff looking younger.
[02:50:07.920 --> 02:50:12.160]   - No.
[02:50:12.160 --> 02:50:13.000]   - Which one's the younger?
[02:50:13.000 --> 02:50:14.080]   - Which one's the younger?
[02:50:14.080 --> 02:50:15.400]   Which one's the younger?
[02:50:15.400 --> 02:50:16.720]   - Which one?
[02:50:16.720 --> 02:50:18.080]   - Mine was worse than that.
[02:50:18.080 --> 02:50:18.960]   - Guess maybe the top one?
[02:50:18.960 --> 02:50:20.400]   - Mine was worse than that.
[02:50:20.400 --> 02:50:22.880]   - The top one I was a little so disappointed.
[02:50:22.880 --> 02:50:24.560]   - So disappointed.
[02:50:24.560 --> 02:50:27.960]   - I, let me see if I can get it to work here.
[02:50:27.960 --> 02:50:29.880]   So in order to do this,
[02:50:29.880 --> 02:50:34.880]   you have to swipe into your TikTok plus pictures
[02:50:34.880 --> 02:50:37.200]   and I'm gonna do a filter
[02:50:37.200 --> 02:50:39.320]   and I'm gonna do the younger filter.
[02:50:39.320 --> 02:50:41.000]   Where do they stick that one?
[02:50:41.000 --> 02:50:41.960]   Is it portrait?
[02:50:41.960 --> 02:50:45.160]   - I came to it by Googling.
[02:50:45.160 --> 02:50:50.160]   - Original sunny, vitality, warmth, vlog, dim peach, pink,
[02:50:50.160 --> 02:50:53.200]   youth, youth.
[02:50:53.200 --> 02:50:56.880]   Look how much younger I look than I did.
[02:50:56.880 --> 02:50:58.360]   It's the yout.
[02:50:58.360 --> 02:50:59.600]   It doesn't look any different.
[02:50:59.600 --> 02:51:00.960]   Turn it on.
[02:51:00.960 --> 02:51:02.160]   Turn it off.
[02:51:02.160 --> 02:51:03.480]   Turn on.
[02:51:03.480 --> 02:51:04.320]   Turn it off.
[02:51:04.320 --> 02:51:05.720]   It doesn't change a thing.
[02:51:05.720 --> 02:51:08.880]   And I have it at 700%
[02:51:08.880 --> 02:51:09.720]   100%.
[02:51:09.720 --> 02:51:16.280]   - Dark filters and ant probably looks like a dashing,
[02:51:16.280 --> 02:51:17.120]   you know, high school.
[02:51:17.120 --> 02:51:18.160]   - Yeah, it makes him look younger.
[02:51:18.160 --> 02:51:20.280]   It makes me look the same age.
[02:51:20.280 --> 02:51:21.960]   - Jason was like this.
[02:51:21.960 --> 02:51:23.880]   - Oh, I am. - I want to try it here.
[02:51:23.880 --> 02:51:25.440]   You be my guest.
[02:51:25.440 --> 02:51:27.240]   See if it does anything.
[02:51:27.240 --> 02:51:28.080]   - What is it?
[02:51:28.080 --> 02:51:30.360]   It's effects and then-- - Oh, so, yeah.
[02:51:30.360 --> 02:51:31.840]   That's off.
[02:51:31.840 --> 02:51:33.440]   - Okay.
[02:51:33.440 --> 02:51:34.760]   Is it even doing anything?
[02:51:34.760 --> 02:51:36.840]   - Oh, maybe I'm not doing it right.
[02:51:36.840 --> 02:51:37.800]   - Is it under effect?
[02:51:37.800 --> 02:51:38.760]   - How about I even know where to look?
[02:51:38.760 --> 02:51:41.320]   - Yeah, it's in the filters.
[02:51:41.320 --> 02:51:42.560]   Let me see though,
[02:51:42.560 --> 02:51:46.000]   Jeff has also posted his glamour look and that I--
[02:51:46.000 --> 02:51:47.800]   - That's a controversial one right now.
[02:51:47.800 --> 02:51:49.840]   Is that what he's using the glamour one?
[02:51:51.800 --> 02:51:56.040]   - Yeah, it's kind of, that one made you look younger.
[02:51:56.040 --> 02:51:57.280]   You look beautiful.
[02:51:57.280 --> 02:52:00.240]   I just want to kiss your big rugged and some face.
[02:52:00.240 --> 02:52:01.560]   - Yeah. - Yeah.
[02:52:01.560 --> 02:52:03.360]   - She goes straight down my beard.
[02:52:03.360 --> 02:52:04.560]   - Teenage look.
[02:52:04.560 --> 02:52:06.560]   - Oh, okay, so you weren't in the right area.
[02:52:06.560 --> 02:52:08.800]   - Oh, there we go.
[02:52:08.800 --> 02:52:10.360]   - Oh, oh, oh.
[02:52:10.360 --> 02:52:12.840]   - Oh, yes, I do look.
[02:52:12.840 --> 02:52:13.680]   - Well, let me try.
[02:52:13.680 --> 02:52:14.520]   - I know how to do that.
[02:52:14.520 --> 02:52:15.600]   - There you go. - Oh.
[02:52:15.600 --> 02:52:16.520]   - Now you got it.
[02:52:16.520 --> 02:52:20.280]   - I do look younger. - I do look younger.
[02:52:20.280 --> 02:52:22.160]   - Oh, yeah, it's freaky.
[02:52:22.160 --> 02:52:23.320]   - What is the filter called?
[02:52:23.320 --> 02:52:25.480]   'Cause I don't see this.
[02:52:25.480 --> 02:52:26.600]   - My hair is darker.
[02:52:26.600 --> 02:52:29.080]   - A little darker on the top there, yeah.
[02:52:29.080 --> 02:52:32.600]   Yeah, it's a little strange.
[02:52:32.600 --> 02:52:34.360]   - I looked like grandpa Munster.
[02:52:34.360 --> 02:52:35.920]   I don't know how that helps.
[02:52:35.920 --> 02:52:37.240]   - Yes, it does.
[02:52:37.240 --> 02:52:40.240]   - Okay, thank you.
[02:52:40.240 --> 02:52:41.080]   Thank you.
[02:52:41.080 --> 02:52:41.920]   - You're so good.
[02:52:41.920 --> 02:52:42.880]   - There are so many filters in here.
[02:52:42.880 --> 02:52:44.360]   - I know, I was looking at the wrong one.
[02:52:44.360 --> 02:52:45.360]   I was looking at youth,
[02:52:45.360 --> 02:52:46.960]   but you were supposed to be teenage.
[02:52:46.960 --> 02:52:49.120]   Well, there's, I think you were,
[02:52:49.120 --> 02:52:50.440]   yeah, I think there's a difference between,
[02:52:50.440 --> 02:52:52.520]   like when you're in the general camera mode,
[02:52:52.520 --> 02:52:54.520]   there's like basic filters.
[02:52:54.520 --> 02:52:56.120]   And then there's the filter section,
[02:52:56.120 --> 02:52:58.200]   which are like all of the like expanded
[02:52:58.200 --> 02:53:00.120]   like trending things and stuff.
[02:53:00.120 --> 02:53:00.960]   That's what it's like.
[02:53:00.960 --> 02:53:02.040]   - Oh, that was a basic filter.
[02:53:02.040 --> 02:53:03.240]   It wasn't a good one.
[02:53:03.240 --> 02:53:04.080]   - Right.
[02:53:04.080 --> 02:53:05.840]   - Well, now I want this all the time.
[02:53:05.840 --> 02:53:07.000]   - Oh my gosh.
[02:53:07.000 --> 02:53:12.040]   Ladies and gentlemen, it is now time for Ant Fruitz
[02:53:12.040 --> 02:53:14.400]   and his pick of the week.
[02:53:14.400 --> 02:53:17.200]   - Oh my gosh, let me put this TikTok stuff down
[02:53:17.200 --> 02:53:19.720]   'cause I don't know what I'm doing with it.
[02:53:19.720 --> 02:53:23.800]   My pick, Amaran, and well, actually it's Aperture
[02:53:23.800 --> 02:53:26.160]   and their other brand, Amaran.
[02:53:26.160 --> 02:53:28.960]   They announced some new gear yesterday.
[02:53:28.960 --> 02:53:33.640]   In particular, the affordable Amaran COB Chippon board,
[02:53:33.640 --> 02:53:35.680]   S series lights.
[02:53:35.680 --> 02:53:38.080]   I've spoken about them before,
[02:53:38.080 --> 02:53:40.040]   but these are their updated versions
[02:53:40.040 --> 02:53:42.680]   and they're still quite nicely priced.
[02:53:42.680 --> 02:53:44.200]   You can use these for photography.
[02:53:44.200 --> 02:53:46.320]   You can use these for video.
[02:53:46.320 --> 02:53:47.960]   Some of them are really portable.
[02:53:47.960 --> 02:53:51.200]   If you can just attach a D-Tap battery to them.
[02:53:51.200 --> 02:53:53.600]   They work with a Citus Link mobile app
[02:53:53.600 --> 02:53:56.760]   so you can do some effects and things like that
[02:53:56.760 --> 02:54:00.520]   or control the brightness and so forth.
[02:54:00.520 --> 02:54:02.600]   So yeah, check them out.
[02:54:02.600 --> 02:54:03.440]   Amaran.
[02:54:03.440 --> 02:54:04.600]   - I would like to have this for a trip
[02:54:04.600 --> 02:54:06.440]   'cause then Lisa could hold this.
[02:54:06.440 --> 02:54:09.440]   I would use my Oso 6.
[02:54:09.440 --> 02:54:12.080]   - There you go.
[02:54:12.080 --> 02:54:12.920]   - And they're the selfie stick.
[02:54:12.920 --> 02:54:14.080]   And people would think I'm an influencer.
[02:54:14.080 --> 02:54:15.080]   Maybe we get photos.
[02:54:15.080 --> 02:54:16.920]   - That's exactly what they would say.
[02:54:16.920 --> 02:54:17.760]   - They'd let you in.
[02:54:17.760 --> 02:54:19.000]   - I bet he's on TikTok.
[02:54:19.000 --> 02:54:21.440]   - Especially if I use the teenage filter.
[02:54:21.440 --> 02:54:22.600]   - Yeah, yeah.
[02:54:22.600 --> 02:54:23.440]   - The young him.
[02:54:23.440 --> 02:54:25.440]   (humming)
[02:54:25.440 --> 02:54:27.680]   - C-O-B lights.
[02:54:27.680 --> 02:54:29.000]   C-S lights.
[02:54:29.000 --> 02:54:29.840]   - S series.
[02:54:29.840 --> 02:54:34.000]   - From APUTURE.com.
[02:54:34.000 --> 02:54:35.440]   You have a book too though, right?
[02:54:35.440 --> 02:54:36.280]   - Aperture.
[02:54:36.280 --> 02:54:39.760]   Yeah, and then my last pick is called "Hi Y'all Doin'?"
[02:54:39.760 --> 02:54:40.720]   By Liz Lee Jordan.
[02:54:40.720 --> 02:54:42.120]   I listened to the audio book
[02:54:42.120 --> 02:54:43.880]   and I really enjoyed this book.
[02:54:44.880 --> 02:54:46.560]   He talks about his struggles.
[02:54:46.560 --> 02:54:47.400]   He's a comedian.
[02:54:47.400 --> 02:54:49.680]   He died recently.
[02:54:49.680 --> 02:54:52.920]   Had a car accident down in LA.
[02:54:52.920 --> 02:54:55.520]   But he's a comedian.
[02:54:55.520 --> 02:54:57.200]   He's from Nash.
[02:54:57.200 --> 02:54:58.960]   I want to say Nashville area.
[02:54:58.960 --> 02:55:00.960]   And he's so southern.
[02:55:00.960 --> 02:55:03.280]   - Let me play a little bit of the sound here.
[02:55:03.280 --> 02:55:06.880]   - So excited to be playing Beverly again.
[02:55:06.880 --> 02:55:09.800]   - Oh, he sounds like Truman Capote a little bit.
[02:55:09.800 --> 02:55:10.800]   - Oh yeah, very nice.
[02:55:10.800 --> 02:55:11.640]   - Yeah.
[02:55:11.640 --> 02:55:14.600]   - And he is, I love listening to a part
[02:55:14.600 --> 02:55:16.200]   where he's always talking about it.
[02:55:16.200 --> 02:55:17.200]   It's just awful.
[02:55:17.200 --> 02:55:18.400]   It was just awful.
[02:55:18.400 --> 02:55:19.240]   - I'm down low than this.
[02:55:19.240 --> 02:55:20.160]   - It's southern accent.
[02:55:20.160 --> 02:55:21.000]   - I can't wait.
[02:55:21.000 --> 02:55:22.080]   - This sounds great.
[02:55:22.080 --> 02:55:23.480]   Gets me every time.
[02:55:23.480 --> 02:55:24.640]   He's so funny.
[02:55:24.640 --> 02:55:27.440]   But again, he talks about his battles with alcoholism.
[02:55:27.440 --> 02:55:30.440]   He talks about his battles of being in the South
[02:55:30.440 --> 02:55:32.760]   and gay and the stuff that he had to deal with
[02:55:32.760 --> 02:55:35.720]   growing up back in the long time ago.
[02:55:35.720 --> 02:55:38.840]   - But he became famous on Insta, right?
[02:55:38.840 --> 02:55:40.920]   - On Instagram is when he blew up.
[02:55:41.760 --> 02:55:45.080]   Just a handful of years ago, right when the pandemic
[02:55:45.080 --> 02:55:47.720]   started and he knew nothing about Instagram.
[02:55:47.720 --> 02:55:49.080]   He talks about it in the book.
[02:55:49.080 --> 02:55:51.720]   And just, it's a really good story.
[02:55:51.720 --> 02:55:54.480]   And some of his stuff talking about his therapy sessions
[02:55:54.480 --> 02:55:57.840]   were great in how people deal with fear,
[02:55:57.840 --> 02:56:01.560]   in how people deal with manufactured fear
[02:56:01.560 --> 02:56:02.400]   and stuff like that.
[02:56:02.400 --> 02:56:03.600]   It's a really good book.
[02:56:03.600 --> 02:56:05.360]   And it's called "How Y'all Doing?"
[02:56:05.360 --> 02:56:06.480]   by Leslie Jordan.
[02:56:06.480 --> 02:56:09.600]   And a good book on dealing with acceptance
[02:56:09.600 --> 02:56:13.360]   and self-acceptance and being able to walk in the room
[02:56:13.360 --> 02:56:16.120]   and just loving you for who you are,
[02:56:16.120 --> 02:56:18.120]   regardless of the room that you're in.
[02:56:18.120 --> 02:56:19.920]   And it's a good one.
[02:56:19.920 --> 02:56:21.480]   - I love that.
[02:56:21.480 --> 02:56:23.600]   I'm just putting on my list.
[02:56:23.600 --> 02:56:25.880]   - It's so fun.
[02:56:25.880 --> 02:56:26.720]   - I love his voice, too.
[02:56:26.720 --> 02:56:29.280]   - He was on Will and Grace, too.
[02:56:29.280 --> 02:56:31.200]   - Years ago, if you've ever saw that show.
[02:56:31.200 --> 02:56:32.040]   - Okay.
[02:56:32.040 --> 02:56:37.560]   - Misadventures and mischief from a life well lived.
[02:56:37.560 --> 02:56:40.120]   - And yeah, this is when you want to listen to.
[02:56:40.120 --> 02:56:41.680]   - Yeah, listen to this.
[02:56:41.680 --> 02:56:42.520]   - Here is voice.
[02:56:42.520 --> 02:56:43.800]   - Yeah, extra personality.
[02:56:43.800 --> 02:56:44.640]   - Nice.
[02:56:44.640 --> 02:56:45.640]   - Comes through that way.
[02:56:45.640 --> 02:56:46.480]   - Thank you, Ed.
[02:56:46.480 --> 02:56:48.400]   Good recommendations.
[02:56:48.400 --> 02:56:49.960]   - And thank you all for joining us.
[02:56:49.960 --> 02:56:51.760]   Thank you for filling in for Stacy.
[02:56:51.760 --> 02:56:52.600]   My pleasure.
[02:56:52.600 --> 02:56:53.440]   - This was a few feels for him.
[02:56:53.440 --> 02:56:55.160]   - And producing at the same time.
[02:56:55.160 --> 02:56:56.760]   - He's an amazing fella.
[02:56:56.760 --> 02:56:57.760]   Added a lot to the show.
[02:56:57.760 --> 02:56:58.600]   - Oh, I thought too.
[02:56:58.600 --> 02:57:00.160]   So I really appreciate you coming in.
[02:57:00.160 --> 02:57:01.200]   - Always.
[02:57:01.200 --> 02:57:02.560]   - Doing the show.
[02:57:02.560 --> 02:57:04.120]   I told him you don't have to work today,
[02:57:04.120 --> 02:57:05.440]   so that I'm working.
[02:57:05.440 --> 02:57:07.360]   (laughing)
[02:57:07.360 --> 02:57:09.480]   - Yeah, how what I said was like, we're doing a show.
[02:57:09.480 --> 02:57:11.040]   It's kind of not work like.
[02:57:11.040 --> 02:57:12.400]   - I'm here.
[02:57:12.400 --> 02:57:13.240]   - For doing a show.
[02:57:13.240 --> 02:57:14.240]   - Oh.
[02:57:14.240 --> 02:57:15.240]   - Yeah, you're right.
[02:57:15.240 --> 02:57:16.080]   It isn't work.
[02:57:16.080 --> 02:57:16.920]   - We're talking.
[02:57:16.920 --> 02:57:17.760]   - We shouldn't call this work.
[02:57:17.760 --> 02:57:18.600]   It's too much fun.
[02:57:18.600 --> 02:57:19.440]   - Yes, yes, exactly.
[02:57:19.440 --> 02:57:20.960]   - You'll catch Jason on all about Android
[02:57:20.960 --> 02:57:23.880]   every Tuesday night and tomorrow on Tech News Weekly.
[02:57:23.880 --> 02:57:25.080]   - Maybe get Mike McEwan.
[02:57:25.080 --> 02:57:25.920]   That'll be interesting.
[02:57:25.920 --> 02:57:26.840]   - Yeah, I'll look into it.
[02:57:26.840 --> 02:57:27.680]   - Once we're done here.
[02:57:27.680 --> 02:57:28.520]   - Next time.
[02:57:28.520 --> 02:57:29.360]   - Nice guy.
[02:57:29.360 --> 02:57:30.200]   - Very nice guy.
[02:57:30.200 --> 02:57:31.040]   - Yeah, yeah.
[02:57:31.040 --> 02:57:31.960]   - Do you happen to have his contact?
[02:57:31.960 --> 02:57:32.800]   - I have his mask.
[02:57:32.800 --> 02:57:33.640]   - I know.
[02:57:33.640 --> 02:57:34.480]   - He just did him.
[02:57:34.480 --> 02:57:37.360]   - He's a pinging, he tuted us.
[02:57:37.360 --> 02:57:39.120]   - I hung out with him not too long ago
[02:57:39.120 --> 02:57:40.720]   here in the Bay on his boat.
[02:57:40.720 --> 02:57:41.720]   - Yeah, he knows who we are.
[02:57:41.720 --> 02:57:42.560]   - Yeah, yeah.
[02:57:42.560 --> 02:57:43.400]   - Cool.
[02:57:43.400 --> 02:57:44.240]   - On his boat?
[02:57:44.240 --> 02:57:45.880]   - On his sailboat.
[02:57:45.880 --> 02:57:49.040]   - Oh, that guy, okay.
[02:57:49.040 --> 02:57:50.880]   (laughing)
[02:57:50.880 --> 02:57:53.280]   - Yeah, I'll never hear the story of the sailboat.
[02:57:53.280 --> 02:57:55.240]   Yes, I will reach out to him
[02:57:55.240 --> 02:57:57.280]   and see you about tomorrow for T&W.
[02:57:57.280 --> 02:57:58.120]   - Very nice.
[02:57:58.120 --> 02:57:58.960]   - Yeah, thank you.
[02:57:58.960 --> 02:58:00.480]   - We will see you then.
[02:58:00.480 --> 02:58:03.320]   - Thank you to Mr. Jeff Jarvis.
[02:58:04.320 --> 02:58:05.800]   - Should I read it all?
[02:58:05.800 --> 02:58:06.640]   I'll do it.
[02:58:06.640 --> 02:58:07.480]   - No, no bother.
[02:58:07.480 --> 02:58:08.480]   - He's the Leonard Taub Professor
[02:58:08.480 --> 02:58:10.040]   for journalistic innovation.
[02:58:10.040 --> 02:58:11.800]   We got the singers are standing by.
[02:58:11.800 --> 02:58:12.640]   - Yeah, they're there.
[02:58:12.640 --> 02:58:13.480]   - I can't even spell the show.
[02:58:13.480 --> 02:58:15.360]   - They didn't scare an ad for three hours.
[02:58:15.360 --> 02:58:16.920]   At the Craig Newmark.
[02:58:16.920 --> 02:58:19.760]    Craig Newmark 
[02:58:19.760 --> 02:58:20.600]   - That's it.
[02:58:20.600 --> 02:58:21.920]   Talking about not having a job.
[02:58:21.920 --> 02:58:23.280]   Graduate School of Journalism
[02:58:23.280 --> 02:58:26.680]   at the City University of New York City.
[02:58:26.680 --> 02:58:28.520]   - Of New York City.
[02:58:28.520 --> 02:58:30.080]   - Thank you, Jeff.
[02:58:30.080 --> 02:58:31.400]   We appreciate you being here.
[02:58:31.400 --> 02:58:32.480]   And of course, Aunt Pruitt,
[02:58:32.480 --> 02:58:34.640]   catch him on Hands-On Photography.
[02:58:34.640 --> 02:58:39.480]   And wherever he is, joy follows.
[02:58:39.480 --> 02:58:40.440]   He comes in on Sundays.
[02:58:40.440 --> 02:58:42.200]   We're gonna see a little bit more.
[02:58:42.200 --> 02:58:43.880]   And on Sunday as we,
[02:58:43.880 --> 02:58:45.840]   it's gonna be fun to watch.
[02:58:45.840 --> 02:58:49.080]   Micah, try to help you understand
[02:58:49.080 --> 02:58:51.520]   how the Macintosh works.
[02:58:51.520 --> 02:58:52.840]   It's all new, it's all different.
[02:58:52.840 --> 02:58:54.800]   - Then we're gonna have to get you deep programmed.
[02:58:54.800 --> 02:58:55.640]   - Geez.
[02:58:55.640 --> 02:58:56.480]   - A little Mac handle.
[02:58:56.480 --> 02:58:57.320]   - Yeah.
[02:58:57.320 --> 02:58:59.080]   I'm enjoying the system.
[02:58:59.080 --> 02:59:02.680]   Other than now I have to buy another DAGG monitor
[02:59:02.680 --> 02:59:04.520]   because my other monitor died.
[02:59:04.520 --> 02:59:05.640]   - Oh.
[02:59:05.640 --> 02:59:07.440]   - That's the rabbit hole that I was afraid of.
[02:59:07.440 --> 02:59:08.800]   I buy this new computer
[02:59:08.800 --> 02:59:10.120]   and I'm gonna end up having to keep
[02:59:10.120 --> 02:59:11.800]   spending more and more money.
[02:59:11.800 --> 02:59:14.040]   And sure enough, that's exactly what's happening now.
[02:59:14.040 --> 02:59:16.120]   - Yeah, but at the end of the day,
[02:59:16.120 --> 02:59:18.640]   you're gonna have another new monitor.
[02:59:18.640 --> 02:59:19.640]   That's good news.
[02:59:19.640 --> 02:59:20.760]   (laughing)
[02:59:20.760 --> 02:59:21.880]   - That's the Twitway.
[02:59:21.880 --> 02:59:23.920]   (laughing)
[02:59:23.920 --> 02:59:25.360]   - A little programming note.
[02:59:25.360 --> 02:59:26.600]   I will be back Friday,
[02:59:26.600 --> 02:59:29.360]   9 a.m. Pacific noon Eastern
[02:59:29.360 --> 02:59:30.720]   for a special triangulation.
[02:59:30.720 --> 02:59:32.840]   You know, when we can get somebody really interesting,
[02:59:32.840 --> 02:59:34.800]   I like to bring triangulation back.
[02:59:34.800 --> 02:59:37.480]   The club will be invited to join us for questioning.
[02:59:37.480 --> 02:59:38.320]   And of course we'll put it out
[02:59:38.320 --> 02:59:40.800]   as a triangulation and a Twit event feed.
[02:59:40.800 --> 02:59:42.880]   George Church, who is widely regarded
[02:59:42.880 --> 02:59:45.680]   as the founding father of Genomics.
[02:59:45.680 --> 02:59:47.600]   This guy has a lot to say.
[02:59:47.600 --> 02:59:49.440]   He's at Harvard, Robert Winthrop,
[02:59:49.440 --> 02:59:52.080]   professor of genetics at Harvard Medical School,
[02:59:52.080 --> 02:59:53.240]   professor of health sciences
[02:59:53.240 --> 02:59:55.920]   at technology at Harvard University, MIT,
[02:59:55.920 --> 02:59:57.800]   founding member of the Weiss Institute
[02:59:57.800 --> 03:00:01.600]   for Biologically Inspired Engineering at Harvard.
[03:00:01.600 --> 03:00:02.540]   How about that?
[03:00:02.540 --> 03:00:07.440]   He's a serial startup guy, serial entrepreneur,
[03:00:07.440 --> 03:00:08.880]   but I'm very interested in what he thinks
[03:00:08.880 --> 03:00:10.360]   about the future of biotech
[03:00:10.360 --> 03:00:12.480]   and particularly of Genomics.
[03:00:12.480 --> 03:00:16.160]   The father of Genomics joins us Friday
[03:00:16.160 --> 03:00:18.080]   for a special triangulation.
[03:00:18.080 --> 03:00:22.160]   9 a.m. Pacific, get up early, I will.
[03:00:22.160 --> 03:00:25.440]   And we look forward to talking to him.
[03:00:25.440 --> 03:00:27.680]   He's also a member of the Bulletin of Atomic Scientists,
[03:00:27.680 --> 03:00:29.720]   the one that sits the clock.
[03:00:29.720 --> 03:00:31.720]   The hands on the clock.
[03:00:31.720 --> 03:00:34.720]   And it's by the way, the clock is closer to midnight
[03:00:34.720 --> 03:00:35.560]   than it's ever been.
[03:00:35.560 --> 03:00:36.400]   And it's ever been.
[03:00:36.400 --> 03:00:38.240]   Oh, that's fascinating.
[03:00:38.240 --> 03:00:39.640]   That'll be an interesting conversation.
[03:00:39.640 --> 03:00:41.240]   Yes.
[03:00:41.240 --> 03:00:42.480]   We thank you all for joining us.
[03:00:42.480 --> 03:00:45.640]   We do Twig on Wednesdays, 2 p.m. Pacific, 5 p.m.
[03:00:45.640 --> 03:00:49.000]   Eastern 23, sorry, 2200 UTC.
[03:00:49.000 --> 03:00:50.120]   You can watch us do it live
[03:00:50.120 --> 03:00:54.720]   if you want to get the first edition at live.twit.tv.
[03:00:54.720 --> 03:00:56.680]   If you're watching live, join us in the chatroom.
[03:00:56.680 --> 03:00:58.200]   The IRC's open to all.
[03:00:58.200 --> 03:01:00.440]   You don't even need IRC client,
[03:01:00.440 --> 03:01:02.600]   just go to IRC.twit.tv.
[03:01:02.600 --> 03:01:06.600]   We also have of course that fun Discord now with AI.
[03:01:06.600 --> 03:01:09.240]   And if you're in the club,
[03:01:09.240 --> 03:01:13.240]   join everybody's putting up their AI creations.
[03:01:13.240 --> 03:01:14.360]   That's pretty funny.
[03:01:14.360 --> 03:01:15.600]   Is that me?
[03:01:15.600 --> 03:01:16.760]   As free to call.
[03:01:16.760 --> 03:01:17.600]   Okay.
[03:01:17.600 --> 03:01:19.240]   I like it.
[03:01:19.240 --> 03:01:20.680]   I like it.
[03:01:20.680 --> 03:01:21.920]   If you're in the Discord,
[03:01:21.920 --> 03:01:23.000]   you can join us there too.
[03:01:23.000 --> 03:01:25.600]   We have a conversation going throughout the show.
[03:01:25.600 --> 03:01:27.160]   After the fact on demand versions,
[03:01:27.160 --> 03:01:31.360]   the show available to all ads supported at twit.tv/twig.
[03:01:31.360 --> 03:01:34.200]   There's a YouTube channel also ads supported,
[03:01:34.200 --> 03:01:35.800]   dedicated to this week in Google.
[03:01:35.800 --> 03:01:37.480]   And of course you could subscribe
[03:01:37.480 --> 03:01:38.800]   in your favorite podcast client.
[03:01:38.800 --> 03:01:41.720]   Get it automatically the minute we're done.
[03:01:41.720 --> 03:01:42.880]   Fun show today everybody.
[03:01:42.880 --> 03:01:44.200]   Thank you for being here.
[03:01:44.200 --> 03:01:47.040]   We'll see you next time on this week in Google.
[03:01:47.040 --> 03:01:47.880]   Bye bye.
[03:01:47.880 --> 03:01:50.360]   Bye bye.
[03:01:50.360 --> 03:01:51.400]   Hey, I know you're super busy.
[03:01:51.400 --> 03:01:52.480]   So I won't keep you long,
[03:01:52.480 --> 03:01:54.760]   but I wanted to tell you about a show here
[03:01:54.760 --> 03:01:58.360]   on the Twit Network called Tech News Weekly.
[03:01:58.360 --> 03:01:59.840]   You are a busy person.
[03:01:59.840 --> 03:02:01.120]   And during your week,
[03:02:01.120 --> 03:02:04.080]   you may want to learn about all the tech news
[03:02:04.080 --> 03:02:07.840]   that's fit to, well, say, not print here on Twit.
[03:02:07.840 --> 03:02:09.360]   It's Tech News Weekly.
[03:02:09.360 --> 03:02:12.160]   Me, Mike, Sergeant, my co-host, Jason Howell.
[03:02:12.160 --> 03:02:14.960]   We talk to and about the people making
[03:02:14.960 --> 03:02:16.400]   and breaking the tech news.
[03:02:16.400 --> 03:02:20.000]   And we love the opportunity to get to share those stories
[03:02:20.000 --> 03:02:22.600]   with you and let the people who wrote them
[03:02:22.600 --> 03:02:24.560]   or broke them share them as well.
[03:02:24.560 --> 03:02:27.000]   So I hope you check it out every Thursday
[03:02:27.000 --> 03:02:28.320]   right here on Twit.
[03:02:28.320 --> 03:02:30.900]   (upbeat music)
[03:02:30.900 --> 03:02:33.480]   (upbeat music)
[03:02:33.480 --> 03:02:36.060]   (upbeat music)
[03:02:36.060 --> 03:02:37.060]   .
[03:02:37.060 --> 03:02:39.820]   [MUSIC PLAYING]

