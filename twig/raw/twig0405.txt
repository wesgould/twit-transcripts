;FFMETADATA1
title=Google I/O
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=405
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.320]   It's time for Twig this week at Google. We're in some strange surroundings.
[00:00:04.320 --> 00:00:06.720]   Jason Howell is sitting to my right.
[00:00:06.720 --> 00:00:10.460]   I've got Jeff Jarvis to my left and floating in the air where she belongs,
[00:00:10.460 --> 00:00:12.400]   the angel Stacy Higginbotham.
[00:00:12.400 --> 00:00:17.400]   We've gathered together to talk about Google I/O's keynote for 2017.
[00:00:17.400 --> 00:00:20.400]   Lots of announcements we're going to go through one by one next.
[00:00:20.400 --> 00:00:21.400]   On Twig.
[00:00:21.400 --> 00:00:26.000]   Netcast you love.
[00:00:26.000 --> 00:00:28.000]   Front people you trust.
[00:00:28.000 --> 00:00:33.000]   This is Twig.
[00:00:33.000 --> 00:00:41.000]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:41.000 --> 00:00:48.000]   This is Twig this week in Google.
[00:00:48.000 --> 00:00:53.000]   Episode 405 recorded Wednesday, May 17, 2017.
[00:00:53.000 --> 00:00:55.000]   Google I/O.
[00:00:55.000 --> 00:01:01.000]   This week at Google is brought to you by Upside, the smart new way to save big money on business travel.
[00:01:01.000 --> 00:01:08.000]   Visit Upside.com and enter the code Twig to be guaranteed to receive at least a $100 gift card when you book your first trip.
[00:01:08.000 --> 00:01:18.000]   It's time for Twig this week in Google in a little bit of an unusual circumstance because today was Google I/O, their Developers Conference.
[00:01:18.000 --> 00:01:28.000]   We were down at Shoreline Amphitheater for Google I/O in Mountain View, California joining me in a beautiful studio provided by Google.
[00:01:28.000 --> 00:01:30.000]   Thank you Google and our good friend Chris Dibono.
[00:01:30.000 --> 00:01:35.000]   We'll talk to Chris in just a second from Google's open source software initiative.
[00:01:35.000 --> 00:01:37.000]   But first let me introduce our panel.
[00:01:37.000 --> 00:01:44.000]   Of course, Jeff Jarvis is here unusually next to me, which means I can do any can do.
[00:01:44.000 --> 00:01:46.000]   And that's great.
[00:01:46.000 --> 00:01:53.000]   I'm not sure we have figured out the technology to do that with the TV avatars that we use in the studio at home.
[00:01:53.000 --> 00:01:55.000]   But I'm glad we could do that here.
[00:01:55.000 --> 00:01:58.000]   Google is amazing really. They're just amazing.
[00:01:58.000 --> 00:02:00.000]   They are indeed.
[00:02:00.000 --> 00:02:06.000]   Also here, Jeff from our All About Android show and Tech News today, Jason Howell.
[00:02:06.000 --> 00:02:07.000]   Good to see you.
[00:02:07.000 --> 00:02:08.000]   I am doing great.
[00:02:08.000 --> 00:02:14.000]   Three of us set together in the box seats, the press box seats to watch the event.
[00:02:14.000 --> 00:02:17.000]   But Stacey Higginbotham probably had the best seat.
[00:02:17.000 --> 00:02:21.000]   She was back in our studios watching the stream with Nathan Noliver's child.
[00:02:21.000 --> 00:02:22.000]   Hello Stacey.
[00:02:22.000 --> 00:02:26.000]   Hello, yes, I have the same seat I have every week.
[00:02:26.000 --> 00:02:27.000]   It's a good seat.
[00:02:27.000 --> 00:02:28.000]   It's a comfortable seat.
[00:02:28.000 --> 00:02:29.000]   I like it.
[00:02:29.000 --> 00:02:30.000]   What a dog thing.
[00:02:30.000 --> 00:02:33.000]   You know, the dog was under the bed.
[00:02:33.000 --> 00:02:34.000]   She didn't have any opinions.
[00:02:34.000 --> 00:02:37.000]   She's not in the room today.
[00:02:37.000 --> 00:02:42.000]   Usually I am where you were, which is watching the stream of a keynote.
[00:02:42.000 --> 00:02:45.000]   And then commenting on it.
[00:02:45.000 --> 00:02:47.000]   And thank you for doing that this morning.
[00:02:47.000 --> 00:02:48.000]   I appreciate it.
[00:02:48.000 --> 00:02:53.000]   But I thought for once maybe the last time I thought I'd come into one of these events
[00:02:53.000 --> 00:02:54.000]   in person.
[00:02:54.000 --> 00:02:56.000]   Actually, it was kind of fun.
[00:02:56.000 --> 00:02:59.000]   Shoreline Amphitheater, as you may or may not know, is a concert venue.
[00:02:59.000 --> 00:03:02.000]   It's the same place Google did this last year.
[00:03:02.000 --> 00:03:03.000]   And it's open air.
[00:03:03.000 --> 00:03:09.000]   And we were sitting right underneath the edge between the sky and the, you know, the sheltered
[00:03:09.000 --> 00:03:13.000]   space just waiting for the sun to come over and start broiling us.
[00:03:13.000 --> 00:03:14.000]   I guess last year.
[00:03:14.000 --> 00:03:15.000]   Never came.
[00:03:15.000 --> 00:03:16.000]   We froze.
[00:03:16.000 --> 00:03:18.000]   Last year you broiled.
[00:03:18.000 --> 00:03:20.000]   There were cases of sunburn out there.
[00:03:20.000 --> 00:03:22.000]   They gave us Google a sudden lotion.
[00:03:22.000 --> 00:03:24.000]   Oh, that's nice.
[00:03:24.000 --> 00:03:31.000]   So let's, let's, I guess I, I, I'm going to follow Ron Amadio's live blog at Rars Technica
[00:03:31.000 --> 00:03:32.000]   as he went through the events.
[00:03:32.000 --> 00:03:36.000]   So that way we can kind of chronologically cover and make sure we don't miss anything
[00:03:36.000 --> 00:03:38.000]   that Google announced.
[00:03:38.000 --> 00:03:43.040]   But before we do, I'm curious, Stacy, what you and Nate thought of it from your vantage
[00:03:43.040 --> 00:03:44.040]   point.
[00:03:44.040 --> 00:03:48.760]   Was this an exciting gripping keynote?
[00:03:48.760 --> 00:03:51.600]   So I was going to ask you guys if it was different for y'all.
[00:03:51.600 --> 00:03:56.680]   So on our end watching it, we were not super pumped.
[00:03:56.680 --> 00:04:01.240]   None of the things made us go wow to the extent.
[00:04:01.240 --> 00:04:05.880]   And most, we noticed that the two themes that emerged was context, which is really
[00:04:05.880 --> 00:04:08.720]   exciting, but it's really hard to show.
[00:04:08.720 --> 00:04:14.520]   So context was big building on AI and giving people, giving AI context about where we are
[00:04:14.520 --> 00:04:17.840]   to give us better suggestions, which is awesome.
[00:04:17.840 --> 00:04:21.560]   And then two, none of it's going to be out now.
[00:04:21.560 --> 00:04:24.880]   Like nothing that was super exciting seems to be available.
[00:04:24.880 --> 00:04:28.880]   And of course I was thrilled about the TPU stuff, but that's just me.
[00:04:28.880 --> 00:04:33.560]   Yeah, we'll get to the TPU real soon because that was very interesting, I thought.
[00:04:33.560 --> 00:04:38.240]   In fact, that was part of Sundar Pichai's context, as you say, talking about how it
[00:04:38.240 --> 00:04:41.360]   was a mobile first world until recently.
[00:04:41.360 --> 00:04:43.760]   And now it's an AI first world.
[00:04:43.760 --> 00:04:44.760]   This is not something new.
[00:04:44.760 --> 00:04:48.280]   Google said this last year at Google I/O as well, I think.
[00:04:48.280 --> 00:04:52.360]   No, I mean, I think the whole thing was incremental.
[00:04:52.360 --> 00:04:56.200]   They took the base they had last year and they added considerably to it.
[00:04:56.200 --> 00:04:57.840]   The problem is, as Stacy said, it's quite right.
[00:04:57.840 --> 00:05:01.560]   The things that are impressive are not things that you hold in your hand as a consumer and
[00:05:01.560 --> 00:05:03.360]   say, well, that's a great new gadget.
[00:05:03.360 --> 00:05:05.280]   The AI work is phenomenal.
[00:05:05.280 --> 00:05:10.560]   I think that the photo recognition stuff, Google Lens, is going to be really important
[00:05:10.560 --> 00:05:14.520]   because what it says is that Google will know the context of Stacy said of where you
[00:05:14.520 --> 00:05:15.880]   are and add value to that.
[00:05:15.880 --> 00:05:16.880]   That's a big deal.
[00:05:16.880 --> 00:05:19.600]   So there's a lot of big deal technical stuff underneath.
[00:05:19.600 --> 00:05:21.960]   The tensor's new stuff is great.
[00:05:21.960 --> 00:05:27.040]   But in terms of, if I were trying to write a consumer story, there's a lot of German
[00:05:27.040 --> 00:05:28.040]   journalists who are here.
[00:05:28.040 --> 00:05:29.360]   One guy has to write like 15 stories.
[00:05:29.360 --> 00:05:32.320]   I don't envy him trying to figure out what stories to write for a consumer audience out
[00:05:32.320 --> 00:05:33.320]   of this.
[00:05:33.320 --> 00:05:34.320]   How about you, Jason?
[00:05:34.320 --> 00:05:40.280]   Well, I think obviously there's a big kind of dedication to, like you say, moving from
[00:05:40.280 --> 00:05:45.720]   mobile first to AI first, which we've seen in the past year and a half, especially everyone
[00:05:45.720 --> 00:05:48.240]   is buckling down on AI.
[00:05:48.240 --> 00:05:52.440]   And I think the challenge that Google had kind of going into IO with an event that is
[00:05:52.440 --> 00:05:59.440]   so heavily focused on AI is that we almost have been inundated with AI as it is already,
[00:05:59.440 --> 00:06:03.200]   as the answer to everyone's prayers for all of these different things.
[00:06:03.200 --> 00:06:06.800]   So when Google comes out and says, look, what else we're doing with AI, it's a what
[00:06:06.800 --> 00:06:09.600]   else instead of you've never seen this before.
[00:06:09.600 --> 00:06:10.600]   You know what I mean?
[00:06:10.600 --> 00:06:16.720]   There were few announcements this time that you could easily point at and say, oh my God,
[00:06:16.720 --> 00:06:18.040]   mind blown.
[00:06:18.040 --> 00:06:20.000]   That's amazing that you can even do that right now.
[00:06:20.000 --> 00:06:23.520]   There's some really impressive things, but we've already kind of bought into the idea
[00:06:23.520 --> 00:06:28.360]   that AI is this grand savior and is able to take computing to a next level.
[00:06:28.360 --> 00:06:32.320]   So it's kind of like, yeah, I could have assumed that they'd probably go there next.
[00:06:32.320 --> 00:06:33.400]   There were a lot of things missing.
[00:06:33.400 --> 00:06:36.920]   I mean, we wanted to see Google Chrome, Google Pixel maybe.
[00:06:36.920 --> 00:06:40.680]   At least some mention of Chromebooks and Chrome OS.
[00:06:40.680 --> 00:06:41.840]   No mention at all.
[00:06:41.840 --> 00:06:42.840]   No new Pixel C.
[00:06:42.840 --> 00:06:44.840]   No new tablet.
[00:06:44.840 --> 00:06:47.680]   So that's a good thing.
[00:06:47.680 --> 00:06:53.200]   I was going to say Nathan was upset that there was no VR stuff, but going back to this
[00:06:53.200 --> 00:06:54.200]   AI concept.
[00:06:54.200 --> 00:06:59.920]   Well, wait a minute, before you say that, there is a VR keynote tomorrow.
[00:06:59.920 --> 00:07:02.120]   So and there was VR stuff at the end.
[00:07:02.120 --> 00:07:03.120]   I don't know.
[00:07:03.120 --> 00:07:04.120]   Maybe you didn't get to the end.
[00:07:04.120 --> 00:07:05.920]   No, I was there the whole time.
[00:07:05.920 --> 00:07:06.920]   I promise.
[00:07:06.920 --> 00:07:09.080]   Even paying attention.
[00:07:09.080 --> 00:07:13.120]   He was sad that they didn't have a dedicated AR device that they could show on stage.
[00:07:13.120 --> 00:07:14.440]   They introduced this.
[00:07:14.440 --> 00:07:18.720]   What they did is they introduced a reference design and said HTC and Lenovo were going
[00:07:18.720 --> 00:07:20.320]   to build a dedicated AR.
[00:07:20.320 --> 00:07:21.320]   I'm sorry.
[00:07:21.320 --> 00:07:22.400]   But the big deal.
[00:07:22.400 --> 00:07:24.600]   This is a standalone VR today.
[00:07:24.600 --> 00:07:28.480]   No one makes this even HTC with the vibe is tethered to a PC.
[00:07:28.480 --> 00:07:30.760]   That's Oculus without all the crap.
[00:07:30.760 --> 00:07:32.560]   Imagine, I mean, I don't know the price.
[00:07:32.560 --> 00:07:33.560]   You're right.
[00:07:33.560 --> 00:07:34.560]   We don't know anything.
[00:07:34.560 --> 00:07:35.560]   It's not here yet.
[00:07:35.560 --> 00:07:36.560]   Yeah.
[00:07:36.560 --> 00:07:37.560]   And we don't know the price point.
[00:07:37.560 --> 00:07:38.560]   I just want to put that in there.
[00:07:38.560 --> 00:07:42.440]   But well, but I don't think Google announces something like that unless they believe it can
[00:07:42.440 --> 00:07:44.560]   be made and made in a sense.
[00:07:44.560 --> 00:07:47.960]   I mean, it would be embarrassing if it weren't made this year, for instance.
[00:07:47.960 --> 00:07:54.240]   So Isis, I think it shows Google says we now have this is somewhat similar in my mind to
[00:07:54.240 --> 00:07:56.400]   what they did with the camera.
[00:07:56.400 --> 00:07:59.560]   They had this great technology, this image processing technology.
[00:07:59.560 --> 00:08:03.400]   They've developed using TensorFlow and AI.
[00:08:03.400 --> 00:08:07.840]   And then they had the Pixel phone and they said, well, we have limitations in the hardware
[00:08:07.840 --> 00:08:09.040]   that's in the phone.
[00:08:09.040 --> 00:08:13.960]   But what if we applied some of this great software and AI to what we're doing, the process we're
[00:08:13.960 --> 00:08:18.760]   doing something, frankly, that Apple's been doing and had scooped everybody with for years.
[00:08:18.760 --> 00:08:23.400]   And suddenly the Google Pixel is the best camera phone on the market.
[00:08:23.400 --> 00:08:29.520]   And I think what they're saying, as I interpret it, admittedly, they said nothing.
[00:08:29.520 --> 00:08:34.600]   It's we figured out a way to take consumer hardware, something that's going to be relatively
[00:08:34.600 --> 00:08:35.920]   affordable.
[00:08:35.920 --> 00:08:38.640]   It's going to have has to have some battery life, right?
[00:08:38.640 --> 00:08:40.760]   It has to have decent screens.
[00:08:40.760 --> 00:08:43.160]   It has to you talked about latency.
[00:08:43.160 --> 00:08:45.000]   Head latency is a huge problem in VR.
[00:08:45.000 --> 00:08:46.800]   It's what makes people nauseous.
[00:08:46.800 --> 00:08:52.520]   And they've figured out a way to use some form of motion sensitivity in this device to
[00:08:52.520 --> 00:08:53.520]   reduce latency.
[00:08:53.520 --> 00:08:54.520]   They've done some stuff.
[00:08:54.520 --> 00:09:02.200]   And I suspect a lot of it is done with existing hardware and as with the camera applying some
[00:09:02.200 --> 00:09:04.560]   great software smarts based on AI to it.
[00:09:04.560 --> 00:09:08.960]   So I'm reading well between the lines, I understand, but I think that there might be
[00:09:08.960 --> 00:09:11.120]   something of great interest there.
[00:09:11.120 --> 00:09:13.600]   And to be fair, we won't find out till tomorrow.
[00:09:13.600 --> 00:09:15.400]   That's when the bird is it tomorrow?
[00:09:15.400 --> 00:09:18.480]   Yeah, it was the VR kind of keynote.
[00:09:18.480 --> 00:09:23.080]   Yeah, I'm very curious to see of the standalone, like the question that I've had about the
[00:09:23.080 --> 00:09:27.800]   idea, because this was a rumor that Wall Street Journal had, I think, early last year.
[00:09:27.800 --> 00:09:32.320]   And we've kind of been waiting for the standalone VR headset to happen is, OK, if Google's doing
[00:09:32.320 --> 00:09:37.720]   a standalone VR headset, is this mobile VR, but without the need to put your phone in?
[00:09:37.720 --> 00:09:44.760]   Or is it HTC Vive/Oculus Rift level VR, kind of like the premium level of VR?
[00:09:44.760 --> 00:09:47.120]   Or is this somewhere in between?
[00:09:47.120 --> 00:09:51.920]   And kind of explaining why that's necessary if it veers more for the mobile side, which
[00:09:51.920 --> 00:09:54.520]   is what I worry it might do.
[00:09:54.520 --> 00:09:57.480]   But what if you could do Daydream without buying?
[00:09:57.480 --> 00:09:59.640]   It's got to be less than an Android phone, right?
[00:09:59.640 --> 00:10:03.000]   Because Daydream could be done right now with a $700, $800 phone.
[00:10:03.000 --> 00:10:09.200]   So let's say it's a $500 or $400 price point, but it's as good as Daydream VR today.
[00:10:09.200 --> 00:10:10.560]   So is that going to be worth it?
[00:10:10.560 --> 00:10:12.760]   I'd be very mad about that.
[00:10:12.760 --> 00:10:13.760]   Not about that.
[00:10:13.760 --> 00:10:15.760]   I'd be very mad about that.
[00:10:15.760 --> 00:10:16.760]   Yeah, about that.
[00:10:16.760 --> 00:10:21.400]   If their whole idea is here's a dedicated device that allows you to do VR in the same way
[00:10:21.400 --> 00:10:24.920]   that you already can with the device that you have and a very inexpensive viewer, but
[00:10:24.920 --> 00:10:28.920]   you don't need your phone anymore, that really does very little, that's a little extra value.
[00:10:28.920 --> 00:10:29.920]   Wait, wait, wait.
[00:10:29.920 --> 00:10:31.920]   You're not thinking about the market, the classroom, for instance.
[00:10:31.920 --> 00:10:37.560]   Right now, while the daydream VR for 30 kids in the classroom, you'd have to buy 30 high-end
[00:10:37.560 --> 00:10:38.560]   smartphones.
[00:10:38.560 --> 00:10:39.560]   Okay, all right.
[00:10:39.560 --> 00:10:44.240]   So there is a market there, but I think what we're really dancing around is how today,
[00:10:44.240 --> 00:10:45.640]   how big is the VR market?
[00:10:45.640 --> 00:10:50.000]   I think it's a lot smaller than anybody wishes it were.
[00:10:50.000 --> 00:10:54.920]   And who was it was telling us, Levy was telling us that Sundar put up or somebody put up another
[00:10:54.920 --> 00:10:57.720]   post separately today saying be patient with VR.
[00:10:57.720 --> 00:10:59.360]   It's not there yet, we know.
[00:10:59.360 --> 00:11:04.040]   But I think the VR guy is relieving lunch.
[00:11:04.040 --> 00:11:05.040]   Is it clay?
[00:11:05.040 --> 00:11:06.040]   Is it clay?
[00:11:06.040 --> 00:11:07.040]   Is it?
[00:11:07.040 --> 00:11:08.040]   Yeah.
[00:11:08.040 --> 00:11:09.040]   Okay, put up post.
[00:11:09.040 --> 00:11:11.040]   So, but the flavor, B-A-V-O-R.
[00:11:11.040 --> 00:11:12.040]   Yeah.
[00:11:12.040 --> 00:11:13.840]   So the problem is $500.
[00:11:13.840 --> 00:11:14.840]   I'd say it's $500.
[00:11:14.840 --> 00:11:18.440]   Is everybody going to buy that at Christmas?
[00:11:18.440 --> 00:11:19.440]   That's wild speculation.
[00:11:19.440 --> 00:11:21.440]   I don't, yeah, no, I don't think it's designed for that.
[00:11:21.440 --> 00:11:22.440]   So I'm all at $350.
[00:11:22.440 --> 00:11:25.920]   I don't know that there's a consumer market for VR as a device yet.
[00:11:25.920 --> 00:11:28.680]   The advantage is you add $100 on your phone.
[00:11:28.680 --> 00:11:29.680]   Okay.
[00:11:29.680 --> 00:11:30.680]   Yeah, that's a hassle.
[00:11:30.680 --> 00:11:32.400]   Yes, it doesn't go with the full capability.
[00:11:32.400 --> 00:11:36.720]   But to buy a separate dedicated device for VR at pick a price.
[00:11:36.720 --> 00:11:41.720]   Well, pick a price that's good for schools because I think that this is where Google,
[00:11:41.720 --> 00:11:42.720]   I mean, schools are poor.
[00:11:42.720 --> 00:11:45.240]   Google has done a real add-on for them.
[00:11:45.240 --> 00:11:51.440]   A Chromebook they can afford because it does curricular material and makes them more efficient
[00:11:51.440 --> 00:11:54.640]   to buy an extra device to show VR in a school.
[00:11:54.640 --> 00:11:56.360]   You've got to be a private school to afford that.
[00:11:56.360 --> 00:11:57.800]   We're talking a little bit in a vacuum.
[00:11:57.800 --> 00:12:01.320]   We'll find out more tomorrow and maybe more later on.
[00:12:01.320 --> 00:12:04.040]   And there's really nothing to say until they ship a product.
[00:12:04.040 --> 00:12:05.040]   But you're right.
[00:12:05.040 --> 00:12:06.040]   I'm skeptical about the VR market.
[00:12:06.040 --> 00:12:07.040]   We'll see.
[00:12:07.040 --> 00:12:11.640]   I think it's not surprising that Clay would post something that says, "Hey, you know,
[00:12:11.640 --> 00:12:12.640]   everybody expects someone for VR.
[00:12:12.640 --> 00:12:13.640]   They're asking for Chinese.
[00:12:13.640 --> 00:12:14.640]   I think AR stuff.
[00:12:14.640 --> 00:12:19.920]   I mean, Facebook has done Soto Voche on VR and loud on AR.
[00:12:19.920 --> 00:12:24.120]   And in a way, I think what we saw today from Google with the photo stuff, with the "Here
[00:12:24.120 --> 00:12:25.120]   is your environment.
[00:12:25.120 --> 00:12:26.120]   Here is that restaurant.
[00:12:26.120 --> 00:12:29.680]   I will understand the image of that restaurant and I will give you data based on that."
[00:12:29.680 --> 00:12:30.680]   Oh, boy, that was...
[00:12:30.680 --> 00:12:31.680]   That's an AR world.
[00:12:31.680 --> 00:12:33.680]   The answers are going to come in AR first.
[00:12:33.680 --> 00:12:34.680]   I think.
[00:12:34.680 --> 00:12:35.680]   Sure.
[00:12:35.680 --> 00:12:36.680]   And they're going to come through your camera on your phone.
[00:12:36.680 --> 00:12:37.680]   I think...
[00:12:37.680 --> 00:12:40.120]   I plan to do this in chronological order, which is just...
[00:12:40.120 --> 00:12:41.720]   Just briefly blown away.
[00:12:41.720 --> 00:12:45.600]   We've actually jumped to the end of the keynote somehow.
[00:12:45.600 --> 00:12:46.600]   And it was...
[00:12:46.600 --> 00:12:51.360]   It was like we started at the end of the book.
[00:12:51.360 --> 00:12:52.360]   But that wasn't the payoff.
[00:12:52.360 --> 00:12:53.800]   There was a lot of stuff.
[00:12:53.800 --> 00:12:59.000]   Sundor Pachay took the stage at the beginning, two minutes past the hour, which was pretty
[00:12:59.000 --> 00:13:05.400]   timely for any keynote and gave out some numbers that were pretty amazing for any company.
[00:13:05.400 --> 00:13:13.520]   One billion active users a month, one billion monthly active users for Gmail, for Android,
[00:13:13.520 --> 00:13:16.760]   for Chrome, for Google Play and for Maps.
[00:13:16.760 --> 00:13:17.760]   How is that?
[00:13:17.760 --> 00:13:20.520]   That's a pretty good coincidence that it'll all be exactly one billion.
[00:13:20.520 --> 00:13:21.520]   A gibberate.
[00:13:21.520 --> 00:13:22.520]   Yeah.
[00:13:22.520 --> 00:13:25.760]   This is an AI world where you do with round numbers.
[00:13:25.760 --> 00:13:26.760]   [laughter]
[00:13:26.760 --> 00:13:27.760]   Approximation.
[00:13:27.760 --> 00:13:29.240]   Okay, here's another round number.
[00:13:29.240 --> 00:13:30.240]   500 million active...
[00:13:30.240 --> 00:13:31.240]   I'm sorry.
[00:13:31.240 --> 00:13:33.000]   I just lowered my seat.
[00:13:33.000 --> 00:13:36.160]   500 million active users on Google Photos.
[00:13:36.160 --> 00:13:37.160]   With 1.2 billion.
[00:13:37.160 --> 00:13:40.040]   You're taking all the numbers, but I've got number three.
[00:13:40.040 --> 00:13:43.000]   1.2 billion photos a day.
[00:13:43.000 --> 00:13:44.000]   That's amazing.
[00:13:44.000 --> 00:13:45.000]   That is.
[00:13:45.000 --> 00:13:46.000]   2 billion active Android devices.
[00:13:46.000 --> 00:13:48.400]   But Jason and I turned each other in this moment.
[00:13:48.400 --> 00:13:51.480]   Imagine if they started Google Plus now.
[00:13:51.480 --> 00:13:52.480]   Google Plus.
[00:13:52.480 --> 00:13:55.200]   What's that?
[00:13:55.200 --> 00:13:56.200]   Nobody mentioned Google Plus.
[00:13:56.200 --> 00:13:57.200]   No, no, no.
[00:13:57.200 --> 00:14:00.360]   But imagine if you had this functionality around the photos and everything would be an entirely
[00:14:00.360 --> 00:14:01.360]   different effort.
[00:14:01.360 --> 00:14:02.360]   Right.
[00:14:02.360 --> 00:14:03.360]   That might be.
[00:14:03.360 --> 00:14:08.400]   2 billion Android devices, active Android devices, that's worth anything else.
[00:14:08.400 --> 00:14:09.400]   That's insane.
[00:14:09.400 --> 00:14:11.400]   There's nothing even close.
[00:14:11.400 --> 00:14:12.400]   That's even worse.
[00:14:12.400 --> 00:14:14.880]   The sheer numbers of that, it's hard to even understand.
[00:14:14.880 --> 00:14:18.320]   A little later on, we'll talk about it, but they announced something called Android Go,
[00:14:18.320 --> 00:14:24.120]   which is a low-cost version of Android designed for phones with less than a gigabyte of memory,
[00:14:24.120 --> 00:14:28.160]   lowers processor speeds, lower cost, obviously, is key to that.
[00:14:28.160 --> 00:14:31.560]   That's the key to getting, as they said, to the next billion.
[00:14:31.560 --> 00:14:35.600]   3 billion active users by next year, we'll see.
[00:14:35.600 --> 00:14:36.840]   Let's see, what else?
[00:14:36.840 --> 00:14:41.160]   Search Maps and other services all use machine learning.
[00:14:41.160 --> 00:14:47.240]   And actually, we've seen machine learning already in Google's Assistant, which has some
[00:14:47.240 --> 00:14:49.040]   smart reply features.
[00:14:49.040 --> 00:14:52.720]   But Chai had asked that smart reply is going to roll out to all Gmail users.
[00:14:52.720 --> 00:14:54.240]   There's a lot of questions right there.
[00:14:54.240 --> 00:14:57.360]   So smart reply, I like and I use it.
[00:14:57.360 --> 00:14:59.600]   But every time I use it, I feel guilty.
[00:14:59.600 --> 00:15:01.080]   I feel like I'm saying to...
[00:15:01.080 --> 00:15:03.440]   I feel like, well, you're not good enough to respond to myself.
[00:15:03.440 --> 00:15:05.360]   I'm going to let Google respond for me.
[00:15:05.360 --> 00:15:08.160]   I just had a few exclamation marks, maybe some happy faces.
[00:15:08.160 --> 00:15:10.080]   Well, the problem is, you're still here.
[00:15:10.080 --> 00:15:11.560]   I thought you left.
[00:15:11.560 --> 00:15:14.800]   No, it's very hard to like jump in on such a...
[00:15:14.800 --> 00:15:17.200]   It's harder than usual Stacey, right?
[00:15:17.200 --> 00:15:18.200]   It's hard.
[00:15:18.200 --> 00:15:19.200]   It's always hard.
[00:15:19.200 --> 00:15:20.200]   It's so sorry.
[00:15:20.200 --> 00:15:21.920]   No, it's okay.
[00:15:21.920 --> 00:15:28.000]   So smart reply, so etiquette master here, Jeff, I think you're fine because smart reply
[00:15:28.000 --> 00:15:32.160]   really only works well when you're trying to convey super basic information.
[00:15:32.160 --> 00:15:36.200]   Like if someone tells you their kid is dying, you're not going to get a great smart reply,
[00:15:36.200 --> 00:15:37.200]   right?
[00:15:37.200 --> 00:15:38.200]   Then you...
[00:15:38.200 --> 00:15:39.520]   That's when you craft the heartfelt message.
[00:15:39.520 --> 00:15:43.240]   So I wouldn't feel guilty for just replying to something like, "I'll be there later,"
[00:15:43.240 --> 00:15:44.240]   or "Let's set a date."
[00:15:44.240 --> 00:15:48.560]   I think it's better than what probably happens now, which is you put it away and you never
[00:15:48.560 --> 00:15:49.560]   answer the email.
[00:15:49.560 --> 00:15:52.720]   So there were two Googlers in the room who laughed at that line.
[00:15:52.720 --> 00:15:57.840]   Is this something that struck true to you?
[00:15:57.840 --> 00:16:02.720]   No, I have to say though, even the smart reply they showed on the screen wasn't that smart
[00:16:02.720 --> 00:16:06.640]   because the question was, "Hey, let's go to a show.
[00:16:06.640 --> 00:16:08.000]   You want to go Saturday or Sunday?"
[00:16:08.000 --> 00:16:13.840]   And the three smart replies were, "Either one is okay, Saturday or whatever."
[00:16:13.840 --> 00:16:14.840]   No mention of Sunday.
[00:16:14.840 --> 00:16:15.840]   Sunday.
[00:16:15.840 --> 00:16:16.840]   What?
[00:16:16.840 --> 00:16:17.840]   Sunday.
[00:16:17.840 --> 00:16:18.840]   Because it knows your business Sunday.
[00:16:18.840 --> 00:16:19.840]   It knows.
[00:16:19.840 --> 00:16:20.840]   There is a mess context.
[00:16:20.840 --> 00:16:21.840]   No, I don't think...
[00:16:21.840 --> 00:16:24.800]   But then either would not be available.
[00:16:24.800 --> 00:16:25.800]   Yeah.
[00:16:25.800 --> 00:16:26.800]   That's cute.
[00:16:26.800 --> 00:16:27.800]   Yeah.
[00:16:27.800 --> 00:16:28.800]   Okay.
[00:16:28.800 --> 00:16:29.800]   One day it's just...
[00:16:29.800 --> 00:16:30.800]   Here's the other thing.
[00:16:30.800 --> 00:16:31.800]   Stacey.
[00:16:31.800 --> 00:16:37.120]   So the other problem is that the replies add more bangers, as we say in the news business.
[00:16:37.120 --> 00:16:38.560]   More exclamation points that I would ever use.
[00:16:38.560 --> 00:16:41.280]   Now, does that because Google thinks that I use a lot of exclamation points or does it
[00:16:41.280 --> 00:16:42.920]   do it for everybody?
[00:16:42.920 --> 00:16:45.520]   I don't think it's customized to you yet.
[00:16:45.520 --> 00:16:46.520]   It's not customized to me.
[00:16:46.520 --> 00:16:49.440]   So the whole world is going crazy with exclamation points.
[00:16:49.440 --> 00:16:50.760]   Everyone loves exclamation points.
[00:16:50.760 --> 00:16:51.760]   I use them all with...
[00:16:51.760 --> 00:16:52.760]   They're awesome.
[00:16:52.760 --> 00:16:53.760]   They're awesome.
[00:16:53.760 --> 00:16:54.760]   They're a great new Jeff.
[00:16:54.760 --> 00:16:55.760]   Oh, he's a professor.
[00:16:55.760 --> 00:16:57.560]   He probably uses the Oxford comma.
[00:16:57.560 --> 00:16:58.560]   Yeah.
[00:16:58.560 --> 00:17:02.040]   I do use the Oxford comma and it's supposed to be...
[00:17:02.040 --> 00:17:03.040]   Two spaces are one F.
[00:17:03.040 --> 00:17:05.040]   No, I don't even get into it.
[00:17:05.040 --> 00:17:06.040]   No.
[00:17:06.040 --> 00:17:07.040]   No.
[00:17:07.040 --> 00:17:08.040]   So I would love smart reply to...
[00:17:08.040 --> 00:17:09.040]   It's not a smart reply to...
[00:17:09.040 --> 00:17:12.480]   It could smart reply add emoji replies.
[00:17:12.480 --> 00:17:13.480]   Yeah.
[00:17:13.480 --> 00:17:18.960]   You know, I find that the emoji replies so the keyboard auto connects or auto doesn't
[00:17:18.960 --> 00:17:22.400]   correct but it suggests emojis but it always does it way too late for me.
[00:17:22.400 --> 00:17:25.240]   I've already typed the word and then it's like, "Oh, would you like an emoji?"
[00:17:25.240 --> 00:17:28.280]   And that totally defeats the purpose of an emoji, in my opinion.
[00:17:28.280 --> 00:17:30.120]   I'm not having to type the word.
[00:17:30.120 --> 00:17:31.120]   Yeah.
[00:17:31.120 --> 00:17:33.920]   Emoji is pre-literate, not post-literate.
[00:17:33.920 --> 00:17:34.920]   Exactly.
[00:17:34.920 --> 00:17:35.920]   Ugh.
[00:17:35.920 --> 00:17:36.920]   Ugh.
[00:17:36.920 --> 00:17:37.920]   Ugh.
[00:17:37.920 --> 00:17:38.920]   Image recognition.
[00:17:38.920 --> 00:17:39.920]   Wait, wait, wait.
[00:17:39.920 --> 00:17:40.920]   I'm sorry, I'm going to say this.
[00:17:40.920 --> 00:17:43.600]   Wait till Trump discovers emojis.
[00:17:43.600 --> 00:17:44.600]   He's never used an emoji.
[00:17:44.600 --> 00:17:45.600]   He's never used an emoji.
[00:17:45.600 --> 00:17:47.320]   He's got somebody research that.
[00:17:47.320 --> 00:17:52.800]   If you imagine Wolf What's Her Having To Speak, what the president said today, he's
[00:17:52.800 --> 00:17:53.800]   spoken emojis.
[00:17:53.800 --> 00:17:54.800]   Okay.
[00:17:54.800 --> 00:17:58.080]   Is Donald Trump ever used an emoji in a tweet?
[00:17:58.080 --> 00:18:00.440]   It won't know that.
[00:18:00.440 --> 00:18:02.280]   How could it possibly know that?
[00:18:02.280 --> 00:18:03.960]   She's thinking.
[00:18:03.960 --> 00:18:05.440]   I've never seen her think this long.
[00:18:05.440 --> 00:18:08.080]   You think she's actually going after you think she's thinking?
[00:18:08.080 --> 00:18:09.080]   I don't think she's done.
[00:18:09.080 --> 00:18:10.080]   She's done.
[00:18:10.080 --> 00:18:11.080]   That's a stupid query.
[00:18:11.080 --> 00:18:13.760]   I think she did what a lot of people do when you bring up that topic.
[00:18:13.760 --> 00:18:14.760]   She just like...
[00:18:14.760 --> 00:18:15.760]   This guy I don't want to talk about.
[00:18:15.760 --> 00:18:16.920]   That is not going to talk about it.
[00:18:16.920 --> 00:18:18.640]   Let's talk about image recognition.
[00:18:18.640 --> 00:18:22.200]   This was a very interesting slide.
[00:18:22.200 --> 00:18:28.160]   Image recognition error rate for image recognition has now improved so that it is better than
[00:18:28.160 --> 00:18:30.520]   human beings for image recognition.
[00:18:30.520 --> 00:18:33.560]   What does that mean by the way?
[00:18:33.560 --> 00:18:36.560]   I don't know, but it sounds good.
[00:18:36.560 --> 00:18:37.960]   People, I don't know what that is.
[00:18:37.960 --> 00:18:38.960]   I do.
[00:18:38.960 --> 00:18:39.960]   I'm Google.
[00:18:39.960 --> 00:18:40.960]   What does it mean?
[00:18:40.960 --> 00:18:41.960]   This is speech recognition.
[00:18:41.960 --> 00:18:42.960]   Not image recognition.
[00:18:42.960 --> 00:18:43.960]   No, it's image recognition.
[00:18:43.960 --> 00:18:44.960]   No, image recognition.
[00:18:44.960 --> 00:18:45.960]   No image also.
[00:18:45.960 --> 00:18:46.960]   Image is now.
[00:18:46.960 --> 00:18:47.960]   Okay.
[00:18:47.960 --> 00:18:48.960]   I was like...
[00:18:48.960 --> 00:18:50.520]   The chart you're showing says speech and I remember them talking about speech but I did
[00:18:50.520 --> 00:18:51.520]   not remember image.
[00:18:51.520 --> 00:18:52.520]   I'm not showing it.
[00:18:52.520 --> 00:18:53.520]   That's the problem.
[00:18:53.520 --> 00:18:56.800]   Carsten is looking very quickly for the image.
[00:18:56.800 --> 00:19:05.400]   It's a graph showing how algorithms have gone from 30% error rate to less than zero error
[00:19:05.400 --> 00:19:06.400]   rate.
[00:19:06.400 --> 00:19:07.400]   I don't know how you get less.
[00:19:07.400 --> 00:19:08.400]   How do you do that?
[00:19:08.400 --> 00:19:09.840]   No, I'm sorry.
[00:19:09.840 --> 00:19:11.560]   Approaching zero error rate.
[00:19:11.560 --> 00:19:13.560]   There is a standard human error rate.
[00:19:13.560 --> 00:19:15.880]   I guess humans...
[00:19:15.880 --> 00:19:17.680]   What could you look at and you would go...
[00:19:17.680 --> 00:19:18.680]   I don't know what that is.
[00:19:18.680 --> 00:19:21.280]   Think about any optical illusion.
[00:19:21.280 --> 00:19:23.680]   Maybe you look at...
[00:19:23.680 --> 00:19:25.000]   Or think about how bad...
[00:19:25.000 --> 00:19:29.920]   I don't know if this messes with this but think about how unreliable witnesses are.
[00:19:29.920 --> 00:19:32.560]   Or how many times have you...
[00:19:32.560 --> 00:19:34.240]   Maybe it's just face recognition.
[00:19:34.240 --> 00:19:35.840]   It might be better that...
[00:19:35.840 --> 00:19:36.840]   Okay.
[00:19:36.840 --> 00:19:37.840]   We have found...
[00:19:37.840 --> 00:19:39.440]   This Google has come up with the...
[00:19:39.440 --> 00:19:44.560]   I don't know if you can see this at home but these are the emojis that Donald Trump...
[00:19:44.560 --> 00:19:45.560]   Stop that!
[00:19:45.560 --> 00:19:51.120]   Jeff, you're trolling me and you're even here.
[00:19:51.120 --> 00:19:57.320]   So, the reason he mentioned this image recognition improvement is to announce a new project called
[00:19:57.320 --> 00:20:01.760]   Google Lens which I think is very exciting and very interesting.
[00:20:01.760 --> 00:20:05.040]   It's initially going to come with Google Assistant which by the way is now going to be in the
[00:20:05.040 --> 00:20:08.920]   iPhone as well as more places than ever before.
[00:20:08.920 --> 00:20:11.480]   You can hear the cheers in the room.
[00:20:11.480 --> 00:20:14.480]   We're from the front most biggest Google executives.
[00:20:14.480 --> 00:20:16.640]   They are all the iPhone users.
[00:20:16.640 --> 00:20:17.640]   Yes.
[00:20:17.640 --> 00:20:18.640]   So...
[00:20:18.640 --> 00:20:20.640]   I love this.
[00:20:20.640 --> 00:20:21.640]   Yes.
[00:20:21.640 --> 00:20:26.120]   I have been wanting this forever since we had smartphones that were smart enough because
[00:20:26.120 --> 00:20:30.720]   I constantly do things like walk through my yard and I'm like, "Ooh, is this a weed or
[00:20:30.720 --> 00:20:31.800]   is it a plant?
[00:20:31.800 --> 00:20:32.800]   Can I eat this?"
[00:20:32.800 --> 00:20:33.800]   Can I eat this?
[00:20:33.800 --> 00:20:37.320]   That's something that a human would be not like this.
[00:20:37.320 --> 00:20:41.480]   If you show that weed to me, I'd say, "Yeah, eat it.
[00:20:41.480 --> 00:20:42.840]   Let's see what happens."
[00:20:42.840 --> 00:20:44.920]   But Google would know.
[00:20:44.920 --> 00:20:46.960]   Google Lens would know if you could eat it.
[00:20:46.960 --> 00:20:47.960]   Exactly.
[00:20:47.960 --> 00:20:52.960]   I'm excited.
[00:20:52.960 --> 00:20:53.960]   I'm excited.
[00:20:53.960 --> 00:20:54.960]   Don't listen to Leo.
[00:20:54.960 --> 00:20:55.960]   He's trying to kill you.
[00:20:55.960 --> 00:20:56.960]   I like the image he showed.
[00:20:56.960 --> 00:20:57.960]   He showed a flower.
[00:20:57.960 --> 00:20:58.960]   I do this all the time, too, Stacy.
[00:20:58.960 --> 00:20:59.960]   He showed a flower.
[00:20:59.960 --> 00:21:00.960]   I say this all the time.
[00:21:00.960 --> 00:21:01.960]   I'm looking out.
[00:21:01.960 --> 00:21:02.960]   We're taking a walk.
[00:21:02.960 --> 00:21:03.960]   I'm looking out the window.
[00:21:03.960 --> 00:21:04.960]   What is that?
[00:21:04.960 --> 00:21:07.800]   He showed a flower and it actually figured out what the flower was and then assisted offered
[00:21:07.800 --> 00:21:14.160]   you more things you could do with that, including buy it, which I thought was really interesting.
[00:21:14.160 --> 00:21:16.960]   That's going to be very, very valuable.
[00:21:16.960 --> 00:21:19.600]   Is this your name is Google Goggles?
[00:21:19.600 --> 00:21:22.440]   I was just going to say, you remember the old product.
[00:21:22.440 --> 00:21:24.800]   I'm looking at an article from 2011.
[00:21:24.800 --> 00:21:27.680]   Google Goggles gets faster, smarter, solves Sudoku.
[00:21:27.680 --> 00:21:28.680]   It can scan barcodes.
[00:21:28.680 --> 00:21:32.680]   But it can also take a picture of a bridal dress and be able to go to the...
[00:21:32.680 --> 00:21:34.760]   Everything gets advanced a bit at a time.
[00:21:34.760 --> 00:21:35.760]   Exactly.
[00:21:35.760 --> 00:21:39.080]   Because Google Goggles was an app that you could install and use and do a lot of this,
[00:21:39.080 --> 00:21:46.040]   so at least some of this stuff, albeit now with the power of the technology nowadays
[00:21:46.040 --> 00:21:50.800]   and AI working on the back end, it's like this on steroids.
[00:21:50.800 --> 00:21:55.280]   I think they had to program the data from Google Goggles.
[00:21:55.280 --> 00:21:58.160]   Is different here.
[00:21:58.160 --> 00:22:01.480]   This is what I vaguely remember from Google Goggles.
[00:22:01.480 --> 00:22:04.240]   It had to be already entered in some sort of image database.
[00:22:04.240 --> 00:22:07.720]   It had to be a famous monument and things like that.
[00:22:07.720 --> 00:22:11.880]   What's cool here is the image recognition is doing the work.
[00:22:11.880 --> 00:22:14.120]   This isn't a predetermined database.
[00:22:14.120 --> 00:22:15.120]   Does that make sense?
[00:22:15.120 --> 00:22:17.160]   Yes, that's exactly the difference.
[00:22:17.160 --> 00:22:18.480]   I think you nailed it.
[00:22:18.480 --> 00:22:24.600]   He pointed a camera, bar-coded Wi-Fi credentials and it said, "Connected.
[00:22:24.600 --> 00:22:26.240]   It locked you automatically."
[00:22:26.240 --> 00:22:27.240]   That was awesome, right?
[00:22:27.240 --> 00:22:29.080]   That was an awesome thing.
[00:22:29.080 --> 00:22:32.080]   That was the biggest cheers I think of the day.
[00:22:32.080 --> 00:22:36.440]   Well everybody in the audience has crawled under furniture and looked under the bottom
[00:22:36.440 --> 00:22:39.920]   of routers and tried to figure out what the logins are.
[00:22:39.920 --> 00:22:42.560]   That would really be fantastic.
[00:22:42.560 --> 00:22:54.000]   All of this is being improved over time with the massive training that Google did with
[00:22:54.000 --> 00:22:55.000]   for instance AlphaGo.
[00:22:55.000 --> 00:23:03.560]   They mentioned winning the Go match is beating the Go master using tensor processing.
[00:23:03.560 --> 00:23:04.560]   I love it, Stacey.
[00:23:04.560 --> 00:23:06.920]   You got excited about the tensor processing units.
[00:23:06.920 --> 00:23:09.000]   It has only you can.
[00:23:09.000 --> 00:23:11.760]   No, no, this was exciting.
[00:23:11.760 --> 00:23:21.240]   One of the big challenges in building specialized chips for machine learning is that they've
[00:23:21.240 --> 00:23:27.040]   used GPUs historically because they need a lot of fast I/O and they need to do a lot
[00:23:27.040 --> 00:23:29.680]   of things in parallel.
[00:23:29.680 --> 00:23:31.560]   That's why you've got these dedicated chips.
[00:23:31.560 --> 00:23:36.600]   What's been a challenge is you've only been able to run jobs on relatively small clusters
[00:23:36.600 --> 00:23:43.480]   of GPUs because you can't distribute your job over like when I was last reporting deeply
[00:23:43.480 --> 00:23:44.920]   on this it was like eight.
[00:23:44.920 --> 00:23:47.480]   You couldn't go beyond eight different GPUs.
[00:23:47.480 --> 00:23:52.440]   What Google has done has done 64 which is 8x the power.
[00:23:52.440 --> 00:23:53.440]   Is it 8x?
[00:23:53.440 --> 00:23:55.920]   Is it exponential that way?
[00:23:55.920 --> 00:24:01.160]   Anyway, that's actually a really big deal because that just means you can do your job.
[00:24:01.160 --> 00:24:05.440]   It's like Moore's law for machine learning in a way.
[00:24:05.440 --> 00:24:11.840]   They showed little especially engineered units that had there were quad core units, right?
[00:24:11.840 --> 00:24:18.960]   Each one I can't remember was it 180 teraflops and then they showed larger TPU units that
[00:24:18.960 --> 00:24:21.200]   combine many of those.
[00:24:21.200 --> 00:24:22.760]   11.5 petaflops.
[00:24:22.760 --> 00:24:24.320]   Yeah, the rack.
[00:24:24.320 --> 00:24:31.360]   So if you could show the rack, Carsten, that has the slide that has four of these in a rack.
[00:24:31.360 --> 00:24:35.400]   Each of the units in this rack did, what was it 100?
[00:24:35.400 --> 00:24:36.400]   How many petaflops?
[00:24:36.400 --> 00:24:39.440]   It was 11.5 petaflops.
[00:24:39.440 --> 00:24:43.960]   So what you were looking at is four racks each could do 11.5 petaflops.
[00:24:43.960 --> 00:24:46.960]   You're talking 46 or 40.
[00:24:46.960 --> 00:24:51.760]   That's kind of an unimaginable amount of processing power but that's also on the cloud.
[00:24:51.760 --> 00:24:57.400]   So if you can you hear using the Google Compute Engine and using TensorFlow, suddenly you
[00:24:57.400 --> 00:25:03.200]   have devices designed for both parts of machine learning, the training which is massively
[00:25:03.200 --> 00:25:04.200]   compute intensive.
[00:25:04.200 --> 00:25:08.120]   You're showing it a lot of images over and over and over again.
[00:25:08.120 --> 00:25:12.120]   And then the actual learning that there it is, that's the slide.
[00:25:12.120 --> 00:25:13.120]   The inference, the execution.
[00:25:13.120 --> 00:25:17.760]   And the meta part of that was that the learning systems test the learning systems and there's
[00:25:17.760 --> 00:25:20.560]   the meta learning that goes, "Ah, yes.
[00:25:20.560 --> 00:25:23.200]   The neural networks train the neural networks."
[00:25:23.200 --> 00:25:24.200]   Yes.
[00:25:24.200 --> 00:25:29.480]   When he was doing that I was dying.
[00:25:29.480 --> 00:25:37.020]   So if you're a machine learning student or a guru, they had us in the audience of 17
[00:25:37.020 --> 00:25:40.560]   year old who had built a process to do this.
[00:25:40.560 --> 00:25:43.640]   It's just an amazing story.
[00:25:43.640 --> 00:25:48.720]   This is now online for you to use for free.
[00:25:48.720 --> 00:25:55.120]   It's open source, he said that TensorFlow is the number one machine learning program
[00:25:55.120 --> 00:25:56.120]   on GitHub.
[00:25:56.120 --> 00:25:57.120]   Thank you, Chris Dibona.
[00:25:57.120 --> 00:25:58.800]   I'm sure you helped.
[00:25:58.800 --> 00:26:00.960]   The other, that helped put that online.
[00:26:00.960 --> 00:26:03.960]   That's really remarkable to offer that kind of thing.
[00:26:03.960 --> 00:26:09.080]   And of course, Google benefits for it by it but we all benefit by it.
[00:26:09.080 --> 00:26:12.160]   I think I asked one of my amateur questions.
[00:26:12.160 --> 00:26:16.080]   So the processing power you get from this is just for learning systems.
[00:26:16.080 --> 00:26:19.040]   It's a different kind of logic, a different kind of programming.
[00:26:19.040 --> 00:26:24.800]   The speed and everything else is a parallel universe of computing versus great computational
[00:26:24.800 --> 00:26:25.800]   work.
[00:26:25.800 --> 00:26:26.800]   It could do standard.
[00:26:26.800 --> 00:26:32.520]   It could do van noise and stuff but it's optimized for this kind of massively parallel
[00:26:32.520 --> 00:26:33.520]   technology.
[00:26:33.520 --> 00:26:34.520]   It's designed for truth.
[00:26:34.520 --> 00:26:35.520]   180.
[00:26:35.520 --> 00:26:36.520]   You know what?
[00:26:36.520 --> 00:26:37.520]   We should bring Chris Dibona over here.
[00:26:37.520 --> 00:26:40.520]   This would be a good time to get somebody who knows what he's talking about.
[00:26:40.520 --> 00:26:41.520]   I don't know.
[00:26:41.520 --> 00:26:47.400]   You happen to have Marshall McLuhan right here.
[00:26:47.400 --> 00:26:49.360]   Don't go too far.
[00:26:49.360 --> 00:26:53.120]   Jeff, now I get confused by the headphones and the rest.
[00:26:53.120 --> 00:26:54.880]   It's great to, by the way, great to see you.
[00:26:54.880 --> 00:26:58.400]   Don't be confused because the sound is going to be weird but great to see you.
[00:26:58.400 --> 00:27:00.080]   Chris Dibona is an old friend.
[00:27:00.080 --> 00:27:05.880]   I mean, you started Floss Weekly probably 12 years ago, 10 years ago, something like
[00:27:05.880 --> 00:27:07.600]   that.
[00:27:07.600 --> 00:27:11.840]   At the time you were not at Google, you were doing some game.
[00:27:11.840 --> 00:27:16.480]   That was about 13, 14 years ago.
[00:27:16.480 --> 00:27:18.480]   We did Floss Weekly and then...
[00:27:18.480 --> 00:27:20.000]   It can't be that long.
[00:27:20.000 --> 00:27:21.440]   Google is only 12 years old.
[00:27:21.440 --> 00:27:22.440]   I mean, Google.
[00:27:22.440 --> 00:27:24.920]   Tweet is only 12 years old.
[00:27:24.920 --> 00:27:26.720]   But you've been at Google how long now?
[00:27:26.720 --> 00:27:28.200]   12 and a half years.
[00:27:28.200 --> 00:27:29.640]   Well, so it was before you went?
[00:27:29.640 --> 00:27:30.640]   Yeah, you were right.
[00:27:30.640 --> 00:27:32.080]   It's before you went to Google.
[00:27:32.080 --> 00:27:36.360]   We did a few Floss Weekly's live in studios here.
[00:27:36.360 --> 00:27:37.360]   And I thank you for that.
[00:27:37.360 --> 00:27:41.720]   And I thank you for arranging this year and last year as well a place for us to do this
[00:27:41.720 --> 00:27:42.720]   show.
[00:27:42.720 --> 00:27:43.720]   Well, I mean, and I don't want to take too much credit.
[00:27:43.720 --> 00:27:46.520]   I mean, Richard did a lot of the work last year and this year.
[00:27:46.520 --> 00:27:47.520]   Well, thank you, Richard.
[00:27:47.520 --> 00:27:49.880]   And the people who run the studios here.
[00:27:49.880 --> 00:27:53.560]   So tell us, you are in charge of open source at Google.
[00:27:53.560 --> 00:27:54.760]   What's that mean?
[00:27:54.760 --> 00:27:59.000]   So I look after all the incoming and outgoing open source code.
[00:27:59.000 --> 00:28:03.840]   I look after the health of the open source community in general so that we can continue
[00:28:03.840 --> 00:28:07.520]   to draw from it and release to it.
[00:28:07.520 --> 00:28:11.640]   And I also look after a small science education group called Making in Science.
[00:28:11.640 --> 00:28:14.000]   It will be at Maker Faire on Friday.
[00:28:14.000 --> 00:28:15.000]   Oh, good.
[00:28:15.000 --> 00:28:16.000]   So we'll look for you.
[00:28:16.000 --> 00:28:17.000]   Right.
[00:28:17.000 --> 00:28:18.000]   Yes, we have it with there.
[00:28:18.000 --> 00:28:22.240]   And then I also do some funny sort of government relations and stuff.
[00:28:22.240 --> 00:28:23.920]   I do some unusual things.
[00:28:23.920 --> 00:28:29.560]   The process of taking TensorFlow, which was an internal project and making it open source
[00:28:29.560 --> 00:28:32.640]   and putting it on GitHub, tell me what that involves.
[00:28:32.640 --> 00:28:41.000]   Well, so Will, who's my compatriot, runs my compliance and outreach team.
[00:28:41.000 --> 00:28:47.960]   And we try to have a really greased machine to help people release software from Google.
[00:28:47.960 --> 00:28:49.760]   But TensorFlow is really special.
[00:28:49.760 --> 00:28:56.560]   So there's a couple of engineers here, one by the name of Jeff Dean and another Rajat,
[00:28:56.560 --> 00:29:02.840]   who really, really wanted the world of machine learning and convolutional neural networks
[00:29:02.840 --> 00:29:05.640]   to develop the way we've been doing.
[00:29:05.640 --> 00:29:11.640]   And so there was sort of a danger, but there was a likelihood where they wouldn't develop
[00:29:11.640 --> 00:29:15.560]   neural networks the way that we are doing it here at Google.
[00:29:15.560 --> 00:29:20.280]   And so we're like, those other methods are OK, but we think this is a better way.
[00:29:20.280 --> 00:29:24.440]   And so Jeff very wisely said, we should open source technology.
[00:29:24.440 --> 00:29:28.280]   We should get people developing the way that we're developing so that we can all move
[00:29:28.280 --> 00:29:30.480]   forward in this technical direction.
[00:29:30.480 --> 00:29:32.000]   Set a standard.
[00:29:32.000 --> 00:29:38.680]   Well, the set of standard and also to have a nice frictionless place where we can all
[00:29:38.680 --> 00:29:41.400]   work together on this technology.
[00:29:41.400 --> 00:29:44.000]   Because TensorFlow has been incredibly popular.
[00:29:44.000 --> 00:29:45.880]   We've taken probably thousands.
[00:29:45.880 --> 00:29:49.960]   Will can confirm how many patches we've taken from outside Google into TensorFlow.
[00:29:49.960 --> 00:29:53.120]   At least hundreds.
[00:29:53.120 --> 00:29:55.160]   I think thousands though.
[00:29:55.160 --> 00:29:59.200]   And it's become a really great way to move the technology forward.
[00:29:59.200 --> 00:30:02.800]   And remember, it's under an Apache license, correct?
[00:30:02.800 --> 00:30:05.160]   Why don't you double check that?
[00:30:05.160 --> 00:30:06.160]   Yeah.
[00:30:06.160 --> 00:30:07.440]   So that's interesting.
[00:30:07.440 --> 00:30:11.800]   So you'll use GPL, you use different licenses depending on--
[00:30:11.800 --> 00:30:12.800]   Yeah.
[00:30:12.800 --> 00:30:18.960]   So for instance, when we release a JavaScript tool, we'll use BSD or MIT because that's
[00:30:18.960 --> 00:30:21.400]   what JavaScript people like.
[00:30:21.400 --> 00:30:28.040]   And if we're releasing something for Linux, it's going to be GPL for the kernel.
[00:30:28.040 --> 00:30:32.240]   So we try to help the software find its developer community.
[00:30:32.240 --> 00:30:35.760]   And so we're not too dogmatic about licenses.
[00:30:35.760 --> 00:30:41.760]   But yeah, what's nice is now you've taken TensorFlow, which is an open source project.
[00:30:41.760 --> 00:30:46.520]   And now you're giving people the hardware, some very sophisticated hardware, these new
[00:30:46.520 --> 00:30:52.840]   TPUs to work with, I presume, works in conjunction with their TensorFlow projects.
[00:30:52.840 --> 00:30:53.840]   Yeah.
[00:30:53.840 --> 00:30:58.440]   So if you're using TensorFlow and you push that load to a cloud TPU like that, you're
[00:30:58.440 --> 00:31:06.160]   getting an incredibly efficient way of doing your inference and training.
[00:31:06.160 --> 00:31:13.040]   So this is the same chip that we used for the AlphaGo win with Least Sodele.
[00:31:13.040 --> 00:31:20.720]   And a rack of those TPUs replaced-- I'm not kidding-- 100,000 machines.
[00:31:20.720 --> 00:31:23.760]   So it was an incredible savings of power.
[00:31:23.760 --> 00:31:28.080]   It was an incredible increase in efficiency and speed of the inference and training models.
[00:31:28.080 --> 00:31:29.080]   And it's fantastic.
[00:31:29.080 --> 00:31:36.040]   So can I just ask you-- so when you're talking about the way people want to-- you wanted
[00:31:36.040 --> 00:31:40.000]   people to do machine learning, are you talking about like as opposed to using something like
[00:31:40.000 --> 00:31:43.040]   CNTK or CAFE or--
[00:31:43.040 --> 00:31:44.440]   So partially yes.
[00:31:44.440 --> 00:31:50.800]   And also if you look at CAFE and Tor, Gentiana is licensing, we're using the Apache license,
[00:31:50.800 --> 00:31:56.040]   which provides-- so any patents that we have that reads on TensorFlow that we've released,
[00:31:56.040 --> 00:31:57.520]   we're basically giving you a license to that.
[00:31:57.520 --> 00:32:00.480]   We're saying we're not going to rent, seek, we're not going to go math for you.
[00:32:00.480 --> 00:32:05.440]   And the other machine learning tools do not have that assurance.
[00:32:05.440 --> 00:32:08.960]   So that was another thing that we were really sort of worried about, that machine learning
[00:32:08.960 --> 00:32:14.000]   would end up getting locked up in sort of this world of, you know, pay me.
[00:32:14.000 --> 00:32:15.800]   It's open source, but you still have to pay me.
[00:32:15.800 --> 00:32:17.880]   And we didn't want that to happen.
[00:32:17.880 --> 00:32:18.880]   So--
[00:32:18.880 --> 00:32:24.480]   So are we going to see a vertical integration from the processor to your framework language
[00:32:24.480 --> 00:32:26.840]   and then onto like cloud?
[00:32:26.840 --> 00:32:31.200]   Like what is the optimal way you think this should could or would develop?
[00:32:31.200 --> 00:32:32.200]   So that makes sense.
[00:32:32.200 --> 00:32:37.320]   The way it's going to happen is that-- so right now you can download TensorFlow and
[00:32:37.320 --> 00:32:41.360]   point it at a zillion GPUs or whatever you might have at home.
[00:32:41.360 --> 00:32:44.520]   And you do pay for the Google Compute Cloud.
[00:32:44.520 --> 00:32:45.520]   I mean, it's not free.
[00:32:45.520 --> 00:32:49.760]   Yeah, I think we have like a free tier, you know, that we do so that you can try things
[00:32:49.760 --> 00:32:50.760]   out.
[00:32:50.760 --> 00:32:51.760]   Like for students.
[00:32:51.760 --> 00:32:52.760]   Well, yeah.
[00:32:52.760 --> 00:32:55.040]   Or you get your feet wet, kind of.
[00:32:55.040 --> 00:32:57.760]   Yeah, I think it's limited to your workload type of.
[00:32:57.760 --> 00:32:58.760]   Yeah.
[00:32:58.760 --> 00:33:02.600]   But yeah, once you get serious about machine learning, though, the thing that happens
[00:33:02.600 --> 00:33:05.720]   is you end up using a ton of compute resources.
[00:33:05.720 --> 00:33:06.720]   Right.
[00:33:06.720 --> 00:33:13.000]   So we're saying, listen, you can go out and go buy a zillion GPUs and go to town.
[00:33:13.000 --> 00:33:20.080]   But the power costs, the actual cost of the GPU and the rest are actually, you know, they're
[00:33:20.080 --> 00:33:21.080]   really considerable.
[00:33:21.080 --> 00:33:25.760]   So instead, you can just literally-- the idea is that eventually you'll be able to just
[00:33:25.760 --> 00:33:32.320]   point your model and your training set at a cloud TPU and it'll just get done cheaper
[00:33:32.320 --> 00:33:34.880]   and more efficient and faster than the rest.
[00:33:34.880 --> 00:33:42.560]   Shazam says they're doing this and using Google Compute Cloud to do their recognition.
[00:33:42.560 --> 00:33:43.560]   Yeah.
[00:33:43.560 --> 00:33:44.760]   That's pretty impressive.
[00:33:44.760 --> 00:33:48.760]   So is this a business for Google?
[00:33:48.760 --> 00:33:49.760]   What's in it for Google?
[00:33:49.760 --> 00:33:51.960]   Obviously, there is a business there.
[00:33:51.960 --> 00:33:55.960]   So I look at it on two axes, right?
[00:33:55.960 --> 00:34:04.000]   So Jeff and Rijat's goal with Open Source IntensorFlow is to keep people developing neural
[00:34:04.000 --> 00:34:06.800]   networks the way that we've been doing it.
[00:34:06.800 --> 00:34:12.040]   And also, the way we justify all of this, and it's not cheap to develop a team that's
[00:34:12.040 --> 00:34:17.640]   eyes and deploy these kinds of chips and all the rest is that people will pay to use the
[00:34:17.640 --> 00:34:20.720]   TPUs when they're doing inference and training.
[00:34:20.720 --> 00:34:21.720]   So yeah.
[00:34:21.720 --> 00:34:26.960]   So I mean, there's a very clear business case inside our cloud business for this.
[00:34:26.960 --> 00:34:32.120]   So I think the pricing is actually very aggressive and good.
[00:34:32.120 --> 00:34:33.120]   Yeah.
[00:34:33.120 --> 00:34:37.520]   So, Chris, can I thank you once again for bringing us here?
[00:34:37.520 --> 00:34:38.520]   Nice.
[00:34:38.520 --> 00:34:39.520]   You're thanking me.
[00:34:39.520 --> 00:34:40.520]   Great guy.
[00:34:40.520 --> 00:34:41.520]   I'm glad to have you here.
[00:34:41.520 --> 00:34:42.520]   So thank you.
[00:34:42.520 --> 00:34:43.520]   Thank you.
[00:34:43.520 --> 00:34:44.520]   We miss you right here.
[00:34:44.520 --> 00:34:47.640]   And I want to tell your listeners, your viewers, what do you call them?
[00:34:47.640 --> 00:34:48.640]   Both.
[00:34:48.640 --> 00:34:49.640]   People watching and listening.
[00:34:49.640 --> 00:34:50.640]   Okay.
[00:34:50.640 --> 00:34:52.840]   The people on the chat line.
[00:34:52.840 --> 00:34:59.440]   It's such a treat for us to have Leo and Jeff and Jason here.
[00:34:59.440 --> 00:35:03.880]   So you know, and like, you know, Googlers, oh, can I come?
[00:35:03.880 --> 00:35:06.240]   Like we have a couple people hanging out.
[00:35:06.240 --> 00:35:07.720]   It's really great for us.
[00:35:07.720 --> 00:35:08.720]   So so thanks for coming.
[00:35:08.720 --> 00:35:10.320]   You know, we're really glad to be here.
[00:35:10.320 --> 00:35:11.320]   We'll come anytime.
[00:35:11.320 --> 00:35:12.320]   This is a great studio.
[00:35:12.320 --> 00:35:14.160]   This is actually, you must do other things in here.
[00:35:14.160 --> 00:35:15.160]   This is not a good one.
[00:35:15.160 --> 00:35:16.160]   Oh, yeah.
[00:35:16.160 --> 00:35:19.360]   This is where they film all the developer videos, like a ton of things that were shown
[00:35:19.360 --> 00:35:20.360]   on the screens.
[00:35:20.360 --> 00:35:21.360]   Yeah.
[00:35:21.360 --> 00:35:23.000]   Well, I don't know if Matt has here.
[00:35:23.000 --> 00:35:24.000]   Yeah.
[00:35:24.000 --> 00:35:25.000]   Yeah.
[00:35:25.000 --> 00:35:26.880]   I get a Johnny I've white room look.
[00:35:26.880 --> 00:35:27.880]   Can that be done?
[00:35:27.880 --> 00:35:29.080]   Can you do that here?
[00:35:29.080 --> 00:35:30.080]   I'd love it.
[00:35:30.080 --> 00:35:31.880]   And there's a green screen right behind me.
[00:35:31.880 --> 00:35:32.880]   All right.
[00:35:32.880 --> 00:35:34.920]   Actually, I'm kind of partially Charlie Rose.
[00:35:34.920 --> 00:35:36.320]   Look, I'm getting right now.
[00:35:36.320 --> 00:35:37.320]   He's really like that.
[00:35:37.320 --> 00:35:39.320]   Leo's really like that.
[00:35:39.320 --> 00:35:41.000]   We're going to take a little break.
[00:35:41.000 --> 00:35:43.480]   We'll come back with more.
[00:35:43.480 --> 00:35:46.440]   Smoke them if you got them or whatever it is people do these days.
[00:35:46.440 --> 00:35:49.880]   Vape it if you got it.
[00:35:49.880 --> 00:35:50.880]   But we'll be back with more.
[00:35:50.880 --> 00:35:55.480]   And I really want to thank, as I said, the folks at Google for making this possible.
[00:35:55.480 --> 00:35:59.040]   Of course, Jason Howell, who came all the way down and you're going to spend the whole
[00:35:59.040 --> 00:36:00.920]   three days here at Google.
[00:36:00.920 --> 00:36:02.360]   I'm here till the end.
[00:36:02.360 --> 00:36:04.280]   I'm here till Friday.
[00:36:04.280 --> 00:36:09.560]   And watch all about Andrew on Tuesday because on Friday you are doing a very special series
[00:36:09.560 --> 00:36:14.400]   of interviews and we'll be airing that on all about Android and I don't want to give away
[00:36:14.400 --> 00:36:15.400]   anything.
[00:36:15.400 --> 00:36:16.400]   Yeah.
[00:36:16.400 --> 00:36:19.400]   We'll go ahead and keep that a little bit of a secret.
[00:36:19.400 --> 00:36:20.400]   But absolutely.
[00:36:20.400 --> 00:36:22.320]   I'm really looking forward to Friday.
[00:36:22.320 --> 00:36:23.320]   That's all I'm going to say.
[00:36:23.320 --> 00:36:25.640]   Oh, we're going to be great.
[00:36:25.640 --> 00:36:29.960]   So watch all about Android on Tuesday about 5 p.m. but you and Flow will be down Florence
[00:36:29.960 --> 00:36:30.960]   Island.
[00:36:30.960 --> 00:36:31.960]   Also be here for that.
[00:36:31.960 --> 00:36:35.880]   Jeff Jarvis is nice to be in the same room with all of them that happens.
[00:36:35.880 --> 00:36:37.400]   Yeah, I appreciate that.
[00:36:37.400 --> 00:36:40.760]   You came out for this conference but you're going back home.
[00:36:40.760 --> 00:36:42.040]   We will have you on Twitter on Sunday.
[00:36:42.040 --> 00:36:43.040]   Wait, yes.
[00:36:43.040 --> 00:36:46.080]   But back from the usual, my usual den where I will be all pink is supposed to be where
[00:36:46.080 --> 00:36:47.600]   I look like human.
[00:36:47.600 --> 00:36:50.040]   And Stacy, thank you for doing double duty today.
[00:36:50.040 --> 00:36:54.080]   You did that keynote, that long two hour keynote and you're back here for more.
[00:36:54.080 --> 00:36:56.760]   But I'm glad you're getting a massage afterwards.
[00:36:56.760 --> 00:36:57.760]   Indeed.
[00:36:57.760 --> 00:37:04.480]   Our show today brought you by Upside the smart new way to save money on business travel.
[00:37:04.480 --> 00:37:06.880]   You of course have heard of Priceline, right?
[00:37:06.880 --> 00:37:12.600]   The guy who founded Priceline, Jay Walker is a legend in the travel industry after he
[00:37:12.600 --> 00:37:18.040]   sold Priceline and got out of the travel business for a while.
[00:37:18.040 --> 00:37:21.880]   He decided he wanted to get back in but he wanted to do something kind of different.
[00:37:21.880 --> 00:37:24.080]   He wanted to address business travel.
[00:37:24.080 --> 00:37:26.680]   So there's a real gap in business travel.
[00:37:26.680 --> 00:37:32.040]   If you ever worked for a big company, I did for Ziff Davis for years, you know that there
[00:37:32.040 --> 00:37:33.560]   was a travel department.
[00:37:33.560 --> 00:37:37.720]   You'd call the travel department and they would arrange your travel and that's that.
[00:37:37.720 --> 00:37:41.600]   You don't book your own travel and you don't get to choose.
[00:37:41.600 --> 00:37:43.120]   They just do it for you.
[00:37:43.120 --> 00:37:47.160]   But if you're a small business or a medium business, I know when we started to it, it
[00:37:47.160 --> 00:37:52.240]   was like, well, I guess I'm doing that, that you don't get the deals because they don't
[00:37:52.240 --> 00:37:54.640]   have, you know, you don't have any negotiating power.
[00:37:54.640 --> 00:37:58.760]   You're really, it's a lot of work and it's not much fun.
[00:37:58.760 --> 00:38:03.520]   Well, this is Upside is the way to do business travel from now on.
[00:38:03.520 --> 00:38:04.920]   It just takes a couple of minutes.
[00:38:04.920 --> 00:38:08.760]   You can do it right now at Upside.com.
[00:38:08.760 --> 00:38:12.400]   It actually use the promo code Twig and I'll tell you why in a second.
[00:38:12.400 --> 00:38:19.080]   The way it works is Upside has already negotiated great deals with not just flights, not just
[00:38:19.080 --> 00:38:21.520]   airlines, but also with hotels.
[00:38:21.520 --> 00:38:22.520]   Now this isn't Priceline.
[00:38:22.520 --> 00:38:23.840]   You're going to know the name of the hotel.
[00:38:23.840 --> 00:38:25.800]   You're going to know the name of the airline.
[00:38:25.800 --> 00:38:27.440]   You're going to know the time you're traveling.
[00:38:27.440 --> 00:38:29.280]   There's no mystery here at all.
[00:38:29.280 --> 00:38:31.560]   They will simplify the choices.
[00:38:31.560 --> 00:38:34.600]   They'll give you the top six flights and hotels.
[00:38:34.600 --> 00:38:36.160]   You choose what you want.
[00:38:36.160 --> 00:38:40.600]   You get all your frequent flyer miles just as always, but they will give you a may at
[00:38:40.600 --> 00:38:43.320]   your business anyway, amazing savings.
[00:38:43.320 --> 00:38:48.200]   Then there's a little reward for you because every time you book a trip at Upside, you're
[00:38:48.200 --> 00:38:54.800]   going to get a gift card that's worth one, two, even $300.
[00:38:54.800 --> 00:38:56.600]   This isn't some junkie gift card.
[00:38:56.600 --> 00:38:59.520]   These are Amazon Target, 50 top brands, Nordstrom.
[00:38:59.520 --> 00:39:01.000]   You get to choose.
[00:39:01.000 --> 00:39:05.920]   They even have a page dedicated to your current status with gift cards so you can keep track
[00:39:05.920 --> 00:39:07.000]   of it.
[00:39:07.000 --> 00:39:12.680]   After you book your travel, your gift card arrives via email just 72 hours later.
[00:39:12.680 --> 00:39:16.920]   You'll get even more overseas trips and there's no limit on trips or gift cards.
[00:39:16.920 --> 00:39:19.720]   Lisa and I are booking our own travel for our own business.
[00:39:19.720 --> 00:39:21.200]   This was cool.
[00:39:21.200 --> 00:39:23.080]   Lisa Wright, I'm not making this up.
[00:39:23.080 --> 00:39:26.840]   We were able to apply the gift card immediately instead of getting an Amazon deal.
[00:39:26.840 --> 00:39:31.720]   We just applied and saved $300 on our flight and our hotels.
[00:39:31.720 --> 00:39:33.440]   $1,300?
[00:39:33.440 --> 00:39:34.440]   Wow.
[00:39:34.440 --> 00:39:38.880]   Where are we going?
[00:39:38.880 --> 00:39:40.120]   It was our trip to New York.
[00:39:40.120 --> 00:39:42.040]   Wow, that is fantastic.
[00:39:42.040 --> 00:39:45.440]   I want you to try it Upside.com.
[00:39:45.440 --> 00:39:49.600]   If you're booking for your business, you get to save the money right up front.
[00:39:49.600 --> 00:39:51.440]   Just apply the gift card to the travel.
[00:39:51.440 --> 00:39:55.760]   If you're booking for the company you work for, they get a great discount and you get
[00:39:55.760 --> 00:39:57.120]   some nice gift cards.
[00:39:57.120 --> 00:40:00.800]   I think that's a great win-win for everybody all around.
[00:40:00.800 --> 00:40:07.920]   Today's really figured out a sweet spot for Upside.com, the best way to save big money
[00:40:07.920 --> 00:40:09.840]   on business travel.
[00:40:09.840 --> 00:40:11.880]   Don't forget to use the OfferCode Twig.
[00:40:11.880 --> 00:40:12.880]   Now, here's why.
[00:40:12.880 --> 00:40:16.520]   You're guaranteed at least $100 gift card when you book your first trip.
[00:40:16.520 --> 00:40:17.840]   You might even get more.
[00:40:17.840 --> 00:40:21.760]   You've got to use the OfferCode Twig though to guarantee at least $100.
[00:40:21.760 --> 00:40:22.760]   Minimum purchase required.
[00:40:22.760 --> 00:40:25.080]   You've got to see the site for complete details.
[00:40:25.080 --> 00:40:26.080]   Upside.
[00:40:26.080 --> 00:40:28.280]   UPSide.com.
[00:40:28.280 --> 00:40:33.160]   And use the OfferCode Twig that does two things, guarantees you $100 minimum gift card and
[00:40:33.160 --> 00:40:35.760]   lets them know that you heard it right here.
[00:40:35.760 --> 00:40:38.920]   That helps us a little bit with the show.
[00:40:38.920 --> 00:40:41.200]   On with the show, we go.
[00:40:41.200 --> 00:40:46.120]   I want to thank also John Slanina, who is our studio manager, Colleen Goldstein, who
[00:40:46.120 --> 00:40:48.360]   was our off-site production booker.
[00:40:48.360 --> 00:40:51.000]   One of the best producers I've ever worked with.
[00:40:51.000 --> 00:40:54.360]   One of the best studio engineers I've ever worked with, who worked very hard to set this
[00:40:54.360 --> 00:40:55.360]   all up.
[00:40:55.360 --> 00:41:01.400]   And of course, Carson Bondi and Burke McQuinn back at the studio.
[00:41:01.400 --> 00:41:03.200]   It's a hard thing to do.
[00:41:03.200 --> 00:41:11.040]   Yeah, because we didn't bring studio cameras and big labored satellite trucks and all of
[00:41:11.040 --> 00:41:12.040]   that stuff.
[00:41:12.040 --> 00:41:13.040]   We don't have that stuff.
[00:41:13.040 --> 00:41:14.040]   We brought the Skype.
[00:41:14.040 --> 00:41:16.120]   Next time satellite trucks come with us.
[00:41:16.120 --> 00:41:23.520]   I've gone that route and it is no is A, no better and B, a lot more expensive.
[00:41:23.520 --> 00:41:28.960]   You know that tech TV spent a million dollars every time they hit the road to do one of
[00:41:28.960 --> 00:41:30.960]   those CES's or contacts.
[00:41:30.960 --> 00:41:31.960]   A million dollars.
[00:41:31.960 --> 00:41:33.960]   The money you witnessed there was amazing.
[00:41:33.960 --> 00:41:36.160]   Well, I was seeing there basically.
[00:41:36.160 --> 00:41:37.160]   Yeah.
[00:41:37.160 --> 00:41:39.960]   The guy who had the money could afford it.
[00:41:39.960 --> 00:41:41.720]   But we really probably should have paid more attention to it.
[00:41:41.720 --> 00:41:46.800]   It was Paul Allen, of course, because eventually he got tired of losing money and said, yeah,
[00:41:46.800 --> 00:41:47.800]   I'm done.
[00:41:47.800 --> 00:41:48.800]   See you guys.
[00:41:48.800 --> 00:41:49.800]   Bye-bye.
[00:41:49.800 --> 00:41:50.800]   Let's see.
[00:41:50.800 --> 00:41:52.080]   We are going to continue on.
[00:41:52.080 --> 00:41:53.080]   We stopped.
[00:41:53.080 --> 00:41:55.960]   We got very excited because Stacy got so excited.
[00:41:55.960 --> 00:41:57.800]   Like that.
[00:41:57.800 --> 00:42:00.960]   About the Google Cloud TPUs.
[00:42:00.960 --> 00:42:05.080]   Have you now learned everything you wanted to know, Stacy, about those?
[00:42:05.080 --> 00:42:06.720]   I will continue learning.
[00:42:06.720 --> 00:42:13.080]   But for me, the big question, when we think about AI being the future, right, everything
[00:42:13.080 --> 00:42:17.000]   is going to be built on some form of machine learning.
[00:42:17.000 --> 00:42:20.800]   Do you have to own the hardware all the way through?
[00:42:20.800 --> 00:42:24.800]   Are people going to move to a cloud to get this service?
[00:42:24.800 --> 00:42:25.800]   Probably.
[00:42:25.800 --> 00:42:29.800]   And then how are the big companies going to differentiate themselves?
[00:42:29.800 --> 00:42:37.160]   So Google has their TPU based stuff for the clouds that also works with GPUs from NVIDIA.
[00:42:37.160 --> 00:42:42.720]   But Amazon also has a machine learning platform as part of Amazon Web Services.
[00:42:42.720 --> 00:42:46.960]   So I'm just kind of thinking about how that's going to evolve, I guess.
[00:42:46.960 --> 00:42:49.920]   My brain's always looking ahead for it.
[00:42:49.920 --> 00:42:58.200]   Finally for Google, this is a way to assert themselves in this very competitive cloud computing
[00:42:58.200 --> 00:42:59.200]   field.
[00:42:59.200 --> 00:43:00.760]   Amazon's the king.
[00:43:00.760 --> 00:43:03.480]   Microsoft love to be huge here.
[00:43:03.480 --> 00:43:05.120]   Google I think has some great offerings.
[00:43:05.120 --> 00:43:08.600]   And this is a unique offering as far as I can tell.
[00:43:08.600 --> 00:43:13.400]   It didn't Amazon announce some stuff fairly recently, Stacy as well.
[00:43:13.400 --> 00:43:16.840]   Feels like they're doing some machine learning stuff.
[00:43:16.840 --> 00:43:20.960]   They don't have their own framework.
[00:43:20.960 --> 00:43:25.040]   They don't have the equivalent of a TensorFlow or a TPU.
[00:43:25.040 --> 00:43:28.760]   They are reselling, let's see, machine learning.
[00:43:28.760 --> 00:43:31.440]   I think they're reselling NVIDIA based GPUs.
[00:43:31.440 --> 00:43:34.840]   This is where Google is like the king.
[00:43:34.840 --> 00:43:36.320]   They're well ahead of everybody else.
[00:43:36.320 --> 00:43:38.960]   I have in my opinion, in machine learning.
[00:43:38.960 --> 00:43:43.880]   If the war is over machine learning, Google is ahead not only on that, but also he or
[00:43:43.880 --> 00:43:48.200]   she who has the most data sets has the advantage when it comes to machine learning, right?
[00:43:48.200 --> 00:43:53.200]   But the more data you have to train the systems and who has more data about the world's knowledge
[00:43:53.200 --> 00:43:54.280]   than Google.
[00:43:54.280 --> 00:43:59.320]   So has some that Google doesn't have about our behavior?
[00:43:59.320 --> 00:44:01.200]   Amazon has strictly our commerce.
[00:44:01.200 --> 00:44:04.320]   Apple doesn't have much of any of that.
[00:44:04.320 --> 00:44:08.240]   So even though Google on some matters, we've seen catches up on the front end hardware,
[00:44:08.240 --> 00:44:10.680]   on the back end, their way ahead.
[00:44:10.680 --> 00:44:11.680]   Stacy, sorry.
[00:44:11.680 --> 00:44:14.400]   No, well, there's a question here.
[00:44:14.400 --> 00:44:20.360]   This is really subtle and kind of geeky, so we may not want to get too much into it.
[00:44:20.360 --> 00:44:24.120]   When you train, you can train generically for images, for example, like here, do you
[00:44:24.120 --> 00:44:25.200]   recognize this?
[00:44:25.200 --> 00:44:30.440]   But when you start using machine learning to do things like predict if I show you these,
[00:44:30.440 --> 00:44:33.320]   if you've bought this, are you going to buy that?
[00:44:33.320 --> 00:44:38.400]   Then you actually need to optimize your machine learning right now very specifically.
[00:44:38.400 --> 00:44:43.000]   So it's not as general as language processing or computer vision, which are kind of the
[00:44:43.000 --> 00:44:44.760]   two big leaps.
[00:44:44.760 --> 00:44:51.520]   So we're going to have these massive, and we are experiencing these massive leaps ahead
[00:44:51.520 --> 00:44:55.080]   in computers that can see and computers that can understand us.
[00:44:55.080 --> 00:44:59.680]   But when we start trying to make that a little more granular for circumstances, which inevitably
[00:44:59.680 --> 00:45:07.040]   we will want to do, there's a real open question if we can do kind of general machine learning,
[00:45:07.040 --> 00:45:08.040]   right?
[00:45:08.040 --> 00:45:09.040]   Can you see the difference?
[00:45:09.040 --> 00:45:10.040]   Yeah, absolutely.
[00:45:10.040 --> 00:45:11.040]   And a lot of
[00:45:11.040 --> 00:45:12.040]   So in that's
[00:45:12.040 --> 00:45:13.040]   God.
[00:45:13.040 --> 00:45:20.800]   A lot of what we saw on stage today, and of course this is not deep learning for us.
[00:45:20.800 --> 00:45:22.280]   This is shallow learning.
[00:45:22.280 --> 00:45:29.520]   What we saw on stage today was in very specific domains, things like reading a biopsy slide
[00:45:29.520 --> 00:45:35.280]   and not improving necessarily on a pathologist, but being a backstop and additional point
[00:45:35.280 --> 00:45:37.680]   of information for pathologists.
[00:45:37.680 --> 00:45:43.440]   They showed this 17-year-old, this high school student who developed better tests, better
[00:45:43.440 --> 00:45:46.480]   ways to read mammograms, which I thought was
[00:45:46.480 --> 00:45:56.240]   For a high school kid, he taught himself how to do this stuff by watching YouTube videos.
[00:45:56.240 --> 00:46:01.280]   I mean, it's kind of a side testament to YouTube.
[00:46:01.280 --> 00:46:04.120]   So I love the idea that making this available.
[00:46:04.120 --> 00:46:09.120]   You can find out more at Google, set up an informational website at google.ai.
[00:46:09.120 --> 00:46:15.800]   They also have announced that they're going to make 1,000 of these TPUs, these 180 teraflop
[00:46:15.800 --> 00:46:23.040]   TPUs available in the cloud at no cost to machine language researchers via something
[00:46:23.040 --> 00:46:25.600]   they call the TensorFlow Research Cloud.
[00:46:25.600 --> 00:46:26.840]   You can find out more at TensorFlow.
[00:46:26.840 --> 00:46:27.840]   It's kind of cool.
[00:46:27.840 --> 00:46:31.320]   There's the philanthropy of processing power.
[00:46:31.320 --> 00:46:38.320]   Yeah, and I think Google has to do that because there is also the optics of building Skynet.
[00:46:38.320 --> 00:46:39.320]   Right.
[00:46:39.320 --> 00:46:45.800]   They have to balance the idea of all of this AI slurp, you know, their AI computers in
[00:46:45.800 --> 00:46:49.120]   the background slurping up all of your personal data and using it.
[00:46:49.120 --> 00:46:53.560]   Yes, for things that are convenient for you, but for, you know, in ways that allow them
[00:46:53.560 --> 00:46:59.040]   more access to you, on the other hand, it can be applied to real-world, real problems
[00:46:59.040 --> 00:47:04.760]   that need solving, that need smart brains to tackle it.
[00:47:04.760 --> 00:47:06.960]   And then we'll contribute to the issues.
[00:47:06.960 --> 00:47:09.040]   There were three cases, and we'll get to a couple of them later.
[00:47:09.040 --> 00:47:13.480]   Three cases where I saw Google using corporate responsibility as part of the announcements.
[00:47:13.480 --> 00:47:14.480]   Right.
[00:47:14.480 --> 00:47:15.480]   Yeah.
[00:47:15.480 --> 00:47:16.480]   One was this.
[00:47:16.480 --> 00:47:20.000]   One was what you mentioned earlier, Leo, in terms of the inexpensive phones to make
[00:47:20.000 --> 00:47:22.440]   the phones to get more connectivity worldwide.
[00:47:22.440 --> 00:47:26.880]   The third we'll talk about later, I assume, is facial recognition and how they've pitched
[00:47:26.880 --> 00:47:27.880]   that.
[00:47:27.880 --> 00:47:30.400]   I need to try to think about these things responsibly as they go.
[00:47:30.400 --> 00:47:34.120]   Yeah, facial recognition, as we know, is very high at a creepy scale.
[00:47:34.120 --> 00:47:35.120]   Right.
[00:47:35.120 --> 00:47:36.440]   Potentially, I think it's great.
[00:47:36.440 --> 00:47:39.080]   In fact, I wish Google had released it for...
[00:47:39.080 --> 00:47:43.320]   But I understand why they didn't allow me to just arbitrarily benefit from anything.
[00:47:43.320 --> 00:47:44.320]   They wouldn't for anybody.
[00:47:44.320 --> 00:47:45.320]   They wouldn't for anybody.
[00:47:45.320 --> 00:47:49.760]   It would not be the first ones to do that.
[00:47:49.760 --> 00:47:53.000]   All these companies have a big PR issue.
[00:47:53.000 --> 00:47:58.640]   I think they're genuine in their desire to push the world forward.
[00:47:58.640 --> 00:48:03.280]   And better than anyone, they understand the potential of the technologies that they are
[00:48:03.280 --> 00:48:08.840]   offering and that they are improving on and that they are helping others work on.
[00:48:08.840 --> 00:48:16.440]   But they realize that the general public is becoming increasingly antsy about privacy
[00:48:16.440 --> 00:48:19.720]   and security.
[00:48:19.720 --> 00:48:22.840]   In order to get people to use things like the Google Home and the Google Assistant, which
[00:48:22.840 --> 00:48:27.320]   we're about to talk about in just a second, they've really got to do, I think, some work
[00:48:27.320 --> 00:48:31.800]   to make sure people are comfortable with this, are comfortable with the rules.
[00:48:31.800 --> 00:48:32.800]   And I don't know...
[00:48:32.800 --> 00:48:35.800]   Do you feel they're doing a good job of that?
[00:48:35.800 --> 00:48:36.800]   I think they're trying...
[00:48:36.800 --> 00:48:37.800]   Oh, sorry.
[00:48:37.800 --> 00:48:38.800]   Go ahead, Stacey.
[00:48:38.800 --> 00:48:41.520]   Oh, I was just going to say, I think about this a lot.
[00:48:41.520 --> 00:48:43.200]   And I think this is where actually...
[00:48:43.200 --> 00:48:44.400]   Don't hate me, y'all.
[00:48:44.400 --> 00:48:48.800]   But we actually need a civil debate over this.
[00:48:48.800 --> 00:48:51.040]   We need government to actually...
[00:48:51.040 --> 00:48:55.840]   We need an agency or lawmakers to actually start this conversation.
[00:48:55.840 --> 00:49:02.040]   And I mean a real deep conversation, not regulate the internet type of conversation,
[00:49:02.040 --> 00:49:05.160]   but really understanding what's at stake in talking about.
[00:49:05.160 --> 00:49:07.680]   Or our present lawmakers capable of doing that, Stacey?
[00:49:07.680 --> 00:49:08.680]   Oh, no.
[00:49:08.680 --> 00:49:09.680]   And the sad thing is...
[00:49:09.680 --> 00:49:10.680]   And that's what's frustrating.
[00:49:10.680 --> 00:49:12.800]   This is just one of many fields, bioethics.
[00:49:12.800 --> 00:49:16.960]   We're about ready to introduce the CRISPR gene to the world.
[00:49:16.960 --> 00:49:20.280]   And retrude genomic splicing.
[00:49:20.280 --> 00:49:25.240]   And that's a huge conversation we need to have about what's okay, what's not okay, what
[00:49:25.240 --> 00:49:28.320]   we want to allow, what we want to have to happen.
[00:49:28.320 --> 00:49:31.440]   Do we want people to be able to select the sex of their children?
[00:49:31.440 --> 00:49:34.000]   They're going to be able to do that easily and simply.
[00:49:34.000 --> 00:49:35.000]   Do we want...
[00:49:35.000 --> 00:49:41.040]   I would just forgive me, just finish Daniel Suarez' book about CRISPR, about genetic modification.
[00:49:41.040 --> 00:49:42.040]   It's called...
[00:49:42.040 --> 00:49:43.040]   What is it called?
[00:49:43.040 --> 00:49:44.040]   Fake?
[00:49:44.040 --> 00:49:45.040]   Say again?
[00:49:45.040 --> 00:49:46.040]   Change it.
[00:49:46.040 --> 00:49:53.280]   It starts off, you can go to the store and buy a stamina for your kid, buy 30 extra IQ
[00:49:53.280 --> 00:49:56.400]   points for your kid, whatever you can afford.
[00:49:56.400 --> 00:50:01.640]   There's so many things coming down the pike, autonomous vehicles.
[00:50:01.640 --> 00:50:03.920]   Who's responsible when a car's driving?
[00:50:03.920 --> 00:50:06.480]   Do we want to let cars drive?
[00:50:06.480 --> 00:50:07.760]   We are just ill-equipped.
[00:50:07.760 --> 00:50:09.760]   And I have to say, let's be practical.
[00:50:09.760 --> 00:50:11.160]   I agree with you, Stacey.
[00:50:11.160 --> 00:50:14.520]   The sane, sensible thing to do would be have a Chautauqua.
[00:50:14.520 --> 00:50:15.920]   And let's all talk about it.
[00:50:15.920 --> 00:50:17.640]   It's never going to happen.
[00:50:17.640 --> 00:50:18.640]   Never.
[00:50:18.640 --> 00:50:19.640]   Who was it?
[00:50:19.640 --> 00:50:23.480]   So Brad Smith just released a statement saying that we need a Geneva conference.
[00:50:23.480 --> 00:50:24.480]   From Microsoft.
[00:50:24.480 --> 00:50:25.480]   Yeah.
[00:50:25.480 --> 00:50:26.480]   Microsoft.
[00:50:26.480 --> 00:50:27.480]   On these issues.
[00:50:27.480 --> 00:50:28.920]   And it was partly it was also about...
[00:50:28.920 --> 00:50:29.920]   It was after Microsoft...
[00:50:29.920 --> 00:50:30.920]   It's about cyber security.
[00:50:30.920 --> 00:50:31.920]   Cyber security.
[00:50:31.920 --> 00:50:32.920]   NSA.
[00:50:32.920 --> 00:50:33.920]   Yeah.
[00:50:33.920 --> 00:50:36.880]   Cracked windows and then it leaked out because they didn't...
[00:50:36.880 --> 00:50:39.320]   They weren't good stewards of the cracks.
[00:50:39.320 --> 00:50:41.120]   And then everybody got...
[00:50:41.120 --> 00:50:44.720]   Want to cry over the weekend, not everybody, but 200,000 people did.
[00:50:44.720 --> 00:50:49.560]   Stacey, so what you're saying, Stacey, is that you want this conversation.
[00:50:49.560 --> 00:50:52.920]   We should have this conversation, but we're probably not equipped to have the conversation
[00:50:52.920 --> 00:50:53.920]   right now.
[00:50:53.920 --> 00:50:55.720]   Well, we're going to have it.
[00:50:55.720 --> 00:51:00.400]   And I think that one of the things, and you should tell your students too, that journalists
[00:51:00.400 --> 00:51:03.200]   are going to have to do is we're going to have to bring these issues to the forefront
[00:51:03.200 --> 00:51:04.200]   in a way that people can understand.
[00:51:04.200 --> 00:51:07.680]   The other place that does this, and you can make fun of me for this, but the World Economic
[00:51:07.680 --> 00:51:09.680]   Forum, Davos, is...
[00:51:09.680 --> 00:51:10.680]   Fine.
[00:51:10.680 --> 00:51:11.680]   Right.
[00:51:11.680 --> 00:51:17.320]   And indeed convene these people around these issues, and they've opened up a Silicon Valley
[00:51:17.320 --> 00:51:22.240]   outpost in the Presidio, where they're trying to bring these folks together.
[00:51:22.240 --> 00:51:24.840]   And I think there's an opportunity to convene...
[00:51:24.840 --> 00:51:31.880]   As you say, Stacey, it's a civil conversation as in all of society and the various constituents.
[00:51:31.880 --> 00:51:34.920]   You need business there, you need technology there, you need academics there, you do need
[00:51:34.920 --> 00:51:38.800]   government there, find good people.
[00:51:38.800 --> 00:51:41.360]   And you need educators there.
[00:51:41.360 --> 00:51:44.040]   This is our rulemaking for society.
[00:51:44.040 --> 00:51:45.040]   Yeah.
[00:51:45.040 --> 00:51:48.400]   Unless we shouldn't stop the progress.
[00:51:48.400 --> 00:51:49.560]   Stacey, go ahead.
[00:51:49.560 --> 00:51:51.600]   No, no, I am agreeing with you.
[00:51:51.600 --> 00:51:58.560]   I was thinking about the future tense, which is New America Foundation, Arizona State,
[00:51:58.560 --> 00:52:04.320]   and Slate do a good job from a journalistic perspective talking about these conversations.
[00:52:04.320 --> 00:52:12.000]   And this is far beyond Google I/O, but these are things that if we don't talk about them
[00:52:12.000 --> 00:52:20.440]   now, if we don't find a way to develop a moral and a cultural consensus, which is admittedly
[00:52:20.440 --> 00:52:24.240]   going to be hard, we're just going to...
[00:52:24.240 --> 00:52:27.720]   Basically, what we're doing now is shrugging our shoulders and saying, "Ah, capitalism
[00:52:27.720 --> 00:52:32.360]   will take care of it," which just means that people who have money will be further and
[00:52:32.360 --> 00:52:34.560]   further advanced beyond...
[00:52:34.560 --> 00:52:36.400]   And the divide just continues to grow.
[00:52:36.400 --> 00:52:37.400]   That is it.
[00:52:37.400 --> 00:52:38.400]   Right.
[00:52:38.400 --> 00:52:40.680]   Meanwhile, let's talk about some other great things you can buy.
[00:52:40.680 --> 00:52:41.840]   Hey, Google Home!
[00:52:41.840 --> 00:52:42.840]   Hey, Google Home!
[00:52:42.840 --> 00:52:43.840]   Octopus Dumplings.
[00:52:43.840 --> 00:52:44.840]   Let's talk...
[00:52:44.840 --> 00:52:49.480]   Man, I really got a yen for octopus dumplings during that.
[00:52:49.480 --> 00:52:51.680]   You do look really good.
[00:52:51.680 --> 00:52:56.440]   Oh, I think when he was doing this, I was like, "Yeah, you don't want to just go up and
[00:52:56.440 --> 00:52:59.720]   eat Japanese street food without having an idea of what it is."
[00:52:59.720 --> 00:53:00.720]   Oh, right.
[00:53:00.720 --> 00:53:02.880]   Scott Huffman came up on stage.
[00:53:02.880 --> 00:53:08.160]   He's a lot of fun and a really great presenter and demonstrated some of the new features of
[00:53:08.160 --> 00:53:14.920]   the Google Lens in Assistant, including taking a picture of a Japanese menu board on the
[00:53:14.920 --> 00:53:17.600]   street and finding out what it is.
[00:53:17.600 --> 00:53:24.520]   Not only did it define what it was, but it showed him information about it like pictures.
[00:53:24.520 --> 00:53:28.520]   He said, "Can I see some pictures of those and where you can buy them?"
[00:53:28.520 --> 00:53:29.520]   Fascinating things.
[00:53:29.520 --> 00:53:33.640]   He then showed using Google Assistant...
[00:53:33.640 --> 00:53:37.920]   Apparently, you could turn on Word Lens in Google Assistant or will soon be able to get
[00:53:37.920 --> 00:53:39.640]   some point in the next few weeks.
[00:53:39.640 --> 00:53:46.000]   He showed a theater marquee that had a stone foxes concert coming up May 17th, 9 p.m.
[00:53:46.000 --> 00:53:50.080]   It not only interpreted it, it said, "Do you want to hear some stone foxes?
[00:53:50.080 --> 00:53:51.560]   Do you want to know more about the stone foxes?
[00:53:51.560 --> 00:53:53.960]   Do you want to buy tickets to this show?
[00:53:53.960 --> 00:53:55.800]   Would you like to add this to your calendar?"
[00:53:55.800 --> 00:53:58.440]   It added it to his calendar perfectly.
[00:53:58.440 --> 00:54:01.320]   Although it did say it was only going to be a one-hour concert, and I thought that was
[00:54:01.320 --> 00:54:04.000]   a little short of a stone.
[00:54:04.000 --> 00:54:05.960]   I'm noticing the mistakes.
[00:54:05.960 --> 00:54:07.960]   Anyway, I can't wait.
[00:54:07.960 --> 00:54:09.880]   I think that'll be great.
[00:54:09.880 --> 00:54:12.000]   These features are really cool.
[00:54:12.000 --> 00:54:16.960]   Sometimes you run the risk in Google events of seeing these features and going, "Wow,
[00:54:16.960 --> 00:54:17.960]   that's really amazing.
[00:54:17.960 --> 00:54:20.320]   That's going to change how I use my device."
[00:54:20.320 --> 00:54:23.640]   Then flash forward six months later, you've lived with the device, you've used it a handful
[00:54:23.640 --> 00:54:26.120]   of times, and nothing.
[00:54:26.120 --> 00:54:31.080]   However, at the same time, really cool technology, and you know it's important to Google because
[00:54:31.080 --> 00:54:32.640]   it kept coming up.
[00:54:32.640 --> 00:54:34.480]   They mentioned it at the beginning.
[00:54:34.480 --> 00:54:37.280]   It found its way into all of these other topics.
[00:54:37.280 --> 00:54:44.240]   This is a really good representation of their ability of leveraging their theme of AI and
[00:54:44.240 --> 00:54:51.160]   turning it into something that consumers might pull consumers in and allow them to warm up
[00:54:51.160 --> 00:54:55.080]   to the idea of letting Google in more and more and getting more access.
[00:54:55.080 --> 00:54:56.080]   It's really true, Jason.
[00:54:56.080 --> 00:54:59.800]   If you compare it to Facebook's presentation where they're going after the same tools of
[00:54:59.800 --> 00:55:05.440]   AI and AR, but Google has so many more practical ways for users, "Hey, we can have sharks swimming
[00:55:05.440 --> 00:55:06.440]   around your cereal bowl."
[00:55:06.440 --> 00:55:07.440]   "Oh, I can't wait.
[00:55:07.440 --> 00:55:08.440]   Who cares?"
[00:55:08.440 --> 00:55:15.360]   Versus here, I can make a concert by tickets to a concert that I have to walk by, and I
[00:55:15.360 --> 00:55:19.640]   can converse with the machine with my images and what I see.
[00:55:19.640 --> 00:55:21.080]   That's really powerful.
[00:55:21.080 --> 00:55:25.280]   Lots of new languages, like a lot of new languages.
[00:55:25.280 --> 00:55:27.080]   Everybody will be pleased to see that.
[00:55:27.080 --> 00:55:32.760]   You'll be able to then, Valerie demonstrated buying something from Panera, which was kind
[00:55:32.760 --> 00:55:39.040]   of amazing because essentially she said to Google Assistant, "I want delivery from Panera."
[00:55:39.040 --> 00:55:43.840]   Then a male voice took over, saying, "This is Panera."
[00:55:43.840 --> 00:55:46.960]   Although it was clearly a machine voice, right?
[00:55:46.960 --> 00:55:48.720]   It wasn't an actual clerk.
[00:55:48.720 --> 00:55:50.440]   They had a conversation.
[00:55:50.440 --> 00:55:54.000]   She ordered and paid, there's voice pay now.
[00:55:54.000 --> 00:56:00.360]   She was able to pay entirely without leaving Google Assistant.
[00:56:00.360 --> 00:56:03.200]   I presume you could do that on Google Home as well.
[00:56:03.200 --> 00:56:06.040]   Assistant has a lot of nice new features.
[00:56:06.040 --> 00:56:08.840]   I thought one of the most interesting features, and we're going to have to move it along a
[00:56:08.840 --> 00:56:13.200]   little bit, so I'm going to skip through some of the little details.
[00:56:13.200 --> 00:56:18.360]   I think one of the nice new features in the Google Home, and it's not here yet because
[00:56:18.360 --> 00:56:22.800]   I kept trying to call my mom, is hands-free calling.
[00:56:22.800 --> 00:56:25.880]   I look back at Jeff and I said, "We've come a long way."
[00:56:25.880 --> 00:56:26.880]   They announced...
[00:56:26.880 --> 00:56:28.240]   On the far, it's looked at each other, yeah.
[00:56:28.240 --> 00:56:30.000]   They announced, you could say, "Call Mom."
[00:56:30.000 --> 00:56:34.600]   They said, "Hands-free calling for free to any phone."
[00:56:34.600 --> 00:56:36.800]   We're not talking to another Google Home.
[00:56:36.800 --> 00:56:41.920]   We're not talking to a computer to any phone in the US for free.
[00:56:41.920 --> 00:56:43.520]   Remember when we used to have to...
[00:56:43.520 --> 00:56:45.520]   Are you on the phone still at long distances?
[00:56:45.520 --> 00:56:46.520]   It costs us a fortune.
[00:56:46.520 --> 00:56:47.520]   Can you get off the phone?
[00:56:47.520 --> 00:56:48.520]   Exactly.
[00:56:48.520 --> 00:56:50.600]   My mom's saying, "Get off the line.
[00:56:50.600 --> 00:56:51.840]   I want to talk."
[00:56:51.840 --> 00:56:57.000]   Or me being on a modem and my kid picking up the phone and disconnecting me in the middle
[00:56:57.000 --> 00:56:59.360]   of my Dungeons and Dragons game.
[00:56:59.360 --> 00:57:03.000]   You can sync it to your own number too.
[00:57:03.000 --> 00:57:06.720]   So technically, I could take my number and make it so that if I make a call through my
[00:57:06.720 --> 00:57:12.240]   Google Home, the number that they see on the other end is my actual number and not some
[00:57:12.240 --> 00:57:14.200]   random number synced through the home.
[00:57:14.200 --> 00:57:17.480]   I wonder what point you're able to receive calls.
[00:57:17.480 --> 00:57:19.320]   Through your Google Home as well.
[00:57:19.320 --> 00:57:22.320]   At some point, have your phone connected so that someone calls your phone.
[00:57:22.320 --> 00:57:23.320]   They just turned on.
[00:57:23.320 --> 00:57:25.360]   It turns out these had Bluetooth all along.
[00:57:25.360 --> 00:57:26.360]   We didn't know it.
[00:57:26.360 --> 00:57:27.360]   They said that'll be turned on.
[00:57:27.360 --> 00:57:31.080]   So you put parrot to your phone so I guess you could.
[00:57:31.080 --> 00:57:33.520]   Stacey, did you get excited about that?
[00:57:33.520 --> 00:57:34.520]   I did.
[00:57:34.520 --> 00:57:37.920]   I got excited about two things related to that.
[00:57:37.920 --> 00:57:39.400]   Well, one was this.
[00:57:39.400 --> 00:57:45.040]   This is just thinking ahead because if you can have people call your Google Home one
[00:57:45.040 --> 00:57:51.040]   day, then that becomes and the echo suddenly becomes your home phone again, which means
[00:57:51.040 --> 00:57:53.960]   you need a number for it that's not your mobile phone.
[00:57:53.960 --> 00:57:58.800]   So I started thinking about that just because I've already had to get my Google Home, its
[00:57:58.800 --> 00:57:59.800]   own email address.
[00:57:59.800 --> 00:58:03.080]   So now I might need its own phone number.
[00:58:03.080 --> 00:58:06.720]   Can you pretty soon I'll need its own room in the house.
[00:58:06.720 --> 00:58:09.680]   Can you call from the Amazon echo?
[00:58:09.680 --> 00:58:13.280]   Can you call a phone or I think you can only call another guy?
[00:58:13.280 --> 00:58:15.840]   I call people who have the Amazon echo app.
[00:58:15.840 --> 00:58:17.280]   That's what I thought.
[00:58:17.280 --> 00:58:18.280]   So this is really cool.
[00:58:18.280 --> 00:58:19.640]   Sorry to be a LESS app.
[00:58:19.640 --> 00:58:22.480]   It's kind of fun because Google just stepped up quite a bit.
[00:58:22.480 --> 00:58:25.640]   We call it the echo app here because we don't want anybody to get upset.
[00:58:25.640 --> 00:58:27.440]   Yes, sorry everybody.
[00:58:27.440 --> 00:58:32.600]   Incidentally, the phones in the audience were going off right and left because they said,
[00:58:32.600 --> 00:58:36.720]   "Okay, see, goo all the time."
[00:58:36.720 --> 00:58:42.960]   And everything was waking up in my pocket.
[00:58:42.960 --> 00:58:47.520]   And I think this was their way of thumbing their nose at Amazon.
[00:58:47.520 --> 00:58:50.600]   Oh, echo can make phone calls to other echoes.
[00:58:50.600 --> 00:58:55.120]   You can call any phone in the US or Canada for free.
[00:58:55.120 --> 00:58:57.520]   Sorry.
[00:58:57.520 --> 00:59:02.680]   They also made good on the video that they launched six months ago with this which had
[00:59:02.680 --> 00:59:07.480]   notifications by saying, "Hey, here's some more stuff about the notifications we're
[00:59:07.480 --> 00:59:08.480]   going to have.
[00:59:08.480 --> 00:59:09.480]   Is that what they call it?"
[00:59:09.480 --> 00:59:10.560]   Yeah, I thought about that.
[00:59:10.560 --> 00:59:14.480]   I think they really couldn't do it until you could have separate accounts.
[00:59:14.480 --> 00:59:20.640]   So they recently added the ability to have more than one account, up to six accounts,
[00:59:20.640 --> 00:59:23.280]   on any given home Google Home device.
[00:59:23.280 --> 00:59:28.720]   So I think that that was, if you think about it, if only one person's getting notifications,
[00:59:28.720 --> 00:59:29.720]   that's ridiculous.
[00:59:29.720 --> 00:59:32.320]   And differentiating voices too was critical to them.
[00:59:32.320 --> 00:59:35.080]   Yes, it could tell who was talking to it.
[00:59:35.080 --> 00:59:37.360]   I wonder if it could tell who's in the room.
[00:59:37.360 --> 00:59:41.600]   Well, it was also, they really said that through AI, when they originally designed Google
[00:59:41.600 --> 00:59:48.960]   Home, it had eight microphones, but they only needed three because of the AI processing
[00:59:48.960 --> 00:59:50.200]   that was the learning system process.
[00:59:50.200 --> 00:59:53.320]   It was allowed them to see where the voices were, it seemed.
[00:59:53.320 --> 00:59:55.800]   That wasn't the only shot across Amazon's bow.
[00:59:55.800 --> 01:00:01.800]   They'll remember Amazon released and I immediately ordered the, what do they call it, the echo
[01:00:01.800 --> 01:00:02.800]   show?
[01:00:02.800 --> 01:00:03.800]   Look, is that?
[01:00:03.800 --> 01:00:04.800]   Show.
[01:00:04.800 --> 01:00:05.800]   Oh, which one?
[01:00:05.800 --> 01:00:06.800]   Show the screen.
[01:00:06.800 --> 01:00:07.800]   Show is the one with the screen.
[01:00:07.800 --> 01:00:08.800]   Look, is that with the camera?
[01:00:08.800 --> 01:00:10.800]   What's the one you want your closet?
[01:00:10.800 --> 01:00:11.800]   That's the one.
[01:00:11.800 --> 01:00:12.800]   That's the look.
[01:00:12.800 --> 01:00:13.800]   You got the look.
[01:00:13.800 --> 01:00:14.800]   You got the look.
[01:00:14.800 --> 01:00:15.800]   I like to.
[01:00:15.800 --> 01:00:16.800]   No better.
[01:00:16.800 --> 01:00:17.800]   I'm sorry.
[01:00:17.800 --> 01:00:20.240]   I don't know what happened there.
[01:00:20.240 --> 01:00:21.800]   I just channeled 1974.
[01:00:21.800 --> 01:00:22.800]   Blackout.
[01:00:22.800 --> 01:00:23.800]   I blacked out.
[01:00:23.800 --> 01:00:24.800]   The fever is coming back.
[01:00:24.800 --> 01:00:27.320]   No, the Google, so the Google Home has a screen on it.
[01:00:27.320 --> 01:00:30.040]   So, I mean, the Amazon Home has a screen on it.
[01:00:30.040 --> 01:00:36.320]   So Google says, well, we could do that or don't you have screens all over the house?
[01:00:36.320 --> 01:00:40.880]   So what we're going to do instead, and I thought this was wild, you can send your calendar
[01:00:40.880 --> 01:00:44.280]   to your TV if it's Chrome, if it's Chromecast enabled.
[01:00:44.280 --> 01:00:49.720]   And now, now, is the installed base of Chromecast has to be much, much larger than Amazon Fire.
[01:00:49.720 --> 01:00:53.000]   Well, almost all the, we just bought a new Vizio TV cast enabled.
[01:00:53.000 --> 01:00:54.000]   You're seeing that.
[01:00:54.000 --> 01:00:55.720]   Have a built in or it's built in.
[01:00:55.720 --> 01:00:56.840]   It doesn't have to have it built in.
[01:00:56.840 --> 01:01:00.160]   If you have a Chromecast plugged in, I'm sure that works as well.
[01:01:00.160 --> 01:01:01.360]   But you could send photos.
[01:01:01.360 --> 01:01:03.560]   You can send YouTube videos.
[01:01:03.560 --> 01:01:04.560]   That's not new.
[01:01:04.560 --> 01:01:08.240]   What was there was something else I thought was really cool.
[01:01:08.240 --> 01:01:15.880]   So, well, are we talking about telling Amazon to send something to your screen?
[01:01:15.880 --> 01:01:16.880]   Yes.
[01:01:16.880 --> 01:01:20.400]   It was also what you guys were watching a show across devices that would know where you left
[01:01:20.400 --> 01:01:23.000]   off because it's knows it's you.
[01:01:23.000 --> 01:01:27.480]   So when you say, show me whatever, as I remember they said, it would pick up where you last
[01:01:27.480 --> 01:01:28.480]   left off.
[01:01:28.480 --> 01:01:36.760]   Well, it also does show you any YouTube stuff and the added partner HBO, Hulu, YouTube
[01:01:36.760 --> 01:01:45.920]   TV, Google Play, CBS, Food Network, the CW, HGTV, Red Bull, Travel, Crackle, DIY, Vicki.
[01:01:45.920 --> 01:01:49.960]   So glad they finally have the Vicki channel and the cooking channel.
[01:01:49.960 --> 01:01:52.440]   So that's, I mean, look at it.
[01:01:52.440 --> 01:01:54.800]   It's not my, well, if you subscribe to YouTube TV.
[01:01:54.800 --> 01:01:55.800]   Oh, what's the recipe?
[01:01:55.800 --> 01:02:00.400]   What's the recipe on the Food Network right now?
[01:02:00.400 --> 01:02:01.400]   That's what they can ask.
[01:02:01.400 --> 01:02:04.880]   It was, I don't know if that's what got you excited, but I was like, who?
[01:02:04.880 --> 01:02:06.200]   That one can stay safe.
[01:02:06.200 --> 01:02:07.200]   That's great.
[01:02:07.200 --> 01:02:08.760]   I'll tell you what got me excited.
[01:02:08.760 --> 01:02:12.240]   If you have YouTube TV and your DVR, you could watch anything that you've DVR'd.
[01:02:12.240 --> 01:02:17.920]   You could just say, hey, show me last night's late show and it would just pop up on your
[01:02:17.920 --> 01:02:18.920]   TV.
[01:02:18.920 --> 01:02:23.280]   I just, I think these are all in the videos that we saw last time, but I think they've
[01:02:23.280 --> 01:02:25.760]   really, they're starting to really.
[01:02:25.760 --> 01:02:26.760]   It shows the weather.
[01:02:26.760 --> 01:02:32.440]   I guess what Stacy said before too, it's the context and intent that comes from understanding
[01:02:32.440 --> 01:02:33.440]   the antecedent.
[01:02:33.440 --> 01:02:34.440]   Right.
[01:02:34.440 --> 01:02:36.440]   You say, buy me tickets for that.
[01:02:36.440 --> 01:02:38.440]   That knows what the context of the conversation is.
[01:02:38.440 --> 01:02:42.040]   And that's, that is, that is, they showed that leap a year ago.
[01:02:42.040 --> 01:02:45.720]   Now they're showing the application of it a dozen ways.
[01:02:45.720 --> 01:02:50.440]   I'll tell you what, I ordered another Google home actually to add the keynote because I
[01:02:50.440 --> 01:02:53.200]   thought I'm going to, I'm going to use this more.
[01:02:53.200 --> 01:02:55.800]   And I, and I have a lot of Chromecast enabled TVs.
[01:02:55.800 --> 01:02:59.680]   And I think this is just really going to be very useful.
[01:02:59.680 --> 01:03:04.400]   I also am excited about the new photos features.
[01:03:04.400 --> 01:03:08.360]   They added two interesting features that I think people will like.
[01:03:08.360 --> 01:03:12.560]   I think this is, this is the kind of thing Google does that people, normal people will
[01:03:12.560 --> 01:03:14.680]   go, oh, I can use that.
[01:03:14.680 --> 01:03:16.360]   And I think they've surfaced it in a good way.
[01:03:16.360 --> 01:03:19.800]   They're suggested sharing and shared libraries.
[01:03:19.800 --> 01:03:25.920]   I was a little worried about sharing at first because it reminded me of when we first heard
[01:03:25.920 --> 01:03:31.200]   about the photo, the shared events that you could go to an event, open up a shared event
[01:03:31.200 --> 01:03:35.080]   and everybody would share their photos from like at a wedding or whatever.
[01:03:35.080 --> 01:03:39.920]   I was like, okay, if you're doing this sort of sharing, which essentially with suggested
[01:03:39.920 --> 01:03:46.600]   sharing allows you to kind of share photos from your, from your camera bowl that's stored
[01:03:46.600 --> 01:03:51.520]   in the cloud or a full album or whatever with somebody else, the receive on the receiving
[01:03:51.520 --> 01:03:54.880]   end, they have to be using something that they already have, hopefully.
[01:03:54.880 --> 01:03:57.800]   And thankfully they've tied it into the Google Photos app.
[01:03:57.800 --> 01:04:02.880]   So as long as you and the person who's sharing with it with have the photos app installed
[01:04:02.880 --> 01:04:07.320]   on the device, and you pretty much just pick the people or, and I thought this was really
[01:04:07.320 --> 01:04:08.320]   cool.
[01:04:08.320 --> 01:04:12.200]   Matching, because there's already the face matching aspect of Google Photos.
[01:04:12.200 --> 01:04:13.200]   You can, you can do that.
[01:04:13.200 --> 01:04:15.120]   You've been able to do that for a couple of years.
[01:04:15.120 --> 01:04:20.600]   And now it will detect who's in your photos from that event and offer those up as options
[01:04:20.600 --> 01:04:21.600]   to share to directly.
[01:04:21.600 --> 01:04:22.600]   And this is what I think is so critical.
[01:04:22.600 --> 01:04:26.560]   We mentioned this earlier in the show, but I think this is so critical that Google's been
[01:04:26.560 --> 01:04:30.040]   talking about facial recognition for years and, and, and it's near the creepy lines so
[01:04:30.040 --> 01:04:31.040]   they stayed back from it.
[01:04:31.040 --> 01:04:33.680]   Facebook announced that they were going to do it and didn't kind of acknowledge the
[01:04:33.680 --> 01:04:34.680]   creepy line.
[01:04:34.680 --> 01:04:39.160]   But what happened today was they said, well, you're going to love facial recognition because
[01:04:39.160 --> 01:04:42.840]   these are people you already know who are already your photos, who you want to share
[01:04:42.840 --> 01:04:45.880]   with, and it's with your permission.
[01:04:45.880 --> 01:04:50.400]   And it makes facial recognition cuddly and okay.
[01:04:50.400 --> 01:04:51.400]   Exactly.
[01:04:51.400 --> 01:04:52.400]   Exactly.
[01:04:52.400 --> 01:04:55.600]   It's around people you do know or your family and friends.
[01:04:55.600 --> 01:04:56.600]   It's for utility.
[01:04:56.600 --> 01:04:58.680]   It's the other story that I, that I always tell the Germans.
[01:04:58.680 --> 01:05:00.360]   Oh, and it's easy to use.
[01:05:00.360 --> 01:05:03.760]   And it just kind of shows up if you're using Google Photos is right there.
[01:05:03.760 --> 01:05:04.760]   Right.
[01:05:04.760 --> 01:05:05.760]   At least to a function.
[01:05:05.760 --> 01:05:09.560]   So the story I've told many times is that is the journey went nuts over everything about
[01:05:09.560 --> 01:05:12.800]   privacy and Google except for priority inbox.
[01:05:12.800 --> 01:05:14.160]   Google's reading my email.
[01:05:14.160 --> 01:05:15.160]   Yes.
[01:05:15.160 --> 01:05:16.480]   Thank you very much and it helps be great appeal.
[01:05:16.480 --> 01:05:21.240]   So as long as the utilities are orderly people, the Germans, they are.
[01:05:21.240 --> 01:05:24.280]   They're also grateful for good features and good services.
[01:05:24.280 --> 01:05:25.920]   There are a lot of German reporters here, man.
[01:05:25.920 --> 01:05:28.080]   They love covering Google.
[01:05:28.080 --> 01:05:31.560]   But, but so, so, so facial recognition now becomes okay.
[01:05:31.560 --> 01:05:32.560]   Right.
[01:05:32.560 --> 01:05:35.480]   The creepy line has been fudged.
[01:05:35.480 --> 01:05:37.760]   I think it was very important to them.
[01:05:37.760 --> 01:05:39.040]   I think we're going to use it.
[01:05:39.040 --> 01:05:43.800]   I wonder, Stacey, did you run to Andrew and say, Hey, honey, look at this automatic sharing
[01:05:43.800 --> 01:05:48.040]   of my photos of the kids with you and you share your photos of the kids with me.
[01:05:48.040 --> 01:05:52.000]   Suddenly we'll have like an out a joint album automatically.
[01:05:52.000 --> 01:05:56.480]   So no, I didn't because I still I'm a little creeped out by Google Photos.
[01:05:56.480 --> 01:05:57.480]   I'll just be honest.
[01:05:57.480 --> 01:05:58.480]   I'm sorry.
[01:05:58.480 --> 01:06:03.840]   And I actually share I have a automatic upload folder and Dropbox set up on my phone and
[01:06:03.840 --> 01:06:06.720]   I share that with my friends who love seeing my.
[01:06:06.720 --> 01:06:07.720]   My expert.
[01:06:07.720 --> 01:06:10.640]   It will require one time effort for me.
[01:06:10.640 --> 01:06:15.080]   But here's here's what I was looking for and I want to know the answer to.
[01:06:15.080 --> 01:06:22.080]   So ask around, how do you turn it off in the case of divorce or a breakup or something?
[01:06:22.080 --> 01:06:24.640]   And then what happens to the photos that you shared?
[01:06:24.640 --> 01:06:26.160]   Do they pull them off the device?
[01:06:26.160 --> 01:06:27.160]   Do they keep?
[01:06:27.160 --> 01:06:28.160]   No, I think we've talked about that before.
[01:06:28.160 --> 01:06:30.120]   The president is there, Stacey.
[01:06:30.120 --> 01:06:33.520]   When they talked about photos originally, if you shared a photo with someone and they
[01:06:33.520 --> 01:06:35.320]   already have it.
[01:06:35.320 --> 01:06:37.040]   It's it's it's like sharing anything.
[01:06:37.040 --> 01:06:38.040]   They have it.
[01:06:38.040 --> 01:06:39.040]   Right.
[01:06:39.040 --> 01:06:40.040]   Pull it back.
[01:06:40.040 --> 01:06:43.000]   And the president there is if you gave a print of a photo to somebody else, where you
[01:06:43.000 --> 01:06:46.200]   can go, you know, unless you're a roger Simpson, you're going to break into their house and
[01:06:46.200 --> 01:06:47.360]   steal it back.
[01:06:47.360 --> 01:06:48.360]   Right.
[01:06:48.360 --> 01:06:49.720]   And that's my assumption.
[01:06:49.720 --> 01:06:53.120]   But then there also it also needs to be easy to unshare with people.
[01:06:53.120 --> 01:06:54.120]   Well, yes, it does.
[01:06:54.120 --> 01:06:55.800]   It also makes you start.
[01:06:55.800 --> 01:07:02.680]   So he didn't there was the unintended joke on stage when he said, I'm sure he was the
[01:07:02.680 --> 01:07:05.400]   issue because he started sharing with his wife.
[01:07:05.400 --> 01:07:08.240]   But you also and this there was no accident here.
[01:07:08.240 --> 01:07:09.240]   Yes.
[01:07:09.240 --> 01:07:10.560]   And set the date the sharing starts.
[01:07:10.560 --> 01:07:16.040]   He says, I'm going to start sharing with her from the time I met her because obviously
[01:07:16.040 --> 01:07:18.880]   it's not going to share earlier photos.
[01:07:18.880 --> 01:07:22.080]   Or, you know, who knows who's in those photos, right?
[01:07:22.080 --> 01:07:23.080]   Someone else in your life.
[01:07:23.080 --> 01:07:25.440]   Honey, who's that strange woman in your picture?
[01:07:25.440 --> 01:07:27.040]   But I don't think it's for that purpose.
[01:07:27.040 --> 01:07:32.640]   I think it's really going to be mostly used by couples with children who and you know,
[01:07:32.640 --> 01:07:34.920]   he made an excellent point.
[01:07:34.920 --> 01:07:40.880]   I am not in any of my kids pictures ever any of them because I took them.
[01:07:40.880 --> 01:07:47.800]   And so now, you know, your spouse can share the pictures that she took of you and the
[01:07:47.800 --> 01:07:48.800]   kids.
[01:07:48.800 --> 01:07:52.960]   You can share your pictures that you took of her and the kids and you'll get a unified
[01:07:52.960 --> 01:07:54.480]   album that'll have her.
[01:07:54.480 --> 01:07:55.480]   That's just one application.
[01:07:55.480 --> 01:08:00.040]   Tammy, Tammy the book too though, because I want to say you could also say who's you
[01:08:00.040 --> 01:08:04.360]   only share pictures of your husband with each other.
[01:08:04.360 --> 01:08:05.360]   That's it.
[01:08:05.360 --> 01:08:06.360]   That's really cool.
[01:08:06.360 --> 01:08:07.360]   Especially.
[01:08:07.360 --> 01:08:08.360]   I don't want to see my whiteboard pictures.
[01:08:08.360 --> 01:08:12.760]   Yes, my husband's going to hate my gadget picture.
[01:08:12.760 --> 01:08:14.320]   Actually my daughter, she goes through it.
[01:08:14.320 --> 01:08:16.240]   She's like, who's that man in the photo?
[01:08:16.240 --> 01:08:19.040]   I'm like, oh, that's the FCC commissioner.
[01:08:19.040 --> 01:08:20.040]   Just ignore that.
[01:08:20.040 --> 01:08:23.880]   Why is there a picture of Ajit Pai and a photo screen?
[01:08:23.880 --> 01:08:25.960]   A former, former FCC commissioner.
[01:08:25.960 --> 01:08:27.000]   Oh, I know.
[01:08:27.000 --> 01:08:29.240]   We all loved him.
[01:08:29.240 --> 01:08:31.080]   I did.
[01:08:31.080 --> 01:08:37.040]   I was a little gratified when the Google Photos guy showed his photo stream because he, his
[01:08:37.040 --> 01:08:41.440]   stream looked much like mine, full of kind of some nice pictures and then pictures of
[01:08:41.440 --> 01:08:45.520]   receipts and then pictures of whiteboards and of menus and you know, just kind of the
[01:08:45.520 --> 01:08:48.760]   craft, the random craft we use our camera phones for.
[01:08:48.760 --> 01:08:52.560]   And I thought, well, at least they're acknowledging that that's what's in there.
[01:08:52.560 --> 01:08:56.840]   And they're giving you ways to not share obviously your receipts and things like this.
[01:08:56.840 --> 01:09:00.080]   Look, they're the last to the party with photo books.
[01:09:00.080 --> 01:09:01.080]   Apple's done it.
[01:09:01.080 --> 01:09:02.880]   Adobe does it.
[01:09:02.880 --> 01:09:08.600]   But I have to say we actually have some physical examples of these photo books and they're well
[01:09:08.600 --> 01:09:09.600]   printed.
[01:09:09.600 --> 01:09:10.600]   They're well high quality.
[01:09:10.600 --> 01:09:13.200]   This is the paperback version.
[01:09:13.200 --> 01:09:14.880]   It's perfect bound.
[01:09:14.880 --> 01:09:18.080]   They have some and the paperback version is some full bleed and then they did one thing
[01:09:18.080 --> 01:09:19.080]   which I really liked.
[01:09:19.080 --> 01:09:20.080]   It must have been.
[01:09:20.080 --> 01:09:22.920]   Yeah, it must have been with a panel.
[01:09:22.920 --> 01:09:24.120]   It went across two pages.
[01:09:24.120 --> 01:09:29.040]   I'm going to try to find it here.
[01:09:29.040 --> 01:09:30.880]   Can you find it while I show the other one?
[01:09:30.880 --> 01:09:32.680]   This is the hardcover.
[01:09:32.680 --> 01:09:37.080]   This is $20 or starts at $20 which is very good price, something like this.
[01:09:37.080 --> 01:09:41.840]   But more importantly, because photo books, everybody's got photo books, more importantly
[01:09:41.840 --> 01:09:47.280]   the automation involved in making these photo books is really neat.
[01:09:47.280 --> 01:09:54.000]   You know, it basically, the idea is anyway that Google photos chooses the best photos.
[01:09:54.000 --> 01:09:56.320]   You just deselect the ones you don't want.
[01:09:56.320 --> 01:09:58.640]   You can put text on the cover.
[01:09:58.640 --> 01:10:01.680]   It doesn't seem to be text on the pages on captions.
[01:10:01.680 --> 01:10:03.600]   So it's a pure photo book.
[01:10:03.600 --> 01:10:08.520]   And that's because I think that's an acknowledgement that it's a pain in the butt to caption every
[01:10:08.520 --> 01:10:09.520]   page.
[01:10:09.520 --> 01:10:10.520]   It takes a lot of time.
[01:10:10.520 --> 01:10:11.520]   It's work.
[01:10:11.520 --> 01:10:12.520]   It's all on it.
[01:10:12.520 --> 01:10:17.120]   It's post work and especially around photo, especially our huge photo libraries.
[01:10:17.120 --> 01:10:22.000]   That's one reason why Google photos as a product, it has consistently been one of my favorite
[01:10:22.000 --> 01:10:27.440]   Google products that maps because they're constantly rolling out new features that actually make
[01:10:27.440 --> 01:10:33.480]   it enjoyable to do something that was so painful before managing those large libraries.
[01:10:33.480 --> 01:10:34.480]   Building a photo book.
[01:10:34.480 --> 01:10:39.520]   Like what I really like about that is they showed the idea, like you're talking about
[01:10:39.520 --> 01:10:43.880]   in your photo roll, you have receipts and all this other kind of cruft, you know, scattered
[01:10:43.880 --> 01:10:44.880]   throughout.
[01:10:44.880 --> 01:10:48.280]   They kind of showed the example of just highlight a bunch of the photos.
[01:10:48.280 --> 01:10:50.200]   Doesn't matter if that other stuff gets in there.
[01:10:50.200 --> 01:10:54.160]   More AI, you know, our machines on the back end are going to analyze those pictures and
[01:10:54.160 --> 01:10:57.520]   we're going to realize we're going to understand that you don't actually want to print that
[01:10:57.520 --> 01:10:59.280]   receipt on page 22.
[01:10:59.280 --> 01:11:03.880]   We're going to pick the very best things, put it up there and it just lowers that bar so
[01:11:03.880 --> 01:11:08.360]   people can act on impulse because how many times have I looked at the thousands upon
[01:11:08.360 --> 01:11:12.160]   thousands of photos of my photo roll and thought to myself, wow, I've done absolutely nothing
[01:11:12.160 --> 01:11:14.840]   with any of these moving on with my life.
[01:11:14.840 --> 01:11:18.800]   You know, it's just too difficult for me to like think about jumping through those hoops
[01:11:18.800 --> 01:11:19.800]   to do something like that.
[01:11:19.800 --> 01:11:22.920]   But now that the bar is much lower, I might actually consider that.
[01:11:22.920 --> 01:11:23.920]   I'm sure that would be the same thing.
[01:11:23.920 --> 01:11:27.720]   I've done a lot of grommons work for years putting photos into an hour of this.
[01:11:27.720 --> 01:11:31.680]   And now I could imagine saying that you order a whole series of these with it was you had
[01:11:31.680 --> 01:11:34.600]   a young kid for six months for six months.
[01:11:34.600 --> 01:11:35.600]   Yeah, right.
[01:11:35.600 --> 01:11:36.600]   Do your yearly thing.
[01:11:36.600 --> 01:11:37.600]   Yeah, absolutely.
[01:11:37.600 --> 01:11:39.160]   And they're and they're keepsakes.
[01:11:39.160 --> 01:11:42.640]   So I always used apples because apples is a photo books.
[01:11:42.640 --> 01:11:45.480]   They use a very high quality service to do it.
[01:11:45.480 --> 01:11:49.160]   And I think Apple and Adobe, of course, are probably the best to breathe.
[01:11:49.160 --> 01:11:50.640]   There's Blurb, which does a very good job.
[01:11:50.640 --> 01:11:52.000]   I don't know who's sourcing these.
[01:11:52.000 --> 01:11:53.240]   I think they're very high quality.
[01:11:53.240 --> 01:11:55.280]   The page, a stock is good.
[01:11:55.280 --> 01:11:56.280]   It's coded.
[01:11:56.280 --> 01:11:57.280]   It's it's thick.
[01:11:57.280 --> 01:11:58.280]   They're screened.
[01:11:58.280 --> 01:12:01.800]   Any printed photos going to be screened so you can see dots if you look up close.
[01:12:01.800 --> 01:12:03.960]   But I think the quality that's good.
[01:12:03.960 --> 01:12:07.360]   In the sample that they provided to improve the quality of the photo before this.
[01:12:07.360 --> 01:12:08.360]   That's interesting.
[01:12:08.360 --> 01:12:09.720]   I wouldn't be surprised.
[01:12:09.720 --> 01:12:11.600]   Would you use these Stacy?
[01:12:11.600 --> 01:12:14.280]   I'm not sentimental.
[01:12:14.280 --> 01:12:15.280]   I don't keep anything.
[01:12:15.280 --> 01:12:16.280]   I was serious.
[01:12:16.280 --> 01:12:18.760]   Look at her office, ladies and gentlemen.
[01:12:18.760 --> 01:12:20.760]   There's nothing in it.
[01:12:20.760 --> 01:12:23.000]   That's how you do that.
[01:12:23.000 --> 01:12:24.320]   Now I get it.
[01:12:24.320 --> 01:12:26.360]   See, can you see my photo board?
[01:12:26.360 --> 01:12:27.840]   That's my photo board.
[01:12:27.840 --> 01:12:28.840]   Wow.
[01:12:28.840 --> 01:12:29.840]   You'll be sorry.
[01:12:29.840 --> 01:12:32.800]   That's the extent of photos that I have.
[01:12:32.800 --> 01:12:37.560]   You're old like me and you want to look back and those golden years you had with your beautiful
[01:12:37.560 --> 01:12:40.040]   daughter, you'll be sorry.
[01:12:40.040 --> 01:12:41.720]   I don't even have a baby book for it.
[01:12:41.720 --> 01:12:43.880]   Again, I'm not the right person to ask.
[01:12:43.880 --> 01:12:46.800]   I think it's the same book I had in about 20 years.
[01:12:46.800 --> 01:12:48.040]   This is a woman who had.
[01:12:48.040 --> 01:12:51.320]   I've been working on the railroad play to her wedding.
[01:12:51.320 --> 01:12:54.320]   I'm just saying.
[01:12:54.320 --> 01:12:57.920]   So there was a little bit of stage craft in this particular.
[01:12:57.920 --> 01:12:59.720]   There wasn't much stage craft in this.
[01:12:59.720 --> 01:13:02.200]   I have to say, it was just, it was very, which is fine.
[01:13:02.200 --> 01:13:03.200]   It was very interesting.
[01:13:03.200 --> 01:13:04.200]   It disappointed Jason terribly.
[01:13:04.200 --> 01:13:06.240]   What did Jason want to walk across the stage?
[01:13:06.240 --> 01:13:11.360]   Well, I'm always holding out hope for Boston Dynamics robot, but that never happens.
[01:13:11.360 --> 01:13:17.040]   I was hoping Sir Gabe Bren would break out the blimp and floated over a shoreline amphitheater.
[01:13:17.040 --> 01:13:23.440]   Wouldn't that have been awesome with the longest pass ever made to Sundar?
[01:13:23.440 --> 01:13:26.920]   Oh, and really could have been good.
[01:13:26.920 --> 01:13:28.280]   Opportunity miss Sergey.
[01:13:28.280 --> 01:13:30.320]   I'm just saying.
[01:13:30.320 --> 01:13:35.120]   But there was a little stage craft because the photos guy brought out his.
[01:13:35.120 --> 01:13:36.120]   Can you, what is his name?
[01:13:36.120 --> 01:13:37.720]   Because I want to give him credit.
[01:13:37.720 --> 01:13:39.120]   Was it Bradley?
[01:13:39.120 --> 01:13:40.120]   No, Anil.
[01:13:40.120 --> 01:13:42.320]   Anil, that's it.
[01:13:42.320 --> 01:13:43.320]   You're right.
[01:13:43.320 --> 01:13:44.680]   Boy, you're good.
[01:13:44.680 --> 01:13:49.440]   So Anil said, my kids are in school, but he brought out a cardboard cut out of his kids
[01:13:49.440 --> 01:13:54.280]   to do the selfie and demonstrate how that picture in real time would be shared to his
[01:13:54.280 --> 01:13:55.920]   wife's album, which it did.
[01:13:55.920 --> 01:13:58.040]   It was really cool.
[01:13:58.040 --> 01:14:02.040]   And so that she would see that picture of him with cardboard cuts, that cut outs of his
[01:14:02.040 --> 01:14:03.280]   children.
[01:14:03.280 --> 01:14:10.400]   There was a second attempt at stage craft that I personally don't think went quite as well.
[01:14:10.400 --> 01:14:12.120]   Susan Wojcicki was on stage.
[01:14:12.120 --> 01:14:14.400]   She's in charge of YouTube.
[01:14:14.400 --> 01:14:19.080]   And then introduced a YouTuber.
[01:14:19.080 --> 01:14:20.200]   I don't know what her name was.
[01:14:20.200 --> 01:14:21.680]   She sure was Pepe.
[01:14:21.680 --> 01:14:24.840]   They've been McDonald's last night on chat.
[01:14:24.840 --> 01:14:25.840]   Super chat.
[01:14:25.840 --> 01:14:26.840]   Super chat.
[01:14:26.840 --> 01:14:27.840]   Barbara, Super chat.
[01:14:27.840 --> 01:14:28.840]   McDonald's.
[01:14:28.840 --> 01:14:30.240]   Super chat.
[01:14:30.240 --> 01:14:35.040]   And two guys, the slow mo guys, apparently YouTube stars who were out on the, you know,
[01:14:35.040 --> 01:14:40.000]   we are surely an amphitheater, which is an open air concert venue that has a lawn, you
[01:14:40.000 --> 01:14:41.360]   know, and they were out on the lawn.
[01:14:41.360 --> 01:14:46.480]   It's surrounded by a bunch of people in blue jumpsuits who had a role we would find out
[01:14:46.480 --> 01:14:49.640]   later to demonstrate a feature that isn't that new.
[01:14:49.640 --> 01:14:53.360]   Actually, it's been around for a few months in a little creepy chat.
[01:14:53.360 --> 01:14:56.440]   It's actually a follow on on what twitch.tv does.
[01:14:56.440 --> 01:15:01.640]   Twitch has really done well by letting its presenters make money on their channel through
[01:15:01.640 --> 01:15:04.240]   donations from the chatters.
[01:15:04.240 --> 01:15:10.760]   And so Super chat was added to YouTube as a way for chatters to gift $5, $10, $1, $1
[01:15:10.760 --> 01:15:17.000]   and $5, $500 to the presenter in during live streams.
[01:15:17.000 --> 01:15:22.600]   So they use super chat to raise money for, I think, a very good cause, a group that's
[01:15:22.600 --> 01:15:31.240]   printing with 3D printing, prosthetic fingers, which is okay, that's great.
[01:15:31.240 --> 01:15:34.960]   So Barbara asked everybody to participate.
[01:15:34.960 --> 01:15:38.040]   I think turned off the chat about halfway through it.
[01:15:38.040 --> 01:15:40.040]   At some point, the chat stopped moving.
[01:15:40.040 --> 01:15:41.040]   Yeah, I know.
[01:15:41.040 --> 01:15:42.040]   Would you need to assume?
[01:15:42.040 --> 01:15:45.040]   I'll make a reason why.
[01:15:45.040 --> 01:15:50.840]   But they had to turn it back on because she was going to give them a super chat of $500.
[01:15:50.840 --> 01:15:56.520]   And that meant that 500 water balloons would be thrown at the guy.
[01:15:56.520 --> 01:15:58.960]   This was, they cut the holes.
[01:15:58.960 --> 01:16:03.480]   So, yeah, I wasn't thrilled about the segment, but here's the one thing I thought was cute
[01:16:03.480 --> 01:16:06.480]   slash cool, not okay, cool, interesting.
[01:16:06.480 --> 01:16:07.800]   Let's go with interesting.
[01:16:07.800 --> 01:16:11.800]   It's this idea that through super chat, they were tying it.
[01:16:11.800 --> 01:16:15.560]   Did you notice how they said they can tie it to things like light bulbs and other stuff
[01:16:15.560 --> 01:16:17.160]   in the real world?
[01:16:17.160 --> 01:16:23.680]   And that's like a little glimpse of, I've got to organize my thing here.
[01:16:23.680 --> 01:16:28.640]   It's a little glimpse of where Google can take us once we have all of this stuff connected
[01:16:28.640 --> 01:16:30.400]   on the back end.
[01:16:30.400 --> 01:16:33.160]   And that is really compelling.
[01:16:33.160 --> 01:16:36.560]   In the same way, like last year, I was super excited about instant articles and then we
[01:16:36.560 --> 01:16:39.040]   saw nothing from it ever not instant articles.
[01:16:39.040 --> 01:16:40.040]   I'm sorry.
[01:16:40.040 --> 01:16:41.600]   No, not a hap.
[01:16:41.600 --> 01:16:42.600]   No, no.
[01:16:42.600 --> 01:16:43.600]   Progressive Web Apps.
[01:16:43.600 --> 01:16:44.600]   What was it?
[01:16:44.600 --> 01:16:50.480]   What Google product were you excited about that didn't emerge?
[01:16:50.480 --> 01:16:53.560]   Instant app, I think was what it was called.
[01:16:53.560 --> 01:16:55.880]   Oh, instant Android, Android, Android, instant apps.
[01:16:55.880 --> 01:16:56.880]   Yes.
[01:16:56.880 --> 01:16:59.720]   There were a lot of instant apps from last year that never emerged, right?
[01:16:59.720 --> 01:17:02.480]   No, wait, no, but you saw it this year.
[01:17:02.480 --> 01:17:07.480]   You saw Android instant apps, which was this promise that you could not have to download
[01:17:07.480 --> 01:17:08.480]   a full app.
[01:17:08.480 --> 01:17:10.000]   You could just go someplace and grab something.
[01:17:10.000 --> 01:17:12.480]   You saw it in that Panero demo.
[01:17:12.480 --> 01:17:13.480]   That was-
[01:17:13.480 --> 01:17:14.480]   Oh, was that on?
[01:17:14.480 --> 01:17:15.480]   That was on.
[01:17:15.480 --> 01:17:19.200]   I don't know if it was the same back end architecture, but that was the promise.
[01:17:19.200 --> 01:17:22.880]   The promise is you could do things without going to other places.
[01:17:22.880 --> 01:17:28.000]   And what we're seeing is they're building this infrastructure thanks to AI, thanks to
[01:17:28.000 --> 01:17:33.120]   the contextual awareness they're showing off that you're going to be able to tie now,
[01:17:33.120 --> 01:17:37.840]   and Super Chat was cool because you tied the real world to what was happening online
[01:17:37.840 --> 01:17:39.840]   in the virtual world.
[01:17:39.840 --> 01:17:40.840]   That's awesome.
[01:17:40.840 --> 01:17:46.160]   Yeah, actually, Jerry Ellsworth, who created Cast AR and is a good friend of the network.
[01:17:46.160 --> 01:17:47.160]   I love Jerry.
[01:17:47.160 --> 01:17:48.360]   She's a pinball wizard.
[01:17:48.360 --> 01:17:52.560]   She would do live streaming from her laboratory while she was soldering and doing stuff.
[01:17:52.560 --> 01:17:57.240]   And she'd set it up so that people in the chat room could make things happen in her
[01:17:57.240 --> 01:17:58.960]   physical space.
[01:17:58.960 --> 01:18:05.800]   Like the thing you want, Stacey, the punching bag that comes out and hits for the boxing
[01:18:05.800 --> 01:18:07.000]   glove that comes out.
[01:18:07.000 --> 01:18:08.680]   So that is something they said.
[01:18:08.680 --> 01:18:12.240]   They didn't give any details about how that works, what kind of programming would need
[01:18:12.240 --> 01:18:13.240]   or anything.
[01:18:13.240 --> 01:18:16.560]   But yeah, that's what you're talking about, though, right?
[01:18:16.560 --> 01:18:17.560]   Yeah.
[01:18:17.560 --> 01:18:22.560]   And imagine, I bet at least three people in the chat right now would pay like a dollar
[01:18:22.560 --> 01:18:24.960]   to punch you.
[01:18:24.960 --> 01:18:28.120]   I think we found a new revenue model, Lisa.
[01:18:28.120 --> 01:18:31.840]   Maybe we could just, you know, raise money that way.
[01:18:31.840 --> 01:18:40.600]   I know, I know at least two people who are doing it for you, Stacey.
[01:18:40.600 --> 01:18:41.600]   How much would you pay?
[01:18:41.600 --> 01:18:42.600]   Let me ask the chat room.
[01:18:42.600 --> 01:18:45.040]   How much would you pay?
[01:18:45.040 --> 01:18:46.920]   I did order toots the unicorn.
[01:18:46.920 --> 01:18:47.920]   You're right.
[01:18:47.920 --> 01:18:53.040]   The geeks reminding me I ordered a little unicorn that when money was paid somewhere,
[01:18:53.040 --> 01:18:57.760]   somehow we hadn't figured out, are you tweeted me or whatever, it would fart a rainbow.
[01:18:57.760 --> 01:18:58.760]   Is that close?
[01:18:58.760 --> 01:18:59.760]   I remember that.
[01:18:59.760 --> 01:19:00.760]   That was impressive.
[01:19:00.760 --> 01:19:02.760]   Still hasn't arrived yet.
[01:19:02.760 --> 01:19:03.760]   All right.
[01:19:03.760 --> 01:19:04.760]   Super.
[01:19:04.760 --> 01:19:10.080]   Just an apps, by the way, this wasn't on the stage, but someone in chat just posted
[01:19:10.080 --> 01:19:11.080]   this in.
[01:19:11.080 --> 01:19:12.080]   Who was it?
[01:19:12.080 --> 01:19:14.960]   Scooter X and Android Instant Apps now available to all developers.
[01:19:14.960 --> 01:19:17.720]   They did a trial run in January.
[01:19:17.720 --> 01:19:18.960]   And now they're now they're rolling it out.
[01:19:18.960 --> 01:19:22.840]   So developers has the work to do to make their apps more modular, but they can get
[01:19:22.840 --> 01:19:23.840]   it on that.
[01:19:23.840 --> 01:19:27.240]   Let's wrap this up with the Android because now we really have come to that portion of
[01:19:27.240 --> 01:19:31.760]   the show where they talked about Android, two billion activations.
[01:19:31.760 --> 01:19:37.240]   They announced the preview is finally available today and both both these guys on my left
[01:19:37.240 --> 01:19:41.840]   and right immediately signed up to get Android O and their pixels and and you're using it
[01:19:41.840 --> 01:19:42.840]   now.
[01:19:42.840 --> 01:19:44.120]   Did you get it yet?
[01:19:44.120 --> 01:19:45.120]   I did.
[01:19:45.120 --> 01:19:46.120]   I finally got it.
[01:19:46.120 --> 01:19:47.120]   Yeah.
[01:19:47.120 --> 01:19:49.320]   So this is what's to do with it.
[01:19:49.320 --> 01:19:50.320]   Yeah, exactly.
[01:19:50.320 --> 01:19:51.320]   I got it.
[01:19:51.320 --> 01:19:55.200]   It's going to take me some time to run into the little pieces throughout.
[01:19:55.200 --> 01:19:58.040]   What was some of the things that's all?
[01:19:58.040 --> 01:19:59.040]   What's that?
[01:19:59.040 --> 01:20:01.040]   What was some of the things?
[01:20:01.040 --> 01:20:03.720]   It's supposedly and I'd like you to try this right now.
[01:20:03.720 --> 01:20:10.080]   Boots twice as fast as Android and in fact they divided it into two different particular
[01:20:10.080 --> 01:20:18.120]   areas, one of which is really more of a branding area where they're going to kind of surface
[01:20:18.120 --> 01:20:19.120]   security.
[01:20:19.120 --> 01:20:21.080]   This is called vitals.
[01:20:21.080 --> 01:20:25.000]   They're going to surface security that's already there.
[01:20:25.000 --> 01:20:29.520]   Things like the fact that people and people don't know it that the play store is scanning
[01:20:29.520 --> 01:20:34.160]   your apps regularly to look for malware.
[01:20:34.160 --> 01:20:43.240]   So I long press on an app.
[01:20:43.240 --> 01:20:44.800]   And so I long press.
[01:20:44.800 --> 01:20:49.080]   So if I have a notification from Twitter, I don't have to pull down the notification
[01:20:49.080 --> 01:20:54.560]   and do all the Macgilla, I can long press on the Twitter icon and it will show me all
[01:20:54.560 --> 01:20:55.560]   the notifications there.
[01:20:55.560 --> 01:20:56.560]   Right.
[01:20:56.560 --> 01:20:57.560]   Yeah.
[01:20:57.560 --> 01:20:59.680]   And notification dots was another part of that, right?
[01:20:59.680 --> 01:21:02.520]   Like I can see on my Pinterest app, apparently I have an update on Pinterest.
[01:21:02.520 --> 01:21:03.520]   It's not that much.
[01:21:03.520 --> 01:21:07.280]   I don't think it comes through on the screen at all, but probably not.
[01:21:07.280 --> 01:21:10.720]   But there's a little dot above Pinterest that tells me I have a notification there.
[01:21:10.720 --> 01:21:11.720]   There is.
[01:21:11.720 --> 01:21:14.920]   Tap and hold and I get the notification as a pop-up window.
[01:21:14.920 --> 01:21:15.920]   Yeah.
[01:21:15.920 --> 01:21:18.680]   I'd also like you to try out a new feature.
[01:21:18.680 --> 01:21:25.400]   They're using some of this AI in messages, in text, whether it's email or whatever.
[01:21:25.400 --> 01:21:28.240]   Machine learning, they say, we'll pick out business names.
[01:21:28.240 --> 01:21:32.600]   So normally when you tap on a word, it will select just the word.
[01:21:32.600 --> 01:21:34.600]   Now it'll say, oh, that's part of a business name.
[01:21:34.600 --> 01:21:37.920]   It'll select the whole business name.
[01:21:37.920 --> 01:21:40.120]   And then you can use this tap feature.
[01:21:40.120 --> 01:21:42.280]   So let me look at my hotel last night.
[01:21:42.280 --> 01:21:43.280]   It's curious.
[01:21:43.280 --> 01:21:45.240]   You could, for instance, say I want to navigate there.
[01:21:45.240 --> 01:21:48.400]   It'll also recognize addresses and phone numbers.
[01:21:48.400 --> 01:21:50.640]   Tapping on them will highlight them.
[01:21:50.640 --> 01:21:54.720]   And then if you long press it in the pop-up, for instance, because it says, oh, that's
[01:21:54.720 --> 01:22:00.120]   a business or an address, it'll show you maps instead of just cut and paste.
[01:22:00.120 --> 01:22:02.520]   It'll show you maps is one of the options.
[01:22:02.520 --> 01:22:04.400]   So smart text selection.
[01:22:04.400 --> 01:22:05.400]   Smart text.
[01:22:05.400 --> 01:22:06.400]   Select.
[01:22:06.400 --> 01:22:08.200]   No, the promise is, it's only just trying to open.
[01:22:08.200 --> 01:22:09.660]   Hold on a minute.
[01:22:09.660 --> 01:22:13.640]   They call it a neural network specifically for your phone because all of this is done
[01:22:13.640 --> 01:22:15.000]   apparently on phone.
[01:22:15.000 --> 01:22:16.880]   This is an on server.
[01:22:16.880 --> 01:22:19.520]   Which was a part of their announcement also around TensorFlow.
[01:22:19.520 --> 01:22:24.840]   TensorFlow Lite for on-device kind of neural network processing.
[01:22:24.840 --> 01:22:28.800]   So we're going to see more and more of that happening on the device instead of relying
[01:22:28.800 --> 01:22:29.800]   on the cloud.
[01:22:29.800 --> 01:22:34.120]   They even said that they expect machine learning specific processors in the phone.
[01:22:34.120 --> 01:22:35.120]   Right.
[01:22:35.120 --> 01:22:36.800]   Which I think is a variant would be variant.
[01:22:36.800 --> 01:22:37.800]   Yeah, it's not an example.
[01:22:37.800 --> 01:22:38.800]   It's not an example.
[01:22:38.800 --> 01:22:43.560]   It's an email for something that has a business name or has it maybe try it in text.
[01:22:43.560 --> 01:22:46.240]   There's something for you to play with.
[01:22:46.240 --> 01:22:47.240]   You can just play with.
[01:22:47.240 --> 01:22:49.040]   You can just shrink video if you're watching.
[01:22:49.040 --> 01:22:53.880]   If you're in a video chat and access, you know, other applications.
[01:22:53.880 --> 01:22:55.360]   You just turn the tube as well.
[01:22:55.360 --> 01:22:56.360]   Yeah.
[01:22:56.360 --> 01:22:57.360]   Picture and picture.
[01:22:57.360 --> 01:22:58.360]   Yeah.
[01:22:58.360 --> 01:22:59.360]   Anything else?
[01:22:59.360 --> 01:23:00.360]   What else?
[01:23:00.360 --> 01:23:01.360]   Security hits.
[01:23:01.360 --> 01:23:03.160]   There's one other thing I guess.
[01:23:03.160 --> 01:23:04.160]   Oh.
[01:23:04.160 --> 01:23:05.160]   Yeah.
[01:23:05.160 --> 01:23:07.720]   So at the end, Sundar came back on.
[01:23:07.720 --> 01:23:08.720]   Oh, no, no, no.
[01:23:08.720 --> 01:23:09.720]   Let's see.
[01:23:09.720 --> 01:23:10.720]   Okay.
[01:23:10.720 --> 01:23:11.720]   That's what I saw.
[01:23:11.720 --> 01:23:12.720]   I saw it says the whole thing.
[01:23:12.720 --> 01:23:16.160]   The whole Android as it were.
[01:23:16.160 --> 01:23:19.200]   They say better battery life because they're finally doing something Apple did from day
[01:23:19.200 --> 01:23:24.680]   one, which is there limit how long you could sit in the background.
[01:23:24.680 --> 01:23:26.520]   And that's a really important component.
[01:23:26.520 --> 01:23:32.080]   I feel like I've been very actually envious of that aspect of iOS and how iOS operates.
[01:23:32.080 --> 01:23:38.080]   Obviously, they're two completely different ethos as far as how Android is very open and
[01:23:38.080 --> 01:23:43.920]   giving to developers access to the underlying architecture versus iOS, which locks it down.
[01:23:43.920 --> 01:23:48.000]   But then you get, you know, kind of better background management on iOS as a result.
[01:23:48.000 --> 01:23:53.000]   The apps can't go rogue, let's say, and kind of terrorize, you know, your device performance
[01:23:53.000 --> 01:23:54.000]   in the background.
[01:23:54.000 --> 01:23:58.840]   So apparently, Android, I was going to make some big steps to kind of rein in the stuff
[01:23:58.840 --> 01:24:00.480]   that's happening in the background.
[01:24:00.480 --> 01:24:01.480]   They say that.
[01:24:01.480 --> 01:24:02.480]   Give it a little bit less.
[01:24:02.480 --> 01:24:04.720]   They say that every, but it is.
[01:24:04.720 --> 01:24:05.720]   It does.
[01:24:05.720 --> 01:24:07.560]   Incredibly, it does get better and better.
[01:24:07.560 --> 01:24:08.560]   Yeah.
[01:24:08.560 --> 01:24:11.480]   And it is better, I have to say, does work.
[01:24:11.480 --> 01:24:16.880]   If you want Android, oh, you can get it at android.com/beta, but you'll have to have
[01:24:16.880 --> 01:24:19.200]   a pixel, I think, maybe a Nexus.
[01:24:19.200 --> 01:24:24.440]   Some of the Nexus devices, the Pixel C you can do it on.
[01:24:24.440 --> 01:24:26.880]   So the Nexus 5X6P pixel.
[01:24:26.880 --> 01:24:29.400]   Oh, it's a little bit worse than my devices.
[01:24:29.400 --> 01:24:30.400]   Basically, yes.
[01:24:30.400 --> 01:24:31.400]   It's a Google device.
[01:24:31.400 --> 01:24:32.400]   Yeah.
[01:24:32.400 --> 01:24:33.400]   Right.
[01:24:33.400 --> 01:24:34.400]   At least for now.
[01:24:34.400 --> 01:24:37.480]   This is the first time they've done a public beta, I guess.
[01:24:37.480 --> 01:24:44.240]   Google is adding Kotlin as an equal peer to Java as a programming language for Android.
[01:24:44.240 --> 01:24:45.240]   Must be a good thing.
[01:24:45.240 --> 01:24:46.240]   The developers cheered.
[01:24:46.240 --> 01:24:51.120]   Kotlin is a fairly young language that was created by JetBrains, the folks that do the
[01:24:51.120 --> 01:24:57.760]   IDE that Google bought and later made the very nice IDE, I think, for Android development.
[01:24:57.760 --> 01:24:59.080]   They've been working on it for a while.
[01:24:59.080 --> 01:25:04.800]   It's a strongly typed, statically typed language, kind of like Scala, they say.
[01:25:04.800 --> 01:25:08.840]   And I think developers were mostly cheering because they just hate Java.
[01:25:08.840 --> 01:25:09.840]   And are excited.
[01:25:09.840 --> 01:25:12.120]   Ooh, look, you got the mini window on there.
[01:25:12.120 --> 01:25:13.120]   Show the folks at home.
[01:25:13.120 --> 01:25:14.120]   Show the folks.
[01:25:14.120 --> 01:25:15.120]   So I got the picture in pictures.
[01:25:15.120 --> 01:25:17.160]   So that is a picture in pictures.
[01:25:17.160 --> 01:25:18.160]   Yeah, down there.
[01:25:18.160 --> 01:25:19.520]   And I can think I can move it around.
[01:25:19.520 --> 01:25:21.560]   Yeah, I can move it around the screen.
[01:25:21.560 --> 01:25:23.800]   I can't tell my directions.
[01:25:23.800 --> 01:25:24.800]   Right.
[01:25:24.800 --> 01:25:25.800]   So a picture in pictures.
[01:25:25.800 --> 01:25:30.520]   So I can be doing other things while the video is open and let it roll, which Facebook
[01:25:30.520 --> 01:25:32.320]   talked about as well, which is a problem.
[01:25:32.320 --> 01:25:38.800]   You don't want us to turn into a TV, you want to be able to do multiple things.
[01:25:38.800 --> 01:25:40.480]   And two big screens, I don't think it's sufficient.
[01:25:40.480 --> 01:25:43.360]   So this way it's a see it there, see what's going on.
[01:25:43.360 --> 01:25:49.680]   And that's the first time that the notion of video on Twitter makes any sense to me.
[01:25:49.680 --> 01:25:56.480]   So you don't want to turn it into a TV, but you don't mind turning it into a PC.
[01:25:56.480 --> 01:26:04.080]   Next, they talked about VR, AR and Tango.
[01:26:04.080 --> 01:26:05.360]   Tango is getting smaller and smaller.
[01:26:05.360 --> 01:26:08.880]   We already mentioned the dedicated VR headset.
[01:26:08.880 --> 01:26:10.480]   I think that's potentially very interesting.
[01:26:10.480 --> 01:26:11.480]   We'll learn more about that.
[01:26:11.480 --> 01:26:15.400]   Tell us a mention with Tango that I've been waiting for.
[01:26:15.400 --> 01:26:17.360]   I'm surprised we haven't seen more application with this.
[01:26:17.360 --> 01:26:24.160]   That God said this could be for the scene impaired to be able to navigate an environment
[01:26:24.160 --> 01:26:29.280]   and have you give audio or other cues about how to go around or just be key to me.
[01:26:29.280 --> 01:26:32.160]   And I think we haven't seen enough of that yet.
[01:26:32.160 --> 01:26:36.560]   And then Sundar came back.
[01:26:36.560 --> 01:26:37.600]   We missed one.
[01:26:37.600 --> 01:26:39.760]   We missed a pretty big one actually.
[01:26:39.760 --> 01:26:42.000]   Was Android Go.
[01:26:42.000 --> 01:26:43.680]   We talked about it earlier briefly.
[01:26:43.680 --> 01:26:44.200]   Yeah.
[01:26:44.200 --> 01:26:45.440]   What is Android Go?
[01:26:45.440 --> 01:26:47.200]   Android Go, inexpensive version.
[01:26:47.200 --> 01:26:51.360]   Yeah, we mentioned it briefly, but just to dive into it a little bit more, it's not
[01:26:51.360 --> 01:26:53.200]   a separate version of Android.
[01:26:53.200 --> 01:26:59.920]   It's basically a minimized, let's say, version of Android that's tailored to devices that
[01:26:59.920 --> 01:27:02.600]   have a gig of RAM or less.
[01:27:02.600 --> 01:27:06.240]   So when you're talking about, well, I guess we did kind of dive into this somewhat earlier,
[01:27:06.240 --> 01:27:07.240]   didn't we?
[01:27:07.240 --> 01:27:08.240]   The kind of emerging markets.
[01:27:08.240 --> 01:27:09.240]   Yeah.
[01:27:09.240 --> 01:27:10.240]   The emerging markets.
[01:27:10.240 --> 01:27:11.240]   But there's a lot of things they're going to do.
[01:27:11.240 --> 01:27:13.240]   They're going to limit the apps that you can use.
[01:27:13.240 --> 01:27:14.480]   The apps will in themselves.
[01:27:14.480 --> 01:27:15.480]   Right.
[01:27:15.480 --> 01:27:16.480]   Developers will be able to.
[01:27:16.480 --> 01:27:18.320]   Creating their own versions of the apps.
[01:27:18.320 --> 01:27:19.320]   Yeah.
[01:27:19.320 --> 01:27:20.320]   Yeah.
[01:27:20.320 --> 01:27:21.320]   YouTube Go.
[01:27:21.320 --> 01:27:22.320]   I think that's a great thing.
[01:27:22.320 --> 01:27:25.760]   We did mention it with 2 billion active users now.
[01:27:25.760 --> 01:27:26.960]   This is the next billion.
[01:27:26.960 --> 01:27:27.960]   Yeah.
[01:27:27.960 --> 01:27:28.960]   The next billion.
[01:27:28.960 --> 01:27:29.960]   Active users.
[01:27:29.960 --> 01:27:30.960]   Yeah.
[01:27:30.960 --> 01:27:31.960]   That's right.
[01:27:31.960 --> 01:27:32.960]   My apologies.
[01:27:32.960 --> 01:27:33.960]   I forgot about that.
[01:27:33.960 --> 01:27:37.320]   Google, I should remind people this is no longer a everything under the Sun keynote because
[01:27:37.320 --> 01:27:39.840]   Google is now just a part of Alphabet.
[01:27:39.840 --> 01:27:42.240]   This isn't an Alphabet keynote.
[01:27:42.240 --> 01:27:49.040]   So for people who are interested in flying cars, blimps, contact lenses that can read
[01:27:49.040 --> 01:27:52.760]   your future that would be for a keynote that doesn't exist.
[01:27:52.760 --> 01:27:53.760]   It exists.
[01:27:53.760 --> 01:27:54.760]   Maybe it should.
[01:27:54.760 --> 01:27:57.160]   I kind of missed the days of so good.
[01:27:57.160 --> 01:27:58.160]   Yeah.
[01:27:58.160 --> 01:27:59.160]   Because I think shooting in.
[01:27:59.160 --> 01:28:00.160]   What was talking about?
[01:28:00.160 --> 01:28:01.160]   I didn't write this.
[01:28:01.160 --> 01:28:03.200]   I was trying to write this tweet and I couldn't have the same.
[01:28:03.200 --> 01:28:08.520]   But back in the wonderful days of Bell Labs, when we were kids and we get the movie projector
[01:28:08.520 --> 01:28:12.240]   was wheeled in the classroom and you see this guy with the centurion voice telling you
[01:28:12.240 --> 01:28:16.760]   about the wonders of the universe that Bell Labs was the closest thing we had besides
[01:28:16.760 --> 01:28:20.360]   universities to a pure science operation.
[01:28:20.360 --> 01:28:23.080]   Now Google is the closest thing we have to that I think.
[01:28:23.080 --> 01:28:26.360]   And there's things that are happening in this company that I think are important to get
[01:28:26.360 --> 01:28:32.200]   excitement about science, excitement about technology, excitement about what can happen.
[01:28:32.200 --> 01:28:35.720]   Now, then you get the Alaria building his own island where he'll move to, which will be
[01:28:35.720 --> 01:28:40.480]   happening in Toronto very soon.
[01:28:40.480 --> 01:28:44.440]   But I do wish there was this opportunity for Google to G-Wiz this little more.
[01:28:44.440 --> 01:28:47.400]   Maybe they don't want you right now because I think this goes to the very final point.
[01:28:47.400 --> 01:28:49.240]   Are we there now?
[01:28:49.240 --> 01:28:51.080]   That Sundar came on about jobs.
[01:28:51.080 --> 01:28:52.080]   Sure.
[01:28:52.080 --> 01:28:55.960]   I think that was critically important moment because basically it was the other point we
[01:28:55.960 --> 01:29:01.520]   mentioned earlier about how Google was being sensitive around these other issues of corporate
[01:29:01.520 --> 01:29:02.520]   responsibility.
[01:29:02.520 --> 01:29:06.000]   Well, the final one was jobs where I think what's going to happen is that the unemployed
[01:29:06.000 --> 01:29:10.120]   in America are going to wake up one day and realize that it's not strangers from across
[01:29:10.120 --> 01:29:15.040]   borders who are causing their change in their job situation, it's technology.
[01:29:15.040 --> 01:29:18.640]   And as Eric Schmidt has always said, when it comes to wanting to fight technology, who
[01:29:18.640 --> 01:29:19.640]   do people go after?
[01:29:19.640 --> 01:29:22.040]   The largest guy in the field and that's Google.
[01:29:22.040 --> 01:29:26.400]   So I think what we saw was a preemptive strike by Google saying that we're going to, they're
[01:29:26.400 --> 01:29:33.400]   going to come up with functionality to help you find the right jobs down to a far more
[01:29:33.400 --> 01:29:38.920]   granular level so that you can even check on jobs that are within end minutes of commuting
[01:29:38.920 --> 01:29:41.360]   time for you and things like that.
[01:29:41.360 --> 01:29:44.600]   And that's Google trying to help people with jobs.
[01:29:44.600 --> 01:29:46.480]   Now, does it go far enough?
[01:29:46.480 --> 01:29:49.440]   No, what we have to do is have more education as a society.
[01:29:49.440 --> 01:29:52.880]   We have to do a far great deal more.
[01:29:52.880 --> 01:29:57.200]   But I think it is an acknowledgement that technology in general and Google in particular
[01:29:57.200 --> 01:29:58.920]   will have an impact on jobs.
[01:29:58.920 --> 01:30:03.360]   It also struck me and people have already yelled at me in my Twitter feed about this.
[01:30:03.360 --> 01:30:04.880]   That's what newspapers should have done years ago.
[01:30:04.880 --> 01:30:05.880]   And people said, well, newspapers did.
[01:30:05.880 --> 01:30:06.880]   They had classifies.
[01:30:06.880 --> 01:30:07.880]   Yeah.
[01:30:07.880 --> 01:30:11.200]   And they charged for them and they really didn't see that as a service.
[01:30:11.200 --> 01:30:17.000]   Google now sees this idea of getting people jobs as a service that they need to help accomplish
[01:30:17.000 --> 01:30:18.920]   for people as part of their corporate responsibility.
[01:30:18.920 --> 01:30:21.600]   So I think it was a big deal at the end.
[01:30:21.600 --> 01:30:22.600]   Good point.
[01:30:22.600 --> 01:30:26.080]   And the fact that it came from Sundar said this is a corporate responsibility.
[01:30:26.080 --> 01:30:27.080]   Good point.
[01:30:27.080 --> 01:30:28.680]   Well, let's wrap this up.
[01:30:28.680 --> 01:30:30.640]   Stacey, what was the high point?
[01:30:30.640 --> 01:30:32.160]   Was there anything we've left out?
[01:30:32.160 --> 01:30:34.600]   Something you thought was really cool?
[01:30:34.600 --> 01:30:36.160]   No.
[01:30:36.160 --> 01:30:37.160]   We touched on that.
[01:30:37.160 --> 01:30:39.000]   Thank you very much for joining us.
[01:30:39.000 --> 01:30:41.840]   We'll get to our picks in a second.
[01:30:41.840 --> 01:30:43.840]   How about you, Jeff?
[01:30:43.840 --> 01:30:44.840]   Anything?
[01:30:44.840 --> 01:30:46.760]   No, I think we've very much.
[01:30:46.760 --> 01:30:50.400]   I think we left out one thing, which was a big moment.
[01:30:50.400 --> 01:30:52.800]   Scooter X pointed us out in our chatroom.
[01:30:52.800 --> 01:30:56.880]   Unfortunately, I don't know if this is how this will lurk or how it'll work.
[01:30:56.880 --> 01:31:02.560]   But Google Photos is going to add automatic obstruction removal to the Pixel camera.
[01:31:02.560 --> 01:31:08.200]   And one of the things they showed was a picture all of us as parents have taken of our kid
[01:31:08.200 --> 01:31:12.360]   in a T-ball batting shot through a chain link fence.
[01:31:12.360 --> 01:31:15.240]   And there's a chain link fence in the way of the shot.
[01:31:15.240 --> 01:31:19.240]   And magically, he pushed a button and the chain link fence just disappeared.
[01:31:19.240 --> 01:31:20.240]   And the kid was--
[01:31:20.240 --> 01:31:22.240]   This is where a journalist freak out.
[01:31:22.240 --> 01:31:23.240]   Right.
[01:31:23.240 --> 01:31:25.440]   This is-- you worry about fake news?
[01:31:25.440 --> 01:31:26.440]   Yeah.
[01:31:26.440 --> 01:31:31.120]   This AI has the potential to create fake news that is unfreaking believable.
[01:31:31.120 --> 01:31:33.120]   Or believable, actually.
[01:31:33.120 --> 01:31:34.120]   So--
[01:31:34.120 --> 01:31:36.120]   That's the problem, isn't it?
[01:31:36.120 --> 01:31:37.120]   Yeah.
[01:31:37.120 --> 01:31:38.120]   Yeah.
[01:31:38.120 --> 01:31:44.640]   This particular photo, like object removal from Google Photos is interesting.
[01:31:44.640 --> 01:31:48.160]   It reminded me immediately of-- I found a video here.
[01:31:48.160 --> 01:31:49.160]   Maybe I can get it to Carson.
[01:31:49.160 --> 01:31:55.640]   It was a SIGGRAPH 2015 paper where they showed how you do obstruction-free photography.
[01:31:55.640 --> 01:31:56.640]   And I think what it is--
[01:31:56.640 --> 01:31:57.640]   Oh, I remember that.
[01:31:57.640 --> 01:31:58.640]   Yeah.
[01:31:58.640 --> 01:32:00.600]   It's less about taking a single picture.
[01:32:00.600 --> 01:32:04.840]   It's more about taking potentially a series or a movement.
[01:32:04.840 --> 01:32:09.560]   And because of that, it can match the patterns that exist behind the object and track the
[01:32:09.560 --> 01:32:14.240]   object in front and then fill in the pieces and give you a single image based on those
[01:32:14.240 --> 01:32:15.240]   images.
[01:32:15.240 --> 01:32:16.240]   So-- but that's super cool.
[01:32:16.240 --> 01:32:18.880]   It's a really cool kind of feature to roll in.
[01:32:18.880 --> 01:32:19.880]   There it is.
[01:32:19.880 --> 01:32:20.880]   Carson's showing it right now.
[01:32:20.880 --> 01:32:24.920]   I don't-- you know, we'll have to wait and see how well that works.
[01:32:24.920 --> 01:32:26.880]   That's a lot to promise.
[01:32:26.880 --> 01:32:28.920]   That got an ooh from the audience.
[01:32:28.920 --> 01:32:30.880]   Like nobody exists.
[01:32:30.880 --> 01:32:35.360]   Well, we know how hard that is to do, particularly because the things that are behind the chain
[01:32:35.360 --> 01:32:38.240]   link things don't exist for the camera.
[01:32:38.240 --> 01:32:44.280]   And so you have to replace pieces of the picture by extrapolating what's there.
[01:32:44.280 --> 01:32:48.480]   And what does it-- does it not just extrapolate the pixels, but does it know that's a person?
[01:32:48.480 --> 01:32:50.960]   And this is where a person's body goes.
[01:32:50.960 --> 01:32:55.240]   I think it's more like that what you were talking about, Jason, which is it's going to
[01:32:55.240 --> 01:33:01.400]   gather information over a longer period of time and the micro movements in the background
[01:33:01.400 --> 01:33:04.120]   and then the kid will help it fill in the blanks.
[01:33:04.120 --> 01:33:08.360]   But it's still-- it's not an easy thing to do.
[01:33:08.360 --> 01:33:09.360]   And you know what?
[01:33:09.360 --> 01:33:12.320]   If they can do it, that just shows you how powerful their AI is.
[01:33:12.320 --> 01:33:16.920]   Stacey, you had a pick of the week before we let you go?
[01:33:16.920 --> 01:33:19.560]   It's really-- it was not a pick of the week exactly.
[01:33:19.560 --> 01:33:21.120]   It is something I found.
[01:33:21.120 --> 01:33:23.480]   Do you want to go to an ad or do you-- I still have time?
[01:33:23.480 --> 01:33:24.480]   No.
[01:33:24.480 --> 01:33:25.480]   Do it.
[01:33:25.480 --> 01:33:26.480]   Do it to it.
[01:33:26.480 --> 01:33:27.480]   We don't need any more ads.
[01:33:27.480 --> 01:33:28.480]   We've had enough ads.
[01:33:28.480 --> 01:33:30.480]   It's an all Google show today.
[01:33:30.480 --> 01:33:32.480]   Look what I--
[01:33:32.480 --> 01:33:33.480]   A speaking spell.
[01:33:33.480 --> 01:33:34.480]   Where'd you get a speaking spell?
[01:33:34.480 --> 01:33:39.160]   We found it in my husband's attic, but the best part, ready?
[01:33:39.160 --> 01:33:40.160]   It still works.
[01:33:40.160 --> 01:33:41.160]   Wow.
[01:33:41.160 --> 01:33:42.160]   That--
[01:33:42.160 --> 01:33:45.640]   Yeah, you can rewire that and build the first prototype compact computer.
[01:33:45.640 --> 01:33:47.640]   Oh, no, wait a minute.
[01:33:47.640 --> 01:33:49.440]   That already happened in 1982.
[01:33:49.440 --> 01:33:54.440]   I like-- I was really excited because this is like my childhood right here.
[01:33:54.440 --> 01:33:56.440]   And the fact that it worked was awesome.
[01:33:56.440 --> 01:34:01.480]   Are you going to give it to your daughter and see if she gets smarter?
[01:34:01.480 --> 01:34:03.880]   She's already been playing with it.
[01:34:03.880 --> 01:34:05.600]   She just makes it say, "Are you OK?"
[01:34:05.600 --> 01:34:07.600]   A bunch.
[01:34:07.600 --> 01:34:10.640]   It's just-- it must-- she might think it's so primitive.
[01:34:10.640 --> 01:34:12.720]   She's like, "This is lame."
[01:34:12.720 --> 01:34:16.560]   Mom, how could you think that was cool?
[01:34:16.560 --> 01:34:17.560]   That's so funny.
[01:34:17.560 --> 01:34:18.560]   But you know what?
[01:34:18.560 --> 01:34:23.920]   Stacey, you're a smart cookie and we can thank Texas Instruments and speak and spell for
[01:34:23.920 --> 01:34:24.920]   that.
[01:34:24.920 --> 01:34:26.720]   Anything you'd like to leave us with?
[01:34:26.720 --> 01:34:27.960]   No, I have no number.
[01:34:27.960 --> 01:34:30.480]   We went through all the numbers possible.
[01:34:30.480 --> 01:34:32.480]   They were all in the billions.
[01:34:32.480 --> 01:34:33.640]   They were all billions.
[01:34:33.640 --> 01:34:37.960]   I just want to reiterate thanks to the Google staff for making this possible.
[01:34:37.960 --> 01:34:38.960]   And it's good.
[01:34:38.960 --> 01:34:39.960]   Great.
[01:34:39.960 --> 01:34:40.960]   I just want to say this.
[01:34:40.960 --> 01:34:42.920]   So there have been three Googlers sitting in the room.
[01:34:42.920 --> 01:34:45.960]   And it's interesting to do this show because what I wrote a book called, "What Would Google
[01:34:45.960 --> 01:34:46.960]   Do?"
[01:34:46.960 --> 01:34:47.960]   And I came to do a talk here.
[01:34:47.960 --> 01:34:51.520]   I thought, "Uh-oh, they're all going to know how full of crap I was."
[01:34:51.520 --> 01:34:54.200]   And we could say things here as we always do every week.
[01:34:54.200 --> 01:34:57.320]   We had Google's looking at us and I was kind of watching your eyeballs thinking, "Well,
[01:34:57.320 --> 01:34:59.320]   if they roll like, like, "Anner's a Cooper."
[01:34:59.320 --> 01:35:00.320]   I'm not right.
[01:35:00.320 --> 01:35:04.120]   If you pull an "Anner's a Cooper," I think, "Oh, I got that wrong."
[01:35:04.120 --> 01:35:05.480]   But they're good actors.
[01:35:05.480 --> 01:35:07.080]   And so, but I'm grateful.
[01:35:07.080 --> 01:35:08.080]   Thank you.
[01:35:08.080 --> 01:35:09.080]   That's nice.
[01:35:09.080 --> 01:35:10.080]   How about you?
[01:35:10.080 --> 01:35:11.080]   Anything you want to say, Jason?
[01:35:11.080 --> 01:35:15.280]   Well, I will go ahead and give a quick app to check out.
[01:35:15.280 --> 01:35:17.600]   If you don't mind, it's totally Iowa-related.
[01:35:17.600 --> 01:35:22.280]   I have not checked it out myself yet, but today, right when Iowa was starting, Google
[01:35:22.280 --> 01:35:25.280]   released a new app to the Play Store called Audio Factory.
[01:35:25.280 --> 01:35:31.360]   And apparently, it's meant for Daydream and it's a representation of kind of some of Android
[01:35:31.360 --> 01:35:38.400]   O's new developments around spatial audio experiments or whatever.
[01:35:38.400 --> 01:35:39.400]   So I don't know.
[01:35:39.400 --> 01:35:40.400]   It's worth checking out.
[01:35:40.400 --> 01:35:42.840]   I'm curious to dive in there and see it kind of sets up some scenery.
[01:35:42.840 --> 01:35:47.600]   I have to imagine there's a little bit of kind of three-dimensional, you know, surround
[01:35:47.600 --> 01:35:49.640]   sound sort of thing when you've got the headphones on.
[01:35:49.640 --> 01:35:53.040]   I honestly don't know what it's all about, but it looks very intriguing.
[01:35:53.040 --> 01:35:57.760]   And speaking of which, I completely dropped the fact that at the beginning of the event
[01:35:57.760 --> 01:36:01.800]   before Cinder Pocha, I told you on stage, we had two GJs.
[01:36:01.800 --> 01:36:03.800]   I don't know if there's other things.
[01:36:03.800 --> 01:36:04.800]   Does anybody left your name?
[01:36:04.800 --> 01:36:08.640]   Yeah, performing on the left and the right using little tap boxes and little percussive
[01:36:08.640 --> 01:36:13.320]   devices attached to their Android phone.
[01:36:13.320 --> 01:36:19.560]   And I think that that was Google's very subtle way of telling musicians it's okay to come
[01:36:19.560 --> 01:36:20.560]   into the water.
[01:36:20.560 --> 01:36:24.360]   Musicians have, I think I'm right, Jason, you correct me on this, but musicians have
[01:36:24.360 --> 01:36:29.080]   eschewed Android because it's famous for, and this is a problem, really in the Linux kernel,
[01:36:29.080 --> 01:36:36.880]   a 10 millisecond latency in the audio channels that no one has ever been able to get rid
[01:36:36.880 --> 01:36:37.880]   of.
[01:36:37.880 --> 01:36:40.800]   And the any latency really impairs live performance.
[01:36:40.800 --> 01:36:41.800]   Absolutely.
[01:36:41.800 --> 01:36:46.680]   It's one of the reasons there have been a dearth of musical performance apps on Android, while
[01:36:46.680 --> 01:36:50.080]   there are many, many on iOS.
[01:36:50.080 --> 01:36:51.840]   And it's not just the 10 milliseconds.
[01:36:51.840 --> 01:36:53.640]   It adds, adds, adds, adds, and adds.
[01:36:53.640 --> 01:36:57.920]   And eventually you can get a significant a half second delay or more, which really makes
[01:36:57.920 --> 01:36:59.200]   it unusable.
[01:36:59.200 --> 01:37:04.360]   Those guys were playing in sync in time on different places, different places on different
[01:37:04.360 --> 01:37:07.320]   Android phones with no appreciable latency.
[01:37:07.320 --> 01:37:11.840]   And I have a feeling that that has, that there was a reason why they put that up on the stage.
[01:37:11.840 --> 01:37:14.760]   They had mentioned that they thought that they had solved this with the Android O.
[01:37:14.760 --> 01:37:17.600]   So maybe that's part of what you should try out.
[01:37:17.600 --> 01:37:18.600]   Yeah, absolutely.
[01:37:18.600 --> 01:37:23.080]   You know, they're going to have the space here to go in and play around with it.
[01:37:23.080 --> 01:37:24.240]   They did last year.
[01:37:24.240 --> 01:37:27.880]   They had, they had kind of a dedicated music production area to kind of play around with
[01:37:27.880 --> 01:37:28.880]   it.
[01:37:28.880 --> 01:37:32.560]   And they really have touted at least as one of the, one of the features of O is improvements
[01:37:32.560 --> 01:37:36.000]   around professional audio and getting that latency down.
[01:37:36.000 --> 01:37:40.400]   So, you know, I don't, I just don't wonder if it's a little too little too late because
[01:37:40.400 --> 01:37:44.920]   a lot of the musicians have just kind of gone full board on iOS at this point.
[01:37:44.920 --> 01:37:45.920]   But I'd rather have it than not.
[01:37:45.920 --> 01:37:47.480]   And I'm curious to see what it's all about.
[01:37:47.480 --> 01:37:49.480]   Have you got to know who and everybody else?
[01:37:49.480 --> 01:37:51.600]   Did you sign up for O?
[01:37:51.600 --> 01:37:54.680]   I did not because I'm going to travel with my Pixel.
[01:37:54.680 --> 01:37:57.880]   I use Google fly and it's going to be a great traveling phone for me.
[01:37:57.880 --> 01:38:00.800]   And I don't want to mess it up right before we hit the road.
[01:38:00.800 --> 01:38:01.800]   Yeah.
[01:38:01.800 --> 01:38:03.640]   It's time to say goodbye.
[01:38:03.640 --> 01:38:05.080]   Thank you everybody for being here.
[01:38:05.080 --> 01:38:09.520]   Stacey Higginbotham is at Stacey on IOT.com.
[01:38:09.520 --> 01:38:11.160]   Also IOT podcast.com.
[01:38:11.160 --> 01:38:13.920]   You'll find her on Twitter at Gigastacey.
[01:38:13.920 --> 01:38:15.680]   You really worked hard today.
[01:38:15.680 --> 01:38:16.680]   Harder than me.
[01:38:16.680 --> 01:38:19.040]   So thank you for being here both times.
[01:38:19.040 --> 01:38:21.120]   In fact, look at yourself a massage.
[01:38:21.120 --> 01:38:22.120]   Yeah.
[01:38:22.120 --> 01:38:25.560]   And it's, it's, charge it to Uncle Leo.
[01:38:25.560 --> 01:38:28.440]   Like it's, it's my mother's day present.
[01:38:28.440 --> 01:38:29.640]   Nice.
[01:38:29.640 --> 01:38:31.160]   Have a great time.
[01:38:31.160 --> 01:38:33.160]   Thank you very much for coming all this way.
[01:38:33.160 --> 01:38:34.160]   Oh, thank you.
[01:38:34.160 --> 01:38:35.160]   Not for the show.
[01:38:35.160 --> 01:38:36.960]   I know, but I, it's always nice to see you.
[01:38:36.960 --> 01:38:40.080]   No, no, no, no, it's always, it's always the part of the reason to be out here.
[01:38:40.080 --> 01:38:43.960]   Professor of Journalism at the City University of New York, the blogger at Buzzmachine.com
[01:38:43.960 --> 01:38:48.360]   and the other great books like what would Google do, public parts.
[01:38:48.360 --> 01:38:51.840]   And should I mention keep sparing gifts, you always, you always laugh.
[01:38:51.840 --> 01:38:54.920]   I got to do the show without crossing a bridge.
[01:38:54.920 --> 01:38:55.920]   There you go.
[01:38:55.920 --> 01:38:56.920]   It's happy.
[01:38:56.920 --> 01:38:57.920]   There you go.
[01:38:57.920 --> 01:38:58.920]   How nice is that?
[01:38:58.920 --> 01:38:59.920]   You're right.
[01:38:59.920 --> 01:39:00.920]   Just go down from the airport.
[01:39:00.920 --> 01:39:01.920]   You did cross the peninsula.
[01:39:01.920 --> 01:39:02.920]   Is that okay?
[01:39:02.920 --> 01:39:03.920]   You don't mind.
[01:39:03.920 --> 01:39:04.920]   It's just bridges.
[01:39:04.920 --> 01:39:05.920]   Okay.
[01:39:05.920 --> 01:39:06.920]   Bridges that fall down.
[01:39:06.920 --> 01:39:13.200]   We'll see a lot more of Jason Howell.
[01:39:13.200 --> 01:39:14.920]   Of course, Monday through Friday, 4 p.m.
[01:39:14.920 --> 01:39:22.320]   Not today, but most of the time, 4 p.m. Pacific, 7 p.m. Eastern time, 2300 UTC on TNT.
[01:39:22.320 --> 01:39:27.520]   And every Tuesday, 5 p.m. Pacific, 8 p.m. Eastern time on all about Android and reminder.
[01:39:27.520 --> 01:39:29.960]   Once again, something very special on Tuesday.
[01:39:29.960 --> 01:39:33.720]   We won't say what, but he's got some big interviews here at Google I/O and you're going to see
[01:39:33.720 --> 01:39:36.280]   the results of those interviews.
[01:39:36.280 --> 01:39:37.280]   I want to say it.
[01:39:37.280 --> 01:39:39.960]   I want to tell you, but I'm not going to.
[01:39:39.960 --> 01:39:42.080]   You're such a tease, Jason Howell.
[01:39:42.080 --> 01:39:44.560]   Such a tease.
[01:39:44.560 --> 01:39:47.520]   We're going to be at the Maker Fair on Friday.
[01:39:47.520 --> 01:39:52.600]   If you're in the Bay Area, the San Mateo Maker Fair, we will be Friday afternoon from
[01:39:52.600 --> 01:39:54.600]   one to five.
[01:39:54.600 --> 01:39:56.440]   Father Robert Ballisar and I will be down there.
[01:39:56.440 --> 01:39:58.600]   We're going to bring our cameras right, Colleen.
[01:39:58.600 --> 01:40:02.760]   So if people have some projects, they want to show us, bring them on by.
[01:40:02.760 --> 01:40:08.520]   No projects too small or silly and we'd love to see it and meet you and say hello.
[01:40:08.520 --> 01:40:14.520]   So come by one to five Friday afternoon if you're out in San Mateo, California, for the,
[01:40:14.520 --> 01:40:16.600]   that's the mothership of Maker Fair.
[01:40:16.600 --> 01:40:17.680]   I'm Leo LaPorte.
[01:40:17.680 --> 01:40:22.480]   We thank you so much for joining us and we will see you next time on This Week in Google.
[01:40:22.480 --> 01:40:22.680]   Bye-bye.
[01:40:22.680 --> 01:40:32.680]   [MUSIC]

