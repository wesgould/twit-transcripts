;FFMETADATA1
title=Blame it on the Tesla
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-01-19
track=699
language=English
genre=Podcast
comment=Google to SCOTUS, Google Images
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.240]   It's time for twig this week in Google Stasis here Jeff's here ants here
[00:00:06.240 --> 00:00:09.560]   We got a whole bunch of interested things to talk about
[00:00:09.560 --> 00:00:13.560]   Including Stasis encounter with offense
[00:00:13.560 --> 00:00:16.640]   She says
[00:00:16.640 --> 00:00:22.600]   Blamed it on the Tesla. We'll also talk about chat GPT and some of the interesting ways
[00:00:22.600 --> 00:00:27.920]   It's being used and of course the debate over algorithms and section 230
[00:00:28.360 --> 00:00:33.400]   It's it's up to the Supreme Court now all that more coming up on twig
[00:00:33.400 --> 00:00:41.800]   Once again time for the Twit audience survey the annual survey helps us understand you so we can make your listening experience
[00:00:41.800 --> 00:00:46.880]   Even better and only takes a couple of minutes, but it sure helps us out a lot completely optional
[00:00:46.880 --> 00:00:50.360]   But if you could please go to twit.tv/survey
[00:00:50.360 --> 00:00:54.160]   23 that's twit.tv/survey
[00:00:54.160 --> 00:00:56.240]   23
[00:00:56.240 --> 00:00:59.320]   You have till the end of the month, but if you would do me a favor and do it today
[00:00:59.320 --> 00:01:05.280]   I can stop mentioning it twit.tv/survey 23 in thanks in advance
[00:01:05.280 --> 00:01:11.800]   Podcasts you love from people you trust
[00:01:11.800 --> 00:01:14.640]   This is twig
[00:01:14.640 --> 00:01:21.480]   This is twig
[00:01:21.640 --> 00:01:28.960]   Episodes 699 recorded Wednesday January 18th, 2023 blame it on the Tesla
[00:01:28.960 --> 00:01:31.640]   This week in Google is brought to you by
[00:01:31.640 --> 00:01:37.440]   ACI Learning if you love IT pro you'll love ACI learning
[00:01:37.440 --> 00:01:44.280]   ACI learning offers fully customizable training for your team and formats for all types of learners across audit
[00:01:44.280 --> 00:01:51.120]   cyber security and IT from entry level training to putting people on the moon ACI learning has
[00:01:51.360 --> 00:01:55.640]   You covered visit ACI learning calm to learn more
[00:01:55.640 --> 00:02:00.120]   Thanks for listening to this show as an ad supported network
[00:02:00.120 --> 00:02:07.080]   We are always looking for new partners with products and services that will benefit our qualified audience
[00:02:07.080 --> 00:02:13.160]   Are you ready to grow your business reach out to advertise at twit.tv and launch your campaign now?
[00:02:13.160 --> 00:02:21.100]   It's time for twig this week in Google the show we covered the latest news from Google the Fediverse the Facebook
[00:02:21.360 --> 00:02:23.020]   first the Twitter worst
[00:02:23.020 --> 00:02:31.320]   Stacey Higgins bathrooms here Stacey on IOT.com or podcast with Kevin Tofol the IOT podcast hello Stacey
[00:02:31.320 --> 00:02:36.980]   Hello everybody. Hello. Are you freezing cold? You've got a giant scarf. I
[00:02:36.980 --> 00:02:42.240]   Am so cold. I wish I had a giant scarf. I like the look of scarves
[00:02:42.240 --> 00:02:50.020]   You could easily have a scar could actually I have plenty at home. I even have scarves a few from
[00:02:50.700 --> 00:02:55.060]   Faded startups like I have a chip monk scarf
[00:02:55.060 --> 00:02:57.620]   Remer chip, but
[00:02:57.620 --> 00:03:02.700]   Monk this is an infinity scarf so oh
[00:03:02.700 --> 00:03:06.180]   Also known as a circle
[00:03:06.180 --> 00:03:08.340]   Yeah, yeah
[00:03:08.340 --> 00:03:15.580]   Fine well, it's not a Mobius strip. Let's put it that way. Maybe it is I don't we could turn it into what maybe good. Yeah, just twist it
[00:03:15.580 --> 00:03:19.020]   Mr. Jeff off this that's the voice of
[00:03:19.820 --> 00:03:21.820]   Reason Jeff
[00:03:21.820 --> 00:03:26.460]   That's not you know, he's ever called you the voice of reason have they no no one ever does
[00:03:26.460 --> 00:03:30.180]   We've called him the voice of moral panic the voice of passion
[00:03:30.180 --> 00:03:38.180]   No, I was more authentic the Leonard Ta professor for journalistic innovation at the Craig Newmark graduate school of journalism
[00:03:38.180 --> 00:03:42.180]   City University of New York
[00:03:43.220 --> 00:03:46.180]   Require we should get a pigeon choir to sing his name
[00:03:46.180 --> 00:03:49.780]   Yes, we love his pigeons
[00:03:49.780 --> 00:03:53.460]   And
[00:03:53.460 --> 00:03:57.520]   From points west slightly it's aunt Pruitt
[00:03:57.520 --> 00:04:02.020]   Actually points north and he's got his infinity Clemson infinity scarf
[00:04:02.020 --> 00:04:06.420]   Look at that just just trying to be like I like it. He can bought them. Yeah
[00:04:06.420 --> 00:04:10.500]   Never achieve mrs. Hagenbotham status
[00:04:12.100 --> 00:04:19.080]   I noticed if you tell if it's Mrs. Hagenbotham sound like a role Robin Williams might have played in a film version of
[00:04:19.080 --> 00:04:23.460]   Hickenbotham's at large
[00:04:23.460 --> 00:04:28.420]   All right, what is happening first of all the Twitter auctions over sorry
[00:04:28.420 --> 00:04:36.140]   If you just already happen are you good. Did you get the bird? Did you do it? No somebody did $31,000 31
[00:04:36.140 --> 00:04:38.540]   something like that. Yeah
[00:04:39.220 --> 00:04:42.300]   It ended just just about two hours ago
[00:04:42.300 --> 00:04:46.780]   So I didn't realize it was still on I thought it was gonna be over at 10 a.m
[00:04:46.780 --> 00:04:48.780]   That's what the auction site said but
[00:04:48.780 --> 00:04:55.060]   It it kind of it kind of went on they auctioned off really a lot of junk for yonk
[00:04:55.060 --> 00:05:00.220]   For the most part 600 pieces of used furniture a lot of kitchen
[00:05:00.220 --> 00:05:06.140]   How much could I have gotten a coffee machine for yeah, that's what I want to know. Oh, they had some good coffee machines
[00:05:06.140 --> 00:05:13.020]   I think they went early heritage global partners. Oh wait a minute. It looks like there's still some items available
[00:05:13.020 --> 00:05:17.900]   Oh only a minute left to get these six no Bertoya stools
[00:05:17.900 --> 00:05:26.780]   I've kind of tempted to get this 24 seconds left. Should I bid at $16,000? No, it's a conference room
[00:05:26.780 --> 00:05:31.540]   And what you have only one ad today. No, it's a it's a coin. It's a cone of silence
[00:05:32.100 --> 00:05:37.380]   That you can have your but how look how jammed in this conference room is I wouldn't want to know that's terrible
[00:05:37.380 --> 00:05:42.380]   Imagine that given our current COVID and ventilation crisis, but well, I wouldn't use it for that
[00:05:42.380 --> 00:05:47.980]   I would turn it into a sound studio for just me. Yeah, wouldn't that be cool next audio book?
[00:05:47.980 --> 00:05:51.020]   I could put that in the backyard. Oh, it's over. Oh
[00:05:51.020 --> 00:05:57.420]   But I think it's so for like more than 10 grand. There's still a few things left
[00:05:57.420 --> 00:05:59.780]   They're winding down like now
[00:06:00.260 --> 00:06:03.700]   If you wanted to get most of this stuff, I really wouldn't want a
[00:06:03.700 --> 00:06:07.420]   multi-function video conference system
[00:06:07.420 --> 00:06:15.160]   It fits right under your cabinet. Look how crappy this looks. Does it come with its own chassis? Yeah, this is chassis
[00:06:15.160 --> 00:06:22.260]   I included I don't know only $900 if you want to buy this. It's kind of crappy wiring. I was very unimpressed by the wiring
[00:06:22.260 --> 00:06:24.900]   Used at Twitter
[00:06:25.020 --> 00:06:31.620]   It there is a sadness to this though. Oh, there is here's the here's the little one this for half the price
[00:06:31.620 --> 00:06:33.180]   I could just get a one-man
[00:06:33.180 --> 00:06:36.660]   Some that's first anybody was in trouble. They go there were a lot of these I
[00:06:36.660 --> 00:06:42.980]   They say phone booth cleaning up and bring in some stuff. Hopefully I don't think they're bringing in new stuff
[00:06:42.980 --> 00:06:48.140]   Oh, no, they're not breaking stuff in there. They're not even bringing in toilet paper. I feel like it's kind of
[00:06:48.140 --> 00:06:53.660]   I don't know it's sad to me like when someday we do the same thing for our studios
[00:06:54.340 --> 00:07:00.580]   It'll be kind of the end of a monitor arm really that was on there 1500 bucks for this human scale
[00:07:00.580 --> 00:07:04.420]   M2.1 with M connect monitor arms
[00:07:04.420 --> 00:07:12.300]   You gotta be kidding. Oh wait a minute. Oh, it's 200 of them. Oh my god. It's 200 in a bag
[00:07:12.300 --> 00:07:18.860]   This is the kind of thing that you put on your seawall to keep pigeons from landing. I don't
[00:07:18.860 --> 00:07:21.940]   That's ridiculous
[00:07:21.940 --> 00:07:25.420]   You'd have to figure out wherever oh nicely stored Twitter
[00:07:25.420 --> 00:07:29.460]   Do you think Elon came and just tore them off the wall and threw them in a box?
[00:07:29.460 --> 00:07:34.620]   Does it sound I think one of the people like you fired was probably the
[00:07:34.620 --> 00:07:42.980]   Your life. Oh my goodness. Oh, sorry. You can say that in this case YouTube. It's gonna be okay
[00:07:42.980 --> 00:07:48.780]   Yes, yeah, YouTube was for some reason demonetizing people swore in the first 15 seconds
[00:07:48.860 --> 00:07:52.100]   It's okay. We're already more than 15 seconds in so I
[00:07:52.100 --> 00:07:54.820]   Did you can beat me out? No
[00:07:54.820 --> 00:07:59.340]   We always believe because we want it to be family-friendly a lot of those chairs
[00:07:59.340 --> 00:08:04.860]   Some of this stuff. I don't want a 1300 bucks for a standing desk. No
[00:08:04.860 --> 00:08:08.900]   Spine these I guess some repo
[00:08:08.900 --> 00:08:16.700]   Yeah, yeah repo depot is gonna sell them in the warehouse. Where's the coffee machine framp? It's gone. No, no, somebody got it
[00:08:16.700 --> 00:08:18.700]   It's gone. You can't even see what it went for
[00:08:18.700 --> 00:08:20.940]   No, no, once it's sold. They're gone. Oh
[00:08:20.940 --> 00:08:24.060]   So they're slowly dwindling away
[00:08:24.060 --> 00:08:27.900]   The drags the drags of Twitter. I
[00:08:27.900 --> 00:08:32.640]   Don't know it makes me sad, but Twitter seems to be continuing on
[00:08:32.640 --> 00:08:38.220]   Somewhat the biggest story this week is that they turned off the third-party
[00:08:38.220 --> 00:08:40.820]   Apis and there are some very upset
[00:08:40.820 --> 00:08:44.340]   software developers Craig Hockenberry who does
[00:08:44.980 --> 00:08:49.620]   Was it Twitter if they didn't tell anybody yeah, they just turned it off last Thursday
[00:08:49.620 --> 00:08:52.380]   He was gonna open it up. I can't remember
[00:08:52.380 --> 00:08:59.620]   You know he also said he was gonna quit he said I'll have a poll and I'll do what it says and then he didn't yeah
[00:08:59.620 --> 00:09:02.060]   Yeah, so who the heck knows?
[00:09:02.060 --> 00:09:07.380]   But yeah, the the guy who does Twitter if it was really upset
[00:09:07.380 --> 00:09:13.900]   Well, so it was Paul had out who does tweet bot the tap bots folks. They're doing ivory which is a mastodon
[00:09:14.620 --> 00:09:19.380]   I was for iOS, but the Twitterific fella
[00:09:19.380 --> 00:09:23.500]   Quit quit in a huff. He was very upset
[00:09:23.500 --> 00:09:32.900]   It's it's kind of tragic that's sad yeah, no communications still
[00:09:32.900 --> 00:09:35.180]   from
[00:09:35.180 --> 00:09:41.220]   From Twitter's non communications department they fired everybody over there. All right. Yeah. Yep
[00:09:42.580 --> 00:09:46.900]   So I don't know about you. I love you on bus key hates journalists
[00:09:46.900 --> 00:09:51.620]   I mean even Tesla for the long well, they had a PR person actually it was a friend of mine
[00:09:51.620 --> 00:09:57.660]   And then yeah, then he left and I don't know if he was replaced
[00:09:57.660 --> 00:10:01.580]   So I don't know what your feeds if you're looking at them still online
[00:10:01.580 --> 00:10:05.220]   I mean the on mastodon, but I still look at Twitter and
[00:10:06.380 --> 00:10:13.260]   So now I get tons of lower and bober to my do not follow but the but obviously because one journalist
[00:10:13.260 --> 00:10:18.620]   I know follows them who I follow and that is their excuse for putting her in my feed like crazy
[00:10:18.620 --> 00:10:25.700]   I've seen a lot of people complain. So they have this new for you tab which they've eliminated the chronological feed as far as I can tell
[00:10:25.700 --> 00:10:32.100]   Remember these to have that little twinkling button and you could choose from chronological or latest tweets
[00:10:32.180 --> 00:10:38.020]   Now they've got very much like tiktok a for you and a following tab same as same as poster
[00:10:38.020 --> 00:10:44.900]   Post has post does that my following is in is it chronological chronological?
[00:10:44.900 --> 00:10:48.500]   Maybe this is it is in chronal. Maybe this is roughly the same feeds as before
[00:10:48.500 --> 00:10:53.860]   Mmm, but I've seen a lot of people complain that before you tab which is not people you're following it has gone
[00:10:53.860 --> 00:11:01.020]   Completely right wing. I haven't had that experience. So maybe it oh I have maybe it's who you're following. I do see you
[00:11:01.020 --> 00:11:03.020]   less I
[00:11:03.020 --> 00:11:04.780]   See less people I care about yeah
[00:11:04.780 --> 00:11:08.100]   Mine is definitely not chronological a
[00:11:08.100 --> 00:11:13.940]   Be your following I have my list I have my following I have my list of
[00:11:13.940 --> 00:11:17.420]   History wonks that I love and my as my was my covid list. I
[00:11:17.420 --> 00:11:22.540]   Used to you know spend an hour going through my book history wonks list now
[00:11:22.540 --> 00:11:27.100]   I think all around it's he's showing me a lot fewer tweets. Yeah
[00:11:28.380 --> 00:11:32.100]   Honestly because I don't that many people when I go to Twitter because I know and I don't do it that often
[00:11:32.100 --> 00:11:36.420]   But I mostly go to see what's going on not to see who's tweeting but what's happening at Twitter
[00:11:36.420 --> 00:11:39.500]   I'm mostly surprised that the people who are still here like
[00:11:39.500 --> 00:11:48.260]   I mean, I guess you're like Stacy like the three of you are still there. So I just I'm still there
[00:11:48.260 --> 00:11:50.260]   I will say this I
[00:11:50.260 --> 00:11:55.740]   I have been I guess quiet on it for at least two weeks now
[00:11:56.340 --> 00:12:03.660]   And just been more of a broadcast or if you will over there because when I open it up that whole four-year stuff or whatever this algorithm
[00:12:03.660 --> 00:12:10.660]   That's in place now. It's been a little bit more depressing to me and I just think I don't have time for it
[00:12:10.660 --> 00:12:13.620]   It's either been more depressing or is more stuff about
[00:12:13.620 --> 00:12:20.860]   Celebrities who I could give to craps right now. I do not do anything for me or nor are they really doing anything
[00:12:20.860 --> 00:12:26.020]   But a world so it's not important to me and I'm sick of seeing that so I just sort of been away
[00:12:26.020 --> 00:12:30.500]   And I know people have been tweeting me and whatnot. I'm I'm sorry
[00:12:30.500 --> 00:12:39.620]   Just I really don't have a desire to hang out in there right now saying even with with mastodon quite frankly because as I scroll through
[00:12:39.620 --> 00:12:44.420]   Mastodon, I'm still putting in filters because it's just so much depressing news and
[00:12:44.420 --> 00:12:51.960]   commentary happening over there for me. You gotta you gotta follow some happier people. I know I gotta find some people that are
[00:12:51.960 --> 00:12:55.180]   When I could when I go on Mastodon, I'll tell you and you can follow me
[00:12:55.300 --> 00:13:00.460]   I'm very cheerful. We're waiting for you Stacey. I know we are waiting for you
[00:13:00.460 --> 00:13:04.180]   You know, I hey nice there. We have some hosts
[00:13:04.180 --> 00:13:08.940]   I try to encourage people as you know to move to Mastodon. I don't but if you don't move, that's fine
[00:13:08.940 --> 00:13:12.420]   And I have some host. I'll do it eventually. Yeah, no, that's fine. I don't
[00:13:12.420 --> 00:13:18.660]   This is the beauty of Mastodon. There's no pre-repressure. It's just if you want to you can but it's just you know
[00:13:18.660 --> 00:13:23.860]   It's just it's there for me man. It's just there. It's just there. I don't well. I kind of like that. I don't I
[00:13:24.260 --> 00:13:26.260]   Do I want to get away from this whole
[00:13:26.260 --> 00:13:31.380]   Fall wash. Yeah crap. Yeah, anyway
[00:13:31.380 --> 00:13:37.180]   So that's the you know, we haven't done a lot of Elon stories lately because I'm tired of it and I know you are
[00:13:37.180 --> 00:13:42.700]   But that that is kind of it's an interesting choice on his part. I really I mean
[00:13:42.700 --> 00:13:46.780]   Even Jack Dorsey who did this before we talked about this before
[00:13:46.780 --> 00:13:49.100]   when
[00:13:49.100 --> 00:13:52.860]   tweet deck was starting to be a threat and he was really worried that your buddy
[00:13:53.860 --> 00:13:57.700]   No, gross. So gross was about to kind of buy up all the third-party apps
[00:13:57.700 --> 00:14:01.580]   He jacked or see cut off the third-party API
[00:14:01.580 --> 00:14:05.660]   And he says now is it was a dumb move. It was a mistake
[00:14:05.660 --> 00:14:13.700]   But a mistake that Elon be I like all the mistakes the Twitter's ever made it's destined doomed to replay make him again. Yeah
[00:14:13.700 --> 00:14:16.460]   and I
[00:14:16.460 --> 00:14:18.460]   feel really bad for people like
[00:14:19.340 --> 00:14:25.380]   Hockenberry who does I think it's it's a Twitterific and Paul had not it as tweetbot and
[00:14:25.380 --> 00:14:32.380]   The various folks who have third-party apps all of them have stopped work to stop working Talon on Android
[00:14:32.380 --> 00:14:37.080]   And especially weird because there's no communication. There was no
[00:14:37.080 --> 00:14:40.220]   We're gonna do this. There's no even we did this
[00:14:40.220 --> 00:14:47.100]   Well, do you think do you think that it was actually purposeful or something the information the information does it as an excuse to the information?
[00:14:48.500 --> 00:14:55.540]   Got a peak at internal slack messages. Obviously some unhappy Twitter employees
[00:14:55.540 --> 00:14:59.020]   No, over that wouldn't be hard to find it
[00:14:59.020 --> 00:15:03.020]   The slack messages were all about okay. We're gonna turn it off
[00:15:03.020 --> 00:15:09.040]   You know, it was it was the smoking gun that they did it. Here's the here's the exclusive from Aaron Woo
[00:15:09.040 --> 00:15:17.460]   Musk's Twitter intentionally suspended tweetbot third-party apps messages show and they quote a number of
[00:15:18.300 --> 00:15:26.220]   Internal a senior software engineer in the slack on Thursday night quote third-party app suspensions are intentional
[00:15:26.220 --> 00:15:33.100]   This was in the internal Twitter command center slack channel used by employees to handle outages interruptions to Twitter service
[00:15:33.100 --> 00:15:36.620]   So that's to let the other engineers. No, no, no, this isn't a bug
[00:15:36.620 --> 00:15:46.340]   This is intentional a Twitter employee working on product partnerships asked Friday morning when employees could expect a quote list of approved talking points
[00:15:46.940 --> 00:15:51.820]   For questions from partners related to quote third-party clients revoked access
[00:15:51.820 --> 00:15:56.580]   There is no comms departments and nobody to write those talking points a product manager
[00:15:56.580 --> 00:16:01.340]   Responding on slack that morning said the company had quoted started to work on comms end quote
[00:16:01.340 --> 00:16:03.980]   But there's no estimate when it was ready clearly. It's not
[00:16:03.980 --> 00:16:08.260]   Yeah
[00:16:08.260 --> 00:16:13.020]   Nobody none of the developers as far as I know have received any communications at all
[00:16:15.220 --> 00:16:21.300]   Well say same as you know, I was a pain customer of Twitter with the old Twitter blue you were just cut off
[00:16:21.300 --> 00:16:25.500]   And um, yeah, just disappeared have they taken away your check cut off
[00:16:25.500 --> 00:16:31.820]   So and it didn't cut off. It was still charging me so I had to manually go in and fix it. That's most worse
[00:16:31.820 --> 00:16:37.140]   Yeah, I still have my blue check. Let me see what the what doesn't even do anything when I hover over it now
[00:16:37.140 --> 00:16:39.540]   Didn't use to know if you click on it
[00:16:40.180 --> 00:16:46.060]   Oh, see I've gone back and forth between whether I'm notable or it's just we don't know and it's left over
[00:16:46.060 --> 00:16:49.420]   So I'm not a lot of I'm really notable
[00:16:49.420 --> 00:16:53.620]   You're currently notable, but I wasn't for a while it goes back. I might or may not be
[00:16:53.620 --> 00:16:55.700]   And I'm a
[00:16:55.700 --> 00:17:00.820]   I may not be look how I violated the precepts. I have the link to my mastodon in my name
[00:17:00.820 --> 00:17:05.500]   Look at that. I only have one tweet which says going forward reach me at twitch.com
[00:17:05.500 --> 00:17:10.500]   Social and it has a link to my Twitter. So I'm wondering how I'm getting special interesting
[00:17:10.500 --> 00:17:13.940]   Okay in my mind are you ready? Are you ready for?
[00:17:13.940 --> 00:17:20.820]   The solopistic interpretation the self-centered interpretation in my mind Elon feels bad that he called himself chief twit
[00:17:20.820 --> 00:17:23.900]   And he and he doesn't be a nice
[00:17:23.900 --> 00:17:28.860]   Because he doesn't want to you know, it's not bad about that is what you just soon talk about self
[00:17:28.860 --> 00:17:35.480]   That man doesn't need a lawsuit. Oh, he's got plenty. He doesn't need more and I don't think he worries about long
[00:17:35.480 --> 00:17:37.480]   So it's
[00:17:37.480 --> 00:17:39.680]   There's the beautiful
[00:17:39.680 --> 00:17:46.320]   Planter the at sign planter which I I can't remember it went for tens of thousands of dollars
[00:17:46.320 --> 00:17:48.320]   I don't know the exact price, but it went for a lot
[00:17:48.320 --> 00:17:50.720]   Mostly because it's history
[00:17:50.720 --> 00:17:56.680]   Also, it's pretty cool. Yeah, I just it's kind of sad. I just is sad
[00:17:56.680 --> 00:18:01.160]   Anyway, it's over now. Are you want to be scared by robots?
[00:18:02.480 --> 00:18:09.000]   Yes, this is not scared. This is cute. This is cute. Yeah, this is cute Boston dynamic
[00:18:09.000 --> 00:18:15.440]   It's time and let me pause this and explain this says it's Boston. I am ex tweets
[00:18:15.440 --> 00:18:23.760]   It's time for atlas, which is it's a bipedal robot to pick up a new set of skills and gets hands-on
[00:18:23.760 --> 00:18:30.400]   Do you have my audio? Let me make sure it's on and we will play I just says I forgot my tools. That's all okay
[00:18:32.040 --> 00:18:36.880]   So he's pressing a button on a tablet. There's Atlas down on the ground the guy's up in the air
[00:18:36.880 --> 00:18:42.800]   He's Atlas is looking around with his laser pointer finds a board
[00:18:42.800 --> 00:18:51.360]   Puts the board backs up then jumps and turns around. That's kind of cool. That's the board on a scaffold and now
[00:18:51.360 --> 00:18:59.120]   Well, we walked he's really kind of easy. He picks up a little to Walt bag with tools in the step. Yeah
[00:18:59.120 --> 00:19:03.720]   He is he's looking at his step. He's got Pep in his get on
[00:19:03.720 --> 00:19:05.240]   Oh
[00:19:05.240 --> 00:19:09.440]   Chuckunk up up up. He's close to talk it up
[00:19:09.440 --> 00:19:12.960]   And look first then pushes over a box
[00:19:12.960 --> 00:19:16.360]   jumps onto the box
[00:19:16.360 --> 00:19:19.360]   Jump wait, there's more
[00:19:19.360 --> 00:19:27.000]   Wow does a flip onto the ground. I saw somebody tweet. I am studying this carefully
[00:19:27.560 --> 00:19:33.840]   Looking for weak points for when the terminators come it looks like those ankles might be
[00:19:33.840 --> 00:19:38.080]   Place you could just
[00:19:38.080 --> 00:19:43.920]   Achilles to Achilles heel a killer look how skinny they are the ankles so just tip for everybody
[00:19:43.920 --> 00:19:48.520]   Shoot for the ankles the ankles. Mm-hmm the rest of it looks pretty
[00:19:48.520 --> 00:19:52.760]   I'm not the gut not the torso definitely not the torso hit the ankles
[00:19:54.440 --> 00:19:57.640]   You know I would have liked to have seen the testing for this
[00:19:57.640 --> 00:20:01.520]   You know as it went up to push the wall down like what were some of the other?
[00:20:01.520 --> 00:20:05.400]   Situations that were right there is it pushed the wall down and it knew huh?
[00:20:05.400 --> 00:20:10.240]   I can actually land on top of that wall down on the ground. What happened if it tried to push?
[00:20:10.240 --> 00:20:15.840]   I don't know I'm gonna guess that they had the same guy directed as directed 20 2016
[00:20:15.840 --> 00:20:18.400]   Tesla self-driving demo
[00:20:20.560 --> 00:20:27.920]   Couple of takes okay might have been so you remember and this was tragic the engineer in Silicon Valley who?
[00:20:27.920 --> 00:20:30.400]   crashed into a
[00:20:30.400 --> 00:20:34.920]   divider and died in a fiery crash in 2018 his family is
[00:20:34.920 --> 00:20:37.720]   suing and
[00:20:37.720 --> 00:20:40.120]   They got a statement from
[00:20:40.120 --> 00:20:45.080]   Tesla's auto pilot director of autopilot software a show killer swarming
[00:20:45.080 --> 00:20:47.680]   Under oath in a deposition
[00:20:48.120 --> 00:20:51.800]   He talks about the video that is still on the Tesla website
[00:20:51.800 --> 00:20:57.880]   The video begins with the caption the person in the driver's seat is only there for legal reasons
[00:20:57.880 --> 00:21:03.800]   He's not doing anything the car is driving itself. This is the famous painted black
[00:21:03.800 --> 00:21:14.480]   Video you see the Tesla Model X by the way. I had a model X at this exact time and even I knew you can't do this
[00:21:15.000 --> 00:21:20.120]   The heavily edited video shows the model X driving around stopping for road junctions and red lights
[00:21:20.120 --> 00:21:24.360]   Nope all the while the human has his hands near but not on the steering wheel
[00:21:24.360 --> 00:21:32.480]   Upon reaching a Tesla facility the human leaves the model X which goes off to park itself avoiding running over a pedestrian in the process
[00:21:32.480 --> 00:21:36.920]   At the time Elon tweeted
[00:21:36.920 --> 00:21:42.340]   Tesla drives itself no human input at all through urban streets to highway to streets and finds a parking spot
[00:21:42.880 --> 00:21:49.320]   He went on to add eight cameras 12 ultra sonars and radar all flush mounted and body color beauty remains well in this
[00:21:49.320 --> 00:21:52.240]   deposition
[00:21:52.240 --> 00:21:58.840]   Ella Swarmy said the intent of the video was not to accurately portray what was available for customers in 2016 and again as a customer
[00:21:58.840 --> 00:22:03.600]   I couldn't get it it was to portray what was possible to build into the system
[00:22:03.600 --> 00:22:11.280]   3d master used to pre-program the route where to stop and during the self-parking demo the Tesla crashed into a fence
[00:22:11.360 --> 00:22:14.800]   which they edited out I
[00:22:14.800 --> 00:22:18.120]   Think this is going to be pretty persuasive to the jury
[00:22:18.120 --> 00:22:21.760]   Because of course
[00:22:21.760 --> 00:22:29.840]   It was a autopilot my Tesla crashed into a fence oh really was itself driving I
[00:22:29.840 --> 00:22:34.040]   Know I was driving it was a really terrible
[00:22:34.040 --> 00:22:40.320]   You did what's the responsibility here's Stacy
[00:22:41.160 --> 00:22:46.320]   Lame the machine. Yes, we always want to blame the machine the car machine
[00:22:46.320 --> 00:22:50.360]   Hold what to do
[00:22:50.360 --> 00:22:57.320]   By the way if it crashes into a fence it really goes all the way through that fence
[00:22:57.320 --> 00:23:08.800]   It was an austin it went over a parking hump again pedal error so it went over the parking hump smashed
[00:23:08.800 --> 00:23:11.960]   I'm so glad no one was on the other side. I'm so glad there wasn't a dog
[00:23:11.960 --> 00:23:15.040]   It took out that whole fence they thought I was driving a Hummer
[00:23:15.040 --> 00:23:19.800]   They're like were you driving like a big truck or like a hummer? Well, that's my way something to remember
[00:23:19.800 --> 00:23:25.480]   And somebody recently said this is a problem these cars are very heavy heavier than they look
[00:23:25.480 --> 00:23:31.640]   There's a ton of they come off the line super fast. So you got a lot of
[00:23:31.640 --> 00:23:34.320]   What's the word I'm looking for? Yeah?
[00:23:35.600 --> 00:23:40.560]   Accelerators to the brake it's not slowly accelerating. It's gonna leap forward
[00:23:40.560 --> 00:23:45.000]   It it that whole thing because it's like three thousand pounds
[00:23:45.000 --> 00:23:50.120]   So yeah, it was pretty awful big was a home or a or what you
[00:23:50.120 --> 00:23:52.720]   It was someone's home
[00:23:52.720 --> 00:23:55.320]   And then I had to knock on the door and
[00:23:55.320 --> 00:24:02.440]   She was home. So I had the car hit your fence. I'm driving a Tesla. It's not my fault
[00:24:03.280 --> 00:24:09.840]   I'm the car. I did not tell them I was driving a Tesla. I just was like, oh, yeah, it was just really scary
[00:24:09.840 --> 00:24:14.400]   No, that happens a lot people hit the wrong pedal. That's actually very very common
[00:24:14.400 --> 00:24:20.000]   Yeah, it's just so much very only when you're driving one of these cars. It's your face Jeff
[00:24:20.000 --> 00:24:23.000]   Hush your face
[00:24:23.000 --> 00:24:27.840]   Well, how much damage was there to your car
[00:24:29.400 --> 00:24:34.920]   Really? Oh, I mean, not really it was just expensive. It wasn't like you could drive. I drove the car away
[00:24:34.920 --> 00:24:42.840]   Back over the hump and how quickly were you able to get it fixed was he that was one of the reasons I didn't
[00:24:42.840 --> 00:24:49.880]   Yeah, yeah, it was a lot. There's a long there was when I had it a long wait for parts and I just I
[00:24:49.880 --> 00:24:53.200]   Didn't like that. I thought that made me nervous. So oh
[00:24:53.200 --> 00:24:56.840]   Scooter X says my Tesla actually weighs
[00:24:57.880 --> 00:24:59.880]   Two and two tons
[00:24:59.880 --> 00:25:02.920]   4,5, 61 pounds
[00:25:02.920 --> 00:25:11.000]   Who was it? I just saw a story about that the Teslas and all of these all of these EVs are heavy
[00:25:11.000 --> 00:25:14.920]   That's why they want to ban them in what state is that
[00:25:14.920 --> 00:25:24.760]   Although the heaviness is good because they don't flip as much. They're very low center of gravity
[00:25:24.760 --> 00:25:28.680]   Nice to drive a level, but you don't want to be hit by them. Yeah
[00:25:28.680 --> 00:25:30.920]   Oh no
[00:25:30.920 --> 00:25:37.960]   I mean look at that bridge video. We had last week the one car goes up because the Tesla's an immovable object. Yeah
[00:25:37.960 --> 00:25:45.240]   I'm not moving. It's why everybody should drive one. So then you'll be safe
[00:25:45.240 --> 00:25:52.280]   That's the SUV. Yeah, that's the logic that gets us these incredibly tall scvs and trucks, right?
[00:25:52.840 --> 00:25:58.040]   Well, even better everybody should drive an electric hummer. Then you'd really be safe. Wow
[00:25:58.040 --> 00:26:04.600]   Oh, yeah, and and the battery and the electric hummer weighs more than most cars
[00:26:04.600 --> 00:26:07.480]   Just the battery
[00:26:07.480 --> 00:26:09.480]   It's huge
[00:26:09.480 --> 00:26:14.680]   Uh range on that sucker. Oh, I don't know. I don't know it's uh, I mean if you're gonna drive a hummer
[00:26:14.680 --> 00:26:21.000]   I guess it's marginally better to drive an electric hummer. I've seen them. They're they're very massive too. Yeah
[00:26:21.560 --> 00:26:25.320]   So can we can I ask you about Wyoming because I was curious about the
[00:26:25.320 --> 00:26:30.200]   Well, like you can't drive through them or you just can't sell them in Wyoming
[00:26:30.200 --> 00:26:36.600]   Is that they want to they want to out-loss selling them? It's oil. It's all oil, but it's just one guy in Wyoming
[00:26:36.600 --> 00:26:41.400]   Yeah, it's actually six of them. Okay, six geophiles. Oh, you have 10 people there anyway
[00:26:41.400 --> 00:26:44.760]   So they've been one loud person is all it takes and a lot of this stuff
[00:26:44.760 --> 00:26:49.880]   And I think it's very much a GOP talking point because in california, they're gonna ban
[00:26:50.200 --> 00:26:52.200]   uh gas vehicles so
[00:26:52.200 --> 00:26:58.040]   We're gonna ban at the same year by the way 2035 sales of electric vehicles
[00:26:58.040 --> 00:27:00.520]   Yes, both
[00:27:00.520 --> 00:27:04.600]   Here are the here are the very diverse very diverse group of
[00:27:04.600 --> 00:27:10.520]   Members of the state of wyoming's legislature. Oh, that's a hair dye job. Oh
[00:27:10.520 --> 00:27:17.080]   Oh, there's one young guy. That's real hair. That's but this is the beer the beer
[00:27:17.240 --> 00:27:20.760]   You don't buy the beer. It just doesn't work. You're and your hair is
[00:27:20.760 --> 00:27:23.160]   Auburn and your beard is
[00:27:23.160 --> 00:27:25.720]   A good year tire black
[00:27:25.720 --> 00:27:28.280]   No, no, well you're tired
[00:27:28.280 --> 00:27:32.040]   One of the one of the senators name is uh, brian boner
[00:27:32.040 --> 00:27:37.640]   And uh, but that's not funny because we are mature. There's nothing funny about that
[00:27:37.640 --> 00:27:40.600]   But he was he was on fox. That's how I know his name
[00:27:40.600 --> 00:27:43.320]   He told the cowboy state daily
[00:27:46.680 --> 00:27:49.800]   I am making zero of this up. I just want to say
[00:27:49.800 --> 00:27:57.480]   It's uh, he says one might even say tongue and cheek but obviously it's a very serious issue that deserves some public discussion
[00:27:57.480 --> 00:28:00.520]   I'm interested in making sure that the solutions that some folks
[00:28:00.520 --> 00:28:05.880]   Want to the so-called climate crisis or actually practical in real life
[00:28:05.880 --> 00:28:10.600]   I just don't appreciate it when other states try to force technology that isn't ready
[00:28:11.960 --> 00:28:17.800]   The legislature would be saying if you don't like our petroleum cars, well, we don't like your electric cars
[00:28:17.800 --> 00:28:26.040]   Mm-hmm. Mm-hmm. I don't think chances of this passing are very high. No, you know what let them do it
[00:28:26.040 --> 00:28:31.800]   Let them shoot themselves in the proverbial foot. I have no problems with that. Yeah, then the proverbial tire
[00:28:31.800 --> 00:28:35.080]   It's it's one of it's another one of those performative
[00:28:35.080 --> 00:28:39.480]   Yeah, I saw I legislated things by the way, I uh
[00:28:40.440 --> 00:28:42.440]   I didn't realize this but twitter has admitted
[00:28:42.440 --> 00:28:48.200]   It's breaking third-party apps because of our long-standing api rules
[00:28:48.200 --> 00:28:53.320]   They didn't say what rules the developers had uh had violated
[00:28:53.320 --> 00:28:57.960]   Uh, but apparently twitter is enforcing. This was uh tweeted
[00:28:57.960 --> 00:29:02.920]   Yesterday I missed it. I should check twitter every day shouldn't I
[00:29:02.920 --> 00:29:07.720]   My part of the rundown too if you check that you'd see it. That's where I saw it twitter is enforcing
[00:29:07.720 --> 00:29:12.120]   It's long-standing api rules that may result in some apps not working
[00:29:12.120 --> 00:29:16.440]   So that's the talking points. They apparently have come up with them
[00:29:16.440 --> 00:29:25.640]   Uh, craig hawkenberry creator twitterific said we have been respectful of their api rules as published for the past 16 years
[00:29:25.640 --> 00:29:29.720]   We have no knowledge of these rules have changed recently or what those changes might be
[00:29:29.720 --> 00:29:33.800]   Tapbox says tweet bot's been around for over 10 years
[00:29:33.800 --> 00:29:37.240]   We've always complied with the twitter api rules if there's some existing rule
[00:29:37.240 --> 00:29:41.480]   We need to comply with we'd be happy to do so if possible but we do need to know what it is
[00:29:41.480 --> 00:29:43.960]   This is
[00:29:43.960 --> 00:29:45.560]   Bogosity
[00:29:45.560 --> 00:29:47.560]   They're just making stuff up now
[00:29:47.560 --> 00:29:51.640]   Did you know that jennifer lopez
[00:29:51.640 --> 00:29:54.920]   was the reason
[00:29:54.920 --> 00:29:58.440]   Google created google images see I saw this headline jeff
[00:29:58.440 --> 00:30:02.840]   And I didn't even read the story. I thought that's link bait if ever I heard it
[00:30:03.080 --> 00:30:08.760]   This is according to a well-known website called ifl science
[00:30:08.760 --> 00:30:12.120]   But at least had a quote in it. Google images
[00:30:12.120 --> 00:30:15.560]   Because of j-lo this was her plunging neckline
[00:30:15.560 --> 00:30:18.280]   uh dress from
[00:30:18.280 --> 00:30:23.320]   The Oscars no, no, it's the grammy see that grammy's i'm sorry what year what year was it?
[00:30:23.320 --> 00:30:25.720]   2000
[00:30:25.720 --> 00:30:26.920]   2000
[00:30:26.920 --> 00:30:31.560]   40 second grammy awards a low neckline dress drew a baffling amount of coverage around the world
[00:30:32.440 --> 00:30:38.360]   Uh, she said it didn't seem that out there to me. It was a good luck in dress. I had no idea. It was such a big deal
[00:30:38.360 --> 00:30:43.800]   But it didn't just impact regular media according to ifl science
[00:30:43.800 --> 00:30:47.480]   It had a lasting effect on google search engine because
[00:30:47.480 --> 00:30:52.680]   Cause if you think about it to search for the dress they searched the dress
[00:30:52.680 --> 00:30:54.760]   Google didn't know
[00:30:54.760 --> 00:31:01.560]   That how to look for dresses. This is a quote had to understand the images from eric schmitt he said this in a 2015
[00:31:02.360 --> 00:31:06.760]   People wanted more than just text this first became apparent after the 2000 grammys
[00:31:06.760 --> 00:31:13.640]   Where j-lo wore green dress that caught the world's attention at the time was the most popular search query we had ever seen
[00:31:13.640 --> 00:31:20.360]   But we had no surefire way of getting users exactly what they wanted, which was of course a picture of j-lo in that dress
[00:31:20.360 --> 00:31:26.520]   Google image search was born. Well if eric smith said it then it must be true. That's good enough for me
[00:31:26.680 --> 00:31:33.000]   No, it's totally true. They they even talk about it. Um, they referenced it recently in one of their i-o presentations
[00:31:33.000 --> 00:31:36.040]   Oh, probably yeah one of the cool story
[00:31:36.040 --> 00:31:40.600]   It is a cool story. That's why I put it in there. Yeah, I'm glad you noticed it
[00:31:40.600 --> 00:31:42.200]   um
[00:31:42.200 --> 00:31:43.240]   So
[00:31:43.240 --> 00:31:47.640]   This google engineer explains this in uh, this is in gq
[00:31:47.640 --> 00:31:51.160]   A story from a couple of years ago by racio rachel tascian
[00:31:51.160 --> 00:31:53.960]   um
[00:31:53.960 --> 00:31:56.760]   She wore this actually, uh
[00:31:56.760 --> 00:32:02.600]   In grammys in 2000 and again in 2020 with don't tell of her sachi
[00:32:02.600 --> 00:32:05.720]   Uh at the woman's wear collection
[00:32:05.720 --> 00:32:08.360]   Um
[00:32:08.360 --> 00:32:14.120]   And she acts in this article in the gq article they actually quote an engineer kathy edwards director of
[00:32:14.120 --> 00:32:16.600]   engineering a product for google images
[00:32:16.600 --> 00:32:21.000]   She says it wasn't overnight, but it was definitely the impetus is completely true
[00:32:21.160 --> 00:32:25.720]   She said in the google hangout on friday, but it's also not the case that has happened in the next day
[00:32:25.720 --> 00:32:28.760]   We said oh we should build an image search engine at that point
[00:32:28.760 --> 00:32:32.920]   She said the company was only two years old 2000. It was only two years old wow
[00:32:32.920 --> 00:32:39.080]   What with a very small number of employees and everyone at the time was of course we need to build an image search engine
[00:32:39.080 --> 00:32:41.640]   But they weren't sure how much priority to give it
[00:32:41.640 --> 00:32:47.240]   When the low-pest red oh, I see this was when it was only two not in 2000 but before that
[00:32:47.640 --> 00:32:50.920]   Thank you when the low-pest red that makes more sense because when did google start 90?
[00:32:50.920 --> 00:33:00.040]   I don't remember 898 so was two years old when the low-pest dress moment happened in february 2000
[00:33:00.040 --> 00:33:05.000]   Quote it became so clear. This was important, but they didn't have anyone to do it end quote
[00:33:05.000 --> 00:33:07.640]   So google hired a
[00:33:07.640 --> 00:33:12.680]   That summer a college grad recent college graduate we con joo as an engineer
[00:33:12.680 --> 00:33:15.000]   partnered him with
[00:33:15.000 --> 00:33:19.240]   Susan Wojcick current CEO of CEO he she was then a project product manager
[00:33:19.240 --> 00:33:23.560]   And then be I'll be and they worked together to build it. So Susan Wojcicki
[00:33:23.560 --> 00:33:27.800]   Was one of two people developed image search
[00:33:27.800 --> 00:33:34.040]   And single-handedly almost launched it in july 2001 now actually that's of more interest to me than
[00:33:34.040 --> 00:33:39.960]   Then the fact that it was j-lo's dress that was Susan Wojcicki was just good. Yeah, because we're geeks
[00:33:39.960 --> 00:33:43.560]   Yeah, we care about that and she's now I also care about her dress
[00:33:44.040 --> 00:33:46.040]   She see you care about the dress
[00:33:46.040 --> 00:33:50.840]   Pretty it's a nice dress. It's a historical dress
[00:33:50.840 --> 00:33:58.520]   Actually, is it common for executives to have started out as part of um the development team
[00:33:58.520 --> 00:34:03.720]   Is it is like you're small? I mean, I know we have zuckerberg, you know, um
[00:34:03.720 --> 00:34:06.200]   but is it
[00:34:06.200 --> 00:34:12.760]   For all of these big the wojcicki family is kind of interesting because her sister of horses 24 at 23 and me
[00:34:13.640 --> 00:34:16.200]   Yes, um and who was married to
[00:34:16.200 --> 00:34:23.320]   Shh me no no no no. Larry. Okay. Okay. Okay. Sorry. Larry. Okay. Okay. Okay. Okay. Okay. Which one
[00:34:23.320 --> 00:34:27.480]   Oh, and her and her there's three sisters. Well, Oprah. Yeah
[00:34:27.480 --> 00:34:33.240]   There's three sisters the uh the other sister and their mother is quite amazing too. I know her
[00:34:33.240 --> 00:34:40.280]   She's an educator paula. Well, hi. Hi ester. Yeah, ester. She's wonderful. Yeah, she's quite well known as well
[00:34:40.280 --> 00:34:45.560]   In fact, it was in their garage was in their garage. That's Susan and Janet first guys started they were
[00:34:45.560 --> 00:34:49.000]   Uh high school students in paulo alto
[00:34:49.000 --> 00:34:51.720]   Um
[00:34:51.720 --> 00:34:57.960]   Yeah, it's actually kind of an interesting story. Isn't it? So her sister j is it janet is uh
[00:34:57.960 --> 00:35:00.760]   Yes, sir gay brin
[00:35:00.760 --> 00:35:06.840]   Uh and wojcicki was married to uh, sir gay and then she found a 23 and me
[00:35:08.520 --> 00:35:11.720]   And then susan her sister of course is currently ceo of youtube
[00:35:11.720 --> 00:35:16.600]   And quite successful. I might add a third. I think it's professor or something. Yeah. Yeah
[00:35:16.600 --> 00:35:18.200]   Yeah
[00:35:18.200 --> 00:35:21.880]   They grew up on the stanford campus just like samuel bankman freed
[00:35:21.880 --> 00:35:30.440]   See he could go one way or the other a new blogger a new sub stacker. Yes. I can't believe he's got a sub stack
[00:35:30.440 --> 00:35:33.640]   Of course. He's a sub stack. Yeah, of course
[00:35:33.640 --> 00:35:36.600]   course
[00:35:36.840 --> 00:35:41.320]   Uh wojcicki's grandfather was a people's party and polish people's partitian
[00:35:41.320 --> 00:35:46.600]   elected a member of parlament during the 1947 polish legislative election
[00:35:46.600 --> 00:35:51.000]   And her grandmother was a librarian at the library of congress
[00:35:51.000 --> 00:35:55.080]   Responsible for building the largest collection of polish material in the united states
[00:35:55.080 --> 00:36:01.720]   Very accomplished family. Yeah. Oh, yeah, and just for you here is a picture in of the 2020
[00:36:01.720 --> 00:36:04.600]   version of the dress worn by
[00:36:04.600 --> 00:36:06.280]   Wow
[00:36:06.280 --> 00:36:08.280]   modern by uh, uh
[00:36:08.280 --> 00:36:09.480]   Jlo
[00:36:09.480 --> 00:36:31.280]   And that's don't a televerte
[00:36:31.280 --> 00:36:33.560]   Well, we talked about this last week
[00:36:33.560 --> 00:36:35.880]   that you could get um
[00:36:36.360 --> 00:36:41.400]   You could get it in your in your other devices, but now they're going to allow you to do a custom alarm sound
[00:36:41.400 --> 00:36:47.000]   So I think that leo you should record an alarm sound people can put on their google devices and you can wake up
[00:36:47.000 --> 00:36:49.720]   with film
[00:36:49.720 --> 00:36:54.680]   Well in theory, it's allowing you to record your own voice. I know but I think you should do it
[00:36:54.680 --> 00:37:00.200]   I could just I could offer it for download to our club. Yes. Yes. Hey folks. How would I do it? Exactly?
[00:37:00.200 --> 00:37:03.240]   exactly
[00:37:03.240 --> 00:37:05.240]   You have all kinds of technical spark people run
[00:37:05.240 --> 00:37:12.600]   I don't know how to talk out loud into a micro there was a steven fry talking
[00:37:12.600 --> 00:37:15.320]   uh alarm clock
[00:37:15.320 --> 00:37:19.320]   Where he would wake you up in his character as geeves. Do you remember that?
[00:37:19.320 --> 00:37:24.280]   Let me see if I can find the sound the sound of that it's the good morning, sir
[00:37:24.280 --> 00:37:30.920]   Talking alarm clock. Oh, it's no longer for sale, but he would say oh, it was so funny. What he would say
[00:37:31.000 --> 00:37:33.000]   Let me see if I gotta play this for you
[00:37:33.000 --> 00:37:41.800]   Because I would say this I could say this but frankly I would if I were you I would uh, I would
[00:37:41.800 --> 00:37:46.280]   Here we go. Here's somebody playing it into their youtube
[00:37:57.640 --> 00:38:00.920]   You hear birds excuse me madam your horoscope
[00:38:00.920 --> 00:38:07.960]   Advises a demonstration of courage in the face of adverse circumstances very good madam
[00:38:07.960 --> 00:38:12.200]   Okay, I like that
[00:38:12.200 --> 00:38:15.560]   I like that
[00:38:15.560 --> 00:38:18.680]   Here's another one. Oh
[00:38:18.680 --> 00:38:21.240]   There's my favorite one
[00:38:21.240 --> 00:38:26.680]   Okay, geez. All right. The favorite one is him alerting the media. I mean, it's if I can find this
[00:38:27.240 --> 00:38:31.400]   You would think the people selling this device for a hundred bucks would offer you some samples
[00:38:31.400 --> 00:38:35.960]   But I guess they want you to buy it first. There's 126 different messages
[00:38:35.960 --> 00:38:40.200]   Excuse me, sir. I'm so sorry to disturb you
[00:38:40.200 --> 00:38:45.160]   But it appears to be morning very inconvenient. I agree
[00:38:45.160 --> 00:38:49.480]   I believe it is the rotation of the earth that is to blame, sir
[00:38:49.480 --> 00:38:56.120]   Or this is my favorite shy and form the news agencies that you are about to rise, sir
[00:38:56.920 --> 00:39:01.800]   Okay, chat room. What do you want Leo to say to wake you up in wait?
[00:39:01.800 --> 00:39:04.600]   I'd rather have Stephen Fry to be honest with you
[00:39:04.600 --> 00:39:11.880]   I didn't realize there's 126 different ones. That's amazing. That was from the according session. Yeah, no kidding
[00:39:11.880 --> 00:39:15.960]   I I'm sorry disturb you sir
[00:39:15.960 --> 00:39:20.840]   I want to choose everyone should have a jeeps. Don't you think morning? Yeah
[00:39:22.040 --> 00:39:27.880]   You know what someone someone in our chat room says they have those two alarm clocks. Oh no
[00:39:27.880 --> 00:39:31.400]   Male and female
[00:39:31.400 --> 00:39:35.320]   Well, I guess good morning madam or good morning, sir. Oh, I see
[00:39:35.320 --> 00:39:43.160]   Good morning. Why are they in storage my martyst? Why don't you use after a while you get very tired of it?
[00:39:43.160 --> 00:39:50.440]   I think geek used to sell it. Uh, then they got bought. They're very ugly though says my martyst
[00:39:51.000 --> 00:39:56.440]   Oh, I think they look pretty good. I'm delighted to say I survived another night
[00:39:56.440 --> 00:40:01.320]   That's the best you didn't get it for the look you get it for the sound, right?
[00:40:01.320 --> 00:40:09.000]   Yeah, you don't get it for the look. I mean it still has to sit in your bedroom somewhere though. So, you know, I can appreciate
[00:40:09.000 --> 00:40:12.200]   All right, um moving right along
[00:40:12.200 --> 00:40:16.520]   And I think you should run with this. I think this is a new club benefit
[00:40:17.160 --> 00:40:24.200]   I like the idea sir. So, yeah, I think I think you're gonna do it the other 27 different ways the other day
[00:40:24.200 --> 00:40:27.720]   Uh, well after the flooding in the northern, California
[00:40:27.720 --> 00:40:33.800]   We decided to we had to get home and we decided to use ways to help us wind our way through this
[00:40:33.800 --> 00:40:40.840]   You know the streets to get home and a very pleasant voice was giving us directions and I suddenly realized oh, that's me
[00:40:43.320 --> 00:40:46.360]   Jesus said you didn't know you
[00:40:46.360 --> 00:40:49.080]   You know, I said
[00:40:49.080 --> 00:40:53.160]   Yeah, I just sounded like a nice what I didn't realize it was me because I had recorded years ago
[00:40:53.160 --> 00:40:56.120]   I'd recorded a whole set of directions for our ways
[00:40:56.120 --> 00:40:59.960]   Uh in fact, I posted them somewhere they were somewhere and so people could
[00:40:59.960 --> 00:41:03.000]   anybody
[00:41:03.000 --> 00:41:06.600]   Wait a minute. That's me
[00:41:06.600 --> 00:41:10.200]   Um a little clue
[00:41:10.680 --> 00:41:16.200]   That sounds is that one of those narcissist moments? Oh, man, it's like anti-versus
[00:41:16.200 --> 00:41:22.680]   Great. I didn't say like that sounds good. I said in my mind. I did remember saying oh, that's a nice voice they chose
[00:41:22.680 --> 00:41:28.760]   He sounds friendly. He sounds friendly. I like that guy. I don't know who he is
[00:41:28.760 --> 00:41:32.120]   I like me. I like me
[00:41:37.080 --> 00:41:41.080]   So twitter is not dead. In fact, it looks like dozens according to axios
[00:41:41.080 --> 00:41:45.480]   Dozens of media companies have content deals for 2023
[00:41:45.480 --> 00:41:47.000]   Jesus, can you believe it?
[00:41:47.000 --> 00:41:51.720]   Even ones where they they still haven't let twitter still hasn't let their journalists back on they're still doing stuff
[00:41:51.720 --> 00:41:55.640]   Na foul nba nhl mlb nascar pga tour
[00:41:55.640 --> 00:41:57.400]   Uh
[00:41:57.400 --> 00:42:00.280]   CBS sports turner sports esp and fox
[00:42:00.280 --> 00:42:03.720]   Univision tell or univision tele mundo
[00:42:04.280 --> 00:42:09.240]   The wall street journal nbc you roiders axios. Oh axios is in this list. Oh, yes
[00:42:09.240 --> 00:42:16.360]   Hey, we're doing it too bloomberg Forbes conda nast usa today
[00:42:16.360 --> 00:42:21.560]   Amazing amaze paramount disney
[00:42:21.560 --> 00:42:26.200]   So don't you know twitter for whatever it's going through still
[00:42:26.200 --> 00:42:31.480]   Create and then this is by the way the answer to the rhetorical question. Why are you guys still on there?
[00:42:31.880 --> 00:42:38.120]   It still carries huge amount of cultural weight. It's not buzz. It's all buzz. Yeah, it's far from well
[00:42:38.120 --> 00:42:41.400]   We're in the business of buzz. These are the houses that hype built
[00:42:41.400 --> 00:42:47.400]   That's why they exist. I'm kind of feel very fortunate that I no longer have to worry about buzz
[00:42:47.400 --> 00:42:51.640]   Uh, and I can get off of there and it's not you know, yeah, of course
[00:42:51.640 --> 00:42:55.240]   I had half of more than half a million followers. Although I know that dumber is dwindling
[00:42:55.240 --> 00:43:01.400]   Uh, so any sensible my son's like yelling at me. What are you getting off there for? You got half a million
[00:43:01.880 --> 00:43:04.600]   No, because I don't want to be there and I don't
[00:43:04.600 --> 00:43:06.680]   I'm getting a lot of
[00:43:06.680 --> 00:43:11.080]   I'm getting a lot of new followers that I don't think are real people or if they are real people
[00:43:11.080 --> 00:43:16.920]   I don't know if I want to engage with them. Isn't that funny that I'm not trying to answer for all you like said about bots
[00:43:16.920 --> 00:43:19.960]   Oh, it's done nothing to discourage them
[00:43:19.960 --> 00:43:23.720]   Yeah, because I I'm just like
[00:43:23.720 --> 00:43:25.800]   of them are like
[00:43:25.800 --> 00:43:26.680]   You know
[00:43:26.680 --> 00:43:32.360]   MAGA lover or something and I'm like are you real and if you are real why why are you following?
[00:43:32.360 --> 00:43:35.400]   I am down. I said the same thing. I've gotten 50,000
[00:43:35.400 --> 00:43:36.520]   followers
[00:43:36.520 --> 00:43:38.520]   Yeah, then I'm like I
[00:43:38.520 --> 00:43:44.360]   What makes me interesting for you to follow me, but then with the whole third party api access
[00:43:44.360 --> 00:43:47.000]   Shouldn't that kill some of the bots?
[00:43:47.000 --> 00:43:49.000]   I shouldn't yeah
[00:43:49.000 --> 00:43:55.480]   I uh, I'm down about 10 percent, which is fine. Like I said since I have not moved. I've been at 175 for
[00:43:56.120 --> 00:43:59.160]   It might be completely organic because I've said I'm not tweeting. I'm not doing it
[00:43:59.160 --> 00:44:01.240]   So I might might be completely organic
[00:44:01.240 --> 00:44:07.880]   Uh, revenue you'll see there is down 40 percent a twitter executive told twitter employees
[00:44:07.880 --> 00:44:13.480]   Get ready. Donald Trump according to nbc is coming back to twitter
[00:44:13.480 --> 00:44:20.440]   Oh, yeah, so it's not just follow him then won't follow him now
[00:44:20.440 --> 00:44:23.720]   so
[00:44:23.880 --> 00:44:27.960]   Who who did you say that was it's this little company called nbc
[00:44:27.960 --> 00:44:36.840]   Again, it's all about buzz. Yeah, they're gonna broadcast something and
[00:44:36.840 --> 00:44:41.320]   People are going to talk about it good good or especially if it's really bad
[00:44:41.320 --> 00:44:46.920]   So why not be there they're going to continue to get attention in eyeballs if you're running for president
[00:44:46.920 --> 00:44:53.160]   Which trump says he is you you got to be on twitter. I know I note that joe biden posts all the time on twitter
[00:44:54.120 --> 00:44:56.120]   At least daily, right?
[00:44:56.120 --> 00:45:03.880]   You know, but what about truth social though? What any any word on that and it's he's got a contractual obligation
[00:45:03.880 --> 00:45:07.080]   To stay off a competitor for a certain amount of time
[00:45:07.080 --> 00:45:10.520]   But not that he ever paid attention to contracts. Let's let's say that
[00:45:10.520 --> 00:45:15.000]   I will go in there by the way. He's in good company with elon musk, isn't he?
[00:45:15.000 --> 00:45:17.640]   who has not paid his
[00:45:18.760 --> 00:45:25.720]   His bills, but there was there was a study leo in december in release december 7 2022 by karsten muley
[00:45:25.720 --> 00:45:28.200]   and karlos farts
[00:45:28.200 --> 00:45:31.880]   That's that had four and they looked at twitter after trump was kicked off
[00:45:31.880 --> 00:45:38.680]   The toxicity of tweets sent by trump followers relative to the representer sample dropped by 25
[00:45:38.680 --> 00:45:43.640]   Percent after trump disappeared. Yeah, the effect is larger for pro trump tweets. Of course
[00:45:44.440 --> 00:45:51.320]   Um, it resulted in it reduced the number the total number of tweets suggesting a drop in engagement
[00:45:51.320 --> 00:45:58.040]   And bc also saying that trump yesterday formally petitioned meta
[00:45:58.040 --> 00:46:02.760]   To unblock his account remember that the meta did not the meta
[00:46:02.760 --> 00:46:06.600]   Supervisor board board say they should advisor
[00:46:06.600 --> 00:46:10.440]   They should they should they should read they should not make it
[00:46:11.160 --> 00:46:16.440]   Indeterminate and they should set a time when they decide and I think that they said it was january 6th
[00:46:16.440 --> 00:46:19.000]   It's about about round now. Yeah
[00:46:19.000 --> 00:46:26.840]   One year later or two years later just as just as as meta is trying to get it as far away from any controversy in politics as they can
[00:46:26.840 --> 00:46:31.000]   There was a big story we could go about how they tried to get rid of all politics and it kind of backfired
[00:46:31.000 --> 00:46:35.480]   Because it meant that only crappy outlets were there but they're trying to get away from it
[00:46:35.480 --> 00:46:38.040]   They just want they just want puppies and parties and legless
[00:46:38.680 --> 00:46:45.080]   Virtual human beings. That's all they were a meta spokesperson according to nbc declined to comment about trump beyond saying the company
[00:46:45.080 --> 00:46:49.880]   Quote will announce a decision in the coming weeks in line with the process. We laid out
[00:46:49.880 --> 00:46:56.200]   Again their advisory board said you should reinstate him two years later or they no no they didn't say they
[00:46:56.200 --> 00:46:58.760]   Said a day and then
[00:46:58.760 --> 00:47:02.680]   A day when you when you decide I believe facebook said fault facebook
[00:47:02.680 --> 00:47:08.280]   This is this is again according nbc news ultimately decided to institute a limited ban on trump that would come up
[00:47:08.280 --> 00:47:11.960]   For review ah after two years starting january 7th
[00:47:11.960 --> 00:47:14.920]   So they're reviewing it
[00:47:14.920 --> 00:47:16.840]   right now
[00:47:16.840 --> 00:47:21.720]   The oversight board had another decision yesterday, which is interesting. What was that?
[00:47:21.720 --> 00:47:24.680]   That is that um
[00:47:24.680 --> 00:47:29.400]   They said that you have to reconsider your no
[00:47:29.400 --> 00:47:32.280]   They could female breast rule
[00:47:32.280 --> 00:47:34.920]   This is why I let you
[00:47:34.920 --> 00:47:37.400]   Tell us this story
[00:47:37.400 --> 00:47:41.800]   Yes, uh mainly uh in interest of trans people
[00:47:41.800 --> 00:47:47.640]   Wait a minute not nursing moms wait a minute. Well that that's already been there
[00:47:47.640 --> 00:47:54.840]   They had people they had people who were transitioning who wanted to show their breasts
[00:47:54.840 --> 00:47:57.640]   Yes
[00:47:57.640 --> 00:48:00.120]   And the board said well you ought to let them
[00:48:00.120 --> 00:48:02.760]   Yep
[00:48:02.760 --> 00:48:06.280]   And you want to reconsider your whole it is kind of a double standard
[00:48:06.280 --> 00:48:09.720]   I mean you a man show his breasts. Yep, I remember
[00:48:09.720 --> 00:48:18.680]   It's it's a nipple standard because nipples are what's like pornographic according to but male nipples are not FCC or some mining right
[00:48:18.680 --> 00:48:23.000]   Oh, you know, I mean I get it
[00:48:23.000 --> 00:48:27.080]   I mean it's very american your article if you ever saw
[00:48:27.080 --> 00:48:33.240]   Pictures of just nipples you would be hard pressed to say that's a male nipple or a female nipple
[00:48:33.880 --> 00:48:36.040]   True in some cases really
[00:48:36.040 --> 00:48:42.600]   Somebody I can't remember who somebody did that I can't remember was instagram or whatever
[00:48:42.600 --> 00:48:47.960]   And there was it was just like a bunch of nipples. I mean if they're hairy, okay, maybe but
[00:48:47.960 --> 00:48:52.520]   I'm sorry. We got into this conversation really deeply regretting it
[00:48:52.520 --> 00:49:00.440]   Oversight board overturns your you're like you're like Stacy in the car seeing you're you're gonna try to blame the story or blame me
[00:49:00.680 --> 00:49:04.760]   No, you did it. We were going fine. We were doing all right and you went into the fence
[00:49:04.760 --> 00:49:10.600]   When it's in a fix all right, but what about this
[00:49:10.600 --> 00:49:18.200]   Try another one don't don't we have another story we could talk about yes. Yes. He's right as we are moving on
[00:49:18.200 --> 00:49:23.160]   He's looking desperately for one. No, I'm in fact there was a strain. I don't have it in front of me, but I
[00:49:23.160 --> 00:49:25.320]   Go ahead
[00:49:25.320 --> 00:49:30.520]   Well, I was gonna ask what happened with the 230 news. Yeah, we could do that. I was
[00:49:30.520 --> 00:49:33.640]   It was just was a related story that I saw
[00:49:33.640 --> 00:49:38.760]   On tech dirt. I'll find it that it turns out social media didn't
[00:49:38.760 --> 00:49:42.520]   The Russian planned overturn the 2016 election
[00:49:42.520 --> 00:49:47.080]   No impact didn't really have any impact now you they didn't say anything about
[00:49:47.080 --> 00:49:52.680]   Cable news, which I think probably had some impact but as far as this study could tell
[00:49:53.400 --> 00:49:57.000]   Social media did not change anybody's opinion
[00:49:57.000 --> 00:49:59.640]   Yes, 26. Oh
[00:49:59.640 --> 00:50:03.080]   It's it is in here. I I've had it in yours. It's in mine
[00:50:03.080 --> 00:50:05.880]   You just ignored it because you thought you had nothing
[00:50:05.880 --> 00:50:08.120]   Uh, I did. I just went right to hear stuff
[00:50:08.120 --> 00:50:13.800]   Uh, Mike Masek writing no, of course Russian twitter trolls did not impact the 2016 election
[00:50:13.800 --> 00:50:15.960]   I feel a little vindicated because
[00:50:15.960 --> 00:50:18.920]   And this was on the on my radio show back in
[00:50:20.200 --> 00:50:23.800]   Like november or december 2016. I said that's absurd
[00:50:23.800 --> 00:50:29.720]   It doesn't make sense that a troll farm tweeting or posting on facebook could change anybody's vote
[00:50:29.720 --> 00:50:37.400]   Can change the minds of people right how how weak-minded and idiotic we are as society if that's all it takes
[00:50:37.400 --> 00:50:41.960]   Right the study which is well, that's the third person effect, which is i'm immune
[00:50:41.960 --> 00:50:44.520]   All the media
[00:50:44.520 --> 00:50:47.160]   The study published in nature
[00:50:47.560 --> 00:50:51.080]   Uh looked at whether or not Russian trolls on social media had any impact
[00:50:51.080 --> 00:50:56.440]   They said finally we find no evidence of a meaningful relationship between exposure
[00:50:56.440 --> 00:50:59.800]   They didn't deny that there was a Russian foreign influence campaign
[00:50:59.800 --> 00:51:01.160]   Oh, that's obvious
[00:51:01.160 --> 00:51:05.160]   But they said there was no evidence of a meaningful relationship between exposure to that media
[00:51:05.160 --> 00:51:10.440]   Campaign and changes in attitudes polarization or voting behavior
[00:51:10.440 --> 00:51:13.480]   It's the same as the as the kamer gentelitica
[00:51:14.840 --> 00:51:19.480]   Sheerbs they had no impact on anything and every researcher I know at the time said
[00:51:19.480 --> 00:51:22.760]   This is this is ridiculous. Yeah
[00:51:22.760 --> 00:51:27.320]   The problem with kamer gentelitica was merely that facebook allowed them
[00:51:27.320 --> 00:51:30.600]   To snake a bunch of information about us
[00:51:30.600 --> 00:51:35.160]   But kamer gentelitica all along but it wasn't violation of capabilities
[00:51:35.160 --> 00:51:40.600]   Right, right. It was third party friends of friends information which facebook eventually it shut down
[00:51:41.560 --> 00:51:45.160]   Um, so yeah, I mean look a study is only as good as
[00:51:45.160 --> 00:51:50.440]   You know the methodology and so forth. I don't know. Josh Tucker's very good. This is this is a very good group
[00:51:50.440 --> 00:51:57.240]   Okay, I know this group personally. Yeah, but it kind of honestly I always thought that I it was in politics to say it but I always thought
[00:51:57.240 --> 00:51:58.600]   Yeah
[00:51:58.600 --> 00:52:01.800]   You know, I think watching a lot of fox news might have impacted you
[00:52:01.800 --> 00:52:04.840]   But I think reading a bunch of tweets from a Russian troll form
[00:52:04.840 --> 00:52:07.160]   Yeah, I'm not surprised that didn't have much of a fact
[00:52:07.160 --> 00:52:13.240]   But then there's that whole element of something like a news source such as fox and whomever are our trust where it got
[00:52:13.240 --> 00:52:19.560]   It's trust where this stuff. Oh, they're they're pulling tweets and I would also say the more people around you that are
[00:52:19.560 --> 00:52:25.560]   Behaving in a certain way that maybe it first. It's like, oh, that's extreme
[00:52:25.560 --> 00:52:30.440]   But the more you see it the less extreme it is. Yeah, and then fox could pick that up
[00:52:30.440 --> 00:52:34.520]   Case about q and on as well. Yeah, it does. Yeah
[00:52:34.920 --> 00:52:38.120]   Yeah, nobody said that q and on was not influenced by social
[00:52:38.120 --> 00:52:46.200]   Media. I mean that clearly started in and was fostered by social media completely. Although I know guy it was his yoga class, but
[00:52:46.200 --> 00:52:53.320]   Let's not forget there was there was another study that said you know youtube was supposedly what took you down the rabbit holes
[00:52:53.320 --> 00:52:59.160]   Yeah, and a study found out that almost all of the traffic to these radical videos came from links from outside
[00:52:59.160 --> 00:53:04.040]   Oh, it wasn't the algorithm that did it. YouTube was a repository. We're hosting. That's a problem
[00:53:04.680 --> 00:53:07.400]   But it wasn't youtube that that you know
[00:53:07.400 --> 00:53:12.840]   Radicalized people they came in radicalized looking for the good stuff. Yeah, interesting
[00:53:12.840 --> 00:53:19.720]   Uh, print now tons of this research right now for the next book. So you're gonna bore you. I don't know
[00:53:19.720 --> 00:53:23.320]   I mean I see my child on tick-tock and youtube
[00:53:23.320 --> 00:53:28.680]   Because once it figured out that they they were a teenager they started showing
[00:53:28.680 --> 00:53:31.800]   uh a lot of like
[00:53:32.120 --> 00:53:41.720]   Oh, what's it? There's a word for the anorexia kind of stuff eating disorder videos or yeah a lot of eating disorder videos and kind of stuff that
[00:53:41.720 --> 00:53:47.800]   I know that they have never been any sort of interested in
[00:53:47.800 --> 00:53:53.320]   Do you think she might be influenced though by seeing a lot of those that maybe she'd say you know, I should try not eating
[00:53:53.320 --> 00:53:57.720]   No, they they decided to uninstall tick-tock actually
[00:53:59.720 --> 00:54:02.920]   Miss Stacy. So so those videos where they just
[00:54:02.920 --> 00:54:06.200]   Warnings about the eating disorder or no
[00:54:06.200 --> 00:54:11.160]   Young girls who are showing like what they eaten a day
[00:54:11.160 --> 00:54:18.760]   And then you know, oh gotcha talking about like how much they exercise and tips like if you eat an apple
[00:54:18.760 --> 00:54:23.480]   And seven almonds and then a gallon of water you'll be full all day long
[00:54:23.480 --> 00:54:26.120]   Yeah, there was hashtag
[00:54:26.120 --> 00:54:27.800]   aspiration
[00:54:27.800 --> 00:54:32.360]   Hash yeah hashtag. That's where I was thinking. Yeah hashtag skinny check
[00:54:32.360 --> 00:54:37.160]   I could see it be easy. I could see especially a young person could be
[00:54:37.160 --> 00:54:43.720]   Go I'll try that. I mean I tried intermittent fasting for a while. It's easy to say. Oh, that sounds
[00:54:43.720 --> 00:54:46.600]   That sounds good. That sounds reasonable. Maybe that would help me
[00:54:46.600 --> 00:54:51.480]   Yeah, you did some crazy stuff tick-tock still do crazy stuff
[00:54:51.480 --> 00:54:53.560]   But mostly it's just the internet
[00:54:54.120 --> 00:54:57.560]   Op opens you to new ideas and some of them are not good ideas
[00:54:57.560 --> 00:55:02.600]   Remember what people used to get in giant vats of super cooled liquid
[00:55:02.600 --> 00:55:05.640]   Oh, they still do
[00:55:05.640 --> 00:55:10.200]   They buy those now the plunge pools the cold plunge pools are now at like a thing in the wall street journal
[00:55:10.200 --> 00:55:13.400]   You can read all that I would love to have one myself. I miss them
[00:55:13.400 --> 00:55:19.720]   You just need a big old trash can't talk about an ice tub. I mean, I'm talking about really cool. Kevin rose used to get in like
[00:55:20.600 --> 00:55:28.120]   Yeah, yeah, 20 below. I mean it wasn't just ice. Oh like cryotherapy. Yeah, there were cryo tanks
[00:55:28.120 --> 00:55:35.160]   Oh, there's still cryotherapy. Yeah, but it was a fad. I think it kind of I don't I don't I don't
[00:55:35.160 --> 00:55:45.400]   There were there were actually storefront places you could go remember that there's one near me. Yeah, it's still around one. Yeah
[00:55:45.400 --> 00:55:46.760]   Yeah
[00:55:46.760 --> 00:55:52.360]   Like there's a new one. I know well, please do cryotherapy, but I mean it's like licensed outfits
[00:55:52.360 --> 00:55:55.720]   You know, well, so if you're a pitcher even have it there
[00:55:55.720 --> 00:56:00.040]   If your pitcher and your arm is getting inflamed and you and you want to put a nice bath
[00:56:00.040 --> 00:56:05.880]   I can understand that although there's more and more evidence. That's the wrong. That's not a good thing to do, but okay, really
[00:56:05.880 --> 00:56:09.960]   Yeah, well, I was gonna say and this will be my pro tip for everyone
[00:56:09.960 --> 00:56:15.640]   If you end your showers with like 30 seconds to a minute. No, no, no, no
[00:56:15.640 --> 00:56:21.480]   No, okay, just give me a second. Give me a second. It does make you happy. I love it. Oh
[00:56:21.480 --> 00:56:26.120]   No, no, it'll make you happy when you stop
[00:56:26.120 --> 00:56:34.920]   I love it. It helps with your like mental happiness. It really does go to finland. Fine. They do that in finland
[00:56:34.920 --> 00:56:37.960]   That's funny. I've done this. I don't do that part
[00:56:37.960 --> 00:56:42.520]   When I have morning showers and I do my whatever I need to do in the shower
[00:56:42.520 --> 00:56:48.600]   I always rinse off with cold as long as I could stay in there and I swear that first five seconds
[00:56:48.600 --> 00:56:50.360]   It's painful
[00:56:50.360 --> 00:56:53.400]   But then at that is it's almost like just happy gas
[00:56:53.400 --> 00:56:57.240]   Half of you people are here. The twig twig team
[00:56:57.240 --> 00:57:01.160]   Take cold showers. Well younger half
[00:57:01.160 --> 00:57:08.280]   And I feel great and then again, I love the ice bucket stuff. I miss those from
[00:57:08.840 --> 00:57:12.440]   Actually leo when I are 38 39. We just don't take cold showers
[00:57:12.440 --> 00:57:18.520]   Look old wait a minute. All right. I'll try it tomorrow. So what do you do?
[00:57:18.520 --> 00:57:24.760]   Five seconds. It's hot just the last 30 seconds. Then you just turn the knob all the way to the cold
[00:57:24.760 --> 00:57:29.320]   Yeah, all the way to cold just they scream and bear it you may have to scream
[00:57:29.320 --> 00:57:31.720]   Scream if it's green
[00:57:31.720 --> 00:57:35.080]   Part of it probably right. Yeah, you will scream
[00:57:37.640 --> 00:57:40.440]   And yeah, that's your wake up sound folks
[00:57:40.440 --> 00:57:45.960]   Queen prude is gonna get that every day the funniest thing because you know, she she's never tried anything like that
[00:57:45.960 --> 00:57:50.600]   And I can remember her going giving it a go the first couple of times and yeah, it got loud
[00:57:50.600 --> 00:57:53.000]   What would you probably scream?
[00:57:53.000 --> 00:57:56.360]   Yeah, so what do you think the
[00:57:56.360 --> 00:58:00.360]   Mechanism is uh that makes this work
[00:58:00.360 --> 00:58:03.320]   Well, the same as the stuff
[00:58:03.560 --> 00:58:08.760]   It activates the mammalian dive reflex, but I don't know if that's true dive dive. I'm gonna die
[00:58:08.760 --> 00:58:16.360]   What do you see then triggers the lymphatic system in the body
[00:58:16.360 --> 00:58:22.360]   So the lymphatic system is your uh kind of uh immune system. It's what fights
[00:58:22.360 --> 00:58:27.480]   Oh, this is this is from some this is from a site that's even less reliable than the one I gave you for j-lo's dress
[00:58:27.480 --> 00:58:32.600]   But it says that uh it allows for all toxins and body ways to
[00:58:33.400 --> 00:58:34.600]   Stop
[00:58:34.600 --> 00:58:39.800]   Don't buy that. Yeah, yeah, no, not exactly it does make your blood vessels can stir
[00:58:39.800 --> 00:58:42.520]   constrict thank you. I was like this one
[00:58:42.520 --> 00:58:46.840]   Um and it I think it does
[00:58:46.840 --> 00:58:54.440]   Put out some sort of endorphins because it does you you feel good. You're like, oh, I don't I don't I don't remember if it's a dopamine
[00:58:54.440 --> 00:58:59.640]   But it's definitely some happy gas you you feel a difference. Here's what uh, my weird
[00:58:59.720 --> 00:59:04.040]   But why I love it. Here's what my uh, niva artificial intelligence says
[00:59:04.040 --> 00:59:10.360]   Taking cold showers can have a number of health benefits including stimulating the immune system. That's that lymph thing
[00:59:10.360 --> 00:59:18.040]   Increasing resistance to illness improving circulation deepening sleep spiking energy levels that I believe reducing inflammation
[00:59:18.040 --> 00:59:22.360]   And I get me out of here fast. Yeah, potentially relieving depressive syndromes
[00:59:22.360 --> 00:59:26.520]   Cleveland clinics a good source. Let's read with the cleveland clinic says
[00:59:27.160 --> 00:59:31.720]   Our cold showers good for you. Now. This is a cold shower. I'm not taking a cold shower
[00:59:31.720 --> 00:59:34.120]   No, you don't take it. That's the whole point. You don't
[00:59:34.120 --> 00:59:37.560]   Get in there and get comfortable. It has to be a shock
[00:59:37.560 --> 00:59:40.360]   You know for it to work
[00:59:40.360 --> 00:59:45.960]   All right, I will try it tomorrow. I will I'll absolutely try it tomorrow stimulates blood full flow
[00:59:45.960 --> 00:59:52.360]   It's good for your overall health. Your skin gets clearer and healthier with increased circulation because the blood okay the blood jumps in
[00:59:53.640 --> 00:59:57.560]   But there are better ways to get your blood pumping than the false chiffrin go for it
[00:59:57.560 --> 01:00:03.800]   You guys don't I agree. I agree. If you're gonna do the torture part. Do you do the sauna part? I do
[01:00:03.800 --> 01:00:07.880]   I like you take a hot shower and then yeah, you're but you end in
[01:00:07.880 --> 01:00:12.120]   Also, it's great for your skin and it's great for your hair
[01:00:12.120 --> 01:00:17.720]   Because it closes off your hair follicles. So it's still pink and my hair is still white
[01:00:17.720 --> 01:00:22.840]   I'm not gonna be a clinical trial in the netherlands found the cold showers now. These are cold showers
[01:00:22.840 --> 01:00:28.200]   Not just brief ones, but cold showers 29% reduction in people calling off sick from work
[01:00:28.200 --> 01:00:32.520]   Another study connected cold showers to improve cancer survival
[01:00:32.520 --> 01:00:37.560]   On the mental health side researchers found cold showers may help relieve symptoms of depression
[01:00:37.560 --> 01:00:41.560]   Yeah, that's why I started doing it and it does help you guess
[01:00:41.560 --> 01:00:44.760]   happy gas
[01:00:44.760 --> 01:00:50.840]   Okay, I'll try it. I'm uh see now this by the way, and you can start with 10 seconds
[01:00:51.000 --> 01:00:55.800]   This is exactly what we're just talking about. That's enough pain. This is exactly. We're just talking about
[01:00:55.800 --> 01:00:58.520]   This was an experiment to see
[01:00:58.520 --> 01:01:05.720]   If tecta was wrong that in fact you could be influenced by social media. Well, and leo has been
[01:01:05.720 --> 01:01:08.520]   Yeah, leo will leo is a good place
[01:01:08.520 --> 01:01:15.160]   I'm not doing it. I'm not taking cold showers. I'm not walking on board. Okay. And jeff has not jeff was not
[01:01:15.160 --> 01:01:17.400]   He is strong-minded
[01:01:18.200 --> 01:01:21.480]   Yes stubborn now. Let's take a break then we come back
[01:01:21.480 --> 01:01:27.560]   Injure in our irc does make a good point says ask your doc first you may cause heart stress
[01:01:27.560 --> 01:01:31.320]   Hence why only 10 seconds or so is totally fine
[01:01:31.320 --> 01:01:34.280]   because you'll feel a difference
[01:01:34.280 --> 01:01:36.440]   but it has to be
[01:01:36.440 --> 01:01:43.080]   Not a gradual turn of the knob to to go to cooler temperatures if I am not here next week full board
[01:01:43.080 --> 01:01:47.080]   You will know why
[01:01:47.960 --> 01:01:51.720]   All right, we're going to talk about the supreme court in section 230
[01:01:51.720 --> 01:01:58.520]   Uh, they haven't heard arguments yet. Have they on this? They're just they're briefs coming brief from google now
[01:01:58.520 --> 01:02:02.600]   But arguments still to come but they are hearing and actually
[01:02:02.600 --> 01:02:07.080]   I have uh, you know what this this particular
[01:02:07.080 --> 01:02:14.040]   Case is somewhat persuasive to me and i'll tell you what the debate is over and when we continue in just a bit
[01:02:14.280 --> 01:02:18.840]   First to read the google brief to be persuaded the other way. No, you're obviously easily persuaded
[01:02:18.840 --> 01:02:21.320]   Because you're going to go take a shower tomorrow
[01:02:21.320 --> 01:02:25.560]   The good ice I will tell you the google brief persuaded me otherwise
[01:02:25.560 --> 01:02:28.440]   And I think it'll do the same with the justices
[01:02:28.440 --> 01:02:30.840]   But i'll tell you why in a second
[01:02:30.840 --> 01:02:32.840]   I do want to talk about you've probably seen this
[01:02:32.840 --> 01:02:35.640]   And you're going to see signage and so forth
[01:02:35.640 --> 01:02:39.400]   during the shows a little lower third pops up that says
[01:02:39.400 --> 01:02:43.880]   That the studios are brought to you by the great folks at aci
[01:02:44.520 --> 01:02:48.920]   Learning we've for years in fact since they were founded talked about it pro right?
[01:02:48.920 --> 01:02:56.920]   It pro is now part of aci learning together it pro and aci learning are expanding
[01:02:56.920 --> 01:03:00.520]   Expanding their production capabilities. It's a matchmate in heaven
[01:03:00.520 --> 01:03:05.800]   So now you get the content and the style of learning you need at any stage in your development
[01:03:05.800 --> 01:03:09.800]   If you want to get ahead in it there is no better way to do it
[01:03:09.800 --> 01:03:13.800]   Whether you want individual training for yourself or you want to train your whole team
[01:03:13.800 --> 01:03:17.560]   aci learning at it pro have you covered
[01:03:17.560 --> 01:03:25.320]   227,000 members of the it pro learning community. I suspect many of them are listeners here
[01:03:25.320 --> 01:03:28.200]   Uh, we know there's a great overlap
[01:03:28.200 --> 01:03:34.920]   6800 hours of content and now thanks to aci learning there's new content and a lot of areas being added
[01:03:35.960 --> 01:03:44.680]   It's pretty exciting. It's of course team training is available for comp tia and microsoft it and sisco and linux and apple and security and cloud and more
[01:03:44.680 --> 01:03:51.640]   One of the most widely recognized certifications that most people start their career with is the comp tia a plus
[01:03:51.640 --> 01:03:53.800]   That's the desktop support
[01:03:53.800 --> 01:03:55.000]   assert
[01:03:55.000 --> 01:03:59.480]   And the best way to get that cert comp tia courses from it pro and aci learning
[01:03:59.480 --> 01:04:03.240]   It makes it easy for you to level up or your employees to level up
[01:04:04.200 --> 01:04:08.120]   Especially if they have and you should have a vested interest in cyber security
[01:04:08.120 --> 01:04:13.000]   The most popular certs offered by aci learning includes cis sp a w s
[01:04:13.000 --> 01:04:17.080]   isaka is aca and ccna
[01:04:17.080 --> 01:04:23.400]   other in-demand tech skills and certification courses offered or technical support specialist
[01:04:23.400 --> 01:04:25.880]   computer users support specialist
[01:04:25.880 --> 01:04:28.520]   Information security analyst and much more
[01:04:28.520 --> 01:04:33.800]   I think as an individual if you're listening you're thinking those would be such great skills to bring to the job
[01:04:33.800 --> 01:04:40.520]   And if you're a company with a security team or an it team you've got to be thinking I need my team to have those skills
[01:04:40.520 --> 01:04:44.280]   Serts are more than just you know proving a skill set they let
[01:04:44.280 --> 01:04:49.080]   Customers they let clients and then employers see that you're committed
[01:04:49.080 --> 01:04:56.440]   To keeping your knowledge up to date and if you've got an organization, it's a great way of showing your customers. Yeah, we we pay attention to this
[01:04:56.440 --> 01:05:02.360]   aci learning and it pro are with you every step of the way with an it pro business plan
[01:05:02.920 --> 01:05:05.560]   aci learning offers fully customizable training
[01:05:05.560 --> 01:05:10.200]   The dashboard is fantastic. You can track your teams results manage your seats
[01:05:10.200 --> 01:05:12.520]   Assign and unassigned
[01:05:12.520 --> 01:05:16.440]   Individual team members or group team members you could access monthly usage reports
[01:05:16.440 --> 01:05:20.200]   You'll see metrics like logins viewing time tracks completed and more
[01:05:20.200 --> 01:05:26.440]   It's very easy to manage your teams too. You could say you you need to study this you you guys you need to study this
[01:05:26.440 --> 01:05:32.280]   You can customize the assignments fully keep an eye on progress report on usage, which is helpful
[01:05:32.680 --> 01:05:35.960]   If you're showing the boss, this is you know, we're we're doing it. We're learning it
[01:05:35.960 --> 01:05:40.120]   And your team will love it because it's such good quality content
[01:05:40.120 --> 01:05:42.600]   fun engaging
[01:05:42.600 --> 01:05:44.840]   informative they appreciate it
[01:05:44.840 --> 01:05:52.200]   These assignments can be full courses, but they also can be individual efforts episodes within courses and because there's transcripts for every course
[01:05:52.200 --> 01:05:56.360]   You could fight exactly that thing you're trying to learn or try to get your staff to learn
[01:05:56.360 --> 01:06:01.320]   And you get the best advanced reporting immediate insight into your team's viewing patterns and progress
[01:06:01.720 --> 01:06:04.840]   You can do it over any period of time the reports can be very visual
[01:06:04.840 --> 01:06:07.480]   again the boss likes pictures
[01:06:07.480 --> 01:06:13.000]   respective companies and government agencies around the globe use it pro and aci learning
[01:06:13.000 --> 01:06:17.720]   Year after year to help them maintain their competitive edges really the best name
[01:06:17.720 --> 01:06:20.680]   in training
[01:06:20.680 --> 01:06:26.600]   Supporting organizations across more than just it now it's audit it cyber security
[01:06:26.600 --> 01:06:30.120]   aci learning keeps you and your team at the top
[01:06:30.760 --> 01:06:34.840]   Of your game from entry level training to putting people on the moon
[01:06:34.840 --> 01:06:43.480]   ACI learning has you covered maintain your company's competitive edge with aci learning visit aci learning
[01:06:43.480 --> 01:06:46.360]   dot com
[01:06:46.360 --> 01:06:53.400]   It's a great partnership producing fantastic content that'll help you get ahead in the business and help your team
[01:06:53.400 --> 01:06:56.760]   Learn the things it needs to know to keep your company
[01:06:56.760 --> 01:06:59.400]   safe aci learning
[01:06:59.880 --> 01:07:06.920]   Dot com with thank aci learning so much for supporting not only twig but supporting the entire operation here with your studio sponsorship
[01:07:06.920 --> 01:07:11.960]   We are coming to you from the twit studios by aci learning. Thank you aci learning
[01:07:11.960 --> 01:07:18.520]   All right, so let's first of all talk about the supreme court case because it's an interesting case
[01:07:18.520 --> 01:07:21.480]   uh, it is actually about
[01:07:21.480 --> 01:07:24.440]   radicalization
[01:07:24.440 --> 01:07:28.440]   Specifically it's a it is a suit against youtube am I right?
[01:07:29.640 --> 01:07:33.320]   Uh, which is why google's brief is you know important
[01:07:33.320 --> 01:07:36.120]   uh
[01:07:36.120 --> 01:07:40.360]   The the issue is gonzalez versus google llc
[01:07:40.360 --> 01:07:44.840]   uh brought by the gonzalez family
[01:07:44.840 --> 01:07:50.920]   who uh, let me see if I can find the facts of the case so I can I can
[01:07:50.920 --> 01:07:57.000]   She was in the brief is it in the brief? Okay good. I'll go over the brief
[01:07:57.640 --> 01:08:00.760]   She was one of the victims of that horrific
[01:08:00.760 --> 01:08:03.640]   uh paris shooting right
[01:08:03.640 --> 01:08:07.480]   Let me open this brief up here so we can talk about it
[01:08:07.480 --> 01:08:12.040]   Oh, it's many pages many many pages
[01:08:12.040 --> 01:08:16.280]   Slowly 68, um, but there's got a handy table of contents
[01:08:16.280 --> 01:08:19.320]   Um
[01:08:19.320 --> 01:08:23.880]   She so the family her family is suing. No actually been that it's not it's not in there. Yeah
[01:08:24.520 --> 01:08:28.120]   Potentially content that youtube violated the anti-terrorism act
[01:08:28.120 --> 01:08:31.000]   by displaying isis
[01:08:31.000 --> 01:08:34.280]   videos to users watching similar
[01:08:34.280 --> 01:08:37.560]   videos in other words recommending those videos
[01:08:37.560 --> 01:08:42.840]   To those users and thereby radicalizing them causing them to become terrorists
[01:08:42.840 --> 01:08:45.560]   causing uh
[01:08:45.560 --> 01:08:51.080]   This mass killing before that they were perfectly normal, but go ahead well as in paris so
[01:08:51.960 --> 01:08:55.240]   Here's the issue. Here's the here's what I would propose
[01:08:55.240 --> 01:08:57.880]   Here's what I would propose
[01:08:57.880 --> 01:08:58.840]   Uh
[01:08:58.840 --> 01:09:03.640]   I don't think section 230 should in fact protect algorithmic recommendations
[01:09:03.640 --> 01:09:09.560]   I want you to make the case for that jeff. I think section 230 should protect me and google
[01:09:09.560 --> 01:09:12.120]   uh against those videos on google
[01:09:12.120 --> 01:09:14.280]   But if google says
[01:09:14.280 --> 01:09:18.840]   Uh or twitter or facebook or even twit if we start recommending
[01:09:19.880 --> 01:09:23.880]   Radicalizing content that is not I don't believe that you're protected
[01:09:23.880 --> 01:09:28.040]   But what does that algorithm start from they don't just appear on their own, right?
[01:09:28.040 --> 01:09:30.760]   Who's responsible for the algorithm?
[01:09:30.760 --> 01:09:32.680]   Google
[01:09:32.680 --> 01:09:34.680]   Well, let me let me try to answer your question
[01:09:34.680 --> 01:09:37.240]   I'm gonna start not with 230 with the first amendment
[01:09:37.240 --> 01:09:41.320]   And this is the piece that I wrote in a british publication a week ago
[01:09:41.320 --> 01:09:43.720]   that choice
[01:09:43.720 --> 01:09:45.720]   is speech
[01:09:45.800 --> 01:09:51.480]   You choose who to have on this show and editor decides is nacy's leaving she knows she's in for a waffle time
[01:09:51.480 --> 01:09:54.680]   Um, it's the great thing about the zoom thing. We can now see this
[01:09:54.680 --> 01:09:57.160]   So yeah, i'll spend a minute
[01:09:57.160 --> 01:10:03.000]   We have a gentleman's rule not to mention. I know we don't but i think it's fun. Okay, um
[01:10:03.000 --> 01:10:07.960]   So, uh, because she knows what what jeff gets going. I got four minutes. I can do it
[01:10:07.960 --> 01:10:10.520]   So so choice is
[01:10:10.520 --> 01:10:12.280]   itself speech
[01:10:12.280 --> 01:10:13.080]   right
[01:10:13.080 --> 01:10:15.640]   Um, an editor a publisher a platform
[01:10:15.640 --> 01:10:19.240]   I'll be able to leave me
[01:10:19.240 --> 01:10:26.600]   Jeff I can hear you when I leave. I know I want you to know that I know that I thought it was funny
[01:10:26.600 --> 01:10:29.080]   Uh, I got to get my water
[01:10:29.080 --> 01:10:30.680]   Oh water, huh?
[01:10:30.680 --> 01:10:37.400]   And I you know and if you were going to explain how the first amendment relates to section 230. I knew i had plenty of time
[01:10:37.400 --> 01:10:39.400]   You're out of here. Yeah
[01:10:40.040 --> 01:10:44.600]   The point is can speech includes choice and the choice of
[01:10:44.600 --> 01:10:49.240]   What you choose to carry is protected by the first amendment in this country
[01:10:49.240 --> 01:10:53.000]   Uh and should be protected by the spring court forget 230
[01:10:53.000 --> 01:10:57.960]   When you get to 230 and say well, they shouldn't be protected from this well against what?
[01:10:57.960 --> 01:11:04.520]   Um to be sued for carrying it. Okay, but it's not a is it a legal speech?
[01:11:04.520 --> 01:11:06.120]   Um
[01:11:06.120 --> 01:11:08.920]   Then they are they prosecuted. What do we say about that?
[01:11:09.480 --> 01:11:10.600]   um
[01:11:10.600 --> 01:11:13.400]   but 230 protects them from the liability
[01:11:13.400 --> 01:11:15.080]   of
[01:11:15.080 --> 01:11:19.400]   Others actions and I think it's absolutely vital because the point is at scale
[01:11:19.400 --> 01:11:23.400]   Algorithms are going to be necessary
[01:11:23.400 --> 01:11:27.400]   To give us what we want in recommending content
[01:11:27.400 --> 01:11:31.640]   Okay, so that's what you're saying is is because
[01:11:31.640 --> 01:11:38.600]   We're just trying to not we people are trying to give their customers what they want be at twitter google
[01:11:38.680 --> 01:11:42.520]   What have you and they're going about it from a level of code
[01:11:42.520 --> 01:11:49.560]   The code should be protected and and the people right in that code should be protected down rhythms to be protected
[01:11:49.560 --> 01:11:51.720]   I think so. Yes
[01:11:51.720 --> 01:11:53.720]   I think you're wrong there
[01:11:53.720 --> 01:12:00.840]   I think your argument is basically like we we have to trust this because it's the only way we can get content out to people at scale
[01:12:00.840 --> 01:12:03.160]   so
[01:12:03.160 --> 01:12:05.160]   I don't think but
[01:12:05.160 --> 01:12:10.120]   The new york times now uses an algorithm to recommend content. Here's my here's my reason. I don't
[01:12:10.120 --> 01:12:15.720]   I think I understand what you're saying. I understand what you're saying and you're right choice is it
[01:12:15.720 --> 01:12:17.960]   Is it is a first amendment right? But
[01:12:17.960 --> 01:12:21.480]   I don't want to lose 230 protection
[01:12:21.480 --> 01:12:29.320]   Because we went to the wall to protect the alga. That's a political question. Well, but it's not a principle question. Okay, but ultimately
[01:12:29.320 --> 01:12:33.880]   You're negotiating already. But we need to negotiate because we're going to lose it
[01:12:35.000 --> 01:12:41.240]   There's a strong risk of losing it and I'll tell you what if so I'm not protecting somebody's algorithm
[01:12:41.240 --> 01:12:45.960]   If that means I can't have a chatroom. I can't have comments. I can't have a forum
[01:12:45.960 --> 01:12:50.760]   I can't have a mastodon because none of that is any longer protected by 230
[01:12:50.760 --> 01:12:53.000]   But it's not just the algorithm, right?
[01:12:53.000 --> 01:12:55.640]   I finally figured out a way to put this recently is you have the left
[01:12:55.640 --> 01:13:00.440]   Is where you have widened in 230 said we're going to create a sword and a shield
[01:13:00.840 --> 01:13:06.360]   A shield so that you're not liable for others actions a sword so that you are free to
[01:13:06.360 --> 01:13:09.800]   moderate and have a better discussion to get rid of junk
[01:13:09.800 --> 01:13:12.280]   And and we're gonna give you both well
[01:13:12.280 --> 01:13:16.440]   The left is going after the shield because they say well you shouldn't be protected
[01:13:16.440 --> 01:13:18.680]   You should be going after this bad stuff. You should go rid of bad stuff
[01:13:18.680 --> 01:13:23.720]   The right is going after the sword saying how dare you take down our bad stuff. That's our stuff
[01:13:23.720 --> 01:13:28.040]   And we're gonna require you in texas and and florida to keep it up
[01:13:28.680 --> 01:13:32.600]   And so it's part of that larger discussion. So if you I guess you were to say in leo
[01:13:32.600 --> 01:13:36.760]   Okay, let's negotiate this but there's no negotiation with
[01:13:36.760 --> 01:13:42.760]   Where this discussion is headed. That's the problem. But do you but I've always said these algorithms are a bad idea
[01:13:42.760 --> 01:13:48.760]   No, no, I just I think they're a bad idea. I think there's a difference in editorial judgment as in the new york times
[01:13:48.760 --> 01:13:51.800]   Picking what's on its front page. There's a very different
[01:13:51.800 --> 01:13:55.160]   Thing from that and something like what meta facebook
[01:13:55.640 --> 01:14:02.040]   Google world of warcraft due to generate more revenue by promoting extremist content
[01:14:02.040 --> 01:14:03.320]   You know what? I'm missing
[01:14:03.320 --> 01:14:09.000]   I'm you know what just you talked about the new york times algorithm being like recommending stories to you
[01:14:09.000 --> 01:14:13.720]   That's less offensive because the new york times has an editor that puts up all of their stories
[01:14:13.720 --> 01:14:17.160]   So if i'm saying from this existing curated list
[01:14:17.160 --> 01:14:20.920]   I'm recommending so only new york times writers get protected
[01:14:20.920 --> 01:14:24.760]   But we schmucks out here in social media and blog and don't
[01:14:25.560 --> 01:14:29.640]   That's why i'm saying with the no problem. The icis videos are not protected first of all
[01:14:29.640 --> 01:14:32.920]   Yeah, in fact one of google's points is oh, we took care of that a long time ago
[01:14:32.920 --> 01:14:38.360]   Uh the icis by the way, not protected. There's no proof that the people involved saw those videos
[01:14:38.360 --> 01:14:43.160]   Yeah, none so okay. So maybe on the on the actual facts of this case there isn't a case
[01:14:43.160 --> 01:14:46.760]   But I don't want to go to the wall defecting
[01:14:46.760 --> 01:14:53.480]   Defending the right of these massive tech companies to use algorithms to encourage
[01:14:54.440 --> 01:14:57.320]   revenue growth through engagement let me try the algorithm
[01:14:57.320 --> 01:14:59.960]   10 to point you in directions
[01:14:59.960 --> 01:15:05.960]   And twitter does this too where it's more you're more outraged you are you complain just complain about what you're seeing okay?
[01:15:05.960 --> 01:15:08.600]   So let me see the reason
[01:15:08.600 --> 01:15:15.320]   Let me finish the reason you're seeing laurin bobert on twitter is because twitter has decided on buss said so well
[01:15:15.320 --> 01:15:18.600]   No, because I believe the algorithm says no
[01:15:19.080 --> 01:15:25.560]   Oh, no excited upset this gets jeff see how much more he tweets see how many more hours he spends on twitter
[01:15:25.560 --> 01:15:30.200]   That's purely political purely political. I'm not seeing uh bernie sanders
[01:15:30.200 --> 01:15:35.880]   Well, i would submit the elons right to promote laurin bobert to you is protected speech
[01:15:35.880 --> 01:15:42.040]   The algorithms right to promote laurin bobert to you to give you a different center money is not protected speech
[01:15:42.040 --> 01:15:46.360]   Okay, let's take let's take economics out of this for a moment if we may we can't and let's go
[01:15:46.600 --> 01:15:49.560]   Let's wait wait wait let's go to mastodon
[01:15:49.560 --> 01:15:52.600]   Now and mastodon I miss
[01:15:52.600 --> 01:15:59.160]   An algorithm because there's stuff overnight that I miss that I wish I could see I want an algorithm that says hey jeff
[01:15:59.160 --> 01:16:02.760]   Here's the good stuff you missed and i'm seeing some examples of this people are working on out there
[01:16:02.760 --> 01:16:06.360]   The algorithm and algorithm for algarden's sake is not evil
[01:16:06.360 --> 01:16:10.920]   An algorithm can be done badly in the case of the lower laurin bobert
[01:16:10.920 --> 01:16:14.440]   And I and I guarantee you that's purely political not economic
[01:16:15.000 --> 01:16:17.000]   Um, but whatever
[01:16:17.000 --> 01:16:21.080]   But an algorithm if we demonize algorithms if we demonize math
[01:16:21.080 --> 01:16:25.160]   We demonize knowledge that's going to be a bigger problem here
[01:16:25.160 --> 01:16:31.080]   And I can see in an activity pub world where there may be five or six algorithms that I will choose to have
[01:16:31.080 --> 01:16:34.120]   Because they find stuff that I care about
[01:16:34.120 --> 01:16:40.280]   And they're not economically motivated and I can choose them and they're optional and they're transparent
[01:16:40.280 --> 01:16:43.080]   I know what i'm getting my point is algorithms aren't evil
[01:16:43.480 --> 01:16:45.960]   Well, I think what you're saying is advertising is evil
[01:16:45.960 --> 01:16:51.160]   Well, that's a different problem. I think the fact that jeff javis is too lazy to scroll back through his feet is a lousy
[01:16:51.160 --> 01:16:55.080]   Reason for us to defend algorithms on any platform
[01:16:55.080 --> 01:17:03.080]   Um algorithms are the problem with the algorithm is my problem is that algorithms are being forced. I should be able to just
[01:17:03.080 --> 01:17:06.280]   Go through chronologically. I'll submit to you
[01:17:06.280 --> 01:17:08.840]   The real problem with algorithms
[01:17:08.920 --> 01:17:12.920]   And is what they're what they're optimizing for
[01:17:12.920 --> 01:17:19.720]   So if an algorithm is merely optimizing to surface the stuff that you would have been interested in overnight
[01:17:19.720 --> 01:17:24.040]   That's benign, but if the algorithm and I submit this is what it's being
[01:17:24.040 --> 01:17:29.480]   It's what it's being optimized for is to generate more engagement and you just demonize
[01:17:29.480 --> 01:17:33.000]   Algorithms as a whole no. Yeah, no, that's a business
[01:17:33.000 --> 01:17:38.520]   I'm not demonizing algorithms as a whole the problem is we can't impute what the intent is of an algorithm
[01:17:39.080 --> 01:17:43.480]   But I can guarantee you that that's what happens. They don't algorithmize
[01:17:43.480 --> 01:17:49.480]   Your your tweets that that you see overnight because oh, I think jeff would be interested in this
[01:17:49.480 --> 01:17:54.040]   They're saying we must I like the algorithm. I chose
[01:17:54.040 --> 01:17:58.920]   Over the long it understand how it works from a computer programmer's point of view
[01:17:58.920 --> 01:18:02.200]   He there's it's far too hard to write that algorithm
[01:18:02.200 --> 01:18:07.080]   But here's an easy algorithm to write how much time did jeff spend on the platform yesterday?
[01:18:07.320 --> 01:18:11.320]   All right, let's tweak it. Did he spend more time? Yeah, let's tweak it. Did he spend more time?
[01:18:11.320 --> 01:18:14.600]   And that's how it does because no programmer is going to write in well
[01:18:14.600 --> 01:18:21.080]   This person's interested in a B and C. So we should give him more dog pictures. He likes those. So let me tell you this
[01:18:21.080 --> 01:18:27.080]   It's much easier and this is how every single algo works on every single social platform
[01:18:27.080 --> 01:18:33.880]   Including tick tock including world of warcraft including google facebook trust me. I understand how it's coded
[01:18:34.360 --> 01:18:38.600]   Is it's directly tied to engagement? I'm gonna give you a good algorithm right now
[01:18:38.600 --> 01:18:44.360]   If you go to at colarusso_algo at law dot builders on master dot
[01:18:44.360 --> 01:18:50.120]   This is an algorithm that goes through this guy's feed but no business is implementing that
[01:18:50.120 --> 01:18:54.840]   Because that's not what a business is nothing wrong with an algorithm about profit
[01:18:54.840 --> 01:18:59.800]   Well profits the New York Times makes a profit too profits not evil you communist
[01:19:00.840 --> 01:19:07.480]   No, it's not I absolutely i'm not going to the wall to defend google or facebook or twitter's right
[01:19:07.480 --> 01:19:12.440]   To keep us engaged by keeping us outraged in in order to protect sex
[01:19:12.440 --> 01:19:17.960]   That is not going to happen. Well, I think you've thrown the baby out with the constitution
[01:19:17.960 --> 01:19:21.640]   You've you've just gone over the line might
[01:19:21.640 --> 01:19:27.880]   Stated arms it would be fine if again if the algorithm wasn't just spitting out
[01:19:28.760 --> 01:19:35.000]   Again, I started the show saying i'm not really on twitter or any social media much nowadays
[01:19:35.000 --> 01:19:41.080]   Unless i'm just broadcasting because what's being presented to me is a bunch of depressing and angry stuff
[01:19:41.080 --> 01:19:44.360]   I don't know how you get that because it's not to happen with me my friend
[01:19:44.360 --> 01:19:51.240]   And so i'm not sure what's happening. I hate that normally i pop it open and I just see my community of people
[01:19:51.240 --> 01:19:58.600]   But now what i'm seeing is my community of people are angry because they've been fed something and it's just a vicious cycle of
[01:19:58.680 --> 01:20:05.720]   So many depressing stories of so many ups upsets you also like the news right you said the same thing about about news
[01:20:05.720 --> 01:20:12.840]   Yeah, I say right yeah, and I could say the exact same thing about local news because if i turn on just because it's pretty much the same way
[01:20:12.840 --> 01:20:18.920]   Yeah, and just because the algorithm may be working for you jeff doesn't mean it works for everybody
[01:20:18.920 --> 01:20:21.960]   I'm not saying it does but it also means it does it does not work for everybody
[01:20:21.960 --> 01:20:25.560]   And so if you want to eliminate the algorithm for me too, you've taken away a tool
[01:20:26.280 --> 01:20:32.520]   Yeah, they take it away. That's like arguing like don't take away something that benefits me even if it harms others
[01:20:32.520 --> 01:20:35.240]   That's that's silly. No
[01:20:35.240 --> 01:20:40.680]   No, i'm not saying that what i'm saying is what you've made your choice platforms
[01:20:40.680 --> 01:20:41.640]   Yeah, I agree with you
[01:20:41.640 --> 01:20:42.840]   I agree with your choice
[01:20:42.840 --> 01:20:49.480]   Let me just do the chronological old school way of going back and seeing what people posted that actually give us
[01:20:49.480 --> 01:20:52.200]   give a care about
[01:20:52.440 --> 01:20:57.080]   Um, versus just suggesting things to me because everybody else is talking about it
[01:20:57.080 --> 01:20:58.920]   Give me a choice
[01:20:58.920 --> 01:21:05.160]   John is a Zitrain at harvard argued. I was in the room when he argued at the facebook at pardon me Davos when I used to get invited
[01:21:05.160 --> 01:21:06.840]   um
[01:21:06.840 --> 01:21:11.800]   Where he said you know what you should do you should give everybody a dial and if they want to choose the
[01:21:11.800 --> 01:21:13.000]   um
[01:21:13.000 --> 01:21:17.720]   Disney algo that's all cleaned up or they want to choose the Alex jones algo
[01:21:18.360 --> 01:21:22.040]   Give it make that the responsibility of the user and then you're gonna get in less trouble
[01:21:22.040 --> 01:21:27.000]   By the way because the users they chose it. It wasn't forced on well, but users are choosing it on an engagement
[01:21:27.000 --> 01:21:32.440]   I mean, that's kind of what leo leo's dial it or leo's and and that's they're right
[01:21:32.440 --> 01:21:37.160]   They want to watch Alex shows what let him watch Alex joe. I think he's awful. It's a choice
[01:21:37.160 --> 01:21:41.560]   Yeah, but it's no one saying they can't watch alex jones or even isos videos
[01:21:41.560 --> 01:21:46.120]   Uh, but what i'm saying is it shouldn't be promoting those
[01:21:46.760 --> 01:21:48.760]   Uh complete it doesn't come by the way
[01:21:48.760 --> 01:21:54.120]   Uh you need to understand how computers and algorithms work jeff before you start defending algorithms
[01:21:54.120 --> 01:22:00.520]   The algorithm. I need you also need to read research on this that says that this is not what's happening at youtube
[01:22:00.520 --> 01:22:05.160]   Well, i'm just saying it's a lot harder to write an algorithm
[01:22:05.160 --> 01:22:07.720]   and almost
[01:22:07.720 --> 01:22:13.160]   A fail like look at the recommendation engines by the way how poorly they do those those do not work
[01:22:13.480 --> 01:22:17.800]   What works really well is a feedback loop between your time spent
[01:22:17.800 --> 01:22:21.640]   You're the amount of engagement the amount of tweeting the amount of tweets you read
[01:22:21.640 --> 01:22:28.200]   And the content and it doesn't care about what the content is it's just says well when we serve him this he spends more time here
[01:22:28.200 --> 01:22:33.160]   Is staying so we should give him more of that. That's easy. I'll go to write. That's how tiktok works
[01:22:33.160 --> 01:22:36.120]   That's how every algorithm i've ever seen on social works
[01:22:36.840 --> 01:22:41.400]   My tiktok problem is if you look at the netflix recommendations, they suck
[01:22:41.400 --> 01:22:46.600]   You know why they don't have that feedback loop all they they're trying to recommend something based on
[01:22:46.600 --> 01:22:53.640]   You know your interests that doesn't work. That's why none of that's not enough programming, but that's another story
[01:22:53.640 --> 01:22:59.160]   Well, i'm all depressing. I can tell you exactly how all the algorithms on social networks work
[01:22:59.160 --> 01:23:04.600]   And has nothing to do with goodness or what i'm saying you're setting a prestige here
[01:23:05.000 --> 01:23:09.400]   In wanting the supreme court to say i'll go evil you're setting a precedent
[01:23:09.400 --> 01:23:15.880]   That affects so much more. No, and so i'm not willing to die on that hill you are i'm not i don't i don't
[01:23:15.880 --> 01:23:21.160]   You're gonna take a cold shower. I'm not gonna. I don't want to say i'll go evil. I do you want to say
[01:23:21.160 --> 01:23:24.520]   Allow me to step away from it if I and I agree with that
[01:23:24.520 --> 01:23:30.280]   I agree with that in the recommendation stuff. It's smart. That is a whole different ball of why it's like you said mr
[01:23:30.280 --> 01:23:35.960]   Laport. Um, I think that takes more effort on the consumer side of things because yeah
[01:23:35.960 --> 01:23:40.360]   YouTube used to be pretty bad for me and netflix was pretty bad, but
[01:23:40.360 --> 01:23:47.400]   The effort that I did with hitting those stupid thumbs up thumbs down and all of that on the things that I watch
[01:23:47.400 --> 01:23:51.400]   It did tend to make a difference after a while so now the stuff that I get it's is
[01:23:51.400 --> 01:23:56.120]   Useful, but everybody is not gonna do that because most people have a life
[01:23:56.120 --> 01:23:59.720]   I don't you have to you're gaming the algorithm and you can game the algorithm
[01:23:59.800 --> 01:24:05.320]   But you can only game it temporarily. They know perfectly well eventually you're gonna give up and just watch what we tell you to watch
[01:24:05.320 --> 01:24:10.520]   They'll throw that number one. What is it? This says
[01:24:10.520 --> 01:24:13.320]   Number one is a
[01:24:13.320 --> 01:24:17.720]   Does it want to watch it just because big tech does it or doesn't do it doesn't mean it's right or wrong
[01:24:17.720 --> 01:24:21.480]   I think that is a mistake. I'm not gonna buy into that uh, but
[01:24:21.480 --> 01:24:27.560]   They don't have an interest in promoting the social wheel. They have an interest in making money
[01:24:28.040 --> 01:24:33.000]   And and that is the capitalist way and I don't think that that has anything to do with free speech
[01:24:33.000 --> 01:24:35.960]   Or protect his patreon section to 30
[01:24:35.960 --> 01:24:40.120]   But well, what what about every book publisher? What about your company right here?
[01:24:40.120 --> 01:24:45.560]   What about every newspaper? What about every magazine? What about every TV network? They all make money
[01:24:45.560 --> 01:24:51.400]   Do you think they should have no liability for the social consequences of their algorithmic choices?
[01:24:51.400 --> 01:24:57.080]   I think that's what section is very wise. It takes away all my ability is wise
[01:24:57.240 --> 01:24:59.400]   It doesn't take away all liability if you have libel
[01:24:59.400 --> 01:25:00.840]   You know
[01:25:00.840 --> 01:25:06.680]   Well, it takes away the ability for those choices or those things that are done by others outside
[01:25:06.680 --> 01:25:12.760]   On the platform and without that as there's a great quote in the google brief, which I have up on the rundown
[01:25:12.760 --> 01:25:15.160]   We would end up with nothing but
[01:25:15.160 --> 01:25:16.600]   Billboards
[01:25:16.600 --> 01:25:19.240]   We'll end up with nothing but absolute papp and crap
[01:25:19.240 --> 01:25:24.360]   If if this goes away because no one's going to take the liability of making choices
[01:25:24.360 --> 01:25:26.680]   All they're going to do completely agree with you
[01:25:27.160 --> 01:25:30.520]   That's why we don't want to throw the baby out with the bathwater
[01:25:30.520 --> 01:25:38.200]   Screw google screw twitter screw tiktok and their creepy algorithms. Let's protect my chatroom
[01:25:38.200 --> 01:25:45.720]   My mastodon where it's not algorithmic, but I need to be protected so that I am not liable for things people post there
[01:25:45.720 --> 01:25:49.000]   That is absolutely necessity. I agree with you
[01:25:49.000 --> 01:25:51.800]   It would be the end of the internet of section 230 went away
[01:25:52.200 --> 01:25:57.720]   Your attempt to defend an indefensible algorithm is the problem not
[01:25:57.720 --> 01:26:00.920]   Not what again. I'm going to go back to the research
[01:26:00.920 --> 01:26:06.040]   What this is what google says in the brief is that there's a lot of accusations being presumed here and made
[01:26:06.040 --> 01:26:08.520]   That that aren't backed up by the facts
[01:26:08.520 --> 01:26:15.480]   And so on that basis on the basis of get ready ant get ready ant ant and telling you get ready on the basis of such
[01:26:15.480 --> 01:26:17.240]   moral panic
[01:26:17.240 --> 01:26:19.240]   We can lose rights
[01:26:19.240 --> 01:26:21.720]   at a greater level
[01:26:22.520 --> 01:26:26.120]   So yeah, I think I think ant didn't have a cold shower this morning
[01:26:26.120 --> 01:26:33.320]   He was a little slow on the uptake that wasn't me. That's this dad. Goin computed. Oh, you're blaming the some point the tesla too, huh?
[01:26:33.320 --> 01:26:36.520]   Blame at the tesla
[01:26:36.520 --> 01:26:44.760]   I'm sorry. I'm sorry. Blame it. Stacy you I'm sorry. That's wrong. You you confessed an embarrassing moment in life
[01:26:44.760 --> 01:26:50.040]   And and I shouldn't and you just kicked me while I was dead. I did. I'm sorry
[01:26:50.840 --> 01:26:52.840]   I expected no less
[01:26:52.840 --> 01:26:55.320]   Or no more
[01:26:55.320 --> 01:26:58.600]   All right leo, I think we've set our boundaries here
[01:26:58.600 --> 01:27:01.080]   Uh, what else is going on?
[01:27:01.080 --> 01:27:09.800]   We have not really I mean that was a google story. I feel like we haven't got other google stories about which um
[01:27:09.800 --> 01:27:12.920]   about heck
[01:27:12.920 --> 01:27:14.920]   tech tech tech tech
[01:27:15.560 --> 01:27:20.200]   We want to talk about all the AI you've got a bunch of chat gpt and stable diffusion stories
[01:27:20.200 --> 01:27:25.880]   I did think there I did think nick take nick cages takedown of chat gpt is historical
[01:27:25.880 --> 01:27:32.600]   Explain to the folks so some people have I guess it's become a thing to post
[01:27:32.600 --> 01:27:35.160]   Uh
[01:27:35.160 --> 01:27:39.880]   For some reason lyrics written in the style of nick cave written by chat gpt
[01:27:41.640 --> 01:27:45.800]   So somebody asked nick cave hey look at this the depths of the night
[01:27:45.800 --> 01:27:48.920]   I hear a call a voice that echoes through the hall
[01:27:48.920 --> 01:27:53.240]   It's a siren song that pulls me in takes me to a place where I can't begin
[01:27:53.240 --> 01:27:57.320]   I am the sinner. I am the saint. I am the doctor. I don't know anyway for whatever
[01:27:57.320 --> 01:27:59.800]   uh
[01:27:59.800 --> 01:28:01.800]   Nick cave responded
[01:28:01.800 --> 01:28:03.800]   on his personal blog
[01:28:03.800 --> 01:28:08.840]   What chat gpt is in this instance his replication has travesty
[01:28:08.840 --> 01:28:10.840]   uh
[01:28:10.840 --> 01:28:17.640]   Chappy tpt may be able to write a speech or an essay or a sermon or an obituary
[01:28:17.640 --> 01:28:20.600]   But it cannot create a genuine song
[01:28:20.600 --> 01:28:25.560]   It could perhaps in time create a song that is on the surface indistinguishable from original
[01:28:25.560 --> 01:28:30.120]   But it would always be a replication a kind of burlesque now. This is the important part
[01:28:30.120 --> 01:28:32.280]   But by the way
[01:28:32.280 --> 01:28:36.840]   This is completely obvious to everybody who has anything to do with this stuff, but nick cave
[01:28:37.560 --> 01:28:41.400]   You deserve the right to say this songs arise out of suffering
[01:28:41.400 --> 01:28:48.440]   By which I mean they're predicated upon the complex internal human struggle of creation and well as far as I know
[01:28:48.440 --> 01:28:54.600]   Algorithms don't feel data doesn't suffer chat gpt has no inner being
[01:28:54.600 --> 01:28:57.320]   Well to which I respond well duh
[01:28:57.320 --> 01:29:00.360]   Yeah
[01:29:00.360 --> 01:29:02.760]   It has been nowhere
[01:29:02.760 --> 01:29:08.840]   But people this is so the reason people have embraced this and it's got a lot of traction is it's like um
[01:29:08.840 --> 01:29:15.000]   It's like an anthem for humanity in this world of machines
[01:29:15.000 --> 01:29:18.520]   John henry. Yeah, i'll sing a song and beat you
[01:29:18.520 --> 01:29:19.640]   Yeah
[01:29:19.640 --> 01:29:25.880]   Uh, it has endured nothing and has had not it has not had the audacity to reach beyond its limitations
[01:29:25.880 --> 01:29:30.120]   Like we do and hence it doesn't have the capacity for a shared
[01:29:30.840 --> 01:29:32.840]   Ascended experience
[01:29:32.840 --> 01:29:38.200]   We should get john perry barlow's ghost to read this as it has no limitations from which to transcend
[01:29:38.200 --> 01:29:43.480]   chat gpt's melon collie role is it is destined to imitate
[01:29:43.480 --> 01:29:51.240]   And can never have authentic human experience no matter how devalued and inconsequential the human experience may in time become
[01:29:51.240 --> 01:29:54.840]   Again, wow you're fighting a fight
[01:29:54.840 --> 01:29:57.160]   Nobody's trying to fight
[01:29:57.160 --> 01:30:04.200]   Nobody's nobody knows this is why they chose you to make fun of right through this mechanism because you take yourself too seriously
[01:30:04.200 --> 01:30:06.040]   That's pretty funny
[01:30:06.040 --> 01:30:10.840]   Stacy was heading this toward I think the suits i really do race them a lot of fascinating issues here
[01:30:10.840 --> 01:30:12.920]   Because you've got geddy
[01:30:12.920 --> 01:30:14.200]   suing
[01:30:14.200 --> 01:30:20.120]   Um stable diffusion. Yeah because stable diffusion has many many geddy images in its database
[01:30:20.120 --> 01:30:25.480]   Uh in and getting knows this because the watermark is uh, you know, it's greek, but it's there
[01:30:26.120 --> 01:30:32.440]   Um, so it's obvious, but of course what's also obvious is all of those images were posted publicly on the internet
[01:30:32.440 --> 01:30:39.000]   Uh, what if what if geddy what if I bought a license to geddy and train used it to train
[01:30:39.000 --> 01:30:44.440]   Oh, that's interesting an AI because like so right now our ip laws don't have anything saying
[01:30:44.440 --> 01:30:47.160]   You could do that. I mean we had an ip change
[01:30:47.160 --> 01:30:54.120]   We re-realt contracts for the internet and maybe we need to rewrite our contracts for creating content for
[01:30:54.680 --> 01:30:57.640]   A chat gpt error stable diffusion era point
[01:30:57.640 --> 01:31:03.880]   So the line what is it here the one where I say it's a point-by-point refuital rebuttal. Thank you ant
[01:31:03.880 --> 01:31:06.840]   82 yes good point
[01:31:06.840 --> 01:31:12.760]   Um goes down to exactly what stacey's saying is that it raises issues and the problem is the suit
[01:31:12.760 --> 01:31:15.960]   Says that chat gpt is a collage
[01:31:15.960 --> 01:31:19.880]   And what this reputation is it's anonymous. I don't know who did it
[01:31:19.880 --> 01:31:24.120]   Um says is no the chat gpt stores no photos
[01:31:25.080 --> 01:31:30.760]   it looks at the this huge scrape of the web and looks at stuff and um
[01:31:30.760 --> 01:31:35.320]   Abstracts from it and then uses that abstraction to make stuff
[01:31:35.320 --> 01:31:38.120]   and so
[01:31:38.120 --> 01:31:41.720]   Copyright just doesn't isn't built for this to stacey's point
[01:31:41.720 --> 01:31:49.640]   What the derivatives? Yeah, is it but you know can you say that no no no you're not allowed to be inspired by that
[01:31:50.120 --> 01:31:55.880]   No, no, I have the right to tell you i'm nick cave and you cannot be inspired by my song. You cannot do that
[01:31:55.880 --> 01:31:59.480]   Um, I don't know
[01:31:59.480 --> 01:32:06.280]   With um, no, I mean it's like it's just like
[01:32:06.280 --> 01:32:08.120]   um
[01:32:08.120 --> 01:32:10.120]   I
[01:32:10.120 --> 01:32:15.720]   If you uh, I guess if you post it publicly and you get scraped, I mean I feel for the artists
[01:32:16.360 --> 01:32:20.520]   I am on my google home device. I have the arts and uh
[01:32:20.520 --> 01:32:22.680]   the google arts
[01:32:22.680 --> 01:32:30.280]   Paintings and stuff on there and there are a lot of classic old paintings rotating through and all of them look like stable diffusion now to me
[01:32:30.280 --> 01:32:31.880]   Because
[01:32:31.880 --> 01:32:38.840]   Or need journey. They all have us their own aesthetic because they well the re and it isn't their own aesthetic. That's the artist's point is ours
[01:32:38.840 --> 01:32:41.880]   Right and uh and yeah
[01:32:41.880 --> 01:32:43.320]   Uh
[01:32:43.320 --> 01:32:49.800]   All of this is the problem with all AI. It's trained on uh, you know data that it gets from somewhere
[01:32:49.800 --> 01:32:52.760]   Whether it's face recognition that doesn't have enough people of color in it
[01:32:52.760 --> 01:32:56.520]   Uh, or it's using artists images that it's scraping from the internet
[01:32:56.520 --> 01:32:59.800]   It needs it needs data to do what it does
[01:32:59.800 --> 01:33:02.680]   um
[01:33:02.680 --> 01:33:07.720]   I mean, I don't know if it's I mean the courts who knows what the courts are going to do
[01:33:07.720 --> 01:33:12.920]   It's going to depend on my so confused by the it's going to be very much on the points of view of the judges and juries
[01:33:12.920 --> 01:33:15.400]   It get to rule on this because where's he choose?
[01:33:15.400 --> 01:33:17.400]   to pay users
[01:33:17.400 --> 01:33:20.200]   So I would say
[01:33:20.200 --> 01:33:27.320]   And china's trying to regulate uh a i deep fix with this this is absolutely going to be for the next decade
[01:33:27.320 --> 01:33:30.200]   a big arena of discussion because
[01:33:30.200 --> 01:33:33.560]   Uh, you know, we thought this is funny because we really thought
[01:33:33.560 --> 01:33:39.000]   It was going to be biotech that by now we would be roiled in
[01:33:39.880 --> 01:33:41.880]   Uh court cases over jeans
[01:33:41.880 --> 01:33:44.280]   patenting jeans
[01:33:44.280 --> 01:33:46.280]   You know modifying jeans
[01:33:46.280 --> 01:33:52.760]   Should people be allowed to choose what sex their baby's going to be let alone how smart they're going to be or in what color their eyes are
[01:33:52.760 --> 01:33:57.880]   And we it didn't happen it will I guess at some point, but this is what happens with new technologies
[01:33:57.880 --> 01:34:04.040]   We have to as a society figure out. Hmm. What surprises you how do we yeah it surprises you and now we have to figure out?
[01:34:04.040 --> 01:34:06.120]   Well, what do we do about it? So let me ask you tech
[01:34:06.600 --> 01:34:09.640]   I just want to say I want to see more bear breasts on facebook
[01:34:09.640 --> 01:34:15.400]   Oh, see you got away from leo you got away from it and then you just it couldn't help
[01:34:15.400 --> 01:34:18.360]   Get the petals confused and you went through another fence
[01:34:18.360 --> 01:34:21.880]   It didn't learn the first time you went to the best. It's tesla's fault
[01:34:21.880 --> 01:34:27.720]   Um, let me ask you a conservatives think chat gbt has gone woke
[01:34:27.720 --> 01:34:35.560]   Well, i'm sure they can train it to be anti-Semitic soon enough. Oh, no problem. We can do that
[01:34:36.440 --> 01:34:41.720]   I mean, there is a lot of course. I'm not dismissing the issue. There's a larger issue
[01:34:41.720 --> 01:34:45.800]   I mean this is the whole issue of you know stochastic parents. We talk parents. We talked about yeah
[01:34:45.800 --> 01:34:49.480]   Yeah, I mean building a guru guru brought this up
[01:34:49.480 --> 01:34:54.760]   Um, it's more than a year ago. Uh now yeah, well if you want if you want to make it better
[01:34:54.760 --> 01:34:58.120]   You know one argument. I did this just to be provocative on massed on
[01:34:58.120 --> 01:35:03.960]   Then you could say that every single book should be in the learning set because books are better than the web
[01:35:05.000 --> 01:35:07.000]   of course then then um
[01:35:07.000 --> 01:35:12.840]   Glick went went after me on massed on as a result. I'm kind of a fan of the idea of letting this
[01:35:12.840 --> 01:35:17.720]   You know, this is my personal opinion. It's gonna be a judge's opinion that's gonna decide it or a jury's
[01:35:17.720 --> 01:35:21.880]   But my personal opinion is this is so new. This is kind of what you say about the internet
[01:35:21.880 --> 01:35:26.360]   Yeah, they're hazards, but let's at least let this go and see what happens. What comes
[01:35:26.360 --> 01:35:28.440]   Let's play out. What do you think about copilot?
[01:35:28.440 --> 01:35:33.720]   What do you think about code and copilot? Is that different from being inspired? No, it's exactly the same
[01:35:34.440 --> 01:35:40.040]   Copilot is more obvious because it's lifting but again, it's the same issue which is it's
[01:35:40.040 --> 01:35:43.320]   Code that is put on github publicly
[01:35:43.320 --> 01:35:49.240]   So it's it's available to anybody who wants to look at it and that's all it's doing is scraping that and yeah
[01:35:49.240 --> 01:35:55.400]   I mean when you ask uh copilot to write some code it's somebody else's code that it's pasting in there something very similar to it
[01:35:55.400 --> 01:36:03.240]   Um, it's not good. What's there smart with they've done something interesting with chat gbt. It's more synthetic. I don't think
[01:36:03.880 --> 01:36:08.280]   Mm-hmm. It's quite as clearly lifted from somebody else. Yes
[01:36:08.280 --> 01:36:12.680]   Now, Alex cantor with and we were trying to get him on twit. Unfortunately. He's traveling right now
[01:36:12.680 --> 01:36:15.080]   We'll get him on and we have him scheduled in a couple of months
[01:36:15.080 --> 01:36:18.200]   But his you know, he's the guy who does the big technology blog
[01:36:18.200 --> 01:36:22.040]   and his he had a very interesting story
[01:36:22.040 --> 01:36:28.040]   About getting plagiarized by what turns out to be a sub stack written by an AI
[01:36:28.040 --> 01:36:31.720]   And he has the quotes
[01:36:32.520 --> 01:36:36.920]   To prove it how close is it? It's 100% so let me see if I can
[01:36:36.920 --> 01:36:41.240]   This is a big the big chat gpt or was it well?
[01:36:41.240 --> 01:36:47.000]   We don't know because bless her brand by the way. I will also add I got sucked in by it. I actually bookmarked the
[01:36:47.000 --> 01:36:51.400]   Faith it's a it's a new sub stack called the rationalist
[01:36:51.400 --> 01:36:55.560]   And you might have seen this too it published a post
[01:36:55.560 --> 01:36:59.960]   Uh, which with a very, um, I thought a very
[01:37:00.520 --> 01:37:03.960]   Provocative title which caught my eye the creator economy
[01:37:03.960 --> 01:37:06.760]   the top 1% and everyone else
[01:37:06.760 --> 01:37:12.680]   The creator economy's middle-class myth why only the top echelon of online creators are able to make a living
[01:37:12.680 --> 01:37:18.200]   I bookmarked this I bookmarked passages from it. I was going to talk about it on this show
[01:37:18.200 --> 01:37:21.960]   until we could get into an argument about that until
[01:37:21.960 --> 01:37:25.240]   I saw Alex's post
[01:37:25.560 --> 01:37:30.360]   In which he said over the weekend a new sub stack called the rationalist lifted analysis and writing
[01:37:30.360 --> 01:37:32.840]   directly from big technology
[01:37:32.840 --> 01:37:37.800]   It plagiarized a post on the creator economy, which we covered days prior
[01:37:37.800 --> 01:37:42.920]   But here's why I saw the fake post it went viral. It hit the front page of hacker news
[01:37:42.920 --> 01:37:46.920]   Sparked the conversation. We know it was made by AI or just a bad plagiarist
[01:37:46.920 --> 01:37:49.400]   well
[01:37:49.400 --> 01:37:51.400]   I think Alex
[01:37:51.400 --> 01:37:54.360]   So what made the case of the this is from Alex's blog post
[01:37:54.360 --> 01:37:59.160]   So made the or the sub stack post what made the case of the rationalist particularly striking though was its author
[01:37:59.160 --> 01:38:03.320]   An avatar by the name of petra admitted they'd used
[01:38:03.320 --> 01:38:10.040]   AI tools to produce the story including those from open AI jasper and hugging face
[01:38:10.040 --> 01:38:12.840]   They said it said
[01:38:12.840 --> 01:38:17.240]   Or the human behind it said well, I english is not my native language
[01:38:18.040 --> 01:38:21.720]   So I asked these tools to help me translate what I was
[01:38:21.720 --> 01:38:28.280]   Saying but if you so here's the quote for instance from big technology from Alex's story
[01:38:28.280 --> 01:38:32.200]   with the days of zero eight interest rate froth ending
[01:38:32.200 --> 01:38:35.560]   The investments are becoming more difficult to justify
[01:38:35.560 --> 01:38:40.520]   Two days later the rationalist wrote with the end of zero interest rate froth
[01:38:40.520 --> 01:38:45.480]   Those investments are becoming more difficult to justify. I think that's a hundred percent plagiarized
[01:38:45.960 --> 01:38:49.480]   It's a slight leave paraphrased. It's fact. It's almost so bad. It looks human
[01:38:49.480 --> 01:38:51.240]   Uh-huh
[01:38:51.240 --> 01:38:56.120]   Online content creation writes Alex is still mostly viable for the very top echelon of online creators
[01:38:56.120 --> 01:39:02.120]   The rationalists said only the top echelon of creators are able to make a viable income. That's a little bit better paraphrase
[01:39:02.120 --> 01:39:06.120]   honestly
[01:39:06.120 --> 01:39:07.160]   the
[01:39:07.160 --> 01:39:09.720]   The the AI did a better job of linkbaiting me
[01:39:09.720 --> 01:39:15.800]   I didn't see I subscribed Alex's newsletter. I did but I would have done the same thing
[01:39:16.280 --> 01:39:22.360]   He's damned algorithms. I only clicked on that article in Reddit. Yeah, it's a great line. Yeah. Yeah
[01:39:22.360 --> 01:39:26.120]   Um, anyway, Alex was pissed
[01:39:26.120 --> 01:39:32.760]   Uh, but but as usual with Alex, he's not taking it personally. He's really writing about
[01:39:32.760 --> 01:39:35.400]   You know, whoa, this is an issue
[01:39:35.400 --> 01:39:42.040]   Given the rationalist success he writes more advanced efforts to copy and remix others work with AI will likely take place
[01:39:42.440 --> 01:39:44.440]   It should be easy to improve
[01:39:44.440 --> 01:39:47.240]   The rationalist was sloppy lifting clauses word for word
[01:39:47.240 --> 01:39:52.120]   But as publications with similar intent refine their systems and I think we're getting close to that
[01:39:52.120 --> 01:39:55.640]   Already they'll be able to remove all traces of the original writing
[01:39:55.640 --> 01:40:01.960]   For which you could substitute painting and just pass along the ideas and it shouldn't be hard to automate either
[01:40:01.960 --> 01:40:08.600]   I bet this was more human than machine and but but the point stands that the machine will do a better job of it. Yeah
[01:40:09.240 --> 01:40:14.600]   Imagine AI remixing. He says the financial times 10 most red stories of the day or the informations
[01:40:14.600 --> 01:40:18.200]   VC coverage and making the reporting available without a paywall
[01:40:18.200 --> 01:40:24.840]   Uh, he also reminds us that we talked about this last week and actually I had the editor in chief of cnet on
[01:40:24.840 --> 01:40:32.920]   On sunday and asked her about it. AI is already writing stories fresh stories for cnet 75 stories for their personal finance
[01:40:32.920 --> 01:40:37.880]   Uh section. She had Connie guyema was on the show and I asked her about it
[01:40:37.880 --> 01:40:40.600]   And she said well, you know, she wrote a blog post about it. She said
[01:40:40.600 --> 01:40:45.640]   Uh, there are a lot of stories real reporters don't want to write because they're so boring and dull and mundane
[01:40:45.640 --> 01:40:47.880]   true, uh, so
[01:40:47.880 --> 01:40:52.120]   Uh, what we did is we had an AI write them then had a human review it for factual
[01:40:52.120 --> 01:40:58.920]   Uh content and to rewrite it so it's better written. Well, this is the fascinating thing around education now
[01:40:58.920 --> 01:41:01.640]   We're we're at nyc public schools. We talked about last week
[01:41:01.640 --> 01:41:05.880]   Outlaw and others are trying to out lot but the smart educators are saying
[01:41:07.720 --> 01:41:13.240]   Turn out the AI stuff and then have the students judge it and grade it and and and and that's smarter
[01:41:13.240 --> 01:41:17.720]   Jessica lesson ceo of the information said told alix
[01:41:17.720 --> 01:41:24.440]   Art by the way note that I am quoting alix's stuff forbade him, but I am giving him credit
[01:41:24.440 --> 01:41:27.960]   That's what yes, but this is how we make a living
[01:41:27.960 --> 01:41:30.760]   Is we take I don't do any
[01:41:30.760 --> 01:41:36.360]   Uh enterprise reporting. We we take other people we know we know we comment. Thank you stacy
[01:41:36.920 --> 01:41:39.480]   You do I don't
[01:41:39.480 --> 01:41:41.000]   Uh
[01:41:41.000 --> 01:41:45.800]   Jessica says our competitors rip us off all the time essentially remixing stuff and sharing
[01:41:45.800 --> 01:41:51.640]   The information subscribers are smart to get it from the source, but i'm watching all this with fascination of course
[01:41:51.640 --> 01:41:56.440]   Um, I am sure alix and jessica both of whom have been on twit
[01:41:56.440 --> 01:42:01.320]   Know what we do and understand and I always attend oh you're promoting them
[01:42:01.320 --> 01:42:03.480]   It's like it's like
[01:42:04.680 --> 01:42:10.120]   And I hope I I mean I subscribe to both and I hope that others do and read the original
[01:42:10.120 --> 01:42:14.040]   But we are commenting on it. We're kind of at we're glossing what they wrote
[01:42:14.040 --> 01:42:18.200]   Yeah, that's like tv news
[01:42:18.200 --> 01:42:20.920]   as a but as a
[01:42:20.920 --> 01:42:25.880]   Has a written site like there is a vast difference between
[01:42:25.880 --> 01:42:34.600]   Like it like giga home for example. I would usually like do a link to somebody's reporting and a quick
[01:42:34.680 --> 01:42:38.920]   Insight and then I'd go off and do like if you're writing three to five stories a day
[01:42:38.920 --> 01:42:42.120]   You can't do original reporting on all five. It's not
[01:42:42.120 --> 01:42:43.960]   possible
[01:42:43.960 --> 01:42:49.240]   Um, but I still think you should still do some of your own original reporting. Um
[01:42:49.240 --> 01:42:54.760]   I'm not a reporter. I'm not a that's not what I do. Well, no, but you do have people
[01:42:54.760 --> 01:42:58.520]   I mean so your value that you do is you test things out
[01:42:58.520 --> 01:43:03.320]   So you play with devices and you figure out like when
[01:43:03.960 --> 01:43:07.720]   It may not be on this show for example, but you have shows where you've done
[01:43:07.720 --> 01:43:12.600]   What I think of as reporting. Yeah, but mostly what I bring to the table is
[01:43:12.600 --> 01:43:17.960]   Years of experience doing that very some perspective attaching that on top of other people's work
[01:43:17.960 --> 01:43:21.480]   Should I stop doing what i'm doing? Is it gonna explain you more? No, no
[01:43:21.480 --> 01:43:26.120]   I don't think it's a moral. No, you're I mean you're quoting other people. I got a book coming out
[01:43:26.120 --> 01:43:28.120]   I need to show to be around what comes out
[01:43:28.120 --> 01:43:31.400]   See he's the
[01:43:31.960 --> 01:43:35.560]   Capitalist, you know those ads where the guy plays mayhem
[01:43:35.560 --> 01:43:39.000]   And uh, and he I love those they're on football
[01:43:39.000 --> 01:43:42.920]   That's why you guys don't see him probably uh and uh, and he's like
[01:43:42.920 --> 01:43:49.080]   He's like a mouse running around the house causing you to jump and scream and knock over the christmas tree and cause a fire or
[01:43:49.080 --> 01:43:52.680]   Hold that guy the insurance. Yeah. Yeah, it's for progressive. I don't even
[01:43:52.680 --> 01:43:56.840]   Progressive I don't even know what the insurance company is because the ads are too good. I don't pay attention
[01:43:56.840 --> 01:44:01.160]   But I think somebody should do algorithm and go around
[01:44:01.960 --> 01:44:03.480]   And
[01:44:03.480 --> 01:44:05.480]   And cause havoc and mayhem
[01:44:05.480 --> 01:44:11.240]   Uh, maybe we can get that guy to do it. He's very good as mayhem. He's very he was he was tina phase boyfriend on 30 rock
[01:44:11.240 --> 01:44:16.600]   Oh, I didn't know reality. Yes. He was yeah. Yeah, he was Dennis. Was his name Dennis? I think so. Yeah
[01:44:16.600 --> 01:44:19.640]   Well, so lee are you scared of what read huffman demonstrated?
[01:44:19.640 --> 01:44:21.240]   Uh this week
[01:44:21.240 --> 01:44:25.080]   I don't I think it's wrong what rehuffman did but all right. Let's talk about that
[01:44:25.080 --> 01:44:28.520]   I was gonna suggest I was gonna suggest we try to do it on this show. Maybe just do it
[01:44:28.680 --> 01:44:33.320]   Yeah, well, yeah, I mean others have done this too. We've been together. It's we played
[01:44:33.320 --> 01:44:39.560]   Uh, I think on this show a podcast featuring joe rogan and steve jobs that was generated by
[01:44:39.560 --> 01:44:45.320]   That was trying to fake you out that they were them whereas read is talking to chat gpt as chat as a guest
[01:44:45.320 --> 01:44:50.520]   Well, it wasn't though. I mean that the people do that podcast the whole podcast is generated and they say it's generated
[01:44:50.520 --> 01:44:55.240]   These are generated voices. See this is this is where it's really going to get weird when you get good enough
[01:44:55.640 --> 01:45:02.360]   To create the content artificially and make it sound real or even in time look real
[01:45:02.360 --> 01:45:07.880]   So that you could be watching a show like this and not know if it's a human doing it
[01:45:07.880 --> 01:45:11.800]   Uh, what read was doing was trying to interview the technology
[01:45:11.800 --> 01:45:16.280]   It was read open read huffman is a founder of netflix who's now an investor. No, not netflix
[01:45:16.280 --> 01:45:17.800]   uh
[01:45:17.800 --> 01:45:22.920]   Linked in sorry. I confuse him with read Hastings who was the founding of uh netflix
[01:45:23.080 --> 01:45:29.640]   Yeah, he's linked in fatter who is now vc at graylock and the graylock gray matter podcast is an interview
[01:45:29.640 --> 01:45:33.400]   With chat gpt. It's a gimmick. I mean i would it's a gimmick
[01:45:33.400 --> 01:45:36.760]   Yeah, he he he he asked the question that came back
[01:45:36.760 --> 01:45:40.920]   He used artificial voice to come back and turn it into what sounded like a podcast. Yeah
[01:45:40.920 --> 01:45:46.600]   Let's start with talking about the Turing tests as read perhaps you can kick us off with the description and then
[01:45:46.600 --> 01:45:49.560]   Listen to listen to it. Should I play a little bit of it the voices?
[01:45:49.560 --> 01:45:52.600]   They got to be here with the voice read will take us down. I don't know
[01:45:52.600 --> 01:45:58.680]   Let me skip it and identify patterns and correlations that may not be immediately a matter of
[01:45:58.680 --> 01:46:03.000]   Which can help to drive new insights and discoveries is that uncanny valley?
[01:46:03.000 --> 01:46:11.000]   I can also be used to develop and test hypotheses and models and to optimize and improve existence
[01:46:11.000 --> 01:46:17.320]   That's not bad. I'm gonna guess it graylock has an investment in the technology used to create that voice. I wouldn't be surprised at all
[01:46:18.360 --> 01:46:21.240]   That's why I don't listen to podcasts from venture capital companies
[01:46:21.240 --> 01:46:25.480]   They often have heat and agenda
[01:46:25.480 --> 01:46:27.560]   Um
[01:46:27.560 --> 01:46:32.120]   Yeah, I mean it's a gimmicky kind of thing. I wouldn't do it, but uh, no we spent a lot of time
[01:46:32.120 --> 01:46:38.200]   Make you know reading poems written by chat bg GPT and stuff. It's very interesting. It was fun for a while
[01:46:38.200 --> 01:46:41.240]   I actually asked this on windows weekly earlier and i'm wondering what you guys think
[01:46:41.240 --> 01:46:46.280]   We've been talking a lot about oh, it's gonna be and i've been saying it's a paradigm shift
[01:46:46.280 --> 01:46:48.280]   We're at an inflection point. It's a cambrian
[01:46:48.280 --> 01:46:58.520]   Uh, you know revolution of ai suddenly you see you know art and and text being written very credibly by ai
[01:46:58.520 --> 01:47:03.320]   You know suddenly we're having this turn and and i've been saying that but i'm also thinking
[01:47:03.320 --> 01:47:08.200]   It could be just another ai winner. We're just heading into blithely
[01:47:08.200 --> 01:47:13.160]   This happened before remember we thought cars would be able to drive themselves by now
[01:47:13.720 --> 01:47:16.600]   And they can't as stacey is proven
[01:47:16.600 --> 01:47:28.440]   What do you think stacey are are is is this a turning point in the ai or is this just a gimmick a magic trick
[01:47:28.440 --> 01:47:33.960]   I think the police allerge sneeze attacks. That's why
[01:47:33.960 --> 01:47:37.320]   Yeah, um
[01:47:37.320 --> 01:47:39.080]   No, I think this is
[01:47:39.080 --> 01:47:40.120]   legit
[01:47:40.120 --> 01:47:41.160]   and
[01:47:41.160 --> 01:47:47.720]   Real kind of like how we had this burst of innovation after 2012 when we finally
[01:47:47.720 --> 01:47:50.760]   managed to create viable image search
[01:47:50.760 --> 01:47:53.480]   I I think this sort of thing
[01:47:53.480 --> 01:47:59.800]   It's easier than self-driving for one thing. So I think this is real. I think we're gonna see a lot more benefits from it
[01:47:59.800 --> 01:48:02.360]   and I do think it's going to be
[01:48:02.360 --> 01:48:09.240]   I'll call it sudden because I think it's gonna be sudden compared to like the launch of the printing press for example
[01:48:09.320 --> 01:48:16.920]   But like I think at the next five years we're gonna see some real shifts in the type of content and the business models around content creation
[01:48:16.920 --> 01:48:19.800]   Did you say good bird?
[01:48:19.800 --> 01:48:23.240]   I did I did well. I I just talked about the printing press but yes good
[01:48:23.240 --> 01:48:29.800]   Well, please if you're gonna say the printing roots, I know take a drink. I want a drink
[01:48:29.800 --> 01:48:34.280]   Right. Yeah, I think I think stacey what it really would be seen that story does
[01:48:35.960 --> 01:48:41.720]   Does it commodify does it the final commodification of content everything's writing in content or special
[01:48:41.720 --> 01:48:44.120]   reporting special to your point
[01:48:44.120 --> 01:48:52.040]   But is writing is blathering special no man. Do the clown or charam sings says and she's right radio is a gimmick until it wasn't
[01:48:52.040 --> 01:48:55.160]   Yeah, yeah, yeah true
[01:48:55.160 --> 01:48:58.280]   Uh google and uh everybody
[01:48:58.280 --> 01:49:01.320]   Kind of going all in microsoft's
[01:49:01.320 --> 01:49:05.880]   uh, we're we're to be putting 10 billion dollars into uh open ai
[01:49:06.360 --> 01:49:11.480]   Satya najela speaking at it might have been davos somewhere like davos
[01:49:11.480 --> 01:49:15.400]   Uh admitting yeah, he didn't say the 10 billion dollar number, but he said yeah
[01:49:15.400 --> 01:49:19.800]   We're absolutely want to start incorporating artificial intelligence into all of our products
[01:49:19.800 --> 01:49:25.880]   Oh, they're gonna add it a day as your as your as your as your as your and as your and also into open into office rather
[01:49:25.880 --> 01:49:31.080]   Microsoft 365 here's google's blog post why we focus on ai
[01:49:31.080 --> 01:49:34.120]   and to what end
[01:49:34.440 --> 01:49:41.720]   Oh, it's new. I mean I would love a chat gpt auto response that's polite to emails that I could just be like
[01:49:41.720 --> 01:49:48.360]   Boom. I mean I have talked about my I wanted a macro for I will honor the embargo, but like I don't cover this
[01:49:48.360 --> 01:49:51.320]   I don't care or yeah, let's go schedule this
[01:49:51.320 --> 01:49:55.720]   I like this post uh this post from um james
[01:49:55.720 --> 01:50:00.120]   Manica who's the senior vice president for technology and society
[01:50:00.680 --> 01:50:06.440]   You know, it's pretty thoughtful. They say we believe that ai is a foundational and transformational technology
[01:50:06.440 --> 01:50:10.680]   That will provide compelling and helpful benefits to people in society
[01:50:10.680 --> 01:50:17.960]   Through its capacity to assist complement and power and inspire people in almost every field of human endeavor. I would like to point out google's very good
[01:50:17.960 --> 01:50:19.960]   at um
[01:50:19.960 --> 01:50:24.200]   Taking its money making efforts and making them good for people
[01:50:24.200 --> 01:50:30.120]   It has the potential to contribute to tackling some of society's most pressing challenges and opportunities from the every day
[01:50:30.520 --> 01:50:36.760]   To the more creative and imaginative at the same time we understand that ai as a still emerging technology poses various and evolving
[01:50:36.760 --> 01:50:43.320]   Complexities and risks. This is uh, this is what a high schooler would write when asked about did slavery cost
[01:50:43.320 --> 01:50:48.280]   Yeah, our development use of ai must address these risks
[01:50:48.280 --> 01:50:53.880]   That's why we as a company consider it an imperative to pursue ai responsibly
[01:50:53.880 --> 01:50:59.160]   We're committed to leading and setting the standard of developing and shipping useful and beneficial applications
[01:50:59.480 --> 01:51:02.440]   applying ethical principles grounded in human values
[01:51:02.440 --> 01:51:06.760]   And evolving our approaches as we learn from research experience users in the wider
[01:51:06.760 --> 01:51:09.160]   Community we believe in getting ai
[01:51:09.160 --> 01:51:12.120]   Right. This is appropriate that this is what they would say
[01:51:12.120 --> 01:51:19.240]   It's not so the white house remember a few weeks ago put out a a white paper on principles for ai ethical ai
[01:51:19.240 --> 01:51:26.440]   Uh five simple principles safe and effective systems protection from algorithmic discrimination and inequitable systems
[01:51:27.080 --> 01:51:31.160]   Protection from abusive data practices and agency about how personal data is used
[01:51:31.160 --> 01:51:35.800]   Knowledge of when automated system is used understanding of its impact. That's very important ability
[01:51:35.800 --> 01:51:40.120]   To opt out of the automated system and for a human alternative. Could you?
[01:51:40.120 --> 01:51:46.840]   Could you make a law and should you make a law that's that says uh, you must disclose
[01:51:46.840 --> 01:51:53.800]   When ai is used well, I used it in spell checker. I used it in translate. Do I wear is the line?
[01:51:53.800 --> 01:51:55.800]   That's a good point. What does use be a point?
[01:51:55.800 --> 01:51:57.800]   I
[01:51:57.800 --> 01:52:03.080]   Do think having a human in the mix is key because as we've seen in all of these things
[01:52:03.080 --> 01:52:06.280]   it gets it doesn't
[01:52:06.280 --> 01:52:14.040]   Doesn't capture everything and people do get screwed over and you look at things like loan
[01:52:14.040 --> 01:52:21.400]   Approvals you can look at even things like parole there. I can't remember what state it was but
[01:52:22.120 --> 01:52:26.280]   the judges didn't have the ability to overturn bail for
[01:52:26.280 --> 01:52:34.680]   The ai generated bail so even if they looked at the situation we're like, mmm, this is not right or this might be kind of racist
[01:52:34.680 --> 01:52:40.600]   Uh, the judges had no options. So I I think making sure that you can bring a human in is
[01:52:40.600 --> 01:52:44.360]   Essential for something like this especially when it's still so new
[01:52:44.360 --> 01:52:50.200]   But stacy google search is an ai you got to get on the phone and call somebody and say no
[01:52:50.200 --> 01:52:52.200]   No, I didn't like this result. Can you send me?
[01:52:52.200 --> 01:52:58.680]   No, then we think about the impact so maybe it is only when it
[01:52:58.680 --> 01:53:01.320]   deals with dollar amounts
[01:53:01.320 --> 01:53:05.240]   Dollar amounts over a certain amount because I mean I think about things like
[01:53:05.240 --> 01:53:12.840]   I mean, there's medical care. There's ai that says oh, yeah, you shouldn't be screened for breast cancer when maybe you should or
[01:53:12.840 --> 01:53:18.520]   Right. Maybe I mean well that was my thing. I think that that's why I thought I should argue
[01:53:18.600 --> 01:53:22.760]   That's why I thought you should have to disclose it so that you as a consumer would be able to say
[01:53:22.760 --> 01:53:28.520]   Maybe I'll get a second opinion because but it's gonna be so much in everything. That's the problem. Um, yeah
[01:53:28.520 --> 01:53:31.720]   Yeah, no, and I see I see that problem, but I'm also like
[01:53:31.720 --> 01:53:34.520]   We've got to figure out
[01:53:34.520 --> 01:53:38.680]   Then the the way we need to write a law and that's doable
[01:53:38.680 --> 01:53:45.240]   To make sure we don't screw people over and not let them have access to a human being
[01:53:45.240 --> 01:53:47.720]   in something that's really important
[01:53:48.520 --> 01:53:53.400]   Hey, everybody. It's leo l'apport the founder and host of many of the twit
[01:53:53.400 --> 01:54:00.200]   Podcasts. I don't normally talk to you about advertising, but I want to take a moment to do that right now
[01:54:00.200 --> 01:54:07.560]   Um our mission statement at twit. We're dedicated to building a highly engaged community of tech enthusiasts
[01:54:07.560 --> 01:54:11.320]   That's our audience and you I guess since you're listening
[01:54:11.320 --> 01:54:16.840]   By offering them the knowledge they need to understand and use technology in today's world
[01:54:17.400 --> 01:54:21.160]   To do that. We also create partnerships with trusted brands
[01:54:21.160 --> 01:54:25.000]   And make important introductions between them and our audience. That's how we
[01:54:25.000 --> 01:54:31.640]   Finance our podcasts, but it's also and our audience tells us this all the time a part of the service we offer
[01:54:31.640 --> 01:54:33.960]   It's a valued bit of information
[01:54:33.960 --> 01:54:39.400]   For our audience members. They want to know about great brands like yours
[01:54:39.400 --> 01:54:43.240]   So can we help you by introducing you
[01:54:43.960 --> 01:54:47.080]   To our highly qualified audience and why do you get a lot?
[01:54:47.080 --> 01:54:56.040]   With advertising on the twit podcasts partnering with twit means you're going to get if I may say so humbly the gold standard and podcast advertising
[01:54:56.040 --> 01:55:00.760]   And we throw in a lot of valuable services. You get a full service continuity team
[01:55:00.760 --> 01:55:05.160]   Supporting everything from copywriting to graphic design. I don't think anybody else
[01:55:05.160 --> 01:55:07.960]   Does this or does this as well as that we do
[01:55:07.960 --> 01:55:13.560]   You get ads that are embedded in our content that are unique every time I read them or hosts read them
[01:55:14.120 --> 01:55:17.320]   We always over deliver on impressions and frankly
[01:55:17.320 --> 01:55:21.880]   We're here to talk about your product. So we really give
[01:55:21.880 --> 01:55:27.560]   Our listeners a great introduction to what you offer. We've got on boarding services
[01:55:27.560 --> 01:55:33.960]   Adtech with pod sites. That's free for direct clients. We give you a lot of reporting. So you know who saw your
[01:55:33.960 --> 01:55:38.440]   advertisement you'll even know how many responded by going to your website
[01:55:39.000 --> 01:55:43.880]   We'll also give you courtesy commercials that you can share across social media and landing pages
[01:55:43.880 --> 01:55:49.800]   We think these are really valuable people like me and our other hosts talking about your products sincerely
[01:55:49.800 --> 01:55:56.280]   Uh and informationally those are incredibly valuable. You also get other free goodies mentions in our weekly newsletter
[01:55:56.280 --> 01:55:59.640]   That's sent out to thousands of fans. We give bonus ads
[01:55:59.640 --> 01:56:05.160]   Uh to people who buy a significant amount of advertising you'll get social media promotion too
[01:56:05.560 --> 01:56:10.440]   But let me tell you we are looking for an advertising partner. That's going to be with us long-term
[01:56:10.440 --> 01:56:17.240]   Visit twit.tv/advertise check out our partner testimonials tim broom founder of it pro tv
[01:56:17.240 --> 01:56:24.680]   They started it pro tv in 2013 immediately started advertising with us and grew that company
[01:56:24.680 --> 01:56:31.400]   To a really amazing success hundreds of thousands of ongoing customers
[01:56:31.960 --> 01:56:35.880]   They've been on our network for more than 10 years and they say and i'll quote tim
[01:56:35.880 --> 01:56:42.200]   We would not be where we are today without the twit network. That's just one example. Mark McCrory who's the ceo of authentic
[01:56:42.200 --> 01:56:48.600]   Uh, he was actually uh one of the first people to buy ads on our network. He's been with us for 16 years
[01:56:48.600 --> 01:56:56.040]   He said and i'm quoting the feedback from many advertisers over those 16 years across a range of product categories
[01:56:56.040 --> 01:56:59.800]   Is that if ads and podcasts are going to work for a brand?
[01:57:00.440 --> 01:57:06.040]   They're going to work on twit shows. I'm proud to say that the ads we do over deliver they work
[01:57:06.040 --> 01:57:13.080]   Really well because they're honest they have integrity our audience trusts us and we say this is a great product
[01:57:13.080 --> 01:57:15.640]   They believe it they listen
[01:57:15.640 --> 01:57:19.720]   Our listeners are highly intelligent. They're heavily engaged their tech savvy
[01:57:19.720 --> 01:57:28.280]   They're dedicated to our network and that's partly because we only work with high integrity partners that we have thoroughly and personally
[01:57:28.760 --> 01:57:32.920]   vetted i approve every single advertiser on the network
[01:57:32.920 --> 01:57:37.800]   If you're ready to elevate your brand and you've got a great product. I want you to reach out to us
[01:57:37.800 --> 01:57:40.760]   Advertise at twit.tv
[01:57:40.760 --> 01:57:45.400]   So I want you to break out of the advertising norm grow your brand
[01:57:45.400 --> 01:57:47.960]   with host red authentic
[01:57:47.960 --> 01:57:54.120]   Ads on twit.tv visit twit.tv slash advertise for more details or email us
[01:57:54.120 --> 01:57:58.200]   Advertise at twit.tv if you're ready to launch your campaign now
[01:57:59.160 --> 01:58:04.520]   Oh, this is what his parents is critical. This is what you were talking about the nest the guy who founded nest
[01:58:04.520 --> 01:58:06.840]   with the
[01:58:06.840 --> 01:58:08.840]   Yes matt rogers
[01:58:08.840 --> 01:58:14.440]   There are a lot of these I would do this now they they they say it's 33 bucks a month
[01:58:14.440 --> 01:58:19.320]   They take your scraps and feed them to chickens after composting well you've got a male
[01:58:19.320 --> 01:58:20.920]   the
[01:58:20.920 --> 01:58:24.920]   Old switch case you should explain it brother. She knows what i do. Okay. So this is a Wi-Fi connected
[01:58:25.320 --> 01:58:29.480]   Giant compost bin it looks like a trash can there are other similar compost bins that
[01:58:29.480 --> 01:58:34.120]   Deodorize and then you get a thing that you would then put on your plants or whatever
[01:58:34.120 --> 01:58:37.480]   But it's designed for your kitchen. This one's a little different
[01:58:37.480 --> 01:58:40.920]   Go ahead stacy
[01:58:40.920 --> 01:58:45.240]   So you pay 33 dollars a month and you scrap your food in there and then
[01:58:45.240 --> 01:58:47.960]   the bin whenever you put food in
[01:58:47.960 --> 01:58:50.840]   it will
[01:58:50.840 --> 01:58:57.160]   dehydrate it and shrink chops it up dehydrates it shrinks it down into like pellets that just when that gets
[01:58:57.160 --> 01:58:59.480]   full
[01:58:59.480 --> 01:59:04.600]   It will signal it will generate a box sent to your home you pack those pellets or
[01:59:04.600 --> 01:59:10.440]   Powder or whatever it is in the box and then you ship it out to somebody who feeds it to chickens
[01:59:10.440 --> 01:59:13.080]   um
[01:59:13.080 --> 01:59:18.280]   You would really talk to me about why you did this does this is not a product that would be great for me
[01:59:18.280 --> 01:59:21.880]   But i'm curious in california they passed a bill
[01:59:21.880 --> 01:59:26.200]   Was signing the law and it took effect i think first of the year
[01:59:26.200 --> 01:59:34.440]   That you have to compost everybody has to compost so we got an extra bin. It's free. It's not we only pay for
[01:59:34.440 --> 01:59:40.520]   Trash that goes to the landfill recycling. We don't pay for so we have a we have two new bins
[01:59:40.520 --> 01:59:44.520]   We have a green recycling bin for yard waste and stuff
[01:59:45.000 --> 01:59:49.240]   We can throw food waste into that and then we have another bin we could also throw into for composting
[01:59:49.240 --> 01:59:54.440]   They take it so we separate our food scraps out. It's a little messy because a little smelly
[01:59:54.440 --> 02:00:01.240]   You can't it's a little smelly a little messy, but you know we every day we take the compost out of the house and put it in the bin
[02:00:01.240 --> 02:00:05.480]   Outside where it is we put the bin far from the right door
[02:00:05.480 --> 02:00:06.600]   It's a little ripe
[02:00:06.600 --> 02:00:09.720]   But then they take it and they use it and i don't get charged for it
[02:00:09.720 --> 02:00:15.240]   But the i presume that it's being used appropriately in giant compost heaps somewhere
[02:00:15.240 --> 02:00:19.000]   So we get charged we compost too. We are charged
[02:00:19.000 --> 02:00:23.800]   $12 or $10 a month for our compost and we we also
[02:00:23.800 --> 02:00:26.360]   Have a little bin by our sink and then we
[02:00:26.360 --> 02:00:31.240]   When that gets full we empty it out into the big bin because it's cold here. We don't really worry so much about it
[02:00:31.240 --> 02:00:33.240]   It's better when it's cold. Yeah
[02:00:33.240 --> 02:00:36.760]   Yeah, but this this this new thing would would deal with that right it
[02:00:37.240 --> 02:00:45.480]   It reduces volume and smell and all that and you wouldn't have to put it outside now our compost bin also takes like leaf litter
[02:00:45.480 --> 02:00:52.680]   And it's not just food and it takes things like paper towels. Yeah, I don't think this we put our paper towels in there in this one too. Yeah
[02:00:52.680 --> 02:00:55.080]   So
[02:00:55.080 --> 02:00:58.040]   I'm having Matt come on the show next week. So he'll be our guest
[02:00:58.040 --> 02:00:59.800]   To us next week
[02:00:59.800 --> 02:01:01.400]   Yeah, I have a lot of like
[02:01:01.400 --> 02:01:04.760]   I think it's an interesting idea. I don't think the consumer market
[02:01:04.840 --> 02:01:09.000]   I mean, I think there's a small segment of the consumer market that would pay $33 a month for this
[02:01:09.000 --> 02:01:14.200]   I question the environmental benefits because you know, you've got a truck come into pick your
[02:01:14.200 --> 02:01:16.040]   Oxes, right?
[02:01:16.040 --> 02:01:21.400]   In california, it's against the law to throw food scraps into the garbage
[02:01:21.400 --> 02:01:27.480]   First offense fine is 50 to 100. Third offense and subsequent offenses $500
[02:01:27.480 --> 02:01:31.000]   Uh, they're pretty serious about it. I think a lot of people don't do it
[02:01:31.000 --> 02:01:34.200]   And I know I doubt our police officers are going around looking at our garbage
[02:01:34.760 --> 02:01:39.160]   But I don't they were you don't do it. Are you ready for that big fine?
[02:01:39.160 --> 02:01:44.360]   I'm I'm the whole garbage thing and recycling stuff. I'll just be frank
[02:01:44.360 --> 02:01:47.640]   It seems like a really big racket out here
[02:01:47.640 --> 02:01:49.320]   um
[02:01:49.320 --> 02:01:54.600]   With the recycling. I remember when we first got here and we were told the whole rules of recycling and
[02:01:54.600 --> 02:01:58.920]   Apparently I could never get it right because they were not empty
[02:01:58.920 --> 02:02:02.600]   They would know they'll put a little they'll look in they'll put a notice in there
[02:02:02.600 --> 02:02:05.560]   So what you're putting lumber in though yard waste for things like that
[02:02:05.560 --> 02:02:07.480]   Yeah, and I'm like well
[02:02:07.480 --> 02:02:10.600]   And they they can be a sheet and I'd read the sheet and
[02:02:10.600 --> 02:02:14.680]   It's not that hard. I swear I'm following it. We're
[02:02:14.680 --> 02:02:19.800]   And doesn't do woke garbage. You should can't send him a note saying i'm from north caracolackey
[02:02:19.800 --> 02:02:21.480]   We don't do this
[02:02:21.480 --> 02:02:26.120]   We've heard it in the backyard is like you can recycle this but you can't recycle that and I'm like well
[02:02:26.120 --> 02:02:29.320]   Most people would recycle this as well as recycling
[02:02:29.320 --> 02:02:33.240]   It makes me a little angry and then the whole washing this and washing that
[02:02:33.240 --> 02:02:37.000]   You have to wash stuff out you've moistened so much water washing this out
[02:02:37.000 --> 02:02:39.800]   Even though we're in a drought all the dad gum time over here
[02:02:39.800 --> 02:02:43.480]   Yeah, I had issues with all of this trash stuff over here in Sonoma county
[02:02:43.480 --> 02:02:45.240]   Oh, we do it religiously aunt
[02:02:45.240 --> 02:02:51.160]   Lisa is uh is absolutely adamant about that we were we separate everything out we wash it out
[02:02:51.160 --> 02:02:57.320]   We don't we try not to wash with a lot of water. Yeah, but I mean we're trying to do our our good our good deed
[02:02:57.560 --> 02:03:04.200]   I recycle things but man you you know what bottom of me and actually I have a friend who's on the commission for
[02:03:04.200 --> 02:03:07.080]   Petaluma re garbage garbage
[02:03:07.080 --> 02:03:12.040]   Yeah, and I and I asked you to look into it because they say we will take plastic bottles
[02:03:12.040 --> 02:03:19.480]   And as far as I know that's just a lie from the plastic industry that right 90% of plastic is not recyclable
[02:03:19.480 --> 02:03:22.280]   Well different municipalities have different
[02:03:23.560 --> 02:03:27.800]   Vendors that they use and vendors can change their policies
[02:03:27.800 --> 02:03:31.560]   Especially when China's not accepting everybody. Yeah, they claim they recycle it
[02:03:31.560 --> 02:03:36.040]   Well, Europe is now just passed a law saying you can't have recycling
[02:03:36.040 --> 02:03:39.240]   Imperialism you can't
[02:03:39.240 --> 02:03:46.440]   No, that's right. Make little Chinese children burn your garbage. It's not okay. Right exactly. I agree
[02:03:46.440 --> 02:03:49.960]   So I would so
[02:03:49.960 --> 02:03:55.720]   I mean and I would say coming from north carolina. The south is not very progressive in
[02:03:55.720 --> 02:03:59.240]   I
[02:03:59.240 --> 02:04:06.120]   Think we're even if we don't like it. We're gonna end up doing more of it
[02:04:06.120 --> 02:04:11.320]   But that's the thing I do try. I mean we we do make an effort but when I first got here
[02:04:11.320 --> 02:04:15.720]   It seems like for the longest time I was not doing it. You have to learn how to
[02:04:15.720 --> 02:04:19.320]   It's not easy and it's good that they're telling you
[02:04:20.040 --> 02:04:22.040]   That they won't accept it because if you put
[02:04:22.040 --> 02:04:27.640]   Bad stuff like crappy trash in yeah, it can damage the whole
[02:04:27.640 --> 02:04:34.520]   Like load so it's actually probably a good indicator that they're very serious about recycling as much as they can
[02:04:34.520 --> 02:04:40.280]   We have the other issue tape tronc Lisa called them and got them to send it. We have taped on our
[02:04:40.280 --> 02:04:42.280]   refrigerator
[02:04:42.280 --> 02:04:45.560]   All of the exact list of what they take and don't take in what thing
[02:04:46.280 --> 02:04:52.360]   I've done that too. Yeah, you have to because it's complicated put it right there on the wall where we have the bin
[02:04:52.360 --> 02:04:55.320]   And I also had it outside on the bin outside
[02:04:55.320 --> 02:04:57.320]   But they say they take plastic bottles which
[02:04:57.320 --> 02:05:02.760]   As far as I know, I mean they put you know the plastic industries loves to pretend it's recyclable
[02:05:02.760 --> 02:05:07.720]   They put the little symbol on it and stuff, but nobody as far as I know recyclers, but apparently maybe there's
[02:05:07.720 --> 02:05:13.640]   Hard plastic that was yeah the one and two plastic. The only ones our place will recycle now
[02:05:13.720 --> 02:05:20.440]   They make bashes out of it and stuff. Well, that's that's what they do with the you know the the the the the bags
[02:05:20.440 --> 02:05:23.400]   uh, but you know the real issue that's raised here by
[02:05:23.400 --> 02:05:28.120]   Stacey's upcoming guest is food is though is the sin of food waste in this country
[02:05:28.120 --> 02:05:33.560]   Yeah, and this doesn't make any things better. Well, he claims it does in this article with cnbc
[02:05:33.560 --> 02:05:38.840]   mill now makes it for people to get rid of food waste and reduce their carbon footprint
[02:05:38.840 --> 02:05:41.640]   By feeding it to chickens
[02:05:42.360 --> 02:05:45.240]   Yeah, I mean it does not miss house
[02:05:45.240 --> 02:05:48.840]   I eat my food waste
[02:05:48.840 --> 02:05:55.960]   And I tell you one thing everybody in this house they they better not have a problem with leftovers. We eat every day
[02:05:55.960 --> 02:05:59.960]   We eat all my leftovers too. Yeah, we try to but it's not just leftovers
[02:05:59.960 --> 02:06:03.800]   I mean, it's like, you know, you I know like when I got sick for example
[02:06:03.800 --> 02:06:09.320]   When we all have the flu none of us wanted to eat like whatever we had in the first stuff wasted goes bad and yeah, yeah
[02:06:09.320 --> 02:06:10.920]   but
[02:06:10.920 --> 02:06:15.240]   Yeah, um, oh, maybe I now I know what my thing will be this week
[02:06:15.240 --> 02:06:21.400]   What's it for? Allie in californians? Good. Well, I can't wait to hear your interview with matt because I really uh, yeah
[02:06:21.400 --> 02:06:25.480]   I would get this I would even pay 33 bucks a month if I thought it helped the world
[02:06:25.480 --> 02:06:30.600]   Because I care and if it reduced the the cheesy smell out in your driveway
[02:06:30.600 --> 02:06:37.800]   It is very cheesy. It is a little stinky. It is gross. Like like like blue cheesy
[02:06:38.760 --> 02:06:44.680]   Yeah, I like it smells like um, something died. You know bad spinach
[02:06:44.680 --> 02:06:49.480]   It's it's real quick smells like teen spirit
[02:06:49.480 --> 02:06:52.440]   That's more
[02:06:52.440 --> 02:06:57.320]   All right, um, I think we're done. Oh wait a minute. We didn't do the change like play that
[02:06:57.320 --> 02:07:00.200]   Yeah, we have one. Yeah, yeah
[02:07:00.200 --> 02:07:03.640]   Play that uh, whatever thing
[02:07:03.640 --> 02:07:06.440]   change lock
[02:07:06.920 --> 02:07:08.920]   There's not much doesn't be quick
[02:07:08.920 --> 02:07:18.120]   The google change log
[02:07:18.120 --> 02:07:21.720]   YouTube is testing a hub
[02:07:21.720 --> 02:07:24.360]   A hub of free cable style
[02:07:24.360 --> 02:07:27.640]   Channels every it's so funny. We got you know
[02:07:27.640 --> 02:07:33.400]   We we want a disaggregation now. We're just aggregating. They're all right back up again
[02:07:33.400 --> 02:07:35.320]   Funny how that works. Yep
[02:07:35.320 --> 02:07:41.480]   Company is reportedly and talks with media companies to feature their shows and movies free v is the hot thing, right?
[02:07:41.480 --> 02:07:45.400]   Amazon had free v there's roku tv fox has to be
[02:07:45.400 --> 02:07:48.440]   Paramount has pluto tv
[02:07:48.440 --> 02:07:53.400]   Uh, and now youtube wants to have its ad supported free
[02:07:53.400 --> 02:07:55.320]   TV
[02:07:55.320 --> 02:07:59.960]   Channels the service is said to have team coordinate on gadget teamed up with the likes of lions gate a and e
[02:07:59.960 --> 02:08:02.680]   and film rise
[02:08:03.400 --> 02:08:08.280]   YouTube already offers ad supported movies you probably seen that but this would give you even more
[02:08:08.280 --> 02:08:14.040]   Um with channels like they'll be uh, you know, dr. Who's south park, frasier
[02:08:14.040 --> 02:08:17.080]   Yeah, that's fine
[02:08:17.080 --> 02:08:22.280]   Uh, did I say youtube tv? It's not their youtube tv service. It's youtube itself youtube. Yeah, right. Yeah
[02:08:22.280 --> 02:08:29.720]   That's the difference your google stadia controller. We mentioned this last week, but i'll mention again as of today
[02:08:30.440 --> 02:08:32.440]   Stadia is no more
[02:08:32.440 --> 02:08:39.720]   But your controller is still usable if you download the tool that may turns it into a bluetooth
[02:08:39.720 --> 02:08:42.040]   controller
[02:08:42.040 --> 02:08:44.040]   the last day for
[02:08:44.040 --> 02:08:47.480]   Stadia they also released a snake game
[02:08:47.480 --> 02:08:51.160]   Like a worm game that you can play
[02:08:51.160 --> 02:08:53.960]   Uh, even without stadia
[02:08:53.960 --> 02:08:56.520]   Most dies but snake lives on yeah
[02:08:56.520 --> 02:08:58.760]   Thanks
[02:08:58.760 --> 02:09:04.360]   most uh most of the games on stadia apparently have uh given you a way to cross
[02:09:04.360 --> 02:09:11.400]   Save migrate your characters and your saves to uh, another platform. You probably should do that
[02:09:11.400 --> 02:09:13.800]   Um
[02:09:13.800 --> 02:09:16.280]   But today's the last day stadia is done
[02:09:16.280 --> 02:09:19.640]   done, done, done
[02:09:19.640 --> 02:09:22.760]   The google home apps new tv remote adds
[02:09:22.760 --> 02:09:26.360]   volume and play controls g
[02:09:28.040 --> 02:09:30.040]   Seems like that something a remote should have
[02:09:30.040 --> 02:09:32.680]   You could
[02:09:32.680 --> 02:09:36.120]   What is new that's a new that's new log worthy
[02:09:36.120 --> 02:09:38.600]   Yeah
[02:09:38.600 --> 02:09:43.240]   It apparently didn't have volume or play controls. I don't know what it goes controlling at all
[02:09:43.240 --> 02:09:48.280]   Um, it's just for google tv. So you have to use
[02:09:48.280 --> 02:09:50.360]   those
[02:09:50.360 --> 02:09:51.400]   like
[02:09:51.400 --> 02:09:54.360]   TVs, I guess that have google tv
[02:09:54.360 --> 02:09:56.440]   that's just
[02:09:56.440 --> 02:10:02.040]   Silly by now you've already given up trying to use that remote. I come i'm almost certain right because it doesn't do anything
[02:10:02.040 --> 02:10:05.160]   Google is getting in on the air tag business actually
[02:10:05.160 --> 02:10:09.560]   I don't know how good this is for apple apple sells these air tags it are trackers
[02:10:09.560 --> 02:10:11.800]   I put one of my luggage. I put one of my keys
[02:10:11.800 --> 02:10:14.600]   google says
[02:10:14.600 --> 02:10:20.840]   That they are going to well actually I shouldn't say google says it a android researcher
[02:10:20.840 --> 02:10:26.120]   Kubo wojikowski has spotted code for a google bluetooth tracker
[02:10:26.840 --> 02:10:28.840]   a tracker code named
[02:10:28.840 --> 02:10:31.240]   groggie
[02:10:31.240 --> 02:10:32.360]   groggoo
[02:10:32.360 --> 02:10:35.320]   Is this a mandalorian character groggie?
[02:10:35.320 --> 02:10:40.840]   groggoo, it's the name of the child. Is that the baby weasel?
[02:10:40.840 --> 02:10:44.520]   Yes, okay
[02:10:44.520 --> 02:10:47.960]   He's a weasel right? Yo does a weasel isn't he?
[02:10:47.960 --> 02:10:51.560]   Uh groggoo. Oh, I didn't know that okay
[02:10:52.200 --> 02:10:57.240]   Uh wojikowski found references that check nearly every major box you would want a bluetooth tracker
[02:10:57.240 --> 02:11:03.160]   It has a speaker uwb compatibility so you can aim your phone and figure out where stuff is sports bluetooth le
[02:11:03.160 --> 02:11:05.960]   being built by the nest team
[02:11:05.960 --> 02:11:10.280]   They're going to face a lot of the same
[02:11:10.280 --> 02:11:15.000]   uh criticism zapple got for its air tag because people were using it to stalk
[02:11:15.000 --> 02:11:18.280]   X's and so forth
[02:11:18.280 --> 02:11:20.600]   uh one of the regulars who's uh on our um
[02:11:21.400 --> 02:11:23.400]   all about android show uh michal ramen
[02:11:23.400 --> 02:11:29.880]   Also has posted recently about a locator tag option in google's fast pair developer console
[02:11:29.880 --> 02:11:35.160]   Michelle's very good at finding this stuff. So there's more androids out there than iphones, right?
[02:11:35.160 --> 02:11:38.440]   Exactly. Oh well worldwide. It's a vast, you know
[02:11:38.440 --> 02:11:45.160]   Market share so yeah, this will be interesting. They will I will very curious one of the reasons the apple air tag works
[02:11:45.160 --> 02:11:50.600]   So well, is there a lot of iphones in the us anyway? There are a lot of iphones so everywhere I go
[02:11:51.400 --> 02:11:57.000]   Uh if an iphone sees this it can phone home. It could say oh, uh, you know
[02:11:57.000 --> 02:12:03.080]   I can I can go onto my iphone. I could say where's my air tag and is that doing that constantly Leo? Yeah, is it is it
[02:12:03.080 --> 02:12:08.920]   So why is there not so good to talk to me about the privacy pieces of this the surveillance well
[02:12:08.920 --> 02:12:13.640]   Apple put a lot of features in to say to keep um
[02:12:13.640 --> 02:12:19.400]   Well, first of all the government can't see it only I can see it and when another phone sees this
[02:12:19.800 --> 02:12:23.320]   It is not identified as mine. It's it's it they have a system
[02:12:23.320 --> 02:12:28.360]   I think a very good system of relaying that information back to me without completely anonymously
[02:12:28.360 --> 02:12:32.040]   Uh, I put them in the government's appeal to that
[02:12:32.040 --> 02:12:38.600]   That leakage well. Yeah, but they have my phone also following me around. I mean it's not like it's not like they know
[02:12:38.600 --> 02:12:41.800]   And it isn't where I am by the way. It is wherever I've attached it
[02:12:41.800 --> 02:12:47.400]   So I for instance whenever I travel I have one in my luggage and we've seen many stories of people
[02:12:48.280 --> 02:12:52.680]   Who uh, you know, there was a guy deltay airline said, yeah, you're we don't know where your bag is
[02:12:52.680 --> 02:12:55.480]   And he says why do it's actually in london. I can tell
[02:12:55.480 --> 02:13:00.280]   And so there've been a lot of stories like that. I do keep one in my bag
[02:13:00.280 --> 02:13:04.200]   I keep one of my keys more for me because that's the one thing I you know, I have to leave that
[02:13:04.200 --> 02:13:07.240]   But if you are not ever did a burger mystery here
[02:13:07.240 --> 02:13:11.960]   You're a murderer or you turned your phone off so you make a track, but you forgot you carry the tag with you
[02:13:11.960 --> 02:13:15.000]   He keys are with you. Yeah, right?
[02:13:15.320 --> 02:13:20.120]   Then that can be that people are seem to be much more worried about the fact that I could snick slip this
[02:13:20.120 --> 02:13:25.560]   Into somebody's pockets growth and follow her home and that has happened
[02:13:25.560 --> 02:13:29.080]   Uh, it ignores the fact that there are already gps trackers
[02:13:29.080 --> 02:13:33.720]   That actually much better at doing that that are not very expensive fact less than an air tag
[02:13:33.720 --> 02:13:36.200]   But apples done a lot of things for instance
[02:13:36.200 --> 02:13:41.480]   If you're being followed around by an air tag that's not yours if you have an iPhone the iPhone will say I see
[02:13:41.480 --> 02:13:43.880]   A foreign air tag following us
[02:13:44.280 --> 02:13:49.560]   You know, is that yours? And if you this is the problem if you don't have an iPhone there is a app
[02:13:49.560 --> 02:13:53.400]   You have to install on your android phone, which works kind of not so great
[02:13:53.400 --> 02:13:57.560]   That'll do the same thing so you but you have to remember to run that app and all that
[02:13:57.560 --> 02:14:00.520]   Google's gonna run into these same problems. It'll be interesting to see how
[02:14:00.520 --> 02:14:03.720]   Google handles this. I think apples handle it as well as you can
[02:14:03.720 --> 02:14:09.160]   Uh handle and it's safe to say google is just gonna try to follow the same suit right?
[02:14:09.160 --> 02:14:12.920]   I mean they both sort of borrow from each other features and
[02:14:13.880 --> 02:14:15.880]   ideas
[02:14:15.880 --> 02:14:21.480]   From now it's time for scooter x's change log because he's much better at finding stuff than i is way better
[02:14:21.480 --> 02:14:24.440]   Google translate now supports 33
[02:14:24.440 --> 02:14:27.800]   New languages and it does it offline
[02:14:27.800 --> 02:14:30.600]   Look at these
[02:14:30.600 --> 02:14:34.280]   Wow scooter x has like 40 things here. I know he's really good. Bask
[02:14:34.280 --> 02:14:36.840]   Freezian hawaiian
[02:14:36.840 --> 02:14:38.760]   Hmong Igbo
[02:14:38.760 --> 02:14:45.400]   Japanese Khmer wow, I mean the languages of the world zulu your ruba yiddish
[02:14:45.400 --> 02:14:47.960]   weaker
[02:14:47.960 --> 02:14:53.080]   All of which is I you're using you didn't hear me my excellent pronunciation. Oh, sir
[02:14:53.080 --> 02:14:56.280]   Hosa
[02:14:56.280 --> 02:14:58.280]   Hosa
[02:14:58.280 --> 02:15:01.640]   There are only 24 languages in the repertoire that do not
[02:15:01.640 --> 02:15:04.600]   Yet support offline
[02:15:04.600 --> 02:15:06.920]   But they're working on it. That's impressive
[02:15:07.880 --> 02:15:14.040]   Fitbit step and heart rate data is appearing now in google fit good news remember google bought fitbit
[02:15:14.040 --> 02:15:18.120]   Uh, but now they're able to get into the google fit tracker
[02:15:18.120 --> 02:15:24.600]   Digital well-being preps focus mode holidays and removes work on live paper don't know what it means
[02:15:24.600 --> 02:15:26.920]   But it's in the change log
[02:15:26.920 --> 02:15:34.680]   Google i'm just reading them now google messages end-to-end encryption for group chats is currently limited to 21 people
[02:15:36.040 --> 02:15:38.600]   Our car dealership lost our keys. Oh, no, that's not it
[02:15:38.600 --> 02:15:45.320]   I should just read the chat someday just a way to fast whereas
[02:15:45.320 --> 02:15:50.760]   Yeah scooter x clear calling on the pixel seven is like noise cancellation for hard to hear phone calls
[02:15:50.760 --> 02:15:57.080]   Chrome os 109 is rolling out with a tweak to how android apps launch now scooter x is like challenge
[02:15:57.080 --> 02:15:59.560]   She's gonna start cranking them out big time google spring
[02:16:01.400 --> 02:16:07.240]   Google's bringing new features to older versions of android and right out of redesign rolling out dragable seek bar music apps
[02:16:07.240 --> 02:16:10.680]   Google discover rolls out three column ui ahead of pixel tablet
[02:16:10.680 --> 02:16:16.360]   Android 13 is running on 5.2 percent of all devices five months after launch
[02:16:16.360 --> 02:16:22.440]   And a story that's not google firefox has found a way to keep bad blockers working with manifest v3 remember goo
[02:16:22.440 --> 02:16:27.160]   Scooter x do you have you have this all ready for the show just no this moment? It's just fast every week good
[02:16:27.160 --> 02:16:31.080]   He's like mr. He's good. He's very scooter x is very fast. Oh, I know that
[02:16:31.080 --> 02:16:36.360]   I just wow that was a singularity his his like already. He's already achieved. He says I has
[02:16:36.360 --> 02:16:39.560]   Scooter x is chat cheap
[02:16:39.560 --> 02:16:46.680]   I often wonder and that's the google change log we're gonna wrap this up before you can come up with some more
[02:16:46.680 --> 02:16:52.520]   That was exhausting
[02:16:52.520 --> 02:16:55.880]   Before we get to our picks of the week. I just want to give a little plug
[02:16:56.680 --> 02:17:01.880]   For our fabulous folks in the club twit. I thank you club twit members
[02:17:01.880 --> 02:17:07.000]   Squir x is one of them for supporting everything we do. It is very much appreciated
[02:17:07.000 --> 02:17:09.800]   Stacy had a wonderful book club. I hear
[02:17:09.800 --> 02:17:13.240]   What happened to you man?
[02:17:13.240 --> 02:17:18.280]   That my tesla drove into a fence and I couldn't know
[02:17:18.280 --> 02:17:22.680]   No, I I got up early
[02:17:23.160 --> 02:17:28.040]   Because I knew it was a nine o'clock thing last thursday. You had a cold shower right then. I should have
[02:17:28.040 --> 02:17:31.320]   I should have okay because I had a cup of coffee
[02:17:31.320 --> 02:17:35.560]   You know, I got geared up for it and I thought well, I'm just gonna lie down close my eyes for a minute
[02:17:35.560 --> 02:17:37.320]   Oh, no
[02:17:37.320 --> 02:17:39.560]   I woke up at 9 45 and I thought
[02:17:39.560 --> 02:17:42.840]   famous last words
[02:17:42.840 --> 02:17:46.840]   You needed the leo wake up call. Yeah, I did
[02:17:46.840 --> 02:17:49.480]   I'm sony sir, but
[02:17:49.480 --> 02:17:56.040]   You just time to wake up and smell the roses. I apologize. I'm so sorry. Did it go well? Did you have a good time?
[02:17:56.040 --> 02:18:02.120]   Um, yeah, I mean we had fun. I think yeah, I'm sorry. I love that book. I would have loved to
[02:18:02.120 --> 02:18:05.400]   Participate have you selected a new book yet for the club?
[02:18:05.400 --> 02:18:15.480]   Hi, not yet. We have our closes. Um, I think the vote closes next Wednesday. I can't remember. Oh, oh what are the candidate dates?
[02:18:17.240 --> 02:18:22.680]   Oh, let me go. I'll go to if I go to a book book club in our
[02:18:22.680 --> 02:18:26.680]   Book Club poll for our next read the school for good mothers
[02:18:26.680 --> 02:18:31.560]   the sea of tranquility or tell me and
[02:18:31.560 --> 02:18:35.480]   Yeah, um
[02:18:35.480 --> 02:18:38.040]   You want to talk about this? The most literary
[02:18:38.040 --> 02:18:45.480]   Um, and it's really good. Um, it's about time travel in space. I've never read any of these school for good mothers is
[02:18:45.960 --> 02:18:52.520]   AI and it's actually ties into should there be a human in the loop when AI is making decisions about people in this case?
[02:18:52.520 --> 02:18:55.400]   moms, um, and then
[02:18:55.400 --> 02:18:57.080]   the
[02:18:57.080 --> 02:18:59.080]   the tell me an ending is
[02:18:59.080 --> 02:19:03.000]   Black mirror meets severance in this thrilling
[02:19:03.000 --> 02:19:07.160]   Novel about a tech company that deletes unwanted memories
[02:19:07.160 --> 02:19:15.560]   The consequences for those forced to deal with what they tried to forget and the doctor who seeks to protect her patients from further harm
[02:19:15.960 --> 02:19:17.480]   Um
[02:19:17.480 --> 02:19:20.280]   Joe harkin the author of that tell me an ending
[02:19:20.280 --> 02:19:23.480]   school for good mothers is by jessim and chan
[02:19:23.480 --> 02:19:29.320]   And uh, sea of tranquility by amily st. john mandell
[02:19:29.320 --> 02:19:35.960]   She's the one who did uh, what's the call station 11? Yeah. Thank you
[02:19:35.960 --> 02:19:40.120]   Nice
[02:19:40.120 --> 02:19:41.000]   I have no brain
[02:19:41.000 --> 02:19:45.640]   So if you are in the club and you participate in the book club you can go to book club in our
[02:19:45.640 --> 02:19:52.840]   Twit club and vote because there's only a few days left the uh rerun of the project. Hellmary, uh,
[02:19:52.840 --> 02:19:55.400]   uh, twit
[02:19:55.400 --> 02:20:01.000]   Book club stacy's book club show that I missed is available in the twit plus feed that's by the way
[02:20:01.000 --> 02:20:05.720]   This is all benefits to members of the club and I really encourage you to join seven bucks a month
[02:20:05.720 --> 02:20:10.920]   And we we'd really thank you aunt and stacey and everybody who bends over backwards to put
[02:20:10.920 --> 02:20:14.440]   Some really great stuff together some really good programming for the club
[02:20:14.440 --> 02:20:16.840]   You get all of the shows ad-free
[02:20:16.840 --> 02:20:20.520]   You get the twit shows on twit plus plus shows that we don't put out
[02:20:20.520 --> 02:20:27.000]   In public my hands on mac at tosh hands on windows on title linux show you also get these special events tomorrow
[02:20:27.000 --> 02:20:28.760]   4 o'clock
[02:20:28.760 --> 02:20:32.360]   Lisa and I will do another inside twit. We haven't done one in a year or two
[02:20:32.360 --> 02:20:38.120]   So that'll be fun when to dow will do one in february night only for club members
[02:20:38.680 --> 02:20:42.280]   And i'm very excited daniel swarez will be joining us
[02:20:42.280 --> 02:20:45.400]   Talk about his new book critical mass, which is all
[02:20:45.400 --> 02:20:50.680]   I just I love his stuff. Yeah, he's gonna be great. So he'll so the way that's gonna work
[02:20:50.680 --> 02:20:52.360]   We're gonna do it in the club
[02:20:52.360 --> 02:20:57.880]   But a portion of it will appear on in the ask the tech guys show and then the full interview will be available as a triangulation show
[02:20:57.880 --> 02:21:01.720]   So everybody will get to hear that interview, but if you want to ask daniel questions
[02:21:01.720 --> 02:21:08.360]   We're gonna have a q&a in the club sammable sammett is doing a uh, ama on march second just added
[02:21:09.000 --> 02:21:10.440]   Thank you aunt
[02:21:10.440 --> 02:21:12.440]   That's too much. My job. You have really great
[02:21:12.440 --> 02:21:19.000]   Seven bucks a month gets you all of that plus the discord and we have great conversations in the discord
[02:21:19.000 --> 02:21:25.800]   I hang out in the coding section. We have lovely little back and forths in the coding section
[02:21:25.800 --> 02:21:33.800]   uh, we also uh have a let's play section. We have a a couple of really interesting mine crafts servers
[02:21:33.800 --> 02:21:36.200]   You have to be a member to join so
[02:21:36.600 --> 02:21:41.160]   Uh, again, I think there's some real benefits for your seven dollars and it really helps us
[02:21:41.160 --> 02:21:48.600]   You know someday. I would love it if it was if just the club was it, you know, that was that was the whole kit and kaboodle
[02:21:48.600 --> 02:21:50.440]   Um
[02:21:50.440 --> 02:21:56.120]   We're not there yet, but no ads no algorithms. No algorithms. No ads just content
[02:21:56.120 --> 02:21:59.880]   The algorithm with with algorithm would be whatever the club wants to do
[02:21:59.880 --> 02:22:02.440]   Which is not so bad
[02:22:02.440 --> 02:22:09.160]   Um, go to twit.tv/club twit if you're not yet a member and and join the fun. It's a great group of wonderful people
[02:22:09.160 --> 02:22:15.640]   Stacy you said earlier. Oh, I have something to talk about. What is your thing this week?
[02:22:15.640 --> 02:22:23.160]   Well, I have two ones really easy though one since you mentioned composting and since we've been composting for
[02:22:23.160 --> 02:22:28.360]   Four years since we came here to Washington where we have two for years now
[02:22:29.320 --> 02:22:34.520]   Yeah, wow job has any accent left at all. Yeah, just yesterday you were a Texan
[02:22:34.520 --> 02:22:37.560]   Talking to the Texan, I guess
[02:22:37.560 --> 02:22:44.120]   Anyway, um, this is the oxo makes a really good compost bin. Um, the reason I like it. There's
[02:22:44.120 --> 02:22:49.960]   And I'll just tell you all about it. It's really smooth on the inside. So it's very easy to wash
[02:22:49.960 --> 02:22:56.520]   Um, which is important in a compost bin and it's small. So it'll it's big enough for like
[02:22:57.000 --> 02:22:59.000]   I actually have this
[02:22:59.000 --> 02:23:00.840]   Yeah, dude
[02:23:00.840 --> 02:23:07.400]   I mean the one downside I will say is when you've got something yucky on the bottom and it's stuck and you whack it against your big bin
[02:23:07.400 --> 02:23:10.680]   That lid might fall off. So just be aware of that put everything else
[02:23:10.680 --> 02:23:17.720]   The goon. It's very small. I I have I haven't been able to get anybody to use it
[02:23:17.720 --> 02:23:23.400]   Oh, I love this one. It's pretty on your camera. Yeah, that's the idea food
[02:23:23.960 --> 02:23:28.600]   And it is seals so you don't smell it and then you empty that you have to empty it every day because it's small
[02:23:28.600 --> 02:23:32.360]   But it should be small you want to yeah, you don't want to keep a big compost
[02:23:32.360 --> 02:23:34.440]   I mean, I know it's kind of inconvenient
[02:23:34.440 --> 02:23:40.840]   But you want it small enough like big enough to hold a full meals prep but not too big because then it just sits there
[02:23:40.840 --> 02:23:46.040]   Anyway, so that's it you line it or you just put stuff in there. You don't you don't line it now. It's so smooth
[02:23:46.040 --> 02:23:48.200]   It's it's so easy to clean. Yeah
[02:23:48.200 --> 02:23:52.520]   I mean we wash it out like maybe once or once a week or so
[02:23:52.520 --> 02:23:56.600]   I think I got the bigger I get this one. It was three quarters of a gallon
[02:23:56.600 --> 02:24:01.320]   I think I got the little bit bigger. Yeah, this is the one I got one one point seven five gallons
[02:24:01.320 --> 02:24:03.880]   Yeah, that was too big for my counter. Yeah
[02:24:03.880 --> 02:24:11.000]   And we're we're a small family like if you if you make a lot like we're a small family. We're the same number three people
[02:24:11.000 --> 02:24:15.400]   So we should probably use that if I can get people to use it because right now
[02:24:15.400 --> 02:24:21.160]   We put it in the regular garbage and it gets soggy and we line it with the plastic thing
[02:24:21.160 --> 02:24:24.440]   But we can't throw the plastic thing in the compost. It's kind of not a good
[02:24:24.440 --> 02:24:27.880]   They won't let us put any
[02:24:27.880 --> 02:24:33.480]   Even though there's you know compostable bags or anything they won't let us put those in the compost
[02:24:33.480 --> 02:24:35.560]   It has to just be food food nothing else
[02:24:35.560 --> 02:24:44.280]   Okay, but my actual thing that I had planned was because Kevin did a review on this because his wife bought this and barb does not buy anything
[02:24:44.280 --> 02:24:46.920]   That's smart. It's a smart kettle and I never
[02:24:48.600 --> 02:24:50.600]   Bies I only buy some things
[02:24:50.600 --> 02:24:55.640]   No, she just anything that's connected barb is like really nice terminals
[02:24:55.640 --> 02:24:59.560]   So what is this?
[02:24:59.560 --> 02:25:06.040]   This is the govie smart kettle and I don't know if I would have chosen the govie just because it is a chinese company
[02:25:06.040 --> 02:25:12.600]   They do have good privacy policies. I don't know about this. Wait, wait, what could they know about your tea?
[02:25:12.600 --> 02:25:17.160]   Well, it's on your network. I'm less concerned about the privacy policy
[02:25:18.040 --> 02:25:24.840]   Also security is really the big issue anytime you put anything on your on your Wi-Fi. Yeah, it has the great pour over
[02:25:24.840 --> 02:25:30.200]   Kettle spout though. I like that. Yeah, it has that goo snacks spout. Yeah, but
[02:25:30.200 --> 02:25:32.680]   This is I I want this
[02:25:32.680 --> 02:25:37.720]   You can tell amazon or google power on the kettle
[02:25:37.720 --> 02:25:42.520]   Like whatever temperature what and it'll just you can also remotely start it from the app
[02:25:42.520 --> 02:25:47.080]   But who wants to open an app to start you just want to make sure there's always water in it obviously, but yes
[02:25:47.640 --> 02:25:49.960]   It's induction so that's burn or anything
[02:25:49.960 --> 02:25:52.680]   Yes
[02:25:52.680 --> 02:25:58.840]   And apparently kevin didn't he tried to he said he's turned on when there was no water in it
[02:25:58.840 --> 02:26:04.280]   But govie says that it won't stay on so kevin i'm not a tribe for long enough
[02:26:04.280 --> 02:26:09.800]   So does it not boil over as well because that's my problem with my goose neck sometimes I
[02:26:09.800 --> 02:26:12.520]   Forget a few minutes
[02:26:12.520 --> 02:26:16.440]   And you don't want to fill over the max and it will turn itself off when it boils
[02:26:16.440 --> 02:26:18.680]   It doesn't oh it doesn't continue to boil
[02:26:18.680 --> 02:26:22.200]   That's the beauty of this but ours, you know, we have a Breville kettle
[02:26:22.200 --> 02:26:23.960]   It's not online, but it does all of that
[02:26:23.960 --> 02:26:28.200]   But the one thing it doesn't do is turn on when you're in bed you have to get up
[02:26:28.200 --> 02:26:30.600]   Well, yeah kevin created a good morning routine
[02:26:30.600 --> 02:26:36.760]   And now as part of that it turns it turns the kettle on to heat the water which did he rally it on shower
[02:26:38.120 --> 02:26:42.120]   Yeah, he always you up and tells you that the tea is on good morning, sir
[02:26:42.120 --> 02:26:45.480]   The tea is on the world is fresh and bright
[02:26:45.480 --> 02:26:48.840]   Your cold shower is ready cold shower
[02:26:48.840 --> 02:26:53.800]   And the price for this is
[02:26:53.800 --> 02:26:58.680]   Well, barb got it for 65. I think it's on sale most of the time. It's like 70
[02:26:58.680 --> 02:27:03.160]   $80 but I see a 15-dollar coupon on my amazon
[02:27:03.160 --> 02:27:04.200]   So
[02:27:04.200 --> 02:27:06.680]   I think you could get it for what barb got it for
[02:27:07.320 --> 02:27:09.320]   65
[02:27:09.320 --> 02:27:10.760]   Yeah, nice
[02:27:10.760 --> 02:27:12.280]   the govie
[02:27:12.280 --> 02:27:14.280]   smart electric kel
[02:27:14.280 --> 02:27:15.860]   Mr. Jeff
[02:27:15.860 --> 02:27:22.200]   Fellows makes oh, sorry. I was gonna say I think it's a knockoff of the fellows kettle, which is like 165. Oh, so
[02:27:22.200 --> 02:27:25.000]   There you go. Nice
[02:27:25.000 --> 02:27:27.160]   Govie makes life smarter
[02:27:27.160 --> 02:27:30.440]   Okay, they'd make a lot of knockoffs
[02:27:30.440 --> 02:27:31.880]   Really?
[02:27:31.880 --> 02:27:33.720]   To the kenza
[02:27:33.720 --> 02:27:36.440]   They have an electric space heater smart humidifiers
[02:27:36.920 --> 02:27:42.200]   Their lights are like all their smart lights are like straight up he replicas. Oh wow
[02:27:42.200 --> 02:27:46.920]   China amazing china. Oh, they're amazing
[02:27:46.920 --> 02:27:49.800]   Mr. Jeff Jarvis give us a number
[02:27:49.800 --> 02:27:52.280]   so um
[02:27:52.280 --> 02:27:54.280]   Just today I got the um
[02:27:54.280 --> 02:27:59.960]   Hyperset version of the book I now get to spend my weekend reading my words again and again and again
[02:27:59.960 --> 02:28:05.880]   And you know how much I love seraphs. We talked about this. Oh, no, it's not. It's kind of bright. I can't
[02:28:06.600 --> 02:28:11.320]   Right, so we I love seraphs. I use the dubs font and the sabin font. We're here with a vice
[02:28:11.320 --> 02:28:15.160]   I'm glad and fly. I think for books. I think you should have a seraph font
[02:28:15.160 --> 02:28:17.480]   I think you should yeah, but the state department
[02:28:17.480 --> 02:28:20.920]   Has just is gonna phase out times roman
[02:28:20.920 --> 02:28:24.120]   in favor of the seraphless
[02:28:24.120 --> 02:28:34.040]   Collaborate 14 point and it's a good seraph. Well, I don't know I don't it's because it's for accessibility
[02:28:34.680 --> 02:28:39.720]   Oh, okay. Okay. Secretary blinkin said a cable. They're still sending cables
[02:28:39.720 --> 02:28:42.200]   Senate cable to
[02:28:42.200 --> 02:28:45.160]   To all embassies today directing staff not to send him
[02:28:45.160 --> 02:28:51.560]   Any more papers with times new roman the subject the times new roman are a changin
[02:28:51.560 --> 02:29:00.200]   They want you to use calibri which used to be a cigarette lighter, but I guess now it's a sancera font and 14 point
[02:29:00.200 --> 02:29:03.800]   There is actually
[02:29:04.280 --> 02:29:06.280]   a new font designed
[02:29:06.280 --> 02:29:09.000]   By the braille institute
[02:29:09.000 --> 02:29:13.560]   Oh, oh to be more legible for people with low vision
[02:29:13.560 --> 02:29:17.720]   Um and actually a bookmark it because i'm a font like you
[02:29:17.720 --> 02:29:24.600]   Like lenn Fleischman. I'm kind of a typeface guy. I like to collect typefaces. Let me see if I bookmarked it so I could
[02:29:24.600 --> 02:29:28.440]   Down there's also one later does being recommended for the dyslexic
[02:29:29.160 --> 02:29:33.880]   Yeah, there's does there and I don't like that one because it's it's very much modified
[02:29:33.880 --> 02:29:39.960]   It's for dyslexic people, but I think let me see the the braille institute font actually is very usable
[02:29:39.960 --> 02:29:42.760]   they recommended for websites and and others
[02:29:42.760 --> 02:29:50.520]   uh, because uh, it's called the atkinson hyper legible font and it's free
[02:29:50.520 --> 02:29:52.680]   Uh
[02:29:52.680 --> 02:29:56.760]   And they show as an example. This is the font above but if you have low vision
[02:29:57.240 --> 02:30:01.000]   And it's very blurry. You still can distinguish for instance the one from the capital I
[02:30:01.000 --> 02:30:06.440]   From the lowercase i from the l you see, but it's a you know, it's a good font that anybody
[02:30:06.440 --> 02:30:09.560]   Would use for anything because it's a good sans serif
[02:30:09.560 --> 02:30:13.640]   font, but uh, but if you wanted to make it accessible for people with low vision
[02:30:13.640 --> 02:30:18.280]   This uh, atkinson hyper legible is available from the braille institute
[02:30:18.280 --> 02:30:21.320]   for fonts two weights
[02:30:21.320 --> 02:30:23.640]   1,340 total glyphs
[02:30:25.240 --> 02:30:30.200]   And accent characters for 27 languages, so that's a good I think that's a good thing for the braille institute to do
[02:30:30.200 --> 02:30:34.280]   So speaking of fonts
[02:30:34.280 --> 02:30:41.080]   Calibri I don't like calibri to be honest. I'm not crazy about it either. Yeah, is it what does microsoft use a little lot or yeah?
[02:30:41.080 --> 02:30:43.080]   I think it comes with windows
[02:30:43.080 --> 02:30:45.560]   um, yeah
[02:30:45.560 --> 02:30:47.640]   This is a uh
[02:30:47.640 --> 02:30:49.880]   Subject we could go on and on about the programmers
[02:30:50.600 --> 02:30:56.840]   Collect monoface monotype face fonts for their programming and we all have very strong opinions
[02:30:56.840 --> 02:30:59.720]   I am correct. What?
[02:30:59.720 --> 02:31:07.240]   Programmers have strong opinions. I am currently a fan of iosevka. Don't tell me otherwise aunt pro it. What's your pick of the week?
[02:31:07.240 --> 02:31:09.160]   Oh, man
[02:31:09.160 --> 02:31:14.920]   Well, my pick of the week is a book that I've been listening to because I've had a little bit more time
[02:31:14.920 --> 02:31:19.640]   since project hail mayor was one that I had already read and uh
[02:31:20.520 --> 02:31:25.320]   I've been off of social media, so I had more time and I wanted to check out time's a gurus book
[02:31:25.320 --> 02:31:31.320]   I'd like to play alone. Please. Oh, I like him. He's fun. Me. Yeah, very very funny comedian
[02:31:31.320 --> 02:31:34.840]   He tells a bunch of funny stories in here including his
[02:31:34.840 --> 02:31:42.360]   journey to um being a big stand-up comic now and you know gives all the details of how he got started and
[02:31:42.360 --> 02:31:44.840]   the pain and trauma of that and
[02:31:45.400 --> 02:31:51.880]   funny anecdotes far as um how we are as people socially induce and really quirky things and uh
[02:31:51.880 --> 02:31:57.320]   You can turn it into comedy if you if you want to this is uh, this is unaudible
[02:31:57.320 --> 02:32:02.840]   So you're listening to him. Let me play a little bit of it so you can hear his piece. He narrates it because you'll know you'll know when you hear his voice
[02:32:02.840 --> 02:32:08.600]   Maybe i'm not a junior something. He's pointed out to me his own son
[02:32:08.600 --> 02:32:12.760]   No less than 4,000 times. I like it when comics do their own books
[02:32:13.480 --> 02:32:16.600]   It just comes alive. I love it and it also has some um
[02:32:16.600 --> 02:32:19.320]   PDFs in it if you get the audio version
[02:32:19.320 --> 02:32:23.720]   Where you can pull it up an app and look at some of the pictures that he's describing with
[02:32:23.720 --> 02:32:27.400]   You know hanging out with celebrities and whatnots pretty his pretty funny book
[02:32:27.400 --> 02:32:34.360]   So for i'm almost done with this it's fairly short his last book thrilled has a my favorite cover
[02:32:41.480 --> 02:32:45.640]   It's um, it's the uh, it's a playgirl pose
[02:32:45.640 --> 02:32:48.360]   Michael Jackson
[02:32:48.360 --> 02:32:53.320]   Take up this really album and he is not exactly a michael jackney
[02:32:53.320 --> 02:32:56.360]   Supermodel not a spelt man
[02:32:56.360 --> 02:33:03.160]   He's so funny. He um his dad is a white man and his mother is peruvian
[02:33:03.160 --> 02:33:06.920]   so he he tells a lot of stories about their relationship and
[02:33:06.920 --> 02:33:10.200]   him growing up learning to speak spanish and just the
[02:33:11.080 --> 02:33:15.960]   You know being and being in the south and and having that dynamic and with his family
[02:33:15.960 --> 02:33:19.000]   It's all pretty funny stuff. So now this other one
[02:33:19.000 --> 02:33:23.080]   I think you just put this in because you're trying to get me to take a cold shower
[02:33:23.080 --> 02:33:26.680]   Well speaking of cold tubs
[02:33:26.680 --> 02:33:34.440]   You know I talked about kevin heart um the other week where i've been diving into because youtube
[02:33:34.440 --> 02:33:38.440]   Knows how i how to recommend to me because i do thumbs up on things
[02:33:39.160 --> 02:33:43.080]   And he has this show on lw network called cold as balls
[02:33:43.080 --> 02:33:49.320]   Where um he interviews celebrities, but they do the interviews in a locker room
[02:33:49.320 --> 02:33:53.560]   And there's a cold tub and they start out with their feet in the cold tub
[02:33:53.560 --> 02:33:57.240]   And as the interview goes on they sit down in the cold tub
[02:33:57.240 --> 02:34:00.200]   And as it goes further along they fully
[02:34:00.200 --> 02:34:02.840]   Put the water all the way up to their neck
[02:34:02.840 --> 02:34:09.080]   In the cold tub. This is this is a whole genre of podcasts where you torture your guests or try the ghost
[02:34:09.080 --> 02:34:13.000]   Yeah, and I really it's like the hot ones. It's so good the cold ones
[02:34:13.000 --> 02:34:16.600]   I preferred your wine version last week here. It's so good though
[02:34:16.600 --> 02:34:19.160]   It's so good because kevin heart is so funny
[02:34:19.160 --> 02:34:23.160]   But I love his premise. He says you know what you're going to be in this cold warden
[02:34:23.160 --> 02:34:27.640]   You're going to say some stuff that's pretty funny and oh my god. It looks so cold
[02:34:27.640 --> 02:34:30.200]   I can't believe he would do this to himself
[02:34:30.200 --> 02:34:33.880]   That's what I don't get oh god. It looks awful
[02:34:33.880 --> 02:34:36.200]   Just awful
[02:34:36.200 --> 02:34:39.720]   Marsha lint she probably is used to it. Those things are great
[02:34:39.720 --> 02:34:45.720]   I'm telling you those cold tubs are with ice rate that cold. Yes. Yeah. Oh my gosh
[02:34:45.720 --> 02:34:50.360]   I'm sitting here looking at that now and thinking man. My elbows would feel so much better right now
[02:34:50.360 --> 02:34:57.400]   Oh, I feel great nice and toasty and warm. Yeah. Yeah, that's how my elbows feel good
[02:35:00.680 --> 02:35:06.840]   And don't you get in that hot tub that cold tub? I have a hot tub. I get in not a cold tub
[02:35:06.840 --> 02:35:09.960]   I get in a hot tub too. But when something I don't want to see you
[02:35:09.960 --> 02:35:15.880]   When our boy sits the sets when Michael sets the hot tub to like a hundred two, I said what's happening? It's cold
[02:35:15.880 --> 02:35:19.960]   It's too cold
[02:35:19.960 --> 02:35:25.640]   In the summer for some reason he sets it the lower temperature as I don't want it to be a hundred degrees. That's cold
[02:35:26.440 --> 02:35:30.600]   What do you set your hot tub to? Well, unfortunately in the state of California
[02:35:30.600 --> 02:35:36.280]   We can't set it to hotter than a hundred four and I've been begging hackers to come over and hack my tub
[02:35:36.280 --> 02:35:42.360]   Because a hot bill should be a hundred six, but they don't want me to die. I can have a bathtub
[02:35:42.360 --> 02:35:45.960]   That's a hundred six, but I can't have a hot tub. That's a hundred six. Yeah
[02:35:45.960 --> 02:35:50.760]   Yeah, huh. It seems wrong. What do you say you're a hot tub too?
[02:35:50.760 --> 02:35:55.800]   um a hundred and two or a hundred and three in the winners loop and then look one
[02:35:56.680 --> 02:36:01.720]   Hundred fours and the back and I can't even say I just I think that's the legal limit
[02:36:01.720 --> 02:36:06.120]   They they won't let you I think so. Yeah, because they don't want you boil yourself to death
[02:36:06.120 --> 02:36:09.560]   It seems reasonable
[02:36:09.560 --> 02:36:15.080]   No, that's the nanny state telling me what I can do in my own hot tub
[02:36:15.080 --> 02:36:19.560]   We need a section 230 for hot tub temperatures
[02:36:19.560 --> 02:36:24.040]   Protecting the right to boil California, man
[02:36:25.160 --> 02:36:30.200]   Yes, he right nowhere else in this country. Do you debate hot tubs? It is such a cliche
[02:36:30.200 --> 02:36:36.360]   It is pure California for sure mike says I lived there. It was a joke and it's still a joke
[02:36:36.360 --> 02:36:37.560]   I love my hot
[02:36:37.560 --> 02:36:41.160]   Reverb mike says I set my hot tub to 1977
[02:36:41.160 --> 02:36:52.760]   Got a hot tub time machine ladies gentlemen. We are done for the day. Thank you so much stacy higgin botham stacy on iot
[02:36:53.400 --> 02:37:01.560]   Dot com at giga stacy don't forget her podcast with kevin toful the iot podcast featuring
[02:37:01.560 --> 02:37:07.960]   mr. Compost this week so that'll be fun. Oh, this week is not mr. Compost next week is mr. Compost
[02:37:07.960 --> 02:37:11.800]   Next week mr. Matt comps at compost
[02:37:11.800 --> 02:37:16.440]   Well this week was this week something great too. I'm sure
[02:37:16.440 --> 02:37:21.800]   Uh, yeah, it's a little bit more industrial industrial. Yeah
[02:37:22.280 --> 02:37:24.280]   Oh
[02:37:24.280 --> 02:37:28.200]   Sounds good. It's the CEO of a company called once
[02:37:28.200 --> 02:37:31.560]   They they have a really cool business model. That's all I'll say
[02:37:31.560 --> 02:37:34.680]   Hate to love that theme song right there
[02:37:34.680 --> 02:37:42.760]   Look at this. Look at her discipline one hour and one minute long. That is discipline
[02:37:42.760 --> 02:37:47.480]   Oh, did you feel this was a long show kevin and I were a little crazy this morning
[02:37:47.480 --> 02:37:50.920]   Did you feel bad because you went one minute longer than an hour and an hour?
[02:37:51.080 --> 02:37:52.120]   And over
[02:37:52.120 --> 02:37:55.960]   No, we we we aim to not go too far above an hour though
[02:37:55.960 --> 02:38:01.800]   Listen to this hour before 57 minutes week before an hour three minutes week before that an hour one minute
[02:38:01.800 --> 02:38:09.320]   She's doing an hour long podcast. She's the higgin bot. She's a machine. Is that too long? No
[02:38:09.320 --> 02:38:17.000]   Too long. How long is ours? I asked well, we got we got twice as many people. Oh, maybe that's why that's a good excuse
[02:38:19.000 --> 02:38:23.320]   We all want our air time everybody asked us. Yeah, thank you stacey
[02:38:23.320 --> 02:38:26.440]   Jeff Jarvis this cat right here, man
[02:38:26.440 --> 02:38:31.080]   Frank Sinatra called him a bum ray crock called a man nickel millionaire
[02:38:31.080 --> 02:38:34.040]   He lived in tomato fields in his youth
[02:38:34.040 --> 02:38:37.560]   And he has trapped in the jungle gym of life
[02:38:37.560 --> 02:38:43.240]   But i'm think furthermore the director of the townite center for entrepreneurial journalism at the cray
[02:38:43.240 --> 02:38:47.960]   cray cray cray cray cray cray
[02:38:48.840 --> 02:38:51.320]   craic there's a lot of cranks in this craic
[02:38:51.320 --> 02:38:58.040]   School of journalism is pretty city university in New York
[02:38:58.040 --> 02:39:05.800]   We thank our listeners for sending those in please send it send in i would like i would like a c shanty version of craig new mark if you don't
[02:39:05.800 --> 02:39:14.520]   Oh, oh that'd be good. Yeah, that'd be good. And Pruitt hosts hands-on photography to it that tv slash hop hop on over and listen the latest episode
[02:39:14.520 --> 02:39:16.520]   What are you talking about this week?
[02:39:16.520 --> 02:39:22.520]   Okay, i'm finally going to talk about and another option for macro photography this week
[02:39:22.520 --> 02:39:25.480]   I put it off last time in this week that's going to be the
[02:39:25.480 --> 02:39:29.240]   Discussion and save some folks some money to be able to take some cool
[02:39:29.240 --> 02:39:34.040]   macro photography shots should i be saving up my toilet paper rolls just for this episode
[02:39:34.040 --> 02:39:37.400]   Nope it no
[02:39:37.400 --> 02:39:43.080]   Nope toilet paper and tin foil no tp rolls required
[02:39:43.080 --> 02:39:48.760]   So just get just have a good flashlight and yeah, we're gonna tip excellent excellent
[02:39:48.760 --> 02:39:54.760]   Uh and also as a community manager of club twit and that's such a good job. We really appreciate that
[02:39:54.760 --> 02:40:00.840]   Uh, we will be on to it tomorrow. Lisa and i i think you could take the day off will Lisa and i can handle this i think
[02:40:00.840 --> 02:40:05.240]   Uh, it'll be funny. It'll be funny here as fight, but you don't want to be there
[02:40:05.240 --> 02:40:10.280]   You don't want to be in the middle of that. Uh, we will be doing that at four o'clock tomorrow pacific
[02:40:10.680 --> 02:40:14.600]   Seven eastern and i hope you will tune in for that new club twit members
[02:40:14.600 --> 02:40:20.520]   Uh, I thank you so much for tuning in for this week in google. We do this week in google every third wednesday
[02:40:20.520 --> 02:40:26.200]   Uh, wednesday at two p.m pacific five p.m eastern 2200 utc
[02:40:26.200 --> 02:40:29.560]   You can watch us do it live at live dot twit.tv
[02:40:29.560 --> 02:40:34.520]   Uh chat with us at irc.twit.tv that's open to all or in our club twit
[02:40:34.520 --> 02:40:37.720]   Uh club house if you are a club twit member
[02:40:38.200 --> 02:40:42.840]   We also have uh versions of the show for your listening pleasure after the fact
[02:40:42.840 --> 02:40:49.800]   It's called a podcast if you go to twit.tv/twig you can download a copy there or you see those buttons you can subscribe in your
[02:40:49.800 --> 02:40:52.280]   Favorite podcast player. There's also a youtube
[02:40:52.280 --> 02:40:56.680]   Uh channel dedicated to the program
[02:40:56.680 --> 02:41:01.640]   And you can watch it there or listen to it there or share clips of it from youtube
[02:41:01.640 --> 02:41:05.880]   Which is always a nice way if you hear something you like and you want to tell your friends about it
[02:41:05.880 --> 02:41:08.920]   Just share a clip from youtube so they can discover the goodness
[02:41:08.920 --> 02:41:12.920]   The goodness gracious that is this week in google
[02:41:12.920 --> 02:41:20.600]   Well folks we'll be back next week if uh the good lords will end the creeks don't rise have a wonderful evening and we'll see you next time
[02:41:20.600 --> 02:41:22.200]   on twig
[02:41:22.200 --> 02:41:23.400]   Bye-bye
[02:41:23.400 --> 02:41:27.240]   Don't miss all about android every week. We talk about the latest news
[02:41:27.240 --> 02:41:33.000]   Hardware apps and now all the developer e goodness happening in the android ecosystem
[02:41:33.240 --> 02:41:41.240]   I'm Jason howl also joined by ron ritards florin scion and our newest co-host on the panel when to dao who brings her developer chops
[02:41:41.240 --> 02:41:47.320]   Really great stuff. We also invite people from all over the android ecosystem to talk about this mobile platform
[02:41:47.320 --> 02:41:52.520]   We love so much join us every tuesday all about android on twit.tv
[02:41:52.520 --> 02:41:56.520]   [Music]
[02:41:56.520 --> 02:42:03.800]   [Music]

