;FFMETADATA1
title=Oath It!
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=399
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.280]   It's time for Twig this week at Google Stacey Higginbotham is here.
[00:00:03.280 --> 00:00:07.600]   I'm sorry to say Jeff Jarvis has the week off. He's in Italy, but good news.
[00:00:07.600 --> 00:00:12.560]   Mike Galgan is here. We're going to give you a tour of the brand new YouTube TV.
[00:00:12.560 --> 00:00:17.760]   Just came out today and find out why Mike says this is the worst news for cable television ever.
[00:00:17.760 --> 00:00:19.280]   Coming up next on Twig.
[00:00:19.280 --> 00:00:24.160]   Netcast you love.
[00:00:24.160 --> 00:00:26.160]   From people you trust.
[00:00:29.840 --> 00:00:31.280]   This is Twig.
[00:00:31.280 --> 00:00:36.080]   Bandwidth for this week in Google is provided by cash fly.
[00:00:36.080 --> 00:00:39.280]   C-A-C-H-E-F-L-Y dot com.
[00:00:39.280 --> 00:00:50.240]   This is Twig this week in Google episode 399 recorded Wednesday April 5th 2017.
[00:00:50.240 --> 00:00:55.680]   Oh, this week at Google is brought to you by Rocket Mortgage from Quick and Loans.
[00:00:55.680 --> 00:00:58.400]   When it comes to the big decision of choosing a mortgage lender,
[00:00:58.400 --> 00:01:01.120]   work with one that has your best interest in mind.
[00:01:01.120 --> 00:01:06.640]   Use Rocket Mortgage for a transparent, trustworthy, home loan process that's completely online
[00:01:06.640 --> 00:01:09.200]   at quickandlones.com/twig.
[00:01:09.200 --> 00:01:13.680]   It's time for Twig this week in Google the show where we cover well.
[00:01:13.680 --> 00:01:14.960]   It's not really about Google at all.
[00:01:14.960 --> 00:01:21.600]   It's kind of false advertising. It's about whatever we want to talk about.
[00:01:21.600 --> 00:01:25.440]   Jeff Jarvis isn't here so it'll be a little less on the journalism side.
[00:01:25.440 --> 00:01:27.680]   He's the professor of journalism at CUNY.
[00:01:27.680 --> 00:01:29.280]   But Stacey Higginbotham is here.
[00:01:29.280 --> 00:01:31.520]   Of course, she's the queen of IoT.
[00:01:31.520 --> 00:01:33.120]   IoTpodcast.com.
[00:01:33.120 --> 00:01:35.440]   She's also the king of IoT.
[00:01:35.440 --> 00:01:36.960]   I don't want to be sexist about anything.
[00:01:36.960 --> 00:01:39.440]   You could still call me the queen of.
[00:01:39.440 --> 00:01:41.200]   She's in charge of IoT.
[00:01:41.200 --> 00:01:44.000]   And at Gigas Stacey, hi Stacey.
[00:01:44.000 --> 00:01:45.040]   Hello.
[00:01:45.040 --> 00:01:48.880]   Also joining us today because we don't have Jeff.
[00:01:48.880 --> 00:01:51.600]   We've got, but I'm even more happy to say.
[00:01:51.600 --> 00:01:54.880]   The next best thing from Gastronomad.net.
[00:01:54.880 --> 00:01:56.320]   Mike Elgin.
[00:01:56.320 --> 00:01:57.040]   Yay, Mike.
[00:01:57.040 --> 00:01:57.600]   Yay.
[00:01:57.600 --> 00:01:59.280]   Long time to be here here at Twig.
[00:01:59.280 --> 00:02:02.640]   And writes for Computer World and Fast Company and everybody.
[00:02:02.640 --> 00:02:06.480]   I love doing this show because even when I listen every single week,
[00:02:06.480 --> 00:02:09.600]   I'm always interjecting things and nobody can hear me.
[00:02:09.600 --> 00:02:10.080]   Right.
[00:02:10.080 --> 00:02:10.720]   No, they can.
[00:02:10.720 --> 00:02:11.440]   Now you got a mic.
[00:02:11.440 --> 00:02:11.680]   Yes.
[00:02:11.680 --> 00:02:17.440]   And by the speaking of digital nomad, you're going to be starting the next week.
[00:02:17.440 --> 00:02:18.640]   You're going to be going to Spain.
[00:02:18.640 --> 00:02:19.680]   That's right.
[00:02:19.680 --> 00:02:21.760]   To get set up for your Barcelona.
[00:02:22.480 --> 00:02:26.240]   That's right. My wife is spearheading a new business where we have
[00:02:26.240 --> 00:02:29.680]   Gastronomad experiences where we invite people to come with us.
[00:02:29.680 --> 00:02:34.160]   If anybody out there loves traveling and food, this is going to be a unique experience.
[00:02:34.160 --> 00:02:35.600]   So we're going to go out there and do the research.
[00:02:35.600 --> 00:02:38.480]   So we're going to be doing the hard work of tasting everything
[00:02:38.480 --> 00:02:42.160]   and trying all these different foods and wines and things like that in Barcelona.
[00:02:42.160 --> 00:02:45.520]   And then we're going to go off to Fez Morrocco and we're going to do the same thing there.
[00:02:45.520 --> 00:02:46.960]   Somebody's got to do it, Leo.
[00:02:46.960 --> 00:02:47.840]   Somebody's got to do it.
[00:02:47.840 --> 00:02:48.880]   How fun is this?
[00:02:48.880 --> 00:02:51.520]   What a great idea to do this.
[00:02:52.160 --> 00:02:52.960]   Yeah.
[00:02:52.960 --> 00:02:54.160]   It's going to be so much fun.
[00:02:54.160 --> 00:02:55.280]   Barcelona's first.
[00:02:55.280 --> 00:03:00.560]   And by the way, I hope Amira cooks a little bit for this because she's the best cook I know,
[00:03:00.560 --> 00:03:01.840]   literally the best cook I know.
[00:03:01.840 --> 00:03:04.000]   She is the best cook I know as well.
[00:03:04.000 --> 00:03:06.080]   She's probably the best cook in the universe.
[00:03:06.080 --> 00:03:07.760]   That's my best estimate.
[00:03:07.760 --> 00:03:11.920]   And she is going to be teaching people how to make paella and things like that in each location.
[00:03:11.920 --> 00:03:12.800]   She does that anyway.
[00:03:12.800 --> 00:03:13.360]   It's really funny.
[00:03:13.360 --> 00:03:16.080]   We go to like Greece and then she masters all the Greek cooking
[00:03:16.080 --> 00:03:17.520]   and she's doing it better than the Greeks.
[00:03:18.240 --> 00:03:20.640]   And it's going to be fantastic.
[00:03:20.640 --> 00:03:26.000]   So much fun and now you know Stacy now you know why I want to go.
[00:03:26.000 --> 00:03:27.760]   Yes.
[00:03:27.760 --> 00:03:30.160]   I'm like, oh, I also would like to go.
[00:03:30.160 --> 00:03:30.800]   Yes.
[00:03:30.800 --> 00:03:32.000]   We're all going to do it.
[00:03:32.000 --> 00:03:32.560]   Stacy, do it.
[00:03:32.560 --> 00:03:33.760]   Mike, stop booking.
[00:03:33.760 --> 00:03:35.360]   We're just everybody at Twitch going to come.
[00:03:35.360 --> 00:03:40.240]   The Barcelona experience September 12 through 17th and then the Morocco experience
[00:03:40.240 --> 00:03:42.640]   September 25th, October 2nd.
[00:03:42.640 --> 00:03:47.120]   You're going to do Mexico next year and I'm sure there'll be a lot more to come on this, right?
[00:03:47.680 --> 00:03:48.240]   Absolutely.
[00:03:48.240 --> 00:03:51.920]   One of the places we're going to be, we're going to be in 10 for 10 days in Prosecco, Italy.
[00:03:51.920 --> 00:03:54.080]   So we're going to do a Prosecco one next year.
[00:03:54.080 --> 00:03:58.640]   Lots of people haven't discovered Prosecco, but obviously they know the wine, but the
[00:03:58.640 --> 00:04:02.800]   region is just a fantastic foodie area north of Venice.
[00:04:02.800 --> 00:04:05.920]   Actually, that's where Jeff is right now.
[00:04:05.920 --> 00:04:06.400]   He's in Italy.
[00:04:06.400 --> 00:04:09.120]   He's in Perugia for the International Journalist Festival.
[00:04:09.120 --> 00:04:09.920]   Have you ever been there?
[00:04:09.920 --> 00:04:10.000]   Yeah.
[00:04:10.000 --> 00:04:11.920]   No, I haven't.
[00:04:11.920 --> 00:04:12.640]   You went, right?
[00:04:12.640 --> 00:04:14.240]   No, I've never been.
[00:04:14.240 --> 00:04:15.920]   Oh, almost goes.
[00:04:15.920 --> 00:04:17.680]   And Matthew always goes.
[00:04:17.680 --> 00:04:18.400]   Yeah.
[00:04:18.400 --> 00:04:19.440]   I don't know if Ome always goes.
[00:04:19.440 --> 00:04:22.080]   Ome's going a few times because I've seen his pictures from Perugia.
[00:04:22.080 --> 00:04:23.120]   Okay.
[00:04:23.120 --> 00:04:23.600]   Well, there you go.
[00:04:23.600 --> 00:04:24.560]   Matthew, you're right.
[00:04:24.560 --> 00:04:26.000]   Matthew goes every year, Matthew Ingram.
[00:04:26.000 --> 00:04:31.920]   Yes, because Lord help him if he can't post other things on his, like, it's not Instagram,
[00:04:31.920 --> 00:04:33.840]   his Christmas letter, whatever that makes you jealous.
[00:04:33.840 --> 00:04:36.160]   Like the Canadian Lake in the Italy.
[00:04:36.160 --> 00:04:36.720]   I know.
[00:04:36.720 --> 00:04:37.280]   I know.
[00:04:37.280 --> 00:04:39.760]   Every summer I watch his Instagram.
[00:04:39.760 --> 00:04:41.360]   He's always canoeing out in the lake.
[00:04:41.360 --> 00:04:43.040]   It's like, oh, what a life.
[00:04:43.040 --> 00:04:45.840]   Oh, it was terrible working with him.
[00:04:45.840 --> 00:04:46.320]   I bet.
[00:04:46.320 --> 00:04:50.480]   Stacey, I have to file this before too because, you know, I got to get out of the lake.
[00:04:50.480 --> 00:04:58.960]   Big scoop from Jessica Lessons, the information, and they've got, you know, they have so much
[00:04:58.960 --> 00:05:01.040]   good connections.
[00:05:01.040 --> 00:05:03.840]   We knew the information would be really good on covering Google.
[00:05:03.840 --> 00:05:09.200]   And Amir Afrati is really connected, but this one actually is from Kevin McLaughlin.
[00:05:09.200 --> 00:05:11.120]   So Kevin is really great.
[00:05:11.120 --> 00:05:14.880]   He actually broke a really nice Cisco story a couple days ago too about them separating
[00:05:14.880 --> 00:05:16.080]   their hardware and software business.
[00:05:16.080 --> 00:05:16.400]   Wow.
[00:05:16.400 --> 00:05:18.800]   Kevin is a stellar reporter.
[00:05:18.800 --> 00:05:19.440]   It's really, yeah.
[00:05:19.440 --> 00:05:20.560]   Super good.
[00:05:20.560 --> 00:05:22.080]   Old school.
[00:05:22.080 --> 00:05:23.200]   So this is interesting.
[00:05:23.200 --> 00:05:26.960]   It's the headline is Google Plots New Hardware to Take On Echo.
[00:05:26.960 --> 00:05:32.720]   But one of the things that's revealed in this is that the Google's considering making the
[00:05:32.720 --> 00:05:39.040]   Google home, which is right now kind of an echo competitor speaker, making it also a Wi-Fi
[00:05:39.040 --> 00:05:42.400]   router, part of their Google mesh Wi-Fi.
[00:05:43.200 --> 00:05:45.040]   And this makes it all the sense in the world.
[00:05:45.040 --> 00:05:45.760]   Yeah.
[00:05:45.760 --> 00:05:46.320]   Absolutely.
[00:05:46.320 --> 00:05:50.800]   Remember when they were rumored to be coming out with the initial router,
[00:05:50.800 --> 00:05:53.760]   and they were also rumored to be coming out with Google Home, and everybody assumed,
[00:05:53.760 --> 00:05:54.960]   "Oh, they're going to do this great thing.
[00:05:54.960 --> 00:05:56.400]   They're going to add the two together.
[00:05:56.400 --> 00:05:59.840]   Every router, which you'll have two or three in the house or more,
[00:05:59.840 --> 00:06:02.800]   will be a Google Home device and won't that be great?"
[00:06:02.800 --> 00:06:03.840]   And then they didn't do it.
[00:06:03.840 --> 00:06:04.320]   Yep.
[00:06:04.320 --> 00:06:04.800]   Yeah.
[00:06:04.800 --> 00:06:06.960]   No, the on hub was practically dead on arrival.
[00:06:06.960 --> 00:06:08.000]   It was a terrible router.
[00:06:08.000 --> 00:06:10.560]   It promised all this additional stuff.
[00:06:10.560 --> 00:06:11.600]   It had built into it.
[00:06:12.400 --> 00:06:14.960]   Didn't have a Zigbee radio, a Stacey built into it?
[00:06:14.960 --> 00:06:16.160]   It still does.
[00:06:16.160 --> 00:06:17.760]   Yeah, but it was never turned on, right?
[00:06:17.760 --> 00:06:23.120]   I think you can actually control your hues with the on hub router.
[00:06:23.120 --> 00:06:23.760]   Oh, okay.
[00:06:23.760 --> 00:06:25.520]   I think that's actually a thing you can do.
[00:06:25.520 --> 00:06:28.640]   And it supports your August lock with the Bluetooth in it.
[00:06:28.640 --> 00:06:29.120]   Oh, wow.
[00:06:29.120 --> 00:06:31.840]   So you could do two things.
[00:06:31.840 --> 00:06:34.800]   But they don't do they still sell that?
[00:06:34.800 --> 00:06:38.720]   And if you have an on hub, does it automatically join with a Google Wi-Fi to make a mesh?
[00:06:38.720 --> 00:06:41.600]   So the on hub does work with Google Wi-Fi.
[00:06:41.600 --> 00:06:43.600]   Let's see if I can still buy an on hub router.
[00:06:43.600 --> 00:06:49.360]   I had one ordered the minute they announced it and then got cold feet.
[00:06:49.360 --> 00:06:53.280]   When I read Rana Manio's review of it in ours, Technikon, he said,
[00:06:53.280 --> 00:06:55.520]   "The thing is slow is molasses."
[00:06:55.520 --> 00:06:56.960]   So Kevin likes his.
[00:06:56.960 --> 00:06:59.280]   There's two versions.
[00:06:59.280 --> 00:07:01.920]   There's the Google version and there was the TP-Link version.
[00:07:01.920 --> 00:07:03.840]   Because they made that they actually made the hardware.
[00:07:03.840 --> 00:07:06.960]   The TP-Link one doesn't seem to be available.
[00:07:06.960 --> 00:07:07.440]   Interesting.
[00:07:07.440 --> 00:07:09.280]   You can buy the Asus one.
[00:07:09.280 --> 00:07:12.320]   I'm adding it to my Google store card.
[00:07:12.320 --> 00:07:13.440]   Oh, I don't know. Asus sold one.
[00:07:13.440 --> 00:07:14.160]   That's interesting.
[00:07:14.160 --> 00:07:14.560]   Yeah.
[00:07:14.560 --> 00:07:17.360]   It's this round black thing.
[00:07:17.360 --> 00:07:20.480]   It's also $129 just like the Google home.
[00:07:20.480 --> 00:07:24.000]   So I love the idea and you're right.
[00:07:24.000 --> 00:07:25.360]   Somebody's in the chat.
[00:07:25.360 --> 00:07:26.400]   I'm saying this isn't hardware.
[00:07:26.400 --> 00:07:27.360]   This is just software.
[00:07:27.360 --> 00:07:27.520]   Right.
[00:07:27.520 --> 00:07:29.120]   This should be an easy thing to turn on.
[00:07:29.120 --> 00:07:30.000]   Maybe it depends.
[00:07:30.000 --> 00:07:30.560]   Well, no.
[00:07:30.560 --> 00:07:30.960]   Because-
[00:07:30.960 --> 00:07:31.680]   Do they have the mic?
[00:07:31.680 --> 00:07:32.800]   The far field microphones.
[00:07:32.800 --> 00:07:33.360]   Exactly.
[00:07:33.360 --> 00:07:37.120]   And do they have a processor that can handle the always on listening?
[00:07:37.120 --> 00:07:37.360]   Right.
[00:07:37.360 --> 00:07:39.360]   So those two things actually need to be there.
[00:07:39.360 --> 00:07:42.400]   And here's my-
[00:07:42.400 --> 00:07:42.800]   Here-
[00:07:42.800 --> 00:07:43.920]   There are a couple things here.
[00:07:43.920 --> 00:07:46.000]   It makes all the sense in the world to do this.
[00:07:46.000 --> 00:07:49.760]   And actually I'm seeing a lot of IoT devices like light switches
[00:07:49.760 --> 00:07:54.400]   and even things like light bulbs that have Wi-Fi repeaters built into them.
[00:07:54.400 --> 00:07:59.040]   So combining a device that's going to be in every room with a Wi-Fi repeater
[00:07:59.040 --> 00:08:00.000]   makes tons of sense.
[00:08:00.000 --> 00:08:02.560]   Or, you know, making it into a mesh.
[00:08:02.560 --> 00:08:03.600]   However, you want to think about it.
[00:08:03.600 --> 00:08:04.480]   I know they're different.
[00:08:06.720 --> 00:08:09.840]   But with this Google Home thing,
[00:08:09.840 --> 00:08:13.760]   I don't know how many people put their routers still
[00:08:13.760 --> 00:08:17.520]   in a place that is accessible like a listening device needs to be.
[00:08:17.520 --> 00:08:18.080]   Does that make sense?
[00:08:18.080 --> 00:08:21.520]   That's exactly the right problem for this.
[00:08:21.520 --> 00:08:24.640]   And I actually think that in the larger scheme of things,
[00:08:24.640 --> 00:08:27.120]   15 years from now, 10 years from now,
[00:08:27.120 --> 00:08:29.680]   we'll be looking back at this era and say,
[00:08:29.680 --> 00:08:31.040]   remember that Amazon Echo?
[00:08:31.040 --> 00:08:31.600]   What was it called?
[00:08:31.600 --> 00:08:32.240]   The Echo?
[00:08:32.240 --> 00:08:32.800]   You think?
[00:08:32.800 --> 00:08:34.240]   This whole category out of the gone.
[00:08:34.240 --> 00:08:34.560]   Because-
[00:08:34.560 --> 00:08:36.320]   They own this category right now.
[00:08:36.800 --> 00:08:41.840]   It's just a stopgap until we're all wired up with microphones on our physical person.
[00:08:41.840 --> 00:08:46.320]   And so one of the things Stacy's talking about is the fatal problem with this.
[00:08:46.320 --> 00:08:49.840]   So which is that you tuck your router down behind the desk and behind the-
[00:08:49.840 --> 00:08:53.040]   You don't put it out there where you can hear it and where it can hear you.
[00:08:53.040 --> 00:08:55.520]   And so that's a real problem.
[00:08:55.520 --> 00:08:58.960]   Actually, the best place for this sort of utility is in light bulbs.
[00:08:58.960 --> 00:09:00.720]   Light bulbs always have electricity.
[00:09:00.720 --> 00:09:02.160]   They're always in places where you are.
[00:09:02.160 --> 00:09:05.520]   Like you think of a reading light or a lamp or lights in the bathroom
[00:09:05.520 --> 00:09:07.200]   or lights over the dining room table.
[00:09:07.200 --> 00:09:08.720]   If those are all wired up with microphones,
[00:09:08.720 --> 00:09:12.240]   that would be much better because that's where the human-centric activity is.
[00:09:12.240 --> 00:09:17.280]   But ultimately, as soon as we have the ability to interact with our virtual assistants
[00:09:17.280 --> 00:09:20.480]   through our smart glasses and through our wearable devices,
[00:09:20.480 --> 00:09:22.880]   then the behavior will be identical.
[00:09:22.880 --> 00:09:25.120]   We'll talk and Alexa will talk back.
[00:09:25.120 --> 00:09:27.200]   But then we'll be able to do it outside.
[00:09:27.200 --> 00:09:28.720]   We'll be able to do it no matter where we are.
[00:09:28.720 --> 00:09:29.520]   And that'll be better.
[00:09:29.520 --> 00:09:31.280]   Ultimately, I think that's where it's going.
[00:09:31.280 --> 00:09:34.400]   And until somebody like Apple comes out with smart glasses,
[00:09:34.400 --> 00:09:35.360]   I don't think that's going to happen.
[00:09:35.360 --> 00:09:38.160]   Well, so that's why I thought,
[00:09:38.160 --> 00:09:41.200]   so I wrote a big thing when the EarPods came out from Apple,
[00:09:41.200 --> 00:09:46.000]   how this is the beginning of kind of always on listening potential.
[00:09:46.000 --> 00:09:46.320]   Yeah.
[00:09:46.320 --> 00:09:47.840]   So exactly what you're talking about.
[00:09:47.840 --> 00:09:52.240]   I will say though, I think Amazon, we may forget the device,
[00:09:52.240 --> 00:09:56.320]   but what they're building is with their AVS,
[00:09:56.320 --> 00:09:59.120]   so she who shall not be named as a voice services.
[00:09:59.120 --> 00:10:03.920]   What they're building there is basically the way you talk to any device.
[00:10:04.320 --> 00:10:07.200]   So if they can win with basically the voice UI,
[00:10:07.200 --> 00:10:08.560]   because that's what they're doing.
[00:10:08.560 --> 00:10:10.160]   So if I talk to my thermostat,
[00:10:10.160 --> 00:10:14.560]   these are the words I used to do it because I have a thermostat with,
[00:10:14.560 --> 00:10:17.520]   you know, Amazon Echo integration in it,
[00:10:17.520 --> 00:10:21.040]   that's what they're doing now.
[00:10:21.040 --> 00:10:22.080]   So I can totally be like,
[00:10:22.080 --> 00:10:23.680]   oh yeah, there used to be a standalone device.
[00:10:23.680 --> 00:10:25.040]   And all I did was listen.
[00:10:25.040 --> 00:10:29.440]   But in the future, everything should have some sort of voice capability,
[00:10:29.440 --> 00:10:31.760]   which is actually freaking scary if we worry about hacking,
[00:10:31.760 --> 00:10:35.120]   because you can't turn the mic off on all these other things as easily.
[00:10:35.120 --> 00:10:37.440]   Do you think people are, I mean,
[00:10:37.440 --> 00:10:39.440]   certainly privacy has become a big concern.
[00:10:39.440 --> 00:10:45.200]   Of course, President Trump on Monday signed the bill overturning the ISP privacy regulations
[00:10:45.200 --> 00:10:48.160]   that were promulgated last year by the FCC.
[00:10:48.160 --> 00:10:50.320]   We've seen the CIA, WikiLeaks dump,
[00:10:50.320 --> 00:10:52.720]   and now a new hack that shows that almost all,
[00:10:52.720 --> 00:10:58.560]   something like 95% of all smart TVs can be hacked remotely to turn on cameras and microphone.
[00:10:59.760 --> 00:11:03.280]   Do you think that the general public's awareness of this
[00:11:03.280 --> 00:11:08.320]   is high enough and their concern about privacy is high enough to actually stymie
[00:11:08.320 --> 00:11:11.280]   the development of these voice services?
[00:11:11.280 --> 00:11:12.080]   Oh, for sure.
[00:11:12.080 --> 00:11:13.680]   I have tons of people who are like,
[00:11:13.680 --> 00:11:16.000]   I'm interested in the Amazon Echo,
[00:11:16.000 --> 00:11:18.560]   but oh, I don't want anything else listening to me.
[00:11:18.560 --> 00:11:19.040]   Interesting.
[00:11:19.040 --> 00:11:20.640]   I mean, I get lots of that.
[00:11:20.640 --> 00:11:20.880]   Yeah.
[00:11:20.880 --> 00:11:24.960]   There was actually a new poll that came out yesterday on this,
[00:11:24.960 --> 00:11:29.360]   and shockingly, 75% of adults said they would not let
[00:11:29.360 --> 00:11:32.880]   investigators, FBI or whatever, tap into their internet activity
[00:11:32.880 --> 00:11:34.880]   to help combat terrorism.
[00:11:34.880 --> 00:11:35.920]   This is way up.
[00:11:35.920 --> 00:11:39.520]   This is up from 67% in June, 2013.
[00:11:39.520 --> 00:11:42.720]   So I think that all the news around hacking and surveillance
[00:11:42.720 --> 00:11:46.400]   is starting to have an effect on the public, which kind of surprised me.
[00:11:46.400 --> 00:11:48.160]   Usually the public is willing to, you know,
[00:11:48.160 --> 00:11:51.520]   they'll give up all their privacy if they can save $5 on something.
[00:11:51.520 --> 00:11:56.880]   And, you know, they tend to not be aware of the larger implications of what happens
[00:11:56.880 --> 00:11:58.320]   when you give up all your privacy.
[00:11:58.320 --> 00:12:01.520]   But I think all the stories coming out are chipping away at the resistance
[00:12:01.520 --> 00:12:03.680]   and the lack of awareness in the public.
[00:12:03.680 --> 00:12:06.320]   And I think we're getting to a point where people are going to really start
[00:12:06.320 --> 00:12:11.920]   seeking out encryption in a meaningful way and looking for privacy.
[00:12:11.920 --> 00:12:15.040]   And I saw a really interesting quote that I retweeted this morning,
[00:12:15.040 --> 00:12:21.680]   which I think was a fantastic perspective on privacy and on encryption.
[00:12:21.680 --> 00:12:24.720]   It's according to this quote, I'm paraphrasing it,
[00:12:24.720 --> 00:12:31.120]   is one of the only technologies that radically favors the privacy of the person.
[00:12:31.120 --> 00:12:33.520]   And it doesn't go in both ways.
[00:12:33.520 --> 00:12:38.960]   Most technologies, hacking or anti-hacking, it can benefit hackers.
[00:12:38.960 --> 00:12:41.280]   It can benefit the surveillance crowd.
[00:12:41.280 --> 00:12:42.800]   It can benefit both sides.
[00:12:42.800 --> 00:12:47.760]   Encryption benefits only the side of the person who's trying to keep their
[00:12:47.760 --> 00:12:50.400]   information private and offers nothing to the other side.
[00:12:50.400 --> 00:12:54.400]   And that's why it's fundamentally in the future going to be tied
[00:12:54.400 --> 00:12:56.160]   to the question of human rights.
[00:12:56.160 --> 00:12:58.640]   Like, do you have a right to have a private conversation?
[00:12:58.640 --> 00:13:01.440]   If so, then you have the right to encryption and then encryption.
[00:13:01.440 --> 00:13:04.160]   So I think that's a really interesting perspective.
[00:13:04.160 --> 00:13:06.960]   But I do think it's becoming a political issue.
[00:13:06.960 --> 00:13:08.320]   The trust of the government is low.
[00:13:08.320 --> 00:13:09.600]   The trust of companies are low.
[00:13:09.600 --> 00:13:12.160]   And so people are going to take that on in the privacy sphere.
[00:13:12.160 --> 00:13:15.280]   Well, man, if all that's true, does that stymie this technology though?
[00:13:15.280 --> 00:13:19.360]   I mean, should Amazon say, well, I guess we won't do the echo now.
[00:13:19.360 --> 00:13:24.080]   Should Google say, well, we probably want to add voice to our wireless hubs.
[00:13:24.080 --> 00:13:28.000]   So with Amazon, I think there's two things.
[00:13:28.000 --> 00:13:29.360]   There's government efforts.
[00:13:29.360 --> 00:13:32.640]   And then there's, I don't actually trust the government with my data.
[00:13:32.640 --> 00:13:38.240]   One, general incompetence to surveillance efforts, that kind of thing.
[00:13:38.240 --> 00:13:41.840]   I actually trust Amazon with my data.
[00:13:41.840 --> 00:13:42.960]   Isn't that ironic?
[00:13:42.960 --> 00:13:44.720]   It's super ironic.
[00:13:44.720 --> 00:13:46.080]   Well, what's the, but I've always said that.
[00:13:46.080 --> 00:13:47.600]   What's the worst Amazon can do?
[00:13:47.600 --> 00:13:48.880]   Try to sell you something.
[00:13:48.880 --> 00:13:50.480]   What's the worst the government can do?
[00:13:50.480 --> 00:13:51.520]   The sky's the one.
[00:13:51.520 --> 00:13:51.760]   Yeah.
[00:13:52.880 --> 00:13:54.320]   Like, oh, they could put me in jail.
[00:13:54.320 --> 00:13:54.880]   They could kill you.
[00:13:54.880 --> 00:13:56.480]   Or kill me.
[00:13:56.480 --> 00:13:57.840]   Jail kill you.
[00:13:57.840 --> 00:14:04.240]   The thing is that Amazon itself is hopefully a closed system, whereas government surveillance,
[00:14:04.240 --> 00:14:08.400]   what they're looking for, back doors and things like that, which can be exploited by others.
[00:14:08.400 --> 00:14:09.440]   Like having ended.
[00:14:09.440 --> 00:14:10.800]   But that's the nexus, right?
[00:14:10.800 --> 00:14:15.120]   Because the government goes to Amazon and says, you got that echo in there.
[00:14:15.120 --> 00:14:16.880]   Let's turn that on if you don't mind.
[00:14:16.880 --> 00:14:19.200]   And Amazon's helpless to say no.
[00:14:19.200 --> 00:14:22.560]   So it is really the nexus of this stuff.
[00:14:22.560 --> 00:14:24.000]   You may not trust government.
[00:14:24.000 --> 00:14:25.280]   You may trust Amazon.
[00:14:25.280 --> 00:14:26.960]   But if you've got an echo in your house, Daisy,
[00:14:26.960 --> 00:14:31.040]   you're giving the government that microphone in effect.
[00:14:31.040 --> 00:14:31.200]   Yeah.
[00:14:31.200 --> 00:14:32.640]   In effect.
[00:14:32.640 --> 00:14:35.600]   Now, I assume that Amazon pays attention.
[00:14:35.600 --> 00:14:39.360]   Like Amazon doesn't roll over like AT&T did when the government was like,
[00:14:39.360 --> 00:14:44.240]   hey, yoke, can we tap into your major network and spy on all your users?
[00:14:44.240 --> 00:14:48.560]   So far, Amazon is not doing that to my knowledge.
[00:14:49.360 --> 00:14:52.400]   Yeah. And I've talked to people in Amazon about this.
[00:14:52.400 --> 00:14:55.920]   They are not willing to do that.
[00:14:55.920 --> 00:14:57.600]   Doesn't mean they're going to win that battle, by the way.
[00:14:57.600 --> 00:15:01.680]   It doesn't. But I know they're going to fight it in Amazon.
[00:15:01.680 --> 00:15:02.080]   Yeah, they're going to fight it.
[00:15:02.080 --> 00:15:04.160]   But they're probably lose.
[00:15:04.160 --> 00:15:05.360]   So what?
[00:15:05.360 --> 00:15:11.360]   Fighting is more a PR thing than it is a real, I mean, I understand they don't want to do it.
[00:15:11.360 --> 00:15:17.360]   But there was a recent episode of the show, The Good Fight, which is a spin-off of The Good Wife.
[00:15:18.000 --> 00:15:19.520]   And it's a very good show.
[00:15:19.520 --> 00:15:23.120]   It's exactly like The Good Wife minus the two previous main characters.
[00:15:23.120 --> 00:15:30.000]   And they did an episode recently where there was an Amazon Echo like device in the office
[00:15:30.000 --> 00:15:31.280]   of one of the lawyers.
[00:15:31.280 --> 00:15:33.520]   And it was subpoenaed or whatever.
[00:15:33.520 --> 00:15:36.240]   They grabbed the device, whatever they call it.
[00:15:36.240 --> 00:15:40.000]   And they got all this data and all these conversations and it recorded everything.
[00:15:40.000 --> 00:15:41.360]   And that's just simply not accurate.
[00:15:41.360 --> 00:15:44.240]   As Stacy is saying, I mean, if the device doesn't actually
[00:15:44.960 --> 00:15:51.280]   record when you don't trigger the secret keyword, then there's not a recording for the
[00:15:51.280 --> 00:15:54.400]   government or anybody else to get their hands on.
[00:15:54.400 --> 00:15:55.440]   After the vote.
[00:15:55.440 --> 00:15:57.520]   After the government could.
[00:15:57.520 --> 00:16:00.320]   I don't see any reason why the government couldn't say to Amazon.
[00:16:00.320 --> 00:16:02.720]   Maybe Amazon would fight it, probably try to make it public.
[00:16:02.720 --> 00:16:07.520]   But as we know, thanks to the Patriot Act, the government has ways to not to keep this private.
[00:16:07.520 --> 00:16:09.520]   So it may even already be happening.
[00:16:09.520 --> 00:16:12.080]   Well, like everything else, don't you think they could go to the Amazon and say,
[00:16:12.080 --> 00:16:15.280]   turn on Stacy's echo and just let's hear everything?
[00:16:15.280 --> 00:16:19.760]   I mean, I think that will happen if that's possible.
[00:16:19.760 --> 00:16:21.200]   I mean, it will happen if it's possible.
[00:16:21.200 --> 00:16:22.800]   Let me just say it more forcefully.
[00:16:22.800 --> 00:16:23.200]   Right.
[00:16:23.200 --> 00:16:26.000]   And then once it does happen, it'll be a court case and it'll go to
[00:16:26.000 --> 00:16:26.480]   for it.
[00:16:26.480 --> 00:16:28.880]   Microphone and camera, you happen to carry in your pocket all the time.
[00:16:28.880 --> 00:16:30.320]   So that's what I was going to say.
[00:16:30.320 --> 00:16:34.160]   I'm like, this, there is nothing to stop that from happening with my phone,
[00:16:34.160 --> 00:16:35.520]   which is why people who are.
[00:16:35.520 --> 00:16:36.400]   The phone service.
[00:16:36.400 --> 00:16:39.680]   The phone companies are absolutely in the pocket of the back pocket.
[00:16:39.680 --> 00:16:39.920]   So.
[00:16:39.920 --> 00:16:41.360]   Fight it.
[00:16:41.360 --> 00:16:44.160]   So I mean, here, take it.
[00:16:44.160 --> 00:16:44.800]   What do you want?
[00:16:44.800 --> 00:16:49.920]   I mean, honestly, if I were reporting on something really dicey, like,
[00:16:49.920 --> 00:16:53.520]   if I were Lawrence, right, the guy who writes for the New Yorker and does all the Al kind of
[00:16:53.520 --> 00:16:54.720]   reporting, you know what?
[00:16:54.720 --> 00:16:56.320]   I would not have an echo.
[00:16:56.320 --> 00:16:57.600]   No, but that's.
[00:16:57.600 --> 00:17:01.040]   So that's getting back to this original question, which we still haven't answered,
[00:17:01.040 --> 00:17:05.200]   which is, is this going to stop this technology and its tracks?
[00:17:05.200 --> 00:17:06.160]   Do people care enough?
[00:17:06.160 --> 00:17:06.960]   Are they worried enough?
[00:17:06.960 --> 00:17:10.320]   Are they aware enough that they're just going to say, no, we're not going to buy this.
[00:17:10.320 --> 00:17:11.200]   It hasn't yet.
[00:17:11.200 --> 00:17:12.480]   Yeah.
[00:17:12.480 --> 00:17:14.160]   The answer is no, it's not going to stop it.
[00:17:14.160 --> 00:17:15.920]   Because the convenience that weighs it, right?
[00:17:15.920 --> 00:17:16.720]   Yeah.
[00:17:16.720 --> 00:17:20.800]   Nothing is going to stop the inevitability of a world in which we're constantly interacting
[00:17:20.800 --> 00:17:23.040]   with a virtual assistant by voice.
[00:17:23.040 --> 00:17:23.840]   That is happening.
[00:17:23.840 --> 00:17:30.960]   And at the same time, I think it's safe to say that there is no measure, the government
[00:17:30.960 --> 00:17:35.600]   or security professionals or companies could take that will keep the stuff private.
[00:17:35.600 --> 00:17:40.240]   So you will be carrying a microphone in your pocket, wearing it on your head.
[00:17:40.240 --> 00:17:42.800]   It'll be in your living room and maybe all three.
[00:17:42.800 --> 00:17:48.640]   And if a government is determined enough or a bad guy is determined enough, they'll get it.
[00:17:48.640 --> 00:17:55.040]   I think that's one of the lessons of the second to the last WikiLeaks thing about the CIA.
[00:17:55.040 --> 00:17:57.920]   We learned, you know, with that they can hack TVs and all that kind of stuff.
[00:17:57.920 --> 00:18:04.080]   Really, the way that a surveillance minded or espionage minded organization thinks about
[00:18:04.080 --> 00:18:07.680]   these things is that let me see what microphones are already in place.
[00:18:07.680 --> 00:18:09.760]   There's the phones, there's the TV.
[00:18:09.760 --> 00:18:12.080]   We have to put one in if they've already got several.
[00:18:12.080 --> 00:18:12.800]   Exactly.
[00:18:12.800 --> 00:18:13.120]   Right.
[00:18:13.120 --> 00:18:14.960]   It's too crowded, too many microphones.
[00:18:14.960 --> 00:18:17.760]   And so just hack one, you know, and you have surveillance.
[00:18:17.760 --> 00:18:18.720]   There will be people.
[00:18:18.720 --> 00:18:19.040]   Yeah.
[00:18:19.040 --> 00:18:23.840]   Like there's going to be people like my mom, she does not, she drives like a car that doesn't
[00:18:23.840 --> 00:18:28.320]   have automatic locks because she doesn't want any electronics in her car.
[00:18:28.320 --> 00:18:30.560]   Yeah.
[00:18:30.560 --> 00:18:32.560]   She's a real interesting character sometimes.
[00:18:35.200 --> 00:18:38.800]   But she didn't want to turn it in her Note 7 because she really liked that phone.
[00:18:38.800 --> 00:18:39.120]   Right.
[00:18:39.120 --> 00:18:39.440]   Right.
[00:18:39.440 --> 00:18:43.200]   So yeah, I'm like, people, people they are made of contradictions.
[00:18:43.200 --> 00:18:43.840]   Go figure.
[00:18:43.840 --> 00:18:46.560]   You know, but you make a great point.
[00:18:46.560 --> 00:18:51.280]   The world, the pre digital world was understandable.
[00:18:51.280 --> 00:18:54.480]   And a perfect example is bugging phones.
[00:18:54.480 --> 00:18:58.480]   When you used to have to bug a phone, you had these big phones with the dial or the rotary
[00:18:58.480 --> 00:19:00.880]   dial or the button dial.
[00:19:00.880 --> 00:19:04.880]   And when that phone was hung up, there was a physical mechanism that
[00:19:04.880 --> 00:19:08.080]   prevented you from using the microphone built into the phone.
[00:19:08.080 --> 00:19:14.400]   So what they did was they'd have to break into the house or whatever and install a separate
[00:19:14.400 --> 00:19:18.400]   microphone in the hardware of the phone or inside the receiver of the phone.
[00:19:18.400 --> 00:19:21.440]   And that was doable because there was a lot of air and space in there.
[00:19:21.440 --> 00:19:25.600]   But unless they, if they couldn't break in, they couldn't use the microphone present in the phone.
[00:19:25.600 --> 00:19:30.400]   Because of the physical mechanism, you could take a phone apart and see exactly how it was
[00:19:30.400 --> 00:19:31.520]   physically disconnected.
[00:19:32.720 --> 00:19:37.520]   And this gets to not anything terribly newsy, but something that I spend a lot of time thinking
[00:19:37.520 --> 00:19:42.080]   about because right now I'm watching the Americans because it's back on.
[00:19:42.080 --> 00:19:43.600]   And it's an awesome show.
[00:19:43.600 --> 00:19:44.160]   Right show.
[00:19:44.160 --> 00:19:48.560]   And we are watching this and all of the shenanigans they get into and we're like,
[00:19:48.560 --> 00:19:52.400]   God, it must be super hard to be a spy right now with cameras everywhere.
[00:19:52.400 --> 00:20:00.080]   And the ability, like, I'm like, we were talking about AT&T got a contract for an Air Force base
[00:20:00.080 --> 00:20:02.800]   to do connected perimeter monitoring.
[00:20:02.800 --> 00:20:08.240]   And like, oh, you know, think of all the times that you've seen your characters and a TV show
[00:20:08.240 --> 00:20:12.320]   break into some highly secure facility by cutting through the chain link fence.
[00:20:12.320 --> 00:20:15.440]   And I'm like, yeah, not going to happen anymore.
[00:20:15.440 --> 00:20:15.680]   Yeah.
[00:20:15.680 --> 00:20:19.040]   Tradecraft, spycraft has to have changed a lot.
[00:20:19.040 --> 00:20:19.280]   But.
[00:20:19.280 --> 00:20:21.440]   Yeah, they won't tell you about it though.
[00:20:21.440 --> 00:20:28.880]   The bad guys know that if you want to have a private conversation, you go out in a field
[00:20:28.880 --> 00:20:33.760]   and you talk to somebody personally, if you want to have a private conversation over the internet,
[00:20:33.760 --> 00:20:37.280]   you use PGP or something similar and encrypt.
[00:20:37.280 --> 00:20:43.680]   The people who really want to have privacy know how to do it and it's not hard.
[00:20:43.680 --> 00:20:49.200]   Even if we talked about this yesterday on security now, Steve and I are both of the opinion.
[00:20:49.200 --> 00:20:57.040]   The UK government is on the verge now of banning strong encryption or requiring a government
[00:20:57.040 --> 00:20:59.920]   backdoor. Russia has already done that.
[00:20:59.920 --> 00:21:05.200]   And I think Steve and I give our opinion, given the current climate in Washington, DC,
[00:21:05.200 --> 00:21:11.520]   that's inevitable in the United States as well, that there will be a law, Congress will pass a law
[00:21:11.520 --> 00:21:18.160]   that will require either a backdoor or make strong encryption illegal or both.
[00:21:18.160 --> 00:21:24.640]   And it will make a difference in the cell phone manufacturers, Apple and Samsung and Google will
[00:21:24.640 --> 00:21:28.400]   have to weaken the security of the cell phone.
[00:21:28.400 --> 00:21:37.040]   It won't stop people from using encryption apps. You may be illegal to use, imagine a world
[00:21:37.040 --> 00:21:40.720]   where it's illegal to use WhatsApp or WhatsApp apps.
[00:21:40.720 --> 00:21:44.160]   Can you carry Doctorow imagine this for like one of his little brothers?
[00:21:44.160 --> 00:21:50.800]   Yeah, his little brother, exactly. But nevertheless, because the math is out there,
[00:21:50.800 --> 00:21:56.160]   the algorithms are simple. You can write a public key crypto algorithm on a t-shirt, literally.
[00:21:56.160 --> 00:22:03.680]   So that means that there will be bespoke encryption apps. In other words, bad guys will not be
[00:22:03.680 --> 00:22:09.280]   deterred because they have an incentive to go the extra mile to encrypt and provide themselves
[00:22:09.280 --> 00:22:16.480]   in privacy. It will only be us, the good guys, the normal people who will be surveilled and surveilled
[00:22:16.480 --> 00:22:21.840]   in 100 different ways by 100 different entities. And I don't see any way to really avoid that.
[00:22:21.840 --> 00:22:28.560]   What worries me is that the hackers and malicious actors will also have this,
[00:22:28.560 --> 00:22:33.360]   that we won't be able to protect ourselves any longer against malicious actors. If my phone no
[00:22:33.360 --> 00:22:37.920]   longer is encrypted, then everything that's on my phone is available, not just to government,
[00:22:37.920 --> 00:22:40.480]   but to anybody else who wants to take the trouble to get it.
[00:22:40.480 --> 00:22:46.560]   One of the aspects about privacy violation that I think is radically underappreciated by the public
[00:22:46.560 --> 00:22:52.240]   is whether something is able to be done at a mass scale or has to be targeted. So,
[00:22:52.240 --> 00:22:57.360]   perfectly example is the recent CIA revelations versus the NSA revelations.
[00:22:57.360 --> 00:22:59.040]   The calls that was all targeted.
[00:22:59.040 --> 00:23:05.120]   Exactly. The NSA stuff was all about that. Exactly. And that's a huge difference.
[00:23:05.120 --> 00:23:09.200]   And I wrote a column recently about the nature of biometrics. People think
[00:23:09.200 --> 00:23:13.920]   well biometrics, biometrics, fingerprint, face recognition, iris, vein recognition,
[00:23:13.920 --> 00:23:18.720]   there are all these different kinds of biometrics. But the public needs to understand that the one's
[00:23:18.720 --> 00:23:24.000]   type of biometric is really dangerous. And that is face recognition. And everything is going
[00:23:24.000 --> 00:23:27.840]   toward face recognition now. And here's why it's dangerous. It can be done in mass.
[00:23:27.840 --> 00:23:35.120]   You can capture somebody's biometric data, their face from 300 yards away with a camera.
[00:23:36.080 --> 00:23:41.200]   Their face is on the internet and it can be entered into and is being entered into
[00:23:41.200 --> 00:23:46.080]   various algorithms for identifying face recognition. There's a website. So there was
[00:23:46.080 --> 00:23:52.240]   an interesting hoax that went around on Facebook the other day of last month called FaceZam.
[00:23:52.240 --> 00:23:57.760]   And it was a it was a marketing was a bad marketing hoax publicity stunt type of thing.
[00:23:57.760 --> 00:24:01.520]   But basically it was a story that circulated that said that anyone could track you down by
[00:24:01.520 --> 00:24:05.280]   just scanning your Facebook photos. They could take your Facebook photos and find find you
[00:24:05.280 --> 00:24:10.160]   elsewhere on the internet. Well, what's funny about that is that actually exists. That's totally
[00:24:10.160 --> 00:24:14.560]   doable right now. So there's a there's a website you can go to called FindFace. It's a Russian
[00:24:14.560 --> 00:24:18.720]   face recognition site. You can upload somebody's face and they'll show you their Twitter account
[00:24:18.720 --> 00:24:23.680]   half the time. Half the time it doesn't work half the time it does. But the point is that
[00:24:23.680 --> 00:24:32.640]   face recognition is something that is like NSA harvesting of metadata on phone and email records.
[00:24:32.640 --> 00:24:39.040]   It can be done by the millions. And so face recognition really, really should bother people.
[00:24:39.040 --> 00:24:45.680]   It's the least secure the most privacy invading form of biometrics by far.
[00:24:45.680 --> 00:24:51.680]   We should reiterate that FaceZam, which is by the way, I take off on Shazam the music recognition
[00:24:51.680 --> 00:24:57.280]   tool is a hoax is not a hoax. It is a hoax. The telegraph was fooled by this. But FindFace
[00:24:57.280 --> 00:25:02.480]   is real. Well, and we yeah, I mean, we know this is actually a fairly trivial thing to do, right?
[00:25:03.040 --> 00:25:08.800]   Well, you do crazy. Oh, God, I was going to tell people about crazy contouring to make their face
[00:25:08.800 --> 00:25:15.280]   not recognizable. You're the queen of contour. I love it. You're again. This is another realization
[00:25:15.280 --> 00:25:19.280]   we're all going to get to at some point. The only way to have privacy is through disinformation
[00:25:19.280 --> 00:25:25.280]   and not protection. Yeah. Yeah. That's why I always wear heavy makeup when I'm when I'm
[00:25:25.280 --> 00:25:32.160]   having my picture taken. But I give a my readers a step by step way to in in about 60 seconds
[00:25:32.160 --> 00:25:36.080]   probably. You can go from having somebody's photo to having their home address.
[00:25:36.080 --> 00:25:43.280]   Super easy. Go to FindFace, upload the picture. They'll show you the Twitter account. You'll get
[00:25:43.280 --> 00:25:47.840]   the full name. You take the name, you go to family tree now, run it through the database. You have
[00:25:47.840 --> 00:25:52.720]   their you have their their home address. You have the all their relatives. You have their age. You
[00:25:52.720 --> 00:25:58.880]   have the gender you have everything takes no time at all. This is right now. So these face
[00:25:58.880 --> 00:26:03.280]   recognition algorithms are getting like way better every single year. They're actually better than
[00:26:03.280 --> 00:26:11.120]   the governments. That's the irony of it. Houseovers like come in last week released a report on the
[00:26:11.120 --> 00:26:17.440]   FBI's face recognition program that said approximately half of adult Americans photographs are stored
[00:26:17.440 --> 00:26:23.120]   in facial recognition databases that are accessed by the FBI without their knowledge or consent in
[00:26:23.120 --> 00:26:28.240]   the hunt for suspected criminals about 80% of the photos in the network are non criminal entries
[00:26:28.240 --> 00:26:32.960]   including pictures from drivers license and passports. The algorithms used to identify matches are
[00:26:32.960 --> 00:26:38.160]   inaccurate about 15% of the time. By the way, that's not bad 15% of the time and are more likely
[00:26:38.160 --> 00:26:44.960]   to misidentify black people than white people. So that's because we Silicon Valley trains are
[00:26:44.960 --> 00:26:52.240]   a recognition software. They all look alike, right? No, no federal law controls this technology.
[00:26:52.240 --> 00:27:00.560]   No court decision limits it. It's completely out of control. Well, Facebook is really amazing.
[00:27:00.560 --> 00:27:06.560]   Facebook's works. One of the reasons it works is it uses artificial intelligence.
[00:27:06.560 --> 00:27:11.520]   And what it does is basically you can show a person. So people upload lots and lots of pictures
[00:27:11.520 --> 00:27:19.760]   and they can process those. But you can upload. So let's say Stacy uploads four pictures of
[00:27:19.760 --> 00:27:27.280]   herself at a party and her face is present in all of those. You can take a fifth picture from that
[00:27:27.280 --> 00:27:32.800]   party where her face is not even shown and you'll know she was in Facebook will identify. Because it
[00:27:32.800 --> 00:27:37.760]   also knows the sweater and it's all. It's a canny, isn't it? It's gotten very good. I've noticed that
[00:27:37.760 --> 00:27:44.720]   several times in Facebook. The irony is as to your original points, Stacy, Facebook knowing who you
[00:27:44.720 --> 00:27:49.280]   are, who you say you are is merely annoying. The government arresting you saying you
[00:27:49.280 --> 00:27:55.440]   match the picture of the crime, the criminal, and putting you in jail is a lot more than annoying.
[00:27:55.440 --> 00:28:00.320]   So 15% failure rates are significant. I was going to say, you kind of made that wasn't
[00:28:00.320 --> 00:28:03.680]   you said that wasn't so bad. I'm like, actually, I feel like that's pretty terrible. Yeah.
[00:28:03.680 --> 00:28:09.040]   One in seven, we screw up. No big deal. Yeah, it happens. What are you going to do?
[00:28:10.160 --> 00:28:14.320]   In one show, end up in jail, they take your fingerprints as part of the whole process.
[00:28:14.320 --> 00:28:21.440]   Well, this is all pretty grim. I mean, is there any good news? If I were Amazon, I would say,
[00:28:21.440 --> 00:28:25.680]   well, we better not do the echo. That was a bad idea. If I were Google, I'd say, let's not put
[00:28:25.680 --> 00:28:33.680]   voice on hub and Google Wi-Fi. I mean, are you? Or no, no, they're going to go full steam ahead.
[00:28:33.680 --> 00:28:38.160]   We don't have any control of this at all. Well, voice and face recognition are different things.
[00:28:38.160 --> 00:28:41.920]   No, but right. No, I understand. Voice is probably even more accurate.
[00:28:41.920 --> 00:28:48.720]   But they're not able to authenticate on voice yet. So we've got time.
[00:28:48.720 --> 00:28:52.480]   Actually, Google does authenticate on voice. I don't know how accurate it is with the Google
[00:28:52.480 --> 00:28:57.920]   Assistant. You can unlock your phone. So when you install Google Assistant on an Android phone,
[00:28:57.920 --> 00:29:02.560]   yeah, it'll say, OK, I know your voice now. By the way, you only have to say, OK, you know,
[00:29:02.560 --> 00:29:06.000]   who three times and says, OK, I know your voice now. Do you want me to unlock your phone when you
[00:29:06.000 --> 00:29:10.640]   ask me for something? Oh, yeah, it does offer me that. I don't know how accurate it is. But
[00:29:10.640 --> 00:29:15.520]   the point is they're doing that. This is going to be one of the benefits of eyeglasses based
[00:29:15.520 --> 00:29:23.280]   interaction with a virtual assistant because it could be designed so that it will only receive
[00:29:23.280 --> 00:29:29.600]   voice input from the foot away. And so you don't have to authenticate your personal voice and be
[00:29:29.600 --> 00:29:34.640]   identified in that way. Just the fact that it's on your face means that you're the only one who
[00:29:34.640 --> 00:29:38.240]   can talk to it. And then it goes through bone conduction. So you're the only one who can hear it.
[00:29:38.240 --> 00:29:43.200]   So there's a level of privacy that's improved theoretically in one aspect.
[00:29:43.200 --> 00:29:47.520]   Back to the face recognition thing, there's good news and bad news. You asked for good news,
[00:29:47.520 --> 00:29:51.760]   but first I'm going to give you a little bit more bad news, which is that with you really
[00:29:51.760 --> 00:29:57.440]   don't have control with with with the social networks because other people can identify you.
[00:29:57.440 --> 00:30:01.840]   So for example, I have pictures of you Leo and I can go and say, oh, that's Leo Laport.
[00:30:01.840 --> 00:30:05.920]   And now Facebook knows who I am. Yeah. And you had nothing to do with. There's no
[00:30:05.920 --> 00:30:10.720]   offing out other than like wearing a clown mask all the time or something like that. Now,
[00:30:10.720 --> 00:30:16.960]   the good news is this is good contouring. Right. Stacey's going to say my good news is you can get
[00:30:16.960 --> 00:30:22.880]   good contouring videos and cheap, cheap powders next week on the twig. Stacy's going to show you how
[00:30:22.880 --> 00:30:26.960]   to do the contouring. And I'm going to teach you how to talk like this.
[00:30:28.160 --> 00:30:33.440]   Right. There's actually there's actually an artist who specializes in that sort of thing.
[00:30:33.440 --> 00:30:38.160]   I actually mentioned them in the column that I mentioned a minute ago. And let me see if I
[00:30:38.160 --> 00:30:41.840]   can find the wrong person. We're on that. It's sounding like baskets the clown before we're done.
[00:30:41.840 --> 00:30:46.480]   Yeah. And look at. So there's a Louis Anderson. It's a design and a proud Harvey.
[00:30:46.480 --> 00:30:53.680]   So so in so a designer named Adam Harvey has hairstyles and makeup that fools fake recognition.
[00:30:53.680 --> 00:30:58.000]   So there's like it's a hair and makeup. Yeah. Yeah. So it's really cool.
[00:30:58.000 --> 00:31:05.120]   All right. So Adam Harvey. Adam Harvey also has fabrics that full face recognition computers
[00:31:05.120 --> 00:31:11.120]   and does it in a crazy way. Like there's a coat that he has that has it looks just like kind of
[00:31:11.120 --> 00:31:15.600]   like camouflage or something has this random pattern, but face recognition algorithm to perceive it
[00:31:15.600 --> 00:31:20.400]   as having faces faces all over it. But the problem is you have to walk around looking like this person.
[00:31:21.920 --> 00:31:24.720]   Like this could be this could be stylish at some point.
[00:31:24.720 --> 00:31:30.800]   Okay. You saw it here first. How long before this is the fad everybody starts dressing
[00:31:30.800 --> 00:31:32.480]   and wearing their hair and weird.
[00:31:32.480 --> 00:31:37.440]   Okay. So I would totally do some of this stuff like this pixelated makeup. The dude is wearing.
[00:31:37.440 --> 00:31:42.320]   I think that's pretty cool. Or this lady this tiger lady down. Yeah. And would you do the hair
[00:31:42.320 --> 00:31:47.280]   like that? I already have blue hair. I just need to make it straight and comb half of it over my face.
[00:31:48.880 --> 00:31:55.840]   Test patterns for stylists. These block these are this is not just like crazy style. These block
[00:31:55.840 --> 00:32:01.440]   face detection algorithms open CV, eB, learn, Vara look and Apple face detection.
[00:32:01.440 --> 00:32:10.320]   Yeah. So the idea is what this is crazy. They learn where like what eyes look like on a face,
[00:32:10.320 --> 00:32:15.920]   right? They learn something that what they knows in the spot two eyes in this spot and lips that
[00:32:15.920 --> 00:32:24.000]   this is a person and then they track they match against that. That's why taking big blocks of color
[00:32:24.000 --> 00:32:29.040]   and putting it in weird spots fools it because it's like, wait, is that the eye? No, that's the eye.
[00:32:29.040 --> 00:32:34.320]   What's it doing on our cheek? Wow. The other thing you could do.
[00:32:34.320 --> 00:32:40.640]   And we actually were going to Colleen was going to build this. She now ironically works for Facebook.
[00:32:41.760 --> 00:32:51.440]   We were going to put a build either a collar or maybe a little tiara that has UV lights in it.
[00:32:51.440 --> 00:32:55.520]   You don't see them human eye doesn't see them. It blinds cameras. You look like a glowing ball
[00:32:55.520 --> 00:33:01.680]   to a camera. You could I that's another fashion. I could see people going around wearing headbands
[00:33:01.680 --> 00:33:06.640]   with you know lights that the block cameras. That's actually a probably really good product.
[00:33:06.640 --> 00:33:11.760]   Let's do the Kickstarter now. So here's a related thing and I'm trying to find the video.
[00:33:11.760 --> 00:33:18.400]   I think I saw it at South by Southwest. It was an IOT device that somebody built that had
[00:33:18.400 --> 00:33:25.520]   a detected IR from surveillance cameras and every time it passed and a surveillance camera,
[00:33:25.520 --> 00:33:30.080]   it detected IR and it would tap the person who was wearing this shirt. It would give them a half
[00:33:30.080 --> 00:33:36.880]   day zip on their shoulder. So like it was like Twitch, Twitch every like he was walking down
[00:33:36.880 --> 00:33:41.920]   the streets of London. It's pretty cool. Wow. If you are going to do that, chat room saying it
[00:33:41.920 --> 00:33:47.600]   probably better to use infrared than ultraviolet. I don't know which would blind a camera but
[00:33:47.600 --> 00:33:55.040]   pick one end of the spectrum. Clearly there's going to be an arms race. This is just bizarre.
[00:33:57.600 --> 00:34:03.040]   I mean we're headed rapidly toward 1984. I think it's interesting by the way a number of movie
[00:34:03.040 --> 00:34:10.480]   theaters in the country are showing 1984 once again in a kind of quiet political statement.
[00:34:10.480 --> 00:34:15.520]   But we are really approaching 1984 with it. Remember the the telescreens in 1984 not you
[00:34:15.520 --> 00:34:19.360]   not only watched them, they watched you. It was a two-way device in your house. Hey, we've got them
[00:34:19.360 --> 00:34:24.880]   now. It's called a Samsung TV. We installed them voluntarily and you know it's ironic that you know
[00:34:24.880 --> 00:34:29.520]   1984 is one of the great novels. A lot of people are assigned it in high school and
[00:34:29.520 --> 00:34:35.200]   therefore don't remember a word of it. And so a lot of people are rediscovering it and reading it again.
[00:34:35.200 --> 00:34:42.960]   But it's ironic that this novel was brought to us by England, by the UK essentially and the UK
[00:34:42.960 --> 00:34:48.800]   essentially in China maybe leading the charge into that very world. There are more surveillance
[00:34:48.800 --> 00:34:54.400]   cameras per square foot or per person or something in the UK than anywhere else in the world.
[00:34:54.400 --> 00:34:59.280]   And now they're trying to ban encryption. So that's kind of like not learning from history and not
[00:34:59.280 --> 00:35:04.240]   learning from literature. Well, it goes both ways. I mean, I think that maybe the reason 1984 and
[00:35:04.240 --> 00:35:08.320]   Brave New World were written by British writers is because they were well aware of the
[00:35:08.320 --> 00:35:16.640]   tendency in the British state to do that. Anyways, the most surveilled big city in the world.
[00:35:16.640 --> 00:35:21.680]   London. London. So I promise I promise some good news in all this and that is that as of today I
[00:35:21.680 --> 00:35:27.680]   believe the Google Photos app has a new feature that uses this in a more convenient way. So when you
[00:35:27.680 --> 00:35:33.680]   open a picture in Google Photos, any photo there's there's always a little information, a little
[00:35:33.680 --> 00:35:39.680]   eye, lowercase eye with a circle. But that is existed. But now when you poke that, it'll show you
[00:35:39.680 --> 00:35:47.360]   a screen that shows all the people and what their names are and you tapping that photo
[00:35:47.360 --> 00:35:53.360]   shows you all the pictures of that person. So it's a more convenient way to see everything Google
[00:35:53.360 --> 00:35:57.360]   knows about you in terms of face recognition. By the way, I apparently I didn't know this.
[00:35:57.360 --> 00:36:04.640]   Our sponsor Audible is also offering their audio version of 1984 for free right now. So highly
[00:36:04.640 --> 00:36:09.040]   recommended. I did not see this Pepsi ad. Let's watch it together. This is the Pepsi ad with
[00:36:09.040 --> 00:36:14.160]   Kendall Jenner that was pulled by Pepsi just today because everybody was so upset over it. And you're
[00:36:14.160 --> 00:36:21.840]   going to stay see because you're in charge of woking Leo. You're going to explain. I wouldn't
[00:36:21.840 --> 00:36:28.800]   be woke without you. You're going to explain why this ad is so horrific. So we're on a rooftop
[00:36:28.800 --> 00:36:35.440]   a guy's playing the cello. It's a big city. There's some sparks. There's a join the conversation
[00:36:35.440 --> 00:36:42.720]   protest. It's not exactly a protest. Join the conversation protest. Everybody. Oh, there's peace
[00:36:42.720 --> 00:36:47.680]   signs. Okay. So that's probably non controversial. Everybody likes peace. And then somebody's drawing
[00:36:47.680 --> 00:36:54.000]   she's making her sign. Wall drinking Pepsi. And that's Kendall Jenner. Is that by the way who that
[00:36:54.000 --> 00:36:59.840]   is? Because I don't. Yes. Yes. And she's famous because she's Bruce Jenner's daughter. She's famous
[00:36:59.840 --> 00:37:05.520]   because she's a Kardashian. She's a Kardashian. Well, she yes, she's the Jenner of the Kardashians.
[00:37:05.520 --> 00:37:11.280]   So she Kylie and Kendall and Kim and Khloe and so wait a minute. A Kardashian
[00:37:11.280 --> 00:37:17.520]   interbreaded with Jenner. Oh my God. That shouldn't be legal. Is that legal? That's
[00:37:17.520 --> 00:37:27.840]   Chris Jenner. Chris. Chris Jenner is Kim Kardashian's mom. Oh my God. My world is blowing up. Okay. So
[00:37:27.840 --> 00:37:34.560]   she's wearing a kind of a metal dress. And she's a model, right? Which is a model, right?
[00:37:35.440 --> 00:37:40.800]   Yes. So she's posing the photographers taking pictures of her. While the protest or the protest
[00:37:40.800 --> 00:37:45.840]   is going by, join the conversation, the join the conversation protest, which is a really
[00:37:45.840 --> 00:37:52.000]   as innocuous a protest as you can have. Yeah. Join the I'm not protesting. I think I'm just
[00:37:52.000 --> 00:37:55.360]   asking you to join the conversation. But wait a minute. Kendall Jenner has
[00:37:55.360 --> 00:38:00.560]   spottled this for some reason. There's still a guy playing cello solo in his now in his loft
[00:38:00.560 --> 00:38:05.120]   warehouse. Wait a minute. His eye has been drawn by the join the conversation protest or no.
[00:38:05.440 --> 00:38:12.240]   By a can of Black Pepsi. He's drinking that. Now he's going to his balcony of his massive loft in
[00:38:12.240 --> 00:38:18.480]   New York City. Beautiful people are having a cafe lunch. There's an Islam woman who's wearing a
[00:38:18.480 --> 00:38:24.800]   burqa or whatever. What do they call the just the little the headscarf headscarf. Yeah. And then
[00:38:24.800 --> 00:38:30.160]   there she is. Oh, she's disappointed by her photos. She doesn't like the pictures. She's looking out
[00:38:30.160 --> 00:38:34.720]   the window. There's the join the conversation protest. People are very angry that some people
[00:38:34.720 --> 00:38:39.680]   are not joining the protest. She grabs her camera. She's going to snap some photos of the oh, there's
[00:38:39.680 --> 00:38:45.840]   the guy with the cello. He's a now he's walking in the guys. There's somebody great dancing playing
[00:38:45.840 --> 00:38:55.200]   the guitar. And this is a long damn commercial. By the way, Pepsi is getting more attention from
[00:38:55.200 --> 00:38:59.920]   this commercial that they got in trouble for than anything they've ever done before. Yeah. This is
[00:38:59.920 --> 00:39:04.560]   there. I like to teach the world a sing moment. Wait, even more than when Michael Jackson caught
[00:39:04.560 --> 00:39:08.240]   on fire. Oh, no. I'll say you're right. You're right. You're right. If if they could get this
[00:39:08.240 --> 00:39:16.480]   guy's hair to catch on fire then. So apparently Kendall Jenner has locked eyes with somebody in the
[00:39:16.480 --> 00:39:22.080]   in the protest. And he's he's saying, come on. Oh, he's the cello guy. Join me. Yeah, he's the
[00:39:22.080 --> 00:39:27.040]   chilog. Is that her? Yeah. Okay. She's like eight years old. Okay. Wait a minute. Her hair has gone
[00:39:27.040 --> 00:39:30.720]   from blonde to brunette for reasons we don't know. She's wiping off her lipstick. She's joining
[00:39:30.720 --> 00:39:36.880]   the conversation. Now these are the most peaceful, happy protesters ever. But for some reason,
[00:39:36.880 --> 00:39:43.440]   there's a police presence. Clearly not Berkeley. Yeah. So Kendall has decided that she is going to
[00:39:43.440 --> 00:39:54.880]   give this stone called Fox policeman a Pepsi. Wow. I'm touched. This is the generations coming
[00:39:54.880 --> 00:40:02.480]   together. And then the lady with the car. That's it. That's it. Takes the picture. And the crowd
[00:40:02.480 --> 00:40:08.800]   cheers because everyone's everyone's hugging. They're all they're all drinking Pepsi. And Leo,
[00:40:08.800 --> 00:40:13.040]   by talking about it, aren't we in fact joining the conversation? We have joined the conversation.
[00:40:13.040 --> 00:40:18.480]   We're all joining with the Pepsi generation live boulder. Yeah, there's nothing more bold
[00:40:18.480 --> 00:40:24.160]   than joining the conversation. That is a bold statement. Drinking Pepsi is pretty bold. Live louder.
[00:40:24.160 --> 00:40:29.680]   Drinking Pepsi's bold. There's a person with bluish hair in there. Live for now. So why is this
[00:40:29.680 --> 00:40:36.560]   controversial in the least? So I think it's just co-opting what people. It's co-opting that one
[00:40:36.560 --> 00:40:42.960]   beautiful image, right? Of the of the African American woman with their dress flowing in the
[00:40:42.960 --> 00:40:48.080]   tax squad coming out of her at her that that image, right? Why it makes me. I'm just I'm just
[00:40:48.080 --> 00:40:53.440]   just co-opting it. Oh, fine. Pepsi. Pepsi. Just listen. Isn't this refreshing sounding?
[00:40:53.440 --> 00:41:02.000]   No. That's what a beer sounds like when you crack that open. That's what you said. There was
[00:41:02.000 --> 00:41:10.720]   only one one ad. It's not an ad if you're not getting paid for it. Good point. So I just think
[00:41:12.560 --> 00:41:19.840]   I think that people who are engaging in protests are awfully touchy about having the whole concept
[00:41:19.840 --> 00:41:25.760]   of protesting being co-opted by this shameless naked commercialization. I'll go with you on that.
[00:41:25.760 --> 00:41:36.400]   There's also there's a couple things. So one, it's co-opting with this stupid. I mean, like,
[00:41:36.400 --> 00:41:41.600]   if you're going to say something, Pepsi, say something, not a stupid join the conversation.
[00:41:41.600 --> 00:41:47.520]   That's just BS. And so in people today, you know, they're smart. They're cynical.
[00:41:47.520 --> 00:41:55.200]   I would say this is if before there was Twitter, there would have been nothing about this ad.
[00:41:55.200 --> 00:41:59.360]   Well, there wasn't had before there was Twitter and nothing happened.
[00:41:59.360 --> 00:42:02.720]   This is it's called I'd like to buy the world a Coke. It's the same thing.
[00:42:02.720 --> 00:42:08.480]   As I said, this is a the Twitter outrage mill, which actually is starting to scare me. Now,
[00:42:08.480 --> 00:42:12.960]   I agree. This is a dopey ad. I don't think it's particularly controversial. It's just dopey.
[00:42:12.960 --> 00:42:18.480]   But what I find interesting and the reason I'm bringing this up on this show is because this is
[00:42:18.480 --> 00:42:23.200]   Twitter at work, right? It's completely the Twitter backlash. When they say there's a backlash,
[00:42:23.200 --> 00:42:29.760]   it's all on Twitter. Yeah. It's on Facebook too. Except nobody saw it because.
[00:42:31.200 --> 00:42:39.440]   Well, there's I really am worried about the power of Twitter and Twitter outrage.
[00:42:39.440 --> 00:42:43.120]   Yeah. This I wouldn't worry about that. Really? Oh good. Okay.
[00:42:43.120 --> 00:42:49.040]   Well, I mean, okay. I feel better. I'm set on Twitter. People are upset on Facebook.
[00:42:49.040 --> 00:42:55.920]   These things come and go and I think it gives people an outlet for a time when they feel like
[00:42:55.920 --> 00:43:04.640]   nothing is in their control. It also helps promote genuine social awareness and change in a way.
[00:43:04.640 --> 00:43:10.880]   Like, I think you're just becoming more aware of how people are different from each other.
[00:43:10.880 --> 00:43:17.280]   And I think what's hard is we still don't treat the internet like it's real people on the other
[00:43:17.280 --> 00:43:24.000]   end. And I think that's going to be something that is decades of change, right? And it'll happen,
[00:43:24.000 --> 00:43:30.640]   hopefully. I don't know if that made any sense. It does. And I think it is a good and a bad thing.
[00:43:30.640 --> 00:43:37.120]   I agree with you completely. I mean, as we get sucked into virtual reality and into the Twitter
[00:43:37.120 --> 00:43:41.280]   sphere and all that kind of stuff, we have to always do a gut check and think about what is the
[00:43:41.280 --> 00:43:47.520]   actual world as we experience it like. For example, I personally talked to lots of people all the time
[00:43:47.520 --> 00:43:52.960]   and not a single one has been outraged or cared at all. Well, my point is precisely, I mean, if we
[00:43:52.960 --> 00:43:58.400]   could just ignore Twitter and it could just exist in a vacuum or whatever, do whatever it does,
[00:43:58.400 --> 00:44:04.400]   and it didn't then bleed into the real world, I wouldn't have a problem with it. But it's actually,
[00:44:04.400 --> 00:44:10.240]   that's the point, right? That it's exerting undue influence on actual policy.
[00:44:10.240 --> 00:44:15.040]   But I don't think this is bad. I don't think it's bad that people said, "Hey, yo, Pepsi,
[00:44:15.040 --> 00:44:21.280]   your ad sucked because it was offensive to people who actually protest and do things that care about.
[00:44:22.560 --> 00:44:29.840]   It used Kendall freaking Jenner." But who cares? It's just a two-care.
[00:44:29.840 --> 00:44:36.320]   Ads are dopey. All ads are dopey. All ads use celebrities. Ads are not just dopey. Ads
[00:44:36.320 --> 00:44:41.440]   are actually a huge part of the cultural conversation. Think about it. We still talk about certain,
[00:44:41.440 --> 00:44:44.480]   like the "Where's the Beef?" lady or we all knew about Michael Jackson and his Pepsi ad. Nobody
[00:44:44.480 --> 00:44:47.920]   would be talking about this Pepsi ad if it weren't for the Twitter outrage engine. It would have
[00:44:47.920 --> 00:44:54.400]   just gone away without a trace. I don't think I don't know. I think the outrage is partly based on
[00:44:54.400 --> 00:45:00.960]   the content of protests nowadays. For example, yes, we have this trivialized culture, but there
[00:45:00.960 --> 00:45:05.680]   are people out there trying to affect meaningful change. People who are deeply invested in the
[00:45:05.680 --> 00:45:13.760]   Black Lives Matter movement see these protests as a surrogate for the kind of protests they've
[00:45:13.760 --> 00:45:20.240]   done. What they're marching for is people's lives. It's a very big, heavy thing.
[00:45:20.240 --> 00:45:23.360]   All right. All right. A lot of people are protesting the president.
[00:45:23.360 --> 00:45:27.840]   So maybe this is a bad example, but you guys, don't you agree that the Twitter outrage engine is
[00:45:27.840 --> 00:45:35.200]   given disproportionate power in the media? Absolutely. Yes, because it's easy to find a story.
[00:45:35.200 --> 00:45:37.520]   Like here's a great example. That's lazy reporting.
[00:45:38.240 --> 00:45:45.680]   Is the United kicking the girls off the airline for wearing leggings that traveling a travel?
[00:45:45.680 --> 00:45:50.080]   That's an example of the instant outrage engine. Yes, I could not believe that this story,
[00:45:50.080 --> 00:45:57.120]   quote unquote, had legs and made it over. I mean, granted as a weekend, so slow news, but
[00:45:57.120 --> 00:46:02.880]   and yes, I think United was totally in the wrong, but I was like, what the heck is happening?
[00:46:03.520 --> 00:46:08.640]   People look at people always are outraged. People wake up and they have outrage before their
[00:46:08.640 --> 00:46:15.040]   Wheaties. The difference is that now they have this outlet and most important that that by itself
[00:46:15.040 --> 00:46:20.640]   would be meaningless. And most importantly, the news media picks it up and says people are outraged
[00:46:20.640 --> 00:46:28.160]   instead of some grumpy people tweeted. Well, then what about Fox News? Fox News and public TV kind of
[00:46:28.160 --> 00:46:34.160]   news is the original outrage machine. I mean, think about maybe it's like this. You've probably
[00:46:34.160 --> 00:46:39.200]   read this. Joseph Bernstein Buzzfeed. Never mind the Russians. Meet the bot king who helps Trump
[00:46:39.200 --> 00:46:47.520]   win Twitter. Did you read about this guy? Microchip? So this guy is a Midwestern, not a Russian
[00:46:47.520 --> 00:46:52.160]   Midwestern guy. Don't look it up on Twitter, though. It's gone. They took it down. Yeah, but
[00:46:52.160 --> 00:46:57.680]   they take it down all the time and he comes back all the time. He's been banned countless numbers
[00:46:57.680 --> 00:47:02.720]   of time. The example they're giving is the most recent Susan Rice unmasking. He decided that he
[00:47:02.720 --> 00:47:07.040]   decided this guy decided that was going to be a story. So he began blasting out dozens of tweets
[00:47:07.040 --> 00:47:15.440]   and retweets. He tweets, it would be nice to get Susan Rice trending. And then he did it. Now,
[00:47:15.440 --> 00:47:21.760]   this guy is apparently a web developer with good skills. He hides his traces very well so that
[00:47:21.760 --> 00:47:27.360]   nobody really able to figure out his real identity. He agreed to talk to Buzzfeed because he's a
[00:47:27.360 --> 00:47:34.080]   publicity junkie as well. Over the 24 hours following his own call to arms microchip,
[00:47:34.080 --> 00:47:38.800]   tweeted or retweeted more than 300 times about Rice, including everything from a
[00:47:38.800 --> 00:47:43.600]   photoshopped image of Donald Trump eating her head out of a taco bowl to demands that she die in
[00:47:43.600 --> 00:47:49.680]   jail, almost always accompanied by the tag hashtag Susan Rice. And then he implored others to do
[00:47:49.680 --> 00:47:55.680]   this. These are these these are the tweets. This is what this micro magic jingle is gone,
[00:47:55.680 --> 00:47:58.960]   but he'll have another account because it's very easy to create accounts on Twitter.
[00:47:58.960 --> 00:48:05.440]   By 9am, the next day the tag was tweeted 20,000 times an hour was trending on Twitter,
[00:48:05.440 --> 00:48:11.760]   34,000 times an hour by 11am. And then it got the most important retweet of all the retweet from
[00:48:11.760 --> 00:48:18.800]   the president. And this basically here's his here's his triumphant tweet that evening starting with a
[00:48:18.800 --> 00:48:23.760]   single Cernovich article. We knocked this one out of the park. We can do this every day just like we
[00:48:23.760 --> 00:48:30.560]   did before. And it becomes a story. They know how to make it a story. And that this guy is one guy
[00:48:30.560 --> 00:48:37.200]   who's manipulating Twitter. He says, he so he buzzfeed news got an interview with him.
[00:48:37.200 --> 00:48:43.280]   And he says, I feel like I'm a scientist showing electricity to natives that have been convinced
[00:48:43.280 --> 00:48:49.840]   electricity is created by Satan. So they murder the scientist. He he says, he's discovered how to
[00:48:49.840 --> 00:48:55.680]   manipulate and use Twitter. He says, it's all us not Russians and we're not going to stop.
[00:48:55.680 --> 00:49:03.440]   This is this is now I I'm not bashing on Twitter again. I know you I know you don't like that,
[00:49:03.440 --> 00:49:07.680]   but I'm not even able to know that because it's not Twitter's fault. Twitter just exists like any
[00:49:07.680 --> 00:49:11.840]   social network. It's really the fault of the media that gives this all this weight because this is
[00:49:11.840 --> 00:49:16.480]   just a guy running a bunch of bots. But it's given weight. Twitter is outraged.
[00:49:18.320 --> 00:49:21.440]   And actually they don't ever say Twitter's outraged because they know that would mean
[00:49:21.440 --> 00:49:26.080]   that would diminish the importance of the article. They say people are outraged.
[00:49:26.080 --> 00:49:32.560]   Thousands of people are protesting. So Leo, I know that there's a certain amount of
[00:49:32.560 --> 00:49:38.400]   fatigue around bashing Twitter, but why isn't it Twitter's fault? We can't wait until every human
[00:49:38.400 --> 00:49:43.040]   being is virtuous because we would wait a long time. It wouldn't be because it wouldn't be a
[00:49:43.040 --> 00:49:48.800]   problem if it weren't picked up and then covered by mainstream media. But it wouldn't go viral if
[00:49:48.800 --> 00:49:53.520]   Twitter didn't if Twitter were able to block bots if Twitter could do that.
[00:49:53.520 --> 00:50:02.640]   For example, like you look at Google+, or Facebook Facebook has a real names policy Google+ used to
[00:50:02.640 --> 00:50:08.080]   Google+, is totally closed outside. So it's Facebook. So doing bots and things like that is a little
[00:50:08.080 --> 00:50:16.080]   harder. I think there's just a lot more. There's so much damage that's done for a nominal benefit
[00:50:16.080 --> 00:50:23.520]   by Twitter. And so I think that the media has, he showed Buzzfeed dozens of accounts he has ready
[00:50:23.520 --> 00:50:28.640]   to take over when he gets suspended because he always does get suspended. But he uses VPNs.
[00:50:28.640 --> 00:50:35.200]   He hides his name. And this is the payoff quote. Michael Chip says he has an ideal platform in
[00:50:35.200 --> 00:50:39.840]   Twitter in which to shape a message quote, Twitter is easier than other social networks and more
[00:50:39.840 --> 00:50:45.520]   volatile, he said, emotions run high at 140 characters. The chaos is perfect.
[00:50:45.520 --> 00:50:56.080]   So if you're worried about fake news, fine, but these are perfect platforms for manipulation.
[00:50:56.080 --> 00:50:57.040]   Yeah.
[00:50:57.040 --> 00:51:02.400]   Michael Chip says I can make whatever claims I want to make. This is how the game works.
[00:51:03.520 --> 00:51:05.680]   This is why I don't source my stories from Twitter.
[00:51:05.680 --> 00:51:10.160]   Right. Thank you. That's all I'm saying. Stop sources stories from Twitter.
[00:51:10.160 --> 00:51:18.240]   But bloggers and journalists are not going to do that. So the media.
[00:51:18.240 --> 00:51:24.160]   That's not true. I think the challenge is here is one of the challenges.
[00:51:24.160 --> 00:51:32.320]   You have to decide like the original people reporting on this are media organizations with
[00:51:32.320 --> 00:51:40.160]   an agenda. One, two, when you're a reputable news organization, you assess like,
[00:51:40.160 --> 00:51:46.320]   is this a tempest in a teapot or is this part of a deeper issue that's important to lots of people?
[00:51:46.320 --> 00:51:53.440]   And that's how that's how you should. I mean, you can't just say I can't source anything on
[00:51:53.440 --> 00:51:57.920]   Twitter. I have to ignore Twitter. I think it's a good indicator of where the zeitgeist is.
[00:51:57.920 --> 00:52:00.960]   But then you have to do actual reporting around it. And I don't think these.
[00:52:00.960 --> 00:52:04.480]   Like, I mean, isn't Twitter a bubble? I mean, everybody always says, oh, you know,
[00:52:04.480 --> 00:52:07.760]   Silicon Valley is a bubble and, you know, they're not real people.
[00:52:07.760 --> 00:52:13.840]   Twitter only has what? How many active users a month? 300 million monthly active users.
[00:52:13.840 --> 00:52:17.840]   Max. And half of those are bots. So.
[00:52:17.840 --> 00:52:19.120]   And the other half are journalists.
[00:52:19.120 --> 00:52:26.240]   It doesn't. It reflects a limited zeitgeist. I mean, it's not it's not how people in the real
[00:52:26.240 --> 00:52:30.560]   world are thinking or is it? Well, I mean, this is this is the problem. So
[00:52:30.560 --> 00:52:34.320]   so there are journalists and there are people in the public who don't think, oh, there's this
[00:52:34.320 --> 00:52:38.400]   problem that people are sending out this box. What they think is, oh, this is true. This is the
[00:52:38.400 --> 00:52:44.960]   truth getting past the dishonest media. Right. And it's a democratizing platform. And enough of
[00:52:44.960 --> 00:52:50.400]   it circulates to make it a phenomenon. So even if the mainstream media, some of these things are
[00:52:50.400 --> 00:52:54.960]   so viral that it's actually a dereliction to not cover the virality of the message that's going
[00:52:54.960 --> 00:52:59.440]   out there. I mean, the fact that lots of people are repeating and sharing this information itself
[00:52:59.440 --> 00:53:05.600]   is in fact, legitimate news, which which as you see on the main street on the cable news shows,
[00:53:05.600 --> 00:53:11.360]   just covering the fact that something is a fake news story legitimizes the fake news story.
[00:53:11.360 --> 00:53:15.760]   That's just the nature of that medium. There was a great piece circulating today,
[00:53:15.760 --> 00:53:21.920]   pointing out how why comedians are so much better at covering fake news and false statements by
[00:53:21.920 --> 00:53:27.280]   politicians and others, because they don't have this sense of fairness that media, the media
[00:53:27.280 --> 00:53:34.720]   requires the sense of fairness is being manipulated by the fake news crowd and the propagandists.
[00:53:34.720 --> 00:53:38.800]   And they almost can't help it, whereas comedians can go right for the jugular and say, oh, this is
[00:53:38.800 --> 00:53:46.160]   ridiculous. It was awesome. Yeah. Yeah. Although that's partly also because comedians have no
[00:53:46.160 --> 00:53:51.280]   responsibility to be accurate or balanced or informed or have facts or any of that stuff.
[00:53:51.280 --> 00:53:55.440]   They just have to be funny. It's a lot easier if that's the only requirement.
[00:53:56.000 --> 00:54:01.600]   Craig, a new Mark, a founder of Craigslist has teamed up with Mozilla and Facebook to launch a
[00:54:01.600 --> 00:54:08.400]   14 million dollar fund to support, they say news integrity. They don't want to use the fake news
[00:54:08.400 --> 00:54:14.000]   term. The news integrity initiative will be it was created with the goal of increasing trust
[00:54:14.000 --> 00:54:20.640]   and journalism worldwide and better informing the public conversation. They've already, I think
[00:54:20.640 --> 00:54:28.240]   they've already given a chunk of change. But this is, I should disclaim this, that Jeff Jarvis is
[00:54:28.240 --> 00:54:32.240]   involved in this. The CUNY Graduate School of Journalism is administering the project.
[00:54:32.240 --> 00:54:37.440]   And Jeff is very intimately involved. And we're going to ask him about it next week. He asked me
[00:54:37.440 --> 00:54:42.880]   to his great credit. He emailed me when this was announced and said, I'm really excited about
[00:54:42.880 --> 00:54:49.200]   this. But if you feel that this disqualifies me from Twig, I'll be glad to resign. I said, no,
[00:54:49.200 --> 00:54:55.520]   absolutely not. His, I guess his concern is the relationship with Facebook. But he says, of course,
[00:54:55.520 --> 00:55:02.640]   it'll continue to be a skewer Facebook as much as I want. It is being called the Facebook Journalism
[00:55:02.640 --> 00:55:11.040]   Project. And who do they give you some money to? So Arizona State's working out at the Center for
[00:55:11.040 --> 00:55:16.160]   Community and Ethnic Media at CUNY, the constructive institute at our Houssey University in Denmark,
[00:55:16.160 --> 00:55:23.760]   Edelman based on the Edelman interesting big PR firm. Lots of universities and schools,
[00:55:23.760 --> 00:55:28.960]   Weber, Shandwick, another PR firm. What do I PR firms are interested in this?
[00:55:28.960 --> 00:55:36.080]   Having a credible media that you can talk to is probably better for them than having.
[00:55:36.080 --> 00:55:39.520]   Yeah, good. I don't know. I like that. I hope that's true.
[00:55:40.240 --> 00:55:46.960]   Beta works involved as well, the Democracy Fund, the Ford Foundation. And I'll be, you know what,
[00:55:46.960 --> 00:55:51.040]   we'll talk about it when Jeff gets back next week. And I would like to understand what they're
[00:55:51.040 --> 00:55:56.080]   going to try to do. I guess what they're doing is giving out some chunks of change to
[00:55:56.080 --> 00:56:02.480]   people that are doing good work. And my guess also is that they're going to try to figure out
[00:56:02.480 --> 00:56:06.960]   what is the problem really and how can it be best to dress. And that's most practice is among
[00:56:06.960 --> 00:56:10.080]   journalists. Now, one of the things that I think is interesting is that there's,
[00:56:10.080 --> 00:56:14.880]   you know, the fake news label has been thrown in the other direction by the president and others,
[00:56:14.880 --> 00:56:20.800]   basically confusing like what's fake news and people now disagree. It's a point of disagreement
[00:56:20.800 --> 00:56:25.040]   about whether the mainstream media is fake news or real news and whether they.
[00:56:25.040 --> 00:56:28.240]   Well, I was always uncomfortable with that term fake news. And that's exactly why.
[00:56:28.240 --> 00:56:31.280]   Yeah, but it's in the eye of the beholder.
[00:56:31.280 --> 00:56:34.800]   It's interesting to see who's driving initiatives like this and who isn't.
[00:56:36.000 --> 00:56:43.520]   So I will say this debate over objective journalism over perspectives and where you're coming from
[00:56:43.520 --> 00:56:49.600]   has been around, at least as long as I've been in journalism, which is like 20 something years
[00:56:49.600 --> 00:56:53.760]   and probably as long as Leo's been in journalism, which is like 100 years, which is yeah.
[00:56:53.760 --> 00:57:02.800]   Actually, actually, I've quoted this before, but the first fake news appeared in the very first
[00:57:02.800 --> 00:57:10.000]   newspaper in 1832, where there was a newspaper where in New York City and the Herald published
[00:57:10.000 --> 00:57:14.320]   a story apparently that there was a very powerful microscope and we've discovered life on Mars and
[00:57:14.320 --> 00:57:21.200]   had a series of articles about the Martians in order to build circulation.
[00:57:21.200 --> 00:57:23.120]   But that was the good kind of fake news.
[00:57:27.600 --> 00:57:31.120]   You just have to be accurate. The challenges.
[00:57:31.120 --> 00:57:33.920]   Now the world has gotten so complicated.
[00:57:33.920 --> 00:57:37.120]   Even accurate news reporting can be fake news.
[00:57:37.120 --> 00:57:41.200]   I'll go to finish it.
[00:57:41.200 --> 00:57:46.080]   I'm going to try to be sincere and not cynical here. But what's happening?
[00:57:46.080 --> 00:57:52.880]   Our world is so complicated. I can report a series of facts.
[00:57:52.880 --> 00:58:00.240]   And because those facts influence things like policy, people will interpret it different ways
[00:58:00.240 --> 00:58:03.040]   or find ways to take what I'm saying and.
[00:58:03.040 --> 00:58:06.560]   That's right. It'll be interpreted as polemic instead of reporting.
[00:58:06.560 --> 00:58:14.480]   So this is a challenge. It's a challenge that's only gotten worse with a president who has decided
[00:58:14.480 --> 00:58:20.320]   not to respect the historical accurate news organizations of our time.
[00:58:21.680 --> 00:58:25.600]   I would go farther with a president who has decided not to respect facts in any way.
[00:58:25.600 --> 00:58:26.480]   Okay. Well, that's true.
[00:58:26.480 --> 00:58:30.480]   This is really not a trivial thing.
[00:58:30.480 --> 00:58:39.280]   No. If the leadership lies to you so that blatantly, so much so that you start to doubt,
[00:58:39.280 --> 00:58:43.760]   it's gaslighting, so much so that you start to doubt your own mental faculties,
[00:58:43.760 --> 00:58:47.040]   that is a huge problem and it's a much bigger problem than fake news.
[00:58:47.040 --> 00:58:48.160]   Yeah.
[00:58:48.160 --> 00:58:53.280]   I mean, I talked to my dad who was a Trump supporter, actually for the first time.
[00:58:53.280 --> 00:58:58.000]   And since Trump was inaugurated and initiated his Muslim band.
[00:58:58.000 --> 00:59:06.960]   And it was astonishing the different level of facts my dad is working with.
[00:59:06.960 --> 00:59:10.480]   That is leading him to the conclusions that he's been making.
[00:59:10.480 --> 00:59:12.560]   Even which are the stories that matter?
[00:59:15.120 --> 00:59:22.880]   I mean, we can talk about stories in his frame of reference is completely different than mine.
[00:59:22.880 --> 00:59:26.400]   And I, of course, am I crazy?
[00:59:26.400 --> 00:59:29.520]   Is he crazy?
[00:59:29.520 --> 00:59:30.560]   It is.
[00:59:30.560 --> 00:59:39.680]   It's why we have this growing chasm in America because we aren't even talking about the same
[00:59:39.680 --> 00:59:42.800]   things anymore. His facts aren't your facts anymore.
[00:59:43.520 --> 00:59:48.720]   Right, and it's very frustrating because there's no way, because both of us can agree on
[00:59:48.720 --> 00:59:52.320]   things not in the abstract, right?
[00:59:52.320 --> 00:59:54.080]   >> Is the sky blue?
[00:59:54.080 --> 00:59:54.960]   Is the sun up?
[00:59:54.960 --> 00:59:55.520]   Is it the sun?
[00:59:55.520 --> 01:00:00.240]   >> Well, no, I was going to, an issue is a friend of mine who is Muslim and who is an American
[01:00:00.240 --> 01:00:05.120]   citizen. He no longer can travel because he gets stopped.
[01:00:05.120 --> 01:00:08.800]   For business, it is inefficient for him to leave the country right now.
[01:00:08.800 --> 01:00:09.520]   >> It's shocking.
[01:00:09.520 --> 01:00:13.200]   And it's distressing. He is a CEO of a company.
[01:00:13.200 --> 01:00:13.760]   >> It's really distressing.
[01:00:13.760 --> 01:00:21.760]   >> And my dad can agree that that is distressing to him and feel bad that a guy he knows has that
[01:00:21.760 --> 01:00:24.640]   problem. But he can't take that
[01:00:24.640 --> 01:00:31.520]   and tie it to the greater policies because those facts are completely different.
[01:00:31.520 --> 01:00:35.920]   And it's an invert gaslighting.
[01:00:35.920 --> 01:00:42.240]   >> And I try to vote myself in the position, and I also have no friends and close to many
[01:00:42.240 --> 01:00:46.480]   people who voted for Trump and who support him as president.
[01:00:46.480 --> 01:00:52.320]   And I understand that what they really like is the traditional conservative point of view of
[01:00:52.320 --> 01:00:55.200]   small government and less regulation and all that stuff.
[01:00:55.200 --> 01:01:00.080]   And they're kind of, I have to think at some point, they're just kind of a little bit holding
[01:01:00.080 --> 01:01:08.000]   their nose and trying to ignore the conflicts of interest, the corruption, the weird tweeting.
[01:01:08.000 --> 01:01:13.280]   And just kind of saying, well, that's just we have to put up every president has got some
[01:01:13.280 --> 01:01:18.000]   skeletons in their closet. Bill Clinton was no winner either. But because
[01:01:18.000 --> 01:01:24.800]   they support my political leanings, then I'm going to support this guy because in the long run,
[01:01:24.800 --> 01:01:28.800]   we're going to get what we want, which is a smaller government and less regulation.
[01:01:28.800 --> 01:01:31.920]   And we're going to get a more conservative Supreme Court. And that's going to be good.
[01:01:31.920 --> 01:01:36.880]   And so the guy's a lunatic, but they never want to think that. I'm sure your dad doesn't
[01:01:36.880 --> 01:01:39.600]   think that there's something wrong with Donald Trump, but they're just kind of
[01:01:39.600 --> 01:01:43.840]   willing willfully kind of saying, just won't pay too much attention to that.
[01:01:43.840 --> 01:01:49.200]   >> A lot of Trump supporters know that he's a shameless con man, but he's
[01:01:49.200 --> 01:01:50.480]   our shameless. >> Yeah, he's a sales guy.
[01:01:50.480 --> 01:01:58.080]   >> So one of the things that we talked about accidental fake news, I'm really concerned mostly
[01:01:58.080 --> 01:02:04.320]   with the real kind, this concept was launched by the KGB years ago. It's called
[01:02:04.320 --> 01:02:08.720]   Disinformatia, and the Russians have kind of perfected it. And the way this works is that you
[01:02:08.720 --> 01:02:14.480]   have multiple credible sounding versions of every single event. And the perfect example was when
[01:02:14.480 --> 01:02:22.400]   that Malaysia Airlines plane was shot down over the Ukraine some time ago. And the Russian news had
[01:02:22.960 --> 01:02:27.200]   five or six or 10 different explanations for what caught John Podesta did it.
[01:02:27.200 --> 01:02:30.080]   They're all these different- >> Podesta did it, really.
[01:02:30.080 --> 01:02:33.120]   >> They're all these, well, I didn't read them, I didn't read Russian.
[01:02:33.120 --> 01:02:33.680]   >> [LAUGH]
[01:02:33.680 --> 01:02:37.920]   >> There were all these multiple plausible explanations. And so depending on your political
[01:02:37.920 --> 01:02:41.280]   leanings and your political loyalties and who were- >> I heard it was time to resale,
[01:02:41.280 --> 01:02:44.720]   but okay, go ahead. >> It was down over to, and so you pick
[01:02:44.720 --> 01:02:48.960]   the one that you like the best. And there's no truth. And every time anybody tries to say,
[01:02:48.960 --> 01:02:52.320]   well, no, the Russians shot it down. They trucked in the thing. No, no, no,
[01:02:52.320 --> 01:02:54.480]   that's what they want you to believe. >> Well, look at that.
[01:02:54.480 --> 01:02:59.200]   Look at people who think that 9/11 was an inside job. >> Yeah.
[01:02:59.200 --> 01:03:03.600]   >> Or I feel so bad for the parents and families in Sandy Hook,
[01:03:03.600 --> 01:03:11.120]   who just this other, just yesterday, said, can please, can you stop spreading the news,
[01:03:11.120 --> 01:03:16.480]   the fake conspiracy theory that this was a false flag government operation? Our children died.
[01:03:16.480 --> 01:03:22.000]   This was not a false flag. And it makes me so sad, but go to YouTube.
[01:03:22.000 --> 01:03:27.280]   You can read, there's, someone's just saying, there's hours of videos about how the earth is flat.
[01:03:27.280 --> 01:03:29.120]   >> Yeah. >> Mm-hm.
[01:03:29.120 --> 01:03:33.840]   >> Demonstrably false, but people believe it. >> Yeah.
[01:03:33.840 --> 01:03:37.680]   >> I guess it's human. I don't know. I guess it's human.
[01:03:37.680 --> 01:03:41.200]   >> Well, there are actually some really good books about this right now.
[01:03:41.200 --> 01:03:44.240]   I will come with the reading list next time, because I can't remember the names of them right now.
[01:03:44.240 --> 01:03:49.520]   >> Good. Well, the big picture is that this is the great challenge of our generation of journalism.
[01:03:49.520 --> 01:03:56.000]   We have to figure out how to communicate the truth and also take into account alternative
[01:03:56.000 --> 01:03:59.680]   perspectives and find the difference between alternative perspectives.
[01:03:59.680 --> 01:04:03.280]   >> There are things that are debatable. That's debatable. We can have a debate. We have a conversation.
[01:04:03.280 --> 01:04:09.120]   But then there are things that are not debatable. The earth is not flat. That's not really up for
[01:04:09.120 --> 01:04:13.120]   conversation. That's demonstrably, in factually incorrect.
[01:04:13.120 --> 01:04:19.920]   >> I am a partisan for not only newspapers, but also print newspapers.
[01:04:19.920 --> 01:04:23.520]   And I've been subscribed to The New York Times and other publications for many, many years.
[01:04:23.520 --> 01:04:30.720]   I think that we had the solution and we are eager to flush that solution down the toilet.
[01:04:30.720 --> 01:04:37.040]   When you subscribe to several newspapers and news magazines that are of high quality and that
[01:04:37.040 --> 01:04:44.400]   run the political spectrum, subscribe to Reason and the national review, but also the New Yorker
[01:04:44.400 --> 01:04:48.080]   and the Atlantic Monthly and The New York Times and maybe The Post or whatever it is.
[01:04:48.080 --> 01:04:56.000]   Choose credible publications and read them from front to back. You're getting great perspective.
[01:04:56.000 --> 01:05:01.200]   You're getting the legitimate. And we're so eager to get rid of that and get our news on Facebook,
[01:05:01.200 --> 01:05:06.000]   which is a terrible idea. >> Well, I mean,
[01:05:06.880 --> 01:05:14.000]   people think that, you know, Infowars is a legitimate publication and the people who believe that
[01:05:14.000 --> 01:05:16.160]   sincerely believe that. >> Including our president.
[01:05:16.160 --> 01:05:19.600]   >> So, I mean, it's one thing. >> Is everybody watching Homeland?
[01:05:19.600 --> 01:05:22.960]   >> Do they talk about Alex Jones on Homeland?
[01:05:22.960 --> 01:05:27.680]   >> They have an Alex Jones type person who's in cahoots with a shady operative. I won't give
[01:05:27.680 --> 01:05:33.360]   too many spoilers, but it's Alex Jones on crack. I mean, it's like so scary what they're depicting
[01:05:33.360 --> 01:05:36.720]   in that show and it's kind of sort of what's happening in the world globally.
[01:05:36.720 --> 01:05:43.440]   >> Has this always been, this must always have been part of the human experience, right?
[01:05:43.440 --> 01:05:46.480]   They've always been. >> So, it's probably.
[01:05:46.480 --> 01:05:47.440]   >> Viral news. >> We didn't have.
[01:05:47.440 --> 01:05:50.800]   >> And we didn't have a president who believed in it.
[01:05:50.800 --> 01:05:54.880]   >> I don't know if that's true. I mean, who don't know. Maybe Andrew Jackson really believed
[01:05:54.880 --> 01:05:58.880]   in the red, red Indians were to Satan's spawn. I mean.
[01:05:58.880 --> 01:06:01.440]   >> Oh, I believe that Andrew Jackson actually probably thought that.
[01:06:01.440 --> 01:06:06.480]   >> He did. He killed, he created the Trail of Tears. I mean, and by the way, the hero,
[01:06:06.480 --> 01:06:12.640]   his portrait hangs in the Oval Office. But I'm just saying that we just didn't know
[01:06:12.640 --> 01:06:15.440]   because we didn't have all this electronic media.
[01:06:15.440 --> 01:06:17.600]   >> Yeah, we know too much. We know too much.
[01:06:17.600 --> 01:06:20.960]   >> I know too. >> So often, and I'm the worst person for this,
[01:06:20.960 --> 01:06:26.960]   because I read a thousand stories today. I mean, I'm like, I'm so into information
[01:06:26.960 --> 01:06:29.680]   overload and reading all this stuff. And sometimes I'm reading a story and I'm like,
[01:06:30.400 --> 01:06:36.000]   you know, some kid lost his puppy and whatever. I would never be reading about this.
[01:06:36.000 --> 01:06:38.160]   >> I know. >> Why is this a general interest?
[01:06:38.160 --> 01:06:40.640]   >> It's all the time. That's called using Facebook, Mike.
[01:06:40.640 --> 01:06:42.240]   >> Yeah. >> Why would I be reading this?
[01:06:42.240 --> 01:06:43.440]   >> That's what it is. >> Right.
[01:06:43.440 --> 01:06:47.360]   >> I would never see this. >> And then it passes and you go to the next thing.
[01:06:47.360 --> 01:06:58.640]   >> For a while, one brief shining utopian moment, I thought that we were going to live in a society
[01:06:58.640 --> 01:07:03.520]   where facts had become commoditized because they were so readily available and everybody could
[01:07:03.520 --> 01:07:09.520]   agree on them. And this would make for a marvelous future where we didn't have to worry about fact
[01:07:09.520 --> 01:07:14.080]   gathering. I remember my father-in-law when he first saw an iPad and he was looking,
[01:07:14.080 --> 01:07:18.480]   like he was one of the science apps. And he was saying, man, if Copernicus had had this,
[01:07:18.480 --> 01:07:23.360]   he would have had all the data he needed to come to figure out that the Earth revolved the sun,
[01:07:23.360 --> 01:07:28.480]   he would have had to grind all that glass and spend his entire lifetime making telescopes
[01:07:29.040 --> 01:07:33.680]   and so we were going to live in this era where all of this information was available and then
[01:07:33.680 --> 01:07:39.520]   now the highest aspiration, where humans could start to really excel, start to being data synthesis
[01:07:39.520 --> 01:07:44.560]   machines and we'd come up with brilliant new insights and instead we just can't agree on the
[01:07:44.560 --> 01:07:50.320]   facts anymore. >> Well, we can when it comes to technology, I mean, like look at the toaster
[01:07:50.320 --> 01:07:53.760]   challenge where you try to get people to build a toaster. I mean, we're doing- >> I don't know if
[01:07:53.760 --> 01:07:57.520]   it's hysterical, isn't it? >> We're making really awesome things.
[01:07:57.520 --> 01:08:01.440]   >> You know why that is? >> You know why that is because in technology,
[01:08:01.440 --> 01:08:03.760]   and this is why I love engineers, it either works or it doesn't work.
[01:08:03.760 --> 01:08:07.200]   >> Right. >> It's very black and white. So you don't
[01:08:07.200 --> 01:08:12.560]   ever have to say, well, I think I can fly because you either can or you can't and if you don't have
[01:08:12.560 --> 01:08:19.120]   the technology, you'll never fly. So- >> It also has to be said that today's Copernicus are not
[01:08:20.000 --> 01:08:25.760]   spending all their time on Facebook getting fake news. That glorious world where all the facts
[01:08:25.760 --> 01:08:31.440]   are available to the Copernicus is of the world exists and they're out there taking advantage of
[01:08:31.440 --> 01:08:36.960]   it. It's a wonderful world and it's also a sewer. >> Yeah, I hope that's true. And I hope that maybe
[01:08:36.960 --> 01:08:40.880]   that's kind of what I was saying in the beginning, which is maybe if we all spend a little less time
[01:08:40.880 --> 01:08:49.760]   paying attention to what Twitter outrage storms and Facebook happy dog pictures, maybe we could
[01:08:49.760 --> 01:08:53.600]   start actually making some progress. >> Yeah, because it's outrageous.
[01:08:53.600 --> 01:08:59.200]   >> Hey, maybe we want to talk about like- >> I want to talk about something good. Google TV,
[01:08:59.200 --> 01:09:02.400]   how about that? >> Okay, I was going to say Google Home has some cool stuff.
[01:09:02.400 --> 01:09:07.040]   >> Yeah, let's see. >> Google TV works too. >> No more, no more, no more a big. This show is
[01:09:07.040 --> 01:09:12.000]   really an interesting mix of big subjects like that and then, hey, you want to see the new phone?
[01:09:12.000 --> 01:09:18.080]   This thing's cool. That's what's fun about this show. You never know what you're going to get.
[01:09:18.080 --> 01:09:24.160]   Stacey Higginbotham is here from Stacey on IoT. She's at theiotpodcast.com. Great show she does
[01:09:24.160 --> 01:09:33.120]   with Kevin Tofill writes about IoT and is a brilliant and inspiring journalist and we're
[01:09:33.120 --> 01:09:37.440]   thrilled to have her on the show. I would say exactly the same about my good friend Mike Elgin,
[01:09:37.440 --> 01:09:42.400]   who has found a lifestyle that we all envy. >> Yep. >> Damn you.
[01:09:42.400 --> 01:09:45.760]   >> Yeah, travel the world. >> Yeah. >> And eat everything.
[01:09:45.760 --> 01:09:52.640]   Oh man, gastronomad.net and you'll see his, right? In fact, you wrote a piece on why YouTube TV
[01:09:52.640 --> 01:09:56.960]   matters, which we will talk about. They just launched it and I signed up immediately.
[01:09:56.960 --> 01:10:05.040]   So we'll talk about shiny things and just a happy thing. >> Happy shiny people.
[01:10:05.040 --> 01:10:09.360]   >> No shiny, happy people. >> Okay, whatever.
[01:10:09.360 --> 01:10:15.040]   >> I got the facts wrong. Waze is going to be an Android Auto.
[01:10:15.040 --> 01:10:18.480]   >> Nice. >> Got Android Auto. >> Nobody.
[01:10:18.480 --> 01:10:25.200]   >> Just checking. >> Well, we have our own, we're in our own little world.
[01:10:25.200 --> 01:10:30.640]   Did you see that both Stacey and I drive Tesla's? Did you see that Elon has started a new company
[01:10:30.640 --> 01:10:36.160]   to facilitate the brain, the bio chip interface? >> I thought that was an April Fool's joke.
[01:10:36.160 --> 01:10:39.680]   >> Is it? >> That wasn't Neura Linker.
[01:10:39.680 --> 01:10:43.360]   >> Oh, is that, oh, did I get fooled? >> Hold on.
[01:10:43.360 --> 01:10:46.320]   >> I thought I saw. >> I thought I saw. >> I freaking hate, I freaking hate.
[01:10:46.320 --> 01:10:52.320]   >> No, is that. >> Okay, no, no, it might be real.
[01:10:52.320 --> 01:10:55.360]   >> If it's a fake, then he spent a lot of energy on building the website.
[01:10:55.360 --> 01:10:57.760]   >> No, it's not. >> I can't wait for the mental autopilot.
[01:10:57.760 --> 01:11:02.880]   No, this is what we wanted. This is what's in sci-fi. I want to jack in.
[01:11:02.880 --> 01:11:07.520]   Forget the visor in the HoloLens. >> Right. >> Google Glass, the earpods.
[01:11:07.520 --> 01:11:10.880]   >> It's worried about government surveillance because we have cameras and microphones.
[01:11:10.880 --> 01:11:14.640]   >> I don't care. It'll be worth it. They can read my mind. I don't care.
[01:11:14.640 --> 01:11:20.400]   I just want the metaverse, please. >> Like that guy in the matrix. I don't want to
[01:11:20.400 --> 01:11:23.600]   remember nothing. Plug me into the power plant. I don't want to remember nothing.
[01:11:23.600 --> 01:11:24.960]   >> Where's that blue pill? Give me the blue pill.
[01:11:24.960 --> 01:11:29.120]   Our show today brought to you by Rocket Mortgage from Quick and Loans.
[01:11:30.160 --> 01:11:33.680]   >> Great mortgage lender. The best in the country. Just look at all those JD Power
[01:11:33.680 --> 01:11:38.480]   customer satisfaction awards year after year after year. And they put together a product for you.
[01:11:38.480 --> 01:11:43.520]   If you're buying a house or refying and you want to do it all online, this is amazing.
[01:11:43.520 --> 01:11:47.040]   Now, I know some people want to meet your mortgage lender. I don't know why.
[01:11:47.040 --> 01:11:50.480]   You want to go through the, you love the nostalgia of going through your old bank
[01:11:50.480 --> 01:11:55.280]   statements and pay stuffs and finding all the paperwork. Oh, remember that week. That was a
[01:11:55.280 --> 01:12:01.680]   great week. We really got paid that week. Be my guest. But if you would love to do this all online,
[01:12:01.680 --> 01:12:08.240]   submit all the paperwork with a touch of a finger and get it done in minutes instead of days,
[01:12:08.240 --> 01:12:12.480]   weeks, months. It took us a month the last time we bought a house, a month to get approved for the
[01:12:12.480 --> 01:12:18.000]   loan. And they kept coming back to us again and again saying, "Okay, do you have this? Do you have
[01:12:18.000 --> 01:12:23.520]   that? I wish Rocket Mortgage had been around then. Believe me, from now on Rocket Mortgage,
[01:12:23.520 --> 01:12:29.920]   go to quickenloans.com/twig. Do it all online. Friction free. It only takes minutes to get
[01:12:29.920 --> 01:12:35.440]   approved for a mortgage that's just right for you. You can even adjust the rate and length of your
[01:12:35.440 --> 01:12:41.680]   loan in real time. Quickenloans.com/twig for Rocket Mortgage Equal Housing Lender,
[01:12:41.680 --> 01:12:48.640]   licensed in all 50 states, nmlsconsumeraccess.org, number 30, 30. Skip the bank. Skip the waiting.
[01:12:48.640 --> 01:12:54.560]   Go completely online. Quickenloans.com/twig. I'm going to have a little bit of some delicious
[01:12:54.560 --> 01:13:04.880]   Pepsi Cola right now while Mike Elgin tells us about YouTube TV. I wish I were getting paid for this.
[01:13:04.880 --> 01:13:12.800]   They would make you slurp it. A little more slurp-elicious. I think you muted your mic there.
[01:13:14.480 --> 01:13:20.640]   There it is. Thank you. Yeah, so YouTube TV. YouTube TV was announced about a month ago.
[01:13:20.640 --> 01:13:26.960]   Nobody knew when it was going to launch or wear. It has launched today in five American cities.
[01:13:26.960 --> 01:13:32.800]   I think that everybody in the nation in the US can sign up, but you only get local programming
[01:13:32.800 --> 01:13:38.560]   from the area of one of those cities. New York Los Angeles, San Francisco Bay Area, Chicago,
[01:13:38.560 --> 01:13:44.400]   and Philadelphia. AMC is going to be on the network. It's kind of right now. It's kind of the sports
[01:13:44.400 --> 01:13:48.800]   and locals network with a little news thrown in. It's got all the news networks as well.
[01:13:48.800 --> 01:13:55.600]   Yes, but I believe that this is really, really, really bad news for the cable TV industry and a
[01:13:55.600 --> 01:13:59.520]   really, really big deal, much bigger than people realize. I'm going to tell you exactly why.
[01:13:59.520 --> 01:14:07.120]   Now, just a quick roundup of what it is. It's 40 channels for the basic... 35 bucks.
[01:14:07.120 --> 01:14:12.800]   Package. 35 bucks. That's five accounts, each of which has unlimited DVR. You can watch three of
[01:14:12.800 --> 01:14:19.360]   them simultaneously. You can watch it on a phone, on a tablet, on a TV with Chromecast, etc.
[01:14:19.360 --> 01:14:26.160]   It has ABC, CBS, NBC, Fox, the CW, ESPN, USA, Bravo, etc. etc. Disney Channel on and on.
[01:14:26.160 --> 01:14:30.480]   It does not have H.C. TV. No, but it has Oprah.
[01:14:30.480 --> 01:14:36.240]   What else you need? It does not have HBO, AMC, and TV.
[01:14:36.240 --> 01:14:40.160]   It has Bravo. It has... Yeah, you at least get desperate housewives.
[01:14:40.160 --> 01:14:42.960]   Yeah, and some cooking shows.
[01:14:42.960 --> 01:14:47.200]   It also has... It also has a DVR functionality.
[01:14:47.200 --> 01:14:49.280]   Unlimited storage for the DVR.
[01:14:49.280 --> 01:14:50.800]   I mean, this is how easy it is.
[01:14:50.800 --> 01:14:55.760]   Other accounts can have its own DVR so each of your kids can have their own unlimited DVR,
[01:14:55.760 --> 01:14:56.400]   which is pretty interesting.
[01:14:56.400 --> 01:14:59.360]   So let's say I am an Archer fan, which I am, and I want to record Archer.
[01:14:59.360 --> 01:15:05.200]   I could just click it to... Boom, I press the plus button. It will now record every Archer
[01:15:05.200 --> 01:15:10.800]   episode, and I will have that available to me to watch Time Shifted. Oh, this is for you, Stacy.
[01:15:10.800 --> 01:15:13.120]   Total Divas. You like that?
[01:15:13.120 --> 01:15:13.760]   Total Divas.
[01:15:13.760 --> 01:15:16.160]   Let's add... I don't know, but I think you're going to want to watch it.
[01:15:16.160 --> 01:15:20.560]   Add Total Divas to my DVR, the NCAA Championships, Modern...
[01:15:20.560 --> 01:15:23.360]   So this... And it's got live locals, right?
[01:15:23.360 --> 01:15:25.360]   This is amazing.
[01:15:25.360 --> 01:15:27.440]   Does it have live locals for sports?
[01:15:27.440 --> 01:15:28.000]   Yes.
[01:15:28.000 --> 01:15:32.640]   Because an MLB live, you know, your baseball games are blocked out if you're...
[01:15:32.640 --> 01:15:38.000]   Oh, am I? I don't know. That's a good question, because that is a little bit of a problem.
[01:15:38.000 --> 01:15:38.560]   That's annoying.
[01:15:38.560 --> 01:15:43.360]   Sometimes they promise that they've got Major League Baseball, but then you get blacked out.
[01:15:43.360 --> 01:15:48.880]   They're probably going to be some of that, but probably less than on other services.
[01:15:48.880 --> 01:15:52.240]   The sports coverage is really pretty good.
[01:15:52.240 --> 01:15:52.880]   This is a sports...
[01:15:52.880 --> 01:15:59.200]   ESPN, ESPN2, ESPNNU, the Big Ten networks, SEC Network. There's much better college coverage.
[01:15:59.200 --> 01:16:02.800]   And FOX Sports 1 and 2 and VC Sports.
[01:16:02.800 --> 01:16:05.440]   Here in the Bay Area, we've got the Bay Area networks,
[01:16:05.440 --> 01:16:10.320]   so we can watch Giants Baseball, Warriors Basketball, CBS Sports Network.
[01:16:10.320 --> 01:16:11.360]   It's even got golf.
[01:16:11.360 --> 01:16:15.520]   It's got a... You know, the kids stuff sprouts Disney.
[01:16:15.520 --> 01:16:17.760]   It's got...
[01:16:17.760 --> 01:16:19.040]   Finneas and Furb!
[01:16:19.040 --> 01:16:20.480]   Finneas and Furb, baby!
[01:16:20.480 --> 01:16:23.040]   So that's a good show.
[01:16:23.040 --> 01:16:28.880]   The thing that movies and TV shows are becoming more and more available
[01:16:28.880 --> 01:16:32.240]   on multiple places, a you name it, Hulu, etc.
[01:16:32.240 --> 01:16:36.880]   But what's becoming more rare and valuable is event television, including sports.
[01:16:36.880 --> 01:16:38.320]   So the Oscars, the Olympics, the...
[01:16:38.320 --> 01:16:40.960]   That's the missing piece from any court.
[01:16:40.960 --> 01:16:41.840]   That's the missing piece.
[01:16:41.840 --> 01:16:46.880]   This provides all of it.
[01:16:46.880 --> 01:16:52.000]   You know, and I can compare this to PlayStation, Vue, AT&T's Direct TV.
[01:16:52.000 --> 01:16:56.000]   There's four of these services now, but this is Google.
[01:16:56.000 --> 01:16:58.080]   The UI is spectacular.
[01:16:58.080 --> 01:16:59.280]   So here I am.
[01:16:59.280 --> 01:17:01.680]   This is my cable guide.
[01:17:01.680 --> 01:17:03.680]   These are the live channels I can see.
[01:17:03.680 --> 01:17:07.680]   And you said Finneas and Furb, so I hover over it, and now I'm watching it.
[01:17:07.680 --> 01:17:10.320]   It's actually playing as a preview.
[01:17:10.320 --> 01:17:12.080]   I get a...
[01:17:12.080 --> 01:17:13.280]   It says 17 minutes left.
[01:17:13.280 --> 01:17:15.040]   I get a great description of it.
[01:17:15.040 --> 01:17:16.320]   I can see if I want to watch it.
[01:17:16.320 --> 01:17:17.680]   I see what's next.
[01:17:17.680 --> 01:17:19.200]   I can DVR it from here.
[01:17:19.200 --> 01:17:20.160]   I can go full screen.
[01:17:20.160 --> 01:17:22.480]   This is how...
[01:17:22.480 --> 01:17:25.440]   You know, when Steve Jobs said I've looked TV,
[01:17:25.440 --> 01:17:28.000]   it's, you know, this is what I would have imagined Apple would have done.
[01:17:28.000 --> 01:17:28.480]   And Apple...
[01:17:28.480 --> 01:17:29.600]   This is what he would have looked.
[01:17:29.600 --> 01:17:31.600]   And Apple has not been able to put this together.
[01:17:31.600 --> 01:17:37.120]   You know, the thing that's powerful about this is that people under the age of 30 are
[01:17:37.120 --> 01:17:38.320]   obsessed with YouTube.
[01:17:38.320 --> 01:17:42.080]   But as they get older and start families and go to college and all that kind of stuff,
[01:17:42.080 --> 01:17:45.360]   they're going to want to watch more mainstream TV, but they love YouTube already.
[01:17:45.360 --> 01:17:50.720]   And so they're going to start watching their live TV and their cable TV on YouTube.
[01:17:50.720 --> 01:17:54.080]   This is very devastating to the cable providers.
[01:17:54.080 --> 01:18:00.400]   I think one of the things it's going to do is finally turn phones into a legitimate and
[01:18:00.400 --> 01:18:02.560]   widespread medium for live TV.
[01:18:02.560 --> 01:18:04.400]   Because this will work on my mobile device.
[01:18:04.400 --> 01:18:06.720]   Might even sell some iPads, right?
[01:18:06.720 --> 01:18:07.680]   It might.
[01:18:07.680 --> 01:18:10.640]   It'll work on all these mobile devices.
[01:18:10.640 --> 01:18:12.800]   The other thing that's really devastating to cable,
[01:18:12.800 --> 01:18:17.280]   and I think it's really underappreciated and underreported by all those fake news people,
[01:18:17.280 --> 01:18:23.360]   is the fact that their advertising abilities are through the roof.
[01:18:23.360 --> 01:18:29.840]   The advertising industry has been champing it a bit for years to get for Google and YouTube to
[01:18:29.840 --> 01:18:30.880]   launch exactly this.
[01:18:30.880 --> 01:18:34.640]   Because their data on the users is amazing.
[01:18:34.640 --> 01:18:39.440]   And they'll be able to very tightly target advertising, and YouTube would be able to charge
[01:18:39.440 --> 01:18:45.920]   a fortune for advertising because it'd be so tightly and accurately targeted at the demographics
[01:18:45.920 --> 01:18:46.880]   of the viewers.
[01:18:46.880 --> 01:18:49.760]   The premium channel is a little bit lacking.
[01:18:49.760 --> 01:18:50.640]   They have Showtime.
[01:18:50.640 --> 01:18:51.920]   They do not have HBO.
[01:18:51.920 --> 01:18:53.360]   You'd have to buy HBO now.
[01:18:53.360 --> 01:18:58.480]   I mean, at 35 bucks, this is the least expensive offering.
[01:18:58.480 --> 01:19:01.040]   The speed's PlayStation is the lowest offering at 39.
[01:19:01.040 --> 01:19:08.320]   So for NFL fans, you'll fall under blackouts on mobiles because Verizon has this.
[01:19:08.320 --> 01:19:10.800]   I'm still trying to figure out the blackout situation.
[01:19:10.800 --> 01:19:13.920]   It well, good luck because it's terrible.
[01:19:13.920 --> 01:19:14.720]   So look at this though.
[01:19:14.720 --> 01:19:18.480]   Here's Sports on now, and I'm actually getting live mini feeds of the Giants game.
[01:19:20.960 --> 01:19:23.840]   Athletics game, the NCAA championships.
[01:19:23.840 --> 01:19:29.440]   Here's ESPN soccer, and Total Divas is apparently a sports show.
[01:19:29.440 --> 01:19:32.400]   I don't know that's the case, but it's in there, right?
[01:19:32.400 --> 01:19:33.120]   And these are all.
[01:19:33.120 --> 01:19:34.160]   I'm not sure I'd like that show.
[01:19:34.160 --> 01:19:34.560]   Yeah.
[01:19:34.560 --> 01:19:35.920]   These are all what they call soccer.
[01:19:35.920 --> 01:19:37.120]   Yeah, Total Divas.
[01:19:37.120 --> 01:19:37.680]   Oh, that's right.
[01:19:37.680 --> 01:19:39.840]   It's the drop and flop.
[01:19:39.840 --> 01:19:46.960]   This is the UI that you hit and nailed it, by the way.
[01:19:46.960 --> 01:19:48.480]   Oh my God.
[01:19:48.480 --> 01:19:50.240]   Total Divas is a sports.
[01:19:50.880 --> 01:19:51.440]   Show.
[01:19:51.440 --> 01:19:52.160]   I had to look it up.
[01:19:52.160 --> 01:19:52.560]   Sorry.
[01:19:52.560 --> 01:19:56.720]   It is a docu-series following the top female WWE superstars.
[01:19:56.720 --> 01:19:58.080]   Oh, but it's wrestling.
[01:19:58.080 --> 01:19:59.680]   So it's kind of got a little bit of both.
[01:19:59.680 --> 01:20:00.560]   Yeah.
[01:20:00.560 --> 01:20:03.040]   It's like reality TV on reality TV.
[01:20:03.040 --> 01:20:05.200]   This would not interest me.
[01:20:05.200 --> 01:20:07.680]   The title of this episode, season six, episode seven,
[01:20:07.680 --> 01:20:10.240]   a win-win situation.
[01:20:10.240 --> 01:20:12.240]   So you be the judge.
[01:20:12.240 --> 01:20:14.720]   Now, I have to say for the YouTube generation,
[01:20:14.720 --> 01:20:18.000]   YouTube's mixed right in, including the YouTube originals, right?
[01:20:18.560 --> 01:20:21.680]   Shows on YouTube, YouTube videos.
[01:20:21.680 --> 01:20:26.080]   I could see my son, my 22-year-old son,
[01:20:26.080 --> 01:20:28.240]   who does not watch TV, he watches YouTube.
[01:20:28.240 --> 01:20:30.560]   This is what he wants.
[01:20:30.560 --> 01:20:33.760]   And the beauty of it is that that family plan,
[01:20:33.760 --> 01:20:35.360]   the kids are going to be nagging the parents
[01:20:35.360 --> 01:20:36.560]   to get the family plan.
[01:20:36.560 --> 01:20:39.360]   The $35 a month covers five people.
[01:20:39.360 --> 01:20:42.560]   So each of the kids gets their own separate account,
[01:20:42.560 --> 01:20:43.440]   their own DVR.
[01:20:43.440 --> 01:20:45.680]   This is big news.
[01:20:45.680 --> 01:20:47.280]   This is going to be really popular, I think.
[01:20:47.280 --> 01:20:51.120]   Now, what is the, so here's the big issue I see,
[01:20:51.120 --> 01:20:54.800]   is this is a lean forward experience on mobile,
[01:20:54.800 --> 01:20:55.840]   and even on computer.
[01:20:55.840 --> 01:20:57.840]   What is the lean back experience?
[01:20:57.840 --> 01:21:00.000]   Do I watch it on Chromecast, Android TV?
[01:21:00.000 --> 01:21:01.600]   I guess Android TV, right?
[01:21:01.600 --> 01:21:05.040]   Yes, and Chromecast, and Google said that they're going to be
[01:21:05.040 --> 01:21:07.600]   making announcements this year of additional TVs.
[01:21:07.600 --> 01:21:09.280]   So I have a shield in the home.
[01:21:09.280 --> 01:21:11.520]   This will probably be now on my shield,
[01:21:11.520 --> 01:21:12.560]   because I have YouTube on there.
[01:21:12.560 --> 01:21:14.080]   Wow.
[01:21:15.120 --> 01:21:18.720]   But I do think that the subtle play here is a behavior one,
[01:21:18.720 --> 01:21:22.240]   and I do think it's going to turn live TV and event TV
[01:21:22.240 --> 01:21:24.000]   into a lean back experience.
[01:21:24.000 --> 01:21:25.680]   Getting the live locals is a big deal.
[01:21:25.680 --> 01:21:29.280]   Now, I'm in the San Francisco area of dominant influence,
[01:21:29.280 --> 01:21:32.080]   but we are 50 miles north of San Francisco.
[01:21:32.080 --> 01:21:35.360]   We can't, over the air, get any San Francisco stations.
[01:21:35.360 --> 01:21:38.640]   So, you know, I hear it up here in Petaluma,
[01:21:38.640 --> 01:21:40.480]   but I am getting the locals.
[01:21:40.480 --> 01:21:42.160]   I'm in the San Francisco Bay area,
[01:21:42.160 --> 01:21:43.600]   so that's really interesting.
[01:21:43.600 --> 01:21:44.240]   That's awesome.
[01:21:44.240 --> 01:21:46.160]   This could replace cable for me.
[01:21:46.160 --> 01:21:47.840]   This is fantastic.
[01:21:47.840 --> 01:21:51.520]   The Bay area coverage seems to be much broader than the other
[01:21:51.520 --> 01:21:52.160]   MetroCovere--
[01:21:52.160 --> 01:21:53.360]   Yeah, they call it Bay Area.
[01:21:53.360 --> 01:21:53.920]   Yeah.
[01:21:53.920 --> 01:21:55.920]   Wow.
[01:21:55.920 --> 01:22:00.560]   This is just in the nick of time for baseball season to start.
[01:22:00.560 --> 01:22:02.960]   You don't have MLB live?
[01:22:02.960 --> 01:22:04.960]   Not anymore.
[01:22:04.960 --> 01:22:05.600]   It's expensive.
[01:22:05.600 --> 01:22:07.040]   Not anymore.
[01:22:07.040 --> 01:22:08.960]   It's $149 for the season.
[01:22:08.960 --> 01:22:11.440]   This is $35 a month.
[01:22:11.440 --> 01:22:12.720]   I guess it's the same.
[01:22:12.720 --> 01:22:14.880]   But I get a lot of other stuff.
[01:22:14.880 --> 01:22:16.000]   It sounds cheaper, though, doesn't it?
[01:22:16.000 --> 01:22:17.360]   You've been calling baseball seasoners.
[01:22:17.360 --> 01:22:19.040]   Sounds, sounds, sounds cheaper, though.
[01:22:19.040 --> 01:22:20.080]   They should have a weekly rate.
[01:22:20.080 --> 01:22:20.880]   That would be really cheap.
[01:22:20.880 --> 01:22:22.080]   Wow.
[01:22:22.080 --> 01:22:25.760]   And I have to say, I played with--
[01:22:25.760 --> 01:22:27.920]   for iOS today, we did a review.
[01:22:27.920 --> 01:22:31.200]   I did a review of all of the streaming packages,
[01:22:31.200 --> 01:22:35.520]   PlayStation View, which I think was, until now, the best AT&T
[01:22:35.520 --> 01:22:37.600]   Direct TV, which did not work very well.
[01:22:37.600 --> 01:22:40.000]   A lot of people continue to complain that it's not working
[01:22:40.000 --> 01:22:40.480]   very well.
[01:22:40.480 --> 01:22:41.280]   That it's not playing.
[01:22:41.280 --> 01:22:41.920]   They're bugs.
[01:22:41.920 --> 01:22:43.360]   Who else?
[01:22:43.360 --> 01:22:46.720]   Slingbox, Sling TV.
[01:22:46.720 --> 01:22:49.760]   What are some of the other--
[01:22:49.760 --> 01:22:53.840]   I think that's the big three right there, Sling TV, View, and AT&T.
[01:22:53.840 --> 01:22:56.800]   This one beats them up, down, in sideways.
[01:22:56.800 --> 01:22:59.760]   In every respect, plus it's cheaper.
[01:22:59.760 --> 01:23:00.880]   Yeah.
[01:23:00.880 --> 01:23:03.120]   Here's another subtle benefit.
[01:23:03.120 --> 01:23:04.240]   You can pause it.
[01:23:04.240 --> 01:23:06.960]   So if you're going to be out of the country for a month or two,
[01:23:06.960 --> 01:23:09.120]   you can not pay during those months.
[01:23:09.120 --> 01:23:14.240]   And if nobody on your family account logs in for three months,
[01:23:14.240 --> 01:23:15.520]   they automatically pause it.
[01:23:15.520 --> 01:23:16.080]   What?
[01:23:16.080 --> 01:23:16.560]   It's not charging.
[01:23:16.560 --> 01:23:17.280]   That's nice.
[01:23:17.280 --> 01:23:20.000]   That is very anti-cable company.
[01:23:20.000 --> 01:23:20.640]   Yeah.
[01:23:20.640 --> 01:23:21.120]   Yeah.
[01:23:21.120 --> 01:23:24.160]   Well, that's where competition really-- we really benefit from that.
[01:23:24.160 --> 01:23:25.600]   Yep.
[01:23:25.600 --> 01:23:26.960]   And so here's some movies.
[01:23:26.960 --> 01:23:28.480]   Let's say I did want to see the Matrix.
[01:23:28.480 --> 01:23:30.000]   They know their geek audience.
[01:23:30.000 --> 01:23:30.560]   I click on it.
[01:23:30.560 --> 01:23:32.000]   It's going to be on Sci-Fi.
[01:23:32.000 --> 01:23:34.560]   I just pressed the plus button and now it's DVR'd.
[01:23:34.560 --> 01:23:35.040]   That's it.
[01:23:35.040 --> 01:23:35.840]   It's simple as that.
[01:23:35.840 --> 01:23:36.000]   Wow.
[01:23:36.000 --> 01:23:38.400]   Wow.
[01:23:38.400 --> 01:23:38.960]   That's right.
[01:23:38.960 --> 01:23:42.080]   Now, I'd be interested if I'll be able to skip commercials.
[01:23:42.080 --> 01:23:44.000]   Then they also have--
[01:23:44.000 --> 01:23:44.880]   Yeah, that was not known.
[01:23:44.880 --> 01:23:45.360]   Yeah.
[01:23:45.360 --> 01:23:46.560]   Related on YouTube.
[01:23:46.560 --> 01:23:48.480]   And here's all the YouTube content.
[01:23:48.480 --> 01:23:50.640]   So this is a great cross promotion for YouTube.
[01:23:50.640 --> 01:23:53.440]   And then there's more information about the cast.
[01:23:53.440 --> 01:23:58.480]   This is exactly what the online consuming TV audience
[01:23:58.480 --> 01:24:01.040]   is now kind of expecting, right?
[01:24:01.040 --> 01:24:02.000]   Now, Leo, imagine--
[01:24:02.000 --> 01:24:03.920]   Unless it has a lot of advertising.
[01:24:03.920 --> 01:24:04.240]   Right.
[01:24:04.240 --> 01:24:05.040]   But it doesn't seem to--
[01:24:05.040 --> 01:24:07.280]   Imagine if they bought Twitter and integrated Twitter
[01:24:07.280 --> 01:24:08.240]   into that experience.
[01:24:08.240 --> 01:24:09.120]   Ooh.
[01:24:09.120 --> 01:24:10.080]   How great that would be.
[01:24:10.080 --> 01:24:12.880]   Oh, that's what's missing from this is a kind of real time
[01:24:12.880 --> 01:24:14.720]   interactivity and outrage.
[01:24:14.720 --> 01:24:18.480]   How dare they put Lauren Lakes--
[01:24:18.480 --> 01:24:19.360]   Hey, TV chef.
[01:24:19.360 --> 01:24:22.400]   How dare they put Lauren Lakes paternity court on at 4.30?
[01:24:22.400 --> 01:24:25.360]   Oh, time doesn't matter.
[01:24:25.360 --> 01:24:26.160]   [LAUGHTER]
[01:24:26.160 --> 01:24:26.880]   Not anymore.
[01:24:26.880 --> 01:24:30.400]   I still remember my daughter saying to me,
[01:24:30.400 --> 01:24:32.480]   probably when she was like six or seven,
[01:24:32.480 --> 01:24:33.840]   she's like, "I really--"
[01:24:33.840 --> 01:24:35.280]   'Cause we've been court cutters forever.
[01:24:36.000 --> 01:24:38.560]   I feel really sorry for my friends who have to watch
[01:24:38.560 --> 01:24:40.320]   television shows when they're on.
[01:24:40.320 --> 01:24:41.200]   [LAUGHTER]
[01:24:41.200 --> 01:24:44.960]   I was like, "What are you talking about?
[01:24:44.960 --> 01:24:45.760]   Also, wow."
[01:24:45.760 --> 01:24:48.480]   So you get a first month's free,
[01:24:48.480 --> 01:24:50.400]   so I'm on the free trial.
[01:24:50.400 --> 01:24:52.000]   By the way, most of these other services--
[01:24:52.000 --> 01:24:54.320]   In fact, all of the other services have one week free trial.
[01:24:54.320 --> 01:24:55.920]   So in every respect,
[01:24:55.920 --> 01:24:58.640]   YouTube has clearly paid attention to what's going on out there.
[01:24:58.640 --> 01:25:00.960]   Apple really wanted to launch this, by the way.
[01:25:00.960 --> 01:25:04.880]   But even if Apple does launch exactly the same service,
[01:25:04.880 --> 01:25:06.720]   it'll be limited at Apple TV.
[01:25:06.720 --> 01:25:08.000]   They don't have YouTube.
[01:25:08.000 --> 01:25:08.480]   And they don't have--
[01:25:08.480 --> 01:25:09.120]   Oh, wait.
[01:25:09.120 --> 01:25:11.120]   I could do this instead of Hulu.
[01:25:11.120 --> 01:25:12.560]   Sorry, my brain just clicked on.
[01:25:12.560 --> 01:25:14.080]   Although it's kind of more expensive compared--
[01:25:14.080 --> 01:25:15.920]   'Cause I use Hulu as my network TV,
[01:25:15.920 --> 01:25:19.840]   but I get so irritated because they window things
[01:25:19.840 --> 01:25:21.280]   that if you don't catch it.
[01:25:21.280 --> 01:25:22.960]   I don't know, see, it's not in Austin yet.
[01:25:22.960 --> 01:25:28.000]   So New York, LA, San Francisco, Chicago, and Philadelphia.
[01:25:28.000 --> 01:25:29.280]   We don't get that.
[01:25:29.280 --> 01:25:30.000]   Or Google 5.
[01:25:30.000 --> 01:25:31.040]   They're going to have AMC,
[01:25:31.040 --> 01:25:33.680]   which is-- that's the Walking Dead crowd.
[01:25:34.640 --> 01:25:37.680]   But again, Stacy, I'm pretty sure you can just get the Chicago one.
[01:25:37.680 --> 01:25:37.920]   Really?
[01:25:37.920 --> 01:25:39.360]   Is that the case?
[01:25:39.360 --> 01:25:39.920]   Pretty sure.
[01:25:39.920 --> 01:25:40.960]   You don't have to be in that area.
[01:25:40.960 --> 01:25:42.880]   Why don't you try that right now, Stacy?
[01:25:42.880 --> 01:25:44.640]   tv.youtube.com.
[01:25:44.640 --> 01:25:48.240]   tv.youtube.com.
[01:25:48.240 --> 01:25:50.960]   And--
[01:25:50.960 --> 01:25:52.880]   Hulu is working on its own.
[01:25:52.880 --> 01:25:54.560]   These are called skinny bundles.
[01:25:54.560 --> 01:25:56.720]   Terrible name.
[01:25:56.720 --> 01:25:58.320]   Try one month free.
[01:25:58.320 --> 01:26:01.600]   Oh my god, but I've got to pick one of my eight Google accounts.
[01:26:01.600 --> 01:26:02.800]   I guess my personal one.
[01:26:04.080 --> 01:26:05.200]   It's one month free.
[01:26:05.200 --> 01:26:06.960]   Just remember to cancel it in a month.
[01:26:06.960 --> 01:26:08.640]   That's what I did as I set reminders
[01:26:08.640 --> 01:26:10.880]   to cancel all of those other services,
[01:26:10.880 --> 01:26:11.920]   because I did not want to have it.
[01:26:11.920 --> 01:26:13.040]   Oh wait, hold on.
[01:26:13.040 --> 01:26:14.240]   Now I've picked my account.
[01:26:14.240 --> 01:26:16.080]   It's asking me about my location.
[01:26:16.080 --> 01:26:16.400]   Okay.
[01:26:16.400 --> 01:26:21.280]   They guessed my location wrong,
[01:26:21.280 --> 01:26:23.440]   but it is not available in my area.
[01:26:23.440 --> 01:26:23.680]   Yeah.
[01:26:23.680 --> 01:26:25.200]   So they're not letting you do it.
[01:26:25.200 --> 01:26:27.200]   So what if you have to be in those bedrooms?
[01:26:27.200 --> 01:26:27.760]   Chicago.
[01:26:27.760 --> 01:26:30.800]   Give me a zip code for Chicago.
[01:26:31.840 --> 01:26:33.680]   What is the Spiegel catalog, right?
[01:26:33.680 --> 01:26:35.040]   60601?
[01:26:35.040 --> 01:26:37.280]   6060.
[01:26:37.280 --> 01:26:38.320]   I'm not even speaking of it.
[01:26:38.320 --> 01:26:39.120]   I'm not even speaking of it again.
[01:26:39.120 --> 01:26:39.120]   I'm not even speaking of it again.
[01:26:39.120 --> 01:26:39.680]   I'm not even speaking of it again.
[01:26:39.680 --> 01:26:40.480]   I'm not even speaking of it again.
[01:26:40.480 --> 01:26:41.280]   I'm not even speaking of it again.
[01:26:41.280 --> 01:26:43.760]   Oh, Chicago area.
[01:26:43.760 --> 01:26:45.360]   YouTube TV is available in my area,
[01:26:45.360 --> 01:26:48.160]   but unfortunately you're going to have to wait until you're home to sign up.
[01:26:48.160 --> 01:26:51.360]   But--
[01:26:51.360 --> 01:26:52.160]   So what if you use--
[01:26:52.160 --> 01:26:53.360]   I just pull you Google.
[01:26:53.360 --> 01:26:53.600]   VPN.
[01:26:53.600 --> 01:26:56.240]   Yes, I could--
[01:26:56.240 --> 01:26:58.720]   VPN, but that's not going to be satisfactory.
[01:26:58.720 --> 01:27:01.520]   The question really is how soon do they roll this out nationwide?
[01:27:02.000 --> 01:27:02.960]   And will they?
[01:27:02.960 --> 01:27:06.320]   I suspect that they have to make these negotiations with the locals
[01:27:06.320 --> 01:27:07.520]   for in each market, right?
[01:27:07.520 --> 01:27:07.920]   Or no.
[01:27:07.920 --> 01:27:13.920]   You can't go to CBS and say, hey, can we run KPIX, our local CBS station?
[01:27:13.920 --> 01:27:16.320]   Let's see what the quality is.
[01:27:16.320 --> 01:27:19.200]   It's starting off not great.
[01:27:19.200 --> 01:27:20.320]   And remember, we have a--
[01:27:20.320 --> 01:27:21.040]   Oh, but I can--
[01:27:21.040 --> 01:27:23.280]   the auto 360.
[01:27:23.280 --> 01:27:24.800]   I can go up to 480p.
[01:27:24.800 --> 01:27:25.920]   I don't see HD.
[01:27:25.920 --> 01:27:28.320]   That might be another issue.
[01:27:28.320 --> 01:27:30.560]   So Judge Judy doesn't look as good as she auto.
[01:27:31.360 --> 01:27:33.840]   Is that Judge Judy?
[01:27:33.840 --> 01:27:35.680]   No, I don't know who it is.
[01:27:35.680 --> 01:27:36.880]   You're asking the wrong guy.
[01:27:36.880 --> 01:27:38.000]   Judge Judy said she's got--
[01:27:38.000 --> 01:27:39.200]   she's older and has shorter--
[01:27:39.200 --> 01:27:39.840]   who is that?
[01:27:39.840 --> 01:27:40.960]   That's the Beatles court.
[01:27:40.960 --> 01:27:42.720]   This is the Wopner replacement.
[01:27:42.720 --> 01:27:45.040]   Time for Wopner.
[01:27:45.040 --> 01:27:47.120]   Judge Judy's coming up.
[01:27:47.120 --> 01:27:48.800]   Oh, OK.
[01:27:48.800 --> 01:27:50.240]   Yeah, that's Judge Judy.
[01:27:50.240 --> 01:27:51.760]   That's Judge Judy.
[01:27:51.760 --> 01:27:52.480]   I got fooled.
[01:27:52.480 --> 01:27:53.920]   What do you think?
[01:27:53.920 --> 01:27:55.120]   Close captioning?
[01:27:55.120 --> 01:27:57.760]   When you're already in on--
[01:27:57.760 --> 01:28:00.560]   you're over your head in just--
[01:28:00.560 --> 01:28:02.000]   Audio track primary.
[01:28:02.000 --> 01:28:02.800]   I guess I can do--
[01:28:02.800 --> 01:28:03.680]   Well, it's like you're in that place.
[01:28:03.680 --> 01:28:05.120]   Let's see if that sounds in Spanish.
[01:28:05.120 --> 01:28:06.560]   OK, so then he--
[01:28:06.560 --> 01:28:07.360]   Subtitles.
[01:28:07.360 --> 01:28:08.880]   He called me.
[01:28:08.880 --> 01:28:10.560]   He said he was unhappy.
[01:28:10.560 --> 01:28:11.520]   He wanted to come back.
[01:28:11.520 --> 01:28:12.560]   Oh, watch.
[01:28:12.560 --> 01:28:13.680]   This is the way to watch it.
[01:28:13.680 --> 01:28:15.120]   I can watch it at double speed.
[01:28:15.120 --> 01:28:16.160]   [LAUGHTER]
[01:28:16.160 --> 01:28:17.120]   [INAUDIBLE]
[01:28:17.120 --> 01:28:19.200]   Oh my god, that would be awesome
[01:28:19.200 --> 01:28:20.800]   for all those reality videos.
[01:28:20.800 --> 01:28:22.960]   We play everything a million times.
[01:28:22.960 --> 01:28:23.680]   So I'll come back.
[01:28:23.680 --> 01:28:26.000]   I wish they'd make the voices twice as high.
[01:28:26.000 --> 01:28:28.320]   And then his behavior just became a radic-pap.
[01:28:28.320 --> 01:28:28.960]   He was as close.
[01:28:28.960 --> 01:28:30.560]   He would sit up, start talking, rambling.
[01:28:30.560 --> 01:28:32.560]   At one point, he took a pack of pros and meat.
[01:28:32.560 --> 01:28:33.360]   That's--
[01:28:33.360 --> 01:28:35.120]   Why are you acting like this?
[01:28:35.120 --> 01:28:36.480]   Why are you acting like this?
[01:28:36.480 --> 01:28:37.680]   Why are you doing this to me?
[01:28:37.680 --> 01:28:40.000]   That's really interesting.
[01:28:40.000 --> 01:28:41.360]   YouTube TV.
[01:28:41.360 --> 01:28:43.040]   Wow, I think this might be a--
[01:28:43.040 --> 01:28:45.680]   and I have to say, I've seen Google do some stupid things,
[01:28:45.680 --> 01:28:47.440]   but this is a killer product.
[01:28:47.440 --> 01:28:48.560]   Yep.
[01:28:48.560 --> 01:28:48.880]   Yep.
[01:28:48.880 --> 01:28:52.400]   By the way, you can get a Chromecast if you sign up, right?
[01:28:54.080 --> 01:28:56.400]   The $70 one or the $35 non-ish?
[01:28:56.400 --> 01:28:56.960]   Oh, I don't know.
[01:28:56.960 --> 01:29:00.240]   I mean, they're given those $35 ones.
[01:29:00.240 --> 01:29:00.800]   I'm not going to lie.
[01:29:00.800 --> 01:29:01.040]   Yeah.
[01:29:01.040 --> 01:29:02.880]   Fairly interesting.
[01:29:02.880 --> 01:29:04.000]   Do you know what else they're giving away?
[01:29:04.000 --> 01:29:05.520]   What?
[01:29:05.520 --> 01:29:07.760]   A Google Home if you buy an LG--
[01:29:07.760 --> 01:29:08.720]   is it six?
[01:29:08.720 --> 01:29:09.040]   Really?
[01:29:09.040 --> 01:29:10.560]   Yeah, let's go.
[01:29:10.560 --> 01:29:11.120]   The LG--
[01:29:11.120 --> 01:29:12.160]   Yeah, the LG--
[01:29:12.160 --> 01:29:12.640]   Those are the--
[01:29:12.640 --> 01:29:15.200]   I have-- that's what I have is a B6,
[01:29:15.200 --> 01:29:16.240]   but they also make a--
[01:29:16.240 --> 01:29:17.120]   I think a G6.
[01:29:17.120 --> 01:29:20.640]   These are the 4K LG TVs.
[01:29:20.640 --> 01:29:22.800]   They're really nice, by the way.
[01:29:22.800 --> 01:29:26.000]   Can I say completely off the subject?
[01:29:26.000 --> 01:29:29.280]   But if you do get a 4K UHD TV
[01:29:29.280 --> 01:29:31.840]   and you get a 4K UHD Blu-ray player,
[01:29:31.840 --> 01:29:36.000]   you need both, you must get BBC's Planet Earth series,
[01:29:36.000 --> 01:29:36.480]   the new one.
[01:29:36.480 --> 01:29:38.400]   They reshot the whole thing in 4K.
[01:29:38.400 --> 01:29:40.400]   You're all invited to my house.
[01:29:40.400 --> 01:29:42.480]   It's incredible.
[01:29:42.480 --> 01:29:44.160]   Thank God.
[01:29:44.160 --> 01:29:46.560]   I don't-- I never have to leave the house again.
[01:29:46.560 --> 01:29:48.320]   I don't care about the visa situation.
[01:29:48.320 --> 01:29:49.680]   I could just stay in my house.
[01:29:49.680 --> 01:29:50.960]   It's--
[01:29:50.960 --> 01:29:52.640]   No, you have to come to the studio.
[01:29:52.640 --> 01:29:54.000]   It's beautiful.
[01:29:54.000 --> 01:29:54.880]   Oh, yeah, I come here.
[01:29:54.880 --> 01:29:55.760]   That's OK.
[01:29:55.760 --> 01:29:58.160]   So far, there's no border patrol between here in my house.
[01:29:58.160 --> 01:30:02.960]   They're not going to get that extreme vetting thing through, are they?
[01:30:02.960 --> 01:30:04.720]   Wait, OK.
[01:30:04.720 --> 01:30:06.560]   Hey, let's talk about something that's not political.
[01:30:06.560 --> 01:30:07.360]   OK, thank you.
[01:30:07.360 --> 01:30:08.800]   You're going to depress me.
[01:30:08.800 --> 01:30:09.920]   I can't handle this.
[01:30:09.920 --> 01:30:10.480]   Yeah, I know.
[01:30:10.480 --> 01:30:11.040]   I understand.
[01:30:11.040 --> 01:30:11.680]   I understand.
[01:30:11.680 --> 01:30:14.240]   But really, I don't think--
[01:30:14.240 --> 01:30:15.600]   I would never come visit the US
[01:30:15.600 --> 01:30:17.840]   if I had to give them my social media passwords.
[01:30:17.840 --> 01:30:20.480]   Your social media password.
[01:30:20.480 --> 01:30:22.560]   You have to give them your Facebook
[01:30:22.560 --> 01:30:24.320]   and Twitter password if you're visiting--
[01:30:24.320 --> 01:30:26.640]   if you're a Frenchman visiting--
[01:30:26.640 --> 01:30:27.200]   Now, they're not going--
[01:30:27.200 --> 01:30:28.560]   there's no way this gets through,
[01:30:28.560 --> 01:30:30.080]   but this is what the White House is proposing.
[01:30:30.080 --> 01:30:31.200]   I'm sorry.
[01:30:31.200 --> 01:30:32.080]   I'm sorry, Stacey.
[01:30:32.080 --> 01:30:32.960]   But if you're a Frenchman--
[01:30:32.960 --> 01:30:33.600]   Why do you think you want to get through?
[01:30:33.600 --> 01:30:35.760]   Because it's insane.
[01:30:35.760 --> 01:30:38.480]   No one will ever visit the US again.
[01:30:38.480 --> 01:30:42.160]   All every academic conference will have to be canceled.
[01:30:42.160 --> 01:30:43.120]   It's called the Trump sump.
[01:30:43.120 --> 01:30:44.080]   They're looking--
[01:30:44.080 --> 01:30:47.840]   facing billions of dollars in lost revenue in the hospitality industry
[01:30:47.840 --> 01:30:49.840]   because people are canceling their vacations to the US.
[01:30:49.840 --> 01:30:51.360]   Imagine now if you were coming--
[01:30:51.360 --> 01:30:53.280]   and it's not from Muslim countries.
[01:30:53.280 --> 01:30:54.480]   If you were coming from France,
[01:30:54.480 --> 01:30:58.000]   an ally nation in the UK,
[01:30:58.000 --> 01:30:59.200]   in the United States,
[01:30:59.200 --> 01:31:00.480]   the border patrol can say,
[01:31:00.480 --> 01:31:02.960]   "Give me your social media passwords.
[01:31:02.960 --> 01:31:06.080]   I want to see your context list to see what you've talked to."
[01:31:06.080 --> 01:31:07.280]   And by the way,
[01:31:07.280 --> 01:31:08.400]   tell me about your politics.
[01:31:08.400 --> 01:31:10.000]   Do you like President Trump?
[01:31:10.000 --> 01:31:11.440]   And for any reason at all,
[01:31:11.440 --> 01:31:12.960]   at any point you say no,
[01:31:12.960 --> 01:31:14.480]   bye-bye, you're going home.
[01:31:14.480 --> 01:31:17.920]   And other countries will copy and retaliate.
[01:31:17.920 --> 01:31:20.480]   That's the worst thing I'm worried about traveling.
[01:31:20.480 --> 01:31:21.040]   Exactly.
[01:31:21.040 --> 01:31:23.920]   So again, the best defense is this information.
[01:31:23.920 --> 01:31:25.680]   I have a second Twitter account,
[01:31:25.680 --> 01:31:27.440]   a second Facebook account that's really--
[01:31:27.440 --> 01:31:28.880]   No, don't lie to these guys,
[01:31:28.880 --> 01:31:30.160]   and they could throw you in jail.
[01:31:30.160 --> 01:31:33.600]   I think we're--
[01:31:33.600 --> 01:31:34.080]   Oh my god.
[01:31:34.080 --> 01:31:35.120]   Anyway, don't get me started.
[01:31:35.120 --> 01:31:36.320]   You're right, Stacey.
[01:31:36.320 --> 01:31:37.440]   Sorry about that.
[01:31:37.440 --> 01:31:39.680]   I'm also concerned because I'm going to be fine to Morocco.
[01:31:39.680 --> 01:31:41.840]   I'm going to be trapped in one of the countries on the list where--
[01:31:41.840 --> 01:31:42.800]   You can't--
[01:31:42.800 --> 01:31:44.160]   on certain airlines out of Morocco,
[01:31:44.160 --> 01:31:45.360]   you can't have a laptop.
[01:31:45.360 --> 01:31:47.040]   So I'm going to check my--
[01:31:47.040 --> 01:31:47.760]   Morocco?
[01:31:47.760 --> 01:31:48.960]   Morocco's on that list.
[01:31:48.960 --> 01:31:49.680]   Yeah.
[01:31:49.680 --> 01:31:50.000]   Wow.
[01:31:50.960 --> 01:31:52.880]   But not if you fly American Airlines,
[01:31:52.880 --> 01:31:55.040]   if you fly United Delta, American.
[01:31:55.040 --> 01:31:55.920]   Yeah.
[01:31:55.920 --> 01:31:56.720]   Right, go ahead.
[01:31:56.720 --> 01:31:58.400]   Because you're a good person.
[01:31:58.400 --> 01:32:00.240]   So you can have a laptop.
[01:32:00.240 --> 01:32:03.200]   But again, I think these things are--
[01:32:03.200 --> 01:32:06.640]   they spread globally to other countries, other airlines.
[01:32:06.640 --> 01:32:07.200]   Yeah.
[01:32:07.200 --> 01:32:10.160]   And so I just don't like the whole trend of passwords
[01:32:10.160 --> 01:32:11.360]   and no laptops on the--
[01:32:11.360 --> 01:32:12.000]   it's all bad.
[01:32:12.000 --> 01:32:16.880]   Okay, we'll talk about this after the show.
[01:32:16.880 --> 01:32:19.680]   Stacey, seriously.
[01:32:19.680 --> 01:32:22.800]   Because I have a theory about the whole thing that is very depressing.
[01:32:22.800 --> 01:32:23.520]   Stacey, you're--
[01:32:23.520 --> 01:32:24.560]   [LAUGHTER]
[01:32:24.560 --> 01:32:27.040]   No, no, no, if you guys can keep--
[01:32:27.040 --> 01:32:27.520]   [LAUGHTER]
[01:32:27.520 --> 01:32:30.800]   I'm just going to go and cry into my beer.
[01:32:30.800 --> 01:32:32.480]   [LAUGHTER]
[01:32:32.480 --> 01:32:34.080]   I'm just going to go watch TV with Leo.
[01:32:34.080 --> 01:32:35.040]   [LAUGHTER]
[01:32:35.040 --> 01:32:35.760]   I have a great TV.
[01:32:35.760 --> 01:32:35.760]   For me to--
[01:32:35.760 --> 01:32:36.240]   For great TV.
[01:32:36.240 --> 01:32:37.440]   Come to my house.
[01:32:37.440 --> 01:32:40.000]   We can learn about the world by watching TV.
[01:32:40.000 --> 01:32:41.200]   That's what they do in North Korea.
[01:32:41.200 --> 01:32:43.680]   Speaking of TV.
[01:32:43.680 --> 01:32:47.040]   Because Google added--
[01:32:47.760 --> 01:32:50.560]   well, they added a bunch of partners last week to Google Home.
[01:32:50.560 --> 01:32:54.240]   Yay, including Wink, August, LG, not LG.
[01:32:54.240 --> 01:32:54.640]   I'm sorry.
[01:32:54.640 --> 01:32:57.120]   Best Pie insignia, brand stuff.
[01:32:57.120 --> 01:33:00.400]   There's another big one that I was excited about.
[01:33:00.400 --> 01:33:01.200]   Oh, TP Link.
[01:33:01.200 --> 01:33:05.440]   But they also added Logitech,
[01:33:05.440 --> 01:33:08.400]   which means, Leo, your Logitech Harmony Home Hub.
[01:33:08.400 --> 01:33:13.440]   Now, you can integrate it with your Google Home.
[01:33:13.440 --> 01:33:16.560]   I gave my Google Home to my daughter.
[01:33:16.560 --> 01:33:17.440]   I need it back.
[01:33:17.440 --> 01:33:18.240]   Abby.
[01:33:18.240 --> 01:33:18.720]   Yeah.
[01:33:18.720 --> 01:33:19.600]   [LAUGHTER]
[01:33:19.600 --> 01:33:20.240]   You know what, though?
[01:33:20.240 --> 01:33:21.200]   You have to do one of those songs.
[01:33:21.200 --> 01:33:22.240]   I just want to point out,
[01:33:22.240 --> 01:33:24.000]   Abby doesn't have a stereo system,
[01:33:24.000 --> 01:33:26.720]   but the speaker is pretty good on the Google Home.
[01:33:26.720 --> 01:33:30.800]   And I said, you can ask for music because it's on my account,
[01:33:30.800 --> 01:33:32.880]   so you can play anything on my Google Play music.
[01:33:32.880 --> 01:33:33.920]   You can listen to podcasts.
[01:33:33.920 --> 01:33:35.120]   You can listen to news.
[01:33:35.120 --> 01:33:36.240]   It's her stereo system.
[01:33:36.240 --> 01:33:37.040]   She loves it.
[01:33:37.040 --> 01:33:37.760]   Yeah.
[01:33:37.760 --> 01:33:40.560]   It's a really great device for that.
[01:33:40.560 --> 01:33:42.560]   So I'm actually trading--
[01:33:42.560 --> 01:33:44.240]   So we're moving the Echo,
[01:33:44.240 --> 01:33:45.520]   which sits in my living room.
[01:33:46.320 --> 01:33:48.960]   Kitchen-- I have an open kitchen dining living
[01:33:48.960 --> 01:33:50.480]   just massive space downstairs.
[01:33:50.480 --> 01:33:52.720]   That's where our Echo lives.
[01:33:52.720 --> 01:33:54.720]   We're switching it out with the Google Home now that
[01:33:54.720 --> 01:33:56.560]   the Google Home has all these integrations,
[01:33:56.560 --> 01:33:59.440]   and we're moving the Echo upstairs to my daughter's room.
[01:33:59.440 --> 01:34:01.680]   Because-- and she is freaking thrilled.
[01:34:01.680 --> 01:34:04.320]   Because all she does is tell it to play,
[01:34:04.320 --> 01:34:05.920]   you know, the Pokemon theme song.
[01:34:05.920 --> 01:34:06.480]   Oh.
[01:34:06.480 --> 01:34:07.840]   And Pentatops.
[01:34:07.840 --> 01:34:10.720]   Over and over and over and over again.
[01:34:10.720 --> 01:34:11.760]   [LAUGHTER]
[01:34:11.760 --> 01:34:12.400]   In her room--
[01:34:12.400 --> 01:34:12.960]   Oh, my god.
[01:34:12.960 --> 01:34:13.680]   --or--
[01:34:13.680 --> 01:34:14.160]   Yes.
[01:34:14.160 --> 01:34:15.440]   But now it'll be in her room.
[01:34:15.440 --> 01:34:17.120]   So we're all very excited about that.
[01:34:17.120 --> 01:34:21.760]   And so yes, Logitech Harmony Home--
[01:34:21.760 --> 01:34:22.960]   a little bit different though.
[01:34:22.960 --> 01:34:24.560]   So something to note.
[01:34:24.560 --> 01:34:27.600]   Most things you ask for Google,
[01:34:27.600 --> 01:34:29.120]   you just go to the Google Home app,
[01:34:29.120 --> 01:34:32.240]   and then you add the device from there.
[01:34:32.240 --> 01:34:35.520]   For the Logitech integration,
[01:34:35.520 --> 01:34:38.160]   you actually have to tell it to--
[01:34:38.160 --> 01:34:41.520]   you actually tell the Google Home OKG
[01:34:43.680 --> 01:34:46.000]   open-- I'm trying to remember the exact words.
[01:34:46.000 --> 01:34:47.920]   It's like open Logitech Harmony--
[01:34:47.920 --> 01:34:50.480]   or connect Harmony Hub.
[01:34:50.480 --> 01:34:54.720]   And then it's going to throw up a card on your phone app,
[01:34:54.720 --> 01:34:56.000]   and then you connect through that.
[01:34:56.000 --> 01:34:57.760]   So it's a little weird.
[01:34:57.760 --> 01:35:00.400]   I had a really hard time because my phone kept saying,
[01:35:00.400 --> 01:35:02.000]   I can't help you with that.
[01:35:02.000 --> 01:35:04.800]   Because it was talking instead of the Hub,
[01:35:04.800 --> 01:35:07.120]   it was talking to the home or the Google Assistant.
[01:35:07.120 --> 01:35:09.440]   So I finally put my phone in another room
[01:35:09.440 --> 01:35:11.200]   and then talked to the Google Home.
[01:35:11.200 --> 01:35:11.840]   And that worked.
[01:35:11.840 --> 01:35:13.840]   [CHUCKLES]
[01:35:13.840 --> 01:35:17.360]   So--
[01:35:17.360 --> 01:35:17.840]   Good.
[01:35:17.840 --> 01:35:23.280]   Can I get the pedatonics singing the Pokemon theme on my Google Home Hub?
[01:35:23.280 --> 01:35:25.120]   You know, I don't know if they do the Pokemon theme.
[01:35:25.120 --> 01:35:27.200]   They certainly do a wide variety of other things.
[01:35:27.200 --> 01:35:29.600]   Inquiring lines, want to know.
[01:35:29.600 --> 01:35:30.480]   Oh.
[01:35:30.480 --> 01:35:32.560]   Oh, hey, guys.
[01:35:32.560 --> 01:35:33.440]   Look who's coming in.
[01:35:33.440 --> 01:35:34.080]   Come on in.
[01:35:34.080 --> 01:35:36.000]   Let's hear the Pokemon theme.
[01:35:36.000 --> 01:35:38.800]   Who literally just busted into the doorway.
[01:35:38.800 --> 01:35:40.080]   This is like a good-seat guy.
[01:35:40.080 --> 01:35:42.080]   He's trying to pull a BBC thing.
[01:35:42.080 --> 01:35:43.200]   Oh, no, it's a dog.
[01:35:43.200 --> 01:35:44.400]   Oh, but it's a cute dog.
[01:35:44.400 --> 01:35:48.560]   Oh, we don't blame you.
[01:35:48.560 --> 01:35:52.160]   Now, if your husband comes and drags him out by the tail
[01:35:52.160 --> 01:35:56.080]   while crouching the load, ostensibly below the camera line,
[01:35:56.080 --> 01:35:57.360]   then you've got a viral video.
[01:35:57.360 --> 01:35:58.080]   That'd be awesome.
[01:35:58.080 --> 01:35:58.240]   Yeah.
[01:35:58.240 --> 01:35:59.680]   Yeah.
[01:35:59.680 --> 01:36:00.880]   My dog is not--
[01:36:00.880 --> 01:36:01.840]   But she did.
[01:36:01.840 --> 01:36:02.720]   The door was shut.
[01:36:02.720 --> 01:36:03.840]   How did she open it?
[01:36:03.840 --> 01:36:05.920]   She has learned how to use the knob.
[01:36:05.920 --> 01:36:06.880]   [CHUCKLES]
[01:36:06.880 --> 01:36:07.360]   That is a problem.
[01:36:07.360 --> 01:36:08.320]   Be afraid.
[01:36:08.320 --> 01:36:09.600]   Be very afraid.
[01:36:10.480 --> 01:36:11.040]   All right.
[01:36:11.040 --> 01:36:11.680]   Oh.
[01:36:11.680 --> 01:36:13.520]   Oh, now you've got to meet my dog, too.
[01:36:13.520 --> 01:36:14.160]   What's your name?
[01:36:14.160 --> 01:36:15.600]   Sophie.
[01:36:15.600 --> 01:36:16.560]   Sophie's adorable.
[01:36:16.560 --> 01:36:17.520]   She's part gorgi.
[01:36:17.520 --> 01:36:19.040]   I think so.
[01:36:19.040 --> 01:36:19.360]   Yeah.
[01:36:19.360 --> 01:36:20.720]   There's a look a little gorgi in there.
[01:36:20.720 --> 01:36:22.720]   Oh, she's adorable.
[01:36:22.720 --> 01:36:25.200]   She-- oh, she-- oh, look.
[01:36:25.200 --> 01:36:25.760]   Oh, my God.
[01:36:25.760 --> 01:36:26.480]   She does tricks.
[01:36:26.480 --> 01:36:28.320]   Now she wants a treat.
[01:36:28.320 --> 01:36:29.280]   Yes.
[01:36:29.280 --> 01:36:29.760]   I'm sorry.
[01:36:29.760 --> 01:36:31.280]   I have nothing for you.
[01:36:31.280 --> 01:36:34.480]   Just give her one of your toes.
[01:36:34.480 --> 01:36:35.120]   She'll like that.
[01:36:35.120 --> 01:36:36.400]   Oh.
[01:36:36.400 --> 01:36:36.880]   [CHUCKLES]
[01:36:36.880 --> 01:36:37.360]   Uh.
[01:36:37.360 --> 01:36:39.440]   Gosh, that got dark, too.
[01:36:39.840 --> 01:36:41.280]   Is there something wrong with me?
[01:36:41.280 --> 01:36:42.560]   [CHUCKLES]
[01:36:42.560 --> 01:36:46.080]   I brought you a dog that totally live with things up in your life.
[01:36:46.080 --> 01:36:47.440]   There's something wrong with me.
[01:36:47.440 --> 01:36:48.880]   Even a puppy couldn't help.
[01:36:48.880 --> 01:36:51.200]   [CHUCKLES]
[01:36:51.200 --> 01:36:56.400]   Actually, I will be able to watch the NFL on not Twitter this year, but Amazon.
[01:36:56.400 --> 01:36:57.120]   Yeah.
[01:36:57.120 --> 01:37:01.840]   And Amazon got the deal by spending 10 times more than Twitter spent.
[01:37:01.840 --> 01:37:03.040]   Yeah.
[01:37:03.040 --> 01:37:06.480]   Five games, 50 million shmakers.
[01:37:06.480 --> 01:37:09.120]   That's kind of amazing.
[01:37:09.120 --> 01:37:09.760]   And--
[01:37:09.760 --> 01:37:10.880]   I'm sorry, 10 games.
[01:37:10.880 --> 01:37:12.080]   Five million again.
[01:37:12.080 --> 01:37:14.240]   And it's free to Amazon Prime members.
[01:37:14.240 --> 01:37:14.720]   And so--
[01:37:14.720 --> 01:37:15.280]   Oh.
[01:37:15.280 --> 01:37:20.000]   You know, it's yet another case where any time you try to do anything with Amazon,
[01:37:20.000 --> 01:37:23.760]   it's always like, wow, it really makes a lot of sense to have an Amazon Prime account.
[01:37:23.760 --> 01:37:25.840]   What did Twitter game by streaming on Twitter?
[01:37:25.840 --> 01:37:26.640]   Nothing.
[01:37:26.640 --> 01:37:31.280]   Well, Amazon game, you know, a thousand, hundred thousand Prime subscribers, maybe?
[01:37:31.280 --> 01:37:32.080]   I don't know.
[01:37:32.080 --> 01:37:36.480]   They'll get new customers who will, in their lifetime, spend a million dollars on Amazon.
[01:37:36.480 --> 01:37:36.960]   I know.
[01:37:36.960 --> 01:37:38.320]   I know this base for itself.
[01:37:39.200 --> 01:37:43.680]   There used to be a service that you could link your Amazon account to that would tell you how
[01:37:43.680 --> 01:37:45.600]   much you've spent on Amazon over time.
[01:37:45.600 --> 01:37:47.600]   Oh, I can do that.
[01:37:47.600 --> 01:37:48.640]   I can text--
[01:37:48.640 --> 01:37:50.880]   I have a-- actually, you should be my pick this week.
[01:37:50.880 --> 01:37:56.480]   It's a text financial messaging service that sends you texts every once in a while.
[01:37:56.480 --> 01:38:00.720]   And then if you want, you can say, you know, tell me how much I spent on Amazon or any other,
[01:38:00.720 --> 01:38:04.080]   you know, as long as it's all the accounts are tied in.
[01:38:04.080 --> 01:38:05.040]   See, you can put your--
[01:38:05.040 --> 01:38:05.600]   See, you can put your--
[01:38:05.600 --> 01:38:06.080]   I can do those.
[01:38:06.080 --> 01:38:07.200]   And I don't remember what they are either.
[01:38:07.200 --> 01:38:10.800]   But I-- and so I can't cancel them because I'm like, "The 20s this and I don't care that much."
[01:38:10.800 --> 01:38:11.600]   So--
[01:38:11.600 --> 01:38:12.560]   I don't really want to know.
[01:38:12.560 --> 01:38:18.640]   Mine's called-- well, it told me-- so, okay.
[01:38:18.640 --> 01:38:20.400]   But it doesn't say its name.
[01:38:20.400 --> 01:38:21.840]   Yeah.
[01:38:21.840 --> 01:38:23.600]   Why was it going to say I can do a race that night?
[01:38:23.600 --> 01:38:24.480]   It's called Trim.
[01:38:24.480 --> 01:38:25.920]   Trim, that's the one I have, yeah.
[01:38:25.920 --> 01:38:30.560]   So it just said-- so it sent me a text a couple of days ago.
[01:38:30.560 --> 01:38:35.760]   Was this month spending angel or monkey with his hands over his ears?
[01:38:35.760 --> 01:38:37.840]   Because they use clever emoticons.
[01:38:37.840 --> 01:38:40.400]   Reply, spend Amazon is see how much you spent there.
[01:38:40.400 --> 01:38:42.400]   So I had to spend Amazon.
[01:38:42.400 --> 01:38:46.160]   You spent $3,586 on Amazon this month.
[01:38:46.160 --> 01:38:46.400]   Wow.
[01:38:46.400 --> 01:38:47.840]   What the hell?
[01:38:47.840 --> 01:38:49.120]   You win.
[01:38:49.120 --> 01:38:51.840]   I think that includes-- no, but that-- no, but that includes--
[01:38:51.840 --> 01:38:54.240]   It's you too, John.
[01:38:54.240 --> 01:38:57.120]   It includes the whole network.
[01:38:57.120 --> 01:38:58.720]   This is like, we buy stuff.
[01:38:58.720 --> 01:38:59.040]   No?
[01:38:59.520 --> 01:39:04.560]   No, it does, because it's got our business cards are also tied into this.
[01:39:04.560 --> 01:39:07.120]   Leo, tell us about your TV again.
[01:39:07.120 --> 01:39:08.160]   How amazing is that?
[01:39:08.160 --> 01:39:10.800]   You got it on Amazon.
[01:39:10.800 --> 01:39:17.120]   You can also say spend McDonald's.
[01:39:17.120 --> 01:39:19.440]   Let me see how much I spend at McDonald's this month.
[01:39:19.440 --> 01:39:21.680]   Zero.
[01:39:21.680 --> 01:39:22.800]   Please, it's got to be zero.
[01:39:22.800 --> 01:39:24.000]   Oh, it's definitely zero.
[01:39:24.000 --> 01:39:27.680]   Did you have you seen the movie called "Founder" about Ray Crock?
[01:39:27.680 --> 01:39:29.040]   Oh, such a great movie.
[01:39:29.040 --> 01:39:30.240]   Oh, I actually wanted to see that.
[01:39:30.240 --> 01:39:30.720]   It's fascinating.
[01:39:30.720 --> 01:39:33.360]   I don't know if it's a great movie, but it is very interesting.
[01:39:33.360 --> 01:39:34.320]   He was kind of a jerk.
[01:39:34.320 --> 01:39:38.560]   He was a jerk, but it's fascinating from the perspective of thinking about
[01:39:38.560 --> 01:39:43.680]   Silicon Valley and startups and how, and almost all the biggest properties,
[01:39:43.680 --> 01:39:47.360]   the thing they thought was not valuable turns out to be the most valuable thing.
[01:39:47.360 --> 01:39:47.840]   Right.
[01:39:47.840 --> 01:39:49.680]   And in the case of McDonald's, it was real estate.
[01:39:49.680 --> 01:39:51.520]   And they did a great job depicting that.
[01:39:51.520 --> 01:39:52.720]   That was well done.
[01:39:52.720 --> 01:39:54.880]   BJ Novak, formerly of the office,
[01:39:55.440 --> 01:39:57.680]   turns out who knew to be a business genius.
[01:39:57.680 --> 01:40:01.680]   And he's at the bank and he over here's Ray Crock saying,
[01:40:01.680 --> 01:40:02.960]   "Please don't take my house."
[01:40:02.960 --> 01:40:05.920]   And he says, "You keep doing it all wrong."
[01:40:05.920 --> 01:40:08.560]   Because I guess the original deal, so it's fascinating how he's
[01:40:08.560 --> 01:40:10.560]   basically screwed the McDonald's brothers.
[01:40:10.560 --> 01:40:11.520]   Yeah.
[01:40:11.520 --> 01:40:15.280]   And but then he's, Crock is such a doofus.
[01:40:15.280 --> 01:40:16.640]   He screws him and gets nothing.
[01:40:16.640 --> 01:40:19.840]   He gets one and a half percent of revenue.
[01:40:19.840 --> 01:40:22.080]   And it's like he's losing his shirt.
[01:40:22.640 --> 01:40:25.920]   And so this guy comes over ends up being the CEO of McDonald's and says,
[01:40:25.920 --> 01:40:27.280]   "You know, you got it all wrong.
[01:40:27.280 --> 01:40:30.720]   This is a real estate play and explains it and they do it.
[01:40:30.720 --> 01:40:32.720]   And now it's the big, you know, it's a huge company."
[01:40:32.720 --> 01:40:33.600]   Yeah.
[01:40:33.600 --> 01:40:39.200]   But in the long run, what, and by the way, Ray Crock is very nicely played,
[01:40:39.200 --> 01:40:43.200]   I thought, by Batman.
[01:40:43.200 --> 01:40:44.240]   I want to say Tom Hanks.
[01:40:44.240 --> 01:40:44.960]   Val Kilmer?
[01:40:44.960 --> 01:40:47.040]   No, no, no.
[01:40:47.040 --> 01:40:48.160]   The guy was in Beetlejuice.
[01:40:48.160 --> 01:40:50.560]   Yeah, the guy was in the thing, you know.
[01:40:50.560 --> 01:40:51.680]   What?
[01:40:51.680 --> 01:40:51.920]   Yeah.
[01:40:52.560 --> 01:40:52.560]   Yeah.
[01:40:52.560 --> 01:40:54.080]   Michael Keaton.
[01:40:54.080 --> 01:40:55.360]   Oh, Michael Keaton.
[01:40:55.360 --> 01:40:56.480]   He's played by Beetlejuice.
[01:40:56.480 --> 01:40:57.840]   That's Batman.
[01:40:57.840 --> 01:40:58.320]   Yeah.
[01:40:58.320 --> 01:40:59.280]   That's Beetlejuice.
[01:40:59.280 --> 01:40:59.920]   Hey, Batman.
[01:40:59.920 --> 01:41:01.440]   Hey, Batman doesn't narrow it down much.
[01:41:01.440 --> 01:41:02.720]   Beetlejuice narrows it down a lot.
[01:41:02.720 --> 01:41:07.600]   Michael Keaton, at the end, I know this isn't,
[01:41:07.600 --> 01:41:08.640]   can you have a spoiler?
[01:41:08.640 --> 01:41:10.000]   No, everybody knows what happened.
[01:41:10.000 --> 01:41:12.960]   At the end, he says, "You guys thought that the,
[01:41:12.960 --> 01:41:17.120]   the Keaton McDonald's was, you know, the layout, the speedy service, all this."
[01:41:17.120 --> 01:41:18.480]   I thought it was the bathrooms.
[01:41:18.480 --> 01:41:19.200]   The bathrooms.
[01:41:19.200 --> 01:41:21.840]   People traveling interstate highways wanted to go into.
[01:41:21.840 --> 01:41:22.400]   Yeah.
[01:41:22.400 --> 01:41:23.440]   You guys never knew.
[01:41:23.440 --> 01:41:26.080]   All this time, you never knew what was really valuable at McDonald's.
[01:41:26.080 --> 01:41:26.800]   It's the name.
[01:41:26.800 --> 01:41:29.200]   And Ray Kroc really did say this.
[01:41:29.200 --> 01:41:32.320]   They show video of him at the end, the real Ray Kroc saying this.
[01:41:32.320 --> 01:41:34.080]   The name McDonald's, it's America.
[01:41:34.080 --> 01:41:38.160]   He said, "I don't like those gimmicky names, burger this or Jack-in-the-that."
[01:41:38.160 --> 01:41:40.720]   McDonald's, it's America.
[01:41:40.720 --> 01:41:44.480]   He tells the McDonald's brothers, "You cannot use the name McDonald's anymore."
[01:41:44.480 --> 01:41:45.760]   He won't let them use their own name.
[01:41:47.520 --> 01:41:48.400]   Oh, it's good.
[01:41:48.400 --> 01:41:52.720]   And Nick Offerman plays one of the McDonald's brothers brilliantly.
[01:41:52.720 --> 01:41:53.360]   The smart one.
[01:41:53.360 --> 01:41:54.400]   The smart one.
[01:41:54.400 --> 01:41:54.960]   So good.
[01:41:54.960 --> 01:41:56.400]   Yeah, he was really good.
[01:41:56.400 --> 01:41:57.840]   That cheered me up.
[01:41:57.840 --> 01:42:02.640]   Oh, he didn't spend any money at McDonald's.
[01:42:02.640 --> 01:42:03.520]   Was months, so.
[01:42:03.520 --> 01:42:04.000]   I didn't.
[01:42:04.000 --> 01:42:04.800]   That's good.
[01:42:04.800 --> 01:42:05.840]   It was actually zero.
[01:42:05.840 --> 01:42:09.520]   That also cheers me up.
[01:42:09.520 --> 01:42:13.040]   It says, "We couldn't find a McDonald's transaction since the beginning of the month.
[01:42:13.040 --> 01:42:14.160]   Try spend lift."
[01:42:17.120 --> 01:42:18.480]   I also spent nothing on lift.
[01:42:18.480 --> 01:42:19.600]   See, that's what I service.
[01:42:19.600 --> 01:42:20.160]   That's trim.
[01:42:20.160 --> 01:42:20.640]   It's called.
[01:42:20.640 --> 01:42:21.120]   Yeah.
[01:42:21.120 --> 01:42:21.920]   Okay.
[01:42:21.920 --> 01:42:22.400]   I like it.
[01:42:22.400 --> 01:42:24.240]   Because it's all text-based.
[01:42:24.240 --> 01:42:25.120]   So, yeah, you can ask so much.
[01:42:25.120 --> 01:42:26.240]   You can also just tell it all accounts.
[01:42:26.240 --> 01:42:29.360]   And it just tell you the balance of all your accounts in one text.
[01:42:29.360 --> 01:42:33.040]   The way they get you is, and actually this really worked with me,
[01:42:33.040 --> 01:42:38.640]   is they say, "You've got monthly subscriptions to a bunch of stuff.
[01:42:38.640 --> 01:42:41.600]   Find out how much it's losing and help and will help you cancel it."
[01:42:41.600 --> 01:42:45.840]   So you can say, "My subscriptions," and then it gives you all your subscriptions.
[01:42:45.840 --> 01:42:48.640]   And then they say, "Cancel Audible," which I'm not real happy about.
[01:42:48.640 --> 01:42:50.560]   Got it.
[01:42:50.560 --> 01:42:51.040]   We'll cancel.
[01:42:51.040 --> 01:42:52.240]   And then we'll cancel it for you.
[01:42:52.240 --> 01:42:56.720]   So that's what got me because I have a lot of monthly subscriptions that I lost track of.
[01:42:56.720 --> 01:42:57.280]   We all do.
[01:42:57.280 --> 01:42:58.240]   Oh, yeah.
[01:42:58.240 --> 01:43:02.880]   Well, my husband keeps a close eye on all of our monthly subscriptions.
[01:43:02.880 --> 01:43:03.600]   Man, it's good.
[01:43:03.600 --> 01:43:04.400]   Somebody's got to.
[01:43:04.400 --> 01:43:05.760]   Yeah, no.
[01:43:05.760 --> 01:43:10.880]   I mean, because we have probably 30 or 40 monthly subscriptions.
[01:43:10.880 --> 01:43:15.200]   There are so many businesses that rely entirely on people forgetting that the money is flowing
[01:43:15.200 --> 01:43:16.080]   in their direction.
[01:43:16.080 --> 01:43:17.280]   You think about gift cards.
[01:43:17.280 --> 01:43:19.280]   Get a gift card.
[01:43:19.280 --> 01:43:20.080]   You get one as a gift.
[01:43:20.080 --> 01:43:20.720]   You forget about it.
[01:43:20.720 --> 01:43:22.880]   It loses it, it ends up in the trash or whatever.
[01:43:22.880 --> 01:43:24.480]   They still keep the money.
[01:43:24.480 --> 01:43:26.240]   It's terrible.
[01:43:26.240 --> 01:43:28.000]   Let's see.
[01:43:28.000 --> 01:43:30.160]   I have quite a few subscriptions.
[01:43:30.160 --> 01:43:31.600]   Oh, wait.
[01:43:31.600 --> 01:43:34.000]   Nevermind.
[01:43:34.000 --> 01:43:34.480]   Okay.
[01:43:34.480 --> 01:43:35.520]   It's a great service.
[01:43:35.520 --> 01:43:36.960]   They'd make a great advertiser.
[01:43:36.960 --> 01:43:37.520]   I like Trim.
[01:43:37.520 --> 01:43:38.080]   I do.
[01:43:38.080 --> 01:43:38.720]   I like it.
[01:43:38.720 --> 01:43:41.520]   And it's one of those things, another one I signed up for, but it's free.
[01:43:41.520 --> 01:43:42.000]   Right?
[01:43:42.000 --> 01:43:42.400]   It's free.
[01:43:42.400 --> 01:43:43.680]   It's a terrible name.
[01:43:43.680 --> 01:43:44.240]   Trim.
[01:43:44.240 --> 01:43:44.560]   Yeah.
[01:43:45.360 --> 01:43:45.600]   Yeah.
[01:43:45.600 --> 01:43:51.280]   But it's one of the few messaging bots that actually I actually like.
[01:43:51.280 --> 01:43:51.760]   Yes.
[01:43:51.760 --> 01:43:56.000]   That's what everybody's talking about messaging bots and they're boring, usually useless.
[01:43:56.000 --> 01:43:57.760]   This one is actually really good.
[01:43:57.760 --> 01:43:58.160]   Yeah.
[01:43:58.160 --> 01:44:03.120]   Well, you could also take, I mean, like, so I can pull from like, I guess it's quick in
[01:44:03.120 --> 01:44:05.760]   my spending habits from my credit card.
[01:44:05.760 --> 01:44:08.560]   If you submit, mint will tell you this, for instance.
[01:44:08.560 --> 01:44:10.880]   Now, imagine this is an echo skill.
[01:44:10.880 --> 01:44:14.320]   So you can be like, hey, hey, hey, lady, hey.
[01:44:14.320 --> 01:44:14.960]   Yes.
[01:44:14.960 --> 01:44:16.720]   What did I spend on Amazon?
[01:44:16.720 --> 01:44:17.840]   Well, I like Lady A.
[01:44:17.840 --> 01:44:19.040]   Let's call her that from now on.
[01:44:19.040 --> 01:44:19.920]   Sure.
[01:44:19.920 --> 01:44:21.040]   I like that.
[01:44:21.040 --> 01:44:22.720]   Nice job, Lady A.
[01:44:22.720 --> 01:44:25.360]   And then S-I-R-I could be Lady S.
[01:44:25.360 --> 01:44:27.040]   Yeah.
[01:44:27.040 --> 01:44:32.880]   I, Siri, you know, we call Lady A, Madam A, but I'm thinking Siri is actually more like a madam-ess.
[01:44:32.880 --> 01:44:34.240]   Madam-A, madam-ess.
[01:44:34.240 --> 01:44:37.600]   Because Lady A sounds better because they A-A.
[01:44:37.600 --> 01:44:37.760]   Yeah.
[01:44:37.760 --> 01:44:40.400]   And then I'm still going with A-G until we come up with some time.
[01:44:40.400 --> 01:44:41.360]   A-G, A-G.
[01:44:42.480 --> 01:44:44.080]   Mr. G, son of G.
[01:44:44.080 --> 01:44:45.840]   O-G.
[01:44:45.840 --> 01:44:46.960]   Do we want to talk about?
[01:44:46.960 --> 01:44:48.320]   No, because it'll depress everybody.
[01:44:48.320 --> 01:44:49.440]   Online privacy rules?
[01:44:49.440 --> 01:44:50.480]   No, let's not talk about that.
[01:44:50.480 --> 01:44:54.720]   I did a story about what it means for the Internet of Things.
[01:44:54.720 --> 01:44:58.160]   What does it mean for the Internet of Things?
[01:44:58.160 --> 01:44:59.120]   Should you care?
[01:44:59.120 --> 01:44:59.840]   Is there anything good?
[01:44:59.840 --> 01:45:00.000]   I have.
[01:45:00.000 --> 01:45:01.600]   No.
[01:45:01.600 --> 01:45:02.080]   No.
[01:45:02.080 --> 01:45:02.560]   It's not.
[01:45:02.560 --> 01:45:03.440]   Here's some good news.
[01:45:03.440 --> 01:45:06.240]   The Minnesota Senate, and I suspect this is what will happen.
[01:45:06.240 --> 01:45:09.520]   Has voted 58 to 9 to pass Internet privacy protections.
[01:45:09.520 --> 01:45:13.200]   So that may be the best side of this is that the states might get involved.
[01:45:13.200 --> 01:45:17.200]   But then you'll have to move to a state where your privacy is respected.
[01:45:17.200 --> 01:45:19.440]   Texas is never going to do that.
[01:45:19.440 --> 01:45:20.800]   You're not going to get that one, though.
[01:45:20.800 --> 01:45:25.200]   I mean, I don't think that's necessarily that terrible of a thing.
[01:45:25.200 --> 01:45:30.400]   I think that generally, I think that, for example, California is moving heavily on,
[01:45:30.400 --> 01:45:33.360]   more heavily than it would have on environmental protections.
[01:45:33.360 --> 01:45:38.240]   And they're probably going further than the nation would.
[01:45:39.200 --> 01:45:43.360]   Even if we had somebody who was president who was really into environmental protections.
[01:45:43.360 --> 01:45:48.880]   And so if California does certain things, it's very influential nationwide because there's
[01:45:48.880 --> 01:45:50.320]   such a big state.
[01:45:50.320 --> 01:45:54.160]   And so the California environmental protections are not only going to
[01:45:54.160 --> 01:45:57.920]   reflect the values of the state, but they're also going to influence other states.
[01:45:57.920 --> 01:46:01.280]   So I don't think it's the worst thing that states are more offensive than that.
[01:46:01.280 --> 01:46:06.880]   If a big state does it, then every company has to kind of adhere to it if they want to do business in that state.
[01:46:06.880 --> 01:46:07.360]   Right?
[01:46:07.360 --> 01:46:07.680]   Yeah.
[01:46:07.680 --> 01:46:09.120]   So Comcast is national.
[01:46:09.120 --> 01:46:15.760]   Which is why everything I buy has a sticker telling me things that are classified as poisonous
[01:46:15.760 --> 01:46:16.320]   or cancerous.
[01:46:16.320 --> 01:46:17.120]   Because of us.
[01:46:17.120 --> 01:46:17.120]   Yeah.
[01:46:17.120 --> 01:46:18.640]   I'm like, oh, California.
[01:46:18.640 --> 01:46:21.760]   The world's famous prop 60 warning prop 60.
[01:46:21.760 --> 01:46:22.720]   It's so ridiculous.
[01:46:22.720 --> 01:46:27.920]   You can't go into a garage without being warned that inside there are things that
[01:46:27.920 --> 01:46:29.680]   know the state of California that's through you.
[01:46:29.680 --> 01:46:30.560]   My Christmas lights.
[01:46:30.560 --> 01:46:31.360]   I'm like, oh.
[01:46:31.360 --> 01:46:36.480]   And then an example, not only of a bad regulation, and this was one of the propositions, you know,
[01:46:36.480 --> 01:46:38.960]   so that we have this proposition process is really broken.
[01:46:38.960 --> 01:46:44.640]   But it's also an example of, and I remember this when I was a kid, we went to a power plant,
[01:46:44.640 --> 01:46:47.280]   a coal-fired power plant, Providence, Rhode Island, where I lived.
[01:46:47.280 --> 01:46:50.640]   And I was at, already I was a trouble.
[01:46:50.640 --> 01:46:54.400]   I was, you know, I said, why are all those dead fish in that cooling pond?
[01:46:54.400 --> 01:46:56.240]   And the guy said, no, no, no, let's look over here.
[01:46:56.240 --> 01:47:01.920]   But one of the other things I saw was everywhere in the place they saw, I saw signs and say,
[01:47:01.920 --> 01:47:08.000]   conserve, save energy, green, and this was in the 70s, early, maybe late 60s, green.
[01:47:08.000 --> 01:47:09.040]   Ecology now.
[01:47:09.040 --> 01:47:10.160]   Ecology now.
[01:47:10.160 --> 01:47:13.360]   And I asked my teacher, this is a coal-fired power plant.
[01:47:13.360 --> 01:47:17.680]   She said, because it dulls you to the notion.
[01:47:17.680 --> 01:47:20.960]   So the more you see this stuff, the less it means.
[01:47:20.960 --> 01:47:25.920]   So this prop 60 warning, in effect, dulls you the idea that you're surrounded by poisonous
[01:47:25.920 --> 01:47:26.720]   chemicals.
[01:47:26.720 --> 01:47:29.200]   And as a result, you pay no attention to it any longer.
[01:47:29.200 --> 01:47:31.840]   The net result is that exactly like there are no signs.
[01:47:31.840 --> 01:47:32.320]   Exactly.
[01:47:32.320 --> 01:47:33.120]   The eye is glazed over.
[01:47:33.120 --> 01:47:35.120]   Not only no signs, but no regulations.
[01:47:35.120 --> 01:47:35.760]   Exactly.
[01:47:35.760 --> 01:47:37.200]   Damn them.
[01:47:37.200 --> 01:47:42.160]   So do we talk about, I think we did, but I don't know if we did the whole thing.
[01:47:42.160 --> 01:47:45.840]   Ashley Feinberg's amazing detective work in Gizmodo.
[01:47:45.840 --> 01:47:48.400]   Do we talk about that last week?
[01:47:48.400 --> 01:47:49.520]   We did not talk.
[01:47:49.520 --> 01:47:50.640]   Well, I wasn't here last week.
[01:47:50.640 --> 01:47:51.280]   Oh, so you don't know.
[01:47:51.280 --> 01:47:51.600]   I don't know.
[01:47:51.600 --> 01:47:52.960]   Were you here?
[01:47:52.960 --> 01:47:55.120]   No, I wasn't here either.
[01:47:55.120 --> 01:47:56.560]   I wasn't here.
[01:47:56.560 --> 01:47:57.360]   Nobody was here.
[01:47:57.920 --> 01:48:01.280]   So I apologize to all the listeners who were here.
[01:48:01.280 --> 01:48:03.600]   But we didn't talk about it.
[01:48:03.600 --> 01:48:09.200]   This is why privacy on the internet is so important.
[01:48:09.200 --> 01:48:10.160]   We know.
[01:48:10.160 --> 01:48:12.880]   So you didn't talk about it because it came out on Thursday.
[01:48:12.880 --> 01:48:13.760]   Okay, good.
[01:48:13.760 --> 01:48:20.160]   We know that you can have anonymized search results and go through them.
[01:48:20.160 --> 01:48:21.280]   In fact, somebody did a study.
[01:48:21.280 --> 01:48:26.000]   They took a pile of anonymized search results and they were able to identify the
[01:48:26.000 --> 01:48:33.440]   searcher 70% accuracy, the searcher of those search results just by looking at the search results.
[01:48:33.440 --> 01:48:37.040]   There is no such thing as anonymous stuff on the internet.
[01:48:37.040 --> 01:48:39.680]   If you have the data, you can figure out who it is.
[01:48:39.680 --> 01:48:44.960]   So Ashley Feinberg, who's a brilliant reporter at Gizmodo, she does a great job,
[01:48:44.960 --> 01:48:46.640]   did some investigation.
[01:48:46.640 --> 01:48:52.960]   And it all came from the fact that at the National Intelligence and Security Alliance
[01:48:52.960 --> 01:49:00.000]   leadership dinner, the director of the FBI, James Comey, said he had a secret Twitter
[01:49:00.000 --> 01:49:01.200]   and Instagram account.
[01:49:01.200 --> 01:49:02.640]   It was just an anecdote.
[01:49:02.640 --> 01:49:03.520]   He was talking about stars.
[01:49:03.520 --> 01:49:06.000]   And of course, if you're the director of the FBI, you probably don't want, you know,
[01:49:06.000 --> 01:49:07.600]   you might have a public account.
[01:49:07.600 --> 01:49:12.080]   But if you want to have a social account just for you, you're going to make it a secret.
[01:49:12.080 --> 01:49:16.240]   And of course, Ashley said, "Hmm, how secret is that?"
[01:49:16.240 --> 01:49:22.240]   Well, he gave out a few details that were helpful.
[01:49:22.240 --> 01:49:25.280]   He said, for instance, his Instagram account has only nine followers.
[01:49:25.280 --> 01:49:27.600]   Uh-huh.
[01:49:27.600 --> 01:49:30.880]   So Ashley says, "Who am I to say no to a challenge?"
[01:49:30.880 --> 01:49:36.560]   So the only hint he offered about Twitter was that he has to be on Twitter now.
[01:49:36.560 --> 01:49:38.800]   I mean, the account would likely be relatively new.
[01:49:38.800 --> 01:49:44.640]   And she learned a lot more about the Instagram account.
[01:49:44.640 --> 01:49:49.840]   He says, "I treasure my privacy and security on the internet, ironic.
[01:49:49.840 --> 01:49:51.360]   My job is public safety.
[01:49:52.000 --> 01:49:55.280]   He's the guy Comey has taped over his camera as well.
[01:49:55.280 --> 01:49:59.680]   Unfortunately, Instagram isn't conducive to custom searching.
[01:49:59.680 --> 01:50:03.200]   There was no way any of his five children or wife would be using their full names.
[01:50:03.200 --> 01:50:06.400]   Twitter, however, gives us a little more leverage, she writes after.
[01:50:06.400 --> 01:50:10.000]   Some trial and error I found is 22-year-old son, Brian Comey,
[01:50:10.000 --> 01:50:14.640]   seems to have the largest online presence as a basketball star at Canyon College.
[01:50:14.640 --> 01:50:15.200]   Go, lords!
[01:50:15.200 --> 01:50:19.440]   It wasn't easy to find Brian Comey on Twitter, though, because his first name is also the middle name
[01:50:19.440 --> 01:50:24.400]   of his father, B-R-I-E-N, helpful to have an unusual spelling.
[01:50:24.400 --> 01:50:30.800]   So after a few frustrated attempts, I've tried the following Twitter search on a whim,
[01:50:30.800 --> 01:50:32.480]   Brian Comey minus James.
[01:50:32.480 --> 01:50:37.440]   So that would get all of the Brian Comey mentions without any references to his father.
[01:50:37.440 --> 01:50:42.000]   This led me to a tweet from the Twitter account of Canyon College, the basketball team,
[01:50:42.000 --> 01:50:48.560]   showing Comey teaching basketball to some school kids, and @mentioned the now-dead Twitter
[01:50:48.560 --> 01:50:49.920]   account @Twittafuzz.
[01:50:49.920 --> 01:50:55.360]   That account appears to have been previously owned by Brian Comey.
[01:50:55.360 --> 01:50:59.920]   At least if you believe the folks on Twitter congratulating Twitter Fuzz for his dad's
[01:50:59.920 --> 01:51:01.760]   ascension to the head of the FBI.
[01:51:01.760 --> 01:51:03.280]   That seems like smoking on there.
[01:51:03.280 --> 01:51:09.360]   That led her to this periscope video of Brian Comey.
[01:51:09.360 --> 01:51:12.400]   Clinked through the linked photo, you'll find that a well-wishers left a comment
[01:51:12.400 --> 01:51:15.040]   in which Brian Comey's tagged.
[01:51:15.840 --> 01:51:17.920]   Now, our FBI director has trained his son well.
[01:51:17.920 --> 01:51:19.360]   His Instagram account is locked down.
[01:51:19.360 --> 01:51:21.680]   Instagram itself, however, offers a little loophole.
[01:51:21.680 --> 01:51:25.760]   It's terrible for user privacy, but wonderfully helpful for our purposes today.
[01:51:25.760 --> 01:51:29.760]   She created a fake Instagram account, actually has a fake Instagram account,
[01:51:29.760 --> 01:51:33.920]   which she keeps for tracking Donald Trump Jr. and Newt Gingrich.
[01:51:33.920 --> 01:51:34.960]   I don't know why.
[01:51:34.960 --> 01:51:39.040]   So using that anonymous account, Ashley requested access to Comey's account.
[01:51:39.040 --> 01:51:42.000]   As soon as I did, this popped up.
[01:51:42.000 --> 01:51:43.440]   This is the really great one.
[01:51:43.920 --> 01:51:46.800]   Suggestions for you since you requested Brian Comey.
[01:51:46.800 --> 01:51:52.880]   Maybe you'd be interested in his mom, Patrice, or this weird Reinhold Neibur account,
[01:51:52.880 --> 01:51:54.960]   among others.
[01:51:54.960 --> 01:52:00.800]   Comey is the plot thickens.
[01:52:00.800 --> 01:52:04.160]   Patrice's Comey's wife, so she knew these were Comey's.
[01:52:04.160 --> 01:52:08.720]   Among these various comies, only two of the suggested accounts lacked both real names and
[01:52:08.720 --> 01:52:12.560]   profile photos. And only one had anywhere near the nine followers.
[01:52:12.560 --> 01:52:17.920]   James Comey claimed to have, and that was Reinhold Neibur.
[01:52:17.920 --> 01:52:19.040]   This account is private.
[01:52:19.040 --> 01:52:23.920]   I wasn't sure it was James Comey, but a Google search turned up this article on
[01:52:23.920 --> 01:52:25.840]   Comey's time at William and Mary.
[01:52:25.840 --> 01:52:29.600]   By senior year, Comey was a double major in religion and chemistry,
[01:52:29.600 --> 01:52:35.200]   writing a senior thesis on theologian Reinhold Neibur, who,
[01:52:35.840 --> 01:52:40.160]   incidentally, I think is also Barack Obama's favorite theologian.
[01:52:40.160 --> 01:52:44.400]   The thesis also covered Jerry Falwell, the televangelist.
[01:52:44.400 --> 01:52:45.600]   It's a weird combination.
[01:52:45.600 --> 01:52:51.520]   Reinhold Neibur was a pastor who wrote the serenity prayer, among other things.
[01:52:51.520 --> 01:52:54.080]   He was also a prolific writer.
[01:52:54.080 --> 01:52:57.440]   So she feels like, "Okay, I got it."
[01:52:57.440 --> 01:52:59.760]   That's the Instagram account.
[01:52:59.760 --> 01:53:01.360]   So she goes back to Twitter, and you know what?
[01:53:01.360 --> 01:53:03.600]   There's an @ Reinhold Neibur,
[01:53:05.120 --> 01:53:08.160]   who tweets among their other things last year.
[01:53:08.160 --> 01:53:10.640]   What's the point of watching fully clothed beach volleyball?
[01:53:10.640 --> 01:53:13.840]   Right there, I would have said that.
[01:53:13.840 --> 01:53:14.960]   It couldn't be James Comey.
[01:53:14.960 --> 01:53:17.760]   Fortunately, there are only seven accounts on Twitter,
[01:53:17.760 --> 01:53:20.080]   currently using some variation of that name.
[01:53:20.080 --> 01:53:22.800]   So she found all of them.
[01:53:22.800 --> 01:53:26.560]   And only one seemed to be operating in stealth.
[01:53:26.560 --> 01:53:33.600]   See, this volleyball one was one of the non stealthy ones.
[01:53:33.600 --> 01:53:34.560]   This one wasn't, though.
[01:53:34.560 --> 01:53:37.760]   Project Xile 7 got an egg.
[01:53:37.760 --> 01:53:40.480]   Joined in February 2014.
[01:53:40.480 --> 01:53:43.360]   Project Xile 7 hasn't tweeted.
[01:53:43.360 --> 01:53:47.440]   How to be sure there's only one person currently following the account?
[01:53:47.440 --> 01:53:50.000]   Benjamin Wittes of Law Fair.
[01:53:50.000 --> 01:53:52.880]   Now, Wittes is no Twitter Neophyte, she says.
[01:53:52.880 --> 01:53:54.880]   He's an active user, 25,000 followers.
[01:53:54.880 --> 01:53:58.880]   He only follows, you know, 1,000 or more accounts,
[01:53:58.880 --> 01:54:01.440]   which means he doesn't automatically follow everybody.
[01:54:01.440 --> 01:54:04.320]   If he's following a random egg and is the only account following it,
[01:54:04.320 --> 01:54:05.280]   there's probably a reason.
[01:54:05.280 --> 01:54:09.360]   Well, maybe it's because he's a personal friend of James Comey.
[01:54:09.360 --> 01:54:15.920]   Project Xile, remember that was the actual Twitter name, Project Xile 7.
[01:54:15.920 --> 01:54:19.920]   Turns out to be a federal program, James Comey helped develop while he was
[01:54:19.920 --> 01:54:21.440]   a U.S. Attorney Living Enrichment.
[01:54:21.440 --> 01:54:25.520]   Project Xile 7 follows 27 other accounts,
[01:54:25.520 --> 01:54:28.800]   mostly reporters, news outlets, government, law enforcement accounts,
[01:54:29.920 --> 01:54:32.560]   and writers for the New York Times and the Washington Post,
[01:54:32.560 --> 01:54:38.160]   all of whom have been covering the FBI investigation into Trump's context with Russia.
[01:54:38.160 --> 01:54:45.200]   Donald Trump is on there, but Project Xile 7 seems to have begun
[01:54:45.200 --> 01:54:46.640]   following him relatively recently.
[01:54:46.640 --> 01:54:48.560]   His first follow was the New York Times.
[01:54:48.560 --> 01:54:50.800]   There are two outliers, William and Mary News.
[01:54:50.800 --> 01:54:53.360]   Ooh, Comey went to William and Mary and the onion.
[01:54:53.360 --> 01:54:57.120]   I'm glad to know James Comey follows the onion.
[01:54:57.120 --> 01:54:57.440]   Oops.
[01:54:57.440 --> 01:54:59.360]   Now, what have I done?
[01:54:59.360 --> 01:55:00.240]   I clicked something.
[01:55:00.240 --> 01:55:02.800]   You're recording a bunch of videos now.
[01:55:02.800 --> 01:55:03.920]   Your DVR is going to fill up.
[01:55:03.920 --> 01:55:04.720]   What is going on?
[01:55:04.720 --> 01:55:05.680]   I know, I might be.
[01:55:05.680 --> 01:55:07.040]   Help me.
[01:55:07.040 --> 01:55:08.240]   Help me, Mr. Wizard.
[01:55:08.240 --> 01:55:10.560]   Okay, back to the story.
[01:55:10.560 --> 01:55:12.880]   I'm sorry, this is so long, but it's such a good detective.
[01:55:12.880 --> 01:55:14.080]   She did such a good job.
[01:55:14.080 --> 01:55:17.120]   39 tweets the account is liked.
[01:55:17.120 --> 01:55:20.720]   See, you don't have to have any tweets to leave a paper trail on Twitter.
[01:55:20.720 --> 01:55:23.520]   Eight refer directly to the FBI or Comey himself.
[01:55:23.520 --> 01:55:27.600]   So you can even find this all as really easy to find out.
[01:55:27.600 --> 01:55:28.880]   These are all the things he's liked.
[01:55:28.880 --> 01:55:32.640]   One deals with an active FBI investigation.
[01:55:32.640 --> 01:55:35.680]   Four refer to the Trump administration in general.
[01:55:35.680 --> 01:55:38.240]   Ashley Feinberg.
[01:55:38.240 --> 01:55:40.800]   None of this is proof, of course.
[01:55:40.800 --> 01:55:45.600]   Take what you will from the fact that the director of the FBI appears to have liked
[01:55:45.600 --> 01:55:49.600]   a tweet from New York Times about Mike Flynn and Jared Kushner meeting a Russian envoy in December.
[01:55:49.600 --> 01:55:54.400]   We've reached out to the FBI for comment and we'll update when we hear back.
[01:55:54.400 --> 01:55:57.200]   In the meantime, Project Exile 7, I would love a follow.
[01:55:58.000 --> 01:56:03.680]   So.
[01:56:03.680 --> 01:56:06.320]   Wait, what did, what did he say?
[01:56:06.320 --> 01:56:07.680]   What he said.
[01:56:07.680 --> 01:56:10.560]   I actually commented earlier today on Comey's Twitter account on Twitter.
[01:56:10.560 --> 01:56:11.760]   No less beyond that public statement.
[01:56:11.760 --> 01:56:12.560]   I have nothing to say.
[01:56:12.560 --> 01:56:15.520]   He says day 70, I've been ignoring all the speculation about
[01:56:15.520 --> 01:56:17.760]   Comey's Twitter account, but this one's pretty damn funny.
[01:56:17.760 --> 01:56:18.720]   Let's see.
[01:56:18.720 --> 01:56:25.680]   And then he he refers to this tweet and she's just laughing at the tweet that this is James Fumey's
[01:56:25.680 --> 01:56:27.280]   James Comey's Twitter account.
[01:56:27.280 --> 01:56:31.360]   FBI's statement, hello, we don't have any comment.
[01:56:31.360 --> 01:56:31.680]   Thank you.
[01:56:31.680 --> 01:56:39.120]   But Lisa wasn't we have spoken enough about James Covey's Twitter account.
[01:56:39.120 --> 01:56:40.320]   I know what you're talking about now.
[01:56:40.320 --> 01:56:42.880]   So what do you think?
[01:56:42.880 --> 01:56:43.840]   I think she's right on.
[01:56:43.840 --> 01:56:45.520]   I think there's too many coincidences in that.
[01:56:45.520 --> 01:56:47.040]   Yeah, I think that's.
[01:56:47.040 --> 01:56:47.520]   I think it is.
[01:56:47.520 --> 01:56:50.320]   And I don't bring this up because we've learned anything.
[01:56:50.320 --> 01:56:50.320]   Sure.
[01:56:50.320 --> 01:56:51.120]   Sure.
[01:56:51.120 --> 01:56:51.920]   It's right.
[01:56:51.920 --> 01:56:55.600]   It is, but it's great detective work, but it points out that there is this
[01:56:55.600 --> 01:57:04.080]   data cloud around you at all times that tells investigative reporters, government agents,
[01:57:04.080 --> 01:57:07.360]   marketers more about you than you might imagine.
[01:57:07.360 --> 01:57:14.800]   And Twitter is a wonderful tool for finding things out that you don't think you'd be able
[01:57:14.800 --> 01:57:15.360]   to find out.
[01:57:15.360 --> 01:57:18.400]   And one of the best examples of that, of course, is Nick Bilton's
[01:57:18.400 --> 01:57:22.160]   hatching Twitter, where he's giving all these accounts and describing the weather and what
[01:57:22.160 --> 01:57:23.200]   people are wearing all this stuff.
[01:57:23.200 --> 01:57:25.680]   And he got all that data from Twitter by doing searches.
[01:57:25.680 --> 01:57:31.520]   He's doing historical, like, sort of painting a picture of things and finding out who was
[01:57:31.520 --> 01:57:35.600]   where at what time and who was meeting with whomever, just by seeing what people posted on Twitter.
[01:57:35.600 --> 01:57:42.160]   It's really a great repository of random data about people that even the people who
[01:57:42.160 --> 01:57:47.600]   are using Twitter don't realize reveals them to be who they are and what they do and what they're
[01:57:47.600 --> 01:57:51.200]   thinking about and what they're talking to and who their friends are and all that kind of stuff.
[01:57:51.200 --> 01:57:52.960]   So amazing.
[01:57:52.960 --> 01:57:56.880]   Somebody UOS DeWiz in the chair and said, wait a minute,
[01:57:56.880 --> 01:57:59.760]   Comey and Project Exile 7 are never in the same room.
[01:57:59.760 --> 01:58:01.760]   Oh, that's just silly, Lois.
[01:58:01.760 --> 01:58:09.280]   Anyway, the point, I know there's a long way around just to point out that this is why privacy
[01:58:09.280 --> 01:58:10.240]   on the Internet is important.
[01:58:10.240 --> 01:58:14.560]   I don't think there is any such thing as privacy on the Internet with enough data mining.
[01:58:14.560 --> 01:58:16.480]   Actually, a better point.
[01:58:16.480 --> 01:58:18.080]   A better point.
[01:58:18.080 --> 01:58:18.720]   Sorry, Leo.
[01:58:18.720 --> 01:58:20.800]   If you're putting it on the Internet, it's in public.
[01:58:20.800 --> 01:58:25.040]   Even if you're James Comey, the director of the FBI who presumably has some skills in
[01:58:25.040 --> 01:58:27.280]   ops sec, doesn't matter.
[01:58:27.280 --> 01:58:28.960]   I know.
[01:58:28.960 --> 01:58:30.560]   I have a story that will lighten the mood.
[01:58:30.560 --> 01:58:35.680]   AOL and Yahoo have come up with a new name after the Verizon deal.
[01:58:35.680 --> 01:58:38.640]   They're going to call themselves oath.
[01:58:38.640 --> 01:58:43.440]   What oath, OATH oath.
[01:58:43.440 --> 01:58:47.440]   You can't even say it in a way that somebody knows what you just said.
[01:58:48.160 --> 01:58:51.200]   Oh, he put the actual point from Yahoo after it.
[01:58:51.200 --> 01:58:58.080]   Oh, oh, oh, oh, oh, hey, can you find that?
[01:58:58.080 --> 01:59:00.000]   Well, I can't Google or maybe I can oath it.
[01:59:00.000 --> 01:59:03.600]   Yeah, that's not a good name.
[01:59:03.600 --> 01:59:07.120]   Why, why did they choose?
[01:59:07.120 --> 01:59:08.480]   Did they explain why they first?
[01:59:08.480 --> 01:59:14.560]   I mean, even if it's something like alphabet, whichever is not the actual name,
[01:59:14.560 --> 01:59:15.120]   which is fine.
[01:59:15.120 --> 01:59:15.600]   Makes sense.
[01:59:15.600 --> 01:59:17.680]   Cause you got a B C that makes sense.
[01:59:17.680 --> 01:59:22.000]   But oath is an actual word that presumably means something.
[01:59:22.000 --> 01:59:24.480]   Although Apple, they named it Apple because.
[01:59:24.480 --> 01:59:25.200]   Apples are friendly.
[01:59:25.200 --> 01:59:25.600]   They're nice.
[01:59:25.600 --> 01:59:26.240]   They're fruit.
[01:59:26.240 --> 01:59:28.080]   There were actually is a long story about that.
[01:59:28.080 --> 01:59:29.040]   Oh, what is an oath?
[01:59:29.040 --> 01:59:29.760]   It's a vow.
[01:59:29.760 --> 01:59:31.200]   A pledge.
[01:59:31.200 --> 01:59:34.480]   They pledged to keep your data.
[01:59:34.480 --> 01:59:37.680]   This was again good.
[01:59:37.680 --> 01:59:42.720]   Again, I got to give credit to the business insider and Avery Hartman's and Julie Bort.
[01:59:42.720 --> 01:59:43.680]   Good detective work.
[01:59:43.680 --> 01:59:50.320]   They found this big scoop and because of it, AOL CEO, Tim Armstrong ended up tweeting, yep.
[01:59:50.320 --> 01:59:59.360]   Billion plus consumers, 20 plus brands, unstoppable team hashtag take the oath of a rising company.
[01:59:59.360 --> 02:00:02.720]   Not an exclamation mark this time a colon.
[02:00:02.720 --> 02:00:09.760]   Can't you not a colon oath colon?
[02:00:11.120 --> 02:00:12.560]   I'm going to open colon.
[02:00:12.560 --> 02:00:16.480]   It should have been like oath, like question mark.
[02:00:16.480 --> 02:00:18.480]   And the same like why her or her.
[02:00:18.480 --> 02:00:23.360]   Oh, well, I have nothing to say about that.
[02:00:23.360 --> 02:00:25.280]   Cause it's just not great.
[02:00:25.280 --> 02:00:28.240]   Rissa Meyer will not be the CEO of oath and she's at this point.
[02:00:28.240 --> 02:00:29.600]   She's going, thank God.
[02:00:29.600 --> 02:00:31.040]   Yeah, give me the 50 million.
[02:00:31.040 --> 02:00:31.920]   I got to get out of here.
[02:00:31.920 --> 02:00:32.720]   I can't take it.
[02:00:32.720 --> 02:00:36.240]   All right, let's wrap it up.
[02:00:36.240 --> 02:00:37.840]   Anything else anybody wants to say?
[02:00:37.840 --> 02:00:40.640]   Any stories that I missed that you we could talk about uber.
[02:00:41.280 --> 02:00:45.920]   Actually, the big revelation I thought in this uber story was that Anthony Levendowski,
[02:00:45.920 --> 02:00:52.080]   who left Google, the Google car autonomous car, eventually what became Waymo wasn't
[02:00:52.080 --> 02:00:55.920]   Waymo at the time, but left took documents and apparently attempted or allegedly took
[02:00:55.920 --> 02:01:01.600]   documents and is accused of trying to bring his entire team with him when he went over to auto.
[02:01:01.600 --> 02:01:04.960]   The autonomous truck startup was then bought by uber.
[02:01:04.960 --> 02:01:09.760]   Of course, Google's suing uber saying, you know, Levendowski took everything.
[02:01:10.560 --> 02:01:20.480]   One of the things we've learned in this suit, Levendowski was paid $120 million by Google.
[02:01:20.480 --> 02:01:23.920]   And he still left.
[02:01:23.920 --> 02:01:29.440]   Well, yeah, I'd work for six months for $120 million.
[02:01:29.440 --> 02:01:30.800]   It just be all right.
[02:01:30.800 --> 02:01:31.360]   I'm good.
[02:01:31.360 --> 02:01:31.520]   Thanks.
[02:01:31.520 --> 02:01:34.800]   100 for one month.
[02:01:34.800 --> 02:01:36.080]   We knew Google paid well.
[02:01:38.080 --> 02:01:41.120]   I would literally push the self driving car everywhere it went.
[02:01:41.120 --> 02:01:44.640]   Wow.
[02:01:44.640 --> 02:01:46.000]   And he left.
[02:01:46.000 --> 02:01:47.440]   I need to talk to you about my pay.
[02:01:47.440 --> 02:01:48.720]   And he was unhappy.
[02:01:48.720 --> 02:01:52.080]   He said, I got a better job over here.
[02:01:52.080 --> 02:01:54.240]   Okay.
[02:01:54.240 --> 02:01:56.240]   I want to work for it with a more ethical company.
[02:01:56.240 --> 02:01:57.040]   So I want to uber.
[02:01:57.040 --> 02:02:04.000]   You know, at a certain point in time, money doesn't mean it's meaningless.
[02:02:04.000 --> 02:02:06.160]   It's meaning it's big numbers that don't mean anything.
[02:02:07.120 --> 02:02:10.240]   But I wish I could be in that group of people, though, don't you?
[02:02:10.240 --> 02:02:13.120]   We're at 120 million just numbers.
[02:02:13.120 --> 02:02:14.080]   Yeah.
[02:02:14.080 --> 02:02:16.240]   I just what would I do?
[02:02:16.240 --> 02:02:17.760]   I guess I could donate a lot more money.
[02:02:17.760 --> 02:02:22.320]   I read or heard somewhere that somebody calculated that 20 million
[02:02:22.320 --> 02:02:24.720]   to anything above $20 million doesn't matter.
[02:02:24.720 --> 02:02:28.320]   Because you live in the same house, the same you eat at the same restaurant,
[02:02:28.320 --> 02:02:29.360]   you do all the same stuff.
[02:02:29.360 --> 02:02:31.680]   It's like beyond 20 million after the first 20 million.
[02:02:31.680 --> 02:02:32.480]   It's all just the same.
[02:02:32.480 --> 02:02:34.560]   That's what comedian Dana Carvey said.
[02:02:34.560 --> 02:02:37.040]   You know, he made his fortune on Saturday at Live playing
[02:02:37.040 --> 02:02:38.240]   characters like George Bush.
[02:02:38.240 --> 02:02:42.800]   And he said, really, after a certain point, it just means you got a bigger bedroom to watch TV.
[02:02:42.800 --> 02:02:46.400]   And yeah, that's kind of.
[02:02:46.400 --> 02:02:47.200]   I didn't hear you up.
[02:02:47.200 --> 02:02:49.360]   So that's not true.
[02:02:49.360 --> 02:02:52.720]   Cause then you can just buy bigger toys like at 20 million dollars,
[02:02:52.720 --> 02:02:55.600]   you probably can't have your own private jet and have it be economical.
[02:02:55.600 --> 02:02:56.080]   Oh, that's true.
[02:02:56.080 --> 02:02:56.960]   But you could have a yacht.
[02:02:56.960 --> 02:02:58.720]   Really?
[02:02:58.720 --> 02:02:59.440]   For 20 million?
[02:02:59.440 --> 02:03:00.320]   Well, not a big yacht.
[02:03:00.320 --> 02:03:03.520]   I mean, yachts take a lot longer than just get anywhere.
[02:03:04.480 --> 02:03:04.880]   You're right.
[02:03:04.880 --> 02:03:05.600]   See 20 million.
[02:03:05.600 --> 02:03:05.920]   It's not enough.
[02:03:05.920 --> 02:03:06.640]   I got to keep working.
[02:03:06.640 --> 02:03:07.360]   You're right.
[02:03:07.360 --> 02:03:07.920]   Inflation.
[02:03:07.920 --> 02:03:08.480]   Inflation.
[02:03:08.480 --> 02:03:10.000]   What am I saying?
[02:03:10.000 --> 02:03:10.320]   Okay.
[02:03:10.320 --> 02:03:11.520]   Are we moving?
[02:03:11.520 --> 02:03:12.160]   Are we going?
[02:03:12.160 --> 02:03:13.120]   I'm getting hungry.
[02:03:13.120 --> 02:03:13.760]   Yeah, we're going.
[02:03:13.760 --> 02:03:15.280]   We're doing numbers and all that stuff.
[02:03:15.280 --> 02:03:15.680]   We're done.
[02:03:15.680 --> 02:03:16.000]   Yeah.
[02:03:16.000 --> 02:03:17.040]   Numbers.
[02:03:17.040 --> 02:03:17.920]   You want to do a number?
[02:03:17.920 --> 02:03:18.800]   Go do number.
[02:03:18.800 --> 02:03:22.720]   I got some quick ones just to leave the minds boggled when we close on a high note.
[02:03:22.720 --> 02:03:23.040]   Okay.
[02:03:23.040 --> 02:03:25.920]   So comparing the market cap of Tesla and Ford.
[02:03:25.920 --> 02:03:30.400]   Tesla is now more valuable than Ford.
[02:03:30.400 --> 02:03:30.480]   Wow.
[02:03:30.480 --> 02:03:34.320]   50 billion dollars is Tesla's market cap as of about an hour ago.
[02:03:34.320 --> 02:03:37.920]   And Ford's is 44.7 billion.
[02:03:37.920 --> 02:03:41.760]   This is in part because Tesla had an amazing quarter.
[02:03:41.760 --> 02:03:42.160]   Okay.
[02:03:42.160 --> 02:03:44.080]   Let's compare two other companies.
[02:03:44.080 --> 02:03:48.080]   Walmart's of course is retail giant.
[02:03:48.080 --> 02:03:51.440]   And their market cap is $218 billion.
[02:03:51.440 --> 02:03:56.880]   Amazon's is now $435 billion.
[02:03:56.880 --> 02:03:57.360]   Wow.
[02:03:59.360 --> 02:04:00.560]   Wow.
[02:04:00.560 --> 02:04:07.440]   This is the power of the tech slash digital slash algorithmly based businesses.
[02:04:07.440 --> 02:04:10.240]   Hey, even it comes with its own soundtrack.
[02:04:10.240 --> 02:04:11.360]   I like that about you.
[02:04:11.360 --> 02:04:11.600]   Yeah.
[02:04:11.600 --> 02:04:12.480]   Yes.
[02:04:12.480 --> 02:04:16.640]   And then I have one last mind boggling number.
[02:04:16.640 --> 02:04:18.480]   The number is 800,000.
[02:04:18.480 --> 02:04:18.880]   What's that?
[02:04:18.880 --> 02:04:24.000]   800,000 is the number of police officers in the United States.
[02:04:24.000 --> 02:04:24.480]   Roughly.
[02:04:24.480 --> 02:04:24.880]   Roughly.
[02:04:24.880 --> 02:04:26.000]   Give or take 100,000.
[02:04:27.120 --> 02:04:30.720]   Exxon, which is the company formerly known as Taser,
[02:04:30.720 --> 02:04:36.400]   has offered to provide every single police officer in the United States with a body cam.
[02:04:36.400 --> 02:04:41.360]   And with the training and other infrastructure that would support that.
[02:04:41.360 --> 02:04:47.520]   They offered every single police officer a free body cap for a year.
[02:04:47.520 --> 02:04:54.160]   800,000 is the number of police officers that technically would be covered if every single one of them
[02:04:54.160 --> 02:04:55.360]   took them up on their office.
[02:04:55.360 --> 02:04:56.480]   Oh, offer.
[02:04:56.480 --> 02:04:58.720]   So that's kind of amazing, I think.
[02:04:58.720 --> 02:04:59.040]   Wow.
[02:04:59.040 --> 02:05:04.240]   In other words, I think that because of this stunt,
[02:05:04.240 --> 02:05:07.360]   I think we're going to see a lot more cops with body camps.
[02:05:07.360 --> 02:05:07.840]   I hope so.
[02:05:07.840 --> 02:05:11.440]   I think that already people are saying, well, that's a Trojan horse.
[02:05:11.440 --> 02:05:12.400]   We don't want to take it.
[02:05:12.400 --> 02:05:17.440]   Taser makes, I mean, acts on makes tasers, which a lot of police officers use.
[02:05:17.440 --> 02:05:18.480]   Right.
[02:05:18.480 --> 02:05:21.760]   I would love, I would love, I think I've even said this on the show before.
[02:05:21.760 --> 02:05:28.080]   I would love for Taser to build a Taser that has a camera in it that shows the
[02:05:28.080 --> 02:05:30.800]   taste victim in slow-mo as they're being tased.
[02:05:30.800 --> 02:05:32.080]   But that's just mean.
[02:05:32.080 --> 02:05:33.520]   For amusement purposes only.
[02:05:33.520 --> 02:05:38.640]   It's valuable for leisure or legal issues because they.
[02:05:38.640 --> 02:05:40.880]   But it's more fun that slow motion watch.
[02:05:40.880 --> 02:05:41.760]   Yeah.
[02:05:41.760 --> 02:05:41.920]   Yeah.
[02:05:41.920 --> 02:05:42.080]   It's.
[02:05:42.080 --> 02:05:42.480]   Yeah.
[02:05:42.480 --> 02:05:48.880]   Unless it's a reporter on a nightly news show doing it for ratings, it's not funny.
[02:05:48.880 --> 02:05:51.520]   We did it on the new screen on the old screen savers.
[02:05:51.520 --> 02:05:53.040]   We tased Yoshi.
[02:05:53.040 --> 02:05:55.680]   We had the guy with Taser who was like dressed in a black suit with a black
[02:05:55.680 --> 02:05:56.080]   turtleneck.
[02:05:56.080 --> 02:05:57.760]   He looked like a blonde and black woman.
[02:05:57.760 --> 02:05:58.960]   Like a guy who worked at Taser.
[02:05:58.960 --> 02:05:59.520]   Yeah.
[02:05:59.520 --> 02:06:01.120]   And he shot Yoshi.
[02:06:01.120 --> 02:06:01.360]   Yeah.
[02:06:01.360 --> 02:06:03.200]   Yoshi just giggled the whole way down.
[02:06:03.200 --> 02:06:14.080]   Stacy, what's your pick of the week?
[02:06:14.080 --> 02:06:17.360]   So I have a bunch of devices, but they are under embargo.
[02:06:17.360 --> 02:06:18.560]   So I have two things.
[02:06:18.560 --> 02:06:21.920]   One, you're not going to believe in, but one is an actual app.
[02:06:21.920 --> 02:06:25.200]   So I don't know if this has existed for a long time,
[02:06:25.200 --> 02:06:29.680]   but I came home last week from being at the Bluetooth World Conference.
[02:06:29.680 --> 02:06:32.240]   And this was in my house and oh my god, it is awesome.
[02:06:32.240 --> 02:06:35.520]   This is a it's a dish soap holder.
[02:06:35.520 --> 02:06:36.960]   Don't don't hate me, you guys.
[02:06:36.960 --> 02:06:39.120]   The IOT dish soap holder.
[02:06:39.120 --> 02:06:41.360]   No, it is it is not an IOT dish soap holder.
[02:06:41.360 --> 02:06:42.960]   It is there's nothing connected.
[02:06:42.960 --> 02:06:45.680]   But instead of having our dish soap brush.
[02:06:45.680 --> 02:06:45.840]   Yeah.
[02:06:47.200 --> 02:06:49.760]   Sit on the countertop, which is where it's always sat.
[02:06:49.760 --> 02:06:50.160]   Yeah.
[02:06:50.160 --> 02:06:51.680]   There's a little stand for it now.
[02:06:51.680 --> 02:06:55.280]   I know very low tech.
[02:06:55.280 --> 02:06:56.640]   It's only 1159.
[02:06:56.640 --> 02:06:58.480]   It's from Oxo.
[02:06:58.480 --> 02:06:59.760]   Of course, it's from Oxo.
[02:06:59.760 --> 02:07:00.560]   They're so good.
[02:07:00.560 --> 02:07:04.160]   So yes, I know this is not high tech.
[02:07:04.160 --> 02:07:07.520]   And maybe this is this has existed for a long time,
[02:07:07.520 --> 02:07:09.280]   but I have never seen one before.
[02:07:09.280 --> 02:07:10.800]   And I was really excited.
[02:07:10.800 --> 02:07:14.000]   I was excited when I learned that you could put the dish soap in the handle,
[02:07:14.000 --> 02:07:16.160]   let alone that there's a stand.
[02:07:16.160 --> 02:07:18.240]   And now you don't have to have it like,
[02:07:18.240 --> 02:07:20.640]   you know, laying on the side getting all grungy.
[02:07:20.640 --> 02:07:21.440]   Anyway, okay.
[02:07:21.440 --> 02:07:25.520]   So I also offer you a more tech thing.
[02:07:25.520 --> 02:07:26.960]   And this is not a new app,
[02:07:26.960 --> 02:07:29.280]   but it is an app that I've been really playing with.
[02:07:29.280 --> 02:07:29.520]   Yes.
[02:07:29.520 --> 02:07:32.480]   So it's FitStar, which.
[02:07:32.480 --> 02:07:33.440]   Oh, I love FitStar.
[02:07:33.440 --> 02:07:34.960]   Yeah, FitBitbottom.
[02:07:34.960 --> 02:07:35.520]   Yep.
[02:07:35.520 --> 02:07:38.000]   I have totally like, I'm going to shell out the,
[02:07:38.000 --> 02:07:40.240]   I think I might just go for the whole year.
[02:07:40.240 --> 02:07:42.720]   You just want Tony Gonzalez on your smartphone.
[02:07:42.720 --> 02:07:43.360]   I know you.
[02:07:44.160 --> 02:07:47.920]   I, yeah, actually he is my preferred trainer other than the other lady.
[02:07:47.920 --> 02:07:48.720]   No, he's cute.
[02:07:48.720 --> 02:07:52.400]   But, but yes, this is great.
[02:07:52.400 --> 02:07:55.280]   And I love the exercises and I can do them in hotel rooms.
[02:07:55.280 --> 02:07:56.080]   It's really good.
[02:07:56.080 --> 02:07:59.200]   I really, and they time it and they ask you how hard it was.
[02:07:59.200 --> 02:08:02.080]   And so they can tailor it to your abilities.
[02:08:02.080 --> 02:08:02.960]   No, it's really good.
[02:08:02.960 --> 02:08:03.920]   I love FitStar.
[02:08:03.920 --> 02:08:05.440]   It's the best of all these apps, I think.
[02:08:05.440 --> 02:08:07.360]   But it's super pricey.
[02:08:07.360 --> 02:08:09.520]   So I think I'm just going to do the $40 a year
[02:08:09.520 --> 02:08:11.600]   because otherwise it's like $8 a month.
[02:08:11.600 --> 02:08:11.840]   Yeah.
[02:08:11.840 --> 02:08:12.840]   Yeah.
[02:08:12.840 --> 02:08:13.840]   I was like, what is that?
[02:08:13.840 --> 02:08:15.120]   $4 is bad for a year.
[02:08:15.120 --> 02:08:16.160]   That's not bad.
[02:08:16.160 --> 02:08:16.880]   No, that's great.
[02:08:16.880 --> 02:08:18.480]   But $8 a month felt like a lot.
[02:08:18.480 --> 02:08:19.520]   You know what I always say?
[02:08:19.520 --> 02:08:20.880]   It's cheaper than a heart attack.
[02:08:20.880 --> 02:08:23.280]   Yes.
[02:08:23.280 --> 02:08:24.480]   I always say that.
[02:08:24.480 --> 02:08:27.360]   It's also cheaper than buying new jeans.
[02:08:27.360 --> 02:08:27.680]   So.
[02:08:27.680 --> 02:08:29.600]   It's cheaper than even buying new jeans.
[02:08:29.600 --> 02:08:30.480]   Exactly.
[02:08:30.480 --> 02:08:31.040]   Exactly.
[02:08:31.040 --> 02:08:32.240]   All right.
[02:08:32.240 --> 02:08:35.440]   My thing is another Internet of Thing and it's just dumb.
[02:08:35.440 --> 02:08:40.800]   But it comes from dev.2, a startup Andrew Buntine.
[02:08:40.800 --> 02:08:45.280]   He figured out a way to have personalized theme music play
[02:08:45.280 --> 02:08:48.240]   for all his employees when they entered the office.
[02:08:48.240 --> 02:08:52.560]   So what happens is your phone joins the office Wi-Fi.
[02:08:52.560 --> 02:08:57.520]   It turns out some routers offer a log of when people join
[02:08:57.520 --> 02:08:58.560]   and leave the Wi-Fi.
[02:08:58.560 --> 02:09:00.480]   And no, I'm playing it.
[02:09:00.480 --> 02:09:01.520]   You could play it.
[02:09:01.520 --> 02:09:02.640]   This is the guy's theme music.
[02:09:02.640 --> 02:09:03.280]   He's coming in.
[02:09:03.280 --> 02:09:03.600]   Listen.
[02:09:03.600 --> 02:09:10.000]   He walks in the office in his theme music stream.
[02:09:10.880 --> 02:09:11.920]   That's awesome.
[02:09:11.920 --> 02:09:19.920]   And it's an easy thing to do because if your Wi-Fi router logs
[02:09:19.920 --> 02:09:24.240]   entrances and exits, he just wrote a little script that grabs the log,
[02:09:24.240 --> 02:09:31.520]   parses it, and then plays the appropriate music when somebody walks in.
[02:09:31.520 --> 02:09:33.520]   You got to do that a twit.
[02:09:33.520 --> 02:09:34.320]   Isn't that a great...
[02:09:34.320 --> 02:09:34.800]   I know.
[02:09:34.800 --> 02:09:35.680]   I want to do this.
[02:09:35.680 --> 02:09:39.600]   Maybe we'll get Father Robert and Brian to do it on know
[02:09:39.600 --> 02:09:41.680]   how that would be such a great project.
[02:09:41.680 --> 02:09:44.720]   Play theme music when you enter the office.
[02:09:44.720 --> 02:09:56.560]   Ladies and gentlemen, that concludes this week in getting Leo really sad,
[02:09:56.560 --> 02:09:57.760]   but cheering him up at the end.
[02:09:57.760 --> 02:10:03.440]   And I want to thank our partners, our team for making that happen.
[02:10:03.440 --> 02:10:06.800]   Stacey, it's great to have you.
[02:10:06.800 --> 02:10:12.800]   Stacey on IOT.com, the IOT podcast she does with Kevin Tofles at IOTpodcast.com.
[02:10:12.800 --> 02:10:13.440]   Mike Elgin.
[02:10:13.440 --> 02:10:14.960]   Oh, Mike.
[02:10:14.960 --> 02:10:16.720]   He's going to Barcelona.
[02:10:16.720 --> 02:10:17.440]   Join us.
[02:10:17.440 --> 02:10:20.320]   Getting ready for gastronomad.net, please.
[02:10:20.320 --> 02:10:22.000]   Please.
[02:10:22.000 --> 02:10:23.840]   I just want to do this so bad.
[02:10:23.840 --> 02:10:24.800]   Maybe next year.
[02:10:24.800 --> 02:10:25.840]   We'll have you out there in one of these.
[02:10:25.840 --> 02:10:26.080]   Yeah.
[02:10:26.080 --> 02:10:29.360]   Morocco one, I think, is the one I'd like to do, but I have so many travels,
[02:10:29.360 --> 02:10:30.000]   Blandum.
[02:10:30.000 --> 02:10:30.240]   Yeah.
[02:10:30.240 --> 02:10:31.600]   It's in a 400-year-old Riyadh.
[02:10:31.600 --> 02:10:32.720]   It's going to be fantastic.
[02:10:32.720 --> 02:10:33.680]   Oh, man.
[02:10:33.680 --> 02:10:35.920]   Are you going to have plain pasta so I can bring my kid?
[02:10:36.800 --> 02:10:38.160]   Yeah, I will.
[02:10:38.160 --> 02:10:38.960]   If you bring your kid.
[02:10:38.960 --> 02:10:40.000]   I have a kid like that too.
[02:10:40.000 --> 02:10:41.440]   You spaghetti with butter, right?
[02:10:41.440 --> 02:10:41.760]   That's it.
[02:10:41.760 --> 02:10:42.080]   Yeah.
[02:10:42.080 --> 02:10:45.520]   I'm like, she'll sit through the restaurants that we go to.
[02:10:45.520 --> 02:10:45.600]   Right.
[02:10:45.600 --> 02:10:47.200]   As long as we're spaghetti with butter.
[02:10:47.200 --> 02:10:47.440]   Yep.
[02:10:47.440 --> 02:10:49.520]   Or a plain hamburger.
[02:10:49.520 --> 02:10:51.600]   That's fun.
[02:10:51.600 --> 02:10:52.720]   Yes.
[02:10:52.720 --> 02:10:53.120]   Disgusting.
[02:10:53.120 --> 02:10:54.880]   What's wrong with kids?
[02:10:54.880 --> 02:10:57.520]   Some people say that's how you taste the meat.
[02:10:57.520 --> 02:10:58.400]   I'm not one of those people.
[02:10:58.400 --> 02:10:59.360]   I don't want to taste the meat.
[02:10:59.360 --> 02:11:01.920]   I want to taste the condiments.
[02:11:01.920 --> 02:11:05.200]   I want ketchup and pickles and tomatoes and everything else.
[02:11:05.760 --> 02:11:08.640]   I bet Mike Elgin's kids are gourmet's though.
[02:11:08.640 --> 02:11:10.800]   I bet the boys were brought up.
[02:11:10.800 --> 02:11:15.760]   I know Princess Squishy Face will be brought up tasting the finest food.
[02:11:15.760 --> 02:11:19.680]   She's already eyeball in the solid food and just you can tell she can't wait.
[02:11:19.680 --> 02:11:24.320]   Well, she's lucky Grandma Amira will make her anything she wants.
[02:11:24.320 --> 02:11:25.360]   Yep.
[02:11:25.360 --> 02:11:29.280]   We do this week in Google every Wednesday about 130 Pacific 430 Eastern.
[02:11:29.280 --> 02:11:30.560]   That's 2030 UTC.
[02:11:30.560 --> 02:11:34.160]   I'd love it if you'd join us live and join us in the chatroom at irc.tuit.tv
[02:11:34.160 --> 02:11:38.160]   or even come to the studio as our three intrepid visitors did.
[02:11:38.160 --> 02:11:39.760]   Just email tickets@twit.tv.
[02:11:39.760 --> 02:11:41.360]   We'll put a chair out for you.
[02:11:41.360 --> 02:11:43.280]   Of course, if you can't be here when we're here,
[02:11:43.280 --> 02:11:47.600]   you could easily get on-demand audio and video of everything we do at Twit
[02:11:47.600 --> 02:11:50.320]   on our website, twit.tv/twig.
[02:11:50.320 --> 02:11:54.560]   For this particular show, you can also subscribe and man,
[02:11:54.560 --> 02:11:55.280]   we're everywhere.
[02:11:55.280 --> 02:11:58.640]   Just find your favorite podcatcher and search for Twit.
[02:11:58.640 --> 02:12:02.960]   Subscribe to the shows you want and you will never be lacking for fine entertainment
[02:12:02.960 --> 02:12:04.320]   on your portable device.
[02:12:04.320 --> 02:12:06.240]   Thanks for being here.
[02:12:06.240 --> 02:12:12.800]   We'll see you next time on Twit.
[02:12:12.800 --> 02:12:18.080]   [Music]

