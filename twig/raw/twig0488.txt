;FFMETADATA1
title=The Year's Best
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=488
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.920]   Hey everybody, how are you? Great to see you. I hope you're having a wonderful holiday season. This is Boxing Day.
[00:00:06.920 --> 00:00:12.040]   I think that means you put everything back in the boxes and give it back. Isn't that? I think that's what it means.
[00:00:12.040 --> 00:00:16.760]   I'm not giving my presents back. I hope you've got something great for the holiday season.
[00:00:16.760 --> 00:00:23.060]   I have something great for you for this holiday season. Our best of, we've collected clips throughout the year.
[00:00:23.060 --> 00:00:26.120]   Great moments, great insights, great conversations.
[00:00:26.520 --> 00:00:32.360]   For all of you on this holiday season, the best of this week in Google. Enjoy.
[00:00:32.360 --> 00:00:39.240]   Netcasts you love. From people you trust.
[00:00:39.240 --> 00:00:44.680]   This is Twig.
[00:00:44.680 --> 00:00:56.200]   This is Twig. This week in Google, episode 488 for Wednesday, December 26th, 2018.
[00:00:56.840 --> 00:00:58.200]   The year's best.
[00:00:58.200 --> 00:01:06.520]   Well, happy holidays everybody. I wish Jeff Jarvis and Stacey Higginbotham could be here, but we gave them a week off,
[00:01:06.520 --> 00:01:14.680]   as I think is appropriate. On this Boxing Day, we want to, instead of doing a new show, give you some of the best moments from 2018.
[00:01:14.680 --> 00:01:18.680]   Thanks to our producers and Carson Bondi, especially who put all this together, our editors.
[00:01:18.680 --> 00:01:24.840]   We have some of the best moments of 2018. So I'm going to sit back and enjoy some eggnog.
[00:01:25.400 --> 00:01:33.240]   And as we travel back through time, some of the best moments from this week in Google, 2018. Enjoy.
[00:01:33.240 --> 00:01:37.800]   Speaking of stealing the deal, Logan Paul, god dang it.
[00:01:37.800 --> 00:01:46.520]   This is the YouTube, the, I'm going to say the nihilistic YouTube star,
[00:01:46.520 --> 00:01:52.600]   who is, you know, I mean, he and his brother have done things like burn mattresses in the swimming pool of their Beverly Hills home
[00:01:53.320 --> 00:01:59.000]   to generate YouTube views. They kind of, they took the cake on this one. They, he was in the famous
[00:01:59.000 --> 00:02:08.120]   suicide forest in Japan and posted a video of a suicide, which I won't show, of course. In fact,
[00:02:08.120 --> 00:02:11.400]   it's, he took it down after three days and 6.7 million views.
[00:02:11.400 --> 00:02:21.000]   But, you know, I think the point is clear here. A, you, as some have pointed out, no AI is going to be able to find
[00:02:21.480 --> 00:02:28.840]   that video and block it. And there is an intense race now to become the next most extreme thing.
[00:02:28.840 --> 00:02:32.200]   You saw it last through with PewDiePie and his racist stuff.
[00:02:32.200 --> 00:02:38.520]   Yeah, let me, let me try to find some good news in this, which is that our norms
[00:02:38.520 --> 00:02:44.120]   survived this, that he was roundly condemned at all quarters.
[00:02:44.120 --> 00:02:47.640]   PewDiePie, both in Logan Paul were, yeah.
[00:02:47.640 --> 00:02:53.240]   And, and I might have ruined his chances to make money. That's up to, to YouTube. I think they should ruin it.
[00:02:53.240 --> 00:02:59.800]   And so, you know, there, there are limits and that's what we're setting. It's what we're negotiating.
[00:02:59.800 --> 00:03:05.480]   It's what we're trying to figure out. Now, can we automate those limits? That's, that's hard at scale.
[00:03:05.480 --> 00:03:10.280]   The scale where we are is very hard, but, but norms one, civil, civilization one,
[00:03:10.280 --> 00:03:14.840]   civilization didn't say, oh, okay, what's the big deal? It's a video, bro. No, civilization said,
[00:03:14.840 --> 00:03:18.360]   F you, Mr. Paul, right, and made it take it down.
[00:03:18.360 --> 00:03:24.440]   But I think he'll have long term harm or maybe it will, in fact, help build his brand.
[00:03:24.440 --> 00:03:30.280]   Well, that's, that's, say with PewDiePie, that's up to, and great measure, the platforms, whether
[00:03:30.280 --> 00:03:34.840]   they choose to have their brands associated with the likes of this. If I were a brand, I would say,
[00:03:34.840 --> 00:03:37.480]   no, you know, one strike, you're out. No.
[00:03:37.480 --> 00:03:43.160]   Shooting pie is still on YouTube. All they did was they took him off of their VIP advertising program,
[00:03:43.160 --> 00:03:48.760]   which did cost him millions of dollars at ad revenue. You, he only made 15 million last year.
[00:03:48.760 --> 00:03:54.680]   Yeah. I just feel like there's such a, there's such pressure. It's like link bait.
[00:03:54.680 --> 00:04:02.520]   And when it was mainstream media, I think there were, you know, societal restraints that kept them
[00:04:02.520 --> 00:04:07.240]   from going too crazy. I mean, we, they went crazy enough with reality TV and stuff,
[00:04:07.240 --> 00:04:13.640]   but I feel like on YouTube, first of all, it happened so fast that, and it's the issue.
[00:04:13.640 --> 00:04:17.960]   Yeah. It happens so fast that it's, I mean, three days he gets 6.7 million views. Those
[00:04:17.960 --> 00:04:21.960]   views cannot be taken back. I don't know how many kids saw. I don't know how many kids were
[00:04:21.960 --> 00:04:28.200]   damaged by it or upset by it. Well, the other question there is, is how long did it take people
[00:04:28.200 --> 00:04:32.520]   to report this? That's, that's to me the most extremely part. Then the second is how long to
[00:04:32.520 --> 00:04:37.160]   take YouTube react. Apparently they, it was reported. Some say it was reported and YouTube did not react.
[00:04:37.480 --> 00:04:43.000]   And that's YouTube. That's not bad on YouTube. You know, if it's, if it's ramping up that fast,
[00:04:43.000 --> 00:04:47.640]   it would probably be in the top, you know, fraction of a percent of videos on YouTube. And so,
[00:04:47.640 --> 00:04:53.080]   therefore, it should be on to review Q quickly, you know, YouTube, yeah, YouTube has a lot of
[00:04:53.080 --> 00:04:57.560]   videos and you can't review them all. We know that, but it should be to review the ones that are
[00:04:57.560 --> 00:05:03.800]   ramping up the trending trending. Well, you both have teenage sons as, or young sons as I do,
[00:05:04.440 --> 00:05:10.840]   and it, and it, it's 23. Still just, no, no, they're not teenagers more, but 21 and 22.
[00:05:10.840 --> 00:05:16.360]   Yeah. And same, Jake is about that age. 25. 25. So I'm on the boat with these kids and I'm watching
[00:05:16.360 --> 00:05:20.760]   the videos they like and they have a very, and they've developed a very extreme taste for videos
[00:05:20.760 --> 00:05:26.760]   on YouTube. That's, this is pushed them to a more extreme taste. And the stuff they watch is,
[00:05:26.760 --> 00:05:30.280]   and they were telling me about the people they like. They, they, they watch a guy who eats hot
[00:05:30.280 --> 00:05:34.680]   peppers, hottest pepper possible. When you watch him in agony as he's eating this pepper, they love
[00:05:34.680 --> 00:05:41.320]   that they laugh. They howl. There's some other guy who is just completely an anarchist. He has a
[00:05:41.320 --> 00:05:47.560]   beautiful set starts a show before he gets his first guest on, he destroys the set. And, and then
[00:05:47.560 --> 00:05:51.720]   does the show. And then, and there's this appetite for more and more extreme stuff. And the, and
[00:05:51.720 --> 00:05:55.640]   the kids eat it up. They love it. That's what they want. And then, of course, when they try to
[00:05:55.640 --> 00:05:59.080]   watch something like this, it's like, well, did nothing happens. You didn't, you didn't blow anything
[00:05:59.080 --> 00:06:03.320]   up. What's going on? But even David Letterman back of the day threw things off the roof.
[00:06:03.320 --> 00:06:08.200]   Yeah. You know, and I have, I have a bit's respect for him. But yeah, he was, he was in essence doing
[00:06:08.200 --> 00:06:12.120]   the baby version of the same thing. Yeah. But I'm just saying that mainstream media,
[00:06:12.120 --> 00:06:17.400]   because they were run, I'm not praising mainstream media by any means, but because they were run by
[00:06:17.400 --> 00:06:21.000]   big corporations like General Electric, they had a reputation to maintain. There was always a
[00:06:21.000 --> 00:06:26.360]   little bit of a lid on this. I mean, obviously Google and YouTube are big corporations who have
[00:06:26.360 --> 00:06:31.240]   a reputation to maintain. But I don't, I don't know. I mean, I feel like there's such incentive
[00:06:31.240 --> 00:06:34.200]   to get more and more extreme. I don't see it going the other way. I don't see it coming.
[00:06:34.200 --> 00:06:38.360]   No, on a issue of scale. So it's, it's, it's that case on YouTube of the
[00:06:38.360 --> 00:06:43.880]   ridiculous videos that figured out how to manipulate the system. Right. Yeah.
[00:06:43.880 --> 00:06:52.840]   Well, the stuff like synthesized kids cartoon videos,
[00:06:53.320 --> 00:07:00.680]   right. Generally, that stuff is creepy. There's, there's a sort of, and this is in Charlie's talk
[00:07:00.680 --> 00:07:08.520]   as well, there's people are now starting to throw weird, weird AI machinery at these things to try
[00:07:08.520 --> 00:07:13.800]   makes, makes stuff up that's, that's more convincing. But also they're attacking,
[00:07:13.800 --> 00:07:17.800]   it's attacking multiple levels. It is this weird thing of what they're actually trying to do is
[00:07:17.800 --> 00:07:22.600]   game other computers. So it's sort of adversarial AI fighting AI. They're trying to game the ranking
[00:07:22.600 --> 00:07:26.680]   other in, in YouTube. They're trying to game the censorship algorithm in YouTube as well,
[00:07:26.680 --> 00:07:31.880]   trying to do something that looks enough like the things that, that should be let through,
[00:07:31.880 --> 00:07:36.760]   but will, will follow the ranking algorithms and therefore generate ad revenue.
[00:07:36.760 --> 00:07:42.600]   It's very, very strange. Nick Bostrom was kind of sort of right, but it's instead of paper
[00:07:42.600 --> 00:07:47.960]   clips, it's the fore-chanting of the world. The entire world is just going to turn into fore-chan.
[00:07:50.600 --> 00:07:56.360]   This is why I'm going to go back to my good news thing. There was also a study out this week that
[00:07:56.360 --> 00:08:00.120]   said that, you know, all of the talk about fake news in my world and, you know, I've got money to
[00:08:00.120 --> 00:08:06.440]   deal with it, said, you know, the impact is actually very small. Oh, I hope that's true. I hope that's
[00:08:06.440 --> 00:08:12.600]   true. And other study, and other study, these were both on the rundown, said that, let's see if I
[00:08:12.600 --> 00:08:17.960]   got this one straight, this one said that when, I'm not picking on Trump in this case or Trump voters,
[00:08:17.960 --> 00:08:23.240]   in this case, but when Trump voters, Trump fans were exposed to stories about the Russian
[00:08:23.240 --> 00:08:30.040]   connections, their opinion of Trump and trust in him decreased, which is to say that news has an
[00:08:30.040 --> 00:08:35.480]   impact on people. You put the two together and there is an opportunity for a flight to quality
[00:08:35.480 --> 00:08:41.160]   here. There is an opportunity for us to negotiate our norms for an improved society. You know,
[00:08:41.160 --> 00:08:45.880]   you're going to hear my spiel for the hundredth time. It's early days, 1479 Gutenberg years,
[00:08:45.880 --> 00:08:51.160]   we're figuring this out. We're smart as a civilization. We will figure it out. Society's not in the
[00:08:51.160 --> 00:08:59.320]   dumper quite yet. So this was an IOT CES, right? It is. That was the big theme of the show. If I'm
[00:08:59.320 --> 00:09:05.080]   pushing anything, it's all IOT all the time. Like a Michael, Mike multi-cooker with the assistant
[00:09:05.080 --> 00:09:09.720]   built in. Okay, let's not focus on the assistant. The big focus, everyone's like,
[00:09:10.360 --> 00:09:16.840]   "Voice is everywhere." What's really awesome about it is the fact that voices everywhere means we
[00:09:16.840 --> 00:09:21.800]   finally stop talking about standards. We're never going to get there on standards, but we found a
[00:09:21.800 --> 00:09:27.560]   way around it. You mean like Zigbee Z wave and all that? All of that, yes. In even things like
[00:09:27.560 --> 00:09:32.760]   device schemas, which super nerdy, but like saying things like, "I'm a light bulb. I can do the
[00:09:32.760 --> 00:09:37.560]   following things." Like my hope was there would be like a universal protocol that does all that.
[00:09:38.200 --> 00:09:42.920]   We never got there, but now because the Amazon Echo and Google Assistant and all are trying,
[00:09:42.920 --> 00:09:49.160]   they're building that for us. So it's not ideal. I would rather have it be more open, but it's where
[00:09:49.160 --> 00:09:54.920]   we are. Hey, cool. Tell us about some of the devices or things you saw that you thought were
[00:09:54.920 --> 00:10:01.000]   interesting. Okay, so you guys are going to be so mad at me, but I haven't had much of a chance
[00:10:01.000 --> 00:10:05.080]   to walk the show floor. So I've got a couple things that... Are you been doing meetings the
[00:10:05.080 --> 00:10:09.560]   whole time? I've been doing a lot of meetings. Kevin is the guy walking the floor.
[00:10:09.560 --> 00:10:14.920]   Okay. I do have some cool stuff. I would never call Kevin a floor walker, but okay.
[00:10:14.920 --> 00:10:19.400]   Oh, that's what he excels at that. He walks through and he's just like talking to a
[00:10:19.400 --> 00:10:30.600]   runner. He's a floor runner. Exactly. Well, that's true. Yeah. Kevin Tofo.
[00:10:30.600 --> 00:10:39.080]   He's my partner. I know. It's awesome. No, it's fun. So the thing was thing.
[00:10:39.080 --> 00:10:45.720]   Okay. I'm trying to decide. Okay. One for normal people. I love the fact that both
[00:10:45.720 --> 00:10:52.040]   Kohler and Delta are doing faucets with not just Amazon Echo enabled, but are doing it
[00:10:52.040 --> 00:10:55.160]   to add additional values. So you can tell your faucet
[00:10:55.160 --> 00:10:59.320]   dispense a cup of water and it will do it. Exactly a cup.
[00:10:59.320 --> 00:11:04.920]   Exactly a cup. Or you can even train it like my spaghetti pot. I can train it like fill the
[00:11:04.920 --> 00:11:09.320]   spaghetti pot and that means fill this much water. That's cool. And then later on I could do it. So I
[00:11:09.320 --> 00:11:17.000]   love that. I also saw some cool. This is super geeky, but Bosch was showing off a MEMS projector
[00:11:17.000 --> 00:11:24.520]   that basically is this tiny 30 by 70 millimeter module. And what it can do is it can project
[00:11:24.520 --> 00:11:33.560]   a display at 480 to 720p. It can even run video and you can actually interact with that display
[00:11:33.560 --> 00:11:40.200]   on any surface, including your skin. So your couch, your skin, your kitchen table,
[00:11:40.200 --> 00:11:45.560]   like an Amazon Echo like device projected an image of the people walking up to your door.
[00:11:45.560 --> 00:11:53.000]   So you don't need a screen. It almost could just appear. Yes. That's exactly it. I like that.
[00:11:53.000 --> 00:11:58.120]   It's awesome. Is that available this year or is it a concept or?
[00:11:58.680 --> 00:12:03.800]   No, it's an actual working, it's a MEMS product. So that's a semiconductor product. So people will
[00:12:03.800 --> 00:12:08.040]   have to build that into their devices. So you won't see that for another, I don't know, 18 months
[00:12:08.040 --> 00:12:17.400]   to two years. Who knows? Wow, that's cool. Low low power. It's relatively low power. What's unique,
[00:12:17.400 --> 00:12:22.600]   the most unique thing about it is its form factor is now small enough and it can project on any
[00:12:22.600 --> 00:12:29.080]   surface without messing messing things up. Look at you finding these things.
[00:12:29.080 --> 00:12:35.800]   That's a car. It's good. It can do video. It can show video. Actually, I don't think this is
[00:12:35.800 --> 00:12:44.920]   actually, that's the accelerometer. But that's okay. I did learn about the accelerometer. I can
[00:12:44.920 --> 00:12:49.240]   tell you why that's cool if you really want to know. They use MEMS for that as well.
[00:12:50.360 --> 00:12:55.160]   Yeah, MEMS are, I always explain MEMS to people, like MEMS are the hidden heroes of the Internet
[00:12:55.160 --> 00:13:01.160]   of Things. Because MEMS are these tiny things that take the analog real world signals of every day
[00:13:01.160 --> 00:13:07.080]   and they convert it to digital for the computer street. So you've got the real world meets the
[00:13:07.080 --> 00:13:15.880]   computer in this one tiny little device. This projector uses lasers too, which is cool. Lasers.
[00:13:18.120 --> 00:13:23.960]   So we were talking about this Bosch bathroom because they've got a toilet, they've got a shower,
[00:13:23.960 --> 00:13:29.560]   they've got a tub, they've got a faucet. I do like the idea of, you came up with it.
[00:13:29.560 --> 00:13:34.520]   And I mentioned, well, the idea is you tell it what temperature you like the shower to be and
[00:13:34.520 --> 00:13:40.680]   have it warmed up and ready for you and that kind of stuff. But now you know that the central
[00:13:40.680 --> 00:13:44.680]   hall has had a complete blackout for about an hour and a half. It's just now coming back.
[00:13:45.720 --> 00:13:52.520]   Kevin told me that. He's like, I had to move. What would happen to your fancy bathroom if you
[00:13:52.520 --> 00:13:58.120]   couldn't use it? It's a bad purpose. All right, guys, here's what happens. It
[00:13:58.120 --> 00:14:03.960]   reverse back to state. So I have a faucet. It has a faucet. Yeah, I have a faucet that's a
[00:14:03.960 --> 00:14:10.680]   capacitive touch faucet. And when the power goes out, then I use the hands. You just have to live like a
[00:14:12.040 --> 00:14:19.320]   yeah, it's like a savage. We actually can't use anything. Now we can't because we have a
[00:14:19.320 --> 00:14:25.320]   generator, but we have well water. Yeah. So if we lose power, we lose the water too. Right.
[00:14:25.320 --> 00:14:30.440]   No, yeah, that's bad. So it's like when an escalator breaks, it just becomes stairs.
[00:14:30.440 --> 00:14:38.200]   Exactly. What else? What else? Come on, you're our eyes and ears in Vegas.
[00:14:39.160 --> 00:14:44.440]   More cool stuff. More cool stuff. You put how long have you been there? When did you get there?
[00:14:44.440 --> 00:14:51.320]   Monday. Okay. Okay, go to our Instagram. And I'm not doing that to advertise, but I'm doing it to
[00:14:51.320 --> 00:14:58.680]   what's your Instagram? It's Stacy on it's Instagram, whatever slash Stacy and IOT. Because then you'll see
[00:14:58.680 --> 00:15:08.920]   everything that Kevin and I have seen, basically. Okay, so you saw these little Sony smart speakers
[00:15:08.920 --> 00:15:12.040]   with Google Assistant. Nice. Those are not exciting.
[00:15:12.040 --> 00:15:19.080]   Here's the Samsung smart thing refrigerator. So that's a really big deal in like
[00:15:19.080 --> 00:15:25.640]   Samsung three years ago, Samsung was like, IOT is going to happen. It's going to be open. It's
[00:15:25.640 --> 00:15:30.920]   going to be awesome. And we it was very kumbaya and Kevin and I made total fun of it along with
[00:15:30.920 --> 00:15:37.400]   the rest of the industry. This year, they actually are making that happen and they're doing it at
[00:15:37.400 --> 00:15:43.240]   a higher level. So three years ago, they talked all about like, Tizen and stuff like that. Normal
[00:15:43.240 --> 00:15:51.000]   people were probably like, Now they're basically they've integrated all of their like 60 some odd
[00:15:51.000 --> 00:15:56.520]   apps into the smart things cloud. They built all of their back and cloud stuff is now smart things.
[00:15:56.520 --> 00:16:02.440]   And this is the TV showing this. Yeah. Oh, that's cool. So your TV is going to be able to control
[00:16:02.440 --> 00:16:07.160]   all these crazy devices and it's going to have the same interface across all Samsung products. So
[00:16:07.160 --> 00:16:10.520]   it's a big freaking deal. And it doesn't have to be just Samsung. I see we have an
[00:16:10.520 --> 00:16:18.040]   Arlo camera on here. We have an air purifier. Like so many devices are already in a great
[00:16:18.040 --> 00:16:23.720]   out there. Kevin and believe it or not, this looks like a selfie, but no, he's looking in the mirror.
[00:16:23.720 --> 00:16:29.640]   This is a smart mirror. Oh, see, it's got the temperature.
[00:16:29.640 --> 00:16:36.200]   It's the music. So you're in the bathroom. You're looking at the mirror. Nice. It can be connected to
[00:16:36.200 --> 00:16:41.000]   a scale and a skin sensor. So you can see that's from higher. That's kind of cool.
[00:16:41.000 --> 00:16:46.520]   I'm getting I'm getting a smart mirror ship to me. I think next week. So I'll bring it on the show
[00:16:46.520 --> 00:16:53.080]   either next week or the week after I love this. I would get a build one. How do you how do you do
[00:16:53.080 --> 00:16:59.240]   it? Is it like something behind the mirror that shines through? I guess it is. Oh, the screen on
[00:16:59.240 --> 00:17:05.400]   the mirror? Yes. I don't. I stayed in a hotel once it had a mirror. But when you turn on the TV,
[00:17:06.360 --> 00:17:09.560]   it was shined through the mirror. It's just like one way mirror or something.
[00:17:09.560 --> 00:17:15.080]   There was a Google engineer who built one with a Raspberry. Oh, I remember that. Yeah, we talked
[00:17:15.080 --> 00:17:20.680]   about it. Oh, yeah. Yeah. All right. Higher. I'm sorry. Huawei. You can't use these because they're
[00:17:20.680 --> 00:17:27.560]   from China. Has a bunch of sensors, cameras, motion detectors. I'm just going through.
[00:17:27.560 --> 00:17:34.600]   Yeah, the TDK sensors. There's tons of sensors. Everybody has a smart home section.
[00:17:35.560 --> 00:17:40.520]   I mean, I don't know what else to say about that. Lenovo, I'm sure we'll get there eventually,
[00:17:40.520 --> 00:17:45.160]   but Lenovo has a very nice Google Assistant. It's the Echo Show for the Google Assistant.
[00:17:45.160 --> 00:17:50.040]   We talked about that and I'm excited to say it won't be out till the summer. So it's beautiful.
[00:17:50.040 --> 00:17:54.920]   It's good. It's affordable. Oh, you heard it. Yeah, it's got a 10 watt speaker in it.
[00:17:54.920 --> 00:18:00.600]   Yeah. The big ones, 249, the small ones, 199, I think. That's a very competitive price. That's
[00:18:00.600 --> 00:18:07.400]   basically an Echo Show, which is a seven inch screen. I like that. It looks good. I think
[00:18:07.400 --> 00:18:14.120]   in Google, I do believe Google will probably win this race if you start thinking about what
[00:18:14.120 --> 00:18:17.960]   needs to happen. They're starting to pull ahead. You hear you are at the WeMo booth. And of course,
[00:18:17.960 --> 00:18:23.960]   WeMo announced a home kit compatibility across the board, which is a big deal because Apple
[00:18:23.960 --> 00:18:28.600]   HomeKit has been very slow off the block. But I noticed a big hey Google sign in the back there.
[00:18:28.600 --> 00:18:35.480]   So obviously they also work with Google. Yeah. And I'm kind of down on HomeKit. There's a lot
[00:18:35.480 --> 00:18:41.160]   of HomeKit compatible stuff as usual. Apple is not here, but I just found out that earlier this
[00:18:41.160 --> 00:18:47.320]   summer, I'm trying to get rid of that blue line there, earlier this summer, Lenar had partnered
[00:18:47.320 --> 00:18:53.160]   with Apple for HomeKit enabled buildings like smart new builds for homes. And they actually
[00:18:53.960 --> 00:18:59.000]   switched over to smart things now. So I was like, oh, that's no good.
[00:18:59.000 --> 00:19:06.520]   So I saw the Gebo there. Now I have to say my Gebo failed. And I have a dead Gebo, which is
[00:19:06.520 --> 00:19:11.560]   really the reality. But I wasn't too unhappy because I'm sure there's a way to turn this off,
[00:19:11.560 --> 00:19:18.120]   but I couldn't find it. The Gebo snored. I am not kidding.
[00:19:19.560 --> 00:19:29.480]   It was the first time. It didn't sound like it would just make little sounds every once in a while,
[00:19:29.480 --> 00:19:36.840]   which was disconcerting. That's to comfort you. It didn't comfort me. It was creepy. So I was
[00:19:36.840 --> 00:19:42.120]   kind of glad when it died. I have to contact the Gebo folks. And I don't know if I want a
[00:19:42.120 --> 00:19:47.880]   replacement or I might want my money back on it. How much was the Gebo? 500 bucks. That was a
[00:19:47.880 --> 00:19:51.320]   special Indiegogo price. I think it's more than that now.
[00:19:51.320 --> 00:19:59.000]   There are a ton of robots here. Are there? Did you see the curry that we was our advertiser?
[00:19:59.000 --> 00:20:03.560]   Are they there? I didn't see the curry. I want to say it was the Aurelius maybe.
[00:20:03.560 --> 00:20:09.880]   A-E-U-R. The curry was supposed to be out by now. And I'm worried that they aren't out.
[00:20:09.880 --> 00:20:15.080]   It's hard to build a good robot. It's hard. Stacy, did you see the Chloe video?
[00:20:16.600 --> 00:20:23.160]   LG. That didn't work. LG was trying to show their... Yes, yes. I saw that after the fact.
[00:20:23.160 --> 00:20:29.160]   The guy didn't couldn't get it to respond at all. So I was very interested. I don't know if you saw
[00:20:29.160 --> 00:20:38.280]   this. Johnson Controls has a beautiful thermostat that's a glass. The $319 Cortana thermostat.
[00:20:38.280 --> 00:20:45.640]   Yeah. GLAS. I loved it. Did you see it? I've seen it. It's pretty. What goal look for it?
[00:20:45.640 --> 00:20:51.320]   Because it's there. Gorgeous. No, no, it is gorgeous. I'm hoping that technology comes down in price.
[00:20:51.320 --> 00:20:58.120]   Because that changed the game in the way Nes didn't quite and making a thermostat look like a
[00:20:58.120 --> 00:21:05.720]   luxury object. Yes. Here's the solace, my home solace 250. Google Assistant or Echo enabled.
[00:21:05.720 --> 00:21:12.040]   And this controls everything. These are everywhere. Everything. This is a very common category.
[00:21:13.080 --> 00:21:18.680]   Yeah. Panels of stuff with little tiles on them that control all your devices and have a bajillion
[00:21:18.680 --> 00:21:28.680]   radios. Yes. The Fobaro, I like the name Smart Plug monitors. Yeah, with the USB charger. I like
[00:21:28.680 --> 00:21:32.920]   that because I was going to have to rip out all of my outlets and put USB chargers and now I can just
[00:21:32.920 --> 00:21:40.120]   pull those in. It's like a smart plug in from Fobaro. The Blink doorbell. Battery powered front door.
[00:21:40.120 --> 00:21:46.280]   So it competes with our sponsor Ring. Looks similar. It does. Amazon bought Ring.
[00:21:46.280 --> 00:21:51.160]   Sorry. Amazon bought Blink. Oh, so this is Amazon's company now.
[00:21:51.160 --> 00:21:57.960]   Now this is like Christmas. The 23rd or so came. You're right.
[00:21:57.960 --> 00:22:05.080]   And they don't charge a monthly fee for this storage. That's a big problem. I think with a lot
[00:22:05.080 --> 00:22:12.120]   of these devices is that they all add up. It's the storage fees. You get pretty soon.
[00:22:12.120 --> 00:22:16.440]   That is true. Hundreds of bucks. Here's the Netatmo smart home bot.
[00:22:16.440 --> 00:22:23.080]   It's like Lighthouse. But without the video, it's talked to it like a human being and then it does
[00:22:23.080 --> 00:22:29.880]   it. Could you switch the lights on? Oh, that's nice. Yeah. I'm just going through. Oh, the spire.
[00:22:29.880 --> 00:22:36.360]   I had a spire. I did too. And I was kind of like on this, but it's actually
[00:22:36.360 --> 00:22:43.240]   the new one is an activity tracker. So activity, respiration, heart rate. Oh,
[00:22:43.240 --> 00:22:48.440]   I feel like something else. And the coolest thing about this is you buy them in a pack of like eight
[00:22:48.440 --> 00:22:52.280]   or 10. Oh, this is a good one. Stick them in your underwear. Yeah. And they're washable.
[00:22:52.280 --> 00:22:56.600]   That's a good idea. Because that's the problem with all of these things. They're easy to lose.
[00:22:56.600 --> 00:23:00.040]   If you wash them, they break some. The one on the right is the original spire.
[00:23:00.040 --> 00:23:04.520]   So they've replaced it basically with a with a. Wait, I think this is an additional because the
[00:23:04.520 --> 00:23:09.720]   original spire didn't have all those features. Right. Right. Right. Did you say, did you say
[00:23:09.720 --> 00:23:14.440]   stick it in your underwear? Yes, she did. That is where you're supposed to stick it.
[00:23:14.440 --> 00:23:18.760]   Did that excite you, Matthew? Not exactly. No.
[00:23:21.320 --> 00:23:28.680]   Wait till you take my age. No, when you get to your age, the underwear sticks you.
[00:23:28.680 --> 00:23:36.760]   This is the Kevin, which is named after your co-host. Indeed. What does the Kevin do?
[00:23:36.760 --> 00:23:44.040]   Um, I don't know. It makes when you leave home and automatically make sound
[00:23:44.040 --> 00:23:48.520]   and eliminates randomly. Oh, it makes it makes people think you're home.
[00:23:49.160 --> 00:23:53.400]   Right. Okay. I have an app that does that on my wink. Yeah, who's home?
[00:23:53.400 --> 00:23:57.720]   Kevin Phillips. Oh, yeah. We left Kevin to take care of the house. That's what they.
[00:23:57.720 --> 00:24:03.000]   So it simulates arguments. I told you to take out the trash.
[00:24:03.000 --> 00:24:05.320]   It plays the one from last night.
[00:24:05.320 --> 00:24:11.880]   As you point out, as we've talked about, Google is everywhere. Google Assistant is,
[00:24:11.880 --> 00:24:15.080]   what is this? A gumball machine that you say, hey, Google to, what is this?
[00:24:15.640 --> 00:24:19.240]   So this is in the stands. It's a gumball machine. You tell it, like, hey, Google,
[00:24:19.240 --> 00:24:25.160]   do this and it does it. And it's just, it's eye candy. And then over at the Las Vegas Convention
[00:24:25.160 --> 00:24:31.720]   Center, they have a slide in a big, not a booth, whatever those, but they had a closet because the
[00:24:31.720 --> 00:24:37.160]   rain. They had their closet. Yes. So it's a water slide. And there are the people in the Google
[00:24:37.160 --> 00:24:42.440]   outfits that we were mocking with the little, uh, where's Waldo hats and the white jumpsuit.
[00:24:43.000 --> 00:24:49.800]   They're everywhere. Good Lord. Oh, it's kind of, I guess you. That's creepy. It's
[00:24:49.800 --> 00:24:55.320]   cute. That I mean, I, what would be creepy is if they had like random facts memorized. And when
[00:24:55.320 --> 00:25:01.000]   you said, Hey, Google, tell me that they're human Google assistants. That would be funny.
[00:25:01.000 --> 00:25:04.520]   That would be awesome. Actually, Google totally could have done that. They could have been
[00:25:04.520 --> 00:25:09.320]   assistant using their voice kit thing. And then when you talk to it, oh, you put it in their ear.
[00:25:09.320 --> 00:25:14.200]   Yeah. Yeah. Yeah. I'm thinking, could you actually get gumballs though?
[00:25:14.200 --> 00:25:19.640]   I don't know if you could. I did not stop to try to get a giant bowling ball sized gumball.
[00:25:19.640 --> 00:25:24.040]   Here's that a novo, uh, Google assistant speaker we were talking about. What this is cool. This,
[00:25:24.040 --> 00:25:27.480]   this view is good because it shows you how it stands up. It's kind of curved back.
[00:25:27.480 --> 00:25:33.640]   And it's bamboo and it's so pretty. That's kind of cool. You know, people hated how the echo
[00:25:33.640 --> 00:25:39.160]   show looked. And there's a million of those. Oh, the other thing on this is it has a camera cover.
[00:25:39.240 --> 00:25:44.040]   I think that's really important. And I have it interesting coming from Google, which is not.
[00:25:44.040 --> 00:25:47.400]   Yeah, I generally make fun of camera covers, you know, because,
[00:25:47.400 --> 00:25:53.000]   by the way, but in that case, I absolutely agree. Well, you know that Echo spot that I showed
[00:25:53.000 --> 00:25:58.280]   before I left the alarm clock with the camera on it when I when I got home, it was face down
[00:25:58.280 --> 00:26:05.080]   the bedroom. And so I moved it out of the bedroom because it probably shouldn't have a camera
[00:26:05.080 --> 00:26:10.120]   in the bedroom. It's in my closet though. There's not much to see in there. It's usually dark.
[00:26:10.120 --> 00:26:16.680]   Here's another Google assistant speaker from JBL eight inch link view, nice screen, high
[00:26:16.680 --> 00:26:22.680]   res audio and video video calling as well. Does it use? Does it use duo when they do these?
[00:26:22.680 --> 00:26:29.000]   Use these? Okay. So you call any phone there. Oh, now duo makes a little bit more sense.
[00:26:30.520 --> 00:26:35.320]   Because it's the phone. Well, I who is going to use it, right? But now it makes sense because you
[00:26:35.320 --> 00:26:40.440]   if people have a duo speaker, you could they can call you on the phone.
[00:26:40.440 --> 00:26:47.640]   Wasn't there a report that Facebook was going to make a video screen calling to us? Yes. Yeah.
[00:26:47.640 --> 00:26:55.240]   Here's those. Here's that. Oh, this is a Delta faucet that you have capacitive touch on your faucet.
[00:26:55.240 --> 00:26:59.960]   So this is I do and this one has Wi-Fi and talks to the echo and dispenses and does all the magic
[00:26:59.960 --> 00:27:06.200]   stuff. Okay. All right. You obviously. I'm trying to convince the guys that the booth to retrofit
[00:27:06.200 --> 00:27:13.000]   my faucet with the Wi-Fi module. And finally, a very cute picture of the two of you.
[00:27:13.000 --> 00:27:19.880]   The TOEFL and the HIGGIN BOTHAM, the IOT podcast team in action. The HIGGIN TOEFL.
[00:27:19.880 --> 00:27:20.760]   The HIGGIN TOEFL.
[00:27:23.000 --> 00:27:30.760]   The viral hit of the of the week is this Google Arts and Letters app that everybody's going.
[00:27:30.760 --> 00:27:35.480]   All the kids are going crazy about that takes a picture of you. It's been out for months. We've
[00:27:35.480 --> 00:27:40.280]   known about this for months, but all of a sudden it was discovered. Thank you, Instagram, for making
[00:27:40.280 --> 00:27:45.640]   it a hit. Didn't they add a new feature? It wasn't the matching was a new feature for this app.
[00:27:45.640 --> 00:27:52.840]   Oh, okay. So what you do and you've not done it, but I'll just give you an example and maybe
[00:27:53.800 --> 00:28:00.360]   a cautionary word of warning. This is what I did. I did this. I can't remember which show it was
[00:28:00.360 --> 00:28:07.240]   that I was doing. So that I'm just warning you. The results are not always flattering. So what
[00:28:07.240 --> 00:28:14.680]   happens is it takes a picture of you and in the app, it's a little hard to find. I'll help you
[00:28:14.680 --> 00:28:19.640]   do it in a second. And then it goes through its vast arts and culture collection of artworks
[00:28:19.640 --> 00:28:24.600]   and comes up with what it using face matching technology. What do you think, Jeff? Is that a
[00:28:24.600 --> 00:28:28.680]   that's a pretty good likeness, you think? Oh, yeah. Oh, yeah. I saw on the mystery of a say, I got
[00:28:28.680 --> 00:28:35.640]   one or not. Yeah, I got one. See here. You don't look like beard. It looks like Abe Lincoln's
[00:28:35.640 --> 00:28:41.400]   secretary and I got another guy with the beard. Of course, yeah, Google can't see my face because
[00:28:41.400 --> 00:28:48.280]   I'm actually a ghost. I feel like there's a side view of this guy in a way. This this one of me was
[00:28:48.280 --> 00:28:54.520]   good because it reminded me that even though I don't feel like an old man, apparently I am.
[00:28:54.520 --> 00:29:02.920]   I did it earlier. I was just thinking that this this guy is probably my age, right? It's a he's
[00:29:02.920 --> 00:29:08.360]   Charles Wellington first. Oh, no, I'm sorry. Sir George Grove. The painter is Charles Wellington
[00:29:08.360 --> 00:29:12.600]   first from the Royal College of Art and Music. And he looks a little like Ben Franklin.
[00:29:14.280 --> 00:29:19.880]   He does. Am I just identified by guy as older gentleman? Older gentleman.
[00:29:19.880 --> 00:29:27.000]   This be awesome. So are you ready, Stacey? So I can't because Texas is the other state.
[00:29:27.000 --> 00:29:35.800]   That is so says the chat room. So you can try it. This is in Illinois and Texas. Illinois has a
[00:29:35.800 --> 00:29:41.720]   state law. What the what that says you can't do biometric facial recognition.
[00:29:42.920 --> 00:29:49.160]   Can't you see permission for it? I mean, it seems odd because obviously a lot of what like if you
[00:29:49.160 --> 00:29:53.960]   have an iPhone 10, you're doing it. If you post a picture to Facebook, you're doing it.
[00:29:53.960 --> 00:30:00.680]   Does it is Facebook or is it Texas? Well, no, but so this is a privacy aspect. So the iPhone
[00:30:00.680 --> 00:30:08.520]   does not share your face with Google's or Apple's AI efforts. So that's what this is about.
[00:30:09.560 --> 00:30:15.880]   Facebook has been sued by Illinois attorney general. Okay, because this is exactly what Facebook does.
[00:30:15.880 --> 00:30:25.000]   So yeah, oh, they've sued a lot of people. So Illinois has argued with Snapchat,
[00:30:25.000 --> 00:30:28.600]   Shutterfly, United Airlines. This is like a strange law.
[00:30:28.600 --> 00:30:34.840]   Maybe it wasn't written correctly, but I mean, if you choose to do it,
[00:30:35.400 --> 00:30:39.560]   yeah, you should be able to do it. Yeah, but Google isn't exactly a front about like when your
[00:30:39.560 --> 00:30:44.040]   friends like check out this cool app. It's not like Google saying the app could stop and say,
[00:30:44.040 --> 00:30:50.520]   you know, you're doing now. Do you have to be not to understand it's uploading your picture to
[00:30:50.520 --> 00:30:57.000]   Google? So I can search. I mean, it's kind of okay. That is a very high bar. There's a lot of people
[00:30:57.000 --> 00:31:03.560]   who want to understand what's happening. Okay, just saying. So there is a article in the Houston
[00:31:03.560 --> 00:31:08.200]   Chronicle, if you're really interested by a good friend named Dwight Silverman, how to get around
[00:31:08.200 --> 00:31:13.320]   the block in Texas in Illinois, just turn off location, basically.
[00:31:13.320 --> 00:31:22.120]   iOS settings, privacy location services and just turn it off. Or you could use a VPN.
[00:31:22.120 --> 00:31:27.240]   It's not just a I could do a VPN, but I'm not really that excited about that. That worthwhile.
[00:31:27.240 --> 00:31:32.920]   Here's my name in California. I'll see to prove that it's possible. Dwight did it.
[00:31:32.920 --> 00:31:40.440]   He's in Houston. And he looks exactly like in a portrait of Baron Samuel Von somebody or other.
[00:31:40.440 --> 00:31:45.960]   You know what? I'll be honest with you. I don't none of them are that spot on.
[00:31:45.960 --> 00:31:53.320]   No, there was this thing. People standing in museums. You're doing that just to get me annoyed.
[00:31:53.320 --> 00:32:00.280]   Especially since I can't do it and get some sort of. Oh, yeah. Yeah. Yeah. You're safe. You're safe.
[00:32:02.360 --> 00:32:05.720]   I'm sorry, Jeff. I'm sorry. Pickering. Go ahead. What were you saying?
[00:32:05.720 --> 00:32:15.800]   Mom, Dad. There was there was a meme of people standing next to paintings that looks like them.
[00:32:15.800 --> 00:32:18.040]   That was actually good.
[00:32:18.040 --> 00:32:22.680]   Oh, it is. Yeah. But you have to. That's a lot of work. You have to go.
[00:32:22.680 --> 00:32:26.360]   There we go. Yeah. You can leave your images for people standing next to paintings.
[00:32:26.360 --> 00:32:31.080]   Go ahead and look. I searched for people standing next to paintings that look like them.
[00:32:31.080 --> 00:32:39.720]   Can't you just understand what I want? Google? Oh, that would probably be illegal
[00:32:39.720 --> 00:32:47.640]   in Illinois and the violation. There's a few of them here. There's like one or two of them here.
[00:32:47.640 --> 00:32:52.040]   There's a guy with the guy on the top there. That's a pretty good one,
[00:32:52.040 --> 00:32:55.880]   except for the that's pretty good one. The teak shirt. That's a good one. That's pretty good.
[00:32:55.880 --> 00:33:02.760]   The bald guy, balding guy is good. Those are actually the people in American Gothic,
[00:33:02.760 --> 00:33:18.920]   just so you know. But Ken Tucker, who used to be the TV critic at Edward
[00:33:18.920 --> 00:33:24.840]   has voted that if you want to do business in Montana as an internet service provider,
[00:33:25.480 --> 00:33:32.120]   you have to have you have to be neutral. That's a good thing, right? What could possibly.
[00:33:32.120 --> 00:33:37.000]   Yeah, that's definitely good news. If all the states do this, if California did it,
[00:33:37.000 --> 00:33:40.920]   it would be a big thing. Montana's, I mean, it's populist, but it's not like.
[00:33:40.920 --> 00:33:46.120]   We never did Montana. I'm just saying that Montana, like a rural area, most of them don't
[00:33:46.120 --> 00:33:51.480]   want to serve rural areas anyway, because it's highly expensive. If they can get out of that,
[00:33:51.480 --> 00:33:57.160]   they would. So imagine if you're like, you have to be in a neutral. They're like,
[00:33:57.160 --> 00:34:01.800]   Oh, okay. Never mind. Bye bye. You know, it's cool. It was an executive order. The
[00:34:01.800 --> 00:34:06.440]   governor, Stephen Bullock, just declared Monday that an internet service provider with a state
[00:34:06.440 --> 00:34:13.640]   government contract cannot block or charge more for faster delivery of websites to any customer
[00:34:13.640 --> 00:34:18.920]   in the state. I mean, whoever, of course, remember the FCC said we don't want the states to make
[00:34:18.920 --> 00:34:22.360]   their own rules. Right. So there could be legal challenges.
[00:34:22.360 --> 00:34:27.800]   But I admire them for doing that. I mean, it's, I think it's still worthwhile that they,
[00:34:27.800 --> 00:34:32.040]   that they're even trying to do it. That's better than lots of places.
[00:34:32.040 --> 00:34:41.240]   And then this one, Randall, Randall Stevenson of the CEO of AT&T said AT&T is committed. They bought
[00:34:41.240 --> 00:34:50.200]   big full page ads in a bunch of newspapers saying they want net neutrality. AT&T is
[00:34:50.200 --> 00:34:54.600]   committed to an open internet. We don't block websites. We don't censor online content. We
[00:34:54.600 --> 00:34:58.280]   don't throttle, discriminate or a grade network performance based on content. But they were
[00:34:58.280 --> 00:35:05.080]   a mirror rating. Well, and they also want Facebook and Google, their arch nemesis is
[00:35:05.640 --> 00:35:13.560]   nimisai. I don't know. Nemesis to be subject to regulations as well. They are trying to.
[00:35:13.560 --> 00:35:18.600]   This is their, okay, I go for that. It's a good one. It's, it's a different.
[00:35:18.600 --> 00:35:23.640]   They need to be regulated in a different way. I'm not going to argue that we should not do some
[00:35:23.640 --> 00:35:29.560]   form of regulation over like maybe data monopolies. I actually had a guest on my podcast talking about
[00:35:29.560 --> 00:35:35.160]   what an anti competitive regime for the 21st century looks like. And it was really interesting.
[00:35:35.160 --> 00:35:39.240]   I have to admit, when you read that it's AT&T, you have to scratch your head and go, okay,
[00:35:39.240 --> 00:35:43.720]   what are they up to? Because they have never been really. Well, they want their,
[00:35:43.720 --> 00:35:49.080]   they want their definition of net neutrality. So their definition includes, allows them to do
[00:35:49.080 --> 00:35:55.720]   things like zero rating, which I think should not be allowed. They want the Congress to pass,
[00:35:55.720 --> 00:36:00.440]   and which by the way, I think the congressional solution is the right solution. If you can get
[00:36:00.440 --> 00:36:04.280]   congressional agreement and we're one vote away in the Senate, but that doesn't mean it's
[00:36:04.280 --> 00:36:10.840]   given close to passing in that house. Congressional action, writes AT&T has needed to establish an
[00:36:10.840 --> 00:36:15.800]   internet bill of rights that applies to all internet companies, by which they mean us and
[00:36:15.800 --> 00:36:21.880]   the information services and guarantees neutrality, transparency, openness,
[00:36:21.880 --> 00:36:27.000]   non discrimination and privacy protection for all internet users. You know what, I don't tell
[00:36:27.000 --> 00:36:35.640]   me why that's a bad thing, Stacey. So my issue is regulating ISPs, net neutrality is a solution for
[00:36:35.640 --> 00:36:41.480]   a lack of competition. It's a lack of competition at an infrastructure level.
[00:36:41.480 --> 00:36:48.600]   Now, it is the right solution. So common carriage type of rules are the right solution when you're
[00:36:48.600 --> 00:36:53.800]   dealing with physical infrastructure in a non competitive market. Now, where it gets interesting
[00:36:54.840 --> 00:36:58.920]   is when AT&T is like, let's do the same rules, we'll follow the same rules that Google and
[00:36:58.920 --> 00:37:03.000]   Facebook do. I don't think that's right, because they're in a very different competitive environment.
[00:37:03.000 --> 00:37:06.920]   Their competitive environment isn't necessarily based on physical infrastructure, although their
[00:37:06.920 --> 00:37:14.280]   data centers are a huge element of that, right? But their competitive advantage is based on data.
[00:37:14.280 --> 00:37:20.040]   So then they should be regulated based on that competitive advantage. So what you really need
[00:37:20.040 --> 00:37:27.400]   is two separate laws. And you can talk about, yes, you have other, Google is still, I use DuckDuckGo,
[00:37:27.400 --> 00:37:32.920]   I don't have to use Google, you could use being blah, blah, blah. But there is a data
[00:37:32.920 --> 00:37:38.520]   monopoly developing among the big companies. And we do need to look at that and figure out a solution
[00:37:38.520 --> 00:37:46.920]   for making sure that having all of this information doesn't create a barrier to other
[00:37:46.920 --> 00:37:53.320]   companies being able to come in and innovate. And that's fine. I think that makes sense. I just don't
[00:37:53.320 --> 00:38:02.040]   put in the same rules around them in AT&T, don't make sense at all. Because AT&T is providing
[00:38:02.040 --> 00:38:06.600]   physical infrastructure. Physical access, exactly. How about this Burger King?
[00:38:06.600 --> 00:38:13.640]   Oh, yeah, we forgot to talk about that, yes. I am really jumping all over today. I apologize.
[00:38:16.120 --> 00:38:22.520]   Burger King deviously explains net neutrality by making people wait longer for Whoppers.
[00:38:22.520 --> 00:38:29.320]   That is pretty awesome. Is this real? Yeah, they really did this to people and recorded it.
[00:38:29.320 --> 00:38:34.440]   And apparently some people whopper neutrality net neutrality was just revealed. It was.
[00:38:34.440 --> 00:38:42.520]   And it's very confusing. With that, some people, for what it's worth, just
[00:38:42.920 --> 00:38:45.560]   look, I know whatever stops me on the streets and ask to find these.
[00:38:45.560 --> 00:38:51.560]   Because if they did and they recorded it, they wouldn't use it. They wouldn't use anything.
[00:38:51.560 --> 00:38:55.480]   Right. It's the same thing with, you know, they do that on the late shows, right? They ask people,
[00:38:55.480 --> 00:39:02.040]   you know, who's the president of the United States? Eugene Debs. And then actually, that's
[00:39:02.040 --> 00:39:07.160]   exactly what you thought was. I'm like a socialist revolutionary for the 30th century.
[00:39:07.160 --> 00:39:13.960]   You don't get King Kong and those and everybody laughs at how stupid people are. But that's because
[00:39:13.960 --> 00:39:20.920]   they as 20 people to get the really I presume maybe not. Okay, sorry. Back to the back to the
[00:39:20.920 --> 00:39:32.760]   whopper new. Number one. Number 98. You got the whopper? Yeah. So you got the slow access
[00:39:32.760 --> 00:39:41.560]   whopper pass. It's on the menu right there with the fast medium and slow. Slow MBPS, fast
[00:39:41.560 --> 00:39:47.720]   MBPS or hyperfast MBPS, MBPS, for making burgers per second.
[00:39:47.720 --> 00:39:52.840]   So if you have a whopper now, we have to pay 26 dollars. Well, that's how you get it fast.
[00:39:52.840 --> 00:39:57.560]   It's the highest priority. This is like a laden system. We like 15. Yeah, fast lane slow lanes.
[00:39:57.560 --> 00:40:03.640]   Like maybe like 15, 20 minutes. He gets it. What are you talking about? What are you talking about?
[00:40:03.640 --> 00:40:06.920]   Make more money selling the chicken sandwiches and chicken fries. And now they're slowing down
[00:40:06.920 --> 00:40:10.920]   the access to the whopper. Were you given an option of a chicken sandwich or? Yeah, I don't
[00:40:10.920 --> 00:40:16.440]   want a chicken sandwich. Robert, I want a whopper. Do you have any whoppers ready? I admire Burger King
[00:40:16.440 --> 00:40:21.240]   for for being willing to annoy the hell out of it. I wonder if anybody got the sandwiches ready.
[00:40:21.240 --> 00:40:24.920]   I'm just not allowed to actually get it. Why do you can't give me the sandwich? It's ready,
[00:40:24.920 --> 00:40:29.000]   but you can't give it to me. The whopper neutrality was repealed. They voted on it.
[00:40:29.000 --> 00:40:31.560]   Whopper neutrality. I have no other choice.
[00:40:31.560 --> 00:40:36.760]   Oh, this is the worst thing I've ever heard of. See, like you got the best whopper neutrality.
[00:40:36.760 --> 00:40:43.320]   Are you kidding me? You paid 26 dollars for a whopper? That is my favorite. Are you kidding me?
[00:40:43.320 --> 00:40:48.440]   Some guy actually paid. If you like if you like if you like a bad dream right now,
[00:40:48.440 --> 00:40:51.080]   just want to burger for the burger. You want to.
[00:40:51.080 --> 00:40:56.760]   I'm going to take this. Here's what I'll do. You have the bag.
[00:40:56.760 --> 00:41:00.520]   I can put it in the bag in 42 seconds.
[00:41:00.520 --> 00:41:11.320]   10 seconds. You don't make the rules. I presume these guys are not actual employees.
[00:41:11.320 --> 00:41:12.920]   Unfortunately, we have to. They have to be.
[00:41:12.920 --> 00:41:16.680]   I was being taken advantage of in a sense.
[00:41:16.680 --> 00:41:20.520]   Just as a customer coming in to get their food. It's felt like a power move.
[00:41:20.520 --> 00:41:23.560]   They already changed the policies overnight or whatever.
[00:41:23.560 --> 00:41:26.520]   I didn't think that this is brilliant.
[00:41:26.520 --> 00:41:27.320]   We're ordering a waffle.
[00:41:27.320 --> 00:41:28.520]   Thank you Burger King.
[00:41:28.520 --> 00:41:29.960]   Oh my eyes up to net neutrality.
[00:41:29.960 --> 00:41:33.640]   The whopper actually told me about net neutrality. It's stupid, but true.
[00:41:33.640 --> 00:41:35.480]   Stupid, but true.
[00:41:35.480 --> 00:41:36.760]   That is.
[00:41:36.760 --> 00:41:37.960]   That's my favorite.
[00:41:37.960 --> 00:41:39.480]   I am going to cry.
[00:41:39.480 --> 00:41:41.000]   It's that's awesome.
[00:41:41.000 --> 00:41:43.880]   Why did the king do that?
[00:41:43.880 --> 00:41:45.880]   That's brilliant.
[00:41:45.880 --> 00:41:46.760]   I don't like the king.
[00:41:46.760 --> 00:41:48.840]   The king is creepy as hell.
[00:41:48.840 --> 00:41:49.400]   Can I just say?
[00:41:49.400 --> 00:41:51.720]   King is creepy as heck, but that was a good video.
[00:41:51.720 --> 00:41:56.520]   I want to thank Burger King because they did in fact save net neutrality, didn't they?
[00:41:56.520 --> 00:41:58.280]   Oh, no, they didn't.
[00:41:58.280 --> 00:42:02.200]   We're going to keep working on that 2019 would be the year.
[00:42:02.200 --> 00:42:05.480]   I do want to.
[00:42:05.480 --> 00:42:12.040]   I'm always a little nervous when I use the term naked mole rat in front of Stacy Higginbotham.
[00:42:12.040 --> 00:42:17.800]   I feel like she's just cringes, but I couldn't help but report this story.
[00:42:17.800 --> 00:42:21.880]   Google was using naked mole rats to extend human life.
[00:42:21.880 --> 00:42:22.680]   That's important.
[00:42:22.680 --> 00:42:25.080]   Watch naked mole rats.
[00:42:25.080 --> 00:42:26.760]   Naked mole rats.
[00:42:26.760 --> 00:42:28.120]   What's the deal with naked?
[00:42:28.120 --> 00:42:30.440]   What's the deal with naked mole rats?
[00:42:30.440 --> 00:42:32.360]   Naked mole rats and Google could make their life.
[00:42:32.360 --> 00:42:33.480]   Not naked mole rats.
[00:42:33.480 --> 00:42:34.280]   That's something else.
[00:42:35.240 --> 00:42:37.400]   Oh, mole rats.
[00:42:37.400 --> 00:42:38.440]   Naked mole rats.
[00:42:38.440 --> 00:42:39.160]   Naked mole rats.
[00:42:39.160 --> 00:42:41.560]   And Google can make your life longer.
[00:42:41.560 --> 00:42:46.440]   So we know that naked mole rats are a wonderful thing to study.
[00:42:46.440 --> 00:42:52.360]   The heterocephalus glauber, as it's known, or the sand puppy.
[00:42:52.360 --> 00:42:55.160]   Oh, very good wheel and that in the same five minutes.
[00:42:55.160 --> 00:43:01.560]   It is a naked, sightless, disgusting animal.
[00:43:03.560 --> 00:43:07.000]   But for some reason, scientists keep wanting to study it.
[00:43:07.000 --> 00:43:08.680]   Is it their social organization?
[00:43:08.680 --> 00:43:11.720]   They don't feel pain.
[00:43:11.720 --> 00:43:15.320]   Well, they don't actually live a long time because they're not eternal life,
[00:43:15.320 --> 00:43:18.120]   but they have a little cap to protect their DNA.
[00:43:18.120 --> 00:43:19.880]   DNA cap.
[00:43:19.880 --> 00:43:21.880]   DNA condom.
[00:43:21.880 --> 00:43:25.880]   So they also, as you said, lack of pain.
[00:43:25.880 --> 00:43:26.760]   They don't have pain.
[00:43:26.760 --> 00:43:30.120]   They have very low metabolic and respiratory rates.
[00:43:30.120 --> 00:43:32.920]   You too could live like a naked mole rat.
[00:43:32.920 --> 00:43:34.600]   I'm not sure if I'd want to live longer.
[00:43:34.600 --> 00:43:38.120]   Their legs are thin and short.
[00:43:38.120 --> 00:43:41.080]   They can move backward as fast as they can move forward.
[00:43:41.080 --> 00:43:43.240]   Their large protruding teeth are used to dig,
[00:43:43.240 --> 00:43:45.560]   and their lips are sealed just behind the teeth
[00:43:45.560 --> 00:43:47.560]   so that their mouth doesn't get full of dirt.
[00:43:47.560 --> 00:43:50.200]   How do they eat?
[00:43:50.200 --> 00:43:51.720]   I don't know.
[00:43:51.720 --> 00:43:53.240]   I don't care.
[00:43:53.240 --> 00:43:55.240]   I really don't.
[00:43:55.240 --> 00:44:00.600]   So they can actually live an air that is only 5% oxygen for 5 hours.
[00:44:02.680 --> 00:44:04.360]   And they can hold their breath for 18 minutes.
[00:44:04.360 --> 00:44:06.520]   They can roll that roll nets.
[00:44:06.520 --> 00:44:08.840]   They can roll rats on Mars.
[00:44:08.840 --> 00:44:10.920]   Mars rats.
[00:44:10.920 --> 00:44:12.920]   They have a high resistance to tumors.
[00:44:12.920 --> 00:44:18.440]   Basically, it's the cap on their telomeres that we're excited about.
[00:44:18.440 --> 00:44:19.480]   And we like their...
[00:44:19.480 --> 00:44:22.920]   Because we know that longer telomeres are good for longevity.
[00:44:22.920 --> 00:44:26.200]   Now, let's not get too excited.
[00:44:26.200 --> 00:44:28.360]   They live as long as 32 years.
[00:44:28.360 --> 00:44:30.600]   Dude, there's a queen mole rat.
[00:44:32.440 --> 00:44:34.040]   And they're guarded by worker rats.
[00:44:34.040 --> 00:44:34.920]   I love this.
[00:44:34.920 --> 00:44:36.200]   This you would love.
[00:44:36.200 --> 00:44:37.400]   I'll be your worker rat.
[00:44:37.400 --> 00:44:40.760]   There's the queen of the mole rat.
[00:44:40.760 --> 00:44:43.480]   Stacey, you are our queen mole rat.
[00:44:43.480 --> 00:44:44.520]   Your queen mole rat.
[00:44:44.520 --> 00:44:46.600]   It is what they call...
[00:44:46.600 --> 00:44:49.320]   All hell, Stacey, the queen mole rat.
[00:44:49.320 --> 00:44:50.120]   It's the first mammal.
[00:44:50.120 --> 00:44:53.560]   See, they're ants and bees and stuff exhibit this eusociality.
[00:44:53.560 --> 00:44:58.600]   But it's the first mammal that they've ever found that does that.
[00:44:58.600 --> 00:45:01.240]   The queen and one to three males reproduce
[00:45:01.240 --> 00:45:03.320]   while the rest of the members of the colony just work.
[00:45:03.320 --> 00:45:05.400]   No sex for you.
[00:45:05.400 --> 00:45:05.720]   Work.
[00:45:05.720 --> 00:45:06.760]   Dig.
[00:45:06.760 --> 00:45:07.960]   Dig.
[00:45:07.960 --> 00:45:07.960]   Dig.
[00:45:07.960 --> 00:45:08.920]   Dig.
[00:45:08.920 --> 00:45:09.960]   Workers are sterile.
[00:45:09.960 --> 00:45:15.880]   What is it?
[00:45:15.880 --> 00:45:16.680]   Telomeres?
[00:45:16.680 --> 00:45:17.880]   They got a telomere cap?
[00:45:17.880 --> 00:45:18.360]   Telomeres.
[00:45:18.360 --> 00:45:18.760]   It's good.
[00:45:18.760 --> 00:45:19.720]   They've got a cap.
[00:45:19.720 --> 00:45:25.080]   So basically, the idea is that protects their DNA from mutations
[00:45:25.080 --> 00:45:26.360]   and all kinds of other fun stuff.
[00:45:26.360 --> 00:45:27.160]   So they're going to...
[00:45:27.160 --> 00:45:28.120]   They're basically...
[00:45:28.120 --> 00:45:29.080]   Verily's like,
[00:45:29.080 --> 00:45:32.040]   "Hey, look, these animals, which are creepy weird,
[00:45:32.040 --> 00:45:33.160]   have this cool feature,
[00:45:33.160 --> 00:45:34.520]   so we're going to research that further."
[00:45:34.520 --> 00:45:36.120]   And that's the story.
[00:45:36.120 --> 00:45:37.240]   I...
[00:45:37.240 --> 00:45:38.200]   If I could Google this,
[00:45:38.200 --> 00:45:39.080]   I'd just make some more fun.
[00:45:39.080 --> 00:45:41.880]   We learned a lot about naked mole rats,
[00:45:41.880 --> 00:45:44.920]   and I feel that that is always a worthwhile endeavor.
[00:45:44.920 --> 00:45:47.000]   I hope you're enjoying our best of 2018.
[00:45:47.000 --> 00:45:48.600]   We'll get right back to the clips in just a second,
[00:45:48.600 --> 00:45:50.760]   but I do want to ask a little favor of you.
[00:45:50.760 --> 00:45:53.560]   A gift, if you will, that you can give us this holiday season.
[00:45:53.560 --> 00:45:54.840]   We do this every once in a while.
[00:45:54.840 --> 00:45:55.480]   A survey.
[00:45:55.480 --> 00:45:57.320]   We like to get to know our audience a little bit better.
[00:45:57.320 --> 00:46:01.960]   This year, our survey focuses on how and if, I guess,
[00:46:01.960 --> 00:46:03.800]   you use collaborative software work.
[00:46:03.800 --> 00:46:05.000]   I know a lot of you do.
[00:46:05.000 --> 00:46:08.200]   In fact, most workplaces these days use collaborative software.
[00:46:08.200 --> 00:46:09.720]   So we just want some information.
[00:46:09.720 --> 00:46:12.680]   It'll help us program shows for you
[00:46:12.680 --> 00:46:13.960]   and help us with advertisers
[00:46:13.960 --> 00:46:16.760]   so we can explain what our audience is interested in.
[00:46:16.760 --> 00:46:18.680]   It shouldn't take more than six minutes.
[00:46:18.680 --> 00:46:20.280]   And I know this is important to you.
[00:46:20.280 --> 00:46:21.640]   It's certainly important to me.
[00:46:21.640 --> 00:46:24.200]   No, we will not collect any personal data.
[00:46:24.200 --> 00:46:25.640]   We do appreciate you're doing it, though.
[00:46:26.840 --> 00:46:31.400]   Twit.to/survey12 is the address.
[00:46:31.400 --> 00:46:33.400]   Twit.to/survey12.
[00:46:33.400 --> 00:46:34.440]   You can do it while you're listening.
[00:46:34.440 --> 00:46:35.880]   It won't take you very long.
[00:46:35.880 --> 00:46:38.120]   And I do appreciate you're doing it.
[00:46:38.120 --> 00:46:39.560]   Now, on we go with a show.
[00:46:39.560 --> 00:46:40.680]   Oh, no.
[00:46:40.680 --> 00:46:42.200]   John Perry Barlow has passed.
[00:46:42.200 --> 00:46:43.800]   Yes, he's been there.
[00:46:43.800 --> 00:46:45.080]   Oh, I'm sorry.
[00:46:45.080 --> 00:46:46.840]   No, that makes me so sad.
[00:46:46.840 --> 00:46:48.360]   He's been ill for quite some time.
[00:46:48.360 --> 00:46:52.200]   John Perry Barlow, he was a grateful dead lyricist.
[00:46:52.200 --> 00:46:54.440]   We're the great gentlemen of the Open Internet.
[00:46:54.440 --> 00:46:58.440]   A classy guy, a Montana rancher who also helped
[00:46:58.440 --> 00:47:00.600]   founded the Electronic Frontier Foundation
[00:47:00.600 --> 00:47:04.920]   and did so much to promote the free internet.
[00:47:04.920 --> 00:47:06.440]   But he wasn't that old.
[00:47:06.440 --> 00:47:09.080]   No, he was, he'd been ill.
[00:47:09.080 --> 00:47:11.800]   In fact, we went to a really wonderful benefit for him
[00:47:11.800 --> 00:47:17.880]   in San Rafael last year with Bobby Weir of the Grateful Dead
[00:47:17.880 --> 00:47:20.840]   and Ramblin Jack Elliott, Sean Lennon.
[00:47:22.120 --> 00:47:24.600]   It was a really amazing tribute to him to raise money
[00:47:24.600 --> 00:47:26.920]   for his hospital bills because he had been...
[00:47:26.920 --> 00:47:29.080]   What was the beginning of his ailment, do you know?
[00:47:29.080 --> 00:47:33.240]   Just piled on, I know, but I don't know where it started.
[00:47:33.240 --> 00:47:37.080]   It says in his obituary, he'd been dealing with one health
[00:47:37.080 --> 00:47:39.320]   issue after another in recent years died in his sleep.
[00:47:39.320 --> 00:47:41.720]   He was only 70.
[00:47:41.720 --> 00:47:43.400]   You're right, he wasn't very old.
[00:47:43.400 --> 00:47:48.280]   He led up a very energetic life.
[00:47:48.280 --> 00:47:51.720]   And maybe he just burnt the candle faster than some of us.
[00:47:52.600 --> 00:47:56.360]   He was a fellow student at the Fountain Valley School
[00:47:56.360 --> 00:47:58.600]   in Colorado Springs with Bob Weir.
[00:47:58.600 --> 00:48:00.200]   So they went to high school together
[00:48:00.200 --> 00:48:02.440]   and were of course friends for years.
[00:48:02.440 --> 00:48:07.080]   He wrote many of the dead's iconic songs with Weir,
[00:48:07.080 --> 00:48:08.600]   including "Cassity, Throwing Stones,
[00:48:08.600 --> 00:48:10.120]   "Hell in a Bucket, Lost Sailor,
[00:48:10.120 --> 00:48:13.080]   "Saint of Circumstance, Black Throated Wind."
[00:48:13.080 --> 00:48:15.800]   The music never stops, looks like rain,
[00:48:15.800 --> 00:48:16.680]   Mexico, like blues.
[00:48:16.680 --> 00:48:20.760]   It was really fun to hear Bob Weir and his pick up band
[00:48:20.760 --> 00:48:22.120]   play many of those songs.
[00:48:22.120 --> 00:48:28.680]   For John, the tribute there, and John joined it via Skype.
[00:48:28.680 --> 00:48:31.640]   He joined the well in 1986.
[00:48:31.640 --> 00:48:32.600]   I was there too.
[00:48:32.600 --> 00:48:34.600]   Then four years later, 1990,
[00:48:34.600 --> 00:48:36.760]   founded the Electronic Frontier Foundation.
[00:48:36.760 --> 00:48:41.880]   He and Mitch Capor and John Gilmore,
[00:48:41.880 --> 00:48:44.360]   who was one of the cipher punks.
[00:48:44.360 --> 00:48:47.080]   And EFF of course remains to this day,
[00:48:47.080 --> 00:48:49.240]   a great fighter for the freedom
[00:48:50.280 --> 00:48:51.480]   of the internet.
[00:48:51.480 --> 00:48:53.880]   Many of us remember John Perry Barlow's,
[00:48:53.880 --> 00:48:56.440]   in fact, I think you asked me to get him to read it again
[00:48:56.440 --> 00:48:57.720]   when I interviewed him on "Drangolas."
[00:48:57.720 --> 00:48:58.040]   I did.
[00:48:58.040 --> 00:48:59.080]   I did.
[00:48:59.080 --> 00:49:02.440]   What was the, uh, we the people of the internet?
[00:49:02.440 --> 00:49:02.680]   Yes.
[00:49:02.680 --> 00:49:04.040]   Did he do it?
[00:49:04.040 --> 00:49:04.920]   He didn't do it, right?
[00:49:04.920 --> 00:49:07.560]   Um, he kind of, I don't know,
[00:49:07.560 --> 00:49:08.840]   we'll have to go back and find it.
[00:49:08.840 --> 00:49:09.400]   He was kind of...
[00:49:09.400 --> 00:49:10.920]   He finally did redo it.
[00:49:10.920 --> 00:49:11.320]   But...
[00:49:11.320 --> 00:49:13.080]   Yeah, he was kind of poo-pooted.
[00:49:13.080 --> 00:49:13.960]   He was embarrassed.
[00:49:13.960 --> 00:49:16.760]   Yeah, but then I think he recognized how much,
[00:49:16.760 --> 00:49:17.480]   not to people.
[00:49:17.480 --> 00:49:18.920]   It was the first...
[00:49:18.920 --> 00:49:19.480]   It was the first...
[00:49:19.480 --> 00:49:20.120]   It was a wonderful document.
[00:49:20.120 --> 00:49:21.800]   It is a wonderful document.
[00:49:21.800 --> 00:49:23.720]   Kind of the first kind of statement that
[00:49:23.720 --> 00:49:26.440]   something was happening,
[00:49:26.440 --> 00:49:30.760]   uh, that it was, uh, that it was, um,
[00:49:30.760 --> 00:49:34.920]   it was a nation of its own, sort of, right?
[00:49:34.920 --> 00:49:39.000]   The Declaration of Independence of Cyberspace.
[00:49:39.000 --> 00:49:41.240]   Written in Doubbles.
[00:49:41.240 --> 00:49:44.680]   Governments of the industrial world,
[00:49:44.680 --> 00:49:45.880]   you weary giants.
[00:49:45.880 --> 00:49:46.680]   Give us some more.
[00:49:46.680 --> 00:49:47.480]   Give us some more.
[00:49:47.480 --> 00:49:48.360]   Give us some more.
[00:49:48.360 --> 00:49:49.240]   Do it for John.
[00:49:49.240 --> 00:49:49.960]   Do it for John.
[00:49:49.960 --> 00:49:53.000]   Governments of the industrial world,
[00:49:53.000 --> 00:49:56.440]   you weary giants of flesh and steel.
[00:49:56.440 --> 00:49:58.680]   I come from cyberspace.
[00:49:58.680 --> 00:50:01.080]   The new home of mine.
[00:50:01.080 --> 00:50:03.400]   On behalf of the future,
[00:50:03.400 --> 00:50:07.400]   I ask you of the past to leave us alone.
[00:50:07.400 --> 00:50:10.120]   You are not welcome among us.
[00:50:10.120 --> 00:50:13.560]   You have no sovereignty where we gather.
[00:50:13.560 --> 00:50:16.280]   We have no elected government,
[00:50:16.280 --> 00:50:18.040]   nor are we likely to have one.
[00:50:18.840 --> 00:50:21.640]   So I address you with no greater authority than that which,
[00:50:21.640 --> 00:50:24.680]   with which liberty itself always speaks.
[00:50:24.680 --> 00:50:27.320]   Uh, it's just beautiful.
[00:50:27.320 --> 00:50:30.040]   I can't do it justice because it's so great.
[00:50:30.040 --> 00:50:31.960]   And maybe I could find him reading it.
[00:50:31.960 --> 00:50:33.160]   He did.
[00:50:33.160 --> 00:50:35.800]   He did finally read it about about two years ago, I think.
[00:50:35.800 --> 00:50:37.960]   Um.
[00:50:37.960 --> 00:50:45.640]   Anyway, uh, yeah, I was privileged to have met him many times
[00:50:45.640 --> 00:50:46.600]   and talked to him and you.
[00:50:46.600 --> 00:50:47.000]   Awesome.
[00:50:47.000 --> 00:50:49.480]   I went to, I went to like early loggercons with him.
[00:50:49.480 --> 00:50:52.200]   He was just, it's just a true, wonderful gentleman.
[00:50:52.200 --> 00:50:52.440]   Yeah.
[00:50:52.440 --> 00:50:53.960]   I remember.
[00:50:53.960 --> 00:50:54.760]   That's a huge loss.
[00:50:54.760 --> 00:50:59.560]   Going to his home, uh, for a meeting of the EFF and others.
[00:50:59.560 --> 00:51:05.880]   And yeah, he was grace, graceful, um, uh, brilliant, fun, funny.
[00:51:05.880 --> 00:51:10.760]   Uh, and, and yeah, gentleman, uh, to, to the very end.
[00:51:10.760 --> 00:51:12.920]   And we were really going to miss John Perry Park.
[00:51:12.920 --> 00:51:15.640]   It might be the last post that he put up on Facebook.
[00:51:15.640 --> 00:51:17.080]   It's just a beautiful little picture.
[00:51:17.080 --> 00:51:18.920]   I presume it's a granddaughter grandson.
[00:51:18.920 --> 00:51:20.440]   I mean, he's getting a breast, I think.
[00:51:20.440 --> 00:51:25.640]   Um, and, and he loved his daughters dearly and, and.
[00:51:25.640 --> 00:51:28.840]   The declaration of the independence of cyberspace.
[00:51:28.840 --> 00:51:34.920]   As written by John Perry Barlow at the World Economic Forum
[00:51:34.920 --> 00:51:41.880]   in Davos, Switzerland on February 8th, 1996.
[00:51:42.120 --> 00:51:50.520]   Uh, governments of the industrial world, you weary giants of flesh and steel.
[00:51:50.520 --> 00:51:54.600]   I come from cyberspace, the new home of mind.
[00:51:54.600 --> 00:52:03.720]   On behalf of the future, I ask you of the past to leave us alone.
[00:52:03.720 --> 00:52:07.320]   You are not welcome among us.
[00:52:08.520 --> 00:52:12.520]   You have no sovereignty where we gather.
[00:52:12.520 --> 00:52:17.960]   We have no elected government, nor are we likely to have one.
[00:52:17.960 --> 00:52:25.080]   So I address you with no greater authority than that with which liberty itself always speaks.
[00:52:25.080 --> 00:52:33.320]   I declare the global social space we are building to be naturally independent
[00:52:33.320 --> 00:52:37.160]   of the tyrannies you seek to impose on us.
[00:52:38.440 --> 00:52:45.480]   You have no moral right to rule us, nor do you possess any methods of enforcement.
[00:52:45.480 --> 00:52:48.120]   We have true reason to fear.
[00:52:48.120 --> 00:52:55.720]   Governments derive their just powers from the consent of the governed.
[00:52:55.720 --> 00:52:59.960]   You have neither solicited nor received ours.
[00:52:59.960 --> 00:53:02.440]   We did not invite you.
[00:53:02.440 --> 00:53:07.720]   You do not know us nor do you know our world.
[00:53:08.680 --> 00:53:11.880]   Cyberspace does not lie within your borders.
[00:53:11.880 --> 00:53:18.600]   Do not think that you can build it as though it were a public works project.
[00:53:18.600 --> 00:53:20.760]   You cannot.
[00:53:20.760 --> 00:53:29.320]   It is an act of nature and it grows itself through our collective actions.
[00:53:29.320 --> 00:53:37.480]   You have not engaged in our great and gathering conversation, nor did you create
[00:53:37.480 --> 00:53:39.720]   the wealth of our marketplaces.
[00:53:39.720 --> 00:53:45.000]   You do not know our culture, our ethics, or the unwritten codes
[00:53:45.000 --> 00:53:53.560]   that already provide our society more order than could be obtained by any of your external
[00:53:53.560 --> 00:53:54.280]   impositions.
[00:53:54.280 --> 00:54:01.320]   You claim there are problems among us that you need to solve.
[00:54:02.680 --> 00:54:07.480]   You use this claim as an excuse to invade our precincts.
[00:54:07.480 --> 00:54:11.560]   Many of these problems don't exist.
[00:54:11.560 --> 00:54:19.320]   Where there are real conflicts where there are wrongs, we will identify them and address them
[00:54:19.320 --> 00:54:20.520]   by our means.
[00:54:20.520 --> 00:54:24.920]   We are forming our own social contract.
[00:54:26.040 --> 00:54:32.840]   This governance will arise according to the conditions of our world, not yours.
[00:54:32.840 --> 00:54:35.240]   Our world is different.
[00:54:35.240 --> 00:54:43.560]   Cyberspace consists of transactions, relationships, and thought itself,
[00:54:43.560 --> 00:54:48.600]   a raid like a standing wave in the web of our communications.
[00:54:50.040 --> 00:54:59.400]   Ours is a world that is both everywhere and nowhere, but it is not where bodies live.
[00:54:59.400 --> 00:55:08.120]   We are creating a world that all may enter without privilege or prejudice
[00:55:08.120 --> 00:55:15.000]   accorded by race, economic power, military force, or station of birth.
[00:55:16.360 --> 00:55:23.160]   We are creating a world where anyone, anywhere, may express his or her beliefs
[00:55:23.160 --> 00:55:30.440]   no matter how singular without fear of being coerced into silence or conformity.
[00:55:30.440 --> 00:55:41.640]   Your legal concepts of property, expression, identity, movement, and context do not apply to us.
[00:55:42.840 --> 00:55:47.080]   They are based on matter. There is no matter here.
[00:55:47.080 --> 00:55:56.120]   Our identities have no bodies, so unlike you, we cannot obtain order by physical coercion.
[00:55:56.120 --> 00:56:03.560]   We believe that from ethics, enlightened self-interest, and the common wheel,
[00:56:03.560 --> 00:56:06.120]   our governance will emerge.
[00:56:06.120 --> 00:56:12.280]   Our identities may be distributed across many of your jurisdictions.
[00:56:13.160 --> 00:56:21.880]   The only law that all of our constituent cultures would generally recognize is the golden rule.
[00:56:21.880 --> 00:56:28.440]   We hope we will be able to build our particular solutions on that basis,
[00:56:28.440 --> 00:56:34.280]   but we cannot accept the solutions you are attempting to impose.
[00:56:34.280 --> 00:56:38.680]   In the United States, you have today created a law.
[00:56:39.640 --> 00:56:45.160]   The telecommunications reform act, which repudiates your own constitution,
[00:56:45.160 --> 00:56:52.840]   and insults the dreams of Jefferson, Washington, Mill, Madison, DeTocqueville, and Brandeis.
[00:56:52.840 --> 00:56:59.400]   These dreams must now be born anew in us.
[00:56:59.400 --> 00:57:08.760]   You are terrified of your own children, since they are natives, in a world where you will
[00:57:08.760 --> 00:57:16.680]   always be immigrants. Because you fear them, you entrust your bureaucracies.
[00:57:16.680 --> 00:57:22.280]   With the parental responsibilities, you are too cowardly to confront yourselves.
[00:57:22.280 --> 00:57:30.760]   In our world, all the sentiments and expressions of humanity, from the debasing to the angelic,
[00:57:30.760 --> 00:57:37.080]   are parts of a seamless whole, the global conversation of bits.
[00:57:37.720 --> 00:57:45.960]   We cannot separate the air that chokes from the air upon which wings beat.
[00:57:45.960 --> 00:57:53.800]   In China, Germany, France, Russia, Singapore, Italy, and the United States,
[00:57:53.800 --> 00:58:02.200]   you are trying to ward off the virus of liberty by erecting guard posts at the frontiers of
[00:58:02.200 --> 00:58:10.520]   cyberspace. These may keep out the contagion for a small time, but they will not work
[00:58:10.520 --> 00:58:17.960]   in a world that will soon be blanketed with bit-bearing media.
[00:58:17.960 --> 00:58:25.240]   Your increasingly obsolete information industries would perpetuate themselves
[00:58:25.240 --> 00:58:34.920]   by proposing laws in America and elsewhere that claim to own speech itself throughout the world.
[00:58:34.920 --> 00:58:45.880]   These laws would declare ideas to be another industrial product no more noble than pig iron.
[00:58:47.640 --> 00:58:56.040]   In our world, whatever the human mind may create can be reproduced and distributed
[00:58:56.040 --> 00:59:05.560]   infinitely at no cost. The global conveyance of thought no longer requires your factories to
[00:59:05.560 --> 00:59:13.160]   accomplish. These increasingly hostile and colonial measures place us in the same position
[00:59:13.880 --> 00:59:23.000]   as those previous lovers of freedom and self-determination who had to reject the authorities of distant,
[00:59:23.000 --> 00:59:31.240]   uninformed powers. We must declare our virtual selves, immune to your sovereignty,
[00:59:31.240 --> 00:59:37.640]   even as we continue to consent to your rule over our bodies.
[00:59:39.240 --> 00:59:44.680]   We will spread ourselves across the planet so that no one can arrest our thoughts.
[00:59:44.680 --> 00:59:53.720]   We will create a civilization of the mind in cyberspace. May it be more humane and fair
[00:59:53.720 --> 00:59:57.880]   than the world your governments have made before.
[00:59:59.800 --> 01:00:11.000]   Davos Switzerland, February 8, 1996, and read in New York City, July 30, 2013.
[01:00:11.000 --> 01:00:16.840]   So he wrote that 22 years ago, John Perry Barlow. Thank you for joining that.
[01:00:16.840 --> 01:00:24.520]   Well worth playing it. John Perry Barlow, I'm raised away today at the age of 70.
[01:00:25.080 --> 01:00:30.600]   And I'm glad he recorded that. And thanks to IdeaLog and the Q Department for a department of
[01:00:30.600 --> 01:00:34.920]   records for making that recording and for posting it on Vimeo, you can find it on Vimeo
[01:00:34.920 --> 01:00:42.280]   if you search for it. So there was a very good New York Times, the Daily Yester.
[01:00:42.280 --> 01:00:46.760]   It was actually an observer guardian that they gave the New York Times. So the way you're talking
[01:00:46.760 --> 01:00:53.320]   about? Well it was in their podcast. So it was the Daily. But yes,
[01:00:53.320 --> 01:00:59.880]   observer had the story of course, broke the story and it was a whistleblower Christopher Wiley from
[01:00:59.880 --> 01:01:04.040]   Cambridge Analytica who went to the observer. But I thought a very good synopsis of this,
[01:01:04.040 --> 01:01:10.440]   which I will kind of attempt to rehash because I think before we even talk about Facebook's
[01:01:10.440 --> 01:01:15.960]   culpability in all this, it would be good to understand what exactly a lot of the coverage is
[01:01:15.960 --> 01:01:21.880]   way wrong. And it doesn't make a lot of sense for Facebook to parse breach versus not breach,
[01:01:21.880 --> 01:01:27.320]   but it does make sense for us to get the facts right. Yeah. So this began, Christopher Wiley
[01:01:27.320 --> 01:01:32.120]   will start with him. He was the whistleblower. He was a high school dropout from BC from Canada
[01:01:32.120 --> 01:01:39.400]   who became fascinated with data, big data, when he was volunteering at the Obama campaign
[01:01:39.400 --> 01:01:48.200]   in 2012 and was very intrigued by what data was tell them about reaching out to voters. You know,
[01:01:48.200 --> 01:01:52.840]   how to advertise to them, who to who to advertise to and all of that stuff.
[01:01:52.840 --> 01:02:01.560]   He had was following an interesting study. And I'm trying to remember if that was from Cambridge
[01:02:01.560 --> 01:02:08.600]   or somewhere else, I think not, where some researchers found out that if we get 50 of your likes,
[01:02:08.600 --> 01:02:14.520]   50, so it was 50 of your likes, we can tell whether what race you are, we can tell what kind of
[01:02:14.520 --> 01:02:19.480]   weight education you have. We tell a lot about you. Give us 500 of your likes. We'll know more
[01:02:19.480 --> 01:02:25.640]   about you than your spouse does. And they were able to do that by correlating kind of, it was a
[01:02:25.640 --> 01:02:31.640]   kind of imperfect system correlating. You can likes were scrapeable. I think they still are
[01:02:31.640 --> 01:02:35.800]   scrapeable from Facebook. You don't have to have permission to see what people like. It's in there.
[01:02:35.800 --> 01:02:41.000]   It's right there on their on their profile. So they would they basically get as much of that
[01:02:41.000 --> 01:02:45.240]   like data as they could and see if they could find correlations. And this is very different from
[01:02:45.240 --> 01:02:49.240]   you or me. I mean, one of the things they found out, for instance, if if you like Hello Kitty,
[01:02:49.240 --> 01:02:54.920]   what what are they? I don't even remember, but it said something about you was that you were
[01:02:54.920 --> 01:02:59.560]   you were a nice person, but not a rule follower, something like that.
[01:02:59.560 --> 01:03:05.000]   So you could deduce all of these things. And they did this again by taking great lumps of data
[01:03:05.000 --> 01:03:09.960]   and making correlations and then projecting out why he was fascinated by this. Thought this
[01:03:09.960 --> 01:03:15.480]   was very interesting started doing some of this himself for the Obama campaign and then went to
[01:03:15.480 --> 01:03:22.760]   a work for a British company, run a a group in the British company ran run by this was in Andrew
[01:03:22.760 --> 01:03:31.080]   Nick's, the CEO now ousted of Cambridge Analytica. They realized that they could do a lot more if
[01:03:31.080 --> 01:03:37.560]   they could get more access to the Facebook data. And this is where the story became very interesting.
[01:03:38.040 --> 01:03:48.040]   Nick's ran into somebody from the I think was the the Martin the Cruz campaign, the Ted Cruz
[01:03:48.040 --> 01:03:57.480]   campaign and decided that it would be great to get into the American market and and decided that
[01:03:57.480 --> 01:04:01.160]   if they could do a little bit better targeting that they might be able to do this kind of targeting
[01:04:01.160 --> 01:04:05.640]   they were already doing in the UK in the American market got the attention of billionaire Robert
[01:04:05.640 --> 01:04:12.360]   Mercer who funded it. Steve Bannon who became the chairman and created Cambridge Analytica and
[01:04:12.360 --> 01:04:17.080]   Cambridge Analytica did this is the thing that they did that really has gotten them in trouble
[01:04:17.080 --> 01:04:23.960]   with Facebook is they found a researcher at Cambridge who created a psychological quiz
[01:04:23.960 --> 01:04:29.080]   and they put out on Facebook and offer if you take this quiz it was a real quiz it was not a game
[01:04:29.080 --> 01:04:34.120]   of thrones, you know, which vegetable are you kind of a quiz it was a real psychological profile,
[01:04:34.120 --> 01:04:38.600]   standard kind of profile. If you take this quiz you'll have to download an app and take it offline
[01:04:38.600 --> 01:04:43.880]   off Facebook. We'll give you some money, we'll pay you. And they got a significant number of people
[01:04:43.880 --> 01:04:48.440]   to do that through Facebook so they had their Facebook login and they had the quiz they were
[01:04:48.440 --> 01:04:55.160]   able to use the data they gained from that quiz to it was about a quarter with I think 275,000 people
[01:04:55.160 --> 01:05:00.280]   two hundred seventy thousand. Yeah, to create this what you really need in this machine learning
[01:05:00.280 --> 01:05:06.120]   database in effect making correlations between what the Facebook profile told you about this
[01:05:06.120 --> 01:05:12.600]   person what their psychological profile was. And they also got something which Facebook shut
[01:05:12.600 --> 01:05:16.200]   down a few years ago which is this friends of friends stuff which I always thought was
[01:05:16.200 --> 01:05:20.840]   reprehensible. If you take a quiz or give somebody access to your Facebook account they not only
[01:05:20.840 --> 01:05:25.480]   get your information your likes but they get all your friends information all your friends likes.
[01:05:25.480 --> 01:05:29.080]   Basically, well, just to clarify here that's that's your public information they don't get
[01:05:29.080 --> 01:05:31.080]   anything that's behind private. No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,
[01:05:31.080 --> 01:05:35.000]   not credit card numbers it's stuff you're posted. They get access to their accounts though. They
[01:05:35.000 --> 01:05:39.960]   don't have to be you don't have to have your own relationship at that time. You shouldn't have to
[01:05:39.960 --> 01:05:45.320]   have your own business relationship or or or. That was how people thought Facebook worked. I mean,
[01:05:45.320 --> 01:05:50.360]   I think everybody thought Facebook worked is people can see my page if I allow them to, you know,
[01:05:50.360 --> 01:05:54.760]   if they friend me and I friend them back then then we are sharing data. But as it turns out if
[01:05:54.760 --> 01:05:58.440]   they friend you and you friend them back then anybody who friends them can see your data which
[01:05:58.440 --> 01:06:02.280]   was at the time that was something Facebook stopped a couple of years. And by the way,
[01:06:02.280 --> 01:06:06.840]   just just real quickly, I worked with lots of media companies that said goody goody and did that
[01:06:06.840 --> 01:06:13.480]   like crazy. Obama's campaign had a had a million users and did the same thing. And so it was very
[01:06:13.480 --> 01:06:17.160]   common at the time. But it was all part of that kerfuffle that we talked about in the show at the
[01:06:17.160 --> 01:06:22.440]   time about people not understanding the circles of privacy. We have been talking about this for years
[01:06:22.920 --> 01:06:29.640]   and trying to raise people's awareness of this for years. So Cambridge Analytica got all of this
[01:06:29.640 --> 01:06:35.400]   data for it turns out once they got the 275, 270,000 people to take the quiz, they got their friends
[01:06:35.400 --> 01:06:39.960]   of friends and that ended up being a 50 million person database. A very. Should have been handed
[01:06:39.960 --> 01:06:47.000]   over to him. Well, it's no, no, no, no, it is that they shouldn't have access to it. They got
[01:06:47.000 --> 01:06:54.520]   legitimate access to it saying it was a research. And the rule said, no, the rule said you couldn't
[01:06:54.520 --> 01:07:00.440]   pass that on to say commercial entity. Right. And it was passed on. They were told to research
[01:07:00.440 --> 01:07:06.520]   or who had ties to Russia, by the way, was actually passing along to a commercial entity illegally.
[01:07:06.520 --> 01:07:12.200]   Facebook against Facebook against their rules. It's not illegal, but against their rules. And
[01:07:12.200 --> 01:07:17.640]   then Cambridge Analytica used it apparently quite effectively to target advertising.
[01:07:17.640 --> 01:07:23.640]   And you know, Wiley was saying so they say, which is a whole other. There's been a lot of
[01:07:23.640 --> 01:07:31.640]   question about how we've got we've seen now the videos of its CEO admitting to all sorts of
[01:07:31.640 --> 01:07:36.360]   chicanery. But again, they were boasting about stuff they probably didn't do in the past.
[01:07:36.360 --> 01:07:41.560]   So there's no way of knowing if they actually do hire prostitutes to subordinate candidates
[01:07:41.560 --> 01:07:44.440]   and things like that. And it's also there's debate among researchers. I was at a
[01:07:44.440 --> 01:07:49.800]   Annenberg event at Penn some weeks ago, where most of the researchers in the room said that
[01:07:49.800 --> 01:07:54.440]   they did not believe for a second that Cambridge Analytica had any near the skills that they say
[01:07:54.440 --> 01:07:59.800]   they have. Again, that's irrelevant to this discussion insofar as things that shouldn't have
[01:07:59.800 --> 01:08:06.200]   happened happened. But how much impact it then had is something that could be up for debate.
[01:08:06.200 --> 01:08:11.240]   A lot of people say, hey, Obama did the same thing. The only reason this is a hot button is
[01:08:11.240 --> 01:08:17.720]   because Trump used it to win and not not a Democrat. Fair enough. That's a fair criticism.
[01:08:17.720 --> 01:08:22.680]   Facebook said we should we it was we understand it's a breach of trust. And we expected this is
[01:08:22.680 --> 01:08:29.080]   Mark Zuckerberg's comment earlier. He said we have a responsibility to protect your data. If we can't
[01:08:29.080 --> 01:08:33.080]   we don't deserve to serve you. I've been working to understand exactly what happened and how to
[01:08:33.080 --> 01:08:39.720]   make sure this doesn't happen again. He's going to be on CNN at 6 p.m. Pacific 9 p.m. Eastern
[01:08:39.720 --> 01:08:45.800]   tonight on the Anderson Cooper. Let's not stay on until then. No, we'll be off by then. So
[01:08:45.800 --> 01:08:52.040]   Cambridge Analytica, which was first of all was not supposed to have that data and then told
[01:08:52.040 --> 01:08:55.800]   Facebook they had deleted that data apparently did not delete the data. And until
[01:08:55.800 --> 01:09:01.400]   Wiley stepped forward, we didn't really understand exactly what had happened. But the whistleblower
[01:09:01.400 --> 01:09:09.640]   who was, you know, one of the principles explained the whole process. So a number of people in
[01:09:09.640 --> 01:09:15.000]   including Ryan Acton, who was one of the founders of what's happened was paid $16 billion along
[01:09:15.000 --> 01:09:21.880]   with Yankum, his co-founder for the app by Facebook said it's time hashtag delete Facebook. A lot
[01:09:21.880 --> 01:09:32.200]   of people that hashtags gotten very popular. I deleted Facebook not so much because I've I mean,
[01:09:32.200 --> 01:09:38.520]   I've everybody knows all that information. I just don't like it really brought home to me,
[01:09:38.520 --> 01:09:43.480]   you know, something I've been thinking for a long time, which is I don't trust them.
[01:09:43.480 --> 01:09:48.760]   Now we should mention and I'm sure Jeff will mention that you actually work with Facebook
[01:09:48.760 --> 01:09:54.200]   and the foundation that they do. Just to make just get the full clear disclosure,
[01:09:54.200 --> 01:09:58.440]   I raised $14 million from Facebook, Craig Newmark, the Ford Foundation,
[01:09:58.440 --> 01:10:03.640]   Epinexis and others to start the news integrity initiative. We run it independently of Facebook
[01:10:03.640 --> 01:10:08.120]   and I am not paid by never have been paid by any of the platforms and disclosure.
[01:10:08.280 --> 01:10:13.160]   Yeah. The parent company of Cambridge Analytica has also been banned as has
[01:10:13.160 --> 01:10:21.080]   camera camera, camera, general, for all of this. So there's there was clearly a violation of Facebook's
[01:10:21.080 --> 01:10:27.880]   terms. And I think Mark Zuckerberg is taking some culpability, he's taking some responsibility
[01:10:27.880 --> 01:10:33.960]   for this saying, yeah, we we didn't do a great job. Although there's also the sense that Facebook
[01:10:33.960 --> 01:10:37.800]   is as much a victim as the as the end users that Cambridge Analytica
[01:10:37.800 --> 01:10:44.760]   lied to. You disagree? That's like that's like saying Equifax is as much a victim as the end
[01:10:44.760 --> 01:10:50.360]   users. I mean, this is a company that's dedicated to collecting data about people.
[01:10:50.360 --> 01:10:55.800]   Well, Facebook's paying the price. They lost $50 billion in stock market value of the last
[01:10:55.800 --> 01:10:59.080]   phrase. Are they really paying the price? I mean, I don't know. It depends. I mean,
[01:10:59.080 --> 01:11:03.880]   this is a really interesting nexus. It's kind of a turning point. It could be the end of Facebook.
[01:11:03.880 --> 01:11:09.800]   Some ways, right? What do you think? Yeah, you know, I I'm not a huge Facebook user. I've never been,
[01:11:09.800 --> 01:11:19.560]   but I wouldn't be. We need to have something like Facebook. Do we? Yeah, I do think we do. I
[01:11:19.560 --> 01:11:26.280]   think we should have next people. I mean, I have a phone and an email and mail I could connect to.
[01:11:26.280 --> 01:11:31.720]   It connects people superficial. It makes superficial connections feel
[01:11:32.760 --> 01:11:36.360]   more accessible. So it helps you scale your connections, I would say.
[01:11:36.360 --> 01:11:40.200]   That may be the fault. That may be it's one of its flaws, though, by the way,
[01:11:40.200 --> 01:11:45.080]   because they're not genuine connections. Maybe I don't know. I think they can be genuine
[01:11:45.080 --> 01:11:50.200]   connections. I mean, if you use it well, you can create communities. You just have to
[01:11:50.200 --> 01:11:54.280]   you have to curate them in a way that a lot of people aren't very disciplined at. I mean,
[01:11:54.280 --> 01:12:00.200]   think about how sophisticated people are in different types of people. I don't have a lot of
[01:12:00.200 --> 01:12:04.920]   close friends, for example. I'm not a huge Facebook user, not because of that, but I think those
[01:12:04.920 --> 01:12:12.840]   things are correlated. And there are lots of people who seek others approval. And those people tend to
[01:12:12.840 --> 01:12:18.680]   be on Facebook and establish, I would say to them, meaningful relationships. And I'm not going to
[01:12:18.680 --> 01:12:25.000]   denigrate that. And I think it's something a lot of people need and want. Oracle, I remember,
[01:12:25.000 --> 01:12:35.160]   was suing Google over Java in the Android devices. And they have managed, they lost in court. Google
[01:12:35.160 --> 01:12:46.760]   has now, Oracle has now revived it. And Google may owe them $8.8 billion. The federal court,
[01:12:46.760 --> 01:12:52.600]   US Court of Appeals for the Federal Circuit, that's the patent court out of DC, right?
[01:12:52.600 --> 01:12:57.960]   That's the one that really is out of sync, I think, with the rest of them ruled on Tuesday
[01:12:57.960 --> 01:13:06.520]   that Google's use of Java shortcuts. This was not, by the way, that they stole Java. This is the
[01:13:06.520 --> 01:13:11.720]   API. So shortcuts isn't quite right. It's what's the point of an API?
[01:13:11.720 --> 01:13:18.920]   Can't copyright an API. But this stupid court, they're one ignorant judge away from a really
[01:13:18.920 --> 01:13:23.640]   disastrous rule. That's a good way of putting it. We're all one ignorant judge away from disastrous.
[01:13:23.640 --> 01:13:32.600]   Yeah, well, APIs are the bedrock of the modern economy. So having to put, yeah, this is really
[01:13:32.600 --> 01:13:38.280]   distressing. The dispute, according to Bloomberg technology, which could have far-reaching implications
[01:13:38.280 --> 01:13:43.400]   for the entire software industry, has divided Silicon Valley for years between those who develop
[01:13:43.400 --> 01:13:50.920]   the code that makes software steps function. I don't know. I think that that's a typo.
[01:13:50.920 --> 01:13:56.680]   And those who develop software programs develop the code that makes software steps.
[01:13:56.680 --> 01:14:01.880]   I think this is a mangled oracle's trying to milk it because, I mean, as Steve was saying on
[01:14:01.880 --> 01:14:10.040]   security now this week, the number one scenario for developers is their companies forced them to
[01:14:10.040 --> 01:14:15.240]   use Java, but they'd really love to use Python. That was from Stack Overflow.
[01:14:15.240 --> 01:14:23.800]   There was a, they actually had that as one of the things that they deduced from Stack Overflow usage.
[01:14:23.800 --> 01:14:30.200]   I think that that's true. But this isn't even exactly this is exactly exactly. This is that they
[01:14:30.200 --> 01:14:36.440]   use the Java API. But it makes you kind of not trust oracle as much as a developer. I mean,
[01:14:36.440 --> 01:14:41.000]   you get this Bloomberg story, by the way, is so messed up. I'm sorry. I even pulled it up.
[01:14:41.000 --> 01:14:44.440]   By using the API's programmers don't have to write new code from scratch.
[01:14:44.440 --> 01:14:51.000]   No. This guy, whoever wrote this doesn't understand an API. What an API is that's a library. Sorry,
[01:14:51.000 --> 01:14:54.520]   Susan. I'm going to have to explain to Susan Decker what an API is. I'll explain to you all what an
[01:14:54.520 --> 01:15:00.280]   API is. What's an API, Lee? It stands for application programming interface. And it's what allows you
[01:15:01.000 --> 01:15:06.440]   to, for instance, I'll give you a good example. Twit has an API. We decided when we redesigned
[01:15:06.440 --> 01:15:11.480]   our website that we would do it in two parts that we would use Drupal, which is a very good
[01:15:11.480 --> 01:15:15.560]   content management system, but we'd use something called headless Drupal. You never see the Drupal.
[01:15:15.560 --> 01:15:20.760]   Drupal sits in the background and we use it for all of our workflow when we create a new show
[01:15:20.760 --> 01:15:29.400]   and so forth. And you can get keys to our API and then interact with the database through the API.
[01:15:29.400 --> 01:15:35.480]   It's something we want you to do because then you can write an application. Our application was
[01:15:35.480 --> 01:15:41.880]   our website. The website talks to the database through our Twit API and generates a beautiful
[01:15:41.880 --> 01:15:46.680]   website. There's also an app, my Twit. There's several apps now, but my Twit's one of them that
[01:15:46.680 --> 01:15:52.440]   actually pulls from the API. And we prefer that because that means when you have an app that's
[01:15:52.440 --> 01:15:56.440]   a Twit app, even though we didn't develop it, you will always have the latest information because
[01:15:56.440 --> 01:16:04.360]   you query our database. That's a good thing. You publish the API. We publish it on APRE,
[01:16:04.360 --> 01:16:09.960]   which is a site for APIs, so that people can use it. And the only reason we make you get a key is so
[01:16:09.960 --> 01:16:15.880]   that everybody doesn't scrape all our data and kill our bandwidth, but it's an API is supposed
[01:16:15.880 --> 01:16:21.560]   to be open. That's how we communicate. So copywriting an API, meaning you can't duplicate an API,
[01:16:21.560 --> 01:16:27.640]   it's going to have a horrific chilling effect on so. As if Walmart sued Amazon because Jeff Bezos
[01:16:27.640 --> 01:16:30.920]   walked through the front door of a Walmart. That's essential. This is.
[01:16:30.920 --> 01:16:38.440]   Okay. So I think you guys are being maybe a bit naive or maybe a bit tech focused here.
[01:16:38.440 --> 01:16:46.440]   And I'm not, I'm not just, I totally agree with all of this. However, I, as I said before,
[01:16:46.440 --> 01:16:51.800]   APIs are becoming the bedrock of kind of the economy, especially with IoT, because we're having
[01:16:51.800 --> 01:16:58.600]   so many devices and services talk to so many other services. So what's happening is instead of just
[01:16:58.600 --> 01:17:04.040]   a tech kind of layer to this, we're getting a political and business goals layer to this.
[01:17:04.040 --> 01:17:10.280]   And so Oracle's case isn't a technical case. It's a business case. And they're saying,
[01:17:10.280 --> 01:17:14.920]   hey, we don't want you to use our APIs to build a competing platform. There are a lot of people
[01:17:14.920 --> 01:17:22.760]   who keep their APIs private just for that reason. And so I would say that we do need some sort of
[01:17:22.760 --> 01:17:28.360]   legal framework around APIs, but we also need to understand that they are no longer just a bridge
[01:17:28.360 --> 01:17:34.120]   between services. And I think your definition is wonderful, Leo, but I just explain it to normal
[01:17:34.120 --> 01:17:38.760]   people like, Hey, it's a bridge that lets my software talk to your software and speak kind of
[01:17:38.760 --> 01:17:44.760]   the same language. But my question is, why can't Oracle use technical means to prevent Google from
[01:17:44.760 --> 01:17:49.240]   accessing the API? Well, it's not that they were asked. It's that they duplicated the API.
[01:17:49.240 --> 01:17:55.800]   Right. Right. So if if if Google wants to use Java, Oracle says, that's fine, but you need to
[01:17:55.800 --> 01:17:59.320]   license it because you're making a commercial product and you made 42 billion, according to
[01:17:59.320 --> 01:18:06.040]   them, 42 billion dollars off of Android. So Google says, Oh, well, then we won't use Oracle Java.
[01:18:06.040 --> 01:18:12.200]   We'll write our own code. But in order for it to work with Java programs, it has to support the
[01:18:12.200 --> 01:18:19.960]   Java API. So this is kind of like what Microsoft does with the Linux for Windows system or
[01:18:19.960 --> 01:18:26.840]   Wine does Wine does is to Microsoft to allow so they duplicate the API so that a program
[01:18:26.840 --> 01:18:31.400]   that's sitting on an Android machine doesn't know that it's not talking to Java. It could be
[01:18:31.400 --> 01:18:38.520]   talking to Dalvik, which is some other Java compatible. And I guess there you could make the case if
[01:18:38.520 --> 01:18:43.960]   you're Oracle, well, they should use Java and they should license Java. But this is there's a long
[01:18:43.960 --> 01:18:51.720]   standing tradition of companies duplicating API's. The API by itself contains no knowledge.
[01:18:51.720 --> 01:18:55.960]   It's not the program. It's like, how do you connect? It's how they talk to me.
[01:18:55.960 --> 01:19:02.360]   It's as if you can't copyright an error. So this will be like the Carter phone decision for
[01:19:02.360 --> 01:19:06.520]   the API era. Think about it that way, because they're trying to govern how someone talks to
[01:19:06.520 --> 01:19:11.720]   their software. And they're saying we have a business case for wanting this to be. And
[01:19:11.720 --> 01:19:17.480]   technologists historically and rightly so are like, hey, we want this to be as open and
[01:19:17.480 --> 01:19:21.880]   be as generous so we can all have things work together and talk to each other. And we don't have
[01:19:21.880 --> 01:19:25.240]   to make our lives miserable. This isn't this has been going on for six years. Remember,
[01:19:25.240 --> 01:19:33.800]   Judge Alsop who taught himself Java. He ruled in 2012 that APIs are not copyrightable. And I
[01:19:33.800 --> 01:19:40.200]   think that's the correct ruling because to do so would basically tie up.
[01:19:40.200 --> 01:19:46.680]   I mean, I kind of I do understand Oracle's point of view, but but but but we need to have open
[01:19:46.680 --> 01:19:51.880]   APIs. We just really do and to say you can copyright it is not you can block somebody from
[01:19:51.880 --> 01:19:57.240]   using Java, but you can't block how you interact with a Java like program. I think that's in a
[01:19:57.240 --> 01:20:00.920]   program. It just seems like Oracle's a wrong kind of company to be in control of all that.
[01:20:00.920 --> 01:20:05.320]   Well, that's another issue. But this is a lot of this has to do with personalities, right?
[01:20:05.320 --> 01:20:10.120]   Google's got a lot of money. They may the the circuit court even said Google made 42 billion
[01:20:10.120 --> 01:20:17.640]   dollars on Android. They should give some of it to Oracle. No, Larry Ellison already owns in
[01:20:17.640 --> 01:20:22.920]   a Hawaiian island. He doesn't need Larry's just fine. If you want, I went, I'm going to pitch
[01:20:22.920 --> 01:20:27.960]   my stuff. I wrote a story on this actually two weeks ago, not on the lawsuit, but on the role
[01:20:27.960 --> 01:20:35.640]   of APIs and how they're expanding from pure tech to a business tool. So if you're excited about
[01:20:35.640 --> 01:20:46.600]   APIs, maybe that's it. So this is a weird case, by the way, because it changed over time because
[01:20:46.600 --> 01:20:51.800]   of various rulings in Oracle trying to find ways to keep it alive. So after Alsop
[01:20:53.480 --> 01:21:00.280]   turned it down in 2012, it went back federal district, federal circuit in 2014,
[01:21:00.280 --> 01:21:06.840]   reversed Alsop. Alsop found the Java APIs are copyrightable, but gave Google an out saying,
[01:21:06.840 --> 01:21:13.560]   this is EFF a little more accurate, giving the possibility that Google could have a fair
[01:21:13.560 --> 01:21:19.240]   use defense, right? You have a fair use of a copyrighted thing. Now, that's frankly what all
[01:21:19.240 --> 01:21:24.120]   of the subsequent court decisions have been on, not whether you could copyright an API. That actually
[01:21:24.120 --> 01:21:30.440]   was considered settled. What isn't settled is, is there a fair use defense against reusing the API?
[01:21:30.440 --> 01:21:36.600]   Google asked the Supreme Court to review it. The Supreme Court said, no. The case went to
[01:21:36.600 --> 01:21:44.600]   district court for the fair use defense. And in 2016, a jury unanimously agreed that Google's use
[01:21:44.600 --> 01:21:52.280]   of Java was fair use of the APIs was fair use. Oracle appealed again, and they appealed this time
[01:21:52.280 --> 01:21:57.560]   with that Washington DC Federal Circuit Court, which has historically been problematic for tech
[01:21:57.560 --> 01:22:03.720]   companies. They've really upheld strong intellectual property rights. And this is what just happened
[01:22:03.720 --> 01:22:10.120]   as they said, no, no, you have no fair use defense because you made $42 billion on the API.
[01:22:10.120 --> 01:22:16.120]   And you know, on that very narrow interpretation, I think that that's probably the case. How can
[01:22:16.120 --> 01:22:20.680]   you say it's fair use if you use it commercially and you made 42 billion. Well, they didn't make
[01:22:20.680 --> 01:22:25.880]   that much money on the API. They, I mean, how much money did they make on the API itself?
[01:22:25.880 --> 01:22:29.240]   That's the question. You calculated. On the other hand, would Android have been successful
[01:22:29.240 --> 01:22:34.040]   as successful if you couldn't use Java to develop for it and use the Java, you could actually have
[01:22:34.040 --> 01:22:38.360]   your own API. That wouldn't have been a hard thing to do. Google and hindsight probably should have done
[01:22:38.360 --> 01:22:46.200]   that. Oh, I'm sure they wish they did now. Yeah. 8.8 billion dollars is a significant portion of
[01:22:46.200 --> 01:22:51.640]   their own confidence. That's another one. Let me play. This is the thing that I don't know if this
[01:22:51.640 --> 01:22:58.680]   is creepy, but this is very impressive. So this is now another technology that Google announced
[01:22:58.680 --> 01:23:05.400]   that is going to allow the assistant to actually place calls on your behalf. Watch this. This one
[01:23:05.400 --> 01:23:14.120]   is wild. And it's to help you get things done. It turns out a big part of getting things done
[01:23:14.120 --> 01:23:19.480]   is making a phone call. You may want to get an oil change schedule, maybe call a plumber in the
[01:23:19.480 --> 01:23:25.320]   middle of the week or even schedule a haircut appointment. You know, we are working hard to
[01:23:25.320 --> 01:23:31.320]   help users through those moments. We want to connect users to businesses in a good way.
[01:23:32.360 --> 01:23:39.080]   Businesses actually rely a lot on this, but even in the US, 60% of small businesses
[01:23:39.080 --> 01:23:44.840]   don't have an online booking system, et cetera. We think AI can help with this problem.
[01:23:44.840 --> 01:23:52.440]   So let's go back to this example. Let's say you want to ask Google to make your haircut appointment
[01:23:52.440 --> 01:23:59.960]   on Tuesday between 10 and noon. What happens is the Google assistant makes the call seamlessly in
[01:23:59.960 --> 01:24:06.760]   the background for you. So what you're going to hear is the Google assistant actually calling a real
[01:24:06.760 --> 01:24:14.520]   salon to schedule the appointment for you. Let's listen. Now this is wild. You'll hear, by the way,
[01:24:14.520 --> 01:24:21.160]   this sounds like a real assistant. They intentionally have her sticking in arms and pauses. But the
[01:24:21.160 --> 01:24:27.160]   thing that's most impressive to me is not that, but how she responds to a real human, and this is
[01:24:27.160 --> 01:24:33.720]   like the self-driving car problem, and sometimes humans steer you wrong, or say, and it seemed
[01:24:33.720 --> 01:24:38.520]   to do, and of course these are canned demos, but it seemed to do a very good job of handling
[01:24:38.520 --> 01:24:42.760]   real world situations. Listen to this. This is the assistant.
[01:24:42.760 --> 01:24:50.840]   That's a real salon. I'm calling the Google women's haircut for our client. I'm looking for
[01:24:50.840 --> 01:24:55.560]   something on May 3rd. That's the assistant. I'm looking for something on May 3rd. That's
[01:24:56.440 --> 01:25:02.760]   that's a computer. Do I get me one second? This is a human. She's up talking too. She is. She's
[01:25:02.760 --> 01:25:09.880]   up talking. Sure. What time are you looking for? She said, "Mm-hmm." At 12 p.m. We do not have a
[01:25:09.880 --> 01:25:16.200]   12 p.m. available. The closest we have to that is a 1/16. Now how is the assistant going to deal
[01:25:16.200 --> 01:25:23.560]   with that one, right? Do you have anything between 10 a.m. and 12 p.m. Depending on what service
[01:25:23.560 --> 01:25:28.440]   she would like, what service is she looking for? Just a woman's haircut for now.
[01:25:28.440 --> 01:25:34.280]   Okay, we have a 10 o'clock. 10 a.m. is fine. Okay, what's her birthday?
[01:25:34.280 --> 01:25:41.400]   The first name is Lisa. Okay, perfect. So I will be inside 10 o'clock on May 3rd.
[01:25:41.400 --> 01:25:46.360]   Okay, great. Thanks. Great. Have a great day. Bye. One of the things that scares me about that
[01:25:46.360 --> 01:25:52.680]   is that it's clear they're trying to make it fool the receptionist. What did you hear?
[01:25:52.680 --> 01:25:57.960]   It's heard around the robocall now. Yeah, I mean, I can imagine this technology in the wrong hands
[01:25:57.960 --> 01:26:03.000]   is terrifying. Well, it also, I get this all the time. Don't you want to make it this whether they're
[01:26:03.000 --> 01:26:06.760]   trying to fake that they're human. Yeah. And you ask them, "Are you a robot?" And they have some
[01:26:06.760 --> 01:26:13.080]   can't answer. And they're not. Sorry. Go ahead, Stacy. No, I was very excited from a business
[01:26:13.080 --> 01:26:17.640]   perspective. Google's talking about using this for calling restaurants to see if they're open
[01:26:17.640 --> 01:26:24.440]   for holiday hours, for example. And that, if you can imagine the automation of data collection at
[01:26:24.440 --> 01:26:30.600]   that kind of scale, that is mind boggling how quickly Google could gain an edge.
[01:26:30.600 --> 01:26:37.640]   Well, and this is really an example. To me, I'm kind of lately on a kick saying, "Stop calling it AI.
[01:26:37.640 --> 01:26:41.880]   That's a marketing term. Machine learning I can live with, but AI is a marketing term,
[01:26:41.880 --> 01:26:49.800]   except that this is AI now. This is actually here. Listen to this one. This is a much more challenging
[01:26:49.800 --> 01:26:52.280]   call. I couldn't have done this phone call. I couldn't have done this phone call. I couldn't
[01:26:52.280 --> 01:26:57.080]   have done this phone call. But it's a small restaurant, which is not easily available to book online.
[01:26:57.080 --> 01:27:02.040]   The call actually goes a bit differently than expected. So take a listen.
[01:27:07.240 --> 01:27:12.120]   See how I hear you? Hi, I'd like to reserve a table for Wednesday, the seven.
[01:27:12.120 --> 01:27:23.240]   Oh, seven people. Okay, so the human misunderstood the assistant. He was saying at 7 p.m. She thought
[01:27:23.240 --> 01:27:29.080]   he said for seven people. Maybe she's a robot. That would be. She did a worse job of understanding
[01:27:29.080 --> 01:27:34.440]   him than she does. He doesn't understand or it does of understanding her. She's actually pretty
[01:27:34.440 --> 01:27:40.200]   difficult to understand. Listen to how it continues. So she's already throwing it. I got to say it.
[01:27:40.200 --> 01:27:47.160]   It's really tempting to say him, isn't it? It a curve. It's for four people. For people when
[01:27:47.160 --> 01:27:57.480]   on Wednesday at 6 p.m. Oh, actually we need to serve for like opera, like a five people.
[01:27:57.480 --> 01:28:03.880]   Now if you are not watching the video and didn't see the caption, what the what the human just said
[01:28:03.880 --> 01:28:12.680]   is we only allow reservations for more than five people. For so for four people, you can come.
[01:28:12.680 --> 01:28:19.080]   How long is the way usually to be seated? Now that's actually the right response to that, which is,
[01:28:19.080 --> 01:28:24.760]   oh, I don't need a reservation. Well, how long do I have to wait? And that's a very sophisticated,
[01:28:24.760 --> 01:28:29.720]   I think, response. When tomorrow or week air?
[01:28:31.480 --> 01:28:38.360]   For next Wednesday, the seven. Oh, no, it's not too easy. You can count on four people, okay?
[01:28:38.360 --> 01:28:47.720]   Oh, I got you. Thanks. Bye bye. Wow. This is Turing test level. Turing test level. This is
[01:28:47.720 --> 01:28:53.080]   yeah, this is very impressive. You know, Google duplex since then has become available for some
[01:28:53.080 --> 01:28:59.480]   users of Google Assistant. I think I get calls from Google Google duplex from time to time.
[01:28:59.480 --> 01:29:06.760]   They certainly seem like kind of odd people with the umbs and the yes. We also talked about something
[01:29:06.760 --> 01:29:12.520]   that is a story that is still going on and is going to become bigger and bigger. The Chinese
[01:29:12.520 --> 01:29:17.400]   social credit system towards the end of the year Beijing announced it too was going to adapt
[01:29:17.400 --> 01:29:21.720]   the social credit system. And it's just a matter of time before it's used all over China. That's
[01:29:21.720 --> 01:29:27.640]   terrifying watch. I just put on the bottom of the rundown. George Neema has an old friend of mine
[01:29:27.640 --> 01:29:33.000]   who's in Europe went to China and learned about the social credit system. Oh, fascinating. Oh,
[01:29:33.000 --> 01:29:38.840]   that is good. I can easily be scary. What happens in China, of course, is you find is that the
[01:29:38.840 --> 01:29:43.560]   populist finds things that don't get caught. So white supremacists in America now are talking
[01:29:43.560 --> 01:29:50.920]   about pit bulls and killing pit bulls is a secret language for anti-white interview. Oh, interesting.
[01:29:50.920 --> 01:29:56.440]   So like actually killing that thing. No, no, they just say it because the pit bulls are
[01:29:56.440 --> 01:30:02.120]   proxies for white people. Words can. So he writes, George writes here words cannot describe the way
[01:30:02.120 --> 01:30:07.880]   I feel right now. I am shaking physically. But the Chinese government is implementing a system
[01:30:07.880 --> 01:30:12.440]   which will assign a social credit rating to every citizen based on government data regarding
[01:30:12.440 --> 01:30:18.440]   their economic and social status. It works as a mass surveillance tool. It uses big data analysis
[01:30:18.440 --> 01:30:23.160]   technology. In addition, it's all cement to rate businesses operating in the Chinese market. If
[01:30:23.160 --> 01:30:28.600]   you've seen Black Mirror's nose dive, this is it. I mean, we have seen that. That's exactly what
[01:30:28.600 --> 01:30:38.440]   it sounds like. The rating algorithm known only to the government president, she's plan is based
[01:30:38.440 --> 01:30:45.160]   on the stated government principle. Once untrustworthy, always restricted. That's chilling.
[01:30:45.160 --> 01:30:50.520]   Manorities. We don't let felons vote. Right. Manorities and minority opinions will be negatively
[01:30:50.520 --> 01:30:56.120]   rated as a result of this program, political distance, journalists, and Jay Walker's petty
[01:30:56.120 --> 01:31:01.640]   thieves and others. The consequences of negative ratings. And we knew we've heard some of this
[01:31:01.640 --> 01:31:07.400]   include versus restricted movement, career potential access to services. People who this one get
[01:31:07.400 --> 01:31:12.760]   this one who associate with people with negative rankings will also be negatively ranked.
[01:31:12.760 --> 01:31:17.320]   So there's an incentive to isolate and marginalize minority opinions.
[01:31:18.600 --> 01:31:23.880]   There is no appeal process. Once you're rated, that's it. There's no system to challenge
[01:31:23.880 --> 01:31:29.160]   positive or negative ratings. And he sees you, Jeff.
[01:31:29.160 --> 01:31:35.960]   Yes. Kevin and I talk about this stuff all the time because we're intrigued by it. What's
[01:31:35.960 --> 01:31:43.000]   crazy is the amount of technology that we have deployed that China is using in this process.
[01:31:44.040 --> 01:31:51.320]   So I think the surveillance cameras think actually they're putting wearables on workers to track
[01:31:51.320 --> 01:31:56.840]   their emotions during work so they can understand what makes a person productive.
[01:31:56.840 --> 01:32:00.200]   We do have something very similar here in the United States. It's called a FICO score.
[01:32:00.200 --> 01:32:05.080]   Yeah. This is not like a FICO score. Good luck appealing your FICO score.
[01:32:05.080 --> 01:32:11.000]   Yes, but that only affects your credit rating, which does I'm not saying that's not insignificant.
[01:32:11.000 --> 01:32:15.720]   But this affects this can affect like if your kid gets into the right school and
[01:32:15.720 --> 01:32:20.760]   again, not being able to track your FICO score. There's redlining.
[01:32:20.760 --> 01:32:24.120]   We never got rid of redlining in this country.
[01:32:24.120 --> 01:32:29.640]   And there's all sorts of subtle economic redlining. I agree. We are not anywhere near what China's
[01:32:29.640 --> 01:32:35.960]   doing, but that is absolutely the end game, the potential for this kind of stuff.
[01:32:35.960 --> 01:32:39.160]   And that's why it really is important we pay attention. Like governments for right in the
[01:32:39.160 --> 01:32:45.080]   companies. Well, so that's a good question. We assume that the government's more frightening
[01:32:45.080 --> 01:32:49.240]   because they have these broad unrestricted powers, but the way our current government's
[01:32:49.240 --> 01:32:53.640]   going, if you look at what happened with, oh gosh, is it Experian or Equifax? I can't remember
[01:32:53.640 --> 01:32:59.320]   which one. Equifax. I called him Experian for a while, but it's Equifax. So not getting in any
[01:32:59.320 --> 01:33:05.880]   trouble. If you look at how companies are corrupting our government, it's kind of boiling down to the
[01:33:05.880 --> 01:33:09.880]   same thing. Well, I think that's why I mentioned FICO scores because that is not governmental.
[01:33:09.880 --> 01:33:17.960]   That is private industry. And while it is nowhere near as nasty as the social credit system in China,
[01:33:17.960 --> 01:33:22.760]   it has some of the same attributes. And I don't, I mean, it doesn't have to just be government.
[01:33:22.760 --> 01:33:30.520]   If it's going to happen in the US, it's probably going to happen through private corporations.
[01:33:30.520 --> 01:33:35.320]   I would bet. Well, and there are insurance firms who look at your Facebook friends to
[01:33:35.320 --> 01:33:38.360]   see if like employers who look at your Facebook friends,
[01:33:38.360 --> 01:33:47.000]   it's credit risk too. Yeah. It's all in there. So it's just, it's just, this is a cautionary tale.
[01:33:47.000 --> 01:33:50.840]   This is something we should be very much. This is the end game on stuff like this. You got this,
[01:33:50.840 --> 01:33:55.480]   you know, here's something I might get addicted to. Larry Page's flying car
[01:33:55.480 --> 01:34:04.280]   made its public debut today. Kitty Hawk unwrapped its updated vehicle on Wednesday and allowed a
[01:34:04.280 --> 01:34:12.200]   CNN reporter to pilot the vehicle with less than an hour's worth of training. The update says it's so
[01:34:12.200 --> 01:34:20.600]   easy that you can learn how to fly this flying car without hardly any training at all.
[01:34:20.600 --> 01:34:25.800]   I'm going to give you the predecessor here. Very important. Hold on one second. This is, this is
[01:34:25.800 --> 01:34:31.640]   something that may be tooled even for you and we got it. Look at it. It's so cute. It's got to
[01:34:31.640 --> 01:34:34.840]   be noisy. Can you hear the, are you, are you getting my audio car?
[01:34:34.840 --> 01:34:39.400]   Maybe they're not going to give us the,
[01:34:39.400 --> 01:34:48.280]   look at that. That's kind of cool. That is cool though. So that happens to the wheels. So
[01:34:48.280 --> 01:34:54.760]   what it lands in propellers. How does it? It's got to what lands vertically. And then does it drive?
[01:34:54.760 --> 01:34:58.440]   I want to see it drive. It's a car. Is it just land in the water?
[01:34:58.440 --> 01:35:06.760]   It's just a wheel. I thought it was like a car. It's a, it's a drone. Oh no. See, well, I don't know.
[01:35:06.760 --> 01:35:11.000]   Kitty Hawk. They're exciting. Kitty Hawk. The mysterious flying car startup funded by Google
[01:35:11.000 --> 01:35:17.160]   co-founder Larry Page unwrapped a huge update on Wednesday. It's promising to get people in the air
[01:35:17.160 --> 01:35:24.680]   in under an hour. It's now open for test flights for people in place to place pre-orders.
[01:35:24.680 --> 01:35:29.160]   Was it me in the air under an hour without training? You mean that's how much training you need.
[01:35:29.160 --> 01:35:34.680]   Right now they're testing exclusively over water at a facility in Las Vegas.
[01:35:34.680 --> 01:35:41.640]   After just an hour's training, the CNN reporter Rachel Crane was able to fly the machine at a speed
[01:35:41.640 --> 01:35:47.800]   of, now this must be wrong. Six miles an hour. That can't be right.
[01:35:47.800 --> 01:35:49.960]   I had to have 10 feet of water. That can't be right.
[01:35:49.960 --> 01:35:56.520]   Or Sebastian Runestrikes again. Sebastian's the creator of the Google self-driving car.
[01:35:56.520 --> 01:36:03.480]   He says it's as easy to use as playing Minecraft. It's powered by a lithium-ion battery, which
[01:36:03.480 --> 01:36:14.360]   can be used for 20 minutes. Six wires an hour. You can't think ahead to passing one car.
[01:36:14.360 --> 01:36:18.440]   That means you can go to the big hills. The joystick is so intuitive.
[01:36:18.440 --> 01:36:24.040]   But it's not the most comfortable thing I ever sat in said Rachel. You can definitely feel the
[01:36:24.040 --> 01:36:32.280]   vibrations. 10 motors, two control sticks. It's conducted 1500 test flights so far among
[01:36:32.280 --> 01:36:36.760]   interested business partners and social influencers. We have a chance.
[01:36:36.760 --> 01:36:39.000]   Why didn't they invite you? Me? You?
[01:36:39.000 --> 01:36:45.400]   I'm not going up in that thing. Maybe this is good for like, you know,
[01:36:45.400 --> 01:36:50.920]   in Sydney with the water taxis. Or if you think about a place like, I don't know,
[01:36:50.920 --> 01:36:58.280]   they hope the vehicles will be able one day to fly it up to 100 miles an hour over populated areas.
[01:36:59.160 --> 01:37:04.600]   Ah, as opposed to swamps. I think the six miles an hour must be a typo.
[01:37:04.600 --> 01:37:12.200]   No, I mean, that might be. I mean, this is brand new, right? So, I don't know how fast the
[01:37:12.200 --> 01:37:16.520]   Wright brothers plane went. I see in the video how fast it looks like it's going.
[01:37:16.520 --> 01:37:22.120]   Looks like it's going a little fast. I mean, not. Oh, that's fasted in six miles an hour.
[01:37:22.120 --> 01:37:25.400]   No, it's faster than six miles an hour. Six miles an hour is as fast as you walk.
[01:37:25.400 --> 01:37:33.400]   You walk. Well, you walk. That's, that's, it's interesting. I don't, I don't know. See, I,
[01:37:33.400 --> 01:37:38.440]   just want to keep my feet on the ground. That's why I never really anchored to be a flyer either.
[01:37:38.440 --> 01:37:43.560]   I don't mind commercial flights, but. Daisy, would you, if you were given a free hour of training,
[01:37:43.560 --> 01:37:48.440]   would you do it? Oh, yeah, I totally do it. I'll tell you what I want to go hang gliding.
[01:37:48.440 --> 01:37:53.640]   So I am clearly nuts. I really want to buy this. You are. I did it once I did it in Napa
[01:37:53.640 --> 01:37:58.280]   after enough wine samples. Well, I'm going to glider in Napa. Yeah.
[01:37:58.280 --> 01:38:02.840]   You want to hang gliding. You did a hang gliding. No, I'm not doing that. No, I'm not doing that.
[01:38:02.840 --> 01:38:07.720]   You're in a glider. You see, it's like a plane with really big wings and then they
[01:38:07.720 --> 01:38:12.760]   pull you up in the air. But what he didn't tell me is that right before the flight,
[01:38:12.760 --> 01:38:15.880]   they were trying to decide it was so windy if they should go up or not. And they said, well,
[01:38:15.880 --> 01:38:21.720]   this will be the last flight of the day. You ignore your life. And then as, as I'm getting in,
[01:38:22.280 --> 01:38:28.600]   my wife, I just, we just got married and said, do you have insurance? I said, yes, but it's not in
[01:38:28.600 --> 01:38:39.560]   your name. I waved. I waved goodbye. Nice. We've just gotten married. I hadn't had a chance.
[01:38:39.560 --> 01:38:46.840]   I think people know by now that Jeff Jarvis is a German speaker. He loves German. He goes
[01:38:46.840 --> 01:38:52.600]   at Germany all the time, even though Germany is kind of funny about privacy, I think of all the
[01:38:52.600 --> 01:38:59.720]   countries in the world, the Germans are most worried about Google. The funny thing is,
[01:38:59.720 --> 01:39:05.080]   Germans, as Jeff Jarvis has pointed out many times, will all go naked in a sauna together.
[01:39:05.080 --> 01:39:11.960]   But heaven help us if you should be snooping on them while you're shopping for brought.
[01:39:12.680 --> 01:39:15.400]   Why don't we're going to let you go, Jeff? So why don't you give us your number?
[01:39:15.400 --> 01:39:22.760]   OK, OK, I'll maybe a bonus. A bonus. A little thing of the week. I'm a non-number one, but I like
[01:39:22.760 --> 01:39:30.920]   this. So where was it here? So somebody on Twitter posted this, I loved it. I signed a Timo Gründ
[01:39:30.920 --> 01:39:39.720]   posted this German bakery. Yeah. A Dutch Schruzgrund Fereldung, a data protection,
[01:39:40.600 --> 01:39:48.280]   basic right, Aktung. In our bakery, we are likely to recognize you by name and your favorite
[01:39:48.280 --> 01:39:57.560]   baked good. If you do not want this, shout as soon as you enter, I do not agree. And we will try
[01:39:57.560 --> 01:40:06.520]   our best to not know you. Can we print this out in the original German and put it on the door?
[01:40:07.320 --> 01:40:10.680]   Yes. Aktung. Aktung. So I'll write a letter. And then.
[01:40:10.680 --> 01:40:18.600]   Werenzee Auch Gileggen Tlich, Mitt Demnämen Aungusbroken, und Wehrmärken Werns, Würke's Eir,
[01:40:18.600 --> 01:40:25.080]   Liebliggenskär Bechist. We will know. He does Nick Wallen, Rufenze Einfach, Bime Beyfrenen,
[01:40:25.080 --> 01:40:31.240]   Ich binicht Einfürstanden. We'll know if every once in a while, somebody shouts,
[01:40:31.240 --> 01:40:37.640]   "Ek binicht Einfürstanden!" They read German. I just love that so much. I just got a number,
[01:40:37.640 --> 01:40:44.280]   but I want to print that sucker out. That device, Jeff, that you just unboxed and you started setting
[01:40:44.280 --> 01:40:49.400]   up, that's running on Android things. Did you know that? I do. But I have to explain what that
[01:40:49.400 --> 01:40:53.720]   means to me because I just open it up. Well, it's like a stripped down, more specialized version of
[01:40:53.720 --> 01:41:00.120]   Android specifically made for IoT implementations. But what you'll notice when you're playing around
[01:41:00.120 --> 01:41:07.320]   with the UI on that is that it's very much in line with Google's new direction of user experience
[01:41:07.320 --> 01:41:12.680]   and UI. It doesn't look like Android necessarily. It has its own thing going,
[01:41:12.680 --> 01:41:16.280]   but along their style guidelines, it's really nice.
[01:41:16.280 --> 01:41:28.920]   And it comes with additional support. It's not still crazy long support, but you'll have it for a
[01:41:28.920 --> 01:41:35.640]   little bit longer. That's good because these devices shouldn't die. Some of them won't die after
[01:41:35.640 --> 01:41:40.920]   three years, for example. Let's hope not. What all can I do with this wonderful little thing?
[01:41:40.920 --> 01:41:48.440]   Can I watch this? Okay. You can. Kevin bought one and we talked about it on our show earlier
[01:41:48.440 --> 01:41:53.560]   today. I can tell you if you would like to know all the things that Kevin has done with his 36
[01:41:53.560 --> 01:42:00.440]   hours of the device. Kevin is a show off, but that's okay. He is the most delightful show off. So he
[01:42:00.440 --> 01:42:08.280]   connected it to his YouTube TV account. So he watched CNN. He said he could voice control it.
[01:42:08.280 --> 01:42:16.440]   So he would say turn to channel 10 on YouTube TV and it would work. Netflix is built in. So you
[01:42:16.440 --> 01:42:24.520]   can say, hey, watch, nailed it on Netflix and that'll happen. YouTube works. It's also a Chromecast.
[01:42:24.520 --> 01:42:29.480]   And here's my favorite thing. You can set it up to Chromecast your Google Photos. Sorry,
[01:42:29.480 --> 01:42:35.080]   not Chromecast. You can set it up to show your Google Photos when it's in neutral mode. So off,
[01:42:35.080 --> 01:42:39.240]   but not really off. So those are all really cool things. And then it ties into your,
[01:42:39.240 --> 01:42:44.120]   if you have a Nest doorbell or a Nest camera, it's going to tie into those. So you can say, hey,
[01:42:45.080 --> 01:42:50.200]   show me the front door and it will. But there's like a six to seven second delay.
[01:42:50.200 --> 01:42:56.200]   Yeah, but it does. If this is in your kitchen or whatever, which I think is kind of the,
[01:42:56.200 --> 01:43:01.880]   kind of the default, like, what do I do with this? It seems like a great kitchen device,
[01:43:01.880 --> 01:43:04.920]   right? If you're in the kitchen, you're cooking, maybe you want to watch something or maybe you
[01:43:04.920 --> 01:43:10.280]   want to recipe is always another big use case for something like this. Looks really nice.
[01:43:10.280 --> 01:43:11.880]   Okay. What's the news?
[01:43:13.320 --> 01:43:14.040]   You're the latest.
[01:43:14.040 --> 01:43:17.640]   Prepare yourself.
[01:43:17.640 --> 01:43:23.400]   And prepare yourself for the latest news. Take a deep breath, Jeff.
[01:43:23.400 --> 01:43:27.400]   No, I don't want that. Don't stop. I don't want that now. Stop me.
[01:43:27.400 --> 01:43:33.720]   And you'll, as you play around with it, you'll get kind of more used to it took me a little
[01:43:33.720 --> 01:43:38.440]   getting used to last night on all about Android, the kind of swipe actions from the side to get
[01:43:38.440 --> 01:43:43.000]   different pieces of information that it can surface and all that kind of stuff. But
[01:43:43.000 --> 01:43:48.040]   now, now it's like in the back, how do I, it's in the background playing me this? I don't want it.
[01:43:48.040 --> 01:43:57.000]   Tell it to stop. Hey, stop. Oh, that's right. Hey, stop. I feel so powerful.
[01:43:57.000 --> 01:44:01.240]   You did that, Jeff. Congratulations.
[01:44:01.240 --> 01:44:04.040]   I do. I'm only because Stacy told me.
[01:44:04.040 --> 01:44:08.200]   I'm going to do the dad. I'm probably these dad hands or Jeff hands from now on, because
[01:44:08.600 --> 01:44:10.920]   that's the exact same thing I do when something I'm like,
[01:44:10.920 --> 01:44:16.600]   do it. What do I do? Oh, my favorite thing you do, Stacy, is when you get excited with you.
[01:44:16.600 --> 01:44:21.240]   Yeah. That's right. I'm going to get excited about stuff later today,
[01:44:21.240 --> 01:44:24.120]   because I was at Google's cloud conference last week. That's right.
[01:44:24.120 --> 01:44:31.240]   And those were those. Yeah, it's exciting. Okay. Someone in the chat room asked, but works with
[01:44:31.240 --> 01:44:37.000]   ring. It doesn't work with ring yet. So it is a bummer ring, of course, sponsor on the
[01:44:37.000 --> 01:44:40.680]   Twitter network. But that would be I can find the call.
[01:44:40.680 --> 01:44:44.520]   I would want to use it for in the kitchen. That's one of the things I can do. I can find
[01:44:44.520 --> 01:44:52.440]   calm. I lost it about 40 years ago. I don't think Google's going to help you with that, Jeff.
[01:44:52.440 --> 01:44:54.200]   I can play. What is it doing?
[01:44:54.200 --> 01:44:57.240]   She noises like white noise and stuff.
[01:44:57.240 --> 01:45:02.760]   That's incredibly irritating and will not make me calm.
[01:45:06.680 --> 01:45:12.920]   Jeff, I think something might be wrong with your Lenovo smart display.
[01:45:12.920 --> 01:45:19.800]   Don't ever do that again, Jeff. Can it go portrait to landscape or is it one way?
[01:45:19.800 --> 01:45:26.680]   Most of you. Yeah. But I. Oh, it's close to when we do video call. Let me see if I can.
[01:45:26.680 --> 01:45:32.600]   Yeah, I'll show it once the call is made because it shows information on the screen.
[01:45:32.600 --> 01:45:36.040]   You know, I don't want to show your number or all the people in my contacts list.
[01:45:36.040 --> 01:45:37.800]   Let's see here. So I'll put in.
[01:45:37.800 --> 01:45:39.320]   Oh, I get a spam calls anyway.
[01:45:39.320 --> 01:45:42.680]   So it says I have to invite you.
[01:45:42.680 --> 01:45:47.080]   Okay. You called me a minute ago.
[01:45:47.080 --> 01:45:54.120]   Well, no, I went into duo and it's in order to call Jeff. It said I had to invite you.
[01:45:54.120 --> 01:45:56.440]   That's because I'm trying to call you from.
[01:45:56.440 --> 01:45:59.320]   Add me on duo. Okay. Add me on duo.
[01:45:59.320 --> 01:46:04.120]   Use the number. I did it through the context this time and for whatever reason it.
[01:46:05.240 --> 01:46:06.520]   Okay. So give me a second here.
[01:46:06.520 --> 01:46:11.800]   Yeah, it just takes me to hangouts now. That's weird. It worked before.
[01:46:11.800 --> 01:46:15.400]   Phone number. Okay.
[01:46:15.400 --> 01:46:17.960]   This is the story of the internet right here.
[01:46:17.960 --> 01:46:23.400]   See, it just works every time. Oh, hey, now it's working.
[01:46:23.400 --> 01:46:27.720]   Now it's coming into my phone, but it's not coming into the duo, into the thing.
[01:46:27.720 --> 01:46:33.240]   It's not. Oh, what? Oh, you probably haven't hooked your phone up to the duo,
[01:46:33.240 --> 01:46:37.400]   so it knows your number. Does the duo, did you, when you signed up on the duo with the app?
[01:46:37.400 --> 01:46:39.720]   I think I did it. It was a whole, yeah. A whole map.
[01:46:39.720 --> 01:46:43.720]   Okay. Well, Jeff, I'll call you.
[01:46:43.720 --> 01:46:46.600]   Yeah. So I just put my number into the doc chat.
[01:46:46.600 --> 01:46:49.320]   Okay. I want to take that and do that.
[01:46:49.320 --> 01:46:50.360]   We'll see if that works.
[01:46:50.360 --> 01:46:55.480]   Explore more things you can do. Where's make a call? Oh, I see. I just tell it, right?
[01:46:55.480 --> 01:46:58.920]   Yes. Hey, G call Jason Howell.
[01:47:00.760 --> 01:47:07.160]   I wonder if that'll work. Okay. Oh, hey, hey, call, make a call.
[01:47:07.160 --> 01:47:16.520]   I don't know how to call make a call. Okay. Make a call.
[01:47:16.520 --> 01:47:21.160]   What do you want to call it? So now I.
[01:47:21.160 --> 01:47:23.160]   Say Jason.
[01:47:23.160 --> 01:47:29.160]   Yeah, I might not be saved. You're not like, I can't just put a number in.
[01:47:30.600 --> 01:47:35.480]   You should be able to, so I can dial from Google Home. I can say, okay, call
[01:47:35.480 --> 01:47:39.160]   512-555-6680.
[01:47:39.160 --> 01:47:40.760]   Okay. Hold on. But if you do that, yeah. I'm you.
[01:47:40.760 --> 01:47:49.080]   Probably a good idea. And we can, we could just wait for it to happen.
[01:47:49.080 --> 01:47:53.320]   See these on these on-show demonstrations always work right.
[01:47:53.320 --> 01:47:55.960]   Well, this is how people can see the learning curve. They're like,
[01:47:57.960 --> 01:48:01.480]   oh, oh, Google actually dialed that for me. Oh,
[01:48:01.480 --> 01:48:08.360]   Thankfully, it was just a 555, like fake number.
[01:48:08.360 --> 01:48:13.000]   At least you didn't say 911. That would have been awkward.
[01:48:13.000 --> 01:48:18.200]   It's giving me a few notes first telling me emergency numbers aren't available.
[01:48:18.200 --> 01:48:24.680]   Okay, good. All right. So I'm waiting. Once I start to see something here, I'll let you.
[01:48:25.400 --> 01:48:32.440]   I'm getting a notice on my pixel book. I'm not getting, oh, no, I'm getting a private call number
[01:48:32.440 --> 01:48:39.560]   or private incoming call. That's not duo. No, that took me to phone. That's you.
[01:48:39.560 --> 01:48:45.640]   Hi. But it's only audio. There's no video. Oh, it's a call call.
[01:48:45.640 --> 01:48:50.520]   How's it going? Yes. This is the, oh, I'm, I'm very confused. I need to find some
[01:48:50.520 --> 01:48:56.920]   calm. I think we all need to find some calm. I'm going to hang up on you now. Oh, my gosh, you guys.
[01:48:56.920 --> 01:49:03.000]   Hey, how do you guys feel about notches? I think I know how good it feels about notches.
[01:49:03.000 --> 01:49:10.280]   So line my notches. You just kind of go blind to them, but three notches. Where are they putting
[01:49:10.280 --> 01:49:14.120]   all these notches? That was my question. I'm not sure that we'll see a three-not. Well,
[01:49:14.120 --> 01:49:21.240]   we definitely won't on Android because apparently only two notches are allowed in Android P.
[01:49:21.240 --> 01:49:26.680]   They've mandated a requirement of only two notches, only the bottom and top and only one
[01:49:26.680 --> 01:49:30.520]   per side. So you couldn't. So in other words, you couldn't have your two notches both on top.
[01:49:30.520 --> 01:49:35.800]   Not sure why you would want to do that, but Google has gone to the lengths to make sure that you don't
[01:49:35.800 --> 01:49:40.840]   because who knows. You know, someone might just die on a phone. It's all notches.
[01:49:41.560 --> 01:49:46.120]   Yeah, that's kind of what I was thinking like a almost like a border, a border.
[01:49:46.120 --> 01:49:48.440]   Yeah, like a scrapbook border. Like a cute little like.
[01:49:48.440 --> 01:49:55.320]   Why? Because we can. This is for the notch lover. First of all, we should say that Google has
[01:49:55.320 --> 01:50:03.880]   not announced this, but there was a rumor that Google was going to work on a censored version
[01:50:03.880 --> 01:50:10.840]   of Google search. This comes from the intercept, Ryan Gallagher for China leaked documents,
[01:50:10.840 --> 01:50:14.840]   reveal Google's planning to launch a censored version of its search engine in China that will
[01:50:14.840 --> 01:50:20.760]   blacklist websites and search terms about human rights, democracy, religion, peaceful protest,
[01:50:20.760 --> 01:50:27.080]   all the things that Chinese government doesn't like. According to the intercept, the project
[01:50:27.080 --> 01:50:32.520]   code named Dragonfly has been under way since the spring of last year was accelerated in
[01:50:32.520 --> 01:50:36.440]   December when Sundar Pichai met with a top Chinese government official.
[01:50:36.440 --> 01:50:45.240]   And there are huge protests at Google from Google employees who say, no, we got out of China
[01:50:45.240 --> 01:50:50.280]   because we didn't. For a reason. For a good reason. They're human rights record.
[01:50:50.280 --> 01:50:56.920]   So we talked about this last week. Oh, it's over. Okay. Never mind. No, no, no, no. I was going to say
[01:50:56.920 --> 01:51:01.000]   stories kept going. Yeah. Yeah, the story kept going. So last week, it had just broken.
[01:51:01.000 --> 01:51:06.440]   Oh, okay. And, you know, Jeff and I were talking about, you know, Jeff, as one can imagine, was
[01:51:06.440 --> 01:51:10.680]   this is evil. Google should draw a line and stick to it. I didn't use the word evil, but I said I
[01:51:10.680 --> 01:51:16.120]   was disappointed. Yeah. And I think that I said before when it comes to Apple, for instance,
[01:51:16.120 --> 01:51:21.800]   working with the Chinese government that American companies need to stand up and say,
[01:51:21.800 --> 01:51:25.400]   we're not going to do we're not going to work in authoritarian regimes, even if we could make a
[01:51:25.400 --> 01:51:32.360]   lot of money there. You could work with authoritarian regimes right now in the US, but
[01:51:32.360 --> 01:51:39.640]   this is not an authoritarian regime. Come on. You may not like it, but we do. This is not
[01:51:39.640 --> 01:51:47.800]   comparable to China yet. There are some practices that are actually, but I was going to say,
[01:51:47.800 --> 01:51:57.080]   I was waiting for you're not an immigrant. It's okay here. Anyway, I was waiting for Google
[01:51:57.080 --> 01:52:04.280]   employees to weigh in and predictably, they're unhappy. So I will see I'm curious what happens
[01:52:04.280 --> 01:52:11.960]   from that perspective, if Google individual Googlers end up preventing this or what goes on there.
[01:52:13.080 --> 01:52:18.920]   This is that old argument companies make that, well, we have to obey the laws of the countries
[01:52:18.920 --> 01:52:23.080]   we work in. The thing that's different about Google is you choose to be in them. Google decided
[01:52:23.080 --> 01:52:27.480]   not to do business in China because of its human rights record. And now they're
[01:52:27.480 --> 01:52:31.320]   apparently considering going back. It's of course tempting because there's a lot of money
[01:52:31.320 --> 01:52:36.280]   to be made in China. There's a billion Chinese people who are active
[01:52:36.280 --> 01:52:42.440]   avid internet users in many cases. I'll make a prediction. Go ahead. Even if they do everything
[01:52:42.440 --> 01:52:46.680]   in China once, which is a lot more now than it would have been when they left, which is a story
[01:52:46.680 --> 01:52:52.920]   up on the rundown that the rules have gotten worse and worse. Even if they did everything that
[01:52:52.920 --> 01:52:56.200]   China wanted, they'll fail there because you have to be a Chinese company to succeed.
[01:52:56.200 --> 01:53:02.440]   Yeah, Baidu has said, we'll beat them easy. Come on. And Google Baidu is the Google of China.
[01:53:03.480 --> 01:53:08.360]   So China already blocks Facebook, Twitter, Instagram and Snapchat. So Google would have to block all
[01:53:08.360 --> 01:53:15.960]   that. This is from the observer. Anything that's anti communism, party content, any political
[01:53:15.960 --> 01:53:20.280]   dissidents, of course, that's obvious, controversial news content, pornography,
[01:53:20.280 --> 01:53:25.400]   being defined as things that are critical of government. Yeah. Revelations of business
[01:53:25.400 --> 01:53:33.800]   or political scandals. Anything harmful to social stability, pornography, gambling and violence.
[01:53:33.800 --> 01:53:38.600]   Winnie the poo is banned in China. And that goes back to the fact that
[01:53:38.600 --> 01:53:43.960]   dissidents compared President Xi to a bear.
[01:53:43.960 --> 01:53:54.360]   During his visit to the US in 2013, a meme comparing a photo of him in Barack Obama
[01:53:54.360 --> 01:53:59.000]   to an image of poo and Tigger went viral in China. She didn't like it winning the
[01:53:59.000 --> 01:54:02.840]   Pooh's been banned ever since. Google, do you really want to be in a country where
[01:54:02.840 --> 01:54:08.440]   an autocratic leader can ban an image because he doesn't like it? Because he finds it
[01:54:08.440 --> 01:54:13.320]   something. No, it's just the world. No.
[01:54:13.320 --> 01:54:20.840]   Can I tell you my favorite story of the week? What's that? Smart reply story.
[01:54:20.840 --> 01:54:24.280]   Okay, so kick that one off because I think I may have missed this one.
[01:54:24.280 --> 01:54:28.360]   So yeah, I put it in the last minute. So smart replies, you know, looks at all these responses
[01:54:28.360 --> 01:54:32.760]   and makes up a response that you should give an email. And I've talked about how I feel guilty
[01:54:32.760 --> 01:54:37.400]   using it. Like the human didn't really send you this reply, which says, yes, I'll be there.
[01:54:37.400 --> 01:54:42.360]   No problem. Right. All these smart replies are there. Well, AI goes off our behavior. So guess
[01:54:42.360 --> 01:54:47.640]   what smart reply tried to get people to say often. Which could by the way, I think improve the whole
[01:54:47.640 --> 01:54:54.360]   world. I love you. Oh, that was nice. I'm down with that. Yeah.
[01:54:54.360 --> 01:55:00.120]   But I'm thinking, imagine, I mean, you got to you got to reveal with your boss. You actually
[01:55:00.120 --> 01:55:07.320]   hit. I love you. Yeah, there's still that signal, though. You know, I'm down with that
[01:55:07.320 --> 01:55:12.520]   plant plant that's the Google. Maybe Google should force us all to do that.
[01:55:13.400 --> 01:55:17.000]   That's you know, this hate going around in this world plant that was the
[01:55:17.000 --> 01:55:21.160]   of that. I totally dig that as annoying as it may be to some.
[01:55:21.160 --> 01:55:26.440]   Don't even give us the opportunity. Just start sneaking that in at the very end of
[01:55:26.440 --> 01:55:31.320]   everyone or emails to each other and suddenly think of the impact that would have. If suddenly
[01:55:31.320 --> 01:55:37.720]   everyone's on email and I love you. There would be change, man.
[01:55:38.680 --> 01:55:45.000]   You know, with regards to the smart replies, I do like the fact that there's one little detail
[01:55:45.000 --> 01:55:51.560]   within them and that's the exclamation point. Speaking with a colleague of mine over the years,
[01:55:51.560 --> 01:55:58.040]   regarding corporate communication to people. And I had to train them up. I said, you know,
[01:55:58.040 --> 01:56:03.640]   you could say the same thing and put a period on the end versus an exclamation point. Yeah.
[01:56:03.640 --> 01:56:09.480]   And it just makes a world of difference to an end user. You know, if you just say, thank you,
[01:56:09.480 --> 01:56:16.360]   period, they just thank you any other IT jerk. But if you say, thank you at an exclamation point,
[01:56:16.360 --> 01:56:21.000]   they feel like, hey, he has personality, even though it you're no different.
[01:56:21.000 --> 01:56:26.120]   So I was thinking about it. Something about that exclamation point really makes a difference.
[01:56:26.120 --> 01:56:30.600]   And I'm starting to see that more and more in the smart replies that come up in my messages.
[01:56:30.600 --> 01:56:36.520]   Yeah. Well, and is that a, but that's a reflection of how you communicate, right?
[01:56:36.520 --> 01:56:41.160]   Smart. I know. I don't like that. You don't, you don't add a lot of explanation.
[01:56:41.160 --> 01:56:45.800]   I don't talk. Exclamation points. I have always used tons of exclamation points,
[01:56:45.800 --> 01:56:49.800]   almost to the point to where I feel like I overuse them. And so I've had to like peel back in the
[01:56:49.800 --> 01:56:53.560]   other direction. But I've definitely noticed my smart replies use them. And I just kind of assumed
[01:56:53.560 --> 01:56:57.560]   it was because I do. No, friendly. No. Okay.
[01:56:58.120 --> 01:57:03.880]   Red Remind said that she was dating a guy, Yale, which goes to this. And he had a rule that said
[01:57:03.880 --> 01:57:07.240]   you should use three exclamation points in life. And he'd already used one.
[01:57:07.240 --> 01:57:19.240]   I like you. I said, I said, I'm glad you didn't marry him. So I'm old enough, gentlemen. I am old
[01:57:19.240 --> 01:57:24.600]   enough. Not only did I use a typewriter, not only did I use a manual typewriter,
[01:57:25.160 --> 01:57:29.240]   I was thinking about this the other day. I completely forgot this. In my day,
[01:57:29.240 --> 01:57:35.320]   when I took typing class in sixth grade, the way to get an exclamation point was you had to hit a
[01:57:35.320 --> 01:57:43.800]   apostrophe, then backspace period. Wow. So you better really mean to use the
[01:57:43.800 --> 01:57:49.480]   exclamation point with you. Yeah, you had to do DIY exclamation points. Interesting. Wow.
[01:57:49.480 --> 01:57:55.480]   Yeah. How old is that? We'd be a lot less assertive if it took that much work.
[01:57:55.480 --> 01:58:03.400]   You know, but see, my personality, I've always tried to be, you know,
[01:58:03.400 --> 01:58:08.600]   black and white binary, if you will, because I personally think that that
[01:58:08.600 --> 01:58:14.360]   keeps confusion down when I'm trying to communicate a plan to someone or, you know,
[01:58:15.320 --> 01:58:19.720]   whatever that we have going on, because I hate it when I was working in the world of support,
[01:58:19.720 --> 01:58:27.000]   saying something to a user, and they just totally go the opposite direction of what I was expecting
[01:58:27.000 --> 01:58:33.640]   or they felt like I was going to do a different action at a different time. And so I trained
[01:58:33.640 --> 01:58:41.080]   myself to just be as robotic as possible. It may sound rude to people, but that's how I got
[01:58:41.960 --> 01:58:48.760]   stuff done, you know, just black and white periods, commas, perfect capitalization.
[01:58:48.760 --> 01:58:53.240]   Every now and then through an exclamation point, if I felt like somebody needed a warm and fuzzy,
[01:58:53.240 --> 01:59:00.200]   but most of the time, no. Oh man, I'm so I'm so the opposite of that. And I realized it.
[01:59:00.200 --> 01:59:07.400]   Ask my family. Yeah, like I so often I'll write an email and then I have to go and
[01:59:07.400 --> 01:59:12.920]   impart of my checking the email is to make sure I have put in too many exclamation points. Like
[01:59:12.920 --> 01:59:17.720]   every sentence ends with it. I'm like, why am I so excited about every single thing I'm saying
[01:59:17.720 --> 01:59:23.000]   right now to replace them with periods? And I have to specifically go in there and do that.
[01:59:23.000 --> 01:59:26.520]   I'm I'm I realize I'm admitting too much right now.
[01:59:26.520 --> 01:59:31.320]   Being a little too open to you all. It's because it's because I love you.
[01:59:32.840 --> 01:59:38.920]   Well, despite all the leaks all year long, in fact, I think the Pixel 3 phone was the most
[01:59:38.920 --> 01:59:42.760]   leaked phone of all time. And there wasn't anything we didn't know about it. When it came out,
[01:59:42.760 --> 01:59:47.720]   we were still pretty excited about it. Here's our first look at Google's latest phone.
[01:59:47.720 --> 01:59:50.920]   I got a Pixel 3 XL.
[01:59:50.920 --> 01:59:54.760]   Whoa, let's see the notch. Can't even see the notch.
[01:59:54.760 --> 01:59:58.200]   Well, yeah, that's because it's an iPhone.
[01:59:58.200 --> 02:00:04.440]   Hang on. That has a notch too, by the way. There is the notch. That's not that big.
[02:00:04.440 --> 02:00:10.280]   That's that's obnoxious. It's awful. It's it's double the height. It brings the status bar
[02:00:10.280 --> 02:00:14.760]   almost to like double the height. And that's what people are going off about. And I I get it.
[02:00:14.760 --> 02:00:23.960]   It's not ideal, but you know, I don't think it's terrible. 799 for a 64 gig version, 100 bucks more
[02:00:23.960 --> 02:00:31.720]   for a double the memory 128 gig storage version for the XL and 100 bucks less in each category
[02:00:31.720 --> 02:00:40.120]   for the smaller. Was it 799? Oh, that's what it was. 799 for the Pixel 3 899 for the XL.
[02:00:40.120 --> 02:00:48.760]   I end up spending 999. I got double. I am not going to buy, you know, I've been a Pixel or
[02:00:48.760 --> 02:00:55.320]   Google hardware person forever. That's too much money. And I would get the smaller version because,
[02:00:55.320 --> 02:00:59.800]   you know, tiny hands and pockets and all that nonsense, but. And there's no notch in the smaller
[02:00:59.800 --> 02:01:06.760]   version, right, Kevin? Correct. It's just a small chin and forehead. So chin and forehead? That's
[02:01:06.760 --> 02:01:12.280]   what we that's what we call it. Like the old Pixel 2 XL. Right. A top and bottom bezel. There's a
[02:01:12.280 --> 02:01:18.120]   chin. What the kids used to say and a and a forehead. No, it's a chin. A forehead. No,
[02:01:18.120 --> 02:01:24.840]   which which everybody wonders because the new XL has a chin, but no forehead. Instead,
[02:01:24.840 --> 02:01:34.520]   it's like my first girlfriend's lobotomy. So, um, I'm sorry, Kathy, that was not a good thing to say.
[02:01:34.520 --> 02:01:43.640]   You had a little bit of a forehead, but she didn't have a notch. I can, I'm pretty sure. So, um,
[02:01:43.640 --> 02:01:48.920]   why do you have to do it for you now? Yeah. Thank you. Did you see me just open my mouth and
[02:01:48.920 --> 02:01:58.920]   horror there? I'm like, Oh, it's a show. It's a show. It's a show. Yeah. So I'm going to come in a
[02:01:58.920 --> 02:02:04.840]   year to Kevin and be like, Yo, Kevin, give me a cheap phone or give me a good recommendation for
[02:02:04.840 --> 02:02:09.480]   because with with Android, what is it? What is Google called the now everyone gets Android at
[02:02:09.480 --> 02:02:13.800]   the same time kind of effort? Android one. Android one. I was going to say Android first.
[02:02:13.800 --> 02:02:20.520]   And Treble makes that possible. Yeah. For code name. Treble. So, yeah. I still wouldn't get a
[02:02:20.520 --> 02:02:25.720]   Google phone. I don't trust anybody else. Why did you or what? What a nice you would find in the
[02:02:25.720 --> 02:02:29.720]   other room. And I don't trust Sam's time with it. So what you said before was over. You ordered
[02:02:29.720 --> 02:02:34.440]   water. Pressed you. Well, I think the camera and I'm sure Kevin will have something to say about
[02:02:34.440 --> 02:02:39.880]   that. But it's and so far I've heard from and seen it from a number of people who were at the
[02:02:39.880 --> 02:02:44.600]   event and handled it that it's everything that last years was not that the screen this time is
[02:02:44.600 --> 02:02:49.320]   very good that the feel in the hand is very good. I like it that it has wireless charging because
[02:02:49.320 --> 02:02:53.560]   every other phone I use has wireless charging. That's a real convenience for me. I bought the
[02:02:53.560 --> 02:02:57.720]   stand just to try it because it sounded like it had special features. We had somehow recognized.
[02:02:57.720 --> 02:03:03.720]   Actually, that's another question which is yet unanswered. Does it automatically turn on the
[02:03:03.720 --> 02:03:11.720]   screen saver the daydream photo album and the larger clock if it's not on its pixel stand?
[02:03:11.720 --> 02:03:17.640]   Are we going to see an unboxing now? We could do that. I haven't opened it yet only because I have
[02:03:17.640 --> 02:03:24.040]   a wireless charger already. And those special features either are not enabled yet on the review
[02:03:24.040 --> 02:03:29.640]   model because there are a few software bits not here yet. Or it's smart enough to know, hey,
[02:03:29.640 --> 02:03:33.960]   this isn't a pixel stand. I'm not going to do the special stuff. But you're right that that
[02:03:33.960 --> 02:03:40.760]   when on that stand, it kind of turns the phone into a limited mini Google Home Hub in a sense because
[02:03:40.760 --> 02:03:48.280]   it can show you your photos. It has your routine buttons for the Google Assistant. And it'll even
[02:03:48.280 --> 02:03:52.840]   show your Nest camera doorbell or doorbell camera rather if somebody rings the doorbell.
[02:03:52.840 --> 02:03:55.480]   Yeah, so that's kind of so Kevin, would you buy it?
[02:03:55.480 --> 02:04:02.360]   You know, Jeff, I'm in a different position at this point because I'm kind of like trying to
[02:04:02.360 --> 02:04:07.960]   budget things for the first time in my life. And I literally just bought two months ago
[02:04:08.520 --> 02:04:17.640]   $325 Nokia 7 Plus, which lasts for two days. It's speedy enough for me. The camera is decent enough.
[02:04:17.640 --> 02:04:23.240]   So given that, I'd probably say no. If I had the money to burn, yeah, I probably would just because
[02:04:23.240 --> 02:04:27.720]   the camera features alone had blown me away in the past just 24 hours.
[02:04:27.720 --> 02:04:33.000]   Does highlight Google's problem though? If people have enough money, they'll probably buy an iPhone
[02:04:33.800 --> 02:04:38.520]   because people who buy Android phones tend to be budget consumers. I don't know about that.
[02:04:38.520 --> 02:04:42.120]   I think so. I think that's the real problem. And if they're not budget consumers, they're
[02:04:42.120 --> 02:04:48.360]   probably buying Samsung. We know that Google only sold 10 million pixel twos in a year.
[02:04:48.360 --> 02:04:57.800]   Go on, Kevin. Well, I think you're partly right and partly wrong. And I say that because the
[02:04:57.800 --> 02:05:02.280]   difference is there really isn't many cheap iPhone options unless you want something that's two or
[02:05:02.280 --> 02:05:06.680]   three years old. So if you're going to have iOS, you've got to pay X on the dollar.
[02:05:06.680 --> 02:05:09.560]   That's not saying. If you've got a lot of money, you're going to get a life.
[02:05:09.560 --> 02:05:17.080]   Sure. Well, Samsung does sell a lot of galaxies. It's not their biggest money maker in terms of
[02:05:17.080 --> 02:05:23.640]   overall volume times profit. So that's why they have a low and mid-range phone.
[02:05:23.640 --> 02:05:29.000]   That's problem number two for Google is if you've got a small, I think there's a smaller number of
[02:05:29.000 --> 02:05:32.760]   Android people who are willing to spend money if you've got, but those people are buying Samsungs.
[02:05:32.760 --> 02:05:38.120]   So problem number two, at the very best Google's dividing the high end of the Android market
[02:05:38.120 --> 02:05:43.080]   with Samsung at the best. What's their business goal? Is it to be a highly profitable item or is
[02:05:43.080 --> 02:05:48.040]   it still to set the bar? Sure looked like it. Watching the event, it looked like that they
[02:05:48.040 --> 02:05:51.800]   think they're a player. They want to be a player. It's not a reference problem.
[02:05:51.800 --> 02:05:57.640]   Yeah. I would agree with that. They must have some, and this probably comes into their CFO who's
[02:05:57.640 --> 02:06:01.960]   been there what two, at least two years now, they probably have some hard targets on that
[02:06:01.960 --> 02:06:05.320]   hardware division now because things have changed ever since Ruth Poorat came on.
[02:06:05.320 --> 02:06:09.000]   They want to make serious money out of that hardware division.
[02:06:09.000 --> 02:06:12.840]   So I have my year old Pixel 2. Oh, by the way, before I forget,
[02:06:12.840 --> 02:06:17.480]   do you still get the always on notification screen thing like this?
[02:06:17.480 --> 02:06:23.800]   Yes. I love that. I do love that. I do love that a lot. So there's a year old phone. It's
[02:06:23.800 --> 02:06:31.480]   perfectly good. It's wonderful. Kevin, your advice to me, not that I'm rich, I'm not, but is it worth?
[02:06:31.480 --> 02:06:35.640]   Is the camera alone worth upgrading? Put it that way.
[02:06:35.640 --> 02:06:39.080]   From what do you have now, Jeff? The Pixel 2.
[02:06:39.080 --> 02:06:43.800]   You know, I don't have one of those anymore. I have a first gen Pixel.
[02:06:43.800 --> 02:06:46.920]   Here's part of the question mark, and maybe Kevin can help us with this.
[02:06:46.920 --> 02:06:51.080]   Google wasn't completely clear about which of the camera features are migrating to the Pixel 2
[02:06:51.080 --> 02:06:55.880]   and which are not. Both of them are using that same chip, the image engine.
[02:06:55.880 --> 02:07:01.400]   And it sounded like they in the announcement that some of these features, if not all of these
[02:07:01.400 --> 02:07:05.640]   features are going to be available, at least on the Pixel 2, if not other Android phones.
[02:07:05.640 --> 02:07:13.320]   Yeah, I'm not. I suspect the night site is not going to be pushed down, and that is just an
[02:07:13.320 --> 02:07:17.400]   opinion. I don't have hard information on that. I also believe, and I've been trying to research
[02:07:17.400 --> 02:07:23.240]   this afternoon, and I'm kind of coming up short, if they upgraded that visual core chip in this
[02:07:23.240 --> 02:07:27.000]   model. I know they put the Titan security chip in here, but obviously, it has nothing to do with
[02:07:27.000 --> 02:07:33.160]   photos. I seem to believe they have updated the visual core chip, in which case, some things may
[02:07:33.160 --> 02:07:39.000]   not come all the way down the line. This is a life hacker article on how to get some of the
[02:07:39.000 --> 02:07:43.720]   features or what features, and this is the problem, is Google wasn't clear in the announcement,
[02:07:44.600 --> 02:07:49.880]   but what features will migrate down? No, I don't want that. Thank you.
[02:07:49.880 --> 02:07:55.480]   Google nights. Here you go. According to Google night site, we'll be rolling out to previously
[02:07:55.480 --> 02:08:05.400]   released Pixel smartphones in the coming weeks. Okay, duplex. duplex is getting a full release
[02:08:05.400 --> 02:08:09.320]   on the Pixel 3 devices and will be coming to other Pixel phones in November.
[02:08:09.320 --> 02:08:16.040]   Remind us what duplex is. That's that assistant that talks to you and adds us.
[02:08:16.040 --> 02:08:22.760]   So, they announced... Was that your imitation of a computer? No, no, that was just...
[02:08:22.760 --> 02:08:32.040]   The call screening, which I think everybody in the room, Kevin, enjoyed a lot, the idea that the
[02:08:32.040 --> 02:08:37.000]   call screening driven by Google Assistant on phone call screening will also be on
[02:08:38.040 --> 02:08:45.000]   all Pixel phones. I think they stole that. They stole it from themselves with Google voice.
[02:08:45.000 --> 02:08:50.920]   Ah, yes, Google voice does that. But Google voice, it's not quite the same because with Google voice,
[02:08:50.920 --> 02:08:55.960]   you have to have a preset in the Google voice settings where you say, "Hey, if they're not
[02:08:55.960 --> 02:09:02.200]   on my... As for instance, if they're not on my contact list screen or if they're an unknown
[02:09:02.200 --> 02:09:06.840]   caller screen, this one, as a call comes in, you can press a button that says screen that one.
[02:09:07.800 --> 02:09:11.160]   Yeah, in real time, Google voice does not do that. That is awesome.
[02:09:11.160 --> 02:09:19.240]   So, because I'm getting calls all the time from like Apple. And I love just press, instead of just
[02:09:19.240 --> 02:09:28.040]   hang up on them, screen it or something. Playground and improved AR will launch first in the Pixel 3,
[02:09:28.040 --> 02:09:32.440]   but roll out to other Pixel's phone. So, I might be sticking with... I can't wait, we're not there yet.
[02:09:32.440 --> 02:09:36.840]   I know and I don't want to get ahead. But I'm mostly looking forward to is Kevin's view of the
[02:09:36.840 --> 02:09:40.840]   Pixel book. Okay, we'll get there. We're going to get there. I know we're out there yet. That's
[02:09:40.840 --> 02:09:46.040]   my... If I'm going to spend a thousand bucks or more, where's it going to go?
[02:09:46.040 --> 02:09:52.280]   Let's continue on with the phone. Because, Kevin, you said something that I think intrigues
[02:09:52.280 --> 02:09:55.640]   everybody listening. You were very excited about the camera.
[02:09:55.640 --> 02:10:01.480]   Yes. So, I've been playing with the camera. Again, it does not have the late say yet.
[02:10:01.480 --> 02:10:06.360]   It did just get the playground feature this morning. So, I was having Ironman run around my house,
[02:10:06.360 --> 02:10:13.160]   virtual AR type kind of thing. The front facing selfie mode,
[02:10:13.160 --> 02:10:20.280]   portrait mode rather, and selfie, I thought is amazing. Absolutely amazing. I don't know if we
[02:10:20.280 --> 02:10:27.720]   can share a picture that I took, but I took one in. It's on the rundown. I try to share it through
[02:10:27.720 --> 02:10:32.440]   Google photos. Hopefully, it is shareable or viewable, I should say.
[02:10:32.440 --> 02:10:38.280]   All right. Where did you put it in the... It's line 20.
[02:10:38.280 --> 02:10:42.520]   Oh, line 20. Oh, self-portrait. Here it is. I've got it.
[02:10:42.520 --> 02:10:47.560]   Can you see it? I'm not sure. So, one of the things Marques Brownlee did is took a picture of
[02:10:47.560 --> 02:10:53.880]   himself in his car and he said, "Sure isn't softened like the iPhone 10s." Yeah, look at that.
[02:10:53.880 --> 02:10:59.240]   Whoa, look at that detail. Wow, stubble. We can see we can count his stubble.
[02:10:59.240 --> 02:11:03.560]   Now, we're blowing this up a lot, but actually that looks... And look at the bokeh.
[02:11:03.560 --> 02:11:08.280]   Yeah, that's cool. This looks good. This looks really good.
[02:11:08.280 --> 02:11:13.640]   Yeah, I'm very impressed with that. The way the iPhone is doing it is actually kind of weird.
[02:11:13.640 --> 02:11:18.360]   It's softening a lot of it. It's over processing it and a lot of people complain about that.
[02:11:19.400 --> 02:11:22.280]   That's really the competition, of course, is in new iPhone 10s.
[02:11:22.280 --> 02:11:29.880]   The iPhone is now doing what Google did last time with the Pixel 2, which is taking multiple
[02:11:29.880 --> 02:11:35.320]   images. Did you say they don't yet have the best shot feature? Is that not there or...?
[02:11:35.320 --> 02:11:42.360]   Yeah, so they have to turn the motion on. There's a setting in the phone to do that.
[02:11:42.360 --> 02:11:47.560]   And you can turn the motion on, but it's not yet showing you which is the recommended photos,
[02:11:47.560 --> 02:11:52.920]   nor is it letting you choose a photo. However, that picture I just took or just showed was actually
[02:11:52.920 --> 02:11:57.080]   at the Google because I went there after lunch to hang with some of my old teammates. And
[02:11:57.080 --> 02:12:01.880]   I did see that feature and got to use it. They're dog-fooding it right now. So it's
[02:12:01.880 --> 02:12:08.280]   probably going to show up like any day on the Pixel 3 XL. It was very, very cool in the demo,
[02:12:08.280 --> 02:12:12.120]   and then I got to use it and it worked really, really well. We were being goofballs like Blinken
[02:12:12.120 --> 02:12:16.520]   and sticking our tongues out. And it was able still to capture a decent photo.
[02:12:16.520 --> 02:12:20.440]   So this was actually my favorite moment in the event. It's worth probably showing it again.
[02:12:20.440 --> 02:12:25.960]   This is Google explaining why they did Top Shot. Do you hear the audio? You don't hear the audio.
[02:12:25.960 --> 02:12:29.000]   Yeah, this was a great video.
[02:12:29.000 --> 02:12:40.760]   So there are a lot of pictures of just the wrong moment. Guys jump off the cliff, but you don't get
[02:12:40.760 --> 02:12:46.200]   lots of people with terrible faces. She's blown out the candles, but they only get it afterwards.
[02:12:46.200 --> 02:12:53.560]   The tie is in the face of the bride. Guys, walk them through the picture. Everybody has taken these
[02:12:53.560 --> 02:13:01.560]   pictures where you just miss the shot. I love this.
[02:13:01.560 --> 02:13:12.120]   So what Google lets you do is, so this interest, you have to have live picture turned on, is go back.
[02:13:13.000 --> 02:13:18.360]   It will make a recommend based on, they said, AI recommended Top Shot, but you could also
[02:13:18.360 --> 02:13:23.800]   choose from other ones. It doesn't store every one of those, because it's free storage anyway,
[02:13:23.800 --> 02:13:29.800]   so what does it matter? I'm gonna have to defer to Kevin because he's the only one who owns one.
[02:13:29.800 --> 02:13:34.040]   Mine will come a week from tomorrow. Yours too probably, right? I have an order one.
[02:13:34.040 --> 02:13:39.800]   Yes, it will. You're gonna do just order it now. You're the devil. He's waiting for
[02:13:39.800 --> 02:13:44.360]   it to hear about the Pixel Book. Let's wait. Let's help Jeff out. Wait, wait, wait. We have the
[02:13:44.360 --> 02:13:51.160]   suspense. I did not order this late, so this will be an interesting opportunity to go on to.
[02:13:51.160 --> 02:13:57.880]   So you said the Top Shot is not enabled. I wonder though, on the iPhone, if live photos
[02:13:57.880 --> 02:14:04.680]   turned on, it actually degrades the quality a little bit. The other images are not as good
[02:14:04.680 --> 02:14:09.720]   quality as the one you took. I'm sure Google's not doing it that way. They have enough horsepower
[02:14:09.720 --> 02:14:15.000]   in this thing, especially with the image, especially dedicated image chip, that they could have full
[02:14:15.000 --> 02:14:23.000]   quality images, I hope. But that's a problem with live photo. I think it's a combination of,
[02:14:23.000 --> 02:14:28.040]   and something that Google I think is better at than Apple. It's a combination of a burst mode,
[02:14:28.040 --> 02:14:36.360]   camera type implementation, as well as a processing software side and AI to kind of
[02:14:37.000 --> 02:14:40.680]   choose the best image for you. Does it take the image? So it must
[02:14:40.680 --> 02:14:46.120]   start capturing the minute you go to the camera or the minute you raise the phone.
[02:14:46.120 --> 02:14:50.840]   Certainly it starts capturing before you press the button, because that's the problem.
[02:14:50.840 --> 02:14:55.960]   I don't know exactly when it starts to capture. It might be always capturing and then just saving
[02:14:55.960 --> 02:15:01.320]   the few seconds before and after. I have no idea. I await the privacy, please.
[02:15:02.360 --> 02:15:06.120]   No, this is this. Well, it doesn't take it if your camera is not open, presumably.
[02:15:06.120 --> 02:15:12.040]   I think even the privacy, please can't be. It waits. You just waited.
[02:15:12.040 --> 02:15:18.680]   Did you try? Did you put childish Gambino in your in with Norm? Because that would have been cute.
[02:15:18.680 --> 02:15:22.600]   I didn't do that yet. No, not yet. Not yet.
[02:15:22.600 --> 02:15:28.600]   There's the that's the augmented reality. So again, we saw that this is not actually new. We just
[02:15:28.600 --> 02:15:33.800]   write. Did they show? Did they offer a compelling use case yet? For this?
[02:15:33.800 --> 02:15:37.240]   Augmented reality. Serious. I'm like, are we still just?
[02:15:37.240 --> 02:15:43.720]   I actually see it in industrial IoT and some other things, but no, I haven't seen anything for it
[02:15:43.720 --> 02:15:48.280]   to be on my actual phone. So I was just curious. Someone might come up with something, y'all.
[02:15:48.280 --> 02:15:54.440]   It looks like see hello, we've been Seattle before you stop saying y'all.
[02:15:54.440 --> 02:15:57.640]   Oh, was we're off? Oh, no, they love it in Seattle. Oh, is that it?
[02:15:57.640 --> 02:16:00.760]   Yeah, they just think that's so cute. It is cute.
[02:16:00.760 --> 02:16:07.640]   Here's an interesting point from the Google keyword blog, product blog. It does notice where
[02:16:07.640 --> 02:16:14.760]   you're taking a picture and then suggest an appropriate overlay. This is like Snapchat light,
[02:16:14.760 --> 02:16:23.960]   kind of. So he's he's doing some gardening and it's suggested that she put a little green thumb
[02:16:24.840 --> 02:16:30.600]   bubble. So what that says, it knows it knows the place and it kind of knows the activity.
[02:16:30.600 --> 02:16:34.360]   I expect more of this from Google because this is the kind of thing Google Photos is already
[02:16:34.360 --> 02:16:38.120]   doing. It's contextual. It kind of understands what it's seeing.
[02:16:38.120 --> 02:16:44.200]   It's almost like bringing Google lens and Google Assistant together into your compositions.
[02:16:44.200 --> 02:16:51.400]   Right. But again, you can't judge this stuff by demos, by blog posts. You got to get the phone.
[02:16:51.400 --> 02:16:54.840]   I'm going to wait till you get it. Yeah. Well, I won't have it next week,
[02:16:54.840 --> 02:16:57.880]   but I'll have it the week after. I won't be getting it at all.
[02:16:57.880 --> 02:17:01.960]   So Stacy, which one are you carrying now? I have the pixel to.
[02:17:01.960 --> 02:17:09.560]   To. Okay. So, yes. And it's it's fine. I'm I might agree. I might be there too. I might stick with
[02:17:09.560 --> 02:17:15.560]   that. So I'm good with it until I drop it or do something horrible and then poof.
[02:17:15.560 --> 02:17:20.120]   That's me. Okay. Would you say Kevin, I should get the Nokia something something?
[02:17:20.680 --> 02:17:26.120]   The new Nokia seven. It's kind of technically called the 7.1, but it's basically what I just bought,
[02:17:26.120 --> 02:17:32.280]   upgraded, slightly smaller and better screen. And unlike mine, which is an international version,
[02:17:32.280 --> 02:17:39.240]   it has the US band support. So that would work here and it's $350. But I think your issue is going
[02:17:39.240 --> 02:17:46.760]   to be Verizon. Oh, or I don't know if I don't know if I don't know if the pixel will be available.
[02:17:46.760 --> 02:17:51.160]   Pixel three will be available on Verizon. I want you to know. We're going to get the actual.
[02:17:51.160 --> 02:17:57.560]   They gave us a Verizon SIM card with the test. I have received. Am I go ahead.
[02:17:57.560 --> 02:18:01.000]   Do you stay see anything else you want to say about? Oh, I was going to segue us off of old
[02:18:01.000 --> 02:18:07.640]   journalism to go new, exciting things. The newest latest gizmo has arrived. Okay. Just to give you
[02:18:07.640 --> 02:18:12.280]   a sense for those of you watching at home, if you're listening, I'll describe this. This is the
[02:18:12.280 --> 02:18:17.800]   Google home hub. The thing Google announced at Google IO finally came today. This is a pixel
[02:18:17.800 --> 02:18:24.440]   three phone also announced at the same time. These are almost exactly the same size. So it was hard
[02:18:24.440 --> 02:18:29.880]   to tell in the announcement how big this home hub was. It ain't big. It's, it's actually a little
[02:18:29.880 --> 02:18:34.600]   bit function a little bit smaller than the pixel seven, seven inch tab or the Nexus seven,
[02:18:34.600 --> 02:18:40.760]   seven inch tablet that Google used to sell. They sold out for $127. This is $149. No camera.
[02:18:40.760 --> 02:18:45.400]   That's a light sensor. It's an ambient light sensor. If I cover it up, you'll see at night,
[02:18:45.400 --> 02:18:52.680]   the pictures get darker and darker. So it won't keep you up at night. I have to say the excellent
[02:18:52.680 --> 02:18:59.880]   camera. This is a nice photo frame. It integrates really well with Google photos. What I did for
[02:18:59.880 --> 02:19:06.040]   this is I picked people. You could choose people that you want to have in your pictures. So I chose
[02:19:06.040 --> 02:19:11.960]   my family. And every one of these pictures has somebody sometimes in a tiny, tiny picture
[02:19:11.960 --> 02:19:18.120]   of my family. And it's really fun because it goes back 20 years ever since I started taking
[02:19:18.120 --> 02:19:22.840]   digital photos. So it's a really fun photo frame. Now I've sent it for every 10 seconds.
[02:19:22.840 --> 02:19:26.040]   You can make it every minute, every five minutes, every hour. You don't have to change it that
[02:19:26.040 --> 02:19:32.680]   frequently, but just for purposes of display. And it has also got Google Assistant. Let me
[02:19:32.680 --> 02:19:42.280]   turn the microphone on and say, I don't know, listen to this week in Google. Oh, I have to say something
[02:19:42.280 --> 02:19:49.480]   first. Pardon me. Hey, play this week in Google. I don't know if it'll work.
[02:19:49.960 --> 02:19:56.280]   Sure. Here's the latest episode of this week in Google MP3, twig 478, when a doorbell rings. A
[02:19:56.280 --> 02:20:06.440]   roadie gets its wings. Hey, stop. So the sound is I think pretty good. It's coming out of these
[02:20:06.440 --> 02:20:11.960]   fabric covered speakers on the back, which is pretty good. I'm sad to say I thought, oh,
[02:20:11.960 --> 02:20:17.000]   I'll have a type C connector. It does. And it has one of those barrel plugs proprietary barrel plugs.
[02:20:17.000 --> 02:20:23.400]   So and it has and I can't show you because it's plugged in the poorest design for a plug I've
[02:20:23.400 --> 02:20:27.560]   ever seen. It's so round and people have already started complaining it doesn't work very well
[02:20:27.560 --> 02:20:33.320]   in a power strip to just be prepared for that. Yeah, Google did that with the home. Is it the
[02:20:33.320 --> 02:20:38.760]   minis? I think? Yeah, I did that. Yeah, I was really disappointed in that. Please come on.
[02:20:38.760 --> 02:20:44.120]   So you know what we're doing with this? The default display is a photo. You see, it's still
[02:20:44.120 --> 02:20:47.480]   showing the twig. It's the default display. Oh, let me turn it down a little bit because it's a
[02:20:47.480 --> 02:20:53.240]   little bright for us to see that you swipe up to get controls from the bottom. Use the Google Home
[02:20:53.240 --> 02:21:03.560]   app to to control it. And I can't see anything. It's I love the photo slideshow. And I think that
[02:21:03.560 --> 02:21:09.400]   the screen is very good for that. It's more than more than adequate for the for the slideshow.
[02:21:09.400 --> 02:21:14.520]   It also has a as you can see, a clock display. It has a variety of digital and analog clocks you
[02:21:14.520 --> 02:21:18.600]   can use. But I think everybody's going to use this as a photo frame. No photo, no calling, no
[02:21:18.600 --> 02:21:23.000]   video calling. You can do audio calling. But you can do duo on it though. Kevin and I just
[02:21:23.000 --> 02:21:30.120]   come at that. But no, but no video. Right. Yes. I like this at an I think at $149. This makes a
[02:21:30.120 --> 02:21:35.320]   lot of sense putting this in the kitchen in the bedroom. No camera, right? On the bathroom.
[02:21:35.320 --> 02:21:38.360]   I don't think it looks good in the kitchen. I'd say bedroom, maybe bathroom.
[02:21:38.360 --> 02:21:42.600]   Kitchen, I feel like I would want the larger or maybe that JBL you've got there or maybe the
[02:21:42.600 --> 02:21:47.080]   Lenovo smart display that I've got. You have the Lenovo. This is just kind of small.
[02:21:47.080 --> 02:21:52.440]   That's a little small. I would like the the 10 inch Lenovo, which I've got with me,
[02:21:52.440 --> 02:21:57.720]   is great for watching YouTube. You can watch YouTube on it. You can't watch Netflix yet on the
[02:21:57.720 --> 02:22:02.680]   larger ones, although you are supposed to eventually. Yeah. How much is the Lenovo?
[02:22:03.560 --> 02:22:09.400]   The bigger Lenovo is 250. The smaller Lenovo is 200. The smaller is 8 inches. The big one is 10
[02:22:09.400 --> 02:22:15.320]   inches. You like this? I can slide down from the top and see my home automation stuff.
[02:22:15.320 --> 02:22:21.160]   Yes. And you can control the lights. I'm very excited. Isn't that nice? Yeah. So these are the
[02:22:21.160 --> 02:22:26.760]   lights media broadcast. I could view the rooms. This one's in the shed.
[02:22:30.200 --> 02:22:36.040]   I thought that was funny. Anyway, I think this is actually a very nice device, especially at that
[02:22:36.040 --> 02:22:43.080]   price. I would say I'm going to get a couple more. I'm really pretty happy with it.
[02:22:43.080 --> 02:22:50.760]   I really love the photo frame. It's for me. This makes more sense if you've stored a lot of photos
[02:22:50.760 --> 02:22:57.480]   in Google Photos. If you've been using the face recognition, it really is fun because it goes back
[02:22:57.480 --> 02:23:02.840]   in time. Here's a picture from Hawaii from like four or five years ago. It's just completely random.
[02:23:02.840 --> 02:23:07.560]   So I like that. It doesn't sound bad. We played some music. It's not horrible.
[02:23:07.560 --> 02:23:13.160]   It's better than one would expect for something this size. But of course, yeah, I'm sure that JBL
[02:23:13.160 --> 02:23:17.080]   will sound better. And the Lenovo sounds pretty good too. I would say the Lenovo is not hugely
[02:23:17.080 --> 02:23:22.840]   better than this. I think the Lenovo sounds better. Of course, I'm not in the same room.
[02:23:22.840 --> 02:23:27.560]   So you're centering at the prom. I definitely like it. I would like it definitely bigger. I've been
[02:23:27.560 --> 02:23:34.280]   trying to actually have a computer dedicated server that just plays photos. And I like it to be,
[02:23:34.280 --> 02:23:41.240]   I like them to be big so I can see. Yeah. If you want really big, of course, if you've got a TV
[02:23:41.240 --> 02:23:47.320]   that's cast enabled, you can always just put it on TV, right? Let's get 70 inches of photos.
[02:23:48.280 --> 02:23:53.240]   Did you get up this morning and watch the Samsung developers conference keynote? Stacy?
[02:23:53.240 --> 02:23:59.000]   I did not get up this morning to watch that. However, I did. I did see the news.
[02:23:59.000 --> 02:24:05.560]   Do we want to go into that? Because I can. There's a foldable phone. That's what I was watching.
[02:24:05.560 --> 02:24:10.600]   It looked really cool in the demos, but then someone was tweeting how with the lights on,
[02:24:10.600 --> 02:24:17.000]   you can clearly see the the the the line, the Wednesday, the market. The guy who was demonstrating
[02:24:17.000 --> 02:24:22.600]   it said, this is not what it looks like. This is it. We've hidden it inside a K. This is normal
[02:24:22.600 --> 02:24:27.080]   practice with a new phone. Apple does it too. They hide him inside of kind of industrial generic
[02:24:27.080 --> 02:24:33.320]   cases. So you can't see the details. But what he's showing is a phone that has a screen on the
[02:24:33.320 --> 02:24:37.960]   front looks kind of like a thick normal phone. Again, we don't know how thick because of the case.
[02:24:37.960 --> 02:24:42.600]   And then it is you watching that loop and he opens it up and it's got one screen that has
[02:24:42.600 --> 02:24:47.480]   unfolded. Would that any perceptible line down the middle?
[02:24:47.480 --> 02:24:52.200]   So yeah, in the in the other video with the lights on, you see the line down the middle.
[02:24:52.200 --> 02:25:03.960]   Yes. Who they demoed it in the dark. Oh, that's a little. There was a tweet from someone in the
[02:25:03.960 --> 02:25:10.440]   tech press who saw it the night before during the rehearsal run through. But I did not like it.
[02:25:10.440 --> 02:25:12.200]   So it's possible I will never see it again.
[02:25:12.200 --> 02:25:19.800]   Yeah. So I was paying attention to some of the smart things stuff because I OT.
[02:25:19.800 --> 02:25:24.440]   That's when I thought of you is during during the IOT presentation. They didn't mention security
[02:25:24.440 --> 02:25:31.800]   at all. Well, they mentioned interoperability. That seemed to be a key.
[02:25:31.800 --> 02:25:38.280]   Yeah. This is this is a different interoperability than they had pitched. So I've been watching Samsung
[02:25:38.280 --> 02:25:49.160]   do this for let's see, 2004 CES 2014. Maybe it was or 2013 when Samsung's
[02:25:49.160 --> 02:25:54.680]   oh, who was it who keynoted and he just was like, la la la internet of things,
[02:25:54.680 --> 02:25:58.920]   hand holding, everything's going to be magic. This was kind of a return to that.
[02:25:59.720 --> 02:26:08.600]   And the problem is they're grafting this vision, which is actually still very Samsung centric
[02:26:08.600 --> 02:26:15.320]   onto a platform that was designed for super nerds. And the super nerds hate the platform.
[02:26:15.320 --> 02:26:18.600]   And the normal people are looking at this and going, what the hell do I want my dishwasher to
[02:26:18.600 --> 02:26:24.840]   talk to my television? And the demo they used had a refrigerator with a screen bigger than this on
[02:26:24.840 --> 02:26:32.360]   it. I mean, it was huge. So, you know, I could be doing twig from my my refrigerator cam one day.
[02:26:32.360 --> 02:26:40.760]   I'm not I'm not a fan. But yeah, so it's kind of a cool vision, but it's not as open as you would
[02:26:40.760 --> 02:26:47.640]   think. It's not as it's not as transparent how we're going to get there to normal people. You
[02:26:47.640 --> 02:26:52.440]   have to do a lot on the back end. And I'm not sure if it's really worth it to people who aren't
[02:26:52.440 --> 02:26:58.920]   named Samsung. So that's my quick 20 second take there. It seems like it's really pushed Bixby as
[02:26:58.920 --> 02:27:06.040]   well. Their voice assistant said they're going to spend $22 billion on AI a higher 1000 AI developers.
[02:27:06.040 --> 02:27:13.560]   They brought the former developer of Siri, who of course, when Apple acquired Siri, left Apple
[02:27:13.560 --> 02:27:20.760]   and started VIV, which is Samsung that acquired. He's still in Samsung and really promoted the
[02:27:20.760 --> 02:27:24.840]   idea that Bixby is going to be in everything. And how do we think that's going to work?
[02:27:24.840 --> 02:27:33.400]   I'm not I'm not having a big speed couple of Bixby phones of the last year, the note nine and the
[02:27:33.400 --> 02:27:39.160]   S nine with the hardware dedicated hardware button never used it. But that was from a different
[02:27:39.160 --> 02:27:45.480]   era. That's when Bixby was just wasn't a competitor to Amazon's Echo or Siri. It was just designed
[02:27:45.480 --> 02:27:50.600]   to open apps on the phone to do things with apps on the phone. Now it's supposed to be a full-fledged
[02:27:50.600 --> 02:27:55.000]   voice assistant, which they kept saying is smarter than any other voice assistant you've used.
[02:27:55.000 --> 02:27:59.880]   It could be with VIV. So if they do this right and they create a voice assistant where you're like,
[02:27:59.880 --> 02:28:07.960]   hey, Bixby, let me think of it. Have my doorbell contact me when the postman gets here. If I can
[02:28:07.960 --> 02:28:14.600]   say something like that, and that actually works. And VIV actually makes that very easy to develop
[02:28:14.600 --> 02:28:18.920]   on the back end theoretically. That's what they're showing time. That's what they showed. They showed
[02:28:18.920 --> 02:28:25.560]   a really nice developer environment. He booked a room on Mars at a hotel that was very expensive.
[02:28:25.560 --> 02:28:31.800]   And that was the demo. And he did a little bit of magic, turned a tape tie into a real tie,
[02:28:31.800 --> 02:28:42.360]   which distracted me. But it's all wishing right now from Samsung. But you know what?
[02:28:42.360 --> 02:28:48.200]   This is a nascent technology. Amazon has the lead right now. Google's coming on strong. Apple would
[02:28:48.200 --> 02:28:52.360]   like to be there as well. Microsoft's given up with Cortana. They've really said,
[02:28:52.360 --> 02:28:57.640]   I don't think we're going to they actually moved it to the Office 365 group this week.
[02:28:57.640 --> 02:29:03.400]   That makes sense though, because Cortana clearly isn't going to be in your home life. So it's only
[02:29:03.400 --> 02:29:08.840]   going to be your assistant for business. I'll say for Bixby, here's where Bixby with the VIV could
[02:29:08.840 --> 02:29:17.000]   actually the VIV with a VIV could win or do well. It's with creating those experiences in a way that's
[02:29:17.000 --> 02:29:22.200]   very seamless for the user and relatively easy for the developer. Because right now,
[02:29:22.200 --> 02:29:27.800]   if you want to do Amazon or Google, especially Google's still a pain to develop for.
[02:29:27.800 --> 02:29:34.120]   It's a little bit harder. And with VIV, the theory is you would tell it,
[02:29:34.120 --> 02:29:42.280]   you basically use this AI, and I'm using that in big quotes here, AI, to understand what a user
[02:29:42.280 --> 02:29:48.680]   wants to do and automatically pull together the right APIs to make that happen. So if that actually
[02:29:48.680 --> 02:29:55.080]   does work, that could get them into a lot of places. Just because if I had one Samsung thing
[02:29:55.080 --> 02:29:57.880]   and I wanted to tie it to other things, it's possible that that might work.
[02:29:57.880 --> 02:30:03.320]   It's for every one of these companies, it's an ecosystem play. And this is the thing that's a
[02:30:03.320 --> 02:30:08.600]   little frustrating. And it's one of the things that's held back, IOT in my opinion, is everybody's a
[02:30:08.600 --> 02:30:14.680]   silo. So there's always this promise. In fact, smart things started was a kickstarter based on
[02:30:14.680 --> 02:30:18.280]   the premise that we're going to be universal. We'll talk to Zigbee, we'll talk to Z-Wave.
[02:30:18.280 --> 02:30:25.240]   Is it still that universal? Stacy? It would like to be. I mean, yes, it will talk to all those radios.
[02:30:25.240 --> 02:30:33.640]   Yes, they have features that will tie into other APIs if a device hasn't. The problem is,
[02:30:33.640 --> 02:30:38.360]   people are not building things that tie into smart things. Instead, everyone wants to lock down
[02:30:38.360 --> 02:30:43.480]   their own APIs and make it like, "Oh, you want to talk to the world pool?
[02:30:43.480 --> 02:30:47.720]   Samsung says they'll talk to anybody." And I actually believe that they would. But
[02:30:47.720 --> 02:30:51.880]   world pool is not going to bust its hump to be like, "Oh, yeah, let me have my appliances,
[02:30:51.880 --> 02:30:54.200]   talk to Samsung." That's the problem. Make sure they work.
[02:30:54.200 --> 02:30:57.800]   It's the historic problem with home automation going back 20 years.
[02:30:57.800 --> 02:31:03.320]   Is everybody wants to own it? Everybody wants a silo. And all I see is Samsung saying,
[02:31:03.320 --> 02:31:10.280]   "Our silo is silo, not Amazon's, not Apple's, not Google's. Our silo, our silo." And nobody
[02:31:10.280 --> 02:31:15.960]   wants to be... I think maybe. I feel like nobody... No consumer wants to say, "Okay, I'm just going to
[02:31:15.960 --> 02:31:18.520]   buy all Apple stuff." It'll all work better.
[02:31:18.520 --> 02:31:22.520]   Yeah. Well, consumers do actually buy all Apple stuff, just to make it work better.
[02:31:22.520 --> 02:31:26.120]   They're the only one. That's the only company. Samsung know, right?
[02:31:26.120 --> 02:31:30.760]   Well, if people boil the Samsung and say, "I'm only going to buy Samsung stuff."
[02:31:30.760 --> 02:31:35.000]   So, some countries. I'm sure some... Yeah. But in Korea,
[02:31:35.000 --> 02:31:44.280]   South Korea, they're really big there. One of the problems is we don't have... That's what
[02:31:44.280 --> 02:31:52.920]   OCF was trying to do. We don't have the equivalent of a hypertext for the Internet of Things.
[02:31:52.920 --> 02:31:59.320]   So we don't have a data schema that everything uses that you can build around. You have to do
[02:31:59.320 --> 02:32:05.320]   custom for everything. And that was the problem that we all saw back in 2013, even when Samsung was
[02:32:05.320 --> 02:32:11.400]   like, "We'll be open. It'll be amazing." That doesn't scale. That doesn't scale beyond
[02:32:11.400 --> 02:32:16.760]   computing because the real world has so many more devices and things you have to care about.
[02:32:16.760 --> 02:32:24.360]   And the computing guys are... I don't know if they're willfully blind to this or what, but... Yeah.
[02:32:27.640 --> 02:32:32.840]   And there still aren't great use cases for some of this stuff. Do I need my refrigerator to talk
[02:32:32.840 --> 02:32:39.400]   to my television? I really don't think so. But is Stacey, is there an unused standard in this?
[02:32:39.400 --> 02:32:47.000]   Well, there's the OCF, which is long and storied and went through some efforts. And that's got Intel,
[02:32:47.000 --> 02:32:52.280]   Samsung, Huawei, LG are all involved. And that's basically a data schema for
[02:32:53.560 --> 02:32:58.280]   over 100 different devices. So it says basically, like, "I'm a light bulb." If you're a light bulb
[02:32:58.280 --> 02:33:03.320]   and you use OCF, it's going to say, "This is how you recognize a light bulb. This is how it turns
[02:33:03.320 --> 02:33:10.600]   on off, goes through X number of colors, dims, etc." You see Amazon took that and instead of using
[02:33:10.600 --> 02:33:14.520]   something like the data schema from OCF, they created a voice-related data schema
[02:33:14.520 --> 02:33:21.480]   with three different things, an on/off, a range, and a mode setting. So again, that's...
[02:33:23.160 --> 02:33:28.600]   That's another way of doing it. And they exist. There's just a lot of them.
[02:33:28.600 --> 02:33:35.000]   What would it take? I'm asking the two of you. For you... I mean, Stacey is actually a bad
[02:33:35.000 --> 02:33:39.160]   person to ask because you do all of this. You try all. You've got lines that won't open, but
[02:33:39.160 --> 02:33:45.000]   could, if Andrew would just say the right words, that kind of thing. But maybe I should ask Jeff,
[02:33:45.000 --> 02:33:49.640]   but I'll ask you, Stacey, put yourself in a normal person's mindset. What would it take
[02:33:50.680 --> 02:34:00.440]   for home automation to take off? I thought we were there. I thought way back in 2012 and 2013,
[02:34:00.440 --> 02:34:06.120]   I thought I saw the creation of those standards. It was all joined. OCF was coming around the
[02:34:06.120 --> 02:34:10.600]   pike and things like SmartThings that really did want to try to work with everything.
[02:34:10.600 --> 02:34:16.360]   You'll agree, though, we're not there. We're not. We're not. So it takes a universal standard.
[02:34:16.360 --> 02:34:23.560]   It takes cheaper devices. At the time, you had devices that were very expensive.
[02:34:23.560 --> 02:34:28.200]   It also takes a really good and coherent communication of the value to end user,
[02:34:28.200 --> 02:34:35.400]   which the tech companies just blew right past. You can remotely turn on your crockpot is a really
[02:34:35.400 --> 02:34:41.080]   crappy reason to spend $150 for a connected crockpot. You're talking to the guy who got bluetooth
[02:34:41.080 --> 02:34:47.560]   pots and pans. I mean, it's right. It's dopey. It's dopey. And that's why you see things like
[02:34:47.560 --> 02:34:53.000]   security systems. Those do really well. You saw the nest do well for a while because it had energy
[02:34:53.000 --> 02:34:59.640]   savings associated with it. For a while. Why did the nest stop doing well? I think there's a very
[02:34:59.640 --> 02:35:04.920]   big price ceiling of number of people are going to spend that much. And there you go back to price.
[02:35:05.720 --> 02:35:12.600]   But I think also it's just like if you could, Jeff, if you could move into a home that it was
[02:35:12.600 --> 02:35:18.680]   all set up and everything worked with your voice and it worked reliably, that would be okay.
[02:35:18.680 --> 02:35:23.400]   It's like, you know, imagine when we had cat five through the whole house.
[02:35:23.400 --> 02:35:31.320]   And but Stacy says it's right. It's about standards. And it's not just about standards that
[02:35:31.320 --> 02:35:37.000]   exist today. It's about standards. You have the faith are going to exist in five, ten years,
[02:35:37.000 --> 02:35:42.440]   even more. My house just to have an intercom in it. Yeah, there wasn't standards, but it's
[02:35:42.440 --> 02:35:48.680]   but they were bad and they died and there's still people with wires, speakers in their house.
[02:35:48.680 --> 02:35:52.520]   They spent a lot of money to put wired speakers in there. You're old enough to remember
[02:35:52.520 --> 02:35:59.800]   why are speakers? I think I don't. I think I have wired speakers. Well, you're not again,
[02:35:59.800 --> 02:36:01.800]   in a no way. Do you wear a
[02:36:01.800 --> 02:36:02.840]   ring of IOT?
[02:36:02.840 --> 02:36:09.160]   What a year it has been. It's been such a pleasure working with Jeff Jarvis,
[02:36:09.160 --> 02:36:14.360]   Stacy Higginbotham and of course, the many other folks who've stopped by like Matthew Ingram,
[02:36:14.360 --> 02:36:21.080]   Matt Cuts, it's Joan Donovan. It's really fun to do this week in Google. I think of it as our
[02:36:21.080 --> 02:36:28.920]   kind of most intellectual show where we really get down and talk about the philosophy of technology
[02:36:28.920 --> 02:36:33.160]   and how it's changing our world. I hope you'll come back for 2019. I know we have lots more good
[02:36:33.160 --> 02:36:37.960]   shows planned for you. In fact, we'll be back next week, January 2nd with a brand new this
[02:36:37.960 --> 02:36:45.720]   weekend, Google every Wednesday, 1.30 p.m. Pacific, 4.30 Eastern, 2.130 UTC, stop by and join us if
[02:36:45.720 --> 02:36:51.960]   you can watch live at twit.tv/live. In fact, we're loved to have you in the studio. The eggnog's on me.
[02:36:51.960 --> 02:36:57.080]   Just email tickets at twit.tv. We'll put a chair out for you. But of course, as with all of our
[02:36:57.080 --> 02:37:02.680]   shows, on-demand audio and video are available at our website. In this case, twit.tv/twig.
[02:37:02.680 --> 02:37:08.280]   Best thing to do though, and I wish you would do this for all of our shows. Subscribe. That way,
[02:37:08.280 --> 02:37:13.080]   you'll always have a brand new fresh this weekend Google to listen to. Just use your favorite
[02:37:13.080 --> 02:37:20.120]   podcast application and subscribe to this weekend Google. I hope you have a wonderful 2019. I hope
[02:37:20.120 --> 02:37:25.480]   the holidays have been good to you. And I do hope you get to enjoy the embrace of family and friends
[02:37:25.480 --> 02:37:30.520]   at this special time of year. I know I plan to. And thank you for letting us take a little bit of
[02:37:30.520 --> 02:37:35.000]   time off. Thanks to Karsten Bondi for putting together our best of and all of our editors who
[02:37:35.000 --> 02:37:39.880]   work so hard so that they can take a little time with their family. Here's to you for 2019.
[02:37:39.880 --> 02:37:46.440]   Have a wonderful, safe, happy and peaceful new year. Thanks for joining us. Bye-bye.
[02:37:46.440 --> 02:37:56.200]   [Music]

