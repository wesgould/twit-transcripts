;FFMETADATA1
title=Clean As a Whistle
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-06-01
track=718
language=English
genre=Podcast
comment=<p>AI scientist statement, Nvidia Spectrum-X, Amazon&\#039\;s kill list, Mirai botnet</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.160]   It's time for Twig this week in Google.
[00:00:03.160 --> 00:00:09.360]   We're going to talk about, of course, AI and that weird statement from a bunch of AI scientists
[00:00:09.360 --> 00:00:14.240]   and important people saying, "We've got to avoid a mass extinction event."
[00:00:14.240 --> 00:00:14.840]   What?
[00:00:14.840 --> 00:00:17.000]   Well, also weird, right?
[00:00:17.000 --> 00:00:22.000]   We'll also talk a little bit about Amazon and all the things they're discontinuing,
[00:00:22.000 --> 00:00:25.800]   including the celebrity voices I paid for.
[00:00:26.680 --> 00:00:34.240]   And then, finally, NVIDIA and TPUs, CPUs, GPUs and NPUs.
[00:00:34.240 --> 00:00:35.840]   What's the difference?
[00:00:35.840 --> 00:00:39.080]   We talk chips and more next on Twig.
[00:00:39.080 --> 00:00:44.960]   Podcasts you love from people you trust.
[00:00:44.960 --> 00:00:47.800]   This is Twig.
[00:00:52.520 --> 00:01:00.840]   This is Twig. This week at Google, Episode 718, recorded Wednesday, May 31st, 2023.
[00:01:00.840 --> 00:01:02.640]   Clean as a whistle.
[00:01:02.640 --> 00:01:08.640]   This episode of This Week at Google is brought to you by HPE GreenLake,
[00:01:08.640 --> 00:01:15.000]   orchestrated by the experts at CDW, who can help you consolidate and manage all your data
[00:01:15.000 --> 00:01:20.040]   in one flexible, edge to cloud platform to scale and innovate.
[00:01:20.280 --> 00:01:24.680]   Learn more at CDW.com/HPE.
[00:01:24.680 --> 00:01:26.960]   It's time for Twig.
[00:01:26.960 --> 00:01:31.080]   This week in Google to show we cover the latest news from everybody,
[00:01:31.080 --> 00:01:36.800]   but Google, Twitter, meta, Facebook, Google's hair, my hair.
[00:01:36.800 --> 00:01:38.680]   We were doing my hair earlier.
[00:01:38.680 --> 00:01:40.560]   Stacey Igan-Boffam is here.
[00:01:40.560 --> 00:01:41.720]   Hello, Stacey.
[00:01:41.720 --> 00:01:43.080]   Miss you last week.
[00:01:43.080 --> 00:01:45.880]   Stacey on IOT.com.
[00:01:47.480 --> 00:01:50.600]   Also, Mr. Ant-Pro at Miss Jew last week.
[00:01:50.600 --> 00:01:51.080]   Yeah.
[00:01:51.080 --> 00:01:54.120]   Hands-on photography, twit.tv/hop.
[00:01:54.120 --> 00:01:55.080]   Yep, yep, yep.
[00:01:55.080 --> 00:01:56.520]   Good to see you.
[00:01:56.520 --> 00:01:57.520]   Sir.
[00:01:57.520 --> 00:02:00.320]   You had the week off or no, you were a little under the weather.
[00:02:00.320 --> 00:02:01.360]   I was sick.
[00:02:01.360 --> 00:02:02.080]   Yeah. Sorry.
[00:02:02.080 --> 00:02:03.120]   Well, I'm you feeling better, though.
[00:02:03.120 --> 00:02:04.720]   I feel much better now.
[00:02:04.720 --> 00:02:06.920]   I think it's for doing too many pull-ups, personally.
[00:02:06.920 --> 00:02:08.040]   No, never.
[00:02:08.040 --> 00:02:08.680]   Never. OK.
[00:02:08.680 --> 00:02:11.000]   I'll actually make that part of my pick at the week discussion.
[00:02:11.000 --> 00:02:11.640]   Oh, good.
[00:02:11.640 --> 00:02:12.040]   All right.
[00:02:12.040 --> 00:02:12.600]   Good. All right.
[00:02:12.600 --> 00:02:13.960]   We'll save it for them.
[00:02:13.960 --> 00:02:17.960]   Also with us, the Leonard Tau Professor for the journalistic innovation
[00:02:17.960 --> 00:02:21.680]   at the Craig Newmark Graduate School of Journalism
[00:02:21.680 --> 00:02:24.680]   at the City University of New York.
[00:02:24.680 --> 00:02:27.600]   Hello, Jeff. Hello.
[00:02:27.600 --> 00:02:29.200]   Did we stop the singing?
[00:02:29.200 --> 00:02:29.440]   I didn't.
[00:02:29.440 --> 00:02:30.400]   Benito has a day off.
[00:02:30.400 --> 00:02:31.920]   John hasn't figured out how to push the buttons.
[00:02:31.920 --> 00:02:32.920]   Oh.
[00:02:32.920 --> 00:02:34.280]   Got it.
[00:02:34.280 --> 00:02:38.520]   There's the button.
[00:02:38.520 --> 00:02:40.080]   I feel better now.
[00:02:40.080 --> 00:02:47.440]   So why, why, why have the AI scientists now doubled down
[00:02:47.440 --> 00:02:50.200]   with a what is it?
[00:02:50.200 --> 00:02:53.400]   A 22 word press release.
[00:02:53.400 --> 00:02:57.680]   It's not even, you know, like a it doesn't explain it or anything.
[00:02:57.680 --> 00:03:04.880]   It's just like a statement, a statement released by leading names in AI.
[00:03:04.880 --> 00:03:08.480]   It's basically this.
[00:03:08.480 --> 00:03:09.400]   I can read it to you.
[00:03:09.400 --> 00:03:12.960]   It won't take long mitigating the risk of extinction from AI.
[00:03:12.960 --> 00:03:15.000]   Of it.
[00:03:15.000 --> 00:03:15.720]   Let me wait a minute.
[00:03:15.720 --> 00:03:16.280]   Slow down.
[00:03:16.280 --> 00:03:17.000]   What did you say?
[00:03:17.000 --> 00:03:18.040]   Extinction.
[00:03:18.040 --> 00:03:19.400]   What the hell does that mean?
[00:03:19.400 --> 00:03:19.920]   What?
[00:03:19.920 --> 00:03:27.400]   Mitigating the risk of extinction from AI should be a global
[00:03:27.400 --> 00:03:29.040]   priority.
[00:03:29.040 --> 00:03:29.720]   Priority.
[00:03:29.720 --> 00:03:33.680]   And then they had, I think somebody said, Hey, you guys, there's other issues to
[00:03:33.680 --> 00:03:38.640]   Oh, alongside other societal scale risks like pandemics
[00:03:39.000 --> 00:03:40.240]   and nuclear war.
[00:03:40.240 --> 00:03:44.360]   I think honestly, if you're talking extinction event, most scientists
[00:03:44.360 --> 00:03:49.800]   agree global warming is the is the next extinction event and maybe an asteroid
[00:03:49.800 --> 00:03:55.080]   AI signed by Jeffrey Hinton, the Google guy who retired.
[00:03:55.080 --> 00:03:55.440]   Right.
[00:03:55.440 --> 00:03:56.440]   Yeah.
[00:03:56.440 --> 00:04:00.840]   Professor of Computer Science at the University of Montreal, Sam Almond,
[00:04:00.840 --> 00:04:02.600]   the CEO of OpenAI.
[00:04:02.600 --> 00:04:05.480]   As he would decide the moratorium because he has to keep working.
[00:04:05.760 --> 00:04:06.960]   So yeah, that's what's weird.
[00:04:06.960 --> 00:04:12.200]   These are people who did not sign the moratorium letter in some cases.
[00:04:12.200 --> 00:04:14.760]   Martin Hellman from Stanford.
[00:04:14.760 --> 00:04:18.520]   It's not as long as Bill McKibben.
[00:04:18.520 --> 00:04:22.480]   He's the guy who wrote about the, you know, the end of the world
[00:04:22.480 --> 00:04:23.560]   and then the New Yorker.
[00:04:23.560 --> 00:04:26.920]   He's the climate change climate change.
[00:04:26.920 --> 00:04:28.040]   Climate change ended the world.
[00:04:28.040 --> 00:04:28.480]   Yeah.
[00:04:28.480 --> 00:04:30.360]   Well, OK, so there's nothing.
[00:04:30.360 --> 00:04:30.800]   Lawrence.
[00:04:30.800 --> 00:04:31.680]   Controversial.
[00:04:31.680 --> 00:04:32.120]   Harvard.
[00:04:32.120 --> 00:04:32.640]   I like it.
[00:04:32.640 --> 00:04:34.040]   It's controversial to say to me.
[00:04:34.040 --> 00:04:35.360]   I trust him.
[00:04:35.360 --> 00:04:36.720]   Destroy mankind.
[00:04:36.720 --> 00:04:39.480]   That mean what what bigger BSD?
[00:04:39.480 --> 00:04:40.520]   Could you imagine?
[00:04:40.520 --> 00:04:44.920]   Ian Goodfellow, who's at Google DeepMind, principal scientist?
[00:04:44.920 --> 00:04:48.200]   I mean, this is a a who's money.
[00:04:48.200 --> 00:04:49.280]   Where's the money?
[00:04:49.280 --> 00:04:51.560]   Lex Friedman, the guy that has the podcast.
[00:04:51.560 --> 00:04:52.960]   Oh, he's just ridiculous.
[00:04:52.960 --> 00:04:54.680]   The guy who does the podcast.
[00:04:54.680 --> 00:04:55.880]   My goodness.
[00:04:55.880 --> 00:04:57.360]   Well, that case.
[00:04:57.360 --> 00:04:58.480]   My goodness.
[00:04:58.480 --> 00:04:59.240]   We should listen to him.
[00:04:59.240 --> 00:05:03.920]   So Dustin Moskowitz, who was one of the founders of a Facebook.
[00:05:03.920 --> 00:05:04.760]   He's now in a song.
[00:05:04.760 --> 00:05:05.480]   Again.
[00:05:05.480 --> 00:05:05.880]   Yeah.
[00:05:05.880 --> 00:05:06.960]   Bruce Schneier.
[00:05:06.960 --> 00:05:09.040]   I love Bruce Schneier, security guru.
[00:05:09.040 --> 00:05:12.720]   I'm maybe we should get Bruce Schneier on it to explain why.
[00:05:12.720 --> 00:05:15.240]   No, you know, you know, you need Emily Bender on.
[00:05:15.240 --> 00:05:16.760]   OK, Bender.
[00:05:16.760 --> 00:05:19.240]   She's not a signatory, obviously, right?
[00:05:19.240 --> 00:05:19.840]   Well, exactly.
[00:05:19.840 --> 00:05:21.320]   She's not because she has sanity.
[00:05:21.320 --> 00:05:25.800]   OK, she is the University of Washington computational linguist who says
[00:05:25.800 --> 00:05:27.240]   enough boys enough.
[00:05:27.240 --> 00:05:28.720]   You got you got problems.
[00:05:28.720 --> 00:05:29.960]   The problems are present tense.
[00:05:29.960 --> 00:05:32.800]   Do you want to know all this is so much chest thumping?
[00:05:33.520 --> 00:05:36.280]   Well, yes, that is it is marketing.
[00:05:36.280 --> 00:05:39.640]   But, you know, a statement, it's not offensive.
[00:05:39.640 --> 00:05:40.600]   It's just silly.
[00:05:40.600 --> 00:05:44.640]   But I already get in anything out of this, this 20 word.
[00:05:44.640 --> 00:05:47.600]   Well, I mean, OK, this is the real cynic in me.
[00:05:47.600 --> 00:05:50.920]   But the cynic in me says people like Sam Altman are because it over.
[00:05:50.920 --> 00:05:53.480]   It's so overstates what's going on with AI.
[00:05:53.480 --> 00:05:56.520]   It makes you think, wow, we're we actually were they could.
[00:05:56.520 --> 00:05:57.760]   Microsoft says we got a rush.
[00:05:57.760 --> 00:05:58.840]   We got to install this stuff.
[00:05:58.840 --> 00:06:00.040]   Oh, my God, it's huge.
[00:06:00.040 --> 00:06:00.720]   It's powerful.
[00:06:00.720 --> 00:06:03.320]   Well, we're just I think they're given a lot more credit.
[00:06:03.560 --> 00:06:06.400]   If it's just spicy auto correct, it ain't going to
[00:06:06.400 --> 00:06:09.560]   think kind of wipe out man kind of the face of the earth.
[00:06:09.560 --> 00:06:09.880]   Right.
[00:06:09.880 --> 00:06:14.360]   So this is I don't I'm being very cynical by saying this, but it
[00:06:14.360 --> 00:06:17.600]   does it does promote the notion that the AI is really more
[00:06:17.600 --> 00:06:20.360]   significant than I and many others think it is.
[00:06:20.360 --> 00:06:20.840]   I agree.
[00:06:20.840 --> 00:06:21.560]   I agree.
[00:06:21.560 --> 00:06:24.760]   And I think the other thing you get to get out of it is regulatory capture.
[00:06:24.760 --> 00:06:26.400]   Is that they're there.
[00:06:26.400 --> 00:06:30.880]   You know, it's you watch all the non-congress is let us help design
[00:06:30.920 --> 00:06:33.520]   how to regulate us and cut out everybody else.
[00:06:33.520 --> 00:06:37.840]   But this is going to scare Chris Anderson of Ted Talks.
[00:06:37.840 --> 00:06:40.640]   Grimes, the musician and artist.
[00:06:40.640 --> 00:06:42.640]   Well, well, then then that's it.
[00:06:42.640 --> 00:06:43.680]   Forget the podcast.
[00:06:43.680 --> 00:06:44.680]   Aza Raskin.
[00:06:44.680 --> 00:06:48.960]   I mean, there are people I was trusted Harris, of course, of course, always.
[00:06:48.960 --> 00:06:51.640]   But there are people I respect on here as well as people I don't.
[00:06:51.640 --> 00:06:56.000]   Well, and if someone sent you that statement, I was like, Hey, layout, look,
[00:06:56.000 --> 00:06:57.160]   AI is a big deal.
[00:06:57.160 --> 00:06:58.760]   We need Congress to regulate it.
[00:06:58.760 --> 00:07:00.440]   Can you sign this statement?
[00:07:00.920 --> 00:07:02.880]   I mean, my God, you're going to sign it.
[00:07:02.880 --> 00:07:04.680]   These things come from your friends.
[00:07:04.680 --> 00:07:08.160]   Well, it's also the case that you're not going to sign it.
[00:07:08.160 --> 00:07:09.920]   Well, what are you saying?
[00:07:09.920 --> 00:07:14.120]   You're saying you're not saying that there is a risk from it of extinction for me.
[00:07:14.120 --> 00:07:16.560]   I just that if there were one, you should you should be.
[00:07:16.560 --> 00:07:17.960]   You shouldn't do it.
[00:07:17.960 --> 00:07:22.600]   There's basically like, look, AI has the potential to be like really problematic.
[00:07:22.600 --> 00:07:26.040]   And you know, maybe we should consider that.
[00:07:26.040 --> 00:07:27.760]   So does the other big problem.
[00:07:27.760 --> 00:07:29.200]   So does social media.
[00:07:29.200 --> 00:07:30.000]   All of that has.
[00:07:30.000 --> 00:07:32.640]   Yeah, you could really probably.
[00:07:32.640 --> 00:07:34.200]   I know that's why they signed it.
[00:07:34.200 --> 00:07:36.040]   You know, that's what I'm saying.
[00:07:36.040 --> 00:07:40.840]   You could safely replace the word AI or the letters AI in here with any number of
[00:07:40.840 --> 00:07:44.120]   things mitigating the risk of extinction from social media should be a global
[00:07:44.120 --> 00:07:44.720]   priority.
[00:07:44.720 --> 00:07:47.600]   Alcohol, alcohol, chipotle.
[00:07:47.600 --> 00:07:49.360]   You could put anything in there.
[00:07:49.360 --> 00:07:53.080]   Hey, he didn't say Taco Bell.
[00:07:53.080 --> 00:07:53.920]   Wrong.
[00:07:53.920 --> 00:07:55.000]   It wouldn't be wrong.
[00:07:55.000 --> 00:07:58.080]   You you start you start diving on on a touch away.
[00:07:58.080 --> 00:08:01.880]   Pepe, you're in trouble mitigating the risk of extinction from
[00:08:01.880 --> 00:08:05.160]   Kachoei Pepe should be a global priority.
[00:08:05.160 --> 00:08:09.320]   That's, I mean, you can't say, well, you're right.
[00:08:09.320 --> 00:08:11.360]   I mean, global priorities a little higher, right.
[00:08:11.360 --> 00:08:14.240]   But especially for Kachoei Pepe.
[00:08:14.240 --> 00:08:18.240]   So I guess the real question microwave test, AC, that's a global threat.
[00:08:18.240 --> 00:08:21.480]   The real question I have for you, you three is.
[00:08:21.480 --> 00:08:24.480]   Is there a risk of extinction from AI?
[00:08:24.480 --> 00:08:26.120]   And no, no.
[00:08:27.280 --> 00:08:28.200]   Is there Jeff?
[00:08:28.200 --> 00:08:32.800]   Is there is there any way you could say there's looking Leo for any scenario that
[00:08:32.800 --> 00:08:36.040]   takes me through how we go extinct and honest to God, the only thing I have found
[00:08:36.040 --> 00:08:38.600]   thus far is Max Tegmark.
[00:08:38.600 --> 00:08:45.640]   I think it was Max Tegmark, MIT theorizing that the AI could at some point decide
[00:08:45.640 --> 00:08:49.400]   to cut off oxygen so that it wouldn't rust the machines.
[00:08:49.400 --> 00:08:52.000]   No, but you don't find any any.
[00:08:53.000 --> 00:08:57.320]   Give me sense. First of all, you can't cut off the oxygen to the earth.
[00:08:57.320 --> 00:08:59.320]   So well, that's why.
[00:08:59.320 --> 00:09:01.080]   Yeah, that's why this is so stupid.
[00:09:01.080 --> 00:09:05.440]   I mean, you could you could not open the pod bay doors and then Dave's going to be
[00:09:05.440 --> 00:09:06.760]   stuck in space.
[00:09:06.760 --> 00:09:10.640]   But but unless you give a eyes agency, maybe that's what they're saying is just
[00:09:10.640 --> 00:09:13.040]   don't let them give them the nuclear codes.
[00:09:13.040 --> 00:09:16.720]   The only thing I can think of is like with the food, food supply chain.
[00:09:16.720 --> 00:09:18.920]   And but we're not giving them control of the food.
[00:09:18.920 --> 00:09:20.120]   Yeah, they don't.
[00:09:20.120 --> 00:09:21.040]   Yeah.
[00:09:21.040 --> 00:09:25.240]   Nor are they a threat to her.
[00:09:25.240 --> 00:09:30.520]   I think the biggest threat would be sewing discord via deep fakes and like
[00:09:30.520 --> 00:09:33.680]   leading to some sort of war slash global catastrophe.
[00:09:33.680 --> 00:09:35.360]   And I think that's actually a viable thing.
[00:09:35.360 --> 00:09:37.600]   That would be there.
[00:09:37.600 --> 00:09:39.600]   We don't need to know.
[00:09:39.600 --> 00:09:45.280]   But Alex Stamos, who's the leader of the Internet Observatory at Stanford,
[00:09:45.280 --> 00:09:47.920]   they would look, this is what they watch for is disinformation.
[00:09:48.200 --> 00:09:52.600]   Says that there will likely be an AI driven avalanche of disinformation
[00:09:52.600 --> 00:09:56.160]   and misinformation leading up to the 2024 presidential.
[00:09:56.160 --> 00:09:58.040]   Again, we've got plenty of that already.
[00:09:58.040 --> 00:10:00.280]   I think it's going to get worse.
[00:10:00.280 --> 00:10:01.720]   And I think we're going to see.
[00:10:01.720 --> 00:10:05.640]   I don't think it's like a bay of pig situation.
[00:10:05.640 --> 00:10:09.120]   I think what's going to happen is we're going to have much more.
[00:10:09.120 --> 00:10:15.880]   What's it called cranky people with weapons during our elections process?
[00:10:17.080 --> 00:10:18.200]   Disrupting things.
[00:10:18.200 --> 00:10:20.360]   Yes, I'm more worried about that.
[00:10:20.360 --> 00:10:21.600]   Not AI.
[00:10:21.600 --> 00:10:26.960]   But we'll know you're going to rile those people up with non truths using
[00:10:26.960 --> 00:10:28.840]   existing already there.
[00:10:28.840 --> 00:10:31.720]   I know they're there, but sorry, go ahead.
[00:10:31.720 --> 00:10:35.560]   I was like, I know they're already there, but look at what's happening with
[00:10:35.560 --> 00:10:38.920]   like that one lady who went into the target and saw the swimsuit.
[00:10:38.920 --> 00:10:40.400]   It was laying around in the kids.
[00:10:40.400 --> 00:10:43.560]   So that led to an escalation.
[00:10:43.560 --> 00:10:43.880]   Right.
[00:10:43.880 --> 00:10:46.120]   It was almost ridiculous.
[00:10:46.120 --> 00:10:47.720]   If you think about how that got there.
[00:10:47.720 --> 00:10:51.320]   So I've been saying that about Twitter since 2016.
[00:10:51.320 --> 00:10:52.360]   And I was right, by the way.
[00:10:52.360 --> 00:10:52.840]   Yeah.
[00:10:52.840 --> 00:10:56.160]   But I've been saying that I said, we have, we have, you can go back and look,
[00:10:56.160 --> 00:10:59.360]   we've weaponized social media, social media, which was this great thing.
[00:10:59.360 --> 00:11:02.320]   We were the bad guys were able to weaponize.
[00:11:02.320 --> 00:11:03.360]   Russian.
[00:11:03.360 --> 00:11:07.920]   These are bigger bullets in or compelling bullets.
[00:11:07.920 --> 00:11:10.520]   So I think then at the time, you have enough of them.
[00:11:10.520 --> 00:11:13.760]   People said, well, you're not going to ban social media because of that.
[00:11:13.760 --> 00:11:14.200]   Are you?
[00:11:15.440 --> 00:11:17.440]   Would you say that I, doesn't that?
[00:11:17.440 --> 00:11:22.080]   No, I would not say be an AI because we do think we need to understand.
[00:11:22.080 --> 00:11:24.800]   Well, I don't, that's a good question.
[00:11:24.800 --> 00:11:30.320]   Like I, I'm like, we need to understand how these things propagate,
[00:11:30.320 --> 00:11:32.760]   how we can stop them from propagate.
[00:11:32.760 --> 00:11:34.720]   I mean, maybe stopping them isn't the right.
[00:11:34.720 --> 00:11:35.880]   How to diffuse this.
[00:11:35.880 --> 00:11:37.200]   Here's a scenario, Stacy.
[00:11:37.200 --> 00:11:37.720]   Just go.
[00:11:37.720 --> 00:11:43.360]   This is so I'm going to go the opposite way that there's so much junk out there that no one
[00:11:43.360 --> 00:11:47.960]   believes anything and everyone becomes cynical and critical again.
[00:11:47.960 --> 00:11:51.120]   And we're actually better off that, by the way, you and I will do that.
[00:11:51.120 --> 00:11:54.600]   But then there's plenty of people who are just, they've got issue trigger fingers
[00:11:54.600 --> 00:11:56.520]   and there aren't those really problems.
[00:11:56.520 --> 00:11:58.400]   He's not AI or social media.
[00:11:58.400 --> 00:12:00.800]   I mean, this is what I was told.
[00:12:00.800 --> 00:12:04.560]   This is what I was told when I said social media is the problem is then you have to
[00:12:04.560 --> 00:12:05.680]   educate the electorate.
[00:12:05.680 --> 00:12:07.280]   You have to teach people critical thinking.
[00:12:07.280 --> 00:12:10.880]   I was told the solution isn't doing something that is social media.
[00:12:10.880 --> 00:12:13.720]   That's just the solution is to fix people.
[00:12:13.720 --> 00:12:15.280]   That didn't work so well.
[00:12:15.280 --> 00:12:19.200]   And I and so I listened to Jeff Charlotte's book Undertale right now.
[00:12:19.200 --> 00:12:22.040]   And Jeff's a really good academic and journalist.
[00:12:22.040 --> 00:12:26.240]   And the scariest thing in the book, by far to me, is this silliness you're about to hear.
[00:12:26.240 --> 00:12:32.840]   Is that people argued that the really rabid Trump followers argue that everything in his
[00:12:32.840 --> 00:12:33.960]   tweets is purposeful.
[00:12:33.960 --> 00:12:36.800]   Every capital letter, every capitalization has meaning.
[00:12:36.800 --> 00:12:37.080]   Yeah.
[00:12:37.080 --> 00:12:41.920]   And they add up the numbers from the capital letters for secret messages, right?
[00:12:41.920 --> 00:12:42.320]   It's Q and I.
[00:12:42.320 --> 00:12:43.640]   Those are Taylor Swift fans.
[00:12:43.640 --> 00:12:45.480]   Well, it's human, isn't it?
[00:12:45.480 --> 00:12:47.040]   It's a very human thing.
[00:12:47.040 --> 00:12:52.280]   Oh, when I was a DJ, the craziest people, you know, you'd always anytime you're in the
[00:12:52.280 --> 00:12:55.240]   public eye, a certain percentage of the population will give them a phone.
[00:12:55.240 --> 00:12:55.640]   Yeah.
[00:12:55.640 --> 00:12:59.320]   So those there were lots of people who thought, Oh, you're playing that song for me.
[00:12:59.320 --> 00:13:03.200]   Because our brain is very able of making these connections.
[00:13:03.200 --> 00:13:06.640]   And the person would say, Oh, yeah, no, you're sending me a message.
[00:13:06.640 --> 00:13:07.520]   I know you are.
[00:13:07.520 --> 00:13:11.840]   That's a very common form of mental illness, by the way, that anybody who's been in radio
[00:13:11.840 --> 00:13:18.520]   or TV knows and we've in 50 years ago, a guy came into the KGO radio station
[00:13:18.520 --> 00:13:19.120]   at San Francisco.
[00:13:19.120 --> 00:13:23.040]   I know you remember this, uh, Jeff and shot the place up because he said you're
[00:13:23.040 --> 00:13:25.600]   putting your broadcasting into my head.
[00:13:25.600 --> 00:13:27.960]   I had somebody that was a joint cleaning movie.
[00:13:27.960 --> 00:13:29.880]   Maybe.
[00:13:29.880 --> 00:13:32.760]   So you had a lady cosmic lady call you cosmic lady.
[00:13:32.760 --> 00:13:35.560]   She would call and she discovered that my husband machine, because I was one of the
[00:13:35.560 --> 00:13:39.520]   had were the few ones in the examiner that you she could leave an hour long
[00:13:39.520 --> 00:13:40.000]   message.
[00:13:40.000 --> 00:13:40.520]   Mm.
[00:13:40.520 --> 00:13:42.240]   I said, she did.
[00:13:42.240 --> 00:13:43.120]   Oh, she did.
[00:13:43.120 --> 00:13:43.560]   Oh, she did.
[00:13:43.560 --> 00:13:45.280]   I found the solution here.
[00:13:45.280 --> 00:13:47.320]   Felix Simon on on Twitter.
[00:13:47.320 --> 00:13:50.080]   Uh, the one thing I'm going to get behind is this.
[00:13:50.080 --> 00:13:54.280]   Please consider signing my open letter for moratorium on AI open letters.
[00:13:54.280 --> 00:13:55.720]   Absolutely.
[00:13:55.720 --> 00:13:57.960]   Good call.
[00:13:57.960 --> 00:14:02.360]   Here's what the information, uh, writes Martin peers.
[00:14:02.360 --> 00:14:04.040]   Well, you could afford to read it.
[00:14:04.040 --> 00:14:05.240]   Yeah, I could pay for this.
[00:14:05.400 --> 00:14:07.680]   Well, yeah, that's one way to get attention.
[00:14:07.680 --> 00:14:08.640]   Yeah, exactly.
[00:14:08.640 --> 00:14:14.640]   The question is why are very people responsible for AI's development, suggesting
[00:14:14.640 --> 00:14:19.200]   it should be a priority to stop the new technology from killing off every human.
[00:14:19.200 --> 00:14:23.560]   Aren't they the best people to ensure that doesn't happen?
[00:14:23.560 --> 00:14:28.400]   Yes and no, he goes on to write AI scientists want government's help.
[00:14:28.400 --> 00:14:31.640]   That goes back to that regulation capture that.
[00:14:31.640 --> 00:14:32.640]   Okay.
[00:14:32.640 --> 00:14:33.040]   All right.
[00:14:33.040 --> 00:14:34.560]   Yeah.
[00:14:34.880 --> 00:14:35.240]   Yeah.
[00:14:35.240 --> 00:14:38.440]   Um, it's also part of this whole long termism crap.
[00:14:38.440 --> 00:14:44.440]   Um, and acceleration is, um, let's get there as fast as we can to prove how big we are.
[00:14:44.440 --> 00:14:44.800]   Yeah.
[00:14:44.800 --> 00:14:48.240]   Uh, you know, it's, there's a lot of crazy stuff going on.
[00:14:48.240 --> 00:14:52.440]   And again, the academics who were actually studying this stuff, the authors of the
[00:14:52.440 --> 00:14:55.640]   stochastic parents paper, how often do you see them quoted anywhere?
[00:14:55.640 --> 00:14:58.280]   They're the ones who actually worry about the present case dangers.
[00:14:58.280 --> 00:14:59.400]   They warn of them.
[00:14:59.400 --> 00:15:04.600]   They say they are present case dangers, but that all gets that's jumped
[00:15:04.600 --> 00:15:07.560]   onto the rug because we're going to talk about things that are just supposedly
[00:15:07.560 --> 00:15:10.040]   just, just going to destroy mankind.
[00:15:10.040 --> 00:15:13.960]   It's peers goes on to write cynics will say, we shouldn't read too much.
[00:15:13.960 --> 00:15:16.400]   This is me, by the way, we shouldn't read too much into this statement.
[00:15:16.400 --> 00:15:21.800]   Scientists are essentially virtue signaling preempting political calls for regulation
[00:15:21.800 --> 00:15:24.800]   by taking the lead in that campaign nailed it themselves.
[00:15:24.800 --> 00:15:28.280]   An alternative cynical view, you can choose this one if you wish, is that
[00:15:28.280 --> 00:15:32.800]   existing AI firms are simply pushing for regulation to make life, life difficult
[00:15:32.800 --> 00:15:34.000]   for future entrants.
[00:15:34.200 --> 00:15:35.040]   That's regulatory.
[00:15:35.040 --> 00:15:40.920]   Either way, those involved with today's statements are taking a big risk because
[00:15:40.920 --> 00:15:43.080]   remember, there's this thing called Congress.
[00:15:43.080 --> 00:15:46.080]   And but Congress can't do anything.
[00:15:46.080 --> 00:15:48.120]   A, they're going to be part of this.
[00:15:48.120 --> 00:15:52.760]   Cause I want to learn from Mark Zuckerberg and has the charm tool, charm tour and
[00:15:52.760 --> 00:15:54.760]   see they're not going for revenue.
[00:15:54.760 --> 00:15:55.760]   They're going for investment.
[00:15:55.760 --> 00:16:00.480]   Well, Pierce says the only logical response that Congress might come up with is to
[00:16:00.480 --> 00:16:06.120]   ban AI or regulate it so tightly that even firms already in the business
[00:16:06.120 --> 00:16:07.480]   suffer that you said,
[00:16:07.480 --> 00:16:09.760]   Oh, I said license, fine licenses and give me a license.
[00:16:09.760 --> 00:16:10.760]   Yeah, give me the license.
[00:16:10.760 --> 00:16:13.760]   That's surely not what these scientists are looking for, but what do they want?
[00:16:13.760 --> 00:16:17.360]   Per this is a good line from Martin Pierce.
[00:16:17.360 --> 00:16:21.240]   Perhaps instead of issuing a single sentence statement meant to freak everyone out.
[00:16:21.240 --> 00:16:25.360]   AI scientists should use their considerable skills to figure out a solution to the
[00:16:25.360 --> 00:16:26.880]   problem they have.
[00:16:26.880 --> 00:16:29.560]   But I don't think we even know what the problem is.
[00:16:29.560 --> 00:16:30.400]   No, exactly.
[00:16:30.400 --> 00:16:33.600]   Is it?
[00:16:33.600 --> 00:16:34.280]   Oh, but okay.
[00:16:34.280 --> 00:16:35.080]   Let's be on.
[00:16:35.080 --> 00:16:35.960]   Let's be fair.
[00:16:35.960 --> 00:16:36.840]   It's not a.
[00:16:36.840 --> 00:16:37.920]   Is there any.
[00:16:37.920 --> 00:16:40.800]   There are some several concerns, not necessarily.
[00:16:40.800 --> 00:16:41.200]   Yeah.
[00:16:41.200 --> 00:16:41.880]   But concerns.
[00:16:41.880 --> 00:16:45.800]   Is there any reason to say that the, well, maybe we should be looking more closely at
[00:16:45.800 --> 00:16:47.800]   this, the threat of AI.
[00:16:47.800 --> 00:16:49.360]   I mean, we, we talked about disinformation.
[00:16:49.360 --> 00:16:50.000]   That's real.
[00:16:50.000 --> 00:16:52.800]   We know it causes disinformation.
[00:16:52.800 --> 00:16:57.360]   We know that people because we're all lazy tend to rely on things that make our
[00:16:57.360 --> 00:16:59.120]   lives easier without checking it.
[00:16:59.120 --> 00:17:02.960]   So we know there's a danger with hallucinations or whatnot.
[00:17:02.960 --> 00:17:08.280]   So there's over reliance on AI over trust in AI, which is ironic given the disinformation.
[00:17:08.280 --> 00:17:11.440]   We know that it can be used to scale propaganda.
[00:17:11.440 --> 00:17:20.000]   We know that it can be used to that we know that people trust what they see and trust.
[00:17:20.000 --> 00:17:23.480]   Visual mediums pretty highly.
[00:17:23.760 --> 00:17:28.440]   So that's a weakness we have that can be taken by that.
[00:17:28.440 --> 00:17:30.640]   Yeah, we have to teach people not to trust photos.
[00:17:30.640 --> 00:17:33.080]   I don't think that's possible.
[00:17:33.080 --> 00:17:33.680]   Not to try.
[00:17:33.680 --> 00:17:37.200]   I just saw a deep fake thing where you could take a single picture, a still picture of
[00:17:37.200 --> 00:17:42.320]   anyone's head, put it on top of somebody else's moving body and it makes a video.
[00:17:42.320 --> 00:17:43.280]   That's in this.
[00:17:43.280 --> 00:17:44.640]   It's very impressive.
[00:17:44.640 --> 00:17:49.480]   We, and even if this is just a parlor trick, this will be something that will happen.
[00:17:49.480 --> 00:17:52.000]   Dare I return to resolve that dare.
[00:17:52.000 --> 00:17:54.040]   I say this is exactly what happened with print.
[00:17:54.040 --> 00:17:57.880]   Nobody trusted what came up with the printing press.
[00:17:57.880 --> 00:18:04.360]   And then we had to invent institutions to imbue trust and faith like publishers and
[00:18:04.360 --> 00:18:06.760]   editors and libraries and so on.
[00:18:06.760 --> 00:18:08.760]   We don't have those institutions now.
[00:18:08.760 --> 00:18:11.560]   We're going to have to reinvent them because those old institutions ain't going to work.
[00:18:11.560 --> 00:18:16.920]   But yeah, I think people will become grossly mistrustful of much of what they see.
[00:18:16.920 --> 00:18:17.520]   And that's fine.
[00:18:17.520 --> 00:18:19.320]   Look at the surveys you see about social media.
[00:18:19.320 --> 00:18:21.440]   People say, Oh, I mistrust most of what I see on social media.
[00:18:21.640 --> 00:18:24.600]   God, they still are easily influenced by it.
[00:18:24.600 --> 00:18:25.720]   That's what's hard.
[00:18:25.720 --> 00:18:31.760]   I mean, the other thing is to on social media that people are, they trust just as much.
[00:18:31.760 --> 00:18:32.920]   What's what's put out there?
[00:18:32.920 --> 00:18:38.400]   Because they saw it on Facebook or saw it on Twitter from a sort of trusted source.
[00:18:38.400 --> 00:18:43.960]   Seeing both of you and you're going to see people lose their jobs because of this.
[00:18:43.960 --> 00:18:48.160]   And then that's going to create a layer of societal unrest that we already have
[00:18:48.160 --> 00:18:50.000]   based on like income inequality.
[00:18:50.000 --> 00:18:55.200]   So that is another area where that's a second layer effect, but it's still a very real one.
[00:18:55.200 --> 00:19:00.720]   If we did this exercise, there's still cab drivers that still hasn't happened yet.
[00:19:00.720 --> 00:19:02.120]   But we've gone through this before.
[00:19:02.120 --> 00:19:03.120]   Yes, we went through.
[00:19:03.120 --> 00:19:06.520]   Looms and threshers.
[00:19:06.520 --> 00:19:10.480]   I don't think it's going to be quite to that level.
[00:19:10.480 --> 00:19:13.760]   I think what I've already talked to companies that are hiring like one or two
[00:19:13.760 --> 00:19:17.640]   fewer software developers because there's greater efficiencies in.
[00:19:18.560 --> 00:19:20.320]   Thanks to AI.
[00:19:20.320 --> 00:19:21.440]   And they make a difference.
[00:19:21.440 --> 00:19:23.720]   It's a good business decision, right?
[00:19:23.720 --> 00:19:24.440]   Yeah.
[00:19:24.440 --> 00:19:25.440]   I'm not.
[00:19:25.440 --> 00:19:28.440]   The point is there's going to be trickles.
[00:19:28.440 --> 00:19:34.200]   These are small effects individually, but in aggregate, you've got to think, OK,
[00:19:34.200 --> 00:19:39.360]   if you can cut down by the workforce in certain industries by 20 or 30 percent,
[00:19:39.360 --> 00:19:41.320]   because it makes sense.
[00:19:41.320 --> 00:19:43.800]   That's a big change.
[00:19:43.800 --> 00:19:48.080]   We make as well happens in this case, in Stacey, we end up creating new jobs, too.
[00:19:48.080 --> 00:19:50.880]   At the same time, we very well could.
[00:19:50.880 --> 00:19:53.040]   We very well could.
[00:19:53.040 --> 00:19:57.400]   And I hope we do, but there's also a lack of people that are be trained for those
[00:19:57.400 --> 00:20:00.560]   new jobs and what about people that are 55?
[00:20:00.560 --> 00:20:05.200]   Isn't it the case, though, that we could go through this very similar exercise with
[00:20:05.200 --> 00:20:08.480]   any kind of massive world changing technology?
[00:20:08.480 --> 00:20:11.520]   If you said the smartphone instead of AI,
[00:20:11.520 --> 00:20:14.960]   people would lose jobs.
[00:20:14.960 --> 00:20:18.080]   You know, I've lost all their value.
[00:20:18.080 --> 00:20:19.800]   Operators are gone.
[00:20:19.800 --> 00:20:23.320]   I think new textbooks are really the big advertising is down.
[00:20:23.320 --> 00:20:26.720]   The big world changing technologies have these kinds of massive
[00:20:26.720 --> 00:20:28.600]   impacts.
[00:20:28.600 --> 00:20:30.960]   Should we, but we don't want to.
[00:20:30.960 --> 00:20:32.280]   We don't want Stasis.
[00:20:32.280 --> 00:20:34.760]   We don't want the world to stop evolving.
[00:20:34.760 --> 00:20:36.120]   But we do need to think of.
[00:20:36.120 --> 00:20:44.440]   I mean, think about how right now the economy feels precarious for so many people.
[00:20:44.800 --> 00:20:50.800]   And one of those is a that is a function of technology and runaway capitalism.
[00:20:50.800 --> 00:20:55.040]   But if you think about like the gay economy and we've talked about this, that creates
[00:20:55.040 --> 00:20:57.520]   a lot less economic certainty for people.
[00:20:57.520 --> 00:21:00.880]   We see people already struggling for things like buying a house.
[00:21:00.880 --> 00:21:05.480]   You could argue that that's a result of technology in the sense that Airbnb has
[00:21:05.480 --> 00:21:08.760]   bought up, you know, real estate and cities and cause.
[00:21:08.760 --> 00:21:13.560]   I mean, like, right, these are very real effects that are actually like we could be like,
[00:21:13.600 --> 00:21:15.640]   Oh, I'm not saying they're not, but I'm just saying that.
[00:21:15.640 --> 00:21:17.960]   Oh, but how much do you do?
[00:21:17.960 --> 00:21:21.440]   Are you how about how wise are we to be able to understand those effects fully in control?
[00:21:21.440 --> 00:21:22.480]   Jeff, have you ever seen?
[00:21:22.480 --> 00:21:23.320]   We can't.
[00:21:23.320 --> 00:21:26.000]   Have you ever seen Jeff the pessimists archive?
[00:21:26.000 --> 00:21:27.120]   Oh, I love that.
[00:21:27.120 --> 00:21:28.560]   I bet him in my office two weeks ago.
[00:21:28.560 --> 00:21:29.080]   He's great.
[00:21:29.080 --> 00:21:33.120]   So this is somebody brought this up on Twitter on Sunday and I thought, Oh, Jeff Jarvis would
[00:21:33.120 --> 00:21:33.560]   love this.
[00:21:33.560 --> 00:21:37.320]   So at the bottom is a timeline of technological
[00:21:37.760 --> 00:21:45.120]   innovations like the telegraph, the telephone, the camera and then a collection of clippings
[00:21:45.120 --> 00:21:52.600]   from that era talking about the hazards of, you know, these new technologies.
[00:21:52.600 --> 00:21:54.800]   And it's very, it's a very similar conversation.
[00:21:54.800 --> 00:21:58.760]   Yeah, the bicycle, you know, that's the way that's, that's the great one.
[00:21:58.760 --> 00:22:03.520]   Cause Tristan Harris and his anti social media film said, who've nobody freaked out
[00:22:03.520 --> 00:22:04.240]   about the bicycle?
[00:22:04.240 --> 00:22:07.160]   Well, indeed, there was huge on bicycle fright.
[00:22:08.000 --> 00:22:13.280]   What the medical record says of this common phenomenon, it may attack any writer.
[00:22:13.280 --> 00:22:17.000]   The remedy is suggested is to avoid looking at the dreaded object.
[00:22:17.000 --> 00:22:20.960]   The writer should never watch the wheel or the pedals.
[00:22:20.960 --> 00:22:26.640]   So moral panic is nothing new.
[00:22:26.640 --> 00:22:28.360]   I guess you've known that all along, Jeff.
[00:22:28.360 --> 00:22:31.160]   Elevators chapter four of the next book.
[00:22:31.160 --> 00:22:31.560]   Yeah.
[00:22:31.640 --> 00:22:40.000]   Elevators, all of the things that the elevator sickness cases of elevator sickness are on
[00:22:40.000 --> 00:22:40.760]   the rise.
[00:22:40.760 --> 00:22:42.480]   Said Dr. E. C.
[00:22:42.480 --> 00:22:44.400]   Nolten of Chicago.
[00:22:44.400 --> 00:22:48.640]   When physicians first began to claim there was such a thing as elevator sickness.
[00:22:48.640 --> 00:22:53.640]   Their statements were usually discredited, but it is now becoming well defined.
[00:22:53.640 --> 00:22:58.520]   Its effects are found in an increased number of cases of brain fever and
[00:22:58.520 --> 00:23:01.000]   disordered nervous systems.
[00:23:01.000 --> 00:23:05.360]   Everyone has felt a sense of emptiness in their heads of sensation as if they were
[00:23:05.360 --> 00:23:09.800]   falling when riding rapidly in an elevator, especially if it's going down.
[00:23:09.800 --> 00:23:12.880]   This is from the Cincinnati.
[00:23:12.880 --> 00:23:16.760]   This, these articles are from the turn of the century.
[00:23:16.760 --> 00:23:18.160]   This is 1894.
[00:23:18.160 --> 00:23:19.240]   Okay.
[00:23:19.240 --> 00:23:23.760]   So before like airplanes are riding real fast and cars downhill, there's an
[00:23:23.760 --> 00:23:28.320]   amazing book, the four reasons to turn off TV.
[00:23:28.320 --> 00:23:30.760]   This was this is first of all, just to get rid of TV.
[00:23:30.760 --> 00:23:31.360]   Do I have it here?
[00:23:31.360 --> 00:23:32.440]   Somewhere around the hair do.
[00:23:32.440 --> 00:23:36.480]   And one of the reasons was that the artificial light was going to be terrible.
[00:23:36.480 --> 00:23:37.080]   I was whole book.
[00:23:37.080 --> 00:23:37.280]   Oh, yeah.
[00:23:37.280 --> 00:23:38.560]   My mom used to say that.
[00:23:38.560 --> 00:23:38.960]   Yeah.
[00:23:38.960 --> 00:23:42.240]   Turn on a light in the room because you're going to go blind.
[00:23:42.240 --> 00:23:47.040]   Yeah, I used to hear that, which was a PR thing from the light electrical industry.
[00:23:47.040 --> 00:23:47.400]   Yeah.
[00:23:47.400 --> 00:23:51.680]   Children's addiction to TV presents serious problems.
[00:23:51.680 --> 00:23:54.160]   This is from the Valley Times, 1957.
[00:23:54.160 --> 00:23:59.280]   First of four articles gives the remarks of psycho analyst, Dr.
[00:23:59.280 --> 00:24:04.440]   Isidore and his Zifferstein made to a Valley audience recently reported in the Valley Times.
[00:24:04.440 --> 00:24:06.520]   But I mean, I can go on and on.
[00:24:06.520 --> 00:24:10.600]   This is this is very mander who was the author of four segments for the elimination of television.
[00:24:10.600 --> 00:24:10.880]   Yeah.
[00:24:10.880 --> 00:24:14.000]   Television literally enters inside human beings.
[00:24:14.000 --> 00:24:16.200]   Listen, the language is so much like social media, right?
[00:24:16.200 --> 00:24:20.360]   Inside our homes, our minds, our bodies, making possible the reordering of human
[00:24:20.360 --> 00:24:22.280]   processes from inside.
[00:24:22.280 --> 00:24:23.840]   How about jazz?
[00:24:23.840 --> 00:24:25.640]   Here's from the twenties.
[00:24:25.640 --> 00:24:29.880]   Jazz blamed for the delinquency of girls today.
[00:24:29.880 --> 00:24:37.240]   So OK, so we can stipulate that there is often a certain amount of moral panic
[00:24:37.240 --> 00:24:39.880]   that goes along with the technology.
[00:24:39.880 --> 00:24:44.160]   But isn't isn't there some a kernel of truth in this or no?
[00:24:44.160 --> 00:24:45.480]   Concerned.
[00:24:45.480 --> 00:24:46.080]   Stacy's right.
[00:24:46.080 --> 00:24:49.440]   There's concerns and we've got to deal with those, but the present tense concerns.
[00:24:49.880 --> 00:24:55.200]   That's what that's what Emily Bender argues by talking about this, this destroying
[00:24:55.200 --> 00:24:57.080]   mankind dinosaur moment stuff.
[00:24:57.080 --> 00:25:04.120]   They're distracting from current issues around privacy, around disinformation, around
[00:25:04.120 --> 00:25:05.960]   veracity and truth.
[00:25:05.960 --> 00:25:12.240]   That's the kind of point that you would probably say is it should be a primary concern.
[00:25:12.240 --> 00:25:18.960]   Well, this statement and previous statements from AI scientists has garnered a lot of news,
[00:25:18.960 --> 00:25:20.160]   print and so forth.
[00:25:20.160 --> 00:25:24.120]   We are facing down the barrel of a climate apocalypse.
[00:25:24.120 --> 00:25:24.520]   Yes.
[00:25:24.520 --> 00:25:27.880]   That we're doing nothing about that's going to happen in 50 or 60 years.
[00:25:27.880 --> 00:25:28.400]   Yep.
[00:25:28.400 --> 00:25:32.840]   And I mean, I guess we just are in denial about it.
[00:25:32.840 --> 00:25:35.280]   I mean, that's a mass extinction event.
[00:25:35.280 --> 00:25:39.840]   I don't understand why we're so worried about this purely hypothetical.
[00:25:39.840 --> 00:25:44.840]   It's like Eliza is going to kill you when we are literally looking at something
[00:25:44.840 --> 00:25:47.960]   horrible that no one wants to talk about.
[00:25:47.960 --> 00:25:49.520]   Nuclear weapons take that.
[00:25:49.520 --> 00:25:54.120]   It's because we can't solve any of these, but this is one you can read your.
[00:25:54.120 --> 00:25:55.800]   It's it's a wrap your head around.
[00:25:55.800 --> 00:25:59.160]   It's it's well, I was going to say you can ring your hands over.
[00:25:59.160 --> 00:26:04.000]   Climate change is not something we can do anything about.
[00:26:04.000 --> 00:26:04.520]   It is.
[00:26:04.520 --> 00:26:08.040]   It's not quite too late, but it is going to be if we don't.
[00:26:08.040 --> 00:26:13.480]   As we can take steps for the future as an individual, there is very little.
[00:26:13.480 --> 00:26:15.280]   You as an individual can do.
[00:26:15.280 --> 00:26:17.000]   You can call your country.
[00:26:17.000 --> 00:26:17.840]   You can call it.
[00:26:17.840 --> 00:26:18.200]   Right.
[00:26:18.200 --> 00:26:24.280]   Well, I know, but in this is where, I mean, so sure, there is an element of, oh, look at the AI.
[00:26:24.280 --> 00:26:26.120]   While we're going to be burning up, but.
[00:26:26.120 --> 00:26:33.280]   There's a lot of linear television right now because it's not football season, but.
[00:26:33.280 --> 00:26:34.960]   We have all of these.
[00:26:34.960 --> 00:26:37.920]   Sorry, we don't have all of we have all of this.
[00:26:37.920 --> 00:26:42.480]   This mainstream news talking about how horrible, quote unquote, AI is.
[00:26:43.320 --> 00:26:47.880]   Or why isn't there anything out there on in linear television and mainstream that says, hey,
[00:26:47.880 --> 00:26:49.680]   let's have some common sense about this.
[00:26:49.680 --> 00:26:55.040]   It's it's always on podcasts, like ours where I hear the common sense mentioned about AI.
[00:26:55.040 --> 00:26:58.520]   Why isn't there a push for the regular.
[00:26:58.520 --> 00:26:59.840]   Sense on podcasts.
[00:26:59.840 --> 00:27:05.440]   Why isn't there a push on, you know, on NBC, ABC, whatever?
[00:27:05.440 --> 00:27:06.400]   Because it does.
[00:27:06.400 --> 00:27:09.840]   Because everything we accuse social media of doing it, a clickbait,
[00:27:10.360 --> 00:27:14.320]   a needing attention, making money from, from engaging people and exciting them.
[00:27:14.320 --> 00:27:15.880]   That was invented by media.
[00:27:15.880 --> 00:27:16.720]   That's what they do.
[00:27:16.720 --> 00:27:21.400]   I'm ashamed of my entire life and career because that's what I've been part of.
[00:27:21.400 --> 00:27:23.960]   Also, there's not a lot of great imagery.
[00:27:23.960 --> 00:27:27.520]   I mean, I know we could create some awesome imagery, but like the.
[00:27:27.520 --> 00:27:33.760]   The narrative in television, because you've got such a short time frame, it has to be a really.
[00:27:33.760 --> 00:27:38.880]   Well, it's got to be a big, easy to understand narrative and it's got to have good visuals with it.
[00:27:39.200 --> 00:27:42.560]   And that's why linear TV is probably failing us on the AI front.
[00:27:42.560 --> 00:27:47.520]   The reasonableness doesn't doesn't cut it on TV.
[00:27:47.520 --> 00:27:50.560]   But but again, because we've been.
[00:27:50.560 --> 00:27:53.200]   We've been a lot of advantages of AI.
[00:27:53.200 --> 00:27:56.320]   As well as all of the misinformation stuff out there.
[00:27:56.320 --> 00:27:58.880]   We do mention a lot of different advantages.
[00:27:58.880 --> 00:28:04.000]   And I also I'm also one of the people saying, calm down on that, by the way.
[00:28:04.000 --> 00:28:04.520]   Yeah.
[00:28:04.520 --> 00:28:04.920]   Yeah.
[00:28:04.920 --> 00:28:08.080]   I think you said that last week on the show with with with Chris.
[00:28:08.200 --> 00:28:10.320]   You see them, and I think I was thinking about more Leo.
[00:28:10.320 --> 00:28:12.600]   I think you're right that it's overdone.
[00:28:12.600 --> 00:28:17.440]   That one of my favorite story that we can I set you guys stuff over the weekend was the lawyer who used chat.
[00:28:17.440 --> 00:28:18.000]   She beat you.
[00:28:18.000 --> 00:28:18.920]   Oh, yeah.
[00:28:18.920 --> 00:28:19.440]   Oh, yeah.
[00:28:19.440 --> 00:28:20.440]   That was a mess.
[00:28:20.440 --> 00:28:25.040]   I'm waiting for every every journalist and everybody else to do the same thing.
[00:28:25.040 --> 00:28:27.200]   It's it doesn't it can't do fact.
[00:28:27.200 --> 00:28:28.240]   It's really bad.
[00:28:28.240 --> 00:28:29.160]   It's there.
[00:28:29.160 --> 00:28:31.400]   Oh, they're so overplaying its capability.
[00:28:31.400 --> 00:28:32.760]   Yeah, it sounds like us.
[00:28:32.760 --> 00:28:34.680]   And that's pretty pretty darn amazing.
[00:28:34.680 --> 00:28:36.400]   And there's neat stuff we could do with it.
[00:28:36.400 --> 00:28:36.720]   Yeah.
[00:28:36.720 --> 00:28:38.760]   But it's really limited what it could do.
[00:28:38.760 --> 00:28:40.920]   I think I think your point last week was exactly right, Leo.
[00:28:40.920 --> 00:28:42.080]   Here's a
[00:28:42.080 --> 00:28:47.400]   box for everything to back it up by a guy named Rodney Brooks, who's a legendary roboticist.
[00:28:47.400 --> 00:28:55.600]   And once one time head of the AI department and MIT, his actual title, he was the director
[00:28:55.600 --> 00:29:00.440]   of the computer science and artificial intelligence laboratory at MIT until 2007.
[00:29:00.440 --> 00:29:02.440]   He knows what he's talking about his head.
[00:29:02.440 --> 00:29:04.560]   This is in the IEEE Spectrum Journal.
[00:29:04.880 --> 00:29:13.200]   Just calm down about chat GPT for already and stop confusing performance with competence.
[00:29:13.200 --> 00:29:16.000]   He's saying exactly this.
[00:29:16.000 --> 00:29:17.920]   He actually wrote a great article.
[00:29:17.920 --> 00:29:19.640]   And now it's amazing.
[00:29:19.640 --> 00:29:24.440]   Six years ago, the seven deadly sins of predicting the future of AI.
[00:29:24.440 --> 00:29:29.480]   And we are giving into every one of those sins.
[00:29:29.480 --> 00:29:30.600]   But you know what?
[00:29:30.600 --> 00:29:32.040]   That's what humans do.
[00:29:32.040 --> 00:29:35.920]   We always performance always wins with us.
[00:29:35.920 --> 00:29:36.200]   Yeah.
[00:29:36.200 --> 00:29:37.000]   For competence.
[00:29:37.000 --> 00:29:37.640]   Think about it.
[00:29:37.640 --> 00:29:40.640]   We're that's just what we do.
[00:29:40.640 --> 00:29:42.120]   Well, and I've said this before.
[00:29:42.120 --> 00:29:48.440]   I've been saying this for years that people often think I've been in fact, I've been saying this for more
[00:29:48.440 --> 00:29:49.720]   than 25 years.
[00:29:49.720 --> 00:29:52.720]   People often think of a computer as a thinking machine.
[00:29:52.720 --> 00:29:54.800]   There's even a computer company called Thinking Machines.
[00:29:54.800 --> 00:29:57.400]   And and it's really important to understand.
[00:29:57.400 --> 00:29:58.400]   They don't.
[00:29:58.400 --> 00:29:59.720]   That's an anthropomorphism.
[00:29:59.720 --> 00:30:04.200]   Computers don't think they basically are very simple calculating machines.
[00:30:04.200 --> 00:30:10.120]   They just do it so fast that it looks like we think we project on to them.
[00:30:10.120 --> 00:30:11.760]   And they're actually thinking, but they're not.
[00:30:11.760 --> 00:30:16.280]   They're just performing calculations incredibly rapidly, even including AI.
[00:30:16.280 --> 00:30:17.280]   It's not thinking.
[00:30:17.280 --> 00:30:18.960]   It isn't any right.
[00:30:18.960 --> 00:30:26.240]   But those those calculations when performed super, I mean, you could argue that our brain is not thinking.
[00:30:26.240 --> 00:30:30.320]   It's just performing analysis of inputs really fast.
[00:30:30.320 --> 00:30:35.280]   OK, I mean, we're you're like the way our brain has developed.
[00:30:35.280 --> 00:30:37.640]   And I'm not I'm just thinking.
[00:30:37.640 --> 00:30:38.280]   No, I agree.
[00:30:38.280 --> 00:30:39.280]   Sure.
[00:30:39.280 --> 00:30:43.840]   This is an unsolved computer science question, which is what's the difference
[00:30:43.840 --> 00:30:49.720]   between a rapid calculator and the human brain is the human brain just so sophisticatedly calculating
[00:30:49.720 --> 00:30:52.400]   that we don't see exactly how, but we could eventually.
[00:30:52.400 --> 00:30:56.080]   Well, and we think I mean, think about like how dogs, like they're
[00:30:56.080 --> 00:31:00.360]   finding dogs that can detect like various diseases, really early consent.
[00:31:00.360 --> 00:31:01.000]   Yeah.
[00:31:01.000 --> 00:31:07.240]   So the same thing, I mean, we don't give dogs the same faunted, you know, actually, we might.
[00:31:07.240 --> 00:31:08.160]   I don't know.
[00:31:08.160 --> 00:31:09.480]   Oh, yes.
[00:31:09.480 --> 00:31:10.480]   I was a little as they are.
[00:31:10.480 --> 00:31:16.160]   Unfortunately, the conversation always like often goes down to, well, there's a human has a soul
[00:31:16.160 --> 00:31:17.320]   and the machine doesn't.
[00:31:17.320 --> 00:31:22.000]   And I think that that people reject that that like, well, you can't, that's not.
[00:31:22.000 --> 00:31:23.920]   That's that's magical thinking.
[00:31:23.920 --> 00:31:28.040]   But I do think there may be something that we do in our brains that is very, very hard
[00:31:28.040 --> 00:31:30.480]   from a von Neumann machine to do.
[00:31:30.480 --> 00:31:31.000]   Yeah.
[00:31:31.000 --> 00:31:37.080]   Well, OK, for a von Neumann machine, but I also would say I would argue that we're no more special
[00:31:37.080 --> 00:31:39.200]   than any of these other things.
[00:31:39.200 --> 00:31:42.760]   The only reason we think we're so special is we're judging us, right?
[00:31:42.760 --> 00:31:43.640]   If you think about.
[00:31:43.640 --> 00:31:51.760]   I like if you think about animals, octopuses, the way they think their whole separate, I mean,
[00:31:52.360 --> 00:31:55.760]   way their brain has developed to process their environments.
[00:31:55.760 --> 00:31:58.680]   I mean, what are we doing?
[00:31:58.680 --> 00:32:02.760]   We're actually really crappy at processing our environment because otherwise we would have
[00:32:02.760 --> 00:32:05.960]   done something about the damage that we're causing in some ways.
[00:32:05.960 --> 00:32:11.400]   I'm like, we've designed, we've been designed in such a way to respond to certain environmental
[00:32:11.400 --> 00:32:18.960]   stimulus like a tiger in the brush and not to other environmental stimulus.
[00:32:18.960 --> 00:32:20.920]   And so we don't long term thinking and stuff.
[00:32:21.280 --> 00:32:25.160]   There are there are there's this is the realm of philosophy as much as it is.
[00:32:25.160 --> 00:32:26.160]   It's science.
[00:32:26.160 --> 00:32:28.880]   But I mean, there are some say, we need more discussion of this.
[00:32:28.880 --> 00:32:29.320]   Yeah.
[00:32:29.320 --> 00:32:33.800]   What humans what distinguishes humans from animals is that we know we're going to die.
[00:32:33.800 --> 00:32:37.160]   An animal doesn't know what's going to die.
[00:32:37.160 --> 00:32:38.120]   We know we are.
[00:32:38.120 --> 00:32:42.560]   We don't know that dogs go off by themselves and so do wolves.
[00:32:42.560 --> 00:32:46.680]   Well, they know when they're sick and they don't feel well, but humans go through life
[00:32:46.680 --> 00:32:49.040]   with the knowledge that they will eventually die.
[00:32:49.040 --> 00:32:50.200]   Animals don't have that.
[00:32:50.800 --> 00:32:51.800]   All right.
[00:32:51.800 --> 00:32:53.160]   We don't know that.
[00:32:53.160 --> 00:32:54.160]   They watch the world.
[00:32:54.160 --> 00:32:55.840]   To the savannah and take a look.
[00:32:55.840 --> 00:32:59.400]   See, those elephant funerals they have.
[00:32:59.400 --> 00:33:01.240]   I mean, yeah, what's their second time?
[00:33:01.240 --> 00:33:04.240]   They know they're dead, but do they know that they're going to die?
[00:33:04.240 --> 00:33:05.760]   The loss of their kin.
[00:33:05.760 --> 00:33:07.200]   Yeah, yeah, because they're dead.
[00:33:07.200 --> 00:33:09.440]   But do they know that they're going to die?
[00:33:09.440 --> 00:33:13.800]   You don't think that if they're if they can mourn, they don't recognize that that is.
[00:33:13.800 --> 00:33:14.840]   I could be next.
[00:33:14.840 --> 00:33:15.400]   Oh, maybe.
[00:33:15.400 --> 00:33:15.760]   Yeah.
[00:33:15.760 --> 00:33:16.080]   All right.
[00:33:16.080 --> 00:33:17.800]   So we can't we can't know that I guess.
[00:33:18.680 --> 00:33:19.680]   And there are others.
[00:33:19.680 --> 00:33:21.480]   No, I'm just saying what people have said.
[00:33:21.480 --> 00:33:22.360]   Let's put it that way.
[00:33:22.360 --> 00:33:23.040]   No, I don't.
[00:33:23.040 --> 00:33:23.400]   Yeah.
[00:33:23.400 --> 00:33:27.040]   I there's others who have the opinion that it was.
[00:33:27.040 --> 00:33:31.200]   I mean, by the way, that's what some have given us the explanation of the Cambrian
[00:33:31.200 --> 00:33:35.080]   explosion was there was sudden realization that we could in fact die.
[00:33:35.080 --> 00:33:38.720]   I think Leonard Schlane said that in one of his books.
[00:33:38.720 --> 00:33:44.040]   There are those who say are not our sense of time.
[00:33:45.000 --> 00:33:51.560]   You know, we don't I mean, many scientists and maybe some philosophers who think who
[00:33:51.560 --> 00:33:53.920]   understand that there is no such thing as time.
[00:33:53.920 --> 00:33:57.640]   Time is what we think happens.
[00:33:57.640 --> 00:34:00.760]   But there, but it isn't that is our impressions.
[00:34:00.760 --> 00:34:06.120]   There was a really good interview that your your buddy Lex Friedman did with
[00:34:09.480 --> 00:34:17.640]   we will Stephen Wolfram in which Wolfram pointed out that really what we think of
[00:34:17.640 --> 00:34:21.920]   his time is just our averaging of all the all the events that happen all at the same
[00:34:21.920 --> 00:34:22.440]   time.
[00:34:22.440 --> 00:34:25.560]   We overlay this sequentiality on it.
[00:34:25.560 --> 00:34:26.680]   It's very interesting.
[00:34:26.680 --> 00:34:31.200]   It's a physicist's point of view, not necessarily a philosophers point of view,
[00:34:31.200 --> 00:34:38.600]   although the point is I do think there's something different from what a computer
[00:34:38.600 --> 00:34:39.320]   does.
[00:34:39.320 --> 00:34:46.440]   Now, I interviewed the guy who invented graffiti, a Jeff Hawkins, who's also a neuroscientist
[00:34:46.440 --> 00:34:48.720]   in his his premise.
[00:34:48.720 --> 00:34:54.720]   He wrote a number of really interesting books on brain neuroscience is that the thing
[00:34:54.720 --> 00:34:58.560]   that distinguishes humans are our rate of calculations much slower than a computer,
[00:34:58.560 --> 00:35:00.280]   but we're massively parallel.
[00:35:00.280 --> 00:35:03.800]   And he said the mistake we're making is trying to take a von Neumann machine,
[00:35:03.800 --> 00:35:05.120]   which is much more sequential.
[00:35:05.480 --> 00:35:09.360]   And and what we really need to do is build massively parallel machines.
[00:35:09.360 --> 00:35:12.840]   So he started a company, Newmenta, you've heard of this, Stacy.
[00:35:12.840 --> 00:35:16.840]   I know because we talked about it that was designed to create memory chips that
[00:35:16.840 --> 00:35:19.360]   worked more in parallel like our brain does.
[00:35:19.360 --> 00:35:27.080]   Not achieving success yet, but in any event, the point being, I think, and maybe
[00:35:27.080 --> 00:35:31.640]   you're right, maybe it's just a kind of a subjective and self-centered point of view.
[00:35:31.840 --> 00:35:37.600]   There is something different about the way humans think that and machines think.
[00:35:37.600 --> 00:35:42.880]   And I don't think making a machine think faster or with more data is going to ever
[00:35:42.880 --> 00:35:46.440]   achieve whatever it is that we've got going on consciousness.
[00:35:46.440 --> 00:35:48.720]   That seems to be the case.
[00:35:48.720 --> 00:35:54.480]   You could maybe argue, maybe what we're doing.
[00:35:54.480 --> 00:35:59.200]   So computers deal with exact information unless we're talking about probabilistic
[00:35:59.200 --> 00:36:02.240]   computing, but let's pretend we're not because that is a whole different realm
[00:36:02.240 --> 00:36:04.160]   of both chip design and programming.
[00:36:04.160 --> 00:36:09.560]   But in general, although AI is probabilities, we're teaching them to use
[00:36:09.560 --> 00:36:11.280]   exact calculations to.
[00:36:11.280 --> 00:36:14.480]   Well, and quantum is in fact also not.
[00:36:14.480 --> 00:36:16.040]   OK, quantum computing isn't really yet.
[00:36:16.040 --> 00:36:17.200]   We're not going to go there yet.
[00:36:17.200 --> 00:36:18.960]   I'm just talking about what's available to us.
[00:36:18.960 --> 00:36:20.920]   It's there.
[00:36:20.920 --> 00:36:21.920]   It's just not a scale.
[00:36:21.920 --> 00:36:23.200]   OK, yes.
[00:36:23.200 --> 00:36:26.160]   I'm like, no, don't bring that in yet.
[00:36:26.160 --> 00:36:27.760]   OK.
[00:36:28.080 --> 00:36:33.440]   So what we're good at is seeing things in making an intuitive leap and not being
[00:36:33.440 --> 00:36:35.200]   able to account for our work.
[00:36:35.200 --> 00:36:40.080]   What we're getting with with AI and what makes us uncomfortable is we're
[00:36:40.080 --> 00:36:45.640]   actually allowing the AIs to make those what we think of as intuitive leaps.
[00:36:45.640 --> 00:36:50.680]   They're making based on calculations on much of the data and making a probabilistic
[00:36:50.680 --> 00:36:51.160]   assumption.
[00:36:51.160 --> 00:36:55.040]   So in some ways, they may be computing like us.
[00:36:55.040 --> 00:36:55.400]   I don't.
[00:36:55.400 --> 00:36:57.520]   I'm kind of thinking out loud on this.
[00:36:58.000 --> 00:37:04.320]   So, but we trust human intuition a lot sometimes.
[00:37:04.320 --> 00:37:09.320]   But then we also want like we value the data driven aspects of a computer.
[00:37:09.320 --> 00:37:13.600]   So we might be having our issue is we've always thought of computers as like
[00:37:13.600 --> 00:37:16.840]   neutral sources of data, but now that they're making these probabilistic
[00:37:16.840 --> 00:37:22.880]   assumptions that mimic intuition, we're like, oh, this is scary.
[00:37:22.880 --> 00:37:24.400]   This is this could be a problem.
[00:37:24.400 --> 00:37:25.880]   And I don't know if that's really true.
[00:37:26.280 --> 00:37:28.880]   I would submit that we can just trust them as intuitive things.
[00:37:28.880 --> 00:37:33.160]   What we're seeing and so far, and I think it's going to prove to be the case is
[00:37:33.160 --> 00:37:38.240]   that by itself, whether it's a self driving vehicle or chat, GPT writing,
[00:37:38.240 --> 00:37:40.040]   a novel, they're not very good.
[00:37:40.040 --> 00:37:44.320]   In conjunction with human thinking, they're a very good partner.
[00:37:44.320 --> 00:37:46.040]   Oh, right.
[00:37:46.040 --> 00:37:47.680]   I love the premise of that.
[00:37:47.680 --> 00:37:48.080]   Yeah.
[00:37:48.080 --> 00:37:52.320]   And I think it's a better way to think of these as a way of augmenting human
[00:37:52.320 --> 00:37:57.560]   abilities, but leaving the human out of it almost always is disaster.
[00:37:57.560 --> 00:37:59.800]   I mean, you left, let a Tesla drive itself.
[00:37:59.800 --> 00:38:02.560]   We can't be out of it because our data is what feeds it.
[00:38:02.560 --> 00:38:05.360]   Well, but that's not sufficiently in it.
[00:38:05.360 --> 00:38:09.640]   I think, well, but I agree with you, but I also think that, you know, one of the
[00:38:09.640 --> 00:38:13.320]   interesting things is that not that I think was actually about AI is not that
[00:38:13.320 --> 00:38:15.440]   it produces good content, produces bad content.
[00:38:15.440 --> 00:38:20.680]   And in that sense, it reflects our biases and our presumptions and our mistakes
[00:38:20.720 --> 00:38:22.440]   and foibles over time.
[00:38:22.440 --> 00:38:23.600]   And that's what's interesting about it.
[00:38:23.600 --> 00:38:24.680]   We're not doing that.
[00:38:24.680 --> 00:38:26.560]   We're thinking it's always it's now a computer brain.
[00:38:26.560 --> 00:38:28.040]   It's going to produce perfect content.
[00:38:28.040 --> 00:38:30.320]   It ain't because it's based on us.
[00:38:30.320 --> 00:38:35.600]   It's produces a amalgam of what it gets from us, but it's not judging it.
[00:38:35.600 --> 00:38:36.720]   It's not thinking about it.
[00:38:36.720 --> 00:38:38.480]   It's not processing it.
[00:38:38.480 --> 00:38:41.120]   It's just statistically generating it.
[00:38:41.120 --> 00:38:43.440]   And I don't think that listening to useful.
[00:38:43.440 --> 00:38:50.560]   I watch a video of the chief scientist at OpenAI who used to be
[00:38:51.520 --> 00:38:52.520]   a freak in his name suddenly.
[00:38:52.520 --> 00:38:53.880]   It's on the rundown.
[00:38:53.880 --> 00:38:56.120]   But it was to Microsoft.
[00:38:56.120 --> 00:38:57.600]   It was an hour long about how all this works.
[00:38:57.600 --> 00:39:01.160]   My favorite part of it, the only funny part of it was he said that you can ask
[00:39:01.160 --> 00:39:06.480]   it to do things like give it a first break up your queries in the pieces.
[00:39:06.480 --> 00:39:07.880]   Don't give it the monster query.
[00:39:07.880 --> 00:39:10.320]   And then he said, you can tell it.
[00:39:10.320 --> 00:39:11.880]   I want you to be smart.
[00:39:11.880 --> 00:39:13.280]   I want you to have 120 IQ.
[00:39:13.280 --> 00:39:14.920]   He said, don't tell it.
[00:39:14.920 --> 00:39:18.600]   You want to have a 400 IQ because then it'll get into all kinds of sci-fi crap.
[00:39:18.600 --> 00:39:21.720]   He's great.
[00:39:21.720 --> 00:39:25.280]   Crow Hartman was on Floss Weekly a couple of weeks ago.
[00:39:25.280 --> 00:39:28.480]   And I'm pretty sure you're quite a fan of his being Mr.
[00:39:28.480 --> 00:39:29.760]   Lynet's Colonel and all.
[00:39:29.760 --> 00:39:33.600]   But I remember him mentioning AI and he wasn't really sold on it.
[00:39:33.600 --> 00:39:37.760]   And he said something along the lines of AI is just pattern matching.
[00:39:37.760 --> 00:39:38.400]   Yeah, sort of.
[00:39:38.400 --> 00:39:38.840]   It is.
[00:39:38.840 --> 00:39:40.680]   So that's essentially what it is.
[00:39:40.680 --> 00:39:41.160]   Yes.
[00:39:41.160 --> 00:39:43.680]   And it has some value.
[00:39:43.680 --> 00:39:48.320]   So it's Paul Thorett said, you know, Stephen King has a terrible time ending his novels,
[00:39:48.480 --> 00:39:52.960]   but probably with a little help from AI, he could come up with an end for his novels.
[00:39:52.960 --> 00:39:55.360]   I think Stephen would still have to write it.
[00:39:55.360 --> 00:39:55.680]   Yeah.
[00:39:55.680 --> 00:39:59.720]   But maybe the AI could come up with some solutions that Stephen have a part of.
[00:39:59.720 --> 00:40:00.840]   Have a partnership there.
[00:40:00.840 --> 00:40:06.760]   I mean, I think that's useful, but I think AI always needs to be done in a context of a human.
[00:40:06.760 --> 00:40:07.840]   Absolutely.
[00:40:07.840 --> 00:40:08.680]   As an assistant.
[00:40:08.680 --> 00:40:15.440]   And I mean, how many people are actually not including granted?
[00:40:15.440 --> 00:40:18.120]   Yes, people, companies are doing like really crappy jobs
[00:40:18.120 --> 00:40:21.440]   with chatbots, but they were doing that before AI got good, right?
[00:40:21.440 --> 00:40:28.520]   So like if something really matters and if it's quarter your product or quarter,
[00:40:28.520 --> 00:40:33.920]   your competency, I don't think we'll see AI without humans.
[00:40:33.920 --> 00:40:35.480]   And I think, I mean, like.
[00:40:35.480 --> 00:40:42.440]   Well, in which case, so that answers two of our qualms, which is, OK, so for every
[00:40:42.440 --> 00:40:43.800]   AI, there's going to be a human.
[00:40:43.800 --> 00:40:46.320]   So don't worry about job loss, maybe job change.
[00:40:46.720 --> 00:40:52.920]   But but humans will still be job loss if you're going to take, you know, 40% less work.
[00:40:52.920 --> 00:40:53.880]   Yeah.
[00:40:53.880 --> 00:40:59.360]   Well, I mean, but there still needs to be human taking the output of the AI and massaging it
[00:40:59.360 --> 00:41:00.200]   or using it.
[00:41:00.200 --> 00:41:04.360]   And no, I think there'll be a human making sure the output of the AI fits with the standard.
[00:41:04.360 --> 00:41:05.000]   Well, we'll see.
[00:41:05.000 --> 00:41:11.360]   My guess is, and this has been the way it is in full self driving in other areas
[00:41:11.360 --> 00:41:16.360]   where AI has let us down that you may be surprised by how much human input is needed.
[00:41:17.120 --> 00:41:18.440]   Going forward.
[00:41:18.440 --> 00:41:19.040]   So that's one.
[00:41:19.040 --> 00:41:21.480]   It also solves the problem of mass extinction.
[00:41:21.480 --> 00:41:28.080]   Unless the human is a evil genius intent on the destruction of mankind.
[00:41:28.080 --> 00:41:29.520]   Well, that's a different story.
[00:41:29.520 --> 00:41:30.520]   That's only could happen.
[00:41:30.520 --> 00:41:32.200]   But that is the AI or the human.
[00:41:32.200 --> 00:41:32.920]   That's the problem.
[00:41:32.920 --> 00:41:35.520]   Well, it just it makes us more scalable.
[00:41:35.520 --> 00:41:40.480]   Like if you think about it makes our workloads to be a scalable, evil genius.
[00:41:40.480 --> 00:41:42.040]   I think I see your point here.
[00:41:42.040 --> 00:41:42.560]   Yeah.
[00:41:43.120 --> 00:41:47.200]   I mean, I can scale up and write more thanks to AI, right?
[00:41:47.200 --> 00:41:49.360]   I can have it generate ideas for me.
[00:41:49.360 --> 00:41:50.920]   Does that put people out of work though?
[00:41:50.920 --> 00:41:56.240]   I mean, well, not in my case, because I'm ethical and I'm not venture backed.
[00:41:56.240 --> 00:42:01.480]   But if you're OK, a lot of people have seen that are out of work or ended up getting
[00:42:01.480 --> 00:42:04.440]   fired because Red Ventures decided to use AI.
[00:42:04.440 --> 00:42:06.280]   But I'm going to say, how well did that work?
[00:42:06.280 --> 00:42:08.640]   Yeah, I must say that that didn't work out so well for the Red.
[00:42:08.640 --> 00:42:11.600]   As well as Schwartz Esquire, the lawyer, right?
[00:42:12.240 --> 00:42:20.280]   Yeah, but I think I think under our current economic regime, if you can cut out costs
[00:42:20.280 --> 00:42:26.000]   and people are more expensive, well, actually, AI is not as economical as we think it is right now.
[00:42:26.000 --> 00:42:28.640]   It's very expensive and we don't really talk about that.
[00:42:28.640 --> 00:42:31.800]   Oh, that's another little bit of Levena bring that up about the cost.
[00:42:31.800 --> 00:42:33.280]   Yeah, well, we talked about it on Sunday.
[00:42:33.280 --> 00:42:37.040]   Yeah, we're Daniel, we're being I brought it up to I keep bringing it up.
[00:42:37.040 --> 00:42:39.120]   You know, this is not a cheap technology.
[00:42:39.640 --> 00:42:42.640]   Well, interesting part of it is not so expensive.
[00:42:42.640 --> 00:42:44.680]   The part that's expensive is creating the models.
[00:42:44.680 --> 00:42:51.160]   But actually, once you have a model, you can you can run queries against the model more
[00:42:51.160 --> 00:42:52.600]   cheaply. That's still expensive.
[00:42:52.600 --> 00:42:54.480]   Well, it's still, I mean,
[00:42:54.480 --> 00:42:58.240]   same model says is 10 times more expensive than a Google search chat.
[00:42:58.240 --> 00:43:00.720]   Yeah, so it's still so pricey.
[00:43:00.720 --> 00:43:05.040]   But Bruce and I are had a really good place in the slate talking about where the action
[00:43:05.040 --> 00:43:08.600]   is going to be is in the small open source models for that reason, right?
[00:43:08.920 --> 00:43:10.920]   Yeah, right.
[00:43:10.920 --> 00:43:13.640]   By the way, Snire is one of the people who control.
[00:43:13.640 --> 00:43:16.600]   Snire is one of the signatories to that statement, which I don't.
[00:43:16.600 --> 00:43:18.960]   I love Bruce. I have huge respect for him.
[00:43:18.960 --> 00:43:21.080]   That's what? Yeah.
[00:43:21.080 --> 00:43:24.280]   Well, ask him why he did it.
[00:43:24.280 --> 00:43:25.520]   Yeah, we got to get him on.
[00:43:25.520 --> 00:43:26.880]   I'm betting.
[00:43:26.880 --> 00:43:28.760]   Well, get on Emily Bender.
[00:43:28.760 --> 00:43:30.360]   Get on Timnidt Gebru.
[00:43:30.360 --> 00:43:31.960]   Get on Margaret Mitchell.
[00:43:31.960 --> 00:43:33.720]   Get on one of those Jason's from St.
[00:43:33.720 --> 00:43:34.480]   Goss Jason's.
[00:43:34.480 --> 00:43:38.120]   You write this off down for Tech News Weekly, Jason and Micah.
[00:43:38.120 --> 00:43:43.480]   Bruce Snire versus Emily Bender, by itself would be great, I think.
[00:43:43.480 --> 00:43:45.600]   Must see TV. Must see TV.
[00:43:45.600 --> 00:43:49.400]   Yeah, I think this is one area where I think we can do a good job,
[00:43:49.400 --> 00:43:52.600]   better job of the mainstream media, because maybe we're a little more
[00:43:52.600 --> 00:43:54.320]   informed and a little more skeptical.
[00:43:54.320 --> 00:43:58.440]   Yeah, and we have three hours and we have three freaking hours to fill.
[00:43:58.440 --> 00:44:02.160]   We don't have to have three hours.
[00:44:02.160 --> 00:44:02.680]   Three.
[00:44:02.680 --> 00:44:05.240]   I just want you to know it could just be two.
[00:44:05.240 --> 00:44:06.360]   I didn't know it was three.
[00:44:06.360 --> 00:44:06.680]   Good.
[00:44:06.680 --> 00:44:08.040]   I'm going for two this week.
[00:44:08.040 --> 00:44:11.720]   You'll be glad enough, Stacy, which is why I am going to take a little break,
[00:44:11.720 --> 00:44:15.960]   a little teeny, tiny time out so you can go get a waffle.
[00:44:15.960 --> 00:44:19.160]   And I'm going to have a chip question for Stacy if I can.
[00:44:19.160 --> 00:44:20.360]   And a chip question.
[00:44:20.360 --> 00:44:21.560]   A chip question for Stacy.
[00:44:21.560 --> 00:44:23.520]   I think he has one too, actually.
[00:44:23.520 --> 00:44:25.320]   But what Stacy is doing?
[00:44:25.320 --> 00:44:26.360]   What's Stacy doing?
[00:44:26.360 --> 00:44:31.880]   Stacy, my child brought young lemon bar that they made from their class.
[00:44:31.880 --> 00:44:33.040]   Wow.
[00:44:33.040 --> 00:44:34.760]   So, yeah, you take that ad break.
[00:44:34.760 --> 00:44:36.600]   I mean, I'm eating that ad break.
[00:44:36.600 --> 00:44:37.880]   I'm having lemons.
[00:44:37.880 --> 00:44:42.040]   Let's make a let's like make some lemonade out of back called
[00:44:42.040 --> 00:44:42.520]   Stacy's number.
[00:44:42.520 --> 00:44:44.120]   That's sour.
[00:44:44.120 --> 00:44:46.040]   I showed it.
[00:44:46.040 --> 00:44:53.240]   They brought to you by HPE Green Lake orchestrated by those great experts at CDW,
[00:44:53.240 --> 00:44:59.880]   the helpful people at CDW understand your organization needs simple management over its big data.
[00:44:59.880 --> 00:45:02.520]   But you know, this is an interesting world.
[00:45:02.520 --> 00:45:07.000]   We live in some people need their workloads on prem for organizational requirements.
[00:45:07.000 --> 00:45:10.520]   If that's the case, it could be challenging, right, to organize and optimize your data.
[00:45:10.520 --> 00:45:16.200]   And that's where CDW can help your organization by consolidating and managing all your data in one
[00:45:16.200 --> 00:45:23.320]   flexible, unified experience with the HPE Green Lake edge to cloud platform.
[00:45:23.320 --> 00:45:27.880]   Experience you get with HPE Green Lake is unique because no matter where your data applications live,
[00:45:27.880 --> 00:45:33.240]   you can free up energy and resources with automated processes and streamline management.
[00:45:33.240 --> 00:45:35.880]   We all can use some more streamlining.
[00:45:35.880 --> 00:45:42.440]   I know again, not only that HPE Green Lake creates a seamless cloud experience among multiple
[00:45:42.440 --> 00:45:43.240]   data environments.
[00:45:43.240 --> 00:45:48.680]   Thanks to the Aviz as a service model that meets your remote workforce on the edge.
[00:45:48.680 --> 00:45:52.440]   And with unrivaled scalability, you'll see an instant increase in capacity,
[00:45:52.440 --> 00:45:56.280]   allowing for greater flexibility and accelerated business growth.
[00:45:56.280 --> 00:45:59.880]   So your team can tackle bigger priorities like innovation.
[00:45:59.880 --> 00:46:05.880]   When you need to get more out of your technology, HPE makes data transformation possible.
[00:46:05.880 --> 00:46:08.600]   CDW makes it powerful.
[00:46:08.600 --> 00:46:13.560]   Learn more at CDW.com/HPE.
[00:46:13.560 --> 00:46:18.600]   We thank CDW so much for supporting this week in Google and all of our Twitch shows.
[00:46:18.600 --> 00:46:31.000]   CDW.com/HPE.
[00:46:31.000 --> 00:46:33.880]   Do you want to do a chip story? Are you through with a lemon bar?
[00:46:33.880 --> 00:46:38.200]   Stacy, a question. I want to explain if she's not too puckered up to talk.
[00:46:38.200 --> 00:46:43.800]   I think she's in a lemon bar or a lemon bar or a comma right now.
[00:46:43.800 --> 00:46:44.360]   She is.
[00:46:44.360 --> 00:46:45.320]   She's not here.
[00:46:45.320 --> 00:46:46.040]   Word she's saying.
[00:46:46.040 --> 00:46:46.840]   We can't be here.
[00:46:46.840 --> 00:46:48.360]   So did we finish the ad?
[00:46:48.360 --> 00:46:49.080]   Yeah.
[00:46:49.080 --> 00:46:50.120]   Sorry.
[00:46:50.120 --> 00:46:51.880]   Did you finish the lemon bar?
[00:46:51.880 --> 00:46:54.120]   No, I started writing something.
[00:46:54.120 --> 00:46:57.000]   Oh, are you like a little ADD?
[00:46:57.000 --> 00:46:58.280]   Are you a little ADD?
[00:46:58.280 --> 00:46:59.080]   Just out of care.
[00:46:59.080 --> 00:47:01.000]   I am a lot ADD.
[00:47:01.000 --> 00:47:03.800]   Who is the co-worker is not ADD?
[00:47:03.800 --> 00:47:06.920]   It's always me.
[00:47:06.920 --> 00:47:07.400]   It's always me.
[00:47:07.400 --> 00:47:08.360]   It's always me.
[00:47:08.360 --> 00:47:09.800]   You're not ADD.
[00:47:09.800 --> 00:47:11.160]   Everywhere I go.
[00:47:11.160 --> 00:47:12.280]   Everybody else is ADD.
[00:47:12.280 --> 00:47:13.160]   Everywhere I go.
[00:47:13.160 --> 00:47:14.760]   It's our study.
[00:47:14.760 --> 00:47:16.520]   It says it's the fluoride in the water.
[00:47:17.640 --> 00:47:18.680]   Oh, you.
[00:47:18.680 --> 00:47:19.400]   I'm not a well.
[00:47:19.400 --> 00:47:20.520]   I don't have fluoride.
[00:47:20.520 --> 00:47:22.200]   Let me just have more so maybe I can be like that.
[00:47:22.200 --> 00:47:23.480]   That's it, Leo.
[00:47:23.480 --> 00:47:26.840]   The AI is going to figure out a way to put too much fluoride in the water.
[00:47:26.840 --> 00:47:27.720]   And we're all dead.
[00:47:27.720 --> 00:47:28.920]   That's the extinction event.
[00:47:28.920 --> 00:47:29.720]   Oh, okay.
[00:47:29.720 --> 00:47:30.440]   There it is.
[00:47:30.440 --> 00:47:32.280]   All right.
[00:47:32.280 --> 00:47:33.720]   Here's my double question for Stacy.
[00:47:33.720 --> 00:47:35.400]   Yeah.
[00:47:35.400 --> 00:47:42.600]   Was it accidental that AI said, oh, these graphic chips are good at this stuff
[00:47:42.600 --> 00:47:44.920]   that that everybody happened to make?
[00:47:45.480 --> 00:47:46.600]   Or,
[00:47:46.600 --> 00:47:50.520]   and video is now the first trillion dollar company of AI
[00:47:50.520 --> 00:47:52.440]   because their chips are used in all of this stuff.
[00:47:52.440 --> 00:47:57.480]   Those chips already existed before for graphics and games and things.
[00:47:57.480 --> 00:47:58.840]   And then it was Bitcoin, right?
[00:47:58.840 --> 00:48:04.680]   So remember how Leo was talking about the humans are really great at parallel processing?
[00:48:04.680 --> 00:48:09.080]   To do graphics, you have to be great at parallel processing.
[00:48:09.080 --> 00:48:11.880]   So graphics chips are different from like Intel's chips.
[00:48:12.920 --> 00:48:16.280]   Because they do a bunch of jobs in parallel.
[00:48:16.280 --> 00:48:17.960]   They chunk a data with their cats.
[00:48:17.960 --> 00:48:20.200]   Yeah, they do a lot of.
[00:48:20.200 --> 00:48:23.640]   So Nvidia, and this is why, Beth, when I was at Fortune,
[00:48:23.640 --> 00:48:26.680]   I wanted to put Jensen on the cover of Fortune back in 2015.
[00:48:26.680 --> 00:48:27.160]   Oh, you were right.
[00:48:27.160 --> 00:48:28.840]   I'm like, oh, too early.
[00:48:28.840 --> 00:48:30.120]   Oh, you were so right.
[00:48:30.120 --> 00:48:33.000]   AI, the way that she's talking Jensen Huang,
[00:48:33.000 --> 00:48:36.840]   the founder of Nvidia who has gone from being the $200 man
[00:48:36.840 --> 00:48:38.680]   that is being the $30 billion man.
[00:48:38.680 --> 00:48:40.200]   It's really a great story.
[00:48:40.200 --> 00:48:42.760]   You should write to all those editors who stopped you and say,
[00:48:42.760 --> 00:48:43.640]   "Me and you."
[00:48:43.640 --> 00:48:45.480]   I mean, Jensen is real.
[00:48:45.480 --> 00:48:46.680]   Yeah, they know.
[00:48:46.680 --> 00:48:47.240]   They blew it.
[00:48:47.240 --> 00:48:47.960]   I don't.
[00:48:47.960 --> 00:48:49.080]   I'm so confused.
[00:48:49.080 --> 00:48:49.720]   You can tell, yeah.
[00:48:49.720 --> 00:48:53.080]   So that's why graphics chips.
[00:48:53.080 --> 00:49:00.520]   And graphics chips, remember like the seminal moment in computer vision was back in 2012
[00:49:00.520 --> 00:49:02.040]   when it was actually Jeff Hinton.
[00:49:02.040 --> 00:49:04.520]   They did a ResNet.
[00:49:04.520 --> 00:49:05.480]   Was it ImageNet?
[00:49:05.480 --> 00:49:06.360]   Sorry, ImageNet.
[00:49:06.360 --> 00:49:12.680]   And they basically were able to prove out Jeff Hinton's theories.
[00:49:12.680 --> 00:49:19.480]   About how quickly and how well a computer could name things,
[00:49:19.480 --> 00:49:21.400]   like recognize an image.
[00:49:21.400 --> 00:49:24.200]   So they were able to do that thanks to graphics chips
[00:49:24.200 --> 00:49:26.200]   and their massively parallel processing.
[00:49:26.200 --> 00:49:28.440]   So it is not a secret or it's not like--
[00:49:28.440 --> 00:49:29.000]   No, no, no.
[00:49:29.000 --> 00:49:31.240]   I guess what I'm trying to ask is this.
[00:49:31.240 --> 00:49:33.880]   They looked around and said,
[00:49:33.880 --> 00:49:35.640]   "Oh, graphics chips already do that.
[00:49:35.640 --> 00:49:41.000]   If you were going to design chips from scratch to do what AI does now,
[00:49:41.000 --> 00:49:42.840]   would it look a lot like a graphics chip?
[00:49:42.840 --> 00:49:48.680]   Or was it just incredibly good timing to have really good graphics chips that then had other uses?"
[00:49:48.680 --> 00:49:52.120]   And I'm going to throw in a little extra on this one.
[00:49:52.120 --> 00:49:58.920]   What is the difference between a GPU and a NPU or when Intel's calling a VPU?
[00:49:58.920 --> 00:49:59.400]   Thank you.
[00:49:59.400 --> 00:50:01.800]   A neural processing or visual processing.
[00:50:01.800 --> 00:50:04.920]   Yeah, these are the new AI chips in addition to GPU.
[00:50:04.920 --> 00:50:05.960]   Right, good.
[00:50:05.960 --> 00:50:09.080]   I don't know what the difference between a GPU and an S.
[00:50:09.080 --> 00:50:09.720]   I'm glad you asked.
[00:50:09.720 --> 00:50:10.360]   I'm guessing.
[00:50:10.360 --> 00:50:14.520]   It depends on--
[00:50:14.520 --> 00:50:18.920]   So we built the algorithms for the silicon we have.
[00:50:18.920 --> 00:50:19.240]   Right?
[00:50:19.240 --> 00:50:23.320]   And then we're like, it's kind of a market and where they're running in lockstep.
[00:50:23.320 --> 00:50:26.440]   So now we have things and we're developing chips.
[00:50:26.440 --> 00:50:28.360]   So we're like, oh, okay.
[00:50:28.360 --> 00:50:29.160]   So you have to move.
[00:50:29.160 --> 00:50:33.880]   When a computer is doing a calculation, you have memory.
[00:50:33.880 --> 00:50:34.040]   Right?
[00:50:34.040 --> 00:50:36.280]   You leave stuff in memory right next to--
[00:50:36.280 --> 00:50:37.400]   There's two levels of memory.
[00:50:37.400 --> 00:50:39.400]   There's on-chip memory and then there's the memory.
[00:50:39.400 --> 00:50:43.960]   Like that used to be tape or other memory.
[00:50:43.960 --> 00:50:44.440]   Discs.
[00:50:44.440 --> 00:50:46.760]   Discs or even--
[00:50:46.760 --> 00:50:48.040]   It's not on the same chip.
[00:50:48.040 --> 00:50:49.160]   It's not right next door.
[00:50:49.160 --> 00:50:49.800]   Right?
[00:50:49.800 --> 00:50:50.520]   So you've got to go--
[00:50:50.520 --> 00:50:51.080]   If you've got to--
[00:50:51.080 --> 00:50:55.960]   So that memory is limited.
[00:50:55.960 --> 00:50:59.480]   So you can only store so much information in the chip.
[00:50:59.480 --> 00:51:02.680]   If it's massively parallel, it can pull all of that in and then it needs more.
[00:51:02.680 --> 00:51:05.320]   But that's like IO to get--
[00:51:05.320 --> 00:51:09.080]   You've got to go to the next building over to get your more of the data that you need
[00:51:09.080 --> 00:51:12.200]   to process and break it over to the room that's next to the big processor.
[00:51:12.200 --> 00:51:12.760]   Right?
[00:51:12.760 --> 00:51:15.160]   Just think of a chip as a series of rooms.
[00:51:15.160 --> 00:51:21.000]   So you could design a chip with a bigger memory room that's right,
[00:51:21.000 --> 00:51:23.240]   that's accessible to your processor.
[00:51:23.240 --> 00:51:27.400]   There's also things like the way we do our algorithms.
[00:51:27.400 --> 00:51:29.880]   Like there's things like matrix multiplication.
[00:51:29.880 --> 00:51:31.960]   That type of math is really important.
[00:51:31.960 --> 00:51:36.200]   So you can build chips that are optimized for better matrix multiplication.
[00:51:38.120 --> 00:51:44.200]   But we're also developing new algorithms and new ways to do this all the time.
[00:51:44.200 --> 00:51:48.120]   So you actually might find something that's really effective,
[00:51:48.120 --> 00:51:52.280]   like from either cost or from a power or performance perspective.
[00:51:52.280 --> 00:51:55.160]   And you might actually be like, I want to do this so much.
[00:51:55.160 --> 00:51:55.720]   And I'm Google.
[00:51:55.720 --> 00:51:57.720]   I'm going to build a whole new chip design for this.
[00:51:57.720 --> 00:51:58.360]   Is this a teaser?
[00:51:58.360 --> 00:52:01.640]   It could be tensor, but it could be--
[00:52:01.640 --> 00:52:07.880]   Like if some genius at Google decides they want to build a whole different type
[00:52:07.880 --> 00:52:09.960]   of algorithm, they might--
[00:52:09.960 --> 00:52:13.240]   If they decided it was the best algorithm for search,
[00:52:13.240 --> 00:52:17.080]   it would be economically viable for them to design a completely new chip just for that.
[00:52:17.080 --> 00:52:18.360]   And this is quite a way to--
[00:52:18.360 --> 00:52:21.240]   This is quite a microsoft, has done with the NPUs,
[00:52:21.240 --> 00:52:25.480]   because they're basically FPGAs, they're programmable processors.
[00:52:25.480 --> 00:52:30.040]   So you can optimize them for new workloads or new styles of calculation.
[00:52:30.040 --> 00:52:35.080]   So Stacy, when you pushed NVIDIA Cover Story, what was your pitch then?
[00:52:36.120 --> 00:52:41.320]   My pitch was they're enabling AI evolution.
[00:52:41.320 --> 00:52:46.440]   They're one of the only contenders, because at the time, Intel didn't have a great graphics option.
[00:52:46.440 --> 00:52:53.240]   AMD did, but Intel-- or sorry, but NVIDIA was doing just so well at it that--
[00:52:53.240 --> 00:52:57.720]   And there was very little competition that wasn't a startup,
[00:52:57.720 --> 00:53:00.120]   that they were going to win for a long time to come.
[00:53:00.120 --> 00:53:04.280]   Did you see these other uses for graphics processors on the horizon at that time?
[00:53:05.640 --> 00:53:12.680]   You saw it starting-- I mean, once-- again, once that image net competition happened in 2012, yes.
[00:53:12.680 --> 00:53:14.600]   And I went to a couple-- they called it NIPs.
[00:53:14.600 --> 00:53:17.560]   They used-- that's sexist, so they don't call it that now.
[00:53:17.560 --> 00:53:24.840]   I went to a couple of their shows and listened to the researchers talk about the types of chips
[00:53:24.840 --> 00:53:29.880]   they wanted. So then I was trying to work with-- because all these startups were coming with
[00:53:29.880 --> 00:53:30.920]   new architectures.
[00:53:30.920 --> 00:53:32.360]   So basically--
[00:53:32.360 --> 00:53:34.520]   I'm going to see if I can summarize it.
[00:53:34.520 --> 00:53:35.720]   It's just simple work.
[00:53:35.720 --> 00:53:36.520]   Yeah, I'm sorry.
[00:53:36.520 --> 00:53:37.160]   I'm sorry.
[00:53:37.160 --> 00:53:37.960]   It was actually--
[00:53:37.960 --> 00:53:38.920]   I don't know.
[00:53:38.920 --> 00:53:41.400]   I think I'm doing this in a sense.
[00:53:41.400 --> 00:53:48.040]   So the way computers, since time and memorial have been designed is something called the von Neumann
[00:53:48.040 --> 00:53:52.040]   architecture, which is basically a single instruction.
[00:53:52.040 --> 00:53:57.160]   Von Neumann machine has a processor, it has memory, it might have storage,
[00:53:57.160 --> 00:53:58.840]   and it's a single pipeline.
[00:53:58.840 --> 00:54:01.640]   It's like an assembly line for calculation.
[00:54:02.440 --> 00:54:07.160]   Very quickly, we realized especially when graphics became popular and Intel did something
[00:54:07.160 --> 00:54:08.520]   called the MMX extensions.
[00:54:08.520 --> 00:54:13.800]   We realized-- or SIMD extensions-- that we should be able to operate instead of on a single thing
[00:54:13.800 --> 00:54:21.480]   at a time on a mass of a texture, for instance, in a graphics environment.
[00:54:21.480 --> 00:54:24.840]   In one instruction operator on all of that data.
[00:54:24.840 --> 00:54:28.760]   So they created these more parallel forms of processing.
[00:54:28.760 --> 00:54:31.160]   And that's when the first graphics processors came along.
[00:54:31.160 --> 00:54:34.040]   You might probably remember the 3D FX Voodoo car.
[00:54:34.040 --> 00:54:39.880]   And these graphics processors were just basically glorified multi-processor machines.
[00:54:39.880 --> 00:54:43.560]   All we've done is-- and that's what NVIDIA actually bought.
[00:54:43.560 --> 00:54:47.560]   I think bought 3D FX and incorporate their technology along with a lot of others.
[00:54:47.560 --> 00:54:51.160]   Created these massively parallel processors and not von Neumann machines.
[00:54:51.160 --> 00:54:51.640]   Well, they are.
[00:54:51.640 --> 00:54:54.680]   There are just many, many, many von Neumann machines all working at once.
[00:54:54.680 --> 00:55:00.040]   And when it turns out, if you're doing a specific kind of thing, not graphics,
[00:55:00.040 --> 00:55:06.200]   but machine language training, large language models and GANs and things like that,
[00:55:06.200 --> 00:55:11.960]   you can specialize even further in these graphics chips into something that's designed for
[00:55:11.960 --> 00:55:14.040]   particularly artificial intelligence applications.
[00:55:14.040 --> 00:55:18.440]   Those are NPUs or TPUs or what Intel calls VPUs.
[00:55:18.440 --> 00:55:25.480]   They are basically GPUs that have been specialized for the kinds of calculations you do in machine learning.
[00:55:25.480 --> 00:55:30.280]   Am I-- is that accurate? Stacey, is that a fair way of summarizing that?
[00:55:30.280 --> 00:55:31.240]   That's much better.
[00:55:31.240 --> 00:55:34.680]   No, Stacey, yours was really informative.
[00:55:34.680 --> 00:55:35.000]   All right.
[00:55:35.000 --> 00:55:35.880]   No, and I used to have this--
[00:55:35.880 --> 00:55:37.640]   Go ahead.
[00:55:37.640 --> 00:55:37.800]   Go ahead.
[00:55:37.800 --> 00:55:38.280]   Go on.
[00:55:38.280 --> 00:55:41.800]   Well, I was going to say there-- and there's all these cool new things coming down the pipeline.
[00:55:41.800 --> 00:55:46.760]   Because one of the issues is when you have all of these parallel processing,
[00:55:46.760 --> 00:55:50.440]   you have to figure out how to tell everything how to work together and do the job.
[00:55:50.440 --> 00:55:50.920]   Right.
[00:55:50.920 --> 00:55:54.120]   And so you see like the rise of these fabrics inside, right?
[00:55:54.120 --> 00:55:54.680]   Right.
[00:55:54.680 --> 00:55:59.960]   And communicating what's going on and moving all that data around is becoming a bottleneck.
[00:55:59.960 --> 00:56:06.760]   So your next generation, like, we're not there yet, but I'm really excited about like photonics
[00:56:06.760 --> 00:56:08.920]   on chip. I know you're excited about quantum Leo.
[00:56:08.920 --> 00:56:09.880]   Yep.
[00:56:09.880 --> 00:56:12.440]   Photonics too is a little closer actually.
[00:56:12.440 --> 00:56:13.800]   Yeah.
[00:56:13.800 --> 00:56:19.160]   Both Microsoft and Apple have fabrics to support this kind of movement.
[00:56:19.160 --> 00:56:24.680]   Apple's what Apple's done is actually put the NPU on chip. They call it the machine-- what
[00:56:24.680 --> 00:56:27.800]   do they call it? They have a name for it, machine learning module or something like that.
[00:56:27.800 --> 00:56:32.760]   And so that's one way of doing it. This is perhaps even faster because it's integrated.
[00:56:32.760 --> 00:56:37.000]   Apple's making these giant chips that have everything, including RAM on money.
[00:56:37.000 --> 00:56:39.560]   Well, now what you're talking about is the rise of chiplets.
[00:56:39.560 --> 00:56:41.400]   Right. And so Cs, yeah.
[00:56:41.400 --> 00:56:45.960]   Yeah. It's basically-- I mean, we would call it a system on a chip, which is a bunch of chips,
[00:56:46.840 --> 00:56:49.800]   you know, on a big chunk of die.
[00:56:49.800 --> 00:56:52.120]   Die, thank you. I was like, I'm a big chunk away from it. What do you call that?
[00:56:52.120 --> 00:56:55.000]   Where does Tensor fit into this?
[00:56:55.000 --> 00:56:57.080]   It's Google's branding. It serves as custom designed.
[00:56:57.080 --> 00:56:59.640]   Yeah. For their neural process.
[00:56:59.640 --> 00:57:00.600]   Their version of this. Yeah.
[00:57:00.600 --> 00:57:01.640]   All right. One more question.
[00:57:01.640 --> 00:57:04.920]   I'm sorry, but this is, you know, Jeff Learn's moment.
[00:57:04.920 --> 00:57:08.360]   Now put this into the discussion of neural networks.
[00:57:08.360 --> 00:57:15.160]   Is it just a whole bunch of mass processing going on in NASA processing?
[00:57:15.640 --> 00:57:17.160]   [laughter]
[00:57:17.160 --> 00:57:22.600]   So a neural network. I mean, when we talk about AI, a lot of what we're talking about are neural
[00:57:22.600 --> 00:57:25.080]   networks, which is-- That's why I'm asking.
[00:57:25.080 --> 00:57:27.560]   Yeah. I mean, a neural network is just--
[00:57:27.560 --> 00:57:33.560]   So this is where the computer scientists are saying, well, we know the brain is very,
[00:57:33.560 --> 00:57:41.080]   very parallel. So maybe that's how the best way to simulate learning in a machine is to do this
[00:57:41.080 --> 00:57:46.440]   highly parallel processing. So it's parallel.
[00:57:46.440 --> 00:57:49.240]   We're trying to duplicate neurons, right? That's the word neural.
[00:57:49.240 --> 00:57:50.200]   The ship with the--
[00:57:50.200 --> 00:57:52.440]   The ship was parallel, was capable of parallel.
[00:57:52.440 --> 00:57:54.600]   And now you want to do a whole bunch of parallels.
[00:57:54.600 --> 00:57:58.280]   Well, and then Stacy brought up matrixes. And that's really a better way of describing it.
[00:57:58.280 --> 00:58:01.240]   So parallel sounds like lanes that are going in the same direction.
[00:58:01.240 --> 00:58:01.720]   Right. Right.
[00:58:01.720 --> 00:58:05.000]   These are all interconnected in a variety of ways. They're more like a grid.
[00:58:05.000 --> 00:58:06.360]   Right? Yeah.
[00:58:06.360 --> 00:58:11.560]   And so it's much more flexible in the kinds of parallel processing.
[00:58:11.560 --> 00:58:12.280]   Look, at where--
[00:58:12.280 --> 00:58:16.120]   Yeah. And Stacy is obviously more of an expert on this than I am.
[00:58:16.120 --> 00:58:19.720]   And I'm just trying to understand it in layman's terms.
[00:58:19.720 --> 00:58:24.600]   It's part of the computer revolution that's going on. It really is.
[00:58:24.600 --> 00:58:25.880]   Yeah. Go ahead, Stacy.
[00:58:25.880 --> 00:58:32.200]   Well, and we have all these branches that we tried, and what's really fun, I think,
[00:58:32.200 --> 00:58:40.120]   what's really fun is right now we're seeing like there's one way to train and run a model
[00:58:40.120 --> 00:58:43.080]   that's on these massive NVIDIA type cards, right?
[00:58:43.080 --> 00:58:45.320]   And you know, you all know I'm excited about tiny ML.
[00:58:45.320 --> 00:58:50.600]   But there's a lot of research going into other places, like how do we shrink those models?
[00:58:50.600 --> 00:58:55.160]   And it's turning out that the way you build the models on the big machines
[00:58:55.160 --> 00:58:58.920]   are probably not the best way to build models that are going to run on the small machines,
[00:58:58.920 --> 00:59:02.200]   which means we're going-- so I just sat through like three hours of like--
[00:59:02.200 --> 00:59:02.920]   That's interesting.
[00:59:02.920 --> 00:59:09.160]   Yeah. So like things like back propagation and pruning are really good ways to like--
[00:59:09.160 --> 00:59:10.920]   This is what Google was talking about.
[00:59:10.920 --> 00:59:11.480]   That's what you--
[00:59:11.480 --> 00:59:17.080]   That Google I/O was how we get these massive models into your phone, right?
[00:59:17.080 --> 00:59:20.120]   And I didn't realize it's different. You change the way you do the model.
[00:59:20.120 --> 00:59:20.360]   It's--
[00:59:20.360 --> 00:59:25.000]   Which is also part of the Bruce Steyer article is all the open source,
[00:59:25.000 --> 00:59:28.920]   being able to use it on smaller machines of any sort and have a real impact.
[00:59:28.920 --> 00:59:29.640]   Right.
[00:59:29.640 --> 00:59:29.720]   Well--
[00:59:29.720 --> 00:59:32.520]   What is considered small machine versus big machine?
[00:59:32.520 --> 00:59:33.080]   That's what I'm--
[00:59:33.080 --> 00:59:37.480]   Yeah. And the other thing you have to be real clear about when we're talking about this is
[00:59:37.480 --> 00:59:40.840]   there's the training, which is the building of the model,
[00:59:40.840 --> 00:59:43.240]   and then there's the inference for the running of the models.
[00:59:43.240 --> 00:59:49.560]   So what I'm-- so when you're talking about training, it's all in these big computers.
[00:59:49.560 --> 00:59:56.840]   But the cutting edge thinking that I'm listening to research on is actually--
[00:59:56.840 --> 01:00:00.520]   Let's figure out how to train on these small computers.
[01:00:00.520 --> 01:00:04.840]   But when we train, we're going to actually run completely different types of algorithms
[01:00:04.840 --> 01:00:14.440]   and help establish weights and focus for those algorithms for tiny machines.
[01:00:14.440 --> 01:00:18.840]   What Google's talking about with their large language models like Gecko and Unicorn and
[01:00:19.720 --> 01:00:22.840]   Chevenai, and I don't remember how they get from big to small.
[01:00:22.840 --> 01:00:28.280]   They're-- they have two-- I think they talked about having two different models.
[01:00:28.280 --> 01:00:36.280]   And one is just shrinking it down to run inference on smaller and smaller machines.
[01:00:36.280 --> 01:00:42.040]   In small machines, most people when they say small machines, they're talking about like a gateway,
[01:00:42.040 --> 01:00:44.200]   which is still running like a computer chip.
[01:00:44.200 --> 01:00:46.120]   I used to have a gateway computer.
[01:00:46.120 --> 01:00:48.120]   Sorry, not that kind of a convenience.
[01:00:48.120 --> 01:00:48.920]   It was all the details.
[01:00:48.920 --> 01:00:49.560]   With the count.
[01:00:49.560 --> 01:00:53.960]   So like any machine with an application processor that runs Linux,
[01:00:53.960 --> 01:01:00.120]   when I talk about things like TinyML, I'm talking about running on a freaking sensor.
[01:01:00.120 --> 01:01:01.640]   So like TensorFlow--
[01:01:01.640 --> 01:01:02.040]   Okay.
[01:01:02.040 --> 01:01:03.800]   Yeah.
[01:01:03.800 --> 01:01:07.720]   So-- and I'm trying to think of how small TensorFlow Lite can go.
[01:01:07.720 --> 01:01:08.760]   I don't think it's--
[01:01:08.760 --> 01:01:09.560]   Well, see your phone.
[01:01:09.560 --> 01:01:10.760]   Yeah.
[01:01:10.760 --> 01:01:15.960]   So-- and your phone is actually-- like when people talk about running machine learning at the
[01:01:15.960 --> 01:01:17.400]   edge, that's usually where they're talking about it.
[01:01:17.400 --> 01:01:19.080]   It's freaking impressive.
[01:01:19.080 --> 01:01:23.960]   What I'm talking to people about is running machine learning on like a micro process,
[01:01:23.960 --> 01:01:25.080]   of like a micro controller.
[01:01:25.080 --> 01:01:30.120]   Because see, I would assume a smartphone is a small computer,
[01:01:30.120 --> 01:01:32.360]   I mean, other than the physical form.
[01:01:32.360 --> 01:01:33.160]   I know, right?
[01:01:33.160 --> 01:01:38.760]   But I would-- I would say this smartphone is a bigger computer because of how all of the
[01:01:38.760 --> 01:01:41.000]   process and power it has, right?
[01:01:41.000 --> 01:01:41.560]   The issue--
[01:01:41.560 --> 01:01:41.560]   Yes.
[01:01:41.560 --> 01:01:44.840]   So what Bruce is talking about in this article in general is
[01:01:45.800 --> 01:01:49.720]   because these models have been so big and require so much horsepower,
[01:01:49.720 --> 01:01:57.080]   it's dominated by Google, Microsoft, Facebook, big companies with big capitalization.
[01:01:57.080 --> 01:01:58.520]   And big data and big data.
[01:01:58.520 --> 01:02:01.880]   And what Bruce is saying is there is a movement--
[01:02:01.880 --> 01:02:06.440]   and actually, ironically, it's Facebook's L-L-L-A-M-A--
[01:02:06.440 --> 01:02:06.920]   Lama.
[01:02:06.920 --> 01:02:08.520]   Lama, I guess you pronounce that--
[01:02:08.520 --> 01:02:11.880]   or Yama that is opening the way to this.
[01:02:11.880 --> 01:02:16.120]   This was actually leaked out and now is being widely used by the open source community because
[01:02:16.120 --> 01:02:22.520]   now the idea is to take the power away from-- or at least to keep them from owning it entirely,
[01:02:22.520 --> 01:02:29.480]   Google and Facebook and Apple and Microsoft and Amazon and let small developers work in this space.
[01:02:29.480 --> 01:02:33.240]   Because if you don't do that, then you just don't get innovation,
[01:02:33.240 --> 01:02:37.880]   you just get what Sam Altman clearly wants, which is to the big guys owning it.
[01:02:37.880 --> 01:02:38.200]   Yeah.
[01:02:39.800 --> 01:02:43.880]   And good luck trying to control that if you want to try to stop AI because it's going to
[01:02:43.880 --> 01:02:47.640]   destroy everything. Well, there's going to be a whole bunch of little factories.
[01:02:47.640 --> 01:02:51.480]   Why it surprises me that Bruce signed that statement to be honest with you because--
[01:02:51.480 --> 01:02:52.600]   I agree.
[01:02:52.600 --> 01:02:53.480]   He's an advocate for us.
[01:02:53.480 --> 01:02:55.240]   It's such a namby-pamby statement.
[01:02:55.240 --> 01:02:55.640]   Yeah.
[01:02:55.640 --> 01:02:57.000]   It's like, yeah, this could be a problem.
[01:02:57.000 --> 01:02:57.480]   It's safe to be a problem.
[01:02:57.480 --> 01:02:58.840]   That's not quite namby-pamby.
[01:02:58.840 --> 01:03:02.600]   Hey, so Bruce writes, "This isn't the first time companies have ignored the power of the open source
[01:03:02.600 --> 01:03:07.480]   community. Sun never understood Linux, Netscape never understood the Apache web server.
[01:03:07.480 --> 01:03:11.880]   Open source is very good at original innovations. But once an invasion is seen and picked up,
[01:03:11.880 --> 01:03:16.280]   the community can be a pretty overwhelming thing. The large companies may respond by trying--
[01:03:16.280 --> 01:03:21.400]   and this is what they're doing-- to retrench and pulling their models back from the open source
[01:03:21.400 --> 01:03:27.880]   community. But it's too late. We've ended an era of large language model democratization.
[01:03:27.880 --> 01:03:32.920]   By showing that smaller models could be highly effective, enabling easy experimentation,
[01:03:32.920 --> 01:03:37.400]   diversifying control, and providing incentives that are not profit motivated,
[01:03:37.400 --> 01:03:41.160]   open source initiatives are moving us into a more dynamic and inclusive
[01:03:41.160 --> 01:03:43.720]   AI landscape. Maybe you should get Bruce on floss.
[01:03:43.720 --> 01:03:45.480]   We can try.
[01:03:45.480 --> 01:03:46.520]   Yeah, that would be a really--
[01:03:46.520 --> 01:03:48.120]   This is a real news.
[01:03:48.120 --> 01:03:51.080]   Yeah, Bruce has been on our shows before I'm sure Doc knows Bruce.
[01:03:51.080 --> 01:03:55.000]   This article is from Slate by Bruce Schnier and Jim Waldo.
[01:03:55.000 --> 01:03:59.800]   I bet you, Bruce would like to come on and talk about this, because that's really important.
[01:03:59.800 --> 01:04:03.480]   You don't want-- I mean, honestly, that's the threat.
[01:04:04.600 --> 01:04:10.360]   It's not to a mass extinction of the human race, but it might be to extinction of small companies
[01:04:10.360 --> 01:04:13.160]   and individuals. Or vice versa. This is where there's no moat.
[01:04:13.160 --> 01:04:16.840]   We talked about that two weeks ago, that there's no moat around the big guys,
[01:04:16.840 --> 01:04:20.920]   because the little guys can do it. And it also goes back to the stochastic parrots, folks,
[01:04:20.920 --> 01:04:26.360]   once again, as much as many rivers do, where if you have smaller models with smaller data sets,
[01:04:26.360 --> 01:04:30.680]   that's more manageable in terms of being able to audit what they're doing, where you get your
[01:04:30.680 --> 01:04:36.840]   stuff, what's happening there. And so a small version of AI may be a really more interesting
[01:04:36.840 --> 01:04:41.400]   way to look at all of this. Now, I wonder if you fast forward into the future if you've got--
[01:04:41.400 --> 01:04:47.320]   Because with open source, there's a liability issue, especially if you give these jobs that are
[01:04:47.320 --> 01:04:55.160]   more complicated. So there's a quality assurance aspect to this that you could then build.
[01:04:55.160 --> 01:05:00.040]   I'm just picturing in the future world, and I'm like, "Oh, man, I need to chatbot to answer my phone
[01:05:00.040 --> 01:05:05.400]   while I'm reporting or editing or something, right?" And to evaluate if it's a good story.
[01:05:05.400 --> 01:05:10.680]   So who am I going to hire? What company has the best chatbot for that to mimic my voice,
[01:05:10.680 --> 01:05:14.200]   to see if a story is interesting and worth taking a note on or interrupting the call?
[01:05:14.200 --> 01:05:18.440]   So you have Tesla doing full self-driving, but then a guy named George Hott's
[01:05:18.440 --> 01:05:19.800]   creating a--
[01:05:19.800 --> 01:05:20.440]   Wait, no, they're not.
[01:05:20.440 --> 01:05:21.800]   Pardon me?
[01:05:21.800 --> 01:05:24.360]   They're not doing full self-driving.
[01:05:24.360 --> 01:05:25.240]   Well, they're attempting.
[01:05:25.240 --> 01:05:26.600]   But then there's a guy--
[01:05:26.600 --> 01:05:29.640]   Oh, I thought you were saying that it was real. Sorry, I was like, no, it's not.
[01:05:30.120 --> 01:05:33.480]   Well, but this is the point is the big company attempting to do this.
[01:05:33.480 --> 01:05:36.760]   And then there's this guy, George Hott's, who's created a little company
[01:05:36.760 --> 01:05:39.880]   that has an open source self-driving--
[01:05:39.880 --> 01:05:44.120]   You just put their phone in your window, self-driving car system.
[01:05:44.120 --> 01:05:45.640]   Oh, God, it scares me.
[01:05:45.640 --> 01:05:47.320]   Well, I think it's at least as good.
[01:05:47.320 --> 01:05:51.560]   It's scary, but it's innovation, though.
[01:05:51.560 --> 01:05:55.720]   Yeah, and it's as least as good as full self-driving.
[01:05:55.720 --> 01:06:03.480]   George started comma AI he has left since, but George Hott's iPhone hacker and Elon Musk
[01:06:03.480 --> 01:06:04.200]   antagonist.
[01:06:04.200 --> 01:06:06.200]   Well, we like him.
[01:06:06.200 --> 01:06:07.880]   Yeah, that's what the Verge calls him.
[01:06:07.880 --> 01:06:11.480]   But yeah, he created this--
[01:06:11.480 --> 01:06:12.040]   Yeah, there he is.
[01:06:12.040 --> 01:06:15.640]   That's George or Geo Hott or Geo Hott's.
[01:06:15.640 --> 01:06:20.760]   He did this-- he unlocked the iPhone at the age of 17.
[01:06:20.760 --> 01:06:21.560]   He was a hacker.
[01:06:22.440 --> 01:06:26.120]   Got in trouble with Sony for unlocking the PlayStation 3 a couple of years later.
[01:06:26.120 --> 01:06:28.920]   And Musk tried to hire him.
[01:06:28.920 --> 01:06:34.680]   Then he said, "No, I'm not going to work for you, and I'm going to make a better autopilot than
[01:06:34.680 --> 01:06:35.160]   Tesla."
[01:06:35.160 --> 01:06:38.280]   Which, as far as I could tell, actually is true.
[01:06:38.280 --> 01:06:39.160]   Or suspect it.
[01:06:39.160 --> 01:06:39.400]   Yeah.
[01:06:39.400 --> 01:06:44.120]   So his new company will be called, I think, the Tiny Corporation.
[01:06:44.120 --> 01:06:49.320]   He says, "Under a thousand lines, under three people, three times faster than Pi Torch,"
[01:06:49.320 --> 01:06:52.280]   which is the Python library for machine learning.
[01:06:52.760 --> 01:06:55.320]   For smaller models, there's so much left on the table.
[01:06:55.320 --> 01:06:56.680]   So that's why--
[01:06:56.680 --> 01:06:57.800]   He's not just doing self-driving.
[01:06:57.800 --> 01:06:58.520]   He's doing models.
[01:06:58.520 --> 01:06:59.080]   He's doing models.
[01:06:59.080 --> 01:07:00.760]   That's why you want this to be--
[01:07:00.760 --> 01:07:01.240]   Yeah.
[01:07:01.240 --> 01:07:03.240]   It's shrunk down, made--
[01:07:03.240 --> 01:07:05.720]   But in Europe, the European regulation that's coming out
[01:07:05.720 --> 01:07:10.840]   is there's some discussion that it's going to ban open source versions.
[01:07:10.840 --> 01:07:11.640]   Yeah, that's a mistake.
[01:07:11.640 --> 01:07:12.200]   Okay.
[01:07:12.200 --> 01:07:13.480]   And then, and it's going to--
[01:07:13.480 --> 01:07:15.720]   Liability on every model that's created,
[01:07:15.720 --> 01:07:20.280]   every action that anybody then does with it is going to go back to the source model maker.
[01:07:20.280 --> 01:07:24.280]   And they give a reason why they want to ban the open source out of it.
[01:07:24.280 --> 01:07:26.040]   Because they don't-- because they want to be able to control.
[01:07:26.040 --> 01:07:28.280]   They don't want an AI extinction event.
[01:07:28.280 --> 01:07:30.440]   So to circle all the way around, now,
[01:07:30.440 --> 01:07:36.040]   think about what that statement really was is this stuff's scary.
[01:07:36.040 --> 01:07:37.160]   The boy is doing it.
[01:07:37.160 --> 01:07:40.040]   Can't let little guys do it because it's out of control.
[01:07:40.040 --> 01:07:40.600]   It's got to roll.
[01:07:40.600 --> 01:07:42.040]   Let us take care of it.
[01:07:42.040 --> 01:07:43.160]   So now we know--
[01:07:43.160 --> 01:07:44.040]   I mean, I don't--
[01:07:44.040 --> 01:07:47.000]   Now I'm no longer thinking that's the cynical view.
[01:07:47.000 --> 01:07:48.280]   Oh, no, it's the right view.
[01:07:48.280 --> 01:07:49.720]   That's the right view.
[01:07:49.720 --> 01:07:51.240]   Sad to say.
[01:07:51.240 --> 01:07:55.240]   All right, can we change the subject a little bit?
[01:07:55.240 --> 01:07:57.240]   Are you guys can vote if you want to go--
[01:07:57.240 --> 01:07:59.880]   No, I wanted to ask Ms. Stacey--
[01:07:59.880 --> 01:08:01.080]   Oh, you have a good article.
[01:08:01.080 --> 01:08:02.120]   Let me-- let's pull that up.
[01:08:02.120 --> 01:08:02.440]   Yeah.
[01:08:02.440 --> 01:08:02.520]   Yeah.
[01:08:02.520 --> 01:08:02.840]   Yeah.
[01:08:02.840 --> 01:08:07.960]   The headline is the Spectrum X Ethernet switch offers lossless,
[01:08:07.960 --> 01:08:14.120]   quote, transmission via new kind of congestion control, says NVIDIA.
[01:08:14.120 --> 01:08:17.320]   And when I saw that, the first question came with two questions.
[01:08:18.040 --> 01:08:21.400]   Does this even matter in number two?
[01:08:21.400 --> 01:08:24.920]   Why does it matter if it really does matter?
[01:08:24.920 --> 01:08:27.160]   By the way, I love it.
[01:08:27.160 --> 01:08:32.840]   Then NVIDIA names their chips after great female computer scientists of the ages.
[01:08:32.840 --> 01:08:34.200]   This is the Grace Hopper.
[01:08:34.200 --> 01:08:34.680]   Grace Hopper.
[01:08:34.680 --> 01:08:35.160]   Grace Hopper.
[01:08:35.160 --> 01:08:35.480]   Yeah.
[01:08:35.480 --> 01:08:38.920]   So this is the ZDNet article?
[01:08:38.920 --> 01:08:39.080]   Yeah.
[01:08:39.080 --> 01:08:40.360]   Spectrum X.
[01:08:40.360 --> 01:08:43.720]   Jetson Wong showed off the first iteration of the Spectrum X,
[01:08:43.720 --> 01:08:45.960]   the Spectrum 4 with 100 billion.
[01:08:45.960 --> 01:08:46.360]   Wow.
[01:08:46.840 --> 01:08:50.040]   100 billion transistors in a 90 millimeter squared die.
[01:08:50.040 --> 01:08:53.000]   And this is--
[01:08:53.000 --> 01:08:53.400]   This is--
[01:08:53.400 --> 01:08:54.600]   It's like ridiculous man.
[01:08:54.600 --> 01:08:55.400]   Oh, OK.
[01:08:55.400 --> 01:08:56.360]   This is fabric, I think.
[01:08:56.360 --> 01:08:57.240]   This is--
[01:08:57.240 --> 01:08:58.520]   This is the fabric issue.
[01:08:58.520 --> 01:08:58.840]   Yeah.
[01:08:58.840 --> 01:09:00.840]   It was like, this is a routing.
[01:09:00.840 --> 01:09:02.200]   This is basically--
[01:09:02.200 --> 01:09:07.480]   You got to move that data real fast if you're talking about parallel processing that much stuff.
[01:09:07.480 --> 01:09:08.760]   NVIDIA is very smart.
[01:09:08.760 --> 01:09:09.160]   It's awesome.
[01:09:09.160 --> 01:09:09.640]   Yeah, they--
[01:09:09.640 --> 01:09:10.200]   Which--
[01:09:10.200 --> 01:09:12.040]   They have pivoted.
[01:09:12.040 --> 01:09:14.760]   And so they were the gaming company,
[01:09:14.760 --> 01:09:17.720]   and then they saw the opportunity in self-driving vehicles.
[01:09:17.720 --> 01:09:20.040]   They do a lot of chips for cars.
[01:09:20.040 --> 01:09:23.000]   The part runs at 500 watts, y'all.
[01:09:23.000 --> 01:09:23.000]   That's a lot.
[01:09:23.000 --> 01:09:24.440]   That is so much power.
[01:09:24.440 --> 01:09:26.760]   It's for a networking operation.
[01:09:26.760 --> 01:09:27.320]   This isn't--
[01:09:27.320 --> 01:09:29.720]   Yeah, this is inside a super computer, basically.
[01:09:29.720 --> 01:09:34.120]   This is a switch using ethernet, which is more capacity.
[01:09:34.120 --> 01:09:35.960]   This is where photonics would actually help,
[01:09:35.960 --> 01:09:38.680]   because if you had a photonic thing, you wouldn't have to move--
[01:09:38.680 --> 01:09:39.480]   It wouldn't--
[01:09:39.480 --> 01:09:39.960]   It wouldn't be so.
[01:09:39.960 --> 01:09:41.320]   I don't think it would run as hot.
[01:09:41.320 --> 01:09:44.200]   And it would be faster, and you wouldn't have to have such smarts in there.
[01:09:44.200 --> 01:09:45.240]   But that's cool.
[01:09:45.240 --> 01:09:46.760]   Yeah, I think this is OK.
[01:09:46.760 --> 01:09:49.320]   You know, so Gens and Gave his first day in person keynote
[01:09:49.320 --> 01:09:51.080]   in a long time a couple of days ago.
[01:09:51.080 --> 01:09:52.680]   In fact, I wish we'd covered it.
[01:09:52.680 --> 01:09:54.600]   We just kind of jammed up with the keynotes.
[01:09:54.600 --> 01:09:55.000]   Yeah.
[01:09:55.000 --> 01:09:56.840]   But they're really firing it all so interesting.
[01:09:56.840 --> 01:09:57.800]   It's amazing.
[01:09:57.800 --> 01:09:59.640]   I was almost at Computex.
[01:09:59.640 --> 01:10:00.360]   Really?
[01:10:00.360 --> 01:10:01.080]   Oh.
[01:10:01.080 --> 01:10:03.000]   Yeah, I decided not to go last minute, though.
[01:10:03.000 --> 01:10:03.960]   We--
[01:10:03.960 --> 01:10:04.360]   I get an--
[01:10:04.360 --> 01:10:08.200]   You know, I get an invitation every year from one company or another.
[01:10:08.200 --> 01:10:11.480]   I think so did--
[01:10:11.480 --> 01:10:13.240]   I get them.
[01:10:13.240 --> 01:10:15.240]   Micah got one too, and you got one.
[01:10:15.240 --> 01:10:16.680]   I get a copy of it.
[01:10:16.680 --> 01:10:19.320]   I'm always like weird about going because I don't--
[01:10:19.320 --> 01:10:20.840]   It's like, well, I don't know.
[01:10:20.840 --> 01:10:22.760]   I don't want to take this free trip.
[01:10:22.760 --> 01:10:23.960]   It's like a junket.
[01:10:23.960 --> 01:10:27.080]   So Computex was a sponsor on my show.
[01:10:27.080 --> 01:10:28.840]   I should mention that last month.
[01:10:28.840 --> 01:10:29.400]   They were a sponsor.
[01:10:29.400 --> 01:10:30.200]   Well, that's a little different.
[01:10:30.200 --> 01:10:32.280]   Then they're a sponsor that's different, I think.
[01:10:32.280 --> 01:10:32.680]   No.
[01:10:32.680 --> 01:10:34.680]   Yeah, but I still would have had to pay for my own trip.
[01:10:34.680 --> 01:10:35.160]   I don't--
[01:10:35.160 --> 01:10:37.560]   Oh, there are a lot of companies that want to give us free trips.
[01:10:37.560 --> 01:10:39.000]   I always turn it down.
[01:10:39.000 --> 01:10:39.560]   Miss Stacy?
[01:10:39.560 --> 01:10:40.600]   Yeah, I never take a free trip.
[01:10:40.600 --> 01:10:41.240]   Yeah.
[01:10:41.240 --> 01:10:45.640]   He just said that NVIDIA was quote a game in company,
[01:10:45.640 --> 01:10:48.840]   and now they're pivoting into the AI side of things.
[01:10:48.840 --> 01:10:51.640]   And it made me think about AMD.
[01:10:51.640 --> 01:10:56.760]   AMD has had its issues with its rise in performance here and there.
[01:10:56.760 --> 01:11:02.280]   But is there a way for them to get into the AI side of things at some point?
[01:11:02.280 --> 01:11:05.800]   They do have-- I mean, they do have chips that are used in AI.
[01:11:05.800 --> 01:11:10.280]   So, NVIDIA is not the only company.
[01:11:10.280 --> 01:11:14.200]   Like, people do use AMD chips for training and whatnot.
[01:11:14.200 --> 01:11:17.320]   They don't have as great of a marketing department.
[01:11:17.320 --> 01:11:20.440]   They don't have a CEO's charismatic and a black leather jacket.
[01:11:20.440 --> 01:11:22.920]   She's a cool CEO, though.
[01:11:22.920 --> 01:11:24.840]   Lisa Sue?
[01:11:24.840 --> 01:11:27.000]   Oh, yeah, she's not, but she's not--
[01:11:27.000 --> 01:11:29.720]   I've met both of them.
[01:11:29.720 --> 01:11:36.440]   You know when you meet Jensen, and he comes on real strong.
[01:11:36.440 --> 01:11:40.600]   And Lisa Sue, you know that this is a really smart,
[01:11:40.600 --> 01:11:43.720]   sharp person who is going to-- like, you don't want to mess with.
[01:11:43.720 --> 01:11:45.080]   You don't want to disappoint Lisa Sue.
[01:11:45.080 --> 01:11:53.400]   But they're not the same from a marketing or storytelling perspective.
[01:11:53.400 --> 01:12:00.040]   But real fast, with this switch, I should know, what NVIDIA is doing is they're actually
[01:12:00.040 --> 01:12:03.960]   creating underlying hardware that is the full cloud product.
[01:12:03.960 --> 01:12:07.720]   Right? So, when he's talking about two data centers and creating an AI data center,
[01:12:07.720 --> 01:12:09.480]   NVIDIA is creating that AI data center.
[01:12:09.480 --> 01:12:12.280]   So, if I'm Amazon, I'd be looking out at this and going--
[01:12:12.280 --> 01:12:12.840]   Oh, kidding.
[01:12:12.840 --> 01:12:15.320]   What about like AWS?
[01:12:15.320 --> 01:12:16.280]   Interesting.
[01:12:16.280 --> 01:12:19.480]   So, I just thought I'd share that with you all, because y'all would think that's nice.
[01:12:19.480 --> 01:12:22.600]   Yeah, they unveiled a supercomputer platform, basically.
[01:12:22.600 --> 01:12:27.080]   In fact, at the end of the keynote, Jensen Huang says, "It's too much.
[01:12:27.080 --> 01:12:28.360]   I know it's too much."
[01:12:28.360 --> 01:12:31.560]   That's crazy.
[01:12:32.280 --> 01:12:36.760]   Sales forecast for the current quarter, $4 billion above analyst estimates
[01:12:36.760 --> 01:12:43.960]   that put NVIDIA into the trillion dollar company club, which is a very small club.
[01:12:43.960 --> 01:12:47.000]   Good for them.
[01:12:47.000 --> 01:12:50.840]   I've been saying for some time that NVIDIA is very impressive, because they are firing
[01:12:50.840 --> 01:12:51.720]   on all cylinders.
[01:12:51.720 --> 01:12:54.120]   It's gaming crypto for a long time.
[01:12:54.120 --> 01:13:00.040]   But they were lucky, because as crypto fell off the map, AI came on and replaced that one.
[01:13:00.040 --> 01:13:00.600]   Yeah.
[01:13:00.600 --> 01:13:01.080]   Yeah.
[01:13:01.080 --> 01:13:07.400]   And they also way back in 2008, when I went out and visited them,
[01:13:07.400 --> 01:13:09.800]   and they had, I called it a sexy processor.
[01:13:09.800 --> 01:13:11.000]   It was a mobile processor.
[01:13:11.000 --> 01:13:14.680]   They basically have been trying to push parallel gaming style processors.
[01:13:14.680 --> 01:13:15.160]   Yeah.
[01:13:15.160 --> 01:13:17.000]   It was a set room on the mobile devices.
[01:13:17.000 --> 01:13:17.560]   Yeah.
[01:13:17.560 --> 01:13:19.800]   I have a Tegra in my NVIDIA Shield.
[01:13:19.800 --> 01:13:20.520]   Yeah.
[01:13:20.520 --> 01:13:25.240]   That thing is six, seven years old, and it's as fast as I'm on about Tegra.
[01:13:25.240 --> 01:13:25.720]   Yeah.
[01:13:25.720 --> 01:13:31.160]   But they've been pushing this style of computing since 2006.
[01:13:31.160 --> 01:13:32.840]   They're like, "What do we got?
[01:13:32.840 --> 01:13:35.160]   We got the world's best hammer.
[01:13:35.160 --> 01:13:36.600]   We're going to put this hammer.
[01:13:36.600 --> 01:13:37.720]   Everything needs a hammer."
[01:13:37.720 --> 01:13:38.040]   Right.
[01:13:38.040 --> 01:13:39.240]   But they were right.
[01:13:39.240 --> 01:13:43.400]   And graphics, you don't bet against graphics.
[01:13:43.400 --> 01:13:47.960]   And on the phone, and then they were like, "We're going to put it into laptops,"
[01:13:47.960 --> 01:13:50.840]   because that was the available computing infrastructure they had.
[01:13:50.840 --> 01:13:54.200]   They were like, "Corporate presentations are going to need better graphics processors.
[01:13:54.200 --> 01:13:54.920]   Look at slideshows.
[01:13:54.920 --> 01:13:55.400]   Look at this.
[01:13:55.400 --> 01:13:56.440]   Look at YouTube."
[01:13:56.440 --> 01:14:01.000]   And then they were like, "Oh, we're going to get a general at one point in time just because crypto
[01:14:01.000 --> 01:14:05.400]   died off, you still needed something that's going to process red 8K footage."
[01:14:05.400 --> 01:14:06.120]   Yeah.
[01:14:06.120 --> 01:14:10.760]   They're smart because they have had these inroads into all these different markets.
[01:14:10.760 --> 01:14:13.080]   Then it doesn't matter if crypto goes away.
[01:14:13.080 --> 01:14:15.080]   They're not leaving the gaming market either.
[01:14:15.080 --> 01:14:20.680]   The other thing Huang announced is a special AI for non-player characters.
[01:14:20.680 --> 01:14:21.640]   Oh boy.
[01:14:21.640 --> 01:14:22.760]   They call Ace.
[01:14:23.320 --> 01:14:25.320]   They're going to give it to gaming companies.
[01:14:25.320 --> 01:14:30.200]   In fact, the first one is a Santa Clara, California company that will use it to
[01:14:30.200 --> 01:14:37.320]   create non-player characters in their games that hear what you say and respond by it.
[01:14:37.320 --> 01:14:38.360]   Oh, dude, I'm not cute.
[01:14:38.360 --> 01:14:39.560]   You're the moral panic.
[01:14:39.560 --> 01:14:40.680]   That's coming up next.
[01:14:40.680 --> 01:14:47.720]   NVIDIA Ace will listen to what the gamer says to a character converted into text,
[01:14:47.720 --> 01:14:53.080]   then dump that into a generative AI program to create a more natural off the cuff response.
[01:14:53.720 --> 01:14:55.560]   They're not giving up on gaming either.
[01:14:55.560 --> 01:14:56.680]   That's awesome.
[01:14:56.680 --> 01:15:00.600]   And of course, they have the best graphics cars right now on the market, right?
[01:15:00.600 --> 01:15:03.640]   The Fortnite's with ray tracing.
[01:15:03.640 --> 01:15:10.760]   And this is why Intel tried to do graphics with Laraby and failed, and then they made that...
[01:15:10.760 --> 01:15:14.520]   They have a low-end video card now Intel does.
[01:15:14.520 --> 01:15:15.160]   It's not bad.
[01:15:15.160 --> 01:15:16.120]   Yeah, finally.
[01:15:16.120 --> 01:15:16.920]   It's low cost.
[01:15:16.920 --> 01:15:17.640]   No, it's better than...
[01:15:17.640 --> 01:15:19.240]   It was at the Ace.
[01:15:19.240 --> 01:15:20.120]   I can't remember what they call it.
[01:15:20.120 --> 01:15:20.680]   No, Intel.
[01:15:20.680 --> 01:15:21.000]   I can't remember.
[01:15:21.000 --> 01:15:22.280]   GPU.
[01:15:22.280 --> 01:15:23.160]   I can't remember the new one.
[01:15:23.160 --> 01:15:31.080]   If you were in the chip world, you saw the need for this arc way back in the day,
[01:15:31.080 --> 01:15:32.760]   and they were trying.
[01:15:32.760 --> 01:15:33.720]   They tried so hard.
[01:15:33.720 --> 01:15:36.680]   Oh, remember they tried mobile and they were such a failure.
[01:15:36.680 --> 01:15:38.680]   They ended up selling off the division.
[01:15:38.680 --> 01:15:40.680]   I don't have high-low-street.
[01:15:40.680 --> 01:15:41.240]   And now they're going to factories.
[01:15:41.240 --> 01:15:42.120]   Yeah.
[01:15:42.120 --> 01:15:47.240]   They started out and they kept their fabs the whole time,
[01:15:47.240 --> 01:15:51.080]   and they were really innovative until they got Brian West's name.
[01:15:51.800 --> 01:15:52.600]   Cresinich?
[01:15:52.600 --> 01:15:53.320]   Cresinich.
[01:15:53.320 --> 01:15:55.080]   Now they get packed.
[01:15:55.080 --> 01:15:56.360]   He was terrible for Intel.
[01:15:56.360 --> 01:15:56.760]   Although...
[01:15:56.760 --> 01:15:57.800]   Gelsinger's...
[01:15:57.800 --> 01:16:00.200]   There's like...
[01:16:00.200 --> 01:16:03.080]   We were talking about this on Windows Weekly.
[01:16:03.080 --> 01:16:06.200]   There's a little trouble at the top apparently.
[01:16:06.200 --> 01:16:06.760]   Intel.
[01:16:06.760 --> 01:16:10.440]   Serious leadership issues.
[01:16:10.440 --> 01:16:12.360]   Once...
[01:16:12.360 --> 01:16:13.880]   Here's the story from the Wall Street Journal.
[01:16:13.880 --> 01:16:15.960]   Again, every time I read about tech in the Wall Street Journal,
[01:16:15.960 --> 01:16:17.480]   I always have to think, well,
[01:16:17.480 --> 01:16:19.240]   whose ox is being gored here.
[01:16:19.240 --> 01:16:22.920]   Once mighty Intel struggles to escape Mud Hole.
[01:16:22.920 --> 01:16:26.120]   My good...
[01:16:26.120 --> 01:16:26.520]   My good...
[01:16:26.520 --> 01:16:31.160]   My good hippopotamus or a dinosaur stuck in the La Brea tarp.
[01:16:31.160 --> 01:16:32.840]   Its rivals, such as Nvidia,
[01:16:32.840 --> 01:16:34.920]   have left the chip company far behind.
[01:16:34.920 --> 01:16:38.600]   CEO Pat Gelsinger aims to reverse firms' fortunes by,
[01:16:38.600 --> 01:16:42.440]   as you said, Stacey Vastli, expanding its factories.
[01:16:42.440 --> 01:16:46.520]   Gelsinger said,
[01:16:46.520 --> 01:16:48.680]   "We had serious issues in terms of leadership.
[01:16:48.680 --> 01:16:50.280]   People, methodology, etc."
[01:16:50.280 --> 01:16:52.920]   That's him throwing...
[01:16:52.920 --> 01:16:54.120]   No, that's him throwing...
[01:16:54.120 --> 01:16:54.760]   Throw in shade.
[01:16:54.760 --> 01:16:55.960]   Brian under the bus.
[01:16:55.960 --> 01:16:56.600]   Yeah.
[01:16:56.600 --> 01:16:57.960]   Yeah, because he was terrible.
[01:16:57.960 --> 01:17:01.640]   He had no sense of innovation.
[01:17:01.640 --> 01:17:04.440]   And then he was like,
[01:17:04.440 --> 01:17:05.240]   "We need to get to..."
[01:17:05.240 --> 01:17:07.800]   I don't remember if it was three nanometers or what.
[01:17:07.800 --> 01:17:08.600]   They were going for it.
[01:17:08.600 --> 01:17:09.400]   And then they were like,
[01:17:09.400 --> 01:17:10.120]   "Well, we're having trouble."
[01:17:10.120 --> 01:17:11.000]   He's like, "No, we're not!"
[01:17:11.000 --> 01:17:13.160]   And then just carried on.
[01:17:13.160 --> 01:17:16.680]   So that is a problem when you're dealing with actual physics.
[01:17:16.680 --> 01:17:18.200]   And then here's the problem,
[01:17:18.200 --> 01:17:20.360]   speaking of physics with crashing.
[01:17:20.360 --> 01:17:23.800]   This is the stock performance of Nvidia.
[01:17:23.800 --> 01:17:25.480]   Oh, yeah.
[01:17:25.480 --> 01:17:26.040]   AMD.
[01:17:26.040 --> 01:17:26.520]   Oh, yeah.
[01:17:26.520 --> 01:17:26.520]   AMD.
[01:17:26.520 --> 01:17:27.320]   That's right.
[01:17:27.320 --> 01:17:27.960]   Into...
[01:17:27.960 --> 01:17:30.680]   What happens to Intel?
[01:17:30.680 --> 01:17:31.960]   Does it die?
[01:17:31.960 --> 01:17:33.080]   Is it a scenario where it...
[01:17:33.080 --> 01:17:34.040]   No, no, no.
[01:17:34.040 --> 01:17:35.720]   Well, eventually it could.
[01:17:35.720 --> 01:17:36.680]   I mean, Gelsinger says,
[01:17:36.680 --> 01:17:38.200]   "We're trying to turn into a service business."
[01:17:38.200 --> 01:17:39.480]   That's what the Foundry business is.
[01:17:39.480 --> 01:17:40.600]   We have no history of this...
[01:17:41.640 --> 01:17:49.080]   No, expertise at this.
[01:17:49.080 --> 01:17:49.800]   There are services.
[01:17:49.800 --> 01:17:51.560]   Well, no, IBM has...
[01:17:51.560 --> 01:17:52.040]   IBM has...
[01:17:52.040 --> 01:17:54.280]   I'm so sorry, y'all.
[01:17:54.280 --> 01:17:55.800]   I love IBM's research division,
[01:17:55.800 --> 01:17:57.880]   but IBM has nothing going for it right now.
[01:17:57.880 --> 01:17:59.720]   But it's still limping along,
[01:17:59.720 --> 01:18:01.240]   and it will for quite some time.
[01:18:01.240 --> 01:18:02.760]   Devorak always said that the only thing
[01:18:02.760 --> 01:18:05.880]   keeping IBM alive is it has a lot of patents.
[01:18:05.880 --> 01:18:07.960]   But I suspect those patents are expiring.
[01:18:07.960 --> 01:18:10.440]   They can't be that much use these days.
[01:18:10.920 --> 01:18:14.280]   So maybe that might be what ends up putting them under.
[01:18:14.280 --> 01:18:18.840]   Anyway, Intel's getting eaten by TSMC,
[01:18:18.840 --> 01:18:23.000]   a lot of other companies in video.
[01:18:23.000 --> 01:18:25.720]   Interesting article.
[01:18:25.720 --> 01:18:28.760]   I like the tarpit analogy.
[01:18:28.760 --> 01:18:33.880]   Gelsinger's growth plan is rooted in the expectation
[01:18:33.880 --> 01:18:36.040]   that the chip demand will come roaring back.
[01:18:36.040 --> 01:18:39.080]   That's part of the problem too, isn't it?
[01:18:39.080 --> 01:18:42.040]   The demand is falling off the face of the earth.
[01:18:42.040 --> 01:18:43.320]   People aren't buying PCs.
[01:18:43.320 --> 01:18:44.600]   PC sales are down to...
[01:18:44.600 --> 01:18:45.800]   No, no, no, no, no, no, no, 30, 40 percent.
[01:18:45.800 --> 01:18:46.280]   Way down.
[01:18:46.280 --> 01:18:46.840]   Don't have to.
[01:18:46.840 --> 01:18:52.680]   Intel's issue is they threw everything at PCs and servers.
[01:18:52.680 --> 01:18:56.520]   And now, they're still doing great in servers,
[01:18:56.520 --> 01:19:01.080]   but all of our new server monolithic jobs are AI.
[01:19:01.080 --> 01:19:04.360]   Like, that is the big driver of Compute demand, AI and crypto.
[01:19:05.480 --> 01:19:09.640]   And they don't have their automotive stuff.
[01:19:09.640 --> 01:19:10.920]   They have stuff that runs in there.
[01:19:10.920 --> 01:19:14.440]   But the rest of the world is consuming chips that aren't Intel
[01:19:14.440 --> 01:19:17.240]   and will be consuming chips that aren't going to be Intel chips forever.
[01:19:17.240 --> 01:19:19.640]   So they got to make up.
[01:19:19.640 --> 01:19:22.840]   Jesus, I just wasn't going to make a PDP joke, a deck joke.
[01:19:22.840 --> 01:19:27.720]   The PDP 11 came out 53 years ago.
[01:19:27.720 --> 01:19:27.960]   Yeah.
[01:19:27.960 --> 01:19:31.880]   I have a PDP...
[01:19:33.480 --> 01:19:37.640]   I think it's an 8, 9 or 10, but it's really a Raspberry Pi.
[01:19:37.640 --> 01:19:39.800]   It's actually a little faster.
[01:19:39.800 --> 01:19:44.120]   Faster than the original running in my office.
[01:19:44.120 --> 01:19:51.080]   Coming up in just a bit is cybersecurity and unsolvable problem.
[01:19:51.080 --> 01:19:54.920]   That was quite an awesome Nvidia segment.
[01:19:54.920 --> 01:19:55.640]   Yeah, thank you.
[01:19:55.640 --> 01:19:56.200]   Quite a stacy.
[01:19:56.200 --> 01:19:58.200]   That was a very great chippy.
[01:19:58.200 --> 01:19:59.080]   It was very chippy.
[01:19:59.080 --> 01:19:59.480]   Very chippy.
[01:19:59.480 --> 01:19:59.720]   Very chippy.
[01:19:59.720 --> 01:19:59.800]   Very chippy.
[01:19:59.800 --> 01:20:00.920]   Stacy, thank you.
[01:20:00.920 --> 01:20:03.640]   Now we're going to talk about the philosophy and cybersecurity.
[01:20:03.640 --> 01:20:06.040]   Scott Shapiro's new book, which by the way is great.
[01:20:06.040 --> 01:20:07.560]   Fancy Bear Goes Fishing.
[01:20:07.560 --> 01:20:10.520]   I'll tell you a little story from that book.
[01:20:10.520 --> 01:20:12.120]   I've been reading it.
[01:20:12.120 --> 01:20:18.280]   But he also proposes that you'll never have a secure computer.
[01:20:18.280 --> 01:20:20.680]   And we'll talk about that when we come back just a bit.
[01:20:20.680 --> 01:20:23.720]   First, a plea, an encouragement.
[01:20:23.720 --> 01:20:28.280]   Help me with this, Ant, because you're the community manager
[01:20:28.920 --> 01:20:30.680]   of our great Twit Club.
[01:20:30.680 --> 01:20:32.840]   Club Twit Daddy, sir.
[01:20:32.840 --> 01:20:36.440]   This was something Lisa came up with a couple of years ago.
[01:20:36.440 --> 01:20:41.080]   Her thinking being, we would like to be supported by our audience.
[01:20:41.080 --> 01:20:42.680]   We're an ad supported network.
[01:20:42.680 --> 01:20:43.400]   I like that.
[01:20:43.400 --> 01:20:45.640]   I like that we can give shows away for free.
[01:20:45.640 --> 01:20:49.960]   That even if you have not a penny, two pennies to rub together,
[01:20:49.960 --> 01:20:53.000]   you can listen to our shows and we want that to continue.
[01:20:53.000 --> 01:20:56.920]   But at the same time, advertising dollars are going way down.
[01:20:56.920 --> 01:20:57.800]   It's not just us.
[01:20:57.800 --> 01:20:59.320]   It's the end of the story.
[01:20:59.320 --> 01:20:59.960]   Across the board.
[01:20:59.960 --> 01:21:04.600]   And we think the club is really our future.
[01:21:04.600 --> 01:21:07.000]   So I want to invite you to join Club Twit.
[01:21:07.000 --> 01:21:08.520]   I think it's a very good deal.
[01:21:08.520 --> 01:21:12.120]   Lisa really wanted to make sure that you got value for dollar.
[01:21:12.120 --> 01:21:18.040]   So primarily the first benefit, and we thought this would be the big one, is no more ads.
[01:21:18.040 --> 01:21:24.440]   You get all the shows we do completely ad-free, no trackers in there, no ads.
[01:21:24.440 --> 01:21:27.000]   It's yours and you get to listen to it.
[01:21:27.000 --> 01:21:29.400]   That's I think a great benefit.
[01:21:29.400 --> 01:21:31.160]   Turned out that wasn't the only benefit.
[01:21:31.160 --> 01:21:33.800]   In fact, my mind, maybe not even the best benefit.
[01:21:33.800 --> 01:21:35.240]   We have this great discord.
[01:21:35.240 --> 01:21:37.400]   The magical discord server.
[01:21:37.400 --> 01:21:39.720]   Twit's always had this amazing conversations.
[01:21:39.720 --> 01:21:41.160]   Always had an amazing community.
[01:21:41.160 --> 01:21:44.680]   And this is really a place for the community to get together.
[01:21:44.680 --> 01:21:45.640]   We have our forums.
[01:21:45.640 --> 01:21:47.160]   We have our masks on instance.
[01:21:47.160 --> 01:21:48.920]   But this is where club members get together.
[01:21:48.920 --> 01:21:50.920]   Not only to talk about the shows.
[01:21:50.920 --> 01:21:55.640]   We have a club twit chat right now going on this week in Google.
[01:21:55.640 --> 01:21:59.240]   But also about all of the subjects people are interested in.
[01:21:59.240 --> 01:22:01.960]   Geeks particularly, there's a whole AI section.
[01:22:01.960 --> 01:22:07.640]   Anime, autos, coding and comics, food and fitness, movies and music.
[01:22:07.640 --> 01:22:08.920]   Music and sports and there.
[01:22:08.920 --> 01:22:10.760]   There's sports talk in our discord.
[01:22:10.760 --> 01:22:11.720]   Did you know that, sir?
[01:22:11.720 --> 01:22:13.320]   Sports and sports.
[01:22:13.320 --> 01:22:14.520]   Yes, geeks.
[01:22:14.520 --> 01:22:15.800]   Talkers, sports.
[01:22:15.800 --> 01:22:16.280]   Wow.
[01:22:16.280 --> 01:22:17.880]   I bet there's a lot of pickleball going on.
[01:22:17.880 --> 01:22:18.360]   No, no, no.
[01:22:18.360 --> 01:22:20.680]   It's been NBA. There's been F1.
[01:22:20.680 --> 01:22:22.280]   I mean, they go added in this.
[01:22:22.280 --> 01:22:23.640]   Right now, queso.
[01:22:23.640 --> 01:22:24.520]   Far more important.
[01:22:24.520 --> 01:22:26.280]   Queso is a queso chat.
[01:22:26.280 --> 01:22:29.720]   I don't think it has this dedicated queso channel.
[01:22:29.720 --> 01:22:33.240]   But anyway, we could get one because ants in charge.
[01:22:33.240 --> 01:22:34.840]   So that's the second benefit.
[01:22:34.840 --> 01:22:37.160]   That free shows the discord.
[01:22:37.160 --> 01:22:39.720]   Not everybody who joins club twit goes in the discord.
[01:22:39.720 --> 01:22:41.240]   No, I really encourage you to.
[01:22:41.240 --> 01:22:43.640]   I think sometimes it's a little intimidating to people.
[01:22:43.640 --> 01:22:44.840]   You go, I don't get it.
[01:22:44.840 --> 01:22:45.880]   But you will be welcome.
[01:22:45.880 --> 01:22:47.880]   And it really is a great way to communicate.
[01:22:47.880 --> 01:22:49.400]   Can I ask a question here, Ant?
[01:22:49.400 --> 01:22:49.720]   Sure.
[01:22:49.720 --> 01:22:51.960]   Will the whole drama around Leo's hair
[01:22:51.960 --> 01:22:54.600]   be the kind of thing that just club members will get to watch?
[01:22:54.600 --> 01:22:55.800]   I guarantee it.
[01:22:55.800 --> 01:22:57.240]   So, okay.
[01:22:57.240 --> 01:22:57.880]   That's the other thing.
[01:22:57.880 --> 01:22:58.840]   We don't want to miss that.
[01:22:58.840 --> 01:23:02.280]   We have special content only for club members.
[01:23:02.280 --> 01:23:05.960]   It's the twit plus feed that we offer to all club members.
[01:23:05.960 --> 01:23:08.920]   So, yeah, it's stuff like the things that happen before the show
[01:23:08.920 --> 01:23:10.680]   and after the show, the bloopers.
[01:23:10.680 --> 01:23:13.720]   And when we stop the show because there's noise next door.
[01:23:13.720 --> 01:23:15.560]   Yeah, it's part of the fun of it.
[01:23:15.560 --> 01:23:17.240]   If you're watching live, you get all of that.
[01:23:17.240 --> 01:23:18.920]   But not everybody can watch live.
[01:23:18.920 --> 01:23:20.520]   So, we excerpt some of that stuff.
[01:23:20.520 --> 01:23:23.640]   But we also are able, because we have revenue from the club,
[01:23:23.640 --> 01:23:27.240]   to you, I mean, a lot of what that club revenue goes towards developing new shows,
[01:23:27.240 --> 01:23:30.280]   we don't want to bring you new content.
[01:23:30.280 --> 01:23:33.560]   So, that's why there's Michael Sargent's hands on Mac and Tasha in there,
[01:23:33.560 --> 01:23:36.600]   Paul Therott's hands on Windows, the untitled Linux show,
[01:23:36.600 --> 01:23:39.000]   the Gizfiz Stacey's book club every other month.
[01:23:39.000 --> 01:23:40.680]   This weekend space started there.
[01:23:40.680 --> 01:23:42.680]   Well, and we can launch shows out of there.
[01:23:42.680 --> 01:23:44.840]   Quite literally, this weekend space started.
[01:23:44.840 --> 01:23:45.400]   So, it's really.
[01:23:45.400 --> 01:23:47.720]   Subsidized by the club, but as it built an audience,
[01:23:47.720 --> 01:23:49.320]   we were able to put it out in public.
[01:23:49.320 --> 01:23:52.920]   Right now, in the club, Home Theater Geeks with Scott Wilkinson,
[01:23:52.920 --> 01:23:56.040]   that was a show we produced for a long time,
[01:23:56.040 --> 01:23:59.400]   couldn't develop an audience for or an advertising base for.
[01:23:59.400 --> 01:24:02.920]   But it had a good audience, it just wasn't as big as it needed to be.
[01:24:02.920 --> 01:24:04.760]   So, we're doing it in the club.
[01:24:04.760 --> 01:24:08.200]   And that's the other benefit is that money.
[01:24:08.200 --> 01:24:10.120]   And I promise you, that's where that money goes.
[01:24:10.120 --> 01:24:13.080]   It goes to keeping the lights on, keeping the staff employed,
[01:24:13.080 --> 01:24:14.840]   developing new programming.
[01:24:14.840 --> 01:24:15.800]   It's really important to us.
[01:24:15.800 --> 01:24:17.240]   We're now almost 8,000 strong.
[01:24:17.240 --> 01:24:17.800]   We're growing.
[01:24:17.800 --> 01:24:18.280]   It's awesome.
[01:24:18.280 --> 01:24:18.680]   It's awesome.
[01:24:18.680 --> 01:24:19.080]   We're growing.
[01:24:19.080 --> 01:24:20.200]   But I have a vision.
[01:24:20.200 --> 01:24:21.000]   I have a dream.
[01:24:21.000 --> 01:24:22.280]   Sorry.
[01:24:22.280 --> 01:24:23.080]   Wait a minute.
[01:24:23.080 --> 01:24:27.240]   I've been to the mountaintop.
[01:24:27.240 --> 01:24:31.080]   I have a vision, let's say that, of getting, I would like,
[01:24:31.080 --> 01:24:33.560]   we right now, it's a little more than 1% of everybody
[01:24:33.560 --> 01:24:35.560]   listening to our shows joins the club.
[01:24:35.560 --> 01:24:38.600]   I don't ever expect it to be half even.
[01:24:38.600 --> 01:24:39.960]   I'd like to see it be 5%.
[01:24:40.680 --> 01:24:43.320]   That would be, which is public radio territory.
[01:24:43.320 --> 01:24:43.800]   Yeah.
[01:24:43.800 --> 01:24:45.240]   6% to 12% public radio.
[01:24:45.240 --> 01:24:47.320]   I don't want to have to beg as much as public radio.
[01:24:47.320 --> 01:24:51.160]   We're as generous as those tote bag, toteen.
[01:24:51.160 --> 01:24:51.400]   Yeah.
[01:24:51.400 --> 01:24:53.240]   We don't give you tote bags.
[01:24:53.240 --> 01:24:54.760]   Proud, eating, eating anymore.
[01:24:54.760 --> 01:24:55.880]   Latte sipping.
[01:24:55.880 --> 01:24:58.760]   Public radio people, aren't we?
[01:24:58.760 --> 01:25:00.200]   I think, well, see, one of the things,
[01:25:00.200 --> 01:25:04.760]   we are very much more a specific narrow cast niche of technology.
[01:25:04.760 --> 01:25:06.600]   I think technology is really important.
[01:25:06.600 --> 01:25:08.440]   I think if you want to know what's happening,
[01:25:08.440 --> 01:25:12.040]   we are going to be a great resource for you.
[01:25:12.040 --> 01:25:13.960]   I came up this morning with a new tagline.
[01:25:13.960 --> 01:25:16.600]   "Need."
[01:25:16.600 --> 01:25:17.400]   What did I write?
[01:25:17.400 --> 01:25:18.440]   Oh, I thought it was so good.
[01:25:18.440 --> 01:25:19.400]   Netcast you.
[01:25:19.400 --> 01:25:20.440]   No, no, no, no.
[01:25:20.440 --> 01:25:22.760]   Because Lisa said, we need a quick one.
[01:25:22.760 --> 01:25:23.400]   A-D-D.
[01:25:23.400 --> 01:25:23.800]   I see.
[01:25:23.800 --> 01:25:27.480]   The home for A-D-D.
[01:25:27.480 --> 01:25:28.280]   That's us.
[01:25:28.280 --> 01:25:30.600]   Need to know what's next?
[01:25:30.600 --> 01:25:32.040]   Tune in, twit.
[01:25:32.040 --> 01:25:32.600]   How about that?
[01:25:32.600 --> 01:25:33.720]   Turn on, twit.
[01:25:33.720 --> 01:25:34.600]   Something like that.
[01:25:34.600 --> 01:25:37.320]   This is where you see what's happening today.
[01:25:37.320 --> 01:25:39.400]   What it means for you in the future.
[01:25:39.400 --> 01:25:43.640]   And I think we really do a good, sensible, even-handed job.
[01:25:43.640 --> 01:25:47.000]   And more importantly, informed job with all of the experts we have on the show.
[01:25:47.000 --> 01:25:49.320]   This show and every show we do.
[01:25:49.320 --> 01:25:50.760]   I think we're an important thing.
[01:25:50.760 --> 01:25:54.280]   I want to keep doing the club.
[01:25:54.280 --> 01:25:55.560]   We're an important thing.
[01:25:55.560 --> 01:25:56.600]   We're an important thing.
[01:25:56.600 --> 01:25:57.720]   That's a good slogan.
[01:25:57.720 --> 01:26:00.440]   We're not tuning in.
[01:26:00.440 --> 01:26:02.040]   We hear you.
[01:26:02.040 --> 01:26:06.920]   So if you would help us out, that would be great.
[01:26:07.800 --> 01:26:09.000]   I hate to beg.
[01:26:09.000 --> 01:26:11.480]   I'm terrible at doing this and I don't like to do it.
[01:26:11.480 --> 01:26:13.080]   But this was a good big hit.
[01:26:13.080 --> 01:26:14.840]   I really think it's important.
[01:26:14.840 --> 01:26:17.560]   I believe in what we're doing and I want to keep doing what we're doing.
[01:26:17.560 --> 01:26:21.960]   And I think increasingly it's going to be up to the audience to make sure that that happens.
[01:26:21.960 --> 01:26:25.160]   Twit.tv/clubtwit.
[01:26:25.160 --> 01:26:31.320]   If you love our bags, you'll love Club Twit.
[01:26:31.320 --> 01:26:33.000]   That's all I can say.
[01:26:33.000 --> 01:26:35.160]   Thank you, Mandy, to clown.
[01:26:35.160 --> 01:26:35.800]   Yes, stuff.
[01:26:35.800 --> 01:26:36.600]   Mandy's great.
[01:26:36.600 --> 01:26:38.280]   We, you know, I love our community.
[01:26:38.280 --> 01:26:42.040]   And that's one of the reasons I like doing the club because I really love
[01:26:42.040 --> 01:26:43.320]   those people.
[01:26:43.320 --> 01:26:48.040]   I love it so much when it's not showtime and I'm saying something on
[01:26:48.040 --> 01:26:50.200]   Mastodon or Blue Sky or Twitter.
[01:26:50.200 --> 01:26:55.720]   And it's a Twit fan who's there with the reference, with the understanding, with the joke.
[01:26:55.720 --> 01:26:57.080]   Yeah, we get the joke.
[01:26:57.080 --> 01:26:57.480]   Yeah.
[01:26:57.480 --> 01:27:01.320]   It's just great to see that you folks everywhere.
[01:27:01.320 --> 01:27:02.280]   Community, baby.
[01:27:02.280 --> 01:27:03.160]   Community.
[01:27:03.160 --> 01:27:03.640]   Yeah.
[01:27:03.640 --> 01:27:04.040]   Yeah.
[01:27:04.040 --> 01:27:05.720]   Less low, more community.
[01:27:06.600 --> 01:27:07.880]   No, we love playing.
[01:27:07.880 --> 01:27:10.040]   Cheese.
[01:27:10.040 --> 01:27:10.280]   Yes.
[01:27:10.280 --> 01:27:11.480]   Cheese.
[01:27:11.480 --> 01:27:12.440]   No, I agree with you.
[01:27:12.440 --> 01:27:13.320]   No, I agree with you.
[01:27:13.320 --> 01:27:14.120]   Hard, hard.
[01:27:14.120 --> 01:27:14.600]   Hardest.
[01:27:14.600 --> 01:27:14.920]   I agree with you.
[01:27:14.920 --> 01:27:17.400]   Hard, hard, hard, and Higginbotham.
[01:27:17.400 --> 01:27:18.120]   No.
[01:27:18.120 --> 01:27:23.160]   All I am is the guy who got it started.
[01:27:23.160 --> 01:27:28.200]   At this point, I want to go on much, you know, beyond that.
[01:27:28.200 --> 01:27:29.880]   I don't have any attachments.
[01:27:29.880 --> 01:27:32.360]   It was like, I wanted to go on while I go on Crucines.
[01:27:32.360 --> 01:27:33.640]   Well, I go away.
[01:27:33.640 --> 01:27:34.440]   I go away.
[01:27:34.440 --> 01:27:35.160]   You go on.
[01:27:35.160 --> 01:27:36.200]   How about that?
[01:27:36.200 --> 01:27:37.240]   Thank you, Doug M.
[01:27:37.240 --> 01:27:38.440]   Doug M.
[01:27:38.440 --> 01:27:39.400]   What did Doug M say?
[01:27:39.400 --> 01:27:40.840]   Doug M's in Discord says,
[01:27:40.840 --> 01:27:44.840]   "The best $7 spent every month for the past two and a half years."
[01:27:44.840 --> 01:27:45.480]   Thank you, Doug M.
[01:27:45.480 --> 01:27:45.880]   Thank you.
[01:27:45.880 --> 01:27:46.120]   Wow.
[01:27:46.120 --> 01:27:47.560]   Has it been two and a half years?
[01:27:47.560 --> 01:27:48.120]   Thank you.
[01:27:48.120 --> 01:27:49.080]   It's more than two.
[01:27:49.080 --> 01:27:49.960]   We started it into it.
[01:27:49.960 --> 01:27:51.320]   I think, yeah, it's almost two and a half, yeah.
[01:27:51.320 --> 01:27:53.800]   Oh, and there's one other thing.
[01:27:53.800 --> 01:27:55.800]   If you subscribe, please tell your spouse.
[01:27:55.800 --> 01:27:56.280]   Yeah.
[01:27:56.280 --> 01:27:57.480]   We have family plans now.
[01:27:57.480 --> 01:27:59.320]   But do you know about the first time?
[01:27:59.320 --> 01:28:00.680]   Oh, yeah, there's an issue.
[01:28:00.680 --> 01:28:01.800]   Um, yeah.
[01:28:01.800 --> 01:28:04.200]   If you want to subscribe to Club Twit,
[01:28:04.200 --> 01:28:05.960]   make sure, you know, hey,
[01:28:05.960 --> 01:28:07.480]   I'm going to go ahead and subscribe to Club Twit.
[01:28:07.480 --> 01:28:08.840]   Hey, Queen Prude, I'm subscribing.
[01:28:08.840 --> 01:28:09.240]   Okay.
[01:28:09.240 --> 01:28:10.520]   Tell the spouse, um,
[01:28:10.520 --> 01:28:12.440]   because we had a guy subscribing, his wife said,
[01:28:12.440 --> 01:28:13.960]   "What is his fraudulent charge?"
[01:28:13.960 --> 01:28:18.760]   And it costs us time and money when you get a charge back.
[01:28:18.760 --> 01:28:19.160]   Yeah.
[01:28:19.160 --> 01:28:21.480]   And then actually, if you get enough charge back,
[01:28:21.480 --> 01:28:22.920]   Stripe starts looking funny.
[01:28:22.920 --> 01:28:23.240]   Yeah.
[01:28:23.240 --> 01:28:26.920]   So, please tell your family when you buy Twitter,
[01:28:26.920 --> 01:28:28.600]   tell whoever else looks at your checkbook.
[01:28:28.600 --> 01:28:31.560]   Oh, that weird charge, that's okay.
[01:28:31.560 --> 01:28:33.240]   It's, I promise it's okay.
[01:28:33.240 --> 01:28:33.800]   I promise.
[01:28:33.800 --> 01:28:35.800]   It's just Club Twit.
[01:28:35.800 --> 01:28:37.320]   Did the guy, did the guy resubscribe?
[01:28:37.320 --> 01:28:39.160]   Yes, they're good to go.
[01:28:39.160 --> 01:28:39.720]   Did they?
[01:28:39.720 --> 01:28:40.760]   Because we didn't know,
[01:28:40.760 --> 01:28:42.680]   we didn't know exactly what to do.
[01:28:42.680 --> 01:28:44.840]   I remember Lisa was going to go back and forth on what,
[01:28:44.840 --> 01:28:45.640]   how to handle this.
[01:28:45.640 --> 01:28:46.200]   They're good to go.
[01:28:46.200 --> 01:28:46.840]   They're good to go.
[01:28:46.840 --> 01:28:47.080]   Okay.
[01:28:47.080 --> 01:28:50.360]   Oh, boy, but that was funny.
[01:28:50.360 --> 01:28:52.360]   Joseph, free marriage counseling.
[01:28:52.360 --> 01:28:52.840]   Yeah.
[01:28:52.840 --> 01:28:53.800]   With your subscription.
[01:28:53.800 --> 01:28:54.440]   Yeah.
[01:28:54.440 --> 01:28:55.400]   I'll come to your door.
[01:28:55.400 --> 01:28:57.560]   So I started reading this and actually,
[01:28:57.560 --> 01:28:58.360]   I kind of threw,
[01:28:58.360 --> 01:28:59.800]   Lisa was there when I started it.
[01:28:59.800 --> 01:29:03.000]   I read it out loud and I kind of threw the Kindle across the room
[01:29:03.000 --> 01:29:05.000]   when I, oh boy, oh boy.
[01:29:05.000 --> 01:29:09.000]   Scott J. Zepiro, he is, he's been a developer.
[01:29:09.000 --> 01:29:11.400]   He, you know, he's not ignorant of technology.
[01:29:11.400 --> 01:29:14.600]   He's a technologist, but he's become a philosopher of law.
[01:29:14.600 --> 01:29:20.120]   But he did write a new book called Fancy Bear Goes Fishing,
[01:29:20.120 --> 01:29:22.280]   the dark history of the information age
[01:29:22.280 --> 01:29:24.600]   in five extraordinary hacks.
[01:29:24.600 --> 01:29:27.480]   It starts with the first worm ever,
[01:29:27.480 --> 01:29:31.880]   which you may remember happened in the,
[01:29:31.880 --> 01:29:37.240]   I think in the 80s with, oh, now I've forgotten his name.
[01:29:37.240 --> 01:29:40.040]   He went on to be a co-founder of Y Combinator.
[01:29:40.040 --> 01:29:45.400]   But he, the Morris worm, it was called Robert Tappen Morris, Jr.
[01:29:45.400 --> 01:29:49.800]   There is an excerpt which I really enjoyed
[01:29:49.800 --> 01:29:53.960]   of the chapter on Fancy Bear.
[01:29:53.960 --> 01:29:55.800]   Let me see where that is.
[01:29:55.800 --> 01:29:58.120]   Because that, I would recommend everybody read,
[01:29:58.120 --> 01:30:01.080]   because that's a, that is a wild story.
[01:30:01.960 --> 01:30:05.160]   But there's an interesting premise in this that I'm not sure.
[01:30:05.160 --> 01:30:06.440]   I don't know.
[01:30:06.440 --> 01:30:07.960]   I'm not sure I agree with it.
[01:30:07.960 --> 01:30:10.040]   And it's kind of, he's, you know, he's a philosopher.
[01:30:10.040 --> 01:30:13.880]   You know, philosophers like to argue from first principles
[01:30:13.880 --> 01:30:16.520]   and build up a story.
[01:30:16.520 --> 01:30:20.280]   The, the IEEE spectrum has the strange story
[01:30:20.280 --> 01:30:22.840]   of the teams behind the Mirai botnet.
[01:30:22.840 --> 01:30:25.800]   I would say this is well worth reading.
[01:30:25.800 --> 01:30:28.360]   This was a botnet that we talked about.
[01:30:28.360 --> 01:30:31.480]   And you remember Stacey, because it took over IoT devices.
[01:30:31.480 --> 01:30:35.320]   It took over routers and was a, it was a nightmare.
[01:30:35.320 --> 01:30:41.720]   It was all because a student at Rutgers didn't like the fact
[01:30:41.720 --> 01:30:47.640]   that upper class students got prior to enroll in computer science electives.
[01:30:47.640 --> 01:30:50.280]   He was pissed.
[01:30:50.280 --> 01:30:57.560]   So he didost the entire 40,000 bots, primarily in Eastern Europe and China,
[01:30:57.560 --> 01:31:00.360]   and leashed them on Rutgers computers.
[01:31:00.360 --> 01:31:04.520]   And his, so his classmates couldn't get through to register.
[01:31:04.520 --> 01:31:07.720]   He did it again the next semester.
[01:31:07.720 --> 01:31:12.920]   And in fact, he said, the reason I can do this is because Rutgers has a really inferior
[01:31:12.920 --> 01:31:14.920]   DDoS protection company.
[01:31:14.920 --> 01:31:16.120]   They should use mine.
[01:31:16.120 --> 01:31:18.120]   He has a DDoS.
[01:31:18.120 --> 01:31:19.000]   He graduated.
[01:31:19.000 --> 01:31:20.280]   Uh, he dropped out.
[01:31:20.280 --> 01:31:22.760]   Um, that's normal.
[01:31:22.760 --> 01:31:23.720]   Had that story goes.
[01:31:24.280 --> 01:31:28.600]   And was caught.
[01:31:28.600 --> 01:31:29.320]   He was from a fan would.
[01:31:29.320 --> 01:31:30.760]   I don't know where that is in New Jersey.
[01:31:30.760 --> 01:31:31.400]   Well, I know, of course.
[01:31:31.400 --> 01:31:31.720]   Yeah.
[01:31:31.720 --> 01:31:31.960]   Yeah.
[01:31:31.960 --> 01:31:32.280]   Right.
[01:31:32.280 --> 01:31:33.560]   On the train line in New York.
[01:31:33.560 --> 01:31:33.720]   Yeah.
[01:31:33.720 --> 01:31:33.720]   Yeah.
[01:31:33.720 --> 01:31:33.720]   Yeah.
[01:31:33.720 --> 01:31:38.520]   Uh, he was, uh, you know, kind of one of those kids, a lot of, uh,
[01:31:38.520 --> 01:31:45.240]   computer kids are like this who was super smart, but it was ADHD and didn't do well in school.
[01:31:45.240 --> 01:31:46.840]   Probably because he was too smart.
[01:31:46.840 --> 01:31:50.040]   Uh, but, uh, as usual, I got this too.
[01:31:50.040 --> 01:31:53.320]   When I was a kid, the teachers attributed it to laziness.
[01:31:53.320 --> 01:31:53.640]   Right.
[01:31:53.640 --> 01:31:54.440]   Apathy.
[01:31:54.440 --> 01:31:57.160]   You're not living up to your potential layout.
[01:31:57.160 --> 01:31:59.000]   Uh, didn't help.
[01:31:59.000 --> 01:32:02.520]   So he sought refuge in computers to himself out of code when he was 12.
[01:32:02.520 --> 01:32:09.240]   Um, he started playing Minecraft in ninth grade, started hosting servers.
[01:32:09.240 --> 01:32:11.160]   He was DDoSed.
[01:32:11.160 --> 01:32:12.920]   One of his Minecraft servers or DDoS.
[01:32:12.920 --> 01:32:15.880]   That's how he learned about distributed denial of service attacks.
[01:32:15.880 --> 01:32:21.960]   So he studied them, started writing them himself, gotten to Rutgers for computer science.
[01:32:23.080 --> 01:32:29.160]   Um, but ended up, as you know, uh, DDoSing Rutgers eventually got caught by the FBI because,
[01:32:29.160 --> 01:32:31.240]   uh, he's, it's spread.
[01:32:31.240 --> 01:32:35.400]   I mean, he basically was the first to create DDoS as a service.
[01:32:35.400 --> 01:32:39.240]   He was renting his DDoS capability to other people.
[01:32:39.240 --> 01:32:41.400]   Oh, said low impact.
[01:32:41.400 --> 01:32:44.360]   There was some, I'm sorry, go ahead.
[01:32:44.360 --> 01:32:48.600]   I just had a weird brain thought about DDoSing tools.
[01:32:48.600 --> 01:32:51.480]   There was one that had a popular name.
[01:32:52.120 --> 01:32:55.000]   Um, so he, he actually got in a fight.
[01:32:55.000 --> 01:33:00.760]   There was a gang fight between other DDoS gangs, lizard squad and VDoS.
[01:33:00.760 --> 01:33:04.040]   Uh, they created something called poodle corp.
[01:33:04.040 --> 01:33:11.400]   They could do 400 gigabits a second, which is a lot of bandwidth to throw at any, uh, server.
[01:33:11.400 --> 01:33:16.520]   Uh, so, uh, this kid decided I'm going to take down poodle corp.
[01:33:16.520 --> 01:33:20.680]   In any event, um, he got caught by the, uh, FBI.
[01:33:20.680 --> 01:33:27.160]   The FBI's Anchorage, Alaska and New Haven cyber units first shut down poodle corp.
[01:33:27.160 --> 01:33:33.480]   And, uh, and then the Mariah group who survived, they've been in this gang war,
[01:33:33.480 --> 01:33:39.720]   uh, started attacking and, uh, the special agent, uh, Peterson said, oh, now that he's
[01:33:39.720 --> 01:33:42.680]   taken down poodle corp, let's go after the Mariah botnet.
[01:33:42.680 --> 01:33:43.480]   They did catch him.
[01:33:43.480 --> 01:33:43.880]   Mm.
[01:33:43.880 --> 01:33:47.960]   Uh, but what's interesting is he did not serve time.
[01:33:47.960 --> 01:33:48.840]   He pled guilty.
[01:33:49.480 --> 01:33:54.600]   He was indicted twice once in New Jersey for his attack on Rutgers, once in Alaska for the
[01:33:54.600 --> 01:33:59.960]   Mariah botnet, uh, violations of the computer fraud and abuse act facing up to 10 years in
[01:33:59.960 --> 01:34:00.840]   federal prison.
[01:34:00.840 --> 01:34:07.240]   Uh, he pled guilty, expressed remorse for his actions.
[01:34:07.240 --> 01:34:13.320]   He apologized for the harm he'd caused businesses, Rutgers, the faculty, his fellow students.
[01:34:13.320 --> 01:34:16.360]   Doj said we're not going to ask for jail time.
[01:34:18.040 --> 01:34:20.520]   Uh, but maybe they could help us out.
[01:34:20.520 --> 01:34:25.320]   The government said, uh, we're going to give you five years probation,
[01:34:25.320 --> 01:34:29.480]   2,500 hours of community service, but that community service should include continued work
[01:34:29.480 --> 01:34:30.440]   with the FBI.
[01:34:30.440 --> 01:34:37.400]   They had already logged a thousand hours helping the FBI hunt and shut down Mirai copycats.
[01:34:37.400 --> 01:34:41.240]   They contributed to more than a dozen law enforcement and research efforts.
[01:34:41.240 --> 01:34:46.280]   In one instance, the founders of the Mirai botnet, there were three of them,
[01:34:46.280 --> 01:34:51.240]   Paras, Josiah and Dalton helped stopping the nation state, a nation state hacking group.
[01:34:51.240 --> 01:34:57.480]   They helped the FBI prevent DDoS attacks aimed at disrupting Christmas holiday shopping.
[01:34:57.480 --> 01:35:01.240]   No jail time, just community service.
[01:35:01.240 --> 01:35:04.120]   The most poignant, this is all from the book.
[01:35:04.120 --> 01:35:09.320]   Uh, the most poignant moments in the hearing were Paras and Dalton singling out for praise.
[01:35:09.320 --> 01:35:11.240]   The person who caught them.
[01:35:12.280 --> 01:35:17.320]   Paras, this Rutgers kid said two years ago when I first met special agent,
[01:35:17.320 --> 01:35:22.440]   Elliot Peterson, I was an arrogant fool believing that somehow I was untouchable.
[01:35:22.440 --> 01:35:26.520]   When I met him in person for the second time, he told me something you will never, I will never
[01:35:26.520 --> 01:35:28.840]   forget you're in a hole right now.
[01:35:28.840 --> 01:35:30.840]   It's time you stop digging.
[01:35:30.840 --> 01:35:33.080]   It's an inch.
[01:35:33.080 --> 01:35:34.200]   It's a great story.
[01:35:34.200 --> 01:35:37.560]   And as people who from the outside, we covered this with the security now.
[01:35:37.560 --> 01:35:39.000]   I think on this show too.
[01:35:39.000 --> 01:35:42.200]   And so watching this from the outside to actually read what's inside.
[01:35:42.200 --> 01:35:44.280]   But this book has more going on with it.
[01:35:44.280 --> 01:35:46.920]   And this is why I'm bringing it into the class today.
[01:35:46.920 --> 01:35:49.000]   Uh, because...
[01:35:49.000 --> 01:35:52.120]   Is it going to be on the final?
[01:35:52.120 --> 01:35:54.040]   I'd like to share this with the class.
[01:35:54.040 --> 01:35:55.080]   No, I'm not the professor.
[01:35:55.080 --> 01:35:56.680]   I'm just a student, my throw.
[01:35:56.680 --> 01:36:01.400]   Uh, there's an interview with him in Ars Technica.
[01:36:01.400 --> 01:36:05.960]   And basically his premise is because these are touring machines,
[01:36:05.960 --> 01:36:12.120]   you know, to, I'll let Stacy explain what a touring machine is named after Alan Turing.
[01:36:12.120 --> 01:36:16.760]   Who, by the way, described how computers should work before there even was an existing one.
[01:36:16.760 --> 01:36:19.080]   It was all a mental exercise for him, but he was right.
[01:36:19.080 --> 01:36:19.560]   Wow.
[01:36:19.560 --> 01:36:27.400]   Um, because they're touring machines and that any computer that you might use is general enough
[01:36:27.400 --> 01:36:32.680]   to be touring, what they call "turing complete", it's impossible to protect them.
[01:36:32.680 --> 01:36:38.520]   Any touring complete device will always be subject to hacking.
[01:36:41.880 --> 01:36:42.680]   Yes.
[01:36:42.680 --> 01:36:43.560]   That makes sense.
[01:36:43.560 --> 01:36:43.640]   Does that make sense?
[01:36:43.640 --> 01:36:44.360]   You think so?
[01:36:44.360 --> 01:36:44.600]   Yeah.
[01:36:44.600 --> 01:36:46.680]   Yes, it is impossible.
[01:36:46.680 --> 01:36:51.400]   It is impossible to have a secure computer.
[01:36:51.400 --> 01:36:55.000]   It is impossible to have true security.
[01:36:55.000 --> 01:37:00.040]   Because either you're going to be hacked, like electronically, physically,
[01:37:00.040 --> 01:37:04.200]   like via those channels, or you're going to be social engineered.
[01:37:04.200 --> 01:37:06.200]   Because most people are hackable.
[01:37:06.200 --> 01:37:08.920]   I don't know, my Chromebook.
[01:37:09.640 --> 01:37:11.640]   Pretty good.
[01:37:11.640 --> 01:37:14.600]   Chromebook's brilliant, but you can still like that.
[01:37:14.600 --> 01:37:16.360]   Oh, Lord.
[01:37:16.360 --> 01:37:18.600]   You can still be socially engineered.
[01:37:18.600 --> 01:37:21.160]   Well, and you can Chromebook extensions.
[01:37:21.160 --> 01:37:23.640]   I've said before, Chromebooks are unhackable.
[01:37:23.640 --> 01:37:24.760]   And people have said, no, no, no.
[01:37:24.760 --> 01:37:25.560]   Of course they can be hacked.
[01:37:25.560 --> 01:37:27.560]   Of course they can, but they do a lot to prevent hacking.
[01:37:27.560 --> 01:37:28.600]   No, he wants to bother.
[01:37:28.600 --> 01:37:29.480]   He's slowing down.
[01:37:29.480 --> 01:37:36.840]   Every company out there should recognize that they are going to get hacked.
[01:37:36.840 --> 01:37:40.440]   You should obviously look for the ways that you're most vulnerable,
[01:37:40.440 --> 01:37:42.040]   and figure that out.
[01:37:42.040 --> 01:37:45.320]   And most of it will be to social engineering attacks, in quite honesty.
[01:37:45.320 --> 01:37:45.320]   Yeah.
[01:37:45.320 --> 01:37:51.240]   And then figure out how, figure out strategies for mitigating the damage.
[01:37:51.240 --> 01:37:53.480]   Maybe that's backups, if you're worried about ransomware.
[01:37:53.480 --> 01:37:59.480]   Maybe it's keeping some of your most valuable data, like not on the internet.
[01:37:59.480 --> 01:38:04.040]   I mean, there's lots of options, but yeah, no computer is ever going to be secure.
[01:38:04.040 --> 01:38:04.200]   Yeah.
[01:38:05.960 --> 01:38:10.680]   He writes in his conclusion, which is called the death of solutionism.
[01:38:10.680 --> 01:38:23.320]   Basically, solutionism is the idea that there's a solution that technology can solve our problems.
[01:38:23.320 --> 01:38:23.560]   Right?
[01:38:23.560 --> 01:38:26.040]   There is a solution to any issue.
[01:38:26.040 --> 01:38:27.320]   Or a little problem that creates.
[01:38:27.320 --> 01:38:27.640]   Yeah.
[01:38:27.640 --> 01:38:33.480]   He writes that Scott writes, "The solutionism is ubiquitous in cybersecurity.
[01:38:34.040 --> 01:38:38.360]   Every cybersecurity firm promises its technology will keep your data safe."
[01:38:38.360 --> 01:38:43.640]   But he says, "I think this guy has a load of crap on that front.
[01:38:43.640 --> 01:38:48.360]   Most cybersecurity experts I talk to are like, look, you're going to get hacked.
[01:38:48.360 --> 01:38:48.920]   Right.
[01:38:48.920 --> 01:38:51.080]   You need to monitor.
[01:38:51.080 --> 01:38:55.640]   Nobody's telling me, okay, I obviously have not heard from every single cybersecurity vendor
[01:38:55.640 --> 01:39:00.840]   out there, but I have never met a cybersecurity vendor who's like, this will solve everything.
[01:39:01.400 --> 01:39:05.160]   They're like, this will solve this segment of the problem and be monitored."
[01:39:05.160 --> 01:39:14.920]   Anyway, I think a good book raising really interesting questions and certainly fascinating
[01:39:14.920 --> 01:39:21.160]   stories about five hacks that you and I all covered or paid attention to as they were
[01:39:21.160 --> 01:39:21.640]   happening.
[01:39:21.640 --> 01:39:25.880]   Maybe not the Robert Morris worm because that was so far so long ago, but it's a great story
[01:39:25.880 --> 01:39:27.880]   because it's the first internet virus.
[01:39:30.040 --> 01:39:35.960]   And I think, okay, so you concur and I'm glad to hear you say that, because these are
[01:39:35.960 --> 01:39:40.920]   in most courses he writes, I teach the at least one student
[01:39:40.920 --> 01:39:46.040]   remains skeptical throughout the entire semester, refusing to buy the intellectual goods
[01:39:46.040 --> 01:39:46.840]   I've been selling.
[01:39:46.840 --> 01:39:49.880]   My guess is you are similarly skeptical.
[01:39:49.880 --> 01:39:58.440]   So he goes through the Turing proof in the epilogue, the very end and explains why
[01:39:59.880 --> 01:40:04.680]   because it's a general computer, it will always be attackable.
[01:40:04.680 --> 01:40:09.000]   Finally convinced the students says, actually, I have one more question.
[01:40:09.000 --> 01:40:13.000]   Now that the course is over, what happened to Paris Hilton and Lindsay Lohan?
[01:40:13.000 --> 01:40:19.320]   I say that I have to run, but the student can take my course next semester to find out.
[01:40:19.320 --> 01:40:24.680]   In the words of PT Barnum, always leave him wanting more.
[01:40:24.680 --> 01:40:25.560]   Anyway, I recommend it.
[01:40:25.560 --> 01:40:28.680]   Fancy Burgos fishing, Scott Shapiro, it's challenging.
[01:40:28.680 --> 01:40:31.160]   I mean, like I said, I was going, no, that is he saying?
[01:40:31.160 --> 01:40:38.280]   Because it's kind of very philosophers point of view to this question of our computers ever
[01:40:38.280 --> 01:40:39.400]   going to be secure.
[01:40:39.400 --> 01:40:43.400]   But if you're in the business, if you cover this, if you're interested, if you use computers,
[01:40:43.400 --> 01:40:44.840]   very much.
[01:40:44.840 --> 01:40:45.640]   You had a great title.
[01:40:45.640 --> 01:40:45.880]   Yeah.
[01:40:45.880 --> 01:40:51.800]   Dr. Drew and the Discord found that tool that I was thinking of off the top of my head.
[01:40:51.800 --> 01:40:58.040]   I was saying, Loewick and it's low orbit, Ion Cannon.
[01:40:58.040 --> 01:40:58.680]   Oh, yeah.
[01:40:58.680 --> 01:41:03.240]   I think we talked about that on a gazillion years, right?
[01:41:03.240 --> 01:41:03.960]   Yeah.
[01:41:03.960 --> 01:41:05.720]   We've talked about that on security now.
[01:41:05.720 --> 01:41:06.040]   Yeah.
[01:41:06.040 --> 01:41:09.960]   I don't remember the whole story.
[01:41:09.960 --> 01:41:16.280]   Low orbit, Ion Cannon is an open source network stress testing in DDoS application.
[01:41:16.280 --> 01:41:16.760]   Interesting.
[01:41:16.760 --> 01:41:16.760]   Interesting.
[01:41:16.760 --> 01:41:17.480]   We're just in C#.
[01:41:17.480 --> 01:41:18.680]   Nice.
[01:41:18.680 --> 01:41:22.200]   Crowdsourced DDoS, basically.
[01:41:22.200 --> 01:41:22.440]   Yeah.
[01:41:22.440 --> 01:41:24.600]   All right.
[01:41:24.600 --> 01:41:26.200]   Now I think we could do
[01:41:27.080 --> 01:41:33.480]   if you would like, we didn't do it this week in AI because it was really the whole first half of the show.
[01:41:33.480 --> 01:41:36.520]   The Google change log.
[01:41:36.520 --> 01:41:39.480]   The Google change log.
[01:41:39.480 --> 01:41:43.960]   I didn't sign up.
[01:41:43.960 --> 01:41:44.280]   I don't know.
[01:41:44.280 --> 01:41:45.400]   Maybe somebody here did.
[01:41:45.400 --> 01:41:46.200]   Maybe you did.
[01:41:46.200 --> 01:41:50.120]   Google's SGE is available now in preview.
[01:41:50.120 --> 01:41:50.920]   This is their.
[01:41:52.840 --> 01:41:55.320]   No, because you're a works workspace.
[01:41:55.320 --> 01:41:56.760]   Well, I even tried to get it.
[01:41:56.760 --> 01:41:57.880]   You wouldn't let me in because of that.
[01:41:57.880 --> 01:42:00.760]   Then I want to, you know, a regular Google account and still said I couldn't get it.
[01:42:00.760 --> 01:42:01.240]   Well, yeah.
[01:42:01.240 --> 01:42:04.280]   I mean, I'm on the waiting list, I guess, but this is there.
[01:42:04.280 --> 01:42:07.160]   I'm not sure I want it, but I want to play with it, right?
[01:42:07.160 --> 01:42:07.400]   There.
[01:42:07.400 --> 01:42:10.760]   There's their AI-enabled search.
[01:42:10.760 --> 01:42:11.960]   Sure.
[01:42:11.960 --> 01:42:12.360]   Keep it.
[01:42:12.360 --> 01:42:15.320]   I'm okay with that for now.
[01:42:15.320 --> 01:42:15.640]   Yeah.
[01:42:15.640 --> 01:42:15.960]   I mean,
[01:42:15.960 --> 01:42:18.520]   I want to try it, but I haven't been able to yet.
[01:42:18.520 --> 01:42:20.840]   I played with Bard and compared to chat GPT.
[01:42:20.840 --> 01:42:22.360]   I think chat GPT is better.
[01:42:22.920 --> 01:42:24.920]   But I think it's interesting.
[01:42:24.920 --> 01:42:25.960]   So the idea is,
[01:42:25.960 --> 01:42:29.640]   Bard and chat GPT are frozen in time, right?
[01:42:29.640 --> 01:42:33.800]   Chat GPT is like late last year, September 2021, I think actually.
[01:42:33.800 --> 01:42:38.120]   But if you add search indexes, you're adding current information into it.
[01:42:38.120 --> 01:42:39.800]   So maybe it's more useful than I don't know.
[01:42:39.800 --> 01:42:41.320]   I don't know.
[01:42:41.320 --> 01:42:46.680]   Google Assistant's third party notes and lists integration is shutting down.
[01:42:51.240 --> 01:42:51.720]   Surprise.
[01:42:51.720 --> 01:42:57.480]   So you've been able to set a notes and lists provider.
[01:42:57.480 --> 01:43:01.800]   I used to use any do, which I really like, which is a to-do list,
[01:43:01.800 --> 01:43:03.160]   the third party to list.
[01:43:03.160 --> 01:43:05.560]   And so I could say to my Google voice,
[01:43:05.560 --> 01:43:08.840]   add something to my to-do list and would add it to my any-do list.
[01:43:08.840 --> 01:43:10.840]   No, not so fast anymore.
[01:43:10.840 --> 01:43:12.760]   Now it's going to be Google Keep.
[01:43:12.760 --> 01:43:20.680]   Or I guess Google Tasks.
[01:43:21.560 --> 01:43:23.560]   Is it going to lose Google Keep as well?
[01:43:23.560 --> 01:43:25.560]   I thought Tasks was going.
[01:43:25.560 --> 01:43:25.960]   Maybe it's going.
[01:43:25.960 --> 01:43:26.760]   That's interesting.
[01:43:26.760 --> 01:43:27.400]   That was going.
[01:43:27.400 --> 01:43:29.720]   Yeah, I used any do and it used gone for sure.
[01:43:29.720 --> 01:43:30.360]   I know that.
[01:43:30.360 --> 01:43:33.560]   The notes and lists integration.
[01:43:33.560 --> 01:43:35.880]   Wait, no, not keep.
[01:43:35.880 --> 01:43:37.080]   I use keep all the time.
[01:43:37.080 --> 01:43:38.200]   It's got to have keys.
[01:43:38.200 --> 01:43:40.360]   I thought Tasks was going by by.
[01:43:40.360 --> 01:43:43.880]   Okay, we asked the company whether Google Keep will be impacted,
[01:43:43.880 --> 01:43:46.120]   or if the upcoming Google Tasks integration,
[01:43:46.120 --> 01:43:48.200]   which is not yet widely wrote out,
[01:43:48.200 --> 01:43:50.440]   is the only service they say,
[01:43:50.440 --> 01:43:53.160]   no, you can use Google Keep and Tasks.
[01:43:53.160 --> 01:43:55.080]   For now, but that's it.
[01:43:55.080 --> 01:43:58.040]   Well, they're both, at least they're both Google, right?
[01:43:58.040 --> 01:44:01.240]   You remember Magic Compose?
[01:44:01.240 --> 01:44:04.280]   They showed that off where you could write a letter of,
[01:44:04.280 --> 01:44:08.280]   in various degrees of nastiness complaining about something.
[01:44:08.280 --> 01:44:11.160]   Message is Magic Compose beta starts rolling out.
[01:44:11.160 --> 01:44:15.000]   It's only for RCS and you need,
[01:44:15.000 --> 01:44:17.720]   you'll have priority if you're Google One subscriber,
[01:44:17.720 --> 01:44:18.680]   which I am.
[01:44:18.680 --> 01:44:22.920]   So you'll have to sign up for the Google Messages beta program on the Play Store.
[01:44:22.920 --> 01:44:29.800]   It's only available in English on Android phones with US SIM cards for those 18 and older.
[01:44:29.800 --> 01:44:32.520]   Well, that rules out everybody in this room.
[01:44:32.520 --> 01:44:37.240]   Lastly, Google will give priority access to Google on premium members as more spots become
[01:44:37.240 --> 01:44:38.120]   available.
[01:44:38.120 --> 01:44:39.960]   Oh, whoa, wait a minute.
[01:44:39.960 --> 01:44:43.560]   The subscriptions tier starts at $10 a month or $100 a year.
[01:44:43.560 --> 01:44:45.160]   Oh, no, that's, I'm sorry.
[01:44:45.160 --> 01:44:45.640]   You have one?
[01:44:45.640 --> 01:44:46.360]   That's Google One.
[01:44:46.360 --> 01:44:46.760]   Yeah.
[01:44:46.760 --> 01:44:47.320]   Yeah.
[01:44:47.320 --> 01:44:49.560]   I thought they were talking about this feature.
[01:44:49.560 --> 01:44:53.400]   I am paying for Magic Compose, baby.
[01:44:53.400 --> 01:44:58.120]   When activated in a conversation, Magic Compose will send up to 20 previous messages
[01:44:58.120 --> 01:45:02.280]   to Google servers to generate relevant contextual suggestions,
[01:45:02.280 --> 01:45:06.360]   including emojis, reactions, and URLs, but not attached to
[01:45:06.360 --> 01:45:07.320]   a freak people out.
[01:45:07.320 --> 01:45:08.840]   Voice messages are images.
[01:45:08.840 --> 01:45:15.480]   And then you'll be presented from now on with a list of suggestions to.
[01:45:15.480 --> 01:45:20.520]   So it's like autocomplete, but with much more knowledge of how the conversation has been going.
[01:45:20.520 --> 01:45:28.120]   And if you tap the pencil with a sparkle icon, it'll let you customize it with one of seven styles.
[01:45:28.120 --> 01:45:32.440]   Remix, which is just give me another one.
[01:45:32.440 --> 01:45:33.160]   Excited.
[01:45:33.160 --> 01:45:34.200]   Hey, wait a minute.
[01:45:34.200 --> 01:45:35.000]   Oh, let's go.
[01:45:35.000 --> 01:45:36.360]   Chill.
[01:45:36.360 --> 01:45:37.080]   Okay, man.
[01:45:37.080 --> 01:45:38.360]   No problem.
[01:45:38.360 --> 01:45:40.120]   Shakespeare.
[01:45:40.600 --> 01:45:42.200]   Priti Milaud.
[01:45:42.200 --> 01:45:43.720]   Shall we dine tonight?
[01:45:43.720 --> 01:45:44.600]   I'll fescue.
[01:45:44.600 --> 01:45:46.920]   Lyrical.
[01:45:46.920 --> 01:45:51.080]   What is lyrical?
[01:45:51.080 --> 01:45:52.920]   Like, I don't know.
[01:45:52.920 --> 01:45:55.480]   Like the New Yorker?
[01:45:55.480 --> 01:45:57.080]   I don't lyrical like a poem.
[01:45:57.080 --> 01:46:00.600]   After many a summer dies the swan.
[01:46:00.600 --> 01:46:02.280]   On the other hand, we could go for a walk.
[01:46:02.280 --> 01:46:05.480]   Formal or short.
[01:46:05.480 --> 01:46:08.840]   Some are particularly heavy about inserting emojis.
[01:46:09.400 --> 01:46:11.560]   And of course, you're not forced to take any of them.
[01:46:11.560 --> 01:46:13.640]   You can thumbs up or thumbs down suggestions.
[01:46:13.640 --> 01:46:14.520]   Look for that.
[01:46:14.520 --> 01:46:15.640]   Magic compose.
[01:46:15.640 --> 01:46:18.200]   I'm excited.
[01:46:18.200 --> 01:46:22.040]   NFL Sunday ticket and it feels coming back when it's late August, usually, right?
[01:46:22.040 --> 01:46:23.000]   September.
[01:46:23.000 --> 01:46:23.560]   September.
[01:46:23.560 --> 01:46:26.680]   I already paid for my Sunday ticket 300 bucks.
[01:46:26.680 --> 01:46:27.640]   Oh, bless your heart.
[01:46:27.640 --> 01:46:28.360]   Smackers.
[01:46:28.360 --> 01:46:29.320]   Oh, gosh.
[01:46:29.320 --> 01:46:30.040]   It will support.
[01:46:30.040 --> 01:46:31.640]   Are you going to invite and over to watch?
[01:46:31.640 --> 01:46:32.600]   Can't come on over.
[01:46:32.600 --> 01:46:33.400]   Okay, sure.
[01:46:33.400 --> 01:46:35.400]   It'll be unlimited simultaneous streams.
[01:46:35.400 --> 01:46:36.360]   We'll have one alone.
[01:46:36.360 --> 01:46:36.840]   We'll have to add.
[01:46:36.840 --> 01:46:37.960]   Yeah, bring everybody.
[01:46:37.960 --> 01:46:38.840]   And their whole team.
[01:46:38.840 --> 01:46:39.720]   Oh, well, I'll tell you.
[01:46:39.720 --> 01:46:43.800]   That's a good idea because of the bars, you know, right?
[01:46:43.800 --> 01:46:46.920]   All of the TVs in the bars in one subscription.
[01:46:46.920 --> 01:46:47.960]   You have five TVs.
[01:46:47.960 --> 01:46:48.920]   You can have all the games.
[01:46:48.920 --> 01:46:50.040]   Listen to them in a different license.
[01:46:50.040 --> 01:46:51.400]   Oh, I don't know.
[01:46:51.400 --> 01:46:54.360]   Yeah, they say in your home.
[01:46:54.360 --> 01:46:57.560]   Oh, no matter where you purchase NFL Sunday ticket,
[01:46:57.560 --> 01:47:00.120]   you'll have access to unlimited streams in your home
[01:47:00.120 --> 01:47:02.680]   and two additional streams outside of your home.
[01:47:02.680 --> 01:47:02.920]   See?
[01:47:02.920 --> 01:47:04.360]   So this is an idea.
[01:47:04.360 --> 01:47:06.520]   YouTube TV as expensive as it is
[01:47:06.520 --> 01:47:08.120]   because it supports six people.
[01:47:08.120 --> 01:47:11.640]   You know, you can, I mean, you can't,
[01:47:11.640 --> 01:47:13.560]   you have to kind of be in the same geographic area.
[01:47:13.560 --> 01:47:14.040]   Yeah, I know.
[01:47:14.040 --> 01:47:16.120]   Well, I think the caveat was like
[01:47:16.120 --> 01:47:19.400]   every month or something weird like that,
[01:47:19.400 --> 01:47:20.520]   or every couple of months.
[01:47:20.520 --> 01:47:21.000]   Yeah.
[01:47:21.000 --> 01:47:23.240]   I had that issue with the hard hit up in college.
[01:47:23.240 --> 01:47:26.760]   And when he came home, it was fine for a little while
[01:47:26.760 --> 01:47:29.240]   when he went back to college to watch television.
[01:47:29.240 --> 01:47:31.400]   Oh, I wonder if it's like, you know,
[01:47:31.400 --> 01:47:33.560]   how Netflix does its password sharing.
[01:47:33.560 --> 01:47:34.520]   It's anti-password sharing.
[01:47:34.520 --> 01:47:36.120]   They do it every 30 days.
[01:47:36.120 --> 01:47:38.520]   You have to check into your home IP address.
[01:47:38.520 --> 01:47:39.400]   Oh, okay.
[01:47:39.400 --> 01:47:40.600]   Your device does.
[01:47:40.600 --> 01:47:43.080]   Yeah, Google's got all these geographic rules.
[01:47:43.080 --> 01:47:47.240]   I know, because I inadvertently told that I was in North Carolina,
[01:47:47.240 --> 01:47:48.440]   or somehow, right?
[01:47:48.440 --> 01:47:49.560]   I thought I was in Carolina,
[01:47:49.560 --> 01:47:52.200]   and then it took me off in the Bay Area.
[01:47:52.200 --> 01:47:52.520]   Yep.
[01:47:52.520 --> 01:47:53.880]   And then I had to call them and they said,
[01:47:53.880 --> 01:47:55.800]   well, then I'm going to have to cancel your subscription.
[01:47:55.800 --> 01:47:58.120]   I'm going to give you a credit and you can redo it.
[01:47:58.120 --> 01:47:59.320]   And anyway, it worked.
[01:47:59.320 --> 01:48:00.840]   You actually got a human?
[01:48:00.840 --> 01:48:01.720]   I talked to a human.
[01:48:01.720 --> 01:48:02.440]   Yeah, in chat.
[01:48:02.440 --> 01:48:02.920]   Wow.
[01:48:02.920 --> 01:48:04.120]   He's laying on the porch.
[01:48:04.120 --> 01:48:05.880]   No, no, I didn't say, you know why.
[01:48:05.880 --> 01:48:06.920]   I am.
[01:48:06.920 --> 01:48:08.120]   I said, no, and I'm kidding.
[01:48:08.120 --> 01:48:10.040]   When you pay real money for these things,
[01:48:10.040 --> 01:48:11.000]   you get a real human.
[01:48:11.000 --> 01:48:11.800]   That's right. I pay a lot.
[01:48:11.800 --> 01:48:13.480]   Yeah, I was almost 100 bucks a month.
[01:48:13.480 --> 01:48:14.280]   Yeah.
[01:48:14.280 --> 01:48:16.920]   So this was originally going to be two concurrent streams.
[01:48:16.920 --> 01:48:18.520]   On Reddit, they said, update,
[01:48:18.520 --> 01:48:21.080]   no more streaming limits in your home,
[01:48:21.080 --> 01:48:23.720]   which is good because our kid wants to watch the Packers games.
[01:48:23.720 --> 01:48:24.200]   Yep.
[01:48:24.200 --> 01:48:26.200]   We're going to be watching the Niners games.
[01:48:26.200 --> 01:48:27.800]   Maybe somebody's going to come over and watch.
[01:48:27.800 --> 01:48:28.680]   It's me and in the house.
[01:48:28.680 --> 01:48:29.080]   Yeah.
[01:48:29.080 --> 01:48:30.040]   What's your team?
[01:48:30.040 --> 01:48:33.000]   Packers, baby.
[01:48:33.000 --> 01:48:35.560]   Maybe somebody's going to watch Aaron Rodgers
[01:48:35.560 --> 01:48:36.440]   play for the Jets.
[01:48:36.440 --> 01:48:37.160]   I don't know.
[01:48:37.160 --> 01:48:38.200]   I'm OK with that.
[01:48:38.200 --> 01:48:42.520]   So anyway, that's actually, interestingly,
[01:48:42.520 --> 01:48:45.640]   Google doing this, they actually responded in Reddit
[01:48:45.640 --> 01:48:47.720]   in answer to people complaining,
[01:48:47.720 --> 01:48:49.720]   oh, two chances and enough.
[01:48:49.720 --> 01:48:50.600]   We need more.
[01:48:50.600 --> 01:48:53.720]   So good on them for doing that.
[01:48:53.720 --> 01:48:58.520]   Google is phasing out the original Chromecast.
[01:48:58.520 --> 01:48:59.960]   I still have quite a few of these.
[01:48:59.960 --> 01:49:03.160]   It's more than 10 years old now.
[01:49:03.160 --> 01:49:05.080]   Man, I killed that thing pretty quickly.
[01:49:05.080 --> 01:49:06.760]   Because it kept getting too daggum hot.
[01:49:06.760 --> 01:49:08.440]   It does get hot.
[01:49:08.440 --> 01:49:08.760]   Yeah.
[01:49:08.760 --> 01:49:11.080]   They've ended support.
[01:49:11.080 --> 01:49:14.040]   Your last update came out, I think, pretty recently.
[01:49:14.040 --> 01:49:16.840]   Remember that $35 streaming?
[01:49:16.840 --> 01:49:18.520]   It was a cool value.
[01:49:18.520 --> 01:49:22.120]   But again, it was totally useless for us.
[01:49:22.120 --> 01:49:24.520]   It just would overheat and it would shut down.
[01:49:24.520 --> 01:49:26.840]   If yours is working now, it'll still work.
[01:49:26.840 --> 01:49:30.040]   They say users may notice a degradation in performance.
[01:49:30.040 --> 01:49:34.120]   The end of support was at the end of last months.
[01:49:34.920 --> 01:49:37.480]   Last firmware update on your Chromecast.
[01:49:37.480 --> 01:49:43.240]   And finally, it's the end of the line for YouTube stories.
[01:49:43.240 --> 01:49:47.080]   I guess we could have sucked.
[01:49:47.080 --> 01:49:47.560]   It hardly knew you.
[01:49:47.560 --> 01:49:48.680]   Yeah, we hardly knew you.
[01:49:48.680 --> 01:49:52.280]   Actually, June 26th will be the last day.
[01:49:52.280 --> 01:49:54.440]   They're moving it over to whatever they have.
[01:49:54.440 --> 01:49:55.880]   The other thing is shorts, right?
[01:49:55.880 --> 01:49:56.600]   They're shorts.
[01:49:56.600 --> 01:49:59.000]   Which is going totally fine for them.
[01:49:59.000 --> 01:49:59.240]   Yeah.
[01:49:59.240 --> 01:50:00.520]   Is it?
[01:50:00.520 --> 01:50:01.720]   Okay.
[01:50:01.720 --> 01:50:04.600]   This was basically their TikTok.
[01:50:04.840 --> 01:50:05.560]   Clone, right?
[01:50:05.560 --> 01:50:06.040]   Yep.
[01:50:06.040 --> 01:50:11.240]   So this is often the case where companies copy other companies
[01:50:11.240 --> 01:50:12.600]   and then it doesn't work and they...
[01:50:12.600 --> 01:50:19.640]   So the company launched in 2017 under the name Reels,
[01:50:19.640 --> 01:50:21.640]   which is what Facebook and Instagram use.
[01:50:21.640 --> 01:50:22.360]   Instagram.
[01:50:22.360 --> 01:50:22.920]   Yeah.
[01:50:22.920 --> 01:50:25.320]   Later renamed to YouTube stories in 2018.
[01:50:25.320 --> 01:50:28.520]   June 26th, the last day to upload a story.
[01:50:28.520 --> 01:50:32.520]   Move it on over to shorts.
[01:50:33.320 --> 01:50:36.120]   YouTube shorts was a bit of a mess when they launched.
[01:50:36.120 --> 01:50:41.240]   The interface was clunky and you couldn't always upload stuff
[01:50:41.240 --> 01:50:43.960]   that you would make offline to put into it.
[01:50:43.960 --> 01:50:45.720]   But now it's so much smoother and better.
[01:50:45.720 --> 01:50:47.560]   They've definitely been working on it.
[01:50:47.560 --> 01:50:50.040]   And I'm figuring they're watching carefully
[01:50:50.040 --> 01:50:51.000]   what happens to TikTok.
[01:50:51.000 --> 01:50:53.320]   Because if TikTok gets banned, why not?
[01:50:53.320 --> 01:50:54.680]   They're ready to step in.
[01:50:54.680 --> 01:50:55.640]   Come on over to the Google side.
[01:50:55.640 --> 01:50:56.760]   I mean, the YouTube side.
[01:50:56.760 --> 01:50:57.000]   Yeah.
[01:50:57.000 --> 01:50:57.800]   The Google side.
[01:50:57.800 --> 01:50:59.400]   (laughing)
[01:50:59.400 --> 01:51:01.560]   So if you're on YouTube, there's a shortzer
[01:51:01.560 --> 01:51:02.920]   in the left-hand side here.
[01:51:03.800 --> 01:51:05.880]   There's home and then there's shorts.
[01:51:05.880 --> 01:51:09.560]   I click that and then I get a short,
[01:51:09.560 --> 01:51:10.680]   which by the way,
[01:51:10.680 --> 01:51:12.680]   does that look a little bit like a TikTok?
[01:51:12.680 --> 01:51:13.400]   No comment.
[01:51:13.400 --> 01:51:13.720]   Yeah.
[01:51:13.720 --> 01:51:16.520]   Jennifer Lawrence wants to ask him,
[01:51:16.520 --> 01:51:18.440]   I think I saw this on TikTok.
[01:51:18.440 --> 01:51:22.360]   Before the internet, nobody could search things.
[01:51:22.360 --> 01:51:23.320]   Eddie Van Halen.
[01:51:23.320 --> 01:51:24.840]   Like on Searles playing guitar.
[01:51:24.840 --> 01:51:26.120]   But Eddie Van Halen, buddy.
[01:51:26.120 --> 01:51:27.480]   It does sort of look like that.
[01:51:27.480 --> 01:51:28.520]   There's David Letterman.
[01:51:28.520 --> 01:51:29.640]   It's a bunch of old people.
[01:51:29.640 --> 01:51:31.400]   Aubrey Plaza.
[01:51:31.400 --> 01:51:32.520]   I love her.
[01:51:32.520 --> 01:51:33.000]   Yeah.
[01:51:33.000 --> 01:51:36.040]   Here's Obama talking about between two ferns.
[01:51:36.040 --> 01:51:37.880]   This is all old.
[01:51:37.880 --> 01:51:38.760]   This is all old stuff.
[01:51:38.760 --> 01:51:39.880]   Best reaction.
[01:51:39.880 --> 01:51:40.120]   Yeah.
[01:51:40.120 --> 01:51:41.160]   If you want to be hip and with it,
[01:51:41.160 --> 01:51:42.040]   you go to TikTok.
[01:51:42.040 --> 01:51:44.120]   You don't go to YouTube shorts.
[01:51:44.120 --> 01:51:46.680]   But boy, talk about a TikTok clone.
[01:51:46.680 --> 01:51:48.120]   It's vertical videos.
[01:51:48.120 --> 01:51:49.400]   You scroll up.
[01:51:49.400 --> 01:51:51.640]   It's just like it.
[01:51:51.640 --> 01:51:55.720]   Where's all the women in bikinis though?
[01:51:55.720 --> 01:51:56.520]   I don't understand.
[01:51:56.520 --> 01:51:57.560]   I believe they show it.
[01:51:57.560 --> 01:51:59.320]   It hasn't learned you yet.
[01:51:59.320 --> 01:52:01.640]   You know what else gets women in bikinis.
[01:52:01.640 --> 01:52:03.880]   Just me.
[01:52:03.880 --> 01:52:04.920]   Just me.
[01:52:04.920 --> 01:52:05.320]   Okay.
[01:52:05.320 --> 01:52:05.880]   All right.
[01:52:05.880 --> 01:52:06.440]   I'm almost to.
[01:52:06.440 --> 01:52:09.880]   Here's a guy making quarter pounders.
[01:52:09.880 --> 01:52:11.400]   God, I did this for a long time.
[01:52:11.400 --> 01:52:13.640]   There hasn't changed at all.
[01:52:13.640 --> 01:52:14.840]   That's exactly how you do it.
[01:52:14.840 --> 01:52:16.440]   You accept you don't have two people.
[01:52:16.440 --> 01:52:17.400]   Like onions.
[01:52:17.400 --> 01:52:17.720]   Yeah.
[01:52:17.720 --> 01:52:19.640]   You go home, you smell like the grill.
[01:52:19.640 --> 01:52:21.720]   Always struck me as the probably the most part of the whole.
[01:52:21.720 --> 01:52:22.440]   So cheap.
[01:52:22.440 --> 01:52:24.680]   Well, did you see those machines for squirting the mustard
[01:52:24.680 --> 01:52:25.240]   and the ketchup?
[01:52:25.240 --> 01:52:26.600]   It's exactly measured out.
[01:52:26.600 --> 01:52:27.160]   Exactly.
[01:52:27.160 --> 01:52:27.640]   Yeah.
[01:52:27.640 --> 01:52:29.240]   That's how much mustard and ketchup.
[01:52:29.240 --> 01:52:29.800]   That's bright.
[01:52:29.800 --> 01:52:31.160]   Here comes the quarter pounder.
[01:52:31.160 --> 01:52:31.640]   There you go.
[01:52:31.640 --> 01:52:33.400]   They don't know they don't do mustard anymore.
[01:52:33.400 --> 01:52:35.080]   That's a it's regional.
[01:52:35.080 --> 01:52:35.800]   This is me off.
[01:52:35.800 --> 01:52:36.600]   I can't get my what?
[01:52:36.600 --> 01:52:39.400]   Look, see mustard and ketchup must do.
[01:52:39.400 --> 01:52:40.440]   That's regional.
[01:52:40.440 --> 01:52:42.200]   It's somewhere where they do the mushrooms still.
[01:52:42.200 --> 01:52:43.160]   Some places they don't do it.
[01:52:43.160 --> 01:52:45.640]   Wait, but if you ask for musters,
[01:52:45.640 --> 01:52:46.520]   you can get it, right?
[01:52:46.520 --> 01:52:47.640]   A little packet of mustard.
[01:52:47.640 --> 01:52:49.400]   That's not a bet.
[01:52:49.400 --> 01:52:50.680]   That's tall.
[01:52:50.680 --> 01:52:51.240]   Shocking.
[01:52:51.240 --> 01:52:54.040]   Oh, here's a man of bikini.
[01:52:54.040 --> 01:52:56.920]   I don't know what he's doing.
[01:52:56.920 --> 01:52:57.880]   What is he doing?
[01:52:57.880 --> 01:52:59.480]   Dude, he's rolling around in the water.
[01:52:59.480 --> 01:53:00.440]   Aren't you confused?
[01:53:00.440 --> 01:53:02.040]   He's very confused.
[01:53:02.040 --> 01:53:02.360]   All right.
[01:53:02.360 --> 01:53:05.480]   Here they are painting a directional arrow in a parking lot.
[01:53:05.480 --> 01:53:05.720]   Okay.
[01:53:05.720 --> 01:53:07.000]   This is boring.
[01:53:07.000 --> 01:53:10.840]   That's the Google change log.
[01:53:10.840 --> 01:53:13.320]   Yay.
[01:53:13.320 --> 01:53:15.880]   I see why you get the women in bikinis.
[01:53:15.880 --> 01:53:24.520]   Stacey, I want to let you know that before we do the Google change log,
[01:53:24.520 --> 01:53:26.680]   he said that was the first half of the show.
[01:53:26.680 --> 01:53:27.480]   Just see it up.
[01:53:27.480 --> 01:53:29.800]   Almost done.
[01:53:29.800 --> 01:53:30.920]   Another lemon bar.
[01:53:30.920 --> 01:53:31.720]   I am now.
[01:53:31.720 --> 01:53:33.320]   We're almost done.
[01:53:33.320 --> 01:53:36.280]   I am now to Jeff Jarvis's stories.
[01:53:36.280 --> 01:53:38.200]   Yay.
[01:53:38.200 --> 01:53:39.400]   You only did one of yours.
[01:53:39.400 --> 01:53:42.840]   Stop staring at travel.
[01:53:42.840 --> 01:53:43.800]   I'm not dying yet.
[01:53:43.800 --> 01:53:45.080]   Y'all keep going.
[01:53:45.080 --> 01:53:48.120]   She had a lemon bar to hold her over.
[01:53:48.120 --> 01:53:49.960]   I had a lemon bar to tide me over.
[01:53:49.960 --> 01:53:51.960]   I've got big plans.
[01:53:51.960 --> 01:53:52.920]   Oh, yeah.
[01:53:52.920 --> 01:53:53.720]   You going out tonight?
[01:53:53.720 --> 01:53:55.320]   I am.
[01:53:55.960 --> 01:53:58.360]   Oh, where does one go out in Cambridge Island?
[01:53:58.360 --> 01:54:00.120]   Is there like a disco?
[01:54:00.120 --> 01:54:03.240]   I'm taking my child.
[01:54:03.240 --> 01:54:04.920]   My child and I are going to go out to eat.
[01:54:04.920 --> 01:54:06.120]   Oh, that'll be fun.
[01:54:06.120 --> 01:54:07.560]   That's always fun.
[01:54:07.560 --> 01:54:10.280]   We're celebrating the end of their school year.
[01:54:10.280 --> 01:54:12.840]   Oh, is there a graduation?
[01:54:12.840 --> 01:54:15.640]   No, they're a junior.
[01:54:15.640 --> 01:54:19.480]   So next year, how about junior college?
[01:54:19.480 --> 01:54:21.000]   Was there a junior college?
[01:54:21.000 --> 01:54:21.240]   Hell.
[01:54:21.240 --> 01:54:22.280]   Yes.
[01:54:23.720 --> 01:54:26.120]   Eris was in charge of the junior poem.
[01:54:26.120 --> 01:54:28.600]   So I had to help decorate.
[01:54:28.600 --> 01:54:29.800]   It was terrible.
[01:54:29.800 --> 01:54:30.200]   What was the theme?
[01:54:30.200 --> 01:54:33.560]   It was a mid-summer night's dream.
[01:54:33.560 --> 01:54:34.200]   That's cool.
[01:54:34.200 --> 01:54:36.040]   I mean, yeah.
[01:54:36.040 --> 01:54:36.040]   It's nice.
[01:54:36.040 --> 01:54:36.280]   Yeah, we...
[01:54:36.280 --> 01:54:38.520]   And you had to decorate it.
[01:54:38.520 --> 01:54:39.000]   It is.
[01:54:39.000 --> 01:54:39.320]   Yeah.
[01:54:39.320 --> 01:54:42.440]   Well, there was a little rebellion happening.
[01:54:42.440 --> 01:54:44.040]   Very much drama.
[01:54:44.040 --> 01:54:45.000]   I cannot wait these children.
[01:54:45.000 --> 01:54:45.480]   Of course.
[01:54:45.480 --> 01:54:45.960]   Right, right.
[01:54:45.960 --> 01:54:46.440]   I get it.
[01:54:46.440 --> 01:54:47.240]   Of course.
[01:54:47.240 --> 01:54:47.880]   Exactly.
[01:54:47.880 --> 01:54:48.520]   I'm like, "Ah!"
[01:54:48.520 --> 01:54:50.680]   But it's all good.
[01:54:52.120 --> 01:54:55.000]   Uh, Supreme Court has rejected a lawsuit,
[01:54:55.000 --> 01:54:59.400]   did not, uh, give cert to a lawsuit
[01:54:59.400 --> 01:55:03.480]   to hold Reddit responsible for hosting child pornography.
[01:55:03.480 --> 01:55:08.120]   This seems to be a support of Section 230.
[01:55:08.120 --> 01:55:10.680]   I get once again right after last week.
[01:55:10.680 --> 01:55:11.400]   Good news.
[01:55:11.400 --> 01:55:14.520]   Yesterday, the decline to take up a case,
[01:55:14.520 --> 01:55:19.720]   uh, of a victim of sex trafficking who sought to hold Reddit
[01:55:19.720 --> 01:55:22.360]   responsible for hosting images of child pornography.
[01:55:22.360 --> 01:55:24.840]   Obviously Reddit, like any responsible site,
[01:55:24.840 --> 01:55:28.200]   as soon as it discovers that immediately deletes it,
[01:55:28.200 --> 01:55:29.080]   blocks the user.
[01:55:29.080 --> 01:55:30.600]   I mean, they don't just let it sit there.
[01:55:30.600 --> 01:55:32.280]   It's not 4chan or anything.
[01:55:32.280 --> 01:55:32.520]   Right.
[01:55:32.520 --> 01:55:38.120]   So I think, uh, after the justices,
[01:55:38.120 --> 01:55:38.760]   let's see here.
[01:55:38.760 --> 01:55:41.160]   This is from the Supreme CNN.
[01:55:41.160 --> 01:55:43.880]   After the justices avoided any meaningful ruling
[01:55:43.880 --> 01:55:46.840]   on the scope of immunity for tech companies in the Google case,
[01:55:46.840 --> 01:55:51.480]   we talked about that, Tamna and, uh, Gonzalez for today's denial of review.
[01:55:51.480 --> 01:55:55.320]   And the Reddit case suggests their version was more than just about the Google case,
[01:55:55.320 --> 01:55:58.040]   specifically in the court is willing, at least for now,
[01:55:58.040 --> 01:56:01.880]   to leave any changes to Section 232.
[01:56:01.880 --> 01:56:03.080]   Use me, Congress.
[01:56:03.080 --> 01:56:06.040]   Like, like, like there's a presumption that it should change.
[01:56:06.040 --> 01:56:06.360]   Right.
[01:56:06.360 --> 01:56:07.240]   No, leave it alone.
[01:56:07.240 --> 01:56:07.400]   Yeah.
[01:56:07.400 --> 01:56:08.920]   Just leave it alone.
[01:56:08.920 --> 01:56:09.240]   Yeah.
[01:56:09.240 --> 01:56:14.840]   Uh, which, which is not to say we're in favor of child pornography or sex traffic or anything.
[01:56:14.840 --> 01:56:15.320]   No.
[01:56:15.320 --> 01:56:16.520]   That's not what this is about.
[01:56:16.680 --> 01:56:21.880]   This is, is Reddit or any social network responsible for stuff other, you know,
[01:56:21.880 --> 01:56:26.920]   its users put online, especially since they're making an effort to take it down and get rid of it.
[01:56:26.920 --> 01:56:26.920]   Exactly.
[01:56:26.920 --> 01:56:27.560]   That's the key.
[01:56:27.560 --> 01:56:27.960]   Yeah.
[01:56:27.960 --> 01:56:30.040]   And, and 230 gives them the freedom to do so.
[01:56:30.040 --> 01:56:30.280]   Yes.
[01:56:30.280 --> 01:56:31.720]   So good.
[01:56:31.720 --> 01:56:34.200]   That's, uh, that's a happy, uh, story.
[01:56:34.200 --> 01:56:36.440]   I'm going to let you, Jeff.
[01:56:36.440 --> 01:56:37.800]   I've ignored all the rest of them.
[01:56:37.800 --> 01:56:39.720]   I'm going to let you pick one and only one.
[01:56:39.720 --> 01:56:41.560]   One and only one.
[01:56:41.560 --> 01:56:41.800]   Wow.
[01:56:41.800 --> 01:56:42.760]   You can pick a couple.
[01:56:42.760 --> 01:56:43.560]   You can pick a couple.
[01:56:43.560 --> 01:56:45.400]   Why none of these seem that interesting.
[01:56:45.400 --> 01:56:48.280]   No, they're not the Andy Jassy kill list.
[01:56:48.280 --> 01:56:51.000]   I found interesting that Jassy is the end of Google one.
[01:56:51.000 --> 01:56:51.640]   Okay.
[01:56:51.640 --> 01:57:00.440]   So Andy Jassy, the CEO of Amazon has a list of 37 projects that were created during Jeff Bezos.
[01:57:00.440 --> 01:57:01.720]   It has gone.
[01:57:01.720 --> 01:57:03.960]   That are going.
[01:57:03.960 --> 01:57:05.640]   We're some of them celebrity voices.
[01:57:05.640 --> 01:57:08.440]   Oh, it is mentioned that I am pissed on.
[01:57:08.440 --> 01:57:10.760]   I bought Samuel Jackson.
[01:57:10.760 --> 01:57:12.440]   I bought Melissa McCarthy.
[01:57:12.440 --> 01:57:13.080]   Oh my God.
[01:57:13.080 --> 01:57:13.960]   Just get a refund.
[01:57:15.240 --> 01:57:15.560]   Go ahead.
[01:57:15.560 --> 01:57:21.560]   I liked being able to say it was really fun.
[01:57:21.560 --> 01:57:25.240]   I could, if I had my echo here, I'd show you,
[01:57:25.240 --> 01:57:28.920]   I could say, what's the weather and then it would do it in the, you know, the normal echo voice.
[01:57:28.920 --> 01:57:30.680]   And then I say, hey, Samuel, what's the weather?
[01:57:30.680 --> 01:57:35.240]   And he'd swear he'd say something profane, of course, and nasty.
[01:57:35.240 --> 01:57:37.080]   And then tell me the weather in his voice, which I loved.
[01:57:37.080 --> 01:57:40.520]   And then I'd say, hey, Melissa, and then she'd like do something Melissa McCarthy
[01:57:40.520 --> 01:57:42.040]   ask and give me the weather.
[01:57:42.040 --> 01:57:42.920]   I love that.
[01:57:42.920 --> 01:57:44.680]   You'll be okay, sir.
[01:57:45.560 --> 01:57:48.920]   I think you were one of the few people who were doing this.
[01:57:48.920 --> 01:57:50.920]   Apparently, we're in a lot of people doing it.
[01:57:50.920 --> 01:57:53.720]   Andy Jassy is tougher than what's your name at Google?
[01:57:53.720 --> 01:57:55.240]   Ruth Porat?
[01:57:55.240 --> 01:57:56.200]   Ruth Porat, I think.
[01:57:56.200 --> 01:57:58.520]   Yeah, he leaves slicing through.
[01:57:58.520 --> 01:57:59.480]   Let's see.
[01:57:59.480 --> 01:58:00.520]   What's on your kill list?
[01:58:00.520 --> 01:58:06.840]   Alexa built in a feature for the Alexa smartphone app to let people summon the digital
[01:58:06.840 --> 01:58:08.520]   assistant using only their voice.
[01:58:08.520 --> 01:58:10.360]   It's a little too much like Beetlejuice.
[01:58:10.360 --> 01:58:12.600]   Alexa for business.
[01:58:13.800 --> 01:58:15.320]   I'm going to say Alexa.
[01:58:15.320 --> 01:58:18.040]   Madam A.
[01:58:18.040 --> 01:58:21.720]   Echo, HIPAA compliant health skills.
[01:58:21.720 --> 01:58:23.000]   Just hit.
[01:58:23.000 --> 01:58:27.000]   Madam A.
[01:58:27.000 --> 01:58:29.000]   The web version of Madam A.
[01:58:29.000 --> 01:58:30.040]   No, no, no, no, no.
[01:58:30.040 --> 01:58:32.120]   That was not the what that was.
[01:58:32.120 --> 01:58:33.560]   This was Alexa.com.
[01:58:33.560 --> 01:58:34.120]   This was the Alexa.com.
[01:58:34.120 --> 01:58:34.520]   Yeah.
[01:58:34.520 --> 01:58:37.080]   Oh, this has nothing to do with Madam A or Echo.
[01:58:37.080 --> 01:58:37.960]   Web traffic.
[01:58:37.960 --> 01:58:39.480]   And this was always the source of confusion.
[01:58:39.480 --> 01:58:42.840]   You remember back in the day the Alexa site rankings.
[01:58:42.840 --> 01:58:44.040]   I don't even remember that.
[01:58:44.040 --> 01:58:44.200]   Right.
[01:58:44.200 --> 01:58:44.600]   Oh, yeah.
[01:58:44.600 --> 01:58:47.480]   It was the only way we really knew Amazon bought it.
[01:58:47.480 --> 01:58:49.000]   This is old stuff too.
[01:58:49.000 --> 01:58:49.800]   Yeah, Amazon bought it.
[01:58:49.800 --> 01:58:51.000]   He's still in 1999.
[01:58:51.000 --> 01:58:53.400]   It's a long time ago.
[01:58:53.400 --> 01:58:55.480]   But now if you go there, if you go to Alexa.com,
[01:58:55.480 --> 01:58:57.160]   yeah, it says coming soon.
[01:58:57.160 --> 01:58:57.800]   I guess not.
[01:58:57.800 --> 01:59:00.600]   Dying soon.
[01:59:00.600 --> 01:59:06.200]   Amazon 4* which is a store which sold highly rated items.
[01:59:06.200 --> 01:59:06.920]   Did you ever go to those?
[01:59:06.920 --> 01:59:07.400]   No.
[01:59:07.400 --> 01:59:09.000]   Yeah, they were so weird.
[01:59:09.000 --> 01:59:10.040]   It was just very weird.
[01:59:10.040 --> 01:59:12.280]   It was stuff they sell on Amazon that people like.
[01:59:13.160 --> 01:59:14.440]   Would you go there, look at it,
[01:59:14.440 --> 01:59:16.760]   and then order it on Amazon or could you buy it there?
[01:59:16.760 --> 01:59:17.720]   No, you could buy it right there.
[01:59:17.720 --> 01:59:19.480]   Because it was this weird hodgepodge.
[01:59:19.480 --> 01:59:20.120]   Yeah, it's not.
[01:59:20.120 --> 01:59:20.600]   There's no.
[01:59:20.600 --> 01:59:23.080]   Yeah, it was like the dollar store,
[01:59:23.080 --> 01:59:24.600]   except it wasn't a dollar.
[01:59:24.600 --> 01:59:28.360]   It was so much stuff.
[01:59:28.360 --> 01:59:29.480]   It was so clever.
[01:59:29.480 --> 01:59:31.320]   It's more than the dollar store.
[01:59:31.320 --> 01:59:31.800]   Amazon.
[01:59:31.800 --> 01:59:32.760]   They no sense together.
[01:59:32.760 --> 01:59:35.640]   There was no taxonomy to it.
[01:59:35.640 --> 01:59:38.120]   Amazon Academy in India.
[01:59:38.120 --> 01:59:40.040]   Amazon said we are.
[01:59:40.040 --> 01:59:41.960]   We remain committed to India.
[01:59:41.960 --> 01:59:45.720]   Amazon assist in a browser extension for price comparisons
[01:59:45.720 --> 01:59:47.000]   between Amazon and other sites.
[01:59:47.000 --> 01:59:49.480]   This is Amazon Books.
[01:59:49.480 --> 01:59:51.480]   Oh, that was the physical bookstore.
[01:59:51.480 --> 01:59:53.000]   Yeah, that made me sad.
[01:59:53.000 --> 01:59:53.960]   I thought were they good?
[01:59:53.960 --> 01:59:56.280]   I've never been to one, but they were okay.
[01:59:56.280 --> 01:59:58.760]   If you like this, you might like this.
[01:59:58.760 --> 01:59:59.960]   This is Amazon.
[01:59:59.960 --> 02:00:03.160]   Andy has really been grappling with this whole brick and mortar strategy
[02:00:03.160 --> 02:00:03.960]   that Jeff had.
[02:00:03.960 --> 02:00:05.000]   Yeah.
[02:00:05.000 --> 02:00:08.520]   And they have said that they're not going to get rid of the Go stores.
[02:00:08.520 --> 02:00:10.520]   They're not going to get rid of whole foods.
[02:00:10.520 --> 02:00:12.280]   But they are getting rid of the Go stores.
[02:00:12.280 --> 02:00:13.080]   Is that one of them?
[02:00:13.080 --> 02:00:14.920]   That's one of them.
[02:00:14.920 --> 02:00:15.720]   Amazon Go.
[02:00:15.720 --> 02:00:18.200]   It close to eight of the things and says it.
[02:00:18.200 --> 02:00:20.760]   Yeah.
[02:00:20.760 --> 02:00:21.640]   Amazon Care.
[02:00:21.640 --> 02:00:23.000]   Amazon Distribution India.
[02:00:23.000 --> 02:00:23.960]   Amazon Drive.
[02:00:23.960 --> 02:00:27.160]   That's free for prime members.
[02:00:27.160 --> 02:00:30.600]   But they're making it out just photos.
[02:00:30.600 --> 02:00:33.720]   Amazon Explore, which was a virtual tours portal.
[02:00:33.720 --> 02:00:36.040]   Amazon Flex.
[02:00:36.040 --> 02:00:37.480]   Amazon Food India.
[02:00:37.480 --> 02:00:38.360]   Amazon Glow.
[02:00:39.320 --> 02:00:42.840]   Oh, the children's video conferencing and game device.
[02:00:42.840 --> 02:00:43.800]   Remember that would project.
[02:00:43.800 --> 02:00:48.120]   That was such an interesting concept, but it was only there for like six months.
[02:00:48.120 --> 02:00:48.840]   It was pretty brief.
[02:00:48.840 --> 02:00:49.160]   Yeah.
[02:00:49.160 --> 02:00:50.360]   I didn't think it was that last year.
[02:00:50.360 --> 02:00:53.320]   There's Amazon Go April 2023.
[02:00:53.320 --> 02:00:55.640]   The company closed eight of its cashier lists.
[02:00:55.640 --> 02:00:56.440]   The locations.
[02:00:56.440 --> 02:01:00.360]   It still has 20 more stores and it says we're.
[02:01:00.360 --> 02:01:01.800]   Yeah, there's a little licensing attack.
[02:01:01.800 --> 02:01:03.000]   There's still a couple out there.
[02:01:03.000 --> 02:01:03.640]   Yeah, just one.
[02:01:03.640 --> 02:01:05.720]   Yeah, they're licensing it to Walmart and others.
[02:01:05.720 --> 02:01:06.680]   Yes, right.
[02:01:06.680 --> 02:01:09.080]   There's also one that combines with a Starbucks pickup.
[02:01:09.080 --> 02:01:10.120]   You're being baptized.
[02:01:10.120 --> 02:01:11.080]   Your ties building.
[02:01:11.080 --> 02:01:12.360]   Halo, we know, is gone.
[02:01:12.360 --> 02:01:14.600]   Amazon Ignite.
[02:01:14.600 --> 02:01:17.320]   A hub for teachers and creators of
[02:01:17.320 --> 02:01:20.040]   Indic Education kind of has curriculum site.
[02:01:20.040 --> 02:01:21.720]   Amazon Kid Plus Games.
[02:01:21.720 --> 02:01:24.760]   I can go on and on and on, obviously.
[02:01:24.760 --> 02:01:27.720]   You know, they don't mention the look camera that you got rid of.
[02:01:27.720 --> 02:01:29.640]   Oh, well, that.
[02:01:29.640 --> 02:01:29.960]   Yeah.
[02:01:29.960 --> 02:01:31.320]   Remember that camera thing?
[02:01:31.320 --> 02:01:31.560]   Yeah.
[02:01:31.560 --> 02:01:33.240]   But it is totally gone.
[02:01:33.240 --> 02:01:33.720]   They did.
[02:01:33.720 --> 02:01:35.880]   So there's actually there's even more.
[02:01:35.880 --> 02:01:36.600]   We can add more.
[02:01:36.600 --> 02:01:37.800]   Tp reviews on this list.
[02:01:37.800 --> 02:01:41.160]   Although there's still posting articles to DP review.
[02:01:41.160 --> 02:01:43.880]   I'm wondering, is it you have soft opens.
[02:01:43.880 --> 02:01:45.080]   Can you have a soft close?
[02:01:45.080 --> 02:01:47.560]   Well, I thought something was going on with
[02:01:47.560 --> 02:01:49.000]   pedapixel and DP review.
[02:01:49.000 --> 02:01:50.360]   Pedapixel archive did.
[02:01:50.360 --> 02:01:50.760]   Right.
[02:01:50.760 --> 02:01:53.560]   But if I go to DP review.com,
[02:01:53.560 --> 02:01:56.920]   I've seen new articles.
[02:01:56.920 --> 02:01:57.480]   Sure.
[02:01:57.480 --> 02:02:00.920]   Because I haven't even bothered since Jordan and Chris has been on.
[02:02:00.920 --> 02:02:02.760]   Well, that is new.
[02:02:02.760 --> 02:02:05.640]   This is May 23rd, Richard Butler.
[02:02:06.280 --> 02:02:08.280]   ZV, by the way, I'm thinking of buying this.
[02:02:08.280 --> 02:02:08.680]   I don't know.
[02:02:08.680 --> 02:02:11.000]   Should I ZV one, Mark two, vlogging camera?
[02:02:11.000 --> 02:02:11.720]   Oh, yeah.
[02:02:11.720 --> 02:02:15.320]   That's why I'm really glad that deep review is gone.
[02:02:15.320 --> 02:02:16.440]   Because oh, wait a minute.
[02:02:16.440 --> 02:02:18.920]   No, maybe I'll buy the I have a Q two.
[02:02:18.920 --> 02:02:19.640]   Maybe you should get it.
[02:02:19.640 --> 02:02:21.800]   So you get the like a Q three.
[02:02:21.800 --> 02:02:22.200]   Huh?
[02:02:22.200 --> 02:02:25.720]   Dude, I like six thousand dollars.
[02:02:25.720 --> 02:02:28.280]   You came back for your last trip saying you were going to take cameras.
[02:02:28.280 --> 02:02:29.880]   I don't know, but I love this little one.
[02:02:29.880 --> 02:02:31.400]   It's barely a camera.
[02:02:31.400 --> 02:02:32.440]   Like is a pretty cool.
[02:02:32.440 --> 02:02:34.440]   It's barely a camera.
[02:02:35.240 --> 02:02:36.120]   So they're posting.
[02:02:36.120 --> 02:02:37.320]   So I don't know what happened.
[02:02:37.320 --> 02:02:39.560]   They were supposed to be long gone by now.
[02:02:39.560 --> 02:02:40.040]   I don't know.
[02:02:40.040 --> 02:02:41.000]   Yeah, that's confusing.
[02:02:41.000 --> 02:02:43.400]   Maybe Amazon changed my fabric.com.
[02:02:43.400 --> 02:02:46.600]   Fire TV.
[02:02:46.600 --> 02:02:47.480]   Are you fast free?
[02:02:47.480 --> 02:02:48.600]   Yeah, I think so.
[02:02:48.600 --> 02:02:49.800]   Never heard of it.
[02:02:49.800 --> 02:02:51.880]   E-commerce category.
[02:02:51.880 --> 02:02:53.800]   Yeah, apparently.
[02:02:53.800 --> 02:02:54.600]   So know what it is.
[02:02:54.600 --> 02:02:56.120]   Free US grocery delivery.
[02:02:56.120 --> 02:02:57.640]   Game on.
[02:02:57.640 --> 02:02:58.840]   Anyway, Kindle news stand.
[02:02:58.840 --> 02:02:59.640]   The list keeps going.
[02:02:59.640 --> 02:03:01.480]   It's a long freaking list.
[02:03:01.480 --> 02:03:02.360]   That's good though.
[02:03:03.320 --> 02:03:04.520]   Jesse said, "Look here.
[02:03:04.520 --> 02:03:07.480]   We're Amazon and we're here to make money."
[02:03:07.480 --> 02:03:08.680]   Do we don't need to do everything?
[02:03:08.680 --> 02:03:09.720]   Yeah, that's it.
[02:03:09.720 --> 02:03:11.080]   All right, that was a pretty good one, Jeff.
[02:03:11.080 --> 02:03:12.280]   You've earned yourself another one.
[02:03:12.280 --> 02:03:14.680]   Oh, oh, okay.
[02:03:14.680 --> 02:03:17.880]   Oh, the pressure is on.
[02:03:17.880 --> 02:03:19.320]   I don't know if I can take this.
[02:03:19.320 --> 02:03:21.640]   It's already done quite a few of the stories for me.
[02:03:21.640 --> 02:03:23.080]   You don't have to, man.
[02:03:23.080 --> 02:03:26.200]   Well, now it stays heat.
[02:03:26.200 --> 02:03:27.400]   Now I do have to.
[02:03:27.400 --> 02:03:28.600]   Now I do have to.
[02:03:28.600 --> 02:03:30.920]   Twitter is according to fidelity.
[02:03:30.920 --> 02:03:32.040]   Remember, what did the E-line pay?
[02:03:32.040 --> 02:03:33.000]   $44 billion.
[02:03:33.000 --> 02:03:33.560]   $45 billion.
[02:03:33.560 --> 02:03:36.360]   Fidelity, which is one of the people
[02:03:36.360 --> 02:03:37.960]   who lent money to Elon for this,
[02:03:37.960 --> 02:03:40.840]   have written down their investments.
[02:03:40.840 --> 02:03:42.040]   They've taken notes, huh?
[02:03:42.040 --> 02:03:46.120]   They say Twitter is only worth about $15 billion.
[02:03:46.120 --> 02:03:47.720]   One third is what it was.
[02:03:47.720 --> 02:03:49.480]   Which is what the bankers put into it.
[02:03:49.480 --> 02:03:51.400]   And when the bankers take it over out of bankruptcy,
[02:03:51.400 --> 02:03:52.680]   it isn't a deal.
[02:03:52.680 --> 02:03:56.280]   I have a story that I was excited about.
[02:03:56.280 --> 02:03:57.240]   Okay.
[02:03:57.240 --> 02:03:59.000]   So knows one.
[02:03:59.000 --> 02:04:00.200]   There are 32 point,
[02:04:00.200 --> 02:04:04.280]   or they won their patent infringement fight with Google.
[02:04:04.280 --> 02:04:04.600]   Oh.
[02:04:04.600 --> 02:04:06.680]   So they won $32.5 million.
[02:04:06.680 --> 02:04:07.160]   Good.
[02:04:07.160 --> 02:04:09.080]   I saw that with Mr. Scooter X.
[02:04:09.080 --> 02:04:10.120]   Yeah, I did see that with Mr. Scooter X.
[02:04:10.120 --> 02:04:14.040]   So Sonos was the first to have party mode,
[02:04:14.040 --> 02:04:15.400]   where you'd have multiple speakers.
[02:04:15.400 --> 02:04:16.840]   We use it all the time here in the studio,
[02:04:16.840 --> 02:04:19.080]   with speakers all over and all the offices.
[02:04:19.080 --> 02:04:20.600]   And John, when everybody leaves,
[02:04:20.600 --> 02:04:22.360]   turns on strange music.
[02:04:22.360 --> 02:04:23.880]   It's not when everybody leaves.
[02:04:23.880 --> 02:04:25.160]   It's not when everybody leaves.
[02:04:25.160 --> 02:04:26.920]   Oh, it's so when people wear it.
[02:04:26.920 --> 02:04:27.720]   And it's party mode.
[02:04:27.720 --> 02:04:28.680]   And this was the amazing,
[02:04:28.680 --> 02:04:30.360]   the secret sauce of Sonos.
[02:04:30.360 --> 02:04:33.320]   If you tried to do that with any other speakers,
[02:04:33.320 --> 02:04:35.080]   you'd have weird echo,
[02:04:35.080 --> 02:04:36.360]   because they weren't exactly in sync.
[02:04:36.360 --> 02:04:38.520]   Somehow Sonos made them all synchronized.
[02:04:38.520 --> 02:04:41.560]   Then Google did the same thing with their speakers.
[02:04:41.560 --> 02:04:44.440]   And Sonos said, hey, wait a minute.
[02:04:44.440 --> 02:04:45.720]   This is hard to do.
[02:04:45.720 --> 02:04:46.680]   Hold on there.
[02:04:46.680 --> 02:04:51.480]   The other thing that was worth noting in there is they won.
[02:04:51.480 --> 02:04:52.840]   It was a per unit royalty.
[02:04:52.840 --> 02:04:55.960]   So $2.30 for every unit sold.
[02:04:55.960 --> 02:05:01.240]   So Google sold $14.1 million speaker devices,
[02:05:01.240 --> 02:05:03.240]   which honestly, that's not a lot.
[02:05:03.240 --> 02:05:04.840]   That's a low number.
[02:05:04.840 --> 02:05:05.560]   That's a low number.
[02:05:05.560 --> 02:05:07.320]   I guess it is.
[02:05:07.320 --> 02:05:08.280]   Don't you for...
[02:05:08.280 --> 02:05:10.920]   No, yeah, because I have about five of them.
[02:05:10.920 --> 02:05:14.840]   I mean, I've bought at least four, five.
[02:05:14.840 --> 02:05:16.280]   Yeah, I bought quite a few.
[02:05:16.280 --> 02:05:17.560]   Oh, those were infringing.
[02:05:17.560 --> 02:05:19.960]   That's a lot of talk, isn't it?
[02:05:19.960 --> 02:05:23.480]   So a jury in San Francisco found Google
[02:05:24.280 --> 02:05:28.920]   liable on Friday, and they're going to have to pay out 32 and a half mill.
[02:05:28.920 --> 02:05:36.360]   Google has been pulling features from the speakers and displays prior to the results.
[02:05:36.360 --> 02:05:38.760]   So they clearly felt like this was going to be better.
[02:05:38.760 --> 02:05:46.040]   Google has sued Sonos in return, and that trial is ongoing.
[02:05:46.040 --> 02:05:48.040]   So may not really be over.
[02:05:48.040 --> 02:05:48.520]   I don't know.
[02:05:50.120 --> 02:05:55.960]   Sonos is the plucky little guy here, and they did own that market, and it did hurt them, I'm sure,
[02:05:55.960 --> 02:05:58.440]   that everybody now can do this party mode.
[02:05:58.440 --> 02:05:59.800]   There's nobody that can, I think.
[02:05:59.800 --> 02:06:01.560]   Somebody figured it out.
[02:06:01.560 --> 02:06:06.040]   Sorry, Jeff.
[02:06:06.040 --> 02:06:08.040]   I stole a story from you, but you know right ahead.
[02:06:08.040 --> 02:06:08.760]   Jeff's happy?
[02:06:08.760 --> 02:06:11.400]   I think that's about it.
[02:06:11.400 --> 02:06:13.560]   Let's do this.
[02:06:13.560 --> 02:06:16.440]   Wait, I can't steal from Jeff.
[02:06:16.440 --> 02:06:17.720]   I think it's over.
[02:06:17.720 --> 02:06:18.440]   Come on.
[02:06:18.440 --> 02:06:19.160]   It's over.
[02:06:20.120 --> 02:06:24.120]   Well, we should, we shouldn't, let me complain for about this is that Twitter has
[02:06:24.120 --> 02:06:27.480]   the one place where academics could study social media.
[02:06:27.480 --> 02:06:28.440]   All those bastards.
[02:06:28.440 --> 02:06:30.760]   And they've completely cut that off now.
[02:06:30.760 --> 02:06:33.960]   And so this, you can get Facebook data, you can get Twitter data.
[02:06:33.960 --> 02:06:35.720]   It's, it's awful.
[02:06:35.720 --> 02:06:39.640]   And, and not data doesn't matter as much as I love it.
[02:06:39.640 --> 02:06:43.640]   And so, yeah, you can't get it for free is what you're saying.
[02:06:43.640 --> 02:06:44.120]   No, right.
[02:06:44.120 --> 02:06:46.440]   Yeah, they want to charge you millions, one millions, one millions.
[02:06:46.440 --> 02:06:48.760]   They can do that, right?
[02:06:49.720 --> 02:06:51.000]   Well, yeah, but it isn't.
[02:06:51.000 --> 02:06:56.040]   Wait, like usually you carve out options for researchers.
[02:06:56.040 --> 02:07:00.040]   And so it's one way to be like when, when you've got an actual researcher trying to
[02:07:00.040 --> 02:07:04.520]   do something to say, no, I don't think you should be able to look this stuff up.
[02:07:04.520 --> 02:07:05.800]   We're going to keep this all secret.
[02:07:05.800 --> 02:07:06.600]   That's okay.
[02:07:06.600 --> 02:07:07.960]   Facebook did it for a while.
[02:07:07.960 --> 02:07:09.560]   Yeah, that is a problem.
[02:07:09.560 --> 02:07:14.600]   But I mean, I could see them saying, you know, we can charge you a smaller fee, a much,
[02:07:14.600 --> 02:07:15.240]   much smaller fee.
[02:07:15.240 --> 02:07:17.640]   I think if you already bought it, you should get to keep it.
[02:07:18.200 --> 02:07:21.880]   I don't understand how they can demand that you delete stuff you already bought.
[02:07:21.880 --> 02:07:23.640]   That's what puzzles me.
[02:07:23.640 --> 02:07:24.760]   Maybe you only rented it.
[02:07:24.760 --> 02:07:26.440]   I guess that was the deal.
[02:07:26.440 --> 02:07:27.400]   You didn't really own it.
[02:07:27.400 --> 02:07:31.880]   But a lot of researchers are complaining because they, they aren't going to be able to
[02:07:31.880 --> 02:07:35.480]   keep the data that they purchased for research studies.
[02:07:35.480 --> 02:07:37.560]   Did they not download it?
[02:07:37.560 --> 02:07:38.840]   Yeah, they downloaded it.
[02:07:38.840 --> 02:07:43.160]   But, but they, but Twitter saying, no, you now you have to delete it.
[02:07:43.160 --> 02:07:45.000]   So it must have been an ongoing license, right?
[02:07:45.800 --> 02:07:47.960]   And, and they don't want to break the rules.
[02:07:47.960 --> 02:07:49.080]   So they're going to delete it.
[02:07:49.080 --> 02:07:55.720]   So today I just talked to folks who were, uh, convened the black Twitter summit.
[02:07:55.720 --> 02:07:58.840]   And some of them were working on a new neat project, which we should mention here.
[02:07:58.840 --> 02:08:03.400]   If you go to better platform.net is to work on a people's history of Twitter.
[02:08:03.400 --> 02:08:04.280]   Oh, I love that.
[02:08:04.280 --> 02:08:07.000]   I'm trying to raise so many.
[02:08:07.000 --> 02:08:13.880]   I'm a little, I think, you know, so I went on a blue sky where apparently it was decided.
[02:08:14.920 --> 02:08:19.560]   Some people, black Twitter movers decided, no, we're not going to do massive on there.
[02:08:19.560 --> 02:08:20.600]   I'm not, not welcoming.
[02:08:20.600 --> 02:08:22.600]   And so they went to blue sky.
[02:08:22.600 --> 02:08:24.920]   And now there's a massive moderation.
[02:08:24.920 --> 02:08:28.200]   Most every time I go to blue sky, all they're talking about is moderation.
[02:08:28.200 --> 02:08:31.640]   Because they really screwed up, they screwed the puppy on one, on one bad case.
[02:08:31.640 --> 02:08:35.640]   One of the people who convinced a lot of black Twitter people to come on to blue sky,
[02:08:35.640 --> 02:08:40.840]   then got harassed by a racist and they did finally take down the post,
[02:08:40.840 --> 02:08:44.360]   but they didn't take down the account and they went back and forth and now they have new rules.
[02:08:44.360 --> 02:08:50.280]   They're learning as they go and they screwed up a big case early on and people are angry about that.
[02:08:50.280 --> 02:08:52.760]   So this is the almost they were.
[02:08:52.760 --> 02:08:53.160]   I see.
[02:08:53.160 --> 02:08:55.480]   Blue sky is not as fun as I wanted it to be.
[02:08:55.480 --> 02:08:56.120]   I'll be honest.
[02:08:56.120 --> 02:08:58.520]   Well, it goes through weird.
[02:08:58.520 --> 02:08:59.960]   Now there are more than 100,000 now.
[02:08:59.960 --> 02:09:04.840]   But it goes through these weird, you know, for a while it was all about alph.
[02:09:04.840 --> 02:09:08.920]   It goes through these weird things where everybody's silly.
[02:09:08.920 --> 02:09:11.480]   They're silly and then they got very serious about moderation.
[02:09:13.400 --> 02:09:14.840]   Yeah, that's too bad.
[02:09:14.840 --> 02:09:17.400]   I feel like an opportunity was missed.
[02:09:17.400 --> 02:09:22.040]   I mean, if you really wanted a black Twitter diaspora, you could be,
[02:09:22.040 --> 02:09:26.120]   why didn't somebody start a mastodon instance that was black mastodon?
[02:09:26.120 --> 02:09:27.080]   Why didn't they just do that?
[02:09:27.080 --> 02:09:30.360]   Well, that was part of the issue.
[02:09:30.360 --> 02:09:34.200]   This was Jonathan Flowers, who was one of the commuters of the white Twitter summit.
[02:09:34.200 --> 02:09:37.960]   You have that discussion and people come and we'll make your own.
[02:09:38.680 --> 02:09:44.200]   And so we don't really care about your lived experience for mastodon as a whole,
[02:09:44.200 --> 02:09:45.320]   but you can go have your.
[02:09:45.320 --> 02:09:46.040]   That's not true.
[02:09:46.040 --> 02:09:51.160]   If any, and I certainly Andrea Langston and a number of black users,
[02:09:51.160 --> 02:09:53.800]   this guy named Anpruit are on Twitch social.
[02:09:53.800 --> 02:09:55.800]   They're you don't feel unwelcome, do you?
[02:09:55.800 --> 02:09:56.200]   No.
[02:09:56.200 --> 02:09:56.440]   No.
[02:09:56.440 --> 02:09:59.240]   I'm all right very carefully to make sure nobody's harassed.
[02:09:59.240 --> 02:10:00.040]   No, that wasn't that.
[02:10:00.040 --> 02:10:01.480]   That's about going to your point explicitly.
[02:10:01.480 --> 02:10:03.640]   Oh, it's when you tell someone to create your own.
[02:10:03.640 --> 02:10:08.360]   You're also saying in some cases, well, we don't care about your experience
[02:10:08.360 --> 02:10:09.000]   in the whole.
[02:10:09.000 --> 02:10:09.720]   I understand.
[02:10:09.720 --> 02:10:13.400]   I'm not saying that they're more than welcome on anybody's welcome on Twitch social.
[02:10:13.400 --> 02:10:15.320]   There are plenty of instances there would be more than welcome.
[02:10:15.320 --> 02:10:17.000]   Yeah, of course, I didn't see any issue.
[02:10:17.000 --> 02:10:18.440]   Frankly, that people weren't welcome.
[02:10:18.440 --> 02:10:23.960]   And look, it's one of the great things you can do with mastodon that you can't yet do with blue
[02:10:23.960 --> 02:10:26.440]   sky is somebody who's technically literate.
[02:10:26.440 --> 02:10:28.040]   I'll help you do it if you want to do it.
[02:10:28.040 --> 02:10:34.600]   Ant can create a mastodon instance that is absolutely guaranteed to be a home to people
[02:10:34.600 --> 02:10:38.440]   and welcoming to people of color or not or whatever they want.
[02:10:38.440 --> 02:10:42.440]   Anybody can do that as long as they're willing to put in the work is the bottom.
[02:10:42.440 --> 02:10:43.240]   And it's not that hard.
[02:10:43.240 --> 02:10:45.080]   I mean, I don't put in a lot of work.
[02:10:45.080 --> 02:10:47.880]   I'm not a wizard of anything or anything.
[02:10:47.880 --> 02:10:53.080]   I just, I went to a place that offers hosting and I bought the hosting and it worked.
[02:10:53.080 --> 02:10:57.800]   So anyway, good.
[02:10:57.800 --> 02:10:58.040]   Good.
[02:10:58.040 --> 02:10:58.840]   You went to blue sky.
[02:10:58.840 --> 02:11:00.520]   How did that work out for you?
[02:11:00.520 --> 02:11:02.840]   I think any central isolutions are bad ideas.
[02:11:02.840 --> 02:11:05.400]   And well, depends on what they really do with federation.
[02:11:05.400 --> 02:11:11.480]   The one thing they did in the last week was they now have a whole bunch of pick your own algorithms.
[02:11:11.480 --> 02:11:12.360]   Well, then best.
[02:11:12.360 --> 02:11:14.040]   Yes, I know they have those fees.
[02:11:14.040 --> 02:11:14.760]   Best case.
[02:11:14.760 --> 02:11:18.680]   They federate and then you could do what you could already do on mastodon.
[02:11:18.680 --> 02:11:21.480]   I'm straight right here on blue sky instance for black Twitter.
[02:11:21.480 --> 02:11:23.960]   You already got that.
[02:11:23.960 --> 02:11:25.720]   You see, it baffles me.
[02:11:25.720 --> 02:11:27.720]   I think people wanted to be upset.
[02:11:27.720 --> 02:11:31.000]   Anyway, better platform.net.
[02:11:31.000 --> 02:11:37.480]   Which is the folks who wanted to buy Twitter for some time back that they wanted to do a 16.
[02:11:37.480 --> 02:11:39.000]   Well, that happened.
[02:11:39.000 --> 02:11:43.480]   But now they're trying to do a people's history of Twitter and they're bringing in
[02:11:43.480 --> 02:11:48.680]   academics and ex-employees and others and I'm looking to try to raise some money for us.
[02:11:48.680 --> 02:11:49.960]   How do we get a week contribute?
[02:11:49.960 --> 02:11:50.840]   Just go to the site.
[02:11:50.840 --> 02:11:52.760]   Yeah, I think so.
[02:11:52.760 --> 02:11:56.760]   It says up there on the top, collaborate with help, right history.
[02:11:56.760 --> 02:11:58.040]   Click to collaborate.
[02:11:58.040 --> 02:11:59.640]   Nice.
[02:11:59.640 --> 02:12:03.880]   I just need to find more people to follow on blue sky because it's just a bit
[02:12:03.880 --> 02:12:09.000]   yeah, it's boring to me because there's a lot now granted.
[02:12:09.000 --> 02:12:12.200]   There is a lot of activity on there, but I just don't care about half the stuff on there.
[02:12:12.200 --> 02:12:17.320]   I don't want to get on there and continue to talk about the bull crap of US politics or
[02:12:17.320 --> 02:12:21.560]   right or all of the other problems going on.
[02:12:21.560 --> 02:12:27.960]   I just like to go in and have mindless conversation about interesting stuff.
[02:12:28.760 --> 02:12:34.520]   Just once mindless conversation about we have a service that is filled with mindless conversation.
[02:12:34.520 --> 02:12:37.080]   You know, I'm right.
[02:12:37.080 --> 02:12:40.040]   We're hanging out here on podcast at D table.
[02:12:40.040 --> 02:12:44.200]   We have mindless good conversation, you know, even though we have this whole thing.
[02:12:44.200 --> 02:12:44.600]   Thanks a lot.
[02:12:44.600 --> 02:12:46.280]   You found your you found your people.
[02:12:46.280 --> 02:12:47.320]   Yes, that's it.
[02:12:47.320 --> 02:12:48.040]   We're right here.
[02:12:48.040 --> 02:12:51.720]   Now, would you know what I mean though?
[02:12:51.720 --> 02:12:57.320]   It's just I scroll through there and I'm following right now 26 people, but most of my
[02:12:57.320 --> 02:12:58.440]   last part is tick.
[02:12:58.440 --> 02:13:00.520]   Well, that's the thing I had to find who makes some lists.
[02:13:00.520 --> 02:13:06.840]   Most of the people I've most of the people in my stream showing up is tech mean and John
[02:13:06.840 --> 02:13:07.400]   Scalzy.
[02:13:07.400 --> 02:13:09.480]   It's small enough.
[02:13:09.480 --> 02:13:14.360]   If you go to somebody you like, not me and Stacy's not doing it much.
[02:13:14.360 --> 02:13:14.920]   Follow me.
[02:13:14.920 --> 02:13:15.400]   I follow.
[02:13:15.400 --> 02:13:15.640]   I love it.
[02:13:15.640 --> 02:13:16.520]   How do you like the both?
[02:13:16.520 --> 02:13:21.560]   I'm going to do I follow and then go see who they follow and you can build a good list.
[02:13:21.560 --> 02:13:21.960]   Who do I have?
[02:13:21.960 --> 02:13:22.600]   How many of them?
[02:13:22.600 --> 02:13:24.280]   Yeah, that's how I got a couple of people.
[02:13:24.280 --> 02:13:26.280]   I'm following 582.
[02:13:26.280 --> 02:13:27.640]   You're following Mr. Jarvis.
[02:13:27.640 --> 02:13:31.000]   And I'm following 374 people and I'm following good people.
[02:13:31.000 --> 02:13:33.480]   I'm only 110 of their good people.
[02:13:33.480 --> 02:13:34.120]   They're good people.
[02:13:34.120 --> 02:13:35.400]   And you know, it makes it really easy.
[02:13:35.400 --> 02:13:38.920]   If you do that, you could see there's a follow one button right next to it.
[02:13:38.920 --> 02:13:39.480]   You can follow.
[02:13:39.480 --> 02:13:41.320]   It's quite a mix.
[02:13:41.320 --> 02:13:43.800]   It's because my problem and the knocks is in there.
[02:13:43.800 --> 02:13:44.680]   Who would have thought of that?
[02:13:44.680 --> 02:13:46.280]   Here's my problem.
[02:13:46.280 --> 02:13:47.480]   A lot of people first of all,
[02:13:47.480 --> 02:13:49.960]   A, I think it's we got to recognize there's never going to be another Twitter.
[02:13:50.840 --> 02:13:55.320]   It may be the Twitter ends up surviving and becomes Twitter again.
[02:13:55.320 --> 02:13:57.000]   I think that's actually a possibility.
[02:13:57.000 --> 02:13:59.720]   Just like saying we need a new Instagram.
[02:13:59.720 --> 02:14:01.160]   We're not going to get another Instagram.
[02:14:01.160 --> 02:14:01.640]   It's sad.
[02:14:01.640 --> 02:14:02.680]   I'm warning that too.
[02:14:02.680 --> 02:14:03.400]   It's not going to happen.
[02:14:03.400 --> 02:14:04.200]   Instagram's gone.
[02:14:04.200 --> 02:14:04.920]   That's very sad.
[02:14:04.920 --> 02:14:05.720]   It's a real loss.
[02:14:05.720 --> 02:14:10.360]   I talked to Andre Brock Jr. just before the show, the author of Distributed Blackness,
[02:14:10.360 --> 02:14:11.880]   one of the great experts on Twitter.
[02:14:11.880 --> 02:14:13.640]   And you know, he's still there.
[02:14:13.640 --> 02:14:15.560]   And I said, what would it take for you to leave?
[02:14:15.560 --> 02:14:17.800]   And he said, pretty much the wheels just fall off.
[02:14:17.800 --> 02:14:18.120]   Right.
[02:14:18.120 --> 02:14:18.680]   If it stops working.
[02:14:18.680 --> 02:14:19.720]   He stops working.
[02:14:19.720 --> 02:14:23.800]   I don't I'm not there because I don't really want to support what Elon's doing with it.
[02:14:23.800 --> 02:14:26.760]   I feel like adding content to it is supporting it.
[02:14:26.760 --> 02:14:30.520]   I do think that Elon at some point,
[02:14:30.520 --> 02:14:33.800]   Elon's going to tire of this toy where the banks are going to foreclose
[02:14:33.800 --> 02:14:36.760]   and somebody is going to get ahold of it.
[02:14:36.760 --> 02:14:38.680]   And what's the point of the CEO?
[02:14:38.680 --> 02:14:41.480]   Well, it's really interesting.
[02:14:41.480 --> 02:14:44.360]   You raise that because because one of the stories that put on the rundown,
[02:14:44.360 --> 02:14:45.640]   see, I snuck one more in.
[02:14:45.640 --> 02:14:47.880]   I'm kind of saying well played, sir.
[02:14:47.880 --> 02:14:49.560]   Stacy's going to sleep.
[02:14:49.560 --> 02:14:54.200]   Um, is that, um, they hired an ad executive.
[02:14:54.200 --> 02:14:56.360]   He hired an executive as a CEO.
[02:14:56.360 --> 02:15:00.200]   Yet the story said that they basically have given up selling ads and they're just going programmatic.
[02:15:00.200 --> 02:15:04.440]   So that's how you're seeing, you know, more bad crappy ads thrown in there.
[02:15:04.440 --> 02:15:06.440]   So they know that there's not a good ad business there.
[02:15:06.440 --> 02:15:09.960]   They've they've, um, Ben and Jerry's just stopped doing it.
[02:15:09.960 --> 02:15:11.080]   Others are going to stop doing it.
[02:15:11.080 --> 02:15:11.960]   They're not going to come back.
[02:15:11.960 --> 02:15:14.680]   It's worth the third of what it was.
[02:15:14.680 --> 02:15:17.960]   I think it was even more politics.
[02:15:17.960 --> 02:15:18.760]   Yeah.
[02:15:18.760 --> 02:15:22.840]   But there is a, there's something still alive there.
[02:15:22.840 --> 02:15:27.080]   There's a little colonel, you know, there's a, and I do think it's, it seems likely to me that
[02:15:27.080 --> 02:15:27.880]   it's not going to die.
[02:15:27.880 --> 02:15:31.160]   That Elon's going to lose control of it at some point.
[02:15:31.160 --> 02:15:32.360]   Somebody's going to take it over.
[02:15:32.360 --> 02:15:36.520]   You know, maybe it'll be somebody like Conde Nast who has done a decent job with Reddit.
[02:15:36.520 --> 02:15:37.560]   Not perfect, but decent.
[02:15:37.560 --> 02:15:42.200]   Somebody who is a benign corporate overlord who says, well, let's fix this.
[02:15:42.200 --> 02:15:42.760]   Let's fix this.
[02:15:42.760 --> 02:15:46.440]   There's a lot of enough people to keep this running and it comes back.
[02:15:46.440 --> 02:15:48.920]   And I think there's a lot of people like you and who would go back.
[02:15:48.920 --> 02:15:51.800]   It would very happily go back.
[02:15:51.800 --> 02:15:51.800]   I'm not straight up.
[02:15:51.800 --> 02:15:52.840]   I'm not cast mode over there.
[02:15:52.840 --> 02:15:55.800]   All of my feed is just broadcasting stuff for hard hit.
[02:15:55.800 --> 02:15:59.320]   Well, I'm sure it drives Lisa crazy and, or marking people crazy.
[02:15:59.320 --> 02:16:02.200]   I still have half a million people over on Twitter and I don't post.
[02:16:02.200 --> 02:16:07.080]   I'm sure that drives him nuts, but I just don't want to, I don't want to participate.
[02:16:07.080 --> 02:16:09.720]   And, um, but I don't think it's dead.
[02:16:09.720 --> 02:16:12.280]   It's like a tree that does not look good.
[02:16:12.280 --> 02:16:14.440]   It's like my snake plant.
[02:16:14.440 --> 02:16:19.080]   It's, it's kind of brown at the tips and some of the leaves are gone, but it could be,
[02:16:19.080 --> 02:16:20.440]   it could be resuscitated.
[02:16:20.440 --> 02:16:21.560]   Yeah.
[02:16:21.560 --> 02:16:22.680]   And coaches are still there.
[02:16:22.680 --> 02:16:25.400]   I'll say that because I still talk to some coaches.
[02:16:25.400 --> 02:16:28.200]   A lot of people are surprisingly large number of people have not left.
[02:16:28.200 --> 02:16:35.880]   I was a little worried about blue sky because it's a lot of people left Twitter for blue sky.
[02:16:35.880 --> 02:16:40.920]   And it hit it, you know, the, as the problem is a bigger that blue sky gets.
[02:16:41.640 --> 02:16:45.800]   The main instance gets the hardest going to be for federation to actually take off.
[02:16:45.800 --> 02:16:50.120]   But damn it, stop leaving Twitter and go on to blue sky to come talk about Twitter.
[02:16:50.120 --> 02:16:51.080]   Well, that's bad too.
[02:16:51.080 --> 02:16:53.000]   I see a lot of that.
[02:16:53.000 --> 02:16:56.840]   Like you really came over here to, to, to fuss about Twitter.
[02:16:56.840 --> 02:17:03.880]   Blue sky is the closest thing to keeping Twitter alive, but Twitter isn't dead yet.
[02:17:03.880 --> 02:17:07.400]   Um, I don't, I don't know what the answer is.
[02:17:07.400 --> 02:17:08.040]   I really don't.
[02:17:08.040 --> 02:17:11.080]   I love our little mastodon instance, but it's not this, not a Twitter.
[02:17:11.080 --> 02:17:12.280]   It's not going to do that.
[02:17:12.280 --> 02:17:12.280]   Yeah.
[02:17:12.280 --> 02:17:14.200]   Mastodon is a entirely different experience.
[02:17:14.200 --> 02:17:14.440]   Yeah.
[02:17:14.440 --> 02:17:19.160]   I like these, uh, uh, what do they call them?
[02:17:19.160 --> 02:17:20.920]   Feeds, my feeds that they created.
[02:17:20.920 --> 02:17:23.080]   I think this, these are really great.
[02:17:23.080 --> 02:17:25.640]   Oh, I just have not noticed that.
[02:17:25.640 --> 02:17:26.600]   That's a nice new feature.
[02:17:26.600 --> 02:17:27.640]   That's fairly new.
[02:17:27.640 --> 02:17:28.520]   Oh, it's brand new.
[02:17:28.520 --> 02:17:29.880]   So let's go to the live feeds.
[02:17:29.880 --> 02:17:33.720]   You can pick and go to my feeds and settings and then you can pick different feeds that people
[02:17:33.720 --> 02:17:34.120]   have.
[02:17:34.120 --> 02:17:36.200]   They have some base feeds that they've started.
[02:17:36.200 --> 02:17:39.080]   And then there are new feeds that you can subscribe to.
[02:17:39.080 --> 02:17:40.360]   And in mobile, it's even easier.
[02:17:40.360 --> 02:17:41.560]   It's at the bottom.
[02:17:41.560 --> 02:17:46.120]   It's the upside down breast with, uh, things coming off of it.
[02:17:46.120 --> 02:17:47.080]   It's okay.
[02:17:47.080 --> 02:17:50.520]   Same feeds.
[02:17:50.520 --> 02:17:51.000]   Yeah.
[02:17:51.000 --> 02:17:54.200]   So I subscribed to what's hot popular with friends.
[02:17:54.200 --> 02:18:00.440]   So I know what you, my friends are following, mutuals, which are people who we follow each other.
[02:18:00.440 --> 02:18:04.200]   Catch up in case I meet something and then there's a blue sky team has.
[02:18:04.200 --> 02:18:05.480]   This was discovered new feeds.
[02:18:05.480 --> 02:18:06.440]   Click on that.
[02:18:06.440 --> 02:18:07.080]   There's all kinds of things.
[02:18:07.080 --> 02:18:08.360]   Oh, a whole bunch of them.
[02:18:08.360 --> 02:18:09.640]   I don't have any of this.
[02:18:09.640 --> 02:18:12.040]   I don't have discovered new feeds.
[02:18:12.040 --> 02:18:12.200]   Yeah.
[02:18:12.200 --> 02:18:15.800]   So start with go to my feeds, click the gear.
[02:18:15.800 --> 02:18:16.920]   I'm in my feeds.
[02:18:16.920 --> 02:18:17.640]   Click the gear.
[02:18:17.640 --> 02:18:18.920]   Oh, it's a year.
[02:18:18.920 --> 02:18:21.960]   Then at the bottom, discover new feeds.
[02:18:21.960 --> 02:18:24.760]   Then here's the other thing that's confusing.
[02:18:24.760 --> 02:18:27.480]   If you want to find, if you think you're going to find all these feeds,
[02:18:27.480 --> 02:18:29.720]   you've subscribed to under my feeds, you won't find them there.
[02:18:29.720 --> 02:18:32.040]   You find them on the home page on the top.
[02:18:32.040 --> 02:18:32.680]   They become.
[02:18:32.680 --> 02:18:35.400]   If you look at the top here, they become a tab.
[02:18:35.400 --> 02:18:37.640]   There's a black sky.
[02:18:37.640 --> 02:18:39.960]   That's it.
[02:18:39.960 --> 02:18:41.720]   Which is close from black users.
[02:18:41.720 --> 02:18:44.280]   It's an easy way to go to hashtag.
[02:18:44.280 --> 02:18:46.280]   Yeah, I, you know, on Twitter, I have.
[02:18:46.280 --> 02:18:46.360]   Do not pictures.
[02:18:46.360 --> 02:18:47.560]   Oh, yeah.
[02:18:47.560 --> 02:18:48.760]   Don't show pictures.
[02:18:48.760 --> 02:18:51.800]   There was a, there's been weird trends.
[02:18:51.800 --> 02:18:53.320]   Like there was a trend to show bottoms.
[02:18:53.320 --> 02:18:58.520]   Oh, there's like, there's a whole section for ludes and for not suitable for works
[02:18:58.520 --> 02:18:59.640]   and for more nudes.
[02:18:59.640 --> 02:18:59.880]   Yeah.
[02:18:59.880 --> 02:19:01.400]   There's quite a few of those.
[02:19:01.400 --> 02:19:01.640]   Yeah.
[02:19:01.640 --> 02:19:04.360]   But on the other hand, if you follow cat picks,
[02:19:04.920 --> 02:19:07.000]   the cats are naked, but who cares?
[02:19:07.000 --> 02:19:09.000]   Well, that's not a cat.
[02:19:09.000 --> 02:19:14.120]   I like the cat picks feed.
[02:19:14.120 --> 02:19:16.200]   All cats all the time.
[02:19:16.200 --> 02:19:17.320]   Technics.
[02:19:17.320 --> 02:19:19.800]   I still, can I just search sports?
[02:19:19.800 --> 02:19:22.920]   Cause I haven't seen one thing about sports in here.
[02:19:22.920 --> 02:19:23.960]   Well, it's probably.
[02:19:23.960 --> 02:19:26.360]   You could start talking about sports and people will find you
[02:19:26.360 --> 02:19:27.880]   and then you can follow them.
[02:19:27.880 --> 02:19:28.120]   Yeah.
[02:19:28.120 --> 02:19:30.120]   Yeah, let's give that a try.
[02:19:30.120 --> 02:19:34.760]   Cause I have a whole, I have a hand full of toots or skits or whatever
[02:19:34.760 --> 02:19:37.320]   these things are called and it is like nothing.
[02:19:37.320 --> 02:19:38.520]   No response on them.
[02:19:38.520 --> 02:19:38.920]   Hardly.
[02:19:38.920 --> 02:19:40.200]   A fee.
[02:19:40.200 --> 02:19:42.600]   So a list is just a list of people to follow.
[02:19:42.600 --> 02:19:47.720]   The feed can also be created to follow hashtags and other things.
[02:19:47.720 --> 02:19:51.800]   It's a much more sophisticated technology to create feeds.
[02:19:51.800 --> 02:19:54.840]   So just, you know, here's NBA.
[02:19:54.840 --> 02:19:57.720]   Post so then hash seats.
[02:19:57.720 --> 02:19:59.240]   So this is just a hashtag.
[02:19:59.240 --> 02:20:03.400]   If there's a hashtag NBA in it, that'll be all the, that'll be, that'll be in the feed.
[02:20:03.960 --> 02:20:07.240]   So that's where it's a little more sophisticated than just lists.
[02:20:07.240 --> 02:20:08.920]   Oh, podcasting.
[02:20:08.920 --> 02:20:09.800]   Cause I saw that.
[02:20:09.800 --> 02:20:10.280]   I should have that.
[02:20:10.280 --> 02:20:14.200]   Well, I wonder how much stuff is in here.
[02:20:14.200 --> 02:20:16.600]   11, like by 11 users.
[02:20:16.600 --> 02:20:20.760]   So this is by name.
[02:20:20.760 --> 02:20:22.360]   There's Amanda Knox you mentioned.
[02:20:22.360 --> 02:20:27.240]   Anyway, yeah, the good news is there's a lot going on.
[02:20:27.240 --> 02:20:28.200]   Certainly.
[02:20:28.200 --> 02:20:29.000]   And there's a lot going on.
[02:20:29.000 --> 02:20:30.360]   There's a lot of creativity happening.
[02:20:31.400 --> 02:20:34.680]   Obviously the internet community has decided we want something like Twitter.
[02:20:34.680 --> 02:20:35.320]   We need Twitter.
[02:20:35.320 --> 02:20:38.440]   And if it's not going to be Twitter, then let's try some other things.
[02:20:38.440 --> 02:20:39.480]   And I think that's all great.
[02:20:39.480 --> 02:20:40.760]   I don't, I don't, I don't know.
[02:20:40.760 --> 02:20:41.960]   Are you using no straddle?
[02:20:41.960 --> 02:20:44.520]   No, I have an account on it, but I haven't used it.
[02:20:44.520 --> 02:20:47.400]   That's another kind of very similar to blue sky, right?
[02:20:47.400 --> 02:20:49.640]   It has its own Federation protocol.
[02:20:49.640 --> 02:20:51.480]   Jack put money into it.
[02:20:51.480 --> 02:20:54.520]   People who are trying to avoid Jack.
[02:20:54.520 --> 02:20:56.600]   Apple has left scuttlebutt to go there.
[02:20:57.160 --> 02:20:59.400]   Rebel has left scuttlebutt.
[02:20:59.400 --> 02:20:59.800]   Yeah.
[02:20:59.800 --> 02:21:00.360]   Yeah.
[02:21:00.360 --> 02:21:02.600]   That was the story thing.
[02:21:02.600 --> 02:21:04.120]   There's a story in the rundown.
[02:21:04.120 --> 02:21:05.080]   I hate to do it again.
[02:21:05.080 --> 02:21:08.680]   No, no, what is this creeping rundown?
[02:21:08.680 --> 02:21:11.640]   No, no, line 60.
[02:21:11.640 --> 02:21:15.720]   You like the analysis of blue sky versus duster?
[02:21:15.720 --> 02:21:17.560]   Oh, this could be a good 20 minute or.
[02:21:17.560 --> 02:21:23.320]   This wouldn't be so bad, except I get so excited when it's time for a thing.
[02:21:23.320 --> 02:21:26.680]   And I'm like, ladies and gentlemen, and then that goes for like 20 minutes.
[02:21:26.680 --> 02:21:29.160]   And girls, children of all ages, you know what's next?
[02:21:29.160 --> 02:21:31.400]   It's circus time.
[02:21:31.400 --> 02:21:34.840]   No, it's it's going to be our picks of the week coming up.
[02:21:34.840 --> 02:21:39.560]   We'll start the picks of the week this week with Stacey Hugambotham.
[02:21:39.560 --> 02:21:40.280]   What's your thing?
[02:21:40.280 --> 02:21:44.040]   I don't have a thing in.
[02:21:44.040 --> 02:21:46.040]   You were excited and now you don't have a thing.
[02:21:46.040 --> 02:21:47.400]   You're confusing me here.
[02:21:47.400 --> 02:21:48.680]   What's going on, Stacey?
[02:21:48.680 --> 02:21:53.400]   Instead, instead y'all.
[02:21:55.880 --> 02:21:56.920]   Calm yourself.
[02:21:56.920 --> 02:22:00.520]   And I can't remember it.
[02:22:00.520 --> 02:22:02.040]   Did I?
[02:22:02.040 --> 02:22:05.320]   Okay, I read this book called poverty.
[02:22:05.320 --> 02:22:07.240]   Very recently.
[02:22:07.240 --> 02:22:08.280]   Did I tell you about it yet?
[02:22:08.280 --> 02:22:09.960]   No, I think you might have mentioned it.
[02:22:09.960 --> 02:22:11.240]   But I can't remember.
[02:22:11.240 --> 02:22:11.240]   I sure remember.
[02:22:11.240 --> 02:22:12.200]   Maybe I wasn't here.
[02:22:12.200 --> 02:22:15.000]   Okay, so if y'all don't remember it, then that is my pick of the week.
[02:22:15.000 --> 02:22:18.440]   Because I've been reading it and God, man, I feel real bad.
[02:22:18.440 --> 02:22:20.520]   It's called poverty by America.
[02:22:20.520 --> 02:22:23.320]   It's by Matthew Desmond, who is the guy who did evicted.
[02:22:24.760 --> 02:22:26.520]   You need to read this book.
[02:22:26.520 --> 02:22:28.680]   It shows how we...
[02:22:28.680 --> 02:22:30.680]   Do we create poverty?
[02:22:30.680 --> 02:22:33.400]   Make it like individuals.
[02:22:33.400 --> 02:22:37.800]   We have a role in the creation of poverty.
[02:22:37.800 --> 02:22:43.480]   And there's a reason why so many people in America are so incredibly poor.
[02:22:43.480 --> 02:22:47.400]   And I think given the amount of income inequality,
[02:22:47.400 --> 02:22:53.800]   given the amount of hair pulling we have about like dealing with homeless people,
[02:22:54.520 --> 02:23:01.640]   you need to read this and you need to really consider our role and how we,
[02:23:01.640 --> 02:23:06.120]   what we need to do and what we need to give up and how to talk about this.
[02:23:06.120 --> 02:23:12.360]   If he alone had a solution for homelessness, I would be thrilled.
[02:23:12.360 --> 02:23:14.680]   Does he talk about what we can do about it?
[02:23:14.680 --> 02:23:22.840]   He talks about the costs and like nimbism and owning second homes and landlords.
[02:23:23.800 --> 02:23:31.480]   Yeah, so he talks about why we have such systemic poverty and he blames us.
[02:23:31.480 --> 02:23:36.760]   It's not a book that you're going to feel great reading, but you should read it anyway.
[02:23:36.760 --> 02:23:38.920]   It's really...
[02:23:38.920 --> 02:23:45.080]   It's going to make you angry, but probably feel a little guilty too.
[02:23:45.080 --> 02:23:48.760]   So that is my...
[02:23:49.560 --> 02:23:52.360]   You should read A.I. It's capitalism.
[02:23:52.360 --> 02:23:57.160]   It's kind of late stage capitalism. That's the problem.
[02:23:57.160 --> 02:24:01.240]   Did I tell you my book idea? I'm not going to write it because I would never write a book.
[02:24:01.240 --> 02:24:03.560]   It's an entity to write it and it's...
[02:24:03.560 --> 02:24:04.920]   Yeah, you're done.
[02:24:04.920 --> 02:24:13.640]   So my book topic is why AI and the IOT are incompatible with capitalism.
[02:24:13.640 --> 02:24:17.400]   It is basically we are building ourselves into this coercive,
[02:24:17.400 --> 02:24:21.800]   so the surveillance state that is never going to do us any good and it's just going to grind us down.
[02:24:21.800 --> 02:24:23.560]   Oh, no, Shoshua, I'm a Zuboff. Oh, no.
[02:24:23.560 --> 02:24:28.440]   And you can call it moral panic, but I think I would make compelling arguments.
[02:24:28.440 --> 02:24:31.880]   It's more Corey Doctorow than Shoshua, Shana Zuboff. How about that?
[02:24:31.880 --> 02:24:34.520]   Yeah, Corey Doctorow and I were on the same page.
[02:24:34.520 --> 02:24:36.920]   Absolutely agree with Corey in every respect.
[02:24:36.920 --> 02:24:38.920]   Hey, have you read Red Team? Is it Red Team?
[02:24:38.920 --> 02:24:40.680]   No, I haven't read it yet. No, I know. He's...
[02:24:40.680 --> 02:24:41.400]   It's really good.
[02:24:41.400 --> 02:24:42.040]   Is it?
[02:24:42.040 --> 02:24:44.120]   Oh, maybe we'll have to make it a bright title.
[02:24:44.120 --> 02:24:46.040]   We'll have to make it a book. Red Team Blues.
[02:24:46.040 --> 02:24:49.080]   It's about a forensic accountant. You know how exciting they are.
[02:24:49.080 --> 02:24:51.480]   Yay.
[02:24:51.480 --> 02:24:53.640]   He does a good job.
[02:24:53.640 --> 02:24:57.720]   He is the new James Bond. He's a forensic accountant and he's going to be the next James Bond.
[02:24:57.720 --> 02:25:01.480]   I think Corey has goals, visions of having a movie series.
[02:25:01.480 --> 02:25:03.560]   He's talking to Albert R. Broccoli, the whole thing.
[02:25:03.560 --> 02:25:06.600]   I will read it. It's on my list.
[02:25:06.600 --> 02:25:11.320]   I've got, in fact, you know what? Let me add that to my books.
[02:25:11.320 --> 02:25:15.080]   And there's a very good New York Times book review of poverty, if you want to kind of
[02:25:15.960 --> 02:25:18.280]   the short version of the book.
[02:25:18.280 --> 02:25:22.680]   But I did buy it.
[02:25:22.680 --> 02:25:26.440]   You're going to see fewer things from Stacey because Stacey is buying fewer things.
[02:25:26.440 --> 02:25:27.240]   But as a big...
[02:25:27.240 --> 02:25:31.160]   Well, you know what I'm doing now is I got the new Kindle scribe that you write with,
[02:25:31.160 --> 02:25:32.040]   you know? You have...
[02:25:32.040 --> 02:25:32.680]   Oh!
[02:25:32.680 --> 02:25:35.560]   And it's big. It's 8x10. It's like a nice size.
[02:25:35.560 --> 02:25:40.600]   Well, I know it seems big to hold it, but on the other hand, I like it because it's more like reading a book.
[02:25:40.600 --> 02:25:45.320]   And I still feel I like it. So I'm buying more stuff on the Kindle.
[02:25:45.320 --> 02:25:46.760]   How do you compare it to the remarkable?
[02:25:46.760 --> 02:25:53.080]   Well, I'm glad you asked because this Sunday I will be doing a rundown of the Kindle scribe,
[02:25:53.080 --> 02:25:58.040]   the remarkable two, and an iPad running good note software and the pros and cons of each.
[02:25:58.040 --> 02:25:59.240]   You said you're doing this Sunday.
[02:25:59.240 --> 02:26:00.440]   Do you need your Kobo back?
[02:26:00.440 --> 02:26:00.920]   Do you need your Kobo back?
[02:26:00.920 --> 02:26:03.480]   It would be on a show that I like to call Ask the Tech guys.
[02:26:03.480 --> 02:26:04.440]   Oh, okay.
[02:26:04.440 --> 02:26:06.120]   Thank you for asking. Good show.
[02:26:06.120 --> 02:26:08.040]   Sunday 11 a.m. to 2 p.m.
[02:26:08.040 --> 02:26:08.840]   Little preview.
[02:26:08.840 --> 02:26:09.880]   Do you still use this like this?
[02:26:09.880 --> 02:26:10.760]   Call in show?
[02:26:10.760 --> 02:26:12.520]   No, I don't. I don't use the remarkable...
[02:26:13.560 --> 02:26:17.160]   I'm not a pen guy and the reason, but this is what I'm going to...
[02:26:17.160 --> 02:26:20.360]   I'll give you a preview of what I'm probably going to say is it really depends on your use case.
[02:26:20.360 --> 02:26:20.760]   Right.
[02:26:20.760 --> 02:26:25.640]   So for me, the problem with the remarkable, you can read books on it, but it's not a book reader.
[02:26:25.640 --> 02:26:27.240]   The Kindle is it has a backlight.
[02:26:27.240 --> 02:26:32.600]   It's great for reading books and it adds the annotation and the note taking capability.
[02:26:32.600 --> 02:26:33.480]   What it lacks.
[02:26:33.480 --> 02:26:34.360]   LKPD-UPS.
[02:26:34.360 --> 02:26:38.280]   Yes, you can, but what it lacks is the remarkable.
[02:26:38.280 --> 02:26:41.960]   It has an app and you can just see what you've written on the app.
[02:26:41.960 --> 02:26:46.200]   Remember, we went over this and you never got it working, but it works or on the desktop.
[02:26:46.200 --> 02:26:49.960]   But now they've added a subscription, which is I'm not thrilled about.
[02:26:49.960 --> 02:26:58.360]   So if export, if you want to read legal briefs, annotate them and export that out,
[02:26:58.360 --> 02:27:00.520]   the remarkable is obviously the choice.
[02:27:00.520 --> 02:27:06.600]   But for me, this is a book reader that allows me to do more with annotations and have a notebook
[02:27:06.600 --> 02:27:11.320]   as well, because I sometimes need to sketch stuff out, especially when I'm doing my coding
[02:27:11.320 --> 02:27:13.400]   challenges. I need to sketch things out and stuff.
[02:27:13.400 --> 02:27:15.400]   And so I need a little notebook to do that with.
[02:27:15.400 --> 02:27:16.360]   And this is a good choice.
[02:27:16.360 --> 02:27:17.080]   Anyway, cool.
[02:27:17.080 --> 02:27:22.200]   I think it'll be an interesting review, but as a result, I'm buying more stuff to read.
[02:27:22.200 --> 02:27:24.440]   Like I'm going to buy poverty on the Kindle.
[02:27:24.440 --> 02:27:25.800]   I did buy poverty on the Kindle.
[02:27:25.800 --> 02:27:29.400]   Don't buy Corey Doctorow's book on the Kindle because he will straight up come.
[02:27:29.400 --> 02:27:31.160]   Yeah, you know better than that.
[02:27:31.160 --> 02:27:31.880]   That's a good point.
[02:27:31.880 --> 02:27:33.080]   How do I?
[02:27:33.080 --> 02:27:36.520]   I have to go to.
[02:27:36.520 --> 02:27:39.000]   Well, you do not want to do that.
[02:27:39.000 --> 02:27:41.720]   I can get an ebook and mail it to him though.
[02:27:41.720 --> 02:27:44.280]   No, no, I can get the ebook and mail it to me.
[02:27:44.280 --> 02:27:46.120]   But you can get it as a PDF as a as a as a.
[02:27:46.120 --> 02:27:48.680]   Yeah, let's see the format as an open like a ebook.
[02:27:48.680 --> 02:27:49.080]   E-pub.
[02:27:49.080 --> 02:27:49.560]   E-pub.
[02:27:49.560 --> 02:27:49.960]   E-pub.
[02:27:49.960 --> 02:27:51.240]   Not Moby.
[02:27:51.240 --> 02:27:51.880]   Not Moby.
[02:27:51.880 --> 02:27:52.440]   E-pub.
[02:27:52.440 --> 02:27:55.800]   Mr. Jeff Jarvis, do you have a number?
[02:27:55.800 --> 02:27:58.360]   Uh, sure I do.
[02:27:58.360 --> 02:27:58.920]   Sure you do.
[02:27:58.920 --> 02:28:01.400]   I have at least five or six of them to keep space in your office.
[02:28:01.400 --> 02:28:01.960]   Give us one.
[02:28:01.960 --> 02:28:02.760]   I, um,
[02:28:02.760 --> 02:28:04.920]   uh,
[02:28:04.920 --> 02:28:07.720]   well, I'm going to plug.
[02:28:07.720 --> 02:28:10.680]   Uh, really interesting discussion between Reed Hoffman and Trevor Noah.
[02:28:10.680 --> 02:28:15.080]   Reed's doing a podcast series of famous people because he's
[02:28:15.080 --> 02:28:15.640]   read often.
[02:28:15.640 --> 02:28:17.160]   They'll, they'll listen to the film.
[02:28:17.160 --> 02:28:17.960]   They'll come on his show.
[02:28:17.960 --> 02:28:21.560]   And Trevor, I didn't know, has been doing work with Microsoft for years.
[02:28:21.560 --> 02:28:22.920]   He's very knowledgeable about AI.
[02:28:22.920 --> 02:28:25.560]   He was the voice of AI in Black Panther, which I didn't realize.
[02:28:25.560 --> 02:28:28.200]   And so they had a religious conversation about AI.
[02:28:28.200 --> 02:28:29.080]   I wanted to plug that.
[02:28:29.080 --> 02:28:34.120]   Google is off-loading, trying to off-loading it as a proper firm.
[02:28:34.120 --> 02:28:37.960]   1.4 million square feet of Bay Area office space.
[02:28:37.960 --> 02:28:40.840]   They just opened the new huge campus.
[02:28:40.840 --> 02:28:41.400]   Holy cow.
[02:28:41.400 --> 02:28:46.040]   That's a lot of office space down in the peninsula, mainly.
[02:28:46.040 --> 02:28:48.600]   Uh, so geez.
[02:28:48.600 --> 02:28:53.560]   They, you know, it's funny because right before COVID, all these companies
[02:28:53.560 --> 02:28:55.720]   were building out massive office space.
[02:28:55.720 --> 02:28:55.960]   Mm-hmm.
[02:28:55.960 --> 02:28:56.920]   No, exactly.
[02:28:56.920 --> 02:28:57.720]   Oh, speed.
[02:28:57.720 --> 02:28:58.760]   This is sort of related.
[02:28:58.760 --> 02:29:00.600]   Well, San Jose is not really that close.
[02:29:00.600 --> 02:29:01.400]   Never mind.
[02:29:01.400 --> 02:29:02.280]   I was going to say.
[02:29:02.280 --> 02:29:05.080]   And then we're doing this huge San Jose project, which they put on.
[02:29:05.080 --> 02:29:10.840]   So Austin, based on the census data, Austin hit the top 10,
[02:29:10.840 --> 02:29:14.280]   like, largest cities based on census data out a couple of weeks ago.
[02:29:14.280 --> 02:29:16.680]   Everybody left California to go to Texas.
[02:29:16.680 --> 02:29:20.760]   They, they left San Jose because it knocked out San Jose.
[02:29:20.760 --> 02:29:21.720]   Wow.
[02:29:21.720 --> 02:29:24.040]   Just throwing that out there.
[02:29:24.040 --> 02:29:26.920]   I'm going to mention one quick thing for AI back to the early part,
[02:29:26.920 --> 02:29:31.320]   was that I saw one paper this week said that they're trying to solve the problem of
[02:29:31.320 --> 02:29:34.600]   faxing AI, which I don't think is solvable with large language models,
[02:29:34.600 --> 02:29:35.880]   which I don't think is solvable.
[02:29:35.880 --> 02:29:40.920]   One thing they're doing, one experiment was to have one AI cross examine the other.
[02:29:40.920 --> 02:29:43.800]   And that that helped.
[02:29:43.800 --> 02:29:45.080]   Which is an interesting approach.
[02:29:45.080 --> 02:29:47.080]   That's it.
[02:29:47.080 --> 02:29:47.480]   I'm done.
[02:29:47.480 --> 02:29:48.760]   Mr.
[02:29:48.760 --> 02:29:49.400]   Hand Pruitt.
[02:29:49.400 --> 02:29:57.240]   With my pick the week, I thought about Mr. Jarvis, but again,
[02:29:57.240 --> 02:29:59.160]   I, there's another one that I wanted to share.
[02:29:59.160 --> 02:30:00.680]   And I said, no, I'm not going to torture him.
[02:30:00.680 --> 02:30:01.880]   So I wanted to go with this one.
[02:30:01.880 --> 02:30:07.480]   And it's talking science and sports with Stephen A Smith and Neil DeGrasse Tyson.
[02:30:07.480 --> 02:30:08.120]   Oh, interesting.
[02:30:08.120 --> 02:30:10.040]   On Stephen A Smith's show.
[02:30:10.040 --> 02:30:13.880]   And it was a fabulous, fabulous conversation.
[02:30:13.880 --> 02:30:19.240]   I wasn't, wasn't a lot of sports talk on it, but it was just really, really good information
[02:30:19.240 --> 02:30:20.760]   between these two gentlemen.
[02:30:20.760 --> 02:30:22.760]   I'm really unhappy.
[02:30:22.760 --> 02:30:26.280]   I was supposed to tomorrow morning, I was supposed to go and,
[02:30:27.560 --> 02:30:31.880]   sit down for a fire side with Neil DeGrasse Tyson after at an AI event.
[02:30:31.880 --> 02:30:38.840]   Oh, they canceled me because they wanted somebody who would speak to the AI CEOs there.
[02:30:38.840 --> 02:30:40.280]   Wow.
[02:30:40.280 --> 02:30:41.720]   Why not you?
[02:30:41.720 --> 02:30:42.920]   I don't know.
[02:30:42.920 --> 02:30:43.640]   I'm insulted.
[02:30:43.640 --> 02:30:45.560]   Who did they pick instead?
[02:30:45.560 --> 02:30:46.200]   I'm curious.
[02:30:46.200 --> 02:30:47.240]   Find out.
[02:30:47.240 --> 02:30:48.600]   They said, they'll give you a ticket.
[02:30:48.600 --> 02:30:49.480]   Well, no, thank you.
[02:30:49.480 --> 02:30:49.960]   That tells you.
[02:30:49.960 --> 02:30:52.520]   Oh, there's a little something going on there.
[02:30:52.520 --> 02:30:52.920]   Wow.
[02:30:52.920 --> 02:30:55.560]   They must have, they used to realize you work with me and they said, no.
[02:30:55.560 --> 02:30:56.040]   Yeah.
[02:30:56.040 --> 02:30:57.480]   It can't be on all you.
[02:30:57.480 --> 02:30:58.120]   Can't use it.
[02:30:58.120 --> 02:30:58.840]   All you.
[02:30:58.840 --> 02:31:01.960]   Steven A Smith talks science.
[02:31:01.960 --> 02:31:02.360]   Yep.
[02:31:02.360 --> 02:31:07.160]   As I remember Mr. Jarvis meant, Steven A Smith or was in the same room with him
[02:31:07.160 --> 02:31:08.360]   recently at a conference.
[02:31:08.360 --> 02:31:08.840]   Yes, it was.
[02:31:08.840 --> 02:31:09.240]   Something else.
[02:31:09.240 --> 02:31:09.720]   It was.
[02:31:09.720 --> 02:31:11.880]   And I'd be in the same room with Neil DeGrasse Tyson.
[02:31:11.880 --> 02:31:13.640]   And it was really good.
[02:31:13.640 --> 02:31:14.440]   I enjoyed it.
[02:31:14.440 --> 02:31:15.960]   It's about an hour long conversation.
[02:31:15.960 --> 02:31:16.120]   Nice.
[02:31:16.120 --> 02:31:17.480]   Just thank you.
[02:31:17.480 --> 02:31:21.800]   Because I'm a TD for this week in space.
[02:31:21.800 --> 02:31:27.320]   And I joke with Mr. Rod Powell all the time about how I really don't give a crap about
[02:31:27.320 --> 02:31:28.200]   space.
[02:31:28.200 --> 02:31:33.560]   But I have a good time every time I'm writing is gonna listen to your blog.
[02:31:33.560 --> 02:31:38.200]   And Tardic talk about space because they, they, it's enlightening.
[02:31:38.200 --> 02:31:42.600]   And they put it in a conversation that I can understand that someone that's not
[02:31:42.600 --> 02:31:47.080]   super technical about the, the ins and outs of space and so forth.
[02:31:47.080 --> 02:31:47.320]   Yeah.
[02:31:47.320 --> 02:31:49.320]   So I just recorded an episode.
[02:31:49.320 --> 02:31:49.960]   You know,
[02:31:51.320 --> 02:31:57.160]   I just recorded an episode for like two weeks from now on building LTE networks on the moon.
[02:31:57.160 --> 02:31:58.120]   It was awesome.
[02:31:58.120 --> 02:31:59.160]   Oh wow.
[02:31:59.160 --> 02:32:01.000]   So that comes smart.
[02:32:01.000 --> 02:32:03.640]   I was, I was like, yeah.
[02:32:03.640 --> 02:32:06.840]   You see, you see, that's my slogan now.
[02:32:06.840 --> 02:32:07.960]   Know what's next.
[02:32:07.960 --> 02:32:09.560]   See turn on Twitch.
[02:32:09.560 --> 02:32:10.520]   Turn on Twitch.
[02:32:10.520 --> 02:32:11.640]   LTE on the moon.
[02:32:11.640 --> 02:32:12.600]   Turn on Stacy.
[02:32:12.600 --> 02:32:15.400]   But Stacy's part of it, right?
[02:32:15.400 --> 02:32:17.080]   Stacy on IOT.com.
[02:32:17.080 --> 02:32:17.800]   That's the website.
[02:32:17.800 --> 02:32:20.120]   The podcast is there with Kevin Tofel.
[02:32:20.120 --> 02:32:23.560]   Will that be part of the state of the IOT casters at a separate?
[02:32:23.560 --> 02:32:23.880]   Yeah.
[02:32:23.880 --> 02:32:24.360]   Yeah.
[02:32:24.360 --> 02:32:26.760]   I'll, I'll let you all know when it comes out because I'm really excited about it.
[02:32:26.760 --> 02:32:27.320]   I can't wait to hear it.
[02:32:27.320 --> 02:32:27.560]   Sorry.
[02:32:27.560 --> 02:32:29.640]   I mean, I don't mean to to my own horn, but I'd like.
[02:32:29.640 --> 02:32:32.680]   She's fucked up.
[02:32:32.680 --> 02:32:35.960]   The only reason any of us are here is to toot our own horn.
[02:32:35.960 --> 02:32:38.520]   Well, that's no comment.
[02:32:38.520 --> 02:32:39.240]   We can't hear this.
[02:32:39.240 --> 02:32:41.080]   No comment.
[02:32:41.080 --> 02:32:43.800]   Toot your own horn, Jeff Jarvis.
[02:32:43.800 --> 02:32:48.440]   He is the author of the Gutenberg parenthesis now available for pre-order.
[02:32:48.440 --> 02:32:54.680]   If you go to Gutenberg parenthesis.com, you can get your own copy from a variety of sources.
[02:32:54.680 --> 02:32:56.440]   You choose.
[02:32:56.440 --> 02:32:57.880]   Will there be a Kindle version?
[02:32:57.880 --> 02:32:59.000]   I bet there will.
[02:32:59.000 --> 02:33:03.000]   Yes, there will be a, they just told me, they just told me that they're going to do an audio
[02:33:03.000 --> 02:33:03.480]   version.
[02:33:03.480 --> 02:33:03.640]   Yeah.
[02:33:03.640 --> 02:33:04.200]   Oh, you're kidding.
[02:33:04.200 --> 02:33:04.280]   Yay.
[02:33:04.280 --> 02:33:07.240]   Oh, the late, but yeah, I will do it.
[02:33:07.240 --> 02:33:07.880]   Yeah.
[02:33:07.880 --> 02:33:08.520]   Nice.
[02:33:08.520 --> 02:33:08.840]   Cool.
[02:33:08.840 --> 02:33:10.200]   I'll be that.
[02:33:10.200 --> 02:33:11.000]   I will read it.
[02:33:11.000 --> 02:33:12.600]   Yeah, you should read it.
[02:33:12.600 --> 02:33:13.320]   Absolutely.
[02:33:13.320 --> 02:33:15.960]   You read your other audio books, right?
[02:33:15.960 --> 02:33:16.840]   That what would Google do?
[02:33:16.840 --> 02:33:17.480]   You did that.
[02:33:17.480 --> 02:33:17.480]   Yeah.
[02:33:17.480 --> 02:33:17.960]   Yeah.
[02:33:17.960 --> 02:33:25.800]   But you know, I listened to the audio books mainly at 1.75 to 2x, which is the same little slower than me,
[02:33:25.800 --> 02:33:27.240]   actually, in normal speed.
[02:33:27.240 --> 02:33:31.400]   So when I listen to an audio book at regular speed, I just can't.
[02:33:31.400 --> 02:33:32.760]   It's funny.
[02:33:32.760 --> 02:33:37.720]   I have to start, I have to get used to listening at higher speed because you do that, Lisa does that.
[02:33:37.720 --> 02:33:39.960]   I take forever to read a book.
[02:33:39.960 --> 02:33:43.160]   I mean, it takes me a month to read an audio book because I don't commute anymore.
[02:33:43.160 --> 02:33:45.480]   And so I asked to be like bit here, bit there.
[02:33:45.480 --> 02:33:46.680]   There's one in the quarter.
[02:33:46.680 --> 02:33:47.560]   You should walk.
[02:33:47.560 --> 02:33:48.680]   I do walk.
[02:33:48.680 --> 02:33:51.160]   You know, I'm at 1.65.
[02:33:51.160 --> 02:33:52.760]   Yes, this is the same.
[02:33:52.760 --> 02:33:53.320]   This is the same.
[02:33:53.320 --> 02:33:58.600]   People who go through audio books don't listen at 1.0, same with podcasts, but a lot of people,
[02:33:58.600 --> 02:34:01.880]   when they listen to us in real time, think we're all drunk.
[02:34:01.880 --> 02:34:02.440]   Sounds like, yep.
[02:34:02.440 --> 02:34:06.200]   Well, yeah, because we talk for half a year.
[02:34:06.200 --> 02:34:13.560]   Jeff is the director of the Townite Center for Entrepreneurial Journalism at
[02:34:14.280 --> 02:34:21.480]   the Craig Newmark graduate school of journalism at the University of New York.
[02:34:21.480 --> 02:34:22.200]   Spoke to Craig.
[02:34:22.200 --> 02:34:24.360]   I saw Craig at an event this whole just last night.
[02:34:24.360 --> 02:34:26.120]   Did he say strangle that La Porte?
[02:34:26.120 --> 02:34:26.520]   That's feller.
[02:34:26.520 --> 02:34:30.200]   Something face killer.
[02:34:30.200 --> 02:34:30.760]   Ghost face.
[02:34:30.760 --> 02:34:32.360]   Ghost, fella, killers coming after me.
[02:34:32.360 --> 02:34:33.560]   Ghost face killer.
[02:34:33.560 --> 02:34:36.520]   And Pruitt is, of course, the host of Hands on Photography.
[02:34:36.520 --> 02:34:37.240]   We see him there.
[02:34:37.240 --> 02:34:43.160]   He's all, of course, all over the place at the Twit offices, including in our club where he is the
[02:34:43.800 --> 02:34:45.320]   manager, the community manager.
[02:34:45.320 --> 02:34:49.960]   And you can find his prints at antpruit.com/
[02:34:49.960 --> 02:34:53.720]   And also go get your colonoscopy people.
[02:34:53.720 --> 02:34:56.360]   Is that why you were out?
[02:34:56.360 --> 02:34:57.000]   Yes.
[02:34:57.000 --> 02:34:59.400]   Oh, go get your colonoscopy people.
[02:34:59.400 --> 02:35:00.200]   Not everybody.
[02:35:00.200 --> 02:35:07.560]   As a black man, I am quite concerned and wanted to make sure I get my checkups regularly.
[02:35:07.560 --> 02:35:09.560]   I just read an article that says,
[02:35:10.360 --> 02:35:15.000]   TV doctors want you to get a checkup every year, but really it's a bad idea.
[02:35:15.000 --> 02:35:17.480]   So I get a...
[02:35:17.480 --> 02:35:20.040]   No, colon cancer rates are on the rise.
[02:35:20.040 --> 02:35:23.560]   Younger people, especially black and Latin America.
[02:35:23.560 --> 02:35:25.400]   It used to be 50 years old.
[02:35:25.400 --> 02:35:27.000]   It was the time, but no.
[02:35:27.000 --> 02:35:28.920]   Yeah, I got one on prostate.
[02:35:28.920 --> 02:35:30.040]   And prostate kills.
[02:35:30.040 --> 02:35:32.760]   And I got a sick moi to ask a pea when I was, which is a little...
[02:35:32.760 --> 02:35:34.760]   Let's talk about the details.
[02:35:34.760 --> 02:35:36.440]   It's less than I go and ask a pea.
[02:35:36.440 --> 02:35:38.520]   Guys, less not talk about this.
[02:35:38.520 --> 02:35:39.400]   He said, my chagrin.
[02:35:39.400 --> 02:35:42.200]   He said, my colon was clean as a whistle.
[02:35:42.200 --> 02:35:43.720]   Excellent.
[02:35:43.720 --> 02:35:45.080]   Yeah, that's good, right?
[02:35:45.080 --> 02:35:45.480]   I think.
[02:35:45.480 --> 02:35:47.400]   I can't whistle.
[02:35:47.400 --> 02:35:48.360]   But yeah, get check.
[02:35:48.360 --> 02:35:49.880]   My colon, but whistle for the spot.
[02:35:49.880 --> 02:35:50.120]   Yeah.
[02:35:50.120 --> 02:35:53.800]   Anyway, ask your doctor.
[02:35:53.800 --> 02:35:57.880]   Don't just go out to the colonoscopy shop on the corner and say,
[02:35:57.880 --> 02:35:58.440]   I'm here.
[02:35:58.440 --> 02:36:00.920]   You should get a prescription.
[02:36:00.920 --> 02:36:03.720]   Make sure your doctor says to get one, obviously.
[02:36:03.720 --> 02:36:05.800]   But yeah, don't hide from it.
[02:36:05.800 --> 02:36:07.000]   That's what we're saying.
[02:36:07.000 --> 02:36:07.400]   Come on.
[02:36:07.400 --> 02:36:09.000]   Don't hide from it.
[02:36:09.000 --> 02:36:10.200]   Don't hide from it.
[02:36:10.200 --> 02:36:10.760]   Yeah.
[02:36:10.760 --> 02:36:11.160]   Get your son.
[02:36:11.160 --> 02:36:12.520]   They will prostate folks.
[02:36:12.520 --> 02:36:14.440]   Like for one who's had one.
[02:36:14.440 --> 02:36:16.200]   Well, you know, it's funny, I wanted a PSA.
[02:36:16.200 --> 02:36:18.520]   They do blood tests for prostate.
[02:36:18.520 --> 02:36:19.160]   I wanted a PSA.
[02:36:19.160 --> 02:36:20.680]   My doctor said, we don't do those anymore.
[02:36:20.680 --> 02:36:23.960]   And it turns out, yeah, this is again,
[02:36:23.960 --> 02:36:25.880]   this is a new trend in medicine of...
[02:36:25.880 --> 02:36:26.840]   I still don't.
[02:36:26.840 --> 02:36:28.760]   They don't want to do over-diagnosis.
[02:36:28.760 --> 02:36:30.920]   So you said, before I'll give you a PSA test,
[02:36:30.920 --> 02:36:33.080]   you have to go on the site and read about it,
[02:36:33.080 --> 02:36:35.320]   what the ups and the pros and cons are.
[02:36:35.320 --> 02:36:38.040]   It turns out it's not unusual to have a false positive.
[02:36:39.000 --> 02:36:41.080]   And people have procedures they don't need,
[02:36:41.080 --> 02:36:42.200]   which are life-threatening.
[02:36:42.200 --> 02:36:44.680]   And there's no need for them.
[02:36:44.680 --> 02:36:47.880]   So now they're saying...
[02:36:47.880 --> 02:36:50.920]   On the other hand, it's the same with mammograms though.
[02:36:50.920 --> 02:36:53.880]   You're either a parmesquisitical pool,
[02:36:53.880 --> 02:36:55.160]   where they say, well, okay, you're...
[02:36:55.160 --> 02:36:56.360]   Or you're one person.
[02:36:56.360 --> 02:36:56.840]   Right.
[02:36:56.840 --> 02:37:00.360]   And as one person, I wish I still had mine.
[02:37:00.360 --> 02:37:02.840]   But when they took it out and they did the biopsy,
[02:37:02.840 --> 02:37:05.320]   they said it was the fast growing type.
[02:37:05.320 --> 02:37:06.280]   Oh, that's good. You did it.
[02:37:07.080 --> 02:37:09.160]   I did, by the way, go ahead and get a PSA,
[02:37:09.160 --> 02:37:10.440]   and my PSA was very good.
[02:37:10.440 --> 02:37:13.400]   So I don't know what that means.
[02:37:13.400 --> 02:37:14.920]   But they no longer...
[02:37:14.920 --> 02:37:16.840]   Why this is really TMI.
[02:37:16.840 --> 02:37:20.360]   They no longer do the turn your head and cause thing.
[02:37:20.360 --> 02:37:20.760]   No.
[02:37:20.760 --> 02:37:22.760]   They stopped doing that because for the same reason.
[02:37:22.760 --> 02:37:25.000]   It turned out it wasn't useful.
[02:37:25.000 --> 02:37:25.720]   Right.
[02:37:25.720 --> 02:37:27.160]   And it was no fun for the doctor.
[02:37:27.160 --> 02:37:27.800]   It wasn't even looking for...
[02:37:27.800 --> 02:37:29.080]   Yeah, it was no fun.
[02:37:29.080 --> 02:37:30.840]   They were looking for a large prostate,
[02:37:30.840 --> 02:37:32.520]   but apparently it was not worthwhile.
[02:37:32.520 --> 02:37:34.040]   I thought it was for hemorrhage or something.
[02:37:34.040 --> 02:37:34.440]   Or, or...
[02:37:34.440 --> 02:37:35.400]   Not like for hemorrhoids.
[02:37:35.400 --> 02:37:36.600]   It's a little deeper than that.
[02:37:37.240 --> 02:37:39.320]   I don't know what stuff is.
[02:37:39.320 --> 02:37:41.080]   How far did he roll up his sleeves?
[02:37:41.080 --> 02:37:41.880]   That's a good question.
[02:37:41.880 --> 02:37:47.240]   Ladies and gentlemen, we do this week in Google every Wednesday.
[02:37:47.240 --> 02:37:47.720]   Stop running up his sleeves.
[02:37:47.720 --> 02:37:51.000]   We'll be with you in a moment, Mr. Jarvis.
[02:37:51.000 --> 02:37:52.840]   I can't get it higher.
[02:37:52.840 --> 02:37:55.560]   We do this week in Google every Wednesday,
[02:37:55.560 --> 02:37:57.320]   2 p.m. Pacific 5 p.m. Eastern time.
[02:37:57.320 --> 02:37:59.400]   Jack Hammers permitting.
[02:37:59.400 --> 02:38:04.840]   That's live.tv.
[02:38:04.840 --> 02:38:05.560]   You can watch this.
[02:38:05.560 --> 02:38:07.800]   Laughing you just heard came from the club,
[02:38:07.800 --> 02:38:08.760]   which you better join.
[02:38:08.760 --> 02:38:10.520]   It's a nervous laugh.
[02:38:10.520 --> 02:38:11.480]   Let me put it that way.
[02:38:11.480 --> 02:38:15.240]   After the fact on demand versions of the show,
[02:38:15.240 --> 02:38:18.600]   available at twit.tv/twig.
[02:38:18.600 --> 02:38:19.640]   There's a YouTube channel.
[02:38:19.640 --> 02:38:20.200]   You know what, though?
[02:38:20.200 --> 02:38:22.760]   I really want to urge everybody to subscribe.
[02:38:22.760 --> 02:38:23.880]   It's the easiest thing to do.
[02:38:23.880 --> 02:38:26.120]   Find your favorite podcast application.
[02:38:26.120 --> 02:38:28.120]   Search for twig or search for twit.
[02:38:28.120 --> 02:38:29.720]   Subscribe to all the shows.
[02:38:29.720 --> 02:38:31.160]   Get them automatically.
[02:38:31.160 --> 02:38:33.960]   That way you got plenty of stuff to listen to.
[02:38:33.960 --> 02:38:35.560]   Anytime you've got a free moment,
[02:38:35.560 --> 02:38:38.040]   we'll be glad to be here for you.
[02:38:38.040 --> 02:38:40.760]   So subscribe in your favorite podcast player.
[02:38:40.760 --> 02:38:42.360]   Thank you everybody for being here.
[02:38:42.360 --> 02:38:43.720]   Thank you for putting up with us.
[02:38:43.720 --> 02:38:45.080]   Thank you, Stacey.
[02:38:45.080 --> 02:38:48.040]   We'll be back next week for another long,
[02:38:48.040 --> 02:38:50.600]   grueling episode of This Week in Google.
[02:38:50.600 --> 02:38:51.080]   Bye-bye.
[02:38:51.080 --> 02:38:57.800]   If you love all things, Android,
[02:38:57.800 --> 02:38:59.560]   well, I'm going to show for you to check out.
[02:38:59.560 --> 02:39:01.080]   It's called All About Android.
[02:39:01.080 --> 02:39:03.080]   I'll give you three guesses what we talk about.
[02:39:03.080 --> 02:39:06.920]   We talk about Android, the latest news, hardware, apps.
[02:39:06.920 --> 02:39:07.880]   We answer feedback.
[02:39:07.880 --> 02:39:10.040]   It's me, Jason Howell, Ron Richards,
[02:39:10.040 --> 02:39:13.880]   Winswood Dow, and a whole cast of awesome characters
[02:39:13.880 --> 02:39:16.440]   talking about the operating system that we love.
[02:39:16.440 --> 02:39:20.120]   You can find All About Android at twit.tv/aa.
[02:39:20.120 --> 02:39:31.400]   [Music]

