;FFMETADATA1
title=Trouble at the Cheese Factory
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=457
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.000]   [MUSIC PLAYING]
[00:00:03.000 --> 00:00:05.080]   Netcast you love.
[00:00:05.080 --> 00:00:06.440]   From people you trust.
[00:00:06.440 --> 00:00:10.280]   [MUSIC PLAYING]
[00:00:10.280 --> 00:00:12.720]   This is Twig.
[00:00:12.720 --> 00:00:14.520]   Bandwidth for this week in Google
[00:00:14.520 --> 00:00:19.800]   is provided by cashfly, C-A-C-H-E-F-L-Y.com.
[00:00:19.800 --> 00:00:24.080]   [MUSIC PLAYING]
[00:00:24.080 --> 00:00:28.400]   This is Twig, this week in Google, episode 457,
[00:00:28.400 --> 00:00:31.720]   recorded Wednesday, May 16, 2018.
[00:00:31.720 --> 00:00:35.080]   Trouble at the Cheese Factory.
[00:00:35.080 --> 00:00:37.840]   This week in Google is brought to you by Mughsoft.
[00:00:37.840 --> 00:00:41.280]   Reduce IT alerts and tickets by up to 99%.
[00:00:41.280 --> 00:00:45.440]   Visit Mughsoft.com to learn more and sign up for a demo.
[00:00:45.440 --> 00:00:48.080]   And by WordPress, reach more customers
[00:00:48.080 --> 00:00:51.560]   when you build your business website at WordPress.com.
[00:00:51.560 --> 00:00:53.760]   Plan start at just $4 a month.
[00:00:53.760 --> 00:00:58.360]   Get 15% off any new plan purchase at WordPress.com/twirl.
[00:00:58.360 --> 00:01:00.160]   This week.
[00:01:00.160 --> 00:01:02.200]   This is Twig, ladies and gentlemen.
[00:01:02.200 --> 00:01:04.200]   This week in Google.
[00:01:04.200 --> 00:01:07.800]   And we couldn't have a more googly panel for today's show.
[00:01:07.800 --> 00:01:09.320]   Just waking up from her barbecue,
[00:01:09.320 --> 00:01:12.360]   comma, Stacey Higginbotham in Austin, Texas.
[00:01:12.360 --> 00:01:13.480]   She had Franklin's today.
[00:01:13.480 --> 00:01:14.880]   It looks so mellow.
[00:01:14.880 --> 00:01:17.000]   She's so barbed with mellow.
[00:01:17.000 --> 00:01:18.000]   I am.
[00:01:18.000 --> 00:01:20.320]   If you're lucky, you might see Meat Sweds later.
[00:01:20.320 --> 00:01:23.880]   Oh, it's something to look forward to.
[00:01:23.880 --> 00:01:25.320]   If you want to, you can just lie down
[00:01:25.320 --> 00:01:26.680]   on that Pilates reformer behind you.
[00:01:26.680 --> 00:01:29.680]   Is that an Ember mug you're drinking out of right there?
[00:01:29.680 --> 00:01:31.440]   No, no, this is my just normal mug.
[00:01:31.440 --> 00:01:35.000]   It looks like the Ember mug, the one that is USB powered
[00:01:35.000 --> 00:01:35.840]   and heats.
[00:01:35.840 --> 00:01:38.280]   No, no, that's totally different and it's bigger.
[00:01:38.280 --> 00:01:40.280]   No, no, I have one just like that.
[00:01:40.280 --> 00:01:41.280]   Really?
[00:01:41.280 --> 00:01:42.280]   You have this kind?
[00:01:42.280 --> 00:01:46.920]   Yeah, they make a kind of more standard coffee mug as well.
[00:01:46.920 --> 00:01:47.840]   What did you go for?
[00:01:47.840 --> 00:01:50.880]   The brisket, the ribs, the chicken?
[00:01:50.880 --> 00:01:52.200]   I went for everything.
[00:01:52.200 --> 00:01:55.240]   I was really, my, my.
[00:01:55.240 --> 00:01:56.240]   You are.
[00:01:56.240 --> 00:01:57.240]   I didn't have any sausage.
[00:01:57.240 --> 00:01:58.240]   I'm not a huge sausage.
[00:01:58.240 --> 00:01:59.240]   I always go for the links.
[00:01:59.240 --> 00:02:00.240]   I like the hot links.
[00:02:00.240 --> 00:02:01.640]   Love the links.
[00:02:01.640 --> 00:02:07.600]   But I've been told, ATX Food Guy tells me you got a, you got a, you can only judge a
[00:02:07.600 --> 00:02:09.600]   barbecue by the brisket.
[00:02:09.600 --> 00:02:10.600]   That's true.
[00:02:10.600 --> 00:02:12.080]   And you got a judge by the fatty brisket.
[00:02:12.080 --> 00:02:13.080]   Fatty brisket.
[00:02:13.080 --> 00:02:14.080]   Or the wet brisket.
[00:02:14.080 --> 00:02:15.080]   However, you want to call it.
[00:02:15.080 --> 00:02:16.080]   Oh, God, that's disgusting.
[00:02:16.080 --> 00:02:17.880]   It's delicious.
[00:02:17.880 --> 00:02:21.440]   See, Jeff Jarvis is also here.
[00:02:21.440 --> 00:02:25.680]   He's a professor of journalism at the City University of New York, but in his younger
[00:02:25.680 --> 00:02:32.640]   days, he judged pizza for the, for the, not only did I judge pizza, but I also judged
[00:02:32.640 --> 00:02:37.200]   the Russian River slugfest cookoffs.
[00:02:37.200 --> 00:02:38.800]   What kind of food was it?
[00:02:38.800 --> 00:02:39.800]   Slugs?
[00:02:39.800 --> 00:02:40.800]   I was awful.
[00:02:40.800 --> 00:02:42.440]   The winner of that year was sluggeroni.
[00:02:42.440 --> 00:02:44.000]   This is the Russian River treat.
[00:02:44.000 --> 00:02:45.000]   Oh, God.
[00:02:45.000 --> 00:02:46.000]   Oh, I say.
[00:02:46.000 --> 00:02:51.200]   They tried to make slugs into a positive thing since they were overrun with them.
[00:02:51.200 --> 00:02:52.200]   They're really slugs.
[00:02:52.200 --> 00:02:55.400]   Well, the French ain't snails, but they take the snails and they put the snails in.
[00:02:55.400 --> 00:02:57.600]   They put them in a barrel of meal for a season.
[00:02:57.600 --> 00:02:59.000]   Oh, they did stuff like that to these.
[00:02:59.000 --> 00:03:00.000]   Yes, supposedly.
[00:03:00.000 --> 00:03:01.000]   And then it de-slimes them.
[00:03:01.000 --> 00:03:02.000]   I could do it.
[00:03:02.000 --> 00:03:03.360]   I don't want to eat any food.
[00:03:03.360 --> 00:03:05.000]   You have to de-slime.
[00:03:05.000 --> 00:03:07.280]   Just, you know, I'm just, I'm just saying.
[00:03:07.280 --> 00:03:08.280]   Wait, wait.
[00:03:08.280 --> 00:03:11.400]   No, Oprah has to be de-slimed.
[00:03:11.400 --> 00:03:13.400]   People are saying Oprah what?
[00:03:13.400 --> 00:03:17.560]   Oh, oh, Chris.
[00:03:17.560 --> 00:03:25.000]   And all the way from San Pietro de FÃ©leto in beautiful downtown Italy, Mike Elgin.
[00:03:25.000 --> 00:03:27.280]   He's in a cheese factory.
[00:03:27.280 --> 00:03:29.640]   That's right called Perinzine.
[00:03:29.640 --> 00:03:32.160]   There's so much cheese in this building that I'm in.
[00:03:32.160 --> 00:03:33.360]   It's like not even funny.
[00:03:33.360 --> 00:03:38.200]   There's like walls and walls of Parmesan and all those different cheese.
[00:03:38.200 --> 00:03:39.200]   They have blue spell.
[00:03:39.200 --> 00:03:40.200]   They have blue spell.
[00:03:40.200 --> 00:03:41.200]   Like pinky spell.
[00:03:41.200 --> 00:03:43.000]   Man, I'm so jealous.
[00:03:43.000 --> 00:03:45.800]   Your Prosecco event is coming up, right?
[00:03:45.800 --> 00:03:49.480]   Yeah, that starts Sunday, Monday, Monday.
[00:03:49.480 --> 00:03:51.680]   So people will be arriving in a few days.
[00:03:51.680 --> 00:03:52.680]   Right.
[00:03:52.680 --> 00:03:53.680]   Wow.
[00:03:53.680 --> 00:03:54.680]   That's right.
[00:03:54.680 --> 00:03:58.200]   In fact, it's just going to be ridiculously, I don't know how everybody's going to eat
[00:03:58.200 --> 00:04:01.400]   as much as we've got planned for them to eat.
[00:04:01.400 --> 00:04:05.840]   For that matter, I don't know how much, I don't know how Stacy eats as much as she just
[00:04:05.840 --> 00:04:06.840]   eats.
[00:04:06.840 --> 00:04:09.640]   But it can be done.
[00:04:09.640 --> 00:04:12.280]   It can be done and it will be done.
[00:04:12.280 --> 00:04:16.880]   The key is not to starve beforehand because then your stomach just shrinks.
[00:04:16.880 --> 00:04:19.880]   Oh, you've got to maintain stomach volume.
[00:04:19.880 --> 00:04:21.600]   You've got to build it up.
[00:04:21.600 --> 00:04:22.920]   Build up its strength.
[00:04:22.920 --> 00:04:24.160]   Is that what you're saying?
[00:04:24.160 --> 00:04:25.160]   Yes, basically.
[00:04:25.160 --> 00:04:26.160]   Oh, surprisingly.
[00:04:26.160 --> 00:04:29.960]   I am just, you know what?
[00:04:29.960 --> 00:04:33.280]   I'm just grateful that I don't live in Austin as much as I would like to.
[00:04:33.280 --> 00:04:36.160]   I think I would most likely kill myself.
[00:04:36.160 --> 00:04:38.400]   I mean, I would just overeat and I would just...
[00:04:38.400 --> 00:04:39.600]   But what a way to go.
[00:04:39.600 --> 00:04:42.280]   What a way to go.
[00:04:42.280 --> 00:04:44.720]   So we're all coming down from our Google I/O.
[00:04:44.720 --> 00:04:46.600]   Hi, Stacy was at Build.
[00:04:46.600 --> 00:04:51.120]   Do you feel like, Stacy, that you maybe made the wrong choice?
[00:04:51.120 --> 00:04:57.480]   Actually, no, because I/O did not have a lot of great I/O/T stuff for me personally.
[00:04:57.480 --> 00:05:02.480]   And Build actually had a really compelling and it pushed forward a lot of the conversation
[00:05:02.480 --> 00:05:04.160]   we're having about I/O/T at the edge.
[00:05:04.160 --> 00:05:07.080]   So I actually think I made a good choice.
[00:05:07.080 --> 00:05:08.080]   Well, good.
[00:05:08.080 --> 00:05:14.240]   They gave all of us a press of free I/O/T kits.
[00:05:14.240 --> 00:05:21.760]   So I don't take freebies from press stuff, even cheaper stuff.
[00:05:21.760 --> 00:05:24.080]   They gave everybody a tear, but yes.
[00:05:24.080 --> 00:05:25.080]   I know.
[00:05:25.080 --> 00:05:27.080]   Even when they gave the first...
[00:05:27.080 --> 00:05:28.400]   Remember the first Google phone?
[00:05:28.400 --> 00:05:29.400]   God, what was that called?
[00:05:29.400 --> 00:05:30.400]   The first Android phone.
[00:05:30.400 --> 00:05:31.400]   Oh, the H1.
[00:05:31.400 --> 00:05:34.240]   Yeah, the HTC one.
[00:05:34.240 --> 00:05:38.400]   So they gave that away and I tried it out and then a month later I gave it back.
[00:05:38.400 --> 00:05:39.400]   Yeah.
[00:05:39.400 --> 00:05:43.680]   When they gave out the Pixel, the first Pixel, I guess it was called Pixel.
[00:05:43.680 --> 00:05:45.680]   I don't even remember now.
[00:05:45.680 --> 00:05:47.000]   And it was a very high-end thing.
[00:05:47.000 --> 00:05:48.440]   It was a brand new thing.
[00:05:48.440 --> 00:05:52.040]   And I loved it, but I actually gave it back and then bought one.
[00:05:52.040 --> 00:05:53.040]   Yeah.
[00:05:53.040 --> 00:05:54.040]   That's what I do with...
[00:05:54.040 --> 00:05:55.040]   You guys said it.
[00:05:55.040 --> 00:05:56.040]   You got class.
[00:05:56.040 --> 00:05:57.040]   You got class.
[00:05:57.040 --> 00:06:00.040]   I kept it and bought one.
[00:06:00.040 --> 00:06:01.680]   That's the best of both worlds.
[00:06:01.680 --> 00:06:03.680]   And the next chapter.
[00:06:03.680 --> 00:06:04.680]   That's right.
[00:06:04.680 --> 00:06:09.360]   So I'm now dreading all the stuff that's going to come out this fall that I'm going
[00:06:09.360 --> 00:06:10.360]   to have buy.
[00:06:10.360 --> 00:06:11.360]   I believe me.
[00:06:11.360 --> 00:06:13.360]   I bought plenty.
[00:06:13.360 --> 00:06:17.960]   But I'm dreading there's going to be tons of stuff to buy come fall.
[00:06:17.960 --> 00:06:18.960]   Oh, I hate it.
[00:06:18.960 --> 00:06:19.960]   Yeah.
[00:06:19.960 --> 00:06:25.280]   And every year I say I'm not buying another iPhone/Galaxyphone/ Pixel phone.
[00:06:25.280 --> 00:06:28.080]   And Lisa says, "Well, no, I'm sincere.
[00:06:28.080 --> 00:06:29.080]   I don't want to."
[00:06:29.080 --> 00:06:30.720]   And Lisa says, "You have to."
[00:06:30.720 --> 00:06:31.720]   And it's just right.
[00:06:31.720 --> 00:06:34.000]   I mean, I do have to.
[00:06:34.000 --> 00:06:39.480]   I've been using, since Google I/O, I put, in fact, I think it was during the show, I
[00:06:39.480 --> 00:06:47.000]   put Android P. God, I've got to come up with a better name for that on here.
[00:06:47.000 --> 00:06:48.600]   And I am really liking it.
[00:06:48.600 --> 00:06:50.640]   It's very stable, by the way.
[00:06:50.640 --> 00:06:51.640]   I like it too.
[00:06:51.640 --> 00:06:52.640]   It's really, really nice.
[00:06:52.640 --> 00:06:53.640]   Yeah.
[00:06:53.640 --> 00:06:58.040]   I have one blog where it wouldn't show me all my apps on the AppView reported as a bug.
[00:06:58.040 --> 00:07:00.960]   And you know, Charley's did as well and it's fixed.
[00:07:00.960 --> 00:07:01.960]   Good.
[00:07:01.960 --> 00:07:03.800]   You know, it's working very well.
[00:07:03.800 --> 00:07:05.920]   And the gestures are making sense.
[00:07:05.920 --> 00:07:11.320]   I thought I was going to be sticking my tongue out and be in confused, but no, I'm fine.
[00:07:11.320 --> 00:07:13.040]   You can stick your tongue out for other reasons.
[00:07:13.040 --> 00:07:14.040]   I like, you know what?
[00:07:14.040 --> 00:07:15.040]   It's simple.
[00:07:15.040 --> 00:07:18.760]   I like the fact that when you press the up-down volume button, you get a very nice control
[00:07:18.760 --> 00:07:22.080]   center that makes it very quick and easy to control the volume right there where your
[00:07:22.080 --> 00:07:23.640]   fingers already are.
[00:07:23.640 --> 00:07:24.920]   That seems so simple.
[00:07:24.920 --> 00:07:27.080]   It's those little things that I think make it...
[00:07:27.080 --> 00:07:30.680]   It was a great discussion on old planeted right about how they arrived at that.
[00:07:30.680 --> 00:07:35.240]   And then they just looked at the usage data and said, "People really actually never changed
[00:07:35.240 --> 00:07:36.240]   the rate or volume."
[00:07:36.240 --> 00:07:37.240]   It's either on or off.
[00:07:37.240 --> 00:07:38.240]   Right.
[00:07:38.240 --> 00:07:39.480]   Well, they changed the media volume.
[00:07:39.480 --> 00:07:40.480]   That's exactly right.
[00:07:40.480 --> 00:07:47.280]   So, you're going to be able to see with the simple on/off mute or not, they are a success.
[00:07:47.280 --> 00:07:50.720]   And right above it is the vibrate or silence button.
[00:07:50.720 --> 00:07:51.720]   Right.
[00:07:51.720 --> 00:07:53.000]   That's the call mute.
[00:07:53.000 --> 00:07:54.000]   That's exactly what you want.
[00:07:54.000 --> 00:07:55.000]   Yeah.
[00:07:55.000 --> 00:07:56.000]   Right.
[00:07:56.000 --> 00:07:57.000]   That's exactly what you're on.
[00:07:57.000 --> 00:07:59.640]   And then with the media volume, those are the things you want.
[00:07:59.640 --> 00:08:02.720]   If you really need to, there's a gear there if you want to get deeper into it.
[00:08:02.720 --> 00:08:03.760]   I like that a lot.
[00:08:03.760 --> 00:08:07.440]   The gestures, have you all turned on the gestures?
[00:08:07.440 --> 00:08:08.520]   I really love that.
[00:08:08.520 --> 00:08:10.360]   I think that's just so sensible.
[00:08:10.360 --> 00:08:14.360]   Jason had to show me how to, but I don't know why it's not on by default.
[00:08:14.360 --> 00:08:15.920]   I guess they didn't want to confuse people.
[00:08:15.920 --> 00:08:23.040]   But this is a big shift for Android because it gets rid of the back home recents button
[00:08:23.040 --> 00:08:26.680]   and replaces instead with a lozenge, which you slide.
[00:08:26.680 --> 00:08:29.080]   It's a back button when it's appropriate.
[00:08:29.080 --> 00:08:31.360]   And then there's a back button as appropriate.
[00:08:31.360 --> 00:08:34.440]   The pull down, what do you call those things?
[00:08:34.440 --> 00:08:35.440]   The pull down.
[00:08:35.440 --> 00:08:37.440]   This app here, the control panel or whatever.
[00:08:37.440 --> 00:08:38.440]   Yeah.
[00:08:38.440 --> 00:08:39.440]   I think it's nice.
[00:08:39.440 --> 00:08:44.640]   One thing I'm having trouble getting used to is that you have to hold down the press
[00:08:44.640 --> 00:08:46.280]   on, let's say, Wi-Fi to get Wi-Fi.
[00:08:46.280 --> 00:08:48.040]   Otherwise, it just turns on Roth.
[00:08:48.040 --> 00:08:49.360]   Didn't they always do that?
[00:08:49.360 --> 00:08:50.360]   That's not.
[00:08:50.360 --> 00:08:51.360]   Oh, okay.
[00:08:51.360 --> 00:08:55.880]   You could hold, you could touch the words below and go to the full Wi-Fi settings.
[00:08:55.880 --> 00:08:56.880]   Oh, all right.
[00:08:56.880 --> 00:08:58.240]   Touch the icon and turn it off.
[00:08:58.240 --> 00:08:59.240]   Yes.
[00:08:59.240 --> 00:09:00.840]   If you just tap it, it's a quick off button.
[00:09:00.840 --> 00:09:01.840]   That's what Apple does.
[00:09:01.840 --> 00:09:06.160]   So I guess that's why I didn't throw me so much.
[00:09:06.160 --> 00:09:13.200]   I have to say the usability on P is, it's small details, but it's good enough for instance
[00:09:13.200 --> 00:09:19.320]   that I abandoned my third party launchers, Nova launcher, and I'm using the Pixel launcher.
[00:09:19.320 --> 00:09:20.320]   Yeah.
[00:09:20.320 --> 00:09:23.640]   The only thing, I think we talked about this last week, they got to replace Google Now's
[00:09:23.640 --> 00:09:27.240]   the swipe, when you swipe right, they got to Google Now's.
[00:09:27.240 --> 00:09:28.240]   It's the Google News.
[00:09:28.240 --> 00:09:29.240]   Should have Google News.
[00:09:29.240 --> 00:09:32.040]   I got to Google.
[00:09:32.040 --> 00:09:33.480]   They are good.
[00:09:33.480 --> 00:09:37.600]   I put that as a widget right there on my second page, so I have it.
[00:09:37.600 --> 00:09:38.600]   Are you like it?
[00:09:38.600 --> 00:09:39.600]   Still not Google News.
[00:09:39.600 --> 00:09:41.880]   Oh, wait, no, because I'm not running a P. Never mind.
[00:09:41.880 --> 00:09:43.360]   No, you don't need P.
[00:09:43.360 --> 00:09:44.600]   You don't need P.
[00:09:44.600 --> 00:09:48.520]   So you know the Google Play News stand app?
[00:09:48.520 --> 00:09:50.920]   That updates itself into something called Google News.
[00:09:50.920 --> 00:09:53.760]   So if you don't have that installed, you'll need to install that.
[00:09:53.760 --> 00:09:55.760]   And that'll be another news app.
[00:09:55.760 --> 00:09:56.760]   Yeah.
[00:09:56.760 --> 00:09:57.920]   Well, this is no.
[00:09:57.920 --> 00:09:58.920]   You know what?
[00:09:58.920 --> 00:10:00.600]   Get rid of all the other news apps.
[00:10:00.600 --> 00:10:01.600]   This is the one.
[00:10:01.600 --> 00:10:02.600]   Yeah.
[00:10:02.600 --> 00:10:03.600]   This is the one.
[00:10:03.600 --> 00:10:04.600]   This is the one.
[00:10:04.600 --> 00:10:05.600]   Do you agree, Mike?
[00:10:05.600 --> 00:10:06.600]   Have you been using it?
[00:10:06.600 --> 00:10:07.600]   Totally.
[00:10:07.600 --> 00:10:10.080]   It's really, really good.
[00:10:10.080 --> 00:10:13.080]   They don't try to give you just the personalized news.
[00:10:13.080 --> 00:10:15.240]   They don't try to give you just the generic news.
[00:10:15.240 --> 00:10:16.240]   You get them both.
[00:10:16.240 --> 00:10:17.760]   And it's very intelligent.
[00:10:17.760 --> 00:10:18.760]   It learns.
[00:10:18.760 --> 00:10:22.280]   It's just, it's really the best news app I've ever seen.
[00:10:22.280 --> 00:10:26.720]   And they made a point of saying, and I thought this was really interesting, once you get to
[00:10:26.720 --> 00:10:28.680]   the tab, you know, there's the for you tab.
[00:10:28.680 --> 00:10:33.120]   But once you go to the tab that's headlines or you do the press the button that gives
[00:10:33.120 --> 00:10:37.880]   you full coverage, you get the same news everybody gets.
[00:10:37.880 --> 00:10:43.120]   And I thought that was a really interesting point is that part of the problem is we're
[00:10:43.120 --> 00:10:46.160]   all getting, this is that filter bubble thing, I guess.
[00:10:46.160 --> 00:10:49.200]   We're all getting our own news.
[00:10:49.200 --> 00:10:52.360]   And now we're all getting the same news so we can be on the same page so we can have
[00:10:52.360 --> 00:10:53.360]   a conversation.
[00:10:53.360 --> 00:10:55.200]   I think that's really a good thing.
[00:10:55.200 --> 00:10:56.200]   Part of being informed.
[00:10:56.200 --> 00:11:00.760]   And this is an unappreciated idea, but part of being informed is not just knowing what
[00:11:00.760 --> 00:11:04.960]   you know, but also knowing what other people are, the news that other people are being
[00:11:04.960 --> 00:11:06.040]   exposed to.
[00:11:06.040 --> 00:11:12.280]   So that's something frustrating Google search when I search for something.
[00:11:12.280 --> 00:11:14.000]   The results don't tell me about these results.
[00:11:14.000 --> 00:11:15.280]   I don't know what other people are getting.
[00:11:15.280 --> 00:11:16.280]   That's really frustrating.
[00:11:16.280 --> 00:11:19.180]   So when it comes to news, it's good to go to a place and say, here's what Google is presenting
[00:11:19.180 --> 00:11:20.180]   to the world.
[00:11:20.180 --> 00:11:24.440]   Then if they do something wrong, they screw it up or they have fake news or Russian propaganda
[00:11:24.440 --> 00:11:28.080]   or something like that, you can complain to them.
[00:11:28.080 --> 00:11:29.920]   And if they fix it, they fix it for everyone.
[00:11:29.920 --> 00:11:34.160]   Unlike Facebook where they fix it for you and then nobody else.
[00:11:34.160 --> 00:11:36.400]   These are terrible stories.
[00:11:36.400 --> 00:11:38.000]   You just got it.
[00:11:38.000 --> 00:11:40.320]   Well, that's for the for you page, right?
[00:11:40.320 --> 00:11:41.800]   You're looking for you.
[00:11:41.800 --> 00:11:44.120]   So that's what Google thinks I like?
[00:11:44.120 --> 00:11:46.520]   Well, it'll get better.
[00:11:46.520 --> 00:11:51.080]   Google's given it eight out of the 10 stories are like celebrity gossip like Jennifer Lawrence
[00:11:51.080 --> 00:11:56.000]   Scott, what is this?
[00:11:56.000 --> 00:11:58.160]   Yeah, you need to go, but go into your favorites.
[00:11:58.160 --> 00:11:59.160]   That third tab.
[00:11:59.160 --> 00:12:00.160]   You're kidding.
[00:12:00.160 --> 00:12:02.640]   No, I got dead.
[00:12:02.640 --> 00:12:06.200]   My top story is the FCR, the Senate votes to save net neutrality.
[00:12:06.200 --> 00:12:07.200]   Yes.
[00:12:07.200 --> 00:12:09.920]   But then it knows you know, it knows you.
[00:12:09.920 --> 00:12:14.280]   But it took number seven is Trump discloses payment to Cohen and financial report, which
[00:12:14.280 --> 00:12:19.680]   I feel is larger news way above Deadpool and J Jennifer Lawrence.
[00:12:19.680 --> 00:12:21.160]   What about Deadpool?
[00:12:21.160 --> 00:12:23.560]   Yeah, yeah, above Deadpool.
[00:12:23.560 --> 00:12:25.800]   But you can change that.
[00:12:25.800 --> 00:12:31.720]   Go to favorites and you can and you can select the sources that you want.
[00:12:31.720 --> 00:12:33.480]   I get a lot of getting these.
[00:12:33.480 --> 00:12:35.160]   Is it because I'm a girl?
[00:12:35.160 --> 00:12:36.160]   I hope not.
[00:12:36.160 --> 00:12:39.280]   Because I've got so much celebrity news.
[00:12:39.280 --> 00:12:40.280]   Yes.
[00:12:40.280 --> 00:12:41.880]   See, I disabled that in the favor.
[00:12:41.880 --> 00:12:42.880]   Okay.
[00:12:42.880 --> 00:12:43.880]   All right.
[00:12:43.880 --> 00:12:44.880]   I'm going to favor it.
[00:12:44.880 --> 00:12:45.880]   So sorry to you.
[00:12:45.880 --> 00:12:46.880]   I'm just like, why is this so sucky?
[00:12:46.880 --> 00:12:49.040]   Yeah, I have no celebrity news in my favor.
[00:12:49.040 --> 00:12:51.040]   I love it when you say y'all.
[00:12:51.040 --> 00:12:54.040]   No, no, y'all.
[00:12:54.040 --> 00:12:59.560]   If you were to use the app every day and always ignore the celebrity news and always not ignore
[00:12:59.560 --> 00:13:02.080]   other news, it would fade away.
[00:13:02.080 --> 00:13:03.480]   You can't ignore it.
[00:13:03.480 --> 00:13:04.480]   You can't ignore it.
[00:13:04.480 --> 00:13:06.200]   That's exactly right.
[00:13:06.200 --> 00:13:08.200]   You have to look.
[00:13:08.200 --> 00:13:11.520]   It is bringing you out your true essence.
[00:13:11.520 --> 00:13:12.720]   And this drives me nuts.
[00:13:12.720 --> 00:13:17.480]   So I once once you guys clicked on news about the royal wedding because as a journalist,
[00:13:17.480 --> 00:13:24.480]   I feel like I have to keep abreast of what you were talking about.
[00:13:24.480 --> 00:13:30.240]   And now I have to unlike every single member of the royal family, do you know how many
[00:13:30.240 --> 00:13:34.760]   of them there are and more of them keep popping out?
[00:13:34.760 --> 00:13:35.760]   It's distressing.
[00:13:35.760 --> 00:13:38.680]   You know, Apple News was the same way for me.
[00:13:38.680 --> 00:13:41.040]   It would surface a lot of gossip.
[00:13:41.040 --> 00:13:44.880]   But the problem is those are link-baity headlines and you really kind of almost have
[00:13:44.880 --> 00:13:46.120]   to click on them.
[00:13:46.120 --> 00:13:50.640]   So I don't even want them in my feet at all.
[00:13:50.640 --> 00:13:54.360]   They don't even have them there because that way I won't be tempted.
[00:13:54.360 --> 00:13:55.360]   But you can do that.
[00:13:55.360 --> 00:13:56.640]   You have to go to the favorites there.
[00:13:56.640 --> 00:13:57.640]   I'm very happy.
[00:13:57.640 --> 00:13:58.640]   I agree with you, Mike.
[00:13:58.640 --> 00:13:59.640]   I'm very happy with this.
[00:13:59.640 --> 00:14:00.840]   I think they've done a good job.
[00:14:00.840 --> 00:14:05.840]   I love that they kind of front and center the idea of everybody being, you know, getting
[00:14:05.840 --> 00:14:07.840]   the same feed.
[00:14:07.840 --> 00:14:08.840]   Yeah.
[00:14:08.840 --> 00:14:12.720]   And they finally come up with an idea that I think is better than a newspaper.
[00:14:12.720 --> 00:14:17.440]   So a newspaper, I love newspapers, especially in print.
[00:14:17.440 --> 00:14:26.400]   But the idea that you have an intelligently curated deep dive on multiple publications,
[00:14:26.400 --> 00:14:28.440]   that's really where you start to understand what's happening in the world.
[00:14:28.440 --> 00:14:33.560]   Now, when you read just one publication or when you're awash in whatever social media
[00:14:33.560 --> 00:14:38.640]   throws at you, if you can take a deep dive into an issue and see what the Daily Beast
[00:14:38.640 --> 00:14:42.680]   says about it and what, you know, some other publication says about it, then you
[00:14:42.680 --> 00:14:44.080]   really start to understand it.
[00:14:44.080 --> 00:14:45.080]   And I think that's really important.
[00:14:45.080 --> 00:14:50.120]   Yeah, I think in a way, that's kind of what the New York Times was saying when they said
[00:14:50.120 --> 00:14:52.400]   all the news that's fit to print.
[00:14:52.400 --> 00:14:54.080]   That was the all the news part, right?
[00:14:54.080 --> 00:14:57.800]   But it was still one, I understand it was one editor, one journal.
[00:14:57.800 --> 00:14:58.800]   Exactly.
[00:14:58.800 --> 00:14:59.800]   Yeah.
[00:14:59.800 --> 00:15:02.680]   And I love the New York Times, but it's nice to, you know, sometimes with these filter
[00:15:02.680 --> 00:15:05.360]   bubbles, it sort of magnifies itself.
[00:15:05.360 --> 00:15:09.680]   And sometimes you'll go to it to it, for example, you read a lot of left wing publications,
[00:15:09.680 --> 00:15:15.400]   go to a right wing publication like Reason or National Review, Reason is arguably right
[00:15:15.400 --> 00:15:16.400]   wing.
[00:15:16.400 --> 00:15:21.000]   And you're like, oh my God, they've got a good point that they're making that you never
[00:15:21.000 --> 00:15:23.280]   hear in the usual suspects.
[00:15:23.280 --> 00:15:27.680]   No, I actually subscribed to, I subscribed to the paper version of Reason for that.
[00:15:27.680 --> 00:15:28.680]   Very reason.
[00:15:28.680 --> 00:15:31.120]   It's more libertarian, I'd say than.
[00:15:31.120 --> 00:15:32.120]   It's libertarian.
[00:15:32.120 --> 00:15:34.120]   It is completely a libertarian.
[00:15:34.120 --> 00:15:38.680]   And most of it is, it's not a great, great publication like, you know, The New Yorker
[00:15:38.680 --> 00:15:44.680]   or something like that, but they will put out a perspective consistently that's worth
[00:15:44.680 --> 00:15:45.680]   listening to.
[00:15:45.680 --> 00:15:48.480]   It's thoughtful and intelligent, at least, right?
[00:15:48.480 --> 00:15:51.480]   I mean, it's not the Atlantic, but it's trying, it's trying to be.
[00:15:51.480 --> 00:15:53.080]   And it's a different perspective.
[00:15:53.080 --> 00:15:55.280]   Yes, that's the point.
[00:15:55.280 --> 00:15:59.520]   You know, without getting into withdrawal or hate or anything, it just tries to make
[00:15:59.520 --> 00:16:01.160]   it take it differently, which is fun.
[00:16:01.160 --> 00:16:07.120]   Did you see that Facebook says that they have clobbered in the first three months of
[00:16:07.120 --> 00:16:12.800]   2018, more than half a billion fake accounts?
[00:16:12.800 --> 00:16:14.360]   I hate stuff.
[00:16:14.360 --> 00:16:15.360]   Is that a lot?
[00:16:15.360 --> 00:16:16.360]   That's supposed to be a lot.
[00:16:16.360 --> 00:16:17.360]   I have no idea.
[00:16:17.360 --> 00:16:18.360]   Half a billion is a lot.
[00:16:18.360 --> 00:16:19.360]   Oh, I'm sorry.
[00:16:19.360 --> 00:16:20.360]   Wait a minute.
[00:16:20.360 --> 00:16:21.360]   Wait a minute.
[00:16:21.360 --> 00:16:22.360]   Over a billion users.
[00:16:22.360 --> 00:16:25.440]   Yeah, over six months, 1.3 billion.
[00:16:25.440 --> 00:16:26.440]   Yeah.
[00:16:26.440 --> 00:16:31.440]   So what I say, what if there's a billion that they didn't catch?
[00:16:31.440 --> 00:16:33.240]   But these are a whole accounts.
[00:16:33.240 --> 00:16:39.920]   So if you have two billion accounts and in six months, you've killed 1.3 billion.
[00:16:39.920 --> 00:16:41.080]   That tells you something.
[00:16:41.080 --> 00:16:44.280]   Yeah, well, it tells you the final.
[00:16:44.280 --> 00:16:49.640]   Yeah, it tells you the fake news people are creating accounts as fast as they can.
[00:16:49.640 --> 00:16:52.560]   Because of latency, sometimes Mike, it's going to be a little hard.
[00:16:52.560 --> 00:16:53.560]   Oh, sorry.
[00:16:53.560 --> 00:16:54.560]   It's not much.
[00:16:54.560 --> 00:16:56.720]   Well, because last time I was shooting myself every two seconds.
[00:16:56.720 --> 00:16:57.720]   You're good.
[00:16:57.720 --> 00:16:58.720]   I hear you.
[00:16:58.720 --> 00:16:59.720]   Go ahead.
[00:16:59.720 --> 00:17:00.720]   Well, too much.
[00:17:00.720 --> 00:17:01.720]   All too much.
[00:17:01.720 --> 00:17:02.720]   You can go back to that muting thing, Jeff.
[00:17:02.720 --> 00:17:03.720]   We're enjoying that.
[00:17:03.720 --> 00:17:04.720]   No, it's not.
[00:17:04.720 --> 00:17:11.520]   So at F8, Zuckerberg said on stage that they were killing a million fake accounts a day.
[00:17:11.520 --> 00:17:14.520]   And then someone else on stage later said, "millions."
[00:17:14.520 --> 00:17:15.960]   Yeah.
[00:17:15.960 --> 00:17:17.680]   So we don't even know what the real number is.
[00:17:17.680 --> 00:17:21.200]   It sounds like the scale of this is something we'll solve.
[00:17:21.200 --> 00:17:27.600]   So when somebody said at F8, you know, more wish to report fake accounts to you, they
[00:17:27.600 --> 00:17:28.600]   said, "Well, that's nice.
[00:17:28.600 --> 00:17:31.360]   But it really doesn't help."
[00:17:31.360 --> 00:17:34.560]   That's what I presume is doing kind of the same thing Twitter announced this week, that
[00:17:34.560 --> 00:17:38.960]   they're going to start watching all the signals, which will help them.
[00:17:38.960 --> 00:17:43.520]   Twitter is going to not mute, but slow down accounts that get blocked a lot, things like
[00:17:43.520 --> 00:17:44.520]   that.
[00:17:44.520 --> 00:17:45.520]   Yeah.
[00:17:45.520 --> 00:17:48.600]   I mean, that is the definition.
[00:17:48.600 --> 00:17:52.120]   There are hardcore trolls, and then there are kind of like soft trolls.
[00:17:52.120 --> 00:17:54.520]   Many, many trolls don't know that they're trolls.
[00:17:54.520 --> 00:17:55.840]   Part-time trolls.
[00:17:55.840 --> 00:17:59.720]   They are just reacting and stuff like that.
[00:17:59.720 --> 00:18:02.280]   Yeah, but they do exactly what Twitter is going after.
[00:18:02.280 --> 00:18:05.440]   You have this great conversation going, and some Debbie Downer steps in there and just
[00:18:05.440 --> 00:18:07.880]   derails the whole conversation.
[00:18:07.880 --> 00:18:10.400]   That's a troll, either whether they know it or not.
[00:18:10.400 --> 00:18:13.880]   And that's the kind of thing they say they're going to go after.
[00:18:13.880 --> 00:18:17.600]   Personally, I would prefer, you know, Twitter, everything Twitter does is they're trying
[00:18:17.600 --> 00:18:22.160]   to make sure that, you know, the solution obviously is to give users control over what's
[00:18:22.160 --> 00:18:29.700]   deleted in the comment thread that falls in hopes to tweet.
[00:18:29.700 --> 00:18:32.440]   Just like everybody else, it's like Facebook and everywhere else where you can delete other
[00:18:32.440 --> 00:18:33.440]   people's comments.
[00:18:33.440 --> 00:18:34.680]   They don't want you to be able to do that.
[00:18:34.680 --> 00:18:36.760]   So they're coming up with some automatic way.
[00:18:36.760 --> 00:18:38.560]   The problem with that is it's a black box.
[00:18:38.560 --> 00:18:39.720]   You don't know what they're deleting.
[00:18:39.720 --> 00:18:41.680]   You don't know why they're deleting it.
[00:18:41.680 --> 00:18:47.120]   It's not the kind of thing you can really criticize because you don't know it's happening.
[00:18:47.120 --> 00:18:54.040]   Really good article by Charlie Wartzell and Buzzfeed about fake Facebook profiles that
[00:18:54.040 --> 00:18:56.760]   you can buy to like you.
[00:18:56.760 --> 00:18:59.020]   He refers to this profile.
[00:18:59.020 --> 00:19:03.820]   Audrey Mitchell is a 23 year old New York City transplant from London, an aspiring model
[00:19:03.820 --> 00:19:05.740]   working at KFC.
[00:19:05.740 --> 00:19:11.340]   KFC, I don't know, you know, you got to keep you got to work.
[00:19:11.340 --> 00:19:15.020]   She has 921 friends.
[00:19:15.020 --> 00:19:16.220]   She likes the New York Knicks.
[00:19:16.220 --> 00:19:19.700]   The movies me and Erlen the dying girl.
[00:19:19.700 --> 00:19:20.700]   I guess that's one movie.
[00:19:20.700 --> 00:19:21.700]   I don't know.
[00:19:21.700 --> 00:19:26.540]   And the Saint Paul Minnesota based hip hop duo, idea and abilities.
[00:19:26.540 --> 00:19:34.120]   She's currently single, but she loves cupcakes, but she doesn't exist.
[00:19:34.120 --> 00:19:36.880]   Charlie says, I know this for certain because it just took me a few clicks and one painless
[00:19:36.880 --> 00:19:42.960]   $13 Bitcoin transaction on a Russian website to buy Audrey Mitchell and her believably constructed
[00:19:42.960 --> 00:19:43.960]   digital footprint.
[00:19:43.960 --> 00:19:45.760]   13 Bitcoin.
[00:19:45.760 --> 00:19:47.320]   That's hella expensive.
[00:19:47.320 --> 00:19:49.520]   $13 dollars in Bitcoin.
[00:19:49.520 --> 00:19:51.520]   Oh, I know.
[00:19:51.520 --> 00:19:52.920]   I know 13 Bitcoin.
[00:19:52.920 --> 00:19:55.440]   Hey, you can have a follower too.
[00:19:55.440 --> 00:19:57.040]   I'm like, I'll sell my profile.
[00:19:57.040 --> 00:19:58.040]   Yeah, kidding.
[00:19:58.040 --> 00:19:59.540]   Really for 13 Bitcoin.
[00:19:59.540 --> 00:20:03.760]   I'd sell mine after the transaction and email link offered up a downloadable file containing
[00:20:03.760 --> 00:20:05.800]   the unique phone number.
[00:20:05.800 --> 00:20:09.600]   Audrey's account was registered under a password and a registered birthday.
[00:20:09.600 --> 00:20:13.960]   All the credentials necessary to gain access to the account within 30 minutes.
[00:20:13.960 --> 00:20:16.800]   I was behind the wheel of Audrey's page.
[00:20:16.800 --> 00:20:21.880]   So I guess there, you know, there's a, there's a factory somewhere cranking out fake book,
[00:20:21.880 --> 00:20:22.880]   face, face.
[00:20:22.880 --> 00:20:23.880]   Oh, I like that.
[00:20:23.880 --> 00:20:24.880]   Oh, God.
[00:20:24.880 --> 00:20:25.880]   I'm not going to buy this profile.
[00:20:25.880 --> 00:20:27.880]   I'm going to buy this profile.
[00:20:27.880 --> 00:20:29.880]   I'm going to buy this profile.
[00:20:29.880 --> 00:20:31.880]   I'm going to buy this profile.
[00:20:31.880 --> 00:20:32.880]   I'm going to buy this profile.
[00:20:32.880 --> 00:20:33.880]   I'm going to buy this profile.
[00:20:33.880 --> 00:20:34.880]   I'm going to buy this profile.
[00:20:34.880 --> 00:20:35.880]   I'm going to buy this profile.
[00:20:35.880 --> 00:20:36.880]   I'm going to buy this profile.
[00:20:36.880 --> 00:20:37.880]   I'm going to buy this profile.
[00:20:37.880 --> 00:20:38.880]   I'm going to buy this profile.
[00:20:38.880 --> 00:20:39.880]   I'm going to buy this profile.
[00:20:39.880 --> 00:20:40.880]   I'm going to buy this profile.
[00:20:40.880 --> 00:20:41.880]   I'm going to buy this profile.
[00:20:41.880 --> 00:20:42.880]   I'm going to buy this profile.
[00:20:42.880 --> 00:20:43.880]   I'm going to buy this profile.
[00:20:43.880 --> 00:20:44.880]   I'm going to buy this profile.
[00:20:44.880 --> 00:20:45.880]   I'm going to buy this profile.
[00:20:45.880 --> 00:20:46.880]   I'm going to buy this profile.
[00:20:46.880 --> 00:20:47.880]   I'm going to buy this profile.
[00:20:47.880 --> 00:20:48.880]   I'm going to buy this profile.
[00:20:48.880 --> 00:20:55.880]   I'm going to buy this profile.
[00:20:55.880 --> 00:20:56.880]   I'm going to buy this profile.
[00:20:56.880 --> 00:20:57.880]   I'm going to buy this profile.
[00:20:57.880 --> 00:20:58.880]   I'm going to buy this profile.
[00:20:58.880 --> 00:20:59.880]   I'm going to buy this profile.
[00:20:59.880 --> 00:21:00.880]   I'm going to buy this profile.
[00:21:00.880 --> 00:21:01.880]   I'm going to buy this profile.
[00:21:01.880 --> 00:21:02.880]   I'm going to buy this profile.
[00:21:02.880 --> 00:21:03.880]   I'm going to buy this profile.
[00:21:03.880 --> 00:21:04.880]   I'm going to buy this profile.
[00:21:04.880 --> 00:21:05.880]   I'm going to buy this profile.
[00:21:05.880 --> 00:21:06.880]   I'm going to buy this profile.
[00:21:06.880 --> 00:21:07.880]   I'm going to buy this profile.
[00:21:07.880 --> 00:21:08.880]   I'm going to buy this profile.
[00:21:08.880 --> 00:21:09.880]   I'm going to buy this profile.
[00:21:09.880 --> 00:21:10.880]   I'm going to buy this profile.
[00:21:10.880 --> 00:21:11.880]   I'm going to buy this profile.
[00:21:11.880 --> 00:21:12.880]   I'm going to buy this profile.
[00:21:12.880 --> 00:21:13.880]   I'm going to buy this profile.
[00:21:13.880 --> 00:21:14.880]   I'm going to buy this profile.
[00:21:14.880 --> 00:21:15.880]   I'm going to buy this profile.
[00:21:15.880 --> 00:21:16.880]   I'm going to buy this profile.
[00:21:16.880 --> 00:21:18.880]   I'm going to buy an account that's been around for a little while.
[00:21:18.880 --> 00:21:19.880]   I'm going to invest it in it.
[00:21:19.880 --> 00:21:21.880]   You use that to amplify whatever you're doing.
[00:21:21.880 --> 00:21:22.880]   You buy a hundred accounts.
[00:21:22.880 --> 00:21:24.880]   You somehow or another hundred accounts.
[00:21:24.880 --> 00:21:29.880]   You use that to amplify the crap you're putting out.
[00:21:29.880 --> 00:21:32.880]   It makes sense.
[00:21:32.880 --> 00:21:39.880]   I mean, if you can get $13, well, that's, I mean, you must have, I mean, this is an interesting business.
[00:21:39.880 --> 00:21:40.880]   But it is.
[00:21:40.880 --> 00:21:42.880]   I also did just think else today.
[00:21:42.880 --> 00:21:43.880]   Go ahead.
[00:21:43.880 --> 00:21:48.880]   I'm sorry.
[00:21:48.880 --> 00:21:54.880]   $13 to get a single follower, even though you can control that followers actually quite a bit.
[00:21:54.880 --> 00:21:56.880]   That's a lot of money for one.
[00:21:56.880 --> 00:21:58.880]   If you want a substantial following.
[00:21:58.880 --> 00:21:59.880]   No, but there.
[00:21:59.880 --> 00:22:00.880]   No, no, no.
[00:22:00.880 --> 00:22:03.880]   But see, they're not selling again and they're not selling followers here.
[00:22:03.880 --> 00:22:05.880]   Followers, you can make a lot of money in volume.
[00:22:05.880 --> 00:22:08.880]   They're selling a whole account.
[00:22:08.880 --> 00:22:16.880]   So I can buy 10,000 followers for a few bucks on Twitter because that one account can be sold again and again and again and again.
[00:22:16.880 --> 00:22:19.880]   In this case, you're getting control of the account.
[00:22:19.880 --> 00:22:25.880]   So I think, and considering how much effort must have been put into building that account, it's got to, you know, I mean, this is.
[00:22:25.880 --> 00:22:26.880]   It has to tell you.
[00:22:26.880 --> 00:22:27.880]   Yeah.
[00:22:27.880 --> 00:22:28.880]   Yeah.
[00:22:28.880 --> 00:22:29.880]   It's like.
[00:22:29.880 --> 00:22:31.880]   Okay.
[00:22:31.880 --> 00:22:41.880]   You have a real person that or someone who looks real who then can amplify your messages or let's say it's like a fake reviewer that's credible.
[00:22:41.880 --> 00:22:43.880]   For anything you want to use it for.
[00:22:43.880 --> 00:22:44.880]   Yeah.
[00:22:44.880 --> 00:22:52.880]   I mean, if you had a thousand of those, let's say you're united and you killed a dog, then you get this person to say, I ship my pet on United all the time.
[00:22:52.880 --> 00:22:53.880]   Right.
[00:22:53.880 --> 00:22:54.880]   It does well.
[00:22:54.880 --> 00:22:58.880]   And $13,000 for a company and you have a thousand voices.
[00:22:58.880 --> 00:23:01.880]   That isn't that for a government or for run up.
[00:23:01.880 --> 00:23:03.880]   I found out something else today at great Scott.
[00:23:03.880 --> 00:23:11.880]   So last year at an I/O, the analysis new thing on YouTube where you could pay to buy up your comment, right?
[00:23:11.880 --> 00:23:13.880]   Five bucks in your comment.
[00:23:13.880 --> 00:23:14.880]   Remember that?
[00:23:14.880 --> 00:23:15.880]   Yeah.
[00:23:15.880 --> 00:23:16.880]   Oh, yeah.
[00:23:16.880 --> 00:23:25.880]   So what I found out is that in some of these really dark worlds where they're doing horrible versus misogynist stuff, they're making thousands of dollars that way.
[00:23:25.880 --> 00:23:26.880]   Yeah.
[00:23:26.880 --> 00:23:37.880]   Well, and I have to point out that she's an attractive young lady and which leads me to believe there are other ways you could use this account.
[00:23:37.880 --> 00:23:44.880]   Like as a honeypot to, you know, to get money out of guys, right?
[00:23:44.880 --> 00:23:46.880]   I mean, there's a lot to do with this.
[00:23:46.880 --> 00:23:52.880]   Do I don't know if I want to follow that train of thought?
[00:23:52.880 --> 00:23:57.880]   I don't know, but I'm sure that that's the, I'm almost certain that that's the real, that's how that's going to be used.
[00:23:57.880 --> 00:24:00.880]   I used to do it on.
[00:24:00.880 --> 00:24:02.880]   Those are fake accounts.
[00:24:02.880 --> 00:24:04.880]   Try to befriend you, right, Leo?
[00:24:04.880 --> 00:24:05.880]   Yeah.
[00:24:05.880 --> 00:24:07.880]   You don't send them like weird compromising things.
[00:24:07.880 --> 00:24:09.880]   I don't, but some people might.
[00:24:09.880 --> 00:24:11.880]   And it's not necessarily weird compromising things.
[00:24:11.880 --> 00:24:16.880]   Some of the things you do is you say, send me $10 or I, here's my wish list.
[00:24:16.880 --> 00:24:18.880]   Send me things from my wish list.
[00:24:18.880 --> 00:24:25.880]   There's lots of ways you can monetize an attractive young lady or a fake account by a fake attractive young lady.
[00:24:25.880 --> 00:24:30.880]   I think that does not, that I would guess is the real use for that.
[00:24:30.880 --> 00:24:42.880]   Because if you're looking for, if you're united, let's, I'm sure United doesn't do this, but if you're some company that wanted merely to have a lot of positive comments, you wouldn't have attractive models.
[00:24:42.880 --> 00:24:46.880]   You'd have all kinds of people, mostly schlumpy, mostly real people, right?
[00:24:46.880 --> 00:24:50.880]   I guess to test that theory, we would see how much, how these are valued.
[00:24:50.880 --> 00:24:55.880]   So is she $13 because she's pretty or is she $13 because she has X number of followers?
[00:24:55.880 --> 00:25:00.880]   I think Stacy, you need to do a deep dive.
[00:25:00.880 --> 00:25:03.880]   I already deal with enough gross people in the world.
[00:25:03.880 --> 00:25:05.880]   I don't need to go here.
[00:25:05.880 --> 00:25:06.880]   No, I don't either.
[00:25:06.880 --> 00:25:09.880]   I don't have to go here because I know it's, I'm sure that that's the case.
[00:25:09.880 --> 00:25:12.880]   By the way, we're going to let Mike go because the quality of his.
[00:25:12.880 --> 00:25:14.880]   Oh, I'm sorry.
[00:25:14.880 --> 00:25:16.880]   I find factory line is bad.
[00:25:16.880 --> 00:25:22.880]   No, no, he couldn't, he couldn't participate very well because he of the latency and the, and the.
[00:25:22.880 --> 00:25:23.880]   Or goes, I've been there.
[00:25:23.880 --> 00:25:24.880]   Yeah, you have.
[00:25:24.880 --> 00:25:26.880]   And we've let you go sometimes too.
[00:25:26.880 --> 00:25:32.880]   So I hate to do that, but because I love the contribution, but it's great.
[00:25:32.880 --> 00:25:34.880]   It makes it hard to have the conversation.
[00:25:34.880 --> 00:25:37.880]   I was really hoping for like cheese, a cheese factory tour.
[00:25:37.880 --> 00:25:38.880]   I know.
[00:25:38.880 --> 00:25:42.880]   We should mention though, even though I'm sure it's too late for the.
[00:25:42.880 --> 00:25:49.880]   Prosecco tour, which is coming up this weekend that there are other events you can go to.
[00:25:49.880 --> 00:25:54.880]   If you go to Mike's website, gastronomad.com, click the experiences link.
[00:25:54.880 --> 00:26:01.880]   He's going to have many more events, a provost event coming up next in June, Morocco, November,
[00:26:01.880 --> 00:26:03.880]   Mexico City, December.
[00:26:03.880 --> 00:26:04.880]   Wow.
[00:26:04.880 --> 00:26:06.880]   He and his wife, Amir, Amir, is a brilliant cook.
[00:26:06.880 --> 00:26:09.880]   They, I can't remember how many it is.
[00:26:09.880 --> 00:26:11.880]   It's like 12 people, six couples.
[00:26:11.880 --> 00:26:14.880]   You cook together, you eat together, you see the sites.
[00:26:14.880 --> 00:26:17.880]   They go there ahead of time and become experts in the area.
[00:26:17.880 --> 00:26:18.880]   It's a really nice idea.
[00:26:18.880 --> 00:26:19.880]   Gastronomad.net.
[00:26:19.880 --> 00:26:21.880]   We'll give him a plug.
[00:26:21.880 --> 00:26:27.880]   So this is the article Facebook disabled according to their transparency report.
[00:26:27.880 --> 00:26:29.880]   This is kind of mind-boggling to me.
[00:26:29.880 --> 00:26:31.880]   My boggling, yeah.
[00:26:31.880 --> 00:26:40.880]   Have more than half a billion fake accounts, not to mention millions of spam, sex, and hate speech posts.
[00:26:40.880 --> 00:26:43.880]   I don't know if the internet is--
[00:26:43.880 --> 00:26:44.880]   I don't know if the internet is--
[00:26:44.880 --> 00:26:45.880]   Yeah, well--
[00:26:45.880 --> 00:26:46.880]   It was kind of like that's low.
[00:26:46.880 --> 00:26:48.880]   Do you think they've been doing that all along?
[00:26:48.880 --> 00:26:49.880]   Yeah.
[00:26:49.880 --> 00:26:50.880]   Yeah, I think probably doing more now.
[00:26:50.880 --> 00:26:54.880]   But what's fascinating to me is that we keep on talking about technology being the problem
[00:26:54.880 --> 00:26:56.880]   and the platform being the problem.
[00:26:56.880 --> 00:27:01.880]   What's really happening here is that I heard this conference today is that governments
[00:27:01.880 --> 00:27:05.880]   are putting companies in the position of being an intermediary.
[00:27:05.880 --> 00:27:09.880]   What was somebody called it, "a large-scale micro-justice?"
[00:27:09.880 --> 00:27:14.880]   Where you're expected to make all these judgments and all these decisions.
[00:27:14.880 --> 00:27:20.880]   And there's fear that a lot of legitimate discussion in a free speech goes out with that bathwater.
[00:27:20.880 --> 00:27:28.880]   But yeah, right now the hot seat they're on, they're going to kill stuff right left if they have any clue.
[00:27:28.880 --> 00:27:29.880]   I agree with you.
[00:27:29.880 --> 00:27:33.880]   That's not exactly who you don't want to put in that position, but who else?
[00:27:33.880 --> 00:27:37.880]   So here's the question.
[00:27:37.880 --> 00:27:38.880]   It's really interesting.
[00:27:38.880 --> 00:27:40.880]   I've been thinking about this a lot lately.
[00:27:40.880 --> 00:27:43.880]   Number one, we've met the problem in guess who?
[00:27:43.880 --> 00:27:44.880]   It's human.
[00:27:44.880 --> 00:27:45.880]   It's not technology.
[00:27:45.880 --> 00:27:47.880]   It's not technology that doesn't lead us down this path.
[00:27:47.880 --> 00:27:55.880]   It's that a small number of people manipulate these things in bad ways and we say that the platforms fix it.
[00:27:55.880 --> 00:27:57.880]   But you're fixing a societal problem.
[00:27:57.880 --> 00:27:58.880]   Right?
[00:27:58.880 --> 00:27:59.880]   And that's difficult.
[00:27:59.880 --> 00:28:00.880]   Number one.
[00:28:00.880 --> 00:28:02.880]   Number two, we're concentrating all this energy.
[00:28:02.880 --> 00:28:06.880]   Sorry, so stage number two that I'll shut up.
[00:28:06.880 --> 00:28:09.880]   We're concentrating all this energy on the bad side of things.
[00:28:09.880 --> 00:28:13.880]   All this resource, all this effort is trying to tamp down, which we're never going to do.
[00:28:13.880 --> 00:28:14.880]   We're going to fail at it.
[00:28:14.880 --> 00:28:20.880]   Meanwhile, we're ignoring the quality end where we could be putting a resource in the same ignore that junk.
[00:28:20.880 --> 00:28:21.880]   Yeah, there's junk there.
[00:28:21.880 --> 00:28:22.880]   There's always going to be junk there.
[00:28:22.880 --> 00:28:23.880]   Ignore that.
[00:28:23.880 --> 00:28:25.880]   But here's good stuff and we're going to help you find it.
[00:28:25.880 --> 00:28:26.880]   We don't do that.
[00:28:26.880 --> 00:28:28.880]   And we should be able to space it.
[00:28:28.880 --> 00:28:29.880]   Oh, no.
[00:28:29.880 --> 00:28:30.880]   I'm glad you had the numbers.
[00:28:30.880 --> 00:28:33.880]   That clued me to the fact that I was like, wait, nope.
[00:28:33.880 --> 00:28:43.880]   So I would argue that these platforms, they have to recognize their role as a tool for information,
[00:28:43.880 --> 00:28:46.880]   gathering information or not gathering, delivering information.
[00:28:46.880 --> 00:28:49.880]   And as such, they actually do have a responsibility.
[00:28:49.880 --> 00:28:52.880]   And I argue with Matthew Ingram about this all the time.
[00:28:52.880 --> 00:28:55.880]   He hates the idea that Facebook is going to censor people.
[00:28:55.880 --> 00:29:09.880]   However, if you build a platform with the intent that people will use it, you have to build in controls knowing that some people are going to use it in a way that is counterproductive to good citizenship or civility or whatever else.
[00:29:09.880 --> 00:29:15.880]   And to say, we built it, we have no responsibility.
[00:29:15.880 --> 00:29:18.880]   That's what we've done with the nuclear bomb.
[00:29:18.880 --> 00:29:21.880]   But at the same time, that hasn't turned out super well for people.
[00:29:21.880 --> 00:29:32.880]   So we have to figure out the same sort of frameworks and conversations we have about other tools that we've created that cause problems or could be used for good.
[00:29:32.880 --> 00:29:33.880]   Well, and...
[00:29:33.880 --> 00:29:34.880]   I'll just try.
[00:29:34.880 --> 00:29:35.880]   Go ahead.
[00:29:35.880 --> 00:29:37.880]   We know what happens if you don't do that.
[00:29:37.880 --> 00:29:38.880]   You get Twitter.
[00:29:38.880 --> 00:29:41.880]   So I think...
[00:29:41.880 --> 00:29:44.880]   Twitter is good and bad.
[00:29:44.880 --> 00:29:45.880]   Yeah, but I'm...
[00:29:45.880 --> 00:29:46.880]   I like Twitter.
[00:29:46.880 --> 00:29:48.880]   Twitter is a good for me.
[00:29:48.880 --> 00:29:49.880]   Yeah.
[00:29:49.880 --> 00:29:50.880]   Twitter is good.
[00:29:50.880 --> 00:29:58.880]   Not saying Twitter is not good, but Twitter unfettered free speech on Twitter became a massive problem.
[00:29:58.880 --> 00:30:02.880]   I think that they're actually starting to get a handle on that, but it became a massive problem.
[00:30:02.880 --> 00:30:05.880]   And I don't mean for me personally, I mean, just in general.
[00:30:05.880 --> 00:30:06.880]   Don't...
[00:30:06.880 --> 00:30:07.880]   Well, the problem, of course, it could be seen.
[00:30:07.880 --> 00:30:09.880]   It could be seen now.
[00:30:09.880 --> 00:30:10.880]   It could be heard now.
[00:30:10.880 --> 00:30:17.880]   In every booth at Denny's, there was this kind of idiocy and hatred and awful stuff going on for years, but we couldn't see it.
[00:30:17.880 --> 00:30:18.880]   We couldn't hear it.
[00:30:18.880 --> 00:30:19.880]   That can't be.
[00:30:19.880 --> 00:30:23.880]   I believe it does help push that agenda and cultivate it, though.
[00:30:23.880 --> 00:30:29.880]   It's one thing, because when you find like-minded people who all spout the same thing, you're like, "Hey, I'm totally normal."
[00:30:29.880 --> 00:30:30.880]   Cool.
[00:30:30.880 --> 00:30:31.880]   I don't think Charlottesville...
[00:30:31.880 --> 00:30:37.880]   It's not a coincidence Charlottesville happened in the Twitter era.
[00:30:37.880 --> 00:30:41.880]   I think that that empowered a certain type of person and gave them an...
[00:30:41.880 --> 00:30:46.880]   Just as it helped organize the Arab Spring, it helped organize white supremacists.
[00:30:46.880 --> 00:30:47.880]   It gave them a voice.
[00:30:47.880 --> 00:30:52.880]   It gave them a way to organize, to find each other, and you nailed it, Stacey.
[00:30:52.880 --> 00:30:53.880]   It validated them.
[00:30:53.880 --> 00:30:57.880]   I'm not just some loner in my mom's basement.
[00:30:57.880 --> 00:31:00.880]   It not always to the...
[00:31:00.880 --> 00:31:01.880]   Not always to the negative.
[00:31:01.880 --> 00:31:04.880]   I mean, the same thing the Internet has done for those people.
[00:31:04.880 --> 00:31:05.880]   It's also helped people...
[00:31:05.880 --> 00:31:09.880]   I ran into a bunch of crazy migraine sufferers who were on the same drugs I was on.
[00:31:09.880 --> 00:31:10.880]   100%.
[00:31:10.880 --> 00:31:15.880]   But that's why it is on the platform to promote the good and to...
[00:31:15.880 --> 00:31:17.880]   At best, it can damp down the bad.
[00:31:17.880 --> 00:31:21.880]   That's why I'm saying Twitter is an example of where it got out of control.
[00:31:21.880 --> 00:31:25.880]   Now, I think they're realizing that and they're taking it in hand.
[00:31:25.880 --> 00:31:26.880]   Look at what...
[00:31:26.880 --> 00:31:29.880]   I think their responsibility isn't actively...
[00:31:29.880 --> 00:31:31.880]   It may not be to actively tamp down.
[00:31:31.880 --> 00:31:39.880]   I think their responsibility is to recognize the potential for harm and to take steps to mitigate
[00:31:39.880 --> 00:31:44.880]   may not be the right word, but have those conversations and get the right people involved.
[00:31:44.880 --> 00:31:48.880]   So I actually don't want Twitter, those guys all thinking...
[00:31:48.880 --> 00:31:53.880]   When Facebook decides what's harmful, they ended up banning breastfeeding pictures.
[00:31:53.880 --> 00:31:58.880]   So you have to open up your platform to outside groups.
[00:31:58.880 --> 00:32:04.880]   Honestly, that's going to be different for maybe each geography.
[00:32:04.880 --> 00:32:05.880]   See, this is why it's...
[00:32:05.880 --> 00:32:08.880]   The process for breastfeeding pictures worked out though.
[00:32:08.880 --> 00:32:09.880]   Because they banned it.
[00:32:09.880 --> 00:32:11.880]   Oh my God, it was such a pain though.
[00:32:11.880 --> 00:32:15.880]   But they banned it and then people got together and said, "No, you can't ban it."
[00:32:15.880 --> 00:32:20.880]   And after, I think, a good period of debate, Facebook said, "Oh, yeah, you're right."
[00:32:20.880 --> 00:32:23.880]   That was a period of debate that lasted over a year.
[00:32:23.880 --> 00:32:25.880]   Well, they need to be responsive.
[00:32:25.880 --> 00:32:26.880]   They need to be responsive.
[00:32:26.880 --> 00:32:29.880]   They have to take responsibility in both directions.
[00:32:29.880 --> 00:32:30.880]   So here's...
[00:32:30.880 --> 00:32:31.880]   You're both right.
[00:32:31.880 --> 00:32:33.880]   But the question, the hard question, of course, is how.
[00:32:33.880 --> 00:32:36.880]   I interviewed Yona Tinsinger this week about justice.
[00:32:36.880 --> 00:32:37.880]   Love fascinating discussion.
[00:32:37.880 --> 00:32:38.880]   Can you ask him to get...
[00:32:38.880 --> 00:32:39.880]   Come on to the show.
[00:32:39.880 --> 00:32:40.880]   I want to get a whole episode.
[00:32:40.880 --> 00:32:41.880]   He's wonderful.
[00:32:41.880 --> 00:32:42.880]   Love him.
[00:32:42.880 --> 00:32:45.880]   And he was the star of Google+, he's left Google now.
[00:32:45.880 --> 00:32:48.880]   But I wanted to talk about this moral question.
[00:32:48.880 --> 00:32:49.880]   And he...
[00:32:49.880 --> 00:32:53.880]   What he says to your point, what he called on was civil engineering.
[00:32:53.880 --> 00:32:54.880]   So you had engineering.
[00:32:54.880 --> 00:32:55.880]   You had engineering.
[00:32:55.880 --> 00:32:56.880]   You had engineering.
[00:32:56.880 --> 00:32:57.880]   You built buildings and you built bridges.
[00:32:57.880 --> 00:32:58.880]   And they fell down.
[00:32:58.880 --> 00:33:00.880]   Oops, we can't do that.
[00:33:00.880 --> 00:33:08.880]   And so you did recognize some level of responsibility and standards and governance around that.
[00:33:08.880 --> 00:33:12.880]   So I think we're starting to reach an agreement about that technology.
[00:33:12.880 --> 00:33:14.880]   I think Kristin Harris goes way too far.
[00:33:14.880 --> 00:33:17.880]   And Roger back to me is just doing his practice and call me tour.
[00:33:17.880 --> 00:33:20.880]   And they're not being humble for years.
[00:33:20.880 --> 00:33:22.880]   Yeah, they're not being helpful.
[00:33:22.880 --> 00:33:29.880]   But the hard part that we're not getting to is, I think we're agreeing that there is a level of responsibility.
[00:33:29.880 --> 00:33:31.880]   But there are lines, we don't want them to cross.
[00:33:31.880 --> 00:33:32.880]   Right.
[00:33:32.880 --> 00:33:33.880]   And we don't look...
[00:33:33.880 --> 00:33:35.880]   But we don't have.
[00:33:35.880 --> 00:33:36.880]   And I flew myself on that.
[00:33:36.880 --> 00:33:37.880]   I thought I saw this.
[00:33:37.880 --> 00:33:41.880]   Because I don't think if we sat down today and said, "Okay, here's the rulebook, folks.
[00:33:41.880 --> 00:33:43.880]   We wouldn't know how to write it."
[00:33:43.880 --> 00:33:44.880]   Yet we expected that rule.
[00:33:44.880 --> 00:33:47.880]   I don't think we need to process.
[00:33:47.880 --> 00:33:48.880]   What if we don't create a rulebook?
[00:33:48.880 --> 00:33:50.880]   What if we create a process by which...
[00:33:50.880 --> 00:33:55.880]   I mean, because if you think about the judiciary, and this is kind of a crappy example because
[00:33:55.880 --> 00:33:58.880]   it takes for freaking ever and it's a gameable process.
[00:33:58.880 --> 00:34:03.880]   But thinking about something like that probably makes a little bit more sense.
[00:34:03.880 --> 00:34:07.880]   Because then it can change over time with the mores of society.
[00:34:07.880 --> 00:34:10.880]   Yes, so there was an effort put up today.
[00:34:10.880 --> 00:34:15.880]   I don't have it right in front of me, but somebody put out a proposal for this kind of rules-based structure.
[00:34:15.880 --> 00:34:23.880]   And that there are some principles that include if your content is taken down, you need to be given notice before it's done.
[00:34:23.880 --> 00:34:26.880]   There needs to be some measure of appeal.
[00:34:26.880 --> 00:34:27.880]   Things like that.
[00:34:27.880 --> 00:34:29.880]   So there's a process and a structure.
[00:34:29.880 --> 00:34:33.880]   So I thought, Stacey, here's the issue.
[00:34:33.880 --> 00:34:37.880]   At the scale, at a billion fake accounts.
[00:34:37.880 --> 00:34:40.880]   And so then I go and game the system and I appeal each one of those.
[00:34:40.880 --> 00:34:50.880]   And so what's happened in Ukraine is the Russians are going in and they're finding anything they can to kill legitimate accounts of anti-Russian activists in Ukraine.
[00:34:50.880 --> 00:34:52.880]   And they're flooding the process.
[00:34:52.880 --> 00:34:53.880]   Right.
[00:34:53.880 --> 00:34:54.880]   So I agree with you.
[00:34:54.880 --> 00:34:56.880]   I agree with you about process, but that's hard too.
[00:34:56.880 --> 00:35:06.880]   This gets back to my core thing, which is we have engineered all of technology, all of computing to be computer-like, to be binary.
[00:35:06.880 --> 00:35:08.880]   And we don't think of it as adversarial.
[00:35:08.880 --> 00:35:21.880]   And so this is where I think when we start trying to scale out stuff that interacts as tech, interacts in more facets of our life becomes more valuable and decides more things that matter to people, we have to think about how people will game the system.
[00:35:21.880 --> 00:35:26.880]   When you say adversarial, you mean a process like the legal system, like the adversarial process?
[00:35:26.880 --> 00:35:28.880]   Yes, for everything.
[00:35:28.880 --> 00:35:38.880]   Otherwise, we're basically sticking all of this on computers with the same mindset we've had since the 1980s and '70s, which is if I program it to do it, it will do it.
[00:35:38.880 --> 00:35:40.880]   I think people aren't the best at that.
[00:35:40.880 --> 00:35:42.880]   I think that's what's happening right now, frankly.
[00:35:42.880 --> 00:35:45.880]   I think that's the conversation that's happening.
[00:35:45.880 --> 00:35:50.880]   Henry Kissinger wrote a piece in the Atlantic, which I thought was fairly ridiculous.
[00:35:50.880 --> 00:35:54.880]   By a tweet by Sparky Tweet, he was, "Henry Kissinger discovers 2018."
[00:35:54.880 --> 00:36:00.880]   But he's kind of arguing that point, Stacy, where he says, "It's just out today, I just put him on the rundown."
[00:36:00.880 --> 00:36:05.880]   Where he says that the pre-impress, the enlightenment was about things that were not binary.
[00:36:05.880 --> 00:36:08.880]   The enlightenment was about huge discussion and philosophy and all that.
[00:36:08.880 --> 00:36:12.880]   And they were losing that nuance and going back to binary.
[00:36:12.880 --> 00:36:15.880]   And we have systems, we don't know how they operate, and a lot of this is naive.
[00:36:15.880 --> 00:36:18.880]   Henry just words what computers do.
[00:36:18.880 --> 00:36:19.880]   But I think you're right, Stacy.
[00:36:19.880 --> 00:36:24.880]   I think that we pronounce it as a presumption of a binary world, and it's not.
[00:36:24.880 --> 00:36:30.880]   Yeah, I mean, the engineers building this stuff don't think of it as something...
[00:36:30.880 --> 00:36:39.880]   People now have a bigger motivation to "hack this" or to get around these things because they're in control of so much more.
[00:36:39.880 --> 00:36:46.880]   And when it was just silly games or nerdy pursuits, most people didn't have that.
[00:36:46.880 --> 00:36:50.880]   But now we've got financial incentives, we've just got so much.
[00:36:50.880 --> 00:36:52.880]   And so, of course, we're going to hack it.
[00:36:52.880 --> 00:36:58.880]   And now we have to rethink the way we program and engineer and build these things and shove them out into the world.
[00:36:58.880 --> 00:37:01.880]   This new Facebook transparency report actually is fascinating.
[00:37:01.880 --> 00:37:02.880]   Quite an eye-opener.
[00:37:02.880 --> 00:37:06.880]   And answers some of that question that we were asking about whether Facebook's stepping this up.
[00:37:06.880 --> 00:37:12.880]   This graph shows the increase in graphic violence violations on Facebook.
[00:37:12.880 --> 00:37:21.880]   This is not, I don't think, because Facebook started policing it, but they've noticed a growth of .16 to .19%.
[00:37:21.880 --> 00:37:28.880]   I'm sorry, 16 to 19% in October, December 2017 to .22 to 27%.
[00:37:28.880 --> 00:37:35.880]   Almost a doubling. And you could see it across the board in adult nudity and sexual activity.
[00:37:35.880 --> 00:37:38.880]   Terrorist propaganda. How much content do we take action on?
[00:37:38.880 --> 00:37:41.880]   Big growths, quarter to quarter. This is quarter to quarter.
[00:37:41.880 --> 00:37:46.880]   Hate speech. Spam. Actually less of a growth in spam than all the others.
[00:37:46.880 --> 00:37:49.880]   Fake accounts actually are down a little bit.
[00:37:49.880 --> 00:37:51.880]   But if you look at the...
[00:37:51.880 --> 00:37:55.880]   This is their buying account. They're fake.
[00:37:55.880 --> 00:37:58.880]   One person said today at this conference that...
[00:37:58.880 --> 00:38:01.880]   Boy, it's a good conference from quarter to five times.
[00:38:01.880 --> 00:38:05.880]   Said that the metrics are all wrong. The metrics are how much did we take down?
[00:38:05.880 --> 00:38:10.880]   So volume became basis. And this is so easy to scale.
[00:38:10.880 --> 00:38:12.880]   The volume is the real measurement.
[00:38:12.880 --> 00:38:16.880]   Well, I think this is... Read more deeply into this report.
[00:38:16.880 --> 00:38:17.880]   It's really interesting.
[00:38:17.880 --> 00:38:24.880]   For instance, Facebook asserts that they find about 80% of violating content before people report it.
[00:38:24.880 --> 00:38:33.880]   It's not based on reporting. The little thin thing here, 5%, 5.6% of content was acted upon because users reported it first.
[00:38:33.880 --> 00:38:38.880]   95% acted on by Facebook flagged by Facebook.
[00:38:38.880 --> 00:38:39.880]   Do they have a...
[00:38:39.880 --> 00:38:40.880]   Do they have a mechanism?
[00:38:40.880 --> 00:38:41.880]   Maybe it is.
[00:38:41.880 --> 00:38:45.880]   Yeah, 99% of terrorist content was flagged by Facebook.
[00:38:45.880 --> 00:38:48.880]   Do they have a mechanism for disputing that?
[00:38:48.880 --> 00:38:54.880]   I'd be very interested in, like, I have automatically taken all this down and then how much of that was disputed by...
[00:38:54.880 --> 00:38:57.880]   It's a transparency report. It's not transparent.
[00:38:57.880 --> 00:38:59.880]   Please.
[00:38:59.880 --> 00:39:01.880]   It's telling me, though.
[00:39:01.880 --> 00:39:06.880]   When it comes to hate speech, Facebook is not very good at detecting it.
[00:39:06.880 --> 00:39:08.880]   It's hard to detect Facebook.
[00:39:08.880 --> 00:39:11.880]   Because it's in the eye of the beholder, isn't it?
[00:39:11.880 --> 00:39:15.880]   There's all kinds of ways to get around it.
[00:39:15.880 --> 00:39:19.880]   They should employ the same people who do the China's firewall.
[00:39:19.880 --> 00:39:21.880]   Yeah, those guys are good.
[00:39:21.880 --> 00:39:24.880]   Those guys can adapt to everything.
[00:39:24.880 --> 00:39:26.880]   They're good.
[00:39:26.880 --> 00:39:29.880]   That's the mirror.
[00:39:29.880 --> 00:39:32.880]   I've got to show you something.
[00:39:32.880 --> 00:39:36.880]   So Joe gave an example of a meme. I won't say the meme, but I want to spread it.
[00:39:36.880 --> 00:39:45.880]   But it was a racist meme that tries to make being whites okay kind of a meme.
[00:39:45.880 --> 00:39:47.880]   And they didn't put it on Facebook.
[00:39:47.880 --> 00:39:51.880]   They put it on the meme and they basically said, "Get this thing on the campuses.
[00:39:51.880 --> 00:39:53.880]   Get it on the walls and restaurants and stuff.
[00:39:53.880 --> 00:39:55.880]   People will take pictures of it.
[00:39:55.880 --> 00:39:58.880]   They'll put it on social media. It looks like it's coming up independently.
[00:39:58.880 --> 00:40:02.880]   It looks like it's kind of a movement and media will start writing about it and they did."
[00:40:02.880 --> 00:40:08.880]   And so they're using things offline now to push back online.
[00:40:08.880 --> 00:40:13.880]   But that's also -- that's been something that's been around for a while.
[00:40:13.880 --> 00:40:15.880]   You look at the AstroTurfing efforts.
[00:40:15.880 --> 00:40:23.880]   I mean, this is not -- again, people want to push their agenda and they're going to optimize how they do it.
[00:40:23.880 --> 00:40:28.880]   And we just have to become more savvy over time about what we're reading.
[00:40:28.880 --> 00:40:32.880]   I mean, this makes it easier.
[00:40:32.880 --> 00:40:38.880]   It's like email suddenly made it very easy to send junk mail to people, but we're going to get over this too.
[00:40:38.880 --> 00:40:47.880]   I was a direct and ringside witness to one of the very earliest examples of this, which was the rise and fall of Dig.
[00:40:47.880 --> 00:40:56.880]   Because it was started by my friend Kevin Rose in 2004 and was a really -- I thought initially great idea to crowd source news.
[00:40:56.880 --> 00:41:00.880]   You know, have people vote on news stories and they would rise to the top.
[00:41:00.880 --> 00:41:02.880]   It'd be a great way to find the best stories.
[00:41:02.880 --> 00:41:08.880]   But it was very quickly people started gaming it.
[00:41:08.880 --> 00:41:10.880]   And the gaming got worse and worse.
[00:41:10.880 --> 00:41:16.880]   And Kevin went through four or five iterations trying to find ways to get around the gaming.
[00:41:16.880 --> 00:41:25.880]   But every time he came up with a fix, the people who were doing the gaming are so motivated and so clever that they just got around it.
[00:41:25.880 --> 00:41:27.880]   And it eventually put him out of business.
[00:41:27.880 --> 00:41:30.880]   This goes to your Chinese wife, Stacey.
[00:41:30.880 --> 00:41:32.880]   I just put on the bottom of the rundown.
[00:41:32.880 --> 00:41:38.880]   A pea at George Neema, who's an old friend of mine in Europe, went to China and learned about the social credit system.
[00:41:38.880 --> 00:41:39.880]   Oh, fascinating.
[00:41:39.880 --> 00:41:40.880]   Oh, that is good.
[00:41:40.880 --> 00:41:41.880]   I can't.
[00:41:41.880 --> 00:41:42.880]   Really, really scary.
[00:41:42.880 --> 00:41:48.880]   What happens in China, of course, is that the populist finds things that don't get caught.
[00:41:48.880 --> 00:41:52.880]   So white supremacists in America are now talking about pit bulls.
[00:41:52.880 --> 00:41:57.880]   And killing pit bulls is the secret language for anti-white interview.
[00:41:57.880 --> 00:41:58.880]   Oh, interesting.
[00:41:58.880 --> 00:42:00.880]   So, like, actually killing them?
[00:42:00.880 --> 00:42:05.880]   No, no, they just say it because the pit bulls are proxies for white people.
[00:42:05.880 --> 00:42:06.880]   Words can't...
[00:42:06.880 --> 00:42:07.880]   So he writes...
[00:42:07.880 --> 00:42:10.880]   George writes, "Your words cannot describe the way I feel right now.
[00:42:10.880 --> 00:42:12.880]   I am shaking physically.
[00:42:12.880 --> 00:42:21.880]   But the Chinese government is implementing a system which will assign a social credit rating to every citizen based on government data regarding their economic and social status.
[00:42:21.880 --> 00:42:23.880]   It works as a mass surveillance tool.
[00:42:23.880 --> 00:42:25.880]   It uses big data analysis technology.
[00:42:25.880 --> 00:42:29.880]   In addition, it's also meant to rate businesses operating in the Chinese market.
[00:42:29.880 --> 00:42:32.880]   If you've seen Black Mirror's Nosedive, this is it.
[00:42:32.880 --> 00:42:34.880]   I mean, we have seen that.
[00:42:34.880 --> 00:42:36.880]   That's exactly what it sounds like.
[00:42:36.880 --> 00:42:40.880]   The rating algorithm known only to the government.
[00:42:40.880 --> 00:42:50.880]   President Xi's plan is based on the stated government principle once untrustworthy, always restricted.
[00:42:50.880 --> 00:42:52.880]   That's chilling.
[00:42:52.880 --> 00:42:54.880]   We don't let felons vote.
[00:42:54.880 --> 00:42:55.880]   Right.
[00:42:55.880 --> 00:43:04.880]   Minorities and minority opinions will be negatively rated as a result of this program, political distance, journalists, and jaywalkers, petty thieves and others.
[00:43:04.880 --> 00:43:07.880]   The consequences of negative ratings.
[00:43:07.880 --> 00:43:12.880]   And we've heard some of this include restricted movement, career potential access to services.
[00:43:12.880 --> 00:43:20.880]   People who, this one, get this one, who associate with people with negative rankings will also be negatively ranked.
[00:43:20.880 --> 00:43:25.880]   So there's an incentive to isolate and marginalize minority opinions.
[00:43:25.880 --> 00:43:27.880]   There is no appeal process.
[00:43:27.880 --> 00:43:29.880]   Once you're rated, that's it.
[00:43:29.880 --> 00:43:34.880]   There's no system to challenge positive or negative ratings.
[00:43:34.880 --> 00:43:37.880]   And he sees you, Jeff.
[00:43:37.880 --> 00:43:43.880]   Yes, this is, I mean, Kevin and I talk about this stuff all the time because we're intrigued by it.
[00:43:43.880 --> 00:43:50.880]   What's crazy is the amount of technology that we have deployed, that China is using in this process.
[00:43:50.880 --> 00:44:04.880]   So think the surveillance cameras think actually they're putting wearables on workers to track their emotions during work so they can understand what makes a person productive.
[00:44:04.880 --> 00:44:08.880]   So we do have something very similar here in the United States. It's called a FICO score.
[00:44:08.880 --> 00:44:10.880]   Yeah, this is not like a FICO score.
[00:44:10.880 --> 00:44:13.880]   Good luck appealing your FICO score.
[00:44:13.880 --> 00:44:25.880]   Yes, but that only affects your credit rating, which does, I'm not saying that's not insignificant, but this affects this can affect like if your kid gets into the right school and again not being able to.
[00:44:25.880 --> 00:44:27.880]   Take your FICO score.
[00:44:27.880 --> 00:44:28.880]   There's redlining.
[00:44:28.880 --> 00:44:31.880]   You know, I mean, we never got rid of redlining in this in this country.
[00:44:31.880 --> 00:44:34.880]   No, and there's all sorts of subtle economic redlining.
[00:44:34.880 --> 00:44:43.880]   We are not anywhere near what China is doing, but that is absolutely the end game, the potential for this kind of stuff.
[00:44:43.880 --> 00:44:45.880]   And that's why it really is important we pay attention.
[00:44:45.880 --> 00:44:47.880]   Like governments for frightening the companies.
[00:44:47.880 --> 00:44:49.880]   Well, so that's a good question.
[00:44:49.880 --> 00:45:00.880]   Is the, I mean, we assume that the government's more frightening because they have these broad unrestricted powers, but the way our current government's going, if you look at like what happened with, oh gosh, is it Experian or Equifax?
[00:45:00.880 --> 00:45:01.880]   I can't remember which one.
[00:45:01.880 --> 00:45:02.880]   Equifax.
[00:45:02.880 --> 00:45:03.880]   Not getting.
[00:45:03.880 --> 00:45:05.880]   I called him Experian for a while, but it's Equifax.
[00:45:05.880 --> 00:45:07.880]   So not getting in any trouble.
[00:45:07.880 --> 00:45:13.880]   If you look at how companies are corrupting our government, it's kind of boiling down to the same thing.
[00:45:13.880 --> 00:45:17.880]   Well, I think that's why I mentioned FICO scores because that is not governmental.
[00:45:17.880 --> 00:45:19.880]   That is private industry.
[00:45:19.880 --> 00:45:26.880]   And while it is nowhere near as nasty as the social credit system in China, it has some of the same attributes.
[00:45:26.880 --> 00:45:30.880]   And I don't, I mean, it doesn't have to just be government.
[00:45:30.880 --> 00:45:37.880]   If it's going to happen in the US, it's probably going to happen through private corporations.
[00:45:37.880 --> 00:45:38.880]   I would say.
[00:45:38.880 --> 00:45:43.880]   Well, and there are insurance firms who look at like your Facebook friends to see if like your.
[00:45:43.880 --> 00:45:45.880]   Our employers who look at your Facebook friends.
[00:45:45.880 --> 00:45:46.880]   Credit risk.
[00:45:46.880 --> 00:45:48.880]   Credit risk too.
[00:45:48.880 --> 00:45:49.880]   Yeah.
[00:45:49.880 --> 00:45:51.880]   It's all in there.
[00:45:51.880 --> 00:45:54.880]   So it's just, it's just this is a cautionary tale.
[00:45:54.880 --> 00:45:55.880]   This is something we should be.
[00:45:55.880 --> 00:45:57.880]   This is the end of the game on stuff like this.
[00:45:57.880 --> 00:46:06.880]   You got this is, you know, and when we, when we, when we say we want to get rid of bad behavior, we want to, we want to stigmatize bad behavior in obvious cases.
[00:46:06.880 --> 00:46:07.880]   I agree.
[00:46:07.880 --> 00:46:11.880]   But, but what system are you creating and how will it be used?
[00:46:11.880 --> 00:46:12.880]   Yeah.
[00:46:12.880 --> 00:46:13.880]   Yeah.
[00:46:13.880 --> 00:46:24.880]   It was going to Japan was really interesting for me because on the one hand, they do what you've always advocated, Jeff, instead of making a law, they use social shaming to keep people in line.
[00:46:24.880 --> 00:46:26.880]   And it's by advocate.
[00:46:26.880 --> 00:46:30.880]   Well, you said that really norms should guide behavior, not laws.
[00:46:30.880 --> 00:46:31.880]   Right.
[00:46:31.880 --> 00:46:32.880]   Okay.
[00:46:32.880 --> 00:46:33.880]   That's kind of the same thing.
[00:46:33.880 --> 00:46:36.880]   That's like, that's like being in a small town in the Midwest or the deep south.
[00:46:36.880 --> 00:46:37.880]   I mean, you could be deep.
[00:46:37.880 --> 00:46:38.880]   You could be.
[00:46:38.880 --> 00:46:40.880]   You have responsibility.
[00:46:40.880 --> 00:46:41.880]   Right.
[00:46:41.880 --> 00:46:45.880]   So, yeah, but I have to point out your day for possibility for the behavior of the town.
[00:46:45.880 --> 00:46:49.880]   I have to point out that, you know, again, this can be misused.
[00:46:49.880 --> 00:46:50.880]   Yeah.
[00:46:50.880 --> 00:46:51.880]   Can be repressive.
[00:46:51.880 --> 00:46:53.880]   Can I mean?
[00:46:53.880 --> 00:47:01.880]   Well, but in the case of in the case of the US in you can always leave your small town, which is why a lot of people who, you know, don't fit the norms leave.
[00:47:01.880 --> 00:47:02.880]   Right.
[00:47:02.880 --> 00:47:05.880]   Mobility is a good is a good antidote, isn't it?
[00:47:05.880 --> 00:47:06.880]   It is.
[00:47:06.880 --> 00:47:07.880]   It is.
[00:47:07.880 --> 00:47:13.880]   It is probably why a lot of people say I'm moving.
[00:47:13.880 --> 00:47:14.880]   That's it.
[00:47:14.880 --> 00:47:15.880]   I'm moving.
[00:47:15.880 --> 00:47:20.880]   Why do you think I went to four high schools, three states, three schools, and three states.
[00:47:20.880 --> 00:47:21.880]   That's the geographic.
[00:47:21.880 --> 00:47:27.880]   Although in China, you can't move because your poor social store keeps you in the country.
[00:47:27.880 --> 00:47:29.880]   Well, and okay, you really want to get deep.
[00:47:29.880 --> 00:47:34.880]   It's also why public policy in this country really promotes people owning homes.
[00:47:34.880 --> 00:47:37.880]   That's why there's a mortgage interest deduction.
[00:47:37.880 --> 00:47:39.880]   It's not because it's good for the country.
[00:47:39.880 --> 00:47:45.880]   It's because it reduces mobility, reduces social mobility, keeps people locked in where they are.
[00:47:45.880 --> 00:47:46.880]   Whoa.
[00:47:46.880 --> 00:47:47.880]   Really?
[00:47:47.880 --> 00:47:48.880]   Yes.
[00:47:48.880 --> 00:47:51.880]   A lot of economists have said that that's actually a bad thing for the economy.
[00:47:51.880 --> 00:47:53.680]   It's a very bad thing.
[00:47:53.680 --> 00:47:59.120]   Look at Detroit, where people, you know, just abandon homes because that was the only way
[00:47:59.120 --> 00:48:02.320]   they could get out was walking away from their mortgage and their home.
[00:48:02.320 --> 00:48:03.880]   Same with employer health insurance.
[00:48:03.880 --> 00:48:05.280]   You're stuck with your job.
[00:48:05.280 --> 00:48:06.280]   Yeah.
[00:48:06.280 --> 00:48:11.440]   So a lot of the, I mean, I've always felt the main role of governments to preserve the
[00:48:11.440 --> 00:48:17.880]   status quo and to keep, and mostly that means for the affluent and successful and wealthy
[00:48:17.880 --> 00:48:23.520]   to have their privilege and keep the lower classes out of their, out of the way and suppressed
[00:48:23.520 --> 00:48:25.600]   and working hard for them.
[00:48:25.600 --> 00:48:28.560]   And this is one way the government does that within it's subtle.
[00:48:28.560 --> 00:48:32.880]   But I think this is exactly the kind of thing that's, I think policy often is all about
[00:48:32.880 --> 00:48:33.880]   that.
[00:48:33.880 --> 00:48:36.960]   Well, policy has plenty of unintended consequences.
[00:48:36.960 --> 00:48:39.320]   I don't think it's an unintended, as Mike.
[00:48:39.320 --> 00:48:40.320]   Well, okay.
[00:48:40.320 --> 00:48:41.320]   Yes.
[00:48:41.320 --> 00:48:43.480]   I think it's an, but I think it's exactly the consequence.
[00:48:43.480 --> 00:48:44.480]   Maybe.
[00:48:44.480 --> 00:48:49.080]   I mean, I'm thinking about things like in the army, for example, wives of people who serve
[00:48:49.080 --> 00:48:53.200]   in the military often can't get jobs because their spouses move so often, but the military
[00:48:53.200 --> 00:48:58.560]   is refused to kind of create programs that could help their spouses.
[00:48:58.560 --> 00:49:04.840]   And part of that, I think is just a, again, a status quo and not wanting to change things
[00:49:04.840 --> 00:49:08.200]   that for a while weren't perceived to be broken.
[00:49:08.200 --> 00:49:10.240]   But I mean, it is hard to change things.
[00:49:10.240 --> 00:49:14.000]   But you know, up here in Canada, they've never been able to write off their mortgage
[00:49:14.000 --> 00:49:19.960]   interest and they buy homes like crazy and their real estate values have soared incredibly.
[00:49:19.960 --> 00:49:24.560]   Well, there are many ways to get people to do what you want them to do.
[00:49:24.560 --> 00:49:25.560]   All right.
[00:49:25.560 --> 00:49:27.960]   Economic incentives are just one.
[00:49:27.960 --> 00:49:28.960]   Okay.
[00:49:28.960 --> 00:49:29.960]   Yeah.
[00:49:29.960 --> 00:49:36.840]   Oh, no, I really think that that's, I mean, if you're, if you're in a legislature, your
[00:49:36.840 --> 00:49:41.640]   goal is, and of course, the campaign laws really promote this, your goal is really to
[00:49:41.640 --> 00:49:46.400]   maintain, you know, even kings maintain and even keel.
[00:49:46.400 --> 00:49:48.840]   Let's just, yeah, let's just keep it the way it is.
[00:49:48.840 --> 00:49:52.000]   Where everybody's happy the way it is.
[00:49:52.000 --> 00:49:53.520]   And so there are lots of ways to do that.
[00:49:53.520 --> 00:49:55.680]   Hey, did you buy a Google clips?
[00:49:55.680 --> 00:49:57.680]   Stacy, did you all end up getting one of those?
[00:49:57.680 --> 00:50:04.600]   No, no, I thought it was creepy at first and that's even creepier.
[00:50:04.600 --> 00:50:07.200]   It's candidates of hugs and kisses.
[00:50:07.200 --> 00:50:09.400]   Candids of hugs and kisses.
[00:50:09.400 --> 00:50:15.000]   We'll announce today that there's smart camera, that little clips camera that you don't control.
[00:50:15.000 --> 00:50:22.720]   It controls itself is getting better at noticing things like hugs, kisses, jumps and dance
[00:50:22.720 --> 00:50:23.720]   moves.
[00:50:23.720 --> 00:50:26.000]   Oh, well, this is it again.
[00:50:26.000 --> 00:50:27.600]   That's like parenting right there.
[00:50:27.600 --> 00:50:28.600]   You want it?
[00:50:28.600 --> 00:50:29.600]   It is.
[00:50:29.600 --> 00:50:33.680]   I know this person thinks it's super creepy, but you know, no, that's just that.
[00:50:33.680 --> 00:50:35.520]   That's just the link bait headline.
[00:50:35.520 --> 00:50:38.960]   But yeah, I mean, I want to see when my kid is dancing.
[00:50:38.960 --> 00:50:43.360]   That's perfect time to take a picture or if one of my kids, she doesn't have a sibling,
[00:50:43.360 --> 00:50:46.600]   but if she were kissing her sibling as a parent, those are the kind of brief moments
[00:50:46.600 --> 00:50:49.800]   that make it feel like, oh, I didn't screw this up entirely.
[00:50:49.800 --> 00:50:50.800]   Yes.
[00:50:50.800 --> 00:50:51.800]   If I'm programming.
[00:50:51.800 --> 00:50:52.800]   At an art stage.
[00:50:52.800 --> 00:50:53.800]   Right.
[00:50:53.800 --> 00:50:57.040]   If I'm programming an AI, that's exactly what I'm looking for.
[00:50:57.040 --> 00:51:00.320]   Moments of motion.
[00:51:00.320 --> 00:51:01.320]   So yeah.
[00:51:01.320 --> 00:51:02.320]   Yeah.
[00:51:02.320 --> 00:51:09.120]   But now what would really be creepy is if it recognized when you were sobbing and started.
[00:51:09.120 --> 00:51:13.120]   And now recognizes existential despair and you get all these pictures of your kids staring
[00:51:13.120 --> 00:51:14.120]   up at the bed.
[00:51:14.120 --> 00:51:15.120]   I bet it does.
[00:51:15.120 --> 00:51:16.120]   But I bet they write it in.
[00:51:16.120 --> 00:51:17.120]   It doesn't take a picture.
[00:51:17.120 --> 00:51:20.280]   Don't take a picture of that.
[00:51:20.280 --> 00:51:21.760]   I'm almost certain it does.
[00:51:21.760 --> 00:51:23.560]   And it says don't take a picture.
[00:51:23.560 --> 00:51:29.000]   You get this Google junk where it will tell you it takes time out.
[00:51:29.000 --> 00:51:31.320]   We're sending you some Xanax by FedEx.
[00:51:31.320 --> 00:51:32.320]   And you.
[00:51:32.320 --> 00:51:36.480]   Well, that's when it shows you ads for retail therapy guys.
[00:51:36.480 --> 00:51:37.480]   Exactly.
[00:51:37.480 --> 00:51:40.120]   Have you ever thought of skyping your therapist?
[00:51:40.120 --> 00:51:41.120]   Well, it'll tell you.
[00:51:41.120 --> 00:51:42.120]   I have to.
[00:51:42.120 --> 00:51:43.120]   I have to skyping my therapist.
[00:51:43.120 --> 00:51:44.120]   But you do.
[00:51:44.120 --> 00:51:45.120]   Yeah.
[00:51:45.120 --> 00:51:46.120]   No.
[00:51:46.120 --> 00:51:47.120]   It's a good tomorrow.
[00:51:47.120 --> 00:51:48.120]   Actually, I am skyping my therapist.
[00:51:48.120 --> 00:51:51.640]   You see, you see, the world is a better place.
[00:51:51.640 --> 00:51:52.640]   Thanks to technology.
[00:51:52.640 --> 00:51:53.640]   They're going to.
[00:51:53.640 --> 00:52:01.280]   If you say Stacey, you wouldn't mind having a computerized therapist?
[00:52:01.280 --> 00:52:03.880]   Oh, like a bot for basic stuff.
[00:52:03.880 --> 00:52:05.880]   I actually think it might be kind of helpful.
[00:52:05.880 --> 00:52:06.880]   Yeah.
[00:52:06.880 --> 00:52:07.880]   For some people.
[00:52:07.880 --> 00:52:09.280]   Why do you say that?
[00:52:09.280 --> 00:52:10.280]   Why do you say that?
[00:52:10.280 --> 00:52:11.280]   Exactly.
[00:52:11.280 --> 00:52:13.680]   What do you think that means?
[00:52:13.680 --> 00:52:14.680]   Mm hmm.
[00:52:14.680 --> 00:52:15.680]   Mm hmm.
[00:52:15.680 --> 00:52:16.680]   Mm hmm.
[00:52:16.680 --> 00:52:17.680]   Yeah.
[00:52:17.680 --> 00:52:21.320]   Actually, if you're a Freudian therapist, you don't even say that.
[00:52:21.320 --> 00:52:22.320]   What do you think that means?
[00:52:22.320 --> 00:52:24.320]   You just go, uh huh.
[00:52:24.320 --> 00:52:25.320]   Uh huh.
[00:52:25.320 --> 00:52:27.120]   It's all you need.
[00:52:27.120 --> 00:52:28.120]   Uh huh.
[00:52:28.120 --> 00:52:30.640]   You think you should come every day from now on?
[00:52:30.640 --> 00:52:31.640]   [laughter]
[00:52:31.640 --> 00:52:34.680]   So, I was hoping somebody had bought clips.
[00:52:34.680 --> 00:52:38.880]   They're going to update in May and I was just curious if any of you, we probably should
[00:52:38.880 --> 00:52:42.160]   get one just to see the changes and all that.
[00:52:42.160 --> 00:52:43.160]   I don't know.
[00:52:43.160 --> 00:52:46.280]   Well, does it also recognize people to stay in the middle finger at you?
[00:52:46.280 --> 00:52:47.280]   You know, that's...
[00:52:47.280 --> 00:52:51.920]   Again, I'm sure it does and avoids photographs of that.
[00:52:51.920 --> 00:52:52.920]   [laughter]
[00:52:52.920 --> 00:52:58.200]   What would be interesting is if you could offer feedback so you train it on what the
[00:52:58.200 --> 00:53:00.440]   great emotions are for you.
[00:53:00.440 --> 00:53:05.240]   Now imagine how the algorithms could change over the life of your child, for example.
[00:53:05.240 --> 00:53:09.760]   Like, as a teen, the pictures you'd want or they'd want to get would be very different.
[00:53:09.760 --> 00:53:13.840]   Well, they're getting signals because if you upload your photos to Google Photos, they
[00:53:13.840 --> 00:53:18.440]   know how often you view those photos, edit them, spend time with them, go back to them.
[00:53:18.440 --> 00:53:19.440]   What are they using them?
[00:53:19.440 --> 00:53:25.160]   Well, what I'm saying is I don't you think Google is?
[00:53:25.160 --> 00:53:29.200]   If I could think of it, if I could think of that, I'm sure some Google engineer has
[00:53:29.200 --> 00:53:30.200]   already thought of that.
[00:53:30.200 --> 00:53:33.800]   There is actually limited compute resources in the world.
[00:53:33.800 --> 00:53:35.640]   No, no, no.
[00:53:35.640 --> 00:53:37.680]   Yes, yes, there are.
[00:53:37.680 --> 00:53:39.880]   Not with the new water-cooled TPUs.
[00:53:39.880 --> 00:53:41.880]   I don't know water-cooled TPU.
[00:53:41.880 --> 00:53:42.880]   [laughter]
[00:53:42.880 --> 00:53:48.840]   Well, I mean, really, that's the point of Google Photos, is it not?
[00:53:48.840 --> 00:53:52.360]   I mean, they're not giving that stuff away for free for nothing.
[00:53:52.360 --> 00:53:54.360]   Well, yeah.
[00:53:54.360 --> 00:53:55.360]   Of course.
[00:53:55.360 --> 00:53:58.360]   But I mean, really, come on.
[00:53:58.360 --> 00:53:59.360]   I mean, really, come on.
[00:53:59.360 --> 00:54:00.920]   I mean, it's a vector, Stacey.
[00:54:00.920 --> 00:54:03.000]   Well, I'm like, yeah, yeah, OK.
[00:54:03.000 --> 00:54:04.400]   But, you know, I mean, not an iPad.
[00:54:04.400 --> 00:54:07.800]   Don't you think Google knows exactly which photos you've looked at the most, which ones
[00:54:07.800 --> 00:54:08.800]   you've edited?
[00:54:08.800 --> 00:54:11.000]   Not just you, but everybody.
[00:54:11.000 --> 00:54:12.000]   Well, but-
[00:54:12.000 --> 00:54:13.000]   It doesn't.
[00:54:13.000 --> 00:54:17.520]   But it's valuable to know about you, too, because then your clips can be, get a feedback
[00:54:17.520 --> 00:54:20.280]   loop saying Stacey wants more of that.
[00:54:20.280 --> 00:54:22.560]   She doesn't know it, but she wants more.
[00:54:22.560 --> 00:54:24.600]   That's what Facebook does with the newsfeed.
[00:54:24.600 --> 00:54:27.840]   Why wouldn't Google do it with photos and clips?
[00:54:27.840 --> 00:54:28.840]   Because it's a problem.
[00:54:28.840 --> 00:54:30.520]   What if you start missing things?
[00:54:30.520 --> 00:54:35.760]   Your life then becomes like your memories of your life become this weird, edited thing.
[00:54:35.760 --> 00:54:40.200]   Yeah, it's a new filter, but you only see pictures of stuff you want to see pictures of.
[00:54:40.200 --> 00:54:45.480]   So, I mean, granted, we only see the things we want in our loved ones anyway, but sometimes
[00:54:45.480 --> 00:54:48.480]   you come across a picture and you're like, wait a second.
[00:54:48.480 --> 00:54:51.960]   So this is this is this is this photographic so much convincing you're happy all the time,
[00:54:51.960 --> 00:54:52.960]   even if you're not.
[00:54:52.960 --> 00:54:55.160]   Here's the real problem.
[00:54:55.160 --> 00:54:59.040]   In real world, this happens all the time in many ways.
[00:54:59.040 --> 00:55:02.040]   Feedback loops with your spouse and your loved ones and all that stuff.
[00:55:02.040 --> 00:55:03.880]   But it's a therapy sport.
[00:55:03.880 --> 00:55:12.480]   It's a very subtle, I think, very sophisticated, highly nuanced process.
[00:55:12.480 --> 00:55:16.360]   Computers are by their very nature blunt instruments.
[00:55:16.360 --> 00:55:20.120]   And I don't care how much you sell neural networks and machine learning and amazing
[00:55:20.120 --> 00:55:21.560]   technology.
[00:55:21.560 --> 00:55:23.160]   It's a very blunt instrument.
[00:55:23.160 --> 00:55:30.480]   So no matter what, whether it's making Twitter and Facebook a powerful tool for good and kind
[00:55:30.480 --> 00:55:33.880]   of deprecating the bad or making your pictures more.
[00:55:33.880 --> 00:55:37.040]   No matter what, it's going to be uncanny valley time.
[00:55:37.040 --> 00:55:42.160]   It's going to be a little bit off because it's a computer.
[00:55:42.160 --> 00:55:44.520]   They're just they're machines.
[00:55:44.520 --> 00:55:45.720]   They're not that good.
[00:55:45.720 --> 00:55:48.160]   You watch Westworld on Sunday.
[00:55:48.160 --> 00:55:49.360]   I don't watch this show.
[00:55:49.360 --> 00:55:50.760]   Oh, I'm so far behind.
[00:55:50.760 --> 00:55:51.960]   I could never touch up.
[00:55:51.960 --> 00:55:58.280]   Oh, I can't sit no spoilers, but you got to catch up because Sunday was a big one.
[00:55:58.280 --> 00:56:01.120]   I'm not saying anything.
[00:56:01.120 --> 00:56:03.200]   I'm not saying anything.
[00:56:03.200 --> 00:56:06.120]   All right, let's take a little break here and talk about.
[00:56:06.120 --> 00:56:08.760]   Do you remember the MOOC synthesizer?
[00:56:08.760 --> 00:56:09.760]   Yeah.
[00:56:09.760 --> 00:56:15.960]   So the guy who started MOOC Soft was a big fan of the MOOC synthesizer.
[00:56:15.960 --> 00:56:20.400]   He wanted to call the company, see the cow on the logo?
[00:56:20.400 --> 00:56:23.480]   MOOC Soft, but somebody already had that.
[00:56:23.480 --> 00:56:28.480]   So he said, oh, well, we could call it MOOC Soft and a great website was born and great
[00:56:28.480 --> 00:56:30.480]   technology for people in IT.
[00:56:30.480 --> 00:56:34.440]   MOOC Soft is all about eliminating the noise.
[00:56:34.440 --> 00:56:40.520]   I really wanted when I was in Japan, we didn't get around to it, to go to a Pachinko parlor
[00:56:40.520 --> 00:56:42.360]   and get some video.
[00:56:42.360 --> 00:56:44.000]   I don't know if you've ever been.
[00:56:44.000 --> 00:56:47.440]   The Pachinko parlors in Japan are basically casinos now.
[00:56:47.440 --> 00:56:51.120]   They used to be just those little metal balls going through and they were always noisy.
[00:56:51.120 --> 00:56:53.040]   But now they've also got roulette.
[00:56:53.040 --> 00:56:54.040]   Not roulette.
[00:56:54.040 --> 00:56:59.520]   What do you, you know, a slot machine slot machines?
[00:56:59.520 --> 00:57:00.520]   Thank you.
[00:57:00.520 --> 00:57:04.960]   They now they now slot machines and it's really noisy.
[00:57:04.960 --> 00:57:08.760]   You go in there and it's an assaultive and I wanted to get a video of it because that's
[00:57:08.760 --> 00:57:13.520]   what it's like when you're the ID guy and those alerts and those tickets are lighting
[00:57:13.520 --> 00:57:14.520]   up your monitor.
[00:57:14.520 --> 00:57:16.080]   Big, big, big, big, big, big, big.
[00:57:16.080 --> 00:57:17.280]   You can't be productive.
[00:57:17.280 --> 00:57:20.360]   You can only go stressed out.
[00:57:20.360 --> 00:57:21.920]   That's what MOOC Soft's all about.
[00:57:21.920 --> 00:57:30.640]   Taking all those alerts, analyzing them, eliminating 99% alerts, just giving you the stuff you need.
[00:57:30.640 --> 00:57:33.840]   It's an algorithmic IT ops platform.
[00:57:33.840 --> 00:57:37.840]   Now they call it AI ops but the AI doesn't stand for artificial intelligence.
[00:57:37.840 --> 00:57:44.160]   It stands for algorithmic IT ops and it will reduce your IT alerts and tickets by up to
[00:57:44.160 --> 00:57:47.000]   99% guaranteed.
[00:57:47.000 --> 00:57:50.080]   It doesn't do that by keeping you out of the know.
[00:57:50.080 --> 00:57:53.400]   It does it by giving you the alerts that matter.
[00:57:53.400 --> 00:57:57.680]   MOOC Soft's AI ops platform integrates with all your existing IT tools.
[00:57:57.680 --> 00:57:58.680]   Look who uses it.
[00:57:58.680 --> 00:58:00.760]   I mean everybody uses this.
[00:58:00.760 --> 00:58:05.800]   So you'll get all the information you need but they correlate events into actionable
[00:58:05.800 --> 00:58:06.800]   work items.
[00:58:06.800 --> 00:58:09.600]   They call them situations like, "Okay, we got a situation and here's what you need to
[00:58:09.600 --> 00:58:10.600]   do."
[00:58:10.600 --> 00:58:16.600]   They even have ways of you were kind of recording a knowledge base of how you handle these and
[00:58:16.600 --> 00:58:18.400]   it gets and so it's getting better.
[00:58:18.400 --> 00:58:20.200]   It's got a feedback loop all the time.
[00:58:20.200 --> 00:58:22.800]   You focus on tackling the stuff that matters.
[00:58:22.800 --> 00:58:24.920]   You know, I'll give you a case study.
[00:58:24.920 --> 00:58:27.120]   HCL Technologies.
[00:58:27.120 --> 00:58:30.800]   They are a managed service provider, a global business.
[00:58:30.800 --> 00:58:33.880]   They have a product called Dry Ice, big award winner.
[00:58:33.880 --> 00:58:35.120]   You probably heard of it.
[00:58:35.120 --> 00:58:40.280]   They include MOOC Soft AI ops as the event management layer within the Dry Ice platform.
[00:58:40.280 --> 00:58:42.840]   That way clients can streamline their operational flows.
[00:58:42.840 --> 00:58:45.320]   They reduce time in the lifecycle of incident tech.
[00:58:45.320 --> 00:58:47.480]   They call it detect to correct.
[00:58:47.480 --> 00:58:51.560]   That's the lifecycle from detection to correction of incident tickets.
[00:58:51.560 --> 00:58:53.400]   And this is the big number here.
[00:58:53.400 --> 00:59:00.600]   HCL experienced a 33% reduction, one third reduction in mean time to restore so they
[00:59:00.600 --> 00:59:04.920]   can support more customers with service quality and keep operational costs low and efficiency
[00:59:04.920 --> 00:59:05.920]   high.
[00:59:05.920 --> 00:59:10.000]   If you're a global IT managed service provider, you get more alerts than anybody.
[00:59:10.000 --> 00:59:12.920]   You need MOOC Soft AI ops.
[00:59:12.920 --> 00:59:13.920]   That's why they use it.
[00:59:13.920 --> 00:59:20.120]   MOOC Soft AI ops, you can reduce your IT alerts and your tickets by up to 99% right now.
[00:59:20.120 --> 00:59:25.360]   Go to MOOCsoft.com and get a demo.
[00:59:25.360 --> 00:59:29.240]   M-O-O-G-S-O-F-T.com.
[00:59:29.240 --> 00:59:30.480]   MOOC Soft.
[00:59:30.480 --> 00:59:36.400]   And we thank MOOCsoft.com for their support of this week in Google.
[00:59:36.400 --> 00:59:37.480]   We started with Mike Elgin.
[00:59:37.480 --> 00:59:46.040]   We had to let him go because of a bad connection in his cheese factory in San Pietro de Filetto.
[00:59:46.040 --> 00:59:47.760]   But we've still got Jeff Jarvis here.
[00:59:47.760 --> 00:59:50.000]   He's in Toronto.
[00:59:50.000 --> 00:59:53.680]   And hey, Stacey Egan-Botham, she's in Austin.
[00:59:53.680 --> 00:59:56.280]   She's covered in barbecue sauce.
[00:59:56.280 --> 00:59:59.240]   Jeff is covered in conference.
[00:59:59.240 --> 01:00:02.000]   What is the conference you're at and tell us about it?
[01:00:02.000 --> 01:00:09.000]   It's gone.
[01:00:09.000 --> 01:00:13.000]   So it's like 2,000 people up here dealing with all the issues we talk about.
[01:00:13.000 --> 01:00:15.000]   You know, about how do we run the internet and preserve human rights and what are those
[01:00:15.000 --> 01:00:16.000]   rights that we're trying to preserve?
[01:00:16.000 --> 01:00:17.000]   It's really fascinating.
[01:00:17.000 --> 01:00:23.640]   I'm going to be on the panel tomorrow about a terrorist video and YouTube and the platforms
[01:00:23.640 --> 01:00:24.640]   we expect them to do.
[01:00:24.640 --> 01:00:25.640]   But it goes all the way from that.
[01:00:25.640 --> 01:00:27.040]   That's the kind of easy end of this.
[01:00:27.040 --> 01:00:33.960]   When you go down into behavioral stuff, and so to one hand you have them saying platforms
[01:00:33.960 --> 01:00:34.960]   fix this stuff.
[01:00:34.960 --> 01:00:40.640]   On the other hand, you have a panel here about how maybe we need a new laws to guarantee
[01:00:40.640 --> 01:00:42.720]   freedom of speech against the platforms.
[01:00:42.720 --> 01:00:44.840]   So to one hand, we're telling the platforms kill stuff together.
[01:00:44.840 --> 01:00:45.840]   Don't kill stuff.
[01:00:45.840 --> 01:00:50.600]   And you just see this accordion there stuck in here.
[01:00:50.600 --> 01:00:55.800]   So what you're saying is there's no more consensus there than there is here.
[01:00:55.800 --> 01:00:57.800]   No, it's not easy.
[01:00:57.800 --> 01:01:01.240]   This is hard stuff.
[01:01:01.240 --> 01:01:05.880]   But you've always said we need to dig in and address it instead of turning our back
[01:01:05.880 --> 01:01:06.880]   on.
[01:01:06.880 --> 01:01:13.600]   But a lot of fears here, you know, also interesting about government should be highly involved.
[01:01:13.600 --> 01:01:15.240]   Government shouldn't.
[01:01:15.240 --> 01:01:17.080]   And you know, I'm not in this crowd.
[01:01:17.080 --> 01:01:22.400]   This is a crowd that wants to regulate things and control things and guarantee people's
[01:01:22.400 --> 01:01:23.600]   rights and so on.
[01:01:23.600 --> 01:01:28.600]   But on the other hand, the German law that that's the gay law that if you leave high
[01:01:28.600 --> 01:01:37.160]   speed 24 hours horrible things happen, you're looking at the impact of this.
[01:01:37.160 --> 01:01:38.160]   There's a lot of issues.
[01:01:38.160 --> 01:01:39.160]   This is hard.
[01:01:39.160 --> 01:01:40.160]   We can't fix these solutions are easy.
[01:01:40.160 --> 01:01:43.120]   We ought to have a lot of discussion about it, which is all we ever do is discuss and
[01:01:43.120 --> 01:01:44.120]   discuss and discuss it.
[01:01:44.120 --> 01:01:46.040]   You see, we're helping the world that way.
[01:01:46.040 --> 01:01:49.440]   Well, some Google employees are doing more than discuss.
[01:01:49.440 --> 01:01:55.520]   You talked before about Google employees, a 4,000 signature petition saying Google has
[01:01:55.520 --> 01:01:59.520]   to stop helping the Defense Department with Project Maven.
[01:01:59.520 --> 01:02:07.760]   It's a project that uses a face recognition to help artificial intelligence to help the
[01:02:07.760 --> 01:02:13.560]   military analyzed drone footage by automatically classifying images of objects and people.
[01:02:13.560 --> 01:02:16.880]   According to Gizmodo, which who talked to some of them about a dozen of these employees
[01:02:16.880 --> 01:02:22.840]   have now resigned saying Google has not been responsive and they finally felt they had
[01:02:22.840 --> 01:02:25.000]   to take some action.
[01:02:25.000 --> 01:02:28.840]   So it's happening everywhere.
[01:02:28.840 --> 01:02:35.200]   The employees who talked to Gizmodo, this is according to Kate Conker writing for Gizmodo,
[01:02:35.200 --> 01:02:39.920]   the employees who talked to Gizmodo say that executives have become less transparent with
[01:02:39.920 --> 01:02:44.640]   their workforce about controversial business decisions and seem less interested in listening
[01:02:44.640 --> 01:02:49.040]   to workers objections than they once did.
[01:02:49.040 --> 01:02:55.480]   Google said that this face recognition that stuff they're doing is purely defensive, that
[01:02:55.480 --> 01:02:58.880]   they don't want to help the Defense Department with offensive capabilities.
[01:02:58.880 --> 01:03:09.760]   But of course, there's nothing that can't be used for offense.
[01:03:09.760 --> 01:03:11.480]   More of the same, you know?
[01:03:11.480 --> 01:03:13.200]   I think this is all surfaced lately.
[01:03:13.200 --> 01:03:19.160]   We're just all very aware of it now.
[01:03:19.160 --> 01:03:23.520]   If I could figure out how people like how this zeitgeist worked, I would be such a better
[01:03:23.520 --> 01:03:24.520]   reporter.
[01:03:24.520 --> 01:03:25.520]   Yeah.
[01:03:25.520 --> 01:03:31.200]   I mean, I feel like why did it hit an inflection point at this point in time?
[01:03:31.200 --> 01:03:37.720]   I think the election has been going on since November 2016 is how long this has been going
[01:03:37.720 --> 01:03:38.720]   on.
[01:03:38.720 --> 01:03:40.280]   It's been going on way before that.
[01:03:40.280 --> 01:03:41.280]   Did you see that?
[01:03:41.280 --> 01:03:43.640]   But we put up with it until 2016 for some reason.
[01:03:43.640 --> 01:03:48.360]   I don't think we really, I mean, like, I found the article about, and maybe it's because
[01:03:48.360 --> 01:03:54.400]   I'm in Texas, but I don't know if you guys remember the Jade Home Holarity, which is where
[01:03:54.400 --> 01:03:56.800]   the army was at the army.
[01:03:56.800 --> 01:04:03.400]   One branch of the military was doing, you know, basic exercises in Texas and it somehow
[01:04:03.400 --> 01:04:04.400]   became...
[01:04:04.400 --> 01:04:05.400]   Black helicopters are coming.
[01:04:05.400 --> 01:04:12.840]   Yeah, people in Texas decided that this was actually a military exercise against the citizens
[01:04:12.840 --> 01:04:13.840]   of Texas.
[01:04:13.840 --> 01:04:19.840]   And our lieutenant governor at the time was like, yeah, and they made official statements
[01:04:19.840 --> 01:04:20.840]   about it.
[01:04:20.840 --> 01:04:23.200]   And all this was propagated by Russian bots.
[01:04:23.200 --> 01:04:24.200]   And so...
[01:04:24.200 --> 01:04:25.800]   Oh, I didn't hear that part.
[01:04:25.800 --> 01:04:28.360]   That's the subsequent follow up, right?
[01:04:28.360 --> 01:04:29.360]   Yes.
[01:04:29.360 --> 01:04:33.600]   So the Texas Tribune, I think it was awesome organization.
[01:04:33.600 --> 01:04:36.920]   They did a story linking that to...
[01:04:36.920 --> 01:04:38.760]   So that was actually way before the election.
[01:04:38.760 --> 01:04:43.240]   And that actually was an impetus to get people who were interested in using this as a form
[01:04:43.240 --> 01:04:47.360]   of propaganda, fear-mongering, vote disruption.
[01:04:47.360 --> 01:04:53.040]   We're like, oh, hey, we can totally affect policy if we get a bunch of crazy people spouting
[01:04:53.040 --> 01:04:56.160]   crazy ideas.
[01:04:56.160 --> 01:05:04.240]   So I think now we're becoming aware of it at a level that is, I guess, the politicians
[01:05:04.240 --> 01:05:05.240]   care.
[01:05:05.240 --> 01:05:07.000]   I don't know.
[01:05:07.000 --> 01:05:09.400]   Yes, Daisy, I think a couple things happen.
[01:05:09.400 --> 01:05:12.360]   I think that we...
[01:05:12.360 --> 01:05:17.120]   That putts was employed for years dealing with the economic manipulation of the systems
[01:05:17.120 --> 01:05:20.440]   that were created.
[01:05:20.440 --> 01:05:23.640]   Only now we're seeing the both political and psychological manipulation.
[01:05:23.640 --> 01:05:29.040]   Psychologically, I mean things like trolls and misogynists and all that.
[01:05:29.040 --> 01:05:30.800]   And so I think a few things could have happened.
[01:05:30.800 --> 01:05:37.560]   One is that it took them longer to learn the power of this.
[01:05:37.560 --> 01:05:39.960]   It took a few big players to have a real impact.
[01:05:39.960 --> 01:05:41.560]   It took the election to have an impact.
[01:05:41.560 --> 01:05:45.320]   It took impact on politicians for them to understand it, took impact on our country to
[01:05:45.320 --> 01:05:46.320]   understand.
[01:05:46.320 --> 01:05:47.320]   All these things came together.
[01:05:47.320 --> 01:05:51.640]   I think your sense is right that this was not the case forever.
[01:05:51.640 --> 01:05:54.120]   It was smaller, but now it's huge.
[01:05:54.120 --> 01:05:56.120]   So maybe not.
[01:05:56.120 --> 01:05:57.840]   Are we over-blowing this?
[01:05:57.840 --> 01:06:00.280]   Are we over-blowing this?
[01:06:00.280 --> 01:06:03.400]   There are plenty of people who have no idea.
[01:06:03.400 --> 01:06:05.120]   I think we talked about this a couple of weeks ago.
[01:06:05.120 --> 01:06:07.040]   And I know it's...
[01:06:07.040 --> 01:06:08.120]   And that's always been the case.
[01:06:08.120 --> 01:06:12.680]   I mean, there've always been people who are like, "Ah, we're worried about pesticides
[01:06:12.680 --> 01:06:13.680]   and the water."
[01:06:13.680 --> 01:06:18.360]   And until it takes a silent spring to be like, "Oh, yeah."
[01:06:18.360 --> 01:06:21.720]   And then only maybe 30% of the population cares about it.
[01:06:21.720 --> 01:06:22.720]   So I think...
[01:06:22.720 --> 01:06:26.720]   I mean, there's still people who scoff at like, "Oh, yeah, I'm full of pesticides.
[01:06:26.720 --> 01:06:27.720]   I don't care."
[01:06:27.720 --> 01:06:30.400]   So I think there...
[01:06:30.400 --> 01:06:35.400]   I don't know where the zeitgeist is if it's actually changed, what the tipping point for
[01:06:35.400 --> 01:06:41.560]   action, who you have to influence, but definitely we're at a point where things will change
[01:06:41.560 --> 01:06:46.880]   because the right number of people, the right people care about it.
[01:06:46.880 --> 01:06:50.120]   90% of people don't care about anything ever.
[01:06:50.120 --> 01:06:51.880]   No, they care about...
[01:06:51.880 --> 01:06:52.880]   You know what they care about?
[01:06:52.880 --> 01:06:53.880]   I'll tell you.
[01:06:53.880 --> 01:06:57.440]   Restaurant openings in there, restaurant openings in their area.
[01:06:57.440 --> 01:06:59.440]   And if they can afford them.
[01:06:59.440 --> 01:07:00.440]   They're local...
[01:07:00.440 --> 01:07:01.960]   They're local...
[01:07:01.960 --> 01:07:02.960]   Not local.
[01:07:02.960 --> 01:07:04.880]   They're phones and device specs.
[01:07:04.880 --> 01:07:06.920]   Not to a crazy level, but like...
[01:07:06.920 --> 01:07:08.560]   Is this from research you've done?
[01:07:08.560 --> 01:07:12.640]   No, this is like when I worked at a newspaper.
[01:07:12.640 --> 01:07:14.840]   These were the stories that always did well.
[01:07:14.840 --> 01:07:15.840]   Not the same...
[01:07:15.840 --> 01:07:16.840]   Any restaurant...
[01:07:16.840 --> 01:07:17.840]   Yeah.
[01:07:17.840 --> 01:07:18.840]   Oh, God, no.
[01:07:18.840 --> 01:07:19.840]   Did you...
[01:07:19.840 --> 01:07:24.920]   So I thought it might be kind of a fun exercise to go through some of the Facebook ads that
[01:07:24.920 --> 01:07:25.920]   were...
[01:07:25.920 --> 01:07:32.440]   The Democrats in the House Intelligence Committee released some of the thousands of Russian
[01:07:32.440 --> 01:07:35.760]   purchased Facebook ads.
[01:07:35.760 --> 01:07:36.760]   So all of these...
[01:07:36.760 --> 01:07:40.480]   All of what you're about to see came from the Internet Research Agency, the Russian troll
[01:07:40.480 --> 01:07:44.000]   factory ads purchased on Facebook.
[01:07:44.000 --> 01:07:48.640]   Here's one from the heart of Texas, which is actually the heart of St. Petersburg.
[01:07:48.640 --> 01:07:50.600]   Don't mess with Texas border patrol.
[01:07:50.600 --> 01:07:52.880]   Always guided by God.
[01:07:52.880 --> 01:07:58.560]   You know, I think I got that as a campaign ad because my husband was actually a registered
[01:07:58.560 --> 01:08:00.720]   Republican, so we get a lot of crazy stuff.
[01:08:00.720 --> 01:08:01.720]   Yeah.
[01:08:01.720 --> 01:08:02.720]   Well, you get both sides.
[01:08:02.720 --> 01:08:04.560]   Here's Bernie Sanders.
[01:08:04.560 --> 01:08:10.880]   The Clinton Foundation is a problem, but it's not really born liberal or Bernie Sanders.
[01:08:10.880 --> 01:08:14.680]   It's the Internet Research Agency.
[01:08:14.680 --> 01:08:17.680]   Get ready to secede Texas.
[01:08:17.680 --> 01:08:25.320]   So the corrupt media does not talk about the crimes committed by Hillary, Rotten Clinton.
[01:08:25.320 --> 01:08:27.360]   So why...
[01:08:27.360 --> 01:08:28.360]   What is the...
[01:08:28.360 --> 01:08:35.240]   Here's a number of the Black Lives Matters pages were fake.
[01:08:35.240 --> 01:08:41.520]   So it's hard to understand what the point of this campaign is except to just make this
[01:08:41.520 --> 01:08:42.880]   way crazy.
[01:08:42.880 --> 01:08:43.880]   It's divided.
[01:08:43.880 --> 01:08:44.880]   It's division.
[01:08:44.880 --> 01:08:45.880]   Division.
[01:08:45.880 --> 01:08:46.880]   Division.
[01:08:46.880 --> 01:08:49.960]   Yeah, sewing division because the more divided we are, the less we can do the...
[01:08:49.960 --> 01:08:53.480]   I mean, it's classic warfare, actually.
[01:08:53.480 --> 01:08:54.480]   Yeah.
[01:08:54.480 --> 01:08:55.480]   Yeah.
[01:08:55.480 --> 01:08:56.480]   Because...
[01:08:56.480 --> 01:08:57.480]   We talked about this before.
[01:08:57.480 --> 01:09:04.320]   This is the Rand report and the NATO report on Russian Information Warfare.
[01:09:04.320 --> 01:09:07.640]   You want to sow discord in your enemy.
[01:09:07.640 --> 01:09:09.800]   That's what they're doing.
[01:09:09.800 --> 01:09:12.080]   Do you feel like this is sophisticated?
[01:09:12.080 --> 01:09:16.800]   It's sophisticated in the sense that none of us noticed it.
[01:09:16.800 --> 01:09:17.800]   I mean, is it...
[01:09:17.800 --> 01:09:20.280]   Well, we didn't because they were dark ads.
[01:09:20.280 --> 01:09:23.000]   None of us, in most cases, we didn't see them.
[01:09:23.000 --> 01:09:24.000]   The people...
[01:09:24.000 --> 01:09:25.640]   Right, but even the people who were affected of it.
[01:09:25.640 --> 01:09:28.960]   But it was people who were the most likely to agree with them that saw them, right?
[01:09:28.960 --> 01:09:34.120]   They were very careful to show these ads only to people who already felt this way, right?
[01:09:34.120 --> 01:09:35.120]   Yes, but...
[01:09:35.120 --> 01:09:37.320]   Four out of three cases.
[01:09:37.320 --> 01:09:39.400]   Yeah, so in that just...
[01:09:39.400 --> 01:09:43.720]   That's sophisticated in that it's preying on like our worst or our...
[01:09:43.720 --> 01:09:44.720]   It's not worst.
[01:09:44.720 --> 01:09:45.720]   It's just our...
[01:09:45.720 --> 01:09:46.720]   It was polarizing.
[01:09:46.720 --> 01:09:47.720]   Got reaction.
[01:09:47.720 --> 01:09:48.720]   This is my favorite one.
[01:09:48.720 --> 01:09:49.720]   Well, I'm just...
[01:09:49.720 --> 01:09:51.920]   Satan and Jesus arm-wrestling.
[01:09:51.920 --> 01:09:52.920]   Satan says...
[01:09:52.920 --> 01:09:56.360]   If I win, content wins.
[01:09:56.360 --> 01:09:59.720]   Jesus says, not if I can help it.
[01:09:59.720 --> 01:10:03.320]   Press like to help Jesus win.
[01:10:03.320 --> 01:10:05.760]   Who wouldn't like that?
[01:10:05.760 --> 01:10:08.920]   That one we saw at the time, that one got around because it was something good, ridiculously
[01:10:08.920 --> 01:10:09.920]   ridiculous.
[01:10:09.920 --> 01:10:10.920]   Yeah.
[01:10:10.920 --> 01:10:11.920]   But you know what's going to happen?
[01:10:11.920 --> 01:10:12.920]   You know what?
[01:10:12.920 --> 01:10:13.920]   Well, let me tell you how you got that.
[01:10:13.920 --> 01:10:20.120]   If you showed a like, if you liked Laura Ingraham, God, Ron Paul, Christianity, Bill O'Reilly,
[01:10:20.120 --> 01:10:25.920]   Rush Limbaugh, Andrea Bightbart, Bible conservatism, Michael Savage, Faith, Mike Huckabee, or Jesus.
[01:10:25.920 --> 01:10:32.520]   If you showed those interests, you were the ages of 18 to 65 plus, you would get this.
[01:10:32.520 --> 01:10:33.520]   God.
[01:10:33.520 --> 01:10:38.040]   I don't know why I didn't get it.
[01:10:38.040 --> 01:10:40.840]   I'm surprised they didn't use blonde Jesus.
[01:10:40.840 --> 01:10:43.600]   You know, well, he's pretty white.
[01:10:43.600 --> 01:10:45.800]   He's super white.
[01:10:45.800 --> 01:10:47.680]   But it looks like Yani.
[01:10:47.680 --> 01:10:48.680]   They use Yani Jesus.
[01:10:48.680 --> 01:10:50.840]   But you know what's going to happen?
[01:10:50.840 --> 01:10:52.480]   Facebook has got to clean a lot of this up.
[01:10:52.480 --> 01:10:54.200]   You know where this is going to go?
[01:10:54.200 --> 01:10:55.200]   What's that?
[01:10:55.200 --> 01:10:56.200]   That's right.
[01:10:56.200 --> 01:10:58.120]   There's no way to shut this up.
[01:10:58.120 --> 01:10:59.280]   There's no way to shut this up.
[01:10:59.280 --> 01:11:00.280]   I agree.
[01:11:00.280 --> 01:11:01.280]   We're not on WhatsApp.
[01:11:01.280 --> 01:11:05.200]   But it's already on WhatsApp for like the elections in India and places that actually
[01:11:05.200 --> 01:11:06.200]   use WhatsApp.
[01:11:06.200 --> 01:11:07.200]   Exactly.
[01:11:07.200 --> 01:11:08.200]   Columbia.
[01:11:08.200 --> 01:11:09.200]   Absolutely.
[01:11:09.200 --> 01:11:12.200]   So yeah, super fun.
[01:11:12.200 --> 01:11:13.200]   All right.
[01:11:13.200 --> 01:11:18.560]   So I want to know is Google One Cloud Storage a good deal.
[01:11:18.560 --> 01:11:19.800]   They've dropped the price.
[01:11:19.800 --> 01:11:20.800]   Well, it's funny.
[01:11:20.800 --> 01:11:22.920]   Actually, Paul Thorett put it more accurately.
[01:11:22.920 --> 01:11:25.360]   They doubled the storage.
[01:11:25.360 --> 01:11:27.360]   So 99 bucks was one terabyte.
[01:11:27.360 --> 01:11:31.240]   Now you get two, right?
[01:11:31.240 --> 01:11:38.200]   So yeah, I mean, here's what I discovered at one point that I had five or six of these
[01:11:38.200 --> 01:11:39.200]   accounts.
[01:11:39.200 --> 01:11:43.440]   And I just decided to cut back.
[01:11:43.440 --> 01:11:49.360]   And so that's it's not bad maybe to have two because you can have redundancy.
[01:11:49.360 --> 01:11:54.400]   Drive is I think Google drives a very good choice because drive is everywhere, right?
[01:11:54.400 --> 01:12:00.320]   Dropbox is also because of they've been around forever.
[01:12:00.320 --> 01:12:04.240]   Dropbox is a good deal, although now they're going to have to drop prices to compete with
[01:12:04.240 --> 01:12:05.240]   Google Drive.
[01:12:05.240 --> 01:12:09.520]   They're half the cost, which is probably the issue.
[01:12:09.520 --> 01:12:12.560]   Yeah, remember when micro what was Microsoft storage?
[01:12:12.560 --> 01:12:14.360]   They bought one drive.
[01:12:14.360 --> 01:12:15.360]   Oh, right.
[01:12:15.360 --> 01:12:16.360]   Sorry.
[01:12:16.360 --> 01:12:17.360]   Yes.
[01:12:17.360 --> 01:12:18.360]   I don't wish I could do it.
[01:12:18.360 --> 01:12:19.600]   But we've talked about this a billion times the show.
[01:12:19.600 --> 01:12:25.440]   I still wish I could do a backup to Google drives.
[01:12:25.440 --> 01:12:26.520]   Like all of your stuff.
[01:12:26.520 --> 01:12:29.600]   I think it's too, I think it's too expensive to do that.
[01:12:29.600 --> 01:12:30.600]   It is.
[01:12:30.600 --> 01:12:32.480]   You'd fill it up.
[01:12:32.480 --> 01:12:34.400]   I pay for two terabytes now.
[01:12:34.400 --> 01:12:38.160]   I had a one terabyte, but I pay for two terabytes because I put all my photos on it.
[01:12:38.160 --> 01:12:41.840]   But I also pay for two terabytes with Adobe and I put all my photos there too.
[01:12:41.840 --> 01:12:43.440]   I think it's not bad to have to.
[01:12:43.440 --> 01:12:45.120]   Oh, see you do it.
[01:12:45.120 --> 01:12:47.000]   Well, but I don't pay anything.
[01:12:47.000 --> 01:12:49.600]   I pay for Dropbox, but I don't pay anything on Google.
[01:12:49.600 --> 01:12:51.120]   Is it because I'm not using enough data?
[01:12:51.120 --> 01:12:53.400]   Yeah, you get a lot free.
[01:12:53.400 --> 01:12:54.400]   They're not.
[01:12:54.400 --> 01:12:56.400]   They're pretty liberal with their like a hundred bags free.
[01:12:56.400 --> 01:12:57.400]   It's amazing.
[01:12:57.400 --> 01:12:58.400]   Yeah.
[01:12:58.400 --> 01:12:59.400]   Something like that.
[01:12:59.400 --> 01:13:01.120]   Yeah, more than enough for most.
[01:13:01.120 --> 01:13:04.240]   I don't send most of my photos to Google photos though either.
[01:13:04.240 --> 01:13:08.400]   No, that's how you fill it up or keep a full Gmail box or that kind of thing.
[01:13:08.400 --> 01:13:13.640]   But if you if you use a pixel and do full quality, you don't pay for those photographs.
[01:13:13.640 --> 01:13:14.640]   I know.
[01:13:14.640 --> 01:13:15.640]   What does that feel?
[01:13:15.640 --> 01:13:16.640]   Yeah.
[01:13:16.640 --> 01:13:19.720]   Yeah, we have unlimited free free storage of photos on Google.
[01:13:19.720 --> 01:13:21.240]   Oh, no wonder I don't pay it.
[01:13:21.240 --> 01:13:22.240]   Yeah.
[01:13:22.240 --> 01:13:24.080]   Speaking of Android, good news.
[01:13:24.080 --> 01:13:27.720]   Well, I did we talk about this last week or did I talk about it on Twitter?
[01:13:27.720 --> 01:13:29.600]   Maybe I talked about it on Sunday.
[01:13:29.600 --> 01:13:32.640]   Project Treble is really making a difference.
[01:13:32.640 --> 01:13:39.040]   We're going to see Google has said that they are going to start at some point to require
[01:13:39.040 --> 01:13:42.000]   OEMs to take Google security updates.
[01:13:42.000 --> 01:13:44.000]   And of course, Treble, we did talk about it here.
[01:13:44.000 --> 01:13:46.520]   Yeah, let's talk about Project Treble and remind people what it was.
[01:13:46.520 --> 01:13:50.400]   I had forgotten, which is it's their anti fragmentation effort for Android.
[01:13:50.400 --> 01:13:51.400]   Correct.
[01:13:51.400 --> 01:13:52.400]   Right.
[01:13:52.400 --> 01:13:53.400]   Well, it's even more than that.
[01:13:53.400 --> 01:13:57.640]   In fact, one of the reasons Android P is available to public beta on so many different
[01:13:57.640 --> 01:13:58.640]   handsets.
[01:13:58.640 --> 01:14:03.000]   In fact, virtually everybody except Samsung is because of Project Treble.
[01:14:03.000 --> 01:14:10.360]   Treble means that the hardware layer, the unique layer that the manufacturer creates
[01:14:10.360 --> 01:14:12.560]   is separate from the rest of Android.
[01:14:12.560 --> 01:14:18.840]   It's a hardware abstraction layer so that Google can say to the manufacturer, here's
[01:14:18.840 --> 01:14:24.120]   the updates and the manufacturer can very quickly roll them out because they're separate
[01:14:24.120 --> 01:14:26.480]   from the actual hardware.
[01:14:26.480 --> 01:14:28.880]   So it's not as big a compatibility issue.
[01:14:28.880 --> 01:14:30.320]   It's not as big as a testing issue.
[01:14:30.320 --> 01:14:33.840]   We talked about this on, I promise to say this, all about Android too.
[01:14:33.840 --> 01:14:34.840]   That's right.
[01:14:34.840 --> 01:14:38.560]   I could tell that the Android folks are very proud of the impact they have.
[01:14:38.560 --> 01:14:42.520]   Well, they're proud because they've done a good job and it's working.
[01:14:42.520 --> 01:14:47.760]   It was in necessity because there are so many Android handsets that are not updated that
[01:14:47.760 --> 01:14:53.160]   are still really out of date, dangerously out of date, insecure versions of Android,
[01:14:53.160 --> 01:14:58.560]   particularly in the underdeveloped, what are we supposed to call it now?
[01:14:58.560 --> 01:14:59.560]   Not the third world.
[01:14:59.560 --> 01:15:00.560]   Developing?
[01:15:00.560 --> 01:15:01.560]   Developing?
[01:15:01.560 --> 01:15:02.560]   Not underdeveloped.
[01:15:02.560 --> 01:15:06.200]   Developing in the less cooked nations.
[01:15:06.200 --> 01:15:10.840]   Yeah, I'm pretty sure that's not what we're calling it.
[01:15:10.840 --> 01:15:17.080]   In the developing world, even that feels a little...
[01:15:17.080 --> 01:15:19.680]   No, it's because we're developed.
[01:15:19.680 --> 01:15:20.680]   It does.
[01:15:20.680 --> 01:15:21.680]   It does.
[01:15:21.680 --> 01:15:24.840]   There is another one you're supposed to use since then I forgot.
[01:15:24.840 --> 01:15:26.320]   I should tell this for you.
[01:15:26.320 --> 01:15:28.200]   You went to Davos, Jeff.
[01:15:28.200 --> 01:15:30.200]   That's the sole reason you're there.
[01:15:30.200 --> 01:15:31.200]   Come on.
[01:15:31.200 --> 01:15:32.200]   They don't care about that.
[01:15:32.200 --> 01:15:33.200]   That's the other side.
[01:15:33.200 --> 01:15:37.720]   Yeah, we'll just call it not Davos.
[01:15:37.720 --> 01:15:38.720]   How about that?
[01:15:38.720 --> 01:15:39.720]   Not Davos.
[01:15:39.720 --> 01:15:41.720]   Not in OECD countries.
[01:15:41.720 --> 01:15:46.760]   He's going to start his competitive conference to Davos in China.
[01:15:46.760 --> 01:15:47.760]   Who is?
[01:15:47.760 --> 01:15:48.760]   Mike Bloomberg.
[01:15:48.760 --> 01:15:49.760]   Who?
[01:15:49.760 --> 01:15:50.760]   Mike Bloomberg.
[01:15:50.760 --> 01:15:51.760]   Mike Bloomberg.
[01:15:51.760 --> 01:15:52.760]   Oh, Bloomberg.
[01:15:52.760 --> 01:15:59.200]   You were cutting out at crucial moments there, Jeff.
[01:15:59.200 --> 01:16:00.720]   Oh, it sounded like Umberg.
[01:16:00.720 --> 01:16:01.720]   Mike Umberg.
[01:16:01.720 --> 01:16:03.720]   Oh, really?
[01:16:03.720 --> 01:16:05.480]   Bloomberg in China, really?
[01:16:05.480 --> 01:16:06.480]   Yes.
[01:16:06.480 --> 01:16:07.480]   Yes.
[01:16:07.480 --> 01:16:08.480]   Pop leaders.
[01:16:08.480 --> 01:16:13.760]   Love Sunday's tweet from President Trump saying let's make China great again?
[01:16:13.760 --> 01:16:14.760]   What?
[01:16:14.760 --> 01:16:15.760]   Nope.
[01:16:15.760 --> 01:16:16.760]   Nope.
[01:16:16.760 --> 01:16:19.320]   We're not going to talk about that.
[01:16:19.320 --> 01:16:20.320]   Let's talk about it.
[01:16:20.320 --> 01:16:21.320]   We should.
[01:16:21.320 --> 01:16:27.240]   It's about ZTE, big Android phone manufacturer that was banned by the Commerce Department
[01:16:27.240 --> 01:16:33.720]   because A, they were selling illegally selling against the Iran sanction.
[01:16:33.720 --> 01:16:36.920]   So I understand why Trump would say, okay, well, that's off.
[01:16:36.920 --> 01:16:40.840]   But B, because the Commerce Department warned that no company should be using their products
[01:16:40.840 --> 01:16:43.960]   because they probably were spying on you.
[01:16:43.960 --> 01:16:46.840]   But which is the same rationale we had for blocking Huawei.
[01:16:46.840 --> 01:16:47.840]   Not Huawei.
[01:16:47.840 --> 01:16:48.840]   Right.
[01:16:48.840 --> 01:16:49.840]   Yeah.
[01:16:49.840 --> 01:16:50.840]   Yes.
[01:16:50.840 --> 01:16:58.440]   So all of a sudden the president talks to Xi, who he likes a lot apparently, might have
[01:16:58.440 --> 01:17:02.880]   something to do with a half trillion dollar loan in Tunisia too.
[01:17:02.880 --> 01:17:06.080]   But anyway, let's make, what is it?
[01:17:06.080 --> 01:17:11.000]   Bringing back Chinese jobs.
[01:17:11.000 --> 01:17:13.200]   I'm confused.
[01:17:13.200 --> 01:17:15.400]   I'm just confused.
[01:17:15.400 --> 01:17:16.400]   Yeah.
[01:17:16.400 --> 01:17:21.080]   Anyway, I actually like ZTE phones.
[01:17:21.080 --> 01:17:24.520]   The Axon 7 from ZTE was my pick a couple of years ago.
[01:17:24.520 --> 01:17:26.960]   It was a really great phone.
[01:17:26.960 --> 01:17:30.000]   On the other hand, I'm not sure I want to use it if it's going to spy on me.
[01:17:30.000 --> 01:17:31.320]   I don't know.
[01:17:31.320 --> 01:17:36.200]   You know that Alexa is a much less popular baby name than ever before.
[01:17:36.200 --> 01:17:39.040]   I would believe that.
[01:17:39.040 --> 01:17:40.400]   What about Siri?
[01:17:40.400 --> 01:17:43.040]   I don't think anybody named their kid Siri before.
[01:17:43.040 --> 01:17:50.000]   Actually last year, 20 girls were named Siri in the US, one per every hundred thousand.
[01:17:50.000 --> 01:17:51.240]   But where were they?
[01:17:51.240 --> 01:17:53.040]   Anybody named their kid Google?
[01:17:53.040 --> 01:17:57.960]   Hey, I'm sure someone his name or Cortana.
[01:17:57.960 --> 01:17:59.960]   No.
[01:17:59.960 --> 01:18:06.160]   Alexa, in 2015, there were 6,050 baby girls in the United States.
[01:18:06.160 --> 01:18:11.320]   The name popularity according to the Social Security Administration is declined by 33%
[01:18:11.320 --> 01:18:13.760]   just 3,883.
[01:18:13.760 --> 01:18:16.040]   Alexa was born last year.
[01:18:16.040 --> 01:18:17.840]   All right.
[01:18:17.840 --> 01:18:18.840]   Where they born?
[01:18:18.840 --> 01:18:19.840]   Oh, this is in the US.
[01:18:19.840 --> 01:18:20.840]   Okay.
[01:18:20.840 --> 01:18:21.840]   Yeah.
[01:18:21.840 --> 01:18:22.840]   I mean, that would be the suckiest name.
[01:18:22.840 --> 01:18:29.800]   In the non-davos nations, we don't know.
[01:18:29.800 --> 01:18:34.840]   We're going to get me just by the time we're taking over.
[01:18:34.840 --> 01:18:37.320]   How many who?
[01:18:37.320 --> 01:18:38.320]   Cadence?
[01:18:38.320 --> 01:18:39.480]   There's only one cadence.
[01:18:39.480 --> 01:18:40.480]   Here's Cadence.
[01:18:40.480 --> 01:18:47.400]   Cadence is our newest baby boy, Patrick Delahandy, our engineer, brought him by during the show
[01:18:47.400 --> 01:18:48.400]   and I missed him.
[01:18:48.400 --> 01:18:53.000]   But we have a lovely picture of Patrick wearing his screensavers t-shirt.
[01:18:53.000 --> 01:18:57.880]   Do you have that on the, on the Tricaster and little baby Cadence?
[01:18:57.880 --> 01:18:59.860]   It was, how big was Cadence?
[01:18:59.860 --> 01:19:01.900]   9.7.7 ounces.
[01:19:01.900 --> 01:19:04.100]   It's a big boy.
[01:19:04.100 --> 01:19:05.100]   That's a honk of a baby.
[01:19:05.100 --> 01:19:06.100]   So it was dad.
[01:19:06.100 --> 01:19:07.360]   Dad was 10.7.
[01:19:07.360 --> 01:19:08.360]   Ah.
[01:19:08.360 --> 01:19:15.200]   Amazon's going to adopt a Rooney rule.
[01:19:15.200 --> 01:19:16.660]   Does that mean they're going to start whining?
[01:19:16.660 --> 01:19:18.160]   Oh, there you go.
[01:19:18.160 --> 01:19:22.040]   There's Jason and they're the tall guys looking at the big boy.
[01:19:22.040 --> 01:19:23.040]   That's good.
[01:19:23.040 --> 01:19:24.480]   Who looks very small?
[01:19:24.480 --> 01:19:25.480]   Very tiny.
[01:19:25.480 --> 01:19:27.520]   But you know how fast they grow.
[01:19:27.520 --> 01:19:29.360]   Thank God.
[01:19:29.360 --> 01:19:33.760]   Is the Rooney ruled named after Andy Rooney?
[01:19:33.760 --> 01:19:34.760]   No.
[01:19:34.760 --> 01:19:38.920]   The owner of the, oh, yours is just to.
[01:19:38.920 --> 01:19:41.440]   Oh, that Rooney.
[01:19:41.440 --> 01:19:42.440]   What's his name?
[01:19:42.440 --> 01:19:43.440]   The football guy.
[01:19:43.440 --> 01:19:44.920]   It's named after him?
[01:19:44.920 --> 01:19:49.560]   The owner of the first adopted in the NFL requires all teams to interview at least one
[01:19:49.560 --> 01:19:54.960]   person of color each time a head coaching or general manager rule comes open.
[01:19:54.960 --> 01:19:58.560]   Amazon apparently is going to do that good for them.
[01:19:58.560 --> 01:19:59.560]   It's magnificent.
[01:19:59.560 --> 01:20:03.560]   But I know for my sister who's a minister, I know that how often she would get in a list
[01:20:03.560 --> 01:20:07.960]   of jobs earlier in her career because they needed to interview a woman.
[01:20:07.960 --> 01:20:08.960]   Right.
[01:20:08.960 --> 01:20:10.360]   So you can get Rooney to two.
[01:20:10.360 --> 01:20:11.360]   Yeah.
[01:20:11.360 --> 01:20:14.160]   Getting interviewed doesn't do anything higher.
[01:20:14.160 --> 01:20:15.160]   Right.
[01:20:15.160 --> 01:20:16.160]   Should be.
[01:20:16.160 --> 01:20:17.160]   Yes.
[01:20:17.160 --> 01:20:19.880]   There are three women on Amazon's board.
[01:20:19.880 --> 01:20:24.800]   No people of color, but there are three women on Amazon's board.
[01:20:24.800 --> 01:20:28.320]   Actually, they initially opposed the policy.
[01:20:28.320 --> 01:20:29.320]   Right.
[01:20:29.320 --> 01:20:36.360]   They didn't want to do the Rooney rule, according to recode, which annoyed some Amazon employees.
[01:20:36.360 --> 01:20:39.040]   On Monday, they said they would adopt that policy.
[01:20:39.040 --> 01:20:41.800]   Women and people of color will be included in the candidates, pool of candidates for
[01:20:41.800 --> 01:20:46.360]   all board openings.
[01:20:46.360 --> 01:20:51.880]   The board is so small that probably the board's feeling was, you know, didn't they say that
[01:20:51.880 --> 01:20:53.280]   they did this already?
[01:20:53.280 --> 01:20:55.120]   Oh, maybe.
[01:20:55.120 --> 01:21:01.640]   However, I think formalizing it helps drive like awareness and kind of creates a pull
[01:21:01.640 --> 01:21:03.280]   for the right.
[01:21:03.280 --> 01:21:04.800]   It's a good thing.
[01:21:04.800 --> 01:21:05.800]   Candidates.
[01:21:05.800 --> 01:21:06.800]   Yeah.
[01:21:06.800 --> 01:21:09.320]   Like, and we care and we're going to make an effort here.
[01:21:09.320 --> 01:21:11.560]   Well, it's like the bare minimum effort.
[01:21:11.560 --> 01:21:12.560]   Yeah.
[01:21:12.560 --> 01:21:16.200]   It's not hiring anybody.
[01:21:16.200 --> 01:21:19.640]   Amazon Apple wants a billion dollars from Samsung.
[01:21:19.640 --> 01:21:20.560]   They're back in court.
[01:21:20.560 --> 01:21:23.120]   I don't even want to tell you the story.
[01:21:23.120 --> 01:21:24.920]   We used to have to do those stories all the time.
[01:21:24.920 --> 01:21:25.920]   We kind of stopped.
[01:21:25.920 --> 01:21:26.920]   Let's stop.
[01:21:26.920 --> 01:21:27.920]   All right.
[01:21:27.920 --> 01:21:28.920]   I didn't mention on Mac break weekly.
[01:21:28.920 --> 01:21:29.920]   I'm sorry I mentioned it here.
[01:21:29.920 --> 01:21:30.920]   I'm so sorry.
[01:21:30.920 --> 01:21:34.800]   You want to talk about, let's see, we could talk about TPUs.
[01:21:34.800 --> 01:21:38.360]   We could talk about the duplex service and Google's caving on that since we talked about
[01:21:38.360 --> 01:21:39.360]   it a couple of weeks ago.
[01:21:39.360 --> 01:21:40.360]   Yeah, we did last week.
[01:21:40.360 --> 01:21:41.360]   Yeah.
[01:21:41.360 --> 01:21:43.640]   Google, what they said, well, we don't know what they mean.
[01:21:43.640 --> 01:21:46.240]   They said they're going to tell you it's a robot.
[01:21:46.240 --> 01:21:47.240]   Yeah.
[01:21:47.240 --> 01:21:48.840]   We don't know what it means.
[01:21:48.840 --> 01:21:53.000]   Oh, well, so the new thing, I think the verge to the story on this, it talked about not
[01:21:53.000 --> 01:21:56.200]   only would it tell you that it was going to be a robot, but we also are.
[01:21:56.200 --> 01:22:01.280]   They also wondered how it would fare with eavesdropping and wiretapping laws in various
[01:22:01.280 --> 01:22:02.280]   states.
[01:22:02.280 --> 01:22:06.840]   So like in Texas, for example, if I'm recording an interview because Google records these,
[01:22:06.840 --> 01:22:09.920]   if I'm recording an interview, I don't actually have to tell the second party because I'm
[01:22:09.920 --> 01:22:10.920]   a one party.
[01:22:10.920 --> 01:22:11.920]   We're in a two party state.
[01:22:11.920 --> 01:22:12.920]   So they'd have to tell.
[01:22:12.920 --> 01:22:13.920]   Yeah.
[01:22:13.920 --> 01:22:14.920]   And you'd have to get consent.
[01:22:14.920 --> 01:22:16.480]   You can't even record until you get consent.
[01:22:16.480 --> 01:22:19.360]   So this is how the call begins.
[01:22:19.360 --> 01:22:20.360]   Hello.
[01:22:20.360 --> 01:22:22.840]   This is Leo's Google assistant calling.
[01:22:22.840 --> 01:22:24.880]   May I record this call?
[01:22:24.880 --> 01:22:25.880]   What?
[01:22:25.880 --> 01:22:30.040]   No, like, I was trying to get.
[01:22:30.040 --> 01:22:31.040]   Hello.
[01:22:31.040 --> 01:22:32.040]   Hello.
[01:22:32.040 --> 01:22:33.040]   Hello.
[01:22:33.040 --> 01:22:34.040]   This is Leo's Google assistant calling.
[01:22:34.040 --> 01:22:37.360]   I want to help save your time and bring you new customers on a holiday and know your
[01:22:37.360 --> 01:22:38.360]   hours.
[01:22:38.360 --> 01:22:39.360]   Is it okay if I do that in record?
[01:22:39.360 --> 01:22:40.360]   Does it go right?
[01:22:40.360 --> 01:22:42.360]   Or how about this one?
[01:22:42.360 --> 01:22:43.360]   Hi.
[01:22:43.360 --> 01:22:47.280]   This is Leo's Google assistant calling to make an appointment.
[01:22:47.280 --> 01:22:50.880]   Do you mind if I record this?
[01:22:50.880 --> 01:22:53.480]   Just for quality assurance?
[01:22:53.480 --> 01:22:56.400]   Yeah, okay.
[01:22:56.400 --> 01:22:58.680]   Thank you.
[01:22:58.680 --> 01:23:00.200]   How about that?
[01:23:00.200 --> 01:23:02.880]   All right.
[01:23:02.880 --> 01:23:05.120]   That's probably closer to what it would be.
[01:23:05.120 --> 01:23:09.560]   Somebody's saying we should just move Stacy's monitor to the center and let her make it
[01:23:09.560 --> 01:23:12.160]   this week in IoT.
[01:23:12.160 --> 01:23:13.160]   Why?
[01:23:13.160 --> 01:23:15.160]   Because I don't know.
[01:23:15.160 --> 01:23:18.400]   There's not a lot of IoT news out there.
[01:23:18.400 --> 01:23:19.400]   You do.
[01:23:19.400 --> 01:23:20.400]   Yeah.
[01:23:20.400 --> 01:23:21.840]   You know what I'm really trying to avoid?
[01:23:21.840 --> 01:23:22.840]   What?
[01:23:22.840 --> 01:23:23.840]   Yanny and Laurel.
[01:23:23.840 --> 01:23:25.840]   Oh, let's not talk about that yet.
[01:23:25.840 --> 01:23:26.840]   Let's talk about TPU.
[01:23:26.840 --> 01:23:28.200]   Try new voice standard.
[01:23:28.200 --> 01:23:29.760]   All right, TPUs.
[01:23:29.760 --> 01:23:30.920]   Tell us all about it.
[01:23:30.920 --> 01:23:36.520]   TPUs are the intelligence process, the 10 sort processing units, the machine learning
[01:23:36.520 --> 01:23:40.600]   units at Google last year announced at Google I/O that they were going to put online for
[01:23:40.600 --> 01:23:46.600]   anybody using TensorFlow and Google code to create their own artificial intelligence
[01:23:46.600 --> 01:23:49.040]   applications or machine learning applications.
[01:23:49.040 --> 01:23:52.520]   They updated them this year with water cooled or something.
[01:23:52.520 --> 01:23:54.960]   I don't know, faster, better ones.
[01:23:54.960 --> 01:24:00.560]   So I think without going into the crazy chip level details, because that's a little crazy
[01:24:00.560 --> 01:24:06.320]   for everyone, the big takeaway here that you've got to think about is Google is spending
[01:24:06.320 --> 01:24:12.800]   so much money and so much effort on water cooling and a data center.
[01:24:12.800 --> 01:24:17.200]   That's a significant investment in your infrastructure, both to bring water cooling in computers and
[01:24:17.200 --> 01:24:18.200]   water don't mix.
[01:24:18.200 --> 01:24:20.160]   So there's a lot of effort to keep them separate.
[01:24:20.160 --> 01:24:24.960]   So when you think about that, you have to realize the enormous value that Google is
[01:24:24.960 --> 01:24:31.760]   ascribing to these processors and what it saves them or the opportunity it gives them
[01:24:31.760 --> 01:24:33.000]   to reduce their...
[01:24:33.000 --> 01:24:36.640]   I always think it was reducing their cost of goods sold, which is why Google builds their
[01:24:36.640 --> 01:24:38.360]   own infrastructure all the time.
[01:24:38.360 --> 01:24:42.480]   Their cost of goods sold used to be serving up a web search result, right?
[01:24:42.480 --> 01:24:44.400]   Now it's machine learning.
[01:24:44.400 --> 01:24:51.000]   And so if you look at what's happened here, this is a big expensive investment and apparently
[01:24:51.000 --> 01:24:53.040]   they think it's totally worth it.
[01:24:53.040 --> 01:24:56.040]   So that's the big macro economic viewpoint on TPUs.
[01:24:56.040 --> 01:25:01.560]   I bet you that they did the back envelope calculation on what it would cost and realized
[01:25:01.560 --> 01:25:02.560]   enough.
[01:25:02.560 --> 01:25:07.040]   They didn't know when they announced this last year how many people would use it.
[01:25:07.040 --> 01:25:08.040]   Enough people use it.
[01:25:08.040 --> 01:25:13.880]   They thought, "When you use this, you're using it with share of other users.
[01:25:13.880 --> 01:25:16.040]   They're probably making money on it, don't you think?"
[01:25:16.040 --> 01:25:19.040]   They're totally making money on it.
[01:25:19.040 --> 01:25:21.720]   And the other thing is they were always going to optimize this.
[01:25:21.720 --> 01:25:25.600]   And if you look at what they've done here, and it's unclear because I don't think we
[01:25:25.600 --> 01:25:28.000]   have the TPUV3 block diagrams.
[01:25:28.000 --> 01:25:34.360]   So it's not 100% clear how much has changed one year between chip iterations.
[01:25:34.360 --> 01:25:35.960]   There's actually not a lot of time.
[01:25:35.960 --> 01:25:39.360]   All we know is they're eight times faster.
[01:25:39.360 --> 01:25:42.160]   For one year, that's a lot, right?
[01:25:42.160 --> 01:25:47.000]   Well, I think they, did they quin...
[01:25:47.000 --> 01:25:48.000]   Is it four?
[01:25:48.000 --> 01:25:49.000]   It's a quad.
[01:25:49.000 --> 01:25:50.000]   Four times the cores.
[01:25:50.000 --> 01:25:51.000]   Hold on.
[01:25:51.000 --> 01:25:52.640]   They four times the cores.
[01:25:52.640 --> 01:25:56.400]   The four times the quadrupled the cores.
[01:25:56.400 --> 01:26:01.560]   So that's not actually crazy to get 8X performance from that.
[01:26:01.560 --> 01:26:05.000]   I mean, that's not actually a lot of optimization on the actual silicon.
[01:26:05.000 --> 01:26:06.800]   It doesn't seem like that seems like an optimization.
[01:26:06.800 --> 01:26:11.840]   Yeah, actually if you look at this, TPU 1.0, which is two generations old, was 28 nanometers
[01:26:11.840 --> 01:26:16.040]   at 700 megahertz consumed 40 watts of power.
[01:26:16.040 --> 01:26:24.160]   TPU 2.0, which I announced last year, had single precision floating point, eight gigs
[01:26:24.160 --> 01:26:27.800]   of memory to each TPU to improve performance.
[01:26:27.800 --> 01:26:30.560]   This is from Extreme Tech.
[01:26:30.560 --> 01:26:37.400]   A TPU cluster was last year, 180 teraflops of computational power, 64 gigs of memory,
[01:26:37.400 --> 01:26:43.840]   and 2400 gigabytes, or 2.4 terabytes of memory bandwidth in total.
[01:26:43.840 --> 01:26:46.160]   But we don't know you're right.
[01:26:46.160 --> 01:26:49.160]   We don't know what the specs are yet for TPU.
[01:26:49.160 --> 01:26:54.120]   Yeah, and so TPU, what's really noticeable is their power consumption.
[01:26:54.120 --> 01:27:01.920]   So I put in a story in the chat from the next platform on the TPUs, because they have a
[01:27:01.920 --> 01:27:07.480]   lovely chart where they, if you look at TPU V1, they've got a 40 watt power consumption
[01:27:07.480 --> 01:27:09.920]   on a 28 nanometer process.
[01:27:09.920 --> 01:27:17.320]   And as your die size shrinks, as you go from 28 to 20 nanometers to lower, you're going
[01:27:17.320 --> 01:27:20.600]   to, you should be able to reduce your power consumption.
[01:27:20.600 --> 01:27:23.400]   But what they've actually done is boosted the core count.
[01:27:23.400 --> 01:27:30.240]   So they went from 40 watts to 200 to 250 watts estimated from one to two.
[01:27:30.240 --> 01:27:32.760]   And three were still at 200 watts.
[01:27:32.760 --> 01:27:36.560]   So they're doing some further optimization here.
[01:27:36.560 --> 01:27:38.920]   But again, these are pretty nerdy, deep stuff.
[01:27:38.920 --> 01:27:40.600]   And I don't really know how many people care about that.
[01:27:40.600 --> 01:27:42.240]   Oh, our audience loves it.
[01:27:42.240 --> 01:27:43.240]   Are you kidding?
[01:27:43.240 --> 01:27:44.240]   So.
[01:27:44.240 --> 01:27:45.240]   Thank you.
[01:27:45.240 --> 01:27:49.800]   But I mean, big takeaway is the other thing to do.
[01:27:49.800 --> 01:27:52.120]   They have their own metrics.
[01:27:52.120 --> 01:27:56.640]   So the super computing world talks about floating points, the math, the high precision computing
[01:27:56.640 --> 01:27:57.640]   world.
[01:27:57.640 --> 01:28:00.360]   So super computing, deep science, math kind of things.
[01:28:00.360 --> 01:28:06.760]   But they actually have a brain floating point format, which I don't, this is optimized solely
[01:28:06.760 --> 01:28:09.000]   for Google's beef flops.
[01:28:09.000 --> 01:28:11.200]   It's, I don't know if it's beef float.
[01:28:11.200 --> 01:28:12.720]   So this is floating points.
[01:28:12.720 --> 01:28:16.600]   So in Stigat, so beef flops, you're right.
[01:28:16.600 --> 01:28:18.080]   But they call it beef float.
[01:28:18.080 --> 01:28:24.360]   And this is, this is a good indicator of like, cool doesn't care about your workloads because
[01:28:24.360 --> 01:28:25.680]   Google has its own workloads.
[01:28:25.680 --> 01:28:28.560]   And they're totally again optimized for this.
[01:28:28.560 --> 01:28:35.120]   This is such bad news for like the big chip players because they're being cut out of this
[01:28:35.120 --> 01:28:38.400]   market effectively for one of the largest customers.
[01:28:38.400 --> 01:28:43.040]   It's basically like what Apple did with their phone chips and what they're going to do to
[01:28:43.040 --> 01:28:46.000]   Intel on the PC side or the laptop side.
[01:28:46.000 --> 01:28:48.000]   So I think it's fascinating.
[01:28:48.000 --> 01:28:51.200]   It's pretty amazing.
[01:28:51.200 --> 01:28:57.400]   What I find interesting is the lead that Google has at this point over anybody else.
[01:28:57.400 --> 01:29:00.680]   Not only doing their own AI, well maybe not over China.
[01:29:00.680 --> 01:29:02.240]   I don't know what's going on over there.
[01:29:02.240 --> 01:29:03.240]   I don't know.
[01:29:03.240 --> 01:29:04.360]   I mean, people don't talk.
[01:29:04.360 --> 01:29:07.760]   This is really like deep competitive stuff.
[01:29:07.760 --> 01:29:12.480]   So Microsoft has their FPGAs and they've had those for a long time for various processing
[01:29:12.480 --> 01:29:19.280]   and their brainwave stuff that they announced last in 2016.
[01:29:19.280 --> 01:29:21.840]   They talked about that again at build.
[01:29:21.840 --> 01:29:24.240]   So Google has Mindshare.
[01:29:24.240 --> 01:29:28.160]   Maybe they don't have technical advantage, but they certainly have Mindshare.
[01:29:28.160 --> 01:29:31.400]   I don't, I mean, Nvidia has all the Mindshare in this space right now.
[01:29:31.400 --> 01:29:33.400]   Only if you're a Bitcoin miner.
[01:29:33.400 --> 01:29:38.280]   They actually said this quarter that their Bitcoin revenue is going to be going down
[01:29:38.280 --> 01:29:42.560]   is one of the courts has the story.
[01:29:42.560 --> 01:29:51.240]   There's a there's a there's a picks and shovels update on the Bitcoin mining rush right there.
[01:29:51.240 --> 01:29:54.080]   And then you said, OK, you mentioned TPUs and what was the other thing you wanted to talk
[01:29:54.080 --> 01:29:55.080]   about?
[01:29:55.080 --> 01:29:56.560]   Oh, I already forgot.
[01:29:56.560 --> 01:29:57.560]   Look at me.
[01:29:57.560 --> 01:29:58.560]   I got so excited about it.
[01:29:58.560 --> 01:29:59.560]   I'm like, Silicon baby.
[01:29:59.560 --> 01:30:00.560]   Let me go back.
[01:30:00.560 --> 01:30:02.560]   Oh, no, we talked about it.
[01:30:02.560 --> 01:30:03.560]   Duplex.
[01:30:03.560 --> 01:30:04.560]   Oh, duplex.
[01:30:04.560 --> 01:30:05.560]   Yeah, we talked about duplex.
[01:30:05.560 --> 01:30:06.560]   I pooped them.
[01:30:06.560 --> 01:30:07.560]   What were your two blocks?
[01:30:07.560 --> 01:30:09.160]   What were your two flex ideas?
[01:30:09.160 --> 01:30:10.160]   No, they were fine.
[01:30:10.160 --> 01:30:11.160]   We talked about that.
[01:30:11.160 --> 01:30:13.200]   I apologize for pooping them.
[01:30:13.200 --> 01:30:14.200]   I just really wanted to do it.
[01:30:14.200 --> 01:30:15.200]   I like duplex.
[01:30:15.200 --> 01:30:16.440]   I can't wait to have to.
[01:30:16.440 --> 01:30:20.400]   And by the way, I think they they they implied that they'll be available the next month or
[01:30:20.400 --> 01:30:21.400]   two.
[01:30:21.400 --> 01:30:22.400]   What?
[01:30:22.400 --> 01:30:23.400]   I would.
[01:30:23.400 --> 01:30:24.400]   Although.
[01:30:24.400 --> 01:30:29.000]   So who calls to make appointments nowadays?
[01:30:29.000 --> 01:30:33.840]   Like I can't I do all of the only call person I call to make an appointment with is my doctor
[01:30:33.840 --> 01:30:37.200]   and my doctor would probably say, no, you can't record this because of hyperpilations.
[01:30:37.200 --> 01:30:38.200]   Right.
[01:30:38.200 --> 01:30:39.200]   Right.
[01:30:39.200 --> 01:30:41.960]   No, I think it.
[01:30:41.960 --> 01:30:43.960]   Yeah, no, I don't.
[01:30:43.960 --> 01:30:45.760]   The call thing isn't that interesting to me.
[01:30:45.760 --> 01:30:53.320]   I just like the I just want better for the more human sounding interactions with me.
[01:30:53.320 --> 01:30:57.240]   That's really I mean really isn't that the most interesting thing if if you could have
[01:30:57.240 --> 01:31:01.440]   an interaction with the Google Assistant that is as rich as the ones we heard on those
[01:31:01.440 --> 01:31:02.440]   phone calls.
[01:31:02.440 --> 01:31:04.960]   Yeah, I don't care about making a dinner date.
[01:31:04.960 --> 01:31:06.720]   Think about Chuck E. Cheese.
[01:31:06.720 --> 01:31:08.800]   I I don't want that.
[01:31:08.800 --> 01:31:12.000]   I don't want to have such a close relationship with the computer alone.
[01:31:12.000 --> 01:31:16.040]   I become like I become like that dude in her.
[01:31:16.040 --> 01:31:17.040]   Yes.
[01:31:17.040 --> 01:31:18.040]   Yes.
[01:31:18.040 --> 01:31:19.040]   That guy was sad.
[01:31:19.040 --> 01:31:22.960]   Those people are sad.
[01:31:22.960 --> 01:31:23.960]   Come out of that world.
[01:31:23.960 --> 01:31:26.760]   You wait to see Higgins just you wait.
[01:31:26.760 --> 01:31:28.200]   Yes, you wait.
[01:31:28.200 --> 01:31:29.200]   Stacey.
[01:31:29.200 --> 01:31:30.200]   That's good.
[01:31:30.200 --> 01:31:31.200]   I like that.
[01:31:31.200 --> 01:31:32.400]   That's not that why I don't worry.
[01:31:32.400 --> 01:31:33.400]   Yeah.
[01:31:33.400 --> 01:31:34.400]   Just you wait.
[01:31:34.400 --> 01:31:35.400]   Stacey Higgins.
[01:31:35.400 --> 01:31:39.720]   You'll be sorry, but your tears will be too late.
[01:31:39.720 --> 01:31:44.120]   I love watching Leo.
[01:31:44.120 --> 01:31:46.600]   The TPUs in Spain.
[01:31:46.600 --> 01:31:53.080]   The TPUs in Spain for mainly in the water cooling vein.
[01:31:53.080 --> 01:31:55.920]   Oh, dude.
[01:31:55.920 --> 01:31:58.320]   I sort of I.
[01:31:58.320 --> 01:31:59.320]   Oh, God.
[01:31:59.320 --> 01:32:10.880]   Oh, God.
[01:32:10.880 --> 01:32:13.920]   Is there any IOT news or you just there?
[01:32:13.920 --> 01:32:15.120]   Oh, the Wi-Fi stuff.
[01:32:15.120 --> 01:32:16.120]   That's what I would say.
[01:32:16.120 --> 01:32:17.120]   Wi-Fi.
[01:32:17.120 --> 01:32:18.120]   The new Wi-Fi easy mesh.
[01:32:18.120 --> 01:32:19.120]   You're a easy.
[01:32:19.120 --> 01:32:20.120]   What is easy?
[01:32:20.120 --> 01:32:21.120]   What is easy?
[01:32:21.120 --> 01:32:22.120]   I'm an.
[01:32:22.120 --> 01:32:24.960]   Monday you have not heard about what easy.
[01:32:24.960 --> 01:32:26.360]   What is me.
[01:32:26.360 --> 01:32:29.120]   So on Monday.
[01:32:29.120 --> 01:32:33.520]   The Wi-Fi Alliance announced a new standard for mesh Wi-Fi.
[01:32:33.520 --> 01:32:35.600]   Oh, oh, wait.
[01:32:35.600 --> 01:32:36.600]   Don't go there.
[01:32:36.600 --> 01:32:40.120]   Go to Stacey on IOT and read Kevin's story because it's way better.
[01:32:40.120 --> 01:32:41.120]   Much better.
[01:32:41.120 --> 01:32:45.920]   Although the ads aren't nearly as rich and exciting.
[01:32:45.920 --> 01:32:47.720]   No, they're not.
[01:32:47.720 --> 01:32:50.160]   And you're going to have to close my little sign up for my newsletter things.
[01:32:50.160 --> 01:32:51.640]   I do that every time.
[01:32:51.640 --> 01:32:52.640]   It's okay.
[01:32:52.640 --> 01:32:53.640]   I signed up.
[01:32:53.640 --> 01:32:54.800]   That's the thing you got to know somehow that I signed up.
[01:32:54.800 --> 01:32:57.040]   Well, so it's supposed to only pop up every two weeks.
[01:32:57.040 --> 01:33:00.920]   This is an argument I have with my business partner slash husband.
[01:33:00.920 --> 01:33:02.440]   Anyway, okay.
[01:33:02.440 --> 01:33:03.840]   So what's happening here is.
[01:33:03.840 --> 01:33:07.280]   The good, the bad and the ugly from Kevin Tofel.
[01:33:07.280 --> 01:33:11.720]   Historically, you had to buy all in on Euro or all in on Orbi.
[01:33:11.720 --> 01:33:13.960]   And now with this, you could buy in.
[01:33:13.960 --> 01:33:17.480]   I'm going to buy the Orbi router, but then maybe later you're like, eh, I would like
[01:33:17.480 --> 01:33:18.480]   to use some Google mesh.
[01:33:18.480 --> 01:33:20.000]   So standard for how these work?
[01:33:20.000 --> 01:33:21.000]   Yes.
[01:33:21.000 --> 01:33:22.000]   Yeah.
[01:33:22.000 --> 01:33:24.800]   So who's not into this?
[01:33:24.800 --> 01:33:29.200]   Euro sponsor, Euro.
[01:33:29.200 --> 01:33:35.840]   So yeah, I saw by the way, the first Comcast ad showing off the plumes.
[01:33:35.840 --> 01:33:36.840]   Oh, yes.
[01:33:36.840 --> 01:33:37.840]   That's yeah.
[01:33:37.840 --> 01:33:38.840]   Thanks, finity.
[01:33:38.840 --> 01:33:39.840]   Wi-Fi.
[01:33:39.840 --> 01:33:41.400]   They'll be distributing those.
[01:33:41.400 --> 01:33:48.720]   So I can kind of think about what Euro might have said, which is something like, well, this
[01:33:48.720 --> 01:33:53.400]   might be a standard, but we've got better proprietary technology.
[01:33:53.400 --> 01:33:54.400]   That is exact.
[01:33:54.400 --> 01:33:55.880]   Oh, you are so smart.
[01:33:55.880 --> 01:33:58.720]   That is exactly what they said when we emailed them.
[01:33:58.720 --> 01:33:59.840]   Just to make sure.
[01:33:59.840 --> 01:34:03.840]   You can get excited about easy mesh, but we've got true mesh.
[01:34:03.840 --> 01:34:05.320]   Yes.
[01:34:05.320 --> 01:34:07.800]   And I've got ganesh.
[01:34:07.800 --> 01:34:14.160]   So anyway, I think this is a little too late because many people have already invested
[01:34:14.160 --> 01:34:15.160]   in these.
[01:34:15.160 --> 01:34:21.440]   I also have to say that anybody who's had any experience with Wi-Fi alliances standards
[01:34:21.440 --> 01:34:28.160]   and I put that in air quotes, knows that this is not in any way a good, you know, I mean,
[01:34:28.160 --> 01:34:33.000]   if they're really worth standards and everybody agreed and blah, blah, blah, but you can't use
[01:34:33.000 --> 01:34:39.400]   third, you can't use Wi-Fi extenders that are supposedly compatible if they're not from
[01:34:39.400 --> 01:34:40.600]   the same manufacturer.
[01:34:40.600 --> 01:34:44.000]   So I don't have high hopes that this is maybe that maybe it will.
[01:34:44.000 --> 01:34:45.840]   Maybe I love mesh.
[01:34:45.840 --> 01:34:47.920]   I think it's a great thing.
[01:34:47.920 --> 01:34:53.200]   What is, though, really this key feature of mesh?
[01:34:53.200 --> 01:34:57.120]   It's that you get coverage everywhere in your house, which now matters.
[01:34:57.120 --> 01:35:02.720]   The duplex conversations between the back end and the access point, is that the main thing?
[01:35:02.720 --> 01:35:05.600]   No, it's an easy way to extend coverage.
[01:35:05.600 --> 01:35:07.240]   That's really the in for me.
[01:35:07.240 --> 01:35:10.360]   It was the technology that makes that so exciting.
[01:35:10.360 --> 01:35:13.280]   Oh, like the actual technology?
[01:35:13.280 --> 01:35:20.360]   It's a software overlay that allows these things to see each other as one big network.
[01:35:20.360 --> 01:35:22.640]   So that's it's software.
[01:35:22.640 --> 01:35:23.640]   Yeah.
[01:35:23.640 --> 01:35:27.520]   I mean, there's all sorts of cool, like dynamic spectrum sharing.
[01:35:27.520 --> 01:35:31.600]   Those are all cool things that some companies have, but not other companies.
[01:35:31.600 --> 01:35:37.680]   And those are oftentimes like part of a mesh system, like plume, for example, does dynamic
[01:35:37.680 --> 01:35:39.040]   spectrum sharing.
[01:35:39.040 --> 01:35:41.520]   But that's not every mesh company does that.
[01:35:41.520 --> 01:35:47.200]   So here's a deeply technical infographic that will explain it all.
[01:35:47.200 --> 01:35:49.480]   Oh, Lord.
[01:35:49.480 --> 01:35:54.440]   Yeah, that is not.
[01:35:54.440 --> 01:35:55.880]   So let me just help.
[01:35:55.880 --> 01:35:58.360]   The internet comes in here.
[01:35:58.360 --> 01:35:59.720]   This is your gateway.
[01:35:59.720 --> 01:36:01.200]   Is that a true blio?
[01:36:01.200 --> 01:36:06.880]   And then the internet goes upstairs, downstairs, and even to the garage.
[01:36:06.880 --> 01:36:11.360]   Thank you very much.
[01:36:11.360 --> 01:36:15.240]   I'm really glad they provided all the technical details I'm needing.
[01:36:15.240 --> 01:36:21.280]   Yes, that is definitely not, you know, the technical photo.
[01:36:21.280 --> 01:36:24.960]   Oh, I file lines.
[01:36:24.960 --> 01:36:27.880]   Who is the Wi-Fi Alliance?
[01:36:27.880 --> 01:36:28.880]   When they're at home.
[01:36:28.880 --> 01:36:29.880]   Is it?
[01:36:29.880 --> 01:36:30.880]   Yeah.
[01:36:30.880 --> 01:36:33.320]   Oh, they throw a hello party, I'm sure.
[01:36:33.320 --> 01:36:34.320]   No.
[01:36:34.320 --> 01:36:36.520]   Answer that is.
[01:36:36.520 --> 01:36:40.920]   Fortunately, their webpage has a handy link.
[01:36:40.920 --> 01:36:45.680]   Wi-Fi Alliance is the worldwide network of companies that brings you Wi-Fi registered
[01:36:45.680 --> 01:36:47.240]   trademark.
[01:36:47.240 --> 01:36:53.400]   From the moment we coined the term Wi-Fi registered trademark to today, Wi-Fi Alliance has worked
[01:36:53.400 --> 01:36:56.480]   diligently to make Wi-Fi one of the world's most.
[01:36:56.480 --> 01:36:58.760]   Here's why I don't like the Wi-Fi Alliance.
[01:36:58.760 --> 01:37:00.320]   I'll tell you the truth.
[01:37:00.320 --> 01:37:03.080]   They have screwed up.
[01:37:03.080 --> 01:37:05.360]   WEP.
[01:37:05.360 --> 01:37:07.560]   They have screwed up WPA.
[01:37:07.560 --> 01:37:17.280]   So, okay, but for a standard, I will say Wi-Fi has been really successful because you can
[01:37:17.280 --> 01:37:23.080]   bring, I can bring my computer into any Wi-Fi hotspot.
[01:37:23.080 --> 01:37:28.000]   The problem is we're putting more and more features onto Wi-Fi.
[01:37:28.000 --> 01:37:30.160]   So it became from a base radio.
[01:37:30.160 --> 01:37:31.720]   Oh, it's so a mess.
[01:37:31.720 --> 01:37:37.040]   But it's still better than like, oh my God, look at the hot mess that is Zigbee.
[01:37:37.040 --> 01:37:38.040]   I mean, yeah.
[01:37:38.040 --> 01:37:41.840]   Well, so I guess if you compare it to Zigbee, okay.
[01:37:41.840 --> 01:37:44.520]   Or like, look at DLNA.
[01:37:44.520 --> 01:37:45.520]   Oh my God.
[01:37:45.520 --> 01:37:46.920]   I know, you know, that's a good point.
[01:37:46.920 --> 01:37:51.600]   I mean, by its very success, clearly something went right with Wi-Fi.
[01:37:51.600 --> 01:37:52.600]   Yeah.
[01:37:52.600 --> 01:37:56.280]   I think probably there are companies out there like Cisco that would say, well, yeah, but
[01:37:56.280 --> 01:37:57.840]   we did all the work or whatever.
[01:37:57.840 --> 01:37:58.840]   But anyway.
[01:37:58.840 --> 01:38:01.240]   Well, someone has to do the work.
[01:38:01.240 --> 01:38:06.640]   Somebody has to do the, you know, in 2005, Wi-Fi was added to Merriam Websters.
[01:38:06.640 --> 01:38:07.640]   Really?
[01:38:07.640 --> 01:38:08.640]   Yeah.
[01:38:08.640 --> 01:38:11.640]   According to the Wi-Fi Alliance.
[01:38:11.640 --> 01:38:14.960]   It was until then.
[01:38:14.960 --> 01:38:19.040]   We didn't have any, remember when Intel was doing the unwired laptop commercials?
[01:38:19.040 --> 01:38:22.520]   That was like 2005 or 2004.
[01:38:22.520 --> 01:38:26.520]   So way back in the day, I worked on a, it's in the office, but I worked on a tablet Intel
[01:38:26.520 --> 01:38:28.920]   never released.
[01:38:28.920 --> 01:38:30.400]   And it wasn't Wi-Fi.
[01:38:30.400 --> 01:38:31.760]   It wasn't Bluetooth.
[01:38:31.760 --> 01:38:37.040]   It was, what was the Intel version?
[01:38:37.040 --> 01:38:38.040]   Crap.
[01:38:38.040 --> 01:38:39.040]   It was anywhere.
[01:38:39.040 --> 01:38:40.040]   Yeah.
[01:38:40.040 --> 01:38:41.040]   Yeah.
[01:38:41.040 --> 01:38:42.040]   Yeah.
[01:38:42.040 --> 01:38:43.040]   Yeah.
[01:38:43.040 --> 01:38:44.040]   Yeah.
[01:38:44.040 --> 01:38:45.040]   Yeah.
[01:38:45.040 --> 01:38:46.040]   Yeah.
[01:38:46.040 --> 01:38:46.880]   So the idea was you had your desktop, because it's all you had at the time, you had your desktop
[01:38:46.880 --> 01:38:49.120]   computer there in the den.
[01:38:49.120 --> 01:38:52.120]   And this used whatever their version was on a tablet.
[01:38:52.120 --> 01:38:56.480]   I've got it in my office where you can sit on the couch and all it would do is mirror
[01:38:56.480 --> 01:38:57.480]   the computer.
[01:38:57.480 --> 01:39:01.720]   So you did on the tablet would do it on the computer and then do it up to the internet.
[01:39:01.720 --> 01:39:05.440]   But yeah, they made a mistake and didn't use Wi-Fi.
[01:39:05.440 --> 01:39:09.000]   I'll tell you, the big complaint, and Steve Gibson makes this complaint about Wi-Fi
[01:39:09.000 --> 01:39:14.720]   alliance, is that they keep all the protocols proprietary and closed so that people, that's
[01:39:14.720 --> 01:39:19.680]   how they got in trouble with web, that people can't look at what they're proposing except
[01:39:19.680 --> 01:39:24.040]   the people who pay $15,000 a year to be members of the Wi-Fi alliance.
[01:39:24.040 --> 01:39:25.560]   That's what UL does.
[01:39:25.560 --> 01:39:26.560]   Yeah.
[01:39:26.560 --> 01:39:28.120]   And yet we're talking about, and ISO.
[01:39:28.120 --> 01:39:30.920]   I mean, so that's when we're talking about security standards.
[01:39:30.920 --> 01:39:33.680]   Yeah, I guess if it's a security standard.
[01:39:33.680 --> 01:39:34.680]   I don't know.
[01:39:34.680 --> 01:39:39.600]   I feel like this one, Steve Gibson, be better at talking about this than I.
[01:39:39.600 --> 01:39:41.760]   I'm not saying that it's the right thing to do.
[01:39:41.760 --> 01:39:43.280]   I'm just saying it's not uncommon.
[01:39:43.280 --> 01:39:44.280]   Yeah.
[01:39:44.280 --> 01:39:45.280]   No, that makes sense.
[01:39:45.280 --> 01:39:49.000]   But it's where you get into trouble because you assume, oh, we know how to do crypto and
[01:39:49.000 --> 01:39:50.000]   then they don't.
[01:39:50.000 --> 01:39:52.480]   And then you put out a standard that is easily.
[01:39:52.480 --> 01:39:53.480]   Who assumes that?
[01:39:53.480 --> 01:39:59.520]   I mean, you have got to be the wealthiest person in the room to think, yeah, I got this crypto
[01:39:59.520 --> 01:40:00.520]   thing down.
[01:40:00.520 --> 01:40:01.520]   I mean, that's the thing.
[01:40:01.520 --> 01:40:06.640]   I think crypto almost never benefits from being closed.
[01:40:06.640 --> 01:40:11.960]   It almost always you need other, you need other cryptographers and experts looking at
[01:40:11.960 --> 01:40:14.640]   your protocols, trying to figure out what's wrong with them.
[01:40:14.640 --> 01:40:16.000]   You can't just do it yourself.
[01:40:16.000 --> 01:40:17.000]   Oh, look.
[01:40:17.000 --> 01:40:19.240]   That's like a philosophical divide.
[01:40:19.240 --> 01:40:20.240]   It is.
[01:40:20.240 --> 01:40:21.240]   It is.
[01:40:21.240 --> 01:40:23.400]   I mean, insecurity.
[01:40:23.400 --> 01:40:29.920]   So secure us speaking of insecurity in the news.
[01:40:29.920 --> 01:40:39.240]   This is a company that works in the highly valued inmate cell, inmate phone booth space.
[01:40:39.240 --> 01:40:41.280]   God, such a.
[01:40:41.280 --> 01:40:42.360]   It is.
[01:40:42.360 --> 01:40:43.320]   Such a.
[01:40:43.320 --> 01:40:49.400]   So it's a, that's a good, that's a profitable business to be in.
[01:40:49.400 --> 01:40:55.200]   It's almost as good as, as a jails themselves, but they, they, in addition to providing service
[01:40:55.200 --> 01:40:59.880]   to provide and monitor calls for inmates, they apparently for some years have been
[01:40:59.880 --> 01:41:05.680]   offering law enforcement in the United States unfettered access to location data from all
[01:41:05.680 --> 01:41:12.480]   the major cell phone carriers and at a, at a low cost.
[01:41:12.480 --> 01:41:16.960]   It's one of securuses, fabulous features.
[01:41:16.960 --> 01:41:21.640]   Unfortunately, it has been misused in, in this New York Times story this week.
[01:41:21.640 --> 01:41:26.160]   They talk about Corey Hutcheson, who was a former Missouri sheriff who's been accused
[01:41:26.160 --> 01:41:32.400]   of using securuses service to track judges, members of the state highway patrol.
[01:41:32.400 --> 01:41:33.400]   Jesus.
[01:41:33.400 --> 01:41:35.200]   He has pled not guilty.
[01:41:35.200 --> 01:41:41.360]   So I don't want to convict him in the court of public opinion, but, and then hackers now
[01:41:41.360 --> 01:41:46.680]   have just recently, after this story, of course, the hackers got to work and hacked
[01:41:46.680 --> 01:41:47.680]   securuses.
[01:41:47.680 --> 01:41:51.840]   So it was really, should be called insecure us.
[01:41:51.840 --> 01:41:55.560]   And let me see what, what they came out with.
[01:41:55.560 --> 01:41:58.720]   This is from a motherboard article that came out today.
[01:41:58.720 --> 01:42:04.440]   Hacker breaches securuses and is provided motherboard with the login details for a company
[01:42:04.440 --> 01:42:08.520]   that buys phone location data from major telecom companies and then sells at the law
[01:42:08.520 --> 01:42:12.800]   enforcement.
[01:42:12.800 --> 01:42:19.480]   The hacker provided motherboard with a spreadsheet marked police.
[01:42:19.480 --> 01:42:24.600]   There's over 2,800 usernames, email addresses, phone numbers and hash passwords, plus security
[01:42:24.600 --> 01:42:30.800]   questions of the Popo, stretching from 2011 up to this year.
[01:42:30.800 --> 01:42:34.720]   So just, you know, if you have been using securuses, if you're a law enforcement official's been
[01:42:34.720 --> 01:42:40.040]   using securuses, you might want to change your password.
[01:42:40.040 --> 01:42:43.040]   And motherboard now knows who you are.
[01:42:43.040 --> 01:42:50.720]   Oh, do you want to talk about Comcast and their $90 installation?
[01:42:50.720 --> 01:42:52.440]   Oh, isn't that sweet?
[01:42:52.440 --> 01:42:53.440]   Why is it?
[01:42:53.440 --> 01:42:54.440]   Oh, Lord.
[01:42:54.440 --> 01:42:56.440]   What's the deal?
[01:42:56.440 --> 01:43:00.280]   John Botkin over at Ars Technica has a lovely story, including Comcast.
[01:43:00.280 --> 01:43:01.480]   I think that's not true.
[01:43:01.480 --> 01:43:06.200]   And then coming back after he gives them their own data from their website saying, oh, yeah,
[01:43:06.200 --> 01:43:07.200]   okay.
[01:43:07.200 --> 01:43:10.520]   You know, but I don't think anybody who's a Comcast customer doesn't already know that
[01:43:10.520 --> 01:43:13.960]   Comcast will charge you $90 for nothing.
[01:43:13.960 --> 01:43:17.680]   I mean, this is, this isn't a surprise.
[01:43:17.680 --> 01:43:22.000]   Even if you provide your own modem and plug it in yourself, in many cases, they will charge
[01:43:22.000 --> 01:43:24.640]   you an installation fee.
[01:43:24.640 --> 01:43:30.400]   Not not that the key here is that if the house already, even when it's not warranted,
[01:43:30.400 --> 01:43:32.680]   so it's fine if they're going to come out and set it up.
[01:43:32.680 --> 01:43:34.680]   I'm going to use to call that.
[01:43:34.680 --> 01:43:35.680]   I'll hook up the truck roll.
[01:43:35.680 --> 01:43:37.720]   Say, oh, we have to go back at the whole walk.
[01:43:37.720 --> 01:43:38.720]   Yeah.
[01:43:38.720 --> 01:43:41.480]   We have some switches.
[01:43:41.480 --> 01:43:43.400]   It's expensive.
[01:43:43.400 --> 01:43:46.600]   You know, it did not cost me anything I don't think.
[01:43:46.600 --> 01:43:50.720]   But then again, the problem with this is it's often just kind of in your bill.
[01:43:50.720 --> 01:43:57.160]   Well, it's a profit grab for by a company that has a veritable monopoly on high speed
[01:43:57.160 --> 01:43:59.040]   internet in many areas.
[01:43:59.040 --> 01:44:03.920]   So you want, you want over a hundred megs, you're going to have to go with Comcast and
[01:44:03.920 --> 01:44:06.920]   if who doesn't want that nowadays?
[01:44:06.920 --> 01:44:08.920]   It's at fun.
[01:44:08.920 --> 01:44:10.840]   Comcast cares.
[01:44:10.840 --> 01:44:12.360]   Now Comcast did email me.
[01:44:12.360 --> 01:44:14.560]   I should see what they said.
[01:44:14.560 --> 01:44:19.600]   They say if the customer at a home that currently has service disconnects or schedules a future
[01:44:19.600 --> 01:44:25.960]   disconnect, then a customer ordering new internet only service to that home would get a self-install
[01:44:25.960 --> 01:44:26.960]   option.
[01:44:26.960 --> 01:44:27.960]   Right.
[01:44:27.960 --> 01:44:28.880]   Now.
[01:44:28.880 --> 01:44:32.360]   [laughs]
[01:44:32.360 --> 01:44:36.760]   A lot of people schedule the disconnect, like I disconnect my cable before I move in.
[01:44:36.760 --> 01:44:40.160]   Yeah, you don't want to leave it on for the next person.
[01:44:40.160 --> 01:44:44.000]   So you disconnect it and this is not a truck roll.
[01:44:44.000 --> 01:44:45.200]   There's no truck roll required.
[01:44:45.200 --> 01:44:49.320]   You plug in your modem and all that has to happen is somebody at the central office has
[01:44:49.320 --> 01:44:51.920]   to go, oh, yeah, let's, let's re-enable that.
[01:44:51.920 --> 01:44:53.400]   Now sometimes a truck roll is required.
[01:44:53.400 --> 01:44:54.400]   I think that's fine.
[01:44:54.400 --> 01:44:57.400]   That's appropriate to charge for that because somebody is going to come in.
[01:44:57.400 --> 01:44:58.400]   Yeah.
[01:44:58.400 --> 01:45:00.800]   Months and months and years and years and years and money on you.
[01:45:00.800 --> 01:45:01.800]   Yeah.
[01:45:01.800 --> 01:45:04.880]   Because their modem fee, I didn't realize this because I hadn't been tracking it since
[01:45:04.880 --> 01:45:06.320]   I don't cover broadband anymore.
[01:45:06.320 --> 01:45:07.680]   Their modem fee is now $11.
[01:45:07.680 --> 01:45:08.680]   I know.
[01:45:08.680 --> 01:45:09.680]   Isn't that outrageous?
[01:45:09.680 --> 01:45:10.680]   That's...
[01:45:10.680 --> 01:45:12.960]   You could buy a very good cable modem for under a hundred bucks.
[01:45:12.960 --> 01:45:16.520]   So you'll quickly make that money back if you just do your own thing.
[01:45:16.520 --> 01:45:19.280]   And it's probably a better modem than Comcast provides.
[01:45:19.280 --> 01:45:23.360]   But then they get you because you noticed by the way that they said if you only, if you
[01:45:23.360 --> 01:45:26.880]   order internet only service, I don't think this is a cash grab.
[01:45:26.880 --> 01:45:31.360]   I think this is a way to cut it and set your own car.
[01:45:31.360 --> 01:45:32.360]   Yeah.
[01:45:32.360 --> 01:45:34.240]   You're able to get to triple play because if you only have a double play you might get
[01:45:34.240 --> 01:45:36.760]   paid too according to the R's article.
[01:45:36.760 --> 01:45:39.120]   If you get the triple play, it will wave all fees.
[01:45:39.120 --> 01:45:43.120]   It's really about, I think, Comcast really wants to incent people.
[01:45:43.120 --> 01:45:46.800]   They don't like cord cutters for obvious reasons.
[01:45:46.800 --> 01:45:50.040]   And they would love you to get their phone service too for obvious reasons.
[01:45:50.040 --> 01:45:55.880]   So those fees I think are as much to encourage you to do that as to get a little extra out
[01:45:55.880 --> 01:45:56.880]   of you.
[01:45:56.880 --> 01:45:57.880]   Okay.
[01:45:57.880 --> 01:46:04.760]   But you know, this is, this is one, it's a hard argument to win because they can say,
[01:46:04.760 --> 01:46:08.600]   well, you don't understand it's going to, you know, we've got to flash the modem or
[01:46:08.600 --> 01:46:11.440]   whatever, you know, they got a good news and bad news.
[01:46:11.440 --> 01:46:16.960]   The good news is the Senate has voted to overturn Ajit Pais net neutrality repeal by
[01:46:16.960 --> 01:46:20.040]   We shall over time.
[01:46:20.040 --> 01:46:25.160]   So 52 to 47, the bad news is now it goes to the house.
[01:46:25.160 --> 01:46:32.600]   And if, if unaccountably the house passes it, it's still got to get past the president.
[01:46:32.600 --> 01:46:36.080]   So it's really more one of those tilt and it win mills kinds of things.
[01:46:36.080 --> 01:46:40.840]   I'll say though, you know, the way the president goes right now, you know, get a wild hair.
[01:46:40.840 --> 01:46:42.360]   Make sure I have great again.
[01:46:42.360 --> 01:46:48.200]   It could just, you know, we'll have president she call say, I'll tell you what you make,
[01:46:48.200 --> 01:46:51.720]   give an open internet, but we'll, we'll give you our social credit system free.
[01:46:51.720 --> 01:46:54.280]   That's what we have to do.
[01:46:54.280 --> 01:46:57.280]   Yeah, we have to start lobbying G and Putin.
[01:46:57.280 --> 01:47:00.840]   We want to talk to the wrong people.
[01:47:00.840 --> 01:47:02.400]   Don't talk to Democrats.
[01:47:02.400 --> 01:47:03.640]   They've got no power.
[01:47:03.640 --> 01:47:08.560]   Talk to somebody knows how to wield power.
[01:47:08.560 --> 01:47:11.560]   Maybe that Duterte guy will help us with the net neutrality thing.
[01:47:11.560 --> 01:47:12.560]   That's his new territory.
[01:47:12.560 --> 01:47:13.560]   We have two e-wheels.
[01:47:13.560 --> 01:47:14.560]   That's all right.
[01:47:14.560 --> 01:47:16.360]   It's too late now.
[01:47:16.360 --> 01:47:23.600]   I say, in for a penny, in for a pound, I'm going to get a red hat that says make China
[01:47:23.600 --> 01:47:24.600]   great again.
[01:47:24.600 --> 01:47:25.600]   I am.
[01:47:25.600 --> 01:47:28.000]   I swear to God.
[01:47:28.000 --> 01:47:30.720]   Let's take a break and come back.
[01:47:30.720 --> 01:47:34.480]   I think we really need something lighter here, don't you?
[01:47:34.480 --> 01:47:41.200]   Perhaps, perhaps a thing of the week from Stacy, a number of the week from you and a caution
[01:47:41.200 --> 01:47:43.720]   of the week from me.
[01:47:43.720 --> 01:47:46.080]   How about all of the three?
[01:47:46.080 --> 01:47:48.000]   But first, I have a recommendation.
[01:47:48.000 --> 01:47:51.920]   By the way, I see that my electric scooter has arrived.
[01:47:51.920 --> 01:47:57.240]   My eco-reco that I ordered an Indiegogo in July of 2017.
[01:47:57.240 --> 01:48:01.600]   I nearly got killed by those damn things in San Jose.
[01:48:01.600 --> 01:48:02.600]   Were they the birds?
[01:48:02.600 --> 01:48:03.600]   Did you see the birds?
[01:48:03.600 --> 01:48:04.600]   Yeah, they were all over.
[01:48:04.600 --> 01:48:05.600]   Yeah.
[01:48:05.600 --> 01:48:08.120]   Man, I saw lots of close calls.
[01:48:08.120 --> 01:48:09.600]   This is a very controversial thing.
[01:48:09.600 --> 01:48:12.160]   I first saw it down in Santa Monica.
[01:48:12.160 --> 01:48:16.640]   There's electric scooters just littering the sidewalk all over the place.
[01:48:16.640 --> 01:48:18.680]   It's been very successful down there.
[01:48:18.680 --> 01:48:23.440]   And then they came to San Francisco and man, the supervisors did not like it.
[01:48:23.440 --> 01:48:26.920]   I don't know if they're still there or not, but because people just leave.
[01:48:26.920 --> 01:48:29.240]   The idea is, I think it's a good idea.
[01:48:29.240 --> 01:48:30.240]   I think it's a good idea.
[01:48:30.240 --> 01:48:31.800]   It's a good idea.
[01:48:31.800 --> 01:48:32.800]   You're right.
[01:48:32.800 --> 01:48:35.320]   You just get a scooter, you scoot somewhere.
[01:48:35.320 --> 01:48:37.440]   And you leave it there.
[01:48:37.440 --> 01:48:39.840]   Well, you go around.
[01:48:39.840 --> 01:48:44.760]   I ever tell you the dumbest thing I ever bought, the single stupidest thing I ever bought besides
[01:48:44.760 --> 01:48:45.760]   Google Glass.
[01:48:45.760 --> 01:48:46.760]   What?
[01:48:46.760 --> 01:48:49.760]   I bought a moped in San Francisco.
[01:48:49.760 --> 01:48:51.760]   Good morning.
[01:48:51.760 --> 01:48:54.760]   Oh, Jeff, that's adorable.
[01:48:54.760 --> 01:49:01.320]   This show is moving on as fast as a moped in San Francisco.
[01:49:01.320 --> 01:49:02.840]   All right.
[01:49:02.840 --> 01:49:06.400]   You were saying, Leo, by the way, notice I'm wearing.
[01:49:06.400 --> 01:49:11.880]   It's a little late, Anzac Day was a few weeks ago, but I'm wearing my Anzac pin thanks
[01:49:11.880 --> 01:49:15.800]   to one of our fine Aussie fans.
[01:49:15.800 --> 01:49:19.280]   It's one of them Aussie hats.
[01:49:19.280 --> 01:49:20.960]   What's it called?
[01:49:20.960 --> 01:49:23.800]   Slouchat.
[01:49:23.800 --> 01:49:27.560]   And underneath it says Australian New Zealand Army Corps, right?
[01:49:27.560 --> 01:49:29.160]   That's what Anzac stands for, right?
[01:49:29.160 --> 01:49:30.160]   Anzac.
[01:49:30.160 --> 01:49:33.000]   Oh, I was going to ask him, like, what esoteric tech acronym is this?
[01:49:33.000 --> 01:49:35.920]   It's not a tech acronym, but it's an it is an esoteric acronym.
[01:49:35.920 --> 01:49:39.120]   Unless you're in Australia, New Zealand, then you know it because they have Anzac Day
[01:49:39.120 --> 01:49:40.120]   every year.
[01:49:40.120 --> 01:49:46.000]   But do are the armed forces of New Zealand and Australia still combined?
[01:49:46.000 --> 01:49:48.320]   In some situations.
[01:49:48.320 --> 01:49:51.720]   If we go down and attack them, they will get together.
[01:49:51.720 --> 01:49:55.320]   100 years of Anzac.
[01:49:55.320 --> 01:50:00.080]   And the best thing about Anzac Day is Anzac Bikis, which are a lovely cookie.
[01:50:00.080 --> 01:50:01.080]   Bikis?
[01:50:01.080 --> 01:50:05.000]   Bikis, shirt for biscuits.
[01:50:05.000 --> 01:50:07.120]   I speak Aussie.
[01:50:07.120 --> 01:50:08.520]   I wish I spoke Aussie.
[01:50:08.520 --> 01:50:10.040]   Oh, man.
[01:50:10.040 --> 01:50:14.960]   It's Australian for oats, flour, sugar and coconut.
[01:50:14.960 --> 01:50:15.960]   They're really good.
[01:50:15.960 --> 01:50:18.040]   Oh, look what I did.
[01:50:18.040 --> 01:50:19.520]   Better than Tim Tams?
[01:50:19.520 --> 01:50:20.520]   Yes.
[01:50:20.520 --> 01:50:21.520]   Actually, yes.
[01:50:21.520 --> 01:50:22.520]   No.
[01:50:22.520 --> 01:50:24.280]   Tim Tams are good.
[01:50:24.280 --> 01:50:26.680]   Anzac biscuits are amazing.
[01:50:26.680 --> 01:50:30.000]   And they keep forever.
[01:50:30.000 --> 01:50:34.840]   It's usually not something that tastes good.
[01:50:34.840 --> 01:50:36.760]   Oh, how could that bad could this be?
[01:50:36.760 --> 01:50:39.920]   Oats, flour, coconut, sugar, butter and golden syrup.
[01:50:39.920 --> 01:50:41.880]   It sounds like a nature's valley granola bar.
[01:50:41.880 --> 01:50:42.880]   It does.
[01:50:42.880 --> 01:50:48.000]   I think that was the inspiration for the nature's valley granola bar.
[01:50:48.000 --> 01:50:54.640]   They were long been established with Anzac because they were sent by wives to soldiers
[01:50:54.640 --> 01:50:57.480]   abroad because the ingredients do not spoil easily.
[01:50:57.480 --> 01:51:00.360]   And this is what I'm looking for in a baked good.
[01:51:00.360 --> 01:51:03.080]   They keep well during naval transportation.
[01:51:03.080 --> 01:51:15.360]   There's a really bad joke in the Master and Commander series where Captain Jack Aubrey
[01:51:15.360 --> 01:51:20.320]   has one of those crackers that has been on the boat for four years.
[01:51:20.320 --> 01:51:24.760]   And they tap it and a couple of weevils come out.
[01:51:24.760 --> 01:51:28.760]   He said, well, I prefer the lesser of the two weevils.
[01:51:28.760 --> 01:51:30.760]   Yeah.
[01:51:30.760 --> 01:51:35.000]   It's the one joke in the whole 21 book series.
[01:51:35.000 --> 01:51:37.000]   Wow.
[01:51:37.000 --> 01:51:40.840]   No, it's really good reading.
[01:51:40.840 --> 01:51:41.920]   It's really good reading.
[01:51:41.920 --> 01:51:45.720]   I have many of my all my male friends have said this to me.
[01:51:45.720 --> 01:51:47.160]   It's kind of a guy thing.
[01:51:47.160 --> 01:51:57.360]   It's riding this roaring mane on the big seed boat on the poop deck and fighting with swords
[01:51:57.360 --> 01:51:58.360]   and cannons.
[01:51:58.360 --> 01:52:03.880]   It seems to be the books you read right before you switch over to World War II and war histories.
[01:52:03.880 --> 01:52:04.880]   Yeah.
[01:52:04.880 --> 01:52:06.560]   So what is it about us guys?
[01:52:06.560 --> 01:52:09.760]   We just wanted to blow things up.
[01:52:09.760 --> 01:52:11.760]   Is it?
[01:52:11.760 --> 01:52:14.760]   Actually, Master and Commander is a very, very good series.
[01:52:14.760 --> 01:52:17.760]   If you're into naval warfare.
[01:52:17.760 --> 01:52:25.000]   I don't know what's going on.
[01:52:25.000 --> 01:52:28.960]   You know, I'm always punchy by the time I get to the end of my week, right?
[01:52:28.960 --> 01:52:34.720]   Which is funny because my week's two days long.
[01:52:34.720 --> 01:52:38.120]   But for some reason by the end of it, I'm just exhausted.
[01:52:38.120 --> 01:52:41.040]   I'm just I'm exhausted.
[01:52:41.040 --> 01:52:42.040]   So go to your spot.
[01:52:42.040 --> 01:52:44.880]   I'm going to go to my spot and then we're going to go to your number.
[01:52:44.880 --> 01:52:48.720]   Our show today brought to you by the folks at WordPress.
[01:52:48.720 --> 01:52:49.800]   Love the WordPress.
[01:52:49.800 --> 01:52:50.800]   Leo Laporte.com.
[01:52:50.800 --> 01:52:53.920]   My blog is on WordPress, but I've been using WordPress since 2003.
[01:52:53.920 --> 01:52:57.320]   I think it was Matt Mullenweg first put it out there.
[01:52:57.320 --> 01:53:03.680]   I had used all the other content they call them at the time they call them blogging engines,
[01:53:03.680 --> 01:53:05.360]   content management systems.
[01:53:05.360 --> 01:53:07.840]   And I came to WordPress and said, how will you?
[01:53:07.840 --> 01:53:13.160]   I've found the platform even back then I knew this is this is good stuff.
[01:53:13.160 --> 01:53:17.200]   But I was doing it wrong because I ran my own server and installed WordPress on that.
[01:53:17.200 --> 01:53:20.360]   And that meant I was the one to keep it up to date, had to get the plugins.
[01:53:20.360 --> 01:53:24.960]   In fact, I spent so much time managing my WordPress server and put a whole lot of content
[01:53:24.960 --> 01:53:27.200]   up there fast forward a few years.
[01:53:27.200 --> 01:53:34.160]   I found wordpress.com all the benefits of WordPress, but they hosted they keep it up
[01:53:34.160 --> 01:53:35.160]   to date.
[01:53:35.160 --> 01:53:37.560]   They put all the plugins and the features in.
[01:53:37.560 --> 01:53:39.200]   You don't have to.
[01:53:39.200 --> 01:53:42.520]   And it's less it costs less than it did for me to have my own server.
[01:53:42.520 --> 01:53:44.520]   I love WordPress.com.
[01:53:44.520 --> 01:53:46.840]   They of course have great customer support team there.
[01:53:46.840 --> 01:53:49.640]   They're 24/7 Monday through Friday weekends too.
[01:53:49.640 --> 01:53:52.880]   So if you ever have any questions, you can get them answered quick, but you probably
[01:53:52.880 --> 01:53:53.880]   won't.
[01:53:53.880 --> 01:53:55.120]   You don't need to be super technical.
[01:53:55.120 --> 01:53:57.000]   You're not doing the coding and the designing.
[01:53:57.000 --> 01:53:59.720]   WordPress gives you everything you need to get your site up and running.
[01:53:59.720 --> 01:54:03.920]   I just my daughter Abby just set up her site and I've been watching her do this because
[01:54:03.920 --> 01:54:05.880]   it's been informative to me to see.
[01:54:05.880 --> 01:54:07.560]   And she really loves it.
[01:54:07.560 --> 01:54:10.040]   She loves the templates.
[01:54:10.040 --> 01:54:12.960]   She asked me, she said, I want something really professional looking.
[01:54:12.960 --> 01:54:14.920]   So she's on a vacation right now when she gets back.
[01:54:14.920 --> 01:54:18.480]   We're going to pick out a really nice you get hundreds to choose from.
[01:54:18.480 --> 01:54:20.000]   So also have great plugins.
[01:54:20.000 --> 01:54:25.720]   I use the probably the best anti spam plugin ever made for comments on blogs, the Akismet
[01:54:25.720 --> 01:54:26.720]   engine.
[01:54:26.720 --> 01:54:32.360]   I use the Google amp plugin that makes my pages load super fast on mobile.
[01:54:32.360 --> 01:54:37.600]   Of course, HTTPS everywhere, which means you're not only more secure, but you're going
[01:54:37.600 --> 01:54:40.280]   to rank higher on Google because of it.
[01:54:40.280 --> 01:54:44.560]   If you've got a business and you need a website, if you are an individual, the reason I wanted
[01:54:44.560 --> 01:54:48.360]   Abby to do this is because I want her to put her best foot forward and people search for
[01:54:48.360 --> 01:54:49.360]   her name.
[01:54:49.360 --> 01:54:53.720]   I want them to see her best worker portfolio, her articles.
[01:54:53.720 --> 01:54:55.520]   That's where you do it.
[01:54:55.520 --> 01:54:57.240]   That's where you take over the internet.
[01:54:57.240 --> 01:55:00.280]   You put yourself out there, WordPress.com.
[01:55:00.280 --> 01:55:02.320]   And of course, it's very affordable.
[01:55:02.320 --> 01:55:06.520]   Plan started just $4 a month as you get more professional.
[01:55:06.520 --> 01:55:11.080]   If you want a commerce solution, they've got that to everything from a simple and effective
[01:55:11.080 --> 01:55:14.080]   buy button to a complete online store.
[01:55:14.080 --> 01:55:15.480]   They have SEO.
[01:55:15.480 --> 01:55:21.000]   That means you're going to rank higher on search because of those features.
[01:55:21.000 --> 01:55:22.320]   Social sharing, which is awesome.
[01:55:22.320 --> 01:55:26.720]   That helps you rank higher too because your viewers, your readers, the people who visit
[01:55:26.720 --> 01:55:32.560]   your site, your customers, when they share, when they put your content up on Twitter and
[01:55:32.560 --> 01:55:35.400]   Facebook, it helps your ranking and it helps spread the word.
[01:55:35.400 --> 01:55:39.360]   And of course, no matter what your needs, hundreds of great specialized plugins.
[01:55:39.360 --> 01:55:44.200]   Really, it's no surprise 30% of all the websites in the world, 30% globally.
[01:55:44.200 --> 01:55:50.600]   All the websites in the world, 30% run on the WordPress platform.
[01:55:50.600 --> 01:55:53.120]   That's pretty amazing.
[01:55:53.120 --> 01:55:54.120]   Get started today.
[01:55:54.120 --> 01:55:58.400]   You get 15% off any new plan purchase at WordPress.com/tweid.
[01:55:58.400 --> 01:56:04.880]   Create your new website at WordPress.com/tweid.
[01:56:04.880 --> 01:56:10.480]   We thank WordPress for their support of this week.
[01:56:10.480 --> 01:56:14.400]   In Google Stacy, you have anything you want to talk about?
[01:56:14.400 --> 01:56:21.240]   I have a device for people who want to control their smart home locally.
[01:56:21.240 --> 01:56:24.480]   It is from a company called Hubitat.
[01:56:24.480 --> 01:56:28.160]   This is a $115.
[01:56:28.160 --> 01:56:31.000]   And this is a smart home hub.
[01:56:31.000 --> 01:56:36.280]   It actually runs Groovy, which is what the SmartThings platform runs on.
[01:56:36.280 --> 01:56:41.560]   So it was created actually by the guy who created the rules engine for smart things.
[01:56:41.560 --> 01:56:44.880]   And so this is a local hub.
[01:56:44.880 --> 01:56:49.880]   So when you connect it to your stuff, you can actually control all your data, control
[01:56:49.880 --> 01:56:50.880]   all that stuff.
[01:56:50.880 --> 01:56:52.480]   If your internet goes down, this stays up.
[01:56:52.480 --> 01:56:54.120]   All the stuff is executed there.
[01:56:54.120 --> 01:56:56.040]   This does limit some of the things you can do.
[01:56:56.040 --> 01:56:59.120]   Like if your internet's out, you can't make cloud to cloud connections.
[01:56:59.120 --> 01:57:04.080]   So talking to Madam A or Google is not going to necessarily work.
[01:57:04.080 --> 01:57:07.200]   But the rest of the time it will work.
[01:57:07.200 --> 01:57:11.400]   So they actually have already supported a bunch of big name devices.
[01:57:11.400 --> 01:57:16.000]   Ones that they haven't supported JET are Nest and EkoB.
[01:57:16.000 --> 01:57:19.800]   But they have Philips Hue, Fobaro, a bunch of nice products.
[01:57:19.800 --> 01:57:26.480]   So this is a pretty cool option if you're a nerdy developer type.
[01:57:26.480 --> 01:57:31.560]   If you are a normal person who does not want to play with your smart home, don't buy this.
[01:57:31.560 --> 01:57:34.680]   So would this replace the Hue Hub?
[01:57:34.680 --> 01:57:35.680]   Is this it?
[01:57:35.680 --> 01:57:38.240]   It works with the Hue Hub because you actually have to talk.
[01:57:38.240 --> 01:57:42.560]   Now it could because it does have a ZigBee radio in it.
[01:57:42.560 --> 01:57:43.560]   So you could--
[01:57:43.560 --> 01:57:44.560]   Yes, Z-Wave, I see.
[01:57:44.560 --> 01:57:45.560]   And Z-Wave.
[01:57:45.560 --> 01:57:47.800]   So ZigBee, Z-Wave, Wi-Fi and Bluetooth.
[01:57:47.800 --> 01:57:51.880]   Now you won't be able to do from the ZigBee.
[01:57:51.880 --> 01:57:55.040]   If you don't have the Hue Hub, you won't be able to do all the fancy things.
[01:57:55.040 --> 01:57:59.480]   Like some things you just can't do with their SDK.
[01:57:59.480 --> 01:58:03.320]   But this will control your lights and turn them on and off.
[01:58:03.320 --> 01:58:06.320]   Yeah, this is really interesting.
[01:58:06.320 --> 01:58:07.720]   And it has an API.
[01:58:07.720 --> 01:58:08.720]   So you can--
[01:58:08.720 --> 01:58:09.720]   Yep.
[01:58:09.720 --> 01:58:11.000]   So that's what's interesting, right?
[01:58:11.000 --> 01:58:13.440]   That you could create your own rules and stuff.
[01:58:13.440 --> 01:58:14.440]   Exactly.
[01:58:14.440 --> 01:58:15.440]   And--
[01:58:15.440 --> 01:58:17.440]   You could say turn off the Ecos at night, that kind of thing.
[01:58:17.440 --> 01:58:18.440]   Yes.
[01:58:18.440 --> 01:58:26.160]   You can actually post your rules and then other people can download them to their hub.
[01:58:26.160 --> 01:58:29.560]   But it's all a pull request as opposed to being pushed out.
[01:58:29.560 --> 01:58:30.960]   So there will be perhaps--
[01:58:30.960 --> 01:58:31.960]   An app store.
[01:58:31.960 --> 01:58:33.960]   A library, an app store.
[01:58:33.960 --> 01:58:35.560]   Oh, that's really interesting.
[01:58:35.560 --> 01:58:36.560]   Oh.
[01:58:36.560 --> 01:58:37.560]   So yeah.
[01:58:37.560 --> 01:58:38.560]   And it's small.
[01:58:38.560 --> 01:58:39.560]   I thought it was just cool.
[01:58:39.560 --> 01:58:40.560]   It's very tiny.
[01:58:40.560 --> 01:58:41.560]   It's tiny.
[01:58:41.560 --> 01:58:42.560]   So that's interesting.
[01:58:42.560 --> 01:58:43.560]   That's good.
[01:58:43.560 --> 01:58:48.000]   This is a software engineer's answer response.
[01:58:48.000 --> 01:58:51.920]   It's to the tower of Babel that has become--
[01:58:51.920 --> 01:58:57.440]   Well, and it's a little bit easier to use looking at what I've seen so far.
[01:58:57.440 --> 01:59:02.800]   It's a little bit easier to use than some of the things like OpenHab and HomeBridge and
[01:59:02.800 --> 01:59:04.360]   I think Home Assistant.
[01:59:04.360 --> 01:59:06.800]   Some of those are pretty tough.
[01:59:06.800 --> 01:59:10.920]   I mean, it spares you the effort of getting everything up and running on a pie.
[01:59:10.920 --> 01:59:11.920]   Not working.
[01:59:11.920 --> 01:59:15.520]   Oh, it looks like you have to add your own Wi-Fi to it.
[01:59:15.520 --> 01:59:17.480]   That dongle is for Z-Wave and Z-V support.
[01:59:17.480 --> 01:59:18.480]   Oh, OK.
[01:59:18.480 --> 01:59:19.480]   Oh, that's interesting.
[01:59:19.480 --> 01:59:20.480]   But it comes with it.
[01:59:20.480 --> 01:59:22.480]   Oh, I get it.
[01:59:22.480 --> 01:59:23.480]   OK.
[01:59:23.480 --> 01:59:25.480]   And then-- huh.
[01:59:25.480 --> 01:59:26.480]   Huh.
[01:59:26.480 --> 01:59:31.480]   I'm trying to get my head around it.
[01:59:31.480 --> 01:59:32.480]   Huh.
[01:59:32.480 --> 01:59:36.040]   Because normally, in a way, your hub would be your phone for this.
[01:59:36.040 --> 01:59:40.640]   No, usually for things like this, that's only in HomeKit world.
[01:59:40.640 --> 01:59:45.480]   Everywhere else you have a specified dedicated hub device.
[01:59:45.480 --> 01:59:46.880]   Like the smart things hub.
[01:59:46.880 --> 01:59:48.480]   Like smart things, like Wink.
[01:59:48.480 --> 01:59:49.480]   Right.
[01:59:49.480 --> 01:59:50.480]   OK.
[01:59:50.480 --> 01:59:51.480]   Like a pie running over here.
[01:59:51.480 --> 01:59:54.680]   Not supporting Nest and Ecobee's kind of a problem.
[01:59:54.680 --> 01:59:55.680]   Ah, yeah.
[01:59:55.680 --> 01:59:56.680]   But I mean--
[01:59:56.680 --> 02:00:00.240]   Those are the big guys, aren't they?
[02:00:00.240 --> 02:00:05.440]   For one particular thing, for thermostats, I think they support--
[02:00:05.440 --> 02:00:06.760]   They support Nest cameras.
[02:00:06.760 --> 02:00:09.680]   They support Nest doorbells, Ecobee locks.
[02:00:09.680 --> 02:00:11.760]   Ecobee doesn't make locks.
[02:00:11.760 --> 02:00:14.040]   Well, they don't support them.
[02:00:14.040 --> 02:00:15.040]   You're right.
[02:00:15.040 --> 02:00:16.040]   They don't.
[02:00:16.040 --> 02:00:17.040]   That's why.
[02:00:17.040 --> 02:00:20.600]   Ecobee makes a light switch, and they make a sensor for temperature
[02:00:20.600 --> 02:00:21.760]   for your air conditioner.
[02:00:21.760 --> 02:00:25.160]   Those are the only things Ecobee makes right now.
[02:00:25.160 --> 02:00:25.720]   Very nice.
[02:00:25.720 --> 02:00:28.200]   This looks kind of cool.
[02:00:28.200 --> 02:00:31.160]   I think Kevin Tofel might buy and tread it.
[02:00:31.160 --> 02:00:34.440]   Kevin Tofel is actually going to get a review unit
[02:00:34.440 --> 02:00:35.680]   and test it out for us.
[02:00:35.680 --> 02:00:38.400]   What I suspected.
[02:00:38.400 --> 02:00:39.520]   We actually fought over it.
[02:00:39.520 --> 02:00:44.080]   We were like, OK, rock paper scissors, who gets it?
[02:00:44.080 --> 02:00:47.840]   It's always fun with Kevin and I fight for a device.
[02:00:47.840 --> 02:00:52.320]   Jeff Jarvis, do you have a numero for us?
[02:00:52.320 --> 02:00:52.960]   Oh, you're muted.
[02:00:52.960 --> 02:00:53.960]   Now you are muted.
[02:00:53.960 --> 02:00:55.280]   Oh, there you go.
[02:00:55.280 --> 02:00:57.200]   United Airlines retired.
[02:00:57.200 --> 02:00:58.400]   It's last 747.
[02:00:58.400 --> 02:00:59.720]   They hit a big hula out of it.
[02:00:59.720 --> 02:01:02.360]   But they're going to milk a little more out,
[02:01:02.360 --> 02:01:04.320]   because you can use your miles and an auction
[02:01:04.320 --> 02:01:06.200]   to go visit the boneyard.
[02:01:06.200 --> 02:01:08.640]   Oh, I'd love to do that.
[02:01:08.640 --> 02:01:09.920]   I thought that's why I put this in here.
[02:01:09.920 --> 02:01:10.640]   I thought you'd like to--
[02:01:10.640 --> 02:01:13.240]   Oh, I've always wanted to see the boneyard.
[02:01:13.240 --> 02:01:18.640]   In the last 747, and then you have dinner underneath it.
[02:01:18.640 --> 02:01:19.160]   Wow.
[02:01:19.160 --> 02:01:20.600]   And then I don't know if you can see the truck.
[02:01:20.600 --> 02:01:23.400]   So you're actually-- it's kind of like accompanying--
[02:01:23.400 --> 02:01:25.000]   it's kind of like going to the funeral,
[02:01:25.000 --> 02:01:27.840]   like accompanying it to the graveyard.
[02:01:27.840 --> 02:01:28.840]   Yeah.
[02:01:28.840 --> 02:01:30.960]   Now where is the boneyard?
[02:01:30.960 --> 02:01:32.280]   The boneyard is hundreds of--
[02:01:32.280 --> 02:01:34.880]   Boneyard is an-- oh, I forget that we're shooting.
[02:01:34.880 --> 02:01:36.600]   Is it an air zone?
[02:01:36.600 --> 02:01:41.280]   Must be somewhere dry and spacious.
[02:01:41.280 --> 02:01:43.200]   I thought it was Arizona, but let's see.
[02:01:43.200 --> 02:01:45.240]   Is it Memphis?
[02:01:45.240 --> 02:01:48.000]   Memphis is dry.
[02:01:48.000 --> 02:01:48.720]   Where is it?
[02:01:48.720 --> 02:01:51.920]   You are Saturday.
[02:01:51.920 --> 02:01:54.520]   The highlight for most would be the--
[02:01:54.520 --> 02:01:54.840]   Oh, the Mississippi.
[02:01:54.840 --> 02:01:58.640]   June 2nd visit to nearby Tupelo, Mississippi,
[02:01:58.640 --> 02:02:02.280]   where you know it's the Universal Asset Management Aircraft
[02:02:02.280 --> 02:02:04.160]   Disassembly Center.
[02:02:04.160 --> 02:02:05.560]   So they're not just sitting there.
[02:02:05.560 --> 02:02:07.240]   They're taking them apart.
[02:02:07.240 --> 02:02:08.720]   Yeah.
[02:02:08.720 --> 02:02:12.240]   Oh, the military aircraft boneyard is in, apparently.
[02:02:12.240 --> 02:02:13.800]   That's the one I want to go to.
[02:02:13.800 --> 02:02:14.480]   Yeah.
[02:02:14.480 --> 02:02:17.160]   The Davis-Monthan Air Force Boneyard in Tucson.
[02:02:17.160 --> 02:02:20.480]   I'm sure I really want to go to Tupelo.
[02:02:20.480 --> 02:02:21.480]   Wait, is it an--
[02:02:21.480 --> 02:02:23.920]   Oh, Tupelo, honey is?
[02:02:23.920 --> 02:02:25.840]   Tupelo is where Elvis was born.
[02:02:25.840 --> 02:02:27.640]   Right.
[02:02:27.640 --> 02:02:29.080]   I wouldn't mind buying-- you know what?
[02:02:29.080 --> 02:02:31.280]   Sometimes I get so much work done on a long flight.
[02:02:31.280 --> 02:02:33.120]   I think I want to buy one of the business--
[02:02:33.120 --> 02:02:34.960]   I'm in a new office.
[02:02:34.960 --> 02:02:37.960]   Yeah, I just make people want to play with no wife
[02:02:37.960 --> 02:02:38.920]   while I'm working.
[02:02:38.920 --> 02:02:40.160]   And I'll get stuff done.
[02:02:40.160 --> 02:02:42.760]   That's a good idea.
[02:02:42.760 --> 02:02:48.080]   Mike Elgin, who unfortunately we lost because of poor internet,
[02:02:48.080 --> 02:02:52.840]   was going to mention as his pick squid, which
[02:02:52.840 --> 02:02:53.840]   used to be papyrus.
[02:02:53.840 --> 02:02:56.200]   It actually, I think it comes with the Pixelbook.
[02:02:56.200 --> 02:02:57.040]   Maybe not.
[02:02:57.040 --> 02:02:58.400]   I know mine has it.
[02:02:58.400 --> 02:03:01.280]   And he says, this is a great way to take notes
[02:03:01.280 --> 02:03:04.080]   on not only your Pixelbook, but any tablets or Android
[02:03:04.080 --> 02:03:06.440]   devices you have.
[02:03:06.440 --> 02:03:08.000]   You write with your finger or--
[02:03:08.000 --> 02:03:09.360]   and I think about the Pixelbook.
[02:03:09.360 --> 02:03:10.920]   It comes with the stylus.
[02:03:10.920 --> 02:03:12.400]   That's why I feel like my--
[02:03:12.400 --> 02:03:15.120]   do you have squid on your Pixelbook, Jeff?
[02:03:15.120 --> 02:03:15.960]   I think I do.
[02:03:15.960 --> 02:03:19.840]   I think it comes with it because of the stylus, the pen
[02:03:19.840 --> 02:03:20.360]   on the Pixelbook.
[02:03:20.360 --> 02:03:20.920]   Yes, I do.
[02:03:20.920 --> 02:03:22.400]   Yes, I do.
[02:03:22.400 --> 02:03:26.880]   So this is a handy little note-taking app.
[02:03:26.880 --> 02:03:28.640]   I have yet to really--
[02:03:28.640 --> 02:03:30.280]   I know I can do it.
[02:03:30.280 --> 02:03:31.640]   You know what I do?
[02:03:31.640 --> 02:03:33.960]   Yeah, I want to type most of the time.
[02:03:33.960 --> 02:03:38.360]   But there are times I need to sketch out things,
[02:03:38.360 --> 02:03:39.920]   because I'm trying to figure something out,
[02:03:39.920 --> 02:03:41.600]   and I need to make a drawing of it.
[02:03:41.600 --> 02:03:43.280]   It's good for stuff like that.
[02:03:43.280 --> 02:03:45.200]   I do do that a lot.
[02:03:45.200 --> 02:03:46.200]   Yeah.
[02:03:46.200 --> 02:03:49.000]   Mostly on my Apple iPad, though, with a pencil as a put.
[02:03:49.000 --> 02:03:52.000]   But you could easily do it on this.
[02:03:52.000 --> 02:03:55.520]   And you know I use my pencil or my pen on my surface
[02:03:55.520 --> 02:03:56.880]   all the time.
[02:03:56.880 --> 02:03:58.800]   Yes, you show salt all the time.
[02:03:58.800 --> 02:04:01.840]   And I have a pick that isn't available yet,
[02:04:01.840 --> 02:04:08.320]   maybe hugely expensive, but is so gosh darn cool.
[02:04:08.320 --> 02:04:13.160]   Cool looking that I just want it.
[02:04:13.160 --> 02:04:15.760]   And it's the new Microsoft Surface Hub.
[02:04:15.760 --> 02:04:18.400]   Have you seen the video for this yet?
[02:04:18.400 --> 02:04:19.960]   Yes.
[02:04:19.960 --> 02:04:23.640]   It is very, very sexy.
[02:04:23.640 --> 02:04:26.320]   So the Surface Hub--
[02:04:26.320 --> 02:04:28.080]   actually, this is not their video.
[02:04:28.080 --> 02:04:29.760]   I want to find their videos.
[02:04:29.760 --> 02:04:31.000]   Here's Microsoft's video.
[02:04:31.000 --> 02:04:32.960]   It's on YouTube.
[02:04:32.960 --> 02:04:34.280]   And let me just play this a little bit,
[02:04:34.280 --> 02:04:39.400]   and you can salivate.
[02:04:39.400 --> 02:04:43.040]   So the Surface Hub, as it stands now,
[02:04:43.040 --> 02:04:49.160]   is a fairly expensive 55-inch Windows sort of Windows
[02:04:49.160 --> 02:04:52.560]   device that allows you--
[02:04:52.560 --> 02:04:56.320]   corporations buy it for their conference room and stuff.
[02:04:56.320 --> 02:04:57.960]   Now they're showing the guts, but when
[02:04:57.960 --> 02:05:00.960]   we get to the screen part, this is very exciting.
[02:05:00.960 --> 02:05:03.040]   Now Microsoft's pre-announcing it.
[02:05:03.040 --> 02:05:04.560]   It won't be available until next year.
[02:05:04.560 --> 02:05:06.240]   And what they didn't say is how much it will be,
[02:05:06.240 --> 02:05:09.360]   but they did say it will be less expensive than the current
[02:05:09.360 --> 02:05:14.120]   Surface Hub, Surface Hub 2, which we're hoping.
[02:05:14.120 --> 02:05:15.800]   Now, A, look at that.
[02:05:15.800 --> 02:05:18.440]   She just rotated it into portrait mode.
[02:05:18.440 --> 02:05:19.280]   Now here's another thing.
[02:05:19.280 --> 02:05:20.720]   That's a fingerprint sensor.
[02:05:20.720 --> 02:05:23.160]   As soon as it saw her fingerprint,
[02:05:23.160 --> 02:05:25.520]   it logged her into the hub.
[02:05:25.520 --> 02:05:27.600]   Now she's going to use probably--
[02:05:27.600 --> 02:05:28.920]   well, there's the whiteboard, but she's
[02:05:28.920 --> 02:05:34.080]   going to probably use Skype for Teams here.
[02:05:34.080 --> 02:05:36.720]   And now you're having a conversation
[02:05:36.720 --> 02:05:40.560]   with somebody in full human scale.
[02:05:40.560 --> 02:05:43.040]   Yeah.
[02:05:43.040 --> 02:05:44.040]   It rotates.
[02:05:44.040 --> 02:05:45.040]   Look at that.
[02:05:45.040 --> 02:05:45.540]   Not that I want.
[02:05:45.540 --> 02:05:46.040]   Ooh, that's fun.
[02:05:46.040 --> 02:05:47.040]   That's cool.
[02:05:47.040 --> 02:05:48.560]   It's got a pencil.
[02:05:48.560 --> 02:05:51.080]   You can draw on it.
[02:05:51.080 --> 02:05:53.960]   Here's why I'm waiting to see what the price is.
[02:05:53.960 --> 02:05:56.000]   But one of the things we always want to do--
[02:05:56.000 --> 02:05:59.960]   well, there's a price point I'd be willing to pay for this,
[02:05:59.960 --> 02:06:01.040]   because one of the things we always
[02:06:01.040 --> 02:06:04.120]   wanted to do with our avatars, you guys,
[02:06:04.120 --> 02:06:05.400]   if for people watching the video,
[02:06:05.400 --> 02:06:10.840]   see our hosts in landscape TV displays.
[02:06:10.840 --> 02:06:13.360]   But my original idea was to have them in portraits.
[02:06:13.360 --> 02:06:15.760]   So you would be roughly like, life's--
[02:06:15.760 --> 02:06:17.320]   yeah, yes.
[02:06:17.320 --> 02:06:19.440]   But that's the problem.
[02:06:19.440 --> 02:06:22.040]   We never-- John, was it that we couldn't figure it out
[02:06:22.040 --> 02:06:23.760]   or that it was just too expensive to buy
[02:06:23.760 --> 02:06:27.120]   the additional hardware that would make this--
[02:06:27.120 --> 02:06:28.840]   it would be very expensive.
[02:06:28.840 --> 02:06:33.120]   But now this surface hub, man, maybe I could--
[02:06:33.120 --> 02:06:36.240]   maybe you guys will be living in one next-- this time next year.
[02:06:36.240 --> 02:06:40.520]   Look, you could put four together in portrait mode.
[02:06:40.520 --> 02:06:43.320]   In landscape mode, you could put two side by side.
[02:06:43.320 --> 02:06:44.680]   And they're synced together.
[02:06:44.680 --> 02:06:48.800]   So the four together make one giant display.
[02:06:48.800 --> 02:06:50.600]   55 inches.
[02:06:50.600 --> 02:06:51.440]   Here's the catch.
[02:06:51.440 --> 02:06:53.600]   Not available till next year.
[02:06:53.600 --> 02:06:56.360]   And they didn't say how much it would cost.
[02:06:56.360 --> 02:06:58.040]   An arm and a leg.
[02:06:58.040 --> 02:07:02.360]   If you have to ask, well, you can get for $8,000.
[02:07:02.360 --> 02:07:03.560]   You can get the 55 inch now.
[02:07:03.560 --> 02:07:10.560]   So it'll probably be less, like $79.99.
[02:07:10.560 --> 02:07:11.800]   Too rich for my blood.
[02:07:11.800 --> 02:07:16.280]   Burke says you'd need a scalar for each TV.
[02:07:16.280 --> 02:07:18.400]   No, because I don't think so, Burke, because they're
[02:07:18.400 --> 02:07:20.600]   designed to work with Skype.
[02:07:20.600 --> 02:07:21.920]   So I think you just need to--
[02:07:21.920 --> 02:07:23.840]   Oh, but we don't use the business Skype.
[02:07:23.840 --> 02:07:27.200]   Well, we would start, because by then I think we'll have to.
[02:07:27.200 --> 02:07:28.200]   Oh!
[02:07:28.200 --> 02:07:29.200]   Sorry.
[02:07:29.200 --> 02:07:30.200]   I know.
[02:07:30.200 --> 02:07:32.640]   That is how I feel about the end of the business Skype.
[02:07:32.640 --> 02:07:33.140]   I know.
[02:07:33.140 --> 02:07:34.640]   Everybody hates it, but--
[02:07:34.640 --> 02:07:35.880]   Yeah.
[02:07:35.880 --> 02:07:40.440]   But imagine you could be sideways life-sized next to me.
[02:07:40.440 --> 02:07:43.920]   So when you punch me, it'll really look real.
[02:07:43.920 --> 02:07:46.640]   I feel that, like, then did you see that woman?
[02:07:46.640 --> 02:07:48.440]   She had to wear pants.
[02:07:48.440 --> 02:07:52.000]   So that's kind of a downside.
[02:07:52.000 --> 02:07:53.920]   Why did she have to wear pants because of the hub?
[02:07:53.920 --> 02:07:55.520]   Because our whole body was like--
[02:07:55.520 --> 02:07:58.120]   Body there.
[02:07:58.120 --> 02:07:59.080]   She's also standing.
[02:07:59.080 --> 02:08:01.080]   I would not do the whole show standing.
[02:08:01.080 --> 02:08:05.960]   Oh, I work with refused to ever use Skype video,
[02:08:05.960 --> 02:08:07.920]   because they have to wear pants.
[02:08:07.920 --> 02:08:09.200]   No, because they take a few.
[02:08:09.200 --> 02:08:10.720]   I'm very confused.
[02:08:10.720 --> 02:08:12.280]   I don't want to be--
[02:08:12.280 --> 02:08:13.880]   I don't want to-- this is the phone.
[02:08:13.880 --> 02:08:15.320]   Oh, yeah, my mom won't take it either.
[02:08:15.320 --> 02:08:17.920]   Yeah, I hate using video for, like, a lot of--
[02:08:17.920 --> 02:08:18.760]   Phone calls.
[02:08:18.760 --> 02:08:19.280]   Yeah.
[02:08:19.280 --> 02:08:20.120]   Phone calls.
[02:08:20.120 --> 02:08:22.600]   Or when I'm taking notes, because then I'm trying to do my job
[02:08:22.600 --> 02:08:23.920]   and take notes on a call, and they're, like,
[02:08:23.920 --> 02:08:25.000]   trying to make eye contact.
[02:08:25.000 --> 02:08:25.600]   And I'm like, ah.
[02:08:25.600 --> 02:08:28.360]   Yeah, no, that's not good.
[02:08:28.360 --> 02:08:30.400]   Any either of you use duo ever?
[02:08:30.400 --> 02:08:30.880]   Google's--
[02:08:30.880 --> 02:08:31.440]   Yes.
[02:08:31.440 --> 02:08:32.640]   You do.
[02:08:32.640 --> 02:08:33.640]   Really?
[02:08:33.640 --> 02:08:36.280]   With only two people that I managed to finagle into,
[02:08:36.280 --> 02:08:37.960]   like, calling and using it.
[02:08:37.960 --> 02:08:41.400]   You think I think it's one of which was--
[02:08:41.400 --> 02:08:43.160]   Not my husband.
[02:08:43.160 --> 02:08:45.240]   He's like, no.
[02:08:45.240 --> 02:08:46.120]   I'm trying-- good.
[02:08:46.120 --> 02:08:48.920]   Maybe you and I can duo, because I can't find anybody who
[02:08:48.920 --> 02:08:50.160]   will do it with me.
[02:08:50.160 --> 02:08:54.320]   But is it better than FaceTime or Skype?
[02:08:54.320 --> 02:08:56.880]   It's better than Skype, just because it's easier.
[02:08:56.880 --> 02:08:57.400]   Yeah.
[02:08:57.400 --> 02:08:58.480]   It's not--
[02:08:58.480 --> 02:08:59.320]   It's kind of a life face time.
[02:08:59.320 --> 02:09:01.400]   It's definitely not better than FaceTime, because you--
[02:09:01.400 --> 02:09:03.160]   here's what you do.
[02:09:03.160 --> 02:09:05.760]   I get-- I text-- or I'm like, ooh, I'll call them on duo.
[02:09:05.760 --> 02:09:09.160]   And then I send them the text, because they're not on duo.
[02:09:09.160 --> 02:09:10.640]   And then I have this text conversation.
[02:09:10.640 --> 02:09:11.680]   Stacy, did you send me this?
[02:09:11.680 --> 02:09:13.120]   Or am I being trolled?
[02:09:13.120 --> 02:09:14.000]   No, I sent you that.
[02:09:14.000 --> 02:09:15.080]   Can you download this app?
[02:09:15.080 --> 02:09:16.160]   Why would I want to download that app?
[02:09:16.160 --> 02:09:17.800]   Where would I go to download that app?
[02:09:17.800 --> 02:09:19.280]   Stacy, no one's downloading this app.
[02:09:19.280 --> 02:09:20.920]   Oh, it wants my password.
[02:09:20.920 --> 02:09:25.280]   I sent texts, because it says invite people.
[02:09:25.280 --> 02:09:28.360]   I invited both my mother, who I do a lot of FaceTiming with,
[02:09:28.360 --> 02:09:30.960]   and my daughter, who no longer has an iPhone
[02:09:30.960 --> 02:09:33.280]   and can't talk to grandma anymore.
[02:09:33.280 --> 02:09:34.920]   And I was trying to match make them.
[02:09:34.920 --> 02:09:37.560]   I said, you can use duo to talk to each other.
[02:09:37.560 --> 02:09:39.960]   Neither one signed up.
[02:09:39.960 --> 02:09:42.320]   Yeah, this was my--
[02:09:42.320 --> 02:09:44.160]   Yeah.
[02:09:44.160 --> 02:09:47.440]   All right, and somebody in the chatroom, trust no wan--
[02:09:47.440 --> 02:09:49.040]   love it, good name--
[02:09:49.040 --> 02:09:51.320]   points out that the Google Jamboard, which
[02:09:51.320 --> 02:09:55.400]   looks similar to that other Microsoft thing,
[02:09:55.400 --> 02:09:57.360]   the Surface board, is only five ways.
[02:09:57.360 --> 02:09:59.280]   And that uses Hangouts, right?
[02:09:59.280 --> 02:10:00.440]   Yeah, I don't think--
[02:10:00.440 --> 02:10:02.280]   We have a Jamboard at school.
[02:10:02.280 --> 02:10:02.760]   We got one.
[02:10:02.760 --> 02:10:04.360]   How does that work?
[02:10:04.360 --> 02:10:05.080]   Works pretty well.
[02:10:05.080 --> 02:10:05.880]   It really does.
[02:10:05.880 --> 02:10:08.280]   You can cooperate and do things.
[02:10:08.280 --> 02:10:08.760]   Yeah.
[02:10:08.760 --> 02:10:10.960]   Well, five grand, it's considerably less than the Surface
[02:10:10.960 --> 02:10:11.960]   hub.
[02:10:12.960 --> 02:10:17.720]   I don't know if it's cool, however.
[02:10:17.720 --> 02:10:20.120]   Wait, did you just call a Microsoft product cool?
[02:10:20.120 --> 02:10:22.160]   Isn't that interesting?
[02:10:22.160 --> 02:10:22.680]   Yeah.
[02:10:22.680 --> 02:10:24.880]   But it's only cool in juxtaposition
[02:10:24.880 --> 02:10:28.240]   to the etch-a-sketch look of the Jamboard.
[02:10:28.240 --> 02:10:30.680]   [LAUGHING]
[02:10:30.680 --> 02:10:32.160]   All right, kids.
[02:10:32.160 --> 02:10:32.680]   All right.
[02:10:32.680 --> 02:10:33.760]   Time to call it a day.
[02:10:33.760 --> 02:10:36.080]   Thank you, Jeff Jarvis, City University of New York.
[02:10:36.080 --> 02:10:36.960]   CUNY, we call it.
[02:10:36.960 --> 02:10:39.480]   He's also at buzzmachine.com.
[02:10:39.480 --> 02:10:41.080]   It is always a slice.
[02:10:41.080 --> 02:10:44.080]   You could find Jeff's work everywhere,
[02:10:44.080 --> 02:10:47.680]   but do get the books because they're good.
[02:10:47.680 --> 02:10:48.440]   What would Google do?
[02:10:48.440 --> 02:10:50.200]   Public parts.
[02:10:50.200 --> 02:10:52.320]   And there's other geek-sparing gifts
[02:10:52.320 --> 02:10:54.880]   all about reinventing news.
[02:10:54.880 --> 02:10:56.440]   Always a pleasure, Jeff.
[02:10:56.440 --> 02:10:57.000]   Thank you.
[02:10:57.000 --> 02:10:57.760]   Always, sir.
[02:10:57.760 --> 02:10:58.640]   Always boss.
[02:10:58.640 --> 02:11:06.080]   Stacey Higginbotham soaked now in a fine mist of barbecue sauce.
[02:11:06.080 --> 02:11:07.920]   Oh, I'll say you're just saying, "Be sweat."
[02:11:07.920 --> 02:11:09.440]   And me sweat.
[02:11:09.440 --> 02:11:12.440]   Well, doesn't it come out your pores later in the day?
[02:11:12.440 --> 02:11:14.360]   You know, I don't think I ate enough for a meat sweats,
[02:11:14.360 --> 02:11:15.360]   because I would have them by now.
[02:11:15.360 --> 02:11:16.360]   Darn it.
[02:11:16.360 --> 02:11:18.480]   Everyone here is very disappointed, I'm sure.
[02:11:18.480 --> 02:11:20.480]   [LAUGHING]
[02:11:20.480 --> 02:11:24.600]   I learned about meat sweats from Stacey in Austin, Texas.
[02:11:24.600 --> 02:11:26.400]   So thank you.
[02:11:26.400 --> 02:11:27.240]   Have a run.
[02:11:27.240 --> 02:11:30.920]   You'll find Stacey's wonderful work at Stacey on IoT.com.
[02:11:30.920 --> 02:11:32.160]   Subscribe to her free newsletter.
[02:11:32.160 --> 02:11:34.640]   Listen to the podcast she does with Kevin.
[02:11:34.640 --> 02:11:35.440]   Kevin's great.
[02:11:35.440 --> 02:11:37.520]   I'm so glad he can podcast with us now.
[02:11:37.520 --> 02:11:39.520]   It's been really fun when he comes on.
[02:11:39.520 --> 02:11:40.360]   Yeah.
[02:11:40.360 --> 02:11:41.800]   Give him my regards.
[02:11:41.800 --> 02:11:44.200]   We should get him on again soon.
[02:11:44.200 --> 02:11:45.320]   I'll be gone next week.
[02:11:45.320 --> 02:11:48.200]   I'm sure you could convince him.
[02:11:48.200 --> 02:11:50.960]   There you go.
[02:11:50.960 --> 02:11:52.120]   Thank you, everybody, for joining us.
[02:11:52.120 --> 02:11:54.480]   We do this week in Google right after Windows Weekly
[02:11:54.480 --> 02:11:57.560]   on Wednesdays, about 130 Pacific, 430 Eastern.
[02:11:57.560 --> 02:12:01.640]   OK, we are 15 seconds late today, but pretty much 130--
[02:12:01.640 --> 02:12:03.440]   We were on time.
[02:12:03.440 --> 02:12:05.080]   That was wrong.
[02:12:05.080 --> 02:12:09.560]   On time, maybe, 130 PM Pacific, 430 Eastern, 2030 UTC.
[02:12:09.560 --> 02:12:12.360]   If you want to watch live, go to twit.tv/live.
[02:12:12.360 --> 02:12:14.200]   And if you do that, you're really
[02:12:14.200 --> 02:12:18.360]   ought to launch the chat room and join the crazy kids
[02:12:18.360 --> 02:12:19.160]   in the back of the class.
[02:12:19.160 --> 02:12:22.920]   They're at irc.twit.tv.
[02:12:22.920 --> 02:12:23.720]   That's a website.
[02:12:23.720 --> 02:12:25.560]   You can use your browser to chat.
[02:12:25.560 --> 02:12:28.440]   But if you get serious about it, get an IRC client
[02:12:28.440 --> 02:12:29.520]   and all the instructions are there.
[02:12:29.520 --> 02:12:33.040]   At irc.twit.tv.
[02:12:33.040 --> 02:12:35.360]   Now, if you can't watch live and I understand
[02:12:35.360 --> 02:12:38.840]   middle of the day, Wednesday is probably not ideal.
[02:12:38.840 --> 02:12:39.120]   Of course--
[02:12:39.120 --> 02:12:39.960]   It feels ideal to me.
[02:12:39.960 --> 02:12:43.320]   [LAUGHTER]
[02:12:43.320 --> 02:12:45.280]   You come on really at dinner time.
[02:12:45.280 --> 02:12:47.360]   It's kind of dinner time for you.
[02:12:47.360 --> 02:12:49.120]   Yes, well, today-- this is the first time
[02:12:49.120 --> 02:12:51.920]   I've not been hungry for this show.
[02:12:51.920 --> 02:12:53.120]   Although I am getting chest pains.
[02:12:53.120 --> 02:12:53.480]   So I am--
[02:12:53.480 --> 02:12:55.200]   I am the beat, Swetz.
[02:12:55.200 --> 02:12:57.640]   Here they come.
[02:12:57.640 --> 02:12:59.960]   You can get on to man versions and watch it
[02:12:59.960 --> 02:13:03.680]   at your leisure, twetz.tv/twig, or subscribe
[02:13:03.680 --> 02:13:05.520]   in your favorite pond catcher.
[02:13:05.520 --> 02:13:09.760]   That way you'll always have a copy ready to go.
[02:13:09.760 --> 02:13:10.440]   Thank you, everybody.
[02:13:10.440 --> 02:13:12.280]   We'll see you next time on This Week in Google.
[02:13:12.280 --> 02:13:12.840]   Bye-bye.
[02:13:12.840 --> 02:13:16.000]   [MUSIC PLAYING]
[02:13:16.000 --> 02:13:19.360]   [MUSIC PLAYING]
[02:13:19.360 --> 02:13:21.940]   (upbeat music)

