;FFMETADATA1
album=This Week In Google
artist=Leo Laporte, Jeff Jarvis and Gina Trapani
iTunPGAP=0
comment=http://twit.tv/twig135
encoded_by=iTunes v7.0
genre=Tech News
TGID=http://www.podtrac.com/pts/redirect.mp3/twit.cachefly.net/twig0115.mp3
TDES=Hosts: Leo Laporte, Jeff Jarvis, and Gina Trapani\
\
Apple's iPhone 4S keynote, what is PhoneGap, \# on Google +, Google won't screw up Android, and more cloud news.\
\
Download or subscribe to this show at twit.tv/twig.\
\
We invite you to read, add to, and amend our show notes.\
\
Friendfeed links for this episode.\
\
Thanks to Cachefly for the bandwidth for this show.\
\
Running time: 1:29:14
track=135
title=This Week In Google 135: I Dream Of Jarvis
date=2012
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.000]   It's time for Twink, this week in Google.
[00:00:03.000 --> 00:00:09.920]   What a great conversation we're about to have about privacy, Google, cookies, iOS, and what
[00:00:09.920 --> 00:00:13.560]   it means to be a good netizen with Rebecca McKinnon, author of the brand new book called
[00:00:13.560 --> 00:00:16.720]   Consent of the Networked, it's all coming up next on Twink.
[00:00:16.720 --> 00:00:17.720]   Stay here.
[00:00:17.720 --> 00:00:18.720]   [MUSIC]
[00:00:18.720 --> 00:00:19.720]   Netcast, you love.
[00:00:19.720 --> 00:00:20.720]   From people you trust.
[00:00:20.720 --> 00:00:21.720]   This is Twink.
[00:00:21.720 --> 00:00:37.400]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:37.400 --> 00:00:50.240]   This is Twink, this week in Google, Episode 135, recorded February 22nd, 2012.
[00:00:50.240 --> 00:00:52.680]   I dream of Jarvis.
[00:00:52.680 --> 00:00:57.240]   This week in Google is brought to you by Ford, featuring the EcoBoost engine with turbocharger
[00:00:57.240 --> 00:00:58.440]   and direct injection.
[00:00:58.440 --> 00:01:05.360]   Look for EcoBoost on the 2012 Explorer and Edge, the 2013 Escape, and at Ford.com/technology.
[00:01:05.360 --> 00:01:11.960]   It's time for Twink, this week in Google to show the covers Google.
[00:01:11.960 --> 00:01:15.000]   Oh boy, is there a lot to talk about this week.
[00:01:15.000 --> 00:01:22.560]   But also other services like Facebook and Twitter, the cloud, but it's not social media, it's
[00:01:22.560 --> 00:01:25.400]   whatever Jeff and Gina want to talk about is really what it is.
[00:01:25.400 --> 00:01:29.280]   Jeff Jarvis is here, he is of course professor of journalism at the City University of New
[00:01:29.280 --> 00:01:34.520]   York and the author of Public Parts Fabulous Book All About Life in Public.
[00:01:34.520 --> 00:01:35.520]   Great to have you.
[00:01:35.520 --> 00:01:36.520]   Always wonderful.
[00:01:36.520 --> 00:01:37.520]   Highlight of the week.
[00:01:37.520 --> 00:01:39.160]   Highlight of my week too.
[00:01:39.160 --> 00:01:43.560]   Gina Trapani is also here from SmarterWare.org.
[00:01:43.560 --> 00:01:45.560]   She's the author of Think Up App.
[00:01:45.560 --> 00:01:48.960]   Steve Martin is going to join us one of your customers, ladies today.
[00:01:48.960 --> 00:01:49.960]   Is that today?
[00:01:49.960 --> 00:01:50.960]   Yeah.
[00:01:50.960 --> 00:01:57.600]   Well, and he used and the reason it's relevant is that he contacted me about how do I save
[00:01:57.600 --> 00:01:58.600]   my tweets?
[00:01:58.600 --> 00:02:02.360]   I might want to write a book and I said I know who could do it.
[00:02:02.360 --> 00:02:09.160]   Gina Trapani, you hooked him up with Think Up and he wrote the book.
[00:02:09.160 --> 00:02:12.560]   So I don't have a copy yet, but I'll have to ask is there an acknowledgement?
[00:02:12.560 --> 00:02:14.720]   I'm sure there is to think up.
[00:02:14.720 --> 00:02:20.480]   We speaking of books, we have somebody that I'm very excited to talk to with us as well.
[00:02:20.480 --> 00:02:24.520]   Rebecca McKinnon is a journalist and the author of a brand, hello, Rebecca.
[00:02:24.520 --> 00:02:25.520]   Good to see you.
[00:02:25.520 --> 00:02:26.520]   Good to see you.
[00:02:26.520 --> 00:02:29.360]   Also a Schwartz Senior Fellow at the New America Foundation.
[00:02:29.360 --> 00:02:35.520]   She co-founded Global Voices, which is about the coolest thing in the world and is the author
[00:02:35.520 --> 00:02:37.840]   of a brand new book called Consent of the Network.
[00:02:37.840 --> 00:02:41.360]   In fact, I think your ears must have been burning because we've been talking about you
[00:02:41.360 --> 00:02:43.760]   on this show for a couple of weeks now.
[00:02:43.760 --> 00:02:44.760]   Oh, really?
[00:02:44.760 --> 00:02:45.760]   Yeah.
[00:02:45.760 --> 00:02:49.760]   Well, you make some very relevant points.
[00:02:49.760 --> 00:02:50.760]   So welcome, Rebecca.
[00:02:50.760 --> 00:02:52.560]   Thanks very much.
[00:02:52.560 --> 00:02:57.320]   And in fact, this week is probably a good week to have Rebecca on because Google's in
[00:02:57.320 --> 00:02:58.320]   all sorts of do-do.
[00:02:58.320 --> 00:03:10.880]   In fact, they're now facing a class action lawsuit over the Safari Cookie Circumvention.
[00:03:10.880 --> 00:03:13.400]   The policy is a subject, but this is not just about privacy.
[00:03:13.400 --> 00:03:17.680]   The subtitle is The Worldwide Struggle for Internet Freedom.
[00:03:17.680 --> 00:03:20.040]   It's a balancing act, isn't it?
[00:03:20.040 --> 00:03:21.040]   Absolutely.
[00:03:21.040 --> 00:03:27.600]   I mean, we have a situation now where our lives as citizens, not only our personal lives and
[00:03:27.600 --> 00:03:33.880]   our jobs and our businesses depend on the internet, but our politics increasingly depend
[00:03:33.880 --> 00:03:35.400]   on the internet.
[00:03:35.400 --> 00:03:40.440]   And the internet is primarily, you know, we're depending on these privately owned platforms
[00:03:40.440 --> 00:03:45.200]   and services to conduct our politics more and more.
[00:03:45.200 --> 00:03:50.800]   And so there's this issue of how do we ensure that the internet evolves in a way that's
[00:03:50.800 --> 00:03:56.360]   really compatible with the kind of political values that we have and is compatible with
[00:03:56.360 --> 00:04:01.160]   the kind of rights that we believe we're entitled to and that people around the world
[00:04:01.160 --> 00:04:04.520]   are risking their lives to achieve every day.
[00:04:04.520 --> 00:04:11.480]   And how do we make sure that not only governments are respecting our rights when they're seeking
[00:04:11.480 --> 00:04:16.920]   to regulate the internet, but also that companies are thinking about how there are various commercial
[00:04:16.920 --> 00:04:22.440]   choices and cool features and so on are going to affect people politically all over the
[00:04:22.440 --> 00:04:23.540]   world.
[00:04:23.540 --> 00:04:28.080]   You have a unique perspective because for nine or ten years you were stationed in Beijing
[00:04:28.080 --> 00:04:29.840]   as a correspondent for CNN.
[00:04:29.840 --> 00:04:33.360]   The years the internet arose, 1992 to 2001.
[00:04:33.360 --> 00:04:34.360]   That's right.
[00:04:34.360 --> 00:04:37.680]   That must have been a very interesting perspective.
[00:04:37.680 --> 00:04:40.720]   Were you following what was happening in the US with the internet at the same time as you
[00:04:40.720 --> 00:04:42.880]   were in Beijing or were you focused on China?
[00:04:42.880 --> 00:04:44.960]   Well, I was covering China.
[00:04:44.960 --> 00:04:48.080]   We were just covering, we were responsible for coming.
[00:04:48.080 --> 00:04:50.760]   China wasn't a tech reporter at all.
[00:04:50.760 --> 00:04:51.800]   So I read the news.
[00:04:51.800 --> 00:04:57.960]   So I, you know, as a consumer and a user of the internet as it appeared, I was sort of
[00:04:57.960 --> 00:05:03.160]   aware generally, but not as an internet techy specialist person.
[00:05:03.160 --> 00:05:07.240]   But it was really interesting because the commercial internet showed up in China in
[00:05:07.240 --> 00:05:13.360]   1995 and immediately the journalists there and a lot of diplomats and business people
[00:05:13.360 --> 00:05:16.680]   were like, wow, the Chinese Communist Party is never going to survive this thing.
[00:05:16.680 --> 00:05:18.120]   There's just no way.
[00:05:18.120 --> 00:05:27.600]   But the Chinese government has actually managed to adapt to the internet and also manage the
[00:05:27.600 --> 00:05:34.280]   internet both through blocking foreign sites and also basically controlling domestic internet
[00:05:34.280 --> 00:05:39.880]   companies to the extent that, yeah, people do feel freer to talk about a lot of things,
[00:05:39.880 --> 00:05:44.100]   but they still can't use the internet to organize an opposition party or to call for
[00:05:44.100 --> 00:05:47.680]   multi-party democracy without going to jail as usual.
[00:05:47.680 --> 00:05:53.320]   And so the fact that China is kind of exhibit A for how an authoritarian government can actually
[00:05:53.320 --> 00:05:57.040]   survive the internet is the cautionary tale.
[00:05:57.040 --> 00:06:02.320]   And kind of poke some holes, I think, in some assumptions that some of us make about the
[00:06:02.320 --> 00:06:07.480]   nature of the internet and the relationship between the internet and politics and government.
[00:06:07.480 --> 00:06:10.480]   Jeff and Gina, please don't let me hog this.
[00:06:10.480 --> 00:06:14.080]   I was, I was, I was a big fan of Rebecca's.
[00:06:14.080 --> 00:06:18.160]   I was, I was privileged to be there in the meeting where Global Voices started, where
[00:06:18.160 --> 00:06:19.160]   she started it.
[00:06:19.160 --> 00:06:23.360]   And it was, it was an amazing thing to watch at Harvard's Berkman Center going way back.
[00:06:23.360 --> 00:06:26.840]   And so I have immense respect for Rebecca on these topics.
[00:06:26.840 --> 00:06:28.440]   And I'm really glad that she wrote the book.
[00:06:28.440 --> 00:06:31.600]   At the end of mine, I too, try to come around to say that we need principles to protect
[00:06:31.600 --> 00:06:32.600]   the internet.
[00:06:32.600 --> 00:06:37.160]   So did you happen to see Robert McDowell's, I presume you did op-ed in the Wall Street
[00:06:37.160 --> 00:06:45.040]   Journal today, the FCC Commissioner who warns of United Nations governments of the internet.
[00:06:45.040 --> 00:06:51.680]   And there's just so many players now who are under the guise of piracy, privacy, security,
[00:06:51.680 --> 00:06:59.920]   policy, or just simple power are trying to step in and claim some sovereignty over the
[00:06:59.920 --> 00:07:00.920]   net.
[00:07:00.920 --> 00:07:02.560]   What was your talk about the Bells piece?
[00:07:02.560 --> 00:07:03.560]   Yeah.
[00:07:03.560 --> 00:07:08.080]   Well, I read that piece and I thought he raised a lot of really important points.
[00:07:08.080 --> 00:07:13.680]   And in fact, I have a chapter in the book dealing with these internet governance fights.
[00:07:13.680 --> 00:07:18.640]   And you know, the internet, the standards and protocols of the internet and our addressing
[00:07:18.640 --> 00:07:24.560]   system and IP addressing system and so on are all managed by kind of multi stakeholder
[00:07:24.560 --> 00:07:26.240]   organizations of different kinds.
[00:07:26.240 --> 00:07:32.720]   And ICANN, which manages the domain name system as a nonprofit company based in San Diego.
[00:07:32.720 --> 00:07:38.400]   And the policy processes that they use are what we call multi stakeholder involves industry
[00:07:38.400 --> 00:07:44.360]   and, you know, nonprofit groups and consultations with governments and all kinds of different
[00:07:44.360 --> 00:07:51.560]   people who have a stake and who also have expertise in how the internet should be developed.
[00:07:51.560 --> 00:07:55.880]   But there are a lot of governments who feel that these functions need to be taken under
[00:07:55.880 --> 00:08:01.240]   UN control and the argument of China and Russia and quite a lot of other countries is
[00:08:01.240 --> 00:08:05.840]   that internet governance should be left to governments.
[00:08:05.840 --> 00:08:09.760]   And of course, when you're concerned about human rights and you're concerned about kind
[00:08:09.760 --> 00:08:16.920]   of the bureaucrats from various countries like China and Russia determining how our
[00:08:16.920 --> 00:08:21.080]   standards and protocols evolve, that's a big concern.
[00:08:21.080 --> 00:08:26.080]   So you know, the argument being that when it comes to internet governance, it needs
[00:08:26.080 --> 00:08:28.480]   to stay multi stakeholder.
[00:08:28.480 --> 00:08:34.880]   It needs to continue to involve not just governments, but the engineering committee, I'm sorry,
[00:08:34.880 --> 00:08:40.520]   the engineering community and companies, the industry that actually develop the technologies
[00:08:40.520 --> 00:08:44.320]   who actually understand the technology better than the government bureaucrats.
[00:08:44.320 --> 00:08:48.960]   And also civil society, you know, the people who actually use the internet for activism
[00:08:48.960 --> 00:08:51.480]   and need their rights to be protected.
[00:08:51.480 --> 00:08:55.520]   And a lot of governments aren't interested in those people's rights being protected.
[00:08:55.520 --> 00:09:01.280]   So we need kind of new ways of coordinating how the internet evolves to make sure that,
[00:09:01.280 --> 00:09:07.600]   you know, everybody who uses it is going to be able to use it in the manner that is really
[00:09:07.600 --> 00:09:11.200]   consistent with our universal rights.
[00:09:11.200 --> 00:09:14.560]   We need new ways or do we just need the present ways that have worked up to now not to be
[00:09:14.560 --> 00:09:16.040]   messed with?
[00:09:16.040 --> 00:09:22.520]   Well, you know, I guess it depends on how you define what's been working and what's
[00:09:22.520 --> 00:09:24.040]   not and so on.
[00:09:24.040 --> 00:09:34.240]   I think definitely with ICANN and with some of these organizations that coordinate protocols
[00:09:34.240 --> 00:09:38.560]   and standards, you know, they tend to work pretty well.
[00:09:38.560 --> 00:09:41.080]   There are some issues with ICANN.
[00:09:41.080 --> 00:09:45.120]   You know, the people who are kind of ICANN insiders will say that, you know, certain
[00:09:45.120 --> 00:09:50.920]   copyright industries have had too much say over how domain names are organized or, you
[00:09:50.920 --> 00:09:57.080]   know, there's a lot of issues and questions about whether civil society sort of the non-governmental
[00:09:57.080 --> 00:10:03.640]   and non-commercial interests have enough say in these processes, but also the internet's
[00:10:03.640 --> 00:10:04.760]   changing, right?
[00:10:04.760 --> 00:10:13.040]   So it used to be that the businesses that were involved with really creating the internet
[00:10:13.040 --> 00:10:19.560]   and the engineers involved with developing protocols and standards were all in the West
[00:10:19.560 --> 00:10:20.840]   and that has changed.
[00:10:20.840 --> 00:10:22.760]   This is the development now with the internet.
[00:10:22.760 --> 00:10:24.840]   The growth is all in the developing world.
[00:10:24.840 --> 00:10:26.400]   It's all in the non-West.
[00:10:26.400 --> 00:10:30.800]   You have a lot of non-Western companies that are becoming increasingly powerful.
[00:10:30.800 --> 00:10:37.920]   You have Chinese companies that are more and more influential in bodies like the IETF,
[00:10:37.920 --> 00:10:41.440]   the engineering body that sets standards and protocols.
[00:10:41.440 --> 00:10:47.280]   And so things are internationalizing and getting more complicated and there are more
[00:10:47.280 --> 00:10:53.320]   countries who are saying, well, you know, it's fine and good for these people in the West
[00:10:53.320 --> 00:10:57.280]   to say, oh, you know, the internet should be this way because it's good for people in
[00:10:57.280 --> 00:11:02.080]   the West, but it may or may not serve the needs of people in villages with slow bandwidth
[00:11:02.080 --> 00:11:03.560]   and other things.
[00:11:03.560 --> 00:11:10.960]   So there are some valid reasons why perhaps more voices and more, there needs to be more
[00:11:10.960 --> 00:11:17.600]   representation in coordinating the internet's development from the non-West and from the
[00:11:17.600 --> 00:11:19.600]   developing world.
[00:11:19.600 --> 00:11:25.200]   I think there's some legitimate arguments to be made there, but I don't think necessarily
[00:11:25.200 --> 00:11:29.000]   it's up to governments to take control either.
[00:11:29.000 --> 00:11:34.280]   So, you know, as the internet evolves, I think the things that worked when the internet was
[00:11:34.280 --> 00:11:39.200]   just kind of a Western thing may not work so well anyway as we evolve.
[00:11:39.200 --> 00:11:43.960]   So it is going to have to change in different ways, but I don't think a UN style solution
[00:11:43.960 --> 00:11:47.120]   is the solution.
[00:11:47.120 --> 00:11:50.120]   People in the chatroom say, why do we have to get government involved at all?
[00:11:50.120 --> 00:11:51.600]   Well, it's involved.
[00:11:51.600 --> 00:11:53.120]   It's been evolved for a long time.
[00:11:53.120 --> 00:11:54.120]   It's too late.
[00:11:54.120 --> 00:11:57.000]   You know, it's way, way too late.
[00:11:57.000 --> 00:12:01.080]   And the reason why it's involved is not just because it barged in there.
[00:12:01.080 --> 00:12:06.800]   It's because a lot of voters and internet users invited government in there because people
[00:12:06.800 --> 00:12:13.520]   say, oh, you know, I don't want my children to be accessing porn on the internet or people
[00:12:13.520 --> 00:12:18.160]   say, oh, you know, I need to have my networks protected from cybercrime.
[00:12:18.160 --> 00:12:25.720]   And so there, you know, we have invited regulation and invited legislation and invited government
[00:12:25.720 --> 00:12:30.160]   in to the internet to help us solve a lot of problems that are serious problems that
[00:12:30.160 --> 00:12:31.360]   do need to be solved.
[00:12:31.360 --> 00:12:37.080]   But the problem is how do you make sure that when the government is in there that it's
[00:12:37.080 --> 00:12:39.880]   behaving accountably and not abusing its power?
[00:12:39.880 --> 00:12:43.800]   I mean, that's ultimately the issue.
[00:12:43.800 --> 00:12:48.040]   And part of the problem is even in democratic countries, you know, here in the U.S., we
[00:12:48.040 --> 00:12:52.520]   had a big fight over the Stop Online Piracy Act, you know, the whole question of how
[00:12:52.520 --> 00:12:55.480]   you deal with copyright infringement.
[00:12:55.480 --> 00:13:01.920]   And you know, the problem is that you get legislators or congresspeople who, you know,
[00:13:01.920 --> 00:13:07.640]   have constituents asking them to solve a problem and they grasp at a solution that ends up,
[00:13:07.640 --> 00:13:12.240]   you know, creating problems in other areas or infringing on people's rights in other
[00:13:12.240 --> 00:13:13.240]   ways.
[00:13:13.240 --> 00:13:17.640]   And so they're not kind of trying to think about, okay, how does this affect everybody
[00:13:17.640 --> 00:13:22.000]   who has a stake on the internet in all the different ways it might affect people?
[00:13:22.000 --> 00:13:28.440]   And so there's not, you know, it's -- and then you end up having mechanisms put in place,
[00:13:28.440 --> 00:13:31.400]   you know, for instance, fighting cyber crime.
[00:13:31.400 --> 00:13:35.080]   Oftentimes the solution that people grasp at is, oh, we need more surveillance.
[00:13:35.080 --> 00:13:39.880]   We need more government access to networks so that we can catch the bad guys.
[00:13:39.880 --> 00:13:42.720]   But you know, that comes with a price, right?
[00:13:42.720 --> 00:13:47.680]   And so how do you hold accountable the abuse of power?
[00:13:47.680 --> 00:13:52.320]   How do you prevent the abuse of power as you're trying to solve these problems?
[00:13:52.320 --> 00:13:58.080]   And unfortunately, it seems that instead of some sort of consensus between engineering
[00:13:58.080 --> 00:14:02.600]   interests, government interests, and we should mention commercial interests too, Tim Wu,
[00:14:02.600 --> 00:14:07.680]   and the master switch kind of posits that there's no way we can maintain this as a open
[00:14:07.680 --> 00:14:13.480]   system because just with other previous open systems like long distance, commercial interests
[00:14:13.480 --> 00:14:15.440]   are just going to say, hey, okay, thanks very much.
[00:14:15.440 --> 00:14:16.440]   See you all later.
[00:14:16.440 --> 00:14:17.440]   It's ours now.
[00:14:17.440 --> 00:14:21.520]   We really have at least three parties, and if you include the public, I guess there's
[00:14:21.520 --> 00:14:26.400]   four parties who all have a vested interest, which all seem at odds with one another to
[00:14:26.400 --> 00:14:27.400]   be frank.
[00:14:27.400 --> 00:14:29.480]   Well, oftentimes they are.
[00:14:29.480 --> 00:14:31.200]   How do you reach consensus?
[00:14:31.200 --> 00:14:32.200]   Yeah.
[00:14:32.200 --> 00:14:36.880]   Well, see, you know, well, look at how we do it kind of in a physical democracy.
[00:14:36.880 --> 00:14:38.640]   You don't really reach consensus.
[00:14:38.640 --> 00:14:40.680]   You just kind of balance off the interest.
[00:14:40.680 --> 00:14:41.680]   Nobody's happy.
[00:14:41.680 --> 00:14:42.680]   Nobody's happy.
[00:14:42.680 --> 00:14:48.200]   But you kind of set up the system in such a way that as people pursue their self-interest,
[00:14:48.200 --> 00:14:52.960]   they're holding each other in check so nobody can go too far without a backlash, you know,
[00:14:52.960 --> 00:14:54.600]   or that's how it's meant to be.
[00:14:54.600 --> 00:14:59.120]   So I think, you know, that's likely to be where it kind of goes.
[00:14:59.120 --> 00:15:03.280]   And you also sometimes get alliances like with the anti-SOPA stuff.
[00:15:03.280 --> 00:15:11.360]   You saw industry and the human rights community joining forces against the legislation, which
[00:15:11.360 --> 00:15:13.480]   was fascinating.
[00:15:13.480 --> 00:15:18.280]   And so yeah, I think we need to find a way to kind of have a balance of power in a way.
[00:15:18.280 --> 00:15:21.240]   But the title of your book is Consented of the Networked.
[00:15:21.240 --> 00:15:27.120]   And really we are, as the public, we are the ones who have the most at risk, most to gain.
[00:15:27.120 --> 00:15:32.680]   And unfortunately have the least, not only authority, but least understanding of what
[00:15:32.680 --> 00:15:33.680]   the issues are.
[00:15:33.680 --> 00:15:35.760]   Yeah, that's right.
[00:15:35.760 --> 00:15:40.480]   And I think if we gain more understanding and we start thinking about these digital spaces
[00:15:40.480 --> 00:15:46.080]   that we're spending growing amounts of time in, these services that we're increasingly
[00:15:46.080 --> 00:15:52.720]   relying on, if we think of ourselves less as a user of these things, just sort of passive
[00:15:52.720 --> 00:15:57.440]   and more sort of as I use the term "netizen," sort of a citizen of these spaces, and think
[00:15:57.440 --> 00:16:03.360]   about, you know, what are the levers I can use, but as a consumer, as an investor, as
[00:16:03.360 --> 00:16:10.000]   a citizen, as a voter, how can I sort of exercise my concerns, voice my concerns, and
[00:16:10.000 --> 00:16:17.520]   exercise my rights, and push the people who do hold power in this digital realm to change
[00:16:17.520 --> 00:16:24.760]   what they're doing, or to modify what they're doing and respect our rights, and to recognize
[00:16:24.760 --> 00:16:27.400]   it's in their long-term self-interest to do so.
[00:16:27.400 --> 00:16:31.200]   It's in the government's interest to respect our rights, otherwise they have no legitimacy
[00:16:31.200 --> 00:16:34.440]   in the long-term, which is politically bad for them.
[00:16:34.440 --> 00:16:39.080]   And with companies, if they don't respect our rights over the long run, and we totally
[00:16:39.080 --> 00:16:44.160]   distrust them, that's going to diminish the value of what they're trying to sell and what
[00:16:44.160 --> 00:16:45.880]   they're trying to do.
[00:16:45.880 --> 00:16:52.400]   And so I think in the long run, the legitimacy of the internet as a whole and the various
[00:16:52.400 --> 00:17:01.320]   actors on it does depend on behaving in a way that respects people, but they're only
[00:17:01.320 --> 00:17:08.720]   going to understand that if people make that clear and impose costs when our rights aren't
[00:17:08.720 --> 00:17:09.720]   respected.
[00:17:09.720 --> 00:17:12.640]   But there's not, well, by the way, we're talking Rebecca McKinnon, her book is The Consent of
[00:17:12.640 --> 00:17:15.200]   the Networked Brand New from Basic Books.
[00:17:15.200 --> 00:17:18.040]   The website is at consentofthenetworked.com.
[00:17:18.040 --> 00:17:19.040]   Correct.
[00:17:19.040 --> 00:17:20.920]   So you can go there and find it.
[00:17:20.920 --> 00:17:21.920]   There it is.
[00:17:21.920 --> 00:17:23.280]   You can find out more about that.
[00:17:23.280 --> 00:17:28.880]   Although, you know, you even have disputes among individuals over what we want from the
[00:17:28.880 --> 00:17:29.880]   internet.
[00:17:29.880 --> 00:17:35.360]   For instance, the European Union wants to pass this right to be forgotten legislation, which
[00:17:35.360 --> 00:17:40.760]   I'm sure not only do governments in Europe, but many people, particularly Germans, in
[00:17:40.760 --> 00:17:42.400]   ... No, I'm just kidding.
[00:17:42.400 --> 00:17:45.600]   I'll get the email in Europe.
[00:17:45.600 --> 00:17:50.560]   Think this is a good thing, but it's technically ridiculous.
[00:17:50.560 --> 00:17:54.920]   And this is why this seems to be such a Gordian knot, to be honest.
[00:17:54.920 --> 00:17:55.920]   Yeah.
[00:17:55.920 --> 00:17:58.040]   Well, this happens on a lot of issues.
[00:17:58.040 --> 00:18:02.440]   I mean, you get other constituencies in a number of democracies that are calling for
[00:18:02.440 --> 00:18:06.400]   censorship of the internet to protect children from pornography.
[00:18:06.400 --> 00:18:11.520]   And they're demanding that their elected representatives do something.
[00:18:11.520 --> 00:18:14.840]   And so ... That's why God created Lamar Smith.
[00:18:14.840 --> 00:18:19.920]   You know, so I think people ... There's part of it is that there just needs to be more
[00:18:19.920 --> 00:18:24.680]   nuanced public discussion about kind of how the internet works and what's possible and
[00:18:24.680 --> 00:18:26.160]   so on.
[00:18:26.160 --> 00:18:34.160]   And I think maybe industry needs to kind of treat the concerns with respect, even if
[00:18:34.160 --> 00:18:39.800]   the solutions proposed might seem idiotic, because the concerns are real.
[00:18:39.800 --> 00:18:44.360]   And I think if those concerns are treated with respect, then ...
[00:18:44.360 --> 00:18:45.840]   Well, not mocked.
[00:18:45.840 --> 00:18:46.920]   ... I don't like to be trust.
[00:18:46.920 --> 00:18:47.920]   Not mocked anyway.
[00:18:47.920 --> 00:18:48.920]   Yeah, not mocked.
[00:18:48.920 --> 00:18:49.920]   Yeah.
[00:18:49.920 --> 00:18:56.120]   I mean, it's ... I think one has to respect the fact that a person is freaked out by good
[00:18:56.120 --> 00:18:58.800]   Google Earth showing what's in their backyard.
[00:18:58.800 --> 00:18:59.800]   You know, I think ...
[00:18:59.800 --> 00:19:00.800]   But they did no way, way, way, way.
[00:19:00.800 --> 00:19:01.800]   Google doesn't do that.
[00:19:01.800 --> 00:19:02.960]   Google doesn't do that.
[00:19:02.960 --> 00:19:03.960]   So that's just the problem.
[00:19:03.960 --> 00:19:07.600]   It's that kind of presumption that causes them.
[00:19:07.600 --> 00:19:12.520]   But I am going to defend Rebecca in the sense that we cannot ... We will not be able to
[00:19:12.520 --> 00:19:17.160]   reach consensus if we don't look to the other person and say, "Well, I understand your
[00:19:17.160 --> 00:19:20.920]   desire for privacy and why you might want to blur your building."
[00:19:20.920 --> 00:19:25.560]   But there does need to be some dialogue about what ... How does that work?
[00:19:25.560 --> 00:19:30.080]   It's legal, but ... But if you look at the basis of Vivian Redeens regulations in the
[00:19:30.080 --> 00:19:35.040]   EU, it's polls that say things like, "Are you concerned about your privacy?"
[00:19:35.040 --> 00:19:36.520]   100% should say they are.
[00:19:36.520 --> 00:19:38.120]   Of course, I am.
[00:19:38.120 --> 00:19:39.120]   Everyone is, right?
[00:19:39.120 --> 00:19:41.400]   Without any real discussion of the harm.
[00:19:41.400 --> 00:19:43.760]   Well, but I think that's ... And I think that's what we ...
[00:19:43.760 --> 00:19:46.600]   It's our mission is to educate.
[00:19:46.600 --> 00:19:48.520]   There's a lot of education needs to be done.
[00:19:48.520 --> 00:19:49.520]   There's tons.
[00:19:49.520 --> 00:19:50.520]   And you look at ...
[00:19:50.520 --> 00:19:54.320]   And also all these issues, it's balances and it's trade-offs.
[00:19:54.320 --> 00:20:03.600]   And how to negotiate that on a global network where the European regulations end up perfecting
[00:20:03.600 --> 00:20:07.520]   people outside of Europe, et cetera, et cetera.
[00:20:07.520 --> 00:20:17.760]   So it's ... How do we hold them accountable for affecting how our internet works?
[00:20:17.760 --> 00:20:23.640]   We don't really have the way of approaching that that can bring us to a solution at the
[00:20:23.640 --> 00:20:24.640]   moment.
[00:20:24.640 --> 00:20:30.000]   And your solution, and ... I'm extrapolating, correct me if I'm ...
[00:20:30.000 --> 00:20:34.480]   Is that by ... The last chapter of your book is Building a Netizen-centric Internet.
[00:20:34.480 --> 00:20:35.480]   It is involvement.
[00:20:35.480 --> 00:20:39.520]   Involvement of us as net ... We need to be good netizens.
[00:20:39.520 --> 00:20:43.320]   You also talk about utopianism versus reality.
[00:20:43.320 --> 00:20:47.880]   Because I think Jeff and I are utopian.
[00:20:47.880 --> 00:20:49.120]   We're like John Perry Barlow.
[00:20:49.120 --> 00:20:52.640]   We believe in power to the people, freedom of cyber space.
[00:20:52.640 --> 00:20:53.640]   No, I mean ...
[00:20:53.640 --> 00:20:59.480]   It's definitely ... You have to start with awareness.
[00:20:59.480 --> 00:21:01.760]   You have to start with people being involved.
[00:21:01.760 --> 00:21:08.440]   And just to clarify my critique of utopianism, which is I think different than some other
[00:21:08.440 --> 00:21:17.880]   people's critique of utopianism, which I won't get into here, really has to do with the fact
[00:21:17.880 --> 00:21:22.320]   that I started out my academic life studying the origins of the Chinese and the Soviet
[00:21:22.320 --> 00:21:25.400]   revolutions that were started by utopianism.
[00:21:25.400 --> 00:21:35.600]   We thought that you could change human nature and had, I think, overly unrealistic ideals
[00:21:35.600 --> 00:21:39.560]   about what could be changed.
[00:21:39.560 --> 00:21:47.560]   And the idealistic people got used by very evil people and very bad things happened.
[00:21:47.560 --> 00:21:52.960]   And so I just tend to feel that, look, the internet and what's going on on the internet
[00:21:52.960 --> 00:21:55.440]   is an extension of human nature.
[00:21:55.440 --> 00:22:00.080]   Human beings are pretty screwed up a lot.
[00:22:00.080 --> 00:22:03.560]   And I don't think that the internet changes human nature.
[00:22:03.560 --> 00:22:06.280]   I think we need to work under the assumption that ...
[00:22:06.280 --> 00:22:08.280]   Agreed.
[00:22:08.280 --> 00:22:15.360]   We remain human and different aspects of human nature get amplified on the internet in all
[00:22:15.360 --> 00:22:17.080]   kinds of interesting ways.
[00:22:17.080 --> 00:22:25.200]   Some very positive and some very negative and some just delightful and some just strange.
[00:22:25.200 --> 00:22:32.920]   But given human nature and what it is, I just think we need to be realistic that, yes, there
[00:22:32.920 --> 00:22:39.480]   are many delightful and wonderful things that the internet empowers, but it also empowers
[00:22:39.480 --> 00:22:47.520]   another set of things that are kind of part of who we are as a society and as a species.
[00:22:47.520 --> 00:22:50.840]   And that's not going to be changed by technology.
[00:22:50.840 --> 00:23:00.040]   And so we need to kind of embrace reality and accept that absolute power corrupts absolutely
[00:23:00.040 --> 00:23:06.000]   as much in digital space as it does in physical space.
[00:23:06.000 --> 00:23:14.800]   That it doesn't matter how enlightened a particular technology CEO might be or how idealistic
[00:23:14.800 --> 00:23:18.000]   or utopian certain programmers might be.
[00:23:18.000 --> 00:23:19.160]   They're going to mess up.
[00:23:19.160 --> 00:23:21.120]   They may abuse power.
[00:23:21.120 --> 00:23:29.840]   And we need to kind of construct a digital world that kind of defaults for the absolute
[00:23:29.840 --> 00:23:36.280]   certainty that bad things will happen and that we need to constrain that.
[00:23:36.280 --> 00:23:37.280]   It's made the internet.
[00:23:37.280 --> 00:23:39.000]   It's made of people.
[00:23:39.000 --> 00:23:40.000]   That's right.
[00:23:40.000 --> 00:23:41.000]   Yeah.
[00:23:41.000 --> 00:23:42.000]   It is made of people.
[00:23:42.000 --> 00:23:43.000]   Go ahead, Gina.
[00:23:43.000 --> 00:23:44.000]   Please.
[00:23:44.000 --> 00:23:47.880]   You know, I was going to say that, you know, certainly the solution is for everyone to get
[00:23:47.880 --> 00:23:51.920]   more involved and to take their role as a netizen more seriously.
[00:23:51.920 --> 00:23:55.840]   And I may be saying this just because I'm a web developer, but for me, like if I have
[00:23:55.840 --> 00:23:59.560]   to choose between the engineering community, the government, commercial interests and the
[00:23:59.560 --> 00:24:06.480]   public, I expect the engineering community, especially to step up and to explain the government
[00:24:06.480 --> 00:24:10.840]   how technology works and the public so we can have those more nuanced discussions.
[00:24:10.840 --> 00:24:11.840]   And I kind of.
[00:24:11.840 --> 00:24:12.840]   And inform discussions.
[00:24:12.840 --> 00:24:13.840]   You're right, Gina.
[00:24:13.840 --> 00:24:20.560]   And I look, I like to see benign engineers, you know, kind of making a difference.
[00:24:20.560 --> 00:24:23.280]   I think code is some of the best kind of activism possible.
[00:24:23.280 --> 00:24:27.600]   You know, when you look at what Reddit did with SOPA and you look at things, even like
[00:24:27.600 --> 00:24:29.880]   like screw, screw, screw or focus on the user.
[00:24:29.880 --> 00:24:32.680]   I mean, these are people who have opinions and are expressing them through tools.
[00:24:32.680 --> 00:24:35.940]   And I think that's really important because the engineers are the ones that really understand
[00:24:35.940 --> 00:24:37.280]   how this stuff works.
[00:24:37.280 --> 00:24:42.120]   And the engineering community has to figure out how to communicate that well to the public
[00:24:42.120 --> 00:24:45.880]   and to the government so that these decisions can be informed.
[00:24:45.880 --> 00:24:50.760]   I mean, the problem is that, you know, the culture of being an engineer, sort of growing
[00:24:50.760 --> 00:24:54.880]   up a nerd or a computer programmer is that you sort of you view yourself as an outcast
[00:24:54.880 --> 00:24:58.960]   as someone who, you know, it is necessarily the most popular kid in school, perhaps.
[00:24:58.960 --> 00:25:02.480]   I really want to see that mythology change in the next couple of generations that has
[00:25:02.480 --> 00:25:03.480]   to happen.
[00:25:03.480 --> 00:25:07.040]   I really want to see, you know, a president of the United States who used to be a software
[00:25:07.040 --> 00:25:10.160]   developer like I really look forward to that day.
[00:25:10.160 --> 00:25:12.600]   But I and again, I wouldn't that be something.
[00:25:12.600 --> 00:25:13.600]   It would never happen.
[00:25:13.600 --> 00:25:14.600]   Okay.
[00:25:14.600 --> 00:25:15.600]   But yeah, pointing that up.
[00:25:15.600 --> 00:25:16.600]   But good.
[00:25:16.600 --> 00:25:21.160]   And then you need people who understand people as well as they understand code and there's
[00:25:21.160 --> 00:25:22.360]   often a disconnect there.
[00:25:22.360 --> 00:25:27.040]   But for our audience at Twig, we have a lot of engineers or at least just technology
[00:25:27.040 --> 00:25:34.000]   enthusiasts who understand how tech works more than the public government or even the
[00:25:34.000 --> 00:25:37.920]   guys at business suits who just care about, you know, next quarters earnings.
[00:25:37.920 --> 00:25:39.720]   And I want to see, I want to see more of that.
[00:25:39.720 --> 00:25:41.360]   And I would say again, that's our mission.
[00:25:41.360 --> 00:25:46.440]   I mean, and I unfortunately we speak to enthusiasts who already probably have a pretty good idea.
[00:25:46.440 --> 00:25:51.000]   But that is really the mission of everything that I do is we got to get people to understand
[00:25:51.000 --> 00:25:52.800]   technology given the facts.
[00:25:52.800 --> 00:25:57.880]   Let me let them make their decisions, but make make informed decisions.
[00:25:57.880 --> 00:26:02.080]   I would also guess that some of the issue is is that the perfect is the enemy of the
[00:26:02.080 --> 00:26:04.400]   good engineers are purists.
[00:26:04.400 --> 00:26:06.320]   See this black and white.
[00:26:06.320 --> 00:26:13.320]   And this the soap of discussion was a perfect example where people who are more political
[00:26:13.320 --> 00:26:18.000]   like me like Patel said, well, we got to find a middle ground because and then purists like
[00:26:18.000 --> 00:26:22.600]   me say, but there's and Jeff say, it's good for everybody.
[00:26:22.600 --> 00:26:23.600]   Screw it.
[00:26:23.600 --> 00:26:27.840]   And I think engineers are art art art art art art.
[00:26:27.840 --> 00:26:32.200]   Utopians, which we're accused of doing and we laugh about, but engineers are optimists
[00:26:32.200 --> 00:26:36.880]   because they basically try to find solutions problems and believe there is a solution to
[00:26:36.880 --> 00:26:40.800]   a problem that can be found if we're smart enough and work hard enough on it.
[00:26:40.800 --> 00:26:45.600]   But politics is messy. It is the art of consensus is the art of just as you who are saying Rebecca,
[00:26:45.600 --> 00:26:47.000]   there isn't a perfect solution.
[00:26:47.000 --> 00:26:49.400]   It's the art of you give a little, I'll give a little.
[00:26:49.400 --> 00:26:52.160]   And that's something engineers may not be very good at.
[00:26:52.160 --> 00:26:58.520]   And I think one of the other challenges too is increasingly these tools and platforms
[00:26:58.520 --> 00:27:08.320]   are being used by people in cultures and political contexts and languages that the engineers
[00:27:08.320 --> 00:27:12.760]   you know here in the United States developing certain platforms anyway, just can't kind
[00:27:12.760 --> 00:27:20.640]   of conceive of how that experience ends up interacting in a particular political or cultural
[00:27:20.640 --> 00:27:24.120]   context and what ends up meaning for people.
[00:27:24.120 --> 00:27:29.280]   And so this is this is where I think not only diversifying the community of engineers and
[00:27:29.280 --> 00:27:35.480]   bringing people kind of from the developing world and from the non West into the engineering
[00:27:35.480 --> 00:27:44.680]   community is really important so that the concerns and needs of the world's fastest growing part
[00:27:44.680 --> 00:27:47.880]   of the internet can really be understood better.
[00:27:47.880 --> 00:27:55.800]   But also find ways to kind of consult with types of people who face risks and vulnerabilities
[00:27:55.800 --> 00:28:00.200]   that somebody sitting in Palo Alto just might never have been able to dream about.
[00:28:00.200 --> 00:28:01.760]   Yeah, that's exciting.
[00:28:01.760 --> 00:28:02.760]   Absolutely.
[00:28:02.760 --> 00:28:05.760]   Diversity is so important for that reason.
[00:28:05.760 --> 00:28:08.560]   Luckily, we have a global network that can help us.
[00:28:08.560 --> 00:28:09.560]   Thank God.
[00:28:09.560 --> 00:28:13.480]   Cuz Sandhill Road is the opposite of our first.
[00:28:13.480 --> 00:28:17.520]   Right, but so many of our tools are built to conceive in Palo Alto, California, which
[00:28:17.520 --> 00:28:18.520]   is a completely different thing.
[00:28:18.520 --> 00:28:19.520]   We got to change that.
[00:28:19.520 --> 00:28:20.520]   Yes, we absolutely do.
[00:28:20.520 --> 00:28:22.680]   And I do think it is starting to change.
[00:28:22.680 --> 00:28:23.680]   And I see more and more as well.
[00:28:23.680 --> 00:28:29.200]   I loved going to the web and seeing the vital startup community in Europe and it's just now
[00:28:29.200 --> 00:28:31.960]   it's all over the world and there are startups all over the world.
[00:28:31.960 --> 00:28:36.840]   And immigration into here and investment out of here is both important.
[00:28:36.840 --> 00:28:37.840]   Yep.
[00:28:37.840 --> 00:28:42.240]   Although I have to say, and Rebecca, you speak Chinese and you have a unique perspective
[00:28:42.240 --> 00:28:47.400]   on China, the language barrier is really a significant wall between what's happened.
[00:28:47.400 --> 00:28:51.320]   And there is a ton of stuff happening in China.
[00:28:51.320 --> 00:28:55.520]   And because of the language barrier, the membrane is impermeable.
[00:28:55.520 --> 00:29:00.360]   Yeah, well, that's what makes, you know, there's kind of a set of engineers and technical
[00:29:00.360 --> 00:29:05.920]   people who are bilingual and those people I think are going to become really valuable.
[00:29:05.920 --> 00:29:06.920]   Huge.
[00:29:06.920 --> 00:29:07.920]   Yeah.
[00:29:07.920 --> 00:29:08.920]   Hugely valuable.
[00:29:08.920 --> 00:29:10.400]   You actually grew up part of your life in China, right?
[00:29:10.400 --> 00:29:13.600]   Yeah, I spent a couple of years of grade school in Beijing.
[00:29:13.600 --> 00:29:15.280]   I envy you.
[00:29:15.280 --> 00:29:20.000]   And what a change you've seen in Beijing over 20 years, it's just remarkable.
[00:29:20.000 --> 00:29:21.000]   Yeah.
[00:29:21.000 --> 00:29:24.040]   Yeah, it's Beijing's unrecognizable from when I was a kid.
[00:29:24.040 --> 00:29:26.760]   Just I mean, this is completely off topic, but I'm going to throw it anyway.
[00:29:26.760 --> 00:29:28.000]   By the way, book, let's give you a plug.
[00:29:28.000 --> 00:29:31.360]   Consider the network Rebecca McKinnon, everybody buy it.
[00:29:31.360 --> 00:29:35.280]   What is your sense of the, what role, you know, I was in Beijing a couple of years ago
[00:29:35.280 --> 00:29:36.280]   and it blew me away.
[00:29:36.280 --> 00:29:38.080]   I was a Chinese studies major in college like you.
[00:29:38.080 --> 00:29:39.080]   Really?
[00:29:39.080 --> 00:29:40.080]   Yeah.
[00:29:40.080 --> 00:29:41.080]   And I fell in love with China.
[00:29:41.080 --> 00:29:42.080]   I don't know, I've forgotten all my Chinese.
[00:29:42.080 --> 00:29:43.560]   I fell in love with China.
[00:29:43.560 --> 00:29:46.800]   And when I went back and I had never gone and I went back, I went there for the first
[00:29:46.800 --> 00:29:49.400]   time, blew me away.
[00:29:49.400 --> 00:29:53.440]   And I, and I actually said at the time, I have a feeling this is the Chinese century,
[00:29:53.440 --> 00:29:55.640]   not the last century was the American century.
[00:29:55.640 --> 00:29:57.880]   This might be the Chinese century.
[00:29:57.880 --> 00:30:00.600]   And some people there said, "Man, there's some structural issues.
[00:30:00.600 --> 00:30:02.200]   It may not be economic."
[00:30:02.200 --> 00:30:05.480]   What's your sense of the role China will play going forward globally?
[00:30:05.480 --> 00:30:06.480]   Yeah.
[00:30:06.480 --> 00:30:08.080]   Well, I mean, it's hard to predict.
[00:30:08.080 --> 00:30:11.880]   I mean, obviously there are already a major economic powerhouse.
[00:30:11.880 --> 00:30:16.160]   You know, when it comes to technology, you know, you've got Chinese networking companies
[00:30:16.160 --> 00:30:24.680]   like Huawei and ZTE and so on that are globally competitive with Cisco and, and other major
[00:30:24.680 --> 00:30:27.320]   multinationals.
[00:30:27.320 --> 00:30:30.440]   And so, you know, there's, there's a lot going on.
[00:30:30.440 --> 00:30:34.600]   I think they're very smart and innovative people in China.
[00:30:34.600 --> 00:30:38.960]   I think the innovation is stifled somewhat by the system.
[00:30:38.960 --> 00:30:44.520]   There are a lot of things, particularly anything related to user generated content.
[00:30:44.520 --> 00:30:48.280]   There's a real disincentive from doing anything new because you don't know how to control
[00:30:48.280 --> 00:30:52.120]   it, you know, and, and then you might get in trouble and get your business shut down.
[00:30:52.120 --> 00:30:56.920]   So you're better off just kind of cloning stuff that already exists elsewhere.
[00:30:56.920 --> 00:31:02.160]   So there's, you know, I think there are some issues with the nature of their political
[00:31:02.160 --> 00:31:07.720]   system that does kind of get in the way of the Chinese people being as great as they
[00:31:07.720 --> 00:31:10.240]   really could be.
[00:31:10.240 --> 00:31:15.360]   But you know, certainly we're seeing in so many ways how, you know, it's that country
[00:31:15.360 --> 00:31:16.360]   is a player.
[00:31:16.360 --> 00:31:17.360]   Do they recognize that?
[00:31:17.360 --> 00:31:23.640]   Do they realize that, that their restrictive policies are holding them back economically?
[00:31:23.640 --> 00:31:24.640]   I think it's true.
[00:31:24.640 --> 00:31:28.280]   Certainly I know a lot of Chinese people who do feel that way.
[00:31:28.280 --> 00:31:32.840]   I know people, business people in China who feel that way.
[00:31:32.840 --> 00:31:38.440]   If they say that publicly, they're, you know, if a CEO of a Chinese company who says that
[00:31:38.440 --> 00:31:44.960]   to his friends goes out and says that to the New York Times, that's like, you know, gonna
[00:31:44.960 --> 00:31:47.160]   be problematic for him.
[00:31:47.160 --> 00:31:50.680]   So there's actually a lot of people who do recognize that.
[00:31:50.680 --> 00:31:54.620]   And there's even a lot of people in the government who, you know, if you talk to them privately,
[00:31:54.620 --> 00:32:00.340]   they'll say, yeah, you know, our system is, you know, ossified and we do need to find
[00:32:00.340 --> 00:32:03.320]   some way to move it forward.
[00:32:03.320 --> 00:32:08.080]   But the problem is like, how do you move from A to B without everything falling apart?
[00:32:08.080 --> 00:32:12.460]   And everybody in the system has so much at stake and they don't want to lose their jobs
[00:32:12.460 --> 00:32:15.600]   or they don't want to lose their business or, you know, a lot of people have a lot to
[00:32:15.600 --> 00:32:17.640]   lose these days in China.
[00:32:17.640 --> 00:32:21.160]   And so they don't want to take too much risk of getting in trouble.
[00:32:21.160 --> 00:32:25.300]   Because everybody's just sort of muddling along and just kind of making do with the system
[00:32:25.300 --> 00:32:26.820]   as it is.
[00:32:26.820 --> 00:32:31.940]   And nobody's kind of real, you know, I mean, I think if the business community in China
[00:32:31.940 --> 00:32:34.740]   really kind of stood up and said, you know, we're not going to take this anymore.
[00:32:34.740 --> 00:32:37.340]   This is lame.
[00:32:37.340 --> 00:32:43.820]   Perhaps things might change, but they don't have the kahones to do that at this moment.
[00:32:43.820 --> 00:32:44.820]   It's fascinating.
[00:32:44.820 --> 00:32:46.340]   We're going to take a break.
[00:32:46.340 --> 00:32:47.740]   Rebecca McKinnon is here.
[00:32:47.740 --> 00:32:48.740]   Consentofthenetwork.com.
[00:32:48.740 --> 00:32:53.280]   If you want to know more about her book, The Worldwide Struggle for Internet Freedom.
[00:32:53.280 --> 00:32:54.520]   And it couldn't be more timely, really.
[00:32:54.520 --> 00:32:59.560]   I mean, with the Arab Spring last year, and it's just a fascinating world we're living
[00:32:59.560 --> 00:33:01.400]   in and a very rapidly changing system.
[00:33:01.400 --> 00:33:08.360]   I do want to talk about Google and iOS.
[00:33:08.360 --> 00:33:11.840]   And Rebecca, if you would like to stick around and join us, it really is about privacy.
[00:33:11.840 --> 00:33:15.280]   So I think you might have something to say about it.
[00:33:15.280 --> 00:33:16.640]   It's an interesting debate.
[00:33:16.640 --> 00:33:20.500]   But before we do that, I want to talk also about Ford Motor Company.
[00:33:20.500 --> 00:33:26.740]   Our sponsor Ford, of course, is a 21st century car company, a global company, by all means
[00:33:26.740 --> 00:33:28.340]   making amazing vehicles.
[00:33:28.340 --> 00:33:32.700]   And it's just really inspiring when I've had the chance to speak to people at Ford
[00:33:32.700 --> 00:33:37.300]   about what they're doing, not just in the US, but worldwide.
[00:33:37.300 --> 00:33:38.340]   It's kind of inspiring.
[00:33:38.340 --> 00:33:43.240]   Here's a company that sells in pretty much every country of the world.
[00:33:43.240 --> 00:33:45.660]   They manufacture everywhere.
[00:33:45.660 --> 00:33:53.220]   They're trying to create a system that benefits from economies of scale in markets that are
[00:33:53.220 --> 00:33:54.840]   very diverse all over the world.
[00:33:54.840 --> 00:33:56.640]   And they have done some amazing things.
[00:33:56.640 --> 00:34:02.360]   And it's technology that is taking this 20th century industrial power into the technological
[00:34:02.360 --> 00:34:03.360]   21st century.
[00:34:03.360 --> 00:34:04.360]   It's very fascinating.
[00:34:04.360 --> 00:34:09.920]   You find out more about the technology story at Ford.com/technology.
[00:34:09.920 --> 00:34:14.460]   One of the things you can't forget is we talk about electric cars.
[00:34:14.460 --> 00:34:17.620]   They're doing it hybrid, hybrid electric diesel.
[00:34:17.620 --> 00:34:23.140]   But it's still the-- as Alan Malali calls it, the petrol car.
[00:34:23.140 --> 00:34:26.200]   I think we're the only country in the world that still calls it gasoline.
[00:34:26.200 --> 00:34:29.180]   The petrol engine, he calls it.
[00:34:29.180 --> 00:34:36.100]   And EcoBoost is wow, fascinating solution to an interesting problem that might seem
[00:34:36.100 --> 00:34:37.180]   impossible.
[00:34:37.180 --> 00:34:42.100]   How do you give consumers what they want, which is big, powerful cars and big, powerful
[00:34:42.100 --> 00:34:46.860]   engines, and still get the fuel economy that not only do consumers want, but the government's
[00:34:46.860 --> 00:34:47.860]   mandate.
[00:34:47.860 --> 00:34:50.740]   And that's where the EcoBoost is quite interesting.
[00:34:50.740 --> 00:34:57.060]   I drove the 3.5 liter EcoBoost in the SHO and I was blown away by the pickup, the torque.
[00:34:57.060 --> 00:34:58.460]   And yet the great gas mileage.
[00:34:58.460 --> 00:35:04.180]   They've got two liter EcoBoost engines now in the new 2012 Ford Explorer and the Ford
[00:35:04.180 --> 00:35:10.260]   Edge getting 28 highway miles per gallon of that two liter engine in a big utility vehicle.
[00:35:10.260 --> 00:35:11.260]   Amazing.
[00:35:11.260 --> 00:35:17.500]   They do it with two technologies, direct injection, producing a cooler-- I'm going to read this
[00:35:17.500 --> 00:35:20.700]   because I'm no automotive technologist.
[00:35:20.700 --> 00:35:26.860]   DI produces a cooler denser charge that generates more power per drop of fuel.
[00:35:26.860 --> 00:35:27.860]   And then they use turbo charges.
[00:35:27.860 --> 00:35:32.620]   I know a little bit about that because we've had turbocharged engines for years.
[00:35:32.620 --> 00:35:35.540]   But traditionally, that single turbine takes a while to spin up.
[00:35:35.540 --> 00:35:37.460]   It spins up from the exhaust.
[00:35:37.460 --> 00:35:40.860]   And the turbine compresses the air going into the piston, which again, gives you a much
[00:35:40.860 --> 00:35:43.060]   more efficient, powerful ride.
[00:35:43.060 --> 00:35:44.060]   But the spin up is slow.
[00:35:44.060 --> 00:35:45.340]   It's a little hesitation.
[00:35:45.340 --> 00:35:46.660]   Not with these new Ford's.
[00:35:46.660 --> 00:35:52.900]   They're using smaller dual turbo chargers, engine, cool turbo chargers to do the same
[00:35:52.900 --> 00:35:53.900]   work.
[00:35:53.900 --> 00:35:54.900]   So they spin up fast.
[00:35:54.900 --> 00:35:56.300]   There's virtually no lag.
[00:35:56.300 --> 00:36:01.540]   And you get incredible low-end torque and in responsiveness.
[00:36:01.540 --> 00:36:06.220]   So I have to say, if you're interested in this stuff and I know many of you are, go
[00:36:06.220 --> 00:36:08.020]   to Ford.com/technology.
[00:36:08.020 --> 00:36:10.820]   Learn about the technologies Ford is putting into its 21st century vehicle.
[00:36:10.820 --> 00:36:16.020]   Including the new EcoBoost engines and the Ford Sync and the My Ford Touch.
[00:36:16.020 --> 00:36:17.020]   It just goes on and on.
[00:36:17.020 --> 00:36:21.340]   And of course, the gas free engines, the electric vehicles.
[00:36:21.340 --> 00:36:23.020]   Ford.com/technology.
[00:36:23.020 --> 00:36:29.460]   And you know, if you went to your Ford dealer, they might let you drive one today, the 2012
[00:36:29.460 --> 00:36:34.420]   Explorer, the 2012 Edge and the 2013 Escape coming in just a few months.
[00:36:34.420 --> 00:36:35.980]   Ford.com/technology.
[00:36:35.980 --> 00:36:40.460]   We thank them for their support of our show.
[00:36:40.460 --> 00:36:43.300]   So I do not know what to say about this thing.
[00:36:43.300 --> 00:36:45.420]   I have been myself.
[00:36:45.420 --> 00:36:49.660]   I've put my head into the turbine.
[00:36:49.660 --> 00:36:51.740]   My hair is a little shorter than it was.
[00:36:51.740 --> 00:36:53.100]   Or maybe joining you there.
[00:36:53.100 --> 00:36:54.100]   Well, good.
[00:36:54.100 --> 00:36:56.820]   I'd like to, you know, and we're see, I think you and I, Jeff, are sometimes seen as Google
[00:36:56.820 --> 00:36:57.980]   apologists.
[00:36:57.980 --> 00:37:01.900]   And I want to emphasize, I don't believe that I'm a Google apologist here, but I want
[00:37:01.900 --> 00:37:03.980]   to raise all the issues.
[00:37:03.980 --> 00:37:08.980]   And I think that the Wall Street Journal was perhaps a slightly slanted.
[00:37:08.980 --> 00:37:14.980]   They published an article based on work from a security researcher about what Google was
[00:37:14.980 --> 00:37:17.740]   doing, which was absolutely sneaky.
[00:37:17.740 --> 00:37:23.780]   I have, you know, they, they, it was, there's no question that it was, you agree, sneaky.
[00:37:23.780 --> 00:37:24.780]   Yes.
[00:37:24.780 --> 00:37:26.300]   We're not denying the sneakiness thereof.
[00:37:26.300 --> 00:37:32.660]   They were invisibly submitting a form and or they were posting a form in an invisible
[00:37:32.660 --> 00:37:36.820]   iframe in the background without telling you when you visited a website.
[00:37:36.820 --> 00:37:37.820]   Let's just.
[00:37:37.820 --> 00:37:38.820]   That's pretty sneaky.
[00:37:38.820 --> 00:37:39.820]   Yeah.
[00:37:39.820 --> 00:37:40.820]   That's pretty sneaky.
[00:37:40.820 --> 00:37:44.900]   Now, the reason they were doing this, and this is, this is where it gets to be a larger,
[00:37:44.900 --> 00:37:48.020]   a larger story, but I'm, I'm not going to deny that it was absolutely sneaky.
[00:37:48.020 --> 00:37:49.020]   It was wrong.
[00:37:49.020 --> 00:37:50.420]   I'm sure Google will not do it anymore.
[00:37:50.420 --> 00:37:52.340]   They said, and this is getting, wearing a little thin.
[00:37:52.340 --> 00:37:54.340]   Oh, we made a mistake.
[00:37:54.340 --> 00:37:58.420]   We just did it so that we could put plus one buttons on the ads.
[00:37:58.420 --> 00:38:04.500]   The reason they did it is because on iOS, Safari, by default, without, and I have to
[00:38:04.500 --> 00:38:09.780]   say without telling the user does something, it disables third party cookies.
[00:38:09.780 --> 00:38:11.420]   And in Safari settings, there are three settings.
[00:38:11.420 --> 00:38:13.060]   This is on the desktop.
[00:38:13.060 --> 00:38:14.060]   Most browsers now do this.
[00:38:14.060 --> 00:38:15.060]   Chrome does do this.
[00:38:15.060 --> 00:38:17.860]   You have three choices except all cookies, except no cookies.
[00:38:17.860 --> 00:38:19.340]   Of course, you accept no cookies.
[00:38:19.340 --> 00:38:22.380]   Cookies are those persistent state information settings.
[00:38:22.380 --> 00:38:24.580]   Basically, they're saved on your hard drive, except no cookies.
[00:38:24.580 --> 00:38:26.380]   A lot of the web doesn't work.
[00:38:26.380 --> 00:38:30.660]   There is a middle ground, however, and this is what a Safari does by default, which is
[00:38:30.660 --> 00:38:35.900]   accept cookies, but only from the site that you were specifically visiting.
[00:38:35.900 --> 00:38:40.820]   So when you go to, let's say, the Wall Street Journal, only the Wall Street Journal can
[00:38:40.820 --> 00:38:46.740]   set cookies, not the 15 different ads services that they have on that site, which are setting,
[00:38:46.740 --> 00:38:50.020]   which are attempting to set cookies from their domain.
[00:38:50.020 --> 00:38:51.020]   Why do they do that?
[00:38:51.020 --> 00:38:55.380]   Well, because they want to track you what you look at and try to serve you ads that
[00:38:55.380 --> 00:38:57.580]   are more targeted.
[00:38:57.580 --> 00:38:59.820]   It's also something Facebook has to do with the like button.
[00:38:59.820 --> 00:39:01.660]   Google has to do with a plus one button.
[00:39:01.660 --> 00:39:06.700]   If you go to a site with a like button, a cookie may be set by that site, but the like
[00:39:06.700 --> 00:39:11.380]   button could also set a cookie as a third party unless it's disabled.
[00:39:11.380 --> 00:39:17.060]   Apple decided only on iOS, not on its desktop, to disable these third party cookies.
[00:39:17.060 --> 00:39:21.420]   Some say John Battelle specifically, and again, John has a dog in this hunt because he's the
[00:39:21.420 --> 00:39:28.500]   head of federated media and ad servicing agency, but he points out that there may be reasons
[00:39:28.500 --> 00:39:33.740]   Apple set this policy on iOS that have nothing to do with privacy, despite the fact that
[00:39:33.740 --> 00:39:35.620]   that's what Apple says it's all about.
[00:39:35.620 --> 00:39:39.940]   But in fact, Apple, and I think this is pretty clear that this is Apple's intent anyway,
[00:39:39.940 --> 00:39:45.380]   across the board is if you're an Apple iOS user, you're an Apple customer and the hell
[00:39:45.380 --> 00:39:48.500]   with anybody else who wants to find out anything about you, Apple keeps that information to
[00:39:48.500 --> 00:39:49.500]   itself.
[00:39:49.500 --> 00:39:53.460]   And we should point out uses it to sell its own ads on I ads.
[00:39:53.460 --> 00:39:59.260]   So John's saying a sad state of an internet affairs, the journal on Google Apple and quote
[00:39:59.260 --> 00:40:03.660]   privacy, there's no question that journal got it wrong with the headline.
[00:40:03.660 --> 00:40:05.580]   There's no iPhone tracking going on here.
[00:40:05.580 --> 00:40:07.700]   That is not what's going on.
[00:40:07.700 --> 00:40:12.140]   Yeah, that is the worst headline in the history of.
[00:40:12.140 --> 00:40:15.740]   Yeah, and probably the case that Julia Angwin did not write the headline.
[00:40:15.740 --> 00:40:16.740]   Usually they have.
[00:40:16.740 --> 00:40:19.580]   But it's still it's still it's still the journal is hysteria.
[00:40:19.580 --> 00:40:23.460]   That's the moral panic and hysteria the journal has brought to this entire topic from the
[00:40:23.460 --> 00:40:24.460]   beginning.
[00:40:24.460 --> 00:40:28.220]   Now I had Cara Swisher on Twitter and I challenge her with that because she writes for all things
[00:40:28.220 --> 00:40:33.540]   D, which is also a news Corp company and she did work for the journal and she said,
[00:40:33.540 --> 00:40:39.580]   "Look, you got to understand that Julia who writes these articles is not driven by,
[00:40:39.580 --> 00:40:43.540]   you know, that she's doing, she's exposing these things."
[00:40:43.540 --> 00:40:48.500]   And, you know, headline aside, I think there was nothing wrong in what she said in her
[00:40:48.500 --> 00:40:49.500]   article, right?
[00:40:49.500 --> 00:40:54.180]   No, I'm going to disagree there because I think that it mischaracterizes what was happening
[00:40:54.180 --> 00:40:57.540]   and I'll point to Lauren Weinstein's very good blog post on this.
[00:40:57.540 --> 00:41:02.660]   Let's get back to some non-hysterical basis of these, right?
[00:41:02.660 --> 00:41:07.540]   Cookies were invented, correct me if I'm wrong Gina, but to set state to understand from
[00:41:07.540 --> 00:41:10.740]   one request to the next what's happening here.
[00:41:10.740 --> 00:41:13.580]   And we can look at this in a lot of ways, right?
[00:41:13.580 --> 00:41:18.900]   One is that Apple, I think, is almost engaging in a form of antitrust by trying to cut off
[00:41:18.900 --> 00:41:25.180]   everybody else's advertising revenue in this because you need this to be able to do it.
[00:41:25.180 --> 00:41:27.380]   I know I'm frozen all...
[00:41:27.380 --> 00:41:28.380]   That's okay.
[00:41:28.380 --> 00:41:29.380]   It's a good look.
[00:41:29.380 --> 00:41:30.380]   Turn off again.
[00:41:30.380 --> 00:41:31.380]   Yeah, I know.
[00:41:31.380 --> 00:41:32.380]   I'll turn off and turn on again.
[00:41:32.380 --> 00:41:34.980]   And it always happens when I start a rant.
[00:41:34.980 --> 00:41:37.740]   It's this guy trying to tell me something.
[00:41:37.740 --> 00:41:41.180]   So a few things here.
[00:41:41.180 --> 00:41:46.260]   One is I want to stay signed into Google and I don't know that Apple is necessarily representing
[00:41:46.260 --> 00:41:47.760]   my desires.
[00:41:47.760 --> 00:41:51.140]   They change, just like Facebook has been accused of doing, they changed their preference in
[00:41:51.140 --> 00:41:56.220]   the background without telling me that has an economic impact on media that I care about.
[00:41:56.220 --> 00:41:57.820]   That's an issue too.
[00:41:57.820 --> 00:42:02.060]   Second, since the beginning of advertising, very, very soon after advertising began on
[00:42:02.060 --> 00:42:06.900]   the web after Wired had the first ad long, long ago, General Motors insisted on serving
[00:42:06.900 --> 00:42:09.060]   its own ads and thus its own cookies.
[00:42:09.060 --> 00:42:15.820]   And that has been the standard of internet advertising since like 1997.
[00:42:15.820 --> 00:42:19.060]   It has been a constant and it's not evil and it's not wrong.
[00:42:19.060 --> 00:42:24.500]   Third, I think we've got to get past this idea of calling anonymous cookies a privacy
[00:42:24.500 --> 00:42:25.500]   violation.
[00:42:25.500 --> 00:42:30.140]   If you don't know who I am, how could it be a privacy violation?
[00:42:30.140 --> 00:42:35.540]   And so what Lauren points out was that Google used a well-known, wasn't even a hack.
[00:42:35.540 --> 00:42:41.780]   It was a door that was put there for a reason and the problem with it was, in engineering
[00:42:41.780 --> 00:42:45.180]   senses according to Lauren, is Apple's.
[00:42:45.180 --> 00:42:48.780]   That the door was meant to judge all Google intended to do was have its cookies stay there
[00:42:48.780 --> 00:42:50.860]   so that it keep you signed in.
[00:42:50.860 --> 00:42:54.220]   What happened with the way it was engineered with Apple was that it allowed other cookies
[00:42:54.220 --> 00:42:58.580]   and other things to be said that one would not have otherwise guessed.
[00:42:58.580 --> 00:43:04.740]   And so I think there was, in this entire story, there's, but it's approaching a moral panic
[00:43:04.740 --> 00:43:06.940]   that I think has become rather ridiculous.
[00:43:06.940 --> 00:43:11.260]   Well, by the way, I just learned something on our security now when we had quite a debate
[00:43:11.260 --> 00:43:15.500]   on this as well because Steve Gibson, our security guy, does believe third-party cookies
[00:43:15.500 --> 00:43:17.660]   are privacy violation.
[00:43:17.660 --> 00:43:20.940]   I think you should have the right to turn them off and I'm glad that Apple has that
[00:43:20.940 --> 00:43:21.940]   setting.
[00:43:21.940 --> 00:43:23.940]   I don't think that that's a, I have no problem with that.
[00:43:23.940 --> 00:43:24.940]   But here's an interesting point.
[00:43:24.940 --> 00:43:27.700]   I don't know if it's a data point, but interesting.
[00:43:27.700 --> 00:43:32.180]   WebKit, which of course both Chrome and Safari are based on.
[00:43:32.180 --> 00:43:34.220]   Gina, you'll vouch for this.
[00:43:34.220 --> 00:43:35.220]   Yeah.
[00:43:35.220 --> 00:43:42.700]   That bug or hole or opening was fixed in WebKit months ago by Google.
[00:43:42.700 --> 00:43:52.660]   Apple did not adopt that patch on Safari yet, but it's been out for, I don't know how long,
[00:43:52.660 --> 00:43:55.180]   but months, my sense has been out for a long time.
[00:43:55.180 --> 00:44:00.460]   So the ability to post to the iframe and the matter, the way that Google is disabled
[00:44:00.460 --> 00:44:02.060]   is webKit and Chrome.
[00:44:02.060 --> 00:44:03.060]   Right.
[00:44:03.060 --> 00:44:08.260]   So the idea is if you have disabled third-party cookies, which mobile Safari does by default,
[00:44:08.260 --> 00:44:09.260]   it blocks that.
[00:44:09.260 --> 00:44:10.500]   No other browser does by default.
[00:44:10.500 --> 00:44:12.540]   So there's a couple of issues here.
[00:44:12.540 --> 00:44:16.860]   So you know, and but tell, John Patel says, everybody in the world, every browser in the
[00:44:16.860 --> 00:44:20.300]   world by default allows third-party cookies.
[00:44:20.300 --> 00:44:25.740]   So the fact that Apple has changed this default setting in order so that they can say that
[00:44:25.740 --> 00:44:28.860]   mobile Safari is more secure, right?
[00:44:28.860 --> 00:44:32.860]   So they can use it kind of as a PR thing was wrong, that they shouldn't have done that.
[00:44:32.860 --> 00:44:34.940]   I have a little bit of a problem with that.
[00:44:34.940 --> 00:44:40.340]   I mean, I just as a developer, it's Apple's decided to change the default setting because
[00:44:40.340 --> 00:44:41.620]   they thought it was a better experience.
[00:44:41.620 --> 00:44:45.100]   I mean, the user, I mean, granted, most users don't change their default setting.
[00:44:45.100 --> 00:44:47.340]   Most users have no clue what third-party cookies are.
[00:44:47.340 --> 00:44:48.340]   No, no, no, no.
[00:44:48.340 --> 00:44:49.340]   Right.
[00:44:49.340 --> 00:44:50.340]   Let's talk about that for a second.
[00:44:50.340 --> 00:44:51.340]   Yeah, let's talk about it.
[00:44:51.340 --> 00:44:52.340]   Because I was thinking about this.
[00:44:52.340 --> 00:44:53.340]   I was thinking about this today.
[00:44:53.340 --> 00:44:56.020]   And Rebecca, don't let these will out mounts keep you out of the conversation.
[00:44:56.020 --> 00:44:57.420]   You have anything to throw in.
[00:44:57.420 --> 00:44:58.420]   I'll throw in when I'm ready.
[00:44:58.420 --> 00:45:02.020]   And by these lab mounts, I mean me.
[00:45:02.020 --> 00:45:07.180]   And I'm going to throw in and give you an advertisement to get you out of it.
[00:45:07.180 --> 00:45:10.820]   If we believe, so let's compare this to piracy.
[00:45:10.820 --> 00:45:15.620]   If we believe that a business, a content business has the right to set its own business terms,
[00:45:15.620 --> 00:45:18.700]   which in the case of piracy is I will charge you for content.
[00:45:18.700 --> 00:45:23.980]   And to violate that is a violation of their rights and is wrong and by God, we should pass
[00:45:23.980 --> 00:45:25.500]   all kinds of laws around that.
[00:45:25.500 --> 00:45:30.460]   Well, the same count, a content business that is advertising supported sets a business term,
[00:45:30.460 --> 00:45:31.940]   which says, I'm going to give you advertising.
[00:45:31.940 --> 00:45:38.140]   And if you turn that off, which even though the entire web enables that capability, if
[00:45:38.140 --> 00:45:43.020]   you turn that off, why aren't you robbing value for me to say way up, damn pirate is.
[00:45:43.020 --> 00:45:44.020]   Well, wait a second.
[00:45:44.020 --> 00:45:48.980]   When you say the entire web enables it, you're saying that most browsers have that
[00:45:48.980 --> 00:45:52.060]   partnership by default, have allowed third party cook.
[00:45:52.060 --> 00:45:56.140]   My question is, why doesn't Google, you know, because it's doing this crazy JavaScript
[00:45:56.140 --> 00:46:00.540]   to submit this form in the background, why does it just pop an alert that say, hey,
[00:46:00.540 --> 00:46:01.900]   that's what they should have done.
[00:46:01.900 --> 00:46:03.940]   So, you know, allow Google cookies?
[00:46:03.940 --> 00:46:04.940]   Okay, cancel.
[00:46:04.940 --> 00:46:05.940]   You know what I mean?
[00:46:05.940 --> 00:46:06.940]   Like, that's what's sneaky about it.
[00:46:06.940 --> 00:46:08.940]   They're doing this thing in the background.
[00:46:08.940 --> 00:46:11.380]   Well, that's the argument against it.
[00:46:11.380 --> 00:46:12.380]   We had last year, right.
[00:46:12.380 --> 00:46:14.540]   I think we have to go back to Apple.
[00:46:14.540 --> 00:46:18.500]   But what I'm saying, even without this specification, I don't know, actually, Gina, there may
[00:46:18.500 --> 00:46:19.500]   be a technical reason.
[00:46:19.500 --> 00:46:21.660]   Remember, Apple tightly controls this platform.
[00:46:21.660 --> 00:46:26.420]   It may not be possible for Google to re-enable that programmatically from the website.
[00:46:26.420 --> 00:46:28.540]   In fact, I guarantee you they can't.
[00:46:28.540 --> 00:46:34.060]   So, what they would have to say is, by the way, you can't use the plus one button on iOS
[00:46:34.060 --> 00:46:35.060]   Safari.
[00:46:35.060 --> 00:46:38.180]   You need to go into settings, go into Safari.
[00:46:38.180 --> 00:46:39.180]   Right.
[00:46:39.180 --> 00:46:41.700]   You know, so that they couldn't do it themselves.
[00:46:41.700 --> 00:46:43.340]   You'd have to go through this thing.
[00:46:43.340 --> 00:46:45.500]   And I don't blame Google for not wanting to do that.
[00:46:45.500 --> 00:46:47.780]   I do blame them for sneaking through.
[00:46:47.780 --> 00:46:50.980]   Maybe they should have just put out a blog post saying, hey, we can't do it.
[00:46:50.980 --> 00:46:52.540]   Let's just show you should know.
[00:46:52.540 --> 00:46:56.260]   But the other argument is it only happens if you're logged in, if you're logged into
[00:46:56.260 --> 00:46:57.540]   a Google account, right?
[00:46:57.540 --> 00:46:59.300]   It doesn't happen if you're not logged into Google.
[00:46:59.300 --> 00:47:00.300]   Right.
[00:47:00.300 --> 00:47:01.300]   Right.
[00:47:01.300 --> 00:47:02.300]   I mean, they just tried it one last time.
[00:47:02.300 --> 00:47:03.300]   I don't want to get Gina to argue with me.
[00:47:03.300 --> 00:47:04.300]   That would be lovely.
[00:47:04.300 --> 00:47:07.060]   Forget the Google case.
[00:47:07.060 --> 00:47:12.380]   Just go with my economic argument here that says that if you violate the business terms
[00:47:12.380 --> 00:47:19.140]   of an advertising supported service, why is that any different in economic terms than
[00:47:19.140 --> 00:47:21.300]   piracy is alleged to be?
[00:47:21.300 --> 00:47:23.980]   And why are we not having fits about that the same way we do?
[00:47:23.980 --> 00:47:26.060]   Now, in fact, in fact, the web is set up.
[00:47:26.060 --> 00:47:28.500]   Most browsers do let you turn off cookies.
[00:47:28.500 --> 00:47:29.700]   You could kill cookies.
[00:47:29.700 --> 00:47:31.300]   You can go to incognito windows.
[00:47:31.300 --> 00:47:32.460]   It does you allow all those things.
[00:47:32.460 --> 00:47:34.500]   And I think it's fine that you have that capability.
[00:47:34.500 --> 00:47:40.260]   But there's no moral outrage about cutting off the economic support for the New York
[00:47:40.260 --> 00:47:41.260]   Times.
[00:47:41.260 --> 00:47:45.140]   There is a moral outrage with better lobbyists for cutting off the economic support to Warner
[00:47:45.140 --> 00:47:46.140]   Brothers.
[00:47:46.140 --> 00:47:47.140]   Wow.
[00:47:47.140 --> 00:47:48.140]   Jeff, I want to thank you for making look more.
[00:47:48.140 --> 00:47:51.700]   I see the lobbyists going to Congress on this one next.
[00:47:51.700 --> 00:47:59.860]   There is a point to be made, which is you are in fact cutting off their ability to monetize.
[00:47:59.860 --> 00:48:02.580]   They can still monetize without these third party cookies.
[00:48:02.580 --> 00:48:03.880]   The ads are still seen.
[00:48:03.880 --> 00:48:08.540]   What it does is it reduces the amount of money they can make because they can't sell targeted
[00:48:08.540 --> 00:48:09.540]   ads.
[00:48:09.540 --> 00:48:11.540]   I fast forward the commercials on my Tivo.
[00:48:11.540 --> 00:48:12.540]   Okay.
[00:48:12.540 --> 00:48:15.940]   I pay for and then I fast forward the commercials.
[00:48:15.940 --> 00:48:17.940]   We should make a law against that.
[00:48:17.940 --> 00:48:20.580]   Well, what we do in the case of piracy, don't they?
[00:48:20.580 --> 00:48:21.580]   It's not a matter for them.
[00:48:21.580 --> 00:48:22.580]   Somebody probably tried.
[00:48:22.580 --> 00:48:23.580]   Yeah.
[00:48:23.580 --> 00:48:25.260]   Actually, they did.
[00:48:25.260 --> 00:48:28.780]   In fact, there was a very good company.
[00:48:28.780 --> 00:48:32.820]   Was it replay TV that got sued out of existence because they had a 30 second ad skip button
[00:48:32.820 --> 00:48:33.820]   on their remote.
[00:48:33.820 --> 00:48:34.820]   Yeah.
[00:48:34.820 --> 00:48:35.820]   I mean, this has come up.
[00:48:35.820 --> 00:48:36.820]   This is the thing.
[00:48:36.820 --> 00:48:37.820]   I just think that that's a closer analogy.
[00:48:37.820 --> 00:48:41.100]   I mean, I think there's a difference between bits or ending an HBO show and skipping through
[00:48:41.100 --> 00:48:44.860]   the commercials, you know, when you're when you're watching TV.
[00:48:44.860 --> 00:48:49.180]   You could argue that it's the same, the same result, but there are different kind of levels
[00:48:49.180 --> 00:48:50.180]   here.
[00:48:50.180 --> 00:48:51.180]   You could argue that it's not a very good thing.
[00:48:51.180 --> 00:48:52.180]   You could argue that it's not a very good thing.
[00:48:52.180 --> 00:48:53.180]   You could argue that it's not a very good thing.
[00:48:53.180 --> 00:48:54.180]   You could argue that it's not a very good thing.
[00:48:54.180 --> 00:48:55.180]   You could argue that it's not a very good thing.
[00:48:55.180 --> 00:48:56.180]   You could argue that it's not a very good thing.
[00:48:56.180 --> 00:48:57.180]   You could argue that it's not a very good thing.
[00:48:57.180 --> 00:48:58.180]   You could argue that it's not a very good thing.
[00:48:58.180 --> 00:48:59.180]   You could argue that it's not a very good thing.
[00:48:59.180 --> 00:49:00.180]   You could argue that it's not a very good thing.
[00:49:00.180 --> 00:49:01.180]   You could argue that it's not a very good thing.
[00:49:01.180 --> 00:49:02.180]   You could argue that it's not a very good thing.
[00:49:02.180 --> 00:49:03.180]   You could argue that it's not a very good thing.
[00:49:03.180 --> 00:49:04.180]   You could argue that it's not a very good thing.
[00:49:04.180 --> 00:49:05.180]   You could argue that it's not a very good thing.
[00:49:05.180 --> 00:49:12.180]   You could argue that it's not a very good thing.
[00:49:12.180 --> 00:49:15.180]   You could argue that it's not a very good thing.
[00:49:15.180 --> 00:49:18.180]   You could argue that it's not a very good thing.
[00:49:18.180 --> 00:49:20.180]   You could argue that it's not a very good thing.
[00:49:20.180 --> 00:49:22.180]   You could argue that it's not a very good thing.
[00:49:22.180 --> 00:49:24.180]   You could argue that it's not a very good thing.
[00:49:24.180 --> 00:49:26.180]   You could argue that it's not a very good thing.
[00:49:26.180 --> 00:49:27.180]   You could argue that it's not a very good thing.
[00:49:27.180 --> 00:49:28.180]   You could argue that it's not a very good thing.
[00:49:28.180 --> 00:49:29.180]   You could argue that it's not a very good thing.
[00:49:29.180 --> 00:49:30.180]   You could argue that it's not a very good thing.
[00:49:30.180 --> 00:49:31.180]   You could argue that it's not a very good thing.
[00:49:31.180 --> 00:49:32.180]   You could argue that it's not a very good thing.
[00:49:32.180 --> 00:49:33.180]   You could argue that it's not a very good thing.
[00:49:33.180 --> 00:49:35.180]   You could argue that it's not a very good thing.
[00:49:35.180 --> 00:49:36.180]   You could argue that it's not a very good thing.
[00:49:36.180 --> 00:49:37.180]   You could argue that it's not a very good thing.
[00:49:37.180 --> 00:49:38.180]   You could argue that it's not a very good thing.
[00:49:38.180 --> 00:49:39.180]   You could argue that it's not a very good thing.
[00:49:39.180 --> 00:49:40.180]   You could argue that it's not a very good thing.
[00:49:40.180 --> 00:49:41.180]   You could argue that it's not a very good thing.
[00:49:41.180 --> 00:49:42.180]   You could argue that it's not a very good thing.
[00:49:42.180 --> 00:49:43.180]   You could argue that it's not a very good thing.
[00:49:43.180 --> 00:49:44.180]   You could argue that it's not a very good thing.
[00:49:44.180 --> 00:49:45.180]   You could argue that it's not a very good thing.
[00:49:45.180 --> 00:49:46.180]   You could argue that it's not a very good thing.
[00:49:46.180 --> 00:49:47.180]   You could argue that it's not a very good thing.
[00:49:47.180 --> 00:49:48.180]   You could argue that it's not a very good thing.
[00:49:48.180 --> 00:49:49.180]   You could argue that it's not a very good thing.
[00:49:49.180 --> 00:50:08.180]   You could argue that it's not a very good thing.
[00:50:08.180 --> 00:50:15.180]   You could argue that it's not a very good thing.
[00:50:15.180 --> 00:50:18.180]   You could argue that it's not a very good thing.
[00:50:18.180 --> 00:50:19.180]   You could argue that it's not a very good thing.
[00:50:19.180 --> 00:50:20.180]   You could argue that it's not a very good thing.
[00:50:20.180 --> 00:50:21.180]   You could argue that it's not a very good thing.
[00:50:21.180 --> 00:50:22.180]   You could argue that it's not a very good thing.
[00:50:22.180 --> 00:50:23.180]   You could argue that it's not a very good thing.
[00:50:23.180 --> 00:50:24.180]   You could argue that it's not a very good thing.
[00:50:24.180 --> 00:50:25.180]   You could argue that it's not a very good thing.
[00:50:25.180 --> 00:50:26.180]   You could argue that it's not a very good thing.
[00:50:26.180 --> 00:50:27.180]   You could argue that it's not a very good thing.
[00:50:27.180 --> 00:50:28.180]   You could argue that it's not a very good thing.
[00:50:28.180 --> 00:50:29.180]   You could argue that it's not a very good thing.
[00:50:29.180 --> 00:50:30.180]   You could argue that it's not a very good thing.
[00:50:30.180 --> 00:50:31.180]   You could argue that it's not a very good thing.
[00:50:31.180 --> 00:50:38.180]   You could argue that it's not a very good thing.
[00:50:38.180 --> 00:50:39.180]   You could argue that it's not a very good thing.
[00:50:39.180 --> 00:50:40.180]   You could argue that it's not a very good thing.
[00:50:40.180 --> 00:50:41.180]   You could argue that it's not a very good thing.
[00:50:41.180 --> 00:50:42.180]   You could argue that it's not a very good thing.
[00:50:42.180 --> 00:50:43.180]   You could argue that it's not a very good thing.
[00:50:43.180 --> 00:50:44.180]   You could argue that it's not a very good thing.
[00:50:44.180 --> 00:50:45.180]   You could argue that it's not a very good thing.
[00:50:45.180 --> 00:50:46.180]   You could argue that it's not a very good thing.
[00:50:46.180 --> 00:50:47.180]   You could argue that it's not a very good thing.
[00:50:47.180 --> 00:50:48.180]   You could argue that it's not a very good thing.
[00:50:48.180 --> 00:50:49.180]   You could argue that it's not a very good thing.
[00:50:49.180 --> 00:50:50.180]   You could argue that it's not a very good thing.
[00:50:50.180 --> 00:50:51.180]   You could argue that it's not a very good thing.
[00:50:51.180 --> 00:50:52.180]   You could argue that it's not a very good thing.
[00:50:52.180 --> 00:50:59.180]   You could argue that it's not a very good thing.
[00:50:59.180 --> 00:51:06.180]   You could argue that it's not a very good thing.
[00:51:06.180 --> 00:51:13.180]   You could argue that it's not a very good thing.
[00:51:13.180 --> 00:51:18.180]   You could argue that it's not a very good thing.
[00:51:18.180 --> 00:51:23.180]   I don't think you, for instance, I ads, which Apple is an Apple company
[00:51:23.180 --> 00:51:28.180]   and Apple advertising on third party apps that Apple sells.
[00:51:28.180 --> 00:51:31.180]   I'm sure they track you cross app.
[00:51:31.180 --> 00:51:33.180]   We don't know, but I'm sure they track you cross app.
[00:51:33.180 --> 00:51:36.180]   Apple does plenty of things that are troubling.
[00:51:36.180 --> 00:51:40.180]   The way in which they choose to censor apps, there have been stories,
[00:51:40.180 --> 00:51:46.180]   I think, including in the Wall Street Journal about the collection of people's movements
[00:51:46.180 --> 00:51:51.180]   that were a bug on the phone, they've screwed up themselves on privacy
[00:51:51.180 --> 00:51:54.180]   and tracking on their devices.
[00:51:54.180 --> 00:51:57.180]   There's been all kinds of, none of these companies have pure motives.
[00:51:57.180 --> 00:52:03.180]   Some of them are disingenuously, you've also got Microsoft trying to use
[00:52:03.180 --> 00:52:08.180]   Google's grew up to their advantage competitively and so on.
[00:52:08.180 --> 00:52:14.180]   But I think this points to a bigger problem, which is that
[00:52:14.180 --> 00:52:20.180]   I think some of the news stories may, as Jeff says, kind of over hype,
[00:52:20.180 --> 00:52:24.180]   but that people don't feel like they can trust what's going on.
[00:52:24.180 --> 00:52:28.180]   People are feeling increasingly distrustful of these services
[00:52:28.180 --> 00:52:32.180]   that we're now all kind of dependent on and that we want to use
[00:52:32.180 --> 00:52:34.180]   for all kinds of reasons.
[00:52:34.180 --> 00:52:36.180]   We don't want to disconnect from them.
[00:52:36.180 --> 00:52:40.180]   But at the same time, we feel creeped out half the time.
[00:52:40.180 --> 00:52:45.180]   In some cases, there are people who have real concerns
[00:52:45.180 --> 00:52:50.180]   about how their information might get exposed or used by various authorities
[00:52:50.180 --> 00:52:53.180]   or various people seeking to do them harm.
[00:52:53.180 --> 00:52:59.180]   And so we need to, how can we ensure that industry will do a better job
[00:52:59.180 --> 00:53:05.180]   of kind of making people feel reassured that their best interest
[00:53:05.180 --> 00:53:08.180]   is actually being thought about?
[00:53:08.180 --> 00:53:12.180]   And yes, we are making trade-offs using these services.
[00:53:12.180 --> 00:53:15.180]   But at the same time, trust is eroding.
[00:53:15.180 --> 00:53:20.180]   And if companies can't figure out a way to rebuild that trust,
[00:53:20.180 --> 00:53:22.180]   and I think that the owners-
[00:53:22.180 --> 00:53:23.180]   That's absolutely true.
[00:53:23.180 --> 00:53:25.180]   -it's partially on them on the companies.
[00:53:25.180 --> 00:53:27.180]   There's a lack of trust.
[00:53:27.180 --> 00:53:30.180]   And if they don't work harder to build that trust, they're going to get
[00:53:30.180 --> 00:53:34.180]   regulated in all kinds of crazy ways that aren't going to be good.
[00:53:34.180 --> 00:53:35.180]   And they need to go out-
[00:53:35.180 --> 00:53:38.180]   -they allowed- -to be allowed-
[00:53:38.180 --> 00:53:39.180]   -by being mysterious.
[00:53:39.180 --> 00:53:41.180]   They allowed cookies to be demonized, mystified and demonized,
[00:53:41.180 --> 00:53:43.180]   because they weren't transparent about, they weren't open about what they were doing
[00:53:43.180 --> 00:53:44.180]   and why they were doing it.
[00:53:44.180 --> 00:53:46.180]   And they asked for a lot of this trouble.
[00:53:46.180 --> 00:53:49.180]   The result, though, could hurt us all and hurt the internet
[00:53:49.180 --> 00:53:52.180]   and regulate the internet in ways that frighten me.
[00:53:52.180 --> 00:53:54.180]   -So the question is, where do we go from-
[00:53:54.180 --> 00:53:57.180]   How do we move forward towards a good result?
[00:53:57.180 --> 00:53:59.180]   -Good. That's good.
[00:53:59.180 --> 00:54:04.180]   So, by the way, it was March 2010 that this bug was fixed everywhere but iOS.
[00:54:04.180 --> 00:54:10.180]   March 2010 is more than almost two years ago that this bug-
[00:54:10.180 --> 00:54:11.180]   -Oh, wow.
[00:54:11.180 --> 00:54:14.180]   -It's a webkit bug I just posted in the chat, the link-
[00:54:14.180 --> 00:54:17.180]   Laura Weinstein, Laura Weinstein actually put this in here post.
[00:54:17.180 --> 00:54:22.180]   This bug was fixed by everybody but Apple two years ago.
[00:54:22.180 --> 00:54:26.180]   So the question is why was this not patched?
[00:54:26.180 --> 00:54:28.180]   -Well, that's not a bad tool. -Two years ago.
[00:54:28.180 --> 00:54:29.180]   -Yeah, that's bad.
[00:54:29.180 --> 00:54:31.180]   -So I think that that is a little bit of a smoking gun.
[00:54:31.180 --> 00:54:34.180]   So look, at the only point that we have here,
[00:54:34.180 --> 00:54:37.180]   I know people are going to assume Jeff and I are Google apologists.
[00:54:37.180 --> 00:54:38.180]   That's not the point.
[00:54:38.180 --> 00:54:43.180]   The point is, there is a more complicated, more nuanced discussion
[00:54:43.180 --> 00:54:47.180]   than the journal would have you think or any of the other tech blogs would have you think,
[00:54:47.180 --> 00:54:50.180]   because it really does make for great copy to say,
[00:54:50.180 --> 00:54:52.180]   Google tracked iPhones.
[00:54:52.180 --> 00:54:54.180]   That is link-based.
[00:54:54.180 --> 00:55:00.180]   But I think Rebecca, you're absolutely right and you're the cooler head prevailing here.
[00:55:00.180 --> 00:55:03.180]   Whatever side you come down on, the question is,
[00:55:03.180 --> 00:55:07.180]   what should policy be going forward?
[00:55:07.180 --> 00:55:10.180]   How do we avoid this happening again?
[00:55:10.180 --> 00:55:14.180]   Not at fault, who's sneaky, who's nice.
[00:55:14.180 --> 00:55:16.180]   But how do we handle this?
[00:55:16.180 --> 00:55:20.180]   -How do we handle this? -How do we handle this and is this for government to sort out?
[00:55:20.180 --> 00:55:23.180]   Or do companies need to be more proactive?
[00:55:23.180 --> 00:55:26.180]   -I think we all hope government doesn't get involved in this.
[00:55:26.180 --> 00:55:30.180]   -I'm here in Washington and I can tell you there's people down the street on Capitol Hill
[00:55:30.180 --> 00:55:32.180]   just really wanting to get involved with this.
[00:55:32.180 --> 00:55:35.180]   -The FTC has already asked Google, what the hell are you doing?
[00:55:35.180 --> 00:55:39.180]   -There's all kinds of bills being proposed and so on.
[00:55:39.180 --> 00:55:43.180]   But if industry doesn't want to get regulated, they've got to step up.
[00:55:43.180 --> 00:55:48.180]   They've got to prove that they can be trusted and they are not providing
[00:55:48.180 --> 00:55:51.180]   particularly solid evidence in that regard.
[00:55:51.180 --> 00:55:56.180]   And so they've got to figure out how to interact with their users or constituents
[00:55:56.180 --> 00:56:00.180]   or whatever you want to call them to try and build trust.
[00:56:00.180 --> 00:56:02.180]   -I think Rebecca, I think you're absolutely right.
[00:56:02.180 --> 00:56:04.180]   And let me do an 80/20 rule on this.
[00:56:04.180 --> 00:56:08.180]   I think 80% of it is that they have to act far better, be more transparent,
[00:56:08.180 --> 00:56:11.180]   be more open, just think this way in general.
[00:56:11.180 --> 00:56:15.180]   I've used the example before that people don't complain about Gmail priority inbox
[00:56:15.180 --> 00:56:17.180]   because they get value out of it.
[00:56:17.180 --> 00:56:18.180]   They've got to think about value.
[00:56:18.180 --> 00:56:22.180]   But the other 20% I think is they do have to start fighting back
[00:56:22.180 --> 00:56:27.180]   when they see things like these headlines and say, "Whoa, that's not true.
[00:56:27.180 --> 00:56:29.180]   Here's what's really going on."
[00:56:29.180 --> 00:56:35.180]   And you said before, don't mock the -- I would agree about not mocking
[00:56:35.180 --> 00:56:40.180]   the consumer's concerns, but one should mock bad headlines.
[00:56:40.180 --> 00:56:43.180]   -Mm-hmm, yeah.
[00:56:43.180 --> 00:56:45.180]   -It is such an interesting story.
[00:56:45.180 --> 00:56:49.180]   And it's going to resonate.
[00:56:49.180 --> 00:56:54.180]   And it's not -- unfortunately for Google, if you're Google,
[00:56:54.180 --> 00:56:57.180]   not unfortunately for anybody else, but unfortunately for Google, if you're Google,
[00:56:57.180 --> 00:57:01.180]   this is just one more piece of mud on the -- I mean, they are --
[00:57:01.180 --> 00:57:03.180]   they're buried by this stuff.
[00:57:03.180 --> 00:57:04.180]   They don't respond well.
[00:57:04.180 --> 00:57:06.180]   There was apparently debate internally about,
[00:57:06.180 --> 00:57:08.180]   "Should we make a video explaining this?"
[00:57:08.180 --> 00:57:11.180]   And they said, "No, nobody will understand it anyway."
[00:57:11.180 --> 00:57:14.180]   -It's the area that is the dog with your belly up.
[00:57:14.180 --> 00:57:15.180]   -Yeah.
[00:57:15.180 --> 00:57:17.180]   -Okay, okay, okay.
[00:57:17.180 --> 00:57:20.180]   You know, forgive me, even if what you did wasn't wrong.
[00:57:20.180 --> 00:57:22.180]   But again, Rebecca's still absolutely right.
[00:57:22.180 --> 00:57:25.180]   What they've been wrong about -- and it's not just technology companies.
[00:57:25.180 --> 00:57:27.180]   -They've got to take the mold by the horse.
[00:57:27.180 --> 00:57:29.180]   -It's also very much media companies and advertising companies
[00:57:29.180 --> 00:57:33.180]   have been -- have just been terribly mysterious about this.
[00:57:33.180 --> 00:57:37.180]   And that's what does -- I'm going to use the word "creep people out"
[00:57:37.180 --> 00:57:40.180]   because they don't know what's going on.
[00:57:40.180 --> 00:57:42.180]   So, you know, radical transparency,
[00:57:42.180 --> 00:57:45.180]   radically thinking truly about the consumers, the customers,
[00:57:45.180 --> 00:57:50.180]   desires, and serving them is the only way to start digging yourself out for real.
[00:57:50.180 --> 00:57:51.180]   It's not messaging.
[00:57:51.180 --> 00:57:55.180]   -And if it's not enough just to say, "Well, we owe this to our customers,"
[00:57:55.180 --> 00:57:59.180]   there should be motivation in the idea that if you don't,
[00:57:59.180 --> 00:58:02.180]   government will and they all screw it up mercilessly.
[00:58:02.180 --> 00:58:04.180]   -Boy, will they ever --
[00:58:04.180 --> 00:58:08.180]   -And it's also -- you can almost make a kind of a sustainability argument.
[00:58:08.180 --> 00:58:14.180]   If you want the web to be as valuable as possible for everybody who's using it
[00:58:14.180 --> 00:58:16.180]   and doing business on it -- -Yeah.
[00:58:16.180 --> 00:58:21.180]   -And if you want it to be a place that people want to be,
[00:58:21.180 --> 00:58:27.180]   you know, you have to kind of take responsibility for it developing in a manner
[00:58:27.180 --> 00:58:32.180]   that, you know, to continue the creepy meme, you know, doesn't creep people out.
[00:58:32.180 --> 00:58:33.180]   -Right.
[00:58:33.180 --> 00:58:34.180]   -Right.
[00:58:34.180 --> 00:58:37.180]   -Google has to be in that as in just as much as we have to be in that as in.
[00:58:37.180 --> 00:58:39.180]   So we all need to be in that as in.
[00:58:39.180 --> 00:58:45.180]   And at some point, the excuse that, well, this is new territory
[00:58:45.180 --> 00:58:49.180]   where you're all learning doesn't hold water anymore.
[00:58:49.180 --> 00:58:52.180]   And I think now is the time.
[00:58:52.180 --> 00:58:55.180]   -Okay, that's it. No more free passes.
[00:58:55.180 --> 00:59:01.180]   If an app developer, if Facebook, Microsoft, Google, or Apple, or Amazon
[00:59:01.180 --> 00:59:06.180]   was unclear up to this point that people really care about this stuff,
[00:59:06.180 --> 00:59:09.180]   guess what they do?
[00:59:09.180 --> 00:59:10.180]   No more excuses.
[00:59:10.180 --> 00:59:12.180]   Oh, gosh, we never thought about that.
[00:59:12.180 --> 00:59:15.180]   That doesn't -- it's not going to hold water in the future.
[00:59:15.180 --> 00:59:18.180]   All right, moving on. Google to sell.
[00:59:18.180 --> 00:59:20.180]   -Google to sell. -That's it.
[00:59:20.180 --> 00:59:22.180]   -That's it. I'm done. -We use -- that's it.
[00:59:22.180 --> 00:59:24.180]   -That's it. Where's my gab? -We're done.
[00:59:24.180 --> 00:59:25.180]   -We're done. We're done.
[00:59:25.180 --> 00:59:27.180]   -You know what? You could go on for hours.
[00:59:27.180 --> 00:59:30.180]   And we have -- I mean, this discussion, if you want more, folks -- and I know you don't --
[00:59:30.180 --> 00:59:34.180]   but if you did, security now, earlier today, this week in tech,
[00:59:34.180 --> 00:59:37.180]   on Sunday, this was an absolute topic of discussion.
[00:59:37.180 --> 00:59:40.180]   And thank God Jeff Jarvis has made me look like a moderate
[00:59:40.180 --> 00:59:44.180]   by saying it's akin to piracy, what Apple's doing.
[00:59:44.180 --> 00:59:45.180]   Thank you, Jeff.
[00:59:45.180 --> 00:59:49.180]   -What a hack. -Because up to now, I was all alone on this hill.
[00:59:49.180 --> 00:59:52.180]   Jeff's the one in the red shirt, kids.
[00:59:52.180 --> 00:59:55.180]   -I'll take four for you, boss.
[00:59:55.180 --> 00:59:59.180]   -Google's going to sell heads-up display glasses by years in.
[00:59:59.180 --> 01:00:03.180]   I do not know why Nick Bilton wrote this article, but he did.
[01:00:03.180 --> 01:00:07.180]   -I don't know who will buy this, but I guess they will.
[01:00:07.180 --> 01:00:11.180]   According to sources at Google familiar with the project,
[01:00:11.180 --> 01:00:15.180]   $250 to $600 for glasses that looked like those oakly thumps.
[01:00:15.180 --> 01:00:17.180]   They were sunglasses that had earbuds.
[01:00:17.180 --> 01:00:21.180]   Apparently, if you have an Android phone, you'll have a heads-up display
[01:00:21.180 --> 01:00:24.180]   on your glasses that will show you crap.
[01:00:24.180 --> 01:00:27.180]   -Do you not want this? Am I the only one here that wants this?
[01:00:27.180 --> 01:00:28.180]   Come on. -Oh, I want this.
[01:00:28.180 --> 01:00:31.180]   -Oh, absolutely. -Google I/O giveaway, baby.
[01:00:31.180 --> 01:00:36.180]   The glasses will have a low -- we're all going to Google I/O.
[01:00:36.180 --> 01:00:37.180]   That's all right.
[01:00:37.180 --> 01:00:40.180]   The glasses will have a low resolution built-in camera
[01:00:40.180 --> 01:00:42.180]   that will monitor the world in real time.
[01:00:42.180 --> 01:00:46.180]   It's augmented reality and overlay information about locations surrounding buildings
[01:00:46.180 --> 01:00:48.180]   and friends who might be nearby.
[01:00:48.180 --> 01:00:49.180]   You know what this reminds me of?
[01:00:49.180 --> 01:00:52.180]   I don't know if you've read Daniel Suarez' book, "Demon and --"
[01:00:52.180 --> 01:00:55.180]   -That's exactly what I was going to say.
[01:00:55.180 --> 01:00:59.180]   -In that world, you could see a metaverse that was superimposed
[01:00:59.180 --> 01:01:01.180]   on the real universe, and it was like games.
[01:01:01.180 --> 01:01:05.180]   People would have their reputation score floating in bubbles over their head.
[01:01:05.180 --> 01:01:08.180]   -And they had self-driving cars.
[01:01:08.180 --> 01:01:09.180]   -Yeah.
[01:01:09.180 --> 01:01:13.180]   So I think obviously Larry Page is a big Daniel Suarez fan.
[01:01:13.180 --> 01:01:17.180]   -So with these glasses on, if I look at you, Leo, am I going to see the number of circles
[01:01:17.180 --> 01:01:19.180]   you're in kind of above your head?
[01:01:19.180 --> 01:01:20.180]   -Damn straight.
[01:01:20.180 --> 01:01:26.180]   -Yeah, and as Suarez is a novel, too, it was like you could see people's approval ratings,
[01:01:26.180 --> 01:01:29.180]   their popularity ratings, you know, all kinds of stuff.
[01:01:29.180 --> 01:01:31.180]   -Yeah, it's about reputation.
[01:01:31.180 --> 01:01:34.180]   It's like gaming where you have hit points, experience points, all of that stuff.
[01:01:34.180 --> 01:01:36.180]   You actually have levels.
[01:01:36.180 --> 01:01:38.180]   I think you could level up as I remember.
[01:01:38.180 --> 01:01:39.180]   -Yeah.
[01:01:39.180 --> 01:01:42.180]   -The project is currently being built according to Nick.
[01:01:42.180 --> 01:01:45.180]   Nick, I just -- Nick has really been on it lately.
[01:01:45.180 --> 01:01:51.180]   In the Google X offices, a secret lab near Google's main campus
[01:01:51.180 --> 01:01:55.180]   that is charged with working on robots, space elevators, and dozens of other
[01:01:55.180 --> 01:01:57.180]   futuristic projects.
[01:01:57.180 --> 01:02:01.180]   Steve Lee, who created Latitude, is involved.
[01:02:01.180 --> 01:02:06.180]   Sergey Brin is spending most of his time in the X Labs.
[01:02:06.180 --> 01:02:09.180]   -I love the idea that there are multiple space elevators being worked on.
[01:02:09.180 --> 01:02:10.180]   -Oh, that's right.
[01:02:10.180 --> 01:02:11.180]   It's plural.
[01:02:11.180 --> 01:02:12.180]   -Yes, plural.
[01:02:12.180 --> 01:02:13.180]   You know, all those space elevators.
[01:02:13.180 --> 01:02:15.180]   -There's different ways to do it.
[01:02:15.180 --> 01:02:18.180]   The glasses will send data to the cloud and use things like --
[01:02:18.180 --> 01:02:19.180]   -I'm all like -- you got --
[01:02:19.180 --> 01:02:23.180]   -Well, if you think about it, this is what Eric Schmidt was talking about when you --
[01:02:23.180 --> 01:02:25.180]   with your pants -- the pants analogy, right?
[01:02:25.180 --> 01:02:27.180]   This is more of that.
[01:02:27.180 --> 01:02:33.180]   They just want you to -- they want to -- they -- but think of the privacy implications
[01:02:33.180 --> 01:02:38.180]   with the phrase, "The glasses will send data to the cloud."
[01:02:38.180 --> 01:02:44.180]   Now Google will know not only where you are, but what you're looking at --
[01:02:44.180 --> 01:02:47.180]   -What you're looking at -- Google goggles.
[01:02:47.180 --> 01:02:50.180]   It'll keep -- it'll keep twigging business for a long time.
[01:02:50.180 --> 01:02:56.180]   -And Apple -- Apple will probably say somehow have a way to avoid you using this somehow.
[01:02:56.180 --> 01:02:57.180]   Block it.
[01:02:57.180 --> 01:02:58.180]   Block it.
[01:02:58.180 --> 01:03:01.180]   We have the Apple glasses that you put over the Google glasses.
[01:03:01.180 --> 01:03:02.180]   -Well, okay.
[01:03:02.180 --> 01:03:03.180]   Now, wait, wait, wait.
[01:03:03.180 --> 01:03:05.180]   What would Apple glasses do?
[01:03:05.180 --> 01:03:09.180]   Apple glasses would just show you the things that Apple will sell to you.
[01:03:09.180 --> 01:03:10.180]   -No, no.
[01:03:10.180 --> 01:03:13.180]   They would only -- no, Jeff, they would only show you things that are beautiful.
[01:03:13.180 --> 01:03:14.180]   -That's right.
[01:03:14.180 --> 01:03:15.180]   -That's right.
[01:03:15.180 --> 01:03:16.180]   -That's right.
[01:03:16.180 --> 01:03:17.180]   -Yes.
[01:03:17.180 --> 01:03:18.180]   -Anything with hard corners?
[01:03:18.180 --> 01:03:19.180]   -Yes.
[01:03:19.180 --> 01:03:22.180]   -You're going to have to go to the square corners and round them.
[01:03:22.180 --> 01:03:23.180]   -Right.
[01:03:23.180 --> 01:03:24.180]   It automatically rounds corners.
[01:03:24.180 --> 01:03:26.180]   -You will live in a world of beautiful design.
[01:03:26.180 --> 01:03:28.180]   -I love everything like the highlight effect.
[01:03:28.180 --> 01:03:29.180]   -And everything like the highlight effect.
[01:03:29.180 --> 01:03:31.180]   -Well, you know, I'd buy those.
[01:03:31.180 --> 01:03:33.180]   It's such a goofy story.
[01:03:33.180 --> 01:03:36.180]   I don't -- but it's Nick Bilton in the New York Times.
[01:03:36.180 --> 01:03:37.180]   -I mean, it's Nick Bilton's.
[01:03:37.180 --> 01:03:38.180]   -It's Nick Bilton.
[01:03:38.180 --> 01:03:39.180]   How could it be false?
[01:03:39.180 --> 01:03:40.180]   -I say June Google I/O.
[01:03:40.180 --> 01:03:44.180]   We're all going to be walking around that place where everyone will be walking around
[01:03:44.180 --> 01:03:45.180]   -That would be a sight.
[01:03:45.180 --> 01:03:47.180]   -Bumping into each other because you won't be able to see anything.
[01:03:47.180 --> 01:03:49.180]   -Does that be a sight?
[01:03:49.180 --> 01:03:52.180]   -This is my hoping --
[01:03:52.180 --> 01:03:55.180]   -Like bumper cars wearing these things.
[01:03:55.180 --> 01:04:00.180]   -It's definitely a way to get press.
[01:04:00.180 --> 01:04:03.180]   [laughter]
[01:04:03.180 --> 01:04:07.180]   I can just see the front page in the New York Times with all us geeks walking around.
[01:04:07.180 --> 01:04:12.180]   So have you played with the new latitude -- Google's going after four-square, I guess.
[01:04:12.180 --> 01:04:13.180]   -Oh.
[01:04:13.180 --> 01:04:14.180]   -Yeah.
[01:04:14.180 --> 01:04:17.180]   So latitude has always been the automatic check-in.
[01:04:17.180 --> 01:04:20.180]   It knows exactly where I go, right?
[01:04:20.180 --> 01:04:21.180]   And you can't see this.
[01:04:21.180 --> 01:04:26.180]   Only I can see this, but I'm now showing the world that I have traveled 74,577 miles,
[01:04:26.180 --> 01:04:30.180]   which is apparently a third of the weight of the moon.
[01:04:30.180 --> 01:04:33.180]   I spent -- now, I just want everybody to note this.
[01:04:33.180 --> 01:04:35.180]   74 hours at work last week.
[01:04:35.180 --> 01:04:36.180]   -Oh.
[01:04:36.180 --> 01:04:38.180]   -Despite the fact that my average is 29.
[01:04:38.180 --> 01:04:40.180]   -I buy stock in that company.
[01:04:40.180 --> 01:04:41.180]   [laughter]
[01:04:41.180 --> 01:04:43.180]   -12 hours out.
[01:04:43.180 --> 01:04:46.180]   Countries visited airplane trips, my trips.
[01:04:46.180 --> 01:04:48.180]   But that's not the interesting thing.
[01:04:48.180 --> 01:04:50.180]   There's also a leaderboard now.
[01:04:50.180 --> 01:04:51.180]   These are all my check-ins.
[01:04:51.180 --> 01:04:55.180]   I guess the leaderboard is only on the phone.
[01:04:55.180 --> 01:04:57.180]   Is it only on the phone?
[01:04:57.180 --> 01:04:58.180]   Maybe it is.
[01:04:58.180 --> 01:05:00.180]   -Well, apparently you're in the lead.
[01:05:00.180 --> 01:05:01.180]   -I am in the lead.
[01:05:01.180 --> 01:05:02.180]   -There it is.
[01:05:02.180 --> 01:05:06.180]   That's the engadget of screenshot that puts me number one.
[01:05:06.180 --> 01:05:08.180]   But I think I have five points.
[01:05:08.180 --> 01:05:09.180]   I don't think that counts.
[01:05:09.180 --> 01:05:11.180]   -I think it's just because it's new.
[01:05:11.180 --> 01:05:12.180]   -I guess.
[01:05:12.180 --> 01:05:14.180]   -I don't think that's like a global leader.
[01:05:14.180 --> 01:05:15.180]   I don't know.
[01:05:15.180 --> 01:05:16.180]   I don't even know that guy, Taryn Sobrien.
[01:05:16.180 --> 01:05:18.180]   I don't know why I'm on his list.
[01:05:18.180 --> 01:05:20.180]   But anyway, there you go.
[01:05:20.180 --> 01:05:23.180]   And this is a little creepy.
[01:05:23.180 --> 01:05:25.180]   I guess, again, only I can see this.
[01:05:25.180 --> 01:05:26.180]   This is today.
[01:05:26.180 --> 01:05:28.180]   Let's go to yesterday.
[01:05:28.180 --> 01:05:30.180]   These are all the places I visited.
[01:05:30.180 --> 01:05:36.180]   And you can go hour by hour through your travels.
[01:05:36.180 --> 01:05:37.180]   -Can you blacklist some places?
[01:05:37.180 --> 01:05:39.180]   Can you be like when I go to the --
[01:05:39.180 --> 01:05:41.180]   -No, you don't think you can blacklist.
[01:05:41.180 --> 01:05:42.180]   -Don't you can delete.
[01:05:42.180 --> 01:05:43.180]   -Don't check in.
[01:05:43.180 --> 01:05:44.180]   -You can --
[01:05:44.180 --> 01:05:46.180]   -My visits to the porn shop, you can delete.
[01:05:46.180 --> 01:05:48.180]   But I don't know if you can block.
[01:05:48.180 --> 01:05:55.180]   -You know, I got to say, you know, after spending nine years in China living under pretty heavy surveillance,
[01:05:55.180 --> 01:05:59.180]   I just cannot bring myself to use four square or Google latitude.
[01:05:59.180 --> 01:06:00.180]   -Okay.
[01:06:00.180 --> 01:06:04.180]   -I just have this thing about not wanting people to know where I am.
[01:06:04.180 --> 01:06:05.180]   -It's okay.
[01:06:05.180 --> 01:06:06.180]   It's just Google.
[01:06:06.180 --> 01:06:07.180]   It's not the government.
[01:06:07.180 --> 01:06:08.180]   -So it didn't make you more comfortable.
[01:06:08.180 --> 01:06:10.180]   It made you less comfortable.
[01:06:10.180 --> 01:06:12.180]   -Being under surveillance all the time.
[01:06:12.180 --> 01:06:13.180]   -Well, yeah.
[01:06:13.180 --> 01:06:14.180]   I mean, yes.
[01:06:14.180 --> 01:06:16.180]   -You know, it feels safer.
[01:06:16.180 --> 01:06:19.180]   -It's definitely an uncomfortable experience.
[01:06:19.180 --> 01:06:23.180]   -Can you -- we got guys in cars and stuff like you would look at their own.
[01:06:23.180 --> 01:06:25.180]   -Oh, there was a bit of that.
[01:06:25.180 --> 01:06:28.180]   You know, but my phone conversations were bugged.
[01:06:28.180 --> 01:06:29.180]   -Wow.
[01:06:29.180 --> 01:06:30.180]   -You know, my apartment was bugged.
[01:06:30.180 --> 01:06:31.180]   My office was bugged.
[01:06:31.180 --> 01:06:33.180]   If you wanted to have a really private conversation,
[01:06:33.180 --> 01:06:38.180]   you leave your phone behind and, you know, go for a walk in the woods or something.
[01:06:38.180 --> 01:06:39.180]   -And you did that for a decade.
[01:06:39.180 --> 01:06:40.180]   -Yeah.
[01:06:40.180 --> 01:06:43.180]   And now, soon we're going to have to leave our glasses behind, too.
[01:06:43.180 --> 01:06:44.180]   [ Laughter ]
[01:06:44.180 --> 01:06:48.180]   -When I visited China for the first time, like a few months back,
[01:06:48.180 --> 01:06:52.180]   and I was just the cameras and having to hand over your ID in order to do anything,
[01:06:52.180 --> 01:06:56.180]   to check in the hotel -- well, check in the hotel, but like ride the subway or go anywhere.
[01:06:56.180 --> 01:06:59.180]   That was an interesting -- it was an interesting experience for me as my first time.
[01:06:59.180 --> 01:07:04.180]   And I understood that culturally handing over your ID is just a part of what you do,
[01:07:04.180 --> 01:07:06.180]   and it's not that big a deal.
[01:07:06.180 --> 01:07:10.180]   So I wondered, you know, if it became sort of less of an issue,
[01:07:10.180 --> 01:07:13.180]   but certainly where I traveled, to my knowledge,
[01:07:13.180 --> 01:07:15.180]   the rooms were unbugged and my phone wasn't bugged.
[01:07:15.180 --> 01:07:16.180]   So that's just why I wanted to --
[01:07:16.180 --> 01:07:17.180]   -Oh, that's to your knowledge.
[01:07:17.180 --> 01:07:18.180]   -Yeah, well, it's my knowledge.
[01:07:18.180 --> 01:07:20.180]   -You were probably staying in nice tourist hotels, though.
[01:07:20.180 --> 01:07:22.180]   -Yes, yes.
[01:07:22.180 --> 01:07:23.180]   So I don't know.
[01:07:23.180 --> 01:07:25.180]   -One of the big hotels in Beijing is actually --
[01:07:25.180 --> 01:07:26.180]   -Not even the first of me, sure.
[01:07:26.180 --> 01:07:30.180]   -One of the hotels in Beijing is actually owned by the security services.
[01:07:30.180 --> 01:07:31.180]   -Oh, sure.
[01:07:31.180 --> 01:07:32.180]   -Yeah.
[01:07:32.180 --> 01:07:36.180]   And I'm told that's -- if you want to -- if you want to prostitute, that's the place to go.
[01:07:36.180 --> 01:07:37.180]   -Seriously.
[01:07:37.180 --> 01:07:39.180]   -That is often true.
[01:07:39.180 --> 01:07:40.180]   -Yeah.
[01:07:40.180 --> 01:07:45.180]   -To report this week that the Guardian is looking at starting the Guardian branded hotel,
[01:07:45.180 --> 01:07:47.180]   which just struck me as it was like faulty towers.
[01:07:47.180 --> 01:07:51.180]   [laughter]
[01:07:51.180 --> 01:07:53.180]   -I don't know.
[01:07:53.180 --> 01:07:54.180]   I guess, you know what?
[01:07:54.180 --> 01:07:58.180]   I think some of this is -- I've so brow beaten by all this other --
[01:07:58.180 --> 01:07:59.180]   I don't care anymore.
[01:07:59.180 --> 01:08:01.180]   It's like, "Oh, head, follow me around.
[01:08:01.180 --> 01:08:02.180]   What are they going to get?"
[01:08:02.180 --> 01:08:03.180]   -Yeah.
[01:08:03.180 --> 01:08:04.180]   -Yeah.
[01:08:04.180 --> 01:08:05.180]   -What don't they know?
[01:08:05.180 --> 01:08:06.180]   -I think that's how most people feel.
[01:08:06.180 --> 01:08:09.180]   But, you know, they're -- and this is a thing.
[01:08:09.180 --> 01:08:14.180]   There are always some people for whom it's really non-trivial and really serious.
[01:08:14.180 --> 01:08:17.180]   -Those people should log out of Google.
[01:08:17.180 --> 01:08:21.180]   -Yeah, but, you know, so let's take, for an example.
[01:08:21.180 --> 01:08:23.180]   -They should use burner phones.
[01:08:23.180 --> 01:08:29.180]   -So, let's say you are just to take an example, a victim of spousal abuse.
[01:08:29.180 --> 01:08:30.180]   -Right.
[01:08:30.180 --> 01:08:35.180]   -And you're involved with sort of political activities to try and get the law to protect
[01:08:35.180 --> 01:08:39.180]   people against spousal abuse better or something like that.
[01:08:39.180 --> 01:08:43.180]   And so, in order to conduct your activism and communicate, you need to use these services,
[01:08:43.180 --> 01:08:45.180]   because that's where people are.
[01:08:45.180 --> 01:08:50.180]   But, you know, you're not going to be able to use the services,
[01:08:50.180 --> 01:08:53.180]   because that's where people are.
[01:08:53.180 --> 01:08:57.180]   But, you know, at the same time, you want your privacy protected,
[01:08:57.180 --> 01:09:00.180]   and you have really good reason to want your privacy protected.
[01:09:00.180 --> 01:09:06.180]   Or you're living in a country where, you know, your political activity could be putting
[01:09:06.180 --> 01:09:07.180]   your life at risk.
[01:09:07.180 --> 01:09:12.180]   And, you know, there are a lot of places where that's the case.
[01:09:12.180 --> 01:09:18.180]   And I think a lot of times the concern for privacy may not necessarily be for the sake
[01:09:18.180 --> 01:09:23.180]   of the majority, because I think the majority of people in any given country
[01:09:23.180 --> 01:09:27.180]   probably don't care that much or may feel like it doesn't matter.
[01:09:27.180 --> 01:09:33.180]   But, you know, if people who are unpopular, people who are physically vulnerable,
[01:09:33.180 --> 01:09:40.180]   you know, people who have peaceful yet, you know, kind of strange ideas that make others uncomfortable,
[01:09:40.180 --> 01:09:47.180]   if they have nowhere to go unless they're engineers or programmers or geeks,
[01:09:47.180 --> 01:09:53.180]   you know, if you're just an ordinary person who uses technology about the normal level
[01:09:53.180 --> 01:09:59.180]   and you don't have technical skills, if you're not able to kind of participate,
[01:09:59.180 --> 01:10:01.180]   that's a real problem.
[01:10:01.180 --> 01:10:02.180]   No, I completely agree.
[01:10:02.180 --> 01:10:06.180]   I completely agree and that's something to be very aware of.
[01:10:06.180 --> 01:10:10.180]   And the point, and I guess the point, and we'll sum up the conversation this way,
[01:10:10.180 --> 01:10:12.180]   is you deserve a choice.
[01:10:12.180 --> 01:10:15.180]   But, you do have a choice.
[01:10:15.180 --> 01:10:20.180]   I guess the issue is, is the internet, and this was an interesting conversation
[01:10:20.180 --> 01:10:23.180]   we had a couple of weeks ago, a human right.
[01:10:23.180 --> 01:10:31.180]   And if it is, then you know, and I think increasingly it's not a right, it's a need,
[01:10:31.180 --> 01:10:37.180]   then you should have the right to participate on the internet without being tracked if that's your desire.
[01:10:37.180 --> 01:10:40.180]   Yeah, I mean, increasingly you can't apply for a job, you know.
[01:10:40.180 --> 01:10:43.180]   There are some communities where they're shutting down post offices,
[01:10:43.180 --> 01:10:49.180]   and if you don't have the internet, you know, your ability to communicate with the world is getting constrained.
[01:10:49.180 --> 01:10:52.180]   And so, so yeah, there.
[01:10:52.180 --> 01:10:55.180]   But I think track is the wrong way to put it, Leo, because that's the most.
[01:10:55.180 --> 01:11:00.180]   Yeah, but we do need meaningful choices and we need meaningful levels of access.
[01:11:00.180 --> 01:11:01.180]   I agree completely.
[01:11:01.180 --> 01:11:07.180]   I think what it comes down to, a different way to put it, is that there are appropriate times
[01:11:07.180 --> 01:11:12.180]   and needs for anonymity and sitter-nimity, and some control over that.
[01:11:12.180 --> 01:11:15.180]   And you may not be able to use that everywhere.
[01:11:15.180 --> 01:11:18.180]   You know, you can't drive if you don't say who you are and prove that.
[01:11:18.180 --> 01:11:19.180]   That's true.
[01:11:19.180 --> 01:11:20.180]   And we agree with that.
[01:11:20.180 --> 01:11:26.180]   But for Rebecca's case, what you really need is a way to securely maintain your anonymity
[01:11:26.180 --> 01:11:31.180]   or under your pseudonym and know that you're secure in that way.
[01:11:31.180 --> 01:11:36.180]   And if you're a whistleblower, if you're a vulnerable person and so on, there are needs for that.
[01:11:36.180 --> 01:11:39.180]   And there are other cases where this is not appropriate, where you have to say who you are,
[01:11:39.180 --> 01:11:41.180]   because otherwise you can't get a license or get a passport and so on.
[01:11:41.180 --> 01:11:45.180]   These are norms we'll figure out.
[01:11:45.180 --> 01:11:46.180]   Mm-hmm.
[01:11:46.180 --> 01:11:52.180]   We're going to wrap things up with, because it's time for TNT, with Gina Trapani's tip of the week.
[01:11:52.180 --> 01:11:53.180]   Two tips today.
[01:11:53.180 --> 01:11:55.180]   Well, yeah, two tips today.
[01:11:55.180 --> 01:11:59.180]   The first one goes along with this conversation really well.
[01:11:59.180 --> 01:12:05.180]   March 1st, Google's new privacy policy consolidation goes into effect.
[01:12:05.180 --> 01:12:10.180]   And if you want to start with a clean slate, basically, if you've been tracking your web history
[01:12:10.180 --> 01:12:14.180]   or if Google has been keeping track of your web history up until now, when the new privacy policy goes
[01:12:14.180 --> 01:12:17.180]   into effect on March 1st, it'll be retroactive.
[01:12:17.180 --> 01:12:18.180]   So Google will be able to use the data.
[01:12:18.180 --> 01:12:24.180]   It's collected in your web history to inform ads and other suggestions and its other products.
[01:12:24.180 --> 01:12:27.180]   You want to start with a clean slate before the privacy policy consolidation.
[01:12:27.180 --> 01:12:30.180]   The EFF, actually, we should post this in the show notes.
[01:12:30.180 --> 01:12:32.180]   The EFF just walks through.
[01:12:32.180 --> 01:12:39.180]   You can go to google.com/history, sign into your Google account, and you can basically clean your web history up until now.
[01:12:39.180 --> 01:12:41.180]   Not saying you can turn it off if you want.
[01:12:41.180 --> 01:12:43.180]   It's basically three steps.
[01:12:43.180 --> 01:12:48.180]   Go to google.com/history and then remove all web history.
[01:12:48.180 --> 01:12:49.180]   And that's it.
[01:12:49.180 --> 01:12:52.180]   I'm not saying that you should do this or that you should turn it off.
[01:12:52.180 --> 01:12:53.180]   Web history is actually really useful.
[01:12:53.180 --> 01:12:58.180]   I'm just saying that when this privacy policy consolidation goes into effect on March 1st,
[01:12:58.180 --> 01:13:02.180]   if you don't want Google to use all the data, it's collected thus far and just want to start with a clean slate on that day,
[01:13:02.180 --> 01:13:04.180]   then clean out your web history.
[01:13:04.180 --> 01:13:06.180]   There's your right to be forgotten.
[01:13:06.180 --> 01:13:10.180]   Google deserves to be commended a little bit for the fact that they do give you this.
[01:13:10.180 --> 01:13:13.180]   They give you a dashboard that lets you know what they know.
[01:13:13.180 --> 01:13:15.180]   They give you the ability to delete it.
[01:13:15.180 --> 01:13:20.180]   They give you the ability to extract your data from Google and move on, none of which Apple gives you.
[01:13:20.180 --> 01:13:27.180]   So, you know, admittedly Google was bad and wrong and sneaky, but they do give you the tools to get out.
[01:13:27.180 --> 01:13:29.180]   It's true.
[01:13:29.180 --> 01:13:32.180]   And I think as long as companies communicate, what are they doing with this data?
[01:13:32.180 --> 01:13:33.180]   How are they collecting it?
[01:13:33.180 --> 01:13:34.180]   How do I clear mine out?
[01:13:34.180 --> 01:13:37.180]   And they have a very good privacy policy, very clear privacy policy.
[01:13:37.180 --> 01:13:40.180]   And now it's very clear and a lot more simple.
[01:13:40.180 --> 01:13:42.180]   Second tip, Google Docs for Android.
[01:13:42.180 --> 01:13:44.180]   A new update just came out today.
[01:13:44.180 --> 01:13:47.180]   You can see real-time collaboration.
[01:13:47.180 --> 01:13:48.180]   So, there's a little video up.
[01:13:48.180 --> 01:13:49.180]   Wow.
[01:13:49.180 --> 01:13:54.180]   Jeff and Leo and I are all working on the twig, rundown for today of Stories.
[01:13:54.180 --> 01:14:01.180]   I can see on my Android phone, you know, Jeff and Leo making changes to the spreadsheet or the document in real-time on my phone.
[01:14:01.180 --> 01:14:02.180]   You know, that was happening.
[01:14:02.180 --> 01:14:06.180]   I didn't know why, but that was happening while I was working on the doc right now.
[01:14:06.180 --> 01:14:07.180]   I was looking at things.
[01:14:07.180 --> 01:14:10.180]   I saw these people in their mod of, wow, that's weird.
[01:14:10.180 --> 01:14:11.180]   That is...
[01:14:11.180 --> 01:14:12.180]   Yes.
[01:14:12.180 --> 01:14:13.180]   Holy cow.
[01:14:13.180 --> 01:14:14.180]   So, the video is kind of cute.
[01:14:14.180 --> 01:14:18.180]   You can see that they're putting a best man is going to give his toast at the wedding and he's got all of his friends.
[01:14:18.180 --> 01:14:19.180]   This is a good idea.
[01:14:19.180 --> 01:14:21.180]   Making suggestions on their phones.
[01:14:21.180 --> 01:14:22.180]   That's a very good idea.
[01:14:22.180 --> 01:14:23.180]   So, it's a good idea.
[01:14:23.180 --> 01:14:24.180]   It's a good idea.
[01:14:24.180 --> 01:14:29.180]   I mean, the real-time collaboration has always been great in Google Docs, but the fact that you can see it on your phone is pretty incredible.
[01:14:29.180 --> 01:14:32.180]   UI is pretty good even for small screens.
[01:14:32.180 --> 01:14:35.180]   So, it just lets you get worked on on the go.
[01:14:35.180 --> 01:14:36.180]   I love that.
[01:14:36.180 --> 01:14:39.180]   And it's Android only, I believe, right now.
[01:14:39.180 --> 01:14:43.180]   Don't write a best man speech without it.
[01:14:43.180 --> 01:14:47.180]   That's a pretty good example.
[01:14:47.180 --> 01:14:49.180]   Yeah, it was a good example.
[01:14:49.180 --> 01:14:55.180]   Of course, in reality, most of us use Google Docs to, you know, write for ports and write and spreadsheets.
[01:14:55.180 --> 01:14:56.180]   Real stuff.
[01:14:56.180 --> 01:14:57.180]   Yeah.
[01:14:57.180 --> 01:14:59.180]   Jeff Jarvis, your number of the week.
[01:14:59.180 --> 01:15:00.180]   I'll go with this one.
[01:15:00.180 --> 01:15:01.180]   Five.
[01:15:01.180 --> 01:15:05.180]   I was down this morning at Nike doing a big HUHA presentation.
[01:15:05.180 --> 01:15:19.180]   They were kind of, actually, as soon as their Apple product release, they released the fuel band, which went on sale last week, or went on sale today, which is like the up and like the fit bit and so on, but it's done in a Nike way.
[01:15:19.180 --> 01:15:22.180]   But the thing that they, oh, man, I'm frozen.
[01:15:22.180 --> 01:15:24.180]   Well, here's the fuel band.
[01:15:24.180 --> 01:15:25.180]   You can show it to you.
[01:15:25.180 --> 01:15:27.180]   I'm not sure what the fuel points for moving around.
[01:15:27.180 --> 01:15:31.180]   Did you get, because I'm wearing up, but this might work, which would be nice.
[01:15:31.180 --> 01:15:33.180]   I'm supposed to get one tonight.
[01:15:33.180 --> 01:15:34.180]   I'm being able to play with it tonight.
[01:15:34.180 --> 01:15:41.180]   But what they did, what they showed today was they all showed new shoes with five sensors in them.
[01:15:41.180 --> 01:15:42.180]   Wow.
[01:15:42.180 --> 01:15:50.180]   And ties, it ties directly by Bluetooth to your phone, to your iPod, I mean, your iPhone.
[01:15:50.180 --> 01:16:01.180]   So they showed, for example, being able to take a video of you playing basketball and superimposing on that video, how many fuel points you got, how fast you went, how high you jumped.
[01:16:01.180 --> 01:16:06.180]   They have people like Rafael Nadal doing workouts, and you can pair yourself with that workout.
[01:16:06.180 --> 01:16:08.180]   But of course, here's the cloud touch.
[01:16:08.180 --> 01:16:16.180]   You can put it into the cloud and it becomes social and you can compete with other people over a workout a day, a month, whatever.
[01:16:16.180 --> 01:16:21.180]   And their argument is that it makes workouts not be work and be more fun.
[01:16:21.180 --> 01:16:25.180]   It also creates an incredible amount of fascinating data here.
[01:16:25.180 --> 01:16:27.180]   And we can get in the privacy discussion too.
[01:16:27.180 --> 01:16:33.180]   But they're going to have aggregated data about people's exercise that I think will be really interesting.
[01:16:33.180 --> 01:16:39.180]   So it's like, somebody said on Twitter as I was tweeting, it's like connect, but you can actually take it outside.
[01:16:39.180 --> 01:16:40.180]   Right.
[01:16:40.180 --> 01:16:41.180]   Right.
[01:16:41.180 --> 01:16:49.180]   You know, I have to say, this is why I don't like this whole privacy discussion because that's the kind of thing that kills this kind of innovation.
[01:16:49.180 --> 01:16:50.180]   Yes.
[01:16:50.180 --> 01:16:54.180]   And, you know, it's opt out as long as it's opt out. You don't have to wear one.
[01:16:54.180 --> 01:16:59.180]   You don't have to send it anywhere. You don't have to do anything. But you know, oh my God, Nike knows how fast I run.
[01:16:59.180 --> 01:17:02.180]   I'll be telling you, right now, Nike, that's very slowly.
[01:17:02.180 --> 01:17:05.180]   I'll tell you, I'll give you that information.
[01:17:05.180 --> 01:17:09.180]   I put air quotes around run when I go do it.
[01:17:09.180 --> 01:17:13.180]   By the way, so you would be both. Look at the stuff that I just try.
[01:17:13.180 --> 01:17:17.180]   This is a great video. You got to look at the stuff that that doesn't count in it.
[01:17:17.180 --> 01:17:20.180]   Let's see if I can rewind it. I can't. There's some.
[01:17:20.180 --> 01:17:27.180]   There are some really great clips here. This is very funny stuff that doesn't count the dude from the big Lebowski sitting in his chair does not count.
[01:17:27.180 --> 01:17:30.180]   [laughter]
[01:17:30.180 --> 01:17:33.180]   Very funny. Yeah, I love that.
[01:17:33.180 --> 01:17:39.180]   My tool is something I had never heard of. And then Trey Radcliffe was here yesterday doing Twitter photo.
[01:17:39.180 --> 01:17:44.180]   And Trey is somehow magically wired into the Google verse.
[01:17:44.180 --> 01:17:48.180]   And he's offering invites to something called Google Schemer.
[01:17:48.180 --> 01:17:51.180]   Did you, had either of you heard of Google Schemer?
[01:17:51.180 --> 01:17:53.180]   No. I love Schemer.
[01:17:53.180 --> 01:17:56.180]   Oh. And I'm sorry that I didn't bring it up sooner. It's really cool.
[01:17:56.180 --> 01:18:02.180]   Thanks for telling us. I know. I'm sorry. I love aspirational things, especially social aspirational things.
[01:18:02.180 --> 01:18:05.180]   It's like I really want to run a 5K and then you can tell your friends.
[01:18:05.180 --> 01:18:07.180]   But I'll let you explain it. It's a really cool app.
[01:18:07.180 --> 01:18:11.180]   Well, the only explanation I'm going to give is it's invite only.
[01:18:11.180 --> 01:18:17.180]   But somehow Trey has seems to have an infinite number of invites at his website, Stocking Customs.com.
[01:18:17.180 --> 01:18:25.180]   Because he's using it to get people to sign up for his photo walk, which is coming up on Sunday with Tom Anderson,
[01:18:25.180 --> 01:18:30.180]   the guy who started MySpace, which is kind of cool. I met Tom yesterday. He's a pretty cool guy.
[01:18:30.180 --> 01:18:33.180]   He's got a cool. I tried to get him on the show before.
[01:18:33.180 --> 01:18:35.180]   No, he's very private. He won't come on.
[01:18:35.180 --> 01:18:38.180]   You know, it's very private. He started MySpace.
[01:18:38.180 --> 01:18:42.180]   You know what he did before MySpace, by the way?
[01:18:42.180 --> 01:18:44.180]   Sorry.
[01:18:44.180 --> 01:18:47.180]   It's pretty funny, isn't it? He's a nice guy. I have his friend on the corner.
[01:18:47.180 --> 01:18:52.180]   He's done. He doesn't want to. He's like, I did. I don't want to be in public. I just want to be me.
[01:18:52.180 --> 01:18:56.180]   And I completely don't blame him and he's worth, you know, a good jillion dollars.
[01:18:56.180 --> 01:18:57.180]   So he does could do whatever he wants.
[01:18:57.180 --> 01:19:02.180]   Before he did MySpace, I hope I'm not revealing a confidence. He did two things that I've done.
[01:19:02.180 --> 01:19:08.180]   He wrote chapters for John C. Devorex books. In fact, one that I wrote a chapter for.
[01:19:08.180 --> 01:19:13.180]   And he was an intern at Tech TV.
[01:19:13.180 --> 01:19:15.180]   Oh, wow.
[01:19:15.180 --> 01:19:21.180]   He was an intern for three months for internet tonight. I knew him. I didn't know I knew him.
[01:19:21.180 --> 01:19:26.180]   He was the ease, by the way, now I think officially the most successful tech TV intern in history.
[01:19:26.180 --> 01:19:28.180]   And unlikely to be unseated.
[01:19:28.180 --> 01:19:31.180]   Not just financially, but also out supporting Rupert Murdoch.
[01:19:31.180 --> 01:19:35.180]   I thought it. Yeah. He, right. He sold MySpace before.
[01:19:35.180 --> 01:19:38.180]   Take that, Rupert. Yeah.
[01:19:38.180 --> 01:19:44.180]   Anyway, so here's where you go. Go to stuck in customs.com, LA Photo Walk Entry.
[01:19:44.180 --> 01:19:48.180]   And right here, there's a link that will actually give you an invite to Schemer.
[01:19:48.180 --> 01:19:54.180]   So Schemer is, first of all, it doesn't look like Google. It looks so cool.
[01:19:54.180 --> 01:19:56.180]   It's really neat.
[01:19:56.180 --> 01:19:59.180]   Gina, I just got it. What is it? Tell me what it does.
[01:19:59.180 --> 01:20:02.180]   I can sign up for events. I can say, this is what I want to do.
[01:20:02.180 --> 01:20:07.180]   Yeah. So you can enter a scheme, say, like, I want to learn Italian or I want to climb Mount Everest.
[01:20:07.180 --> 01:20:12.180]   So it's kind of like aspirational to-do list, like a big kind of lifeless goal type thing.
[01:20:12.180 --> 01:20:13.180]   Right.
[01:20:13.180 --> 01:20:17.180]   And then you can see schemes that people from your Google circles have proposed.
[01:20:17.180 --> 01:20:21.180]   Like, you know, some of my friends have said that they want to write a book.
[01:20:21.180 --> 01:20:24.180]   They want to visit certain museums.
[01:20:24.180 --> 01:20:27.180]   And so then you can say, hey, I want to do this too.
[01:20:27.180 --> 01:20:30.180]   And you can mark schemes as things that you've already done.
[01:20:30.180 --> 01:20:34.180]   So it's kind of like a social, you know, lifeless or kind of bucket list type thing.
[01:20:34.180 --> 01:20:39.180]   What I like about it is that they're kind of big goals and a lot of fun stuff.
[01:20:39.180 --> 01:20:41.180]   You can really tell a lot about people.
[01:20:41.180 --> 01:20:45.180]   And it's just fun to kind of browse around and say, oh, yeah, you know, I want to do that too.
[01:20:45.180 --> 01:20:48.180]   And then people can leave comments and that kind of thing.
[01:20:48.180 --> 01:20:51.180]   Would you say there's functionality to it or is it just fun?
[01:20:51.180 --> 01:20:59.180]   Yeah, I mean, I think it's fun in that it's a social app just sort of centered around the idea of like stuff that you want to do.
[01:20:59.180 --> 01:21:03.180]   And you can, you know, the language is like you inspire people.
[01:21:03.180 --> 01:21:08.180]   If you add a thing that a lot of people say they want to do, you can say, oh, hey, you've inspired 50 people to want to do this.
[01:21:08.180 --> 01:21:11.180]   In fact, there's quite a few people who want to appear on Twitch.
[01:21:11.180 --> 01:21:12.180]   I noticed that.
[01:21:12.180 --> 01:21:15.180]   That's become a very popular thing is to-
[01:21:15.180 --> 01:21:16.180]   So you can see-
[01:21:16.180 --> 01:21:17.180]   Yeah.
[01:21:17.180 --> 01:21:19.180]   You can see who's done it and who wants to do it.
[01:21:19.180 --> 01:21:21.180]   So it's something that you want to do.
[01:21:21.180 --> 01:21:23.180]   You can talk to people and say, hey, how did you go about this?
[01:21:23.180 --> 01:21:25.180]   I want to take a hot air balloon ride.
[01:21:25.180 --> 01:21:26.180]   You know, where did you go?
[01:21:26.180 --> 01:21:27.180]   How was it?
[01:21:27.180 --> 01:21:28.180]   That kind of thing.
[01:21:28.180 --> 01:21:29.180]   Bring home a steak from Shobs.
[01:21:29.180 --> 01:21:30.180]   I mean, some of this is weird.
[01:21:30.180 --> 01:21:32.180]   Yeah, some of it is really weird.
[01:21:32.180 --> 01:21:33.180]   Yeah.
[01:21:33.180 --> 01:21:34.180]   But it's fun.
[01:21:34.180 --> 01:21:35.180]   It's fun and it's social.
[01:21:35.180 --> 01:21:39.180]   And it's- for me, it's like one of those things that you go to and just kind of feel inspired.
[01:21:39.180 --> 01:21:41.180]   Like, oh, yeah, this is- this sounds really fun.
[01:21:41.180 --> 01:21:42.180]   I do want to do this.
[01:21:42.180 --> 01:21:46.180]   Oh, I didn't know that so-and-so, you know, is it pilot or, you know, that kind of thing.
[01:21:46.180 --> 01:21:48.180]   Or had, you know, filed a patent.
[01:21:48.180 --> 01:21:51.180]   I think things that you would, you know-
[01:21:51.180 --> 01:21:55.180]   Next time your kid comes up to you and says, I'm bored.
[01:21:55.180 --> 01:21:59.180]   You say, log into Schemer and find something to do.
[01:21:59.180 --> 01:22:00.180]   Find something to do.
[01:22:00.180 --> 01:22:01.180]   Exactly.
[01:22:01.180 --> 01:22:05.180]   There's a lot of options and a lot of inspiration.
[01:22:05.180 --> 01:22:09.180]   It's way more fun than a to-do list app where it's like pick up the milk or the dentist.
[01:22:09.180 --> 01:22:10.180]   It is.
[01:22:10.180 --> 01:22:11.180]   It's fun stuff, right?
[01:22:11.180 --> 01:22:13.180]   It's stuff you want to do.
[01:22:13.180 --> 01:22:16.180]   I want to thank Rebecca McKinnon for being here.
[01:22:16.180 --> 01:22:17.180]   So great to have Rebecca.
[01:22:17.180 --> 01:22:19.180]   She talked about an inspiration.
[01:22:19.180 --> 01:22:24.180]   You know, we mentioned the book, which is brand new consent of the networked from basic books,
[01:22:24.180 --> 01:22:31.180]   but I also really want to give her props for GlobalVoicesOnline.org, which is an amazing-
[01:22:31.180 --> 01:22:34.180]   It's kind of a journalism Wikipedia, global-
[01:22:34.180 --> 01:22:42.180]   It's a community of bloggers who report citizen media from around the world, and it is just brilliant.
[01:22:42.180 --> 01:22:44.180]   And it is so important, I think.
[01:22:44.180 --> 01:22:49.180]   Well, I've got to say too, the people working on that and the community around it is so inspiring.
[01:22:49.180 --> 01:22:51.180]   I just learned so much from them.
[01:22:51.180 --> 01:22:54.180]   But thank you so much for having me on.
[01:22:54.180 --> 01:23:01.180]   If you want to get the news disintermediated direct from people who are living it, this is the place to go GlobalVoicesOnline.org.
[01:23:01.180 --> 01:23:02.180]   Absolutely.
[01:23:02.180 --> 01:23:03.180]   Yeah, amazing.
[01:23:03.180 --> 01:23:04.180]   Amazing.
[01:23:04.180 --> 01:23:07.180]   And get the book because this is inspiring too, and I think it's homework.
[01:23:07.180 --> 01:23:08.180]   This is to-do too.
[01:23:08.180 --> 01:23:09.180]   This is homework.
[01:23:09.180 --> 01:23:16.180]   But it's fun. I'm told by people who are not connected to me that they think it's an easy read.
[01:23:16.180 --> 01:23:18.180]   It's not a slog.
[01:23:18.180 --> 01:23:19.180]   No, it is.
[01:23:19.180 --> 01:23:24.180]   It's quite- because you load it with stories and anecdotes, which I think is key to something like this.
[01:23:24.180 --> 01:23:27.180]   So there's a lot of examples of- and it's really- it's fun.
[01:23:27.180 --> 01:23:28.180]   It is. It's a great read.
[01:23:28.180 --> 01:23:29.180]   Thank you, Rebecca.
[01:23:29.180 --> 01:23:30.180]   Thank you so much.
[01:23:30.180 --> 01:23:31.180]   Please come back.
[01:23:31.180 --> 01:23:33.180]   You were really great. Everybody in the chatroom agrees.
[01:23:33.180 --> 01:23:35.180]   We want you coming back on the show.
[01:23:35.180 --> 01:23:36.180]   Love to do that.
[01:23:36.180 --> 01:23:37.180]   Thanks.
[01:23:37.180 --> 01:23:45.180]   We do this show every Wednesday, right after security now, which can- you know, it's usually one o'clock Pacific, 4 p.m. Eastern Time on twit.tv.
[01:23:45.180 --> 01:23:49.180]   UTC, that would be 2,100. But you know, you don't have to watch live.
[01:23:49.180 --> 01:23:57.180]   It's fun to watch live, but if you can't, we make audio and video available for download absolutely free on our website, twit.tv, or just subscribe
[01:23:57.180 --> 01:24:04.180]   and iTunes or whatever podcast client you use, a downcaster on iOS or listen on Android.
[01:24:04.180 --> 01:24:06.180]   And you'll get those shows automatically.
[01:24:06.180 --> 01:24:08.180]   And I think that's worth doing.
[01:24:08.180 --> 01:24:16.180]   Gina is of course blogging at smarterware.org, writing software all the time for expert labs, including ThinkUp app.
[01:24:16.180 --> 01:24:18.180]   Thank you so much for being here, Gina.
[01:24:18.180 --> 01:24:19.180]   Great to have you.
[01:24:19.180 --> 01:24:20.180]   Great to have you.
[01:24:20.180 --> 01:24:21.180]   Great to have you.
[01:24:21.180 --> 01:24:24.180]   Jeff Jarvis makes me look moderate, and that's saying something.
[01:24:24.180 --> 01:24:33.180]   Buzzmachine.com is his blog at Jeff Jarvis on Twitter at Google+ of course, and he's the author of a great book, which you must read
[01:24:33.180 --> 01:24:35.180]   and I'm here to talk about the book.
[01:24:35.180 --> 01:24:37.180]   I'm here to talk about the book.
[01:24:37.180 --> 01:24:39.180]   I'm here to talk about the book.
[01:24:39.180 --> 01:24:41.180]   I'm here to talk about the book.
[01:24:41.180 --> 01:24:43.180]   I'm here to talk about the book.
[01:24:43.180 --> 01:24:45.180]   I'm here to talk about the book.
[01:24:45.180 --> 01:24:47.180]   I'm here to talk about the book.
[01:24:47.180 --> 01:24:49.180]   I'm here to talk about the book.
[01:24:49.180 --> 01:24:51.180]   I'm here to talk about the book.
[01:24:51.180 --> 01:24:53.180]   I'm here to talk about the book.
[01:24:53.180 --> 01:24:54.180]   I'm here to talk about the book.
[01:24:54.180 --> 01:24:55.180]   I'm here to talk about the book.
[01:24:55.180 --> 01:24:57.180]   I'm here to talk about the book.
[01:24:57.180 --> 01:24:58.180]   I'm here to talk about the book.
[01:24:58.180 --> 01:24:59.180]   I'm here to talk about the book.
[01:24:59.180 --> 01:25:00.180]   I'm here to talk about the book.
[01:25:00.180 --> 01:25:01.180]   I'm here to talk about the book.
[01:25:01.180 --> 01:25:08.180]   [Music]

