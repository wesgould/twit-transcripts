;FFMETADATA1
title=It's Bots All the Way Down
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=348
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2016
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.000]   It's time for Twig this week in Google.
[00:00:03.000 --> 00:00:06.000]   Oh, we're going to have fun today. Kevin Marks is here.
[00:00:06.000 --> 00:00:12.000]   Matt Cutts is here from Google. And Jeff Jarvis, he's live from the Facebook F8 conference.
[00:00:12.000 --> 00:00:14.000]   We're not exactly his life in the lunch tent.
[00:00:14.000 --> 00:00:21.000]   But we'll have the latest from F8 VR robots and Louis CK thrown in just to spice things up.
[00:00:21.000 --> 00:00:22.000]   It's all next.
[00:00:22.000 --> 00:00:23.000]   On Twig.
[00:00:23.000 --> 00:00:27.000]   Netcast you love.
[00:00:27.000 --> 00:00:29.000]   From people you trust.
[00:00:29.000 --> 00:00:34.000]   This is Twig.
[00:00:34.000 --> 00:00:42.000]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:42.000 --> 00:00:50.000]   This is Twig this week in Google.
[00:00:50.000 --> 00:00:55.000]   Episode 348 recorded Wednesday, April 13, 2016.
[00:00:55.000 --> 00:00:58.000]   It's bots all the way down.
[00:00:58.000 --> 00:01:04.000]   This week in Google is brought to you by Texture, the mobile app that lets you access the world's most popular magazines
[00:01:04.000 --> 00:01:08.000]   anytime, anywhere, using your phone or tablet.
[00:01:08.000 --> 00:01:12.000]   For your free trial visit texture.com/twig.
[00:01:12.000 --> 00:01:15.000]   It's time for Twig this week in Google, the show where we cover.
[00:01:15.000 --> 00:01:19.000]   You know, I've been thinking about this. I really should rename this show.
[00:01:19.000 --> 00:01:24.000]   Because it isn't, as anybody listens, about just about Google.
[00:01:24.000 --> 00:01:28.000]   I think it really should be this week in the cloud.
[00:01:28.000 --> 00:01:30.000]   Or I was thinking maybe something...
[00:01:30.000 --> 00:01:32.000]   That's really the truth.
[00:01:32.000 --> 00:01:35.000]   There's Jeff Jarvis, Professor of Journalism at CUNY, the City University of New York.
[00:01:35.000 --> 00:01:40.000]   He is in San Francisco right now at the Fort Mason Center at F8, right?
[00:01:40.000 --> 00:01:46.000]   Yes, I am. I'm sitting on the floor by an access point outside the main Herbst Auditorium
[00:01:46.000 --> 00:01:51.000]   so I can get connectivity without wind.
[00:01:51.000 --> 00:01:58.000]   We were going to walk by thinking this is pretty strange, but I'm just sitting here doing anything for the good of Twig.
[00:01:58.000 --> 00:02:02.000]   That's pretty funny. I think that should be our motto.
[00:02:02.000 --> 00:02:07.000]   Without wind. What is it? Audio without wind.
[00:02:07.000 --> 00:02:10.000]   Hot air, but no wind.
[00:02:10.000 --> 00:02:14.000]   Hey, we actually have an amazing panel because we weren't sure whether we would get you or not.
[00:02:14.000 --> 00:02:17.000]   So we brought in the big guns. I hope you don't mind.
[00:02:17.000 --> 00:02:21.000]   Matt Cutz is also here. Googler himself, one of the early Googlers.
[00:02:21.000 --> 00:02:22.000]   Yay, Matt. Love Matt.
[00:02:22.000 --> 00:02:23.000]   Good to see everybody.
[00:02:23.000 --> 00:02:25.000]   Great to see you.
[00:02:25.000 --> 00:02:28.000]   And as if that weren't enough, so is Kevin Marks.
[00:02:28.000 --> 00:02:34.000]   A former Googler, former Appler, former BBCer, former British telecomer.
[00:02:34.000 --> 00:02:37.000]   He's former many things.
[00:02:37.000 --> 00:02:42.000]   Go ahead, give him a high five there. High five Kevin to your right.
[00:02:42.000 --> 00:02:46.000]   Oh, I find either way.
[00:02:46.000 --> 00:02:48.000]   Right.
[00:02:48.000 --> 00:02:50.000]   They're all together.
[00:02:50.000 --> 00:02:53.000]   We brought the gang all together for this.
[00:02:53.000 --> 00:02:57.000]   How's F8 been, Jeff? Has it been fun?
[00:02:57.000 --> 00:03:02.000]   I suspect we're going to be talking a lot about F8 today.
[00:03:02.000 --> 00:03:07.000]   Oh, poor Jeff.
[00:03:07.000 --> 00:03:11.000]   See, what's good about Jeff? But who is that?
[00:03:11.000 --> 00:03:13.000]   Oh, that was F8. Okay.
[00:03:13.000 --> 00:03:19.000]   What's good about Jeff is he really believes in dog food and Google products.
[00:03:19.000 --> 00:03:24.000]   So he will not buy a laptop. He insists on using a Chromebook.
[00:03:24.000 --> 00:03:27.000]   And now he's using a Pixel C, right?
[00:03:27.000 --> 00:03:29.000]   Yeah, I am. Well, this way I can do Skype.
[00:03:29.000 --> 00:03:32.000]   I have my laptop in the hotel room, but it will do hangout.
[00:03:32.000 --> 00:03:35.000]   This is fine. I'm not complaining. I'm just saying.
[00:03:35.000 --> 00:03:38.000]   Oh, yes you are. But that's what you're doing.
[00:03:38.000 --> 00:03:42.000]   And good on you, my friend.
[00:03:42.000 --> 00:03:45.000]   So what did you think of Zux? We did not see the keynote.
[00:03:45.000 --> 00:03:49.000]   We did not broadcast the keynote today, but we watched and broadcast the keynote yesterday.
[00:03:49.000 --> 00:03:50.000]   What did you think?
[00:03:50.000 --> 00:03:53.000]   I think it was pretty amazing because he came out and said we're going to do something different.
[00:03:53.000 --> 00:04:01.000]   We're going to talk about a 10 year plan for Facebook about connectivity and access and connectivity,
[00:04:01.000 --> 00:04:04.000]   artificial intelligence and VR AR.
[00:04:04.000 --> 00:04:06.000]   And so he took through a roadmap there.
[00:04:06.000 --> 00:04:13.000]   And I think it was also very important that he made a political statement too, saying that you can't progress to the world by building walls.
[00:04:13.000 --> 00:04:15.000]   You do it by building bridges.
[00:04:15.000 --> 00:04:19.000]   And that Facebook believes in connecting people and that good comes from connecting people.
[00:04:19.000 --> 00:04:21.000]   And that's what this company is about.
[00:04:21.000 --> 00:04:25.000]   That kind of stunned me. I was surprised that he was so political.
[00:04:25.000 --> 00:04:33.000]   So now it's, you know, that the scorecard is Tim Cook and Mark Zuckerberg both stepping up and being very political in their keynotes.
[00:04:33.000 --> 00:04:40.000]   Yes. And I think in this case, you know, in Cook's case, it was about an issue that affects the company directly.
[00:04:40.000 --> 00:04:45.000]   In Zuckerberg's, it was about a philosophy that says that we're trying to connect the world.
[00:04:45.000 --> 00:04:49.000]   The world was getting more disjointed by this kind of talk.
[00:04:49.000 --> 00:04:56.000]   How reasonable is it though for Facebook, which is not yet 10 years old to talk about its 10 year roadmap?
[00:04:56.000 --> 00:05:02.000]   That's like a 15 year old saying, when I'm 40, I'm going to be working in the NBA.
[00:05:02.000 --> 00:05:10.000]   Yeah, it's pretty amazing, but he does have a large strategic view where, I mean, he's not giving up India, be damned.
[00:05:10.000 --> 00:05:13.000]   He's not giving up on the dream of connecting the world.
[00:05:13.000 --> 00:05:20.000]   They put up a map showing where the population lives versus connectivity. This is clearly where they see huge opportunity.
[00:05:20.000 --> 00:05:27.000]   AI, that's what the newsfeed does, right? It gives us what it thinks we want based on knowing us.
[00:05:27.000 --> 00:05:32.000]   And VRAR is all about going from discovery to immersion, as they said.
[00:05:32.000 --> 00:05:39.000]   And so that was all very interesting. And then they did a whole separate track of things yesterday just for media, which was also interesting.
[00:05:39.000 --> 00:05:47.000]   Around live, around VR, around instant articles, and they care about media and they care about content.
[00:05:47.000 --> 00:05:49.000]   So I'm really glad I came. It's really interesting.
[00:05:49.000 --> 00:05:57.000]   One of the things Mark talked about is why they have kind of deconstructed the Blue App.
[00:05:57.000 --> 00:06:06.000]   I mean, it was, in fact, it even shows on this roadmap, you know, the big Blue App, the Facebook App was the heart and soul of Facebook until recently.
[00:06:06.000 --> 00:06:11.000]   And then remember they acquired WhatsApp, they acquired Instagram, they split off Messenger.
[00:06:11.000 --> 00:06:19.000]   And I think for the first time, he kind of, to me, succinctly explained, you know, there's many reasons you might say that you did this.
[00:06:19.000 --> 00:06:25.000]   You know, one is, of course, to occupy more home screen real estate. The other is to get notifications and more of them.
[00:06:25.000 --> 00:06:29.000]   But I think he described it well when he said there's different ways people share.
[00:06:29.000 --> 00:06:36.000]   They share individually, hence the messaging apps. They share to family and friends. That would be for Facebook.
[00:06:36.000 --> 00:06:40.000]   And then they share to socially when that would be Instagram.
[00:06:40.000 --> 00:06:47.000]   And that kind of all makes, he mentioned Facebook groups, which I don't think half of people use Facebook don't even know they have groups.
[00:06:47.000 --> 00:06:48.000]   Yeah.
[00:06:48.000 --> 00:06:56.000]   But it showed that he's thinking a little bit about the different slices of the user base.
[00:06:56.000 --> 00:06:59.000]   Well, it's not just the user base, it's also the use cases.
[00:06:59.000 --> 00:07:00.000]   Right.
[00:07:00.000 --> 00:07:11.000]   The lesson I think that Zuckerberg gave us in media when it comes to breaking up the big Blue App was that we still try to get people to come to one destination that's going to do everything for everybody.
[00:07:11.000 --> 00:07:23.000]   That is the mass media way to treat the world. And Zuckerberg recognizes not only that you're an individual, but also that within the day you have different uses and different needs for connectivity and information and entertainment.
[00:07:23.000 --> 00:07:27.000]   That's the limit.
[00:07:27.000 --> 00:07:33.000]   What, let's talk, there's so much to talk about here. Let me start with bots with messaging.
[00:07:33.000 --> 00:07:38.000]   And I want to make sure that Matt and Kevin have some, something to talk about too.
[00:07:38.000 --> 00:07:40.000]   We can talk about those.
[00:07:40.000 --> 00:07:51.000]   Well, let's talk about it, Kevin. What is, Facebook is better positioned than anybody to take advantage of messaging the messaging platform. Yes?
[00:07:51.000 --> 00:07:56.000]   Maybe. I mean, the thing is they're all, yes, they've already got two big messaging apps.
[00:07:56.000 --> 00:07:59.000]   And a billion, almost a billion users for each.
[00:07:59.000 --> 00:08:03.000]   Yes. I mean, the other big ones, the other people who have lots of users are Google and Apple.
[00:08:03.000 --> 00:08:11.000]   And then there's the various Asian messaging apps that already exist and already doing this. And that's the template they're all looking at.
[00:08:11.000 --> 00:08:15.000]   And the part of this is the app bubble is over.
[00:08:15.000 --> 00:08:21.000]   The people aren't installing apps. They're not making money from apps. And a big chunk of Facebook's revenue was from selling ads for apps.
[00:08:21.000 --> 00:08:29.000]   So that they've been seeing that vanish for a while. So that they're looking for a new platform that they can use and encourage people to build stuff on.
[00:08:29.000 --> 00:08:33.000]   And they can, Facebook has the ability to direct people's attention. That's something they're good at.
[00:08:33.000 --> 00:08:42.000]   They have control over the streams. And so they can feed people to that for a while to make that something that people are paying attention.
[00:08:42.000 --> 00:08:46.000]   Is this driven by people's needs or driven by businesses needs?
[00:08:46.000 --> 00:08:53.000]   Is it driven by what Facebook wants or Amazon or Google or Apple? Or is it driven by what users are looking for?
[00:08:53.000 --> 00:08:57.000]   Is this how users use it now, right?
[00:08:57.000 --> 00:09:06.000]   Well, I'm not sure. I mean, I think it's driven by them wanting a new conduit and a new place to encourage people to interact.
[00:09:06.000 --> 00:09:15.000]   Part of it is that users interact with notifications. And the notifications have taken over from the apps as the sort of primary interface to your phone.
[00:09:15.000 --> 00:09:23.000]   And the messaging is the notifications that you don't turn off, because you may get messages from people.
[00:09:23.000 --> 00:09:27.000]   Somebody called it the last unpoluted part of our phones, which I think was a very good way of phrasing it.
[00:09:27.000 --> 00:09:30.000]   And then now they're busy coming along to pollute it and fill it with a question.
[00:09:30.000 --> 00:09:37.000]   I mean, I have to say it wasn't very compelling to look at 1-800-Flowers and ordering flowers in a messaging app. That was not particularly compelling.
[00:09:37.000 --> 00:09:40.000]   It was not something I would want to do particularly.
[00:09:40.000 --> 00:09:47.000]   No, but imagine if you're just you talk to your phone and you just want to say send flowers to Mark Zuckerberg, which by the way Danny Sullivan did.
[00:09:47.000 --> 00:09:48.000]   It was a brilliant.
[00:09:48.000 --> 00:09:49.000]   Oh, that's funny.
[00:09:49.000 --> 00:09:53.000]   He immediately tested out. So we sent flowers to Mark Zuckerberg.
[00:09:53.000 --> 00:09:57.000]   And then he said, Oh, my wife's not happy. So we said flowers to his wife.
[00:09:57.000 --> 00:10:02.000]   I saw Danny just a few minutes ago, and he said that Zuck sent him immediately said, Thank you for the flowers.
[00:10:02.000 --> 00:10:03.000]   Oh, he knew.
[00:10:03.000 --> 00:10:04.000]   Yeah.
[00:10:04.000 --> 00:10:05.000]   Yeah.
[00:10:05.000 --> 00:10:06.000]   Great touching.
[00:10:06.000 --> 00:10:15.000]   Yeah, I don't think messaging per se, I mean, you know, basically all messaging apps do right now is cut up content and feed it to you in spoonfuls.
[00:10:15.000 --> 00:10:16.000]   That's no use.
[00:10:16.000 --> 00:10:26.000]   But if you imagine conversation as a platform and if you imagine truly agents who are going to go out and get stuff for you and come back and alert you when it really is worth an alert,
[00:10:26.000 --> 00:10:28.000]   then I start to get happy about it.
[00:10:28.000 --> 00:10:32.000]   I followed CNN, you know, as one of I followed a couple of bots just to see.
[00:10:32.000 --> 00:10:35.000]   And it isn't a really good use for news.
[00:10:35.000 --> 00:10:38.000]   We saw the quartz app, which was basically the same interface.
[00:10:38.000 --> 00:10:45.000]   What's interesting is that the Facebook platform does allow them to do a carousel within messengers.
[00:10:45.000 --> 00:10:53.000]   So I actually get the stories, you know, kind of scrolling horizontally, you know, which is interesting.
[00:10:53.000 --> 00:10:58.000]   And then there's buttons on every one so I can read the story, get a summary, ask CNN.
[00:10:58.000 --> 00:11:01.000]   And I presume that alerts will come through this way.
[00:11:01.000 --> 00:11:04.000]   I don't know if this is better than an app from the point of view of the user.
[00:11:04.000 --> 00:11:11.000]   I'll tell you one thing from the point of view of the user, I would much rather talk to a phone representative than have to interact with the company in messaging.
[00:11:11.000 --> 00:11:13.000]   That's going to be painful.
[00:11:13.000 --> 00:11:16.000]   Do you think that there's a company with a lot of friends?
[00:11:16.000 --> 00:11:17.000]   Go ahead.
[00:11:17.000 --> 00:11:19.000]   Let's let Matt wait here.
[00:11:19.000 --> 00:11:23.000]   Well, I'm just curious if there's an age difference because I'm kind of with you, Leo.
[00:11:23.000 --> 00:11:32.000]   I'm like, this sounds like my worst nightmare to have to talk to companies on a chat system and an automated, you know, AI chat system doesn't sound all that much better.
[00:11:32.000 --> 00:11:39.000]   But I'm wondering if people who are, you know, a little younger might feel more comfortable having those sorts of conversations.
[00:11:39.000 --> 00:11:41.000]   Maybe I'm the cranky old man or weird, the cranky old man.
[00:11:41.000 --> 00:11:42.000]   I don't know.
[00:11:42.000 --> 00:11:48.000]   Well, it does recognize that younger people are much more comfortable interacting with their phone than a tablet or a computer.
[00:11:48.000 --> 00:11:50.000]   I mean, that's really where they live.
[00:11:50.000 --> 00:11:53.000]   They're probably more facile than you and I are on the keyboard.
[00:11:53.000 --> 00:11:59.000]   But I don't, I mean, remember, young people aren't even using Facebook very much.
[00:11:59.000 --> 00:12:01.000]   They're using Snapchat.
[00:12:01.000 --> 00:12:15.000]   And so this is not, this is going to be a tough one for Facebook because while they are dominant, for instance, I think Amazon's Echo is a much better platform for ordering flowers than a text-based chat.
[00:12:15.000 --> 00:12:21.000]   Well, okay, I'm going to push back on that one, I think, because the problem with Echo is that it is just voice.
[00:12:21.000 --> 00:12:23.000]   It doesn't have any way of showing you an image.
[00:12:23.000 --> 00:12:29.000]   So you can say, I'd like to warn some flowers and it'll say, okay, I'll do some flowers, but you don't actually get to see what the flowers look like.
[00:12:29.000 --> 00:12:33.000]   Whereas, so you're, you're, you're kind of, it's like, you know, an IVR.
[00:12:33.000 --> 00:12:36.000]   It's like phoning 1-800 flowers and it's like, I'd like some flowers.
[00:12:36.000 --> 00:12:38.000]   What have you got, you know?
[00:12:38.000 --> 00:12:43.000]   Whereas the point about the chat thing is they can put up other UI like you were showing with your carousel.
[00:12:43.000 --> 00:12:58.000]   So you can, you can say, and there's a good post by Stephen Wolfram about this where he says, you know, the difference between a conversation with a human and a conversation with a computer is that the computer can give you a user interface that you can then interact with it.
[00:12:58.000 --> 00:13:08.000]   So you can ask, and obviously he's talking about the stuff that you put with Wolfram, Wolfram Alpha, but does Alpha, does Alpha is an interactive environment like that?
[00:13:08.000 --> 00:13:17.000]   I think that's sponsored. Yeah, so Alpha, you can talk to it and then it gives you, you can say, you know, what's the population of the US over the last 50 years and it'll give you a graph back and stuff like that.
[00:13:17.000 --> 00:13:20.000]   I would love to have a Wolfram Alpha chat bot.
[00:13:20.000 --> 00:13:36.000]   Right. And it's, you know, that's, that's, that's basically something, yeah, that's, and it's, it's not that good at understanding speech, but it is, his take is basically we're putting in a command language that we sort of would teach you how to talk to it and it'll, it'll respond to you in that way.
[00:13:36.000 --> 00:13:41.000]   And the same we do with Siri, Siri or Google now at the moment, you know, there's a set of commands in there, you can give it.
[00:13:41.000 --> 00:13:54.000]   And the challenge with these chat UIs is working out, you know, knowing what the command is for the bot or having something at the other end that's good enough to map through from what you typed in into the subset of things it can do.
[00:13:54.000 --> 00:13:59.000]   And I, and I do have to, I mean, we have to acknowledge that it's very early days.
[00:13:59.000 --> 00:14:07.000]   So just as in the early days of the, of the web and of apps, we're going to see a lot of failed experiments as we kind of learn what the medium needs and wants.
[00:14:07.000 --> 00:14:10.000]   Well, it's not really a good days.
[00:14:10.000 --> 00:14:16.000]   We've had these chat bots for 30 years, you know, well, we had a license.
[00:14:16.000 --> 00:14:20.000]   It's, it's, you know, if you think about the way we use it in IRC or in places like that.
[00:14:20.000 --> 00:14:26.000]   No, no, but we know I've, I've, indie webcam, I see channel has a bot and it called Loki that Aaron wrote.
[00:14:26.000 --> 00:14:29.000]   And it will, it'll respond to you. It'll let you set timers.
[00:14:29.000 --> 00:14:35.000]   Oh, yeah. People who use IRC have had experience of this, but that's a, that's a tiny sliver of the population.
[00:14:35.000 --> 00:14:38.000]   Most people have never experienced anything like this.
[00:14:38.000 --> 00:14:39.000]   No, that's a true.
[00:14:39.000 --> 00:14:40.000]   What about Slack?
[00:14:40.000 --> 00:14:54.000]   Well, in fact, it slacks an interesting counter example because Slack, everybody loved Slack at first, but as you add bots and connectivity to Slack as more and more people use it, suddenly it's as annoying as your inbox.
[00:14:54.000 --> 00:14:58.000]   It's not a replacement for email. It's as bad as email without the controls.
[00:14:58.000 --> 00:15:03.000]   So what's also going to be extremely important is control for end user control.
[00:15:03.000 --> 00:15:06.000]   Well, and in the same way that if you go ahead, Jeff.
[00:15:06.000 --> 00:15:07.000]   No, no, you ahead.
[00:15:07.000 --> 00:15:14.000]   Oh, I was just going to say in the same way that if you like something on a company on Facebook, then you're kind of like joining their mailing list.
[00:15:14.000 --> 00:15:20.000]   It seems like if you follow a chat bot, you're likely to get notifications from them at all times when you don't want.
[00:15:20.000 --> 00:15:37.000]   And I think that's bad. I think what's really going to be annoying is bad AI because even, you know, so I've got an Alexa and I like it, but the idea that you have to tell it which skills you want it to know as opposed to sort of automatically figuring out here's what I want to do and going to the right service provider.
[00:15:37.000 --> 00:15:49.000]   Like if Amazon hasn't figured that out yet, then most, you know, individual companies are going to write really bad bots that are really brittle and then you're just going to feel your blood pressure rising as you're trying to take your internet of things and get your blood pressure.
[00:15:49.000 --> 00:15:51.000]   You won't know the exact right way to ask for it.
[00:15:51.000 --> 00:15:53.000]   It is an opportunity though for some of you.
[00:15:53.000 --> 00:15:57.000]   We talked about Sam lessons Finn last week.
[00:15:57.000 --> 00:16:03.000]   Sam who did the timeline on Facebook and then left and is now has a startup Finn F.I.N.
[00:16:03.000 --> 00:16:07.000]   dot com, which is an artificial intelligence, which I haven't been able to get in the beta.
[00:16:07.000 --> 00:16:10.000]   Maybe you guys have you can use on Alexa, but you can use it in other ways.
[00:16:10.000 --> 00:16:13.000]   Didn't Jeff didn't you say it's like 150 bucks a month?
[00:16:13.000 --> 00:16:14.000]   Yes, a month.
[00:16:14.000 --> 00:16:16.000]   So maybe that's a month starter.
[00:16:16.000 --> 00:16:24.000]   But if you can't figure out how to make money with a chat bot, a free chat bot, I don't think you're thinking very hard.
[00:16:24.000 --> 00:16:29.000]   I mean, seriously, this is a way to this is a way to coin money.
[00:16:29.000 --> 00:16:41.000]   Ben Evans said today, you know, he's so smart from and Richard Horowitz said that that he thinks bots are ridiculous because they're basically a really bad way to do a web page.
[00:16:41.000 --> 00:16:46.000]   A piece at a time, but he thinks the bots are good versus email.
[00:16:46.000 --> 00:16:51.000]   I think that bots are nothing until they're true agents.
[00:16:51.000 --> 00:16:54.000]   It's all going to come down a good AI, is it?
[00:16:54.000 --> 00:16:56.000]   Yeah.
[00:16:56.000 --> 00:17:07.000]   Yeah, and that's hard and that's rare and there's going to be like at white combinator demo days, there were so many chat bot companies and you know, it just feels like we're back in the 90s.
[00:17:07.000 --> 00:17:19.000]   We've got chat bots and VR and okay, I was there the first time and it wasn't that great, you know, and I'm hoping it's better this time around, but the idea that everybody's going to magically really enjoy talking.
[00:17:19.000 --> 00:17:20.000]   It is.
[00:17:20.000 --> 00:17:26.000]   It's very much like an interactive voice response system like you're on voicemail except you're with a company that can keep talking to you over and over.
[00:17:26.000 --> 00:17:28.000]   I mean, we'll see.
[00:17:28.000 --> 00:17:29.000]   We'll see.
[00:17:29.000 --> 00:17:32.000]   But I'm a little skeptical.
[00:17:32.000 --> 00:17:33.000]   Right.
[00:17:33.000 --> 00:17:41.000]   I mean, it's like, you know, we also the knowledge navigator video in 1987, wherever it was.
[00:17:41.000 --> 00:17:47.000]   Which was John Scully's vision for the future computing and that was that was the guy at the desk.
[00:17:47.000 --> 00:17:49.000]   His knowledge navigator.
[00:17:49.000 --> 00:17:51.000]   Annoying his colleagues at last minute.
[00:17:51.000 --> 00:17:55.000]   If you actually watch that back, we built chunks of that stuff.
[00:17:55.000 --> 00:18:05.000]   But it's still a bit fishy.
[00:18:05.000 --> 00:18:15.000]   In some ways, the closest thing has been when you, if you like the okay Google thing where you say that and ask a question and it will filter through the world and give you something back.
[00:18:15.000 --> 00:18:18.000]   But that's trying to do a very broad domain.
[00:18:18.000 --> 00:18:22.000]   There's obviously you can do things in a narrow domain, but then effects for your learning and command language.
[00:18:22.000 --> 00:18:25.000]   You're basically learning a set of commands that interact with the thing.
[00:18:25.000 --> 00:18:29.000]   And so you've got your back to, you know, command line interfaces again.
[00:18:29.000 --> 00:18:36.000]   Here's the knowledge navigator, the Scully era apple knowledge navigator, which is an AI in a book at a bow tie.
[00:18:36.000 --> 00:18:40.000]   I'm sorry about the quality, but it's from a VHS tape, I think.
[00:18:40.000 --> 00:18:42.000]   And it is extraordinarily annoying.
[00:18:42.000 --> 00:18:43.000]   I mean, it's just.
[00:18:43.000 --> 00:18:44.000]   The Amazon rainforest.
[00:18:44.000 --> 00:18:45.000]   Right.
[00:18:45.000 --> 00:18:48.000]   But it's, it's like a human assisted.
[00:18:48.000 --> 00:18:51.000]   The new lecture notes from last semester.
[00:18:51.000 --> 00:18:53.000]   We're way far away from this.
[00:18:53.000 --> 00:18:54.000]   And this was 1987.
[00:18:54.000 --> 00:18:56.000]   I mean, this was 20 years ago.
[00:18:56.000 --> 00:18:57.000]   No, 30 years ago.
[00:18:57.000 --> 00:18:58.000]   30 years ago.
[00:18:58.000 --> 00:18:59.000]   But this is the point.
[00:18:59.000 --> 00:19:04.000]   This is the vision of the future as given by a Pepsi exec who is having a secretary.
[00:19:04.000 --> 00:19:11.000]   And that's, you know, that's kind of what a lot of these things are.
[00:19:11.000 --> 00:19:16.000]   It's like, oh, I'm used to being this like self important guy who shouts at people and gets stuff done.
[00:19:16.000 --> 00:19:18.000]   And I want the computers to understand that.
[00:19:18.000 --> 00:19:19.000]   Yeah.
[00:19:19.000 --> 00:19:23.000]   And, you know, maybe, maybe we can make things that do a bit of that.
[00:19:23.000 --> 00:19:34.000]   But it's missing, it's missing all the actual work that people who are executive assistants actually do, which is model the world for the boss and, and shield them from it and let things through selectively.
[00:19:34.000 --> 00:19:42.000]   And that's actually a very hard thing to do artificially because it relies on, you know, emotional, emotional knowledge and understanding, which is the thing that we can't model.
[00:19:42.000 --> 00:19:44.000]   So I'm, yeah, I'm skeptical in that way.
[00:19:44.000 --> 00:19:53.000]   One of the things that struck me, Jeff, about Mark's remarks is that in some ways I think he didn't understand the optics.
[00:19:53.000 --> 00:19:55.000]   I've seen Google do the same thing.
[00:19:55.000 --> 00:20:06.000]   And I seem a little tone deaf when he showed off, for instance, his drone, his UAV that looked like a stealth bomber and was called a killer.
[00:20:06.000 --> 00:20:12.000]   It seemed like he didn't, I did immediately.
[00:20:12.000 --> 00:20:15.000]   And I think the rest of the world might be a little bit afraid of Mark.
[00:20:15.000 --> 00:20:24.000]   And their tenure vision did sound a little bit like something out of Skynet with AIs and drones and virtual reality helmets.
[00:20:24.000 --> 00:20:28.000]   I think that a normal person, of course, normal people aren't watching FA.
[00:20:28.000 --> 00:20:31.000]   This is for developers and people like us.
[00:20:31.000 --> 00:20:33.000]   But I wonder how normal people.
[00:20:33.000 --> 00:20:36.000]   Yeah, I mean, I think that's a point.
[00:20:36.000 --> 00:20:44.000]   You know, I said on the show a couple of years ago that Google thinks the world thinks Google is Godzilla and Google thinks it's snuffle-upicus.
[00:20:44.000 --> 00:20:47.000]   And I think the same is true of Facebook.
[00:20:47.000 --> 00:20:50.000]   That, you know, who could be against connecting people?
[00:20:50.000 --> 00:20:54.000]   Who could be against bringing people more knowledge?
[00:20:54.000 --> 00:20:56.000]   Who could be against this?
[00:20:56.000 --> 00:20:57.000]   Well, India was.
[00:20:57.000 --> 00:20:59.000]   Others could be.
[00:20:59.000 --> 00:21:02.000]   So it is the optimism.
[00:21:02.000 --> 00:21:06.000]   It is the fundamental essential optimism of this part of the world.
[00:21:06.000 --> 00:21:07.000]   Yeah.
[00:21:07.000 --> 00:21:10.000]   And that conflicts with the nerves of others.
[00:21:10.000 --> 00:21:23.000]   So, so 30 years ago, we had the Pepsi executive in his isolated, you know, corner office with a personal assistant who would yell through the door at.
[00:21:23.000 --> 00:21:30.000]   Today, we have the guy in the, in the gray T-shirt who was in a different kind of bubble, but just in some ways, just as much of a bubble.
[00:21:30.000 --> 00:21:34.000]   I don't think Mark Zuckerberg is necessarily anymore in touch.
[00:21:34.000 --> 00:21:35.000]   This was scary.
[00:21:35.000 --> 00:21:36.000]   This drone.
[00:21:36.000 --> 00:21:39.000]   This was, this is not what people want.
[00:21:39.000 --> 00:21:42.000]   This is the eye in the sky watching.
[00:21:42.000 --> 00:21:50.000]   It's just like Mark marching down the aisle at Mobile World Congress while people are, you know, obliviously wearing headsets.
[00:21:50.000 --> 00:21:54.000]   I think maybe, maybe it's just me.
[00:21:54.000 --> 00:21:59.000]   Matt, you probably, I mean, Matt, you had to deal with this too, this perception.
[00:21:59.000 --> 00:22:01.000]   And Google's in some ways back down.
[00:22:01.000 --> 00:22:10.000]   I think the sale of Boston Dynamics to some degree must have to do with the fact that these are scary robot dogs and probably better not for Google to be developing them.
[00:22:10.000 --> 00:22:15.000]   I don't know about the on Boston Dynamics, but have you seen the shaft robot video?
[00:22:15.000 --> 00:22:17.000]   It's like a much cuter robot.
[00:22:17.000 --> 00:22:18.000]   Isn't that interesting?
[00:22:18.000 --> 00:22:19.000]   Yes.
[00:22:19.000 --> 00:22:21.000]   That robot would hardly kill me at all.
[00:22:21.000 --> 00:22:22.000]   It was very friendly.
[00:22:22.000 --> 00:22:23.000]   Yes.
[00:22:23.000 --> 00:22:25.000]   I thought that was interesting.
[00:22:25.000 --> 00:22:30.000]   That's a Google, that was their DARPA of winning robot.
[00:22:30.000 --> 00:22:31.000]   Yeah.
[00:22:31.000 --> 00:22:35.000]   Well, I think shaft was Japanese, but there's in the rundown.
[00:22:35.000 --> 00:22:38.000]   There's actually a video of the shaft robot.
[00:22:38.000 --> 00:22:40.000]   Why did I think Google owned the shaft robot?
[00:22:40.000 --> 00:22:41.000]   Maybe I misunderstood.
[00:22:41.000 --> 00:22:43.000]   I think they do.
[00:22:43.000 --> 00:22:44.000]   I think they do.
[00:22:44.000 --> 00:22:45.000]   Oh, okay.
[00:22:45.000 --> 00:22:46.000]   I think they do.
[00:22:46.000 --> 00:22:50.000]   You know, but I, for some reason I thought that it was.
[00:22:50.000 --> 00:22:52.000]   Maybe Jivellic, Japan.
[00:22:52.000 --> 00:22:54.000]   I mean, that doesn't mean Google can't.
[00:22:54.000 --> 00:22:56.000]   Yeah, the keynote was in Tokyo.
[00:22:56.000 --> 00:22:58.000]   That's what the connection was.
[00:22:58.000 --> 00:23:00.000]   Let me find the video so we can show it.
[00:23:00.000 --> 00:23:04.000]   Yeah, it's a much, much friend you're looking for a robot.
[00:23:04.000 --> 00:23:05.000]   Nothing wrong.
[00:23:05.000 --> 00:23:07.000]   Nothing wrong with nice robots.
[00:23:07.000 --> 00:23:11.000]   Put some felt on him and make him more furry.
[00:23:11.000 --> 00:23:16.000]   He needs a nice a la cantrachella.
[00:23:16.000 --> 00:23:18.000]   I can't find it in here.
[00:23:18.000 --> 00:23:19.000]   It's under alphabet.
[00:23:19.000 --> 00:23:20.000]   Oh, there you go.
[00:23:20.000 --> 00:23:21.000]   I'm looking under Google.
[00:23:21.000 --> 00:23:22.000]   My mistake.
[00:23:22.000 --> 00:23:24.000]   It's a whoops alphabet owns.
[00:23:24.000 --> 00:23:25.000]   See, I got it.
[00:23:25.000 --> 00:23:26.000]   This must be hard for you too.
[00:23:26.000 --> 00:23:28.000]   You've been there for a long time to get.
[00:23:28.000 --> 00:23:33.000]   I usually just say Google all the time and people don't worry about it.
[00:23:33.000 --> 00:23:36.000]   People know what you're talking about.
[00:23:36.000 --> 00:23:37.000]   Here is.
[00:23:37.000 --> 00:23:43.000]   Yeah, so it's a bipedal, very kind of cute bipedal robot.
[00:23:43.000 --> 00:23:45.000]   Carrying a bomb, but never mind.
[00:23:45.000 --> 00:23:47.000]   Well, yeah, this is a little creepy too.
[00:23:47.000 --> 00:23:50.000]   I mean, it's not perfectly uncreepy.
[00:23:50.000 --> 00:23:52.000]   Compared to the robot.
[00:23:52.000 --> 00:23:54.000]   How do you manage the creepy line?
[00:23:54.000 --> 00:23:57.000]   What do you expect these people to do with new technologies?
[00:23:57.000 --> 00:23:59.000]   The technology itself is what might.
[00:23:59.000 --> 00:24:01.000]   Well, do you not show it?
[00:24:01.000 --> 00:24:02.000]   No, no, no.
[00:24:02.000 --> 00:24:03.000]   I'll tell you how they do it in movies.
[00:24:03.000 --> 00:24:05.000]   They don't try to make them bipedal.
[00:24:05.000 --> 00:24:09.000]   The more you make it look like a human, the creepier it is.
[00:24:09.000 --> 00:24:14.000]   So I don't, I really don't understand this kind of motivation to make robots human-like.
[00:24:14.000 --> 00:24:19.000]   I understand that some robots, a rescue robot needs to be able to go through a door and go upstairs and stuff like that.
[00:24:19.000 --> 00:24:21.000]   This robot's bringing you food.
[00:24:21.000 --> 00:24:22.000]   Right.
[00:24:22.000 --> 00:24:23.000]   I don't know.
[00:24:23.000 --> 00:24:27.000]   But the point is that, you know, we built a world around us.
[00:24:27.000 --> 00:24:32.000]   So, you know, there are staircases and things that are hard to deal with if you've got wheels.
[00:24:32.000 --> 00:24:37.000]   And you think, I suppose you could just fly in with a drone, but that's creeping it a different way.
[00:24:37.000 --> 00:24:38.000]   Yeah.
[00:24:38.000 --> 00:24:40.000]   The hotel robot's not creepy, right?
[00:24:40.000 --> 00:24:41.000]   Right.
[00:24:41.000 --> 00:24:47.000]   But it goes in elevators and it just is like a little post office box, a little mailbox.
[00:24:47.000 --> 00:24:49.000]   Well, how do they do hospitals?
[00:24:49.000 --> 00:24:50.000]   Yeah.
[00:24:50.000 --> 00:24:51.000]   Yeah.
[00:24:51.000 --> 00:24:52.000]   It's not.
[00:24:52.000 --> 00:24:56.000]   Oh, you know, those are go-wizzing around hospitals to everything pills to people.
[00:24:56.000 --> 00:24:57.000]   Yeah.
[00:24:57.000 --> 00:24:58.000]   And they just, they're right.
[00:24:58.000 --> 00:24:59.000]   They look like mailboxes and they use elevators and-
[00:24:59.000 --> 00:25:01.000]   I think there's a reason they look like that.
[00:25:01.000 --> 00:25:06.000]   They're not bipedal and they don't have faces and go, "Here are your pills, sir."
[00:25:06.000 --> 00:25:07.000]   That would not be-
[00:25:07.000 --> 00:25:10.000]   I will watch you for compliance.
[00:25:10.000 --> 00:25:11.000]   Yes.
[00:25:11.000 --> 00:25:12.000]   You must take it.
[00:25:12.000 --> 00:25:13.000]   That's in humans, right?
[00:25:13.000 --> 00:25:14.000]   Isn't that in humans?
[00:25:14.000 --> 00:25:19.760]   I love that in humans they have these android nurses and they exactly say, "You've not been
[00:25:19.760 --> 00:25:21.880]   taking your pills.
[00:25:21.880 --> 00:25:23.880]   You naughty boy."
[00:25:23.880 --> 00:25:27.000]   That's ratchet robot, right?
[00:25:27.000 --> 00:25:30.000]   Yeah, it's kind of like a nurse ratchet robot.
[00:25:30.000 --> 00:25:31.000]   What else?
[00:25:31.000 --> 00:25:33.000]   Let's see, virtual reality, the Oculus Rift.
[00:25:33.000 --> 00:25:38.960]   Of course, you know, Facebook owns Oculus Rift, but Oculus is going to see some tough competition.
[00:25:38.960 --> 00:25:41.080]   The Vive is coming out right now.
[00:25:41.080 --> 00:25:44.920]   People are saying in many ways it's better, it at least has the paddles that the Oculus
[00:25:44.920 --> 00:25:47.560]   Rift is delayed on.
[00:25:47.560 --> 00:25:55.000]   The move, PlayStation VR is going to be coming out soon too.
[00:25:55.000 --> 00:26:01.640]   I feel like though, what's interesting to me on this, both on messaging and on VR is how
[00:26:01.640 --> 00:26:05.680]   seemingly far behind Google and Apple are on this.
[00:26:05.680 --> 00:26:06.680]   Yes.
[00:26:06.680 --> 00:26:10.480]   That's why I find that the catch-up games that are going on here in various areas are
[00:26:10.480 --> 00:26:13.280]   interesting to me and then that's one.
[00:26:13.280 --> 00:26:14.280]   Yeah.
[00:26:14.280 --> 00:26:18.560]   I think live video is Google's also behind on and it's going to catch up.
[00:26:18.560 --> 00:26:19.560]   Google should own that.
[00:26:19.560 --> 00:26:26.160]   I mean, Google really, they had the jump camera, that's basically what Mark has copied a year
[00:26:26.160 --> 00:26:27.160]   ago.
[00:26:27.160 --> 00:26:29.360]   But his has 17 cameras, not 16.
[00:26:29.360 --> 00:26:30.360]   One better.
[00:26:30.360 --> 00:26:31.360]   It's one more.
[00:26:31.360 --> 00:26:32.360]   It's one more.
[00:26:32.360 --> 00:26:33.360]   It's one more.
[00:26:33.360 --> 00:26:34.360]   It's one more.
[00:26:34.360 --> 00:26:36.360]   That's better, right?
[00:26:36.360 --> 00:26:37.360]   Sure.
[00:26:37.360 --> 00:26:41.240]   It's just one better.
[00:26:41.240 --> 00:26:46.960]   It is interesting to me that thinking about Google, I recently burned like four or five
[00:26:46.960 --> 00:26:51.000]   nights watching the premium shows that YouTube Red has.
[00:26:51.000 --> 00:26:57.920]   So I think there was something where Google or YouTube wanted to spend some resources
[00:26:57.920 --> 00:26:59.720]   getting a good subscription product.
[00:26:59.720 --> 00:27:04.120]   So you subscribe to YouTube Red and I watch Scare PewDiePie and Frank Academy and there's
[00:27:04.120 --> 00:27:06.120]   actually pretty entertaining.
[00:27:06.120 --> 00:27:07.520]   I like more than I expected to be.
[00:27:07.520 --> 00:27:11.880]   My kids watching Scare PewDiePie, I didn't expect him to.
[00:27:11.880 --> 00:27:17.080]   My initial reaction is, oh, this is old media trying to imagine what young kids want to
[00:27:17.080 --> 00:27:18.080]   watch.
[00:27:18.080 --> 00:27:19.280]   It's the Fred Figglesworth model.
[00:27:19.280 --> 00:27:21.120]   You know, he was a huge YouTube star.
[00:27:21.120 --> 00:27:26.560]   But the minute he was on Nickelodeon making movies, it was like, yeah.
[00:27:26.560 --> 00:27:28.440]   But there's actually a little bit of a story arc.
[00:27:28.440 --> 00:27:30.280]   Like they play a trick on PewDiePie.
[00:27:30.280 --> 00:27:31.280]   Yeah.
[00:27:31.280 --> 00:27:32.280]   It plays out all through the season.
[00:27:32.280 --> 00:27:33.280]   No, it's funny.
[00:27:33.280 --> 00:27:34.280]   Yeah.
[00:27:34.280 --> 00:27:35.280]   So it kind of, that kind of surprised me.
[00:27:35.280 --> 00:27:39.400]   It's always easy to say, oh, you know, YouTube should have done live video or YouTube should
[00:27:39.400 --> 00:27:40.400]   do this.
[00:27:40.400 --> 00:27:45.280]   But I do think if they could get a base of subscription subscribers where that can support,
[00:27:45.280 --> 00:27:49.080]   you know, creators on YouTube, you know, that's a pretty good thing to work on.
[00:27:49.080 --> 00:27:52.200]   And you'll always have people saying, oh, YouTube should have done Periscope First
[00:27:52.200 --> 00:27:54.640]   or Meerkat First or live video first.
[00:27:54.640 --> 00:27:56.040]   And that's fair criticism.
[00:27:56.040 --> 00:28:01.000]   But I do think I didn't realize how good the premium shows on YouTube red were.
[00:28:01.000 --> 00:28:04.160]   And that sort of makes me say, okay, well, yeah.
[00:28:04.160 --> 00:28:06.280]   For example, take the chat stuff.
[00:28:06.280 --> 00:28:11.200]   You know, probably getting the artificial intelligence machine learning down is a little
[00:28:11.200 --> 00:28:16.200]   harder than the chat or the app and getting that exactly right.
[00:28:16.200 --> 00:28:21.280]   And so, you know, maybe we haven't invested all of the resources, you know, completely
[00:28:21.280 --> 00:28:22.280]   optimally.
[00:28:22.280 --> 00:28:26.600]   But I do feel like Google now and the artificial intelligence aspect of it are pretty well
[00:28:26.600 --> 00:28:30.920]   positioned if we can then find an app or a way of communicating that people really like
[00:28:30.920 --> 00:28:33.440]   and can sort of come together on.
[00:28:33.440 --> 00:28:39.760]   It's very, that's very astute information, very guidance.
[00:28:39.760 --> 00:28:43.360]   I guess we have to use that financial term, the guidance from.
[00:28:43.360 --> 00:28:46.640]   Well, this is all just my personal opinion looking from the outside.
[00:28:46.640 --> 00:28:48.800]   Yeah, I know outside outside.
[00:28:48.800 --> 00:28:53.000]   But that makes sense because Hangouts feels like it's been kind of left to drift, but
[00:28:53.000 --> 00:28:54.280]   you're absolutely right.
[00:28:54.280 --> 00:28:56.280]   That's an easy thing to play catch up with.
[00:28:56.280 --> 00:28:57.840]   That's trivial.
[00:28:57.840 --> 00:29:03.080]   But the hard thing is the AI is the and Google now is way ahead of anything anybody else
[00:29:03.080 --> 00:29:04.080]   is doing.
[00:29:04.080 --> 00:29:05.080]   We're pretty good shape on AI.
[00:29:05.080 --> 00:29:09.800]   Now, to be fair, like getting the network effects, like it's hard to try to come back
[00:29:09.800 --> 00:29:12.200]   from behind once everybody's on one platform.
[00:29:12.200 --> 00:29:13.200]   That is true.
[00:29:13.200 --> 00:29:15.160]   So that is tricky.
[00:29:15.160 --> 00:29:21.160]   But, you know, I think they've got good people working on all the aspects of the problem.
[00:29:21.160 --> 00:29:25.080]   So I'm sure they do.
[00:29:25.080 --> 00:29:29.440]   Yeah, I feel like I want to.
[00:29:29.440 --> 00:29:33.720]   So Apple's going to have trouble because right now they're not cross platform.
[00:29:33.720 --> 00:29:38.160]   And even if they made a cross platform messages, it's really an iOS thing.
[00:29:38.160 --> 00:29:41.720]   And so that's going to, Facebook really just seems to have the inside track just because
[00:29:41.720 --> 00:29:44.040]   everybody's on Facebook.
[00:29:44.040 --> 00:29:48.680]   And you know, the big thing that happened to me was a few weeks ago, I guess this was
[00:29:48.680 --> 00:29:55.400]   part of Marshmallow Android 6, but all of a sudden instead of just using Google's messaging
[00:29:55.400 --> 00:30:02.920]   app or the Samsung SMS app or Hangouts, I could use Facebook Messenger said, "Oh, you want
[00:30:02.920 --> 00:30:04.240]   me to be your SMS app?
[00:30:04.240 --> 00:30:05.640]   I could do that too."
[00:30:05.640 --> 00:30:07.600]   And then others did.
[00:30:07.600 --> 00:30:09.880]   And so now I have many choices.
[00:30:09.880 --> 00:30:13.880]   And I'm actually trying Facebook Messenger as my SMS app.
[00:30:13.880 --> 00:30:19.040]   And because it's Facebook, it has, you know, a lot of features that are superior.
[00:30:19.040 --> 00:30:24.720]   So you can choose, this is a Galaxy S7, but if you go into default apps, right now I
[00:30:24.720 --> 00:30:30.800]   can choose from a variety of them, including Signal, which is the super secret encrypted
[00:30:30.800 --> 00:30:32.800]   one.
[00:30:32.800 --> 00:30:36.800]   And I don't know why Telegram isn't in there, but that's a big change.
[00:30:36.800 --> 00:30:42.120]   And so if I use Messenger as my app, now Facebook messaging and SMS messaging is all
[00:30:42.120 --> 00:30:45.000]   going to the same one, that's a huge advantage.
[00:30:45.000 --> 00:30:46.000]   That is.
[00:30:46.000 --> 00:30:50.080]   And Google kind of did a weird thing when I got Project 5, they said, "Oh, please don't
[00:30:50.080 --> 00:30:51.800]   use Hangouts.
[00:30:51.800 --> 00:30:52.960]   Use the messaging app."
[00:30:52.960 --> 00:30:54.440]   Maybe that was carriers.
[00:30:54.440 --> 00:30:57.120]   But it might have been carriers enforcing that one.
[00:30:57.120 --> 00:31:00.480]   Well, Google's got three apps that do it, which is really confusing.
[00:31:00.480 --> 00:31:03.160]   So the carriers want you to stick with SMS, right?
[00:31:03.160 --> 00:31:05.000]   They don't want you to use data.
[00:31:05.000 --> 00:31:11.920]   Well, there's the AOSP app, then there's Messenger, which is Google's one that's confusing when
[00:31:11.920 --> 00:31:13.920]   you search for Facebook Messenger.
[00:31:13.920 --> 00:31:15.920]   And then there's Hangouts.
[00:31:15.920 --> 00:31:18.280]   And then there's still Google Voice looking in there somewhere.
[00:31:18.280 --> 00:31:19.720]   And they turned off the widget.
[00:31:19.720 --> 00:31:23.920]   They turned off the Hangouts widget, which was another way of nudging you out of the Hangouts
[00:31:23.920 --> 00:31:28.720]   eco sphere at exactly the time where if I were them and not knowing everything about
[00:31:28.720 --> 00:31:33.400]   what's going on inside, I would have said, "Let's get more people to use our platform."
[00:31:33.400 --> 00:31:36.620]   Yeah, I think that's fair criticism.
[00:31:36.620 --> 00:31:37.620]   Yeah.
[00:31:37.620 --> 00:31:38.620]   Things got muddled.
[00:31:38.620 --> 00:31:39.620]   Yeah, I would agree with that.
[00:31:39.620 --> 00:31:40.620]   Yeah.
[00:31:40.620 --> 00:31:41.620]   Yep.
[00:31:41.620 --> 00:31:42.620]   Yeah.
[00:31:42.620 --> 00:31:48.560]   Apple's going to miss out no matter what happens because they want…they doubled down on privacy,
[00:31:48.560 --> 00:31:50.200]   security.
[00:31:50.200 --> 00:31:53.040]   They want to keep you in the Apple ecosystem.
[00:31:53.040 --> 00:31:56.080]   There's a strategy text for that, and that's the strategy text.
[00:31:56.080 --> 00:31:59.440]   Facebook's in a really good position, especially since they also own WhatsApp.
[00:31:59.440 --> 00:32:01.840]   So they really have global dominance.
[00:32:01.840 --> 00:32:05.720]   But Google can be in there, and Amazon's a dark horse because no matter what you say,
[00:32:05.720 --> 00:32:09.600]   I think Echo is a messaging platform.
[00:32:09.600 --> 00:32:13.760]   And it's just a matter of time before they start integrating some of these skills, which
[00:32:13.760 --> 00:32:16.720]   you're right, are kind of ghettoized right now.
[00:32:16.720 --> 00:32:17.720]   They're kind of separate.
[00:32:17.720 --> 00:32:21.760]   You have to turn them on, and you have to use this weird syntax to get to this skill.
[00:32:21.760 --> 00:32:28.040]   But that's just…I think that's probably more just kind of a Darwinian selection process.
[00:32:28.040 --> 00:32:32.840]   And eventually, I would imagine Amazon will kind of unify that platform.
[00:32:32.840 --> 00:32:37.200]   Well, that's my theory on the Google versus Alexa thing is that they need to make one
[00:32:37.200 --> 00:32:40.720]   of those, but one that integrates with…
[00:32:40.720 --> 00:32:41.720]   Now.
[00:32:41.720 --> 00:32:42.720]   With now.
[00:32:42.720 --> 00:32:45.600]   But now is the obvious thing, but also it needs to integrate with nearby screens.
[00:32:45.600 --> 00:32:48.200]   So it needs to be able to put stuff on the screen here.
[00:32:48.200 --> 00:32:49.200]   Yeah.
[00:32:49.200 --> 00:32:52.400]   So you can say Google Box, or let's say, "Hawn Hub."
[00:32:52.400 --> 00:32:57.320]   On Hub, a play Game of Thrones for me on my living room TV.
[00:32:57.320 --> 00:33:00.160]   And I can do that with my iPhone.
[00:33:00.160 --> 00:33:02.160]   I can say, "Watch."
[00:33:02.160 --> 00:33:03.680]   I can say, "Play this."
[00:33:03.680 --> 00:33:07.480]   And I've got the Chromecast and it can go where to send it.
[00:33:07.480 --> 00:33:11.520]   The thing that the Alexa can do is know where in the room you are, because it's got a microphone
[00:33:11.520 --> 00:33:12.520]   array.
[00:33:12.520 --> 00:33:15.120]   So it can actually, if you've got an open plan house like I have and there's two TVs
[00:33:15.120 --> 00:33:19.000]   that are available, it can pick which one you're near to and make that work.
[00:33:19.000 --> 00:33:20.000]   So that's…
[00:33:20.000 --> 00:33:25.080]   The thing that you get by having that physical object that has six or seven microphones in
[00:33:25.080 --> 00:33:28.120]   it is you actually get to triangulate on people.
[00:33:28.120 --> 00:33:29.120]   So that…
[00:33:29.120 --> 00:33:32.880]   I think that if you remember what was it called?
[00:33:32.880 --> 00:33:33.880]   Oblong.
[00:33:33.880 --> 00:33:36.640]   Do you remember Oblong?
[00:33:36.640 --> 00:33:42.760]   They were the company that did the stuff for the…
[00:33:42.760 --> 00:33:45.640]   What was that movie?
[00:33:45.640 --> 00:33:46.640]   Oblong.
[00:33:46.640 --> 00:33:53.760]   So basically, what Oblong did was they would let you attach gadgets to TV sets in a room
[00:33:53.760 --> 00:33:57.360]   and have a pointing device and say, "Put that stuff there and put stuff there."
[00:33:57.360 --> 00:34:00.480]   And have control open, like do a presentation.
[00:34:00.480 --> 00:34:03.800]   So they were saying this to large companies as a…
[00:34:03.800 --> 00:34:07.120]   In your war room, you need a bunch of screens and we can control all the screens and you
[00:34:07.120 --> 00:34:09.840]   can move stuff back and forth.
[00:34:09.840 --> 00:34:12.280]   And I've played with it and had demos of it.
[00:34:12.280 --> 00:34:13.280]   It's cool tech.
[00:34:13.280 --> 00:34:15.840]   It was sort of pro-prosture and required.
[00:34:15.840 --> 00:34:18.200]   You spend a lot of money with them.
[00:34:18.200 --> 00:34:23.000]   But we're sort of walking to war with having that ability in the home as we gradually have
[00:34:23.000 --> 00:34:29.080]   a bunch of screens and we put a little $35 widget in the back of every screen.
[00:34:29.080 --> 00:34:33.880]   And Apple could do this certainly because they've got Apple TV as well.
[00:34:33.880 --> 00:34:37.680]   But Google's have this ability and have it cheaper.
[00:34:37.680 --> 00:34:41.080]   So it's easier to stick a bunch of Chrome cards and things.
[00:34:41.080 --> 00:34:45.500]   And the Chrome class will do the thing of switch to that channel and show you the show if you
[00:34:45.500 --> 00:34:47.200]   plug it in correctly.
[00:34:47.200 --> 00:34:51.240]   Which means you can do that thing of, "I want to order some flowers."
[00:34:51.240 --> 00:34:54.600]   And it would say, "Turn the screen on and say which one of these do you want?"
[00:34:54.600 --> 00:34:58.080]   And show you that visual interface as well.
[00:34:58.080 --> 00:35:01.640]   So there's a bunch of interesting bits and pieces to be worked out there.
[00:35:01.640 --> 00:35:03.880]   And it does involve having lots of stuff in the house.
[00:35:03.880 --> 00:35:07.400]   But I think that's a natural next step for a lot of these things.
[00:35:07.400 --> 00:35:13.880]   And it's the back and forth between the spoken, stroke, typed query and the richer visual
[00:35:13.880 --> 00:35:16.920]   response that's going to be the next iteration of this stuff.
[00:35:16.920 --> 00:35:18.880]   Facebook's saying we're very serious about that.
[00:35:18.880 --> 00:35:24.240]   They just hired away Alphabet's head, Regina Dugan, head of the Advanced Technology and
[00:35:24.240 --> 00:35:25.240]   Products Group.
[00:35:25.240 --> 00:35:28.200]   Actually, she was at Google.
[00:35:28.200 --> 00:35:31.800]   Before that, she was a director of DARPA.
[00:35:31.800 --> 00:35:41.200]   She is going to come to Facebook with a $100 million plus budget to do R&D in Building 8.
[00:35:41.200 --> 00:35:42.200]   Zuckerberg says hundreds.
[00:35:42.200 --> 00:35:44.040]   That's not about consumer products, right?
[00:35:44.040 --> 00:35:47.400]   That's about big advanced research.
[00:35:47.400 --> 00:35:48.400]   It's about the drones.
[00:35:48.400 --> 00:35:49.400]   What is it?
[00:35:49.400 --> 00:35:50.400]   A kila?
[00:35:50.400 --> 00:35:51.400]   A kila?
[00:35:51.400 --> 00:35:54.400]   Two lanes of Facebook.
[00:35:54.400 --> 00:35:59.680]   We have a drone to fly over the campus.
[00:35:59.680 --> 00:36:01.200]   So this isn't consumer facing.
[00:36:01.200 --> 00:36:03.200]   This is like what?
[00:36:03.200 --> 00:36:04.200]   I don't know.
[00:36:04.200 --> 00:36:05.200]   I'm asking.
[00:36:05.200 --> 00:36:08.000]   Is that -- she's not going to be picking phones, is she?
[00:36:08.000 --> 00:36:09.000]   Well, yes.
[00:36:09.000 --> 00:36:10.400]   ATAP was doing Project ARRA.
[00:36:10.400 --> 00:36:16.080]   I doubt very much she's going to be doing modular phones for Facebook.
[00:36:16.080 --> 00:36:17.880]   But she certainly knows about a lot more than that.
[00:36:17.880 --> 00:36:21.280]   I mean DARPA was about very big ideas, right?
[00:36:21.280 --> 00:36:22.760]   Yeah.
[00:36:22.760 --> 00:36:25.720]   And I suspect AI.
[00:36:25.720 --> 00:36:29.720]   According to Bloomberg Technology, she's going to direct efforts to build hardware that
[00:36:29.720 --> 00:36:36.680]   uses Facebook software and complements technologies from Oculus VR and the company's artificial intelligence
[00:36:36.680 --> 00:36:39.000]   research unit.
[00:36:39.000 --> 00:36:42.360]   So AI VR?
[00:36:42.360 --> 00:36:46.120]   It's like the guy at Hoolie who got promoted up.
[00:36:46.120 --> 00:36:48.520]   No, it's not.
[00:36:48.520 --> 00:36:49.520]   Come on.
[00:36:49.520 --> 00:36:51.320]   Do you know Regina, Matt?
[00:36:51.320 --> 00:36:52.680]   Do you know her?
[00:36:52.680 --> 00:36:55.880]   I don't know her personally, but I think Regina is super smart.
[00:36:55.880 --> 00:36:59.720]   And the article says she's going to be building things that are mission aligned.
[00:36:59.720 --> 00:37:06.160]   And certainly stuff like Tango, things like Johnny Lee had worked on where he had worked
[00:37:06.160 --> 00:37:08.320]   on the Connect before that.
[00:37:08.320 --> 00:37:10.600]   So there's some really neat stuff that happens at ATAP.
[00:37:10.600 --> 00:37:13.880]   They've got radar where you can go like this to scroll stuff.
[00:37:13.880 --> 00:37:18.320]   So I think if she's working on interesting hardware that aligns with things that Facebook
[00:37:18.320 --> 00:37:20.880]   wants to do, I think that'll be really interesting.
[00:37:20.880 --> 00:37:25.240]   You know, it's always said when somebody leaves Google, the idea that somebody would put hundreds
[00:37:25.240 --> 00:37:28.840]   of millions of dollars, hundreds of people and hundreds of millions of dollars into
[00:37:28.840 --> 00:37:32.880]   this effort over time, I can see the appeal of that to her.
[00:37:32.880 --> 00:37:35.600]   So we'll see what happens over time.
[00:37:35.600 --> 00:37:42.280]   Such a huge advantage to have as Facebook and Apple and Google do this just massive,
[00:37:42.280 --> 00:37:48.680]   unbelievably massive bankroll to do this kind of blue sky R&D.
[00:37:48.680 --> 00:37:51.800]   It's pretty exciting.
[00:37:51.800 --> 00:37:53.640]   Oh, yeah.
[00:37:53.640 --> 00:37:54.640]   Yeah.
[00:37:54.640 --> 00:37:59.040]   It's good to have all these ideas coming out and people trying out a bunch of new things,
[00:37:59.040 --> 00:38:05.080]   whether they be satellites or drones or airplanes or closer to the home kind of tech.
[00:38:05.080 --> 00:38:09.120]   But it's nice that people aren't just resting on their laurels.
[00:38:09.120 --> 00:38:13.360]   I do worry a little bit about whether some of the chatbot stuff is just a step back and
[00:38:13.360 --> 00:38:15.760]   everybody's going to wait in a year from now.
[00:38:15.760 --> 00:38:17.200]   They'll be like, well, that was a fad.
[00:38:17.200 --> 00:38:19.200]   What were we thinking?
[00:38:19.200 --> 00:38:20.200]   Right.
[00:38:20.200 --> 00:38:21.760]   We'll see what happens there.
[00:38:21.760 --> 00:38:23.120]   Well, you have to do that though, right?
[00:38:23.120 --> 00:38:28.200]   I mean, I think it's still possible that could be the case for VR, for instance, despite
[00:38:28.200 --> 00:38:30.200]   all the money and publicity it's been getting.
[00:38:30.200 --> 00:38:34.520]   And I'm having a lot of fun playing with binoculars rift, but I don't know if the fun
[00:38:34.520 --> 00:38:39.080]   translates into something more lasting or if it's just, you know, like a steering wheel
[00:38:39.080 --> 00:38:41.360]   for your driving game, I mean, it makes it more realistic.
[00:38:41.360 --> 00:38:42.360]   I don't know.
[00:38:42.360 --> 00:38:43.360]   It's hard to tell.
[00:38:43.360 --> 00:38:44.360]   Right.
[00:38:44.360 --> 00:38:45.360]   So actually work on--
[00:38:45.360 --> 00:38:51.240]   Today, here with social VR where you play with somebody else and then you give each
[00:38:51.240 --> 00:38:56.720]   other images to step into and you create each other's avatars and you can have an
[00:38:56.720 --> 00:38:58.040]   interaction with each other's avatars.
[00:38:58.040 --> 00:39:05.680]   And they even had, they showed off today an Oculus Rift virtual selfie stick where you're
[00:39:05.680 --> 00:39:09.280]   in the same environment together and the two avatars come together and take a picture
[00:39:09.280 --> 00:39:12.880]   of themselves and then they put it in a little box and it went right on to Facebook.
[00:39:12.880 --> 00:39:16.920]   I mean, it's fun stuff to think about VR as social, but you're right.
[00:39:16.920 --> 00:39:17.920]   I don't know.
[00:39:17.920 --> 00:39:20.120]   Well, they're an eventual Westminster bridge or something.
[00:39:20.120 --> 00:39:21.120]   That was the bit that was--
[00:39:21.120 --> 00:39:22.280]   Yeah, there it is.
[00:39:22.280 --> 00:39:25.280]   But what's weird is-- well, not weird, but what's disappointing is that you're not going
[00:39:25.280 --> 00:39:31.080]   to be able to do this for some time because they're way delayed on these paddles September
[00:39:31.080 --> 00:39:32.080]   at the earliest.
[00:39:32.080 --> 00:39:35.360]   So all of these things are cool, but it's not anything anybody can do except in the
[00:39:35.360 --> 00:39:37.720]   Facebook labs.
[00:39:37.720 --> 00:39:42.560]   Part of the problem for me at least is I actually worked on virtual reality stuff back in grad
[00:39:42.560 --> 00:39:44.320]   school in the late '90s.
[00:39:44.320 --> 00:39:49.600]   And, you know, like 70% of all virtual reality demos are like, "Okay, we're going to put
[00:39:49.600 --> 00:39:52.680]   two people in a space and they're going to look at a computer part."
[00:39:52.680 --> 00:39:56.280]   And we're like, "Oh, we need to change that computer parts tiny little parametric thing
[00:39:56.280 --> 00:39:57.280]   here.
[00:39:57.280 --> 00:39:58.600]   Oh, now everything's ready to go.
[00:39:58.600 --> 00:40:00.440]   Send it off to the 3D printer."
[00:40:00.440 --> 00:40:05.000]   And if you look more closely, like every single one of these demos is like, "Why couldn't
[00:40:05.000 --> 00:40:07.480]   they have done this in person or over a video conference?"
[00:40:07.480 --> 00:40:08.480]   Yeah.
[00:40:08.480 --> 00:40:11.880]   And the idea that they're actually going to change the design of some CAD part together
[00:40:11.880 --> 00:40:12.880]   in a virtual space.
[00:40:12.880 --> 00:40:17.400]   I'm just allergic to that demo and that demo happens a lot with this sort of technology.
[00:40:17.400 --> 00:40:18.400]   Yeah.
[00:40:18.400 --> 00:40:22.960]   Well, that bizarre Microsoft one where they put his daughter in the room with him, except
[00:40:22.960 --> 00:40:24.920]   she couldn't see him.
[00:40:24.920 --> 00:40:27.360]   And when they did put the guy who couldn't see him and then all you could see was each
[00:40:27.360 --> 00:40:28.360]   other's goggles.
[00:40:28.360 --> 00:40:29.520]   So he couldn't actually make eye contact.
[00:40:29.520 --> 00:40:30.520]   It's like, "Ah!"
[00:40:30.520 --> 00:40:33.840]   You're missing the point in a painful way here.
[00:40:33.840 --> 00:40:36.360]   Well, it's good for gaming.
[00:40:36.360 --> 00:40:37.360]   Yeah.
[00:40:37.360 --> 00:40:38.360]   Entertainment.
[00:40:38.360 --> 00:40:39.360]   Absolutely.
[00:40:39.360 --> 00:40:43.720]   So I have an app for this, which was next to our conversation before the show, which
[00:40:43.720 --> 00:40:47.360]   is the thing that actually makes sense for this is theater.
[00:40:47.360 --> 00:40:49.160]   My son puts his play on in London.
[00:40:49.160 --> 00:40:53.840]   I'd like to see that Facebook, Google camera in the front row and watch it from there,
[00:40:53.840 --> 00:40:58.120]   rather than have someone filming on their phone from the front row and not seeing all
[00:40:58.120 --> 00:40:59.120]   the action.
[00:40:59.120 --> 00:41:03.400]   Because you're either too far back and you can't see what's going on, or you're too
[00:41:03.400 --> 00:41:07.320]   far forward and then they've got a panel over the place to try and follow the action.
[00:41:07.320 --> 00:41:14.000]   Whereas, to actually, to actually establish a property takes six cameras in the director
[00:41:14.000 --> 00:41:16.040]   and then a bunch of editing afterwards.
[00:41:16.040 --> 00:41:19.480]   Whereas, if you could just stick one of those things in one of the seats, that would actually
[00:41:19.480 --> 00:41:21.840]   be a useful way of getting the experience.
[00:41:21.840 --> 00:41:23.560]   So I think that's the thing that makes sense.
[00:41:23.560 --> 00:41:28.800]   It also makes sense in that you can't edit in VR.
[00:41:28.800 --> 00:41:32.600]   As soon as you can't, you've got to wait five, ten seconds for somebody to work out what
[00:41:32.600 --> 00:41:33.960]   happened and where they are.
[00:41:33.960 --> 00:41:34.960]   You can't do montage.
[00:41:34.960 --> 00:41:36.640]   You can't cut through stuff.
[00:41:36.640 --> 00:41:42.240]   So you either have to direct it like Birdman, or you have to sit the camera in one place
[00:41:42.240 --> 00:41:43.840]   and have it act around you.
[00:41:43.840 --> 00:41:47.000]   And that's where the theatre stuff makes sense.
[00:41:47.000 --> 00:41:51.600]   In some ways, it makes more sense for this kind of small room experimental in the round
[00:41:51.600 --> 00:41:56.120]   theatre than it does for the proscenium arch style theatre.
[00:41:56.120 --> 00:42:00.160]   So I think that there's a set of interesting things that will happen there.
[00:42:00.160 --> 00:42:03.560]   But it's going to be hard for people who used to direct in video because all the grammar
[00:42:03.560 --> 00:42:05.400]   used to do video doesn't work.
[00:42:05.400 --> 00:42:12.840]   So I tried out the Jungle Book demo for VR, which, do you see that one?
[00:42:12.840 --> 00:42:13.840]   No.
[00:42:13.840 --> 00:42:14.840]   So they think...
[00:42:14.840 --> 00:42:15.840]   This is un-arkulous.
[00:42:15.840 --> 00:42:16.840]   Was it obviously?
[00:42:16.840 --> 00:42:19.080]   No, it was one of the other ones.
[00:42:19.080 --> 00:42:20.080]   I think it was...
[00:42:20.080 --> 00:42:21.080]   It would be fine.
[00:42:21.080 --> 00:42:22.080]   Samsung VR thing.
[00:42:22.080 --> 00:42:23.080]   Okay.
[00:42:23.080 --> 00:42:25.400]   I'll have that, but I could try that.
[00:42:25.400 --> 00:42:29.520]   But they said you go into the room and you're in the room where Louis is and then Baloo comes
[00:42:29.520 --> 00:42:30.520]   in.
[00:42:30.520 --> 00:42:33.680]   The problem is I could hear Louis speaking, but I didn't know where he was.
[00:42:33.680 --> 00:42:36.000]   I thought this little monkey in front of me was him.
[00:42:36.000 --> 00:42:39.040]   Then I turned around and this very large monkey behind was the one who was actually talking
[00:42:39.040 --> 00:42:40.040]   to me.
[00:42:40.040 --> 00:42:43.200]   And it's like the mis-ons...
[00:42:43.200 --> 00:42:45.760]   The audio wasn't good enough to give me a cue of which direction I was supposed to
[00:42:45.760 --> 00:42:46.760]   be looking.
[00:42:46.760 --> 00:42:51.080]   So I missed like a big chunk of the action here.
[00:42:51.080 --> 00:42:52.080]   What...
[00:42:52.080 --> 00:42:56.040]   Matt, you said you did this kind of research in college.
[00:42:56.040 --> 00:42:57.040]   How does...
[00:42:57.040 --> 00:42:58.040]   What we're seeing today...
[00:42:58.040 --> 00:43:01.400]   It seems to me that with the exception of the headsets, there's smaller and lighter
[00:43:01.400 --> 00:43:07.200]   and they'll have a big cord coming off the back to a Silicon Graphics workstation that
[00:43:07.200 --> 00:43:09.560]   what we're seeing isn't so far different from the stuff.
[00:43:09.560 --> 00:43:12.720]   I remember trying at SIGGRAPH in the early '90s.
[00:43:12.720 --> 00:43:15.760]   Well, I don't want to be...
[00:43:15.760 --> 00:43:16.760]   I don't know.
[00:43:16.760 --> 00:43:17.760]   I don't want to be the skeptic.
[00:43:17.760 --> 00:43:18.760]   It is true.
[00:43:18.760 --> 00:43:19.760]   You get more polygons.
[00:43:19.760 --> 00:43:20.760]   You get better rendering.
[00:43:20.760 --> 00:43:21.760]   You have better resolution.
[00:43:21.760 --> 00:43:26.240]   But we had a ceiling that would light up and you'd use a Coleman filter and it would
[00:43:26.240 --> 00:43:28.480]   do 20,000 projections per second.
[00:43:28.480 --> 00:43:30.680]   So it had very low latency.
[00:43:30.680 --> 00:43:34.640]   What is new is you have better inertial sensors, better accelerometers.
[00:43:34.640 --> 00:43:37.040]   Cell phones have driven the price of those way down.
[00:43:37.040 --> 00:43:39.200]   So you can do this stuff self-contained.
[00:43:39.200 --> 00:43:44.960]   But unless you really dedicate your hardware in very specific ways, the latency is bad,
[00:43:44.960 --> 00:43:45.960]   then people get sick.
[00:43:45.960 --> 00:43:47.760]   You can't register in the augmented reality.
[00:43:47.760 --> 00:43:52.480]   And I really don't think most people have actually solved this problem yet, which is...
[00:43:52.480 --> 00:43:53.480]   No interesting.
[00:43:53.480 --> 00:43:57.880]   Maybe some of these dedicated hardware is will, but if you just slap a phone in front
[00:43:57.880 --> 00:44:03.160]   of your face or if you just use phone hardware without specifically trying to make sure
[00:44:03.160 --> 00:44:07.440]   the latency is good, it's going to be fun for 10 minutes, but people aren't going to
[00:44:07.440 --> 00:44:08.440]   work in it.
[00:44:08.440 --> 00:44:10.840]   They're not going to actually collaborate or do too much social stuff.
[00:44:10.840 --> 00:44:15.560]   So I'm a little skeptical that people have really done the hard work.
[00:44:15.560 --> 00:44:19.520]   The technology has gotten better, but you need special purpose hardware to really do
[00:44:19.520 --> 00:44:20.520]   this stuff well.
[00:44:20.520 --> 00:44:22.720]   We got VR.
[00:44:22.720 --> 00:44:23.720]   We got augmented reality.
[00:44:23.720 --> 00:44:27.240]   But we can't get a goddamn Skype connection to work to save our lives.
[00:44:27.240 --> 00:44:29.280]   And I'm all the goddamn four.
[00:44:29.280 --> 00:44:30.280]   I know.
[00:44:30.280 --> 00:44:32.240]   I think we got a ways to go to be honest with you.
[00:44:32.240 --> 00:44:33.720]   But you're lighting looks good.
[00:44:33.720 --> 00:44:34.720]   Technology.
[00:44:34.720 --> 00:44:37.560]   I can't just work on the basics for a little while.
[00:44:37.560 --> 00:44:38.560]   Yeah.
[00:44:38.560 --> 00:44:42.720]   I just squirted a picture of my perspective here at FA doing the show.
[00:44:42.720 --> 00:44:43.720]   That's...
[00:44:43.720 --> 00:44:44.720]   You know what?
[00:44:44.720 --> 00:44:45.720]   Are you comfortable?
[00:44:45.720 --> 00:44:46.720]   I see you got a little lunch.
[00:44:46.720 --> 00:44:47.720]   No.
[00:44:47.720 --> 00:44:52.760]   I'm sitting on some FA t-shirts trying to cushion my bony ass.
[00:44:52.760 --> 00:44:56.320]   I feel so bad.
[00:44:56.320 --> 00:45:00.640]   Right across me is the snack line and the water cooler.
[00:45:00.640 --> 00:45:02.560]   Oh, go get something to eat.
[00:45:02.560 --> 00:45:06.720]   I don't want to wait for my machine and somebody's going to steal it.
[00:45:06.720 --> 00:45:09.240]   Oh, no one would steal it.
[00:45:09.240 --> 00:45:10.240]   F8.
[00:45:10.240 --> 00:45:11.240]   No one would steal that.
[00:45:11.240 --> 00:45:13.440]   Pixel C. Are you kidding me?
[00:45:13.440 --> 00:45:16.680]   You're more likely to get it thrown out.
[00:45:16.680 --> 00:45:19.880]   Somebody left some old junk on the floor there.
[00:45:19.880 --> 00:45:20.880]   Hey, sweet dad.
[00:45:20.880 --> 00:45:22.040]   No, I love my Pixel C.
[00:45:22.040 --> 00:45:23.040]   I'm just teasing.
[00:45:23.040 --> 00:45:24.520]   Here's the picture.
[00:45:24.520 --> 00:45:26.520]   Here's Jeff's POV.
[00:45:26.520 --> 00:45:32.000]   Leo seems to be falling asleep in that picture.
[00:45:32.000 --> 00:45:33.000]   Yeah, yeah.
[00:45:33.000 --> 00:45:34.000]   So somebody says hi here, Leo.
[00:45:34.000 --> 00:45:35.000]   Come on in.
[00:45:35.000 --> 00:45:36.000]   Come on in.
[00:45:36.000 --> 00:45:37.000]   Get the...
[00:45:37.000 --> 00:45:38.000]   Put us up.
[00:45:38.000 --> 00:45:40.000]   Hey, it's Colleen!
[00:45:40.000 --> 00:45:41.000]   Hi Colleen!
[00:45:41.000 --> 00:45:43.920]   Oh my God.
[00:45:43.920 --> 00:45:48.080]   Colleen works for Facebook now, of course, the...
[00:45:48.080 --> 00:45:52.400]   Our genius engineer who put Twitter on the map.
[00:45:52.400 --> 00:45:53.400]   How are you?
[00:45:53.400 --> 00:45:59.480]   Just getting ready to have the end of the live stream so then I can go home and then
[00:45:59.480 --> 00:46:01.600]   have hopefully a vacation for a while.
[00:46:01.600 --> 00:46:05.120]   So this is what you've been doing at Facebook, is streaming.
[00:46:05.120 --> 00:46:08.520]   Yeah, but you'll notice that...
[00:46:08.520 --> 00:46:14.000]   So like, did you see the demo that was like the green screen stuff where my friend Joyce
[00:46:14.000 --> 00:46:19.800]   was playing with other people in VR or I'm also in the 360 camera shoot.
[00:46:19.800 --> 00:46:22.600]   So I've been working on VR and spherical and stuff like that.
[00:46:22.600 --> 00:46:23.600]   Cool!
[00:46:23.600 --> 00:46:25.280]   Come up here and visit us.
[00:46:25.280 --> 00:46:29.040]   You can show me how to use this Oculus Rift thing.
[00:46:29.040 --> 00:46:31.240]   We still miss you at Google Colleen.
[00:46:31.240 --> 00:46:32.240]   Oh man.
[00:46:32.240 --> 00:46:33.240]   You still miss you at Google.
[00:46:33.240 --> 00:46:36.240]   You stole her from us and then Facebook stole her from you.
[00:46:36.240 --> 00:46:38.240]   Okay, I love you Matt.
[00:46:38.240 --> 00:46:39.240]   I love you too.
[00:46:39.240 --> 00:46:40.880]   Nice to be in demand.
[00:46:40.880 --> 00:46:42.480]   I didn't realize you knew Matt too.
[00:46:42.480 --> 00:46:43.480]   That's great.
[00:46:43.480 --> 00:46:44.480]   Yeah, yeah, yeah.
[00:46:44.480 --> 00:46:45.480]   That's great.
[00:46:45.480 --> 00:46:46.480]   Yeah.
[00:46:46.480 --> 00:46:47.480]   We had a good time at Google.
[00:46:47.480 --> 00:46:48.480]   The cross-foot world.
[00:46:48.480 --> 00:46:54.280]   Well, your lighting is really good, Jeff.
[00:46:54.280 --> 00:46:55.280]   You looked dramatic.
[00:46:55.280 --> 00:46:56.280]   It's the window.
[00:46:56.280 --> 00:46:57.280]   The floor level.
[00:46:57.280 --> 00:46:59.680]   I'm right next to the garbage can.
[00:46:59.680 --> 00:47:01.720]   And the ass.
[00:47:01.720 --> 00:47:02.720]   Was it Tantalus?
[00:47:02.720 --> 00:47:08.440]   Yeah, Tantalus could never get down to the water or up to the fruit and you're just right
[00:47:08.440 --> 00:47:09.440]   there.
[00:47:09.440 --> 00:47:10.440]   He's being tantalized.
[00:47:10.440 --> 00:47:13.760]   It's more of a Kevin Marks kind of reference than a man.
[00:47:13.760 --> 00:47:14.760]   Yeah, no kidding.
[00:47:14.760 --> 00:47:17.360]   He just works him a pygony in there.
[00:47:17.360 --> 00:47:19.320]   You got it, man.
[00:47:19.320 --> 00:47:24.240]   If we could just reify something.
[00:47:24.240 --> 00:47:29.760]   We're going to Jeff suggested that we buy this and for the next broadcast you can use
[00:47:29.760 --> 00:47:30.760]   this.
[00:47:30.760 --> 00:47:37.840]   This is one of the cameras that they demonstrated yesterday on the stage at FA for live video.
[00:47:37.840 --> 00:47:43.720]   So we played a lot with Facebook Live last week and it definitely had a big role at
[00:47:43.720 --> 00:47:46.360]   the is had a big role at FA, right, Jeff?
[00:47:46.360 --> 00:47:47.600]   Yes, it really has.
[00:47:47.600 --> 00:47:48.600]   It's huge.
[00:47:48.600 --> 00:47:49.600]   And this is the front.
[00:47:49.600 --> 00:47:51.840]   They announced the Facebook Live API.
[00:47:51.840 --> 00:47:56.320]   So what I believe that means is that you'll be able to take your output Leo and stick it
[00:47:56.320 --> 00:47:59.480]   into Facebook live.
[00:47:59.480 --> 00:48:02.360]   So without having to use the tablet or be the mess.
[00:48:02.360 --> 00:48:06.080]   It also enables you to pull other things so that you can put comments straight in.
[00:48:06.080 --> 00:48:11.000]   So there's a one of the API users, CNN was demonstrating that they can have a stream of
[00:48:11.000 --> 00:48:14.840]   comments and then you can pull a comment out and just put it on the screen.
[00:48:14.840 --> 00:48:18.600]   So finally the chat room can feel like they're legitimate.
[00:48:18.600 --> 00:48:22.560]   And then this Meebo camera is the other thing that was first built with the Facebook API,
[00:48:22.560 --> 00:48:24.320]   the live video API.
[00:48:24.320 --> 00:48:28.840]   It's kind of interesting because it gets a 4K picture into the camera which you can then
[00:48:28.840 --> 00:48:33.920]   control with an iPhone and you can pick your shot or you can use face recognition for it
[00:48:33.920 --> 00:48:37.440]   to follow you because what you're going to get is a 1080 piece.
[00:48:37.440 --> 00:48:42.120]   You're going to get one quarter of the screen somewhere and it's streaming out live.
[00:48:42.120 --> 00:48:45.840]   It's a three camera shot with one camera and an iPhone.
[00:48:45.840 --> 00:48:49.480]   So if you go to the video there Leo, you can see how it shows that you just pointed
[00:48:49.480 --> 00:48:50.480]   somebody's face.
[00:48:50.480 --> 00:48:51.480]   The cooking demo is a good one.
[00:48:51.480 --> 00:48:53.280]   Yeah, it's the only one as far as I can tell.
[00:48:53.280 --> 00:48:54.880]   That's why I'm a little suspicious.
[00:48:54.880 --> 00:49:01.000]   But as soon as they put another video up, I'll be thrilled.
[00:49:01.000 --> 00:49:02.000]   Yeah.
[00:49:02.000 --> 00:49:11.520]   So, so Taste Made, which is I guess a cooking podcast, used this and so the camera is tiny.
[00:49:11.520 --> 00:49:15.440]   But the problem is you still need a director and the director instead of using a tri-cast
[00:49:15.440 --> 00:49:16.440]   or using an iPhone.
[00:49:16.440 --> 00:49:21.760]   But I was thinking, what if you started today and that were you used to have to switch yourself?
[00:49:21.760 --> 00:49:22.760]   Yeah.
[00:49:22.760 --> 00:49:25.440]   What if you have all you do is you point at one person's face and it'll zoom in on that
[00:49:25.440 --> 00:49:26.440]   person like right here.
[00:49:26.440 --> 00:49:27.440]   Right.
[00:49:27.440 --> 00:49:28.440]   And that makes that shot.
[00:49:28.440 --> 00:49:30.080]   Then you can move that box down and show their hands.
[00:49:30.080 --> 00:49:31.080]   No, I like it.
[00:49:31.080 --> 00:49:32.080]   I like it.
[00:49:32.080 --> 00:49:35.600]   We'll have to see how smoothly the transitions go and all of that.
[00:49:35.600 --> 00:49:37.600]   But that's a very intriguing idea.
[00:49:37.600 --> 00:49:41.480]   Well, I think this is what I was saying about the problem of capturing life.
[00:49:41.480 --> 00:49:44.120]   Theta is it's that same thing.
[00:49:44.120 --> 00:49:48.760]   I want a single thing to point at the stage, but I can edit it afterwards.
[00:49:48.760 --> 00:49:54.400]   And you know, you can do that with a little 360 cam, but the problem is this raises not
[00:49:54.400 --> 00:49:55.400]   to it yet.
[00:49:55.400 --> 00:49:57.640]   You can do it with the big 360 cam.
[00:49:57.640 --> 00:49:59.440]   But this so this looks like an interesting.
[00:49:59.440 --> 00:50:00.440]   Yeah.
[00:50:00.440 --> 00:50:02.200]   Facebook is $30,000.
[00:50:02.200 --> 00:50:05.680]   The Ozo from Nokia $60,000.
[00:50:05.680 --> 00:50:11.800]   I imagine if you built a Google jump camera be comparably, 16 cameras is going to be very,
[00:50:11.800 --> 00:50:12.800]   very expensive.
[00:50:12.800 --> 00:50:13.800]   Yeah.
[00:50:13.800 --> 00:50:15.640]   This is just a single high res camera and a very wide angle.
[00:50:15.640 --> 00:50:19.320]   It's 150 degree angle lens.
[00:50:19.320 --> 00:50:23.080]   And I saw the guys from the company and I said, Oh, that's it.
[00:50:23.080 --> 00:50:25.000]   And they said, can I get where can I get it?
[00:50:25.000 --> 00:50:27.000]   So you can order it now on $100 off.
[00:50:27.000 --> 00:50:28.920]   Yeah, we did.
[00:50:28.920 --> 00:50:29.920]   You did too?
[00:50:29.920 --> 00:50:30.920]   Yeah.
[00:50:30.920 --> 00:50:31.920]   All right.
[00:50:31.920 --> 00:50:32.920]   Did you see the bus?
[00:50:32.920 --> 00:50:34.960]   The new labs camera camera kit?
[00:50:34.960 --> 00:50:37.040]   No, tell us about that.
[00:50:37.040 --> 00:50:42.080]   So bus fee labs is a is there like sort of research thingy they have in San Francisco.
[00:50:42.080 --> 00:50:44.400]   And I went to their open day a couple of weeks ago.
[00:50:44.400 --> 00:50:51.520]   And one of the things they've built is the like the field the field reporters 360 video,
[00:50:51.520 --> 00:50:55.400]   which is basically two GoPros and a good microphone.
[00:50:55.400 --> 00:50:57.600]   And boy does that look professional.
[00:50:57.600 --> 00:51:01.080]   Okay, don't 3D print it in red.
[00:51:01.080 --> 00:51:03.840]   Maybe that would make a little bit better.
[00:51:03.840 --> 00:51:04.840]   Oh, my God.
[00:51:04.840 --> 00:51:05.840]   That's all right.
[00:51:05.840 --> 00:51:08.080]   With the watermelon is a little worried.
[00:51:08.080 --> 00:51:09.080]   Right.
[00:51:09.080 --> 00:51:12.520]   It's an intermediate step between the there's a little camera you can buy that's random
[00:51:12.520 --> 00:51:18.600]   box, but that is that the is lower res, whereas this is full full res GoPros that are gen
[00:51:18.600 --> 00:51:19.600]   locked.
[00:51:19.600 --> 00:51:20.600]   So it can give you a.
[00:51:20.600 --> 00:51:21.600]   Oh, they're gen locked.
[00:51:21.600 --> 00:51:22.880]   Well, that's okay.
[00:51:22.880 --> 00:51:23.880]   That's a big deal.
[00:51:23.880 --> 00:51:24.880]   An indecent mic.
[00:51:24.880 --> 00:51:25.880]   And so they've put this.
[00:51:25.880 --> 00:51:27.520]   Yeah, the zoom is a great mic.
[00:51:27.520 --> 00:51:28.520]   Yeah.
[00:51:28.520 --> 00:51:29.520]   Yeah.
[00:51:29.520 --> 00:51:32.840]   What could be what would be a sensible thing to do this with that's still like reasonably
[00:51:32.840 --> 00:51:33.840]   affordable.
[00:51:33.840 --> 00:51:35.160]   And this is 360 as well.
[00:51:35.160 --> 00:51:36.160]   It is.
[00:51:36.160 --> 00:51:37.440]   Yeah, it's immersive as well.
[00:51:37.440 --> 00:51:38.720]   So they basically they put this together.
[00:51:38.720 --> 00:51:42.360]   They're using it, but they also put together a here's how you can make your own and and
[00:51:42.360 --> 00:51:44.160]   plugs together for other reporters.
[00:51:44.160 --> 00:51:45.160]   Same thing.
[00:51:45.160 --> 00:51:48.800]   YouTube's going to have four or three or four live streams from Coachella.
[00:51:48.800 --> 00:51:53.960]   I imagine at least one of them will be 360 degrees because YouTube of course supports
[00:51:53.960 --> 00:51:54.960]   it.
[00:51:54.960 --> 00:51:55.960]   Facebook supports it.
[00:51:55.960 --> 00:51:58.240]   I I in as almost even more than VR.
[00:51:58.240 --> 00:52:02.560]   I think 360 degrees is is going to be useful because of that immersion thing that you can
[00:52:02.560 --> 00:52:03.560]   be there.
[00:52:03.560 --> 00:52:05.720]   I think 180 180 is going to want to surprise us.
[00:52:05.720 --> 00:52:07.200]   I'm not seeing more 180 degree care.
[00:52:07.200 --> 00:52:11.400]   You want 80s plenty to be honest because I shot a whole bunch of stuff at you know, I
[00:52:11.400 --> 00:52:15.080]   just I just put my my theta up in the air right there.
[00:52:15.080 --> 00:52:17.480]   You know stuff you don't care what's behind me.
[00:52:17.480 --> 00:52:18.480]   Right.
[00:52:18.480 --> 00:52:19.960]   Well, the same thing with theater.
[00:52:19.960 --> 00:52:23.920]   I guess unless you put it in the middle of the action, but if you put it in the front
[00:52:23.920 --> 00:52:28.480]   of the stage, you really don't care what the audience is doing behind you.
[00:52:28.480 --> 00:52:29.480]   Yeah.
[00:52:29.480 --> 00:52:33.760]   So exactly so you don't necessarily need to have the back of the camera.
[00:52:33.760 --> 00:52:35.560]   So this camera has 150.
[00:52:35.560 --> 00:52:39.120]   So it's not quite all the way around, but it's pretty close not all the way around.
[00:52:39.120 --> 00:52:42.400]   I mean all the way in front, but it's close enough.
[00:52:42.400 --> 00:52:43.400]   Yeah.
[00:52:43.400 --> 00:52:45.440]   You know, that would probably be adequate for us, for instance.
[00:52:45.440 --> 00:52:47.960]   Yeah, that would be interesting to play with.
[00:52:47.960 --> 00:52:55.480]   But the other part of this is the difference between the reason they have 16 or 17 cameras
[00:52:55.480 --> 00:52:58.040]   is that that actually gets you good stereoscopy.
[00:52:58.040 --> 00:53:01.800]   So if you've got just got two lenses pointing opposite directions, you just get a sphere.
[00:53:01.800 --> 00:53:02.800]   Right.
[00:53:02.800 --> 00:53:06.040]   Whereas if you've got lots of them like that, you can actually move your head and you get
[00:53:06.040 --> 00:53:11.960]   that sense of being there that you get with the Oculus, when it's actually giving you separate
[00:53:11.960 --> 00:53:14.640]   eye views and understanding where your head is.
[00:53:14.640 --> 00:53:18.120]   And that's one of the, there's two things, a 3D perception.
[00:53:18.120 --> 00:53:19.840]   Dana Boyd wrote some interesting research on this.
[00:53:19.840 --> 00:53:25.600]   But there's the parallax between the viewpoint.
[00:53:25.600 --> 00:53:31.520]   But there's also you look at the object and adjust how you're looking and that decides,
[00:53:31.520 --> 00:53:34.360]   is that in front of it or is it behind it?
[00:53:34.360 --> 00:53:37.280]   Kevin, Kevin, the children of the coroner behind you.
[00:53:37.280 --> 00:53:39.800]   No, I'm just, I'm sorry.
[00:53:39.800 --> 00:53:43.240]   There's a storm going on back there.
[00:53:43.240 --> 00:53:45.320]   I don't know what's going on.
[00:53:45.320 --> 00:53:48.560]   I need to go and put a towel at my microphone.
[00:53:48.560 --> 00:53:51.360]   I think, is that lightning I'm seeing?
[00:53:51.360 --> 00:53:53.360]   Oh my God.
[00:53:53.360 --> 00:53:57.600]   We're not in Kansas anymore.
[00:53:57.600 --> 00:54:01.040]   We're just spiral up.
[00:54:01.040 --> 00:54:07.440]   Speaking of ill wins, what does this deal with Buzzfeed having to cut its revenue expectations
[00:54:07.440 --> 00:54:09.720]   in half, Jeff?
[00:54:09.720 --> 00:54:11.880]   And then the CEO's denying it.
[00:54:11.880 --> 00:54:15.280]   Jonah was on the stage yesterday and then I heard that the PR people were saying that,
[00:54:15.280 --> 00:54:16.640]   no, no, no, this was overblown.
[00:54:16.640 --> 00:54:18.320]   The numbers were right.
[00:54:18.320 --> 00:54:20.800]   And it wasn't so much that they lost money.
[00:54:20.800 --> 00:54:22.360]   It's that the expectations were very high.
[00:54:22.360 --> 00:54:23.880]   They thought they were going to make half a billion.
[00:54:23.880 --> 00:54:25.800]   They're only going to make a quarter of a billion.
[00:54:25.800 --> 00:54:27.680]   And revenue, not profit, revenue.
[00:54:27.680 --> 00:54:28.960]   Yeah, which is bad management.
[00:54:28.960 --> 00:54:29.960]   I mean, I don't know.
[00:54:29.960 --> 00:54:35.880]   I think the product they sell, my argument is that Buzzfeed sells a service to advertisers.
[00:54:35.880 --> 00:54:37.800]   We can make our stuff viral and make your stuff viral.
[00:54:37.800 --> 00:54:38.640]   And that's not easy.
[00:54:38.640 --> 00:54:40.840]   It's not cheap to do.
[00:54:40.840 --> 00:54:43.160]   Is this the end of date of advertising?
[00:54:43.160 --> 00:54:45.440]   No, I can only hope.
[00:54:45.440 --> 00:54:46.440]   I don't know.
[00:54:46.440 --> 00:54:50.640]   Jonah was on stage yesterday with Chris Cox and the end of Nice Chat about how wonderful
[00:54:50.640 --> 00:54:52.880]   Facebook is and what a Buzzfeed is.
[00:54:52.880 --> 00:54:55.840]   And nobody asked anything about that.
[00:54:55.840 --> 00:54:56.840]   Yeah.
[00:54:56.840 --> 00:54:57.840]   Yeah.
[00:54:57.840 --> 00:54:58.840]   Is this new?
[00:54:58.840 --> 00:55:06.240]   I can go to m.me/myfacebookhandle and that's my messenger feed.
[00:55:06.240 --> 00:55:07.240]   I wonder if that's new.
[00:55:07.240 --> 00:55:08.320]   All the bots are there too.
[00:55:08.320 --> 00:55:16.560]   So m.me/cnn, for instance, is how you would add the CNN bot to your messenger feed one
[00:55:16.560 --> 00:55:17.560]   way anyway.
[00:55:17.560 --> 00:55:18.920]   You can also search for it.
[00:55:18.920 --> 00:55:20.080]   That's by the way one problem.
[00:55:20.080 --> 00:55:21.880]   It's kind of hard to find.
[00:55:21.880 --> 00:55:25.640]   So I went to m.me/cnn, and then it pops up.
[00:55:25.640 --> 00:55:26.960]   It's hard to find some of these bots.
[00:55:26.960 --> 00:55:28.640]   I don't know if they're going to have a bot store.
[00:55:28.640 --> 00:55:32.200]   A third parties are doing that.
[00:55:32.200 --> 00:55:37.800]   But that's kind of unknown.
[00:55:37.800 --> 00:55:39.480]   The store is critical, isn't it?
[00:55:39.480 --> 00:55:44.800]   Because that's what-- if you're going to be the app for the platform, then you're going
[00:55:44.800 --> 00:55:47.640]   to need to have somewhere to go to get the apps.
[00:55:47.640 --> 00:55:50.160]   All right, let's take a break.
[00:55:50.160 --> 00:55:52.360]   We're going to come back with more.
[00:55:52.360 --> 00:55:55.760]   We have the greatest panel ever.
[00:55:55.760 --> 00:55:59.800]   The bots need to install an app because it's a web server effectively.
[00:55:59.800 --> 00:56:01.960]   It's just a different UI to a web server.
[00:56:01.960 --> 00:56:07.400]   But you need to follow them, which is the equivalent of installing them.
[00:56:07.400 --> 00:56:09.400]   You have to say hello bot.
[00:56:09.400 --> 00:56:11.040]   I mean, yeah, there's nothing to install.
[00:56:11.040 --> 00:56:12.040]   It's a lot easier.
[00:56:12.040 --> 00:56:13.040]   Right.
[00:56:13.040 --> 00:56:14.040]   But you do have--
[00:56:14.040 --> 00:56:17.620]   Yeah, you-- well, that's the point of that is you've now got a URL that lets you
[00:56:17.620 --> 00:56:19.960]   open a chat window, which Twitter has too.
[00:56:19.960 --> 00:56:22.560]   Twitter has a thing that lets you open a DM.
[00:56:22.560 --> 00:56:24.560]   There's a URL linked to do that.
[00:56:24.560 --> 00:56:25.720]   It'll spawn it in the browser.
[00:56:25.720 --> 00:56:26.720]   It'll spawn it on your--
[00:56:26.720 --> 00:56:27.720]   Right.
[00:56:27.720 --> 00:56:29.720]   Yeah, it's just using a URL scheme.
[00:56:29.720 --> 00:56:30.720]   Yeah.
[00:56:30.720 --> 00:56:31.720]   Well, it's just you make sure you be scheme.
[00:56:31.720 --> 00:56:34.360]   But then it redirects depending on what's installed.
[00:56:34.360 --> 00:56:35.360]   Right.
[00:56:35.360 --> 00:56:39.640]   And they both made that work, which is a sensible thing to do.
[00:56:39.640 --> 00:56:41.480]   Because making things linkable is a good idea.
[00:56:41.480 --> 00:56:44.040]   Well, and I might put it in my email signature, right?
[00:56:44.040 --> 00:56:45.040]   Yeah.
[00:56:45.040 --> 00:56:47.620]   I want to fill my inbox with spam.
[00:56:47.620 --> 00:56:49.460]   I think I will.
[00:56:49.460 --> 00:56:50.460]   Right.
[00:56:50.460 --> 00:56:56.140]   Well, it makes-- the thing that's only to its companies is like click here to chat to
[00:56:56.140 --> 00:56:59.820]   somebody and you're chatting inside the environment you're comfortable with.
[00:56:59.820 --> 00:57:00.820]   Right.
[00:57:00.820 --> 00:57:01.820]   Right.
[00:57:01.820 --> 00:57:02.820]   Chat with us.
[00:57:02.820 --> 00:57:03.820]   Yes.
[00:57:03.820 --> 00:57:06.980]   Because when your washing machine breaks, what you really don't do is chat with them.
[00:57:06.980 --> 00:57:11.860]   So the reason I like it more to install is because as soon as you communicate with a
[00:57:11.860 --> 00:57:13.460]   chat bot, it's there.
[00:57:13.460 --> 00:57:14.460]   It doesn't go away.
[00:57:14.460 --> 00:57:17.160]   It's not like going to a web page and you don't have to go back to that web page.
[00:57:17.160 --> 00:57:19.760]   No, you've invited in your home like a vampire.
[00:57:19.760 --> 00:57:20.760]   Yeah.
[00:57:20.760 --> 00:57:24.780]   And now-- hey, my god, it's old home week.
[00:57:24.780 --> 00:57:27.360]   There's Alex Lindsey in the headset.
[00:57:27.360 --> 00:57:30.800]   Alex, are you working or are you visiting?
[00:57:30.800 --> 00:57:31.800]   No comment.
[00:57:31.800 --> 00:57:32.800]   OK.
[00:57:32.800 --> 00:57:33.800]   [LAUGHTER]
[00:57:33.800 --> 00:57:35.800]   Just wandering around.
[00:57:35.800 --> 00:57:36.800]   Just having fun.
[00:57:36.800 --> 00:57:37.800]   Having fun.
[00:57:37.800 --> 00:57:38.800]   Yeah.
[00:57:38.800 --> 00:57:40.440]   Anyway, I just thought I'd say hi.
[00:57:40.440 --> 00:57:41.440]   Nice to see you.
[00:57:41.440 --> 00:57:42.440]   Yeah.
[00:57:42.440 --> 00:57:43.440]   I see.
[00:57:43.440 --> 00:57:45.440]   Are there all people who used to work with me?
[00:57:45.440 --> 00:57:46.440]   My studio.
[00:57:46.440 --> 00:57:48.860]   I have better jobs.
[00:57:48.860 --> 00:57:49.860]   It's good.
[00:57:49.860 --> 00:57:52.860]   So why are Skype doesn't work so well?
[00:57:52.860 --> 00:57:54.900]   I mean, just for-- yeah, I know.
[00:57:54.900 --> 00:57:58.740]   Just for warning that the last talk just ended at Herbs, I could get kicked out of here,
[00:57:58.740 --> 00:58:02.540]   which is I'll go find some place under a tree next to an access point.
[00:58:02.540 --> 00:58:03.540]   All right.
[00:58:03.540 --> 00:58:05.740]   Well, let me do the ad and then we can just end the damn show.
[00:58:05.740 --> 00:58:06.740]   All right.
[00:58:06.740 --> 00:58:07.740]   Keep going.
[00:58:07.740 --> 00:58:08.740]   I'm just warning you.
[00:58:08.740 --> 00:58:09.740]   That's all.
[00:58:09.740 --> 00:58:10.740]   People are walking out.
[00:58:10.740 --> 00:58:11.740]   You do the ad.
[00:58:11.740 --> 00:58:14.740]   I'm just going to be a little bit nervous.
[00:58:14.740 --> 00:58:16.740]   I'm going to be a little bit nervous.
[00:58:16.740 --> 00:58:17.740]   I'm going to be a little bit nervous.
[00:58:17.740 --> 00:58:18.740]   I'm going to be a little bit nervous.
[00:58:18.740 --> 00:58:19.740]   I'm going to be a little bit nervous.
[00:58:19.740 --> 00:58:20.740]   I'm going to be a little bit nervous.
[00:58:20.740 --> 00:58:21.740]   I'm going to be a little bit nervous.
[00:58:21.740 --> 00:58:22.740]   I'm going to be a little bit nervous.
[00:58:22.740 --> 00:58:23.740]   I'm going to be a little bit nervous.
[00:58:23.740 --> 00:58:24.740]   I'm going to be a little bit nervous.
[00:58:24.740 --> 00:58:25.740]   I'm going to be a little bit nervous.
[00:58:25.740 --> 00:58:26.740]   I'm going to be a little bit nervous.
[00:58:26.740 --> 00:58:27.740]   I'm going to be a little bit nervous.
[00:58:27.740 --> 00:58:28.740]   I'm going to be a little bit nervous.
[00:58:28.740 --> 00:58:29.740]   I'm going to be a little bit nervous.
[00:58:29.740 --> 00:58:30.740]   I'm going to be a little bit nervous.
[00:58:30.740 --> 00:58:31.740]   I'm going to be a little bit nervous.
[00:58:31.740 --> 00:58:32.740]   I'm going to be a little bit nervous.
[00:58:32.740 --> 00:58:38.740]   I'm going to be a little bit nervous.
[00:58:38.740 --> 00:58:39.740]   I'm going to be a little bit nervous.
[00:58:39.740 --> 00:58:40.740]   I'm going to be a little bit nervous.
[00:58:40.740 --> 00:58:41.740]   I'm going to be a little bit nervous.
[00:58:41.740 --> 00:58:42.740]   I'm going to be a little bit nervous.
[00:58:42.740 --> 00:58:43.740]   I'm going to be a little bit nervous.
[00:58:43.740 --> 00:58:44.740]   I'm going to be a little bit nervous.
[00:58:44.740 --> 00:58:45.740]   I'm going to be a little bit nervous.
[00:58:45.740 --> 00:58:46.740]   I'm going to be a little bit nervous.
[00:58:46.740 --> 00:58:47.740]   I'm going to be a little bit nervous.
[00:58:47.740 --> 00:58:48.740]   I'm going to be a little bit nervous.
[00:58:48.740 --> 00:58:49.740]   I'm going to be a little bit nervous.
[00:58:49.740 --> 00:58:50.740]   I'm going to be a little bit nervous.
[00:58:50.740 --> 00:58:51.740]   I'm going to be a little bit nervous.
[00:58:51.740 --> 00:58:52.740]   I'm going to be a little bit nervous.
[00:58:52.740 --> 00:58:53.740]   I'm going to be a little bit nervous.
[00:58:53.740 --> 00:58:54.740]   I'm going to be a little bit nervous.
[00:58:54.740 --> 00:58:55.740]   I'm going to be a little bit nervous.
[00:58:55.740 --> 00:59:00.740]   I'm going to be a little bit nervous.
[00:59:00.740 --> 00:59:07.740]   I'm going to be a little bit nervous.
[00:59:07.740 --> 00:59:08.740]   I'm going to be a little bit nervous.
[00:59:08.740 --> 00:59:09.740]   I'm going to be a little bit nervous.
[00:59:09.740 --> 00:59:10.740]   I'm going to be a little bit nervous.
[00:59:10.740 --> 00:59:11.740]   I'm going to be a little bit nervous.
[00:59:11.740 --> 00:59:12.740]   I'm going to be a little bit nervous.
[00:59:12.740 --> 00:59:13.740]   I'm going to be a little bit nervous.
[00:59:13.740 --> 00:59:14.740]   I'm going to be a little bit nervous.
[00:59:14.740 --> 00:59:15.740]   I'm going to be a little bit nervous.
[00:59:15.740 --> 00:59:16.740]   I'm going to be a little bit nervous.
[00:59:16.740 --> 00:59:17.740]   I'm going to be a little bit nervous.
[00:59:17.740 --> 00:59:18.740]   I'm going to be a little bit nervous.
[00:59:18.740 --> 00:59:19.740]   I'm going to be a little bit nervous.
[00:59:19.740 --> 00:59:20.740]   I'm going to be a little bit nervous.
[00:59:20.740 --> 00:59:21.740]   I'm going to be a little bit nervous.
[00:59:21.740 --> 00:59:22.740]   I'm going to be a little bit nervous.
[00:59:22.740 --> 00:59:23.740]   I'm going to be a little bit nervous.
[00:59:23.740 --> 00:59:24.740]   I'm going to be a little bit nervous.
[00:59:24.740 --> 00:59:25.740]   I'm going to be a little bit nervous.
[00:59:25.740 --> 00:59:26.740]   I'm going to be a little bit nervous.
[00:59:26.740 --> 00:59:27.740]   I'm going to be a little bit nervous.
[00:59:27.740 --> 00:59:28.740]   I'm going to be a little bit nervous.
[00:59:28.740 --> 00:59:29.740]   I'm going to be a little bit nervous.
[00:59:29.740 --> 00:59:30.740]   I'm going to be a little bit nervous.
[00:59:30.740 --> 00:59:31.740]   I'm going to be a little bit nervous.
[00:59:31.740 --> 00:59:32.740]   I'm going to be a little bit nervous.
[00:59:32.740 --> 00:59:33.740]   I'm going to be a little bit nervous.
[00:59:33.740 --> 00:59:34.740]   I'm going to be a little bit nervous.
[00:59:34.740 --> 00:59:35.740]   I'm going to be a little bit nervous.
[00:59:35.740 --> 00:59:36.740]   I'm going to be a little bit nervous.
[00:59:36.740 --> 00:59:37.740]   I'm going to be a little bit nervous.
[00:59:37.740 --> 00:59:38.740]   I'm going to be a little bit nervous.
[00:59:38.740 --> 00:59:39.740]   I'm going to be a little bit nervous.
[00:59:39.740 --> 00:59:40.740]   I'm going to be a little bit nervous.
[00:59:40.740 --> 00:59:41.740]   I'm going to be a little bit nervous.
[00:59:41.740 --> 01:00:00.740]   I'm going to be a little bit nervous.
[01:00:00.740 --> 01:00:01.740]   I'm going to be a little bit nervous.
[01:00:01.740 --> 01:00:02.740]   I'm going to be a little bit nervous.
[01:00:02.740 --> 01:00:03.740]   I'm going to be a little bit nervous.
[01:00:03.740 --> 01:00:04.740]   I'm going to be a little bit nervous.
[01:00:04.740 --> 01:00:05.740]   I'm going to be a little bit nervous.
[01:00:05.740 --> 01:00:06.740]   I'm going to be a little bit nervous.
[01:00:06.740 --> 01:00:07.740]   I'm going to be a little bit nervous.
[01:00:07.740 --> 01:00:08.740]   I'm going to be a little bit nervous.
[01:00:08.740 --> 01:00:09.740]   I'm going to be a little bit nervous.
[01:00:09.740 --> 01:00:10.740]   I'm going to be a little bit nervous.
[01:00:10.740 --> 01:00:11.740]   I'm going to be a little bit nervous.
[01:00:11.740 --> 01:00:12.740]   I'm going to be a little bit nervous.
[01:00:12.740 --> 01:00:13.740]   I'm going to be a little bit nervous.
[01:00:13.740 --> 01:00:14.740]   I'm going to be a little bit nervous.
[01:00:14.740 --> 01:00:15.740]   I'm going to be a little bit nervous.
[01:00:15.740 --> 01:00:16.740]   I'm going to be a little bit nervous.
[01:00:16.740 --> 01:00:17.740]   I'm going to be a little bit nervous.
[01:00:17.740 --> 01:00:18.740]   I'm going to be a little bit nervous.
[01:00:18.740 --> 01:00:19.740]   I'm going to be a little bit nervous.
[01:00:19.740 --> 01:00:20.740]   I'm going to be a little bit nervous.
[01:00:20.740 --> 01:00:21.740]   I'm going to be a little bit nervous.
[01:00:21.740 --> 01:00:22.740]   I'm going to be a little bit nervous.
[01:00:22.740 --> 01:00:23.740]   I'm going to be a little bit nervous.
[01:00:23.740 --> 01:00:24.740]   I'm going to be a little bit nervous.
[01:00:24.740 --> 01:00:25.740]   I'm going to be a little bit nervous.
[01:00:25.740 --> 01:00:26.740]   I'm going to be a little bit nervous.
[01:00:26.740 --> 01:00:27.740]   I'm going to be a little bit nervous.
[01:00:27.740 --> 01:00:28.740]   I'm going to be a little bit nervous.
[01:00:28.740 --> 01:00:35.740]   I'm going to be a little bit nervous.
[01:00:35.740 --> 01:00:36.740]   I'm going to be a little bit nervous.
[01:00:36.740 --> 01:00:37.740]   I'm going to be a little bit nervous.
[01:00:37.740 --> 01:00:38.740]   I'm going to be a little bit nervous.
[01:00:38.740 --> 01:00:39.740]   I'm going to be a little bit nervous.
[01:00:39.740 --> 01:00:40.740]   I'm going to be a little bit nervous.
[01:00:40.740 --> 01:00:41.740]   I'm going to be a little bit nervous.
[01:00:41.740 --> 01:00:42.740]   I'm going to be a little bit nervous.
[01:00:42.740 --> 01:00:43.740]   I'm going to be a little bit nervous.
[01:00:43.740 --> 01:00:44.740]   I'm going to be a little bit nervous.
[01:00:44.740 --> 01:00:45.740]   I'm going to be a little bit nervous.
[01:00:45.740 --> 01:00:46.740]   I'm going to be a little bit nervous.
[01:00:46.740 --> 01:00:47.740]   I'm going to be a little bit nervous.
[01:00:47.740 --> 01:00:48.740]   I'm going to be a little bit nervous.
[01:00:48.740 --> 01:00:49.740]   I'm going to be a little bit nervous.
[01:00:49.740 --> 01:00:50.740]   I'm going to be a little bit nervous.
[01:00:50.740 --> 01:00:51.740]   I'm going to be a little bit nervous.
[01:00:51.740 --> 01:00:52.740]   I'm going to be a little bit nervous.
[01:00:52.740 --> 01:00:57.740]   I'm going to be a little bit nervous.
[01:00:57.740 --> 01:01:04.740]   I'm going to be a little bit nervous.
[01:01:04.740 --> 01:01:05.740]   I'm going to be a little bit nervous.
[01:01:05.740 --> 01:01:06.740]   I'm going to be a little bit nervous.
[01:01:06.740 --> 01:01:07.740]   I'm going to be a little bit nervous.
[01:01:07.740 --> 01:01:08.740]   I'm going to be a little bit nervous.
[01:01:08.740 --> 01:01:09.740]   I'm going to be a little bit nervous.
[01:01:09.740 --> 01:01:10.740]   I'm going to be a little bit nervous.
[01:01:10.740 --> 01:01:11.740]   I'm going to be a little bit nervous.
[01:01:11.740 --> 01:01:12.740]   I'm going to be a little bit nervous.
[01:01:12.740 --> 01:01:13.740]   I'm going to be a little bit nervous.
[01:01:13.740 --> 01:01:14.740]   I'm going to be a little bit nervous.
[01:01:14.740 --> 01:01:15.740]   I'm going to be a little bit nervous.
[01:01:15.740 --> 01:01:16.740]   I'm going to be a little bit nervous.
[01:01:16.740 --> 01:01:17.740]   I'm going to be a little bit nervous.
[01:01:17.740 --> 01:01:18.740]   I'm going to be a little bit nervous.
[01:01:18.740 --> 01:01:19.740]   I'm going to be a little bit nervous.
[01:01:19.740 --> 01:01:20.740]   I'm going to be a little bit nervous.
[01:01:20.740 --> 01:01:21.740]   I'm going to be a little bit nervous.
[01:01:21.740 --> 01:01:22.740]   I'm going to be a little bit nervous.
[01:01:22.740 --> 01:01:23.740]   I'm going to be a little bit nervous.
[01:01:23.740 --> 01:01:24.740]   I'm going to be a little bit nervous.
[01:01:24.740 --> 01:01:25.740]   I'm going to be a little bit nervous.
[01:01:25.740 --> 01:01:26.740]   I'm going to be a little bit nervous.
[01:01:26.740 --> 01:01:27.740]   I'm going to be a little bit nervous.
[01:01:27.740 --> 01:01:28.740]   I'm going to be a little bit nervous.
[01:01:28.740 --> 01:01:29.740]   I'm going to be a little bit nervous.
[01:01:29.740 --> 01:01:30.740]   I'm going to be a little bit nervous.
[01:01:30.740 --> 01:01:31.740]   I'm going to be a little bit nervous.
[01:01:31.740 --> 01:01:32.740]   I'm going to be a little bit nervous.
[01:01:32.740 --> 01:01:33.740]   I'm going to be a little bit nervous.
[01:01:33.740 --> 01:01:34.740]   I'm going to be a little bit nervous.
[01:01:34.740 --> 01:01:35.740]   I'm going to be a little bit nervous.
[01:01:35.740 --> 01:01:36.740]   I'm going to be a little bit nervous.
[01:01:36.740 --> 01:01:37.740]   I'm going to be a little bit nervous.
[01:01:37.740 --> 01:01:38.740]   I'm going to be a little bit nervous.
[01:01:38.740 --> 01:02:04.740]   I'm going to be a little bit nervous.
[01:02:04.740 --> 01:02:11.740]   I'm going to be a little bit nervous.
[01:02:11.740 --> 01:02:18.740]   I'm going to be a little bit nervous.
[01:02:18.740 --> 01:02:25.740]   I'm going to be a little bit nervous.
[01:02:25.740 --> 01:02:32.740]   I'm going to be a little bit nervous.
[01:02:32.740 --> 01:02:33.740]   I'm going to be a little bit nervous.
[01:02:33.740 --> 01:02:34.740]   I'm going to be a little bit nervous.
[01:02:34.740 --> 01:02:35.740]   I'm going to be a little bit nervous.
[01:02:35.740 --> 01:02:36.740]   I'm going to be a little bit nervous.
[01:02:36.740 --> 01:02:37.740]   I'm going to be a little bit nervous.
[01:02:37.740 --> 01:02:38.740]   I'm going to be a little bit nervous.
[01:02:38.740 --> 01:02:39.740]   I'm going to be a little bit nervous.
[01:02:39.740 --> 01:02:40.740]   I'm going to be a little bit nervous.
[01:02:40.740 --> 01:02:41.740]   I'm going to be a little bit nervous.
[01:02:41.740 --> 01:02:42.740]   I'm going to be a little bit nervous.
[01:02:42.740 --> 01:02:43.740]   I'm going to be a little bit nervous.
[01:02:43.740 --> 01:02:44.740]   I'm going to be a little bit nervous.
[01:02:44.740 --> 01:02:45.740]   I'm going to be a little bit nervous.
[01:02:45.740 --> 01:02:46.740]   I'm going to be a little bit nervous.
[01:02:46.740 --> 01:02:47.740]   I'm going to be a little bit nervous.
[01:02:47.740 --> 01:02:48.740]   I'm going to be a little bit nervous.
[01:02:48.740 --> 01:02:49.740]   I'm going to be a little bit nervous.
[01:02:49.740 --> 01:02:50.740]   I'm going to be a little bit nervous.
[01:02:50.740 --> 01:03:16.740]   I'm going to be a little bit nervous.
[01:03:16.740 --> 01:03:23.740]   I'm going to be a little bit nervous.
[01:03:23.740 --> 01:03:30.740]   I'm going to be a little bit nervous.
[01:03:30.740 --> 01:03:37.740]   I'm going to be a little bit nervous.
[01:03:37.740 --> 01:03:44.740]   I'm going to be a little bit nervous.
[01:03:44.740 --> 01:03:46.740]   I'm going to be a little bit nervous.
[01:03:46.740 --> 01:03:47.740]   I'm going to be a little bit nervous.
[01:03:47.740 --> 01:03:48.740]   I'm going to be a little bit nervous.
[01:03:48.740 --> 01:03:49.740]   I'm going to be a little bit nervous.
[01:03:49.740 --> 01:03:50.740]   I'm going to be a little bit nervous.
[01:03:50.740 --> 01:03:51.740]   I'm going to be a little bit nervous.
[01:03:51.740 --> 01:03:52.740]   I'm going to be a little bit nervous.
[01:03:52.740 --> 01:03:53.740]   I'm going to be a little bit nervous.
[01:03:53.740 --> 01:03:54.740]   I'm going to be a little bit nervous.
[01:03:54.740 --> 01:03:55.740]   I'm going to be a little bit nervous.
[01:03:55.740 --> 01:03:56.740]   I'm going to be a little bit nervous.
[01:03:56.740 --> 01:03:57.740]   I'm going to be a little bit nervous.
[01:03:57.740 --> 01:03:58.740]   I'm going to be a little bit nervous.
[01:03:58.740 --> 01:03:59.740]   I'm going to be a little bit nervous.
[01:03:59.740 --> 01:04:00.740]   I'm going to be a little bit nervous.
[01:04:00.740 --> 01:04:01.740]   I'm going to be a little bit nervous.
[01:04:01.740 --> 01:04:02.740]   I'm going to be a little bit nervous.
[01:04:02.740 --> 01:04:03.740]   I'm going to be a little bit nervous.
[01:04:03.740 --> 01:04:09.740]   I'm going to be a little bit nervous.
[01:04:09.740 --> 01:04:16.740]   I'm going to be a little bit nervous.
[01:04:16.740 --> 01:04:17.740]   I'm going to be a little bit nervous.
[01:04:17.740 --> 01:04:18.740]   I'm going to be a little bit nervous.
[01:04:18.740 --> 01:04:19.740]   I'm going to be a little bit nervous.
[01:04:19.740 --> 01:04:20.740]   I'm going to be a little bit nervous.
[01:04:20.740 --> 01:04:21.740]   I'm going to be a little bit nervous.
[01:04:21.740 --> 01:04:22.740]   I'm going to be a little bit nervous.
[01:04:22.740 --> 01:04:23.740]   I'm going to be a little bit nervous.
[01:04:23.740 --> 01:04:24.740]   I'm going to be a little bit nervous.
[01:04:24.740 --> 01:04:25.740]   I'm going to be a little bit nervous.
[01:04:25.740 --> 01:04:26.740]   I'm going to be a little bit nervous.
[01:04:26.740 --> 01:04:27.740]   I'm going to be a little bit nervous.
[01:04:27.740 --> 01:04:28.740]   I'm going to be a little bit nervous.
[01:04:28.740 --> 01:04:29.740]   I'm going to be a little bit nervous.
[01:04:29.740 --> 01:04:30.740]   I'm going to be a little bit nervous.
[01:04:30.740 --> 01:04:31.740]   I'm going to be a little bit nervous.
[01:04:31.740 --> 01:04:32.740]   I'm going to be a little bit nervous.
[01:04:32.740 --> 01:04:33.740]   I'm going to be a little bit nervous.
[01:04:33.740 --> 01:04:34.740]   I'm going to be a little bit nervous.
[01:04:34.740 --> 01:04:35.740]   I'm going to be a little bit nervous.
[01:04:35.740 --> 01:04:36.740]   I'm going to be a little bit nervous.
[01:04:36.740 --> 01:04:37.740]   I'm going to be a little bit nervous.
[01:04:37.740 --> 01:04:38.740]   I'm going to be a little bit nervous.
[01:04:38.740 --> 01:04:39.740]   I'm going to be a little bit nervous.
[01:04:39.740 --> 01:04:40.740]   I'm going to be a little bit nervous.
[01:04:40.740 --> 01:04:41.740]   I'm going to be a little bit nervous.
[01:04:41.740 --> 01:04:42.740]   I'm going to be a little bit nervous.
[01:04:42.740 --> 01:04:43.740]   I'm going to be a little bit nervous.
[01:04:43.740 --> 01:04:44.740]   I'm going to be a little bit nervous.
[01:04:44.740 --> 01:04:45.740]   I'm going to be a little bit nervous.
[01:04:45.740 --> 01:04:46.740]   I'm going to be a little bit nervous.
[01:04:46.740 --> 01:04:47.740]   I'm going to be a little bit nervous.
[01:04:47.740 --> 01:04:48.740]   I'm going to be a little bit nervous.
[01:04:48.740 --> 01:04:49.740]   I'm going to be a little bit nervous.
[01:04:49.740 --> 01:04:50.740]   I'm going to be a little bit nervous.
[01:04:50.740 --> 01:05:09.740]   I'm going to be a little bit nervous.
[01:05:09.740 --> 01:05:16.740]   I'm going to be a little bit nervous.
[01:05:16.740 --> 01:05:23.740]   I'm going to be a little bit nervous.
[01:05:23.740 --> 01:05:24.740]   I'm going to be a little bit nervous.
[01:05:24.740 --> 01:05:25.740]   I'm going to be a little bit nervous.
[01:05:25.740 --> 01:05:26.740]   I'm going to be a little bit nervous.
[01:05:26.740 --> 01:05:27.740]   I'm going to be a little bit nervous.
[01:05:27.740 --> 01:05:28.740]   I'm going to be a little bit nervous.
[01:05:28.740 --> 01:05:29.740]   I'm going to be a little bit nervous.
[01:05:29.740 --> 01:05:30.740]   I'm going to be a little bit nervous.
[01:05:30.740 --> 01:05:31.740]   I'm going to be a little bit nervous.
[01:05:31.740 --> 01:05:32.740]   I'm going to be a little bit nervous.
[01:05:32.740 --> 01:05:33.740]   I'm going to be a little bit nervous.
[01:05:33.740 --> 01:05:34.740]   I'm going to be a little bit nervous.
[01:05:34.740 --> 01:05:35.740]   I'm going to be a little bit nervous.
[01:05:35.740 --> 01:05:36.740]   I'm going to be a little bit nervous.
[01:05:36.740 --> 01:05:37.740]   I'm going to be a little bit nervous.
[01:05:37.740 --> 01:05:38.740]   I'm going to be a little bit nervous.
[01:05:38.740 --> 01:05:39.740]   I'm going to be a little bit nervous.
[01:05:39.740 --> 01:05:40.740]   I'm going to be a little bit nervous.
[01:05:40.740 --> 01:05:45.740]   I'm going to be a little bit nervous.
[01:05:45.740 --> 01:05:52.740]   I'm going to be a little bit nervous.
[01:05:52.740 --> 01:05:59.740]   I'm going to be a little bit nervous.
[01:05:59.740 --> 01:06:06.740]   I'm going to be a little bit nervous.
[01:06:06.740 --> 01:06:11.740]   I'm going to be a little bit nervous.
[01:06:11.740 --> 01:06:18.740]   I'm going to be a little bit nervous.
[01:06:18.740 --> 01:06:25.740]   I'm going to be a little bit nervous.
[01:06:25.740 --> 01:06:32.740]   I'm going to be a little bit nervous.
[01:06:32.740 --> 01:06:37.740]   I'm going to be a little bit nervous.
[01:06:37.740 --> 01:06:44.740]   I'm going to be a little bit nervous.
[01:06:44.740 --> 01:06:51.740]   I'm going to be a little bit nervous.
[01:06:51.740 --> 01:06:58.740]   I'm going to be a little bit nervous.
[01:06:58.740 --> 01:07:03.740]   I'm going to be a little bit nervous.
[01:07:03.740 --> 01:07:08.740]   I'm going to be a little bit nervous.
[01:07:08.740 --> 01:07:13.740]   I'm going to be a little bit nervous.
[01:07:13.740 --> 01:07:18.740]   I'm going to be a little bit nervous.
[01:07:18.740 --> 01:07:25.740]   I'm going to be a little bit nervous.
[01:07:25.740 --> 01:07:30.740]   I'm going to be a little bit nervous.
[01:07:30.740 --> 01:07:35.740]   I'm going to be a little bit nervous.
[01:07:35.740 --> 01:07:40.740]   I'm going to be a little bit nervous.
[01:07:40.740 --> 01:07:45.740]   I'm going to be a little bit nervous.
[01:07:45.740 --> 01:07:52.740]   I'm going to be a little bit nervous.
[01:07:52.740 --> 01:07:57.740]   I'm going to be a little bit nervous.
[01:07:57.740 --> 01:08:02.740]   I'm going to be a little bit nervous.
[01:08:02.740 --> 01:08:07.740]   I'm going to be a little bit nervous.
[01:08:07.740 --> 01:08:12.740]   I'm going to be a little bit nervous.
[01:08:12.740 --> 01:08:19.740]   I'm going to be a little bit nervous.
[01:08:19.740 --> 01:08:24.740]   I'm going to be a little bit nervous.
[01:08:24.740 --> 01:08:29.740]   I'm going to be a little bit nervous.
[01:08:29.740 --> 01:08:34.740]   I'm going to be a little bit nervous.
[01:08:34.740 --> 01:08:39.740]   I'm going to be a little bit nervous.
[01:08:39.740 --> 01:08:44.740]   I'm going to be a little bit nervous.
[01:08:44.740 --> 01:08:48.740]   I'm going to be a little bit nervous.
[01:08:48.740 --> 01:08:53.740]   I'm going to be a little bit nervous.
[01:08:53.740 --> 01:08:58.740]   I'm going to be a little bit nervous.
[01:08:58.740 --> 01:09:03.740]   I'm going to be a little bit nervous.
[01:09:03.740 --> 01:09:08.740]   I'm going to be a little bit nervous.
[01:09:08.740 --> 01:09:13.740]   I'm going to be a little bit nervous.
[01:09:13.740 --> 01:09:17.740]   I'm going to be a little bit nervous.
[01:09:17.740 --> 01:09:22.740]   I'm going to be a little bit nervous.
[01:09:22.740 --> 01:09:27.740]   I'm going to be a little bit nervous.
[01:09:27.740 --> 01:09:32.740]   I'm going to be a little bit nervous.
[01:09:32.740 --> 01:09:37.740]   I'm going to be a little bit nervous.
[01:09:37.740 --> 01:09:42.740]   I'm going to be a little bit nervous.
[01:09:42.740 --> 01:09:46.740]   I'm going to be a little bit nervous.
[01:09:46.740 --> 01:09:51.740]   I'm going to be a little bit nervous.
[01:09:51.740 --> 01:09:56.740]   I'm going to be a little bit nervous.
[01:09:56.740 --> 01:10:01.740]   I'm going to be a little bit nervous.
[01:10:01.740 --> 01:10:06.740]   I'm going to be a little bit nervous.
[01:10:06.740 --> 01:10:11.740]   I'm going to be a little bit nervous.
[01:10:11.740 --> 01:10:15.740]   I'm going to be a little bit nervous.
[01:10:15.740 --> 01:10:20.740]   I'm going to be a little bit nervous.
[01:10:20.740 --> 01:10:25.740]   I'm going to be a little bit nervous.
[01:10:25.740 --> 01:10:30.740]   I'm going to be a little bit nervous.
[01:10:30.740 --> 01:10:35.740]   I'm going to be a little bit nervous.
[01:10:35.740 --> 01:10:40.740]   I'm going to be a little bit nervous.
[01:10:40.740 --> 01:10:44.740]   I'm going to be a little bit nervous.
[01:10:44.740 --> 01:10:49.740]   I'm going to be a little bit nervous.
[01:10:49.740 --> 01:10:54.740]   I'm going to be a little bit nervous.
[01:10:54.740 --> 01:10:59.740]   I'm going to be a little bit nervous.
[01:10:59.740 --> 01:11:04.740]   I'm going to be a little bit nervous.
[01:11:04.740 --> 01:11:09.740]   I'm going to be a little bit nervous.
[01:11:09.740 --> 01:11:13.740]   I'm going to be a little bit nervous.
[01:11:13.740 --> 01:11:18.740]   I'm going to be a little bit nervous.
[01:11:18.740 --> 01:11:23.740]   I'm going to be a little bit nervous.
[01:11:23.740 --> 01:11:28.740]   I'm going to be a little bit nervous.
[01:11:28.740 --> 01:11:33.740]   I'm going to be a little bit nervous.
[01:11:33.740 --> 01:11:38.740]   I'm going to be a little bit nervous.
[01:11:38.740 --> 01:11:42.740]   I'm going to be a little bit nervous.
[01:11:42.740 --> 01:11:47.740]   I'm going to be a little bit nervous.
[01:11:47.740 --> 01:11:52.740]   I'm going to be a little bit nervous.
[01:11:52.740 --> 01:11:57.740]   I'm going to be a little bit nervous.
[01:11:57.740 --> 01:12:02.740]   I'm going to be a little bit nervous.
[01:12:02.740 --> 01:12:07.740]   I'm going to be a little bit nervous.
[01:12:07.740 --> 01:12:11.740]   I'm going to be a little bit nervous.
[01:12:11.740 --> 01:12:15.740]   You had to go episode for episode.
[01:12:15.740 --> 01:12:16.740]   Can I play a little bit?
[01:12:16.740 --> 01:12:19.740]   I don't do.
[01:12:19.740 --> 01:12:22.740]   We may have to blooper this.
[01:12:22.740 --> 01:12:26.740]   There's no laugh track.
[01:12:26.740 --> 01:12:29.740]   You're incapable of a real debate.
[01:12:29.740 --> 01:12:32.740]   You think you're the good guys?
[01:12:32.740 --> 01:12:37.740]   You think that gives you a pass to do random criminal acts?
[01:12:37.740 --> 01:12:40.740]   I saved an owl and I can kill a baby.
[01:12:40.740 --> 01:12:45.740]   I'm going to be a little bit nervous.
[01:12:45.740 --> 01:12:50.740]   I'm going to be a little bit nervous.
[01:12:50.740 --> 01:12:55.740]   I'm going to be a little bit nervous.
[01:12:55.740 --> 01:13:00.740]   I'm going to be a little bit nervous.
[01:13:00.740 --> 01:13:05.740]   I'm going to be a little bit nervous.
[01:13:05.740 --> 01:13:10.740]   I'm going to be a little bit nervous.
[01:13:10.740 --> 01:13:15.740]   I'm going to be a little bit nervous.
[01:13:15.740 --> 01:13:20.740]   I'm going to be a little bit nervous.
[01:13:20.740 --> 01:13:25.740]   I'm going to be a little bit nervous.
[01:13:25.740 --> 01:13:30.740]   I'm going to be a little bit nervous.
[01:13:30.740 --> 01:13:34.740]   I'm going to be a little bit nervous.
[01:13:34.740 --> 01:13:39.740]   I'm going to be a little bit nervous.
[01:13:39.740 --> 01:13:44.740]   I'm going to be a little bit nervous.
[01:13:44.740 --> 01:13:49.740]   I'm going to be a little bit nervous.
[01:13:49.740 --> 01:13:54.740]   I'm going to be a little bit nervous.
[01:13:54.740 --> 01:13:59.740]   I'm going to be a little bit nervous.
[01:13:59.740 --> 01:14:03.740]   I'm going to be a little bit nervous.
[01:14:03.740 --> 01:14:08.740]   I'm going to be a little bit nervous.
[01:14:08.740 --> 01:14:13.740]   I'm going to be a little bit nervous.
[01:14:13.740 --> 01:14:18.740]   I'm going to be a little bit nervous.
[01:14:18.740 --> 01:14:23.740]   I'm going to be a little bit nervous.
[01:14:23.740 --> 01:14:28.740]   I'm going to be a little bit nervous.
[01:14:28.740 --> 01:14:32.740]   I'm going to be a little bit nervous.
[01:14:32.740 --> 01:14:37.740]   I'm going to be a little bit nervous.
[01:14:37.740 --> 01:14:42.740]   I'm going to be a little bit nervous.
[01:14:42.740 --> 01:14:47.740]   I'm going to be a little bit nervous.
[01:14:47.740 --> 01:14:52.740]   I'm going to be a little bit nervous.
[01:14:52.740 --> 01:14:57.740]   I'm going to be a little bit nervous.
[01:14:57.740 --> 01:15:01.740]   I'm going to be a little bit nervous.
[01:15:01.740 --> 01:15:06.740]   I'm going to be a little bit nervous.
[01:15:06.740 --> 01:15:11.740]   I'm going to be a little bit nervous.
[01:15:11.740 --> 01:15:16.740]   I'm going to be a little bit nervous.
[01:15:16.740 --> 01:15:21.740]   I'm going to be a little bit nervous.
[01:15:21.740 --> 01:15:26.740]   I'm going to be a little bit nervous.
[01:15:26.740 --> 01:15:30.740]   I'm going to be a little bit nervous.
[01:15:30.740 --> 01:15:33.740]   If he looks like him, he pays $3.
[01:15:33.740 --> 01:15:36.740]   If he looks like you, he pays $4.50.
[01:15:36.740 --> 01:15:40.740]   So just out and out to discrimination.
[01:15:40.740 --> 01:15:45.740]   Are you aware how totally unfair and not okay that is?
[01:15:45.740 --> 01:15:49.740]   I'm not sure what group, but I'm not Jewish.
[01:15:49.740 --> 01:15:50.740]   I'm gay.
[01:15:50.740 --> 01:15:53.740]   Here's the thing, you're getting more for your money than he is.
[01:15:53.740 --> 01:15:54.740]   How so?
[01:15:54.740 --> 01:15:57.740]   Because, well, see, you come here and you make fun of the place
[01:15:57.740 --> 01:16:01.740]   because it's an old Brooklyn dive bar so you and your friends get to enjoy that part of it.
[01:16:01.740 --> 01:16:04.740]   And then also you get to have a beer, but he just gets the beer.
[01:16:04.740 --> 01:16:09.740]   So you're here ironically, but he's really here because he just lives on the camera.
[01:16:09.740 --> 01:16:12.740]   So it's like a douche tax?
[01:16:12.740 --> 01:16:14.740]   Yeah, kind of.
[01:16:14.740 --> 01:16:16.740]   See, I love them.
[01:16:16.740 --> 01:16:17.740]   That is the greatest.
[01:16:17.740 --> 01:16:18.740]   That's Louis C.K.
[01:16:18.740 --> 01:16:19.740]   You can hear it.
[01:16:19.740 --> 01:16:20.740]   That's his humor.
[01:16:20.740 --> 01:16:23.740]   That's, you know, he tells it like it is.
[01:16:23.740 --> 01:16:27.740]   So the chat room says you can get all 10 episodes for $31.
[01:16:27.740 --> 01:16:29.740]   I'm buying them all just to support it.
[01:16:29.740 --> 01:16:30.740]   Yeah, just play.
[01:16:30.740 --> 01:16:31.740]   Leo.
[01:16:31.740 --> 01:16:38.740]   Yeah, because it's not that it's frankly not that entertaining, but still spend the money so that you support it.
[01:16:38.740 --> 01:16:40.740]   I want to support it exactly.
[01:16:40.740 --> 01:16:41.740]   It's the same thing.
[01:16:41.740 --> 01:16:42.740]   It's why people support us.
[01:16:42.740 --> 01:16:48.740]   It's not that good, but I want to, you know, I'd like to make sure Leo can keep doing that.
[01:16:48.740 --> 01:16:50.740]   Right, you are, Leo.
[01:16:50.740 --> 01:16:51.740]   Right, you are.
[01:16:51.740 --> 01:16:56.740]   WebRTC is coming to what?
[01:16:56.740 --> 01:16:57.740]   Apple?
[01:16:57.740 --> 01:16:59.740]   Well, that would be nice.
[01:16:59.740 --> 01:17:07.740]   Yeah, Apple, which of course famously said FaceTime's open and it's an open platform and someday we'll tell you how it works.
[01:17:07.740 --> 01:17:17.740]   Never opened it up, but they have now said that they are going to place WebRTC into development for WebKit, the engine that powers the Safari browser.
[01:17:17.740 --> 01:17:21.740]   That is fabulous news.
[01:17:21.740 --> 01:17:22.740]   WebRTC.
[01:17:22.740 --> 01:17:25.740]   I don't, I think of it as a Google technology.
[01:17:25.740 --> 01:17:27.740]   It's not, right?
[01:17:27.740 --> 01:17:28.740]   It's open.
[01:17:28.740 --> 01:17:29.740]   Yeah.
[01:17:29.740 --> 01:17:30.740]   It's an open technology.
[01:17:30.740 --> 01:17:33.740]   Google kind of has been promoting it.
[01:17:33.740 --> 01:17:35.740]   And of course Hangouts uses it.
[01:17:35.740 --> 01:17:37.740]   WebRTC, right?
[01:17:37.740 --> 01:17:40.740]   I believe Kevin Kevin is our expert on codecs.
[01:17:40.740 --> 01:17:42.740]   Yeah, Hangouts has switched to using it.
[01:17:42.740 --> 01:17:48.740]   Hangouts used an installable thing you had to put in for a while, but it will use a WebRTC when it can.
[01:17:48.740 --> 01:17:52.740]   And I think it still makes you install something on Safari.
[01:17:52.740 --> 01:17:53.740]   I'm not sure.
[01:17:53.740 --> 01:17:54.740]   Yeah.
[01:17:54.740 --> 01:17:55.740]   Well, it won't have to going forward.
[01:17:55.740 --> 01:17:56.740]   That's the point.
[01:17:56.740 --> 01:18:01.740]   Now, WebRTC is not a codec, however.
[01:18:01.740 --> 01:18:04.740]   Doesn't specify the codec.
[01:18:04.740 --> 01:18:08.740]   Well, no, but then you've got to get overlapping codecs for it to work.
[01:18:08.740 --> 01:18:11.740]   So Google uses VPA, I think, right?
[01:18:11.740 --> 01:18:12.740]   Because it's unencumbered.
[01:18:12.740 --> 01:18:15.740]   Yeah, but they also support MP4.
[01:18:15.740 --> 01:18:22.740]   And MP4, which is encumbered because you have to go to the MPEG group to license it.
[01:18:22.740 --> 01:18:29.740]   It's probably what Apple will do, H.264 or something like that.
[01:18:29.740 --> 01:18:30.740]   Okay.
[01:18:30.740 --> 01:18:37.740]   So, good news, Kevin?
[01:18:37.740 --> 01:18:38.740]   He's thinking.
[01:18:38.740 --> 01:18:43.740]   I feel like we're in a Louis C.K. story.
[01:18:43.740 --> 01:18:45.740]   It seems like good news.
[01:18:45.740 --> 01:18:46.740]   It seems like good news.
[01:18:46.740 --> 01:18:48.740]   Okay, that's all I want to know.
[01:18:48.740 --> 01:18:50.740]   I know what happens to you.
[01:18:50.740 --> 01:19:01.740]   I mean, there's this continual cycling around Web standards stuff where, you know, one of the browsers will introduce something and then we want them all to do it so that we can all support that stuff and use it.
[01:19:01.740 --> 01:19:17.740]   So there was a transition recently, which is that now Chrome and Firefox and Microsoft's new browser are all on a six-week update cycle effectively, whereas Apple is on an update cycle when they release new products.
[01:19:17.740 --> 01:19:24.740]   And so there was a point where Safari was the place that had all the new stuff, and now it lags a bit because they've been doing anything.
[01:19:24.740 --> 01:19:26.740]   Either when they're releasing a new product, the reason you're always there.
[01:19:26.740 --> 01:19:31.740]   There is this purple Safari purple now, which they are getting, letting anybody use.
[01:19:31.740 --> 01:19:34.740]   I just installed it, which is their technology preview.
[01:19:34.740 --> 01:19:40.740]   So maybe Apple's becoming a little cognizant of the fact that they are not keeping up with the Joneses.
[01:19:40.740 --> 01:19:47.740]   And they were the one who made sure they were developing the access tab.
[01:19:47.740 --> 01:19:52.740]   And before you would get access, but you would get it through newer versions of the OS effectively.
[01:19:52.740 --> 01:20:00.740]   So you wouldn't get if you sign up for a program and you get the seeds for the next version of iOS and the next version of macOS, then there will be updates for in that as well.
[01:20:00.740 --> 01:20:06.740]   But yeah, there's a bunch of other web things that people are sort of waiting for Apple to add support for as well.
[01:20:06.740 --> 01:20:11.740]   I thought WebKit was kind of an open source project.
[01:20:11.740 --> 01:20:14.740]   Apple controls it.
[01:20:14.740 --> 01:20:22.740]   So WebKit is an open source project and it was the basis for Chrome, but then Chrome got...
[01:20:22.740 --> 01:20:24.740]   ...forked and became Blink.
[01:20:24.740 --> 01:20:26.740]   And so there's the now separate codebase.
[01:20:26.740 --> 01:20:29.740]   And there was always some separation of the codebase of the rendering and stuff.
[01:20:29.740 --> 01:20:36.740]   So now that basically pretty much Apple is using WebKit and I'm not sure if anyone else is building browsers based on that.
[01:20:36.740 --> 01:20:38.740]   Whereas others are building based on Chromium.
[01:20:38.740 --> 01:20:41.740]   No, WebKit is definitely used in some Linux situations.
[01:20:41.740 --> 01:20:43.740]   I know.
[01:20:43.740 --> 01:20:47.740]   I'm not sure. Not Chromium, but some Linux browsers are using it.
[01:20:47.740 --> 01:20:51.740]   And I see the libraries.
[01:20:51.740 --> 01:20:53.740]   GTK has a WebKit library.
[01:20:53.740 --> 01:20:56.740]   So I know they're still used somewhere.
[01:20:56.740 --> 01:20:58.740]   Here is definite good news.
[01:20:58.740 --> 01:21:00.740]   No doubt about Internet hyperlinks.
[01:21:00.740 --> 01:21:03.740]   Do not infringe copyright.
[01:21:03.740 --> 01:21:06.740]   Well, says it in an advisory opinion.
[01:21:06.740 --> 01:21:08.740]   Oh, look at that in Europe.
[01:21:08.740 --> 01:21:12.740]   But this tells the court that now that wouldn't be such a good idea.
[01:21:12.740 --> 01:21:16.740]   Do we're they actually considering such a nutty idea?
[01:21:16.740 --> 01:21:17.740]   Yes.
[01:21:17.740 --> 01:21:20.740]   Because, well, you know what, listen, it's already there to understand.
[01:21:20.740 --> 01:21:31.740]   If you link to copyright violations in some places, there is an argument that you're violating copyright yourself, which obviously is absurd because that everything you link to.
[01:21:31.740 --> 01:21:36.740]   Yes, there was legal discussion of this going on in some places in Europe.
[01:21:36.740 --> 01:21:45.740]   Well, yes, in the Netherlands, a Web site provided a link to an Australian site showing pictures of a Dutch celebrity taken by Playboy magazine.
[01:21:45.740 --> 01:21:49.740]   The Australian site did not have Playboy's consent to do so.
[01:21:49.740 --> 01:21:58.740]   And so I guess it was Playboy saying, "Hey, we want to go after the Dutch site because you linked to the Australian site."
[01:21:58.740 --> 01:22:02.740]   Same argument being made that if you link well, but the same argument being made as you link to the Australian site.
[01:22:02.740 --> 01:22:06.740]   Pirate Bay.
[01:22:06.740 --> 01:22:08.740]   Pirate Bay doesn't have any pirated content on it.
[01:22:08.740 --> 01:22:12.740]   It just has torrent links, which are just pieces of information that say, "Here's how to get something."
[01:22:12.740 --> 01:22:13.740]   Right.
[01:22:13.740 --> 01:22:16.740]   Now, you could make a stronger case, I guess, against that.
[01:22:16.740 --> 01:22:19.740]   Well, could you versus Web links?
[01:22:19.740 --> 01:22:20.740]   I don't know.
[01:22:20.740 --> 01:22:21.740]   I guess it's not that different.
[01:22:21.740 --> 01:22:25.740]   Because then the precedent you said is that you are responsible for the content of everything you link to.
[01:22:25.740 --> 01:22:27.740]   That's absurd on its face.
[01:22:27.740 --> 01:22:30.740]   But yes, there was discussion around that.
[01:22:30.740 --> 01:22:35.740]   And it was triggered by Pirate Bay and the lawsuits they've already had around that.
[01:22:35.740 --> 01:22:42.740]   If you think about it, though, if you're linking to a torrent of a pirated content, there really is no other.
[01:22:42.740 --> 01:22:43.740]   That's it, right?
[01:22:43.740 --> 01:22:49.740]   I mean, you're basically, I understand you're not a pirate, but you're facilitating it.
[01:22:49.740 --> 01:22:53.740]   Whereas a Web link...
[01:22:53.740 --> 01:22:54.740]   I don't know.
[01:22:54.740 --> 01:23:01.740]   I guess...
[01:23:01.740 --> 01:23:02.740]   The same place that has some cracked version of Horace and Pete could also have a Linux distribution.
[01:23:02.740 --> 01:23:05.740]   So it's not clear why somebody might be looking somewhere up.
[01:23:05.740 --> 01:23:07.740]   Not all torrent links are bad.
[01:23:07.740 --> 01:23:10.740]   In fact, I've downloaded many a Linux distribution over a bit torrent.
[01:23:10.740 --> 01:23:14.740]   In the early days of Twit, we used BitTorrent to distribute.
[01:23:14.740 --> 01:23:15.740]   Oh, I can't remember.
[01:23:15.740 --> 01:23:16.740]   That's cool.
[01:23:16.740 --> 01:23:17.740]   Yeah.
[01:23:17.740 --> 01:23:18.740]   It was actually cool.
[01:23:18.740 --> 01:23:23.740]   It was cool because you know what I like about it is the first people who downloaded it have seeded.
[01:23:23.740 --> 01:23:29.740]   So you kind of get them to agree, "Okay, yeah, I'll seed it because the more seeds you have, the more effective it is."
[01:23:29.740 --> 01:23:31.740]   And so that was really neat.
[01:23:31.740 --> 01:23:34.740]   It really felt like it was kind of crowd distributing it.
[01:23:34.740 --> 01:23:36.740]   I liked it. I would have kept doing it.
[01:23:36.740 --> 01:23:41.740]   But not enough people really had the capability and nobody really understood what was going on.
[01:23:41.740 --> 01:23:43.740]   [laughter]
[01:23:43.740 --> 01:23:45.740]   We didn't have doing just links.
[01:23:45.740 --> 01:23:46.740]   You're always a headlamp.
[01:23:46.740 --> 01:23:48.740]   Oh, I would love to distribute by it.
[01:23:48.740 --> 01:23:53.740]   In fact, we were thinking about BitTorrent's sync and all sorts of things.
[01:23:53.740 --> 01:23:59.740]   We had long conversations with the bra and the BitTorrent folks about different ways to do it.
[01:23:59.740 --> 01:24:03.740]   And certainly a portion of our audience is sophisticated enough and would love to have that.
[01:24:03.740 --> 01:24:05.740]   A significant portion, but not all of them.
[01:24:05.740 --> 01:24:06.740]   And I want everybody.
[01:24:06.740 --> 01:24:07.740]   Are they throwing you out, Jeff?
[01:24:07.740 --> 01:24:09.740]   No, I'm outside.
[01:24:09.740 --> 01:24:10.740]   Yeah, they threw me out of that.
[01:24:10.740 --> 01:24:13.740]   But I'm now outside in the food tent, which is now being torn up.
[01:24:13.740 --> 01:24:14.740]   But I'm still here.
[01:24:14.740 --> 01:24:16.740]   Well, we could wrap it up.
[01:24:16.740 --> 01:24:17.740]   You've done a yeoman job.
[01:24:17.740 --> 01:24:21.740]   The tent may come down on my head, but by God, I'm still here.
[01:24:21.740 --> 01:24:24.740]   You and your bony butt are not going anywhere.
[01:24:24.740 --> 01:24:25.740]   Uh-uh.
[01:24:25.740 --> 01:24:28.740]   This week at Bony Butts.
[01:24:28.740 --> 01:24:29.740]   Good news.
[01:24:29.740 --> 01:24:35.740]   The next big thing in Silicon Valley, according to the Washington Post, poets.
[01:24:35.740 --> 01:24:36.740]   The--
[01:24:36.740 --> 01:24:37.740]   Yeah.
[01:24:37.740 --> 01:24:38.740]   What?
[01:24:38.740 --> 01:24:40.740]   Fily employment for poets for the first time this year.
[01:24:40.740 --> 01:24:43.740]   Full employment for-- what are these poets doing?
[01:24:43.740 --> 01:24:45.740]   They're writing scripts for boats.
[01:24:45.740 --> 01:24:47.740]   Ah, bot scripts.
[01:24:47.740 --> 01:24:48.740]   Yay.
[01:24:48.740 --> 01:24:51.740]   So you're going to have a poetic bot.
[01:24:51.740 --> 01:24:53.740]   Where we're going to have a poet bubble.
[01:24:53.740 --> 01:24:55.740]   [LAUGHTER]
[01:24:55.740 --> 01:24:57.740]   It could go either way.
[01:24:57.740 --> 01:25:03.740]   Well, I think Siri could use-- I think Siri could use some--
[01:25:03.740 --> 01:25:08.740]   Well, let me just-- you know that Siri is a big fan of the Game of Thrones, right?
[01:25:08.740 --> 01:25:10.740]   Do you know that?
[01:25:10.740 --> 01:25:14.740]   Is Jon Snow dead?
[01:25:14.740 --> 01:25:15.740]   Oh, you can't hear.
[01:25:15.740 --> 01:25:16.740]   I got to turn it up.
[01:25:16.740 --> 01:25:18.740]   She's very quiet.
[01:25:18.740 --> 01:25:19.740]   [LAUGHTER]
[01:25:19.740 --> 01:25:20.740]   Unlock her.
[01:25:20.740 --> 01:25:22.740]   She actually has an opinion on the subject.
[01:25:22.740 --> 01:25:23.740]   [LAUGHTER]
[01:25:23.740 --> 01:25:24.740]   Turn this way up.
[01:25:24.740 --> 01:25:25.740]   Turn off the sound.
[01:25:25.740 --> 01:25:26.740]   Let's try it again.
[01:25:26.740 --> 01:25:28.740]   Hey, Siri.
[01:25:28.740 --> 01:25:30.740]   Is Jon Snow dead?
[01:25:30.740 --> 01:25:32.740]   Well, you know what they say to death.
[01:25:32.740 --> 01:25:33.740]   Not today.
[01:25:33.740 --> 01:25:35.740]   But why would tomorrow be any better?
[01:25:35.740 --> 01:25:38.740]   Anyway, I'm not exactly sure.
[01:25:38.740 --> 01:25:40.740]   But is Jon Snow dead?
[01:25:40.740 --> 01:25:42.740]   [LAUGHTER]
[01:25:42.740 --> 01:25:43.740]   That is dead.
[01:25:43.740 --> 01:25:45.740]   Or is it what's dead may never die?
[01:25:45.740 --> 01:25:46.740]   No.
[01:25:46.740 --> 01:25:47.740]   Wait.
[01:25:47.740 --> 01:25:48.740]   Death is so terribly final.
[01:25:48.740 --> 01:25:49.740]   I give up.
[01:25:49.740 --> 01:25:51.740]   You know, I think they hired a poet.
[01:25:51.740 --> 01:25:52.740]   [LAUGHTER]
[01:25:52.740 --> 01:25:53.740]   Oh.
[01:25:53.740 --> 01:25:56.740]   Is Jon Snow dead?
[01:25:56.740 --> 01:26:00.740]   Well, you know what they say to death.
[01:26:00.740 --> 01:26:01.740]   Not today.
[01:26:01.740 --> 01:26:03.740]   But why would tomorrow be any better?
[01:26:03.740 --> 01:26:04.740]   Anyway--
[01:26:04.740 --> 01:26:06.740]   Siri, winter is coming.
[01:26:06.740 --> 01:26:07.740]   [LAUGHTER]
[01:26:07.740 --> 01:26:08.740]   Poetor.
[01:26:08.740 --> 01:26:09.740]   [LAUGHTER]
[01:26:09.740 --> 01:26:10.740]   Poetor.
[01:26:10.740 --> 01:26:11.740]   [LAUGHTER]
[01:26:11.740 --> 01:26:12.740]   That's good.
[01:26:12.740 --> 01:26:13.740]   [LAUGHTER]
[01:26:13.740 --> 01:26:14.740]   That's fun.
[01:26:14.740 --> 01:26:15.740]   [LAUGHTER]
[01:26:15.740 --> 01:26:16.740]   That's it, right?
[01:26:16.740 --> 01:26:17.740]   That's poet.
[01:26:17.740 --> 01:26:20.740]   That's the full employment for poets act of 1923.
[01:26:20.740 --> 01:26:23.740]   That's how they could get scale.
[01:26:23.740 --> 01:26:24.740]   [LAUGHTER]
[01:26:24.740 --> 01:26:25.740]   Get poet scale.
[01:26:25.740 --> 01:26:31.740]   So I once went to the Red Loaf Writers Conference in a rare moment when I was trying to be classy
[01:26:31.740 --> 01:26:32.740]   in life.
[01:26:32.740 --> 01:26:33.740]   Yeah.
[01:26:33.740 --> 01:26:36.740]   And if you wrote fiction or nonfiction, people would presume you did that for a living.
[01:26:36.740 --> 01:26:40.740]   Or if you were a poet without fail, people would say, oh, what do you do for a job?
[01:26:40.740 --> 01:26:41.740]   [LAUGHTER]
[01:26:41.740 --> 01:26:43.740]   Well, that's kind of true.
[01:26:43.740 --> 01:26:44.740]   It is.
[01:26:44.740 --> 01:26:47.740]   I don't want to say anything.
[01:26:47.740 --> 01:26:48.740]   So--
[01:26:48.740 --> 01:26:53.740]   So there is a bot that has poetry.
[01:26:53.740 --> 01:26:59.740]   If you go to confetti.cool, this is a bot that the known guy has put together.
[01:26:59.740 --> 01:27:05.740]   You connect to your flat channel and it sends you news posts in poet poem form.
[01:27:05.740 --> 01:27:06.740]   Oh.
[01:27:06.740 --> 01:27:08.740]   And I'm not quite sure why they built this, but it's kind of cute.
[01:27:08.740 --> 01:27:13.740]   I think the point of it was to demonstrate that they've got a bot kit that you can use to send news to Slack.
[01:27:13.740 --> 01:27:20.740]   Confetti.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.cool.
[01:27:20.740 --> 01:27:21.740]   I'm probably spelling confetti wrong.
[01:27:21.740 --> 01:27:22.740]   No, I'm not.
[01:27:22.740 --> 01:27:23.740]   I'm part Italian.
[01:27:23.740 --> 01:27:24.740]   [LAUGHTER]
[01:27:24.740 --> 01:27:25.740]   A very serious news service.
[01:27:25.740 --> 01:27:27.740]   Oh, it's a Slack bot.
[01:27:27.740 --> 01:27:29.740]   A lot of the best bots are written for Slack.
[01:27:29.740 --> 01:27:32.740]   I hope some of these will get migrated over to Facebook.
[01:27:32.740 --> 01:27:35.740]   Have you looked Kevin and how hard it is to write a Facebook bot?
[01:27:35.740 --> 01:27:36.740]   Is it difficult?
[01:27:36.740 --> 01:27:37.740]   It doesn't look that hard.
[01:27:37.740 --> 01:27:38.740]   Yeah.
[01:27:38.740 --> 01:27:43.740]   It looks similar to the Slack bot thing, which is like, here's the prompt and here's your response.
[01:27:43.740 --> 01:27:45.740]   And they have an author-age tool.
[01:27:45.740 --> 01:27:52.740]   They've got an authoring tool where you can give it a set of conversations and it will try and extract it for you or something.
[01:27:52.740 --> 01:27:53.740]   I haven't had it with that sort of.
[01:27:53.740 --> 01:27:55.740]   I just saw it this morning for the first time.
[01:27:55.740 --> 01:27:58.740]   This is a bot engine based on wit.ai.
[01:27:58.740 --> 01:27:59.740]   Is that what it is?
[01:27:59.740 --> 01:28:00.740]   Yes.
[01:28:00.740 --> 01:28:01.740]   Yeah.
[01:28:01.740 --> 01:28:02.740]   Telegram has that too.
[01:28:02.740 --> 01:28:07.740]   I have a bot on Telegram and I didn't have to do a liquor code.
[01:28:07.740 --> 01:28:12.740]   So that makes it very easy to do stuff that--
[01:28:12.740 --> 01:28:15.740]   So the advantage of it is it's push.
[01:28:15.740 --> 01:28:19.740]   So a thousand people follow my bot on Telegram.
[01:28:19.740 --> 01:28:22.740]   I don't know why, but you get a push.
[01:28:22.740 --> 01:28:24.740]   When a new show comes out, you get a push when I arrive at the old--
[01:28:24.740 --> 01:28:25.740]   It's still you.
[01:28:25.740 --> 01:28:26.740]   You arrived.
[01:28:26.740 --> 01:28:27.740]   Yeah.
[01:28:27.740 --> 01:28:31.740]   So I guess that's the point is it's push, right?
[01:28:31.740 --> 01:28:33.740]   So easy a poet could do it.
[01:28:33.740 --> 01:28:36.740]   Easy a poet could do it.
[01:28:36.740 --> 01:28:37.740]   So build bot's easy.
[01:28:37.740 --> 01:28:39.740]   You tell us what your user said.
[01:28:39.740 --> 01:28:41.740]   We tell you what your bot should do next.
[01:28:41.740 --> 01:28:42.740]   Oh, this is pretty easy.
[01:28:42.740 --> 01:28:43.740]   There's no coding here.
[01:28:43.740 --> 01:28:44.740]   Right.
[01:28:44.740 --> 01:28:46.740]   I'm going to have to try it.
[01:28:46.740 --> 01:28:49.740]   This is wit.ai and Facebook acquired him.
[01:28:49.740 --> 01:28:52.740]   Oh, I'm definitely making a Leo bot.
[01:28:52.740 --> 01:28:56.740]   Oh, this looks good.
[01:28:56.740 --> 01:28:59.740]   All right.
[01:28:59.740 --> 01:29:00.740]   All right.
[01:29:00.740 --> 01:29:04.740]   So you can attach this to the direct messaging thing.
[01:29:04.740 --> 01:29:06.740]   That link and then people talk to that.
[01:29:06.740 --> 01:29:10.740]   Well, you know, the Telegram bot gets its content from Twitter.
[01:29:10.740 --> 01:29:14.740]   So I have a Twitter bot and it gets its content from other--
[01:29:14.740 --> 01:29:16.740]   from if this than that and zappier.
[01:29:16.740 --> 01:29:18.740]   And then it tells the Telegram bot what to do.
[01:29:18.740 --> 01:29:21.740]   And there's a little latency, but it's not too bad.
[01:29:21.740 --> 01:29:25.740]   I have it set up so that in my automatic, which is that OBD2 thing,
[01:29:25.740 --> 01:29:29.740]   when I pull up in the vicinity of the studio, the automatic tweets,
[01:29:29.740 --> 01:29:32.740]   no, the automatic goes if this than that, if this than that tweets,
[01:29:32.740 --> 01:29:36.740]   and then the Twitter gets pulled into the Telegram bot.
[01:29:36.740 --> 01:29:38.740]   It's curdle bots all the way down.
[01:29:38.740 --> 01:29:40.740]   It's tevots all the way down.
[01:29:40.740 --> 01:29:42.740]   And that cuts so great to see you.
[01:29:42.740 --> 01:29:44.740]   What are you up to these days?
[01:29:44.740 --> 01:29:46.740]   Are you still on your 30-day challenge?
[01:29:46.740 --> 01:29:48.740]   Was it a food challenge?
[01:29:48.740 --> 01:29:49.740]   Always.
[01:29:49.740 --> 01:29:50.740]   So what's it?
[01:29:50.740 --> 01:29:51.740]   It's such fun.
[01:29:51.740 --> 01:29:54.740]   April-- March was so much fun.
[01:29:54.740 --> 01:29:57.740]   March was I'm trying to exercise not just once a day, but twice a day.
[01:29:57.740 --> 01:29:58.740]   Oh, wow.
[01:29:58.740 --> 01:30:01.740]   I decided to double it up and repeat it.
[01:30:01.740 --> 01:30:04.740]   So I'm doing the same 30-day challenge back to back because winter,
[01:30:04.740 --> 01:30:06.740]   I didn't get out as much as I wanted.
[01:30:06.740 --> 01:30:08.740]   And I'm like, oh, I got to do some cleanup.
[01:30:08.740 --> 01:30:10.740]   So it's been going really well.
[01:30:10.740 --> 01:30:12.740]   I thought-- Oh, you scared me.
[01:30:12.740 --> 01:30:14.740]   I thought you were going to do four times a day.
[01:30:14.740 --> 01:30:15.740]   Oh, no.
[01:30:15.740 --> 01:30:16.740]   No, no.
[01:30:16.740 --> 01:30:18.740]   Just continue forward at times.
[01:30:18.740 --> 01:30:20.740]   It's an experience you're doubling.
[01:30:20.740 --> 01:30:21.740]   Yeah.
[01:30:21.740 --> 01:30:23.740]   Next time I'm on the show, I'll just have a treasure.
[01:30:23.740 --> 01:30:27.740]   I'll just have a treadmill, and I'll just be doing a show on the treadmill nonstop.
[01:30:27.740 --> 01:30:31.740]   I've been trying to make this resolution that I will not watch TV unless I'm on the treadmill.
[01:30:31.740 --> 01:30:33.740]   Oh, come on.
[01:30:33.740 --> 01:30:35.740]   No, that was a good one.
[01:30:35.740 --> 01:30:37.740]   I think that's really good.
[01:30:37.740 --> 01:30:39.740]   Should I do that for 30 days?
[01:30:39.740 --> 01:30:40.740]   Yeah.
[01:30:40.740 --> 01:30:47.740]   Both you make more creative.
[01:30:47.740 --> 01:30:48.740]   We didn't mention that.
[01:30:48.740 --> 01:30:50.740]   I find that incredibly annoying.
[01:30:50.740 --> 01:30:51.740]   Yes.
[01:30:51.740 --> 01:30:54.740]   Just like you bragging about TV.
[01:30:54.740 --> 01:30:56.740]   You're going to watch a treadmill.
[01:30:56.740 --> 01:30:58.740]   Come on.
[01:30:58.740 --> 01:31:03.740]   So what Google's new calendar bought does is looks for free time.
[01:31:03.740 --> 01:31:04.740]   You tell it what your goals are.
[01:31:04.740 --> 01:31:06.740]   Actually, do I have it or not?
[01:31:06.740 --> 01:31:09.740]   I never forget it to do again.
[01:31:09.740 --> 01:31:10.740]   No, that's different.
[01:31:10.740 --> 01:31:12.740]   That's the reminders thing.
[01:31:12.740 --> 01:31:17.740]   The blog post has some good GIFs.
[01:31:17.740 --> 01:31:18.740]   Okay.
[01:31:18.740 --> 01:31:21.740]   Of people adding stuff to the calendar.
[01:31:21.740 --> 01:31:27.740]   So you can see you want to work out three times a week for an hour in the evening.
[01:31:27.740 --> 01:31:30.740]   It shows on which is in the rundown.
[01:31:30.740 --> 01:31:36.740]   The GIFs show how it will fit those into slots in your calendar so that you work on important
[01:31:36.740 --> 01:31:38.740]   stuff, not just urgent stuff.
[01:31:38.740 --> 01:31:43.740]   So you tell it kind of what your goals are in life, how often you want to work out.
[01:31:43.740 --> 01:31:45.740]   This is what I don't like.
[01:31:45.740 --> 01:31:49.740]   It looks for free time in your calendar and it sticks it in there.
[01:31:49.740 --> 01:31:51.740]   That's super cool.
[01:31:51.740 --> 01:31:52.740]   Are you guys kidding?
[01:31:52.740 --> 01:31:53.740]   This is awesome.
[01:31:53.740 --> 01:31:54.740]   I do you.
[01:31:54.740 --> 01:31:59.740]   I do it.
[01:31:59.740 --> 01:32:00.740]   How much time?
[01:32:00.740 --> 01:32:01.740]   I mean, I'm sorry.
[01:32:01.740 --> 01:32:03.740]   I spend like three hours a day surfing the web.
[01:32:03.740 --> 01:32:07.740]   If I spend an hour on a treadmill while I'm surfing the web, that's a good outcome.
[01:32:07.740 --> 01:32:08.740]   Oh, God.
[01:32:08.740 --> 01:32:12.540]   Oh, just when I want my phone to nag me.
[01:32:12.540 --> 01:32:15.260]   What happens when you don't do it?
[01:32:15.260 --> 01:32:16.740]   It reschedule it.
[01:32:16.740 --> 01:32:17.740]   Oh, God.
[01:32:17.740 --> 01:32:18.740]   Yeah.
[01:32:18.740 --> 01:32:19.740]   It's painful.
[01:32:19.740 --> 01:32:20.740]   It's painful.
[01:32:20.740 --> 01:32:22.740]   It's painful.
[01:32:22.740 --> 01:32:28.500]   And you can defer, edit or complete your goals and then it will choose even better times
[01:32:28.500 --> 01:32:29.500]   in the future.
[01:32:29.500 --> 01:32:30.500]   It's 2 AM.
[01:32:30.500 --> 01:32:31.500]   Why aren't you running?
[01:32:31.500 --> 01:32:32.500]   Run.
[01:32:32.500 --> 01:32:35.740]   Now, Matt, do we have to double exercise?
[01:32:35.740 --> 01:32:37.740]   Because he's actually last month's exercise.
[01:32:37.740 --> 01:32:39.740]   Go over to his calendar.
[01:32:39.740 --> 01:32:40.740]   Yeah.
[01:32:40.740 --> 01:32:41.740]   Yeah.
[01:32:41.740 --> 01:32:42.740]   I'm sorry.
[01:32:42.740 --> 01:32:43.740]   Got a catch up.
[01:32:43.740 --> 01:32:45.740]   By the way, this is their 10th anniversary widget.
[01:32:45.740 --> 01:32:48.300]   They turned 10 on April 12th.
[01:32:48.300 --> 01:32:49.940]   Happy birthday Google Calendar.
[01:32:49.940 --> 01:32:51.460]   And I live on Google Calendar.
[01:32:51.460 --> 01:32:53.740]   I have never found anything better.
[01:32:53.740 --> 01:32:54.740]   It's just fantastic.
[01:32:54.740 --> 01:32:56.100]   Oh, you know what?
[01:32:56.100 --> 01:32:59.900]   I finally switched over to inbox because I couldn't use it at first because, of course,
[01:32:59.900 --> 01:33:02.820]   of my usual rant about apps accounts.
[01:33:02.820 --> 01:33:04.740]   And then I was kind of scared to use it on the show.
[01:33:04.740 --> 01:33:07.220]   I got really frightened by it and I ran away.
[01:33:07.220 --> 01:33:08.220]   I love it now.
[01:33:08.220 --> 01:33:09.220]   Oh, God.
[01:33:09.220 --> 01:33:10.220]   I waited too long.
[01:33:10.220 --> 01:33:11.220]   It's great.
[01:33:11.220 --> 01:33:12.220]   Inbox is awesome.
[01:33:12.220 --> 01:33:13.220]   Yeah.
[01:33:13.220 --> 01:33:14.220]   Yeah, it's really nice.
[01:33:14.220 --> 01:33:17.420]   Jeff, did you have a number you'd like to share with us?
[01:33:17.420 --> 01:33:18.420]   Oh, no.
[01:33:18.420 --> 01:33:21.820]   I have to go to my phone here so I can see it because of my--
[01:33:21.820 --> 01:33:23.820]   You can't do it on your pixel.
[01:33:23.820 --> 01:33:25.900]   My pixel seed.
[01:33:25.900 --> 01:33:27.460]   God knows what will happen.
[01:33:27.460 --> 01:33:29.660]   I'll shush both of you.
[01:33:29.660 --> 01:33:30.660]   Shush.
[01:33:30.660 --> 01:33:33.260]   So there's a calculation.
[01:33:33.260 --> 01:33:39.660]   We're taking it from what it's worth that says that crappy ad tech costs iOS users alone
[01:33:39.660 --> 01:33:44.460]   $8 billion a year.
[01:33:44.460 --> 01:33:49.540]   Just pulling up pop-ups and weird ads and just junk you don't want.
[01:33:49.540 --> 01:33:50.540]   All this.
[01:33:50.540 --> 01:33:51.700]   Why ad blocking?
[01:33:51.700 --> 01:33:52.700]   Why the need for AMP?
[01:33:52.700 --> 01:33:54.860]   Why the need for instant articles?
[01:33:54.860 --> 01:33:55.860]   Here you go.
[01:33:55.860 --> 01:33:56.860]   Wow.
[01:33:56.860 --> 01:34:03.580]   I'd say a user who uses ad block in the US on iOS could save $7.19 a month in excess
[01:34:03.580 --> 01:34:04.580]   bandwidth fees.
[01:34:04.580 --> 01:34:07.660]   Matt, I didn't get to ask you for a tip.
[01:34:07.660 --> 01:34:10.940]   I see you have a few in here besides the Google workout.
[01:34:10.940 --> 01:34:11.940]   Yeah.
[01:34:11.940 --> 01:34:13.940]   Do you want something that's happy or sad?
[01:34:13.940 --> 01:34:14.940]   Happy, please.
[01:34:14.940 --> 01:34:16.420]   All right.
[01:34:16.420 --> 01:34:22.700]   So if you go on a mobile phone and you do a search for something like Europe destinations,
[01:34:22.700 --> 01:34:26.340]   it will show you all kinds of cool places you can go.
[01:34:26.340 --> 01:34:31.060]   This came out like a few weeks ago, but I didn't hear it covered on the way.
[01:34:31.060 --> 01:34:32.060]   No, we didn't.
[01:34:32.060 --> 01:34:37.100]   And you can basically select somewhere like Rome and it will show you places to go and
[01:34:37.100 --> 01:34:40.260]   typical tourist attractions and stuff like that.
[01:34:40.260 --> 01:34:45.340]   So it works well with broad queries like Europe destinations or Canada destinations,
[01:34:45.340 --> 01:34:46.900]   that sort of stuff.
[01:34:46.900 --> 01:34:53.860]   But it kind of shows you like suggested itineraries, 72 hours in Rome, day one, day two, day three.
[01:34:53.860 --> 01:34:58.420]   It looks at what a lot of people recommend, Colosseum, Pantheon, and it's really kind
[01:34:58.420 --> 01:34:59.420]   of nice.
[01:34:59.420 --> 01:35:02.980]   So if you just, you want to take a trip, you don't know exactly where.
[01:35:02.980 --> 01:35:07.180]   This shows you the kinds of places you can go and the sorts of things that a lot of other
[01:35:07.180 --> 01:35:11.540]   people do and even can show you what, you know, trips would cost and all that sort of
[01:35:11.540 --> 01:35:12.540]   stuff.
[01:35:12.540 --> 01:35:13.540]   So it's a lot of fun to explore.
[01:35:13.540 --> 01:35:16.900]   We had also mentioned on the radio show, Johnny Jett, our travel expert, that Google
[01:35:16.900 --> 01:35:25.420]   now allows you to find better deals on flights by choosing one way flights over round flights.
[01:35:25.420 --> 01:35:27.780]   And that's built into that site.
[01:35:27.780 --> 01:35:32.660]   I suspect the ITA technology is, as would this be, the ITA something Google acquired
[01:35:32.660 --> 01:35:34.740]   a couple of years ago.
[01:35:34.740 --> 01:35:40.740]   And yeah, it turns out that especially if it's a, you have a stop, that it's often cheaper
[01:35:40.740 --> 01:35:45.540]   to get it one way and then buy the return than it is to buy the round trip.
[01:35:45.540 --> 01:35:48.780]   I always thought round trips are cheaper.
[01:35:48.780 --> 01:35:49.780]   Just a.
[01:35:49.780 --> 01:35:50.780]   Yeah.
[01:35:50.780 --> 01:35:51.780]   Yeah.
[01:35:51.780 --> 01:35:52.780]   Nice stuff.
[01:35:52.780 --> 01:35:53.780]   Thank you, Google.
[01:35:53.780 --> 01:35:55.580]   Kevin Marks got something for us.
[01:35:55.580 --> 01:35:57.780]   Well, a couple of indie web things.
[01:35:57.780 --> 01:36:01.300]   There's indie webcap Nuremberg this weekend.
[01:36:01.300 --> 01:36:02.300]   So that's...
[01:36:02.300 --> 01:36:05.220]   Are you going to have a big rally there?
[01:36:05.220 --> 01:36:07.020]   Oh, dear.
[01:36:07.020 --> 01:36:13.860]   You're going to play, you're going to play, Dickie, sorry.
[01:36:13.860 --> 01:36:16.060]   You can put people on trial, you know.
[01:36:16.060 --> 01:36:18.620]   The triumph of the link.
[01:36:18.620 --> 01:36:19.620]   So that's...
[01:36:19.620 --> 01:36:20.620]   Sorry.
[01:36:20.620 --> 01:36:21.620]   That's...
[01:36:21.620 --> 01:36:22.620]   That's...
[01:36:22.620 --> 01:36:23.620]   This is the end.
[01:36:23.620 --> 01:36:25.620]   And then there's another indie webcap.
[01:36:25.620 --> 01:36:27.980]   Who writes your material, I don't know, but it's...
[01:36:27.980 --> 01:36:29.500]   I'm going to talk to Louis C.K.
[01:36:29.500 --> 01:36:32.380]   I think I might be able to get a job as a poet.
[01:36:32.380 --> 01:36:34.940]   I think it's why you're going to have to blouse your joke.
[01:36:34.940 --> 01:36:35.940]   And then...
[01:36:35.940 --> 01:36:38.900]   What you get like there's wearing a Dusseldorf in Mac.
[01:36:38.900 --> 01:36:41.020]   No, actually, Nuremberg's a wonderful sound.
[01:36:41.020 --> 01:36:46.380]   And if you're in Nuremberg or going to Nuremberg, get one of the Nuremberg sausages.
[01:36:46.380 --> 01:36:52.300]   And then if you really want to annoy people in Nuremberg, what do you say, Jeff?
[01:36:52.300 --> 01:36:55.500]   You say the sausages are so much better in Wertzburg.
[01:36:55.500 --> 01:36:57.900]   And it'll make them crazy.
[01:36:57.900 --> 01:37:02.340]   They're all little tiny sausages.
[01:37:02.340 --> 01:37:05.180]   They're actually quite good.
[01:37:05.180 --> 01:37:06.180]   Trump size.
[01:37:06.180 --> 01:37:09.660]   For people with baby hands.
[01:37:09.660 --> 01:37:12.420]   And then what's this web mention rocks?
[01:37:12.420 --> 01:37:18.900]   So, web mention is the standard we're working on for when a site notifies on the site.
[01:37:18.900 --> 01:37:24.060]   It's done something like commented on it or replied or RSVP to something and so on.
[01:37:24.060 --> 01:37:29.860]   So web mention rocks is a site that Aaron Perraki put up as the web mentions spec is
[01:37:29.860 --> 01:37:34.380]   going through the RS3C for you to check that you support web mention properly.
[01:37:34.380 --> 01:37:36.180]   So there's a series of pages.
[01:37:36.180 --> 01:37:40.740]   So if you link to one of those test one test to click to one of those, the idea is you
[01:37:40.740 --> 01:37:43.660]   link to one of those and then web mention it and it shows your web mention there.
[01:37:43.660 --> 01:37:46.340]   So you can check support all these various ways of discovering it.
[01:37:46.340 --> 01:37:50.420]   And I'm sure known does perfectly in all eight tests.
[01:37:50.420 --> 01:37:51.420]   It does now.
[01:37:51.420 --> 01:37:54.780]   In the week, which is why we have the test.
[01:37:54.780 --> 01:37:56.140]   That's why you have the test.
[01:37:56.140 --> 01:37:57.140]   That's awesome.
[01:37:57.140 --> 01:37:58.140]   Yeah.
[01:37:58.140 --> 01:38:01.540]   But if you click through, you will see all the responses that people have done to it and
[01:38:01.540 --> 01:38:03.100]   which things they've they've they've.
[01:38:03.100 --> 01:38:04.100]   Oh, that's cool.
[01:38:04.100 --> 01:38:06.500]   It becomes a blog post of its own.
[01:38:06.500 --> 01:38:07.500]   Yes.
[01:38:07.500 --> 01:38:08.500]   Because it's what web mentions do.
[01:38:08.500 --> 01:38:11.820]   They let you send, you know, they let you send stuff on one side to another.
[01:38:11.820 --> 01:38:12.820]   There's bigger.
[01:38:12.820 --> 01:38:15.780]   This is all of this testing it out in different browsers and things.
[01:38:15.780 --> 01:38:19.060]   Different indie websites and known and.
[01:38:19.060 --> 01:38:20.060]   Perfect.
[01:38:20.060 --> 01:38:21.060]   Perfect, perfect.
[01:38:21.060 --> 01:38:23.740]   I shall I shall be testing tonight.
[01:38:23.740 --> 01:38:28.140]   So the thing is if you're we're at the state with web mention now, there's a really good
[01:38:28.140 --> 01:38:31.900]   idea starting from it because it's about to go through the RS3C standards process.
[01:38:31.900 --> 01:38:35.940]   I don't have to do anything to get web mentions and known turned on.
[01:38:35.940 --> 01:38:36.980]   No, no, just does it.
[01:38:36.980 --> 01:38:37.980]   It just does it.
[01:38:37.980 --> 01:38:40.980]   If you want it in WordPress, you've got to download and install a plugin.
[01:38:40.980 --> 01:38:41.980]   Yeah, it's the problem.
[01:38:41.980 --> 01:38:42.980]   Yeah.
[01:38:42.980 --> 01:38:43.980]   Yeah.
[01:38:43.980 --> 01:38:47.660]   And we're working on, you know, trying to get that a search of full plugin.
[01:38:47.660 --> 01:38:51.060]   We're working on trying to get installed by default in other sites as well.
[01:38:51.060 --> 01:38:54.060]   So we receive more of a return of the track back.
[01:38:54.060 --> 01:38:56.500]   But we've better presentation.
[01:38:56.500 --> 01:38:58.060]   I hope so.
[01:38:58.060 --> 01:39:00.060]   I hope you're ready for any spam.
[01:39:00.060 --> 01:39:02.860]   Yeah, track back really became a problem, didn't it?
[01:39:02.860 --> 01:39:04.700]   The chat bots.
[01:39:04.700 --> 01:39:05.700]   Yes.
[01:39:05.700 --> 01:39:08.220]   Well, yeah, we got some ideas on that.
[01:39:08.220 --> 01:39:10.900]   But yes, we were always ready to listen to your thoughts on it.
[01:39:10.900 --> 01:39:15.060]   The difference is, of course, that we now are, you know, prepared for it.
[01:39:15.060 --> 01:39:18.220]   I think what track, you know, when blogging was first starting, we didn't have these
[01:39:18.220 --> 01:39:19.220]   problems.
[01:39:19.220 --> 01:39:20.900]   Ah, the innocence.
[01:39:20.900 --> 01:39:21.900]   We were innocent.
[01:39:21.900 --> 01:39:23.180]   Now we are not so innocent.
[01:39:23.180 --> 01:39:24.180]   We know.
[01:39:24.180 --> 01:39:25.180]   Oh.
[01:39:25.180 --> 01:39:26.180]   Shucks.
[01:39:26.180 --> 01:39:29.740]   Jeff, thank you for sitting your bony ass down there.
[01:39:29.740 --> 01:39:33.980]   The beautiful Fort, whatever it is.
[01:39:33.980 --> 01:39:34.980]   Mason Mason.
[01:39:34.980 --> 01:39:36.540]   Thank you.
[01:39:36.540 --> 01:39:38.100]   I love Fort Mason.
[01:39:38.100 --> 01:39:39.500]   Oh, this is what it is.
[01:39:39.500 --> 01:39:43.020]   Oh, he looks like he's in pain.
[01:39:43.020 --> 01:39:45.940]   You'll find Jeff at Buzzmachine.com.
[01:39:45.940 --> 01:39:50.380]   He also is the author of many great books, including Geeks, Bearing Gifts, his latest Gutenberg,
[01:39:50.380 --> 01:39:54.460]   the geek, public parts, and what would Google do?
[01:39:54.460 --> 01:39:57.900]   And if you're lucky enough to be going to CUNY, you should go over there to the CUNY
[01:39:57.900 --> 01:40:03.220]   School of Journalism and just sit at the foot of the master.
[01:40:03.220 --> 01:40:04.900]   Your students are lucky, lucky people.
[01:40:04.900 --> 01:40:06.300]   Thank you, Jeff, for being here.
[01:40:06.300 --> 01:40:08.260]   Next week, are you going to be here or you're not?
[01:40:08.260 --> 01:40:09.260]   You're in Germany next week, right?
[01:40:09.260 --> 01:40:12.460]   I'm in Germany, but I'm going to hope to, so we should be safe for...
[01:40:12.460 --> 01:40:16.460]   We'll have an extra person as we did this week, just in case.
[01:40:16.460 --> 01:40:18.820]   Dan Lyons and Danny Sullivan are next week.
[01:40:18.820 --> 01:40:19.820]   Oh, fun.
[01:40:19.820 --> 01:40:23.020]   Oh, we're going to get the inside scoop on HubSpot.
[01:40:23.020 --> 01:40:24.020]   Good.
[01:40:24.020 --> 01:40:25.020]   Dan Lyons is...
[01:40:25.020 --> 01:40:26.020]   I saw Danny.
[01:40:26.020 --> 01:40:27.820]   He said, "I'll be on next week."
[01:40:27.820 --> 01:40:29.860]   And Sullivan, we love...
[01:40:29.860 --> 01:40:34.740]   We can ask him about sending flowers to Zuck.
[01:40:34.740 --> 01:40:35.740]   That'll be fun.
[01:40:35.740 --> 01:40:36.740]   That's next week.
[01:40:36.740 --> 01:40:38.100]   Thank you so much for being here too.
[01:40:38.100 --> 01:40:39.460]   Matt Cutts, always a thrill.
[01:40:39.460 --> 01:40:40.700]   We love having you on.
[01:40:40.700 --> 01:40:42.700]   You're just welcome anytime you want.
[01:40:42.700 --> 01:40:43.860]   Thanks for having me.
[01:40:43.860 --> 01:40:46.740]   One of the nicest guys in technology.
[01:40:46.740 --> 01:40:49.540]   It's just really great to see you.
[01:40:49.540 --> 01:40:54.300]   And of course, Kevin Marks, who is the king of the indie web.
[01:40:54.300 --> 01:40:57.020]   He's bringing the web back to the way it used to be.
[01:40:57.020 --> 01:40:59.340]   I'm not going to say the king.
[01:40:59.340 --> 01:41:01.460]   One of the citizens of the indie web, one is...
[01:41:01.460 --> 01:41:02.460]   Citizen.
[01:41:02.460 --> 01:41:03.460]   Citizen of kings.
[01:41:03.460 --> 01:41:04.460]   Oh, no, no, no.
[01:41:04.460 --> 01:41:05.820]   You are the mayor of the indie web.
[01:41:05.820 --> 01:41:06.820]   Yes.
[01:41:06.820 --> 01:41:07.820]   The rulers.
[01:41:07.820 --> 01:41:08.820]   The rulers.
[01:41:08.820 --> 01:41:09.820]   The rulers.
[01:41:09.820 --> 01:41:10.820]   The rulers of the indie...
[01:41:10.820 --> 01:41:11.820]   He's the first among equals in the indie web.
[01:41:11.820 --> 01:41:12.820]   There's the...
[01:41:12.820 --> 01:41:13.820]   There's the...
[01:41:13.820 --> 01:41:14.820]   So I'm going to split you on this.
[01:41:14.820 --> 01:41:19.100]   No, and thank goodness, because I think the open web is such a beautiful, and elegant,
[01:41:19.100 --> 01:41:22.580]   and wonderful, and important thing we want to keep it alive.
[01:41:22.580 --> 01:41:26.380]   For all the wonderful things Facebook brings us, it's not the open web.
[01:41:26.380 --> 01:41:28.380]   The open web rocks.
[01:41:28.380 --> 01:41:30.620]   I'm sure we all agree on that.
[01:41:30.620 --> 01:41:31.620]   Thanks, Kevin.
[01:41:31.620 --> 01:41:32.620]   It's great to see you.
[01:41:32.620 --> 01:41:33.620]   Good to see you.
[01:41:33.620 --> 01:41:37.980]   We do this week in Google, which really we got to call, you know, cloudy with a chance
[01:41:37.980 --> 01:41:39.980]   of meatballs or something.
[01:41:39.980 --> 01:41:43.100]   We got to change the name up a little bit, because really it's about the cloud.
[01:41:43.100 --> 01:41:45.740]   It's about everything going on in the internet today.
[01:41:45.740 --> 01:41:50.180]   We do it every Wednesday at about 1.30 p.m. Pacific for 30 Eastern.
[01:41:50.180 --> 01:41:51.180]   It's 20.30 UTC.
[01:41:51.180 --> 01:41:56.340]   If you want to watch live or be in the chatroom at irc.twit.tv, we'd love having you.
[01:41:56.340 --> 01:42:00.460]   If you can't watch live on demand, audio, and video of all of our shows is available
[01:42:00.460 --> 01:42:01.460]   at our website.
[01:42:01.460 --> 01:42:04.740]   For this show, it's twit.tv/twig.
[01:42:04.740 --> 01:42:06.100]   You'll also find it on YouTube.
[01:42:06.100 --> 01:42:11.300]   Just go to youtube.com/twit for a directory of all of our shows on YouTube, and subscribe.
[01:42:11.300 --> 01:42:16.740]   We got podcasts, podcatchers all over the place.
[01:42:16.740 --> 01:42:19.380]   There's wonderful Twit apps on every platform.
[01:42:19.380 --> 01:42:21.940]   Find your favorite apps, subscribe so you don't miss an episode.
[01:42:21.940 --> 01:42:22.940]   Thanks for being here.
[01:42:22.940 --> 01:42:25.700]   We'll see you next time on this week in Google.
[01:42:25.700 --> 01:42:26.700]   [Music]
[01:42:26.700 --> 01:42:33.700]   [Music]
[01:42:33.700 --> 01:42:40.700]   ♪ ♪

