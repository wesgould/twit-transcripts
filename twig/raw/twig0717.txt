;FFMETADATA1
title=A Book Shaped Object
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-05-25
track=717
language=English
genre=Podcast
comment=<p>Chris&\#039\; hashtag story, DeSantis Twitter Spaces FAIL, Neeva next steps</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.640]   It's time for Twig this week in Google. Wow. Wow. What a great show we have for you.
[00:00:06.640 --> 00:00:11.800]   Ants not here, so we've got Mike Elgin dialing in from Italy, Jeff Jarvis all the way from
[00:00:11.800 --> 00:00:16.960]   New Jersey, and our special guest sitting in for Stacey Higginbotham, Chris Messina, the
[00:00:16.960 --> 00:00:24.240]   inventor of the hashtag and discoverer of so many amazing technologies. We will talk
[00:00:24.240 --> 00:00:29.920]   about AI, of course, the build conference and what Microsoft announced, but AI in general,
[00:00:29.920 --> 00:00:36.400]   including Bard and a whole lot more. And while we're doing the show, we're going to listen
[00:00:36.400 --> 00:00:43.460]   in on Rhonda Santis's Twitter space where he announces his candidacy. The result may
[00:00:43.460 --> 00:00:46.440]   surprise you. It's all coming up next.
[00:00:46.440 --> 00:01:10.120]   This is Twig. This week in Google, episode 717 recorded Wednesday, May 24th, 2023, a book
[00:01:10.120 --> 00:01:17.800]   shaped object. This week in Google is brought to you by AG1 by Athletic Greens. If you're
[00:01:17.800 --> 00:01:23.560]   looking for a simpler and cost effective supplement routine, AG1 is giving away a free one year
[00:01:23.560 --> 00:01:29.360]   supply of vitamin D and five free travel packs with your first purchase of a subscription.
[00:01:29.360 --> 00:01:36.720]   Go to athleticgreens.com/twig and buy ZipRecruiter. Whether you're starting a new business or
[00:01:36.720 --> 00:01:40.800]   growing one, if you want to be successful, you need the most talented people on your
[00:01:40.800 --> 00:01:50.000]   team. That's where ZipRecruiter comes in. And right now, you can try it for free at zipprecruiter.com/twig.
[00:01:50.000 --> 00:01:58.120]   And buy Fastmail. Reclaim your privacy, boost productivity, and make email yours with Fastmail.
[00:01:58.120 --> 00:02:06.680]   Try it now free for 30 days at fastmail.com/twit.
[00:02:06.680 --> 00:02:14.120]   It's time for Twig this week in Google. And we have a couple of substitutions to announce
[00:02:14.120 --> 00:02:20.360]   at the beginning because we have brought in the understudies. Mike Elgin is here filling
[00:02:20.360 --> 00:02:25.160]   in for Ant-Pruit, taking a couple of much needed days off. Hello, Mike.
[00:02:25.160 --> 00:02:29.000]   Hello, Leo. Thank you for having me. This is really exciting.
[00:02:29.000 --> 00:02:32.200]   From Italy today. It's always from in the world.
[00:02:32.200 --> 00:02:38.160]   The Veneto Italy, Mike and the Mira. Are you getting ready for your Prosecco Gastronome
[00:02:38.160 --> 00:02:43.240]   Ant-Pruit? That starts Tuesday. The Venice Prosecco experience starts Tuesday. And that's
[00:02:43.240 --> 00:02:47.200]   going to be in this general area, the Prosecco Hills and stuff like that.
[00:02:47.200 --> 00:02:51.880]   I was telling you, my whole life, when people asked, I said, "Oh, yeah, I'm Genovese because
[00:02:51.880 --> 00:02:58.880]   I thought my grandmother is from Genoa. So you know, my Italian roots. I'm Genovese."
[00:02:58.880 --> 00:03:03.840]   And then my sister the other day, I said, "We were in Genoa." And I remember that's
[00:03:03.840 --> 00:03:12.200]   where the family villa was. She said, "You're not Genovese." She said, "My grandmother's
[00:03:12.200 --> 00:03:19.160]   stepfather was Genovese, Jacobi." But she said, "Grandma was born." Actually, not
[00:03:19.160 --> 00:03:24.400]   even. No, no, we call her. "Was born in Schio in the Veneto, very near where you are."
[00:03:24.400 --> 00:03:25.400]   Yes.
[00:03:25.400 --> 00:03:30.480]   And all this time, I'm Venetian. And I didn't know. That's right. You're Venetian. You're
[00:03:30.480 --> 00:03:34.120]   a child of the Venetian Empire, which is not bad.
[00:03:34.120 --> 00:03:37.360]   Yeah. Your grandmother has curtains just like those behind my...
[00:03:37.360 --> 00:03:42.800]   I love those. That might be your grandma's curtains. It could be my grandma's curtains.
[00:03:42.800 --> 00:03:48.280]   It could be. That's Jeff Jarvis, the director of the Townite Center for Entrepreneurial Journalism
[00:03:48.280 --> 00:03:50.600]   at the Craig. Go ahead. Go ahead. Go ahead. Go ahead. Go ahead. Go ahead. Go ahead.
[00:03:50.600 --> 00:03:55.120]   Singers. Jump your cue. Craig Newmark Graduate School of Journalism at the City University
[00:03:55.120 --> 00:03:58.400]   of New York. Hi, Jeff. His new book, The Real World Brothers.
[00:03:58.400 --> 00:04:02.640]   Yeah, Lisa said, "Oh, his book is here. That's the hardcover. That's not the galleys. That's
[00:04:02.640 --> 00:04:04.880]   the real deal." No, that's the real thing.
[00:04:04.880 --> 00:04:10.560]   GutenbergPrethesis.com. No? Is that right? Yes. I think so.
[00:04:10.560 --> 00:04:13.680]   You still pre-ordering, though, because it's not officially until June. Pre-ordered until
[00:04:13.680 --> 00:04:18.560]   June 29 in the US. The ink is still wet on that one.
[00:04:18.560 --> 00:04:22.320]   Stacey Iggenbatham also has a day off, so I thought this would be a great opportunity
[00:04:22.320 --> 00:04:28.720]   to get an internet legend and man about town on the show. Chris Messina is here. Hi, Chris.
[00:04:28.720 --> 00:04:35.600]   Hey, good to see you. We've known Chris forever. Chris, one of his many claims to fame,
[00:04:35.600 --> 00:04:40.080]   he's a big, you were a big Twitter user. You actually left Twitter.
[00:04:40.080 --> 00:04:45.040]   And it's a flame of blaze of glory a couple of about a month ago.
[00:04:46.240 --> 00:04:52.480]   Sorry. But for that best known, because you introduced the hashtag, which has really
[00:04:52.480 --> 00:04:58.720]   gotten a life of its own, but you suggested using it, I guess, on Twitter as a canned search.
[00:04:58.720 --> 00:05:08.480]   Yeah. Back in 2007, Twitter was a simpler place. But yeah, we needed a place to share about what
[00:05:08.480 --> 00:05:12.400]   was going on, and most importantly, a way to filter things out that weren't relevant.
[00:05:12.400 --> 00:05:18.080]   And so the hashtag actually was meant to have both purposes. One was for adding labels,
[00:05:18.080 --> 00:05:23.200]   essentially metadata to posts so people could follow and track those topics. And then also
[00:05:23.200 --> 00:05:27.520]   my proposal, which was just a blog post. The idea was that you could mute things that you were
[00:05:27.520 --> 00:05:31.600]   not interested in, because the problem actually that we were solving for that I was trying to solve for
[00:05:31.600 --> 00:05:38.160]   was in March of that year, a bunch of us had gone down to South by Southwest. And those who are
[00:05:38.160 --> 00:05:42.080]   not at South by Southwest were really annoyed by all the tweets that were coming out about
[00:05:42.080 --> 00:05:46.480]   South by Southwest, but they're drunken antics and the rest. And so I remember this.
[00:05:46.480 --> 00:05:54.720]   It was so annoying. I do remember that. And the hashtag was if we kind of encouraged,
[00:05:54.720 --> 00:05:59.840]   if you're going to tweet about South by just hashtag S X S W. So then we can ignore it.
[00:05:59.840 --> 00:06:06.400]   We can ignore it. This was there was no functionality built into this. It's a Twitter to do this,
[00:06:06.400 --> 00:06:12.640]   of course. But Twitter at the time had something called track. So you could track a hashtag.
[00:06:12.640 --> 00:06:20.320]   That's right. So in fact, that was that was one of the core drivers of the format. Because back then,
[00:06:20.320 --> 00:06:25.280]   if you wanted to see what was going on with different topics, then and when you followed someone,
[00:06:25.280 --> 00:06:29.200]   that also meant that everything that they tweeted would get sent to your phone as a text message.
[00:06:29.200 --> 00:06:33.840]   You know, this is before there was push notifications before the iPhone really. And so
[00:06:34.880 --> 00:06:39.200]   with hashtags, you could string together a series of words and then track that series of words,
[00:06:39.200 --> 00:06:42.960]   because it was, you know, to Twitter, it just looked like a big blob of characters.
[00:06:42.960 --> 00:06:48.960]   And so that was actually one of the first use cases in October of that year. My friend Nate
[00:06:48.960 --> 00:06:53.600]   Ritter was down in San Diego and was tweeting about the wildfires that were going on. And, you know,
[00:06:53.600 --> 00:06:59.840]   it was using sand space, Diego space fire colon as a prefix to all of his tweets. But you couldn't
[00:06:59.840 --> 00:07:03.600]   track that. You know, you couldn't get updates on that specific topic. And I was like, Hey, Nate,
[00:07:03.600 --> 00:07:07.360]   this would be a great use case for using hashtags. And so he used hashtags,
[00:07:07.360 --> 00:07:11.280]   San Diego fire and people could subscribe to that. And then they saw him doing it.
[00:07:11.280 --> 00:07:15.040]   And then they started to do it, even though I didn't know why he was doing it. And that's
[00:07:15.040 --> 00:07:18.720]   basically one of the first kind of examples of citizen journalism using hashtags.
[00:07:18.720 --> 00:07:21.040]   Hey, here a huge argument for
[00:07:21.040 --> 00:07:28.080]   preserving old tweets. Because this is the tweet in August of 2007.
[00:07:28.880 --> 00:07:34.320]   How do you feel about using pound for groups as an hashtag bar camp message?
[00:07:34.320 --> 00:07:38.800]   Oh, those are the days. What's interesting is that a lot of the functionality of Twitter was,
[00:07:38.800 --> 00:07:43.840]   in fact, you know, created like this by its users, said, you know, we need an ad reply. We need a
[00:07:43.840 --> 00:07:47.920]   hashtag. Who gets credit for the ad reply? Does anybody that was r s g?
[00:07:47.920 --> 00:07:54.640]   If I recall correctly, I mean, even the retweet was a user convention. I mean, it used to be
[00:07:54.640 --> 00:08:00.480]   if you recall, like it was RT and then at username and then the content. And it was just a copy
[00:08:00.480 --> 00:08:07.920]   paste job. You know, I mean, it was so simple. Wow. Yeah. That was one of the secrets just for
[00:08:07.920 --> 00:08:12.320]   somebody who wants to build the next Twitter. One of the secrets of Twitter was it was kind of
[00:08:12.320 --> 00:08:18.720]   formless, which allowed its users to merge into it was an have emergent properties.
[00:08:20.240 --> 00:08:25.760]   And that was what made it so incredible. Sad to say, if you follow the, you know, Christmas
[00:08:25.760 --> 00:08:34.400]   scene tweets, his last one is goodbye. Yeah. So you left, in fact, you wrote an article about it.
[00:08:34.400 --> 00:08:39.840]   You find you finally, by the way, Elon Musk's tweet from March of this year.
[00:08:39.840 --> 00:08:45.760]   I hate hashtags. Of course you do, Elon. Of course you do. Of course, it's all good things.
[00:08:45.760 --> 00:08:55.040]   Of course. Hashtag goodbye, says Christmas, Sina. That's so why what was the last,
[00:08:55.040 --> 00:08:58.160]   the final stride? I don't need to ask why. What was the final straw?
[00:08:58.160 --> 00:09:05.680]   You know, it's such a strange thing. And I feel like I'm living in this, like, or I had been living
[00:09:05.680 --> 00:09:11.280]   in this weird world of gaslighting on Twitter, essentially, where it was hard to know what was
[00:09:11.280 --> 00:09:15.600]   intentionally being done to degrade the service versus what was accidental versus what was
[00:09:15.760 --> 00:09:20.560]   just sort of circumstance. And, you know, my intention along had been to give, you know,
[00:09:20.560 --> 00:09:24.240]   Elon his six months, you know, to sort of see where he was going to take the platform.
[00:09:24.240 --> 00:09:29.920]   You know, I am a Tesla owner. And so, you know, I have appreciation for, you know, his products
[00:09:29.920 --> 00:09:34.560]   and for the things he's done. And in this case, you know, I've been, the funny thing is, I was on
[00:09:34.560 --> 00:09:41.600]   Twitter since 2006. So that means that the longest relationship in my life, beyond my marriage,
[00:09:41.600 --> 00:09:46.080]   beyond my current romantic partnership or anything like that was with Twitter, okay, like, which is,
[00:09:46.080 --> 00:09:51.840]   I know, very terrible and sad thing to say. But even still, to have, you know, come up on the
[00:09:51.840 --> 00:09:59.040]   platform as the 1,186th user and to see it grow as it did meant that I wanted to see, you know,
[00:09:59.040 --> 00:10:03.440]   maybe there's some things that can be done here that only Elon Musk's character could pull off.
[00:10:03.440 --> 00:10:08.640]   And, you know, after four or five months, especially with so much focus and emphasis on
[00:10:08.640 --> 00:10:12.560]   blue checks and verification and eroding a lot of the trust on the platform,
[00:10:12.560 --> 00:10:17.760]   it just, it just seemed to me, you know, that the time had come for me to really look at and
[00:10:17.760 --> 00:10:22.560]   consider this relationship. And so what had happened was there was a person who is a sort of
[00:10:22.560 --> 00:10:29.600]   Twitter slash Elon Stan, who retweeted this 14 year old app researcher, Mima's tweet,
[00:10:29.600 --> 00:10:32.640]   didn't provide what I consider to be adequate
[00:10:32.640 --> 00:10:38.080]   credit. Essentially, this person sort of takes other people's content just like
[00:10:38.080 --> 00:10:41.120]   Elon does with memes and then sort of gets the credit and all the likes and all the
[00:10:41.120 --> 00:10:45.680]   adoration. And I was just like, you know, to do this to a 14 year old kid, you know, who was so
[00:10:45.680 --> 00:10:50.400]   positive in the world, he lives in Iran, like of all places, to take his work and then
[00:10:50.400 --> 00:10:55.520]   to share it as though it's your own and to sort of leave it ambiguous in terms of where it came from
[00:10:55.520 --> 00:11:01.760]   was sort of a like, a strike too far for me. So I called this behavior out, of course, on Twitter,
[00:11:01.760 --> 00:11:06.400]   and I was like, what the hell? This is ridiculous. This is stupid. And then gotten this back and
[00:11:06.400 --> 00:11:10.960]   forth without person in DMs. And, you know, I was like, whatever, like, I don't care, I'm leaving
[00:11:10.960 --> 00:11:16.480]   my tweet up, you know, and within an hour or two, Mima actually sent me a message and he's like,
[00:11:16.480 --> 00:11:20.080]   where did your blue check go? And I was like, what are you talking about? Oh my God. So I went to my
[00:11:20.080 --> 00:11:24.800]   account and it was gone. I was like, you got to be kidding me. This was like days before,
[00:11:24.800 --> 00:11:28.880]   like blue check again, like when they were all going to go away for everybody. And I was just,
[00:11:28.880 --> 00:11:32.400]   you know, assuming an accepting, like, I'm not going to pay for Twitter blue, I'm going to lose
[00:11:32.400 --> 00:11:37.360]   my check. It's fine. But to then lose my check, like days beforehand, just like smacked of
[00:11:37.360 --> 00:11:41.280]   such small- >> It was retribution. It was.
[00:11:41.280 --> 00:11:44.320]   >> Yeah. >> Yeah. So I'm like, you know what, like,
[00:11:44.320 --> 00:11:47.840]   I don't need to stay here. I don't need to be providing my content and the things that I do
[00:11:47.840 --> 00:11:51.280]   out in the world. Like, you know, we've had a relationship for a long time and now it's become
[00:11:51.280 --> 00:11:55.280]   abusive and I just need to leave. And so that's what happened. That's why I like it.
[00:11:55.280 --> 00:11:57.920]   >> How does it feel if you set Shiva for a while?
[00:11:57.920 --> 00:12:06.480]   >> Mm-hmm. You know, it's hard. Like, Twitter is inter-culture. Like, Twitter is, you know,
[00:12:06.480 --> 00:12:13.360]   a circulation system or circulatory system, I suppose, for the tech world. And so many
[00:12:13.360 --> 00:12:19.040]   conversations go on there that normally I would participate in. So I'm still, I don't say that
[00:12:19.040 --> 00:12:22.640]   I'm like going through like any kind of withdrawal or anything like that. You know, I have other
[00:12:22.640 --> 00:12:27.600]   platforms that I have other places where I can share, but it does feel like leaving, you know,
[00:12:27.600 --> 00:12:31.840]   the town or city where you've spent, again, I did, I've always spent years kind of growing up.
[00:12:31.840 --> 00:12:38.800]   But, you know, it's like, it's becoming something new and different and I don't need to stick around
[00:12:38.800 --> 00:12:42.320]   for that transition process. >> Do you read it at all still?
[00:12:42.320 --> 00:12:46.480]   >> I do. I mean, again, I get lots of threads sent to me. You know, I'm deep in kind of like
[00:12:46.480 --> 00:12:51.120]   the AI world right now. And so there's like the AI community really is there. They haven't really
[00:12:51.120 --> 00:12:56.240]   found another place. And so I find myself going through, but I have decided that I will just not
[00:12:56.240 --> 00:13:01.840]   post publicly. I still use DMs. You know, a lot of people use that as a primary way to get in
[00:13:01.840 --> 00:13:08.880]   touch with me. And so that's what you're doing in AI, Chris. You know, mostly right now learning,
[00:13:08.880 --> 00:13:14.960]   I would say. I've done a few angel investments here and there, but you know, I think it's like,
[00:13:14.960 --> 00:13:22.080]   I've been on Praditant for a long time, also since around 2014. And last December,
[00:13:22.080 --> 00:13:26.160]   I happened to catch Chachi PT as it was being announced and I posted it to Praditant.
[00:13:26.160 --> 00:13:29.360]   >> And you are the zeling of everything. >> Zeling!
[00:13:29.360 --> 00:13:35.600]   >> Product time we should mention, if you don't know about it, it's worth checking out is a place
[00:13:35.600 --> 00:13:42.320]   where founders go to announce their new products. And then people like Chris amplify that. And
[00:13:42.320 --> 00:13:47.840]   it's a really great place to see what's hot because of people like you. >> Totally.
[00:13:47.840 --> 00:13:52.240]   >> You like to contribute your content to people, other people.
[00:13:52.240 --> 00:13:58.800]   >> I like being useful. I like being helpful. And so, you know, for a long time, you know, like,
[00:13:58.800 --> 00:14:04.240]   so in my career, my career is very strange because I've done many things. I'm sort of a
[00:14:04.240 --> 00:14:08.320]   generalist, a maximalist. >> What did you start? You started as a designer, right?
[00:14:08.320 --> 00:14:13.680]   >> That's right. I did. Yeah. So I came to Silicon Valley in 2004 and I worked on the launch of
[00:14:13.680 --> 00:14:19.840]   Firefox. And that was my first foray into the tech world was through open source.
[00:14:19.840 --> 00:14:23.360]   You know, and as a designer in the open source world back in 2004, there was like two or three of us.
[00:14:23.360 --> 00:14:30.480]   You know? And so, I just kind of like got going there and, you know, worked on browsers,
[00:14:30.480 --> 00:14:34.960]   browser technology. I mean, even the hashtag was sort of intended to be kind of an open
[00:14:34.960 --> 00:14:39.200]   format or standard that anybody could use to build social platforms and social software.
[00:14:39.200 --> 00:14:45.760]   And so, along the way, there was some predecessors to Prodigent that would allow you to post the
[00:14:45.760 --> 00:14:49.520]   products that you used. One of them was called I Use This. And then I Use This eventually went
[00:14:49.520 --> 00:14:54.000]   away and Prodigent came out. And it was a way for me to help founders, makers, builders,
[00:14:54.000 --> 00:14:57.920]   you know, get the word out, you know? And so because I had worked with engineers and open
[00:14:57.920 --> 00:15:02.800]   source people and they're not always the best at selling themselves or marketing themselves,
[00:15:02.800 --> 00:15:06.240]   I could help translate the things that they were doing to a broader audience. And so Prodigent
[00:15:06.240 --> 00:15:10.800]   was one of the channels that I found was really useful to have interesting conversations about
[00:15:10.800 --> 00:15:15.280]   what people were building. I should mention it. It's astonishing to see the degree to which
[00:15:15.280 --> 00:15:22.240]   in generative AI tools have taken over product time. It's unbelievable. What would it really
[00:15:22.240 --> 00:15:28.720]   is 70% now? That is the number that I use. 70 to 80%. Every day, there's like some. And I think
[00:15:28.720 --> 00:15:33.040]   this is one of the things that's for people who are not too close to it or just reading the headlines,
[00:15:33.600 --> 00:15:40.400]   I think what they're missing is the degree to which developers and founders have to embrace
[00:15:40.400 --> 00:15:46.880]   this new platform and this shift. And two, the degree that they are, they are building things.
[00:15:46.880 --> 00:15:51.680]   And they are sort of taking a crack at how to use this stuff. And I think for me, as someone who's
[00:15:51.680 --> 00:15:56.800]   always worked in tech and thinks about communication and language and interfaces and product and
[00:15:56.800 --> 00:16:07.120]   design and platforms, this new shift is as big as probably the web in Web 2.0 was, if not bigger,
[00:16:07.120 --> 00:16:10.720]   simply because it changes the nature of how people interact with computers and what they expect
[00:16:10.720 --> 00:16:14.720]   computers should be able to do for them. So we are just at the very early innings and the very
[00:16:14.720 --> 00:16:22.320]   start. And watching it on product on unfold, since I launched Chatubt in December and it
[00:16:22.320 --> 00:16:26.480]   became the number one product of the year, I've just never seen a product that I've launched.
[00:16:26.720 --> 00:16:33.440]   Have that kind of like spots. Yeah. Yeah. And Chatubt is obviously very interesting.
[00:16:33.440 --> 00:16:40.800]   It's obviously making a huge impact, but it's the tools built using APIs to services like Chatubt
[00:16:40.800 --> 00:16:45.920]   is the real thing. People say, Oh, Chatubt makes errors and hallucinates, all this kind of stuff.
[00:16:45.920 --> 00:16:52.320]   That's not the point at all. It's like the dataset can be your dataset. It can be any data
[00:16:52.320 --> 00:16:56.560]   static. No, this is the thing that people don't understand. It's not a search engine.
[00:16:56.560 --> 00:17:05.280]   That's right. It's an approach to information. As Jerome Lanier said, it's a way for humans to
[00:17:05.280 --> 00:17:10.560]   collaborate on a massive scale. And so it's really the dataset, the errors, all that stuff is really
[00:17:10.560 --> 00:17:15.440]   beside the point is all these little tools of the kind you find on product hunt that are so
[00:17:15.440 --> 00:17:19.840]   fascinating and so surprising. Every day it's like, wow, I didn't think of that, that you could
[00:17:19.840 --> 00:17:25.840]   do something like that. It's pretty amazing. What's the proportion, both of your views, of
[00:17:25.840 --> 00:17:33.760]   real things versus bandwagon hopping? It's hard to tell, I know, but I'm just curious.
[00:17:33.760 --> 00:17:40.560]   It is because you'd have to first define what it means to be a real thing. And we are in the
[00:17:40.560 --> 00:17:46.000]   stage right now of experimentation. Obviously, Chatubt is a real thing. I would say, given what
[00:17:46.000 --> 00:17:50.960]   Microsoft talked about at their build conference this week, co-pilots is a new category, just like
[00:17:50.960 --> 00:17:56.400]   social media became a category. And so you can launch all sorts of co-pilots that are differentiated
[00:17:56.400 --> 00:18:02.720]   based on the dataset that they work on, based on user empathy and understanding what people's
[00:18:02.720 --> 00:18:08.160]   needs are and how sophisticated those users might be. For example, I use a number of text
[00:18:08.160 --> 00:18:13.600]   editing tools and they have all added some sort of AI in a generation. But then even within
[00:18:13.600 --> 00:18:20.080]   those components, you could have a co-pilot or AI generation within their code generation tools.
[00:18:20.080 --> 00:18:26.880]   In other words, if you can paste, let's say, mark down over some code, let's say that you put
[00:18:26.880 --> 00:18:31.360]   into a document because you're writing developer docs, well, within the code editor itself, you
[00:18:31.360 --> 00:18:37.120]   could have an AI tool that helps to debug or improve the clarity of your code. So at all these
[00:18:37.120 --> 00:18:40.560]   different levels, you just imagine that there's a thousand tiny interns running around in all
[00:18:40.560 --> 00:18:45.520]   these different text boxes. And I mean, that's kind of like where we're at. So to say, like,
[00:18:45.520 --> 00:18:47.840]   what's real and what's not, I think it's still a little bit early for that.
[00:18:47.840 --> 00:18:53.120]   Yeah. Yeah. One of the things that was kind of interesting, we covered the build keynotes
[00:18:53.120 --> 00:18:59.280]   earlier this morning and yesterday morning. And the most interesting part of the keynotes was
[00:18:59.280 --> 00:19:07.600]   actually at the very end of today's keynote when Stevie Batish came on. He's a technical fellow at
[00:19:07.600 --> 00:19:13.280]   Microsoft. He used to do hardware, now does software. And he more than anybody at Microsoft positioned
[00:19:13.280 --> 00:19:21.840]   AI's role in software going forward. He said, there's three stages we're going to see. There's
[00:19:21.840 --> 00:19:26.720]   where is AI? It's beside what you're doing. It's inside what you're doing. And then it's
[00:19:26.720 --> 00:19:30.560]   outside what you're doing. Let me explain what each of those means. Beside is kind of where we
[00:19:30.560 --> 00:19:39.120]   are now as AI as a helper, as a sidebar, being chat, minimally disruptive, most of the stuff you do
[00:19:39.120 --> 00:19:45.760]   continues as it used to, but you can use AI as a little assistant. He says, the next stage,
[00:19:45.760 --> 00:19:49.120]   and I think this is going to be interesting. And this is clearly what my Microsoft wants to be.
[00:19:49.120 --> 00:19:54.240]   Maybe they're better positioned to do this than anybody else, especially in what they did not mention,
[00:19:54.880 --> 00:20:02.080]   but inferred would be Windows 12, where AI is everywhere. It's the main scaffolding.
[00:20:02.080 --> 00:20:10.960]   He showed the very famous, or somebody showed the very famous Doug Engelbart,
[00:20:10.960 --> 00:20:17.040]   mother of all demos with the mouse and the first mouse and the pointer. And he said,
[00:20:17.040 --> 00:20:23.200]   we are now moving to the next stage where it's not mouse and pointer. And the input loop isn't
[00:20:23.200 --> 00:20:30.480]   waiting for a keystroke or a mouse stroke. It's AI, which means that the UI is going to change,
[00:20:30.480 --> 00:20:33.040]   but AI is going to be everywhere. And then the final stage,
[00:20:33.040 --> 00:20:38.240]   wait, wait, wait, wait, hold on there for one second. I still draw that picture a little more for me.
[00:20:38.240 --> 00:20:44.560]   Well, Matt, you know, one of the things we do right now with any computer, your silly Chrome book.
[00:20:44.560 --> 00:20:52.480]   Hey, hey, hey, my Linux OS in front of me, Mac OS in front of, actually, you have iPad,
[00:20:52.480 --> 00:20:55.360]   probably Mike Gelligan. I won't for
[00:20:55.360 --> 00:20:56.880]   I'm rocking a Mac. Yeah, rocking a Mac.
[00:20:56.880 --> 00:21:03.360]   Is you have applications which you launch to create a document.
[00:21:03.360 --> 00:21:10.880]   And for years, we've had this application kind of centric of view of computing. Long time,
[00:21:10.880 --> 00:21:15.440]   though, people have been talking about Chris, remember this, of course, document centric computing.
[00:21:15.440 --> 00:21:22.480]   We've tried to do this many times. But the idea is that's more how you work as a human.
[00:21:22.480 --> 00:21:27.920]   You know, humans don't say, do I need word or Excel? Or maybe we do now, but in the old days,
[00:21:27.920 --> 00:21:32.320]   you wouldn't you would say, what do I want to build? Do I need a hammer or screwdriver?
[00:21:32.320 --> 00:21:37.120]   You say, what do I need to build? And you focus on what you're building and then use various tools
[00:21:37.120 --> 00:21:45.360]   as appropriate. AI will facilitate this because it is the master tool. You'll, you'll talk to
[00:21:45.360 --> 00:21:49.680]   AI. It'll figure out what tool to use. Yeah, that's what that's what Grammarly showed.
[00:21:49.680 --> 00:21:55.600]   A few weeks ago, an interface for this would be like Fortnite, you know, where essentially,
[00:21:55.600 --> 00:22:00.240]   build kind of whatever you want. Right. And right. Like that's that's actually the problems that
[00:22:00.240 --> 00:22:05.680]   you'll have infinite choice. And the question then is how does the AI coach you into like working
[00:22:05.680 --> 00:22:11.040]   from what is, you know, an enormous space of possibility down to a set of clear either,
[00:22:11.040 --> 00:22:15.440]   you know, buttons or interactions or, you know, almost like mad lib style. Okay, like, who is
[00:22:15.440 --> 00:22:19.440]   this for? What are you trying to achieve? Like, I feel like, um, I think Google was the one that
[00:22:19.440 --> 00:22:23.440]   really writes good complaint letters, I guess, for getting refunds and things like that. Yeah.
[00:22:23.440 --> 00:22:27.360]   Yeah. Right. Yes. I mean, they've seen enough email. You know, it's good. Yes. They
[00:22:27.360 --> 00:22:31.440]   demoed that at Google AI. And they were, I know, and they said, if you really want to
[00:22:31.440 --> 00:22:36.800]   show right and angry one here, where do I send the complaint letter to Google? Yeah. Right.
[00:22:36.800 --> 00:22:40.000]   Right. There's no one there to be. Sorry. Sorry. I
[00:22:40.000 --> 00:22:42.880]   was going to run number three. Well, Fortnite's a good example, Chris or
[00:22:42.880 --> 00:22:49.920]   the master hand and Zelda, the tears of the kingdom. One of the reasons Zelda is so hot right now,
[00:22:49.920 --> 00:22:55.040]   is because you have arbitrary build capabilities. And that's transformed the game.
[00:22:55.040 --> 00:23:01.280]   It's fascinating to watch people. So this is, so I think Microsoft's what Microsoft's saying is,
[00:23:01.280 --> 00:23:06.400]   we're, they almost said this explicitly. We're a platform company. They use, they actually put
[00:23:06.400 --> 00:23:11.520]   Bill Gates famous quote up on the screen saying, you know, you know, something's a platform when
[00:23:11.520 --> 00:23:16.160]   the people use it make more money than the platform maker that the platform facilitates
[00:23:16.160 --> 00:23:23.440]   people to create new things. And they're going to even benefit more. And I think that's what
[00:23:23.440 --> 00:23:28.080]   Microsoft sees itself as is the AI platform builder. And they, for, you know, for all of the
[00:23:28.080 --> 00:23:33.040]   attention that open AI gets, let's not forget Microsoft owns nearly half of them.
[00:23:33.040 --> 00:23:39.360]   And all of it is running on Azure. So in effect, Microsoft is financing the AI revolution.
[00:23:39.360 --> 00:23:44.640]   So that's in, that's the second stage beside then inside. But the one I think is really most
[00:23:44.640 --> 00:23:51.120]   interesting is outside. And Microsoft talked a lot about disparate data sources orchestration
[00:23:51.120 --> 00:23:56.880]   across multiple apps, pulling in acting as an agent. And Chris also will remember this, Mike
[00:23:56.880 --> 00:24:04.960]   will as well. This, you know, the very famous John Scully video with the knowledge navigator and
[00:24:04.960 --> 00:24:11.280]   the little AI with the bow tie. The whole idea of it acting as your agent and pulling things in
[00:24:11.280 --> 00:24:18.000]   your Jarvis, if you will, Jeff, pulling things and from everywhere, from the, from all of the world
[00:24:18.000 --> 00:24:23.680]   and becoming your kind of intermediary to the real world. And this is what I see this is Microsoft
[00:24:24.320 --> 00:24:28.480]   saying this is what we think the future is going to be. And this is where we want to be. Now,
[00:24:28.480 --> 00:24:34.240]   whether will be or not is another matter. But another, yeah, another perspective on the, on the
[00:24:34.240 --> 00:24:39.520]   external use of AI occurred to me and it's, I'm sure it's not original thought at all, but it,
[00:24:39.520 --> 00:24:44.960]   it just sort of kind of blew me away when I was really thinking about it, which is that no matter
[00:24:44.960 --> 00:24:49.600]   where augmented reality goes, and I'm certain that augmented reality is going to be much bigger
[00:24:49.600 --> 00:24:54.320]   than virtual reality. Even if it isn't, it's going to be huge. Augmented reality is
[00:24:54.320 --> 00:24:59.600]   something we're all going to be talking about it after the Apple, WWDC next week. And
[00:24:59.600 --> 00:25:05.520]   some of us have been talking about it for a decade, but there's no augmented reality. The
[00:25:05.520 --> 00:25:12.800]   future of augmented reality is going to be heavily dependent on AI to the point where the AI,
[00:25:12.800 --> 00:25:18.160]   the AI part is the indispensable part, the visuals, the holograms, all that stuff is
[00:25:18.160 --> 00:25:23.680]   theoretically dispensable. What I mean by that is AI is going to see and interpret the world for us.
[00:25:23.680 --> 00:25:28.720]   It's going to take in all that data, figure out what's going on, and then use that information
[00:25:28.720 --> 00:25:34.320]   to then give us advice, not just here and there, answer our questions to all that kind of stuff
[00:25:34.320 --> 00:25:38.960]   with an awareness about what's going on in the world. So the, so the, the AI to interpret the
[00:25:38.960 --> 00:25:46.080]   world and then AI to figure out how to help us with the world that our surroundings is the going
[00:25:46.080 --> 00:25:50.160]   to be the most, for sure, going to be the most important part of augmented reality.
[00:25:50.160 --> 00:25:56.080]   And you almost have to have it in order to interact with the world. There'll be so much input.
[00:25:56.080 --> 00:26:03.440]   You'll need it. The filter it. The smoke and mirrors part, the visuals, the holograms, the 3D,
[00:26:03.440 --> 00:26:08.080]   you know, the magic leap stuff, well, that stuff's going to take forever. And the applications for
[00:26:08.080 --> 00:26:14.080]   that kind of thing are going to take forever. I'm ready. And I think everybody's ready for an AI
[00:26:14.080 --> 00:26:22.080]   augmented reality right now, which is just voice, right? And so I think we're like, on the brink of,
[00:26:22.080 --> 00:26:26.320]   I think this year, we're going to see a lot of things like that where we're talking to a voice,
[00:26:26.320 --> 00:26:30.960]   the voice is talking to us, and AI is interpreting the world and figuring things out for us.
[00:26:30.960 --> 00:26:35.840]   All right. Since I'm in a reality, I have at least two, if not three bullish AI bullish
[00:26:35.840 --> 00:26:41.280]   people on. I want to tell you my dirty little secret. I don't buy any of this.
[00:26:42.400 --> 00:26:54.720]   I am not, I am not, I am bearish on AI. And here's why, you know, I think about self-driving cars,
[00:26:54.720 --> 00:27:00.960]   I think about speech to text interfaces, handwriting recognition. It seems to have been always the
[00:27:00.960 --> 00:27:08.240]   case that the first 90% can be done and be very impressive. But the last 10% is such a failure
[00:27:09.040 --> 00:27:16.560]   that cars run into kids and Siri has no idea what you just asked it. And your handwriting turns
[00:27:16.560 --> 00:27:23.520]   into farm. And I think we've seen this over and over again. And I worry that we are once again
[00:27:23.520 --> 00:27:28.480]   falling for what is a hype cycle, because the first 90% is so damn impressive.
[00:27:28.480 --> 00:27:35.360]   It's not just the last 10%, the last 10% of the last 10% is almost impossible.
[00:27:35.360 --> 00:27:38.480]   Yes, it's Zeno's paradox. You're never going to get there.
[00:27:38.480 --> 00:27:44.320]   Right. So Chris, I'll let you defend this because Chris, obviously you are a believer.
[00:27:44.320 --> 00:27:48.160]   I mean, history might be on my side. Why is this going to be different?
[00:27:48.160 --> 00:27:54.000]   You know, like the way I'm looking at this is really about resetting assumptions about
[00:27:54.000 --> 00:27:59.120]   what computing is. So, you know, I often try to think about generationally,
[00:27:59.120 --> 00:28:04.560]   you know, kids coming up now who are in school, who are learning to work with chat TPT,
[00:28:04.560 --> 00:28:08.400]   will have a completely different set of assumptions about what the interaction paradigm should be
[00:28:08.400 --> 00:28:12.400]   when they pick up a device. Right. You know, already there's some kind of skepticism.
[00:28:12.400 --> 00:28:15.440]   You know, we grew up obviously and these things are still magical. Like, you know,
[00:28:15.440 --> 00:28:19.040]   those of us who, you know, choose to stay on Twitter still think that Twitter is amazing,
[00:28:19.040 --> 00:28:22.800]   whereas a younger generation is like, this is like, it's just text. Like, this is so boring.
[00:28:22.800 --> 00:28:27.280]   Where's the video? Where's the interaction? Where's the game like stuff? So, I do think that,
[00:28:27.280 --> 00:28:31.840]   you know, for a certain generation, certain cohort, certain sort of people, you know,
[00:28:31.840 --> 00:28:35.120]   the AI is not going to be that relevant to that. I mean, you know, there's a story I think today
[00:28:35.120 --> 00:28:40.240]   about how car manufacturers are trying to like take out AM radio, you know, from the cars.
[00:28:40.240 --> 00:28:43.920]   And there's some contingent of folks who are like, no, like, you know, over my dead, you know,
[00:28:43.920 --> 00:28:49.040]   body or whatever. In a similar way, I think there will be so many folks who resist what AI
[00:28:49.040 --> 00:28:56.080]   can do for you. And yet another cohort of people for whom these types of conversational interactions
[00:28:56.080 --> 00:29:00.960]   will become the de facto way that they actually compute. So, that to me is the shift and that's
[00:29:00.960 --> 00:29:07.760]   the difference. And so, we have now 30 years, you know, if not longer, maybe 40 or 50 years
[00:29:07.760 --> 00:29:14.880]   since the mother of all demos that define the computing paradigm based on changing human expression
[00:29:14.880 --> 00:29:21.520]   to meet what a computer could understand. So, moving our finger around to point to things
[00:29:21.520 --> 00:29:26.880]   through a metaphor, which was the mouse, or having, you know, a set of keys, rows of keys,
[00:29:26.880 --> 00:29:31.360]   that you have to like put your fingers on in a certain order to generate commands so the
[00:29:31.360 --> 00:29:35.680]   computer knows what to do. Now, we are finally in, you know, I built a conversational AI company
[00:29:35.680 --> 00:29:41.920]   in 2016 that was just too early with the assumption that we have built the APIs, we have built the
[00:29:41.920 --> 00:29:47.840]   platforms. What was missing was the interface that would allow kind of just in time coupling
[00:29:47.840 --> 00:29:55.840]   of those services through a conversational or language based one time. And that's to me,
[00:29:55.840 --> 00:30:01.680]   the shift that is happening and it will change everything because again, people will start building
[00:30:01.680 --> 00:30:04.960]   from those new set of assumptions. They won't be building with an assumption that, you know,
[00:30:04.960 --> 00:30:09.280]   you have to go through an app store to deliver a little chiclet on people's phones and all the
[00:30:09.280 --> 00:30:13.920]   mechanics around like ads and all that other stuff. Now, it's going to be intent based, which,
[00:30:13.920 --> 00:30:17.520]   you know, Android has been working on for some time, but you still have to have the metaphor of the
[00:30:17.520 --> 00:30:22.960]   app. And I think it just dissolves a lot of those boundaries and the way that we've organized so
[00:30:22.960 --> 00:30:26.640]   much of the digital world that that I think is what's going to fundamentally change.
[00:30:26.640 --> 00:30:32.160]   And Chris, that's the 10,000 foot view. And I, to me, the 100,000 foot view is that we've been
[00:30:32.160 --> 00:30:37.680]   using computers since we've been using electrical powered computers since the first half of the
[00:30:37.680 --> 00:30:43.280]   20th century. And in the entire evolution of computing, the way that it changes involves is
[00:30:43.280 --> 00:30:47.840]   that the computers become more powerful and they work much, much harder to speak the language of
[00:30:47.840 --> 00:30:55.360]   humans. And the language of humans ultimately is just storytelling of verbal conversations
[00:30:55.360 --> 00:31:02.960]   that we have with other people. So it was inevitable since, you know, the 1930s, that someday we would
[00:31:02.960 --> 00:31:06.880]   have computers that we just talked to and they would talk back to us and they would make sense
[00:31:06.880 --> 00:31:11.360]   and they would understand things and help us do things and do things for us and that sort of thing.
[00:31:11.360 --> 00:31:15.840]   It's always been the direction of computers. They've always evolved.
[00:31:15.840 --> 00:31:22.320]   The voice interface is that, I mean, right now we're typing that voice. It's not, it's a
[00:31:22.320 --> 00:31:28.080]   tremendous language. It's language. So it's all the things that humans do. We're visual animals.
[00:31:28.080 --> 00:31:33.840]   We're, we're, we have language that we, you know, this is, this is what we are. It's, it's,
[00:31:33.840 --> 00:31:37.520]   we're bending it to become something that's, that's pointed out.
[00:31:37.520 --> 00:31:41.600]   Ultimately, what we're sort of talking about and pointing to is, it's sense making and
[00:31:41.600 --> 00:31:45.520]   it's communication. And you're absolutely right. Like all the ways in which humans
[00:31:45.520 --> 00:31:51.840]   understand the world through our sense, sensory input, right? First has to be, you know, felt
[00:31:51.840 --> 00:31:57.120]   and experienced. And then the second part is then how do you translate those experiences verbally
[00:31:57.120 --> 00:32:02.800]   into phonemes that I can then shoot at your ears and that you can also see, you know, my movements
[00:32:02.800 --> 00:32:07.680]   and things like that to try to understand the medic, you know, things that are swirling around
[00:32:07.680 --> 00:32:11.280]   in my brain. And so that, that, that is how we create a kind of union while we are still
[00:32:11.280 --> 00:32:15.440]   boundaries. So I'm still me and you are still you. And yet as I describe these things with words,
[00:32:15.440 --> 00:32:20.720]   as I shape that reality in your mind, we then come to an understanding. And up until now,
[00:32:20.720 --> 00:32:27.840]   we've had to use a lossy type of communication style with computers to approximate what we mean
[00:32:27.840 --> 00:32:33.200]   because of the huge number amount of sort of sequential compute that's needed to get to
[00:32:33.200 --> 00:32:37.760]   something similar to what humans are talking about. Like you can give a computer, an image or a
[00:32:37.760 --> 00:32:42.800]   photograph. And you know, it's a bunch of, you know, texts, if you ever try to like open up a jpeg
[00:32:42.800 --> 00:32:47.840]   in a text editor, like it's, it's gibberish. It's nonsense. It's not concepts. But now,
[00:32:47.840 --> 00:32:54.800]   thanks to machine learning largely, we've trained the system to have a better sense for
[00:32:54.800 --> 00:32:59.920]   patterns and phenomena and to label those phenomenon with human concepts and ideas that now allow
[00:32:59.920 --> 00:33:05.040]   the computer to respond to us in a way that is much closer to the way that we understand and
[00:33:05.040 --> 00:33:09.760]   interpret and experience the world. And that changes how often and how frequently people
[00:33:09.760 --> 00:33:13.600]   want to use computers because it becomes a lot more accessible and a lot more delightful
[00:33:13.600 --> 00:33:17.120]   than the current method, which all of us are quite good at.
[00:33:17.120 --> 00:33:18.080]   Exactly.
[00:33:18.080 --> 00:33:20.800]   It also changes who can use language.
[00:33:20.800 --> 00:33:25.440]   This has been my argument about about this is that it opens broadens literacy,
[00:33:25.440 --> 00:33:32.080]   both in text and in visuals, images and in programming, people who can now communicate.
[00:33:32.080 --> 00:33:36.400]   Chris, let me ask you since since you went 10,000 feet and Mike went 100,000 feet, I'll go down to
[00:33:36.400 --> 00:33:37.680]   hell. Great.
[00:33:37.680 --> 00:33:43.920]   I'm trying, I keep on trying to look for anyone who has a credible scenario in which they argue
[00:33:43.920 --> 00:33:49.840]   that this could destroy mankind. I don't believe it, but have you seen it? What's I've never seen
[00:33:49.840 --> 00:33:56.240]   the script for that? Except, oh my God, we're all powerful. We have big shlongs and we can do
[00:33:56.240 --> 00:34:01.520]   what we want with the world. I've never seen something that takes me through the steps by which
[00:34:01.520 --> 00:34:08.880]   that happens. You know, I'm obviously there's plenty of sci-fi and movies that attempt to,
[00:34:08.880 --> 00:34:13.440]   you know, tell the story of these things. But I feel like it's a little bit lazy to just make
[00:34:13.440 --> 00:34:17.680]   this assumption that, you know, once you add plugins into chat, TPT, suddenly both the system
[00:34:17.680 --> 00:34:22.880]   is somehow sentient and also sees humans as a blocker to, you know, enablement. Like,
[00:34:22.880 --> 00:34:26.720]   one, I think, although these systems can become extremely powerful, I mean, you can have all
[00:34:26.720 --> 00:34:33.280]   sorts of different conditions that emerge from that. It feels like the real risk is almost like,
[00:34:33.280 --> 00:34:41.760]   if not a cultural decline that is caused by a set of questions about what is it that humans
[00:34:41.760 --> 00:34:46.000]   should be doing with themselves? How should we be doing work? If suddenly, you know, we have a lot
[00:34:46.000 --> 00:34:49.520]   of, you know, we already have enough income inequality in the country that suddenly now you
[00:34:49.520 --> 00:34:53.600]   have a bunch of sort of executive folks who are hiring AIs to do a bunch of, you know,
[00:34:53.600 --> 00:34:58.160]   roles throughout an organization. And that leaves a lot of people without a lot to do and without
[00:34:58.160 --> 00:35:03.520]   sort of, you know, like, like capitalism starts to break down quite a bit when so much of the work
[00:35:03.520 --> 00:35:07.600]   that currently is done by people to give them purpose and meaning and a way to contribute to
[00:35:07.600 --> 00:35:13.040]   their community, suddenly goes away and can be automated and made to be extremely cheap. You know,
[00:35:13.040 --> 00:35:17.200]   like we've like in this country, we struggled with slavery and this is a new form of slavery
[00:35:17.200 --> 00:35:22.240]   that is not morally reprehensible in the same way. It's different. And so that is something that
[00:35:22.240 --> 00:35:26.960]   what's the machine is a slave? Yes, like, essentially, if you're using chat TPT,
[00:35:26.960 --> 00:35:31.920]   let's say that you're in college, like, there's no reason necessarily for you to write an essay,
[00:35:31.920 --> 00:35:36.160]   if your goal is to get a credential and then to go into the world and to show someone this
[00:35:36.160 --> 00:35:41.680]   credential to get a job that has now been automated through automation technology.
[00:35:42.240 --> 00:35:48.560]   So instead, like the shift, and this is not exactly answering your question, but I'm suggesting that
[00:35:48.560 --> 00:35:53.280]   the thing that I'm most concerned about is that we're not doing a good enough job
[00:35:53.280 --> 00:35:57.440]   teaching the skills that will be necessary in this future world, where there is an assumption
[00:35:57.440 --> 00:36:02.320]   that, again, people are using these tools day in and day out, and it's in their pocket, right?
[00:36:02.320 --> 00:36:09.440]   The tools of empathy, of curiosity, of, you know, listening, proactive listening, non-violent
[00:36:09.440 --> 00:36:13.120]   communication, like all those things are actually quite necessary in this new world, but they're
[00:36:13.120 --> 00:36:16.320]   not the things that are taught in school. For the last 100 years or so, we've been teaching people
[00:36:16.320 --> 00:36:22.400]   how to, you know, rote memorization, facts, all that stuff. And to Mike's point, we've been building
[00:36:22.400 --> 00:36:26.960]   computers that do all that stuff for us so that we don't have to. We need to have people with
[00:36:26.960 --> 00:36:31.600]   humanities involved now. Yes. Yes. So we're sort of coming back full circle into the humanities
[00:36:31.600 --> 00:36:38.400]   and philosophy. Just as they die. Right. Exactly. So, so I think that's like, if anything, maybe it
[00:36:38.400 --> 00:36:45.760]   is that we, you know, bore ourselves without changing what education is about to encourage,
[00:36:45.760 --> 00:36:50.480]   you know, deep curiosity, and then the ability to ask better questions, these reasoning tools
[00:36:50.480 --> 00:36:54.800]   that we're building now will, I think, make a lot of people very like lazy and they'll miss the point
[00:36:54.800 --> 00:36:59.120]   of what education could and should be about in terms of personal development, personal growth,
[00:36:59.120 --> 00:37:05.440]   personal inquiry, etc. In my journalism school, I started a program with my colleague, Kerry Brown,
[00:37:05.440 --> 00:37:08.560]   Engagement Journalism. And the argument is that it doesn't start with our stories. It doesn't
[00:37:08.560 --> 00:37:14.080]   start with a journalist perspective. It starts by listening. And we're oddly bad at that in our field.
[00:37:14.080 --> 00:37:19.840]   And other fields like education and retail and government and listening is a skill that
[00:37:19.840 --> 00:37:24.000]   we haven't had. And part of the problem, part of my argument about the internet and why there's
[00:37:24.000 --> 00:37:28.720]   moral panics around it is because people who have always been there and have always been speaking,
[00:37:28.720 --> 00:37:34.800]   who were never heard in mass media now have the opportunity to be heard and resent not being
[00:37:34.800 --> 00:37:41.600]   heard. And those who held the stage before resent their voices. And so speaking and listening,
[00:37:41.600 --> 00:37:45.760]   take on a whole different definition. I like that. I like what you're headed with that.
[00:37:45.760 --> 00:37:58.800]   Where chat becomes a, what is it? Is it a curator, a middleman, a gatekeeper, a facilitator, or what
[00:37:58.800 --> 00:38:01.440]   in between? Yeah, maybe a facilitator, maybe a
[00:38:01.440 --> 00:38:08.000]   little bit more context, to be either an interpreter or just again, to ask the questions.
[00:38:08.000 --> 00:38:13.200]   Because so many people have such a diverse set of experiences and we're living in a world,
[00:38:13.200 --> 00:38:18.480]   and we're seeing this with social media now, right? Twitter, obviously, was one melting pot of a
[00:38:18.480 --> 00:38:24.000]   lot of different ideas and cultures and so on. Now we're disaggregating again to different edge
[00:38:24.000 --> 00:38:28.960]   networks and platforms. Now it's very hard having left Twitter for me to have a sense of the
[00:38:28.960 --> 00:38:33.760]   zeitgeist of what's going on. So there are little pockets of reality and truth or truthiness that
[00:38:33.760 --> 00:38:38.000]   are popping up and I'm encountering them and they're very confusing. It's very
[00:38:38.000 --> 00:38:46.640]   bewildering maybe. And so anyways, a type of AI or conversational agent that can be there to help
[00:38:46.640 --> 00:38:50.960]   me understand what's gone on before, what is the context of this place that I find myself in?
[00:38:50.960 --> 00:38:56.480]   What are some of the appropriate norms and ways to speak? Could really smooth over our movement
[00:38:56.480 --> 00:38:59.280]   through these digital environments? That's an interesting issue. Blue Sky.
[00:38:59.280 --> 00:39:02.720]   Yeah. Well, blue skies get humans. They're getting ready for agents.
[00:39:02.720 --> 00:39:06.400]   And interesting. The argument is, they're bringing in those agents next.
[00:39:06.400 --> 00:39:11.440]   I think that's interesting that to read only Twitter that is created by AI.
[00:39:11.440 --> 00:39:17.360]   But I have to say, I'm still skeptical because the content AI produces so anodyne,
[00:39:17.360 --> 00:39:24.960]   so phony at this point. And I don't think it's gonna get any better. I think there's a little bit
[00:39:24.960 --> 00:39:28.960]   of a science fiction leap that you're making that it will somehow
[00:39:28.960 --> 00:39:34.480]   achieve this fluency that I don't necessarily think.
[00:39:34.480 --> 00:39:39.840]   I think if you think about it more from a collaboration, like those who use it for collaboration,
[00:39:39.840 --> 00:39:43.520]   I think will actually sprint much further than those who don't.
[00:39:43.520 --> 00:39:46.880]   I also think that poets and novelists were doing just that.
[00:39:46.880 --> 00:39:51.280]   Yeah. Are they making better poetry and better fiction because of it?
[00:39:52.560 --> 00:39:55.040]   Yes. They're being inspired. The other way, look at Leo.
[00:39:55.040 --> 00:39:55.840]   I do. I do.
[00:39:55.840 --> 00:39:59.040]   If you want to know how to read a tree as much more inspiring,
[00:39:59.040 --> 00:40:01.920]   then the AI would ever be. But okay.
[00:40:01.920 --> 00:40:05.840]   And if you looked up a tree to AI, that would be a book I would want to read.
[00:40:05.840 --> 00:40:08.240]   The one right behind Kristoff.
[00:40:08.240 --> 00:40:09.040]   Yeah, exactly. Yeah.
[00:40:09.040 --> 00:40:12.880]   If you want to, to me, what's interesting is if you have this thing that has three trillion
[00:40:12.880 --> 00:40:18.720]   connections and language, it's telling you everything that all the cliches we've had.
[00:40:18.720 --> 00:40:20.640]   It challenges you then to go beyond them.
[00:40:21.440 --> 00:40:21.920]   That's one.
[00:40:21.920 --> 00:40:25.200]   You still have to think beyond the cliche and all you're getting is the cliche.
[00:40:25.200 --> 00:40:26.400]   The machine will.
[00:40:26.400 --> 00:40:28.080]   So you got a great cliche generation.
[00:40:28.080 --> 00:40:29.120]   It's a collaboration.
[00:40:29.120 --> 00:40:31.200]   What's the value of a cliche generator?
[00:40:31.200 --> 00:40:32.160]   That's not useful.
[00:40:32.160 --> 00:40:35.040]   Have you read any high schoolers essays?
[00:40:35.040 --> 00:40:36.640]   Yes. Exactly.
[00:40:36.640 --> 00:40:37.680]   Exactly.
[00:40:37.680 --> 00:40:38.560]   They're horrible.
[00:40:38.560 --> 00:40:38.880]   Yes.
[00:40:38.880 --> 00:40:41.280]   machines really should be doing those.
[00:40:41.280 --> 00:40:50.160]   But I think the analogy that I like so far, and we'll see if any new emerging facts
[00:40:50.720 --> 00:40:51.680]   jarge this away from me.
[00:40:51.680 --> 00:40:56.000]   But basically, I think it's very similar to photography and what it did to painting.
[00:40:56.000 --> 00:41:01.520]   Before photography, painting was something we used to record visual scenery or visual things.
[00:41:01.520 --> 00:41:05.280]   People sat for portraits and it was a kind of propaganda.
[00:41:05.280 --> 00:41:10.640]   It was a lot like stable diffusion is now, where it's kind of an idealized version of the aristocrats
[00:41:10.640 --> 00:41:14.240]   standing there in a kind of a phony environment.
[00:41:14.240 --> 00:41:18.480]   We used it for capturing faraway scenes and landscapes of places where,
[00:41:18.480 --> 00:41:21.920]   would far away from any place you'd ever go and you would look at it and go like,
[00:41:21.920 --> 00:41:23.120]   "Wow, that's what it's like there.
[00:41:23.120 --> 00:41:24.160]   That's amazing."
[00:41:24.160 --> 00:41:28.560]   And so we used the painting as a way to capture what was really there.
[00:41:28.560 --> 00:41:32.320]   As soon as we had photography that could really capture what was really there,
[00:41:32.320 --> 00:41:37.680]   well, suddenly painting just exploded into a thousand styles of surrealism and all these
[00:41:37.680 --> 00:41:39.520]   different things that we really love.
[00:41:39.520 --> 00:41:40.000]   Yeah, I like that.
[00:41:40.000 --> 00:41:47.040]   It really freed humans to be super creative and to do things that humans can do that cameras can't.
[00:41:47.040 --> 00:41:49.120]   So we no longer need to write the cliches.
[00:41:49.120 --> 00:41:51.600]   The AI will do it and we can do something more original.
[00:41:51.600 --> 00:41:57.760]   Well, I think, so for example, on the rundown is a is a missive by Sundar Pichai.
[00:41:57.760 --> 00:42:00.800]   My feeling is best Sundar Pichai who's writing in the Financial Times.
[00:42:00.800 --> 00:42:05.200]   And his entire thing feels like it was written by AI.
[00:42:05.200 --> 00:42:08.800]   I don't think it was, but it's like you say, it's anodyne.
[00:42:08.800 --> 00:42:10.240]   He's a boring writer.
[00:42:10.240 --> 00:42:12.960]   He doesn't make any mistakes.
[00:42:12.960 --> 00:42:16.560]   He doesn't, he's careful about everything. He's, everything's couched in these sort of
[00:42:16.560 --> 00:42:17.840]   platitudes and so on.
[00:42:17.840 --> 00:42:19.760]   That's how AI writes by default.
[00:42:19.760 --> 00:42:22.240]   Then you can tell it to be sassy or whatever.
[00:42:22.240 --> 00:42:24.000]   And it's not very good at those sort of things.
[00:42:24.000 --> 00:42:25.920]   It's not very good at summary for the most part.
[00:42:25.920 --> 00:42:29.360]   But again, this stuff has been with us since January.
[00:42:29.360 --> 00:42:33.520]   I mean, give it, give it two years, three years.
[00:42:33.520 --> 00:42:39.200]   Yeah, I really think that like the seeing the younger generation come up with this as a core,
[00:42:39.200 --> 00:42:39.760]   you know, tool.
[00:42:39.760 --> 00:42:44.160]   And I think the photography example is pretty good because by and large,
[00:42:44.160 --> 00:42:50.960]   especially given all the cameras in people's pockets now and the sheer volume of terrible,
[00:42:50.960 --> 00:42:55.760]   terrible photos that have been produced as a result of putting cameras in everybody's pockets
[00:42:55.760 --> 00:43:02.800]   doesn't mean that those who have real talent or vision can't then use that tool for something
[00:43:02.800 --> 00:43:03.920]   more transcendent.
[00:43:03.920 --> 00:43:11.680]   And I've seen so far, for example, musicians who maybe normally don't write vocals or lyrics to
[00:43:11.680 --> 00:43:16.000]   their songs, be able to use something like chat, TPT to just get something to get a riff,
[00:43:16.000 --> 00:43:19.760]   to get a line, to get something that sort of inspires or gets them going.
[00:43:19.760 --> 00:43:24.720]   And ultimately, if this is the best solution to the blank slate problem or the blank page problem,
[00:43:24.720 --> 00:43:29.360]   then that means that I mean, you know, I spent a walk about your kind of traveling the world,
[00:43:29.360 --> 00:43:33.360]   thinking that I was going to write a book about my experiences and I got nowhere because of this
[00:43:33.360 --> 00:43:34.640]   blank page problem.
[00:43:34.640 --> 00:43:39.840]   So if I have chat TPT or something like it, that not only harasses me into like sitting down,
[00:43:39.840 --> 00:43:43.680]   like spending, you know, an hour writing, and then also is like, okay, tell me about this or
[00:43:43.680 --> 00:43:49.520]   tell me about that. I feel like as a creative tool to extract things out of a person, then
[00:43:49.520 --> 00:43:54.080]   that's a way to, again, to understand these tools and to put them into the current context.
[00:43:54.080 --> 00:43:56.000]   I mean, it's, you know, think about it from a different perspective.
[00:43:56.000 --> 00:44:00.320]   This is something that we actually try to work on on Google+, which, you know, I'm like,
[00:44:00.320 --> 00:44:04.000]   I know you were there for that. You know, is the question like, what is the prompt?
[00:44:04.000 --> 00:44:07.280]   And currently, whether it's Twitter or whether it's BlueSky or any of these other platforms,
[00:44:07.280 --> 00:44:11.120]   they ask you something, you know, the prompt like, what's going on? What's on your mind? Like,
[00:44:11.120 --> 00:44:17.920]   what's happening? There's been loads of tests, but all within a very sort of fine mix.
[00:44:17.920 --> 00:44:22.400]   You can imagine that a chat TPT or some other GPT tool could actually propose topics that you
[00:44:22.400 --> 00:44:26.880]   might be interested in based on what's, you know, both your interests personally, and then what's
[00:44:26.880 --> 00:44:31.520]   been going on in your network, right? So for lots of people who currently find it hard to
[00:44:31.520 --> 00:44:36.720]   participate in social media or are mostly lurkers, this could be a facilitation to get their voice,
[00:44:36.720 --> 00:44:40.720]   their perspective into the mix. And then they can be coached on how to express themselves in a way
[00:44:40.720 --> 00:44:44.480]   that gives them greater confidence, just like having spell check or grammar check.
[00:44:44.480 --> 00:44:48.480]   This is almost like concept check. And that could actually be a great enabler for hearing from
[00:44:48.480 --> 00:44:53.760]   more people. I need to take a little break. If you want to rush over to Twitter and the Twitter
[00:44:53.760 --> 00:44:57.200]   space, or Rhonda Santis is about to announce his
[00:44:57.200 --> 00:45:02.240]   policy for press. So it said it was in two hours. Yeah, no, it's at three o'clock.
[00:45:02.240 --> 00:45:08.240]   It's right now, three o'clock Pacific. 27,000 people are in there right now.
[00:45:08.240 --> 00:45:15.280]   I don't see Mr. DeSantis in there yet. Nor do I see Elon Musk, nor the man I thought was going to
[00:45:15.280 --> 00:45:23.280]   be the host, David Sats. So I don't know what's happening, but no, it's now. But Lord a man is missing.
[00:45:24.240 --> 00:45:28.880]   A Florida man is gone. I do want to talk a little bit about our sponsor,
[00:45:28.880 --> 00:45:33.520]   AG1 from Athletic Greens. You got it right here. These are the travel packs, which I really,
[00:45:33.520 --> 00:45:39.200]   really like. I love the idea of everything you need in one little pouch you had to water. Yes,
[00:45:39.200 --> 00:45:45.120]   you can use cool water. It tastes fantastic. And you get everything you need. You don't need a lot
[00:45:45.120 --> 00:45:52.720]   of supplements. You don't need anything else. Just AG1 in the morning was founded in 2010.
[00:45:52.720 --> 00:45:56.480]   Parts of millions of routines since effect. I was really surprised. And I said, oh, you haven't
[00:45:56.480 --> 00:46:02.000]   taken that forever. Oh, okay. It saves you time. It saves you confusion. It even saves you money.
[00:46:02.000 --> 00:46:08.160]   Each serving costing less than $3 a day. And let me tell you, it's got everything you need.
[00:46:08.160 --> 00:46:14.800]   Vegan, paleo, keto, and low carb in case you're interested. No GMOs, no gluten. It's actually a
[00:46:14.800 --> 00:46:21.440]   really clean vitamin product, well, not just vitamins, vitamins, minerals, and, you know,
[00:46:21.440 --> 00:46:27.520]   pro and prebiotics, the kinds of things you really do want. It makes it easier for you to take the
[00:46:27.520 --> 00:46:31.520]   very highest quality supplements, whether it's improving digestion, supporting you with sleep.
[00:46:31.520 --> 00:46:37.200]   AG1, the best bang for your buck, just a scoop of AG1 in the morning,
[00:46:37.200 --> 00:46:44.000]   comes your entire day's nutritional basis, bases, bases, bases. Get all your supplements,
[00:46:44.000 --> 00:46:49.200]   support your long term health with AG1's 75 high quality minerals, vitamins, pre and probiotics.
[00:46:49.200 --> 00:46:53.520]   And it actually tastes fantastic. If you're looking for a simpler, cost effective supplement,
[00:46:53.520 --> 00:46:59.680]   routine AG1 by Athletic Greens, if you go right now to athleticgreens.com/twig,
[00:46:59.680 --> 00:47:03.760]   you'll get a free one year supply of vitamin D, little dropper. You can put it as much as you
[00:47:03.760 --> 00:47:09.360]   need each day. And five of these travel packs, free with your first purchase of a subscription.
[00:47:09.360 --> 00:47:16.560]   Athleticgreens.com/twig. We thank you so much for their support of the show. And thank you for
[00:47:16.560 --> 00:47:23.360]   supporting us by going to that address at lannickgreens.com/twig. Are you listening to the...
[00:47:23.360 --> 00:47:27.920]   It says I can't fetch it. Can't fetch it. I'm working.
[00:47:27.920 --> 00:47:31.520]   Yeah, I noticed you were able to fetch space. The number of listeners is going down.
[00:47:31.520 --> 00:47:36.160]   Maybe there's something going on. Maybe Twitter can't handle it.
[00:47:36.160 --> 00:47:44.160]   And then crashed handle it. Yeah. I noticed one of your jobs, I know you consult
[00:47:44.160 --> 00:47:48.160]   Christmas scene. I noticed one of your clients is Neva.
[00:47:48.160 --> 00:47:57.600]   Yes. So sad about Neva. I've been flogging Neva for months. He has said, "Oh, he has been..."
[00:47:57.600 --> 00:48:04.080]   I love it. I love the AI-generated subscription beginning. I was paying five bucks a month
[00:48:04.080 --> 00:48:10.480]   because I didn't want to add. Now they're selling out and they're closing down and
[00:48:10.480 --> 00:48:13.920]   they're refunding my money. Do you have any insight into what happened?
[00:48:13.920 --> 00:48:21.920]   I started out advising on the street arm. Certainly there was a lot of alignment in
[00:48:21.920 --> 00:48:25.040]   terms of what Neva was working on. He was at Google ahead of advertising at Google.
[00:48:25.040 --> 00:48:32.960]   Yes. So we have that background together. But I think there were a number of things that happened.
[00:48:32.960 --> 00:48:37.680]   They did a bunch of different and interesting experiments. There was a whole
[00:48:37.680 --> 00:48:43.520]   effort that they focused on around web 3 and NFTs and just trying lots of stuff to see
[00:48:43.520 --> 00:48:50.960]   what the next wave of the web might be like. I don't have any specifically insider information
[00:48:50.960 --> 00:48:53.920]   about this particular thing. But I think the thing that he said, which is the thing that I would...
[00:48:53.920 --> 00:48:59.360]   We saw this with Google Plus as well, is hard. It's like changing behavior. Even if you have a better
[00:48:59.360 --> 00:49:05.520]   product, just the muscle memory and the distribution that Google has is really, really difficult
[00:49:05.520 --> 00:49:11.360]   to break through. It's interesting. One of the reasons why I reached out before was of course
[00:49:11.360 --> 00:49:17.120]   talking about Twitter and activity pub and those things. We faced a similar question as to whether
[00:49:17.120 --> 00:49:22.320]   or if some of these other networks that are coming up now, Google doesn't even have some
[00:49:22.320 --> 00:49:26.640]   network effects. But for the search experience, it's largely a private personal experience.
[00:49:26.640 --> 00:49:31.360]   And yet the muscle memory is so ingrained. You almost have to do some psychedelics or something,
[00:49:31.360 --> 00:49:36.560]   just break free from the way you think about it. But I think that was the thing that Niva really
[00:49:36.560 --> 00:49:40.480]   struggled to overcome. It's the tyranny of the default.
[00:49:40.480 --> 00:49:47.840]   I was souring on Google just because the above the fold stuff was less and less
[00:49:47.840 --> 00:49:53.360]   search results. I just said, I need something that's going to give me search results.
[00:49:53.360 --> 00:49:58.000]   Niva gave me very, I think, pretty good search results. And then when they turned on the AI,
[00:49:58.000 --> 00:50:02.560]   I kept showing all these, these four guys are so bored with it. I kept showing it to them.
[00:50:02.560 --> 00:50:04.080]   June 2nd.
[00:50:04.080 --> 00:50:07.680]   Chris, do you ever use a search engine called find PH IND?
[00:50:07.680 --> 00:50:10.160]   Find P. I have not actually.
[00:50:10.160 --> 00:50:16.080]   I'm looking for Niva replacement. I'll have to check AI search engine for developers.
[00:50:16.080 --> 00:50:17.200]   For developers.
[00:50:17.200 --> 00:50:22.480]   I don't know why it's for developers. It works for all purpose searches.
[00:50:22.480 --> 00:50:29.040]   Have you tried a perplexity? Google like result and also AI, JAPT like result side by side.
[00:50:29.040 --> 00:50:32.400]   And it's pretty good.
[00:50:32.400 --> 00:50:38.480]   I think perplexity is sort of in the similar realm. I think it's perplexity.ai.
[00:50:38.480 --> 00:50:45.040]   It's interesting how there's a new, it feels like a standard in interface design that was largely
[00:50:45.040 --> 00:50:48.320]   inspired by chat JAPT, where it's sort of like chat box, but then they give you sort of like
[00:50:49.040 --> 00:50:54.880]   starter conversations to get going. But yeah, I don't know. The Niva thing is super interesting.
[00:50:54.880 --> 00:50:59.280]   Obviously, they've raised a bunch of money to go after this problem. But I think the LOM
[00:50:59.280 --> 00:51:03.760]   stuff changed everything and it woke up the sleeping giants. And now it's like game on.
[00:51:03.760 --> 00:51:09.440]   And so it's Microsoft versus Google. And as a num start, had like chat JAPT and OpenAI
[00:51:09.440 --> 00:51:13.680]   really not come on the scene as they did, I think Niva could have had their runway and continued
[00:51:15.200 --> 00:51:20.080]   chasing that dragon. But now that the big guys are awake and actually like launching their stuff,
[00:51:20.080 --> 00:51:24.880]   like Google had all this stuff behind closed doors, that makes it much harder.
[00:51:24.880 --> 00:51:28.960]   You know, with that is billions of dollars of investment to do the same.
[00:51:28.960 --> 00:51:30.000]   Yep.
[00:51:30.000 --> 00:51:31.600]   Yeah, it's pretty good actually.
[00:51:31.600 --> 00:51:35.600]   I'm just trying to get it to write some code for me.
[00:51:35.600 --> 00:51:40.160]   Let's see if we can do it.
[00:51:40.160 --> 00:51:43.280]   See if you can spin up a Twitter spaces.
[00:51:44.720 --> 00:51:49.120]   Yeah, I need the dial tone of the internet. Can you can you get to work on that?
[00:51:49.120 --> 00:51:55.600]   Okay, sort of good. And what was the other one perspective?
[00:51:55.600 --> 00:51:59.360]   Perplexity. I'm sorry, perplexity.
[00:51:59.360 --> 00:52:05.600]   Yeah. But a lot of these unfortunately, it's so hard to make a search engines index. A lot of them
[00:52:05.600 --> 00:52:09.600]   are just using bings or Google's index.
[00:52:10.640 --> 00:52:19.120]   That's good. Well, that's another question. Do not just indexes, but trading sets become a commodity.
[00:52:19.120 --> 00:52:24.640]   I mean, obviously, like, one of the ways that I think about AI
[00:52:24.640 --> 00:52:30.640]   is a lot of people have talked about data as kind of a new oil. And I just, one, I think that
[00:52:30.640 --> 00:52:36.480]   metaphor is a little bit gross and tired. I tend to think about it more like varietals and,
[00:52:36.480 --> 00:52:40.320]   you know, Greston in California. So I would think that. But like, you know, I think that
[00:52:41.120 --> 00:52:45.280]   with data, it's a little bit like terroir. It's, you know, it's the specific kind of, you know,
[00:52:45.280 --> 00:52:49.680]   region where different aspects of the data has different levels of complexity, different levels
[00:52:49.680 --> 00:52:54.400]   of expressivity, different metadata that's attached to it, you know, and so, you know, those
[00:52:54.400 --> 00:52:58.640]   varietals are kind of, you know, the weather, the atmosphere, the ground, you know, the great
[00:52:58.640 --> 00:53:02.480]   growing area, as well as the people who are then cultivating those things. And they have a certain
[00:53:02.480 --> 00:53:08.320]   taste or preference. And so when it comes to building, whether it's a search engine or a reasoning
[00:53:08.320 --> 00:53:13.840]   tool built around a large language model, it isn't so much that I think the training data becomes a
[00:53:13.840 --> 00:53:17.440]   commodity. But there are different expressions of it in different use cases. You might have a,
[00:53:17.440 --> 00:53:21.920]   you know, general purpose, like large government body data set. And it might have a bunch of
[00:53:21.920 --> 00:53:28.320]   extraneous stuff that just isn't really that relevant or that useful. And so tweaking those
[00:53:28.320 --> 00:53:32.560]   models, deciding what goes in, what stays in, what goes out, how to, you know, cross reference,
[00:53:32.560 --> 00:53:36.720]   like identity, ironically, which is something I spent a lot of my career working on becomes
[00:53:36.720 --> 00:53:40.800]   an even bigger problem, because now you want to be able to know from one data set to another,
[00:53:40.800 --> 00:53:45.440]   what are the same, like, you know, reflectivity? Like, is this object in this data set? The same
[00:53:45.440 --> 00:53:48.800]   thing is in that data set. And what are the other relationships between those things?
[00:53:48.800 --> 00:53:54.720]   So that's also the way in which these things are not necessarily commodity per se, but they can
[00:53:54.720 --> 00:53:59.760]   be blended, if you will, to create new types of data sets, whether they're synthetic or not.
[00:54:01.120 --> 00:54:08.080]   Do you, what do you think of the argument in the stochastic parrots paper that ever larger
[00:54:08.080 --> 00:54:17.280]   learning sets are are are meanless and counterproductive and hard to manage and really not the point?
[00:54:17.280 --> 00:54:23.360]   Hold on a second. Let me just interrupt. I was in the wrong group. Elon has launched his
[00:54:23.360 --> 00:54:29.120]   group for the San Jose campaign, his presidential campaign. I can't get it to run, but he's got
[00:54:29.120 --> 00:54:35.840]   375,000 in there. So I think I was in a side, and this is one of the problems, of course,
[00:54:35.840 --> 00:54:42.160]   I was in a side group. You were in one from a verified user, right? Yeah, exactly. Yeah, this is
[00:54:42.160 --> 00:54:47.760]   what you even find it. Yeah, it's just it's in the top. Yeah. Elon Musk is the host. I'm a listener,
[00:54:47.760 --> 00:54:53.840]   apparently. So is Yesher Ali, Alexis O'Hanean, Michael Beshloss, Alex Wagner, Laura Ingraham,
[00:54:53.840 --> 00:55:03.600]   Caitlyn Jenner. It's a who's who of Tomi Lauren, Matt Gates, Rudy Giuliani. And there's Rand
[00:55:03.600 --> 00:55:09.360]   DeSantis. He's just listening for right now. We can't figure out how to get this. How do I talk
[00:55:09.360 --> 00:55:14.400]   here? I guess Elon, Rudy Giuliani ran for president honest to God, his Twitter account was private.
[00:55:14.400 --> 00:55:20.720]   Can I let me just turn on the turn on the sound here. Can you hear? I'm just curious.
[00:55:20.720 --> 00:55:26.000]   I think it's Elon talking at this point. Can I re-broadcast this? I don't know.
[00:55:26.000 --> 00:55:32.240]   You know what? I wonder if I should report this space as being offensive to my liberal years.
[00:55:32.240 --> 00:55:37.920]   That's right. I imagine they'll have that turned off. Anyway, well, I guess we can't hear it.
[00:55:37.920 --> 00:55:42.800]   So well, there's no staff there to shut it down anyway. So that's true. That's true.
[00:55:42.800 --> 00:55:47.520]   And he's still not paying the rent, which really is kind of an amazing
[00:55:48.560 --> 00:55:52.880]   but the actor on history, this building is not earthquake safe. So maybe he'll use that as a
[00:55:52.880 --> 00:55:56.240]   there you go. This is a dumb question. Yeah. How do you get to spaces?
[00:55:56.240 --> 00:56:04.320]   There is a thing called Twitter. You'll find it. Twitter.com. Yeah, I'm there. Yeah. And then
[00:56:04.320 --> 00:56:09.440]   over on the on the right hand side, well, I mean, let me refresh. Because probably it doesn't.
[00:56:09.440 --> 00:56:14.480]   I see tech. Oh, it's not there anymore. That's not there. They've been moving stuff.
[00:56:14.480 --> 00:56:18.400]   Mobile. Check it out on mobile. It was there. It was right here in the trending.
[00:56:18.400 --> 00:56:26.880]   Maybe if I explore. Wow. My spaces bar just went away. Yeah. Twitter crashing hard. It's crashing
[00:56:26.880 --> 00:56:31.840]   hard. That's what's happening. Yeah. That's there. We go. Listen live in spaces. It should be up here.
[00:56:31.840 --> 00:56:36.640]   Preparing to launch is the name of that one. Now, four hundred seventy three thousand people in there,
[00:56:36.640 --> 00:56:42.640]   including all these famous and semi famous. That would be so good. If like, you know,
[00:56:42.640 --> 00:56:47.680]   the one moment where where Elon tries to like showcase his new purchase is the moment when
[00:56:47.680 --> 00:56:53.440]   Twitter actually finally crashes like all the predictions. Yeah. Yeah. Right. Somewhere in
[00:56:53.440 --> 00:56:58.560]   her presidential campaign. Fire. You say it. I knew this had happened. Oh, I heard I heard Elon.
[00:56:58.560 --> 00:57:05.200]   All right. Well, it's certainly an incredible honor to have Governor Santos make this stark
[00:57:05.200 --> 00:57:11.760]   announcement. What the hell? What the hell is that? Please wait.
[00:57:11.760 --> 00:57:14.480]   [laughter]
[00:57:14.480 --> 00:57:17.920]   That's exciting. Okay. I guess we're going tomorrow's guys.
[00:57:17.920 --> 00:57:22.240]   We're on his his Roadster off somewhere in the.
[00:57:22.240 --> 00:57:26.880]   Cheese Louise. All right. I don't know what that is, but I'm closing it.
[00:57:26.880 --> 00:57:33.680]   You mentioned AM radio for it has reversed its course and says, we are okay. Well, all right.
[00:57:33.680 --> 00:57:38.720]   Believe the AM radio on. And your decent cost in your car. Cost them nothing. In fact,
[00:57:38.720 --> 00:57:46.080]   it's a software update. If you. What? Yes. Kind of amazing for any owners of 40 EVs without
[00:57:46.080 --> 00:57:53.120]   AM broadcast capability. We'll offer a software update to restore it said CEO, Jim Farley. So
[00:57:53.120 --> 00:57:56.880]   apparently they're building them with AM radios. They just disable them in software.
[00:57:56.880 --> 00:58:03.600]   I want is that huh? Sorry. Like mine is my mind is going to like several places because there's a
[00:58:03.600 --> 00:58:06.400]   lot of things that have been changing in the audio world. Obviously we saw it with Twitter
[00:58:06.400 --> 00:58:15.600]   spaces. But like when it comes to audio in cars, obviously that's a really relevant ad channel.
[00:58:15.600 --> 00:58:21.040]   And so whether you're streaming Spotify or other platforms like that, that's all coming over.
[00:58:21.040 --> 00:58:26.400]   I presume, you know, digital frequencies perhaps. So if it's a software update,
[00:58:26.400 --> 00:58:31.840]   does that mean that the hardware is just being turned on and turned off for AM? Or is AM content
[00:58:31.840 --> 00:58:38.480]   being streamed over like I don't think so. I don't think so. Because one of the reasons
[00:58:38.480 --> 00:58:43.600]   I think they're bringing AM back is because there's been movement in Congress. AM is a critical
[00:58:43.600 --> 00:58:49.920]   emergency channel for us. That's what I would expect. Yes. And so you need a hardware update.
[00:58:49.920 --> 00:58:54.160]   Yeah. Yeah. You need a hardware radio because there are plenty of places in the US you can't get
[00:58:54.160 --> 00:58:59.200]   your that's right. Self signals. Right. And so if it is like Starlink and other types of satellite
[00:58:59.200 --> 00:59:03.760]   coverage, I'm not disagreeing. I have the same question. Elon is very strange.
[00:59:03.760 --> 00:59:07.680]   As you know, in your Tesla, Elon does not put a radio in your test. Does not.
[00:59:07.680 --> 00:59:12.640]   So if everyone needs to go to Mars, I will want to not be notified. You know,
[00:59:12.640 --> 00:59:15.360]   that message goes out over AM. It'll be on Twitter spaces.
[00:59:15.360 --> 00:59:22.000]   Yeah. So I'm looking at the debacle that is this announcement in Twitter keeps crashing.
[00:59:22.000 --> 00:59:25.120]   The sand just keeps being demoted to a listener rather than speaker.
[00:59:26.800 --> 00:59:30.880]   It's symbolic. It's symbolic. There are half a million people. The number
[00:59:30.880 --> 00:59:37.040]   continues to rise. People trying to listen. They should use clubhouse. Here's Brian. Brian
[00:59:37.040 --> 00:59:43.760]   Stelter's tweet is this space is not available. Yeah. I think it's in fact, I can't get into
[00:59:43.760 --> 00:59:47.360]   do anything. It's kind of just kind of dark now.
[00:59:47.360 --> 00:59:55.280]   Another. There we go. Playing Blunder for Elon Musk. Let me join for primetime.
[00:59:55.280 --> 01:00:01.600]   Lauren, Laura Ingraham and see what's going on. One more time. We'll give it a shot here.
[01:00:01.600 --> 01:00:05.840]   I bet he's throwing all of his ad inventory at everyone who's trying to get on the space right
[01:00:05.840 --> 01:00:09.360]   now. He's got a guy. Those are all, man.
[01:00:09.360 --> 01:00:16.240]   Well, there you go. So I'm sure we'll find out more about this
[01:00:16.240 --> 01:00:21.440]   on the evening news where. Well, if you go to the explore tab, at least for me,
[01:00:21.440 --> 01:00:24.800]   again, I don't know what's personalized and what's not. But Musk is a trending topic with
[01:00:24.800 --> 01:00:29.840]   986,000 tweets. And DeSantis has 170,000 tweets. So talk about being a ratio.
[01:00:29.840 --> 01:00:34.560]   Yeah. He's not even. That's why Elon likes it. And that's how we're going to keep it.
[01:00:34.560 --> 01:00:39.440]   Is Joe Biden tweeting? Oh, well, let's see what the president has to say.
[01:00:39.440 --> 01:00:50.000]   Jack Dorsey. He just came back. Who? That is silence again. Are you listening?
[01:00:50.000 --> 01:00:58.240]   We're ready to go here. That was silence again. Oh, Lord. Jack Dorsey, apparently,
[01:00:58.240 --> 01:01:01.360]   is now endorsing. I don't see him. We find that story.
[01:01:01.360 --> 01:01:08.480]   Dorsey, who RFK Jr, at least he's he's retweeting. Yeah.
[01:01:08.480 --> 01:01:15.120]   Posted two videos in the past week featuring RFK Jr. Oh, man. Of course, a big COVID and
[01:01:15.120 --> 01:01:23.920]   any of ex conspiracy nut and more. Yeah. So there. What is it about people? They get red
[01:01:23.920 --> 01:01:29.360]   pill. Oh, I hear a hearing on. That was B.
[01:01:29.360 --> 01:01:33.760]   You say there's too many people?
[01:01:33.760 --> 01:01:36.560]   Yes. I'm sorry. I've never seen this problem.
[01:01:38.960 --> 01:01:46.640]   Oh, this is not good. No. Are you Chris? Do you mourn this a little bit? The loss of
[01:01:46.640 --> 01:01:50.560]   Twitter and where Twitter is gone? I'm mourning. I'm going to, like it's sad.
[01:01:50.560 --> 01:01:56.480]   I went through definitely a period of grief, you know, just because again,
[01:01:56.480 --> 01:01:59.440]   this place and space, it is like a neighborhood that sort of-
[01:01:59.440 --> 01:02:00.000]   We need it.
[01:02:00.000 --> 01:02:04.480]   That goes downhill. Yeah. And it was a place that you could go to. And you have like a
[01:02:04.480 --> 01:02:06.560]   down moment. There was always something interesting. I mean, there's still
[01:02:06.560 --> 01:02:10.240]   interesting things going on, but it's sort of like, you know, they've taken out the plumbing and,
[01:02:10.240 --> 01:02:14.080]   you know, just like porta-potties around and you're kind of like, is anyone going to clean this up?
[01:02:14.080 --> 01:02:21.280]   So it's yeah, in that sense, it's sad. And you know, like, I don't think there's much conversation
[01:02:21.280 --> 01:02:26.480]   about grieving of the loss of Twitter. And yet in terms of the purpose that it served,
[01:02:26.480 --> 01:02:30.400]   it doesn't serve that function anymore. There's going to be a generation that grows up without
[01:02:30.400 --> 01:02:35.120]   Twitter as we knew it, you know? There is that discussion around community about Black Twitter.
[01:02:35.840 --> 01:02:40.240]   There's a story in the rundown today that the climate scientists are kind of leaving on mass
[01:02:40.240 --> 01:02:45.520]   as a group. I think that we see it around communities rather than Twitter as a whole.
[01:02:45.520 --> 01:02:48.000]   That's really cool. I don't know what to do.
[01:02:48.000 --> 01:02:51.680]   I mean, so this is again, the thing that I was interested in the conversation you guys had
[01:02:51.680 --> 01:02:57.200]   previously about Activity Pub and about these migrations. I do think that the dissolution of
[01:02:57.200 --> 01:03:01.360]   a social platform like this, like, I think it's interesting having been on the inside of Google
[01:03:01.360 --> 01:03:08.240]   Plus as it was being built and having designed parts of it. And also struggling mightily,
[01:03:08.240 --> 01:03:13.040]   you know, in a lot of the kind of idea, some of the ideas were good. Some of the ideas were
[01:03:13.040 --> 01:03:17.360]   completely orthogonal to what or how humans tend to want to interact with each other. And so,
[01:03:17.360 --> 01:03:22.000]   you know, if you come at it from a more, I don't know, observational perspective,
[01:03:22.000 --> 01:03:24.640]   then maybe you can build better stuff. Or anyways, this question of like, where these
[01:03:24.640 --> 01:03:28.720]   communities go and how they move and whether or not we should build into the fabric of the
[01:03:28.720 --> 01:03:34.400]   social web, that ability to, you know, leave if a place becomes toxic or no longer serves its function.
[01:03:34.400 --> 01:03:39.200]   Is anyone afraid that a year from now that Twitter will be the right wing,
[01:03:39.200 --> 01:03:41.520]   Twitter and blue sky will be the left wing Twitter?
[01:03:41.520 --> 01:03:45.520]   And is that a good thing? You might think about a week from now, Mike.
[01:03:45.520 --> 01:03:53.120]   Yeah, I'm gonna say here is the Biden tweet. A little, throw a little sub tweet, I think.
[01:03:55.040 --> 01:03:59.840]   This link works. Yeah, no, it's on. Yeah, but to give money.
[01:03:59.840 --> 01:04:06.160]   Okay. Yeah, it's a good money. Yeah. Interesting. Yeah, he's not having a, he's not having a
[01:04:06.160 --> 01:04:12.400]   spaces chat. Neither is Elon right now. Apparently. Are you monitoring it?
[01:04:12.400 --> 01:04:20.000]   I was trying to, but now it's now there's nothing, huh? Interest. Most of what I'm seeing are
[01:04:20.000 --> 01:04:25.120]   people saying that it's not working. The surgeon general. Dr. Vivek Murthy says,
[01:04:25.120 --> 01:04:33.520]   social media may harm children and adolescents, a profound risk of harm to adolescent mental health
[01:04:33.520 --> 01:04:38.800]   urges families to set limits and governments. Uh oh. To say it looks like, sorry, David Saxe,
[01:04:38.800 --> 01:04:43.760]   if you go to David Saxe's tweets, he just posted a space with Rhonda Santas. So,
[01:04:43.760 --> 01:04:49.840]   who are they created? A new one? Yeah. Yeah. Maybe because Elon has too many followers. So,
[01:04:49.840 --> 01:04:55.360]   Elon can't. Oh, that might be. That could be. That might be. So let's see here.
[01:04:55.360 --> 01:05:04.240]   Yeah. Talsi Gabbard in there. David Saxe is hosting this one. And there's Ron and a bunch of
[01:05:04.240 --> 01:05:13.760]   other people. Oh, it's it's that noise again.
[01:05:13.760 --> 01:05:23.040]   Who shows this music? Guess. Take a wild, wild guess. Is this what DeSantis plays that is?
[01:05:23.040 --> 01:05:30.320]   This gathering is unbelievable. Uh so the Biden administration's surgeon general,
[01:05:30.320 --> 01:05:35.840]   let's point that out, called on tech companies to enforce minimum age limits and to create
[01:05:35.840 --> 01:05:40.080]   default settings for children with high safety and privacy standards. And he urged the government
[01:05:40.080 --> 01:05:46.960]   to create age appropriate health and safety standards for technology platforms. Um, you know,
[01:05:46.960 --> 01:05:54.000]   fine. What are you guys doing below? Um below I have a thread on 79 where I went back. I so
[01:05:54.000 --> 01:05:59.680]   happens I was I was researching this just the part of the book I'm writing now. Um where I went back
[01:05:59.680 --> 01:06:06.320]   to 51 years ago and the surgeon general report on television violence, five volume monstrous
[01:06:06.320 --> 01:06:14.320]   thing 40 years ago on video games. Um then, you know, every day, every day, one's here's
[01:06:14.320 --> 01:06:19.760]   they're all the same. They all end up the same. And then the interesting thing was, um,
[01:06:19.760 --> 01:06:26.800]   Professor Corn Revere, who's a really good first amendment scholar had a really good post about the,
[01:06:28.880 --> 01:06:35.040]   uh, a video game one, it went to the Supreme Court and you see the first amendment lesson
[01:06:35.040 --> 01:06:40.400]   had happened again and again and again and again. And it has to be retaught. And so in the Supreme
[01:06:40.400 --> 01:06:48.240]   Court ruling on, uh, Brown, California versus whatever entertainment company was, uh, there's a quote
[01:06:48.240 --> 01:06:55.840]   that gets reused now constantly about the first amendment that but in one of the footnotes, um,
[01:06:57.120 --> 01:07:05.760]   Scalia was arguing with, with, I think, uh, Alito somebody and he described a ridiculous law that
[01:07:05.760 --> 01:07:11.040]   should never happen. And what he described was Utah's law that just just went into effect. And so we go
[01:07:11.040 --> 01:07:16.240]   around on this moral panic Mandela over and over and over again. And that's not to say that there
[01:07:16.240 --> 01:07:20.400]   isn't plenty to worth studying and there isn't plenty to be concerned about and there isn't plenty to
[01:07:20.400 --> 01:07:26.720]   support parents with. And that's all true. But the, the surgeon general's report says nothing
[01:07:26.720 --> 01:07:33.600]   about guns in the schools about, it seems like a more proximate danger about, uh, climate change,
[01:07:33.600 --> 01:07:38.720]   about the economy, about all of these things. Oh, no, no, no, it's all phones. Yeah.
[01:07:38.720 --> 01:07:44.800]   Um, well, the, the weird thing and I think this was acknowledged that there's a, there's a certain
[01:07:44.800 --> 01:07:50.560]   degree of, um, I guess it's refreshing honesty or surprise. It's more honest, the, the certain
[01:07:50.560 --> 01:07:55.680]   general's, uh, statements than, than I would have expected. But he basically points out that the
[01:07:55.680 --> 01:08:01.920]   problem is that we haven't studied it enough. Oh my God. We studied it. Oh my God.
[01:08:01.920 --> 01:08:08.080]   Yeah, they have been studies and they point out that there's a, you know, there's probably more harm
[01:08:08.080 --> 01:08:12.800]   than benefits, but the benefits are there. Actually, the existence of social media and the lives of
[01:08:12.800 --> 01:08:17.920]   young people is actually a huge benefit in a number of ways that include, you know, a sense of,
[01:08:17.920 --> 01:08:23.200]   a place to express creativity, a place to, to find connection with people to, to, to,
[01:08:23.600 --> 01:08:28.560]   for people to help, you know, find people to help them through their adolescent problems and so on.
[01:08:28.560 --> 01:08:33.200]   On the downside, there's, there's bullying problems. There's like self esteem issues,
[01:08:33.200 --> 01:08:39.680]   has it much problems? Exactly. So, but Mike, very important to me that you bring up a really
[01:08:39.680 --> 01:08:44.400]   important point, Claire Kane Miller at the times through great credit and the times for the
[01:08:44.400 --> 01:08:49.360]   great credit had a story today talking about how important the social media is to LGBTQ teams,
[01:08:49.360 --> 01:08:54.080]   especially now, when they're being assaulted in some states all around, they're not being allowed
[01:08:54.080 --> 01:09:02.640]   to live as themselves. And so the existence and which, which, Merti Merti Merti acknowledged
[01:09:02.640 --> 01:09:06.400]   in his report, he had a thick paragraph, but the benefits, just as you just did, Mike,
[01:09:06.400 --> 01:09:12.640]   and he acknowledged the importance to LGBTQ young people. Without that, they'd be even more lost.
[01:09:12.640 --> 01:09:18.320]   I mean, like, what doesn't it seem like the other piece of this is to suggest that social media
[01:09:18.320 --> 01:09:23.600]   either also harms adults and parents, or that they are actually one of the number like leading causes
[01:09:23.600 --> 01:09:29.200]   of some of this harm and abuse, because, you know, adults and parents tend to set the norms and
[01:09:29.200 --> 01:09:33.920]   the behaviors for their kids and to assume that kids should somehow develop, you know,
[01:09:33.920 --> 01:09:39.920]   some sort of moral clarity or alacrity in the absence of those examples seems to ignore,
[01:09:39.920 --> 01:09:44.480]   like, some of the fundamental distortions from social media. In fact, I think they do do exactly
[01:09:44.480 --> 01:09:49.280]   that. You see the fluidity of movement of younger people from one social network to another for
[01:09:49.280 --> 01:09:56.080]   various reasons. I mean, you saw people rushing the Snapchat because, you know, years ago, because
[01:09:56.080 --> 01:10:01.280]   they could have a group of, you know, interact with, like, you know, 12 people instead of 1000 people.
[01:10:01.280 --> 01:10:07.840]   Now, Discord is that for a lot of teens. Exactly. Exactly. So it seems like on the whole, one of
[01:10:07.840 --> 01:10:14.240]   the interesting things about younger people as a demographic is that they're quicker and sort of
[01:10:14.240 --> 01:10:19.440]   better at finding spaces on social media. In some ways, it's more disposable to them.
[01:10:19.440 --> 01:10:24.480]   Yeah. Right. Like, it's a boy. Like, and again, it's sort of reflecting back on how we grew up
[01:10:24.480 --> 01:10:28.400]   with this. And so, you know, like, my accounts on social media are so valuable to me. They're so
[01:10:28.400 --> 01:10:32.800]   personal. And I want to hold on to them like forever. I think what I see in, you know, this is true in
[01:10:32.800 --> 01:10:38.480]   a lot of other communities is kind of a willingness to abandon or like go of or just kind of use
[01:10:38.480 --> 01:10:43.120]   disposable identities, you know, constantly, and especially earlier on, like, I think one of the
[01:10:43.120 --> 01:10:47.760]   things that's interesting about this report, and this is a constant conversation that I just do not
[01:10:47.760 --> 01:10:54.720]   know how to square with, you know, American conventions, values and laws, which is causing
[01:10:54.720 --> 01:11:00.320]   social media platforms to be better at identifying young people under age and then restricting their
[01:11:00.320 --> 01:11:05.840]   access and use without some sort of identity regime that requires everyone, you know, every kid,
[01:11:05.840 --> 01:11:12.000]   basically, to have an ID that is attached to them somehow in a way that is completely against so
[01:11:12.000 --> 01:11:17.920]   many of the principles of privacy and security. So we want, you know, the cake to eat it too,
[01:11:17.920 --> 01:11:23.280]   but it's not really realistic, I think, to put the onus of parenting and controlling access to
[01:11:23.280 --> 01:11:28.320]   these platforms exclusively on the platforms themselves. Chris, there's a great quote. I just
[01:11:28.320 --> 01:11:35.120]   read this paper today by Kirsten Drotner from Pedagodjica Historica. I read all the fun things.
[01:11:37.280 --> 01:11:42.000]   What was one of those things that attracted us? No. Is that a real library behind you?
[01:11:42.000 --> 01:11:49.520]   One line quote, "On a social level, media panics basically attempt to reestablish a generational
[01:11:49.520 --> 01:11:55.040]   status quo that the youthful pioneers seem to undermine." You're right on target that it is
[01:11:55.040 --> 01:11:59.440]   about age, it is about generations. It's about parents either saying they're protecting their
[01:11:59.440 --> 01:12:05.440]   young people or as so much in moral panics, it's about hooliganism and vandalism and delinquency.
[01:12:06.000 --> 01:12:11.040]   So it's either condemning our young people or protecting them because you don't trust them in
[01:12:11.040 --> 01:12:15.760]   that way either. And the young people know what the hell they're doing and they're figuring it out
[01:12:15.760 --> 01:12:21.680]   and you're right, there are soldiers or not. I mean, I feel like the ways to, well, I don't know,
[01:12:21.680 --> 01:12:30.800]   I do have a role as a parent now and I do think a lot about how to equip kids to be in these
[01:12:30.800 --> 01:12:36.240]   media environments and to understand what is, if not real, just what is important about what
[01:12:36.240 --> 01:12:42.720]   people say and don't say. And we've got a 13-year-old here at home who runs a Discord server and I've
[01:12:42.720 --> 01:12:48.880]   had to teach them about being responsible as a server admin because they determine who is
[01:12:48.880 --> 01:12:52.160]   in the server and who is not in the server. I would love to hear those moderation lessons.
[01:12:52.160 --> 01:12:57.760]   Well, I'm learning as I go, but thankfully I've run my own Discord server so I can have that
[01:12:57.760 --> 01:13:00.720]   conversation. Think about all the parents out there who have not run their own Discord server
[01:13:00.720 --> 01:13:06.160]   as let alone even know what a Discord server is. Right? So, and yet those folks, those are the
[01:13:06.160 --> 01:13:10.640]   folks who are trying to tell their kids how to behave and how to do that. And there's a whole
[01:13:10.640 --> 01:13:17.360]   like sub-genre or subterranean world of social interactions that are happening in these spaces
[01:13:17.360 --> 01:13:20.880]   that are affecting kids and their mental health and their sense of security and safety and their
[01:13:20.880 --> 01:13:24.720]   communities and their environments that are completely opaque to parents. And so parents are like,
[01:13:24.720 --> 01:13:29.760]   oh, let's get the certain general to say this is a problem and to crack down and to restrict access.
[01:13:29.760 --> 01:13:34.800]   But that doesn't solve for the underlying interactions and behaviors that are going to happen anyway.
[01:13:34.800 --> 01:13:40.640]   As you said, like in the lunchroom, that is a visible space and yet school interactions happen
[01:13:40.640 --> 01:13:44.960]   outside of school now, whether it's on text messaging or whether it's in these chats.
[01:13:44.960 --> 01:13:48.880]   And so the vernacular in the language that needs, I mean, we talked about this earlier with like,
[01:13:48.880 --> 01:13:54.400]   you know, chat TPT being sort of a context provider for parents, where it's like,
[01:13:54.400 --> 01:13:58.000]   what is it that your child understands about social media and technology? And then what do you
[01:13:58.000 --> 01:14:01.520]   understand about social media and technology and where are the gaps? And can you facilitate a
[01:14:01.520 --> 01:14:07.600]   conversation so that both can at least come into those conversations with a broader lens for what
[01:14:07.600 --> 01:14:12.960]   belonging like in this moment means to that generation? Yes. But that'd be useful. Or that sounds like
[01:14:12.960 --> 01:14:19.200]   what you were answering Jeff's question about. What are the hazards of AI? The AI is prescribing
[01:14:19.200 --> 01:14:22.480]   parental. It depends on, you know, the relationship that you want to have with your kids.
[01:14:23.520 --> 01:14:27.680]   The machine said I should be talking about sex. So sit down and shut up.
[01:14:27.680 --> 01:14:33.280]   Just to be the one the government enforcing it. I agree. You don't want the government.
[01:14:33.280 --> 01:14:38.240]   Exactly. By the time you get around to doing something about it, they're all gone and off
[01:14:38.240 --> 01:14:44.000]   somewhere else. Right. It's like, I just don't, you know, there's so much dynamism
[01:14:44.000 --> 01:14:53.120]   in trends in social interaction among teenagers that it's like, you don't have a prayer of like,
[01:14:53.120 --> 01:14:57.440]   it's game of whack-a-mole. You don't have a prayer of climbing down on TikTok because by the time
[01:14:57.440 --> 01:15:03.360]   you do, they'll be off TikTok and on the newest thing that doesn't even exist yet. So as we predicted
[01:15:03.360 --> 01:15:09.840]   five creators in Montana have sued now over the state law and TikTok has now entered the fray.
[01:15:09.840 --> 01:15:14.960]   TikTok is also right. It is also suing. Yeah. I thought it might not just let the creators do the
[01:15:14.960 --> 01:15:21.520]   heavy lifting, but they file the federal lawsuit against Montana. It's clearly a
[01:15:21.520 --> 01:15:23.920]   foot of a button. It's a first amendment. Who knows what this is for?
[01:15:23.920 --> 01:15:28.320]   But it is. It's wild. That's happening in Montana of all places.
[01:15:28.320 --> 01:15:34.320]   It's also lazy. Right. First of all, Montana is doing something that they're basically doing
[01:15:34.320 --> 01:15:41.600]   a, you know, international relations as a state. But the other thing is that they're just basically,
[01:15:41.600 --> 01:15:47.040]   they're saying, okay, Apple and Google, you guys, we're going to crack down on you if you don't
[01:15:47.040 --> 01:15:52.640]   enforce this stuff. And it's pretty ridiculous. You know, it's like if you were on one side of a
[01:15:52.640 --> 01:15:57.760]   state line, you can download an app. If you're on the other side of the state line five feet away,
[01:15:57.760 --> 01:16:04.560]   you can't. This is not going to work. Apple has said prior to the law being introduced,
[01:16:04.560 --> 01:16:09.120]   or actually when it was introduced, but not passed, that they did not have a technical means to
[01:16:09.120 --> 01:16:14.000]   prevent people in Montana from downloading TikTok. I find that hard to believe. I think you could
[01:16:14.000 --> 01:16:18.320]   create the Great Firewall of Montana if you really wanted to.
[01:16:18.320 --> 01:16:24.480]   I don't think that's how the internet works, but I'm glad that you've hallucinated that idea.
[01:16:24.480 --> 01:16:28.400]   Oh, can't you just say, well, let me look at that IP address. Oh, yeah, you're in Montana.
[01:16:28.400 --> 01:16:33.280]   No, you could. I mean, they did this for Pornhub too. You know, and they do it now for you to
[01:16:33.280 --> 01:16:39.600]   hear it. Yeah. Yeah. Right. If Pornhub can block itself as you guys. Yeah. Yeah. Right.
[01:16:39.600 --> 01:16:44.720]   It's great news for VPN providers. Exactly. Exactly. Yeah. Some of them might still have a business.
[01:16:44.720 --> 01:16:49.280]   Are you excited about Instagram's new text based app for conversation? This is
[01:16:49.280 --> 01:16:55.600]   from G. Adley seems to have Federation built in and it wants to interoperate.
[01:16:55.600 --> 01:16:58.480]   They say soon our app will be competitive.
[01:16:58.480 --> 01:17:02.960]   Activity Pub? Soon our app will be compatible with certain other apps like Mastodon.
[01:17:02.960 --> 01:17:06.560]   Don't I'm not sure what that means, but it sounds like they'll be activity pub.
[01:17:06.560 --> 01:17:11.040]   Well, some race blue sky and Instagram. Who's going to get there first? Yeah.
[01:17:11.040 --> 01:17:15.600]   Well, I mean, I've been having a lot of conversations with people who are actually building bridges
[01:17:15.600 --> 01:17:22.320]   between the other and blue sky or at protocol. And I think that that's like,
[01:17:22.320 --> 01:17:27.280]   it's so funny because I started working on this stuff in 2006, 2007. And finally,
[01:17:27.280 --> 01:17:31.520]   some of these ideas are coming to fruition. You know, and so it's interesting to see that
[01:17:31.520 --> 01:17:35.120]   blue sky picked up a bunch of these ideas and yet kind of reinvented the wheel once again.
[01:17:35.120 --> 01:17:38.640]   So now it's like, oh great. Now we have like five standards. Now we have to choose among them,
[01:17:38.640 --> 01:17:42.480]   but actually they're close enough that maybe we can get to some alignment.
[01:17:42.480 --> 01:17:45.440]   And maybe this Instagram thing could actually push it forward. I mean, we'll see.
[01:17:45.440 --> 01:17:49.920]   You don't have a favorite between AT proto and an activity pub?
[01:17:49.920 --> 01:17:52.960]   I think they're actually complimentary. As I've dug in a little bit more,
[01:17:52.960 --> 01:17:56.320]   I understand that what blue sky is trying to solve for is actually different than activity pub.
[01:17:56.320 --> 01:18:01.360]   And so considering like, I helped to create the activity streams format that then turn into
[01:18:01.360 --> 01:18:06.160]   activity pub, the original goal was about syndicating a greater richness of activities
[01:18:06.160 --> 01:18:11.600]   beyond just what RSS encoded, which was blog posts. And so now that activity pub is quite
[01:18:11.600 --> 01:18:16.240]   mature and now there's software behind it, the alignment between what blue sky is trying to do
[01:18:16.240 --> 01:18:20.960]   from an identity portability perspective is compatible with the way of expressing the
[01:18:20.960 --> 01:18:28.240]   activities that activity pub uses. So essentially you get some aspects of identity and the tree
[01:18:28.240 --> 01:18:30.960]   structure and I don't know, there's a bunch of like weird mutation stuff that I don't quite
[01:18:30.960 --> 01:18:36.880]   understand in the app protocol, but it's not completely incompatible. In fact, the app protocol
[01:18:36.880 --> 01:18:41.440]   uses things like an actor and a verb, you know, to encode its content. So it's already there.
[01:18:41.440 --> 01:18:44.480]   And the work is being done currently to see if those things can actually
[01:18:44.480 --> 01:18:49.360]   allow us to flip board. Well, it's really interesting because basically from within flip board,
[01:18:49.360 --> 01:18:54.320]   you can you can you can reply to you can like you can repost content from either
[01:18:55.600 --> 01:19:02.720]   pixel fed or blue sky. And it's like, wow, if you can do that on a site like like like flip board,
[01:19:02.720 --> 01:19:10.720]   then wow, it's a lot of publishers, you know, just your earlier point about media folks,
[01:19:10.720 --> 01:19:14.480]   you know, leaving and you know, trying to find someplace other than Twitter. In fact,
[01:19:14.480 --> 01:19:18.000]   if there is federation and an underlying protocol that can link all these things together, it
[01:19:18.000 --> 01:19:21.680]   almost doesn't matter in the short term, where they end up as long as they kind of move together.
[01:19:21.680 --> 01:19:28.400]   Dream. Well, this is a Dave Weiner, Leo, or if you saw Dave Weiner put up a set up at RSS
[01:19:28.400 --> 01:19:32.240]   speed for your blue sky. Fortunately, you have nothing to feed into it. He said about RSS
[01:19:32.240 --> 01:19:37.280]   heat for my blue sky. Yes. Yes, yours and mine and a few others. Well, I better because I mean,
[01:19:37.280 --> 01:19:40.080]   you always have to back to this or I think that's speed.
[01:19:40.080 --> 01:19:48.240]   Does blue sky support RSS natively and a mess it on does? Do you have to doubt it? Yeah,
[01:19:49.040 --> 01:19:51.120]   mostly because I don't think it's super relevant to their audience.
[01:19:51.120 --> 01:19:56.560]   I mean, I don't think it'd be that hard to build. And now that the clients are open source,
[01:19:56.560 --> 01:20:01.280]   yeah, exactly. You must be scraping it probably. It's sort of ironic to like take something that's
[01:20:01.280 --> 01:20:07.360]   like rich, like an activity object and then reduce it down to like RSS. It's like one way,
[01:20:07.360 --> 01:20:11.760]   like you have like a P&G or really great graphics and you're like, let's smash this
[01:20:11.760 --> 01:20:17.680]   into a JPEG. I'm sure that Dave has answers. I know. It is. The last time I talked to Dave,
[01:20:17.680 --> 01:20:23.680]   I said, why do you make me sign up with Twitter on all of your web stuff? And he got mad at me
[01:20:23.680 --> 01:20:29.280]   and I haven't spoken to him since. So I'm glad he's not going to go. I know my clue.
[01:20:29.280 --> 01:20:33.440]   I feed by the way, did you guys see that? It looks like snowflake has actually gone through with
[01:20:33.440 --> 01:20:39.040]   the Niva acquisition. Ah, good. I mean, I guess good. Good for Niva. Yeah. Yeah. Yeah. Snowflake
[01:20:39.040 --> 01:20:43.200]   is an AI company. They were interested not in Niva's search capabilities, but in their AI.
[01:20:44.240 --> 01:20:48.480]   Yes. Was Niva using, do you know, chat GPT or were they doing something separate? I think they
[01:20:48.480 --> 01:20:53.360]   came out before chat GPT. I mean, they might have been using GPT though. They might have been using
[01:20:53.360 --> 01:20:58.800]   the open AI APIs for what they were doing. But yeah. I thought that was actually a good use for
[01:20:58.800 --> 01:21:04.080]   search, but even at Microsoft's event, which there's going on right now,
[01:21:04.080 --> 01:21:10.720]   Bill, talking about search every time on the very tiny print at the bottom, it says results AI
[01:21:10.720 --> 01:21:16.080]   generated not factual accuracy, not guaranteed. And I think that that's a problem
[01:21:16.080 --> 01:21:22.720]   in the long run or maybe just the short run. Well, search may be the wrong place for it.
[01:21:22.720 --> 01:21:26.720]   Yeah. I mean, it's weird that we've lived so long without disclaimers at the bottom of search
[01:21:26.720 --> 01:21:32.320]   results. The fake news reports might be sponsors. Good point. Good point. Okay. You're right. Yeah.
[01:21:32.320 --> 01:21:37.200]   But at least the link is to something that exists. We know that you could also be completely
[01:21:37.200 --> 01:21:41.440]   fabricated. Right. You know, like when you click something on LinkedIn, it's like, oh,
[01:21:41.440 --> 01:21:44.720]   you're leaving the LinkedIn walled garden now and you're like, yeah, watch out. Does that
[01:21:44.720 --> 01:21:50.240]   mean there are snakes? Yeah. Yeah. Do we want to start a pool on how many people get fired at
[01:21:50.240 --> 01:21:58.000]   Twitter after these spaces to back off? Is there anybody? Is Linda is Linda weighing in?
[01:21:58.000 --> 01:22:03.600]   Linda was in the space. So poor. Yeah. Yeah. I don't know. He has more black owned businesses
[01:22:03.600 --> 01:22:07.280]   than any state in the nation. That's what he just said. Who does? Ron's walking around.
[01:22:07.280 --> 01:22:12.000]   Ron is saying Ron is saying Ron is saying he's talking. I have more black owned. That's scary.
[01:22:12.000 --> 01:22:16.080]   That's like saying our binders full of women. Yeah. Because it's merit not identity politics.
[01:22:16.080 --> 01:22:19.760]   But you could you just pop in any second. So it's something interesting.
[01:22:19.760 --> 01:22:25.680]   I know the way that they got this going was by basically booting everybody out there down
[01:22:25.680 --> 01:22:30.800]   about 40,000 starting the server. Yeah. They restarted the server. He loves to do it.
[01:22:30.800 --> 01:22:35.360]   Elon said we've never had there's never been 500,000 people in a room at the same time.
[01:22:35.360 --> 01:22:39.040]   Which is wrong, of course. And that's wrong. And then it crashed.
[01:22:39.040 --> 01:22:48.800]   Be careful what you wish for. That's all I can say. Google has reached a $39.9 million privacy
[01:22:48.800 --> 01:22:55.280]   settlement with Washington state. This is not the only state suing Google over the fact that
[01:22:55.280 --> 01:23:01.760]   they're switched. It turns off location. Didn't turn off location. And they were still collecting
[01:23:01.760 --> 01:23:09.280]   location information. Google has promised. This sort of is such a strange, like it juxtaposes
[01:23:09.280 --> 01:23:14.000]   so well with what we were just talking about with age-eating and tick-tock downloads in Montana.
[01:23:14.000 --> 01:23:18.720]   It's sort of like lots of people would prefer that these companies not collect this data.
[01:23:18.720 --> 01:23:22.720]   But then to do the thing that they actually need to do. In terms of provisioning access,
[01:23:23.280 --> 01:23:28.080]   you can't have the information that's necessary to actually enforce certain rules or laws.
[01:23:28.080 --> 01:23:31.840]   I don't know. It just feels like in the Google case, not guaranteed it's been many years since
[01:23:31.840 --> 01:23:38.160]   I've worked there. But you end up collecting this information by virtue of having an IP address
[01:23:38.160 --> 01:23:42.000]   to deliver a webpage to someone who has come to your server. And it has to go back there.
[01:23:42.000 --> 01:23:48.000]   And the Nexus point has to be someplace on the physical earth. How do you get around that?
[01:23:48.000 --> 01:23:51.840]   Well, this is the other thing about the Facebook find. This huge find.
[01:23:51.840 --> 01:23:58.160]   1.3 billion dollars. Where the issue wasn't Facebook and privacy at all. The issue was America.
[01:23:58.160 --> 01:24:00.480]   It's Europe not trusting data in America.
[01:24:00.480 --> 01:24:04.480]   Same with America and tick-tock and data.
[01:24:04.480 --> 01:24:10.720]   The EU told me that it's the American company and American employees control data that's housed
[01:24:10.720 --> 01:24:13.680]   in a European server. How's that different? They don't trust the American issue. Well,
[01:24:13.680 --> 01:24:16.800]   it's the same tick-tock issue. What difference does it make where this data is?
[01:24:16.800 --> 01:24:22.000]   Tick-tock says, "Well, Oracle's going to store the data." But that wasn't sufficient because
[01:24:22.000 --> 01:24:27.040]   Montana and whoever else said, "But yeah, but the Chinese government can still access it, right?"
[01:24:27.040 --> 01:24:31.360]   So this is what the EU, it's actually Ireland's Data Protection Commission,
[01:24:31.360 --> 01:24:38.000]   is complaining about data being transferred, data collected by Facebook being transferred
[01:24:38.000 --> 01:24:43.920]   off of European servers into the US, about, obviously, about European citizens.
[01:24:43.920 --> 01:24:50.240]   It's so, I don't understand how that works when it comes to edge caching and network optimization.
[01:24:50.240 --> 01:24:57.520]   If I'm a European citizen accessing my Gmail account from Oregon, does that mean that I only
[01:24:57.520 --> 01:25:03.440]   can access my Gmail if it's stored somewhere in the EU? In which case, I'm dealing with the
[01:25:03.440 --> 01:25:07.600]   latency of constantly going back to that. It just seems like it runs against the way in which the
[01:25:07.600 --> 01:25:10.560]   internet was designed and the way its performance works.
[01:25:11.360 --> 01:25:16.720]   If I email somebody in the EU, isn't the entire conversation stored in both places?
[01:25:16.720 --> 01:25:20.640]   Yeah, only one person can see the conversation at a time because they're in different places.
[01:25:20.640 --> 01:25:22.800]   Email's still never meet in the middle.
[01:25:22.800 --> 01:25:31.280]   Yeah, I guess the problem is we have a global internet and it doesn't respect national boundaries,
[01:25:31.280 --> 01:25:34.000]   but nations want it. Imagine that.
[01:25:35.200 --> 01:25:42.880]   And I don't know who wins on that one. You are on T2, which was created by former Twitter
[01:25:42.880 --> 01:25:50.560]   folks, T2.social. It's very much like Twitter. You're on blue sky, Chris.
[01:25:50.560 --> 01:25:55.600]   You're unmasted on. Do you have a dog in this, hon, or are you just watching?
[01:25:55.600 --> 01:26:04.240]   I mean, in the sense that the dream of the original distributed social web,
[01:26:04.240 --> 01:26:10.960]   it's still alive. And I now is more than we've had an opportunity, I think, in most of
[01:26:10.960 --> 01:26:16.720]   internet history to perhaps right the wrong of the centralized social platforms.
[01:26:16.720 --> 01:26:23.040]   As far as it goes, one, I want to be seeing how these platforms are showing up and competing and
[01:26:23.040 --> 01:26:27.440]   how they're innovating and changing things. But then also, if I'm going to be providing guidance
[01:26:27.440 --> 01:26:31.600]   or input to any of the folks who are working on the underlying protocols for interoperability,
[01:26:31.600 --> 01:26:37.920]   we do have to think about what different verbs do in different contexts and what certain
[01:26:37.920 --> 01:26:42.800]   behaviors are allowed in some context or another and what happens when it comes to interoperability
[01:26:42.800 --> 01:26:49.280]   between them. We do have an advantage as kind of led the way. We know what a working social
[01:26:49.280 --> 01:26:53.040]   network looks like. But we know what it looks like from a centralized...
[01:26:53.040 --> 01:26:58.880]   From centralized, yes. They look like it and Google+ was very similar. You have a text box and
[01:26:58.880 --> 01:27:02.320]   you type something or you add some attachments and then it goes into a feed and that's kind of the
[01:27:02.320 --> 01:27:07.200]   whole model. But in this new world, you should be able to follow people that are on different
[01:27:07.200 --> 01:27:10.720]   networks and different platforms that are run by different people. Are you saying they're
[01:27:10.720 --> 01:27:18.320]   different policies? Are you saying that plus didn't take off? No. I think it's useful just to
[01:27:18.320 --> 01:27:23.760]   stay for the record that Google+ was successful for Google. It was not successful for users of the
[01:27:23.760 --> 01:27:31.120]   network, per se. But having... Being the person that designed the Google profile, there were 43
[01:27:31.120 --> 01:27:35.200]   different representations across Google when I got there where you could have a different profile
[01:27:35.200 --> 01:27:40.480]   photo and a different name. There are some benefits to sharding your identity in that way.
[01:27:40.480 --> 01:27:44.160]   But when it comes to interacting with one company, it's not like you want to have 15
[01:27:44.160 --> 01:27:49.440]   different Apple accounts or 15 different YouTube. Maybe you do for business purposes. But as far as
[01:27:49.440 --> 01:27:52.800]   it goes, when you change your name because you got married, you want it to be propagated across
[01:27:52.800 --> 01:27:57.440]   all the places that's necessary with one company. And so what Google+ did was it provided identity
[01:27:57.440 --> 01:28:01.920]   as a backbone for all subsequent Google services to be built around those assumptions, which then
[01:28:01.920 --> 01:28:05.680]   gave you different capabilities and things like Google Docs and Drive so you could share something
[01:28:05.680 --> 01:28:09.600]   to someone and know that the right person had it. And from a business use case, you could then
[01:28:09.600 --> 01:28:14.880]   have people who work for a company and therefore have access to different services or data or
[01:28:14.880 --> 01:28:19.280]   information. And if they leave that company, that information can be revoked without them having
[01:28:19.280 --> 01:28:25.440]   to give up their full Google account. So anyways, in that sense, plus was this success for Google's
[01:28:25.440 --> 01:28:33.200]   business goals. It just wasn't a consumer hit. Now, Google Photos, I will say, is largely a success.
[01:28:33.200 --> 01:28:38.080]   It drives a lot of the machine learning that Google does. And in fact, the product manager,
[01:28:38.080 --> 01:28:42.000]   who's still working on that, is the same product manager that was working on that with Google+.
[01:28:42.000 --> 01:28:46.880]   So it's kind of amazing to see that some folks have really stayed with these products all the way
[01:28:46.880 --> 01:28:49.360]   through, even like 10 years later.
[01:28:49.360 --> 01:28:56.720]   I also think that Google+ improves social networking generally because so many companies,
[01:28:56.720 --> 01:29:02.160]   especially Facebook, try to copy some of the just features. And so even I think this week,
[01:29:02.160 --> 01:29:08.000]   Twitter said, "Oh, we're going to have videos on Twitter that are what, two hours long?"
[01:29:08.000 --> 01:29:12.320]   That Google+ had that in 2000.
[01:29:12.320 --> 01:29:18.720]   Google+ did have a lot of really good ideas. Absolutely. I think the timing was a little
[01:29:18.720 --> 01:29:25.280]   awkward. And the emphasis on desktop browsers was probably it's a killeseal. I remember during
[01:29:25.280 --> 01:29:30.400]   that year, that was when Zuckerberg had his famous hoodie lockdown. And everyone basically had to
[01:29:30.400 --> 01:29:35.040]   stop using Facebook on their desktop laptops and switch over to mobile. And they realized how
[01:29:35.040 --> 01:29:40.720]   bad the experience was, which then allowed Facebook to build what essentially became their mobile
[01:29:40.720 --> 01:29:45.920]   advertising juggernaut, which then of course, Apple then cut off their legs with the app tracking
[01:29:45.920 --> 01:29:51.760]   transparency stuff. But nonetheless, I think if Google had really ironically leaned into Android,
[01:29:51.760 --> 01:29:55.680]   they might have actually done much better by building into the OS. Actually, this is on the
[01:29:55.680 --> 01:30:01.280]   other thing you guys have talked about. But I'm curious to see what happens when WWDC happens this
[01:30:01.280 --> 01:30:05.680]   year. But Apple has been slowly building a social network into the operating system for the last
[01:30:05.680 --> 01:30:12.240]   two or three years. And there is no antitrust concerns because there's no app. There's no kind of
[01:30:12.240 --> 01:30:17.520]   concept of this is the thing that you could pull out from the platform. And yet, that is how Apple
[01:30:17.520 --> 01:30:24.480]   is slowly eroding Facebook's position in the market. For example, if you go to Safari,
[01:30:24.480 --> 01:30:29.840]   there is a 4U section or shared with you that comes from all the different people that have
[01:30:29.840 --> 01:30:34.080]   shared links to you via iMessage. In iMessage, there's a shared with you thing that shows you
[01:30:34.080 --> 01:30:37.920]   all the photos from different people in the photos app. There's stuff from your contacts.
[01:30:37.920 --> 01:30:43.600]   So very, very slowly, you know, boil the frog type method. Apple is introducing social sharing
[01:30:43.600 --> 01:30:48.080]   functions as part of the operating system. That is completely, it seems to me, being
[01:30:48.080 --> 01:30:53.040]   missed by the mainstream press and by analysts because there is no kind of app that you can point
[01:30:53.040 --> 01:30:57.360]   to and say, "Aha, that's the Apple social network." You're exactly right. And if you look at notes
[01:30:57.360 --> 01:31:02.960]   and if you look at free form, you basically share documents that you interact with with people.
[01:31:02.960 --> 01:31:08.400]   And then those connections are persistent. So you go back to it, I do it all the time with family
[01:31:08.400 --> 01:31:14.160]   members and colleagues and stuff like that, where you just go back to the space within those
[01:31:14.160 --> 01:31:18.400]   apps where you were interacting and interact again for a different purpose. And it's just sort of,
[01:31:18.400 --> 01:31:22.240]   you're sort of just sort of connected with all these products.
[01:31:22.240 --> 01:31:29.120]   It's very strange. The contacts in the address book app in Mac OS and on the iPhone are the ways
[01:31:29.120 --> 01:31:32.560]   in which you manage those connections. But there's not an explicit, like,
[01:31:32.560 --> 01:31:36.240]   "Add this person as a friend." You add them to your address book and that is the action that you
[01:31:36.240 --> 01:31:41.920]   take to then connect with them. And that was the vision of Google before and after Google+.
[01:31:41.920 --> 01:31:50.160]   To circle back to your point about mobile and missing mobile, I mean, one of the ways in which
[01:31:50.160 --> 01:31:56.160]   Google+ was light years ahead of everybody else was live streaming. They call it live streaming
[01:31:56.160 --> 01:32:00.960]   back then. But basically you could do a hangout and just basically be live streaming and it would
[01:32:00.960 --> 01:32:06.880]   automatically be live on YouTube and then be stored on YouTube permanently. But it was desktop.
[01:32:06.880 --> 01:32:12.880]   And so if you look at how live streaming turned out to be a huge phenomenon, it was all on mobile.
[01:32:12.880 --> 01:32:17.520]   That's right. Mostly a mobile phenomenon. So if they had just been aggressive, like you said,
[01:32:17.520 --> 01:32:22.240]   about getting that live streaming capability and the group chat live streaming capability
[01:32:22.800 --> 01:32:28.000]   and rushed that to mobile, they would just own the future. That became the big deal and they
[01:32:28.000 --> 01:32:33.520]   just missed it because they were only on the desktop. It was such a blind spot. And looking back,
[01:32:33.520 --> 01:32:38.080]   you can clearly see it, especially everything that's happened with Instagram and Facebook and
[01:32:38.080 --> 01:32:44.400]   WhatsApp. All the parts were there. Google had all of it. But they were just so wedded to again.
[01:32:44.400 --> 01:32:52.240]   And if your business is this huge money volcano, thanks to Search, you don't have the right incentives
[01:32:52.240 --> 01:32:58.000]   really to cut bait from the past. I argued years ago, AOL was Facebook before Facebook.
[01:32:58.000 --> 01:33:02.240]   Sure. AOL was Twitter before Twitter. It could have been all those things, but I couldn't see it.
[01:33:02.240 --> 01:33:13.280]   Where do you stand on the quote tweet? What controversy? Any thoughts? Do we need a quote tweet
[01:33:13.280 --> 01:33:19.520]   unmasted on? Yes. Okay. There you go. That was simple. That was easy. That was fast.
[01:33:19.520 --> 01:33:22.800]   And they're ahead of that way. I would chime in and say that we don't need it, but it would be
[01:33:22.800 --> 01:33:29.920]   nice. I like it. It's easy to construct your own. And as the creator of the hashtag, you know that
[01:33:29.920 --> 01:33:36.640]   these workarounds can be very powerful. You just paste in the link to the original thing if you want
[01:33:36.640 --> 01:33:43.040]   to. But I like, well, tweets, it's easy. It's fun. The people who follow me follow me for my
[01:33:43.040 --> 01:33:50.240]   theoretically, for my opinions, my take on things, as easy to take a tweet and then share my take on
[01:33:50.240 --> 01:33:56.720]   it, it feels good. It's also an ingrained behavior that a lot of people have. So I think it would
[01:33:56.720 --> 01:34:01.120]   be nice. I don't think it'll ever happen. I think the mass, the diverse community is-
[01:34:01.120 --> 01:34:05.600]   No, I think, I think, I think it's been opened to it. When he attended the Black Twitter Summit,
[01:34:05.600 --> 01:34:11.600]   he's been talking about that. And he also, in the Black Twitter Summit we held, he saw
[01:34:12.320 --> 01:34:18.080]   the value of search, open search to a community, to be able to find each other. So he's very open
[01:34:18.080 --> 01:34:22.560]   to those things. And I think there are some really interesting proposals on the mass done get
[01:34:22.560 --> 01:34:28.320]   in terms of putting some limitations on it as a voluntary that you're retweeted. Those kinds of
[01:34:28.320 --> 01:34:34.400]   questions are interesting to reconsider. Yeah, we need it. I think there's a question of enforcement
[01:34:34.400 --> 01:34:42.160]   and behavior and how to conduct a space where you're emphasizing pro-social engagement.
[01:34:42.480 --> 01:34:45.760]   And there's, of course, negative things that are going to happen. I just don't think that the
[01:34:45.760 --> 01:34:49.840]   medium, I mean, yes, there are aspects of the medium itself that can be used to inhibit bad
[01:34:49.840 --> 01:34:56.800]   behavior. But if you're cutting out a huge set of content that people are familiar with and
[01:34:56.800 --> 01:35:02.560]   comfortable using, and they can use that to actually build up other people's voices and provide
[01:35:02.560 --> 01:35:06.160]   distribution and attention, and you're like, well, let's just get rid of all of it,
[01:35:06.800 --> 01:35:15.440]   because 20% of it is nefarious, seems to ignore, again, where the control should be put. If someone
[01:35:15.440 --> 01:35:20.880]   is going to be negative or mean and these things absolutely happen, that to me seems like that
[01:35:20.880 --> 01:35:23.840]   should be at the level of the individual and the person should be blocked or they should
[01:35:23.840 --> 01:35:27.680]   be centered or something else. Exactly. And this is another area where I wish people would
[01:35:27.680 --> 01:35:32.880]   copy Google+ again, because so Google+ had an awesome phase. It was like three years,
[01:35:32.880 --> 01:35:38.240]   which was amazing. And then it sort of turned a crap around the last few years. But there was
[01:35:38.240 --> 01:35:43.440]   one awesome thing that turned a crap in the last few years, which was that a single click would
[01:35:43.440 --> 01:35:51.040]   block, report, and remove the comment. Yes. That was just so powerful. Just boom, and you just nuke
[01:35:51.040 --> 01:35:56.800]   somebody and their comment has gone forever. They reported and they're blocked. If every social
[01:35:56.800 --> 01:36:01.280]   network had that, there would be no trolling. There would be no hate speech. I mean, it would be
[01:36:01.280 --> 01:36:05.200]   amazing. Oh, you're advocating before it. Yeah. Yeah.
[01:36:05.200 --> 01:36:11.680]   Yes, actually. So good. On this point, I don't know if you guys have seen or tried out what Blue
[01:36:11.680 --> 01:36:16.720]   Sky now offers with their moderation lists. And I don't know exactly how they're going to work.
[01:36:16.720 --> 01:36:21.520]   It's very interesting that everything on Blue Sky is public, including who you block and those
[01:36:21.520 --> 01:36:26.080]   interactions. But as a decentralized platform, for better or worse, you do need to embrace some
[01:36:26.080 --> 01:36:30.560]   aspects of transparency in order for the system to function in a decentralized way.
[01:36:30.560 --> 01:36:35.200]   And so now you have the ability to subscribe to block lists. And I think the easiest way to
[01:36:35.200 --> 01:36:39.040]   understand these block lists on Blue Sky is just to think about an ad blocker. If you have an ad
[01:36:39.040 --> 01:36:43.280]   blocker that allows you to subscribe to block lists, then you can have multiple different sources
[01:36:43.280 --> 01:36:47.440]   and different perspectives on the things that you want to remove or get rid of on the internet,
[01:36:47.440 --> 01:36:54.000]   whether those are cookie banners or ads themselves, or there's even some ad block lists that allow
[01:36:54.000 --> 01:36:58.880]   you to allow some ads in that you might consider to be positive, or that you want to support the
[01:36:58.880 --> 01:37:02.320]   producers of different websites or whatever. In a similar way, this is going to bring that
[01:37:02.320 --> 01:37:08.720]   type of capability to the internet and then to crowdsource collaboration on those types of lists.
[01:37:08.720 --> 01:37:13.280]   This is interesting. So have people created, this is Blue Sky. I have people created
[01:37:13.280 --> 01:37:17.440]   this. Mute lists that I would have to know what it was, I guess. And then I could join them.
[01:37:17.440 --> 01:37:22.080]   Yeah. Let me see if I can find one. And your subscription to it is public.
[01:37:22.080 --> 01:37:28.320]   Yeah. But yeah. Yeah. I haven't muted any accounts, nor have I blocked any accounts.
[01:37:28.320 --> 01:37:32.240]   Oh, wait a minute. I did block one account. I don't know why, but well,
[01:37:32.240 --> 01:37:41.440]   but I haven't had much occasion to block Blue Sky. My only playable Blue Sky is this kind of
[01:37:41.440 --> 01:37:47.600]   silly right now because I think we'll giddy having left Twitter. They're just giddy and they're so
[01:37:47.600 --> 01:37:54.160]   happy that you can post pictures of a valve that they just go crazy. But yeah, settling down a
[01:37:54.160 --> 01:38:00.000]   little bit, settling down. I just saw discussion on Blue Sky of race and health threads.
[01:38:00.000 --> 01:38:05.840]   I don't understand the health thread thing. It's accidentally right. Yeah. Let me take a little
[01:38:05.840 --> 01:38:09.840]   break. I want to talk more about this, but I need to take a break. Christopher Cena is here.
[01:38:09.840 --> 01:38:16.880]   And what a great opportunity to have one of the legendary creators of all that is good and
[01:38:16.880 --> 01:38:22.880]   wonderful in the universe with us, including the hashtag and a lot of so I didn't realize you'd
[01:38:22.880 --> 01:38:27.680]   worked on activity stream. I guess I knew that way back when back in the day, it was way back then.
[01:38:27.680 --> 01:38:31.760]   Yeah, I kind of took a hiatus. A supporter of open web standards. We like that.
[01:38:31.760 --> 01:38:39.600]   Also, my gal, my friend from points southeast, north and west, astronomad.net.
[01:38:39.600 --> 01:38:42.960]   Yep. Great to have you. He's in the Veneto right now.
[01:38:42.960 --> 01:38:46.080]   Ready to be here. So that's right. See your ancestral homeland.
[01:38:46.080 --> 01:38:53.040]   My homeland, my people. I would. Oh, man, I would live in Rome if I could. I would just move there.
[01:38:53.040 --> 01:38:59.440]   I would just go. That'd be it. Goodbye. Can't. But I would. It'll be as perfectly delightful.
[01:38:59.440 --> 01:39:04.400]   I mean, Veneto is my favorite. I'd like to die in places. Yeah, I don't think I'd
[01:39:04.400 --> 01:39:09.440]   visit. I've been to Venice, of course, but it's the rest of it. It's unbelievable. It says green
[01:39:09.440 --> 01:39:13.200]   is Ireland and it's just rolling hills of vineyards just having on their own.
[01:39:13.200 --> 01:39:15.280]   Did they have flooding problems last week or no?
[01:39:15.280 --> 01:39:23.200]   Amelia, you did. But they have water everywhere. Everything's so green. There's these big canals
[01:39:23.200 --> 01:39:29.920]   of rushing water, but it's well controlled. They've had problems in the past, but in modern
[01:39:29.920 --> 01:39:35.040]   times, they've really controlled the water flow here. Lovely. And of course, Jeff Jarvis,
[01:39:35.040 --> 01:39:40.960]   who is in New Jersey, that's all I'm going to say. Also, my answer is going to be a throttle today
[01:39:40.960 --> 01:39:44.800]   and not in the show. Oh, I'm sorry. What happened? I just too much going on.
[01:39:44.800 --> 01:39:48.400]   Okay. I'm glad you're here. I bet you're glad you're here stuck with me.
[01:39:48.400 --> 01:39:54.960]   So, I'm always here, Leo. Bad penny. I can't get rid of me. I try. I try. I know you do.
[01:39:54.960 --> 01:39:59.920]   Musmachine.com. I've got something they'll chase you away, actually, in coming up in a little bit.
[01:39:59.920 --> 01:40:05.280]   But first, a word from Zip Recruiter. Whether you're starting a new business or growing one,
[01:40:05.280 --> 01:40:10.080]   I've learned this is a owner of a small business. If you want to be successful,
[01:40:10.080 --> 01:40:15.360]   you got to have the people, right? Businesses are made of people. And the better your people,
[01:40:15.360 --> 01:40:20.320]   the better your team, the better your business. That's where Zip Recruiter comes in.
[01:40:20.320 --> 01:40:28.720]   Right now, you could try it for free at ziprecruiter.com/twig. No matter how big or small your business,
[01:40:28.720 --> 01:40:35.120]   hiring the right person elevates it. Hiring the wrong person can bring it down. So, if you're
[01:40:35.120 --> 01:40:41.520]   starting a new business as small as a taco trucker, as big as the next charity save the world,
[01:40:41.520 --> 01:40:47.440]   you need Zip Recruiter. Because Zip Recruiter has powerful matching technology that finds candidates
[01:40:47.440 --> 01:40:53.200]   for you for a wide range of roles. When you post in Zip Recruiter, we use it. That's how we hired,
[01:40:53.200 --> 01:40:59.520]   most recently hired Viva. We needed somebody in our continuity department. Lisa gets up in the
[01:40:59.520 --> 01:41:04.160]   morning. She's on the post. She posts in Zip Recruiter, immediately goes to 100 plus job boards.
[01:41:04.160 --> 01:41:10.960]   So, you're casting a very wide net. You can add tags that really help grab applicants' attention.
[01:41:10.960 --> 01:41:18.160]   Things like remote, if you're, you know, allow remote worker or training provided or urgent or
[01:41:18.160 --> 01:41:24.640]   whether whatever you want to help your job stand out. Then, but as I said, then Zip Recruiter adds
[01:41:24.640 --> 01:41:28.880]   to the value by going out and looking at all the resumes. They have more than a million current
[01:41:28.880 --> 01:41:33.600]   resumes on hand. And seeing if they can find people who meet your qualifications, if they do,
[01:41:33.600 --> 01:41:37.920]   they tell you about them. You can look at those people and then ask them to apply. And there's
[01:41:37.920 --> 01:41:41.200]   something magic happens when you ask somebody to apply. They're very, they're thrilled. They're
[01:41:41.200 --> 01:41:46.880]   honored. They're touched. They respond. They come to the interviews. It's hard to hire these days,
[01:41:46.880 --> 01:41:50.960]   but Zip Recruiter is the best way to do it. The easiest way to do it. And I'll tell you,
[01:41:50.960 --> 01:41:55.920]   Lisa got up in the morning. We posted the job for Viva. But within an hour, she started to get
[01:41:55.920 --> 01:42:00.640]   great candidates. In fact, that was the hard thing. We had so many great candidates. We had to
[01:42:00.640 --> 01:42:06.240]   choose one. That's beautiful. That's a nice position to be in. Let's Zip Recruiter fill all your roles
[01:42:06.240 --> 01:42:10.800]   with the right candidates. Four out of five employers who post on Zip Recruiter get a quality
[01:42:10.800 --> 01:42:15.200]   candidate within the first day. See for yourself, we've got a great address for you. They'll get
[01:42:15.200 --> 01:42:26.400]   you a chance to try it for free ziprecruiter.com/twig that's ziprecruiter.com/twig that's ziprecruiter.com/twig
[01:42:28.480 --> 01:42:35.840]   the smartest way to hire. We use it. We love it. You should use it to ziprecruiter.com/twig.
[01:42:35.840 --> 01:42:43.200]   Already dug into the Santa's for a minute. He was saying, "Disney is in such bad
[01:42:43.200 --> 01:42:46.160]   ship. They're stocked. They couldn't afford to build that building anyway." Oh, please.
[01:42:46.160 --> 01:42:49.520]   And then he wants to talk about education and they're bringing in Chris Rufo,
[01:42:49.520 --> 01:42:53.280]   major known intellectual, to talk about that. I turned back to.
[01:42:53.280 --> 01:42:56.320]   Already the Tech Crunch reporting technical issues play.
[01:42:56.320 --> 01:43:01.040]   Grande de Santis's presidential announcement on Twitter. You heard it happen here. Live,
[01:43:01.040 --> 01:43:05.680]   the product, the product, which allows live streamed group voice chats cut quickly, cut out
[01:43:05.680 --> 01:43:11.200]   quickly after going live at 3 p.m. The audio glitched in and out of times, only played reverb
[01:43:11.200 --> 01:43:17.440]   noises along with some distorted speech. As of 3.15, there were 580,000 listeners on the Twitter
[01:43:17.440 --> 01:43:24.160]   space update by 3.30. An alternative space with the live. Grande de Santis was up and running,
[01:43:24.880 --> 01:43:28.960]   though it only attracted a fraction of the initial user base. How many do you have in there right now?
[01:43:28.960 --> 01:43:33.360]   Jeff, it was about 147 last time. I heard 47,000 last time I checked.
[01:43:33.360 --> 01:43:37.360]   He announced, of course, as he intended to, that he's running for president.
[01:43:37.360 --> 01:43:45.920]   I also saw Rupert Murdoch's response to this in the Daily Mirror.
[01:43:45.920 --> 01:43:48.720]   Ron disaster, they called it.
[01:43:52.960 --> 01:43:58.480]   Rupert's not a fan, I think. I don't know who he's a fan of, but not of Ron, I guess.
[01:43:58.480 --> 01:44:02.240]   Somebody on blue sky,
[01:44:02.240 --> 01:44:08.720]   skated, right? Is it a ski? It's a ski. Yeah. I think the CEO called it a post, but sure,
[01:44:08.720 --> 01:44:12.480]   you call it a ski. I wanted to call it a bleep. I wanted to call it a bleep. That would have been
[01:44:12.480 --> 01:44:18.080]   so much better. They're gonna be lambs. Anyway, somebody bleeded. This has to get Tucker's Carlson
[01:44:18.080 --> 01:44:24.560]   really scared because Tucker routinely has better on the platform. Multi millions watching his show.
[01:44:24.560 --> 01:44:28.880]   If they can't handle half a million people, that's problematic.
[01:44:28.880 --> 01:44:35.280]   Well, here's some context. Before Elon Musk fired everybody, in November 2022,
[01:44:35.280 --> 01:44:43.840]   Elon Musk himself did a space with 2.7 million people. In December, he did another one with 2.2 million
[01:44:44.560 --> 01:44:53.280]   in May of this month. Actually, the former prime minister of Pakistan did a space with
[01:44:53.280 --> 01:44:59.280]   I guess only 65k, but there have been bigger spaces than this bar.
[01:44:59.280 --> 01:45:02.640]   Interesting. Back in the day, when people still worked there.
[01:45:02.640 --> 01:45:07.120]   Yes, employees are good. He's had six months to really, you know,
[01:45:07.120 --> 01:45:14.000]   firm up that infrastructure. That's the wrong direction. That's wow. Okay.
[01:45:14.000 --> 01:45:20.400]   Okay. How many people can true social support? Like 20? That's a good question.
[01:45:20.400 --> 01:45:24.400]   That's running on a forked masted on. It's probably pretty good. It is.
[01:45:24.400 --> 01:45:29.840]   It might be more stable depending on how well they forked the code.
[01:45:29.840 --> 01:45:31.760]   Probably don't have the money though to pay for it.
[01:45:31.760 --> 01:45:37.120]   Yeah, the Washington Post, the Santa's presidential campaign kickoff on Twitter plagued by technical
[01:45:37.120 --> 01:45:44.160]   issues. This is not not a good look for either DeSantis or a lead story in New York Times,
[01:45:44.160 --> 01:45:47.680]   to Santa's announcement derailed by Twitter malfunction.
[01:45:47.680 --> 01:45:55.040]   Not a good look. This is the post of the Twitter space where you set to announce came to a
[01:45:55.040 --> 01:46:01.040]   halt after roughly 20 glitch plagued minutes filled with long stretches of silence and messages
[01:46:01.600 --> 01:46:05.600]   like details not available. The funniest thing was we heard a little bit of it.
[01:46:05.600 --> 01:46:12.720]   Elon muttering to himself. It's not working. I don't think I got Elon. I would not want to be
[01:46:12.720 --> 01:46:16.480]   shot. I'm afraid of the name is. I know I'm sorry. We shouldn't. We shouldn't.
[01:46:16.480 --> 01:46:22.320]   Daily Mail, the Daily Mail in the UK said it's the biggest fail in campaign history.
[01:46:22.320 --> 01:46:23.200]   Holy.
[01:46:23.200 --> 01:46:27.280]   On Descentis is launched for Shambolic Twitter room presidential announcement of the Elon Musk
[01:46:27.280 --> 01:46:31.360]   that crashes five times and leaves users bamboozled.
[01:46:31.360 --> 01:46:36.800]   It does seem like a strange decision to do it on Twitter, to be honest.
[01:46:36.800 --> 01:46:42.320]   I mean, he's filling the vacuum that Trump had previously. So it had it gone well.
[01:46:42.320 --> 01:46:47.600]   It had the potential to be very smart because if Trump isn't there,
[01:46:47.600 --> 01:46:51.120]   but then Descentis can kind of just rile those folks up and they're looking for the game that
[01:46:51.120 --> 01:46:54.640]   Twitter was while Trump was on the best where the bases were was.
[01:46:54.640 --> 01:47:01.360]   Yeah. But if neither Descentis nor Elon are very good at Twitter,
[01:47:01.360 --> 01:47:04.000]   then maybe that's not such a good campaign strategy.
[01:47:04.000 --> 01:47:08.160]   I prepared a special story just for you, Jeff Jarvis. Today, the 140th,
[01:47:08.160 --> 01:47:14.480]   40th birthday of the Brooklyn Bridge. Probably you don't know this, Chris, but for some reason,
[01:47:14.480 --> 01:47:15.600]   Jeff doesn't like bridges.
[01:47:15.600 --> 01:47:20.640]   I'm Bridge Fobek. I started trying to walk across. I lived in Brooklyn Heights. You'd think
[01:47:20.640 --> 01:47:23.840]   that I would have hung out there and written my novels there or something.
[01:47:23.840 --> 01:47:27.760]   I tried to start walking across it and I said, "I can see down." No, no, no.
[01:47:27.760 --> 01:47:30.240]   You can't. You can't. You can see, did you fly? Do you use airplanes?
[01:47:30.240 --> 01:47:37.040]   Well, yeah, that's okay. I'm in close. They fly. They can fly. Yeah.
[01:47:37.040 --> 01:47:43.280]   You would not like this video. I can't. You would not like this video on the New York Times
[01:47:43.280 --> 01:47:48.880]   website, an aluminum interview with Ken Burns, which immediately shows the cracks in the bridge
[01:47:48.880 --> 01:47:58.080]   and the water beneath. I love the Brooklyn Bridge. It's one of the great landmarks of America.
[01:47:58.080 --> 01:48:04.080]   And I just want to celebrate its 140th birthday. It's a wonderful, amazing thing.
[01:48:04.080 --> 01:48:10.560]   I've driven over it, but happily. New York was inching towards its new role as a world
[01:48:10.560 --> 01:48:17.200]   class city and the bridge helped put it there. You know what I just started? I'm really excited.
[01:48:17.200 --> 01:48:22.800]   Robert Cara's biography of Robert Moses. Yeah, people love it.
[01:48:22.800 --> 01:48:27.040]   Which is, yeah, I mean, look, I barely know who Robert Moses is.
[01:48:27.040 --> 01:48:34.320]   He'd be just a huge impact on New York. On New York. Yeah. And on all kinds of communities
[01:48:34.320 --> 01:48:38.080]   that he's split up. He's a former New Yorker and so forth. And they're still being dealt with.
[01:48:38.080 --> 01:48:42.720]   Oh, interesting. But, and Caro, I've read his LBJ biography. I know he's one of the great
[01:48:42.720 --> 01:48:47.920]   biographers and everybody agrees. This is the biography to beat all biographies. So I can't
[01:48:47.920 --> 01:48:53.360]   wait to read it. I've never actually read a listen to it. Yeah, I'm listening to it. So far,
[01:48:53.360 --> 01:48:57.680]   it's a lot of anecdotes, which I like, you know, and brings it to life, right? Stories about what
[01:48:57.680 --> 01:49:05.520]   what happened. FCC Commissioner nominee. We've got a new one. The White House has nominated
[01:49:05.520 --> 01:49:12.960]   veteran government attorney Anna Gomez. Remember, of course, for a long time, the FCC is still
[01:49:12.960 --> 01:49:18.400]   undermanned because for a long time, Gigi Sohn, who was Biden's first FCC nominee, was blocked.
[01:49:18.400 --> 01:49:26.240]   Since late 2021, Sohn finally pulled herself from consideration last month.
[01:49:26.240 --> 01:49:31.840]   She was a big supporter of net neutrality, one of the people we really wanted to get in there. But
[01:49:32.800 --> 01:49:40.880]   telecom lobbyists did not want her in there. So Biden has nominated Anna Gomez. Don't know what her
[01:49:40.880 --> 01:49:46.560]   opinion is of net neutrality. I hope she's a supporter thereof. At least she didn't come from
[01:49:46.560 --> 01:49:52.480]   the private sector. She was previously a deputy administrator for the National
[01:49:52.480 --> 01:49:58.720]   Telecommunications and Information Administration has worked for the FCC for 12 years. He also nominated
[01:49:59.600 --> 01:50:06.800]   Car is back. I'm surprised he nominated Brendan Carr. I am too. Gee, this car is really one of the
[01:50:06.800 --> 01:50:12.960]   most reactionary FCC commissioners. Yes. And Democrat Jeffrey Starks. Well, don't forget now,
[01:50:12.960 --> 01:50:17.680]   everybody on both sides of the platform hate the internet. So that's fun. I think probably Car
[01:50:17.680 --> 01:50:23.520]   is a gift to the Republicans hoping that they will finally let him fill this fill the open
[01:50:23.520 --> 01:50:33.200]   Democratic seat. But I don't know. What else? Oh, they're laughing about the sandus on
[01:50:33.200 --> 01:50:39.360]   MSNBC right now. Yeah, much learning. Yeah. It is Sean Freud. And I think that's probably
[01:50:39.360 --> 01:50:47.600]   I apologize. I apologize. Oh, we we had Kathy Gellis on Sunday on Twitter because of course she wrote
[01:50:47.600 --> 01:50:55.440]   two amicus briefs that were critical in the Supreme Court's decisions from last week. She won one and
[01:50:55.440 --> 01:51:04.880]   lost one. She lost the fair use case, the Andy Warhol Foundation being sued by a photographer
[01:51:04.880 --> 01:51:11.520]   for a print that Andy Warhol made of a picture the photographer made. Kathy's point, which so far
[01:51:11.520 --> 01:51:18.000]   no one else has picked up on, but I thought was very interesting was this wasn't this was kind of
[01:51:18.000 --> 01:51:26.960]   suing an intermediary. The analogy I made is, you know, Michael Jackson did a song called Bad.
[01:51:26.960 --> 01:51:35.520]   Weird Al made a song called Fat. If I were to play Fat, it would be as if I played Fat on
[01:51:35.520 --> 01:51:42.800]   Twitter and the Michael Jackson estate sued me for playing the parody. And that's in effect what
[01:51:42.800 --> 01:51:46.960]   happened here. She says the Supreme Court got a completely wrong and undermined fair use,
[01:51:46.960 --> 01:51:51.600]   which is too much. Which is an issue for AI too. Big issue for gender.
[01:51:51.600 --> 01:51:56.960]   AI absolutely absolutely right. But before the copyright stuff was going against copyright
[01:51:56.960 --> 01:52:00.960]   when it came to AI, and this reverses it in a way that I think can be very damaging.
[01:52:03.040 --> 01:52:07.440]   But it also has to be said that Weird Al always got permission from the
[01:52:07.440 --> 01:52:11.600]   did and that's why it's not a perfect it's not a perfect analogy. Right.
[01:52:11.600 --> 01:52:16.960]   Better to get something where somebody did a fair use of you know legal parody of something
[01:52:16.960 --> 01:52:21.040]   and then I played the parody but got sued by the originals. The other weird thing about this is
[01:52:21.040 --> 01:52:25.920]   that the magazine had previously got permission to use photograph. That's right. 400 bucks.
[01:52:26.800 --> 01:52:33.840]   But then this is a different use and this is the whole issue. And the problem with this sort of
[01:52:33.840 --> 01:52:41.280]   thing also is that clearly Andy Warhol was using that specific photograph clearly. But
[01:52:41.280 --> 01:52:48.880]   there's a line there that where an artist can use a photograph to the point where it's unrecognizable
[01:52:48.880 --> 01:52:56.720]   and then how do you, so our judges going to be deciding where that line is with AI for example.
[01:52:57.600 --> 01:53:02.000]   Where they're going to look at something and say oh yeah I can tell that that's using this
[01:53:02.000 --> 01:53:08.880]   artist's style. So we're going to rule because I the judge have decided that it reminds me of the
[01:53:08.880 --> 01:53:15.840]   style. It just seems un-unajutable I guess is the thing that's me.
[01:53:15.840 --> 01:53:21.520]   To your point, Jeff, I had thought the courts would uphold the right of AI to create new creations
[01:53:23.200 --> 01:53:28.400]   from stuff that they had learned from existing artists. I thought that was a clear use of terms,
[01:53:28.400 --> 01:53:33.280]   formative use of the work and would easily be supported by the courts now. I don't know.
[01:53:33.280 --> 01:53:37.200]   But this was of course was not a case about AI. This was a case possibly different.
[01:53:37.200 --> 01:53:41.200]   And things aren't recorded. They're learned from.
[01:53:41.200 --> 01:53:48.000]   And the actual work is discarded in a sense out of memory. So that may make a difference. I don't
[01:53:48.000 --> 01:53:50.800]   know Chris, I'm curious to hear your perspective on this. I see you nodding.
[01:53:50.800 --> 01:53:55.760]   Yeah, I was, you know, as you were thinking about this or describing it, you know, it occurred to me
[01:53:55.760 --> 01:54:04.640]   that as we try to figure out what to do with like static media and media that of course exists,
[01:54:04.640 --> 01:54:08.960]   you know, let's say you frame it, you put up on a wall or you print a bunch of magazine covers
[01:54:08.960 --> 01:54:14.880]   of something that is static. Fair use in that context. And I believe in this case was about
[01:54:14.880 --> 01:54:20.480]   whether or not the original work has been essentially usurped or replaced with something that is
[01:54:20.480 --> 01:54:27.200]   more or less equivalent by a different artist who then makes money or commercializes that reuse.
[01:54:27.200 --> 01:54:31.280]   So in other words, the original photograph could have been used for the, I believe this was a
[01:54:31.280 --> 01:54:37.280]   like a 20 year anniversary. Yeah, something, whether it was about prints or whether it was
[01:54:37.280 --> 01:54:41.680]   about the photography. I don't know. It was about prints. Yeah. The photo was of prints. I don't
[01:54:41.680 --> 01:54:46.640]   know if it was anyways, whatever the case was. It was an article about prints. Yeah. I believe that
[01:54:46.640 --> 01:54:53.280]   the Supreme Court was saying, well, essentially, the fair use doctrine really applies to things that
[01:54:53.280 --> 01:54:58.320]   are in sort of a different commercial context. You can take things and editorialize them. You
[01:54:58.320 --> 01:55:02.320]   can take things and talk about them. You can take things and create a derivative work that is
[01:55:02.320 --> 01:55:08.560]   substantially transformed. And of course, the substantially part is of question. But in the
[01:55:08.560 --> 01:55:13.120]   commercial context, I think Leo, what you were saying, where you would be sued by the original
[01:55:13.120 --> 01:55:19.120]   copyright owner is because you are essentially choosing to use the derivative work instead of
[01:55:19.120 --> 01:55:24.880]   the original, although the thing that you paid for is essentially like a copy or replacement of
[01:55:24.880 --> 01:55:29.600]   the original. So you're paying for a facsimile as opposed to the original and therefore the
[01:55:29.600 --> 01:55:36.960]   person who created the original should get compensation for that. In the case of AI artwork and going
[01:55:36.960 --> 01:55:42.960]   forward, AI work in general, it begs the question, if you can simply conjure up a new image every
[01:55:42.960 --> 01:55:49.040]   time someone looks at something, what is the nature of copyright? Do they need to be copyrighted?
[01:55:49.040 --> 01:55:56.960]   Essentially, you can buy stills from a film, for example, put on your wall, it's the celluloid
[01:55:56.960 --> 01:56:03.200]   or whatever. In the future, if most graphics or images are to be generated on the fly by AI
[01:56:03.200 --> 01:56:06.640]   and produced based on a training model, what is it that you're actually
[01:56:06.640 --> 01:56:13.760]   copywriting and what actually is being sold in the marketplace? So this is a somewhat very backwards
[01:56:13.760 --> 01:56:17.280]   kind of perspective. Now granted, a lot of this static content is going to continue to be produced
[01:56:17.280 --> 01:56:20.880]   and we will produce static images that we'd like and then we want to pay for it and want to buy.
[01:56:20.880 --> 01:56:26.480]   But in the case of artificial intelligence, if someone says, oh, that's a copyright infringement
[01:56:26.480 --> 01:56:30.880]   on my work, well, you could literally like roll the dice and just sort of generate a new image
[01:56:31.520 --> 01:56:35.600]   from the same training data that is substantially transformed or different,
[01:56:35.600 --> 01:56:39.760]   such that copyright maybe doesn't apply in the same way. And I don't know how it applies to motion
[01:56:39.760 --> 01:56:46.880]   images, but that's a question. So from Skoda's blog, all agree that Warhol's artistic contribution
[01:56:46.880 --> 01:56:51.840]   is substantial, reflects an aesthetic intention not shared by the photographer Goldsmith.
[01:56:51.840 --> 01:56:58.400]   But for Sotomayor who wrote the majority decision, the only relevant feature of Warhol's use is
[01:56:58.400 --> 01:57:04.160]   that it is a commercial licensing of an image created by Warhol based on Goldsmith's work
[01:57:04.160 --> 01:57:13.200]   for a fee. It was the commercial aspect of it. So if for instance, let's say I buy an AI generated
[01:57:13.200 --> 01:57:17.920]   work of art that looks very much like Thomas Kincaid's paintings, Master of Light.
[01:57:17.920 --> 01:57:19.280]   Ooh, why wouldn't
[01:57:19.280 --> 01:57:24.880]   Then Thomas Kincaid for only example,
[01:57:26.320 --> 01:57:32.880]   and then Thomas Kincaid's sous me saying, and this is the Supreme Court's issue,
[01:57:32.880 --> 01:57:37.760]   instead of buying an original from Thomas Kincaid, I bought an AI version of it.
[01:57:37.760 --> 01:57:44.960]   And that ish as to the use of the magazine in the magazine issue here, those Sotomayor is clear,
[01:57:44.960 --> 01:57:49.760]   Goldsmith's works a celebrity photograph commonly used to accompany stories about the celebrity,
[01:57:49.760 --> 01:57:56.080]   often in magazines. Her point of view is that instead of paying the photographer for the
[01:57:56.080 --> 01:57:59.040]   original image, they use the derivative image.
[01:57:59.040 --> 01:58:01.120]   But they didn't want the original.
[01:58:01.120 --> 01:58:02.720]   And it took money away from the photographer.
[01:58:02.720 --> 01:58:03.680]   It's like a knockoff bag.
[01:58:03.680 --> 01:58:04.720]   It's like a knockoff bag.
[01:58:04.720 --> 01:58:05.200]   It's like a knockoff bag.
[01:58:05.200 --> 01:58:10.000]   And so in the AI example, if I used an AI that was derivative of Kincaid,
[01:58:10.000 --> 01:58:16.000]   Kincaid, according to this judgment, could legitimately go after me for commercial use of
[01:58:16.000 --> 01:58:19.040]   a painting created by an AI, but derivative of his.
[01:58:19.040 --> 01:58:24.480]   I don't know that you could show similarity, but I don't know if you have to look an awful lot
[01:58:24.480 --> 01:58:25.440]   like his, right?
[01:58:25.440 --> 01:58:31.920]   But again, that's human perception as opposed to the actual thing and the way in which these
[01:58:31.920 --> 01:58:38.160]   data sets are trained, it actually is an amalgamation of, let's say, thousands of different
[01:58:38.160 --> 01:58:43.200]   types of art. And so then how do you, then is it fractional infringement?
[01:58:43.200 --> 01:58:46.160]   I have to pay $10 to Kincaid.
[01:58:46.160 --> 01:58:52.160]   At $5, that's what news publishers are begging for, as pay us for radio use.
[01:58:52.800 --> 01:58:56.240]   You raise a really interesting issue with where the EU is going on,
[01:58:56.240 --> 01:59:01.040]   regulating AI, where the intermediary liability takes a whole new quantum leap.
[01:59:01.040 --> 01:59:07.280]   Whoever created the original structure becomes liable for anything ever done by anybody with that
[01:59:07.280 --> 01:59:07.760]   structure.
[01:59:07.760 --> 01:59:13.760]   So if I drive my Tesla, or my Tesla drives into a house and kills somebody,
[01:59:13.760 --> 01:59:17.520]   Elon's responsible, or Elon's programmers are responsible.
[01:59:17.520 --> 01:59:19.120]   Yes, yes, yes.
[01:59:19.120 --> 01:59:23.280]   And that also negates any open sourcing of it.
[01:59:23.280 --> 01:59:23.600]   Right.
[01:59:23.600 --> 01:59:24.800]   Yeah, you can't.
[01:59:24.800 --> 01:59:27.120]   Which is, which is, yeah, how does that work for guns?
[01:59:27.120 --> 01:59:28.720]   Oh, interesting.
[01:59:28.720 --> 01:59:30.400]   Oh, no, we're not going to go there.
[01:59:30.400 --> 01:59:31.040]   Video games.
[01:59:31.040 --> 01:59:31.760]   I'm going to go there.
[01:59:31.760 --> 01:59:32.240]   I'm going to go there.
[01:59:32.240 --> 01:59:32.720]   That's right.
[01:59:32.720 --> 01:59:33.120]   That's right.
[01:59:33.120 --> 01:59:33.280]   Sorry.
[01:59:33.280 --> 01:59:33.680]   Sorry.
[01:59:33.680 --> 01:59:33.680]   Sorry.
[01:59:33.680 --> 01:59:34.240]   Sorry.
[01:59:34.240 --> 01:59:34.240]   Sorry.
[01:59:34.240 --> 01:59:34.240]   Sorry.
[01:59:34.240 --> 01:59:34.720]   Sorry.
[01:59:34.720 --> 01:59:34.720]   Sorry.
[01:59:34.720 --> 01:59:35.680]   You're going to straight.
[01:59:35.680 --> 01:59:36.080]   You're going to straight.
[01:59:36.080 --> 01:59:36.960]   I know.
[01:59:36.960 --> 01:59:37.920]   My apologies.
[01:59:37.920 --> 01:59:42.480]   The other Supreme Court case, and actually it was both cases was Gonzalez versus Google
[01:59:42.480 --> 01:59:44.240]   and Twitter versus Tamna.
[01:59:44.240 --> 01:59:48.960]   The court rolled them up into one and said, because we have decided Tamna,
[01:59:49.840 --> 01:59:51.360]   we don't have to decide Gonzalez.
[01:59:51.360 --> 01:59:55.120]   We don't have to decide Google because it's the same case,
[01:59:55.120 --> 01:59:57.040]   which it kind of was.
[01:59:57.040 --> 02:00:01.040]   Remember though, in the oral arguments for Gonzalez versus Google,
[02:00:01.040 --> 02:00:08.400]   the Gonzalez attorney was saying YouTube was actually responsible because
[02:00:08.400 --> 02:00:16.000]   their algorithm published this ISIS video and created thumbnails.
[02:00:16.640 --> 02:00:19.200]   That made them a publisher of the content.
[02:00:19.200 --> 02:00:27.200]   The good news is while nobody mentioned Section 230, it didn't even get to that point,
[02:00:27.200 --> 02:00:32.960]   most of the decision was on detailed ruling on Tamna was written by Clarence Thomas,
[02:00:32.960 --> 02:00:36.720]   who actually, first of all, the justices voted nine nothing
[02:00:36.720 --> 02:00:40.160]   in favor of Twitter.
[02:00:40.160 --> 02:00:42.560]   So that's, I mean, it's unanimous.
[02:00:43.440 --> 02:00:49.120]   But Clarence Thomas actually kind of surprised everybody, surprised me for sure,
[02:00:49.120 --> 02:00:53.120]   by writing a very well thought out piece.
[02:00:53.120 --> 02:00:55.280]   But it doesn't mention 230.
[02:00:55.280 --> 02:00:58.400]   What it really came down to is what is aiding and abetting?
[02:00:58.400 --> 02:01:03.200]   And can Twitter be considered aiding it because they were going after them on the terrorists,
[02:01:03.200 --> 02:01:04.480]   aiding and abetting of terrorists?
[02:01:04.480 --> 02:01:07.840]   Well, I believe that the thing was that they were essentially asserting that
[02:01:07.840 --> 02:01:10.480]   Twitter was aiding and abetting all terrorists'
[02:01:10.480 --> 02:01:15.040]   right-handest activity as a result of publishing or allowing this as content to be published.
[02:01:15.040 --> 02:01:15.760]   Exactly.
[02:01:15.760 --> 02:01:20.240]   And Thomas was very clear that historically aiding and abetting
[02:01:20.240 --> 02:01:24.480]   did not go that far.
[02:01:24.480 --> 02:01:29.280]   He says, for example, assume that any assistance of any kind were sufficient to create liability.
[02:01:29.280 --> 02:01:34.000]   If that were the case, anyone who passively watched a robbery could be said to commit
[02:01:34.000 --> 02:01:38.480]   aiding and abetting by failing to call the police, yet our legal system generally does
[02:01:38.480 --> 02:01:42.320]   not impose liability for mere emissions in actions or non-feasance.
[02:01:42.320 --> 02:01:46.400]   For these reasons, of course, have long recognized the need to
[02:01:46.400 --> 02:01:52.080]   cabin aiding and abetting liability to cases of truly culpable conduct.
[02:01:52.080 --> 02:01:56.480]   And as a result, Twitter not truly culpable.
[02:01:56.480 --> 02:01:57.920]   It did not aid in a bet.
[02:01:57.920 --> 02:02:00.880]   And nine nothing that Twitter is not responsible.
[02:02:00.880 --> 02:02:01.840]   This completely--
[02:02:01.840 --> 02:02:03.280]   Twitter is safe for now.
[02:02:03.280 --> 02:02:07.440]   Well, it's safe only in the sense that they didn't say anything about 230.
[02:02:07.440 --> 02:02:13.040]   Although Mike Masnick makes a very good point in Techdirt saying Thomas actually gave
[02:02:13.040 --> 02:02:16.720]   is in his whole discussion of aiding and abetting a very good reason for why
[02:02:16.720 --> 02:02:18.880]   230 exists in the first place.
[02:02:18.880 --> 02:02:23.600]   So Kathy's opinion was the only benefit to 230 in this.
[02:02:23.600 --> 02:02:25.520]   It really does not impact 230.
[02:02:25.520 --> 02:02:31.440]   But the only benefit might be that now here's a precedent that could be used to support 230
[02:02:31.440 --> 02:02:32.800]   down the road.
[02:02:32.800 --> 02:02:37.120]   You see, we need this because we need to protect people in a case like this.
[02:02:37.440 --> 02:02:40.400]   You don't hold a platform liable for the speech of its users.
[02:02:40.400 --> 02:02:46.560]   So it was not a victory for 230, but it wasn't bad.
[02:02:46.560 --> 02:02:47.520]   It didn't hurt to--
[02:02:47.520 --> 02:02:48.480]   It was a reprieve.
[02:02:48.480 --> 02:02:49.440]   It was a reprieve.
[02:02:49.440 --> 02:02:51.040]   And Kathy said--
[02:02:51.040 --> 02:02:52.720]   Of course, she's been on the show many times.
[02:02:52.720 --> 02:02:54.080]   She's very smart.
[02:02:54.080 --> 02:02:57.840]   She wrote the amicus brief for this case.
[02:02:57.840 --> 02:03:05.200]   She said it is possible that what Thomas said, that could be used in further cases to protect 230.
[02:03:05.200 --> 02:03:06.800]   So there may be some value to that.
[02:03:07.600 --> 02:03:09.120]   I just wanted to give you a follow up on that.
[02:03:09.120 --> 02:03:13.520]   Do watch-- if you get a chance, do watch the Twit episode from last Sunday.
[02:03:13.520 --> 02:03:16.160]   It was a really good discussion.
[02:03:16.160 --> 02:03:22.720]   And we had Harry McCracken and Kathy Gellis in studio Amanda Silverling for Tech Crunch.
[02:03:22.720 --> 02:03:26.800]   But I really grilled Kathy on both all three of these cases, really.
[02:03:26.800 --> 02:03:28.160]   And then she could answer.
[02:03:28.160 --> 02:03:28.640]   She's great.
[02:03:28.640 --> 02:03:30.640]   She's really great.
[02:03:30.640 --> 02:03:33.040]   So one win, one loss for her.
[02:03:34.800 --> 02:03:36.880]   One and two in the Supreme Court.
[02:03:36.880 --> 02:03:41.440]   Should I go here?
[02:03:41.440 --> 02:03:46.560]   The AI writing novel writing tool that everyone hates is better than I expected.
[02:03:46.560 --> 02:03:48.560]   Rites Addie Robertson in the Verge.
[02:03:48.560 --> 02:03:52.960]   This is actually what you were talking about, Chris Messina.
[02:03:52.960 --> 02:03:56.240]   Because Addie says it helped--
[02:03:56.240 --> 02:03:59.040]   Wasn't that Addie had writer's block?
[02:03:59.040 --> 02:03:59.360]   I don't know.
[02:03:59.360 --> 02:04:00.640]   Is Addie a guy or a woman?
[02:04:00.640 --> 02:04:06.400]   I don't know. They have written 150,000 words of unpublished fiction last year alone.
[02:04:06.400 --> 02:04:08.640]   So writer's block wasn't the issue.
[02:04:08.640 --> 02:04:12.880]   But it was very helpful in generating a novella.
[02:04:12.880 --> 02:04:14.640]   The Electric Sea--
[02:04:14.640 --> 02:04:20.720]   Addie says the Electric Sea was produced with heavy human guidance,
[02:04:20.720 --> 02:04:24.320]   but every final line was created by hitting a generate button.
[02:04:24.320 --> 02:04:27.600]   The story is published on the Tumblr.
[02:04:27.600 --> 02:04:29.040]   I couldn't bring myself to read it.
[02:04:29.840 --> 02:04:34.000]   But Stephen Marchay also wrote one called "The Death of an Author,"
[02:04:34.000 --> 02:04:35.840]   which the Times wrote about this process.
[02:04:35.840 --> 02:04:36.800]   It's really interesting.
[02:04:36.800 --> 02:04:41.040]   So the generative fiction tool is called "Sudo Right."
[02:04:41.040 --> 02:04:41.680]   It's well-named.
[02:04:41.680 --> 02:04:42.560]   I like the name.
[02:04:42.560 --> 02:04:43.440]   SuD-O-M.
[02:04:43.440 --> 02:04:44.720]   "Sudo Right."
[02:04:44.720 --> 02:04:48.080]   It's a called "Story Engine."
[02:04:48.080 --> 02:04:54.880]   And it can write a full novel in a few days with a little help from a human.
[02:04:54.880 --> 02:04:57.840]   Have you played with this, anybody?
[02:04:59.120 --> 02:05:00.880]   I've written it in your ear.
[02:05:00.880 --> 02:05:06.720]   I've been playing with it, but I think that as is always the case with AI,
[02:05:06.720 --> 02:05:16.480]   the places where AI generated content is applicable, press releases, SEO, page descriptions.
[02:05:16.480 --> 02:05:18.320]   Media occur act writing, in other words.
[02:05:18.320 --> 02:05:19.360]   Yes, exactly.
[02:05:19.360 --> 02:05:20.880]   There's a lot--
[02:05:20.880 --> 02:05:24.240]   Most super popular fiction is garbage.
[02:05:24.240 --> 02:05:25.360]   And it's just--
[02:05:25.360 --> 02:05:27.120]   It's fantasy world stuff.
[02:05:28.320 --> 02:05:32.320]   Red recently about this Mormon author, who's nobody's heard of,
[02:05:32.320 --> 02:05:34.400]   who made $55 million last year.
[02:05:34.400 --> 02:05:38.720]   Just he's got this disorder where he just must write all the times.
[02:05:38.720 --> 02:05:39.200]   Oh, I--
[02:05:39.200 --> 02:05:40.720]   Yeah, we're talking about Brandon Sanderson.
[02:05:40.720 --> 02:05:41.520]   Stephen King?
[02:05:41.520 --> 02:05:42.560]   No, Brandon Sanderson.
[02:05:42.560 --> 02:05:43.680]   I'm not Stephen King, no, no.
[02:05:43.680 --> 02:05:44.720]   Stephen King is--
[02:05:44.720 --> 02:05:46.240]   No, but Brandon is amazing.
[02:05:46.240 --> 02:05:48.560]   And that article was such a hit piece on him.
[02:05:48.560 --> 02:05:51.200]   It was really kind of unfair.
[02:05:51.200 --> 02:05:54.080]   Nevertheless, he says he can't not write.
[02:05:54.080 --> 02:05:55.120]   He just spews it out.
[02:05:55.120 --> 02:05:56.960]   But you really realize he edits what he writes.
[02:05:58.160 --> 02:05:59.280]   I want that disease.
[02:05:59.280 --> 02:06:00.560]   But anyway, I think he--
[02:06:00.560 --> 02:06:03.840]   By the way, I tried to read some of his stuff
[02:06:03.840 --> 02:06:05.440]   because I thought, wow, people have--
[02:06:05.440 --> 02:06:06.240]   It's awful.
[02:06:06.240 --> 02:06:06.960]   It's terrible.
[02:06:06.960 --> 02:06:08.640]   You can't write.
[02:06:08.640 --> 02:06:09.120]   People--
[02:06:09.120 --> 02:06:09.920]   People--
[02:06:09.920 --> 02:06:10.480]   People--
[02:06:10.480 --> 02:06:12.000]   A lot of him as a world builder.
[02:06:12.000 --> 02:06:13.760]   And I'm very interested in world building fiction.
[02:06:13.760 --> 02:06:16.640]   But the writing is so god awful,
[02:06:16.640 --> 02:06:18.000]   I couldn't get past the first paragraph.
[02:06:18.000 --> 02:06:18.320]   Yeah.
[02:06:18.320 --> 02:06:21.600]   But so much romance fiction on--
[02:06:21.600 --> 02:06:23.520]   You can buy on Amazon so much,
[02:06:23.520 --> 02:06:26.800]   like of this just sort of fantasy world stuff,
[02:06:26.800 --> 02:06:28.080]   is just bad writing.
[02:06:28.080 --> 02:06:30.240]   And really, AI should be writing it.
[02:06:30.240 --> 02:06:31.040]   It's to--
[02:06:31.040 --> 02:06:31.760]   So here's--
[02:06:31.760 --> 02:06:34.160]   That requires the higher function of human being.
[02:06:34.160 --> 02:06:35.520]   I think this is actually kind of impressive.
[02:06:35.520 --> 02:06:40.400]   Here's the prompt Eddie gave this program.
[02:06:40.400 --> 02:06:42.960]   Introduced Jack hacking at his computer,
[02:06:42.960 --> 02:06:44.880]   focused on vivid descriptions of Jack's
[02:06:44.880 --> 02:06:48.240]   Squallen department and his moral conflict and citizen.
[02:06:48.240 --> 02:06:51.280]   Jack pulls off a corporate espionage job
[02:06:51.280 --> 02:06:52.960]   by navigating a virtual world.
[02:06:52.960 --> 02:06:54.960]   He sends the information to his client.
[02:06:56.000 --> 02:06:57.600]   So there's more--
[02:06:57.600 --> 02:06:59.520]   Jack's moral conflict as he began hacking
[02:06:59.520 --> 02:07:01.920]   to expose secrets in a corporate controlled society,
[02:07:01.920 --> 02:07:04.000]   but is ending up selling them to the highest bidder
[02:07:04.000 --> 02:07:06.560]   in order to finance his continued life as a hacker.
[02:07:06.560 --> 02:07:08.080]   Use made up brand names.
[02:07:08.080 --> 02:07:10.240]   Fictional corporate names use detailed,
[02:07:10.240 --> 02:07:12.560]   literary descriptions of futuristic hacking.
[02:07:12.560 --> 02:07:14.240]   Focus on emotion and character traits.
[02:07:14.240 --> 02:07:15.040]   That's a nice prompt.
[02:07:15.040 --> 02:07:16.960]   This is--
[02:07:16.960 --> 02:07:20.000]   This is the first paragraph generated for that.
[02:07:20.000 --> 02:07:23.680]   Jack sat hunched over his computer fingers,
[02:07:23.680 --> 02:07:27.280]   flying across the grimy keys like deranged spiders.
[02:07:27.280 --> 02:07:28.080]   I like that.
[02:07:28.080 --> 02:07:28.800]   I like that.
[02:07:28.800 --> 02:07:29.360]   It's cool.
[02:07:29.360 --> 02:07:30.240]   Yeah, that's good.
[02:07:30.240 --> 02:07:33.840]   The glow of the screen cast a sickly pallor--
[02:07:33.840 --> 02:07:34.800]   cliche--
[02:07:34.800 --> 02:07:37.360]   on his gaunt face, double cliche,
[02:07:37.360 --> 02:07:39.200]   highlighting the shadows between his eyes.
[02:07:39.200 --> 02:07:40.640]   You've got a hat trick.
[02:07:40.640 --> 02:07:43.760]   His apartment was a squalid den of decay,
[02:07:43.760 --> 02:07:47.520]   a mausoleum to forgotten dreams and lost hopes,
[02:07:47.520 --> 02:07:49.760]   tangled wires and discarded cliches.
[02:07:49.760 --> 02:07:51.200]   Cliches.
[02:07:51.200 --> 02:07:54.320]   Tangled wires and discarded tech littered the floor
[02:07:54.320 --> 02:07:57.680]   while mold crept up the wall like black veins.
[02:07:57.680 --> 02:07:58.320]   That's not bad.
[02:07:58.320 --> 02:07:58.800]   I know.
[02:07:58.800 --> 02:07:59.120]   That's--
[02:07:59.120 --> 02:08:03.120]   But yeah, quite a few little cliche phrases in there.
[02:08:03.120 --> 02:08:03.520]   Yeah.
[02:08:03.520 --> 02:08:04.080]   Come on.
[02:08:04.080 --> 02:08:04.640]   And I wish--
[02:08:04.640 --> 02:08:05.120]   And I wish--
[02:08:05.120 --> 02:08:05.600]   Go ahead.
[02:08:05.600 --> 02:08:07.680]   Use chat GBT to write his books.
[02:08:07.680 --> 02:08:11.520]   His most recent book in 2018 called Bob Honey,
[02:08:11.520 --> 02:08:12.800]   who just does stuff,
[02:08:12.800 --> 02:08:15.520]   was called by the Guardian, "Repellent and Stupid."
[02:08:15.520 --> 02:08:20.400]   And the New York Times said that it called it a book-shaped object.
[02:08:20.400 --> 02:08:21.280]   [LAUGHTER]
[02:08:21.280 --> 02:08:22.400]   He should be using--
[02:08:22.400 --> 02:08:23.200]   [LAUGHTER]
[02:08:23.200 --> 02:08:24.720]   Who's this Nicholas Cage?
[02:08:24.720 --> 02:08:26.240]   [LAUGHTER]
[02:08:26.240 --> 02:08:26.960]   Sean Penn.
[02:08:26.960 --> 02:08:28.320]   Oh, Sean Penn.
[02:08:28.320 --> 02:08:29.280]   I knew it was a bad actor.
[02:08:29.280 --> 02:08:29.920]   Awful, right.
[02:08:29.920 --> 02:08:30.480]   OK.
[02:08:30.480 --> 02:08:32.800]   Sean Penn writing novels.
[02:08:32.800 --> 02:08:33.760]   [LAUGHTER]
[02:08:33.760 --> 02:08:34.560]   I didn't know that.
[02:08:34.560 --> 02:08:35.760]   [INTERPOSING VOICES]
[02:08:35.760 --> 02:08:37.280]   Yo, he's just terrible.
[02:08:37.280 --> 02:08:37.280]   He's just--
[02:08:37.280 --> 02:08:37.600]   He's just terrible, right?
[02:08:37.600 --> 02:08:38.640]   --puts shaped--
[02:08:38.640 --> 02:08:39.040]   [LAUGHTER]
[02:08:39.040 --> 02:08:40.480]   --book shaped objects.
[02:08:40.480 --> 02:08:41.200]   That's beautiful.
[02:08:41.200 --> 02:08:41.760]   That's beautiful.
[02:08:41.760 --> 02:08:42.800]   Oh, god, I wish--
[02:08:42.800 --> 02:08:44.000]   Well, I wish I'd thought of that.
[02:08:44.000 --> 02:08:45.760]   [LAUGHTER]
[02:08:45.760 --> 02:08:49.920]   "Repellent and Stupid on so many levels," says the Guardian.
[02:08:49.920 --> 02:08:52.720]   [LAUGHTER]
[02:08:52.720 --> 02:08:53.520]   By his face.
[02:08:53.520 --> 02:08:54.880]   [LAUGHTER]
[02:08:54.880 --> 02:08:55.840]   To wear the Brits.
[02:08:55.840 --> 02:08:57.200]   Yeah, to wear the Brits.
[02:08:57.200 --> 02:08:59.120]   [LAUGHTER]
[02:08:59.120 --> 02:09:00.880]   Now I want to read it.
[02:09:00.880 --> 02:09:05.680]   I mean, there is such a thing as so bad that it's fun.
[02:09:05.680 --> 02:09:08.160]   Who does the audiobook?
[02:09:08.160 --> 02:09:09.120]   He reads it.
[02:09:09.120 --> 02:09:10.240]   That's a winner.
[02:09:10.240 --> 02:09:13.040]   [LAUGHTER]
[02:09:13.040 --> 02:09:15.440]   Should I play a little bit from Audible of this book?
[02:09:15.440 --> 02:09:16.200]   Oh, please do.
[02:09:16.200 --> 02:09:16.480]   Oh, sure.
[02:09:16.480 --> 02:09:16.880]   Yeah.
[02:09:16.880 --> 02:09:17.920]   Yeah.
[02:09:17.920 --> 02:09:20.160]   Maybe we'll stumble upon a good--
[02:09:20.160 --> 02:09:22.400]   It will be as good as an Elon Musk Twitter space.
[02:09:22.400 --> 02:09:24.880]   [LAUGHTER]
[02:09:24.880 --> 02:09:26.320]   OK, Bob Honey.
[02:09:26.320 --> 02:09:29.920]   Shouldn't be too hard to find a book named Bob Honey, who
[02:09:29.920 --> 02:09:31.920]   doesn't do--
[02:09:31.920 --> 02:09:32.920]   Oh.
[02:09:32.920 --> 02:09:33.920]   --who just do stuff.
[02:09:33.920 --> 02:09:34.720]   --who do stuff.
[02:09:34.720 --> 02:09:36.960]   It's not un-audible.
[02:09:36.960 --> 02:09:38.920]   But maybe the sequel, Bob Honey
[02:09:38.920 --> 02:09:41.280]   sings Jimmy Crack Corn is.
[02:09:41.280 --> 02:09:43.840]   [LAUGHTER]
[02:09:43.840 --> 02:09:47.760]   This was an earlier attempt by Sean Penn.
[02:09:47.760 --> 02:09:48.600]   Let's listen in.
[02:09:48.600 --> 02:09:50.040]   The search of the retirement home
[02:09:50.040 --> 02:09:52.760]   on the night of Spirley Coltea's demise
[02:09:52.760 --> 02:09:55.800]   found Bob's bed magnificently made,
[02:09:55.800 --> 02:09:58.880]   with tucks and folds that had bring a marine core drill
[02:09:58.880 --> 02:10:01.880]   instructor to drool and delight.
[02:10:01.880 --> 02:10:06.480]   Atop its trampoline type duvet, an envelope fat with cash
[02:10:06.480 --> 02:10:09.000]   addressed to the local ASPCA.
[02:10:09.000 --> 02:10:11.160]   Now I think an AI could have written that.
[02:10:11.160 --> 02:10:11.520]   Oh, yeah.
[02:10:11.520 --> 02:10:11.800]   Oh, yeah.
[02:10:11.800 --> 02:10:12.560]   Fat with cash.
[02:10:12.560 --> 02:10:13.360]   Yeah, I'll clear it.
[02:10:13.360 --> 02:10:14.480]   Yeah.
[02:10:14.480 --> 02:10:17.480]   Well, Chris, let me try some idea on you.
[02:10:17.480 --> 02:10:20.560]   The-- if I had a machine--
[02:10:20.560 --> 02:10:21.600]   I've said this in the show before,
[02:10:21.600 --> 02:10:27.680]   but if I had a machine that had tracked all of the relationships
[02:10:27.680 --> 02:10:30.000]   of these unbelievable little rewards come up
[02:10:30.000 --> 02:10:36.880]   with 3 trillion tokens, I'd want to query that database
[02:10:36.880 --> 02:10:40.200]   to see about our biases and connections.
[02:10:40.200 --> 02:10:45.480]   And the predictions-- it's not about creating more of the same.
[02:10:45.480 --> 02:10:47.120]   It's about what are we already produced
[02:10:47.120 --> 02:10:49.160]   and what does it tell us?
[02:10:49.160 --> 02:10:52.160]   Is there anything going on in that vein?
[02:10:52.160 --> 02:10:55.120]   Well, one of the things that I like to think about
[02:10:55.120 --> 02:10:58.840]   is just negative space as a sort of concept
[02:10:58.840 --> 02:11:00.480]   to allow you to sort of understand
[02:11:00.480 --> 02:11:03.120]   that what lives in the positive space-- and this
[02:11:03.120 --> 02:11:05.480]   is something that Alan Watson talked about with a grid of words.
[02:11:05.480 --> 02:11:10.480]   You imagine sort of graph paper with lots of dots.
[02:11:10.480 --> 02:11:12.520]   And it goes on for miles and miles.
[02:11:12.520 --> 02:11:15.040]   And each of those dots is a word.
[02:11:15.040 --> 02:11:16.000]   It's a concept.
[02:11:16.000 --> 02:11:17.280]   It's something that humans know.
[02:11:17.280 --> 02:11:20.560]   But all the negative space, all the non-dot area,
[02:11:20.560 --> 02:11:22.080]   also are filled with concepts.
[02:11:22.080 --> 02:11:24.200]   They just don't have labels that humans have--
[02:11:24.200 --> 02:11:26.120]   and you're not developed.
[02:11:26.120 --> 02:11:29.120]   So to what you're talking about, there probably
[02:11:29.120 --> 02:11:32.000]   is a lot of great wisdom found in these hallucinations
[02:11:32.000 --> 02:11:35.080]   that people are so afraid of because they're
[02:11:35.080 --> 02:11:39.400]   creating adjacencies that are unexpected, that are oblique.
[02:11:39.400 --> 02:11:40.440]   And I feel like--
[02:11:40.440 --> 02:11:41.520]   It's the books you should write.
[02:11:41.520 --> 02:11:42.120]   Yes.
[02:11:42.120 --> 02:11:44.360]   Precisely, because everything else has sort of been done.
[02:11:44.360 --> 02:11:46.920]   And it's following these cliches that you mentioned,
[02:11:46.920 --> 02:11:48.680]   that one time we're not cliches.
[02:11:48.680 --> 02:11:51.880]   Like trending topics and hashtags on Twitter, at one point,
[02:11:51.880 --> 02:11:52.640]   did not exist.
[02:11:52.640 --> 02:11:55.200]   There weren't these words smushed together
[02:11:55.200 --> 02:11:57.640]   into a new concept that is identifying something
[02:11:57.640 --> 02:11:58.880]   in human experience.
[02:11:58.880 --> 02:12:00.760]   And so that aspect of generativity,
[02:12:00.760 --> 02:12:03.200]   I think, absolutely could be something that could be developed.
[02:12:03.200 --> 02:12:05.760]   I haven't seen anything specifically along those lines.
[02:12:05.760 --> 02:12:09.000]   However, I am starting to see people kind of mash together
[02:12:09.000 --> 02:12:11.720]   either different prompts or to--
[02:12:11.720 --> 02:12:14.920]   even Adobe with what they're doing with Firefly,
[02:12:14.920 --> 02:12:16.160]   has started to put out products that
[02:12:16.160 --> 02:12:19.400]   will allow you to do different types of in-painting
[02:12:19.400 --> 02:12:20.640]   or scene changing.
[02:12:20.640 --> 02:12:21.160]   Yes.
[02:12:21.160 --> 02:12:22.760]   For example, there's this object.
[02:12:22.760 --> 02:12:25.360]   I want to replace it with some other object in the space.
[02:12:25.360 --> 02:12:30.560]   Adobe just released a Photoshop brush to do exactly that,
[02:12:30.560 --> 02:12:30.760]   basically.
[02:12:30.760 --> 02:12:31.360]   Exactly.
[02:12:31.360 --> 02:12:32.960]   Have you seen Drag Your GaN?
[02:12:32.960 --> 02:12:33.640]   Firefly.
[02:12:33.640 --> 02:12:34.760]   Oh, it's so good.
[02:12:34.760 --> 02:12:36.080]   What's Drag Your GaN?
[02:12:36.080 --> 02:12:38.880]   Go to line 61, Leo.
[02:12:38.880 --> 02:12:42.280]   But before we do that, I'm about a little palate cleanser,
[02:12:42.280 --> 02:12:46.760]   some Sean Penn, reading from Bob Honey, who just do stuff.
[02:12:46.760 --> 02:12:50.000]   The America Bob Drove from Woodview to New Orleans
[02:12:50.000 --> 02:12:52.080]   was not that one of lore.
[02:12:52.080 --> 02:12:54.720]   Not the one he'd known in his earliest cross-continental
[02:12:54.720 --> 02:12:56.680]   automotive excursions.
[02:12:56.680 --> 02:12:59.920]   Not the one of pay-later gas stations, vintage diners,
[02:12:59.920 --> 02:13:01.800]   and two laying roads.
[02:13:01.800 --> 02:13:05.320]   Only trains remained a spirit refuge to modernity's
[02:13:05.320 --> 02:13:06.520]   craven trumpery.
[02:13:06.520 --> 02:13:07.020]   Whoa.
[02:13:07.020 --> 02:13:07.840]   Oh, my.
[02:13:07.840 --> 02:13:08.840]   Wait a minute.
[02:13:08.840 --> 02:13:09.840]   Whoa.
[02:13:09.840 --> 02:13:10.840]   Whoa.
[02:13:10.840 --> 02:13:11.840]   Oh.
[02:13:11.840 --> 02:13:13.120]   I think that's a sentence no one's ever written before.
[02:13:13.120 --> 02:13:14.120]   I mean, I don't think a chat--
[02:13:14.120 --> 02:13:15.120]   You're a few, really.
[02:13:15.120 --> 02:13:16.120]   No, no, no.
[02:13:16.120 --> 02:13:17.360]   It's not a cliché.
[02:13:17.360 --> 02:13:18.360]   We could say that much.
[02:13:18.360 --> 02:13:20.160]   No, it's not.
[02:13:20.160 --> 02:13:22.000]   What line 79, you said?
[02:13:22.000 --> 02:13:23.000]   61.
[02:13:23.000 --> 02:13:24.000]   61.
[02:13:24.000 --> 02:13:25.000]   Let's go to line 61.
[02:13:25.000 --> 02:13:26.000]   It's pretty amazing.
[02:13:26.000 --> 02:13:27.000]   Oh, Farhad.
[02:13:27.000 --> 02:13:29.240]   You ain't seen nothing yet.
[02:13:29.240 --> 02:13:32.200]   OK, this is Drag Your GaN.
[02:13:32.200 --> 02:13:34.280]   Farhad was worried about photoshopping.
[02:13:34.280 --> 02:13:35.480]   Now it's going to be so easy.
[02:13:35.480 --> 02:13:36.480]   Everyone's going to fake everything.
[02:13:36.480 --> 02:13:37.480]   We want to know what's real.
[02:13:37.480 --> 02:13:43.800]   Interactive point-based manipulation on the generative image manifold.
[02:13:43.800 --> 02:13:45.600]   This is from the Max Planck Institute.
[02:13:45.600 --> 02:13:48.960]   Ooh, it's like it comes up with the subjects for these things.
[02:13:48.960 --> 02:13:50.200]   They're completely inscrutable.
[02:13:50.200 --> 02:13:54.080]   This is a presentation at SIGGRAPH this year.
[02:13:54.080 --> 02:13:55.080]   So what am I--
[02:13:55.080 --> 02:13:56.080]   Go down.
[02:13:56.080 --> 02:13:57.080]   There's videos.
[02:13:57.080 --> 02:13:58.080]   Oh, there's video.
[02:13:58.080 --> 02:13:59.080]   Yeah, I watch this one video.
[02:13:59.080 --> 02:14:00.080]   Main demo.
[02:14:00.080 --> 02:14:01.080]   It's wild.
[02:14:01.080 --> 02:14:02.080]   Accelerated.
[02:14:02.080 --> 02:14:03.640]   I don't know if there's audio, but I'll play it.
[02:14:03.640 --> 02:14:04.640]   OK, let's see.
[02:14:04.640 --> 02:14:06.600]   Here we go.
[02:14:06.600 --> 02:14:09.880]   OK, they're taking a dog and they're stretching it.
[02:14:09.880 --> 02:14:11.040]   The mouth is amazing.
[02:14:11.040 --> 02:14:13.640]   They could just change things and make things smile.
[02:14:13.640 --> 02:14:14.880]   Well, it presumes the dog's intent.
[02:14:14.880 --> 02:14:16.720]   It opens the mouth and knows where the teeth are.
[02:14:16.720 --> 02:14:17.720]   Right, yeah.
[02:14:17.720 --> 02:14:21.120]   So it basically takes a two-dimensional image and then you find a point in that image of
[02:14:21.120 --> 02:14:25.480]   some significant-- you know, sort of like what is it, key points or in video?
[02:14:25.480 --> 02:14:26.480]   It keeps frames.
[02:14:26.480 --> 02:14:27.480]   Yeah.
[02:14:27.480 --> 02:14:28.480]   Yeah.
[02:14:28.480 --> 02:14:29.480]   Right?
[02:14:29.480 --> 02:14:33.160]   So you're doing more or less the same thing, but with a two-dimensional image and what
[02:14:33.160 --> 02:14:35.200]   you would use to do is sort of like stretch.
[02:14:35.200 --> 02:14:38.320]   I remember I'm going to date myself like Kai's Power goo or whatever.
[02:14:38.320 --> 02:14:39.320]   Kai's Power goo.
[02:14:39.320 --> 02:14:40.320]   Yeah.
[02:14:40.320 --> 02:14:41.320]   Right?
[02:14:41.320 --> 02:14:42.320]   Except that didn't--
[02:14:42.320 --> 02:14:43.320]   That just smeared stuff.
[02:14:43.320 --> 02:14:44.320]   Yeah, it was like finger pings.
[02:14:44.320 --> 02:14:45.320]   That's what I'm saying.
[02:14:45.320 --> 02:14:49.760]   So now, you know, 20, 30 years forward, now instead of just smearing it and dragging the
[02:14:49.760 --> 02:14:55.520]   pixels, now you actually change the content of the image based on its understanding of
[02:14:55.520 --> 02:14:56.880]   the subject of the image.
[02:14:56.880 --> 02:15:01.480]   In that example, just now, there was a picture of a woman with her mouth closed and it made
[02:15:01.480 --> 02:15:03.840]   her a smile and it produced teeth for her.
[02:15:03.840 --> 02:15:04.840]   It's probably not her teeth.
[02:15:04.840 --> 02:15:07.560]   It doesn't know what her teeth look like.
[02:15:07.560 --> 02:15:09.040]   It's some gan teeth.
[02:15:09.040 --> 02:15:12.840]   Now, the question is, could this be copyrightable and what are you even copywriting?
[02:15:12.840 --> 02:15:13.840]   Right.
[02:15:13.840 --> 02:15:14.840]   Exactly.
[02:15:14.840 --> 02:15:15.840]   Right?
[02:15:15.840 --> 02:15:18.840]   Like this is where that Supreme Court case is sort of like-- like you do this in Warhol
[02:15:18.840 --> 02:15:21.400]   and sort of go between 15 different iterations.
[02:15:21.400 --> 02:15:24.920]   And none of it's even worth copywriting because it's sort of already in the model.
[02:15:24.920 --> 02:15:26.680]   Imagine Warhol with this tool.
[02:15:26.680 --> 02:15:29.080]   Well, you can do it in Jackie O'Nassus.
[02:15:29.080 --> 02:15:32.960]   The Warhol version of the Prince photo was many versions.
[02:15:32.960 --> 02:15:33.960]   That's true.
[02:15:33.960 --> 02:15:34.960]   That's true.
[02:15:34.960 --> 02:15:35.960]   That's right.
[02:15:35.960 --> 02:15:36.960]   Mm-hmm.
[02:15:36.960 --> 02:15:37.960]   Okay.
[02:15:37.960 --> 02:15:42.760]   So is drag in-- I mean, obviously it's in the lab.
[02:15:42.760 --> 02:15:44.520]   Will they release this at any point?
[02:15:44.520 --> 02:15:46.120]   That's pretty cool.
[02:15:46.120 --> 02:15:48.960]   And if the paper's out there, these techniques become--
[02:15:48.960 --> 02:15:49.960]   No, be able to become--
[02:15:49.960 --> 02:15:50.960]   Yeah.
[02:15:50.960 --> 02:15:51.960]   Yeah.
[02:15:51.960 --> 02:15:58.600]   Two main components featuring one, a feature based motion supervision that drives the handle
[02:15:58.600 --> 02:16:03.720]   point to move towards the target position and two, a new point tracking approach that
[02:16:03.720 --> 02:16:13.080]   leverages-- and this is the key-- the discriminative GAN features generative adversarial network
[02:16:13.080 --> 02:16:14.080]   to keep--
[02:16:14.080 --> 02:16:15.680]   That sounds like something about Killya.
[02:16:15.680 --> 02:16:18.080]   They keep localizing the position and handle points.
[02:16:18.080 --> 02:16:19.760]   That's the technology behind Deepfakes.
[02:16:19.760 --> 02:16:20.760]   Yeah.
[02:16:20.760 --> 02:16:21.760]   Sean Penn's next novel.
[02:16:21.760 --> 02:16:22.760]   Yeah.
[02:16:22.760 --> 02:16:23.760]   Yeah.
[02:16:23.760 --> 02:16:24.760]   Wow.
[02:16:24.760 --> 02:16:27.240]   So GAN is not the same as a large language model.
[02:16:27.240 --> 02:16:34.240]   These are two different ways to take data and create something-- some AI capability.
[02:16:34.240 --> 02:16:38.440]   But you could imagine-- stay with me for a minute, Leo.
[02:16:38.440 --> 02:16:44.600]   You could imagine a language model writing something, and then it provides sliders.
[02:16:44.600 --> 02:16:45.600]   Yeah.
[02:16:45.600 --> 02:16:46.600]   Yeah.
[02:16:46.600 --> 02:16:47.600]   Right.
[02:16:47.600 --> 02:16:48.600]   Just 10% less than that.
[02:16:48.600 --> 02:16:49.600]   Well, they were the 5% less than that.
[02:16:49.600 --> 02:16:54.180]   Microsoft talked about that actually at Build this week.
[02:16:54.180 --> 02:16:57.580]   They already have something sort of like that, so does Google Bard.
[02:16:57.580 --> 02:17:01.100]   Remember the Bard demonstration at Google I/O where you decided to take the complaint
[02:17:01.100 --> 02:17:02.660]   letter and make it nasty?
[02:17:02.660 --> 02:17:04.940]   Microsoft sort of has that now and being chat.
[02:17:04.940 --> 02:17:06.980]   You can have choose different styles.
[02:17:06.980 --> 02:17:11.220]   But they really, at the keynote, described the fact that it could be a slider, that
[02:17:11.220 --> 02:17:15.260]   you could have an infinite scale, and you could turn it up and down until you got something
[02:17:15.260 --> 02:17:16.260]   you liked.
[02:17:16.260 --> 02:17:17.260]   That's probably just more--
[02:17:17.260 --> 02:17:19.220]   See, the thing that's really interesting about--
[02:17:19.220 --> 02:17:20.220]   It's expensive.
[02:17:20.220 --> 02:17:21.220]   This conversation too, right?
[02:17:21.220 --> 02:17:24.660]   Just to think about it from the flip side of the reverse.
[02:17:24.660 --> 02:17:27.620]   Like we're sort of talking about this from the authorship perspective, someone who's
[02:17:27.620 --> 02:17:28.940]   writing something.
[02:17:28.940 --> 02:17:36.380]   But actually, especially with Facebook or Meta released something this week with their
[02:17:36.380 --> 02:17:37.380]   translation service.
[02:17:37.380 --> 02:17:39.180]   A thousand languages.
[02:17:39.180 --> 02:17:40.180]   Yeah.
[02:17:40.180 --> 02:17:41.180]   Unbelievable.
[02:17:41.180 --> 02:17:49.020]   And many languages that actually don't have a lot of recorded examples to train on.
[02:17:49.020 --> 02:17:51.700]   Now imagine this from the receiver's end.
[02:17:51.700 --> 02:17:54.620]   I think this is one of the things that's, again, hard.
[02:17:54.620 --> 02:17:59.740]   We kind of exist-- again, those of us on this podcast are in the positive space.
[02:17:59.740 --> 02:18:02.220]   We see kind of interactions that we use computers in certain ways.
[02:18:02.220 --> 02:18:05.980]   But there's a lot of people who don't have access to computers, who don't speak the language
[02:18:05.980 --> 02:18:09.900]   that computers are written in or the interface is written in.
[02:18:09.900 --> 02:18:15.700]   What some of this allows software to do is to adapt to the user and to the user's needs
[02:18:15.700 --> 02:18:18.380]   or preferences in a way that previously used to be very, very difficult.
[02:18:18.380 --> 02:18:22.700]   Now, one of the things that you guys had talked about earlier was about how Microsoft and
[02:18:22.700 --> 02:18:26.340]   maybe Windows, Windows 12 or something is going to have kind of AI built in and it's
[02:18:26.340 --> 02:18:27.340]   going to be part of the experience.
[02:18:27.340 --> 02:18:32.820]   But one of the things that I think is actually going to hold back Windows in particular compared
[02:18:32.820 --> 02:18:36.140]   to the Mac and especially, I believe, on Linux, although I don't know this-- I'm
[02:18:36.140 --> 02:18:40.380]   really ignorant about this on Linux, is the degree to which Apple has invested in
[02:18:40.380 --> 02:18:42.500]   assistive technologies over the years.
[02:18:42.500 --> 02:18:49.460]   In other words, you can turn on voice controls where the series essentially will tell you
[02:18:49.460 --> 02:18:52.140]   how you're interacting with a computer.
[02:18:52.140 --> 02:18:55.940]   There are ways where you can use assistive touch and things like that, both on your
[02:18:55.940 --> 02:18:57.660]   iPhone or your Mac.
[02:18:57.660 --> 02:19:01.780]   All of those things that allows computers to become more accessible to more people are
[02:19:01.780 --> 02:19:08.020]   the very same tools and scaffolding and structures that AI will be able to use to then manipulate
[02:19:08.020 --> 02:19:09.020]   your environment.
[02:19:09.020 --> 02:19:14.540]   For example, there's a product called rewind.ai and what they do is they record everything
[02:19:14.540 --> 02:19:16.140]   that goes on your screen.
[02:19:16.140 --> 02:19:20.980]   And the idea is to augment your memory by seeing everything that you have seen with your
[02:19:20.980 --> 02:19:25.060]   eyes and capturing video recordings of those things.
[02:19:25.060 --> 02:19:30.220]   Now if they're also able to inventory and index all of the interfaces and buttons that
[02:19:30.220 --> 02:19:33.700]   appear on the screen, then they can see the actions that you could have or would have
[02:19:33.700 --> 02:19:38.220]   taken and then the AI can learn to take the actions that you might have taken.
[02:19:38.220 --> 02:19:41.340]   So there's a lot of this that exists in browser automation and tools like that.
[02:19:41.340 --> 02:19:46.060]   But if you're able to just use common language to tell the computer what you want it to do
[02:19:46.060 --> 02:19:50.700]   and it observes you doing it for one or two times and then can emulate what you've done.
[02:19:50.700 --> 02:19:56.020]   Now you're opening up again accessibility to a whole new range of applications and use
[02:19:56.020 --> 02:19:57.020]   cases.
[02:19:57.020 --> 02:20:01.140]   So I don't know again how much Microsoft has invested in accessibility of its platform.
[02:20:01.140 --> 02:20:02.620]   I don't know how much Linux has either.
[02:20:02.620 --> 02:20:08.020]   But I know this is something that Apple has taken pride in and has done a lot of the instrumentation
[02:20:08.020 --> 02:20:09.020]   for years.
[02:20:09.020 --> 02:20:12.180]   Microsoft is arguably a head of Apple in accessibility.
[02:20:12.180 --> 02:20:15.140]   They've really put a lot of energy and effort into it.
[02:20:15.140 --> 02:20:17.700]   I don't know if it will benefit them in the same way because I don't know if it's with
[02:20:17.700 --> 02:20:19.700]   the same kind of point of view.
[02:20:19.700 --> 02:20:26.140]   But they have, you know, I've met their accessibility ombudsman and many of their ads feature accessibility.
[02:20:26.140 --> 02:20:32.420]   They made a place controller for their Xbox for people with mobility issues, things like
[02:20:32.420 --> 02:20:33.420]   that.
[02:20:33.420 --> 02:20:34.420]   They're very good at that.
[02:20:34.420 --> 02:20:35.420]   That's amazing.
[02:20:35.420 --> 02:20:36.420]   Yeah.
[02:20:36.420 --> 02:20:37.420]   Yeah.
[02:20:37.420 --> 02:20:38.420]   Yeah.
[02:20:38.420 --> 02:20:39.420]   Yeah.
[02:20:39.420 --> 02:20:41.980]   Meta did something interesting to make this, I thought.
[02:20:41.980 --> 02:20:47.780]   It turns out that there are many recordings of the Bible.
[02:20:47.780 --> 02:20:53.340]   So they trained on two new datasets, one that contains audio recordings of the New Testament
[02:20:53.340 --> 02:20:59.700]   and then the corresponding text in 1,107 languages.
[02:20:59.700 --> 02:21:03.900]   Another contained unlabeled New Testament audio recordings of 3,809 languages.
[02:21:03.900 --> 02:21:08.180]   They then matched it up because that's one of the ways you train is match the text to
[02:21:08.180 --> 02:21:10.780]   the speech.
[02:21:10.780 --> 02:21:17.460]   And they were able to create a model that can not only speak 1,000 languages but recognize
[02:21:17.460 --> 02:21:18.540]   more than 4,000.
[02:21:18.540 --> 02:21:23.220]   There are only, I should point out, only 7,000 languages, it's estimated in the world.
[02:21:23.220 --> 02:21:28.300]   So that's a pretty good subset of the world's languages.
[02:21:28.300 --> 02:21:29.300]   Pretty impressive.
[02:21:29.300 --> 02:21:30.300]   That's remarkable.
[02:21:30.300 --> 02:21:31.300]   Yeah.
[02:21:31.300 --> 02:21:32.300]   Let me check.
[02:21:32.300 --> 02:21:34.300]   I'm talking.
[02:21:34.300 --> 02:21:36.900]   Oh.
[02:21:36.900 --> 02:21:38.380]   You want to hear something crazy, Leo?
[02:21:38.380 --> 02:21:39.380]   Yeah.
[02:21:39.380 --> 02:21:46.700]   So there are approximately 1,000 languages, probably around 100 of them are in Oaxaca alone.
[02:21:46.700 --> 02:21:47.700]   I do know that.
[02:21:47.700 --> 02:21:49.220]   You mentioned that when we were there.
[02:21:49.220 --> 02:21:50.220]   Yeah.
[02:21:50.220 --> 02:21:56.380]   There are a lot of native languages and the native languages have varied over time quite
[02:21:56.380 --> 02:22:00.340]   a bit, I guess because they lived in separated enclaves.
[02:22:00.340 --> 02:22:06.460]   They're 65 or so recognized Zapotec languages just among the Zapotec.
[02:22:06.460 --> 02:22:07.460]   Wow.
[02:22:07.460 --> 02:22:08.460]   Yeah.
[02:22:08.460 --> 02:22:10.740]   Are those like dialects or those actual languages?
[02:22:10.740 --> 02:22:11.740]   Actual languages.
[02:22:11.740 --> 02:22:13.940]   So Zapotec is a language family.
[02:22:13.940 --> 02:22:19.580]   And so there are many types of Zapotec that are totally unintelligible to other Zapotec
[02:22:19.580 --> 02:22:20.580]   speakers.
[02:22:20.580 --> 02:22:21.580]   Wow.
[02:22:21.580 --> 02:22:22.580]   Wow.
[02:22:22.580 --> 02:22:28.180]   Oh, as Alberto Echo once said, a dialect is a language without an army and navy.
[02:22:28.180 --> 02:22:29.180]   Yeah.
[02:22:29.180 --> 02:22:30.180]   That's good.
[02:22:30.180 --> 02:22:31.180]   I like that.
[02:22:31.180 --> 02:22:32.180]   You mentioned Adobe.
[02:22:32.180 --> 02:22:36.500]   We were talking about their generator, their AI image generator, Firefly.
[02:22:36.500 --> 02:22:41.020]   It is now or is going to be in Photoshop in the second half of 2023.
[02:22:41.020 --> 02:22:43.540]   And it will do some pretty amazing things.
[02:22:43.540 --> 02:22:47.820]   Here are some of the examples that Adobe offers for what it can do.
[02:22:47.820 --> 02:22:53.780]   I mean, this isn't a GAN stretching, but it's getting there, adding a car, a car shaped
[02:22:53.780 --> 02:22:56.820]   cloud and a reflective puddle of water.
[02:22:56.820 --> 02:23:00.140]   I literally use this, I believe yesterday.
[02:23:00.140 --> 02:23:05.220]   So some of the work that I do currently is helping makers launch on productant.
[02:23:05.220 --> 02:23:11.420]   And that occasionally involves getting into keynote or some other slide tool to basically
[02:23:11.420 --> 02:23:13.660]   produce their gallery.
[02:23:13.660 --> 02:23:18.500]   And one of the makers that I was working with needed an image depicting essentially
[02:23:18.500 --> 02:23:20.700]   a phone being dropped or lost.
[02:23:20.700 --> 02:23:27.380]   And so I went into, I first went to Unsplash where fortunately the images are public domain.
[02:23:27.380 --> 02:23:28.380]   I grabbed that.
[02:23:28.380 --> 02:23:32.660]   I uploaded it to Firefly and I was able to replace a beer can, which for some reason,
[02:23:32.660 --> 02:23:38.140]   dropping in water with an iPhone by selecting the beer can and saying awesome.
[02:23:38.140 --> 02:23:40.660]   And it worked and it literally took like 30 seconds.
[02:23:40.660 --> 02:23:41.660]   So it's amazing.
[02:23:41.660 --> 02:23:43.340]   It's real and it's super helpful.
[02:23:43.340 --> 02:23:45.420]   Yeah, incredible.
[02:23:45.420 --> 02:23:50.900]   When I read this this morning, I went to see if my Adobe photography subscription, which
[02:23:50.900 --> 02:23:55.780]   I thought included for $10 a month Lightroom and Photoshop still included Photoshop.
[02:23:55.780 --> 02:23:56.780]   It didn't.
[02:23:56.780 --> 02:23:58.100]   It took Photoshop out.
[02:23:58.100 --> 02:24:00.180]   So I can solve it.
[02:24:00.180 --> 02:24:02.380]   Yeah, this is pretty impressive.
[02:24:02.380 --> 02:24:07.180]   Firefly, I can't remember where fireflies models come from is.
[02:24:07.180 --> 02:24:09.180]   I don't know who it is.
[02:24:09.180 --> 02:24:10.980]   It's a great question.
[02:24:10.980 --> 02:24:15.180]   When it's, I would think that Adobe would want to run and produce their own.
[02:24:15.180 --> 02:24:17.460]   No, I think they acquired something.
[02:24:17.460 --> 02:24:21.540]   No, I don't know.
[02:24:21.540 --> 02:24:25.940]   We should talk about the Pentagon explosion.
[02:24:25.940 --> 02:24:26.940]   Speaking of Photoshop.
[02:24:26.940 --> 02:24:28.940]   It's and Twitter, right?
[02:24:28.940 --> 02:24:29.940]   Yeah, it's true.
[02:24:29.940 --> 02:24:30.940]   Right.
[02:24:30.940 --> 02:24:31.940]   And blue, blue checks.
[02:24:31.940 --> 02:24:32.940]   Yeah, this is all of it.
[02:24:32.940 --> 02:24:33.940]   We're bringing it all home.
[02:24:33.940 --> 02:24:34.940]   It is all coming home.
[02:24:34.940 --> 02:24:37.900]   The final story, the law bringing it all home.
[02:24:37.900 --> 02:24:41.780]   So the issue was this is a fake photo.
[02:24:41.780 --> 02:24:44.620]   It's not clear whether it was generated by AI or just Photoshop.
[02:24:44.620 --> 02:24:46.180]   It doesn't really matter.
[02:24:46.180 --> 02:24:51.220]   Although most of the news stories I saw a blamed AI for this.
[02:24:51.220 --> 02:24:55.740]   And really it's a terrible, a fake devil of now, the tech folk devil.
[02:24:55.740 --> 02:25:01.740]   The stock market, when this was tweeted by a phony blue checked account called Bloomberg
[02:25:01.740 --> 02:25:05.540]   Feed, not Bloomberg.
[02:25:05.540 --> 02:25:07.060]   The stock market tumbled.
[02:25:07.060 --> 02:25:11.220]   It recovered pretty quickly, I might add.
[02:25:11.220 --> 02:25:15.300]   In every case where it was tweeted or retweeted, it was credited Twitter sources.
[02:25:15.300 --> 02:25:19.140]   Which you're just feeling like a people emoji.
[02:25:19.140 --> 02:25:20.820]   It's got to be true.
[02:25:20.820 --> 02:25:24.300]   Within an hour of it being circulated, government officials stepped in to clarify the tweets
[02:25:24.300 --> 02:25:26.260]   were not true.
[02:25:26.260 --> 02:25:30.220]   The Arlington fire and EMS tweeted, there is no explosion or incident taking place at
[02:25:30.220 --> 02:25:32.700]   or near the Pentagon.
[02:25:32.700 --> 02:25:33.700]   So don't believe it.
[02:25:33.700 --> 02:25:36.740]   Blue check accounts that said that because, I mean, can you really trust them?
[02:25:36.740 --> 02:25:37.740]   If it doesn't.
[02:25:37.740 --> 02:25:44.300]   Dozens of verified accounts on Twitter with large followings spread this photograph.
[02:25:44.300 --> 02:25:48.980]   But the most of blame here, it was, but it was in what I first saw about it.
[02:25:48.980 --> 02:25:52.460]   The story I saw was fake photo, but it was known that it was fake.
[02:25:52.460 --> 02:25:53.460]   It's true.
[02:25:53.460 --> 02:25:56.380]   Belling cat, Belling cat.
[02:25:56.380 --> 02:25:58.380]   Thank God for Belling cat.
[02:25:58.380 --> 02:26:04.020]   Nick Waters tweeted, there are a few signs of making an AI image, including the fence
[02:26:04.020 --> 02:26:06.740]   melds into the crowd crowd barriers.
[02:26:06.740 --> 02:26:09.900]   Yeah, this is kind of AI looking here.
[02:26:09.900 --> 02:26:11.540]   And lost hates Belling cat.
[02:26:11.540 --> 02:26:13.020]   So we'll never go that on Twitter.
[02:26:13.020 --> 02:26:15.540]   Yeah, that's right.
[02:26:15.540 --> 02:26:18.020]   So maybe it is maybe it is.
[02:26:18.020 --> 02:26:20.900]   I mean, what does it matter if it's a Photoshop or AI generated?
[02:26:20.900 --> 02:26:21.900]   You certainly could do it with photos.
[02:26:21.900 --> 02:26:22.900]   It doesn't matter.
[02:26:22.900 --> 02:26:26.900]   It'll be a question of hearings if it's AI generated.
[02:26:26.900 --> 02:26:28.420]   Yeah, right.
[02:26:28.420 --> 02:26:29.420]   That's a difference.
[02:26:29.420 --> 02:26:35.820]   There's another story that the U-Haul that crashed into a gate near the White House.
[02:26:35.820 --> 02:26:43.980]   There were a bunch of blue checked Twitter blue accounts that said that it was a FBI
[02:26:43.980 --> 02:26:46.260]   scyop spreading that sort of.
[02:26:46.260 --> 02:26:57.140]   So Twitter, so quote unquote verified accounts on Twitter are becoming a major source of disinformation.
[02:26:57.140 --> 02:27:02.220]   And so we have this reflex to sort of give credence to anything with a blue check, even
[02:27:02.220 --> 02:27:05.900]   though we know that nowadays you just pay for it.
[02:27:05.900 --> 02:27:08.180]   So this is one of the things that we've learned.
[02:27:08.180 --> 02:27:09.820]   But look at the headline in the New York Times.
[02:27:09.820 --> 02:27:11.940]   Now this is the headline.
[02:27:11.940 --> 02:27:14.860]   And so I'm not going to blame it's the story author Andrew Russ Sorkin.
[02:27:14.860 --> 02:27:18.940]   But an AI generated spoof rattles the markets.
[02:27:18.940 --> 02:27:21.780]   That's not the lead that it's AI generated.
[02:27:21.780 --> 02:27:22.780]   That's not the story.
[02:27:22.780 --> 02:27:24.860]   Yeah, it doesn't matter.
[02:27:24.860 --> 02:27:25.860]   It's really the story.
[02:27:25.860 --> 02:27:28.340]   The Twitter will be an archive photo.
[02:27:28.340 --> 02:27:30.300]   It could have taken a not a line photo up there.
[02:27:30.300 --> 02:27:32.460]   People would have been fooled for 10 minutes.
[02:27:32.460 --> 02:27:38.020]   It may have been the first time an AI generated image moved the market according to Bloomberg.
[02:27:38.020 --> 02:27:40.020]   It's not about the AI.
[02:27:40.020 --> 02:27:41.020]   It's about Twitter.
[02:27:41.020 --> 02:27:42.700]   We just didn't know that the other ones were AI.
[02:27:42.700 --> 02:27:43.700]   Yeah, right.
[02:27:43.700 --> 02:27:45.700]   So I'm not going to say that.
[02:27:45.700 --> 02:27:46.700]   I'm not going to say that.
[02:27:46.700 --> 02:27:47.700]   I'm not going to say that.
[02:27:47.700 --> 02:27:48.700]   I'm not going to say that.
[02:27:48.700 --> 02:27:49.700]   I'm not going to say that.
[02:27:49.700 --> 02:27:50.700]   I'm not going to say that.
[02:27:50.700 --> 02:27:51.700]   I'm not going to say that.
[02:27:51.700 --> 02:27:52.700]   I'm not going to say that.
[02:27:52.700 --> 02:27:53.700]   I'm not going to say that.
[02:27:53.700 --> 02:27:54.700]   I'm not going to say that.
[02:27:54.700 --> 02:27:55.700]   I'm not going to say that.
[02:27:55.700 --> 02:27:56.700]   I'm not going to say that.
[02:27:56.700 --> 02:27:57.700]   I'm not going to say that.
[02:27:57.700 --> 02:27:58.700]   I'm not going to say that.
[02:27:58.700 --> 02:27:59.700]   I'm not going to say that.
[02:27:59.700 --> 02:28:00.700]   I'm not going to say that.
[02:28:00.700 --> 02:28:01.700]   I'm not going to say that.
[02:28:01.700 --> 02:28:02.700]   I'm not going to say that.
[02:28:02.700 --> 02:28:03.700]   I'm not going to say that.
[02:28:03.700 --> 02:28:04.700]   I'm not going to say that.
[02:28:04.700 --> 02:28:05.700]   I'm not going to say that.
[02:28:05.700 --> 02:28:12.700]   I'm not going to say that.
[02:28:12.700 --> 02:28:13.700]   I'm not going to say that.
[02:28:13.700 --> 02:28:14.700]   I'm not going to say that.
[02:28:14.700 --> 02:28:15.700]   I'm not going to say that.
[02:28:15.700 --> 02:28:16.700]   I'm not going to say that.
[02:28:16.700 --> 02:28:17.700]   I'm not going to say that.
[02:28:17.700 --> 02:28:18.700]   I'm not going to say that.
[02:28:18.700 --> 02:28:19.700]   I'm not going to say that.
[02:28:19.700 --> 02:28:20.700]   I'm not going to say that.
[02:28:20.700 --> 02:28:21.700]   I'm not going to say that.
[02:28:21.700 --> 02:28:22.700]   I'm not going to say that.
[02:28:22.700 --> 02:28:23.700]   I'm not going to say that.
[02:28:23.700 --> 02:28:24.700]   I'm not going to say that.
[02:28:24.700 --> 02:28:25.700]   I'm not going to say that.
[02:28:25.700 --> 02:28:26.700]   I'm not going to say that.
[02:28:26.700 --> 02:28:27.700]   I'm not going to say that.
[02:28:27.700 --> 02:28:28.700]   I'm not going to say that.
[02:28:28.700 --> 02:28:29.700]   I'm not going to say that.
[02:28:29.700 --> 02:28:30.700]   I'm not going to say that.
[02:28:30.700 --> 02:28:31.700]   I'm not going to say that.
[02:28:31.700 --> 02:28:32.700]   I'm not going to say that.
[02:28:32.700 --> 02:28:33.700]   I'm not going to say that.
[02:28:33.700 --> 02:28:34.700]   I'm not going to say that.
[02:28:34.700 --> 02:28:35.700]   I'm not going to say that.
[02:28:35.700 --> 02:28:36.700]   I'm not going to say that.
[02:28:36.700 --> 02:28:37.700]   I'm not going to say that.
[02:28:37.700 --> 02:28:38.700]   I'm not going to say that.
[02:28:38.700 --> 02:28:39.700]   I'm not going to say that.
[02:28:39.700 --> 02:28:40.700]   I'm not going to say that.
[02:28:40.700 --> 02:28:41.700]   I'm not going to say that.
[02:28:41.700 --> 02:28:42.700]   I'm not going to say that.
[02:28:42.700 --> 02:28:43.700]   I'm not going to say that.
[02:28:43.700 --> 02:28:44.700]   I'm not going to say that.
[02:28:44.700 --> 02:28:45.700]   I'm not going to say that.
[02:28:45.700 --> 02:28:46.700]   I'm not going to say that.
[02:28:46.700 --> 02:28:47.700]   I'm not going to say that.
[02:28:47.700 --> 02:28:48.700]   I'm not going to say that.
[02:28:48.700 --> 02:28:49.700]   I'm not going to say that.
[02:28:49.700 --> 02:28:50.700]   I'm not going to say that.
[02:28:50.700 --> 02:28:51.700]   I'm not going to say that.
[02:28:51.700 --> 02:28:52.700]   I'm not going to say that.
[02:28:52.700 --> 02:28:53.700]   I'm not going to say that.
[02:28:53.700 --> 02:28:54.700]   I'm not going to say that.
[02:28:54.700 --> 02:28:55.700]   I'm not going to say that.
[02:28:55.700 --> 02:28:56.700]   I'm not going to say that.
[02:28:56.700 --> 02:28:57.700]   I'm not going to say that.
[02:28:57.700 --> 02:28:58.700]   I'm not going to say that.
[02:28:58.700 --> 02:28:59.700]   I'm not going to say that.
[02:28:59.700 --> 02:29:00.700]   I'm not going to say that.
[02:29:00.700 --> 02:29:01.700]   I'm not going to say that.
[02:29:01.700 --> 02:29:02.700]   I'm not going to say that.
[02:29:02.700 --> 02:29:03.700]   I'm not going to say that.
[02:29:03.700 --> 02:29:04.700]   I'm not going to say that.
[02:29:04.700 --> 02:29:05.700]   I'm not going to say that.
[02:29:05.700 --> 02:29:06.700]   I'm not going to say that.
[02:29:06.700 --> 02:29:07.700]   I'm not going to say that.
[02:29:07.700 --> 02:29:08.700]   I'm not going to say that.
[02:29:08.700 --> 02:29:09.700]   I'm not going to say that.
[02:29:09.700 --> 02:29:10.700]   I'm not going to say that.
[02:29:10.700 --> 02:29:11.700]   I'm not going to say that.
[02:29:11.700 --> 02:29:12.700]   I'm not going to say that.
[02:29:12.700 --> 02:29:13.700]   I'm not going to say that.
[02:29:13.700 --> 02:29:14.700]   I'm not going to say that.
[02:29:14.700 --> 02:29:15.700]   I'm not going to say that.
[02:29:15.700 --> 02:29:16.700]   I'm not going to say that.
[02:29:16.700 --> 02:29:17.700]   I'm not going to say that.
[02:29:17.700 --> 02:29:18.700]   I'm not going to say that.
[02:29:18.700 --> 02:29:19.700]   I'm not going to say that.
[02:29:19.700 --> 02:29:20.700]   I'm not going to say that.
[02:29:20.700 --> 02:29:21.700]   I'm not going to say that.
[02:29:21.700 --> 02:29:22.700]   I'm not going to say that.
[02:29:22.700 --> 02:29:23.700]   I'm not going to say that.
[02:29:23.700 --> 02:29:24.700]   I'm not going to say that.
[02:29:24.700 --> 02:29:25.700]   I'm not going to say that.
[02:29:25.700 --> 02:29:26.700]   I'm not going to say that.
[02:29:26.700 --> 02:29:27.700]   I'm not going to say that.
[02:29:27.700 --> 02:29:28.700]   I'm not going to say that.
[02:29:28.700 --> 02:29:29.700]   I'm not going to say that.
[02:29:29.700 --> 02:29:30.700]   I'm not going to say that.
[02:29:30.700 --> 02:29:31.700]   I'm not going to say that.
[02:29:31.700 --> 02:29:32.700]   I'm not going to say that.
[02:29:32.700 --> 02:29:33.700]   I'm not going to say that.
[02:29:33.700 --> 02:29:34.700]   I'm not going to say that.
[02:29:34.700 --> 02:29:35.700]   I'm not going to say that.
[02:29:35.700 --> 02:29:36.700]   I'm not going to say that.
[02:29:36.700 --> 02:29:37.700]   I'm not going to say that.
[02:29:37.700 --> 02:29:38.700]   I'm not going to say that.
[02:29:38.700 --> 02:29:39.700]   I'm not going to say that.
[02:29:39.700 --> 02:29:41.700]   I'm not going to say that.
[02:29:41.700 --> 02:29:42.700]   I'm not going to say that.
[02:29:42.700 --> 02:29:43.700]   I'm not going to say that.
[02:29:43.700 --> 02:29:44.700]   I'm not going to say that.
[02:29:44.700 --> 02:29:45.700]   I'm not going to say that.
[02:29:45.700 --> 02:29:46.700]   I'm not going to say that.
[02:29:46.700 --> 02:29:47.700]   I'm not going to say that.
[02:29:47.700 --> 02:29:48.700]   I'm not going to say that.
[02:29:48.700 --> 02:29:49.700]   I'm not going to say that.
[02:29:49.700 --> 02:29:50.700]   I'm not going to say that.
[02:29:50.700 --> 02:29:51.700]   I'm not going to say that.
[02:29:51.700 --> 02:29:52.700]   I'm not going to say that.
[02:29:52.700 --> 02:29:53.700]   I'm not going to say that.
[02:29:53.700 --> 02:29:54.700]   I'm not going to say that.
[02:29:54.700 --> 02:29:55.700]   I'm not going to say that.
[02:29:55.700 --> 02:29:56.700]   I'm not going to say that.
[02:29:56.700 --> 02:29:57.700]   I'm not going to say that.
[02:29:57.700 --> 02:29:58.700]   I'm not going to say that.
[02:29:58.700 --> 02:29:59.700]   I'm not going to say that.
[02:29:59.700 --> 02:30:00.700]   I'm not going to say that.
[02:30:00.700 --> 02:30:01.700]   I'm not going to say that.
[02:30:01.700 --> 02:30:02.700]   I'm not going to say that.
[02:30:02.700 --> 02:30:03.700]   I'm not going to say that.
[02:30:03.700 --> 02:30:04.700]   I'm not going to say that.
[02:30:04.700 --> 02:30:05.700]   I'm not going to say that.
[02:30:05.700 --> 02:30:06.700]   I'm not going to say that.
[02:30:06.700 --> 02:30:07.700]   I'm not going to say that.
[02:30:07.700 --> 02:30:08.700]   I'm not going to say that.
[02:30:08.700 --> 02:30:09.700]   I'm not going to say that.
[02:30:09.700 --> 02:30:10.700]   I'm not going to say that.
[02:30:10.700 --> 02:30:11.700]   I'm not going to say that.
[02:30:11.700 --> 02:30:12.700]   I'm not going to say that.
[02:30:12.700 --> 02:30:13.700]   I'm not going to say that.
[02:30:13.700 --> 02:30:14.700]   I'm not going to say that.
[02:30:14.700 --> 02:30:15.700]   I'm not going to say that.
[02:30:15.700 --> 02:30:16.700]   I'm not going to say that.
[02:30:16.700 --> 02:30:17.700]   I'm not going to say that.
[02:30:17.700 --> 02:30:18.700]   I'm not going to say that.
[02:30:18.700 --> 02:30:19.700]   I'm not going to say that.
[02:30:19.700 --> 02:30:20.700]   I'm not going to say that.
[02:30:20.700 --> 02:30:21.700]   I'm not going to say that.
[02:30:21.700 --> 02:30:22.700]   I'm not going to say that.
[02:30:22.700 --> 02:30:23.700]   I'm not going to say that.
[02:30:23.700 --> 02:30:24.700]   I'm not going to say that.
[02:30:24.700 --> 02:30:25.700]   I'm not going to say that.
[02:30:25.700 --> 02:30:26.700]   I'm not going to say that.
[02:30:26.700 --> 02:30:27.700]   I'm not going to say that.
[02:30:27.700 --> 02:30:28.700]   I'm not going to say that.
[02:30:28.700 --> 02:30:29.700]   I'm not going to say that.
[02:30:29.700 --> 02:30:30.700]   I'm not going to say that.
[02:30:30.700 --> 02:30:31.700]   I'm not going to say that.
[02:30:31.700 --> 02:30:32.700]   I'm not going to say that.
[02:30:32.700 --> 02:30:33.700]   I'm not going to say that.
[02:30:33.700 --> 02:30:34.700]   I'm not going to say that.
[02:30:34.700 --> 02:30:35.700]   I'm not going to say that.
[02:30:35.700 --> 02:30:36.700]   I'm not going to say that.
[02:30:36.700 --> 02:30:37.700]   I'm not going to say that.
[02:30:37.700 --> 02:30:38.700]   I'm not going to say that.
[02:30:38.700 --> 02:30:39.700]   I'm not going to say that.
[02:30:39.700 --> 02:30:40.700]   I'm not going to say that.
[02:30:40.700 --> 02:30:41.700]   I'm not going to say that.
[02:30:41.700 --> 02:30:42.700]   I'm not going to say that.
[02:30:42.700 --> 02:30:43.700]   I'm not going to say that.
[02:30:43.700 --> 02:30:44.700]   I'm not going to say that.
[02:30:44.700 --> 02:30:45.700]   I'm not going to say that.
[02:30:45.700 --> 02:30:46.700]   I'm not going to say that.
[02:30:46.700 --> 02:30:47.700]   I'm not going to say that.
[02:30:47.700 --> 02:30:48.700]   I'm not going to say that.
[02:30:48.700 --> 02:30:49.700]   I'm not going to say that.
[02:30:49.700 --> 02:30:50.700]   I'm not going to say that.
[02:30:50.700 --> 02:30:51.700]   I'm not going to say that.
[02:30:51.700 --> 02:30:52.700]   I'm not going to say that.
[02:30:52.700 --> 02:30:53.700]   I'm not going to say that.
[02:30:53.700 --> 02:30:54.700]   I'm not going to say that.
[02:30:54.700 --> 02:30:55.700]   I'm not going to say that.
[02:30:55.700 --> 02:30:56.700]   I'm not going to say that.
[02:30:56.700 --> 02:30:57.700]   I'm not going to say that.
[02:30:57.700 --> 02:30:58.700]   I'm not going to say that.
[02:30:58.700 --> 02:30:59.700]   I'm not going to say that.
[02:30:59.700 --> 02:31:00.700]   I'm not going to say that.
[02:31:00.700 --> 02:31:01.700]   I'm not going to say that.
[02:31:01.700 --> 02:31:02.700]   I'm not going to say that.
[02:31:02.700 --> 02:31:03.700]   I'm not going to say that.
[02:31:03.700 --> 02:31:04.700]   I'm not going to say that.
[02:31:04.700 --> 02:31:05.700]   I'm not going to say that.
[02:31:05.700 --> 02:31:06.700]   I'm not going to say that.
[02:31:06.700 --> 02:31:07.700]   I'm not going to say that.
[02:31:07.700 --> 02:31:08.700]   I'm not going to say that.
[02:31:08.700 --> 02:31:09.700]   I'm not going to say that.
[02:31:09.700 --> 02:31:10.700]   I'm not going to say that.
[02:31:10.700 --> 02:31:11.700]   I'm not going to say that.
[02:31:11.700 --> 02:31:12.700]   I'm not going to say that.
[02:31:12.700 --> 02:31:13.700]   I'm not going to say that.
[02:31:13.700 --> 02:31:14.700]   I'm not going to say that.
[02:31:14.700 --> 02:31:15.700]   I'm not going to say that.
[02:31:15.700 --> 02:31:16.700]   I'm not going to say that.
[02:31:16.700 --> 02:31:17.700]   I'm not going to say that.
[02:31:17.700 --> 02:31:18.700]   I'm not going to say that.
[02:31:18.700 --> 02:31:19.700]   I'm not going to say that.
[02:31:19.700 --> 02:31:20.700]   I'm not going to say that.
[02:31:20.700 --> 02:31:21.700]   I'm not going to say that.
[02:31:21.700 --> 02:31:22.700]   I'm not going to say that.
[02:31:22.700 --> 02:31:23.700]   I'm not going to say that.
[02:31:23.700 --> 02:31:24.700]   I'm not going to say that.
[02:31:24.700 --> 02:31:25.700]   I'm not going to say that.
[02:31:25.700 --> 02:31:26.700]   I'm not going to say that.
[02:31:26.700 --> 02:31:27.700]   I'm not going to say that.
[02:31:27.700 --> 02:31:28.700]   I'm not going to say that.
[02:31:28.700 --> 02:31:29.700]   I'm not going to say that.
[02:31:29.700 --> 02:31:30.700]   I'm not going to say that.
[02:31:30.700 --> 02:31:31.700]   I'm not going to say that.
[02:31:31.700 --> 02:31:32.700]   I'm not going to say that.
[02:31:32.700 --> 02:31:33.700]   I'm not going to say that.
[02:31:33.700 --> 02:31:34.700]   I'm not going to say that.
[02:31:34.700 --> 02:31:35.700]   I'm not going to say that.
[02:31:35.700 --> 02:31:36.700]   I'm not going to say that.
[02:31:36.700 --> 02:31:37.700]   I'm not going to say that.
[02:31:37.700 --> 02:31:38.700]   I'm not going to say that.
[02:31:38.700 --> 02:31:39.700]   I'm not going to say that.
[02:31:39.700 --> 02:31:40.700]   I'm not going to say that.
[02:31:40.700 --> 02:31:41.700]   I'm not going to say that.
[02:31:41.700 --> 02:31:42.700]   I'm not going to say that.
[02:31:42.700 --> 02:31:43.700]   I'm not going to say that.
[02:31:43.700 --> 02:31:44.700]   I'm not going to say that.
[02:31:44.700 --> 02:31:45.700]   I'm not going to say that.
[02:31:45.700 --> 02:31:46.700]   I'm not going to say that.
[02:31:46.700 --> 02:31:47.700]   I'm not going to say that.
[02:31:47.700 --> 02:31:48.700]   I'm not going to say that.
[02:31:48.700 --> 02:31:49.700]   I'm not going to say that.
[02:31:49.700 --> 02:31:50.700]   I'm not going to say that.
[02:31:50.700 --> 02:31:51.700]   I'm not going to say that.
[02:31:51.700 --> 02:31:52.700]   I'm not going to say that.
[02:31:52.700 --> 02:31:53.700]   I'm not going to say that.
[02:31:53.700 --> 02:31:54.700]   I'm not going to say that.
[02:31:54.700 --> 02:31:55.700]   I'm not going to say that.
[02:31:55.700 --> 02:31:56.700]   I'm not going to say that.
[02:31:56.700 --> 02:31:57.700]   I'm not going to say that.
[02:31:57.700 --> 02:31:58.700]   I'm not going to say that.
[02:31:58.700 --> 02:31:59.700]   I'm not going to say that.
[02:31:59.700 --> 02:32:00.700]   I'm not going to say that.
[02:32:00.700 --> 02:32:01.700]   I'm not going to say that.
[02:32:01.700 --> 02:32:02.700]   I'm not going to say that.
[02:32:02.700 --> 02:32:03.700]   I'm not going to say that.
[02:32:03.700 --> 02:32:04.700]   I'm not going to say that.
[02:32:04.700 --> 02:32:05.700]   I'm not going to say that.
[02:32:05.700 --> 02:32:06.700]   I'm not going to say that.
[02:32:06.700 --> 02:32:07.700]   I'm not going to say that.
[02:32:07.700 --> 02:32:08.700]   I'm not going to say that.
[02:32:08.700 --> 02:32:09.700]   I'm not going to say that.
[02:32:09.700 --> 02:32:10.700]   I'm not going to say that.
[02:32:10.700 --> 02:32:11.700]   I'm not going to say that.
[02:32:11.700 --> 02:32:12.700]   I'm not going to say that.
[02:32:12.700 --> 02:32:13.700]   I'm not going to say that.
[02:32:13.700 --> 02:32:14.700]   I'm not going to say that.
[02:32:14.700 --> 02:32:15.700]   I'm not going to say that.
[02:32:15.700 --> 02:32:16.700]   I'm not going to say that.
[02:32:16.700 --> 02:32:17.700]   I'm not going to say that.
[02:32:17.700 --> 02:32:18.700]   I'm not going to say that.
[02:32:18.700 --> 02:32:19.700]   I'm not going to say that.
[02:32:19.700 --> 02:32:20.700]   I'm not going to say that.
[02:32:20.700 --> 02:32:21.700]   I'm not going to say that.
[02:32:21.700 --> 02:32:22.700]   I'm not going to say that.
[02:32:22.700 --> 02:32:23.700]   I'm not going to say that.
[02:32:23.700 --> 02:32:24.700]   I'm not going to say that.
[02:32:24.700 --> 02:32:25.700]   I'm not going to say that.
[02:32:25.700 --> 02:32:26.700]   I'm not going to say that.
[02:32:26.700 --> 02:32:27.700]   I'm not going to say that.
[02:32:27.700 --> 02:32:28.700]   I'm not going to say that.
[02:32:28.700 --> 02:32:29.700]   I'm not going to say that.
[02:32:29.700 --> 02:32:30.700]   I'm not going to say that.
[02:32:30.700 --> 02:32:31.700]   I'm not going to say that.
[02:32:31.700 --> 02:32:32.700]   I'm not going to say that.
[02:32:32.700 --> 02:32:33.700]   I'm not going to say that.
[02:32:33.700 --> 02:32:34.700]   I'm not going to say that.
[02:32:34.700 --> 02:32:35.700]   I'm not going to say that.
[02:32:35.700 --> 02:32:36.700]   I'm not going to say that.
[02:32:36.700 --> 02:32:38.700]   I'm not going to say that.
[02:32:38.700 --> 02:32:39.700]   I'm not going to say that.
[02:32:39.700 --> 02:32:40.700]   I'm not going to say that.
[02:32:40.700 --> 02:32:41.700]   I'm not going to say that.
[02:32:41.700 --> 02:32:42.700]   I'm not going to say that.
[02:32:42.700 --> 02:32:43.700]   I'm not going to say that.
[02:32:43.700 --> 02:32:44.700]   I'm not going to say that.
[02:32:44.700 --> 02:32:45.700]   I'm not going to say that.
[02:32:45.700 --> 02:32:46.700]   I'm not going to say that.
[02:32:46.700 --> 02:32:47.700]   I'm not going to say that.
[02:32:47.700 --> 02:32:48.700]   I'm not going to say that.
[02:32:48.700 --> 02:32:49.700]   I'm not going to say that.
[02:32:49.700 --> 02:32:50.700]   I'm not going to say that.
[02:32:50.700 --> 02:32:51.700]   I'm not going to say that.
[02:32:51.700 --> 02:32:52.700]   I'm not going to say that.
[02:32:52.700 --> 02:32:53.700]   I'm not going to say that.
[02:32:53.700 --> 02:32:54.700]   I'm not going to say that.
[02:32:54.700 --> 02:32:55.700]   I'm not going to say that.
[02:32:55.700 --> 02:32:56.700]   I'm not going to say that.
[02:32:56.700 --> 02:32:57.700]   I'm not going to say that.
[02:32:57.700 --> 02:32:58.700]   I'm not going to say that.
[02:32:58.700 --> 02:32:59.700]   I'm not going to say that.
[02:32:59.700 --> 02:33:00.700]   I'm not going to say that.
[02:33:00.700 --> 02:33:01.700]   I'm not going to say that.
[02:33:01.700 --> 02:33:02.700]   I'm not going to say that.
[02:33:02.700 --> 02:33:03.700]   I'm not going to say that.
[02:33:03.700 --> 02:33:04.700]   I'm not going to say that.
[02:33:04.700 --> 02:33:05.700]   I'm not going to say that.
[02:33:05.700 --> 02:33:06.700]   I'm not going to say that.
[02:33:06.700 --> 02:33:07.700]   I'm not going to say that.
[02:33:07.700 --> 02:33:08.700]   I'm not going to say that.
[02:33:08.700 --> 02:33:09.700]   I'm not going to say that.
[02:33:09.700 --> 02:33:10.700]   I'm not going to say that.
[02:33:10.700 --> 02:33:11.700]   I'm not going to say that.
[02:33:11.700 --> 02:33:12.700]   I'm not going to say that.
[02:33:12.700 --> 02:33:13.700]   I'm not going to say that.
[02:33:13.700 --> 02:33:14.700]   I'm not going to say that.
[02:33:14.700 --> 02:33:15.700]   I'm not going to say that.
[02:33:15.700 --> 02:33:16.700]   I'm not going to say that.
[02:33:16.700 --> 02:33:17.700]   I'm not going to say that.
[02:33:17.700 --> 02:33:18.700]   I'm not going to say that.
[02:33:18.700 --> 02:33:19.700]   I'm not going to say that.
[02:33:19.700 --> 02:33:20.700]   I'm not going to say that.
[02:33:20.700 --> 02:33:21.700]   I'm not going to say that.
[02:33:21.700 --> 02:33:22.700]   I'm not going to say that.
[02:33:22.700 --> 02:33:23.700]   I'm not going to say that.
[02:33:23.700 --> 02:33:24.700]   I'm not going to say that.
[02:33:24.700 --> 02:33:25.700]   I'm not going to say that.
[02:33:25.700 --> 02:33:26.700]   I'm not going to say that.
[02:33:26.700 --> 02:33:27.700]   I'm not going to say that.
[02:33:27.700 --> 02:33:28.700]   I'm not going to say that.
[02:33:28.700 --> 02:33:29.700]   I'm not going to say that.
[02:33:29.700 --> 02:33:30.700]   I'm not going to say that.
[02:33:30.700 --> 02:33:31.700]   I'm not going to say that.
[02:33:31.700 --> 02:33:32.700]   I'm not going to say that.
[02:33:32.700 --> 02:33:33.700]   I'm not going to say that.
[02:33:33.700 --> 02:33:34.700]   I'm not going to say that.
[02:33:34.700 --> 02:33:35.700]   I'm not going to say that.
[02:33:35.700 --> 02:33:36.700]   I'm not going to say that.
[02:33:36.700 --> 02:33:37.700]   I'm not going to say that.
[02:33:37.700 --> 02:33:38.700]   I'm not going to say that.
[02:33:38.700 --> 02:33:39.700]   I'm not going to say that.
[02:33:39.700 --> 02:33:40.700]   I'm not going to say that.
[02:33:40.700 --> 02:33:41.700]   I'm not going to say that.
[02:33:41.700 --> 02:33:42.700]   I'm not going to say that.
[02:33:42.700 --> 02:33:43.700]   I'm not going to say that.
[02:33:43.700 --> 02:33:44.700]   I'm not going to say that.
[02:33:44.700 --> 02:33:45.700]   I'm not going to say that.
[02:33:45.700 --> 02:33:46.700]   I'm not going to say that.
[02:33:46.700 --> 02:33:47.700]   I'm not going to say that.
[02:33:47.700 --> 02:33:48.700]   I'm not going to say that.
[02:33:48.700 --> 02:33:49.700]   I'm not going to say that.
[02:33:49.700 --> 02:33:50.700]   I'm not going to say that.
[02:33:50.700 --> 02:33:51.700]   I'm not going to say that.
[02:33:51.700 --> 02:33:52.700]   I'm not going to say that.
[02:33:52.700 --> 02:33:53.700]   I'm not going to say that.
[02:33:53.700 --> 02:33:54.700]   I'm not going to say that.
[02:33:54.700 --> 02:33:55.700]   I'm not going to say that.
[02:33:55.700 --> 02:33:56.700]   I'm not going to say that.
[02:33:56.700 --> 02:33:57.700]   I'm not going to say that.
[02:33:57.700 --> 02:33:58.700]   I'm not going to say that.
[02:33:58.700 --> 02:33:59.700]   I'm not going to say that.
[02:33:59.700 --> 02:34:00.700]   I'm not going to say that.
[02:34:00.700 --> 02:34:01.700]   I'm not going to say that.
[02:34:01.700 --> 02:34:02.700]   I'm not going to say that.
[02:34:02.700 --> 02:34:03.700]   I'm not going to say that.
[02:34:03.700 --> 02:34:04.700]   I'm not going to say that.
[02:34:04.700 --> 02:34:05.700]   I'm not going to say that.
[02:34:05.700 --> 02:34:06.700]   Fastmail is the best.
[02:34:06.700 --> 02:34:07.700]   You ask the experts.
[02:34:07.700 --> 02:34:09.700]   They all tell you that.
[02:34:09.700 --> 02:34:11.700]   Fastmail.com/twig.
[02:34:11.700 --> 02:34:16.900]   In case you're interested in the open source service server, they've contributed back to
[02:34:16.900 --> 02:34:18.700]   that project many, many times.
[02:34:18.700 --> 02:34:20.700]   It's a very good IMAP server.
[02:34:20.700 --> 02:34:22.700]   It really does the job.
[02:34:22.700 --> 02:34:24.700]   Fastmail.com/twig.
[02:34:24.700 --> 02:34:29.100]   Thank you, Fastmail, for being my email service, for doing such a great job for me.
[02:34:29.100 --> 02:34:31.700]   For all these years, I'm glad we could finally get them as an advertiser.
[02:34:31.700 --> 02:34:33.700]   We appreciate your support for Twig.
[02:34:33.700 --> 02:34:38.700]   When you use that address, you let them know you saw it here, right?
[02:34:38.700 --> 02:34:39.700]   Fastmail.com/twig.
[02:34:39.700 --> 02:34:41.700]   T-W-I-T.
[02:34:41.700 --> 02:34:42.700]   Enough.
[02:34:42.700 --> 02:34:44.700]   Ant-Pruit taking the day off.
[02:34:44.700 --> 02:34:46.700]   He'll be back next week.
[02:34:46.700 --> 02:34:48.700]   I'm hoping Stacey will be back next week.
[02:34:48.700 --> 02:34:50.700]   I think she had to do something, right?
[02:34:50.700 --> 02:34:54.700]   But we're so glad we could get Mike Elgin joining us, Gastronomad.net.
[02:34:54.700 --> 02:34:56.700]   That's where he hangs his hat.
[02:34:56.700 --> 02:35:02.700]   These days, of course, Elgin.com is sub-stack, and he's on Mastodon at Mike Elgin.
[02:35:02.700 --> 02:35:09.700]   He is in Italy for Gastronomad experience the Prosecco experience.
[02:35:09.700 --> 02:35:12.700]   Is this the Venetto right there?
[02:35:12.700 --> 02:35:13.700]   That's it.
[02:35:13.700 --> 02:35:14.700]   That's it.
[02:35:14.700 --> 02:35:18.700]   I'm probably a quarter of a mile from that very spot.
[02:35:18.700 --> 02:35:21.700]   What did you have for dinner tonight, Mike?
[02:35:21.700 --> 02:35:22.700]   Prosecco.
[02:35:22.700 --> 02:35:23.700]   Pizza.
[02:35:23.700 --> 02:35:24.700]   Pizza.
[02:35:24.700 --> 02:35:25.700]   Pizza.
[02:35:25.700 --> 02:35:26.700]   Oh, of course.
[02:35:26.700 --> 02:35:27.700]   Pizza.
[02:35:27.700 --> 02:35:28.700]   And salad, yes.
[02:35:28.700 --> 02:35:29.700]   There's a lot of good, really amazing food.
[02:35:29.700 --> 02:35:31.700]   Finishing food is really something.
[02:35:31.700 --> 02:35:32.700]   Yeah.
[02:35:32.700 --> 02:35:34.700]   They go to Morocco next, El Salvador.
[02:35:34.700 --> 02:35:36.700]   Oh, that'll be fun.
[02:35:36.700 --> 02:35:37.700]   That's right.
[02:35:37.700 --> 02:35:39.700]   El Salvador is a brand new one.
[02:35:39.700 --> 02:35:40.700]   That's right.
[02:35:40.700 --> 02:35:43.700]   Amira was born in El Salvador, and she knows a lot of people.
[02:35:43.700 --> 02:35:47.700]   We've been there probably 30, 40 times in the last few years.
[02:35:47.700 --> 02:35:51.700]   And it's a really cool country that's a lot safer now than it used to be.
[02:35:51.700 --> 02:35:56.700]   And so now we're planning this experience, and it is going to be amazing.
[02:35:56.700 --> 02:35:58.700]   And that's in January.
[02:35:58.700 --> 02:36:04.700]   So if you live in Maine or New England or something like that and don't want to freeze your Hynioff,
[02:36:04.700 --> 02:36:09.700]   it'll be in the mid-high 70s in January and El Salvador.
[02:36:09.700 --> 02:36:10.700]   Oh, nice.
[02:36:10.700 --> 02:36:11.700]   Nice.
[02:36:11.700 --> 02:36:12.700]   Gastronomat.net.
[02:36:12.700 --> 02:36:13.700]   Beautiful place.
[02:36:13.700 --> 02:36:19.700]   We went on the Oaxaca experience for the day of the day at last year, and it was incredible.
[02:36:19.700 --> 02:36:20.700]   Just so much fun.
[02:36:20.700 --> 02:36:21.700]   Mine boggling.
[02:36:21.700 --> 02:36:22.700]   Actually, not last year.
[02:36:22.700 --> 02:36:23.700]   It's 20, 20, 20, 20.
[02:36:23.700 --> 02:36:24.700]   That's right.
[02:36:24.700 --> 02:36:25.700]   That's right.
[02:36:25.700 --> 02:36:26.700]   Holy cow.
[02:36:26.700 --> 02:36:27.700]   Couple year, year and a half ago.
[02:36:27.700 --> 02:36:28.700]   Yeah.
[02:36:28.700 --> 02:36:30.700]   Time for a lot of fun.
[02:36:30.700 --> 02:36:32.700]   Time flies.
[02:36:32.700 --> 02:36:38.200]   Also with us, the wonderful Christmas scene of the creator of the most important feature
[02:36:38.200 --> 02:36:42.500]   on Twitter, the hashtag, which is now migrated everywhere.
[02:36:42.500 --> 02:36:47.580]   I do blame you though for every time I travel now, people are all those Instagram influencers
[02:36:47.580 --> 02:36:48.580]   are out there.
[02:36:48.580 --> 02:36:54.700]   Hashtagging, their food, their pictures, their hashtag lavender fields.
[02:36:54.700 --> 02:36:59.700]   Sigh.
[02:36:59.700 --> 02:37:04.700]   When you see young people say hashtag amazing, do you like, oh, yeah, I thought of that.
[02:37:04.700 --> 02:37:05.700]   I thought that up.
[02:37:05.700 --> 02:37:06.700]   That was me.
[02:37:06.700 --> 02:37:07.700]   Sometimes.
[02:37:07.700 --> 02:37:12.700]   It's such a weird thing to have contributed something that becomes part of the culture,
[02:37:12.700 --> 02:37:15.700]   such that people don't even have the concept that there was a time where the thing didn't
[02:37:15.700 --> 02:37:16.700]   exist.
[02:37:16.700 --> 02:37:19.700]   It's sort of like inventing the dollar sign or something.
[02:37:19.700 --> 02:37:20.700]   Yeah.
[02:37:20.700 --> 02:37:21.700]   What was your reaction money?
[02:37:21.700 --> 02:37:22.700]   What was your reaction money?
[02:37:22.700 --> 02:37:24.700]   Black Lives Matter happened.
[02:37:24.700 --> 02:37:26.700]   Oh, yeah.
[02:37:26.700 --> 02:37:28.260]   Hashtag BLM.
[02:37:28.260 --> 02:37:29.260]   That's right.
[02:37:29.260 --> 02:37:30.260]   Yeah.
[02:37:30.260 --> 02:37:33.700]   I mean, that me too, you know, hashtag me too.
[02:37:33.700 --> 02:37:34.700]   Holy cow.
[02:37:34.700 --> 02:37:36.700]   They're like great.
[02:37:36.700 --> 02:37:40.460]   There's an element of just like humbleness to it, right?
[02:37:40.460 --> 02:37:44.420]   And the reason why I got into working on the social lab was because I wanted more people
[02:37:44.420 --> 02:37:47.340]   to be able to contribute their voices and to join the conversation.
[02:37:47.340 --> 02:37:52.620]   And the hashtag was a method by which people could do that with, you know, like what was
[02:37:52.620 --> 02:37:53.780]   becoming a new network.
[02:37:53.780 --> 02:37:56.460]   So I think there's there's some amount of pride.
[02:37:56.460 --> 02:37:57.460]   There's some amount of trepidation.
[02:37:57.460 --> 02:38:01.860]   There's some amount of kind of just awe that it's possible, you know, especially depending
[02:38:01.860 --> 02:38:05.620]   on when you join these platforms and when you get involved with them to really shape
[02:38:05.620 --> 02:38:08.460]   the outcomes and shape how they turn out.
[02:38:08.460 --> 02:38:14.100]   So I think, you know, for, for it's taking kind of a long time, I think for me to get
[02:38:14.100 --> 02:38:18.660]   into the right, maybe like mental space about what it means to have contributed something
[02:38:18.660 --> 02:38:20.420]   like the hashtag to the internet and to culture.
[02:38:20.420 --> 02:38:26.540]   But to me, it also presents an opportunity to tell young people about what it means to
[02:38:26.540 --> 02:38:29.540]   contribute and build these places into the spaces that they actually want to be in and
[02:38:29.540 --> 02:38:32.100]   they want other people to have a good time.
[02:38:32.100 --> 02:38:35.340]   And it doesn't just happen to you, you know, moderation on these platforms doesn't just
[02:38:35.340 --> 02:38:38.900]   kind of, you know, it shouldn't be assumed that the way in which these things are has
[02:38:38.900 --> 02:38:40.540]   to be the way in which they persist.
[02:38:40.540 --> 02:38:45.740]   And the responsibility we all have, yes, when we encourage a fight, we're making, we're
[02:38:45.740 --> 02:38:46.740]   making the internet.
[02:38:46.740 --> 02:38:47.740]   Yeah.
[02:38:47.740 --> 02:38:48.940]   And we're recommending good stuff.
[02:38:48.940 --> 02:38:53.460]   We're making the internet where we put our attention is, you know, where that oftentimes
[02:38:53.460 --> 02:38:54.700]   gets fed back to us.
[02:38:54.700 --> 02:38:57.100]   And it's more and more important, especially now.
[02:38:57.100 --> 02:38:59.660]   Well, I can't thank you enough for what you've done.
[02:38:59.660 --> 02:39:01.940]   We're really glad to have you on.
[02:39:01.940 --> 02:39:06.780]   This is at the end of the show, we'd like to give people a chance to, you know, mention
[02:39:06.780 --> 02:39:08.380]   pics or things they want.
[02:39:08.380 --> 02:39:11.940]   You spend only many hours a day picking things on product.
[02:39:11.940 --> 02:39:16.100]   And is there anything you would like to pick here on the show?
[02:39:16.100 --> 02:39:17.100]   Wow.
[02:39:17.100 --> 02:39:21.860]   Let's see, you know, I mean, like, I do end up hunting a lot of things.
[02:39:21.860 --> 02:39:23.900]   Actually, you know, here, I'll give you this one.
[02:39:23.900 --> 02:39:30.780]   I don't know when this is going to come out, but I can't say specifically what it is, but
[02:39:30.780 --> 02:39:32.420]   there is a web browser that I now use.
[02:39:32.420 --> 02:39:33.420]   It is called Arc.
[02:39:33.420 --> 02:39:34.420]   I use Arc.
[02:39:34.420 --> 02:39:35.420]   It's made by a browser.
[02:39:35.420 --> 02:39:36.420]   Yeah.
[02:39:36.420 --> 02:39:38.900]   The browser company of New York and they have a big launch coming up tomorrow.
[02:39:38.900 --> 02:39:41.860]   You should watch my product hunt account.
[02:39:41.860 --> 02:39:44.180]   You will discover the thing that they're launching.
[02:39:44.180 --> 02:39:50.980]   It's very cool. And as far as, let's just say, the read, write web that, you know, was
[02:39:50.980 --> 02:39:54.380]   something that Richard McManus, of course, was writing about for many years, that idea,
[02:39:54.380 --> 02:39:57.460]   I think, is coming to roost in a very real and important way.
[02:39:57.460 --> 02:40:03.100]   And so just, you know, it starts a new conversation about what the web is and what level of control
[02:40:03.100 --> 02:40:05.580]   people can have over their internet experience.
[02:40:05.580 --> 02:40:07.260]   So watch for that tomorrow.
[02:40:07.260 --> 02:40:09.460]   I will as an Arc user.
[02:40:09.460 --> 02:40:10.900]   We got an inside.
[02:40:10.900 --> 02:40:11.900]   Yeah.
[02:40:11.900 --> 02:40:12.900]   Yeah.
[02:40:12.900 --> 02:40:14.220]   It's from the browser co.
[02:40:14.220 --> 02:40:15.220]   So go to browser.
[02:40:15.220 --> 02:40:18.220]   Well, I guess you go to Arc.net is probably the easiest.
[02:40:18.220 --> 02:40:19.220]   You go to Arc.net.
[02:40:19.220 --> 02:40:20.220]   Yeah.
[02:40:20.220 --> 02:40:23.100]   Now I believe it's still invite only, but nonetheless, get yourself on the list.
[02:40:23.100 --> 02:40:24.940]   They're probably, you know, they want to open it up soon.
[02:40:24.940 --> 02:40:27.540]   They've been working on the windows version, which is pretty interesting.
[02:40:27.540 --> 02:40:32.540]   I mean, just everything that they're doing is super, you know, creative and seeing a
[02:40:32.540 --> 02:40:37.300]   web browser come out of the East Coast, you know, as much as you think it's a global
[02:40:37.300 --> 02:40:38.300]   internet.
[02:40:38.300 --> 02:40:41.660]   New York has different ideas about media and about content.
[02:40:41.660 --> 02:40:45.020]   It is also interesting that my friend Mike Krieger, who was one of the co-founders of
[02:40:45.020 --> 02:40:50.460]   Instagram, and is now working on artifact, which is sort of a TikTok for news, if you
[02:40:50.460 --> 02:40:52.300]   will, is on the board of Arc.
[02:40:52.300 --> 02:40:53.300]   Oh.
[02:40:53.300 --> 02:40:56.100]   And so he's bringing a lot of his ideas into Arc.
[02:40:56.100 --> 02:40:58.660]   I like, I use artifact as well.
[02:40:58.660 --> 02:41:00.220]   We've talked about it.
[02:41:00.220 --> 02:41:01.980]   Very interesting.
[02:41:01.980 --> 02:41:04.540]   All right.
[02:41:04.540 --> 02:41:05.900]   I don't know if I have any invites.
[02:41:05.900 --> 02:41:07.140]   I don't think I do.
[02:41:07.140 --> 02:41:09.580]   So I just will and I'm sure you don't.
[02:41:09.580 --> 02:41:16.220]   So there is, I will say, you know, since we're talking about Arc, there is a Reddit sub, subreddit
[02:41:16.220 --> 02:41:18.460]   that I am an admin of.
[02:41:18.460 --> 02:41:20.860]   It's at r/arcbrowser.
[02:41:20.860 --> 02:41:25.020]   And every month there is a mega invite thread.
[02:41:25.020 --> 02:41:28.460]   And so for folks who want to get an invite, that might be a place where you could solicit
[02:41:28.460 --> 02:41:29.460]   it.
[02:41:29.460 --> 02:41:33.700]   Right now Mac only, but Windows coming, as you said soon and watch for an announcement
[02:41:33.700 --> 02:41:34.700]   on price.
[02:41:34.700 --> 02:41:35.700]   And also iOS tomorrow.
[02:41:35.700 --> 02:41:36.700]   So yeah, I always.
[02:41:36.700 --> 02:41:37.700]   Yes, tomorrow.
[02:41:37.700 --> 02:41:42.260]   I just wanted to mention we didn't put it in the, we didn't do the Google change log,
[02:41:42.260 --> 02:41:46.980]   but I did want to point out that Bard is now putting images in the search.
[02:41:46.980 --> 02:41:50.980]   So I asked Bard, what are some must see sites in the Veneto region of Italy?
[02:41:50.980 --> 02:41:53.740]   Like, see how accurate this is.
[02:41:53.740 --> 02:41:55.100]   Obviously Venice.
[02:41:55.100 --> 02:42:00.860]   And there's like a map from Wikipedia of Venice, Verona, no one known for the Roman
[02:42:00.860 --> 02:42:02.980]   theater, the arena of Verona.
[02:42:02.980 --> 02:42:05.340]   And Romeo and Juliet was set in Verona.
[02:42:05.340 --> 02:42:06.340]   That's right.
[02:42:06.340 --> 02:42:12.340]   And there's a lot of different ways of Shakespeare's Juliet, Capulet Lake is like a De Garda close.
[02:42:12.340 --> 02:42:13.340]   I didn't realize.
[02:42:13.340 --> 02:42:14.340]   Yes.
[02:42:14.340 --> 02:42:15.340]   I've been to guard.
[02:42:15.340 --> 02:42:16.340]   I love guard.
[02:42:16.340 --> 02:42:17.540]   We have friends who have a house there.
[02:42:17.540 --> 02:42:21.340]   The Dolomites and there are many Dolomites is unbelievable.
[02:42:21.340 --> 02:42:22.340]   Okay.
[02:42:22.340 --> 02:42:23.820]   So far, Bard's got a right.
[02:42:23.820 --> 02:42:26.580]   Palladian Villas.
[02:42:26.580 --> 02:42:30.620]   Designed by Andrea Paladio.
[02:42:30.620 --> 02:42:33.900]   Those are just a few of the many, many must see sites.
[02:42:33.900 --> 02:42:37.140]   And notice the images now in Bard.
[02:42:37.140 --> 02:42:38.140]   That's pretty cool.
[02:42:38.140 --> 02:42:39.140]   That's right.
[02:42:39.140 --> 02:42:40.140]   Yep.
[02:42:40.140 --> 02:42:44.460]   The thing we do, of course, I mean, the architecture and the nature is amazing.
[02:42:44.460 --> 02:42:48.540]   But as you know, Leo, we love the food culture and the wine culture.
[02:42:48.540 --> 02:42:51.540]   And this is one of the greatest wine regions in the world.
[02:42:51.540 --> 02:42:53.340]   It's known for Prosecco, of course.
[02:42:53.340 --> 02:42:57.500]   And Americans know one or two types of Prosecco.
[02:42:57.500 --> 02:42:59.500]   There are many dozens of types of Prosecco.
[02:42:59.500 --> 02:43:03.500]   It's an incredibly rich, diverse wine variety.
[02:43:03.500 --> 02:43:05.100]   And then there's a hundred other varieties.
[02:43:05.100 --> 02:43:13.740]   There's a famous guy who hybridized great varieties decades ago, Moscone.
[02:43:13.740 --> 02:43:18.020]   And basically, there's all these wine varieties nobody's ever heard of that are amazing.
[02:43:18.020 --> 02:43:22.780]   But we have these really great friends here who are wine makers, food producers, cheese
[02:43:22.780 --> 02:43:25.580]   makers, all this kind of stuff, who are just geniuses.
[02:43:25.580 --> 02:43:30.100]   And they quietly ply their craft and they teach us all about it.
[02:43:30.100 --> 02:43:33.700]   So it's just an amazing food and wine.
[02:43:33.700 --> 02:43:34.540]   So I followed up.
[02:43:34.540 --> 02:43:35.860]   I said, how about the wines?
[02:43:35.860 --> 02:43:37.980]   And it knew I was still talking about Veneto.
[02:43:37.980 --> 02:43:39.620]   It said Valpolicella.
[02:43:39.620 --> 02:43:41.580]   Okay, we know that one.
[02:43:41.580 --> 02:43:45.060]   Closico, suave, Prosecco.
[02:43:45.060 --> 02:43:47.100]   Well, we know about that.
[02:43:47.100 --> 02:43:49.820]   But it doesn't find any of the lesser known ones.
[02:43:49.820 --> 02:43:53.500]   So I think you're just going to have to go there with Mike and Amira and find out for
[02:43:53.500 --> 02:43:54.500]   yourself.
[02:43:54.500 --> 02:43:56.780]   Here's a little bit of trivia.
[02:43:56.780 --> 02:43:59.780]   The world's first wine critic was Pliny the Elder.
[02:43:59.780 --> 02:44:02.860]   And, you know, 2000 years ago.
[02:44:02.860 --> 02:44:05.580]   And one of his favorite wines was what we now call Prosecco.
[02:44:05.580 --> 02:44:06.700]   It wasn't sparkling back then.
[02:44:06.700 --> 02:44:08.060]   It was a still white wine.
[02:44:08.060 --> 02:44:10.980]   And many styles of Prosecco are still actually.
[02:44:10.980 --> 02:44:16.220]   But it's with the great variety that then was called Prosecco that's now called Glera.
[02:44:16.220 --> 02:44:17.340]   And he was a huge fan.
[02:44:17.340 --> 02:44:21.260]   So it was even during the Roman Empire, they were drinking Prosecco.
[02:44:21.260 --> 02:44:25.300]   Mike, you have some tools to fix news as your pick up a week.
[02:44:25.300 --> 02:44:26.300]   I do.
[02:44:26.300 --> 02:44:27.300]   I do, in fact.
[02:44:27.300 --> 02:44:32.740]   I don't necessarily recommend these as to go and use these tools as your source of news.
[02:44:32.740 --> 02:44:37.740]   But I think it's interesting to see what people are coming up with to fix what they think
[02:44:37.740 --> 02:44:38.740]   is broken about news.
[02:44:38.740 --> 02:44:42.140]   For example, news is too stressful.
[02:44:42.140 --> 02:44:43.620]   News is too sensational.
[02:44:43.620 --> 02:44:44.780]   News is too time consuming.
[02:44:44.780 --> 02:44:49.380]   So there's an app called OneSub.
[02:44:49.380 --> 02:44:56.340]   So you can find it at OneSub.io, which is an app that basically, I'm sorry.
[02:44:56.340 --> 02:45:03.060]   This is a website that gives you the news that's actionable, that's on the positive
[02:45:03.060 --> 02:45:04.060]   side.
[02:45:04.060 --> 02:45:08.980]   It's kind of like it's designed to let you be informed without being freaked out about
[02:45:08.980 --> 02:45:12.260]   the state of the world, which I think is nice.
[02:45:12.260 --> 02:45:15.180]   There's another site called BoringReport.org.
[02:45:15.180 --> 02:45:16.180]   This is hilarious.
[02:45:16.180 --> 02:45:17.180]   This is an app.
[02:45:17.180 --> 02:45:20.900]   The icon for this app, the logo for this app is somebody yawning.
[02:45:20.900 --> 02:45:21.900]   Or boring news.
[02:45:21.900 --> 02:45:22.900]   Just for my news.
[02:45:22.900 --> 02:45:27.900]   Just for my news.
[02:45:27.900 --> 02:45:28.900]   They strip out the sensationalism using AI.
[02:45:28.900 --> 02:45:29.900]   Oh, that's clever.
[02:45:29.900 --> 02:45:30.900]   These are the facts.
[02:45:30.900 --> 02:45:31.900]   Yeah.
[02:45:31.900 --> 02:45:33.460]   Oh, because I hate the link bait crap, man.
[02:45:33.460 --> 02:45:34.700]   I just get mad.
[02:45:34.700 --> 02:45:36.220]   Yeah, exactly.
[02:45:36.220 --> 02:45:38.100]   And you can see their example there.
[02:45:38.100 --> 02:45:39.100]   It's pretty, pretty.
[02:45:39.100 --> 02:45:40.500]   Alien invasion imminent.
[02:45:40.500 --> 02:45:42.220]   Earth doomed to destruction.
[02:45:42.220 --> 02:45:47.620]   Turns it into experts discuss possibility of extraterrestrial life and potential impact
[02:45:47.620 --> 02:45:49.020]   on Earth.
[02:45:49.020 --> 02:45:50.100]   Boing.
[02:45:50.100 --> 02:45:56.340]   And then there's another one that hyper summarizes the news called web. -- well, it's it.
[02:45:56.340 --> 02:45:59.700]   You can find it at web.consize.app.
[02:45:59.700 --> 02:46:04.420]   And what this does is it strips down the news to its bare essence and gives you this bullet
[02:46:04.420 --> 02:46:08.020]   point of the different aspects of a news story.
[02:46:08.020 --> 02:46:13.100]   So it'll take a TechCrunch article, for example, and it'll give you three bullet points that
[02:46:13.100 --> 02:46:15.220]   give you the main points of that article.
[02:46:15.220 --> 02:46:24.380]   And so anyway, again, I don't recommend necessarily that the brilliant news consumer who listens
[02:46:24.380 --> 02:46:30.500]   and watches this podcast rely on any of these, but they should check them out to see what
[02:46:30.500 --> 02:46:33.020]   people are thinking about in terms of news.
[02:46:33.020 --> 02:46:37.380]   I personally am a big fan of a site called AllSides.com.
[02:46:37.380 --> 02:46:39.780]   And I don't know if you've ever talked about it on the show.
[02:46:39.780 --> 02:46:40.780]   No.
[02:46:40.780 --> 02:46:44.500]   But AllSides.com is a fantastic resource that takes every news source.
[02:46:44.500 --> 02:46:49.100]   I think they in their database, they have 1,000 and something news sources.
[02:46:49.100 --> 02:46:55.540]   And they have a crowd sourced rating about whether they're extreme left, moderately left
[02:46:55.540 --> 02:46:59.060]   in the center, moderately right or extreme right.
[02:46:59.060 --> 02:47:04.980]   And then they fold this against how the degree to which they fact check the content that they
[02:47:04.980 --> 02:47:06.220]   produce.
[02:47:06.220 --> 02:47:08.660]   And so you can really explore your news sources.
[02:47:08.660 --> 02:47:11.420]   And I think this is the biggest problem with news.
[02:47:11.420 --> 02:47:15.580]   People just take whatever news that comes to them over the social networks, which is
[02:47:15.580 --> 02:47:17.300]   a terrible way to consume news.
[02:47:17.300 --> 02:47:24.740]   You should be aware of the credibility and the bias or leaning of every news source that
[02:47:24.740 --> 02:47:26.740]   you consume.
[02:47:26.740 --> 02:47:31.980]   And AllSides.com is a great place to start thinking about that.
[02:47:31.980 --> 02:47:38.780]   And you can also contribute your input about whether CNN extreme laughter is it like slightly
[02:47:38.780 --> 02:47:43.300]   left of center or whatever, you can put in your two cents about that.
[02:47:43.300 --> 02:47:47.380]   But anyway, AllSides has been around forever.
[02:47:47.380 --> 02:47:51.820]   These three sites that I'm talking about, I think have emerged in the last week.
[02:47:51.820 --> 02:47:53.500]   So they're all very new.
[02:47:53.500 --> 02:48:01.140]   One thing, it's funny, for OneSub, I actually worked with Jim on his launch on product hunt.
[02:48:01.140 --> 02:48:02.140]   So fantastic.
[02:48:02.140 --> 02:48:03.140]   Fantastic.
[02:48:03.140 --> 02:48:04.140]   Fantastic.
[02:48:04.140 --> 02:48:07.420]   And I have subscribed to OneSub now.
[02:48:07.420 --> 02:48:11.660]   And now I get to choose some of the people I want to follow, I guess.
[02:48:11.660 --> 02:48:12.660]   People most mentioned.
[02:48:12.660 --> 02:48:13.900]   Not Elon Musk, please.
[02:48:13.900 --> 02:48:15.260]   No, or Nikki Haley.
[02:48:15.260 --> 02:48:18.580]   But maybe Jimmy Al Sharpton, maybe.
[02:48:18.580 --> 02:48:19.580]   And AllSides, I think it's great.
[02:48:19.580 --> 02:48:23.060]   Like, have you tried the Summarization feature in Artifact?
[02:48:23.060 --> 02:48:26.780]   No, I didn't know it had a Summarization feature.
[02:48:26.780 --> 02:48:28.260]   It's quite good.
[02:48:28.260 --> 02:48:32.740]   Oh, you need to tell them to make that more obvious.
[02:48:32.740 --> 02:48:36.380]   So they're doing a lot of changes and updates.
[02:48:36.380 --> 02:48:37.460]   They've been adding a lot more.
[02:48:37.460 --> 02:48:40.900]   You can now follow authors, which is kind of interesting.
[02:48:40.900 --> 02:48:44.620]   You can also leave comments in the comments actually are so far positive.
[02:48:44.620 --> 02:48:49.340]   But there's a little sparkly icon, I believe, when you go to a specific story.
[02:48:49.340 --> 02:48:50.340]   Oh, yeah, I see it.
[02:48:50.340 --> 02:48:51.340]   Yes, on top.
[02:48:51.340 --> 02:48:52.340]   It'll say Summarize.
[02:48:52.340 --> 02:48:53.340]   Oh, that's new.
[02:48:53.340 --> 02:48:54.340]   I like that.
[02:48:54.340 --> 02:48:55.340]   Oh.
[02:48:55.340 --> 02:48:56.340]   It's really good.
[02:48:56.340 --> 02:48:59.100]   And especially, you know, they don't, from what I can tell, they don't seem to strip
[02:48:59.100 --> 02:49:01.740]   out the ads because they want to be very publisher friendly.
[02:49:01.740 --> 02:49:04.980]   These are the founders of Instagram, of course, and so they want to support that.
[02:49:04.980 --> 02:49:07.540]   No, no, it's on the phone down here.
[02:49:07.540 --> 02:49:10.900]   I'm just telling our TD pick up money over the shoulder.
[02:49:10.900 --> 02:49:12.060]   There it is.
[02:49:12.060 --> 02:49:13.060]   There it is.
[02:49:13.060 --> 02:49:17.340]   So I did the summary of this Verge article and there is the summary up at the top.
[02:49:17.340 --> 02:49:18.340]   That's cool.
[02:49:18.340 --> 02:49:21.340]   Yep, so it pops up at the top and it's great because you can avoid all of the ads and all
[02:49:21.340 --> 02:49:22.340]   the other kind of like.
[02:49:22.340 --> 02:49:27.100]   And now actually in the latest version, they have the ability to report clickbait articles.
[02:49:27.100 --> 02:49:28.340]   They literally call it that.
[02:49:28.340 --> 02:49:29.340]   Oh, good.
[02:49:29.340 --> 02:49:30.340]   Cause there's a really big.
[02:49:30.340 --> 02:49:31.340]   So great about that.
[02:49:31.340 --> 02:49:33.100]   It's a love scene of the next.
[02:49:33.100 --> 02:49:34.100]   Yes.
[02:49:34.100 --> 02:49:39.620]   So if they sort of, you know, realign incentives so that the clickbait, like one reporting clickbait
[02:49:39.620 --> 02:49:42.660]   is useful and good because there's so much stuff out there that's just like, oh, this
[02:49:42.660 --> 02:49:46.500]   thing was set on Twitter and the whole thing is a tweet and just like content around it.
[02:49:46.500 --> 02:49:47.500]   Right.
[02:49:47.500 --> 02:49:49.540]   Second, you know, you can add the reader mode.
[02:49:49.540 --> 02:49:54.020]   So that cuts out a lot of the other garbage and stuff that pops up all the time and you
[02:49:54.020 --> 02:49:55.020]   can use summarization.
[02:49:55.020 --> 02:49:58.340]   And so you can quickly get the gist of an article and decide if you really want to dive in.
[02:49:58.340 --> 02:50:03.780]   So it's, it's becoming a pretty, you know, regular staple for me in terms of my news consumption.
[02:50:03.780 --> 02:50:04.780]   Awkward silence.
[02:50:04.780 --> 02:50:07.260]   Ronda Sennes, bold Twitter gambit, the flopped.
[02:50:07.260 --> 02:50:10.980]   The Florida governor wanted to show off his tech savvy by announcing his presidential
[02:50:10.980 --> 02:50:12.300]   campaign on Twitter.
[02:50:12.300 --> 02:50:15.340]   It quickly devolved into the conference call from hell.
[02:50:15.340 --> 02:50:16.340]   Yeah.
[02:50:16.340 --> 02:50:17.940]   That was the time summary.
[02:50:17.940 --> 02:50:21.140]   Actually, that was good.
[02:50:21.140 --> 02:50:23.060]   Conference call from hell.
[02:50:23.060 --> 02:50:30.900]   And this is concise, which is doing somewhat the same thing, just pairing down the articles
[02:50:30.900 --> 02:50:31.900]   using AI.
[02:50:31.900 --> 02:50:32.900]   We're going to see more and more of this.
[02:50:32.900 --> 02:50:37.740]   And I love the idea of an AI agent that knows what I'm interested in, kind of like
[02:50:37.740 --> 02:50:44.340]   a Google alerts or the old tracking on Twitter that can generate for me something of value.
[02:50:44.340 --> 02:50:47.140]   Ultimately, I'd love AI to do our whole show rundown.
[02:50:47.140 --> 02:50:52.180]   So I just show up and I say, hey, here's what AI chose for us today.
[02:50:53.180 --> 02:50:57.380]   Oh, so I guess the democracy and the rest of us don't.
[02:50:57.380 --> 02:50:58.380]   I get it.
[02:50:58.380 --> 02:50:59.380]   Jeff Jarvis.
[02:50:59.380 --> 02:51:01.980]   What's your pick of the week this week?
[02:51:01.980 --> 02:51:05.060]   Well, so God bless the Internet Archive.
[02:51:05.060 --> 02:51:06.060]   Absolutely.
[02:51:06.060 --> 02:51:07.060]   Absolutely.
[02:51:07.060 --> 02:51:12.540]   Scanned in USC's optical sound effects library.
[02:51:12.540 --> 02:51:13.540]   Wow.
[02:51:13.540 --> 02:51:14.540]   You can do it before it.
[02:51:14.540 --> 02:51:18.740]   So this little thing for audio here, if you go down on the left about 10 down, you'll
[02:51:18.740 --> 02:51:21.180]   see a crowd applause.
[02:51:21.180 --> 02:51:23.180]   Oh, that's what this is.
[02:51:23.180 --> 02:51:26.900]   I want to be able to have power to use this after I give us a little bit of a thinking
[02:51:26.900 --> 02:51:30.580]   we could instead we could use liquid and mud.
[02:51:30.580 --> 02:51:37.980]   That's that's an explosion.
[02:51:37.980 --> 02:51:40.500]   So these are real to real tapes.
[02:51:40.500 --> 02:51:41.500]   Let's see.
[02:51:41.500 --> 02:51:44.700]   How about a how about a fight?
[02:51:44.700 --> 02:51:47.100]   This might be good for moral panic.
[02:51:47.100 --> 02:51:48.100]   Yeah.
[02:51:48.100 --> 02:51:49.600]   Oh, these are pretty.
[02:51:49.600 --> 02:51:51.500]   These are pretty old.
[02:51:51.500 --> 02:51:53.800]   These are not well recorded.
[02:51:53.800 --> 02:51:55.900]   You can hear all the noise and hiss.
[02:51:55.900 --> 02:51:58.580]   It's like a 70s like movie, though.
[02:51:58.580 --> 02:51:59.580]   It would be so fun.
[02:51:59.580 --> 02:52:00.580]   Yeah, totally.
[02:52:00.580 --> 02:52:02.580]   You know, you know, totally.
[02:52:02.580 --> 02:52:04.920]   How about bells, horns and whistles?
[02:52:04.920 --> 02:52:10.760]   So this is a whole box and these are the different tapes in the box.
[02:52:10.760 --> 02:52:12.320]   This is hysterical.
[02:52:12.320 --> 02:52:14.480]   Let's see.
[02:52:14.480 --> 02:52:15.480]   Department store chimes.
[02:52:15.480 --> 02:52:16.480]   Oh, yeah.
[02:52:16.480 --> 02:52:17.480]   There you go.
[02:52:17.480 --> 02:52:20.280]   Yeah, when department store is existed.
[02:52:20.280 --> 02:52:21.280]   Exactly.
[02:52:21.280 --> 02:52:22.280]   And when they had chimes.
[02:52:22.280 --> 02:52:23.280]   Ollie Leo and I are old.
[02:52:23.280 --> 02:52:24.280]   I don't remember that.
[02:52:24.280 --> 02:52:25.280]   Yes.
[02:52:25.280 --> 02:52:27.080]   Hochoey on floor five.
[02:52:27.080 --> 02:52:28.720]   All right.
[02:52:28.720 --> 02:52:29.720]   This is fun.
[02:52:29.720 --> 02:52:30.720]   Look at this.
[02:52:30.720 --> 02:52:31.720]   And they have like the picture.
[02:52:31.720 --> 02:52:32.720]   Yeah, Jeremy.
[02:52:32.720 --> 02:52:34.400]   I think we all should have our sound boards out of this.
[02:52:34.400 --> 02:52:35.400]   Yeah.
[02:52:35.400 --> 02:52:36.560]   Here's some animal sounds.
[02:52:36.560 --> 02:52:37.560]   This is.
[02:52:37.560 --> 02:52:41.560]   Well, thanks to the Internet Archive for.
[02:52:41.560 --> 02:52:43.560]   Pig snorting.
[02:52:43.560 --> 02:52:45.160]   We got to have some pigs story.
[02:52:45.160 --> 02:52:46.160]   All right.
[02:52:46.160 --> 02:52:47.160]   Let's get some Apple users.
[02:52:47.160 --> 02:52:48.160]   Wait a minute.
[02:52:48.160 --> 02:52:49.560]   They had a slate on it.
[02:52:49.560 --> 02:52:51.560]   Some farmer.
[02:52:51.560 --> 02:52:58.480]   Is that from the Twitter space?
[02:52:58.480 --> 02:53:02.120]   Wow.
[02:53:02.120 --> 02:53:04.080]   The S.S.C.
[02:53:04.080 --> 02:53:05.080]   U.S.C.
[02:53:05.080 --> 02:53:09.680]   Optical sound effects library from the Sunset editorial connection collection.
[02:53:09.680 --> 02:53:11.400]   Oh, it's from the U.S.C.
[02:53:11.400 --> 02:53:12.400]   Cinema department.
[02:53:12.400 --> 02:53:13.400]   So you're right.
[02:53:13.400 --> 02:53:14.400]   These were old movies.
[02:53:14.400 --> 02:53:16.640]   The company was active from 64 to 87.
[02:53:16.640 --> 02:53:19.240]   They didn't episode TV shows like the Witch that I dream of.
[02:53:19.240 --> 02:53:20.880]   Oh my gosh.
[02:53:20.880 --> 02:53:25.080]   569 credits and IMDB.
[02:53:25.080 --> 02:53:26.320]   That's a great find.
[02:53:26.320 --> 02:53:27.320]   Thank you.
[02:53:27.320 --> 02:53:28.960]   Anything else you want to plug?
[02:53:28.960 --> 02:53:29.960]   No, that's enough.
[02:53:29.960 --> 02:53:30.960]   That's enough.
[02:53:30.960 --> 02:53:31.960]   Then let's go home.
[02:53:31.960 --> 02:53:32.960]   Plug, plug, of course.
[02:53:32.960 --> 02:53:33.960]   Wait, wait, wait, wait.
[02:53:33.960 --> 02:53:38.520]   The book is called The Gutenberg Parenthesis.
[02:53:38.520 --> 02:53:43.360]   And the website is gootenbergparenthesis.com.
[02:53:43.360 --> 02:53:45.000]   That's where you can go and order.
[02:53:45.000 --> 02:53:48.480]   At least it was ordering and she wanted to know where to order it from.
[02:53:48.480 --> 02:53:49.480]   What's up to you?
[02:53:49.480 --> 02:53:50.960]   The book is very 10% off.
[02:53:50.960 --> 02:53:56.440]   Blackwell's like out of London, but they shipped to America with even more higher positions.
[02:53:56.440 --> 02:53:59.000]   One of them made Jeff Jarvis more money than others.
[02:53:59.000 --> 02:54:00.440]   Oh, I don't make any money.
[02:54:00.440 --> 02:54:02.000]   I have stuff, but it's fine.
[02:54:02.000 --> 02:54:03.000]   Well, that's what I told.
[02:54:03.000 --> 02:54:06.600]   I said, "Order from Blackwell's because it's an independent bookstore in the UK and we want
[02:54:06.600 --> 02:54:07.600]   to..."
[02:54:07.600 --> 02:54:08.600]   And look, you save $3.
[02:54:08.600 --> 02:54:09.600]   And free shipping.
[02:54:09.600 --> 02:54:10.600]   And free shipping.
[02:54:10.600 --> 02:54:11.600]   That's not...
[02:54:11.600 --> 02:54:12.600]   Free shipping from the UK.
[02:54:12.600 --> 02:54:13.600]   Yeah.
[02:54:13.600 --> 02:54:18.000]   And actually, it's a great thing, Chris, because there are books that are only in the UK and
[02:54:18.000 --> 02:54:19.000]   I can only get there.
[02:54:19.000 --> 02:54:20.160]   That's how I discover them.
[02:54:20.160 --> 02:54:23.280]   So they won't ship from there, but they also sell American books.
[02:54:23.280 --> 02:54:24.280]   Got it.
[02:54:24.280 --> 02:54:25.280]   Got it.
[02:54:25.280 --> 02:54:26.280]   Nice.
[02:54:26.280 --> 02:54:30.040]   Or you could spend $27 and get it on Amazon for crayon.
[02:54:30.040 --> 02:54:33.680]   Well, you get the price guarantee that whatever they decide what it is.
[02:54:33.680 --> 02:54:34.680]   Ah, okay.
[02:54:34.680 --> 02:54:35.680]   It'll go down.
[02:54:35.680 --> 02:54:36.680]   Is there anything about the hashtag in this book?
[02:54:36.680 --> 02:54:39.120]   Do I need to get this book and assign copies?
[02:54:39.120 --> 02:54:40.120]   Yes.
[02:54:40.120 --> 02:54:41.120]   Did Gutenberg have hashtags?
[02:54:41.120 --> 02:54:42.120]   There is.
[02:54:42.120 --> 02:54:43.120]   Absolutely.
[02:54:43.120 --> 02:54:44.120]   What?
[02:54:44.120 --> 02:54:46.120]   I don't know if the index has hashtags.
[02:54:46.120 --> 02:54:47.920]   It's not the index.
[02:54:47.920 --> 02:54:50.720]   You need an AI generated index if it's not in there.
[02:54:50.720 --> 02:54:52.520]   No, it's not in there.
[02:54:52.520 --> 02:54:55.720]   Chris, you'll appreciate this speaking of hashtags.
[02:54:55.720 --> 02:55:00.720]   All throughout Europe, people who speak languages other than English as their main language.
[02:55:00.720 --> 02:55:02.600]   Does it have a hashtag?
[02:55:02.600 --> 02:55:08.120]   For example, if you go to the airport in France, an airport in France, you park and they say,
[02:55:08.120 --> 02:55:12.680]   okay, to get out of the parking garage, you have to do this forcode and then do a pound
[02:55:12.680 --> 02:55:14.680]   of sign that they call it a hashtag.
[02:55:14.680 --> 02:55:19.280]   They say press three, four, two, seven, hashtag to get out because nobody knows.
[02:55:19.280 --> 02:55:20.760]   You're in Europe.
[02:55:20.760 --> 02:55:23.080]   Wasn't the octothorp two?
[02:55:23.080 --> 02:55:24.320]   Wasn't that another?
[02:55:24.320 --> 02:55:25.320]   Yeah, the octothorp.
[02:55:25.320 --> 02:55:26.320]   Yeah.
[02:55:26.320 --> 02:55:28.320]   Chris, you are on page 192.
[02:55:28.320 --> 02:55:29.320]   Oh my God.
[02:55:29.320 --> 02:55:30.320]   Look at that.
[02:55:30.320 --> 02:55:31.320]   Look at that.
[02:55:31.320 --> 02:55:32.320]   Amazing.
[02:55:32.320 --> 02:55:33.320]   It's an amazing game.
[02:55:33.320 --> 02:55:34.320]   Well, read it to us.
[02:55:34.320 --> 02:55:35.320]   What does it say?
[02:55:35.320 --> 02:55:36.320]   Yeah, let me know.
[02:55:36.320 --> 02:55:37.320]   We're proposing 2007 by Christmas.
[02:55:37.320 --> 02:55:40.960]   You know, as a way to bring together groups on Twitter, it was adopted to mark movements
[02:55:40.960 --> 02:55:45.120]   to create call the response conversations, to protest, to joke, to organize otherwise
[02:55:45.120 --> 02:55:47.320]   diffuse information and discussion.
[02:55:47.320 --> 02:55:49.520]   Did AI write that?
[02:55:49.520 --> 02:55:50.920]   Hey, hey.
[02:55:50.920 --> 02:55:55.880]   Well, we meant one thing we're sure of Sean Penn did not write that.
[02:55:55.880 --> 02:55:56.880]   That's great.
[02:55:56.880 --> 02:55:57.880]   I can't wait.
[02:55:57.880 --> 02:55:59.640]   So it's not just Gutenberg.
[02:55:59.640 --> 02:56:04.000]   This book is about it's about our the lessons we have to learn from our entry into the age
[02:56:04.000 --> 02:56:05.080]   of print as we leave it.
[02:56:05.080 --> 02:56:11.440]   So the first half of the book is a loving history of print and its spread and impact
[02:56:11.440 --> 02:56:13.440]   all the way up to computers.
[02:56:13.440 --> 02:56:18.720]   And then the second half of the book is basically four essays about lessons like creativity
[02:56:18.720 --> 02:56:26.760]   versus control, conversation versus content, mass versus community and institutions and
[02:56:26.760 --> 02:56:27.760]   whether they should be updated.
[02:56:27.760 --> 02:56:28.760]   Wow.
[02:56:28.760 --> 02:56:29.760]   Looking forward to reading.
[02:56:29.760 --> 02:56:30.760]   Sounds good.
[02:56:30.760 --> 02:56:31.760]   Thank you.
[02:56:31.760 --> 02:56:32.760]   Yeah.
[02:56:32.760 --> 02:56:33.760]   Thank you.
[02:56:33.760 --> 02:56:39.640]   By the way, when I was in college, I studied in Switzerland for a semester and I actually
[02:56:39.640 --> 02:56:42.280]   worked at a print shop as a janitor.
[02:56:42.280 --> 02:56:43.280]   Oh, wow.
[02:56:43.280 --> 02:56:44.280]   Yeah.
[02:56:44.280 --> 02:56:45.800]   It was cold type by that.
[02:56:45.800 --> 02:56:46.800]   I would presume.
[02:56:46.800 --> 02:56:51.360]   I mean, while it was cold, it was, you know, yeah, I'm supposed to swim in.
[02:56:51.360 --> 02:56:52.360]   Yeah.
[02:56:52.360 --> 02:56:53.360]   But lead, yeah, there was a lead type.
[02:56:53.360 --> 02:56:54.360]   Wow.
[02:56:54.360 --> 02:56:57.720]   So you have to sweep up little slugs and put them in the last eight.
[02:56:57.720 --> 02:57:03.120]   I'm going to appear at the Museum of Printing in Haverhill, Mass.
[02:57:03.120 --> 02:57:05.000]   With our friend, Glenn Fleischman.
[02:57:05.000 --> 02:57:06.720]   Oh, he's done stuff there before.
[02:57:06.720 --> 02:57:07.720]   I think.
[02:57:07.720 --> 02:57:08.720]   Yeah.
[02:57:08.720 --> 02:57:12.640]   Doug Wilson who did the Lannon type book and Marcia Vickery, who did.
[02:57:12.640 --> 02:57:13.640]   Oh, nice.
[02:57:13.640 --> 02:57:14.640]   I'm back in the books.
[02:57:14.640 --> 02:57:15.640]   Oh, yeah.
[02:57:15.640 --> 02:57:17.280]   I'm looking forward to getting that.
[02:57:17.280 --> 02:57:18.280]   That's going to be amazing.
[02:57:18.280 --> 02:57:19.280]   Yeah.
[02:57:19.280 --> 02:57:20.280]   Yeah.
[02:57:20.280 --> 02:57:22.280]   July 8th shift happens meets Etowain Shirdlu.
[02:57:22.280 --> 02:57:25.280]   It's a real word.
[02:57:25.280 --> 02:57:28.120]   It's a word shaped blob.
[02:57:28.120 --> 02:57:29.120]   That's that's that.
[02:57:29.120 --> 02:57:30.120]   Oh, you don't know.
[02:57:30.120 --> 02:57:31.120]   Okay.
[02:57:31.120 --> 02:57:32.120]   Well, third time.
[02:57:32.120 --> 02:57:34.440]   Third word on the Lannon type was different.
[02:57:34.440 --> 02:57:38.440]   And so when they wanted to just clear a line, they would go down the keys and it would
[02:57:38.440 --> 02:57:41.280]   pop out of the line and the keys spelled Etowain Shirdlu.
[02:57:41.280 --> 02:57:43.160]   Wait a minute.
[02:57:43.160 --> 02:57:48.480]   I've always known Etowain Shirdlu as in order the most common letters in the English language.
[02:57:48.480 --> 02:57:51.760]   Well, that was the theory that that's why they got picked in that order, but that's
[02:57:51.760 --> 02:57:53.560]   not necessarily true.
[02:57:53.560 --> 02:57:54.560]   True.
[02:57:54.560 --> 02:57:55.560]   The case.
[02:57:55.560 --> 02:58:00.920]   Oh, that's going to hurt my wordle score because that's what I'm using to figure out
[02:58:00.920 --> 02:58:03.040]   what I should pick and wordle.
[02:58:03.040 --> 02:58:04.040]   You're kidding me.
[02:58:04.040 --> 02:58:09.320]   ETA, O-I-N-S-H-R-D-L-U are not the most frequent letters in order.
[02:58:09.320 --> 02:58:12.160]   They kind of are, but there's some variation on them.
[02:58:12.160 --> 02:58:13.160]   You should ask Chatchipity.
[02:58:13.160 --> 02:58:14.160]   How they do Chatchipity.
[02:58:14.160 --> 02:58:15.160]   I should.
[02:58:15.160 --> 02:58:17.320]   Should I ask Bart or Chatch EPT?
[02:58:17.320 --> 02:58:18.320]   Both.
[02:58:18.320 --> 02:58:19.320]   Okay.
[02:58:19.320 --> 02:58:20.320]   Let's ask Chatchipity.
[02:58:20.320 --> 02:58:21.320]   What's the question?
[02:58:21.320 --> 02:58:25.320]   What are in order of frequency?
[02:58:25.320 --> 02:58:28.000]   Of popularity or frequency of use.
[02:58:28.000 --> 02:58:29.000]   Of use.
[02:58:29.000 --> 02:58:31.680]   The top alphabet.
[02:58:31.680 --> 02:58:32.880]   Okay.
[02:58:32.880 --> 02:58:37.840]   Well, 26 letters in the English.
[02:58:37.840 --> 02:58:40.000]   I got to say English, right?
[02:58:40.000 --> 02:58:41.240]   Because it's going to be different.
[02:58:41.240 --> 02:58:42.240]   Yes, it's true.
[02:58:42.240 --> 02:58:43.240]   True.
[02:58:43.240 --> 02:58:44.920]   In the English, what?
[02:58:44.920 --> 02:58:47.080]   One more Z in the English.
[02:58:47.080 --> 02:58:48.560]   Herbs.
[02:58:48.560 --> 02:58:50.360]   Is English misspelled?
[02:58:50.360 --> 02:58:51.360]   Why is it?
[02:58:51.360 --> 02:58:52.360]   Oh, because it's not capitalized.
[02:58:52.360 --> 02:58:53.560]   It'll figure it out.
[02:58:53.560 --> 02:58:55.560]   In the English alphabet.
[02:58:55.560 --> 02:58:59.840]   Vocabulary.
[02:58:59.840 --> 02:59:00.840]   That's better.
[02:59:00.840 --> 02:59:01.840]   Yeah, yeah.
[02:59:01.840 --> 02:59:03.080]   I'm trying to think of a word.
[02:59:03.080 --> 02:59:06.880]   Actually, you want to know what it is in Chatchipity and your entire...
[02:59:06.880 --> 02:59:10.440]   Well, it only has its corpus to look at.
[02:59:10.440 --> 02:59:11.440]   What else would it look at?
[02:59:11.440 --> 02:59:12.440]   Here we go.
[02:59:12.440 --> 02:59:19.920]   Ladies and gentlemen, according to Chatchipity, ETA, O-I-N-S-H-R-D-L-U.
[02:59:19.920 --> 02:59:20.920]   Whoa.
[02:59:20.920 --> 02:59:21.920]   All right.
[02:59:21.920 --> 02:59:23.640]   Should I ask Bart the same thing?
[02:59:23.640 --> 02:59:28.080]   Because I think it's just confirmed when I have thought all along, irrespective.
[02:59:28.080 --> 02:59:30.680]   You're going to have to use this at your talk.
[02:59:30.680 --> 02:59:33.440]   I just want to point out here.
[02:59:33.440 --> 02:59:34.440]   Some good trivia.
[02:59:34.440 --> 02:59:36.680]   Let's see what Bart says.
[02:59:36.680 --> 02:59:38.600]   Bart comes up with something different.
[02:59:38.600 --> 02:59:39.600]   We're in trouble.
[02:59:39.600 --> 02:59:40.600]   Exactly.
[02:59:40.600 --> 02:59:45.600]   E-A-R-R-T-I-N-O-S-H-D-L-U.
[02:59:45.600 --> 02:59:53.160]   See, I think that the other one just went with common knowledge.
[02:59:53.160 --> 02:59:54.160]   It's got common knowledge.
[02:59:54.160 --> 02:59:55.160]   It's got common knowledge.
[02:59:55.160 --> 02:59:56.160]   It's got common knowledge.
[02:59:56.160 --> 03:00:00.440]   Google said this list is based on analysis of letters occurring in the words listed in
[03:00:00.440 --> 03:00:04.640]   the main entries of the concise Oxford dictionary.
[03:00:04.640 --> 03:00:05.640]   What tells you where it founded?
[03:00:05.640 --> 03:00:06.640]   Yeah.
[03:00:06.640 --> 03:00:07.640]   Let me see.
[03:00:07.640 --> 03:00:09.360]   Let me look at what OpenAI said.
[03:00:09.360 --> 03:00:10.360]   Maybe it...
[03:00:10.360 --> 03:00:11.560]   No citation.
[03:00:11.560 --> 03:00:13.560]   No citation.
[03:00:13.560 --> 03:00:14.560]   Huh.
[03:00:14.560 --> 03:00:18.800]   Well, now we've got a conflict.
[03:00:18.800 --> 03:00:21.120]   An AI battle.
[03:00:21.120 --> 03:00:25.000]   I mean, AI cage mashes are something I'm here for for sure.
[03:00:25.000 --> 03:00:26.560]   AI cage match.
[03:00:26.560 --> 03:00:30.680]   Oh, you know, Bart just pulled this from an article.
[03:00:30.680 --> 03:00:31.680]   That's why.
[03:00:31.680 --> 03:00:32.680]   That's why.
[03:00:32.680 --> 03:00:34.720]   Well, which one do I trust?
[03:00:34.720 --> 03:00:35.720]   The article, right?
[03:00:35.720 --> 03:00:36.720]   XS-P-D-F.com.
[03:00:36.720 --> 03:00:40.320]   Yeah, it's an article, huh?
[03:00:40.320 --> 03:00:44.160]   Because it cites it, whereas Chatsubic.
[03:00:44.160 --> 03:00:47.000]   You used Neva one last time, Leo.
[03:00:47.000 --> 03:00:48.000]   Just for the first time you said.
[03:00:48.000 --> 03:00:49.000]   Oh, man.
[03:00:49.000 --> 03:00:50.000]   I'm going to be so sad.
[03:00:50.000 --> 03:00:52.320]   It's going to be so sad.
[03:00:52.320 --> 03:00:57.600]   Tracker's begun.
[03:00:57.600 --> 03:01:00.600]   It says th-e-i-n-e-n-d-e-r-e-e-r.
[03:01:00.600 --> 03:01:02.400]   What the hell?
[03:01:02.400 --> 03:01:03.400]   All right.
[03:01:03.400 --> 03:01:06.280]   Well, I didn't say single letters.
[03:01:06.280 --> 03:01:07.280]   I didn't.
[03:01:07.280 --> 03:01:09.200]   I didn't.
[03:01:09.200 --> 03:01:10.200]   So it's just...
[03:01:10.200 --> 03:01:11.480]   The other one's figured it out.
[03:01:11.480 --> 03:01:13.280]   It's dip fongs or something.
[03:01:13.280 --> 03:01:15.520]   I don't know what it is.
[03:01:15.520 --> 03:01:17.840]   Thank you, Chris, for being here.
[03:01:17.840 --> 03:01:18.840]   Really appreciated.
[03:01:18.840 --> 03:01:19.840]   I hope you'll come back.
[03:01:19.840 --> 03:01:20.840]   I'm having you on.
[03:01:20.840 --> 03:01:23.120]   Yeah, it's just fantastic.
[03:01:23.120 --> 03:01:24.120]   Have a great day.
[03:01:24.120 --> 03:01:28.920]   Keep up the wonderful stuff you're doing and we'll watch product hunt tomorrow.
[03:01:28.920 --> 03:01:31.120]   Three hours of your life, we're grateful for the controversy.
[03:01:31.120 --> 03:01:32.120]   You shot the hell.
[03:01:32.120 --> 03:01:33.760]   You'll never get back.
[03:01:33.760 --> 03:01:34.760]   Yep.
[03:01:34.760 --> 03:01:36.560]   Jeff Jarvis, thank you so much.
[03:01:36.560 --> 03:01:37.560]   Well, Mike, what time is it there, Mike?
[03:01:37.560 --> 03:01:38.800]   Now, two in the morning.
[03:01:38.800 --> 03:01:39.800]   Jeez.
[03:01:39.800 --> 03:01:40.800]   Oh, God, I forgot about that.
[03:01:40.800 --> 03:01:41.800]   It's just two-e-teen.
[03:01:41.800 --> 03:01:42.800]   All right.
[03:01:42.800 --> 03:01:43.800]   It's all right.
[03:01:43.800 --> 03:01:44.800]   No, it's no big deal.
[03:01:44.800 --> 03:01:45.800]   No big deal.
[03:01:45.800 --> 03:01:49.600]   Jeff Jarvis is a Leonard Taill professor for journalistic innovation at the Craig New
[03:01:49.600 --> 03:01:55.920]   Mark graduate school journalism at the City University of New York.
[03:01:55.920 --> 03:02:01.760]   We are required to read that contractually every single time he's on Buzzmachine.com.
[03:02:01.760 --> 03:02:02.760]   Thank you, Jeff.
[03:02:02.760 --> 03:02:04.320]   Gutenberg parenthesis.com for the book.
[03:02:04.320 --> 03:02:06.760]   Craig wasn't pleased last time when you made-
[03:02:06.760 --> 03:02:09.560]   I know he sent me a nasty note about pigeons.
[03:02:09.560 --> 03:02:10.560]   Yeah.
[03:02:10.560 --> 03:02:11.560]   So don't get- yeah.
[03:02:11.560 --> 03:02:15.520]   I think he's- doesn't he- isn't he doing pigeons on Macedon or blue sky?
[03:02:15.520 --> 03:02:16.520]   He's doing pigeons everywhere.
[03:02:16.520 --> 03:02:19.360]   He has a pigeon rescue foundation.
[03:02:19.360 --> 03:02:20.360]   I know.
[03:02:20.360 --> 03:02:27.040]   But I said foolishly apparently that maybe it'd be nice if Craig put some more money into
[03:02:27.040 --> 03:02:28.040]   what was it-
[03:02:28.040 --> 03:02:31.080]   It was preserving cultural archives or something.
[03:02:31.080 --> 03:02:32.080]   Yeah.
[03:02:32.080 --> 03:02:33.080]   Instead of saving the pigeon.
[03:02:33.080 --> 03:02:35.480]   Those things were going to get dying off of YouTube.
[03:02:35.480 --> 03:02:36.480]   He's just upset you.
[03:02:36.480 --> 03:02:38.520]   Didn't like that at all.
[03:02:38.520 --> 03:02:39.520]   Craig listens.
[03:02:39.520 --> 03:02:40.520]   He knows.
[03:02:40.520 --> 03:02:41.520]   I'm sorry, Craig.
[03:02:41.520 --> 03:02:42.520]   You know I love you.
[03:02:42.520 --> 03:02:45.560]   And I love your pigeons.
[03:02:45.560 --> 03:02:46.560]   Let's see.
[03:02:46.560 --> 03:02:47.560]   What are you writing?
[03:02:47.560 --> 03:02:48.560]   He wrote-
[03:02:48.560 --> 03:02:50.960]   He sent a picture of an angry pigeon.
[03:02:50.960 --> 03:02:52.960]   No, he had to-
[03:02:52.960 --> 03:02:53.960]   And then-
[03:02:53.960 --> 03:02:56.040]   And then he sent a picture of Alfred Hitchcock.
[03:02:56.040 --> 03:02:57.040]   Oh, I sent that.
[03:02:57.040 --> 03:02:58.040]   Oh, okay good.
[03:02:58.040 --> 03:03:00.640]   I was like, wow, that's threatening.
[03:03:00.640 --> 03:03:01.800]   So this is what Craig said.
[03:03:01.800 --> 03:03:03.800]   Just a pigeon given me the evil eye.
[03:03:03.800 --> 03:03:04.800]   The header.
[03:03:04.800 --> 03:03:07.000]   I can't remember what the topic line was.
[03:03:07.000 --> 03:03:09.200]   It just said, "Watch out, Leo."
[03:03:09.200 --> 03:03:11.520]   Oh no, you said that.
[03:03:11.520 --> 03:03:12.520]   Yeah.
[03:03:12.520 --> 03:03:13.520]   He sent it.
[03:03:13.520 --> 03:03:15.760]   GFK would like a word.
[03:03:15.760 --> 03:03:16.760]   What is GFK?
[03:03:16.760 --> 03:03:18.760]   It's the nickname of the bird.
[03:03:18.760 --> 03:03:20.200]   I can't remember what it says.
[03:03:20.200 --> 03:03:21.200]   Oh, okay.
[03:03:21.200 --> 03:03:22.200]   I should know that.
[03:03:22.200 --> 03:03:23.840]   Ghost-faced killer would like a word.
[03:03:23.840 --> 03:03:24.840]   That's it.
[03:03:24.840 --> 03:03:26.240]   Ghost-faced killer, that's right.
[03:03:26.240 --> 03:03:28.440]   That's more of a threat than you, your thing.
[03:03:28.440 --> 03:03:30.080]   Yeah, yeah, I did.
[03:03:30.080 --> 03:03:34.040]   Mr. Algen, thank you so much for staying up late with us in the Veneto.
[03:03:34.040 --> 03:03:35.040]   I appreciate it.
[03:03:35.040 --> 03:03:36.840]   It's my pleasure.
[03:03:36.840 --> 03:03:39.240]   Leo, can I do a 30-second pitch?
[03:03:39.240 --> 03:03:40.240]   Plug.
[03:03:40.240 --> 03:03:41.240]   Plug.
[03:03:41.240 --> 03:03:42.240]   Kevin's thing, man.
[03:03:42.240 --> 03:03:44.480]   Cause we talked about these things.
[03:03:44.480 --> 03:03:45.480]   Hello.
[03:03:45.480 --> 03:03:49.240]   Hello, Chadderbox.com is the thing.
[03:03:49.240 --> 03:03:50.240]   It's called Chatterbox.
[03:03:50.240 --> 03:03:56.080]   It's an educational product where kids build a smart speaker, kind of like an Amazon Echo
[03:03:56.080 --> 03:03:57.440]   or something like that.
[03:03:57.440 --> 03:03:59.880]   And then they teach it how to talk and interact.
[03:03:59.880 --> 03:04:04.680]   It's the only educational product that I know of that uses both chat, GBT and also stable
[03:04:04.680 --> 03:04:05.680]   diffusion.
[03:04:05.680 --> 03:04:06.680]   It's...
[03:04:06.680 --> 03:04:07.680]   That's new.
[03:04:07.680 --> 03:04:08.680]   That's cool.
[03:04:08.680 --> 03:04:09.680]   Kids are protected.
[03:04:09.680 --> 03:04:10.680]   Yes, it is.
[03:04:10.680 --> 03:04:15.440]   Kids are protected from objectionable content and so on like that through the chat.
[03:04:15.440 --> 03:04:17.920]   It's a chat box system.
[03:04:17.920 --> 03:04:21.520]   But the most important thing is that teaches kids how AI works.
[03:04:21.520 --> 03:04:25.520]   It demystifies AI and it gets them to use it as a tool.
[03:04:25.520 --> 03:04:31.920]   And what they end up with is actually a computer that they can use that has no screen that
[03:04:31.920 --> 03:04:33.360]   can do anything they wanted to do.
[03:04:33.360 --> 03:04:34.360]   It can turn on lights.
[03:04:34.360 --> 03:04:36.360]   It can do a million things.
[03:04:36.360 --> 03:04:37.360]   They can customize it.
[03:04:37.360 --> 03:04:42.320]   And this is great for schools and educational environments, whether it's homeschooling or
[03:04:42.320 --> 03:04:43.320]   whatever.
[03:04:43.320 --> 03:04:45.840]   And also individuals can buy it and build it themselves.
[03:04:45.840 --> 03:04:51.400]   I think this is a really great product for kids to learn about AI, especially now that
[03:04:51.400 --> 03:04:53.080]   everybody's talking about AI.
[03:04:53.080 --> 03:04:57.840]   Kids are having AI right there as a and stuff like that and nobody really understands it.
[03:04:57.840 --> 03:05:02.640]   So this takes kids at the age of around eight years old and up and really teaches them what
[03:05:02.640 --> 03:05:09.960]   AI is, how it works and how it can be used for educational purposes instead of the opposite.
[03:05:09.960 --> 03:05:10.960]   Really cool.
[03:05:10.960 --> 03:05:13.040]   Are you connected to the founders of the makers?
[03:05:13.040 --> 03:05:14.480]   I mean, this is the kind of thing I'd love to get on product.
[03:05:14.480 --> 03:05:16.640]   This is my son, my son Kevin in Benjio.
[03:05:16.640 --> 03:05:18.040]   Oh, buddy, you should ask.
[03:05:18.040 --> 03:05:19.040]   There we go.
[03:05:19.040 --> 03:05:21.640]   Well, have them, you know, reach out and I'll get on product for you.
[03:05:21.640 --> 03:05:22.640]   Wonderful.
[03:05:22.640 --> 03:05:23.640]   Yes.
[03:05:23.640 --> 03:05:24.640]   That would be great.
[03:05:24.640 --> 03:05:25.640]   Absolutely.
[03:05:25.640 --> 03:05:26.640]   Absolutely.
[03:05:26.640 --> 03:05:27.640]   Yeah.
[03:05:27.640 --> 03:05:28.640]   Good.
[03:05:28.640 --> 03:05:29.640]   Making connections.
[03:05:29.640 --> 03:05:30.640]   Look at that.
[03:05:30.640 --> 03:05:31.640]   Good.
[03:05:31.640 --> 03:05:32.640]   Look at that.
[03:05:32.640 --> 03:05:33.640]   Who needs AI?
[03:05:33.640 --> 03:05:34.640]   Another twig miracle.
[03:05:34.640 --> 03:05:35.640]   Great.
[03:05:35.640 --> 03:05:36.640]   Thank you so much, everybody.
[03:05:36.640 --> 03:05:37.940]   Thank you all for being here for joining us for being a part of the conversation.
[03:05:37.940 --> 03:05:38.940]   We do twig.
[03:05:38.940 --> 03:05:42.920]   I think one of the most interesting shows in the world, frankly, and I hope you will tune
[03:05:42.920 --> 03:05:45.080]   in and tell your friends we do it.
[03:05:45.080 --> 03:05:50.000]   You can watch it live if you want every Wednesday, 2 p.m. Pacific 5 p.m. Eastern 2100
[03:05:50.000 --> 03:05:55.120]   UTC, but you can also listen after the fact of twit.tv/twig.
[03:05:55.120 --> 03:05:58.600]   If you want to send a clip to your friends, use our YouTube channel.
[03:05:58.600 --> 03:06:04.480]   If you go to twit.tv/twig, there's a link right there or just look for youtube.com/thisweekengoogol.
[03:06:04.480 --> 03:06:08.200]   And it's very easy to send clips and since everybody has access to YouTube, that seems
[03:06:08.200 --> 03:06:09.200]   like a good way to do it.
[03:06:09.200 --> 03:06:12.520]   Of course, the best way for you to get the show would be to subscribe in your favorite
[03:06:12.520 --> 03:06:14.720]   podcast player because that way you'll get it automatically.
[03:06:14.720 --> 03:06:15.720]   You don't have to remember.
[03:06:15.720 --> 03:06:17.720]   You just go, "Oh, I know I must have a new twig."
[03:06:17.720 --> 03:06:18.720]   It's here.
[03:06:18.720 --> 03:06:19.720]   It's here.
[03:06:19.720 --> 03:06:21.840]   We hope you will come back next week.
[03:06:21.840 --> 03:06:23.040]   We thank you for being here.
[03:06:23.040 --> 03:06:25.720]   We will see you next time on this week in Google.
[03:06:25.720 --> 03:06:27.720]   Bye bye everyone.
[03:06:27.720 --> 03:06:28.720]   Ciao.
[03:06:28.720 --> 03:06:29.720]   Ciao.
[03:06:29.720 --> 03:06:30.720]   Ciao.
[03:06:30.720 --> 03:06:31.720]   Ciao.
[03:06:31.720 --> 03:06:35.480]   Listeners of this program get an ad-free version.
[03:06:35.480 --> 03:06:41.440]   If they're members of club twit, $7 a month gives you ad-free versions of all of our shows.
[03:06:41.440 --> 03:06:46.520]   Plus, membership in the club twit discord a great clubhouse for twit listeners.
[03:06:46.520 --> 03:06:51.640]   And finally, the Twit Plus Feed with shows like Stacey's Book Club, The Untitled Linux
[03:06:51.640 --> 03:06:53.920]   Show, The Giz Fizz and more.
[03:06:53.920 --> 03:06:57.360]   Go to twit.tv/clubtwit and thanks for your support.
[03:06:57.360 --> 03:07:04.360]   [Music]
[03:07:04.360 --> 03:07:10.360]   [Music]

