;FFMETADATA1
title=Bring Me a Leaf Blower
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-02-02
track=701
language=English
genre=Podcast
comment=ChatGPT and Bing, Samsung Unpacked 2023, Clear Calling demo, Artifact
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100

[00:00:00.000 --> 00:00:07.520]   It's time for Twig this week in Google, Jeff Stacey and Ant are all here. We'll talk about chat GPT coming to Bing.
[00:00:07.520 --> 00:00:18.800]   Google's response, bring back Larry and Sergey. We'll also talk about Buzzfeed using chat GPT to generate content, the endless Seinfeld stream,
[00:00:18.800 --> 00:00:25.400]   and Is Elon the right guy to run Twitter? At least one founder says, "Yeah, I don't think so." It's all coming up next.
[00:00:25.400 --> 00:00:35.400]   But Twig. Podcasts you love. From people you trust. This is Twig.
[00:00:35.400 --> 00:00:50.400]   This is Twig. This week in Google, Episode 701, recorded Wednesday, February 1st, 2023. Bring me a leaf blower.
[00:00:50.400 --> 00:01:00.400]   This week in Google is brought to you by Miro. Miro is your team's visual platform to connect, collaborate and create together.
[00:01:00.400 --> 00:01:13.400]   Tap into a way to map processes, systems, and plans with the whole team. Get your first three boards for free to start creating your best work yet at Miro.com/podcast.
[00:01:13.400 --> 00:01:23.400]   And by Ate Sleep. Good sleep is the ultimate game changer and the pod cover is the ultimate sleep machine. Go to AteSleep.com/Twig.
[00:01:23.400 --> 00:01:33.400]   To check out the pod cover and save $150 at checkout, Ate Sleep currently ships within the US, Canada, the UK, and select countries in the EU, plus Australia.
[00:01:33.400 --> 00:01:42.400]   Thanks for listening to this show. As an ad supported network, we are always looking for new partners with products and services that will benefit our customers.
[00:01:42.400 --> 00:01:52.400]   We will benefit our qualified audience. Are you ready to grow your business? Reach out to advertise at Twit.tv and launch your campaign now.
[00:01:52.400 --> 00:02:03.400]   It's time for Twig this week in Google. The show we covered the latest news from Google. They Google versus the Twitter, the Facebook, the inverse and the obverse and the teatour.
[00:02:03.400 --> 00:02:16.400]   It's Stacy Higginbotham from the IOTverse. Hello, Stacy. Hello, y'all. Stacy on IOT.com, the IOT podcast. Good to have you here.
[00:02:16.400 --> 00:02:28.400]   Jeff Jarvis is also here. He is the Leonard Taub professor for journalistic innovation at the Craig, Craig Newmark graduate school journalism at the City University of New York.
[00:02:28.400 --> 00:02:35.400]   Hello, Jeff. Hello, hello. What kind of song were you going to commission? I can't remember what you said. We did.
[00:02:35.400 --> 00:02:45.400]   Oh, yeah. We wanted to see Shandis. See, sir. Yeah. The one was a professor named. I don't know.
[00:02:45.400 --> 00:02:56.400]   We have to ask chat GPT to write it. Also here. You know, it's weird. We watched before the show began, just about five minutes of that weird,
[00:02:56.400 --> 00:03:00.520]   "The Rain-Pelt Non-Stop Show about Nothing on Twitch."
[00:03:00.520 --> 00:03:02.840]   Created by an AI.
[00:03:02.840 --> 00:03:05.380]   And an interesting thing happened to my brain,
[00:03:05.380 --> 00:03:10.100]   I'm a little disoriented, like VR disoriented.
[00:03:10.100 --> 00:03:10.780]   (laughing)
[00:03:10.780 --> 00:03:13.580]   Nothing seems real.
[00:03:13.580 --> 00:03:17.040]   And Pruitt's also here for Hands-On Photography.
[00:03:17.040 --> 00:03:19.380]   Twit.tv/hop, do you not feel that, Ant?
[00:03:19.380 --> 00:03:24.000]   - Sir no, what I feel is I think
[00:03:24.000 --> 00:03:27.160]   we are all safe as content creators to competition.
[00:03:27.160 --> 00:03:28.840]   - Well, I feel like it could have been better.
[00:03:28.840 --> 00:03:30.240]   I do, I feel like it could have been better.
[00:03:30.240 --> 00:03:32.160]   - You want to show a little bit of it or not?
[00:03:32.160 --> 00:03:33.000]   Should I?
[00:03:33.000 --> 00:03:33.840]   - No.
[00:03:33.840 --> 00:03:34.680]   - Yeah, just a little bit.
[00:03:34.680 --> 00:03:37.240]   - Stacy says we've wasted enough time.
[00:03:37.240 --> 00:03:38.920]   - Oh yeah. - Nope, nope, go ahead.
[00:03:38.920 --> 00:03:40.000]   - We didn't do it on--
[00:03:40.000 --> 00:03:41.480]   - This is at least for the show, yeah.
[00:03:41.480 --> 00:03:43.160]   - I think it's worth the discussion.
[00:03:43.160 --> 00:03:45.280]   It's so bad I won't--
[00:03:45.280 --> 00:03:48.800]   - So again, it's AI is writing an endless sign film.
[00:03:48.800 --> 00:03:50.660]   It's not called "Sign Films" called, like,
[00:03:50.660 --> 00:03:52.120]   watch me forever. - Nothing forever.
[00:03:52.120 --> 00:03:53.600]   - Yeah, nothing forever.
[00:03:53.600 --> 00:03:54.800]   But, but, but, but, you know, I thought,
[00:03:54.800 --> 00:03:56.240]   oh, this would be, this is a good idea.
[00:03:56.240 --> 00:03:58.760]   Right, 'cause "Sign Films" has structured gags.
[00:03:58.760 --> 00:04:01.080]   You can figure it out, but you'll see.
[00:04:01.080 --> 00:04:04.120]   - We're getting a Samsung ad for their new phone,
[00:04:04.120 --> 00:04:06.320]   which we saw this morning, thanks to Jason,
[00:04:06.320 --> 00:04:08.160]   how all of the Samsung had packed.
[00:04:08.160 --> 00:04:12.120]   I decided not to go because everything had been leaked already.
[00:04:12.120 --> 00:04:13.720]   There was nothing to say about the new phone.
[00:04:13.720 --> 00:04:16.240]   I thought, well, this is just a PEP rally now.
[00:04:16.240 --> 00:04:18.240]   There's not a reveal. - Ooh.
[00:04:18.240 --> 00:04:21.640]   - They needed a PEP rally after those earnings.
[00:04:21.640 --> 00:04:25.120]   - Yeah, all of the tech industry just--
[00:04:25.120 --> 00:04:26.640]   Here we are in--
[00:04:26.640 --> 00:04:27.800]   - Throw it in the garbage can.
[00:04:27.800 --> 00:04:29.320]   - In "Sign Films" home.
[00:04:29.320 --> 00:04:30.160]   - Trash can.
[00:04:30.160 --> 00:04:32.600]   Holy cow.
[00:04:32.600 --> 00:04:33.440]   - Look at that gate.
[00:04:33.440 --> 00:04:35.400]   - We've done something.
[00:04:35.400 --> 00:04:37.240]   Well, no.
[00:04:37.240 --> 00:04:39.720]   I didn't want to get involved in this situation.
[00:04:39.720 --> 00:04:42.280]   I don't know what to make of that.
[00:04:42.280 --> 00:04:43.360]   (laughing)
[00:04:43.360 --> 00:04:44.640]   - Picking up up in the left field
[00:04:44.640 --> 00:04:45.480]   and walking away with a--
[00:04:45.480 --> 00:04:47.160]   - At that, I'm gonna sit on the couch.
[00:04:47.160 --> 00:04:48.960]   - What guy's she sitting on a stool?
[00:04:49.960 --> 00:04:52.240]   You know what that guy was up to?
[00:04:52.240 --> 00:04:54.840]   He was probably trying to get some free garbage.
[00:04:54.840 --> 00:04:57.840]   You know what they say, "One man's trash"
[00:04:57.840 --> 00:04:59.240]   is another man's treasure.
[00:04:59.240 --> 00:05:02.600]   - This is so bad.
[00:05:02.600 --> 00:05:03.440]   - It's awful.
[00:05:03.440 --> 00:05:04.360]   - It's so bad.
[00:05:04.360 --> 00:05:05.240]   But it could be so--
[00:05:05.240 --> 00:05:06.480]   - It's like, "Fun thing is,"
[00:05:06.480 --> 00:05:07.800]   I don't think they're trying
[00:05:07.800 --> 00:05:10.520]   because they could use real "Signfell" voices.
[00:05:10.520 --> 00:05:13.080]   We certainly have the capability of doing that.
[00:05:13.080 --> 00:05:14.040]   We've come so far.
[00:05:14.040 --> 00:05:16.280]   This is like what we thought,
[00:05:16.280 --> 00:05:18.720]   what AI was in 1986, AI.
[00:05:18.720 --> 00:05:23.720]   - Yeah, it's like, "Me, while there's some real AI going on."
[00:05:23.720 --> 00:05:25.200]   - Well, hold on though.
[00:05:25.200 --> 00:05:28.080]   If you think about it, that's performing like,
[00:05:28.080 --> 00:05:29.320]   I don't know, 4D AI.
[00:05:29.320 --> 00:05:31.080]   They're creating the content,
[00:05:31.080 --> 00:05:33.560]   they're generating the imagery.
[00:05:33.560 --> 00:05:37.880]   They're also doing some vocal characterizations.
[00:05:37.880 --> 00:05:40.720]   So maybe a lesson here is if AI's good
[00:05:40.720 --> 00:05:43.200]   at doing one-dimensional AI,
[00:05:43.200 --> 00:05:45.840]   but when you move it to all the dimensions, it sucks.
[00:05:45.840 --> 00:05:49.600]   - I think it could be actually a lot better effect.
[00:05:49.600 --> 00:05:53.520]   There's a, this is, we are in the AI explosion,
[00:05:53.520 --> 00:05:56.320]   you know, right now, the Cambrian explosion of AI.
[00:05:56.320 --> 00:06:02.480]   Rumor today from both Bloomberg and the information
[00:06:02.480 --> 00:06:06.280]   that chat GPT is about to be in the new version.
[00:06:06.280 --> 00:06:10.160]   Chat GPT 4 is about to be integrated
[00:06:10.160 --> 00:06:13.360]   into Microsoft's Bing search.
[00:06:13.360 --> 00:06:14.960]   And this will happen in the next--
[00:06:14.960 --> 00:06:17.680]   - Yeah, and in the next few weeks,
[00:06:17.680 --> 00:06:19.080]   Microsoft, we've mentioned before,
[00:06:19.080 --> 00:06:20.600]   is a big investor in OpenAI,
[00:06:20.600 --> 00:06:22.480]   the creators of chat GPT.
[00:06:22.480 --> 00:06:25.680]   And they just put in another four,
[00:06:25.680 --> 00:06:27.440]   I'm sorry, $10 billion.
[00:06:27.440 --> 00:06:32.200]   And, you know, initially the deal was to get chat GPT
[00:06:32.200 --> 00:06:36.560]   in office, but now the news is,
[00:06:36.560 --> 00:06:39.320]   we're gonna start seeing chat GPT in Bing.
[00:06:39.320 --> 00:06:42.400]   And it won't be the chat GPT we've,
[00:06:42.400 --> 00:06:45.520]   we're used to, it's the new chat GPT.
[00:06:45.520 --> 00:06:47.360]   And this actually addresses your concerns, Stacy,
[00:06:47.360 --> 00:06:51.160]   because this thing is a monster Microsoft
[00:06:51.160 --> 00:06:54.040]   in order to do this, built a new supercomputer,
[00:06:54.040 --> 00:06:57.160]   a massive supercomputer.
[00:06:57.160 --> 00:07:01.200]   Let me see if I can find the stats for it.
[00:07:01.200 --> 00:07:06.200]   Huge amount of storage, huge amount of processors.
[00:07:06.200 --> 00:07:10.360]   - Yeah, it's curious to hear to compute
[00:07:10.360 --> 00:07:11.760]   the processing power.
[00:07:11.760 --> 00:07:13.840]   - Yeah, this is Reed Albergotti writing,
[00:07:13.840 --> 00:07:16.480]   you know Reed didn't he write for Gagong?
[00:07:16.480 --> 00:07:17.320]   I feel like he did.
[00:07:17.320 --> 00:07:18.720]   - He didn't, he used to write for the information.
[00:07:18.720 --> 00:07:20.640]   - Information, okay, he's now at Semaphore,
[00:07:20.640 --> 00:07:22.280]   which is a new publication.
[00:07:22.280 --> 00:07:26.880]   So I think he had the stats, let me see here.
[00:07:26.880 --> 00:07:29.320]   In May 2020, a couple of years ago,
[00:07:29.320 --> 00:07:32.080]   Microsoft says he built one of the top five
[00:07:32.080 --> 00:07:34.280]   publicly disclosed supercomputers in the world
[00:07:34.280 --> 00:07:39.280]   in partnership with and exclusively for OpenAI,
[00:07:39.760 --> 00:07:41.760]   the computer was going to be used to train
[00:07:41.760 --> 00:07:44.260]   extremely large AI models,
[00:07:44.260 --> 00:07:48.600]   285,000 CPUs, 10,000 GPUs,
[00:07:48.600 --> 00:07:53.120]   four gigabits per second of network connectivity.
[00:07:53.120 --> 00:07:55.840]   But, and this is to answer what you were saying about,
[00:07:55.840 --> 00:07:58.040]   well, there was a lot to do to write the script
[00:07:58.040 --> 00:08:00.760]   and do the voices, oh, that's the whole point of this,
[00:08:00.760 --> 00:08:02.600]   is that you could do all this stuff in real time.
[00:08:02.600 --> 00:08:07.360]   And not only that, unlike the current chat GPT-3,
[00:08:07.360 --> 00:08:09.520]   you could be constantly updated.
[00:08:09.920 --> 00:08:12.120]   And that's what makes it useful in search terms.
[00:08:12.120 --> 00:08:13.280]   - Aha.
[00:08:13.280 --> 00:08:15.320]   - Not only will it be constantly spidering the web
[00:08:15.320 --> 00:08:18.760]   with for new information, but every query goes into it.
[00:08:18.760 --> 00:08:21.120]   Every interaction with you goes into it.
[00:08:21.120 --> 00:08:23.240]   - But will it be sensitive to fact,
[00:08:23.240 --> 00:08:24.320]   as we talked about last week?
[00:08:24.320 --> 00:08:26.240]   - And that's obviously a big issue, 'cause--
[00:08:26.240 --> 00:08:27.680]   - Well, how do you say that so quickly?
[00:08:27.680 --> 00:08:29.680]   'Cause now it's just as a word predictor,
[00:08:29.680 --> 00:08:32.880]   how do we know that it will no fact for one fact?
[00:08:32.880 --> 00:08:36.440]   - Well, I mean, obviously it's a mistake
[00:08:36.440 --> 00:08:39.000]   to put an AI into search if it doesn't actually
[00:08:39.000 --> 00:08:41.840]   give you accurate factual information,
[00:08:41.840 --> 00:08:44.000]   but I think it will, I think that's one of the things
[00:08:44.000 --> 00:08:47.760]   chat GPT will do, but according to read,
[00:08:47.760 --> 00:08:52.760]   every chat GPT answer now has a thumbs up and thumbs down.
[00:08:52.760 --> 00:08:54.680]   You can write the ideal answer,
[00:08:54.680 --> 00:08:57.800]   and so this will all be incorporated into chat GPT-4
[00:08:57.800 --> 00:09:00.760]   and Bing, so that we'll have feedback.
[00:09:00.760 --> 00:09:03.200]   I've seen, shown you several times,
[00:09:03.200 --> 00:09:06.120]   it's Neva chat, search engine, I guess.
[00:09:06.120 --> 00:09:08.280]   And it makes itself accurate
[00:09:08.280 --> 00:09:12.160]   because it is only summarizing published information
[00:09:12.160 --> 00:09:14.680]   from reliable sources, and I presume that's what they would do
[00:09:14.680 --> 00:09:16.120]   with Bing, right?
[00:09:16.120 --> 00:09:19.200]   So you're not just making up stuff at a whole cloth,
[00:09:19.200 --> 00:09:21.960]   but you're saying, well, Wikipedia, Microsoft,
[00:09:21.960 --> 00:09:24.480]   and you know, semaphore say this
[00:09:24.480 --> 00:09:25.680]   with footnotes and everything,
[00:09:25.680 --> 00:09:27.320]   I think that's gonna be possibly--
[00:09:27.320 --> 00:09:29.720]   - So we just asked chat GPT to tell a joke
[00:09:29.720 --> 00:09:32.920]   in the style, about AI in the style of Jerry Seinfeld?
[00:09:32.920 --> 00:09:33.760]   - Yeah.
[00:09:33.760 --> 00:09:36.760]   - I'm not so critical of the show now.
[00:09:37.920 --> 00:09:41.160]   The answer was, why did the AI cross the road
[00:09:41.160 --> 00:09:43.680]   to get to the other side of the data set?
[00:09:43.680 --> 00:09:45.480]   - Yeah, nevermind.
[00:09:45.480 --> 00:09:47.680]   - I said, you could do better than that,
[00:09:47.680 --> 00:09:49.280]   and it said, okay, here's another one.
[00:09:49.280 --> 00:09:51.160]   Why did the AI refuse to do the dishes?
[00:09:51.160 --> 00:09:55.120]   Because it was afraid of doing a deep learning experience.
[00:09:55.120 --> 00:09:57.040]   - Yeah, no, this is just chat deep.
[00:09:57.040 --> 00:09:59.240]   That's a toy, what you're using is a toy.
[00:09:59.240 --> 00:10:00.360]   I think-- - I know, I know,
[00:10:00.360 --> 00:10:01.920]   but it's a-- - I think what we're talking about
[00:10:01.920 --> 00:10:04.960]   here is it's gonna be a different thing entirely.
[00:10:04.960 --> 00:10:06.920]   I'm hoping it will be, obviously won't.
[00:10:06.920 --> 00:10:10.920]   And this is what's scaring the hell out of Google right now.
[00:10:10.920 --> 00:10:15.120]   In fact, the side by side story is Google's testing
[00:10:15.120 --> 00:10:20.040]   a chat GPT like chat bot called Apprentice Bard,
[00:10:20.040 --> 00:10:22.880]   which is not a great name.
[00:10:22.880 --> 00:10:25.680]   - Well, it's a code name, not a brand name.
[00:10:25.680 --> 00:10:28.320]   - Okay, it's using Lambda, which that's the one
[00:10:28.320 --> 00:10:30.840]   that Brent Lemoine said was sentient.
[00:10:30.840 --> 00:10:35.440]   They're also testing, this is according to Jennifer Elias
[00:10:35.440 --> 00:10:38.400]   for CNBC, new search page designs
[00:10:38.400 --> 00:10:39.880]   that integrate the chat technology.
[00:10:39.880 --> 00:10:42.280]   This is gonna happen very quickly.
[00:10:42.280 --> 00:10:44.680]   And more employees have been asked to test this internally.
[00:10:44.680 --> 00:10:48.120]   That's probably how she's getting the story.
[00:10:48.120 --> 00:10:50.040]   - Legene. - Yes.
[00:10:50.040 --> 00:10:51.800]   The Alphabet Company's working on a project
[00:10:51.800 --> 00:10:54.000]   under its cloud unit called Atlas,
[00:10:54.000 --> 00:10:59.000]   which is a code red effort to respond to chat GPT.
[00:10:59.000 --> 00:11:01.560]   There's also Apprentice Bard, so that's another one.
[00:11:01.560 --> 00:11:04.440]   Their employees can ask questions and get detailed answers.
[00:11:05.440 --> 00:11:10.160]   I also heard that Google has invited Larry and Sergey back
[00:11:10.160 --> 00:11:11.520]   to help them with this.
[00:11:11.520 --> 00:11:17.080]   - And Sergey just put in his first code in years.
[00:11:17.080 --> 00:11:18.440]   I put in that in the rundown.
[00:11:18.440 --> 00:11:20.280]   - It's fascinating.
[00:11:20.280 --> 00:11:21.120]   - First code request.
[00:11:21.120 --> 00:11:23.080]   What does that mean by the way, a code request?
[00:11:23.080 --> 00:11:26.120]   - I request her a commit.
[00:11:26.120 --> 00:11:31.440]   - It said just filed his first code request in years.
[00:11:31.440 --> 00:11:33.840]   - That sounds like a mistranslation of what he did.
[00:11:33.840 --> 00:11:35.040]   - Sure does.
[00:11:35.040 --> 00:11:36.120]   - Yeah, that's not sure.
[00:11:36.120 --> 00:11:37.920]   - File his first request in access.
[00:11:37.920 --> 00:11:40.960]   Oh, request to access code.
[00:11:40.960 --> 00:11:42.960]   - Oh, I see. - So he wanted to see--
[00:11:42.960 --> 00:11:43.840]   - He's already-- - He wanted,
[00:11:43.840 --> 00:11:45.080]   'cause this stuff's not on GitHub.
[00:11:45.080 --> 00:11:46.240]   You have to actually say,
[00:11:46.240 --> 00:11:48.960]   hey, can I look at that Lambda code?
[00:11:48.960 --> 00:11:50.880]   Two sources said, this is Forbes,
[00:11:50.880 --> 00:11:54.240]   I should always say who's writing, Forbes staff.
[00:11:54.240 --> 00:11:55.480]   - The staff members, that's right.
[00:11:55.480 --> 00:11:58.040]   - Okay, Richard and Eva and Alex Conrad say.
[00:11:58.040 --> 00:12:02.320]   Two sources said the request was related to Lambda.
[00:12:02.320 --> 00:12:03.600]   There's that name again,
[00:12:03.600 --> 00:12:05.800]   Google's natural language chatbot.
[00:12:05.800 --> 00:12:11.680]   Bryn filed a CL short for change list
[00:12:11.680 --> 00:12:14.400]   to gain access to the data that trains Lambda.
[00:12:14.400 --> 00:12:18.560]   It was a two line change to a configuration file
[00:12:18.560 --> 00:12:20.400]   just to add his name to the code.
[00:12:20.400 --> 00:12:24.960]   Oh, several Duds and engineers gave the request,
[00:12:24.960 --> 00:12:27.720]   LGTM approval, which means looks good to me.
[00:12:27.720 --> 00:12:30.000]   (laughing)
[00:12:30.000 --> 00:12:31.440]   LGTM.
[00:12:31.440 --> 00:12:34.280]   I need a rubber stamp that says LGTM, that's good.
[00:12:34.280 --> 00:12:35.680]   - Yeah, I think so, that's a good idea.
[00:12:35.680 --> 00:12:37.880]   - Some of the approvals came from workers outside the team
[00:12:37.880 --> 00:12:40.400]   seemingly eager just to be able to say
[00:12:40.400 --> 00:12:42.160]   they gave code review approval.
[00:12:42.160 --> 00:12:43.280]   (laughing)
[00:12:43.280 --> 00:12:44.360]   The Sergey Bryn.
[00:12:44.360 --> 00:12:47.600]   - Sounds like something at Twitter.
[00:12:47.600 --> 00:12:51.880]   - Yeah, well, yeah, they don't print out his code
[00:12:51.880 --> 00:12:54.240]   and show it to-- - It's weird.
[00:12:54.240 --> 00:12:56.760]   - Do you think Larry and Sergey like are still geniuses
[00:12:56.760 --> 00:12:59.880]   that could come in and-- - I'm sure they're still
[00:12:59.880 --> 00:13:03.160]   geniuses, but I got a hunch, they don't care.
[00:13:03.160 --> 00:13:06.200]   - I don't feel like Sergey has worn shoes in the years.
[00:13:06.200 --> 00:13:07.840]   I don't, what's their legacy?
[00:13:07.840 --> 00:13:10.480]   - Is he working on, he's working on several government
[00:13:10.480 --> 00:13:12.240]   initiatives related to AIs.
[00:13:12.240 --> 00:13:13.080]   - No, is he?
[00:13:13.080 --> 00:13:14.040]   - Oh, he is?
[00:13:14.040 --> 00:13:15.520]   Didn't know, okay, thank you.
[00:13:15.520 --> 00:13:20.520]   - Well, oh, he's a magnet for formerly.
[00:13:20.520 --> 00:13:23.400]   - Bryn's code request also receives some snark
[00:13:23.400 --> 00:13:25.120]   from Googlers, says Forbes.
[00:13:25.120 --> 00:13:27.240]   In response, one person commented,
[00:13:27.240 --> 00:13:30.880]   "Fix Google first," another person wrote,
[00:13:30.880 --> 00:13:33.680]   "At least talk to us,"
[00:13:33.680 --> 00:13:35.680]   a reference to the distance of the co-founders
[00:13:35.680 --> 00:13:37.920]   over the last few years, "ow."
[00:13:37.920 --> 00:13:41.040]   Some people linked to posts from Google's internal memes forum,
[00:13:41.040 --> 00:13:42.720]   some of the memes showed,
[00:13:42.720 --> 00:13:46.680]   "You and me, Jeff, fighting."
[00:13:46.680 --> 00:13:47.680]   No, I'm sorry.
[00:13:47.680 --> 00:13:50.480]   - Old people fighting, old people fighting.
[00:13:50.480 --> 00:13:51.320]   - Go to bed, Larry.
[00:13:51.320 --> 00:13:52.160]   - Go to bed.
[00:13:52.160 --> 00:13:55.480]   - Like maybe those two Muppets, right?
[00:13:55.480 --> 00:13:57.360]   - Walt Stattler and Waldorf.
[00:13:57.360 --> 00:13:58.360]   - Stattler and Waldorf.
[00:13:58.360 --> 00:14:01.720]   - I don't think this code looks good, me neither.
[00:14:01.720 --> 00:14:03.280]   Looks like a monkey roated.
[00:14:03.280 --> 00:14:07.600]   - Anyway, I asked the report before,
[00:14:07.600 --> 00:14:09.360]   we wanted to make sure we mentioned
[00:14:09.360 --> 00:14:12.120]   that Mr. Alverghetti will be on Tech News weekly
[00:14:12.120 --> 00:14:13.640]   this week, I'm gonna interview you.
[00:14:13.640 --> 00:14:14.640]   - To talk about this.
[00:14:14.640 --> 00:14:15.720]   - Oh good.
[00:14:15.720 --> 00:14:16.560]   - Reads great.
[00:14:16.560 --> 00:14:17.600]   - Oh my God, good gut.
[00:14:17.600 --> 00:14:19.440]   - Reads, yeah, good gut, that's great.
[00:14:19.440 --> 00:14:20.960]   - I like to hear more about what the plans
[00:14:20.960 --> 00:14:22.760]   are for Semaphore too.
[00:14:22.760 --> 00:14:26.040]   - Yeah, I like Semaphore so far.
[00:14:26.040 --> 00:14:29.280]   We're quoting from it, that's a good sign.
[00:14:29.280 --> 00:14:30.600]   He's got some scoops.
[00:14:30.600 --> 00:14:33.640]   It could be, you know, it could be,
[00:14:33.640 --> 00:14:36.240]   we're just leading right into another AI winner.
[00:14:36.240 --> 00:14:38.600]   We've been around long enough, you and me,
[00:14:38.600 --> 00:14:41.880]   Jeff, to remember all the excitement over AI.
[00:14:41.880 --> 00:14:43.680]   This has happened several times in the past.
[00:14:43.680 --> 00:14:45.720]   I was gonna transform everything and it ended up,
[00:14:45.720 --> 00:14:48.640]   "Oh, I can't do anything, forget it.
[00:14:48.640 --> 00:14:49.480]   Can't even make a sandwich."
[00:14:49.480 --> 00:14:51.480]   - When was your last AI spring?
[00:14:51.480 --> 00:14:53.360]   (laughs)
[00:14:53.360 --> 00:14:54.200]   - Question.
[00:14:54.200 --> 00:14:58.520]   - Marvin Minsky was at MIT,
[00:14:58.520 --> 00:15:01.120]   probably in the 70s, believe it or not.
[00:15:01.120 --> 00:15:04.560]   It was probably the last time we thought AI was going somewhere.
[00:15:04.560 --> 00:15:07.840]   Oh, maybe even, no, there was a group,
[00:15:07.840 --> 00:15:09.640]   for all I know they're still doing it.
[00:15:09.640 --> 00:15:13.000]   There was a group that was getting people,
[00:15:13.000 --> 00:15:17.440]   like it was like a foundation where thousands of people
[00:15:17.440 --> 00:15:19.400]   were typing as fast as they could
[00:15:19.400 --> 00:15:22.400]   to get data into the AI.
[00:15:22.400 --> 00:15:25.040]   The theory being the more, no, this is a long time ago,
[00:15:25.040 --> 00:15:26.960]   if you thought that, I mean, if you think about it,
[00:15:26.960 --> 00:15:29.840]   that was how they did it, to get more data into the AI.
[00:15:29.840 --> 00:15:32.800]   And as it turns out, that's kind of what this revolution is,
[00:15:32.800 --> 00:15:35.360]   because stable diffusion or mid-journey,
[00:15:35.360 --> 00:15:38.760]   the graphics AI's, chat GPT, their real skill
[00:15:38.760 --> 00:15:40.840]   is looking at a lot of data fast.
[00:15:40.840 --> 00:15:43.280]   And that's all because everything's been put in the internet.
[00:15:43.280 --> 00:15:45.040]   That's kind of what happened, right?
[00:15:45.040 --> 00:15:47.360]   - Well, there's no. - No.
[00:15:47.360 --> 00:15:51.280]   The big transforming element was GPUs
[00:15:51.280 --> 00:15:55.800]   and massive parallel processing coming down in price.
[00:15:55.800 --> 00:15:58.440]   If you look at-- - Because of crypto crash.
[00:15:58.440 --> 00:16:03.240]   - No, no, in 2012, that was our first visual,
[00:16:03.240 --> 00:16:05.920]   like that was when, what was it called?
[00:16:05.920 --> 00:16:08.920]   - Jeff Hinton and his team,
[00:16:08.920 --> 00:16:13.600]   one did a very effective visual,
[00:16:13.600 --> 00:16:15.880]   like computer vision algorithm.
[00:16:15.880 --> 00:16:19.000]   And it leaves and bounds ahead of everything else,
[00:16:19.000 --> 00:16:21.160]   and it was because he had written algorithms
[00:16:21.160 --> 00:16:24.800]   that took advantage of relatively inexpensive GPUs.
[00:16:24.800 --> 00:16:28.600]   Then what's happened is, yes, the data coming in
[00:16:28.600 --> 00:16:30.400]   from the internet, but that's actually been around
[00:16:30.400 --> 00:16:33.040]   for a long time, what we've now been able to do
[00:16:33.040 --> 00:16:36.240]   is optimize different algorithms for, again,
[00:16:36.240 --> 00:16:38.000]   super cheap GPUs.
[00:16:38.000 --> 00:16:41.040]   And I mean, this is expensive,
[00:16:41.040 --> 00:16:43.160]   but it's not as expensive as it was
[00:16:43.160 --> 00:16:45.640]   when it was running on highly proprietary,
[00:16:45.640 --> 00:16:48.880]   like, sun systems or whatever.
[00:16:48.880 --> 00:16:52.240]   - I do think that the availability of massive data sets
[00:16:52.240 --> 00:16:55.560]   without having humans to come and sit down
[00:16:55.560 --> 00:16:57.920]   and type them in probably is also important.
[00:16:57.920 --> 00:17:01.240]   - So we have the internet since 1998,
[00:17:01.240 --> 00:17:03.280]   and especially things like text.
[00:17:03.280 --> 00:17:05.360]   And if you look at, like, I was playing with IBM--
[00:17:05.360 --> 00:17:06.960]   - It's both, obviously. - It's the best.
[00:17:06.960 --> 00:17:08.840]   You need the hardware to process it,
[00:17:08.840 --> 00:17:10.760]   but you need access to massive massive data.
[00:17:10.760 --> 00:17:13.240]   You also need the ability to store these models.
[00:17:13.240 --> 00:17:14.080]   I mean, there's--
[00:17:14.080 --> 00:17:15.920]   - Also the fuzzy logic of AI.
[00:17:15.920 --> 00:17:16.760]   - It's a cut.
[00:17:16.760 --> 00:17:17.880]   Well, see, that's what's interesting.
[00:17:17.880 --> 00:17:19.800]   - The fuzzy logic of AI.
[00:17:19.800 --> 00:17:21.760]   - I think-- - Stacy and I will agree
[00:17:21.760 --> 00:17:24.000]   that it's this hardware stuff,
[00:17:24.000 --> 00:17:27.160]   but I don't know if the logic is new or different.
[00:17:27.160 --> 00:17:28.360]   Maybe it is.
[00:17:28.360 --> 00:17:30.000]   I mean, GAN had new old models.
[00:17:30.000 --> 00:17:35.000]   - They're optimized for massively parallel processors
[00:17:35.000 --> 00:17:39.160]   that are, again, commercially, they're COGS,
[00:17:39.160 --> 00:17:40.680]   commercial off the shelf goods.
[00:17:40.680 --> 00:17:41.520]   - That's right.
[00:17:41.520 --> 00:17:46.320]   - COGS is-- - So the last AI spring
[00:17:46.320 --> 00:17:50.320]   was this is from, towards data science,
[00:17:50.320 --> 00:17:52.280]   which a medium blog about.
[00:17:52.280 --> 00:17:56.880]   In fact, about these AI history of AI from Sean Ray.
[00:17:56.880 --> 00:18:00.360]   The last spring he reports was in the '90s, mid '90s,
[00:18:00.360 --> 00:18:03.400]   or late '90s, Japanese government unveiled plans
[00:18:03.400 --> 00:18:07.040]   to develop a fifth generation computer to advance.
[00:18:07.040 --> 00:18:08.120]   You remember all this?
[00:18:08.120 --> 00:18:12.120]   To advance machine learning, AI enthusiasts believed
[00:18:12.120 --> 00:18:13.600]   soon computers would be able,
[00:18:13.600 --> 00:18:15.600]   this is more than 20 years ago,
[00:18:15.600 --> 00:18:17.600]   be able to carry on conversations,
[00:18:17.600 --> 00:18:20.280]   translate languages, interpreted pictures.
[00:18:20.280 --> 00:18:24.560]   In 1997, Deep Blue defeated Gary Kasparov
[00:18:24.560 --> 00:18:28.520]   to beat the world champion at chess to become--
[00:18:28.520 --> 00:18:32.160]   - Deep Blue was using massively parallel processors.
[00:18:32.160 --> 00:18:33.320]   - I'm not arguing that. - We're just custom-laught.
[00:18:33.320 --> 00:18:34.480]   - I agree with you. - I don't know if it's--
[00:18:34.480 --> 00:18:35.320]   - I agree with you.
[00:18:37.960 --> 00:18:41.240]   Anyway, AI funding drived up in the .com bubble burst
[00:18:41.240 --> 00:18:43.440]   in the early 2000s.
[00:18:43.440 --> 00:18:45.320]   Machine learning continued its march,
[00:18:45.320 --> 00:18:48.200]   thanks to improvement in computer hardware, Stacey.
[00:18:48.200 --> 00:18:51.360]   Corporations and governments successfully used
[00:18:51.360 --> 00:18:54.880]   machine learning methods in narrow domains.
[00:18:54.880 --> 00:18:57.240]   Exponential gains in computer processing power
[00:18:57.240 --> 00:19:01.440]   and storage ability allowed companies to store vast
[00:19:01.440 --> 00:19:04.840]   and crunch vast quantities of data for the first time.
[00:19:06.280 --> 00:19:09.360]   And so it's a confluence of these things.
[00:19:09.360 --> 00:19:10.360]   I don't know if it's--
[00:19:10.360 --> 00:19:13.360]   - Oh, Stacey, I use the wrong term, but remember when
[00:19:13.360 --> 00:19:18.920]   Google Translate made a leap in its quality,
[00:19:18.920 --> 00:19:23.920]   there was an insight into how it operated
[00:19:23.920 --> 00:19:26.600]   that I think was critical too, and I forget--
[00:19:26.600 --> 00:19:30.000]   - Yeah, that's the way they're designing the algorithms.
[00:19:30.000 --> 00:19:31.920]   - That's what I'm saying. - So that's like a, okay.
[00:19:31.920 --> 00:19:34.480]   Well, when people say things like the fuzzy logic
[00:19:34.480 --> 00:19:38.880]   of AI, my inner reporter is like, the hell is that?
[00:19:38.880 --> 00:19:41.960]   That is nothing words.
[00:19:41.960 --> 00:19:42.800]   They don't mean anything.
[00:19:42.800 --> 00:19:44.840]   - Yeah, I'm sorry, I was flippin' in a couple of words.
[00:19:44.840 --> 00:19:47.160]   - In other words, it's all of the bad AI would.
[00:19:47.160 --> 00:19:52.480]   - It's a confluence of a variety of things.
[00:19:52.480 --> 00:19:55.840]   Here's a diagram of AI winters.
[00:19:55.840 --> 00:19:58.100]   (laughing)
[00:19:58.100 --> 00:20:02.280]   So the first AI winner was a 73.
[00:20:02.280 --> 00:20:05.800]   And that was, 'cause the 50s, and I'm aware of this
[00:20:05.800 --> 00:20:07.240]   'cause of John McCarthy and Lisp,
[00:20:07.240 --> 00:20:10.520]   and Lisp was developed for AI programming and stuff,
[00:20:10.520 --> 00:20:12.240]   and everybody was very excited about this.
[00:20:12.240 --> 00:20:14.840]   The 73 people won't forget that.
[00:20:14.840 --> 00:20:18.520]   Then in the 80s boom time, second AI winner in 88,
[00:20:18.520 --> 00:20:21.400]   2012, the deep learning revolution.
[00:20:21.400 --> 00:20:23.160]   Yeah, I think that's about right.
[00:20:23.160 --> 00:20:25.360]   And then--
[00:20:25.360 --> 00:20:26.200]   - Yes.
[00:20:26.200 --> 00:20:28.160]   - The Mark 1 perceptron.
[00:20:28.160 --> 00:20:32.120]   This is the first AI spring.
[00:20:32.120 --> 00:20:34.920]   See, there's a giant letter C,
[00:20:34.920 --> 00:20:37.560]   the camera taking a picture,
[00:20:37.560 --> 00:20:42.360]   and the computer understanding what the letter C is.
[00:20:42.360 --> 00:20:47.200]   Wow, we've come a long, long way.
[00:20:47.200 --> 00:20:49.720]   - It takes a lot of work to change the world.
[00:20:49.720 --> 00:20:50.680]   - Well, but that's the question.
[00:20:50.680 --> 00:20:52.680]   So that's what started this conversation.
[00:20:52.680 --> 00:20:56.160]   Are we in a real AI spring,
[00:20:56.160 --> 00:21:00.240]   or is this just another AI boom,
[00:21:00.240 --> 00:21:01.600]   as people get overexcited?
[00:21:01.600 --> 00:21:04.160]   As we did about self-driving cars.
[00:21:04.160 --> 00:21:06.760]   - Well, what do we think AI is?
[00:21:06.760 --> 00:21:10.160]   I mean, has AI had an actual impact on bottom line
[00:21:10.160 --> 00:21:12.720]   and product design 100%?
[00:21:12.720 --> 00:21:15.080]   So it's actively contributing today
[00:21:15.080 --> 00:21:19.360]   to real physical products that are in the world
[00:21:19.360 --> 00:21:21.280]   and generating returns.
[00:21:21.280 --> 00:21:26.280]   So is it winter because it doesn't call us up
[00:21:26.280 --> 00:21:27.760]   and cook us dinner?
[00:21:27.760 --> 00:21:29.320]   I mean, no, it's just,
[00:21:30.880 --> 00:21:32.800]   I don't know if it doesn't generate
[00:21:32.800 --> 00:21:36.080]   a perfectly mimicked Seinfeld episode.
[00:21:36.080 --> 00:21:38.840]   - My point is only that we're getting very excited about,
[00:21:38.840 --> 00:21:40.920]   I'm getting very excited about this idea of,
[00:21:40.920 --> 00:21:43.120]   for instance, chat, GPT and search,
[00:21:43.120 --> 00:21:44.520]   and Google's getting very nervous,
[00:21:44.520 --> 00:21:46.400]   that Bing might eclipse them.
[00:21:46.400 --> 00:21:51.080]   It seems like there is this explosion of AI
[00:21:51.080 --> 00:21:53.080]   when you look at, I'll tell you, here's another one.
[00:21:53.080 --> 00:21:56.480]   So we've seen images, we've seen text.
[00:21:56.480 --> 00:21:57.720]   Are you ready for,
[00:21:59.000 --> 00:22:01.760]   maybe we aren't ready for AI music?
[00:22:01.760 --> 00:22:05.840]   There's some very interesting,
[00:22:05.840 --> 00:22:09.160]   but I think not quite convincing yet,
[00:22:09.160 --> 00:22:13.920]   examples of AI music.
[00:22:13.920 --> 00:22:18.920]   This is a Google project called Music LM,
[00:22:18.920 --> 00:22:22.600]   a model generating high fidelity music
[00:22:22.600 --> 00:22:24.800]   from text descriptions.
[00:22:24.800 --> 00:22:28.200]   So just like with the AI image generators,
[00:22:28.200 --> 00:22:29.040]   you give it a prompt.
[00:22:29.040 --> 00:22:30.920]   - Oh, it's a little Mozart, but no.
[00:22:30.920 --> 00:22:31.920]   - Yeah, are you ready?
[00:22:31.920 --> 00:22:32.760]   - I'm not having a problem with this.
[00:22:32.760 --> 00:22:34.680]   - Would you like, let's see,
[00:22:34.680 --> 00:22:38.120]   the main soundtrack, I played, didn't I play this for you?
[00:22:38.120 --> 00:22:41.120]   Did I play this for you last week?
[00:22:41.120 --> 00:22:43.960]   - No, no sir, you went over this on Twitch.
[00:22:43.960 --> 00:22:47.720]   - 'Cause I remember you were less than impressed.
[00:22:47.720 --> 00:22:50.880]   A main, that's right, you're in studio for Twitch, that's why.
[00:22:50.880 --> 00:22:53.480]   The main soundtrack of an arcade game,
[00:22:53.480 --> 00:22:57.120]   so this is the prompt, this is the text prompt.
[00:22:57.120 --> 00:22:58.840]   Please, computer, right for me,
[00:22:58.840 --> 00:23:00.640]   the main soundtrack of an arcade game.
[00:23:00.640 --> 00:23:03.880]   It is fast paced and upbeat with a catchy electric guitar riff.
[00:23:03.880 --> 00:23:06.520]   The music is repetitive and easy to remember,
[00:23:06.520 --> 00:23:08.040]   but with unexpected sounds like
[00:23:08.040 --> 00:23:10.760]   cymbal crashes or drum rolls, you wanna hear it?
[00:23:10.760 --> 00:23:13.340]   (upbeat music)
[00:23:13.340 --> 00:23:15.760]   - Sounds like an arcade game.
[00:23:15.760 --> 00:23:16.600]   - Sounds like a arcade game.
[00:23:16.600 --> 00:23:17.440]   - It's like Sonic's, - It's irritating
[00:23:17.440 --> 00:23:19.480]   as every game soundtrack, yes.
[00:23:19.480 --> 00:23:26.160]   It lacks a certain something.
[00:23:26.160 --> 00:23:27.040]   - Yeah, it does.
[00:23:27.040 --> 00:23:28.760]   - Can't stop, please stop it.
[00:23:28.760 --> 00:23:30.200]   Let's see the best of thing.
[00:23:30.200 --> 00:23:34.360]   That's the thing, it lacks something,
[00:23:34.360 --> 00:23:39.080]   but a musician, an actual artist,
[00:23:39.080 --> 00:23:41.960]   can take that and be inspired from that.
[00:23:41.960 --> 00:23:43.400]   - I think it lacks something exactly
[00:23:43.400 --> 00:23:45.440]   the same way as chat GPT lacks something.
[00:23:45.440 --> 00:23:47.680]   It's, or even stable diffusion.
[00:23:47.680 --> 00:23:50.880]   Here's a funky piece with a strong danceable beat
[00:23:50.880 --> 00:23:52.400]   and a prominent bass line.
[00:23:52.400 --> 00:23:56.440]   A catchy melody from a keyboard adds a layer of richness
[00:23:56.440 --> 00:23:58.480]   and complexity to the song.
[00:23:58.480 --> 00:24:02.380]   (upbeat music)
[00:24:02.380 --> 00:24:06.240]   - Oh wait, go to our R&B hip hop with a voice.
[00:24:06.240 --> 00:24:07.080]   - Oh, you wanna hear that?
[00:24:07.080 --> 00:24:07.920]   - That's interesting.
[00:24:07.920 --> 00:24:11.720]   - So the voices are just like text and stable diffusion.
[00:24:11.720 --> 00:24:13.080]   They're greeked, they're garbled.
[00:24:13.080 --> 00:24:15.760]   This is an R&B hip hop music piece.
[00:24:15.760 --> 00:24:19.000]   There's a male vocal rapping and a female vocal singing
[00:24:19.000 --> 00:24:22.280]   in a rap like "Man, or the B" is comprised of a piano
[00:24:22.280 --> 00:24:23.400]   playing the chords of the tune
[00:24:23.400 --> 00:24:25.440]   with an electronic drum backing.
[00:24:25.440 --> 00:24:28.000]   The atmosphere of the piece is playful and energetic.
[00:24:28.000 --> 00:24:29.520]   This piece could be used in the soundtrack
[00:24:29.520 --> 00:24:32.920]   of a high school drama movie or TV show.
[00:24:32.920 --> 00:24:35.240]   It could also be played in birthday parties
[00:24:35.240 --> 00:24:36.160]   or beach parties.
[00:24:36.160 --> 00:24:38.400]   Wow, that's an extensive prompt.
[00:24:38.400 --> 00:24:39.600]   Let's see what we get.
[00:24:39.600 --> 00:24:42.180]   (upbeat music)
[00:24:42.180 --> 00:24:50.280]   - A little better.
[00:24:50.280 --> 00:24:52.200]   - I have to see Kid's Bop going for this.
[00:24:52.200 --> 00:24:53.040]   - K-pop or something.
[00:24:53.040 --> 00:24:53.880]   - Sounds another.
[00:24:53.880 --> 00:24:54.720]   - Sounds Japanese, yeah.
[00:24:54.720 --> 00:24:56.080]   ♪ Nice, listen up ♪
[00:24:56.080 --> 00:24:57.640]   ♪ Hip-dye scores and no-wah ♪
[00:24:57.640 --> 00:24:59.240]   ♪ The cup is in the shop ♪
[00:24:59.240 --> 00:25:01.240]   ♪ Don't keep the gun, K-I-N-C ♪
[00:25:01.240 --> 00:25:02.960]   ♪ Don't kill the non-the-the ♪
[00:25:02.960 --> 00:25:04.960]   ♪ Don't believe you, don't ♪
[00:25:04.960 --> 00:25:05.800]   - Please stop.
[00:25:05.800 --> 00:25:06.800]   (laughing)
[00:25:06.800 --> 00:25:09.040]   Yeah, it's old white man hip hop.
[00:25:09.040 --> 00:25:10.560]   It was a worthy song.
[00:25:10.560 --> 00:25:12.080]   - And again, it's not great, but it's--
[00:25:12.080 --> 00:25:14.680]   - How about Gory and Gantt with a drum machine?
[00:25:14.680 --> 00:25:17.040]   - Yeah, you could do something with it, I guess.
[00:25:17.040 --> 00:25:21.720]   I feel like even the most basic composer
[00:25:21.720 --> 00:25:24.560]   could start with a better point than that, though, Ant.
[00:25:24.560 --> 00:25:25.400]   - Really?
[00:25:25.400 --> 00:25:28.040]   I mean, if they can, they can, but--
[00:25:28.040 --> 00:25:28.880]   - Brianna Woosett--
[00:25:28.880 --> 00:25:29.840]   - But think about this.
[00:25:29.840 --> 00:25:33.280]   - Brianna Woosett, her sci-fi author, husband, Frank Wu,
[00:25:33.280 --> 00:25:34.840]   was stumped, he needed to write a story,
[00:25:34.840 --> 00:25:36.880]   he had chat GPT write the outlines--
[00:25:36.880 --> 00:25:37.720]   - Right, yes.
[00:25:37.720 --> 00:25:39.960]   - And then it gave him the chance to finish the story.
[00:25:39.960 --> 00:25:41.240]   So I guess in that regard, that's--
[00:25:41.240 --> 00:25:42.520]   - Gave him some inspiration.
[00:25:42.520 --> 00:25:44.760]   - Well, here's the thing where I talked to the board
[00:25:44.760 --> 00:25:47.880]   at the Marshall Project, which is about justice,
[00:25:47.880 --> 00:25:50.320]   criminal justice last week.
[00:25:50.320 --> 00:25:53.480]   And they deal with much of the population they serve
[00:25:53.480 --> 00:25:57.160]   as incarcerated people, they have a high level of illiteracy.
[00:25:57.160 --> 00:25:59.960]   And I said, "Chat GPT could be useful to someone saying,
[00:25:59.960 --> 00:26:01.960]   I want help telling my story."
[00:26:01.960 --> 00:26:06.080]   And prompt writing, telling you what you wanted to say--
[00:26:06.080 --> 00:26:07.440]   - But don't you need a high level of job--
[00:26:07.440 --> 00:26:09.920]   - But don't you need a high level of literacy even to start?
[00:26:09.920 --> 00:26:11.560]   - Well, you could do audio and all kinds of things.
[00:26:11.560 --> 00:26:12.400]   - Oh, I guess you could talk to it.
[00:26:12.400 --> 00:26:13.640]   - It's not necessarily, right?
[00:26:13.640 --> 00:26:17.960]   It's, you know, I look, we had someone talk to our faculty
[00:26:17.960 --> 00:26:20.160]   about chat GPT yesterday and it was kind of,
[00:26:20.160 --> 00:26:21.280]   well, what impact does this have?
[00:26:21.280 --> 00:26:22.800]   It's always about us, media.
[00:26:22.800 --> 00:26:23.640]   - Yeah.
[00:26:23.640 --> 00:26:26.160]   - And I think far differently, I wrote a post on media
[00:26:26.160 --> 00:26:31.080]   about this, that we've got to expand the notion of literacy
[00:26:31.080 --> 00:26:33.080]   and more tools for creation or say,
[00:26:33.080 --> 00:26:34.280]   look at it when it comes to art.
[00:26:34.280 --> 00:26:37.120]   I can't draw with a damn, but chat GPT,
[00:26:37.120 --> 00:26:41.680]   or I mean, I mean, Bali can help me express myself
[00:26:41.680 --> 00:26:42.960]   in images.
[00:26:42.960 --> 00:26:44.720]   Yes, it looks fake, yes, it's not great,
[00:26:44.720 --> 00:26:46.680]   but it's better than I could ever do.
[00:26:46.680 --> 00:26:49.160]   And that brings me a power that I didn't have.
[00:26:49.160 --> 00:26:51.560]   That's the kind of stuff that really interests me about this.
[00:26:51.560 --> 00:26:53.920]   And Leo, I keep thinking about your,
[00:26:53.920 --> 00:26:55.040]   sorry, Stacey, just--
[00:26:55.040 --> 00:26:55.880]   - No, no, no.
[00:26:55.880 --> 00:26:56.720]   - Just no more.
[00:26:56.720 --> 00:26:58.600]   - I was going to piggyback, but you finished, I'll go.
[00:26:58.600 --> 00:27:00.040]   - I just, so I can disagree, Leo,
[00:27:00.040 --> 00:27:02.640]   we always want a chance to do that.
[00:27:02.640 --> 00:27:03.880]   What's the name of that search engine thing
[00:27:03.880 --> 00:27:04.720]   you're using now?
[00:27:04.720 --> 00:27:05.560]   - Niva.
[00:27:05.560 --> 00:27:06.400]   - Niva.
[00:27:06.400 --> 00:27:09.240]   I just don't think, I think it's,
[00:27:09.240 --> 00:27:10.880]   my prediction, and I don't predict,
[00:27:10.880 --> 00:27:12.720]   but I'll predict now, is that,
[00:27:12.720 --> 00:27:17.720]   is that textual search results will be like talking
[00:27:17.720 --> 00:27:20.440]   to these devices that you had us all buy,
[00:27:20.440 --> 00:27:22.600]   that nobody really wants to use.
[00:27:22.600 --> 00:27:25.320]   The Madam A's and all that.
[00:27:25.320 --> 00:27:27.200]   I don't think people entered into conversations
[00:27:27.200 --> 00:27:30.840]   with that stuff, and they could do it with their voice,
[00:27:30.840 --> 00:27:31.840]   and I don't think that worked well.
[00:27:31.840 --> 00:27:33.960]   So I don't know that it's going to work in search engines.
[00:27:33.960 --> 00:27:35.560]   Sorry, Stacey, go ahead, Tom.
[00:27:35.560 --> 00:27:36.480]   - No, I was going to say,
[00:27:36.480 --> 00:27:38.960]   I read a really interesting article,
[00:27:38.960 --> 00:27:42.480]   like written by a designer about Dolly,
[00:27:42.480 --> 00:27:44.560]   and stable diffusion, and kind of,
[00:27:44.560 --> 00:27:47.280]   and their idea was like, he talks,
[00:27:47.280 --> 00:27:51.160]   it was one of the creators over at Argo Design,
[00:27:51.160 --> 00:27:52.920]   was the former frog guys.
[00:27:52.920 --> 00:27:55.480]   And he was talking about the role of a designer
[00:27:55.480 --> 00:27:57.880]   going forward in this kind of world,
[00:27:57.880 --> 00:28:01.640]   might be to let technology let you scale out
[00:28:01.640 --> 00:28:04.720]   and test a bunch of different ideas that you can,
[00:28:04.720 --> 00:28:07.280]   and the human has to go through and refine,
[00:28:07.280 --> 00:28:09.240]   - Refine, refine, yes.
[00:28:09.240 --> 00:28:10.080]   - Thank you.
[00:28:10.080 --> 00:28:11.000]   - Yes, thank you.
[00:28:11.000 --> 00:28:14.120]   That's what I've said for the longest,
[00:28:14.120 --> 00:28:17.560]   back when the whole Dolly movement came around.
[00:28:17.560 --> 00:28:20.680]   None of this stuff is going to take away from the artists.
[00:28:20.680 --> 00:28:22.480]   It's only going to help the artists,
[00:28:22.480 --> 00:28:24.640]   you know, get better with the things that they're doing,
[00:28:24.640 --> 00:28:26.640]   especially when you're talking about scale,
[00:28:26.640 --> 00:28:28.920]   like the designer that you just mentioned.
[00:28:28.920 --> 00:28:32.000]   - I did, I tell you the story about,
[00:28:32.000 --> 00:28:36.360]   about the illustrators, when Lincoln died at Harper's,
[00:28:36.360 --> 00:28:37.480]   - I don't recall. - Sure.
[00:28:37.480 --> 00:28:38.720]   - I don't recall.
[00:28:38.720 --> 00:28:41.160]   - So when Lincoln died, they sent illustrators
[00:28:41.160 --> 00:28:43.320]   down to Washington's stat,
[00:28:43.320 --> 00:28:48.320]   and they rode the train back to drawing what they found.
[00:28:48.320 --> 00:28:50.560]   Lincoln had already died by the time they got there.
[00:28:50.560 --> 00:28:54.560]   Then the telegraph hit, it was more connected,
[00:28:54.560 --> 00:28:56.560]   then they would send illustrators down to an event,
[00:28:56.560 --> 00:28:59.120]   and they would telegraph back a description
[00:28:59.120 --> 00:29:02.480]   of their drawings to an illustrator back in the offices
[00:29:02.480 --> 00:29:05.280]   who would draw something and then have it in great.
[00:29:05.280 --> 00:29:07.560]   Then when, who was assassinated?
[00:29:07.560 --> 00:29:10.280]   McKinley, no, I should go this.
[00:29:10.280 --> 00:29:11.760]   - I mean, McKinley shot. - I mean, McKinley shot.
[00:29:11.760 --> 00:29:12.760]   - Cleveland? - McKinley.
[00:29:12.760 --> 00:29:14.000]   - Oh, McKinley was shot.
[00:29:14.000 --> 00:29:15.440]   - So McKinley was shot in Buffalo, I think,
[00:29:15.440 --> 00:29:17.400]   or Rochester or somewhere like that, right?
[00:29:17.400 --> 00:29:21.160]   And so then photography was in.
[00:29:21.160 --> 00:29:24.480]   So then for the first time, they could send photographers up
[00:29:24.480 --> 00:29:26.560]   and they got the photos back that much sooner,
[00:29:26.560 --> 00:29:28.040]   and the New York Times wrote a piece about,
[00:29:28.040 --> 00:29:29.400]   well, there goes the illustrator,
[00:29:29.400 --> 00:29:31.440]   and indeed, there went the illustrator.
[00:29:31.440 --> 00:29:33.840]   Huge numbers of illustrator jobs disappeared
[00:29:33.840 --> 00:29:36.120]   with photography, then.
[00:29:36.120 --> 00:29:38.040]   Technology was trying and trying and trying
[00:29:38.040 --> 00:29:40.360]   to do differently, and I think you're right,
[00:29:40.360 --> 00:29:44.080]   both of you stay CNN, that whether it's for text
[00:29:44.080 --> 00:29:45.880]   or whether it's for illustration,
[00:29:45.880 --> 00:29:50.880]   this can give you all kinds of possible looks,
[00:29:50.880 --> 00:29:54.640]   but then it's all the inspiration for the human to do.
[00:29:54.640 --> 00:29:55.760]   - You don't think that at some point
[00:29:55.760 --> 00:29:57.800]   it'll get so good that the human doesn't need
[00:29:57.800 --> 00:29:58.760]   to get involved?
[00:29:58.760 --> 00:30:00.120]   - Certain kinds of things.
[00:30:00.120 --> 00:30:01.880]   You look, I was thinking about this the other day, Leo.
[00:30:01.880 --> 00:30:04.120]   - Like Stuckart. - Yeah, exactly.
[00:30:04.120 --> 00:30:07.360]   - Stuckart's fine, it's quick and dirty.
[00:30:07.360 --> 00:30:09.000]   - Which is a lot of like post-whip.
[00:30:09.000 --> 00:30:11.440]   - Here's a, that's fine.
[00:30:11.440 --> 00:30:13.760]   Gregorian chant generated by a Google.
[00:30:13.760 --> 00:30:18.760]   - No, you could already synthesize her.
[00:30:18.760 --> 00:30:19.600]   - Yeah, yeah.
[00:30:19.600 --> 00:30:20.440]   All right, here's another one.
[00:30:20.440 --> 00:30:21.680]   - The monks had no synthesizers.
[00:30:21.680 --> 00:30:23.320]   - So there's a Billie Eilish song
[00:30:23.320 --> 00:30:25.600]   you probably know called "Happier Than Ever."
[00:30:25.600 --> 00:30:30.040]   Somebody has used AI to put Ariana Grande's voice on it.
[00:30:30.040 --> 00:30:36.160]   So it's Ariana Grande singing a Billie Eilish.
[00:30:36.160 --> 00:30:39.600]   - Singing a Billie Eilish song.
[00:30:39.600 --> 00:30:40.760]   - Think of both soon.
[00:30:40.760 --> 00:30:43.520]   - I think if you heard this, you wouldn't say,
[00:30:43.520 --> 00:30:44.720]   "Oh, that's an AI."
[00:30:44.720 --> 00:30:47.880]   It's pretty good.
[00:30:47.880 --> 00:30:49.560]   - Yeah, sounds like Ariana Grande.
[00:30:49.560 --> 00:30:51.920]   - You might say, "Oh, I didn't know Ariana Grande
[00:30:51.920 --> 00:30:52.760]   covered that."
[00:30:52.760 --> 00:30:54.240]   Well, she didn't.
[00:30:54.240 --> 00:30:56.080]   So it's pretty good.
[00:30:56.080 --> 00:30:56.920]   - Dangerous.
[00:30:56.920 --> 00:31:00.200]   - Well, so there's some interesting ideas there.
[00:31:00.200 --> 00:31:02.960]   Like, I, when I was younger, I hated this
[00:31:02.960 --> 00:31:04.320]   'cause I thought it was annoying.
[00:31:04.320 --> 00:31:05.720]   But now that I'm older, I really like it.
[00:31:05.720 --> 00:31:08.760]   Like when you go and you go see a show live
[00:31:08.760 --> 00:31:12.120]   or someone covers a song and they add their spit on it,
[00:31:12.120 --> 00:31:14.440]   I used to hate that as a kid, you know?
[00:31:14.440 --> 00:31:16.000]   But now I'm like--
[00:31:16.000 --> 00:31:18.120]   - I love covers now. - You put a lot more value.
[00:31:18.120 --> 00:31:20.200]   Yeah, 'cause you're like, "Oh, what did they do?"
[00:31:20.200 --> 00:31:21.880]   You have just, I don't know why it's a kid I hate.
[00:31:21.880 --> 00:31:23.960]   - They could be an artist, if you will.
[00:31:23.960 --> 00:31:24.840]   - Yeah. - What did you early--
[00:31:24.840 --> 00:31:25.840]   - So I think...
[00:31:25.840 --> 00:31:31.000]   - One of the early podcasts was called "Coverville"
[00:31:31.000 --> 00:31:32.840]   and it was just covers.
[00:31:32.840 --> 00:31:35.080]   And I can remember hearing covers of songs
[00:31:35.080 --> 00:31:36.080]   that I had dismissed.
[00:31:36.080 --> 00:31:40.400]   Like, hit me baby one more time by Britney Spears.
[00:31:40.400 --> 00:31:42.840]   I dismissed as pop trash.
[00:31:42.840 --> 00:31:46.400]   But then I heard somebody do it in a calm,
[00:31:46.400 --> 00:31:48.160]   quiet folk song rendition.
[00:31:48.160 --> 00:31:50.920]   I thought, "Oh, that's actually a pretty good song."
[00:31:50.920 --> 00:31:52.400]   (laughing)
[00:31:52.400 --> 00:31:54.280]   So it does, I agree with you Stacy.
[00:31:54.280 --> 00:31:56.240]   Hearing a cover can sometimes give you a new look
[00:31:56.240 --> 00:31:58.160]   at an old song.
[00:31:58.160 --> 00:31:59.080]   - Yeah.
[00:31:59.080 --> 00:32:00.760]   - So when you have all of this--
[00:32:00.760 --> 00:32:01.600]   - Metallica.
[00:32:02.760 --> 00:32:07.760]   - AI kind of, I think it might make individual artists,
[00:32:07.760 --> 00:32:12.600]   seeing them live become a lot more important, right?
[00:32:12.600 --> 00:32:14.800]   And maybe they'll control their own voice
[00:32:14.800 --> 00:32:18.480]   and their sound in do these things for mass production.
[00:32:18.480 --> 00:32:21.400]   But then you get, and if mass production
[00:32:21.400 --> 00:32:25.080]   truly becomes scalable via AI,
[00:32:25.080 --> 00:32:27.640]   then you're gonna move back to more artistic,
[00:32:27.640 --> 00:32:31.120]   artisanal kind of experiences, I think,
[00:32:31.120 --> 00:32:35.720]   for actual paid consumption, if that makes sense.
[00:32:35.720 --> 00:32:39.000]   - When I think about brands and someone like Nike,
[00:32:39.000 --> 00:32:41.400]   with their huge marketing departments and so forth,
[00:32:41.400 --> 00:32:43.480]   they could probably have someone in-house
[00:32:43.480 --> 00:32:46.240]   that is spending hours upon hours
[00:32:46.240 --> 00:32:50.440]   and just going in and trying to figure out props and so forth.
[00:32:50.440 --> 00:32:53.160]   But they're also gonna spend just as much money
[00:32:53.160 --> 00:32:56.640]   doing market research far as what people are reacting to
[00:32:56.640 --> 00:32:58.920]   and how they can really enhance
[00:32:58.920 --> 00:33:02.280]   what they could get out of the AI products.
[00:33:02.280 --> 00:33:05.000]   I don't think it's just gonna be an end-all
[00:33:05.000 --> 00:33:06.840]   with something that's generated from AI.
[00:33:06.840 --> 00:33:10.160]   It's probably gonna be a lot more human interaction involved
[00:33:10.160 --> 00:33:11.680]   to figure out, will this really stick?
[00:33:11.680 --> 00:33:15.080]   Or how can we make this even more impactful
[00:33:15.080 --> 00:33:17.640]   for our message, for our brand?
[00:33:17.640 --> 00:33:21.960]   - Then there's the case of a company called 11 Labs
[00:33:21.960 --> 00:33:26.920]   that claimed that they were gonna be able to do,
[00:33:27.840 --> 00:33:30.160]   and I think we are seeing this already.
[00:33:30.160 --> 00:33:34.320]   Really amazing voice synthesis that you give it your voice
[00:33:34.320 --> 00:33:39.320]   and it will generate copy with your voice and so forth.
[00:33:39.320 --> 00:33:42.160]   There are other companies offering this,
[00:33:42.160 --> 00:33:46.240]   except the folks at 4chan got a hold of it.
[00:33:46.240 --> 00:33:47.080]   - Right, bro.
[00:33:47.080 --> 00:33:50.600]   - When it came out of beta this past week
[00:33:50.600 --> 00:33:55.360]   and started posting disturbing,
[00:33:55.360 --> 00:33:57.960]   but almost indistinguishable from the real thing.
[00:33:57.960 --> 00:33:59.120]   I don't know, I haven't heard it
[00:33:59.120 --> 00:34:00.600]   'cause I don't wanna go on 4chan,
[00:34:00.600 --> 00:34:03.800]   but things like Emma Watson of the Harry Potter series
[00:34:03.800 --> 00:34:05.440]   reading "Mind Compf".
[00:34:05.440 --> 00:34:10.120]   Donald Trump on Epstein Island.
[00:34:10.120 --> 00:34:13.200]   - No, that's real.
[00:34:13.200 --> 00:34:17.680]   - Ben Shapiro making racist comments to AOC.
[00:34:17.680 --> 00:34:23.640]   And the problem is that I think people could fall for these.
[00:34:23.640 --> 00:34:24.480]   Right?
[00:34:25.480 --> 00:34:28.000]   Rick Sanchez from Rick and Morty saying,
[00:34:28.000 --> 00:34:29.640]   - I'm gonna be my wife, Morty.
[00:34:29.640 --> 00:34:34.360]   - Which of course the creator of the show is in trouble
[00:34:34.360 --> 00:34:38.520]   for doing that as a girlfriend.
[00:34:38.520 --> 00:34:43.520]   Of course, 4chan 11 Labs immediately kind of shut down.
[00:34:43.520 --> 00:34:47.240]   They're open beta, they're trying to figure out a way
[00:34:47.240 --> 00:34:50.400]   to stop misuse, this seems to happen every time, doesn't it?
[00:34:50.400 --> 00:34:51.400]   - Yeah.
[00:34:51.400 --> 00:34:52.240]   - They're gonna ask.
[00:34:52.240 --> 00:34:53.400]   - And then media go crazy about,
[00:34:53.400 --> 00:34:56.480]   oh my God, this is gonna be the ruin of civilization.
[00:34:56.480 --> 00:34:57.320]   We figure it out.
[00:34:57.320 --> 00:34:59.560]   - It is okay, it is not crazy to be like,
[00:34:59.560 --> 00:35:02.760]   hey, it is super accessible to produce highly accurate
[00:35:02.760 --> 00:35:06.280]   seeming fake videos, audio, et cetera,
[00:35:06.280 --> 00:35:08.160]   of anybody in the world.
[00:35:08.160 --> 00:35:11.480]   That is not moral panic, chef, that is a legitimate.
[00:35:11.480 --> 00:35:13.600]   - Did I say the words moral panic?
[00:35:13.600 --> 00:35:15.520]   Did I say the words moral panic?
[00:35:15.520 --> 00:35:17.080]   - No, but you said media gets caused by this.
[00:35:17.080 --> 00:35:19.640]   - It's cause for concern.
[00:35:19.640 --> 00:35:22.600]   But what, and we talked about it on Sunday on Twitter,
[00:35:22.600 --> 00:35:24.200]   even if the conclusion seemed to be,
[00:35:24.200 --> 00:35:25.640]   well, they're just gonna be an onus.
[00:35:25.640 --> 00:35:27.960]   You can't believe everything you hear anymore.
[00:35:27.960 --> 00:35:29.840]   You couldn't for a while. - It's not a man.
[00:35:29.840 --> 00:35:30.760]   - And she's just gonna have--
[00:35:30.760 --> 00:35:32.360]   - Well, people don't believe,
[00:35:32.360 --> 00:35:35.880]   so people use this to condemn social media.
[00:35:35.880 --> 00:35:37.400]   In most every survey out there,
[00:35:37.400 --> 00:35:39.200]   people say that they don't trust social media.
[00:35:39.200 --> 00:35:40.520]   That's good news.
[00:35:40.520 --> 00:35:42.080]   They don't, right?
[00:35:42.080 --> 00:35:44.920]   - They don't, but they still let it influence them.
[00:35:44.920 --> 00:35:48.240]   I like one of my, like my first journalism class,
[00:35:48.240 --> 00:35:51.360]   like journalism 101, you know, our professor got in there
[00:35:51.360 --> 00:35:54.520]   and was like, he told us about this new study
[00:35:54.520 --> 00:35:57.680]   that toothpaste had sugar in it.
[00:35:57.680 --> 00:35:59.960]   And it was actually harmful for people.
[00:35:59.960 --> 00:36:02.360]   And, you know, he passed out an article or something
[00:36:02.360 --> 00:36:06.640]   and we were all like, he was like, all right, you know,
[00:36:06.640 --> 00:36:08.240]   let's talk about it.
[00:36:08.240 --> 00:36:10.520]   And then he asked us, you know, do we believe this?
[00:36:10.520 --> 00:36:15.280]   And, you know, half of the class was like, yeah, here it is.
[00:36:15.280 --> 00:36:18.160]   And the other half was like, well, I don't know.
[00:36:18.160 --> 00:36:20.520]   But then even at the very end,
[00:36:20.520 --> 00:36:24.160]   the class still had pulled like a quarter of the people were like,
[00:36:24.160 --> 00:36:25.920]   oh my God, it's still scary.
[00:36:25.920 --> 00:36:29.760]   I, you know, it's sowed enough down in 24 like half a quarter
[00:36:29.760 --> 00:36:32.880]   of the class that, you know, they may not believe
[00:36:32.880 --> 00:36:35.040]   this sort of thing is true, but they're also like,
[00:36:35.040 --> 00:36:36.120]   well, maybe it is.
[00:36:36.120 --> 00:36:40.400]   - So, by the way, it's not true, right?
[00:36:40.400 --> 00:36:41.800]   There is no sugar in it.
[00:36:41.800 --> 00:36:45.040]   - No, there is no sugar in the taste.
[00:36:45.040 --> 00:36:45.880]   - It's sweet.
[00:36:45.880 --> 00:36:47.200]   So you could believe, you might believe that.
[00:36:47.200 --> 00:36:48.040]   - Yes.
[00:36:48.040 --> 00:36:50.640]   - So that was his, you know, he was like,
[00:36:50.640 --> 00:36:53.840]   people want to believe what it's easy to believe.
[00:36:53.840 --> 00:36:55.000]   And, you know.
[00:36:55.000 --> 00:37:00.840]   - Do you believe this?
[00:37:00.840 --> 00:37:07.720]   I hereby verify that I, Leo Laport,
[00:37:07.720 --> 00:37:11.680]   like Descript to create an overdub version of my voice.
[00:37:11.680 --> 00:37:14.160]   Why do you want this, Anthony?
[00:37:14.160 --> 00:37:15.000]   - No, I do.
[00:37:15.000 --> 00:37:18.000]   (audience cheering)
[00:37:18.440 --> 00:37:21.880]   - Well, hey, hey, it's A.I. Leo Laport, the A.I. Tech Guy.
[00:37:21.880 --> 00:37:23.560]   This week on Twitch.
[00:37:23.560 --> 00:37:25.240]   - That was something Anthony did for us.
[00:37:25.240 --> 00:37:26.320]   - That's pretty tech, I'm good.
[00:37:26.320 --> 00:37:27.160]   - For our firm.
[00:37:27.160 --> 00:37:28.160]   (laughing)
[00:37:28.160 --> 00:37:29.760]   - That's pretty tech, I'm good.
[00:37:29.760 --> 00:37:35.080]   I mean, it's, yes, it had a few of the little robot.
[00:37:35.080 --> 00:37:36.760]   - You could tell it wasn't me.
[00:37:36.760 --> 00:37:38.000]   - It was a little robotic.
[00:37:38.000 --> 00:37:38.840]   - But it was my voice.
[00:37:38.840 --> 00:37:40.000]   So actually the only thing wrong with it
[00:37:40.000 --> 00:37:41.520]   was the inflections were wrong.
[00:37:41.520 --> 00:37:42.360]   - Was hey, hey, hey.
[00:37:42.360 --> 00:37:43.200]   - That's Leo.
[00:37:43.200 --> 00:37:44.040]   - That's Leo. - That's Leo.
[00:37:44.040 --> 00:37:44.920]   - Three hours into the show.
[00:37:44.920 --> 00:37:45.760]   - Yeah.
[00:37:45.760 --> 00:37:46.600]   - But your really voice.
[00:37:46.600 --> 00:37:47.440]   - But the voice was accurate.
[00:37:47.440 --> 00:37:48.440]   - Sounds like my voice.
[00:37:48.440 --> 00:37:50.200]   - Yeah, just give it time.
[00:37:50.200 --> 00:37:55.360]   - OpenAI offers error-prone AI detector.
[00:37:55.360 --> 00:37:58.400]   Amid fears of machine-stuffed future.
[00:37:58.400 --> 00:38:02.420]   This is from Katiana Kwach at the register.
[00:38:02.420 --> 00:38:07.160]   OpenAI has released a free online tune
[00:38:07.160 --> 00:38:09.840]   designed to deflect accusations
[00:38:09.840 --> 00:38:16.040]   from against OpenAI for cheating with GPT.
[00:38:16.040 --> 00:38:18.320]   Instead of the AI text classifier.
[00:38:18.320 --> 00:38:19.400]   Have you seen this yet?
[00:38:19.400 --> 00:38:21.880]   Is this being used in academia, Jeff?
[00:38:21.880 --> 00:38:23.920]   - I've a lot of talk about these things.
[00:38:23.920 --> 00:38:26.200]   There was a kid that did one version,
[00:38:26.200 --> 00:38:29.120]   but I also saw a writer who put her own writing into it
[00:38:29.120 --> 00:38:32.120]   and she was accused of using GPT for what she'd already written.
[00:38:32.120 --> 00:38:34.440]   - A false positive is a problem
[00:38:34.440 --> 00:38:36.520]   because you're gonna accuse a plagiarism
[00:38:36.520 --> 00:38:38.760]   if you would something you really wrote is not good.
[00:38:38.760 --> 00:38:39.600]   - Yeah.
[00:38:39.600 --> 00:38:42.200]   - Schools and universities in the US, France and India
[00:38:42.200 --> 00:38:45.960]   have since banned students from accessing chat GPT,
[00:38:45.960 --> 00:38:48.960]   but then there's teachers like you talked about, Jeff, last week
[00:38:48.960 --> 00:38:51.640]   who are using chat GPT in the classroom.
[00:38:51.640 --> 00:38:54.160]   - There's a woman on TikTok who's just amazing,
[00:38:54.160 --> 00:38:56.040]   very creative about it, wonderful.
[00:38:56.040 --> 00:38:59.080]   And I wanna talk to my colleagues about it.
[00:38:59.080 --> 00:39:00.160]   What one colleague said,
[00:39:00.160 --> 00:39:03.120]   when we use transcription tools these days,
[00:39:03.120 --> 00:39:05.440]   which broadcast students have to use a lot,
[00:39:05.440 --> 00:39:07.040]   it's using AI like crazy.
[00:39:07.040 --> 00:39:07.880]   - Right.
[00:39:07.880 --> 00:39:09.200]   - To do that.
[00:39:09.200 --> 00:39:10.600]   I put up a thing on the rundown,
[00:39:10.600 --> 00:39:11.440]   you're not gonna wanna,
[00:39:11.440 --> 00:39:14.080]   it's an academic paper online at 77,
[00:39:14.080 --> 00:39:15.720]   but I think it was Google,
[00:39:15.720 --> 00:39:17.720]   according to somebody in Maston,
[00:39:17.720 --> 00:39:20.880]   brought together professional writers
[00:39:20.880 --> 00:39:23.680]   to ask how they would see using these tools.
[00:39:23.680 --> 00:39:28.360]   And they broke down various functions of inspiring an idea,
[00:39:28.360 --> 00:39:33.280]   a seed of a story, completing an idea, working it through.
[00:39:33.280 --> 00:39:34.440]   I think that's where,
[00:39:34.440 --> 00:39:36.160]   that's where this becomes interesting to me
[00:39:36.160 --> 00:39:39.120]   is when you see this tool as a tool,
[00:39:39.120 --> 00:39:41.760]   then what can people do with it?
[00:39:41.760 --> 00:39:43.640]   How does it expand what they're capable of?
[00:39:43.640 --> 00:39:45.080]   That's what's interesting.
[00:39:45.080 --> 00:39:47.760]   But it's gonna replace us now.
[00:39:47.760 --> 00:39:52.560]   - What, we talked about CNET's use of artificial intelligence
[00:39:52.560 --> 00:39:53.400]   to write its art.
[00:39:53.400 --> 00:39:55.800]   - Did you ever hear which engine they used?
[00:39:55.800 --> 00:39:58.000]   - They have their own engine called Word Smith.
[00:39:58.000 --> 00:39:59.600]   - Oh, you see, oh, that's actually,
[00:39:59.600 --> 00:40:00.920]   this paper's about Word Smith too.
[00:40:00.920 --> 00:40:01.920]   - Yeah.
[00:40:01.920 --> 00:40:02.760]   - That's interesting.
[00:40:02.760 --> 00:40:03.760]   - Yeah, so.
[00:40:03.760 --> 00:40:05.680]   - It was literally a bad word.
[00:40:05.680 --> 00:40:08.000]   - Well, so it had inaccuracies
[00:40:08.000 --> 00:40:09.000]   which the human editors denied.
[00:40:09.000 --> 00:40:11.320]   - Oh, this is wordcraft, I'm sorry, this is wordcraft.
[00:40:11.320 --> 00:40:14.600]   But that isn't really the issue to me.
[00:40:14.600 --> 00:40:17.800]   The issue to me is, remember those old days of Link Farms?
[00:40:17.800 --> 00:40:19.800]   You were talking about this.
[00:40:19.800 --> 00:40:22.000]   It was somebody's brilliant idea,
[00:40:22.000 --> 00:40:24.120]   put a lot of venture capital into it,
[00:40:24.120 --> 00:40:27.000]   to create sites where you would figure out,
[00:40:27.000 --> 00:40:30.360]   you'd figure out from Google what were the most searched
[00:40:30.360 --> 00:40:31.200]   - Search-tenters.
[00:40:31.200 --> 00:40:33.840]   - Right after about.com did that, that others copied it.
[00:40:33.840 --> 00:40:35.240]   - Well, it was the name of that.
[00:40:35.240 --> 00:40:37.080]   There was a name for that kind of thing.
[00:40:37.080 --> 00:40:40.840]   - Oh, it wasn't Link Farm, but it was like that.
[00:40:40.840 --> 00:40:41.680]   - It was, it was.
[00:40:41.680 --> 00:40:43.040]   - I can't remember.
[00:40:43.040 --> 00:40:43.880]   - Click farming?
[00:40:43.880 --> 00:40:44.760]   No, that's Adclay.
[00:40:44.760 --> 00:40:47.520]   - No, it was like, Link baiting?
[00:40:47.520 --> 00:40:50.120]   - The idea is you figure out what the top search terms are
[00:40:50.120 --> 00:40:53.480]   and then you write articles tailored to get--
[00:40:53.480 --> 00:40:55.280]   - Oh, that's just search optimization, is it?
[00:40:55.280 --> 00:40:57.680]   - Well, no, no, no, you're creating it.
[00:40:57.680 --> 00:41:00.440]   - So let's say you notice that when people look,
[00:41:00.440 --> 00:41:04.960]   that people do a search for belt buckles, a lot.
[00:41:04.960 --> 00:41:06.760]   - Or Mesylothymiomia.
[00:41:06.760 --> 00:41:07.880]   - Or Mesylthymiomia.
[00:41:07.880 --> 00:41:12.880]   And so you create a bunch of articles with the plan about.com.
[00:41:12.880 --> 00:41:14.240]   - Oh, those are content farms.
[00:41:14.240 --> 00:41:15.240]   - Very well. - Content farms.
[00:41:15.240 --> 00:41:16.080]   There you go.
[00:41:16.080 --> 00:41:17.880]   - Content farms, that's it, thank you.
[00:41:17.880 --> 00:41:19.240]   - Also, it did it very well.
[00:41:19.240 --> 00:41:20.400]   Others copied it.
[00:41:20.400 --> 00:41:21.240]   I can't remember the name of the company.
[00:41:21.240 --> 00:41:23.640]   - And then our friend Matt Cutts did everything we could
[00:41:23.640 --> 00:41:25.800]   to stop that from happening, right?
[00:41:25.800 --> 00:41:26.640]   I think successfully.
[00:41:26.640 --> 00:41:27.800]   - That was Panda.
[00:41:27.800 --> 00:41:31.880]   Panda was the first major update to search to deal with that.
[00:41:31.880 --> 00:41:33.560]   - Well, there are those, mostly the verge,
[00:41:33.560 --> 00:41:37.280]   accusing CNET of going and more than CNET,
[00:41:37.280 --> 00:41:41.040]   their private equity owners, Red Ventures,
[00:41:41.040 --> 00:41:45.960]   of deciding to monetize CNET by making it a content farm,
[00:41:45.960 --> 00:41:49.560]   by using these AIs to generate articles
[00:41:49.560 --> 00:41:51.280]   that will show up in search results.
[00:41:51.280 --> 00:41:53.800]   And they also own, Red Ventures doesn't just own CNET,
[00:41:53.800 --> 00:41:58.800]   they own the points guy and some credit card review sites
[00:41:58.800 --> 00:42:01.920]   and all of them make money by generating content
[00:42:01.920 --> 00:42:04.480]   that is gonna show up on search.
[00:42:04.480 --> 00:42:07.560]   So you click on it and then you get affiliate revenue.
[00:42:07.560 --> 00:42:10.080]   - The real thing here is content and writing,
[00:42:10.080 --> 00:42:12.120]   not reporting, I'm gonna anticipate Stacey,
[00:42:12.120 --> 00:42:13.280]   'cause she's gonna say reporting,
[00:42:13.280 --> 00:42:16.840]   but content and writing are utterly commodified now.
[00:42:16.840 --> 00:42:18.840]   It's not special to write anymore.
[00:42:18.840 --> 00:42:21.280]   - Well, that was Conical Yelmo's special agent.
[00:42:21.280 --> 00:42:24.520]   So when Conny was on Twitch, this story had just broken.
[00:42:24.520 --> 00:42:26.560]   She's the editor in chief of CNET and she said,
[00:42:26.560 --> 00:42:28.520]   we have these AIs write the articles
[00:42:28.520 --> 00:42:29.640]   that writers don't wanna do.
[00:42:29.640 --> 00:42:34.560]   They're just boring explainers on personal finance.
[00:42:34.560 --> 00:42:36.600]   And so nobody really wants to write it.
[00:42:36.600 --> 00:42:38.120]   So we just have the AI write it
[00:42:38.120 --> 00:42:40.480]   and then we have an editor check it and then we put it up
[00:42:40.480 --> 00:42:42.920]   because these are useful to people,
[00:42:42.920 --> 00:42:45.920]   but nobody would like our writers to do others.
[00:42:45.920 --> 00:42:46.320]   - But there were things that didn't have been
[00:42:46.320 --> 00:42:47.560]   100 times before.
[00:42:47.560 --> 00:42:52.800]   - But they're also, yeah, a useful personal finance primer
[00:42:52.800 --> 00:42:54.600]   is actually something people would wanna,
[00:42:54.600 --> 00:42:56.840]   I mean, as a writer, you may hate doing it.
[00:42:56.840 --> 00:42:59.640]   Like I used to hate doing, but like a servicey thing
[00:42:59.640 --> 00:43:02.800]   should not be farmed out to AIs.
[00:43:02.800 --> 00:43:04.800]   - Well, I think CNET learned that lesson.
[00:43:04.800 --> 00:43:06.640]   - But I don't think that's what CNET is doing, right?
[00:43:06.640 --> 00:43:08.200]   They're not doing the serviceable stuff
[00:43:08.200 --> 00:43:09.120]   is more of the--
[00:43:09.120 --> 00:43:10.520]   - No, that's what they wanted to do
[00:43:10.520 --> 00:43:15.520]   is have a whole series of 75 articles on personal finance.
[00:43:15.520 --> 00:43:17.880]   So they'd have this encyclopedia.
[00:43:17.880 --> 00:43:21.000]   And it did kind of mesh nicely with Red Ventures,
[00:43:21.000 --> 00:43:23.120]   other holdings of credit card companies.
[00:43:23.120 --> 00:43:26.480]   And there's some suspicion that maybe,
[00:43:26.480 --> 00:43:28.920]   and this often happens with private equity,
[00:43:28.920 --> 00:43:30.840]   that they wanna monetize quickly
[00:43:30.840 --> 00:43:32.200]   'cause they usually have a lot of debt
[00:43:32.200 --> 00:43:35.040]   that they need to pay off to acquire these.
[00:43:35.040 --> 00:43:38.640]   So they maybe were taking this venerable
[00:43:38.640 --> 00:43:42.920]   and valuable brand CNET and turning it into content farms.
[00:43:42.920 --> 00:43:45.680]   Here's the article from Search Engine Landbury Shorts,
[00:43:45.680 --> 00:43:50.040]   is AI written content replacing cheap old content farms?
[00:43:50.040 --> 00:43:53.480]   It sounds like Google Panda.
[00:43:55.360 --> 00:43:57.840]   - You know, let's look at another angle, sir.
[00:43:57.840 --> 00:43:59.120]   - Sure.
[00:43:59.120 --> 00:44:03.760]   - So this AI generated content is bringing eyeballs there.
[00:44:03.760 --> 00:44:06.080]   Yes, there's affiliate links and things of that nature,
[00:44:06.080 --> 00:44:10.560]   but what about the potential of retention
[00:44:10.560 --> 00:44:12.040]   of the eyeballs coming there?
[00:44:12.040 --> 00:44:13.160]   Oh, well, while I'm here,
[00:44:13.160 --> 00:44:14.920]   how about I click on this menu over here
[00:44:14.920 --> 00:44:16.680]   to check out this other topic
[00:44:16.680 --> 00:44:18.080]   that may be of interest to me?
[00:44:18.080 --> 00:44:20.160]   Oh, the new M2 Macs are out.
[00:44:20.160 --> 00:44:22.360]   What did they have to say about that?
[00:44:24.520 --> 00:44:27.600]   - Well, so there's a lot of interest in M2 Macs,
[00:44:27.600 --> 00:44:29.680]   but what you would hope is you would get an article
[00:44:29.680 --> 00:44:31.000]   that was written by a human being
[00:44:31.000 --> 00:44:33.360]   that did some research, did some benchmarking,
[00:44:33.360 --> 00:44:35.280]   did some tests instead of a--
[00:44:35.280 --> 00:44:36.920]   - Hey, CNET does that.
[00:44:36.920 --> 00:44:37.760]   CNET does that.
[00:44:37.760 --> 00:44:38.600]   - They do that.
[00:44:38.600 --> 00:44:40.720]   - You're saying this could be like gateway,
[00:44:40.720 --> 00:44:43.640]   the AI content is like a gateway,
[00:44:43.640 --> 00:44:45.920]   get you in the door when you're searching for--
[00:44:45.920 --> 00:44:47.960]   - They hold you on to the site.
[00:44:47.960 --> 00:44:51.000]   - Well, whatever, it's not real content, that's my point.
[00:44:51.000 --> 00:44:56.000]   It's not written to gain search clicks
[00:44:56.000 --> 00:45:00.360]   as opposed to serve an audience.
[00:45:00.360 --> 00:45:02.680]   - I mean, news has always been written
[00:45:02.680 --> 00:45:05.880]   to gain some sort of eyeball, right?
[00:45:05.880 --> 00:45:08.040]   So you look at-- - You'll redag them headline.
[00:45:08.040 --> 00:45:09.680]   - Yeah, you look at, I mean,
[00:45:09.680 --> 00:45:11.720]   yes, there is actual journalism,
[00:45:11.720 --> 00:45:15.160]   but believe you me, for every real story,
[00:45:15.160 --> 00:45:18.400]   there's three filtered press releases
[00:45:18.400 --> 00:45:19.480]   or silly things that are--
[00:45:19.480 --> 00:45:20.320]   - Yeah, yeah.
[00:45:20.320 --> 00:45:21.160]   - No.
[00:45:21.160 --> 00:45:22.000]   - Here's what the Verge wrote,
[00:45:22.000 --> 00:45:24.640]   "The robot articles published on CNET don't need to be good.
[00:45:24.640 --> 00:45:27.360]   They need to rank highly in Google searches,
[00:45:27.360 --> 00:45:28.880]   so lots of people open them
[00:45:28.880 --> 00:45:32.480]   and click the lucrative affiliate marketing links they contain."
[00:45:32.480 --> 00:45:35.680]   And so that's why you have articles,
[00:45:35.680 --> 00:45:41.800]   things like, what's the best credit card to get for points?
[00:45:41.800 --> 00:45:47.720]   Or, can you buy a gift card with a credit card
[00:45:47.720 --> 00:45:49.400]   or what is Zell and how does it work?
[00:45:49.400 --> 00:45:52.040]   These are all real articles on CNET.
[00:45:52.040 --> 00:45:54.280]   Or, "Bangrate," or "Credicards.com,"
[00:45:54.280 --> 00:45:55.760]   and before they were taken down.
[00:45:55.760 --> 00:45:57.720]   - And talk to me for a minute
[00:45:57.720 --> 00:45:59.160]   because you just said it again for a minute.
[00:45:59.160 --> 00:46:03.160]   You get, I don't know the word is disgusted,
[00:46:03.160 --> 00:46:06.880]   turned off by news.
[00:46:06.880 --> 00:46:10.280]   - Yeah, because a lot of it is sensationalized
[00:46:10.280 --> 00:46:11.960]   and shouldn't have to be,
[00:46:11.960 --> 00:46:14.160]   and then there's a whole lot of slant
[00:46:14.160 --> 00:46:16.000]   when it's supposed to be news.
[00:46:16.000 --> 00:46:17.880]   Just give me the facts.
[00:46:17.880 --> 00:46:19.720]   And I found that to be the case
[00:46:19.720 --> 00:46:22.560]   for just about all of the news outlets out there today.
[00:46:22.560 --> 00:46:24.600]   Instead of just giving me the facts,
[00:46:24.600 --> 00:46:26.520]   they're giving me their opinions
[00:46:26.520 --> 00:46:30.240]   and all kinds of other mess that has nothing to do
[00:46:30.240 --> 00:46:31.080]   with the facts.
[00:46:31.080 --> 00:46:33.280]   I don't need all it, I don't care.
[00:46:33.280 --> 00:46:35.960]   So just, you know, but they're not the silky.
[00:46:35.960 --> 00:46:37.840]   - But I would submit that there is value of value.
[00:46:37.840 --> 00:46:38.920]   - The facts are not helpful.
[00:46:38.920 --> 00:46:40.200]   - Yeah, okay, go ahead.
[00:46:40.200 --> 00:46:42.040]   - You need to interpret it.
[00:46:42.040 --> 00:46:42.880]   - Yeah.
[00:46:42.880 --> 00:46:45.200]   So like, yes, there are certainly times
[00:46:45.200 --> 00:46:47.280]   when the facts make sense, right?
[00:46:47.280 --> 00:46:49.080]   But most people need more than the facts.
[00:46:49.080 --> 00:46:50.520]   They need some sort of context.
[00:46:50.520 --> 00:46:54.360]   So it's not enough to say, like, let's take--
[00:46:54.360 --> 00:46:57.080]   - I agree about context, but--
[00:46:57.080 --> 00:46:59.280]   - Okay, but choosing the context
[00:46:59.280 --> 00:47:01.040]   is what people get frustrated about.
[00:47:01.040 --> 00:47:03.880]   They think that's opinionizing in a lot of ways.
[00:47:03.880 --> 00:47:08.560]   Like, you know, I can choose if I am gonna report on,
[00:47:08.560 --> 00:47:11.240]   let's say I'm gonna report on,
[00:47:11.240 --> 00:47:16.360]   give me something, like a new law--
[00:47:16.360 --> 00:47:17.200]   - I'll give you one.
[00:47:17.200 --> 00:47:18.120]   - City Council. - I'll give you one.
[00:47:18.120 --> 00:47:21.520]   - Okay, give me some. - The Fed today
[00:47:21.520 --> 00:47:24.880]   raised interest rates, 25 basis points.
[00:47:24.880 --> 00:47:26.760]   That's the fact.
[00:47:26.760 --> 00:47:27.680]   - Oh.
[00:47:27.680 --> 00:47:31.920]   - But what a good writer will do is say
[00:47:31.920 --> 00:47:34.640]   that's lower than previous raises,
[00:47:34.640 --> 00:47:38.560]   which some experts believe means the Fed's less worried
[00:47:38.560 --> 00:47:40.640]   about inflation than they used to be.
[00:47:40.640 --> 00:47:42.800]   That's the context.
[00:47:42.800 --> 00:47:44.120]   - You could talk about the, yeah,
[00:47:44.120 --> 00:47:45.600]   you could talk about the last time
[00:47:45.600 --> 00:47:50.120]   it was raised 25 or 250 basis points, what that meant.
[00:47:50.120 --> 00:47:52.840]   - Right, you see, everything you all are saying right now,
[00:47:52.840 --> 00:47:55.080]   that's totally fine.
[00:47:55.080 --> 00:47:57.520]   I'm talking about, there's--
[00:47:57.520 --> 00:48:00.080]   - Well, as I say, last happened under
[00:48:00.080 --> 00:48:02.560]   the Republican administration--
[00:48:02.560 --> 00:48:04.920]   - Right, there's ways to say--
[00:48:04.920 --> 00:48:05.920]   - Hard, hard. - Yeah.
[00:48:05.920 --> 00:48:08.520]   So that's the two different headlines, you could say.
[00:48:08.520 --> 00:48:12.320]   Fed raises interest rates, 25 basis points.
[00:48:12.320 --> 00:48:15.000]   It looks like inflation's not over, folks,
[00:48:15.000 --> 00:48:18.720]   or Fed raises interest rates, 25 basis points.
[00:48:18.720 --> 00:48:20.800]   Looks like they're not so worried about inflation
[00:48:20.800 --> 00:48:22.160]   as they were previously.
[00:48:22.160 --> 00:48:23.920]   Two different interpretations of the same--
[00:48:23.920 --> 00:48:25.200]   - Or Brad, the long has a book out now
[00:48:25.200 --> 00:48:28.800]   that I'm listening to slouching toward utopia,
[00:48:28.800 --> 00:48:30.640]   I think it's called, which he argues strenuously,
[00:48:30.640 --> 00:48:34.080]   that causing a recession to slow down the economy
[00:48:34.080 --> 00:48:36.320]   is basically a moral.
[00:48:36.320 --> 00:48:39.520]   Well, that's a clear opinion, that's a view.
[00:48:39.520 --> 00:48:41.840]   He's an economist, so he has that view.
[00:48:43.040 --> 00:48:45.240]   Now, is that slammed, or is that perspective?
[00:48:45.240 --> 00:48:46.760]   I find it really valuable, it's perspective,
[00:48:46.760 --> 00:48:48.280]   but I know people are gonna disagree with it.
[00:48:48.280 --> 00:48:51.800]   - Perspective is, I'm fine with perspective.
[00:48:51.800 --> 00:48:55.040]   I just don't care for the dad gum cheerleaders
[00:48:55.040 --> 00:48:57.680]   with all of the pom-poms that will lean in one way
[00:48:57.680 --> 00:49:00.880]   or the other, just give me the facts.
[00:49:00.880 --> 00:49:03.280]   - Well, but some people feel like, I mean,
[00:49:03.280 --> 00:49:07.160]   one person's cheerleader is another person's fact person.
[00:49:07.160 --> 00:49:10.520]   I mean, like, people have accused me for years
[00:49:10.520 --> 00:49:13.680]   of being a cheerleader for various industries.
[00:49:13.680 --> 00:49:18.600]   When, in fact, I just acknowledge the technical limitations
[00:49:18.600 --> 00:49:20.680]   of something that a company might have, right?
[00:49:20.680 --> 00:49:21.960]   They're like, no, you can't do this.
[00:49:21.960 --> 00:49:23.320]   I'm like, well, you know.
[00:49:23.320 --> 00:49:29.120]   So, that's what you want isn't, that's just your opinion.
[00:49:29.120 --> 00:49:30.680]   It's just your opinion, man.
[00:49:30.680 --> 00:49:33.800]   - That's only your opinion, man.
[00:49:33.800 --> 00:49:37.720]   I know what you're saying, and I think some of this
[00:49:37.720 --> 00:49:40.480]   comes from the fact that we have in the United States
[00:49:40.480 --> 00:49:45.040]   three 24-hour news channels, each of which have,
[00:49:45.040 --> 00:49:46.560]   varies, they have their own.
[00:49:46.560 --> 00:49:49.040]   - TV news is horrifying.
[00:49:49.040 --> 00:49:50.400]   - Yeah. - Just know.
[00:49:50.400 --> 00:49:51.480]   - Just like us.
[00:49:51.480 --> 00:49:53.400]   - It's a matter of success. - It's a matter of degree.
[00:49:53.400 --> 00:49:54.800]   - It's a matter of degree.
[00:49:54.800 --> 00:49:58.000]   I think we try really hard not to be biased,
[00:49:58.000 --> 00:50:00.880]   but our entire job is not to provide facts
[00:50:00.880 --> 00:50:04.840]   because we're talking about already reporting somebody's done,
[00:50:04.840 --> 00:50:08.560]   but to explain, to elaborate, to elucidate.
[00:50:08.560 --> 00:50:12.840]   So, we are doing that, I guess it's really a matter of degree,
[00:50:12.840 --> 00:50:15.160]   or I don't know.
[00:50:15.160 --> 00:50:17.600]   - Yeah, everything kind of turns into a matter of degree
[00:50:17.600 --> 00:50:18.800]   to be totally honest.
[00:50:18.800 --> 00:50:21.160]   - Yeah, it's all gray-goo at the bottom.
[00:50:21.160 --> 00:50:23.160]   All right, let me take a break.
[00:50:23.160 --> 00:50:24.600]   We got lots more to talk about.
[00:50:24.600 --> 00:50:27.160]   I think we're gonna be talking about a lot about AI.
[00:50:27.160 --> 00:50:28.640]   People are gonna get bored to death with it,
[00:50:28.640 --> 00:50:30.400]   but there's just so much interesting stuff happening.
[00:50:30.400 --> 00:50:31.840]   - Well, it's so fascinating.
[00:50:31.840 --> 00:50:34.240]   - It really is, and it's impact on us,
[00:50:34.240 --> 00:50:37.880]   and it's funny because the whole tech industry,
[00:50:37.880 --> 00:50:40.960]   for the last five years has been looking over there.
[00:50:40.960 --> 00:50:42.480]   Look at VR, AR.
[00:50:42.480 --> 00:50:44.040]   That's gonna be the next big thing.
[00:50:44.040 --> 00:50:46.200]   And all of a sudden, and this always happens,
[00:50:46.200 --> 00:50:49.320]   AI sneaks up on them and they went,
[00:50:49.320 --> 00:50:51.800]   whoa, never mind that VR AI.
[00:50:51.800 --> 00:50:53.140]   AI!
[00:50:53.140 --> 00:50:55.880]   And it's really fascinating to me.
[00:50:55.880 --> 00:50:58.480]   I mean, literally Microsoft fired most of the people
[00:50:58.480 --> 00:51:00.120]   working on augmented reality
[00:51:00.120 --> 00:51:06.480]   when it's the last time Google talked about VR, AR.
[00:51:06.480 --> 00:51:08.040]   - Facebook has kind of-
[00:51:08.040 --> 00:51:10.800]   - Facebook, Facebook put a lot of money,
[00:51:10.800 --> 00:51:13.960]   but I wonder if they're how hard they're working on it
[00:51:13.960 --> 00:51:14.800]   right now.
[00:51:14.800 --> 00:51:17.880]   - They've always had big AI labs, too.
[00:51:17.880 --> 00:51:22.200]   - In fact, Jan Lekoon, who's their AI guru there,
[00:51:22.200 --> 00:51:24.160]   one of the smartest guys in AI,
[00:51:24.160 --> 00:51:27.320]   when talking about chat GPT last week said,
[00:51:27.320 --> 00:51:29.600]   "Oh, that's no big deal, we have that."
[00:51:29.600 --> 00:51:31.920]   (laughing)
[00:51:31.920 --> 00:51:34.160]   Which is either just jealousy.
[00:51:34.160 --> 00:51:35.680]   Oh yeah, we thought of that before.
[00:51:35.680 --> 00:51:37.720]   - Well, look at Buzzfeed.
[00:51:37.720 --> 00:51:38.880]   So Buzzfeed said now,
[00:51:38.880 --> 00:51:40.960]   "They were gonna have AI right quizzes
[00:51:40.960 --> 00:51:42.880]   and that stupid Buzzfeed stuff, right?"
[00:51:42.880 --> 00:51:45.960]   And then Facebook is gonna give them a lot of money
[00:51:45.960 --> 00:51:46.800]   to do that.
[00:51:46.800 --> 00:51:49.640]   Buzzfeed's stock doubled from two cents to four cents, whatever.
[00:51:49.640 --> 00:51:52.400]   But now,
[00:51:52.400 --> 00:51:55.480]   seen as a way,
[00:51:55.480 --> 00:51:57.360]   I think it's going the wrong way once again.
[00:51:57.360 --> 00:51:58.520]   Oh good, more content in the last week.
[00:51:58.520 --> 00:52:00.840]   - This is Jonah Peretti in a nutshell.
[00:52:00.840 --> 00:52:01.920]   - Same thing. - Yes, it is.
[00:52:01.920 --> 00:52:05.760]   It is chasing the flavor of the month.
[00:52:05.760 --> 00:52:07.600]   And so Buzzfeed has always done it.
[00:52:07.600 --> 00:52:09.240]   The industry has always done.
[00:52:09.240 --> 00:52:10.720]   So I hope we're not doing that.
[00:52:10.720 --> 00:52:13.760]   I guess that was what I let off this whole conversation with,
[00:52:13.760 --> 00:52:17.840]   which is, is this another AI spring
[00:52:17.840 --> 00:52:19.360]   that's just gonna end up with an AI winner,
[00:52:19.360 --> 00:52:20.880]   or is this really transformative?
[00:52:20.880 --> 00:52:22.880]   Are we about to enter the next one?
[00:52:22.880 --> 00:52:23.960]   - As Ben Thompson says,
[00:52:23.960 --> 00:52:27.040]   the next epoch of computing.
[00:52:27.040 --> 00:52:29.080]   Equivalent to the internet and smartphones.
[00:52:29.080 --> 00:52:31.960]   And then we need to get that same AI
[00:52:31.960 --> 00:52:34.040]   that did the Seinfeld show to do Twig.
[00:52:34.040 --> 00:52:37.040]   - Oh, it'd be so boring.
[00:52:37.040 --> 00:52:38.640]   Oh my God, it'd be so boring.
[00:52:38.640 --> 00:52:42.480]   We'll talk about AI.
[00:52:42.480 --> 00:52:45.160]   I guess AI is gonna be way bigger
[00:52:45.160 --> 00:52:48.160]   than the blockchain discussions over the last six minutes.
[00:52:48.160 --> 00:52:49.000]   - I would just think of that as.
[00:52:49.000 --> 00:52:51.520]   - Yeah, we kind of got it over a blockchain, didn't we?
[00:52:51.520 --> 00:52:53.040]   - I'm gonna disagree,
[00:52:53.040 --> 00:52:56.320]   because to be blockchain is a fundamental underlying technology
[00:52:56.320 --> 00:52:59.040]   that's also gonna be useful for lots of things.
[00:52:59.040 --> 00:53:03.200]   But we just got sidetracked on this whole crypto finance.
[00:53:03.200 --> 00:53:04.720]   - Crypto, yeah.
[00:53:04.720 --> 00:53:09.720]   - I mean, it's being used actually in businesses right now
[00:53:09.720 --> 00:53:13.720]   for tracking information already.
[00:53:13.720 --> 00:53:16.360]   It's just not gonna be as big as we thought it would be.
[00:53:16.360 --> 00:53:18.120]   It's gonna be like.
[00:53:18.120 --> 00:53:19.960]   - It's not as shiny, right?
[00:53:19.960 --> 00:53:22.000]   - Yeah, it's gonna be like dedoping technology.
[00:53:22.000 --> 00:53:24.560]   You're something that's really essential and useful,
[00:53:24.560 --> 00:53:26.880]   but nobody cares about, right?
[00:53:26.880 --> 00:53:31.520]   - Blockchain is just a decentralized database.
[00:53:31.520 --> 00:53:33.120]   - Right, it's a decentralized one.
[00:53:33.120 --> 00:53:36.600]   - We're as excited about blockchain as we are about MySQL.
[00:53:36.600 --> 00:53:39.840]   It's like, it's like, it's yeah,
[00:53:39.840 --> 00:53:40.840]   people are gonna use it.
[00:53:40.840 --> 00:53:42.640]   It's in some cases.
[00:53:42.640 --> 00:53:47.520]   - For a while, I was really excited about NoSQL databases
[00:53:47.520 --> 00:53:49.960]   and I mean, it was a big deal.
[00:53:49.960 --> 00:53:50.800]   - It's similar.
[00:53:50.800 --> 00:53:53.280]   - It's very, if you wanna say blockchain
[00:53:53.280 --> 00:53:54.520]   is on that level of importance,
[00:53:54.520 --> 00:53:55.360]   I would agree with you.
[00:53:55.360 --> 00:53:56.360]   - Yeah.
[00:53:56.360 --> 00:53:57.200]   - That's what I mean.
[00:53:57.200 --> 00:53:59.200]   - Is it gonna transform the financial world?
[00:53:59.200 --> 00:54:03.880]   Is it gonna empower unbanked people all over the globe?
[00:54:03.880 --> 00:54:04.880]   No.
[00:54:04.880 --> 00:54:05.720]   - No.
[00:54:05.720 --> 00:54:06.560]   - No.
[00:54:06.560 --> 00:54:07.400]   - And poor Kevin Rose.
[00:54:07.400 --> 00:54:09.520]   - Is it a mechanism by which we can have
[00:54:09.520 --> 00:54:11.760]   automated smart contracts?
[00:54:11.760 --> 00:54:12.760]   Yes.
[00:54:12.760 --> 00:54:13.600]   - Yeah.
[00:54:13.600 --> 00:54:14.440]   - And that will be relevant.
[00:54:14.440 --> 00:54:17.000]   It's just not relevant in a way that people are gonna see.
[00:54:17.000 --> 00:54:17.840]   - Yeah.
[00:54:17.840 --> 00:54:18.680]   - Yeah.
[00:54:18.680 --> 00:54:19.760]   - Is there a Kevin update?
[00:54:19.760 --> 00:54:21.960]   - Well, he lost his squiggles.
[00:54:21.960 --> 00:54:26.320]   So Kevin, last week we were reporting as it happened,
[00:54:26.320 --> 00:54:28.600]   that Kevin had announced he'd been hacked.
[00:54:28.600 --> 00:54:32.640]   He has revealed that the NFTs that he lost
[00:54:32.640 --> 00:54:35.720]   were these really, really pretty squiggles.
[00:54:35.720 --> 00:54:37.720]   It's actually, I don't even need to scroll down
[00:54:37.720 --> 00:54:40.360]   into his Twitter, it's the header on his Twitter,
[00:54:40.360 --> 00:54:41.800]   but this is the funny thing.
[00:54:41.800 --> 00:54:43.400]   So he lost that.
[00:54:43.400 --> 00:54:45.120]   - But he could still make it his Twitter.
[00:54:45.120 --> 00:54:46.120]   - It's right there.
[00:54:46.120 --> 00:54:47.360]   - It's right there.
[00:54:47.360 --> 00:54:48.640]   What did you lose?
[00:54:48.640 --> 00:54:51.200]   You lost the right to say I own this.
[00:54:51.200 --> 00:54:53.080]   - You lost the right to sell it to some--
[00:54:53.080 --> 00:54:55.560]   - That's what he really lost, yeah.
[00:54:55.560 --> 00:54:59.520]   He says he lost millions of dollars, he was social.
[00:54:59.520 --> 00:55:02.360]   Oh, here's a picture of somebody sharing him.
[00:55:02.360 --> 00:55:04.520]   Chicken, he should have squiggled away.
[00:55:04.520 --> 00:55:08.280]   - Chicken away to squiggles.
[00:55:08.280 --> 00:55:09.840]   Yeah, I was gonna do an ad.
[00:55:09.840 --> 00:55:12.040]   (laughing)
[00:55:12.040 --> 00:55:14.920]   - Sazy nose, sewer the ad comes, sewer the ad comes.
[00:55:14.920 --> 00:55:17.640]   - Sinner the ad comes, sink the ad comes, look it.
[00:55:17.640 --> 00:55:20.160]   Even though this one was simple, not rare,
[00:55:20.160 --> 00:55:22.520]   loved the pattern, damn.
[00:55:22.520 --> 00:55:26.520]   - Chromie squiggle number 4429.
[00:55:26.520 --> 00:55:30.200]   I'd be very curious what he paid for that actually.
[00:55:30.200 --> 00:55:33.200]   He says, damn, I loved this one too.
[00:55:33.200 --> 00:55:36.840]   Okay.
[00:55:36.840 --> 00:55:40.160]   - Are you sure it's on open C invisible?
[00:55:40.160 --> 00:55:41.000]   - It's pretty.
[00:55:41.000 --> 00:55:42.360]   - He can't get it.
[00:55:42.360 --> 00:55:43.760]   Is that what he's saying?
[00:55:43.760 --> 00:55:47.600]   It's on open C, the marketplace, but he can't get it.
[00:55:47.600 --> 00:55:48.960]   - Well, it was his, he owned it
[00:55:48.960 --> 00:55:51.760]   and he was social engineered out of it.
[00:55:51.760 --> 00:55:56.760]   So it's worth $17,197.
[00:55:56.760 --> 00:56:00.280]   - Is it a devil?
[00:56:00.280 --> 00:56:01.680]   - Is it though?
[00:56:01.680 --> 00:56:03.040]   - To someone.
[00:56:03.040 --> 00:56:04.280]   - Is it?
[00:56:04.280 --> 00:56:05.400]   - Crow vault owns it.
[00:56:05.400 --> 00:56:07.320]   See, Kevin knows, this is the,
[00:56:07.320 --> 00:56:09.480]   this is the benefit of blockchain.
[00:56:09.480 --> 00:56:10.760]   You know who owns it.
[00:56:10.760 --> 00:56:12.640]   Crow vault, whoever that is.
[00:56:12.640 --> 00:56:16.480]   So it used to be, Kevin's.
[00:56:16.480 --> 00:56:21.680]   - Okay, our show today, actually you wanna make a squiggle
[00:56:21.680 --> 00:56:23.280]   worth millions?
[00:56:23.280 --> 00:56:25.080]   Maybe you should check out Miro.
[00:56:25.080 --> 00:56:28.400]   Do you know about Miro? Miro is super cool.
[00:56:28.400 --> 00:56:31.880]   We welcome Miro and brand new sponsor on the network.
[00:56:31.880 --> 00:56:34.240]   Miro is, can be anything.
[00:56:34.240 --> 00:56:36.280]   Do you start with the idea of it's a,
[00:56:36.280 --> 00:56:39.360]   a shared whiteboard, okay?
[00:56:39.360 --> 00:56:42.240]   A collaborative visual whiteboard.
[00:56:42.240 --> 00:56:43.080]   But what is that?
[00:56:43.080 --> 00:56:47.040]   Well, this is why I want you to go to Miro.com/podcast
[00:56:47.040 --> 00:56:47.880]   and take a look at it.
[00:56:47.880 --> 00:56:50.960]   In fact, I most particularly want you to go
[00:56:50.960 --> 00:56:54.240]   to the Miro examples, the templates.
[00:56:54.240 --> 00:56:59.240]   They have a page of other Miro whiteboards people have created
[00:56:59.240 --> 00:57:04.560]   that is so cool, so exciting.
[00:57:04.560 --> 00:57:08.720]   You know, I think you will, you will look at this.
[00:57:08.720 --> 00:57:10.600]   So this is what I want you to do.
[00:57:10.600 --> 00:57:11.760]   So you can understand it.
[00:57:11.760 --> 00:57:12.560]   Who is it for?
[00:57:12.560 --> 00:57:14.240]   It's for everybody.
[00:57:14.240 --> 00:57:16.800]   It's for project, product managers.
[00:57:16.800 --> 00:57:18.240]   It's for marketers.
[00:57:18.240 --> 00:57:19.640]   Are they called the Miroverse
[00:57:19.640 --> 00:57:22.880]   where other people's Miro's are?
[00:57:22.880 --> 00:57:24.480]   So you can, and you can download these
[00:57:24.480 --> 00:57:26.400]   and use them as a template for your own.
[00:57:26.400 --> 00:57:30.840]   Everything from ice breakers for your next meeting,
[00:57:30.840 --> 00:57:34.320]   to flow charts for your next project,
[00:57:34.320 --> 00:57:37.480]   team alignment circles, team psychological safety,
[00:57:37.480 --> 00:57:39.520]   digital campaign planning,
[00:57:39.520 --> 00:57:42.240]   midnight sailboat retrospective.
[00:57:42.240 --> 00:57:46.720]   Here's a Beatles retrospective somebody's created.
[00:57:46.720 --> 00:57:50.840]   It's unlimited what you can do.
[00:57:50.840 --> 00:57:53.000]   Now the next time you use, you got a meeting.
[00:57:53.000 --> 00:57:54.760]   I want you to use Miro for your meeting.
[00:57:54.760 --> 00:57:57.240]   Miro has built in polls, timers.
[00:57:57.240 --> 00:58:01.880]   You can head to consensus using Miro.
[00:58:01.880 --> 00:58:05.120]   It's so much more than just a simple digital whiteboard.
[00:58:05.120 --> 00:58:09.680]   It's a visual collaboration tool packed with features
[00:58:09.680 --> 00:58:12.040]   so that you and your whole team can build on one
[00:58:12.040 --> 00:58:15.040]   of those ideas, create something innovative.
[00:58:15.040 --> 00:58:18.600]   Nowadays with everybody working from home or hybrid,
[00:58:18.600 --> 00:58:21.400]   it's really nice to be able to get the team together
[00:58:21.400 --> 00:58:25.080]   over this Miro whiteboard for brainstorming
[00:58:25.080 --> 00:58:28.040]   and planning and researching and designing
[00:58:28.040 --> 00:58:29.360]   for feedback cycles.
[00:58:29.360 --> 00:58:31.120]   It can live on a Miro board, the integrations.
[00:58:31.120 --> 00:58:32.120]   Look at the integrations.
[00:58:32.120 --> 00:58:35.000]   You could do a can ban on your Miro board.
[00:58:35.000 --> 00:58:36.720]   And it's fast.
[00:58:36.720 --> 00:58:37.560]   It's fast.
[00:58:37.560 --> 00:58:40.280]   Faster input means faster outcomes.
[00:58:40.280 --> 00:58:41.640]   Miro users report.
[00:58:41.640 --> 00:58:45.760]   Miro increases project delivery speed by 29%.
[00:58:45.760 --> 00:58:49.200]   It's just a natural way to work.
[00:58:49.200 --> 00:58:50.520]   You can zoom in and out.
[00:58:50.520 --> 00:58:54.080]   You can view and share the big picture overview in a cinch.
[00:58:54.080 --> 00:58:55.160]   And you know what you'll find
[00:58:55.160 --> 00:58:58.360]   when you're working with teams and everybody has a voice
[00:58:58.360 --> 00:59:01.200]   and everybody's input is welcome.
[00:59:01.200 --> 00:59:04.080]   You could all tap into a single source of truth
[00:59:04.080 --> 00:59:09.080]   that keeps people engaged, invested, happy and productive.
[00:59:09.080 --> 00:59:11.520]   Cut out any confusion on who needs to do
[00:59:11.520 --> 00:59:14.480]   what by mapping out processes, roles, timelines,
[00:59:14.480 --> 00:59:16.080]   get that can ban up.
[00:59:16.080 --> 00:59:19.080]   Several different templates, including Miro's swim lane
[00:59:19.080 --> 00:59:23.280]   diagram, which is a great way to keep track of who's doing what.
[00:59:23.280 --> 00:59:27.400]   Strategic planning becomes easier when it's visual and accessible.
[00:59:27.400 --> 00:59:31.040]   You could tap into a way to map processes, systems and plans
[00:59:31.040 --> 00:59:33.600]   with the whole team so they not only can see it,
[00:59:33.600 --> 00:59:36.160]   they can have input and feedback into it.
[00:59:36.160 --> 00:59:41.480]   The only problem is it's so flexible and capable
[00:59:41.480 --> 00:59:44.000]   it's hard to describe what Miro could do.
[00:59:44.000 --> 00:59:47.480]   That's why I want you to go to Miro.com/podcast.
[00:59:47.480 --> 00:59:51.120]   Go to the Miroverse, take a look at some of the amazing things
[00:59:51.120 --> 00:59:53.320]   people have created with Miro.
[00:59:53.320 --> 00:59:57.840]   Get some ideas, some of the biggest companies in the world
[00:59:57.840 --> 00:59:59.200]   use Miro.
[00:59:59.200 --> 01:00:01.040]   Take a look at the integrations,
[01:00:01.040 --> 01:00:04.480]   all the different things you can put into Miro.
[01:00:04.480 --> 01:00:07.080]   So you're working with the tools you already know,
[01:00:07.080 --> 01:00:12.080]   like Slack and Dropbox and Box and so forth.
[01:00:12.080 --> 01:00:15.280]   And Google, of course Google Drive.
[01:00:15.280 --> 01:00:17.960]   You could create a team workspace you could find,
[01:00:17.960 --> 01:00:21.360]   put in shortcuts so you go right to what you're working on.
[01:00:21.360 --> 01:00:25.480]   I know teams literally literally live in Miro all day.
[01:00:25.480 --> 01:00:28.680]   They keep it open all day, connected to GitHub,
[01:00:28.680 --> 01:00:31.880]   connected to wherever your projects live.
[01:00:31.880 --> 01:00:33.800]   I just love this idea.
[01:00:33.800 --> 01:00:36.360]   It's such a brilliant product.
[01:00:36.360 --> 01:00:39.480]   And I love it that they've put all the tools on
[01:00:39.480 --> 01:00:43.720]   so you can take a look at how you can use it.
[01:00:43.720 --> 01:00:46.440]   And of course it's completely secure.
[01:00:46.440 --> 01:00:47.400]   You're gonna love Miro.
[01:00:47.400 --> 01:00:48.360]   Here's what I want you to do.
[01:00:48.360 --> 01:00:51.840]   I wanted to go to Miro.com/podcast right now.
[01:00:51.840 --> 01:00:54.240]   Take a look at what Miro can do for you.
[01:00:54.240 --> 01:00:57.080]   Strategic planning is so much easier.
[01:00:57.080 --> 01:00:59.400]   It's a great way to map processes and systems
[01:00:59.400 --> 01:01:01.920]   to map plans of the whole team.
[01:01:01.920 --> 01:01:03.360]   And if you're feeling a little tired
[01:01:03.360 --> 01:01:04.640]   and who isn't these days,
[01:01:04.640 --> 01:01:07.040]   you'll be glad to know Miro users report saving
[01:01:07.040 --> 01:01:09.720]   80 hours per user per year
[01:01:09.720 --> 01:01:12.400]   just by cutting down meeting time,
[01:01:12.400 --> 01:01:14.000]   streamlining conversations.
[01:01:14.000 --> 01:01:17.160]   That's two whole weeks, two whole work weeks a year.
[01:01:17.160 --> 01:01:19.240]   It's like having an extra vacation.
[01:01:19.240 --> 01:01:22.440]   More than a million users use Miro every single month.
[01:01:22.440 --> 01:01:24.120]   And now I know why Miro,
[01:01:24.120 --> 01:01:26.960]   M-I-R-O.com/podcast.
[01:01:26.960 --> 01:01:27.920]   Here's the best part.
[01:01:27.920 --> 01:01:29.560]   You can get your first three boards free
[01:01:29.560 --> 01:01:32.080]   so you could try it out right now.
[01:01:32.080 --> 01:01:33.560]   Start working better together.
[01:01:33.560 --> 01:01:37.960]   For Miro.com/podcast.
[01:01:37.960 --> 01:01:40.200]   I told him, I said,
[01:01:40.200 --> 01:01:41.080]   I am so excited.
[01:01:41.080 --> 01:01:42.360]   We're gonna start using this.
[01:01:42.360 --> 01:01:43.640]   For our Ask the Tech guys show,
[01:01:43.640 --> 01:01:46.040]   Mike and I are gonna do some planning on it.
[01:01:46.040 --> 01:01:48.520]   I'm thrilled about it.
[01:01:48.520 --> 01:01:51.520]   But look at this Miroverse for just a kind of a glimpse
[01:01:51.520 --> 01:01:52.960]   that some of the things people are doing with it.
[01:01:52.960 --> 01:01:55.480]   It's pretty incredible.
[01:01:55.480 --> 01:01:56.880]   And you'll never wanna have another meeting
[01:01:56.880 --> 01:01:58.800]   without Miro running in the background
[01:01:58.800 --> 01:02:00.040]   and the timers and the polls
[01:02:00.040 --> 01:02:02.720]   and the consensus building tools.
[01:02:02.720 --> 01:02:03.840]   They're just fantastic.
[01:02:03.840 --> 01:02:05.840]   It's a Harry Potter retrospective
[01:02:05.840 --> 01:02:07.560]   created by the UK government.
[01:02:07.560 --> 01:02:10.200]   (laughs)
[01:02:10.200 --> 01:02:11.400]   Post meeting summary.
[01:02:11.400 --> 01:02:12.440]   Look at this.
[01:02:12.440 --> 01:02:14.520]   What a great board this would be to have.
[01:02:14.520 --> 01:02:17.340]   Collaborative meeting agenda.
[01:02:17.340 --> 01:02:21.000]   Ideation workshop.
[01:02:21.000 --> 01:02:23.760]   Miro.com/andstinkyfish.
[01:02:23.760 --> 01:02:25.880]   Miro.com, I don't know what that is.
[01:02:25.880 --> 01:02:29.480]   It's Niro.com/podcast.
[01:02:29.480 --> 01:02:30.760]   It's fun.
[01:02:30.760 --> 01:02:31.960]   I think a lot of it.
[01:02:31.960 --> 01:02:34.000]   It's visual, it's fun, it's exciting.
[01:02:34.000 --> 01:02:34.840]   Give it a try.
[01:02:34.840 --> 01:02:36.280]   Three boards free.
[01:02:36.280 --> 01:02:39.040]   Miro.com/podcast.
[01:02:39.040 --> 01:02:40.320]   On we go.
[01:02:40.320 --> 01:02:41.360]   Leave the commercial.
[01:02:41.360 --> 01:02:42.520]   Pardon me? Yes.
[01:02:42.520 --> 01:02:43.960]   Before you leave that ad,
[01:02:43.960 --> 01:02:47.240]   my colleague Jeremy Kaplan at the new, new, new, new, new art
[01:02:47.240 --> 01:02:48.320]   for the Graduate School of Journalism
[01:02:48.320 --> 01:02:50.720]   who's a brilliant educator and knows every tool out there.
[01:02:50.720 --> 01:02:51.840]   He loves Miro.
[01:02:51.840 --> 01:02:52.680]   Oh nice.
[01:02:52.680 --> 01:02:56.800]   And he brings it into class all the time,
[01:02:56.800 --> 01:03:00.200]   attaches it to events, attaches it to Zoom.
[01:03:00.200 --> 01:03:01.080]   There's all kinds of ways that--
[01:03:01.080 --> 01:03:02.080]   Yeah, yeah, yeah.
[01:03:02.080 --> 01:03:02.920]   It works.
[01:03:02.920 --> 01:03:05.480]   You can log in with Zoom and use it with your Zoom meetings.
[01:03:05.480 --> 01:03:10.720]   Here's a cover story mockups you could use
[01:03:10.720 --> 01:03:13.480]   to figure out what your cover story is going to look like
[01:03:13.480 --> 01:03:14.680]   or what your story is going to look like
[01:03:14.680 --> 01:03:17.000]   on all these different publications.
[01:03:17.000 --> 01:03:19.360]   There's just so many ways.
[01:03:19.360 --> 01:03:20.520]   Yeah.
[01:03:20.520 --> 01:03:22.800]   Yeah, I think for teaching it'd be really cool.
[01:03:22.800 --> 01:03:23.520]   It's very good.
[01:03:23.520 --> 01:03:24.360]   Yeah, it's like--
[01:03:24.360 --> 01:03:25.520]   The Ball's People in collaboration.
[01:03:25.520 --> 01:03:26.680]   Yeah.
[01:03:26.680 --> 01:03:28.120]   I'm actually--
[01:03:28.120 --> 01:03:30.480]   I'm excited about the idea of these new tools
[01:03:30.480 --> 01:03:32.080]   can give us new ways of working together,
[01:03:32.080 --> 01:03:33.680]   new ways of thinking even.
[01:03:33.680 --> 01:03:35.400]   That's pretty exciting.
[01:03:35.400 --> 01:03:41.520]   Meta has survived its first challenge
[01:03:41.520 --> 01:03:48.120]   and its acquisition of within the virtual reality startup.
[01:03:48.120 --> 01:03:55.600]   A judge at FTC asked for a judge to issue a restraining order
[01:03:55.600 --> 01:03:58.680]   preventing Meta from closing the deal, while the FTC is
[01:03:58.680 --> 01:04:00.440]   considering it.
[01:04:00.440 --> 01:04:03.520]   The judge said, no, no.
[01:04:03.520 --> 01:04:05.240]   I am not going to block it.
[01:04:05.240 --> 01:04:08.280]   US District Judge Edward Davila in an early--
[01:04:08.280 --> 01:04:09.960]   sealed decision early this morning.
[01:04:09.960 --> 01:04:13.160]   It's sealed, but of course immediately leaked.
[01:04:13.160 --> 01:04:15.920]   Leah Nyland Bloomberg had the story.
[01:04:15.920 --> 01:04:18.720]   Denied the FTC's request for a preliminary injunction
[01:04:18.720 --> 01:04:20.880]   to block the proposed transaction,
[01:04:20.880 --> 01:04:24.320]   while the agency pursues a separate case in its in-house
[01:04:24.320 --> 01:04:24.640]   court.
[01:04:24.640 --> 01:04:28.200]   So Meta will now be able to go ahead.
[01:04:28.200 --> 01:04:30.320]   But the FTC can still go after it.
[01:04:30.320 --> 01:04:31.200]   They can stop it later.
[01:04:31.200 --> 01:04:32.160]   They'll still do it or what?
[01:04:32.160 --> 01:04:35.280]   Yeah, they could say, OK, never mind.
[01:04:35.280 --> 01:04:36.880]   So I don't know.
[01:04:36.880 --> 01:04:38.440]   This is the--
[01:04:38.440 --> 01:04:43.120]   within makes a VR game called Supernatural
[01:04:43.120 --> 01:04:46.320]   that is a very, very good game, very popular.
[01:04:46.320 --> 01:04:47.480]   And it's all about--
[01:04:47.480 --> 01:04:49.080]   partly about fitness.
[01:04:49.080 --> 01:04:51.680]   And I think Meta really wants to get into this area.
[01:04:51.680 --> 01:04:54.800]   They already have a program, actually, that does this.
[01:04:54.800 --> 01:04:56.320]   So the argument is they're trying
[01:04:56.320 --> 01:04:57.960]   to do the same thing that they did with Instagram,
[01:04:57.960 --> 01:05:00.560]   which is put competitors or get rid of competitors
[01:05:00.560 --> 01:05:03.000]   by acquiring them.
[01:05:03.000 --> 01:05:05.040]   And so that's, I think, going to be the FTC's argument.
[01:05:05.040 --> 01:05:06.040]   We shall see.
[01:05:06.040 --> 01:05:11.200]   It's not like this space is just--
[01:05:11.200 --> 01:05:12.520]   there's a handful of competitors.
[01:05:12.520 --> 01:05:15.480]   I think there's quite a few, but no, it's not
[01:05:15.480 --> 01:05:18.680]   a monopoly issue.
[01:05:18.680 --> 01:05:22.680]   We talked about Elon Jet, the Elon flight tracker,
[01:05:22.680 --> 01:05:24.400]   which upset Elon so much.
[01:05:24.400 --> 01:05:28.520]   She called it Assassination Coordinates Band,
[01:05:28.520 --> 01:05:30.880]   Jack Sweeney, the college kid from Florida
[01:05:30.880 --> 01:05:34.840]   who does the Elon Jet account from Twitter.
[01:05:34.840 --> 01:05:37.000]   And then when he moved to Maston,
[01:05:37.000 --> 01:05:40.920]   band mention of Maston on Twitter,
[01:05:40.920 --> 01:05:42.600]   he's backed down on that a little bit.
[01:05:42.600 --> 01:05:45.840]   But the flight tracker that Elon Jet and these other Jet
[01:05:45.840 --> 01:05:51.760]   tracking accounts use is called ADSB Exchange.
[01:05:51.760 --> 01:05:53.000]   And it's an interesting story.
[01:05:53.000 --> 01:05:56.360]   This is kind of a collaborative effort.
[01:05:56.360 --> 01:05:59.840]   Turns out, all planes have transponders
[01:05:59.840 --> 01:06:03.080]   identifying them by tail number.
[01:06:03.080 --> 01:06:05.320]   And other planes use this--
[01:06:05.320 --> 01:06:07.480]   I presume traffic control and flight control
[01:06:07.480 --> 01:06:08.920]   uses it as well.
[01:06:08.920 --> 01:06:10.080]   So they know what's up there.
[01:06:10.080 --> 01:06:13.080]   But there is no publicly available database of this.
[01:06:13.080 --> 01:06:16.160]   So what the guy who owns ADSB exchanged it
[01:06:16.160 --> 01:06:18.520]   is he got people to put up little radio receivers
[01:06:18.520 --> 01:06:20.800]   all over the globe and report in.
[01:06:20.800 --> 01:06:22.360]   He set up software that would automatically
[01:06:22.360 --> 01:06:23.640]   let them report in.
[01:06:23.640 --> 01:06:27.120]   And eventually they were able to put thousands of trackers
[01:06:27.120 --> 01:06:28.240]   build a pretty--
[01:06:28.240 --> 01:06:29.840]   Decentralized flight tracking.
[01:06:29.840 --> 01:06:30.720]   Yeah.
[01:06:30.720 --> 01:06:36.880]   They put a really great open platform for this.
[01:06:36.880 --> 01:06:42.400]   Unfortunately, it was owned by one guy.
[01:06:42.400 --> 01:06:45.920]   The costs were high, I imagine, for running this.
[01:06:45.920 --> 01:06:47.320]   He created, I guess, the software.
[01:06:47.320 --> 01:06:49.680]   Dan Strouffer is his name.
[01:06:49.680 --> 01:06:53.440]   He created the software to collect this information
[01:06:53.440 --> 01:06:55.840]   and built this site.
[01:06:55.840 --> 01:07:01.560]   But while revenue is increasing, costs are also increasing.
[01:07:01.560 --> 01:07:07.400]   So last month, as the site was getting all that attention
[01:07:07.400 --> 01:07:10.760]   for Elon Jet, the site's founder and sole owners
[01:07:10.760 --> 01:07:12.560]   announced he was planning--
[01:07:12.560 --> 01:07:14.120]   actually, there were rumors that he was planning
[01:07:14.120 --> 01:07:18.120]   to sell the website to JetNet.
[01:07:18.120 --> 01:07:19.760]   He's the sole owner he did.
[01:07:19.760 --> 01:07:29.120]   And after the deal became public, last week,
[01:07:29.120 --> 01:07:31.760]   there was a mutiny.
[01:07:31.760 --> 01:07:33.280]   Shortly after the deal became public,
[01:07:33.280 --> 01:07:35.040]   Strouffer was removed from the discord
[01:07:35.040 --> 01:07:37.320]   as the site's users contemplated their next move.
[01:07:37.320 --> 01:07:47.080]   A revolt was led by James Stanford.
[01:07:47.080 --> 01:07:49.640]   A revolt just against the idea?
[01:07:49.640 --> 01:07:50.440]   Or they're not?
[01:07:50.440 --> 01:07:53.400]   No, no, because JetNet had previously
[01:07:53.400 --> 01:07:56.520]   been purchased by our favorite villain of the week,
[01:07:56.520 --> 01:07:59.040]   private equity.
[01:07:59.040 --> 01:08:02.800]   And so James Stanford, who is one of ADSB exchanges,
[01:08:02.800 --> 01:08:07.840]   senior administrators, said, hey, this is not good.
[01:08:07.840 --> 01:08:11.360]   You do this, and you're going to lose all of your volunteers
[01:08:11.360 --> 01:08:14.960]   who put these radios up and put this signal.
[01:08:14.960 --> 01:08:15.960]   They're volunteers.
[01:08:15.960 --> 01:08:17.360]   They're just take out--
[01:08:17.360 --> 01:08:19.840]   that's a good indicator to private equity
[01:08:19.840 --> 01:08:22.200]   that maybe they don't want to invest in decentralized things.
[01:08:22.200 --> 01:08:22.600]   Yeah.
[01:08:22.600 --> 01:08:27.040]   Stanford wrote ADSBExchange.com is done.
[01:08:27.040 --> 01:08:32.000]   He posted instructions on how to unplug from the network.
[01:08:32.000 --> 01:08:33.960]   Some-- many follow those instructions,
[01:08:33.960 --> 01:08:35.800]   according to ours, technical, with some flipping over
[01:08:35.800 --> 01:08:39.080]   to some smaller alternatives, like airframes.
[01:08:39.080 --> 01:08:42.520]   In a span of a few hours, it went from 11,000 feeters
[01:08:42.520 --> 01:08:44.640]   to 9,500.
[01:08:44.640 --> 01:08:46.960]   Jack Sweeney, the guy who ran Elon Jet, said,
[01:08:46.960 --> 01:08:49.080]   today is a sad day.
[01:08:49.080 --> 01:08:53.800]   If you defeat ADSBExchange, we encourage you to stop.
[01:08:53.800 --> 01:08:55.640]   ADSBExchange is found on the principles
[01:08:55.640 --> 01:09:02.680]   of hobbyist communities, not for-profit public equity firms.
[01:09:02.680 --> 01:09:05.000]   So yeah, don't--
[01:09:05.000 --> 01:09:07.520]   It reminds me-- remember CDDP, which was, again,
[01:09:07.520 --> 01:09:08.040]   the same thing.
[01:09:08.040 --> 01:09:11.280]   People uploaded-- they would burn their CDs
[01:09:11.280 --> 01:09:12.760]   and then upload track information
[01:09:12.760 --> 01:09:15.240]   to the centralized database so that all could use it.
[01:09:15.240 --> 01:09:17.240]   IMDB, same idea.
[01:09:17.240 --> 01:09:18.960]   These were all acquired.
[01:09:18.960 --> 01:09:22.800]   And then companies profited off of these volunteer efforts.
[01:09:22.800 --> 01:09:24.920]   Stacy's point is really interesting here.
[01:09:24.920 --> 01:09:31.800]   If you look at Mastodon, and you hope
[01:09:31.800 --> 01:09:35.720]   that there is some investment in some of the tools around it,
[01:09:35.720 --> 01:09:38.960]   but your point, Stacy, is that is an investor
[01:09:38.960 --> 01:09:41.800]   going to be scared of anything that can bring revolt.
[01:09:41.800 --> 01:09:43.240]   Right?
[01:09:43.240 --> 01:09:43.680]   Well, yeah.
[01:09:43.680 --> 01:09:47.480]   I mean, a lot of these people--
[01:09:47.480 --> 01:09:49.320]   I mean, it's kind of a tragedy at the commons
[01:09:49.320 --> 01:09:51.320]   in the software world, I guess.
[01:09:51.320 --> 01:09:51.840]   Yes.
[01:09:51.840 --> 01:09:53.960]   You might start building something
[01:09:53.960 --> 01:09:55.360]   because you're interested in it.
[01:09:55.360 --> 01:09:57.000]   But at a certain point in time, there
[01:09:57.000 --> 01:09:59.640]   are costs and effort that you may not be willing to give.
[01:09:59.640 --> 01:10:02.600]   So maybe you open source it, give it out.
[01:10:02.600 --> 01:10:05.360]   Maybe you invest-- you decide to invest in it
[01:10:05.360 --> 01:10:06.680]   and build it for the community.
[01:10:06.680 --> 01:10:11.080]   But once you start doing that, then
[01:10:11.080 --> 01:10:12.720]   there's the very real question, like,
[01:10:12.720 --> 01:10:14.160]   hey, when I die, what happens?
[01:10:14.160 --> 01:10:17.760]   Or if I get sick of doing this, how do I get out?
[01:10:17.760 --> 01:10:20.280]   And if someone comes up and offers you several million
[01:10:20.280 --> 01:10:21.320]   dollars or whatever--
[01:10:21.320 --> 01:10:23.120]   $20 million.
[01:10:23.120 --> 01:10:24.400]   $20 million.
[01:10:24.400 --> 01:10:26.400]   I mean, yeah, they're going to take the money.
[01:10:26.400 --> 01:10:26.880]   Now--
[01:10:26.880 --> 01:10:28.040]   Furthermore, they now--
[01:10:28.040 --> 01:10:28.600]   It's private equity.
[01:10:28.600 --> 01:10:29.920]   Yeah, it's private equity.
[01:10:29.920 --> 01:10:33.240]   They now have to recoup their $20 million.
[01:10:33.240 --> 01:10:35.680]   Yeah, but if you're a private equity fund,
[01:10:35.680 --> 01:10:39.640]   I don't know why, if you're doing your due diligence,
[01:10:39.640 --> 01:10:42.360]   I don't understand why you would invest that much
[01:10:42.360 --> 01:10:45.760]   in something that is reliant on--
[01:10:45.760 --> 01:10:46.120]   Well--
[01:10:46.120 --> 01:10:47.320]   Your core value is reliant on--
[01:10:47.320 --> 01:10:49.000]   Well, Elon might have written a check.
[01:10:49.000 --> 01:10:51.880]   Elon was so upset about Flight Tracker.
[01:10:51.880 --> 01:10:53.080]   I could see him saying--
[01:10:53.080 --> 01:10:54.200]   OK, just take it out.
[01:10:54.200 --> 01:10:55.160]   Take it out, boys.
[01:10:55.160 --> 01:10:59.120]   Flight aware, which is a commercial site and flight radar,
[01:10:59.120 --> 01:11:02.560]   they do the same thing for money.
[01:11:02.560 --> 01:11:03.160]   Maybe they just--
[01:11:03.160 --> 01:11:04.040]   Yeah, I was going to say that.
[01:11:04.040 --> 01:11:06.400]   I mean, we have Flight Tracker and Flight Aware.
[01:11:06.400 --> 01:11:11.920]   They-- I mean, there are services that do this, so--
[01:11:11.920 --> 01:11:13.000]   Yeah, they're all commercial.
[01:11:13.000 --> 01:11:15.520]   I guess it just seems like a bad investment from a private--
[01:11:15.520 --> 01:11:16.040]   I agree.
[01:11:16.040 --> 01:11:16.720]   --prespectively.
[01:11:16.720 --> 01:11:17.720]   Yeah, that's a good point.
[01:11:17.720 --> 01:11:18.040]   Yeah.
[01:11:18.040 --> 01:11:18.560]   I agree.
[01:11:18.560 --> 01:11:19.360]   Someone that's why I think--
[01:11:19.360 --> 01:11:22.520]   The ADSB site and there's no mention of the acquisition
[01:11:22.520 --> 01:11:24.800]   or anything, not even in their forums.
[01:11:24.800 --> 01:11:27.360]   How recent was this last week?
[01:11:27.360 --> 01:11:29.520]   Last week or so.
[01:11:29.520 --> 01:11:31.400]   You would think they would have something up to say,
[01:11:31.400 --> 01:11:33.760]   you know what, things are going to be changing.
[01:11:33.760 --> 01:11:35.240]   Well, maybe they don't want to--
[01:11:35.240 --> 01:11:36.360]   I mean, look, again, it's a volunteer.
[01:11:36.360 --> 01:11:39.800]   It's a volunteer force of people doing this.
[01:11:39.800 --> 01:11:41.800]   Mm.
[01:11:41.800 --> 01:11:45.600]   And when all that went down with Elon Musk and his private jet
[01:11:45.600 --> 01:11:48.800]   or the family's private jet or what have you,
[01:11:48.800 --> 01:11:55.080]   did he publicly announce that, look, I don't want this to be tracked?
[01:11:55.080 --> 01:11:57.120]   Because I thought there were some type of procedures to--
[01:11:57.120 --> 01:11:58.400]   So you could do that, yeah.
[01:11:58.400 --> 01:12:02.600]   Did he follow through with that and still had that information put out
[01:12:02.600 --> 01:12:05.160]   there by these tracking systems?
[01:12:05.160 --> 01:12:05.880]   That's not cool.
[01:12:05.880 --> 01:12:06.880]   That's the case.
[01:12:06.880 --> 01:12:12.840]   Well, there's nothing to do about it,
[01:12:12.840 --> 01:12:19.120]   except a lot of celebrities do, which is anonymously rent jets.
[01:12:19.120 --> 01:12:21.640]   So the tail number is not associated directly with you.
[01:12:21.640 --> 01:12:25.240]   Elon owns that jet, I guess.
[01:12:25.240 --> 01:12:25.520]   But--
[01:12:25.520 --> 01:12:26.800]   I thought-- I think I answered it.
[01:12:26.800 --> 01:12:30.200]   I thought I read something that said, I have no link, so I don't know.
[01:12:30.200 --> 01:12:33.560]   I'm just like a chat GPT without facts.
[01:12:33.560 --> 01:12:36.560]   That said that you could withdraw your tail
[01:12:36.560 --> 01:12:39.120]   from these reporting things.
[01:12:39.120 --> 01:12:43.800]   You can unless you have enthusiasts on the ground tracking
[01:12:43.800 --> 01:12:44.560]   your transponder.
[01:12:44.560 --> 01:12:46.600]   You can't turn off the transponder.
[01:12:46.600 --> 01:12:47.120]   Ah.
[01:12:47.120 --> 01:12:47.680]   That can't--
[01:12:47.680 --> 01:12:48.200]   Right.
[01:12:48.200 --> 01:12:51.400]   You have to do it through a formal, like, a license service.
[01:12:51.400 --> 01:12:53.520]   But if it's an unlicensed service, tracking it--
[01:12:53.520 --> 01:12:54.240]   I see.
[01:12:54.240 --> 01:12:54.880]   Who cares?
[01:12:54.880 --> 01:12:56.360]   Yeah.
[01:12:56.360 --> 01:12:57.840]   Yeah, I mean, it's just people on the ground.
[01:12:57.840 --> 01:13:00.080]   It is illegal not to have a transponder.
[01:13:00.080 --> 01:13:00.440]   Yeah.
[01:13:00.440 --> 01:13:02.640]   Yeah.
[01:13:02.640 --> 01:13:04.680]   Because otherwise planes would run into each other.
[01:13:04.680 --> 01:13:05.200]   Right.
[01:13:05.200 --> 01:13:05.880]   Got it.
[01:13:05.880 --> 01:13:08.720]   OK.
[01:13:08.720 --> 01:13:13.400]   The tracking-- I'm reading-- so this is an article from NBC
[01:13:13.400 --> 01:13:14.320]   when this all happened.
[01:13:14.320 --> 01:13:18.920]   How is it legal to track private planes?
[01:13:18.920 --> 01:13:20.920]   How could that be real?
[01:13:20.920 --> 01:13:23.320]   The tracking capabilities are possible thanks
[01:13:23.320 --> 01:13:28.440]   to a technology called ADBSB, or Automatic Dependent Surveillance
[01:13:28.440 --> 01:13:30.240]   Broadcast.
[01:13:30.240 --> 01:13:31.080]   Everyone--
[01:13:31.080 --> 01:13:32.080]   Can I hear a signal?
[01:13:32.080 --> 01:13:33.200]   Three seconds.
[01:13:33.200 --> 01:13:35.640]   Every three seconds ADSB equipment on a plane
[01:13:35.640 --> 01:13:39.040]   sends out real-time data about the aircraft's location,
[01:13:39.040 --> 01:13:43.680]   altitude, and velocity to avoid mid-air collisions
[01:13:43.680 --> 01:13:45.840]   and to allow people on the ground to know where an airplane is
[01:13:45.840 --> 01:13:48.920]   at all times.
[01:13:48.920 --> 01:13:54.040]   ADSB technology is mandated by the FAA on all aircraft.
[01:13:54.040 --> 01:13:57.760]   It should be at all self-driving cars, too.
[01:13:57.760 --> 01:13:58.320]   Right.
[01:13:58.320 --> 01:14:00.400]   So this is the difference.
[01:14:00.400 --> 01:14:02.600]   You probably could go to flight-aware or flight tracker
[01:14:02.600 --> 01:14:05.800]   and say, hey, please don't put my plane up there.
[01:14:05.800 --> 01:14:06.720]   They're commercial entities.
[01:14:06.720 --> 01:14:07.040]   But--
[01:14:07.040 --> 01:14:08.680]   I'm sure the CIA knows that.
[01:14:08.680 --> 01:14:09.560]   Yeah.
[01:14:09.560 --> 01:14:11.800]   But you can't do that with ADBSB exchange,
[01:14:11.800 --> 01:14:13.760]   because it's just a bunch of enthusiasts with radios.
[01:14:13.760 --> 01:14:18.960]   Dan Strouffer had said, all these folks send data in.
[01:14:18.960 --> 01:14:21.280]   We aggregate the info and put it on a map.
[01:14:21.280 --> 01:14:23.320]   The purpose of this is not for being a paparazzi.
[01:14:23.320 --> 01:14:26.920]   It's for aviation enthusiasts.
[01:14:26.920 --> 01:14:29.120]   There's a really cool plane heading your way.
[01:14:29.120 --> 01:14:31.920]   But it sure did sound like paparazzi, though, in his story.
[01:14:31.920 --> 01:14:33.400]   He says, we publish everything.
[01:14:33.400 --> 01:14:34.920]   It's already public data.
[01:14:34.920 --> 01:14:36.280]   We're not sharing anything you couldn't
[01:14:36.280 --> 01:14:38.040]   find from many other sources.
[01:14:38.040 --> 01:14:40.360]   Flight radar and flight-aware do actively
[01:14:40.360 --> 01:14:44.160]   suppress flight information about planes whose operators have
[01:14:44.160 --> 01:14:46.400]   asked the FAA to block their registration number
[01:14:46.400 --> 01:14:47.720]   from public view.
[01:14:47.720 --> 01:14:51.120]   But the ADSB exchange is not using the FAA feeds.
[01:14:51.120 --> 01:14:52.720]   So it's just enthusiasts.
[01:14:52.720 --> 01:14:55.160]   Yeah, so I was explaining.
[01:14:55.160 --> 01:14:56.800]   Paps probably do use it.
[01:14:56.800 --> 01:14:57.320]   Yeah.
[01:14:57.320 --> 01:15:00.240]   I mean, a journalist would use any tool available to them
[01:15:00.240 --> 01:15:01.480]   to get this guy.
[01:15:01.480 --> 01:15:07.480]   And there's a public interest in maybe not tracking Elon.
[01:15:07.480 --> 01:15:12.280]   But even like Wall Street firms, knowing that a CEO has
[01:15:12.280 --> 01:15:13.440]   flown--
[01:15:13.440 --> 01:15:22.540]   Dis
[01:15:22.540 --> 01:15:26.280]   Shino, like I do.
[01:15:26.280 --> 01:15:28.740]   Apparently, the FAA does have a program
[01:15:28.740 --> 01:15:32.260]   that allows planes to encode their ADSB signals
[01:15:32.260 --> 01:15:34.800]   so that they cannot be matched to other information
[01:15:34.800 --> 01:15:39.940]   in the publicly available civil aviation registry.
[01:15:39.940 --> 01:15:43.660]   It's unclear, according to NBC, whether Musk participates
[01:15:43.660 --> 01:15:45.820]   in that plocking program.
[01:15:45.820 --> 01:15:49.540]   And he says, Strouford says, in many cases,
[01:15:49.540 --> 01:15:51.660]   other high-profile people don't appear
[01:15:51.660 --> 01:15:54.140]   to take advantage of such programs, which
[01:15:54.140 --> 01:15:56.060]   is surprising, Strouford said, a lot
[01:15:56.060 --> 01:15:57.980]   are vocal about the privacy issues.
[01:15:57.980 --> 01:16:00.060]   And sometimes they don't seem to be doing all they can
[01:16:00.060 --> 01:16:00.820]   to prevent this.
[01:16:00.820 --> 01:16:03.220]   So you could do that.
[01:16:03.220 --> 01:16:04.500]   All right.
[01:16:04.500 --> 01:16:06.420]   And still be compliant with the law.
[01:16:06.420 --> 01:16:08.220]   So I don't know why that might only be--
[01:16:08.220 --> 01:16:11.220]   Maybe they just haven't had their personal assistance.
[01:16:11.220 --> 01:16:12.780]   They're not aware to send them off to do so.
[01:16:12.780 --> 01:16:13.700]   That's exactly right.
[01:16:13.700 --> 01:16:14.700]   All right.
[01:16:14.700 --> 01:16:17.100]   That's exactly right.
[01:16:17.100 --> 01:16:18.220]   I was curious.
[01:16:18.220 --> 01:16:21.220]   Big victory against ransomware.
[01:16:21.220 --> 01:16:25.180]   We talked about this yesterday on security now, the FBI.
[01:16:25.180 --> 01:16:30.420]   So Hive, which is a ransomware as a service--
[01:16:30.420 --> 01:16:33.580]   I hate to say company, but that's what they do.
[01:16:33.580 --> 01:16:38.580]   They have affiliates who are people like you and me
[01:16:38.580 --> 01:16:41.340]   who decide they want to make a little money on the side.
[01:16:41.340 --> 01:16:43.940]   And Hive provides the software.
[01:16:43.940 --> 01:16:47.660]   And I guess helpful information will let you attack people
[01:16:47.660 --> 01:16:48.940]   using their software.
[01:16:48.940 --> 01:16:51.740]   You give them back a percentage of your income
[01:16:51.740 --> 01:16:53.420]   from ransomware.
[01:16:53.420 --> 01:16:57.260]   Hive is one of the bigger ransomware as a service groups,
[01:16:57.260 --> 01:17:00.900]   like Revil and others we've talked about on security now.
[01:17:00.900 --> 01:17:05.500]   The DOJ revealed this week that the FBI had been
[01:17:05.500 --> 01:17:11.340]   infiltrated the Hive ransomware group in July of last year.
[01:17:11.340 --> 01:17:15.100]   And that for the last six months, they've been saving people.
[01:17:15.100 --> 01:17:17.820]   They gave out more than 300 decryption keys
[01:17:17.820 --> 01:17:22.580]   to those hacked, so they wouldn't have to pay the ransomware.
[01:17:22.580 --> 01:17:23.220]   Nice.
[01:17:23.220 --> 01:17:24.060]   Aww.
[01:17:24.060 --> 01:17:24.580]   Aww.
[01:17:24.580 --> 01:17:25.660]   So nice.
[01:17:25.660 --> 01:17:26.780]   Hive is nasty.
[01:17:26.780 --> 01:17:29.780]   They were attacking schools, hospitals, financial companies,
[01:17:29.780 --> 01:17:31.780]   even infrastructure.
[01:17:31.780 --> 01:17:34.060]   In more than 80 countries around the world,
[01:17:34.060 --> 01:17:37.060]   one hospital was left unable to accept new patients,
[01:17:37.060 --> 01:17:38.780]   thanks to Hive.
[01:17:38.780 --> 01:17:42.700]   The FBI worked with a number of different nations,
[01:17:42.700 --> 01:17:46.900]   law enforcement agencies, 50 organizations.
[01:17:46.900 --> 01:17:51.580]   In the UK, for instance, they gave 50 companies decryption keys.
[01:17:51.580 --> 01:17:55.660]   They also were able to warn people that you're next
[01:17:55.660 --> 01:17:57.580]   so they could lock themselves down.
[01:17:57.580 --> 01:17:59.500]   You got to do this carefully because they didn't want the Hive
[01:17:59.500 --> 01:18:01.340]   people know that they'd infiltrated them, right?
[01:18:01.340 --> 01:18:03.420]   It's like knowing the Enigma machine.
[01:18:03.420 --> 01:18:05.620]   You can't be too smart.
[01:18:05.620 --> 01:18:10.140]   Thursday, they ended the operation taking down Hive's website
[01:18:10.140 --> 01:18:12.820]   and communications networks with the help of police forces
[01:18:12.820 --> 01:18:14.420]   in Germany and the Netherlands.
[01:18:14.420 --> 01:18:18.020]   Hive, its belief, was operating out of Russia.
[01:18:18.020 --> 01:18:20.260]   They didn't arrest anybody, right?
[01:18:20.260 --> 01:18:21.420]   Because they're in Russia.
[01:18:21.420 --> 01:18:22.340]   Yeah.
[01:18:22.340 --> 01:18:23.860]   One could start-- the Hive could start up again
[01:18:23.860 --> 01:18:24.860]   and start doing it.
[01:18:24.860 --> 01:18:28.300]   That's a good-- and it happens all the time.
[01:18:28.300 --> 01:18:32.460]   Mandiant intelligence head, John Holtquist, said,
[01:18:32.460 --> 01:18:35.460]   until the group is arrested, they will never truly be gone.
[01:18:35.460 --> 01:18:37.740]   They will have to reconstitute, which takes time,
[01:18:37.740 --> 01:18:39.620]   but I bet they reappear in time.
[01:18:39.620 --> 01:18:41.260]   As of now, if you're a Hive affiliate
[01:18:41.260 --> 01:18:43.620]   and you go to the Hive website, you'll see this.
[01:18:43.620 --> 01:18:46.860]   This hidden site has been seized.
[01:18:46.860 --> 01:18:48.980]   The FBI has seized this site.
[01:18:48.980 --> 01:18:50.420]   It looks like malware.
[01:18:50.420 --> 01:18:52.460]   It looks like malware, doesn't it?
[01:18:52.460 --> 01:18:54.300]   Yeah.
[01:18:54.300 --> 01:18:56.460]   Holotai Bongitberg.
[01:18:56.460 --> 01:18:59.340]   Yeah.
[01:18:59.340 --> 01:19:04.100]   They did catch the Reeval guys last year, actually in 2021.
[01:19:04.100 --> 01:19:08.260]   So they are able to arrest them in some cases.
[01:19:08.260 --> 01:19:11.580]   The problem is I think Russia protects these--
[01:19:11.580 --> 01:19:14.460]   ransom-- this is another kind of cold war
[01:19:14.460 --> 01:19:16.420]   that they wage against the West.
[01:19:16.420 --> 01:19:20.900]   And so I believe these groups are protected often by Russia.
[01:19:20.900 --> 01:19:23.140]   But more and more, we're seeing countries fight back.
[01:19:23.140 --> 01:19:27.300]   Australia says, now we're going to hack the hackers.
[01:19:27.300 --> 01:19:29.820]   We'll see how that goes.
[01:19:29.820 --> 01:19:31.300]   Anyway, success.
[01:19:31.300 --> 01:19:32.220]   Well done, Bravo.
[01:19:36.660 --> 01:19:41.460]   Yeah, if Williams says-- I'm sorry, Biz Stone, not Ev.
[01:19:41.460 --> 01:19:45.340]   Biz Stone says, one of the founders of Twitter,
[01:19:45.340 --> 01:19:48.220]   Elon doesn't really seem like the right person to own Twitter.
[01:19:48.220 --> 01:19:55.580]   I love how these Twitter, former Twitter folks
[01:19:55.580 --> 01:19:58.700]   are so reluctant to say really nasty things like--
[01:19:58.700 --> 01:20:00.900]   what a jerk.
[01:20:00.900 --> 01:20:04.340]   Stone says, running social media companies
[01:20:04.340 --> 01:20:07.900]   is not really a win-win situation.
[01:20:07.900 --> 01:20:11.620]   It's always tough because 50% of the people are going to be happy.
[01:20:11.620 --> 01:20:14.340]   50% of the people are going to be upset with you.
[01:20:14.340 --> 01:20:16.540]   You have to be OK with stuff you just don't like
[01:20:16.540 --> 01:20:18.060]   or don't agree with being on there.
[01:20:18.060 --> 01:20:21.260]   Otherwise, you should just go buy a magazine or a newspaper
[01:20:21.260 --> 01:20:25.020]   or something where it's OK to have a specific leaning.
[01:20:25.020 --> 01:20:31.860]   When Biz was asked by the Guardian in an interview
[01:20:31.860 --> 01:20:36.140]   whether Musk was the right owner for Twitter, he said,
[01:20:36.140 --> 01:20:40.620]   doesn't seem like it right now, but I could be wrong.
[01:20:40.620 --> 01:20:44.980]   He doesn't seem like he's having fun.
[01:20:44.980 --> 01:20:48.780]   Doesn't seem like he's having a lot of fun.
[01:20:48.780 --> 01:20:50.540]   So I'm reading a really good book right now,
[01:20:50.540 --> 01:20:52.220]   which I've mentioned right over the time,
[01:20:52.220 --> 01:20:55.660]   Taunt Gillespie's Custodians of the Internet, which
[01:20:55.660 --> 01:20:57.620]   is all about moderation.
[01:20:57.620 --> 01:21:00.260]   And what he says in the beginning,
[01:21:00.260 --> 01:21:02.460]   because that's all I've read so far through the interesting,
[01:21:02.460 --> 01:21:05.380]   is the mistake that Twitter and Facebook at all made
[01:21:05.380 --> 01:21:08.500]   was they didn't realize that they were moderation companies.
[01:21:08.500 --> 01:21:10.380]   They weren't hosting companies.
[01:21:10.380 --> 01:21:14.140]   But the essence of what they do is moderation.
[01:21:14.140 --> 01:21:16.860]   And they need to acknowledge that and admit that
[01:21:16.860 --> 01:21:18.740]   and budget for that and build around that.
[01:21:18.740 --> 01:21:19.740]   And they didn't.
[01:21:19.740 --> 01:21:26.700]   Well, it's just what Amy Webb, one of our favorite
[01:21:26.700 --> 01:21:29.740]   contributors in our shows, she's a futurist, says.
[01:21:29.740 --> 01:21:33.740]   Companies should be doing strategic planning for the future.
[01:21:33.740 --> 01:21:36.780]   They need to think, they can't just do a quarter by quarter.
[01:21:36.780 --> 01:21:40.540]   They really have to look ahead at the risks.
[01:21:40.540 --> 01:21:43.100]   Deeply understand what the risks are of the businesses
[01:21:43.100 --> 01:21:45.340]   that they're in and plan for it.
[01:21:45.340 --> 01:21:50.140]   Wouldn't you say that, I think Google does that?
[01:21:50.140 --> 01:21:52.340]   Well, you wrote the book, what would Google do?
[01:21:52.340 --> 01:21:53.420]   You know better than that.
[01:21:53.420 --> 01:21:55.060]   Facebook thinks it's doing that what they are VR
[01:21:55.060 --> 01:21:56.660]   and giving up on social.
[01:21:56.660 --> 01:21:58.540]   I think they kind of do that.
[01:21:58.540 --> 01:22:00.740]   I mean, Twitter right now is not.
[01:22:00.740 --> 01:22:02.540]   Not that's because of the musket.
[01:22:02.540 --> 01:22:03.540]   Right.
[01:22:03.540 --> 01:22:11.860]   This is an old story, but I became aware of it
[01:22:11.860 --> 01:22:14.620]   thanks to Mike Masnick on Tech Dirt.
[01:22:14.620 --> 01:22:16.460]   Just down the road from you, Stacey,
[01:22:16.460 --> 01:22:18.980]   the Seattle School District.
[01:22:18.980 --> 01:22:21.980]   You tried to talk about this last week and I told you no.
[01:22:21.980 --> 01:22:24.460]   Okay, never mind, never mind.
[01:22:24.460 --> 01:22:26.180]   I'm sorry I brought it up.
[01:22:26.180 --> 01:22:27.700]   You see, it's not a democracy.
[01:22:27.700 --> 01:22:29.740]   If he wants to talk about it, he could talk about it.
[01:22:29.740 --> 01:22:30.580]   Don't cancel.
[01:22:30.580 --> 01:22:31.420]   No, I'll listen to her.
[01:22:31.420 --> 01:22:32.820]   I won't listen to you, but I will listen.
[01:22:32.820 --> 01:22:33.740]   I know that.
[01:22:33.740 --> 01:22:37.820]   I mean, if there was a snowball's chance in hell,
[01:22:37.820 --> 01:22:41.540]   if this was more than just like pandering to a base,
[01:22:41.540 --> 01:22:42.580]   I'd be like, yeah, let's do it.
[01:22:42.580 --> 01:22:46.820]   But there's so in social media because it's bad for the children.
[01:22:46.820 --> 01:22:47.940]   All right, all right.
[01:22:47.940 --> 01:22:52.220]   I did not watch, I did not attend this morning's Samsung
[01:22:52.220 --> 01:22:57.500]   Galaxy Unpacked event in which they announced the S23
[01:22:57.500 --> 01:23:00.780]   phone with the 200 megapixel sensor.
[01:23:00.780 --> 01:23:03.580]   Stop.
[01:23:03.580 --> 01:23:04.980]   Did you watch it, Ant?
[01:23:04.980 --> 01:23:06.060]   Did anybody?
[01:23:06.060 --> 01:23:07.060]   Anybody?
[01:23:07.060 --> 01:23:10.220]   I caught some of it because I wanted to hear Mr.
[01:23:10.220 --> 01:23:12.140]   House thoughts.
[01:23:12.140 --> 01:23:15.220]   You know, but as you had said previously that most of the
[01:23:15.220 --> 01:23:18.820]   stuff about the phone was already leaked on like whatever.
[01:23:18.820 --> 01:23:22.380]   I feel bad because I just, I didn't show up because I thought
[01:23:22.380 --> 01:23:23.780]   it's just going to be a pep rally.
[01:23:23.780 --> 01:23:26.580]   I don't know.
[01:23:26.580 --> 01:23:28.660]   But I want to hear Mr. Jason's thoughts.
[01:23:28.660 --> 01:23:34.180]   And what I did catch was some talk about XR.
[01:23:34.180 --> 01:23:38.340]   And I wanted to hear Miss Miss Stacy's thoughts because they
[01:23:38.340 --> 01:23:40.980]   talked about matter and this being the year of matter.
[01:23:40.980 --> 01:23:44.700]   And you know, what does she think of the announcement today?
[01:23:44.700 --> 01:23:47.540]   Oh, I didn't hear the matter, Bitts.
[01:23:47.540 --> 01:23:48.900]   Now I feel bad.
[01:23:48.900 --> 01:23:56.180]   I was, we, it started as we finished doing our show this
[01:23:56.180 --> 01:23:57.260]   morning recording our show.
[01:23:57.260 --> 01:24:02.580]   So I was going to turn it on, but then I was, I was editing
[01:24:02.580 --> 01:24:08.580]   something even, even so this, this article on Gizmodo,
[01:24:08.580 --> 01:24:13.300]   everything Samsung announced at Galaxy and PEC 2023 is one
[01:24:13.300 --> 01:24:14.260]   paragraph.
[01:24:14.260 --> 01:24:20.900]   Oh, that's, it's, but I'm looking for more.
[01:24:20.900 --> 01:24:21.380]   That's it.
[01:24:21.380 --> 01:24:22.180]   It's one paragraph.
[01:24:22.180 --> 01:24:25.860]   They announced three new versions of the Galaxy S23.
[01:24:25.860 --> 01:24:28.500]   And three new windows laptops.
[01:24:28.500 --> 01:24:29.940]   That's it.
[01:24:29.940 --> 01:24:31.140]   Oh, I thought there were Chromebooks.
[01:24:31.140 --> 01:24:33.220]   They were actual windows devices.
[01:24:33.220 --> 01:24:34.580]   Yeah, I think so.
[01:24:34.580 --> 01:24:35.540]   Are your Chromebooks?
[01:24:35.540 --> 01:24:36.020]   Chromebooks?
[01:24:36.020 --> 01:24:36.500]   No, no, no.
[01:24:36.500 --> 01:24:40.580]   No, they're called Galaxy books, but they're windows based.
[01:24:40.580 --> 01:24:42.500]   So I could see the confusion there.
[01:24:42.500 --> 01:24:45.860]   Three new Galaxy book three offerings, including an
[01:24:45.860 --> 01:24:48.660]   ultra tier starting at 2,400 bucks.
[01:24:48.660 --> 01:24:51.220]   Everything.
[01:24:51.220 --> 01:24:53.700]   Here's, here's the long version of the article.
[01:24:53.700 --> 01:24:54.660]   It's just a picture.
[01:24:55.300 --> 01:24:57.460]   Well, come, geez Louise.
[01:24:57.460 --> 01:25:02.820]   Well, I will say, is part of the whole XR stuff?
[01:25:02.820 --> 01:25:08.580]   And they did have Google folks and Qualcomm folks.
[01:25:08.580 --> 01:25:13.620]   I expect at Mobile World Congress coming up in February
[01:25:13.620 --> 01:25:17.620]   that Qualcomm's going to do a bunch of AR compatible chips.
[01:25:17.620 --> 01:25:19.140]   So I've been talking to some people.
[01:25:19.140 --> 01:25:23.700]   I'm actually trying out these glasses.
[01:25:25.060 --> 01:25:29.140]   Well, I'm trying out a service, but I had to buy glasses to try out this AR service.
[01:25:29.140 --> 01:25:33.780]   And they only work with a very limited number of Qualcomm chips.
[01:25:33.780 --> 01:25:38.260]   And I expect Qualcomm to make some announcements at Mobile World Congress related to that,
[01:25:38.260 --> 01:25:47.060]   especially given that Cristiano was up on stage at the Samsung event.
[01:25:47.060 --> 01:25:54.340]   And the Samsung phones are one of the few places for these fancy Snapdragon AR
[01:25:54.340 --> 01:25:56.020]   two Gen 1 chips.
[01:25:56.020 --> 01:25:58.260]   Yeah, no more Exynos.
[01:25:58.260 --> 01:25:59.860]   They're going to do Qualcomm, I guess.
[01:25:59.860 --> 01:26:01.700]   That was the rumor.
[01:26:01.700 --> 01:26:04.660]   Is that what they did, Jason's, Jason's in the chat room.
[01:26:04.660 --> 01:26:06.100]   Correct.
[01:26:06.100 --> 01:26:06.420]   Yeah.
[01:26:06.420 --> 01:26:07.700]   So that's it.
[01:26:07.700 --> 01:26:10.740]   Qualcomm was there as well as, what's his name?
[01:26:10.740 --> 01:26:12.820]   Hiroshi Lockheimer.
[01:26:12.820 --> 01:26:15.940]   Oh, Hiroshi Lockheimer from Android,
[01:26:15.940 --> 01:26:17.620]   of Android and Google fame.
[01:26:17.620 --> 01:26:20.740]   He was also there talking all Google.
[01:26:20.740 --> 01:26:23.620]   That's funny because for a while Samsung never used the A word.
[01:26:24.500 --> 01:26:25.060]   Right.
[01:26:25.060 --> 01:26:27.220]   They didn't want to mention that these are running on Android.
[01:26:27.220 --> 01:26:30.100]   It's like, though, it's like their own thing or something.
[01:26:30.100 --> 01:26:34.740]   Oh, because I mean, well, Tizen wasn't even a mobile OS, was it?
[01:26:34.740 --> 01:26:37.300]   It was just, I still don't know.
[01:26:37.300 --> 01:26:38.900]   No, Tizen was for their watches.
[01:26:38.900 --> 01:26:40.900]   Yeah, I don't think they put anything but watches.
[01:26:40.900 --> 01:26:41.940]   And their TVs.
[01:26:41.940 --> 01:26:42.740]   Oh, that's right.
[01:26:42.740 --> 01:26:44.500]   They did their, I think it was kind of an IoT.
[01:26:44.500 --> 01:26:45.060]   I think they're in Detroit.
[01:26:45.060 --> 01:26:45.380]   Yeah.
[01:26:45.380 --> 01:26:51.940]   Well, I think when they started, they were worried that Google might make Android less
[01:26:51.940 --> 01:26:55.540]   desirable for them or have more stringent requirements.
[01:26:55.540 --> 01:26:57.940]   So they developed a competing operating system.
[01:26:57.940 --> 01:27:03.140]   And then they kind of gave up because in the latest version of the Galaxy Wear watches,
[01:27:03.140 --> 01:27:08.180]   it's like it's Android Wear plus Tizen, but mostly it's Android Wear.
[01:27:08.180 --> 01:27:10.580]   I don't think Tizen has a big future.
[01:27:10.580 --> 01:27:16.660]   Galaxy Book 3 Ultra got a big flashy introduction on stage.
[01:27:16.660 --> 01:27:19.060]   I think the cameras, of course, are going to be of interest.
[01:27:19.060 --> 01:27:20.980]   And I'm sure, aunt, you're interested in
[01:27:21.860 --> 01:27:23.060]   what the new cameras will do.
[01:27:23.060 --> 01:27:29.220]   Well, what I will say about that, sir, is back when iPhone 14 was announced,
[01:27:29.220 --> 01:27:35.540]   because that was the last big, big time phone release episode 147 of my show,
[01:27:35.540 --> 01:27:36.580]   hands-on photography.
[01:27:36.580 --> 01:27:42.180]   I discussed the camera hype and that 50 megapixels and how was a bunch of bunk.
[01:27:42.180 --> 01:27:44.580]   Don't follow this.
[01:27:44.580 --> 01:27:47.540]   Is 200 megapixels four times the bunk?
[01:27:47.540 --> 01:27:49.620]   You're right.
[01:27:49.620 --> 01:27:50.740]   Well said, sir.
[01:27:50.740 --> 01:27:52.260]   Don't follow this stuff, folks.
[01:27:52.260 --> 01:27:53.700]   But unfortunately, people are.
[01:27:53.700 --> 01:27:58.100]   And also, I'd like to say, don't buy this phone right now.
[01:27:58.100 --> 01:27:59.060]   Give it three months.
[01:27:59.060 --> 01:28:00.500]   You get it a lot cheaper.
[01:28:00.500 --> 01:28:00.980]   Yeah.
[01:28:00.980 --> 01:28:04.020]   Because it's not that much better than what you already have.
[01:28:04.020 --> 01:28:05.380]   I have an S22.
[01:28:05.380 --> 01:28:07.620]   You know, I--
[01:28:07.620 --> 01:28:09.700]   People are keeping their phones longer.
[01:28:09.700 --> 01:28:13.940]   I think Gartner said that they're not expecting phones, computers,
[01:28:13.940 --> 01:28:17.620]   just broadly consumer electronics devices are going to be down for this year.
[01:28:17.620 --> 01:28:18.100]   Yeah.
[01:28:18.100 --> 01:28:22.260]   I believe Mr. Howell mentioned a stat like that today with it.
[01:28:22.260 --> 01:28:24.900]   People hold it onto them a lot longer.
[01:28:24.900 --> 01:28:28.660]   And it is what it is, because the phones, we're at a plateau,
[01:28:28.660 --> 01:28:30.820]   but they can only do so much, you know?
[01:28:30.820 --> 01:28:31.540]   Yeah.
[01:28:31.540 --> 01:28:35.780]   Really, the only reason to get rid of them is your security updates after--
[01:28:35.780 --> 01:28:36.660]   Right.
[01:28:36.660 --> 01:28:37.700]   Depending on what you do.
[01:28:37.700 --> 01:28:42.820]   I shouldn't say, like, for XR, that's why Qualcomm's pushing so hard for XR.
[01:28:42.820 --> 01:28:47.300]   That's why all of these guys are, because that's going to be a jump enough
[01:28:47.300 --> 01:28:50.420]   in performance to get you to upgrade presumably.
[01:28:50.420 --> 01:28:51.540]   Wow, look at this.
[01:28:51.540 --> 01:28:57.700]   I, as a Google Fi user, can upgrade to an S23 plus for 500 bucks,
[01:28:57.700 --> 01:29:01.940]   300 for the S23 for the ultra 600 bucks, half price.
[01:29:01.940 --> 01:29:05.060]   See, see, what's wrong with that picture, sir?
[01:29:05.060 --> 01:29:05.380]   Really?
[01:29:05.380 --> 01:29:07.140]   It's half price for you?
[01:29:07.140 --> 01:29:07.460]   Yeah.
[01:29:07.460 --> 01:29:07.700]   Yeah.
[01:29:07.700 --> 01:29:09.460]   No, does color--
[01:29:09.460 --> 01:29:10.420]   Color should I get green?
[01:29:10.420 --> 01:29:11.140]   Cool.
[01:29:11.140 --> 01:29:11.620]   Cream?
[01:29:11.620 --> 01:29:13.380]   Phantom Black?
[01:29:13.380 --> 01:29:13.860]   Cool.
[01:29:13.860 --> 01:29:14.740]   Or lavender?
[01:29:15.540 --> 01:29:17.620]   These phones are way overpriced.
[01:29:17.620 --> 01:29:18.580]   It's ridiculous.
[01:29:18.580 --> 01:29:20.500]   And we should not have to pay this much--
[01:29:20.500 --> 01:29:21.780]   $5,99,99.
[01:29:21.780 --> 01:29:22.980]   Well, that's a pretty good deal.
[01:29:22.980 --> 01:29:25.940]   I don't even need to trade in anything.
[01:29:25.940 --> 01:29:27.140]   Maybe I'll pick it up for that.
[01:29:27.140 --> 01:29:29.540]   Why?
[01:29:29.540 --> 01:29:29.940]   Why?
[01:29:29.940 --> 01:29:31.060]   For $5,99.
[01:29:31.060 --> 01:29:33.300]   So buy a cool kitchen gadget.
[01:29:33.300 --> 01:29:33.860]   Oh, yeah.
[01:29:33.860 --> 01:29:34.580]   $5,99 I get it.
[01:29:34.580 --> 01:29:35.940]   $1,000 I don't.
[01:29:35.940 --> 01:29:36.820]   These phones just--
[01:29:36.820 --> 01:29:37.860]   They're not worth that.
[01:29:37.860 --> 01:29:39.780]   Well, the list for this $5,99 is twice that.
[01:29:39.780 --> 01:29:41.300]   $13,79.
[01:29:41.300 --> 01:29:41.860]   Oh, yeah.
[01:29:41.860 --> 01:29:43.540]   $13,79.
[01:29:43.540 --> 01:29:44.900]   I can show you a list price.
[01:29:45.780 --> 01:29:46.820]   You know, if you want to pay it,
[01:29:46.820 --> 01:29:47.300]   feel free.
[01:29:47.300 --> 01:29:50.420]   $600 off for Google Fi customers.
[01:29:50.420 --> 01:29:52.660]   How could I not buy it?
[01:29:52.660 --> 01:29:56.100]   A free list.
[01:29:56.100 --> 01:29:56.500]   How could I--
[01:29:56.500 --> 01:29:59.300]   By the way, you were going to test something for me for this week.
[01:29:59.300 --> 01:29:59.860]   What was that?
[01:29:59.860 --> 01:30:03.220]   The new better voice stuff on the--
[01:30:03.220 --> 01:30:04.180]   Oh, crap, I was.
[01:30:04.180 --> 01:30:04.660]   7.
[01:30:04.660 --> 01:30:05.460]   Let me call you.
[01:30:05.460 --> 01:30:07.140]   Disappointed me again.
[01:30:07.140 --> 01:30:09.380]   Let me call you.
[01:30:09.380 --> 01:30:09.940]   I'll just call--
[01:30:09.940 --> 01:30:10.660]   I turned it on.
[01:30:10.660 --> 01:30:11.220]   Hold on.
[01:30:11.220 --> 01:30:14.500]   I turned it on, but I forgot to do anything with it.
[01:30:14.500 --> 01:30:15.380]   Who could I call?
[01:30:15.380 --> 01:30:17.140]   Who could I call?
[01:30:17.140 --> 01:30:18.100]   Do we have a phone number?
[01:30:18.100 --> 01:30:18.580]   I could call--
[01:30:18.580 --> 01:30:18.900]   Call me if you can.
[01:30:18.900 --> 01:30:19.300]   Call me if you can.
[01:30:19.300 --> 01:30:19.700]   --put on the ear.
[01:30:19.700 --> 01:30:23.300]   No, I need to be able to put it on the ear.
[01:30:23.300 --> 01:30:23.380]   Well, Lisa?
[01:30:23.380 --> 01:30:25.780]   Have jammer B go to the booth
[01:30:25.780 --> 01:30:27.860]   and stand next to the mic there in the booth.
[01:30:27.860 --> 01:30:31.860]   Oh, I could call my iPhone.
[01:30:31.860 --> 01:30:34.340]   You need background noise.
[01:30:34.340 --> 01:30:35.540]   I need to turn on the--
[01:30:35.540 --> 01:30:36.180]   Help me out.
[01:30:36.180 --> 01:30:37.460]   The leaf blower.
[01:30:37.460 --> 01:30:38.420]   Do we have a leaf blower?
[01:30:38.420 --> 01:30:39.780]   Oh.
[01:30:39.780 --> 01:30:40.340]   Get me--
[01:30:40.340 --> 01:30:41.140]   Where is Mr. Ber--
[01:30:41.140 --> 01:30:42.020]   Get me a leaf blower.
[01:30:42.020 --> 01:30:42.500]   I can--
[01:30:42.500 --> 01:30:44.420]   I could ring the doorbell in my dog.
[01:30:44.420 --> 01:30:45.220]   Can go-- oh, wait.
[01:30:45.220 --> 01:30:46.740]   No, you need my dog to be with you.
[01:30:46.740 --> 01:30:47.620]   Never mind.
[01:30:47.620 --> 01:30:48.820]   I'm going to call myself.
[01:30:48.820 --> 01:30:49.860]   There's a TV's on the background.
[01:30:49.860 --> 01:30:55.380]   I'm going to call myself and then--
[01:30:55.380 --> 01:30:58.100]   I could type.
[01:30:58.100 --> 01:30:59.940]   What if you call me while I'm typing?
[01:30:59.940 --> 01:31:01.780]   If it can cut out Stacy's typing.
[01:31:01.780 --> 01:31:02.980]   Well, then we'll call it.
[01:31:02.980 --> 01:31:04.900]   It cuts out my end of the thing.
[01:31:04.900 --> 01:31:08.020]   Oh, sorry.
[01:31:08.020 --> 01:31:09.220]   I need this technology.
[01:31:09.220 --> 01:31:10.580]   Oh, it went straight to voicemail
[01:31:10.580 --> 01:31:11.780]   because I'm on the radio.
[01:31:11.780 --> 01:31:12.580]   [LAUGHTER]
[01:31:12.580 --> 01:31:14.020]   I'm looking at the--
[01:31:14.020 --> 01:31:16.580]   I'm looking at the gizmodo article.
[01:31:16.580 --> 01:31:18.420]   I could call something, but I--
[01:31:18.420 --> 01:31:18.740]   The phone.
[01:31:18.740 --> 01:31:21.700]   I need to be really loud behind me.
[01:31:21.700 --> 01:31:24.900]   Like, you need to be mowing the carpet or something.
[01:31:24.900 --> 01:31:27.220]   Does--
[01:31:27.220 --> 01:31:27.380]   Does--
[01:31:27.380 --> 01:31:28.660]   Do you have an air compressor?
[01:31:28.660 --> 01:31:29.620]   Do we have an air compressor?
[01:31:29.620 --> 01:31:30.340]   That'd be good.
[01:31:30.340 --> 01:31:30.820]   Anyway.
[01:31:30.820 --> 01:31:34.180]   You failed me again.
[01:31:34.180 --> 01:31:37.060]   So get the dice in and vacuum behind me.
[01:31:37.060 --> 01:31:40.660]   [LAUGHTER]
[01:31:40.660 --> 01:31:41.460]   I called--
[01:31:41.460 --> 01:31:43.460]   OK, we're going to turn on the radio.
[01:31:43.460 --> 01:31:44.900]   Get the dice in.
[01:31:44.900 --> 01:31:45.380]   Hold on.
[01:31:45.380 --> 01:31:46.580]   I got to get Jeff's number here.
[01:31:46.580 --> 01:31:47.620]   Do I have your number?
[01:31:47.620 --> 01:31:49.300]   Oh, this is gold.
[01:31:49.300 --> 01:31:50.340]   I might have your number.
[01:31:50.340 --> 01:31:52.260]   Oh, in here comes Mr. Nielsen.
[01:31:52.260 --> 01:31:53.540]   OK, I do.
[01:31:53.540 --> 01:31:55.300]   It's just a 908 number, right?
[01:31:55.300 --> 01:31:56.580]   Yeah, that's right.
[01:31:56.580 --> 01:31:56.980]   OK.
[01:31:56.980 --> 01:32:01.700]   OK, I'm going to call you right now, Jeff.
[01:32:01.700 --> 01:32:03.380]   Video call.
[01:32:03.380 --> 01:32:05.620]   No, just regular phone freaking call.
[01:32:05.620 --> 01:32:06.580]   OK.
[01:32:06.580 --> 01:32:07.620]   I'm calling--
[01:32:07.620 --> 01:32:09.700]   Oh, that's why it was phones cost $1,000.
[01:32:09.700 --> 01:32:11.540]   I'm calling Jeff right now.
[01:32:11.540 --> 01:32:12.500]   OK.
[01:32:12.500 --> 01:32:14.180]   And pick up Jeff.
[01:32:14.180 --> 01:32:14.900]   Hello.
[01:32:14.900 --> 01:32:16.020]   Hello, Jeff.
[01:32:16.020 --> 01:32:17.460]   It's me, Leo, calling.
[01:32:17.460 --> 01:32:17.940]   Oh, is this fan out?
[01:32:17.940 --> 01:32:20.100]   It's a little bit loud back here.
[01:32:20.100 --> 01:32:20.820]   Hello, I'm looking for--
[01:32:20.820 --> 01:32:22.100]   Go ahead, turn on the radio.
[01:32:22.100 --> 01:32:23.540]   Bring on more noise.
[01:32:23.540 --> 01:32:24.900]   Mute your microphone, Mr. Jarvis.
[01:32:24.900 --> 01:32:25.140]   Can you hear me?
[01:32:25.140 --> 01:32:25.940]   Does it sound OK?
[01:32:25.940 --> 01:32:27.300]   Do you hear--
[01:32:27.300 --> 01:32:27.780]   Do you hear--
[01:32:27.780 --> 01:32:28.740]   [LAUGHTER]
[01:32:28.740 --> 01:32:30.820]   I thought that's great.
[01:32:30.820 --> 01:32:32.660]   [LAUGHTER]
[01:32:32.660 --> 01:32:33.060]   Wait, wait.
[01:32:33.060 --> 01:32:33.460]   I don't--
[01:32:33.460 --> 01:32:34.420]   I'm trying--
[01:32:34.420 --> 01:32:35.700]   I barely hear the vacuum.
[01:32:35.700 --> 01:32:37.060]   You can barely hear the vacuum.
[01:32:37.060 --> 01:32:38.020]   It's nice in the--
[01:32:38.020 --> 01:32:38.500]   It's nice.
[01:32:38.500 --> 01:32:39.140]   The vacuum--
[01:32:39.140 --> 01:32:40.900]   Put the vacuum right next to--
[01:32:40.900 --> 01:32:42.260]   It's really noisy in here.
[01:32:42.260 --> 01:32:42.900]   Can you hear me?
[01:32:42.900 --> 01:32:43.540]   OK.
[01:32:43.540 --> 01:32:44.660]   You hear it kind of working.
[01:32:44.660 --> 01:32:45.540]   It's trying to get rid of it.
[01:32:45.540 --> 01:32:46.260]   No, it gets rid of the vacuum.
[01:32:46.260 --> 01:32:47.940]   It does get rid of the vacuum.
[01:32:47.940 --> 01:32:48.500]   Look at that.
[01:32:48.500 --> 01:32:49.060]   Look at that.
[01:32:49.060 --> 01:32:49.540]   Look at that.
[01:32:49.540 --> 01:32:50.500]   Oh, the one that sounds tinny.
[01:32:50.500 --> 01:32:51.060]   Hold here.
[01:32:51.060 --> 01:32:53.780]   I'm going to turn off my mic and hold your phone up to your mic.
[01:32:53.780 --> 01:33:00.340]   Did you hear--
[01:33:00.340 --> 01:33:01.140]   Can you hear them?
[01:33:01.140 --> 01:33:01.780]   The vacuum.
[01:33:01.780 --> 01:33:02.820]   Come back in my head.
[01:33:02.820 --> 01:33:05.380]   So, yeah, I don't hear the vacuum at all.
[01:33:05.380 --> 01:33:06.500]   I don't hear vacuum.
[01:33:06.500 --> 01:33:07.220]   I don't hear it.
[01:33:07.220 --> 01:33:07.460]   Yeah.
[01:33:08.100 --> 01:33:08.660]   Yeah.
[01:33:08.660 --> 01:33:09.220]   Here.
[01:33:09.220 --> 01:33:10.900]   If you-- I mean, you can hear it.
[01:33:10.900 --> 01:33:11.380]   Yeah.
[01:33:11.380 --> 01:33:12.180]   If you get over there--
[01:33:12.180 --> 01:33:13.540]   Yeah, if you hear it with the studio, my--
[01:33:13.540 --> 01:33:14.580]   But not with the phone.
[01:33:14.580 --> 01:33:16.180]   [LAUGHTER]
[01:33:16.180 --> 01:33:17.620]   Get close-- tell him to get close.
[01:33:17.620 --> 01:33:18.340]   I don't know if I'm out of the mic here,
[01:33:18.340 --> 01:33:20.580]   but you don't hear it on the phone.
[01:33:20.580 --> 01:33:22.260]   So that's pretty good.
[01:33:22.260 --> 01:33:23.620]   It works well.
[01:33:23.620 --> 01:33:24.900]   Yeah.
[01:33:24.900 --> 01:33:27.300]   That's impressive.
[01:33:27.300 --> 01:33:29.860]   No, now that the noise is gone, can you still hear me?
[01:33:29.860 --> 01:33:31.380]   Is the victors not going to enjoy editing?
[01:33:31.380 --> 01:33:32.340]   I can hear you, yes.
[01:33:32.340 --> 01:33:33.340]   Oh, OK.
[01:33:33.340 --> 01:33:34.180]   [LAUGHTER]
[01:33:34.180 --> 01:33:37.140]   Your voice sounds a little titty on it, but--
[01:33:37.140 --> 01:33:38.740]   Um, well, I'm on a phone.
[01:33:38.740 --> 01:33:40.660]   Why you couldn't hear the vacuum?
[01:33:40.660 --> 01:33:42.420]   I'm so used to you being in high quality.
[01:33:42.420 --> 01:33:43.060]   Yeah, that's right.
[01:33:43.060 --> 01:33:44.660]   But you couldn't hear the vacuum.
[01:33:44.660 --> 01:33:45.220]   I'm a Leo.
[01:33:45.220 --> 01:33:46.500]   So I could--
[01:33:46.500 --> 01:33:47.540]   That's pretty good.
[01:33:47.540 --> 01:33:49.060]   Just try to get in with the--
[01:33:49.060 --> 01:33:50.020]   And then it went out.
[01:33:50.020 --> 01:33:50.900]   New to take it out.
[01:33:50.900 --> 01:33:51.940]   Everyone's while you're hearing it.
[01:33:51.940 --> 01:33:52.900]   I need to pull his--
[01:33:52.900 --> 01:33:54.180]   So pull his levels up.
[01:33:54.180 --> 01:33:54.740]   Hold on.
[01:33:54.740 --> 01:33:56.900]   This is the new feature on the Google--
[01:33:56.900 --> 01:33:59.300]   on the Pixel called--
[01:33:59.300 --> 01:34:00.500]   I'm going to hang up on you now, Jeff.
[01:34:00.500 --> 01:34:00.900]   I wish I could--
[01:34:00.900 --> 01:34:01.780]   Hang on Jesus.
[01:34:01.780 --> 01:34:02.660]   Leo.
[01:34:02.660 --> 01:34:03.140]   Leo.
[01:34:03.140 --> 01:34:04.260]   Oh, rude.
[01:34:04.260 --> 01:34:04.740]   [LAUGHTER]
[01:34:04.740 --> 01:34:06.420]   This is the new clear calling.
[01:34:06.420 --> 01:34:07.380]   Good day, sir.
[01:34:07.380 --> 01:34:07.780]   Good day, sir.
[01:34:07.780 --> 01:34:09.940]   I said good day, sir.
[01:34:09.940 --> 01:34:12.020]   You can turn on the phone.
[01:34:12.020 --> 01:34:14.420]   And I guess it works.
[01:34:14.420 --> 01:34:16.180]   So that's good.
[01:34:16.180 --> 01:34:16.900]   It wasn't--
[01:34:16.900 --> 01:34:17.940]   That was impressive.
[01:34:17.940 --> 01:34:19.540]   Yeah, it's in the system.
[01:34:19.540 --> 01:34:21.540]   Here you can see over my shoulder,
[01:34:21.540 --> 01:34:22.580]   if you can see over the shoulder.
[01:34:22.580 --> 01:34:24.420]   But use clear calling.
[01:34:24.420 --> 01:34:25.700]   Clear calling is available.
[01:34:25.700 --> 01:34:28.060]   Depending on your Wi-Fi and mobile network connection,
[01:34:28.060 --> 01:34:31.380]   content from your call is not sent to Google.
[01:34:31.380 --> 01:34:34.380]   Reduces background noises during calls.
[01:34:34.380 --> 01:34:35.620]   So there you go.
[01:34:35.620 --> 01:34:37.500]   Not available, though, on other Android phones.
[01:34:37.500 --> 01:34:38.580]   This is a Pixel.
[01:34:38.580 --> 01:34:40.660]   Oh, it is 7, which I'm wondering
[01:34:40.660 --> 01:34:41.940]   whether I want to buy the 7.
[01:34:41.940 --> 01:34:44.780]   Well, it says it uses the AI chip in here, right?
[01:34:44.780 --> 01:34:45.260]   It's using--
[01:34:45.260 --> 01:34:45.780]   Yeah, exactly.
[01:34:45.780 --> 01:34:46.780]   That's why it's only available there.
[01:34:46.780 --> 01:34:47.620]   --in the processor.
[01:34:47.620 --> 01:34:48.140]   So--
[01:34:48.140 --> 01:34:50.260]   Yeah, it worked good.
[01:34:50.260 --> 01:34:51.300]   Pretty good.
[01:34:51.300 --> 01:34:53.140]   Pretty sweet.
[01:34:53.140 --> 01:34:54.220]   All right, let's take a little break.
[01:34:54.220 --> 01:34:57.820]   I got to talk about a sponsor, and we will continue on
[01:34:57.820 --> 01:34:59.340]   with our fabulous show.
[01:34:59.340 --> 01:35:01.460]   Oh, I'm excited, because this week I have a little chore
[01:35:01.460 --> 01:35:02.140]   to do.
[01:35:02.140 --> 01:35:05.380]   I'm upgrading my eight-sleep pod to the new
[01:35:05.380 --> 01:35:07.460]   pod three cover.
[01:35:07.460 --> 01:35:09.540]   Now, I don't-- you know, I'm going to do it,
[01:35:09.540 --> 01:35:10.460]   because we want to try it.
[01:35:10.460 --> 01:35:13.980]   We've had the pod eight-sleep pod cover for more than a year
[01:35:13.980 --> 01:35:14.340]   now.
[01:35:14.340 --> 01:35:17.500]   You may remember talking about putting it on and everything.
[01:35:17.500 --> 01:35:20.100]   We've been through a summer, a hot, hot summer,
[01:35:20.100 --> 01:35:21.460]   and a cold, cold winter band.
[01:35:21.460 --> 01:35:23.380]   It's really cold right now.
[01:35:23.380 --> 01:35:26.460]   It's literally-- we're having frost every night.
[01:35:26.460 --> 01:35:27.180]   And it's great.
[01:35:27.180 --> 01:35:28.300]   Lisa gets in.
[01:35:28.300 --> 01:35:30.260]   She says, I'm turning it up to 10.
[01:35:30.260 --> 01:35:31.620]   She said, fine.
[01:35:31.620 --> 01:35:36.700]   The eight-sleep is so cool, because each side of the bed
[01:35:36.700 --> 01:35:39.620]   has its own settings.
[01:35:39.620 --> 01:35:41.940]   It can cool-- and in the summer, you're
[01:35:41.940 --> 01:35:44.660]   going to love this-- as cool as 55 degrees.
[01:35:44.660 --> 01:35:48.420]   You can heat up to 110 degrees.
[01:35:48.420 --> 01:35:51.420]   And the whole point is to give you a better night's sleep,
[01:35:51.420 --> 01:35:55.100]   not merely to keep you warm and cozy or cool during a hot night,
[01:35:55.100 --> 01:35:56.220]   but to give you a good night's sleep.
[01:35:56.220 --> 01:35:59.820]   In fact, I have the sleep doctor turn on with the eight-sleep,
[01:35:59.820 --> 01:36:02.820]   and it automatically monitors my heart rate, my breathing,
[01:36:02.820 --> 01:36:04.020]   my movements.
[01:36:04.020 --> 01:36:07.140]   And it cools off as I get into deeper sleep,
[01:36:07.140 --> 01:36:10.060]   which it actually nudges me into deep sleep.
[01:36:10.060 --> 01:36:12.620]   So the way I have it said, it's warm when I get in bed,
[01:36:12.620 --> 01:36:14.300]   because I get under the covers.
[01:36:14.300 --> 01:36:16.860]   Oh, it's nice and cozy, at least in the winter.
[01:36:16.860 --> 01:36:20.140]   And then it cools off.
[01:36:20.140 --> 01:36:22.940]   And I go into deeper and deeper sleep.
[01:36:22.940 --> 01:36:25.420]   And then I have it set to start warming up in the morning,
[01:36:25.420 --> 01:36:29.300]   and I wake up in the morning, and I'm nice and cozy again.
[01:36:29.300 --> 01:36:32.180]   Look, good sleep is the ultimate game changer.
[01:36:32.180 --> 01:36:34.780]   Nature's gentle nurse.
[01:36:34.780 --> 01:36:38.220]   And the pod cover, the ultimate sleep machine.
[01:36:38.220 --> 01:36:40.140]   Consistent good sleep can reduce the likelihood
[01:36:40.140 --> 01:36:41.780]   of serious health issues.
[01:36:41.780 --> 01:36:43.740]   You could decrease the risk of heart disease.
[01:36:43.740 --> 01:36:44.980]   It can lower your blood pressure,
[01:36:44.980 --> 01:36:47.860]   even reduce the risk of Alzheimer's.
[01:36:47.860 --> 01:36:49.700]   God knows I need that.
[01:36:49.700 --> 01:36:51.300]   If you're struggling to fall asleep,
[01:36:51.300 --> 01:36:52.700]   or if you wake up in the middle of the night,
[01:36:52.700 --> 01:36:55.100]   or you fight with your partner over the thermostat,
[01:36:55.100 --> 01:36:57.580]   you will love the eight-sleep pod cover works hard
[01:36:57.580 --> 01:36:59.180]   all night long to improve your sleep.
[01:36:59.180 --> 01:37:00.020]   So you don't have to.
[01:37:00.020 --> 01:37:02.100]   And Lisa and I get different temperatures.
[01:37:02.100 --> 01:37:04.500]   It also saves us money, because I don't put on the heat at night.
[01:37:04.500 --> 01:37:07.420]   I don't have to use the AC in the summer,
[01:37:07.420 --> 01:37:08.820]   because we're automatic.
[01:37:08.820 --> 01:37:12.780]   It's actually a great way to efficiently keep you comfortable
[01:37:12.780 --> 01:37:14.380]   all night long.
[01:37:14.380 --> 01:37:16.820]   The pod cover fits on any mattress.
[01:37:16.820 --> 01:37:19.100]   You can adjust the temperature of your sleeping environment.
[01:37:19.100 --> 01:37:21.580]   So you get the optimal temperature.
[01:37:21.580 --> 01:37:23.980]   As I said, Lisa likes it really hot.
[01:37:23.980 --> 01:37:24.860]   That's nice.
[01:37:24.860 --> 01:37:25.620]   Come on, reach over.
[01:37:25.620 --> 01:37:26.700]   Feel how hot it is.
[01:37:26.700 --> 01:37:28.780]   How can you take that?
[01:37:28.780 --> 01:37:29.700]   I love it.
[01:37:29.700 --> 01:37:32.220]   I love it.
[01:37:32.220 --> 01:37:35.300]   I'm a little bit more moderate in all of that.
[01:37:35.300 --> 01:37:37.540]   In addition to the best in class temperature regulation,
[01:37:37.540 --> 01:37:39.700]   it even knows what the room temperature is.
[01:37:39.700 --> 01:37:42.060]   So it really is responding to the environment,
[01:37:42.060 --> 01:37:44.620]   as well as to how you're sleeping, your biometrics,
[01:37:44.620 --> 01:37:46.300]   your sleep stages.
[01:37:46.300 --> 01:37:48.500]   They've got sensors to track your health and sleep metrics
[01:37:48.500 --> 01:37:51.420]   without the need to wear any wearable devices.
[01:37:51.420 --> 01:37:53.820]   I don't have to wear my Apple watch or anything like that.
[01:37:53.820 --> 01:37:55.620]   I get all those metrics in the morning.
[01:37:55.620 --> 01:37:58.020]   It tells me how I slept.
[01:37:58.020 --> 01:37:58.660]   I can't wait.
[01:37:58.660 --> 01:38:00.340]   We're going to put the pod three cover on this weekend.
[01:38:00.340 --> 01:38:00.820]   I can't wait.
[01:38:00.820 --> 01:38:01.980]   It's got more sensors.
[01:38:01.980 --> 01:38:04.220]   That's the one you'll be getting.
[01:38:04.220 --> 01:38:05.380]   Better sleep.
[01:38:05.380 --> 01:38:09.540]   It's the health habit you will love sticking to night after night.
[01:38:09.540 --> 01:38:12.340]   Wake up fully energized with the pod cover.
[01:38:12.340 --> 01:38:15.700]   And you can tackle whatever life throws at you.
[01:38:15.700 --> 01:38:17.940]   Go to atesleep.com/twits.
[01:38:17.940 --> 01:38:18.620]   Spell it out.
[01:38:18.620 --> 01:38:21.540]   E-I-G-H-T atesleep.com/twits.
[01:38:21.540 --> 01:38:24.260]   Save $150 to check out on the pod cover.
[01:38:24.260 --> 01:38:27.700]   atesleep currently ships within the US, Canada, the UK.
[01:38:27.700 --> 01:38:30.060]   Select countries in the EU and Good News,
[01:38:30.060 --> 01:38:32.060]   because it's really hot in Australia right now.
[01:38:32.060 --> 01:38:33.900]   It ships to Australia as well.
[01:38:33.900 --> 01:38:37.900]   atesleep.com/twit.
[01:38:37.900 --> 01:38:42.700]   It's so nice to have exactly the right temperature when you get in a bed.
[01:38:42.700 --> 01:38:43.460]   I love it.
[01:38:43.460 --> 01:38:45.100]   atesleep.com/twit.
[01:38:45.100 --> 01:38:49.100]   Thank you, atesleep, for supporting our show.
[01:38:49.100 --> 01:38:54.220]   And thank you for supporting the show by going to atesleep.com/twit.
[01:38:54.220 --> 01:38:57.100]   That thing is a game changer.
[01:38:57.100 --> 01:38:59.260]   Oh, you just got one, didn't you?
[01:38:59.260 --> 01:39:00.460]   Oh, you love it?
[01:39:00.460 --> 01:39:01.460]   Oh, gosh.
[01:39:01.460 --> 01:39:02.900]   It's a game changer.
[01:39:02.900 --> 01:39:07.100]   You know, the hat that I normally wear into the studio service.
[01:39:07.100 --> 01:39:08.460]   You wear that to bed?
[01:39:08.460 --> 01:39:10.180]   Follically challenged.
[01:39:10.180 --> 01:39:15.380]   I wear that to bed typically because Queen Perrit likes to turn on the fan.
[01:39:15.380 --> 01:39:19.220]   And my hair just came into that.
[01:39:19.220 --> 01:39:20.220]   I don't like that.
[01:39:20.220 --> 01:39:21.860]   So now the fan doesn't come away.
[01:39:21.860 --> 01:39:23.220]   Yeah, because she's cool.
[01:39:23.220 --> 01:39:25.580]   Yeah, yeah, the fan doesn't come on anymore.
[01:39:25.580 --> 01:39:29.460]   And that side of the big, it really chills down.
[01:39:29.460 --> 01:39:31.100]   It's game changer.
[01:39:31.100 --> 01:39:36.100]   I envision you in one of those cartoonies.
[01:39:36.100 --> 01:39:40.940]   I almost bought one of those just to prove a point to her.
[01:39:40.940 --> 01:39:48.100]   I in my night shirt and mom and cat just settle down to a long winter's nap.
[01:39:48.100 --> 01:39:52.180]   Well, yeah, I'm so glad you got one.
[01:39:52.180 --> 01:39:53.180]   You have the pod three.
[01:39:53.180 --> 01:39:57.460]   I'm a little jealous, but I'm going to install it this weekend.
[01:39:57.460 --> 01:39:58.460]   Just follow the instructions.
[01:39:58.460 --> 01:40:00.300]   Follow the instructions, please.
[01:40:00.300 --> 01:40:01.300]   They give you.
[01:40:01.300 --> 01:40:02.300]   Did you have any trouble?
[01:40:02.300 --> 01:40:04.020]   It was me because it's easy.
[01:40:04.020 --> 01:40:08.740]   If you watch the video ahead of time, I put it on upside down.
[01:40:08.740 --> 01:40:10.580]   It was just it was dumb.
[01:40:10.580 --> 01:40:13.980]   It was me.
[01:40:13.980 --> 01:40:15.860]   We are learning more about the Google layoffs.
[01:40:15.860 --> 01:40:19.980]   Of course, it was great to have Richard Hay on last week to talk about the human side
[01:40:19.980 --> 01:40:20.980]   of this.
[01:40:20.980 --> 01:40:27.220]   Among other groups laid off, we mentioned area one, 20, the R and D division.
[01:40:27.220 --> 01:40:29.900]   Completely stripped of employees.
[01:40:29.900 --> 01:40:35.860]   Google's fuchsia OS was also hard hit.
[01:40:35.860 --> 01:40:40.100]   I'm kind of fuchsia lost 16% of its employees.
[01:40:40.100 --> 01:40:42.180]   The rest of it was Google was 6%.
[01:40:42.180 --> 01:40:44.060]   So it's a higher percentage.
[01:40:44.060 --> 01:40:47.860]   But does that mean there's nobody left at fuchsia?
[01:40:47.860 --> 01:40:51.860]   There's 400 people working on the new operating system.
[01:40:51.860 --> 01:40:54.460]   Fuchsia is a new operating system from Google.
[01:40:54.460 --> 01:40:55.620]   Probably blue sky, right?
[01:40:55.620 --> 01:41:01.940]   I don't know if they want to replace windows or Android with it, but they were working
[01:41:01.940 --> 01:41:02.940]   on it.
[01:41:02.940 --> 01:41:05.900]   It has some really interesting ideas, right?
[01:41:05.900 --> 01:41:08.020]   It's an IOT.
[01:41:08.020 --> 01:41:14.060]   It's four devices, not just like not smartphones or computers.
[01:41:14.060 --> 01:41:17.820]   It's for like other devices, is my thinking or is what I thought fuchsia was about.
[01:41:17.820 --> 01:41:24.500]   And it's designed to go all the way up from a large capable processor down to a more restrained
[01:41:24.500 --> 01:41:26.140]   processor.
[01:41:26.140 --> 01:41:30.580]   So I looked at it as a unifying operating system that could go like where OS goes all
[01:41:30.580 --> 01:41:34.020]   the way up to running on like a Google display.
[01:41:34.020 --> 01:41:35.020]   And you're right.
[01:41:35.020 --> 01:41:36.020]   We're getting it.
[01:41:36.020 --> 01:41:39.940]   If you have a Nest smart display, it's on there on most of them.
[01:41:39.940 --> 01:41:43.340]   So it is running, it replaced cast OS.
[01:41:43.340 --> 01:41:44.900]   It's not based on Linux.
[01:41:44.900 --> 01:41:45.900]   It's ground up.
[01:41:45.900 --> 01:41:46.900]   It's blue sky.
[01:41:46.900 --> 01:41:47.900]   Right.
[01:41:47.900 --> 01:41:48.900]   So it might be.
[01:41:48.900 --> 01:41:50.700]   I've got my colors mixed up.
[01:41:50.700 --> 01:41:54.340]   It's green field as well as blue sky.
[01:41:54.340 --> 01:41:57.500]   Well, I mean, it exists.
[01:41:57.500 --> 01:42:01.500]   So it's not I feel like once it's out in the world, it's not really blue sky, right?
[01:42:01.500 --> 01:42:05.540]   Well, I only say blue sky because I think it's I don't even think Google knows exactly
[01:42:05.540 --> 01:42:06.740]   what they want to do with it.
[01:42:06.740 --> 01:42:07.740]   They haven't really said.
[01:42:07.740 --> 01:42:08.980]   Yeah, it does.
[01:42:08.980 --> 01:42:09.980]   It's okay.
[01:42:09.980 --> 01:42:12.580]   Is that blue sky or pie in the sky?
[01:42:12.580 --> 01:42:17.900]   Well, there's there's a there's 16% fewer people working on it.
[01:42:17.900 --> 01:42:20.740]   What's 16% of 400?
[01:42:20.740 --> 01:42:22.500]   They've been working on it for so long.
[01:42:22.500 --> 01:42:24.300]   I've never built an operating system.
[01:42:24.300 --> 01:42:28.380]   So, you know, but my understanding is it takes a long time and there's several different
[01:42:28.380 --> 01:42:29.380]   layers.
[01:42:29.380 --> 01:42:32.580]   So maybe it was something that's killed off just from the layoffs.
[01:42:32.580 --> 01:42:34.740]   It's just a reduced workforce.
[01:42:34.740 --> 01:42:36.460]   Is what you're saying?
[01:42:36.460 --> 01:42:39.220]   Well, maybe they've done all the hardest work.
[01:42:39.220 --> 01:42:40.220]   Maybe.
[01:42:40.220 --> 01:42:41.220]   Okay.
[01:42:41.220 --> 01:42:44.580]   I don't know.
[01:42:44.580 --> 01:42:49.740]   Maybe they need different people for a new phase of the OS.
[01:42:49.740 --> 01:42:55.300]   You know, according to ours, Technica, the biggest questions surrounding future are why
[01:42:55.300 --> 01:42:57.660]   does this exist?
[01:42:57.660 --> 01:43:04.620]   And what are its goals is fuchsia and eventual replacement for Android or Chrome OS around
[01:43:04.620 --> 01:43:09.140]   2018 when we got future running on a pixel book, the source code documentation for the
[01:43:09.140 --> 01:43:16.580]   custom kernel says it targets modern phones and modern personal computers with fast processors.
[01:43:16.580 --> 01:43:22.380]   But as you point out, up to now it's been entirely used on IoT devices.
[01:43:22.380 --> 01:43:24.420]   So it's just unclear.
[01:43:24.420 --> 01:43:29.460]   I would still love to see a coming together of Android and Chrome.
[01:43:29.460 --> 01:43:31.340]   Yeah.
[01:43:31.340 --> 01:43:35.260]   The same way Apple is doing it with really this energy.
[01:43:35.260 --> 01:43:39.620]   Well, I don't know Apple well enough anymore.
[01:43:39.620 --> 01:43:40.860]   Here's why I say it's blue sky.
[01:43:40.860 --> 01:43:46.140]   Hiroshi Lockheimer in 2019 said we're looking at what a new take on an operating system could
[01:43:46.140 --> 01:43:47.540]   be like.
[01:43:47.540 --> 01:43:51.260]   And so I know out there people are getting pretty excited saying, Oh, this is the new
[01:43:51.260 --> 01:43:55.900]   Android or Oh, this is the new Chrome OS fuchsia is really not about that.
[01:43:55.900 --> 01:44:01.540]   Fuchsia is just about pushing the state of the art in terms of operating systems and things
[01:44:01.540 --> 01:44:04.580]   that we learned from fuchsia we can incorporate in other products.
[01:44:04.580 --> 01:44:05.580]   It's pretty blue sky.
[01:44:05.580 --> 01:44:06.580]   Right?
[01:44:06.580 --> 01:44:08.820]   Oh, dear God, I don't even know why it exists.
[01:44:08.820 --> 01:44:09.820]   Yeah.
[01:44:09.820 --> 01:44:14.100]   Why did they only fire 16% of the people at their point?
[01:44:14.100 --> 01:44:19.580]   So that's four times 16 is 56 people.
[01:44:19.580 --> 01:44:21.700]   So that's not a huge number out of 400.
[01:44:21.700 --> 01:44:23.460]   But it's, you know, it's something.
[01:44:23.460 --> 01:44:24.460]   Yeah.
[01:44:24.460 --> 01:44:26.260]   It does surround you.
[01:44:26.260 --> 01:44:27.260]   64.
[01:44:27.260 --> 01:44:28.260]   Did I do my math wrong?
[01:44:28.260 --> 01:44:29.260]   I did.
[01:44:29.260 --> 01:44:30.260]   Yeah, you did.
[01:44:30.260 --> 01:44:31.260]   That's okay.
[01:44:31.260 --> 01:44:38.460]   Yeah, in the same year 2022 last year, one fuchsia team members said on hacker news, fuchsia
[01:44:38.460 --> 01:44:42.900]   isn't necessarily targeting end users or application developers.
[01:44:42.900 --> 01:44:47.620]   Fuchsia exists to make products easier to build and maintain products are responsible
[01:44:47.620 --> 01:44:50.820]   for the app developer and end user experience.
[01:44:50.820 --> 01:44:52.420]   Well, that's clear.
[01:44:52.420 --> 01:44:56.620]   It's what a nerdy nerdy.
[01:44:56.620 --> 01:44:59.340]   It's a it's not going anywhere.
[01:44:59.340 --> 01:45:01.340]   It's a nerdy.
[01:45:01.340 --> 01:45:05.220]   I think there's a small chance that everything that fuchsia has done ends up being inside
[01:45:05.220 --> 01:45:07.580]   the Linux kernel.
[01:45:07.580 --> 01:45:08.580]   There you go.
[01:45:08.580 --> 01:45:11.300]   It's what a nerdy it ain't going anywhere.
[01:45:11.300 --> 01:45:13.580]   It's working on this.
[01:45:13.580 --> 01:45:17.180]   You know, I'd file those 60 fire those 64 people are, you know what?
[01:45:17.180 --> 01:45:18.180]   They would have been better.
[01:45:18.180 --> 01:45:19.180]   It's moved on.
[01:45:19.180 --> 01:45:20.180]   They're people.
[01:45:20.180 --> 01:45:21.180]   They're people.
[01:45:21.180 --> 01:45:22.180]   They're people.
[01:45:22.180 --> 01:45:23.180]   I agree.
[01:45:23.180 --> 01:45:27.780]   And in fact, that's why I like what Intel did Intel had a terrible quarter.
[01:45:27.780 --> 01:45:29.300]   Yeah, they did.
[01:45:29.300 --> 01:45:34.740]   But instead of firing people, they all took a cut and paint, including a 25% for the CEO
[01:45:34.740 --> 01:45:35.740]   pack, Gelsinger.
[01:45:35.740 --> 01:45:37.380]   Oh, good for them.
[01:45:37.380 --> 01:45:42.740]   It was the worst decade at worst earnings in over a decade.
[01:45:42.740 --> 01:45:44.220]   32% drop in revenue.
[01:45:44.220 --> 01:45:50.900]   Yeah, that required some sort of public apology remonstration kind of thing.
[01:45:50.900 --> 01:45:51.900]   Yeah.
[01:45:51.900 --> 01:45:52.900]   Yeah.
[01:45:52.900 --> 01:45:58.980]   You know, 5% cut and paid to the rank and file 25% to the CEO.
[01:45:58.980 --> 01:46:00.900]   That's a good.
[01:46:00.900 --> 01:46:04.980]   That's an interesting take a better way, maybe than firing 12,000 people, which is what Google
[01:46:04.980 --> 01:46:05.980]   did.
[01:46:05.980 --> 01:46:06.980]   So what is it?
[01:46:06.980 --> 01:46:07.980]   Earn afterwards?
[01:46:07.980 --> 01:46:08.980]   How does it change?
[01:46:08.980 --> 01:46:10.980]   Oh, I'm sure Gelsinger is doing just fine.
[01:46:10.980 --> 01:46:11.980]   Thank you.
[01:46:11.980 --> 01:46:12.980]   Yeah.
[01:46:12.980 --> 01:46:15.780]   I mean, he's legitimately saved several big companies.
[01:46:15.780 --> 01:46:16.780]   So I'm not.
[01:46:16.780 --> 01:46:18.780]   It's, yeah, no, it's pro.
[01:46:18.780 --> 01:46:22.620]   It's more symbolic that he took that, right?
[01:46:22.620 --> 01:46:23.620]   I respect that.
[01:46:23.620 --> 01:46:28.900]   He didn't necessarily have to make it public, but I respect that.
[01:46:28.900 --> 01:46:32.620]   What triggered these massive losses for Intel?
[01:46:32.620 --> 01:46:37.180]   I can't assume that Apple is eating that much of their pie with its...
[01:46:37.180 --> 01:46:41.900]   No, PC sales are down, down, down, even Apple sales are down, down, down, down, but PC sales
[01:46:41.900 --> 01:46:45.780]   down by 30% to 40% for the manufacturers.
[01:46:45.780 --> 01:46:48.420]   Down from pandemic.
[01:46:48.420 --> 01:46:53.540]   Down from pandemic highs, but still down, even from pre-pandemic numbers.
[01:46:53.540 --> 01:46:54.540]   It's not been a good...
[01:46:54.540 --> 01:46:56.500]   Yeah, and Intel never got...
[01:46:56.500 --> 01:47:00.460]   They never did well on the graphics front, and it's really in their lunch now, because
[01:47:00.460 --> 01:47:03.380]   they didn't have a compelling piece of silicon to do graphics.
[01:47:03.380 --> 01:47:05.380]   So AMD's...
[01:47:05.380 --> 01:47:07.300]   I mean, yeah.
[01:47:07.300 --> 01:47:11.980]   And remember that under Gelsinger, Intel has kind of changed course.
[01:47:11.980 --> 01:47:16.220]   They've decided instead of being an integrated chip design and manufacturer company, they're
[01:47:16.220 --> 01:47:17.860]   going to kind of be both.
[01:47:17.860 --> 01:47:20.660]   They're going to do chip design and they're going to be foundries for other companies
[01:47:20.660 --> 01:47:22.500]   like Apple.
[01:47:22.500 --> 01:47:29.420]   Just to answer your questions, Stacey, Gelsinger's pay will be cut by $312,000.
[01:47:29.420 --> 01:47:34.380]   So this again involves math, but that sounds like he's still getting a good amount of money.
[01:47:34.380 --> 01:47:35.380]   Yeah.
[01:47:35.380 --> 01:47:37.860]   A million bucks still in his pocket.
[01:47:37.860 --> 01:47:41.020]   His base pay was one and a quarter million.
[01:47:41.020 --> 01:47:42.020]   Intel's...
[01:47:42.020 --> 01:47:44.020]   That's actually not terrible.
[01:47:44.020 --> 01:47:45.020]   That's not bad.
[01:47:45.020 --> 01:47:46.020]   That's...
[01:47:46.020 --> 01:47:47.020]   I'm like, go, go him.
[01:47:47.020 --> 01:47:48.020]   Yeah.
[01:47:48.020 --> 01:47:49.020]   Yeah.
[01:47:49.020 --> 01:47:54.700]   Well, the majority of his compensation is from stock awards and options.
[01:47:54.700 --> 01:47:55.700]   So...
[01:47:55.700 --> 01:47:56.700]   Right.
[01:47:56.700 --> 01:48:00.820]   You know, when we're talking salary, that's not all of it.
[01:48:00.820 --> 01:48:03.780]   Total value of his 2021 compensation package.
[01:48:03.780 --> 01:48:04.780]   Stand back.
[01:48:04.780 --> 01:48:05.780]   $178 million.
[01:48:05.780 --> 01:48:08.340]   There we go.
[01:48:08.340 --> 01:48:09.340]   Shares.
[01:48:09.340 --> 01:48:12.540]   There it is.
[01:48:12.540 --> 01:48:14.020]   Holy kamole.
[01:48:14.020 --> 01:48:20.620]   He got $140 million in stock awards last year or 2021.
[01:48:20.620 --> 01:48:23.460]   Oh, never mind.
[01:48:23.460 --> 01:48:30.100]   My former CEO had a track record of going in and helping rebuild companies, if you will,
[01:48:30.100 --> 01:48:32.820]   and get them back on track and profitable.
[01:48:32.820 --> 01:48:38.340]   And when I was in school, one of my classmates was like, "Hey, I know you're a CEO.
[01:48:38.340 --> 01:48:40.660]   He came in and helped us out.
[01:48:40.660 --> 01:48:43.020]   And when we tried to pay him, he didn't want to get paid.
[01:48:43.020 --> 01:48:44.740]   You just wanted options.
[01:48:44.740 --> 01:48:49.020]   And sure enough, that he made way more money that way.
[01:48:49.020 --> 01:48:50.020]   Oh, sure.
[01:48:50.020 --> 01:48:51.220]   Versus, well, get into salary.
[01:48:51.220 --> 01:48:52.740]   I rest my case.
[01:48:52.740 --> 01:48:53.740]   Geez.
[01:48:53.740 --> 01:49:01.020]   Intel's going to cut 401(k) matching from 5% to 2.5%.
[01:49:01.020 --> 01:49:05.060]   Executive team members will take a 15% cut senior managers, mid-level managers will see
[01:49:05.060 --> 01:49:08.140]   10% and 5% respectively.
[01:49:08.140 --> 01:49:12.300]   Lower level employees will not be affected according to the Wall Street Journal.
[01:49:12.300 --> 01:49:13.300]   So that's good.
[01:49:13.300 --> 01:49:17.300]   I mean, you're not going to have the same 401(k) contributions, but you won't.
[01:49:17.300 --> 01:49:20.740]   Your salary isn't going to be cut.
[01:49:20.740 --> 01:49:26.900]   And again, same said CEO at the time when we were struggling, part of the discussion
[01:49:26.900 --> 01:49:29.860]   was, "All right, we're not going to lay people off.
[01:49:29.860 --> 01:49:33.900]   Health insurance prices are raising, but we're going to handle that.
[01:49:33.900 --> 01:49:36.940]   We just can't give you all any raises at the moment, but we're going to make sure your
[01:49:36.940 --> 01:49:41.700]   health insurance is squared away and we're not going to match as much on the 401(k) or what
[01:49:41.700 --> 01:49:42.700]   is heavy.
[01:49:42.700 --> 01:49:45.220]   And it kept everybody employed and it was totally fine.
[01:49:45.220 --> 01:49:48.900]   I think it got better.
[01:49:48.900 --> 01:49:55.060]   Intel is targeting $3 billion in cost cuts this year, increasing to as much as $10 billion
[01:49:55.060 --> 01:49:57.820]   a year by 2025.
[01:49:57.820 --> 01:50:01.260]   And they're not ruling out layoffs.
[01:50:01.260 --> 01:50:02.820]   But so far.
[01:50:02.820 --> 01:50:04.860]   So Intel hasn't done layoffs yet?
[01:50:04.860 --> 01:50:05.860]   Not yet.
[01:50:05.860 --> 01:50:06.860]   Okay.
[01:50:06.860 --> 01:50:07.860]   Everybody else has.
[01:50:07.860 --> 01:50:08.860]   Good for them.
[01:50:08.860 --> 01:50:09.860]   Yeah.
[01:50:09.860 --> 01:50:10.860]   Because the rest of the Valley is...
[01:50:10.860 --> 01:50:11.860]   Oh, that's just Valley now.
[01:50:11.860 --> 01:50:16.020]   Apple hasn't either, but Apple didn't hire as many people during pandemic.
[01:50:16.020 --> 01:50:17.020]   So...
[01:50:17.020 --> 01:50:18.020]   Yes, yes.
[01:50:18.020 --> 01:50:22.260]   You want to do a change log?
[01:50:22.260 --> 01:50:23.420]   Why don't we do a change log?
[01:50:23.420 --> 01:50:26.140]   Play that trumpet set.
[01:50:26.140 --> 01:50:30.860]   The Google change log.
[01:50:30.860 --> 01:50:34.380]   Somebody said, "I've been going too fast through the change log, so I'm going to really
[01:50:34.380 --> 01:50:35.380]   slow this down."
[01:50:35.380 --> 01:50:40.380]   You're going to slow it down and drag it out.
[01:50:40.380 --> 01:50:43.380]   Oh, no.
[01:50:43.380 --> 01:50:44.380]   Android 13.
[01:50:44.380 --> 01:50:45.380]   Oh, no.
[01:50:45.380 --> 01:50:46.380]   Stacey, go get your waffle.
[01:50:46.380 --> 01:50:47.380]   You got to lose these time points.
[01:50:47.380 --> 01:50:49.900]   We're going to need some waffle time.
[01:50:49.900 --> 01:50:54.740]   2PR beta 2 now available for Pixel phones.
[01:50:54.740 --> 01:50:55.740]   Bueller.
[01:50:55.740 --> 01:50:56.740]   Bueller.
[01:50:56.740 --> 01:51:01.300]   There's a fix coming to the latest Pixel Buds A series.
[01:51:01.300 --> 01:51:05.860]   I feel like there's always a fix coming to the Pixel Buds series.
[01:51:05.860 --> 01:51:09.300]   3.519.0 firmware update.
[01:51:09.300 --> 01:51:12.420]   Oh, the current one breaks Bluetooth pairing.
[01:51:12.420 --> 01:51:13.420]   Oh, whoops.
[01:51:13.420 --> 01:51:15.540]   So they're going to fix that.
[01:51:15.540 --> 01:51:17.020]   Hey, I don't...
[01:51:17.020 --> 01:51:18.660]   Why are these so bad?
[01:51:18.660 --> 01:51:22.060]   Why are good earbuds so hard for Google to do?
[01:51:22.060 --> 01:51:23.700]   I just don't get it.
[01:51:23.700 --> 01:51:24.900]   The ones I got now, I like.
[01:51:24.900 --> 01:51:27.060]   I know I had a problem with the series.
[01:51:27.060 --> 01:51:28.060]   Yeah.
[01:51:28.060 --> 01:51:29.060]   Okay.
[01:51:29.060 --> 01:51:31.780]   Don't forget, I was a major complainer, but the last one was...
[01:51:31.780 --> 01:51:33.740]   Yeah, mine have been all right too.
[01:51:33.740 --> 01:51:38.340]   Other than I have one in a bag of rice right now because...
[01:51:38.340 --> 01:51:39.340]   Whoops.
[01:51:39.340 --> 01:51:42.500]   Just swallow up the heat with a few reps.
[01:51:42.500 --> 01:51:44.940]   I got in a shower, wouldn't pay any attention.
[01:51:44.940 --> 01:51:47.220]   I was listening to a book and...
[01:51:47.220 --> 01:51:48.780]   I do that every now and then.
[01:51:48.780 --> 01:51:52.420]   Don't they make a waterproof ones you could wear in the shower?
[01:51:52.420 --> 01:51:54.060]   Yeah, but is that waterproof?
[01:51:54.060 --> 01:51:56.780]   You can only do so much.
[01:51:56.780 --> 01:51:59.540]   That IP rate was IPS rating.
[01:51:59.540 --> 01:52:02.980]   I don't remember what it is, but apparently it's not good enough for the shower.
[01:52:02.980 --> 01:52:06.300]   So one is in a bag of rice at the moment, hopefully...
[01:52:06.300 --> 01:52:07.300]   Good luck, Matt.
[01:52:07.300 --> 01:52:08.300]   God bless.
[01:52:08.300 --> 01:52:11.900]   YouTube TV is losing MLB network starting today.
[01:52:11.900 --> 01:52:15.580]   That's kind of bad news because baseball is about to start up.
[01:52:15.580 --> 01:52:16.580]   Pictures and countries report.
[01:52:16.580 --> 01:52:17.580]   Yeah, it is deceasing.
[01:52:17.580 --> 01:52:18.580]   Yeah.
[01:52:18.580 --> 01:52:19.580]   Yeah.
[01:52:19.580 --> 01:52:24.260]   Starting January 31st, MLB network content will no longer be available on YouTube TV.
[01:52:24.260 --> 01:52:27.740]   We did mention though that YouTube did get the NFL Sunday ticket.
[01:52:27.740 --> 01:52:28.740]   Yep.
[01:52:28.740 --> 01:52:33.300]   So they're going to have football, but they could not make a deal with the Major League
[01:52:33.300 --> 01:52:37.900]   Baseball, so the baseball channel will no longer be available.
[01:52:37.900 --> 01:52:39.420]   You know, they're not done.
[01:52:39.420 --> 01:52:44.860]   Google says we're going to continue conversations in the hope of restoring their content.
[01:52:44.860 --> 01:52:50.620]   Meanwhile, you'll be able to continue watching select MLB games via coverage on your local
[01:52:50.620 --> 01:52:54.500]   football, you know, Foxy as fair, TBS, yeah.
[01:52:54.500 --> 01:52:59.700]   March 30th, baseball begins.
[01:52:59.700 --> 01:53:10.820]   Uh, Google is hosting Google next 2023 in August in person.
[01:53:10.820 --> 01:53:11.820]   That's a weird time.
[01:53:11.820 --> 01:53:13.300]   Well, it's not Google I/O.
[01:53:13.300 --> 01:53:16.060]   Isn't Google I/O usually earlier, but they didn't mention?
[01:53:16.060 --> 01:53:18.220]   I always usually in May.
[01:53:18.220 --> 01:53:22.260]   And then they have their little cloud, which is just cloud.
[01:53:22.260 --> 01:53:23.260]   Yeah.
[01:53:23.260 --> 01:53:24.260]   Yeah.
[01:53:24.260 --> 01:53:26.260]   Google next is usually in.
[01:53:26.260 --> 01:53:27.260]   Shhh.
[01:53:27.260 --> 01:53:28.620]   Well, this is their cloud thing.
[01:53:28.620 --> 01:53:29.620]   It's cloud next.
[01:53:29.620 --> 01:53:30.620]   Cloud.
[01:53:30.620 --> 01:53:31.620]   Yeah.
[01:53:31.620 --> 01:53:32.620]   Yeah.
[01:53:32.620 --> 01:53:38.300]   It'll host an in person event for the first time since, uh, 20, actually since 2019, the 2020
[01:53:38.300 --> 01:53:39.300]   event was canceled.
[01:53:39.300 --> 01:53:41.940]   2021 was virtual.
[01:53:41.940 --> 01:53:45.180]   There was a physical component last year, but not 2019.
[01:53:45.180 --> 01:53:46.620]   Oh, okay.
[01:53:46.620 --> 01:53:51.340]   So things are back to normal with the Moscone Center cloud next 23.
[01:53:51.340 --> 01:53:53.500]   I've never been have you been to a next?
[01:53:53.500 --> 01:53:54.500]   I've never been.
[01:53:54.500 --> 01:53:55.500]   I have.
[01:53:55.500 --> 01:53:59.380]   I actually met a, uh, not a reader, a listener.
[01:53:59.380 --> 01:54:04.420]   I was in line for a thing and someone was like, Stacy, I want you on this weekend.
[01:54:04.420 --> 01:54:05.420]   Google it.
[01:54:05.420 --> 01:54:06.420]   I was like, holy.
[01:54:06.420 --> 01:54:07.420]   Oh, nice.
[01:54:07.420 --> 01:54:08.420]   Wow.
[01:54:08.420 --> 01:54:09.420]   It was very nice.
[01:54:09.420 --> 01:54:10.420]   Yes.
[01:54:10.420 --> 01:54:11.420]   They were a very nice person.
[01:54:11.420 --> 01:54:12.580]   They, uh, I guess next they'll have a lot of stuff though.
[01:54:12.580 --> 01:54:16.300]   It learned about the latest Google advancements in AI data security productivity and more
[01:54:16.300 --> 01:54:22.500]   explore keynotes, break out sessions, demos, hands on labs, training, certification opportunities
[01:54:22.500 --> 01:54:29.180]   on site here updates on product roadmaps, connect with Google thought leaders.
[01:54:29.180 --> 01:54:37.260]   There will be select next 23 experiences online and on demand.
[01:54:37.260 --> 01:54:41.820]   The rumor is Google I will take place in May, but they haven't made an announcement yet.
[01:54:41.820 --> 01:54:43.500]   Will they let the press back in?
[01:54:43.500 --> 01:54:48.780]   Well, if this, if Google Cloud next is in person, I wonder maybe they will.
[01:54:48.780 --> 01:54:49.780]   What was in person?
[01:54:49.780 --> 01:54:54.300]   Cloud next is an actual like developer focus conference.
[01:54:54.300 --> 01:54:56.900]   I guess IO is too, but it.
[01:54:56.900 --> 01:54:57.900]   It's confusing.
[01:54:57.900 --> 01:54:58.900]   Next seems bigger.
[01:54:58.900 --> 01:55:01.700]   Yeah, bigger than IO really.
[01:55:01.700 --> 01:55:07.940]   Well, I'm just judging by the press events at IO.
[01:55:07.940 --> 01:55:08.940]   Yeah.
[01:55:08.940 --> 01:55:13.380]   Stacy, do you think next is more customer based for cloud services?
[01:55:13.380 --> 01:55:19.500]   I mean, it is more like, I guess I associate next with being a larger event than IO.
[01:55:19.500 --> 01:55:24.460]   But when I think about IO, I only go to like the big first day keynote launch stuff.
[01:55:24.460 --> 01:55:26.820]   Yeah, IO journalists usually are kicked out.
[01:55:26.820 --> 01:55:29.060]   Aren't they after the keynote and you have to go home?
[01:55:29.060 --> 01:55:30.060]   I can't remember.
[01:55:30.060 --> 01:55:31.380]   No, maybe that was Apple.
[01:55:31.380 --> 01:55:32.380]   That was Apple.
[01:55:32.380 --> 01:55:33.380]   It did.
[01:55:33.380 --> 01:55:34.380]   All right.
[01:55:34.380 --> 01:55:35.380]   They just kick you out.
[01:55:35.380 --> 01:55:36.380]   They kick me out.
[01:55:36.380 --> 01:55:40.580]   Chrome for Android is rolling out fingerprint unlock for incognito tabs.
[01:55:40.580 --> 01:55:42.580]   What?
[01:55:42.580 --> 01:55:43.580]   Okay.
[01:55:43.580 --> 01:55:44.580]   Crawl.
[01:55:44.580 --> 01:55:45.580]   Sure.
[01:55:45.580 --> 01:55:47.380]   Let me parse that sentence.
[01:55:47.380 --> 01:55:52.460]   We got poured on your on your tab and your boss comes in.
[01:55:52.460 --> 01:55:55.300]   You can't find out what was on your tab unless you're there with your fingerprint.
[01:55:55.300 --> 01:55:56.300]   Yeah.
[01:55:56.300 --> 01:55:57.300]   Hold against your will.
[01:55:57.300 --> 01:55:58.300]   Yeah.
[01:55:58.300 --> 01:55:59.300]   Oh boy.
[01:55:59.300 --> 01:56:05.900]   Yeah, you could turn on a fingerprint unlock or lock.
[01:56:05.900 --> 01:56:08.380]   Lock incognito tabs when you leave Chrome.
[01:56:08.380 --> 01:56:10.300]   That'll now be a switch.
[01:56:10.300 --> 01:56:13.580]   And you can unlock with your fingerprint or a pin obviously.
[01:56:13.580 --> 01:56:17.180]   Huh, that's nice.
[01:56:17.180 --> 01:56:22.540]   And here's a little public service announcement from your friends at the Verge.
[01:56:22.540 --> 01:56:28.460]   Gmail's new package tracking interfaces live, but you have to turn it on.
[01:56:28.460 --> 01:56:33.260]   Did you know they were going to put a package tracking interface?
[01:56:33.260 --> 01:56:34.260]   I did.
[01:56:34.260 --> 01:56:36.540]   It's opt in.
[01:56:36.540 --> 01:56:38.940]   So what you do.
[01:56:38.940 --> 01:56:47.740]   What you do here is you go to an iOS, you go to the package, you go to the data privacy
[01:56:47.740 --> 01:56:50.060]   menu and there's a package tracking toggle there.
[01:56:50.060 --> 01:56:52.020]   Of course data privacy, that makes sense.
[01:56:52.020 --> 01:56:54.700]   On Android, it's in general.
[01:56:54.700 --> 01:56:57.140]   It's in general.
[01:56:57.140 --> 01:56:59.140]   It's in general.
[01:56:59.140 --> 01:57:03.500]   These changelogs sure are fascinating.
[01:57:03.500 --> 01:57:04.500]   Fascinating.
[01:57:04.500 --> 01:57:05.780]   Let's spend a lot of time on that.
[01:57:05.780 --> 01:57:07.780]   Let's spend more time.
[01:57:07.780 --> 01:57:11.020]   Now it's time for scooter X's changelog.
[01:57:11.020 --> 01:57:13.580]   Here it is.
[01:57:13.580 --> 01:57:17.420]   Because we haven't had it up.
[01:57:17.420 --> 01:57:21.820]   New leak spec suggests pixel tablet pro.
[01:57:21.820 --> 01:57:23.460]   Might not be a thing after all.
[01:57:23.460 --> 01:57:25.860]   Might not be a thing.
[01:57:25.860 --> 01:57:26.860]   We are going to get.
[01:57:26.860 --> 01:57:28.700]   The racks, you just, that's a cruel.
[01:57:28.700 --> 01:57:30.580]   You boost me up and let me down.
[01:57:30.580 --> 01:57:31.860]   Yes, he's mean.
[01:57:31.860 --> 01:57:36.580]   We are going to get a review copy of this just for you, Jeff, so I can talk about it.
[01:57:36.580 --> 01:57:41.220]   Father Robert brought it in after CES during a Twitter a few weeks ago.
[01:57:41.220 --> 01:57:43.020]   Acer is going to send us their new Chromebooks.
[01:57:43.020 --> 01:57:48.900]   These are nice looking devices.
[01:57:48.900 --> 01:57:57.420]   The Chromebook Vira 712, which is designed to be beat in upper classrooms.
[01:57:57.420 --> 01:58:03.300]   And then there's the top of the line, the beautiful aluminum Vero.
[01:58:03.300 --> 01:58:05.500]   I think these are going to be very nice.
[01:58:05.500 --> 01:58:07.460]   The Vero 712, $429.
[01:58:07.460 --> 01:58:10.260]   The HP will launch in April.
[01:58:10.260 --> 01:58:16.540]   The model with the i3 will be 529.
[01:58:16.540 --> 01:58:19.100]   So there you go.
[01:58:19.100 --> 01:58:21.780]   They're also going to do some lower end devices.
[01:58:21.780 --> 01:58:29.100]   The classroom ready stuff is going to be available by the end of April as well.
[01:58:29.100 --> 01:58:34.820]   Let's see what else is in the scooter exchange log.
[01:58:34.820 --> 01:58:37.780]   Google says, lens and maps live.
[01:58:37.780 --> 01:58:43.100]   You are a prelude to its long term vision for AR.
[01:58:43.100 --> 01:58:53.620]   That's when Hiroshi Lockheimer strode onto the stage at the event this morning.
[01:58:53.620 --> 01:58:59.060]   Apparently, even though Hiroshi was there, Jason says he only said the one.
[01:58:59.060 --> 01:59:01.300]   The word Google once, they never said the word Android.
[01:59:01.300 --> 01:59:04.140]   So that's weird.
[01:59:04.140 --> 01:59:05.620]   Yeah, that's weird.
[01:59:05.620 --> 01:59:13.300]   Lockheimer highlighted Google meet with live sharing for Samsung notes and RCS and messages.
[01:59:13.300 --> 01:59:16.780]   Do you still have the flip phones, Stacy, the Galaxy Flip?
[01:59:16.780 --> 01:59:17.780]   I do.
[01:59:17.780 --> 01:59:21.020]   Is the screen holding up and everything?
[01:59:21.020 --> 01:59:22.020]   Yeah.
[01:59:22.020 --> 01:59:23.020]   You like it?
[01:59:23.020 --> 01:59:24.020]   It works.
[01:59:24.020 --> 01:59:25.020]   Yeah.
[01:59:25.020 --> 01:59:26.020]   I mean, I'm still using it.
[01:59:26.020 --> 01:59:27.020]   So they don't.
[01:59:27.020 --> 01:59:32.020]   Go on.
[01:59:32.020 --> 01:59:38.020]   Okay, I guess I'll talk.
[01:59:38.020 --> 01:59:44.020]   I was like, no one else is frozen, but Leo's frozen.
[01:59:44.020 --> 01:59:45.580]   I mean, he was legit frozen.
[01:59:45.580 --> 01:59:46.580]   That was pretty.
[01:59:46.580 --> 01:59:47.580]   That was impressive.
[01:59:47.580 --> 01:59:48.580]   That was good.
[01:59:48.580 --> 01:59:49.580]   No one to do.
[01:59:49.580 --> 01:59:50.580]   I'm melting.
[01:59:50.580 --> 01:59:53.220]   I was like, dang, that's a glitch in the matrix.
[01:59:53.220 --> 01:59:56.220]   They don't usually do the folding funds till the fall.
[01:59:56.220 --> 02:00:02.900]   I don't think I've fallen for the ad campaign, but I think that actually that flip.
[02:00:02.900 --> 02:00:04.940]   I might get the next one when it comes out.
[02:00:04.940 --> 02:00:09.020]   But the main thing, of course, is the concern about the screen, but you use it, right?
[02:00:09.020 --> 02:00:10.260]   You open it and close it open.
[02:00:10.260 --> 02:00:11.260]   Yeah.
[02:00:11.260 --> 02:00:12.260]   And it survives.
[02:00:12.260 --> 02:00:17.580]   I mean, I think I use my phone a normal amount or an abnormal amount.
[02:00:17.580 --> 02:00:20.100]   I probably pick it up 20 or 30 times a day.
[02:00:20.100 --> 02:00:21.100]   Good.
[02:00:21.100 --> 02:00:22.100]   That's good to know.
[02:00:22.100 --> 02:00:24.300]   The hospital I saw a guy using it to do a video call.
[02:00:24.300 --> 02:00:26.140]   I think it's cool because you can leave it there.
[02:00:26.140 --> 02:00:28.140]   I'm folded half folded right on the table.
[02:00:28.140 --> 02:00:29.140]   I don't know.
[02:00:29.140 --> 02:00:31.420]   I'm thinking maybe I don't do that.
[02:00:31.420 --> 02:00:33.340]   I guess I could.
[02:00:33.340 --> 02:00:40.100]   But Chromecast with Google TV 4K gets its first update of 2023.
[02:00:40.100 --> 02:00:44.780]   Google's reportedly testing an alternate homepage with chat, GPT stock, Q&A props.
[02:00:44.780 --> 02:00:49.380]   So that's from the Verge confirming what we thought.
[02:00:49.380 --> 02:00:53.380]   Project Bard.
[02:00:53.380 --> 02:00:56.060]   I think that's it.
[02:00:56.060 --> 02:00:57.060]   It's a great activity.
[02:00:57.060 --> 02:00:58.860]   Yes, that was great.
[02:00:58.860 --> 02:00:59.860]   One more.
[02:00:59.860 --> 02:01:01.580]   I saw this concern to me a little bit.
[02:01:01.580 --> 02:01:03.380]   You don't want it to end too soon.
[02:01:03.380 --> 02:01:05.700]   No, no, we love the changelock.
[02:01:05.700 --> 02:01:10.740]   Google Fi says hackers accessed customer information.
[02:01:10.740 --> 02:01:14.620]   There was a data breach likely related to the security incident at T-Mobile, according
[02:01:14.620 --> 02:01:16.780]   to TechCrunch.
[02:01:16.780 --> 02:01:22.820]   T-Mobile lost a significant, I think, set was at 17 million customer records.
[02:01:22.820 --> 02:01:27.580]   And the email sent to customers on Monday, which I have not received, but a couple of
[02:01:27.580 --> 02:01:29.580]   people yesterday said they had.
[02:01:29.580 --> 02:01:33.780]   Google said the primary network provider for Google Fi recently informed the company that
[02:01:33.780 --> 02:01:38.060]   had been suspicious activity relating to a third party support system containing a limited
[02:01:38.060 --> 02:01:39.740]   amount of Google Fi customer data.
[02:01:39.740 --> 02:01:45.500]   Of course, Google Fi rides on T-Mobile, so it makes sense that this would be an issue with
[02:01:45.500 --> 02:01:52.220]   T-Mobile's hack.
[02:01:52.220 --> 02:01:56.500]   Google said hackers did not take customer's personal information, payment card data,
[02:01:56.500 --> 02:02:01.780]   passwords, pins, or even the contents of text messages or calls.
[02:02:01.780 --> 02:02:05.620]   While some email said there's no action required, at least one Google Fi customer said in a
[02:02:05.620 --> 02:02:14.180]   Reddit post that their phone number had been briefly hijacked, sim swapped, for two hours.
[02:02:14.180 --> 02:02:17.100]   So that's something to be aware of.
[02:02:17.100 --> 02:02:20.700]   I've always felt fairly secure with my Google Fi phone number.
[02:02:20.700 --> 02:02:23.660]   I use that for most of my two factor.
[02:02:23.660 --> 02:02:27.580]   When they say you got to have a phone number for that, I use the Google Fi number, not
[02:02:27.580 --> 02:02:31.380]   the T-Mobile number, but now I'm starting to think, "Huh."
[02:02:31.380 --> 02:02:35.300]   And ladies and gentlemen, that's the Google change law.
[02:02:35.300 --> 02:02:38.500]   Oh, we hardly knew you changed law.
[02:02:38.500 --> 02:02:39.500]   That was a quickie.
[02:02:39.500 --> 02:02:40.940]   You're over so soon.
[02:02:40.940 --> 02:02:43.420]   So soon.
[02:02:43.420 --> 02:02:44.420]   Is there anything?
[02:02:44.420 --> 02:02:47.380]   Who is it who gives you such stuff about change law?
[02:02:47.380 --> 02:02:50.700]   Oh, random listeners.
[02:02:50.700 --> 02:02:53.220]   There's people who really care about the change law.
[02:02:53.220 --> 02:02:54.220]   I get email.
[02:02:54.220 --> 02:02:55.220]   I get email.
[02:02:55.220 --> 02:02:56.660]   I get a lot of email.
[02:02:56.660 --> 02:02:58.780]   I get feedback via email.
[02:02:58.780 --> 02:03:00.620]   Thank you, everybody.
[02:03:00.620 --> 02:03:01.860]   Do you care about this?
[02:03:01.860 --> 02:03:03.660]   Is it the Democracy segment?
[02:03:03.660 --> 02:03:04.660]   Yeah.
[02:03:04.660 --> 02:03:10.100]   Do you care about artifact Kevin's system's new thing?
[02:03:10.100 --> 02:03:11.100]   Yeah.
[02:03:11.100 --> 02:03:14.700]   So the founders of Instagram have started something new, which I immediately, of course,
[02:03:14.700 --> 02:03:21.020]   and I'm sure you did too, asked for admission to, but it's not open to the public yet.
[02:03:21.020 --> 02:03:22.580]   It's called artifact.
[02:03:22.580 --> 02:03:29.980]   And the idea is it is AI curated news, which I honestly know how that's different from
[02:03:29.980 --> 02:03:33.380]   everything else that's out to you.
[02:03:33.380 --> 02:03:36.980]   And what's the state?
[02:03:36.980 --> 02:03:38.980]   Who invented the hashtag?
[02:03:38.980 --> 02:03:40.820]   Oh, Chris Messina.
[02:03:40.820 --> 02:03:44.060]   Chris Messina, we're just in a discussion on Mastod about it.
[02:03:44.060 --> 02:03:46.620]   So Mike Bekie is a flip board guy, right?
[02:03:46.620 --> 02:03:47.620]   Founder of flip board.
[02:03:47.620 --> 02:03:48.620]   Yeah.
[02:03:48.620 --> 02:03:49.620]   So he has some interest in this.
[02:03:49.620 --> 02:03:50.620]   Little flip boardy.
[02:03:50.620 --> 02:03:51.620]   Yeah.
[02:03:51.620 --> 02:03:52.620]   Yeah.
[02:03:52.620 --> 02:03:54.100]   Here's the post.
[02:03:54.100 --> 02:03:58.380]   Excited to announce what Kevin and I have been working on with the talented team the past
[02:03:58.380 --> 02:04:05.780]   year artifact, a personalized news feed driven by the latest in artificial intelligence.
[02:04:05.780 --> 02:04:07.740]   Have you gotten an invite yet, Jeff?
[02:04:07.740 --> 02:04:08.740]   Nope.
[02:04:08.740 --> 02:04:11.820]   Sometimes I feel like these people just had one good idea.
[02:04:11.820 --> 02:04:15.860]   And now they're just like Silicon Valley is full of people who had one good idea.
[02:04:15.860 --> 02:04:19.380]   They managed to make a scale and then they give them money the next time the big idea
[02:04:19.380 --> 02:04:23.340]   comes around to give them something that like, I don't know, I feel like it's just my
[02:04:23.340 --> 02:04:24.340]   skeptical.
[02:04:24.340 --> 02:04:25.340]   Exactly.
[02:04:25.340 --> 02:04:29.100]   One person at Williams is the person who's done it three times.
[02:04:29.100 --> 02:04:31.500]   Meme was me in a huge success.
[02:04:31.500 --> 02:04:33.460]   No, but it's still alive.
[02:04:33.460 --> 02:04:34.460]   It's still there.
[02:04:34.460 --> 02:04:35.460]   Yeah.
[02:04:35.460 --> 02:04:37.900]   Tony Sullivan's does not get it.
[02:04:37.900 --> 02:04:46.580]   Spotify among many tech companies doing big layoffs, including their head of podcasting.
[02:04:46.580 --> 02:04:47.580]   Whoops.
[02:04:47.580 --> 02:04:49.740]   Oh, what do we think of that?
[02:04:49.740 --> 02:04:50.740]   Oops.
[02:04:50.740 --> 02:04:52.820]   Little on the host.
[02:04:52.820 --> 02:04:57.780]   Well, I don't, I don't know.
[02:04:57.780 --> 02:05:05.140]   This is Daniel X email or blog post.
[02:05:05.140 --> 02:05:09.940]   As part of this change, Don Ostrich has decided to depart Spotify.
[02:05:09.940 --> 02:05:11.740]   Don made a tremendous mark, not only on spot.
[02:05:11.740 --> 02:05:13.140]   I'm giving them any money anymore.
[02:05:13.140 --> 02:05:14.140]   Yeah, right.
[02:05:14.140 --> 02:05:17.660]   If you're not going to pay me, I guess I'll quit.
[02:05:17.660 --> 02:05:22.780]   Don is made a tremendous mark, not only on Spotify, but in the audio industry overall.
[02:05:22.780 --> 02:05:27.740]   Because of her efforts, Spotify grew our podcast content by 40 times.
[02:05:27.740 --> 02:05:30.500]   40 times zero is still zero.
[02:05:30.500 --> 02:05:34.660]   So I must have had some.
[02:05:34.660 --> 02:05:39.580]   So if you're of significant innovation in the medium and became the leading music and
[02:05:39.580 --> 02:05:43.220]   podcast service in many markets, blah, blah, blah.
[02:05:43.220 --> 02:05:47.940]   Thanks to her work, Spotify was able to innovate on the ads format itself and more than double
[02:05:47.940 --> 02:05:54.340]   the revenue of our advertising business to how much you think Spotify makes on ads.
[02:05:54.340 --> 02:05:55.340]   No.
[02:05:55.340 --> 02:05:58.040]   They spend a lot.
[02:05:58.040 --> 02:06:01.460]   One point five billion euros.
[02:06:01.460 --> 02:06:03.260]   That's a lot.
[02:06:03.260 --> 02:06:04.260]   Okay.
[02:06:04.260 --> 02:06:07.340]   Whoa is right.
[02:06:07.340 --> 02:06:10.700]   That's like twice what we make.
[02:06:10.700 --> 02:06:15.580]   Where's Anne's coffee machine then?
[02:06:15.580 --> 02:06:21.340]   In the near term, Don will assume the role of senior advisor to help facilitate this
[02:06:21.340 --> 02:06:22.340]   transition.
[02:06:22.340 --> 02:06:23.340]   Yeah.
[02:06:23.340 --> 02:06:27.940]   Do you think there'll be media stories now saying, Oh podcasting is dead because Spotify
[02:06:27.940 --> 02:06:28.940]   is.
[02:06:28.940 --> 02:06:32.380]   If that, yes, of course, that's exactly how the media.
[02:06:32.380 --> 02:06:39.340]   I mean, there were stories earlier, the podcasting hit hit a bump in the road.
[02:06:39.340 --> 02:06:42.780]   So yeah, there's a podcasting winter as it were.
[02:06:42.780 --> 02:06:48.460]   There was a story that last year 80, certainly 80% fewer new podcasts were launched, which
[02:06:48.460 --> 02:06:50.100]   as far as I'm concerned is great.
[02:06:50.100 --> 02:06:51.100]   Keep up the good work.
[02:06:51.100 --> 02:06:53.020]   Stop launching those podcasts.
[02:06:53.020 --> 02:06:55.780]   When they also, another thing they've learned in a Spotify is learning this, but others
[02:06:55.780 --> 02:06:58.260]   like I Heart have learned it better.
[02:06:58.260 --> 02:07:02.740]   These celebrity podcasts are not a panacea.
[02:07:02.740 --> 02:07:06.740]   You know, just cause you got a big name doing a podcast, you get some initial interest,
[02:07:06.740 --> 02:07:09.060]   but it does not mean long term listenership.
[02:07:09.060 --> 02:07:10.060]   All right.
[02:07:10.060 --> 02:07:14.980]   So on the other hand, they, I think Joe Rogan is a good part, good percentage of that 1.5
[02:07:14.980 --> 02:07:16.540]   billion euros.
[02:07:16.540 --> 02:07:18.380]   He's been very successful for Spotify.
[02:07:18.380 --> 02:07:22.340]   Well, is it advertising or is it, is it subscription fees that he drives?
[02:07:22.340 --> 02:07:25.060]   Oh, ads, ads are like a million bucks on Joe Rogan.
[02:07:25.060 --> 02:07:26.460]   They're very expensive.
[02:07:26.460 --> 02:07:27.460]   Wow, really?
[02:07:27.460 --> 02:07:28.460]   Yeah.
[02:07:28.460 --> 02:07:31.140]   Why do you want to talk to those guys that much?
[02:07:31.140 --> 02:07:33.420]   Do they buy that much stuff?
[02:07:33.420 --> 02:07:39.860]   They buy pickup trucks with false go to ads in the back.
[02:07:39.860 --> 02:07:40.860]   I don't know.
[02:07:40.860 --> 02:07:41.860]   It's gone.
[02:07:41.860 --> 02:07:43.100]   You mean those little swinging metal?
[02:07:43.100 --> 02:07:44.100]   Yeah.
[02:07:44.100 --> 02:07:49.780]   No, I don't think, I think Joe's audience is young men and advertisers are desperate
[02:07:49.780 --> 02:07:55.700]   to get young men because they don't, they're not traditional media consumers.
[02:07:55.700 --> 02:07:58.540]   So if that's the only way they can reach them, that's where Joe's going to make a lot of
[02:07:58.540 --> 02:07:59.540]   money.
[02:07:59.540 --> 02:08:02.020]   I'm sure that was the logic anyway behind it.
[02:08:02.020 --> 02:08:03.020]   Yeah.
[02:08:03.020 --> 02:08:08.780]   Men, men like 18 to 25 are no, well, humans 18 to 25 are notoriously hard to reach in
[02:08:08.780 --> 02:08:09.780]   traditional media.
[02:08:09.780 --> 02:08:10.780]   They don't watch TV.
[02:08:10.780 --> 02:08:13.020]   They don't buy magazines.
[02:08:13.020 --> 02:08:14.180]   They barely see billboards.
[02:08:14.180 --> 02:08:16.860]   They don't listen to the radio.
[02:08:16.860 --> 02:08:19.180]   So podcaster, sometimes you only see billboards.
[02:08:19.180 --> 02:08:22.100]   They go whizzing by them at high speed.
[02:08:22.100 --> 02:08:23.100]   Hey, everybody.
[02:08:23.100 --> 02:08:24.100]   Leo LePort here.
[02:08:24.100 --> 02:08:28.700]   I'm the founder and one of the hosts at the Twitch podcast network.
[02:08:28.700 --> 02:08:32.460]   I want to talk to you a little bit about what we do here at Twitch because I think it's
[02:08:32.460 --> 02:08:33.740]   unique.
[02:08:33.740 --> 02:08:41.460]   And I think for anybody who is bringing a product or a service to a tech audience, you
[02:08:41.460 --> 02:08:44.540]   need to know about what we do here at Twitch.
[02:08:44.540 --> 02:08:50.860]   We've built an amazing audience of engaged, intelligent, affluent listeners who listen
[02:08:50.860 --> 02:08:54.660]   to us and trust us when we recommend a product.
[02:08:54.660 --> 02:08:59.700]   Our mission statement is to build a highly engaged community of tech enthusiasts.
[02:08:59.700 --> 02:09:04.980]   But already you should be your ears should be perking up at that because highly engaged
[02:09:04.980 --> 02:09:06.380]   is good for you.
[02:09:06.380 --> 02:09:09.740]   Tech enthusiasts, if that's who you're looking for, this is the place.
[02:09:09.740 --> 02:09:14.460]   We do it by offering them the knowledge they need to understand and use technology in today's
[02:09:14.460 --> 02:09:15.460]   world.
[02:09:15.460 --> 02:09:20.540]   And I hear from our audience all the time part of that knowledge comes from our advertisers.
[02:09:20.540 --> 02:09:21.540]   We are very careful.
[02:09:21.540 --> 02:09:28.660]   We pick advertisers with great products, great services with integrity and introduce them
[02:09:28.660 --> 02:09:33.860]   to our audience with authenticity and genuine enthusiasm.
[02:09:33.860 --> 02:09:37.900]   And that makes our host red ads different from anything else you can buy.
[02:09:37.900 --> 02:09:46.020]   We are literally bringing you to the attention of our audience and giving you a big fat endorsement.
[02:09:46.020 --> 02:09:50.700]   We like to create partnerships with trusted brands, brands who are in it for the long
[02:09:50.700 --> 02:09:54.780]   run, long term partners that want to grow with us.
[02:09:54.780 --> 02:09:57.260]   And we have so many great success stories.
[02:09:57.260 --> 02:10:03.220]   Tim Broome, who founded ITProTV in 2013, started advertising with us on day one has
[02:10:03.220 --> 02:10:05.260]   been with us ever since.
[02:10:05.260 --> 02:10:10.220]   He said, quote, "We would not be where we are today without the Twitter network."
[02:10:10.220 --> 02:10:12.580]   I think the proof is in the pudding.
[02:10:12.580 --> 02:10:17.300]   Others like ITProTV and Audible that have been with us for more than 10 years.
[02:10:17.300 --> 02:10:20.980]   They stick around because their ads work.
[02:10:20.980 --> 02:10:24.020]   And honestly, isn't that why you're buying advertising?
[02:10:24.020 --> 02:10:25.180]   You get a lot with Twit.
[02:10:25.180 --> 02:10:27.620]   We have a very full service attitude.
[02:10:27.620 --> 02:10:32.980]   We almost think of it as kind of artisanal advertising, boutique advertising.
[02:10:32.980 --> 02:10:36.820]   You'll get a full service continuity team.
[02:10:36.820 --> 02:10:40.900]   People who are on the phone with you, who are in touch with you, who support you from
[02:10:40.900 --> 02:10:44.380]   with everything from copywriting to graphic design.
[02:10:44.380 --> 02:10:46.980]   So you are not alone in this.
[02:10:46.980 --> 02:10:49.740]   We embed our ads into the shows.
[02:10:49.740 --> 02:10:51.740]   They're not added later.
[02:10:51.740 --> 02:10:53.060]   They're part of the shows.
[02:10:53.060 --> 02:10:57.580]   In fact, often they're such a part of our shows that our other hosts will chime in on
[02:10:57.580 --> 02:10:59.620]   the ad saying, "Yeah, I love that."
[02:10:59.620 --> 02:11:04.900]   Or just the other day, one of our hosts said, "Man, I really got to buy that."
[02:11:04.900 --> 02:11:09.820]   That's an additional benefit to you because you're hearing people, our audience trusts
[02:11:09.820 --> 02:11:12.740]   saying, "Yeah, that sounds great."
[02:11:12.740 --> 02:11:15.700]   We deliver, always over deliver on impressions.
[02:11:15.700 --> 02:11:19.100]   So you know you're going to get the impressions you expect.
[02:11:19.100 --> 02:11:21.060]   The ads are unique every time.
[02:11:21.060 --> 02:11:22.900]   We don't pre-record them and roll them in.
[02:11:22.900 --> 02:11:26.860]   We are genuinely doing those ads in the middle of the show.
[02:11:26.860 --> 02:11:31.340]   We'll give you great onboarding services, ad tech with pod sites that's free for direct
[02:11:31.340 --> 02:11:32.860]   clients.
[02:11:32.860 --> 02:11:36.660]   Gives you a lot of reporting, gives you a great idea of how well your ads are working.
[02:11:36.660 --> 02:11:40.820]   You'll get courtesy commercials where you actually can take our ads and share them across social
[02:11:40.820 --> 02:11:44.420]   media and landing pages that really extends the reach.
[02:11:44.420 --> 02:11:48.140]   There are other free goodies too, including mentions in our weekly newsletter that sent
[02:11:48.140 --> 02:11:52.580]   to thousands of fans, engaged fans who really want to see this stuff.
[02:11:52.580 --> 02:11:56.580]   We give you bonus ads and social media promotion too.
[02:11:56.580 --> 02:12:03.460]   So if you want to be a long-term partner, introduce your product to a savvy, engaged tech audience.
[02:12:03.460 --> 02:12:07.980]   At twit.tv/advertise, check out those testimonials.
[02:12:07.980 --> 02:12:10.180]   Mark McCrary is the CEO of Authentic.
[02:12:10.180 --> 02:12:15.020]   You probably know him, one of the biggest original podcast advertising companies.
[02:12:15.020 --> 02:12:18.260]   We've been with him for 16 years.
[02:12:18.260 --> 02:12:24.380]   Mark said the feedback from many advertisers over 16 years across a range of product categories.
[02:12:24.380 --> 02:12:29.060]   Everything from razors to computers is that if ads and podcasts are going to work for
[02:12:29.060 --> 02:12:31.940]   a brand, they're going to work on Twitch shows.
[02:12:31.940 --> 02:12:35.380]   I'm very proud of what we do because it's honest.
[02:12:35.380 --> 02:12:36.380]   It's got integrity.
[02:12:36.380 --> 02:12:37.660]   It's authentic.
[02:12:37.660 --> 02:12:43.620]   And it really is a great introduction to our audience of your brand.
[02:12:43.620 --> 02:12:45.420]   Our listeners are smart.
[02:12:45.420 --> 02:12:46.420]   They're engaged.
[02:12:46.420 --> 02:12:47.860]   They're tech savvy.
[02:12:47.860 --> 02:12:49.980]   They're dedicated to our network.
[02:12:49.980 --> 02:12:54.220]   And that's one of the reasons we only work with high integrity partners that we've personally
[02:12:54.220 --> 02:12:55.580]   and thoroughly vetted.
[02:12:55.580 --> 02:12:58.380]   I have absolute approval on everybody.
[02:12:58.380 --> 02:13:01.420]   If you've got a great product, I want to hear from you.
[02:13:01.420 --> 02:13:05.780]   Elevate your brand by reaching out today at advertise at twit.tv.
[02:13:05.780 --> 02:13:07.300]   Break out of the advertising norm.
[02:13:07.300 --> 02:13:10.340]   Grow your brand with host red ads on twit.tv.
[02:13:10.340 --> 02:13:18.340]   Visit twit.tv/advertise for more details or you can email us advertise at twit.tv if
[02:13:18.340 --> 02:13:20.060]   you're ready to launch your campaign now.
[02:13:20.060 --> 02:13:21.500]   I can't wait to see your product.
[02:13:21.500 --> 02:13:23.340]   So give us a ring.
[02:13:23.340 --> 02:13:24.980]   All right.
[02:13:24.980 --> 02:13:25.980]   I'm done.
[02:13:25.980 --> 02:13:27.260]   I don't have anything else to say.
[02:13:27.260 --> 02:13:30.580]   Let's do our picks of the week and move on with our lives.
[02:13:30.580 --> 02:13:32.700]   What do you say, Stacey?
[02:13:32.700 --> 02:13:35.980]   What's your thing of the week?
[02:13:35.980 --> 02:13:36.980]   Sorry.
[02:13:36.980 --> 02:13:41.860]   I just today I've been like so sleepy and cranky.
[02:13:41.860 --> 02:13:42.860]   Good Lord.
[02:13:42.860 --> 02:13:44.300]   All right.
[02:13:44.300 --> 02:13:48.060]   My thing of the week is a book because I was playing with the device that I was going
[02:13:48.060 --> 02:13:49.300]   to tell you about it and I hate it.
[02:13:49.300 --> 02:13:50.300]   I hate this device.
[02:13:50.300 --> 02:13:51.780]   I'm not telling you about it.
[02:13:51.780 --> 02:13:52.780]   This is people.
[02:13:52.780 --> 02:13:53.780]   No.
[02:13:53.780 --> 02:13:58.100]   I don't mind saying something's terrible because we often we only review the good stuff and
[02:13:58.100 --> 02:13:59.780]   people start to think you like everything.
[02:13:59.780 --> 02:14:01.540]   It's good to warn people away.
[02:14:01.540 --> 02:14:03.460]   I will tell you, you know, those smart goggles?
[02:14:03.460 --> 02:14:08.340]   I sent the smart goggles that I was talking about back just so you all know.
[02:14:08.340 --> 02:14:09.580]   Smart glasses.
[02:14:09.580 --> 02:14:11.580]   The smart glasses.
[02:14:11.580 --> 02:14:13.260]   The Thera body smart glasses.
[02:14:13.260 --> 02:14:15.220]   I showed them all the phones back.
[02:14:15.220 --> 02:14:18.940]   I told you all that I was probably going to send them back, but I did send them back.
[02:14:18.940 --> 02:14:20.540]   So did we change your life?
[02:14:20.540 --> 02:14:22.500]   Didn't improve your life.
[02:14:22.500 --> 02:14:26.460]   It did nothing for us and it actively annoyed my teenager.
[02:14:26.460 --> 02:14:30.780]   The idea is you put it on and would massage your eyes.
[02:14:30.780 --> 02:14:37.180]   It would it had a heart rate sensor in the glasses that would also try to match the massage
[02:14:37.180 --> 02:14:41.420]   to your heart rate to help relax you more.
[02:14:41.420 --> 02:14:42.420]   I don't know.
[02:14:42.420 --> 02:14:43.420]   Anyway, you know what?
[02:14:43.420 --> 02:14:46.700]   I like the $10 I spent on that thing that you put in the refrigerator and then put on
[02:14:46.700 --> 02:14:47.700]   your head.
[02:14:47.700 --> 02:14:49.940]   That was that was just silly.
[02:14:49.940 --> 02:14:50.940]   That was it.
[02:14:50.940 --> 02:14:51.940]   Even a river.
[02:14:51.940 --> 02:14:54.700]   But that was a good that was a good buy a lot better than a $200.
[02:14:54.700 --> 02:14:56.700]   Eyeball massager.
[02:14:56.700 --> 02:14:57.700]   Well, good.
[02:14:57.700 --> 02:15:02.580]   Yeah, someone I guess at one point in time, I was really desperate for things.
[02:15:02.580 --> 02:15:08.820]   So I told somebody about the oxo brush thing with the little stand that I had.
[02:15:08.820 --> 02:15:09.820]   Yeah.
[02:15:09.820 --> 02:15:12.060]   Two people emailed me and said they bought it and they loved it.
[02:15:12.060 --> 02:15:16.020]   So I was like, well, I'm glad you should never, never judge people.
[02:15:16.020 --> 02:15:19.660]   You know, Lisa tweeted a picture of me or mastodon.
[02:15:19.660 --> 02:15:23.620]   Tooted a picture of me wearing it and they were all these people saying, what is that?
[02:15:23.620 --> 02:15:25.620]   I must have it.
[02:15:25.620 --> 02:15:27.500]   So I think it's genuine.
[02:15:27.500 --> 02:15:29.100]   I think it's real.
[02:15:29.100 --> 02:15:30.100]   Yeah.
[02:15:30.100 --> 02:15:34.820]   So this week, I'm giving you all a book because, you know, I read a lot.
[02:15:34.820 --> 02:15:39.140]   So, but I really liked this book and I liked it in y'all are kind of nerdy.
[02:15:39.140 --> 02:15:43.060]   It's not a science fiction book, but it has some things that we like and it's called the
[02:15:43.060 --> 02:15:45.380]   Sorcerer of Pyongyang.
[02:15:45.380 --> 02:15:46.380]   Is that how we would say that?
[02:15:46.380 --> 02:15:47.380]   Yeah, yeah.
[02:15:47.380 --> 02:15:48.380]   Yeah.
[02:15:48.380 --> 02:15:49.380]   Yeah.
[02:15:49.380 --> 02:15:54.260]   And it's really, there's no magic in this book.
[02:15:54.260 --> 02:16:03.180]   It's about a boy in North Korea who finds a Dungeons and Dragons book in like the 80s.
[02:16:03.180 --> 02:16:11.900]   And then he plays it with basically that is a thinly used plot device to hinge an entire
[02:16:11.900 --> 02:16:19.020]   story about what it was like to grow up in North Korea in the last 30s, which is really
[02:16:19.020 --> 02:16:20.020]   fascinating.
[02:16:20.020 --> 02:16:22.740]   And so do you think the road knows how what it was like?
[02:16:22.740 --> 02:16:24.540]   I mean, how what's his?
[02:16:24.540 --> 02:16:27.020]   No, he did a lot of research.
[02:16:27.020 --> 02:16:31.780]   He also wrote a book called Far North, I believe.
[02:16:31.780 --> 02:16:33.980]   Yes.
[02:16:33.980 --> 02:16:34.980]   Very fair.
[02:16:34.980 --> 02:16:35.980]   Oh, but that is not.
[02:16:35.980 --> 02:16:36.980]   Yeah.
[02:16:36.980 --> 02:16:41.340]   Oh, it says here, drawing on the author's personal experience of North Korea.
[02:16:41.340 --> 02:16:44.340]   So you guess he has some time spent some time there.
[02:16:44.340 --> 02:16:45.340]   The Sorcerer of Pyongyang.
[02:16:45.340 --> 02:16:51.940]   And it's really well done and it's like, I don't read a lot about North Korea.
[02:16:51.940 --> 02:16:53.740]   You don't?
[02:16:53.740 --> 02:16:54.740]   So I don't.
[02:16:54.740 --> 02:16:58.780]   I mean, I've read like three or four books about it.
[02:16:58.780 --> 02:16:59.780]   Yeah.
[02:16:59.780 --> 02:17:06.580]   So I'd be curious if people actually have a much, you know, I love entering new cultures
[02:17:06.580 --> 02:17:10.780]   through the medium of fiction just because I agree.
[02:17:10.780 --> 02:17:17.180]   It's a little less like, I feel like you get a more sense of like, especially if it's
[02:17:17.180 --> 02:17:20.940]   written by someone from that culture, this is obviously not that you get a little bit
[02:17:20.940 --> 02:17:22.460]   more of a flavor.
[02:17:22.460 --> 02:17:26.940]   Like that's why I liked reading like the three body problem because it gave me kind of, I
[02:17:26.940 --> 02:17:31.580]   felt like an insight into what it would be like to be not an individualistic society.
[02:17:31.580 --> 02:17:33.580]   Oh boy, you saw me in the Genre B with that one.
[02:17:33.580 --> 02:17:34.580]   I know.
[02:17:34.580 --> 02:17:38.980]   Anyway, the point is I really liked this book.
[02:17:38.980 --> 02:17:40.740]   It was engaging.
[02:17:40.740 --> 02:17:44.820]   It taught me or it talks actually about the hacking armies.
[02:17:44.820 --> 02:17:51.340]   Like the people who, yeah, anyway, like North Korea's ransomware and revenue generation
[02:17:51.340 --> 02:17:58.420]   through ransomware and hacking and other special ways that they generate revenue despite sanctions.
[02:17:58.420 --> 02:18:04.180]   And also I played D&D as a kid and, you know, it was pretty magic for me growing up and
[02:18:04.180 --> 02:18:08.020]   maybe a future Stacey's book club selection.
[02:18:08.020 --> 02:18:10.100]   There is not any.
[02:18:10.100 --> 02:18:12.140]   It's not sci-fi.
[02:18:12.140 --> 02:18:13.140]   It's not sci-fi.
[02:18:13.140 --> 02:18:15.580]   It's just a novel.
[02:18:15.580 --> 02:18:19.980]   Stacey's book club, which is coming up March 30th is the book you've chosen the book.
[02:18:19.980 --> 02:18:23.500]   Wait, we've chosen the book and we may have to change the date because I think I have
[02:18:23.500 --> 02:18:25.220]   to give a talk on the 29.
[02:18:25.220 --> 02:18:26.220]   We could do that.
[02:18:26.220 --> 02:18:27.220]   That's far and away.
[02:18:27.220 --> 02:18:31.980]   The book though, and you might want to get started as Emily St. John Mandel's sea of
[02:18:31.980 --> 02:18:34.660]   tranquility.
[02:18:34.660 --> 02:18:37.220]   It's fairly short, I believe so.
[02:18:37.220 --> 02:18:38.220]   Yeah, some time.
[02:18:38.220 --> 02:18:40.900]   Yeah, it's five hours, 47 minutes if you listen to it.
[02:18:40.900 --> 02:18:42.060]   That's not good.
[02:18:42.060 --> 02:18:43.060]   Good narrator too.
[02:18:43.060 --> 02:18:44.060]   I love John Lee.
[02:18:44.060 --> 02:18:45.060]   Yeah.
[02:18:45.060 --> 02:18:46.060]   Yeah.
[02:18:46.060 --> 02:18:47.060]   Nice.
[02:18:47.060 --> 02:18:48.060]   It's a beautiful book.
[02:18:48.060 --> 02:18:53.460]   If you're not a member of Club Twin, this is one of the many wonderful reasons to join
[02:18:53.460 --> 02:18:54.460]   club to it.
[02:18:54.460 --> 02:18:56.020]   We've actually got some events coming up.
[02:18:56.020 --> 02:19:01.540]   When to Dow, the host of all about Android, will be on February 9th, a week from tomorrow
[02:19:01.540 --> 02:19:02.900]   for a fireside chat.
[02:19:02.900 --> 02:19:09.180]   And the next day, Daniel Suarez, who is the author of many great books, including Demon
[02:19:09.180 --> 02:19:13.460]   and Freedom TM, his new book, Critical Mask, just came out yesterday.
[02:19:13.460 --> 02:19:14.500]   I started reading it.
[02:19:14.500 --> 02:19:15.500]   I'm excited.
[02:19:15.500 --> 02:19:20.020]   He's going to be our guest on February 10th for a triangulation, but club members will
[02:19:20.020 --> 02:19:21.020]   get special access.
[02:19:21.020 --> 02:19:23.140]   They'll be able to ask questions and so forth.
[02:19:23.140 --> 02:19:29.180]   Samable Sammons coming up, the book club and Victor, one of our great editors, long
[02:19:29.180 --> 02:19:32.300]   time employee, is going to do a chat.
[02:19:32.300 --> 02:19:36.140]   We've been doing some behind the scenes, meeting some of the behind scenes people.
[02:19:36.140 --> 02:19:37.140]   That's all coming up.
[02:19:37.140 --> 02:19:42.860]   Thanks to Ant, our club community manager and ringleader.
[02:19:42.860 --> 02:19:45.940]   If you're not a member of Club Twin, now's the time.
[02:19:45.940 --> 02:19:52.060]   Go to twit.tv/clubtwit and sign up today, seven bucks a month, $84 a year, or you can
[02:19:52.060 --> 02:19:55.140]   get a corporate membership for everybody in your company.
[02:19:55.140 --> 02:19:58.260]   You'll get ad-free show versions of all the shows, including this one.
[02:19:58.260 --> 02:20:03.020]   You'll get special shows we don't put out on the Twit Plus feed in public.
[02:20:03.020 --> 02:20:07.820]   For instance, hands on Macintosh with Micah Sargent, hands on Windows with Paul Therat,
[02:20:07.820 --> 02:20:10.340]   untitled Linux show with Jonathan Bennett.
[02:20:10.340 --> 02:20:14.700]   You also get access to the Discord, which is so much fun.
[02:20:14.700 --> 02:20:16.300]   Just love all the folks in the Discord.
[02:20:16.300 --> 02:20:17.300]   We have so much fun.
[02:20:17.300 --> 02:20:18.900]   Stacy's in there and Jeff's in there.
[02:20:18.900 --> 02:20:21.820]   We're just having a good old time.
[02:20:21.820 --> 02:20:22.820]   Lots of memes.
[02:20:22.820 --> 02:20:23.820]   Memes.
[02:20:23.820 --> 02:20:26.340]   There's lots of fun people and we're in there too.
[02:20:26.340 --> 02:20:28.340]   It's a pleasure.
[02:20:28.340 --> 02:20:30.620]   A pleasure in there.
[02:20:30.620 --> 02:20:33.220]   I hang out in the club all the time.
[02:20:33.220 --> 02:20:34.220]   I love it.
[02:20:34.220 --> 02:20:35.220]   Twit.tv/clubtwit.
[02:20:35.220 --> 02:20:36.220]   Join us.
[02:20:36.220 --> 02:20:37.220]   Jeff Jarvis, number...
[02:20:37.220 --> 02:20:39.220]   Wow, here's a new one.
[02:20:39.220 --> 02:20:42.780]   We've got a moral panic, Jeff, with your old picture.
[02:20:42.780 --> 02:20:44.580]   I'm like, I'm gonna steal that one.
[02:20:44.580 --> 02:20:45.580]   Thank you very much.
[02:20:45.580 --> 02:20:46.580]   I like that.
[02:20:46.580 --> 02:20:47.580]   That's the race.
[02:20:47.580 --> 02:20:48.580]   Yeah.
[02:20:48.580 --> 02:20:49.580]   It's a sticker.
[02:20:49.580 --> 02:20:50.580]   I love that.
[02:20:50.580 --> 02:20:51.580]   Next book.
[02:20:51.580 --> 02:20:52.580]   It's a sticker in the server.
[02:20:52.580 --> 02:20:53.580]   Everybody can use it now.
[02:20:53.580 --> 02:20:54.580]   Oh, man.
[02:20:54.580 --> 02:20:55.580]   That's a good picture of you.
[02:20:55.580 --> 02:20:57.300]   Isn't that the one you bought?
[02:20:57.300 --> 02:20:59.020]   It looks like the one I bought.
[02:20:59.020 --> 02:21:00.020]   It's roughly the same thing.
[02:21:00.020 --> 02:21:01.020]   I love it.
[02:21:01.020 --> 02:21:03.940]   It looks like an AI generated version.
[02:21:03.940 --> 02:21:04.940]   Anyway.
[02:21:04.940 --> 02:21:10.100]   All right, so I've got a number, a Stacy bake.
[02:21:10.100 --> 02:21:11.100]   Stacy bake.
[02:21:11.100 --> 02:21:12.100]   What?
[02:21:12.100 --> 02:21:13.100]   Stacy bake.
[02:21:13.100 --> 02:21:14.100]   Stacy bake.
[02:21:14.100 --> 02:21:15.100]   How you said a Stacy bake?
[02:21:15.100 --> 02:21:17.060]   I was like, is it a waffle?
[02:21:17.060 --> 02:21:18.060]   It's a cake.
[02:21:18.060 --> 02:21:20.460]   I'm in there.
[02:21:20.460 --> 02:21:24.580]   Appliance makers said that 50% of customers won't connect their smart appliances.
[02:21:24.580 --> 02:21:25.580]   Yeah, I saw that.
[02:21:25.580 --> 02:21:26.580]   Oh, yeah.
[02:21:26.580 --> 02:21:28.540]   We talked about that last week on the show.
[02:21:28.540 --> 02:21:30.980]   It's up a lot.
[02:21:30.980 --> 02:21:33.900]   Just so you know, remove buys a smart device.
[02:21:33.900 --> 02:21:36.340]   It doesn't put it on the internet because they're all smart now.
[02:21:36.340 --> 02:21:37.340]   I don't.
[02:21:37.340 --> 02:21:38.340]   Yeah, they're all smart.
[02:21:38.340 --> 02:21:43.780]   I have a I actually I have two large appliances, an oven and a washing machine that are internet
[02:21:43.780 --> 02:21:47.740]   connected, but I have only connected the oven and not the washing machine.
[02:21:47.740 --> 02:21:48.740]   Right.
[02:21:48.740 --> 02:21:49.980]   So I fit this.
[02:21:49.980 --> 02:21:50.980]   Stacy.
[02:21:50.980 --> 02:21:55.340]   Even Stacy, they cry, cry deep tears over that.
[02:21:55.340 --> 02:21:56.580]   So that was one.
[02:21:56.580 --> 02:21:59.180]   The other one is a little tic-tac quarter moment.
[02:21:59.180 --> 02:22:02.020]   I have found my spirit animal.
[02:22:02.020 --> 02:22:03.660]   Team dog.
[02:22:03.660 --> 02:22:05.180]   I'm on team dog here.
[02:22:05.180 --> 02:22:06.340]   My spirit animal.
[02:22:06.340 --> 02:22:07.900]   You'll see what I mean here.
[02:22:07.900 --> 02:22:11.740]   Ladies and gentlemen, I give you team dog.
[02:22:11.740 --> 02:22:14.100]   He's walking like an Egyptian.
[02:22:14.100 --> 02:22:15.180]   I can't play the music.
[02:22:15.180 --> 02:22:17.700]   If I could, I would.
[02:22:17.700 --> 02:22:18.900]   He's now going to cross the brain.
[02:22:18.900 --> 02:22:20.900]   Oh, no, he's not going to cross the brain.
[02:22:20.900 --> 02:22:21.900]   Now I know.
[02:22:21.900 --> 02:22:24.500]   He says, no, no bridges.
[02:22:24.500 --> 02:22:25.500]   A smart dog.
[02:22:25.500 --> 02:22:27.300]   Who just sparked dog?
[02:22:27.300 --> 02:22:28.300]   No, bridge.
[02:22:28.300 --> 02:22:30.700]   No, that you wouldn't be afraid of that bridge.
[02:22:30.700 --> 02:22:31.700]   Would you, Jeff?
[02:22:31.700 --> 02:22:34.540]   Little plank bridge across a ravine.
[02:22:34.540 --> 02:22:35.540]   No.
[02:22:35.540 --> 02:22:37.500]   Oh, that dog really doesn't want to go.
[02:22:37.500 --> 02:22:38.500]   Poor pooch.
[02:22:38.500 --> 02:22:41.740]   Oh, what a matter of what it's done.
[02:22:41.740 --> 02:22:42.740]   I promise.
[02:22:42.740 --> 02:22:43.740]   Oh, poor dog.
[02:22:43.740 --> 02:22:44.740]   I'm just dug in.
[02:22:44.740 --> 02:22:45.740]   He says, I ain't going.
[02:22:45.740 --> 02:22:46.740]   No, no, no.
[02:22:46.740 --> 02:22:47.740]   No.
[02:22:47.740 --> 02:22:51.780]   No, no, no, no, no, you don't pet me.
[02:22:51.780 --> 02:22:52.780]   It doesn't matter.
[02:22:52.780 --> 02:22:53.780]   I don't want to do it.
[02:22:53.780 --> 02:22:56.020]   I don't want to go cross the bridge.
[02:22:56.020 --> 02:22:59.060]   Mr. Ant Pruitt, what should we give?
[02:22:59.060 --> 02:23:02.540]   What plugs would you like to give?
[02:23:02.540 --> 02:23:06.740]   My plug is I watched a movie here recently.
[02:23:06.740 --> 02:23:09.460]   Critics hated it, and the audience hated it.
[02:23:09.460 --> 02:23:12.220]   And I guess I get it.
[02:23:12.220 --> 02:23:17.700]   But at the same time, it made me reflect in this movie called You
[02:23:17.700 --> 02:23:19.340]   People is on Netflix.
[02:23:19.340 --> 02:23:20.340]   I like the one.
[02:23:20.340 --> 02:23:21.340]   I'm trying to hear about this one.
[02:23:21.340 --> 02:23:22.340]   Yeah.
[02:23:22.340 --> 02:23:24.340]   featuring what's his name?
[02:23:24.340 --> 02:23:25.340]   Jonah Hill.
[02:23:25.340 --> 02:23:26.340]   Jonah Hill.
[02:23:26.340 --> 02:23:27.340]   Jonah Hill.
[02:23:27.340 --> 02:23:31.940]   And I believe the lady, her last name is London.
[02:23:31.940 --> 02:23:37.420]   And I thought it was OK, but it points out with the two of them being in an interracial
[02:23:37.420 --> 02:23:38.980]   relationship.
[02:23:38.980 --> 02:23:42.620]   And she has a Muslim background.
[02:23:42.620 --> 02:23:47.460]   He has a Jewish background in the family's meat and all of the tropes and stuff that
[02:23:47.460 --> 02:23:48.820]   comes along with that.
[02:23:48.820 --> 02:23:55.220]   But at the same time, it made me think about some of my past experiences, actually, as
[02:23:55.220 --> 02:24:03.780]   recent as 2020 in 2019, where I pretty much was tokenized at times by people that I thought
[02:24:03.780 --> 02:24:04.780]   were my friends.
[02:24:04.780 --> 02:24:05.780]   Hear me.
[02:24:05.780 --> 02:24:06.780]   And it didn't hit me until later.
[02:24:06.780 --> 02:24:07.780]   I thought, hear.
[02:24:07.780 --> 02:24:08.780]   Not hear.
[02:24:08.780 --> 02:24:09.780]   Not hear.
[02:24:09.780 --> 02:24:10.780]   Not hear.
[02:24:10.780 --> 02:24:11.780]   Not hear.
[02:24:11.780 --> 02:24:12.780]   By people that I thought were my friends.
[02:24:12.780 --> 02:24:19.300]   That movie just reminded me of it and it made me think how there are a lot of people on both
[02:24:19.300 --> 02:24:22.820]   both sides of the spectrum where we go in with assumptions.
[02:24:22.820 --> 02:24:26.900]   So when I introduce you to my friends is my black friend, you don't like that?
[02:24:26.900 --> 02:24:27.900]   Yeah, we're going to fight.
[02:24:27.900 --> 02:24:29.900]   I'll just go in there as you know, right now.
[02:24:29.900 --> 02:24:34.540]   Oh, I was like, you've been warned.
[02:24:34.540 --> 02:24:35.540]   I liked it.
[02:24:35.540 --> 02:24:37.980]   You did my black friend and my Jewish friend.
[02:24:37.980 --> 02:24:38.980]   Right.
[02:24:38.980 --> 02:24:39.980]   I don't know that.
[02:24:39.980 --> 02:24:41.460]   It goes over some stuff like that.
[02:24:41.460 --> 02:24:46.220]   And I thought they did a good job of at least putting it out there and trying to have some
[02:24:46.220 --> 02:24:47.220]   fun with it.
[02:24:47.220 --> 02:24:48.940]   It's the perfect name for that, isn't it?
[02:24:48.940 --> 02:24:49.940]   It is.
[02:24:49.940 --> 02:24:51.500]   You people.
[02:24:51.500 --> 02:24:55.580]   And I read through a lot of the critics and the audience's audiences comments there
[02:24:55.580 --> 02:25:01.140]   on rotten tomatoes and it just fascinates me how people are so upset about this movie.
[02:25:01.140 --> 02:25:07.180]   And then I looked at it and I was like, yeah, this person is white and you don't have that
[02:25:07.180 --> 02:25:11.020]   same perspective that I would have someone that lived it.
[02:25:11.020 --> 02:25:15.100]   All right, I'm going to watch it knowing that I'm going to put myself in your shoes and
[02:25:15.100 --> 02:25:16.460]   watch it and that in mind.
[02:25:16.460 --> 02:25:17.460]   Is it a comedy?
[02:25:17.460 --> 02:25:18.460]   It's a comedy.
[02:25:18.460 --> 02:25:19.460]   It's a comedy.
[02:25:19.460 --> 02:25:20.460]   It's a true comedy.
[02:25:20.460 --> 02:25:22.260]   It's like Eddie Murphy's in it.
[02:25:22.260 --> 02:25:24.060]   It's kind of an amazing cast.
[02:25:24.060 --> 02:25:27.540]   I mean, he's done serious or action stuff.
[02:25:27.540 --> 02:25:29.980]   I love, you know what, Eddie's back.
[02:25:29.980 --> 02:25:32.100]   He's done some great stuff.
[02:25:32.100 --> 02:25:33.100]   What was that?
[02:25:33.100 --> 02:25:34.580]   He was good in this one.
[02:25:34.580 --> 02:25:36.940]   Yeah, he was good in this one.
[02:25:36.940 --> 02:25:38.180]   Oh, you know what?
[02:25:38.180 --> 02:25:39.340]   We forgot something, Leo.
[02:25:39.340 --> 02:25:40.340]   What was that?
[02:25:40.340 --> 02:25:43.100]   I mentioned, you were done with your things.
[02:25:43.100 --> 02:25:44.100]   I'm sorry.
[02:25:44.100 --> 02:25:45.100]   No, what is it?
[02:25:45.100 --> 02:25:46.100]   Just one flight show.
[02:25:46.100 --> 02:25:47.700]   We're going to do a more next week.
[02:25:47.700 --> 02:25:48.700]   Yeah.
[02:25:48.700 --> 02:25:49.700]   Next week.
[02:25:49.700 --> 02:25:50.700]   Right.
[02:25:50.700 --> 02:25:51.700]   Mr.
[02:25:51.700 --> 02:25:52.700]   Flour.
[02:25:52.700 --> 02:25:58.220]   We should get a J. Jason, if you get Glenn on the show next week, that'd be good.
[02:25:58.220 --> 02:25:59.220]   Yeah.
[02:25:59.220 --> 02:26:00.220]   Just to talk about it.
[02:26:00.220 --> 02:26:01.220]   It doesn't have to be for the whole show.
[02:26:01.220 --> 02:26:02.220]   I can't be.
[02:26:02.220 --> 02:26:03.220]   Sweet.
[02:26:03.220 --> 02:26:04.220]   Yeah.
[02:26:04.220 --> 02:26:07.220]   I don't think Stacy, well, you bet Glenn in person, but you haven't been on the show
[02:26:07.220 --> 02:26:08.220]   with him.
[02:26:08.220 --> 02:26:11.660]   We're going to be together because usually, you know, have not been the same person.
[02:26:11.660 --> 02:26:12.660]   Yeah.
[02:26:12.660 --> 02:26:13.660]   Yeah.
[02:26:13.660 --> 02:26:14.660]   We don't know.
[02:26:14.660 --> 02:26:17.140]   And I let me tell you about my new thoughts.
[02:26:17.140 --> 02:26:18.140]   There's an AI.
[02:26:18.140 --> 02:26:20.060]   Well, he is your neighbor.
[02:26:20.060 --> 02:26:23.260]   I mean, he lives in the Seattle area, so you should get to know him.
[02:26:23.260 --> 02:26:24.260]   He's great.
[02:26:24.260 --> 02:26:25.260]   A cross a body of water.
[02:26:25.260 --> 02:26:26.260]   I do know him.
[02:26:26.260 --> 02:26:27.260]   Oh, never mind.
[02:26:27.260 --> 02:26:28.260]   I've been a person.
[02:26:28.260 --> 02:26:31.260]   Oh, I've done Glenn for years.
[02:26:31.260 --> 02:26:33.260]   What do you know?
[02:26:33.260 --> 02:26:38.940]   Now, is Matt as HE double hockey sticks, and he's not going to take it anymore.
[02:26:38.940 --> 02:26:40.420]   No, I'm not.
[02:26:40.420 --> 02:26:48.900]   And this is, I've spoken and offline to some folks about this here recently, but I'm campaigning,
[02:26:48.900 --> 02:26:52.700]   if you will, and I'm campaigning for my son.
[02:26:52.700 --> 02:26:57.820]   The picture there that you have on the screen that reads that he is the back of the year.
[02:26:57.820 --> 02:27:02.020]   He's first team this and honorable mentioned that and so on and so forth.
[02:27:02.020 --> 02:27:04.220]   And you've seen me share highlights of him.
[02:27:04.220 --> 02:27:05.220]   It's amazing.
[02:27:05.220 --> 02:27:06.220]   Throughout the season.
[02:27:06.220 --> 02:27:07.220]   Yeah.
[02:27:07.220 --> 02:27:12.700]   You know, his very first playoff football game, he had seven deck, um, touchdowns.
[02:27:12.700 --> 02:27:16.780]   And he does not have one scholarship offer as of yet.
[02:27:16.780 --> 02:27:17.780]   What?
[02:27:17.780 --> 02:27:18.780]   Yeah, his teammates.
[02:27:18.780 --> 02:27:24.420]   Yeah, there's a teammate that has a scholarship offer, but he does not.
[02:27:24.420 --> 02:27:29.660]   And it pisses me off because this is me taking dad out.
[02:27:29.660 --> 02:27:30.900]   Okay.
[02:27:30.900 --> 02:27:34.580]   I noticed kid works hard, works really, really hard.
[02:27:34.580 --> 02:27:37.940]   I noticed kid has a GPA of like 4.2.
[02:27:37.940 --> 02:27:38.940]   Wow.
[02:27:38.940 --> 02:27:40.820]   I noticed kid is loved in the community.
[02:27:40.820 --> 02:27:45.620]   I, you know, I go out and people recognize me because yeah, it's not a lot of black people
[02:27:45.620 --> 02:27:46.620]   here.
[02:27:46.620 --> 02:27:47.620]   Oh, yeah.
[02:27:47.620 --> 02:27:50.820]   Your son's a quarterback and he's so great and he come in and he helped us do this and
[02:27:50.820 --> 02:27:54.340]   blah, you know, he's, he's a good kid.
[02:27:54.340 --> 02:27:58.380]   And it sort of bugs me that there's people out there in this world that are always trying
[02:27:58.380 --> 02:28:00.620]   to do the right thing and do the good thing.
[02:28:00.620 --> 02:28:04.140]   And they always tend to come up short for whatever reason.
[02:28:04.140 --> 02:28:10.060]   Yet there's idiots out there and evil people out there and that just get away with stuff
[02:28:10.060 --> 02:28:15.460]   and half ass things and don't work hard and they end up being a zillionaires or just
[02:28:15.460 --> 02:28:16.940]   skirting rules and all of that.
[02:28:16.940 --> 02:28:18.340]   And that just pisses me off.
[02:28:18.340 --> 02:28:24.940]   So again, I am just campaign, campaign in for my boy because I can't afford to pay for
[02:28:24.940 --> 02:28:28.340]   college, but I know he wants to go to college and he deserves to go to college.
[02:28:28.340 --> 02:28:34.860]   And football is one way in in addition to his brain and track and so forth.
[02:28:34.860 --> 02:28:38.140]   So Jacob Pruitt, I'm campaigning for you, boy.
[02:28:38.140 --> 02:28:40.620]   I've told him personally as a proud dad.
[02:28:40.620 --> 02:28:43.300]   I'm going to for your consideration.
[02:28:43.300 --> 02:28:44.300]   Jacob Pruitt.
[02:28:44.300 --> 02:28:46.660]   He's the next Patrick Mahomes.
[02:28:46.660 --> 02:28:49.900]   Guys, you know, get in there and get him while you can.
[02:28:49.900 --> 02:28:50.900]   Yeah.
[02:28:50.900 --> 02:28:51.900]   Look at that kid.
[02:28:51.900 --> 02:28:52.900]   Watch this.
[02:28:52.900 --> 02:28:53.900]   Watch this.
[02:28:53.900 --> 02:28:54.900]   Oh my goodness.
[02:28:54.900 --> 02:28:55.900]   He's got legs.
[02:28:55.900 --> 02:28:56.900]   Holy cow.
[02:28:56.900 --> 02:28:57.900]   Running them all.
[02:28:57.900 --> 02:29:00.900]   Oh, they must hate him.
[02:29:00.900 --> 02:29:02.900]   Oh, no, no, no.
[02:29:02.900 --> 02:29:05.900]   What are you going to do with a guy in his row?
[02:29:05.900 --> 02:29:07.900]   He can laser him a throw.
[02:29:07.900 --> 02:29:10.140]   Oh my goodness.
[02:29:10.140 --> 02:29:14.980]   You know, and he's had he's had some visitors from some scouts here recently, but these
[02:29:14.980 --> 02:29:18.820]   scouts, you know, it is nice, but hey, show me the money.
[02:29:18.820 --> 02:29:19.900]   I'm just going to say like that.
[02:29:19.900 --> 02:29:20.900]   Show me the money.
[02:29:20.900 --> 02:29:21.900]   Say it.
[02:29:21.900 --> 02:29:22.900]   All the scholarships are.
[02:29:22.900 --> 02:29:26.620]   You know, it's such a it's a being in the biz.
[02:29:26.620 --> 02:29:31.860]   The corruption of it is we raise money to pay to ourselves, right?
[02:29:31.860 --> 02:29:33.420]   We don't even pay it to Jacob.
[02:29:33.420 --> 02:29:34.420]   Right.
[02:29:34.420 --> 02:29:35.420]   College pays themselves.
[02:29:35.420 --> 02:29:37.860]   So Jacob can go there and work for the college.
[02:29:37.860 --> 02:29:38.860]   Right.
[02:29:38.860 --> 02:29:43.940]   No, because the money that they raise from alumni for their football programs is very
[02:29:43.940 --> 02:29:45.100]   important.
[02:29:45.100 --> 02:29:48.260]   I know the CUNY football team is amazing, Jeff.
[02:29:48.260 --> 02:29:49.260]   Wow.
[02:29:49.260 --> 02:29:50.740]   Look at him.
[02:29:50.740 --> 02:29:52.820]   He's so fast.
[02:29:52.820 --> 02:29:54.300]   He's just out running everybody.
[02:29:54.300 --> 02:29:55.300]   Look at him go.
[02:29:55.300 --> 02:29:56.300]   Don't blink.
[02:29:56.300 --> 02:29:57.300]   He's gone.
[02:29:57.300 --> 02:29:59.500]   You could you even lost track of him.
[02:29:59.500 --> 02:30:02.500]   Where'd he go?
[02:30:02.500 --> 02:30:03.500]   That's great.
[02:30:03.500 --> 02:30:04.500]   That's awesome.
[02:30:04.500 --> 02:30:05.500]   So he can't harm.
[02:30:05.500 --> 02:30:09.900]   They want to throw the whole, you know, he's a dual.
[02:30:09.900 --> 02:30:13.820]   They call it dual threat quarterback, which means he's a quarterback that can run, but
[02:30:13.820 --> 02:30:15.220]   can't necessarily throw.
[02:30:15.220 --> 02:30:17.740]   But yet I'm looking at touchdown passes right here.
[02:30:17.740 --> 02:30:18.740]   Yeah.
[02:30:18.740 --> 02:30:22.580]   Well, you've made a heck of a real step in his quarterback when another kid got injured,
[02:30:22.580 --> 02:30:23.580]   right?
[02:30:23.580 --> 02:30:24.580]   Right.
[02:30:24.580 --> 02:30:30.900]   And because the senior quarterback got hurt and because he was totally fine, not competing
[02:30:30.900 --> 02:30:34.300]   for that role because the senior quarterback was pretty damn good.
[02:30:34.300 --> 02:30:36.220]   But and we said, you know what?
[02:30:36.220 --> 02:30:37.220]   It's fine.
[02:30:37.220 --> 02:30:38.220]   Your time is coming.
[02:30:38.220 --> 02:30:42.540]   Just just book and contribute somewhere else because, oh, by the way, he's a team player.
[02:30:42.540 --> 02:30:45.140]   He wanted to be on the field and contribute in other ways.
[02:30:45.140 --> 02:30:48.180]   So when is the season for getting these scholarships in?
[02:30:48.180 --> 02:30:50.940]   I mean, is it we just nonstop?
[02:30:50.940 --> 02:30:51.940]   Okay.
[02:30:51.940 --> 02:30:53.580]   So it's not a day is today.
[02:30:53.580 --> 02:30:56.740]   Yeah, signing day is today.
[02:30:56.740 --> 02:31:02.300]   But you know, right now you got to get your name out there because the transfer portal,
[02:31:02.300 --> 02:31:05.180]   excuse me, the transfer portal has changed things.
[02:31:05.180 --> 02:31:09.820]   It made it more difficult because colleges are just going to other colleges and getting
[02:31:09.820 --> 02:31:11.780]   players instead of going to high school.
[02:31:11.780 --> 02:31:12.780]   Where does he want to go to college?
[02:31:12.780 --> 02:31:13.780]   Where does he want to go to school?
[02:31:13.780 --> 02:31:17.860]   It's less than you would love to be in Oregon, duck, but he would love to play pretty much
[02:31:17.860 --> 02:31:22.900]   in a lot of the teams in the Pac-12 because they offer the education that he wants.
[02:31:22.900 --> 02:31:23.900]   He's smart.
[02:31:23.900 --> 02:31:24.900]   He's a scholar athlete.
[02:31:24.900 --> 02:31:25.900]   He doesn't want to just be a foot.
[02:31:25.900 --> 02:31:27.700]   What does he want to study?
[02:31:27.700 --> 02:31:30.940]   He's wanting to get more into art and design.
[02:31:30.940 --> 02:31:31.940]   Good.
[02:31:31.940 --> 02:31:32.940]   Yes.
[02:31:32.940 --> 02:31:33.940]   You know, he's a model now.
[02:31:33.940 --> 02:31:35.820]   He's gorgeous right now.
[02:31:35.820 --> 02:31:38.140]   He's into fashion.
[02:31:38.140 --> 02:31:40.340]   So he's he has a sewing machine here.
[02:31:40.340 --> 02:31:41.340]   He'll sew things together.
[02:31:41.340 --> 02:31:42.340]   What a cool kid.
[02:31:42.340 --> 02:31:44.340]   I'll make his own damn uniform.
[02:31:44.340 --> 02:31:45.340]   If I had a relationship.
[02:31:45.340 --> 02:31:48.420]   So I'm a campaign and for my boy.
[02:31:48.420 --> 02:31:49.420]   Well, you should.
[02:31:49.420 --> 02:31:50.420]   Brown Papa.
[02:31:50.420 --> 02:31:51.420]   Good for you.
[02:31:51.420 --> 02:31:53.460]   Well, we'll support you on that.
[02:31:53.460 --> 02:31:54.460]   How could people.
[02:31:54.460 --> 02:31:55.460]   What should people do?
[02:31:55.460 --> 02:31:56.460]   How can they reach you?
[02:31:56.460 --> 02:31:57.460]   Just go look at your Twitter, right?
[02:31:57.460 --> 02:31:58.460]   And respond.
[02:31:58.460 --> 02:32:02.220]   Just go follow my Twitter because my Twitter has now been dedicated to campaigning him.
[02:32:02.220 --> 02:32:06.940]   You know, your college football fan and you know people that, yeah, contact.
[02:32:06.940 --> 02:32:07.940]   Boostness.
[02:32:07.940 --> 02:32:08.940]   Yeah.
[02:32:08.940 --> 02:32:09.940]   Yeah.
[02:32:09.940 --> 02:32:10.940]   Be a good booster.
[02:32:10.940 --> 02:32:11.940]   Get somebody good for your college team.
[02:32:11.940 --> 02:32:12.940]   I love that.
[02:32:12.940 --> 02:32:13.940]   Thank you.
[02:32:13.940 --> 02:32:14.940]   Thank you.
[02:32:14.940 --> 02:32:15.940]   Thank you.
[02:32:15.940 --> 02:32:16.940]   Thank you.
[02:32:16.940 --> 02:32:17.940]   Thank you.
[02:32:17.940 --> 02:32:18.940]   Thank you.
[02:32:18.940 --> 02:32:19.940]   We'll try to get Glenn Flyschman on.
[02:32:19.940 --> 02:32:20.940]   Glenn has talked about this before, but he's working with a guy named Martian Wishari.
[02:32:20.940 --> 02:32:23.140]   Who is doing a book.
[02:32:23.140 --> 02:32:30.380]   He'll be a Kickstarter in a couple of weeks about keyboards called shift happens.
[02:32:30.380 --> 02:32:36.980]   And he just sent both Jeff and I, both Jeff and me a preliminary promo version.
[02:32:36.980 --> 02:32:39.180]   Isn't that great shift happens?
[02:32:39.180 --> 02:32:41.980]   This was the proof, the brain proof.
[02:32:41.980 --> 02:32:43.460]   It looks so beautiful.
[02:32:43.460 --> 02:32:44.900]   It's a beautiful page.
[02:32:44.900 --> 02:32:45.900]   Beautiful page.
[02:32:45.900 --> 02:32:46.900]   Thank you, Glenn.
[02:32:46.900 --> 02:32:47.900]   And we will get Glenn on.
[02:32:47.900 --> 02:32:52.300]   And we'll also, of course, when that Kickstarter starts, we'll, that'll be our pick of the week
[02:32:52.300 --> 02:32:56.460]   for that week for sure.
[02:32:56.460 --> 02:32:59.220]   We are at a time, but we're not out of waffles.
[02:32:59.220 --> 02:33:01.260]   And that's the good news.
[02:33:01.260 --> 02:33:04.500]   Stacy Higginbotham.
[02:33:04.500 --> 02:33:08.100]   Stacy on IOT.com is the website.
[02:33:08.100 --> 02:33:12.300]   The podcast she does with Kevin Tofel, the IOT podcast is available there.
[02:33:12.300 --> 02:33:13.300]   There's also events there.
[02:33:13.300 --> 02:33:14.300]   Check them out.
[02:33:14.300 --> 02:33:17.300]   There's lots of other stuff.
[02:33:17.300 --> 02:33:18.300]   She is amazing.
[02:33:18.300 --> 02:33:19.300]   Thank you.
[02:33:19.300 --> 02:33:20.300]   Thank you, Stacy, for being here.
[02:33:20.300 --> 02:33:21.780]   We really appreciate it.
[02:33:21.780 --> 02:33:22.780]   And grumpy.
[02:33:22.780 --> 02:33:23.780]   Amazing and grumpy.
[02:33:23.780 --> 02:33:24.780]   You weren't grumpy today.
[02:33:24.780 --> 02:33:25.780]   You know, I don't know.
[02:33:25.780 --> 02:33:26.780]   You've been far grumpier.
[02:33:26.780 --> 02:33:27.780]   You were not grumpy.
[02:33:27.780 --> 02:33:28.780]   You were so much worse.
[02:33:28.780 --> 02:33:32.260]   I don't know why you think you're grumpy.
[02:33:32.260 --> 02:33:35.620]   Jeff Jarvis, who is eternally grumpy, is the director.
[02:33:35.620 --> 02:33:36.620]   Yeah.
[02:33:36.620 --> 02:33:37.620]   That's my department.
[02:33:37.620 --> 02:33:41.260]   He's the grumpy himself, director of the Townite Center for Entrepreneal Journalism at the
[02:33:41.260 --> 02:33:50.020]   Great, Graduate School of Journalism at the City University of New York.
[02:33:50.020 --> 02:33:53.460]   If you had a football team, what would their mascot?
[02:33:53.460 --> 02:33:55.860]   What would they be called?
[02:33:55.860 --> 02:33:57.260]   The ink stained wretches?
[02:33:57.260 --> 02:33:59.020]   Yes, I'd be good.
[02:33:59.020 --> 02:34:00.020]   The Gutenbergs.
[02:34:00.020 --> 02:34:01.020]   The Gutenbergs.
[02:34:01.020 --> 02:34:02.020]   The Gutenbergs.
[02:34:02.020 --> 02:34:08.860]   Mr. Aunt Pruitt, he is on Twitter and underscore Pruitt, but you should also go to his website
[02:34:08.860 --> 02:34:10.980]   auntprout.com and take a look at his beautiful prints.
[02:34:10.980 --> 02:34:12.220]   He's an amazing photographer.
[02:34:12.220 --> 02:34:21.500]   And that's why he's the host of Hands On Photography, twit.tv/hop.
[02:34:21.500 --> 02:34:26.220]   Have a little Abolura Budna for me tonight.
[02:34:26.220 --> 02:34:27.220]   Abolour.
[02:34:27.220 --> 02:34:28.220]   Abolour.
[02:34:28.220 --> 02:34:30.780]   Abolour Abolura for me tonight.
[02:34:30.780 --> 02:34:32.260]   Good old single malt scotch.
[02:34:32.260 --> 02:34:33.860]   Yeah, baby.
[02:34:33.860 --> 02:34:36.100]   Thank you, everybody, for joining us.
[02:34:36.100 --> 02:34:44.460]   We do twig this week in Google every Wednesday afternoon, 2pm Pacific, 5pm Eastern, 2200 UTC.
[02:34:44.460 --> 02:34:47.100]   You can watch us do it live live.twit.tv.
[02:34:47.100 --> 02:34:54.940]   Chat with us while you're watching at IRC.twit.tv or in our club twit discord after the fact
[02:34:54.940 --> 02:35:00.340]   on demand versions of the show are available at the website, twit.tv/twig.
[02:35:00.340 --> 02:35:03.780]   Or you can get it on YouTube.
[02:35:03.780 --> 02:35:06.980]   It's a dedicated YouTube channel for all of our shows.
[02:35:06.980 --> 02:35:11.860]   Actually best place to start is at youtube.com/twit and you can follow the breadcrumbs from
[02:35:11.860 --> 02:35:13.020]   there.
[02:35:13.020 --> 02:35:17.140]   You can also subscribe in your favorite podcast player probably the best way to get this
[02:35:17.140 --> 02:35:18.140]   week in Google.
[02:35:18.140 --> 02:35:21.540]   That way you'll get it every Wednesday evening.
[02:35:21.540 --> 02:35:23.660]   The minute it's available.
[02:35:23.660 --> 02:35:25.980]   Thanks all for joining us.
[02:35:25.980 --> 02:35:30.620]   We will see you next time on This Week in Google.
[02:35:30.620 --> 02:35:32.020]   Bye bye.
[02:35:32.020 --> 02:35:36.540]   You want to hear about the latest news happening in the tech world from the people who write
[02:35:36.540 --> 02:35:39.820]   the article sometimes from the people who are actually making the news?
[02:35:39.820 --> 02:35:42.580]   Well, we got a show for you here at twit.tv.
[02:35:42.580 --> 02:35:44.060]   It's called Tech News Weekly.
[02:35:44.060 --> 02:35:49.180]   Me, Jason Howell and my co-host Micah Sargent, we talk with some amazing people each and
[02:35:49.180 --> 02:35:54.940]   every Thursday on Tech News Weekly and we share a little bit of our own insights in each of
[02:35:54.940 --> 02:35:56.900]   us bringing a story of the week.
[02:35:56.900 --> 02:35:59.460]   That's at twit.tv/tnw.
[02:35:59.460 --> 02:36:00.780]   Subscribe right now.
[02:36:00.780 --> 02:36:10.780]   [MUSIC]
[02:36:10.780 --> 02:36:12.860]   you

