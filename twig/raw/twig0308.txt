;FFMETADATA1
title=Dogheads in the Spaghetti
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=308
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons Attribution Non-Commercial Share-Alike license. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2015
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.000]   We're going to talk about the right to be forgotten, is it?
[00:00:03.000 --> 00:00:04.700]   Coming to America.
[00:00:04.700 --> 00:00:06.960]   Also, should Google Glass live or die?
[00:00:06.960 --> 00:00:10.260]   And are Google's algorithms sexist?
[00:00:10.260 --> 00:00:14.000]   We'll answer all those questions and more coming up next on Twig.
[00:00:14.000 --> 00:00:19.120]   Netcast, you love.
[00:00:19.120 --> 00:00:20.520]   From people you trust.
[00:00:20.520 --> 00:00:26.760]   This is Twig.
[00:00:26.760 --> 00:00:30.560]   Bandwidth for this week in Google is provided by CashFly,
[00:00:30.560 --> 00:00:33.960]   C-A-C-H-E-F-L-Y.com.
[00:00:33.960 --> 00:00:38.080]   [MUSIC PLAYING]
[00:00:38.080 --> 00:00:39.240]   This is Twig.
[00:00:39.240 --> 00:00:44.480]   This week in Google, episode 308, recorded July 8, 2015.
[00:00:44.480 --> 00:00:47.760]   Dog heads in the spaghetti.
[00:00:47.760 --> 00:00:49.360]   This episode of This Week in Google
[00:00:49.360 --> 00:00:52.520]   is brought to you by the Ring Video Doorbell.
[00:00:52.520 --> 00:00:55.040]   With Ring, you can see and talk to anyone at your door
[00:00:55.040 --> 00:00:57.720]   from anywhere in the world using your smartphone.
[00:00:57.720 --> 00:00:59.880]   It's like, call or ID for your home.
[00:00:59.880 --> 00:01:02.320]   Get $25 off the Ring Video Doorbell
[00:01:02.320 --> 00:01:05.720]   when you go to Ring.com/Twig.
[00:01:05.720 --> 00:01:08.240]   And by ZipperKooter, are you hiring?
[00:01:08.240 --> 00:01:11.160]   With ZipperKooter.com, you can post to 100 plus job sites,
[00:01:11.160 --> 00:01:14.000]   including social networks, all with a single click.
[00:01:14.000 --> 00:01:16.600]   Screen, rate, and hire the right candidates fast.
[00:01:16.600 --> 00:01:19.080]   Try ZipperKooter with a free four day trial now
[00:01:19.080 --> 00:01:22.040]   at zipperkooter.com/Twig.
[00:01:22.040 --> 00:01:25.080]   And by Lynda.com, the online learning platform
[00:01:25.080 --> 00:01:27.440]   with over 3,000 on-demand video courses
[00:01:27.440 --> 00:01:30.000]   to help you strengthen your business, technology,
[00:01:30.000 --> 00:01:31.480]   and creative skills.
[00:01:31.480 --> 00:01:35.040]   For a free 10 day trial, visit Lynda.com/Twig
[00:01:35.040 --> 00:01:40.960]   at lynda.com/twig.
[00:01:40.960 --> 00:01:43.360]   It's time for Twig this week in Google.
[00:01:43.360 --> 00:01:44.880]   The show where we talk about Google,
[00:01:44.880 --> 00:01:49.000]   the cloud, social media, journalism, privacy, war, peace,
[00:01:49.000 --> 00:01:51.360]   humanity, the coming AI singularity,
[00:01:51.360 --> 00:01:54.560]   the inevitable robot uprising, and Chipotle.
[00:01:54.560 --> 00:01:56.320]   You never know what we're going to talk about.
[00:01:56.320 --> 00:01:58.160]   Leo is still somewhere in Europe.
[00:01:58.160 --> 00:01:59.720]   He may have gone all Colonel Kurtz on us.
[00:01:59.720 --> 00:02:00.400]   We don't know.
[00:02:00.400 --> 00:02:02.000]   But with us he has.
[00:02:02.000 --> 00:02:02.360]   He has.
[00:02:02.360 --> 00:02:03.200]   He has to show the picture of him.
[00:02:03.200 --> 00:02:04.080]   You have to show the picture.
[00:02:04.080 --> 00:02:04.800]   OK.
[00:02:04.800 --> 00:02:06.000]   While Jason's--
[00:02:06.000 --> 00:02:09.720]   OK, while Jason's going to have to send somebody out there
[00:02:09.720 --> 00:02:10.600]   up the river--
[00:02:10.600 --> 00:02:12.280]   It is later, Jose.
[00:02:12.280 --> 00:02:13.600]   Oh, later, Jose.
[00:02:13.600 --> 00:02:16.040]   Do I just do a search for hashtag later, Jose and Leo
[00:02:16.040 --> 00:02:16.880]   look forward to--
[00:02:16.880 --> 00:02:17.960]   Look up his Facebook.
[00:02:17.960 --> 00:02:18.480]   All right.
[00:02:18.480 --> 00:02:18.960]   All right.
[00:02:18.960 --> 00:02:19.560]   I'll get back to you.
[00:02:19.560 --> 00:02:21.000]   Look at that picture later, Jose.
[00:02:21.000 --> 00:02:23.200]   But that is Jeff Jarvis.
[00:02:23.200 --> 00:02:25.440]   That is Jeff Jarvis, professor of journalism
[00:02:25.440 --> 00:02:27.040]   at the City University of New York,
[00:02:27.040 --> 00:02:29.080]   author of Public Parts, while we do the do in Gutenberg,
[00:02:29.080 --> 00:02:32.080]   and Buzz Machine, so on.
[00:02:32.080 --> 00:02:33.440]   I'm just inspired by Leo.
[00:02:33.440 --> 00:02:34.600]   It is inspiring, isn't it?
[00:02:34.600 --> 00:02:38.240]   I want one of those giant horns, or is that Swiss?
[00:02:38.240 --> 00:02:39.240]   I don't know.
[00:02:39.240 --> 00:02:40.440]   It's always able to buy it.
[00:02:40.440 --> 00:02:41.640]   I'm messing it up here.
[00:02:41.640 --> 00:02:44.800]   But anyway, great to see you again, Jeff.
[00:02:44.800 --> 00:02:47.080]   And we also have as our guests,
[00:02:47.080 --> 00:02:51.560]   two very distinguished gentlemen who I love being on shows with,
[00:02:51.560 --> 00:02:54.200]   starting with Matthew Ingram, who's senior writer at Fortune
[00:02:54.200 --> 00:02:56.200]   Magazine, formerly of Gigahome, who
[00:02:56.200 --> 00:02:58.680]   writes about the evolution of media and the social web.
[00:02:58.680 --> 00:03:01.560]   Welcome to you, Matthew Ingram.
[00:03:01.560 --> 00:03:02.560]   Thank you very much.
[00:03:02.560 --> 00:03:05.560]   It is really cool to have both you and Jeff on the same show,
[00:03:05.560 --> 00:03:09.800]   because the two of you on Twitter and also Jeff on Google
[00:03:09.800 --> 00:03:13.200]   Plus are just like my journalism firehose.
[00:03:13.200 --> 00:03:15.800]   Always talking about all the big issues on journalism.
[00:03:15.800 --> 00:03:18.200]   So we may talk about journalism today, folks.
[00:03:18.200 --> 00:03:19.120]   It's possible.
[00:03:19.120 --> 00:03:21.840]   I'm not-- I'm not-- I don't mean to threaten you,
[00:03:21.840 --> 00:03:23.120]   but this could happen.
[00:03:23.120 --> 00:03:25.840]   We also have Kurt Wagner, associate editor for social media
[00:03:25.840 --> 00:03:28.840]   at Recode, formerly of Mashable and Fortune,
[00:03:28.840 --> 00:03:32.400]   and a frequent guest on America's favorite technology news
[00:03:32.400 --> 00:03:33.440]   show, Tech News Today.
[00:03:33.440 --> 00:03:34.840]   Welcome to you, Kurt.
[00:03:34.840 --> 00:03:36.720]   Thank you so much for having me, Mike.
[00:03:36.720 --> 00:03:40.200]   It's weird to see you in a different show.
[00:03:40.200 --> 00:03:41.280]   It is weird.
[00:03:41.280 --> 00:03:42.560]   It's weird for me, too.
[00:03:42.560 --> 00:03:43.240]   Yes.
[00:03:43.240 --> 00:03:43.840]   But I like it.
[00:03:43.840 --> 00:03:44.240]   I like it.
[00:03:44.240 --> 00:03:45.080]   It's good, weird.
[00:03:45.080 --> 00:03:45.800]   All right.
[00:03:45.800 --> 00:03:48.720]   Well, do we have native Leo yet?
[00:03:48.720 --> 00:03:52.280]   No, see, the problem is I'm too good with my passwords.
[00:03:52.280 --> 00:03:52.480]   Yeah.
[00:03:52.480 --> 00:03:53.960]   So I don't know my Facebook password.
[00:03:53.960 --> 00:03:55.000]   So I'm getting it right now.
[00:03:55.000 --> 00:03:56.080]   So I'll get back to you on this.
[00:03:56.080 --> 00:03:57.960]   So Jason hacks his own computer.
[00:03:57.960 --> 00:03:59.600]   That's pretty much what I'm doing right now.
[00:03:59.600 --> 00:04:00.440]   We'll start with--
[00:04:00.440 --> 00:04:01.760]   I'll start with one, two, three, I think.
[00:04:01.760 --> 00:04:03.200]   That's right.
[00:04:03.200 --> 00:04:05.520]   It's on the sticky note right there.
[00:04:05.520 --> 00:04:06.000]   Yeah.
[00:04:06.000 --> 00:04:10.680]   OK, so Jeff Jarvis, the right to be forgotten
[00:04:10.680 --> 00:04:13.160]   may be coming to our own shores.
[00:04:13.160 --> 00:04:15.280]   If consumer watchdog has its way,
[00:04:15.280 --> 00:04:19.040]   this is a complaint that's been filed with the FTC saying
[00:04:19.040 --> 00:04:21.040]   that, hey, darn it, we want to be like Europe.
[00:04:21.040 --> 00:04:23.320]   We want the right to be forgotten right here in the US
[00:04:23.320 --> 00:04:26.200]   survey, does the Constitution prohibit this?
[00:04:26.200 --> 00:04:28.560]   Yeah, the difference is we have a First Amendment.
[00:04:28.560 --> 00:04:29.240]   Yeah.
[00:04:29.240 --> 00:04:30.520]   And it ain't going to happen here.
[00:04:30.520 --> 00:04:32.320]   And if it is going to happen here, by God,
[00:04:32.320 --> 00:04:34.600]   there's going to be a huge fight, because it really does have
[00:04:34.600 --> 00:04:39.280]   an impact on the other's right to know and to continue to know
[00:04:39.280 --> 00:04:41.840]   and to remember and to speak.
[00:04:41.840 --> 00:04:45.960]   And it's just an absurd notion that has spread dangerously.
[00:04:45.960 --> 00:04:50.120]   I've complained about it since the first mention in the EU.
[00:04:50.120 --> 00:04:51.820]   And it's getting more and more dangerous.
[00:04:51.820 --> 00:04:54.640]   And it's spreading to Matthew's land to an extent.
[00:04:54.640 --> 00:04:57.880]   They're in the cold, nice lands.
[00:04:57.880 --> 00:05:02.920]   This idea that you have a right to anything about you.
[00:05:02.920 --> 00:05:04.720]   That's what's absurd about this, right?
[00:05:04.720 --> 00:05:06.920]   I know something about you, Matthew.
[00:05:06.920 --> 00:05:09.680]   You can't erase that from me unless you're a men in black.
[00:05:09.680 --> 00:05:10.320]   It's in here.
[00:05:10.320 --> 00:05:11.280]   That is my knowledge.
[00:05:11.280 --> 00:05:12.560]   It is my right to hold on to it.
[00:05:12.560 --> 00:05:13.320]   My right to remember it.
[00:05:13.320 --> 00:05:16.040]   My right to repeat it as much as I want.
[00:05:16.040 --> 00:05:21.080]   And that's just the nature of what we call being public.
[00:05:21.080 --> 00:05:24.660]   And to try to restrict that, govern mentally.
[00:05:24.660 --> 00:05:26.580]   Now somebody said on Twitter, well,
[00:05:26.580 --> 00:05:31.740]   drivers, what do you think Google's getting rid of?
[00:05:31.740 --> 00:05:33.180]   Revenge porn, likes.
[00:05:33.180 --> 00:05:34.980]   Well, that's Google's right to do.
[00:05:34.980 --> 00:05:38.140]   But Google, under government fiat to kill things,
[00:05:38.140 --> 00:05:40.580]   that is the definition of censorship.
[00:05:40.580 --> 00:05:43.460]   One of the things, Jeff, that I have a slightly different
[00:05:43.460 --> 00:05:45.500]   obsession about this whole issue than you do
[00:05:45.500 --> 00:05:46.940]   if I can just slip this in there.
[00:05:46.940 --> 00:05:49.100]   Because I think to a certain extent,
[00:05:49.100 --> 00:05:52.880]   people do have the right to privacy about their own
[00:05:52.880 --> 00:05:53.520]   information.
[00:05:53.520 --> 00:05:57.840]   And they can, to some extent, determine that by choosing
[00:05:57.840 --> 00:06:00.800]   social sites or whatever that may guarantee that.
[00:06:00.800 --> 00:06:03.400]   In other words, that could be a service that could be offered
[00:06:03.400 --> 00:06:05.600]   by social sites, by any site.
[00:06:05.600 --> 00:06:06.920]   And where-- so for example, people
[00:06:06.920 --> 00:06:09.400]   are concerned about the drunken pictures when they're
[00:06:09.400 --> 00:06:12.520]   in high school, getting out there, and being seen by the
[00:06:12.520 --> 00:06:13.520]   coworkers, whatever.
[00:06:13.520 --> 00:06:14.520]   Whatever it is.
[00:06:14.520 --> 00:06:17.960]   And so to a certain extent, this kind of privacy could be
[00:06:17.960 --> 00:06:18.960]   offered by services.
[00:06:18.960 --> 00:06:20.120]   People can choose those things.
[00:06:20.120 --> 00:06:23.960]   But the problem with this is it's about search engines.
[00:06:23.960 --> 00:06:25.960]   Search engines are supposed-- and here, this is the
[00:06:25.960 --> 00:06:28.720]   obsession part-- search engines are supposed to reflect
[00:06:28.720 --> 00:06:31.520]   what's actually on the internet.
[00:06:31.520 --> 00:06:34.520]   And if you can prohibit a search engine from doing it,
[00:06:34.520 --> 00:06:36.720]   then what comes from the newspaper more, too?
[00:06:36.720 --> 00:06:37.600]   Well, this is the thing.
[00:06:37.600 --> 00:06:40.400]   They would never, in Europe, for example, they would never go
[00:06:40.400 --> 00:06:42.680]   to the Guardian and say, OK, people have the right to
[00:06:42.680 --> 00:06:43.480]   privacy.
[00:06:43.480 --> 00:06:45.480]   We're going to take those articles.
[00:06:45.480 --> 00:06:47.720]   You're banned from showing--
[00:06:47.720 --> 00:06:49.240]   I mean, all this is--
[00:06:49.240 --> 00:06:51.800]   That's the country that made the Guardian destroy the NSA
[00:06:51.800 --> 00:06:57.000]   computers, which is what forced the Guardian to send the
[00:06:57.000 --> 00:07:02.240]   data to the New York Times so that it was safe from the UK
[00:07:02.240 --> 00:07:04.400]   government.
[00:07:04.400 --> 00:07:07.480]   And Michael, I take your point about the privacy of the
[00:07:07.480 --> 00:07:08.240]   service.
[00:07:08.240 --> 00:07:11.400]   But again, once something is known as known, the danger of
[00:07:11.400 --> 00:07:13.360]   this is the idea of regulating knowledge.
[00:07:13.360 --> 00:07:20.240]   If I know that you noted Gourmet, actually ate a Pizza Hut
[00:07:20.240 --> 00:07:21.480]   hot dog crust pizza--
[00:07:21.480 --> 00:07:22.360]   Now you just mean ridiculous.
[00:07:22.360 --> 00:07:27.000]   If I caught you and I reveal that to the world, you may try
[00:07:27.000 --> 00:07:30.560]   your best to get it erased, but I'm sorry it's out there in
[00:07:30.560 --> 00:07:31.080]   the stream.
[00:07:31.080 --> 00:07:33.400]   In fact, Jason, I think you should order one for
[00:07:33.400 --> 00:07:34.800]   Mike so we can see him eat it.
[00:07:34.800 --> 00:07:36.200]   The bloodstream, you mean?
[00:07:36.200 --> 00:07:37.920]   We should force him to eat it on the air.
[00:07:37.920 --> 00:07:40.320]   There's a dare.
[00:07:40.320 --> 00:07:43.080]   This is really taking a turn for the worse here.
[00:07:43.080 --> 00:07:44.880]   Oh, I'm calling right now.
[00:07:44.880 --> 00:07:46.320]   There's no getting out of this.
[00:07:46.320 --> 00:07:47.440]   I'm not even going to--
[00:07:47.440 --> 00:07:49.080]   Matthew Mingam, what's going on in Canada?
[00:07:49.080 --> 00:07:53.000]   What's the conversation like in Canada around the right to be
[00:07:53.000 --> 00:07:56.200]   forgotten here, there, or anywhere?
[00:07:56.200 --> 00:07:58.640]   I guess we prefer the term "Kanakistan."
[00:07:58.640 --> 00:08:00.200]   OK.
[00:08:00.200 --> 00:08:03.240]   I guess, I mean, I think what Jeff is talking about is
[00:08:03.240 --> 00:08:06.960]   there we have laws that are trying to keep people from
[00:08:06.960 --> 00:08:08.440]   saying bad things.
[00:08:08.440 --> 00:08:11.960]   And the risk is that those laws will be used against anyone
[00:08:11.960 --> 00:08:16.920]   who says anything that someone feels is either bad or could
[00:08:16.920 --> 00:08:18.440]   be interpreted as being bad.
[00:08:18.440 --> 00:08:24.720]   So the impulse is we need a way to go after people who are
[00:08:24.720 --> 00:08:30.240]   posting hate speech, people who are posting terrorist
[00:08:30.240 --> 00:08:33.160]   sympathetic blog posts or whatever.
[00:08:33.160 --> 00:08:36.800]   And so I think the risk is-- and this happens in Britain
[00:08:36.800 --> 00:08:37.800]   all the time--
[00:08:37.800 --> 00:08:41.400]   risk is everyone can think of an example or knows an
[00:08:41.400 --> 00:08:45.320]   example where they would actually support someone's right
[00:08:45.320 --> 00:08:46.400]   to remove information.
[00:08:46.400 --> 00:08:49.560]   So let's say there was a charge and it was reported, you
[00:08:49.560 --> 00:08:51.040]   know, child porn or something.
[00:08:51.040 --> 00:08:53.520]   And it was reported in all the newspapers and all those
[00:08:53.520 --> 00:08:54.760]   articles are everywhere.
[00:08:54.760 --> 00:08:56.840]   And they come up whenever you search someone's name.
[00:08:56.840 --> 00:08:58.840]   But they were actually acquitted.
[00:08:58.840 --> 00:09:00.280]   But no one wrote about the acquittal.
[00:09:00.280 --> 00:09:02.440]   And so none of that shows up, just the ones where they
[00:09:02.440 --> 00:09:03.400]   were charged.
[00:09:03.400 --> 00:09:07.240]   The problem is you can think of an example where having those
[00:09:07.240 --> 00:09:08.840]   laws would be a good thing.
[00:09:08.840 --> 00:09:11.240]   But there are an infinite number of more examples where
[00:09:11.240 --> 00:09:12.840]   it would be a bad thing.
[00:09:12.840 --> 00:09:17.720]   And so the problem is who decides which information
[00:09:17.720 --> 00:09:19.400]   should be forgotten?
[00:09:19.400 --> 00:09:20.360]   That's my problem.
[00:09:20.360 --> 00:09:21.880]   Is it a government panel?
[00:09:21.880 --> 00:09:24.360]   Is it some kind of a shadowy court somewhere?
[00:09:24.360 --> 00:09:25.520]   I mean, that's the risk.
[00:09:25.520 --> 00:09:28.840]   Is it you don't know who's going to decide?
[00:09:28.840 --> 00:09:29.840]   Yeah.
[00:09:29.840 --> 00:09:32.720]   Yeah, it's-- and the other part of that--
[00:09:32.720 --> 00:09:35.760]   and I don't think it matches perfectly with your example.
[00:09:35.760 --> 00:09:39.040]   But there are many examples of a censorship, for example.
[00:09:39.040 --> 00:09:42.720]   There moves in Turkey and elsewhere to remove content
[00:09:42.720 --> 00:09:51.440]   around sort of insurgencies or sort of Islamic
[00:09:51.440 --> 00:09:53.720]   fundamentalist movements and so on.
[00:09:53.720 --> 00:09:57.080]   And the idea is that by removing this information,
[00:09:57.080 --> 00:10:00.240]   you're going to prevent people from being exposed to it.
[00:10:00.240 --> 00:10:03.480]   And therefore, you're sort of suppressed.
[00:10:03.480 --> 00:10:05.400]   That set of ideas.
[00:10:05.400 --> 00:10:07.200]   The problem is that you don't know what
[00:10:07.200 --> 00:10:11.000]   the unintended consequences of any sort of censorship is.
[00:10:11.000 --> 00:10:14.960]   For example, just I guess to try to make your example work
[00:10:14.960 --> 00:10:19.040]   of somebody being accused of child porn or something
[00:10:19.040 --> 00:10:21.400]   like that, then they're acquitted.
[00:10:21.400 --> 00:10:22.840]   And then I think to a certain extent,
[00:10:22.840 --> 00:10:26.240]   it's the media's job to talk about that.
[00:10:26.240 --> 00:10:32.720]   And when people get outraged as they should,
[00:10:32.720 --> 00:10:36.240]   that a person's being talked about in a way
[00:10:36.240 --> 00:10:41.040]   that's mistaken, it's a way to sort of put pressure
[00:10:41.040 --> 00:10:42.720]   on the media to do follows.
[00:10:42.720 --> 00:10:46.360]   I mean, this is, in fact, an ongoing, longstanding problem
[00:10:46.360 --> 00:10:47.080]   with journalism.
[00:10:47.080 --> 00:10:49.360]   They make a big front page error.
[00:10:49.360 --> 00:10:51.280]   And then they do a little tiny correction.
[00:10:51.280 --> 00:10:53.320]   And it doesn't have the same impact.
[00:10:53.320 --> 00:10:54.520]   And I think to a certain extent, it's
[00:10:54.520 --> 00:10:57.560]   legitimate for publications to be held to account
[00:10:57.560 --> 00:11:00.440]   and to go back and make sure that the people who got
[00:11:00.440 --> 00:11:03.440]   the wrong idea get the right idea and so on.
[00:11:03.440 --> 00:11:03.920]   I don't know.
[00:11:03.920 --> 00:11:04.840]   That's a bad example.
[00:11:04.840 --> 00:11:05.520]   I admit it.
[00:11:05.520 --> 00:11:07.800]   The fact is, when you start removing information,
[00:11:07.800 --> 00:11:09.760]   you don't know how you're affecting
[00:11:09.760 --> 00:11:12.120]   what the public thinks about, what needs to change,
[00:11:12.120 --> 00:11:14.040]   what needs to be fixed, and so on.
[00:11:14.040 --> 00:11:14.320]   Right.
[00:11:14.320 --> 00:11:16.000]   And in fact, one of my favorite examples
[00:11:16.000 --> 00:11:19.080]   of censorship that was done for presumably,
[00:11:19.080 --> 00:11:21.400]   I good reason, but has had bad effects,
[00:11:21.400 --> 00:11:24.760]   is Facebook took down a huge number of pages
[00:11:24.760 --> 00:11:28.080]   related to Syrian terrorism.
[00:11:28.080 --> 00:11:30.600]   Eliot Hagan's Brown Moses has written about this before.
[00:11:30.600 --> 00:11:32.080]   I've written about it before.
[00:11:32.080 --> 00:11:34.560]   They took them down, presumably, because they didn't want
[00:11:34.560 --> 00:11:38.840]   quasi-terrorist groups posting videos of people's homes being
[00:11:38.840 --> 00:11:43.280]   blown up, because they thought that's-- no one wants to see that.
[00:11:43.280 --> 00:11:46.200]   But Eliot's point is that removing that information
[00:11:46.200 --> 00:11:49.440]   actually removes a historical record of those events that
[00:11:49.440 --> 00:11:52.480]   can be used by people like him, investigative journalists,
[00:11:52.480 --> 00:11:55.200]   to piece together what's going on in that country.
[00:11:55.200 --> 00:11:59.280]   So that's a significant negative impact.
[00:11:59.280 --> 00:11:59.520]   All right.
[00:11:59.520 --> 00:12:02.320]   Well, I think we've beaten this one to death.
[00:12:02.320 --> 00:12:05.200]   I think that it's more or less unanimous
[00:12:05.200 --> 00:12:07.720]   that the right to be forgotten is a bad idea.
[00:12:07.720 --> 00:12:09.680]   And the US is a bad idea in Europe.
[00:12:09.680 --> 00:12:12.640]   And so we'll keep an eye on this and make sure
[00:12:12.640 --> 00:12:13.640]   that it doesn't happen.
[00:12:13.640 --> 00:12:15.200]   But again, as Jeff pointed out, we
[00:12:15.200 --> 00:12:16.200]   do have the First Amendment.
[00:12:16.200 --> 00:12:17.200]   Thank goodness.
[00:12:17.200 --> 00:12:20.560]   If that hadn't been enacted when it was,
[00:12:20.560 --> 00:12:22.680]   give it another 100 years, we would never
[00:12:22.680 --> 00:12:25.120]   have had the First Amendment.
[00:12:25.120 --> 00:12:27.440]   We would never have had any of this stuff.
[00:12:27.440 --> 00:12:30.040]   And it's a great thing that the United States was founded
[00:12:30.040 --> 00:12:30.600]   when it was.
[00:12:30.600 --> 00:12:32.400]   So we got that set in stone.
[00:12:32.400 --> 00:12:35.440]   And it's very difficult or impossible to change.
[00:12:35.440 --> 00:12:37.560]   And that's why we have that protection.
[00:12:37.560 --> 00:12:39.960]   Well, we're going to check in on Leo here, who
[00:12:39.960 --> 00:12:44.080]   has reportedly gone completely native on us.
[00:12:44.080 --> 00:12:45.240]   If you're their children in the room,
[00:12:45.240 --> 00:12:47.440]   please have them leave the room.
[00:12:47.440 --> 00:12:49.760]   There we go.
[00:12:49.760 --> 00:12:50.960]   Oh boy.
[00:12:50.960 --> 00:12:51.880]   Are we worried yet?
[00:12:51.880 --> 00:12:52.880]   Yeah, we're worried.
[00:12:52.880 --> 00:12:53.880]   Yeah, we're worried.
[00:12:53.880 --> 00:12:54.520]   Yeah.
[00:12:54.520 --> 00:12:55.960]   See, the shirt is all wrong.
[00:12:55.960 --> 00:12:57.800]   Like the later housing are fine.
[00:12:57.800 --> 00:12:58.880]   The shirt's all wrong.
[00:12:58.880 --> 00:13:00.840]   That's what you want to talk to Leo about that.
[00:13:00.840 --> 00:13:03.560]   To me, it's all about the socks.
[00:13:03.560 --> 00:13:04.600]   Yeah, they're OK.
[00:13:04.600 --> 00:13:05.840]   Yeah.
[00:13:05.840 --> 00:13:11.480]   And he can also carry six gigantic mugs full of beer, too.
[00:13:11.480 --> 00:13:17.360]   So what I'd like to do is run this through Google's deep dream,
[00:13:17.360 --> 00:13:20.960]   deep learning image processing and see what happens.
[00:13:20.960 --> 00:13:21.880]   Please don't do that.
[00:13:21.880 --> 00:13:23.800]   Beautiful segue.
[00:13:23.800 --> 00:13:25.760]   Jason, that was beautiful.
[00:13:25.760 --> 00:13:30.560]   So Google apparently has this deep learning thing that will--
[00:13:30.560 --> 00:13:31.720]   can anybody explain this?
[00:13:31.720 --> 00:13:32.360]   I don't know.
[00:13:32.360 --> 00:13:34.600]   Kurt, have you checked on this story?
[00:13:34.600 --> 00:13:39.080]   Essentially, Google has this software that will make images
[00:13:39.080 --> 00:13:41.200]   based on its own--
[00:13:41.200 --> 00:13:44.720]   I guess it has biases toward human parts and animal parts
[00:13:44.720 --> 00:13:48.680]   and stuff, so it creates these images out of other images.
[00:13:48.680 --> 00:13:53.240]   And the end result is pure, unfiltered LSD.
[00:13:53.240 --> 00:13:55.000]   It's kind of crazy.
[00:13:55.000 --> 00:13:56.800]   I mean, this is--
[00:13:56.800 --> 00:13:59.280]   I'm glad you picked me to try and explain
[00:13:59.280 --> 00:14:03.120]   the deep learning of artificial intelligence technology
[00:14:03.120 --> 00:14:04.040]   that we're talking about.
[00:14:04.040 --> 00:14:05.000]   I have seen this.
[00:14:05.000 --> 00:14:06.680]   I can't explain it.
[00:14:06.680 --> 00:14:09.440]   I know that both Google and Facebook
[00:14:09.440 --> 00:14:11.880]   are obviously really interested in deep learning right now.
[00:14:11.880 --> 00:14:14.560]   I think more for practical matters
[00:14:14.560 --> 00:14:17.840]   than necessarily coming up with really trippy photos.
[00:14:17.840 --> 00:14:19.120]   I saw this cool--
[00:14:19.120 --> 00:14:21.560]   I wish I'm going to try to explain it to you guys.
[00:14:21.560 --> 00:14:24.880]   Saw this cool video yesterday, where essentially,
[00:14:24.880 --> 00:14:30.000]   if you have a string of photos, maybe 20 photos of you walking
[00:14:30.000 --> 00:14:33.240]   through some long hallway, you stitch them together.
[00:14:33.240 --> 00:14:35.680]   And what I believe it's Google and what they're able to do
[00:14:35.680 --> 00:14:38.920]   is kind of fill in the blanks of those photos
[00:14:38.920 --> 00:14:41.320]   in some kind of seamless video.
[00:14:41.320 --> 00:14:43.480]   So essentially, instead of a flip card
[00:14:43.480 --> 00:14:45.280]   where you're going from photo 1, 2, 3,
[00:14:45.280 --> 00:14:49.440]   and there's dramatic sharp pauses between each photo,
[00:14:49.440 --> 00:14:52.600]   it's able to fill in that blank space just based off
[00:14:52.600 --> 00:14:56.240]   of everything that it recognizes in the other images.
[00:14:56.240 --> 00:14:58.680]   And I think that stuff is just so fascinating, right?
[00:14:58.680 --> 00:15:01.520]   I mean, you can see it in their products like Google Photos,
[00:15:01.520 --> 00:15:02.480]   whatever were you.
[00:15:02.480 --> 00:15:05.280]   I don't know if any of you guys use Google Photos,
[00:15:05.280 --> 00:15:08.320]   but you upload images and it creates like a GIF for you
[00:15:08.320 --> 00:15:11.800]   or create a little slideshow based on images
[00:15:11.800 --> 00:15:13.760]   that seem to be in sequence.
[00:15:13.760 --> 00:15:16.520]   I think this stuff is all pretty cool right now.
[00:15:16.520 --> 00:15:19.760]   But there's obviously a long way to go besides just looking
[00:15:19.760 --> 00:15:22.600]   at these trippy images and zoning out at work
[00:15:22.600 --> 00:15:23.440]   for a little while.
[00:15:23.440 --> 00:15:24.920]   And it's called the Deep Dream Project.
[00:15:24.920 --> 00:15:29.320]   It's available on GitHub and it relies on the cafe
[00:15:29.320 --> 00:15:31.120]   deep learning framework.
[00:15:31.120 --> 00:15:31.720]   Isn't that--
[00:15:31.720 --> 00:15:34.480]   Those of you who want audio were now seeing dogs be--
[00:15:34.480 --> 00:15:35.760]   I mean, seriously.
[00:15:35.760 --> 00:15:38.400]   Spaghetti turned into dog faces.
[00:15:38.400 --> 00:15:40.000]   It's just disturbing.
[00:15:40.000 --> 00:15:41.400]   What's interesting about this--
[00:15:41.400 --> 00:15:41.720]   It's fascinating.
[00:15:41.720 --> 00:15:44.120]   Yeah, what is interesting about this and also
[00:15:44.120 --> 00:15:46.440]   the technology that Kurt was talking about,
[00:15:46.440 --> 00:15:48.520]   about filling in the blanks as you're walking down the hallway
[00:15:48.520 --> 00:15:52.240]   or wherever, is this is closer to how the human mind actually
[00:15:52.240 --> 00:15:52.600]   works.
[00:15:52.600 --> 00:15:54.920]   I mean, your mind does fill in blanks.
[00:15:54.920 --> 00:15:55.840]   Oh, Mike.
[00:15:55.840 --> 00:15:56.840]   Yes.
[00:15:56.840 --> 00:15:57.340]   Yeah.
[00:15:57.340 --> 00:15:59.880]   Jason, can you call the guys in the white codes from Mike
[00:15:59.880 --> 00:16:01.320]   if this is how his brain works?
[00:16:01.320 --> 00:16:03.840]   [LAUGHTER]
[00:16:03.840 --> 00:16:05.480]   Let him think he's going to get a pizza
[00:16:05.480 --> 00:16:07.480]   and then take him away and get a pic care of.
[00:16:07.480 --> 00:16:10.680]   People are more likely to see Jesus in their spaghetti
[00:16:10.680 --> 00:16:13.200]   than dogs or other pets.
[00:16:13.200 --> 00:16:15.560]   But people do see things that aren't there.
[00:16:15.560 --> 00:16:19.360]   And that's a good point, Mike.
[00:16:19.360 --> 00:16:20.880]   This is extremely disturbing.
[00:16:20.880 --> 00:16:21.760]   This is horrible.
[00:16:21.760 --> 00:16:23.280]   This is fear of loathing in Las Vegas.
[00:16:23.280 --> 00:16:24.600]   You ran through it, by the way.
[00:16:24.600 --> 00:16:26.200]   Those of you who are listening to this program
[00:16:26.200 --> 00:16:28.520]   are not watching it, good choice.
[00:16:28.520 --> 00:16:30.160]   Super disturbing.
[00:16:30.160 --> 00:16:31.280]   But you're right about it.
[00:16:31.280 --> 00:16:33.040]   A lot of the software is based on trying
[00:16:33.040 --> 00:16:35.120]   to simulate what the human brain does, which
[00:16:35.120 --> 00:16:36.760]   is pattern recognition.
[00:16:36.760 --> 00:16:38.520]   You look at a piece of toast.
[00:16:38.520 --> 00:16:40.320]   You look at grains of sand on the beach.
[00:16:40.320 --> 00:16:41.840]   You look at waves or whatever.
[00:16:41.840 --> 00:16:42.680]   And you see things.
[00:16:42.680 --> 00:16:43.440]   You look at kids.
[00:16:43.440 --> 00:16:44.440]   Look at clouds.
[00:16:44.440 --> 00:16:45.960]   And they see animals or birds or whatever.
[00:16:45.960 --> 00:16:48.200]   This is sort of trying to teach computers
[00:16:48.200 --> 00:16:49.160]   how to do the same thing.
[00:16:49.160 --> 00:16:53.320]   Can you take images and see other things in them?
[00:16:53.320 --> 00:16:55.640]   And obviously, if you take that to a sniff
[00:16:55.640 --> 00:16:59.600]   for conclusion, it's an LSD trip.
[00:16:59.600 --> 00:17:02.120]   And by the way, this is a lot cheaper than LSD.
[00:17:02.120 --> 00:17:03.120]   And a lot safer.
[00:17:03.120 --> 00:17:04.080]   That's way cheaper.
[00:17:04.080 --> 00:17:04.600]   Yeah.
[00:17:04.600 --> 00:17:07.400]   So anyway, just wait until they put this innocuous rift.
[00:17:07.400 --> 00:17:08.360]   Oh, boy.
[00:17:08.360 --> 00:17:09.200]   Oh, man.
[00:17:09.200 --> 00:17:10.880]   That is going to be absolutely horrible.
[00:17:10.880 --> 00:17:12.040]   Well, we're going to take a break.
[00:17:12.040 --> 00:17:14.400]   And when we come back, we're going to talk about less horrible,
[00:17:14.400 --> 00:17:16.360]   less trippy stuff.
[00:17:16.360 --> 00:17:19.960]   But first, I want to talk about this bad boy right here.
[00:17:19.960 --> 00:17:21.880]   This is the future of your doorbell.
[00:17:21.880 --> 00:17:24.480]   It's called the Ring Video Doorbell.
[00:17:24.480 --> 00:17:26.120]   And there are a lot of-- let's face it.
[00:17:26.120 --> 00:17:28.760]   There are a lot of home automation products
[00:17:28.760 --> 00:17:30.280]   that don't make a whole lot of sense.
[00:17:30.280 --> 00:17:32.520]   They add some minor convenience,
[00:17:32.520 --> 00:17:35.400]   enable you to turn on your lights with motion detection.
[00:17:35.400 --> 00:17:35.960]   It's nice.
[00:17:35.960 --> 00:17:37.840]   You don't have to flip the switch.
[00:17:37.840 --> 00:17:40.320]   But this, the Ring Video Doorbell
[00:17:40.320 --> 00:17:42.280]   is something that makes so much sense.
[00:17:42.280 --> 00:17:42.960]   It's not even funny.
[00:17:42.960 --> 00:17:44.380]   Basically, how this works is you install
[00:17:44.380 --> 00:17:45.080]   this.
[00:17:45.080 --> 00:17:47.080]   It has a battery, or you can connect it
[00:17:47.080 --> 00:17:48.960]   into the electricity into your house.
[00:17:48.960 --> 00:17:50.920]   And it functions like a regular doorbell.
[00:17:50.920 --> 00:17:54.440]   Somebody presses the button, the doorbell rings.
[00:17:54.440 --> 00:17:56.480]   But then something else happens.
[00:17:56.480 --> 00:17:58.960]   The virtual doorbell in your smartphone rings.
[00:17:58.960 --> 00:18:00.560]   And you could be in China.
[00:18:00.560 --> 00:18:02.600]   You could be anywhere with an internet connection.
[00:18:02.600 --> 00:18:06.280]   And you will hear your front door bell ringing.
[00:18:06.280 --> 00:18:07.640]   And then you pick up your phone.
[00:18:07.640 --> 00:18:08.280]   You look.
[00:18:08.280 --> 00:18:11.120]   And there it is, the video through the camera
[00:18:11.120 --> 00:18:13.520]   to see who's at your door.
[00:18:13.520 --> 00:18:17.100]   And of course, if they even if they don't ring the doorbell,
[00:18:17.100 --> 00:18:19.140]   it has a motion detector to show you
[00:18:19.140 --> 00:18:21.540]   who's on your front porch or standing in front of your door.
[00:18:21.540 --> 00:18:23.140]   And you get that information.
[00:18:23.140 --> 00:18:24.620]   It is so useful.
[00:18:24.620 --> 00:18:27.380]   You can imagine how somebody comes to your door.
[00:18:27.380 --> 00:18:28.820]   They ring the doorbell.
[00:18:28.820 --> 00:18:32.340]   And they don't know that you're not home.
[00:18:32.340 --> 00:18:34.420]   For all they know, you are there.
[00:18:34.420 --> 00:18:37.180]   And if it's somebody you know, you can call them or whatever.
[00:18:37.180 --> 00:18:40.100]   It's just such a great way to provide security.
[00:18:40.100 --> 00:18:41.860]   And here's something that's really
[00:18:41.860 --> 00:18:44.340]   an interesting fact that everybody should keep in mind,
[00:18:44.340 --> 00:18:48.320]   which is that burglars who usually rob--
[00:18:48.320 --> 00:18:51.600]   it's not the person with the black mask on that comes in
[00:18:51.600 --> 00:18:52.780]   with a flashlight at night.
[00:18:52.780 --> 00:18:54.560]   That's not how burglaries work.
[00:18:54.560 --> 00:18:56.460]   They usually happen during the day.
[00:18:56.460 --> 00:18:58.740]   And what burglars typically do is they come to the front door
[00:18:58.740 --> 00:19:02.860]   and act like they come up with some excuse in case you
[00:19:02.860 --> 00:19:04.020]   out into the door.
[00:19:04.020 --> 00:19:06.640]   And they ring the doorbell to see if you're home.
[00:19:06.640 --> 00:19:10.220]   And when nobody answers the door, they say, OK, let's rob
[00:19:10.220 --> 00:19:11.380]   this house.
[00:19:11.380 --> 00:19:13.580]   But when they ring the doorbell, you're being notified.
[00:19:13.580 --> 00:19:15.580]   You can call the police because you can see what's
[00:19:15.580 --> 00:19:17.500]   happening through the camera.
[00:19:17.500 --> 00:19:20.460]   It's just a great, great alternative
[00:19:20.460 --> 00:19:24.420]   to the old dumb doorbells that almost every house has.
[00:19:24.420 --> 00:19:28.660]   Right now, you can get the Ring Video Doorbell for only $174.
[00:19:28.660 --> 00:19:31.820]   That's $25 off the normal price.
[00:19:31.820 --> 00:19:32.940]   And you can protect your home.
[00:19:32.940 --> 00:19:34.860]   You can have peace of mind and know
[00:19:34.860 --> 00:19:37.100]   what's going on at your house even when you're not there.
[00:19:37.100 --> 00:19:39.940]   So just go to ring.com/twig.
[00:19:39.940 --> 00:19:44.700]   That's ring.com/twig to protect your home
[00:19:44.700 --> 00:19:50.980]   and to have total information awareness of your own front door.
[00:19:50.980 --> 00:19:54.420]   Well, Project Fi--
[00:19:54.420 --> 00:19:59.340]   this is, of course, Google's pretty interesting technology,
[00:19:59.340 --> 00:20:00.180]   I think.
[00:20:00.180 --> 00:20:01.980]   The most interesting thing about Project Fi
[00:20:01.980 --> 00:20:03.620]   is the way that it will enable you
[00:20:03.620 --> 00:20:08.740]   to use two carriers plus Wi-Fi seamlessly transfer calls
[00:20:08.740 --> 00:20:10.580]   and data between them so they always
[00:20:10.580 --> 00:20:15.780]   have a great connection and an efficient connection,
[00:20:15.780 --> 00:20:16.820]   no matter where.
[00:20:16.820 --> 00:20:19.420]   The problem is, according to this article
[00:20:19.420 --> 00:20:23.820]   in the Wall Street Journal by Nathan Oliveris-Giles,
[00:20:23.820 --> 00:20:25.860]   the problem is that it only works on the one phone
[00:20:25.860 --> 00:20:28.700]   and they need to roll this out to a lot more phones.
[00:20:28.700 --> 00:20:32.940]   Has anybody received an invitation to this Project Fi?
[00:20:32.940 --> 00:20:34.140]   It was an energy.
[00:20:34.140 --> 00:20:35.140]   On the panel, no.
[00:20:35.140 --> 00:20:37.340]   Because we all know it was on Nexus 6 only.
[00:20:37.340 --> 00:20:39.020]   We all know it's a pilot.
[00:20:39.020 --> 00:20:40.740]   And he's trying to act like, oh, gee,
[00:20:40.740 --> 00:20:42.140]   I know Nexus 6 is only one phone.
[00:20:42.140 --> 00:20:42.980]   Yeah, we know that.
[00:20:42.980 --> 00:20:44.420]   They're just trying something new here.
[00:20:44.420 --> 00:20:45.180]   And they're pushing.
[00:20:45.180 --> 00:20:45.740]   Come on.
[00:20:45.740 --> 00:20:46.540]   Yeah.
[00:20:46.540 --> 00:20:48.740]   Now, I got to understand that.
[00:20:48.740 --> 00:20:51.220]   Is that-- Matt cuts, we thought, I love you, Matt.
[00:20:51.220 --> 00:20:52.140]   I'm at a lot of clout.
[00:20:52.140 --> 00:20:54.300]   I'm still 12 weeks before they decide
[00:20:54.300 --> 00:20:56.300]   when I get Project Fi.
[00:20:56.300 --> 00:20:57.500]   Wow.
[00:20:57.500 --> 00:20:59.340]   Matt was offering to Leo and--
[00:20:59.340 --> 00:21:00.100]   gee, you too, Jason.
[00:21:00.100 --> 00:21:00.420]   Right?
[00:21:00.420 --> 00:21:01.020]   Plus, we can do that.
[00:21:01.020 --> 00:21:02.260]   He changed his status?
[00:21:02.260 --> 00:21:02.900]   No, I haven't.
[00:21:02.900 --> 00:21:04.220]   No.
[00:21:04.220 --> 00:21:06.020]   Well, it's the waiting.
[00:21:06.020 --> 00:21:07.940]   I'll get it three years from now, so.
[00:21:07.940 --> 00:21:10.980]   Well, the rates start at $30 a month.
[00:21:10.980 --> 00:21:14.300]   And it's just-- it sounds like really interesting.
[00:21:14.300 --> 00:21:16.060]   It's got some good technology.
[00:21:16.060 --> 00:21:17.660]   It's a good thing for Google to be doing.
[00:21:17.660 --> 00:21:20.940]   And again, I think that this is a lot like Google's fiber
[00:21:20.940 --> 00:21:24.260]   project where they're trying to pressure and squeeze
[00:21:24.260 --> 00:21:28.580]   the carriers out there into having better services,
[00:21:28.580 --> 00:21:29.500]   clearer services.
[00:21:29.500 --> 00:21:30.900]   One of the things that's great about this
[00:21:30.900 --> 00:21:34.380]   is that it's a set price.
[00:21:34.380 --> 00:21:36.100]   And everything is upfront.
[00:21:36.100 --> 00:21:40.420]   If you use more, you pay more, and so on.
[00:21:40.420 --> 00:21:43.420]   And it's very clear and open, unlike your typical carrier
[00:21:43.420 --> 00:21:44.860]   situation, where it's like, oh, we're
[00:21:44.860 --> 00:21:46.700]   going to subsidize your phone.
[00:21:46.700 --> 00:21:48.700]   Your phone is only going to be $600.
[00:21:48.700 --> 00:21:50.620]   Your phone's only going to be $200.
[00:21:50.620 --> 00:21:51.980]   And we're subsidizing it.
[00:21:51.980 --> 00:21:53.140]   No, they're not subsidizing it.
[00:21:53.140 --> 00:21:57.020]   Subsidizing it is when they transfer money from one person
[00:21:57.020 --> 00:21:58.620]   and transfer it to another.
[00:21:58.620 --> 00:22:01.660]   In this case, they are--
[00:22:01.660 --> 00:22:03.340]   you're paying-- you're essentially
[00:22:03.340 --> 00:22:05.700]   being subsidized by yourself, because they will make sure
[00:22:05.700 --> 00:22:08.000]   that you pay for that phone for the two years.
[00:22:08.000 --> 00:22:09.460]   So it's stretched out over two years.
[00:22:09.460 --> 00:22:11.100]   And if you keep your phone longer than two years,
[00:22:11.100 --> 00:22:12.980]   you're just going to keep paying for that phone.
[00:22:12.980 --> 00:22:16.460]   That same $20, whatever that they added on to the monthly bill
[00:22:16.460 --> 00:22:19.740]   is going to continue even after the long after the phone is
[00:22:19.740 --> 00:22:20.500]   paid off.
[00:22:20.500 --> 00:22:23.740]   So this is a great thing for that reason.
[00:22:23.740 --> 00:22:26.660]   And you know, I don't know if this is going to work or not,
[00:22:26.660 --> 00:22:30.380]   but I'm all in favor of things that will try and create
[00:22:30.380 --> 00:22:33.620]   more competition for phone companies and cable companies
[00:22:33.620 --> 00:22:34.180]   for that matter.
[00:22:34.180 --> 00:22:36.820]   So if Google wants to offer free fiber,
[00:22:36.820 --> 00:22:39.340]   wants to offer Wi-Fi calling, I think that's great.
[00:22:39.340 --> 00:22:43.580]   More pressure, they can keep on phone companies the better.
[00:22:43.580 --> 00:22:47.620]   All right, so there's a lot more animosity
[00:22:47.620 --> 00:22:51.100]   in the tech journalist community about Google Glass,
[00:22:51.100 --> 00:22:53.220]   because they're back in the news for two reasons.
[00:22:53.220 --> 00:23:01.060]   First is that a product went up for approval
[00:23:01.060 --> 00:23:02.340]   at the FCC website.
[00:23:02.340 --> 00:23:07.100]   It's codename GG1, which is almost certainly Google Glass.
[00:23:07.100 --> 00:23:11.060]   And whatever this thing is, it supports Bluetooth and Wi-Fi.
[00:23:11.060 --> 00:23:14.020]   And it's a mobile device of some kind, probably Google Glass.
[00:23:14.020 --> 00:23:17.100]   The second reason is that there's something in the works
[00:23:17.100 --> 00:23:19.980]   called the Enterprise Edition of Google Glass, which
[00:23:19.980 --> 00:23:23.500]   looks like it may have an actually larger prism, if you
[00:23:23.500 --> 00:23:25.940]   can believe that, and an external battery pack.
[00:23:25.940 --> 00:23:32.180]   This would be for companies to use for vertical applications.
[00:23:32.180 --> 00:23:33.780]   Two things here that I want to talk about.
[00:23:33.780 --> 00:23:37.140]   One is, why does the press hate Google Glass?
[00:23:37.140 --> 00:23:39.340]   Anybody?
[00:23:39.340 --> 00:23:42.620]   Because I bought it.
[00:23:42.620 --> 00:23:44.180]   I paid $2,000 for it.
[00:23:44.180 --> 00:23:45.860]   I'm a schmuck.
[00:23:45.860 --> 00:23:49.660]   I'm a complete and absolute Google fan boy schmuck.
[00:23:49.660 --> 00:23:52.780]   I tried and tried and tried to say, oh, look at all the computer.
[00:23:52.780 --> 00:23:53.780]   Look at all the waters.
[00:23:53.780 --> 00:23:55.020]   No, I'm sorry.
[00:23:55.020 --> 00:23:56.420]   It didn't work.
[00:23:56.420 --> 00:23:57.260]   And you know what?
[00:23:57.260 --> 00:24:00.380]   I got to say, in this case, Google messed it up.
[00:24:00.380 --> 00:24:02.100]   I don't think they've admitted it to themselves.
[00:24:02.100 --> 00:24:03.820]   I don't think they've admitted it to the world,
[00:24:03.820 --> 00:24:06.300]   or to the people who spent thousands of dollars on this stuff.
[00:24:06.300 --> 00:24:10.220]   And I'd say it's one area where Google's worthy of ridicule.
[00:24:10.220 --> 00:24:12.940]   What did they mess up?
[00:24:12.940 --> 00:24:14.700]   It doesn't work well.
[00:24:14.700 --> 00:24:16.100]   Does it make sense?
[00:24:16.100 --> 00:24:17.020]   It hurts.
[00:24:17.020 --> 00:24:21.140]   They try to cram too much functionality into it.
[00:24:21.140 --> 00:24:22.140]   I mean, all kinds of things.
[00:24:22.140 --> 00:24:26.060]   And it brought all kinds of privacy problems on them
[00:24:26.060 --> 00:24:27.420]   that were absolutely unnecessary.
[00:24:27.420 --> 00:24:29.380]   If they'd been a red light or some kind of transparency,
[00:24:29.380 --> 00:24:30.260]   I could go on.
[00:24:30.260 --> 00:24:31.380]   But why should I?
[00:24:31.380 --> 00:24:33.940]   It only brings the pain back out.
[00:24:33.940 --> 00:24:35.220]   I think it was a classic--
[00:24:35.220 --> 00:24:36.420]   you know, I don't know this, but I
[00:24:36.420 --> 00:24:41.660]   think it was a classic sort of the role that was just poorly
[00:24:41.660 --> 00:24:42.420]   handled.
[00:24:42.420 --> 00:24:46.980]   So if it had been described as a really experimental thing,
[00:24:46.980 --> 00:24:49.740]   and only a few people, and originally it was that,
[00:24:49.740 --> 00:24:53.540]   the Explorers Program, if it had stopped as a sort of, hey,
[00:24:53.540 --> 00:24:56.900]   super nerds, help us test this thing.
[00:24:56.900 --> 00:25:00.100]   But it became a kind of, this is the next big thing,
[00:25:00.100 --> 00:25:02.700]   and everyone's going to wear it, and it's going to change your life.
[00:25:02.700 --> 00:25:04.340]   And obviously, it didn't do any of those things,
[00:25:04.340 --> 00:25:06.460]   and most people are never going to wear them,
[00:25:06.460 --> 00:25:07.900]   because you look like an idiot.
[00:25:07.900 --> 00:25:12.580]   But if it had been sort of couched in the right way,
[00:25:12.580 --> 00:25:14.260]   I think it would have been a very cool thing.
[00:25:14.260 --> 00:25:15.660]   Lots of people would have done it.
[00:25:15.660 --> 00:25:18.900]   But the vast majority of people just are not interested.
[00:25:18.900 --> 00:25:19.700]   Never will be.
[00:25:19.700 --> 00:25:22.900]   Kurt, did you ever--
[00:25:22.900 --> 00:25:23.940]   I've warned Google--
[00:25:23.940 --> 00:25:25.420]   Sue, what's about going to--
[00:25:25.420 --> 00:25:27.620]   I was going to echo it, Matt.
[00:25:27.620 --> 00:25:28.460]   Did you glass?
[00:25:28.460 --> 00:25:31.180]   I explored, as they say.
[00:25:31.180 --> 00:25:33.540]   I was going to echo what Matthew just said,
[00:25:33.540 --> 00:25:35.300]   just about the way that they even look.
[00:25:35.300 --> 00:25:37.700]   I mean, I think the big thing is, like, if you're
[00:25:37.700 --> 00:25:40.060]   going to go out on a limb, and you're
[00:25:40.060 --> 00:25:43.300]   going to wear this thing on your face,
[00:25:43.300 --> 00:25:46.860]   then it already makes you look a little weird or a little wonky.
[00:25:46.860 --> 00:25:49.420]   It better be pretty amazing.
[00:25:49.420 --> 00:25:53.620]   And you can see that with a lot of these watches now, even.
[00:25:53.620 --> 00:25:54.980]   A watch isn't even on your face.
[00:25:54.980 --> 00:25:58.100]   And yet, people, as soon as they started wearing smartwatches,
[00:25:58.100 --> 00:26:00.100]   if it didn't dramatically improve their life,
[00:26:00.100 --> 00:26:01.500]   they were like, I'm not going to wear this thing.
[00:26:01.500 --> 00:26:03.820]   It looks like a big calculator on my wrist.
[00:26:03.820 --> 00:26:04.780]   This is ridiculous.
[00:26:04.780 --> 00:26:09.580]   And that's why the Apple watch is so important to the smartwatch
[00:26:09.580 --> 00:26:12.260]   movement, if you will, because actually a lot of people
[00:26:12.260 --> 00:26:13.260]   think it looks good.
[00:26:13.260 --> 00:26:17.180]   So I think the fact that this thing looked like a high school
[00:26:17.180 --> 00:26:21.100]   science fair project did not help Google's cause at all.
[00:26:21.100 --> 00:26:22.100]   Well, I think one of the--
[00:26:22.100 --> 00:26:25.620]   Of course, it also-- it fed into a lot of the things
[00:26:25.620 --> 00:26:27.940]   that people already think about Google,
[00:26:27.940 --> 00:26:30.260]   and already think about Larry and Sergey,
[00:26:30.260 --> 00:26:32.260]   and already think about Silicon Valley,
[00:26:32.260 --> 00:26:34.060]   and already think about Technerts,
[00:26:34.060 --> 00:26:37.380]   that they have no sense of what real life is like,
[00:26:37.380 --> 00:26:39.860]   that they're willing to use any old gadget,
[00:26:39.860 --> 00:26:43.060]   even if they look like the Borg, that it just fed
[00:26:43.060 --> 00:26:46.700]   into all those sort of preconceptions about what
[00:26:46.700 --> 00:26:48.020]   Technerts are like.
[00:26:48.020 --> 00:26:50.300]   I think one of the central problems with it also
[00:26:50.300 --> 00:26:55.140]   is that the camera is the problem.
[00:26:55.140 --> 00:26:58.380]   On the one hand, the camera made everybody nervous.
[00:26:58.380 --> 00:26:59.900]   They felt like when they're talking to somebody,
[00:26:59.900 --> 00:27:02.860]   they thought, falsely, it's a terrible spy device.
[00:27:02.860 --> 00:27:05.260]   You can completely tell if somebody's taken video.
[00:27:05.260 --> 00:27:06.340]   But nobody knew that.
[00:27:06.340 --> 00:27:08.460]   It was an unfamiliar product to the public.
[00:27:08.460 --> 00:27:10.660]   So if you're sitting there talking to somebody,
[00:27:10.660 --> 00:27:12.660]   there's a lot of discomfort around that.
[00:27:12.660 --> 00:27:14.940]   And at the same time, really, the camera is the only thing
[00:27:14.940 --> 00:27:16.660]   anybody used it for.
[00:27:16.660 --> 00:27:20.460]   And so removing the camera is impossible,
[00:27:20.460 --> 00:27:22.260]   and leaving the camera there is impossible.
[00:27:22.260 --> 00:27:24.860]   I mean, I remember one of the first times I realized
[00:27:24.860 --> 00:27:29.660]   that this is actually kind of a new medium as a camera,
[00:27:29.660 --> 00:27:33.460]   is when Gina Trapani took a picture of her baby.
[00:27:33.460 --> 00:27:34.660]   Her hands were full.
[00:27:34.660 --> 00:27:36.860]   Of course, she's a mother holding a baby,
[00:27:36.860 --> 00:27:43.260]   and you got this picture of the baby looking right at her mother.
[00:27:43.260 --> 00:27:45.380]   And Gina's hair was hanging down.
[00:27:45.380 --> 00:27:47.460]   So it was really like a first person
[00:27:47.460 --> 00:27:49.660]   you are there kind of experience.
[00:27:49.660 --> 00:27:53.780]   And I took a lot of pictures with Google Glass
[00:27:53.780 --> 00:27:56.500]   when I was traveling around and had my hands full,
[00:27:56.500 --> 00:27:59.140]   and there were situations where I could take a picture
[00:27:59.140 --> 00:28:02.140]   with a wink, which is one of the gestures that took pictures.
[00:28:02.140 --> 00:28:04.060]   And yeah, I look like a complete idiot,
[00:28:04.060 --> 00:28:06.260]   but I took some really great pictures.
[00:28:06.260 --> 00:28:08.860]   But I ultimately think that this kind of technology
[00:28:08.860 --> 00:28:11.540]   is going to succeed only when they remove the camera.
[00:28:11.540 --> 00:28:14.380]   So if they can build it into ordinary eyeglasses
[00:28:14.380 --> 00:28:18.380]   and have notifications that are subtle and appropriate,
[00:28:18.380 --> 00:28:22.940]   and minimalist and important, and actually benefit people
[00:28:22.940 --> 00:28:28.100]   in a way that's very non-intrusive,
[00:28:28.100 --> 00:28:30.740]   I think that could be an application for this type--
[00:28:30.740 --> 00:28:33.740]   in fact, I'm certain of it that this is the future of eyeglasses
[00:28:33.740 --> 00:28:36.940]   is going to have some sort of intelligence or notifications.
[00:28:36.940 --> 00:28:39.140]   The problem is that you had this giant--
[00:28:39.140 --> 00:28:40.140]   this giant--
[00:28:40.140 --> 00:28:41.620]   [INTERPOSING VOICES]
[00:28:41.620 --> 00:28:44.260]   [INTERPOSING VOICES]
[00:28:44.260 --> 00:28:45.100]   Go ahead, Maggie.
[00:28:45.100 --> 00:28:47.460]   Camera was half of the reason why people were interested in it.
[00:28:47.460 --> 00:28:50.180]   I mean, to me, the one thing I wanted it for
[00:28:50.180 --> 00:28:53.500]   was to walk around Europe in some unfamiliar city
[00:28:53.500 --> 00:28:56.620]   and have the glasses tell me what the heck I was looking at.
[00:28:56.620 --> 00:28:58.300]   I would actually pay money for that.
[00:28:58.300 --> 00:28:59.580]   So am I going to wear it all the time?
[00:28:59.580 --> 00:29:00.180]   No.
[00:29:00.180 --> 00:29:02.220]   But I think it would make a really great kind
[00:29:02.220 --> 00:29:05.220]   of targeted device for specific purposes.
[00:29:05.220 --> 00:29:07.140]   So maybe that's part of what they're thinking of doing.
[00:29:07.140 --> 00:29:08.140]   I don't know.
[00:29:08.140 --> 00:29:10.780]   I think they try to cram too much into one thing.
[00:29:10.780 --> 00:29:12.660]   And there's a category, I think, on the one down
[00:29:12.660 --> 00:29:16.180]   where you could try to frame pictures for it.
[00:29:16.180 --> 00:29:21.260]   I think a point of view camera is its own product.
[00:29:21.260 --> 00:29:23.420]   I think alerts are their own product, which I think
[00:29:23.420 --> 00:29:24.500]   the watch is fine for.
[00:29:24.500 --> 00:29:26.140]   I don't really want them up here.
[00:29:26.140 --> 00:29:29.540]   And then instructions for certain activities
[00:29:29.540 --> 00:29:30.380]   will work up here.
[00:29:30.380 --> 00:29:32.940]   But I don't need and want everything up here in view
[00:29:32.940 --> 00:29:34.660]   because people will just bug me with it.
[00:29:34.660 --> 00:29:36.700]   And so the watch-- so you split the three things up
[00:29:36.700 --> 00:29:37.380]   and they could work.
[00:29:37.380 --> 00:29:38.740]   Sorry, Kurt, you were going to start missing.
[00:29:38.740 --> 00:29:39.620]   Oh, no, no, that's OK.
[00:29:39.620 --> 00:29:41.700]   I was going to mention something along those lines.
[00:29:41.700 --> 00:29:47.100]   And actually, I was going to say that in my thinking
[00:29:47.100 --> 00:29:50.260]   that glass would, in some ways, replace your phone.
[00:29:50.260 --> 00:29:53.540]   But if it doesn't have a camera, then you still need your phone.
[00:29:53.540 --> 00:29:57.660]   And the way that people are taking so many photos now,
[00:29:57.660 --> 00:30:00.860]   videos, you look at like Meerkat and Periscope,
[00:30:00.860 --> 00:30:03.460]   the idea of live broadcasting as well,
[00:30:03.460 --> 00:30:06.580]   if none of that could be done with glass,
[00:30:06.580 --> 00:30:09.180]   I just don't think that the notification stuff alone
[00:30:09.180 --> 00:30:12.420]   is enough to get someone to shut out that kind of money for it.
[00:30:12.420 --> 00:30:15.780]   I mean, obviously, they'd probably make it a little cheaper.
[00:30:15.780 --> 00:30:19.380]   I also think that this product may just be way too early.
[00:30:19.380 --> 00:30:21.900]   I mean, do you remember someone posted something
[00:30:21.900 --> 00:30:23.420]   on Twitter earlier today, hey, remember
[00:30:23.420 --> 00:30:25.740]   when we were all creeped out about sharing our phone's
[00:30:25.740 --> 00:30:26.980]   location?
[00:30:26.980 --> 00:30:28.780]   Do you remember when that was a huge deal?
[00:30:28.780 --> 00:30:31.020]   Now everyone doesn't know one thinks about it.
[00:30:31.020 --> 00:30:34.060]   All kinds of companies have access to your phone location.
[00:30:34.060 --> 00:30:38.100]   I wonder if this is the type of product that years from now
[00:30:38.100 --> 00:30:40.100]   will just think it's totally normal.
[00:30:40.100 --> 00:30:42.580]   Of course, someone's taking pictures with their glasses,
[00:30:42.580 --> 00:30:43.940]   or, of course, whatever.
[00:30:43.940 --> 00:30:47.900]   But it was just too early, too sudden, too much.
[00:30:47.900 --> 00:30:49.340]   Yeah, I agree with that, Matthew.
[00:30:49.340 --> 00:30:50.780]   And I think the way that it's probably
[00:30:50.780 --> 00:30:54.220]   going to enter into the popular culture
[00:30:54.220 --> 00:30:57.420]   is through sports and fitness and other things like that.
[00:30:57.420 --> 00:31:01.420]   I mean, you can easily see that scuba diving masks
[00:31:01.420 --> 00:31:04.020]   and ski goggles and things like that
[00:31:04.020 --> 00:31:07.420]   are going to get data about your speed, your depth.
[00:31:07.420 --> 00:31:09.900]   Important things right now, if you're a scuba diver,
[00:31:09.900 --> 00:31:11.820]   you have a dive computer on your wrist.
[00:31:11.820 --> 00:31:13.180]   It's a gigantic thing.
[00:31:13.180 --> 00:31:15.620]   You're already wearing this, essentially.
[00:31:15.620 --> 00:31:16.860]   It's an interface.
[00:31:16.860 --> 00:31:18.540]   It should be just heads up display right there.
[00:31:18.540 --> 00:31:20.900]   Of course, cars will get heads up displays, which
[00:31:20.900 --> 00:31:22.660]   is better to have it on the dashboard,
[00:31:22.660 --> 00:31:25.660]   to have it on the windshield, or to have it in your--
[00:31:25.660 --> 00:31:26.660]   I don't know.
[00:31:26.660 --> 00:31:27.660]   --in the eyes.
[00:31:27.660 --> 00:31:28.660]   I don't know.
[00:31:28.660 --> 00:31:29.460]   Yeah.
[00:31:29.460 --> 00:31:30.420]   I mean, who knows?
[00:31:30.420 --> 00:31:34.020]   But I think that glamorous applications of heads up
[00:31:34.020 --> 00:31:36.020]   technology on something that's on your face
[00:31:36.020 --> 00:31:37.500]   are going to happen.
[00:31:37.500 --> 00:31:38.980]   And then people will be influenced by it.
[00:31:38.980 --> 00:31:42.100]   Ray Ban, which, of course, is owned by Luxautica, which,
[00:31:42.100 --> 00:31:44.340]   of course, has been reportedly working
[00:31:44.340 --> 00:31:48.540]   on a version of Google Glass for some or one of their brands.
[00:31:48.540 --> 00:31:50.220]   You can see that kind of thing coming
[00:31:50.220 --> 00:31:53.860]   about extreme sports, whatever it is.
[00:31:53.860 --> 00:31:55.900]   And it'll have to get the cache that way.
[00:31:55.900 --> 00:31:57.100]   Look at where GoPro's.
[00:31:57.100 --> 00:31:58.100]   Yeah.
[00:31:58.100 --> 00:31:59.100]   Who do we're GoPro's?
[00:31:59.100 --> 00:32:02.220]   GoPro does a lot of the same things at Glass.
[00:32:02.220 --> 00:32:04.340]   Not the photos so much, but live video.
[00:32:04.340 --> 00:32:06.940]   But it's much more obvious, and it's much more specific
[00:32:06.940 --> 00:32:07.900]   to certain activities.
[00:32:07.900 --> 00:32:10.740]   Nobody walks around with a GoPro all day
[00:32:10.740 --> 00:32:12.580]   during the regular life.
[00:32:12.580 --> 00:32:14.940]   But look how cool it is to have a GoPro on your head,
[00:32:14.940 --> 00:32:17.060]   like to have a helmet with this weird thing.
[00:32:17.060 --> 00:32:18.380]   So you look ridiculous.
[00:32:18.380 --> 00:32:19.860]   But if you're jumping out of an airplane,
[00:32:19.860 --> 00:32:21.460]   nobody's going to think you look ridiculous.
[00:32:21.460 --> 00:32:23.420]   See, Google, what Google needs is somebody
[00:32:23.420 --> 00:32:24.820]   to look even more ridiculous.
[00:32:24.820 --> 00:32:26.940]   So once Oculus is really popular,
[00:32:26.940 --> 00:32:29.020]   and people are wearing basically a face mask,
[00:32:29.020 --> 00:32:30.940]   where you can't see any of their face,
[00:32:30.940 --> 00:32:34.460]   then Google can say, look, we look way better than that.
[00:32:34.460 --> 00:32:38.580]   Well, Google could have convinced a bunch of hardcore snowboarder
[00:32:38.580 --> 00:32:41.580]   and surfers to wear Google Glass instead of all the tech
[00:32:41.580 --> 00:32:42.700]   engineers in San Francisco.
[00:32:42.700 --> 00:32:44.980]   Maybe that would have changed the whole perception
[00:32:44.980 --> 00:32:45.940]   from the get-go, right?
[00:32:45.940 --> 00:32:48.820]   Because I mean, that's what GoPro is so cool, right?
[00:32:48.820 --> 00:32:52.420]   There's a cool feeling to it that Google Glass just never had.
[00:32:52.420 --> 00:32:55.060]   But Google did get the models, right?
[00:32:55.060 --> 00:32:58.940]   The models on the runway to try and get the, I don't know,
[00:32:58.940 --> 00:33:02.420]   the fashion aspect, which actually, in retrospect,
[00:33:02.420 --> 00:33:03.940]   is kind of hilarious, right?
[00:33:03.940 --> 00:33:05.820]   Like they went for fashion.
[00:33:05.820 --> 00:33:09.460]   But in reality, that's what everybody knocks on glasses,
[00:33:09.460 --> 00:33:12.020]   doesn't it look ridiculous?
[00:33:12.020 --> 00:33:16.580]   Yeah, and it doesn't matter what the model's point of view is.
[00:33:16.580 --> 00:33:16.980]   Does it?
[00:33:16.980 --> 00:33:18.380]   I mean, you know what I mean?
[00:33:18.380 --> 00:33:23.260]   It's like a model on a catwalk, their purpose
[00:33:23.260 --> 00:33:24.500]   is to model the clothes.
[00:33:24.500 --> 00:33:29.140]   And it's a mini to one medium, if you will.
[00:33:29.140 --> 00:33:31.180]   The experience of being somebody wearing clothes
[00:33:31.180 --> 00:33:32.380]   is not all that compelling.
[00:33:32.380 --> 00:33:34.740]   I wanted to see Rory McElroy wearing a set,
[00:33:34.740 --> 00:33:39.740]   or somebody doing something that you would want to actually
[00:33:39.740 --> 00:33:42.380]   take part in or see from their point of view.
[00:33:42.380 --> 00:33:44.260]   Yeah, yeah.
[00:33:44.260 --> 00:33:46.100]   All right, well, speaking of things
[00:33:46.100 --> 00:33:48.460]   that Google's getting into that maybe they should,
[00:33:48.460 --> 00:33:50.980]   maybe they shouldn't get into, Google
[00:33:50.980 --> 00:33:55.660]   has launched through its Waze division in Tel Aviv, Israel,
[00:33:55.660 --> 00:33:57.860]   a new service that's not like Uber.
[00:33:57.860 --> 00:34:00.180]   It's more like a carpooling service.
[00:34:00.180 --> 00:34:03.140]   They're testing it just in that one city, Tel Aviv.
[00:34:03.140 --> 00:34:05.300]   Of course, Google bought Waze two years ago.
[00:34:05.300 --> 00:34:09.580]   Waze is a social mapping and navigation app
[00:34:09.580 --> 00:34:13.540]   that's actually very cool in many ways, cooler than Google Maps,
[00:34:13.540 --> 00:34:17.500]   because they crowdsource the location of the police,
[00:34:17.500 --> 00:34:20.860]   of hazards on the road, of gas stations, and things like that.
[00:34:20.860 --> 00:34:22.500]   And you can see other users as they're
[00:34:22.500 --> 00:34:27.260]   driving around with little cartoon avatars and stuff like that.
[00:34:27.260 --> 00:34:30.140]   My question is-- and I'll throw this out to you.
[00:34:30.140 --> 00:34:32.420]   I'll start with you, Kurt.
[00:34:32.420 --> 00:34:33.860]   Is this even a business?
[00:34:33.860 --> 00:34:36.380]   I mean, carpooling?
[00:34:36.380 --> 00:34:39.140]   If Google wanted to get into the Uber and compete with Uber,
[00:34:39.140 --> 00:34:40.700]   I could see, yeah, that's a big business.
[00:34:40.700 --> 00:34:43.500]   But carpooling, it seems like a small potato.
[00:34:43.500 --> 00:34:45.460]   It's for a company like Google.
[00:34:45.460 --> 00:34:49.580]   Yeah, but I mean, I would have probably said the same thing
[00:34:49.580 --> 00:34:53.020]   about Uber and Lyft even just trying to get into their--
[00:34:53.020 --> 00:34:55.060]   what they're essentially doing carpooling, right?
[00:34:55.060 --> 00:34:58.140]   They're shared rides where I'm getting picked up,
[00:34:58.140 --> 00:34:59.340]   and then three bucks later, they're
[00:34:59.340 --> 00:35:01.620]   picking up a stranger and taking us to the same spot.
[00:35:01.620 --> 00:35:04.500]   I mean, I think just the whole idea of taking advantage
[00:35:04.500 --> 00:35:09.100]   of the fact that people are doing the same kind of thing
[00:35:09.100 --> 00:35:12.300]   and like, OK, if we're all doing this, let's do it together.
[00:35:12.300 --> 00:35:14.940]   Yes, it may be small potato right now.
[00:35:14.940 --> 00:35:19.020]   I do think there's a need for taking advantage of that,
[00:35:19.020 --> 00:35:25.940]   and maybe Google thinks that they can help fill that need.
[00:35:25.940 --> 00:35:27.460]   Yeah, the cost is debinnable.
[00:35:27.460 --> 00:35:35.100]   In most ways, it brings far fewer complications
[00:35:35.100 --> 00:35:37.860]   than Uber and Lyft because you're not a professional driver.
[00:35:37.860 --> 00:35:40.300]   You're just, in fact, saying we're shared with this ride.
[00:35:40.300 --> 00:35:42.020]   Now, there may be some liability involved.
[00:35:42.020 --> 00:35:44.580]   Put somebody in a car with somebody dangerous,
[00:35:44.580 --> 00:35:46.060]   and Google does that.
[00:35:46.060 --> 00:35:47.060]   That's another matter.
[00:35:47.060 --> 00:35:48.700]   But it's good for the environment.
[00:35:48.700 --> 00:35:49.860]   It's a lot easier.
[00:35:49.860 --> 00:35:53.620]   It's limited to only be able to two rides per day
[00:35:53.620 --> 00:35:58.220]   so that you don't use it as a fake Uber.
[00:35:58.220 --> 00:35:59.980]   You agree upon what you're charged.
[00:35:59.980 --> 00:36:02.700]   I think Google gets 15% of that.
[00:36:02.700 --> 00:36:03.980]   I think it's really fascinating.
[00:36:03.980 --> 00:36:06.020]   And I think that there are other ways to consider--
[00:36:06.020 --> 00:36:08.620]   I mean, this is more about a true sharing economy.
[00:36:08.620 --> 00:36:10.940]   This is where you're sharing the ride.
[00:36:10.940 --> 00:36:13.700]   And there's a way to make that possible.
[00:36:13.700 --> 00:36:15.460]   That's all.
[00:36:15.460 --> 00:36:19.020]   Yeah, and I think if you think about something like Airbnb,
[00:36:19.020 --> 00:36:21.700]   when Airbnb started, it was literally blow up
[00:36:21.700 --> 00:36:23.940]   mattresses in people's living rooms,
[00:36:23.940 --> 00:36:26.860]   and many people, including me, thought no one is ever
[00:36:26.860 --> 00:36:27.580]   going to do that.
[00:36:27.580 --> 00:36:29.460]   That sounds ridiculous.
[00:36:29.460 --> 00:36:31.940]   It's not clearly not a business.
[00:36:31.940 --> 00:36:33.940]   I'm not saying this is going to turn into that,
[00:36:33.940 --> 00:36:37.860]   but I could see this does make sense to use an app,
[00:36:37.860 --> 00:36:41.860]   to use technology to connect people who need to go places
[00:36:41.860 --> 00:36:43.300]   and want to share a ride.
[00:36:43.300 --> 00:36:46.420]   That's a clear and obvious need where technology,
[00:36:46.420 --> 00:36:49.900]   and particularly social technology, can help.
[00:36:49.900 --> 00:36:50.620]   Unbelievable.
[00:36:50.620 --> 00:36:52.820]   Well, we'll see how that goes.
[00:36:52.820 --> 00:36:55.500]   I, for one, would like to see Google do more with Waze.
[00:36:55.500 --> 00:36:58.380]   Waze is one of those companies, I think, like Nest, which
[00:36:58.380 --> 00:36:59.540]   is off on its own.
[00:36:59.540 --> 00:37:02.780]   It's more or less churning out the same product
[00:37:02.780 --> 00:37:05.660]   that it would have done had Google not acquired it.
[00:37:05.660 --> 00:37:09.220]   And I'd love to see the social features built
[00:37:09.220 --> 00:37:10.620]   into Google Maps, because I think--
[00:37:10.620 --> 00:37:12.620]   I completely adore Waze.
[00:37:12.620 --> 00:37:15.460]   Yeah, I think it's a great service.
[00:37:15.460 --> 00:37:18.260]   It is-- I would argue that if we thought
[00:37:18.260 --> 00:37:20.340]   expansively about journalism, we'd
[00:37:20.340 --> 00:37:23.180]   be thinking about Waze to help the public share what they know.
[00:37:23.180 --> 00:37:26.020]   No, Pan-Tent.
[00:37:26.020 --> 00:37:27.540]   What was up on there?
[00:37:27.540 --> 00:37:28.020]   Waze.
[00:37:28.020 --> 00:37:29.020]   Oh, Waze.
[00:37:29.020 --> 00:37:29.540]   Sorry.
[00:37:29.540 --> 00:37:30.060]   Man.
[00:37:30.060 --> 00:37:31.460]   Oh, man.
[00:37:31.460 --> 00:37:33.820]   Oh, you got--
[00:37:33.820 --> 00:37:35.180]   Is there any word on our job?
[00:37:35.180 --> 00:37:36.900]   Deep dream going on heavy in your--
[00:37:36.900 --> 00:37:37.860]   Yeah, yeah.
[00:37:37.860 --> 00:37:40.020]   I can't get this dog spaghetti out of my mind.
[00:37:40.020 --> 00:37:42.140]   Anyway, please proceed.
[00:37:42.140 --> 00:37:43.940]   But yeah, it's a way that people just
[00:37:43.940 --> 00:37:46.420]   can share information and help each other.
[00:37:46.420 --> 00:37:48.060]   And that's what we ought to be enabling.
[00:37:48.060 --> 00:37:50.620]   But that's-- we don't ever think that way.
[00:37:50.620 --> 00:37:52.700]   And you know, Waze is another one of those things.
[00:37:52.700 --> 00:37:57.100]   Maybe I'm betraying a flaw in my thinking.
[00:37:57.100 --> 00:37:59.340]   But Waze was another thing where I looked at it and thought,
[00:37:59.340 --> 00:38:00.820]   no one's ever going to do that.
[00:38:00.820 --> 00:38:03.220]   No one's ever going to take time to type into their phone.
[00:38:03.220 --> 00:38:06.100]   Hey, there's a cop at this intersection or this light is out
[00:38:06.100 --> 00:38:07.580]   or there's an accident.
[00:38:07.580 --> 00:38:08.540]   You know what?
[00:38:08.540 --> 00:38:10.380]   Hundreds of thousands or millions of people
[00:38:10.380 --> 00:38:12.500]   are more than happy to do that.
[00:38:12.500 --> 00:38:15.980]   And when you're using it, it is incredibly useful,
[00:38:15.980 --> 00:38:17.180]   especially the cop stuff.
[00:38:17.180 --> 00:38:18.180]   Yeah, can we be honest?
[00:38:18.180 --> 00:38:20.180]   It's all about the police knowing where the police are.
[00:38:20.180 --> 00:38:20.740]   Yeah, come on.
[00:38:20.740 --> 00:38:21.220]   So you can speak.
[00:38:21.220 --> 00:38:22.340]   The police thing is great.
[00:38:22.340 --> 00:38:24.460]   But I have gotten to the point where
[00:38:24.460 --> 00:38:27.860]   we have a GPS in our car, which I never look at.
[00:38:27.860 --> 00:38:30.380]   Because it's stupid and it doesn't update.
[00:38:30.380 --> 00:38:33.140]   And it's just-- it's hard to search for things.
[00:38:33.140 --> 00:38:36.620]   Waze is now taking control of when we go somewhere.
[00:38:36.620 --> 00:38:39.340]   If Waze says, don't go that way, go this way,
[00:38:39.340 --> 00:38:40.540]   I'm going to go that way.
[00:38:40.540 --> 00:38:43.700]   I'm not even going to look because every time I do it,
[00:38:43.700 --> 00:38:46.460]   it winds up being better.
[00:38:46.460 --> 00:38:47.020]   Unbelievable.
[00:38:47.020 --> 00:38:48.660]   Well, let's take another break.
[00:38:48.660 --> 00:38:50.020]   And when we come back, we're going
[00:38:50.020 --> 00:38:53.100]   to talk about whether algorithms can be sexist,
[00:38:53.100 --> 00:38:55.900]   or in fact, whether Google's algorithms for job hunting
[00:38:55.900 --> 00:38:57.580]   are, in fact, sexist.
[00:38:57.580 --> 00:39:01.260]   But first, let's talk about hiring great people.
[00:39:01.260 --> 00:39:04.140]   You want to hire great people no matter what gender, no matter
[00:39:04.140 --> 00:39:04.900]   what.
[00:39:04.900 --> 00:39:07.100]   And in the problem with hiring people these days, of course,
[00:39:07.100 --> 00:39:10.460]   is that people who are looking for work are everywhere.
[00:39:10.460 --> 00:39:11.540]   They're on the social networks.
[00:39:11.540 --> 00:39:14.820]   They're on dozens or hundreds of job boards.
[00:39:14.820 --> 00:39:16.300]   They're just absolutely everywhere.
[00:39:16.300 --> 00:39:18.980]   And if you want to go from site to site to site,
[00:39:18.980 --> 00:39:20.540]   you're actually going to need to hire somebody
[00:39:20.540 --> 00:39:21.780]   to help you with that.
[00:39:21.780 --> 00:39:23.140]   There's a much easier way, which is
[00:39:23.140 --> 00:39:25.860]   to go to Zippercruter, one of our sponsors today,
[00:39:25.860 --> 00:39:29.740]   where you can post to 100 plus job sites with a single click
[00:39:29.740 --> 00:39:32.140]   and social networks, Facebook, LinkedIn, Twitter,
[00:39:32.140 --> 00:39:35.060]   and Google+, and the Twitter feature
[00:39:35.060 --> 00:39:39.420]   will even auto post your jobs as soon as you post them,
[00:39:39.420 --> 00:39:40.620]   off goes a tweet.
[00:39:40.620 --> 00:39:42.780]   And so people following your Twitter account
[00:39:42.780 --> 00:39:44.140]   will see that right away.
[00:39:44.140 --> 00:39:47.780]   They even support LinkedIn groups and Facebook pages.
[00:39:47.780 --> 00:39:51.580]   And you can post to profiles, whatever you want to do.
[00:39:51.580 --> 00:39:54.860]   And they will automatically and instantly match you
[00:39:54.860 --> 00:39:59.100]   to candidates from over 4 million resumes in their Jai
[00:39:59.100 --> 00:40:01.380]   Gundo resume database.
[00:40:01.380 --> 00:40:03.980]   They're used by over 400,000 businesses,
[00:40:03.980 --> 00:40:06.620]   and you can try it now for free.
[00:40:06.620 --> 00:40:10.020]   And they also have a feature called Traffic Boost.
[00:40:10.020 --> 00:40:11.500]   So if you get the Traffic Boost feature,
[00:40:11.500 --> 00:40:15.380]   they will put your post into Zippercruter's daily job alerts
[00:40:15.380 --> 00:40:17.780]   for that really, really important job
[00:40:17.780 --> 00:40:19.260]   that you're trying to hire for.
[00:40:19.260 --> 00:40:22.060]   Put it in Traffic Boost, you'll get a huge, huge return.
[00:40:22.060 --> 00:40:26.260]   And you don't have to wade through all of the responses
[00:40:26.260 --> 00:40:29.100]   that you get because Zippercruter will separate
[00:40:29.100 --> 00:40:31.860]   the most qualified from the least qualified
[00:40:31.860 --> 00:40:34.460]   so you can spend your valuable time
[00:40:34.460 --> 00:40:36.620]   on the most qualified candidates.
[00:40:36.620 --> 00:40:39.340]   You also want to check out their blog at zippercruter.com/blog
[00:40:39.340 --> 00:40:40.940]   where they'll give you great advice
[00:40:40.940 --> 00:40:42.820]   for how to hire the best people.
[00:40:42.820 --> 00:40:45.100]   Try Zippercruter today with a free 4D trial
[00:40:45.100 --> 00:40:47.700]   and get your perfect candidate before they go to somebody else.
[00:40:47.700 --> 00:40:50.940]   Just go to zippercruter.com/twig.
[00:40:50.940 --> 00:40:54.700]   That's zippercruter.com/twig.
[00:40:54.700 --> 00:40:57.180]   All right, well, Jeff Jarvis,
[00:40:57.180 --> 00:40:59.820]   can ad targeting systems be sexist?
[00:40:59.820 --> 00:41:00.660]   What do you think?
[00:41:00.660 --> 00:41:05.660]   Is this story on MIT technology review by Tom Simonite?
[00:41:05.660 --> 00:41:09.500]   How do you would say Tom's name?
[00:41:09.500 --> 00:41:11.060]   - I have no idea. - I have no idea.
[00:41:11.060 --> 00:41:13.980]   This is a problem with the print journalist.
[00:41:13.980 --> 00:41:16.140]   - All names should come with links to pronounceers.
[00:41:16.140 --> 00:41:17.140]   - Thank you, yes.
[00:41:17.140 --> 00:41:17.980]   - I do not.
[00:41:17.980 --> 00:41:18.820]   - Maybe not.
[00:41:18.820 --> 00:41:20.940]   Is this sexist? Do you think this studies
[00:41:20.940 --> 00:41:22.940]   onto something? What do you think?
[00:41:22.940 --> 00:41:26.940]   - It's all about every data set has its prejudices
[00:41:26.940 --> 00:41:29.020]   and every algorithm has its prejudices.
[00:41:29.020 --> 00:41:31.860]   Yes, built in and you can only know them when it comes out
[00:41:31.860 --> 00:41:33.380]   and to treat this as if, oh my God,
[00:41:33.380 --> 00:41:35.460]   Google was evil and racist.
[00:41:35.460 --> 00:41:37.660]   No, the algorithm is evil and racist.
[00:41:37.660 --> 00:41:39.580]   No, it's the same problem we had last week.
[00:41:39.580 --> 00:41:41.900]   That horrible, horrible, horrible episode
[00:41:41.900 --> 00:41:45.620]   of the photo algorithm identifying
[00:41:45.620 --> 00:41:48.900]   African Americans not as people.
[00:41:48.900 --> 00:41:52.620]   Awful and immediately seen as terrible and immediately fixed.
[00:41:52.620 --> 00:41:57.300]   And this is what these big technologies will do.
[00:41:57.300 --> 00:42:02.140]   They will reveal themselves, reveal the prejudices
[00:42:02.140 --> 00:42:04.260]   of what was put into them.
[00:42:04.260 --> 00:42:08.020]   Because of that, the same argument made I would argue
[00:42:08.020 --> 00:42:12.780]   about Facebook and the echo chamber is,
[00:42:12.780 --> 00:42:15.300]   well, if all you pay attention to is liberals,
[00:42:15.300 --> 00:42:16.940]   then that's all Facebook is gonna give you a liberal
[00:42:16.940 --> 00:42:18.140]   because that's the data it has in
[00:42:18.140 --> 00:42:19.900]   and that's the formula it has.
[00:42:19.900 --> 00:42:22.900]   So there's something off not so much with the company,
[00:42:22.900 --> 00:42:27.620]   but with the data or the formula and this reveals that.
[00:42:27.620 --> 00:42:30.180]   - But I think there is a larger point,
[00:42:30.180 --> 00:42:32.980]   which Stacey Higginbotham tried to make in a post
[00:42:32.980 --> 00:42:36.420]   at Fortune, her argument was that part of the way,
[00:42:36.420 --> 00:42:38.300]   part of the reason this happens and part of the way
[00:42:38.300 --> 00:42:42.580]   that you can correct it is to have a more diverse workforce
[00:42:42.580 --> 00:42:44.060]   working on those algorithms.
[00:42:44.060 --> 00:42:45.020]   - Yes. - Because presumably,
[00:42:45.020 --> 00:42:46.500]   those things will occur to them.
[00:42:46.500 --> 00:42:47.820]   - Well, as we said last week, Matthew.
[00:42:47.820 --> 00:42:49.460]   - Right now it occurred all the white guys.
[00:42:49.460 --> 00:42:52.660]   - And we said last week, if you had more diverse people,
[00:42:52.660 --> 00:42:57.260]   dog fooding the applications, then that would expose
[00:42:57.260 --> 00:42:59.820]   these things sooner and help you fix and make better
[00:42:59.820 --> 00:43:01.860]   algorithms and get better data.
[00:43:01.860 --> 00:43:04.580]   But to throw our arms up every time and say,
[00:43:04.580 --> 00:43:06.020]   oh my God, what are we trying to do here?
[00:43:06.020 --> 00:43:07.300]   Say, well, the technology's broken.
[00:43:07.300 --> 00:43:08.980]   No, it's not perfect.
[00:43:08.980 --> 00:43:10.020]   It's never gonna be perfect.
[00:43:10.020 --> 00:43:11.460]   It needs to be fixed.
[00:43:11.460 --> 00:43:13.860]   And Stacey and you were quite right about that,
[00:43:13.860 --> 00:43:17.620]   so the more perspectives you have using it,
[00:43:17.620 --> 00:43:18.500]   the better off you're gonna be.
[00:43:18.500 --> 00:43:21.700]   But at some point, at some point it's gonna get out there
[00:43:21.700 --> 00:43:23.300]   and it's gonna discover something else
[00:43:23.300 --> 00:43:25.700]   and we have to be able to just basically say,
[00:43:25.700 --> 00:43:28.420]   this is wrong, what's wrong should be fixed immediately.
[00:43:28.420 --> 00:43:30.420]   If you don't fix it, then we'll get mad.
[00:43:30.420 --> 00:43:33.580]   But if you fix it, then we're collaboratively here
[00:43:33.580 --> 00:43:35.500]   trying to make all these things better.
[00:43:35.500 --> 00:43:38.340]   - And actually this is very similar to the dog heads
[00:43:38.340 --> 00:43:41.060]   in the spaghetti, if I can make that analogy.
[00:43:41.060 --> 00:43:42.620]   - I wish I could forget that.
[00:43:43.700 --> 00:43:45.460]   Matthew's not gonna let you.
[00:43:45.460 --> 00:43:47.380]   - That'll be in your dreams later.
[00:43:47.380 --> 00:43:50.700]   Because we're Google, everyone Facebook,
[00:43:50.700 --> 00:43:54.180]   everyone is trying to teach their algorithms to think
[00:43:54.180 --> 00:43:55.780]   and to recognize patterns.
[00:43:55.780 --> 00:43:59.420]   And so if your algorithm is trained badly,
[00:43:59.420 --> 00:44:02.020]   it's going to recognize faces where there aren't faces.
[00:44:02.020 --> 00:44:06.180]   Or it's gonna imagine that certain things are other things.
[00:44:06.180 --> 00:44:08.900]   And you have to train it better
[00:44:08.900 --> 00:44:10.500]   and one of the ways you can do that, as you said,
[00:44:10.500 --> 00:44:14.060]   is to have more input, to have more diverse input
[00:44:14.060 --> 00:44:16.300]   so that your algorithm gets smarter faster.
[00:44:16.300 --> 00:44:21.300]   - Well, we talked last week about the black users
[00:44:21.300 --> 00:44:24.620]   of Google Photos being identified as gorillas,
[00:44:24.620 --> 00:44:26.380]   which Jeff, you mentioned.
[00:44:26.380 --> 00:44:30.860]   And our solution was you can't just let the algorithms be
[00:44:30.860 --> 00:44:32.900]   without, you have to have human intervention.
[00:44:32.900 --> 00:44:35.140]   And here's another case, according to Stacey,
[00:44:35.140 --> 00:44:38.260]   human intervention, again, is the solution here.
[00:44:38.260 --> 00:44:39.460]   Theoretically, although it's,
[00:44:39.460 --> 00:44:41.700]   I think it's less of a solution in this case,
[00:44:41.700 --> 00:44:44.220]   simply because the problem with gender discrimination
[00:44:44.220 --> 00:44:48.060]   in the workplace is a deep societal problem.
[00:44:48.060 --> 00:44:53.060]   So you'll even find many women who have subconscious
[00:44:53.060 --> 00:44:56.340]   expectations that women are going to be offered,
[00:44:56.340 --> 00:44:59.180]   lower pay, et cetera, even though they don't want it,
[00:44:59.180 --> 00:45:01.300]   even though consciously they don't say it,
[00:45:01.300 --> 00:45:04.500]   it's such a deep problem in our society.
[00:45:04.500 --> 00:45:06.300]   That's why, inevitably it's showing up
[00:45:06.300 --> 00:45:08.260]   in these algorithms and so on.
[00:45:08.260 --> 00:45:10.900]   But, you know, so to a certain extent,
[00:45:10.900 --> 00:45:15.580]   you need something, you need human intervention
[00:45:15.580 --> 00:45:17.900]   and you also need analytical tools
[00:45:17.900 --> 00:45:20.180]   that will surface these things like this report,
[00:45:20.180 --> 00:45:23.140]   like this study, this tool that surface this.
[00:45:23.140 --> 00:45:25.020]   So we can examine it with that in mind
[00:45:25.020 --> 00:45:27.580]   because again, biases are cultural.
[00:45:27.580 --> 00:45:32.580]   - But also, I think, you can use the bias for good.
[00:45:32.580 --> 00:45:35.540]   You can say there's a problem in society.
[00:45:35.540 --> 00:45:37.420]   For example, try this out.
[00:45:37.420 --> 00:45:39.860]   There are, we would agree there are not enough women
[00:45:39.860 --> 00:45:41.300]   in technology.
[00:45:41.300 --> 00:45:44.620]   And it is bad for the technology industry.
[00:45:44.620 --> 00:45:49.620]   You could choose to try to skew such a service
[00:45:49.620 --> 00:45:53.860]   to try to push more technology jobs first to women.
[00:45:53.860 --> 00:45:55.940]   You could use the bias to a different reason.
[00:45:55.940 --> 00:45:57.220]   You could choose to do that.
[00:45:57.220 --> 00:45:59.340]   But you have to be aware of what that bias is
[00:45:59.340 --> 00:46:00.180]   and how you're doing.
[00:46:00.180 --> 00:46:02.020]   Now, the next problem is, oh, they're manipulating the market.
[00:46:02.020 --> 00:46:03.340]   They're manipulating our minds,
[00:46:03.340 --> 00:46:05.740]   they're manipulating anything on Facebook, whatever.
[00:46:05.740 --> 00:46:08.660]   That's where some level of transparency comes in.
[00:46:08.660 --> 00:46:09.620]   What's happened in this case,
[00:46:09.620 --> 00:46:11.860]   I think it was not even transparent to Google.
[00:46:11.860 --> 00:46:14.180]   It was, oh crap, what happened?
[00:46:14.180 --> 00:46:17.780]   And those things need to be tested.
[00:46:17.780 --> 00:46:18.980]   But there are moments where you could say,
[00:46:18.980 --> 00:46:22.580]   let us purposely overcompensate for something that's wrong.
[00:46:22.580 --> 00:46:24.420]   - Yeah, I'm glad you mentioned transparency
[00:46:24.420 --> 00:46:28.980]   because I remember a study where Facebook effectively showed
[00:46:28.980 --> 00:46:33.140]   that our researchers showed that by manipulating the newsfeed
[00:46:33.140 --> 00:46:36.100]   in certain ways, they could encourage more people
[00:46:36.100 --> 00:46:39.780]   to go and vote, which obviously seems like a great thing.
[00:46:39.780 --> 00:46:42.900]   So one thing you could take away from that is,
[00:46:42.900 --> 00:46:46.460]   Facebook can use its algorithm for good.
[00:46:46.460 --> 00:46:48.340]   The other thing you could take away from that is,
[00:46:48.340 --> 00:46:50.340]   holy cow, Facebook can manipulate the newsfeed
[00:46:50.340 --> 00:46:52.460]   and make people do things.
[00:46:52.460 --> 00:46:54.460]   Not always a good thing.
[00:46:54.460 --> 00:46:57.380]   - And there again, it's the devils and the details
[00:46:57.380 --> 00:47:00.900]   and their unintended consequences for everything.
[00:47:00.900 --> 00:47:04.460]   Maybe it's not a good idea for everyone to vote.
[00:47:04.460 --> 00:47:07.380]   And what I mean by that is that this idea
[00:47:07.380 --> 00:47:11.260]   that you would get more people to vote sounds on its face
[00:47:11.260 --> 00:47:12.860]   like an unalloyed good.
[00:47:12.860 --> 00:47:14.940]   So essentially the people who are most interested
[00:47:14.940 --> 00:47:17.180]   in politics, the most engaged in the process,
[00:47:17.180 --> 00:47:19.580]   who read the news, who follow the,
[00:47:19.580 --> 00:47:21.100]   those are the folks who are gonna vote.
[00:47:21.100 --> 00:47:23.860]   And it's people who don't care, who watch reality shows,
[00:47:23.860 --> 00:47:26.660]   don't follow the news, don't care what's going on.
[00:47:26.660 --> 00:47:28.540]   Those are the people you're gonna go get to vote.
[00:47:28.540 --> 00:47:30.620]   It can't sell out your vote.
[00:47:30.620 --> 00:47:32.540]   I mean, I think that yes, it's good
[00:47:32.540 --> 00:47:34.620]   if everybody's informed and everybody votes,
[00:47:34.620 --> 00:47:37.340]   but I don't think it's better that nobody's informed
[00:47:37.340 --> 00:47:38.860]   and everybody votes.
[00:47:38.860 --> 00:47:39.700]   See what I'm saying?
[00:47:39.700 --> 00:47:41.860]   So they're unintended consequences to all this.
[00:47:41.860 --> 00:47:44.700]   - But, you know, let's not forget that journalism tries
[00:47:44.700 --> 00:47:45.940]   to manipulate people too.
[00:47:45.940 --> 00:47:46.780]   - Yes, yes.
[00:47:46.780 --> 00:47:48.060]   - Try to manipulate them to go vote.
[00:47:48.060 --> 00:47:48.900]   - Only for good.
[00:47:48.900 --> 00:47:49.940]   - Mark Heder is trying to manipulate.
[00:47:49.940 --> 00:47:51.580]   Minster is trying to manipulate.
[00:47:51.580 --> 00:47:53.500]   Manipulation's all around us.
[00:47:53.500 --> 00:47:56.460]   And sometimes you can see it and sometimes you can't.
[00:47:56.460 --> 00:47:58.300]   - Okay, so I wanna manipulate this panel
[00:47:58.300 --> 00:48:01.620]   into a conversation that's highly controversial.
[00:48:01.620 --> 00:48:03.540]   And nobody's having this conversation
[00:48:03.540 --> 00:48:05.060]   and I think it really needs to be had.
[00:48:05.060 --> 00:48:07.740]   We sort of kind of started to talk about it on Tech News today
[00:48:07.740 --> 00:48:10.140]   and we didn't really have the chance to do that.
[00:48:10.140 --> 00:48:11.060]   And here's the problem.
[00:48:11.060 --> 00:48:13.940]   So we know that, you know, Jeff, you mentioned,
[00:48:13.940 --> 00:48:14.980]   and this is not on the rundown.
[00:48:14.980 --> 00:48:15.820]   I apologize for that.
[00:48:15.820 --> 00:48:17.820]   But Jeff, you know that you mentioned
[00:48:17.820 --> 00:48:20.220]   that there aren't enough women in technology
[00:48:20.220 --> 00:48:21.540]   in Silicon Valley.
[00:48:21.540 --> 00:48:25.540]   We also know that the diversity numbers also poorly reflect
[00:48:25.540 --> 00:48:28.220]   on the diversity, the ethnic and racial diversity
[00:48:28.220 --> 00:48:29.860]   within the United States.
[00:48:29.860 --> 00:48:33.700]   African Americans are wildly underrepresented.
[00:48:33.700 --> 00:48:36.860]   Hispanic Americans are wildly underrepresented.
[00:48:36.860 --> 00:48:39.420]   Women are wildly unrepresented.
[00:48:39.420 --> 00:48:43.300]   Here's the part of the story that hardly anybody's reporting.
[00:48:43.300 --> 00:48:47.340]   White Americans are somewhat underrepresented.
[00:48:47.340 --> 00:48:52.340]   In other words, the percentage of white employees
[00:48:52.340 --> 00:48:56.300]   of Facebook, for example, is significantly lower
[00:48:56.300 --> 00:48:58.980]   than the percentage in the United States.
[00:48:58.980 --> 00:49:01.860]   And what's really happening is that Asians,
[00:49:01.860 --> 00:49:05.940]   mostly from China and India, are overrepresented
[00:49:05.940 --> 00:49:07.700]   in Silicon Valley.
[00:49:07.700 --> 00:49:10.780]   But that's where you can find women engineers
[00:49:10.780 --> 00:49:12.340]   from China and India.
[00:49:12.340 --> 00:49:18.220]   So you get super talented engineers from Asia
[00:49:18.220 --> 00:49:24.940]   and they're not anywhere near as bad as the United States is
[00:49:24.940 --> 00:49:28.220]   in terms of preparing women to become engineers
[00:49:28.220 --> 00:49:30.220]   and be in other technical fields.
[00:49:30.220 --> 00:49:35.220]   So pulling people from Asia is a great way
[00:49:35.220 --> 00:49:38.900]   to boost the number of women.
[00:49:38.900 --> 00:49:43.900]   But it also contributes to the fact that Chinese
[00:49:43.900 --> 00:49:49.260]   and Indian and Asian employees are so overrepresented
[00:49:49.260 --> 00:49:51.820]   as a function of the population in general.
[00:49:51.820 --> 00:49:54.900]   Of course, Silicon Valley has its own demographics
[00:49:54.900 --> 00:49:59.580]   and many of those are informed by immigration,
[00:49:59.580 --> 00:50:01.420]   H1B visas and all the rest.
[00:50:01.420 --> 00:50:05.860]   I guess the conversation that needs to be had here is,
[00:50:05.860 --> 00:50:10.460]   to what extent is our Silicon Valley technology companies
[00:50:10.460 --> 00:50:14.980]   masking an even greater problem of women in technology
[00:50:14.980 --> 00:50:17.820]   by pulling so many women from Asia
[00:50:17.820 --> 00:50:22.820]   while simultaneously skewing the racial and ethnic numbers
[00:50:24.260 --> 00:50:27.740]   away from wildly underrepresented minorities?
[00:50:27.740 --> 00:50:31.260]   Matthew Ingram, what do you got?
[00:50:31.260 --> 00:50:32.100]   (laughing)
[00:50:32.100 --> 00:50:35.460]   No, see, nobody wants to have this conversation.
[00:50:35.460 --> 00:50:37.460]   - That is not a softball by any means.
[00:50:37.460 --> 00:50:41.100]   I mean, I think I see your point.
[00:50:41.100 --> 00:50:45.380]   I see if Facebook and Google and other large tech companies
[00:50:45.380 --> 00:50:49.700]   are kind of beefing up their number of female employees
[00:50:49.700 --> 00:50:52.900]   by getting female engineers from Asia and India,
[00:50:52.900 --> 00:50:56.740]   then that doesn't help the US solve its problem,
[00:50:56.740 --> 00:51:00.300]   which is how to get more girls and women interested in science
[00:51:00.300 --> 00:51:01.940]   and interested in technical fields.
[00:51:01.940 --> 00:51:03.540]   - Well, actually, if I can interrupt you
[00:51:03.540 --> 00:51:05.540]   to a certain extent it does.
[00:51:05.540 --> 00:51:08.260]   My son works for a Silicon Valley company
[00:51:08.260 --> 00:51:11.580]   and they've got a senior woman engineer in the company
[00:51:11.580 --> 00:51:14.700]   and she spends a lot of time going around to elementary schools
[00:51:14.700 --> 00:51:16.860]   and middle schools and stuff, talking to girls
[00:51:16.860 --> 00:51:19.140]   and telling them what a great career it is
[00:51:19.140 --> 00:51:20.500]   in Silicon Valley and to do engineering.
[00:51:20.500 --> 00:51:23.540]   So there's some benefit in terms of role models and so on,
[00:51:23.540 --> 00:51:25.140]   but I'm sorry, I interrupted you.
[00:51:25.140 --> 00:51:28.180]   - Yeah, I just think, I mean, what you're talking about
[00:51:28.180 --> 00:51:33.180]   is a huge societal problem that has been a problem
[00:51:33.180 --> 00:51:36.220]   for hundreds of years.
[00:51:36.220 --> 00:51:39.100]   I mean, it's not, obviously, technical jobs
[00:51:39.100 --> 00:51:42.500]   haven't been around that long, but those issues
[00:51:42.500 --> 00:51:45.580]   are ones that society has been struggling with
[00:51:45.580 --> 00:51:46.420]   for a long time.
[00:51:46.420 --> 00:51:49.340]   They're not kinds of things that you can change with,
[00:51:50.340 --> 00:51:55.340]   a visa or a tax break or it takes generations
[00:51:55.340 --> 00:51:57.580]   before those things are solved.
[00:51:57.580 --> 00:52:01.140]   I wouldn't claim to understand how those things happen
[00:52:01.140 --> 00:52:02.900]   or how to make them happen faster.
[00:52:02.900 --> 00:52:07.340]   - Yeah, Geoff, do you have any thoughts on this?
[00:52:07.340 --> 00:52:12.340]   - No, I think that it's a mistake to set the goal
[00:52:12.340 --> 00:52:16.780]   as exactly reflecting society in one plane.
[00:52:16.780 --> 00:52:19.700]   - Yeah, there are many, many definitions of diversity
[00:52:19.700 --> 00:52:23.580]   around what a company or a government or a school needs.
[00:52:23.580 --> 00:52:28.140]   And we pride ourselves greatly at CUNY on being diverse
[00:52:28.140 --> 00:52:31.100]   and we try to define that many ways
[00:52:31.100 --> 00:52:34.420]   and that includes things like economic opportunity,
[00:52:34.420 --> 00:52:35.780]   economic discrimination.
[00:52:35.780 --> 00:52:37.380]   It includes international voices.
[00:52:37.380 --> 00:52:42.380]   It includes sexual preference right now.
[00:52:42.380 --> 00:52:46.300]   If you were a institution that had no,
[00:52:46.300 --> 00:52:47.780]   for whatever reason, if you could have,
[00:52:47.780 --> 00:52:48.780]   no I think this is possible,
[00:52:48.780 --> 00:52:53.700]   but if you had no voices of LGBT,
[00:52:53.700 --> 00:52:55.140]   you would have missed out on what was happening
[00:52:55.140 --> 00:52:58.380]   in society at the last 10 years or so.
[00:52:58.380 --> 00:53:01.140]   You would have been a dumber for it, even if you're,
[00:53:01.140 --> 00:53:02.820]   let's say, look at all the marketing companies
[00:53:02.820 --> 00:53:04.540]   that went crazy wonderfully, I think,
[00:53:04.540 --> 00:53:06.060]   after the Supreme Court decision,
[00:53:06.060 --> 00:53:08.580]   that had to come from a diverse viewpoints
[00:53:08.580 --> 00:53:10.980]   within the company saying this matters.
[00:53:10.980 --> 00:53:14.060]   So diversity has many, many, many definitions.
[00:53:14.060 --> 00:53:16.540]   And I think that rather than saying that,
[00:53:16.540 --> 00:53:17.860]   I mean, I don't think it's a,
[00:53:17.860 --> 00:53:20.980]   it's not a problem that whites are underrepresented per se
[00:53:20.980 --> 00:53:23.380]   or that Asians are overrepresented.
[00:53:23.380 --> 00:53:25.900]   I think that's the wrong way to go.
[00:53:25.900 --> 00:53:27.780]   I think that the right way to go is to say,
[00:53:27.780 --> 00:53:33.700]   what perspectives don't we have that are gonna be useful?
[00:53:33.700 --> 00:53:36.900]   And then to try to figure out how to build that.
[00:53:36.900 --> 00:53:39.540]   - And how can you encourage as much diversity as possible?
[00:53:39.540 --> 00:53:43.500]   Not kind of mathematically, how can we represent,
[00:53:43.500 --> 00:53:47.060]   you know, a sort of mythical society or represent,
[00:53:47.060 --> 00:53:50.620]   you know, or reflect the breakdown within the US,
[00:53:50.620 --> 00:53:54.260]   but how can you have as many diverse voices as possible
[00:53:54.260 --> 00:53:56.500]   in your company in different places?
[00:53:56.500 --> 00:54:01.020]   You know, not, I mean, to say all Asians are the same
[00:54:01.020 --> 00:54:02.620]   or Asian women engineers are the same
[00:54:02.620 --> 00:54:05.660]   or you've got, you should have as many different voices
[00:54:05.660 --> 00:54:09.500]   and perspectives represented as possible.
[00:54:09.500 --> 00:54:10.500]   - Yeah. - Yeah.
[00:54:10.500 --> 00:54:11.340]   - I'll go ahead. - Sorry,
[00:54:11.340 --> 00:54:13.220]   if I could just hop in, I mean, on that note,
[00:54:13.220 --> 00:54:15.860]   it's important to remember that these companies
[00:54:15.860 --> 00:54:17.580]   that we're talking about, the apples and Facebooks
[00:54:17.580 --> 00:54:19.380]   and Googles of the world, I mean,
[00:54:19.380 --> 00:54:23.100]   they're building products for a global audience now too, right?
[00:54:23.100 --> 00:54:27.300]   So I mean, the fact that they're bringing in engineers
[00:54:27.300 --> 00:54:30.980]   or employees from Asia or Europe or South America,
[00:54:30.980 --> 00:54:34.700]   I mean, that's obviously very important to building products
[00:54:34.700 --> 00:54:37.140]   that can connect with those communities as well.
[00:54:37.140 --> 00:54:40.500]   So this idea of like, okay, well, it doesn't reflect
[00:54:40.500 --> 00:54:44.980]   the demographic breakdown in the United States is really,
[00:54:44.980 --> 00:54:48.340]   that doesn't necessarily matter in the sense of
[00:54:48.340 --> 00:54:51.860]   their building things for the majority of their users
[00:54:51.860 --> 00:54:53.300]   are outside the United States.
[00:54:53.300 --> 00:54:58.300]   And so maybe this is actually a way to improve their products
[00:54:58.300 --> 00:55:00.980]   in other parts of the world that we're kind of not thinking
[00:55:00.980 --> 00:55:03.580]   of because we're in Silicon that we're here in the United States
[00:55:03.580 --> 00:55:06.060]   and really kind of thinking about ourselves.
[00:55:06.060 --> 00:55:08.780]   - I think that, I think the, if I'm being honest,
[00:55:08.780 --> 00:55:12.580]   the absolute truth about this whole technology diversity issue
[00:55:12.580 --> 00:55:17.020]   is the fact that culturally children are told,
[00:55:17.020 --> 00:55:19.860]   these are the kinds of people who are interested in technology
[00:55:19.860 --> 00:55:21.300]   and these are the kinds of people
[00:55:21.300 --> 00:55:23.980]   who are not supposed to be interested in technology.
[00:55:23.980 --> 00:55:28.220]   It's the cultural messages that kids receive
[00:55:28.220 --> 00:55:30.820]   are so overwhelming, so overpowering.
[00:55:30.820 --> 00:55:34.700]   And I think it's toxic because each and every person
[00:55:34.700 --> 00:55:37.860]   has their own personality and their personality
[00:55:37.860 --> 00:55:40.860]   and their skills and their interests are going to direct,
[00:55:40.860 --> 00:55:42.860]   and we're derailing that to a certain extent,
[00:55:42.860 --> 00:55:45.220]   by telling kids who they're supposed to be
[00:55:45.220 --> 00:55:46.900]   when they're at a very young age, of course,
[00:55:46.900 --> 00:55:51.220]   this problem before the mid-80s barely existed
[00:55:51.220 --> 00:55:54.220]   because we didn't, at that point, get around
[00:55:54.220 --> 00:55:57.380]   to telling girls, for example, that tech was for boys.
[00:55:57.380 --> 00:56:00.460]   Like commercials and movies and TV shows
[00:56:00.460 --> 00:56:02.580]   started doing that in the '80s.
[00:56:02.580 --> 00:56:05.060]   And now we're suffering the effects of that.
[00:56:05.060 --> 00:56:08.220]   So I think that it's a tough problem,
[00:56:08.220 --> 00:56:09.740]   and I think journalism, to a certain extent,
[00:56:09.740 --> 00:56:12.220]   does have a role in this because, you know,
[00:56:12.220 --> 00:56:14.380]   and media, entertainment media,
[00:56:14.380 --> 00:56:16.340]   has an even bigger role, actually.
[00:56:16.340 --> 00:56:21.340]   And so I just think it's a vast societal problem.
[00:56:21.340 --> 00:56:25.500]   And I'll also say that there's an economic element to this.
[00:56:25.500 --> 00:56:30.500]   If you sort of rank the economies of the world
[00:56:30.500 --> 00:56:34.860]   by, you know, richest to poorest,
[00:56:34.860 --> 00:56:39.100]   you'll find a fairly strong correlation
[00:56:39.100 --> 00:56:42.220]   between the degree to which you have inclusion
[00:56:42.220 --> 00:56:47.220]   in the economy and wealth.
[00:56:47.220 --> 00:56:49.500]   The greater the inclusion, the greater the wealth,
[00:56:49.500 --> 00:56:51.500]   and when you get down to the super poor countries,
[00:56:51.500 --> 00:56:53.980]   you see that, you know, only a certain cast
[00:56:53.980 --> 00:56:56.660]   or only a certain group or only a certain gender
[00:56:56.660 --> 00:56:59.220]   or whatever is involved in all the activity.
[00:56:59.220 --> 00:57:01.380]   And so, you know, I think that there's also
[00:57:01.380 --> 00:57:05.940]   an economic problem, not just in the US,
[00:57:05.940 --> 00:57:07.300]   but around the world.
[00:57:07.300 --> 00:57:09.060]   And, you know, so anyway,
[00:57:09.060 --> 00:57:10.780]   so we're not gonna solve it here
[00:57:10.780 --> 00:57:13.420]   the more we talk about diversity, the diverse it gets.
[00:57:13.420 --> 00:57:15.580]   So let's, why don't we take a break?
[00:57:15.580 --> 00:57:17.540]   And when we come back, we're gonna talk about
[00:57:17.540 --> 00:57:21.180]   whether Blackberry should or should not
[00:57:21.180 --> 00:57:22.900]   get into the Android racket
[00:57:22.900 --> 00:57:25.140]   and what their next steps should be.
[00:57:25.140 --> 00:57:30.140]   So for now, though, I wanna talk about lifelong learning.
[00:57:30.140 --> 00:57:33.300]   I wanna talk about following your passion in life.
[00:57:33.300 --> 00:57:36.820]   I wanna talk about Linda.com, one of our sponsors today.
[00:57:36.820 --> 00:57:38.980]   You know, I believe, and I think everybody
[00:57:38.980 --> 00:57:41.260]   on this panel believes that you should never, ever stop
[00:57:41.260 --> 00:57:42.420]   learning.
[00:57:42.420 --> 00:57:44.820]   And, you know, we got a college
[00:57:44.820 --> 00:57:46.380]   or we got a high school go to college,
[00:57:46.380 --> 00:57:48.900]   and then we graduated some point,
[00:57:48.900 --> 00:57:51.540]   and lots of people just, they're done.
[00:57:51.540 --> 00:57:53.580]   They're done learning stuff,
[00:57:53.580 --> 00:57:55.020]   but that's no way to live life.
[00:57:55.020 --> 00:57:56.380]   Life is too short for that.
[00:57:56.380 --> 00:58:00.300]   Linda.com is for problem solvers, for curious people,
[00:58:00.300 --> 00:58:01.900]   for people who wanna make a difference,
[00:58:01.900 --> 00:58:03.740]   who wanna make things happen.
[00:58:03.740 --> 00:58:06.380]   You can learn, for example, how to develop an Android app.
[00:58:06.380 --> 00:58:07.940]   Even I could do that.
[00:58:07.940 --> 00:58:10.740]   You could master Google Analytics or Sharp HTML skills.
[00:58:10.740 --> 00:58:14.140]   Linda.com has everything you need to learn these things.
[00:58:14.140 --> 00:58:16.740]   One of the things that I've learned so much about
[00:58:16.740 --> 00:58:20.260]   from Linda.com is about photography.
[00:58:20.260 --> 00:58:23.660]   You know, I take vastly better pictures
[00:58:23.660 --> 00:58:26.380]   with my smartphone now than I used to
[00:58:26.380 --> 00:58:29.580]   with a very expensive DSLR a few years ago,
[00:58:29.580 --> 00:58:32.460]   because Linda.com showed me so much
[00:58:32.460 --> 00:58:34.860]   and taught me so much about taking the pictures
[00:58:34.860 --> 00:58:38.660]   about editing them, about selecting them,
[00:58:38.660 --> 00:58:39.740]   all aspects of that.
[00:58:39.740 --> 00:58:44.740]   For example, you know, there's a course
[00:58:44.740 --> 00:58:48.060]   of, it's a weekly series actually,
[00:58:48.060 --> 00:58:52.340]   called the DIY photographer with Joseph Lenashki.
[00:58:52.340 --> 00:58:53.580]   Now, if you know about photography,
[00:58:53.580 --> 00:58:56.260]   you know Joseph Lenashki, he's like an awesome dude,
[00:58:56.260 --> 00:58:59.820]   a great teacher, a brilliant photographer.
[00:58:59.820 --> 00:59:02.380]   He'll teach you things like how to make your own macro lens
[00:59:02.380 --> 00:59:03.460]   for a few bucks.
[00:59:03.460 --> 00:59:06.060]   Macro photography is awesome and it's even more awesome
[00:59:06.060 --> 00:59:07.140]   if you're making your own lens.
[00:59:07.140 --> 00:59:10.500]   There's so much to be learned from Joseph Lenashki.
[00:59:10.500 --> 00:59:14.660]   His course is just one of many, many that Linda.com offers.
[00:59:14.660 --> 00:59:16.780]   They have courses on Adobe's Creative Cloud updates
[00:59:16.780 --> 00:59:19.980]   for 2015, including Photoshop, Premiere Pro, Audition,
[00:59:19.980 --> 00:59:22.860]   After Effects, InDesign, and Illustrator.
[00:59:22.860 --> 00:59:25.380]   It's just a fantastic service.
[00:59:25.380 --> 00:59:27.260]   The learning never has to stop
[00:59:27.260 --> 00:59:30.100]   and it's so much more fun with Linda.com.
[00:59:30.100 --> 00:59:32.020]   To a certain extent, a lot of this coursework
[00:59:32.020 --> 00:59:34.340]   is a lean back experience.
[00:59:34.340 --> 00:59:38.700]   You can relax, have a cool beverage
[00:59:38.700 --> 00:59:42.740]   and learn a new skill, enhance your career,
[00:59:42.740 --> 00:59:45.700]   follow your passions, Linda.com has it all for you.
[00:59:45.700 --> 00:59:48.180]   Your Linda.com membership gives you unlimited access
[00:59:48.180 --> 00:59:51.860]   to training on hundreds of topics, all for one flat rate.
[00:59:51.860 --> 00:59:55.220]   That's right, one flat rate and you can learn all you can.
[00:59:55.220 --> 00:59:56.620]   Whether you're looking to become an expert,
[00:59:56.620 --> 00:59:57.860]   whether you're passionate about a hobby
[00:59:57.860 --> 00:59:59.420]   or just want to learn something new,
[00:59:59.420 --> 01:00:01.820]   want you to visit Linda.com/twig
[01:00:01.820 --> 01:00:03.940]   and sign up for your free 10 day trial.
[01:00:03.940 --> 01:00:05.300]   That's right, you can try it for 10 days.
[01:00:05.300 --> 01:00:09.940]   That's l-y-n-d-a.com/t-w-i-g
[01:00:09.940 --> 01:00:13.220]   and we thank Linda.com for their support
[01:00:13.220 --> 01:00:16.580]   and also for teaching me how to take pictures.
[01:00:16.580 --> 01:00:20.300]   All right, well, it looks like we've got some photos
[01:00:20.300 --> 01:00:24.180]   of what might be Blackberry's upcoming Android phone
[01:00:24.180 --> 01:00:26.420]   and it could be coming to AT&T,
[01:00:26.420 --> 01:00:28.420]   which brings up a big question.
[01:00:28.420 --> 01:00:31.100]   There are two big paths in the smartphone world
[01:00:31.100 --> 01:00:33.700]   that Blackberry could take.
[01:00:33.700 --> 01:00:36.380]   One of them is to say, "Okay, we're a software company.
[01:00:36.380 --> 01:00:39.900]   "We have security, we have enterprise readiness,
[01:00:39.900 --> 01:00:42.140]   "we have all these features and benefits.
[01:00:42.140 --> 01:00:43.980]   "Let's de-emphasize the hardware.
[01:00:43.980 --> 01:00:46.220]   "Our specialty was the physical keyboards
[01:00:46.220 --> 01:00:49.260]   "and that's over for the most part."
[01:00:49.260 --> 01:00:51.860]   Or the other approach is say, "No, we're a hardware company.
[01:00:51.860 --> 01:00:53.420]   "We have innovative hardware.
[01:00:53.420 --> 01:00:54.460]   "Everybody's on Android.
[01:00:54.460 --> 01:00:56.340]   "Why don't we become an Android vendor?"
[01:00:56.340 --> 01:00:59.460]   Kurt, what do you think?
[01:00:59.460 --> 01:01:01.260]   Which is the better course
[01:01:01.260 --> 01:01:03.900]   or should they even make that choice for Blackberry?
[01:01:03.900 --> 01:01:05.740]   Should they become an Android company
[01:01:05.740 --> 01:01:08.020]   or should they become a software company
[01:01:08.020 --> 01:01:09.900]   and do their own thing?
[01:01:09.900 --> 01:01:12.100]   - I'm of the mindset that they should become
[01:01:12.100 --> 01:01:14.780]   an Android company and this is a little bit outside
[01:01:14.780 --> 01:01:16.380]   of my realm of expertise.
[01:01:16.380 --> 01:01:17.660]   So take this with a grain of salt.
[01:01:17.660 --> 01:01:22.260]   But I look around and outside of the United States,
[01:01:22.260 --> 01:01:25.660]   I mean, it seems as if the majority of the world
[01:01:25.660 --> 01:01:29.180]   is on Android or maybe not obviously not the majority,
[01:01:29.180 --> 01:01:32.740]   but a huge portion of the world is using Android devices.
[01:01:32.740 --> 01:01:35.780]   And I think that it's an operating system
[01:01:35.780 --> 01:01:38.780]   that people are a familiar with and building for
[01:01:38.780 --> 01:01:40.700]   and thinking about long-term.
[01:01:40.700 --> 01:01:43.820]   And I think that it's smart for a company like Blackberry
[01:01:43.820 --> 01:01:47.420]   to simply associate themselves with something that seems
[01:01:47.420 --> 01:01:49.340]   like it's gonna be around for the long-term
[01:01:49.340 --> 01:01:51.540]   versus trying to get into some kind of a competition
[01:01:51.540 --> 01:01:54.420]   with both Google and Apple, which seems like,
[01:01:54.420 --> 01:01:56.540]   clearly hasn't worked out so far for them
[01:01:56.540 --> 01:01:59.140]   and it doesn't seem like it's worth the effort at this point.
[01:01:59.140 --> 01:02:01.820]   So, if you can build something smart
[01:02:01.820 --> 01:02:04.540]   that offers something unique to the Android audience,
[01:02:04.540 --> 01:02:06.460]   I think that sounds like a better move.
[01:02:06.460 --> 01:02:08.460]   - Yeah.
[01:02:08.460 --> 01:02:12.740]   I think it would have been a great move like five years ago.
[01:02:12.740 --> 01:02:15.780]   I think I'm not convinced it's gonna make a big difference
[01:02:15.780 --> 01:02:17.220]   to Blackberry right now.
[01:02:17.220 --> 01:02:19.820]   I mean, part of me feels like they should just
[01:02:19.820 --> 01:02:21.740]   forget about handsets completely,
[01:02:21.740 --> 01:02:24.020]   or at least de-emphasize them.
[01:02:24.020 --> 01:02:26.020]   - What's it like now, Matthew and Canada?
[01:02:26.020 --> 01:02:29.100]   - Sorry, what's it like now in Canada?
[01:02:29.100 --> 01:02:31.740]   I mean, down here, I made a friend of a friend yesterday
[01:02:31.740 --> 01:02:32.900]   who used to be Blackberry from the Vatican.
[01:02:32.900 --> 01:02:33.740]   I said, "Well, I don't see that.
[01:02:33.740 --> 01:02:34.580]   He's an haven't I, and he just,
[01:02:34.580 --> 01:02:38.860]   "I haven't seen Blackberry in anybody's hand in probably a year."
[01:02:38.860 --> 01:02:40.780]   - Doesn't the president still have one?
[01:02:40.780 --> 01:02:42.740]   - Yes, in Canada, the Blackberry.
[01:02:42.740 --> 01:02:44.260]   - I see them all the time.
[01:02:44.260 --> 01:02:45.100]   - Are they still doing?
[01:02:45.100 --> 01:02:46.820]   - I see them all the time.
[01:02:46.820 --> 01:02:47.900]   I still see them all the time.
[01:02:47.900 --> 01:02:50.140]   Lots of people use the passport.
[01:02:50.140 --> 01:02:51.700]   I make fun of them, of course,
[01:02:51.700 --> 01:02:54.180]   because I think it's a ridiculous device.
[01:02:54.180 --> 01:02:55.940]   But, they seem to like it.
[01:02:55.940 --> 01:02:57.380]   In most cases, they're using it
[01:02:57.380 --> 01:03:00.140]   because their company has a deal with Blackberry
[01:03:00.140 --> 01:03:03.020]   or the arm of government they work for
[01:03:03.020 --> 01:03:04.060]   has a deal with Blackberry.
[01:03:04.060 --> 01:03:05.580]   So, they're kind of stuck with them.
[01:03:05.580 --> 01:03:08.580]   - Is there kind of a Canadian loyalty to it?
[01:03:08.580 --> 01:03:09.580]   - There's a little bit of that,
[01:03:09.580 --> 01:03:11.460]   but the security aspects,
[01:03:11.460 --> 01:03:15.020]   they probably had long-term agreements with Blackberry.
[01:03:15.020 --> 01:03:17.460]   So, to some extent, they're stuck with them.
[01:03:17.460 --> 01:03:20.020]   To some extent, I think lots of people
[01:03:20.020 --> 01:03:22.020]   like the physical keyboard.
[01:03:22.020 --> 01:03:26.820]   So, it's not that there isn't a market there.
[01:03:26.820 --> 01:03:27.900]   I think there probably is.
[01:03:27.900 --> 01:03:29.980]   I'm not sure how big it is.
[01:03:29.980 --> 01:03:31.900]   So, could they do it?
[01:03:31.900 --> 01:03:32.740]   Sure.
[01:03:32.740 --> 01:03:34.980]   But, is it gonna make a big difference to the company?
[01:03:34.980 --> 01:03:36.140]   I'm not convinced.
[01:03:36.140 --> 01:03:38.540]   - Well, good luck to Blackberry.
[01:03:38.540 --> 01:03:39.780]   Let's talk about something else
[01:03:39.780 --> 01:03:41.500]   that I think is a really interesting development,
[01:03:41.500 --> 01:03:45.180]   which of course is the new war
[01:03:45.180 --> 01:03:48.060]   between Facebook and YouTube around video.
[01:03:48.060 --> 01:03:51.380]   Now, YouTube of course is the undisputed heavyweight champion
[01:03:51.380 --> 01:03:53.220]   when it comes to video, video advertising.
[01:03:53.220 --> 01:03:54.980]   It's a fantastic resource.
[01:03:54.980 --> 01:03:56.900]   Lots of people spend lots of hours there,
[01:03:56.900 --> 01:03:58.140]   a lot of eyeballs.
[01:03:58.140 --> 01:04:00.820]   And now Facebook is really, really getting into this
[01:04:00.820 --> 01:04:01.660]   Kurt Wagner.
[01:04:01.660 --> 01:04:04.660]   You wrote a piece late last week about this very thing.
[01:04:04.660 --> 01:04:07.660]   What is Facebook doing to compete with YouTube nowadays?
[01:04:08.620 --> 01:04:10.460]   - Well, Facebook's doing a handful of things,
[01:04:10.460 --> 01:04:13.060]   but the most recent is that they're essentially
[01:04:13.060 --> 01:04:16.940]   trying to create a stream of videos that are all
[01:04:16.940 --> 01:04:19.380]   kind of related around a specific topic.
[01:04:19.380 --> 01:04:21.500]   And there's something that adds against that stream
[01:04:21.500 --> 01:04:24.780]   and in order to lure people to build and excuse me,
[01:04:24.780 --> 01:04:27.540]   create and shoot videos,
[01:04:27.540 --> 01:04:28.820]   they're sharing some revenue.
[01:04:28.820 --> 01:04:31.460]   And that's something that YouTube isn't for a long time.
[01:04:31.460 --> 01:04:34.820]   I try and convince people to put stuff onto YouTube
[01:04:34.820 --> 01:04:36.980]   that can make some money on doing it.
[01:04:36.980 --> 01:04:38.380]   Facebook's never done that before.
[01:04:38.380 --> 01:04:40.180]   So now what they're trying to do is essentially
[01:04:40.180 --> 01:04:43.860]   lure these content creators to come to Facebook
[01:04:43.860 --> 01:04:46.460]   in exchange for taking a little bit of the money
[01:04:46.460 --> 01:04:49.860]   that they might get from the ad revenue in exchange.
[01:04:49.860 --> 01:04:52.380]   - I think that's really smart.
[01:04:52.380 --> 01:04:56.140]   I mean, I was talking to someone off the record,
[01:04:56.140 --> 01:04:59.220]   in the industry who was talking about Facebook and video
[01:04:59.220 --> 01:05:01.660]   and they were saying Facebook is a huge video platform,
[01:05:01.660 --> 01:05:04.540]   but they don't do revenue sharing.
[01:05:04.540 --> 01:05:06.900]   And so YouTube still has one up on them.
[01:05:06.900 --> 01:05:11.260]   This is obviously an attempt to go after that sort of weakness.
[01:05:11.260 --> 01:05:15.140]   - Of course, PewDiePie, who is a YouTube star,
[01:05:15.140 --> 01:05:19.460]   we just learned that he made $7 million last year.
[01:05:19.460 --> 01:05:22.660]   He's a 20, what is he, 24 year old guy
[01:05:22.660 --> 01:05:26.220]   who just does goofball, gaming oriented videos
[01:05:26.220 --> 01:05:27.060]   and other things.
[01:05:27.060 --> 01:05:30.980]   He just came out with a book, a somewhat ridiculous book,
[01:05:30.980 --> 01:05:32.540]   but he's got an incredible platform.
[01:05:32.540 --> 01:05:34.300]   - No, ridiculous. - It's ridiculous.
[01:05:34.300 --> 01:05:35.140]   It is.
[01:05:35.140 --> 01:05:38.220]   It's even more ridiculous if you can believe it
[01:05:38.220 --> 01:05:39.620]   than Snookies book.
[01:05:39.620 --> 01:05:40.300]   But anyway,
[01:05:40.300 --> 01:05:44.020]   - That's what he's kind of defensive about the money to
[01:05:44.020 --> 01:05:46.220]   apparently like he's kind of defensive.
[01:05:46.220 --> 01:05:48.140]   So I make $7 million, leave me alone.
[01:05:48.140 --> 01:05:49.180]   So I do read these ridiculous.
[01:05:49.180 --> 01:05:50.820]   - Which is too bad, he should be victorious.
[01:05:50.820 --> 01:05:51.660]   Shouldn't he?
[01:05:51.660 --> 01:05:53.660]   Look, I'm the future of television.
[01:05:53.660 --> 01:05:55.380]   I've built my own new empire.
[01:05:55.380 --> 01:05:56.820]   He should be proud of it.
[01:05:56.820 --> 01:05:59.820]   - He should be, but I think he appreciates the fact
[01:05:59.820 --> 01:06:01.580]   that he's kind of ridiculous.
[01:06:01.580 --> 01:06:02.420]   - Yeah, well, that's true.
[01:06:02.420 --> 01:06:04.540]   - You know, the name, the shtick.
[01:06:04.540 --> 01:06:07.100]   But is it any more ridiculous than there's a woman
[01:06:07.100 --> 01:06:10.220]   somewhere in Florida who makes $12 million a year,
[01:06:10.220 --> 01:06:12.580]   you know, unboxing kid's toys?
[01:06:12.580 --> 01:06:14.980]   So at least that much more ridiculous?
[01:06:14.980 --> 01:06:16.020]   Not really.
[01:06:16.020 --> 01:06:17.180]   - A lot. - Here's it.
[01:06:17.180 --> 01:06:18.100]   - Yeah.
[01:06:18.100 --> 01:06:21.660]   - Does this set up now, what entices me is,
[01:06:21.660 --> 01:06:24.580]   does this set up competition for YouTube and Facebook
[01:06:24.580 --> 01:06:26.820]   to be one up in each other on business terms
[01:06:26.820 --> 01:06:31.820]   and business benefits to popular video makers?
[01:06:32.900 --> 01:06:33.860]   - I think so.
[01:06:33.860 --> 01:06:36.780]   I think that Facebook's pitch is gonna be,
[01:06:36.780 --> 01:06:41.740]   hey, we have 1.4 billion people on our platform
[01:06:41.740 --> 01:06:43.020]   who are coming here every month.
[01:06:43.020 --> 01:06:46.380]   But more importantly, we can show your video
[01:06:46.380 --> 01:06:48.300]   to people who don't follow you, right?
[01:06:48.300 --> 01:06:50.740]   I mean, that's kind of the point of this suggested video
[01:06:50.740 --> 01:06:55.740]   feed is that your video could appear in front of fans
[01:06:55.740 --> 01:06:58.060]   that you don't even know you have.
[01:06:58.060 --> 01:07:00.100]   And I don't think that YouTube,
[01:07:00.100 --> 01:07:01.380]   I mean, YouTube kind of does that right.
[01:07:01.380 --> 01:07:03.140]   If you watch a video, you get to the end,
[01:07:03.140 --> 01:07:05.140]   it may have suggested videos.
[01:07:05.140 --> 01:07:06.980]   Of course, you're gonna stumble across things.
[01:07:06.980 --> 01:07:09.660]   And I've stumbled across videos all the time from people
[01:07:09.660 --> 01:07:11.140]   I'm not following on YouTube.
[01:07:11.140 --> 01:07:13.700]   I think Facebook's pitch kind of be like,
[01:07:13.700 --> 01:07:16.140]   our algorithm can do this for you.
[01:07:16.140 --> 01:07:18.660]   - Oh, but they have our attention for a stream.
[01:07:18.660 --> 01:07:19.500]   Right, they have our-- - Right, right.
[01:07:19.500 --> 01:07:21.180]   - They can put it-- - But it doesn't have that.
[01:07:21.180 --> 01:07:22.100]   - Facebook has that.
[01:07:22.100 --> 01:07:24.780]   - Well, it has that Google+ but it goes there except Mike.
[01:07:24.780 --> 01:07:27.060]   - Yeah, well, that's not my point.
[01:07:28.900 --> 01:07:31.580]   - But I wanted you to explain the other story you wrote, Kurt,
[01:07:31.580 --> 01:07:33.380]   which is next one down the rundown,
[01:07:33.380 --> 01:07:35.660]   is that Facebook is now separating out,
[01:07:35.660 --> 01:07:36.700]   and I think this is part of this,
[01:07:36.700 --> 01:07:38.420]   that if you're in its stream,
[01:07:38.420 --> 01:07:40.620]   and if certain actions happen there,
[01:07:40.620 --> 01:07:41.860]   there's one business benefit.
[01:07:41.860 --> 01:07:43.460]   If you have to click off to a site, it's another,
[01:07:43.460 --> 01:07:45.780]   can you explain what Facebook is just announced
[01:07:45.780 --> 01:07:46.620]   that are what they're doing?
[01:07:46.620 --> 01:07:48.540]   - Sure, the ad click stuff from today?
[01:07:48.540 --> 01:07:49.380]   - Yes.
[01:07:49.380 --> 01:07:51.780]   - So, yes, so this is interesting.
[01:07:51.780 --> 01:07:53.940]   It's not quite specific to,
[01:07:53.940 --> 01:07:56.620]   well, I guess it kind of falls under this category.
[01:07:56.620 --> 01:07:58.140]   And this is something that Twitter's done
[01:07:58.140 --> 01:07:58.980]   for a while, by the way,
[01:07:58.980 --> 01:08:01.100]   I wanna point that out for one of the very first times.
[01:08:01.100 --> 01:08:03.100]   It seems like Facebook's actually kind of taking a page
[01:08:03.100 --> 01:08:06.700]   for Twitter, whereas typically I feel like we see the opposite.
[01:08:06.700 --> 01:08:09.820]   But what is happening is Facebook kind of changed
[01:08:09.820 --> 01:08:12.940]   the term click and what it means for advertisers.
[01:08:12.940 --> 01:08:15.140]   So before, if I'm an advertiser,
[01:08:15.140 --> 01:08:17.860]   I'm paying for every time someone likes my ad,
[01:08:17.860 --> 01:08:20.700]   every time someone comments on my ad.
[01:08:20.700 --> 01:08:22.980]   If that's not all that helpful to me,
[01:08:22.980 --> 01:08:25.620]   I mean, maybe someone liking my ad, like, great,
[01:08:25.620 --> 01:08:28.500]   maybe I created something good that's reassuring.
[01:08:28.500 --> 01:08:30.900]   But really, I want them coming to my website
[01:08:30.900 --> 01:08:32.900]   or I want them installing the app
[01:08:32.900 --> 01:08:34.580]   that I've included in that ad.
[01:08:34.580 --> 01:08:36.580]   And so I wanna only pay for those things.
[01:08:36.580 --> 01:08:38.980]   And so that's Facebook's, they're going to add
[01:08:38.980 --> 01:08:39.820]   as there's a thing.
[01:08:39.820 --> 01:08:41.500]   We're not gonna charge you
[01:08:41.500 --> 01:08:44.940]   if someone clicks on the thing that you want them to click,
[01:08:44.940 --> 01:08:46.020]   if they go to your website
[01:08:46.020 --> 01:08:49.140]   or if they actually install the app that you're advertising,
[01:08:49.140 --> 01:08:51.660]   we're not gonna charge you for a Facebook post-like
[01:08:51.660 --> 01:08:52.980]   or a comment or a share.
[01:08:52.980 --> 01:08:53.820]   - Oh, wow.
[01:08:53.820 --> 01:08:56.220]   - Yeah, and I mean, there's an option.
[01:08:56.220 --> 01:08:57.620]   If I'm an advertiser, I can choose.
[01:08:57.620 --> 01:08:59.700]   If I still really want likes, you know,
[01:08:59.700 --> 01:09:01.660]   maybe I can pay for those, those will be cheaper.
[01:09:01.660 --> 01:09:03.380]   But if I just simply want app installs,
[01:09:03.380 --> 01:09:04.540]   I'm willing to pay a little bit more
[01:09:04.540 --> 01:09:06.420]   but only pay for those app installs
[01:09:06.420 --> 01:09:08.220]   versus the engagement metrics.
[01:09:08.220 --> 01:09:10.300]   - So it's really good to a Google model
[01:09:10.300 --> 01:09:13.500]   of the original Google of pay per click.
[01:09:13.500 --> 01:09:15.140]   But let me stay on this for a minute more.
[01:09:15.140 --> 01:09:15.980]   - Yeah.
[01:09:15.980 --> 01:09:18.980]   - So in instant articles, what Facebook is saying is
[01:09:18.980 --> 01:09:21.500]   to hell with clicking off to news sites,
[01:09:21.500 --> 01:09:24.700]   experience it here on Facebook with an ad,
[01:09:24.700 --> 01:09:27.580]   with a rev share, and it's better off for everybody.
[01:09:27.580 --> 01:09:28.700]   It's gonna be faster.
[01:09:28.700 --> 01:09:29.900]   It's gonna be more than the experience.
[01:09:29.900 --> 01:09:30.940]   You'll have more engagement.
[01:09:30.940 --> 01:09:33.300]   You'll have richer media in Facebook, blah, blah, blah, blah.
[01:09:33.300 --> 01:09:35.500]   I assume they're gonna say all of that same stuff
[01:09:35.500 --> 01:09:38.220]   they've said to these first nine news partners
[01:09:38.220 --> 01:09:40.380]   to advertisers as well.
[01:09:40.380 --> 01:09:42.060]   And there'll be at some point then,
[01:09:42.060 --> 01:09:46.300]   there's a pricing structure for attention in the stream
[01:09:46.300 --> 01:09:49.060]   versus clicks out of the stream.
[01:09:49.060 --> 01:09:50.620]   And I would think that Facebook is gonna say,
[01:09:50.620 --> 01:09:52.940]   we can really control the industry
[01:09:52.940 --> 01:09:55.780]   and we can charge premium for that.
[01:09:55.780 --> 01:09:57.620]   So just a lot of timing to me too,
[01:09:57.620 --> 01:10:00.220]   that they're valuing clicks higher now at a time
[01:10:00.220 --> 01:10:01.540]   when they're telling media,
[01:10:01.540 --> 01:10:02.460]   "Forget clicks guys."
[01:10:02.460 --> 01:10:04.060]   And they're working.
[01:10:04.060 --> 01:10:08.060]   - Yeah, they have a bit of a interesting back and forth
[01:10:08.060 --> 01:10:10.940]   right now because they also showed off some ad,
[01:10:10.940 --> 01:10:12.460]   these aren't ads that they're actually testing,
[01:10:12.460 --> 01:10:14.700]   but kind of a prototype of an ad
[01:10:14.700 --> 01:10:16.460]   that they could envision on Facebook.
[01:10:16.460 --> 01:10:19.860]   They showed that off at Con earlier,
[01:10:19.860 --> 01:10:23.180]   I guess last month to all the advertisers over in France.
[01:10:23.180 --> 01:10:25.660]   And those ads looked very much like
[01:10:25.660 --> 01:10:27.100]   the instant article type thing.
[01:10:27.100 --> 01:10:31.620]   So I would click on an ad and I'd kind of go to this
[01:10:31.620 --> 01:10:34.620]   almost like unique website that's still on Facebook,
[01:10:34.620 --> 01:10:37.300]   but for that marketer, for that advertiser.
[01:10:37.300 --> 01:10:39.060]   And so I'm looking at their products,
[01:10:39.060 --> 01:10:40.660]   I'm looking at these beautiful images
[01:10:40.660 --> 01:10:41.700]   and I can download an app,
[01:10:41.700 --> 01:10:43.820]   but I'm never leaving Facebook to do that.
[01:10:43.820 --> 01:10:45.260]   And that's kind of what they're saying
[01:10:45.260 --> 01:10:47.100]   they envision the future of ads to look like.
[01:10:47.100 --> 01:10:49.300]   And yet right now, here they are saying,
[01:10:49.300 --> 01:10:50.420]   well, no, we're only gonna charge you
[01:10:50.420 --> 01:10:53.100]   if someone clicks and actually goes off to your website.
[01:10:53.100 --> 01:10:55.740]   So clearly, I think that what they're trying to do
[01:10:55.740 --> 01:10:59.220]   is they're trying to appease advertisers.
[01:10:59.220 --> 01:11:00.660]   I'm sure that he came to them and said,
[01:11:00.660 --> 01:11:02.380]   "Hey, look at Twitter, we're only paying
[01:11:02.380 --> 01:11:03.420]   if they install our app.
[01:11:03.420 --> 01:11:05.900]   We're not paying for a retweet or a favorite.
[01:11:05.900 --> 01:11:08.260]   Like, why are we paying for that here at Facebook?
[01:11:08.260 --> 01:11:10.460]   I'm sure that, you know, when they get enough feedback,
[01:11:10.460 --> 01:11:11.300]   they were like, all right,
[01:11:11.300 --> 01:11:13.940]   well, let's make this an option for people right now.
[01:11:13.940 --> 01:11:15.300]   It'll be interesting to see in the future,
[01:11:15.300 --> 01:11:17.980]   maybe, you know, clicking instead of going to a website,
[01:11:17.980 --> 01:11:21.020]   maybe that click is gonna go to this kind of Facebook version
[01:11:21.020 --> 01:11:22.460]   of a brand's website.
[01:11:22.460 --> 01:11:24.260]   Maybe everything will just live on Facebook.
[01:11:24.260 --> 01:11:25.100]   I'm sure that they would love it.
[01:11:25.100 --> 01:11:26.420]   It's a trap.
[01:11:26.420 --> 01:11:27.260]   It's a trap.
[01:11:27.260 --> 01:11:28.100]   It's a trap, get out.
[01:11:28.100 --> 01:11:29.460]   Oh, yeah.
[01:11:29.460 --> 01:11:31.260]   This is negotiation.
[01:11:31.260 --> 01:11:34.820]   The biggest story here is the degree to which Twitter
[01:11:34.820 --> 01:11:37.740]   and Facebook are becoming to resemble each other.
[01:11:37.740 --> 01:11:41.100]   Mostly, this is Twitter copying Facebook.
[01:11:41.100 --> 01:11:44.340]   And this week, they added the ability for you
[01:11:44.340 --> 01:11:45.820]   to put in your birth date.
[01:11:45.820 --> 01:11:47.980]   You basically go to your profile, hit edit profile,
[01:11:47.980 --> 01:11:50.420]   now there's a new birth date feature
[01:11:50.420 --> 01:11:54.180]   and to incentivize you to add that,
[01:11:54.180 --> 01:11:57.740]   the privacy settings of the date are different from the year.
[01:11:57.740 --> 01:12:00.540]   So you can make your date public
[01:12:00.540 --> 01:12:02.260]   so that everybody wishes you a happy birthday,
[01:12:02.260 --> 01:12:03.940]   but you can keep you a year private
[01:12:03.940 --> 01:12:05.260]   so nobody knows how old you are.
[01:12:05.260 --> 01:12:06.820]   I'm such a Scrooge about this.
[01:12:06.820 --> 01:12:08.380]   I get all these, you know, people, oh,
[01:12:08.380 --> 01:12:10.380]   and somebody's birthday, they're all gonna say happy birthday.
[01:12:10.380 --> 01:12:12.460]   Then the person's gonna say, oh, thank you all.
[01:12:12.460 --> 01:12:15.140]   It happens once a year.
[01:12:15.140 --> 01:12:16.500]   Oh, geez.
[01:12:16.500 --> 01:12:18.580]   Well, it does, 'cause I'm, I went here, you know,
[01:12:18.580 --> 01:12:20.740]   I'm now sick of birthdays completely.
[01:12:20.740 --> 01:12:21.780]   I don't wanna have them.
[01:12:21.780 --> 01:12:22.780]   I don't wanna know them.
[01:12:22.780 --> 01:12:24.100]   I'm either with nor is your life 15th,
[01:12:24.100 --> 01:12:27.300]   which is next week and just act like it's any other day.
[01:12:27.300 --> 01:12:29.700]   - You know what happened there, Jeff?
[01:12:29.700 --> 01:12:31.460]   - Is subtle clap for your birthday.
[01:12:31.460 --> 01:12:33.620]   - When was that, Jeff?
[01:12:33.620 --> 01:12:34.460]   - Uh-uh.
[01:12:34.460 --> 01:12:36.420]   - Let everybody know.
[01:12:36.420 --> 01:12:38.020]   - Well, it's interesting, of course,
[01:12:38.020 --> 01:12:40.580]   I'm sorry, I get alert from Facebook.
[01:12:40.580 --> 01:12:42.820]   Obviously, people lie about their birthdays
[01:12:42.820 --> 01:12:44.540]   because I'm constantly getting alerts.
[01:12:44.540 --> 01:12:46.900]   So and so is having their 157th birthday,
[01:12:46.900 --> 01:12:49.460]   or so and so is having their 12th birthday.
[01:12:49.460 --> 01:12:51.260]   Yeah, so they're clearly putting other wrong dates
[01:12:51.260 --> 01:12:52.620]   'cause they don't want people to know.
[01:12:52.620 --> 01:12:54.500]   - The obvious benefit for Twitter, of course,
[01:12:54.500 --> 01:12:56.260]   is that when they know what, you know,
[01:12:56.260 --> 01:12:58.780]   even though you keep it private, what year you were born,
[01:12:58.780 --> 01:13:00.820]   that's the information they really want
[01:13:00.820 --> 01:13:03.260]   because they can, you know, if you're 18,
[01:13:03.260 --> 01:13:05.620]   that's the one set of advertising, you know,
[01:13:05.620 --> 01:13:06.460]   - Yep.
[01:13:06.460 --> 01:13:08.020]   - And if you're 80, that's a different one.
[01:13:08.020 --> 01:13:10.580]   And so, you know, it's, again,
[01:13:10.580 --> 01:13:11.780]   they're becoming more like Facebook.
[01:13:11.780 --> 01:13:14.580]   But again, this is a pretty rough metric.
[01:13:14.580 --> 01:13:15.900]   They want your location, you know,
[01:13:15.900 --> 01:13:19.860]   but with your location, with your age, with your gender,
[01:13:19.860 --> 01:13:21.060]   and a couple of other things.
[01:13:21.060 --> 01:13:25.580]   They can probably sell ads at a higher cost
[01:13:25.580 --> 01:13:27.740]   than they would otherwise sell as if, you know,
[01:13:27.740 --> 01:13:29.220]   we don't know who these people are.
[01:13:29.220 --> 01:13:31.740]   And it is a creeping phenomenon
[01:13:31.740 --> 01:13:35.660]   that they will increasingly harvest information from you.
[01:13:35.660 --> 01:13:40.460]   They will increasingly algorithmically filter
[01:13:40.460 --> 01:13:42.740]   your streams.
[01:13:42.740 --> 01:13:43.900]   They'll see what you click on,
[01:13:43.900 --> 01:13:45.780]   how much time you spend on this, that, and the other thing.
[01:13:45.780 --> 01:13:49.780]   In other words, they will increasingly become just like Facebook.
[01:13:49.780 --> 01:13:51.820]   - Mike, you got to understand.
[01:13:51.820 --> 01:13:54.020]   Sorry to, I think, you know,
[01:13:54.020 --> 01:13:55.860]   while the year is most important for,
[01:13:55.860 --> 01:13:57.380]   if you're going to do marketing, right?
[01:13:57.380 --> 01:13:59.660]   Hey, I want to hit all the 25 year olds,
[01:13:59.660 --> 01:14:02.140]   all the 35 adults on Twitter, there you go.
[01:14:02.140 --> 01:14:04.140]   I think the birthday is important too right now
[01:14:04.140 --> 01:14:06.740]   because, hey, maybe they don't know how old I am,
[01:14:06.740 --> 01:14:08.980]   but they know that my birthday is in December,
[01:14:08.980 --> 01:14:10.420]   and all of a sudden I'm going to get,
[01:14:10.420 --> 01:14:12.220]   hey, Kurt, happy birthday.
[01:14:12.220 --> 01:14:16.060]   Here's an ad for, you know, 10% off at some restaurant,
[01:14:16.060 --> 01:14:18.780]   go celebrate, or here, happy birthday, you know,
[01:14:18.780 --> 01:14:21.060]   or it's Kurt's birthday, let's go ahead and send him,
[01:14:21.060 --> 01:14:24.180]   send him a gift, you know what I mean?
[01:14:24.180 --> 01:14:26.020]   And you can buy that gift through Twitter
[01:14:26.020 --> 01:14:27.660]   with a discount or something like that.
[01:14:27.660 --> 01:14:31.100]   So I think the specific date could actually be just as valuable
[01:14:31.100 --> 01:14:33.340]   as the year, depending on how it's used.
[01:14:33.340 --> 01:14:34.900]   - And also if they know your relationship,
[01:14:34.900 --> 01:14:36.460]   if they know who your spouse is,
[01:14:36.460 --> 01:14:38.180]   they can say, hey, here's the perfect gift
[01:14:38.180 --> 01:14:41.060]   for that guy in your life who's a geek
[01:14:41.060 --> 01:14:43.060]   who spends a lot of time on social networks
[01:14:43.060 --> 01:14:46.220]   and who lives in San Francisco, intent, you know what I mean?
[01:14:46.220 --> 01:14:49.260]   - Remember when Facebook, you could give people
[01:14:49.260 --> 01:14:50.900]   present, virtual presence,
[01:14:50.900 --> 01:14:52.700]   so you could give them sort of a little,
[01:14:52.700 --> 01:14:54.900]   remember that, I bet Twitter adds that.
[01:14:54.900 --> 01:14:56.100]   - I bet they do.
[01:14:56.100 --> 01:14:58.620]   I saw that most recently on a long list of things
[01:14:58.620 --> 01:15:01.460]   that Facebook introduced that failed.
[01:15:01.460 --> 01:15:04.220]   There was literally like 50 things on the list.
[01:15:04.220 --> 01:15:05.860]   And I remember, I'm sorry, I remember that.
[01:15:05.860 --> 01:15:07.180]   I remember that, what happened to that?
[01:15:07.180 --> 01:15:09.900]   - I guess there was the most fun thing to me in
[01:15:09.900 --> 01:15:13.020]   - The Q and A last week, where somebody said,
[01:15:13.020 --> 01:15:14.380]   why did you event the poke?
[01:15:14.380 --> 01:15:17.220]   And Zux had to, it seemed like a good idea at the time.
[01:15:17.220 --> 01:15:18.980]   - It seemed like a good idea at the time.
[01:15:18.980 --> 01:15:20.460]   - Yeah, man.
[01:15:20.460 --> 01:15:24.620]   - And then he, did you single-handedly write the poke app
[01:15:24.620 --> 01:15:25.460]   or the poke?
[01:15:25.460 --> 01:15:26.620]   - He did.
[01:15:26.620 --> 01:15:27.460]   The feature, yeah.
[01:15:27.460 --> 01:15:28.820]   - Unbelievable, unbelievable.
[01:15:28.820 --> 01:15:32.020]   And that was like the hot thing for like three days.
[01:15:32.020 --> 01:15:33.340]   And then-- - I still get pokes.
[01:15:33.340 --> 01:15:34.660]   - Everyone's so hungry. - Yeah, it's a lot longer
[01:15:34.660 --> 01:15:36.700]   than three days. - Yeah, there's like
[01:15:36.700 --> 01:15:38.460]   three pokes. - Three pokes.
[01:15:38.460 --> 01:15:40.020]   - It went through, it got creepy,
[01:15:40.020 --> 01:15:43.180]   and then it people stopped doing it, and it got stupid,
[01:15:43.180 --> 01:15:46.380]   and then it became a thing again, and then it got creepy again.
[01:15:46.380 --> 01:15:47.500]   Went through a whole cycle.
[01:15:47.500 --> 01:15:48.980]   (laughing)
[01:15:48.980 --> 01:15:51.140]   - It was the cycle of-- - It was the cycle of creepy.
[01:15:51.140 --> 01:15:52.740]   - The cycle of-- - The cycle of--
[01:15:52.740 --> 01:15:53.580]   - Yeah.
[01:15:53.580 --> 01:15:54.900]   - All right, well-- - It's funny.
[01:15:54.900 --> 01:15:56.060]   - Sorry, did you wanna move on?
[01:15:56.060 --> 01:15:57.940]   - No, go ahead, go ahead, Matthew.
[01:15:57.940 --> 01:16:00.420]   - I was gonna say when we were talking about
[01:16:00.420 --> 01:16:03.980]   Twitter copying Facebook, I remember when instant articles
[01:16:03.980 --> 01:16:05.940]   came out, the first thing I thought of was
[01:16:05.940 --> 01:16:08.980]   I bet Twitter wishes they could do something like that.
[01:16:08.980 --> 01:16:11.740]   They tried to do, or they have done,
[01:16:11.740 --> 01:16:13.860]   they have cards, Twitter cards, where you can pull up
[01:16:13.860 --> 01:16:15.780]   an excerpt of a story and so on.
[01:16:15.780 --> 01:16:18.620]   They've tried to have partnerships with media companies,
[01:16:18.620 --> 01:16:20.820]   and I bet that's the type of thing
[01:16:20.820 --> 01:16:23.660]   they would really like to do with their platform.
[01:16:23.660 --> 01:16:25.180]   - Yeah, absolutely.
[01:16:25.180 --> 01:16:27.820]   Well, I think that Twitter has become so much
[01:16:27.820 --> 01:16:30.100]   like Facebook now that somebody, I think there's a big
[01:16:30.100 --> 01:16:34.580]   opportunity for somebody to come out with a messaging service
[01:16:34.580 --> 01:16:36.780]   where you just put in 140 characters or less,
[01:16:36.780 --> 01:16:39.260]   and you can text and do it through SMS,
[01:16:39.260 --> 01:16:40.860]   and just have it be super minimalist.
[01:16:40.860 --> 01:16:42.100]   - No photos, no videos.
[01:16:42.100 --> 01:16:42.940]   - No photos, right.
[01:16:42.940 --> 01:16:45.500]   - Well, the New York Post, the New York Post,
[01:16:45.500 --> 01:16:48.220]   not exactly the leader in business and technology
[01:16:48.220 --> 01:16:51.860]   reporting, but raw speculation today,
[01:16:51.860 --> 01:16:55.460]   because the Sun Valley confab of the rich and powerful
[01:16:55.460 --> 01:16:59.460]   is coming up, that, you know, if we see Zuck sitting down
[01:16:59.460 --> 01:17:00.300]   with--
[01:17:03.100 --> 01:17:05.300]   - What's the door seat? - Door seat.
[01:17:05.300 --> 01:17:06.900]   - Well, that being that Facebook's gonna buy Twitter?
[01:17:06.900 --> 01:17:09.740]   - Not gonna happen, not gonna happen.
[01:17:09.740 --> 01:17:11.900]   - It's amazing how many companies should have,
[01:17:11.900 --> 01:17:14.220]   would have, could have bought Twitter, Apple, Google,
[01:17:14.220 --> 01:17:17.220]   you name it, everybody was gonna buy Twitter.
[01:17:17.220 --> 01:17:20.340]   The rumors have been flying for years and years,
[01:17:20.340 --> 01:17:21.460]   and they just don't wanna sell.
[01:17:21.460 --> 01:17:23.180]   I think that, I hope they never do.
[01:17:23.180 --> 01:17:27.500]   I don't think they would be any good as a sort of a division
[01:17:27.500 --> 01:17:29.740]   of one of the other giant Silicon Valley companies.
[01:17:29.740 --> 01:17:33.540]   Well, why don't we move on to our tools and numbers
[01:17:33.540 --> 01:17:36.060]   and toys and stuff like that?
[01:17:36.060 --> 01:17:39.300]   Oh, the week, Jeff, do you have a number for us?
[01:17:39.300 --> 01:17:41.100]   - Yeah, hold on, oh, you beat me a little faster.
[01:17:41.100 --> 01:17:43.620]   - All right, let's talk about Facebook.
[01:17:43.620 --> 01:17:46.740]   I don't know if there's numbers are old,
[01:17:46.740 --> 01:17:49.100]   but it's just amazing to me every time.
[01:17:49.100 --> 01:17:52.780]   Need 'em doing analysis of commscore data,
[01:17:52.780 --> 01:17:56.260]   and Kurt, you tell me how old this is.
[01:17:56.260 --> 01:18:00.980]   1.44 billion users, an average of 20 plus minutes per day,
[01:18:00.980 --> 01:18:05.060]   accounting for 20% of all time online.
[01:18:05.060 --> 01:18:08.220]   However, wondering whether there's a bit of Facebook fatigue,
[01:18:08.220 --> 01:18:10.460]   which we keep on hearing perennially,
[01:18:10.460 --> 01:18:14.180]   that the US usage is going down.
[01:18:14.180 --> 01:18:16.420]   Nonetheless, it is just an amazing behemoth.
[01:18:16.420 --> 01:18:21.380]   And I have to say, partly inspired by you on Google+, Mike.
[01:18:21.380 --> 01:18:23.660]   I took a bunch of German publishers and editors
[01:18:23.660 --> 01:18:26.940]   around to hip, important media people in New York,
[01:18:26.940 --> 01:18:30.020]   and three times they said to the Germans,
[01:18:30.020 --> 01:18:31.900]   listen, you all may be on Twitter,
[01:18:31.900 --> 01:18:35.260]   but they, pointing out there, are on Facebook.
[01:18:35.260 --> 01:18:36.100]   - Yeah.
[01:18:36.100 --> 01:18:38.460]   - And so in the last six weeks or so,
[01:18:38.460 --> 01:18:40.060]   I've just made a much more concerted effort
[01:18:40.060 --> 01:18:41.260]   to live on Facebook.
[01:18:41.260 --> 01:18:44.420]   I still do Twitter, Google+, not as much, I'm sorry,
[01:18:44.420 --> 01:18:47.380]   but really trying to understand Facebook better.
[01:18:47.380 --> 01:18:52.220]   And it is a pretty amazing phenomenon.
[01:18:52.220 --> 01:18:55.020]   The more I use it, the better it gets,
[01:18:55.020 --> 01:18:56.300]   the more interaction I have.
[01:18:56.300 --> 01:18:59.500]   I have far more followers on Facebook than I do on Twitter.
[01:18:59.500 --> 01:19:02.900]   Granted, I don't know what the algorithm shows
[01:19:02.900 --> 01:19:04.020]   those followers on Facebook,
[01:19:04.020 --> 01:19:05.380]   but then I do know that on Twitter,
[01:19:05.380 --> 01:19:07.980]   I have a lot of people who never see the tweets,
[01:19:07.980 --> 01:19:09.620]   'cause following doesn't really matter as much.
[01:19:09.620 --> 01:19:12.700]   So it's the same problem, but it's pretty amazing.
[01:19:12.700 --> 01:19:15.580]   And it's easy to see how people can spend 20 minutes on it,
[01:19:15.580 --> 01:19:19.180]   and easy to see how the value of that stream
[01:19:19.180 --> 01:19:22.060]   beats the value of page-based media.
[01:19:22.060 --> 01:19:25.300]   So anyway, that's the number of the week.
[01:19:25.300 --> 01:19:27.060]   Kurt, are those numbers really old?
[01:19:27.060 --> 01:19:29.860]   - Bear us a say, I couldn't tell you,
[01:19:29.860 --> 01:19:32.780]   but what I will say is that I think those 20 minutes
[01:19:32.780 --> 01:19:37.060]   are spread out among multiple, multiple sessions.
[01:19:37.060 --> 01:19:37.900]   - Yes.
[01:19:37.900 --> 01:19:39.780]   - I think that it's pretty common for,
[01:19:39.780 --> 01:19:42.260]   no one's really opening their phone
[01:19:42.260 --> 01:19:45.300]   and sitting for 20 minutes, scrolling through their newsfeed.
[01:19:45.300 --> 01:19:47.340]   Although that could change now that the video stuff
[01:19:47.340 --> 01:19:48.180]   is happening, right?
[01:19:48.180 --> 01:19:49.020]   That's what they want.
[01:19:49.020 --> 01:19:51.620]   They want you sitting there sucking your time away,
[01:19:51.620 --> 01:19:54.020]   watching video, but I think what's more important
[01:19:54.020 --> 01:19:56.980]   to Facebook is that you're going to the app
[01:19:56.980 --> 01:19:58.820]   five, six, seven times a day,
[01:19:58.820 --> 01:20:00.740]   whenever you have free time, and that's what they want.
[01:20:00.740 --> 01:20:02.420]   And that just includes Facebook.
[01:20:02.420 --> 01:20:05.260]   Think about Instagram or WhatsApp
[01:20:05.260 --> 01:20:07.780]   or these other properties they own as well.
[01:20:07.780 --> 01:20:10.260]   They're taking over the home screen, that's the goal, right?
[01:20:10.260 --> 01:20:12.740]   They wanna be all of your free times
[01:20:12.740 --> 01:20:15.060]   on some kind of Facebook property.
[01:20:15.060 --> 01:20:17.900]   - I think it's also interesting and worth noting
[01:20:17.900 --> 01:20:20.660]   that the battle between Google and Facebook,
[01:20:20.660 --> 01:20:23.820]   in terms of eyeballs, is interesting in the sense
[01:20:23.820 --> 01:20:25.820]   that Facebook has been so successful
[01:20:25.820 --> 01:20:28.620]   at getting so many people to spend so much time on Facebook.
[01:20:28.620 --> 01:20:30.660]   And on Google, there's really nothing like that.
[01:20:30.660 --> 01:20:32.180]   I mean, of course, the amount of time people
[01:20:32.180 --> 01:20:34.900]   spend on Google+, compared to Facebook,
[01:20:34.900 --> 01:20:37.060]   I mean, it's just minuscule.
[01:20:37.060 --> 01:20:40.340]   And it's pretty small on Twitter too, but back to Google,
[01:20:40.340 --> 01:20:42.500]   things like Search, Google is trying to get people off search
[01:20:42.500 --> 01:20:43.860]   as soon as possible.
[01:20:43.860 --> 01:20:47.700]   It has nothing else except for YouTube.
[01:20:47.700 --> 01:20:51.020]   YouTube is the other big place on the internet where people
[01:20:51.020 --> 01:20:54.140]   spend huge amounts of time.
[01:20:54.140 --> 01:20:56.820]   And again, this is why, back to the story we talked about
[01:20:56.820 --> 01:20:59.140]   a minute ago, why Facebook wants to take that away from Google?
[01:20:59.140 --> 01:21:01.580]   'Cause it's a one place where Google can capture
[01:21:01.580 --> 01:21:05.820]   and hold eyeballs for big dollar advertising
[01:21:05.820 --> 01:21:08.900]   like pre-roll video ads and so on.
[01:21:08.900 --> 01:21:10.980]   And Facebook wants it all.
[01:21:10.980 --> 01:21:13.340]   - Let me ask each of you, all three of you,
[01:21:13.340 --> 01:21:15.340]   in CUM time, which is your point, Kurt,
[01:21:15.340 --> 01:21:18.020]   I think that's right, in cumulative time in a day,
[01:21:18.020 --> 01:21:21.260]   rank the time spent, the attention spent you give
[01:21:21.260 --> 01:21:22.580]   to the various platforms.
[01:21:22.580 --> 01:21:29.260]   - I'll go, this is the journalist answer probably,
[01:21:29.260 --> 01:21:31.220]   is Twitter is probably number one.
[01:21:31.220 --> 01:21:34.460]   I'm refreshing Twitter, what feels like constantly,
[01:21:34.460 --> 01:21:36.140]   unfortunately.
[01:21:36.140 --> 01:21:38.380]   And I'll throw one thing out there about Twitter.
[01:21:38.380 --> 01:21:41.980]   I'm surprised given what Mike just said about finding
[01:21:41.980 --> 01:21:43.580]   places where you can really get sucked in
[01:21:43.580 --> 01:21:46.340]   for multiple minutes at a time.
[01:21:46.340 --> 01:21:49.820]   I'm surprised Vine hasn't been a little bit more
[01:21:49.820 --> 01:21:51.660]   a part of Twitter's plan.
[01:21:51.660 --> 01:21:53.060]   I mean, they're obviously starting to,
[01:21:53.060 --> 01:21:54.980]   they just did autoplay video and all that stuff,
[01:21:54.980 --> 01:21:56.620]   but I would think Vine.
[01:21:56.620 --> 01:21:58.300]   - So much of it is so irritating to me.
[01:21:58.300 --> 01:22:00.180]   - Yeah, it's like in the real world.
[01:22:00.180 --> 01:22:01.740]   - It's not all that great, unfortunately,
[01:22:01.740 --> 01:22:03.580]   but it'll be interesting to see if they can figure out
[01:22:03.580 --> 01:22:05.260]   how to take more advantage of that.
[01:22:05.260 --> 01:22:07.660]   But I would say Twitter's probably first.
[01:22:07.660 --> 01:22:10.500]   And then if I'm just talking about like stuff
[01:22:10.500 --> 01:22:12.900]   I'm actually doing on my phone, Facebook's probably second,
[01:22:12.900 --> 01:22:14.780]   I'm rarely using Google on my phone,
[01:22:14.780 --> 01:22:17.580]   but when I'm on my desktop, working,
[01:22:17.580 --> 01:22:20.860]   gosh, I'd probably go to Google 50 times a day.
[01:22:20.860 --> 01:22:23.380]   I'm just always using it, but it's more of like a means
[01:22:23.380 --> 01:22:26.940]   to an end than it is somewhere where I go to enjoy my time.
[01:22:26.940 --> 01:22:27.900]   - Where were you guys?
[01:22:27.900 --> 01:22:32.780]   - Well, I'd have to say, yeah, sorry, but you go ahead.
[01:22:32.780 --> 01:22:36.260]   - Okay, typical Canadian being polite and saying,
[01:22:36.260 --> 01:22:37.100]   you go ahead.
[01:22:37.100 --> 01:22:37.940]   (laughing)
[01:22:37.940 --> 01:22:41.140]   Yeah, I mean, clearly I spend most of my time on Google Plus,
[01:22:41.140 --> 01:22:45.500]   but I also use Friends Plus me to have that go out
[01:22:45.500 --> 01:22:50.180]   to Tumblr, to Facebook, and Twitter.
[01:22:50.180 --> 01:22:51.220]   And I do that for certain things,
[01:22:51.220 --> 01:22:53.860]   but I spend a lot of time on Google Plus interacting
[01:22:53.860 --> 01:22:56.340]   with comments, 'cause the comments are oftentimes
[01:22:56.340 --> 01:22:57.180]   very involved.
[01:22:57.180 --> 01:22:58.620]   - Has that changed recently, Mike?
[01:22:58.620 --> 01:23:00.420]   Has that going down or not?
[01:23:00.420 --> 01:23:01.500]   - It has changed.
[01:23:01.500 --> 01:23:02.340]   It has changed.
[01:23:02.340 --> 01:23:06.220]   The intensity of the comments on Google Plus
[01:23:06.220 --> 01:23:09.180]   from where it was like two years ago is very,
[01:23:09.180 --> 01:23:13.060]   it's very different not only in the quantity of it,
[01:23:13.060 --> 01:23:14.420]   but in who's doing it.
[01:23:14.420 --> 01:23:16.460]   So nowadays I get lots and lots of people
[01:23:16.460 --> 01:23:18.620]   who don't really quite speak English,
[01:23:18.620 --> 01:23:20.700]   say things that I don't really understand,
[01:23:20.700 --> 01:23:22.340]   even with the translate tool.
[01:23:22.340 --> 01:23:23.460]   And there's a lot of that.
[01:23:23.460 --> 01:23:28.460]   And the number of people who have the kinds of comments
[01:23:28.460 --> 01:23:29.740]   that you went to Google Plus for,
[01:23:29.740 --> 01:23:33.380]   the substantive, intelligent, engaged people,
[01:23:33.380 --> 01:23:36.620]   that's dropped to maybe a third of what it was
[01:23:36.620 --> 01:23:39.140]   a couple of years ago, they're still there.
[01:23:39.140 --> 01:23:42.140]   But you have to weed through more fluff
[01:23:42.140 --> 01:23:45.500]   and you also have to accept the fact that,
[01:23:45.500 --> 01:23:46.780]   it's not gonna be like it was.
[01:23:46.780 --> 01:23:48.980]   Although it used to be overwhelming.
[01:23:48.980 --> 01:23:51.300]   I mean, it used to be way too much.
[01:23:51.300 --> 01:23:53.380]   - Not that those people go, I wonder.
[01:23:53.380 --> 01:23:54.220]   - Who knows.
[01:23:54.220 --> 01:23:55.220]   - Where do those people go?
[01:23:55.220 --> 01:23:56.580]   - Yeah, I mean, I think there's a,
[01:23:56.580 --> 01:23:58.980]   I think people are scattering generally
[01:23:58.980 --> 01:24:02.260]   because there's so many more places to go.
[01:24:02.260 --> 01:24:04.020]   We also have messaging applications
[01:24:04.020 --> 01:24:05.860]   which are competing for people's time.
[01:24:05.860 --> 01:24:07.100]   I think everybody's suffered,
[01:24:07.100 --> 01:24:08.980]   except for Facebook for the most part.
[01:24:08.980 --> 01:24:10.700]   I think they've done really well for themselves.
[01:24:10.700 --> 01:24:13.060]   But I would say that, you know,
[01:24:13.060 --> 01:24:17.620]   so Google Plus first Facebook and Twitter probably tied.
[01:24:17.620 --> 01:24:19.380]   The thing with Twitter that's interesting though
[01:24:19.380 --> 01:24:21.540]   is you either use Twitter on twitter.com
[01:24:21.540 --> 01:24:25.260]   or you use a third party app like Tweet Deck
[01:24:25.260 --> 01:24:27.220]   or like Tweetbot or something like that.
[01:24:27.220 --> 01:24:31.580]   If you use, let's say, Tweetbot on a mobile phone
[01:24:31.580 --> 01:24:34.140]   and you click on a link, you tap on a link,
[01:24:34.140 --> 01:24:35.980]   you kinda remain within Tweetbot.
[01:24:35.980 --> 01:24:38.140]   It has its own internal browser.
[01:24:38.140 --> 01:24:41.580]   And you can go out to Chrome or something like that
[01:24:41.580 --> 01:24:43.020]   on your phone.
[01:24:43.020 --> 01:24:46.540]   But most of the, it's interesting to me, Kurt,
[01:24:46.540 --> 01:24:48.540]   that you spend most of your time on Twitter
[01:24:48.540 --> 01:24:50.620]   because most of the stuff on Twitter
[01:24:50.620 --> 01:24:52.500]   is sending you off Twitter.
[01:24:52.500 --> 01:24:54.180]   Whereas Facebook has worked so hard
[01:24:54.180 --> 01:24:56.620]   to not send you anywhere.
[01:24:56.620 --> 01:24:58.100]   So.
[01:24:58.100 --> 01:24:59.220]   - I just find that I'm afraid,
[01:24:59.220 --> 01:25:03.180]   I just feel like you can barely refresh the feed fast enough.
[01:25:03.180 --> 01:25:04.300]   And there are times throughout the day
[01:25:04.300 --> 01:25:05.700]   where I wanna pull my hair out
[01:25:05.700 --> 01:25:07.740]   and I just close out Twitter
[01:25:07.740 --> 01:25:10.900]   because it's incredibly distracting.
[01:25:10.900 --> 01:25:12.500]   Especially if I'm on deadline
[01:25:12.500 --> 01:25:13.540]   or trying to do something quickly,
[01:25:13.540 --> 01:25:14.940]   it's just like I need this out of here.
[01:25:14.940 --> 01:25:17.980]   But it is definitely where I feel
[01:25:17.980 --> 01:25:20.260]   that I'm looking for news on my beat.
[01:25:20.260 --> 01:25:23.620]   I'm looking to see what other journalists are writing.
[01:25:23.620 --> 01:25:27.540]   I just, Twitter, it seems to be my priority.
[01:25:27.540 --> 01:25:28.540]   - I'm the same.
[01:25:28.540 --> 01:25:29.380]   I'm the same.
[01:25:29.380 --> 01:25:32.220]   So Twitter is probably number one.
[01:25:32.220 --> 01:25:35.220]   Slack would be number two just for work stuff.
[01:25:35.220 --> 01:25:38.100]   But Facebook is way down there.
[01:25:38.100 --> 01:25:41.940]   I don't really go there that often.
[01:25:41.940 --> 01:25:46.060]   I have to be super bored to sort of pull up Facebook.
[01:25:46.060 --> 01:25:48.380]   - That's what I decided I needed to change Matthew
[01:25:48.380 --> 01:25:49.900]   is that I was kind of the same.
[01:25:49.900 --> 01:25:52.580]   I don't know, I'm not quite as down on as you were,
[01:25:52.580 --> 01:25:54.860]   but a lot of times on Twitter.
[01:25:54.860 --> 01:25:58.900]   And just personally, I've got 150,000 followers on Twitter
[01:25:58.900 --> 01:26:00.780]   and 450,000 on Facebook.
[01:26:00.780 --> 01:26:01.620]   - Wow.
[01:26:01.620 --> 01:26:02.900]   - And yeah.
[01:26:02.900 --> 01:26:05.580]   And so I said, what am I doing?
[01:26:05.580 --> 01:26:08.260]   Let me cultivate that, see what that turns into.
[01:26:08.260 --> 01:26:09.260]   There's a lot of people I respect,
[01:26:09.260 --> 01:26:11.500]   like Roger Narrows-Eddie at the Wall Street Journal
[01:26:11.500 --> 01:26:13.100]   contribute a lot of stuff to Facebook.
[01:26:13.100 --> 01:26:14.820]   The groups really work.
[01:26:14.820 --> 01:26:17.060]   I'm really getting into, I met last night
[01:26:17.060 --> 01:26:19.380]   with the guy who was in charge of newsfeed
[01:26:19.380 --> 01:26:23.220]   for a little, he's doing an off the record friendship tour
[01:26:23.220 --> 01:26:24.380]   of media in New York.
[01:26:24.380 --> 01:26:27.580]   And it's really, really interesting.
[01:26:27.580 --> 01:26:29.100]   So I just as an educational thing
[01:26:29.100 --> 01:26:30.580]   is what we do for a living.
[01:26:30.580 --> 01:26:32.580]   I decided that I had to really go all in
[01:26:32.580 --> 01:26:35.860]   on Facebook and I'm enjoying it.
[01:26:35.860 --> 01:26:37.620]   Part of the problem for me was,
[01:26:37.620 --> 01:26:40.020]   my Twitter feed was going over to Facebook.
[01:26:40.020 --> 01:26:42.220]   And that's still the case. - Yeah, I noticed that.
[01:26:42.220 --> 01:26:45.300]   - But when I try to do things specifically for Facebook
[01:26:45.300 --> 01:26:46.740]   that are appropriate to Facebook,
[01:26:46.740 --> 01:26:50.300]   I, it's like Google+, or, that's the only time
[01:26:50.300 --> 01:26:51.180]   that's ever been said.
[01:26:51.180 --> 01:26:52.860]   - Yeah, Facebook's like this thing called Google+,
[01:26:52.860 --> 01:26:53.700]   it's wonderful.
[01:26:53.700 --> 01:26:59.420]   But I would recommend to you to try a time,
[01:26:59.420 --> 01:27:01.380]   like a Matt Cuts 30 Day thing,
[01:27:01.380 --> 01:27:03.100]   try to delve into Facebook,
[01:27:03.100 --> 01:27:06.300]   because the fact that 20% of web time,
[01:27:06.300 --> 01:27:08.340]   of net time is supposedly there,
[01:27:08.340 --> 01:27:10.260]   says we have to understand that well.
[01:27:10.260 --> 01:27:11.100]   Yeah, but--
[01:27:11.100 --> 01:27:13.780]   - Yeah, I guess the problem I have is that,
[01:27:13.780 --> 01:27:17.420]   you know, I tried to quote unquote fix Facebook.
[01:27:17.420 --> 01:27:20.780]   So I, I, friended everyone, you know,
[01:27:20.780 --> 01:27:22.220]   lots of people followed me.
[01:27:22.220 --> 01:27:25.660]   I had thousands of quote unquote friends
[01:27:25.660 --> 01:27:27.020]   and it just was not working.
[01:27:27.020 --> 01:27:29.780]   So I got rid of 80% of those people.
[01:27:29.780 --> 01:27:33.700]   And I tried to make it just people I actually do.
[01:27:33.700 --> 01:27:37.100]   And so the problem now is if I start posting all sorts
[01:27:37.100 --> 01:27:38.620]   of stuff into that stream,
[01:27:38.620 --> 01:27:41.020]   or even cross posting from Twitter,
[01:27:41.020 --> 01:27:43.340]   everyone's gonna get mad at me and unfriend me,
[01:27:43.340 --> 01:27:44.980]   because it'll just be too noisy.
[01:27:44.980 --> 01:27:46.620]   So now I don't know,
[01:27:46.620 --> 01:27:48.740]   I basically don't know what to put there,
[01:27:48.740 --> 01:27:52.300]   other than a link to the odd posts and sunset photos.
[01:27:52.300 --> 01:27:55.660]   I'm like, I don't know what I'm supposed to be doing there.
[01:27:55.660 --> 01:27:57.340]   Is it just connecting with friends?
[01:27:57.340 --> 01:27:59.900]   Or am I trying to do the whole
[01:27:59.900 --> 01:28:01.100]   Matthew Ingram experience?
[01:28:01.100 --> 01:28:01.940]   I just don't know.
[01:28:01.940 --> 01:28:03.060]   - It's great for those Venice,
[01:28:03.060 --> 01:28:06.380]   like floating on the Venice canal photos.
[01:28:06.380 --> 01:28:07.820]   That was awesome.
[01:28:07.820 --> 01:28:09.500]   You know, the other thing that has to be said
[01:28:09.500 --> 01:28:12.860]   is we don't know what these numbers really mean.
[01:28:12.860 --> 01:28:15.780]   I mean, Jeff, you said you have hundreds of thousands
[01:28:15.780 --> 01:28:19.420]   on Facebook, you have millions on Google+.
[01:28:19.420 --> 01:28:21.140]   Twitter, you know, Twitter engagement,
[01:28:21.140 --> 01:28:23.620]   it's kind of like saying, you know,
[01:28:23.620 --> 01:28:26.580]   I have a radio show and X number of people have radios
[01:28:26.580 --> 01:28:27.740]   and therefore--
[01:28:27.740 --> 01:28:29.380]   - Does that mean anything? - It doesn't mean anything.
[01:28:29.380 --> 01:28:30.820]   - Same with Google. - Oh yeah, absolutely.
[01:28:30.820 --> 01:28:32.780]   - In those two cases, an algorithm decides
[01:28:32.780 --> 01:28:34.100]   what's gonna be of interest.
[01:28:34.100 --> 01:28:36.780]   If it decides I'm boring to people.
[01:28:36.780 --> 01:28:39.900]   But, and so you think the Twitter is more realistic
[01:28:39.900 --> 01:28:42.220]   because it is a great river.
[01:28:42.220 --> 01:28:44.900]   But we know how few,
[01:28:44.900 --> 01:28:48.060]   what small percentage of tweets are actually seen by people.
[01:28:48.060 --> 01:28:50.180]   So whatever you write on any of these three platforms
[01:28:50.180 --> 01:28:54.140]   is seen far less than you might dream.
[01:28:54.140 --> 01:28:55.820]   But we don't know how much.
[01:28:55.820 --> 01:28:58.420]   What I don't understand is I will post something
[01:28:58.420 --> 01:29:02.100]   like a picture of my daughter or a picture of me,
[01:29:02.100 --> 01:29:03.980]   you know, on a boat or something.
[01:29:03.980 --> 01:29:07.140]   And it will get liked by dozens of people
[01:29:07.140 --> 01:29:10.700]   in foreign countries who I don't know,
[01:29:10.700 --> 01:29:11.820]   who don't speak English.
[01:29:11.820 --> 01:29:15.260]   I don't understand what are they doing?
[01:29:15.260 --> 01:29:16.460]   Why are they doing that?
[01:29:16.460 --> 01:29:18.500]   What do they hope to achieve?
[01:29:18.500 --> 01:29:20.620]   I don't know whether they're trying to make contact with me.
[01:29:20.620 --> 01:29:21.820]   They never comment.
[01:29:21.820 --> 01:29:23.460]   They never do anything but they will--
[01:29:23.460 --> 01:29:24.700]   - Well, yeah, I thought I was being
[01:29:24.700 --> 01:29:27.420]   a bit voyeuristic, you put up a picture of your boat.
[01:29:27.420 --> 01:29:29.300]   And I saw this thing in the corner and I swear,
[01:29:29.300 --> 01:29:31.220]   it looked like an attractive drive.
[01:29:31.220 --> 01:29:33.100]   So I gave us more to ask for a mark.
[01:29:33.100 --> 01:29:34.420]   And then your family,
[01:29:34.420 --> 01:29:36.940]   what was your family doing back to Fendi, your boat.
[01:29:36.940 --> 01:29:39.780]   It's all a part of your family.
[01:29:39.780 --> 01:29:42.060]   It felt like an odd invasion
[01:29:42.060 --> 01:29:44.180]   into your nice lake in Canada, but I enjoyed it.
[01:29:44.180 --> 01:29:45.620]   - That's one of the weird things about Facebook
[01:29:45.620 --> 01:29:47.460]   is here, you're, you know, somebody you went to,
[01:29:47.460 --> 01:29:50.060]   you were in the sixth grade with gets into a conversation
[01:29:50.060 --> 01:29:51.820]   with a current coworker and you're just like,
[01:29:51.820 --> 01:29:53.700]   my world's a colliding.
[01:29:53.700 --> 01:29:56.060]   But I'm sorry to cut this off,
[01:29:56.060 --> 01:29:58.380]   but Kurt has a hard out and I really want to get his tool.
[01:29:58.380 --> 01:30:00.460]   So I'm sorry to interrupt the conversation
[01:30:00.460 --> 01:30:02.140]   'cause this is great stuff.
[01:30:02.140 --> 01:30:03.580]   But I want to get Kurt's tool in
[01:30:03.580 --> 01:30:04.740]   before he wanders away.
[01:30:04.740 --> 01:30:05.980]   Kurt, go for it.
[01:30:05.980 --> 01:30:07.300]   What's your tool of the week?
[01:30:07.300 --> 01:30:09.820]   - Cool, sorry to, yeah, sorry to kill that conversation.
[01:30:09.820 --> 01:30:12.300]   My tool for the week is an update
[01:30:12.300 --> 01:30:14.380]   that happened just this morning to Mirkat.
[01:30:14.380 --> 01:30:16.940]   So Mirkat is that live streaming app
[01:30:16.940 --> 01:30:18.220]   you guys may be familiar with.
[01:30:18.220 --> 01:30:21.460]   It's got a rival called Periscope that Twitter owns.
[01:30:21.460 --> 01:30:24.300]   And what Mirkat did is there's a product update to it,
[01:30:24.300 --> 01:30:26.060]   but there's also kind of a business angle.
[01:30:26.060 --> 01:30:27.700]   And I'm more interested in the business angle,
[01:30:27.700 --> 01:30:29.500]   but I'll start real quick with the product stuff,
[01:30:29.500 --> 01:30:32.060]   which is that you can now share your live stream
[01:30:32.060 --> 01:30:33.020]   with somebody.
[01:30:33.020 --> 01:30:34.260]   So it'll be interesting that, you know,
[01:30:34.260 --> 01:30:36.060]   if I'm doing a Q&A for example,
[01:30:36.060 --> 01:30:38.180]   and I'm saying, hey, ask me anything about Facebook
[01:30:38.180 --> 01:30:40.300]   and people are actually tuning in and listening,
[01:30:40.300 --> 01:30:42.300]   I could click on another user
[01:30:42.300 --> 01:30:45.580]   and essentially they would show up there, you know,
[01:30:45.580 --> 01:30:48.860]   they would use their own phone to appear on my stream
[01:30:48.860 --> 01:30:49.900]   for up to a minute.
[01:30:49.900 --> 01:30:51.180]   And they could ask me their question,
[01:30:51.180 --> 01:30:53.220]   then I could get rid of them off the screen
[01:30:53.220 --> 01:30:54.300]   and I could answer.
[01:30:54.300 --> 01:30:56.620]   And then I could pick a new user and do the same thing.
[01:30:56.620 --> 01:30:59.180]   So they're kind of trying to create this sharing element
[01:30:59.180 --> 01:31:02.140]   around live video, which I think is kind of interesting.
[01:31:02.140 --> 01:31:03.500]   But from the business standpoint,
[01:31:03.500 --> 01:31:06.180]   what I wrote about this morning was that
[01:31:06.180 --> 01:31:08.180]   they're kind of budding up with Facebook.
[01:31:08.180 --> 01:31:13.180]   So Twitter has Periscope and Twitter somewhat famously,
[01:31:13.180 --> 01:31:15.820]   I guess if you follow this stuff,
[01:31:15.820 --> 01:31:18.860]   cut off Mirkat's access to its social graph.
[01:31:18.860 --> 01:31:20.460]   So when Mirkat first launched,
[01:31:20.460 --> 01:31:21.980]   what it did was you signed up with Twitter
[01:31:21.980 --> 01:31:23.820]   and you were automatically following all your friends
[01:31:23.820 --> 01:31:25.300]   on Twitter who also had the app.
[01:31:25.300 --> 01:31:27.820]   And Twitter said, sorry, we're not gonna let you do that
[01:31:27.820 --> 01:31:29.500]   anymore 'cause we have a rival product
[01:31:29.500 --> 01:31:31.340]   and we don't wanna help you grow your network
[01:31:31.340 --> 01:31:32.180]   over there, Mirkat.
[01:31:32.180 --> 01:31:34.740]   So they cut them off and now Facebook is letting them
[01:31:34.740 --> 01:31:35.980]   do that instead.
[01:31:35.980 --> 01:31:38.100]   And so what's kind of happening is you have this
[01:31:38.100 --> 01:31:41.340]   Twitter Periscope on one side, Facebook and kind of
[01:31:41.340 --> 01:31:43.700]   Mirkat on the other, even though, you know,
[01:31:43.700 --> 01:31:45.620]   Facebook obviously doesn't own Mirkat,
[01:31:45.620 --> 01:31:47.660]   but it'll just be kind of interesting to see
[01:31:47.660 --> 01:31:51.980]   if the integration between Mirkat and Facebook continues
[01:31:51.980 --> 01:31:54.620]   and what that might do for kind of this Twitter Facebook
[01:31:54.620 --> 01:31:56.500]   rivalry that we've already touched on
[01:31:56.500 --> 01:31:58.380]   a few times during the show.
[01:31:58.380 --> 01:31:59.940]   So I think from a business standpoint,
[01:31:59.940 --> 01:32:02.380]   they're aligning themselves with someone,
[01:32:02.380 --> 01:32:06.100]   a big, big social network that may someday down the road
[01:32:06.100 --> 01:32:08.660]   say, hey, live streaming is pretty cool.
[01:32:08.660 --> 01:32:10.780]   Maybe we buy Mirkat, it's already integrated
[01:32:10.780 --> 01:32:11.820]   into a lot of our products.
[01:32:11.820 --> 01:32:12.740]   So there you go.
[01:32:12.740 --> 01:32:14.260]   - They do seem very, very friendly.
[01:32:14.260 --> 01:32:16.900]   Kurt Wagner's associate editor of social media
[01:32:16.900 --> 01:32:21.060]   to recode and it's a great beat and you're great
[01:32:21.060 --> 01:32:22.340]   at what you do, Kurt Wagner.
[01:32:22.340 --> 01:32:23.820]   I love having you on tech news today.
[01:32:23.820 --> 01:32:25.660]   You're pretty much on the show maybe on average
[01:32:25.660 --> 01:32:27.700]   like once a week or something like that.
[01:32:27.700 --> 01:32:28.540]   - Feels like it.
[01:32:28.540 --> 01:32:31.060]   - Yeah, it's always great to have you on.
[01:32:31.060 --> 01:32:32.860]   And thank you so much for coming on Twig today.
[01:32:32.860 --> 01:32:34.380]   Thanks for coming in for great stuff.
[01:32:34.380 --> 01:32:35.300]   - Yeah, thanks guys.
[01:32:35.300 --> 01:32:36.540]   - Thank you, it was a lot of fun.
[01:32:36.540 --> 01:32:37.460]   I appreciate it.
[01:32:37.460 --> 01:32:38.300]   Thanks for having me.
[01:32:38.300 --> 01:32:39.180]   Hopefully we can do it again.
[01:32:39.180 --> 01:32:40.900]   - All right, we'll see you on Facebook.
[01:32:40.900 --> 01:32:41.900]   - Adios.
[01:32:41.900 --> 01:32:43.460]   - All right, take care.
[01:32:43.460 --> 01:32:46.860]   All right, Matthew, what do you got tool-wise?
[01:32:46.860 --> 01:32:51.860]   - I guess I don't, this tool might be old for a lot of people
[01:32:51.860 --> 01:32:56.060]   but I've been having a lot of fun with the Nexus 6.
[01:32:56.060 --> 01:32:59.940]   I got recently, I tried the Nexus 6 for a little while.
[01:32:59.940 --> 01:33:04.580]   I was unconvinced initially that it was gonna be,
[01:33:04.580 --> 01:33:07.340]   that I was gonna wanna use it because it's so huge.
[01:33:07.340 --> 01:33:10.180]   Every time I take it out, people are like,
[01:33:10.180 --> 01:33:12.340]   "Is that a phone? Is it a tablet?"
[01:33:12.340 --> 01:33:14.180]   And I thought, "No one..."
[01:33:14.180 --> 01:33:15.820]   - Are you happy to see me?
[01:33:15.820 --> 01:33:16.660]   Sorry.
[01:33:16.660 --> 01:33:18.060]   - No one has ever gonna use a phone this big.
[01:33:18.060 --> 01:33:19.580]   I can barely hold the thing.
[01:33:19.580 --> 01:33:23.260]   I can't single finger type the way I used to.
[01:33:23.260 --> 01:33:26.580]   But I got so used to it that when I had to give back
[01:33:26.580 --> 01:33:29.620]   the demo I had, I really missed it.
[01:33:29.620 --> 01:33:33.020]   And I could've gotten a 6 plus, I guess,
[01:33:33.020 --> 01:33:38.020]   but since I'm mostly an Android guy now, I thought,
[01:33:38.020 --> 01:33:39.460]   and I've just gotten so used to it,
[01:33:39.460 --> 01:33:42.180]   I can't imagine using a smaller phone.
[01:33:42.180 --> 01:33:45.740]   I try and use even the Nexus 4 I used to have
[01:33:45.740 --> 01:33:48.540]   or God forbid an old iPhone.
[01:33:48.540 --> 01:33:52.180]   And it's like typing on a candy bar, a Barbie phone.
[01:33:52.180 --> 01:33:53.540]   I just can't get used to it.
[01:33:53.540 --> 01:33:56.060]   The screen real estate is amazing.
[01:33:56.060 --> 01:33:57.220]   - Yeah, that is very cool.
[01:33:57.220 --> 01:34:00.180]   I was cleaning out my garage over the weekend
[01:34:00.180 --> 01:34:02.140]   and I ran across a little case that I had
[01:34:02.140 --> 01:34:04.180]   from my Blackberry Pearl.
[01:34:04.180 --> 01:34:06.180]   And I was just like, "Wow."
[01:34:06.180 --> 01:34:08.020]   It's like a little tiny.
[01:34:08.020 --> 01:34:10.020]   It's like a box of chiclets.
[01:34:10.020 --> 01:34:11.620]   So tiny.
[01:34:11.620 --> 01:34:14.620]   How far we've come, how large our phones have become.
[01:34:14.620 --> 01:34:18.020]   And it's like you say, it's like going to first class
[01:34:18.020 --> 01:34:21.460]   when you have to go back to coach, it sucks.
[01:34:21.460 --> 01:34:22.820]   - The only thing I don't like about it,
[01:34:22.820 --> 01:34:25.780]   the only complaint is it's just a little bit too big
[01:34:25.780 --> 01:34:27.980]   for the shirt pocket, right?
[01:34:27.980 --> 01:34:30.900]   And a little bit too big for the front jeans pocket.
[01:34:30.900 --> 01:34:33.820]   I'm always afraid to get stolen on the rear pocket.
[01:34:33.820 --> 01:34:34.980]   - What?
[01:34:34.980 --> 01:34:35.820]   Cargo pants?
[01:34:35.820 --> 01:34:37.500]   - That's what cargo pants were in my shoes.
[01:34:37.500 --> 01:34:39.340]   - You can pull them off the other night, can't imagine.
[01:34:39.340 --> 01:34:40.460]   - Yeah.
[01:34:40.460 --> 01:34:43.820]   Jeff likes to wear skinny jeans, so it's a real problem.
[01:34:43.820 --> 01:34:47.180]   So, all right, well my toy of the week is something
[01:34:47.180 --> 01:34:48.300]   called Hooks.
[01:34:48.300 --> 01:34:50.340]   This is an app from a startup in Spain.
[01:34:50.340 --> 01:34:52.380]   It's been available for iOS for some time.
[01:34:52.380 --> 01:34:56.100]   And just this week, it became available for Android.
[01:34:56.100 --> 01:34:59.020]   This is a fantastic app, especially if you have
[01:34:59.020 --> 01:35:02.060]   a smart watch, because what this is,
[01:35:02.060 --> 01:35:04.980]   the way I like to think about this is that it's like,
[01:35:04.980 --> 01:35:08.500]   I have TGT, if this, then that, which is a simple,
[01:35:08.500 --> 01:35:11.100]   easy to use macro tool that lets you connect
[01:35:11.100 --> 01:35:12.220]   different things together.
[01:35:12.220 --> 01:35:14.380]   Sometimes you can basically have,
[01:35:14.380 --> 01:35:15.900]   if something happens over here in this app,
[01:35:15.900 --> 01:35:17.980]   you send a text message, it will turn your lights on.
[01:35:17.980 --> 01:35:20.460]   That's what IFTTT does.
[01:35:20.460 --> 01:35:23.220]   What Hooks does is very similar in terms of how easy
[01:35:23.220 --> 01:35:28.060]   it is to use, but the end result is always a notification.
[01:35:28.060 --> 01:35:31.060]   So, there are 100 or so channels, there are a million
[01:35:31.060 --> 01:35:33.940]   or more user-created alerts that you can just select,
[01:35:33.940 --> 01:35:36.180]   but it lets you do things like, you can be alerted
[01:35:36.180 --> 01:35:37.380]   when a TV show airs.
[01:35:37.380 --> 01:35:39.620]   So, you can go in there and say, here's the TV show,
[01:35:39.620 --> 01:35:42.980]   and I wanna know, one hour before it airs,
[01:35:42.980 --> 01:35:45.660]   or one hour after, or two minutes before it airs,
[01:35:45.660 --> 01:35:46.900]   give me an alert.
[01:35:46.900 --> 01:35:50.020]   Weather events, let me know when it's gonna rain tomorrow.
[01:35:50.020 --> 01:35:52.700]   Sports scores, whenever the Mets have a game,
[01:35:52.700 --> 01:35:55.620]   just as soon as you know the score, shoot me the score.
[01:35:55.620 --> 01:35:58.460]   Articles published by keywords, whenever Jeff Jarvis
[01:35:58.460 --> 01:36:01.260]   publishes anything, I wanna be a notified comedian.
[01:36:01.260 --> 01:36:03.460]   - You're not a computer comedian. - Immediately.
[01:36:03.460 --> 01:36:06.180]   Movie releases, restaurants, when they reach certain ranking,
[01:36:06.180 --> 01:36:08.580]   so for example, if you live in Boston,
[01:36:08.580 --> 01:36:11.620]   you can say, okay, whenever a restaurant enters
[01:36:11.620 --> 01:36:14.620]   into the top 10 in Yelp, semi-notifications,
[01:36:14.620 --> 01:36:16.620]   so I can go check it out.
[01:36:16.620 --> 01:36:18.620]   Tweets by keyword, when videos are posted
[01:36:18.620 --> 01:36:20.220]   on a specific YouTube channel, for example,
[01:36:20.220 --> 01:36:21.820]   I have a Google channel set up,
[01:36:21.820 --> 01:36:24.580]   so that whenever the Google YouTube channel
[01:36:24.580 --> 01:36:26.860]   has a new video, I get a notification immediately,
[01:36:26.860 --> 01:36:29.180]   I check it out, I'll post it, whatever,
[01:36:29.180 --> 01:36:30.420]   and I can be the first to post it
[01:36:30.420 --> 01:36:31.540]   'cause I'm getting notified.
[01:36:31.540 --> 01:36:34.780]   Software vulnerabilities, RSS content, lottery results,
[01:36:34.780 --> 01:36:38.180]   product, hunt, keywords, new movies on Netflix,
[01:36:38.180 --> 01:36:39.900]   if you name it, there's so many things you can do with this.
[01:36:39.900 --> 01:36:41.780]   If you like notifications, if you're like me
[01:36:41.780 --> 01:36:44.100]   and feel like you don't get enough of them,
[01:36:44.100 --> 01:36:46.020]   then you're gonna love hooks.
[01:36:46.020 --> 01:36:49.580]   Again, that is a free app for Android,
[01:36:49.580 --> 01:36:50.860]   and you gotta check it out,
[01:36:50.860 --> 01:36:52.740]   and here's one of the great things about the fact
[01:36:52.740 --> 01:36:56.340]   that usually is a bummer when you're on one platform
[01:36:56.340 --> 01:36:58.220]   and it's something ships first for the other.
[01:36:58.220 --> 01:36:59.980]   In this case, it's actually a big benefit
[01:36:59.980 --> 01:37:01.320]   because all of those,
[01:37:01.320 --> 01:37:07.700]   those sort of user-created alerts that were created on iOS
[01:37:07.700 --> 01:37:10.700]   by iOS users are available to Android users.
[01:37:10.700 --> 01:37:15.060]   So on day one, this thing had a million things to choose from,
[01:37:15.060 --> 01:37:16.580]   or you can, of course, roll your own.
[01:37:16.580 --> 01:37:21.420]   So, gotta check out hooks, that is my toy of the week.
[01:37:21.420 --> 01:37:22.980]   You guys wanna talk about anything else?
[01:37:22.980 --> 01:37:25.060]   Is there anything on or not on the rundown
[01:37:25.060 --> 01:37:26.380]   that you guys as I wanna discuss
[01:37:26.380 --> 01:37:28.020]   before we shut this thing down?
[01:37:28.020 --> 01:37:29.700]   (laughing)
[01:37:29.700 --> 01:37:32.500]   - We can't do any other topic in two minutes, Matthew.
[01:37:32.500 --> 01:37:33.820]   - We can't, eh?
[01:37:33.820 --> 01:37:35.860]   - All right.
[01:37:35.860 --> 01:37:37.860]   - Well, take a look at the traffic.
[01:37:37.860 --> 01:37:38.700]   - Take a look.
[01:37:38.700 --> 01:37:40.020]   (laughing)
[01:37:40.020 --> 01:37:41.420]   - In your own show.
[01:37:41.420 --> 01:37:42.260]   - That's right.
[01:37:42.260 --> 01:37:43.700]   - Look at the both Facebook and the news.
[01:37:43.700 --> 01:37:45.180]   - That's right.
[01:37:45.180 --> 01:37:47.460]   Jeff Jarvis is professional professor.
[01:37:47.460 --> 01:37:48.420]   I did it last week too.
[01:37:48.420 --> 01:37:49.740]   I said professional.
[01:37:49.740 --> 01:37:50.900]   It says professor, right there.
[01:37:50.900 --> 01:37:51.740]   Nice and professional.
[01:37:51.740 --> 01:37:52.580]   - I like to think of myself.
[01:37:52.580 --> 01:37:54.700]   - You're such a pro that I can't get through
[01:37:54.700 --> 01:37:56.820]   the word professor without saying professional.
[01:37:56.820 --> 01:37:59.340]   Of journalism at the City University of New York,
[01:37:59.340 --> 01:38:01.340]   author of What Would Google Do
[01:38:01.340 --> 01:38:05.500]   and other fine titles and Jeff blogs at buzzmachine.com.
[01:38:05.500 --> 01:38:07.940]   Jeff, as always, it's such a pleasure
[01:38:07.940 --> 01:38:09.140]   to have a conversation with you,
[01:38:09.140 --> 01:38:11.500]   even if whether it's being recorded or not,
[01:38:11.500 --> 01:38:13.060]   I always enjoy it, I always get a lot
[01:38:13.060 --> 01:38:14.140]   and learn a lot from you.
[01:38:14.140 --> 01:38:15.460]   So thank you for that.
[01:38:15.460 --> 01:38:17.100]   - And as Leo was saying right now,
[01:38:17.100 --> 01:38:17.940]   - I'll feed us in.
[01:38:17.940 --> 01:38:18.940]   - Yeah, I'm sure he is.
[01:38:18.940 --> 01:38:20.500]   Oh man, you know what?
[01:38:20.500 --> 01:38:22.540]   The thing is he's gonna wear those to work.
[01:38:22.540 --> 01:38:23.380]   - I know he is.
[01:38:23.380 --> 01:38:24.380]   - It's going to happen.
[01:38:24.380 --> 01:38:25.220]   (laughing)
[01:38:25.220 --> 01:38:27.060]   - You're gonna have to see his knees.
[01:38:27.060 --> 01:38:28.660]   (laughing)
[01:38:28.660 --> 01:38:29.500]   - And there it is.
[01:38:29.500 --> 01:38:30.900]   Wow, unbelievable.
[01:38:30.900 --> 01:38:32.860]   Hopefully I'll have a later
[01:38:32.860 --> 01:38:35.540]   "Hosen appropriate shirt" by then.
[01:38:35.540 --> 01:38:36.820]   He's got plenty of time.
[01:38:36.820 --> 01:38:39.420]   Matthew Ingram, senior writer at Fortune Magazine,
[01:38:39.420 --> 01:38:41.100]   formerly of Gig Oum.
[01:38:41.100 --> 01:38:43.940]   Matt writes about journalism, the social web
[01:38:43.940 --> 01:38:45.540]   and the media in general.
[01:38:45.540 --> 01:38:47.380]   Thank you so much, Matthew Ingram.
[01:38:47.380 --> 01:38:50.380]   You're always insightful, both on these shows
[01:38:50.380 --> 01:38:51.700]   and also on Twitter.
[01:38:51.700 --> 01:38:54.300]   If you're not following Matthew on Twitter,
[01:38:54.300 --> 01:38:56.620]   you are blowing it.
[01:38:56.620 --> 01:38:57.740]   Follow him immediately.
[01:38:57.740 --> 01:38:58.780]   What is your Twitter handle?
[01:38:58.780 --> 01:39:01.500]   It's Matthew I.
[01:39:01.500 --> 01:39:02.340]   Is that correct?
[01:39:02.340 --> 01:39:03.180]   - One team.
[01:39:03.180 --> 01:39:04.020]   - One team.
[01:39:04.020 --> 01:39:04.860]   - One team.
[01:39:04.860 --> 01:39:05.700]   - Are they hard one?
[01:39:05.700 --> 01:39:06.540]   - I'll make the two team mistake.
[01:39:06.540 --> 01:39:07.380]   - We'll do two teams.
[01:39:07.380 --> 01:39:08.220]   - All right, well thank you so much.
[01:39:08.220 --> 01:39:09.060]   - We'll do two teams things.
[01:39:09.060 --> 01:39:09.900]   - Thank you so much, Matthew.
[01:39:09.900 --> 01:39:11.100]   And I'm hoping the next time I see you
[01:39:11.100 --> 01:39:13.260]   is very soon on Tech News Today.
[01:39:13.260 --> 01:39:14.580]   - I hope so too.
[01:39:14.580 --> 01:39:16.180]   - All right, Matthew, thanks again.
[01:39:16.180 --> 01:39:18.940]   All right, well we do this week in Google every Wednesday
[01:39:18.940 --> 01:39:22.620]   at 1 p.m. Pacific, 4 p.m. Eastern, 2000 UTC.
[01:39:22.620 --> 01:39:26.180]   You can watch the show live at Twit.tv,
[01:39:26.180 --> 01:39:28.260]   click on the new magic live button
[01:39:28.260 --> 01:39:31.260]   or you can subscribe to the show at Twit.tv/Twig.
[01:39:31.260 --> 01:39:34.180]   Thank you for joining us today and we and by we,
[01:39:34.180 --> 01:39:35.700]   I mean Leo in the gang.
[01:39:35.700 --> 01:39:37.980]   I will see you next time on Twig.
[01:39:37.980 --> 01:39:39.300]   Thanks for tuning in.
[01:39:39.300 --> 01:39:41.880]   (upbeat music)
[01:39:41.880 --> 01:39:44.460]   (upbeat music)
[01:39:44.460 --> 01:39:47.040]   (upbeat music)

