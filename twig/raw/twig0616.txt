;FFMETADATA1
title=Leave It to Beaver
artist=Leo Laporte, Jeff Jarvis, Ant Pruitt, Joan Donovan
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2021-06-17
track=616
language=English
genre=Podcast
comment=Disinformation, Memes, Twitter's anti-abuse tools, TikTok at VidCon
encoded_by=Uniblab 5.2
date=2021
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.640]   It's time for Twig this week in Google. Joan Donovan is joining us. She has been here in a while.
[00:00:05.640 --> 00:00:10.160]   She's an expert on media, disinformation, teaches at the Harvard Kennedy School.
[00:00:10.160 --> 00:00:16.720]   Of course, Jeff Jarvis is here as well as Ant Pruitt. We will talk about memes. We will talk about sick jokes.
[00:00:16.720 --> 00:00:22.360]   We will talk about Twitter and some things they're doing to make abuse a little bit less of a problem.
[00:00:22.360 --> 00:00:25.000]   We've got lots to talk about next on Twig.
[00:00:28.120 --> 00:00:48.480]   This is Twig. This week in Google, Episode 616, recorded Wednesday, June 16, 2021. Leave it to Beaver.
[00:00:48.480 --> 00:00:55.920]   This week in Google is brought to you by Udacity, gain in-demand tech skills and as little as three months
[00:00:55.920 --> 00:01:04.640]   with Udacity's part-time online tech courses. Visit Udacity.com/Twig and get 75% off any program with the code,
[00:01:04.640 --> 00:01:09.200]   "Twig 75", offerings June 30, 2021.
[00:01:09.200 --> 00:01:17.520]   And by Untucket. Father's Day is coming up and the perfect gift is an untucket shirt for the Father in your life.
[00:01:17.520 --> 00:01:24.720]   Use the code "Twig" for 20% off your first purchase at untucket.com.
[00:01:24.720 --> 00:01:32.000]   And by AT&T Active Armor. We rely so much on our phones these days and are always on them,
[00:01:32.000 --> 00:01:38.920]   whether it's live streaming content, catching up with family on weekly video calls or watching your favorite podcasts.
[00:01:38.920 --> 00:01:47.320]   There's no room for fraud calls. Thankfully, AT&T makes customer security a priority, helping block those pesky calls.
[00:01:47.320 --> 00:01:58.760]   It's not complicated. AT&T Active Armor. 24/7. Proactive network security and fraud call blocking to help stop threats at no extra charge.
[00:01:58.760 --> 00:02:06.720]   Capitable device and service required, visit att.com/active armor for details.
[00:02:06.720 --> 00:02:12.520]   It's time for "Twig". This week in Google is show where we cover all the internet stuff.
[00:02:12.520 --> 00:02:17.120]   How about that? How's that sound? It's Pruitt's here from Hands-On Photography. Hi, Aunt!
[00:02:17.120 --> 00:02:23.320]   Hello, Mr. LePort. Hello! Good to see you. Do you really work out in black socks?
[00:02:23.320 --> 00:02:29.720]   I work out in whatever socks I have on, but yes, this week I did work out in black socks.
[00:02:29.720 --> 00:02:33.320]   He was so happy going back to the gym, lifting the big iron.
[00:02:33.320 --> 00:02:40.520]   Oh! It was so great. It was so great. It was some dead lifts, man.
[00:02:40.520 --> 00:02:45.520]   Oh, it feels great. Is that the one where you're bending over and lifting it up? That's right.
[00:02:45.520 --> 00:02:50.520]   You don't bring it up like to here and then go. No, sir. Not a clean snare. Not a clean engine.
[00:02:50.520 --> 00:02:57.520]   Yeah. I was doing Tabata's this morning. The practic took to kill me. Is that what you eat?
[00:02:57.520 --> 00:03:07.520]   No, it's a high-intensity interval training. Like to kill me. Jeff Jarvis is on the road, on the road again.
[00:03:07.520 --> 00:03:11.520]   I'm in the dog house. For those of you watching live, it's my fault we're late.
[00:03:11.520 --> 00:03:14.520]   Don't worry about it. I don't think this is the wrong thing.
[00:03:14.520 --> 00:03:19.520]   That's such thing as late. Are you in Boston? Where are you?
[00:03:19.520 --> 00:03:24.520]   I'm in the Boston vicinity with my son Jake. Oh, are you visiting Jake?
[00:03:24.520 --> 00:03:28.520]   Well, yes, and also moving to New Hampshire. Oh, nice.
[00:03:28.520 --> 00:03:31.520]   It's been so long since you've actually gone anywhere. It's really nice.
[00:03:31.520 --> 00:03:35.520]   It's absolutely true. It's very wacky. I had to pull the suitcase out.
[00:03:35.520 --> 00:03:38.520]   It was like doing archaeology. Dust it off.
[00:03:38.520 --> 00:03:43.520]   I found all these old headsets and wires and things. Do you feel like?
[00:03:43.520 --> 00:03:46.520]   Are you going to be going back to class in September?
[00:03:46.520 --> 00:03:52.520]   Yep. Yep. Wow. That's exciting. New York is open. California opened yesterday.
[00:03:52.520 --> 00:03:57.520]   It's very exciting. Stacey has the day off, but I am thrilled we had a little opening because...
[00:03:57.520 --> 00:03:58.520]   Oh, we are very thrilled.
[00:03:58.520 --> 00:04:03.520]   We are very lucky to get the research director at the Harvard Kennedy School's Shorincin Center
[00:04:03.520 --> 00:04:09.520]   on media politics and public policy. She is an expert in media manipulation,
[00:04:09.520 --> 00:04:14.520]   disinformation, and adversarial media movements.
[00:04:14.520 --> 00:04:19.520]   The great Joan Donovan. Dr. Donovan. Yay. Good to see you.
[00:04:19.520 --> 00:04:23.520]   Party time. Party time. Party. Party Joan's house.
[00:04:23.520 --> 00:04:28.520]   She's just down the road. She actually invited Jeff to come over for pizza.
[00:04:28.520 --> 00:04:30.520]   We're about 10 minutes from each other.
[00:04:30.520 --> 00:04:33.520]   She actually invited him over for tech support.
[00:04:33.520 --> 00:04:35.520]   Yeah, if you need it.
[00:04:35.520 --> 00:04:41.520]   Last time we talked to you, Joan, we were talking about Russian influence on the elections
[00:04:41.520 --> 00:04:45.520]   and disinformation. That hasn't gone away.
[00:04:45.520 --> 00:04:50.520]   Apparently yesterday, President Putin said, "Oh, we're not taking...
[00:04:50.520 --> 00:04:53.520]   We're not taking you. You're hacking us."
[00:04:53.520 --> 00:04:56.520]   Okay. Sure.
[00:04:56.520 --> 00:04:57.520]   You're hacking us.
[00:04:57.520 --> 00:04:59.520]   That's how this works.
[00:04:59.520 --> 00:05:03.520]   That is one of the tools of the big lie.
[00:05:03.520 --> 00:05:07.520]   It's funny because it happens over and over again, and yet nobody seems to notice,
[00:05:07.520 --> 00:05:12.520]   is just accusing the person, "Uh-oh, what happened?"
[00:05:12.520 --> 00:05:14.520]   I didn't do it. I swear I didn't do it.
[00:05:14.520 --> 00:05:16.520]   Jeff, we went blind. There we go.
[00:05:16.520 --> 00:05:17.520]   I didn't do it for you.
[00:05:17.520 --> 00:05:18.520]   I'm on your back.
[00:05:18.520 --> 00:05:23.520]   Is to accuse your opponent of doing the thing you're doing.
[00:05:23.520 --> 00:05:29.520]   It's an amazing rhetorical trick because it gets them to get all twisted and not defending themselves.
[00:05:29.520 --> 00:05:33.520]   It's a combination of projection and what aboutism and the same thing.
[00:05:33.520 --> 00:05:34.520]   Yeah.
[00:05:34.520 --> 00:05:36.520]   Yeah, projection is a big one.
[00:05:36.520 --> 00:05:40.520]   Like, "Well, everybody, we're hacking you, so you must be hacking us."
[00:05:40.520 --> 00:05:48.520]   We're not just talking about your garden variety, you know, trolls and botnets online.
[00:05:48.520 --> 00:05:53.520]   Hacking the power grid here, hacking infrastructure, hacking...
[00:05:53.520 --> 00:05:54.520]   Meat.
[00:05:54.520 --> 00:05:57.520]   ...critical infrastructure.
[00:05:57.520 --> 00:05:59.520]   It's a little bit different.
[00:05:59.520 --> 00:06:11.520]   The stakes are obviously enormous, but at the same time, Putin's always going to pretend he doesn't know what's happening,
[00:06:11.520 --> 00:06:19.520]   but he himself is very fascinated by technology and knows a lot, and so we should always be wary.
[00:06:19.520 --> 00:06:23.520]   He also had an interesting justification for poisoning the Volney.
[00:06:23.520 --> 00:06:30.520]   I mean, it's just fascinating to watch this guy, and I hate to say it, but it brought back a little...
[00:06:30.520 --> 00:06:34.520]   This is where we were for the last four years.
[00:06:34.520 --> 00:06:37.520]   With a guy who just says anything he wants...
[00:06:37.520 --> 00:06:40.520]   Well, but a guy who's really effing smart.
[00:06:40.520 --> 00:06:42.520]   Well, maybe that's the difference.
[00:06:42.520 --> 00:06:46.520]   I was in the room with him once in Davos.
[00:06:46.520 --> 00:06:48.520]   Joe now goes there instead of me.
[00:06:48.520 --> 00:06:49.520]   I haven't been.
[00:06:49.520 --> 00:06:52.520]   I heard the hotels rent too high.
[00:06:52.520 --> 00:06:54.520]   The rent too high.
[00:06:54.520 --> 00:06:55.520]   Do it.
[00:06:55.520 --> 00:07:02.520]   At the end of the day, you can keep your world power if I save a few thousand bucks.
[00:07:02.520 --> 00:07:07.520]   So Putin saw a male journalist wearing a...
[00:07:07.520 --> 00:07:12.520]   What do you call that greenish ring from like Arizona?
[00:07:12.520 --> 00:07:13.520]   Jade?
[00:07:13.520 --> 00:07:15.520]   No, not Jade.
[00:07:15.520 --> 00:07:16.520]   Turquoise.
[00:07:16.520 --> 00:07:17.520]   Turquoise.
[00:07:17.520 --> 00:07:22.520]   So we're in a big turquoise ring, and he focused on it.
[00:07:22.520 --> 00:07:25.520]   He said, "Do men wear those?"
[00:07:25.520 --> 00:07:26.520]   And he wouldn't let go.
[00:07:26.520 --> 00:07:31.520]   He kept going back on it again, and the guy's wife had given it to him.
[00:07:31.520 --> 00:07:35.520]   And we all try to get the guy to give Putin to make him put it on.
[00:07:35.520 --> 00:07:41.520]   But he just finds an odd little thing about somebody, and he just focuses in.
[00:07:41.520 --> 00:07:43.520]   And he's scary.
[00:07:43.520 --> 00:07:46.520]   Some people call that trolling, though.
[00:07:46.520 --> 00:07:49.520]   Oh, well, his whole nation controls us.
[00:07:49.520 --> 00:07:52.520]   That was my point, sir.
[00:07:52.520 --> 00:07:59.520]   Putin's response to ABC's questioning him about Navalny was, "Will Black Lives Matter?"
[00:07:59.520 --> 00:08:04.520]   Oh, it's like a boy.
[00:08:04.520 --> 00:08:07.520]   Oh, goodness.
[00:08:07.520 --> 00:08:14.520]   Anyway, it's not really on our beat, but I think it's great to have Joan on because what is
[00:08:14.520 --> 00:08:21.520]   on our beat is the use of technology, particularly the internet, these days, to spread lies,
[00:08:21.520 --> 00:08:25.520]   misinformation, to mobilize, to radicalize.
[00:08:25.520 --> 00:08:28.520]   There's all sorts of malign uses.
[00:08:28.520 --> 00:08:33.520]   Now, Jeff is the big defender of Big Tech, and I don't disagree with him.
[00:08:33.520 --> 00:08:34.520]   Big Tech, we expect Joan to receive.
[00:08:34.520 --> 00:08:36.520]   Of the internet, I'm defending the internet.
[00:08:36.520 --> 00:08:37.520]   Yeah, but Joan was saying this.
[00:08:37.520 --> 00:08:41.520]   You were not here at the moment, but Joan was saying this before the show began.
[00:08:41.520 --> 00:08:47.520]   Where would we have been in the last year and a half without Zoom and tech and all that?
[00:08:47.520 --> 00:08:48.520]   Amen.
[00:08:48.520 --> 00:08:51.520]   I wrote a thank you note to the internet on Medium two weeks ago, and I mean it.
[00:08:51.520 --> 00:08:54.520]   I think that we would have been warped without it.
[00:08:54.520 --> 00:09:01.520]   But this is our constant battle on this show, but also just in general.
[00:09:01.520 --> 00:09:04.520]   There's good in tech and there's bad in tech.
[00:09:04.520 --> 00:09:11.520]   You wrote a piece, Joan, for Harvard magazine saying disinformation ain't going away.
[00:09:11.520 --> 00:09:13.520]   You know, can it be stopped?
[00:09:13.520 --> 00:09:17.520]   Oh, that was a piece that I was profiled in.
[00:09:17.520 --> 00:09:18.520]   Oh, okay.
[00:09:18.520 --> 00:09:21.520]   Yeah, so there's a couple other researchers in there.
[00:09:21.520 --> 00:09:28.520]   Yeah, that headline on that piece can disinformation be stopped?
[00:09:28.520 --> 00:09:31.520]   What's your answer for that, Joan?
[00:09:31.520 --> 00:09:33.520]   Well, it has with everything.
[00:09:33.520 --> 00:09:35.520]   It depends.
[00:09:35.520 --> 00:09:40.520]   You know, one of the things that I think the internet has been fantastic at is helping
[00:09:40.520 --> 00:09:44.520]   us root information around the planet, lowering the cost of it.
[00:09:44.520 --> 00:09:53.440]   I was just thinking about high fidelity video and how different it is to be isolated but
[00:09:53.440 --> 00:09:56.520]   not necessarily alone in the same way.
[00:09:56.520 --> 00:10:03.000]   You know, I'm wearing my phone losers of America t-shirt here today because I do, you know,
[00:10:03.000 --> 00:10:07.760]   I'd still like romanticize the phone and voice and you know, just the way in which people
[00:10:07.760 --> 00:10:10.720]   come to hear and learn from one another.
[00:10:10.720 --> 00:10:16.560]   But there is something different about video and when it comes to disinformation, I think
[00:10:16.560 --> 00:10:19.800]   we're dealing with, again, a distribution issue.
[00:10:19.800 --> 00:10:21.600]   We're dealing with a content issue.
[00:10:21.600 --> 00:10:29.280]   We're dealing with, you know, people who are motivated either through money or clout to,
[00:10:29.280 --> 00:10:31.200]   you know, push disinformation around.
[00:10:31.200 --> 00:10:35.560]   Obviously, it's politically rewarding to be able to do that.
[00:10:35.560 --> 00:10:41.400]   And so I wish I could say in this moment that disinformation can be stopped.
[00:10:41.400 --> 00:10:48.960]   I think one thing we have to focus on is figuring out, you know, what should these distribution
[00:10:48.960 --> 00:10:55.160]   channels look like and how should they be run and who should oversee them?
[00:10:55.160 --> 00:10:56.400]   And we got to go from there.
[00:10:56.400 --> 00:11:01.680]   Like we need serious standards and protocols to get at what I've been calling a public
[00:11:01.680 --> 00:11:05.960]   interest internet or an internet that serves the broadest public good.
[00:11:05.960 --> 00:11:07.240]   Actually, the fact-
[00:11:07.240 --> 00:11:11.800]   I don't mean to sound so naive, but I don't think disinformation will ever be stopped
[00:11:11.800 --> 00:11:16.600]   as long as there's some form of profit involved, whether it's money or this political power
[00:11:16.600 --> 00:11:17.600]   or-
[00:11:17.600 --> 00:11:18.600]   Or power or all that.
[00:11:18.600 --> 00:11:22.840]   Well, and as Jeff has pointed out, it predates the internet by a long shot.
[00:11:22.840 --> 00:11:24.840]   Well, so it's witchcraft since print?
[00:11:24.840 --> 00:11:25.840]   Yeah.
[00:11:25.840 --> 00:11:26.840]   But it's part of that.
[00:11:26.840 --> 00:11:27.840]   And maybe it is a little bit more.
[00:11:27.840 --> 00:11:32.280]   I mean, look, we've always had, I'm sure in the pre-print, there were middle ages, there
[00:11:32.280 --> 00:11:37.000]   were conspiracy theories that made the rounds just very, very slowly.
[00:11:37.000 --> 00:11:38.000]   Right.
[00:11:38.000 --> 00:11:44.840]   But as soon as there were newspapers, you know, there's the press wars of the eight, what
[00:11:44.840 --> 00:11:46.840]   is 1820s in the United States where they were-
[00:11:46.840 --> 00:11:52.360]   Well, I mean, you were back to ballots, printed ballots of people going through and that was
[00:11:52.360 --> 00:11:54.960]   the source of a lot of the witchcraft conspiracy theories.
[00:11:54.960 --> 00:11:58.480]   But there is a cynicism that's a little different.
[00:11:58.480 --> 00:12:03.560]   In fact, it's interesting you invoked the phone posters of America, John, because I
[00:12:03.560 --> 00:12:04.560]   think there's-
[00:12:04.560 --> 00:12:10.840]   They're harmless, but they also deal in a form of disinformation.
[00:12:10.840 --> 00:12:21.560]   And there's a certain amount of almost cynicism like a mocking like, oh, we can fool these
[00:12:21.560 --> 00:12:23.600]   people because they're stupid, I guess.
[00:12:23.600 --> 00:12:25.680]   Well, I think it's not stupid.
[00:12:25.680 --> 00:12:27.200]   I think it's trusting, right?
[00:12:27.200 --> 00:12:28.880]   Which is even worse.
[00:12:28.880 --> 00:12:31.080]   Because they're trusting.
[00:12:31.080 --> 00:12:32.480]   That's terrible.
[00:12:32.480 --> 00:12:40.080]   And I mean, but that's what's so interesting here is we have to change how information is
[00:12:40.080 --> 00:12:46.560]   shared and what kind of labels are put on and how it arrives at our door and what we
[00:12:46.560 --> 00:12:49.240]   call news and what we don't call news.
[00:12:49.240 --> 00:12:55.600]   I've been a big pusher of trying to get us to think about, well, we have distribution
[00:12:55.600 --> 00:12:56.600]   pathways.
[00:12:56.600 --> 00:13:03.000]   Social media really favors things that are popular and novel.
[00:13:03.000 --> 00:13:09.320]   And so what if we were to also make sure that social media served things that are timely,
[00:13:09.320 --> 00:13:11.160]   accurate, local knowledge?
[00:13:11.160 --> 00:13:12.600]   But nobody wants that, right?
[00:13:12.600 --> 00:13:13.600]   I mean, that's-
[00:13:13.600 --> 00:13:14.600]   No.
[00:13:14.600 --> 00:13:16.600]   Well, I mean, you don't have to want things to-
[00:13:16.600 --> 00:13:17.760]   Everybody wants to serve them.
[00:13:17.760 --> 00:13:18.760]   Nobody wants the vegetables.
[00:13:18.760 --> 00:13:20.760]   Nobody wants water.
[00:13:20.760 --> 00:13:21.760]   I mean, it's nice.
[00:13:21.760 --> 00:13:23.760]   It's good to have.
[00:13:23.760 --> 00:13:24.760]   It's necessary.
[00:13:24.760 --> 00:13:34.060]   But if you have a choice between water and wine or water in a martini or water in ice
[00:13:34.060 --> 00:13:36.060]   cold seltzer, like I have here-
[00:13:36.060 --> 00:13:39.680]   Which is what Jones drinking is.
[00:13:39.680 --> 00:13:46.400]   But part of it is really trying to figure out what, in what proportions do we need this?
[00:13:46.400 --> 00:13:53.000]   And I think too, so if you think about broad stakes of different conspiracy theories,
[00:13:53.000 --> 00:13:59.200]   probably the most broad stakes that most people are familiar with is this notion of Holocaust
[00:13:59.200 --> 00:14:00.200]   denialism.
[00:14:00.200 --> 00:14:08.080]   And a couple years ago, Zuckerberg came out and said that, well, he's not going to remove
[00:14:08.080 --> 00:14:14.320]   Holocaust denial on his platform because people have the right to be wrong about things.
[00:14:14.320 --> 00:14:21.400]   And later, as we realize how big anti-Semitic movements get organized and how big they
[00:14:21.400 --> 00:14:29.560]   become when they are allowed to exchange information unfettered and reach audiences of people that
[00:14:29.560 --> 00:14:34.080]   do not necessarily understand the stakes of what they're reading.
[00:14:34.080 --> 00:14:41.120]   And it's just kind of laced in with, you know, missives from your aunts and your cousins'
[00:14:41.120 --> 00:14:48.880]   graduation pictures and your best friends' cat and Holocaust denialism is just right
[00:14:48.880 --> 00:14:50.640]   there in the same mix.
[00:14:50.640 --> 00:14:51.640]   It's different.
[00:14:51.640 --> 00:14:57.080]   And so we do need to think about ranking, sorting, labeling.
[00:14:57.080 --> 00:15:01.040]   You know, I think we need more librarians at the helm of the internet.
[00:15:01.040 --> 00:15:02.040]   Hey, men to that.
[00:15:02.040 --> 00:15:03.040]   I like that.
[00:15:03.040 --> 00:15:05.320]   You know, I think it's insurmountable.
[00:15:05.320 --> 00:15:08.840]   I just think that there's different ways of thinking about what the solution might be.
[00:15:08.840 --> 00:15:10.360]   It's a big hill to climb.
[00:15:10.360 --> 00:15:11.360]   Yeah.
[00:15:11.360 --> 00:15:13.120]   So Joan, do me a favor.
[00:15:13.120 --> 00:15:17.680]   And, you know, I think that, so Leo accuses me of being an apologist for the platform.
[00:15:17.680 --> 00:15:20.880]   So I defend the internet and they happen to be the current proprietors, but they're not
[00:15:20.880 --> 00:15:22.560]   the forever proprietors.
[00:15:22.560 --> 00:15:24.000]   These are early days.
[00:15:24.000 --> 00:15:27.520]   So the discussion that I want to change is rather than part of the problem is when we
[00:15:27.520 --> 00:15:32.000]   talk about how Facebook's broken or Twitter's broken, that puts a ceiling on the discussion
[00:15:32.000 --> 00:15:35.000]   that we just try to fix incrementally then.
[00:15:35.000 --> 00:15:41.280]   If you break out of the current proprietors of the net, how would you describe in larger,
[00:15:41.280 --> 00:15:45.080]   more universal terms the internet you want?
[00:15:45.080 --> 00:15:47.760]   It's a good question.
[00:15:47.760 --> 00:15:57.360]   I want things mostly that are reflective of the values that we would instill in what
[00:15:57.360 --> 00:15:59.160]   we might call a library, right?
[00:15:59.160 --> 00:16:03.400]   Like obviously the public library is something that I value.
[00:16:03.400 --> 00:16:13.040]   And so when we have search, I want search results that are indicative of the questions
[00:16:13.040 --> 00:16:19.200]   that I'm asking and have been vetted in such a way that are mostly accurate.
[00:16:19.200 --> 00:16:25.960]   The tension here is obviously about timeliness, which is to say that the thing that gets weaponized
[00:16:25.960 --> 00:16:30.760]   often by disinformation operatives is breaking news events.
[00:16:30.760 --> 00:16:36.160]   If you have something into the world that people want to know about and they don't know enough
[00:16:36.160 --> 00:16:42.480]   about it to not assume that what comes up first isn't accurate.
[00:16:42.480 --> 00:16:44.000]   And we have people weaponizing that.
[00:16:44.000 --> 00:16:46.160]   We have people that are jumping into that.
[00:16:46.160 --> 00:16:49.560]   And so I think that Twitter is actually moving in the right direction.
[00:16:49.560 --> 00:16:52.480]   They now have curators for their trends.
[00:16:52.480 --> 00:16:59.880]   So it's not as easy to trigger a trending topic as it has been in the past.
[00:16:59.880 --> 00:17:04.840]   And so I do think that platforms are learning that when you're in the information business
[00:17:04.840 --> 00:17:08.280]   that quality matters.
[00:17:08.280 --> 00:17:14.080]   But when it comes to an internet, I think the other piece of this is really about making
[00:17:14.080 --> 00:17:17.920]   sure that people have access and then also making sure that there's digital literacy
[00:17:17.920 --> 00:17:25.000]   taught in high schools and colleges so that you understand more about what the information
[00:17:25.000 --> 00:17:28.000]   commons is and how it can be manipulated.
[00:17:28.000 --> 00:17:32.560]   What do we do about digital literacy for grandpa though?
[00:17:32.560 --> 00:17:34.560]   Well in 20.
[00:17:34.560 --> 00:17:35.560]   He'll be dead soon.
[00:17:35.560 --> 00:17:36.560]   Is that what you're saying?
[00:17:36.560 --> 00:17:38.560]   Well, I'm not just joking.
[00:17:38.560 --> 00:17:44.360]   Grandpa is the one screwing up the world more than the kids that are the grandpa.
[00:17:44.360 --> 00:17:48.560]   And that I think is a legacy of like believe what you read, right?
[00:17:48.560 --> 00:17:53.920]   Someone took the effort to write it down and other people are sharing it and it must be
[00:17:53.920 --> 00:17:54.920]   true.
[00:17:54.920 --> 00:17:59.200]   The one thing that's really interesting about my latest research is the notion of the rabbit
[00:17:59.200 --> 00:18:00.200]   hole.
[00:18:00.200 --> 00:18:02.360]   So everybody says it but nobody's really defined it.
[00:18:02.360 --> 00:18:07.800]   So I said to myself, here's an opportunity for an academic to really mess things up.
[00:18:07.800 --> 00:18:10.440]   And so I've started to think about this.
[00:18:10.440 --> 00:18:16.120]   And for me a rabbit hole is you see the same information and it's really repetitive.
[00:18:16.120 --> 00:18:19.000]   And you see it on the same platform over and over.
[00:18:19.000 --> 00:18:21.440]   Then you see the same information and it's redundant.
[00:18:21.440 --> 00:18:25.160]   That is you see the same information across different web spaces.
[00:18:25.160 --> 00:18:30.160]   So you go from YouTube over to Twitter, you see the same thing, you assume, oh well, must
[00:18:30.160 --> 00:18:31.320]   be true.
[00:18:31.320 --> 00:18:35.040]   The other thing the social media provides is responsiveness.
[00:18:35.040 --> 00:18:41.040]   That is if you ask a question on your social media, people are likely to post a link or
[00:18:41.040 --> 00:18:43.120]   give you a response.
[00:18:43.120 --> 00:18:47.160]   Or if you ask a question in a search engine, it's going to give you a response no matter
[00:18:47.160 --> 00:18:48.560]   what it is that you put in there.
[00:18:48.560 --> 00:18:53.800]   It's very rare that you get zero responses from a place like Google or Bing.
[00:18:53.800 --> 00:18:58.560]   And then the last thing that is the troublesome part of the rabbit hole is reinforcement.
[00:18:58.560 --> 00:19:01.880]   So if I go on YouTube and I'm like, what is this stupid QAnon thing?
[00:19:01.880 --> 00:19:02.880]   I don't know.
[00:19:02.880 --> 00:19:03.880]   Right?
[00:19:03.880 --> 00:19:09.240]   And so I type it in and the algorithm, the reinforcement algorithm will remember that.
[00:19:09.240 --> 00:19:14.040]   So say I watched two or three videos and I think to myself, not for me, the algorithm
[00:19:14.040 --> 00:19:17.360]   is actually going to try to get me to re-engage with that.
[00:19:17.360 --> 00:19:19.360]   Yeah, you're watching this.
[00:19:19.360 --> 00:19:20.360]   You're watching these.
[00:19:20.360 --> 00:19:21.360]   Yeah.
[00:19:21.360 --> 00:19:29.520]   And so that's not bad if you're watching greatest home runs or slam dunk contests or
[00:19:29.520 --> 00:19:31.600]   genres of bands.
[00:19:31.600 --> 00:19:37.680]   It's really bad when you're looking at network conspiracies and you're trying to figure out
[00:19:37.680 --> 00:19:39.560]   who killed JFK.
[00:19:39.560 --> 00:19:43.680]   If you're just trying to figure that out, I can tell you you're going to get Oswald,
[00:19:43.680 --> 00:19:47.000]   but I can also tell you're going to get a lot of other stuff.
[00:19:47.000 --> 00:19:53.480]   And so if you think about repetition, redundancy, responsiveness, and then the reinforcement,
[00:19:53.480 --> 00:19:55.160]   that's what we're up against here.
[00:19:55.160 --> 00:19:56.160]   What are the reasons?
[00:19:56.160 --> 00:19:57.720]   So what about expertise?
[00:19:57.720 --> 00:20:03.920]   And in an age, I agree of a web that's more based on expert networks, but this is also
[00:20:03.920 --> 00:20:11.040]   a time when experts are considered elites and for that reason, not trusted.
[00:20:11.040 --> 00:20:13.040]   Is there any hope on education?
[00:20:13.040 --> 00:20:15.600]   Sometimes experts are not necessarily experts, either.
[00:20:15.600 --> 00:20:17.800]   They just tend to brand themselves as such.
[00:20:17.800 --> 00:20:20.320]   You see it all the time on our news coverage.
[00:20:20.320 --> 00:20:22.960]   We brought on experts so and so to talk about this and that.
[00:20:22.960 --> 00:20:27.440]   And when you dig further into them, they really don't know what they're talking about either.
[00:20:27.440 --> 00:20:29.200]   This is a fault offered of the booker.
[00:20:29.200 --> 00:20:36.160]   This comes down to really what the fundamental problem is, which is unfortunately, Kellyanne
[00:20:36.160 --> 00:20:40.240]   Conway was right when she said we live in a post fact society.
[00:20:40.240 --> 00:20:41.240]   If we can't-
[00:20:41.240 --> 00:20:42.240]   Alternative alternative.
[00:20:42.240 --> 00:20:51.080]   If we can't agree on what's true and experts disagree and others who say, "I don't want
[00:20:51.080 --> 00:20:55.000]   what you say to be true to be true, so I'm going to give you a different point of view,"
[00:20:55.000 --> 00:20:56.880]   it's very hard.
[00:20:56.880 --> 00:20:57.960]   I guess that's the difference.
[00:20:57.960 --> 00:21:01.680]   You say, "Should a librarian be the arbiter of truth?
[00:21:01.680 --> 00:21:02.680]   Which library?"
[00:21:02.680 --> 00:21:03.680]   They happen for centuries.
[00:21:03.680 --> 00:21:04.680]   Yeah, but which library?
[00:21:04.680 --> 00:21:11.080]   I mean, like, well, yeah, well, not one, but also, you know, let's lower the temperature
[00:21:11.080 --> 00:21:14.040]   and just think about accuracy.
[00:21:14.040 --> 00:21:18.680]   We're up against the speed of truth, which is to say that knowledge is really expensive
[00:21:18.680 --> 00:21:19.680]   to produce.
[00:21:19.680 --> 00:21:24.880]   I know this because I run a research team and it is really hard to get at facts.
[00:21:24.880 --> 00:21:26.760]   They have to go through a whole vetting process.
[00:21:26.760 --> 00:21:33.640]   And so this is why social media has fundamentally broken breaking news because of the way in
[00:21:33.640 --> 00:21:39.280]   which facts just take a long time to get established.
[00:21:39.280 --> 00:21:45.280]   And so that was one of the very first things that trolls figured out how to meddle with
[00:21:45.280 --> 00:21:50.000]   and then became almost an entire disinformation industry.
[00:21:50.000 --> 00:21:55.600]   And part of the problem is that on the internet, especially in that sped up internet, all facts
[00:21:55.600 --> 00:21:58.280]   are equal or all statements are equal.
[00:21:58.280 --> 00:22:05.880]   Well, that's because everything has, you know, depending upon the SEO and the keyword structure,
[00:22:05.880 --> 00:22:11.720]   not equal in terms of its ability to be discovered, but you have certain people who know how
[00:22:11.720 --> 00:22:14.640]   to make things discoverable.
[00:22:14.640 --> 00:22:17.920]   And therefore their stuff tends to come up first.
[00:22:17.920 --> 00:22:18.920]   That's even worse.
[00:22:18.920 --> 00:22:19.920]   And in that's the answer.
[00:22:19.920 --> 00:22:22.240]   Well, that's really problematic.
[00:22:22.240 --> 00:22:24.520]   And that's because of the design itself.
[00:22:24.520 --> 00:22:32.040]   And that's why Ants Point is so crucial here, which is self definition on the internet.
[00:22:32.040 --> 00:22:33.480]   I am an expert.
[00:22:33.480 --> 00:22:34.480]   Trust me.
[00:22:34.480 --> 00:22:42.800]   I've got 57 articles on this scientific archive, but then you find out that there's no gates
[00:22:42.800 --> 00:22:48.360]   on the archive and anybody can upload articles and you realize, well, that doesn't mean anything.
[00:22:48.360 --> 00:22:54.640]   It's like going and hanging your own painting in a museum and declaring yourself like a
[00:22:54.640 --> 00:22:56.960]   world-renowned artist with a show.
[00:22:56.960 --> 00:22:59.920]   We got our some of this because we wanted to democratize this.
[00:22:59.920 --> 00:23:02.080]   We wanted to get rid of keepers.
[00:23:02.080 --> 00:23:07.640]   We, you know, the academy, which decides what's great art, we decided that's just a bunch
[00:23:07.640 --> 00:23:10.160]   of old white men from Europe.
[00:23:10.160 --> 00:23:11.160]   Forget that.
[00:23:11.160 --> 00:23:14.560]   And so we've kind of broken it all apart.
[00:23:14.560 --> 00:23:17.960]   That was kind of the benefit of the internet was this intermediation.
[00:23:17.960 --> 00:23:22.760]   So did print and it took a while back together because we needed to invent institutions.
[00:23:22.760 --> 00:23:28.400]   But, but, okay, so Joan, because you're from Harvard, and I could say she's at Harvard's
[00:23:28.400 --> 00:23:34.120]   Kennedy School, and that gives you expertise in the eyes of some.
[00:23:34.120 --> 00:23:37.680]   And then in the eyes of others, that would be just qualifying for me.
[00:23:37.680 --> 00:23:43.680]   Eyes of me is I've spent the last decade studying something as a job and a profession
[00:23:43.680 --> 00:23:46.960]   and a labor of love.
[00:23:46.960 --> 00:23:48.840]   But that's self self.
[00:23:48.840 --> 00:23:51.640]   I mean, look, I totally acknowledge.
[00:23:51.640 --> 00:23:53.280]   I read too many books.
[00:23:53.280 --> 00:23:55.360]   I completely acknowledge expertise.
[00:23:55.360 --> 00:24:01.240]   But you understand that's kind of what we're going into is this.
[00:24:01.240 --> 00:24:08.240]   This is very much a process by which other people who are considered experts consider
[00:24:08.240 --> 00:24:09.240]   me an expert.
[00:24:09.240 --> 00:24:14.600]   That's what that's what getting your PhD means is that other people have decided you
[00:24:14.600 --> 00:24:15.600]   can join this.
[00:24:15.600 --> 00:24:17.400]   But we rejecting the academy.
[00:24:17.400 --> 00:24:20.160]   That's just more of the academy.
[00:24:20.160 --> 00:24:21.600]   Maybe we are, but I don't know.
[00:24:21.600 --> 00:24:22.880]   People are still going to college.
[00:24:22.880 --> 00:24:27.200]   And when I show up the class, they listen and they're pretty quiet.
[00:24:27.200 --> 00:24:29.480]   But in some ways, it's self perpetuating.
[00:24:29.480 --> 00:24:33.600]   No, because you can't come teach at Harvard.
[00:24:33.600 --> 00:24:36.080]   It's an institution, Leo.
[00:24:36.080 --> 00:24:41.040]   It's so closed that even if I wanted you to come teach at Harvard, if I was like, Leo
[00:24:41.040 --> 00:24:43.680]   is the smartest guy I know on tech.
[00:24:43.680 --> 00:24:44.680]   I know.
[00:24:44.680 --> 00:24:46.320]   And he should be teaching here.
[00:24:46.320 --> 00:24:48.720]   The whole institution would be like, that's not how it works.
[00:24:48.720 --> 00:24:51.120]   But if I'm Ben Shapiro, I'm saying, that's a good thing.
[00:24:51.120 --> 00:24:52.640]   I can't teach at Harvard.
[00:24:52.640 --> 00:24:55.160]   I wouldn't trust those guys as far as I could throw them.
[00:24:55.160 --> 00:24:56.320]   You should listen to me.
[00:24:56.320 --> 00:25:04.480]   Well, that's the one thing about the internet where we start to, it's not that someone just
[00:25:04.480 --> 00:25:07.200]   declares themselves an expert on the internet.
[00:25:07.200 --> 00:25:11.040]   There's something else happening here, which has to do with engagement metrics and the
[00:25:11.040 --> 00:25:19.480]   amount of something, the flow across platforms, the amount of resources that they put into,
[00:25:19.480 --> 00:25:25.400]   making sure that their point of view is well circulated.
[00:25:25.400 --> 00:25:32.120]   And so it's not the case that any old person through their own grit and knowledge rise to
[00:25:32.120 --> 00:25:33.720]   the top online.
[00:25:33.720 --> 00:25:37.680]   That's what I'm talking about here is the stakes are different because you can engineer
[00:25:37.680 --> 00:25:39.000]   it.
[00:25:39.000 --> 00:25:40.000]   You can circumvent it.
[00:25:40.000 --> 00:25:44.320]   But those people have as big a voice, it may be even bigger.
[00:25:44.320 --> 00:25:46.000]   That's what we've given them with the internet.
[00:25:46.000 --> 00:25:48.280]   We've given them a big megaphone.
[00:25:48.280 --> 00:25:52.960]   So Leo, you're going to hate me for this because it's, I'm going to say some years ago something
[00:25:52.960 --> 00:26:00.680]   happened, but I'm going to go back to the very first call for censorship and printing.
[00:26:00.680 --> 00:26:06.080]   And it came from someone who demanded that the pope censor books, but not because there
[00:26:06.080 --> 00:26:10.200]   was anything evil or wrong or anti-religious in it because they were low quality and the
[00:26:10.200 --> 00:26:11.800]   Latin translation was bad.
[00:26:11.800 --> 00:26:17.280]   And what he was asking for was the institution of the editor and the publishing house.
[00:26:17.280 --> 00:26:19.080]   And eventually we created that.
[00:26:19.080 --> 00:26:24.000]   And yes, it was gatekeepers, but things got published had more value, right?
[00:26:24.000 --> 00:26:25.720]   So maybe it was kind of too far.
[00:26:25.720 --> 00:26:27.640]   The pendulum has swung so far.
[00:26:27.640 --> 00:26:28.640]   Well, that's right.
[00:26:28.640 --> 00:26:29.640]   We never were listening back.
[00:26:29.640 --> 00:26:30.640]   Okay.
[00:26:30.640 --> 00:26:33.200]   But we need new kinds of institutions, right?
[00:26:33.200 --> 00:26:34.760]   We need, John's absolutely right.
[00:26:34.760 --> 00:26:40.560]   The one institution that I think is most transferable is librarians because they understand the
[00:26:40.560 --> 00:26:42.000]   structure and the system.
[00:26:42.000 --> 00:26:45.120]   But the system we had of newspapers and editors there doesn't work so well.
[00:26:45.120 --> 00:26:49.240]   The system of book editors, when there's, we went from hundreds of books to thousands
[00:26:49.240 --> 00:26:52.680]   of books to 400,000 books a year.
[00:26:52.680 --> 00:26:56.800]   And we have imprints that now come out only to publish the stuff no one else would publish
[00:26:56.800 --> 00:26:59.040]   because it's full of lies and crap.
[00:26:59.040 --> 00:27:01.880]   So that institution isn't working so well.
[00:27:01.880 --> 00:27:03.840]   So do you update the current institutions?
[00:27:03.840 --> 00:27:05.560]   Do you invent new institutions?
[00:27:05.560 --> 00:27:07.000]   That's what we're about right now.
[00:27:07.000 --> 00:27:11.720]   That's the internet we're trying to create is an internet based on new institutions given
[00:27:11.720 --> 00:27:15.680]   a new reality to perform the functions we've always needed.
[00:27:15.680 --> 00:27:16.680]   But now.
[00:27:16.680 --> 00:27:18.600]   And this is what some of these.
[00:27:18.600 --> 00:27:23.680]   Well these algorithms were supposed to be doing that, but instead of them being, you
[00:27:23.680 --> 00:27:34.680]   know, the kinds of gatekeepers that would bring expertise and accurate stuff to the top,
[00:27:34.680 --> 00:27:38.840]   we have a system by which lots of people have figured out how to engineer that.
[00:27:38.840 --> 00:27:44.920]   And so when, you know, Davey Albo over the New York Times is writing an article about
[00:27:44.920 --> 00:27:51.520]   coordinated and authentic behavior, what she's really looking at is the networks that are
[00:27:51.520 --> 00:27:57.240]   using, let's say it's happening on Facebook, the networks of pages and accounts that are
[00:27:57.240 --> 00:28:04.440]   using Facebook as it is built to promote a disinformation campaign.
[00:28:04.440 --> 00:28:09.960]   And so it's not easy anymore, but it used to be very easy to spin up a couple hundred
[00:28:09.960 --> 00:28:14.960]   fake Facebook pages and accounts and juice that algorithm so that there was a lot of
[00:28:14.960 --> 00:28:20.920]   engagement on a certain article which then would make it more likely to appear in someone's
[00:28:20.920 --> 00:28:21.920]   newsfeed.
[00:28:21.920 --> 00:28:27.280]   And so there's an entire shadow industry of folks that have figured out how these platforms
[00:28:27.280 --> 00:28:35.040]   work and they know that this is a main vector of attack that has serious implications, especially
[00:28:35.040 --> 00:28:36.720]   for our democracy.
[00:28:36.720 --> 00:28:42.080]   And I've been thinking a lot about the role of information in a democracy and it's not
[00:28:42.080 --> 00:28:47.280]   just that the information should be accurate, but it's also about the distribution of that
[00:28:47.280 --> 00:28:54.360]   information and who controls those access points and how information is spread and in
[00:28:54.360 --> 00:28:56.360]   what proportions.
[00:28:56.360 --> 00:29:04.360]   And if you can manipulate these algorithms by making up a fake network and carrying out
[00:29:04.360 --> 00:29:09.760]   a disinformation campaign, then that it's not just a design flaw.
[00:29:09.760 --> 00:29:17.200]   I mean, it's something that we see governments now putting major money and effort into doing
[00:29:17.200 --> 00:29:19.000]   right ahead of elections.
[00:29:19.000 --> 00:29:24.000]   Is there a history of gaming networks?
[00:29:24.000 --> 00:29:29.040]   What's really what's happened is that people figure out how to game expertise, how to game.
[00:29:29.040 --> 00:29:34.160]   I remember it started with my friend Kevin Rose when he started Dig, which was a great
[00:29:34.160 --> 00:29:35.160]   idea.
[00:29:35.160 --> 00:29:39.080]   It would let people vote on which news stories are important news stories, but it didn't
[00:29:39.080 --> 00:29:45.360]   take long for others to figure out how to game it so that their stories would go up
[00:29:45.360 --> 00:29:46.360]   on top.
[00:29:46.360 --> 00:29:51.040]   And everything we've just described sounds to me like people have figured out how to
[00:29:51.040 --> 00:29:54.040]   game the system.
[00:29:54.040 --> 00:29:56.040]   Is that an intended consequences?
[00:29:56.040 --> 00:30:01.280]   Well, it's an intended consequence of the people who are gaming for the people who wish
[00:30:01.280 --> 00:30:02.280]   it weren't happening.
[00:30:02.280 --> 00:30:03.280]   It's unintended.
[00:30:03.280 --> 00:30:07.320]   They didn't hire Joan Donovan years ago to say guys, this could go wrong.
[00:30:07.320 --> 00:30:08.760]   Let me ask you.
[00:30:08.760 --> 00:30:13.680]   Many years ago, was this many years ago, Jeff, he's the expert in that.
[00:30:13.680 --> 00:30:15.280]   What were is this a history?
[00:30:15.280 --> 00:30:19.640]   I wonder if there's a history of is this like a human sure every decision that can be
[00:30:19.640 --> 00:30:21.760]   game to will be game to be game to be game to be game to be game to be game to be game.
[00:30:21.760 --> 00:30:24.480]   It was only I read this this week only because of the latter time.
[00:30:24.480 --> 00:30:28.920]   Did we get the Daily Mail because there wasn't enough type to be made before, right?
[00:30:28.920 --> 00:30:33.600]   So when you get that point, then that's that's a new entry point for a virus.
[00:30:33.600 --> 00:30:36.600]   So and the problem with the internet was we were too trusting.
[00:30:36.600 --> 00:30:39.320]   Yeah, but but that was the basic problem.
[00:30:39.320 --> 00:30:41.200]   So is that really is that the solution?
[00:30:41.200 --> 00:30:42.200]   Just don't trust anybody.
[00:30:42.200 --> 00:30:44.800]   I know hire Joan Donovan to warn you.
[00:30:44.800 --> 00:30:45.800]   I don't trust you.
[00:30:45.800 --> 00:30:47.800]   I don't want to lose my mind.
[00:30:47.800 --> 00:30:50.800]   I don't do that.
[00:30:50.800 --> 00:30:53.600]   But I will tell you, you can call me anytime.
[00:30:53.600 --> 00:30:56.480]   Like the blondie song says.
[00:30:56.480 --> 00:31:01.880]   But one of the things that the research that I'm doing now around me more is in our book
[00:31:01.880 --> 00:31:04.640]   starts with occupy.
[00:31:04.640 --> 00:31:10.760]   And I had always heard the lore about occupy really being started as this like, you know,
[00:31:10.760 --> 00:31:17.200]   ragtag group of anarcho punks and leftists and liberals and all kinds of folks show up
[00:31:17.200 --> 00:31:23.120]   to Zukati Park and they're mad about inequalities and they've seen what happened in Spain recently
[00:31:23.120 --> 00:31:28.360]   where people occupied the squares and they were just going to have this occupation and
[00:31:28.360 --> 00:31:31.960]   it was going to happen and we'll see what goes on.
[00:31:31.960 --> 00:31:38.200]   But actually when you dig into it, it was a hoax that really got occupy off the ground.
[00:31:38.200 --> 00:31:45.600]   They tweeted out based upon a made up rumor that someone had made the occupy social media
[00:31:45.600 --> 00:31:51.080]   administrators believe that Radiohead was going to play at 4 p.m. that afternoon and
[00:31:51.080 --> 00:31:56.440]   Radiohead was in New York to play like Madison Square Garden or something.
[00:31:56.440 --> 00:32:02.960]   And so Radiohead is then forced to respond and they put out on Facebook saying, hey,
[00:32:02.960 --> 00:32:07.760]   we support these protesters, but we are absolutely not playing in the park at four.
[00:32:07.760 --> 00:32:11.840]   And then of course, all these Radiohead fans are like, of course, they would lie to us.
[00:32:11.840 --> 00:32:17.840]   So we'll go down to the park at four just in case Radiohead shows up.
[00:32:17.840 --> 00:32:25.560]   And that's really what kick starts the movements in the conversation and people start those
[00:32:25.560 --> 00:32:31.920]   kinds of people that were, you know, millennials interested in the politics and the spectacle.
[00:32:31.920 --> 00:32:36.360]   And then we're like, yeah, that, you know, student loans are really dragging me down.
[00:32:36.360 --> 00:32:38.080]   Maybe I should come back, right?
[00:32:38.080 --> 00:32:44.720]   And so then you start to see occupy ripple in other areas and you see lots of other occupations.
[00:32:44.720 --> 00:32:49.240]   And then of course, there's police brutality, which then becomes a very motivating force
[00:32:49.240 --> 00:32:54.480]   for people to say, okay, I'm not going to sit by and watch these people get pepper sprayed.
[00:32:54.480 --> 00:32:57.800]   We're going to do an occupy in my town as well.
[00:32:57.800 --> 00:33:00.800]   And so it just starts to take on a life of its own.
[00:33:00.800 --> 00:33:06.280]   So but when I was looking at the history of this, I started, I watched this documentary
[00:33:06.280 --> 00:33:08.480]   called occupy on mask.
[00:33:08.480 --> 00:33:13.600]   Now if I tell you who made it, I, you know, I didn't even realize it at the time, but
[00:33:13.600 --> 00:33:16.080]   it was Steve Bannon and Andrew Prechle.
[00:33:16.080 --> 00:33:17.080]   Oh my God.
[00:33:17.080 --> 00:33:19.480]   And they were students of occupy.
[00:33:19.480 --> 00:33:28.400]   Oh, figured out is that you can hoax using social media and you can game these algorithms
[00:33:28.400 --> 00:33:33.160]   and that it wasn't just about putting out your blog and getting zero comments, but that
[00:33:33.160 --> 00:33:38.640]   social media contains something special and different.
[00:33:38.640 --> 00:33:43.480]   And the networks themselves were really important to game.
[00:33:43.480 --> 00:33:49.720]   And if we even think back before we called it social media, we called it social networking.
[00:33:49.720 --> 00:33:52.920]   Social media is just what happens when you need to make money.
[00:33:52.920 --> 00:33:57.200]   You know, like they're like, well, we got to put some kind of content here.
[00:33:57.200 --> 00:34:03.120]   And so, so it's really important that we think historically, but maybe we don't have
[00:34:03.120 --> 00:34:10.920]   to go back as far as, you know, the, the reformation, but it does help.
[00:34:10.920 --> 00:34:15.840]   But this is why I think hoaxing the media is such an important aspect of what's going
[00:34:15.840 --> 00:34:20.840]   on here, which is why, and Jeff knows this, I've spent the last several years really,
[00:34:20.840 --> 00:34:27.520]   I was just going to say in the, in the muck with journalists really trying to figure out
[00:34:27.520 --> 00:34:31.520]   what, what is the new journalism we need as well?
[00:34:31.520 --> 00:34:39.400]   That aren't susceptible to this kind of, of hoaxing when it looks like a crowd is up to
[00:34:39.400 --> 00:34:40.400]   something.
[00:34:40.400 --> 00:34:43.880]   It feels to me like there are two kinds of people.
[00:34:43.880 --> 00:34:47.560]   There are people who are trusting and honest.
[00:34:47.560 --> 00:34:54.320]   And then the people who are willing to take any system and game it to their advantage.
[00:34:54.320 --> 00:34:58.840]   And of course, it takes advantage of the people who are trusting who say, well, I, I read
[00:34:58.840 --> 00:34:59.840]   it.
[00:34:59.840 --> 00:35:01.320]   It must be true.
[00:35:01.320 --> 00:35:06.600]   And maybe is that the biggest lesson that people should learn is a certain amount of
[00:35:06.600 --> 00:35:11.680]   cynicism to understand that no matter what the system is, there's somebody out there who's
[00:35:11.680 --> 00:35:14.600]   aiming to game it to their advantage.
[00:35:14.600 --> 00:35:15.600]   Is that?
[00:35:15.600 --> 00:35:16.600]   Yeah.
[00:35:16.600 --> 00:35:19.960]   And I think this is, this goes, this goes back to the history of computer programming
[00:35:19.960 --> 00:35:20.960]   and hacking.
[00:35:20.960 --> 00:35:25.200]   You know, this is something that Dana Boyd talks a lot about, but this kind of hacker
[00:35:25.200 --> 00:35:30.360]   mindset, which is this thing, this designer, this person puts this software into the world
[00:35:30.360 --> 00:35:33.120]   and they think it works this way.
[00:35:33.120 --> 00:35:35.440]   We can show that it works differently.
[00:35:35.440 --> 00:35:41.440]   Now what's very unique about social media is for a long time, like during Occupy, you
[00:35:41.440 --> 00:35:44.080]   actually did need a crowd to move the algorithms.
[00:35:44.080 --> 00:35:50.840]   You needed a bunch of people that were coordinated, or at least somehow interacting with the
[00:35:50.840 --> 00:35:56.320]   system in the same way so that they were all like able to use the same keywords.
[00:35:56.320 --> 00:36:01.880]   That practice, just like a game of basketball, people practiced it over time.
[00:36:01.880 --> 00:36:06.320]   And now they're just like people are a lot better at it.
[00:36:06.320 --> 00:36:12.600]   And then there's also software marketers figured out early on that if I could have one account,
[00:36:12.600 --> 00:36:14.640]   I could have 10,000 accounts.
[00:36:14.640 --> 00:36:22.200]   And so early botnets and marketing software was the tool of the trade for some of these
[00:36:22.200 --> 00:36:26.360]   media manipulators and disinformers around 2016.
[00:36:26.360 --> 00:36:30.040]   Of course, the platform companies have become hip to that now.
[00:36:30.040 --> 00:36:35.680]   But there is something about the way tech is built and how it's all brick-a-lage and
[00:36:35.680 --> 00:36:41.800]   people can build these different components and make things do different stuff.
[00:36:41.800 --> 00:36:48.280]   But there's really no great test for scale.
[00:36:48.280 --> 00:36:51.880]   And you kind of have to deploy the technology to figure out what.
[00:36:51.880 --> 00:36:52.960]   That's a really important point.
[00:36:52.960 --> 00:36:54.680]   So you can't predict your saying.
[00:36:54.680 --> 00:36:55.680]   You have to.
[00:36:55.680 --> 00:36:56.680]   Yeah, no, you can't predict.
[00:36:56.680 --> 00:36:58.160]   It's too late.
[00:36:58.160 --> 00:36:59.160]   That's really, really interesting.
[00:36:59.160 --> 00:37:02.000]   So now I take a little break.
[00:37:02.000 --> 00:37:04.640]   Let me hold that thought.
[00:37:04.640 --> 00:37:10.640]   Professor Donovan, Joan Donovan, it's great to have your intelligence on this because it's
[00:37:10.640 --> 00:37:12.600]   such a challenging topic for all of us.
[00:37:12.600 --> 00:37:14.040]   And it's one of the topics we do.
[00:37:14.040 --> 00:37:19.680]   I asked to have you on because we are constantly going around and around and around about this
[00:37:19.680 --> 00:37:20.680]   stuff.
[00:37:20.680 --> 00:37:24.760]   And I thought, well, maybe Joan can help us cut the Gordy and not.
[00:37:24.760 --> 00:37:26.880]   Joan is, of course, the research director.
[00:37:26.880 --> 00:37:32.160]   I want to get the whole title in here because we can't get it on the lower third.
[00:37:32.160 --> 00:37:34.000]   It's too long.
[00:37:34.000 --> 00:37:38.600]   Research director at Harvard Kennedy School.
[00:37:38.600 --> 00:37:39.960]   Shorenstein, go ahead.
[00:37:39.960 --> 00:37:43.040]   Why don't you tell me what it is?
[00:37:43.040 --> 00:37:47.560]   It's the Shorenstein Center on media, politics and public policy.
[00:37:47.560 --> 00:37:49.160]   It's a bit of a tongue twister.
[00:37:49.160 --> 00:37:50.800]   And you get to teach too.
[00:37:50.800 --> 00:37:52.040]   What classes do you teach?
[00:37:52.040 --> 00:37:53.680]   I'm very curious.
[00:37:53.680 --> 00:37:56.480]   I teach medium manipulation and disinformation.
[00:37:56.480 --> 00:37:57.480]   Can't think.
[00:37:57.480 --> 00:37:58.480]   Good.
[00:37:58.480 --> 00:37:59.480]   Because more people know how to do that.
[00:37:59.480 --> 00:38:00.560]   Yeah, some real skills.
[00:38:00.560 --> 00:38:01.560]   No.
[00:38:01.560 --> 00:38:06.560]   Do you teach people how to understand that and defend against it?
[00:38:06.560 --> 00:38:07.560]   No, you're not.
[00:38:07.560 --> 00:38:08.560]   That's basic stuff.
[00:38:08.560 --> 00:38:09.560]   Oh, yeah.
[00:38:09.560 --> 00:38:10.560]   No, no.
[00:38:10.560 --> 00:38:11.560]   We go into that.
[00:38:11.560 --> 00:38:12.840]   We do internet history.
[00:38:12.840 --> 00:38:14.800]   We do social movements online.
[00:38:14.800 --> 00:38:16.440]   We do political communication.
[00:38:16.440 --> 00:38:18.520]   I mean, it's a real nerd fest.
[00:38:18.520 --> 00:38:19.960]   I'm not going to lie.
[00:38:19.960 --> 00:38:20.960]   Joan also edits me.
[00:38:20.960 --> 00:38:21.960]   We're not controlling.
[00:38:21.960 --> 00:38:22.960]   I know.
[00:38:22.960 --> 00:38:23.960]   I would love to take that class.
[00:38:23.960 --> 00:38:24.960]   You come.
[00:38:24.960 --> 00:38:27.360]   Not only would they not let me teach at Harvard, they wouldn't let me study.
[00:38:27.360 --> 00:38:28.360]   So.
[00:38:28.360 --> 00:38:31.160]   Yeah, I'm going to bring you in.
[00:38:31.160 --> 00:38:33.200]   I'm basically banding you in the back.
[00:38:33.200 --> 00:38:35.000]   I'm basically banding.
[00:38:35.000 --> 00:38:40.680]   Joan also edits me more weekly on medium from the wilds of the internet, which is,
[00:38:40.680 --> 00:38:44.120]   I think fantastic.
[00:38:44.120 --> 00:38:45.120]   Just really.
[00:38:45.120 --> 00:38:47.840]   And it's not just a bunch of memes for no good reason.
[00:38:47.840 --> 00:38:48.840]   It's fascinating.
[00:38:48.840 --> 00:38:50.120]   It's a, you know what?
[00:38:50.120 --> 00:38:54.040]   You want a little Harvard education, meme or weekly.
[00:38:54.040 --> 00:38:56.000]   Jeff Jarvis is also here.
[00:38:56.000 --> 00:38:58.200]   His son Jake helping him out.
[00:38:58.200 --> 00:38:59.200]   Thank goodness.
[00:38:59.200 --> 00:39:00.200]   Rescued me.
[00:39:00.200 --> 00:39:01.200]   Yeah.
[00:39:01.200 --> 00:39:03.640]   And it's great to have you Buzzmachine.com.
[00:39:03.640 --> 00:39:06.200]   I didn't do the whole singing thing.
[00:39:06.200 --> 00:39:09.680]   I mean, Joan should have at least as long as mine.
[00:39:09.680 --> 00:39:10.640]   Yeah.
[00:39:10.640 --> 00:39:11.880]   This is pretty good.
[00:39:11.880 --> 00:39:16.080]   This is the, the intro we use for Jeff Joan.
[00:39:16.080 --> 00:39:22.440]   Jeff Jarvis is the Leonard Tao professor for journalistic innovation at the Craig.
[00:39:22.440 --> 00:39:28.280]   You're a graduate school of journalism at the City University of New York.
[00:39:28.280 --> 00:39:32.560]   We have to sing his name because he endowed the chair and that was one of the requirements.
[00:39:32.560 --> 00:39:33.560]   By the way, Craig.
[00:39:33.560 --> 00:39:36.120]   So Craig did point out that his wife does have chase.
[00:39:36.120 --> 00:39:38.600]   I saw the picture and that's yes.
[00:39:38.600 --> 00:39:39.960]   He tweeted it or something.
[00:39:39.960 --> 00:39:41.720]   I saw the picture to straight.
[00:39:41.720 --> 00:39:43.200]   Very nice kitchen.
[00:39:43.200 --> 00:39:44.200]   Very nice.
[00:39:44.200 --> 00:39:45.200]   Yes.
[00:39:45.200 --> 00:39:46.200]   It is.
[00:39:46.200 --> 00:39:51.400]   Well, I should say that I am also very thankful for Craig's, you know, contribution to the
[00:39:51.400 --> 00:39:52.400]   research that we do.
[00:39:52.400 --> 00:39:56.240]   I don't think we would have survived the pandemic over the Schorrenstein Center without.
[00:39:56.240 --> 00:40:01.000]   Oh, there's true so many Craig has quietly rescued.
[00:40:01.000 --> 00:40:02.720]   Yeah, not just us.
[00:40:02.720 --> 00:40:05.240]   There's a lot of us out there that benefit from him.
[00:40:05.240 --> 00:40:06.840]   And I really love his networks.
[00:40:06.840 --> 00:40:07.840]   Yeah.
[00:40:07.840 --> 00:40:08.840]   That's really fantastic.
[00:40:08.840 --> 00:40:14.920]   Craig has a, I'm sorry, I know I'm interrupting the commercial and Ant promo, but just Craig
[00:40:14.920 --> 00:40:20.920]   has a million list of the people that he supports and it's a phenomenal bunch of people.
[00:40:20.920 --> 00:40:23.080]   Oh, I bet that would be a great thing to get.
[00:40:23.080 --> 00:40:24.080]   Wow.
[00:40:24.080 --> 00:40:25.080]   That's really cool.
[00:40:25.080 --> 00:40:26.080]   Yeah.
[00:40:26.080 --> 00:40:27.520]   And yes, Ant Pruitt is also here.
[00:40:27.520 --> 00:40:29.320]   Great to have Ant.
[00:40:29.320 --> 00:40:32.400]   He represents real people on this show.
[00:40:32.400 --> 00:40:33.400]   That's right.
[00:40:33.400 --> 00:40:34.400]   The realest.
[00:40:34.400 --> 00:40:35.400]   The real people.
[00:40:35.400 --> 00:40:38.400]   He is a host of Hands-Off Photography and our good friends.
[00:40:38.400 --> 00:40:39.400]   Great to have you in.
[00:40:39.400 --> 00:40:40.840]   Our show today brought to you.
[00:40:40.840 --> 00:40:42.200]   Oh, look at that.
[00:40:42.200 --> 00:40:43.200]   Is that your new one?
[00:40:43.200 --> 00:40:44.200]   Is that your new?
[00:40:44.200 --> 00:40:46.000]   No, that's just, that's just my old camera.
[00:40:46.000 --> 00:40:47.640]   Just hold it to the old camera.
[00:40:47.640 --> 00:40:49.120]   I have to sit there.
[00:40:49.120 --> 00:40:53.000]   Our show today brought to you by Udacity.
[00:40:53.000 --> 00:40:57.680]   Now look, I wish I could go to Harvard, but anybody can go to Udacity.
[00:40:57.680 --> 00:41:01.480]   Udacity is an online educational program.
[00:41:01.480 --> 00:41:06.720]   Good time, geared for people who have a job but want to get some new skills, want to take
[00:41:06.720 --> 00:41:08.880]   their technology to the next level.
[00:41:08.880 --> 00:41:12.520]   With specialized, exciting content, a lot of it you can't get anywhere else.
[00:41:12.520 --> 00:41:17.200]   One of Udacity's really smart moves was to partner with companies like Microsoft and
[00:41:17.200 --> 00:41:22.760]   Google and IBM and Amazon to develop their nanodegree programs.
[00:41:22.760 --> 00:41:30.800]   In areas like AI, Deep Learning, Flying Car and Autonomous Flight Engineer, Intro to Self-Driving
[00:41:30.800 --> 00:41:34.280]   Cars, Machine Learning Engineer, Robotics Software Engineer.
[00:41:34.280 --> 00:41:38.320]   If you want to know what the future is going to look like, just browse the Udacity catalog.
[00:41:38.320 --> 00:41:39.320]   It's awesome.
[00:41:39.320 --> 00:41:45.640]   Udacity was started by a Googler because he realized the people who were coming to Google
[00:41:45.640 --> 00:41:49.400]   looking for work didn't have, even though they went to four-year schools, didn't have
[00:41:49.400 --> 00:41:52.440]   the skills that they needed.
[00:41:52.440 --> 00:41:57.320]   These companies have realized we've got to take this into our own hands and train these
[00:41:57.320 --> 00:42:00.640]   people so that we can get the best talent.
[00:42:00.640 --> 00:42:04.240]   Udacity can help you master the latest skills and techniques.
[00:42:04.240 --> 00:42:09.560]   What I love about Udacity when you sign up, of course, you'll be able to watch lectures.
[00:42:09.560 --> 00:42:14.800]   Many of the teachers or team leads at these top companies so they really know their material.
[00:42:14.800 --> 00:42:18.400]   But you also have to get hands on with the material.
[00:42:18.400 --> 00:42:19.960]   I think that's so important.
[00:42:19.960 --> 00:42:23.200]   They're all the courses are project-based.
[00:42:23.200 --> 00:42:27.080]   With active learning, that covers cutting-edge technology, lets you test your knowledge,
[00:42:27.080 --> 00:42:28.400]   lets actually do it.
[00:42:28.400 --> 00:42:30.600]   You can learn about something but until you've done it, you don't.
[00:42:30.600 --> 00:42:32.360]   You don't know it.
[00:42:32.360 --> 00:42:36.080]   In every case, your homework and projects will be reviewed by qualified professionals in
[00:42:36.080 --> 00:42:37.080]   the field.
[00:42:37.080 --> 00:42:38.080]   Real code reviews.
[00:42:38.080 --> 00:42:40.160]   You'll have real human feedback.
[00:42:40.160 --> 00:42:42.560]   But you're not on your own.
[00:42:42.560 --> 00:42:43.560]   You've got 24/7.
[00:42:43.560 --> 00:42:46.160]   You've got access to mentors who can help you.
[00:42:46.160 --> 00:42:48.640]   Could help you understand this stuff.
[00:42:48.640 --> 00:42:52.440]   Very flexible because they know many of you have day jobs.
[00:42:52.440 --> 00:42:56.280]   You can put in just five to ten hours a week, work at your own pace anytime of the day or
[00:42:56.280 --> 00:42:59.840]   night, graduate in as little as three months.
[00:42:59.840 --> 00:43:07.000]   During the World Economic Forum, here's a fun fact, 75 million jobs will be replaced
[00:43:07.000 --> 00:43:09.800]   by automated processes in the next three years.
[00:43:09.800 --> 00:43:12.000]   That might be one of your jobs.
[00:43:12.000 --> 00:43:14.720]   Maybe it's time to prepare for a job of the future.
[00:43:14.720 --> 00:43:21.760]   It's probably why over 14 million people in over 240 countries use Udacity.
[00:43:21.760 --> 00:43:25.760]   Once you enroll as a student, you'll be prompted to view the online course, complete a series
[00:43:25.760 --> 00:43:29.440]   of projects and support courses, get that nano degree.
[00:43:29.440 --> 00:43:30.800]   That is worth something.
[00:43:30.800 --> 00:43:33.680]   That really means something when you apply for those jobs.
[00:43:33.680 --> 00:43:34.720]   Flexible payment options.
[00:43:34.720 --> 00:43:36.200]   You can learn at your own pace and schedule.
[00:43:36.200 --> 00:43:37.720]   There are free courses.
[00:43:37.720 --> 00:43:40.280]   Polish your GitHub profile.
[00:43:40.280 --> 00:43:43.400]   Tune up your LinkedIn resume.
[00:43:43.400 --> 00:43:47.920]   By the way, if your business and your team needs to master cutting edge technologies like
[00:43:47.920 --> 00:43:53.040]   data science and AI and cybersecurity, check out Udacity for Enterprise, which will help
[00:43:53.040 --> 00:43:57.600]   you upskill your entire workforce with real-world project-based learning.
[00:43:57.600 --> 00:44:01.080]   Just check out the Enterprise section at Udacity's website today.
[00:44:01.080 --> 00:44:03.640]   Get the in-demand tech skills you need to advance your career.
[00:44:03.640 --> 00:44:08.280]   Go to Udacity.com/twit-t-w-i-t.
[00:44:08.280 --> 00:44:09.760]   Right now they've got a really good deal.
[00:44:09.760 --> 00:44:11.960]   This is only for the next couple of weeks.
[00:44:11.960 --> 00:44:14.320]   Off-Rens June 30, 2021.
[00:44:14.320 --> 00:44:19.480]   You get 75% off any program with the code, "Twit 75".
[00:44:19.480 --> 00:44:22.120]   75% off.
[00:44:22.120 --> 00:44:24.080]   Use the code "Twit 75".
[00:44:24.080 --> 00:44:25.840]   Don't wait.
[00:44:25.840 --> 00:44:30.720]   This ends June 30, Udacity.com/twit-the-off-recode-twit-75.
[00:44:30.720 --> 00:44:33.080]   Thank you, Udacity, for supporting Twig.
[00:44:33.080 --> 00:44:34.960]   Thank you for supporting us by using that address.
[00:44:34.960 --> 00:44:35.960]   I know you're smart.
[00:44:35.960 --> 00:44:36.960]   You don't have to go there.
[00:44:36.960 --> 00:44:42.200]   But if you want that offer, you've got to go to Udacity.com/twit-and-use-the-off-recode-twit-75.
[00:44:42.200 --> 00:44:43.200]   Udacity.
[00:44:43.200 --> 00:44:49.520]   And in fact, the next room is my son, Jake, who's buffing up his skills right now in
[00:44:49.520 --> 00:44:51.920]   submitting and security on Udacity.
[00:44:51.920 --> 00:44:52.920]   Nice.
[00:44:52.920 --> 00:44:53.920]   You're kidding.
[00:44:53.920 --> 00:44:54.920]   Nice.
[00:44:54.920 --> 00:44:55.920]   Well, there you go.
[00:44:55.920 --> 00:44:56.920]   There you go.
[00:44:56.920 --> 00:44:57.920]   Prove.
[00:44:57.920 --> 00:44:58.920]   Positive.
[00:44:58.920 --> 00:44:59.920]   Unsolicited recommendation.
[00:44:59.920 --> 00:45:01.680]   Good for Jake.
[00:45:01.680 --> 00:45:02.920]   He's a smart guy.
[00:45:02.920 --> 00:45:06.240]   He was writing Facebook apps in high school.
[00:45:06.240 --> 00:45:08.200]   Very smart guy.
[00:45:08.200 --> 00:45:13.520]   So this is related to this because it looks like the Biden administration.
[00:45:13.520 --> 00:45:18.960]   This is, I think, of all the things Congress and the president agree on.
[00:45:18.960 --> 00:45:20.360]   They agree on it for different reasons.
[00:45:20.360 --> 00:45:25.280]   But one of the only things they agree on is big tech's got to be taken down a notch.
[00:45:25.280 --> 00:45:36.440]   Today, Linda Kahn, who is the youngest ever FTC chair, she got approved by the Senate.
[00:45:36.440 --> 00:45:40.760]   I don't think the Senate's had a 69 to 28 vote on anything yet.
[00:45:40.760 --> 00:45:45.880]   It was a cross-party lines, but obviously some people broke with the party.
[00:45:45.880 --> 00:45:50.720]   32 years old that she was sworn in yesterday making her the youngest chair in the FTC's
[00:45:50.720 --> 00:45:52.920]   history.
[00:45:52.920 --> 00:46:00.760]   But one of the reasons the Senate could agree, she's highly critical of big tech filed an
[00:46:00.760 --> 00:46:06.160]   antitrust lawsuit against Facebook last year.
[00:46:06.160 --> 00:46:10.640]   This is a clear shot across the bow.
[00:46:10.640 --> 00:46:16.360]   The FTC is going to be taking a big tech very seriously.
[00:46:16.360 --> 00:46:23.360]   She will probably be a regulator when it comes to Amazon, Facebook, Google and Apple.
[00:46:23.360 --> 00:46:26.480]   She told the Senate committee during her hearings in April that she was worried about the way
[00:46:26.480 --> 00:46:32.600]   tech companies could use their power to dominate new markets.
[00:46:32.600 --> 00:46:41.880]   Anyway, this is what scares me a little bit because while it's hard to think of how you
[00:46:41.880 --> 00:46:49.080]   solve the problem of amplification on Facebook and Twitter and social media, it's also clear
[00:46:49.080 --> 00:46:57.880]   that the government wants, for very different reasons, would like to spank these guys big
[00:46:57.880 --> 00:47:01.880]   time.
[00:47:01.880 --> 00:47:05.560]   Do you think government has a role-jone in solving these problems?
[00:47:05.560 --> 00:47:08.040]   I know you think librarians do.
[00:47:08.040 --> 00:47:09.360]   How about government?
[00:47:09.360 --> 00:47:14.800]   If I say it out loud, Jeff is literally going to drive to my house and set it on fire.
[00:47:14.800 --> 00:47:15.800]   Oh, no.
[00:47:15.800 --> 00:47:17.640]   So I will tread lightly here.
[00:47:17.640 --> 00:47:21.240]   No, no, no, no, no, fret not lightly.
[00:47:21.240 --> 00:47:24.000]   You just said I hate to.
[00:47:24.000 --> 00:47:25.000]   I have cats.
[00:47:25.000 --> 00:47:26.000]   Thank you.
[00:47:26.000 --> 00:47:27.000]   I'm not sure.
[00:47:27.000 --> 00:47:28.000]   He's not stepping on anybody's tail.
[00:47:28.000 --> 00:47:29.000]   Oh, very good.
[00:47:29.000 --> 00:47:30.000]   Very good.
[00:47:30.000 --> 00:47:34.000]   Yeah, so I do.
[00:47:34.000 --> 00:47:41.240]   I say that with a smirk in the sense that I think there's things that government could
[00:47:41.240 --> 00:47:43.200]   encourage.
[00:47:43.200 --> 00:47:48.760]   So there's a few things that I think we actually need some rules about, like harassment and
[00:47:48.760 --> 00:47:55.480]   incitement online where we're in this moment right now where we're trying to learn about
[00:47:55.480 --> 00:48:01.800]   January 6, we're trying to understand the degree to which what happened online was a
[00:48:01.800 --> 00:48:10.920]   big motivator that really encouraged and incited massive coordinated violence, which
[00:48:10.920 --> 00:48:13.200]   was about overthrowing the government.
[00:48:13.200 --> 00:48:14.920]   And so that's one thing.
[00:48:14.920 --> 00:48:21.800]   And then we also have a serious problem with harassment, especially of journalists who
[00:48:21.800 --> 00:48:29.040]   if you are a journalist that covers women's rights, for instance, especially in the healthcare
[00:48:29.040 --> 00:48:35.400]   realm, you have a whole squad of trolls that will spend every waking hour trying to get
[00:48:35.400 --> 00:48:38.120]   you to shut your account down.
[00:48:38.120 --> 00:48:45.160]   Myself included, you know, when I talk about white supremacy online, there's a bunch of
[00:48:45.160 --> 00:48:49.880]   white identarian, white supremacist, white nationalist, whatever you want to call them.
[00:48:49.880 --> 00:48:51.320]   That will attack me.
[00:48:51.320 --> 00:48:56.280]   And it doesn't stop at, oh, I'm going to send her some gross pictures.
[00:48:56.280 --> 00:49:02.160]   It really gets creepy when they're posting your address or they're trying to dig and
[00:49:02.160 --> 00:49:05.160]   find information about you when they're doxing you.
[00:49:05.160 --> 00:49:12.880]   And so there's things that we need some pretty clear rules about and that I think government
[00:49:12.880 --> 00:49:18.520]   can help with that, at least in figuring out like where the bright lines are.
[00:49:18.520 --> 00:49:24.120]   The difficult thing I think is that social media has been built in a certain way.
[00:49:24.120 --> 00:49:26.160]   We're used to it.
[00:49:26.160 --> 00:49:33.760]   Any kind of policy that's going to be coming out of government is going to be almost impossible
[00:49:33.760 --> 00:49:41.320]   to implement because we don't have any transparency or rules about what it means to do oversight
[00:49:41.320 --> 00:49:44.120]   of social media platforms.
[00:49:44.120 --> 00:49:50.000]   But there's some really great research and attention to this by Spencer Overton to suggest
[00:49:50.000 --> 00:49:57.600]   that there are serious civil rights implications for the way that advertising online is designed,
[00:49:57.600 --> 00:50:02.720]   especially when it comes to the way in which you're allowed to select audiences on social
[00:50:02.720 --> 00:50:10.560]   media, Facebook in particular, where if it comes down to it and you're not shown specific
[00:50:10.560 --> 00:50:20.120]   kinds of ads that have to do with housing, jobs, and credit or financial opportunities,
[00:50:20.120 --> 00:50:23.440]   then those are actually considered civil rights violations.
[00:50:23.440 --> 00:50:28.520]   So I think we do need some laws that open up transparency around that.
[00:50:28.520 --> 00:50:35.520]   And then to get back to this internet that we want, I do think the government could also
[00:50:35.520 --> 00:50:40.760]   encourage, like they did with radio, a kind of public interest obligation that would
[00:50:40.760 --> 00:50:50.720]   ensure that when platform companies are serving information to, let's say, 10 million people,
[00:50:50.720 --> 00:50:54.400]   let's make it a pretty high bar there.
[00:50:54.400 --> 00:50:59.600]   When you have 10 million users that every once in a while, you've got to sprinkle in
[00:50:59.600 --> 00:51:04.400]   information about the weather, local news, you know, it's an interesting idea.
[00:51:04.400 --> 00:51:08.800]   Council meetings, just because that's what we expect of radio.
[00:51:08.800 --> 00:51:14.520]   And the pandemic really taught me a lot about that and about the need for timely, accurate,
[00:51:14.520 --> 00:51:19.960]   local knowledge to be circulated on these platforms because nobody's going to do it.
[00:51:19.960 --> 00:51:27.240]   FCC regulates that because the spectrum that's used by broadcasters is public property.
[00:51:27.240 --> 00:51:28.240]   But I think you're right.
[00:51:28.240 --> 00:51:31.440]   I think it's high time we realize the internet is public property.
[00:51:31.440 --> 00:51:32.640]   Well, that's it.
[00:51:32.640 --> 00:51:33.640]   I was fine.
[00:51:33.640 --> 00:51:34.640]   There's no scarcity.
[00:51:34.640 --> 00:51:36.800]   There's a scarcity of spectrum.
[00:51:36.800 --> 00:51:39.400]   Unfortunately, there's no scarcity of bits.
[00:51:39.400 --> 00:51:44.280]   But I think you could say that there's a public interest in something like that.
[00:51:44.280 --> 00:51:45.280]   Yes.
[00:51:45.280 --> 00:51:46.920]   But I'm not sure that's the right way to attack.
[00:51:46.920 --> 00:51:49.160]   I mean, Jones, Jones, cats are safe.
[00:51:49.160 --> 00:51:50.160]   Leave me there safe.
[00:51:50.160 --> 00:51:52.000]   I'm just kidding, Jeff.
[00:51:52.000 --> 00:51:54.000]   I had to set up a straw man.
[00:51:54.000 --> 00:51:55.000]   I know.
[00:51:55.000 --> 00:51:56.000]   For a straw cat.
[00:51:56.000 --> 00:51:57.000]   Yes.
[00:51:57.000 --> 00:51:59.640]   I was in the Wizard of Oz.
[00:51:59.640 --> 00:52:03.560]   And no, the more like a 10 would Joe knows.
[00:52:03.560 --> 00:52:08.480]   The transatlantic high level working group content moderation freedom of expression organized
[00:52:08.480 --> 00:52:13.880]   by Susan Ness that Joan knows because she spoke with us basically endorses that view.
[00:52:13.880 --> 00:52:15.480]   And it comes in two parts.
[00:52:15.480 --> 00:52:21.640]   First, we need the data to have the research to know what the impact is to design the interventions
[00:52:21.640 --> 00:52:25.400]   appropriate to what's actually going on rather than what's being assumed, which is too often
[00:52:25.400 --> 00:52:26.400]   the case.
[00:52:26.400 --> 00:52:32.200]   And the second part of that is that the companies need to make-- this is my word.
[00:52:32.200 --> 00:52:36.000]   A covenant with the public to which they are held to account.
[00:52:36.000 --> 00:52:40.560]   And rather than the government saying, you should all do this, it's not appropriate to
[00:52:40.560 --> 00:52:42.520]   different platforms in different ways.
[00:52:42.520 --> 00:52:43.960]   But they should have.
[00:52:43.960 --> 00:52:45.880]   And I argued with this with the platform directly.
[00:52:45.880 --> 00:52:47.720]   I've said that they need constitutions.
[00:52:47.720 --> 00:52:48.720]   They need covenants.
[00:52:48.720 --> 00:52:51.400]   They need some statement that says, why are we here?
[00:52:51.400 --> 00:52:52.960]   Why is the world better with us?
[00:52:52.960 --> 00:52:55.240]   What do you hold us to account for?
[00:52:55.240 --> 00:52:59.680]   Then a federal trade commission structure comes right into place because the federal
[00:52:59.680 --> 00:53:02.280]   trade commission doesn't tell you exactly how to make this phone.
[00:53:02.280 --> 00:53:06.280]   But if you lie about this phone and say that it can cure cancer, then they're going to
[00:53:06.280 --> 00:53:07.280]   come after you.
[00:53:07.280 --> 00:53:12.080]   If you lie and say, it's 5G, there's no such thing as 5G, they're going to come after
[00:53:12.080 --> 00:53:16.320]   you because you didn't hold up to the standard that you set for yourself.
[00:53:16.320 --> 00:53:18.680]   You understand that that's not what we need to have.
[00:53:18.680 --> 00:53:22.960]   I mean, the Republicans are upset because President Trump was banned.
[00:53:22.960 --> 00:53:24.800]   And so that's what they're going down.
[00:53:24.800 --> 00:53:27.880]   The anti-conservative bias of these.
[00:53:27.880 --> 00:53:33.120]   And David Sisolini of the House Any Trust Committee says Apple should not be allowed
[00:53:33.120 --> 00:53:37.560]   to preload its apps on the iPhone.
[00:53:37.560 --> 00:53:41.920]   So that's really, that's unfortunately, that's the level of-
[00:53:41.920 --> 00:53:42.920]   That's the proper regulation.
[00:53:42.920 --> 00:53:48.680]   Or the latest legislation that being proposed now to forbid all M&A, which is just ridiculous.
[00:53:48.680 --> 00:53:50.160]   Yeah, that doesn't make sense.
[00:53:50.160 --> 00:53:52.520]   Or break them up for the sake of breaking them up.
[00:53:52.520 --> 00:53:53.520]   What does that actually accomplish?
[00:53:53.520 --> 00:53:54.520]   What's the goal?
[00:53:54.520 --> 00:53:56.880]   What are we trying to design here?
[00:53:56.880 --> 00:53:57.880]   There's no design in that.
[00:53:57.880 --> 00:54:00.520]   Again, if Joe wants to charge it, it will work.
[00:54:00.520 --> 00:54:08.840]   In the break them up world, I think from my experience, when you think about Glass-Steagall,
[00:54:08.840 --> 00:54:14.280]   for instance, which was a law in the 30s where you broke up banks so that investment
[00:54:14.280 --> 00:54:17.880]   banking and people's everyday money were separate.
[00:54:17.880 --> 00:54:23.880]   And I think there is a way to, in a need for us to really look at the tech stack when
[00:54:23.880 --> 00:54:32.680]   it comes to the internet and start to devise very important conflicts of interest so that
[00:54:32.680 --> 00:54:39.400]   we understand that if you're in data harvesting and in advertising, you can really participate
[00:54:39.400 --> 00:54:40.880]   in price fixing.
[00:54:40.880 --> 00:54:41.880]   And so we don't want that.
[00:54:41.880 --> 00:54:44.800]   We don't want companies to be able to do price fixing.
[00:54:44.800 --> 00:54:51.320]   But because we lack any kind of oversight, and I've heard people flout an internet protection
[00:54:51.320 --> 00:54:56.320]   agency or some kind of agency that would help us devise these rules, there's something
[00:54:56.320 --> 00:55:02.680]   about the business model here that is really hard to get at, especially when you look at
[00:55:02.680 --> 00:55:07.920]   internet advertising and you realize what a tangled mess it is and how much fraud there
[00:55:07.920 --> 00:55:10.840]   is and how much money is lost every year.
[00:55:10.840 --> 00:55:14.840]   And so I think it doesn't necessarily address the disinformation problem, but I do think
[00:55:14.840 --> 00:55:22.360]   there are other problems related to money, market share and data capture that we do need
[00:55:22.360 --> 00:55:25.080]   to interrogate very seriously.
[00:55:25.080 --> 00:55:29.560]   I don't think anybody who disagrees that Glass-Steagall should be reinstated.
[00:55:29.560 --> 00:55:35.360]   Of course it was thrown out in 1999 under the Clinton administration and lo and behold,
[00:55:35.360 --> 00:55:44.560]   nine years later, banks that had invested in crappy real estate almost collapsed.
[00:55:44.560 --> 00:55:49.520]   However, notice we don't really have it.
[00:55:49.520 --> 00:55:53.080]   So I don't know, I despair.
[00:55:53.080 --> 00:55:54.640]   Maybe I shouldn't.
[00:55:54.640 --> 00:56:00.040]   I feel like a lot of what you're proposing was great and is nuanced beyond the ability
[00:56:00.040 --> 00:56:01.040]   of our Congress.
[00:56:01.040 --> 00:56:02.040]   Of the leaders.
[00:56:02.040 --> 00:56:03.040]   Yes.
[00:56:03.040 --> 00:56:04.040]   Yes.
[00:56:04.040 --> 00:56:05.040]   That's all.
[00:56:05.040 --> 00:56:12.840]   I listen, I learn from the smartest people in government are these state legislative directors
[00:56:12.840 --> 00:56:16.160]   and the people that are really just on it.
[00:56:16.160 --> 00:56:20.480]   The people who scare me the most in government though are the federal elections folks that
[00:56:20.480 --> 00:56:26.960]   know all the rules about what you can and can't say and how warm the polling place needs
[00:56:26.960 --> 00:56:31.520]   to be and how far away your dog needs to be from the entrance.
[00:56:31.520 --> 00:56:33.600]   Those people scare me.
[00:56:33.600 --> 00:56:38.680]   I learn a lot from state legislative directors because they're the ones that have to write
[00:56:38.680 --> 00:56:43.520]   the bills and figure out how they would be implemented and narrow cast them in ways.
[00:56:43.520 --> 00:56:50.840]   I'll tell you, the last Senate hearing that I was on was really about looking at the way
[00:56:50.840 --> 00:56:56.920]   in which people become, they were using a framework of addicted to the internet or they
[00:56:56.920 --> 00:57:00.600]   become addicted to the technology and social media.
[00:57:00.600 --> 00:57:03.080]   I think that that is something we cannot legislate.
[00:57:03.080 --> 00:57:05.680]   I don't think there's any there there.
[00:57:05.680 --> 00:57:06.680]   Right.
[00:57:06.680 --> 00:57:12.280]   Well, you know, it's like, yeah, well, actually, it's funny.
[00:57:12.280 --> 00:57:19.680]   You should say I got shut down by a representative Kennedy who just was like, for my bill, I
[00:57:19.680 --> 00:57:21.080]   was like, I don't vote on bills.
[00:57:21.080 --> 00:57:23.760]   Like, I don't know if you know, I don't work here.
[00:57:23.760 --> 00:57:26.760]   I was like, you know, I don't know.
[00:57:26.760 --> 00:57:28.680]   How about this?
[00:57:28.680 --> 00:57:33.280]   Can you change the orbit of the either the earth or the moon to fix climate change?
[00:57:33.280 --> 00:57:34.280]   How about that?
[00:57:34.280 --> 00:57:35.280]   Can you do that?
[00:57:35.280 --> 00:57:36.280]   I will read your bill.
[00:57:36.280 --> 00:57:40.680]   But I think the main point that I really wanted to get across in that Senate hearing
[00:57:40.680 --> 00:57:47.400]   was that disinformation is now a feature of our political environment.
[00:57:47.400 --> 00:57:52.640]   When I say disinformation, I'm saying something very specific about the actors, behavior,
[00:57:52.640 --> 00:57:56.960]   content and design of the social media ecosystem.
[00:57:56.960 --> 00:58:01.640]   And it's because my team, you know, when we write up our research, which it's all open
[00:58:01.640 --> 00:58:06.400]   access on medium manipulation.org, we just see patterns.
[00:58:06.400 --> 00:58:07.760]   What we talk about are patterns.
[00:58:07.760 --> 00:58:13.800]   We look at, you know, where the technical vulnerabilities are, where the social vulnerabilities are.
[00:58:13.800 --> 00:58:15.360]   And we just write them up.
[00:58:15.360 --> 00:58:18.200]   And I don't want to do this forever.
[00:58:18.200 --> 00:58:21.240]   I don't want to be able to write about the same thing over and over.
[00:58:21.240 --> 00:58:24.520]   Even with the journalists on this beat, they were like, we are so tired of writing the
[00:58:24.520 --> 00:58:32.440]   same story that a bunch of people that shouldn't be influencing X, Y, and Z social issue, manipulated
[00:58:32.440 --> 00:58:33.600]   social media.
[00:58:33.600 --> 00:58:35.760]   And now they're the talk of the town.
[00:58:35.760 --> 00:58:42.200]   And so, you know, there's a whole like way in which we can learn from this field.
[00:58:42.200 --> 00:58:49.040]   But by and large, government has got to act as a pressure lever on to get these techna
[00:58:49.040 --> 00:58:54.240]   companies to build their technologies differently and to safeguard them.
[00:58:54.240 --> 00:58:59.480]   But one of my favorite stories that we can and I can make it up because it's in German.
[00:58:59.480 --> 00:59:03.320]   And I actually put it on the rundown is that great.
[00:59:03.320 --> 00:59:05.320]   Thanks a lot.
[00:59:05.320 --> 00:59:10.240]   Germany worked with Google to put up health information, right?
[00:59:10.240 --> 00:59:12.960]   And we think, Oh, that's a good thing.
[00:59:12.960 --> 00:59:16.320]   But the media regulator said, No, no, no, you're hurting the newspapers.
[00:59:16.320 --> 00:59:17.320]   You can't do that.
[00:59:17.320 --> 00:59:18.320]   Oh, man.
[00:59:18.320 --> 00:59:20.120]   They got it taken down.
[00:59:20.120 --> 00:59:26.640]   So all of our discussion earlier about about about expertise and valuable information and
[00:59:26.640 --> 00:59:28.560]   serving people in the public interest.
[00:59:28.560 --> 00:59:29.800]   And here was an effort to do it.
[00:59:29.800 --> 00:59:33.200]   But the economic back to Ann's point, the money came in.
[00:59:33.200 --> 00:59:36.160]   Serving people in the interest of profit.
[00:59:36.160 --> 00:59:37.160]   Right.
[00:59:37.160 --> 00:59:42.000]   And they're in league with government, the media regulator sided with media against
[00:59:42.000 --> 00:59:43.000]   the public.
[00:59:43.000 --> 00:59:45.000]   That's too bad.
[00:59:45.000 --> 00:59:50.680]   But the other Joan Joan, I think, so presumes this.
[00:59:50.680 --> 00:59:55.120]   I think one of the most important things that she's I'm sure quite tired of a banging
[00:59:55.120 --> 00:59:59.880]   this drum, but it's the most important drum of all, is that we can't get enough data
[00:59:59.880 --> 01:00:02.320]   from the platform so that the Joan Donovan's.
[01:00:02.320 --> 01:00:03.320]   That's a good point.
[01:00:03.320 --> 01:00:07.680]   There are precious few of them can research and tell us what actually goes on.
[01:00:07.680 --> 01:00:11.720]   I've talked about the book, our filter bubble is real by Axel Brun's on this show before
[01:00:11.720 --> 01:00:14.680]   where we presume something.
[01:00:14.680 --> 01:00:19.240]   We then beat the walls and say, we must change this.
[01:00:19.240 --> 01:00:23.720]   This is horrible without the data and without the research that enables us to know what's
[01:00:23.720 --> 01:00:26.280]   actually truly happening.
[01:00:26.280 --> 01:00:29.920]   And we need to have more discussion about what do we actually know, what do we need to
[01:00:29.920 --> 01:00:32.280]   know, and how do we get the data to know it.
[01:00:32.280 --> 01:00:39.720]   And that's where I trust Joan and the people she trusts in turn, that network of researchers,
[01:00:39.720 --> 01:00:46.760]   that's the essence of understanding what we need to do about the internet.
[01:00:46.760 --> 01:00:50.440]   Let's take a little break from this powerful conversation.
[01:00:50.440 --> 01:00:53.720]   Great to have Joan Donovan join us.
[01:00:53.720 --> 01:00:56.080]   Stacy, you'll be back, I think, next week.
[01:00:56.080 --> 01:01:00.440]   She's a research director at Harvard Kennedy School and an expert on disinformation in
[01:01:00.440 --> 01:01:02.000]   media.
[01:01:02.000 --> 01:01:06.640]   Jeff Jarvis, who is, of course, Frank Sinatra called him a bomb.
[01:01:06.640 --> 01:01:12.320]   He's a female former team e-guide and the director of the Townite Center for Entrepreneurial Journalism
[01:01:12.320 --> 01:01:15.600]   at CUNY's Craig Newmark Graduate School of Journalism.
[01:01:15.600 --> 01:01:17.360]   And Pruitt from Hands on Photography.
[01:01:17.360 --> 01:01:22.760]   I think we should know, Burke's filling in for John, so I want to give you plenty of
[01:01:22.760 --> 01:01:24.080]   time.
[01:01:24.080 --> 01:01:29.080]   I think we should play the drums and announce that it's time for...
[01:01:29.080 --> 01:01:32.160]   The Google channel.
[01:01:32.160 --> 01:01:33.800]   Absolutely beautiful.
[01:01:33.800 --> 01:01:37.160]   Mr. Burke, Raveau, Golf Clap.
[01:01:37.160 --> 01:01:40.080]   Golf Clap.
[01:01:40.080 --> 01:01:41.840]   So we do this every episode, Joan.
[01:01:41.840 --> 01:01:43.400]   Just some new stuff from Google.
[01:01:43.400 --> 01:01:48.160]   Actually, it's a very interesting change log this week because Google has decided to
[01:01:48.160 --> 01:01:52.520]   let Google Workspace, which is their form, you know, used to be G Suite, it's their name
[01:01:52.520 --> 01:01:56.440]   for their business, paid business product, now should be open to everyone.
[01:01:56.440 --> 01:02:00.040]   And they're going to start with Google Chat.
[01:02:00.040 --> 01:02:04.920]   This is a big deal we talked about it on Windows Weekly even because it's a shot across the
[01:02:04.920 --> 01:02:07.520]   bow for Microsoft and Microsoft's office.
[01:02:07.520 --> 01:02:09.120]   Isn't it already opened to everyone?
[01:02:09.120 --> 01:02:12.000]   You didn't mean for free?
[01:02:12.000 --> 01:02:13.200]   I don't know what this means.
[01:02:13.200 --> 01:02:15.360]   I think you had to have a business, didn't you?
[01:02:15.360 --> 01:02:18.120]   I don't know exactly how it's different.
[01:02:18.120 --> 01:02:20.520]   Officially available to everyone.
[01:02:20.520 --> 01:02:22.120]   I didn't understand that at all.
[01:02:22.120 --> 01:02:23.120]   Yeah.
[01:02:23.120 --> 01:02:27.160]   Availability to anybody who has a Google account.
[01:02:27.160 --> 01:02:30.840]   I guess, you know, that's a good question.
[01:02:30.840 --> 01:02:33.960]   Yeah, I thought it was available all the time as long as you pay for it.
[01:02:33.960 --> 01:02:36.960]   Not Google Docs, but Workspace, which is different.
[01:02:36.960 --> 01:02:39.240]   Workspace includes, and you do have to pay for it.
[01:02:39.240 --> 01:02:40.240]   I don't know.
[01:02:40.240 --> 01:02:41.520]   Workspace used to be known as...
[01:02:41.520 --> 01:02:43.600]   Ten bucks a month.
[01:02:43.600 --> 01:02:45.440]   So there's a new Google...
[01:02:45.440 --> 01:02:46.800]   No, it's not free.
[01:02:46.800 --> 01:02:51.640]   It's launching a new tier called Google Workspace Individual, $10 a month.
[01:02:51.640 --> 01:02:52.640]   Oh, okay.
[01:02:52.640 --> 01:02:53.640]   You don't...
[01:02:53.640 --> 01:02:55.280]   I guess the difference is you don't have to set up a domain.
[01:02:55.280 --> 01:02:57.680]   Like we use Workspace, but at Twit.tv.
[01:02:57.680 --> 01:02:58.680]   Right.
[01:02:58.680 --> 01:03:02.640]   And because it's a business, then we pay per seat.
[01:03:02.640 --> 01:03:07.080]   Now you can just use your regular, you know, Jeff Jarvis at Gmail.
[01:03:07.080 --> 01:03:08.600]   No, my head's exploding, Liam.
[01:03:08.600 --> 01:03:13.400]   My head's exploding because the only reason I'm on Workspace is because I use my own address.
[01:03:13.400 --> 01:03:17.760]   And because I'm there, it's all the things I scream about in this show in the other week.
[01:03:17.760 --> 01:03:18.760]   Congratulations.
[01:03:18.760 --> 01:03:20.560]   So stay away, people.
[01:03:20.560 --> 01:03:21.560]   Stay away.
[01:03:21.560 --> 01:03:22.560]   I'm learning you.
[01:03:22.560 --> 01:03:26.640]   But wait a minute, but you use Workspace under your...
[01:03:26.640 --> 01:03:27.640]   What address?
[01:03:27.640 --> 01:03:29.840]   You don't use your regular Gmail address for that, right?
[01:03:29.840 --> 01:03:32.080]   I use my domain, my blog domain.
[01:03:32.080 --> 01:03:33.080]   Yeah.
[01:03:33.080 --> 01:03:34.080]   That's the only...
[01:03:34.080 --> 01:03:35.080]   That's the difference.
[01:03:35.080 --> 01:03:36.480]   Now you can do it under your Gmail address.
[01:03:36.480 --> 01:03:37.480]   No, no.
[01:03:37.480 --> 01:03:41.280]   Now it says you don't need to have the address to get all these features that you...
[01:03:41.280 --> 01:03:45.640]   Without requiring that you set up your own domain or a custom email address, that's right.
[01:03:45.640 --> 01:03:47.680]   You can be screwed at your own address.
[01:03:47.680 --> 01:03:48.680]   Right.
[01:03:48.680 --> 01:03:50.880]   That's what I'm saying.
[01:03:50.880 --> 01:03:51.880]   Yes.
[01:03:51.880 --> 01:03:52.880]   Okay.
[01:03:52.880 --> 01:03:53.880]   I guess that's the announcement.
[01:03:53.880 --> 01:03:55.880]   There's a sound bite for the week.
[01:03:55.880 --> 01:03:57.320]   That's the announcement.
[01:03:57.320 --> 01:04:01.000]   They also, which is I think interesting, are going to finally offer a progressive web app
[01:04:01.000 --> 01:04:02.720]   for Google Workspace in September.
[01:04:02.720 --> 01:04:05.560]   You're going to like this on your Chromebook.
[01:04:05.560 --> 01:04:06.560]   You can have these...
[01:04:06.560 --> 01:04:10.920]   Now the Gmail and the Docs and all that will look like an actual desktop app.
[01:04:10.920 --> 01:04:13.200]   Just not just tabs and a browser.
[01:04:13.200 --> 01:04:14.360]   See that too.
[01:04:14.360 --> 01:04:19.080]   I'm so happy in my browser land and the nice aisle of browser do.
[01:04:19.080 --> 01:04:22.080]   But I don't care about these apps you discuss.
[01:04:22.080 --> 01:04:25.640]   One of these apps of which you speak.
[01:04:25.640 --> 01:04:31.520]   Google is adding end-to-end encryption to its messages.
[01:04:31.520 --> 01:04:33.280]   Of course, that's the RCS platform.
[01:04:33.280 --> 01:04:37.640]   Google is now saying is the official messaging platform.
[01:04:37.640 --> 01:04:40.800]   You'll also get suggestions for emoji mashups.
[01:04:40.800 --> 01:04:43.960]   I have to ask Jim what that is.
[01:04:43.960 --> 01:04:45.320]   And there's some other features.
[01:04:45.320 --> 01:04:46.520]   But the end-to-end encryption is...
[01:04:46.520 --> 01:04:50.560]   There's no Android earthquake alerts coming to more countries.
[01:04:50.560 --> 01:04:53.160]   If you're in this list of countries, I guess you have earthquakes.
[01:04:53.160 --> 01:04:59.320]   Turkey, the Philippines, Kazakhstan, the Kyrgyz Republic, Tajikistan, Turkmenistan, and Uzbekistan.
[01:04:59.320 --> 01:05:01.600]   All the stands are getting it.
[01:05:01.600 --> 01:05:06.840]   You can have starred messages if you get sent to text with important info.
[01:05:06.840 --> 01:05:11.720]   You can favorite it, which we'll put it in a category called "starred" so you can find
[01:05:11.720 --> 01:05:13.360]   it faster.
[01:05:13.360 --> 01:05:20.080]   The emoji kitchen is going to let you mash up your emojis.
[01:05:20.080 --> 01:05:22.960]   I don't know how that works.
[01:05:22.960 --> 01:05:27.080]   Did you know that I invented an emoji?
[01:05:27.080 --> 01:05:28.080]   What?
[01:05:28.080 --> 01:05:29.080]   Which one?
[01:05:29.080 --> 01:05:31.680]   Well, funny you should ask.
[01:05:31.680 --> 01:05:33.680]   The beaver emoji.
[01:05:33.680 --> 01:05:35.680]   And yes, I handle it.
[01:05:35.680 --> 01:05:36.680]   Nice beaver.
[01:05:36.680 --> 01:05:37.680]   Yeah, nice beaver.
[01:05:37.680 --> 01:05:38.680]   Exactly.
[01:05:38.680 --> 01:05:39.680]   I just got it stopped.
[01:05:39.680 --> 01:05:40.680]   I was...
[01:05:40.680 --> 01:05:41.680]   Wait a minute.
[01:05:41.680 --> 01:05:42.680]   What?
[01:05:42.680 --> 01:05:46.240]   Are you a fan of the beaver?
[01:05:46.240 --> 01:05:51.320]   Well, what happened was in Jeff knows Jenny ate very well.
[01:05:51.320 --> 01:05:52.320]   So there's a...
[01:05:52.320 --> 01:05:53.320]   Jenny ate, yeah.
[01:05:53.320 --> 01:05:54.320]   I think a country maker.
[01:05:54.320 --> 01:05:55.320]   Yeah.
[01:05:55.320 --> 01:05:56.320]   You know.
[01:05:56.320 --> 01:05:59.360]   And we were having dinner and she was talking about being on the emoji standards council.
[01:05:59.360 --> 01:06:01.760]   And I just was like, wait, how do you do it?
[01:06:01.760 --> 01:06:05.520]   And she was just like, there's an application and you just got to pick something.
[01:06:05.520 --> 01:06:09.560]   This was the hard part is you got to find something that doesn't have an emoji.
[01:06:09.560 --> 01:06:12.880]   It means the same thing in different places.
[01:06:12.880 --> 01:06:15.800]   That is, if people saw it, they'd know what it was.
[01:06:15.800 --> 01:06:19.440]   Beavers are the same all over the world.
[01:06:19.440 --> 01:06:20.440]   Exactly.
[01:06:20.440 --> 01:06:23.200]   But it can't be duplicative of anything else out there.
[01:06:23.200 --> 01:06:26.880]   And so, you know, there was some question as to whether they needed a beaver if there's
[01:06:26.880 --> 01:06:28.760]   already an otter.
[01:06:28.760 --> 01:06:33.160]   And I said, I know exactly what to do here.
[01:06:33.160 --> 01:06:37.720]   And so we wrote up the application and we submitted it.
[01:06:37.720 --> 01:06:39.680]   And it was quite a fun experience.
[01:06:39.680 --> 01:06:42.480]   Me and my spouse teamed up on it.
[01:06:42.480 --> 01:06:43.720]   And it was just fun.
[01:06:43.720 --> 01:06:52.360]   We got to talk about how it's sort of, you know, slang for a woman's parts.
[01:06:52.360 --> 01:06:57.000]   And then we also got to talk about how it's, you know, sports team use it as a logo.
[01:06:57.000 --> 01:07:01.880]   It's an animal that does something really important in the world.
[01:07:01.880 --> 01:07:02.880]   And so, yeah.
[01:07:02.880 --> 01:07:04.960]   So we were just like, people are going to use it.
[01:07:04.960 --> 01:07:08.400]   So now I'm like absolutely horrified at some of the use cases.
[01:07:08.400 --> 01:07:10.480]   I don't know why you're horrified.
[01:07:10.480 --> 01:07:13.440]   I think you do exactly what you were up to.
[01:07:13.440 --> 01:07:14.440]   I know.
[01:07:14.440 --> 01:07:15.440]   I know.
[01:07:15.440 --> 01:07:16.440]   Did I do?
[01:07:16.440 --> 01:07:17.440]   Do you agree?
[01:07:17.440 --> 01:07:18.440]   I'm a jerker at heart.
[01:07:18.440 --> 01:07:22.680]   Do you and your spouse use the beaver emoji frequently and your missives.
[01:07:22.680 --> 01:07:25.520]   We laugh about it more than anything, you know.
[01:07:25.520 --> 01:07:29.520]   It's our version of thumbs up in our chat now.
[01:07:29.520 --> 01:07:31.680]   But it's been fun, you know.
[01:07:31.680 --> 01:07:34.840]   It was a really like learning process.
[01:07:34.840 --> 01:07:35.840]   Tales up.
[01:07:35.840 --> 01:07:36.840]   Yeah.
[01:07:36.840 --> 01:07:37.840]   Yeah, that's sharp.
[01:07:37.840 --> 01:07:38.840]   Beavers don't have thumbs.
[01:07:38.840 --> 01:07:40.840]   They have to use their tails.
[01:07:40.840 --> 01:07:43.240]   Actually Jeremy Burges is on our shows.
[01:07:43.240 --> 01:07:44.240]   Oh, yeah, that's right.
[01:07:44.240 --> 01:07:45.240]   She's a Canadian.
[01:07:45.240 --> 01:07:46.240]   There you go.
[01:07:46.240 --> 01:07:47.240]   Yeah.
[01:07:47.240 --> 01:07:48.240]   Exactly.
[01:07:48.240 --> 01:07:49.240]   Yeah.
[01:07:49.240 --> 01:07:50.240]   So she knows all about that.
[01:07:50.240 --> 01:07:56.040]   Jeremy Burges who's on that Unicode committee for the emoji committee and runs emoji Pedia
[01:07:56.040 --> 01:07:57.200]   is a regular on here.
[01:07:57.200 --> 01:08:01.640]   Whenever the new emojis come out, it's a national holiday.
[01:08:01.640 --> 01:08:02.640]   It's a great year.
[01:08:02.640 --> 01:08:03.640]   Yep.
[01:08:03.640 --> 01:08:04.640]   And so it's great.
[01:08:04.640 --> 01:08:05.640]   It's fun, you know.
[01:08:05.640 --> 01:08:09.880]   And I think people, you know, it's actually kind of a fun thing to have students do as
[01:08:09.880 --> 01:08:15.000]   an activity to learn about internet standards and protocols and why would you need to make
[01:08:15.000 --> 01:08:16.600]   an application for this?
[01:08:16.600 --> 01:08:19.480]   Shouldn't there just be someone out there making emojis, right?
[01:08:19.480 --> 01:08:24.000]   And it's really interesting to reveal the participatoriness of it.
[01:08:24.000 --> 01:08:30.520]   And you know, that was sort of one of the early, you know, features of the web was this
[01:08:30.520 --> 01:08:34.040]   idea that everybody could participate and everybody could contribute.
[01:08:34.040 --> 01:08:38.920]   And I think the emoji standards council still kind of upholds that value.
[01:08:38.920 --> 01:08:44.240]   Well, you'll be glad that the emoji kitchen, which is part of the Google keyboard, will
[01:08:44.240 --> 01:08:48.480]   offer suggestions based on the content of your messages.
[01:08:48.480 --> 01:08:53.280]   So if you get the beaver, you know, you can use it.
[01:08:53.280 --> 01:08:59.960]   And now by the way, and this is again, according to folks at emoji Pedia, the beloved blob emojis
[01:08:59.960 --> 01:09:01.960]   will come back in the form of a movie.
[01:09:01.960 --> 01:09:02.960]   Not so beloved.
[01:09:02.960 --> 01:09:05.160]   No, I don't know why they're beloved.
[01:09:05.160 --> 01:09:06.160]   I don't know.
[01:09:06.160 --> 01:09:07.400]   They're about as beloved as works.
[01:09:07.400 --> 01:09:14.320]   It's like somebody's thumb tip, but they'll be they'll meet back also as part of the impending
[01:09:14.320 --> 01:09:17.560]   suggestion feature.
[01:09:17.560 --> 01:09:18.560]   Okay.
[01:09:18.560 --> 01:09:23.240]   Oh, got a lot more out of that one than I thought.
[01:09:23.240 --> 01:09:26.160]   Let's see where we can go here.
[01:09:26.160 --> 01:09:27.160]   Google meet is added.
[01:09:27.160 --> 01:09:29.240]   This is not going to get your attention.
[01:09:29.240 --> 01:09:33.840]   Google meet is adding a second screen companion mode.
[01:09:33.840 --> 01:09:34.840]   Okay.
[01:09:34.840 --> 01:09:35.840]   Okay.
[01:09:35.840 --> 01:09:39.960]   Sometimes it's hit and miss when you're only on meat.
[01:09:39.960 --> 01:09:41.640]   What is a companion mode do?
[01:09:41.640 --> 01:09:44.000]   What is what it brings a companion to you?
[01:09:44.000 --> 01:09:50.480]   No, no, no, no, it's a second screen so that welcome to meet companion use companion to
[01:09:50.480 --> 01:09:56.360]   enhance your experience and shared conference rooms or when joining calls when joining call
[01:09:56.360 --> 01:09:59.760]   using another device as your primary screen.
[01:09:59.760 --> 01:10:05.080]   So you can I guess have the call on the well, you can have the call on the screen while
[01:10:05.080 --> 01:10:09.480]   you're doing other stuff, which is what we do all the time anyway.
[01:10:09.480 --> 01:10:10.480]   Yeah.
[01:10:10.480 --> 01:10:11.480]   Yeah.
[01:10:11.480 --> 01:10:12.480]   Welcome to the party.
[01:10:12.480 --> 01:10:15.240]   Google Tony buddy.
[01:10:15.240 --> 01:10:18.640]   They're not going to call this a Jeffrey to and feature.
[01:10:18.640 --> 01:10:21.080]   However, oh boy, hubly.
[01:10:21.080 --> 01:10:22.240]   I'm sure there.
[01:10:22.240 --> 01:10:24.080]   I saw him on CNN.
[01:10:24.080 --> 01:10:25.080]   Oh, yeah.
[01:10:25.080 --> 01:10:26.080]   They called him out.
[01:10:26.080 --> 01:10:28.400]   That was, I was painful.
[01:10:28.400 --> 01:10:29.400]   That was painful.
[01:10:29.400 --> 01:10:30.400]   That was painful.
[01:10:30.400 --> 01:10:31.400]   I wonder.
[01:10:31.400 --> 01:10:32.880]   I'm wondering if he'll sue.
[01:10:32.880 --> 01:10:34.560]   No, no, no, it was the deal.
[01:10:34.560 --> 01:10:35.560]   Oh, it was supposed to be in blood.
[01:10:35.560 --> 01:10:36.560]   It's the deal.
[01:10:36.560 --> 01:10:41.080]   The deal is they said, Jeffrey said, can I come back and be a legal expert?
[01:10:41.080 --> 01:10:45.160]   And they said, well, yeah, but you've got to do the tour, the walk of shame first.
[01:10:45.160 --> 01:10:47.920]   And so they had them on, but they boy, that was shame.
[01:10:47.920 --> 01:10:49.520]   Man, it was shame.
[01:10:49.520 --> 01:10:52.280]   Shame back on him.
[01:10:52.280 --> 01:10:53.560]   You just don't like him, Jeff.
[01:10:53.560 --> 01:10:58.480]   I honestly, I feel like, you know, the camera was on.
[01:10:58.480 --> 01:10:59.480]   He forgot.
[01:10:59.480 --> 01:11:07.600]   Yeah, but there's there's there's you think he did on purpose.
[01:11:07.600 --> 01:11:09.080]   I was watching him.
[01:11:09.080 --> 01:11:14.160]   He was so chagrined and embarrassed about the whole thing, as was the anchor who had
[01:11:14.160 --> 01:11:18.960]   to bring the whole thing up and the poor anchor had to ask the question that I felt
[01:11:18.960 --> 01:11:20.160]   like, no, it was an accident.
[01:11:20.160 --> 01:11:23.520]   It was an innocent, not so innocent, but an innocent accident.
[01:11:23.520 --> 01:11:28.920]   Yeah, my opinion has actually very little predates that episode of being crazy.
[01:11:28.920 --> 01:11:29.920]   I know.
[01:11:29.920 --> 01:11:30.920]   Yeah.
[01:11:30.920 --> 01:11:31.920]   Yeah.
[01:11:31.920 --> 01:11:37.640]   I thought, you know, hey, could happen to anyone.
[01:11:37.640 --> 01:11:42.840]   Google is abandoning that which is should be the beginning of every one of the changelogs
[01:11:42.840 --> 01:11:47.000]   and experiment to show simplified domain URLs in.
[01:11:47.000 --> 01:11:52.520]   Apple did this first, right, where they hid everything, but the kind of the nuts.
[01:11:52.520 --> 01:11:54.360]   Yeah, and I hate it.
[01:11:54.360 --> 01:11:56.040]   And I always turn it off on Safari.
[01:11:56.040 --> 01:11:59.080]   And I guess Google thought it was a great idea.
[01:11:59.080 --> 01:12:04.600]   At the time, Google said the reason for doing this was showing the full URL makes it harder
[01:12:04.600 --> 01:12:09.040]   for non-technical users to distinguish between legitimate and malicious phishing sites.
[01:12:09.040 --> 01:12:10.040]   And that is true.
[01:12:10.040 --> 01:12:12.440]   There's a lot of verbiage in that URL.
[01:12:12.440 --> 01:12:16.600]   You might miss the fact that it's, you know, wikipedia.hacker.org.
[01:12:16.600 --> 01:12:17.600]   Yeah.
[01:12:17.600 --> 01:12:21.400]   But what didn't you two like about it?
[01:12:21.400 --> 01:12:30.880]   Well, I like the idea of security experts and end users alike complained about it.
[01:12:30.880 --> 01:12:38.440]   You know, I guess it's because of what we're used to simply hating Chrome's new URL bar
[01:12:38.440 --> 01:12:40.280]   that hides the full URL.
[01:12:40.280 --> 01:12:42.840]   I used to use it to orient myself on the site.
[01:12:42.840 --> 01:12:43.840]   That's true.
[01:12:43.840 --> 01:12:44.840]   Yeah.
[01:12:44.840 --> 01:12:45.840]   Okay.
[01:12:45.840 --> 01:12:46.840]   Like I'm on the home page.
[01:12:46.840 --> 01:12:47.840]   Yeah.
[01:12:47.840 --> 01:12:48.840]   Yeah.
[01:12:48.840 --> 01:12:49.840]   Yeah.
[01:12:49.840 --> 01:12:56.840]   all versions of the Chrome browser. So that is a change that is a disappearance, a removal,
[01:12:56.840 --> 01:13:04.840]   a change of removal killed by Google, killed by Google. Google is letting fiber TV box
[01:13:04.840 --> 01:13:11.880]   customers upgrade to free Chromecast with Google TV. So if you are, I mean, who gets fiber TV?
[01:13:11.880 --> 01:13:17.440]   If you are lucky enough to have Google Fiber, you can now get a free Chromecast. Congratulations.
[01:13:17.440 --> 01:13:24.320]   $30 worth $30. Yeah. It's probably like five people. Yeah. Yeah. Yeah. Google's missing out
[01:13:24.320 --> 01:13:31.640]   on a hundred and fifty bucks. YouTube TV is also giving something away. Free TV stream 4K
[01:13:31.640 --> 01:13:42.240]   or Chromecast with Google TV. Is this for new users only? I don't know. So I guess look for an
[01:13:42.240 --> 01:13:48.280]   email to ensure our loyal YouTube TV members have a great watch experience. We want to give
[01:13:48.280 --> 01:13:56.000]   you a free TV stream 4K device. Nice. How much did TV pay Google for that? Well, Google,
[01:13:56.000 --> 01:14:03.120]   this 95 Google says is because of the competition with Roku. Google TV has got started at 35 bucks,
[01:14:03.120 --> 01:14:12.200]   right? Then it went to 50. I think 65 YouTube TV is well over 80 bucks. 80 bucks?
[01:14:12.200 --> 01:14:17.720]   Now, I believe it was only if you get HBO and some others. I had to drop it. Yeah, it got really
[01:14:17.720 --> 01:14:24.040]   expensive. I can't drop it because I don't know why. I ended up going back to Hulu because of the
[01:14:24.040 --> 01:14:30.000]   price hike. Yes. And I think Hulu gave me a better experience. If you are all into the crypto,
[01:14:30.000 --> 01:14:38.480]   you can now add a Coinbase card to Google Pay and pay with crypto. Nice. I didn't know you can put
[01:14:38.480 --> 01:14:46.680]   crypto on your PayPal now. Yeah, everybody's going. Everybody's going crypto. It's just that it's all
[01:14:46.680 --> 01:14:55.240]   the kids are doing it. And necessarily your wallet, but you get a piece of it. Yeah. Yep. That's the
[01:14:55.240 --> 01:15:01.440]   Google change wrong. That's where you played. You didn't warn him enough. Yeah, he didn't see
[01:15:01.440 --> 01:15:07.600]   it coming. There you go. There you go. That wasn't fair. That wasn't only Mr. Burke. That was not
[01:15:07.600 --> 01:15:12.880]   on you. I have, I know that many of you are looking at me going, that's a mighty nice shirt
[01:15:12.880 --> 01:15:19.800]   you're wearing there, Leo. That's a mighty nice shirt. It's one of my untuckets. I am a bit I so this
[01:15:19.800 --> 01:15:27.120]   is an untucket polo. Untucket shirts are designed to wear untucked. You know, normal shirts are either
[01:15:27.120 --> 01:15:32.120]   too long. You get the weird tails and all that stuff. But who wants you look goofy tuck in a
[01:15:32.120 --> 01:15:37.520]   shirt every time I see my 26 year old, he says, on dad, untuck your shirt. What are you wearing
[01:15:37.520 --> 01:15:43.960]   it tucked in for? If you are looking to to find the perfect father's day gift, this is a gift for
[01:15:43.960 --> 01:15:48.600]   the dad you love a shirt from untucket. They have shirts for all occasions. Now this is because
[01:15:48.600 --> 01:15:54.320]   it's 90 degrees in Petaluma today, I'm wearing the polo, which is by the way very it breathes. It's
[01:15:54.320 --> 01:16:00.480]   nice. It wicks sweat off. It's great. I love the linen shirts. I usually wear those. They have all
[01:16:00.480 --> 01:16:07.160]   kinds of shirts all designed to be worn untucked because, hey, it's a tough style to get right. Untucket
[01:16:07.160 --> 01:16:13.440]   has that just right length. They have many, many different sizes and shapes. So they will all fit
[01:16:13.440 --> 01:16:19.040]   beautifully. Performance shirts like I'm one, I'm wearing that wicks sweats short sleeve button
[01:16:19.040 --> 01:16:25.880]   downs. Actually, I'm wearing a polo made to beat the heat. They have fits for all shapes and sizes
[01:16:25.880 --> 01:16:31.760]   up to triple XL. But even within the sizes, they have slim relaxed and tall fits. I'm wearing relaxed
[01:16:32.040 --> 01:16:37.800]   because I'm a relaxed kind of guy. I don't like to wear a tight shirt. I really don't. But there's
[01:16:37.800 --> 01:16:41.200]   but that's your choice. You see, that's what's great about untucket. And the other thing about
[01:16:41.200 --> 01:16:47.120]   untucket is the quality of the fabrics and stitching is second to none. They really do a great job.
[01:16:47.120 --> 01:16:52.640]   They test every batch of fabric to ensure the best possible quality and consistency. All of their
[01:16:52.640 --> 01:16:57.720]   button downs, their polos or teas, their handlies have that just right length. They fit perfectly.
[01:16:57.960 --> 01:17:02.920]   They have the highest quality control standards in the industry. Every aspect of their shirts
[01:17:02.920 --> 01:17:09.280]   checked five times to ensure they retain their shape, color and strength. And they have excellent
[01:17:09.280 --> 01:17:14.760]   outstanding customer service. This is also, you know, many of us are coming back to the office, we're
[01:17:14.760 --> 01:17:21.280]   opening up on tucket has great work shirts to I have five or six of the wrinkle free collection. I
[01:17:21.280 --> 01:17:24.760]   really like that because I don't have to, you know, it's they're great for travel to you don't
[01:17:24.760 --> 01:17:30.320]   have to iron them short sleeves or long sleeves. There is an untucket store near you there are
[01:17:30.320 --> 01:17:36.440]   85 stores nationwide. But you can also honor order online. They offer free returns and exchanges,
[01:17:36.440 --> 01:17:41.680]   no reason not to give them a try and just take it for me. I love my untucket shirts when I get
[01:17:41.680 --> 01:17:46.960]   home. I wear the casual shirts when I go to work, I wear the dress shirts. It's perfect. Get an
[01:17:46.960 --> 01:17:53.240]   untucket shirt for the dad in your life or the grad. The coo, the to wear under those hot gowns,
[01:17:53.240 --> 01:17:59.520]   you know, wearing one of these nice polos. Use the code TWIT for 20% off your first purchase at
[01:17:59.520 --> 01:18:09.000]   untucket.com untucket.com. And the offer code is TWIT 20% a nice deal. And it helps us because they'll
[01:18:09.000 --> 01:18:19.200]   say, Oh, they saw this on twig offer code TWIT. And the and the website is untucket.com UNT,
[01:18:19.200 --> 01:18:27.960]   UCKIT.com. I'm not tucked in today. I love it. It's cash feels good. It's very I love
[01:18:27.960 --> 01:18:35.720]   mine. Are they nice? They fit you because you have a very your boss. That is that is the thing. I got
[01:18:35.720 --> 01:18:41.680]   one and I was so impressed with the packaging. But it's a nice idea with the quality. Yeah,
[01:18:41.680 --> 01:18:47.960]   let me show you something. Because a lot of people when you buy shirts, you tend to buy them too big.
[01:18:47.960 --> 01:18:52.760]   A lot of times, at least this is what I see. Right. Right. This right here, that little scene goes on
[01:18:52.760 --> 01:18:57.280]   your shoulder. That's where it's supposed to be. Notice that's where it is on the shirt that
[01:18:57.280 --> 01:19:01.880]   actually fits me. That looks like the linen short sleeve linen. Isn't that nice? This is the
[01:19:01.880 --> 01:19:09.320]   linen one I have that was hot today. That is a great shirt. Love it. Well, now we have that's good.
[01:19:09.320 --> 01:19:13.880]   We have some I got it on target. Did you get one? Because I know guess what color I got? You got
[01:19:13.880 --> 01:19:20.760]   one guess. Well, you're a pink oak. So I think you got pink. No, black, of course. They're black
[01:19:20.760 --> 01:19:32.600]   for a dark soul. Black. So dark, so dark, so good news and bad news. The good news is the New
[01:19:32.600 --> 01:19:37.800]   York State Senate has passed electronics right to repair legislation. The bad news is it's highly
[01:19:37.800 --> 01:19:43.800]   unlikely to pass the assembly. So which is ironic, you know, right to repair. Doesn't work in the
[01:19:43.800 --> 01:19:54.920]   assembly get it? 51 senators voting for only 12 against. But they have to the New York
[01:19:54.920 --> 01:20:01.320]   State legislative session ends tomorrow. So they have to work first to be the first of its kind in
[01:20:01.320 --> 01:20:08.520]   the United States, the Digital Fair Repair Act. It's very simple. It makes it requires OEMs like
[01:20:08.520 --> 01:20:13.720]   Apple or John Deere to make available for purposes of diagnosis, maintenance or repair to any
[01:20:13.720 --> 01:20:20.360]   independent repair provider or to the owner of digital electronic equipment manufactured by
[01:20:20.360 --> 01:20:28.280]   our on behalf of our sold by the OEM on fair and reasonable terms, documentation, parts and tools
[01:20:28.280 --> 01:20:32.120]   inclusive of any updates to information or embedded software. That's pretty straightforward.
[01:20:32.680 --> 01:20:39.160]   Got to do it. Got to get past the assembly. It's it's it's nip and tuck right now.
[01:20:39.160 --> 01:20:43.800]   Could gonna be close, but we'll watch that because we think it's important.
[01:20:43.800 --> 01:20:52.120]   Let's see. Podcasts. There's a research story here. There'd be. Oh, let's do that. Yes, please pick
[01:20:52.120 --> 01:20:58.120]   something because I want to make sure Joan now we're gonna find taking advantage of Joan this week.
[01:20:58.760 --> 01:21:04.760]   It was a hell. No, I can't find. Oh, yeah, I know there is. Line 61 61. I love that line. That's one
[01:21:04.760 --> 01:21:11.560]   of my favorite lines using Facebook actually reduces ethnic tension. Of course, Jeff would find this.
[01:21:11.560 --> 01:21:21.240]   People fret about online. But offline echo chambers can be just as strong or stronger.
[01:21:21.240 --> 01:21:27.240]   The study is published by the National Academy of Sciences. Okay, that's respectable.
[01:21:27.880 --> 01:21:35.240]   Uh, it's kind of interesting was conducted in Bosnia and Herzogovina in July. Well,
[01:21:35.240 --> 01:21:40.760]   it was during their week of public remembrance for the genocide of Bosniak Muslims that occurred
[01:21:40.760 --> 01:21:47.800]   in the nineties. Uh, Bosniak Serbian who are largely Slavic and largely Eastern Orthodox and Croatian
[01:21:47.800 --> 01:21:53.160]   who are largely Roman Catholic participants agreed not to use Facebook,
[01:21:54.680 --> 01:21:58.680]   which accounts for roughly 99% of the social media market in the country.
[01:21:58.680 --> 01:22:04.600]   They, uh, you know, after the week was over, researchers surveyed participants about their
[01:22:04.600 --> 01:22:11.400]   attitudes towards those other guys and compared it with a control group that remained active on
[01:22:11.400 --> 01:22:17.400]   Facebook during the period. The offline group had a greater dislike of other ethnicities than the
[01:22:17.400 --> 01:22:22.280]   group that remained online. So I first study think, Joan.
[01:22:22.760 --> 01:22:29.400]   It's hard. This is interesting. I mean, it is hard to measure. And the other thing is,
[01:22:29.400 --> 01:22:35.560]   is the politics are so different in different places, you know, when it comes to how people
[01:22:35.560 --> 01:22:42.280]   perceive each other and what really, um, are the big bright lines, you know, in the, especially
[01:22:42.280 --> 01:22:48.920]   in the U S, of course, like we're in the midst of some very big, you know, back and forth about
[01:22:49.560 --> 01:22:54.760]   education and race. And so it's like, you know, there's a lot of othering going on and a lot of
[01:22:54.760 --> 01:23:01.000]   demonization. I mean, like look at Q and on. They literally think Democrats are demons. Yeah.
[01:23:01.000 --> 01:23:07.960]   Uh, and so that's why we drink baby's blood. Well, it does do a body good.
[01:23:07.960 --> 01:23:08.920]   Stop it.
[01:23:08.920 --> 01:23:14.920]   I could have had a VA. I just clarify.
[01:23:16.840 --> 01:23:20.200]   I thought you look particularly vital today, Joan.
[01:23:20.200 --> 01:23:25.000]   Yeah. Oh my goodness. Um, but yeah, that's what's interesting here is that, uh,
[01:23:25.000 --> 01:23:29.960]   I don't know if the political polarization holds in other countries. And that's just me kind of
[01:23:29.960 --> 01:23:35.800]   showing my, um, edges here, which is, I don't know enough about the situation to say that they
[01:23:35.800 --> 01:23:40.600]   were asking the right question. I'm glad they picked a place where they'd actually had a genocide
[01:23:40.600 --> 01:23:44.920]   over that. Cause at least you know there used to be some animosity a few years ago.
[01:23:45.560 --> 01:23:50.840]   Uh, the results they say were most pronounced when participants lived in homogeneous neighborhoods,
[01:23:50.840 --> 01:23:55.880]   where there was just one ethnicity. Facebook was essentially the only place these people were
[01:23:55.880 --> 01:23:59.960]   likely to encounter someone who belonged to a different ethnicity. That kind of makes sense.
[01:23:59.960 --> 01:24:04.680]   That's what our hope was for the internet that would, you know, bring people together,
[01:24:04.680 --> 01:24:11.320]   that it's hard to hate somebody when you know them. Um, well, I mean, that's, you know, there was a,
[01:24:11.320 --> 01:24:17.800]   I mean, there was a horrible hope study about this, but you know, when you think back to what made, um,
[01:24:17.800 --> 01:24:26.200]   LGBT issues prime for a, a very revolutionary change in marriage laws,
[01:24:26.200 --> 01:24:32.360]   were that people had gay family members, gay colleagues, and that it had become normal
[01:24:32.360 --> 01:24:39.240]   to know gay people. And it wasn't such a scary proposition. Um,
[01:24:40.200 --> 01:24:44.280]   that's, but that's got to go. There was a hoax study that was done where this grad student
[01:24:44.280 --> 01:24:49.880]   lied about all his data, but it's, there's other studies out there. But if people were to look up,
[01:24:49.880 --> 01:24:55.320]   you know, uh, the research on this, they were definitely run into this, um, scam study,
[01:24:55.320 --> 01:25:00.680]   unfortunately, cause they really over exaggerated what it means to, to know the, know someone else.
[01:25:00.680 --> 01:25:07.240]   But, um, that's, that's from my own background. I, you know, when it comes to studies, it's like,
[01:25:07.240 --> 01:25:10.920]   a, you know, you always remember the anomaly that doesn't prove the rule.
[01:25:10.920 --> 01:25:16.120]   I did, I did find it interesting because I grew up in an era where people weren't even out, right?
[01:25:16.120 --> 01:25:21.480]   And, uh, and then there was a stone wall and people started to say, you know, we're here,
[01:25:21.480 --> 01:25:27.880]   we're queer, you know, it's good. And, uh, because, because we've always been surrounded by gay people.
[01:25:27.880 --> 01:25:32.920]   But the problem was you didn't know that uncle John and his long pal that they've been living
[01:25:32.920 --> 01:25:38.680]   together for years, uh, were actually married basically. And you, but once you kind of started
[01:25:38.680 --> 01:25:41.960]   to know that I think you're right, I think that did change. It was the courage of people who,
[01:25:41.960 --> 01:25:46.440]   but it took coming out of the closets to, in which there were tours coming out. I think so. And,
[01:25:46.440 --> 01:25:50.440]   and it was that courage that made it happen. I think that's what I think, but there's that.
[01:25:50.440 --> 01:25:56.840]   And I think also the medicalization, right? The idea that it's not, uh, some kind of medical
[01:25:56.840 --> 01:26:02.920]   disorder really played another important role here, which is to say that, um, you could be gay and
[01:26:02.920 --> 01:26:08.520]   not be somehow, uh, you know, considered mentally ill. And so I think that the activist that really
[01:26:08.520 --> 01:26:15.080]   pushed for that, um, you know, will forever be indebted to. But I do think that there's,
[01:26:15.080 --> 01:26:22.280]   you know, when it comes to fear of the other, um, the internet is so potent when it comes to,
[01:26:22.840 --> 01:26:29.240]   uh, really like trafficking and stereotypes, right? Like with the loss of nuances,
[01:26:29.240 --> 01:26:35.640]   sort of one of the things that social media hat has wrought and, uh, and there's also something on
[01:26:35.640 --> 01:26:41.160]   social media that I, you know, someone should study this, but that kind of moves the fringe into
[01:26:41.160 --> 01:26:46.440]   the mainstream. So things that 10 years ago would have taken a lot longer to become part of the
[01:26:46.440 --> 01:26:53.480]   conversation are so now. And I worry, you know, when people start doing the othering around trans
[01:26:53.480 --> 01:27:01.560]   people right now, um, in particular, because, you know, the, the public discourse is about trans
[01:27:01.560 --> 01:27:07.080]   athletes, but within trans communities, the fight is really about access to metal kick care,
[01:27:07.080 --> 01:27:14.040]   access to job protections and access to the same economic opportunities as other people. So it's,
[01:27:14.040 --> 01:27:20.520]   it's a civil rights struggle from the perspective of trans people, but the national discourse is about
[01:27:20.520 --> 01:27:25.560]   these culture war issues. And so it's really, you know, it's a, it's a hard thing because the,
[01:27:25.560 --> 01:27:31.640]   the social media shaped so much about our political discourse. And then, um, if you don't get a fair
[01:27:31.640 --> 01:27:39.000]   representation of the other, uh, you are inclined to, to fear them. That's, yeah, that's right. So it's
[01:27:39.000 --> 01:27:43.240]   not, it's not, it's not one thing or the other. There are definitely negatives as well. I think
[01:27:43.240 --> 01:27:51.240]   that's a good example of how one side in this is smart enough to game the situation and to change
[01:27:51.240 --> 01:27:56.120]   the dialogue and to say, well, it's really about somebody putting on a wig and competing in women's
[01:27:56.120 --> 01:28:02.600]   sports. It has, which it has nothing to do with, uh, but by sick, by successfully changing the,
[01:28:02.600 --> 01:28:08.120]   the conversation to that, you, you really shut down the conversation about really the real,
[01:28:08.120 --> 01:28:14.200]   very much real issues. I think that are came and there are cynical people on the right who do that
[01:28:14.200 --> 01:28:21.000]   all the time. They're very good at manipulating that. They're gaming it. You know, I think I often
[01:28:21.000 --> 01:28:25.480]   think that the, the phrase climate change was a huge mistake. If you ask somebody if they're in
[01:28:25.480 --> 01:28:30.680]   favor of pollution, they go, no, I hate pollution, but if you call pollution climate change,
[01:28:31.480 --> 01:28:39.400]   it's a lot easier. Right. The reframing is very powerful. Acid rain acid rain. Nobody wants
[01:28:39.400 --> 01:28:46.680]   acid rain. Oh God, let's stop that. Climate change. The Prince song.
[01:28:46.680 --> 01:28:53.540]   Yes. We talked a lot about do not take the Lord's name and
[01:28:53.540 --> 01:29:02.660]   make. All right. New album fans. What do you think about that? Prince has, well, the, of course,
[01:29:02.660 --> 01:29:08.820]   his, uh, his, uh, vaults, his archives are, it's on release materials coming out. Yeah.
[01:29:08.820 --> 01:29:14.260]   He recorded all the time every day, every night he was recording stuff, never released. So the
[01:29:14.260 --> 01:29:19.700]   vaults are loaded there at Paisley Park with new Prince material. So that must make you happy,
[01:29:19.700 --> 01:29:26.500]   Joe. Something new to do. I mean, that's the, the name of the game every, every day you wake up in
[01:29:26.500 --> 01:29:30.500]   the pandemic, you're like, is there one new thing out there? I just want a new Prince album.
[01:29:30.500 --> 01:29:36.580]   Something else I can listen to or welcome to America. It's called, uh, comes out in July.
[01:29:36.580 --> 01:29:43.620]   It's political. Yes. Very interesting. Interesting. Yeah. Yeah. 12 track recorded at Paisley's Park
[01:29:43.620 --> 01:29:52.100]   Studios in 2010. I will. That's like, uh, that's, that's an interesting
[01:29:52.100 --> 01:30:02.100]   moment 2010 too. So, you know, that's post 9/11, but sort of pre Black Lives Matter. And so there's a
[01:30:02.100 --> 01:30:06.980]   lot, there's a lot to discover. You can't play it, you know, because it'll get you banned, right?
[01:30:06.980 --> 01:30:14.100]   But the lyrics in that, in that cut start off very clinically right. Is that which, what's the
[01:30:14.100 --> 01:30:17.220]   first cut that they've released? What's the name of it? Well, it's on that story.
[01:30:17.220 --> 01:30:20.900]   I think. Yeah. Welcome to America. I think it's the
[01:30:20.900 --> 01:30:26.020]   America's name of the album. One of the, uh, one of the songs includes is called Running Game,
[01:30:26.020 --> 01:30:32.580]   Son of a Slave Master, Born to Die. And one day we will all be free. Very interesting.
[01:30:32.580 --> 01:30:38.580]   On the title track, Prince sings distracted by the features of the iPhone, got an application
[01:30:38.580 --> 01:30:44.580]   to fix your situation. Oh, it does sound political. Good. Good. I will be listening to that.
[01:30:44.580 --> 01:30:54.180]   That's, that's back during the, the, the heyday of Blackberry. In 2011, um, the number one ranked
[01:30:54.180 --> 01:30:58.980]   app was the flashlight app. Like people were like, my phone can be a flashlight.
[01:30:58.980 --> 01:31:05.140]   Oh, and if I don't know, you know, Facebook and Twitter, we're not in the top, I don't even
[01:31:05.140 --> 01:31:10.180]   think they were in the top 10 at that point. Probably not. We talk a lot about NFTs. Now,
[01:31:10.180 --> 01:31:16.980]   the founder of the worldwide web is creating an NFT. Tim Berners Lee is going to auction off the
[01:31:16.980 --> 01:31:23.940]   source code for the worldwide web. The first source code he wrote for the worldwide web.
[01:31:24.500 --> 01:31:29.700]   It includes original timestamp files containing the source code and animated visualization of
[01:31:29.700 --> 01:31:37.060]   the code, a letter written by TBL on the code and its creation and a digital poster of the full
[01:31:37.060 --> 01:31:43.380]   code all digitally signed by Tim Berners Lee. And you know, normally I would say, oh, this is
[01:31:43.380 --> 01:31:51.780]   ridiculous. But I think TBL deserves, he's never been able to in any way, make the invention of
[01:31:51.780 --> 01:31:58.100]   the web, make any money on it. So most people have no idea who he is. Yeah, regular people,
[01:31:58.100 --> 01:32:02.580]   I should say deserves it. And you know what, he's not trying to make a killing. They say the
[01:32:02.580 --> 01:32:08.180]   proceeds of the auction will benefit initiatives that he and his wife support. And, uh, and it will
[01:32:08.180 --> 01:32:14.340]   be a starting bid will be a mere $1,000. So they're not trying to cash in, although, who knows?
[01:32:14.340 --> 01:32:20.100]   Do you know how these NFT things work? I have, like, I've tried to figure it out. I've gone down
[01:32:20.100 --> 01:32:24.580]   the rabbit hole on it. It seems pretty complicated. It's very complicated.
[01:32:24.580 --> 01:32:30.180]   Celebrities don't want to do that. So, well, don't you have it minted? That's it. Mr.
[01:32:30.180 --> 01:32:36.500]   Ant-Pruett has an NFT. So tell us if you do you have one out there. If you have one out there,
[01:32:36.500 --> 01:32:41.620]   that's that's it. It's just like putting up art anywhere else. But the problem is just
[01:32:41.620 --> 01:32:45.940]   pretty much a lot. Can you help me figure out how to how to sell the Beaver emoji? Just
[01:32:45.940 --> 01:32:52.500]   like this. Oh, definitely an F T. Yeah. Well, somebody helped me. I don't know. I don't know
[01:32:52.500 --> 01:32:59.300]   what went over here. I'll solve your disinformation problem. You started a little guy. You started
[01:32:59.300 --> 01:33:06.100]   with a piece of art, right? And a photo. Then you go somewhere to turn it into an NFT, right?
[01:33:06.100 --> 01:33:12.500]   That's correct. You turn it you turn it into an NFT, but you have to have a theorem in most cases,
[01:33:12.500 --> 01:33:16.580]   crypto wallet with the theorem, because you got to pay for it to be published. By the way, that's
[01:33:16.580 --> 01:33:24.020]   who's making money on NFTs are the people who mint them. Right. But once you have that,
[01:33:24.020 --> 01:33:29.860]   it's pretty much just like putting your art up anywhere else digitally. The big problem is just
[01:33:29.860 --> 01:33:34.420]   people getting crypto wallets and understanding, Hey, I need to create this wallet and I need to
[01:33:34.420 --> 01:33:39.300]   remember the passwords, write the password down or what have you. But after that, yeah, it's just
[01:33:39.300 --> 01:33:46.100]   like putting up art anywhere else. But basically there's a URL associated with the NFT. And that is
[01:33:46.100 --> 01:33:52.980]   by the way, not a traditional web URL. I think now they're doing IPFS. So that it kind of persists
[01:33:52.980 --> 01:33:59.860]   in case that whatever the site or the address goes down. And then the person owns the URL.
[01:33:59.860 --> 01:34:09.140]   Yes, that piece of art from a digital standpoint, like you were to post it somewhere else online,
[01:34:09.140 --> 01:34:13.060]   like in a tweet or something like that, you, you can't do it.
[01:34:13.060 --> 01:34:19.620]   Did you make your NFT with mine was with open C. Open C. There's open C. There's wearable.
[01:34:19.620 --> 01:34:25.700]   There's mintable. So this is open C. Open S E A dot IO.
[01:34:25.700 --> 01:34:32.900]   The problem right now, though, is the gas fees are really, really, really high because crypto is
[01:34:32.900 --> 01:34:36.340]   sort of down a lot of energy. Yeah. Yeah. So
[01:34:37.460 --> 01:34:41.940]   you set up a long time to do it and create your collection. All right. I'll hold off then.
[01:34:41.940 --> 01:34:47.940]   It's not because I'm lazy. It's actually, I should wait too long because basically NFTs are
[01:34:47.940 --> 01:34:52.420]   like egg salad sitting in the sun. They're not going to. Yeah, it's going to go bad eventually.
[01:34:52.420 --> 01:35:00.980]   I got me here that much longer. Delicious than that. No, never gets more delicious than that. First
[01:35:00.980 --> 01:35:08.500]   day. Actually, speaking of cryptocurrency, Bitcoin is actually getting its first upgrade in four
[01:35:08.500 --> 01:35:14.420]   years. They call it taproot. It will mean greater transaction privacy and officially,
[01:35:14.420 --> 01:35:19.540]   an efficiency and it will unlock the potential for smart contracts.
[01:35:19.540 --> 01:35:28.180]   Who has the right to update Bitcoin? So there used to be a Bitcoin association or a Bitcoin.
[01:35:28.180 --> 01:35:33.940]   There used to be a group. But basically, what you have to do is get the miners to agree.
[01:35:33.940 --> 01:35:39.620]   And there have been attempts to create new Bitcoin code, which have ended up getting forked and
[01:35:39.620 --> 01:35:45.060]   ignored because the miners have to go along. But apparently the miners have agreed, okay,
[01:35:45.060 --> 01:35:51.460]   we can do this. It really aren't there unlimited numbers of miners, including a billion machines
[01:35:51.460 --> 01:35:54.020]   in China that we don't know. Yeah, potentially. Yeah. Yeah.
[01:35:55.540 --> 01:36:01.780]   Bitcoin's makeover has to do with digital signatures. This is from CNBC. The cryptocurrency uses an
[01:36:01.780 --> 01:36:09.220]   elliptic elliptic curve algorithm. Taproot is going to switch over to something known as the
[01:36:09.220 --> 01:36:15.380]   snore signatures, which essentially makes multi signature transactions unreadable.
[01:36:15.380 --> 01:36:22.260]   In other words, greater privacy. Your keys won't be exposed on the chain.
[01:36:23.380 --> 01:36:30.260]   So this is good for good for not anonymity exactly.
[01:36:30.260 --> 01:36:38.260]   So it's complicated. It's complicated. I don't understand at all. It all is. It's all complicated
[01:36:38.260 --> 01:36:42.980]   anyway. It's not for you and me. It's for the young's ass Jake.
[01:36:42.980 --> 01:36:53.300]   Right. Twitter, I think this might be the single best suggestion Twitter has made to
[01:36:53.300 --> 01:36:57.380]   get rid of abuse. I'm curious what Joan thinks of it. The unmention.
[01:36:57.380 --> 01:37:03.300]   Have you heard about this yet, Joan? This is they're trying this right now.
[01:37:03.300 --> 01:37:11.620]   So when you're getting harassed on Twitter, people will use your your handle in the harassment,
[01:37:11.620 --> 01:37:21.220]   right? If you can say in response to that tweet, take me out. It kind of defangs it.
[01:37:23.060 --> 01:37:27.700]   They have to just put your name, not your handle, not your Twitter handle.
[01:37:27.700 --> 01:37:36.180]   I mean, it definitely would get rid of the meme, RIP my mentions, which is like when somebody starts
[01:37:36.180 --> 01:37:43.060]   having a conversation in your, underneath one of a thread where you're mentioned in.
[01:37:43.060 --> 01:37:50.180]   And there had become this, these very almost like chain mail type threads where you would be
[01:37:50.180 --> 01:37:55.460]   within, you know, sometimes it's colleagues and it's great. Other times you're just in this
[01:37:55.460 --> 01:37:59.460]   mismatch of people and you don't even know what the conversation is really, but.
[01:37:59.460 --> 01:38:00.660]   But you're in there.
[01:38:00.660 --> 01:38:02.340]   But I'm not a mention.
[01:38:02.340 --> 01:38:06.500]   I've lost some Jones in there and that you and that's the problem is you can't avoid it.
[01:38:06.500 --> 01:38:08.340]   It's flooding your Twitter.
[01:38:08.340 --> 01:38:10.340]   Exactly. Yeah. There's no way to undo.
[01:38:10.340 --> 01:38:15.140]   I've done the mute conversation a gazillion times because I've gotten
[01:38:15.140 --> 01:38:19.540]   poured into these little things. And when I do that mute conversation,
[01:38:19.540 --> 01:38:22.740]   I don't hear anymore from it. My notifications go back to normal.
[01:38:22.740 --> 01:38:26.580]   Is that something most people can't seem to do anymore?
[01:38:26.580 --> 01:38:32.180]   This is to that. But I think it might also affect search like this to me feels like it would
[01:38:32.180 --> 01:38:39.300]   also mean that if someone were to look up your name, you would have unlinked your name in the
[01:38:39.300 --> 01:38:41.620]   metadata from that. Okay.
[01:38:41.620 --> 01:38:48.020]   Could be good. If like the most, you know, if there's another Ant Pruitt who is, you know,
[01:38:48.900 --> 01:38:50.500]   right is a serial killer.
[01:38:50.500 --> 01:38:53.460]   I want to be able to un mention here and there.
[01:38:53.460 --> 01:38:54.820]   Yeah. Take your name out of it.
[01:38:54.820 --> 01:38:55.620]   Right. Complete.
[01:38:55.620 --> 01:38:56.180]   Yeah. Exactly.
[01:38:56.180 --> 01:38:57.460]   I'd be like wrong and.
[01:38:57.460 --> 01:39:01.460]   Nobody else. I'm happy to have a blue check now.
[01:39:01.460 --> 01:39:01.620]   Yeah.
[01:39:01.620 --> 01:39:04.660]   Twitter. And this is another one I think might work.
[01:39:04.660 --> 01:39:06.980]   They're also, by the way, none of this is official.
[01:39:06.980 --> 01:39:09.380]   They're just trying their early concepts.
[01:39:09.380 --> 01:39:12.820]   Twitter says and they're asking for a user's feedback.
[01:39:12.820 --> 01:39:17.060]   They're also considering adding settings, which would let users restrict certain accounts
[01:39:17.060 --> 01:39:21.060]   from mentioning them entirely. So if you're getting harassed by a particular troll,
[01:39:21.060 --> 01:39:23.380]   you could just say, Hey, you can't use my name.
[01:39:23.380 --> 01:39:28.820]   And they won't be able to mention you anymore, which I think is pretty as a pretty good idea.
[01:39:28.820 --> 01:39:29.780]   Yeah. That's pretty cool.
[01:39:29.780 --> 01:39:32.020]   Yeah. Why should somebody be able to at mention me?
[01:39:32.020 --> 01:39:33.060]   Yeah.
[01:39:33.060 --> 01:39:35.940]   Twitter says it's looking at.
[01:39:35.940 --> 01:39:37.220]   Wait, wait, wait, wait, stay there for a second.
[01:39:37.220 --> 01:39:40.980]   Stay there for a second because it's this discussion I had long ago with somebody who used to be
[01:39:40.980 --> 01:39:48.340]   at Google at the time who argued that email was ruined. We have that discussion around Google+.
[01:39:48.340 --> 01:39:51.860]   Email was ruined when it became sender control rather than recipient controlled.
[01:39:51.860 --> 01:39:53.620]   Yes. Right.
[01:39:53.620 --> 01:40:00.500]   As to what you're saying, basically, is that makes tweets in that form recipient controlled.
[01:40:00.500 --> 01:40:01.300]   A little more.
[01:40:01.300 --> 01:40:01.700]   Yes.
[01:40:01.700 --> 01:40:02.580]   I don't want to be. Yeah.
[01:40:02.580 --> 01:40:04.020]   I don't want to be there. That's very interesting.
[01:40:04.020 --> 01:40:10.500]   You can effect a turn on what TechCrunch calls a piece in quiet mode where you can,
[01:40:10.500 --> 01:40:14.660]   they're looking at this, a switch that could be flipped that says, No one can at mention me
[01:40:14.660 --> 01:40:19.780]   for a period of one day, three days or seven days. So if you are really getting mobbed,
[01:40:19.780 --> 01:40:24.020]   which is not at all unusual, you can just say, Hey, no one can mention me.
[01:40:24.020 --> 01:40:29.460]   I think this all of this is smart. Twitter is finally starting to look at the actual
[01:40:30.100 --> 01:40:36.020]   mechanism of abuse on Twitter. That's Twitter's answer to all this stuff is,
[01:40:36.020 --> 01:40:41.620]   well, just mute them. Just ignore them. Yeah, but everybody else's seen it.
[01:40:41.620 --> 01:40:46.420]   That just makes my life a little easier. It doesn't solve the problem.
[01:40:46.420 --> 01:40:50.980]   These are all good things. I hope Twitter sounds like it's-
[01:40:50.980 --> 01:40:55.300]   Yeah, I'm just guilty of that because I'm one that says just mute them,
[01:40:55.300 --> 01:41:02.100]   but that doesn't really solve the root cause. It's more of a problem for people like Joan,
[01:41:02.100 --> 01:41:05.700]   people in the public eye, where you've got a mob going after you.
[01:41:05.700 --> 01:41:09.700]   Ignoring the mob doesn't mean the mob goes away.
[01:41:09.700 --> 01:41:21.220]   And other people are seeing this. I think Twitter is finally taking abuse a little bit more seriously.
[01:41:21.220 --> 01:41:25.780]   I don't feel like they've really taken it seriously until the last say six months.
[01:41:25.780 --> 01:41:33.940]   So good. I'll give you feedback. Do it. Do it. Do it.
[01:41:33.940 --> 01:41:38.340]   They're all tuned in like the whole work stops.
[01:41:38.340 --> 01:41:41.380]   When the whole office.
[01:41:41.380 --> 01:41:45.220]   I know. I'm really. Yeah. It's time for a twig.
[01:41:45.780 --> 01:41:53.940]   Let's listen. Amazon says, you know, the problem with fake reviews, you know, who's doing that?
[01:41:53.940 --> 01:42:02.420]   Social media. Because apparently fake reviews are solicited on social media platforms.
[01:42:02.420 --> 01:42:08.020]   It's a big problem on Amazon, these fake reviews. Facebook has been repeatedly
[01:42:08.020 --> 01:42:13.620]   Amazon to not name anybody, but Facebook has been repeatedly criticized for not clamping down.
[01:42:14.500 --> 01:42:19.700]   It's another form of what is it in authentic? What is that phrase that they use?
[01:42:19.700 --> 01:42:26.580]   Oh, they use the phrase coordinated inauthentic behavior, but it's, I mean, it's as authentic as,
[01:42:26.580 --> 01:42:31.780]   you know, as everything else. You know, the one thing that's interesting about it is,
[01:42:31.780 --> 01:42:36.420]   is how, you know, Amazon doesn't allow you to coordinate or talk to other people,
[01:42:36.420 --> 01:42:42.020]   except for maybe in comments. But here over on Facebook, there are entire groups where you can
[01:42:42.020 --> 01:42:52.260]   get free products. You can sign up to, you know, essentially, it's a different kind of advertising,
[01:42:52.260 --> 01:42:58.020]   right? It's dark, dark market advertising where, you know, if you're going to buy a,
[01:42:58.020 --> 01:43:04.580]   let's say you're want to buy a new broom and dust pant, are you, you know, they're all kind of the
[01:43:04.580 --> 01:43:10.500]   same. But are you going to buy the one that has a thousand five star reviews? Yes. The one who has
[01:43:10.500 --> 01:43:17.460]   eight reviews, because who reviews a broom and dust pant, right? And so they give away free products
[01:43:17.460 --> 01:43:24.660]   and in exchange, they ask for a review. Sometimes they'll give away actual cash. And so there's a
[01:43:24.660 --> 01:43:31.700]   whole market for this. There's a whole genre of manipulation around Amazon. And, you know,
[01:43:31.700 --> 01:43:37.540]   we've also seen it around, you know, especially around books and things like that where, you know,
[01:43:38.180 --> 01:43:44.820]   it's just kind of hard to get noticed. And so if you have a lot of reviews, you get top or higher
[01:43:44.820 --> 01:43:49.940]   billing, which is why I think, you know, maybe Amazon should also reveal how many of a certain
[01:43:49.940 --> 01:43:57.220]   product have been sold might be a better measure of quality than, than the reviews.
[01:43:57.220 --> 01:44:02.420]   It's, I mean, they definitely have that data because they tend to put out beside the reviews,
[01:44:02.420 --> 01:44:07.060]   a verified purchase. I think that's how they say it. Yeah, that's, that was their first order of
[01:44:07.060 --> 01:44:13.140]   trying to figure that out. But, but we've known about this for years. It's just because it doesn't
[01:44:13.140 --> 01:44:19.860]   really have, you know, I mean, maybe there's, you know, financial implications of it, but because it
[01:44:19.860 --> 01:44:26.020]   doesn't have broad, you know, societal implications, we don't really tend to cover it as a field as
[01:44:26.020 --> 01:44:32.420]   much as probably we should. But it is a form of manipulation, algorithmic manipulation.
[01:44:33.460 --> 01:44:41.300]   Every time I see these interesting DMs or emails, I tend to think about our producer and co-host,
[01:44:41.300 --> 01:44:46.660]   Mr. Jason Howell and Marcus Sergeant, because if I'm getting a ton of them, I can only imagine
[01:44:46.660 --> 01:44:53.140]   the amount of pitches that they're getting to say, Hey, can you go give me a review and check
[01:44:53.140 --> 01:44:59.460]   out this? Oh God, no, no. Toaster. We got offered yesterday. I got a guy who makes rubber boots.
[01:44:59.460 --> 01:45:08.420]   He says, I can supply you with any kind of rubber boot you need. Don't need them. Where you are
[01:45:08.420 --> 01:45:15.460]   in Petaluma. You know what? The biggest one we get now a lot of, and I keep telling my team,
[01:45:15.460 --> 01:45:22.420]   you don't have to answer these is people who want to write paid posts on our, our website. Oh, I
[01:45:22.420 --> 01:45:27.620]   mark those as spam. We get nonstop. Just nonstop. But that's bad. And the worst kind is the one
[01:45:27.620 --> 01:45:32.100]   where they send you one and they send you another one. And they say a third one saying, Hey, didn't
[01:45:32.100 --> 01:45:40.660]   you see my freaking TV? You see it? I'm ignoring you. No, no, no, no, no. Jeff, you love VidCon. Good
[01:45:40.660 --> 01:45:51.140]   news. It's coming back to Anaheim in the fall, October 21 through 24th. It is the, it's, it's the
[01:45:51.140 --> 01:45:56.500]   convention that all the influencers, YouTube stars and everybody goes to, although
[01:45:56.500 --> 01:46:03.540]   reflecting kind of a shift in the industry, the number one sponsor this year is not YouTube.
[01:46:03.540 --> 01:46:10.660]   It's TikTok. Shocking. Just fasting because it used to be basically the YouTube conference.
[01:46:10.660 --> 01:46:17.780]   And Facebook kind of muscle its way in trying to get from cool points as well for a while there
[01:46:17.780 --> 01:46:24.740]   when it had its pivot to video. And now TikTok is the star. You remember in 2019, the last time
[01:46:24.740 --> 01:46:32.420]   they had a live VidCon, TikTok had a big party at VidCon that was so big that a lot of big name
[01:46:32.420 --> 01:46:41.540]   influencers couldn't get in and were pissed. I think, you know, over the pandemic, millennials
[01:46:41.540 --> 01:46:50.100]   really figured it out. And if we can use it, if not post on it. And so the clients
[01:46:50.100 --> 01:46:54.740]   hell and the reach of TikTok has changed. And I don't even know, you know, Jeff, you might know
[01:46:54.740 --> 01:47:00.980]   this better than me. Did they take a significant hit as, you know, Trump was going after bite dance
[01:47:00.980 --> 01:47:06.820]   and was, you know, really making a big deal about where the data lies. I think they grew because of
[01:47:06.820 --> 01:47:12.500]   that. Yeah, I think it went up. I think they grew up. Yeah, because, you know, I mean, I was a big,
[01:47:12.500 --> 01:47:17.460]   I really liked Vine when Vine came out. I love the short video. Talk about Twitter. Hello,
[01:47:17.460 --> 01:47:23.540]   and that's telling me how old you are. Yeah. But yeah, I'm just good. That's good. It's good.
[01:47:23.540 --> 01:47:29.620]   It's a trend. No played. Yeah. Yeah. But there is, you know, there's a serious problem right now.
[01:47:29.620 --> 01:47:36.500]   Maybe I'm breaking news here, which is to say that a couple of young teens have died doing a
[01:47:36.500 --> 01:47:43.380]   blackout challenge on that is that had gone viral on TikTok. And it's, you know, a kind of hold your
[01:47:43.380 --> 01:47:50.660]   breath type challenge. And here in Massachusetts, a young person did, did pass away recently from it.
[01:47:50.660 --> 01:47:57.540]   And so TikTok's got a lot to figure out as a company, especially as these kinds of things where,
[01:47:57.540 --> 01:48:03.540]   you know, they go viral really fast and and teens try them out. They try to stay ahead of the trend.
[01:48:04.580 --> 01:48:10.820]   You know, I think there was a lot of, you know, moral panic around eating tide pods, for instance.
[01:48:10.820 --> 01:48:17.380]   But this, this one is real. Don't hold your breath, kids. Air. It's what's for dinner. You need it.
[01:48:17.380 --> 01:48:22.020]   Yeah. I mean, I remember when I was younger, there was a kind of, you know, people did this
[01:48:22.020 --> 01:48:29.700]   passout challenge. And I'm not unfamiliar with with this in my own youth, but it is a is a really
[01:48:29.700 --> 01:48:36.580]   dangerous setup. So that's, you know, I don't have kids. But if I did, I would be saying to them,
[01:48:36.580 --> 01:48:41.860]   do you see these videos? Don't do that. That's good. Don't do what somebody told you to do.
[01:48:41.860 --> 01:48:46.740]   I assume children listen to their parents, though. That's your, that's clearly,
[01:48:46.740 --> 01:48:52.900]   you have a problem. You don't. You're wrong. But my mom wasn't, you know, on the internet looking
[01:48:52.900 --> 01:48:57.940]   up viral trends and being like, that's terrible. She was out there helping me buy rage against
[01:48:57.940 --> 01:49:03.140]   the machine CDs. Right. Up the counter. And, you know, she was,
[01:49:03.140 --> 01:49:08.660]   her, you know, she was big fan of Janice Joplin stuff. So she's, she was a rocker.
[01:49:08.660 --> 01:49:15.700]   Wow. Oh, yeah. But it is, it's a rough one out there right now for TikTok. And I think that
[01:49:15.700 --> 01:49:20.660]   over the next couple of weeks, if this news does break nationally, it's, it's going to be
[01:49:20.660 --> 01:49:27.380]   difficult for them to weather. They're going to have to put new protocols in place, for sure.
[01:49:28.340 --> 01:49:34.260]   Yeah, that's terrible. I just stick with making food like my son does on his TikTok.
[01:49:34.260 --> 01:49:35.140]   Well, she's latest.
[01:49:35.140 --> 01:49:37.460]   Salt on his skull. Hank. I don't know. Let's see.
[01:49:37.460 --> 01:49:42.660]   Here. Let me, am I getting audio out? He's making looks like he's making onions.
[01:49:42.660 --> 01:49:45.460]   Oh, he's making nachos or something.
[01:49:45.460 --> 01:49:46.020]   I choose.
[01:49:46.020 --> 01:49:47.460]   Do you have that fryer, Hank?
[01:49:47.460 --> 01:49:51.780]   No, I don't, you know what? He goes out and buys more equipment all the time for this.
[01:49:51.780 --> 01:49:53.140]   Oh, gee, I wonder where he got that.
[01:49:55.780 --> 01:49:59.380]   I just wonder what kind of bad influence could there could have been on your son?
[01:49:59.380 --> 01:50:02.260]   I got to get the sound working because a lot of his.
[01:50:02.260 --> 01:50:03.700]   Seviche?
[01:50:03.700 --> 01:50:09.380]   Yeah, it looked like a lot of his thing is sound. Oh, apparently we're, the sound output is
[01:50:09.380 --> 01:50:11.860]   dummy output right now. I don't think dummy output.
[01:50:11.860 --> 01:50:13.540]   I didn't touch it. I didn't touch his.
[01:50:13.540 --> 01:50:15.860]   Yeah. I didn't do it.
[01:50:15.860 --> 01:50:17.140]   Yeah. Yes.
[01:50:17.140 --> 01:50:19.300]   Salt under scrub. I'll give him a plug. Salt under his.
[01:50:19.300 --> 01:50:20.020]   He's on Linux.
[01:50:20.020 --> 01:50:25.300]   Yeah. No, it's not me. It's something we, I probably have to reboot to fix it.
[01:50:25.300 --> 01:50:29.860]   Oh, he made Gilroy Garlic fries the other day that whoo.
[01:50:29.860 --> 01:50:33.460]   Baby. Oh, baby.
[01:50:33.460 --> 01:50:35.140]   He's going to stick for a month.
[01:50:35.140 --> 01:50:40.020]   Notice he's wearing his San Francisco Giants shirt because that's where Gilroy Garlic fries
[01:50:40.020 --> 01:50:45.380]   became famous. It's at the Giants stadium. They are so good. Oh my god. Are they good?
[01:50:45.380 --> 01:50:48.420]   That's a my boy.
[01:50:48.420 --> 01:50:50.820]   For some fries? Come on.
[01:50:50.820 --> 01:50:51.460]   Yeah.
[01:50:51.460 --> 01:50:53.700]   Let's just looks like a ton of work.
[01:50:53.700 --> 01:50:55.380]   Oh, but it's good. It's worth it.
[01:50:55.380 --> 01:50:59.860]   You know, that's why you watch these videos. So you don't have to.
[01:50:59.860 --> 01:51:01.460]   You don't have to.
[01:51:01.460 --> 01:51:04.980]   Also, they condensed it. You know, there's five hours of work becomes 50 seconds.
[01:51:04.980 --> 01:51:09.140]   You work so hard. That was Beth's another of the latest stories about all the social
[01:51:09.140 --> 01:51:12.100]   media is how these guys are getting so burned out. It's so much work.
[01:51:12.100 --> 01:51:18.180]   It's, it's just, it's really, you can't, you know, you got to publish every day.
[01:51:19.140 --> 01:51:24.100]   Like, I don't get a job. Just get a job. Then you go to work and go home. It's over.
[01:51:24.100 --> 01:51:25.700]   Is he making money?
[01:51:25.700 --> 01:51:27.140]   The worst is not yet. Yeah.
[01:51:27.140 --> 01:51:27.620]   Yeah.
[01:51:27.620 --> 01:51:33.140]   No, short making something short is so much harder than anything else.
[01:51:33.140 --> 01:51:38.100]   These are really good editor. So it's you contribute 500 words. I'm like, no.
[01:51:38.100 --> 01:51:41.620]   Yeah. That is way too. I got 3000 words. I'll give it to you later tonight.
[01:51:41.620 --> 01:51:44.340]   500. Forget about it.
[01:51:44.340 --> 01:51:46.980]   Who said I would have written a shorter letter, but I didn't have time.
[01:51:47.860 --> 01:51:49.620]   Exactly. That's twain. Yeah.
[01:51:49.620 --> 01:51:53.220]   Yes. Is it, let me see. Let me look this up.
[01:51:53.220 --> 01:51:54.420]   I could be fake news.
[01:51:54.420 --> 01:51:57.380]   I think it was Pascal, Blaze Pascal.
[01:51:57.380 --> 01:52:03.060]   If I had more time, I would have written it in the chat a shorter. Oh, this is quote
[01:52:03.060 --> 01:52:05.940]   investigator. Let's see. I love quote investigator.
[01:52:05.940 --> 01:52:10.980]   The first known instance was from a text written by Blaze Pascal, the French mathematician and
[01:52:10.980 --> 01:52:19.780]   philosopher in the year 1657. However, however, maybe there are earlier versions. I don't know.
[01:52:19.780 --> 01:52:24.980]   Yeah. I see once Pascal wrote it, everybody else has said it. So, yeah.
[01:52:24.980 --> 01:52:25.700]   It's a good line.
[01:52:25.700 --> 01:52:34.260]   Just a couple of self-serving plugs. If you are listening to this show and you hear ads,
[01:52:34.260 --> 01:52:37.940]   you don't have to. We have an ad free version of all of our shows.
[01:52:37.940 --> 01:52:43.140]   It's part of the benefit of Club Twit. If you want to support what we're doing here at Twit,
[01:52:43.140 --> 01:52:47.780]   seven bucks a month, that's all it costs. You get ad free versions of all our shows,
[01:52:47.780 --> 01:52:51.460]   access to our really fun Discord channel, which has turned out.
[01:52:51.460 --> 01:52:56.180]   Joan, I don't know. I'm sure you're totally hip with a Discord, but I feel like it's the new
[01:52:56.180 --> 01:53:00.740]   social network because you completely control it. I love it. Our Discord is full of great
[01:53:00.740 --> 01:53:06.100]   conversations all the time. You also get the Twit Plus feed of conversations that didn't make
[01:53:06.100 --> 01:53:09.780]   it into the shows. For instance, our pre-show conversation with Joan was really interesting,
[01:53:09.780 --> 01:53:14.180]   maybe that'll make it on the Twit Plus feed. If you'd like to know more about Club Twit,
[01:53:14.180 --> 01:53:23.380]   you want to help us out, we really appreciate it. Go to twit.tv/club.
[01:53:23.380 --> 01:53:26.980]   Twit. You could even put on their yellow yelling at Jeff for screwing up his microphone.
[01:53:26.980 --> 01:53:31.460]   That's fine. I don't think we want to. That was painful. I don't know if we really
[01:53:32.740 --> 01:53:38.660]   can make a TikTok out of it, but that's about it. Let's take a break. When we come back,
[01:53:38.660 --> 01:53:43.300]   I don't know if they warned you, Joan. I don't know if you remember from your previous appearances.
[01:53:43.300 --> 01:53:47.220]   We'd like to do some pics, maybe a book you like, a record you're excited about,
[01:53:47.220 --> 01:53:50.820]   maybe the new print album. What ever you recommend? A paper you recommend,
[01:53:50.820 --> 01:53:58.340]   a Twitter account. You think we should follow that kind of thing. Coming up next, our show
[01:53:58.340 --> 01:54:04.980]   today brought to you by AT&T active armor. We live on our phones these days. We're always on them,
[01:54:04.980 --> 01:54:09.620]   whether live streaming shows like this, catching up with the family on video calls,
[01:54:09.620 --> 01:54:16.820]   watching your favorite TV show. The last thing you want in the middle of any of this is a fraud call.
[01:54:16.820 --> 01:54:23.540]   It's gotten so bad that I actually stopped answering the phone for a while. Fortunately,
[01:54:23.540 --> 01:54:30.900]   AT&T makes customer security a priority and helps block those pesky calls. It's not complicated.
[01:54:30.900 --> 01:54:39.380]   AT&T active armor, 24/7 proactive network security and fraud call blocking to help stop
[01:54:39.380 --> 01:54:48.820]   threats at no extra charge. Combatable device and service required, visit att.com/active armor
[01:54:48.820 --> 01:54:56.020]   for details. Thank you, AT&T. Let's start with Aunt Pruitt, because we'll go backwards this time.
[01:54:56.020 --> 01:55:03.140]   I want to give Jones some time. You're a thing. I got a couple of them. The first one is
[01:55:03.140 --> 01:55:10.180]   dedicated to Mr. Jarvis and there's something on TikTok that it suggested to me and I actually
[01:55:10.180 --> 01:55:18.020]   like it. Isn't it a musical land? If you thought that would be no. Good grief, man.
[01:55:18.020 --> 01:55:26.660]   Now, it's Auntie Bev and she just basically gives you a breakdown of a word of the day that's
[01:55:26.660 --> 01:55:35.540]   being misused. I believe she's got a different handle, but your time is your time to shine.
[01:55:35.540 --> 01:55:43.380]   Your time is your shine. Bev speaks. She's such a nice little lady and she's grown.
[01:55:43.380 --> 01:55:48.500]   I mean, she's got a gazillion followers and I really enjoy just seeing some of the words that
[01:55:48.500 --> 01:55:56.900]   she shares. Stuff like "ax" and "ask" because people screw that up. Don't ever say "eany,
[01:55:56.900 --> 01:56:04.980]   meany, miney, mo." Okay. I've learned that lesson. Next, what else you got? Next is a series I'm
[01:56:04.980 --> 01:56:09.460]   watching on Netflix, Lupin. I believe it's how you say it in French based on a book.
[01:56:09.460 --> 01:56:18.820]   Lupin. Lupin. I don't know. Lupin. But it's been a pretty cool series. The actor,
[01:56:18.820 --> 01:56:23.060]   never heard of him and it's all in French, so you need to turn on your subtitles,
[01:56:23.060 --> 01:56:27.060]   but it's really, really good. I will watch this. I've heard some good things.
[01:56:27.060 --> 01:56:30.980]   Or get Pimsler on Audible and learn French in the back.
[01:56:30.980 --> 01:56:36.180]   That's true. That's true. And guess what time of year it is?
[01:56:36.180 --> 01:56:43.460]   And lastly, I want to give a shout out and recognition for June 19th. We're also known as
[01:56:43.460 --> 01:56:51.140]   Juneteenth. Again, this is something that's... The more I think about it, it sort of bugs me more
[01:56:51.140 --> 01:56:56.900]   and more because this is something that I personally, as a black man, didn't know anything about
[01:56:56.900 --> 01:57:04.020]   until I was an adult. This wasn't taught in schools where I grew up in the South. It is just
[01:57:04.020 --> 01:57:10.340]   a bit of an afterthought. But I want to, again, just bring more recognition to this holiday,
[01:57:10.340 --> 01:57:14.820]   which may now end up potentially being a federal holiday because
[01:57:16.580 --> 01:57:21.220]   Senator... Senator, what was his name? Ron Johnson. Ron Johnson would be blocking it for a year.
[01:57:21.220 --> 01:57:27.780]   And now he's back down, so we may now have this as a federal holiday.
[01:57:27.780 --> 01:57:32.660]   I think we will. The Senate passed it unanimously on Tuesday and the House passed it today,
[01:57:32.660 --> 01:57:39.460]   which would make you 19th. Was it off that day? He finally gave up for some reason. He had other
[01:57:39.460 --> 01:57:45.460]   fish to fry. I think he wanted to give a group a tour of the Capitol building, as a matter of fact.
[01:57:46.020 --> 01:57:57.380]   But... Yeah. So the House voted yes. So I think all that's left is the President's signature,
[01:57:57.380 --> 01:58:03.460]   and it will be a June... It'll be called Juneteenth National Independence Day. It commemorates
[01:58:03.460 --> 01:58:08.900]   not the end of slavery because the Emancipation Proclamation was a couple of years earlier.
[01:58:10.420 --> 01:58:16.340]   The final Black people in America finding out about it, June 19th.
[01:58:16.340 --> 01:58:20.740]   In Texas, Galveston, Galveston, Texas. Yeah. The last people.
[01:58:20.740 --> 01:58:27.140]   No idea that they were free. They didn't know. It's amazing. Let me see if I can find this.
[01:58:27.140 --> 01:58:34.420]   Which goes to your point earlier. 1865. That was in Brattle Books today, and Boston,
[01:58:34.420 --> 01:58:39.540]   a book I should have actually bought, went through how long it took for the
[01:58:39.540 --> 01:58:42.180]   Declaration of Independence to be printed across the country.
[01:58:42.180 --> 01:58:47.220]   And of course, there was something else going on here. People trying to keep
[01:58:47.220 --> 01:58:54.420]   enslaved people from knowing they were free. But it took time for knowledge to pass.
[01:58:54.420 --> 01:58:58.180]   I think this would be a good time for Jeff to give us a number.
[01:58:59.220 --> 01:59:10.100]   Slightly. Slightly. Slightly. So I think Mackenzie Scott has given away another 2.7 billion
[01:59:10.100 --> 01:59:18.740]   dollars in grants. And what I quite liked about her theme this time, 286 teams empowering voices
[01:59:18.740 --> 01:59:24.980]   the world needs to hear. I rant on all the time that mass media run by people who look like me,
[01:59:24.980 --> 01:59:32.980]   old white men, too often do not value the voices and stories and viewpoints and perspectives
[01:59:32.980 --> 01:59:42.020]   of people who do not share our experience and the mass. And so to me, the reason I defend the
[01:59:42.020 --> 01:59:47.300]   internet all the time is because it doesn't give voice to people. People always had their voices.
[01:59:47.300 --> 01:59:51.780]   But it gives the opportunity for voices who previously were not heard through the gatekeepers
[01:59:51.780 --> 01:59:58.900]   to be heard. And so she has 286 teams that are doing amazing work here, links to every one of them
[01:59:58.900 --> 02:00:04.980]   and salute to the Kansas City. That's a huge amount of money to give away.
[02:00:04.980 --> 02:00:18.660]   2.7 billion dollars. Wow. And her ex, Jeff Bezos, who's flop flaunting his wealth by going into space
[02:00:19.220 --> 02:00:23.620]   in a battle, in a battle to see who can be first with the
[02:00:23.620 --> 02:00:31.380]   Virgin's Richard Branson. Apparently there is somebody in the chat room was saying is there's
[02:00:31.380 --> 02:00:35.780]   a petition to let Jeff Bezos go into space, but don't let him back.
[02:00:35.780 --> 02:00:44.420]   It's like 11,000 signatures on it. And I wanted to pay to Senator Johnson and the other
[02:00:44.420 --> 02:00:52.660]   seeking. Yeah. That's hysterical. All right. 2.7 billion. Wow. Really nice. All right,
[02:00:52.660 --> 02:00:57.140]   Joan, I've given you all the time I can time for your pick of the week.
[02:00:57.140 --> 02:01:03.620]   My pick of the week. So I just got this book, if then, by Jill Lepore, how the
[02:01:03.620 --> 02:01:10.820]   Simul, Simulmatics Corporation invented the future. I haven't read it yet. Actually,
[02:01:11.380 --> 02:01:16.820]   it's really interesting. So it's really about the genesis of early data science. I read a sample
[02:01:16.820 --> 02:01:22.180]   chapter online and not going to lie. It was very compelling that, but the idea here is sort of
[02:01:22.180 --> 02:01:27.620]   like looking at the developments, early development of artificial intelligence and what, you know,
[02:01:27.620 --> 02:01:34.660]   election companies were excited about in terms of gathering data and how to deploy it. And so
[02:01:34.660 --> 02:01:39.540]   I'm really looking forward to that. Is that what Simulmatics did? They were a
[02:01:40.100 --> 02:01:44.660]   vote counting machine or vote? Well, they were like trying to predict people's,
[02:01:44.660 --> 02:01:48.660]   you know, what kind of behaviors would they be voting?
[02:01:48.660 --> 02:01:54.100]   Over data analysis. She's a very good writer. I read her in the New York or all the time. So
[02:01:54.100 --> 02:01:58.740]   I'm going to add this to my audio book list, my wishlist.
[02:01:58.740 --> 02:02:03.620]   All right. Get that book. Selling books. Look at me. Selling the books.
[02:02:04.260 --> 02:02:11.380]   How the Simulmatics Corporation invented the future. Jill Lepore, not Lepore, L-E-P-O-R.
[02:02:11.380 --> 02:02:17.540]   Yeah, no relation. No relation. I don't know her. I've never met a...
[02:02:17.540 --> 02:02:24.180]   No, no. No. So yeah, that was, that's my big one is I'm excited to read this book and then,
[02:02:24.180 --> 02:02:31.620]   you know, I don't really have anything else I've been doing or seeing or watching. I've been in a
[02:02:31.620 --> 02:02:40.020]   bit of a, you know, a deep dark well of writing trying to get this book done for July 4th.
[02:02:40.020 --> 02:02:44.820]   So, but this is... Ross, wait a minute. Tell us this. What's this book you're writing now?
[02:02:44.820 --> 02:02:49.460]   You got to get this one. Well, I'm writing a book with co-authors Brian Friedberg, who's sort of
[02:02:49.460 --> 02:02:56.900]   been with me for the last several years as sort of my meme historian and Emily Dreyfus, who
[02:02:57.620 --> 02:03:02.180]   used to be a senior editor at Wired and now is working with us on our team and
[02:03:02.180 --> 02:03:06.180]   writing about meme wars. We're writing about the last decade of the internet and we're trying to
[02:03:06.180 --> 02:03:15.700]   understand how memes became political communication. And so now that quantitatively disinformation
[02:03:15.700 --> 02:03:21.300]   has slowed down because Trump isn't on so many platforms and his blog was a big fail,
[02:03:21.300 --> 02:03:26.900]   we have some time to do some other kinds of thinking and writing. And so I've been deep in
[02:03:27.460 --> 02:03:33.860]   looking at conspiracy theories and just the memetic cultures online and, you know,
[02:03:33.860 --> 02:03:40.420]   what a Shiloh-la-boof have to do with anything. And it's been fun. A question I ask all the time.
[02:03:40.420 --> 02:03:48.340]   Right on inauguration day, funny you should ask. In 2016, he launched this very strange art
[02:03:48.340 --> 02:03:54.660]   project called He Will Not Divide Us. And it became one of the worst places on earth
[02:03:56.020 --> 02:04:02.420]   to be online and IRL. There were just all of these crazy fights and all of these weird memes and
[02:04:02.420 --> 02:04:10.180]   it was a whole thing. And so I've been living in the last, you know, my time in the past week has
[02:04:10.180 --> 02:04:17.620]   been spent in 2016, 2017 really heavily when the meme war heats up. And so it's been fun. It's
[02:04:17.620 --> 02:04:23.300]   been interesting to tell the history of the internet through this lens and I'm really excited for
[02:04:23.300 --> 02:04:30.180]   it to be done. So it's done at the end of the month and then when will we see it?
[02:04:30.180 --> 02:04:34.260]   Hopefully in a year. So I'll get into some of the book.
[02:04:34.260 --> 02:04:41.700]   Wow. I know, but it's the way of the world. It will not stop being relevant though. I can
[02:04:41.700 --> 02:04:48.500]   promise you that because it's, you know, just that kind of, that memes are here to stay and our
[02:04:48.500 --> 02:04:54.180]   politics are becoming much more trolley. And so we're hoping that it helps people see
[02:04:54.180 --> 02:05:00.580]   what's happening in this moment differently and stodgy. It's going to be with Bloomberg.
[02:05:00.580 --> 02:05:07.460]   Very good. Yeah. Yeah. We wanted to go with an academic press and we couldn't get anybody to
[02:05:07.460 --> 02:05:12.660]   agree to full color and I just, it, it irked. Oh, it has to be color memes are in color.
[02:05:12.660 --> 02:05:16.980]   It has to be. Yeah, exactly. And so, I mean, if you get the perfect
[02:05:16.980 --> 02:05:22.500]   one, it's right. Yeah. It's very expensive. So this is going to be a coffee table book.
[02:05:22.500 --> 02:05:29.780]   Well, I will see. I kind of want to get some better development around the art so that we're
[02:05:29.780 --> 02:05:35.140]   not just sending you screenshots. I can't wait. Well, we're going to have you back before then.
[02:05:35.140 --> 02:05:38.580]   It's going to be a weird one. When that comes out, you must come on and we will do a lot.
[02:05:38.580 --> 02:05:44.900]   This is such a great subject. It's fun. You know, when you knocked on the door, I said,
[02:05:44.900 --> 02:05:51.380]   please let them in. Come on in. And the medium meme more weekly has many. Is it,
[02:05:51.380 --> 02:05:56.580]   it's not the same content as in the book or it is. No, it's different content, but similar themes.
[02:05:56.580 --> 02:06:00.740]   We had to, we had to put the emergency break on meme more weekly because it was getting really
[02:06:00.740 --> 02:06:06.020]   dark during the pandemic. Yeah. So we're going to hopefully be spinning things back up again soon.
[02:06:06.020 --> 02:06:12.820]   It's just creepy memes. It got re, it got, you know, the pandemic protest politics. It got
[02:06:13.540 --> 02:06:21.140]   super dark times. You know, there's a professor at Cal named Dundas who writes about
[02:06:21.140 --> 02:06:28.740]   sick joke cycles. And of course, this is out of date now, but, but, you know, 20 or 30 years ago,
[02:06:28.740 --> 02:06:34.980]   Alan Dundas, he, it was his position. I think he was absolutely right that the sick joke cycles
[02:06:36.500 --> 02:06:43.460]   reflected the things that, you know, this deep psych, the psychological things people were worried
[02:06:43.460 --> 02:06:52.180]   about his book was called cracking jokes. And here's, here's the book. For instance, during
[02:06:52.180 --> 02:07:00.180]   the civil rights movement, elephant jokes were really popular. And his contention is that that
[02:07:00.180 --> 02:07:05.860]   represented to the white psyche, black people, and they were appearing in your refrigerator and all
[02:07:05.860 --> 02:07:12.260]   over it. Then during the abortion crisis, when Roe v. Wade started to become the law of the land
[02:07:12.260 --> 02:07:18.020]   and the big fights over abortion, you saw a lot of dead baby jokes. So it's his opinion. And I think
[02:07:18.020 --> 02:07:24.020]   this memes actually are the modern, in my opinion, the modern version of these sick jokes. They reflect
[02:07:24.020 --> 02:07:29.940]   the psyche of what's going on. And of course, during a very dark period in our time, I can
[02:07:29.940 --> 02:07:37.620]   imagine the memes were really, really dark. It was, it got really, it got really traumatizing
[02:07:37.620 --> 02:07:43.940]   points. And so I was like, I couldn't put my, my team through it. But nevertheless, me, Brian,
[02:07:43.940 --> 02:07:50.260]   and Emily couldn't resist. And so we, we're going to be writing, well, we're in the midst of writing
[02:07:50.260 --> 02:07:57.220]   that and wrapping that up. But it is, yeah, the humor, you know, things start as a prank. And then
[02:07:57.220 --> 02:08:04.420]   they turn into real life in a very serious way. And so we're trying to track that.
[02:08:04.420 --> 02:08:09.300]   Yeah, people aren't necessarily going to be honest and forthright if you ask them. But if you see,
[02:08:09.300 --> 02:08:13.460]   if their humor reflects what's really, they're really thinking what's really going on their
[02:08:13.460 --> 02:08:20.740]   mind and their memes absolutely do that. I live on Reddit, which is meme central. Is that, you think
[02:08:20.740 --> 02:08:24.980]   the best place to see what's going on in the world of memes? Where do you go to do your research?
[02:08:25.780 --> 02:08:31.140]   I mean, yeah, Reddit's a great place to go. We're, you know, because we're doing it historically,
[02:08:31.140 --> 02:08:36.180]   of course, like, you know, we go all the way back to Tumbler and Tumbler for sure.
[02:08:36.180 --> 02:08:43.380]   Yeah. Different memetic communities over time. But 4chan is really where memes,
[02:08:43.380 --> 02:08:48.900]   Pepe lives, you know, was happening. And then, and you know, and over the years,
[02:08:48.900 --> 02:08:55.380]   yeah, Reddit has taken up quite a bit of memetic real estate, I should say. And, and, you know,
[02:08:55.380 --> 02:09:02.820]   Facebook in 2016 became a really hot place to share memes and to work on them with other people.
[02:09:02.820 --> 02:09:07.940]   Because I don't think most people understand how collaborative memes are, but it's, it's not the
[02:09:07.940 --> 02:09:12.820]   case that, you know, a meme gets out in the world and nothing happens. The thing about them that's
[02:09:12.820 --> 02:09:18.420]   really interesting is how quickly people remix them. And if you think very, you know, back to the,
[02:09:18.420 --> 02:09:23.140]   I think the only thing people might remember about the inauguration this year was Bernie's
[02:09:23.140 --> 02:09:29.540]   Mittens. Right. I mean, you were, I was the best part. No. What else happens? You might not know.
[02:09:29.540 --> 02:09:36.500]   And so, yeah, so that, you know, memes become sticky in a way and they become very memorable
[02:09:36.500 --> 02:09:43.300]   markers of moments. And, yeah, and so we're just, we're exploring that and then thinking a bit more
[02:09:43.300 --> 02:09:47.300]   in terms of like right-wing politics.
[02:09:47.300 --> 02:09:52.900]   By the way, memes have come to define politics. This is the follow-up of Bernie's Mittens,
[02:09:52.900 --> 02:09:57.460]   the second grade teacher who sent them them, him, the Mittens has now made a deal with the Vermont
[02:09:57.460 --> 02:10:06.260]   Teddy Bear factory to sell to Bernie's Mittens. They looked cozy. They were warm. You live in
[02:10:06.260 --> 02:10:12.820]   California. You don't understand. No. I grew up with Mittens. I don't understand really why
[02:10:12.820 --> 02:10:17.220]   people would wear Mittens instead of gloves, but I'm sure you can explain that. That's another show.
[02:10:17.220 --> 02:10:24.180]   Yeah. Yeah, but I will, I will say on a, on a positive note, the thing that we're concluding on,
[02:10:24.180 --> 02:10:29.460]   and this goes along with the conversation a little bit earlier about Juneteenth is we really mark
[02:10:29.460 --> 02:10:33.860]   Black Lives Matter as the meme of the decade. It is the most brilliant, interesting,
[02:10:33.860 --> 02:10:41.700]   interesting. Meme going, and it's really hard to pin down. It's really hard to associate with
[02:10:41.700 --> 02:10:50.180]   certain kinds of leaders. It's been very, very resilient even in the face of immense attacks.
[02:10:50.180 --> 02:10:56.900]   There was a, there was a moment where people were really trying to make all lives matter a thing,
[02:10:56.900 --> 02:11:01.860]   and it just didn't, it just didn't work because Black Lives Matter was so much more compelling
[02:11:01.860 --> 02:11:11.620]   and remixable and stood for so much more. And so the, not to, not to blow the ending of the book,
[02:11:11.620 --> 02:11:19.460]   but we do, we do look at this, this meme in particular is something that is very culturally
[02:11:19.460 --> 02:11:27.780]   resident, very, very important to American politics, but hasn't gone away in the, in the same way
[02:11:27.780 --> 02:11:33.780]   that many memes just kind of come and go. That's interesting. So in, you're using the word meme,
[02:11:33.780 --> 02:11:39.220]   not merely just, I always think of memes as the image with the character, you know, the text and
[02:11:39.220 --> 02:11:44.260]   stuff like that. The, the boyfriend. Yeah, but this slogan, the idea is it's a unit of culture.
[02:11:44.260 --> 02:11:51.380]   It's just, it's just, and a lot of them are, are three word phrases. There's something, you know,
[02:11:51.380 --> 02:11:56.980]   in, at least the English language that wins to this. So when it comes to a viral slogan,
[02:11:57.620 --> 02:12:02.740]   I want to propose a new one. I just made it up on the spot. Mittens are warmer.
[02:12:02.740 --> 02:12:10.660]   Is that, you think that could do it, catch on around the world? Do it. Making emoji.
[02:12:10.660 --> 02:12:16.900]   It's funny because I, I, if you would ask me, I wouldn't have thought of Black Lives Matter,
[02:12:16.900 --> 02:12:20.020]   but I think you're right, Joan. I still see Black Lives Matter signs.
[02:12:20.020 --> 02:12:26.020]   Almost everybody's windows and Petaluma and stuff. It really, it's, it's a, there's not a lot of
[02:12:26.020 --> 02:12:30.100]   Black people in Petaluma. There's no Black people here. I go ahead and tell you first hand.
[02:12:30.100 --> 02:12:33.460]   Ain't no Black folks here. But I see the,
[02:12:33.460 --> 02:12:39.380]   yeah, everywhere. Everywhere. And I think that it really, it was, there was something about it,
[02:12:39.380 --> 02:12:43.780]   the community, it wasn't me, the community embraced it. It's the, it's the racial
[02:12:43.780 --> 02:12:49.460]   reformation of America. Yeah, it was really interesting. It is what, what didn't occur in mass media,
[02:12:49.460 --> 02:12:54.500]   that, that social media enabled. This is my defense. This is my primary defense of the internet.
[02:12:55.140 --> 02:12:59.940]   Is that, and this is what, because he's gots talking about it and going to the voices too often not heard.
[02:12:59.940 --> 02:13:03.860]   Yeah. Is exactly this. This is what social media enabled the media didn't.
[02:13:03.860 --> 02:13:10.020]   Hey, Joan Donovan. Thank you so, so, so much for being here. We love hearing from you.
[02:13:10.020 --> 02:13:14.260]   You're so smart. I can't wait to read your book. We'll have you back soon.
[02:13:14.260 --> 02:13:20.260]   Joan Donovan, research director at the Harvard Kennedy School, expert in disinformation
[02:13:21.060 --> 02:13:26.340]   at Boston Joan on Twitter. Jeff Jarvis is the director of the townites. There she is.
[02:13:26.340 --> 02:13:28.900]   Did you get to wear a commencement? Did you get to wear,
[02:13:28.900 --> 02:13:35.700]   is that your cap and gown or no? No, that was just, I was like, oh, the Harvard magazine.
[02:13:35.700 --> 02:13:38.340]   I've got one crimson sweater. Oh, that's a stir.
[02:13:38.340 --> 02:13:45.460]   But once they have commencements again, you'll probably have to march in the
[02:13:45.460 --> 02:13:51.380]   commencement and they get these big velvet robes and floppy hats and maces and weird stuff.
[02:13:51.380 --> 02:13:59.780]   That's why I do this. It's the best part. It's the best part. It is. Pretending we're engaged.
[02:13:59.780 --> 02:14:06.660]   I honestly do love seeing my students graduate. It's one of the just small pleasures in life where
[02:14:06.660 --> 02:14:12.820]   you can celebrate other people's big wins and you call them master for a day or doctor and then
[02:14:12.820 --> 02:14:21.540]   you put them back. That's awesome. They call it the regalia just as they would if you were a
[02:14:21.540 --> 02:14:28.740]   royalty and you think that a color. You can get it at the Harvard store at the Harvard
[02:14:28.740 --> 02:14:35.540]   Coop. Get your regalia. Let's see which car. Let's college are you in Kennedy School. Let's see
[02:14:35.540 --> 02:14:41.140]   what your regalia would look like. Oh, no, they're going to make you go through the whole
[02:14:41.140 --> 02:14:45.460]   Yeah, I don't get to see a picture. I want to see the regalia. Oh, well, all right.
[02:14:45.460 --> 02:14:49.460]   Peace is here. Coop is over. Jeff Jarvis is director of the Townite Center for
[02:14:49.460 --> 02:14:59.860]   Entrepreneurial Journalism at the. Oh, Joan joining in. Somebody could sing graduate school
[02:14:59.860 --> 02:15:05.780]   in journalism at the city university of New York. Thank you so much. Jeff, always great to have you.
[02:15:06.340 --> 02:15:12.820]   Ann Ant Pruitt, our good friend from Hands-Off Photography. Twit.tv/hop. What's next on hop?
[02:15:12.820 --> 02:15:19.700]   Oh, man. What is this week? Dang, I just were blind. You record them ahead of time. Oh, I know.
[02:15:19.700 --> 02:15:24.100]   Yeah, I know what's going on now. I'm doing some some feedback on some images that were
[02:15:24.100 --> 02:15:29.220]   safe to me this week. Nice. Great show. I love photography and I love watching Hands-Off Photography.
[02:15:29.220 --> 02:15:35.220]   Thank you so much. And everybody. Yeah. Hey, can't. And it's also in the discord and
[02:15:35.220 --> 02:15:40.660]   on the chat and everywhere. People are always talking to you. Always talking and saying,
[02:15:40.660 --> 02:15:46.500]   hey, yeah, look at my picture. We do Twig every Wednesday, 2 p.m. Pacific, 5 p.m. Eastern.
[02:15:46.500 --> 02:15:52.580]   That would be 2100 UTC. If you want to watch it live, it's twit.tv/live. There's live audio
[02:15:52.580 --> 02:15:58.180]   and video streams there. There's also, if you're watching live and IRC chat room, you can chat along
[02:15:58.180 --> 02:16:07.140]   at IRC.twit.tv. We have on-demand versions of the shows at the website, twit.tv/twig for this one.
[02:16:07.140 --> 02:16:11.940]   While you're there, you'll see a YouTube link. That's the YouTube channel with all the Twitch
[02:16:11.940 --> 02:16:17.780]   videos. You also can subscribe in your favorite podcast application. That's probably a great idea.
[02:16:17.780 --> 02:16:22.820]   If you do that, please leave us a five-star review. Let everybody know how great
[02:16:23.380 --> 02:16:29.700]   Twig is. We thank you for listening and watching. And we will see you next week on this week in Google.
[02:16:29.700 --> 02:16:30.100]   Bye-bye.
[02:16:30.100 --> 02:16:39.780]   You know what's fun, Android. You know what's even more fun, though? All about Android. That's
[02:16:39.780 --> 02:16:44.900]   my show, Jason Howell, along with my co-host Ron Richards, Florence Ion. And we welcome guests
[02:16:44.900 --> 02:16:51.380]   on each and every week from throughout the Android ecosystem, developers, Googlers, journalists,
[02:16:51.380 --> 02:16:56.020]   people who are all geeked out about the Android operating system. We tell you everything you need
[02:16:56.020 --> 02:17:00.740]   to know, twit.tv/AAAeveryTuesday. We'll see you there.
[02:17:00.740 --> 02:17:11.220]   [Music]

