;FFMETADATA1
title=Cauliflower Sex Tape
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=380
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2016
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.200]   It's time for Twig this week in Google our pre-Thanksgiving edition.
[00:00:04.200 --> 00:00:06.360]   No, we're not going to pardon a turkey.
[00:00:06.360 --> 00:00:11.760]   We might talk about some turkeys including Facebook's Dark Posts, Jeff Jarvis's here
[00:00:11.760 --> 00:00:14.160]   Stacey Higginbotham and a lot more.
[00:00:14.160 --> 00:00:15.160]   Stay tuned.
[00:00:15.160 --> 00:00:16.600]   This week in Google is next.
[00:00:16.600 --> 00:00:21.600]   Netcast you love.
[00:00:21.600 --> 00:00:25.280]   From people you trust.
[00:00:25.280 --> 00:00:29.680]   This is Twig.
[00:00:29.680 --> 00:00:40.800]   And with for this week in Google is provided by cash fly cachiefly.com.
[00:00:40.800 --> 00:00:48.600]   This is Twig this week in Google episode 380 recorded Wednesday November 23rd 2016.
[00:00:48.600 --> 00:00:51.000]   Call if flowers sex tape.
[00:00:51.000 --> 00:00:54.960]   This week in Google is brought to you by Rocket Mortgage from Quick and Loans.
[00:00:54.960 --> 00:00:59.240]   Rocket Mortgage brings the mortgage process into the 21st century with a fast easy and
[00:00:59.240 --> 00:01:01.280]   completely online process.
[00:01:01.280 --> 00:01:07.440]   Check out Rocket Mortgage today at quickandlones.com/twig.
[00:01:07.440 --> 00:01:10.640]   It's time for Twig this week in Google to show where we talk about the latest news.
[00:01:10.640 --> 00:01:12.840]   Not just from Google, but we talk about news.
[00:01:12.840 --> 00:01:14.480]   We talk about Facebook.
[00:01:14.480 --> 00:01:15.480]   We talk about Twitter.
[00:01:15.480 --> 00:01:16.920]   We talk about social media.
[00:01:16.920 --> 00:01:18.120]   We talk, thanks.
[00:01:18.120 --> 00:01:20.640]   Thank goodness we have a Stacey Higginbotham here.
[00:01:20.640 --> 00:01:22.160]   We talk about Internet of Things as well.
[00:01:22.160 --> 00:01:24.200]   Stacey is from the IoT podcast.
[00:01:24.200 --> 00:01:26.600]   You may remember her from Gig Aon.
[00:01:26.600 --> 00:01:28.680]   She's still Gig A Stacey on the Twitter.
[00:01:28.680 --> 00:01:31.920]   She's in Houston, Texas for Thanksgiving.
[00:01:31.920 --> 00:01:32.920]   Hi Stacey.
[00:01:32.920 --> 00:01:33.920]   Hello.
[00:01:33.920 --> 00:01:39.080]   You are there for Thanksgiving, I take it.
[00:01:39.080 --> 00:01:40.080]   I am.
[00:01:40.080 --> 00:01:41.240]   I'm visiting my family.
[00:01:41.240 --> 00:01:43.880]   This used to be my childhood bedroom.
[00:01:43.880 --> 00:01:45.880]   You're in your old bedroom?
[00:01:45.880 --> 00:01:46.880]   It's not a bedroom.
[00:01:46.880 --> 00:01:47.880]   It's now my dad's office.
[00:01:47.880 --> 00:01:48.880]   How long?
[00:01:48.880 --> 00:01:50.880]   Where are the Justin Bieber posters?
[00:01:50.880 --> 00:01:53.680]   I was new kids on the block.
[00:01:53.680 --> 00:01:54.680]   Sorry.
[00:01:54.680 --> 00:01:58.640]   How long after you went to college did they convert to a college?
[00:01:58.640 --> 00:02:00.400]   I was like one minute.
[00:02:00.400 --> 00:02:04.240]   Before I left, my mom was bringing in stuff to store under there.
[00:02:04.240 --> 00:02:06.280]   I was like, "Mom, what are you doing?"
[00:02:06.280 --> 00:02:08.680]   She's like, "Stacey, don't be silly.
[00:02:08.680 --> 00:02:10.200]   You're not coming back after college."
[00:02:10.200 --> 00:02:13.600]   I was like, "Okay."
[00:02:13.600 --> 00:02:16.840]   My son did exactly the same thing.
[00:02:16.840 --> 00:02:18.120]   He said, "What, you can't?"
[00:02:18.120 --> 00:02:22.160]   I was like, "All the Christmas wreaths were going in under my bed.
[00:02:22.160 --> 00:02:24.160]   I'm like, "I'm using that."
[00:02:24.160 --> 00:02:25.640]   But I wasn't.
[00:02:25.640 --> 00:02:30.000]   Well, it's nice to see that your bedroom is still there for you to use for podcasting.
[00:02:30.000 --> 00:02:31.000]   That's nice.
[00:02:31.000 --> 00:02:32.000]   Exactly.
[00:02:32.000 --> 00:02:33.000]   It's my new studio.
[00:02:33.000 --> 00:02:34.000]   Yes.
[00:02:34.000 --> 00:02:38.760]   Also with us from his home, as he prepares for American Thanksgiving, Jeff Jarvis from
[00:02:38.760 --> 00:02:43.600]   Buzzemachine.com, Professor of Journalism at CUNY, and new pixel owner.
[00:02:43.600 --> 00:02:48.840]   Now I see you just got a box that says something like Daydream on it.
[00:02:48.840 --> 00:02:50.160]   It is.
[00:02:50.160 --> 00:02:52.040]   Jason Howell got his a couple of days ago.
[00:02:52.040 --> 00:02:53.040]   We've been playing with it.
[00:02:53.040 --> 00:02:55.400]   It's pretty cool.
[00:02:55.400 --> 00:03:00.600]   A little better than cardboard because you get the little remote controller.
[00:03:00.600 --> 00:03:03.080]   Yes, the controller.
[00:03:03.080 --> 00:03:09.800]   But I am still unconvinced of the long-term value of, "Oh, you got it out."
[00:03:09.800 --> 00:03:10.800]   You like that fiber?
[00:03:10.800 --> 00:03:14.200]   Yeah, it kind of feels like I actually want to make underwear out of this.
[00:03:14.200 --> 00:03:16.200]   Well, actually it's exactly what it is.
[00:03:16.200 --> 00:03:17.200]   Is it Jersey?
[00:03:17.200 --> 00:03:18.200]   Cotton Jersey?
[00:03:18.200 --> 00:03:19.200]   No, it's not cotton.
[00:03:19.200 --> 00:03:21.320]   It's a nylon sport fabric.
[00:03:21.320 --> 00:03:25.360]   It's used for underarmer kind of stuff.
[00:03:25.360 --> 00:03:26.360]   Wicking.
[00:03:26.360 --> 00:03:27.360]   It wicks.
[00:03:27.360 --> 00:03:29.280]   It has a removable face plate.
[00:03:29.280 --> 00:03:33.880]   You can wash that, which is nice because it might get kind of sweaty.
[00:03:33.880 --> 00:03:34.880]   And make-up-y.
[00:03:34.880 --> 00:03:36.520]   Make-up-y.
[00:03:36.520 --> 00:03:40.520]   It also works not just with the pixel, but Motorola has now announced it's going to work
[00:03:40.520 --> 00:03:42.800]   with some of their phones, which is good.
[00:03:42.800 --> 00:03:44.480]   Eventually, you have to take...
[00:03:44.480 --> 00:03:46.040]   Okay, don't tell him.
[00:03:46.040 --> 00:03:49.480]   He's like the mini pearl of VR headsets.
[00:03:49.480 --> 00:03:52.640]   You got to take the paper thing out there.
[00:03:52.640 --> 00:03:53.640]   Okay.
[00:03:53.640 --> 00:03:54.640]   Yeah.
[00:03:54.640 --> 00:03:58.640]   He left the price tag in.
[00:03:58.640 --> 00:03:59.800]   You know who mini pearl is?
[00:03:59.800 --> 00:04:01.200]   Stacey, I bet you don't.
[00:04:01.200 --> 00:04:03.480]   I'm an older lady.
[00:04:03.480 --> 00:04:09.160]   We just learned that Jeff and I are Stacey's parents' age.
[00:04:09.160 --> 00:04:11.240]   It just very...
[00:04:11.240 --> 00:04:12.240]   It pops up.
[00:04:12.240 --> 00:04:13.240]   It goes...
[00:04:13.240 --> 00:04:15.200]   Well, you don't have a phone in it yet, do you?
[00:04:15.200 --> 00:04:16.200]   Was that it?
[00:04:16.200 --> 00:04:17.200]   Okay.
[00:04:17.200 --> 00:04:22.200]   It's something new and all complained about it.
[00:04:22.200 --> 00:04:23.440]   Okay, how do I do this?
[00:04:23.440 --> 00:04:24.440]   Put your pixel in there.
[00:04:24.440 --> 00:04:26.120]   It should sense it.
[00:04:26.120 --> 00:04:33.240]   By the way, what we learned on the new screen savers is that you can Chromecast from your
[00:04:33.240 --> 00:04:34.720]   pixel before you put it in.
[00:04:34.720 --> 00:04:35.720]   Oh, it's worse.
[00:04:35.720 --> 00:04:36.720]   No, it won't stay up.
[00:04:36.720 --> 00:04:38.160]   Well, you may have to adjust the headband.
[00:04:38.160 --> 00:04:40.320]   It doesn't have an over-the-top headband.
[00:04:40.320 --> 00:04:41.880]   It's just around the back, which...
[00:04:41.880 --> 00:04:44.520]   And in fact, Jason was complaining about the same thing that the headband's a little...
[00:04:44.520 --> 00:04:45.520]   Yeah, it just...
[00:04:45.520 --> 00:04:46.520]   It flips up.
[00:04:46.520 --> 00:04:47.520]   I'm sure you can adjust it and get it.
[00:04:47.520 --> 00:04:49.800]   But you're still wearing your glasses.
[00:04:49.800 --> 00:04:50.800]   That's good, right?
[00:04:50.800 --> 00:04:54.640]   Yeah, you can wear glasses with it, which is good because it doesn't have any adjustment
[00:04:54.640 --> 00:04:59.200]   for focus, which some of the others do so that you can...
[00:04:59.200 --> 00:05:02.400]   For instance, I wear glasses and if I'm not wearing my contacts, I don't want to wear the
[00:05:02.400 --> 00:05:04.040]   glasses underneath the visor.
[00:05:04.040 --> 00:05:07.960]   So I take them off and adjust the focus and that usually is adequate, but you can't do
[00:05:07.960 --> 00:05:08.960]   that with this one.
[00:05:08.960 --> 00:05:11.520]   So you're going to either have to wear your glasses or wear contacts.
[00:05:11.520 --> 00:05:14.600]   But yeah, it seems to fit over glasses pretty well.
[00:05:14.600 --> 00:05:15.600]   Jeff.
[00:05:15.600 --> 00:05:16.600]   It's okay.
[00:05:16.600 --> 00:05:18.760]   Don't worry about it.
[00:05:18.760 --> 00:05:20.240]   Better lenses than cardboard.
[00:05:20.240 --> 00:05:21.920]   Cardboard has very cheap lenses, obviously.
[00:05:21.920 --> 00:05:22.920]   It has to.
[00:05:22.920 --> 00:05:23.920]   It's a cheap device.
[00:05:23.920 --> 00:05:25.240]   This is $79.
[00:05:25.240 --> 00:05:31.240]   And I think maybe there'll be a dark Friday, a black Friday.
[00:05:31.240 --> 00:05:33.160]   Maybe we should call it dark Friday.
[00:05:33.160 --> 00:05:34.160]   A dark Friday.
[00:05:34.160 --> 00:05:36.880]   I was amazed in Germany where I was just this morning.
[00:05:36.880 --> 00:05:39.440]   The newspaper had circulars for black Friday.
[00:05:39.440 --> 00:05:40.920]   They don't have these given there.
[00:05:40.920 --> 00:05:41.920]   Right.
[00:05:41.920 --> 00:05:43.920]   But you know, commerce knows no bounds.
[00:05:43.920 --> 00:05:44.920]   No GPS.
[00:05:44.920 --> 00:05:46.120]   It's not working at all.
[00:05:46.120 --> 00:05:48.160]   What's going on?
[00:05:48.160 --> 00:05:49.400]   It's to lose.
[00:05:49.400 --> 00:05:50.600]   It's popping up here.
[00:05:50.600 --> 00:05:53.120]   See, it's not a stuck fit.
[00:05:53.120 --> 00:05:55.760]   It's the strap is too high on it.
[00:05:55.760 --> 00:05:59.800]   So it's not actually it's causing it to leak, it'll leave her up.
[00:05:59.800 --> 00:06:02.040]   Yeah, that's not good.
[00:06:02.040 --> 00:06:04.400]   Maybe, maybe I don't know.
[00:06:04.400 --> 00:06:08.480]   There's no pressure face against your desk.
[00:06:08.480 --> 00:06:11.440]   I think my nose is too big.
[00:06:11.440 --> 00:06:12.920]   So did you read the review?
[00:06:12.920 --> 00:06:14.160]   Stacey of the June oven.
[00:06:14.160 --> 00:06:16.040]   I think it was fast company had it.
[00:06:16.040 --> 00:06:17.040]   Yes.
[00:06:17.040 --> 00:06:18.040]   Oh my gosh.
[00:06:18.040 --> 00:06:19.280]   He had the worst time.
[00:06:19.280 --> 00:06:20.600]   He hated it.
[00:06:20.600 --> 00:06:22.120]   He hated it.
[00:06:22.120 --> 00:06:24.720]   Although, okay.
[00:06:24.720 --> 00:06:26.680]   One, he had a very different experience.
[00:06:26.680 --> 00:06:31.400]   He had some errors, but two, it felt like he just did not.
[00:06:31.400 --> 00:06:35.080]   I mean, like it felt like he left it in the oven and he didn't understand how to review
[00:06:35.080 --> 00:06:37.920]   things because he was complaining about like too many notifications.
[00:06:37.920 --> 00:06:42.600]   But like when I set mine up, I turned a lot of the notifications off.
[00:06:42.600 --> 00:06:44.480]   So I was like, "Is that fair?
[00:06:44.480 --> 00:06:45.480]   I don't know."
[00:06:45.480 --> 00:06:52.680]   And then he also was upset because it didn't look done and he cooked it longer and then
[00:06:52.680 --> 00:06:54.600]   he forgot about it or something.
[00:06:54.600 --> 00:06:55.600]   Like I felt like...
[00:06:55.600 --> 00:07:02.160]   He put the salmon steak in there and it said it's going to be done in 15 minutes or 10
[00:07:02.160 --> 00:07:03.240]   minutes.
[00:07:03.240 --> 00:07:05.040]   And then it turned out taken 40 minutes.
[00:07:05.040 --> 00:07:07.320]   And at one point it says it's going to take four hours.
[00:07:07.320 --> 00:07:09.520]   It was all over the place.
[00:07:09.520 --> 00:07:11.960]   I feel like he had a glitchy oven.
[00:07:11.960 --> 00:07:12.960]   But I don't know.
[00:07:12.960 --> 00:07:15.560]   I mean, it is...
[00:07:15.560 --> 00:07:19.040]   Well, I'm a little worried myself because I did order one.
[00:07:19.040 --> 00:07:24.480]   And you had a very good experience with it and reviewed it in PC Magazine, I think fairly
[00:07:24.480 --> 00:07:25.720]   positively, right?
[00:07:25.720 --> 00:07:27.400]   Yeah, I did say.
[00:07:27.400 --> 00:07:32.960]   So at one point in time, I had a problem connecting my app to the...
[00:07:32.960 --> 00:07:38.800]   Or my device to the oven, but I just rebooted it and it connected then.
[00:07:38.800 --> 00:07:44.000]   And then I had one time when the oven, before I had done anything to it, didn't turn on.
[00:07:44.000 --> 00:07:46.400]   And then so I just rebooted it.
[00:07:46.400 --> 00:07:49.000]   And that didn't feel like anything.
[00:07:49.000 --> 00:07:52.080]   I didn't have any of the issues this guy described.
[00:07:52.080 --> 00:07:53.240]   I mean, he had all kinds of...
[00:07:53.240 --> 00:07:57.200]   And we should mention that I'm sure all of these are pre-production because I ordered
[00:07:57.200 --> 00:08:00.440]   it like a normal person and I haven't received it yet.
[00:08:00.440 --> 00:08:01.440]   So these...
[00:08:01.440 --> 00:08:05.280]   Yeah, this is not their beta.
[00:08:05.280 --> 00:08:06.280]   It is...
[00:08:06.280 --> 00:08:07.280]   Sorry, it is their beta.
[00:08:07.280 --> 00:08:13.560]   They were 50 alpha early, early ovens out there, but this was not one of those.
[00:08:13.560 --> 00:08:14.560]   Trying to find this review.
[00:08:14.560 --> 00:08:16.280]   Was it Fast Company or was it somewhere else?
[00:08:16.280 --> 00:08:23.480]   There was one in Fast Company and then USA Today didn't really love it either.
[00:08:23.480 --> 00:08:26.720]   And I knew time thought it was neat but too expensive.
[00:08:26.720 --> 00:08:28.480]   Well, and everybody says that.
[00:08:28.480 --> 00:08:31.120]   I mean, it's $1,500 for a...
[00:08:31.120 --> 00:08:32.120]   Yes, it is a...
[00:08:32.120 --> 00:08:39.920]   I mean, my ovens cost $1,500 but they are full-sized, self-cleaning stainless steel ovens.
[00:08:39.920 --> 00:08:42.120]   So it's very different.
[00:08:42.120 --> 00:08:44.120]   Yeah, I mean, that's an awful lot.
[00:08:44.120 --> 00:08:45.800]   I mean, ridiculous amount of money to pay.
[00:08:45.800 --> 00:08:53.480]   It also is designed by a team that includes the guy who did the camera hardware for the
[00:08:53.480 --> 00:08:58.040]   iPhone and in fact has a camera in it.
[00:08:58.040 --> 00:09:03.520]   It has carbon fiber heating elements and it works with your smartphone and has cameras
[00:09:03.520 --> 00:09:06.120]   because it identifies the food.
[00:09:06.120 --> 00:09:12.240]   And part of the promise of it isn't really realized until down the road.
[00:09:12.240 --> 00:09:13.880]   It's going to learn, right?
[00:09:13.880 --> 00:09:19.880]   So it will recognize food better and better over time.
[00:09:19.880 --> 00:09:20.920]   I can't find this review.
[00:09:20.920 --> 00:09:21.920]   It was so funny.
[00:09:21.920 --> 00:09:23.920]   Oh, the Fast Co review?
[00:09:23.920 --> 00:09:24.920]   Was it in Fast Co?
[00:09:24.920 --> 00:09:25.920]   I can't...
[00:09:25.920 --> 00:09:26.920]   It was in Fast Co.
[00:09:26.920 --> 00:09:29.640]   Yeah, there were some great one-liners in there.
[00:09:29.640 --> 00:09:33.760]   Yeah, it was in sometimes you read a review like that and you think, "The guy is more interested
[00:09:33.760 --> 00:09:38.240]   in getting off BOMO than he is and really, whether it's a good product or not."
[00:09:38.240 --> 00:09:39.600]   But it gave me pause.
[00:09:39.600 --> 00:09:40.920]   I almost canceled my order.
[00:09:40.920 --> 00:09:43.520]   I still, I guess, have time to do that.
[00:09:43.520 --> 00:09:45.920]   They just called it June Intelligent Oven.
[00:09:45.920 --> 00:09:46.920]   Okay.
[00:09:46.920 --> 00:09:48.000]   Oh wait, no, is that...
[00:09:48.000 --> 00:09:49.000]   That's CNET.
[00:09:49.000 --> 00:09:51.760]   Oh, well.
[00:09:51.760 --> 00:09:55.000]   I would like to find this review but I can't.
[00:09:55.000 --> 00:09:56.000]   Anyway, so...
[00:09:56.000 --> 00:09:57.000]   Oh wait, here it is.
[00:09:57.000 --> 00:09:58.000]   Oh wait.
[00:09:58.000 --> 00:09:59.000]   Sorry.
[00:09:59.000 --> 00:10:02.360]   I should have prepared this one better.
[00:10:02.360 --> 00:10:04.440]   I just thought of it.
[00:10:04.440 --> 00:10:05.440]   That's it.
[00:10:05.440 --> 00:10:06.440]   No.
[00:10:06.440 --> 00:10:10.040]   That's everything that's wrong with Silicon Valley design.
[00:10:10.040 --> 00:10:17.960]   If you just search for that, very funny review.
[00:10:17.960 --> 00:10:19.920]   And he...
[00:10:19.920 --> 00:10:22.040]   I mean, everything that's wrong.
[00:10:22.040 --> 00:10:26.160]   Let me get the quotes here.
[00:10:26.160 --> 00:10:31.720]   Because I think the quotes are hysterical.
[00:10:31.720 --> 00:10:35.080]   It's fast company design.
[00:10:35.080 --> 00:10:36.640]   Is that the same as fast company?
[00:10:36.640 --> 00:10:38.360]   Fast company design?
[00:10:38.360 --> 00:10:39.360]   Or is it some...
[00:10:39.360 --> 00:10:40.360]   Maybe it's a design...
[00:10:40.360 --> 00:10:46.040]   Because I don't know, I would understand Mark Wilson writing here.
[00:10:46.040 --> 00:10:51.280]   He shaz images of his salmon cooking will finish in one minute, cooking complete, notification
[00:10:51.280 --> 00:10:52.600]   ETA pessimistic.
[00:10:52.600 --> 00:10:54.960]   That's apparently a error message.
[00:10:54.960 --> 00:10:56.560]   Notification ETA optimistic.
[00:10:56.560 --> 00:10:58.000]   Another error message.
[00:10:58.000 --> 00:11:00.720]   Notification ETA pessimistic.
[00:11:00.720 --> 00:11:04.560]   Please flip your food.
[00:11:04.560 --> 00:11:06.360]   The salmon had become more distracting.
[00:11:06.360 --> 00:11:11.360]   The baby said, "If I just cooked it on my own, it became a metaphor for Silicon Valley
[00:11:11.360 --> 00:11:12.360]   itself.
[00:11:12.360 --> 00:11:17.720]   Automated yet distracting, boastful, yet mediocre, confident, yet wrong."
[00:11:17.720 --> 00:11:21.480]   First of all, the June is a product built less for you, the user, and more for its own
[00:11:21.480 --> 00:11:24.240]   ever impending perfection as a platform.
[00:11:24.240 --> 00:11:27.300]   When you cook salmon wrong, you learn about cooking it right.
[00:11:27.300 --> 00:11:30.720]   When the June cook salmon wrong, its findings are uploaded, aggregated, and averaged it
[00:11:30.720 --> 00:11:35.480]   to a June database that you hope will allow all June ovens to get it right the next time.
[00:11:35.480 --> 00:11:39.320]   Good thing firmware updates are installed automatically.
[00:11:39.320 --> 00:11:43.520]   Even if you don't quite agree with them, it's a pretty funny review.
[00:11:43.520 --> 00:11:44.520]   But you disagree.
[00:11:44.520 --> 00:11:47.560]   You think it's useful and you didn't have the same problems.
[00:11:47.560 --> 00:11:49.720]   I did not have his problems.
[00:11:49.720 --> 00:11:52.360]   It says that he apparently missed some stuff.
[00:11:52.360 --> 00:11:58.200]   I will say, and I'm pretty sure I said this, it does adjust the cooking time.
[00:11:58.200 --> 00:12:02.920]   When I was making a pork tenderloin, I was like, "La la la, I have 40 minutes," and then
[00:12:02.920 --> 00:12:05.240]   I looked over it and was like, "Oh, just kidding.
[00:12:05.240 --> 00:12:06.920]   You have much less time than you thought."
[00:12:06.920 --> 00:12:09.720]   That makes it kind of hard to plan me.
[00:12:09.720 --> 00:12:14.360]   Yeah, if it's so smart, why doesn't it know that going in?
[00:12:14.360 --> 00:12:20.600]   I think what happened in that case is it's got the weight sensors in the bottom.
[00:12:20.600 --> 00:12:27.720]   It thought that the vegetables were with the pork, so I think it thought it was all pork.
[00:12:27.720 --> 00:12:31.320]   Then as it cooked, it realized, "Oh, this pork is cooking way faster than I thought
[00:12:31.320 --> 00:12:33.280]   it would be based on its weight."
[00:12:33.280 --> 00:12:34.280]   Then it adjusted.
[00:12:34.280 --> 00:12:41.040]   He also gives you a time lapse of your food cooking.
[00:12:41.040 --> 00:12:43.680]   He says, "One of those Pillsbury cookie commercials."
[00:12:43.680 --> 00:12:48.320]   Finally, the playback looks something more like my roasted cauliflower's sex tape.
[00:12:48.320 --> 00:12:53.840]   The unappetizing green glow that made me wonder, "Why would anybody bother to film this?"
[00:12:53.840 --> 00:12:56.200]   That was awesome.
[00:12:56.200 --> 00:13:02.920]   The kuda-gross of all the info overload absurdity is a line graph which compares oven temperature
[00:13:02.920 --> 00:13:04.960]   to item temperature.
[00:13:04.960 --> 00:13:08.600]   In case you weren't sure why your salmon took 45 minutes, here you go.
[00:13:08.600 --> 00:13:09.600]   It's science.
[00:13:09.600 --> 00:13:17.200]   Here's a picture of the cauliflower, which is really disgusting looking.
[00:13:17.200 --> 00:13:19.320]   There's the line graph.
[00:13:19.320 --> 00:13:23.160]   You know what?
[00:13:23.160 --> 00:13:24.880]   I'm still going to get one.
[00:13:24.880 --> 00:13:26.600]   I trust you, Stacey.
[00:13:26.600 --> 00:13:27.960]   I think you'll like it.
[00:13:27.960 --> 00:13:28.960]   I really do.
[00:13:28.960 --> 00:13:30.920]   What are you doing?
[00:13:30.920 --> 00:13:31.920]   Did you fix it?
[00:13:31.920 --> 00:13:33.160]   Do you have to look like you fixed it?
[00:13:33.160 --> 00:13:34.160]   I think so.
[00:13:34.160 --> 00:13:36.000]   Adjust the headset until the image is clear.
[00:13:36.000 --> 00:13:38.920]   I can't do that because it won't stay on.
[00:13:38.920 --> 00:13:41.120]   I'll just say this.
[00:13:41.120 --> 00:13:42.800]   So Jeff is complaining again.
[00:13:42.800 --> 00:13:45.240]   Vr is highly over.
[00:13:45.240 --> 00:13:47.240]   Vr, in my opinion, is highly overrated.
[00:13:47.240 --> 00:13:48.960]   How do you feel about AR?
[00:13:48.960 --> 00:13:49.960]   I'm very excited.
[00:13:49.960 --> 00:13:51.800]   I think that's the future, the real future.
[00:13:51.800 --> 00:13:55.480]   Now, it's a solid steady-state image.
[00:13:55.480 --> 00:13:56.480]   I can't move.
[00:13:56.480 --> 00:13:58.600]   I can send June up in the gym.
[00:13:58.600 --> 00:13:59.600]   That's true.
[00:13:59.600 --> 00:14:01.080]   That would be a bit more accurate.
[00:14:01.080 --> 00:14:04.000]   I'll burn down the house.
[00:14:04.000 --> 00:14:08.880]   When my wife was pregnant and on bed rest, I almost killed her with raw chicken.
[00:14:08.880 --> 00:14:10.400]   Toxic fumes.
[00:14:10.400 --> 00:14:16.960]   I can only imagine when they finally tie the daydream to the June oven and you'll get 360-degree
[00:14:16.960 --> 00:14:22.160]   realistic views of your food wallet cooks, as if you're in the oven with it.
[00:14:22.160 --> 00:14:23.160]   That's the future.
[00:14:23.160 --> 00:14:24.960]   It'll start later.
[00:14:24.960 --> 00:14:28.920]   Call the flour a sex tape.
[00:14:28.920 --> 00:14:32.880]   Okay, that's enough, Jeff.
[00:14:32.880 --> 00:14:35.680]   Just wear it like that.
[00:14:35.680 --> 00:14:36.680]   That's very fetching.
[00:14:36.680 --> 00:14:39.360]   Let's get on to this forehead now.
[00:14:39.360 --> 00:14:40.360]   I'll stop.
[00:14:40.360 --> 00:14:41.360]   Yeah.
[00:14:41.360 --> 00:14:42.360]   You'll play with it.
[00:14:42.360 --> 00:14:43.520]   We'll talk again next week.
[00:14:43.520 --> 00:14:46.600]   Those people who wear their glasses on their forehead, I can never do that.
[00:14:46.600 --> 00:14:47.600]   Right?
[00:14:47.600 --> 00:14:48.600]   Right.
[00:14:48.600 --> 00:14:49.600]   It's not a word.
[00:14:49.600 --> 00:14:50.600]   Now you can.
[00:14:50.600 --> 00:14:51.600]   That's a very bro thing to do, I think.
[00:14:51.600 --> 00:14:52.600]   The worst...
[00:14:52.600 --> 00:14:53.600]   No, the more bro thing is to...
[00:14:53.600 --> 00:14:55.600]   I really don't understand this word backwards.
[00:14:55.600 --> 00:14:59.040]   So that the glasses are behind your head and like that.
[00:14:59.040 --> 00:15:00.040]   Have you seen people do that?
[00:15:00.040 --> 00:15:01.040]   The boy works really cool.
[00:15:01.040 --> 00:15:04.560]   I have been at festivals.
[00:15:04.560 --> 00:15:05.560]   Do not like that.
[00:15:05.560 --> 00:15:06.560]   Do not like that.
[00:15:06.560 --> 00:15:07.560]   I've been watching...
[00:15:07.560 --> 00:15:08.560]   Was it called?
[00:15:08.560 --> 00:15:09.560]   I love Bill for Amazon.
[00:15:09.560 --> 00:15:10.560]   I haven't seen that.
[00:15:10.560 --> 00:15:11.560]   I love Dick.
[00:15:11.560 --> 00:15:12.560]   No.
[00:15:12.560 --> 00:15:13.560]   Oh, Kevin Beige.
[00:15:13.560 --> 00:15:14.560]   No.
[00:15:14.560 --> 00:15:15.560]   He's hilarious.
[00:15:15.560 --> 00:15:16.560]   He said I love...
[00:15:16.560 --> 00:15:17.560]   Oh, I love Dick.
[00:15:17.560 --> 00:15:18.560]   Yeah, no, I love Bill.
[00:15:18.560 --> 00:15:19.560]   I love Dick.
[00:15:19.560 --> 00:15:20.560]   Dick for Bill.
[00:15:20.560 --> 00:15:21.560]   Jesus.
[00:15:21.560 --> 00:15:23.280]   You got to be careful saying what I love Dick.
[00:15:23.280 --> 00:15:24.600]   I did see this.
[00:15:24.600 --> 00:15:28.200]   I did see this and I wasn't sure if I would like it.
[00:15:28.200 --> 00:15:33.720]   It's apparently adapted from the lauded feminist novel of the same name.
[00:15:33.720 --> 00:15:37.120]   It takes place in Marfa, Texas.
[00:15:37.120 --> 00:15:38.120]   It's good, huh?
[00:15:38.120 --> 00:15:40.880]   I watched the first episode in the plan.
[00:15:40.880 --> 00:15:42.880]   Is there a real Marfa, Texas?
[00:15:42.880 --> 00:15:43.880]   There is a real Marfa, Texas.
[00:15:43.880 --> 00:15:45.600]   Oh, it's famous.
[00:15:45.600 --> 00:15:47.720]   They do all these writing workshops.
[00:15:47.720 --> 00:15:48.720]   They have a lot of actual art.
[00:15:48.720 --> 00:15:49.640]   Oh, so it's based on that.
[00:15:49.640 --> 00:15:51.720]   What university is in Marfa?
[00:15:51.720 --> 00:15:52.880]   There is no university.
[00:15:52.880 --> 00:15:53.880]   It's just two...
[00:15:53.880 --> 00:15:57.000]   It's pretending it's a college town.
[00:15:57.000 --> 00:15:59.160]   No, it's very small.
[00:15:59.160 --> 00:16:02.200]   It's at the base of the Guadalupe Mountains.
[00:16:02.200 --> 00:16:03.200]   It's in the middle of nowhere.
[00:16:03.200 --> 00:16:08.640]   It's hard to get to, but it's very famous among the literati in an artistic set.
[00:16:08.640 --> 00:16:15.600]   So why is it that at least two now highly praised TV shows, one from HBO, one from Amazon,
[00:16:15.600 --> 00:16:20.360]   are about divorce and failing marriages?
[00:16:20.360 --> 00:16:21.840]   Is this something we're going to see more of?
[00:16:21.840 --> 00:16:22.840]   Is this the new scene?
[00:16:22.840 --> 00:16:24.240]   That's why we need a Cultural Revolution, sir.
[00:16:24.240 --> 00:16:25.240]   Yeah, maybe.
[00:16:25.240 --> 00:16:26.640]   We're back to the American family.
[00:16:26.640 --> 00:16:27.640]   Maybe.
[00:16:27.640 --> 00:16:30.640]   Aren't there other sitcoms about people?
[00:16:30.640 --> 00:16:33.320]   I mean, isn't modern family, aren't they divorced?
[00:16:33.320 --> 00:16:34.320]   No.
[00:16:34.320 --> 00:16:35.320]   Am I thinking about their family?
[00:16:35.320 --> 00:16:38.560]   Well, they are, but it's not about the divorce.
[00:16:38.560 --> 00:16:41.360]   I mean, it's way post-divorce, each is remarried.
[00:16:41.360 --> 00:16:42.360]   No, is that right?
[00:16:42.360 --> 00:16:43.360]   Okay, modern family.
[00:16:43.360 --> 00:16:44.360]   No, no, no, no.
[00:16:44.360 --> 00:16:45.360]   That's your father.
[00:16:45.360 --> 00:16:49.040]   Modern family, her father is remarried to the cha-cha.
[00:16:49.040 --> 00:16:52.560]   Yeah, not Eve Lagoria.
[00:16:52.560 --> 00:16:53.560]   Who's he?
[00:16:53.560 --> 00:16:54.560]   You know who I'm talking about.
[00:16:54.560 --> 00:16:56.320]   Oh, what about who's the boss?
[00:16:56.320 --> 00:16:57.480]   That was back in the '80s.
[00:16:57.480 --> 00:16:58.480]   And she was the...
[00:16:58.480 --> 00:16:59.480]   She was a single mom.
[00:16:59.480 --> 00:17:01.200]   Oh, but she wasn't divorced?
[00:17:01.200 --> 00:17:02.800]   What these shows are about...
[00:17:02.800 --> 00:17:10.520]   No, no, these shows are about the actual process as horrible as it is of falling apart as a
[00:17:10.520 --> 00:17:11.520]   couple.
[00:17:11.520 --> 00:17:17.280]   I mean, of course we've had divorces and single parents before, but what they have never focused
[00:17:17.280 --> 00:17:21.280]   on is the painful process of a dissolution of a marriage.
[00:17:21.280 --> 00:17:25.040]   Actually Woody Allen, scenes from a marriage, or is that Ingmar Bergman?
[00:17:25.040 --> 00:17:26.040]   I can never remember.
[00:17:26.040 --> 00:17:27.040]   Maybe it's Bergman.
[00:17:27.040 --> 00:17:28.440]   I think that's Ingmar Bergman.
[00:17:28.440 --> 00:17:33.440]   Woody Allen did interiors, which was his version of Ingmar Bergman.
[00:17:33.440 --> 00:17:34.440]   Yes.
[00:17:34.440 --> 00:17:35.440]   Scenes from a marriage.
[00:17:35.440 --> 00:17:37.640]   That's the only other example I could think of.
[00:17:37.640 --> 00:17:39.480]   Sophia Vergara.
[00:17:39.480 --> 00:17:40.480]   War of the Roses?
[00:17:40.480 --> 00:17:41.480]   Okay, you're right.
[00:17:41.480 --> 00:17:42.480]   War of the Roses.
[00:17:42.480 --> 00:17:43.480]   Never mind.
[00:17:43.480 --> 00:17:44.480]   I've spotted a cultural trend.
[00:17:44.480 --> 00:17:45.480]   That was the same time.
[00:17:45.480 --> 00:17:46.480]   Well, that's true.
[00:17:46.480 --> 00:17:49.400]   It wasn't.
[00:17:49.400 --> 00:17:57.560]   And there was that Dustin Hoffman movie anyway, where they're fighting over the kid.
[00:17:57.560 --> 00:18:00.800]   Which is my blue Valentine, but that was a movie.
[00:18:00.800 --> 00:18:01.800]   Those are all movies.
[00:18:01.800 --> 00:18:03.640]   But movies we have about divorce for years and years and years.
[00:18:03.640 --> 00:18:04.640]   TV shows.
[00:18:04.640 --> 00:18:05.640]   I don't know.
[00:18:05.640 --> 00:18:07.600]   Slummo said when he was a kid he used to watch divorce court.
[00:18:07.600 --> 00:18:08.880]   I remember that.
[00:18:08.880 --> 00:18:09.880]   Oh, yeah.
[00:18:09.880 --> 00:18:12.520]   That was like Judge Judy with divorce.
[00:18:12.520 --> 00:18:16.160]   Oh, man, this show has not started well.
[00:18:16.160 --> 00:18:17.160]   Let's start this over again.
[00:18:17.160 --> 00:18:19.920]   So there's something, what was I going to talk about here?
[00:18:19.920 --> 00:18:22.840]   Well, besides the Facebook stuff, which we got lots on.
[00:18:22.840 --> 00:18:29.480]   You and a guy from Baderworks wrote a piece for Medium that has been picked up widely.
[00:18:29.480 --> 00:18:30.480]   Very widely.
[00:18:30.480 --> 00:18:31.480]   Borthwick.
[00:18:31.480 --> 00:18:32.480]   John Borthwick.
[00:18:32.480 --> 00:18:33.480]   John Borthwick.
[00:18:33.480 --> 00:18:34.480]   Baderworks.
[00:18:34.480 --> 00:18:36.160]   So yeah, John reached out to me.
[00:18:36.160 --> 00:18:39.200]   I mean, I've been writing about the fake news stuff and reached out to me and said, let's
[00:18:39.200 --> 00:18:40.200]   be practical.
[00:18:40.200 --> 00:18:43.640]   So we went and sat for two hours again with 15 suggestions of what to do about it and
[00:18:43.640 --> 00:18:44.640]   put it out.
[00:18:44.640 --> 00:18:46.960]   Similar to what we had talked about last week.
[00:18:46.960 --> 00:18:50.360]   That's why I was asking because we were always the middle of writing it last week.
[00:18:50.360 --> 00:18:53.400]   So I got it written right up by the next morning, we put it out and got picked up a lot.
[00:18:53.400 --> 00:18:59.600]   And then Zuck, after kind of changing his mind, did write a post as we all saw with
[00:18:59.600 --> 00:19:03.720]   about six of the 15 suggestions that we had in parallel, which is good.
[00:19:03.720 --> 00:19:07.600]   And I put out a comment on Zuck's post saying, you know, what I you're doing this, and by
[00:19:07.600 --> 00:19:09.400]   the way, here's what Borthwick and I wrote.
[00:19:09.400 --> 00:19:11.000]   And Zuck liked the comment.
[00:19:11.000 --> 00:19:12.160]   So nice.
[00:19:12.160 --> 00:19:14.640]   So what did you propose?
[00:19:14.640 --> 00:19:19.880]   Basically, what you had talked about on this show, which was not censoring fake news,
[00:19:19.880 --> 00:19:25.680]   but giving people more information, some signals as they read it so that they could go deeper.
[00:19:25.680 --> 00:19:26.680]   Exactly.
[00:19:26.680 --> 00:19:29.520]   As I remember you poo pooed half of the last week, but that was the kind of the mood you
[00:19:29.520 --> 00:19:30.520]   were in.
[00:19:30.520 --> 00:19:32.280]   No, I'm still in that mood.
[00:19:32.280 --> 00:19:36.800]   I think blaming Facebook for this is reductionist.
[00:19:36.800 --> 00:19:38.360]   Oh, I agree.
[00:19:38.360 --> 00:19:40.120]   I absolutely agree.
[00:19:40.120 --> 00:19:44.320]   But can we improve the quality of discourse on the social platforms?
[00:19:44.320 --> 00:19:45.320]   Yes.
[00:19:45.320 --> 00:19:49.840]   Is there some large proportion of people who would prefer not to pass on lies?
[00:19:49.840 --> 00:19:50.840]   It's a question.
[00:19:50.840 --> 00:19:52.120]   That's a good question.
[00:19:52.120 --> 00:19:53.720]   I have to believe that's it.
[00:19:53.720 --> 00:19:55.280]   Well, and giving them the opportunity and the more information.
[00:19:55.280 --> 00:19:57.520]   And I agreed with you at the end of the conversation.
[00:19:57.520 --> 00:19:58.520]   That's all.
[00:19:58.520 --> 00:19:59.520]   Yeah.
[00:19:59.520 --> 00:20:00.520]   That's all.
[00:20:00.520 --> 00:20:02.880]   That giving everybody more information about the post they're about to share is not a bad
[00:20:02.880 --> 00:20:03.880]   thing.
[00:20:03.880 --> 00:20:04.880]   Yeah.
[00:20:04.880 --> 00:20:05.880]   So there's 50 suggestions.
[00:20:05.880 --> 00:20:07.880]   You're taking all the fun out of this, though.
[00:20:07.880 --> 00:20:08.880]   Yeah.
[00:20:08.880 --> 00:20:13.080]   Because I think I have this, I really think a lot of this stuff is shared not for its
[00:20:13.080 --> 00:20:20.280]   veracity, but more because it's emotionally true and it feels true.
[00:20:20.280 --> 00:20:23.000]   And this is how I feel right now and I'd like to share this.
[00:20:23.000 --> 00:20:27.120]   And it strikes me that that's really what social media is for is for you to share your
[00:20:27.120 --> 00:20:28.560]   feelings.
[00:20:28.560 --> 00:20:36.120]   And I think it's intrusive to ask a social media network to somehow say, well, your feelings
[00:20:36.120 --> 00:20:37.120]   are not okay.
[00:20:37.120 --> 00:20:39.560]   Well, I don't agree with your feelings.
[00:20:39.560 --> 00:20:43.840]   And when those feelings are presented as fact rather than just feelings, then I think it's
[00:20:43.840 --> 00:20:45.960]   perfectly legitimate to say, well, are you sure?
[00:20:45.960 --> 00:20:50.480]   You know, is that fact here's the facts as a journalist, I can't help but say that surely
[00:20:50.480 --> 00:20:56.280]   fact should enter into conversations and that fact based conversation will be in the long
[00:20:56.280 --> 00:20:58.440]   run more civil.
[00:20:58.440 --> 00:21:03.320]   Well, and can product one argument in your favor comes from the Wall Street Journal today.
[00:21:03.320 --> 00:21:06.280]   Most students don't know when news is fake.
[00:21:06.280 --> 00:21:11.880]   According to a Stanford study, 82% of middle schoolers couldn't distinguish between ad-labeled
[00:21:11.880 --> 00:21:15.720]   sponsored content and a real news story on a website.
[00:21:15.720 --> 00:21:17.000]   That's not a fake news problem.
[00:21:17.000 --> 00:21:18.000]   That's a business problem.
[00:21:18.000 --> 00:21:19.400]   That's the native content problem.
[00:21:19.400 --> 00:21:21.400]   And I've always thought that.
[00:21:21.400 --> 00:21:25.720]   But here's CUNY, we have a study coming out shortly about the similar thing, not just
[00:21:25.720 --> 00:21:27.320]   students but with people.
[00:21:27.320 --> 00:21:31.960]   And here's the interesting thing Leo, more people knew what was the sponsored story on
[00:21:31.960 --> 00:21:38.080]   Facebook than in news sites.
[00:21:38.080 --> 00:21:39.080]   More people on Facebook.
[00:21:39.080 --> 00:21:40.080]   Facebook is better labeled.
[00:21:40.080 --> 00:21:42.960]   Facebook has sponsored something better labeled than news sites.
[00:21:42.960 --> 00:21:47.640]   And we've always said native content, whatever people say is designed to trick the reader
[00:21:47.640 --> 00:21:50.680]   into thinking they're reading something that is not an advertisement.
[00:21:50.680 --> 00:21:52.600]   So that was part of the study.
[00:21:52.600 --> 00:21:56.760]   And two out of three middle schoolers couldn't see any valid reason to mistrust a post written
[00:21:56.760 --> 00:22:01.360]   by a bank executive arguing that young adults need more financial planning help.
[00:22:01.360 --> 00:22:02.720]   But here's the other side of it.
[00:22:02.720 --> 00:22:07.800]   Four in ten high school students believed based on the headline that a photo of deformed
[00:22:07.800 --> 00:22:13.320]   daisies on a photo sharing site provided strong evidence of toxic conditions near the
[00:22:13.320 --> 00:22:18.320]   Fukushima Daiichi nuclear plant in Japan, even though no source or location was given
[00:22:18.320 --> 00:22:20.080]   for the photo.
[00:22:20.080 --> 00:22:25.680]   So four out of ten high school kids, 82% of middle school kids.
[00:22:25.680 --> 00:22:29.760]   That's a legitimate audience for what you're saying, you know, young people.
[00:22:29.760 --> 00:22:36.600]   But I do think that much of the, you know, so-called fake news posted during the election
[00:22:36.600 --> 00:22:38.400]   was posted by people who knew it wasn't read.
[00:22:38.400 --> 00:22:42.760]   The pope didn't really endorse Donald Trump, but that's how they felt.
[00:22:42.760 --> 00:22:45.800]   That's what they were saying something by posting that.
[00:22:45.800 --> 00:22:49.120]   No, they really thought that was true.
[00:22:49.120 --> 00:22:50.120]   They really did.
[00:22:50.120 --> 00:22:52.920]   Or they thought it would bolster their cause to the point that convince others.
[00:22:52.920 --> 00:22:55.560]   Okay, it would bolster their cause, right?
[00:22:55.560 --> 00:22:57.000]   By the way, I love this post.
[00:22:57.000 --> 00:22:59.360]   This is from Tenzen Monkeys.
[00:22:59.360 --> 00:23:02.880]   Mark Zuckerberg is a hypocrite right next to so.
[00:23:02.880 --> 00:23:08.120]   So this is Luke Cabran writing right next to Mark Zuckerberg's post on November 18th.
[00:23:08.120 --> 00:23:09.840]   A lot of you asked what we're doing about misinformation.
[00:23:09.840 --> 00:23:10.840]   I wanted to give you an update.
[00:23:10.840 --> 00:23:16.480]   We take him seriously, blah, blah, blah, right next to it on on on lose feed.
[00:23:16.480 --> 00:23:18.960]   Maybe not yours were two ads.
[00:23:18.960 --> 00:23:21.600]   Hugh Hefner ends his life.
[00:23:21.600 --> 00:23:22.600]   Hollywood life.com.
[00:23:22.600 --> 00:23:23.600]   He will be missed.
[00:23:23.600 --> 00:23:27.800]   And sadday and PGA ESPN.com we've lost Tiger forever.
[00:23:27.800 --> 00:23:32.560]   Both were, of course, completely fake and they were advertisements.
[00:23:32.560 --> 00:23:39.360]   This one pointed to a site that's labeled Fox News with shocking news.
[00:23:39.360 --> 00:23:45.600]   The headline, Crystal Hefner reveals the miracle pill that cured Hughes ED permanently.
[00:23:45.600 --> 00:23:47.440]   Oh, Jesus.
[00:23:47.440 --> 00:23:49.720]   And sad day for PGA fans.
[00:23:49.720 --> 00:23:54.800]   That points to a site that does look like ESPN except it's not.
[00:23:54.800 --> 00:23:59.720]   It's ESPN dot com dash magazine dot online.
[00:23:59.720 --> 00:24:03.920]   And of course it ends up being a diet pill.
[00:24:03.920 --> 00:24:08.880]   So the point that Lou is making is if he marks like a bird can't keep this crap off
[00:24:08.880 --> 00:24:13.000]   his own posts, he's obviously got a problem here.
[00:24:13.000 --> 00:24:15.000]   We don't take misinformation seriously.
[00:24:15.000 --> 00:24:16.600]   He's taking money for those.
[00:24:16.600 --> 00:24:18.360]   He's made money on those.
[00:24:18.360 --> 00:24:27.120]   Well, I have to say I'm more that's more appalling to me and not because I think anybody believes
[00:24:27.120 --> 00:24:29.800]   it but just it's really annoying.
[00:24:29.800 --> 00:24:36.600]   It's just the worst kind of link bait.
[00:24:36.600 --> 00:24:46.640]   I also kind of don't like the fake news conversation because I feel like it's a little whiny.
[00:24:46.640 --> 00:24:51.400]   It's like people complaining about the result of the election and looking for a scapegoat.
[00:24:51.400 --> 00:24:55.560]   I wish we had this conversation before the election.
[00:24:55.560 --> 00:24:56.560]   Exactly.
[00:24:56.560 --> 00:24:57.560]   Exactly.
[00:24:57.560 --> 00:24:59.000]   Or six months afterwards that it was in a different context.
[00:24:59.000 --> 00:25:00.000]   It's not just about the election.
[00:25:00.000 --> 00:25:02.000]   It really does feel like it's whiny.
[00:25:02.000 --> 00:25:05.360]   Well, I mean, that's how change happens.
[00:25:05.360 --> 00:25:11.120]   And that's how we decide something's important is, you know, I complained about data caps
[00:25:11.120 --> 00:25:17.960]   for a freakin ever but only when a certain pain point had been reached, like when people
[00:25:17.960 --> 00:25:22.320]   are trying to download updates for video games or whatever, did it suddenly become big issue?
[00:25:22.320 --> 00:25:23.320]   Right?
[00:25:23.320 --> 00:25:28.200]   So, some people have been complaining about fake news but no one cared until after it
[00:25:28.200 --> 00:25:30.320]   possibly cost someone an election.
[00:25:30.320 --> 00:25:32.280]   I guess that's legitimate.
[00:25:32.280 --> 00:25:36.320]   And by the way, speaking of caps, hello, this is a courtesy notice from Comcast.
[00:25:36.320 --> 00:25:40.960]   Let you know you've reached 100% of your terabyte monthly data plan for your affinity
[00:25:40.960 --> 00:25:44.160]   internet service.
[00:25:44.160 --> 00:25:45.760]   And actually, this is not the last one I got.
[00:25:45.760 --> 00:25:48.440]   The next one, the day later said 110%.
[00:25:48.440 --> 00:25:49.440]   Yeah.
[00:25:49.440 --> 00:25:52.160]   You used a terabyte already?
[00:25:52.160 --> 00:25:53.160]   Easy.
[00:25:53.160 --> 00:25:54.880]   Oh, for broadcasting the show?
[00:25:54.880 --> 00:25:57.880]   No, not in the office at home.
[00:25:57.880 --> 00:25:58.880]   Easy.
[00:25:58.880 --> 00:26:02.120]   And I have two internet connections, by the way.
[00:26:02.120 --> 00:26:04.840]   I split it and I still did it.
[00:26:04.840 --> 00:26:07.720]   What are you doing?
[00:26:07.720 --> 00:26:10.760]   I'm watching TV.
[00:26:10.760 --> 00:26:11.760]   I noticed...
[00:26:11.760 --> 00:26:12.760]   It's a lot of television.
[00:26:12.760 --> 00:26:13.760]   4K?
[00:26:13.760 --> 00:26:14.760]   4K, baby.
[00:26:14.760 --> 00:26:15.760]   Okay.
[00:26:15.760 --> 00:26:22.840]   And the thing is, it doesn't take that much if you stream Pandora and you...
[00:26:22.840 --> 00:26:24.720]   I mean, we stream everything.
[00:26:24.720 --> 00:26:30.640]   Even if I'm going to watch an HBO show, I hate the Comcast on-demand so much that I'll
[00:26:30.640 --> 00:26:34.280]   end up watching it on HBO Go, which is streamed instead.
[00:26:34.280 --> 00:26:35.280]   So...
[00:26:35.280 --> 00:26:36.280]   Yeah.
[00:26:36.280 --> 00:26:37.280]   Yeah.
[00:26:37.280 --> 00:26:38.280]   That still feels like a lot.
[00:26:38.280 --> 00:26:39.280]   I mean, we watch...
[00:26:39.280 --> 00:26:40.280]   Oh, you'll get there.
[00:26:40.280 --> 00:26:41.280]   Okay.
[00:26:41.280 --> 00:26:42.280]   No, I'm just trying to think of...
[00:26:42.280 --> 00:26:45.160]   I'm a canary in the line, baby.
[00:26:45.160 --> 00:26:48.280]   We get 250 is probably how many?
[00:26:48.280 --> 00:26:49.720]   Just 250 gigabytes.
[00:26:49.720 --> 00:26:50.720]   So...
[00:26:50.720 --> 00:26:51.720]   That's your cap?
[00:26:51.720 --> 00:26:54.880]   No, that's how much we seem to use.
[00:26:54.880 --> 00:26:55.880]   Oh.
[00:26:55.880 --> 00:26:56.880]   So...
[00:26:56.880 --> 00:26:59.120]   Well, this is the first time I've ever gone through that cap, so maybe something is going
[00:26:59.120 --> 00:27:00.120]   on.
[00:27:00.120 --> 00:27:01.120]   I don't know.
[00:27:01.120 --> 00:27:06.480]   But I mean, if you're downloading 4K or video, it's 15...
[00:27:06.480 --> 00:27:07.480]   What is it?
[00:27:07.480 --> 00:27:08.480]   15 gigs an hour.
[00:27:08.480 --> 00:27:09.480]   So...
[00:27:09.480 --> 00:27:11.240]   I thought it was down to 8.
[00:27:11.240 --> 00:27:13.240]   I thought they had 15 megabytes.
[00:27:13.240 --> 00:27:14.240]   15 megabytes an hour.
[00:27:14.240 --> 00:27:15.240]   What am I saying?
[00:27:15.240 --> 00:27:17.480]   It was 8 gigabytes because HD is 2 gigabytes.
[00:27:17.480 --> 00:27:21.120]   15 megabits, I'm sorry, 15 megabits, so I have to do the math.
[00:27:21.120 --> 00:27:22.120]   Yeah.
[00:27:22.120 --> 00:27:23.120]   It's a lot.
[00:27:23.120 --> 00:27:24.120]   Okay.
[00:27:24.120 --> 00:27:25.120]   It's a lot.
[00:27:25.120 --> 00:27:26.120]   It's a lot.
[00:27:26.120 --> 00:27:27.120]   It's a lot.
[00:27:27.120 --> 00:27:28.120]   All right.
[00:27:28.120 --> 00:27:33.440]   I do think a terabyte, which sounds like a lot, is not.
[00:27:33.440 --> 00:27:39.160]   And it's really Comcast doing what they think people will ignore until such time.
[00:27:39.160 --> 00:27:40.880]   It's exactly what you said, Stacey.
[00:27:40.880 --> 00:27:44.960]   I'm like, I have preached to this choir any time.
[00:27:44.960 --> 00:27:45.960]   Yeah.
[00:27:45.960 --> 00:27:46.960]   No, you nailed it.
[00:27:46.960 --> 00:27:53.680]   And as somebody who's now experiencing it, I will agree with you.
[00:27:53.680 --> 00:27:54.680]   Oh, well...
[00:27:54.680 --> 00:27:55.680]   I was going to say...
[00:27:55.680 --> 00:27:57.240]   I was going to go ahead with Stacey, sorry.
[00:27:57.240 --> 00:28:01.440]   No, I was going to segue away from fake news to Intel's wearable stuff, but we should
[00:28:01.440 --> 00:28:04.160]   stick with Google for a while before we go totally off track.
[00:28:04.160 --> 00:28:05.160]   So...
[00:28:05.160 --> 00:28:06.640]   Let's do our Pixel phone thing.
[00:28:06.640 --> 00:28:08.960]   So a couple of things going on.
[00:28:08.960 --> 00:28:13.080]   Initially, we were getting reports, the Guardian reported that people who were buying
[00:28:13.080 --> 00:28:18.840]   the Pixel phone and then selling it on were getting not only blocked from their Pixel
[00:28:18.840 --> 00:28:22.040]   account, but their entire Google account.
[00:28:22.040 --> 00:28:27.360]   Now Google has unlocked those, has relented according to Engadget.
[00:28:27.360 --> 00:28:29.840]   The digital death penalty has been lifted.
[00:28:29.840 --> 00:28:34.640]   As someone who was often defended Google, this time I'm going to be the executive officer,
[00:28:34.640 --> 00:28:38.360]   this was an abusive power, straight and simple.
[00:28:38.360 --> 00:28:42.680]   This is exactly what people fear about Google tying accounts together, and it can use its
[00:28:42.680 --> 00:28:46.600]   power in one realm to enforce its view in the other realm.
[00:28:46.600 --> 00:28:50.200]   And I think it was a huge mistake for Google to kill these accounts.
[00:28:50.200 --> 00:28:54.040]   Google does say in the terms of service, you may not resell devices purchased for that
[00:28:54.040 --> 00:28:55.040]   online store.
[00:28:55.040 --> 00:28:57.800]   But we're going to cut you off from the entirety of all of the rest of Google, which controls
[00:28:57.800 --> 00:28:59.760]   half the world of Facebook doesn't control.
[00:28:59.760 --> 00:29:00.760]   That's true.
[00:29:00.760 --> 00:29:01.760]   And that's what's wrong.
[00:29:01.760 --> 00:29:03.240]   No Google Maps for you.
[00:29:03.240 --> 00:29:04.600]   No ways for you.
[00:29:04.600 --> 00:29:06.240]   No Gmail for you.
[00:29:06.240 --> 00:29:07.800]   No Docs for you.
[00:29:07.800 --> 00:29:10.400]   No search for you because you're something we didn't like.
[00:29:10.400 --> 00:29:15.000]   So Google says, Google says it's unlocking the accounts because customers weren't aware
[00:29:15.000 --> 00:29:19.120]   of the role, but they also say many of the accounts were created for the sole purpose
[00:29:19.120 --> 00:29:20.120]   of this scheme.
[00:29:20.120 --> 00:29:22.800]   In other words, we created an account to buy a Pixel.
[00:29:22.800 --> 00:29:26.560]   I'm just saying Google better be careful because because you know, I just come back from
[00:29:26.560 --> 00:29:29.680]   Europe and it's the same view that I've never.
[00:29:29.680 --> 00:29:33.920]   And this is the case where I'll agree with the critics to say, watch out Google.
[00:29:33.920 --> 00:29:40.240]   If you overuse this tremendous power you have, then the critics are going to get you.
[00:29:40.240 --> 00:29:42.240]   And they'll be right.
[00:29:42.240 --> 00:29:49.720]   I kind of see their point and I'm going to say, hey, I mean, giving it back to them because
[00:29:49.720 --> 00:29:54.200]   they didn't know the rule is good because no one reads all of the terms and service,
[00:29:54.200 --> 00:29:55.200]   terms of service.
[00:29:55.200 --> 00:29:59.000]   But I mean, you're abusing their accounts.
[00:29:59.000 --> 00:30:00.640]   There are alternatives out there.
[00:30:00.640 --> 00:30:05.640]   And if you don't like them, just it sucks for you, but you did.
[00:30:05.640 --> 00:30:08.160]   It wouldn't matter if it were your Flickr account.
[00:30:08.160 --> 00:30:13.120]   I think Jeff, this is kind of I won't put words in your mouth, but because Google is
[00:30:13.120 --> 00:30:19.600]   so much more important than just, you know, an account, it's your, it's your email.
[00:30:19.600 --> 00:30:20.600]   It's your browsing history.
[00:30:20.600 --> 00:30:21.600]   It's a lot.
[00:30:21.600 --> 00:30:22.600]   I mean, it's everything now.
[00:30:22.600 --> 00:30:23.600]   How would you?
[00:30:23.600 --> 00:30:24.600]   How would you?
[00:30:24.600 --> 00:30:26.440]   Well, how would you do without a Google account?
[00:30:26.440 --> 00:30:27.680]   I guess you could get a different one.
[00:30:27.680 --> 00:30:28.680]   I knew one, right?
[00:30:28.680 --> 00:30:30.680]   But you could lose all of your email.
[00:30:30.680 --> 00:30:31.680]   I load everything.
[00:30:31.680 --> 00:30:36.160]   I mean, I save my contacts, not just to Google, but to my phone.
[00:30:36.160 --> 00:30:38.240]   Yeah, but you're my photos to drop off.
[00:30:38.240 --> 00:30:39.240]   You're one of the few.
[00:30:39.240 --> 00:30:41.880]   I think most people do not, especially Gmail.
[00:30:41.880 --> 00:30:45.200]   How many people back up their Gmail?
[00:30:45.200 --> 00:30:46.200]   Do you?
[00:30:46.200 --> 00:30:47.200]   I do.
[00:30:47.200 --> 00:30:48.200]   It's my brain.
[00:30:48.200 --> 00:30:49.200]   Yeah, you're smart.
[00:30:49.200 --> 00:30:50.200]   And I would recommend.
[00:30:50.200 --> 00:30:51.200]   I was on a company go bust.
[00:30:51.200 --> 00:30:53.680]   So I was like, oh, yeah, that's right.
[00:30:53.680 --> 00:30:57.280]   When you, when Gigo Home went under, you had no warning, right?
[00:30:57.280 --> 00:31:01.600]   I had an hours warning, but they actually gave us our accounts for a little bit longer.
[00:31:01.600 --> 00:31:06.080]   So we had time to do a backup, but we didn't know we were going to have time to do a backup.
[00:31:06.080 --> 00:31:08.960]   So yeah, I mean, that's a significant deal.
[00:31:08.960 --> 00:31:14.640]   There are programs that will back up your entire Gmail account.
[00:31:14.640 --> 00:31:19.280]   And maybe this would be a good thing to consider.
[00:31:19.280 --> 00:31:20.960]   You can do it from Gmail directly.
[00:31:20.960 --> 00:31:22.600]   There's Google Takeout.
[00:31:22.600 --> 00:31:25.680]   There are programs that you can download or you could actually--
[00:31:25.680 --> 00:31:27.320]   There should be companies in the past.
[00:31:27.320 --> 00:31:29.280]   There should be companies that just do this for you.
[00:31:29.280 --> 00:31:30.680]   Back up all your stuff for you.
[00:31:30.680 --> 00:31:31.680]   Yeah.
[00:31:31.680 --> 00:31:32.680]   There used to be--
[00:31:32.680 --> 00:31:33.680]   What was it?
[00:31:33.680 --> 00:31:34.680]   Back up a fight.
[00:31:34.680 --> 00:31:35.680]   Back up a fight.
[00:31:35.680 --> 00:31:38.680]   But there was also like Spanning Cloud, but I think Google bought them.
[00:31:38.680 --> 00:31:40.680]   There is a--
[00:31:40.680 --> 00:31:41.680]   Google's going to buy you.
[00:31:41.680 --> 00:31:45.080]   There's a Mac app I had that did this for you.
[00:31:45.080 --> 00:31:47.560]   And it did it continually.
[00:31:47.560 --> 00:31:49.080]   In the background, download it locally.
[00:31:49.080 --> 00:31:53.080]   Back up a fight was Cloud backup, which is kind of cool.
[00:31:53.080 --> 00:31:55.520]   Anyway, it'd be easy for anybody to find this.
[00:31:55.520 --> 00:31:59.920]   But I would bet 99% of Google mail users don't do that.
[00:31:59.920 --> 00:32:01.720]   That's a really unusual thing.
[00:32:01.720 --> 00:32:03.720]   So backup a fight is still around.
[00:32:03.720 --> 00:32:04.720]   Oh.
[00:32:04.720 --> 00:32:07.640]   Yeah, when they first started, they had free backup services.
[00:32:07.640 --> 00:32:09.640]   And I think now it's all paid.
[00:32:09.640 --> 00:32:11.400]   Someone's saying mail store.
[00:32:11.400 --> 00:32:12.400]   Mail store.
[00:32:12.400 --> 00:32:13.400]   We'll back up our--
[00:32:13.400 --> 00:32:14.400]   Sorter or--
[00:32:14.400 --> 00:32:15.400]   Sorter.
[00:32:15.400 --> 00:32:16.400]   Looks like.
[00:32:16.400 --> 00:32:17.400]   Yeah.
[00:32:17.400 --> 00:32:20.720]   Yeah, I used backup a fight for a long time.
[00:32:20.720 --> 00:32:24.920]   I think I was grand-- initially, grandfathered in on a free account.
[00:32:24.920 --> 00:32:28.960]   They got purchased in 2014.
[00:32:28.960 --> 00:32:33.120]   I think that they're not quite the service they were.
[00:32:33.120 --> 00:32:36.360]   Oh, mail store, the standard in email archiving.
[00:32:36.360 --> 00:32:38.320]   Nice.
[00:32:38.320 --> 00:32:39.880]   I like that.
[00:32:39.880 --> 00:32:43.000]   Wait, there's one that's mail store with an E.
[00:32:43.000 --> 00:32:46.960]   And then there's mail store without an E.
[00:32:46.960 --> 00:32:47.920]   There's two.
[00:32:47.920 --> 00:32:49.240]   Have you seen?
[00:32:49.240 --> 00:32:51.000]   And if you haven't, get ready.
[00:32:51.000 --> 00:32:54.360]   The new Google Duo ad with Blake and DeAndre.
[00:32:54.360 --> 00:32:55.360]   Yes, it's so silly.
[00:32:55.360 --> 00:32:56.360]   Isn't that Blake?
[00:32:56.360 --> 00:32:58.320]   I thought he said he had practice.
[00:32:58.320 --> 00:33:01.680]   This is, by the way, that's a bottle of soda talking.
[00:33:01.680 --> 00:33:02.880]   No, it's mustard.
[00:33:02.880 --> 00:33:03.880]   Mustard.
[00:33:03.880 --> 00:33:04.880]   Mustards.
[00:33:04.880 --> 00:33:05.880]   Oh, they're looking right there.
[00:33:05.880 --> 00:33:06.880]   There's the ketchup.
[00:33:06.880 --> 00:33:07.880]   Nope.
[00:33:07.880 --> 00:33:08.880]   And now they're calling.
[00:33:08.880 --> 00:33:09.880]   And the mustard.
[00:33:09.880 --> 00:33:10.880]   What do I say?
[00:33:10.880 --> 00:33:12.880]   DeAndre's mustard is now duoing.
[00:33:12.880 --> 00:33:14.880]   Hey, what's up?
[00:33:14.880 --> 00:33:17.720]   Who are you dunking with over there?
[00:33:17.720 --> 00:33:18.720]   What?
[00:33:18.720 --> 00:33:20.920]   No, I was just-- what?
[00:33:20.920 --> 00:33:23.000]   I thought we were a team.
[00:33:23.000 --> 00:33:24.000]   No.
[00:33:24.000 --> 00:33:26.560]   No, no, it's cool.
[00:33:26.560 --> 00:33:27.560]   You do you.
[00:33:27.560 --> 00:33:30.800]   The mustard's doing his talking.
[00:33:30.800 --> 00:33:32.480]   You do you.
[00:33:32.480 --> 00:33:34.320]   Are you done?
[00:33:34.320 --> 00:33:36.440]   I don't know.
[00:33:36.440 --> 00:33:37.960]   You're still coming to the party on Sunday?
[00:33:37.960 --> 00:33:38.960]   I have plans.
[00:33:38.960 --> 00:33:43.180]   You're just going to be pigs in a blanket.
[00:33:43.180 --> 00:33:44.320]   All right.
[00:33:44.320 --> 00:33:45.320]   You're in?
[00:33:45.320 --> 00:33:46.320]   I mean--
[00:33:46.320 --> 00:33:47.320]   Get off.
[00:33:47.320 --> 00:33:48.320]   Go.
[00:33:48.320 --> 00:33:49.320]   Go.
[00:33:49.320 --> 00:33:50.320]   Go.
[00:33:50.320 --> 00:33:51.320]   Go, go.
[00:33:51.320 --> 00:33:52.320]   I got you.
[00:33:52.320 --> 00:33:53.320]   I got you.
[00:33:53.320 --> 00:33:55.320]   Vaguely obscene.
[00:33:55.320 --> 00:33:58.240]   For duos who can't stay mad at each other.
[00:33:58.240 --> 00:34:01.880]   Hey, OK, so now since we're going, I'll come out and pick you up.
[00:34:01.880 --> 00:34:03.480]   I'll just meet you there.
[00:34:03.480 --> 00:34:04.480]   Perfect.
[00:34:04.480 --> 00:34:05.480]   I'll pick you up.
[00:34:05.480 --> 00:34:06.480]   Who are Blake and DeAndre?
[00:34:06.480 --> 00:34:07.480]   Am I missing something?
[00:34:07.480 --> 00:34:10.360]   They're NBA basketball players, yeah.
[00:34:10.360 --> 00:34:12.160]   I think so.
[00:34:12.160 --> 00:34:13.160]   That's all I mean.
[00:34:13.160 --> 00:34:14.160]   Are they?
[00:34:14.160 --> 00:34:15.160]   Because if it sports, it's out of my leash.
[00:34:15.160 --> 00:34:16.160]   Chatroom, yeah.
[00:34:16.160 --> 00:34:21.360]   We're not sport ballers here, but chatroom, do you know who that is?
[00:34:21.360 --> 00:34:22.360]   No one knows who that is.
[00:34:22.360 --> 00:34:25.440]   Is it a real American round?
[00:34:25.440 --> 00:34:29.440]   They probably-- they didn't want to spend the money on actual NBA players, so they got--
[00:34:29.440 --> 00:34:30.440]   I don't know.
[00:34:30.440 --> 00:34:35.880]   Does it make you want to use-- does it even give you any understanding of what duo does?
[00:34:35.880 --> 00:34:36.880]   Yes.
[00:34:36.880 --> 00:34:37.880]   It does.
[00:34:37.880 --> 00:34:38.880]   It does.
[00:34:38.880 --> 00:34:39.880]   It's quirky.
[00:34:39.880 --> 00:34:42.440]   I think it appeals maybe to younger people maybe.
[00:34:42.440 --> 00:34:46.440]   OK, those two guys play for the LA Clippers, Steve Balmer's team.
[00:34:46.440 --> 00:34:47.680]   Oh, there you go.
[00:34:47.680 --> 00:34:48.680]   Yeah.
[00:34:48.680 --> 00:34:50.680]   Oh, and they're advertising Google products?
[00:34:50.680 --> 00:34:51.680]   And trouble.
[00:34:51.680 --> 00:34:52.680]   Oh.
[00:34:52.680 --> 00:34:53.680]   Oh.
[00:34:53.680 --> 00:34:59.920]   I mean, you can see that they-- the guy's like, oh crap, I know who's calling because
[00:34:59.920 --> 00:35:05.600]   I could see them on the screen, hold up, and he even knows that the guy-- the mustard
[00:35:05.600 --> 00:35:09.400]   or ketchup is mad at him because he's got the frowny face.
[00:35:09.400 --> 00:35:10.640]   I mean, it's silly.
[00:35:10.640 --> 00:35:16.080]   And by the way, they knew that Dayondre could act because he apparently has one of the most
[00:35:16.080 --> 00:35:19.680]   famous reaction gifs on Twitter.
[00:35:19.680 --> 00:35:23.680]   So that's how they knew.
[00:35:23.680 --> 00:35:24.680]   Nice.
[00:35:24.680 --> 00:35:27.640]   You know, it's easy to become a star these days.
[00:35:27.640 --> 00:35:29.280]   Just get on the Twitter.
[00:35:29.280 --> 00:35:33.280]   Hell, anybody can become president of that way.
[00:35:33.280 --> 00:35:34.280]   Did you?
[00:35:34.280 --> 00:35:35.280]   Yeah.
[00:35:35.280 --> 00:35:36.280]   He's still tweeting.
[00:35:36.280 --> 00:35:37.280]   Oh, yeah.
[00:35:37.280 --> 00:35:38.280]   I'm thrilled.
[00:35:38.280 --> 00:35:44.960]   I think we were going to have the best unmediated view of the presidency we've ever had.
[00:35:44.960 --> 00:35:48.800]   I don't think it's going to be the type of presidency, though, that has historically
[00:35:48.800 --> 00:35:49.800]   existed.
[00:35:49.800 --> 00:35:51.360]   I don't-- well, I'll tell you why.
[00:35:51.360 --> 00:35:52.360]   --a whole new realm.
[00:35:52.360 --> 00:35:56.880]   We don't know what he's going to do, so we really need the Twitter to keep track because
[00:35:56.880 --> 00:35:59.360]   apparently talking to The New York Times, he changed everything.
[00:35:59.360 --> 00:36:01.680]   It's like, well, yeah, I like the Clintons.
[00:36:01.680 --> 00:36:03.120]   I don't want to hurt them.
[00:36:03.120 --> 00:36:05.440]   And it's not really a wall.
[00:36:05.440 --> 00:36:10.800]   I mean, so the Twitter is going to be very valuable in parsing all of this.
[00:36:10.800 --> 00:36:15.000]   I don't think that Donald's Twitter account is actually valuable.
[00:36:15.000 --> 00:36:17.000]   I mean, it's not going to help us parse anything.
[00:36:17.000 --> 00:36:20.720]   It's just like, is it speaking to us in 140 characters?
[00:36:20.720 --> 00:36:22.600]   Yeah, I love it.
[00:36:22.600 --> 00:36:27.160]   But the point is that Michael Wolfe just said in an interview that the reporter's job is
[00:36:27.160 --> 00:36:29.320]   to be a stenographer and is to listen to what they say.
[00:36:29.320 --> 00:36:30.320]   We don't need that anymore.
[00:36:30.320 --> 00:36:31.320]   No.
[00:36:31.320 --> 00:36:33.560]   Because he can reach directly in whatever he wants to say to the world.
[00:36:33.560 --> 00:36:34.560]   Yes, well, look at that.
[00:36:34.560 --> 00:36:36.760]   He made a YouTube video saying, here's my policy.
[00:36:36.760 --> 00:36:37.960]   I don't need the press.
[00:36:37.960 --> 00:36:39.600]   Here's my policy.
[00:36:39.600 --> 00:36:42.400]   I mean, that's the equivalent of FDR's fireside chats.
[00:36:42.400 --> 00:36:44.800]   That's going right to the public.
[00:36:44.800 --> 00:36:47.080]   And bypassing all the gatekeepers.
[00:36:47.080 --> 00:36:49.720]   Now he just has to make it happen.
[00:36:49.720 --> 00:36:50.720]   Now he just has to make it happen.
[00:36:50.720 --> 00:36:52.480]   Very interesting article.
[00:36:52.480 --> 00:36:57.640]   I was at Bloomberg Business Week about Jared Kushner, who apparently that's, of course,
[00:36:57.640 --> 00:36:59.240]   President-elect Trump's son-in-law.
[00:36:59.240 --> 00:37:02.520]   He's married to Ivanka.
[00:37:02.520 --> 00:37:14.080]   He is apparently the architect of this social media strategy.
[00:37:14.080 --> 00:37:16.160]   It's quite interesting.
[00:37:16.160 --> 00:37:20.320]   We often talk about how President Nixon, there it is.
[00:37:20.320 --> 00:37:29.120]   President Nixon was the first president to use an ad agency in his election bid in '68.
[00:37:29.120 --> 00:37:34.640]   And how Obama is in many ways the first social media president.
[00:37:34.640 --> 00:37:44.320]   But what Kushner figured out and did so well was how to not spend money traditionally on
[00:37:44.320 --> 00:37:48.040]   advertising as Forbes that did the interview.
[00:37:48.040 --> 00:37:55.240]   Not spend money on traditional advertising, but instead very cleverly use Facebook.
[00:37:55.240 --> 00:37:59.440]   And it starts with a company called Cambridge Analytica.
[00:37:59.440 --> 00:38:02.520]   And I've just found this endlessly fascinating.
[00:38:02.520 --> 00:38:04.360]   Do you guys follow this whole story?
[00:38:04.360 --> 00:38:05.360]   Cambridge Analytica.
[00:38:05.360 --> 00:38:06.360]   There's another story about them.
[00:38:06.360 --> 00:38:08.080]   They've been doing these polls on Facebook.
[00:38:08.080 --> 00:38:09.080]   Is that the one?
[00:38:09.080 --> 00:38:10.080]   Yeah.
[00:38:10.080 --> 00:38:13.080]   So Cambridge Analytica has been around for a while.
[00:38:13.080 --> 00:38:18.080]   They specialize in, I don't know, how would you characterize it?
[00:38:18.080 --> 00:38:19.880]   Let's go to their website.
[00:38:19.880 --> 00:38:21.960]   Character polls.
[00:38:21.960 --> 00:38:23.480]   Better audience targeting.
[00:38:23.480 --> 00:38:25.200]   Better audience targeting.
[00:38:25.200 --> 00:38:28.000]   Powered by smarter data modeling.
[00:38:28.000 --> 00:38:33.440]   So what they've been doing, what they were doing is doing quizzes on Facebook.
[00:38:33.440 --> 00:38:35.480]   You've probably seen these.
[00:38:35.480 --> 00:38:37.160]   We've all taken them.
[00:38:37.160 --> 00:38:38.400]   Personality quizzes.
[00:38:38.400 --> 00:38:40.400]   And they just seem like a lot of fun.
[00:38:40.400 --> 00:38:42.800]   Yeah, but you know, everybody does it.
[00:38:42.800 --> 00:38:43.800]   Let's do one here.
[00:38:43.800 --> 00:38:45.480]   I get to know your real personality.
[00:38:45.480 --> 00:38:48.040]   This is actually on the Cambridge Analytica site.
[00:38:48.040 --> 00:38:50.480]   I have a strong, I have a vivid imagination.
[00:38:50.480 --> 00:38:51.480]   Yeah, I definitely do.
[00:38:51.480 --> 00:38:52.960]   I believe in the importance of art.
[00:38:52.960 --> 00:38:53.960]   I don't agree.
[00:38:53.960 --> 00:38:54.960]   I seldom feel blue.
[00:38:54.960 --> 00:38:56.640]   Nah, no, I feel blue.
[00:38:56.640 --> 00:38:57.640]   I have a sharp tongue.
[00:38:57.640 --> 00:38:58.640]   Oh God, yes.
[00:38:58.640 --> 00:39:00.520]   I'm not interested in abstract ideas.
[00:39:00.520 --> 00:39:02.120]   No, I don't, I like abstract.
[00:39:02.120 --> 00:39:03.720]   I find it difficult to get down to work.
[00:39:03.720 --> 00:39:04.720]   I'm doing this fast.
[00:39:04.720 --> 00:39:05.960]   I do not panic easily.
[00:39:05.960 --> 00:39:07.240]   I tend to vote for liberal.
[00:39:07.240 --> 00:39:09.480]   Oh, this is interesting.
[00:39:09.480 --> 00:39:10.480]   Liberal political candidates.
[00:39:10.480 --> 00:39:11.480]   Yes.
[00:39:11.480 --> 00:39:12.480]   I'm not easily bothered by.
[00:39:12.480 --> 00:39:14.400]   There's only two Republican candidates.
[00:39:14.400 --> 00:39:15.400]   Yeah.
[00:39:15.400 --> 00:39:17.880]   So what they'll do is these quizzes and if they don't all look like this, you know, it's
[00:39:17.880 --> 00:39:21.480]   often which member of the Downton Abbey family are you, right?
[00:39:21.480 --> 00:39:23.200]   We've all done these.
[00:39:23.200 --> 00:39:27.480]   But what they were doing behind the scenes is gathering information, very, very elaborate
[00:39:27.480 --> 00:39:35.040]   databases on Facebook users, which the then used to target what they're called in dark
[00:39:35.040 --> 00:39:36.040]   advertising.
[00:39:36.040 --> 00:39:43.440]   And I found this very interesting and it was shortly after Trump won the Republican nomination
[00:39:43.440 --> 00:39:49.120]   that Kushner went to Cambridge Analytica and hired them and they started spending money.
[00:39:49.120 --> 00:39:52.000]   He calls it, he says, this is like money ball.
[00:39:52.000 --> 00:39:56.960]   Remember money ball was that wonderful book about how the Boston Red Sox became world
[00:39:56.960 --> 00:39:57.960]   channel.
[00:39:57.960 --> 00:39:58.960]   No, I'm sorry.
[00:39:58.960 --> 00:39:59.960]   Oakland A's became world champions.
[00:39:59.960 --> 00:40:04.720]   The Red Sox use it later using analytics findings, finding players that were undervalued.
[00:40:04.720 --> 00:40:08.680]   Nobody wanted because they didn't really fully understand their value, but modern data analytics,
[00:40:08.680 --> 00:40:13.120]   let them know which players were worth, you know, getting even though they were cheap
[00:40:13.120 --> 00:40:14.400]   because nobody knew it was that valuable.
[00:40:14.400 --> 00:40:15.400]   They were in there.
[00:40:15.400 --> 00:40:19.680]   They created a winning Oakland A's team and then later a winning Boston Red Sox team and
[00:40:19.680 --> 00:40:23.240]   this season a winning Chicago Cubs team.
[00:40:23.240 --> 00:40:34.120]   The same exact technique Kushner used to targeting its advertising very effectively.
[00:40:34.120 --> 00:40:36.760]   For instance, here's this is from the Forbes article.
[00:40:36.760 --> 00:40:42.600]   First Kushner dabbled engaging in what amounted to a beta test using Trump merchandise.
[00:40:42.600 --> 00:40:46.000]   He says, I got to, I called somebody I knew and I got a tutorial on how to use Facebook
[00:40:46.000 --> 00:40:47.680]   micro targeting.
[00:40:47.680 --> 00:40:53.440]   The synced with Trump's blunt, simple messaging, the Trump campaign went from selling $8,000
[00:40:53.440 --> 00:41:00.120]   worth of hats a day to 80,000 generating revenue, expanding the number of human billboards,
[00:41:00.120 --> 00:41:03.840]   but most importantly, proving a concept.
[00:41:03.840 --> 00:41:08.680]   Then he spent $160,000 for a series of low tech policy videos of Trump talking straight
[00:41:08.680 --> 00:41:14.200]   into the camera got 74 million views.
[00:41:14.200 --> 00:41:20.120]   They actually had an organization in San Antonio with 100 person data hub collecting
[00:41:20.120 --> 00:41:24.680]   information, but the best thing he did was he hired Cambridge Analytica to buy dark ads
[00:41:24.680 --> 00:41:30.320]   specifically targeting people because it knew a lot about your Facebook interests and habits,
[00:41:30.320 --> 00:41:31.680]   not nearly about your bent.
[00:41:31.680 --> 00:41:39.280]   For instance, if your worldview was grim, they would show you ads that were kind of the world
[00:41:39.280 --> 00:41:41.360]   sucks and we're going to make it better.
[00:41:41.360 --> 00:41:44.840]   If you were an optimist, they'd show you a different kind of ad they micro targeted to
[00:41:44.840 --> 00:41:47.960]   your emotional tenor, your personality and very.
[00:41:47.960 --> 00:41:51.720]   You want to feel like a violation of privacy of your own head.
[00:41:51.720 --> 00:41:53.000]   It's not Facebook that knows that.
[00:41:53.000 --> 00:41:58.720]   It's a company that made you do these stupid quizzes that can then type you and knows the
[00:41:58.720 --> 00:42:00.760]   true psychographics of where you are.
[00:42:00.760 --> 00:42:02.640]   Ineffective ads were killed in minutes.
[00:42:02.640 --> 00:42:03.640]   Successful ones scaled.
[00:42:03.640 --> 00:42:08.760]   The campaign was sending more than 100,000 uniquely tweaked ads to targeted voters each
[00:42:08.760 --> 00:42:11.400]   day.
[00:42:11.400 --> 00:42:17.640]   So imagine this now, right now this is pretty unsophisticated.
[00:42:17.640 --> 00:42:23.360]   I know it sounds super sophisticated, but imagine it even more targeted, more individualized
[00:42:23.360 --> 00:42:30.680]   and happening faster because you have access to AI that can parse people's emotions and
[00:42:30.680 --> 00:42:33.920]   do the say be testing even faster.
[00:42:33.920 --> 00:42:36.280]   That's the world we're going to live in.
[00:42:36.280 --> 00:42:41.800]   That's the advertising world we're going to live in.
[00:42:41.800 --> 00:42:45.080]   It'll be more of the same.
[00:42:45.080 --> 00:42:46.960]   You can't stop this.
[00:42:46.960 --> 00:42:53.040]   No, and it'll be the next time the Democrats will use it because this is now known.
[00:42:53.040 --> 00:42:54.520]   Super effective.
[00:42:54.520 --> 00:42:55.680]   It's very effective.
[00:42:55.680 --> 00:43:01.200]   But I think what's important is that we understand what's going on here because this is, I think
[00:43:01.200 --> 00:43:02.640]   this is going to be widely used.
[00:43:02.640 --> 00:43:07.400]   This is why I brought in the context of Nixon was the first person to bring in ad executives.
[00:43:07.400 --> 00:43:14.200]   That was a revolution, believe it or not, 1968 to have ad execs creating your advertising.
[00:43:14.200 --> 00:43:16.920]   It was very effective.
[00:43:16.920 --> 00:43:23.360]   Obama was very effective in fundraising in 2008, bringing in a whole data team, people
[00:43:23.360 --> 00:43:28.400]   like Harper Reid and very cleverly use this.
[00:43:28.400 --> 00:43:32.520]   This has been taken to the next level, absolutely.
[00:43:32.520 --> 00:43:37.720]   It really goes to a larger question, not just in campaigns, but about advertising ethically
[00:43:37.720 --> 00:43:39.240]   all around and what's known about it.
[00:43:39.240 --> 00:43:41.080]   Is this unethical?
[00:43:41.080 --> 00:43:44.120]   It's creepy.
[00:43:44.120 --> 00:43:45.120]   It's creepy.
[00:43:45.120 --> 00:43:52.320]   I'll give you a scenario for lack of ethics in a second, but I think it goes back to also
[00:43:52.320 --> 00:43:55.840]   the Vance Packard and the Hidden Persuaders from the 1950s.
[00:43:55.840 --> 00:43:56.840]   That's right.
[00:43:56.840 --> 00:43:59.960]   This idea that we were being sold things in ways we didn't know.
[00:43:59.960 --> 00:44:01.440]   There was a lack of transparency about it.
[00:44:01.440 --> 00:44:03.440]   It was some liminal advertising.
[00:44:03.440 --> 00:44:06.760]   Well, this calls are those same creep reflexes.
[00:44:06.760 --> 00:44:11.520]   The way it could be, I've told the story on the show before, the guy on Facebook who
[00:44:11.520 --> 00:44:15.000]   talked with his family about symptoms he was feeling and then suddenly an ad for multiple
[00:44:15.000 --> 00:44:18.560]   sclerosis came up and that's how he got the diagnosis.
[00:44:18.560 --> 00:44:19.560]   That's so weird.
[00:44:19.560 --> 00:44:22.520]   Now, that case, I would argue that's very helpful in that case.
[00:44:22.520 --> 00:44:24.240]   He didn't know what he had.
[00:44:24.240 --> 00:44:28.040]   It found the signals and it was, and it was, and don't he was making money on that really?
[00:44:28.040 --> 00:44:31.560]   The ad was there, but it was kind of another here and there.
[00:44:31.560 --> 00:44:35.120]   Whereas in this case, it's manipulative to the extreme.
[00:44:35.120 --> 00:44:39.240]   The real question just as when you go back to the hidden Persuaders is how much are we
[00:44:39.240 --> 00:44:42.640]   being manipulated in ways we don't know?
[00:44:42.640 --> 00:44:43.640]   The lack of transparency.
[00:44:43.640 --> 00:44:46.360]   If you know it's, it's a native advertising too.
[00:44:46.360 --> 00:44:50.920]   If you know it's an ad and you choose to pay attention to it or not, that's your privilege.
[00:44:50.920 --> 00:44:56.280]   But to have things that are masquerading not as ads, but as fun quizzes that are then used
[00:44:56.280 --> 00:45:02.240]   to change your view of the world and elect a president, at some point there's an ethics.
[00:45:02.240 --> 00:45:03.600]   Now, Stacy's absolutely right.
[00:45:03.600 --> 00:45:08.080]   Even if you have a community that has an ethical statement around this, others will say, damn
[00:45:08.080 --> 00:45:13.000]   the ethics full speed ahead and use these techniques.
[00:45:13.000 --> 00:45:18.280]   And I don't know what to do about it, but it's it bothers me.
[00:45:18.280 --> 00:45:22.440]   It's almost one of the reasons why I don't think fake news is as much the the issue because
[00:45:22.440 --> 00:45:26.960]   if fake news were blocked on Facebook, you'd still be getting the materials still becoming
[00:45:26.960 --> 00:45:31.560]   through Cambridge Analytica is a spinoff of a British consulting company and sometime
[00:45:31.560 --> 00:45:32.560]   defense contractor.
[00:45:32.560 --> 00:45:36.200]   This is from Mackenzie Funk's op ed by the way, this is an op ed piece in the New York
[00:45:36.200 --> 00:45:37.200]   Times.
[00:45:37.200 --> 00:45:41.200]   I don't I don't know if that has different standards for fact checking Jeff or but it's
[00:45:41.200 --> 00:45:43.400]   on the opinion pages.
[00:45:43.400 --> 00:45:44.400]   A spin.
[00:45:44.400 --> 00:45:47.000]   I don't know what that means.
[00:45:47.000 --> 00:45:50.000]   The New York Times does work over these things with authors and I don't think they had published
[00:45:50.000 --> 00:45:51.240]   something they knew was false.
[00:45:51.240 --> 00:45:52.240]   Okay.
[00:45:52.240 --> 00:45:53.240]   No.
[00:45:53.240 --> 00:45:54.320]   So sometimes defense contractors are known for get this.
[00:45:54.320 --> 00:45:58.520]   It's conerterism, Syops work in Afghanistan.
[00:45:58.520 --> 00:46:03.840]   They claimed that they have psychological profiles of 230 million adult Americans to
[00:46:03.840 --> 00:46:06.240]   130 million.
[00:46:06.240 --> 00:46:10.080]   I should point out only about 130 million voted.
[00:46:10.080 --> 00:46:11.080]   So they have more.
[00:46:11.080 --> 00:46:13.120]   There's only about 310 million.
[00:46:13.120 --> 00:46:18.000]   Yeah, they have they have more profiles than people voted.
[00:46:18.000 --> 00:46:21.960]   They worked for the Brexit campaign, worked for a cruise in the primaries, Trump hired
[00:46:21.960 --> 00:46:27.680]   them for the general and Steve Bannon is a board member.
[00:46:27.680 --> 00:46:34.400]   So they have they claim as many as three to 5000 data points on each of us voting histories,
[00:46:34.400 --> 00:46:38.920]   demographics, age, income debt, hobbies, criminal histories, purchase histories, religious
[00:46:38.920 --> 00:46:41.800]   leanings, health concerns, gun ownership.
[00:46:41.800 --> 00:46:42.800]   So here's a question Leo.
[00:46:42.800 --> 00:46:44.000]   Here's a way to look at it.
[00:46:44.000 --> 00:46:48.440]   If you have a right to get to your credit history because it can have an impact on your
[00:46:48.440 --> 00:46:52.720]   life, should you have a right to get to this data?
[00:46:52.720 --> 00:46:56.240]   Should it be transferred your data about you be transparent as an ethical standard or
[00:46:56.240 --> 00:46:59.880]   as a law?
[00:46:59.880 --> 00:47:01.600]   And you know how people use it once done with it.
[00:47:01.600 --> 00:47:04.120]   Do you have the opportunity to correct it?
[00:47:04.120 --> 00:47:08.520]   This precedent is there when it comes to credit because there's a one on one affiliation
[00:47:08.520 --> 00:47:10.440]   as well, they're not going to get a mortgage.
[00:47:10.440 --> 00:47:16.040]   Well employers could use this data, political campaigns can use this data, creditors could
[00:47:16.040 --> 00:47:17.600]   use this data to look at your creditworthiness.
[00:47:17.600 --> 00:47:22.040]   There are companies out there that do that right now.
[00:47:22.040 --> 00:47:25.880]   Once should there be laws demanding transparency of this stuff?
[00:47:25.880 --> 00:47:30.800]   It was also used according to this piece to suppress voting.
[00:47:30.800 --> 00:47:35.480]   So not only can it be used to influence your vote, but according to Bloomberg, the Trump
[00:47:35.480 --> 00:47:40.240]   campaign sent out as reminding certain selected black voters of Hillary Clinton's infamous
[00:47:40.240 --> 00:47:42.360]   super predator line.
[00:47:42.360 --> 00:47:45.840]   It targeted Miami's little Haiti neighborhood with messages about the Clinton Foundation's
[00:47:45.840 --> 00:47:49.320]   trouble in Haiti after the earthquake.
[00:47:49.320 --> 00:47:52.920]   So it can be used for voter suppression as well.
[00:47:52.920 --> 00:47:58.000]   I mean, you get your, I mean, just like any ad you're trying to get somebody to do something.
[00:47:58.000 --> 00:47:59.960]   Sometimes it's to vote for your guy.
[00:47:59.960 --> 00:48:02.280]   Sometimes it's not to vote at all.
[00:48:02.280 --> 00:48:05.080]   But what's interesting is how effective it was.
[00:48:05.080 --> 00:48:09.640]   The campaign spent much less than the Clinton campaign.
[00:48:09.640 --> 00:48:13.600]   And I would argue got a hell of a lot more for their money.
[00:48:13.600 --> 00:48:19.400]   Well, it also shows, I mean, if I were a TV network reading this, I mean, I'm already
[00:48:19.400 --> 00:48:24.240]   stressed, but now I'm even more stressed because it's bang for your buck.
[00:48:24.240 --> 00:48:27.440]   Micro targeting looks better for getting something done.
[00:48:27.440 --> 00:48:34.240]   So that's worse news for the media industry and those guys trying to scale.
[00:48:34.240 --> 00:48:41.360]   I would say, I would caution though, before we freak out too much and I agree that transparency
[00:48:41.360 --> 00:48:45.640]   would be good, but we don't actually know how accurate the state is.
[00:48:45.640 --> 00:48:50.520]   And I know that I go through and I click on things just to mess with algorithms because
[00:48:50.520 --> 00:48:52.840]   it's fun to see what happens.
[00:48:52.840 --> 00:48:56.760]   You know, it's fun to like do these little experiments and be like, Oh, hey, let's try
[00:48:56.760 --> 00:48:57.760]   this.
[00:48:57.760 --> 00:49:00.520]   Oh, now I'm getting served, you know, this type of advertisement.
[00:49:00.520 --> 00:49:07.440]   So it's possible that this panopticon has really blurry vision for lack of a better
[00:49:07.440 --> 00:49:08.440]   word.
[00:49:08.440 --> 00:49:14.200]   Yeah, I think, I think you're being optimistic.
[00:49:14.200 --> 00:49:21.160]   I think you're, I think your stray signals don't overwhelm a much larger group of signals
[00:49:21.160 --> 00:49:24.040]   that you are unwittingly sending out all the time.
[00:49:24.040 --> 00:49:29.080]   And I think if somebody is determined to get your stuff, and what is by the way unethical
[00:49:29.080 --> 00:49:33.960]   is the quiz saying, you know, getting that information.
[00:49:33.960 --> 00:49:39.680]   Well, it's putting up a quiz and saying, Hey, this is for entertainment purposes and then
[00:49:39.680 --> 00:49:41.640]   using it to create a psychographic profile.
[00:49:41.640 --> 00:49:43.160]   So so I agree.
[00:49:43.160 --> 00:49:45.560]   I think you're smart to try to game the system a little bit.
[00:49:45.560 --> 00:49:47.360]   Stacy, but you're going to have to work harder.
[00:49:47.360 --> 00:49:52.080]   Oh, I'm going to have to work so hard because I have so many connected devices that you're
[00:49:52.080 --> 00:49:53.080]   sending out.
[00:49:53.080 --> 00:50:01.360]   You're sending out so much data smog that, you know, I think you could, you could try
[00:50:01.360 --> 00:50:04.080]   to mess with it all day long and it wouldn't make that much.
[00:50:04.080 --> 00:50:05.080]   It wouldn't make that much.
[00:50:05.080 --> 00:50:08.480]   Well, in certain signals are going to, I mean, they're going to figure out which signals
[00:50:08.480 --> 00:50:09.720]   correlate more highly.
[00:50:09.720 --> 00:50:13.800]   And if you think about like your, your camera, you know, when you're reading something, it
[00:50:13.800 --> 00:50:17.280]   can tell if you're getting emotional about it by looking at, you know, the flush in your
[00:50:17.280 --> 00:50:18.280]   face.
[00:50:18.280 --> 00:50:19.280]   Right.
[00:50:19.280 --> 00:50:22.400]   And all of that, then you're like, Holy cow, even if I click on something completely
[00:50:22.400 --> 00:50:25.000]   unrelated, you'll know.
[00:50:25.000 --> 00:50:28.640]   One defense against this is knowing it's happening so that when you start seeing ads
[00:50:28.640 --> 00:50:32.240]   of a certain kind, for instance, in this Times article, the author posits.
[00:50:32.240 --> 00:50:39.080]   Well, you know, if you're, if they know you're a gun owner who is paranoid and concerned
[00:50:39.080 --> 00:50:42.360]   about the world, they're going to send you a negative ad where, you know, I mean, they're
[00:50:42.360 --> 00:50:48.080]   going to really target you based on emotional analysis as much as factual analysis.
[00:50:48.080 --> 00:50:52.920]   That's why, and by the way, Facebook makes a lot of money on this.
[00:50:52.920 --> 00:50:55.200]   This is why Facebook had such a good quarter.
[00:50:55.200 --> 00:51:01.320]   Uh, you know, why mobile's working so well for them is because all this time, that's really
[00:51:01.320 --> 00:51:02.320]   what they've been doing.
[00:51:02.320 --> 00:51:04.280]   In fact, I don't see it changing.
[00:51:04.280 --> 00:51:08.880]   They've been gathering information about you to be using this extraordinarily targeted
[00:51:08.880 --> 00:51:09.880]   way.
[00:51:09.880 --> 00:51:13.360]   And the reason they call them dark posts and the reason most of us probably aren't even
[00:51:13.360 --> 00:51:19.320]   aware of this is because it's only seen by those highly targeted individuals.
[00:51:19.320 --> 00:51:21.280]   The rest of us will never see it.
[00:51:21.280 --> 00:51:26.200]   So I didn't see a lot of fake news in my Facebook feed.
[00:51:26.200 --> 00:51:31.680]   And I certainly didn't see any of these posts, uh, probably because Cambridge Analytica knew
[00:51:31.680 --> 00:51:38.120]   full well, there was no point in wasting money on me.
[00:51:38.120 --> 00:51:41.840]   I think this is a, it's a brave new world.
[00:51:41.840 --> 00:51:44.400]   And it doesn't just have to do with presidential elections obvious.
[00:51:44.400 --> 00:51:50.880]   So, no, and I, I think as we go ahead and say, no, I was going to, I was going to say, Hey,
[00:51:50.880 --> 00:51:54.360]   Jeff, your idea about seeing this data and having access to it.
[00:51:54.360 --> 00:52:01.640]   But every company is trying to gather these sorts of profiles on people.
[00:52:01.640 --> 00:52:06.840]   And they might be using slightly different signals, but I don't, in the government has
[00:52:06.840 --> 00:52:10.040]   shown no interest so far in regulating or even looking at this.
[00:52:10.040 --> 00:52:14.600]   Well, if you're, and if you got elected president using it, I can pretty much guarantee you
[00:52:14.600 --> 00:52:16.400]   you're not going to mess with it.
[00:52:16.400 --> 00:52:18.360]   That's a good system.
[00:52:18.360 --> 00:52:21.240]   By the way, on Monday, we already did the interview, but we're going to air it on Monday
[00:52:21.240 --> 00:52:22.560]   on triangulation.
[00:52:22.560 --> 00:52:26.200]   We interviewed a mathematician, uh, Kathy O'Neill who wrote a book called weapons of
[00:52:26.200 --> 00:52:27.200]   math destruction.
[00:52:27.200 --> 00:52:28.200]   How big?
[00:52:28.200 --> 00:52:30.480]   It was going to be my pick of the day.
[00:52:30.480 --> 00:52:31.480]   Really?
[00:52:31.480 --> 00:52:32.480]   Yeah.
[00:52:32.480 --> 00:52:33.480]   Good.
[00:52:33.480 --> 00:52:36.280]   Well, I'll let you talk about it that, uh, if you, after you do your pick, if people
[00:52:36.280 --> 00:52:40.600]   want to watch that interview, that'll be on Monday's triangulation, she was great.
[00:52:40.600 --> 00:52:41.600]   Do you know her?
[00:52:41.600 --> 00:52:45.520]   I don't know her, but I read about the book from, it popped up in my Twitter feed at some
[00:52:45.520 --> 00:52:46.520]   point.
[00:52:46.520 --> 00:52:50.240]   I was like, that sounds amazing for her quant person who was like, she was at D E Shaw and
[00:52:50.240 --> 00:52:55.680]   she got, she, she got her eyes opened and joined the occupy Wall Street movement, like
[00:52:55.680 --> 00:53:01.640]   went completely opposite, uh, because she's a mathematician and she understands how math
[00:53:01.640 --> 00:53:09.720]   is being used, uh, in some ways that are less than highly detrimental to the people.
[00:53:09.720 --> 00:53:10.720]   Yes.
[00:53:10.720 --> 00:53:11.720]   Salubrious.
[00:53:11.720 --> 00:53:12.960]   Let's take a break.
[00:53:12.960 --> 00:53:14.400]   Come back with more of this.
[00:53:14.400 --> 00:53:15.800]   Good, good discussion.
[00:53:15.800 --> 00:53:18.600]   Find some, let's find some, uh, what did you want to talk about?
[00:53:18.600 --> 00:53:19.600]   Stacy, you had some stuff.
[00:53:19.600 --> 00:53:20.600]   Was it Facebook?
[00:53:20.600 --> 00:53:25.200]   Well, I was going to talk about Intel, but I also want to talk about Google's, uh, AI
[00:53:25.200 --> 00:53:26.840]   paper because that's actually really interesting.
[00:53:26.840 --> 00:53:27.840]   Lip reading kids.
[00:53:27.840 --> 00:53:28.840]   Really interesting.
[00:53:28.840 --> 00:53:29.840]   Really, really interesting.
[00:53:29.840 --> 00:53:34.600]   But first, let's talk to the geeks among us.
[00:53:34.600 --> 00:53:35.600]   That's all of you.
[00:53:35.600 --> 00:53:38.440]   I presume about rocket mortgage.
[00:53:38.440 --> 00:53:41.880]   There are people in your family, people you may even be having Thanksgiving with tomorrow
[00:53:41.880 --> 00:53:45.240]   who like the way things used to be.
[00:53:45.240 --> 00:53:50.120]   And they want to go through their papers and bring a stack of papers in the folder or maybe
[00:53:50.120 --> 00:53:55.080]   even like one of those big bankers boxes to the mortgage loan officer and he'll go through
[00:53:55.080 --> 00:53:59.100]   it all and he'll look at the, he'll have all the papers and he goes, well, let me find
[00:53:59.100 --> 00:54:00.980]   a loan that's right for you.
[00:54:00.980 --> 00:54:06.620]   He'll lick his fingers and page through the, and eventually how long did it take us to
[00:54:06.620 --> 00:54:08.620]   get our mortgage a month?
[00:54:08.620 --> 00:54:09.620]   Right?
[00:54:09.620 --> 00:54:12.660]   Last time we, for a, forever.
[00:54:12.660 --> 00:54:13.780]   Last time it was ridiculous.
[00:54:13.780 --> 00:54:15.540]   They kept saying more and more and more.
[00:54:15.540 --> 00:54:20.740]   Man, that was a few years ago when Lisa and I bought our, our house, but nowadays, man,
[00:54:20.740 --> 00:54:22.140]   we'd be using rocket mortgage.
[00:54:22.140 --> 00:54:25.980]   Rocket mortgage does the whole thing completely online, completely.
[00:54:25.980 --> 00:54:29.820]   You even can submit your pay stubs and your bank statements and everything.
[00:54:29.820 --> 00:54:31.380]   They have a very simple questionnaire.
[00:54:31.380 --> 00:54:33.060]   It's kind of fun.
[00:54:33.060 --> 00:54:35.100]   It's like a Facebook quiz almost.
[00:54:35.100 --> 00:54:40.940]   You go through it and, and by the time you're done, they will have a loan for you that fits
[00:54:40.940 --> 00:54:42.660]   your exact financial situation.
[00:54:42.660 --> 00:54:45.860]   I mean, we're talking minutes here and you don't even have to get up from the couch.
[00:54:45.860 --> 00:54:47.120]   You could do it on your phone.
[00:54:47.120 --> 00:54:48.120]   You can do it on your tablet.
[00:54:48.120 --> 00:54:50.940]   That's why quick and loans calls it rocket mortgage quick and loans.
[00:54:50.940 --> 00:54:55.820]   Of course, the best mortgage lender in the country and they have put together a product
[00:54:55.820 --> 00:54:58.660]   that is just, just right for us clicks.
[00:54:58.660 --> 00:55:05.260]   Go to quickandloans.com/twig right now to find out about rocket mortgage.
[00:55:05.260 --> 00:55:13.260]   Join the 21st century with a completely online process that's fast, easy, gets the job done,
[00:55:13.260 --> 00:55:14.620]   all from the convenience of your couch.
[00:55:14.620 --> 00:55:18.580]   Actually, they have a video on there of a, I love this of a couple at an open house.
[00:55:18.580 --> 00:55:19.580]   You know, sometimes you do that, right?
[00:55:19.580 --> 00:55:21.060]   You go looking around, you're going to buy a house.
[00:55:21.060 --> 00:55:24.140]   You just want to see what the neighbor's a, you know, house looks like, whatever.
[00:55:24.140 --> 00:55:29.260]   And suddenly you get this house and you go, Oh, gosh, we should, we should buy this house.
[00:55:29.260 --> 00:55:36.540]   So this couple is there at the open house and they, and they go to quickandloans.com/ well,
[00:55:36.540 --> 00:55:39.740]   I twig, of course, and do the rocket mortgage.
[00:55:39.740 --> 00:55:42.900]   And before they leave the open house, they say we're approved.
[00:55:42.900 --> 00:55:50.580]   We'll take it quickandloans.com/twig equal housing lender licensed in all 50 states and
[00:55:50.580 --> 00:55:57.220]   MLS consumer access.org number 30, 30 rocket mortgage, the 21st century's answer to the
[00:55:57.220 --> 00:56:00.380]   homeowner or the refi.
[00:56:00.380 --> 00:56:01.380]   We're talking about Google.
[00:56:01.380 --> 00:56:02.460]   We're talking about journalism.
[00:56:02.460 --> 00:56:04.060]   We're talking about social media.
[00:56:04.060 --> 00:56:05.380]   We're talking about a changing world.
[00:56:05.380 --> 00:56:07.780]   That's really what this show is ultimately all about.
[00:56:07.780 --> 00:56:14.780]   The world changing around us and how big data technology, the internet are changing everything.
[00:56:14.780 --> 00:56:16.540]   And boy, we got two of the best people to do that.
[00:56:16.540 --> 00:56:18.380]   Stacey Higginbotham is here.
[00:56:18.380 --> 00:56:22.740]   She's the expert on internet of things really has made that her beach.
[00:56:22.740 --> 00:56:27.300]   She's got Stacey on IOT.com and the IOT podcast.com.
[00:56:27.300 --> 00:56:32.380]   Also with us Jeff Jarvis, who's been a media watcher forever.
[00:56:32.380 --> 00:56:36.500]   And as a professor of journalism at CUNY, the city university of New York, author of
[00:56:36.500 --> 00:56:39.260]   many books, including what would Google do?
[00:56:39.260 --> 00:56:43.700]   Public parts, Gutenberg, the geek, geek sparing gifts about reinventing the news industry
[00:56:43.700 --> 00:56:48.460]   in his blog is at buzzmachine.com.
[00:56:48.460 --> 00:56:51.740]   So weren't you out here for an Intel thing, Stacey?
[00:56:51.740 --> 00:56:53.780]   Oh, I was.
[00:56:53.780 --> 00:56:56.700]   I was out here for well, IDF a while back.
[00:56:56.700 --> 00:56:57.700]   The developers form.
[00:56:57.700 --> 00:56:58.700]   That's what it was.
[00:56:58.700 --> 00:56:59.700]   Yeah.
[00:56:59.700 --> 00:57:00.940]   So what is Intel up to?
[00:57:00.940 --> 00:57:02.300]   This isn't stuff you learned at IDF.
[00:57:02.300 --> 00:57:03.300]   This is more recent.
[00:57:03.300 --> 00:57:06.100]   Well, oh, well, who knows what Intel's up to?
[00:57:06.100 --> 00:57:09.100]   There are two stories here.
[00:57:09.100 --> 00:57:12.900]   One was they did the roadmap for AI chips.
[00:57:12.900 --> 00:57:13.900]   Which was really important.
[00:57:13.900 --> 00:57:19.100]   And we've talked about a lot of that before, like when they bought Nirvana systems.
[00:57:19.100 --> 00:57:24.580]   I have to say Intel is right now in a kind of a desperate situation where they were the
[00:57:24.580 --> 00:57:26.260]   chip maker to the world.
[00:57:26.260 --> 00:57:30.340]   The PC revolution happened thanks to Intel.
[00:57:30.340 --> 00:57:35.540]   And now we're in a rapidly moving to a post-PC era where Intel chips have no role.
[00:57:35.540 --> 00:57:41.940]   It's all arm chips made by companies like Qualcomm and Apple and Samsung.
[00:57:41.940 --> 00:57:45.860]   And so they need to reinvent themselves.
[00:57:45.860 --> 00:57:46.860]   They do.
[00:57:46.860 --> 00:57:52.660]   And they're also stuck as things, you know, AI is happening.
[00:57:52.660 --> 00:57:56.500]   They've got to figure out how to make their chip work for that.
[00:57:56.500 --> 00:58:01.500]   Basically, in a nutshell, Intel sold itself as a general purpose computing powerhouse.
[00:58:01.500 --> 00:58:09.540]   But because of battery power on one side and a massive scale in putting like single monolithic
[00:58:09.540 --> 00:58:14.820]   apps in the cloud at massive scale, special purpose computing is actually the win right
[00:58:14.820 --> 00:58:15.820]   now.
[00:58:15.820 --> 00:58:20.660]   So they're like a general purpose computing vendor in a special purpose computing.
[00:58:20.660 --> 00:58:22.900]   And that includes IoT, of course.
[00:58:22.900 --> 00:58:23.900]   It does.
[00:58:23.900 --> 00:58:27.980]   I mean, it includes pretty much everything going forward.
[00:58:27.980 --> 00:58:32.460]   So I mean, you look at like Google and its TensorFlow processors as an example of this
[00:58:32.460 --> 00:58:38.500]   Facebook building a lot of their AI stuff on GPUs.
[00:58:38.500 --> 00:58:41.940]   So Intel-- so that's the big macro thing.
[00:58:41.940 --> 00:58:45.740]   So the next big hurdle for Intel after it flowed mobile, it was like, holy cow, we've
[00:58:45.740 --> 00:58:47.700]   got to do something about AI.
[00:58:47.700 --> 00:58:51.900]   And what they've done is actually they've embraced heterogeneous compute.
[00:58:51.900 --> 00:58:58.460]   So they've got-- they bought Nirvana systems, which actually is building out a cloud in a
[00:58:58.460 --> 00:59:01.420]   special processor that looks more like a graphics processor.
[00:59:01.420 --> 00:59:04.900]   Their original stuff is based on NVIDIA's GPUs actually.
[00:59:04.900 --> 00:59:08.060]   And they just modded the firmware.
[00:59:08.060 --> 00:59:10.540]   So that's part of it.
[00:59:10.540 --> 00:59:15.540]   And then they bought a company called Movidius for their computer vision at the edge and
[00:59:15.540 --> 00:59:17.700]   their AI kind of stuff at the edge.
[00:59:17.700 --> 00:59:21.540]   So they talked about that in a special-- they released some software and they kind of gave
[00:59:21.540 --> 00:59:24.260]   their roadmap for AI and it's built all on that.
[00:59:24.260 --> 00:59:26.940]   So that was the one thing that Intel was doing.
[00:59:26.940 --> 00:59:31.580]   And then the other thing that-- I don't know if this is fake news or what, but there
[00:59:31.580 --> 00:59:34.940]   was a story in TechCrunch that Intel was laying off its wearables division.
[00:59:34.940 --> 00:59:35.940]   Yeah, I saw that.
[00:59:35.940 --> 00:59:38.180]   I don't think that's fake news.
[00:59:38.180 --> 00:59:40.340]   Well, yeah, I was like, is it real?
[00:59:40.340 --> 00:59:43.820]   Intel doesn't-- they're like denying it, but they're not really addressing it.
[00:59:43.820 --> 00:59:44.820]   So--
[00:59:44.820 --> 00:59:48.580]   So it wouldn't be fake news, but TechCrunch might have got it wrong.
[00:59:48.580 --> 00:59:49.580]   Right.
[00:59:49.580 --> 00:59:53.060]   Sorry, fake news is not-- this is not large enough to be used.
[00:59:53.060 --> 00:59:57.780]   You read that article about the two guys in Newport Beach or wherever in Southern California
[00:59:57.780 --> 01:00:02.180]   who used to work in a Mexican restaurant, but they couldn't make any money doing that.
[01:00:02.180 --> 01:00:04.900]   So they created one of the most popular fake news sites on the net.
[01:00:04.900 --> 01:00:05.900]   And they're not political.
[01:00:05.900 --> 01:00:07.300]   They just want to sell ads.
[01:00:07.300 --> 01:00:09.620]   They're just making good money.
[01:00:09.620 --> 01:00:12.100]   They didn't even want to-- in the New York Times article, they didn't talk about how
[01:00:12.100 --> 01:00:15.300]   much money they were making because they were afraid people would hit them up.
[01:00:15.300 --> 01:00:16.300]   It's ridiculous.
[01:00:16.300 --> 01:00:19.460]   It's good money that fake news.
[01:00:19.460 --> 01:00:22.180]   I don't think TechCrunch is in that business, I hope.
[01:00:22.180 --> 01:00:23.660]   No, no.
[01:00:23.660 --> 01:00:29.380]   It's unclear how accurate-- I'm not 100% sure how accurate this is, but--
[01:00:29.380 --> 01:00:31.980]   Wearables is not the story, I don't think.
[01:00:31.980 --> 01:00:37.940]   I think if it's too early to know what wearable is going to win, whether it's a Snapchat,
[01:00:37.940 --> 01:00:43.580]   Goggle, a virtual reality or augmented reality headset or a ring or a ear hit earpiece or--
[01:00:43.580 --> 01:00:44.660]   that's not even important.
[01:00:44.660 --> 01:00:49.180]   What's important is the-- I think the machine learning and AI behind it, right?
[01:00:49.180 --> 01:00:51.180]   So they're smart to go through that direction.
[01:00:51.180 --> 01:00:56.780]   They are-- and again, wearables, depending on your wearable, you really do need a specialized
[01:00:56.780 --> 01:01:01.820]   process for each one because the battery life and because the form factor is so small.
[01:01:01.820 --> 01:01:04.540]   So again, specialized.
[01:01:04.540 --> 01:01:07.980]   But if you could do AI hardware, people have always said this.
[01:01:07.980 --> 01:01:11.380]   This is what Jeff Hawkins went to work on.
[01:01:11.380 --> 01:01:18.100]   He was the founder of Graffiti and created Impalm, and he took all his money and has
[01:01:18.100 --> 01:01:22.380]   sunk it into a company called Nementa, which is trying to make memory chips and eventually
[01:01:22.380 --> 01:01:27.260]   processor chips that work like the human brain does, not like a von Neumann device does, but
[01:01:27.260 --> 01:01:31.580]   has a massively parallel brain-like processor.
[01:01:31.580 --> 01:01:33.300]   And I think that that probably makes sense.
[01:01:33.300 --> 01:01:37.940]   Problem is silicon is so fixed and immutable.
[01:01:37.940 --> 01:01:42.460]   Well, that's why Intel bought into FPGAs with a program.
[01:01:42.460 --> 01:01:43.460]   Right?
[01:01:43.460 --> 01:01:44.460]   You could reprogram them, right?
[01:01:44.460 --> 01:01:45.460]   Yeah.
[01:01:45.460 --> 01:01:49.740]   And eventually, I guess they could program themselves.
[01:01:49.740 --> 01:01:50.740]   That feels some next level stuff for care.
[01:01:50.740 --> 01:01:51.900]   Isn't that what you want, though?
[01:01:51.900 --> 01:01:57.380]   You would want a machine that gets smarter and as it gets smarter, can improve its hardware
[01:01:57.380 --> 01:01:58.380]   continuously.
[01:01:58.380 --> 01:02:01.220]   That's what you'd want.
[01:02:01.220 --> 01:02:02.660]   Or is that too sci-fi?
[01:02:02.660 --> 01:02:09.060]   I feel like I'm picturing the Terminator like repositioning and I'm like, "Oh my God,
[01:02:09.060 --> 01:02:10.620]   the scary is all get out."
[01:02:10.620 --> 01:02:12.380]   Well, it's not a brand new idea.
[01:02:12.380 --> 01:02:19.300]   The original artificial intelligence language created by John McCarthy of LISP was designed
[01:02:19.300 --> 01:02:21.620]   to be a language for building artificial intelligence.
[01:02:21.620 --> 01:02:26.940]   And one of its chief abilities, it's a kind of a meta-language.
[01:02:26.940 --> 01:02:33.620]   You can create new languages with LISP and you can use LISP to write new, in effect, new
[01:02:33.620 --> 01:02:34.620]   languages.
[01:02:34.620 --> 01:02:37.020]   And it's about linguistics itself, right?
[01:02:37.020 --> 01:02:38.700]   Yeah, but yeah, absolutely.
[01:02:38.700 --> 01:02:40.140]   The language was a linguist.
[01:02:40.140 --> 01:02:46.940]   But I think the idea is in software, in fact, they even tried to make LISP machines.
[01:02:46.940 --> 01:02:49.020]   They made LISP machines for a while.
[01:02:49.020 --> 01:02:53.300]   Makes sense to have that same kind of capability in hardware.
[01:02:53.300 --> 01:03:00.540]   So what Microsoft has built with their FPGAs and for Azure Cloud is not quite, it doesn't
[01:03:00.540 --> 01:03:07.540]   program itself, but you can actually, they've created basically a networking stack or it's
[01:03:07.540 --> 01:03:12.380]   virtualized networking basically that will run any protocol you want.
[01:03:12.380 --> 01:03:16.900]   So you just basically tell it and it reconfigures itself as needed, which is really powerful.
[01:03:16.900 --> 01:03:22.020]   And so it's not crazy to think.
[01:03:22.020 --> 01:03:26.300]   I guess it's not crazy to think that you could redesign your silicon as needed in an
[01:03:26.300 --> 01:03:27.620]   FPGA, but that-
[01:03:27.620 --> 01:03:30.140]   Field programmable-
[01:03:30.140 --> 01:03:31.140]   Gate array.
[01:03:31.140 --> 01:03:32.140]   Gate array.
[01:03:32.140 --> 01:03:35.820]   Field programmable implies to me that it's something that could be reprogrammed.
[01:03:35.820 --> 01:03:37.540]   It is reprogrammed.
[01:03:37.540 --> 01:03:38.540]   So you can reprogram.
[01:03:38.540 --> 01:03:41.500]   I'm just thinking it's like a, you know, it's a chip design effort.
[01:03:41.500 --> 01:03:42.500]   So it's not-
[01:03:42.500 --> 01:03:44.220]   It's a soft chip.
[01:03:44.220 --> 01:03:45.220]   It's a soft chip.
[01:03:45.220 --> 01:03:53.180]   But it's still- It's a pretty esoteric set of skills, which might make it great for computers.
[01:03:53.180 --> 01:03:59.740]   I'm not enough of a computer programmer or not a computer chip designer to know how
[01:03:59.740 --> 01:04:01.900]   this works at that level.
[01:04:01.900 --> 01:04:05.180]   But if Intel says, well, we still want to be in the hardware business.
[01:04:05.180 --> 01:04:11.660]   We still want to, you know, I mean, Intel's big asset right now is all those chip fabs,
[01:04:11.660 --> 01:04:17.220]   those buildings, you know, billion dollar to $10 billion factories designed to make chips,
[01:04:17.220 --> 01:04:19.140]   which is something you just can't ever read.
[01:04:19.140 --> 01:04:20.140]   Right.
[01:04:20.140 --> 01:04:21.780]   They're already selling.
[01:04:21.780 --> 01:04:27.340]   I mean, they have a foundry deal with a couple companies and they will make arm chips now.
[01:04:27.340 --> 01:04:29.340]   So they have that.
[01:04:29.340 --> 01:04:35.700]   I think they're- They also have a huge, I mean, they have huge software assets in code base
[01:04:35.700 --> 01:04:38.380]   for, you know, all of their x86.
[01:04:38.380 --> 01:04:41.060]   And the x86 license has nothing to scoff at.
[01:04:41.060 --> 01:04:42.060]   They own that.
[01:04:42.060 --> 01:04:44.980]   Yeah, but it's kind of like IBM owning its patents.
[01:04:44.980 --> 01:04:46.340]   It's like, that's fine.
[01:04:46.340 --> 01:04:52.660]   It's kind of an annuity, but it doesn't build a long-term future of growth.
[01:04:52.660 --> 01:04:59.100]   No, but it's enough for- It's enough to give them a runway to play with and figure this
[01:04:59.100 --> 01:05:00.100]   out.
[01:05:00.100 --> 01:05:02.860]   And they're really, like for a while there, I was really worried that they weren't going
[01:05:02.860 --> 01:05:04.620]   to see this and accept it.
[01:05:04.620 --> 01:05:09.180]   At this point, somebody in the chatroom said Intel's turning into Yahoo.
[01:05:09.180 --> 01:05:11.020]   And I can't say I disagree with them.
[01:05:11.020 --> 01:05:13.380]   That's- But that's what happens.
[01:05:13.380 --> 01:05:14.380]   That's what happens.
[01:05:14.380 --> 01:05:15.380]   Come on.
[01:05:15.380 --> 01:05:17.620]   This is where- It's the business cycle.
[01:05:17.620 --> 01:05:21.860]   You know, nothing can, especially in technology with the innovators dilemma and all that.
[01:05:21.860 --> 01:05:24.100]   It's very difficult for a company.
[01:05:24.100 --> 01:05:26.300]   The innovators dilemma was originally an Intel thing.
[01:05:26.300 --> 01:05:30.940]   I mean, if you remember, Intel used to make their own chip manufacturing machines and
[01:05:30.940 --> 01:05:33.660]   then they were like, oh, that is a loser business for us.
[01:05:33.660 --> 01:05:34.660]   We got it.
[01:05:34.660 --> 01:05:36.260]   Like, they have adapted.
[01:05:36.260 --> 01:05:44.220]   I refuse to bet against Intel just yet because I do think Brian Krasinich has recognized
[01:05:44.220 --> 01:05:49.420]   where they are and is seeking ways to fix that.
[01:05:49.420 --> 01:05:50.420]   Of course.
[01:05:50.420 --> 01:05:58.060]   And, you know, look, anybody who's smart knows and sees things changing.
[01:05:58.060 --> 01:06:02.020]   It's a little harder right now, at least it is for me to think of where the future lies.
[01:06:02.020 --> 01:06:07.300]   For many, for decades, we kind of knew wealth computers are going to be smaller.
[01:06:07.300 --> 01:06:10.540]   You know, we're going to have the- They're going to be always on, always connected.
[01:06:10.540 --> 01:06:11.540]   They're going to become portable ultimately.
[01:06:11.540 --> 01:06:13.860]   I mean, we kind of knew where we were headed.
[01:06:13.860 --> 01:06:20.100]   I think that there is a pretty big question mark at this point about- I mean, we know-
[01:06:20.100 --> 01:06:25.500]   We know machine learning and artificial intelligence will be applied as a thin veneer to existing
[01:06:25.500 --> 01:06:27.260]   technology.
[01:06:27.260 --> 01:06:31.860]   So I guess a new company could look at- Or an old company could look at a growth path that
[01:06:31.860 --> 01:06:33.460]   involves that.
[01:06:33.460 --> 01:06:39.420]   But it's often easier to do that for a new company than it is for an older company.
[01:06:39.420 --> 01:06:40.420]   It is.
[01:06:40.420 --> 01:06:43.620]   Well, an Intel's challenge is also that the ASPs on its chips, so the average selling
[01:06:43.620 --> 01:06:47.300]   price of its chips is incredibly high.
[01:06:47.300 --> 01:06:54.180]   And when you talk about chips for things that are selling for $50 or $200, it's-
[01:06:54.180 --> 01:06:55.180]   Right.
[01:06:55.180 --> 01:06:56.660]   Look at the Raspberry Pi at $35 computer.
[01:06:56.660 --> 01:06:57.660]   Was it evident?
[01:06:57.660 --> 01:06:58.660]   An arm chip.
[01:06:58.660 --> 01:06:59.660]   Has a Broadcom chip.
[01:06:59.660 --> 01:07:00.660]   Yep.
[01:07:00.660 --> 01:07:01.660]   Based on arm?
[01:07:01.660 --> 01:07:07.140]   Talking just a minute to go on Windows Weekly about Microsoft moving to arm and Windows on
[01:07:07.140 --> 01:07:08.140]   all of that.
[01:07:08.140 --> 01:07:11.180]   Yeah, VMware goes there, then all is lost.
[01:07:11.180 --> 01:07:12.460]   There's a lot- Yeah, right.
[01:07:12.460 --> 01:07:14.260]   There's a virtualization arm.
[01:07:14.260 --> 01:07:19.060]   And there's a lot of conversation about Apple abandoning Intel.
[01:07:19.060 --> 01:07:24.620]   And I think this is obviously Intel sees this coming and turning to arm.
[01:07:24.620 --> 01:07:29.460]   And eventually it is ultimately the death of the PC platform that we're looking at.
[01:07:29.460 --> 01:07:32.420]   Yeah, well, it's- Again.
[01:07:32.420 --> 01:07:34.060]   End of general purpose computing.
[01:07:34.060 --> 01:07:35.060]   Right.
[01:07:35.060 --> 01:07:38.960]   So, sorry, I'm just going to hammer that point home because I feel like a lot of people
[01:07:38.960 --> 01:07:42.220]   talk about it without seeing the big picture.
[01:07:42.220 --> 01:07:43.220]   Right.
[01:07:43.220 --> 01:07:44.980]   Well, you know why, and I'll tell you why.
[01:07:44.980 --> 01:07:48.620]   It's kind of the innovators dilemma for journalists.
[01:07:48.620 --> 01:07:54.020]   I grew up with general purpose desktop computers, and I kind of like them.
[01:07:54.020 --> 01:07:56.580]   And a lot- Or creating content.
[01:07:56.580 --> 01:08:01.900]   Yeah, well, a lot of us, well, no, but I think we're wrong, but a lot of us kind of go.
[01:08:01.900 --> 01:08:03.060]   They can't get rid of the Mac.
[01:08:03.060 --> 01:08:04.540]   They can't get rid of the PC.
[01:08:04.540 --> 01:08:06.820]   We got a- Who's going to write the software?
[01:08:06.820 --> 01:08:09.700]   Who's going to design this- This is the resistance to the Chromebook.
[01:08:09.700 --> 01:08:10.700]   Right.
[01:08:10.700 --> 01:08:11.700]   Okay, yes.
[01:08:11.700 --> 01:08:13.460]   And how can I run my own apps?
[01:08:13.460 --> 01:08:14.460]   How can I not make my own thing?
[01:08:14.460 --> 01:08:18.820]   I freely admit that time is not on our side.
[01:08:18.820 --> 01:08:21.180]   I'm just being nostalgic at this point.
[01:08:21.180 --> 01:08:22.180]   Yeah, Gramps.
[01:08:22.180 --> 01:08:23.180]   Yup.
[01:08:23.180 --> 01:08:24.180]   Gramps.
[01:08:24.180 --> 01:08:25.180]   Gramps.
[01:08:25.180 --> 01:08:26.180]   Gramps.
[01:08:26.180 --> 01:08:28.620]   It's like you're close to retirement.
[01:08:28.620 --> 01:08:29.620]   It's all good.
[01:08:29.620 --> 01:08:30.620]   Oh, shut up.
[01:08:30.620 --> 01:08:32.820]   I want to go another 30 years.
[01:08:32.820 --> 01:08:38.620]   Well, what I'm hoping is that somebody who- I think I still have a nimble enough mind
[01:08:38.620 --> 01:08:42.020]   to kind of embrace the new hotness.
[01:08:42.020 --> 01:08:49.100]   And what I do bring to the table, you young people, is enough history and background to
[01:08:49.100 --> 01:08:54.780]   be a little cynical sometimes about things like VR that everybody's jumping up and down.
[01:08:55.780 --> 01:09:03.780]   Over and say, "Well, I think that emperors not wearing any clothes."
[01:09:03.780 --> 01:09:06.380]   And by the way, IoT is inevitable.
[01:09:06.380 --> 01:09:07.820]   I mean, that's obvious.
[01:09:07.820 --> 01:09:08.820]   What's happening?
[01:09:08.820 --> 01:09:11.780]   We've been saying for decades- I've been saying for decades what you're going to see.
[01:09:11.780 --> 01:09:12.780]   I mean, this was another trend.
[01:09:12.780 --> 01:09:18.180]   It was plainly obvious is the distribution of computing power.
[01:09:18.180 --> 01:09:20.380]   That's what the personal computer did in effect.
[01:09:20.380 --> 01:09:24.180]   It took the mainframe computer and put it on your desk.
[01:09:24.180 --> 01:09:25.900]   And then it's in your pocket.
[01:09:25.900 --> 01:09:27.580]   And then soon it's going to be everywhere.
[01:09:27.580 --> 01:09:32.700]   It's going to be in your TV and your microwave and your toaster and your mouse.
[01:09:32.700 --> 01:09:38.900]   And of course, the next step to that is to have improved communication between them,
[01:09:38.900 --> 01:09:41.300]   which we're not very good at right now.
[01:09:41.300 --> 01:09:47.860]   And unfortunately, market forces argue against because everybody wants their stuff siloed.
[01:09:47.860 --> 01:09:52.500]   And that's why we've never had a good standard that everybody had adopted because everybody
[01:09:52.500 --> 01:09:54.460]   wanted to own their own thing.
[01:09:54.460 --> 01:10:00.580]   And that's keeping that from going where it should go, which is a mesh of interconnected
[01:10:00.580 --> 01:10:03.500]   self-aware, intelligent items.
[01:10:03.500 --> 01:10:06.220]   Once you get that, that's pretty powerful.
[01:10:06.220 --> 01:10:07.220]   It is.
[01:10:07.220 --> 01:10:08.860]   I'm kind of curious.
[01:10:08.860 --> 01:10:14.980]   So I feel like we're getting closer, though, actually with a usability standard for the
[01:10:14.980 --> 01:10:15.980]   Internet of Things.
[01:10:15.980 --> 01:10:16.980]   And that would be voice.
[01:10:16.980 --> 01:10:21.260]   And you see it with the evolution of the different voice platforms.
[01:10:21.260 --> 01:10:27.140]   Because I feel like at some level, all these fights over protocols can be obviated by running
[01:10:27.140 --> 01:10:32.100]   it through an Echo or a home or some other device that you talk to.
[01:10:32.100 --> 01:10:36.540]   And this kind of popped up in my head because I was in my car, which I can talk to.
[01:10:36.540 --> 01:10:38.060]   And I was like, dang it.
[01:10:38.060 --> 01:10:41.900]   I really want to talk to the Echo in my car.
[01:10:41.900 --> 01:10:46.700]   I want to ask you to know everybody wants to put a dot in their car or tap in their car.
[01:10:46.700 --> 01:10:48.220]   So I was thinking about that.
[01:10:48.220 --> 01:10:52.740]   I'm like, OK, so what we're going to end up with isn't maybe a standard that everyone
[01:10:52.740 --> 01:10:53.940]   works with.
[01:10:53.940 --> 01:10:59.180]   It's going to be some sort of hub for now that's controlled by one of the three vendors.
[01:10:59.180 --> 01:11:04.940]   And we've kind of on this bat that it's going to be the best AI and that'll be Google.
[01:11:04.940 --> 01:11:08.780]   But maybe it's the best ecosystem and that'll be Amazon.
[01:11:08.780 --> 01:11:12.620]   And I don't know what happens with Apple because they drop the ball in this whole ecosystem
[01:11:12.620 --> 01:11:13.620]   idea of things.
[01:11:13.620 --> 01:11:16.820]   Maybe it's the company that can buy the best Facebook targeted advertising.
[01:11:16.820 --> 01:11:17.820]   Who knows?
[01:11:17.820 --> 01:11:18.820]   The dark ass.
[01:11:18.820 --> 01:11:24.660]   I mean, but that's what I'm saying is that the thing that really holds us back at this
[01:11:24.660 --> 01:11:26.900]   point, it's not security.
[01:11:26.900 --> 01:11:28.420]   It's not user adoption.
[01:11:28.420 --> 01:11:30.860]   It's none of the things that have in the past held us back.
[01:11:30.860 --> 01:11:36.340]   It's the sad to say the intense desire of companies to silo everything and to keep everything
[01:11:36.340 --> 01:11:37.340]   in their own ecosystem.
[01:11:37.340 --> 01:11:41.180]   And the reason you don't have an echo in the car is because no auto manufacturer will
[01:11:41.180 --> 01:11:46.020]   allow you to do that because Ford wants you to use its technology and or Android auto.
[01:11:46.020 --> 01:11:48.620]   Well, it's the same thing with Android auto.
[01:11:48.620 --> 01:11:49.620]   Right.
[01:11:49.620 --> 01:11:52.340]   That's it's that seems to me a no brainer.
[01:11:52.340 --> 01:11:53.340]   That's what everybody wants.
[01:11:53.340 --> 01:11:55.060]   That doesn't matter what consumers want.
[01:11:55.060 --> 01:11:58.420]   The companies Mercedes is of my dead body.
[01:11:58.420 --> 01:11:59.420]   Right.
[01:11:59.420 --> 01:12:02.020]   It's not going to be an actually Mercedes is adding an Android auto.
[01:12:02.020 --> 01:12:03.820]   Yeah, but watch how they added.
[01:12:03.820 --> 01:12:04.820]   Seriously.
[01:12:04.820 --> 01:12:05.820]   Ford is adding Alexa.
[01:12:05.820 --> 01:12:06.820]   Sorry.
[01:12:06.820 --> 01:12:07.820]   They are.
[01:12:07.820 --> 01:12:08.820]   We're deciding the echo.
[01:12:08.820 --> 01:12:09.820]   Yeah.
[01:12:09.820 --> 01:12:10.820]   Are they?
[01:12:10.820 --> 01:12:11.820]   They announced it at C.S. last year.
[01:12:11.820 --> 01:12:12.820]   Italy.
[01:12:12.820 --> 01:12:13.820]   Yeah.
[01:12:13.820 --> 01:12:14.820]   Where is that?
[01:12:14.820 --> 01:12:15.820]   Last year, huh?
[01:12:15.820 --> 01:12:16.820]   OK.
[01:12:16.820 --> 01:12:17.820]   Yeah.
[01:12:17.820 --> 01:12:19.820]   Last year they announced that.
[01:12:19.820 --> 01:12:20.820]   I don't have a phone.
[01:12:20.820 --> 01:12:22.820]   They're really running along and moving along on that one.
[01:12:22.820 --> 01:12:24.380]   All right.
[01:12:24.380 --> 01:12:28.020]   I think I think people are scared of Amazon, the scared of Jeff Bezos.
[01:12:28.020 --> 01:12:31.460]   And I think a lot of companies that thought echo was a great platform now are going, yeah,
[01:12:31.460 --> 01:12:34.220]   but all we're going to do is give Jeff Bezos all our data and then he's just going to take
[01:12:34.220 --> 01:12:35.220]   over the world.
[01:12:35.220 --> 01:12:39.980]   Oh, you know, Apple's trying to do that with their home platform for the Internet of things.
[01:12:39.980 --> 01:12:42.620]   And that's and look how slowly that's evolving.
[01:12:42.620 --> 01:12:47.100]   Well, that's evolving slowly for a number of reasons.
[01:12:47.100 --> 01:12:52.340]   Not all of them are about a lot of them were technical at the first iteration.
[01:12:52.340 --> 01:12:55.780]   I would submit that ultimately the reason that it doesn't evolve is because everybody's
[01:12:55.780 --> 01:12:57.180]   scared of Apple.
[01:12:57.180 --> 01:12:59.620]   No, they don't know.
[01:12:59.620 --> 01:13:02.980]   Everybody wants to be with Apple, but Apple's setting these weird rules and it's not clear
[01:13:02.980 --> 01:13:06.740]   how everything's going to work and Apple wants things to be more clear, but it's not clear
[01:13:06.740 --> 01:13:10.180]   because there's no standards and it's like, here's what it is.
[01:13:10.180 --> 01:13:11.180]   I'll tell you right now.
[01:13:11.180 --> 01:13:14.060]   It's the same reason Apple hasn't been able to get the TV companies to go along with its
[01:13:14.060 --> 01:13:15.900]   plans for Apple TV.
[01:13:15.900 --> 01:13:18.900]   Apple wants to own the data.
[01:13:18.900 --> 01:13:23.660]   Apple's insistent on owning the data and they how much can Apple really and you know,
[01:13:23.660 --> 01:13:26.100]   Apple can Apple get a broad swath of data.
[01:13:26.100 --> 01:13:27.100]   This is what we talked before.
[01:13:27.100 --> 01:13:30.300]   Everybody has slices of data based on certain behaviors you have.
[01:13:30.300 --> 01:13:38.700]   Who has the broad swath Facebook Amazon, Facebook, Google, yeah, and Apple Apple uses as an excuse
[01:13:38.700 --> 01:13:42.340]   to that they want to on the data, they want to protect their users privacy.
[01:13:42.340 --> 01:13:45.580]   So they don't trust anybody else to have the data.
[01:13:45.580 --> 01:13:48.220]   You know, maybe that's true or maybe they just want to have the data because that's where
[01:13:48.220 --> 01:13:49.220]   all the money is.
[01:13:49.220 --> 01:13:53.020]   It's also it's also an easier customer relationship.
[01:13:53.020 --> 01:13:59.380]   So that's a big hitching point with home is the companies who are building products
[01:13:59.380 --> 01:14:01.340]   that work with home kit.
[01:14:01.340 --> 01:14:06.220]   Apple wants you as the end user to be able to use that app for everything instead of
[01:14:06.220 --> 01:14:11.060]   having to deal with all these like a door lock app garage door app, whatever doorbell
[01:14:11.060 --> 01:14:12.060]   app.
[01:14:12.060 --> 01:14:13.820]   It wants to show you everything in the home app.
[01:14:13.820 --> 01:14:15.980]   So your life is easier as a user.
[01:14:15.980 --> 01:14:20.100]   But if you're one of those partner companies, you're like, no, no, I want them in my app
[01:14:20.100 --> 01:14:22.940]   because I want to own that relationship with the customer.
[01:14:22.940 --> 01:14:26.540]   So it's not just it's not just about getting data.
[01:14:26.540 --> 01:14:32.540]   It's also because Apple does think about the user and their experience to a crazy amount.
[01:14:32.540 --> 01:14:35.220]   So here's an October article from Fortune.
[01:14:35.220 --> 01:14:41.540]   Thank you, chat room that said final testing is underway for echo integration into Ford
[01:14:41.540 --> 01:14:46.420]   motors by the end of this year, the focus electric fusing energy and C max energy.
[01:14:46.420 --> 01:14:47.740]   All of these are electric cars.
[01:14:47.740 --> 01:14:49.460]   We'll be able to communicate.
[01:14:49.460 --> 01:14:55.500]   See, this is not as interesting with smart home devices using Alexa by pressing a voice
[01:14:55.500 --> 01:14:57.860]   recognition button on the steering wheel.
[01:14:57.860 --> 01:14:59.140]   I want to talk to the car.
[01:14:59.140 --> 01:15:01.740]   I don't want to talk to my thermostat.
[01:15:01.740 --> 01:15:11.780]   So it's not so you can't say it doesn't sound like it says vehicle owners will be able to
[01:15:11.780 --> 01:15:15.540]   send simple commands via echo to smart home appliances and systems.
[01:15:15.540 --> 01:15:17.940]   It doesn't sound like I'll be able to say, and your car.
[01:15:17.940 --> 01:15:22.740]   Echo, I want to listen to the top 100 records right now.
[01:15:22.740 --> 01:15:26.140]   Oh, like streaming Spotify into your car.
[01:15:26.140 --> 01:15:27.140]   Yeah.
[01:15:27.140 --> 01:15:28.140]   I don't know.
[01:15:28.140 --> 01:15:31.700]   You know, Amazon just this month, like just this week, I think made it very easy.
[01:15:31.700 --> 01:15:36.380]   They released like an SDK for speakers.
[01:15:36.380 --> 01:15:41.460]   So if you were an automaker, you could actually put that in and create like an echo speaker.
[01:15:41.460 --> 01:15:46.060]   Well, as consumers, we would all love it if everybody would adopt echo.
[01:15:46.060 --> 01:15:49.980]   I mean, that would be a start anyway, somebody.
[01:15:49.980 --> 01:15:52.060]   But we learned that with trusting Apple.
[01:15:52.060 --> 01:15:54.420]   Remember with music and everything else.
[01:15:54.420 --> 01:15:58.940]   Like that kind of skit, the industry is skittish and then Amazon and books.
[01:15:58.940 --> 01:15:59.940]   Good Lord.
[01:15:59.940 --> 01:16:00.940]   Good Lord.
[01:16:00.940 --> 01:16:01.940]   Good Lord.
[01:16:01.940 --> 01:16:03.300]   The Kindle and publishing.
[01:16:03.300 --> 01:16:04.300]   Right.
[01:16:04.300 --> 01:16:10.060]   Well, yeah, because as so there's a direct correlation between the success of a company
[01:16:10.060 --> 01:16:13.820]   in a company's platform and its desire to take over the world.
[01:16:13.820 --> 01:16:18.740]   And of course, the soon as that becomes apparent, everybody says, well, maybe we won't work
[01:16:18.740 --> 01:16:21.100]   with you after all.
[01:16:21.100 --> 01:16:24.940]   That's why we well, how long have we been trying to connect all this stuff together forever?
[01:16:24.940 --> 01:16:27.940]   That's why we can't have nice things.
[01:16:27.940 --> 01:16:30.540]   So I'm a fan of open platforms.
[01:16:30.540 --> 01:16:32.740]   That way no one company controls it.
[01:16:32.740 --> 01:16:34.780]   Everything works with everything else.
[01:16:34.780 --> 01:16:38.180]   That doesn't seem to be the future in any of this stuff.
[01:16:38.180 --> 01:16:40.900]   Open platforms do not guarantee interoperability.
[01:16:40.900 --> 01:16:46.300]   And people are always like, oh, I'm a fan of open platforms, but I don't say like that.
[01:16:46.300 --> 01:16:47.300]   Sorry.
[01:16:47.300 --> 01:16:49.340]   Oh, I'm a fan of open platforms.
[01:16:49.340 --> 01:16:51.140]   Oh, I love open platforms.
[01:16:51.140 --> 01:16:52.980]   I do have open platforms.
[01:16:52.980 --> 01:16:53.980]   I do too.
[01:16:53.980 --> 01:16:58.900]   I think they're just the bomb, but it doesn't guarantee interoperability.
[01:16:58.900 --> 01:17:00.700]   And that's actually what most people care about.
[01:17:00.700 --> 01:17:02.580]   No, yeah, of course.
[01:17:02.580 --> 01:17:04.700]   We want one ring to rule them all.
[01:17:04.700 --> 01:17:05.700]   Right.
[01:17:05.700 --> 01:17:09.900]   And no one actually nobody wants to give Mordor all that power.
[01:17:09.900 --> 01:17:10.900]   Right.
[01:17:10.900 --> 01:17:17.540]   Macy's Thanksgiving Day Parade will be live streamed on YouTube tomorrow in 360 degree
[01:17:17.540 --> 01:17:18.540]   video.
[01:17:18.540 --> 01:17:23.660]   Get your daydreams out, your Google cardboard, your Galaxy gear VRs.
[01:17:23.660 --> 01:17:24.660]   You can.
[01:17:24.660 --> 01:17:26.580]   I love my cardboard at home.
[01:17:26.580 --> 01:17:27.580]   No.
[01:17:27.580 --> 01:17:31.860]   You could probably make one with a newspaper and some glasses.
[01:17:31.860 --> 01:17:32.860]   Paper.
[01:17:32.860 --> 01:17:33.860]   I don't know.
[01:17:33.860 --> 01:17:38.300]   Oh, you don't have any VR capability there at all.
[01:17:38.300 --> 01:17:41.660]   No, I don't think so.
[01:17:41.660 --> 01:17:42.660]   You can watch it.
[01:17:42.660 --> 01:17:43.660]   I thought I would love that.
[01:17:43.660 --> 01:17:45.740]   Well, you can watch it from your browser, but you just don't get the surround.
[01:17:45.740 --> 01:17:47.580]   You have to move it around like that.
[01:17:47.580 --> 01:17:48.580]   Yeah.
[01:17:48.580 --> 01:17:52.220]   Verizon's doing this with NBC Universal.
[01:17:52.220 --> 01:17:56.820]   Verizon is going to place cameras in five locations along the parade route.
[01:17:56.820 --> 01:18:00.940]   So you can follow your favorite balloon.
[01:18:00.940 --> 01:18:04.780]   Well, there should be a Trump balloon.
[01:18:04.780 --> 01:18:06.300]   I think that'd be interesting.
[01:18:06.300 --> 01:18:09.540]   It's our joke about hot air here.
[01:18:09.540 --> 01:18:13.260]   In the run up to the actual event, which is of course tomorrow, 9am Eastern, Macy's
[01:18:13.260 --> 01:18:18.900]   is also posting 360 degrees videos of behind the scenes studio tours.
[01:18:18.900 --> 01:18:19.900]   Wait a minute.
[01:18:19.900 --> 01:18:20.900]   Let's see.
[01:18:20.900 --> 01:18:24.300]   It's time for the parade.
[01:18:24.300 --> 01:18:26.020]   This is the best PR.
[01:18:26.020 --> 01:18:28.740]   Welcome to Macy's studio.
[01:18:28.740 --> 01:18:30.140]   Now this is in a weird.
[01:18:30.140 --> 01:18:33.260]   This is where all of the magic begins for.
[01:18:33.260 --> 01:18:34.900]   He's looking over here.
[01:18:34.900 --> 01:18:37.060]   You want to come see how it works?
[01:18:37.060 --> 01:18:38.260]   Let's go.
[01:18:38.260 --> 01:18:41.780]   All around the ceiling are models of balloons.
[01:18:41.780 --> 01:18:45.100]   Snoopy has actually had seven different designs.
[01:18:45.100 --> 01:18:50.740]   Mickey Mouse has had five different designs of balloons for the parade.
[01:18:50.740 --> 01:18:52.740]   They all start out the same way.
[01:18:52.740 --> 01:18:53.740]   We start with a snap.
[01:18:53.740 --> 01:18:54.740]   When your daughter loved this?
[01:18:54.740 --> 01:18:55.740]   That's a spider-man.
[01:18:55.740 --> 01:18:56.740]   Yes.
[01:18:56.740 --> 01:18:58.380]   What is that shark doing in the corner?
[01:18:58.380 --> 01:19:00.180]   That's left shark.
[01:19:00.180 --> 01:19:01.820]   He finally got a job.
[01:19:01.820 --> 01:19:02.820]   He's a treasure chest.
[01:19:02.820 --> 01:19:03.820]   Yeah, treasure chest.
[01:19:03.820 --> 01:19:05.780]   We've got to go to three dimensions.
[01:19:05.780 --> 01:19:07.580]   And that means doing a sculpt.
[01:19:07.580 --> 01:19:08.580]   So we sculpt it out of clay.
[01:19:08.580 --> 01:19:09.780]   There's a cloud in the back.
[01:19:09.780 --> 01:19:10.780]   There's a cloud in the back.
[01:19:10.780 --> 01:19:13.380]   So that we can look at them from top and bottom and all around and know exactly the
[01:19:13.380 --> 01:19:14.380]   point.
[01:19:14.380 --> 01:19:15.380]   I was the left shark.
[01:19:15.380 --> 01:19:17.100]   And we do a great studio video.
[01:19:17.100 --> 01:19:21.660]   So this is not, I was hoping this would be one where you could watch it in 2D and move
[01:19:21.660 --> 01:19:22.660]   around inside.
[01:19:22.660 --> 01:19:25.340]   It doesn't look like it's done that way.
[01:19:25.340 --> 01:19:28.340]   It's 4K too by the way.
[01:19:28.340 --> 01:19:29.900]   Here goes another gigabyte.
[01:19:29.900 --> 01:19:30.900]   Yeah.
[01:19:30.900 --> 01:19:33.740]   Now you know why I've gone over the limit.
[01:19:33.740 --> 01:19:37.900]   Yeah, someone said you can only watch like 88 hours of 4K TV.
[01:19:37.900 --> 01:19:42.220]   Oh, I can watch a lot more.
[01:19:42.220 --> 01:19:46.780]   I do have, I have Comcast consumer and that's the one that's gone past the limit because
[01:19:46.780 --> 01:19:49.860]   I also have Comcast business which has no limit.
[01:19:49.860 --> 01:19:55.300]   So what I'm trying now to do is move all my 4K stuff over to the Comcast business class.
[01:19:55.300 --> 01:19:57.500]   Yeah, good plan.
[01:19:57.500 --> 01:19:59.380]   Which is probably not what you're supposed to forget.
[01:19:59.380 --> 01:20:03.020]   I said that I, who me, you're in the know it is business for you.
[01:20:03.020 --> 01:20:04.020]   No, it's business.
[01:20:04.020 --> 01:20:05.020]   That's business class.
[01:20:05.020 --> 01:20:06.020]   You're fine.
[01:20:06.020 --> 01:20:07.020]   Okay.
[01:20:07.020 --> 01:20:09.820]   So that's what they always say.
[01:20:09.820 --> 01:20:12.860]   They're like, if you consume too much, you should be on a business line.
[01:20:12.860 --> 01:20:14.540]   Yeah, I pay for it.
[01:20:14.540 --> 01:20:15.540]   It's a lot of money.
[01:20:15.540 --> 01:20:16.540]   What is it?
[01:20:16.540 --> 01:20:17.540]   What is it?
[01:20:17.540 --> 01:20:18.540]   I don't know.
[01:20:18.540 --> 01:20:19.540]   It's a ton.
[01:20:19.540 --> 01:20:22.860]   I've had some between 200 and 200 months.
[01:20:22.860 --> 01:20:23.860]   That's actually related.
[01:20:23.860 --> 01:20:25.860]   I mean, for naked, it's just for internet.
[01:20:25.860 --> 01:20:27.460]   For naked internet?
[01:20:27.460 --> 01:20:29.260]   Naked broadband.
[01:20:29.260 --> 01:20:30.260]   I'm happy.
[01:20:30.260 --> 01:20:32.100]   It's 100 megabit.
[01:20:32.100 --> 01:20:33.100]   I'm happy.
[01:20:33.100 --> 01:20:34.100]   I put servers on it.
[01:20:34.100 --> 01:20:38.740]   This is the main reason I have it is I have all my servers on it.
[01:20:38.740 --> 01:20:40.340]   So yeah, I don't have Sonic at home.
[01:20:40.340 --> 01:20:41.740]   I don't have fiber to my house.
[01:20:41.740 --> 01:20:42.980]   I'm sad.
[01:20:42.980 --> 01:20:46.140]   So Google is now reading lips.
[01:20:46.140 --> 01:20:50.940]   And it's doing so better than humans do.
[01:20:50.940 --> 01:20:52.460]   Is it Google is doing the lip reading?
[01:20:52.460 --> 01:20:58.180]   This is the same thing as their multilingual neural machine translation system called zero
[01:20:58.180 --> 01:20:59.540]   shot translation.
[01:20:59.540 --> 01:21:01.500]   I don't know.
[01:21:01.500 --> 01:21:03.020]   I saw the lip reading somewhere too.
[01:21:03.020 --> 01:21:04.020]   This is deep mind.
[01:21:04.020 --> 01:21:05.020]   This is their.
[01:21:05.020 --> 01:21:06.020]   This is the neural network.
[01:21:06.020 --> 01:21:07.580]   Oh, this is the one where it's more.
[01:21:07.580 --> 01:21:09.580]   Sentency stuff, right?
[01:21:09.580 --> 01:21:13.980]   It allowed them to do pairs that they ever did before.
[01:21:13.980 --> 01:21:17.340]   And it teaches itself to some extent, right?
[01:21:17.340 --> 01:21:28.980]   So remember the scary scene in 2001 space odyssey where how 9000 reads Dave's lips?
[01:21:28.980 --> 01:21:30.460]   We're getting there.
[01:21:30.460 --> 01:21:37.140]   Google Deep Mind AI destroys human expert and lip reading competition.
[01:21:37.140 --> 01:21:38.140]   The accuracy is.
[01:21:38.140 --> 01:21:39.900]   That real competition?
[01:21:39.900 --> 01:21:40.900]   Well.
[01:21:40.900 --> 01:21:42.580]   Or just a test.
[01:21:42.580 --> 01:21:43.580]   This is just a test.
[01:21:43.580 --> 01:21:45.380]   Professional human lip reader.
[01:21:45.380 --> 01:21:48.180]   Well, I don't think there's a lip reading competition.
[01:21:48.180 --> 01:21:49.180]   That's what I was like.
[01:21:49.180 --> 01:21:50.180]   Lip reading Olympics.
[01:21:50.180 --> 01:21:51.980]   Da da da da.
[01:21:51.980 --> 01:21:53.980]   Mr. Percussion totally got it.
[01:21:53.980 --> 01:21:54.980]   The thrill of victory.
[01:21:54.980 --> 01:21:56.460]   The agony of the lips.
[01:21:56.460 --> 01:22:00.100]   The professional human lip reader whom the researchers compare the results against had
[01:22:00.100 --> 01:22:03.940]   roughly 10 years of experience in the field and a deciphered videos for the royal wedding
[01:22:03.940 --> 01:22:06.140]   and in court for trials as well.
[01:22:06.140 --> 01:22:10.860]   The reader was given a sample of 200 videos for the set used to train W.L.A.S. and the
[01:22:10.860 --> 01:22:15.540]   videos played for 10 times longer than what the in other words they gave the human a
[01:22:15.540 --> 01:22:20.140]   huge advantage 10 times longer than for the AI system.
[01:22:20.140 --> 01:22:26.340]   The human was able to decipher less than one quarter of the words in the sample.
[01:22:26.340 --> 01:22:34.020]   The Google mind could do half for the human expert decipher 12.
[01:22:34.020 --> 01:22:39.660]   This is a report from new scientist human expert decipher 12.4% of their words.
[01:22:39.660 --> 01:22:43.100]   The AI system got 46.8%.
[01:22:43.100 --> 01:22:45.740]   Now this got the words but it didn't get the understanding.
[01:22:45.740 --> 01:22:46.740]   No, of course not.
[01:22:46.740 --> 01:22:47.740]   You have to run that through a new neural network.
[01:22:47.740 --> 01:22:51.340]   I'm just telling people who are freaking out.
[01:22:51.340 --> 01:22:52.340]   Right.
[01:22:52.340 --> 01:22:53.340]   Right.
[01:22:53.340 --> 01:22:56.140]   So how could know what you say but not what it meant?
[01:22:56.140 --> 01:22:57.140]   Right.
[01:22:57.140 --> 01:23:00.860]   So actually that's really interesting because you could do translations like you could actually
[01:23:00.860 --> 01:23:05.620]   deliver words in other languages and then feed those words to somebody who's a good
[01:23:05.620 --> 01:23:06.620]   translate.
[01:23:06.620 --> 01:23:09.180]   Look, this is the this is an example of the video.
[01:23:09.180 --> 01:23:11.180]   Can you understand what she's saying?
[01:23:11.180 --> 01:23:13.580]   No, I can't read this.
[01:23:13.580 --> 01:23:15.060]   You can do the bad lip dub with this.
[01:23:15.060 --> 01:23:16.060]   I love that.
[01:23:16.060 --> 01:23:19.100]   And here's the clip with subtitles provided by the AI.
[01:23:19.100 --> 01:23:21.940]   We have to look at whether it works for the UK or not.
[01:23:21.940 --> 01:23:27.460]   We have to look at whether she's got an accent.
[01:23:27.460 --> 01:23:29.540]   We have to do this for the bad.
[01:23:29.540 --> 01:23:30.540]   The bad.
[01:23:30.540 --> 01:23:32.940]   Well, what is it doesn't matter if it has an accent.
[01:23:32.940 --> 01:23:34.700]   Well, I guess it's a different thing.
[01:23:34.700 --> 01:23:39.100]   I can tell what I'm looking at her that she's speaking in an accent because in that plummy
[01:23:39.100 --> 01:23:44.020]   or not, it's the all not is the giveaway or not or not.
[01:23:44.020 --> 01:23:45.020]   Hmm.
[01:23:45.020 --> 01:23:49.500]   AI shows the way says new scientist.
[01:23:49.500 --> 01:23:50.660]   Wow.
[01:23:50.660 --> 01:23:57.500]   I mean, it wasn't, you know, it's not just random video.
[01:23:57.500 --> 01:24:02.500]   They had to, you know, get some special videos, etc, etc.
[01:24:02.500 --> 01:24:09.460]   The, the Google engineer thinks lip reading, AI's are most likely to be using consumer
[01:24:09.460 --> 01:24:13.820]   devices to help them figure out what we're trying to say.
[01:24:13.820 --> 01:24:18.700]   Oh, isn't that interesting to assist them?
[01:24:18.700 --> 01:24:22.140]   We believe machine lip readers have enormous practical potential with applications and
[01:24:22.140 --> 01:24:26.740]   improving hearing aids, silent dictation in public spaces.
[01:24:26.740 --> 01:24:28.100]   So you don't have to talk to Siri.
[01:24:28.100 --> 01:24:30.460]   You don't have to make it fool yourself.
[01:24:30.460 --> 01:24:32.460]   You could just lip it.
[01:24:32.460 --> 01:24:33.460]   Siri.
[01:24:33.460 --> 01:24:34.460]   Siri.
[01:24:34.460 --> 01:24:36.620]   I feel like, hold on.
[01:24:36.620 --> 01:24:38.420]   And the noise.
[01:24:38.420 --> 01:24:39.980]   I don't actually hold on.
[01:24:39.980 --> 01:24:43.980]   I'm going to look something up because I feel like I saw an app that actually Ava.
[01:24:43.980 --> 01:24:46.220]   Ava can read your lips.
[01:24:46.220 --> 01:24:48.740]   Ava, it was on product hunt.
[01:24:48.740 --> 01:24:51.820]   It is a, no, it's for hard of hearing people.
[01:24:51.820 --> 01:24:53.380]   And I'm trying to think it does.
[01:24:53.380 --> 01:24:54.380]   It provides captions.
[01:24:54.380 --> 01:24:56.100]   Here, let's watch the video.
[01:24:56.100 --> 01:25:00.460]   This is the, the promotional video for, for Ava.
[01:25:00.460 --> 01:25:05.300]   I am not very different from anyone else.
[01:25:05.300 --> 01:25:08.340]   I'm a pretty normal girl, I think.
[01:25:08.340 --> 01:25:10.540]   I could be one of the neighbors.
[01:25:10.540 --> 01:25:11.900]   He's got a cochlear implant.
[01:25:11.900 --> 01:25:14.340]   Maybe we've run into each other.
[01:25:14.340 --> 01:25:15.340]   So these are obviously like just-
[01:25:15.340 --> 01:25:17.820]   I like testing what people say to each other.
[01:25:17.820 --> 01:25:21.300]   When did I float in?
[01:25:21.300 --> 01:25:28.580]   I like when people make efforts to communicate with me.
[01:25:28.580 --> 01:25:29.580]   It really makes my-
[01:25:29.580 --> 01:25:32.740]   I just have to say, he did the wrong.
[01:25:32.740 --> 01:25:34.740]   I am death.
[01:25:34.740 --> 01:25:36.300]   I am hard of hearing.
[01:25:36.300 --> 01:25:40.780]   I am pretty good at reading the left.
[01:25:40.780 --> 01:25:43.380]   But in distinct chatter, she can't understand it.
[01:25:43.380 --> 01:25:45.540]   Can you pass me the gravy?
[01:25:45.540 --> 01:25:46.540]   Now she can.
[01:25:46.540 --> 01:25:47.540]   She can relive.
[01:25:47.540 --> 01:25:52.380]   When there's too many people down, this can get really tough.
[01:25:52.380 --> 01:25:53.820]   So this is a long video.
[01:25:53.820 --> 01:25:56.140]   So this is super long, I think.
[01:25:56.140 --> 01:26:01.660]   So Ava shows you what people say in less than a second easy communication.
[01:26:01.660 --> 01:26:03.580]   But does it relive?
[01:26:03.580 --> 01:26:07.180]   So that was like, wait, did I bring this up?
[01:26:07.180 --> 01:26:08.980]   I don't think it relives.
[01:26:08.980 --> 01:26:13.940]   I think it helps understand the sound and does speech recognition?
[01:26:13.940 --> 01:26:14.940]   Huh.
[01:26:14.940 --> 01:26:17.300]   I can't tell.
[01:26:17.300 --> 01:26:19.300]   Let's see.
[01:26:19.300 --> 01:26:24.380]   My aunt and uncle are deaf, so I'm always interested in these.
[01:26:24.380 --> 01:26:25.380]   Really?
[01:26:25.380 --> 01:26:27.380]   I'm always a geology.
[01:26:27.380 --> 01:26:30.100]   Okay, so it does life captioning.
[01:26:30.100 --> 01:26:32.260]   Oh, live captioning.
[01:26:32.260 --> 01:26:33.260]   Okay.
[01:26:33.260 --> 01:26:36.020]   Well, it says life here, but yeah, I think that's the idea.
[01:26:36.020 --> 01:26:37.100]   So it does caption.
[01:26:37.100 --> 01:26:40.260]   So it's obviously some sort of speech recognition.
[01:26:40.260 --> 01:26:42.460]   Okay, so not as exciting.
[01:26:42.460 --> 01:26:44.100]   But imagine that for like-
[01:26:44.100 --> 01:26:45.100]   Well, right.
[01:26:45.100 --> 01:26:49.540]   And that would get better, especially in a noisy environment, it would get better because
[01:26:49.540 --> 01:26:52.340]   it could also do that.
[01:26:52.340 --> 01:26:55.260]   Well, that's very cool.
[01:26:55.260 --> 01:26:56.860]   And you can think about it this way.
[01:26:56.860 --> 01:26:57.860]   Ready?
[01:26:57.860 --> 01:27:01.900]   Imagine it's on like, it's part of your like doorbell cam, for example.
[01:27:01.900 --> 01:27:05.060]   And then you have the ability for someone to come up-
[01:27:05.060 --> 01:27:07.820]   Oh, you can see the UPS guy swearing at you.
[01:27:07.820 --> 01:27:08.820]   Exactly.
[01:27:08.820 --> 01:27:10.180]   Or, you know, whatever.
[01:27:10.180 --> 01:27:13.340]   Some Girl Scout trying to sell you cookies.
[01:27:13.340 --> 01:27:17.020]   But just like when you start thinking about where you can put, once it's in software,
[01:27:17.020 --> 01:27:20.100]   you can put it anywhere.
[01:27:20.100 --> 01:27:21.100]   So I don't know.
[01:27:21.100 --> 01:27:22.100]   I get excited.
[01:27:22.100 --> 01:27:23.100]   Ow.
[01:27:23.100 --> 01:27:24.100]   Ow.
[01:27:24.100 --> 01:27:25.100]   Ow.
[01:27:25.100 --> 01:27:26.100]   I just have my toe.
[01:27:26.100 --> 01:27:27.100]   Unfamiliar desk.
[01:27:27.100 --> 01:27:28.100]   Uh-oh.
[01:27:28.100 --> 01:27:29.100]   Ow.
[01:27:29.100 --> 01:27:35.780]   Tila Tequila was suspended from Twitter for pro-nancy posts.
[01:27:35.780 --> 01:27:37.340]   So I got this question a lot in Europe.
[01:27:37.340 --> 01:27:38.340]   I just came back in Berlin.
[01:27:38.340 --> 01:27:40.100]   So was Tila Tequila?
[01:27:40.100 --> 01:27:42.100]   I even wanted that.
[01:27:42.100 --> 01:27:43.100]   Okay.
[01:27:43.100 --> 01:27:44.100]   Yeah, that.
[01:27:44.100 --> 01:27:48.060]   What do you do about the requirements to get rid of hate speech for Facebook?
[01:27:48.060 --> 01:27:51.820]   You know, here I am saying- Well, Germany's done that very effectively, have they not?
[01:27:51.820 --> 01:27:52.820]   Yeah.
[01:27:52.820 --> 01:27:56.540]   So it's hard again because of the whole fake news thing.
[01:27:56.540 --> 01:27:58.580]   Well, didn't France do this whole thing?
[01:27:58.580 --> 01:28:02.180]   Like you couldn't sell Nazi Mille de Brabilla on eBay and that was a pain for a while.
[01:28:02.180 --> 01:28:03.180]   That was a big deal.
[01:28:03.180 --> 01:28:04.180]   Germany too.
[01:28:04.180 --> 01:28:05.180]   Yeah.
[01:28:05.180 --> 01:28:06.180]   But it goes beyond that now.
[01:28:06.180 --> 01:28:07.180]   Well, yes.
[01:28:07.180 --> 01:28:09.300]   By the way, I went to a museum where they told you you could take pictures of everything,
[01:28:09.300 --> 01:28:13.460]   but not legally of Nazi member bills, Nazi symbols.
[01:28:13.460 --> 01:28:23.660]   Germany said that it's unlikely that the Brexit Trump kind of move towards authoritarian
[01:28:23.660 --> 01:28:28.820]   populism would happen in Germany because they've been through it already and they're kind
[01:28:28.820 --> 01:28:30.100]   of hypersensitive.
[01:28:30.100 --> 01:28:36.780]   So I gave a talk at the hub conference, which is a 2000 person big tech conference that's
[01:28:36.780 --> 01:28:37.780]   in Berlin.
[01:28:37.780 --> 01:28:40.020]   And it's just went up online by the way.
[01:28:40.020 --> 01:28:45.700]   And I in it, I argued that I said, listen, we're going to be busy for the next four years
[01:28:45.700 --> 01:28:46.700]   here.
[01:28:46.700 --> 01:28:47.700]   Stop defying yourself against America.
[01:28:47.700 --> 01:28:49.540]   We need your leadership.
[01:28:49.540 --> 01:28:53.540]   We need Europe, particularly Germany to be a leader not only in politically, but also
[01:28:53.540 --> 01:28:55.020]   technologically.
[01:28:55.020 --> 01:28:57.460]   That's a controversial thing to say because it makes them uncomfortable.
[01:28:57.460 --> 01:28:59.020]   Makes them a little hot on the contrary.
[01:28:59.020 --> 01:29:00.020]   Really?
[01:29:00.020 --> 01:29:01.020]   Why?
[01:29:01.020 --> 01:29:02.020]   Because we've been leaders before.
[01:29:02.020 --> 01:29:04.700]   Oh, they're really that sensitive to their whole idea.
[01:29:04.700 --> 01:29:06.580]   And so it's a sensitive thing to say.
[01:29:06.580 --> 01:29:07.580]   Interesting.
[01:29:07.580 --> 01:29:10.580]   But there was a tweet.
[01:29:10.580 --> 01:29:12.580]   Oh, I'm going to find this.
[01:29:12.580 --> 01:29:13.580]   I love this tweet.
[01:29:13.580 --> 01:29:14.580]   We want a one second.
[01:29:14.580 --> 01:29:15.580]   A one for the tweet.
[01:29:15.580 --> 01:29:20.580]   So a guy named Errrr Nathaniel Tables.
[01:29:20.580 --> 01:29:23.820]   Stables, tables said quote, oh, I get it.
[01:29:23.820 --> 01:29:28.180]   In World War three, Germany gets to save the rest of the world from fascists.
[01:29:28.180 --> 01:29:30.540]   That's a pretty solid third act.
[01:29:30.540 --> 01:29:31.540]   I saw that.
[01:29:31.540 --> 01:29:32.540]   I saw that tweet.
[01:29:32.540 --> 01:29:33.540]   That was very funny.
[01:29:33.540 --> 01:29:34.540]   It was very good.
[01:29:34.540 --> 01:29:36.820]   So Merkel is up for a reelection soon.
[01:29:36.820 --> 01:29:39.620]   Yeah, so she just announced that she was going to do it again.
[01:29:39.620 --> 01:29:44.540]   And I talked to journalists and liberals who said I've never voted for the CDU before,
[01:29:44.540 --> 01:29:49.860]   but I'm going to now because she ran against a nationalist candidate.
[01:29:49.860 --> 01:29:54.500]   Well, there's something called the alternative for Deutschland, AFD that's out there.
[01:29:54.500 --> 01:29:57.060]   And they could get some are scared that she would lose.
[01:29:57.060 --> 01:30:02.380]   Some are scared that it's going to turn into America, UK, France and such.
[01:30:02.380 --> 01:30:09.100]   So it's kind of the last stable, sane leadership in the Western world.
[01:30:09.100 --> 01:30:10.100]   Damn near it.
[01:30:10.100 --> 01:30:11.100]   It's going to Australia.
[01:30:11.100 --> 01:30:16.860]   I mean, I mean, Austria, Hungary, France is about to go from Rio de Pen, we think.
[01:30:16.860 --> 01:30:19.660]   There's roiling around as there are now.
[01:30:19.660 --> 01:30:23.180]   And so yeah, it's a big topic of discussion there.
[01:30:23.180 --> 01:30:24.580]   No worries.
[01:30:24.580 --> 01:30:25.580]   Me is after this.
[01:30:25.580 --> 01:30:28.380]   I think Kanye might actually win in 2020.
[01:30:28.380 --> 01:30:30.180]   Yeah, I know.
[01:30:30.180 --> 01:30:31.180]   Wow.
[01:30:31.180 --> 01:30:35.060]   Like Kardashian in the White House.
[01:30:35.060 --> 01:30:38.820]   That's about where we're headed.
[01:30:38.820 --> 01:30:40.540]   Yeah, it is.
[01:30:40.540 --> 01:30:42.020]   It is.
[01:30:42.020 --> 01:30:47.980]   The other thing that just mentioned in Germany was renewed talk.
[01:30:47.980 --> 01:30:53.340]   I don't think it would go that far, but renewed talk of trying to give Snowden asylum.
[01:30:53.340 --> 01:30:59.820]   He was called to testify before the legislature and they kind of demanded to be there.
[01:30:59.820 --> 01:31:02.740]   Well, the only way it could be there is if they gave him some conditions as I was talking
[01:31:02.740 --> 01:31:08.380]   again, the Greens are going again, but there's at least talk in some circles about trying
[01:31:08.380 --> 01:31:12.180]   to figure out what to do about Snowden and his vulnerability in Russia.
[01:31:12.180 --> 01:31:13.180]   Okay.
[01:31:13.180 --> 01:31:16.820]   Do we lose Jeff or do we lose me?
[01:31:16.820 --> 01:31:18.820]   We push the wrong button, I think.
[01:31:18.820 --> 01:31:20.460]   You can lip read.
[01:31:20.460 --> 01:31:21.460]   Are you going to lip read?
[01:31:21.460 --> 01:31:22.460]   Let me now.
[01:31:22.460 --> 01:31:23.460]   Hold on.
[01:31:23.460 --> 01:31:25.300]   Let me get my Google AI.
[01:31:25.300 --> 01:31:30.740]   Obama was talking with Joe Spiegel when he was in Germany.
[01:31:30.740 --> 01:31:34.060]   Der Spiegel asked if he was going to pardon Snowden.
[01:31:34.060 --> 01:31:35.300]   President Obama said, "I can't pardon him.
[01:31:35.300 --> 01:31:37.900]   He hasn't gone before a court and presented himself."
[01:31:37.900 --> 01:31:39.660]   That's actually not true.
[01:31:39.660 --> 01:31:47.740]   Remember that Nixon was pardoned by President Ford and had not been in front of a court.
[01:31:47.740 --> 01:31:51.020]   He goes on to say, "It's not something I would comment on at this point."
[01:31:51.020 --> 01:31:53.020]   Now Obama, by the way, is a former law professor.
[01:31:53.020 --> 01:31:56.740]   He knows perfectly well what he can and cannot do, so that's dodging the question.
[01:31:56.740 --> 01:32:00.740]   I think Mr. Snowden raised some legitimate concerns how he did it with something that
[01:32:00.740 --> 01:32:04.500]   did not follow the procedures and practices of our intelligence community.
[01:32:04.500 --> 01:32:07.740]   If everybody took the approach, I make my own—I don't want to read this.
[01:32:07.740 --> 01:32:09.660]   It just makes me angry.
[01:32:09.660 --> 01:32:14.340]   But the acknowledgement that he raised important issues, is that happened before from Obama?
[01:32:14.340 --> 01:32:15.340]   No.
[01:32:15.340 --> 01:32:19.500]   I mean, he's come a little bit in the right direction on this.
[01:32:19.500 --> 01:32:23.500]   It's a trial balloon.
[01:32:23.500 --> 01:32:27.100]   At the point at which Mr. Snowden wants to present himself before the legal authorities
[01:32:27.100 --> 01:32:30.580]   and make his arguments or have his lawyers make his arguments, I think those issues come
[01:32:30.580 --> 01:32:32.940]   into play until that time.
[01:32:32.940 --> 01:32:35.780]   What I've tried to suggest, both the American people but also to the world, is that we have
[01:32:35.780 --> 01:32:38.420]   to balance this issue of privacy and security.
[01:32:38.420 --> 01:32:42.140]   Yeah, he's dodging the question, basically.
[01:32:42.140 --> 01:32:43.140]   Yes.
[01:32:43.140 --> 01:32:44.140]   Yes.
[01:32:44.140 --> 01:32:49.780]   And if Snowden, what about—
[01:32:49.780 --> 01:32:55.060]   Obama has already pardoned three Iranian Americans who haven't stood trial.
[01:32:55.060 --> 01:32:59.820]   This is just clearly like—I'm not going to answer that question.
[01:32:59.820 --> 01:33:01.940]   Without actually saying I'm not answering the question.
[01:33:01.940 --> 01:33:05.860]   Well, he should—I mean, it's pretty clear.
[01:33:05.860 --> 01:33:09.260]   Snoopers charter is now—or is about to be law in the UK.
[01:33:09.260 --> 01:33:15.060]   This is an extreme surveillance law that Theresa May, the current UK Prime Minister, has been
[01:33:15.060 --> 01:33:19.620]   proposing for several years now that she's the Prime Minister.
[01:33:19.620 --> 01:33:21.420]   She got both houses apart on it to approve it.
[01:33:21.420 --> 01:33:26.500]   It just remains for the sovereign to approve it.
[01:33:26.500 --> 01:33:31.340]   It will be the most extreme surveillance law in Europe.
[01:33:31.340 --> 01:33:39.100]   Edward Snowden says it goes farther than many autocracies in a tweet.
[01:33:39.100 --> 01:33:47.060]   I mean, the list of things that the government can now do and is incredible.
[01:33:47.060 --> 01:33:51.020]   By the way, it requires tech companies to collect and retain materials for use by the
[01:33:51.020 --> 01:33:52.020]   government.
[01:33:52.020 --> 01:33:56.180]   In secret, in most cases, it requires back doors.
[01:33:56.180 --> 01:33:59.220]   It eliminates encryption in many cases.
[01:33:59.220 --> 01:34:03.260]   It's a very bad model for what could happen here.
[01:34:03.260 --> 01:34:04.260]   Oof.
[01:34:04.260 --> 01:34:12.580]   Madison Square Garden says hackers have spent a year harvesting visitor credit card data.
[01:34:12.580 --> 01:34:17.420]   I like how patient hackers have become.
[01:34:17.420 --> 01:34:22.140]   They, I think, was for five different events.
[01:34:22.140 --> 01:34:25.980]   malware has been capturing payment card data from a system that processes payments—I'm
[01:34:25.980 --> 01:34:30.700]   sorry, for five different properties, not five different events unauthorized access to
[01:34:30.700 --> 01:34:36.340]   the Madison Square Garden payment processing system and installation of a program that
[01:34:36.340 --> 01:34:41.820]   looked for payment card data as it was being routed through the system for authorization.
[01:34:41.820 --> 01:34:44.100]   So just a little snooper.
[01:34:44.100 --> 01:34:49.420]   Cards used to buy merchandise and food and drinks at these properties from November 9th
[01:34:49.420 --> 01:34:53.860]   of last year through October 24th of this year may have been affected.
[01:34:53.860 --> 01:34:55.980]   Did you buy a hot dog at Madison Square Garden?
[01:34:55.980 --> 01:34:58.140]   Yeah, kind of like a skimmer, but a software.
[01:34:58.140 --> 01:34:59.140]   Sure, how?
[01:34:59.140 --> 01:35:02.500]   Hot dog at Madison Square Garden, the theater at Madison Square Garden, Radio City Music
[01:35:02.500 --> 01:35:05.260]   called Beacon Theater or Chicago Theater.
[01:35:05.260 --> 01:35:06.260]   Good news.
[01:35:06.260 --> 01:35:09.860]   So isn't that data supposed to be encrypted?
[01:35:09.860 --> 01:35:12.260]   Good question.
[01:35:12.260 --> 01:35:14.060]   Okay.
[01:35:14.060 --> 01:35:19.100]   It would be if you used Apple Pay or Android Pay.
[01:35:19.100 --> 01:35:23.540]   Maybe not if you used a credit card.
[01:35:23.540 --> 01:35:28.140]   Metallica's full catalog is now on Napster.
[01:35:28.140 --> 01:35:31.340]   How times you change.
[01:35:31.340 --> 01:35:32.340]   It's not the same.
[01:35:32.340 --> 01:35:33.340]   It's the same Metallica.
[01:35:33.340 --> 01:35:34.340]   It's not the same Napster.
[01:35:34.340 --> 01:35:35.340]   That's the key.
[01:35:35.340 --> 01:35:41.860]   Metallica hasn't changed Napster has.
[01:35:41.860 --> 01:35:45.740]   Tesla's deal to merge with Solar City is a done deal.
[01:35:45.740 --> 01:35:50.580]   If you buy Solar City panels now you'll be buying them from Tesla Solar.
[01:35:50.580 --> 01:35:53.980]   I have Solar City panels and a Tesla.
[01:35:53.980 --> 01:35:57.500]   Now, do you have a zero electricity bill now?
[01:35:57.500 --> 01:36:00.860]   No, because we did the least thing.
[01:36:00.860 --> 01:36:05.580]   Least purchase option or power purchase option, which is they own the...
[01:36:05.580 --> 01:36:11.220]   We didn't pay any money for the installation of the panels or the panels themselves.
[01:36:11.220 --> 01:36:16.900]   We agreed to buy back electricity from Solar City as opposed to from our power company at
[01:36:16.900 --> 01:36:22.420]   a guaranteed fixed rate over 20 years of 18 cents per kilowatt hour, which is considerably
[01:36:22.420 --> 01:36:25.700]   less than the top rate in California.
[01:36:25.700 --> 01:36:28.260]   We were in tier four a lot of the time.
[01:36:28.260 --> 01:36:33.900]   If we use more power than the panels produce, then we'd go back to the utility, but we started
[01:36:33.900 --> 01:36:36.020]   tier one, so it should still save us money.
[01:36:36.020 --> 01:36:39.700]   It saved us, I think, a considerable amount of money.
[01:36:39.700 --> 01:36:45.700]   The idea being with nothing up front, the savings is immediate.
[01:36:45.700 --> 01:36:46.700]   Right.
[01:36:46.700 --> 01:36:47.700]   Right.
[01:36:47.700 --> 01:36:48.700]   It makes me feel good.
[01:36:48.700 --> 01:36:49.700]   That's the main thing.
[01:36:49.700 --> 01:36:53.900]   And if you drive a Tesla, it's nice because you're powering your car.
[01:36:53.900 --> 01:37:01.140]   The local power company, PG&E in California, will not let you put panels in that produce
[01:37:01.140 --> 01:37:04.980]   more electricity than you're currently consuming.
[01:37:04.980 --> 01:37:08.300]   So you can't do it like a profit deal.
[01:37:08.300 --> 01:37:10.180]   There's no solar mining.
[01:37:10.180 --> 01:37:11.340]   Which they should, right?
[01:37:11.340 --> 01:37:14.540]   If somebody wants to generate electricity, let them.
[01:37:14.540 --> 01:37:19.460]   They don't have the proper back and forth yet on the grids, though, to manage that, I
[01:37:19.460 --> 01:37:20.460]   guess.
[01:37:20.460 --> 01:37:24.340]   One more to do with the fact that the business of PG&E is building power plants, frankly.
[01:37:24.340 --> 01:37:26.180]   Okay, that too.
[01:37:26.180 --> 01:37:28.260]   That's always an issue with these guys.
[01:37:28.260 --> 01:37:29.260]   Yeah.
[01:37:29.260 --> 01:37:31.660]   They just have to go kicking and screaming.
[01:37:31.660 --> 01:37:36.140]   You know, they're kicking and screaming, that's all.
[01:37:36.140 --> 01:37:42.220]   Did you see that, okay, so Samsung is the number one smartphone maker, but did you see
[01:37:42.220 --> 01:37:48.380]   that Apple gets 91% of all the smartphone profit worldwide?
[01:37:48.380 --> 01:37:50.140]   That's probably lower now than it used to be.
[01:37:50.140 --> 01:37:51.900]   I used to be like 95%.
[01:37:51.900 --> 01:37:52.900]   So it's down?
[01:37:52.900 --> 01:37:53.900]   I think so.
[01:37:53.900 --> 01:37:58.020]   I'm going to right now do something I should have done a long time ago, which is disable
[01:37:58.020 --> 01:38:00.780]   the flash autoplay video.
[01:38:00.780 --> 01:38:07.100]   Why I've left that on for so long as beyond me.
[01:38:07.100 --> 01:38:08.100]   Plugins.
[01:38:08.100 --> 01:38:09.660]   It says that.
[01:38:09.660 --> 01:38:11.580]   Let me choose when to run plug-in content.
[01:38:11.580 --> 01:38:13.700]   Well, I didn't choose that.
[01:38:13.700 --> 01:38:14.700]   Okay.
[01:38:14.700 --> 01:38:20.100]   So Apple now has 94% of global smart products.
[01:38:20.100 --> 01:38:23.140]   The smartphone profits back in November 2015.
[01:38:23.140 --> 01:38:30.460]   Ah, well, that's the one, but see, it takes a while to get all that information collated.
[01:38:30.460 --> 01:38:33.660]   In July of 2015, it had 92%.
[01:38:33.660 --> 01:38:34.820]   It's going up.
[01:38:34.820 --> 01:38:38.060]   Well, no, that would be so no.
[01:38:38.060 --> 01:38:39.980]   94 was a year ago at this time.
[01:38:39.980 --> 01:38:41.300]   Okay, so go down a little bit.
[01:38:41.300 --> 01:38:43.260]   So then it went down.
[01:38:43.260 --> 01:38:45.380]   So it's going down.
[01:38:45.380 --> 01:38:48.820]   It's still a pretty high number.
[01:38:48.820 --> 01:38:51.140]   90 cents of every dollar in profit.
[01:38:51.140 --> 01:38:52.460]   It's still pretty good.
[01:38:52.460 --> 01:38:54.740]   I was giving you a little historical perspective here.
[01:38:54.740 --> 01:38:55.740]   Well, it's interesting.
[01:38:55.740 --> 01:39:01.020]   There's a Samsung despite the flop that was the Note 7 is still the number one smartphone
[01:39:01.020 --> 01:39:02.820]   maker worldwide.
[01:39:02.820 --> 01:39:05.780]   Oh my gosh, guys.
[01:39:05.780 --> 01:39:07.820]   There are two of those downstairs.
[01:39:07.820 --> 01:39:08.820]   What?
[01:39:08.820 --> 01:39:12.260]   There are two Note 7s in the house.
[01:39:12.260 --> 01:39:13.260]   Yes.
[01:39:13.260 --> 01:39:15.060]   Tell mom and dad.
[01:39:15.060 --> 01:39:16.060]   Your parents?
[01:39:16.060 --> 01:39:17.060]   Yeah.
[01:39:17.060 --> 01:39:18.060]   What is their story?
[01:39:18.060 --> 01:39:20.180]   They don't want to get rid of it.
[01:39:20.180 --> 01:39:21.180]   And it hasn't blown up yet.
[01:39:21.180 --> 01:39:22.180]   So probably isn't right.
[01:39:22.180 --> 01:39:24.380]   And it hasn't blown up yet.
[01:39:24.380 --> 01:39:27.780]   Didn't didn't Samsung like disable charging or something or so?
[01:39:27.780 --> 01:39:28.780]   The newest thing is they're perfect.
[01:39:28.780 --> 01:39:31.340]   You can only charge it to 60%.
[01:39:31.340 --> 01:39:33.860]   Has that happened to them already?
[01:39:33.860 --> 01:39:34.860]   That's what she told me.
[01:39:34.860 --> 01:39:37.180]   No, I don't know how accurate that is.
[01:39:37.180 --> 01:39:41.980]   By the way, what I just said was me echoing what they're probably thinking.
[01:39:41.980 --> 01:39:48.020]   But in fact, that is not true that the fact that it hasn't blown up yet means it won't
[01:39:48.020 --> 01:39:55.180]   because what we think is the battery is too much battery for the space and you get
[01:39:55.180 --> 01:40:03.060]   electroplating of the lithium over time and that at some point it joins the layers together
[01:40:03.060 --> 01:40:05.140]   electrically and shorts.
[01:40:05.140 --> 01:40:08.660]   So it is in fact a process that could happen over time.
[01:40:08.660 --> 01:40:12.940]   So in other words, just because it hasn't caught on fire doesn't mean it won't.
[01:40:12.940 --> 01:40:14.500]   I explain that.
[01:40:14.500 --> 01:40:16.020]   They understand that.
[01:40:16.020 --> 01:40:17.020]   They understand that.
[01:40:17.020 --> 01:40:18.020]   I don't blame them.
[01:40:18.020 --> 01:40:19.020]   What do they love about it?
[01:40:19.020 --> 01:40:20.020]   It's a great phone.
[01:40:20.020 --> 01:40:21.660]   It's the best phone of the year.
[01:40:21.660 --> 01:40:23.580]   They're great phone.
[01:40:23.580 --> 01:40:25.820]   And I think my mom's just stubborn.
[01:40:25.820 --> 01:40:27.660]   I love her, but she's stubborn.
[01:40:27.660 --> 01:40:30.540]   I'm not giving up my phone.
[01:40:30.540 --> 01:40:31.540]   Yeah.
[01:40:31.540 --> 01:40:32.540]   I love it.
[01:40:32.540 --> 01:40:33.540]   I don't blame them.
[01:40:33.540 --> 01:40:38.580]   You didn't talk like that, the Leo.
[01:40:38.580 --> 01:40:40.700]   She probably says, y'all doesn't she sound like?
[01:40:40.700 --> 01:40:41.700]   Oh, no, she's a Yankee.
[01:40:41.700 --> 01:40:42.700]   She's a Yankee.
[01:40:42.700 --> 01:40:43.700]   I'm in state New York.
[01:40:43.700 --> 01:40:44.700]   Oh, my people.
[01:40:44.700 --> 01:40:45.700]   Yeah.
[01:40:45.700 --> 01:40:47.820]   That's where my people are from.
[01:40:47.820 --> 01:40:52.740]   I think we've anything else that I miss anything.
[01:40:52.740 --> 01:40:54.500]   And I am solo energy.
[01:40:54.500 --> 01:40:55.500]   I feel bad.
[01:40:55.500 --> 01:40:56.500]   I'm sorry, you guys.
[01:40:56.500 --> 01:40:57.500]   You're great.
[01:40:57.500 --> 01:40:58.500]   No, I am too.
[01:40:58.500 --> 01:40:59.500]   I'm getting jet lag is hitting me.
[01:40:59.500 --> 01:41:01.500]   Oh, well, let's all go to bed.
[01:41:01.500 --> 01:41:04.100]   We got to be sweet potatoes.
[01:41:04.100 --> 01:41:05.100]   Yum.
[01:41:05.100 --> 01:41:07.180]   You're not making sweet potato pie.
[01:41:07.180 --> 01:41:08.620]   Tell me you're not making sweet potato pie.
[01:41:08.620 --> 01:41:12.740]   No, I'm making I'm making a sneakily healthy version of sweet potatoes, but I'm saying marshmallows
[01:41:12.740 --> 01:41:14.260]   on the top for my daughter, my mom.
[01:41:14.260 --> 01:41:15.260]   I love sweet.
[01:41:15.260 --> 01:41:16.260]   I love that.
[01:41:16.260 --> 01:41:17.260]   I love that oven with you for Thanksgiving.
[01:41:17.260 --> 01:41:19.780]   Imagine what you could do to a potato pie.
[01:41:19.780 --> 01:41:21.580]   I have I have imagined, Jeff.
[01:41:21.580 --> 01:41:24.580]   What do you what's over the menu?
[01:41:24.580 --> 01:41:25.980]   This one is at my parents house.
[01:41:25.980 --> 01:41:33.560]   So we're not doing my we're doing a very traditional turkey breadcrumb and sage and
[01:41:33.560 --> 01:41:38.540]   celery stuffing and green bean casserole.
[01:41:38.540 --> 01:41:41.140]   That's about what we're doing, except we're doing a ham instead of a turkey because I like
[01:41:41.140 --> 01:41:42.780]   to be different.
[01:41:42.780 --> 01:41:43.780]   I like games.
[01:41:43.780 --> 01:41:45.500]   One year we did Cornish game hints.
[01:41:45.500 --> 01:41:46.700]   That was a weird deer.
[01:41:46.700 --> 01:41:51.060]   Yeah, I think that's what Lisa's sister's doing is crunish game.
[01:41:51.060 --> 01:41:52.060]   They're too small.
[01:41:52.060 --> 01:41:54.140]   They're tiny little birds.
[01:41:54.140 --> 01:41:56.700]   It's like it's difficult.
[01:41:56.700 --> 01:42:01.060]   Google home is on this just in Google home is on sale target.
[01:42:01.060 --> 01:42:02.060]   I know.
[01:42:02.060 --> 01:42:04.060]   My Friday special 99 bucks.
[01:42:04.060 --> 01:42:06.060]   Oh, I'm going to pitch it.
[01:42:06.060 --> 01:42:07.060]   My pitch is.
[01:42:07.060 --> 01:42:08.060]   This pisses me off.
[01:42:08.060 --> 01:42:11.580]   Oh, so actually I talked to someone on Twitter about this.
[01:42:11.580 --> 01:42:15.780]   They asked Google if they could get the promo price and the answer was no.
[01:42:15.780 --> 01:42:19.780]   They went to Best Buy to see if they could get the promo price and the answer was maybe.
[01:42:19.780 --> 01:42:21.980]   So let us know if you get that.
[01:42:21.980 --> 01:42:26.780]   But I was going to say this is shameless, but I'm going to do it because it benefits you
[01:42:26.780 --> 01:42:28.220]   guys.
[01:42:28.220 --> 01:42:31.540]   We did for the podcast, the IOT podcast, which is out.
[01:42:31.540 --> 01:42:37.860]   We did some black Friday deals and like don't pay more than $40 for an Amazon Echo dot.
[01:42:37.860 --> 01:42:38.860]   I got the dot.
[01:42:38.860 --> 01:42:41.140]   I paid 50 for the dot, but I was happy with 50.
[01:42:41.140 --> 01:42:43.460]   Well, this is for Black Friday.
[01:42:43.460 --> 01:42:46.780]   Don't pay more than $200 for like a Nest thermostat.
[01:42:46.780 --> 01:42:51.700]   Don't pay more than I think it's 150 for the Nest Cam.
[01:42:51.700 --> 01:42:57.860]   The Ring doorbell, which is the sponsor of the show is I think $50 off.
[01:42:57.860 --> 01:42:58.860]   Nice.
[01:42:58.860 --> 01:43:02.460]   So there's there's a lot of good smart home deals running around out there and the Google
[01:43:02.460 --> 01:43:03.860]   home is $99.
[01:43:03.860 --> 01:43:04.860]   So don't pay more.
[01:43:04.860 --> 01:43:05.860]   I've noticed that.
[01:43:05.860 --> 01:43:08.660]   There's some real cutting going on.
[01:43:08.660 --> 01:43:13.380]   Whereas Oh, and if you like the Arlo's 350 for a pack of four is a best buy door buster.
[01:43:13.380 --> 01:43:14.380]   What is Arlo's?
[01:43:14.380 --> 01:43:15.940]   Oh, you haven't seen them?
[01:43:15.940 --> 01:43:16.940]   They're cute.
[01:43:16.940 --> 01:43:23.380]   They're these little outdoor wireless cameras with motion sensing and it's the neck gear
[01:43:23.380 --> 01:43:24.380]   camera.
[01:43:24.380 --> 01:43:28.340]   Hello, are they good?
[01:43:28.340 --> 01:43:30.340]   Yes.
[01:43:30.340 --> 01:43:35.180]   So I was really excited about all these battery powered cameras right until the batteries
[01:43:35.180 --> 01:43:37.300]   ran out and I realized that I stuck like one of them.
[01:43:37.300 --> 01:43:38.300]   I haven't even a tree.
[01:43:38.300 --> 01:43:39.300]   Yeah.
[01:43:39.300 --> 01:43:40.300]   So you got to climb back up and get.
[01:43:40.300 --> 01:43:44.100]   So that's what I like about the ring because it has a solar panel.
[01:43:44.100 --> 01:43:45.100]   Yeah.
[01:43:45.100 --> 01:43:48.940]   So I tried that and it was all right.
[01:43:48.940 --> 01:43:49.940]   What's wrong?
[01:43:49.940 --> 01:43:50.940]   Does it not?
[01:43:50.940 --> 01:43:54.300]   I did not want to stay connected to the Wi-Fi network very well.
[01:43:54.300 --> 01:43:55.300]   Yeah.
[01:43:55.300 --> 01:43:57.300]   Well, you can't put it out on a trip.
[01:43:57.300 --> 01:43:58.300]   I mean, you can't put it too far.
[01:43:58.300 --> 01:43:59.300]   You know what?
[01:43:59.300 --> 01:44:02.900]   You need the Euro so that you can reach the Wi-Fi can reach out to the Arlo can reach out
[01:44:02.900 --> 01:44:03.900]   to the.
[01:44:03.900 --> 01:44:04.900]   Yes.
[01:44:04.900 --> 01:44:05.900]   Stick up.
[01:44:05.900 --> 01:44:06.900]   Yes.
[01:44:06.900 --> 01:44:12.700]   I have a challenge, but I have ordered the Google Wi-Fi hub and my I guess I got my.
[01:44:12.700 --> 01:44:14.700]   Someone just did a test and said it was great.
[01:44:14.700 --> 01:44:15.700]   Yeah.
[01:44:15.700 --> 01:44:18.300]   You know, somebody commissioned by Google.
[01:44:18.300 --> 01:44:19.700]   Oh, well, yeah.
[01:44:19.700 --> 01:44:21.100]   Oh, well, yeah.
[01:44:21.100 --> 01:44:22.100]   Yeah.
[01:44:22.100 --> 01:44:23.100]   Yeah.
[01:44:23.100 --> 01:44:24.700]   We think it's faster.
[01:44:24.700 --> 01:44:28.020]   I'm going to do the test when I get it and I'm going to put in the same environment
[01:44:28.020 --> 01:44:32.620]   that Euro is because as I mentioned, I have two networks and I can put them side by side
[01:44:32.620 --> 01:44:37.300]   and I will give you a test not paid for by Google.
[01:44:37.300 --> 01:44:42.620]   So I've got some really exciting gadgets coming in the Wi-Fi world.
[01:44:42.620 --> 01:44:43.620]   So stay tuned.
[01:44:43.620 --> 01:44:44.620]   I can't wait.
[01:44:44.620 --> 01:44:45.620]   Any hits?
[01:44:45.620 --> 01:44:46.620]   Yes.
[01:44:46.620 --> 01:44:47.620]   Any hits.
[01:44:47.620 --> 01:44:49.700]   I've written about them before.
[01:44:49.700 --> 01:44:51.380]   It's a different type of Wi-Fi.
[01:44:51.380 --> 01:44:52.540]   It's from Plume.
[01:44:52.540 --> 01:44:53.540]   Plume.
[01:44:53.540 --> 01:44:54.540]   Yeah.
[01:44:54.540 --> 01:44:55.540]   Yeah.
[01:44:55.540 --> 01:44:57.740]   And they're doing Wi-Fi as a service so that he's tiny little hexagons you plug in.
[01:44:57.740 --> 01:44:58.980]   I ordered those as well.
[01:44:58.980 --> 01:44:59.980]   Are yours coming?
[01:44:59.980 --> 01:45:00.980]   Mm-hmm.
[01:45:00.980 --> 01:45:02.980]   I ordered the Luma and the Plume.
[01:45:02.980 --> 01:45:08.300]   And still no word, but I have the Euro and I will be testing the December 9th.
[01:45:08.300 --> 01:45:10.260]   My Googles come.
[01:45:10.260 --> 01:45:11.260]   So.
[01:45:11.260 --> 01:45:12.260]   This is, you know what?
[01:45:12.260 --> 01:45:15.580]   I think really these are all of a category and probably will end up all being roughly
[01:45:15.580 --> 01:45:16.580]   the same.
[01:45:16.580 --> 01:45:17.580]   I can't imagine that much of a difference.
[01:45:17.580 --> 01:45:20.180]   Oh, and we didn't talk about Apple getting out of the router business.
[01:45:20.180 --> 01:45:21.380]   You're talking all the time.
[01:45:21.380 --> 01:45:22.980]   In a timely fashion.
[01:45:22.980 --> 01:45:23.980]   Oh.
[01:45:23.980 --> 01:45:25.620]   Throwing up its hand saying, okay, that's it.
[01:45:25.620 --> 01:45:28.820]   Apple's getting out of every business except the iPhone ultimately.
[01:45:28.820 --> 01:45:29.820]   Yeah.
[01:45:29.820 --> 01:45:30.820]   Right?
[01:45:30.820 --> 01:45:32.220]   It's losing the profit share.
[01:45:32.220 --> 01:45:36.500]   I mean, it's sucking 3% fewer profits out of the industry.
[01:45:36.500 --> 01:45:39.220]   It's, you know, it really feels like it's backing down on the Mac.
[01:45:39.220 --> 01:45:42.020]   They're really moving backwards on the Mac.
[01:45:42.020 --> 01:45:45.380]   It makes sense that they should get rid of their monitor business and their airport
[01:45:45.380 --> 01:45:52.540]   business because those accessories, you know, are duplicated at a lower price by other companies.
[01:45:52.540 --> 01:45:53.820]   Apple would have to do a mesh thing.
[01:45:53.820 --> 01:45:56.500]   They'd have to spend some energy doing that.
[01:45:56.500 --> 01:46:01.580]   So I think that they're really, I think really what's happening is Apple's just narrowing
[01:46:01.580 --> 01:46:04.020]   its focus to the products that make it a lot of money.
[01:46:04.020 --> 01:46:08.620]   And then I presume investing heavily in something that's going to be the next big thing.
[01:46:08.620 --> 01:46:12.620]   And my opinion is it's augmented reality.
[01:46:12.620 --> 01:46:13.620]   Apple sell it.
[01:46:13.620 --> 01:46:14.620]   Tim Cook sent a lot of signals.
[01:46:14.620 --> 01:46:17.900]   That's what this where he thinks the next big thing is.
[01:46:17.900 --> 01:46:23.300]   Not cars, not TV, not watches.
[01:46:23.300 --> 01:46:25.700]   All right.
[01:46:25.700 --> 01:46:27.900]   Microsoft Microsoft was showing that off at Hub.
[01:46:27.900 --> 01:46:30.020]   There are two interesting things on Hub.
[01:46:30.020 --> 01:46:33.620]   One was Microsoft showing off, you know, an elevator repair man going and with a R.
[01:46:33.620 --> 01:46:35.100]   Which makes a lot of sense.
[01:46:35.100 --> 01:46:36.100]   That makes sense.
[01:46:36.100 --> 01:46:37.100]   Like Google's last.
[01:46:37.100 --> 01:46:43.500]   The other thing, by the way, was HP was there showing their 3D printing technology.
[01:46:43.500 --> 01:46:49.220]   And they're going to get to the point that 90% of the parts for a 3D printer will be
[01:46:49.220 --> 01:46:50.700]   made by the 3D printer.
[01:46:50.700 --> 01:46:51.700]   Wow.
[01:46:51.700 --> 01:46:54.500]   So you just to make the first one, the 90 second set.
[01:46:54.500 --> 01:46:55.500]   It's really cool.
[01:46:55.500 --> 01:46:56.500]   Isn't it?
[01:46:56.500 --> 01:46:58.180]   It's bootstrapping the printer.
[01:46:58.180 --> 01:46:59.180]   I like it.
[01:46:59.180 --> 01:47:00.860]   You know, HP, it's interesting.
[01:47:00.860 --> 01:47:03.420]   HP split in 2-2 this past year.
[01:47:03.420 --> 01:47:07.220]   And HP Enterprise did not do great for the year.
[01:47:07.220 --> 01:47:08.580]   Although Meg Whitman said they did.
[01:47:08.580 --> 01:47:12.260]   It was pretty clear that numbers weren't, were, were rocking it.
[01:47:12.260 --> 01:47:18.460]   However, the HP Inc, which is the PC printer division, did very well considering.
[01:47:18.460 --> 01:47:20.860]   So I thought that was interesting.
[01:47:20.860 --> 01:47:22.260]   Now it's time for your picks of the week.
[01:47:22.260 --> 01:47:23.980]   Why don't you start SDC?
[01:47:23.980 --> 01:47:24.980]   Okay.
[01:47:24.980 --> 01:47:27.380]   Well, you mentioned mine.
[01:47:27.380 --> 01:47:31.940]   And I don't have the physical book because I'm not at home and it's on my Kindle.
[01:47:31.940 --> 01:47:33.300]   I'll just be honest.
[01:47:33.300 --> 01:47:34.980]   So you really don't have the physical book.
[01:47:34.980 --> 01:47:38.380]   So show us your phone and say, see it's on here.
[01:47:38.380 --> 01:47:39.380]   Here's the book.
[01:47:39.380 --> 01:47:40.380]   I can show you our home.
[01:47:40.380 --> 01:47:41.380]   I have the book.
[01:47:41.380 --> 01:47:42.780]   I'll show the book.
[01:47:42.780 --> 01:47:44.540]   There it is.
[01:47:44.540 --> 01:47:49.260]   We interviewed Cathy O'Neill on Monday for airing next Monday.
[01:47:49.260 --> 01:47:50.940]   Tell us about the book.
[01:47:50.940 --> 01:47:52.620]   Weapons of math destruction.
[01:47:52.620 --> 01:47:53.620]   And this is a book.
[01:47:53.620 --> 01:47:59.780]   So I have been obsessed with how algorithms will shape people's lives and the future.
[01:47:59.780 --> 01:48:03.580]   And for a time actually, I thought about leaving like years ago, I thought about leaving Giga
[01:48:03.580 --> 01:48:07.900]   home and actually starting a project on my own doing this with a podcast.
[01:48:07.900 --> 01:48:10.300]   And I didn't because I was lazy.
[01:48:10.300 --> 01:48:11.940]   I was intimidating, whatever.
[01:48:11.940 --> 01:48:18.060]   But everything I was trying to do, she actually did and she's a math, she's a former quant
[01:48:18.060 --> 01:48:23.300]   person at a hedge fund who was looking at how math was influencing people, how algorithms
[01:48:23.300 --> 01:48:30.780]   were influencing, how people were given merit pay and served in their community and even
[01:48:30.780 --> 01:48:34.900]   like dictating police presence in their neighborhoods.
[01:48:34.900 --> 01:48:40.180]   And she realized that all of these things, they had such influence and no one knew about
[01:48:40.180 --> 01:48:41.180]   it.
[01:48:41.180 --> 01:48:44.020]   It's kind of like how your credit score can be used in so many different ways that you
[01:48:44.020 --> 01:48:45.940]   might be unaware of, right?
[01:48:45.940 --> 01:48:49.300]   Back in the day or these crazy micro targeted ads.
[01:48:49.300 --> 01:48:54.300]   So the book is basically just her journey in talking about that.
[01:48:54.300 --> 01:48:59.100]   And it's really eye opening if you don't think about this already.
[01:48:59.100 --> 01:49:01.660]   So I really recommend it to anyone who wants to know more about this.
[01:49:01.660 --> 01:49:04.460]   And lots of really good examples.
[01:49:04.460 --> 01:49:12.500]   And her real complaint is that we trust these opaque algorithms because it's math and we're
[01:49:12.500 --> 01:49:15.900]   often thought, you know, well, if you don't trust the math, you must be a lot of them.
[01:49:15.900 --> 01:49:20.220]   Like when in fact, she's saying the math is wrong, the algorithms are opaque and so not
[01:49:20.220 --> 01:49:21.300]   auditable.
[01:49:21.300 --> 01:49:26.660]   And we are basing a lot of policy on these algorithms.
[01:49:26.660 --> 01:49:28.820]   And it's actually a real eye opener.
[01:49:28.820 --> 01:49:33.140]   You highly recommend the book and our interview is Monday on triangulation with Kathy O'Neill.
[01:49:33.140 --> 01:49:34.820]   I'm going to tune in.
[01:49:34.820 --> 01:49:36.460]   She was very impressive.
[01:49:36.460 --> 01:49:37.460]   We did not have enough time.
[01:49:37.460 --> 01:49:38.940]   I want to have her back and talk more.
[01:49:38.940 --> 01:49:44.260]   She's also started a data journalism program at Columbia.
[01:49:44.260 --> 01:49:46.420]   And I wanted to spend more time talking about data journalism.
[01:49:46.420 --> 01:49:48.220]   I know that's something you talk a lot about.
[01:49:48.220 --> 01:49:52.300]   Jeff, Jeff, he just went, boom, math, math, woo, data journalism.
[01:49:52.300 --> 01:49:54.340]   No, no, but that's what this is all about.
[01:49:54.340 --> 01:49:59.500]   Data journalism is going to end up being about, I think, exposing some of these algorithms
[01:49:59.500 --> 01:50:01.260]   as much as you can find it too.
[01:50:01.260 --> 01:50:06.460]   Right now there was a story that came by a study yesterday that showed how often journalists
[01:50:06.460 --> 01:50:07.460]   misuse data.
[01:50:07.460 --> 01:50:08.460]   Yep.
[01:50:08.460 --> 01:50:09.460]   Oh, it has numbers in it.
[01:50:09.460 --> 01:50:10.460]   So we think it's OK and we miss it.
[01:50:10.460 --> 01:50:11.460]   Right.
[01:50:11.460 --> 01:50:15.420]   It's a lot of numeracy and I think we're misusing it badly.
[01:50:15.420 --> 01:50:20.380]   Do you make your students read a math, petition, read the newspaper?
[01:50:20.380 --> 01:50:23.860]   I think one of the classes actually I think the business guy does that I think.
[01:50:23.860 --> 01:50:24.860]   Interesting.
[01:50:24.860 --> 01:50:25.860]   That's a good book.
[01:50:25.860 --> 01:50:26.860]   It is.
[01:50:26.860 --> 01:50:29.260]   I mean, it's tough, but very college.
[01:50:29.260 --> 01:50:35.780]   Kathy worked on the-- I'm sure you'll know this program, the lead program on data journalism.
[01:50:35.780 --> 01:50:40.380]   It's a joint effort of Columbia's School of Journalism and Department of Computer Science.
[01:50:40.380 --> 01:50:46.860]   And it's to give journalists the computational savvy to do this stuff, which is really--
[01:50:46.860 --> 01:50:47.860]   They do.
[01:50:47.860 --> 01:50:48.860]   I want to take that class.
[01:50:48.860 --> 01:50:50.860]   I just don't want to have it to be dark.
[01:50:50.860 --> 01:50:51.860]   I know.
[01:50:51.860 --> 01:50:56.100]   I wish someone would do that in UT.
[01:50:56.100 --> 01:50:57.100]   You should.
[01:50:57.100 --> 01:51:02.260]   You're a great journalist, Stacy, and you really have that enterprise spirit of digging
[01:51:02.260 --> 01:51:04.620]   into a story.
[01:51:04.620 --> 01:51:05.620]   I think we--
[01:51:05.620 --> 01:51:06.620]   I just don't know math.
[01:51:06.620 --> 01:51:07.620]   My stats is terrible.
[01:51:07.620 --> 01:51:08.620]   Yeah.
[01:51:08.620 --> 01:51:09.620]   I failed at me.
[01:51:09.620 --> 01:51:12.460]   She started the lead program at Columbia.
[01:51:12.460 --> 01:51:15.620]   So this is definitely a book to read.
[01:51:15.620 --> 01:51:17.100]   Your number of the week, Mr. Charms.
[01:51:17.100 --> 01:51:19.420]   Well, I guess I'll give you two of them.
[01:51:19.420 --> 01:51:22.060]   One of them is-- it was about a 3D per hour a minute ago.
[01:51:22.060 --> 01:51:29.380]   My friend Bill Gross is a 3D printer company, which is what is it called?
[01:51:29.380 --> 01:51:30.380]   What is it called?
[01:51:30.380 --> 01:51:31.380]   New Matter.
[01:51:31.380 --> 01:51:32.820]   They have a Black Friday sale on.
[01:51:32.820 --> 01:51:34.340]   I just saw this in my email come across.
[01:51:34.340 --> 01:51:37.540]   I got it for $2.99 because that crowd funded.
[01:51:37.540 --> 01:51:39.900]   It's $3.99, but it's going back to $2.99 on Black Friday.
[01:51:39.900 --> 01:51:44.580]   So you get the mod T, which is a really cool black little printer.
[01:51:44.580 --> 01:51:47.460]   And then the other thing was, have you gotten a $15 check from Facebook?
[01:51:47.460 --> 01:51:48.460]   No.
[01:51:48.460 --> 01:51:49.460]   Am I getting one?
[01:51:49.460 --> 01:51:50.460]   Well, you might.
[01:51:50.460 --> 01:51:52.460]   It's part of the settlement that Facebook got for us.
[01:51:52.460 --> 01:51:54.460]   Do I have to give them my actual address?
[01:51:54.460 --> 01:51:56.460]   Ill-fated advertising campaign.
[01:51:56.460 --> 01:52:00.340]   So I wonder who-- you remember that horrible ad structure that was called?
[01:52:00.340 --> 01:52:02.340]   Oh, I forget the name of it.
[01:52:02.340 --> 01:52:03.340]   Isn't there?
[01:52:03.340 --> 01:52:04.340]   Where they--
[01:52:04.340 --> 01:52:05.340]   Oh.
[01:52:05.340 --> 01:52:06.340]   Oh.
[01:52:06.340 --> 01:52:07.340]   I endorse stuff.
[01:52:07.340 --> 01:52:11.220]   I was-- I complained because they were using my image to do ads to my--
[01:52:11.220 --> 01:52:13.540]   Well, you should be getting $15.
[01:52:13.540 --> 01:52:19.700]   Because if you were used in those ads, then you're supposed to get a $15 settlement.
[01:52:19.700 --> 01:52:20.700]   Very good.
[01:52:20.700 --> 01:52:21.700]   $15.
[01:52:21.700 --> 01:52:22.700]   Wow.
[01:52:22.700 --> 01:52:24.700]   That might give me a sandwich down the road.
[01:52:24.700 --> 01:52:26.020]   I think-- hey, how much is the book?
[01:52:26.020 --> 01:52:27.020]   Hold on.
[01:52:27.020 --> 01:52:28.020]   Wait a minute.
[01:52:28.020 --> 01:52:29.020]   No, it's more than $15.
[01:52:29.020 --> 01:52:30.020]   No, it's $15.85.
[01:52:30.020 --> 01:52:31.020]   There you go.
[01:52:31.020 --> 01:52:32.020]   Oh, there.
[01:52:32.020 --> 01:52:33.020]   Oh.
[01:52:33.020 --> 01:52:34.020]   Yeah, close.
[01:52:34.020 --> 01:52:39.540]   Facebook's Filthy Luka to learn something about big data and how Facebook's abusing it.
[01:52:39.540 --> 01:52:41.780]   Yes, the Kindle version is $13.99.
[01:52:41.780 --> 01:52:42.780]   So you too.
[01:52:42.780 --> 01:52:43.780]   Keep a bunch.
[01:52:43.780 --> 01:52:45.780]   There you go.
[01:52:45.780 --> 01:52:50.580]   My pick of the week is a longstanding-- and I've mentioned it before-- a to-do program
[01:52:50.580 --> 01:52:51.580]   called Any Do.
[01:52:51.580 --> 01:52:53.940]   A lot of people use it on iOS and Android.
[01:52:53.940 --> 01:52:57.380]   But the reason I'm very interested all of a sudden is it suddenly is integrated with
[01:52:57.380 --> 01:52:58.380]   Amazon's Echo.
[01:52:58.380 --> 01:52:59.380]   I've always--
[01:52:59.380 --> 01:53:00.380]   Oh.
[01:53:00.380 --> 01:53:01.380]   Yes.
[01:53:01.380 --> 01:53:03.660]   I've always wanted to be able to manage my to-do list from the Echo.
[01:53:03.660 --> 01:53:07.740]   You could, but it was only the Alexa to-do list and the Alexa shopping list.
[01:53:07.740 --> 01:53:10.180]   Well, now that's integrated into Any Do.
[01:53:10.180 --> 01:53:12.020]   It's not even an Alexa-- Alexa task.
[01:53:12.020 --> 01:53:14.460]   It's just Any Do will support it.
[01:53:14.460 --> 01:53:20.780]   And so you can now use any of your Any Do lists with your Amazon Echo and remind yourself
[01:53:20.780 --> 01:53:21.780]   to do things.
[01:53:21.780 --> 01:53:25.180]   And frankly, that's, to me, the easiest way to do it because it's just-- I'm right there
[01:53:25.180 --> 01:53:27.140]   and I can say it and do it.
[01:53:27.140 --> 01:53:34.260]   You could also actually using If This and That, send your Echo list to the Evernote.
[01:53:34.260 --> 01:53:35.260]   Yeah.
[01:53:35.260 --> 01:53:36.260]   To-ever, sorry.
[01:53:36.260 --> 01:53:39.780]   You could in the days when you can do stuff with If This and That.
[01:53:39.780 --> 01:53:42.740]   Oh, oh, so did I need to correct that actually?
[01:53:42.740 --> 01:53:44.660]   Because I found out you-- it's still there.
[01:53:44.660 --> 01:53:47.380]   You can actually make your-- you can actually make your recipes.
[01:53:47.380 --> 01:53:48.380]   You can.
[01:53:48.380 --> 01:53:49.380]   You can.
[01:53:49.380 --> 01:53:51.820]   Even though they turn them all into applets.
[01:53:51.820 --> 01:53:54.980]   They're called applets now, but you actually can go back-- I thought I did that.
[01:53:54.980 --> 01:53:55.980]   I'm sorry.
[01:53:55.980 --> 01:53:58.300]   Yes, you can still make your own recipes.
[01:53:58.300 --> 01:54:00.460]   Well, that would be nice.
[01:54:00.460 --> 01:54:05.260]   So because actually Kevin and I were playing with using the Google Home to control more
[01:54:05.260 --> 01:54:09.260]   stuff through If This and That, because there's a Google Personal Assistant channel on If
[01:54:09.260 --> 01:54:10.500]   This and That.
[01:54:10.500 --> 01:54:15.980]   So you can actually connect it to things that aren't actually connected to the home.
[01:54:15.980 --> 01:54:16.980]   Good.
[01:54:16.980 --> 01:54:17.980]   But there is like a one-second latency.
[01:54:17.980 --> 01:54:18.980]   Good.
[01:54:18.980 --> 01:54:21.780]   Well, if you don't want to create an If This and That script, just use Any Do.
[01:54:21.780 --> 01:54:22.780]   It's free.
[01:54:22.780 --> 01:54:26.620]   It's a free version, but the free version works perfectly well with the Amazon Echo.
[01:54:26.620 --> 01:54:33.500]   And I do like being able to say, you know, add forks to my shopping list or remind me
[01:54:33.500 --> 01:54:37.900]   to buy a Christmas present for at least a few of my reminders and to-dos.
[01:54:37.900 --> 01:54:38.900]   And that's very useful.
[01:54:38.900 --> 01:54:41.060]   That's very useful.
[01:54:41.060 --> 01:54:44.180]   Ladies and gentlemen, the end of the show is nigh.
[01:54:44.180 --> 01:54:45.780]   We thank you all for being here.
[01:54:45.780 --> 01:54:46.780]   Thanks to Stacey Higginbotham.
[01:54:46.780 --> 01:54:51.700]   You'll find her at Stacey on IOT.com and IOTpodcast.com.
[01:54:51.700 --> 01:54:58.180]   Get the newest IOT podcast with Kevin Tofel for your Black Friday IOT deals.
[01:54:58.180 --> 01:55:01.860]   Have a wonderful Thanksgiving in Houston.
[01:55:01.860 --> 01:55:03.660]   Oh, I will.
[01:55:03.660 --> 01:55:04.660]   Yay.
[01:55:04.660 --> 01:55:06.100]   In her childhood bedroom.
[01:55:06.100 --> 01:55:08.500]   Isn't that sweet?
[01:55:08.500 --> 01:55:12.140]   Jeff Jarvis is a professor of journalism at the City University of New York, the author
[01:55:12.140 --> 01:55:19.180]   of many a wonderful tome, including geeks bearing gifts, reinventing the news biz.
[01:55:19.180 --> 01:55:22.100]   And of course he blogs regularly a buzz machine and medium.
[01:55:22.100 --> 01:55:25.420]   Oh, I was going to mention one other thing.
[01:55:25.420 --> 01:55:33.380]   Reminding me about medium is telegram has created a medium like kind of posting system
[01:55:33.380 --> 01:55:37.300]   called telegraph telegraph.ph.
[01:55:37.300 --> 01:55:42.220]   And all you have to do is go to telegraph.ph and start writing.
[01:55:42.220 --> 01:55:45.260]   And you can publish it on telegram or Facebook or whatever.
[01:55:45.260 --> 01:55:46.460]   And it's like instant articles.
[01:55:46.460 --> 01:55:48.980]   It pops up right away.
[01:55:48.980 --> 01:55:50.580]   Which is really cool.
[01:55:50.580 --> 01:55:52.140]   You can embed photos in here.
[01:55:52.140 --> 01:55:59.060]   You can, you know, it's weird because you just get a URL.
[01:55:59.060 --> 01:56:04.060]   Can you use Markdown?
[01:56:04.060 --> 01:56:08.940]   Shall I put a photo in?
[01:56:08.940 --> 01:56:09.940]   Let me put a photo in.
[01:56:09.940 --> 01:56:10.940]   Do I have one?
[01:56:10.940 --> 01:56:11.940]   There we go.
[01:56:11.940 --> 01:56:13.780]   That's a good one.
[01:56:13.780 --> 01:56:21.500]   And then publish it and now I'm going to have a dedicated URL right here that I can post.
[01:56:21.500 --> 01:56:27.740]   And it loads instantly in telegram, the messenger, a whole bunch of other things.
[01:56:27.740 --> 01:56:29.140]   It's an instant messaging.
[01:56:29.140 --> 01:56:34.740]   It's kind of like, you know, whatever that fate, that thing that Google did that I hated
[01:56:34.740 --> 01:56:35.740]   so much.
[01:56:35.740 --> 01:56:36.740]   How's that doing?
[01:56:36.740 --> 01:56:38.980]   It's taking the world by storm.
[01:56:38.980 --> 01:56:41.100]   Which Google thing is this?
[01:56:41.100 --> 01:56:42.100]   You know, ARP.
[01:56:42.100 --> 01:56:43.100]   What's it called?
[01:56:43.100 --> 01:56:44.100]   Jazz.
[01:56:44.100 --> 01:56:45.100]   Oh, no.
[01:56:45.100 --> 01:56:46.100]   Ampe.
[01:56:46.100 --> 01:56:47.100]   Ampe?
[01:56:47.100 --> 01:56:49.100]   I blocked it out.
[01:56:49.100 --> 01:56:50.100]   T-E-L-E.
[01:56:50.100 --> 01:56:54.300]   I was actually going to be my pick, but I'll give you two.
[01:56:54.300 --> 01:56:55.300]   Telegraph.
[01:56:55.300 --> 01:56:57.100]   T-E-L-G-R-A.PH.
[01:56:57.100 --> 01:56:59.260]   Happy Thanksgiving.
[01:56:59.260 --> 01:57:00.260]   Happy Thanksgiving.
[01:57:00.260 --> 01:57:02.340]   Two picks from Leo.
[01:57:02.340 --> 01:57:06.180]   Two picks from Leo.
[01:57:06.180 --> 01:57:10.780]   We do the show every Wednesday, 1.30 pm Pacific 4.30 pm Eastern.
[01:57:10.780 --> 01:57:17.340]   I think I've been getting the UTC times wrong.
[01:57:17.340 --> 01:57:20.340]   I think it's minus eight.
[01:57:20.340 --> 01:57:24.420]   So if it's 4.30 Eastern, which is 16.30, minus eight.
[01:57:24.420 --> 01:57:25.780]   You were saying about math?
[01:57:25.780 --> 01:57:27.780]   Is that 24.30?
[01:57:27.780 --> 01:57:30.420]   I'd say it's Pacific.
[01:57:30.420 --> 01:57:31.420]   Huh?
[01:57:31.420 --> 01:57:38.340]   Oh, 1.30 Pacific minus eight would be 9.30 or 21.30.
[01:57:38.340 --> 01:57:41.260]   Yeah, 21.30 UTC.
[01:57:41.260 --> 01:57:42.420]   That's what I've been saying.
[01:57:42.420 --> 01:57:45.140]   So I guess I'm right.
[01:57:45.140 --> 01:57:46.460]   I take it back.
[01:57:46.460 --> 01:57:49.340]   I do know mathematics.
[01:57:49.340 --> 01:57:51.260]   Time is hard.
[01:57:51.260 --> 01:57:54.100]   Join us live if you can in the chatroom at IRC.twit.tv.
[01:57:54.100 --> 01:57:57.300]   If you can't on demand audio and video is always available at our website.
[01:57:57.300 --> 01:58:05.580]   Twit.tv/twig or wherever you subscribe to your favorite shows.
[01:58:05.580 --> 01:58:07.780]   Thank you everyone for being here.
[01:58:07.780 --> 01:58:08.780]   We appreciate it.
[01:58:08.780 --> 01:58:12.180]   And we will see you next time on This Week in Google.
[01:58:12.180 --> 01:58:14.780]   Have a great Thanksgiving for those of you in the US.
[01:58:14.780 --> 01:58:21.780]   [MUSIC]
[01:58:21.780 --> 01:58:24.360]   (upbeat music)
[01:58:24.360 --> 01:58:45.360]   ♪♪♪

