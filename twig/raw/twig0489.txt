;FFMETADATA1
title=I'm An Engineer, Darn It!
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=489
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:02.600]   It's time for Twig this week in Google.
[00:00:02.600 --> 00:00:04.400]   The old gang is back.
[00:00:04.400 --> 00:00:05.800]   Stacey Higginbotham.
[00:00:05.800 --> 00:00:07.400]   Jeff Jarvis.
[00:00:07.400 --> 00:00:11.800]   We'll talk about, well, Facebook Google.
[00:00:11.800 --> 00:00:13.800]   I don't think we mentioned Twitter.
[00:00:13.800 --> 00:00:17.000]   And a new robot cafe in Japan.
[00:00:17.000 --> 00:00:19.000]   It's all coming up next on Twig.
[00:00:19.000 --> 00:00:24.800]   Netcast, you love.
[00:00:24.800 --> 00:00:26.800]   From people you trust.
[00:00:26.800 --> 00:00:30.440]   [MUSIC PLAYING]
[00:00:30.440 --> 00:00:31.800]   This is Twig.
[00:00:31.800 --> 00:00:38.800]   This is Twig this week in Google.
[00:00:38.800 --> 00:00:44.080]   Episode 489, recorded Wednesday, January 2, 2019.
[00:00:44.080 --> 00:00:46.920]   I'm an engineer, darn it.
[00:00:46.920 --> 00:00:51.040]   This week in Google is brought to you by Calm, the number one
[00:00:51.040 --> 00:00:54.440]   app to help you meditate, sleep, and relax.
[00:00:54.440 --> 00:00:57.760]   Start 2019 off right with Calm for a limited time
[00:00:57.760 --> 00:01:04.600]   get 25% off a Calm premium subscription at calm.com/twig
[00:01:04.600 --> 00:01:06.080]   and buy cash fly.
[00:01:06.080 --> 00:01:09.360]   Give your users the seamless online experience they want.
[00:01:09.360 --> 00:01:12.400]   Power your site or app with cash fly's CDN
[00:01:12.400 --> 00:01:15.200]   and be 30% faster than the competition.
[00:01:15.200 --> 00:01:20.040]   Learn more at twit.cashfly.com.
[00:01:20.040 --> 00:01:21.800]   It's time for Twig this week in Google.
[00:01:21.800 --> 00:01:24.600]   Show where yes, 2019 edition.
[00:01:24.600 --> 00:01:28.960]   Brand new for 2019 with all new hosts.
[00:01:28.960 --> 00:01:31.600]   I'm Leo LaPorte.
[00:01:31.600 --> 00:01:33.520]   That's where we find out Stacey.
[00:01:33.520 --> 00:01:34.440]   We're fired.
[00:01:34.440 --> 00:01:36.960]   Jeff Jarvis right now.
[00:01:36.960 --> 00:01:38.280]   Buzzmachine.com.
[00:01:38.280 --> 00:01:40.320]   No, it's just for people who haven't been watching for a while.
[00:01:40.320 --> 00:01:44.480]   And Stacey Higginbotham from Stacey on IOT.com.
[00:01:44.480 --> 00:01:47.840]   No, actually, it's the old hosts brought back.
[00:01:47.840 --> 00:01:48.800]   It's so great to have you.
[00:01:48.800 --> 00:01:49.800]   I missed you all.
[00:01:49.800 --> 00:01:51.560]   You're like disappointed, everybody.
[00:01:51.560 --> 00:01:52.560]   No, everybody's happy.
[00:01:52.560 --> 00:01:54.720]   Yeah, they're like, oh, new hosts.
[00:01:54.720 --> 00:01:55.480]   How exciting.
[00:01:55.480 --> 00:01:57.080]   And then, oh, it's just them.
[00:01:57.080 --> 00:01:58.360]   It's just them again.
[00:01:58.360 --> 00:01:58.760]   No.
[00:01:58.760 --> 00:02:01.640]   But I do have new resolutions that could really change
[00:02:01.640 --> 00:02:03.080]   who I am as a person.
[00:02:03.080 --> 00:02:04.240]   Do you really?
[00:02:04.240 --> 00:02:05.320]   No.
[00:02:05.320 --> 00:02:09.760]   My daughter is doing Con-Marie.
[00:02:09.760 --> 00:02:11.040]   Oh, I already do that.
[00:02:11.040 --> 00:02:12.080]   Yeah.
[00:02:12.080 --> 00:02:15.920]   Con-Marie, does it spark joy if not?
[00:02:15.920 --> 00:02:16.840]   Clean it, throw it out.
[00:02:16.840 --> 00:02:18.800]   Get rid of.
[00:02:18.800 --> 00:02:22.160]   Something magic of tidying up, the life-changing magic.
[00:02:22.160 --> 00:02:23.640]   Yeah.
[00:02:23.640 --> 00:02:26.640]   Why am I not surprised that you've done that?
[00:02:26.640 --> 00:02:27.560]   Well, no, I haven't done that.
[00:02:27.560 --> 00:02:28.920]   That's how I live my life.
[00:02:28.920 --> 00:02:30.560]   Oh, you didn't need Con-Marie.
[00:02:30.560 --> 00:02:32.000]   It's just who you are.
[00:02:32.000 --> 00:02:35.440]   So do you love my wife love the container store?
[00:02:35.440 --> 00:02:38.160]   You know, I used to, but now I don't have enough stuff
[00:02:38.160 --> 00:02:39.320]   to put my stuff in.
[00:02:39.320 --> 00:02:41.680]   That's what Marie Con-O says.
[00:02:41.680 --> 00:02:42.960]   It's not about containers.
[00:02:42.960 --> 00:02:45.120]   It's about getting rid of.
[00:02:45.120 --> 00:02:48.520]   I have long said my wife's going to bury me in Rubbermaid.
[00:02:48.520 --> 00:02:49.520]   No.
[00:02:49.520 --> 00:02:52.160]   I mean, I do have, and I can't do it because I have a kid.
[00:02:52.160 --> 00:02:56.120]   So, you know, sometimes I'm like, hey, Anna, let's play my favorite game.
[00:02:56.120 --> 00:02:59.040]   She's like, is it Keep-er-throwout?
[00:02:59.040 --> 00:03:03.400]   There is a tie in because Marie Con-O has a new show on Netflix.
[00:03:03.400 --> 00:03:04.400]   It is.
[00:03:04.400 --> 00:03:05.400]   Have you seen it?
[00:03:05.400 --> 00:03:06.400]   No, have you?
[00:03:06.400 --> 00:03:09.400]   No, I'm not really excited about watching people.
[00:03:09.400 --> 00:03:10.400]   Tie-yup.
[00:03:10.400 --> 00:03:11.400]   Perf stuff, I say.
[00:03:11.400 --> 00:03:12.400]   Yeah.
[00:03:12.400 --> 00:03:15.840]   It's perfect, though, for a new year resolution kind of, right?
[00:03:15.840 --> 00:03:16.840]   It is.
[00:03:16.840 --> 00:03:17.840]   Yes.
[00:03:17.840 --> 00:03:19.600]   Are these people like super hoarders?
[00:03:19.600 --> 00:03:20.600]   I mean...
[00:03:20.600 --> 00:03:21.600]   Or are they just...
[00:03:21.600 --> 00:03:24.680]   Or are they just the beginning of her book, which I am almost embarrassed to say I've
[00:03:24.680 --> 00:03:27.160]   read the life-changing magic of tidying up.
[00:03:27.160 --> 00:03:30.280]   She says that she's never had any regressions.
[00:03:30.280 --> 00:03:34.440]   No one has reverted to their old untidy lifestyle.
[00:03:34.440 --> 00:03:36.240]   I don't believe her.
[00:03:36.240 --> 00:03:38.360]   I just can't believe that.
[00:03:38.360 --> 00:03:41.600]   Except maybe that she just doesn't take you if you're going to be the type of person
[00:03:41.600 --> 00:03:43.120]   on my backslide on her.
[00:03:43.120 --> 00:03:49.720]   Jeff's now going to show us how tidy his office is.
[00:03:49.720 --> 00:03:52.400]   Jeff, you only have one remote.
[00:03:52.400 --> 00:03:53.400]   Slacker.
[00:03:53.400 --> 00:03:54.400]   Yeah.
[00:03:54.400 --> 00:03:56.680]   That's more like my office.
[00:03:56.680 --> 00:03:57.680]   I can barely...
[00:03:57.680 --> 00:04:00.240]   You have a lot of credenzas and stuff.
[00:04:00.240 --> 00:04:01.400]   Well, he's a professor.
[00:04:01.400 --> 00:04:02.400]   You have to have credenza.
[00:04:02.400 --> 00:04:03.400]   Oh, yeah.
[00:04:03.400 --> 00:04:04.400]   That's it.
[00:04:04.400 --> 00:04:05.400]   Okay.
[00:04:05.400 --> 00:04:06.400]   And then this is us, apparently.
[00:04:06.400 --> 00:04:08.320]   Jeff, we've lost your audio.
[00:04:08.320 --> 00:04:09.320]   No, really?
[00:04:09.320 --> 00:04:10.560]   Oh, he was just silent.
[00:04:10.560 --> 00:04:11.560]   It was a silent tour.
[00:04:11.560 --> 00:04:13.960]   You just used to be talking all the time.
[00:04:13.960 --> 00:04:14.960]   Yeah.
[00:04:14.960 --> 00:04:15.960]   Like they were shut up.
[00:04:15.960 --> 00:04:17.960]   Is that what you were saying?
[00:04:17.960 --> 00:04:18.960]   No!
[00:04:18.960 --> 00:04:20.280]   Did you guys have a good...
[00:04:20.280 --> 00:04:21.280]   Did you have a good New Year?
[00:04:21.280 --> 00:04:22.720]   Jeff, did you do anything fun?
[00:04:22.720 --> 00:04:24.200]   I fell asleep as early as I could.
[00:04:24.200 --> 00:04:25.200]   Yeah.
[00:04:25.200 --> 00:04:32.760]   I stayed up late watching a car wreck that CNN called New Year's Eve with...
[00:04:32.760 --> 00:04:33.760]   Oh.
[00:04:33.760 --> 00:04:34.760]   Oh.
[00:04:34.760 --> 00:04:35.760]   Cohen.
[00:04:35.760 --> 00:04:36.760]   Five minutes.
[00:04:36.760 --> 00:04:37.760]   Cohen and...
[00:04:37.760 --> 00:04:38.760]   Yeah.
[00:04:38.760 --> 00:04:40.760]   I've actually blanked their names out now.
[00:04:40.760 --> 00:04:42.560]   The guy with the white hair.
[00:04:42.560 --> 00:04:43.560]   Anderson Cooper.
[00:04:43.560 --> 00:04:44.560]   It's a Cooper.
[00:04:44.560 --> 00:04:45.560]   And Michael Cohen.
[00:04:45.560 --> 00:04:46.560]   Andy Cohen.
[00:04:46.560 --> 00:04:47.560]   Andy Cohen.
[00:04:47.560 --> 00:04:49.560]   I just was back.
[00:04:49.560 --> 00:04:50.560]   It was painful.
[00:04:50.560 --> 00:04:51.960]   It was painful.
[00:04:51.960 --> 00:04:52.960]   It was.
[00:04:52.960 --> 00:04:53.960]   Mm-hmm.
[00:04:53.960 --> 00:04:56.320]   And what did you do for New Year's Eve?
[00:04:56.320 --> 00:04:57.320]   Stacey?
[00:04:57.320 --> 00:05:03.200]   I went to a friend's house and we watched the ball drop at 11 o'clock our time and at midnight
[00:05:03.200 --> 00:05:05.760]   our time and we played games.
[00:05:05.760 --> 00:05:06.760]   What?
[00:05:06.760 --> 00:05:12.840]   The game we used to play in Ohio while we were drinking ginger ale and eating ham salad,
[00:05:12.840 --> 00:05:17.000]   which wasn't really ham salad made with bologna, was spit.
[00:05:17.000 --> 00:05:18.720]   Oh, you know what?
[00:05:18.720 --> 00:05:20.440]   I used to be a spit champion.
[00:05:20.440 --> 00:05:21.440]   The card game?
[00:05:21.440 --> 00:05:22.440]   Were you like flip out the cards?
[00:05:22.440 --> 00:05:23.440]   Yeah.
[00:05:23.440 --> 00:05:24.440]   Yeah.
[00:05:24.440 --> 00:05:25.440]   That's a good game.
[00:05:25.440 --> 00:05:26.440]   It's a good game.
[00:05:26.440 --> 00:05:27.440]   I did that.
[00:05:27.440 --> 00:05:28.440]   I did that.
[00:05:28.440 --> 00:05:29.440]   Good New Year's game.
[00:05:29.440 --> 00:05:30.440]   Yeah.
[00:05:30.440 --> 00:05:31.440]   Oh, yeah.
[00:05:31.440 --> 00:05:32.440]   That would be good if you're drinking.
[00:05:32.440 --> 00:05:33.440]   Yeah.
[00:05:33.440 --> 00:05:34.440]   No, we used to play it at before school and middle school.
[00:05:34.440 --> 00:05:35.440]   Well, ginger ale we were drinking.
[00:05:35.440 --> 00:05:36.440]   Yes.
[00:05:36.440 --> 00:05:40.520]   I'm a small town in Ohio just over the border from Indiana called convoy.
[00:05:40.520 --> 00:05:41.800]   It didn't have a liquor store.
[00:05:41.800 --> 00:05:44.000]   It had one light.
[00:05:44.000 --> 00:05:47.080]   And I remember going to have New Year's Eve with her family.
[00:05:47.080 --> 00:05:53.880]   They were very, they were Lutherans and ginger ale, ham salad and spit till midnight.
[00:05:53.880 --> 00:05:58.680]   Then you go out in the backyard, bang the pots and pans and you go to bed.
[00:05:58.680 --> 00:05:59.880]   Wow.
[00:05:59.880 --> 00:06:01.380]   Yeah.
[00:06:01.380 --> 00:06:02.380]   We did.
[00:06:02.380 --> 00:06:05.960]   So a friend of mine is a wedding planner and she was there and she brought wedding sparklers,
[00:06:05.960 --> 00:06:10.120]   which you guys leg up on normal sparklers.
[00:06:10.120 --> 00:06:11.760]   I've got to tell you, they're metal.
[00:06:11.760 --> 00:06:13.560]   They burn for like two minutes.
[00:06:13.560 --> 00:06:14.560]   You can't get those.
[00:06:14.560 --> 00:06:15.560]   Very good sparklers.
[00:06:15.560 --> 00:06:18.920]   This is sparklers I grew up with, but you can't get those anymore because they're not safe
[00:06:18.920 --> 00:06:19.920]   and sane.
[00:06:19.920 --> 00:06:23.480]   Oh, well, I need wedding sparklers.
[00:06:23.480 --> 00:06:24.480]   Wedding sparklers.
[00:06:24.480 --> 00:06:25.480]   They were very pretty.
[00:06:25.480 --> 00:06:26.480]   Interesting.
[00:06:26.480 --> 00:06:27.480]   We enjoyed them.
[00:06:27.480 --> 00:06:28.480]   Yeah.
[00:06:28.480 --> 00:06:29.480]   Those are the old magnesium.
[00:06:29.480 --> 00:06:30.960]   Did you have any queso?
[00:06:30.960 --> 00:06:33.160]   Yes, I did actually.
[00:06:33.160 --> 00:06:35.360]   We had the he does for New Year's dinner.
[00:06:35.360 --> 00:06:37.040]   Yeah, fajito's queso.
[00:06:37.040 --> 00:06:38.040]   Wow.
[00:06:38.040 --> 00:06:40.200]   That's an awesome new year.
[00:06:40.200 --> 00:06:42.680]   I won't be having those for much longer.
[00:06:42.680 --> 00:06:43.680]   I know.
[00:06:43.680 --> 00:06:45.120]   So what you're giving up, what's the plan?
[00:06:45.120 --> 00:06:46.680]   Where are you moving?
[00:06:46.680 --> 00:06:47.680]   June.
[00:06:47.680 --> 00:06:48.680]   Six more months.
[00:06:48.680 --> 00:06:51.640]   And then it's a beautiful tree.
[00:06:51.640 --> 00:06:53.640]   It's so free.
[00:06:53.640 --> 00:06:57.680]   We are selling and we will rent for the first year.
[00:06:57.680 --> 00:06:58.680]   Yeah.
[00:06:58.680 --> 00:06:59.680]   Very smart.
[00:06:59.680 --> 00:07:00.680]   Get to know the neighborhood.
[00:07:00.680 --> 00:07:01.680]   Smart people.
[00:07:01.680 --> 00:07:02.680]   Yeah.
[00:07:02.680 --> 00:07:09.000]   So Bleep Blurps says in Austin, we can only describe sparklers by law.
[00:07:09.000 --> 00:07:12.800]   All right.
[00:07:12.800 --> 00:07:17.240]   I'm stalling because I don't know if there's news.
[00:07:17.240 --> 00:07:18.240]   There is.
[00:07:18.240 --> 00:07:19.240]   You already got it.
[00:07:19.240 --> 00:07:20.240]   There is news.
[00:07:20.240 --> 00:07:22.240]   It looks like.
[00:07:22.240 --> 00:07:26.040]   Zach Whittaker has the headline of the year.
[00:07:26.040 --> 00:07:31.640]   Zuckerberg, Mark Zuckerberg posted something.
[00:07:31.640 --> 00:07:39.040]   This is not the article, but Mark Zuckerberg posted something about what a tough year it
[00:07:39.040 --> 00:07:48.040]   was and how much we're going to work harder next year and all of that year end overview.
[00:07:48.040 --> 00:07:51.560]   And Zach Whittaker had a great headline.
[00:07:51.560 --> 00:07:56.820]   He said, Zuckerberg's tone deaf year end note reads like a thousand words of patting himself
[00:07:56.820 --> 00:08:02.260]   on the back and shows little contrition or empathy for the harm Facebook caused.
[00:08:02.260 --> 00:08:05.900]   I think empathy is something difficult for Zuckerberg.
[00:08:05.900 --> 00:08:06.900]   Yeah.
[00:08:06.900 --> 00:08:07.900]   I truly do.
[00:08:07.900 --> 00:08:08.900]   Yeah.
[00:08:08.900 --> 00:08:09.900]   I think he's also a true.
[00:08:09.900 --> 00:08:15.100]   I think he's a true believer is I think part of the problem.
[00:08:15.100 --> 00:08:19.820]   Like he believes so much in the value that Facebook's adding to the world.
[00:08:19.820 --> 00:08:21.820]   It's hard for him to see any harm.
[00:08:21.820 --> 00:08:22.820]   Yes.
[00:08:22.820 --> 00:08:25.020]   That's where the empathy comes in.
[00:08:25.020 --> 00:08:26.020]   Yeah.
[00:08:26.020 --> 00:08:27.020]   Yeah.
[00:08:27.020 --> 00:08:28.020]   You're right.
[00:08:28.020 --> 00:08:29.020]   Yeah.
[00:08:29.020 --> 00:08:34.020]   Though I also think that he's dated driven and this goes back to his original comment on
[00:08:34.020 --> 00:08:36.060]   the Russian stuff and pooping it.
[00:08:36.060 --> 00:08:41.700]   And then if you look at the different interpretations of the study that the Senate did, there's a
[00:08:41.700 --> 00:08:45.460]   very good story I think in the nation full disclosure as I raised money for my school
[00:08:45.460 --> 00:08:46.460]   from Facebook.
[00:08:46.460 --> 00:08:48.900]   I'm independent of Facebook and I received a money person from Facebook already have
[00:08:48.900 --> 00:08:50.060]   platforms and a disclosure.
[00:08:50.060 --> 00:08:55.460]   But the nation story I think was very good saying this Russian stuff was actually Penny
[00:08:55.460 --> 00:09:01.300]   Annie minor and nothing and we're acting as if they took over the election and the
[00:09:01.300 --> 00:09:03.140]   papers are trying to present it that way.
[00:09:03.140 --> 00:09:06.940]   But he said the data just doesn't back it up and Zuckerberg is data driven.
[00:09:06.940 --> 00:09:11.180]   And I think the problem he goes back to is but the data says nothing really happened
[00:09:11.180 --> 00:09:17.900]   but he cannot say that because what he does, he gets smashed because people are saying no
[00:09:17.900 --> 00:09:22.820]   we expected to be can trade and he wants to say but the data.
[00:09:22.820 --> 00:09:24.100]   But the data.
[00:09:24.100 --> 00:09:25.100]   But the data.
[00:09:25.100 --> 00:09:28.220]   He wrote it a piece on medium.
[00:09:28.220 --> 00:09:33.220]   I kind of had to write something because somebody attacked me because I wasn't attacking Facebook
[00:09:33.220 --> 00:09:35.660]   so it was based on that New York Times story.
[00:09:35.660 --> 00:09:39.780]   But the thing was, perhaps was not the best opportunity as I said the beginning I want
[00:09:39.780 --> 00:09:42.980]   to invade against Facebook but this is not the opportunity because the New York Times
[00:09:42.980 --> 00:09:50.180]   story about Facebook turned out to be a load of hui all at all, not entirely.
[00:09:50.180 --> 00:09:54.100]   But when they complained that oh my God, Netflix is reading your messages.
[00:09:54.100 --> 00:09:59.260]   Yes, because Netflix had read, write and delete access so you could do messages from Netflix.
[00:09:59.260 --> 00:10:02.060]   That's how the works.
[00:10:02.060 --> 00:10:05.180]   And so the Times made a big deal on this big deal.
[00:10:05.180 --> 00:10:14.220]   And in this case, Alex Stamos, who was ex Facebook but I think it's been a honest broker.
[00:10:14.220 --> 00:10:17.060]   There's chief information security.
[00:10:17.060 --> 00:10:19.860]   Said this is the way things are supposed to operate.
[00:10:19.860 --> 00:10:20.860]   We want them to open up.
[00:10:20.860 --> 00:10:22.220]   We want them to do this and go in hell.
[00:10:22.220 --> 00:10:27.140]   Jim's ball, who's a really tough British journalist, said that this is the hill he's
[00:10:27.140 --> 00:10:32.340]   going to die on but sharing the content of messages was what makes messaging work.
[00:10:32.340 --> 00:10:37.580]   So it was bad reporting from the Times.
[00:10:37.580 --> 00:10:41.220]   And the problem, it's a standard media thing you've heard me do it before but I kind of
[00:10:41.220 --> 00:10:48.900]   went off and I said, I think what media wants out of Facebook is blaring, they ruined everybody's
[00:10:48.900 --> 00:10:49.900]   privacy.
[00:10:49.900 --> 00:10:51.540]   They ruined democracy.
[00:10:51.540 --> 00:10:52.540]   Facebook has problems.
[00:10:52.540 --> 00:10:53.540]   Those aren't the problems.
[00:10:53.540 --> 00:10:57.420]   I think Facebook problems are actually a little bit more second if I could.
[00:10:57.420 --> 00:11:02.060]   A little more subtle and nuanced and thus more difficult.
[00:11:02.060 --> 00:11:04.580]   Facebook has cultural problems.
[00:11:04.580 --> 00:11:08.780]   And part of what you said a minute ago, Leo was the optimism piece is what gets them
[00:11:08.780 --> 00:11:12.820]   in trouble is they didn't anticipate and they didn't deal with manipulation.
[00:11:12.820 --> 00:11:14.780]   Thus they didn't protect us from it.
[00:11:14.780 --> 00:11:16.660]   Their opacity is a problem.
[00:11:16.660 --> 00:11:18.260]   Their secrecy is a problem.
[00:11:18.260 --> 00:11:22.860]   Their belief that they can do some crappy things like go after George Soros in private is a
[00:11:22.860 --> 00:11:24.500]   big honking problem.
[00:11:24.500 --> 00:11:26.460]   So Facebook has problems.
[00:11:26.460 --> 00:11:27.460]   Let there be no doubt.
[00:11:27.460 --> 00:11:31.140]   But all I'm trying to say is I think media are trying to go after the wrong problems and
[00:11:31.140 --> 00:11:33.140]   the simplistic problems in the wrong way.
[00:11:33.140 --> 00:11:35.140]   Sorry, Stacey, I'm done now.
[00:11:35.140 --> 00:11:40.460]   Don't know, I'm going to just tweak this a little bit and suggest that this may not
[00:11:40.460 --> 00:11:43.780]   be a load of who we and it may not be.
[00:11:43.780 --> 00:11:48.060]   The New York Times is trying and you see it in several of its tech focus stories.
[00:11:48.060 --> 00:11:54.060]   So imagine the editor there with this worldview that they're trying to educate people about
[00:11:54.060 --> 00:11:58.140]   how they're sharing their data and what that really means because the tech companies aren't
[00:11:58.140 --> 00:11:59.140]   doing it.
[00:11:59.140 --> 00:12:01.980]   So you saw that with their location sharing coverage.
[00:12:01.980 --> 00:12:03.980]   You saw that with Cambridge Analytica.
[00:12:03.980 --> 00:12:07.580]   And honestly, when I look at their coverage of Facebook about the data sharing they had
[00:12:07.580 --> 00:12:10.540]   with Spotify and Netflix, that's how I read this.
[00:12:10.540 --> 00:12:14.260]   I didn't read this so much as Facebook, SuperEvil.
[00:12:14.260 --> 00:12:20.260]   I read it more as, hey, you've been giving these permissions to Facebook and to these
[00:12:20.260 --> 00:12:22.260]   other companies in a way.
[00:12:22.260 --> 00:12:25.420]   No, I'm just smart and you just knew how to read it.
[00:12:25.420 --> 00:12:28.660]   The way I think most of the way I read it and the way that all the tweets came out and
[00:12:28.660 --> 00:12:32.540]   the coverage came out and the TV coverage came out is, oh my God, Facebook's letting Netflix
[00:12:32.540 --> 00:12:34.700]   read your messages.
[00:12:34.700 --> 00:12:36.180]   Yes.
[00:12:36.180 --> 00:12:43.620]   And I think I'm not saying that the coverage was a little.
[00:12:43.620 --> 00:12:47.180]   Let me read the headline in the sun.
[00:12:47.180 --> 00:12:48.180]   Sensationalistic.
[00:12:48.180 --> 00:12:52.380]   I just think that that's what we should expect from the New York Times because they're really
[00:12:52.380 --> 00:12:54.260]   trying to hammer this home.
[00:12:54.260 --> 00:12:58.940]   And before Leo goes on, I really think it's good that they're trying to educate people
[00:12:58.940 --> 00:12:59.940]   about this.
[00:12:59.940 --> 00:13:03.940]   Well, there's also a certain irony in the article because they say the New York Times
[00:13:03.940 --> 00:13:11.140]   is one of the groups of recipients of this large S. The headline, which admittedly was
[00:13:11.140 --> 00:13:15.900]   probably not written by the authors, as Facebook raised a privacy wall, it carved an opening
[00:13:15.900 --> 00:13:17.420]   for tech giants.
[00:13:17.420 --> 00:13:22.020]   The subhead internal documents show the social network gave Microsoft, Amazon, Spotify and
[00:13:22.020 --> 00:13:27.260]   others far greater access to people's data than it has disclosed.
[00:13:27.260 --> 00:13:29.060]   That's who we, that's what I call who we.
[00:13:29.060 --> 00:13:31.100]   That's a journalistic term of journalism school.
[00:13:31.100 --> 00:13:32.100]   Who we?
[00:13:32.100 --> 00:13:36.420]   And I think the point, and we actually made this point on the new screensavers a couple
[00:13:36.420 --> 00:13:43.260]   of weeks ago, is that in every case, these permissions were granted because these programs
[00:13:43.260 --> 00:13:47.660]   needed those permissions to do what you wanted them to do, Spotify needed access to your
[00:13:47.660 --> 00:13:48.660]   direct messages.
[00:13:48.660 --> 00:13:52.180]   So it could send a direct message with what you were listening to on Spotify.
[00:13:52.180 --> 00:13:53.180]   That's part of it.
[00:13:53.180 --> 00:13:54.780]   That was the feature people were turning on.
[00:13:54.780 --> 00:13:58.380]   And allow you to delete that message from Spotify if you wished.
[00:13:58.380 --> 00:13:59.860]   Yeah.
[00:13:59.860 --> 00:14:05.580]   So there is a larger question, which is Facebook neglected to turn off those APIs until this
[00:14:05.580 --> 00:14:07.460]   article appeared.
[00:14:07.460 --> 00:14:14.660]   So they didn't take it as seriously as maybe they should have, even though they turned those
[00:14:14.660 --> 00:14:19.060]   features, they say they turned those features off for those companies.
[00:14:19.060 --> 00:14:23.460]   2017, 2017.
[00:14:23.460 --> 00:14:27.540]   But the fact the API continued till the New York Times article came out.
[00:14:27.540 --> 00:14:28.540]   Right.
[00:14:28.540 --> 00:14:30.860]   So Facebook's problem in this case was not, they're sharing your data.
[00:14:30.860 --> 00:14:34.580]   Facebook's problem is they were being sloppy and they weren't caring enough.
[00:14:34.580 --> 00:14:35.580]   They're sloppy.
[00:14:35.580 --> 00:14:37.220]   But I think that that's the lesson.
[00:14:37.220 --> 00:14:40.100]   And I think Stacy, if there's a lesson that the New York Times is trying to teach, that
[00:14:40.100 --> 00:14:43.820]   is an important lesson is that Facebook is sloppy with your data.
[00:14:43.820 --> 00:14:45.340]   Oh, yes.
[00:14:45.340 --> 00:14:49.300]   And that is too, that is, most of these companies are sloppy with your data.
[00:14:49.300 --> 00:14:51.820]   All of them are, yes.
[00:14:51.820 --> 00:14:53.740]   They don't store it in any reasonable manner.
[00:14:53.740 --> 00:14:55.060]   They don't encrypt it.
[00:14:55.060 --> 00:14:56.820]   They stick it on public databases.
[00:14:56.820 --> 00:14:58.500]   Is that going to change this year?
[00:14:58.500 --> 00:15:05.260]   Is this going to be the year that governments force this and companies in the face of potential
[00:15:05.260 --> 00:15:09.180]   government regulation in the US start paying better attention to this?
[00:15:09.180 --> 00:15:12.340]   Yes, companies are already starting to pay better attention to it.
[00:15:12.340 --> 00:15:15.420]   They're getting genuine costs associated with it.
[00:15:15.420 --> 00:15:16.980]   So yes, we're going to see that.
[00:15:16.980 --> 00:15:22.060]   And I think it's guaranteed that we're going to get some sort of privacy law.
[00:15:22.060 --> 00:15:27.220]   I hate saying that because I feel like our political situation is so unhinged at the moment,
[00:15:27.220 --> 00:15:30.020]   but I do actually think it's going to happen.
[00:15:30.020 --> 00:15:31.860]   I don't think it's like a great law.
[00:15:31.860 --> 00:15:37.900]   I do this will cause some service in the trio here.
[00:15:37.900 --> 00:15:41.300]   But so I had a reaction that probably you won't share to the New York Times story about
[00:15:41.300 --> 00:15:43.980]   Facebook and suicide.
[00:15:43.980 --> 00:15:48.740]   And there's been a few stories about that last week that Facebook, we've talked about
[00:15:48.740 --> 00:15:53.380]   as I showed before, has the ability and it's and they've talked about this very openly
[00:15:53.380 --> 00:16:02.380]   that they have the ability to track warnings of suicide and in some cases specific intentions.
[00:16:02.380 --> 00:16:07.060]   And they try to call authorities as soon as they can on this.
[00:16:07.060 --> 00:16:08.860]   And this is always perfect.
[00:16:08.860 --> 00:16:13.500]   It can be too late and they can be wrong, but they're trying and they see this is a positive
[00:16:13.500 --> 00:16:14.500]   way.
[00:16:14.500 --> 00:16:15.500]   Guess what?
[00:16:15.500 --> 00:16:16.500]   This works in the US.
[00:16:16.500 --> 00:16:17.500]   It works in a lot of countries.
[00:16:17.500 --> 00:16:20.460]   Europe is a GDPR.
[00:16:20.460 --> 00:16:22.060]   And I said this on Twitter.
[00:16:22.060 --> 00:16:27.900]   I said this on Twitter and somebody went after me the same person went after me about Facebook
[00:16:27.900 --> 00:16:33.180]   and you're leaving off by the way, Jeff, a fairly huge part of this, which is that when
[00:16:33.180 --> 00:16:40.640]   Facebook gets it wrong, it could cause unintentional harm like precipitating suicide or compelling
[00:16:40.640 --> 00:16:45.820]   non-suicidal people to undergo psychiatric evaluations or prompting arrests or shootings.
[00:16:45.820 --> 00:16:50.580]   But unless it's 100% perfect, it shouldn't be doing this.
[00:16:50.580 --> 00:16:57.660]   I know you're never going to be 100% perfect with anything Leo, but if one person is saved,
[00:16:57.660 --> 00:17:00.380]   their life is saved, that has value.
[00:17:00.380 --> 00:17:03.580]   I think the point we should take away from this.
[00:17:03.580 --> 00:17:05.180]   So GDPR, yes.
[00:17:05.180 --> 00:17:12.140]   I think if we imagine Facebook as someone who actually cares about people and preventing
[00:17:12.140 --> 00:17:17.740]   suicide, then making a mistake feels warranted.
[00:17:17.740 --> 00:17:24.460]   If we look at this as Facebook trying to burnish its reputation and using data and only data
[00:17:24.460 --> 00:17:29.340]   without really caring what happens, which has historically been how it approaches these
[00:17:29.340 --> 00:17:33.060]   things, that isn't okay.
[00:17:33.060 --> 00:17:39.220]   And I think motivations in this case matter because we are people and Facebook is-
[00:17:39.220 --> 00:17:42.620]   >> Facebook is trying to stop suicide be anything that's cynical.
[00:17:42.620 --> 00:17:48.300]   >> Because if you are trying to stop sin, because if you fail, the repercussions are
[00:17:48.300 --> 00:17:49.300]   big.
[00:17:49.300 --> 00:17:52.660]   So I think it's important, like if I think someone is suicidal, I as a person who cares
[00:17:52.660 --> 00:17:59.180]   about them, and I step in and I'm wrong, the person at least understands my motivation
[00:17:59.180 --> 00:18:07.020]   and the consequences are usually far less because I go in trying to mitigate the fact
[00:18:07.020 --> 00:18:08.620]   that I might be wrong.
[00:18:08.620 --> 00:18:15.420]   If you go in saying my data is infallible and I am going to stop this thing from happening,
[00:18:15.420 --> 00:18:17.260]   you're not coming from a place of caring.
[00:18:17.260 --> 00:18:19.700]   So people are less likely to forgive you, right?
[00:18:19.700 --> 00:18:20.700]   So that's one.
[00:18:20.700 --> 00:18:22.540]   >> That's not that it is infallible.
[00:18:22.540 --> 00:18:24.260]   And turn it around Stacy.
[00:18:24.260 --> 00:18:27.980]   So imagine the New York Times story.
[00:18:27.980 --> 00:18:31.900]   Facebook has data that could indicate suicides, but they choose to do nothing because of
[00:18:31.900 --> 00:18:35.340]   the fear of PR backlash.
[00:18:35.340 --> 00:18:37.500]   People would attack them.
[00:18:37.500 --> 00:18:38.980]   Bit of a no win here.
[00:18:38.980 --> 00:18:41.700]   At some point, I think you've got a balance.
[00:18:41.700 --> 00:18:44.420]   I mean, yeah, if they're wrong all the time, that's screwed up.
[00:18:44.420 --> 00:18:49.540]   >> But we don't know because Facebook said that for privacy reasons, it doesn't track
[00:18:49.540 --> 00:18:52.100]   the outcomes of its calls to the police.
[00:18:52.100 --> 00:18:56.260]   It also has not disclosed how reviewers decide whether to call emergency responders.
[00:18:56.260 --> 00:18:57.260]   So there's no review.
[00:18:57.260 --> 00:19:00.140]   There should be at least peer review.
[00:19:00.140 --> 00:19:06.620]   >> Even better is if Facebook said, hey, you know what?
[00:19:06.620 --> 00:19:08.860]   We have this data.
[00:19:08.860 --> 00:19:16.140]   Let's use this to have a public conversation about preventing suicide in what we as a society
[00:19:16.140 --> 00:19:18.500]   in the US, for example, and in other countries.
[00:19:18.500 --> 00:19:23.860]   They have the power to start those conversations because in a lot of cases calling the police
[00:19:23.860 --> 00:19:27.340]   is not the right way to deal with the suicide, right?
[00:19:27.340 --> 00:19:33.100]   So instead of coming in and being like, the data says this, we're going to try to take
[00:19:33.100 --> 00:19:34.100]   action.
[00:19:34.100 --> 00:19:35.100]   Let's have a conversation.
[00:19:35.100 --> 00:19:40.060]   Silicon Valley is terrible at having conversations with people in organizations because they think
[00:19:40.060 --> 00:19:42.540]   they know because they have data.
[00:19:42.540 --> 00:19:48.060]   And it's hard to have a hard, meaty policy conversation about this.
[00:19:48.060 --> 00:19:52.020]   But imagine the headline of Facebook says, and they're smart.
[00:19:52.020 --> 00:19:56.900]   Hey, we have this data and we want to have a conversation.
[00:19:56.900 --> 00:20:01.700]   We found that when we call police or we have seen the data around police calls to suicide
[00:20:01.700 --> 00:20:07.300]   and they proportionally end up harming people, let's have a conversation about what we should
[00:20:07.300 --> 00:20:08.300]   do instead.
[00:20:08.300 --> 00:20:11.500]   And they start to spearhead that and put their weight behind that.
[00:20:11.500 --> 00:20:13.060]   That's really interesting.
[00:20:13.060 --> 00:20:14.900]   We don't have that kind of- >> So we're going to commit the pushes, Stacy.
[00:20:14.900 --> 00:20:15.900]   We push a little bit, Stacy.
[00:20:15.900 --> 00:20:18.700]   And some of these cases that were in the New York Times story, it wasn't about data.
[00:20:18.700 --> 00:20:22.540]   It was about somebody specifically saying, I'm going home right now to commit suicide.
[00:20:22.540 --> 00:20:24.300]   I'm going to kill myself.
[00:20:24.300 --> 00:20:25.300]   What do they do now?
[00:20:25.300 --> 00:20:26.300]   >> Right.
[00:20:26.300 --> 00:20:27.860]   They do police well checks.
[00:20:27.860 --> 00:20:28.860]   They do.
[00:20:28.860 --> 00:20:29.860]   I mean, that's what they do today.
[00:20:29.860 --> 00:20:31.860]   That's what they're doing.
[00:20:31.860 --> 00:20:33.860]   >> But- >> That's what they're doing.
[00:20:33.860 --> 00:20:34.860]   >> Right.
[00:20:34.860 --> 00:20:37.780]   But with the scale they have, they can send them to places.
[00:20:37.780 --> 00:20:40.380]   It's one thing to do a police well check on Pete Davidson.
[00:20:40.380 --> 00:20:46.140]   It's another to send somebody for some person who is in a predominantly black area of town
[00:20:46.140 --> 00:20:48.580]   that is a high incidence of gang violence.
[00:20:48.580 --> 00:20:50.620]   Those are going to get treated very differently.
[00:20:50.620 --> 00:20:53.140]   And I think it's important to recognize that.
[00:20:53.140 --> 00:20:55.540]   >> Yeah, and I agree we need a conversation.
[00:20:55.540 --> 00:20:59.620]   And I agree that we need to set that they should want standards set from outside.
[00:20:59.620 --> 00:21:04.700]   What I'm trying to say is that it's what it went after me for saying, forget it, GPR,
[00:21:04.700 --> 00:21:06.460]   no, never ever use this data.
[00:21:06.460 --> 00:21:08.460]   And I think that is cynical.
[00:21:08.460 --> 00:21:11.340]   And I think that is murderous in some cases.
[00:21:11.340 --> 00:21:14.900]   And we have something here that can, this has really been a case, not just Facebook,
[00:21:14.900 --> 00:21:17.140]   but other platforms around veterans.
[00:21:17.140 --> 00:21:18.980]   And it's a huge problem.
[00:21:18.980 --> 00:21:25.140]   And if we could together find ways to help people, we should try.
[00:21:25.140 --> 00:21:26.140]   >> Yeah.
[00:21:26.140 --> 00:21:27.140]   >> All right.
[00:21:27.140 --> 00:21:31.380]   All right, okay, let's do a hypothetical.
[00:21:31.380 --> 00:21:34.660]   A lot of TVs these days have cameras on them.
[00:21:34.660 --> 00:21:41.020]   And AI is getting much better at, we know this, reading micro expressions.
[00:21:41.020 --> 00:21:44.500]   The humans maybe sometimes sense, but can't read accurately.
[00:21:44.500 --> 00:21:48.860]   Let's assume AI got really effective at interpreting these micro expressions.
[00:21:48.860 --> 00:21:55.940]   Your TV could watch you and accurately predict when you were suicidal.
[00:21:55.940 --> 00:22:00.140]   I'm going to say no, and here's why, because there is a key difference.
[00:22:00.140 --> 00:22:02.540]   In Facebook, you are publishing.
[00:22:02.540 --> 00:22:03.540]   >> Post-ant.
[00:22:03.540 --> 00:22:07.020]   >> You are saying to the world something, to at least your friends, you are choosing
[00:22:07.020 --> 00:22:08.020]   to say something.
[00:22:08.020 --> 00:22:10.540]   In the case of your TV, you are not choosing.
[00:22:10.540 --> 00:22:14.940]   >> But it would save lives, Jeff.
[00:22:14.940 --> 00:22:18.780]   >> If we allowed it at all, but we don't allow that at all.
[00:22:18.780 --> 00:22:20.260]   It's different here because-
[00:22:20.260 --> 00:22:25.900]   >> Well, we may be allowing it for advertising purposes as opposed to suicide purposes.
[00:22:25.900 --> 00:22:29.900]   So let's say that might be happening.
[00:22:29.900 --> 00:22:31.900]   I think there's evidence it is happening.
[00:22:31.900 --> 00:22:37.900]   But they're not analyzing it first, suicidal intent or depression, they're analyzing it
[00:22:37.900 --> 00:22:39.900]   for whether you want to buy that product.
[00:22:39.900 --> 00:22:40.900]   I don't think it's happening.
[00:22:40.900 --> 00:22:41.900]   You don't think so?
[00:22:41.900 --> 00:22:47.900]   >> Well, they're not using the cameras.
[00:22:47.900 --> 00:22:49.900]   No, they're not using that yet.
[00:22:49.900 --> 00:22:49.900]   >> No.
[00:22:49.900 --> 00:22:51.900]   >> They're using audio for sure.
[00:22:51.900 --> 00:22:56.900]   >> They're using audio not from your home.
[00:22:56.900 --> 00:23:03.900]   Visio was using that, but the FTC was like, hey, guys, stop.
[00:23:03.900 --> 00:23:04.900]   That's not cool.
[00:23:04.900 --> 00:23:06.900]   >> That will happen eventually.
[00:23:06.900 --> 00:23:07.900]   >> Oh, yeah.
[00:23:07.900 --> 00:23:10.900]   I mean, I've seen proposals as far back as 2009 for-
[00:23:10.900 --> 00:23:14.900]   >> CR1, there's chat room says, Sam Sung TV says, "Sir, you've been watching 30 straight
[00:23:14.900 --> 00:23:16.900]   hours of the dukes of hazard.
[00:23:16.900 --> 00:23:17.900]   I'm calling 911."
[00:23:17.900 --> 00:23:20.900]   [LAUGHTER]
[00:23:20.900 --> 00:23:26.900]   >> But that's the point is the ability to do so, the argument you just made, Jeff, which is,
[00:23:26.900 --> 00:23:31.900]   well, it's going to save a life, then the ends justify the means.
[00:23:31.900 --> 00:23:35.900]   >> I think in some cases, Leo, people are signaling on purpose, right?
[00:23:35.900 --> 00:23:36.900]   Often not.
[00:23:36.900 --> 00:23:38.900]   >> But it'll save lives.
[00:23:38.900 --> 00:23:39.900]   >> It'll.
[00:23:39.900 --> 00:23:43.900]   >> It'll save some lives, Jeff, but it can also cause a lot of harm for other lives.
[00:23:43.900 --> 00:23:44.900]   >> I agree, Stacy.
[00:23:44.900 --> 00:23:47.900]   We don't find the balance, but we shouldn't.
[00:23:47.900 --> 00:23:51.500]   What I'm saying is what was said to me was, "No, never not."
[00:23:51.500 --> 00:23:52.900]   And I'm saying that's wrong.
[00:23:52.900 --> 00:23:55.620]   >> Well, I agree with you.
[00:23:55.620 --> 00:23:56.780]   Let's have a conversation about it.
[00:23:56.780 --> 00:24:00.820]   But to say, "No, never not," Facebook, let's lay off, don't do this.
[00:24:00.820 --> 00:24:02.900]   You may know it, but we don't want you to use it because we find it creepy.
[00:24:02.900 --> 00:24:04.900]   >> But there are other people-
[00:24:04.900 --> 00:24:06.820]   >> Facebook doesn't have to do that.
[00:24:06.820 --> 00:24:12.900]   If I post on Facebook, other people are going to see, unless someone has new friends on Facebook.
[00:24:12.900 --> 00:24:13.900]   >> They don't see people who speak.
[00:24:13.900 --> 00:24:20.500]   >> Well, maybe Facebook's solution then isn't to call the police.
[00:24:20.500 --> 00:24:24.980]   Maybe it's to beef its algorithm to say, "Hey, if someone posts this, you'd like to send
[00:24:24.980 --> 00:24:28.140]   a freaking notification to the people who click on their stuff the most."
[00:24:28.140 --> 00:24:34.820]   >> We should point out that the examples that the New York Times uses were in all cases,
[00:24:34.820 --> 00:24:40.660]   people streaming their suicide attempt on Facebook Live, which it seems like a call for help
[00:24:40.660 --> 00:24:41.660]   or something.
[00:24:41.660 --> 00:24:42.660]   >> Yeah.
[00:24:42.660 --> 00:24:45.540]   >> That's not exactly seeking privacy.
[00:24:45.540 --> 00:24:50.620]   I think I'd be willing to go out on a limb and say, "If Facebook detects somebody streaming
[00:24:50.620 --> 00:24:55.460]   a suicide attempt, they should immediately call first responders."
[00:24:55.460 --> 00:24:57.340]   That does not seem to be unreasonable.
[00:24:57.340 --> 00:25:01.340]   The deeper question is whether they're doing a sentiment analysis and that kind of thing
[00:25:01.340 --> 00:25:02.780]   and figuring out if somebody's suicide.
[00:25:02.780 --> 00:25:05.620]   >> I don't think they talked about this.
[00:25:05.620 --> 00:25:06.780]   And they have different interventions.
[00:25:06.780 --> 00:25:09.340]   The intervention is not just to call a police.
[00:25:09.340 --> 00:25:11.500]   I don't want to quote it because I can't remember what it was.
[00:25:11.500 --> 00:25:12.500]   It's online somewhere.
[00:25:12.500 --> 00:25:14.780]   The video is there.
[00:25:14.780 --> 00:25:15.780]   They do know more than that.
[00:25:15.780 --> 00:25:19.900]   But this story, there was a reaction around your right, Leo, that.
[00:25:19.900 --> 00:25:21.660]   Somebody streaming their suicide attempt.
[00:25:21.660 --> 00:25:23.940]   >> I think they have a responsibility.
[00:25:23.940 --> 00:25:27.860]   They probably have a legal responsibility to call 911.
[00:25:27.860 --> 00:25:28.860]   >> Yeah.
[00:25:28.860 --> 00:25:31.820]   If someone's streaming their suicide, then yes, you do have to do that.
[00:25:31.820 --> 00:25:33.900]   >> And by the way, there were a lot in this story.
[00:25:33.900 --> 00:25:35.460]   There, a couple of lives are saved.
[00:25:35.460 --> 00:25:39.780]   In one case, it was too late, which is very sad.
[00:25:39.780 --> 00:25:46.380]   They also quote the director of the National Suicide Prevention Lifeline, "Commending Facebook
[00:25:46.380 --> 00:25:48.900]   for this."
[00:25:48.900 --> 00:25:49.900]   So --
[00:25:49.900 --> 00:25:50.900]   >> Dana Boyes.
[00:25:50.900 --> 00:25:53.420]   >> Oh, but it's talking about two things.
[00:25:53.420 --> 00:25:56.620]   It's talking about people streaming it, but it's also talking about using data analysis.
[00:25:56.620 --> 00:25:57.620]   >> It's the sentiment analysis.
[00:25:57.620 --> 00:25:58.620]   >> Yeah.
[00:25:58.620 --> 00:25:59.620]   But they talk about doing that.
[00:25:59.620 --> 00:26:00.620]   That's the GDPR thing.
[00:26:00.620 --> 00:26:01.620]   >> Well, that's the next.
[00:26:01.620 --> 00:26:03.100]   >> Yeah, one step leads to the next.
[00:26:03.100 --> 00:26:06.140]   >> So you're right, there's a discussion needed.
[00:26:06.140 --> 00:26:07.500]   There's data that needs to be shared.
[00:26:07.500 --> 00:26:09.900]   There's research that needs to be done.
[00:26:09.900 --> 00:26:10.900]   Yeah.
[00:26:10.900 --> 00:26:16.780]   >> They talked to the director of -- and I even know this existed -- the Digital Psychiatry
[00:26:16.780 --> 00:26:22.300]   Division, whatever that is, at Beth Israel Deaconess Medical Center in Boston.
[00:26:22.300 --> 00:26:25.580]   He says, "It's hard to know what Facebook's actually picking up on."
[00:26:25.580 --> 00:26:31.860]   This is right after their illustration from Facebook of how text and comment classifiers
[00:26:31.860 --> 00:26:35.740]   work and how they're flagging based on text signals.
[00:26:35.740 --> 00:26:38.700]   It's hard to know what Facebook is actually picking up on what they're actually acting
[00:26:38.700 --> 00:26:43.460]   on and are they giving the appropriate response to the appropriate risks?
[00:26:43.460 --> 00:26:46.180]   And that's because Facebook themselves aren't saying.
[00:26:46.180 --> 00:26:49.980]   So he calls it Dr. John Torros calls it "Black Box Medicine."
[00:26:49.980 --> 00:26:54.340]   Facebook says it works with suicide prevention experts to do it.
[00:26:54.340 --> 00:26:56.740]   So I mean, I don't know.
[00:26:56.740 --> 00:27:01.220]   >> I think those kind of things have to be a lot more open.
[00:27:01.220 --> 00:27:03.060]   >> It's a really interesting question, which --
[00:27:03.060 --> 00:27:04.060]   >> It is.
[00:27:04.060 --> 00:27:06.660]   >> And the reason I brought up the hypothetical is because increasingly we're going to be
[00:27:06.660 --> 00:27:11.060]   in a world where these kinds of signals are seen and can be interpreted.
[00:27:11.060 --> 00:27:12.060]   And then --
[00:27:12.060 --> 00:27:13.060]   >> Well, what do you do?
[00:27:13.060 --> 00:27:17.060]   >> And we're increasingly going to be in a world where that can be interpreted outside
[00:27:17.060 --> 00:27:18.060]   the home.
[00:27:18.060 --> 00:27:23.500]   So there was just a story today about like -- maybe it was earlier, it was on courts -- on
[00:27:23.500 --> 00:27:27.180]   the language people who are depressed use, for example.
[00:27:27.180 --> 00:27:29.660]   And you think about things like all the apps coming out.
[00:27:29.660 --> 00:27:34.100]   Kevin and I talked about this on our show with CES called Algorithmic Health Care.
[00:27:34.100 --> 00:27:39.100]   There's a lot of data coming out on, "Hey, I can predict if you're going to get Parkinson's.
[00:27:39.100 --> 00:27:42.980]   I can predict if you're in the early stages of diabetes."
[00:27:42.980 --> 00:27:48.140]   So when you start -- I mean, the challenge we have -- and this is what GDPR is trying
[00:27:48.140 --> 00:27:53.420]   to address -- is stopping that train before it leaves the station in some ways.
[00:27:53.420 --> 00:27:59.740]   Because once you can get that just from listening -- sorry.
[00:27:59.740 --> 00:28:01.820]   >> UPS guys here.
[00:28:01.820 --> 00:28:02.820]   >> Someone's there.
[00:28:02.820 --> 00:28:07.180]   >> I can tell from your dog, "Sentiment analysis on your dog tells us you're getting
[00:28:07.180 --> 00:28:09.340]   a delivery right now."
[00:28:09.340 --> 00:28:10.660]   So Jeff, what about this?
[00:28:10.660 --> 00:28:13.300]   Okay, I'm going to try to put words in your mouth, Jeff.
[00:28:13.300 --> 00:28:14.700]   Let's see if they fit.
[00:28:14.700 --> 00:28:15.700]   >> Let's never --
[00:28:15.700 --> 00:28:22.700]   >> It's possible to move too quickly to stymie this.
[00:28:22.700 --> 00:28:29.580]   It would be a better thing to do to watch, learn, discuss, consider the policy implications
[00:28:29.580 --> 00:28:30.940]   for sure.
[00:28:30.940 --> 00:28:35.060]   But I think you're worried -- and I'm not sure I think I would agree with you -- that you
[00:28:35.060 --> 00:28:37.100]   can also move too quickly.
[00:28:37.100 --> 00:28:38.100]   You can move too slowly.
[00:28:38.100 --> 00:28:39.100]   >> You can move too quickly both.
[00:28:39.100 --> 00:28:40.100]   >> Yes.
[00:28:40.100 --> 00:28:45.380]   >> And to be fair, GDPR lets you, if you opt into it, they let you use this data.
[00:28:45.380 --> 00:28:47.820]   Just nobody's -- everyone else is like, "Oh, it's too expensive.
[00:28:47.820 --> 00:28:48.820]   We don't want to mess with that."
[00:28:48.820 --> 00:28:50.100]   Let's just say no.
[00:28:50.100 --> 00:28:55.900]   So if they -- if you say to someone, "Hey, I really want to study this, I imagine a
[00:28:55.900 --> 00:28:58.980]   lot of people would self-opt into this."
[00:28:58.980 --> 00:29:01.940]   And you just have to design something with that in mind.
[00:29:01.940 --> 00:29:06.500]   And what's happening now -- and this is what I think a lot of people who just are like,
[00:29:06.500 --> 00:29:08.620]   "Oh, GDPR, now we can't do anything.
[00:29:08.620 --> 00:29:09.820]   Laws are in the way.
[00:29:09.820 --> 00:29:10.820]   We can't do anything.
[00:29:10.820 --> 00:29:13.620]   I think that's a -- you can't do it without difficulty.
[00:29:13.620 --> 00:29:22.260]   >> Yeah, but I also think that there is also perhaps a rush to shut this stuff down, maybe
[00:29:22.260 --> 00:29:23.260]   -- >> Yeah.
[00:29:23.260 --> 00:29:24.580]   >> That's what I'm saying.
[00:29:24.580 --> 00:29:26.300]   >> That's the techno panic you always refer to.
[00:29:26.300 --> 00:29:27.300]   >> That's what I'm talking about.
[00:29:27.300 --> 00:29:28.300]   >> I hate these discussions.
[00:29:28.300 --> 00:29:29.620]   >> I think we all agree.
[00:29:29.620 --> 00:29:31.180]   These discussions have to be.
[00:29:31.180 --> 00:29:35.420]   We have to have them, and we have to, as a society, decide what our limits are on all
[00:29:35.420 --> 00:29:36.420]   this.
[00:29:36.420 --> 00:29:40.980]   >> So I had another -- this is a little -- this is a cartoonish and it's extreme, but I had
[00:29:40.980 --> 00:29:46.780]   a case today where there was a story, business insider, that Zuckerberg Chan Foundation
[00:29:46.780 --> 00:29:51.660]   is one of the things that's funding is putting sensors directly in the brain.
[00:29:51.660 --> 00:29:55.100]   So some smartass, of course, comes on and says, "Oh, this is like what we want."
[00:29:55.100 --> 00:29:58.180]   Well, say that to somebody with epilepsy or Parkinson's, right?
[00:29:58.180 --> 00:30:03.220]   They're doing it for a reason, but this kind of chatter has damage at some level.
[00:30:03.220 --> 00:30:06.740]   And we're making these presumptions, and yeah, I don't want to get into the whole techno
[00:30:06.740 --> 00:30:07.740]   panic thing today.
[00:30:07.740 --> 00:30:09.500]   It's the beginning of a new year.
[00:30:09.500 --> 00:30:12.460]   But I just -- this is -- I worry about this.
[00:30:12.460 --> 00:30:14.020]   Here's what I would say.
[00:30:14.020 --> 00:30:17.300]   And I think the reason there's -- people are concerned.
[00:30:17.300 --> 00:30:25.060]   There are companies doing this out of a sincere desire to make life better, but Facebook,
[00:30:25.060 --> 00:30:32.140]   Google, and a few others are doing it out of desire to make advertising more effective.
[00:30:32.140 --> 00:30:35.020]   That's their business model, Jeff.
[00:30:35.020 --> 00:30:36.020]   >> I think they have.
[00:30:36.020 --> 00:30:39.460]   >> You said a few minutes ago that you think that Mark Zuckerberg is a big fan of the
[00:30:39.460 --> 00:30:41.180]   believer, and I think he is.
[00:30:41.180 --> 00:30:42.780]   I think to a fault.
[00:30:42.780 --> 00:30:45.540]   Well, I'm not sure what he believes in.
[00:30:45.540 --> 00:30:49.780]   It might be that he believes in getting people together just for the good of it.
[00:30:49.780 --> 00:30:56.180]   It might be that he believes that it's a good business because when people share, not only
[00:30:56.180 --> 00:31:00.060]   do they connect, they also give us information we can use to advertise.
[00:31:00.060 --> 00:31:06.260]   And I really honestly believe that if -- what we don't want is companies for what -- since
[00:31:06.260 --> 00:31:08.980]   we can't know their real intent.
[00:31:08.980 --> 00:31:12.020]   We can only know what they do.
[00:31:12.020 --> 00:31:18.140]   And what -- I don't think companies that are advertising supported should be in this business.
[00:31:18.140 --> 00:31:19.980]   I think that should be -- >> That is not how --
[00:31:19.980 --> 00:31:23.940]   >> The National Institutes of Health should be in this business.
[00:31:23.940 --> 00:31:29.420]   That agencies, the companies that aren't trying to use this information for advertising
[00:31:29.420 --> 00:31:31.980]   purposes should not be in this business.
[00:31:31.980 --> 00:31:32.980]   I just don't think that's right.
[00:31:32.980 --> 00:31:35.740]   >> Then we in media should not be doing health stuff because we --
[00:31:35.740 --> 00:31:36.740]   >> Absolutely.
[00:31:36.740 --> 00:31:40.660]   >> We should not be in the business of planning something in your brain to find out what you're
[00:31:40.660 --> 00:31:41.660]   thinking.
[00:31:41.660 --> 00:31:44.060]   That would be the wrong thing for anybody in media to get out.
[00:31:44.060 --> 00:31:45.780]   >> But in the right years.
[00:31:45.780 --> 00:31:54.020]   >> But, I mean, that's honestly -- that is wrong because we can't know people's intent.
[00:31:54.020 --> 00:31:55.820]   Sometimes people don't even know their own intent.
[00:31:55.820 --> 00:31:58.140]   But we can know what a business a company is in.
[00:31:58.140 --> 00:32:03.620]   If a company's in an advertising business, pure and simple, they shouldn't be doing this.
[00:32:03.620 --> 00:32:07.620]   I include Google in this.
[00:32:07.620 --> 00:32:08.620]   Right?
[00:32:08.620 --> 00:32:09.620]   >> No.
[00:32:09.620 --> 00:32:12.300]   >> No, because again, I'll go back -- I'll go to my business.
[00:32:12.300 --> 00:32:14.540]   There was a big scandal in Germany the last two weeks.
[00:32:14.540 --> 00:32:20.060]   A guy for their Spiegel made up -- fly out made up stories.
[00:32:20.060 --> 00:32:22.340]   And there was this -- and I wrote a piece about it.
[00:32:22.340 --> 00:32:23.420]   I believe you don't want to go into it.
[00:32:23.420 --> 00:32:26.220]   It's boring German journalism stuff.
[00:32:26.220 --> 00:32:28.740]   But it's the seduction of the narrative.
[00:32:28.740 --> 00:32:32.900]   It's the seduction of our business model to hold people's attention.
[00:32:32.900 --> 00:32:36.580]   We do things in my business where we mess around with people's lives, individually and
[00:32:36.580 --> 00:32:38.780]   wholly.
[00:32:38.780 --> 00:32:40.780]   So it's not just platforms.
[00:32:40.780 --> 00:32:41.780]   It's us too.
[00:32:41.780 --> 00:32:43.020]   They just have better tools.
[00:32:43.020 --> 00:32:45.260]   >> What do you think, Stacey?
[00:32:45.260 --> 00:32:46.900]   I've just made up Leo's rule.
[00:32:46.900 --> 00:32:48.900]   Is that a good rule?
[00:32:48.900 --> 00:32:50.180]   >> And the rule -- I'm sorry.
[00:32:50.180 --> 00:32:52.900]   The rule was not too -- >> If you're an advertising business that you
[00:32:52.900 --> 00:32:53.900]   should -- >> You shouldn't --
[00:32:53.900 --> 00:32:58.180]   >> -- set them in an analysis business because that impunes your motives.
[00:32:58.180 --> 00:33:03.500]   >> Well, you can be in the sentiment analysis business to sell stuff to you.
[00:33:03.500 --> 00:33:05.700]   >> Well, that's what I'm assuming they're doing.
[00:33:05.700 --> 00:33:06.700]   >> Oh, okay.
[00:33:06.700 --> 00:33:07.700]   But you --
[00:33:07.700 --> 00:33:11.860]   >> So you see what I'm saying is that it's not -- you can't -- it's hard to then say
[00:33:11.860 --> 00:33:13.340]   oh, that's a good -- >> You can't go to their motives.
[00:33:13.340 --> 00:33:15.540]   >> Yeah, that's a wonderful thing they're doing.
[00:33:15.540 --> 00:33:16.540]   >> Yeah.
[00:33:16.540 --> 00:33:21.180]   >> By the way, I'm going to be interviewing this woman in a week and a half.
[00:33:21.180 --> 00:33:22.180]   Shoshana Zuboff.
[00:33:22.180 --> 00:33:24.180]   >> I love --
[00:33:24.180 --> 00:33:25.900]   >> Boy, boy.
[00:33:25.900 --> 00:33:29.940]   >> The age of surveillance capitalism -- Jeff, it's a thick book, 600 pages.
[00:33:29.940 --> 00:33:32.380]   Very well supported with footnotes.
[00:33:32.380 --> 00:33:37.140]   She's a -- the first female tenured professor at the Harvard Business School.
[00:33:37.140 --> 00:33:38.580]   She's very insightful thinker.
[00:33:38.580 --> 00:33:39.580]   >> Wait, wait, wait, wait.
[00:33:39.580 --> 00:33:40.580]   >> Yeah.
[00:33:40.580 --> 00:33:43.340]   >> The Harvard Business School never had a tenured female professor before that.
[00:33:43.340 --> 00:33:46.140]   She was tenured in 1981.
[00:33:46.140 --> 00:33:47.140]   Even so.
[00:33:47.140 --> 00:33:48.140]   >> Good.
[00:33:48.140 --> 00:33:50.780]   >> Well, okay, one of its first tenured -- >> You want to go -- you want to go -- okay,
[00:33:50.780 --> 00:33:51.780]   I'll go.
[00:33:51.780 --> 00:33:53.460]   You want to go -- I don't know why -- America Business is screwed up.
[00:33:53.460 --> 00:33:55.180]   That tells you something right there.
[00:33:55.180 --> 00:33:56.780]   >> Just a part.
[00:33:56.780 --> 00:33:58.060]   Okay.
[00:33:58.060 --> 00:34:00.860]   I think they can do it.
[00:34:00.860 --> 00:34:04.820]   I think they would be best -- like, there are benefits to having a giant platform gather
[00:34:04.820 --> 00:34:05.820]   data.
[00:34:05.820 --> 00:34:11.460]   One is you can get a larger sample population, which is really interesting for public health.
[00:34:11.460 --> 00:34:18.460]   I think if you are sharing that data with people as opposed to grabbing it and doing
[00:34:18.460 --> 00:34:23.460]   some sort of weird algorithm of your own design without any transparency is when it becomes
[00:34:23.460 --> 00:34:24.900]   a problem.
[00:34:24.900 --> 00:34:29.860]   So maybe I want my cake -- or I want to have my cake and eat it too.
[00:34:29.860 --> 00:34:31.980]   I don't -- anyway.
[00:34:31.980 --> 00:34:33.220]   Maybe I want things both ways.
[00:34:33.220 --> 00:34:38.860]   But I do think that the ability to get lots of data into access data from lots of people,
[00:34:38.860 --> 00:34:42.420]   even if it's opt-in, if Google tells you to do something because they think it'll help
[00:34:42.420 --> 00:34:48.060]   humanity, I think that could be under the right framework.
[00:34:48.060 --> 00:34:49.060]   Okay.
[00:34:49.060 --> 00:34:52.700]   So I would have a little addendum to your law, Leo.
[00:34:52.700 --> 00:34:55.700]   Yeah, I mean, it was only a couple weeks ago.
[00:34:55.700 --> 00:34:59.900]   I said that Google is going to eventually become the AI king.
[00:34:59.900 --> 00:35:04.940]   I mean, it costs a lot of money to do it and is probably better Google do it than the Chinese
[00:35:04.940 --> 00:35:06.980]   government do it.
[00:35:06.980 --> 00:35:12.020]   I just -- I don't -- there's no simple rules.
[00:35:12.020 --> 00:35:15.140]   Well, we're going to stop them from doing this and then everything will be okay.
[00:35:15.140 --> 00:35:16.380]   We're negotiating norms.
[00:35:16.380 --> 00:35:18.780]   We're negotiating rules and standards.
[00:35:18.780 --> 00:35:19.780]   Stacey's absolutely right.
[00:35:19.780 --> 00:35:20.940]   We've got to do it more in public.
[00:35:20.940 --> 00:35:25.220]   In the piece that I wrote, I said that the transparency is not just about admitting your
[00:35:25.220 --> 00:35:28.100]   foibles, which is what Facebook and politicians think it is.
[00:35:28.100 --> 00:35:30.940]   It's also about -- there should be -- from the very top of Facebook, there should be a view
[00:35:30.940 --> 00:35:34.780]   that everything we do here should shine brightly in transparency.
[00:35:34.780 --> 00:35:36.580]   We should be proud of what we do here.
[00:35:36.580 --> 00:35:39.300]   And if you're not, then don't do it.
[00:35:39.300 --> 00:35:42.980]   That's more to the core of what's wrong with Facebook and what they need to fix in their
[00:35:42.980 --> 00:35:43.980]   culture.
[00:35:43.980 --> 00:35:47.860]   That's transparency that matters in a different way that says we're making judgments about
[00:35:47.860 --> 00:35:52.460]   what we do every damn day and we need structures to make good judgments about that and do them
[00:35:52.460 --> 00:35:54.100]   in public.
[00:35:54.100 --> 00:35:58.300]   By the way, the New York Times headline is judicious, to say the least.
[00:35:58.300 --> 00:36:03.420]   And screening for suicide risk, Facebook takes on tricky public health role.
[00:36:03.420 --> 00:36:04.420]   It is indeed.
[00:36:04.420 --> 00:36:06.420]   I don't think that that's -- you know, and in fact --
[00:36:06.420 --> 00:36:07.420]   That's not sensationalist.
[00:36:07.420 --> 00:36:08.420]   It's not sensationalist.
[00:36:08.420 --> 00:36:09.420]   It's exactly right.
[00:36:09.420 --> 00:36:10.420]   It's what we're talking about.
[00:36:10.420 --> 00:36:11.420]   Yes.
[00:36:11.420 --> 00:36:12.420]   And that's true.
[00:36:12.420 --> 00:36:13.420]   It is tricky.
[00:36:13.420 --> 00:36:14.420]   All right.
[00:36:14.420 --> 00:36:15.420]   Should we talk about Google?
[00:36:15.420 --> 00:36:16.420]   Yeah.
[00:36:16.420 --> 00:36:17.420]   Well, this is Google, too, really.
[00:36:17.420 --> 00:36:20.020]   And it's all a piece of the same thing.
[00:36:20.020 --> 00:36:22.020]   She's trying to make us happier here.
[00:36:22.020 --> 00:36:27.620]   No, I'm just -- like, at a certain point in time, we go around to the same kind of --
[00:36:27.620 --> 00:36:29.260]   We always do.
[00:36:29.260 --> 00:36:33.460]   But I predict that this is going to be the year that we will spend a lot of time talking
[00:36:33.460 --> 00:36:39.540]   about improved capabilities for exactly this kind of thing.
[00:36:39.540 --> 00:36:40.540]   I hope so.
[00:36:40.540 --> 00:36:42.900]   I hope we have -- And that debate is important, right?
[00:36:42.900 --> 00:36:43.900]   We agree on that.
[00:36:43.900 --> 00:36:44.900]   Oh, yeah.
[00:36:44.900 --> 00:36:45.900]   Yeah.
[00:36:45.900 --> 00:36:46.900]   No, I totally agree that debate is important.
[00:36:46.900 --> 00:36:49.500]   We're kind of like, "We need to talk about it.
[00:36:49.500 --> 00:36:50.500]   Let's..."
[00:36:50.500 --> 00:36:51.500]   Well, how about this?
[00:36:51.500 --> 00:36:55.140]   "Wielding rocks and knives, Arizona's attacks self-driving cars."
[00:36:55.140 --> 00:36:56.140]   Yeah.
[00:36:56.140 --> 00:36:57.140]   Yeah.
[00:36:57.140 --> 00:36:58.140]   You see that?
[00:36:58.140 --> 00:36:59.140]   That's a Google story.
[00:36:59.140 --> 00:37:01.140]   This is a discussion that's working out just fine.
[00:37:01.140 --> 00:37:04.980]   This is -- These are the Waymo vehicles going around in Arizona.
[00:37:04.980 --> 00:37:08.940]   And apparently, there have been two dozen attacks on Waymo's over the last two years.
[00:37:08.940 --> 00:37:10.740]   That's not a huge number.
[00:37:10.740 --> 00:37:11.740]   That's one of them.
[00:37:11.740 --> 00:37:12.740]   That's what the original Luddites did.
[00:37:12.740 --> 00:37:13.740]   So, I mean, that's --
[00:37:13.740 --> 00:37:14.740]   Luddites.com, yeah.
[00:37:14.740 --> 00:37:15.740]   Yeah.
[00:37:15.740 --> 00:37:16.740]   We talked about before.
[00:37:16.740 --> 00:37:19.220]   People really hate these self-driving cars sometimes.
[00:37:19.220 --> 00:37:20.500]   I don't know why.
[00:37:20.500 --> 00:37:21.500]   People have --
[00:37:21.500 --> 00:37:22.500]   Well, impelted them with rocks.
[00:37:22.500 --> 00:37:23.500]   People hate those robots.
[00:37:23.500 --> 00:37:24.500]   Yeah.
[00:37:24.500 --> 00:37:27.300]   They kick the delivery robots and security robots, yeah.
[00:37:27.300 --> 00:37:32.620]   In one of the more harrowing episodes, a man waved a 22-caliber revolver at a Waymo
[00:37:32.620 --> 00:37:34.460]   vehicle.
[00:37:34.460 --> 00:37:35.620]   By the way, he was mentally ill.
[00:37:35.620 --> 00:37:41.860]   He told police he despises driverless cars partly because of the killing of the female
[00:37:41.860 --> 00:37:44.740]   pedestrian in Tempe.
[00:37:44.740 --> 00:37:49.300]   His wife admitted he finds it entertaining to break hard in front of self-driving vans
[00:37:49.300 --> 00:37:53.060]   and that she herself may have forced them to pull over so she could yell at them to get
[00:37:53.060 --> 00:37:55.580]   out of their neighborhood.
[00:37:55.580 --> 00:37:58.740]   She was yelling at the self-driving car.
[00:37:58.740 --> 00:37:59.740]   Yeah.
[00:37:59.740 --> 00:38:00.740]   They found each other.
[00:38:00.740 --> 00:38:03.420]   The trouble started the couple said when their 10-year-old son was nearly hit by one
[00:38:03.420 --> 00:38:06.460]   of the vehicles when he was playing in a nearby cultist sec.
[00:38:06.460 --> 00:38:09.020]   He says, "I don't want to be there -- I guess he isn't mentally ill.
[00:38:09.020 --> 00:38:10.180]   This was another one.
[00:38:10.180 --> 00:38:11.180]   I apologize.
[00:38:11.180 --> 00:38:12.740]   He has a different one."
[00:38:12.740 --> 00:38:17.140]   He says, "I don't want to be their real-world example or their real-world mistake."
[00:38:17.140 --> 00:38:18.140]   That's a reasonable --
[00:38:18.140 --> 00:38:23.700]   So this gets into something we don't think about a lot, but as we stick technology in
[00:38:23.700 --> 00:38:31.180]   cities, like smart city efforts, as we start taking over more of the world with technology,
[00:38:31.180 --> 00:38:39.300]   we do actually risk not having these conversations as like a democratic process.
[00:38:39.300 --> 00:38:40.740]   They're saying we're the bait.
[00:38:40.740 --> 00:38:44.860]   We're unwillingly debated subjects for this.
[00:38:44.860 --> 00:38:46.980]   I guess that's a real point of view, right?
[00:38:46.980 --> 00:38:47.980]   That's reasonable.
[00:38:47.980 --> 00:38:50.420]   And it's not like -- and that's the other thing.
[00:38:50.420 --> 00:38:52.340]   Lawmakers did decide this.
[00:38:52.340 --> 00:38:54.700]   So could they vote those lawmakers out?
[00:38:54.700 --> 00:38:55.700]   Yes.
[00:38:55.700 --> 00:38:58.700]   Could a politician run on an anti-tech platform?
[00:38:58.700 --> 00:38:59.700]   Probably.
[00:38:59.700 --> 00:39:02.300]   But that's not a great nuanced way to think about this.
[00:39:02.300 --> 00:39:09.460]   And I think that's where technology of companies really need to wake up and realize that they
[00:39:09.460 --> 00:39:11.580]   can't just fall back on data.
[00:39:11.580 --> 00:39:17.220]   They have to start dealing with people in all of our horrifyingly messy glory.
[00:39:17.220 --> 00:39:25.380]   I didn't realize that Douglas Resskoff, who works at CUNY, wrote a book called Throwing
[00:39:25.380 --> 00:39:27.980]   Rocks at the Google Bus.
[00:39:27.980 --> 00:39:29.940]   Yeah.
[00:39:29.940 --> 00:39:34.860]   He says in this article, "Drivelist cars are like robot incarnations of scabs."
[00:39:34.860 --> 00:39:37.500]   Listen, I love him.
[00:39:37.500 --> 00:39:40.180]   He's one of my favorite people on Earth, but he's pure Marxist.
[00:39:40.180 --> 00:39:41.700]   So it's a Marxist.
[00:39:41.700 --> 00:39:44.300]   Well, it's not a -- it's not a -- yeah.
[00:39:44.300 --> 00:39:45.300]   Right.
[00:39:45.300 --> 00:39:51.780]   So it says that there's an intervention here in a job economy.
[00:39:51.780 --> 00:39:52.780]   And that's what causes you.
[00:39:52.780 --> 00:39:55.060]   But that's why the Luddites revolted, right?
[00:39:55.060 --> 00:39:56.060]   Yeah.
[00:39:56.060 --> 00:39:58.060]   They were losing their jobs to jacquard looms.
[00:39:58.060 --> 00:39:59.060]   Yep.
[00:39:59.060 --> 00:40:03.420]   And these will -- I mean, driverless cars are the future.
[00:40:03.420 --> 00:40:08.740]   And I also think tech companies need to talk more about what roles they will have.
[00:40:08.740 --> 00:40:13.780]   I mean, I just -- unfortunately, it's not out yet, but I filed a story for IEEE about
[00:40:13.780 --> 00:40:17.020]   the role of people in autonomous vehicles.
[00:40:17.020 --> 00:40:18.940]   And they will have them.
[00:40:18.940 --> 00:40:23.460]   They will be remote controlled for quite some time in the future.
[00:40:23.460 --> 00:40:27.580]   So will it be a one-to-one?
[00:40:27.580 --> 00:40:28.580]   No.
[00:40:28.580 --> 00:40:34.820]   So, yeah, this is frustrating because I feel like a little bit more transparency, real
[00:40:34.820 --> 00:40:38.740]   conversation between parties would actually go a long way.
[00:40:38.740 --> 00:40:40.900]   And we're not doing that.
[00:40:40.900 --> 00:40:43.140]   We aren't even building the tools to do that.
[00:40:43.140 --> 00:40:45.140]   So I don't know how to solve this.
[00:40:45.140 --> 00:40:47.500]   I'm going to go live under a rock.
[00:40:47.500 --> 00:40:52.460]   I think we need technology that motivates people to loom.
[00:40:52.460 --> 00:40:56.460]   In Ray Kurzweil in his book, The Age of the Spiritual Machines -- this is a great Wikipedia
[00:40:56.460 --> 00:41:02.660]   article, by the way, which I found on Pinboard -- talked about 2019 and some of the things
[00:41:02.660 --> 00:41:03.660]   he expected.
[00:41:03.660 --> 00:41:08.060]   Now, he wrote this 20 years ago, almost, some of the things he expected.
[00:41:08.060 --> 00:41:11.300]   He said about self-driving vehicles.
[00:41:11.300 --> 00:41:13.500]   I mean, I could find it.
[00:41:13.500 --> 00:41:17.060]   Computers do most of the vehicle driving by 2019.
[00:41:17.060 --> 00:41:20.620]   Humans are, in fact, prohibited from driving on highways unassisted.
[00:41:20.620 --> 00:41:24.720]   Furthermore, when humans do take over the wheel of the onboard computer system, constantly
[00:41:24.720 --> 00:41:29.880]   monitors their actions to take control whenever the human drives recklessly.
[00:41:29.880 --> 00:41:34.700]   As a result, there are very few transportation accidents.
[00:41:34.700 --> 00:41:37.060]   I actually kind of wish that were the case.
[00:41:37.060 --> 00:41:39.060]   Oh, but people are going to hate that.
[00:41:39.060 --> 00:41:41.360]   I mean, people hate it now.
[00:41:41.360 --> 00:41:44.760]   Anytime we have something that interferes with our autonomy, we're like, screw you.
[00:41:44.760 --> 00:41:46.860]   If I want to get drunk behind the wheel of the car.
[00:41:46.860 --> 00:41:47.860]   Yeah, I can do it.
[00:41:47.860 --> 00:41:48.860]   That's my right.
[00:41:48.860 --> 00:41:49.860]   It's my right.
[00:41:49.860 --> 00:41:53.780]   I was just talking to a guy I know here, a neighbor here.
[00:41:53.780 --> 00:41:56.160]   And his kids were a little younger than ours.
[00:41:56.160 --> 00:41:57.600]   They said they don't want to drive.
[00:41:57.600 --> 00:41:58.600]   They don't want the driver's license.
[00:41:58.600 --> 00:41:59.600]   They don't want to do it.
[00:41:59.600 --> 00:42:00.600]   They hate it.
[00:42:00.600 --> 00:42:04.000]   I think if there's going to hit a generational point, we're fine.
[00:42:04.000 --> 00:42:07.160]   I can play my video game while I'm getting there.
[00:42:07.160 --> 00:42:08.160]   Fine.
[00:42:08.160 --> 00:42:15.120]   He also predicted that devices that deliver sensations to the skin surface of their users
[00:42:15.120 --> 00:42:18.840]   will make virtual sex possible.
[00:42:18.840 --> 00:42:20.800]   So aren't there those things?
[00:42:20.800 --> 00:42:21.800]   No.
[00:42:21.800 --> 00:42:23.360]   Well, I was about to say those things kind of exist.
[00:42:23.360 --> 00:42:24.360]   I don't know.
[00:42:24.360 --> 00:42:25.360]   Yeah.
[00:42:25.360 --> 00:42:28.700]   That's why I hate predictions.
[00:42:28.700 --> 00:42:29.700]   It's hard.
[00:42:29.700 --> 00:42:30.700]   It's hard to do.
[00:42:30.700 --> 00:42:36.100]   The most hubristic job title on earth, this side of Messiah, is Futurist.
[00:42:36.100 --> 00:42:37.100]   Yeah.
[00:42:37.100 --> 00:42:39.100]   Really not thought leader?
[00:42:39.100 --> 00:42:42.440]   Well, actually, that's what the future says.
[00:42:42.440 --> 00:42:43.440]   They're leading our thoughts.
[00:42:43.440 --> 00:42:44.440]   All right.
[00:42:44.440 --> 00:42:46.340]   I have some happy stories.
[00:42:46.340 --> 00:42:47.340]   Good.
[00:42:47.340 --> 00:42:48.340]   Yes.
[00:42:48.340 --> 00:42:50.500]   Some positive.
[00:42:50.500 --> 00:42:53.580]   I'm sure somewhere.
[00:42:53.580 --> 00:42:54.580]   Some good.
[00:42:54.580 --> 00:42:56.580]   The Google Maps.
[00:42:56.580 --> 00:42:59.220]   I was going to say, what?
[00:42:59.220 --> 00:43:00.660]   He moves them.
[00:43:00.660 --> 00:43:01.660]   He moves them.
[00:43:01.660 --> 00:43:04.260]   Right around the bloke moons Google Maps.
[00:43:04.260 --> 00:43:06.260]   He's a cheeky lad.
[00:43:06.260 --> 00:43:07.260]   Okay.
[00:43:07.260 --> 00:43:12.540]   That wasn't what I was thinking, but it is right under the waymo story.
[00:43:12.540 --> 00:43:15.780]   So I guess we should probably get to that.
[00:43:15.780 --> 00:43:16.780]   Delighted.
[00:43:16.780 --> 00:43:18.180]   He's not maps.
[00:43:18.180 --> 00:43:19.980]   It's that street view car.
[00:43:19.980 --> 00:43:20.980]   Right.
[00:43:20.980 --> 00:43:21.980]   Yeah.
[00:43:21.980 --> 00:43:26.780]   And he's delighted because apparently his bum is immortalized online.
[00:43:26.780 --> 00:43:27.900]   This is from the mirror.
[00:43:27.900 --> 00:43:34.500]   So it looks like they pixelated the brand of the bicycle riding by, but they left his
[00:43:34.500 --> 00:43:39.860]   butt because the, the, we didn't expect a butt.
[00:43:39.860 --> 00:43:43.140]   That's why exactly the algorithm did not know to look for butts because it thought that
[00:43:43.140 --> 00:43:45.180]   people were going to be nicer than that.
[00:43:45.180 --> 00:43:46.180]   Oh, never.
[00:43:46.180 --> 00:43:51.180]   Never.
[00:43:51.180 --> 00:43:52.180]   Never.
[00:43:52.180 --> 00:43:53.180]   Now they have to write an algorithm.
[00:43:53.180 --> 00:43:53.180]   Now they have now some poor schmoe has to write a butt detecting algorithm.
[00:43:53.180 --> 00:44:01.140]   And then from the Wall Street Journal, delivery drones, delivery drones, cheer shoppers,
[00:44:01.140 --> 00:44:04.740]   annoy neighbors, scare dogs.
[00:44:04.740 --> 00:44:08.820]   In one of the world's most advanced drone delivery tests, sunscreen arrives in minutes.
[00:44:08.820 --> 00:44:12.020]   So do complaints.
[00:44:12.020 --> 00:44:14.940]   This is in Australia and Canberra.
[00:44:14.940 --> 00:44:17.620]   She thought it was.
[00:44:17.620 --> 00:44:19.460]   I was amazed by the story of the rundown.
[00:44:19.460 --> 00:44:24.700]   I had not seen this, this further extension that now there's speculation that the airport
[00:44:24.700 --> 00:44:26.980]   closing in the UK that there were no.
[00:44:26.980 --> 00:44:27.980]   Oh, yes.
[00:44:27.980 --> 00:44:28.980]   They're never.
[00:44:28.980 --> 00:44:31.740]   There's a stereo and that's very hello.
[00:44:31.740 --> 00:44:34.140]   The poor couple that was busted.
[00:44:34.140 --> 00:44:35.140]   They let them go.
[00:44:35.140 --> 00:44:37.900]   They said, no, they had nothing to do with it.
[00:44:37.900 --> 00:44:41.300]   Yeah, that was that's horrible.
[00:44:41.300 --> 00:44:43.940]   We talked about it because I trusted the news.
[00:44:43.940 --> 00:44:44.940]   Yeah, my mistake.
[00:44:44.940 --> 00:44:45.940]   Well, that's that.
[00:44:45.940 --> 00:44:49.220]   I mean, you're not trusting that you're trusting.
[00:44:49.220 --> 00:44:55.140]   I mean, the police officers in the army were saying, I mean, they said news was that it
[00:44:55.140 --> 00:44:56.940]   was shut down because of this.
[00:44:56.940 --> 00:44:58.620]   We shut down for 36 hours.
[00:44:58.620 --> 00:45:04.620]   Get what was shut down for a day and a half, but there were no drones.
[00:45:04.620 --> 00:45:08.700]   That's still not definitive, I guess, but it's looking like there were no drones.
[00:45:08.700 --> 00:45:09.940]   That's just weird.
[00:45:09.940 --> 00:45:10.940]   It is.
[00:45:10.940 --> 00:45:15.940]   Everybody just calm down.
[00:45:15.940 --> 00:45:19.140]   It'll be OK.
[00:45:19.140 --> 00:45:21.180]   Oh, I think that that's a metaphor.
[00:45:21.180 --> 00:45:22.180]   That story is really a metaphor.
[00:45:22.180 --> 00:45:24.420]   You might have a 30 years war or two.
[00:45:24.420 --> 00:45:27.820]   Once we get on the other side of that, it'll be OK.
[00:45:27.820 --> 00:45:28.820]   It'll be happy.
[00:45:28.820 --> 00:45:29.820]   Yes.
[00:45:29.820 --> 00:45:32.300]   You know, there'll be a big time to mention it.
[00:45:32.300 --> 00:45:33.300]   To mention it.
[00:45:33.300 --> 00:45:34.300]   It's not a car sponsor.
[00:45:34.300 --> 00:45:36.460]   I see where you're going with this.
[00:45:36.460 --> 00:45:37.740]   It'll be OK.
[00:45:37.740 --> 00:45:39.740]   It's going to be OK.
[00:45:39.740 --> 00:45:40.740]   I'm sorry.
[00:45:40.740 --> 00:45:45.180]   We in fact have a sponsor called calm.com.
[00:45:45.180 --> 00:45:46.180]   Yes.
[00:45:46.180 --> 00:45:47.180]   You didn't know that?
[00:45:47.180 --> 00:45:48.180]   It will be OK.
[00:45:48.180 --> 00:45:49.180]   There you go.
[00:45:49.180 --> 00:45:50.180]   You didn't know that?
[00:45:50.180 --> 00:45:51.180]   You just were saying it.
[00:45:51.180 --> 00:45:52.180]   That was very good.
[00:45:52.180 --> 00:45:54.740]   I'm going to get you 25% off.
[00:45:54.740 --> 00:45:57.660]   So you might want to try this.
[00:45:57.660 --> 00:45:58.660]   Calm.
[00:45:58.660 --> 00:46:00.780]   I think you need it.
[00:46:00.780 --> 00:46:02.140]   So you calm down and you get back on face.
[00:46:02.140 --> 00:46:03.740]   I actually, I already have it.
[00:46:03.740 --> 00:46:06.740]   I have a, I subscribe some time ago.
[00:46:06.740 --> 00:46:08.940]   I love calm.
[00:46:08.940 --> 00:46:15.420]   Calm is the number one app to help you meditate, to help you sleep, and to help you relax.
[00:46:15.420 --> 00:46:19.100]   And I want to point out that it's not just meditation.
[00:46:19.100 --> 00:46:21.660]   It is a great sleep app.
[00:46:21.660 --> 00:46:25.740]   Apple's app of the year 2017.
[00:46:25.740 --> 00:46:30.900]   It's the, it is, was part of their best of 18, 2018 list.
[00:46:30.900 --> 00:46:33.060]   But how about imagine?
[00:46:33.060 --> 00:46:38.900]   Click on sleep because I want to listen to Matthew McConaughey reading me.
[00:46:38.900 --> 00:46:41.140]   A sleep story.
[00:46:41.140 --> 00:46:42.940]   Not just Matthew, but quite a few.
[00:46:42.940 --> 00:46:52.420]   If you go, you get 25% off a calm premium subscription right now, if you go to calmcalem.com/twig.
[00:46:52.420 --> 00:46:57.740]   And you'll get unlimited access to all of the content right now, today.
[00:46:57.740 --> 00:47:00.140]   Plus 25% off.
[00:47:00.140 --> 00:47:01.580]   There are guided meditations.
[00:47:01.580 --> 00:47:06.940]   And what I like about calm is a variety of different voices, a variety of different focuses.
[00:47:06.940 --> 00:47:09.460]   You can work on anxiety and stress.
[00:47:09.460 --> 00:47:13.980]   We know that stress is a huge health factor.
[00:47:13.980 --> 00:47:15.620]   Get rid of the stress.
[00:47:15.620 --> 00:47:18.140]   You could work on focus.
[00:47:18.140 --> 00:47:20.940]   There's a daily, daily new meditation, the daily calm.
[00:47:20.940 --> 00:47:23.540]   I really like it's different every day.
[00:47:23.540 --> 00:47:25.820]   But the sleep stories are often awesome.
[00:47:25.820 --> 00:47:31.020]   How about going to the lavender fields of France with Stephen Fry?
[00:47:31.020 --> 00:47:33.940]   Oh, that's a good idea.
[00:47:33.940 --> 00:47:35.180]   Isn't that a great idea?
[00:47:35.180 --> 00:47:37.180]   That's a very good idea.
[00:47:37.180 --> 00:47:38.180]   I'm going to get one.
[00:47:38.180 --> 00:47:41.140]   I have to log in right now to my calm account so I can do this for you because I really
[00:47:41.140 --> 00:47:42.340]   want to play some of this.
[00:47:42.340 --> 00:47:43.340]   Bob Ross.
[00:47:43.340 --> 00:47:45.340]   It's like Bob Ross.
[00:47:45.340 --> 00:47:46.340]   Yeah, well, it's right there.
[00:47:46.340 --> 00:47:47.340]   It's painting with Bob Ross.
[00:47:47.340 --> 00:47:48.940]   It's a way to put you to sleep.
[00:47:48.940 --> 00:47:49.940]   Actually, it's brilliant.
[00:47:49.940 --> 00:47:50.940]   Yeah.
[00:47:50.940 --> 00:47:51.940]   You want to hear Matthew McConaughey?
[00:47:51.940 --> 00:47:52.940]   Yes.
[00:47:52.940 --> 00:47:53.940]   It's really good.
[00:47:53.940 --> 00:47:54.940]   It's really good.
[00:47:54.940 --> 00:47:55.940]   Listen.
[00:47:55.940 --> 00:47:56.940]   Shh.
[00:47:56.940 --> 00:47:57.940]   Shh.
[00:47:57.940 --> 00:47:58.940]   Quiet.
[00:47:58.940 --> 00:47:59.940]   Well, hello there.
[00:47:59.940 --> 00:48:06.220]   Tonight, I'll be reading a special sleep story called Wonder.
[00:48:06.220 --> 00:48:07.300]   All right.
[00:48:07.300 --> 00:48:12.460]   Before we begin, as you settle in under the covers with your head easing into the pillow
[00:48:12.460 --> 00:48:14.460]   and your body sinking into the...
[00:48:14.460 --> 00:48:16.260]   Oh, I love you, Matthew.
[00:48:16.260 --> 00:48:19.540]   So I think there's a few people who would like that.
[00:48:19.540 --> 00:48:20.540]   Okay.
[00:48:20.540 --> 00:48:21.540]   You don't want that one?
[00:48:21.540 --> 00:48:22.540]   Let's see what else we got here.
[00:48:22.540 --> 00:48:23.540]   I want that one.
[00:48:23.540 --> 00:48:24.540]   I'm like, yes.
[00:48:24.540 --> 00:48:25.540]   I know.
[00:48:25.540 --> 00:48:26.540]   I hate it.
[00:48:26.540 --> 00:48:27.540]   Fuck me in.
[00:48:27.540 --> 00:48:28.540]   Here's the Bob Ross one.
[00:48:28.540 --> 00:48:29.540]   It starts a little energetic.
[00:48:29.540 --> 00:48:36.860]   It's going to calm down.
[00:48:36.860 --> 00:48:38.500]   Shh.
[00:48:38.500 --> 00:48:39.500]   Calm.com/twig.
[00:48:39.500 --> 00:48:49.380]   Hello, I'm Bob Ross, and I'd like to welcome you to series 31 of The Joy of Painting.
[00:48:49.380 --> 00:48:52.100]   Some people that would put them right to sleep.
[00:48:52.100 --> 00:48:54.100]   There you go.
[00:48:54.100 --> 00:48:59.500]   Anyway, I want to find... he has happy little Z's.
[00:48:59.500 --> 00:49:01.780]   There's just so much nice stuff on here.
[00:49:01.780 --> 00:49:05.220]   This is to help you go to sleep.
[00:49:05.220 --> 00:49:07.460]   Help you go to sleep.
[00:49:07.460 --> 00:49:09.060]   And some of them are real stories.
[00:49:09.060 --> 00:49:12.300]   My mom used to read me The Wind and the Willows.
[00:49:12.300 --> 00:49:13.300]   Hi.
[00:49:13.300 --> 00:49:16.460]   Are you ready for a sleep story?
[00:49:16.460 --> 00:49:20.820]   Today, I'll be reading from The Wind and the Willows.
[00:49:20.820 --> 00:49:22.420]   This is such a good way to go to sleep.
[00:49:22.420 --> 00:49:25.420]   I'm just telling you, but meditate, sleep stories.
[00:49:25.420 --> 00:49:26.700]   They have music.
[00:49:26.700 --> 00:49:30.220]   They even have, which is nice, you can just get a page.
[00:49:30.220 --> 00:49:32.100]   You can put up on your screen.
[00:49:32.100 --> 00:49:33.100]   It's very peaceful.
[00:49:33.100 --> 00:49:35.620]   And you can just relax.
[00:49:35.620 --> 00:49:38.780]   This is the best money you'll spend this year.
[00:49:38.780 --> 00:49:42.020]   C-A-L-M.com/twig.
[00:49:42.020 --> 00:49:45.340]   If you go, you'll get 25% off a calm premium subscription.
[00:49:45.340 --> 00:49:48.060]   And as I said, it's even more restful.
[00:49:48.060 --> 00:49:51.540]   We're going to get you 25% off a calm.
[00:49:51.540 --> 00:49:58.780]   I want them to use my voice because I've been told I put people to sleep a lot.
[00:49:58.780 --> 00:50:01.780]   Calm.com/twig.
[00:50:01.780 --> 00:50:07.180]   Oh, I love calm.
[00:50:07.180 --> 00:50:08.700]   Don't you feel better already?
[00:50:08.700 --> 00:50:09.860]   See, I told you.
[00:50:09.860 --> 00:50:10.860]   It's all we need.
[00:50:10.860 --> 00:50:11.860]   Yeah.
[00:50:11.860 --> 00:50:12.860]   Here I'll play.
[00:50:12.860 --> 00:50:15.540]   Now, I'm going to stop the show right now and go lay down.
[00:50:15.540 --> 00:50:17.020]   It's relaxing, isn't it?
[00:50:17.020 --> 00:50:18.020]   It is.
[00:50:18.020 --> 00:50:19.020]   See, you'll feel better now.
[00:50:19.020 --> 00:50:24.020]   Look at this.
[00:50:24.020 --> 00:50:27.020]   Here's the daily meditation.
[00:50:27.020 --> 00:50:29.020]   Just going to relax now.
[00:50:29.020 --> 00:50:30.020]   Remain.
[00:50:30.020 --> 00:50:33.020]   Start by finding a comfortable position.
[00:50:33.020 --> 00:50:38.020]   How about relationships with others, breaking habits, winning the poo.
[00:50:38.020 --> 00:50:41.020]   World Cup penalty series.
[00:50:41.020 --> 00:50:42.020]   Wait a minute.
[00:50:42.020 --> 00:50:43.020]   What is this?
[00:50:43.020 --> 00:50:47.020]   Welcome to this special meditation session for World Cup fans.
[00:50:47.020 --> 00:50:53.300]   Today, we'll learn to soften the anxiety that arises as we follow our teams highs and
[00:50:53.300 --> 00:50:54.300]   lows.
[00:50:54.300 --> 00:50:55.300]   See?
[00:50:55.300 --> 00:50:56.300]   See?
[00:50:56.300 --> 00:50:57.300]   See?
[00:50:57.300 --> 00:50:58.300]   See?
[00:50:58.300 --> 00:50:59.300]   See?
[00:50:59.300 --> 00:51:00.300]   See?
[00:51:00.300 --> 00:51:01.300]   See?
[00:51:01.300 --> 00:51:02.300]   If you're getting stressed from watching too much soccer is, of course, the penalty shoot-out.
[00:51:02.300 --> 00:51:04.100]   I love calm.
[00:51:04.100 --> 00:51:05.180]   They are so great.
[00:51:05.180 --> 00:51:07.100]   This is the greatest company.
[00:51:07.100 --> 00:51:13.420]   They're very creative, very clever, 100 plus guided meditations, exclusive music tracks
[00:51:13.420 --> 00:51:14.620]   to by the way.
[00:51:14.620 --> 00:51:15.780]   This is something it's hard to find.
[00:51:15.780 --> 00:51:19.620]   I really appreciate.
[00:51:19.620 --> 00:51:20.780]   Let's see.
[00:51:20.780 --> 00:51:22.100]   I might do this.
[00:51:22.100 --> 00:51:23.620]   I'm going to get a massage later today.
[00:51:23.620 --> 00:51:27.620]   I might just have this playing during my massage.
[00:51:27.620 --> 00:51:31.420]   Isn't that nice?
[00:51:31.420 --> 00:51:33.900]   It's very relaxing.
[00:51:33.900 --> 00:51:36.060]   Very calming.
[00:51:36.060 --> 00:51:37.620]   Calm.com/tweg.
[00:51:37.620 --> 00:51:45.140]   Don't let World Cup soccer worry you with calm.
[00:51:45.140 --> 00:51:46.140]   Isn't that nice?
[00:51:46.140 --> 00:51:47.780]   Don't let apples announce.
[00:51:47.780 --> 00:51:50.980]   Don't let apples revenue.
[00:51:50.980 --> 00:51:58.420]   By the way, just before the show, the chair says, "Apple's stock has stopped trading."
[00:51:58.420 --> 00:51:59.420]   What?
[00:51:59.420 --> 00:52:02.260]   That usually is something serious.
[00:52:02.260 --> 00:52:03.260]   Yeah, that's...
[00:52:03.260 --> 00:52:05.260]   In this case, it wasn't that serious.
[00:52:05.260 --> 00:52:09.460]   They put out an update to their revenue guidance for the last quarter, which ended December
[00:52:09.460 --> 00:52:10.700]   29th.
[00:52:10.700 --> 00:52:11.860]   I guess it'll be another...
[00:52:11.860 --> 00:52:15.580]   It usually takes about a month before they get the quarterly results, but Tim Cook just
[00:52:15.580 --> 00:52:19.940]   wanted everybody to calm down and just be aware the revenue is going to be down a little
[00:52:19.940 --> 00:52:20.940]   bit.
[00:52:20.940 --> 00:52:21.940]   It's going to be down.
[00:52:21.940 --> 00:52:22.940]   Which apple just doesn't do?
[00:52:22.940 --> 00:52:23.940]   Apple always.
[00:52:23.940 --> 00:52:24.940]   Which is low balls.
[00:52:24.940 --> 00:52:32.500]   Already, the stock and after hours trading is down 7% because Tim Cook said, "Greater China
[00:52:32.500 --> 00:52:36.340]   did not respond to our offers of fan phones."
[00:52:36.340 --> 00:52:39.780]   So there you have it.
[00:52:39.780 --> 00:52:41.500]   This is why you need calm.
[00:52:41.500 --> 00:52:44.180]   Stay more than ever.
[00:52:44.180 --> 00:52:49.020]   No, too calm to do the new.
[00:52:49.020 --> 00:52:53.340]   I'm so relaxed.
[00:52:53.340 --> 00:52:57.540]   This is when you should tell me about European G2BR things that I wouldn't care about.
[00:52:57.540 --> 00:52:59.100]   Oh, that's true.
[00:52:59.100 --> 00:53:00.100]   Let's see.
[00:53:00.100 --> 00:53:02.020]   Well, there's not a lot of huge news this week.
[00:53:02.020 --> 00:53:03.500]   I got to be honest.
[00:53:03.500 --> 00:53:04.500]   Yeah.
[00:53:04.500 --> 00:53:07.260]   Amazon's going to expand whole food stores.
[00:53:07.260 --> 00:53:15.180]   I finally got suckered into putting the whole food app, the Amazon app on my phone because
[00:53:15.180 --> 00:53:20.060]   I was in whole food buying goods for my Christmas dinner.
[00:53:20.060 --> 00:53:21.060]   Did you actually get any discounts?
[00:53:21.060 --> 00:53:23.260]   Look, they don't put that much on.
[00:53:23.260 --> 00:53:24.260]   Yeah, there's...
[00:53:24.260 --> 00:53:25.260]   And all you do is you hold...
[00:53:25.260 --> 00:53:26.260]   But you know what?
[00:53:26.260 --> 00:53:28.220]   I'm going to tell people, use this barcode.
[00:53:28.220 --> 00:53:33.700]   If you want to save, just use mine.
[00:53:33.700 --> 00:53:38.540]   Some members get weekly deep discounts and an extra 10% off sales prices, exclusions,
[00:53:38.540 --> 00:53:39.540]   apply.
[00:53:39.540 --> 00:53:40.940]   And then you see what savings.
[00:53:40.940 --> 00:53:42.900]   Your savings were.
[00:53:42.900 --> 00:53:45.700]   So it shows you your store.
[00:53:45.700 --> 00:53:49.820]   Well, that's a pretty good deal on ground turkey balls.
[00:53:49.820 --> 00:53:52.260]   Regularly 5.99 a pound.
[00:53:52.260 --> 00:53:55.100]   Amazon Prime 3.99 a pound.
[00:53:55.100 --> 00:53:56.100]   Pre-made meatballs?
[00:53:56.100 --> 00:53:57.100]   I don't...
[00:53:57.100 --> 00:53:58.100]   Or is it just ground turkey?
[00:53:58.100 --> 00:54:00.580]   It just says animal welfare rated ground turkey.
[00:54:00.580 --> 00:54:02.500]   Oh, it's ground turkey.
[00:54:02.500 --> 00:54:03.500]   Oh, okay.
[00:54:03.500 --> 00:54:04.500]   Sorry, I was just like...
[00:54:04.500 --> 00:54:05.500]   What?
[00:54:05.500 --> 00:54:06.500]   It's a...
[00:54:06.500 --> 00:54:07.500]   Product.
[00:54:07.500 --> 00:54:08.500]   Salad greens.
[00:54:08.500 --> 00:54:12.500]   Two for six dollars, normally three 99 and four 49 each.
[00:54:12.500 --> 00:54:13.500]   Yeah, it's...
[00:54:13.500 --> 00:54:15.900]   You know, hey, some savings.
[00:54:15.900 --> 00:54:17.300]   Look at that.
[00:54:17.300 --> 00:54:19.900]   Grapefruits, usually 250 pound, now a dollar pound.
[00:54:19.900 --> 00:54:20.900]   Oh, wow.
[00:54:20.900 --> 00:54:21.900]   That's good.
[00:54:21.900 --> 00:54:22.900]   Grapefruits are always cheap in January.
[00:54:22.900 --> 00:54:23.900]   I can't eat them on my stats.
[00:54:23.900 --> 00:54:25.900]   It would kill me.
[00:54:25.900 --> 00:54:27.900]   That's actually something that would kill me right there.
[00:54:27.900 --> 00:54:28.900]   But you can't help me.
[00:54:28.900 --> 00:54:29.900]   See, don't...
[00:54:29.900 --> 00:54:30.900]   It would be interesting.
[00:54:30.900 --> 00:54:31.900]   No.
[00:54:31.900 --> 00:54:33.900]   Amazon do that and do you show you that?
[00:54:33.900 --> 00:54:34.900]   Are you showing you that?
[00:54:34.900 --> 00:54:35.900]   Yeah.
[00:54:35.900 --> 00:54:36.900]   Don't eat grapefruit.
[00:54:36.900 --> 00:54:37.900]   I don't have grapefruit.
[00:54:37.900 --> 00:54:38.900]   Once it's a rare while, I'll have a grapefruit.
[00:54:38.900 --> 00:54:39.900]   So, I don't know.
[00:54:39.900 --> 00:54:45.900]   Before I give out medical advice, let me just see what it says on the internet.
[00:54:45.900 --> 00:54:48.500]   I thought it was like a cancer drug that says you can't do that.
[00:54:48.500 --> 00:54:49.900]   I'm pretty sure.
[00:54:49.900 --> 00:54:50.900]   I'm pretty sure.
[00:54:50.900 --> 00:54:51.900]   Yeah.
[00:54:51.900 --> 00:54:52.900]   Patience can eat.
[00:54:52.900 --> 00:54:55.020]   Don't eat a large amount of grapefruit.
[00:54:55.020 --> 00:54:57.020]   Well, taking this step.
[00:54:57.020 --> 00:54:58.020]   Here it is.
[00:54:58.020 --> 00:55:01.500]   In your fears with healthline.com says...
[00:55:01.500 --> 00:55:03.500]   Why you shouldn't mix grapefruit and statins.
[00:55:03.500 --> 00:55:07.180]   Yeah, I'm just saying.
[00:55:07.180 --> 00:55:08.380]   And I'm on lipotores.
[00:55:08.380 --> 00:55:12.500]   That's the one that's particularly bad with Lipitor and Zocor.
[00:55:12.500 --> 00:55:13.900]   So there.
[00:55:13.900 --> 00:55:15.900]   It could cause muscle breakdown.
[00:55:15.900 --> 00:55:17.460]   Well, who says I have it?
[00:55:17.460 --> 00:55:18.460]   Liver damage.
[00:55:18.460 --> 00:55:20.460]   Digestive problems at Grease Blood Sugar.
[00:55:20.460 --> 00:55:21.460]   I don't mind does that.
[00:55:21.460 --> 00:55:24.020]   And neurological side effects, which explains an awful lot.
[00:55:24.020 --> 00:55:25.020]   It does.
[00:55:25.020 --> 00:55:27.620]   I'm feeling sleepy.
[00:55:27.620 --> 00:55:29.620]   Um...
[00:55:29.620 --> 00:55:32.140]   Okay, this one I am mad about.
[00:55:32.140 --> 00:55:34.140]   New York Times.
[00:55:34.140 --> 00:55:38.460]   Cooper Quinton, who is from the Electronic Frontier Foundation.
[00:55:38.460 --> 00:55:40.460]   Our cell phones aren't safe.
[00:55:40.460 --> 00:55:44.380]   Actually, I think this is a very important article.
[00:55:44.380 --> 00:55:46.220]   Security flaws threaten our privacy and bank accounts.
[00:55:46.220 --> 00:55:47.420]   So why aren't we fixing them?
[00:55:47.420 --> 00:55:49.180]   This is not about privacy, Jeff.
[00:55:49.180 --> 00:55:52.860]   Well, I didn't even click on the headline originally because I thought it was another
[00:55:52.860 --> 00:55:54.460]   negative brain cancer story.
[00:55:54.460 --> 00:55:55.460]   No.
[00:55:55.460 --> 00:55:57.660]   It's actually about SS7.
[00:55:57.660 --> 00:56:01.180]   Which we've been talking about for years on security now.
[00:56:01.180 --> 00:56:09.340]   It's the software signaling system 7 that runs in all cell phones.
[00:56:09.340 --> 00:56:11.700]   It's part of the radio.
[00:56:11.700 --> 00:56:17.180]   There have been weaknesses we've known about for years in SS7.
[00:56:17.180 --> 00:56:19.460]   Nobody's doing anything about it and they never have.
[00:56:19.460 --> 00:56:23.620]   And he talks about in 2017, criminals took advantage, for instance.
[00:56:23.620 --> 00:56:29.740]   This is just one of many examples of a SS7 weaknesses to carry out financial fraud by
[00:56:29.740 --> 00:56:34.540]   redirecting and intercepting text messages containing one-time passwords for bank customers
[00:56:34.540 --> 00:56:37.260]   in Germany.
[00:56:37.260 --> 00:56:40.100]   It's part, you know, you've heard about all those stingrays?
[00:56:40.100 --> 00:56:43.180]   Basically, it lets you spoof a cell tower.
[00:56:43.180 --> 00:56:47.900]   So if you want to spoof a cell tower, people feel that cell networks are very secure.
[00:56:47.900 --> 00:56:48.900]   They're not.
[00:56:48.900 --> 00:56:50.700]   So you're like, "Hey, I'm on a cell network."
[00:56:50.700 --> 00:56:54.940]   But I was talking to somebody in the security world and they were like, "Yeah, we had a
[00:56:54.940 --> 00:57:03.140]   CEO of a Fortune 500 company whose cell phone was talking to a fake cell tower, basically
[00:57:03.140 --> 00:57:04.780]   that they had set up.
[00:57:04.780 --> 00:57:06.780]   That some hackers have set up."
[00:57:06.780 --> 00:57:07.780]   Yeah, a stingray.
[00:57:07.780 --> 00:57:08.780]   It's a stingray.
[00:57:08.780 --> 00:57:10.780]   And they're all over Washington, D.C., by the way.
[00:57:10.780 --> 00:57:11.780]   Yeah.
[00:57:11.780 --> 00:57:12.780]   It's a big deal.
[00:57:12.780 --> 00:57:16.140]   SS7, which was written in 1975.
[00:57:16.140 --> 00:57:17.140]   Oh.
[00:57:17.140 --> 00:57:19.540]   That's older than I am.
[00:57:19.540 --> 00:57:21.300]   It's older than Stacy.
[00:57:21.300 --> 00:57:22.900]   Oh, shush, Stacy, shush.
[00:57:22.900 --> 00:57:25.260]   I'm just saying that's good perspective.
[00:57:25.260 --> 00:57:26.260]   I'm pretty old.
[00:57:26.260 --> 00:57:28.500]   So this is actually...
[00:57:28.500 --> 00:57:29.500]   I have lots of viruses.
[00:57:29.500 --> 00:57:35.540]   I think it's bad to say this headline is bad because it's not quite really the issue
[00:57:35.540 --> 00:57:38.980]   is they're a security nightmare and this is true.
[00:57:38.980 --> 00:57:44.260]   Well, I think, I mean, and I used to run into this a lot when I was writing about technical
[00:57:44.260 --> 00:57:48.660]   topics like this, this is hard to like headline and make people care about.
[00:57:48.660 --> 00:57:50.660]   It really is because you have to explain so much.
[00:57:50.660 --> 00:57:52.700]   So the headline on stuff like this is just a thing.
[00:57:52.700 --> 00:57:53.700]   Oh, I know.
[00:57:53.700 --> 00:57:56.380]   You know, it was on 60 minutes, two years ago.
[00:57:56.380 --> 00:57:57.380]   Yeah.
[00:57:57.380 --> 00:57:58.940]   They showed it being hacked.
[00:57:58.940 --> 00:58:02.740]   They showed a senator who volunteered for this.
[00:58:02.740 --> 00:58:05.460]   They showed how easy it was to hack his phone.
[00:58:05.460 --> 00:58:06.460]   And nothing was...
[00:58:06.460 --> 00:58:07.460]   A senator, a U.N.
[00:58:07.460 --> 00:58:10.420]   United States, nothing has happened.
[00:58:10.420 --> 00:58:12.860]   So anyway, I mentioned it again.
[00:58:12.860 --> 00:58:16.420]   Not that anything will happen, but I mentioned it again.
[00:58:16.420 --> 00:58:21.180]   So I'm excited because the story...
[00:58:21.180 --> 00:58:24.580]   This was an interesting one from the New York magazine about...
[00:58:24.580 --> 00:58:25.860]   Oh, this is a good...
[00:58:25.860 --> 00:58:28.420]   They've been doing great stuff lately.
[00:58:28.420 --> 00:58:30.580]   That's good because they're charging money for it now.
[00:58:30.580 --> 00:58:31.580]   Oh, no.
[00:58:31.580 --> 00:58:33.660]   I got to pay for another pay wall.
[00:58:33.660 --> 00:58:35.460]   Yeah, pay wall.
[00:58:35.460 --> 00:58:36.900]   Courts has a pay wall now.
[00:58:36.900 --> 00:58:37.900]   They have a pay wall.
[00:58:37.900 --> 00:58:38.900]   Everybody.
[00:58:38.900 --> 00:58:41.060]   Oh, the God's children think they're worth a pay wall.
[00:58:41.060 --> 00:58:44.140]   But see, and they are individually.
[00:58:44.140 --> 00:58:45.900]   The problem is I don't want to...
[00:58:45.900 --> 00:58:46.900]   Yeah, Bloomberg.
[00:58:46.900 --> 00:58:49.340]   I still haven't paid for Bloomberg because I just...
[00:58:49.340 --> 00:58:51.500]   So you're talking about which story on...
[00:58:51.500 --> 00:58:53.020]   How much did the Internet escape?
[00:58:53.020 --> 00:58:56.100]   It turns out a lot of it, actually.
[00:58:56.100 --> 00:58:58.260]   This is about click fraud, basically.
[00:58:58.260 --> 00:59:04.620]   Yes, and it kind of fits in with this idea that basically bots have taken over the web.
[00:59:04.620 --> 00:59:11.540]   And we're getting to this worldview where there are no people and it's just bots fighting
[00:59:11.540 --> 00:59:13.420]   bots all the time.
[00:59:13.420 --> 00:59:17.660]   Here is a fascinating statistic from this article.
[00:59:17.660 --> 00:59:25.740]   For a period of time in 2013, the Times reported this year, but five years ago, half of YouTube
[00:59:25.740 --> 00:59:30.060]   traffic was bots masquerading as people.
[00:59:30.060 --> 00:59:35.100]   A portion so high that employees feared an inflection point after which YouTube systems
[00:59:35.100 --> 00:59:42.060]   for detecting fraudulent traffic would begin to regard bots traffic as real and human traffic
[00:59:42.060 --> 00:59:43.140]   as fake.
[00:59:43.140 --> 00:59:44.140]   They called this...
[00:59:44.140 --> 00:59:45.140]   Oh my gosh.
[00:59:45.140 --> 00:59:48.020]   It's like reaching consensus on the blockchain.
[00:59:48.020 --> 00:59:49.020]   Yes.
[00:59:49.020 --> 00:59:53.660]   They called this hypothetical event the inversion.
[00:59:53.660 --> 00:59:55.260]   Now that was five years ago.
[00:59:55.260 --> 00:59:58.420]   Do you think there are fewer bots on YouTube now?
[00:59:58.420 --> 01:00:03.380]   I don't think so.
[01:00:03.380 --> 01:00:07.340]   I don't think so.
[01:00:07.340 --> 01:00:13.260]   And that's always puzzled me because YouTube, the view number, viewership numbers on YouTube
[01:00:13.260 --> 01:00:17.460]   are stunning and they're so fast.
[01:00:17.460 --> 01:00:20.860]   And it just never made sense to me.
[01:00:20.860 --> 01:00:24.100]   And it doesn't make sense to advertisers, by the way, because it's really cheap to buy
[01:00:24.100 --> 01:00:25.260]   YouTube impressions.
[01:00:25.260 --> 01:00:26.260]   Oh, yeah.
[01:00:26.260 --> 01:00:31.060]   No, I have a friend who does...
[01:00:31.060 --> 01:00:34.700]   Who pays people to juice his YouTube desk.
[01:00:34.700 --> 01:00:43.580]   At YouTube, the company says, "Only a tiny fraction of its traffic is fake, but you can
[01:00:43.580 --> 01:00:51.900]   buy 5,000 YouTube views for as low as $15."
[01:00:51.900 --> 01:00:55.860]   But the reason, part of the reason this is people reward that.
[01:00:55.860 --> 01:00:59.220]   People are like, "Yeah, I can't even take you seriously unless you have this many YouTube
[01:00:59.220 --> 01:01:00.220]   views."
[01:01:00.220 --> 01:01:01.220]   That's right.
[01:01:01.220 --> 01:01:02.220]   I'm like, "Yep."
[01:01:02.220 --> 01:01:06.980]   For a while, there was a story was, "If you want to get a better deal as a Hollywood actor,
[01:01:06.980 --> 01:01:12.380]   if you want to get more money per movie, you have more Twitter followers."
[01:01:12.380 --> 01:01:15.860]   Is that still the case or do people finally twigs to the notion that really...
[01:01:15.860 --> 01:01:16.860]   That's a good question.
[01:01:16.860 --> 01:01:17.860]   I'll bet...
[01:01:17.860 --> 01:01:20.020]   I don't know, but I'll bet not, because he can buy them and all that.
[01:01:20.020 --> 01:01:22.460]   It's so easy to buy them.
[01:01:22.460 --> 01:01:23.460]   But that's the...
[01:01:23.460 --> 01:01:24.460]   I have never bought my Twitter followers.
[01:01:24.460 --> 01:01:26.060]   Neither are I.
[01:01:26.060 --> 01:01:29.140]   I have no Twitter followers.
[01:01:29.140 --> 01:01:30.700]   I used to have 600,000.
[01:01:30.700 --> 01:01:34.580]   Although I noticed ever since I deactivated my account, that number is dwindled.
[01:01:34.580 --> 01:01:36.380]   I would imagine.
[01:01:36.380 --> 01:01:42.020]   It had been my experience that the less I tweeted, the more followers I got.
[01:01:42.020 --> 01:01:46.100]   As long as you say nothing, it's the "Vanna White" effect.
[01:01:46.100 --> 01:01:47.780]   People think you're on their side.
[01:01:47.780 --> 01:01:51.980]   The minute you say something, people will go on on following that guy.
[01:01:51.980 --> 01:01:54.980]   Sorry, you guys.
[01:01:54.980 --> 01:01:55.980]   What?
[01:01:55.980 --> 01:01:56.980]   Oh, I yawned.
[01:01:56.980 --> 01:01:57.980]   Sorry.
[01:01:57.980 --> 01:01:58.980]   No.
[01:01:58.980 --> 01:01:59.980]   I yawned.
[01:01:59.980 --> 01:02:02.980]   It happened to be actually looking epic.
[01:02:02.980 --> 01:02:04.980]   This is your story.
[01:02:04.980 --> 01:02:07.540]   Matthew McConaughey does that to me all the time.
[01:02:07.540 --> 01:02:09.460]   He makes me all right.
[01:02:09.460 --> 01:02:13.140]   He doesn't make you buy a Lincoln or a bourbon, because that's...
[01:02:13.140 --> 01:02:14.740]   He's everywhere on my Hulu right now.
[01:02:14.740 --> 01:02:16.540]   I was like, "Good Lord."
[01:02:16.540 --> 01:02:19.300]   I'm having my car.
[01:02:19.300 --> 01:02:24.900]   These are two stories that I'm going to link together with little or no evidence to do so.
[01:02:24.900 --> 01:02:31.300]   And I'm going to suggest that they point to a larger problem just around the corner.
[01:02:31.300 --> 01:02:40.460]   9-1-1 emergency services go down across the US after CenturyLink outage, Thursday at noon
[01:02:40.460 --> 01:02:41.660]   eastern.
[01:02:41.660 --> 01:02:46.180]   CenturyLink went down.
[01:02:46.180 --> 01:02:47.740]   Which is what again?
[01:02:47.740 --> 01:02:50.820]   CenturyLink is one of the largest telecommunic...
[01:02:50.820 --> 01:02:51.940]   You never heard of them?
[01:02:51.940 --> 01:02:58.380]   One of the largest telecommunication providers in the US, they do internet and backbone services
[01:02:58.380 --> 01:03:00.540]   to AT&T and Verizon.
[01:03:00.540 --> 01:03:03.260]   You thought maybe AT&T and Verizon did their own?
[01:03:03.260 --> 01:03:04.260]   No.
[01:03:04.260 --> 01:03:05.700]   Well, they do their own.
[01:03:05.700 --> 01:03:08.860]   They also buy from CenturyLink.
[01:03:08.860 --> 01:03:09.860]   All right.
[01:03:09.860 --> 01:03:10.860]   In any case...
[01:03:10.860 --> 01:03:12.860]   Is it CenturyLink 5 level 3?
[01:03:12.860 --> 01:03:13.860]   I think so.
[01:03:13.860 --> 01:03:14.860]   Or are they just...
[01:03:14.860 --> 01:03:15.860]   No, I think so.
[01:03:15.860 --> 01:03:16.860]   Sorry.
[01:03:16.860 --> 01:03:17.860]   Yeah.
[01:03:17.860 --> 01:03:20.420]   So CenturyLink didn't say what caused the outage beyond this.
[01:03:20.420 --> 01:03:26.060]   It was an issue with a, quote, "Network element."
[01:03:26.060 --> 01:03:29.660]   But it took them an hour or so.
[01:03:29.660 --> 01:03:39.260]   Meanwhile, 9-1-1 services went down in many areas of Seattle, Stacey, Salt Lake City, Idaho,
[01:03:39.260 --> 01:03:42.460]   Oregon, Arizona, and Missouri.
[01:03:42.460 --> 01:03:49.620]   Police in Boston, Massachusetts tweeted CenturyLink outage affecting wireless 9-1-1 capability
[01:03:49.620 --> 01:03:51.780]   in Massachusetts.
[01:03:51.780 --> 01:03:54.260]   Then it came back a couple hours later.
[01:03:54.260 --> 01:03:56.700]   FCC says we're investigating.
[01:03:56.700 --> 01:04:01.860]   We know what that means.
[01:04:01.860 --> 01:04:04.500]   But then, okay, I'm a tie it to this.
[01:04:04.500 --> 01:04:10.060]   A malware attack brought down delivery of the LA Times and the Tribune papers across
[01:04:10.060 --> 01:04:15.340]   the United States, including the New York Times on the West Coast.
[01:04:15.340 --> 01:04:21.300]   As a server outage identified as a malware attack, which appears to have originated from
[01:04:21.300 --> 01:04:25.940]   outside the United States, hobbled computer systems and delayed weekend deliveries of
[01:04:25.940 --> 01:04:29.580]   the Los Angeles Times.
[01:04:29.580 --> 01:04:35.260]   So a couple things.
[01:04:35.260 --> 01:04:39.580]   I think we need to have for Harvard Business School folks, right?
[01:04:39.580 --> 01:04:43.700]   We need to have, understand, the economic importance of resiliency.
[01:04:43.700 --> 01:04:51.820]   And what we talk about today is that is not a priority in supply chains and all kinds of
[01:04:51.820 --> 01:04:58.540]   things because we've only been thinking about the dollar costs of things, right?
[01:04:58.540 --> 01:05:04.420]   And that's where I think as we get more technical, we are more reliant on technology.
[01:05:04.420 --> 01:05:05.900]   Sorry, more reliant on technology.
[01:05:05.900 --> 01:05:11.020]   We need to talk about resiliency and start creating case studies that understand the
[01:05:11.020 --> 01:05:16.660]   impact of losing that because these things happen a lot.
[01:05:16.660 --> 01:05:20.620]   And there are companies that design for this.
[01:05:20.620 --> 01:05:22.300]   And I think it's really important that they do.
[01:05:22.300 --> 01:05:23.740]   And actually, it's a topic at CES.
[01:05:23.740 --> 01:05:25.380]   I was really excited to see that.
[01:05:25.380 --> 01:05:29.020]   So this is just me thinking, you know, I'm like, Oh my gosh.
[01:05:29.020 --> 01:05:31.340]   Now, here's why I connect these.
[01:05:31.340 --> 01:05:34.580]   They both happened on Thursday.
[01:05:34.580 --> 01:05:37.180]   Uh huh.
[01:05:37.180 --> 01:05:41.860]   Now, I don't want to say there's a boogie man under the bed or in the closet, but I honestly
[01:05:41.860 --> 01:05:46.620]   think that nation states are trying out these weapons just to see what they can do.
[01:05:46.620 --> 01:05:50.980]   And I think this is a taste, a for taste.
[01:05:50.980 --> 01:05:55.300]   Yes, there's a, there's definitely an economic and civic reason Stacy that they should be
[01:05:55.300 --> 01:05:57.820]   redundant, but I think there's an even larger reason.
[01:05:57.820 --> 01:06:00.940]   I think that this is the next battle, battlefield.
[01:06:00.940 --> 01:06:02.420]   Mm hmm.
[01:06:02.420 --> 01:06:04.420]   Just saying.
[01:06:04.420 --> 01:06:12.220]   Uh, times in San Diego paper became aware of the problem near midnight on Thursday.
[01:06:12.220 --> 01:06:16.180]   Programmers worked to isolate the bug, which tribute and publishing identified as a malware
[01:06:16.180 --> 01:06:21.620]   attack, but at every turn, the programmers ran into additional issues after identifying
[01:06:21.620 --> 01:06:26.300]   the server outages of virus technology teams made progress Friday, quarantining it and
[01:06:26.300 --> 01:06:27.380]   bringing back servers.
[01:06:27.380 --> 01:06:30.340]   But some of their patches didn't hold.
[01:06:30.340 --> 01:06:31.580]   I don't even know what that means.
[01:06:31.580 --> 01:06:33.500]   It's not like a ship.
[01:06:33.500 --> 01:06:38.780]   Some of their security patches didn't hold and the virus began to re-infect the network.
[01:06:38.780 --> 01:06:43.460]   So, so let me talk about my friends in the newspaper business.
[01:06:43.460 --> 01:06:45.140]   And so happens.
[01:06:45.140 --> 01:06:48.020]   I was at the Chicago Tribune.
[01:06:48.020 --> 01:06:50.020]   They heard trunk.
[01:06:50.020 --> 01:06:56.380]   That's the company when they'd got their second CMS back in the 70s.
[01:06:56.380 --> 01:06:58.060]   And that's how old I am Stacy.
[01:06:58.060 --> 01:07:02.500]   I was doing CMSs before you were born.
[01:07:02.500 --> 01:07:08.740]   And they didn't build in any redundancy or enough memory because they wanted more features.
[01:07:08.740 --> 01:07:10.020]   We're dumb about this stuff.
[01:07:10.020 --> 01:07:12.020]   We're really dumb about this stuff.
[01:07:12.020 --> 01:07:13.020]   Yeah.
[01:07:13.020 --> 01:07:14.020]   It's a bad industry.
[01:07:14.020 --> 01:07:15.020]   It's just.
[01:07:15.020 --> 01:07:17.900]   So I don't know that it's it was like a Chinese or Russian hack.
[01:07:17.900 --> 01:07:18.900]   It could have done either.
[01:07:18.900 --> 01:07:20.100]   And I just waddled stockholder.
[01:07:20.100 --> 01:07:21.700]   Yes, I don't know.
[01:07:21.700 --> 01:07:26.460]   But Pam Dixon, executive director of World Privacy Forum and nonprofit public interest research
[01:07:26.460 --> 01:07:30.860]   group said quote, usually when someone tries to disrupt a significant digital resource
[01:07:30.860 --> 01:07:36.660]   like a newspaper, you're looking at an experienced and sophisticated hacker.
[01:07:36.660 --> 01:07:43.580]   Holidays are a well known time for mischief because organizations are more thinly staffed.
[01:07:43.580 --> 01:07:48.580]   This is from the LA Times, which was in fact held up on Sunday because of the attack
[01:07:48.580 --> 01:07:49.580]   on Thursday.
[01:07:49.580 --> 01:07:51.060]   I don't, you know, maybe they're not related.
[01:07:51.060 --> 01:07:55.540]   That's just when I see two major attacks like that on the same day of infrastructure
[01:07:55.540 --> 01:07:57.740]   that's fairly important.
[01:07:57.740 --> 01:08:01.140]   It seems like I feel like an alarm bell is ringing.
[01:08:01.140 --> 01:08:02.340]   All right.
[01:08:02.340 --> 01:08:06.100]   I do have good news and we're going to get to all the good news, the happy stuff in just
[01:08:06.100 --> 01:08:07.100]   a second.
[01:08:07.100 --> 01:08:10.620]   But first I want to tell you about cash fly because that's a happy thing.
[01:08:10.620 --> 01:08:11.620]   That makes me happy.
[01:08:11.620 --> 01:08:16.220]   I didn't get any calls over the holidays that our network was down, that our servers were
[01:08:16.220 --> 01:08:21.060]   down, that people couldn't download our shows because we're on cash fly.
[01:08:21.060 --> 01:08:23.140]   That's our content distribution network.
[01:08:23.140 --> 01:08:25.060]   And it is the best.
[01:08:25.060 --> 01:08:26.060]   It is the best.
[01:08:26.060 --> 01:08:31.460]   If your users, the seamless online experience, they want power your cider app with the cash
[01:08:31.460 --> 01:08:39.180]   fly CDN and be 30% faster than the competition.
[01:08:39.180 --> 01:08:43.900]   No matter what industry you're in, if you, if your website, if your content is tied to
[01:08:43.900 --> 01:08:48.460]   revenue, give your customer, customers the fast downloads they need with cash flights.
[01:08:48.460 --> 01:08:52.300]   What we do when you anytime you're seeing a show on our website or you're downloading
[01:08:52.300 --> 01:08:57.620]   it from the internet, you're getting it from a cash fly server near you.
[01:08:57.620 --> 01:08:59.820]   That's why it's faster.
[01:08:59.820 --> 01:09:06.180]   They deliver media rich content 10 times faster than traditional delivery methods up to 30.
[01:09:06.180 --> 01:09:13.620]   And this is the thing that blows my mind up to 30% faster than other major CDNs.
[01:09:13.620 --> 01:09:18.580]   And by the way, their service level agreement, their SLA 100%, not four, nine, five, nine,
[01:09:18.580 --> 01:09:23.700]   100, nine, 100%, they guarantee the best user experience for all your customers, no matter
[01:09:23.700 --> 01:09:26.900]   where they are, no matter what device they're on.
[01:09:26.900 --> 01:09:29.140]   We've been with cash fly for a decade.
[01:09:29.140 --> 01:09:33.460]   I wouldn't, I would never dream of changing.
[01:09:33.460 --> 01:09:36.540]   It was the best decision I ever made back in the infancy of Twitch.
[01:09:36.540 --> 01:09:40.780]   Join thousands of others like me who trust cash flies reliable network.
[01:09:40.780 --> 01:09:45.580]   LG does Microsoft does Adobe ours, Technica.
[01:09:45.580 --> 01:09:50.780]   No billing spikes either because they tailor your plan to your needs, to your yearly usage
[01:09:50.780 --> 01:09:51.780]   trends.
[01:09:51.780 --> 01:09:55.100]   So you don't have to worry about, you don't have to check your bill every five minutes,
[01:09:55.100 --> 01:09:57.980]   making sure you didn't exceed some number.
[01:09:57.980 --> 01:10:03.380]   In fact, on average, customers who switch to cash fly save more than 20%.
[01:10:03.380 --> 01:10:13.540]   So 30% faster for 20% less through the math than go to twit.cash fly.com where they're
[01:10:13.540 --> 01:10:19.100]   giving away a no pressure, no hard sell complimentary detail analysis of your current CDN bill and
[01:10:19.100 --> 01:10:20.220]   usage trends.
[01:10:20.220 --> 01:10:25.540]   And they could tell you if you'll save money, find out now, don't don't live in ignorance.
[01:10:25.540 --> 01:10:30.740]   Get the best for less twit twit twit cash fly.com.
[01:10:30.740 --> 01:10:32.540]   Thank you cash fly.
[01:10:32.540 --> 01:10:37.340]   And unlike tribute and company and unlike we weren't down, former companies, I never
[01:10:37.340 --> 01:10:38.540]   hear you complain.
[01:10:38.540 --> 01:10:39.540]   No, no, no.
[01:10:39.540 --> 01:10:43.420]   You were talking about that helm email server, Stacey.
[01:10:43.420 --> 01:10:45.500]   Oh, yeah, like forever.
[01:10:45.500 --> 01:10:47.140]   I bought one.
[01:10:47.140 --> 01:10:48.940]   You had someone on your show from it, right?
[01:10:48.940 --> 01:10:49.940]   Yeah.
[01:10:49.940 --> 01:10:52.540]   So he was on the screen savers Gilligan.
[01:10:52.540 --> 01:10:55.580]   No, that's not his name.
[01:10:55.580 --> 01:10:58.300]   Gary, Gary was on the screen savers.
[01:10:58.300 --> 01:11:03.140]   Gary Schritivossum, who is the one of the founders and the CEO and brought a helm and
[01:11:03.140 --> 01:11:04.140]   set it up.
[01:11:04.140 --> 01:11:05.140]   And I was so impressed.
[01:11:05.140 --> 01:11:06.140]   I bought one.
[01:11:06.140 --> 01:11:10.180]   They're not cheap, they're 500 bucks and you have to pay 100 bucks a year after the first
[01:11:10.180 --> 01:11:12.740]   year for the service.
[01:11:12.740 --> 01:11:17.060]   But it's a, but it's all I've tried to set up email servers before and it's hard to do.
[01:11:17.060 --> 01:11:18.060]   This works pretty well.
[01:11:18.060 --> 01:11:21.140]   I was very impressed.
[01:11:21.140 --> 01:11:22.820]   It's interesting.
[01:11:22.820 --> 01:11:28.020]   If you want to the idea is instead of storing your mail on somebody else's server, you store
[01:11:28.020 --> 01:11:31.260]   it on a server at home, there are all sorts of problems with doing that.
[01:11:31.260 --> 01:11:35.740]   For instance, your IP address, almost all residential IP addresses are blocked.
[01:11:35.740 --> 01:11:40.380]   Not only by your ISP who doesn't want you sending spam, but most recipients won't accept
[01:11:40.380 --> 01:11:44.180]   it because they say, you know, chances are that's a spam reflector.
[01:11:44.180 --> 01:11:46.500]   That's not going to be real email.
[01:11:46.500 --> 01:11:55.100]   So they solve that by you route your mail from your server to their server through a VPN.
[01:11:55.100 --> 01:11:59.420]   They use Amazon Web Services and they use IP blocks that are clear.
[01:11:59.420 --> 01:12:08.300]   They also turn on all of the, you know, reverse DNS, all the other D-Kim.
[01:12:08.300 --> 01:12:16.140]   I can't remember all the different forms for email authentication, D-Mark, that mean that
[01:12:16.140 --> 01:12:19.740]   the recipients are likely to not assume that you're spam.
[01:12:19.740 --> 01:12:23.220]   And in fact, I've had no problem sending mail out through it.
[01:12:23.220 --> 01:12:27.940]   I decided what I do, I got to get a fresh domain for it, which I'm not going to tell
[01:12:27.940 --> 01:12:28.940]   anybody.
[01:12:28.940 --> 01:12:33.900]   I tell you guys off the air because it's a way that it's like going to be now my personal
[01:12:33.900 --> 01:12:39.180]   family server if you want to get through to me because leoville.com is completely swamped.
[01:12:39.180 --> 01:12:44.900]   I tried it at first leoville.com and it was just like a mess.
[01:12:44.900 --> 01:12:45.900]   But this worked pretty well.
[01:12:45.900 --> 01:12:48.220]   So anyway, I know you mentioned this before.
[01:12:48.220 --> 01:12:51.460]   Oh, Stacey, having used it now, I think I could recommend it.
[01:12:51.460 --> 01:12:55.260]   It's not, you know, one thing, and they don't mention this, I think they need to, you have
[01:12:55.260 --> 01:13:00.460]   to have a smartphone to use it because of it, but this is brilliant, but they should really
[01:13:00.460 --> 01:13:02.300]   tell people ahead of time.
[01:13:02.300 --> 01:13:03.700]   Each user has to have a smartphone.
[01:13:03.700 --> 01:13:09.020]   The reason is this is your security token because you can't configure it unless you're
[01:13:09.020 --> 01:13:10.020]   in proximity.
[01:13:10.020 --> 01:13:13.540]   You use Bluetooth LAs, you have to be near it.
[01:13:13.540 --> 01:13:19.580]   And then you get the credentials for it, but you have to do it through an app on a smartphone.
[01:13:19.580 --> 01:13:20.780]   So it's no pointo.
[01:13:20.780 --> 01:13:27.380]   Well, it's more secure, but it's like for a business, if you set this up for business,
[01:13:27.380 --> 01:13:33.020]   every employee has to go stand next to the server or it just happens to count.
[01:13:33.020 --> 01:13:35.700]   So that's a little weird, right?
[01:13:35.700 --> 01:13:38.060]   But I think for a home server, it'd be fine.
[01:13:38.060 --> 01:13:39.060]   Well, not ever.
[01:13:39.060 --> 01:13:42.780]   I mean, certain my daughter still doesn't have a phone.
[01:13:42.780 --> 01:13:43.780]   Right.
[01:13:43.780 --> 01:13:47.900]   So I guess if you had a spare phone, you could set it up for, but each phone actually gets
[01:13:47.900 --> 01:13:48.900]   to, oh, going.
[01:13:48.900 --> 01:13:51.900]   Well, each phone is, it's like a token.
[01:13:51.900 --> 01:13:55.460]   This becomes your security token, basically.
[01:13:55.460 --> 01:13:59.220]   So this isn't related to the news, but it's totally related to Google.
[01:13:59.220 --> 01:14:06.780]   And I'm going to throw this out here because I, in the middle of December, we switched from
[01:14:06.780 --> 01:14:13.700]   being in Madam A. So an Amazon Alexa household to being a cool household.
[01:14:13.700 --> 01:14:15.540]   You just said the name.
[01:14:15.540 --> 01:14:16.540]   I know.
[01:14:16.540 --> 01:14:17.540]   Say Echo.
[01:14:17.540 --> 01:14:18.540]   Sorry.
[01:14:18.540 --> 01:14:19.540]   I'm just teasing.
[01:14:19.540 --> 01:14:20.540]   Go ahead.
[01:14:20.540 --> 01:14:35.580]   We started this and I'm going to do a post after about a month in, but we're encountering
[01:14:35.580 --> 01:14:37.100]   all kinds of issues.
[01:14:37.100 --> 01:14:42.540]   And the biggest issue we have is I had turned on personalization.
[01:14:42.540 --> 01:14:46.180]   And I've complained before because Google doesn't always recognize my voice.
[01:14:46.180 --> 01:14:47.180]   So that's one part.
[01:14:47.180 --> 01:14:48.900]   But my husband trained his voice.
[01:14:48.900 --> 01:14:50.900]   I quote unquote, "trained mind better."
[01:14:50.900 --> 01:14:53.660]   So it gets me about 50% of the time.
[01:14:53.660 --> 01:14:56.260]   But I don't know what to do with my daughter.
[01:14:56.260 --> 01:15:00.180]   And it is a pain in the butt to set up the personalization.
[01:15:00.180 --> 01:15:05.740]   And she has to have an app to do the voice training on her phone.
[01:15:05.740 --> 01:15:10.380]   And so I'm very curious if she doesn't like that or anything like that.
[01:15:10.380 --> 01:15:13.380]   She hasn't, so she has an iPhone that she uses.
[01:15:13.380 --> 01:15:19.220]   And she has a Gmail address that's personal to her.
[01:15:19.220 --> 01:15:24.340]   And then she has her Google stuff for school, which is not linked out to any of the Google
[01:15:24.340 --> 01:15:25.820]   ecosystem.
[01:15:25.820 --> 01:15:30.940]   So but yeah, it's really tough to like...
[01:15:30.940 --> 01:15:33.980]   We're getting into a world where you have to have one of these now.
[01:15:33.980 --> 01:15:35.820]   Well, you should have to have...
[01:15:35.820 --> 01:15:36.820]   No, I agree.
[01:15:36.820 --> 01:15:37.820]   It's problematic.
[01:15:37.820 --> 01:15:38.820]   That.
[01:15:38.820 --> 01:15:40.780]   If you just have that, then that's fine.
[01:15:40.780 --> 01:15:44.540]   But then I shouldn't have to do like an email account.
[01:15:44.540 --> 01:15:47.300]   Like with Amazon, I have to do an email account.
[01:15:47.300 --> 01:15:48.300]   Right.
[01:15:48.300 --> 01:15:52.300]   So well, that's one of the things I'm going to use the helm for is I'm creating...
[01:15:52.300 --> 01:15:54.220]   An email for your house?
[01:15:54.220 --> 01:15:56.140]   Well, yes, basically.
[01:15:56.140 --> 01:15:57.140]   So I have...
[01:15:57.140 --> 01:15:58.740]   I don't want to say the domain, but I have a domain name.
[01:15:58.740 --> 01:16:00.860]   Let's say it's myemail.com.
[01:16:00.860 --> 01:16:03.860]   And then you can have anything that goes to myemail.com.
[01:16:03.860 --> 01:16:09.620]   So I'm creating weird account names for everything that are non-guessable.
[01:16:09.620 --> 01:16:12.860]   That gives me some security, but also that I can trace.
[01:16:12.860 --> 01:16:14.340]   So I know.
[01:16:14.340 --> 01:16:17.620]   So that's what I would do with the Echo, for instance, is I would set it up with one of
[01:16:17.620 --> 01:16:21.540]   these special names that only it has.
[01:16:21.540 --> 01:16:24.220]   Do you see what I'm saying?
[01:16:24.220 --> 01:16:25.220]   So it could be...
[01:16:25.220 --> 01:16:27.540]   This is what you find out who's threw in your over with your privacy.
[01:16:27.540 --> 01:16:30.060]   You see who's selling it to.
[01:16:30.060 --> 01:16:33.340]   Plus it kind of keeps it isolated from your main email.
[01:16:33.340 --> 01:16:35.900]   You just filter it all.
[01:16:35.900 --> 01:16:40.500]   Because anything that comes to Amazon Echo at myemail.com, that came from sending up
[01:16:40.500 --> 01:16:42.100]   for an Echo account.
[01:16:42.100 --> 01:16:45.740]   And so I can filter that out.
[01:16:45.740 --> 01:16:46.980]   I think there's an advantage to that.
[01:16:46.980 --> 01:16:49.700]   You can do that with Gmail if you want to using pluses.
[01:16:49.700 --> 01:16:50.700]   Although I know...
[01:16:50.700 --> 01:16:53.420]   I think people are wise to that now, right?
[01:16:53.420 --> 01:16:56.780]   They don't let you do the pluses much as they used to.
[01:16:56.780 --> 01:16:57.780]   Oh!
[01:16:57.780 --> 01:17:00.780]   So Scooter X sent me a thing all about this.
[01:17:00.780 --> 01:17:04.460]   And I realized the app I wish she wants to use isn't for families.
[01:17:04.460 --> 01:17:06.700]   And that's why I'm encountering some challenges.
[01:17:06.700 --> 01:17:07.700]   Oh!
[01:17:07.700 --> 01:17:10.620]   Googling your child's Google account.
[01:17:10.620 --> 01:17:13.740]   Yeah, she can't turn on our television right now.
[01:17:13.740 --> 01:17:15.380]   Because she's having a 13.
[01:17:15.380 --> 01:17:16.380]   Yes!
[01:17:16.380 --> 01:17:17.380]   Well, that says it should be.
[01:17:17.380 --> 01:17:18.380]   I think that's...
[01:17:18.380 --> 01:17:19.380]   Well, no, she's...
[01:17:19.380 --> 01:17:20.380]   That's intentional.
[01:17:20.380 --> 01:17:21.380]   That's not a bug.
[01:17:21.380 --> 01:17:23.020]   That's a feature.
[01:17:23.020 --> 01:17:24.020]   That's a flaw.
[01:17:24.020 --> 01:17:28.380]   Well, it also things like a babysitter right now.
[01:17:28.380 --> 01:17:31.020]   Maybe...can I turn off voice?
[01:17:31.020 --> 01:17:34.300]   Maybe I can turn off voice recognition for certain things.
[01:17:34.300 --> 01:17:37.420]   Your babysitter does not need access to Madamay.
[01:17:37.420 --> 01:17:39.380]   Oh, no!
[01:17:39.380 --> 01:17:40.380]   That's how you turn on our...
[01:17:40.380 --> 01:17:42.900]   Because it's actually a big selling point for us and why we have to...
[01:17:42.900 --> 01:17:45.300]   Can't turn on the TV without Madamay?
[01:17:45.300 --> 01:17:47.660]   No, you can, but you have to follow...
[01:17:47.660 --> 01:17:50.980]   We've got a receiver and all this.
[01:17:50.980 --> 01:17:53.460]   So this way, it's all done for you.
[01:17:53.460 --> 01:17:56.140]   You just tell her to turn it on and it's always worked.
[01:17:56.140 --> 01:17:57.780]   But now with Google, it's like...
[01:17:57.780 --> 01:17:59.780]   Yeah, I'm afraid I can't do that for you.
[01:17:59.780 --> 01:18:02.380]   Oh, because it's a recognizer voice.
[01:18:02.380 --> 01:18:03.380]   Yes!
[01:18:03.380 --> 01:18:05.620]   So you need a guest account or something?
[01:18:05.620 --> 01:18:07.180]   They need to make a guest account.
[01:18:07.180 --> 01:18:08.260]   Well, no, I just...
[01:18:08.260 --> 01:18:09.700]   No, it's not a guest account.
[01:18:09.700 --> 01:18:14.180]   It would be they need to make it so certain functionalities I don't need to have voice
[01:18:14.180 --> 01:18:15.180]   recognition for.
[01:18:15.180 --> 01:18:16.180]   Right.
[01:18:16.180 --> 01:18:18.180]   Because a guest account won't help you if it's voice recognition.
[01:18:18.180 --> 01:18:19.180]   See?
[01:18:19.180 --> 01:18:21.660]   What this show really is about is...
[01:18:21.660 --> 01:18:26.580]   Real-world meeting digital world and the problems that entails.
[01:18:26.580 --> 01:18:28.980]   Honestly, that is what this show's been about.
[01:18:28.980 --> 01:18:31.340]   And more and more is about what it's about.
[01:18:31.340 --> 01:18:38.020]   Is how there's this impedance mismatch between humans and technology.
[01:18:38.020 --> 01:18:40.300]   So I was going to tell you some good things.
[01:18:40.300 --> 01:18:46.060]   For instance, the copyright law did not get renewed.
[01:18:46.060 --> 01:18:56.540]   So as of yesterday, anything created in 1923 or sooner is in public domain.
[01:18:56.540 --> 01:18:58.540]   Or earlier, you mean?
[01:18:58.540 --> 01:19:01.740]   Yeah, earlier, not whatever sooner means to you.
[01:19:01.740 --> 01:19:03.860]   It was being more recently.
[01:19:03.860 --> 01:19:04.860]   Thank you for correcting me.
[01:19:04.860 --> 01:19:05.860]   I don't even know what that means.
[01:19:05.860 --> 01:19:09.300]   I don't think sooner even means anything in that context.
[01:19:09.300 --> 01:19:10.300]   Or prior.
[01:19:10.300 --> 01:19:11.300]   Prior.
[01:19:11.300 --> 01:19:12.300]   That's the word.
[01:19:12.300 --> 01:19:13.860]   Thank you.
[01:19:13.860 --> 01:19:18.780]   And Mickey Mouse in 2024.
[01:19:18.780 --> 01:19:19.780]   Remember that it was Disney...
[01:19:19.780 --> 01:19:20.780]   Yeah, right.
[01:19:20.780 --> 01:19:21.780]   And Disney got a...
[01:19:21.780 --> 01:19:22.780]   Yeah.
[01:19:22.780 --> 01:19:23.780]   You think Disney will get involved?
[01:19:23.780 --> 01:19:24.780]   No.
[01:19:24.780 --> 01:19:25.780]   Yeah.
[01:19:25.780 --> 01:19:28.740]   The Storywright Extension Act.
[01:19:28.740 --> 01:19:29.740]   That's what happened in 1999.
[01:19:29.740 --> 01:19:32.660]   Oh, next week, we'll be the Stormy Daniels extension act or something.
[01:19:32.660 --> 01:19:34.220]   Yeah, Stormy should run for Congress.
[01:19:34.220 --> 01:19:35.860]   I'd vote for her.
[01:19:35.860 --> 01:19:37.820]   Yeah.
[01:19:37.820 --> 01:19:45.020]   So the old point is every time Mickey Mouse's copyright protection has come close to lapsing,
[01:19:45.020 --> 01:19:46.020]   because he was...
[01:19:46.020 --> 01:19:53.540]   Steamboat Willie was in 1924, Congress voted to extend copyright the last time in '99
[01:19:53.540 --> 01:19:56.940]   for 20 years, and that's why now it's...
[01:19:56.940 --> 01:19:59.340]   But they didn't do it this year because they're so disorganized.
[01:19:59.340 --> 01:20:00.340]   They just forgot.
[01:20:00.340 --> 01:20:02.820]   But they could do it still.
[01:20:02.820 --> 01:20:06.180]   They could still do it, but maybe they'll continue to forget.
[01:20:06.180 --> 01:20:10.060]   Or you know, we don't have a government right now, so...
[01:20:10.060 --> 01:20:11.060]   Yeah.
[01:20:11.060 --> 01:20:12.060]   Right.
[01:20:12.060 --> 01:20:13.060]   Right.
[01:20:13.060 --> 01:20:15.260]   And this is a good thing because I will submit.
[01:20:15.260 --> 01:20:18.060]   It's a good thing for things to become public domain.
[01:20:18.060 --> 01:20:22.020]   Like for instance, I don't know, Grim's fairy tales, because then some guy named Walt
[01:20:22.020 --> 01:20:26.100]   could come along and create movies based on Grim's fairy tales, but no one could create
[01:20:26.100 --> 01:20:30.420]   a movie based on Cinderella because it's still in copyright.
[01:20:30.420 --> 01:20:34.220]   And so the bringing things into the public sphere, this is what Larry Lessig always talked
[01:20:34.220 --> 01:20:38.980]   about and one of the reasons he created Creative Commons, bringing things into this public
[01:20:38.980 --> 01:20:39.980]   sphere is good.
[01:20:39.980 --> 01:20:44.860]   It's reasonable to say there should be some protection for the creator for some reasonable
[01:20:44.860 --> 01:20:47.060]   period of time.
[01:20:47.060 --> 01:20:49.060]   It seems like what is it?
[01:20:49.060 --> 01:20:53.460]   A hundred years almost should be enough.
[01:20:53.460 --> 01:20:56.980]   I think that's probably a little longer than intended.
[01:20:56.980 --> 01:20:59.420]   Rhapsody in Blue will go in the public domain next year.
[01:20:59.420 --> 01:21:01.540]   Great Gatsby in 2021.
[01:21:01.540 --> 01:21:11.780]   The sun also rises 2022, but the big one is January 1st, 2024 when both Batman, Superman,
[01:21:11.780 --> 01:21:12.780]   Snow White...
[01:21:12.780 --> 01:21:14.340]   No, no, that's later.
[01:21:14.340 --> 01:21:16.380]   Mickey Mouse Wolf coming to...
[01:21:16.380 --> 01:21:18.620]   This is all from an article in the series technical.
[01:21:18.620 --> 01:21:20.420]   None of this is going to happen.
[01:21:20.420 --> 01:21:21.420]   That's not happening.
[01:21:21.420 --> 01:21:22.620]   This is all dream on, isn't it?
[01:21:22.620 --> 01:21:28.220]   Disney has a hundred lawyers and 3,000 lobbyists right now working on this.
[01:21:28.220 --> 01:21:33.100]   The problem is Sunny Bono's passed, but as you say, Starmie Daniels Copyright Extension
[01:21:33.100 --> 01:21:34.900]   Act of 2024.
[01:21:34.900 --> 01:21:35.900]   What?
[01:21:35.900 --> 01:21:41.420]   Just a matter of...
[01:21:41.420 --> 01:21:44.340]   That's Jeff's name for it.
[01:21:44.340 --> 01:21:46.300]   It was Sunny Bono's law.
[01:21:46.300 --> 01:21:47.300]   Sunny Bono, do you?
[01:21:47.300 --> 01:21:48.300]   No, I got that.
[01:21:48.300 --> 01:21:53.220]   Well, we got to find a new celebrity congressperson.
[01:21:53.220 --> 01:21:57.860]   You want the Michael Avenatti Copyright Protection Act of 2023?
[01:21:57.860 --> 01:21:58.860]   You can have it.
[01:21:58.860 --> 01:22:03.180]   That's fine.
[01:22:03.180 --> 01:22:04.180]   I had other good things.
[01:22:04.180 --> 01:22:09.860]   Oh, hospitals are now going to have to post their prices online.
[01:22:09.860 --> 01:22:11.500]   God, thank God.
[01:22:11.500 --> 01:22:12.500]   Oh my gosh.
[01:22:12.500 --> 01:22:15.820]   We are living in fear for my daughter's ER room visit.
[01:22:15.820 --> 01:22:18.380]   It's covered, right?
[01:22:18.380 --> 01:22:19.700]   Did you get an ambulance?
[01:22:19.700 --> 01:22:20.700]   No, no.
[01:22:20.700 --> 01:22:21.700]   We took her in.
[01:22:21.700 --> 01:22:26.860]   I know people who have actively eschew ambulances because, well, if you've got an ambulance bill,
[01:22:26.860 --> 01:22:28.860]   you'll be lying.
[01:22:28.860 --> 01:22:31.700]   When I faint, I tell people never to call an ambulance.
[01:22:31.700 --> 01:22:33.780]   Unfortunately, a lot of times I'm unconscious.
[01:22:33.780 --> 01:22:34.780]   So then I get...
[01:22:34.780 --> 01:22:35.780]   You need to try.
[01:22:35.780 --> 01:22:37.780]   You can just say, please don't call an ambulance.
[01:22:37.780 --> 01:22:38.780]   You need a bracelet.
[01:22:38.780 --> 01:22:41.300]   Or bracelet, yeah.
[01:22:41.300 --> 01:22:46.500]   On January 1st, yesterday, a new regulation took effect requiring hospitals to post the
[01:22:46.500 --> 01:22:49.820]   prices of their services online.
[01:22:49.820 --> 01:22:52.980]   This is a provision called...
[01:22:52.980 --> 01:22:59.980]   By the way, Health and Human Services Inpatient Perspective Payment System Rule, they have
[01:22:59.980 --> 01:23:03.580]   to share the prices of standard services online as well as make medical records more
[01:23:03.580 --> 01:23:08.220]   easily accessible by patients themselves and shareable between medical practices.
[01:23:08.220 --> 01:23:09.300]   Why should these be online?
[01:23:09.300 --> 01:23:12.020]   Well, because you don't know.
[01:23:12.020 --> 01:23:13.020]   You don't know.
[01:23:13.020 --> 01:23:16.540]   So one of my graduates from entrepreneurial journalism named Jeannie Pender has been working
[01:23:16.540 --> 01:23:20.020]   on the business on this for years called ClearHealthClass.com.
[01:23:20.020 --> 01:23:21.020]   Good.
[01:23:21.020 --> 01:23:23.700]   Where she works to...
[01:23:23.700 --> 01:23:28.740]   Any way she can, through patients, through insurance companies, through whoever, to get
[01:23:28.740 --> 01:23:31.140]   the cost to be made more transparent.
[01:23:31.140 --> 01:23:35.420]   Not just hospitals, but also doctors, procedures, because they can vary all over the map.
[01:23:35.420 --> 01:23:36.420]   Huge.
[01:23:36.420 --> 01:23:38.700]   Here's some more good news.
[01:23:38.700 --> 01:23:43.300]   If the government shutdown continues, which it probably will, although we'll find out,
[01:23:43.300 --> 01:23:44.300]   right, tomorrow.
[01:23:44.300 --> 01:23:48.780]   Because when is Congress sit tomorrow, right?
[01:23:48.780 --> 01:23:49.780]   The third.
[01:23:49.780 --> 01:23:50.780]   Tomorrow.
[01:23:50.780 --> 01:23:51.780]   Yeah.
[01:23:51.780 --> 01:23:52.780]   So maybe they'll do something about it.
[01:23:52.780 --> 01:23:53.780]   I don't know.
[01:23:53.780 --> 01:23:56.180]   But if they don't, the FCC will shut down.
[01:23:56.180 --> 01:23:57.620]   That can only be good, right?
[01:23:57.620 --> 01:23:59.500]   Because more government is bad government.
[01:23:59.500 --> 01:24:03.540]   Isn't that what they say?
[01:24:03.540 --> 01:24:05.940]   Let's shut the FCC down.
[01:24:05.940 --> 01:24:11.660]   The FCC's suspended activities will include consumer complaint and inquiry phone lines,
[01:24:11.660 --> 01:24:15.740]   consumer protection and local competition enforcement, licensing services, including
[01:24:15.740 --> 01:24:19.100]   broadcast, wireless, wired line, management of radio...
[01:24:19.100 --> 01:24:20.100]   Use bad words.
[01:24:20.100 --> 01:24:21.700]   If you're on it on TV, you can use it better.
[01:24:21.700 --> 01:24:24.620]   You can start swearing kids for competitive tech.
[01:24:24.620 --> 01:24:30.860]   Wouldn't that be funny if the thing that ends the government shutdown is Howard Stern
[01:24:30.860 --> 01:24:34.740]   swearing on broadcast?
[01:24:34.740 --> 01:24:35.740]   That would be great.
[01:24:35.740 --> 01:24:38.740]   That would make me happy.
[01:24:38.740 --> 01:24:42.940]   Athletes do not own their own tattoos.
[01:24:42.940 --> 01:24:44.980]   That was fascinating.
[01:24:44.980 --> 01:24:48.500]   It makes perfect sense, but yeah, a problem.
[01:24:48.500 --> 01:24:57.020]   So it's okay for LeBron's tats to appear on TV during a game, incidentally.
[01:24:57.020 --> 01:25:00.020]   But because there's a presumed license.
[01:25:00.020 --> 01:25:01.100]   Yeah.
[01:25:01.100 --> 01:25:06.540]   But should that tattoo be duplicated in, let's say, a video game?
[01:25:06.540 --> 01:25:15.620]   Now we got a problem because the design belongs to the tattoo artist.
[01:25:15.620 --> 01:25:24.380]   A copyright lawyer who, by the way, represented the guy who did make Tyson's face tattoo.
[01:25:24.380 --> 01:25:28.020]   And successfully, I think, defended it in court, right?
[01:25:28.020 --> 01:25:29.460]   That's exactly what it looks like.
[01:25:29.460 --> 01:25:30.940]   So I could get sued next as well.
[01:25:30.940 --> 01:25:34.140]   I'm putting my hand on my face.
[01:25:34.140 --> 01:25:35.260]   His name is Michael Kahn.
[01:25:35.260 --> 01:25:38.620]   He says video games or probably says it like this.
[01:25:38.620 --> 01:25:41.100]   Video games are entirely no area.
[01:25:41.100 --> 01:25:43.900]   There's LeBron James, but it's not LeBron James.
[01:25:43.900 --> 01:25:45.620]   It's a cartoon version of him.
[01:25:45.620 --> 01:25:48.740]   I'm hoping the guy actually went to Cambridge.
[01:25:48.740 --> 01:25:52.660]   I don't know.
[01:25:52.660 --> 01:25:53.980]   Michael Kahn, Michael Cohen.
[01:25:53.980 --> 01:25:54.980]   Is there a difference?
[01:25:54.980 --> 01:25:56.260]   I don't think so.
[01:25:56.260 --> 01:26:00.220]   So Lionel Messi's heart-eating gorilla.
[01:26:00.220 --> 01:26:03.860]   Oh, no, that's Conor McGregor has a heart-eating gorilla.
[01:26:03.860 --> 01:26:08.340]   Lionel Messi has that sleeve right there.
[01:26:08.340 --> 01:26:13.100]   EA, you got some splitting to do.
[01:26:13.100 --> 01:26:18.180]   They're facing a copyright infringement lawsuit after the cover of the game NFL Street, including
[01:26:18.180 --> 01:26:24.740]   an illustration of running back, I can't read anymore, included an illustration of running
[01:26:24.740 --> 01:26:28.140]   back Ricky Williams and some of his tats.
[01:26:28.140 --> 01:26:31.780]   But the artist withdrew his claim in 2013.
[01:26:31.780 --> 01:26:32.860]   This is the interesting part.
[01:26:32.860 --> 01:26:38.260]   This is where to solve this problem, they're saying when you get a tattoo, if you are a
[01:26:38.260 --> 01:26:44.100]   famous player or something, you should get a license to have that become part of your
[01:26:44.100 --> 01:26:47.540]   digital likeness, which I think is really fascinating.
[01:26:47.540 --> 01:26:52.420]   And honestly, I think as a normal person, I might be like, hey, if I were going to get
[01:26:52.420 --> 01:26:55.140]   a tattoo in a highly visible spot, I might do that too.
[01:26:55.140 --> 01:26:58.580]   Just in case I want to create an accurate digital avatar.
[01:26:58.580 --> 01:26:59.580]   Exactly.
[01:26:59.580 --> 01:27:02.740]   So think about that, kids, when you're getting that face tattoo, make sure you have the artist
[01:27:02.740 --> 01:27:03.860]   sign over the rights.
[01:27:03.860 --> 01:27:05.820]   That's what Mike Evans did.
[01:27:05.820 --> 01:27:08.940]   He's an NFL receiver.
[01:27:08.940 --> 01:27:15.100]   Gotti Flores, who they call the Teflon tattoo artist, has spent at least 40 hours tattooing
[01:27:15.100 --> 01:27:17.460]   Evans.
[01:27:17.460 --> 01:27:20.420]   He had to give permission for his work to be reproduced in the game.
[01:27:20.420 --> 01:27:25.340]   Really, it didn't even matter to me, said Mr. Flores, who signed a waiver for no compensation.
[01:27:25.340 --> 01:27:28.740]   It was dope to have my tattoos on there.
[01:27:28.740 --> 01:27:31.300]   Well, you say, Mr. Flores, you're absolutely right.
[01:27:31.300 --> 01:27:32.460]   It was dope.
[01:27:32.460 --> 01:27:35.260]   It was dope.
[01:27:35.260 --> 01:27:37.620]   But there have been other lawsuits.
[01:27:37.620 --> 01:27:38.740]   I mean, I get it.
[01:27:38.740 --> 01:27:47.140]   I also think, you know, yes, I'm thinking about back in 2003 when I got married, I arranged
[01:27:47.140 --> 01:27:52.340]   to have for my wedding photographer, I actually bought the digital rights as part of my photography
[01:27:52.340 --> 01:27:53.340]   package.
[01:27:53.340 --> 01:27:55.540]   And she's like, no one has ever asked for this before.
[01:27:55.540 --> 01:27:56.540]   Wow.
[01:27:56.540 --> 01:27:57.500]   She very much undercharged me.
[01:27:57.500 --> 01:27:59.660]   So it was nice.
[01:27:59.660 --> 01:28:00.660]   That's smart.
[01:28:00.660 --> 01:28:02.900]   I am a futurist.
[01:28:02.900 --> 01:28:05.620]   Did you get them up under?
[01:28:05.620 --> 01:28:10.300]   I hope you put them up under Creative Commons so we can remix them.
[01:28:10.300 --> 01:28:11.300]   Yeah.
[01:28:11.300 --> 01:28:16.420]   Did you get permission to use the ads of text to syrup on them?
[01:28:16.420 --> 01:28:17.420]   You know what?
[01:28:17.420 --> 01:28:20.020]   I think that situation, I don't know.
[01:28:20.020 --> 01:28:23.660]   I've been working on the railroads in public domain, isn't it?
[01:28:23.660 --> 01:28:25.500]   It was just the music.
[01:28:25.500 --> 01:28:26.940]   Not the same.
[01:28:26.940 --> 01:28:28.900]   No one's saying it.
[01:28:28.900 --> 01:28:30.780]   So what in doubt?
[01:28:30.780 --> 01:28:32.580]   It wasn't the eyes of Texas or upon you.
[01:28:32.580 --> 01:28:34.620]   It was I've been working on the railroad.
[01:28:34.620 --> 01:28:35.620]   There we go.
[01:28:35.620 --> 01:28:44.060]   This is a call back to an earlier episode I invite you all to listen.
[01:28:44.060 --> 01:28:55.180]   A federal judge in Oregon says the state law that defines what an engineer is violates
[01:28:55.180 --> 01:28:56.180]   free speech.
[01:28:56.180 --> 01:28:59.540]   Was this a software engineer issue?
[01:28:59.540 --> 01:29:00.540]   No.
[01:29:00.540 --> 01:29:01.540]   No.
[01:29:01.540 --> 01:29:02.540]   It's an interesting story.
[01:29:02.540 --> 01:29:04.380]   It's a really interesting story.
[01:29:04.380 --> 01:29:05.380]   Yeah.
[01:29:05.380 --> 01:29:11.700]   A Beaverton guy, Matt's Yarlstrom, didn't like how short the yellow lights were in his
[01:29:11.700 --> 01:29:12.700]   town.
[01:29:12.700 --> 01:29:14.700]   He said...
[01:29:14.700 --> 01:29:16.340]   Was it in Portland?
[01:29:16.340 --> 01:29:20.820]   I think it was in Beaverton.
[01:29:20.820 --> 01:29:24.020]   He had got a ticket for running...
[01:29:24.020 --> 01:29:28.260]   Actually, his wife got a ticket for running a red light in 2013.
[01:29:28.260 --> 01:29:37.660]   So Yarlstrom, who is, by the way, has a bachelor's of science, a BS in engineering from Sweden.
[01:29:37.660 --> 01:29:43.660]   He spent three years analyzing the method for calculating the duration of a yellow light
[01:29:43.660 --> 01:29:49.540]   and found the formula failed to account for drivers who have to slow down to make a legal
[01:29:49.540 --> 01:29:52.980]   right turn or left turn, a legal turn.
[01:29:52.980 --> 01:29:54.380]   They have to slow down to make a turn.
[01:29:54.380 --> 01:29:58.540]   They did not account for the fact that they are in the box during the yellow.
[01:29:58.540 --> 01:30:04.100]   It turns to red because they've slowed down to make a turn.
[01:30:04.100 --> 01:30:10.340]   He sent his traffic light calculations to the state board and identified himself as an engineer
[01:30:10.340 --> 01:30:16.340]   to not only local media, but 60 minutes to the story on him.
[01:30:16.340 --> 01:30:24.220]   The state fined him $500 for violating a law that governs who can call themselves an engineer,
[01:30:24.220 --> 01:30:28.460]   finding that he wasn't registered in Oregon as a professional engineer.
[01:30:28.460 --> 01:30:29.460]   He's not an engineer.
[01:30:29.460 --> 01:30:33.500]   He doesn't have the hat or anything.
[01:30:33.500 --> 01:30:36.620]   But maybe the state should have thought a little bit.
[01:30:36.620 --> 01:30:41.460]   This is a guy who spent three years analyzing traffic light data to get his wife out of a
[01:30:41.460 --> 01:30:42.460]   ticket.
[01:30:42.460 --> 01:30:47.300]   If there's a no better definition of an engineer, I can't imagine it.
[01:30:47.300 --> 01:30:51.820]   Well, or someone who is likely to challenge that $500 fine.
[01:30:51.820 --> 01:30:53.380]   He did.
[01:30:53.380 --> 01:31:00.780]   He went all the way up to a federal appeals court and won a 25-page written ruling saying
[01:31:00.780 --> 01:31:06.380]   "Yarlster may study and communicate publicly or privately about his theories relating to
[01:31:06.380 --> 01:31:11.900]   traffic lights as long as their remarks occur outside the context of any employment or contractual
[01:31:11.900 --> 01:31:17.380]   relationship with a governmental or other group that regulates traffic light timing."
[01:31:17.380 --> 01:31:25.460]   He may in fact describe himself both publicly and privately as an engineer.
[01:31:25.460 --> 01:31:35.580]   So, I guess if you were not an engineer and you told the media that you were an engineer,
[01:31:35.580 --> 01:31:38.660]   do you… is that fair?
[01:31:38.660 --> 01:31:41.980]   If I called myself a doctor, I'd be committing fraud, right?
[01:31:41.980 --> 01:31:42.980]   If I'm not actually a doctor.
[01:31:42.980 --> 01:31:43.980]   You studied engineering.
[01:31:43.980 --> 01:31:44.980]   He does, yes.
[01:31:44.980 --> 01:31:45.980]   He does.
[01:31:45.980 --> 01:31:46.980]   He does, yes.
[01:31:46.980 --> 01:31:47.980]   He does, yes.
[01:31:47.980 --> 01:31:48.980]   He does.
[01:31:48.980 --> 01:31:49.980]   He acts of engineering.
[01:31:49.980 --> 01:31:50.980]   Okay.
[01:31:50.980 --> 01:31:51.980]   Yeah.
[01:31:51.980 --> 01:31:57.060]   And there is for a doctor.
[01:31:57.060 --> 01:31:58.060]   You can't pose as an engineer.
[01:31:58.060 --> 01:32:00.780]   You can't come to the state and say, "I'm going to design a bridge for you."
[01:32:00.780 --> 01:32:05.860]   But you could, if you wish, send traffic light calculations to the state board and talk
[01:32:05.860 --> 01:32:09.460]   to 60 minutes and say, "I'm an engineer."
[01:32:09.460 --> 01:32:14.140]   We have that for a while in Texas where you could not… anyone who did software, you
[01:32:14.140 --> 01:32:18.940]   could not call someone a software engineer because of licensing requirements.
[01:32:18.940 --> 01:32:21.300]   So that actually was a big deal here for a while.
[01:32:21.300 --> 01:32:22.300]   I have no idea.
[01:32:22.300 --> 01:32:25.500]   You definitely wouldn't want a programmer to design a bridge.
[01:32:25.500 --> 01:32:26.500]   Well, yeah.
[01:32:26.500 --> 01:32:30.460]   But you wouldn't want a mechanical engineer to design a bridge either.
[01:32:30.460 --> 01:32:31.860]   So software either, right.
[01:32:31.860 --> 01:32:32.860]   Or, yeah, right.
[01:32:32.860 --> 01:32:33.860]   So, yes.
[01:32:33.860 --> 01:32:34.860]   But mechanical engineers were okay.
[01:32:34.860 --> 01:32:36.020]   But there's no original bridge beta.
[01:32:36.020 --> 01:32:37.020]   Right.
[01:32:37.020 --> 01:32:38.020]   Bridge 2.0.
[01:32:38.020 --> 01:32:40.860]   Well, there is if your first one falls down.
[01:32:40.860 --> 01:32:41.860]   Yeah.
[01:32:41.860 --> 01:32:44.500]   So, I think that's a victory for…
[01:32:44.500 --> 01:32:46.500]   Free speech in engineers everywhere.
[01:32:46.500 --> 01:32:47.500]   Everywhere.
[01:32:47.500 --> 01:32:48.500]   Everywhere.
[01:32:48.500 --> 01:32:49.500]   Everywhere.
[01:32:49.500 --> 01:32:50.500]   Are we doing a change log?
[01:32:50.500 --> 01:32:51.500]   Where the hat?
[01:32:51.500 --> 01:32:52.500]   Oh, board.
[01:32:52.500 --> 01:33:02.500]   No, there's not really… I mean, we could, but there's very little change to talk about.
[01:33:02.500 --> 01:33:04.500]   I like found one, I think.
[01:33:04.500 --> 01:33:05.500]   Google…
[01:33:05.500 --> 01:33:06.500]   Google…
[01:33:06.500 --> 01:33:07.500]   Oh, no.
[01:33:07.500 --> 01:33:08.500]   Oh, no.
[01:33:08.500 --> 01:33:09.500]   He can't stop himself.
[01:33:09.500 --> 01:33:11.500]   You push the button.
[01:33:11.500 --> 01:33:13.500]   You push the button.
[01:33:13.500 --> 01:33:14.500]   Um…
[01:33:14.500 --> 01:33:20.060]   Well, Google had this article on kickstarting your New Year's resolutions with Google
[01:33:20.060 --> 01:33:21.060]   Fit.
[01:33:21.060 --> 01:33:23.700]   But it's not really anything.
[01:33:23.700 --> 01:33:24.700]   I see you made it.
[01:33:24.700 --> 01:33:25.700]   That's the change log.
[01:33:25.700 --> 01:33:26.700]   That was it.
[01:33:26.700 --> 01:33:27.700]   All right.
[01:33:27.700 --> 01:33:28.700]   It's the change log.
[01:33:28.700 --> 01:33:29.700]   It's over.
[01:33:29.700 --> 01:33:30.700]   Well done.
[01:33:30.700 --> 01:33:31.700]   First of all, please.
[01:33:31.700 --> 01:33:34.940]   Well, please, sir.
[01:33:34.940 --> 01:33:38.940]   Uh…
[01:33:38.940 --> 01:33:40.620]   This is a very provocative article.
[01:33:40.620 --> 01:33:47.500]   We probably should have led with this in the New York Times today about a move from the
[01:33:47.500 --> 01:33:48.500]   Commerce Department.
[01:33:48.500 --> 01:33:58.380]   They're considering security restrictions on the export of artificial intelligence technologies.
[01:33:58.380 --> 01:34:02.420]   They don't want the Chinese to get ahold of our research, I guess.
[01:34:02.420 --> 01:34:09.380]   But according to the Times, Cade Metz writing in San Francisco, a growing number of Silicon
[01:34:09.380 --> 01:34:14.980]   Valley insiders are worried that proposed export restrictions could actually short circuit
[01:34:14.980 --> 01:34:19.900]   the preeminence of American companies in the next big thing to hit their industry, artificial
[01:34:19.900 --> 01:34:21.900]   intelligence.
[01:34:21.900 --> 01:34:24.420]   So this is actually under discussion right now.
[01:34:24.420 --> 01:34:27.740]   There's only a few days left to comment on this.
[01:34:27.740 --> 01:34:31.140]   You just don't understand how they could do it.
[01:34:31.140 --> 01:34:34.340]   I mean, do you regulate the algorithms?
[01:34:34.340 --> 01:34:40.820]   Well, they tried to do this with strong encryption, you remember, calling it munitions.
[01:34:40.820 --> 01:34:47.860]   The Commerce Department has a January 10 deadline for public comment on AI export rules.
[01:34:47.860 --> 01:34:51.420]   Yes, I could go read the comments.
[01:34:51.420 --> 01:34:55.540]   And what Silicon Valley is worried about is exactly what you just said, which is you
[01:34:55.540 --> 01:34:57.260]   can't do anything about it anyway.
[01:34:57.260 --> 01:34:59.940]   They're not going to make these rules, they're not going to make any difference.
[01:34:59.940 --> 01:35:04.300]   And they could harm companies in the US, help international competitors, and stifle technology
[01:35:04.300 --> 01:35:05.300]   improvements.
[01:35:05.300 --> 01:35:11.300]   Well, so that's why we need to know that how, because I mean, theoretically, I mean, that's
[01:35:11.300 --> 01:35:13.420]   what everyone says when they're faced with these things.
[01:35:13.420 --> 01:35:15.700]   So I just automatically throw that out.
[01:35:15.700 --> 01:35:17.460]   I'm just like, but how would you do it?
[01:35:17.460 --> 01:35:20.740]   Because that would have the impact of that would determine the impact.
[01:35:20.740 --> 01:35:21.740]   Right.
[01:35:21.740 --> 01:35:25.420]   Well, yeah, for instance, if you say, well, no American AI scientists can give papers
[01:35:25.420 --> 01:35:27.060]   outside the United States.
[01:35:27.060 --> 01:35:29.140]   Or you can publish them?
[01:35:29.140 --> 01:35:30.820]   I mean, how do you contain it?
[01:35:30.820 --> 01:35:32.980]   Yeah, I mean, it's well, hello.
[01:35:32.980 --> 01:35:38.380]   This comes from an August Congress passed the Export Controls Act of 2018, which added
[01:35:38.380 --> 01:35:42.020]   export restrictions to emerging and foundational technologies.
[01:35:42.020 --> 01:35:44.100]   So basically you can give it away.
[01:35:44.100 --> 01:35:46.100]   You can't sell it.
[01:35:46.100 --> 01:35:47.100]   Well, we don't know.
[01:35:47.100 --> 01:35:50.140]   That's right, because we got to find out what these rules will be there.
[01:35:50.140 --> 01:35:51.980]   We're still in the comment period.
[01:35:51.980 --> 01:35:56.860]   Well, and do you do things like, oh, you can't manufacture GPUs over there?
[01:35:56.860 --> 01:35:58.460]   Because that's how we train a lot of stuff.
[01:35:58.460 --> 01:36:01.660]   I mean, do not have data sets that are available.
[01:36:01.660 --> 01:36:03.980]   I mean, there's different.
[01:36:03.980 --> 01:36:05.180]   Yeah.
[01:36:05.180 --> 01:36:06.660]   Yeah.
[01:36:06.660 --> 01:36:11.500]   A lot of AI innovation is actually being done and made in public.
[01:36:11.500 --> 01:36:15.980]   The things that are private today is access to the underlying technology, which is highly
[01:36:15.980 --> 01:36:16.980]   commodified.
[01:36:16.980 --> 01:36:18.060]   So that's not a place to do it.
[01:36:18.060 --> 01:36:21.540]   The only other way to do it is access to the data sets.
[01:36:21.540 --> 01:36:26.420]   But the data sets just, that's not the innovative part.
[01:36:26.420 --> 01:36:28.140]   That's like access to steel.
[01:36:28.140 --> 01:36:32.820]   So you can build a cool building when you're really trying to stop some style of architecture
[01:36:32.820 --> 01:36:34.740]   from escaping.
[01:36:34.740 --> 01:36:38.580]   Federal regulations exempt publicly available information from export control.
[01:36:38.580 --> 01:36:43.460]   That means the government is unlikely to bar companies and universities from publishing
[01:36:43.460 --> 01:36:45.580]   fundamental AI research.
[01:36:45.580 --> 01:36:48.220]   That would be a violation of First Amendment.
[01:36:48.220 --> 01:36:49.820]   But yeah, well, I don't know.
[01:36:49.820 --> 01:36:53.660]   But it could establish controls that restrict foreign access to that information.
[01:36:53.660 --> 01:36:54.660]   How?
[01:36:54.660 --> 01:36:59.100]   I don't know.
[01:36:59.100 --> 01:37:01.700]   I don't know.
[01:37:01.700 --> 01:37:03.700]   One thing that might do.
[01:37:03.700 --> 01:37:05.580]   You brought us national boundaries.
[01:37:05.580 --> 01:37:07.020]   The internet takes them away.
[01:37:07.020 --> 01:37:08.020]   Right.
[01:37:08.020 --> 01:37:12.540]   And that's why we have nationalists going crazy because their fundamental view of themselves
[01:37:12.540 --> 01:37:16.020]   is challenged.
[01:37:16.020 --> 01:37:20.980]   And that's why you know who is trying to build a wall in a last futile attempt to hold on
[01:37:20.980 --> 01:37:21.980]   to a border.
[01:37:21.980 --> 01:37:25.140]   And nostalgia for a...
[01:37:25.140 --> 01:37:26.140]   It is.
[01:37:26.140 --> 01:37:27.140]   Something that never really existed.
[01:37:27.140 --> 01:37:34.140]   But that's what Harari talks about in his 21 lessons is that nationalism doesn't have
[01:37:34.140 --> 01:37:41.100]   an answer for global problems like climate change and AI and all of this stuff because
[01:37:41.100 --> 01:37:42.100]   it's nationalistic.
[01:37:42.100 --> 01:37:45.980]   It doesn't apply to a global world.
[01:37:45.980 --> 01:37:47.740]   You know, like the internet.
[01:37:47.740 --> 01:37:48.740]   Yeah.
[01:37:48.740 --> 01:37:51.100]   Overly restrictive rules that prevent, here's one way they would do it.
[01:37:51.100 --> 01:37:55.460]   Different foreign nationals from working on certain technologies in the United States
[01:37:55.460 --> 01:37:58.980]   could push those researchers and companies into other countries.
[01:37:58.980 --> 01:38:02.140]   It might be easier for people to just do this stuff in Europe.
[01:38:02.140 --> 01:38:05.540]   And which means they wouldn't do it in the US.
[01:38:05.540 --> 01:38:09.020]   Where they'd also have to worry about GDPR and be compliant there and have better business
[01:38:09.020 --> 01:38:10.020]   as a result.
[01:38:10.020 --> 01:38:11.020]   Move to China.
[01:38:11.020 --> 01:38:13.980]   If you want to do AI research, move to China, anything else.
[01:38:13.980 --> 01:38:14.980]   Well, well.
[01:38:14.980 --> 01:38:23.340]   Speaking of which, the Chinese takeover of the Indian app ecosystem.
[01:38:23.340 --> 01:38:24.340]   Almost a pass.
[01:38:24.340 --> 01:38:25.340]   Yes.
[01:38:25.340 --> 01:38:26.340]   Wasn't this interesting?
[01:38:26.340 --> 01:38:33.380]   Almost half of the apps currently in use in India, popular in India, are from China.
[01:38:33.380 --> 01:38:34.380]   2018.
[01:38:34.380 --> 01:38:37.740]   So the waiting for China to come to the US.
[01:38:37.740 --> 01:38:38.740]   Uh-uh.
[01:38:38.740 --> 01:38:41.860]   2018 is likely to be remembered as the year when the Chinese took over Indian smartphones.
[01:38:41.860 --> 01:38:46.860]   Well, they're doing it in America too because who owns TikTok?
[01:38:46.860 --> 01:38:50.180]   TikTok is owned by a Chinese company.
[01:38:50.180 --> 01:38:51.580]   It is, yes.
[01:38:51.580 --> 01:38:57.420]   In December 2017, the top 10 mobile apps on Google Play Store in India, five out of the
[01:38:57.420 --> 01:39:00.340]   top 10 are Chinese.
[01:39:00.340 --> 01:39:04.820]   There were 18 Chinese apps among the top 100 across various categories.
[01:39:04.820 --> 01:39:07.300]   You see browsers share it in News Dog.
[01:39:07.300 --> 01:39:09.140]   Oh, that was last year.
[01:39:09.140 --> 01:39:13.940]   This year, 44 out of the top 100.
[01:39:13.940 --> 01:39:16.100]   Almost half.
[01:39:16.100 --> 01:39:19.220]   This is what's changed.
[01:39:19.220 --> 01:39:23.260]   Uh, yeah.
[01:39:23.260 --> 01:39:29.900]   So that's India, of course, but this is another example of, and I've kind of come to understand
[01:39:29.900 --> 01:39:31.380]   this a little bit better too.
[01:39:31.380 --> 01:39:33.300]   Thanks to some people who've written us.
[01:39:33.300 --> 01:39:34.340]   Thank you.
[01:39:34.340 --> 01:39:38.100]   And to some books, including Harari's book.
[01:39:38.100 --> 01:39:41.980]   China is very much interested in controlling what's going on inside of China with its own
[01:39:41.980 --> 01:39:44.700]   citizens, but is very global in its outlook.
[01:39:44.700 --> 01:39:45.700]   Oh, yeah.
[01:39:45.700 --> 01:39:47.180]   Well, yes and no.
[01:39:47.180 --> 01:39:52.540]   I mean, you'd think that the major Chinese apps and companies, Alibaba and such, would
[01:39:52.540 --> 01:39:53.540]   be here.
[01:39:53.540 --> 01:39:54.540]   It's kind of like why bother?
[01:39:54.540 --> 01:39:56.300]   They have so much growth left in China.
[01:39:56.300 --> 01:39:57.300]   Right.
[01:39:57.300 --> 01:39:58.300]   And they don't have a cause.
[01:39:58.300 --> 01:40:00.540]   But I keep on waiting for that moment.
[01:40:00.540 --> 01:40:02.220]   But India is a big economy.
[01:40:02.220 --> 01:40:04.740]   So it makes sense that they approach India.
[01:40:04.740 --> 01:40:05.740]   Yes.
[01:40:05.740 --> 01:40:09.340]   So American companies that want to go to China, I kept on saying go to India first.
[01:40:09.340 --> 01:40:10.340]   Interesting.
[01:40:10.340 --> 01:40:13.860]   Well, there's no mention of China.
[01:40:13.860 --> 01:40:14.860]   Huh?
[01:40:14.860 --> 01:40:15.860]   None.
[01:40:15.860 --> 01:40:16.860]   None.
[01:40:16.860 --> 01:40:18.860]   But it is owned by a Chinese company.
[01:40:18.860 --> 01:40:20.500]   It's among the top apps in the US as well.
[01:40:20.500 --> 01:40:21.500]   Is it not?
[01:40:21.500 --> 01:40:22.500]   I think it is.
[01:40:22.500 --> 01:40:23.500]   Is it?
[01:40:23.500 --> 01:40:24.500]   Yeah.
[01:40:24.500 --> 01:40:27.180]   Well, your, your teenager would be or she's not a teenager yet.
[01:40:27.180 --> 01:40:29.300]   I guess that's the problem.
[01:40:29.300 --> 01:40:35.420]   Um, top apps on Google play.
[01:40:35.420 --> 01:40:39.420]   Or in US, I'm sure TikTok's in the top 10.
[01:40:39.420 --> 01:40:44.300]   Have you used TikTok?
[01:40:44.300 --> 01:40:45.460]   I used it briefly.
[01:40:45.460 --> 01:40:48.460]   It's really not for me.
[01:40:48.460 --> 01:40:49.460]   You old fart you.
[01:40:49.460 --> 01:40:51.500]   It's your old.
[01:40:51.500 --> 01:40:54.940]   It's number two in the US.
[01:40:54.940 --> 01:40:56.900]   Number one is color bump 3D.
[01:40:56.900 --> 01:40:58.900]   I've never heard of color.
[01:40:58.900 --> 01:40:59.900]   Never heard of color.
[01:40:59.900 --> 01:41:02.140]   I can't be right.
[01:41:02.140 --> 01:41:04.740]   These are probably, you know, as of last week or something.
[01:41:04.740 --> 01:41:07.340]   But yeah, TikTok's number two.
[01:41:07.340 --> 01:41:12.420]   And the company's called Musically and, you know, it was originally an American product
[01:41:12.420 --> 01:41:14.860]   that was bought by a Chinese company.
[01:41:14.860 --> 01:41:15.860]   Which one?
[01:41:15.860 --> 01:41:16.860]   Musically or TikTok?
[01:41:16.860 --> 01:41:18.260]   Musically was American, right?
[01:41:18.260 --> 01:41:19.860]   Yeah, TikTok and Musically.
[01:41:19.860 --> 01:41:20.860]   Yeah.
[01:41:20.860 --> 01:41:22.460]   Were they, are they said?
[01:41:22.460 --> 01:41:24.660]   No, they're, they're the same now.
[01:41:24.660 --> 01:41:25.660]   Well, they are.
[01:41:25.660 --> 01:41:26.900]   They were bought musically.
[01:41:26.900 --> 01:41:27.900]   Right.
[01:41:27.900 --> 01:41:28.900]   Okay.
[01:41:28.900 --> 01:41:29.900]   All right.
[01:41:29.900 --> 01:41:30.900]   I didn't know why I thought they left it.
[01:41:30.900 --> 01:41:31.900]   Okay.
[01:41:31.900 --> 01:41:32.900]   No, no, they merged it.
[01:41:32.900 --> 01:41:34.940]   They killed musically and they merged it.
[01:41:34.940 --> 01:41:42.260]   Oh, you guys, the road near my house just got closed down because of falling rock.
[01:41:42.260 --> 01:41:44.260]   I just had to tell you that.
[01:41:44.260 --> 01:41:45.740]   Did you just get a text?
[01:41:45.740 --> 01:41:46.740]   I did.
[01:41:46.740 --> 01:41:47.740]   Yeah.
[01:41:47.740 --> 01:41:48.740]   I just got a notification.
[01:41:48.740 --> 01:41:54.700]   I signed up for those when we had the fires last year, then they call them nixles here.
[01:41:54.700 --> 01:41:56.340]   And I get them all the time now.
[01:41:56.340 --> 01:41:58.340]   There's police action all over the place.
[01:41:58.340 --> 01:42:01.060]   Well, this you have a mountain near you, Stacy.
[01:42:01.060 --> 01:42:03.580]   You have, you have people going after we mo cars.
[01:42:03.580 --> 01:42:05.820]   How do you get rocks?
[01:42:05.820 --> 01:42:06.820]   Limestone.
[01:42:06.820 --> 01:42:10.380]   We have limestone cliff areas that we tunneled through for our roadways.
[01:42:10.380 --> 01:42:11.380]   And so.
[01:42:11.380 --> 01:42:18.180]   Oh, limestone, you know, if you have limestone anywhere in the world, pretty much, you've
[01:42:18.180 --> 01:42:20.380]   got holes in the, in the ground.
[01:42:20.380 --> 01:42:21.380]   Yeah.
[01:42:21.380 --> 01:42:23.860]   Well, central Texas is on a giant carst system.
[01:42:23.860 --> 01:42:24.860]   All right.
[01:42:24.860 --> 01:42:25.860]   It's famous.
[01:42:25.860 --> 01:42:26.860]   Texas carst.
[01:42:26.860 --> 01:42:29.460]   That's what carstom is named after.
[01:42:29.460 --> 01:42:30.460]   I think.
[01:42:30.460 --> 01:42:32.500]   It's named after the Texas carst.
[01:42:32.500 --> 01:42:33.500]   No.
[01:42:33.500 --> 01:42:38.860]   This is our last story, but this will make you feel good.
[01:42:38.860 --> 01:42:41.900]   A cafe has opened in Japan.
[01:42:41.900 --> 01:42:50.980]   It's staffed by robots, but the robots themselves are controlled by paralyzed people.
[01:42:50.980 --> 01:42:54.940]   It's a startup that specializes in robots for disabled people.
[01:42:54.940 --> 01:43:00.500]   The Ori Hime D is a 120 centimeter, four foot tall robot that can be operated remotely
[01:43:00.500 --> 01:43:03.940]   from a paralyzed person's home.
[01:43:03.940 --> 01:43:07.860]   Even if the operator only has control of their eyes, they can command the robot to move, look
[01:43:07.860 --> 01:43:10.420]   around, speak with people and handle objects.
[01:43:10.420 --> 01:43:12.300]   They get paid $8 an hour to do it.
[01:43:12.300 --> 01:43:17.060]   But of course, there's a bigger benefit because they're out there working.
[01:43:17.060 --> 01:43:18.060]   That's really interesting.
[01:43:18.060 --> 01:43:22.980]   Staff of 10 people with conditions like ALS or spinal cord injuries are working from home.
[01:43:22.980 --> 01:43:25.260]   They're paid a thousand yen an hour.
[01:43:25.260 --> 01:43:29.260]   That's the standard wage for part time work in Japan to serve up coffee and interact with
[01:43:29.260 --> 01:43:35.300]   the clientele as robots.
[01:43:35.300 --> 01:43:38.420]   It was crowdfunded.
[01:43:38.420 --> 01:43:42.540]   They raised $26,000 to do it.
[01:43:42.540 --> 01:43:45.980]   It will not stay open after a few months, unfortunately.
[01:43:45.980 --> 01:43:50.420]   But I think what an interesting and I think great idea.
[01:43:50.420 --> 01:43:54.860]   This is a, these are not robots taking jobs, but giving jobs.
[01:43:54.860 --> 01:43:57.860]   Isn't that cool?
[01:43:57.860 --> 01:43:58.860]   Yes.
[01:43:58.860 --> 01:44:04.740]   Really neat story from Sora News 24.
[01:44:04.740 --> 01:44:10.900]   Their headline is bringing you yesterday's news from Japan and Asia today, which is not
[01:44:10.900 --> 01:44:11.900]   okay.
[01:44:11.900 --> 01:44:16.700]   Admittedly, the best slogan.
[01:44:16.700 --> 01:44:22.300]   Maybe a little better than democracy dies in darkness, but it's, you know, the yesterday's
[01:44:22.300 --> 01:44:26.220]   news today from Japan and Asia.
[01:44:26.220 --> 01:44:30.340]   That concludes our list of stories for the day.
[01:44:30.340 --> 01:44:32.140]   Oh, I should say one more thing.
[01:44:32.140 --> 01:44:34.180]   I'm very excited about it.
[01:44:34.180 --> 01:44:35.660]   I got.
[01:44:35.660 --> 01:44:41.580]   So earlier last year, I mentioned that we hadn't gotten our what three words banner.
[01:44:41.580 --> 01:44:45.100]   And I got a nice email from Giles at what three words saying, what?
[01:44:45.100 --> 01:44:47.420]   You didn't get your banner.
[01:44:47.420 --> 01:44:52.940]   So ladies and gentlemen, I can now unveil without further ado the what three words address
[01:44:52.940 --> 01:44:58.740]   of the twit, East side studios, glow walnut nasal.
[01:44:58.740 --> 01:45:00.740]   Wow.
[01:45:00.740 --> 01:45:02.540]   Glow walnut nasal.
[01:45:02.540 --> 01:45:04.780]   We're going to put this up.
[01:45:04.780 --> 01:45:08.220]   Where should we put up outside the door?
[01:45:08.220 --> 01:45:09.660]   Right where you put the numbers, right?
[01:45:09.660 --> 01:45:12.260]   The street numbers glow walnut nasal.
[01:45:12.260 --> 01:45:19.860]   If you ever need to get to our studio, just remember a glowing walnut in Leo's nose.
[01:45:19.860 --> 01:45:26.220]   Can I just say, you know, oh, hey, G, where is or take me to glow walnut nasal?
[01:45:26.220 --> 01:45:30.660]   I think you can in some, some, some, some jurisdictions.
[01:45:30.660 --> 01:45:34.780]   So I just searched on glow walnut nasal besides you talking about this.
[01:45:34.780 --> 01:45:37.780]   The next is nuts about walnut.
[01:45:37.780 --> 01:45:38.780]   Yeah.
[01:45:38.780 --> 01:45:41.580]   A wood magazine and then plastic surgery in Danville.
[01:45:41.580 --> 01:45:44.380]   Oh, you need to get the glowing walnut.
[01:45:44.380 --> 01:45:45.780]   If you need to get removed.
[01:45:45.780 --> 01:45:46.780]   Yeah.
[01:45:46.780 --> 01:45:47.780]   Yeah.
[01:45:47.780 --> 01:45:50.820]   So this is what used Google considerably.
[01:45:50.820 --> 01:45:54.260]   What three the number three, what three words.com.
[01:45:54.260 --> 01:45:59.900]   And if you go to their map, which isn't very complete, but if you go to their map, you'll
[01:45:59.900 --> 01:46:03.980]   see glow walnut nasal leaves you right to our front door.
[01:46:03.980 --> 01:46:05.660]   Yeah.
[01:46:05.660 --> 01:46:13.100]   And then I think there's a way to like then send gloat, send those coordinates to Google.
[01:46:13.100 --> 01:46:20.260]   Like, yeah, like navigate to location on Google maps.
[01:46:20.260 --> 01:46:21.260]   Pick your app.
[01:46:21.260 --> 01:46:22.260]   Pick your app.
[01:46:22.260 --> 01:46:23.260]   Yeah.
[01:46:23.260 --> 01:46:27.700]   So it tells you what it is, how to get to global walnut nasal.
[01:46:27.700 --> 01:46:29.740]   It's one minute, 13 feet away.
[01:46:29.740 --> 01:46:30.740]   I don't know.
[01:46:30.740 --> 01:46:35.620]   It's going to take me less than a minute, I think, but maybe there's some traffic.
[01:46:35.620 --> 01:46:37.020]   Somewhere outside the studio.
[01:46:37.020 --> 01:46:40.220]   It's heavy traffic in the hall outside.
[01:46:40.220 --> 01:46:41.820]   Exactly.
[01:46:41.820 --> 01:46:42.820]   So thank you, Giles.
[01:46:42.820 --> 01:46:43.820]   I got it.
[01:46:43.820 --> 01:46:46.940]   I, I, some, he said we sent it to you.
[01:46:46.940 --> 01:46:48.860]   I have it here, but maybe it got lost.
[01:46:48.860 --> 01:46:53.300]   Maybe the postman stole it because he doesn't want to be put out of a job.
[01:46:53.300 --> 01:46:57.900]   I think I still demand credit for being the first to mention one three words.
[01:46:57.900 --> 01:47:02.420]   Many years ago, the network long ago, very I rediscovered it having forgotten completely.
[01:47:02.420 --> 01:47:03.420]   You did.
[01:47:03.420 --> 01:47:04.420]   Yeah.
[01:47:04.420 --> 01:47:08.140]   Of course, I, I get this that I am made sure to point out I'm just thrilled.
[01:47:08.140 --> 01:47:10.660]   This is, they give this to you for free.
[01:47:10.660 --> 01:47:11.660]   Really?
[01:47:11.660 --> 01:47:12.660]   Yeah.
[01:47:12.660 --> 01:47:13.660]   Yeah.
[01:47:13.660 --> 01:47:14.660]   Anybody can get this.
[01:47:14.660 --> 01:47:16.460]   If you go to what three words.com.
[01:47:16.460 --> 01:47:23.860]   So I have a problem on Google maps is that it thinks my house is about, it doesn't know
[01:47:23.860 --> 01:47:29.220]   that where the street goes exactly why you need this, but it doesn't, it, it refused
[01:47:29.220 --> 01:47:30.220]   stops back there.
[01:47:30.220 --> 01:47:34.340]   See, the thing about what three words is it's not based on street addresses.
[01:47:34.340 --> 01:47:35.340]   Everything is primitive.
[01:47:35.340 --> 01:47:36.340]   Right.
[01:47:36.340 --> 01:47:42.140]   That they've divided the entire world up into three minute squares everywhere in the world.
[01:47:42.140 --> 01:47:47.940]   And every three minute square has its own unique three word description, which I think
[01:47:47.940 --> 01:47:48.940]   is great.
[01:47:48.940 --> 01:47:55.300]   So yeah, I don't know if they still offer the free placard, but I'm thrilled to have
[01:47:55.300 --> 01:47:56.300]   mine.
[01:47:56.300 --> 01:47:57.300]   Yeah.
[01:47:57.300 --> 01:47:58.300]   I like this.
[01:47:58.300 --> 01:48:03.660]   We should, yeah, let's put it next to our number or something or just in the, put it
[01:48:03.660 --> 01:48:04.660]   on the door.
[01:48:04.660 --> 01:48:05.660]   I don't know.
[01:48:05.660 --> 01:48:06.660]   Glow walnut nasal.
[01:48:06.660 --> 01:48:07.660]   All right.
[01:48:07.660 --> 01:48:10.260]   Stacy, you have a pick this week.
[01:48:10.260 --> 01:48:11.260]   I don't.
[01:48:11.260 --> 01:48:17.900]   I had a great one for the week before, but I did not, I'm sorry you guys, the holidays,
[01:48:17.900 --> 01:48:18.900]   everything.
[01:48:18.900 --> 01:48:22.700]   Did all of your case so happens?
[01:48:22.700 --> 01:48:24.580]   Automated stuff work in your house.
[01:48:24.580 --> 01:48:27.060]   Like the Christmas and did all of that happen.
[01:48:27.060 --> 01:48:31.340]   The blinds go dancing and the blinds did go dancing.
[01:48:31.340 --> 01:48:33.060]   The yes, everything happened.
[01:48:33.060 --> 01:48:38.620]   So I just, as I'm, as I'm taking stuff down, I'm also taking other stuff out for the, like
[01:48:38.620 --> 01:48:40.780]   getting ready for the house to sell.
[01:48:40.780 --> 01:48:42.980]   So oh no, I'm dismantling.
[01:48:42.980 --> 01:48:44.380]   This is serious.
[01:48:44.380 --> 01:48:45.380]   You mean it.
[01:48:45.380 --> 01:48:46.380]   Oh yeah.
[01:48:46.380 --> 01:48:48.380]   He's dismantling her IOT creation.
[01:48:48.380 --> 01:48:50.380]   Yeah, it's going to be.
[01:48:50.380 --> 01:48:52.220]   And I'm going to put in some other stuff anyway.
[01:48:52.220 --> 01:48:54.220]   Are you taking the reformer with you?
[01:48:54.220 --> 01:48:56.860]   Yeah, the reformers go with me.
[01:48:56.860 --> 01:48:58.260]   That's yeah.
[01:48:58.260 --> 01:49:00.740]   I've moved my reformer several times now.
[01:49:00.740 --> 01:49:01.740]   Yeah.
[01:49:01.740 --> 01:49:03.740]   I'm here when we're showing the house.
[01:49:03.740 --> 01:49:06.220]   I love my reformer.
[01:49:06.220 --> 01:49:07.860]   Well, how about you, Jeff?
[01:49:07.860 --> 01:49:08.860]   Do you have a number?
[01:49:08.860 --> 01:49:09.860]   I have a few.
[01:49:09.860 --> 01:49:10.860]   Oh good.
[01:49:10.860 --> 01:49:11.860]   Thank you.
[01:49:11.860 --> 01:49:12.860]   Thank you.
[01:49:12.860 --> 01:49:13.860]   I'm going to make up.
[01:49:13.860 --> 01:49:14.860]   I am.
[01:49:14.860 --> 01:49:15.860]   Okay.
[01:49:15.860 --> 01:49:21.860]   Well, one of the business one, I, so Google has evidently says this report, profited $3
[01:49:21.860 --> 01:49:24.780]   billion on made by Google stuff.
[01:49:24.780 --> 01:49:28.260]   Everybody made fun when they had their laptops and their stuff.
[01:49:28.260 --> 01:49:29.980]   That's a significant amount of money.
[01:49:29.980 --> 01:49:30.980]   Three bills.
[01:49:30.980 --> 01:49:31.980]   Wow.
[01:49:31.980 --> 01:49:32.980]   From their hardware.
[01:49:32.980 --> 01:49:35.100]   There's their hardware gains traction.
[01:49:35.100 --> 01:49:37.260]   So I thought that was actually newsworthy.
[01:49:37.260 --> 01:49:41.700]   This is actually an investor predicting that they'll make $3 billion this year from their
[01:49:41.700 --> 01:49:42.700]   hardware.
[01:49:42.700 --> 01:49:43.700]   Yeah.
[01:49:43.700 --> 01:49:45.500]   But the fact that they can even be close to that number would surprise the heck out of
[01:49:45.500 --> 01:49:46.500]   me.
[01:49:46.500 --> 01:49:47.500]   Yeah.
[01:49:47.500 --> 01:49:48.500]   I mean, their revenues are actually pretty good.
[01:49:48.500 --> 01:49:51.060]   It's pretty hard to separate them out.
[01:49:51.060 --> 01:49:53.220]   But yeah, they're selling a lot of Chromecast.
[01:49:53.220 --> 01:49:54.980]   They're selling a lot of stuff.
[01:49:54.980 --> 01:50:00.740]   So the fun ones that's run a podcast world is that in the last week BBC made, I hope you
[01:50:00.740 --> 01:50:05.020]   didn't have this already, 16,000 sound effects available for free.
[01:50:05.020 --> 01:50:06.180]   Carson go mad.
[01:50:06.180 --> 01:50:07.820]   Oh, I want them.
[01:50:07.820 --> 01:50:13.260]   So I did two searches here, one for screams and one for doors.
[01:50:13.260 --> 01:50:14.260]   So you can pick a few.
[01:50:14.260 --> 01:50:19.660]   There's one that I think of the screams at the bottom of that page is orgies.
[01:50:19.660 --> 01:50:20.660]   How about one woman?
[01:50:20.660 --> 01:50:23.460]   Three fairly short screams.
[01:50:23.460 --> 01:50:24.460]   That's a good one.
[01:50:24.460 --> 01:50:25.460]   Yes.
[01:50:25.460 --> 01:50:26.460]   Okay.
[01:50:26.460 --> 01:50:28.940]   It's enough.
[01:50:28.940 --> 01:50:32.180]   How about teenage girls screaming in a theater?
[01:50:32.180 --> 01:50:37.340]   They're a little slow.
[01:50:37.340 --> 01:50:39.340]   What is a burlesque scream?
[01:50:39.340 --> 01:50:40.900]   Well, we'll find out.
[01:50:40.900 --> 01:50:42.900]   Let's find out.
[01:50:42.900 --> 01:50:47.420]   Oh, it's like a, it's like a, yeah.
[01:50:47.420 --> 01:50:50.780]   Most of these are marked comedy, by the way, just be clear here.
[01:50:50.780 --> 01:50:53.980]   I got, I got almost like, that sounds like Stacy.
[01:50:53.980 --> 01:50:57.420]   Oh, my goodness.
[01:50:57.420 --> 01:50:59.420]   The second they're put her page, it had beheadings.
[01:50:59.420 --> 01:51:02.340]   I thought, oh my God, they didn't do that, but they were comedy beheadings.
[01:51:02.340 --> 01:51:14.900]   Wait a minute, this has standard orgies, as opposed to, I don't know.
[01:51:14.900 --> 01:51:16.900]   What's the whip for?
[01:51:16.900 --> 01:51:18.900]   60, 60s.
[01:51:18.900 --> 01:51:19.900]   How is that?
[01:51:19.900 --> 01:51:22.740]   That's totally average.
[01:51:22.740 --> 01:51:25.500]   I think maybe orgies mean something different in the UK.
[01:51:25.500 --> 01:51:27.740]   I think it does.
[01:51:27.740 --> 01:51:33.500]   I think it means like, like, it's probably a technical term used by the BBC to indicate
[01:51:33.500 --> 01:51:37.940]   like riotous, raucous calamity.
[01:51:37.940 --> 01:51:39.420]   There's a lot of funny door ones.
[01:51:39.420 --> 01:51:40.900]   There's machines of all kinds of life.
[01:51:40.900 --> 01:51:44.100]   If you search for things, you search for line type, you can hear a line type.
[01:51:44.100 --> 01:51:50.620]   It's like, it's like, these were, I presume, used, you know, by BBC radio theater and others.
[01:51:50.620 --> 01:51:51.620]   God's.
[01:51:51.620 --> 01:51:52.620]   Yeah.
[01:51:52.620 --> 01:51:53.620]   Wow.
[01:51:53.620 --> 01:51:55.380]   You know, oh, I see.
[01:51:55.380 --> 01:51:57.580]   I can take screams out, right?
[01:51:57.580 --> 01:52:00.180]   Unmodulated groove on a blank record.
[01:52:00.180 --> 01:52:06.780]   See, how many times have you needed this?
[01:52:06.780 --> 01:52:13.380]   I think that's, I think Moby used that in one of his recordings.
[01:52:13.380 --> 01:52:20.060]   Anti-aircraft batteries, Aquaria, army, braiding machines.
[01:52:20.060 --> 01:52:25.220]   Here's what a braiding machine sounds like.
[01:52:25.220 --> 01:52:29.860]   What the hell is a braiding machine?
[01:52:29.860 --> 01:52:30.860]   No idea.
[01:52:30.860 --> 01:52:33.780]   No, I hear some fishing industry sounds.
[01:52:33.780 --> 01:52:34.780]   Wow.
[01:52:34.780 --> 01:52:36.420]   This is great.
[01:52:36.420 --> 01:52:37.420]   It's his embatia.
[01:52:37.420 --> 01:52:40.340]   There's one fizzy liquids was below fishing.
[01:52:40.340 --> 01:52:46.820]   This is why we pay, we pay for our television licenses here in the UK.
[01:52:46.820 --> 01:52:50.220]   Wait, you don't live in the UK?
[01:52:50.220 --> 01:52:52.220]   Never mind.
[01:52:52.220 --> 01:52:58.020]   Oh, this is radio heaven.
[01:52:58.020 --> 01:52:59.020]   Yeah.
[01:52:59.020 --> 01:53:03.940]   So, so, Carson, I think that when we do certain things, you got to have sound effects already
[01:53:03.940 --> 01:53:04.940]   for us.
[01:53:04.940 --> 01:53:06.740]   You can mock us in all kinds of.
[01:53:06.740 --> 01:53:07.980]   I already, I had a few selections.
[01:53:07.980 --> 01:53:09.980]   Let's find something for techno panic.
[01:53:09.980 --> 01:53:12.500]   Let's have a every time someone says techno panic.
[01:53:12.500 --> 01:53:14.460]   We need a screams page.
[01:53:14.460 --> 01:53:15.460]   Yeah.
[01:53:15.460 --> 01:53:16.460]   Yeah.
[01:53:16.460 --> 01:53:19.460]   Here, let's.
[01:53:19.460 --> 01:53:24.740]   Techno panic struck earlier today.
[01:53:24.740 --> 01:53:26.940]   I don't know.
[01:53:26.940 --> 01:53:28.100]   Wow.
[01:53:28.100 --> 01:53:30.580]   This is a lot of these.
[01:53:30.580 --> 01:53:32.580]   This is great.
[01:53:32.580 --> 01:53:39.660]   So this makes, reminds me of the fact that I've been watching Homecoming on, is it Amazon
[01:53:39.660 --> 01:53:40.660]   Prime?
[01:53:40.660 --> 01:53:42.100]   I think it is Amazon Prime.
[01:53:42.100 --> 01:53:43.100]   I watched that.
[01:53:43.100 --> 01:53:45.300]   It's based on a podcast from Kimlet.
[01:53:45.300 --> 01:53:49.380]   Oh, hey, you know what my here is a suggestion.
[01:53:49.380 --> 01:53:52.940]   Although it's old, so it's probably not applicable for most people.
[01:53:52.940 --> 01:53:58.580]   We're listening to the unexplained disappearance of Mars Patel with my daughter and I.
[01:53:58.580 --> 01:54:00.100]   And that is wonderful for kids.
[01:54:00.100 --> 01:54:01.580]   Is that a serial?
[01:54:01.580 --> 01:54:03.260]   It's like serial for kids.
[01:54:03.260 --> 01:54:05.340]   Oh, serial for kids.
[01:54:05.340 --> 01:54:09.300]   It's in they use like kids voices and it's very fun.
[01:54:09.300 --> 01:54:12.980]   If you have a child and where is that?
[01:54:12.980 --> 01:54:14.580]   On all your iTunes, podcast.
[01:54:14.580 --> 01:54:15.580]   It's a podcast.
[01:54:15.580 --> 01:54:16.580]   It's a podcast.
[01:54:16.580 --> 01:54:17.580]   Yeah.
[01:54:17.580 --> 01:54:18.580]   It's just a straight up podcast.
[01:54:18.580 --> 01:54:24.140]   We're doing some fiction podcasts.
[01:54:24.140 --> 01:54:30.620]   I feel like I always wanted to, but I thought, but I need good scripts.
[01:54:30.620 --> 01:54:33.260]   Yes, you would have to write them.
[01:54:33.260 --> 01:54:34.420]   They're hard to come by.
[01:54:34.420 --> 01:54:35.420]   Yeah.
[01:54:35.420 --> 01:54:38.220]   I always start not this Friday, but next Friday.
[01:54:38.220 --> 01:54:43.980]   I am volunteering to teach a podcast in class for my daughter's school and we're either
[01:54:43.980 --> 01:54:48.020]   going to do a story based one that they write or we're going to do a news one.
[01:54:48.020 --> 01:54:50.020]   That's really cool.
[01:54:50.020 --> 01:54:51.020]   Yeah.
[01:54:51.020 --> 01:54:58.420]   So actually speaking of podcasts and you might include this, incorporate this into your curriculum.
[01:54:58.420 --> 01:55:04.500]   This is a new site called listen notes.com written by one guy in San Francisco scratching
[01:55:04.500 --> 01:55:05.500]   his own itch.
[01:55:05.500 --> 01:55:07.900]   It's a podcast search engine.
[01:55:07.900 --> 01:55:17.980]   So if I enter Jeff Jarvis in here, it will come up with your appearances on the good fight.
[01:55:17.980 --> 01:55:24.860]   This week in Google for 88, our last episode avoiding airway disasters with Dr. Jeff.
[01:55:24.860 --> 01:55:25.860]   Charge it.
[01:55:25.860 --> 01:55:26.860]   I think that's that's the other.
[01:55:26.860 --> 01:55:27.860]   That's the other Jeff.
[01:55:27.860 --> 01:55:32.660]   That's the other who actually has talent and is needed in the world.
[01:55:32.660 --> 01:55:35.540]   Here's a German one that you did.
[01:55:35.540 --> 01:55:36.780]   Oh, I'm our media.
[01:55:36.780 --> 01:55:37.780]   My.
[01:55:37.780 --> 01:55:38.780]   My.
[01:55:38.780 --> 01:55:40.900]   So isn't this cool?
[01:55:40.900 --> 01:55:42.300]   And then on a bunch of twigs.
[01:55:42.300 --> 01:55:44.660]   So you can search for any subject.
[01:55:44.660 --> 01:55:47.300]   Let's search for, I don't know, Waymo.
[01:55:47.300 --> 01:55:54.100]   You can find and they search through many, many, many podcasts to find the words.
[01:55:54.100 --> 01:55:57.060]   You know, it's Google for podcasts.
[01:55:57.060 --> 01:55:58.900]   And it's really very cool.
[01:55:58.900 --> 01:56:01.740]   I wanted to give, give him a credit for creating.
[01:56:01.740 --> 01:56:03.340]   Who is he?
[01:56:03.340 --> 01:56:04.980]   Who he it says here.
[01:56:04.980 --> 01:56:06.180]   Someone on the site.
[01:56:06.180 --> 01:56:07.180]   Yeah.
[01:56:07.180 --> 01:56:10.980]   I found it somewhere here it is about.
[01:56:10.980 --> 01:56:12.940]   Sorry.
[01:56:12.940 --> 01:56:16.380]   I asked.
[01:56:16.380 --> 01:56:17.620]   Ben Bin Fong.
[01:56:17.620 --> 01:56:18.620]   He lives in San Francisco.
[01:56:18.620 --> 01:56:24.740]   I quit my day job from next door, 748 days and 23 hours ago.
[01:56:24.740 --> 01:56:28.460]   Obviously a programming nerd.
[01:56:28.460 --> 01:56:30.140]   He says, I'm an avid podcast listener.
[01:56:30.140 --> 01:56:32.820]   I listen to five plus hours every day.
[01:56:32.820 --> 01:56:38.140]   I used to subscribe to only a few, but I still couldn't listen to all episodes.
[01:56:38.140 --> 01:56:39.140]   So he created this.
[01:56:39.140 --> 01:56:43.580]   I built, he said it was another, I can build this in a weekendish project, but it's gotten
[01:56:43.580 --> 01:56:45.060]   bigger and bigger and bigger.
[01:56:45.060 --> 01:56:47.820]   And so I think this is really cool.
[01:56:47.820 --> 01:56:50.380]   He has a form for submitting a missing podcast.
[01:56:50.380 --> 01:56:53.380]   A lot of new, new means or discoveries.
[01:56:53.380 --> 01:56:54.380]   Yeah.
[01:56:54.380 --> 01:56:55.380]   Yeah.
[01:56:55.380 --> 01:56:56.380]   Let me mention something else while we're here.
[01:56:56.380 --> 01:56:57.380]   So, so it's German.
[01:56:57.380 --> 01:57:00.380]   SteadyHQ.com.
[01:57:00.380 --> 01:57:02.780]   I needed when I wrote this piece about the German journalism stuff.
[01:57:02.780 --> 01:57:07.140]   I wanted to read this one guy and there was a paywall and it all hell.
[01:57:07.140 --> 01:57:11.860]   And this is an amazing site that has pages and pages and pages of seven pages filled
[01:57:11.860 --> 01:57:12.860]   with sites.
[01:57:12.860 --> 01:57:13.860]   They're all German.
[01:57:13.860 --> 01:57:15.900]   But membership sites.
[01:57:15.900 --> 01:57:17.740]   And so it makes sense.
[01:57:17.740 --> 01:57:23.700]   Yeah, we're not going to pay for all of them, but it's an easy way to throw some money at
[01:57:23.700 --> 01:57:27.260]   someplace enjoying as opposed to a paywall.
[01:57:27.260 --> 01:57:30.660]   So I wish we had something like this in the US that was an aggregator of membership opportunities.
[01:57:30.660 --> 01:57:32.860]   So you pay once and you get all this time.
[01:57:32.860 --> 01:57:37.620]   You sign up once, no, you sign up once and then you can choose to easily say, okay, I
[01:57:37.620 --> 01:57:40.180]   want to join that one or that one.
[01:57:40.180 --> 01:57:41.180]   SteadyHQ.
[01:57:41.180 --> 01:57:44.180]   SteadyHQ.com.
[01:57:44.180 --> 01:57:45.180]   Explore on the top.
[01:57:45.180 --> 01:57:51.500]   What we're learning in the video world is that there is reaggregation happening because
[01:57:51.500 --> 01:57:57.260]   Roku's just announced that it's going to do as Google already does with YouTube TV and
[01:57:57.260 --> 01:58:04.220]   others to give you a chance to subscribe to Showtime or HBO within the Roku ecosystem.
[01:58:04.220 --> 01:58:08.340]   So you get one bill and I think that's what users want.
[01:58:08.340 --> 01:58:12.580]   And I bet you something like that won't happen with paywalls because nobody wants to get
[01:58:12.580 --> 01:58:14.900]   nickeled and dined after maybe they do.
[01:58:14.900 --> 01:58:16.740]   Isn't that like what texture is?
[01:58:16.740 --> 01:58:21.500]   Yeah, of course, Apple has bought texture and I don't know what's going to happen to
[01:58:21.500 --> 01:58:22.500]   it.
[01:58:22.500 --> 01:58:25.660]   I think it's going to be incorporated into an Apple news product.
[01:58:25.660 --> 01:58:28.740]   That's the rumor, but we don't know yet.
[01:58:28.740 --> 01:58:32.860]   There's some problem because you know, remember, texture was created by magazines like Hearst
[01:58:32.860 --> 01:58:39.260]   and Conde Nast as a way to sell magazines in digital environment, but all those deals
[01:58:39.260 --> 01:58:41.340]   occurred because they were owned by themselves.
[01:58:41.340 --> 01:58:44.220]   Now that they're owned by Apple, I think all those deals are off the table and Apple
[01:58:44.220 --> 01:58:46.380]   has to renegotiate.
[01:58:46.380 --> 01:58:48.900]   And it's I think having a sticky time renegotiate.
[01:58:48.900 --> 01:58:49.900]   I'm not sure.
[01:58:49.900 --> 01:58:50.900]   And magazines are in trouble.
[01:58:50.900 --> 01:58:54.340]   There was a story somewhere to forget where I saw it that women's magazines are particularly
[01:58:54.340 --> 01:58:56.540]   in trouble, which is interesting.
[01:58:56.540 --> 01:58:58.060]   Stacy, do you ever.
[01:58:58.060 --> 01:59:02.060]   I had 17 when I was.
[01:59:02.060 --> 01:59:03.060]   When you were 13.
[01:59:03.060 --> 01:59:04.060]   When you were 13.
[01:59:04.060 --> 01:59:05.060]   Yeah.
[01:59:05.060 --> 01:59:06.060]   Yeah.
[01:59:06.060 --> 01:59:10.900]   I pick up a Vogue every now and then just to look at the fashion spreads, but I don't
[01:59:10.900 --> 01:59:13.300]   subscribe to any of the movies.
[01:59:13.300 --> 01:59:16.260]   You were never a glamour reader or any of that.
[01:59:16.260 --> 01:59:18.580]   I like Cosmo.
[01:59:18.580 --> 01:59:19.580]   Not really.
[01:59:19.580 --> 01:59:23.100]   No, I mean, no, I don't read a lot of.
[01:59:23.100 --> 01:59:26.220]   I only read really high-falutin magazines.
[01:59:26.220 --> 01:59:27.220]   I don't know what.
[01:59:27.220 --> 01:59:30.100]   No, I was like, I read Scientific American.
[01:59:30.100 --> 01:59:33.020]   I read The New Yorker, The Economist.
[01:59:33.020 --> 01:59:35.420]   I'm like, God, I am a pretentious person.
[01:59:35.420 --> 01:59:38.420]   I thought you were a family circle type.
[01:59:38.420 --> 01:59:42.860]   I read websites for my silly women's comment.
[01:59:42.860 --> 01:59:44.620]   Like my fun kind of stuff.
[01:59:44.620 --> 01:59:51.100]   I think this is why they're in trouble because I look at almost everybody I know.
[01:59:51.100 --> 01:59:52.780]   People don't subscribe to Dead Trees anymore.
[01:59:52.780 --> 01:59:53.780]   They just read them.
[01:59:53.780 --> 01:59:54.940]   I used to buy them by the pound.
[01:59:54.940 --> 01:59:57.740]   They had to double bag me at the Hudson News.
[01:59:57.740 --> 01:59:58.740]   Yeah.
[01:59:58.740 --> 02:00:00.180]   And I just don't buy them anymore.
[02:00:00.180 --> 02:00:01.180]   Well, that's good.
[02:00:01.180 --> 02:00:03.860]   The Hudson News has gone too.
[02:00:03.860 --> 02:00:07.580]   I mean, I subscribed to four magazines.
[02:00:07.580 --> 02:00:09.580]   They just are really expensive.
[02:00:09.580 --> 02:00:13.180]   A funny, a funny quote thing happened with The Atlantic.
[02:00:13.180 --> 02:00:18.140]   I have a digital subscription to The Atlantic and I got a paper copy of it last month.
[02:00:18.140 --> 02:00:23.540]   And then I just got an email from them saying, "Due to an error, we sent all our digital
[02:00:23.540 --> 02:00:25.860]   subscribers paper copies.
[02:00:25.860 --> 02:00:32.580]   But if you would like to continue your paper subscription for free, check here."
[02:00:32.580 --> 02:00:38.540]   So I said, "Well, yeah, if it's free, what's that all about, Jeff?
[02:00:38.540 --> 02:00:39.540]   You know about how this is?"
[02:00:39.540 --> 02:00:41.620]   Well, this is now old by The Atlantic.
[02:00:41.620 --> 02:00:42.620]   You got that too?
[02:00:42.620 --> 02:00:44.620]   I got it from Wired.
[02:00:44.620 --> 02:00:45.620]   Oh, Conde.
[02:00:45.620 --> 02:00:46.780]   It's a new trick.
[02:00:46.780 --> 02:00:48.700]   It's a new trick.
[02:00:48.700 --> 02:00:50.980]   Is it to go to subscriber numbers?
[02:00:50.980 --> 02:00:51.980]   Yes.
[02:00:51.980 --> 02:00:52.980]   Okay.
[02:00:52.980 --> 02:00:53.980]   That's...
[02:00:53.980 --> 02:00:54.820]   I got it from the
[02:00:54.820 --> 02:00:55.820]   YouTube channel.
[02:00:55.820 --> 02:00:56.820]   I got it from the YouTube channel.
[02:00:56.820 --> 02:00:57.820]   I got it from the YouTube channel.
[02:00:57.820 --> 02:00:58.820]   I got it from the YouTube channel.
[02:00:58.820 --> 02:00:59.820]   I got it from the YouTube channel.
[02:00:59.820 --> 02:01:00.820]   I got it from the YouTube channel.
[02:01:00.820 --> 02:01:01.820]   I got it from the YouTube channel.
[02:01:01.820 --> 02:01:02.820]   I got it from the YouTube channel.
[02:01:02.820 --> 02:01:03.820]   I got it from the YouTube channel.
[02:01:03.820 --> 02:01:04.820]   I got it from the YouTube channel.
[02:01:04.820 --> 02:01:05.820]   I got it from the YouTube channel.
[02:01:05.820 --> 02:01:06.820]   I got it from the YouTube channel.
[02:01:06.820 --> 02:01:07.820]   I got it from the YouTube channel.
[02:01:07.820 --> 02:01:08.820]   I got it from the YouTube channel.
[02:01:08.820 --> 02:01:09.820]   I got it from the YouTube channel.
[02:01:09.820 --> 02:01:10.820]   I got it from the YouTube channel.
[02:01:10.820 --> 02:01:11.820]   I got it from the YouTube channel.
[02:01:11.820 --> 02:01:12.820]   I got it from the YouTube channel.
[02:01:12.820 --> 02:01:13.820]   I got it from the YouTube channel.
[02:01:13.820 --> 02:01:14.820]   I got it from the YouTube channel.
[02:01:14.820 --> 02:01:15.820]   I got it from the YouTube channel.
[02:01:15.820 --> 02:01:22.820]   I got it from the YouTube channel.
[02:01:22.820 --> 02:01:25.820]   I got it from the YouTube channel.
[02:01:25.820 --> 02:01:26.820]   I got it from the YouTube channel.
[02:01:26.820 --> 02:01:27.820]   I got it from the YouTube channel.
[02:01:27.820 --> 02:01:28.820]   I got it from the YouTube channel.
[02:01:28.820 --> 02:01:29.820]   I got it from the YouTube channel.
[02:01:29.820 --> 02:01:30.820]   I got it from the YouTube channel.
[02:01:30.820 --> 02:01:31.820]   I got it from the YouTube channel.
[02:01:31.820 --> 02:01:32.820]   I got it from the YouTube channel.
[02:01:32.820 --> 02:01:33.820]   I got it from the YouTube channel.
[02:01:33.820 --> 02:01:34.820]   I got it from the YouTube channel.
[02:01:34.820 --> 02:01:35.820]   I got it from the YouTube channel.
[02:01:35.820 --> 02:01:36.820]   I got it from the YouTube channel.
[02:01:36.820 --> 02:01:37.820]   I got it from the YouTube channel.
[02:01:37.820 --> 02:01:38.820]   I got it from the YouTube channel.
[02:01:38.820 --> 02:01:39.820]   I got it from the YouTube channel.
[02:01:39.820 --> 02:01:40.820]   I got it from the YouTube channel.
[02:01:40.820 --> 02:01:41.820]   I got it from the YouTube channel.
[02:01:41.820 --> 02:01:42.820]   I got it from the YouTube channel.
[02:01:42.820 --> 02:01:43.820]   I got it from the YouTube channel.
[02:01:43.820 --> 02:01:44.820]   I got it from the YouTube channel.
[02:01:44.820 --> 02:01:45.820]   I got it from the YouTube channel.
[02:01:45.820 --> 02:01:46.820]   I got it from the YouTube channel.
[02:01:46.820 --> 02:01:47.820]   I got it from the YouTube channel.
[02:01:47.820 --> 02:01:48.820]   I got it from the YouTube channel.
[02:01:48.820 --> 02:01:49.820]   I got it from the YouTube channel.
[02:01:49.820 --> 02:01:50.820]   I got it from the YouTube channel.
[02:01:50.820 --> 02:01:51.820]   I got it from the YouTube channel.
[02:01:51.820 --> 02:01:52.820]   I got it from the YouTube channel.
[02:01:52.820 --> 02:01:53.820]   I got it from the YouTube channel.
[02:01:53.820 --> 02:01:54.820]   I got it from the YouTube channel.
[02:01:54.820 --> 02:01:55.820]   I got it from the YouTube channel.
[02:01:55.820 --> 02:01:56.820]   I got it from the YouTube channel.
[02:01:56.820 --> 02:01:57.820]   I got it from the YouTube channel.
[02:01:57.820 --> 02:01:58.820]   I got it from the YouTube channel.
[02:01:58.820 --> 02:01:59.820]   I got it from the YouTube channel.
[02:01:59.820 --> 02:02:00.820]   I got it from the YouTube channel.
[02:02:00.820 --> 02:02:01.820]   I got it from the YouTube channel.
[02:02:01.820 --> 02:02:02.820]   I got it from the YouTube channel.
[02:02:02.820 --> 02:02:03.820]   I got it from the YouTube channel.
[02:02:03.820 --> 02:02:04.820]   I got it from the YouTube channel.
[02:02:04.820 --> 02:02:05.820]   I got it from the YouTube channel.
[02:02:05.820 --> 02:02:06.820]   I got it from the YouTube channel.
[02:02:06.820 --> 02:02:08.820]   The Washington Post has its Bezos.
[02:02:08.820 --> 02:02:12.740]   Atlantic has its pal Jobs.
[02:02:12.740 --> 02:02:15.980]   Time has its Benny off.
[02:02:15.980 --> 02:02:20.060]   Some of these folks are now going into the magazine world to try to save them.
[02:02:20.060 --> 02:02:25.060]   Do you worry that this is a little bit like the patronage system of old Venice?
[02:02:25.060 --> 02:02:28.900]   No, I mean the New York Times has been...
[02:02:28.900 --> 02:02:31.820]   They make money though, don't they?
[02:02:31.820 --> 02:02:32.820]   Yeah.
[02:02:32.820 --> 02:02:36.820]   The family has control, but the family owns really relatively small.
[02:02:36.820 --> 02:02:38.820]   Yeah, family zoning, I mean like the Hurst Corporation.
[02:02:38.820 --> 02:02:42.820]   But when I'm talking about patronage, where you buy something and run it, you don't expect
[02:02:42.820 --> 02:02:45.820]   to make any money on it just because you want to support good journalism.
[02:02:45.820 --> 02:02:46.820]   I think that...
[02:02:46.820 --> 02:02:47.820]   I think that...
[02:02:47.820 --> 02:02:48.820]   Go ahead Stacey.
[02:02:48.820 --> 02:02:52.820]   Oh, that's going to say I think Benny off wants to make money on this, right?
[02:02:52.820 --> 02:02:56.820]   It was a personal with Benny, Mark and his wife.
[02:02:56.820 --> 02:03:00.820]   I wanted to get over to the journalism school to talk about that.
[02:03:00.820 --> 02:03:02.700]   He wasn't available to do that.
[02:03:02.700 --> 02:03:05.140]   I'm really curious because I don't know what I would do at Time magazine.
[02:03:05.140 --> 02:03:06.140]   I have no idea.
[02:03:06.140 --> 02:03:07.140]   I used to work in the company.
[02:03:07.140 --> 02:03:09.140]   I don't know what I would do with it.
[02:03:09.140 --> 02:03:12.660]   Atlantic is going to get their news from a news weekly.
[02:03:12.660 --> 02:03:14.340]   Not a general interest magazine.
[02:03:14.340 --> 02:03:15.820]   General interest is what's dead.
[02:03:15.820 --> 02:03:18.980]   How about the week, which is a kind of compilation news?
[02:03:18.980 --> 02:03:21.180]   I think that works because it's the value of aggregation.
[02:03:21.180 --> 02:03:22.180]   Yes.
[02:03:22.180 --> 02:03:23.180]   Right.
[02:03:23.180 --> 02:03:28.580]   And the guy who launched the week in the US then worked with the last owner to save the
[02:03:28.580 --> 02:03:30.340]   Atlantic and really made it work.
[02:03:30.340 --> 02:03:31.820]   And then now is it Bloomberg media?
[02:03:31.820 --> 02:03:32.820]   Oh, interesting.
[02:03:32.820 --> 02:03:34.180]   Answer your question about patronage, Leo.
[02:03:34.180 --> 02:03:37.060]   I think that what we're seeing is Gutenberg time.
[02:03:37.060 --> 02:03:38.060]   Watch out.
[02:03:38.060 --> 02:03:44.420]   But I think we're seeing a return to earlier business models as we try out things.
[02:03:44.420 --> 02:03:49.820]   And so patronage is a temporary solution to a problem as we try to understand where the
[02:03:49.820 --> 02:03:51.980]   value really is and rethink our models.
[02:03:51.980 --> 02:03:53.780]   There's a lot of people declaring advertising dead.
[02:03:53.780 --> 02:03:57.860]   I certainly hope not and you certainly hope not.
[02:03:57.860 --> 02:03:58.860]   I don't think it is.
[02:03:58.860 --> 02:04:00.340]   I don't think pay walls are going to solve everything.
[02:04:00.340 --> 02:04:07.340]   But I think patronage is a way to, it was a model of the very early days of publication
[02:04:07.340 --> 02:04:11.220]   of books and it's coming back for a time.
[02:04:11.220 --> 02:04:15.460]   This is a conversation Lisa and I have been having because we looked at GIMLIT.
[02:04:15.460 --> 02:04:17.180]   GIMLIT did three things.
[02:04:17.180 --> 02:04:23.500]   They have ads, but they also got raised at least 20 million in venture capital and they
[02:04:23.500 --> 02:04:27.340]   asked for money from their listeners.
[02:04:27.340 --> 02:04:30.340]   And I've always felt like you can't pick one.
[02:04:30.340 --> 02:04:32.780]   You can't do all three, but I guess you can.
[02:04:32.780 --> 02:04:33.780]   Yeah, you can.
[02:04:33.780 --> 02:04:35.820]   So what do you guys conclude?
[02:04:35.820 --> 02:04:38.660]   Well, Lisa says, well, why don't we do?
[02:04:38.660 --> 02:04:41.820]   Why don't we create a club or something?
[02:04:41.820 --> 02:04:46.140]   Some value added thing that we could then say, hey, support to it.
[02:04:46.140 --> 02:04:49.060]   That's how I discovered you way back when, Leo Laporte.
[02:04:49.060 --> 02:04:50.060]   That's how we started.
[02:04:50.060 --> 02:04:51.060]   But we never raised nothing.
[02:04:51.060 --> 02:04:53.220]   Because Jake Jarvis wanted to give you money.
[02:04:53.220 --> 02:04:55.340]   And I thought, who's this Leo Laporte guy you want to get money to?
[02:04:55.340 --> 02:04:56.340]   Oh, so we worked out.
[02:04:56.340 --> 02:04:57.340]   I did.
[02:04:57.340 --> 02:04:58.340]   I got some money.
[02:04:58.340 --> 02:04:59.340]   I got you.
[02:04:59.340 --> 02:05:00.340]   Young Jake joined.
[02:05:00.340 --> 02:05:02.340]   Well, you didn't know anything about that.
[02:05:02.340 --> 02:05:04.340]   He came out and he was this Google guy.
[02:05:04.340 --> 02:05:05.740]   We knew a Google show.
[02:05:05.740 --> 02:05:08.340]   But yeah, that's how that's how I knew you before you knew me.
[02:05:08.340 --> 02:05:10.540]   That's what we originally thought we would do.
[02:05:10.540 --> 02:05:13.940]   But we never made enough money to support more than one show.
[02:05:13.940 --> 02:05:16.900]   And so, but advertising has been very good for us.
[02:05:16.900 --> 02:05:20.940]   I just do you think it's going to go away?
[02:05:20.940 --> 02:05:24.740]   I think that your advertising is the last that's going to go away because you have the
[02:05:24.740 --> 02:05:28.820]   added value of the relationship to the public and the trust and all that.
[02:05:28.820 --> 02:05:33.220]   But advertisers are constantly under a lot of pressure.
[02:05:33.220 --> 02:05:34.220]   Yeah.
[02:05:34.220 --> 02:05:37.780]   Well, I'm figuring I'll just retire at that point.
[02:05:37.780 --> 02:05:41.820]   But your son, your son, if I were 30, I would be very worried.
[02:05:41.820 --> 02:05:43.060]   Doesn't your son want the business?
[02:05:43.060 --> 02:05:44.060]   Yeah, he does.
[02:05:44.060 --> 02:05:45.060]   Yeah.
[02:05:45.060 --> 02:05:46.540]   But he's going to have to.
[02:05:46.540 --> 02:05:47.900]   If he does, he's going to have to solve it, right?
[02:05:47.900 --> 02:05:49.740]   He's going to have to figure it out.
[02:05:49.740 --> 02:05:51.700]   And the membership patronage.
[02:05:51.700 --> 02:05:52.700]   Yeah.
[02:05:52.700 --> 02:05:54.780]   So, I think that the technical ecosystem will be so different.
[02:05:54.780 --> 02:05:59.580]   So, well, Patreon didn't exist when I was asking Jeff's son for money.
[02:05:59.580 --> 02:06:00.580]   Right.
[02:06:00.580 --> 02:06:03.580]   But Patreon would have made that a lot easier, although I don't know.
[02:06:03.580 --> 02:06:05.020]   I mean, I just don't know.
[02:06:05.020 --> 02:06:06.820]   I just read something.
[02:06:06.820 --> 02:06:09.780]   I'm doing my Gutenberg research.
[02:06:09.780 --> 02:06:12.900]   And I just read a piece, I don't know that pile.
[02:06:12.900 --> 02:06:14.340]   I don't know where it is.
[02:06:14.340 --> 02:06:21.380]   But on books and arguing that, you know, there's a major system of Patreon is now or
[02:06:21.380 --> 02:06:26.020]   on books, there's a major system of self publishing run books that the books don't
[02:06:26.020 --> 02:06:30.220]   die, but the business model of books has already changed pretty radically.
[02:06:30.220 --> 02:06:31.220]   Yeah.
[02:06:31.220 --> 02:06:32.220]   What do you Stacy?
[02:06:32.220 --> 02:06:33.420]   What do you think about?
[02:06:33.420 --> 02:06:38.020]   I mean, you have ads for your shows.
[02:06:38.020 --> 02:06:40.700]   You also charge for newsletter, which I think is, oh, no, you don't.
[02:06:40.700 --> 02:06:41.700]   I don't.
[02:06:41.700 --> 02:06:42.700]   I don't charge for my newsletter.
[02:06:42.700 --> 02:06:45.700]   So you don't have anything you charge for you?
[02:06:45.700 --> 02:06:46.700]   Should you stay?
[02:06:46.700 --> 02:06:52.380]   I don't so I don't have the confidence to charge.
[02:06:52.380 --> 02:06:53.380]   Neither do I.
[02:06:53.380 --> 02:07:04.180]   For my stuff, I guess I know that sounds I just I don't like that model because I think,
[02:07:04.180 --> 02:07:09.980]   especially given if I were slightly more tradish, I would and I am kind of tradish, but I also
[02:07:09.980 --> 02:07:13.820]   deal with like what I feel are really important issues that we should be talking about.
[02:07:13.820 --> 02:07:17.580]   So I want more people to talk about them and think about them.
[02:07:17.580 --> 02:07:21.300]   And I still do a lot of consumer facing stuff.
[02:07:21.300 --> 02:07:23.740]   So that's kind of why.
[02:07:23.740 --> 02:07:24.740]   Yeah.
[02:07:24.740 --> 02:07:27.460]   No, I don't know.
[02:07:27.460 --> 02:07:28.460]   I don't know.
[02:07:28.460 --> 02:07:33.660]   I mean, that's why I hate to hear the notion that advertising might not I love ad supported
[02:07:33.660 --> 02:07:35.820]   free media because it gives us freedom.
[02:07:35.820 --> 02:07:36.820]   Yeah.
[02:07:36.820 --> 02:07:45.300]   A lot and I worry greatly about redlining quality media for just the privileged.
[02:07:45.300 --> 02:07:46.300]   Yeah.
[02:07:46.300 --> 02:07:47.300]   So this amazed me.
[02:07:47.300 --> 02:07:51.420]   So this is this was in wired, which I do have to subscribe to because there's all kinds
[02:07:51.420 --> 02:07:52.420]   of things I want to read.
[02:07:52.420 --> 02:07:54.140]   I paid for one too, but I didn't pay for Bloomberg.
[02:07:54.140 --> 02:07:55.460]   I couldn't bring myself to do that.
[02:07:55.460 --> 02:07:56.460]   No, I'm not.
[02:07:56.460 --> 02:07:58.340]   You know, part of the reason was, God damn it.
[02:07:58.340 --> 02:08:00.180]   Don't they make enough money on those terminals?
[02:08:00.180 --> 02:08:01.180]   I know.
[02:08:01.180 --> 02:08:02.980]   I know.
[02:08:02.980 --> 02:08:06.780]   And so that's but that's part of the reason why you don't go to the well four times.
[02:08:06.780 --> 02:08:11.460]   You know, because I think as as somebody who would buy this stuff, I'm not going to give
[02:08:11.460 --> 02:08:12.460]   them money.
[02:08:12.460 --> 02:08:13.460]   They just got venture capital.
[02:08:13.460 --> 02:08:14.460]   They have ads.
[02:08:14.460 --> 02:08:15.460]   It bugged me.
[02:08:15.460 --> 02:08:18.380]   I had to watch ads after I paid for it.
[02:08:18.380 --> 02:08:21.060]   When I pay for the New York Times and the Washington Post, I'm not paying to buy content.
[02:08:21.060 --> 02:08:23.340]   I'm paying as I support their work.
[02:08:23.340 --> 02:08:24.340]   So that's the difference.
[02:08:24.340 --> 02:08:25.340]   I am.
[02:08:25.340 --> 02:08:26.340]   I do this.
[02:08:26.340 --> 02:08:27.340]   Yeah.
[02:08:27.340 --> 02:08:28.340]   Because I do that.
[02:08:28.340 --> 02:08:29.340]   Yeah.
[02:08:29.340 --> 02:08:30.340]   Yeah.
[02:08:30.340 --> 02:08:34.020]   So, so the future book is here, but it's not what we expected in wired July, I mean, December
[02:08:34.020 --> 02:08:35.020]   2018.
[02:08:35.020 --> 02:08:36.180]   So just out.
[02:08:36.180 --> 02:08:38.140]   So they say Craig Mod says this.
[02:08:38.140 --> 02:08:39.140]   I couldn't believe this.
[02:08:39.140 --> 02:08:43.700]   Almost half of author earnings now come from independently published books.
[02:08:43.700 --> 02:08:44.700]   Wow.
[02:08:44.700 --> 02:08:48.540]   Independent books don't outsell big five books, but they offer higher royalty.
[02:08:48.540 --> 02:08:49.780]   Yeah, that's the point.
[02:08:49.780 --> 02:08:51.580]   70% versus 25%.
[02:08:51.580 --> 02:08:52.580]   Right.
[02:08:52.580 --> 02:08:56.140]   For the first time, perhaps, since the invention of the pre and press authors and small presses
[02:08:56.140 --> 02:08:57.140]   have a viable.
[02:08:57.140 --> 02:09:00.940]   Jeff, did you ever get 25% for a book?
[02:09:00.940 --> 02:09:02.940]   No, that's crazy.
[02:09:02.940 --> 02:09:03.940]   It's 15%.
[02:09:03.940 --> 02:09:04.940]   Yeah, at best.
[02:09:04.940 --> 02:09:07.140]   And it's not 15% of the cover price.
[02:09:07.140 --> 02:09:08.820]   It's 15% of what the bookstore pays.
[02:09:08.820 --> 02:09:11.580]   And you give up 50% of your 50% of the agent.
[02:09:11.580 --> 02:09:12.580]   Yeah.
[02:09:12.580 --> 02:09:13.580]   Okay.
[02:09:13.580 --> 02:09:17.620]   So that's really the it's a low bar because it isn't hard to make more money when you
[02:09:17.620 --> 02:09:23.300]   publish yourself because the publisher is not taking the 85% or more of the revenue.
[02:09:23.300 --> 02:09:27.820]   Yes, but you also don't do I mean, they don't do marketing or things like that.
[02:09:27.820 --> 02:09:29.220]   They don't do marketing anyway.
[02:09:29.220 --> 02:09:31.020]   They don't do marketing anyway.
[02:09:31.020 --> 02:09:33.660]   That was the big lesson to me with the publishers.
[02:09:33.660 --> 02:09:37.020]   If you're if you're Danielle Steele, maybe you'll get a book tour and some marketing
[02:09:37.020 --> 02:09:41.620]   dollars, but any other book you get maybe an editor.
[02:09:41.620 --> 02:09:42.620]   Nothing.
[02:09:42.620 --> 02:09:47.900]   Me, the last time I tell the authors is you've got to hire your own purposes with your own
[02:09:47.900 --> 02:09:48.900]   money.
[02:09:48.900 --> 02:09:49.900]   Yeah.
[02:09:49.900 --> 02:09:53.380]   That's why you might if you're doing that, you might as well hire an indexer and a and
[02:09:53.380 --> 02:09:59.060]   a proffer and a layout person and just do the whole thing yourself or do the old days,
[02:09:59.060 --> 02:10:03.220]   you know, when I was writing books, passed the way to get into the bookstore shelves had
[02:10:03.220 --> 02:10:04.220]   to be straight in the publisher.
[02:10:04.220 --> 02:10:05.220]   Right.
[02:10:05.220 --> 02:10:06.220]   Do you had no choice?
[02:10:06.220 --> 02:10:07.220]   Right.
[02:10:07.220 --> 02:10:08.220]   You didn't get an end cap on your own.
[02:10:08.220 --> 02:10:09.860]   And now, you know, I don't know.
[02:10:09.860 --> 02:10:10.860]   Yeah.
[02:10:10.860 --> 02:10:13.100]   I published 13 books with a publisher.
[02:10:13.100 --> 02:10:15.900]   You don't make any money at it.
[02:10:15.900 --> 02:10:18.740]   But now that's the other thing.
[02:10:18.740 --> 02:10:23.460]   If you reconsidered that kind of stuff now, yeah, I it's all new.
[02:10:23.460 --> 02:10:24.460]   It's all different worlds.
[02:10:24.460 --> 02:10:26.220]   Yeah, business you can roll more.
[02:10:26.220 --> 02:10:27.620]   You can market it here.
[02:10:27.620 --> 02:10:29.820]   I'm too tired.
[02:10:29.820 --> 02:10:34.460]   Speaking of tired, you guys.
[02:10:34.460 --> 02:10:35.460]   Thank you.
[02:10:35.460 --> 02:10:41.740]   Stacy, again, both of everybody go run, subscribe Stacy on IOT.com.
[02:10:41.740 --> 02:10:42.740]   Thank you.
[02:10:42.740 --> 02:10:43.740]   Give her money.
[02:10:43.740 --> 02:10:44.740]   Don't give her money.
[02:10:44.740 --> 02:10:47.100]   Just listen to the ads and buy the products.
[02:10:47.100 --> 02:10:50.620]   She and Kevin do such a great job covering IOT.
[02:10:50.620 --> 02:10:54.340]   You must listen and then get her newsletter because that's free.
[02:10:54.340 --> 02:10:55.340]   It is.
[02:10:55.340 --> 02:10:56.340]   It's free.
[02:10:56.340 --> 02:10:57.540]   And I'd like to think it's totally worth it.
[02:10:57.540 --> 02:10:58.540]   It's totally worth it.
[02:10:58.540 --> 02:10:59.540]   It's awesome.
[02:10:59.540 --> 02:11:02.700]   I subscribe.
[02:11:02.700 --> 02:11:08.380]   Jeff Jarvis, he gets paid handsomely as director of the Townite Center for Entrepreneurial Journalism
[02:11:08.380 --> 02:11:12.060]   at the Craig New York Graduate School of Journalism at the City University of New York.
[02:11:12.060 --> 02:11:13.540]   No, I don't know what he gets paid.
[02:11:13.540 --> 02:11:16.620]   It may be by the by the job title word.
[02:11:16.620 --> 02:11:18.180]   By the word.
[02:11:18.180 --> 02:11:25.660]   Anyway, but he is awesome himself and he publishes regularly at Buzzmachine.com and medium.
[02:11:25.660 --> 02:11:31.580]   He's got numerous wonderful books which you should go by, including public parts and
[02:11:31.580 --> 02:11:33.740]   what would Google do.
[02:11:33.740 --> 02:11:37.700]   And both of them join us almost every week for this show when they're can, when they're
[02:11:37.700 --> 02:11:38.700]   not traveling.
[02:11:38.700 --> 02:11:40.900]   Will we see you both next week?
[02:11:40.900 --> 02:11:43.580]   I will be at CES, so you will not see me.
[02:11:43.580 --> 02:11:44.580]   But I am coming.
[02:11:44.580 --> 02:11:45.580]   I know.
[02:11:45.580 --> 02:11:46.580]   Have you heard of it?
[02:11:46.580 --> 02:11:47.580]   That's brave of you.
[02:11:47.580 --> 02:11:48.580]   Wow.
[02:11:48.580 --> 02:11:51.100]   There will be a lot of IOT stuff there though.
[02:11:51.100 --> 02:11:52.100]   There will be.
[02:11:52.100 --> 02:11:55.500]   And I'm coming on this weekend Tech on Sunday after CES.
[02:11:55.500 --> 02:11:56.500]   Right on.
[02:11:56.500 --> 02:11:58.100]   Oh, that would be great.
[02:11:58.100 --> 02:11:59.260]   Thank you for doing that.
[02:11:59.260 --> 02:12:00.260]   I haven't been to CES.
[02:12:00.260 --> 02:12:01.740]   Stay in car since.
[02:12:01.740 --> 02:12:02.740]   Yeah.
[02:12:02.740 --> 02:12:03.740]   How many years?
[02:12:03.740 --> 02:12:06.420]   God, I don't know.
[02:12:06.420 --> 02:12:08.740]   I think it's been five or six, maybe seven for me.
[02:12:08.740 --> 02:12:09.740]   It's been a while.
[02:12:09.740 --> 02:12:11.460]   We used to go every year and cover it.
[02:12:11.460 --> 02:12:15.100]   You know, we used to have a big booth on the floor and everything.
[02:12:15.100 --> 02:12:16.940]   Wow, that's expensive.
[02:12:16.940 --> 02:12:17.940]   It was.
[02:12:17.940 --> 02:12:18.940]   It was.
[02:12:18.940 --> 02:12:19.940]   That's why we stopped.
[02:12:19.940 --> 02:12:23.100]   But myself by Southwest.
[02:12:23.100 --> 02:12:25.820]   Well, I think we're going to do South by this year.
[02:12:25.820 --> 02:12:26.820]   Really?
[02:12:26.820 --> 02:12:27.820]   Yeah.
[02:12:27.820 --> 02:12:28.820]   Should we all get together?
[02:12:28.820 --> 02:12:30.820]   This could be your last hurrah, Stacey.
[02:12:30.820 --> 02:12:32.820]   Oh, for South.
[02:12:32.820 --> 02:12:34.620]   I was like, dude, I'm not dying.
[02:12:34.620 --> 02:12:36.220]   No, but you're leaving Austin.
[02:12:36.220 --> 02:12:37.220]   I'm leaving.
[02:12:37.220 --> 02:12:38.580]   You're leaving Austin.
[02:12:38.580 --> 02:12:40.980]   I think I'll let you know, I'll get back to you.
[02:12:40.980 --> 02:12:44.620]   But we've had some interest in doing the Capital One House again.
[02:12:44.620 --> 02:12:48.580]   And if we do that, if we do do that, we'd love to have you both there.
[02:12:48.580 --> 02:12:49.580]   We'll figure it out.
[02:12:49.580 --> 02:12:50.580]   Okay.
[02:12:50.580 --> 02:12:51.580]   Yeah.
[02:12:51.580 --> 02:12:52.580]   I might be in.
[02:12:52.580 --> 02:12:55.700]   Sweden during South by, but I might not be.
[02:12:55.700 --> 02:12:56.620]   Because you want to escape?
[02:12:56.620 --> 02:12:58.620]   This is so typical of Austinites.
[02:12:58.620 --> 02:12:59.620]   Yeah.
[02:12:59.620 --> 02:13:01.420]   They just get out of town.
[02:13:01.420 --> 02:13:02.740]   I'm getting paid to talk.
[02:13:02.740 --> 02:13:04.740]   Sorry, we'll get West Faulkner on instead.
[02:13:04.740 --> 02:13:05.740]   There you go.
[02:13:05.740 --> 02:13:06.740]   Good.
[02:13:06.740 --> 02:13:08.980]   There's lots of great Austinites I can hook you up with.
[02:13:08.980 --> 02:13:09.980]   Okay.
[02:13:09.980 --> 02:13:12.860]   Anyway, Sweden, that'll be fun.
[02:13:12.860 --> 02:13:13.860]   I love Sweden.
[02:13:13.860 --> 02:13:15.340]   Oh, give me a Stockholm.
[02:13:15.340 --> 02:13:16.340]   Jeff has some restaurant.
[02:13:16.340 --> 02:13:17.340]   Stockholm and Helsinki.
[02:13:17.340 --> 02:13:20.100]   I love and Stockholm.
[02:13:20.100 --> 02:13:21.900]   I know Helsinki's not in Sweden.
[02:13:21.900 --> 02:13:23.820]   I'm just going to the Nordic countries.
[02:13:23.820 --> 02:13:28.300]   But yes, I am going to Stockholm as well.
[02:13:28.300 --> 02:13:34.140]   While you're in the Nordic countries, don't forget the amazing Bergen to Oslo train.
[02:13:34.140 --> 02:13:41.660]   Oh, this is something about that hat, isn't it?
[02:13:41.660 --> 02:13:42.660]   Thank you for joining us.
[02:13:42.660 --> 02:13:46.060]   We do this week in Google every Wednesday.
[02:13:46.060 --> 02:13:49.580]   130 Pacific, 430 Eastern, 2,130 UTC.
[02:13:49.580 --> 02:13:53.100]   We would love it if you'd stop by and watch live at Twit.TV/live.
[02:13:53.100 --> 02:13:54.580]   I think we kicked the live stream.
[02:13:54.580 --> 02:13:58.500]   It froze, but we got kicked and got started up again.
[02:13:58.500 --> 02:14:02.620]   If you're watching live, join the chatroom, IRC.Twit.TV.
[02:14:02.620 --> 02:14:05.420]   That's where all the other people watching live hang out.
[02:14:05.420 --> 02:14:07.500]   But you don't have to watch anything we do live.
[02:14:07.500 --> 02:14:09.500]   All of the shows we do are available on demand.
[02:14:09.500 --> 02:14:15.340]   That's the real way we expect you to consume our product at your convenience, at your leisure.
[02:14:15.340 --> 02:14:19.540]   When you want to, on your time frame, just go to twit.tv/twig.
[02:14:19.540 --> 02:14:24.620]   Or subscribe in your favorite podcast application.
[02:14:24.620 --> 02:14:27.180]   There's audio and video you pick.
[02:14:27.180 --> 02:14:28.820]   But if you subscribe, you'll get it each week.
[02:14:28.820 --> 02:14:29.820]   The minute it's available.
[02:14:29.820 --> 02:14:32.380]   Thank you so much for listening or watching.
[02:14:32.380 --> 02:14:35.220]   And we will see you next week on This Week in Google.
[02:14:35.220 --> 02:14:35.860]   Bye bye.
[02:14:35.860 --> 02:14:45.860]   [MUSIC]

