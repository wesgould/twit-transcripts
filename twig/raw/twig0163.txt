;FFMETADATA1
title=This Week In Google 163: Jeff in a Dress
artist=Leo Laporte, Jeff Jarvis and Gina Trapani
album=This Week In Google
TDES=Hosts: Leo Laporte, Jeff Jarvis, and Gina Trapani\
\
Apple's iPhone 4S keynote, what is PhoneGap, \# on Google +, Google won't screw up Android, and more cloud news.\
\
Download or subscribe to this show at twit.tv/twig.\
\
We invite you to read, add to, and amend our show notes.\
\
Friendfeed links for this episode.\
\
Thanks to Cachefly for the bandwidth for this show.\
\
Running time: 1:29:14
genre=Tech News
comment=http://twit.tv/twig163
encoded_by=iTunes v7.0
TGID=http://www.podtrac.com/pts/redirect.mp3/twit.cachefly.net/twig0115.mp3
date=2012
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.760]   It's time for Twig this week in Google. We're just coming off the big Apple
[00:00:03.760 --> 00:00:07.320]   announce. We'll talk a little bit about that. We'll also talk about privacy issues
[00:00:07.320 --> 00:00:12.560]   with the Google self-driving cars, what the former First Lady of Germany thinks
[00:00:12.560 --> 00:00:18.200]   about auto-complete, and why Google thinks I'm a douche. It's all coming up next on Twig.
[00:00:18.200 --> 00:00:25.680]   Netcast you love from people you trust.
[00:00:29.320 --> 00:00:31.320]   This is Twig.
[00:00:31.320 --> 00:00:34.680]   Bandwidth for this week in Google is provided by
[00:00:34.680 --> 00:00:39.680]   cashfly, C-A-C-H-E-F-L-Y.com.
[00:00:39.680 --> 00:00:49.960]   This is Twig this week in Google, episode 163 recorded September 12th, 2012.
[00:00:49.960 --> 00:00:56.360]   Jeff in address. This week in Google is brought to you by Ford. Ford invites tech
[00:00:56.360 --> 00:01:01.960]   keeks to join the conversation, submit ideas, and grab a tech geek badge at
[00:01:01.960 --> 00:01:08.840]   social.ford.com. It's time for Twig this week in Google. And we will Google
[00:01:08.840 --> 00:01:13.140]   eyes with our regular Googly eyes, starting with on my left Ms. Jean
[00:01:13.140 --> 00:01:19.040]   Trapani with smarterware.org, founding editor of Life Hacker, and Hacker herself.
[00:01:19.040 --> 00:01:23.160]   Good to see you Gina. Good to see you too. Sorry that we started late, but it gave me
[00:01:23.160 --> 00:01:29.020]   enough time to download the Xcode Gold Master. Oh yes, the iOS 6 Gold
[00:01:29.020 --> 00:01:36.240]   Master is out. Yep. So do you put it on something? Do I put it? I just install it
[00:01:36.240 --> 00:01:41.400]   on my Mac. Yeah, you don't you don't you don't put it on your iPhone or iPad. I
[00:01:41.400 --> 00:01:45.840]   haven't yet. I haven't yet. I didn't install the iOS 6 beta, but now I will
[00:01:45.840 --> 00:01:49.520]   because I want to test my app on iOS 6. Yeah, and you'll be ahead of all the
[00:01:49.520 --> 00:01:54.800]   other cool kids who don't get theirs till Thursday or whatever. Yeah, yeah. Also
[00:01:54.800 --> 00:01:59.480]   with us cool kid himself, Jeff Jarvis of Buzz machine.com, Professor of
[00:01:59.480 --> 00:02:05.320]   Journalism at City University of New York. Hello, Jeff Jarvis, JJ, author of
[00:02:05.320 --> 00:02:12.320]   public parts. And what would Google do? So are you completely appled out, Leo? I
[00:02:12.320 --> 00:02:18.720]   am so Apple that but I will allow you both rebuttal a minute of rebuttal. Are
[00:02:18.720 --> 00:02:24.000]   you excited? So Apple today, if you if you're just tuning in, if you've been in
[00:02:24.000 --> 00:02:30.000]   the woods, announced the new iPhone iPhone 5, you know, it's funny. I walked
[00:02:30.000 --> 00:02:33.720]   around amongst normal people before the before the announcement and I did not
[00:02:33.720 --> 00:02:39.600]   detect a heightened sense of liveliness, awareness, and excitement. I don't think
[00:02:39.600 --> 00:02:43.760]   most people know that this announcement's going on. Apple fans know that it's
[00:02:43.760 --> 00:02:50.000]   going on. It was there. It was one of their incremental updates, right? It's not
[00:02:50.000 --> 00:02:53.920]   supposed to be. No, it's not supposed to be. It's because it's TikTok, right? And so
[00:02:53.920 --> 00:03:00.120]   iPhone 4 was big going from the 3GS to the 4. Huge looks different. I have a
[00:03:00.120 --> 00:03:03.560]   4s looks exactly the same just software stuff. So that was supposedly the talk.
[00:03:03.560 --> 00:03:08.320]   Now we need a tick. Okay, yeah, you're right. So this is supposed to be a big one.
[00:03:08.320 --> 00:03:13.600]   But it's a it's big. I mean, the screen is tall is an icon is a row of icons taller.
[00:03:13.600 --> 00:03:17.660]   Nine millimeters taller. Yeah, four inches, right? Yeah, it's a four inch
[00:03:17.660 --> 00:03:26.760]   screen. So three and a half. Yeah, metal back. Bessel is drilled by diamonds. It's an
[00:03:26.760 --> 00:03:30.800]   eight megapixel camera still, but it's now got a sapphire lens movable inside.
[00:03:30.800 --> 00:03:34.720]   It's got an now exploit African miners as well. Oh, God, yeah.
[00:03:34.720 --> 00:03:42.160]   Asian factory. I didn't want to talk about this on the on the Apple event because
[00:03:42.160 --> 00:03:46.560]   that I don't want to be I hate to be the turd in the punch bowl. If for one of a
[00:03:46.560 --> 00:03:48.960]   better, you got to cold what if you say anything.
[00:03:48.960 --> 00:03:55.360]   phrase. Yeah, I don't I didn't want to be the guy who brings everybody down. But
[00:03:55.360 --> 00:04:03.480]   did you see the article? I in the Shanghai Evening Post, they got a reporter into
[00:04:03.480 --> 00:04:08.080]   the Foxconn factory. Now, remember, we talked a lot about Mike.
[00:04:08.080 --> 00:04:15.160]   Daisy. Yeah, Daisy and Daisy. Yeah. And unfortunately, he tarnished what he was
[00:04:15.160 --> 00:04:19.640]   saying by making up facts. So the but the story was
[00:04:19.640 --> 00:04:23.840]   substantively accurate, which was that the workers at Foxconn, the people who are
[00:04:23.840 --> 00:04:28.760]   making not just this gadget, but every one of these gadgets in our little life,
[00:04:28.760 --> 00:04:33.120]   not just Apple, but everybody, they're all made in China by labor that is not
[00:04:33.120 --> 00:04:37.680]   enjoying the best conditions. So this was a Chinese reporter for a Chinese news
[00:04:37.680 --> 00:04:43.680]   agency who went in undercover. This is good reporting, Jeff. I think you'll agree.
[00:04:43.680 --> 00:04:50.680]   Yeah. To the Foxconn factory pretended to be a new worker on the iPhone 5. He worked
[00:04:50.680 --> 00:04:59.520]   there for 10 days. He did a diary back in March. The factory needed 20,000 more
[00:04:59.520 --> 00:05:05.040]   workers to work on the iPhone 5. So they started this in March. They do 57 million
[00:05:05.040 --> 00:05:12.120]   phones a year for Apple. He could only take it for 10 days. He talked about the
[00:05:12.120 --> 00:05:18.640]   recruitment. Pretty straightforward. He was asked 30 questions for his mental
[00:05:18.640 --> 00:05:24.400]   health. Yes or no questions. One of them have you gotten to a state of mental
[00:05:24.400 --> 00:05:29.960]   trance recently? Remember, they had a problem with suicides there. They're there.
[00:05:29.960 --> 00:05:36.560]   This is pictures of the dormitory, not too beautiful, not too elegant. He said,
[00:05:36.560 --> 00:05:40.760]   "The whole dormitory smells like garbage when I walked in. It's a mixture of
[00:05:40.760 --> 00:05:45.000]   overnight garbage smell plus dirty sweat and foam." I don't know what that smells
[00:05:45.000 --> 00:05:49.880]   like. "Outside, every room fully piled up with unclear trash, cockroaches in the
[00:05:49.880 --> 00:05:55.800]   cupboard. The bed sheets are full of dirt and ashes." Then he's asked to sign a
[00:05:55.800 --> 00:06:04.560]   contract to say that, and he's encouraged by managers to tick no before he even
[00:06:04.560 --> 00:06:09.240]   goes in to say that he didn't hear any noise pollution or toxic pollution before
[00:06:09.240 --> 00:06:18.720]   he even goes in. His job was to put oil dots on the back. I mean, if you read
[00:06:18.720 --> 00:06:23.960]   this, you might have some second thoughts about buying not just your iPhone but
[00:06:23.960 --> 00:06:28.920]   your gadget of any kind. I'm booking that with what happens to them when you
[00:06:28.920 --> 00:06:34.320]   throw them out. Yeah. This is where we want to see Apple innovate. I mean, I
[00:06:34.320 --> 00:06:39.800]   love the Johnny I've video against the white background where he says, "Alumidium,"
[00:06:39.800 --> 00:06:44.120]   and we get to see beautiful corners. Sounds clean, doesn't it? It's amazing,
[00:06:44.120 --> 00:06:48.680]   right? But I would love to see Apple start to innovate here. How do we
[00:06:48.680 --> 00:06:54.240]   manufacture these things without recruiting students out of universities?
[00:06:54.240 --> 00:06:57.840]   That was another story I think that was also Shanghai Daily and make these
[00:06:57.840 --> 00:07:03.000]   conditions better. I would just like to see them talk about that in there.
[00:07:03.000 --> 00:07:06.640]   They've made a lot of great strides environmentally, right? I would like to
[00:07:06.640 --> 00:07:10.840]   see them sort of address this kind of thing in there. This guy had to mark
[00:07:10.840 --> 00:07:16.720]   four points with paint on an iPhone back. This is all he did. He had to do five
[00:07:16.720 --> 00:07:23.720]   a minute. He worked ten hours, three thousand iPhone five backplates. There are
[00:07:23.720 --> 00:07:27.640]   four production lines doing just that. Twelve workers in every line producing
[00:07:27.640 --> 00:07:35.520]   36,000 iPhone backplates in half a day. I mean, mind numbing work. And by the way,
[00:07:35.520 --> 00:07:43.680]   he did two extra hours of overtime for four dollars. So just, I mean, look, I'm
[00:07:43.680 --> 00:07:47.200]   probably going to continue to buy the gadget side by, but it's...
[00:07:47.200 --> 00:07:52.760]   There's responsibility. We've seen this movement with food. One of my
[00:07:52.760 --> 00:07:55.760]   students, a woman named Jenny Evans, has something called Closetor where she
[00:07:55.760 --> 00:07:59.680]   looks at the same considerations with fashion and the story behind what you
[00:07:59.680 --> 00:08:03.720]   get. And certainly, getting the story behind the technology is important as
[00:08:03.720 --> 00:08:08.600]   well. And like Daisy did poison that well, but it's a well, you know, that needs
[00:08:08.600 --> 00:08:12.320]   attention. And I think you're right, Jim. Well, I hope you're right, Jim. I hope that
[00:08:12.320 --> 00:08:15.400]   people would care enough if it becomes a differentiator, especially since you're
[00:08:15.400 --> 00:08:19.400]   paying a premium for Apple, by the way, and a rather considerable premium that
[00:08:19.400 --> 00:08:25.080]   you can get that. Because I really think my takeaway from today was just that
[00:08:25.080 --> 00:08:31.320]   that smartphones were now as boring as phones became. And so there's not a lot
[00:08:31.320 --> 00:08:34.360]   new to be done. There'll be some new features. There'll be some new stuff.
[00:08:34.360 --> 00:08:37.560]   That's fine. But it reminds me of the old days when we used to be excited when
[00:08:37.560 --> 00:08:42.480]   GM announced its new line of a car. And now who cares? Or when NBC announced its
[00:08:42.480 --> 00:08:48.920]   new shows, now who cares? Oh, so Apple announced a new phone. Who cares? It's
[00:08:48.920 --> 00:08:54.720]   become commodity. And I think, you know, that's the interesting thing to me. And
[00:08:54.720 --> 00:09:01.120]   so can you differentiate your phone in part by its virtue? We'll see. If anybody
[00:09:01.120 --> 00:09:05.960]   tries to. I'd love it if so if Apple, well, you know, who did Google did? Remember,
[00:09:05.960 --> 00:09:09.920]   they made a big deal about the queue. Yeah, the queue, which was just an
[00:09:09.920 --> 00:09:14.320]   unfortunate product to make the point with. But it was made in the USA. I mean,
[00:09:14.320 --> 00:09:17.120]   wouldn't you love to see an Apple keynote where they said and these were all made
[00:09:17.120 --> 00:09:22.200]   in the USA? I mean, I would I would intentionally I would turn I would not make
[00:09:22.200 --> 00:09:25.800]   the choice based on features or benefits. I would take the Samsung Galaxy S3, put
[00:09:25.800 --> 00:09:30.520]   it aside and buy only iPhones if they did that. You know, I'm gonna scream, I'm gonna
[00:09:30.520 --> 00:09:32.800]   scream because I think I think we go a little too far with that with, you know,
[00:09:32.800 --> 00:09:37.360]   USA, Jingoism and and you know, I don't want to un-employ a whole bunch of
[00:09:37.360 --> 00:09:41.720]   people in China. I just want them to be employed decently. Well, maybe that's
[00:09:41.720 --> 00:09:45.440]   the solution is to say, okay, we're gonna continue making China. But see, this is
[00:09:45.440 --> 00:09:48.080]   what Apple says they're doing. And that's the problem is they say, but we're
[00:09:48.080 --> 00:09:51.200]   gonna have standards. We're gonna we're gonna really insist on them. We're gonna
[00:09:51.200 --> 00:09:54.720]   go out there. We're gonna step forward. And again, I'm not picking on Apple
[00:09:54.720 --> 00:09:59.680]   because every product we buy from any company that's made in China is
[00:09:59.680 --> 00:10:01.000]   probably made in conditions.
[00:10:01.000 --> 00:10:04.720]   Exactly. And anyone can complain about their jobs exactly rightly or probably
[00:10:04.720 --> 00:10:07.760]   conditions that are worse. In fact, I would bet their conditions that are worse than
[00:10:07.760 --> 00:10:11.440]   Apple. Yeah. And so, you know, is that relativism work or not? And certainly, we
[00:10:11.440 --> 00:10:14.600]   saw stories people complaining about their jobs at Amazon warehouses, which are not
[00:10:14.600 --> 00:10:17.440]   pleasant places and I have certainly have a more pleasant job. Look at what my
[00:10:17.440 --> 00:10:22.880]   surroundings, you know, jobs can be difficult. And I think we have to realize
[00:10:22.880 --> 00:10:26.680]   that realize a little bit of the context nonetheless. I think Gina's point is to
[00:10:26.680 --> 00:10:31.080]   make better conditions a goal and a differentiator and a marketing position
[00:10:31.080 --> 00:10:35.240]   would be a wonderful thing. Yes, huge opportunity. And by the way, the studies
[00:10:35.240 --> 00:10:39.400]   have shown it doesn't add significantly to the cost of the device. I'm talking
[00:10:39.400 --> 00:10:44.440]   $40 to an iPhone. I know people are very price sensitive, but I pay $40 more to
[00:10:44.440 --> 00:10:52.000]   know that the workers were well taken care of. Moving on.
[00:10:52.000 --> 00:10:56.720]   Germany, I just I just going to just say this because it's just going to make
[00:10:56.720 --> 00:11:01.720]   Jeff so happy. What have they done now? Germany's first lady, former first lady.
[00:11:01.720 --> 00:11:08.600]   Yes. Bettina Wolf. I guess there were lots of rumors about Ms. Wolf having a
[00:11:08.600 --> 00:11:12.360]   different career before she married former German president Christian Wolf.
[00:11:12.360 --> 00:11:17.040]   A career of longstanding one might call me. Yes, one of one of the oldest
[00:11:17.040 --> 00:11:22.720]   professions. So when you auto complete, when you do a search for her and you
[00:11:22.720 --> 00:11:28.520]   type auto complete terms like a Scorton prostitute come out and she's mad at Google
[00:11:28.520 --> 00:11:35.720]   for that and she's suing them for defamation. Right. Right. Because people are
[00:11:35.720 --> 00:11:39.560]   writing about this. Yes. Exactly. And so that's the data which which I'm ready to
[00:11:39.560 --> 00:11:43.760]   go, you know, make fun of it for and defend Google and then Google turned
[00:11:43.760 --> 00:11:50.080]   around and there's another story in there. I think it's in there that they have
[00:11:50.080 --> 00:11:56.960]   not so quick to auto complete for pirated sites now. Right. Yes. So if Google does
[00:11:56.960 --> 00:12:01.000]   change for one reason and then doesn't change for the other, it violates its
[00:12:01.000 --> 00:12:07.640]   what algorithmic purity here. And they've been sued in Italy and Japan and lost in
[00:12:07.640 --> 00:12:13.720]   both cases and had to change how auto complete works. It's just a shame because
[00:12:13.720 --> 00:12:16.440]   you and I and everybody listening to this show knows that auto complete has
[00:12:16.440 --> 00:12:20.600]   nothing to do with Google. It's because that's what people are typing in.
[00:12:20.600 --> 00:12:26.480]   It's the people. Well, you know, this whole story about a number of people who
[00:12:26.480 --> 00:12:34.720]   said it suddenly I'll add a tech sock. Zainip said this that that we in America
[00:12:34.720 --> 00:12:40.400]   don't see that the absolute freedom of speech is not something shared around
[00:12:40.400 --> 00:12:43.120]   the world. And she does say that judgmentally at all. She's from Turkey.
[00:12:43.440 --> 00:12:47.680]   And what she's trying to explain to people is that in other countries, hate
[00:12:47.680 --> 00:12:52.200]   speech, negative speech is regulated, is forbidden, is punished. And so when it
[00:12:52.200 --> 00:12:56.360]   isn't here, this in the context of the of the horribly cheesy ridiculous video
[00:12:56.360 --> 00:13:01.720]   that supposedly caused what happened in Egypt, Libya. So when that's not controlled
[00:13:01.720 --> 00:13:06.880]   here, there's a presumption in other countries or the ability to paint it as
[00:13:06.880 --> 00:13:12.360]   if it's approved speech. And I think that that that's a little bit of a cultural
[00:13:12.360 --> 00:13:17.000]   shift to explain I still will defend absolute free speech and sell absolute
[00:13:17.000 --> 00:13:19.720]   free speech to anyone. And I'm not justifying anything that's come out of
[00:13:19.720 --> 00:13:23.360]   that. But I think that that comes in some of this legislation that happens as
[00:13:23.360 --> 00:13:25.840]   well, this is not good. So clearly you should stop this. It shouldn't go through
[00:13:25.840 --> 00:13:29.360]   you. Well, there's there's implications of that that we should we should know
[00:13:29.360 --> 00:13:32.400]   better. Well, yeah, and that's the, you know, I mean, of course, the first thing
[00:13:32.400 --> 00:13:35.360]   that comes to my mind is they just don't understand how it's working. They
[00:13:35.360 --> 00:13:39.120]   yeah, you know, and maybe if somebody explained, well, this is automatic, Ms.
[00:13:39.120 --> 00:13:44.800]   Wolf, this is not Google's not doing this. This is part of auto complete. But I
[00:13:44.800 --> 00:13:47.880]   don't get the sense that she would understand. Maybe she does understand that
[00:13:47.880 --> 00:13:55.240]   she still wants him to stop. Yeah. And what does Google do? You know, Google's
[00:13:55.240 --> 00:13:59.320]   in the it's I'll go back to the video again, Google blocked that infamous video
[00:13:59.320 --> 00:14:03.960]   in Egypt and Libya. It defended the video being online, said the video, which if
[00:14:03.960 --> 00:14:09.160]   anybody has watched it is the cheesiest, cheesiest, most ridiculously, obviously,
[00:14:09.160 --> 00:14:14.840]   stupid thing you could ever imagine horribly done done to piss people off
[00:14:14.840 --> 00:14:19.920]   done intentionally to provoke. Yeah, right. Well, they did. And just just just,
[00:14:19.920 --> 00:14:22.680]   you know, I would say laugh and leave ridiculous, but nothing's laughable about
[00:14:22.680 --> 00:14:27.200]   the situation now. It's tragic. But Google said this meets our standards. It's
[00:14:27.200 --> 00:14:32.280]   staying on YouTube, but they blocked it in Egypt and Libya. And meanwhile,
[00:14:32.280 --> 00:14:37.960]   Afghanistan blocked YouTube in total. And so, you know, I don't know whether I
[00:14:37.960 --> 00:14:41.160]   don't know if there's an easier I don't know what the easy answer is.
[00:14:41.160 --> 00:14:46.120]   But each will be if you're Google. I don't know. I mean, when we talked about
[00:14:46.120 --> 00:14:52.800]   China, early in the early days of this week in Google, I think we agreed that a
[00:14:52.800 --> 00:14:56.200]   company has to if you're going to do business in a country, you have to abide
[00:14:56.200 --> 00:15:01.320]   by the country's laws. And Yahoo and Google don't have Nazi memorabilia. eBay
[00:15:01.320 --> 00:15:05.120]   doesn't have Nazi memorabilia in Germany. For sale.
[00:15:05.120 --> 00:15:09.200]   As they follow the whole law, they follow the law of the land that they are in.
[00:15:09.200 --> 00:15:13.120]   And so either and Google made the right choice, which is say, well, then
[00:15:13.120 --> 00:15:17.680]   that would days we're not going to do business in China. But you can censor
[00:15:17.680 --> 00:15:21.480]   us, but we will not censor ourselves. Right. They said in China, however, it
[00:15:21.480 --> 00:15:26.240]   is a bit hypocritical given what they they do. They do censor in Germany.
[00:15:27.200 --> 00:15:33.280]   But then they open up all of these problems by when I type the pirate bay
[00:15:33.280 --> 00:15:37.440]   into auto complete, I get the pirate movie, the pirates of pen sense.
[00:15:37.440 --> 00:15:40.720]   I don't get pirate Bay.
[00:15:40.720 --> 00:15:43.280]   He bag. Yeah.
[00:15:43.280 --> 00:15:50.360]   Because they demote sites that they've deemed the primary purposes to modify
[00:15:50.360 --> 00:15:54.760]   auto complete. Right. They modify auto complete. So they're saying we can do
[00:15:54.760 --> 00:15:56.320]   it. What do you think, Gina?
[00:15:57.200 --> 00:16:02.560]   Well, I mean, if in Germany, it's true that, you know, saying that this woman
[00:16:02.560 --> 00:16:06.080]   was a prostitute at some point is illegal. That's not protected speech.
[00:16:06.080 --> 00:16:07.840]   Is that what you're saying, Jeff?
[00:16:07.840 --> 00:16:14.800]   No, what I'm saying is culturally what what Zadip says is that just just culturally
[00:16:14.800 --> 00:16:19.240]   there that the presumption is that because hate speech is regulated, if you
[00:16:19.240 --> 00:16:21.120]   haven't regulated it, you're approving it.
[00:16:21.120 --> 00:16:25.760]   Where you say in America, or it says, no, no, no, all speech, including
[00:16:25.760 --> 00:16:27.520]   noxious speech is protected.
[00:16:27.520 --> 00:16:33.280]   Right. Right. So the so because this auto complete suggestion comes up in Germany,
[00:16:33.280 --> 00:16:37.760]   it's it seems as if Google is approving or publishing this information, which is
[00:16:37.760 --> 00:16:41.920]   inaccurate. So she feels so she's saying, this is inaccurate. You shouldn't show it.
[00:16:41.920 --> 00:16:43.200]   It's on you to take it down.
[00:16:43.200 --> 00:16:48.720]   Yeah. I mean, it's tough. It's tough. I do agree that if you're going to do business
[00:16:48.720 --> 00:16:52.280]   in a country, you should sort of abide by their laws. And if this is defamation,
[00:16:52.280 --> 00:16:55.200]   I mean, I don't know if it German law considers this defamation.
[00:16:56.000 --> 00:16:57.400]   Then then Google probably has it.
[00:16:57.400 --> 00:16:59.440]   Who's doing the defaming? It's auto complete.
[00:16:59.440 --> 00:17:02.960]   That's the problem. It's it's yeah, it's the crowd. And if something goes through you,
[00:17:02.960 --> 00:17:07.760]   so should every news stand be arrested for something that's carried or a book store
[00:17:07.760 --> 00:17:10.160]   that's being arrested for securing something? And how haven't we been there before?
[00:17:10.160 --> 00:17:14.160]   Well, that's a little different. What if hate speech goes on on a telephone?
[00:17:14.160 --> 00:17:18.560]   Do you blame the phone company? Exactly. No, that's what I'll point. Yeah. Yeah.
[00:17:18.560 --> 00:17:23.600]   Now, there's stories a little more complicated because this ex president got into a whole
[00:17:23.600 --> 00:17:30.560]   kerfuffle and tried to call and did call left a message for the guy. I know Kai Deakman,
[00:17:30.560 --> 00:17:36.240]   who's the editor of built the biggest paper in Europe. And it became a whole big huha basically
[00:17:36.240 --> 00:17:41.680]   ruined his career. So that's been going on. And he's kind of kind of tender. And then meanwhile,
[00:17:41.680 --> 00:17:47.200]   this is happening too. I see there's a little cultural background there. We miss.
[00:17:47.200 --> 00:17:51.600]   Yeah. I think, by the way, just as an aside, Kai is the editor of this largest paper. It's very,
[00:17:51.600 --> 00:17:57.440]   you know, the tabloidy. And he and the top two top two executives, two of the top executives from
[00:17:57.440 --> 00:18:03.200]   actual Springer, huge publisher in Germany, just moved to Palo Alto for six months to a year.
[00:18:03.200 --> 00:18:07.520]   To get out of Germany? To get into the land of innovation.
[00:18:07.520 --> 00:18:08.000]   Oh, okay.
[00:18:08.000 --> 00:18:14.720]   They're gonna put their jobs and just soak in innovation.
[00:18:16.320 --> 00:18:22.320]   So Google is not admitting to changing its auto complete. But
[00:18:22.320 --> 00:18:30.560]   that just be because if they downgraded the sites in search, would that also potentially
[00:18:30.560 --> 00:18:34.480]   automatically affect auto complete, you think? I mean, it should, right? I mean, that's what's
[00:18:34.480 --> 00:18:39.360]   presumably going on with the Pirate Bay, right? The Pirate Bay is not showing up high in search
[00:18:39.360 --> 00:18:45.440]   results because they're there, they're waiting it because it's they've deemed it a site that's,
[00:18:45.440 --> 00:18:48.240]   you know, there to provide. According to torrent,
[00:18:48.240 --> 00:18:54.800]   according to torrent freak is recently is January 2011. If you typed the THE space
[00:18:54.800 --> 00:18:58.880]   P into Google, you'd immediately get the Pirate Bay. Not anymore.
[00:18:58.880 --> 00:19:04.640]   And so if you complete the search, it does the Pirate Bay come up in? No.
[00:19:04.640 --> 00:19:08.240]   In the results. Oh, that's a good question. Let's see. Let's go to Google.
[00:19:08.240 --> 00:19:12.400]   I think that auto complete suggests, you know, what you're probably going to get.
[00:19:12.400 --> 00:19:15.440]   Oh, I'm wrong. The Pirate Bay, if I search for the Pirate Bay, that is the first,
[00:19:15.440 --> 00:19:22.960]   the first site that I get back. Well, I'm getting stories about the Pirate Bay.
[00:19:22.960 --> 00:19:28.400]   Yeah, no, you're right. There it is. So it's still number one result for the Pirate Bay.
[00:19:28.400 --> 00:19:33.920]   Right. So they're not, but they're not suggesting that you go there.
[00:19:33.920 --> 00:19:38.080]   It's interesting. It's not an easy answer. It really isn't an easy answer.
[00:19:38.080 --> 00:19:41.520]   Well, there's a risk though. As soon as you do start censoring, then you're saying,
[00:19:41.520 --> 00:19:46.880]   not only can we, but we are no longer a carrier. We don't have that safe armor.
[00:19:46.880 --> 00:19:52.240]   Right. Well, it's expected to be. I mean, you got to have a rule set that you live by.
[00:19:52.240 --> 00:19:55.680]   It's very difficult. Yeah, it is. I don't know what Google does. I really don't.
[00:19:55.680 --> 00:20:00.080]   They do censor some words. I'm trying to remember when they first launched this quick suggest,
[00:20:00.080 --> 00:20:04.560]   wasn't it was a arena, a slutsky slutsky? Yeah. Yeah. Yeah.
[00:20:04.560 --> 00:20:08.800]   Arena slutsky. Yeah. Yeah. She said, Hey, I just tried it. And it's not suggesting anything
[00:20:08.800 --> 00:20:13.920]   for my name because that was one of the words. Yeah. Yeah. Yeah. So I mean, there are, there's
[00:20:13.920 --> 00:20:20.240]   definitely some editorial going on here. There's some filtering going on. But how far do you take
[00:20:20.240 --> 00:20:26.400]   it? And for what reasons? Oh, so difficult. I mean, you started on a little late German
[00:20:26.400 --> 00:20:32.320]   story and we went all off. No, I knew we would. That's why I baited you. No, I completely knew
[00:20:32.320 --> 00:20:37.280]   we would. That's why I brought it up. Well, we got crazy things in Europe. What was the other
[00:20:37.280 --> 00:20:47.040]   story from the Netherlands where a court said that if you, a site that had linked to a Playboy
[00:20:47.040 --> 00:20:51.920]   photo of a naked celebrity just by linking to that, this big site kind of, I think,
[00:20:51.920 --> 00:20:58.880]   grudge like violated both copyright and privacy. Pick one. Right.
[00:20:58.880 --> 00:21:03.520]   Look at that point where you cannot even link to something where everyone becomes responsible
[00:21:03.520 --> 00:21:07.360]   for the legality of what they link to. Just think about it for two seconds.
[00:21:07.360 --> 00:21:11.360]   But this is, this is an architectural issue. People don't know what to do with.
[00:21:11.360 --> 00:21:15.680]   And I keep thinking it's just that they don't understand the technology, but maybe it is.
[00:21:15.680 --> 00:21:20.000]   I think you're right. Well, maybe, maybe not. It's just kind of, this is the way the world
[00:21:20.000 --> 00:21:24.080]   should work. We want to work this way. And I keep on coming back to this idea of standing on
[00:21:24.080 --> 00:21:30.240]   principles. You've got to have principles. And this is going to sound terribly American
[00:21:30.240 --> 00:21:34.000]   exceptionalism. And I don't mean it this way. But I think part of it is not just that we have this
[00:21:34.000 --> 00:21:40.480]   view here of free speeches and absolute. I think that the Bill of Rights as an architecture,
[00:21:40.480 --> 00:21:45.680]   the idea of having principle in the Constitution, obviously, having principles and that you
[00:21:45.680 --> 00:21:49.520]   constantly check back to those principles rather than having what many good nations do have,
[00:21:49.520 --> 00:21:58.960]   which is a constantly evolving life, you know, a breathing set of laws that become the nation's
[00:21:58.960 --> 00:22:01.360]   legal view. Maybe that's part of the difference too. I don't know.
[00:22:01.360 --> 00:22:09.440]   Should Google tell this former First Lady what articles are so high ranking that mention
[00:22:09.440 --> 00:22:13.760]   prostitute and her name in them? Should they reveal that to her so that she can go after
[00:22:13.760 --> 00:22:19.040]   those individual websites? Well, there you are in a Twitter land, aren't you? Where Twitter
[00:22:19.040 --> 00:22:26.560]   gets is being told by the judge that it has to hand over the tweets of users. So Twitter,
[00:22:26.560 --> 00:22:30.800]   Google in that case would be handing over data that isn't necessarily public.
[00:22:30.800 --> 00:22:37.840]   Dining those sites. Well, I mean, she could probably just do the search and see the articles.
[00:22:37.840 --> 00:22:42.080]   Yeah, she could see the sites that are coming out. So complete is not
[00:22:42.080 --> 00:22:46.880]   reflecting what people are searching for. It's reflecting what your results will be when you do
[00:22:46.880 --> 00:22:52.400]   that search. I think it's bold. I think it's probably both. Yeah, Gina, the other problem with this
[00:22:52.400 --> 00:22:57.600]   is what if what if it's not her? What if if someone else of a politician, you come up with, you know,
[00:22:57.600 --> 00:23:03.040]   Joe Schmoe thief. And in fact, the guy is a thief. It's not up to Google to adjudicate
[00:23:03.040 --> 00:23:07.920]   the truth of false of accusations. Right, right. So,
[00:23:07.920 --> 00:23:13.040]   accused, there'd be a defensor, but then do have to adjudicate on top of that so they can judge
[00:23:13.040 --> 00:23:18.320]   whether to censor impossibility. Right. And Leo's point was it was a good one. People are
[00:23:18.320 --> 00:23:24.160]   issuing this query, even if there are no articles, it still could show up in the autocomplete as well.
[00:23:24.160 --> 00:23:28.320]   You know, one of the first autocomplete suggestion when you search for my name is divorce. And I'm
[00:23:28.320 --> 00:23:33.760]   not divorced. And I think that's because I public I think I posted a Flickr photo and said something
[00:23:33.760 --> 00:23:37.440]   about divorce and the title. So it's just it's funny. It doesn't bother me because, you know,
[00:23:37.440 --> 00:23:43.840]   whatever who cares. But it, you know, see, I always thought it was more what people
[00:23:43.840 --> 00:23:47.440]   are searching for than results. But I don't know because when I type Leo, report is a,
[00:23:47.440 --> 00:23:55.520]   I get jerk douche. Right. It is probably things said. Yeah. I don't think there's a lot of sites
[00:23:55.520 --> 00:24:02.080]   with jerk douche, shity at gay. Maybe there are. I get.
[00:24:02.080 --> 00:24:06.880]   Okay. So you think I don't know where that comes from. Actually, I want to sue Google gosh,
[00:24:06.880 --> 00:24:12.000]   darn it. You won't think that if you say is right. If I say Jeff Jarvis, I get Twitter, CUNY
[00:24:12.480 --> 00:24:19.040]   bio. No, no, no, no, no, no. It's only if you say is. Right. So that sentences that have is in them.
[00:24:19.040 --> 00:24:25.920]   Is it idiot? Is it twit? Is wrong? Is amoron? I think douche wins.
[00:24:25.920 --> 00:24:33.360]   You're waiting it though. Let me let me let me just see. I'll click that link because that's
[00:24:33.360 --> 00:24:34.640]   going to show me all the results.
[00:24:40.480 --> 00:24:43.040]   What was up with Michael Laryton? That's goddamn. Yeah. I
[00:24:43.040 --> 00:24:53.360]   I don't know. It's I don't know. I'm not sure really. If you do it in quotes, you'll see the actual
[00:24:53.360 --> 00:24:57.680]   results. That's well, you see the pages that have exactly that sentence. See, that probably
[00:24:57.680 --> 00:25:03.040]   narrows it down because it could it could hold right Michael or into the douche, but you got
[00:25:03.040 --> 00:25:06.720]   a I don't think I use that. I don't like that word. I don't think I use that. No, I don't
[00:25:06.720 --> 00:25:10.960]   think I use that. No, it's mostly people calling me a douche actually.
[00:25:10.960 --> 00:25:20.720]   Okay, moving on. We haven't googled ourselves in a while on the show. It's a tradition. It's a
[00:25:20.720 --> 00:25:26.480]   tradition on the show. It's a tradition. Consumer watchdog urges Governor Brown,
[00:25:26.480 --> 00:25:33.520]   California's Governor Brown, a veto Googlers, Google's driverless car bill because of privacy
[00:25:33.520 --> 00:25:40.080]   protection. We got our dumb legislators too, but they're all in California. No, they're not.
[00:25:40.080 --> 00:25:44.960]   You know, consumer watchdog privacy project, Director John Simpson,
[00:25:44.960 --> 00:25:52.320]   said the Senate bill 1298 is completely insufficient. It gives the user no control over what data
[00:25:52.320 --> 00:25:57.360]   will be gathered and how much information will be used. This is the law regulating autonomous
[00:25:57.360 --> 00:26:03.440]   vehicles. And he says this law must provide that driverless cars
[00:26:03.440 --> 00:26:09.200]   gather only the data necessary to operate the vehicle and retain that data only as long as
[00:26:09.200 --> 00:26:15.600]   necessary for the vehicle's operation. What? It's everything that has technology associated
[00:26:15.600 --> 00:26:17.920]   with becomes kudos. There was a story in the story. Oh, that's what it is. It's because
[00:26:17.920 --> 00:26:23.760]   it's technology. So it must be stealing our information. Right. Whereas there was a story in
[00:26:23.760 --> 00:26:27.600]   the Times last week in New York about restaurants and all the things they know about you don't
[00:26:27.600 --> 00:26:31.600]   want to know about you. So the way Mr. LePorte comes in, we want to see him at this kind of table.
[00:26:31.600 --> 00:26:37.280]   Fine. He likes his gin with the white cubes, right? And if you get that, that's cool.
[00:26:37.280 --> 00:26:44.160]   Walk on the stove right away. If you replace the word Google for restaurant in it, oh my god,
[00:26:44.160 --> 00:26:51.840]   there's laws. So the consumer watchdog says there's two reasons why Google won't endorse
[00:26:51.840 --> 00:26:59.120]   privacy protections for itself driving cars. First, Google's entire business model is based on
[00:26:59.120 --> 00:27:04.880]   building digital dossiers about our behavior and using them to sell the most advertising.
[00:27:04.880 --> 00:27:09.760]   Yeah. Well, that's OK. That's a good business. You are not Google's customer. You are its product.
[00:27:09.760 --> 00:27:15.040]   The one that sells to corporations willing to pay any price to reach you. Will the driverless
[00:27:15.040 --> 00:27:20.640]   technology just be about getting us from point to point or about tracking how we got there and
[00:27:20.640 --> 00:27:26.640]   what we did along the way? No, it's going to imprison you in your car and it's going to drive you to
[00:27:26.640 --> 00:27:31.680]   Walmart. Yes. Leave it until it buys what Google wants you to buy or maybe you say,
[00:27:31.680 --> 00:27:34.640]   drive me to Target and it drives you to Walmart instead because they paid more.
[00:27:34.640 --> 00:27:43.280]   Right. Mm. Second. Second computer engineers who believe that more data is always better are
[00:27:43.280 --> 00:27:49.280]   in charge at Google. They may not know what they use the data for today, but they think they may
[00:27:49.280 --> 00:27:53.760]   someday find a use for it and they don't want any restrictions on them. Now, actually neither of
[00:27:53.760 --> 00:27:59.360]   those are strictly speaking false. They are. I'm going to play devil's advocate. OK.
[00:27:59.360 --> 00:28:04.800]   Because somebody has to say it. If there's the point is the point that they're making,
[00:28:04.800 --> 00:28:10.080]   there's no way to opt out out of marketing advertisements or sharing data as you drive
[00:28:10.080 --> 00:28:14.960]   and that the consumer should have the opportunity to opt out if they wanted to.
[00:28:14.960 --> 00:28:19.920]   Now, I understand that the deal is that you use this technology like in return for using this
[00:28:19.920 --> 00:28:22.640]   technology, but getting these great convenience, you're giving your data, right? We've had this
[00:28:22.640 --> 00:28:28.080]   conversation. It's just like club cards and all the other things, but it would be nice.
[00:28:28.080 --> 00:28:34.320]   Some people would say, if you could opt out and say, I'll pay for not sharing my data. I'll pay
[00:28:34.320 --> 00:28:39.200]   with money instead of paying with my data. Does that seem unreasonable? I mean, well, you can mean,
[00:28:39.200 --> 00:28:42.560]   well, hey, I don't think I've seen any business plans that actually have Google using
[00:28:42.560 --> 00:28:45.440]   marketing in this, but you're right. I can well see them doing that.
[00:28:45.440 --> 00:28:49.680]   That's kind of what they're saying is they may not know now, but they'll just save it just in case
[00:28:49.680 --> 00:28:53.920]   because they'll find it. They may. But B, you're going to know what the deal is and you can
[00:28:53.920 --> 00:29:00.800]   opt out of it's all thriving car. So this is the amendment that the author of the legislation
[00:29:00.800 --> 00:29:05.840]   proposed, the manufacturer of the autonomous technology installed in a vehicle should provide
[00:29:05.840 --> 00:29:11.200]   a written disclosure to the purchaser of an autonomous vehicle that describes what information
[00:29:11.200 --> 00:29:16.240]   is collected by the autonomous technology equipped on the vehicle. The consumer watchdog said that
[00:29:16.240 --> 00:29:22.560]   is completely inadequate. I think that's a perfectly good rule all around.
[00:29:22.560 --> 00:29:28.480]   Now, what they're saying though, and I do support this, is they have a concept privacy by design.
[00:29:28.480 --> 00:29:33.280]   It means privacy issues are considered from the very beginning and the solutions are baked in.
[00:29:33.280 --> 00:29:38.560]   The point being that if you try to catch up after a new technology is widely adopted,
[00:29:39.840 --> 00:29:43.040]   it's kind of a done deal. And maybe that's what we've seen on the internet.
[00:29:43.040 --> 00:29:48.160]   I don't know. Yeah, to an extent, but I sat in a meeting at the, whatever the name of the
[00:29:48.160 --> 00:29:53.200]   group I'm supposed to remember of about privacy. And they talked about this. And the problem is,
[00:29:53.200 --> 00:29:56.400]   they just think about privacy. They don't think about the actual use of it first.
[00:29:56.400 --> 00:30:02.560]   So it's not designed by design. Let's make sure that every possible stop,
[00:30:02.560 --> 00:30:05.280]   we're going to put in a stop here. We're going to put a box you tick off. We're going to put all
[00:30:05.280 --> 00:30:08.400]   this stuff and we're not going to sit down and say, well, why does somebody want this? And it's
[00:30:08.400 --> 00:30:15.520]   my favorite example of this is that a Google priority inbox. Google is reading your mail and
[00:30:15.520 --> 00:30:21.360]   it's okay with you and you know what the trade off is. And it's fine. If this same attitude came
[00:30:21.360 --> 00:30:24.640]   to that every time you open an email, it would say, is it okay if I read this, is it okay if I read
[00:30:24.640 --> 00:30:32.400]   that? No, it's just okay. I guess if your job title is privacy watchdog, this is what you do.
[00:30:32.400 --> 00:30:36.560]   This is, and this is exactly, this was Howard Stern's point always about the people who went
[00:30:36.560 --> 00:30:40.640]   after him. There are these organizations that anoint themselves, our protectors with probably
[00:30:40.640 --> 00:30:43.920]   two members and a big mailing list and a nice bank account. Yeah.
[00:30:43.920 --> 00:30:51.200]   Do you think the FFF would say something similar though? I don't think they'd be this stupid.
[00:30:51.200 --> 00:30:56.720]   Well, I don't think they'd be this, no, let me put this way. I think they would be more nuanced
[00:30:56.720 --> 00:31:01.280]   in trying to investigate and try to do what you just did, Gina, which is to say,
[00:31:03.360 --> 00:31:08.240]   well, you know, how could this be used? Let's think that through and if there's any dangers,
[00:31:08.240 --> 00:31:13.040]   let's warn of them. Instead, this is a blanket. If you have any data on me, you're clearly going
[00:31:13.040 --> 00:31:18.800]   to use it for bad and horrible purposes. There's been not a word of Google using self-driving cars
[00:31:18.800 --> 00:31:23.840]   in a marketing sense or what this data might go to. And so they're allowing them to be on the
[00:31:23.840 --> 00:31:29.840]   road so that Google can test them and regulate their safety. But they're not even for sale.
[00:31:29.840 --> 00:31:36.080]   So there's no reasonable though to think that Google's business model that they might
[00:31:36.080 --> 00:31:37.760]   have. They're an advertising company.
[00:31:37.760 --> 00:31:46.400]   If we regulate every potential behavior, every potential behavior will tie ourselves in not.
[00:31:46.400 --> 00:31:48.800]   Right. It kills innovation if you're constantly.
[00:31:48.800 --> 00:31:54.880]   Google could also use it as a taxi service and could rip off people.
[00:31:54.880 --> 00:31:58.880]   And there are laws about how taxis operate. Hey, they could be using it for escorts and
[00:31:58.880 --> 00:32:01.920]   prostitution. Let's put that in there. Germany.
[00:32:01.920 --> 00:32:04.960]   And pick up. Sorry.
[00:32:04.960 --> 00:32:10.080]   I got to try to stop. I stopped. I'm going to admit it.
[00:32:10.080 --> 00:32:15.120]   I'll be my book's on sale right now. So I want to say that.
[00:32:15.120 --> 00:32:22.640]   Yeah. So trying to regulate to potential uses is the problem we are. And we always get to this
[00:32:22.640 --> 00:32:27.040]   problem. What they're doing is trying to regulate the technology versus the behavior.
[00:32:27.840 --> 00:32:33.520]   Right. The privacy laws are already there that if you use data in bad ways, no matter how you
[00:32:33.520 --> 00:32:39.520]   got it, whether it was from Facebook or from a car, the laws are there to establish it.
[00:32:39.520 --> 00:32:45.360]   Let's do that. Let's have good privacy laws that don't talk about any specific application.
[00:32:45.360 --> 00:32:50.000]   But just let's do that. That's not I could go for that.
[00:32:50.000 --> 00:32:54.480]   That's what it should be. That's what it is. We already have privacy laws. We already have
[00:32:54.480 --> 00:32:59.040]   copyright laws. We already have these laws. We don't need a whole mess of new laws for technology,
[00:32:59.040 --> 00:33:03.760]   except from show boating associations that want to raise money and legislators that also
[00:33:03.760 --> 00:33:09.600]   want to raise money. It does feel like a lot of it's scare tactics like, oh, here they go again.
[00:33:09.600 --> 00:33:14.640]   They're going to steal our information. In fact, they do in this press release. I was reading
[00:33:14.640 --> 00:33:21.760]   their press release. They do bring up. They call it the why spy incident. The Google cars,
[00:33:21.760 --> 00:33:26.080]   the Google Street View cars that were accidentally gathering Wi-Fi. The why spy.
[00:33:26.080 --> 00:33:35.760]   As if Google had some nefarious plan. Actually, they do have an nefarious plan. Sergey Brin and
[00:33:35.760 --> 00:33:43.200]   Diane von Furstenberg. Fashion Week is there nefarious plan. This is how you get people to wear Google
[00:33:43.200 --> 00:33:48.160]   glasses. This is the various plan is trying to convince you that those glasses do not look weird.
[00:33:48.160 --> 00:33:52.880]   They look weird. I don't care if Diane von Furstenberg is wearing them or her models
[00:33:52.880 --> 00:34:01.600]   or where they look weird. Here she is with Sergey Brin at Fashion Week. They both look like complete
[00:34:01.600 --> 00:34:07.360]   dorks. They're going to make a video out of the product of the glasses next week.
[00:34:07.360 --> 00:34:16.800]   Oh, really? Which also occurred to be Leo. Come the next Apple event. People happen to be just
[00:34:16.800 --> 00:34:22.000]   wearing their Google glasses. Yeah. Apple's going to throw them out. They're going to say,
[00:34:22.000 --> 00:34:26.400]   "Turn off your glasses." That's going to be interesting. And you won't be allowed to wear them
[00:34:26.400 --> 00:34:31.760]   into a dressing room or a bathroom or a gym. I don't think this makes these look good. I don't
[00:34:31.760 --> 00:34:37.120]   understand even what Diane von Furstenberg is up to here. The glasses, by the way, match the outfit.
[00:34:37.120 --> 00:34:43.440]   Notice? It does look pretty silly. It looks so dorky. It looks weird. I wonder if Apple's
[00:34:43.440 --> 00:34:50.560]   going to make a glass jammers. One of them did a dialogue with to the model. Walk 25 steps,
[00:34:50.560 --> 00:34:57.600]   stop, turn right, turn around. Walk 25 steps, reach your destination. Turn left, turn right.
[00:34:57.600 --> 00:35:03.280]   It said that it looked like Sergey had a cocktail straw sticking out of his head.
[00:35:03.280 --> 00:35:09.120]   I'm sorry. You can't make that look not dorky, but it's fine. Oh, come on.
[00:35:10.720 --> 00:35:14.560]   Oh, my God. Sarah Jessica Parker. Oh, that's perfect. Oh, come on.
[00:35:14.560 --> 00:35:20.240]   Okay. Either way, I did not know that Diane von Furstenberg was married to Barry Diller.
[00:35:20.240 --> 00:35:26.800]   Yeah. Now it's all coming together. He's the guy who's putting the little tiny antennas,
[00:35:26.800 --> 00:35:34.640]   the area antennas out. Google is going to... Yeah, that's good. It's good. No, let's see.
[00:35:34.640 --> 00:35:41.520]   Let's see some high fashion. Better now. Attractive women wearing. No, no, that's the picture right
[00:35:41.520 --> 00:35:46.480]   there. That is the picture right there. A beautiful woman in an interesting coach here,
[00:35:46.480 --> 00:35:55.680]   gown. What does that pair of glasses say to you? Google Schmutz's beauty. I am a borg. I'm a
[00:35:55.680 --> 00:36:01.840]   cyber. I'm ready for my close up and battle star Galactica. It makes one high look like weird.
[00:36:01.840 --> 00:36:06.400]   It's blowing broken. Yes, because the light is shining there. See, that's the...
[00:36:06.400 --> 00:36:09.840]   This so has to be in Big Bang Theory this season. Please, please.
[00:36:09.840 --> 00:36:15.520]   Yes. Oh, they should do a whole episode. Just on the ground. Sheldon making his own Google glasses.
[00:36:15.520 --> 00:36:23.760]   He can't get them, so he makes his own and they're a little bit larger, a little bit less stylish.
[00:36:23.760 --> 00:36:30.480]   Google is making this... Look for Boingo. I almost said Oingo, Boingo.
[00:36:30.480 --> 00:36:34.080]   Look for Boingo Wi-Fi hotspots free across the US this month.
[00:36:34.080 --> 00:36:39.600]   Unless you're using iOS or Windows Phone. What?
[00:36:39.600 --> 00:36:45.200]   So I could have sworn that I was in a New York City subway stop that has this free Wi-Fi
[00:36:45.200 --> 00:36:48.640]   hotspot with my iPhone and it worked fine. Did I imagine that? Maybe I had my...
[00:36:48.640 --> 00:36:51.760]   This is a new deal. This is a new announcement. Yes, that was opened all.
[00:36:51.760 --> 00:36:56.400]   Okay. At that time. This is a new thing. This is for Android only.
[00:36:56.960 --> 00:37:00.480]   I'm dying to know what the experience is. If you have your iPhone, you connect to the Wi-Fi
[00:37:00.480 --> 00:37:04.560]   network, like what does it say? Like, sorry, you get an Android? Like, what does it say? Anything?
[00:37:04.560 --> 00:37:10.000]   Does it just say like not available? I haven't counted it yet. Somebody tweet me with an image.
[00:37:10.000 --> 00:37:14.560]   If you have one, give me a screenshot and we'll show. We'll show and tell.
[00:37:14.560 --> 00:37:22.640]   So it's sponsored by Google Play. So the idea is get you to buy stuff on Google Play.
[00:37:22.640 --> 00:37:27.360]   Right. So you're not going to use it to surf or anything. You should use it to buy stuff from us.
[00:37:27.360 --> 00:37:35.760]   Are there Google Play specials or discounts or? I don't know. That would be a good idea, wouldn't it?
[00:37:35.760 --> 00:37:40.640]   Yeah. Yeah. Stop by this area to get a free app or whatever. Right. Right. Right.
[00:37:40.640 --> 00:37:47.840]   Well, it was the whole idea that the Times had a big story about Google could be with Amazon.
[00:37:49.280 --> 00:37:54.560]   And you said it earlier, Leo, I think in the coverage that how many credit card numbers you
[00:37:54.560 --> 00:37:58.800]   have and thus how many transactions can they buy? Right. That's the next game.
[00:37:58.800 --> 00:38:01.920]   What was the number? I think Apple had something like 153 million.
[00:38:01.920 --> 00:38:09.920]   Yeah. It's amazing. Itun accounts with one click purchases. That's the power right there.
[00:38:09.920 --> 00:38:15.680]   This is, Buingo says, "Anyone accessing the Wi-Fi hotspot will also have access to some free
[00:38:15.680 --> 00:38:24.160]   content, although it doesn't specify what that is." So. YouTube ads. Yeah. YouTube ads. Free.
[00:38:24.160 --> 00:38:27.600]   Well, you know, it could be like watch an episode of your favorite show,
[00:38:27.600 --> 00:38:30.960]   why you wait for the train. That'd be great. Yeah. Yeah.
[00:38:30.960 --> 00:38:39.920]   See here. I thought it was interesting that Google gave an exclusive to Alexis Madrigal in the
[00:38:39.920 --> 00:38:45.680]   Atlantic about how it builds its maps because of course, one of the announcements Apple made
[00:38:45.680 --> 00:38:52.880]   today is the emergence of iOS six soon. I think the 14th, right? And that will no longer have Google
[00:38:52.880 --> 00:39:00.480]   maps built in. Apple's doing its own map things. So, Alexis Madrigal of the Atlantic gets to see
[00:39:00.480 --> 00:39:07.440]   the insides of ground truth, the secret program to build the world's most accurate maps by Google.
[00:39:08.560 --> 00:39:13.600]   And I think what they're showing is, is there, this was absolutely aimed at Apple.
[00:39:13.600 --> 00:39:19.600]   How it's not just the data which everybody could buy. It's how we massage the data. It's how we
[00:39:19.600 --> 00:39:26.880]   make it work. They started with data from the US Census Bureau. On first inspection,
[00:39:26.880 --> 00:39:30.800]   this data looks great. The roads look like they're all there. And you've got the free
[00:39:30.800 --> 00:39:37.600]   ways differentiated. This is a good map to the untrained eye or Apple computer. But let's look
[00:39:37.600 --> 00:39:42.640]   closer. No, I added that. There are issues about where the digital data does not match the physical
[00:39:42.640 --> 00:39:47.680]   world, etc, etc. And so they're talking about how they have this whole building where Google
[00:39:47.680 --> 00:39:52.400]   engineers are massaging it. The Google is driving around with custom cameras.
[00:39:52.400 --> 00:40:01.840]   Literally, Google has five million miles driven for mapping, not just street view, but the cars are
[00:40:01.840 --> 00:40:09.280]   actually driving around to see if there's a road there. Amazing the investment, isn't it?
[00:40:09.280 --> 00:40:14.960]   Yeah. Yeah. I was struck by a couple of these. One is that this goes back to the Apple
[00:40:14.960 --> 00:40:20.240]   announcement is that what's happened with me is that I'm so addicted to Google services,
[00:40:20.240 --> 00:40:24.800]   that led me to the hardware. Whereas before, I liked the hardware and it led me to Apple services.
[00:40:24.800 --> 00:40:31.440]   And I've switched that. That's a very good point. Apple's not really that good at services.
[00:40:31.920 --> 00:40:34.960]   Let's be honest, we haven't been down for a day before the announcement, by the way, doesn't help.
[00:40:34.960 --> 00:40:40.080]   No, I think you're right. I think that it's no longer about hardware.
[00:40:40.080 --> 00:40:45.280]   Right. And so that's the funny thing about Amazon as well. Amazon is going to such lengths to
[00:40:45.280 --> 00:40:51.200]   de-Googleify the Kindle Fire and Android on it. And that's why I'm not going to buy it.
[00:40:51.200 --> 00:40:55.200]   Because a lot of the reason that I like Android is because of the services that I get. So the
[00:40:55.200 --> 00:41:03.440]   services lead me to the hardware. That leads me to other things. And I keep on wondering about
[00:41:03.440 --> 00:41:07.200]   Marissa. And Marissa understands that too. And I keep on thinking that there's probably going to
[00:41:07.200 --> 00:41:11.920]   be something around services and better services with Yahoo too. Because that creates a relationship,
[00:41:11.920 --> 00:41:14.000]   that creates data about you, that creates value.
[00:41:14.000 --> 00:41:22.000]   It's interesting because Bezos said we don't, we want to make money when you use our products,
[00:41:22.000 --> 00:41:26.480]   not only you buy them. And Zuckerberg said to Arianton,
[00:41:26.480 --> 00:41:33.200]   I am not making a phone, Michael, which was hilarious stuff.
[00:41:33.200 --> 00:41:35.760]   Absolutely hilarious. That's the rumor that just will not die.
[00:41:35.760 --> 00:41:42.640]   Well, Michael keeps on, I actually miss Arianton. I have to say that if watching him do his interviews
[00:41:42.640 --> 00:41:46.640]   and watching him with Zuckerberg, I thought it was good.
[00:41:46.640 --> 00:41:48.640]   What is it that he does that you like?
[00:41:49.360 --> 00:41:55.360]   He's just blunt. He doesn't really care. He's not kissing up to Mark Zuckerberg or anybody else.
[00:41:55.360 --> 00:42:00.880]   No, and Zuckerberg still in his natural state is also direct.
[00:42:00.880 --> 00:42:05.360]   And put the two together. And I enjoyed the conversation. What did you think of it?
[00:42:05.360 --> 00:42:07.360]   Oh my God, it was a headline after headline.
[00:42:07.360 --> 00:42:16.480]   It's the most forthright I've ever seen Zuckerberg. More so than at all things D. He said a lot of
[00:42:16.480 --> 00:42:20.320]   stuff. He said, we made a huge mistake betting on HTML5.
[00:42:20.320 --> 00:42:21.040]   Yeah.
[00:42:21.040 --> 00:42:23.760]   Nope. Do you agree with that, by the way?
[00:42:23.760 --> 00:42:30.400]   Well, yeah. I mean, everybody agrees that the HTML5 based app on the iPhone was terrible.
[00:42:30.400 --> 00:42:36.800]   And the new app is so much better. He also said, no, we think most of our business is going to
[00:42:36.800 --> 00:42:40.560]   come from mobile and we know how to, we're going to be able to monetize it.
[00:42:40.560 --> 00:42:44.960]   I have to say, I did start seeing ads in the, in the, in the iPhone app. I think I did on
[00:42:46.240 --> 00:42:49.920]   Facebook. The stock is up 7.7% today. Yeah. He did a good job, didn't he?
[00:42:49.920 --> 00:42:56.240]   Loved the stock. He sold himself. He was disappointed by the stock price, who wouldn't be, but he says
[00:42:56.240 --> 00:43:02.720]   it's not going to hurt his, you know, employing people. He likes being underestimated, which certainly
[00:43:02.720 --> 00:43:06.880]   is the case now. Yeah. You always want to fly under the radar, don't you? Yeah.
[00:43:06.880 --> 00:43:12.880]   Never to be the top guy in front. But it was also interesting to me that this attitude toward
[00:43:12.880 --> 00:43:18.320]   hardware and software and service and so on and so forth, the difference, right? Apple is selling
[00:43:18.320 --> 00:43:24.240]   hardware. And I don't know what that really does them strategically in the long run, especially
[00:43:24.240 --> 00:43:30.320]   when they can't surprise you as they couldn't today. Right. Google was going to addict you to its
[00:43:30.320 --> 00:43:37.120]   services. Amazon is going to give you good deals on stuff. Amazon's got an ecosystem, but so does
[00:43:37.120 --> 00:43:41.200]   Apple. What is Apple? But Apple's always said, and I don't know if this is still true, but for a
[00:43:41.200 --> 00:43:45.200]   long time, they said, oh, we don't make money on iTunes. It's all about selling hardware.
[00:43:45.200 --> 00:43:54.480]   I don't know if they were if that was true or. I think at the point of the 99 cent songs,
[00:43:54.480 --> 00:43:58.320]   and that was pretty much all. Yeah, I would probably was true, but now they're selling so much other
[00:43:58.320 --> 00:44:06.400]   stuff. It's interesting. I think Amazon, I've read a great article. I can't find it.
[00:44:08.800 --> 00:44:13.920]   And I don't think it's in our links where the guy said, the reason Amazon is not so big
[00:44:13.920 --> 00:44:18.400]   internationally is Amazon's only interest because they don't make money on the hardware. They only
[00:44:18.400 --> 00:44:23.760]   make money on the store. Their only interest is to get Amazon hardware into the hands of the people
[00:44:23.760 --> 00:44:28.480]   who use Amazon the most. Those are what those are the people are going to make the most of. If
[00:44:28.480 --> 00:44:35.280]   you use it for a browser, you're wasting your cost to them. Well, there's a dynamic in Germany,
[00:44:35.280 --> 00:44:38.480]   in France. But for all, I know all of the EU, I don't know, but at least those two countries,
[00:44:38.480 --> 00:44:44.000]   you cannot discount books. You're not. Oh, really? Yes, it's to support the book.
[00:44:44.000 --> 00:44:50.320]   Those are the regulation. Right. And so that's how Amazon makes its hay is by giving you great
[00:44:50.320 --> 00:44:57.680]   prices by using its efficiency. Speaking of Facebook of this article, the New Yorker
[00:45:00.720 --> 00:45:06.720]   has a Facebook page in which they posted, okay, brace yourself. If you're watching this at home,
[00:45:06.720 --> 00:45:12.640]   this is going to be Nipilage. They posted this on Facebook and it got pulled down
[00:45:12.640 --> 00:45:17.200]   because there were nipples. It's a cartoon, Adam and Eve sitting under a tree.
[00:45:17.200 --> 00:45:25.440]   And Eve says to Adam, well, it was original. And apparently,
[00:45:29.920 --> 00:45:34.320]   they redrew the cartoon with clothes on, but it just really didn't work. It turns out
[00:45:34.320 --> 00:45:40.080]   that the dots are the offensive part. Now, Facebook does not block male nipples,
[00:45:40.080 --> 00:45:45.360]   only female nipples. So the nipples on top, well, whether or not nipples are just black dots,
[00:45:45.360 --> 00:45:51.680]   because it's a cartoon, after all, those are okay. No, I'm sorry. These are these are the
[00:45:51.680 --> 00:45:56.080]   illegal nipples. Those are the okay nipples. And for those of you listening, it's just two
[00:45:56.080 --> 00:46:05.120]   sets of socks, black dots. Where's Waldo? Yeah, yeah. So it didn't really put clothes on.
[00:46:05.120 --> 00:46:09.360]   He Bob had to be clear at the end of the post that no, no, no, no, we did that as a joke. It's a joke.
[00:46:09.360 --> 00:46:15.200]   And I'm trying to make this show family friendly. I don't find this offensive in the least.
[00:46:15.200 --> 00:46:22.160]   But and now, by the way, it was reinstated after they complained. But somebody at Facebook said,
[00:46:22.160 --> 00:46:35.440]   no, no, no, no, no, you can't have that. Okay, not okay. Just to be clear.
[00:46:36.480 --> 00:46:50.480]   Okay. Okay. Anyway, I don't know, I just thought that was funny. It's not really a big deal.
[00:46:50.480 --> 00:46:56.880]   I did think this was maybe not so funny. It turns out that the this Google fiber by the way,
[00:46:56.880 --> 00:47:03.280]   I think that what was it 98% of Kansas City neighborhoods have now a mass efficient interest
[00:47:03.280 --> 00:47:06.480]   to get Google fiber? It's like pretty much everybody who could get it's gonna get it.
[00:47:06.480 --> 00:47:18.320]   But it turns out that even though a Fred Campbell, a former FCC official portrayed the Google
[00:47:18.320 --> 00:47:23.200]   fiber project in Kansas City as a triumph for free markets, once government gets out of the way,
[00:47:23.200 --> 00:47:28.560]   deregulation promotes private investment that in fact, the Google fiber is getting all sorts
[00:47:28.560 --> 00:47:34.240]   of government support, including tax breaks, expedited permitting city officials actually
[00:47:34.240 --> 00:47:40.880]   assigned staff to help Google one county offered to allow Google to hang wires on its utility poles
[00:47:40.880 --> 00:47:44.880]   for free, all of which I would eagerly support here. Absolutely.
[00:47:44.880 --> 00:47:54.800]   Free rights of way central office space, power, marketing and direct mail.
[00:47:55.600 --> 00:48:00.880]   Well, this is the microcosm of Obama. Yeah, you didn't make it by yourself.
[00:48:00.880 --> 00:48:06.480]   You always make it in concert. Yes. And it's good for everybody, including
[00:48:06.480 --> 00:48:15.120]   citizens and business and business. 90% of the eligible neighborhoods though. Oh, I'm so jealous.
[00:48:15.120 --> 00:48:20.480]   And Google was so smart. They did the thing where you got to get your neighbors to.
[00:48:20.480 --> 00:48:28.160]   So out of the 202 possible neighborhoods, 180 have have a mass
[00:48:28.160 --> 00:48:33.520]   sufficient interest for Google to install. There were campaigns in poorer neighborhoods to make
[00:48:33.520 --> 00:48:38.720]   sure that people signed up. So that's great to get awareness of. That's great. Because it was
[00:48:38.720 --> 00:48:42.800]   wonderful. Actually, Google did a little bit of a push. There were a couple of neighborhoods that
[00:48:42.800 --> 00:48:50.800]   looked like they wouldn't get it. And Google, Google, I guess, said volunteers in to can.
[00:48:50.800 --> 00:48:57.200]   It's like an election. It's really cool. I'm so jealous. I want my Google fiber.
[00:48:57.200 --> 00:49:04.640]   Oh, and by the way, I should have mentioned this. We were talking about the HTML5 and the
[00:49:04.640 --> 00:49:09.360]   lousy Facebook app. It's still HTML5 on Android, but the Zuckerberg among other things did say,
[00:49:09.360 --> 00:49:12.720]   no, no, we're working on that. Full bore Android app as well.
[00:49:12.720 --> 00:49:17.280]   He didn't say when. Yeah, someday. And then Zuckerberg says, you know, when it's ready.
[00:49:17.280 --> 00:49:21.200]   And I thought, well, I stopped the way Facebook operates. It's awful, but some things when they're
[00:49:21.200 --> 00:49:28.160]   not ready. Yeah, it's true. But when it's shipped before it's done, it's okay. It's really okay.
[00:49:28.160 --> 00:49:36.240]   Let's take a break. We'll come back with more Gina Trippani, Jeff Jarvis this week in Google.
[00:49:36.240 --> 00:49:39.920]   Coming up shortly, we're a little bit delayed because of the Apple announcement.
[00:49:39.920 --> 00:49:46.560]   Our triangulation show. Good be good. Good interesting show. Hans Peter Braunmo is our guest.
[00:49:46.560 --> 00:49:52.400]   He is an entrepreneur who was hired. Actually, his company was purchased by Nokia a few years
[00:49:52.400 --> 00:49:58.480]   ago. And he's been charged with keeping an entrepreneurial spirit alive inside of Nokia.
[00:49:58.480 --> 00:50:02.880]   And we'll talk about Nokia's new phones. It's Windows 8 phones. The future of Nokia coming up
[00:50:02.880 --> 00:50:10.720]   in our triangulation show. But first, this episode of this week in Google brought to you by our friends
[00:50:10.720 --> 00:50:18.160]   at Ford and their new site, social.ford.com. A great place to go to get your tech geek badge to
[00:50:18.160 --> 00:50:27.200]   learn about Ford vehicles, to share your ideas. I think this is kind of the best modern 21st century
[00:50:27.200 --> 00:50:33.120]   suggestion box. Lots of ideas here. You can read the ideas, vote on them. In fact, we could sort
[00:50:33.120 --> 00:50:37.520]   by popularity. We could see what the most popular ideas are and the least popular ideas are.
[00:50:37.520 --> 00:50:44.880]   Oh, this is a great idea. Have 110 volt AC outlet in your car. Actually, the Ford Flex has that,
[00:50:44.880 --> 00:50:50.240]   but wouldn't it be good in all vehicles have an AC outlet? Forget the USB thing. Let's do it.
[00:50:50.240 --> 00:50:54.400]   So you like that idea? So do 18 other people just go on over and vote.
[00:50:55.280 --> 00:50:58.960]   Ford's paying attention. I got to tell you, they really want to know what you think.
[00:50:58.960 --> 00:51:02.960]   There's also articles there. I've got one I wrote just a brief one talking about my
[00:51:02.960 --> 00:51:09.200]   conversation with Ford technologist Jim Butchkowski about what's new and sync, what's coming in
[00:51:09.200 --> 00:51:15.600]   autonomous vehicles and so forth. There's articles by Bill Ford in there. He's it. You know, if you
[00:51:15.600 --> 00:51:23.680]   I credit Alan Malali a lot with the CEO for with transforming the company, but a lot of credit.
[00:51:23.680 --> 00:51:31.200]   Oops, I get a badge. 1902 Ford 999. Oh, 93 miles an hour in 1902. Wow.
[00:51:31.200 --> 00:51:40.480]   Bill Ford really has led in some interesting ways the company in terms of its environmental
[00:51:40.480 --> 00:51:46.400]   initiatives and so forth. So there's some good stuff in here. I like this site social.ford.com.
[00:51:46.400 --> 00:51:50.720]   Take a visit. Grab a badge. There's a tech geek badge just for you right over here.
[00:51:51.840 --> 00:51:57.760]   And share your interest in Ford with others. And by the way, don't forget to drive a Ford
[00:51:57.760 --> 00:52:03.840]   vehicle. Your Ford dealer near you. You might be pleasantly surprised. Some amazing stuff coming
[00:52:03.840 --> 00:52:10.560]   from Ford, including that 2012 all electric focus next year, the 2013 energy plug-in
[00:52:10.560 --> 00:52:16.960]   hybrids of the fusion. It's going to be awesome. Social.ford.com. We thank them for their support.
[00:52:20.880 --> 00:52:24.240]   Let's see. Variety magazine. This is so sad.
[00:52:24.240 --> 00:52:30.000]   I just so sad. So variety, which is like that was the Hollywood magazine.
[00:52:30.000 --> 00:52:34.880]   Oh, it was it. It was it. It's got to be worth a ton, right? Yeah. It's for sale.
[00:52:34.880 --> 00:52:41.120]   The price, it's a fire sale. Reed Elsevier can't get anybody to buy variety. The price is now down
[00:52:41.120 --> 00:52:49.680]   to $30 million. That's less than AOL paid for tech crunch. Oh, ouch. It's not that it's not
[00:52:49.680 --> 00:52:56.000]   that variety's got beaten by some other publication. It's that nobody's reading print anymore, right?
[00:52:56.000 --> 00:53:01.200]   And online, they went behind a pay wall when there's tons and tons of competitors.
[00:53:01.200 --> 00:53:06.160]   Yeah, there's there's you know, Perez Hilton. Yeah. It's all you need to say.
[00:53:06.160 --> 00:53:11.920]   They it's amazing how quickly a new brand can come along when I when I worked with Condi and
[00:53:11.920 --> 00:53:20.720]   asked, I believe it or not had to work on Bridal a lot. That's great. It is. It is.
[00:53:20.720 --> 00:53:25.680]   You look good. White. It's good. I want a picture of you with a with a
[00:53:25.680 --> 00:53:31.680]   avail. Somebody has to Photoshop that, please. Whatever I'd go to a Bridal meeting, everyone
[00:53:31.680 --> 00:53:39.200]   would laugh at me. So what was there? We bought we bought the own brides. We bought modern brides.
[00:53:39.200 --> 00:53:43.200]   We bought elegant bride. We talked, oh, well, we're at we got bridal, right? And then did some
[00:53:43.200 --> 00:53:49.600]   market research and thought out the number one brand in Bridal was the not the not right. And
[00:53:49.600 --> 00:53:54.800]   and so I know they're and rebuilt what we did and made it a lot better and and and so on.
[00:53:54.800 --> 00:54:00.960]   But it's possible now for you to leapfrog with new brands. And that's really kind of an interesting
[00:54:00.960 --> 00:54:05.440]   thing. Great. And it happened in print. And I think it's next kind of happened potentially in
[00:54:05.440 --> 00:54:11.600]   mobile. Pew just came out with some more numbers. I don't have them up on the site with continuing.
[00:54:11.600 --> 00:54:18.000]   This goes on unabated that that African Americans and Hispanics index a lot higher
[00:54:18.000 --> 00:54:23.760]   than the white population and total population in smartphones leapfrog opportunity, right?
[00:54:23.760 --> 00:54:29.120]   There's a chance to come in behind and suddenly a new brand can take over a space
[00:54:29.120 --> 00:54:32.880]   because the old one is protecting this old space and that's what happened to variety.
[00:54:33.440 --> 00:54:40.400]   Do you think it has to happen at at when a new media arises that it's not just going to happen
[00:54:40.400 --> 00:54:44.800]   in general because we're all into technology and things are moving faster? It's because there
[00:54:44.800 --> 00:54:49.360]   was a new medium. There's an opportunity. That's that's what poses the opportunity.
[00:54:49.360 --> 00:54:55.680]   Now first you have desktop browsing on the internet. Tablets probably are are kind of in
[00:54:55.680 --> 00:55:00.160]   addition to smartphones another, you know, new opportunity. I think I think mobile apps will
[00:55:00.160 --> 00:55:06.240]   will mobile space will allow a new brand built well for that world to leapfrog, you know,
[00:55:06.240 --> 00:55:11.040]   because somebody if if Facebook didn't get us act together on mobile, can somebody create a new
[00:55:11.040 --> 00:55:17.040]   social there's the opportunity. Yeah, yeah, there is. Yeah. Do you think that the iPad is mobile?
[00:55:17.040 --> 00:55:20.480]   Do you think this? I mean, Zuckerberg did not. It's an interesting debate.
[00:55:20.480 --> 00:55:26.400]   Though I carry around my Nexus seven, right? Really dorkily in my back pocket.
[00:55:26.400 --> 00:55:33.680]   To me, mobile means mobile phone. Not tablet though. My my stock tweet ready line is
[00:55:33.680 --> 00:55:40.640]   that mobile equals local and local equals around me. So whatever is around me is mobile.
[00:55:40.640 --> 00:55:47.040]   Is mobile? Yeah, because it doesn't matter. Yeah, the huge proportion of tablet usage is in the home.
[00:55:47.040 --> 00:55:52.560]   Right. Yeah, makes sense. I think of tablets as a PC replacement really.
[00:55:53.280 --> 00:55:58.240]   Yes. And I think of smartphones as most truly mobile like you're out and about.
[00:55:58.240 --> 00:56:03.520]   Yeah, your phone's always in your pocket. The tablet for me is the living room kitchen computer.
[00:56:03.520 --> 00:56:08.320]   Yeah, another computer. Yeah. Yeah. So Zuckerberg was right when he said that
[00:56:08.320 --> 00:56:16.480]   iPad's not a mobile device. He also said something about TV being closer to a mobile being closer to
[00:56:16.480 --> 00:56:21.440]   TV than it is to computers. Yeah, I didn't I don't know that I totally got that. He said that
[00:56:21.440 --> 00:56:27.040]   because the ads on TV are much more immersive that because you can't have ads on the right hand
[00:56:27.040 --> 00:56:34.560]   column in mobile that they probably got to be more in the to them. Right. You can't just have
[00:56:34.560 --> 00:56:38.240]   the ads just this there. You probably have to take over the experience with the app.
[00:56:38.240 --> 00:56:44.160]   Is that it? Maybe or did he mean inserting ads kind of into the stream the way that ads are kind
[00:56:44.160 --> 00:56:50.480]   of they're interstitial. You can't get away from it. It is it is the fact that the banner ads and
[00:56:50.480 --> 00:56:56.640]   kind of little things at the bottom of the screen probably don't do much. You want to get you want
[00:56:56.640 --> 00:57:02.000]   to take over right you're right you're surrounding the content versus I don't know if it was interstitial
[00:57:02.000 --> 00:57:07.040]   versus in stream ads. You know, like it looks like just part of my news stream and I can you know
[00:57:07.040 --> 00:57:15.040]   scroll past it just the way I can. But that's that's kind of what I have to trick people.
[00:57:16.000 --> 00:57:20.240]   Well, I don't know that that they that they're a trick. I don't know that they have to take over
[00:57:20.240 --> 00:57:25.280]   your your whole phone screen and I don't I think they can show up in your news feed and say like,
[00:57:25.280 --> 00:57:30.960]   hey, you're right near Chipotle like free burrito, you know, and have it be clearly marked as an ad,
[00:57:30.960 --> 00:57:35.360]   but how to be part of your news stream. I like Chipotle. They've got a free burrito thing. I'm
[00:57:35.360 --> 00:57:40.560]   right near one. I see it my new stream. Well, that's the key. I like Chipotle is where the real value is.
[00:57:40.560 --> 00:57:45.680]   Yes. I got knowledge of you is real. I kept on thinking about our conversation last week about
[00:57:45.680 --> 00:57:48.080]   the American Express stuff and how you guys were using that.
[00:57:48.080 --> 00:57:53.520]   Of course, where that is really a powerful, powerful new way to market.
[00:57:53.520 --> 00:57:57.440]   That's not about ads. It's about value to you and knowledge about you.
[00:57:57.440 --> 00:58:04.160]   Right. And if I if I like Chipotle on, you know, I've likely on Facebook when I keep seeing ads
[00:58:04.160 --> 00:58:08.720]   about the, you know, the Glee premiere, I'm like, yeah, I'm stoked. You know, that's that's
[00:58:08.720 --> 00:58:12.480]   advertising. I'm much more open to and I understand why I got it.
[00:58:12.480 --> 00:58:15.760]   We're demographic, Gina. Yes.
[00:58:15.760 --> 00:58:18.000]   Please.
[00:58:18.000 --> 00:58:27.120]   Twitter, this is interesting because this is not, this is not a search warrant or not a subpoena.
[00:58:27.120 --> 00:58:34.800]   Twitter has been ordered to release information, deleted tweets and so forth about a protester in
[00:58:34.800 --> 00:58:44.720]   the Occupy Wall Street movement or be fined by the court. This is a 2703 order, which is a request
[00:58:44.720 --> 00:58:51.680]   for information, but allows prosecutors to get information without a warrant. It's, well,
[00:58:51.680 --> 00:58:55.360]   it's more powerful than a subpoena, but not as strong as a search warrant.
[00:58:55.360 --> 00:58:59.520]   The order is supposed to be issued when prosecutors provide a judge with specific
[00:58:59.520 --> 00:59:04.720]   and articulable facts that show the information is relevant material to a criminal investigation.
[00:59:05.040 --> 00:59:12.400]   Twitter says they move to quash. They said, we're not, they're not going to give you the
[00:59:12.400 --> 00:59:18.000]   information judge, she or Reno ordered Twitter to release the tweets and account information.
[00:59:18.000 --> 00:59:26.160]   Ruling that the defendant had no expectation of privacy when he published tweets,
[00:59:26.160 --> 00:59:32.800]   quote, if you post a tweet, just like if you scream it out the window, there's no reasonable
[00:59:32.800 --> 00:59:38.560]   expectation of privacy. There's no proprietary interest in your tweets. You have now gifted them
[00:59:38.560 --> 00:59:44.800]   to the world. To that extent, I think that's true. But what I learned about this case is that
[00:59:44.800 --> 00:59:51.120]   there are things that they're asking for that are not public. And they didn't ask for direct
[00:59:51.120 --> 00:59:56.320]   messages. And the judge said, no, those would require a warrant based on probable cause for data
[00:59:56.320 --> 01:00:02.560]   like time and place and things wouldn't necessarily be public. And yeah, I could shout out the window,
[01:00:02.560 --> 01:00:07.040]   but if no one heard me and it goes off in the ether, then that's my good luck.
[01:00:07.040 --> 01:00:15.040]   Right. So if I kill the tweet and no one else recorded it, except Twitter,
[01:00:15.040 --> 01:00:22.560]   is Twitter required to dime its users? And let's remember Yahoo and a man just got out of prison
[01:00:22.560 --> 01:00:28.560]   after a full decade. He spent in prison in China. But that was his male. That was,
[01:00:29.200 --> 01:00:33.200]   but this is communication. Yeah, this communication. And so I agree with the judge that when something
[01:00:33.200 --> 01:00:38.320]   is public is public, because anybody can hear that, anybody can record that, anybody can pass that on.
[01:00:38.320 --> 01:00:44.480]   And that's what you did. And that's what it is. But what Twitter I think is saying here and trying
[01:00:44.480 --> 01:00:49.600]   to, you know, save itself as a platform for us is that they're asking for the next layer there.
[01:00:49.600 --> 01:00:55.040]   Right. The court ordered Twitter to release the data or hand over its confidential earnings
[01:00:55.040 --> 01:01:00.240]   statements in the last two quarters so they could determine how much of a fine levy against Twitter.
[01:01:00.240 --> 01:01:07.760]   They have till September 14th. Twitter said Twitter users own their tweets and should have the right
[01:01:07.760 --> 01:01:13.840]   to fight invalid government requests. There's the key. And that's where AMAC, you know, I have
[01:01:13.840 --> 01:01:18.560]   my problems lately with Twitter, but that's that's a really important doctrine. And good for them.
[01:01:19.280 --> 01:01:26.240]   We need protection of our communication, not just the US male. That's again, that's a principle that
[01:01:26.240 --> 01:01:34.000]   should be carried to the new world and and figure out how we enforce it there. And as I said, I'm
[01:01:34.000 --> 01:01:38.640]   not totally against the ruling on what the public part, but there is a line we got to we got to
[01:01:38.640 --> 01:01:40.880]   figure out what that line is. And it's not what the judge is saying. I don't think.
[01:01:41.600 --> 01:01:48.640]   I mean, Gina, you so let's say that Twitter changed its its retention policy and erased
[01:01:48.640 --> 01:01:53.520]   the tweets. Right. When I was on online sites, we tried to retain data as as briefly as possible,
[01:01:53.520 --> 01:01:57.280]   just because we didn't like it subpoenaed all the time. So let's say that Twitter decided
[01:01:57.280 --> 01:02:00.640]   off screw it. Let's let's erase it. But you, of course, are our retention vehicle.
[01:02:00.640 --> 01:02:08.960]   And so the judge comes to you and it over lady. What do you do? What do you say? What's your
[01:02:08.960 --> 01:02:13.600]   principle? What's your argument? Well, so this is the reason why we built Think Up,
[01:02:13.600 --> 01:02:20.640]   which captures tweets as a decentralized app on your own service. I'm not retaining everyone's
[01:02:20.640 --> 01:02:25.280]   tweets. You installed on your own server, you got your own copy, just the way that you, you know,
[01:02:25.280 --> 01:02:29.200]   you off on your Twitter client on your phone. I mean, there's a cash copy of your tweets on your
[01:02:29.200 --> 01:02:33.440]   phone as well. Every device that you look at your tweets on, there's a cash copy somewhere
[01:02:33.440 --> 01:02:37.920]   on that device. But this is one of the main reasons why Think Up is a decentralized app.
[01:02:37.920 --> 01:02:42.000]   You thought about it already? Yeah, no, it's about control.
[01:02:42.000 --> 01:02:46.560]   I think we'd all agree that tweets are public. But remember that the government's asking for
[01:02:46.560 --> 01:02:52.000]   any information Twitter had about the owner of the account, the username, email address,
[01:02:52.000 --> 01:02:56.640]   IP addresses used to access the account to post the tweets. So it's more than just the
[01:02:56.640 --> 01:03:01.040]   public stuff. Yeah, and that's and that's not anything that Think Up or any other app can get.
[01:03:01.040 --> 01:03:03.440]   Yeah, you can't get IP addresses or email addresses
[01:03:05.600 --> 01:03:09.680]   through the API. So I would, I'm not sure if Twitter, which part Twitter is fighting,
[01:03:09.680 --> 01:03:12.880]   if they're saying, Hey, we're not going to give you the private stuff, we'll give you,
[01:03:12.880 --> 01:03:18.160]   but it seems to me that the tweets are public. They were shouted from the rooftops. So,
[01:03:18.160 --> 01:03:23.280]   I don't think they are. Google's cash, right? Yes. I don't think they're protected.
[01:03:23.280 --> 01:03:27.760]   No, I don't think so either. But I do admire Twitter for fighting the good fight.
[01:03:31.680 --> 01:03:39.440]   Speaking of which, I guess Forbes had a profile of of Williams and Bizstone, the creators
[01:03:39.440 --> 01:03:43.760]   of Twitter. What are the Twitter founders up to now? Not so obvious.
[01:03:43.760 --> 01:03:49.440]   It's a little play on the fact that the name of the company is obvious.
[01:03:49.440 --> 01:03:52.240]   Actually, it's actually obvious.
[01:03:55.440 --> 01:04:03.760]   So, so Bizstone stepped down from Twitter and started obvious backup. Interesting.
[01:04:03.760 --> 01:04:11.600]   Right. That was the name of their company back in the day. Yeah, they did ODO and then pivoted to
[01:04:11.600 --> 01:04:18.800]   Twitter. Yep, and then they just launched Medium. Oh, yeah, Medium. That's what it is.
[01:04:18.800 --> 01:04:21.920]   I was trying to remember what they're doing. That's what they're working on. I've got a little
[01:04:21.920 --> 01:04:30.560]   little squib on Medium. Is it Medium.com? Yep. And you're on that, right? I wrote one.
[01:04:30.560 --> 01:04:34.560]   I mean, right another one. It's very, it's an elegantly done interface.
[01:04:34.560 --> 01:04:37.920]   Yeah, it's beautiful. You off through Twitter.
[01:04:37.920 --> 01:04:44.320]   Oh, but you have to get an invite. It's beta. You do. Yeah. But, and we'll get you.
[01:04:44.320 --> 01:04:46.720]   No, I've walked to anything for me.
[01:04:49.360 --> 01:04:57.360]   He's just another person I've pissed off. That's. Oh, no. I want to ask. No, I know. I don't know.
[01:04:57.360 --> 01:05:02.320]   I don't. Maybe I didn't piss him off. He just never calls. He never writes. He never writes. He does.
[01:05:02.320 --> 01:05:10.720]   Uh, I think that's it. I don't see anything else that we should talk about.
[01:05:10.720 --> 01:05:12.640]   Oh, we have Jeff and a veil.
[01:05:14.800 --> 01:05:22.960]   We have Jeff and a veil already. You guys are too good. Uncle Bic. Good job, Uncle Bic.
[01:05:22.960 --> 01:05:26.880]   Oh, that's scary. That is scary, scary, scary.
[01:05:26.880 --> 01:05:31.680]   Right. That's quite a dress. Quick and dirty.
[01:05:31.680 --> 01:05:34.560]   I rather like it. Don't you think? Yeah, nice hips.
[01:05:34.560 --> 01:05:37.280]   My deck of litage is rather appealing. Yes.
[01:05:37.280 --> 01:05:40.880]   I guess it's Vic Lovan who did that originally.
[01:05:42.080 --> 01:05:47.760]   I'm sorry. I had to. I think we have a show title to Jeff and address.
[01:05:47.760 --> 01:05:54.320]   Let us get your tip of the week, Gina, true, pan.
[01:05:54.320 --> 01:05:59.280]   Yeah, I did. We actually didn't talk about the YouTube, the new YouTube app for iOS.
[01:05:59.280 --> 01:06:03.200]   That's going to be my tool. Oh, sorry. Okay. So I won't talk about that.
[01:06:03.200 --> 01:06:05.760]   The new Google Drive. I should write that in.
[01:06:06.960 --> 01:06:13.120]   Google Drive has a new update for both Android and iOS. On iOS, the Google Drive app,
[01:06:13.120 --> 01:06:20.080]   you can now edit documents on your iPhone or iPad. The Android app, I think, got a comment.
[01:06:20.080 --> 01:06:24.080]   So it's just a small incremental update, but nice updates.
[01:06:24.080 --> 01:06:32.960]   We're seeing a few Google apps landing on iOS with iOS 6 coming out on September 19th,
[01:06:32.960 --> 01:06:37.520]   which is next week. And also, I have a quick shameless plug. Speaking of think up, we got a new
[01:06:37.520 --> 01:06:41.760]   URL. Just think up.com. Really easy to remember. Did you have to buy that?
[01:06:41.760 --> 01:06:48.720]   We did. We did. It was not. And we brand new release this week version 1.1.
[01:06:48.720 --> 01:06:54.960]   Now supports a four square. So we have Twitter, Facebook,
[01:06:54.960 --> 01:06:59.680]   four square and Google plus support. So you can capture your posts and visualize them and map
[01:06:59.680 --> 01:07:02.880]   them. And the four square plug-ins really cool. One of our contributors, Aaron,
[01:07:02.880 --> 01:07:08.080]   wrote it. Maps or tweets sees your check ins over time, sees, you know, time machine check ins
[01:07:08.080 --> 01:07:13.120]   from past years. So check it out. Think up.com. Can I ask you a question? Yeah. Talk to me.
[01:07:13.120 --> 01:07:18.880]   What the? So in the open source, I don't know if I could pause this image.
[01:07:18.880 --> 01:07:26.320]   Is that an octopus cat? What is that? What is that? That's octocat. Octocat is, um, is GitHub's,
[01:07:26.320 --> 01:07:31.840]   uh, oh, it's GitHub. Yeah. It's GitHub's mascot. Yeah. There it is. Octocat. That's octocat. So it's
[01:07:31.840 --> 01:07:36.480]   a cat with a human face and a wet tail with suckers on it. That was just confusing me. Exactly.
[01:07:36.480 --> 01:07:42.240]   It's an octopus with a cat face. It's GitHub's, uh, um, logo. So I want to have nightmares tonight.
[01:07:42.240 --> 01:07:49.440]   I love it. I love it. Octocat is awesome. Think up.com. No more think up.
[01:07:49.440 --> 01:07:54.800]   App.com. Think up.com. That's great. New release. It's free, right? It is free. Absolutely free.
[01:07:54.800 --> 01:08:00.240]   And we have a super easy installer for PHP fog and for Amazon EC2. So it's one click. You can just
[01:08:00.240 --> 01:08:03.520]   spin up an instance. It's really painless if you don't feel like, if you don't have your own
[01:08:03.520 --> 01:08:07.440]   web server. Good. I want to do this because I want to add a four square in Google plus and Facebook.
[01:08:07.440 --> 01:08:12.480]   Awesome. It just would collect everything. So what is the business model?
[01:08:12.480 --> 01:08:18.240]   For think up. Yeah. Well, you know, we're working on it at the moment, um, but you know,
[01:08:18.240 --> 01:08:25.360]   there's like, there's opportunities for affiliate fees from cloud providers. Um, there's, we're
[01:08:25.360 --> 01:08:29.600]   working on our 2.0 release right now, which will be, we're going to have the beta channel in the
[01:08:29.600 --> 01:08:34.960]   next few weeks, which is going to be less Google Analytics charts and more of kind of an insight
[01:08:34.960 --> 01:08:37.920]   stream of things going on with your tweets that are interesting. And there's some, there's some
[01:08:37.920 --> 01:08:44.400]   opportunities for, um, discovering interesting content across networks that way. So there could
[01:08:44.400 --> 01:08:50.080]   be, you know, there could be a premium service that you, that you buy to install. It could be
[01:08:50.080 --> 01:08:54.400]   advertising. We're still, we're still working it out. We're right now just focusing on making a
[01:08:54.400 --> 01:08:59.120]   really good product that's compelling. And we're right now it's more like an analyst, Google
[01:08:59.120 --> 01:09:02.320]   Analytics dashboard. You know, you set it up, you look at it, you're like cool charts and then you
[01:09:02.320 --> 01:09:05.680]   don't look at it again. We're working on kind of an insight stream. That's going to be a lot
[01:09:05.680 --> 01:09:10.240]   more compelling and interesting kind of day to day. So just for people who want to know what it
[01:09:10.240 --> 01:09:16.800]   looks like, here's my think up stream and here's the activity and followers by day. Whoops.
[01:09:16.800 --> 01:09:22.720]   I hate that feature of, uh, yeah, Chrome, where you accidentally scroll to a previous
[01:09:22.720 --> 01:09:29.600]   page. Anyway, that just did that. And, uh, you get tweets, you get followers. Oh, this is fun.
[01:09:29.600 --> 01:09:32.560]   Who you follow is good and get more information about them.
[01:09:32.560 --> 01:09:35.520]   Yeah. So that's a one Oh version.
[01:09:36.160 --> 01:09:41.840]   This is, yeah, I got to update it. I got to update it. This hasn't been updated in months.
[01:09:41.840 --> 01:09:46.960]   I'm a bad man. I think I have a, I think I have an installation for you Leo and you too, Jeff.
[01:09:46.960 --> 01:09:50.720]   I'll, I'll email to you. Okay. That would be keeping up to date. Thanks. I would love to have
[01:09:50.720 --> 01:09:55.120]   you preview the new beta. It's good. Love that. It's really cool. Love it. Sure.
[01:09:55.120 --> 01:10:01.760]   Your number of the week, Mr. Jarvis. Well, so we know that, uh, Google was predicting flu
[01:10:01.760 --> 01:10:05.280]   by being able to look at the search results or search requests that people made. And that's
[01:10:05.280 --> 01:10:09.520]   been the case for a while to the point that Google executives and founders have used that is the
[01:10:09.520 --> 01:10:14.720]   reason to argue to governments. They shouldn't be forced to erase data that looking at the,
[01:10:14.720 --> 01:10:18.400]   um, the changes and deltas and data can make a difference. Well, now obviously, I, I, I,
[01:10:18.400 --> 01:10:22.000]   I was probably, I haven't seen more of this. A researcher at University of Rochester,
[01:10:22.000 --> 01:10:29.360]   Adam's set elect, uh, analyzed 4.4 million tweets and found that he could predict up to eight days
[01:10:29.360 --> 01:10:33.840]   in advance with 90% accuracy when healthy people would start getting sick. But how does he know?
[01:10:33.840 --> 01:10:40.800]   What does somebody says? Oh, my head. He probably used the word flu and he is doing geographic, um,
[01:10:40.800 --> 01:10:46.640]   analysis of that. So how would he know ahead of time? Because they're not sick yet. But it's
[01:10:46.640 --> 01:10:53.440]   spread. It's going to next. So he's looking at the, at the, um, what do you call it? Yeah. Um,
[01:10:53.440 --> 01:11:00.800]   vectors, vectors. Wow. That is, that is really cool. That is really cool.
[01:11:01.680 --> 01:11:11.840]   There's a, and there's also a web application called found.in that gives you health insights in
[01:11:11.840 --> 01:11:22.320]   real time. Analyze health at scale through AI. Wow. Wow. That's really neat. There's value in all
[01:11:22.320 --> 01:11:32.080]   of our data and big data, baby. My app, yes, you already know, is the new YouTube for iOS Google,
[01:11:32.080 --> 01:11:40.960]   needed to do this because Apple pulled the YouTube app from, um, iOS six. I think it's a lot better,
[01:11:40.960 --> 01:11:46.080]   frankly. And you know, it needed to be updated. I've logged in. So this is, this is from my account.
[01:11:46.080 --> 01:11:51.760]   So it's showing stuff that are in my feeds and people I follow and so forth. That's kind of handy
[01:11:51.760 --> 01:11:56.080]   too. Cause you know, I follow Jerry Ellsworth. So I've subscribed to her and we're going to start
[01:11:56.080 --> 01:12:00.000]   taking advantage of this, by the way, we're going to have our own twig channel. We're going to have
[01:12:00.000 --> 01:12:05.600]   our own channel for all of which shows so that people can follow twig. And then if you're using
[01:12:05.600 --> 01:12:11.040]   the YouTube app, it'll show up right here inside your channels that you're following. And, uh,
[01:12:11.040 --> 01:12:16.480]   there's also, you know, from YouTube, the popular channel. So you can see Demi Lovato gets owned by
[01:12:16.480 --> 01:12:21.040]   next factory steamed, uh, Brittany learns, oh, this should be good. Uh,
[01:12:21.040 --> 01:12:32.000]   Oh boy, Britney Spears, uh, learns how to a dance gangnam style. Yeah, we'll watch that while, uh,
[01:12:32.000 --> 01:12:37.200]   anyway, that's the new to YouTube app. It's, uh, this is actually an iPhone app, uh, which we're, uh,
[01:12:37.200 --> 01:12:42.000]   demonstrating on an iPad, but you know, he blows up fine on the iPad as well. Let me just jump ahead
[01:12:42.000 --> 01:12:48.480]   and see if she, uh, she's going to learn how to dance gangnam style. Oh my gosh, it's Si.
[01:12:48.480 --> 01:12:59.200]   Oh, there he is. Wow, you know, you miss one day of Ellen and you miss so much. Thank goodness
[01:12:59.200 --> 01:13:04.480]   for YouTube. Uh, well, we'll just save that for an exercise for later.
[01:13:07.600 --> 01:13:15.360]   Father Robert Ballis are now on, uh, on Ellen. Okay. How do I stop it? Stop.
[01:13:15.360 --> 01:13:20.560]   Cause I know everybody's now rushing to YouTube and they've forgotten about this show.
[01:13:20.560 --> 01:13:25.360]   Thank you, Gina, Trippani. Congratulations. Think up.com, the new website.
[01:13:25.360 --> 01:13:29.040]   Thank you very much. Blogs@smarterwear.org.
[01:13:29.040 --> 01:13:33.600]   And, uh, it's always good, always good to have you on the show. We appreciate it.
[01:13:33.600 --> 01:13:35.600]   Great show today. Worth the wait.
[01:13:35.600 --> 01:13:39.600]   Worth. Yeah. Sorry. We ran a little bit late with the, that stupid iPhone.
[01:13:39.600 --> 01:13:42.960]   It was fun. I loved watching your coverage as it went. Thank you.
[01:13:42.960 --> 01:13:45.040]   Did you like the puppet show? It was really good.
[01:13:45.040 --> 01:13:50.080]   Jury was out on the puppet show. You gotta have Sarah puppet and, and, uh,
[01:13:50.080 --> 01:13:57.280]   Sarah was so unhappy. That's great. I'm going to sign with Sarah on everything else, but the
[01:13:57.280 --> 01:14:02.080]   good puppet was okay. Okay. Yeah. Oh, but you gotta do a Sarah puppet now. You have to have a war
[01:14:02.080 --> 01:14:08.000]   of puppets. That is Jeff Jarvis, Sarah Lane. Jeff Jarvis. He is, uh, the author of what would
[01:14:08.000 --> 01:14:12.720]   Google do and his latest public parts. He's also a blogger at buzzmachine.com.
[01:14:12.720 --> 01:14:17.280]   Thank you everybody for joining us. We do this week in Google normally on Wednesdays, 1 p.m.
[01:14:17.280 --> 01:14:24.240]   Pacific, 4 p.m. Eastern time. Uh, that would be 2000 UTC on Twitch.tv, but you get on-demand
[01:14:24.240 --> 01:14:27.680]   audio and video after the fact, like all our shows at Twitch.tv. In this case,
[01:14:27.680 --> 01:14:33.840]   twit.tv/twave. We're on YouTube soon to have our very own YouTube channel, but right now it's
[01:14:33.840 --> 01:14:43.600]   youtube.com/twit. Thanks for joining us and we'll see you next week on Twitch.
[01:14:43.600 --> 01:14:49.600]   [Music]

