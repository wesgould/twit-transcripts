;FFMETADATA1
title=Alex Stamos
artist=Leo Laporte, Jeff Jarvis, Ant Pruitt, Alex Stamos
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-12-01
track=692
language=English
genre=Podcast
comment=Is TikTok a threat, Twitter's Chinese porn spam, 2024 election disinformation
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.160]   It's time for twig this week in Google. We're gonna break format today for I think one of the most important shows
[00:00:06.160 --> 00:00:08.700]   We've ever done Jeff Jarvis is here
[00:00:08.700 --> 00:00:14.840]   Ant Pruitt is here and our special guest filling in for Stacy this week. She'll be back next week
[00:00:14.840 --> 00:00:23.320]   Our special guest is Alex Stemos. This guy has been on the front lines of the security battle starting at Yahoo
[00:00:23.320 --> 00:00:27.400]   Then Facebook he worked at zoom and helped bring them into compliance
[00:00:27.400 --> 00:00:34.740]   He's right now the Stanford Internet Observatory. He is the security guru. He's next on twig
[00:00:34.740 --> 00:00:40.760]   Podcasts you love from people you trust
[00:00:40.760 --> 00:00:43.960]   This is to it
[00:00:43.960 --> 00:00:52.040]   This is twig this week in Google episode
[00:00:52.040 --> 00:00:55.000]   692 recorded Wednesday November 30th
[00:00:55.960 --> 00:00:58.120]   2022 Alex Stemos
[00:00:58.120 --> 00:01:02.760]   This week in Google is brought to you by
[00:01:02.760 --> 00:01:04.240]   Noreva
[00:01:04.240 --> 00:01:08.120]   Noreva has simplified everything about meetings and classroom audio
[00:01:08.120 --> 00:01:16.400]   You get great audio and systems that are easy to install and manage visit noreva.com/twit and get 50% off one Noreva
[00:01:16.400 --> 00:01:22.480]   HDL 300 system for mid-sized rooms when you get a live online demo and buy before December 16th
[00:01:22.480 --> 00:01:25.160]   2022 and by
[00:01:25.640 --> 00:01:30.920]   Rocket money formerly known as true bill. Are you wasting money on subscriptions?
[00:01:30.920 --> 00:01:39.400]   Cancel your unnecessary subscriptions right now at rocket money.com/twig seriously it could save you hundreds per year
[00:01:39.400 --> 00:01:43.000]   And by
[00:01:43.000 --> 00:01:52.680]   Secureworks are you ready for inevitable cyber threats secure works detects evolving adversaries and defends against them with a combination of security analytics and
[00:01:52.880 --> 00:01:59.860]   Threat intelligence directly from their own counter threat unit visit secureworks.com/twit to get a free trial of
[00:01:59.860 --> 00:02:04.560]   Tages extended detection and response also known as XDR
[00:02:04.560 --> 00:02:10.880]   It's time for twig this week in Google to show we cover the latest news from
[00:02:10.880 --> 00:02:19.160]   Everybody but Google pretty much these days. That's certainly been a lot of Twitter maybe a little Facebook, but this week is a very very
[00:02:20.000 --> 00:02:27.720]   Special show before I introduce our special guest. Let me say hello to Jeff Jarvis the Leonard Ta professor for journalistic innovation at the Craig
[00:02:27.720 --> 00:02:35.840]   Newmark graduate school of journalism at the City University of New York. Hello Jeff. Hello. Hello. Hello. I'm very excited about
[00:02:35.840 --> 00:02:40.960]   I know and you're the blame for it. So I think I am. It's my fault. I think it was your idea. The Christmas tree lights are on
[00:02:40.960 --> 00:02:47.500]   You know in celebration of today. Yeah. Oh, we got to decorate. This is the last day of November. It means we can put up the
[00:02:47.960 --> 00:02:49.960]   House of Holly
[00:02:49.960 --> 00:02:52.360]   Remember we do that
[00:02:52.360 --> 00:03:00.600]   Prudence also here from hands-on photography last year. I just issued an e-dict and it all came in and it literally just got
[00:03:00.600 --> 00:03:02.600]   a roll on the studio
[00:03:02.600 --> 00:03:09.680]   Hands-off photography also very important our community manager. Oh, yeah club to it now Jeff
[00:03:09.680 --> 00:03:12.800]   Do you want to introduce Alex or shell? No, no, no, you should introduce Alex
[00:03:12.800 --> 00:03:19.960]   So we have been there Alex. It's it's rare we get the subject of many of our stories actually sitting in the studio
[00:03:19.960 --> 00:03:24.600]   Which makes me really nervous. I hope almost where we're wrong
[00:03:24.600 --> 00:03:31.280]   Too off base, but you know the name Alex Stamos. We talk about Alex a lot
[00:03:31.280 --> 00:03:35.320]   He's a principal in Krebs Stamos secured which is a security
[00:03:35.320 --> 00:03:41.440]   Consultancy, but we've been talking about you Alex since you were at Yahoo
[00:03:41.800 --> 00:03:43.800]   Were you see so what was your title? Yes?
[00:03:43.800 --> 00:03:49.680]   CISO at Yahoo and CSO at Facebook and Facebook where you famously got in
[00:03:49.680 --> 00:03:57.800]   Tussle with management over Russian entities invading Facebook. That is how it has been reported. Yes, I guess we'll get
[00:03:57.800 --> 00:04:06.880]   Okay, we now permission to dish. Oh no, we now permission ish when zoom got in trouble
[00:04:07.360 --> 00:04:11.280]   Because they were claiming end-to-end encryption it turned out it wasn't end-to-end encryption
[00:04:11.280 --> 00:04:17.600]   They said we got a call in the pros from Dover the pro from Dover was Alex Stamos sent up the bat signal
[00:04:17.600 --> 00:04:23.560]   And I have to tell you as a user I was very relieved your reputation precedes you yep
[00:04:23.560 --> 00:04:26.080]   and
[00:04:26.080 --> 00:04:29.320]   Currently you are teaching at Stanford, which is kind of cool in a fellow
[00:04:29.320 --> 00:04:34.240]   Tell us about what what this is the internet observatory. What is that? Yeah?
[00:04:34.240 --> 00:04:41.440]   So that's a cross disciplinary program that I'm the director of we're part of the cyber policy center and Stanford Law School
[00:04:41.440 --> 00:04:42.240]   and
[00:04:42.240 --> 00:04:47.640]   We look at abuse of the internet that causes harm outside of core cybersecurity, right?
[00:04:47.640 --> 00:04:53.400]   So core cyber information security belongs to computer science. It already has a home and academia
[00:04:53.400 --> 00:04:56.160]   What we're trying to do is to create a home for trust and safety, right?
[00:04:56.160 --> 00:04:59.320]   So studying hate speech the use of the internet to
[00:04:59.320 --> 00:05:01.840]   support terrorism
[00:05:01.840 --> 00:05:06.000]   Bullying and harassment child exploitation. We do a lot of work in that area research
[00:05:06.000 --> 00:05:09.520]   We teach a bunch of classes have a book coming out a textbook on trust and safety
[00:05:09.520 --> 00:05:16.480]   So that's kind of stuff what we do there trust and safety is a phrase you hear a lot nowadays. Yeah, most particularly
[00:05:16.480 --> 00:05:23.520]   Attached the name of you all Roth. Yes, who is the chief of trust of safety at Twitter when Elon bought it
[00:05:23.520 --> 00:05:29.360]   You all tweeted extensively about don't worry everything's gonna be fine. What's fired? Yeah?
[00:05:29.680 --> 00:05:33.000]   What is it trusted say for going to do all to you all was fired?
[00:05:33.000 --> 00:05:35.480]   I think he will you'll you'll walked out because
[00:05:35.480 --> 00:05:42.840]   He said there's no point in having a trust and safety officer if one man is making all the decisions, right?
[00:05:42.840 --> 00:05:48.120]   Yeah, yeah, cuz usually trust and safety you have policy people who think about what are the rules?
[00:05:48.120 --> 00:05:53.960]   We want right like let's define a basic rule around hate speech and then something like hate speech turns out to be incredibly complicated
[00:05:53.960 --> 00:05:59.600]   To figure out just in English in the United States what hate speech is you can't just set up a list of keywords that you're
[00:05:59.600 --> 00:06:01.800]   Looking for no no, that's like that when I teach my class
[00:06:01.800 --> 00:06:07.760]   We talk exactly about people who have tried that and what happens when you're like these are just racial slurs that you aren't allowed to
[00:06:07.760 --> 00:06:10.760]   Say the n-word. Oh, that's easy. We'll just get rid of all those right, right?
[00:06:10.760 --> 00:06:17.640]   And then every rap lyric and you know half you know half the movies, you know, but also stuff you can't the people who troll are very
[00:06:17.640 --> 00:06:20.520]   subtle sometimes yes sophisticated right and
[00:06:20.520 --> 00:06:25.000]   They can say things like Hunter Biden's laptop in 12 different ways, right?
[00:06:25.000 --> 00:06:30.720]   And suddenly you've got a problem right trust and safety is a naturally adversarial process you can't just come up with some kind of static rule
[00:06:30.720 --> 00:06:37.960]   You are playing chess against people who sometimes it's the emergent behavior of millions of people billions of people
[00:06:37.960 --> 00:06:42.600]   So sometimes it's intentional necessarily right and sometimes it isn't intentional right some in some of these abuse types
[00:06:42.600 --> 00:06:48.120]   You're talking about professional who are making money or trying to cause harm or manipulate the political sphere
[00:06:48.120 --> 00:06:52.320]   And sometimes it's an individual who's just a troll and sometimes it's a government
[00:06:52.320 --> 00:06:56.200]   Yeah, and that's something our work. We do a lot of it's Stanford IO
[00:06:56.200 --> 00:07:00.520]   Is we write reports on government influence operations?
[00:07:00.520 --> 00:07:04.840]   We do a lot of work to try to understand how is the Russian government manipulating the Internet?
[00:07:04.840 --> 00:07:06.360]   How's the Indian government this year?
[00:07:06.360 --> 00:07:08.080]   We won't wrote one about the US government
[00:07:08.080 --> 00:07:14.200]   We were part of analyzing a takedown on both Twitter and Facebook that turned out to be a five-year campaign
[00:07:14.200 --> 00:07:20.360]   Targeted at Iran and other places in the Middle East in North Africa that was paid for by the US Department of Defense
[00:07:20.520 --> 00:07:23.840]   And so that triggered a series of how the watching post and
[00:07:23.840 --> 00:07:31.680]   Apparently from what I've heard a now review at the top levels of the Pentagon of like what our policy should be as a government about doing Russian style
[00:07:31.680 --> 00:07:34.880]   Influence operations. I'm sure the one argument is what we got to fight fire with fire
[00:07:34.880 --> 00:07:41.840]   I'm sure that's the argument they made unfortunately they were fighting a forced fire with a match because they really sucked at it
[00:07:41.840 --> 00:07:44.640]   Right? This is one of the outcomes of looking at this is one
[00:07:44.640 --> 00:07:48.840]   It's just from my perspective ethically wrong for for a democracy like the US
[00:07:48.840 --> 00:07:52.760]   We should not engage in that behavior right like creating fake Iranians and fake pressure
[00:07:52.760 --> 00:07:58.000]   It's like torture. It's like so many other things that others do doesn't mean we should do it right just as an American
[00:07:58.000 --> 00:07:58.560]   I don't think it's right
[00:07:58.560 --> 00:08:03.440]   But if I didn't care about that I would also just say it was stupid because it turns out our tweet
[00:08:03.440 --> 00:08:09.880]   Announcing our report ended up getting more engagement than anything else that the US government paid for their period times
[00:08:09.880 --> 00:08:15.920]   So somebody made some government contractor and we can't I can't say who I think it is because we haven't totally proved who it is
[00:08:16.040 --> 00:08:20.840]   There are some contracts like the DoD database you could try to pull you know figure it all out
[00:08:20.840 --> 00:08:27.800]   But that contractor made tens of millions of dollars to do worse than we could do with like a couple of undergrads
[00:08:27.800 --> 00:08:33.720]   If that was what we were trying to do. Elon has taken a call you seeing it sciops. Yes, that's just his new thing
[00:08:33.720 --> 00:08:37.960]   Yeah, I mean he's using it in the kind of red pill troll way of
[00:08:37.960 --> 00:08:41.400]   Anytime that somebody says something he disagrees with it's a sciob
[00:08:42.560 --> 00:08:49.040]   But yeah, I mean sciob you could use that term we use influence operation is the term we use because you know fake news is a
[00:08:49.040 --> 00:08:52.920]   Term that got taken over immediately by president Trump and met nothing anymore
[00:08:52.920 --> 00:08:58.880]   That's by the way a smart adversarial maneuver if you don't like something you can co-opt that phrase and make it meaningless
[00:08:58.880 --> 00:09:02.120]   Yeah, absolutely. Yeah, no, he took that and owned it, right? Yeah, um and
[00:09:02.120 --> 00:09:09.000]   It's not really this disinformation misinformation are based upon something being false and a lot of this stuff is not necessarily
[00:09:09.360 --> 00:09:15.760]   Factually false a lot of it's not falsifiable, right? Like even if you look at Russian behavior in 2016 on Twitter and Facebook
[00:09:15.760 --> 00:09:22.600]   A lot of the stuff they were saying were were kind of radical political statements that don't have a falsifiable fact in them, right?
[00:09:22.600 --> 00:09:26.720]   So we have too many immigrants and that's ruining America is that true or false?
[00:09:26.720 --> 00:09:33.320]   We all have opinions on that but you can't really have a fact check right on that and that's the kind of stuff that often these propaganda outlets push
[00:09:33.320 --> 00:09:35.680]   Is that the flooding the zone thing that we talk about?
[00:09:35.680 --> 00:09:41.840]   Yeah, and definitely that's part of the goal for these folks is to try to create the idea that you might have this radical position
[00:09:41.840 --> 00:09:44.840]   Or you're trying to push a radical position in a democratic adversary
[00:09:44.840 --> 00:09:46.920]   You want to make it look like the position is?
[00:09:46.920 --> 00:09:51.840]   Supported by a huge number of people when it's actually a minor minority, right?
[00:09:51.840 --> 00:09:56.120]   But you can try to create all that noise and intimidate people who have more
[00:09:56.120 --> 00:10:01.360]   Mainstream opinions to perhaps push themselves to the outside because it's what the crowd is doing
[00:10:01.360 --> 00:10:08.440]   It's also trivially easy because we're naturally built to well, I see it all over the place. It must be true. Everybody's saying it. Yeah
[00:10:08.440 --> 00:10:16.160]   Yeah, yeah, I mean one of the reasons influence operations work is it plays on some really natural human behavior
[00:10:16.160 --> 00:10:22.200]   Yeah, we we seek out information that makes us feel good about ourselves that makes us feel that we're right that we're part of the good group
[00:10:22.200 --> 00:10:26.360]   The firms are beliefs. Yeah, and so that's like a natural thing that human beings have always done
[00:10:26.360 --> 00:10:30.800]   The internet just makes it so much easier, right? Like we're not in the era where
[00:10:31.080 --> 00:10:39.200]   You know your prototypical mad men, you know the the man comes home from work and puts his 50s hat up on the coat rack
[00:10:39.200 --> 00:10:42.720]   It has like a Wall Street Journal and you know a New York Times
[00:10:42.720 --> 00:10:45.600]   And then can tell that's you and me Jeff by the way
[00:10:45.600 --> 00:10:50.080]   Walter Cronkite and then two other stations now. I know it's the truth
[00:10:50.080 --> 00:10:53.240]   Yeah, right. It's like your choices were for five or six original outlets
[00:10:53.240 --> 00:11:00.280]   Yeah, and now the fact that you have ten thousand different outlets you can choose from you can self select yourself into those kinds of should be good
[00:11:00.280 --> 00:11:02.680]   To I mean Jeff's often argued against the Eli Paris or
[00:11:02.680 --> 00:11:08.600]   Filter bubble argument saying but we do have all these other sources. We are exposed to other information
[00:11:08.600 --> 00:11:14.720]   Right and there is there is real empirical evidence that people are exposed to way more information now than they were in those areas
[00:11:14.720 --> 00:11:17.040]   And that's right could be as beneficial as right
[00:11:17.040 --> 00:11:23.600]   So there there are certain Harvard professors that Jeff has mentioned multiple times on this show who are completely and totally incorrect in their books
[00:11:23.600 --> 00:11:29.320]   And you can tell that there's a whole there's a whole strain of kind of academic tech criticism
[00:11:29.320 --> 00:11:31.400]   That's not at all empirical. It's not based upon any evidence
[00:11:31.400 --> 00:11:36.040]   It's just based upon what and it's a great example because they are believing the things they've been told
[00:11:36.040 --> 00:11:42.680]   Yes, make them feel like this is a good guys. Yeah, so it's yeah, I just read a great paper Alex by Peter Torne turnbag
[00:11:42.680 --> 00:11:51.320]   Okay, they read a couple of them talking about sorting as as the way to look at this that that that it's not about disagreement
[00:11:51.320 --> 00:11:56.760]   It's about identity and we put ourselves into a certain identity and then it leeches out from politics
[00:11:57.000 --> 00:12:03.440]   Into the rest of life. What car you drive how you drink your coffee everything else. Yeah becomes a way to sort yourself in society
[00:12:03.440 --> 00:12:10.160]   And that's not really filter bubbling. It's not about information and disinformation. It's about
[00:12:10.160 --> 00:12:17.120]   Identity is not only how we think of ourselves tribalism. It's how advertisers think about us
[00:12:17.120 --> 00:12:22.960]   I remember 30 20 30 years ago a company called Clara Tuss coming in and doing an interview guys saying well
[00:12:22.960 --> 00:12:26.280]   Tell me your zip code. I'll tell you what kind of car you drive what magazines you subscribe to
[00:12:26.280 --> 00:12:28.800]   We started a little bit. Yeah
[00:12:28.800 --> 00:12:36.600]   There was a recent act apparently operation on Twitter from the Chinese government right to hide
[00:12:36.600 --> 00:12:42.560]   Both city names and hashtags and accounts
[00:12:42.560 --> 00:12:49.920]   Talking about the protests going on in China by flooding those with pornography. Yeah, so we're looking into this right now
[00:12:49.920 --> 00:12:51.520]   So I don't want to scoop my team too much
[00:12:51.520 --> 00:12:57.120]   But I'll give a little preview which is there seems to be a spam operation that we can't attribute to the Chinese government
[00:12:57.120 --> 00:13:00.900]   Things are just gone bad at Twitter. So the truth is is the number people
[00:13:00.900 --> 00:13:08.160]   There is falling apart. Yeah, effectively the entire team that worked on government influence operations has gone
[00:13:08.160 --> 00:13:11.960]   It is evaporated. There's a two people left on the team that do child safety
[00:13:11.960 --> 00:13:18.600]   It's just the trust and safety teams there have been decimated first by the layoffs and then by they're naming people quitting because
[00:13:18.760 --> 00:13:22.680]   You know just like with you all if you work in trust to say if you're somebody who works in child safety
[00:13:22.680 --> 00:13:26.880]   You have to expose yourself to the worst of humanity every single day
[00:13:26.880 --> 00:13:29.280]   Yeah, you're clearly a person who doesn't do that for money
[00:13:29.280 --> 00:13:34.480]   You are mission driven and if you feel the person who sits at the top does not share your mission
[00:13:34.480 --> 00:13:40.440]   You're not gonna last long and that's true for all these folks who work on hate speech who work on our government influence ops
[00:13:40.440 --> 00:13:45.800]   They just don't believe Elon's got their back and so they've quit and so so effectively a zero percent
[00:13:46.360 --> 00:13:49.440]   Unemployment rate for trust and safety professionals right now, right?
[00:13:49.440 --> 00:13:56.080]   Everybody has this problem. That's the other thing that people haven't really thought about too much is we have this huge long tail of new social media
[00:13:56.080 --> 00:14:02.440]   Companies and they all have the same kind of issues except they don't get to grow up with the bad guys like Facebook did right
[00:14:02.440 --> 00:14:07.400]   They have to deal with the people who have been cutting their teeth against Facebook for 15 years
[00:14:07.400 --> 00:14:12.360]   Yeah, and so all the tricks right and so like if you're if you're working at Twitter if you've seen the worst of the worst
[00:14:12.440 --> 00:14:17.160]   You're gonna have 10 job offers by the end of the week if you end up quitting so anyway
[00:14:17.160 --> 00:14:21.800]   So what's going on in China? There is a massive spam issue that is involved with cities
[00:14:21.800 --> 00:14:25.920]   There's no evidence that that's actually been driven by the Chinese government. Oh interesting
[00:14:25.920 --> 00:14:31.440]   It's a hell of a coincidence, but you can we have looked back and have found other
[00:14:31.440 --> 00:14:36.080]   explosions of this kind of spam before it's escort spam mostly and
[00:14:36.080 --> 00:14:42.040]   What they're doing is they're doing photos of women with QR codes you scan the QR code
[00:14:42.040 --> 00:14:48.040]   It does a couple redirects and sends you to a WeChat channel or phone number that you can set to Chinese Chinese citizens
[00:14:48.040 --> 00:14:52.200]   It's intended for Chinese citizens and overseas Chinese now Twitter is blocked for most Chinese citizens
[00:14:52.200 --> 00:14:56.000]   And so it's an interesting place to do yeah, it seems like a be it not productive
[00:14:56.000 --> 00:15:02.600]   There's I mean, there's a decent obviously there's a pretty good sized Chinese diaspora of people who live either in Macau or Hong Kong of
[00:15:02.600 --> 00:15:09.800]   You know Chinese S.A.R.s where they have less of a great firewall or that are you know Mandarin speakers simplified Chinese readers
[00:15:10.120 --> 00:15:14.240]   Who live elsewhere right? So that's going on at the same time
[00:15:14.240 --> 00:15:19.300]   There is a trolling campaign along the lines of what we've seen out of the PRC in the past which is they have a
[00:15:19.300 --> 00:15:27.760]   capability that's referred to as the 50 cent army which is a huge number of patriotic individuals that they're able to mobilize in these situations
[00:15:27.760 --> 00:15:34.440]   Traditionally, so the best paper on this is written from by one of my Stanford colleagues as part of her PhD dissertation Jen Pan
[00:15:35.960 --> 00:15:40.520]   Who you know explore this whole thing and it's changed these amounts since her time
[00:15:40.520 --> 00:15:43.640]   She wrote that because now their ability in English is much improved
[00:15:43.640 --> 00:15:49.120]   And so you've seen that effort for them to invest in not just trolling in a variety of Chinese dialects
[00:15:49.120 --> 00:15:51.600]   But they able to troll in English and so that that is going on
[00:15:51.600 --> 00:15:55.000]   But I think we can't really tie the spam to that
[00:15:55.000 --> 00:16:01.520]   I think the spam just that demonstrates that Twitter is falling apart right that like if you because what happens is if you have a hashtag
[00:16:01.520 --> 00:16:03.920]   That's a big deal the spammers flock to that hashtag
[00:16:04.480 --> 00:16:08.400]   Regardless it's not politics. Yeah, right just follow its conferences. It might even be automated
[00:16:08.400 --> 00:16:12.960]   Right might not even be making decision and so the fact that we're in like day three or four of
[00:16:12.960 --> 00:16:18.400]   If you look up these cities, I was looking at Wuxie like just one Chinese city in Chinese characters
[00:16:18.400 --> 00:16:22.480]   It's completely dominated by the spam like you can't see anything legitimate
[00:16:22.480 --> 00:16:29.560]   Um, and so no matter what it there's a breakdown at Twitter and clearly whether or not it's an intentional thing
[00:16:29.560 --> 00:16:33.760]   Which again, I can't say it's not but I also can't say there's evidence that it is in either way
[00:16:33.760 --> 00:16:40.320]   It demonstrates a real failure there things are kind of really coming off the wheels are coming off with Twitter because they don't have people who can have
[00:16:40.320 --> 00:16:44.720]   Basic control over this kind of stuff. I would I would have to really dominate this interview
[00:16:44.720 --> 00:16:51.840]   So I want to encourage and and and Jeff to to get in this is not triangulation. This is a panel conversation
[00:16:51.840 --> 00:16:57.200]   Fascinated yeah, oh, I but that like I said, we could do 18 hours with
[00:16:57.200 --> 00:17:00.480]   Trapped
[00:17:00.480 --> 00:17:02.480]   There's no bathroom Alex
[00:17:03.440 --> 00:17:08.980]   Too, you know one of the problems with being a police officer is you see the worst of humanity and pretty much collars your perception
[00:17:08.980 --> 00:17:10.980]   Yeah, of the world. Does that happen to you?
[00:17:10.980 --> 00:17:13.680]   Yeah, I mean
[00:17:13.680 --> 00:17:14.960]   I
[00:17:14.960 --> 00:17:21.680]   Have my entire career has been dealing with people to work to cause harm. How did you how did you start of this?
[00:17:21.680 --> 00:17:24.400]   I'm eager for that kind of yeah, um
[00:17:24.400 --> 00:17:29.920]   So I started in the 80s when my Santa Claus brought me a Commodore 64
[00:17:29.920 --> 00:17:33.040]   I was believe I was seven or eight years old and a 300 bod modem
[00:17:33.040 --> 00:17:38.720]   And so I did a lot of stuff as a kid and teenager that for which the statue limitations has run out
[00:17:38.720 --> 00:17:45.360]   You're happy to say I might add yeah, and then but I was fortunate you went to a nice group in Sacramento
[00:17:45.360 --> 00:17:51.920]   the Midwest of California, which means I can both ski and duck hunt right that's how you could tell um and
[00:17:51.920 --> 00:17:57.440]   You know went to like a nice public high school in Sacramento was able to go to Cal
[00:17:58.480 --> 00:18:01.680]   Um, you know did electrical engineering studied under Dave Patterson
[00:18:01.680 --> 00:18:08.000]   Uh, that kind of famous guy um who did some incredible work and then was able to get a career doing legit stuff
[00:18:08.000 --> 00:18:13.760]   So, you know, I had the economic opportunities and the educational opportunities that if I was growing up in Poland or you know
[00:18:13.760 --> 00:18:18.000]   An ex-Soviet state at the same time would not have had you'd be in a ransomware gang. You'd be
[00:18:18.000 --> 00:18:20.000]   I'd be wearing a de-distract suits
[00:18:20.000 --> 00:18:25.360]   I see that so I see that so much and maybe it's your generation
[00:18:25.920 --> 00:18:30.320]   Uh, maybe the younger generation won't be like this, but but you know, I think of people like Kevin mittnik
[00:18:30.320 --> 00:18:34.480]   Who took a career as a hacker and and and made a career as a security professional
[00:18:34.480 --> 00:18:36.480]   Yeah, that seems to be the usual career path
[00:18:36.480 --> 00:18:40.960]   Well, it's a fun one because you can go and actually hack stuff all day and they get paid not go to jail
[00:18:40.960 --> 00:18:44.080]   Right, you can go home at night and have a family and a real life and not live
[00:18:44.080 --> 00:18:48.880]   Yeah, you know that live day-to-day thinking what is it that attracts though your mind to that
[00:18:48.880 --> 00:18:51.040]   uh that that
[00:18:51.040 --> 00:18:55.360]   Penetrating systems. Yeah, I mean it's just fun to break things right like it's it's fun to
[00:18:55.680 --> 00:19:01.520]   It I like doing security both attack and defense like I really enjoy doing instant response when it's not my incident
[00:19:01.520 --> 00:19:04.000]   I really enjoyed as a consultant. It's less fun when you're a sea
[00:19:04.000 --> 00:19:07.840]   So forensics is like you're the guy comes in. There's a murder scene. Right. You got to solve it
[00:19:07.840 --> 00:19:10.960]   Right. Yeah, I mean, I bet if you ask cops they could never say this
[00:19:10.960 --> 00:19:15.680]   Publicly but privately like I kind of like being a murder detective. Yeah, it sucks
[00:19:15.680 --> 00:19:18.240]   Yeah, that somebody died, but like they enjoy the work
[00:19:18.240 --> 00:19:24.320]   So fortunately nobody generally people aren't dying when we're talking about it something bad's happened and you get to investigate and understand and you
[00:19:24.800 --> 00:19:29.360]   Unlike other forms of engineering you have an adversary right so like you build a golden gate bridge
[00:19:29.360 --> 00:19:34.400]   Your adversary is earthquakes and corrosion and wind right non
[00:19:34.400 --> 00:19:39.520]   Intelligent things right that you figure out how and you build the bridge and they're like, okay. We're done
[00:19:39.520 --> 00:19:43.360]   Right you're you never build a bridge in security. You're playing chess
[00:19:43.360 --> 00:19:48.000]   And you whatever you do you can't just read a bunch of books and play the perfect chess game
[00:19:48.000 --> 00:19:52.800]   Your opponents always get better. And so I always found that from a attacker and defender perspective
[00:19:52.800 --> 00:19:55.520]   It's a lot of fun to be planned against real people. That's how
[00:19:55.520 --> 00:20:02.000]   In the cuckoo's egg. Yeah, that was what that whole book was about. We talked to bill cheswick same thing
[00:20:02.000 --> 00:20:08.960]   He he was just an innocent, you know engineer who had to solve a problem built, you know, the first honey pot and uh,
[00:20:08.960 --> 00:20:13.280]   That's how you get into it. Yeah, but I bet because the human mind loves solving problems
[00:20:13.280 --> 00:20:18.000]   These are some of the best most interesting problems because it's a human adversary right right?
[00:20:18.000 --> 00:20:24.960]   Right. I mean if we were growing up in classical roam, you'd have to be a philosopher or like, you know, solve math or you know
[00:20:24.960 --> 00:20:31.840]   A philosopher whatever they called scientists and so yeah, but if you grew up in the latter 20th early 21st century and you want to
[00:20:31.840 --> 00:20:38.640]   Pull things apart and figure out how they work hacking is a pretty good way. These days is the cure also hacking into the human brain. Yes, right?
[00:20:38.640 --> 00:20:41.920]   I want a psychology
[00:20:41.920 --> 00:20:43.120]   Matt cuts
[00:20:43.120 --> 00:20:43.840]   um
[00:20:43.840 --> 00:20:46.480]   Our friend Matt cuts dealing with you know, spam
[00:20:46.800 --> 00:20:52.000]   You think he'd be just a growly nasty miserable person. He's the nicest person. I practically know
[00:20:52.000 --> 00:20:54.400]   Um, and how does it affect?
[00:20:54.400 --> 00:21:01.200]   Because because you're your both the engineering part of it, but also the human part of it right and seeing this part of humanity
[00:21:01.200 --> 00:21:04.960]   Always, you know trying to be a step ahead of you. Is it is it?
[00:21:04.960 --> 00:21:10.000]   Does it ever get you down? Yeah, well, so I mean that's the the change I made in my career
[00:21:10.000 --> 00:21:14.480]   So when I joined before I joined yahoo, I was just pure info sec, right?
[00:21:14.480 --> 00:21:18.640]   So I was a professional hacker. I worked as a consultant. I started my own consulting company
[00:21:18.640 --> 00:21:22.080]   We did a lot of work from microsoft. So if you remember those early trustworthy computing days
[00:21:22.080 --> 00:21:24.800]   I spent a bunch of the early days of our marriage
[00:21:24.800 --> 00:21:29.040]   Uh in several years up in redmond, uh breaking microsoft products
[00:21:29.040 --> 00:21:30.400]   Um
[00:21:30.400 --> 00:21:34.720]   And that's what I did. Yeah. Yeah. Yeah. Yeah, and so you know I did kind of just pure info sec, right?
[00:21:34.720 --> 00:21:38.560]   Which is about the programs. It's about the bugs. It's about the computers
[00:21:38.560 --> 00:21:43.040]   Um, and then when I when I joined yahoo, obviously that was a huge part of my job
[00:21:43.360 --> 00:21:46.880]   A huge part of is in any executive position is just human management, right? Like
[00:21:46.880 --> 00:21:52.400]   One of the challenges with yahoo is it it was a very important company that was dying
[00:21:52.400 --> 00:21:56.080]   Right. Um and by the time I got there, they hadn't had a seesill for about 18 months
[00:21:56.080 --> 00:21:58.800]   They'd already and we want to make this clear because I misstated this last week
[00:21:58.800 --> 00:22:02.880]   They'd already had that billion user breach. That's right. That was in the history. That was in the past
[00:22:02.880 --> 00:22:05.520]   We did have a breach. You had to clean it up. You had to clean it up
[00:22:05.520 --> 00:22:11.840]   Right. So that billion user breach was something that we've we figured out was likely but we never figured out the the root cause
[00:22:12.080 --> 00:22:15.200]   Right partially because yahoo didn't have any logging at all effectively
[00:22:15.200 --> 00:22:20.480]   We have figured out because when I got there the number one security problem was there's a massive account takeover issue
[00:22:20.480 --> 00:22:23.600]   Yes, and so we did. I remember that. Yeah, and people people
[00:22:23.600 --> 00:22:27.040]   People have the time I'm counting and taking over all the time and then spammed. Yeah, and
[00:22:27.040 --> 00:22:34.560]   It seemed to me just from all my consulting work. This is totally statistically improbable compared to every people would call the radio show all the time
[00:22:34.560 --> 00:22:39.760]   My yahoo emails have taken over. They're sending out spam asking for money. Yeah, how did this happen?
[00:22:40.000 --> 00:22:42.160]   Right and so might I asked my team
[00:22:42.160 --> 00:22:46.160]   Hey, I want you to do a statistical analysis of these accounts and I gave them a bunch of cuts
[00:22:46.160 --> 00:22:51.360]   I wanted them to take and we had this young lady who was an intern at the time is now a lawyer
[00:22:51.360 --> 00:22:55.200]   She went to law school and she does great work and I I got to run into her professionally
[00:22:55.200 --> 00:22:56.160]   It's just really cool
[00:22:56.160 --> 00:22:59.280]   um and she did all this data science of just cutting all the data and
[00:22:59.280 --> 00:23:03.600]   One of the most interesting graphs was if you looked at the the date of
[00:23:03.600 --> 00:23:06.720]   Last password change for the accounts that were taken over
[00:23:07.360 --> 00:23:09.760]   It was like this and then there's a super drop-off
[00:23:09.760 --> 00:23:13.440]   Yeah, and when you look at that graph that drop-off happened at the exact month
[00:23:13.440 --> 00:23:18.720]   The resolution was only to the month so we couldn't do to the day the exact month that
[00:23:18.720 --> 00:23:22.960]   Yahoo had changed the way that passwords were stored
[00:23:22.960 --> 00:23:28.720]   And so that was a pretty clear indication that there had been a breach of that the older system
[00:23:28.720 --> 00:23:32.000]   And this has happened before like over a year before I had gone to yahoo
[00:23:32.000 --> 00:23:36.080]   That there was a change in how passwords were stored and secured that when that happened
[00:23:36.080 --> 00:23:39.520]   They had closed the vulnerability that somebody had used now
[00:23:39.520 --> 00:23:42.320]   We didn't have actual logs of how the data was stolen
[00:23:42.320 --> 00:23:47.280]   We didn't have direct evidence and so when we took this to marissa and the other executive team from their perspective
[00:23:47.280 --> 00:23:50.800]   It was like can you prove it and my answer is no it's just highly suggestive evidence
[00:23:50.800 --> 00:23:52.240]   And from their perspective they're like, okay
[00:23:52.240 --> 00:23:56.640]   Well, we're not going to disclose anything because you can't prove anything right and at that time
[00:23:56.640 --> 00:24:00.160]   You know marissa if you remember marissa took over because of an activist investor
[00:24:00.160 --> 00:24:04.720]   By the time I joined it was like a year and a half after she'd been there the honeymoon was over
[00:24:04.880 --> 00:24:07.920]   She had her own activist investor who was now pushing them to make money
[00:24:07.920 --> 00:24:10.400]   She didn't want any kind of negative publicity at all
[00:24:10.400 --> 00:24:16.720]   Um, and so my relationship with her was mostly just her saying can you absolutely totally prove something is true
[00:24:16.720 --> 00:24:18.800]   You're always the bad news guy Alex
[00:24:18.800 --> 00:24:23.200]   Yeah, I mean that that's what talks about being a seesaw right like I would literally walk into a room
[00:24:23.200 --> 00:24:25.280]   And either yahoo or facebook if people get oh
[00:24:25.280 --> 00:24:27.680]   Stomachis here. Yeah. Yeah. Can I say that?
[00:24:27.680 --> 00:24:30.720]   No, no, no, we'll just bleep it. Okay, right
[00:24:31.440 --> 00:24:35.440]   And I I don't think it was because of my personality or how I smelled or something
[00:24:35.440 --> 00:24:38.080]   I think this is I was the grim reaper right like if I was in the room
[00:24:38.080 --> 00:24:43.280]   Something bad at that we've actually been talking about this. What is the liability of a seesaw?
[00:24:43.280 --> 00:24:48.960]   And why seesaws might leave and I think it was probably on a twit the conclusion was
[00:24:48.960 --> 00:24:54.240]   Well, the seesaw's not liable the seesaw does have to bring this information to the board
[00:24:54.240 --> 00:24:57.120]   Does have to bring this information to the sea level right?
[00:24:57.120 --> 00:24:59.120]   But if they don't act if they don't budget
[00:24:59.360 --> 00:25:03.520]   Are you liable? Well, that's your position the u.s. Attorney's office for the northern district
[00:25:03.520 --> 00:25:06.320]   Yes, cornea has a slightly different opinion because this was remember
[00:25:06.320 --> 00:25:12.160]   This was at the very first thing that happened when elan took over twitter was that memo went out from
[00:25:12.160 --> 00:25:15.760]   the council saying
[00:25:15.760 --> 00:25:20.480]   You're on your own now with this ftc consent to right because after like a week I think
[00:25:20.480 --> 00:25:24.240]   The same day the day that they were supposed to write a letter to the ftc
[00:25:24.720 --> 00:25:30.240]   The chief information security officer the chief compliance officer and chief privacy officer all resigned as they signed that letter
[00:25:30.240 --> 00:25:34.080]   And they asked the engineers to self-certify right which is not a thing i'm just gonna say
[00:25:34.080 --> 00:25:39.440]   Self-certifying is not they like that just in it. It's made up. That's just made up
[00:25:39.440 --> 00:25:40.080]   Yeah
[00:25:40.080 --> 00:25:45.200]   No, and and because the other thing that's going on that is in the back of the mind of those three sea levels of which
[00:25:45.200 --> 00:25:47.680]   I have to say two of them are friends of mine. They don't want liability
[00:25:47.680 --> 00:25:50.960]   Right is that you know, have you talked about the joe solven situation at all?
[00:25:51.040 --> 00:25:54.720]   Go ahead. Tell us. Yeah, so joe solven who's my predecessor at facebook
[00:25:54.720 --> 00:25:58.000]   He's the reason I got the facebook job which i'll never forgive him for right?
[00:25:58.000 --> 00:26:01.040]   He took me out to lunch. He asked me he was leaving facebook to go to uber
[00:26:01.040 --> 00:26:07.520]   He asked me to go to uber with him. I had met travis on a boat. Uh, I have a Travis story. That's not for family
[00:26:07.520 --> 00:26:11.920]   Podcast sometimes you get a feeling about a guy right but like you could tell
[00:26:11.920 --> 00:26:18.000]   Just is like I do not want this guy to either meet my sister or be my boss right?
[00:26:18.960 --> 00:26:23.120]   And so I was like, yo, I'm not interested in working with travis. He said well, then you should take my job at facebook
[00:26:23.120 --> 00:26:26.480]   Right. I'll put your nice. Oh, well. Yeah, nice. Not now
[00:26:26.480 --> 00:26:29.360]   Thanks, joe at the time. It seemed like a good idea
[00:26:29.360 --> 00:26:35.440]   It seemed very I was very flattered at the rangoon ruby in paul alto. I was like, oh
[00:26:35.440 --> 00:26:37.920]   Joe, I can't believe you think of me
[00:26:37.920 --> 00:26:41.280]   And then i was like, oh, look it's a bus. It's a horrible drippy bus
[00:26:41.280 --> 00:26:44.160]   Of internal corporate politics
[00:26:44.160 --> 00:26:48.240]   Um, so joe went to uber and was uber there was a security breach
[00:26:48.480 --> 00:26:51.760]   There was guys who who also are bug bounty guys
[00:26:51.760 --> 00:26:54.400]   So the these guys who participated in bug bounties
[00:26:54.400 --> 00:27:00.080]   But in this case they kind of went well beyond the rules of what uber or any other company allows in a bug bounty
[00:27:00.080 --> 00:27:06.880]   Actually grabbed user data and then sent it to them demanding a bounty right and this comes back to
[00:27:06.880 --> 00:27:10.000]   like the real problem with bug bounties is that
[00:27:10.000 --> 00:27:14.880]   Bug bounties are a really good invention the idea that big companies are going to pay hackers
[00:27:15.360 --> 00:27:18.640]   To find bugs and he's going to pay them and then get the bug fixed. It's good for everybody
[00:27:18.640 --> 00:27:21.040]   They're also a form of
[00:27:21.040 --> 00:27:27.200]   Legalized extortion right the whole thing is an extortionate dance between an unintended consequence of it
[00:27:27.200 --> 00:27:30.400]   Yeah, right of of the company's not going to call the fbi on you
[00:27:30.400 --> 00:27:34.480]   Right and have your door kicked in and they're not going to embarrass you publicly
[00:27:34.480 --> 00:27:38.320]   And you'll fix the bug together and then move on and everybody's happy right?
[00:27:38.320 --> 00:27:39.360]   um
[00:27:39.360 --> 00:27:43.040]   And uh, joe made a decision, which is probably not the decision i would made
[00:27:43.280 --> 00:27:46.960]   But he called it a bug bounty he paid them the money
[00:27:46.960 --> 00:27:50.160]   He sent somebody over to have them signed an nda
[00:27:50.160 --> 00:27:53.200]   Had them delete the data and then didn't disclose anything
[00:27:53.200 --> 00:27:55.200]   We did talk about it. Oh, I remember this again
[00:27:55.200 --> 00:28:02.800]   And he was just convicted of two things one misprison of a felony, which is not reporting the initial crime
[00:28:02.800 --> 00:28:04.480]   to the government
[00:28:04.480 --> 00:28:11.040]   um and line to the ftc because at the time the uber was already under an ftc audit and if he was literally in his office
[00:28:11.200 --> 00:28:16.240]   So a cso can be liable in this case. He is going to go to jail. It looks like oh, yeah, um now I
[00:28:16.240 --> 00:28:19.120]   I'm not neutral here. I've known joe for a long time
[00:28:19.120 --> 00:28:24.160]   I'm going to be writing a letter to the judge of asking for for leniency here partially because
[00:28:24.160 --> 00:28:30.560]   If you look at like the top 10 people in the united states who have put child molesters in jail joe selvin's probably on that list
[00:28:30.560 --> 00:28:36.640]   Just because he helped build during the early early days of facebook this incredible capability
[00:28:36.640 --> 00:28:38.800]   Which makes facebook still
[00:28:38.800 --> 00:28:41.920]   Reports 90 of the child they all come from facebook
[00:28:41.920 --> 00:28:45.600]   Yeah, right it's about 90 of the overall reports globally that go to make
[00:28:45.600 --> 00:28:47.680]   We noted that
[00:28:47.680 --> 00:28:50.160]   Remarkable a lot of that is due to him. Yeah, and the work he did
[00:28:50.160 --> 00:28:52.640]   There's other great people in tigney davis a lot of great people
[00:28:52.640 --> 00:28:58.000]   But he needs to serve a lot of credit for that and she's like you got to offset him making a mistake with the ftc
[00:28:58.000 --> 00:29:02.080]   With also the fact that any person he goes to is going to be full of people who he put there
[00:29:02.080 --> 00:29:08.160]   Right in a different one way, but and also this is the dance that happens with bug bounties and that it's kind of an unfortunate
[00:29:08.720 --> 00:29:11.680]   But it probably happens a lot it happens all the time it happened to me multiple times now
[00:29:11.680 --> 00:29:13.680]   Not in a situation where user data was exploited
[00:29:13.680 --> 00:29:18.720]   But we would have bug bounty participants go off the reservation and then effectively blackmail us
[00:29:18.720 --> 00:29:21.280]   um, we had this happen once that I
[00:29:21.280 --> 00:29:24.080]   Instead of
[00:29:24.080 --> 00:29:28.960]   Sky-arrested which is what the legal team wanted right the legal team wanted to call he was canadian
[00:29:28.960 --> 00:29:32.320]   You know the rcmp the royal canadian mountain man
[00:29:32.320 --> 00:29:36.240]   They don't mess with the red outfit stuff, but turns out they have a swatch team
[00:29:36.240 --> 00:29:40.800]   Yeah, and the swatch team is not so nice. They're not oh, yeah, what's this a boot right?
[00:29:40.800 --> 00:29:44.960]   They blow the door in with like a canadian flashbane, right?
[00:29:44.960 --> 00:29:49.280]   And that's what the art and I I was like give me one more chance
[00:29:49.280 --> 00:29:54.080]   To stop this guy from blackmailing us and I called the guy's boss and I'm like hey
[00:29:54.080 --> 00:29:55.920]   He's about to be in real trouble
[00:29:55.920 --> 00:29:59.840]   Somebody he trusts needs to talk him out of it and he talked him down and then like six months later
[00:29:59.840 --> 00:30:02.640]   A story was written about me threatening his boss. Oh dear
[00:30:02.640 --> 00:30:04.640]   So it's like no, it was like kind of no good geek
[00:30:04.640 --> 00:30:09.600]   You know, but say every season runs into this if like I didn't want some canadian teenager to have a
[00:30:09.600 --> 00:30:15.920]   You know federal us felony extradite to the us first computer fraud and abuse act because he did something stupid, right?
[00:30:15.920 --> 00:30:17.200]   Right now joe again
[00:30:17.200 --> 00:30:19.360]   I think the decision he made was not the right one
[00:30:19.360 --> 00:30:22.960]   But I could see how he got there and certainly this
[00:30:22.960 --> 00:30:29.840]   This decision is one made it that nobody wants to be a seesaw right that like if because he was told by travis by the ceo
[00:30:30.800 --> 00:30:38.560]   Don't disclose right and so if you've been told by the ceo and you've received advice from legal counsel that it's okay not to disclose
[00:30:38.560 --> 00:30:45.920]   He's the one going to jail. It feels very unfair. Yeah, because travis is guys billions of dollars and just enjoying himself
[00:30:45.920 --> 00:30:51.440]   Yeah, um and didn't defend joe just threw joe totally under the bus right didn't come to the trial
[00:30:51.440 --> 00:30:53.760]   Didn't help him out in any way
[00:30:53.760 --> 00:30:57.520]   The other thing going on is dara took over uber and so joe is really just a victim of
[00:30:58.080 --> 00:31:02.000]   A setup from inside of uber where dara's team was trying to make travis look as bad as possible
[00:31:02.000 --> 00:31:06.720]   And they're the ones who sent all of the information to the us attorney's office and pushed for the prosecution
[00:31:06.720 --> 00:31:13.120]   So anyway to that context if you're working for you on musk and you're like you want me to sign this ftc letter after you fired
[00:31:13.120 --> 00:31:19.520]   Every team that makes it possible. Mm-hmm. You're insane. Yeah, not gonna do it. Let's take a little break. Hold on a second hold that thought
[00:31:19.520 --> 00:31:25.040]   You're next i'm sorry, but we do need to take a little break this because pay to bills. This is awesome
[00:31:25.040 --> 00:31:27.280]   Alex famous is our guest
[00:31:27.680 --> 00:31:32.560]   From the stanford.io. I like that the internet observatory the stanford.io. We're gonna talk about
[00:31:32.560 --> 00:31:37.760]   His mass is it your master's on instance or you're just a member of of the master's on instance
[00:31:37.760 --> 00:31:39.760]   I run that master's
[00:31:39.760 --> 00:31:45.600]   I'm about why cyber villains. I'll talk about the way you don't want to join cyber villains.com
[00:31:45.600 --> 00:31:51.440]   But we will talk about master's on securities twitter security. Yeah, uh, there's a lot to talk about with Alex tamison
[00:31:51.440 --> 00:31:53.120]   We are thrilled
[00:31:53.120 --> 00:31:55.680]   Beyond measure to have him in studio
[00:31:56.240 --> 00:32:00.080]   Uh, what a great opportunity to kind of get the story from inside
[00:32:00.080 --> 00:32:03.040]   We talk a lot about bug bounties. It seems like on balance
[00:32:03.040 --> 00:32:07.680]   It's a good thing to have because the other option is that these guys go to zerodium or somebody and sell it
[00:32:07.680 --> 00:32:12.640]   Right and then it goes to a nation state, which is then weaponizing against us. Yeah
[00:32:12.640 --> 00:32:18.240]   But I didn't realize it's so fraught and that's uh, I mean you're you're dealing with 20 year olds often
[00:32:18.240 --> 00:32:21.360]   Yeah, and they make no frontal lobe. No frontal lobe. Yeah, right
[00:32:21.360 --> 00:32:24.400]   You know, what did you do as a 20 year old?
[00:32:24.400 --> 00:32:26.400]   Just dangle hot pockets
[00:32:26.400 --> 00:32:31.840]   We'll just take all these other things. I think we put teenagers in these ridiculous where they make a mistake
[00:32:31.840 --> 00:32:36.800]   Instagram at 15. Yeah, like thank god every single person in my social group was not recording everything
[00:32:36.800 --> 00:32:41.280]   Exactly. It's a sad thing. You have kids three. Yeah. Yeah, how old are they?
[00:32:41.280 --> 00:32:46.720]   They are uh 11 year old girl a 13 year old son of 15 year old son. So what do you talk to them about?
[00:32:46.720 --> 00:32:48.720]   What do you tell them?
[00:32:48.720 --> 00:32:53.360]   So one I do every year the speech to the eighth graders at my son's school
[00:32:53.360 --> 00:32:58.640]   Oh boy. Does he love that? Okay. Hang on. I do have to take a break
[00:32:58.640 --> 00:33:00.560]   I want to hear the speech to eighth graders
[00:33:00.560 --> 00:33:00.800]   Wow
[00:33:00.800 --> 00:33:03.680]   Because we got a lot of eighth graders and people were eighth grade in the brain
[00:33:03.680 --> 00:33:09.440]   listening and watching including me and I want to hear the speech to eighth graders Alex Stamos more coming up
[00:33:09.440 --> 00:33:13.520]   Uh jeff Jarvis and prue great to have you in our show today brought to you by
[00:33:14.080 --> 00:33:19.600]   Noreva we're in this weird environment where some of the employees are at work some of the employees are at home
[00:33:19.600 --> 00:33:22.800]   And we have a some go back and forth. We've got this hybrid work environment
[00:33:22.800 --> 00:33:26.400]   And of course that means your huddle room your meeting room is getting a lot of use
[00:33:26.400 --> 00:33:29.360]   You have an all hands meeting half the people are there and half the people aren't
[00:33:29.360 --> 00:33:33.680]   And the biggest problem I could tell you right now. I know because I've lived through it is audio
[00:33:33.680 --> 00:33:35.920]   It's audio
[00:33:35.920 --> 00:33:39.120]   Uh, you can go out you can and a lot of companies do this they spend
[00:33:39.120 --> 00:33:43.440]   Uh, I kind of an a stunning amount of money tens of thousands of dollars
[00:33:44.000 --> 00:33:47.920]   To put in very elaborate video and audio conferencing systems
[00:33:47.920 --> 00:33:51.520]   They have to have them tweaked constantly
[00:33:51.520 --> 00:33:54.240]   They've just got wires everywhere microphones everywhere
[00:33:54.240 --> 00:33:59.120]   And then of course you add with that product shortages delays supply chain
[00:33:59.120 --> 00:34:04.800]   The amount of time your it department now is devoting to just getting this huddle room. We're working
[00:34:04.800 --> 00:34:07.280]   It's just not a good solution
[00:34:07.280 --> 00:34:10.240]   Customers want better. They want intelligent products
[00:34:10.960 --> 00:34:14.400]   They require minimal effort from it to deploy and manage its scale
[00:34:14.400 --> 00:34:19.280]   Zero end user training they want nareva and you are eva
[00:34:19.280 --> 00:34:25.440]   Nareva has been around for a while, but boy, this is their time their patented microphone mist technology
[00:34:25.440 --> 00:34:32.640]   Gives you great audio without any of the wires the microphones without any of the tweaking
[00:34:32.640 --> 00:34:35.600]   zero end user training
[00:34:35.600 --> 00:34:38.880]   And and it works for small and large spaces
[00:34:39.840 --> 00:34:43.360]   It's basically a sound bar looks like a sound bar
[00:34:43.360 --> 00:34:48.720]   But it gives you true full room mic pickup just one or two microphone and speaker bars
[00:34:48.720 --> 00:34:52.560]   You can install if you can install a sound bar. You can install this yourself in half an hour
[00:34:52.560 --> 00:34:56.080]   Maybe you got a big space you're gonna put it to oh, it's gonna take you an hour
[00:34:56.080 --> 00:35:02.000]   You don't even have to get it involved or if you're an it you'll love it because the nareva console means you go back to your office
[00:35:02.000 --> 00:35:06.880]   And even if you have many huddle rooms many conference rooms many all hands meetings
[00:35:07.200 --> 00:35:13.680]   You can tweak them set them monitor them turn them on and off all from the console over the web
[00:35:13.680 --> 00:35:20.880]   It's incredible no more complicated maze of med multiple mics and speakers and dsps and switchers
[00:35:20.880 --> 00:35:26.800]   Uh your room won't go offline for days. You can have it installed before this show is over
[00:35:26.800 --> 00:35:33.440]   Some traditional systems require you to go around from room to room and tweak it right and get all this soft software
[00:35:33.760 --> 00:35:38.720]   Nareva lets you monitor manage update and adjust all your nareva systems from a powerful cloud based platform
[00:35:38.720 --> 00:35:42.560]   The nareva console and it scales for large organizations
[00:35:42.560 --> 00:35:45.680]   For a fraction of the cost of these traditional systems
[00:35:45.680 --> 00:35:49.760]   Take a look at this hdl 300 system. This microphone miss technology means
[00:35:49.760 --> 00:35:55.200]   Everybody in the room can be heard clearly no matter where they're facing no matter how they're social distancing
[00:35:55.200 --> 00:35:59.520]   It's it fills the room with virtual mics is a patented technology. That's incredible
[00:36:01.280 --> 00:36:07.600]   Right now 50% off a nareva hdl 300 system for mid-sized rooms when you get a live online demo
[00:36:07.600 --> 00:36:14.960]   As long as you buy before december 16th 2022 go to nareva and ureva.com/twit
[00:36:14.960 --> 00:36:17.360]   The 300 is the perfect mid-sized system
[00:36:17.360 --> 00:36:25.680]   They're bigger systems. They're little systems, but but I think this is for most conference rooms. This is exactly what you need nureva.com
[00:36:25.680 --> 00:36:28.800]   to it great audio simplified
[00:36:30.000 --> 00:36:32.880]   With nareva. Thank you nareva for supporting this week in tech
[00:36:32.880 --> 00:36:37.920]   You know, we're all about good audio. That's why we make people sit in front of big honka big microphones
[00:36:37.920 --> 00:36:44.880]   Uh, that's that's one of the things it's amazing nareva.com/twit. Okay, you're on a mac
[00:36:44.880 --> 00:36:49.520]   Yeah, Alex stamos not a chrome book not a windows machine
[00:36:49.520 --> 00:36:52.720]   Just have to notice that
[00:36:52.720 --> 00:36:58.640]   Really run doctor containers on my chrome book. So it's all in docker, huh? Yeah a bunch of stuff. Yeah. Yeah, nice
[00:36:59.440 --> 00:37:02.560]   Alex stamos is a guest. I interrupted you aunt. Did you have a question for Alex?
[00:37:02.560 --> 00:37:07.520]   I want to hear his story first because I can piggyback off of his the eighth grade lecture. Yeah
[00:37:07.520 --> 00:37:11.200]   What do you tell kids in the middle school years?
[00:37:11.200 --> 00:37:16.560]   No frontal lobe no executive function when you knew you did what you did when you were there
[00:37:16.560 --> 00:37:24.160]   Their every move is recorded on social media and posted on instagram or snapchat or tick-tock right all of their
[00:37:24.160 --> 00:37:29.280]   Relationships with their friends are intermediated by technology. Yeah, right. Yeah, there's no more
[00:37:29.440 --> 00:37:30.960]   passing notes
[00:37:30.960 --> 00:37:36.240]   No, and that's one of the permanent changes. I think from covid is covid pushed all of the relationships online
[00:37:36.240 --> 00:37:39.840]   And only 20 percent of it came back to in person like you will see
[00:37:39.840 --> 00:37:44.400]   We'll have a group of 15 year olds in our basement and like we've got a pool table
[00:37:44.400 --> 00:37:50.480]   And who's ball table and they're all sitting there texting each other in the same back room. Yes. I see that
[00:37:50.480 --> 00:37:55.440]   They're sharing a pizza. You have kids. Yes. Yes. I have hard hits and they sit in the same room
[00:37:56.160 --> 00:37:59.920]   And you want to go and text each other in the same room like you said it right there
[00:37:59.920 --> 00:38:05.840]   My kids are 30 and 27. So I dodged this bullet. Yes, the internet was there, but they were using aim
[00:38:05.840 --> 00:38:11.680]   I even thought it weird there she would do my daughter was 30 now would do her homework with music and aim
[00:38:11.680 --> 00:38:16.960]   Yeah, and a video chat going on and all that stuff, but this gone beyond but she didn't have a phone in her pocket
[00:38:16.960 --> 00:38:20.720]   No, it's the camera that puts get some trouble the camera. Yeah, um, so
[00:38:20.720 --> 00:38:23.040]   You know, so when he talked to the kids
[00:38:23.040 --> 00:38:28.320]   there are obviously all kinds of bad things that happen to kids, but the the really horrible tragic outcomes, right the
[00:38:28.320 --> 00:38:32.400]   Um, you know, I'm not gonna name any of the teenagers, but I when I teach my class
[00:38:32.400 --> 00:38:35.360]   I talk about teenagers who actually
[00:38:35.360 --> 00:38:41.840]   Ended up taking their own lives. Well, I was he so facebook right due to them being exploited. Oh my god. Yeah, and
[00:38:41.840 --> 00:38:45.120]   those situations are because
[00:38:45.120 --> 00:38:51.280]   The kids didn't have anybody they could turn to that they felt that they could talk to once they made a mistake
[00:38:51.760 --> 00:38:53.760]   Once they kind of slipped up a little bit
[00:38:53.760 --> 00:39:03.040]   That for them there is no outcome other than to take their lives, right? Um, and uh, and so I think a big thing that I say to the kids is
[00:39:03.040 --> 00:39:08.080]   One of the metaphors I have is I asked the kids. Um, has anybody here ever been really badly hurt?
[00:39:08.080 --> 00:39:14.160]   Um had an accident and one kid raises their hand and like yes, and he says oh, I ran through a plate glass window once
[00:39:14.160 --> 00:39:15.600]   I'm like whoa
[00:39:15.600 --> 00:39:20.720]   Okay, that's a good one right like a glass door. Um, was it scary? He's like, yeah, I was covered in blood
[00:39:20.880 --> 00:39:26.080]   I was bleeding everywhere and the kids are cool. Yeah, um, and I asked him
[00:39:26.080 --> 00:39:29.600]   So when when you did that were your parents angry at you?
[00:39:29.600 --> 00:39:32.800]   No, what were they they were scared?
[00:39:32.800 --> 00:39:38.560]   All right, and that's what you have to teach these kids is like would yes your parents spend all the time being angry
[00:39:38.560 --> 00:39:43.280]   A bit you about not turning your homework or not walking the dog or the you know fighting with your sister
[00:39:43.280 --> 00:39:46.560]   When you're in real trouble your parents want to protect you
[00:39:46.720 --> 00:39:51.680]   Wait, that's so good. That's correct. And you need to go if you get into a if you're talking to somebody online
[00:39:51.680 --> 00:39:54.240]   And you're like I don't think this person is who they say they are
[00:39:54.240 --> 00:39:58.640]   Or if you've done something like sent a photo to somebody and regretted it
[00:39:58.640 --> 00:40:02.160]   It is much better for you to go talk to your parents at that moment
[00:40:02.160 --> 00:40:07.120]   Than to try to fix it yourself because the the worst outcomes are when you try to fix it yourself
[00:40:07.120 --> 00:40:12.800]   And for the the really bad child abusers they know that and so they create a stock home syndrome up
[00:40:13.440 --> 00:40:20.240]   You know a fake relationship where it's you and me against your parents. There's a horrific story just happened in san eros up here
[00:40:20.240 --> 00:40:26.640]   Teenage girl catfished. Yes guy figures out where she lives comes to her house kills her mother and her
[00:40:26.640 --> 00:40:32.560]   Grandparents it abducts her and drives off with her. Well horrific story because she was catfished the
[00:40:32.560 --> 00:40:36.480]   Right, that's like the worst worst outcome is worst outcome in person abuse
[00:40:36.480 --> 00:40:40.640]   They by the way they they they caught them and they she escaped safely. Thank god
[00:40:41.040 --> 00:40:43.680]   And they've got the perpetrator, but her parents
[00:40:43.680 --> 00:40:50.160]   You know mom and her grandparents are gone. Yeah, uh horrific story, right and that's catfishing. That's posing
[00:40:50.160 --> 00:40:52.160]   Right as some of the other
[00:40:52.160 --> 00:40:56.560]   And then the where catfishing often goes where it becomes really harmful
[00:40:56.560 --> 00:41:03.520]   It's called sex torsion where you trick a kid into giving you a naked photo and then you leverage that into more and more control over their behavior
[00:41:03.520 --> 00:41:08.160]   Parent good friend of mine just happened to a 17 year old. Yes, it's extremely common
[00:41:08.480 --> 00:41:11.920]   Um, and that's where you need to teach your kids one
[00:41:11.920 --> 00:41:13.760]   You know, it
[00:41:13.760 --> 00:41:18.640]   You don't want to be victim blaming, but you also want to teach kids like you don't send naked photos to your friends ever ever
[00:41:18.640 --> 00:41:23.360]   It's like you have to teach one you have to teach teenage girls a teenage boys are idiots, right?
[00:41:23.360 --> 00:41:25.600]   Like I don't think they understand how dumb we were
[00:41:25.600 --> 00:41:30.000]   Maybe still are but certainly how dumb I was when I was 15 years old. They have no
[00:41:30.000 --> 00:41:34.080]   Intients voice have no impulse control. No ability and so you have like
[00:41:34.800 --> 00:41:40.640]   these nobody dies, but it's a big deal if you end up with the naked photo being distributed with friends and it's technically
[00:41:40.640 --> 00:41:44.880]   Distribution of child sexual abuse material technically a bunch of those kids are committing a felony
[00:41:44.880 --> 00:41:47.040]   Well, but the next sex torsion could go far worse
[00:41:47.040 --> 00:41:49.120]   Right, but if right that's bad
[00:41:49.120 --> 00:41:53.360]   And then if it goes worse that that person that you shared the image with is an adult who's an extortionist
[00:41:53.360 --> 00:41:57.520]   Then they will turn that into a multi-month hell and so we had this guy
[00:41:57.520 --> 00:42:00.000]   That's where you get suicides. That's where you get suicides
[00:42:00.160 --> 00:42:05.440]   And we had this guy who we called the worst man on facebook, which is like a pretty high bar, right?
[00:42:05.440 --> 00:42:08.000]   and
[00:42:08.000 --> 00:42:12.160]   We spent two years tracking this one guy because his op sec was perfect. He never made a mistake
[00:42:12.160 --> 00:42:16.080]   He never used a real phone number. He never used a real ip. He was on tour all the time
[00:42:16.080 --> 00:42:20.080]   And he had he approached thousands of teenage girls
[00:42:20.080 --> 00:42:25.760]   He it turns out later we find out he was tracking how he was approaching them and what approaches were statistically more
[00:42:25.760 --> 00:42:27.200]   Right successful
[00:42:27.200 --> 00:42:33.360]   And that he would he would usually what I guess was successful was he make them think of like oh, I know you sent a nude to your boyfriend
[00:42:33.360 --> 00:42:35.120]   I was able to hack it
[00:42:35.120 --> 00:42:39.280]   Why don't you send me another one and then this will be over with he doesn't have a real new doesn't have anything
[00:42:39.280 --> 00:42:40.240]   but
[00:42:40.240 --> 00:42:43.120]   4% 5% of the girls might believe it right
[00:42:43.120 --> 00:42:46.720]   And so that's the kind of situation that you need to teach your kids
[00:42:46.720 --> 00:42:51.360]   If you get that message, we're not going to be angry you even if you did send your boyfriend nude
[00:42:51.360 --> 00:42:54.400]   You can come talk to us and we can help you out and help you out of this problem
[00:42:54.640 --> 00:43:00.240]   And instead you talk to parents is good that you're given this to these eighth graders and young kids
[00:43:00.240 --> 00:43:05.360]   But is are you speaking to the parents as well and speaking to just adults in general?
[00:43:05.360 --> 00:43:10.160]   Because a lot of times people are my age and a little bit younger a bunch of know-it-alls and don't know
[00:43:10.160 --> 00:43:15.120]   And we're gonna we're gonna talk to you when I when I just know it all it's just it's just
[00:43:15.120 --> 00:43:17.520]   So I mean I talked to people on the radio
[00:43:17.520 --> 00:43:20.800]   It's also another thing yeah, but I'm talking about the people that are know it all
[00:43:20.800 --> 00:43:25.720]   It's when I try to explain things to people about even just password sanitation
[00:43:25.720 --> 00:43:31.280]   They look at me as if I'm talking on the side of my neck and I'm going overboard over security
[00:43:31.280 --> 00:43:34.320]   What what is your approach? We do talk to parents? I
[00:43:34.320 --> 00:43:38.160]   Part I mean part of what you talk to parents about is create that open line of communication
[00:43:38.160 --> 00:43:42.080]   Also, you have to have the expectation until your kids are out of your house. They have no privacy on their phone
[00:43:42.080 --> 00:43:46.640]   I mean it sucks to say it. Oh, I know that you know that right and so just box shots
[00:43:47.360 --> 00:43:52.800]   And what will happen is is we've had this with our kids where we say only one of them has a full-time phone
[00:43:52.800 --> 00:43:55.120]   Right it's time for a phone check
[00:43:55.120 --> 00:43:57.520]   And they will preemptively admit something now so far
[00:43:57.520 --> 00:44:03.200]   Wow kind of small potatoes. Yeah, right, but that's you want that you plan it to see yeah
[00:44:03.200 --> 00:44:08.800]   You plan the seat and you create a situation where like they now have permission almost to share with you
[00:44:08.800 --> 00:44:13.040]   Right. So yeah, I mean I think for parents part of the problem though is the technical controls parents have suck
[00:44:13.360 --> 00:44:17.920]   Right like the the good thing that apple apple has finally made some progress here
[00:44:17.920 --> 00:44:23.200]   And I think apple is the company that has the most opportunity at least in the us because they dominate, you know teenagers
[00:44:23.200 --> 00:44:26.880]   Right. Um is you know, they now have the ability to classify
[00:44:26.880 --> 00:44:30.080]   Photos as they leave your kids device to see if they're naked photos
[00:44:30.080 --> 00:44:34.080]   And to stop them from leaving the device. This was very controversial when apple proposes
[00:44:34.080 --> 00:44:37.680]   Well, there's two different things. There's a part where they're actually scanning your photos
[00:44:37.680 --> 00:44:38.560]   Yeah
[00:44:38.560 --> 00:44:39.600]   as
[00:44:39.600 --> 00:44:43.280]   On the device, right and I oppose that much so much I wrote a new york times op-ed
[00:44:43.280 --> 00:44:47.440]   I remember so I wrote that part but the same time they announce something I think it's great
[00:44:47.440 --> 00:44:50.960]   Which is you can say this is a kid. They should not send naked photos
[00:44:50.960 --> 00:44:54.560]   This is totally in the hands of parents. This is not going back to the corporate or anything
[00:44:54.560 --> 00:44:58.400]   Right and it never calls the cops. Yep. It never does anything. It doesn't even even tell the parents
[00:44:58.400 --> 00:45:03.520]   Right. It just stops the photo from me. Right and it even warns kids. Are you sure you want to right?
[00:45:03.520 --> 00:45:05.440]   It's good. It's actually I think well done
[00:45:05.440 --> 00:45:09.200]   Right and I think those kinds of interventions parents have to turn that on by the way should mention that
[00:45:09.280 --> 00:45:14.320]   Yeah, you have to turn it on. It's in I think it's under screen time settings and and so I think setting the screen time settings
[00:45:14.320 --> 00:45:18.160]   So the other thing for kids is teenagers want to be on their phones all night. They don't get any sleep
[00:45:18.160 --> 00:45:21.920]   And so having the phone turn off at 9 or 10 p.m. Whatever you do for adults
[00:45:21.920 --> 00:45:25.920]   I'm bad at that
[00:45:25.920 --> 00:45:28.800]   At least I could fix my problems with my kids, right?
[00:45:28.800 --> 00:45:32.240]   You know doing the screen time limits and then having no expectation of privacy
[00:45:32.240 --> 00:45:34.240]   And I'd like to see companies get more aggressive about
[00:45:34.240 --> 00:45:38.160]   Those kinds of limits on hey, it looks like you're sending a naked photo
[00:45:38.160 --> 00:45:41.280]   Is that something you really want to do? Here's some things that you need you should read first
[00:45:41.280 --> 00:45:45.520]   Right before you do that right and to at least like because again, you said no for a lobe
[00:45:45.520 --> 00:45:48.560]   Right like their decision making is just create a friction
[00:45:48.560 --> 00:45:51.760]   And so create that friction that they have to think about it. I think is a good thing
[00:45:51.760 --> 00:45:54.640]   Go ahead. You were at gonna follow. Oh, no, that was it
[00:45:54.640 --> 00:46:01.440]   It was just the whole education side of it far as speaking with adults, but actually now I do have another question
[00:46:01.440 --> 00:46:05.840]   You said nobody really wants to be a CISO anymore
[00:46:06.240 --> 00:46:12.800]   Yeah, nowadays at least what I've been seeing and hearing is colleges are really trying to teach kids into getting into entrepreneurship
[00:46:12.800 --> 00:46:15.360]   And things of that nature
[00:46:15.360 --> 00:46:20.080]   Even from the tech side of things are talking about writing code and developing apps and whatnot
[00:46:20.080 --> 00:46:25.280]   But I don't really see a lot about security out there even though we need security
[00:46:25.280 --> 00:46:29.360]   Is it just a push of telling people to be a consultant?
[00:46:29.360 --> 00:46:33.520]   Or just stay away from being a c-suite and just be a
[00:46:34.320 --> 00:46:40.720]   One-person team and consulting for all of these big banks and big corporations or whatever that's going to need information security
[00:46:40.720 --> 00:46:42.720]   Do you carry your kids to hack?
[00:46:42.720 --> 00:46:47.200]   Um, I haven't taught my kids really to hack anything. Yeah, I mean they haven't showed us much interest
[00:46:47.200 --> 00:46:49.760]   Uh, so that's fine. I always wish my kids wanted to
[00:46:49.760 --> 00:46:50.800]   Yeah
[00:46:50.800 --> 00:46:52.960]   Yeah, if dad does it, you don't want to do it
[00:46:52.960 --> 00:46:56.560]   Yeah, no, you don't want to do it. That's right. Um, yeah, I uh
[00:46:56.560 --> 00:47:03.120]   So I mean the good thing is is that most good computer science departments now have security classes you can optionally take right?
[00:47:03.680 --> 00:47:05.680]   That was not true when I was an undergrad there was no
[00:47:05.680 --> 00:47:11.600]   Undergraduate security class. I'd take a graduate seminar. It wasn't like a soup to nuts. We read papers, right?
[00:47:11.600 --> 00:47:18.160]   And so now you can take those classes thanks to you know, uh, Dave Wagner at Berkeley Dan Bonet at stanford
[00:47:18.160 --> 00:47:22.720]   Uh felt and feltin at Princeton. They created these first classes in the early 2000s
[00:47:22.720 --> 00:47:28.720]   Uh, and now that's pretty widespread. It's I don't know if a single school where it's required to do security and this is something
[00:47:28.720 --> 00:47:31.280]   um that matt bishop
[00:47:31.280 --> 00:47:35.840]   Uh at davis, um has thought thought a lot about is how do we integrate?
[00:47:35.840 --> 00:47:43.840]   Security curriculum into just a standard computer science curriculum. It's just so packed right to everything you have to teach students in in four years
[00:47:43.840 --> 00:47:47.920]   Um, and so what a lot of people have tried is okay. Well, we're teaching them basic programming
[00:47:47.920 --> 00:47:50.560]   We'll also teach them basic security side-by-side
[00:47:50.560 --> 00:47:55.680]   Okay, with programming. So I think that's a reasonable thing. Um, you know, but this is why I teach classes, right?
[00:47:55.680 --> 00:48:00.560]   So I teach a cyber security class for non-cs majors. So I have lawyers. I have MBA students
[00:48:00.640 --> 00:48:05.120]   I have a lot of international policy students to teach them the intro to cyber and to get them actually hacking
[00:48:05.120 --> 00:48:11.200]   Right, um, so they use non programming hacking tools like burp and um, what was it wire? Was it wire guard wire shark?
[00:48:11.200 --> 00:48:16.400]   Yeah, so wire shark is when things they use they get to sniff Wi-Fi networks and stuff and then what's the point of that?
[00:48:16.400 --> 00:48:22.480]   They're not gonna do that in their job point there is that is uh, there's a master's in cyber policy at stanford
[00:48:22.480 --> 00:48:26.800]   And that is the first required class. Um, and this is effectively how I got to stanford is that's interesting
[00:48:26.800 --> 00:48:31.520]   But again, is it still just more about planning the seed and having awareness about what's going on?
[00:48:31.520 --> 00:48:33.600]   Right because those people are a digital standpoint, right?
[00:48:33.600 --> 00:48:36.880]   Yeah, yeah for those folks a lot of them if my whole problem was
[00:48:36.880 --> 00:48:41.200]   Basically, I had this anecdote that I said to some of the professors at stanford was you know
[00:48:41.200 --> 00:48:43.440]   I by first time to the white house
[00:48:43.440 --> 00:48:46.320]   I was there because of this fSB breach of yahoo
[00:48:46.320 --> 00:48:53.280]   And we are briefing them on this investigation and all of the techies were on my side and the entire other side of the white house table
[00:48:53.760 --> 00:48:57.200]   Was lawyers and we couldn't speak the same language. Uh, right?
[00:48:57.200 --> 00:48:59.040]   Right, um, um
[00:48:59.040 --> 00:49:02.240]   And so my thing was certainly policymakers need to have this knowledge
[00:49:02.240 --> 00:49:06.800]   But like if you go to national security council the people who do cyber are either coming from a
[00:49:06.800 --> 00:49:14.160]   specialty in a country like they have a phd in chinese studies or in russian literature and they've been a cold warrior
[00:49:14.160 --> 00:49:17.280]   Which by the way was my major but i dropped it came back i dropped
[00:49:17.280 --> 00:49:22.640]   The russian is a good idea. Yeah, and then like everybody was bored about russia for a while
[00:49:22.640 --> 00:49:26.480]   It's all about learning Arabic came back about terrorism or something russia's back big man. That's right
[00:49:26.480 --> 00:49:33.360]   Wait, you see that actually like stanford like the the old russia like the people who are in like the ragan white house are all super excited again
[00:49:33.360 --> 00:49:43.360]   Actually, i'm a little more worried about china and maybe we should be but uh, but but tans question and then I teach you trust
[00:49:43.360 --> 00:49:47.360]   It's safety classes s students which is about hate speech and bullying harassment and
[00:49:48.000 --> 00:49:54.080]   Child exploitation adult exploitation right because i got kind of tired stanford will graduate these 23 year old dudes are always guys
[00:49:54.080 --> 00:49:56.320]   Who are like man i've got a startup
[00:49:56.320 --> 00:50:01.360]   In this startup you can take a photo and then anonymously send that photo to 100 women great
[00:50:01.360 --> 00:50:06.320]   Thinking buddy right what could possibly go wrong and so my goal is like well if you take the class
[00:50:06.320 --> 00:50:10.480]   You'll know the 17 things that have gone wrong any time anybody is allowed to send a photo
[00:50:10.480 --> 00:50:16.320]   Right interesting and the and the intent with the lawyers is not to get them to be a proficient with wire shark but
[00:50:16.960 --> 00:50:20.160]   To have them understand right like what's going on in the language
[00:50:20.160 --> 00:50:23.920]   Right because when they are now in the white house and so actually one of my students is now
[00:50:23.920 --> 00:50:30.400]   Working at the national security council and when she sees a briefing about you know the lazarus group of the
[00:50:30.400 --> 00:50:33.680]   The dprk of north korea
[00:50:33.680 --> 00:50:36.080]   Uh you seekle injection she'd be like
[00:50:36.080 --> 00:50:42.160]   I know what it is. I did that in stomas's case. Yes, right and so yeah is she gonna be a professional hacker?
[00:50:42.160 --> 00:50:44.240]   No, no, no, is she going to have me
[00:50:44.240 --> 00:50:47.680]   But she can at least communicate to the rest of the team that can help
[00:50:47.680 --> 00:50:51.440]   Which is a real problem in cyber policy right it's like the people who do other national security policy
[00:50:51.440 --> 00:50:56.800]   Often come from the military right like the chairman of the joint chiefs. I don't know who i'm sorry. I forget who it is right now
[00:50:56.800 --> 00:51:00.000]   Is it milli? I think it's milli yeah, he was like a marine platoon commander
[00:51:00.000 --> 00:51:04.400]   Right so it's like the dude was in the dirt with his dudes. You probably still field strip it
[00:51:04.400 --> 00:51:06.800]   M16 right
[00:51:06.800 --> 00:51:11.600]   But like the people who went cyber in the government have never they've never changed a hard drive
[00:51:11.760 --> 00:51:14.400]   Never reinsold their operating system. They never hacked a single thing
[00:51:14.400 --> 00:51:20.160]   Right and that's I think what we have to change is that the people who run cyber for our government that they have to have at some point
[00:51:20.160 --> 00:51:25.280]   Been hands-on itself is it your sense? It's getting better because I think this has been a big problem. Yes government not understanding
[00:51:25.280 --> 00:51:31.280]   the the even the ground that they're walking on it is getting and also legislators
[00:51:31.280 --> 00:51:35.040]   I mean, I guess it's their staff that needs to know this it's not getting better in cars
[00:51:35.040 --> 00:51:40.560]   I mean you now have like the key one of the promises is there's no cyber committee and
[00:51:41.200 --> 00:51:44.800]   Five or six congressional committees say cyber security is their problem
[00:51:44.800 --> 00:51:49.680]   Right and so you have a what a cyber committee. Do you really want one the way they mess things up?
[00:51:49.680 --> 00:51:53.040]   I think something like a select committee wouldn't be a horrible idea
[00:51:53.040 --> 00:51:59.920]   Because what you end up right now is you have you know, homeland security and judiciary and ways and means and all these different people
[00:51:59.920 --> 00:52:04.560]   Having some kind of cyber component and and then not being able to have the the real skill set in house, right?
[00:52:04.560 --> 00:52:07.840]   The they're also used to be an office of technology
[00:52:08.480 --> 00:52:13.840]   Policy right oh cp. It's back. Yeah, it's back. It's back. But what's more important. I think the good thing congress did
[00:52:13.840 --> 00:52:18.160]   Uh, is they created sissa the cyber security infrastructure security?
[00:52:18.160 --> 00:52:21.440]   Under huge chest which is huge. Yes, because finally there is a
[00:52:21.440 --> 00:52:24.560]   Technically competent defensive cyber security agency
[00:52:24.560 --> 00:52:29.520]   It used to be the only people in the u.s. Government who really knew anything about cyber was nsa and cyber command
[00:52:29.520 --> 00:52:31.600]   Right it's all offense all classified
[00:52:31.600 --> 00:52:33.680]   Right they could they would never help you out in fact
[00:52:33.680 --> 00:52:38.000]   It was kind of sketchy talking to them because you're like you're telling them about vulnerabilities that you don't want them using
[00:52:38.480 --> 00:52:43.200]   For their own purposes nsa's always been schizophrenic both the defending our shores and attacking
[00:52:43.200 --> 00:52:48.640]   Others and that's always been a little schizophrenic your your partner christ christ christ was ahead of cesa
[00:52:48.640 --> 00:52:51.680]   Yes, and left fired by tweef fired by tweef
[00:52:51.680 --> 00:52:55.680]   It turns out raises all these interesting uh hr questions under the administrative act
[00:52:55.680 --> 00:53:00.000]   Like of your 401k or your pension and stuff like that
[00:53:00.000 --> 00:53:05.120]   Chris of course famously said the elections were well done. They were very best elections we were
[00:53:05.680 --> 00:53:09.760]   Anyway, uh, so cesa though is a little controversial sometimes yeah
[00:53:09.760 --> 00:53:13.600]   Um, but you feel like on balance. It's a it's a good thing. I think i'm balance is great
[00:53:13.600 --> 00:53:16.640]   I think current director jennie sturley the second director ever is fantastic
[00:53:16.640 --> 00:53:21.680]   She has I mean she came out of the military and then had nsa and cyber command experience
[00:53:21.680 --> 00:53:27.120]   So we're in this weird place where there's a bunch of competent people and they all come from nsa
[00:53:27.120 --> 00:53:30.560]   Because it's the only place you can come from in the government and have technical competence
[00:53:30.800 --> 00:53:37.520]   So I think it will be good in five years when hopefully we have people who did not come up through the offensive the spine
[00:53:37.520 --> 00:53:40.640]   To you and stanford and and more programs like that. I think right right
[00:53:40.640 --> 00:53:48.000]   Georgetown does this um in columbia now and so there's a lot of schools who are trying to teach real cyber security skills to folks
[00:53:48.000 --> 00:53:52.000]   Jeff's good to know get in here you can I left out over there left field
[00:53:52.000 --> 00:53:54.560]   I think I think part of the reason that you're here
[00:53:54.560 --> 00:53:58.480]   Um is as as the world started shifting toward mastodon
[00:53:59.040 --> 00:54:01.520]   I saw the uh, oh alak stamos
[00:54:01.520 --> 00:54:07.920]   Uh tweet that's or no it was it was it was it was a toot where you said i'm cataloging problems
[00:54:07.920 --> 00:54:12.400]   And I said, oh and i'm sure it's there so i'm curious about two things
[00:54:12.400 --> 00:54:16.720]   One what you think of the structure of the vetivers, but then second
[00:54:16.720 --> 00:54:18.480]   um
[00:54:18.480 --> 00:54:21.120]   And I was talking to a mutual friend of ours about just this the other day
[00:54:21.120 --> 00:54:23.680]   If funders were to come into this space
[00:54:23.680 --> 00:54:26.400]   As I hope they do to develop
[00:54:26.400 --> 00:54:28.240]   um
[00:54:28.240 --> 00:54:32.000]   At least tests for example around black twitter or around
[00:54:32.000 --> 00:54:35.600]   Persuage selected algorithms or around security
[00:54:35.600 --> 00:54:41.040]   To bolster this new federated wonderful little world that we see a boarding
[00:54:41.040 --> 00:54:43.040]   um
[00:54:43.040 --> 00:54:46.080]   What do you see is the status of it and what kind of work needs to be done
[00:54:46.080 --> 00:54:51.440]   And what does it take to do that? Right those are great questions. So the fediverse has like
[00:54:51.440 --> 00:54:57.760]   Inherent strengths and weaknesses, right? So the inherent strengths obviously being distributed. You're not going to end up with a single
[00:54:58.320 --> 00:55:04.560]   Person getting red-pilled and all of a sudden making everything go nuts, right? Um you as a individual. That's what's happened by the way
[00:55:04.560 --> 00:55:06.800]   I
[00:55:06.800 --> 00:55:08.800]   I think musk is
[00:55:08.800 --> 00:55:14.960]   When you're posting photos with two replica guns and four caffeine free diet cokes. Yeah
[00:55:14.960 --> 00:55:19.680]   The caffeine free diet coke that's like one step short of a diet doctor pepper like
[00:55:19.680 --> 00:55:22.640]   Before diad doctor peppers. I feel like
[00:55:22.640 --> 00:55:27.920]   It was a pun. I think it was send lawyer guns and money was guns and coke
[00:55:27.920 --> 00:55:32.640]   Yeah, I think was the pun. Oh, but it were they replica that were that's a relief one of them
[00:55:32.640 --> 00:55:35.600]   One of them was like it was a video game replica the almost a musket
[00:55:35.600 --> 00:55:39.440]   Which might be a real musket and he has the money that he could own a real revolutionary war musket
[00:55:39.440 --> 00:55:42.640]   Which I think is actually pretty cool. I just think it's a weird. It's a bad thing to be
[00:55:42.640 --> 00:55:45.280]   You own a company like twitter
[00:55:45.280 --> 00:55:48.560]   He's clearly if I was his family member, I'd be really worried about him
[00:55:48.560 --> 00:55:50.640]   I feel like he's having like a breakdown
[00:55:50.640 --> 00:55:53.200]   So the upside of the fediverse is you can't have that come in
[00:55:53.200 --> 00:55:57.920]   Some downsides right first off the kind of privacy model of the fediverse
[00:55:57.920 --> 00:56:05.280]   Is like if you looked at the camera general lika scandal and you said I want to build an entire social media network based upon camera general littica
[00:56:05.280 --> 00:56:10.640]   Right like the closest any major company has been to being as open as any
[00:56:10.640 --> 00:56:14.960]   Fediverse instance is is facebook with graph api v1
[00:56:14.960 --> 00:56:19.760]   Which is the core api that caused the camera general littica and some a bunch of other provers
[00:56:20.240 --> 00:56:24.560]   So that is like a fundamental problem is that you have no privacy in the fediverse
[00:56:24.560 --> 00:56:28.480]   People know this pretty widely right now, but your dms are no way encrypted
[00:56:28.480 --> 00:56:34.640]   They're seen by every single else, but then also it is effectively trivial to make a archive of everything
[00:56:34.640 --> 00:56:36.640]   Anybody says publicly in the fediverse
[00:56:36.640 --> 00:56:37.440]   the
[00:56:37.440 --> 00:56:40.960]   The mastodon developers and such don't build that into the system
[00:56:40.960 --> 00:56:45.120]   They they've you know avoided people having search and stuff, but these are just conventions
[00:56:46.080 --> 00:56:51.040]   I know for a fact people are breaking those conventions and just effectively archiving the entire fediverse in real time
[00:56:51.040 --> 00:56:52.640]   Which then
[00:56:52.640 --> 00:56:58.240]   Does not all the rights that we expect under gdpr under um california's laws now
[00:56:58.240 --> 00:57:00.400]   Are effectively non
[00:57:00.400 --> 00:57:03.920]   Can't be applied to the fediverse and never will under the current design of activity pub
[00:57:03.920 --> 00:57:05.840]   You cannot have privacy
[00:57:05.840 --> 00:57:10.720]   Okay, so that's a fundamental problem as long as people understand that you don't have privacy and dms on twitter now
[00:57:10.720 --> 00:57:12.480]   Right well, no, that's not true
[00:57:12.480 --> 00:57:16.400]   Like your dms on twitter are protected by the electron communications privacy acts in the stored communications
[00:57:16.400 --> 00:57:18.080]   Legally protected are legally protected not
[00:57:18.080 --> 00:57:22.880]   All right, you on musk just publish those he'd be violating. I think it's 18 usc 2701, right?
[00:57:22.880 --> 00:57:24.080]   Okay, that's a real federal law
[00:57:24.080 --> 00:57:26.720]   All right, federal crime so like I
[00:57:26.720 --> 00:57:29.120]   I think that might be true for the fediverse too
[00:57:29.120 --> 00:57:32.800]   So what my colleague who I teach my cyberclass with re_anifefefricorn is a lawyer
[00:57:32.800 --> 00:57:37.600]   And one thing I want to work on with her and the law students is like a legal guide for fediverse because that's
[00:57:38.000 --> 00:57:42.800]   SCA applied to you what kind of responsibilities do you have under the the child exploitation statute?
[00:57:42.800 --> 00:57:47.920]   So 18 usc 2058 a is like the the provider responsibilities
[00:57:47.920 --> 00:57:52.480]   Um, okay, that's one reason why the way why i'm glad dms are not private right right?
[00:57:52.480 --> 00:57:55.840]   Yeah, yes that if people thought they were private they would be much worse
[00:57:55.840 --> 00:58:02.880]   The other interesting issue is as masses on scales and gets more diverse and you end up with more and more people that you don't like on your instance
[00:58:02.880 --> 00:58:05.360]   You're gonna have to do more content moderation. I kick them off
[00:58:05.600 --> 00:58:09.120]   Right and but the tools that exist right now really suck right?
[00:58:09.120 --> 00:58:15.120]   Um, and I mean like you know the reporting when you report something I'm acid on there's not you don't have the ability like you do on instagram
[00:58:15.120 --> 00:58:20.080]   You can choose how to report it and stuff that means it goes into different q right when it gets queued
[00:58:20.080 --> 00:58:24.320]   Amal this is less of a problem as they stay small if they stay small
[00:58:24.320 --> 00:58:31.760]   If if we believe in the vision then we have to believe in a vision where you're gonna have to do a pretty massive amount of moderation
[00:58:32.240 --> 00:58:37.680]   And having a single queue that has no ai amal component right no automatic scanning
[00:58:37.680 --> 00:58:42.640]   None of the tooling that is what people have spent billions of dollars to build a place is like facebook and twitter
[00:58:42.640 --> 00:58:47.520]   Is gonna be a problem. Um, so this is actually one thing we're doing so we run cyber villains.com
[00:58:47.520 --> 00:58:53.280]   I was just doing it for fun and it's been successful enough that we're gonna port it over and make it official stanford.io project
[00:58:53.280 --> 00:58:58.880]   Nice. You only have 94 users right now. It's not it's not intended to be a public instance
[00:58:58.880 --> 00:59:02.800]   No, but like we found some interesting stuff by playing with it. No, it is public. It's it's going to be public
[00:59:02.800 --> 00:59:04.480]   I have it announced yet, but it's gonna be stable
[00:59:04.480 --> 00:59:08.400]   So if people want to be on it what we do is it's intentionally an instance of people can hack
[00:59:08.400 --> 00:59:12.560]   Right, so we have a policy that if you want to try to hack it and find problems with it
[00:59:12.560 --> 00:59:17.840]   So go ahead and hack it right now. That's we say in the privacy policy you have no privacy on this instance
[00:59:17.840 --> 00:59:19.840]   Right that is true for all of my
[00:59:19.840 --> 00:59:24.080]   I should probably say that in my uh privacy policy. You just said you have no privacy
[00:59:24.080 --> 00:59:25.200]   You have no privacy right like you
[00:59:25.200 --> 00:59:29.280]   I absolutely no guarantee of anything. Yeah, because you have no control over the security of that code
[00:59:29.280 --> 00:59:30.480]   Right
[00:59:30.480 --> 00:59:34.480]   And so we're gonna do that but the other thing we're gonna do is we're gonna start running off of a fork of mastodon that
[00:59:34.480 --> 00:59:37.680]   We're gonna maintain at stanford where we work on trust and safety tools
[00:59:37.680 --> 00:59:42.080]   So the I want to build out an api with my students of here are pluggable
[00:59:42.080 --> 00:59:46.880]   Things of like here's my hate speech detector. Here's my classifier for this probably the first one will be
[00:59:46.880 --> 00:59:51.440]   Child exploitation. So we're running a project right now to scan the fediverse for
[00:59:52.720 --> 00:59:57.520]   uh using photo DNA and it's not great so far. Um, so we'll have some results published
[00:59:57.520 --> 01:00:01.360]   Um, but right now the tools available to admins are pretty bad
[01:00:01.360 --> 01:00:04.960]   And so we'd like to build those tools and if they want to accept our patches upstream
[01:00:04.960 --> 01:00:07.920]   That's great. Otherwise people can try to use our our thread
[01:00:07.920 --> 01:00:13.200]   And so that's one of the things that we're gonna be working on is because I do think there's a lot of great future in
[01:00:13.200 --> 01:00:16.000]   Federated networks, but we're going to have to rethink
[01:00:16.000 --> 01:00:20.880]   All this trust and safety stuff is based upon the idea of large well-resourced corporations
[01:00:21.440 --> 01:00:26.960]   Oh the platform and having a legal team that tells them what their legal responsibilities are the timing here
[01:00:26.960 --> 01:00:30.160]   One thing's like the thing I tweeted when musk closed
[01:00:30.160 --> 01:00:36.160]   The deal was that his timing is crazy and that he bought twitter at the hardest
[01:00:36.160 --> 01:00:40.880]   Geopolitical moment for social media companies of everybody wants to regulate social media
[01:00:40.880 --> 01:00:47.280]   It turns right it was midterms, but also the day he bought it the the final text of the digital services act in the EU
[01:00:47.920 --> 01:00:54.000]   The India has been publishing regulations. There's a online safety act that's extremely draconian in the UK
[01:00:54.000 --> 01:00:58.560]   We just passed a not so great child safety law in california
[01:00:58.560 --> 01:01:04.400]   So it's just a crazy time for him to buy himself into this area and that's true for all mastodon
[01:01:04.400 --> 01:01:07.600]   Runners and so if you're going to be part of the fediverse
[01:01:07.600 --> 01:01:13.440]   You're we're going to need to build better tools so you can live up to the responsibilities because the the bar has been set here
[01:01:14.000 --> 01:01:20.720]   By billions of dollars of spend from large corporations and you can't get there but you get somewhere because right now we're down here
[01:01:20.720 --> 01:01:26.640]   So it is not that great you go to report something on that and it's it's work just to report a cast on that
[01:01:26.640 --> 01:01:30.240]   Yeah, so I just added a rule to my instance saying there's no privacy
[01:01:30.240 --> 01:01:35.680]   Don't expect privacy even if you post a DM it consider yourself posting publicly. Thank you
[01:01:35.680 --> 01:01:42.320]   So hold on i'll take a break. I want to talk about you raised some interesting questions
[01:01:42.800 --> 01:01:46.320]   Uh, I know jeff will want to talk about the urniv act in section 230
[01:01:46.320 --> 01:01:50.080]   I want to talk about the assault on edu e end-to-end encryption
[01:01:50.080 --> 01:01:55.520]   Yeah, because there are very few governments in the world that like end-to-end encryption including our own very few
[01:01:55.520 --> 01:01:58.720]   And the stony bill
[01:01:58.720 --> 01:02:02.800]   As of today the uk bill that allis just talked about
[01:02:02.800 --> 01:02:07.040]   Um is basically forbidding encryption. That's right forbidden encryption in the UK
[01:02:07.040 --> 01:02:10.640]   So that's a very hot topic right now. We'll talk about that in a lot more
[01:02:11.200 --> 01:02:15.840]   With a very special guest. This is an unusual addition of this week in google
[01:02:15.840 --> 01:02:18.400]   We might not even get a change
[01:02:18.400 --> 01:02:20.960]   Every week's unusual
[01:02:20.960 --> 01:02:25.760]   Because I feel oh yeah, we'll bring up google we'll figure out a way to just stick
[01:02:25.760 --> 01:02:29.120]   We always try a little stick a little cool in this week. He already mentioned crumb books
[01:02:29.120 --> 01:02:33.120]   It's not much spamming it's just a little bit
[01:02:33.120 --> 01:02:36.960]   Uh, and I want to reassure people who are saying but stacy
[01:02:36.960 --> 01:02:39.760]   Stacey had a variety of
[01:02:39.920 --> 01:02:44.880]   Personal commitments events she had to go to and things we knew this ahead of time
[01:02:44.880 --> 01:02:49.200]   This has given us a great opportunity to have some wonderful special guests. She will be back next week
[01:02:49.200 --> 01:02:56.400]   She is not leaving the show. So don't worry. I'm just off next week. I'm kidding. I'm kidding
[01:02:56.400 --> 01:02:59.680]   Always here
[01:02:59.680 --> 01:03:01.680]   Aren't you leaving sometime soon?
[01:03:01.680 --> 01:03:04.320]   The four keys to dream ahead. Oh, okay
[01:03:06.960 --> 01:03:12.400]   Our show today brought to you by rocket money this cut this single app has saved me
[01:03:12.400 --> 01:03:18.000]   I'm almost must say thousands of dollars. It certainly saved me hundreds of dollars rocket money
[01:03:18.000 --> 01:03:22.160]   I first started using it when it's called true bill. They've renamed it rockets acquired it
[01:03:22.160 --> 01:03:25.280]   They've renamed it and they've made it I think a hundred times better
[01:03:25.280 --> 01:03:31.840]   Let me ask you a question. I think you know the answer to right off the top of your hand. Are you wasting money on subscriptions?
[01:03:31.840 --> 01:03:35.920]   God, we all have hundreds of subscriptions these days, right?
[01:03:35.920 --> 01:03:37.920]   Who could keep track of them?
[01:03:37.920 --> 01:03:41.680]   Rocket money surveyed people and they found 80 percent of people
[01:03:41.680 --> 01:03:45.760]   Have subscriptions they've forgotten about but that's not the worst news
[01:03:45.760 --> 01:03:50.560]   They averaged they thought around $80 a month on subscriptions. Oh, yeah
[01:03:50.560 --> 01:03:53.200]   Yeah, I bet about $80 a month
[01:03:53.200 --> 01:03:58.560]   When when they actually did the digging they found out it was a lot closer to $200 and more a month
[01:03:58.560 --> 01:04:02.400]   Many of those subscriptions not used long forgotten. Do you have a
[01:04:02.960 --> 01:04:06.880]   I so in my case it was a political contribution I made in the last cycle
[01:04:06.880 --> 01:04:10.320]   And you know you go to these sites for political contributions
[01:04:10.320 --> 01:04:14.240]   There's always a checkbox that says make this a recurring contribution
[01:04:14.240 --> 01:04:22.880]   And it's always pre-checked just in case for your convenience. Well, I didn't notice that so I was giving a political campaign after the election
[01:04:22.880 --> 01:04:26.400]   I don't even want to say how much money every month
[01:04:26.400 --> 01:04:31.920]   For months. Yes, you saw me do it. I went what the hell
[01:04:32.640 --> 01:04:36.480]   Thank you. Thank you rocket money. You save me you save me. I turned it off immediately
[01:04:36.480 --> 01:04:40.080]   But that the beauty of this is I didn't even have to turn it off in rocket money
[01:04:40.080 --> 01:04:45.120]   You'll see all your recurring subscriptions. You'll see which are duplicated sometimes you even have two of the same thing
[01:04:45.120 --> 01:04:50.080]   Which is crazy. I did have one for day one, which is a journal. I had two subscriptions like
[01:04:50.080 --> 01:04:54.080]   So anyway, it finds those and then you cancel it in the app
[01:04:54.080 --> 01:04:58.800]   You just say well, yeah, please cancel that and it does it for you. It is incredible
[01:04:59.360 --> 01:05:04.080]   Uh, all you have to do is press the cancel button rocket money takes care of the rest
[01:05:04.080 --> 01:05:07.520]   If it doesn't save you money right out of the box
[01:05:07.520 --> 01:05:10.560]   You're just better more disciplined than I am. I guess
[01:05:10.560 --> 01:05:15.360]   Rocket money they used to call it true bill. Maybe you have true bill if you had true bill on your phone
[01:05:15.360 --> 01:05:21.760]   Look because it's now rocket money has a lot of new and wonderful features. It's great for budgeting to see what your net worth is
[01:05:21.760 --> 01:05:24.640]   See where your money's going. It's all sorts of great stuff
[01:05:24.640 --> 01:05:26.960]   But the single most useful thing
[01:05:27.520 --> 01:05:34.880]   The pays for itself get rid of useless subscriptions with rocket money now go to rocket money.com/twig was that on this show
[01:05:34.880 --> 01:05:40.640]   When I found that it was hysterical. Yeah, I remember you doing our doing the read. I like I
[01:05:40.640 --> 01:05:44.000]   And he was like I did what?
[01:05:44.000 --> 01:05:47.120]   What since 2020
[01:05:47.120 --> 01:05:51.520]   Seriously, it could save you hundreds of dollars per year. It saved me thousands
[01:05:51.520 --> 01:05:54.720]   Rocket money.com/twig
[01:05:55.120 --> 01:05:59.040]   Cancel your unnecessary subscriptions right now rocket money.com/twig
[01:05:59.040 --> 01:06:04.960]   We thank him so much for supporting this week in google you support us of course by going to that address so that they know you saw it here
[01:06:04.960 --> 01:06:09.600]   rocket money.com/twig we are this is more and more
[01:06:09.600 --> 01:06:13.200]   Uh, that's it for the ad you can take down the author
[01:06:13.200 --> 01:06:17.120]   But I do want to comment on this because this is more and more of advertisers are saying to us
[01:06:17.120 --> 01:06:19.520]   Oh, yeah, we want to track all your
[01:06:20.000 --> 01:06:25.040]   Listen, they call it a tracking pixel. Yeah, we want to just want to put a tracking pixel on there
[01:06:25.040 --> 01:06:28.720]   Yeah, so we can't put a tracking pixel. It's a podcast. Oh no, we have ways
[01:06:28.720 --> 01:06:32.320]   We say no you'll be glad to know we say no
[01:06:32.320 --> 01:06:37.840]   Uh, but it's getting harder and harder as a result for us to find advertisers who are happy
[01:06:37.840 --> 01:06:40.400]   Just selling a lot of product
[01:06:40.400 --> 01:06:43.920]   Because with this is the thing that irks us so much
[01:06:43.920 --> 01:06:47.440]   Without exception the ads do really well for them
[01:06:48.080 --> 01:06:51.680]   But they say things like well, we don't know if they came from your show
[01:06:51.680 --> 01:06:55.600]   We don't know if that coupon code maybe they saw it in another site. So
[01:06:55.600 --> 01:07:00.320]   A when you hear an ad use that offer code, you know
[01:07:00.320 --> 01:07:03.840]   Because that way they'll at least they can't deny it I guess
[01:07:03.840 --> 01:07:07.360]   But number two support us through club twit because that is a really great way
[01:07:07.360 --> 01:07:11.600]   For you to spend a little bit of money seven bucks a month less than a blue check I might add
[01:07:11.600 --> 01:07:12.400]   That's right
[01:07:12.400 --> 01:07:17.440]   You could put everybody on club twit in the discord has added blue checks now to their names by the way
[01:07:17.440 --> 01:07:19.440]   Which is so funny
[01:07:19.440 --> 01:07:22.240]   Less than a blue check, but what do you get you get?
[01:07:22.240 --> 01:07:27.520]   Uh every single show we do ad free and that means tracker free absolutely tracker free, right?
[01:07:27.520 --> 01:07:30.400]   So total privacy because rss we don't know anything about you
[01:07:30.400 --> 01:07:33.680]   You also get shows that we don't put out on the regular feeds
[01:07:33.680 --> 01:07:39.120]   Like hands on windows with paul thorat hands on macadosh with mica sergeant the untitled linux show
[01:07:39.120 --> 01:07:44.720]   Uh, which is a fabulous program for a linux lovers. I actually would like that show to get bigger
[01:07:44.720 --> 01:07:48.960]   But you know what here's the problem a show like that. It's hard to sell advertising on the untitled linux show
[01:07:48.960 --> 01:07:51.600]   Maybe if we had a title, I don't know but Jonathan
[01:07:51.600 --> 01:07:56.160]   Shudger job of that show so though so who supports it the members
[01:07:56.160 --> 01:07:59.760]   And that's why it's so important. We want you to join club twit
[01:07:59.760 --> 01:08:05.360]   You also get access to the discord your very own blue check if if that rocks your boat
[01:08:05.360 --> 01:08:09.600]   You can do that uh see everybody's got blue check on there some of them have just little oh no
[01:08:09.600 --> 01:08:13.840]   I have a twit logo, but everybody else has blue checks. We got twit bugs. We got twit. We got all sorts
[01:08:13.840 --> 01:08:17.280]   We got special uh special emojis and things
[01:08:17.280 --> 01:08:22.560]   Uh, and you get the twit plus feed which has all those shows that we don't put out in public plus a lot more stuff
[01:08:22.560 --> 01:08:27.440]   For instance, we had great commerce the best stuff before the show almost always Alex
[01:08:27.440 --> 01:08:30.000]   We're talking about Alex great stuff didn't make in the show
[01:08:30.000 --> 01:08:33.600]   Uh, but we have it on the twit plus feed so what
[01:08:33.600 --> 01:08:43.440]   So no, he didn't say anything bad and uh, I don't think anyway, you can you can know we can pull it down if you
[01:08:44.240 --> 01:08:45.040]   Uh
[01:08:45.040 --> 01:08:48.240]   You did I tell you that on the fediverse you have no privacy. Did I mention that?
[01:08:48.240 --> 01:08:56.480]   Twit.tv/club twit twit.tv/club twit get your blue check for only seven bucks
[01:08:56.480 --> 01:09:01.360]   A month. We thank all our members. We have great members and it's always a great conversation
[01:09:01.360 --> 01:09:06.400]   I joined I officially joined you didn't have to you and I joined and thank you for doing that jeff
[01:09:06.400 --> 01:09:10.080]   And for coders, we have in our coding section. We're all excited
[01:09:10.480 --> 01:09:16.720]   We have our club twit advent of code leaderboard. It starts at 9 p.m. Pacific midnight easter
[01:09:16.720 --> 01:09:21.040]   Tonight, I will be a little groggy for the next 30 days
[01:09:21.040 --> 01:09:29.280]   It's in the coding channel if you want to do advent of code join our private leaderboard john arnold set it up
[01:09:29.280 --> 01:09:35.120]   And uh see compare your coding skills. There it is. Uh, oh, it's so much fun
[01:09:35.280 --> 01:09:40.000]   We we love advent of code is is my is my early christmas present. I love it every year
[01:09:40.000 --> 01:09:46.800]   Uh, oh boy already. I'm seeing can I ask a follow-up question here to the to the master? Yes
[01:09:46.800 --> 01:09:48.400]   Um
[01:09:48.400 --> 01:09:50.400]   It's i'll make it two questions because i'll cheat
[01:09:50.400 --> 01:09:52.000]   Uh
[01:09:52.000 --> 01:09:57.600]   I had a conversation with somebody who's starting to competitor to master dot i'll just leave it at that who said oh and had the codes
[01:09:57.600 --> 01:10:03.440]   Has to be completely rewritten. Well, well, and I don't know enough about this to know what the underlying
[01:10:04.160 --> 01:10:09.120]   Um code base and how good it is that was question one and question two is you have next to you
[01:10:09.120 --> 01:10:13.600]   Uh a host of an instance. What advice do you have for lea as a host?
[01:10:13.600 --> 01:10:15.920]   Um
[01:10:15.920 --> 01:10:19.600]   So on the second one, I you know like I said, I'd like us to be working on
[01:10:19.600 --> 01:10:27.120]   Uh official recommendations for mason on host. I mean right now I you have as there's a open question of whether you're a
[01:10:27.120 --> 01:10:32.720]   uh, electronic service provider as defined by by federal law. I'm screwed. Anyway. We have an irc
[01:10:33.120 --> 01:10:38.480]   We have a forum. We have mastodon. We have a private discord. I got it coming and going
[01:10:38.480 --> 01:10:42.400]   So what I'd love to see is I think it would be great to get kind of
[01:10:42.400 --> 01:10:45.440]   legal and trust and safety services
[01:10:45.440 --> 01:10:49.840]   From a group of service providers that then you could pool and you could pay for insurance
[01:10:49.840 --> 01:10:51.440]   I could totally see that you pay well
[01:10:51.440 --> 01:10:57.600]   We buy it. We have an umbrella insurance policy because you're really like a thousand bucks a month for access to lawyers and a
[01:10:57.600 --> 01:11:02.320]   Pool where the top the tip the top 10 the top 20. Yeah, um, instances. I'll pay for
[01:11:02.640 --> 01:11:06.880]   We don't know I at least for gdpr you have to have a certain amount of income
[01:11:06.880 --> 01:11:10.240]   You have to be a 50 million. I don't think with gdpr. No, no, you're talking about dsa
[01:11:10.240 --> 01:11:12.400]   So dsa has this idea of a very large platform
[01:11:12.400 --> 01:11:18.240]   But for gdpr of some of the stuff kicks in at any level very well processing data. Right so
[01:11:18.240 --> 01:11:22.320]   Yes, it and this is a shame because yeah, we're creating communities
[01:11:22.320 --> 01:11:27.760]   They're nice communities. Right. Uh, I understand. I want to protect people's privacy. I want to protect their security
[01:11:27.760 --> 01:11:30.640]   I want them to have the right to be forgotten if that's what they want
[01:11:30.800 --> 01:11:35.760]   Yeah, but at the same time I want to without massive law and green massive liabilities and massive costs
[01:11:35.760 --> 01:11:39.840]   Which means only big guys can do it. Yes. I want to create communities
[01:11:39.840 --> 01:11:43.840]   Yeah, I mean that is this is I've talked about this for a while
[01:11:43.840 --> 01:11:48.720]   The one of the hard trade-offs in any regulation of this space is it is very hard to come up
[01:11:48.720 --> 01:11:51.840]   You can come with regulations that say it only works for the big guys
[01:11:51.840 --> 01:11:56.080]   Yeah, they're appropriate for facebook. They're appropriate for google. See we say people like just doing that is
[01:11:56.640 --> 01:12:01.600]   Hard and it's in under some legal regimes. It's a difficult thing to do under their rules, right?
[01:12:01.600 --> 01:12:03.760]   Um, and it also doesn't make sense in a lot of cases
[01:12:03.760 --> 01:12:07.920]   It creates gamemanship where people are going to like structure their companies in such ways
[01:12:07.920 --> 01:12:12.880]   Um, but yeah, I mean that this is the problem of gdpr and some other regulations is they they
[01:12:12.880 --> 01:12:20.240]   You can't what I would love to see is that when these regulations are passed that they think about the 10 person volunteer project
[01:12:20.240 --> 01:12:24.480]   Um that you don't want the 11th person to have to be a private european privacy lawyer, right?
[01:12:24.480 --> 01:12:30.080]   That there is a four-page checklist that you can go through to be compatible, but those checklists don't exist
[01:12:30.080 --> 01:12:35.680]   Um, and uh, and this is what we just say can I just assume? Well, I'm a little guy. They're not going to go after me
[01:12:35.680 --> 01:12:40.320]   Sure, I mean that's true for the situations in which you don't have civil liability
[01:12:40.320 --> 01:12:46.720]   For individual users so for most of gdpr you're probably okay because the UK you're screwed
[01:12:46.720 --> 01:12:49.920]   The complaints right UK you're going to be screwed under the new system
[01:12:49.920 --> 01:12:54.000]   Um, yes, but under current gpr it has to go to the data protection commission
[01:12:54.480 --> 01:13:00.000]   To be adjudicated and so individuals don't bring the cases the cases go through government agencies
[01:13:00.000 --> 01:13:04.880]   Um, I'm not totally sure in a digital service act. Anyway, I'm not a lawyer. This is something that we want to work at the law school
[01:13:04.880 --> 01:13:11.040]   Absolutely. I'm not considering this legal advice by any means although deneese howl who is a lawyer and ibler and as a member of
[01:13:11.040 --> 01:13:15.600]   Our community and has been on our shows many times escaping all sorts of advice
[01:13:15.600 --> 01:13:21.840]   Are you red? Do you have a registered dmca agent? I do I am the registered dmc agent right like crazy stuff like that that you never think of
[01:13:21.920 --> 01:13:27.040]   Yeah, that's important. That's so somebody has a dmca take down. They know who to go to they go to the uspto
[01:13:27.040 --> 01:13:31.920]   They do a search. They say oh, it's leo. Here's his email. They said that was the kind of great advice you gave vallex
[01:13:31.920 --> 01:13:38.240]   I saw everywhere just saying do that. Yeah, well and then a couple of big deals are going to be if you're going to be in this space
[01:13:38.240 --> 01:13:43.600]   Is one eventually you will end up getting a request from law enforcement and at that point you have to have lawyers that specialize
[01:13:43.600 --> 01:13:49.360]   Fortunately, there are law firms that specialize in this. Um, the one I usually recommend to startups is called zwill jens
[01:13:49.440 --> 01:13:55.040]   So a guy named marx willinger has a lot of litigation on behalf of big companies. Um and his firm will also do some
[01:13:55.040 --> 01:13:57.040]   Can you leave me your card?
[01:13:57.040 --> 01:14:05.280]   And then the others if you run into any child exploitation material then a bunch of legal requirements attached to you
[01:14:05.280 --> 01:14:07.680]   You have to report you can't just take it down. You have to report right
[01:14:07.680 --> 01:14:11.600]   So actually Stanford inter observatory if you look now in the nickmic database of reports last year
[01:14:11.600 --> 01:14:15.760]   We're now on the list where official reporters nickmic because one thing we start to see everything
[01:14:16.160 --> 01:14:20.160]   Yeah, because we're we're grabbing effectively most of gab in parlor
[01:14:20.160 --> 01:14:24.800]   That must be fun. I will eventually be fired from stanford
[01:14:24.800 --> 01:14:27.360]   But I want to be for some kind of really cool academic freedom issue
[01:14:27.360 --> 01:14:31.680]   I don't want it to be because a 19 year old looked at c-side right?
[01:14:31.680 --> 01:14:35.440]   Accidentally, right? And so we scan all of our incoming now
[01:14:35.440 --> 01:14:40.800]   And we just catch stuff. Um that alerts only the adults students are not told it gets encrypted
[01:14:40.800 --> 01:14:44.320]   We're extremely careful on how we handle it, but anybody who does this kind of stuff
[01:14:44.880 --> 01:14:49.120]   Unfortunately child sexual abuse material is the water that finds all cracks on the internet
[01:14:49.120 --> 01:14:53.920]   So whenever you have any kind of unmoderated space, it does become a problem good grief
[01:14:53.920 --> 01:14:56.000]   Well, I moderate heavily but
[01:14:56.000 --> 01:14:59.920]   You that you make that really important point you have a duty to report you have a duty to report
[01:14:59.920 --> 01:15:02.480]   Which is one of the things I want to work with my students is one
[01:15:02.480 --> 01:15:07.120]   Recommendations for folks, but then also the code that you could just hit a button and do the report automatically
[01:15:07.120 --> 01:15:11.040]   Which the cool thing is there are services microsoft runs a photo DNA service
[01:15:11.520 --> 01:15:17.600]   That will automatically do reporting for you can actually fill out the information and check the box and then it goes through microsoft's team
[01:15:17.600 --> 01:15:21.680]   Um, but there's no way right now to integrate that with mastodon, which is one of the things I want to work on
[01:15:21.680 --> 01:15:26.000]   Not yet. Not yet. And if 230 goes away leo you are really effed
[01:15:26.000 --> 01:15:30.240]   Maybe and he matters what the supreme court does with the first amendment, right? Like a lot
[01:15:30.240 --> 01:15:34.560]   But yes, he's probably after that you don't have you don't win with the initial
[01:15:34.560 --> 01:15:38.000]   You can't win with uh at the beginning of the process
[01:15:38.000 --> 01:15:41.600]   Nice thing about 230 as many many cases are just immediately dismissed
[01:15:41.600 --> 01:15:47.840]   Because of 230 protects you there's no standing by by the judge says yeah, you can't sue this guy
[01:15:47.840 --> 01:15:52.720]   Protected as soon as that goes away great. I have the right to defend myself in court
[01:15:52.720 --> 01:16:00.000]   Right and uh if 230 goes away. I mean the internet will be for the googles and the meadows and the apples. That's really sad, isn't it?
[01:16:00.000 --> 01:16:03.040]   Yes, it will guess it is very sad and it is effectively
[01:16:03.040 --> 01:16:04.960]   I mean I think it's where we're going in europe, right?
[01:16:05.040 --> 01:16:08.880]   Is we're moving towards a direction where it seems that's not what they want though
[01:16:08.880 --> 01:16:14.560]   Well, this is the problem with european politicians is they think they can have everything. Yeah, but they can't right they have to
[01:16:14.560 --> 01:16:17.280]   Well, well in the us we do nothing
[01:16:17.280 --> 01:16:19.280]   Right, so congress just doesn't legislate anymore
[01:16:19.280 --> 01:16:24.400]   The most important kind of 230 cases right now we're all in the supreme court because the us has it well
[01:16:24.400 --> 01:16:26.320]   All these senators talk talk talk
[01:16:26.320 --> 01:16:29.920]   Congress has it seriously moved any bill even out of committee, right?
[01:16:29.920 --> 01:16:32.800]   Um, whereas in europe they really do move legislation
[01:16:32.880 --> 01:16:36.320]   But what's important in the us right now is there's two big cases in scotas next year
[01:16:36.320 --> 01:16:40.720]   They'll come down next year. These are first amendment case first amendment and 230 cases of
[01:16:40.720 --> 01:16:45.280]   Of what limitations and so one of them effectively blames twitter for
[01:16:45.280 --> 01:16:50.000]   terrorist attacks, uh in google so there's one against twitter once against google
[01:16:50.000 --> 01:16:52.000]   I think i'm sorry might be google that's responsible
[01:16:52.000 --> 01:16:55.600]   But basically saying you are responsible if a terrorist use your platform to
[01:16:55.600 --> 01:16:59.520]   Organize a terrorist attack that you're responsible. So it's like a direct attack against 230
[01:17:00.000 --> 01:17:04.640]   So the big guys can't afford they have buildings entirely populated by attorneys
[01:17:04.640 --> 01:17:09.200]   They can afford to defend themselves the little guys just shut down. It has a chilling effect
[01:17:09.200 --> 01:17:14.960]   So there's no competition then which is why mark segabritt wants regulation. It's why you saw him make that swap
[01:17:14.960 --> 01:17:16.160]   Yeah
[01:17:16.160 --> 01:17:19.920]   Of all of a sudden we're seeing all these ads and we're seeing all these ads from facebook
[01:17:19.920 --> 01:17:23.840]   We want regulation we care about the quality of your every new york times podcast
[01:17:23.840 --> 01:17:24.960]   Have you listened?
[01:17:24.960 --> 01:17:27.840]   I don't know if i'm getting targeted if i'm in some segment
[01:17:28.080 --> 01:17:33.360]   But every time i listen to the daily i get a facebook ad for we are asking for regulation
[01:17:33.360 --> 01:17:37.040]   They're everywhere they're in nfl football games there. I mean they're everywhere
[01:17:37.040 --> 01:17:43.520]   Uh, and uh, it makes sense. It's and and you introduced me that term jeff javas regulatory capture
[01:17:43.520 --> 01:17:45.040]   uh
[01:17:45.040 --> 01:17:46.160]   Using
[01:17:46.160 --> 01:17:47.520]   legislation
[01:17:47.520 --> 01:17:51.440]   To protect your interests and to prevent to pull the ladder up after you to prevent the little guys
[01:17:51.440 --> 01:17:56.640]   Because it's worth what they they fought gdpr in europe and then your gdpr turned out to be fantastic for the big american tech
[01:17:56.640 --> 01:17:59.120]   They learned a lesson and so the lesson has been
[01:17:59.120 --> 01:18:02.080]   Instead of resisting we should celebrate it
[01:18:02.080 --> 01:18:07.520]   But what we should fight is any kind of diminumous standard and make sure that everything applies to every single competitor
[01:18:07.520 --> 01:18:09.920]   Yeah, no mr. Jarvis mentioned a
[01:18:09.920 --> 01:18:16.080]   Quote mastodon competitor. Someone you knows i think they're working on well. Yeah
[01:18:16.080 --> 01:18:22.160]   There's no matter the thing that really understands is that mastodon is just a client to the fediverse
[01:18:22.240 --> 01:18:27.520]   That's what i was on as how is it even a competitive service if everybody is just connecting to it
[01:18:27.520 --> 01:18:29.760]   It's just another one pixel fed is another one right
[01:18:29.760 --> 01:18:35.600]   Peer tube is another one. You know i was talking about and i was talking about a a
[01:18:35.600 --> 01:18:38.480]   We're talking about post
[01:18:38.480 --> 01:18:44.160]   Talk about post. Oh, okay. Yeah, I think you know the very fact that post and it's in its very first page
[01:18:44.160 --> 01:18:48.880]   Of its user agreement says that uh billionaires are a protected class
[01:18:49.600 --> 01:18:52.640]   Yeah, really tells you a little something about where that and and
[01:18:52.640 --> 01:18:59.760]   Full disclosure is that i left the advisory board for post and received nothing from them and i'm not associated with them
[01:18:59.760 --> 01:19:01.760]   I don't understand i know i'll put that in
[01:19:01.760 --> 01:19:06.720]   Post made was at the day during the week of the most movement they still had a limited sign up
[01:19:06.720 --> 01:19:09.600]   Right. Yeah, like if you really believe in it then you just have to go
[01:19:09.600 --> 01:19:14.640]   All the way and push your engineers to scale and take the million signups because
[01:19:15.040 --> 01:19:21.520]   Am i a nut for saying i believe an open source non proprietary decentralized systems you know to nut you just weird
[01:19:21.520 --> 01:19:24.240]   No, i i think i mean i think there's definitely a future there for sure
[01:19:24.240 --> 01:19:30.320]   Are they inherently more dangerous i would say they're inherently more dangerous, but it's more complicated and there's no
[01:19:30.320 --> 01:19:34.640]   There's nobody to turn to to make it safer. Yeah, because it's decentralized. It's decentralized
[01:19:34.640 --> 01:19:38.400]   Yeah, which is why i like it which is i think what we'll see is just like with email
[01:19:38.400 --> 01:19:42.240]   One of the reasons you're in gmail is they catch may more fishing spam right?
[01:19:42.240 --> 01:19:48.400]   You know people forget why gmail dominated but one of the reasons was google was very good at spam detection
[01:19:48.400 --> 01:19:54.000]   Right compared to yahoo and aim and hotmail and others and and that was like a real selling point
[01:19:54.000 --> 01:19:58.400]   Was the safety aspect of you're not just flooded with spam and scams all day
[01:19:58.400 --> 01:20:02.320]   Right, so i do think that i think there is a future for a fediverse
[01:20:02.320 --> 01:20:06.560]   But i think part of it will be what protection you get from your instance post
[01:20:06.560 --> 01:20:11.440]   I had a really interesting discussion on the fediverse this last i got into two
[01:20:12.400 --> 01:20:14.400]   threads
[01:20:14.400 --> 01:20:20.640]   One after julani cob who's the dean of the journalism school uptown for me rich people go to called columbia
[01:20:20.640 --> 01:20:22.240]   um
[01:20:22.240 --> 01:20:26.160]   Did a new yorker piece say i left twitter and i said i i don't know
[01:20:26.160 --> 01:20:31.120]   Maybe i side more sarah kenzior's view that we can't seed territory and information war
[01:20:31.120 --> 01:20:37.040]   And then i got people going to have to be like crazy because you're you're supporting the nazi and i said i'm just trying to pull
[01:20:37.040 --> 01:20:42.720]   Friends over the old place to the new place the other one i got into was when i said you know i i miss
[01:20:42.720 --> 01:20:48.960]   the the algorithm a little bit i miss i feel like i'm missing things as they go and i would like to have something in my control
[01:20:48.960 --> 01:20:52.800]   And i got the same kind of scolding it's a very scoldy place
[01:20:52.800 --> 01:20:57.760]   Uh this better than i said when i said if you come in if you march in well with your little twitter
[01:20:57.760 --> 01:21:03.520]   You just form on and your little twitter keppy just and say you know things are better over there
[01:21:04.960 --> 01:21:11.280]   Everybody come back but what came out in the end was was a lot of really interesting good ideas from developers
[01:21:11.280 --> 01:21:18.720]   About one guy was already mocking something up where he could just look at his own feed and see who he missed in the last 12 hours
[01:21:18.720 --> 01:21:23.600]   And it becomes a roll your own choose your own control your own
[01:21:23.600 --> 01:21:25.200]   um
[01:21:25.200 --> 01:21:30.400]   Service for you. That's what i'm really looking forward to in the fediverse is those kinds of add-on services
[01:21:30.960 --> 01:21:36.240]   Uh whether it's to recommend or to authenticate or to block stuff or whatever it is
[01:21:36.240 --> 01:21:40.880]   I think there's a lot of opportunity to build on top of this. That's what excites me most
[01:21:40.880 --> 01:21:43.200]   Well, I think full tech search is just gonna have to
[01:21:43.200 --> 01:21:50.640]   This is one of those like the ideas of the original developers of some of these products are not gonna have they can't be gods forever
[01:21:50.640 --> 01:21:54.960]   Right so we're without full tech search. You're not going to end up with people who really wanted to use it
[01:21:54.960 --> 01:22:00.640]   Um, you know if you think that that the full tech search that oil can oil you jeans
[01:22:01.120 --> 01:22:06.160]   Um, contentions and he's softening on the qt one but that quote tweet and full tech search
[01:22:06.160 --> 01:22:08.160]   were
[01:22:08.160 --> 01:22:13.520]   Vectors of bad behavior you study bad behavior. What do you think about that? I think quote tweet tweets for sure
[01:22:13.520 --> 01:22:16.880]   I think full tech searches unless people use hashtags
[01:22:16.880 --> 01:22:21.360]   Very aggressively. It's a basic part of discovery. Um
[01:22:21.360 --> 01:22:26.400]   The contention is that because I can search for anything. I can use it to harass
[01:22:26.640 --> 01:22:32.080]   Right, but this is the problem is the feta versus totally open you don't even need to have like a registered instance to just pull down
[01:22:32.080 --> 01:22:37.200]   Right the entire content amassan. That's social right so people will in create full tech search engines
[01:22:37.200 --> 01:22:41.440]   Right the question is is will be under control and at all privacy protective or completely non-pro
[01:22:41.440 --> 01:22:46.800]   I split the uh difference on uh twit social by uh turning on elastic search
[01:22:46.800 --> 01:22:52.080]   Yes, so we do have the ability to do some pretty good searching within our own instance not full tech search
[01:22:52.080 --> 01:22:54.160]   But I'd like to start it with just my own posts
[01:22:55.200 --> 01:22:59.760]   That would be a wonderful thing about you could search if you well see you're you're unfortunately you're on some other
[01:22:59.760 --> 01:23:03.520]   I know I'm probably gonna switch but I don't have to switch. I don't want you to switch but
[01:23:03.520 --> 01:23:10.400]   You don't have to switch. Well, that's the beauty of the feta verse you stay in that silly little pople that you call home
[01:23:10.400 --> 01:23:12.960]   but
[01:23:12.960 --> 01:23:15.120]   No, seriously
[01:23:15.120 --> 01:23:19.040]   Tell tell are you on jr. No host or you're on mast on a social
[01:23:19.040 --> 01:23:22.080]   No, I'm a best on social because I wanted to see what it was like gargra and have
[01:23:22.400 --> 01:23:24.960]   Elastic search turned on there. You must have it on there
[01:23:24.960 --> 01:23:29.120]   Maybe it's too big. It's such a big end. Maybe too big. It's so big
[01:23:29.120 --> 01:23:33.600]   Well, you can run it. I mean this is you asked a question before about scaling one of the problems of mast on is it's not built
[01:23:33.600 --> 01:23:34.960]   It's not designed to scale
[01:23:34.960 --> 01:23:37.680]   It's it's not designed to be able to scale out tiers
[01:23:37.680 --> 01:23:43.120]   And so really I think what we're gonna see is a bunch of work to take these components of like sidekick elastic search
[01:23:43.120 --> 01:23:47.680]   And a lot more work around the docarization the yarn configs the kubernetes configs
[01:23:47.680 --> 01:23:50.640]   So you can have individually scaling teals uh tiers
[01:23:50.960 --> 01:23:56.880]   Um like a big company. What right? But like it's clearly something that they were used to running as it one
[01:23:56.880 --> 01:24:00.320]   We never thought all these people would show up for one thing
[01:24:00.320 --> 01:24:05.680]   Well, and also I think traditionally probably you don't have a lot of developers working on this right who have worked at real scale
[01:24:05.680 --> 01:24:10.720]   Companies and so hobby is a pleasure in the first five years didn't know how to do this
[01:24:10.720 --> 01:24:12.720]   Yeah, well, it was a ruby on rails
[01:24:12.720 --> 01:24:14.800]   Right like like that's done
[01:24:14.800 --> 01:24:18.480]   I mean there's also this question of whether mast on's gonna be able to scale while it's still on ruby
[01:24:18.480 --> 01:24:21.840]   I think it will be fine, but they're just gonna have to create a bunch of auto scaling stuff
[01:24:21.840 --> 01:24:26.960]   So you could go put it up at native us there's a huge amount of technology that goes into something like twitter or facebook
[01:24:26.960 --> 01:24:34.560]   It's actually mind-boggling to think of what the scaling takes it is. Well, you you guys were talking about the leap second last week
[01:24:34.560 --> 01:24:38.400]   Yeah, gosh, you know why that's so important to facebook white care facebook care so much in google
[01:24:38.400 --> 01:24:43.280]   So it's the basis of humongous distributed file systems is
[01:24:43.760 --> 01:24:48.320]   Perfectly perfect resolution gps time so in the roof of every facebook data center
[01:24:48.320 --> 01:24:55.760]   There's a very sensitive gps antenna that goes into custom hardware that facebook gets oemmed from and I believe you can actually download
[01:24:55.760 --> 01:25:01.440]   Uh the patent at least the patent but the designs in the paper they run an nt and ntv server right in the
[01:25:01.440 --> 01:25:05.360]   Facebook runs like a post nt something it's more accurate than ntp
[01:25:05.360 --> 01:25:12.960]   Internally because to do these transactions when you do transactions in the distributed database or distributed file system
[01:25:13.200 --> 01:25:15.200]   You use time stamps to figure out
[01:25:15.200 --> 01:25:21.200]   What the eventual consistency is of that and so you have to have all of your data centers within
[01:25:21.200 --> 01:25:24.320]   nanoseconds of each other and doing that's incredibly hard
[01:25:24.320 --> 01:25:27.680]   It's especially hard if all of a sudden you lose a second or gain a second
[01:25:27.680 --> 01:25:28.800]   Right
[01:25:28.800 --> 01:25:30.880]   Well, they won but that's like the kind of thing
[01:25:30.880 --> 01:25:35.680]   But the nice thing is is the difference is is like facebook and google and these guys grew up before the cloud era
[01:25:35.680 --> 01:25:39.600]   Yeah, so I don't think it would take a lot to build scaling
[01:25:39.680 --> 01:25:44.000]   We know now a little bit better how to do this one you can get it from a to bs like the basic infrastructure
[01:25:44.000 --> 01:25:44.800]   Yeah
[01:25:44.800 --> 01:25:52.480]   Like the basic storage of the database the database tier the auto scaling the load balancing the global load balancing the running of a global edge
[01:25:52.480 --> 01:25:54.400]   All of that stuff is taken care of it
[01:25:54.400 --> 01:25:58.400]   Just mastodon has not been built for that like one of one of the first i'm working on this code change
[01:25:58.400 --> 01:26:03.920]   And it like i'm so bad at ruby but um mastodon doesn't pick up ip addresses from aproxi
[01:26:03.920 --> 01:26:08.640]   So I run my instance in gcp behind the google global edge
[01:26:08.640 --> 01:26:13.360]   And so all of the ip addresses in my logs are wrong because it doesn't realize i'm behind a global edge
[01:26:13.360 --> 01:26:17.440]   It's rewriting the ip's and so basic things like that are you know have to get done
[01:26:17.440 --> 01:26:21.440]   And then eventually a bunch of auto scaling work of all sudden a million people just signed up
[01:26:21.440 --> 01:26:26.800]   Well, it's not a big deal because my system detected that it called into AWS it asked for 10 more
[01:26:26.800 --> 01:26:31.920]   Web tier machines to be spun up and it increased the database memory automatically
[01:26:32.160 --> 01:26:38.160]   One of the things that we are kind of at the mercy of a very thin layer of technologists who understand
[01:26:38.160 --> 01:26:40.800]   this stuff and
[01:26:40.800 --> 01:26:42.240]   uh
[01:26:42.240 --> 01:26:47.760]   It's that's a that's a small number. In fact, I was talking earlier about this. I think i'll be talking more about it
[01:26:47.760 --> 01:26:54.880]   There has been an imperative for every company in the world to digitize to become on to go online
[01:26:54.880 --> 01:27:01.600]   And there aren't enough good technologists to do that. Yeah, so very many websites are crap
[01:27:01.840 --> 01:27:04.240]   Yeah, not just insecure but unusable
[01:27:04.240 --> 01:27:10.240]   Uh a huge number of apps are crap. Um, I this came up for me because our son
[01:27:10.240 --> 01:27:14.720]   At least his son is 19. He's joining the union works for a big grocery store
[01:27:14.720 --> 01:27:16.640]   The grocery store site is crap
[01:27:16.640 --> 01:27:20.400]   So he can't use employee ID the union site is crap
[01:27:20.400 --> 01:27:25.440]   Because he can't log in apparently it doesn't like chrome. He can maybe use the far
[01:27:25.440 --> 01:27:26.480]   I don't know it
[01:27:26.480 --> 01:27:30.960]   Anyway, it ends up with a phone call to a tech support person who doesn't really know what's going on
[01:27:31.280 --> 01:27:36.880]   The stack was probably written by somebody who's long gone. No one knows how it works. And this is universal
[01:27:36.880 --> 01:27:38.320]   Yeah, let's see that
[01:27:38.320 --> 01:27:41.840]   It's very expensive these engineers are very expensive. We run on Drupal
[01:27:41.840 --> 01:27:46.000]   We've just been you know, we've now realized that we're going what is a Drupal 9
[01:27:46.000 --> 01:27:53.440]   Uh, the our the version of Drupal. We're on is being deprecated next year. So it'll be insecure and it cost you a fortune
[01:27:53.440 --> 01:27:59.360]   We had two sites. We had to we basically shut down the tech guy site because it was a quarter of a million dollars
[01:27:59.920 --> 01:28:06.480]   Uh, I think we've negotiated the canopy down to 180 thousand dollars to update the twit site
[01:28:06.480 --> 01:28:11.840]   It's very expensive for companies to do this the people who know how to do this are getting
[01:28:11.840 --> 01:28:15.840]   Are are a thin crust of technologies?
[01:28:15.840 --> 01:28:19.040]   That's what i'm saying earlier is it's like people are
[01:28:19.040 --> 01:28:23.600]   Being educated to jump into app development and and we're users
[01:28:23.600 --> 01:28:25.680]   We're used to start up and stuff
[01:28:25.680 --> 01:28:30.240]   But we need some people that are going to be able to be plumbers as well as yes
[01:28:30.240 --> 01:28:33.920]   Someone that can go in and and do a cable drop if we need to you know
[01:28:33.920 --> 01:28:36.880]   Some folks don't even understand that the idea of just dropping the ether
[01:28:36.880 --> 01:28:38.080]   I just worry about it
[01:28:38.080 --> 01:28:40.160]   And I think we're starting to see this in fact some people
[01:28:40.160 --> 01:28:47.120]   There've been stories about how big tech is enjoying what's happening at twitter because it's disenfranchising the technology class the engineer class
[01:28:47.120 --> 01:28:49.520]   Who have been so powerful?
[01:28:49.520 --> 01:28:54.960]   I don't think it is I think they are powerful and I worry that they were going to come. This is the new elite
[01:28:55.600 --> 01:29:00.880]   Yeah, I mean to the area that I have no best we have this big problem in security of
[01:29:00.880 --> 01:29:05.600]   Every fortune 500 company is now a legitimate target of foreign intelligence
[01:29:05.600 --> 01:29:10.800]   And they're not not see so stick around right and the number of people not just seesos but directors and pictures
[01:29:10.800 --> 01:29:15.120]   Everybody individual threat intel people and instant response folks
[01:29:15.120 --> 01:29:20.960]   And security you know product security folks who have worked at that level who have played against administration
[01:29:20.960 --> 01:29:25.760]   Security who have tested themselves against the gru and the svr and have come out on top
[01:29:25.760 --> 01:29:33.680]   The number of people who in that category in like the thousands and they command massive salaries right right and so that's as they can
[01:29:33.680 --> 01:29:38.400]   Because they can perhaps should and we've we've done a really poor job of creating
[01:29:38.400 --> 01:29:43.680]   Economies a scale where you don't have to have a large team that can operate at that level
[01:29:43.680 --> 01:29:49.040]   But like my guess of the fortune 500 is 150 200 those companies or at least playing the game
[01:29:49.280 --> 01:29:52.000]   Right that if they go up against an adversary of that level
[01:29:52.000 --> 01:29:57.760]   They at least have a possibility of detecting it most of them and then the vast majority of the Russell 2000
[01:29:57.760 --> 01:30:01.280]   Yeah, not at all. Oh you have to do is that was so is switch
[01:30:01.280 --> 01:30:06.800]   Right sony sony was in that lower category before they invested a huge amount of money after their breach
[01:30:06.800 --> 01:30:14.560]   They were so cheap for so long that they got bit right and also I think Japanese companies are particularly have a history here of having of security
[01:30:14.720 --> 01:30:19.120]   security is a tough thing like in kind of Japanese corporate culture of being negative of
[01:30:19.120 --> 01:30:23.280]   You know, it's actually I've worked with some Japanese companies and it's actually it's uh
[01:30:23.280 --> 01:30:24.320]   It's an interesting issue
[01:30:24.320 --> 01:30:29.440]   But all I have to do is look at the number of ransomware attacks the number of breaches to know this is we got a problem
[01:30:29.440 --> 01:30:33.200]   We got a really good problem. All right. Let's talk about the psychology you deal with
[01:30:33.200 --> 01:30:37.120]   I mean, I just I think that it's almost like if you were to go back and get another degree there almost be
[01:30:37.120 --> 01:30:39.760]   Psychology. Yeah, I'm good jeff. Thanks
[01:30:42.800 --> 01:30:47.440]   No, it's true though. I mean you have to be a psychologist. You have to understand the adversary as much as anything else
[01:30:47.440 --> 01:30:52.960]   Yeah, what makes them take faster and honestly we've set this up in such a way that it isn't hard to have good ob sec
[01:30:52.960 --> 01:30:57.680]   It's easier to be an attacker than a defender. I think yeah. Well it and for normal people
[01:30:57.680 --> 01:31:01.520]   It's completely unfair that we digitize all their lives
[01:31:01.520 --> 01:31:06.880]   We push all this tech into their lives and we teach them nothing right and by default. They're not secure
[01:31:06.880 --> 01:31:09.440]   They have to take individual steps. You know, I've had
[01:31:10.400 --> 01:31:18.320]   The thing that's also really blown this up has been the explosion of cryptocurrencies because it's turned a bunch of hacking against individuals and actually super profitable
[01:31:18.320 --> 01:31:20.240]   It used to be if you hacked like a normal dude
[01:31:20.240 --> 01:31:21.120]   Yeah, what do you get?
[01:31:21.120 --> 01:31:25.680]   You could steal if you get access their bank account you get a couple thousand bucks before
[01:31:25.680 --> 01:31:34.560]   Bank of America clamps down. That's right, but I have a father of a personal friend who lost effectively his entire retirement savings over a million dollars
[01:31:34.560 --> 01:31:37.040]   Um in a crypto wallet
[01:31:37.200 --> 01:31:43.280]   Custodial or no, no, uh through a scam where he thought it was safe because it's almost impossible to tell the legit from the non-legit
[01:31:43.280 --> 01:31:50.720]   Especially if you're a boomer. Here's the here's the address to use right here. Exactly. Yeah, so it's uh, it's not a good place for
[01:31:50.720 --> 01:31:54.480]   It is really unfair what we the tech industry do for for individuals right now
[01:31:54.480 --> 01:31:57.920]   It sounds like you think though that the solution is a sort of a centralized
[01:31:57.920 --> 01:32:00.400]   service
[01:32:00.400 --> 01:32:07.120]   Economy for security. I think we need more economies of scale where companies can share security services. Yeah
[01:32:07.280 --> 01:32:13.440]   And so especially the small to medium managed service providers have been a big deal for the the small to medium enterprises
[01:32:13.440 --> 01:32:15.760]   And I think that's one of the good things that's happened over the years
[01:32:15.760 --> 01:32:19.200]   We thank god for our managed service provider. Yeah, I mean honest to god
[01:32:19.200 --> 01:32:21.520]   We have somebody so good that we don't feel unsafe
[01:32:21.520 --> 01:32:24.720]   Yeah, but we're just lucky and the move to the cloud right like yeah
[01:32:24.720 --> 01:32:31.360]   You have to be completely insane to run your own exchange server in your board 2022 like yeah, and a masochist
[01:32:31.360 --> 01:32:34.560]   But people still do it but like
[01:32:34.560 --> 01:32:39.360]   That's the kind of thing like one of the benefits is you can go to google or you can go to microsoft and you can pick a provider and you can
[01:32:39.360 --> 01:32:45.200]   You can amortize their huge security team across a million is google doing a good job security wise
[01:32:45.200 --> 01:32:49.360]   Yeah, I mean not only yes, that's question one and question two is for their users
[01:32:49.360 --> 01:32:55.840]   So for their actual internal security, I think google is the best. I think they're probably the most secure enterprise from the outside
[01:32:55.840 --> 01:33:01.600]   That's how from the outside. Yeah, um they have had you know 2009 was a huge turning point the aurora attacks
[01:33:01.680 --> 01:33:05.920]   Which it's nice to talk to guys of a certain age because I could talk about 2009
[01:33:05.920 --> 01:33:13.680]   To my students is like 2009 all right can be garden, right? It's like when my teachers talk about vietnam
[01:33:13.680 --> 01:33:18.560]   Right, right? Like it's that's what 2009 is that? Um god, that's amazing. Yeah
[01:33:18.560 --> 01:33:21.840]   and so uh
[01:33:21.840 --> 01:33:22.800]   Yeah, um
[01:33:22.800 --> 01:33:28.240]   2009 the aurora attacks against a bunch of silicon valley companies most famously google people's liberation army
[01:33:28.960 --> 01:33:35.120]   google invested massively in security after that like they spent and it all came out of the
[01:33:35.120 --> 01:33:41.680]   Co-CEOs, you know, especially sargay that his experience growing up in the soviet union
[01:33:41.680 --> 01:33:46.960]   Distaste for toiletarian countries. They spent a huge amount of money and they built out
[01:33:46.960 --> 01:33:55.120]   A lot of what we consider kind of standard the whole zero trust model comes from this beyond corp idea that google was the first one to popular
[01:33:55.120 --> 01:34:01.840]   It also eventually prompted them leaving china. Yes. They left china to to their credit. They're still not back
[01:34:01.840 --> 01:34:07.040]   Right like they've they've put their toe in and there's their their employees were revolted
[01:34:07.040 --> 01:34:08.240]   um
[01:34:08.240 --> 01:34:13.040]   And uh, you know compared to that I keep on saying this but people need to be reminded
[01:34:13.040 --> 01:34:16.560]   Apple has done more for the chinese communist party. Absolutely
[01:34:16.560 --> 01:34:20.560]   That any tech company is done for any authoritarian state. Absolutely and of late
[01:34:20.880 --> 01:34:27.920]   They just did something to tamp down protests by blocking airdrop to everyone. Yes, and uh
[01:34:27.920 --> 01:34:33.280]   They're getting a pass on it, but this is the this is the deal with the devil
[01:34:33.280 --> 01:34:36.480]   If you want to be in this country operating this country
[01:34:36.480 --> 01:34:41.840]   You're going to be asked to do things like this. Yeah, and that's what google didn't want to do. So yahoo yahoo got
[01:34:41.840 --> 01:34:46.400]   Proper crap for it for yahoo helped dissidents
[01:34:46.400 --> 01:34:48.720]   Get jailed. Yes
[01:34:48.720 --> 01:34:54.160]   That was so I was the first yahoo executive to testify in congress since that famous jerry yane
[01:34:54.160 --> 01:35:00.000]   Situation where tom lantos a holocaust survivor called him a moral pygmy
[01:35:00.000 --> 01:35:03.520]   While he sits in front of the parents of the chinese dissident who had been
[01:35:03.520 --> 01:35:08.480]   Arrested like wow that is not considered a win from like a
[01:35:08.480 --> 01:35:16.880]   By the way great great number of times. Let's pour one out for john. Yeah for uh jacke spear
[01:35:16.960 --> 01:35:20.080]   jacke spear also who's been shot more times in 50 cent and
[01:35:20.080 --> 01:35:26.960]   Went to pongress. Um, jacke was shot in i was not the examiner when that happened
[01:35:26.960 --> 01:35:28.720]   Oh, yeah, yeah
[01:35:28.720 --> 01:35:31.920]   She's pictures of it in her office when you go there horrific, you know a great rep
[01:35:31.920 --> 01:35:35.120]   I don't know this new guy who was representing us, but jacke was my congresswoman. Yep
[01:35:35.120 --> 01:35:42.080]   Um, so google and then the second part of that is do they it seems to be that they're doing as best they can
[01:35:42.080 --> 01:35:46.880]   For their users as well. I think i think i think it's a really encouraging people to
[01:35:46.880 --> 01:35:52.240]   Do the right thing. I think even selling chrome books. Yes is really that that is a computer
[01:35:52.240 --> 01:35:56.960]   I can point to the least sophisticated user and say you're probably safe using this yes
[01:35:56.960 --> 01:36:03.520]   I mean chrome books is I uh over at about 15 years ago 14 years ago when chrome books were first started remember the
[01:36:03.520 --> 01:36:10.720]   The pixel books like the first oh god don't get us started. Yes, so I went one thing started crying
[01:36:10.720 --> 01:36:12.320]   nostalgia
[01:36:12.320 --> 01:36:16.960]   Um, and so I I'm not neutral here as a consultant. I worked on chrome and on chrome o s
[01:36:16.960 --> 01:36:20.480]   Um, and so you know I got to see all the work that went on the inside
[01:36:20.480 --> 01:36:23.920]   And so I bought those early early chrome books and one thanksgiving
[01:36:23.920 --> 01:36:28.160]   I had took four of them back to sacramento and gave them to my parents and my in-laws
[01:36:28.160 --> 01:36:31.120]   And it was the best investment i've ever made in the thanksgiving
[01:36:31.120 --> 01:36:34.640]   Because it was just like don't have to worry anymore
[01:36:34.640 --> 01:36:42.080]   I don't have to clean all the viruses out and not ask like what websites are you on that you got all these viruses fighting for control of your browser
[01:36:42.080 --> 01:36:48.960]   Yeah, it was great. You know, I think google google was really early in having people focus on the psychology of their users
[01:36:48.960 --> 01:36:54.160]   And of doozy user experience studies. Um, so adrian porter felt uh, who's there?
[01:36:54.160 --> 01:36:56.560]   Um, who's unmasted on by the way
[01:36:56.560 --> 01:37:02.000]   Tabriz was her boss for a long time the head of chrome uh security
[01:37:02.000 --> 01:37:08.000]   Uh, you know, there's several people who are there who are like really early in building that out and I think yeah
[01:37:08.000 --> 01:37:13.600]   I mean, I think as far as security from normal people goes. I think google's doing absolutely the best job right now android
[01:37:13.600 --> 01:37:18.880]   Android's got problems a lot of those problems are due to its openness and and I think and the fact that
[01:37:18.880 --> 01:37:23.920]   Android oem's if you're gonna buy an android phone you have to buy google right like
[01:37:23.920 --> 01:37:29.680]   Taking all the extra crap and then waiting for your oem's to patch. I think is not worth it
[01:37:29.680 --> 01:37:33.200]   Get a pixel. That's right. Tell people if you want to be the android ecosystem get a pixel
[01:37:33.200 --> 01:37:37.440]   And in some ways I think pixels are now more secure than iphones like apple has
[01:37:38.080 --> 01:37:45.840]   Really wasted their lead. Um, you know, nso group owning up iphones over and over again zero click x blights zero click
[01:37:45.840 --> 01:37:47.200]   I message exploits holy
[01:37:47.200 --> 01:37:51.200]   It's just really embarrassing now. I know the people at apple and they're working really hard on this
[01:37:51.200 --> 01:37:58.160]   And they finally are getting it together, but they did not have real executive support for for the fundamental changes
[01:37:58.160 --> 01:38:04.640]   They finally have that it got so embarrassing that this one company in israel owned them over and over and over again
[01:38:04.960 --> 01:38:11.920]   Um that they're finally making the fundamental operating system changes to jail iMessage to reduce the attack surface seems like they did as much as they could
[01:38:11.920 --> 01:38:17.200]   Though I mean at one point that was completely not secure and they've they've done a lot to lock it down
[01:38:17.200 --> 01:38:21.120]   So it's not a lot not great. I mean last last door and last door and one of the promises
[01:38:21.120 --> 01:38:23.760]   They're still writing everything in objective c like they have their own
[01:38:23.760 --> 01:38:30.640]   Memory safe wing that turned out to flop right so like I think part of the fantasy was swift gonna make it better
[01:38:31.200 --> 01:38:34.640]   Well, it would I mean swift is much less vulnerable to these kinds of attacks
[01:38:34.640 --> 01:38:38.720]   But they don't use it for any of their core stuff and the core libraries a lot of the core libraries are still see
[01:38:38.720 --> 01:38:44.720]   Wow, right that they call out they call out too from objective c like a lot of their parsing code for video and
[01:38:44.720 --> 01:38:49.440]   Um, trust that's it. That's it. I think I took they've made a fundamental mistake in their language
[01:38:49.440 --> 01:38:53.520]   It's interesting you can make that cultural mistake 10 years ago and you're still living
[01:38:53.520 --> 01:38:58.880]   With it right right and so now I mean the people who are being smart about this is google is one of the companies
[01:38:58.880 --> 01:39:02.640]   That's been really pushing rust. Um, and so finally there was rust support
[01:39:02.640 --> 01:39:09.440]   I think in chrome and there's rust support in the linux kernel and so I think like moving to the memory safe languages is where we have to go
[01:39:09.440 --> 01:39:14.640]   It's always something that has a five-year time horizon. So start now if we had done it five years ago
[01:39:14.640 --> 01:39:18.240]   We'd be in way better shape right, but that's it hopeless at Microsoft
[01:39:18.240 --> 01:39:22.000]   Okay
[01:39:22.000 --> 01:39:24.640]   There's a lot of really good people at Microsoft
[01:39:25.040 --> 01:39:27.840]   Um, we have we know some of them some of them work with us
[01:39:27.840 --> 01:39:34.080]   I think one of the problems with Microsoft is that they still sell all of their old enterprise stuff and they
[01:39:34.080 --> 01:39:39.120]   Legacy the legacy stuff is in the level of sustained engineering. That's not appropriate. Yeah, the
[01:39:39.120 --> 01:39:40.800]   Chinese
[01:39:40.800 --> 01:39:45.440]   The people's liberation army keeps on owning up exchange. Yep over and over and over again
[01:39:45.440 --> 01:39:50.960]   You have these attacks where tens of thousands of companies are being hit by the Chinese and their exchange servers take it over
[01:39:50.960 --> 01:39:54.960]   That's not acceptable. Um, I mean my I think being
[01:39:55.040 --> 01:40:02.080]   On office 365 is fine for corporate side. We're on Microsoft 365 servers not right make it their problem
[01:40:02.080 --> 01:40:06.240]   Yeah, well one of the problems here is you should never run a Microsoft product that Microsoft doesn't use
[01:40:06.240 --> 01:40:10.000]   Right, they use
[01:40:10.000 --> 01:40:13.520]   You're on Microsoft 365
[01:40:13.520 --> 01:40:18.160]   Yeah, we're on uh, Microsoft 365 e5. Yeah, so one you have to pay for e5
[01:40:18.160 --> 01:40:24.320]   Unfortunately, like all the all the levels below that are less secure. They don't have all the security features. Um, but yeah
[01:40:24.480 --> 01:40:26.480]   Yeah, I mean, it's
[01:40:26.480 --> 01:40:29.440]   Unfortunately, like if you're a consultant and you're working with fortune 500 you have to be you have to be
[01:40:29.440 --> 01:40:33.920]   Because they're all share point. They're all office. And so if you want to collaborate them, you have to be on 365
[01:40:33.920 --> 01:40:36.880]   But so I think that's a company. Oh wait, Microsoft
[01:40:36.880 --> 01:40:41.120]   Go ahead. Go ahead. Go ahead. Go ahead. Have you go ahead because I was a change subject to another company
[01:40:41.120 --> 01:40:47.280]   Or with Microsoft knowing the knowing exchange is getting attacked so often and not really doing anything about it
[01:40:47.280 --> 01:40:50.960]   That's just more leverage for them to say, huh? Come on over to the Azure side of things
[01:40:50.960 --> 01:40:54.560]   Right, but they'll never make the cut right like I would love for them to end of life
[01:40:54.560 --> 01:40:57.840]   They're like just like historically
[01:40:57.840 --> 01:40:59.840]   They should just announce that
[01:40:59.840 --> 01:41:03.920]   Yeah, exchanges end of life. They never want to cut off legacy because that's a big part of their business
[01:41:03.920 --> 01:41:07.680]   Right and then they've done a couple of really cool experimental things like the windows s stuff
[01:41:07.680 --> 01:41:09.280]   Which is like the windows chrome book
[01:41:09.280 --> 01:41:13.920]   Which I wish they had really committed to because we're now nobody wants everybody turned it off
[01:41:13.920 --> 01:41:18.240]   Yeah, I think they made some they should have committed to it like chrome book. They should have said this is it
[01:41:18.320 --> 01:41:23.600]   That's all you get. There's no turning it off. Right. And so like and you know again the year of our lord 2022
[01:41:23.600 --> 01:41:26.080]   We're still running like
[01:41:26.080 --> 01:41:27.280]   32 bit
[01:41:27.280 --> 01:41:33.600]   Compiled executables. Yeah, um and and they have to do all this stuff to make the the nt4 api
[01:41:33.600 --> 01:41:37.920]   You know nt dll has barely changed when I did that work at microsoft as a consultant
[01:41:37.920 --> 01:41:41.840]   You see the source code headers and this is like in the 2003 2004 range
[01:41:41.840 --> 01:41:46.800]   And so you look at the people who had worked on those headers as like interns and engineers
[01:41:47.200 --> 01:41:50.800]   And they at that point were evps right and so now those people are retired
[01:41:50.800 --> 01:41:52.800]   Right the people who actually wrote nt dll
[01:41:52.800 --> 01:41:56.160]   All right and all of the api stuff that they have to do all this work to maintain compatibility
[01:41:56.160 --> 01:42:00.640]   But then using virtual is they do all this really smart but kind of hacky stuff
[01:42:00.640 --> 01:42:06.240]   Using virtualization especially to try to maintain backwards compatibility with the win 32 api
[01:42:06.240 --> 01:42:10.720]   And then also reduce the possibility of a single piece of of software going crazy
[01:42:10.720 --> 01:42:14.880]   Alex stamos or guests told on second i don't take a little break
[01:42:14.880 --> 01:42:21.200]   You can try another company and I still have to get to end end-to-end encryption because that's really yes an important topic
[01:42:21.200 --> 01:42:25.360]   That'll probably be how we'll wrap it up because we've gone for a long time already
[01:42:25.360 --> 01:42:27.360]   And we could go out
[01:42:27.360 --> 01:42:35.520]   I do he's trapped he crams in all kinds of knowledge in a short amount of time
[01:42:35.520 --> 01:42:39.680]   It's so great, but I tell you what it's a rich stew of information. I'm enjoying every minute
[01:42:40.160 --> 01:42:47.280]   Uh ks group is his consultancy. He's a professor adjunct professor and the leader of the stanford
[01:42:47.280 --> 01:42:50.080]   You have to say adjunct they get really pissy about that if you're
[01:42:50.080 --> 01:42:54.240]   If you pretend to be a real professor, I'm a adjunct is latin for fake
[01:42:54.240 --> 01:42:58.000]   So but i'm not wrong in using it
[01:42:58.000 --> 01:43:00.400]   You're a real professor are you real?
[01:43:00.400 --> 01:43:01.600]   I'm a fake i'm a fake i'm a faker
[01:43:01.600 --> 01:43:06.080]   I know because my father was a professor and so we know about all these
[01:43:06.560 --> 01:43:09.840]   Assistant professors associate professors adjunct professors
[01:43:09.840 --> 01:43:14.240]   I'm not allowed to live on campus. Yes. That's how you know you're a real professor when they give you a house
[01:43:14.240 --> 01:43:16.080]   Oh that house those houses are nice. Let me go
[01:43:16.080 --> 01:43:22.800]   Director you should get it as director of the stanford internet observatory. At least i think i do not want those neighbors
[01:43:22.800 --> 01:43:26.880]   Very high falu very high falu
[01:43:26.880 --> 01:43:32.400]   Uh, we will have a little bit more with Alex jeff will get to ask his question in just a second
[01:43:33.200 --> 01:43:35.680]   But first a word, you know, this is actually really timely
[01:43:35.680 --> 01:43:38.240]   uh from secure works
[01:43:38.240 --> 01:43:40.240]   We were talking about this whole idea of
[01:43:40.240 --> 01:43:43.680]   kind of uh, you know, there's never enough
[01:43:43.680 --> 01:43:49.120]   People there's never enough time. There's never enough money to really do security right
[01:43:49.120 --> 01:43:52.720]   That's why you got to think about secure works as part of your
[01:43:52.720 --> 01:43:55.360]   security team
[01:43:55.360 --> 01:44:02.160]   Secure works is a leader in cyber security building solutions for security experts by security experts
[01:44:02.240 --> 01:44:05.840]   There your partner secure works offers superior threat detection
[01:44:05.840 --> 01:44:11.520]   Rapid incident response all while making sure customers are never locked into a single vendor
[01:44:11.520 --> 01:44:19.680]   They've got a really powerful and actually I think it's fast and an extended detection and response platform. They call it tages xdr
[01:44:19.680 --> 01:44:26.880]   Uh, and I think if you just listen to show you know why it's time to get it right in 2022 cyber crimes are going to cost the world
[01:44:26.880 --> 01:44:31.600]   $7 trillion in three years that number is going to grow to 10.5 trillion
[01:44:32.320 --> 01:44:35.520]   In 2021 ransome wear totaled as far as we know
[01:44:35.520 --> 01:44:40.560]   20 billion in damages attacks occur to levery 11 seconds by 20 31
[01:44:40.560 --> 01:44:42.240]   It's going to be
[01:44:42.240 --> 01:44:43.600]   Much much worse
[01:44:43.600 --> 01:44:45.200]   265 billion a year
[01:44:45.200 --> 01:44:48.400]   Strike every two seconds. I think that might even be a conservative estimate
[01:44:48.400 --> 01:44:53.600]   Make sure your organization is not the next victim with secure works tages xdr
[01:44:53.600 --> 01:44:59.360]   Tages provides superior detection identifying get this this is where i'm fascinated
[01:45:00.000 --> 01:45:03.600]   470 billion security events a day a day
[01:45:03.600 --> 01:45:07.040]   Then prioritizing the true positive alerts
[01:45:07.040 --> 01:45:09.920]   eliminating alert noise
[01:45:09.920 --> 01:45:14.640]   But giving you the information you need to focus on the real threat
[01:45:14.640 --> 01:45:23.520]   But there's more tages offers unmatched response automated response actions to eliminate threats before the damage is even done
[01:45:23.520 --> 01:45:27.440]   That's fast with secure works tages managed xdr
[01:45:27.440 --> 01:45:31.760]   You can easily leverage secure works experts to investigate and respond to threats on your behalf
[01:45:31.760 --> 01:45:37.680]   This is where they become a part of your team. You can cut dwell times decrease operational burden reduce cost
[01:45:37.680 --> 01:45:40.800]   24 7 by 365 coverage
[01:45:40.800 --> 01:45:45.360]   So if you experience a christmas day security event or half your team is out sick
[01:45:45.360 --> 01:45:48.800]   Don't worry secure works is there they're behind you
[01:45:48.800 --> 01:45:54.720]   And of course with companies facing this shortage. We're just talking about a cyber security talent
[01:45:55.360 --> 01:45:58.480]   Secureworks access is an extension of your security team on day one
[01:45:58.480 --> 01:46:05.280]   alleviating cyber security talent gaps letting you customize the approach get the coverage level you need
[01:46:05.280 --> 01:46:07.120]   Now
[01:46:07.120 --> 01:46:09.120]   And and Alex was just saying this is the fun part
[01:46:09.120 --> 01:46:13.120]   What happens if you think you've been breached you find an intruder in your system
[01:46:13.120 --> 01:46:18.080]   I want you to get a post a note right now and write down this number 1 800 breached
[01:46:18.080 --> 01:46:22.640]   Put it on your put it right there on your desk 1 800 breached
[01:46:23.280 --> 01:46:28.640]   That connects whether you're a customer or not that connects you with the secure works emergency incident response team
[01:46:28.640 --> 01:46:34.480]   They can provide you with immediate assistance. It's so nice to know these guys are on your behalf there on your behalf
[01:46:34.480 --> 01:46:36.240]   24 7
[01:46:36.240 --> 01:46:40.880]   Responding to and remediating a possible cyber incident or data breach
[01:46:40.880 --> 01:46:44.320]   You don't have to go it on your own course. You've got a great team
[01:46:44.320 --> 01:46:51.600]   I'm not saying that just make it better with secure works like secure works you can learn more about the way today's threat environment is evolving
[01:46:52.000 --> 01:46:55.120]   The risks that can present to your organization. They've got case studies
[01:46:55.120 --> 01:47:04.720]   They've got reports from their very prestigious counter threat unit and a lot more visit secure works.com slash twit get a free trial of tages x dr
[01:47:04.720 --> 01:47:07.040]   secure works
[01:47:07.040 --> 01:47:14.480]   Dot com slash twit. Please use that a URL so they know you saw here secure works dot com slash twit twit twit
[01:47:14.480 --> 01:47:16.720]   secure works
[01:47:16.720 --> 01:47:18.720]   defending every corner
[01:47:18.720 --> 01:47:20.720]   of cyberspace
[01:47:20.720 --> 01:47:22.720]   It's exactly what we're just talking about
[01:47:22.720 --> 01:47:28.240]   All right, jeff you had a question about another another company for our esteemed guest. Oh, it's just obvious
[01:47:28.240 --> 01:47:31.840]   Brief gossip. Do you know anybody left at facebook? Do you hear it?
[01:47:31.840 --> 01:47:38.560]   What do you hear? Why do you know the whole thing got steamrolled with twitter blowing up and 11 000 people a lot of people
[01:47:38.560 --> 01:47:40.800]   I know who were there are gone. What are you here?
[01:47:40.800 --> 01:47:42.960]   Yeah, I mean so
[01:47:42.960 --> 01:47:48.800]   They're out of the news somebody pointed out like for the first time ever facebook was not anywhere on the front page of tech meme, right?
[01:47:48.800 --> 01:47:50.640]   Wow
[01:47:50.640 --> 01:47:56.480]   I mean, nobody I said this at the beginning nobody was going to be as happy about this acquisition as marx ockaberg
[01:47:56.480 --> 01:47:57.920]   um
[01:47:57.920 --> 01:48:00.320]   Because you know he the heat is off
[01:48:00.320 --> 01:48:05.680]   His argument is this is a super hard problem. Yeah, we're doing the best we can
[01:48:05.680 --> 01:48:09.680]   Yeah, and unless you've got somebody who just doesn't give a at all
[01:48:09.680 --> 01:48:12.400]   It's hard to make that argument right and
[01:48:12.400 --> 01:48:16.640]   But how he's got that person now he's got that person. He's like, oh, oh, you don't like me
[01:48:16.640 --> 01:48:19.120]   I think you really want to talk to this guy
[01:48:19.280 --> 01:48:21.920]   Yeah, um, so yeah, I mean it is
[01:48:21.920 --> 01:48:29.280]   What's going on there? I mean I think there are some negative trends you know facebook's not doing great from a financial perspective the stock is way down
[01:48:29.280 --> 01:48:36.480]   Um, they had the layoffs a couple people I know got laid off for the most part the security and safety teams were spared from the layoffs
[01:48:36.480 --> 01:48:40.320]   Uh, so the vast majority of the security people I know they're still there
[01:48:40.320 --> 01:48:42.640]   The folks working with newspapers are gone
[01:48:42.640 --> 01:48:48.000]   Right. I think a combination of two things though is worrying me from a trust and safety perspective
[01:48:48.640 --> 01:48:50.480]   one the
[01:48:50.480 --> 01:48:52.480]   I'm a little afraid that
[01:48:52.480 --> 01:48:53.840]   uh
[01:48:53.840 --> 01:49:00.400]   A lot of the investment that was made in the post kind of 2018 time frame into trust and safety work
[01:49:00.400 --> 01:49:08.080]   Which was significant was really based upon unsustainable revenue numbers for facebook and that that will be a place that they cut first
[01:49:08.080 --> 01:49:10.000]   um second
[01:49:10.000 --> 01:49:11.840]   whenever
[01:49:11.840 --> 01:49:16.640]   Mark second word loves to be in a corner, right like he loves to be in the situation where people tell him
[01:49:17.040 --> 01:49:21.280]   I think he doesn't like the fat and happy times. I think he likes the times when people tell him
[01:49:21.280 --> 01:49:26.480]   You can't do this you're going to fail right like the story you always hear like in the
[01:49:26.480 --> 01:49:29.840]   uh, you know, I went through engineering bootcamp, right?
[01:49:29.840 --> 01:49:35.280]   Uh, and um got my butt kicked by all these 23 year olds who had just graduated in my tea, right?
[01:49:35.280 --> 01:49:41.280]   But i'm going through like engineering bootcamp and they tell all these stories of the good old days and um
[01:49:41.280 --> 01:49:45.760]   You know one of my concerns about facebook at the time was it's a very small c conservative company
[01:49:46.080 --> 01:49:51.760]   About looking backwards of the good old days when it was only 300 people in the bad office in palalto
[01:49:51.760 --> 01:49:54.640]   And you know the 2010 or so
[01:49:54.640 --> 01:49:58.400]   2011 people are like you'll never make money you'll never make the move to mobile
[01:49:58.400 --> 01:50:02.480]   And that's when mark segabord was happiest when we could crush it on mobile right?
[01:50:02.480 --> 01:50:07.840]   Proving people wrong. Yeah proving people wrong. So I think he's always looking for that moment where he can prove people wrong
[01:50:07.840 --> 01:50:11.200]   and um, i'm a little afraid that
[01:50:11.920 --> 01:50:16.960]   We are past that from the safety perspective that people said you can't get a handle of these things
[01:50:16.960 --> 01:50:19.040]   And they did better
[01:50:19.040 --> 01:50:21.920]   I have my issues with what the company has done in a number of ways
[01:50:21.920 --> 01:50:26.400]   But they invested a huge amount of money that he has now moved past that and is back to now
[01:50:26.400 --> 01:50:29.120]   Can we make the metaverse work and in doing that?
[01:50:29.120 --> 01:50:33.760]   That the is I is off the ball on the fact that they have three billion users
[01:50:33.760 --> 01:50:36.720]   That are using their current products that are not in the metaverse
[01:50:36.720 --> 01:50:37.680]   um
[01:50:37.680 --> 01:50:41.680]   When they did the the meta thing I I was actually first happy because I thought it would be an
[01:50:41.680 --> 01:50:47.440]   alphabet thing where facebook became its own company and had its own ceo like a chris cocks
[01:50:47.440 --> 01:50:51.280]   Running it and then zuck would be the ceo of meta overall
[01:50:51.280 --> 01:50:55.360]   But zuck still runs the facebook product as well as all the metaverse stuff
[01:50:55.360 --> 01:51:01.360]   And I think like he's probably too distracted there. Um, and so, uh, I'm worried about the distraction
[01:51:01.360 --> 01:51:03.120]   I'm worried about the spend of money
[01:51:03.120 --> 01:51:06.480]   The other thing that's happened is since the hogging documents were
[01:51:06.960 --> 01:51:12.560]   Reliqued so there were some like legitimate scandals in those documents most of those documents are people trying to do their job
[01:51:12.560 --> 01:51:14.640]   Which is a hard job well and
[01:51:14.640 --> 01:51:22.000]   The leak of those documents has massively changed the internal culture at facebook where there's no longer any openness about talking about
[01:51:22.000 --> 01:51:24.720]   Safety issues. Yes integrity issues
[01:51:24.720 --> 01:51:28.800]   And the long-term impact of the that document leak has been that a bunch of people
[01:51:28.800 --> 01:51:31.120]   I think have lost their jobs are not being hired
[01:51:31.120 --> 01:51:34.560]   That there's not going to be investment in quantitative social scientists to do that kind of work
[01:51:34.800 --> 01:51:38.720]   And that I think is a huge loss for the entire industry because facebook was really the only company
[01:51:38.720 --> 01:51:43.520]   Doing that at that level and we we've seen the peak of that and it's over
[01:51:43.520 --> 01:51:48.000]   And now if you're zuck you're like, well, doesn't matter because doing anything is better than you on musk, right?
[01:51:48.000 --> 01:51:53.840]   So like he now has somebody who's so much worse than him who just straight up overrides his team
[01:51:53.840 --> 01:51:57.200]   Who the bar has been lowered bars loaded
[01:51:57.200 --> 01:51:58.960]   I mean twitter's gonna have
[01:51:58.960 --> 01:52:05.840]   Either a child safety scandal or they're gonna have a breach right like that you can't run a company of that size of that complexity
[01:52:05.840 --> 01:52:12.240]   With that small of a team the amount of risk he is running every day is an unsustainable amount of risk that if he had a board of directors
[01:52:12.240 --> 01:52:13.760]   Which he doesn't
[01:52:13.760 --> 01:52:17.920]   Their audit committee should be losing their minds right now, but they don't ease the audit committee, right? So
[01:52:17.920 --> 01:52:20.400]   Um from from zuck's perspective
[01:52:20.400 --> 01:52:25.680]   I think the pressure is really off and so I am really worried about where we're going now that we might have seen the peak
[01:52:26.080 --> 01:52:30.640]   With like the 2020 election was the peak of focus on integrity issues at big companies
[01:52:30.640 --> 01:52:34.800]   And it's just been downhill since then do you think we're gonna have trouble in the 2020 for election?
[01:52:34.800 --> 01:52:36.640]   Uh
[01:52:36.640 --> 01:52:41.120]   It can go both ways. I mean the the upside of what happened in 2022 is the election deniers lost
[01:52:41.120 --> 01:52:44.800]   Right so like in a world where a bunch of election deniers
[01:52:44.800 --> 01:52:49.360]   Secretary of the states won and it was successful for people to election deny
[01:52:49.360 --> 01:52:52.000]   But the fact
[01:52:52.320 --> 01:52:56.480]   The tech stuff has always been downstream of the culture. I think this is one of the places where
[01:52:56.480 --> 01:52:59.200]   A lot of the popular
[01:52:59.200 --> 01:53:03.760]   You might call the the New York Times position on tech is wrong of that
[01:53:03.760 --> 01:53:10.800]   They see the tech as leading the issue whereas I see the tech as reflecting culture the culture the political culture of the united states
[01:53:10.800 --> 01:53:14.480]   And so the shift in the political culture is going to make things better
[01:53:14.480 --> 01:53:19.600]   Not because the companies are doing better just because there's gonna be a lot less demand. It's good news. Yeah, um
[01:53:20.720 --> 01:53:22.880]   I don't think that will be true outside of the united states
[01:53:22.880 --> 01:53:30.560]   And I think um the most important country to keep an eye on in any of these of the retreat of democracy and of tech regulation is india
[01:53:30.560 --> 01:53:32.960]   Right india is legitimately democracy
[01:53:32.960 --> 01:53:38.000]   Mody received roughly as many votes as there are people in the united states, right?
[01:53:38.000 --> 01:53:43.520]   Like he was legitimately elected and he's using the power that was legitimately given to him to crush his enemies
[01:53:43.520 --> 01:53:45.440]   Using the power of the indian state, right?
[01:53:45.440 --> 01:53:48.880]   And that is like a serious problem for these tech companies
[01:53:49.360 --> 01:53:52.240]   Especially if you're facebook and twitter they are locked out of china
[01:53:52.240 --> 01:53:55.680]   It is very hard to be locked out of china and india
[01:53:55.680 --> 01:53:58.400]   So because they are locked out of the entire train's market
[01:53:58.400 --> 01:54:01.440]   Especially for facebook because it's still meta is still a public company
[01:54:01.440 --> 01:54:03.760]   Must can do what he wants to do but for meta
[01:54:03.760 --> 01:54:07.360]   It's a big deal for your shareholders that you're locked out of china
[01:54:07.360 --> 01:54:11.920]   To also be locked out of india would be unacceptable and that gives a huge amount of leverage in japanese
[01:54:11.920 --> 01:54:18.880]   Yeah, so yeah, your your choices are limited well and access to the market right of access to those billions people
[01:54:18.880 --> 01:54:21.280]   So, um, yeah, i'm less worried
[01:54:21.280 --> 01:54:24.880]   I'm a little worried about 2024. I have a foreign policy piece i'm working on right now
[01:54:24.880 --> 01:54:29.520]   Of one of the problems is the entire team at twitter that does looks for foreign interference is gone
[01:54:29.520 --> 01:54:34.400]   So if you are the ministry state security if you're the people's liberation army if you're the gru of russia
[01:54:34.400 --> 01:54:37.920]   Um, if you're the iranities revolutionary guardcore
[01:54:37.920 --> 01:54:39.760]   twitter is your number one focus
[01:54:39.760 --> 01:54:44.800]   I think the one after twitter is probably telegram because telegram is now the most important platform
[01:54:45.280 --> 01:54:50.320]   For the extreme elements in us society. So if you want to manipulate people who might actually turn violent
[01:54:50.320 --> 01:54:53.680]   Then you're investing heavily in telegram. Not much we could do about telegram
[01:54:53.680 --> 01:54:58.880]   Not much you can do about telegram and and have el duroves russian citizen living in do by
[01:54:58.880 --> 01:55:03.360]   Right. He's completely out of our reach. Yes. And you i think you could call
[01:55:03.360 --> 01:55:06.400]   His facebook status with poutine is it's complicated
[01:55:06.400 --> 01:55:08.000]   Right. Yeah
[01:55:08.000 --> 01:55:11.280]   He left russia and fled russia, but mostly for like financial things
[01:55:11.600 --> 01:55:16.000]   Not really political. It's not clear. How to put and stole his company. Yeah, right?
[01:55:16.000 --> 01:55:20.080]   So i'm presuming he doesn't have a lot of love for the regime, right?
[01:55:20.080 --> 01:55:26.880]   But i also i don't think the guys that thomas jefferson, right like i agree. Yeah, you know, and so um, but also even if you were
[01:55:26.880 --> 01:55:31.200]   That platform is kind of intractable. It's a messaging platform. That's pretty hard to police
[01:55:31.200 --> 01:55:36.960]   Well, it's messaging. It's not like the the large groups there are very large groups. Yeah, hundreds of people group
[01:55:37.760 --> 01:55:40.240]   Those you know in our monitoring of election disinformation
[01:55:40.240 --> 01:55:45.360]   There's a i mean one thing that's happened post january six is you have a huge fracturing on the american right
[01:55:45.360 --> 01:55:47.760]   Of social media platforms, right?
[01:55:47.760 --> 01:55:52.880]   gab parlor truth rumble is a big one to watch rumble is competing with youtube
[01:55:52.880 --> 01:55:56.720]   um and uh has effectively no controls on disinformation
[01:55:56.720 --> 01:55:59.040]   Um, uh
[01:55:59.040 --> 01:56:02.160]   But telegram is where we saw people doxing
[01:56:02.720 --> 01:56:08.720]   Pole workers talking about taking their guns to protest stuff like that. So if you're trying to actually drive americans to violence
[01:56:08.720 --> 01:56:13.200]   That's what the organization is happening. And the problem for us is the russians are all over telegram
[01:56:13.200 --> 01:56:20.320]   Is the most important platform for the ukraine invasion, right? So if you're the gru you have a huge team or a dedicated telegram
[01:56:20.320 --> 01:56:25.200]   I think right now one of the things we benefited in 2022 is they've the russians have their own problems
[01:56:25.200 --> 01:56:30.960]   Right. They are focused on the war they are losing in ukraine and the drop of support for poutine domestically
[01:56:32.400 --> 01:56:39.360]   I think a lot of what happens in 2024 vis-a-vis russia will depend on what their domestic political situation looks like just like jina
[01:56:39.360 --> 01:56:41.600]   Yes
[01:56:41.600 --> 01:56:43.600]   settle and bet for us. Okay
[01:56:43.600 --> 01:56:47.200]   So i won't put words in your mouth jeff
[01:56:47.200 --> 01:56:49.920]   Um, why why big this day?
[01:56:49.920 --> 01:56:56.640]   Uh, but you well you you stand in defense of of tech and tech platforms
[01:56:56.640 --> 01:57:01.440]   Um, i stand in defense of the internet of the internet platforms happen to run it now. Yeah
[01:57:01.920 --> 01:57:04.640]   um, I guess that the the bet is the debate is
[01:57:04.640 --> 01:57:07.520]   who's responsible for
[01:57:07.520 --> 01:57:14.560]   Keeping people well and for who is it is it my responsibility as an individual?
[01:57:14.560 --> 01:57:22.320]   Not to fall for this or is it the platforms responsibility and government's responsibility to protect us from disinformation
[01:57:22.320 --> 01:57:27.760]   Who's responsible? Okay. So I don't think there's a right answer here. I think everybody has to have their own kind of
[01:57:28.960 --> 01:57:34.160]   A lot of people will say well look you just got to train people with critical thinking so they're believing both they need to start
[01:57:34.160 --> 01:57:39.120]   You know treating stuff from the internet is not from the mouth of god
[01:57:39.120 --> 01:57:41.920]   Uh, we can teach people that
[01:57:41.920 --> 01:57:44.560]   I always say for for all trust and safety issues
[01:57:44.560 --> 01:57:52.720]   The responsibility of tech companies is not to make things worse, right? Yeah, everything we're talking about here propaganda disinformation
[01:57:52.720 --> 01:57:57.360]   The abuse of children terrorism, right hate racism
[01:57:58.080 --> 01:58:04.000]   Not exactly new things certainly not new in american society. All really deep-seated
[01:58:04.000 --> 01:58:09.360]   Pathologies of human civilization and some of them are specific pathologies of america
[01:58:09.360 --> 01:58:12.720]   Right based upon the original sins of the united states
[01:58:12.720 --> 01:58:14.400]   um
[01:58:14.400 --> 01:58:15.920]   and
[01:58:15.920 --> 01:58:17.920]   And so the responsibility of the companies
[01:58:17.920 --> 01:58:22.080]   I don't think is to make people better or to solve those problems
[01:58:22.080 --> 01:58:26.000]   But it should be to not make the problem worse to not weaponize disinformation
[01:58:26.320 --> 01:58:32.160]   To not build their products in a way that it creates a unique advantage for their products for those things to happen
[01:58:32.160 --> 01:58:37.920]   Right. There you go. That's how I think of it. Um, an algorithm for instance, which exacerbates this
[01:58:37.920 --> 01:58:42.080]   So I have an anecdote here, right?
[01:58:42.080 --> 01:58:48.000]   So like, uh when I first joined facebook the number one trust and safety problem was was terrorism was isis particularly right?
[01:58:48.000 --> 01:58:53.040]   So like, you know, islamic terrorism not a new thing, but isis was unique of being like the first millennial
[01:58:53.680 --> 01:58:59.840]   Um terrorism group where they were internet first in their propaganda strategy in a way that nobody before had been
[01:58:59.840 --> 01:59:07.040]   Very very smart in their use of the internet to radicalize and to recruit especially disaffected european muslim men, right?
[01:59:07.040 --> 01:59:12.960]   Um, it was highly just mainstream all around. Yes, any and everybody knew isis
[01:59:12.960 --> 01:59:16.800]   Very good, um, production values for their disinformation
[01:59:16.800 --> 01:59:22.880]   For the propaganda very good distribution names and the ability to make it seem relevant to people's lives
[01:59:23.360 --> 01:59:26.960]   In a way that like an al-kaida. Yeah, like the first internet first
[01:59:26.960 --> 01:59:31.840]   Terrorist group, right? Yeah, it's fascinating. I never thought of it that way, but you're right. Yeah, yeah
[01:59:31.840 --> 01:59:33.680]   And so when I got to facebook that was the big thing
[01:59:33.680 --> 01:59:38.400]   Right was isis and I was in a meeting because one of our key executives had come back from the uk
[01:59:38.400 --> 01:59:40.960]   There had been a horrible murder in the uk
[01:59:40.960 --> 01:59:46.800]   Where the the people who had committed the murder the terrorists who had done so had been hanging out on facebook
[01:59:46.800 --> 01:59:49.600]   Now they were also under surveillance by mi5
[01:59:49.760 --> 01:59:54.240]   But the uk government kind of covered that up right the conservative government they blamed facebook for this attack
[01:59:54.240 --> 01:59:59.040]   And so our executive had been screened at by david david david cameron the prime minister of the uk
[01:59:59.040 --> 02:00:02.400]   as being personally responsible for the death of this
[02:00:02.400 --> 02:00:05.120]   British off-duty British service member
[02:00:05.120 --> 02:00:08.400]   And so we're in this big executive meeting
[02:00:08.400 --> 02:00:14.480]   And somebody asked we're talking about we need to really upgrade our fight against isis we need to figure out what to do
[02:00:14.480 --> 02:00:19.120]   Which I think was correct, but somebody asked okay. What's our goal right like this is a tech company
[02:00:19.600 --> 02:00:24.320]   You got to think about your goal is and then you could try to come up with okars that you can manage that goal
[02:00:24.320 --> 02:00:28.800]   And that executive who just got yelled at said our goal is to defeat terrorism
[02:00:28.800 --> 02:00:32.400]   And do as credit my boss was general counsel was like whoa
[02:00:32.400 --> 02:00:37.120]   That's not facebook's job. Let's pump the brakes. Yeah, he's like that is not our job
[02:00:37.120 --> 02:00:41.920]   Our job is that we don't make terrorist lives easier, right? Right, but
[02:00:41.920 --> 02:00:48.640]   whether isis is defeated or not is something that is out of our hands and if we make that our goal
[02:00:49.440 --> 02:00:52.400]   Then we will lose and we will lose ourselves, right?
[02:00:52.400 --> 02:00:54.560]   And I think that's true for all these issues
[02:00:54.560 --> 02:00:58.720]   Is that you can't say we're going to solve the problems about american democracy?
[02:00:58.720 --> 02:01:03.920]   We're going to solve this is where you know the the big controversy around twitter and disinformation was the hunter Biden laptop story, right?
[02:01:03.920 --> 02:01:06.080]   Which again only taking down for a couple of hours
[02:01:06.080 --> 02:01:14.480]   Created a massive striza and effect the idea that twitter doing that affected the election is ridiculous for any kind of quantitative measurement
[02:01:14.880 --> 02:01:18.640]   But okay, but I think it was a mistake by twitter. Yeah, and the reason twitter made the mistake
[02:01:18.640 --> 02:01:21.600]   Which you all says and jack says
[02:01:21.600 --> 02:01:26.080]   But you can understand how they got there because when you look at the 2016 campaign by the russians
[02:01:26.080 --> 02:01:29.040]   There was the online trolling stuff that the companies
[02:01:29.040 --> 02:01:33.920]   Really responded to that was a pure online, but there was also a hacking leak campaign
[02:01:33.920 --> 02:01:39.040]   Who was the target of the hacking leak campaign? It was not facebook and twitter. It was the near times it was cnn
[02:01:39.040 --> 02:01:42.000]   It was a washing post it was especially fox news, right? And
[02:01:42.640 --> 02:01:48.320]   So if you're the problem there is that twitter was thinking we're going to defeat foreign interference in our elections
[02:01:48.320 --> 02:01:52.800]   And in doing so they took upon themselves a responsibility that was not their responsibility
[02:01:52.800 --> 02:01:58.720]   Which is keeping the new york post for being manipulate now in the end it turns out the new york post was not being manipulated at the time
[02:01:58.720 --> 02:02:00.960]   It was not a crazy
[02:02:00.960 --> 02:02:03.200]   Assumption it seemed very suspicious
[02:02:03.200 --> 02:02:07.200]   But they took upon a responsibility for which they did not have the appropriate data
[02:02:07.200 --> 02:02:10.320]   They did not have the appropriate kind of like democratically controlled
[02:02:10.800 --> 02:02:16.320]   Mechanisms it was not their responsibility and as a result they overreached and they caused much bigger problems
[02:02:16.320 --> 02:02:19.840]   So ultimately their mistake was taking on too much to try to do too much
[02:02:19.840 --> 02:02:24.880]   Right they should have restrained themselves to yeah, we will we will stop foreign interference on twitter
[02:02:24.880 --> 02:02:31.600]   Right and I think another reasonable thing if you're a tech company is we will help smaller tech companies do better by sharing information
[02:02:31.600 --> 02:02:34.000]   Sharing tools sharing intelligence
[02:02:34.000 --> 02:02:39.200]   Right, but we're not going to solve the problem the problem of foreign interference in us elections
[02:02:39.520 --> 02:02:44.480]   Is actually a universal problem that is built into the first amendment we allow foreigners to have first amendment protected rights
[02:02:44.480 --> 02:02:48.080]   We allow russia today. We can't fix that
[02:02:48.080 --> 02:02:52.960]   Not without losing who we are as americans. Yeah, right and certainly twitter can't fix it
[02:02:52.960 --> 02:02:55.440]   Right and so that's my position on all this stuff
[02:02:55.440 --> 02:03:00.640]   Are you making things you yourself your company making things better for these bad guys
[02:03:00.640 --> 02:03:06.080]   And that is what you have to focus on reducing not on solving the overall problem because that's a much easier job
[02:03:06.640 --> 02:03:12.880]   And these these problems are hundreds years old thousands of years exactly and they were caused by the end they will exist in a thousand years
[02:03:12.880 --> 02:03:16.560]   Good answer. Wow. Yeah, good. Um boy. I wear at a time. I don't
[02:03:16.560 --> 02:03:20.560]   You know, I don't want to open the Pandora's box of end-to-end encryption
[02:03:20.560 --> 02:03:27.360]   The UK is also want to hear about tick-tock. Oh god. We didn't even start with tick-tock. Oh, let's do tick-tock
[02:03:27.360 --> 02:03:29.360]   That's much more interesting
[02:03:29.360 --> 02:03:32.160]   Because this end-to-end encryption thing is going to go on forever
[02:03:32.160 --> 02:03:33.520]   um
[02:03:33.520 --> 02:03:35.520]   should tick-tock be shut down as a
[02:03:36.320 --> 02:03:40.880]   As a channel for the Chinese government to influence. I think that's a harsh way to ask no
[02:03:40.880 --> 02:03:43.680]   I think the United States should pass a federal privacy law
[02:03:43.680 --> 02:03:47.040]   That is our GDPR but more specific
[02:03:47.040 --> 02:03:52.880]   And that unlike GDPR explicitly recognizes that some countries are allies and some are not
[02:03:52.880 --> 02:03:55.440]   Right in some are adversaries and there's a bunch of countries in the middle
[02:03:55.440 --> 02:03:58.160]   And so I would like to see a federal privacy law that
[02:03:58.160 --> 02:04:01.280]   binds all tech companies in a bunch of ways
[02:04:01.280 --> 02:04:04.400]   But ones of the ways it binds them is it does not allow
[02:04:05.040 --> 02:04:10.400]   Searching information about American citizens to be stored or processed or accessed by the citizens of certain countries
[02:04:10.400 --> 02:04:14.400]   And that would include the people's republic of China not force tick-tock to be owned by an american company
[02:04:14.400 --> 02:04:19.440]   Right. I would like this law to apply for tick-tock and apple and apple and facebook. Let's face it
[02:04:19.440 --> 02:04:21.760]   Well, right so facebook just no data processing in china
[02:04:21.760 --> 02:04:27.040]   But it but the domestic stuff around rules the domestic rule should apply to facebook. Yeah, certainly. Yeah
[02:04:27.040 --> 02:04:28.560]   That's a good point
[02:04:28.560 --> 02:04:31.040]   Kristi gnome governor of south Dakota just announced that
[02:04:31.600 --> 02:04:35.520]   Tick-tock is banned among in government offices in south Dakota, which is
[02:04:35.520 --> 02:04:38.160]   I'm sure was a
[02:04:38.160 --> 02:04:42.400]   Chinese so here's my hot take of the Chinese companies we should worry about we chat is a much bigger one for me
[02:04:42.400 --> 02:04:45.920]   Because we chat is a massive tool of surveillance against american companies
[02:04:45.920 --> 02:04:52.480]   We don't know about it because it's used by Chinese nationals abroad, right, but every american company has
[02:04:52.480 --> 02:04:55.280]   Chinese speaking employees. Yeah
[02:04:55.280 --> 02:04:59.120]   They they're not intentionally doing anything bad. No, but they all have we chat
[02:05:00.240 --> 02:05:03.760]   Because it's the only way they can talk to their family. Yeah, which is effectively a
[02:05:03.760 --> 02:05:09.920]   Subsidy by the great firewall. Yeah to make we chat used around the world and every single company has a like
[02:05:09.920 --> 02:05:14.400]   Effectively an employee resource group that is on we chat of Chinese speaking employees
[02:05:14.400 --> 02:05:20.240]   That's fine if what they're talking about is I don't like this manager. This HR thing's going on
[02:05:20.240 --> 02:05:22.400]   It's not as cool when they're talking about tech stuff
[02:05:22.400 --> 02:05:24.000]   And this is something we actually had at facebook
[02:05:24.000 --> 02:05:26.960]   Which is we had information that we knew land in the hands of the chinese
[02:05:27.200 --> 02:05:29.920]   Wow that one of our theories was it had been discussed by
[02:05:29.920 --> 02:05:32.560]   You know Chinese speaking employees
[02:05:32.560 --> 02:05:36.160]   They didn't understand that everything they wrote was being logged in Beijing
[02:05:36.160 --> 02:05:36.560]   Wow
[02:05:36.560 --> 02:05:40.160]   So we chat if we're gonna start with something should be we chat because also tiktok
[02:05:40.160 --> 02:05:43.120]   It doesn't it has almost no private communication on it, right?
[02:05:43.120 --> 02:05:47.840]   It's just watching videos right where we chat, you know, like people buy stuff. They buy drugs
[02:05:47.840 --> 02:05:51.440]   Right, they probably communicate with their mistresses like there's a bunch of blackmail material
[02:05:51.440 --> 02:05:53.600]   We know they're seeing a lot of escorts ads. We know that
[02:05:54.800 --> 02:05:58.400]   And then the other thing we chat to use is it's used to control the Chinese diaspora in the us
[02:05:58.400 --> 02:06:02.640]   Like you see university groups and stuff where if you read any of the
[02:06:02.640 --> 02:06:07.200]   Insaditements of people who have been turned by the mss ministry state security while they're in america
[02:06:07.200 --> 02:06:11.840]   There's just one a week ago by the fbi right before thanksgiving the word we chat's all over it
[02:06:11.840 --> 02:06:14.800]   That is how you reach out to somebody that is how you threaten them
[02:06:14.800 --> 02:06:17.600]   That is how you point out that the mss has control of their parents
[02:06:17.600 --> 02:06:20.160]   We chat is the platform. I think we should worry about
[02:06:21.120 --> 02:06:26.000]   I think we're gonna wrap it up. Just say we got so much more to talk about. I hope you will come back
[02:06:26.000 --> 02:06:31.520]   You have an open invitation. It's really great to talk to you next time you can zoom
[02:06:31.520 --> 02:06:37.120]   I mean thrilled you're in studio. Yeah, it's so much more fun. Yeah
[02:06:37.120 --> 02:06:41.840]   Is there anything you would like to promote or promote?
[02:06:41.840 --> 02:06:46.400]   I mean, so if you want to read the stuff that these reports io.stanford.edu
[02:06:47.040 --> 02:06:49.600]   And if you're a company looking for consulting help, it's ks.duck group
[02:06:49.600 --> 02:06:52.400]   Is uh, yeah, I work with chris crabs. He's a great guy
[02:06:52.400 --> 02:06:55.680]   And we try to help companies out so much respect for chris
[02:06:55.680 --> 02:06:59.280]   So impressed with he's a great guy. I mean when the cards are down. He made the right call
[02:06:59.280 --> 02:07:04.880]   Even though he has five kids. Uh, you know, that's tough to walk away from your kid. Yeah. No kidding
[02:07:04.880 --> 02:07:07.520]   uh, the internet observatory
[02:07:07.520 --> 02:07:14.160]   Uh lots of interesting stuff that that top one that's our report about the us doing this stuff. So if you're a uh
[02:07:14.800 --> 02:07:17.440]   Very interesting we called it unheard voice because nobody
[02:07:17.440 --> 02:07:22.400]   And the sad part is is this report is now being cited by like Iranian propaganda
[02:07:22.400 --> 02:07:23.280]   Yeah, it's probably
[02:07:23.280 --> 02:07:29.200]   See they're doing it too. See they're doing it too. It's just the sad mistake our government made. Yeah, that's why we don't do it
[02:07:29.200 --> 02:07:30.800]   Yeah
[02:07:30.800 --> 02:07:36.240]   Uh, wow. I just uh, such good material. I have food for thought for weeks
[02:07:36.240 --> 02:07:41.760]   I am so glad you were all here. We don't need to do anything, but just wrap the show up. I think all right
[02:07:41.760 --> 02:07:43.360]   Um
[02:07:43.360 --> 02:07:47.920]   Is there anything you want to plug? I want to plug club twit. Thank you. Plug club twit
[02:07:47.920 --> 02:07:53.600]   You are a community manager. I'm community manager and all of the support is greatly appreciated
[02:07:53.600 --> 02:07:57.760]   Everyone here at twit. Yeah, um, I have a
[02:07:57.760 --> 02:08:03.440]   Oh, you're gonna play tomorrow. So tomorrow cards with me at four o'clock. How fun
[02:08:03.440 --> 02:08:07.280]   I might join you in that. That's great. I want to see you do bingo
[02:08:07.280 --> 02:08:09.840]   A well, I figured that out one of these days
[02:08:09.840 --> 02:08:13.760]   We can do this in the let's play chat. That's a channel which is going to be in let's play
[02:08:13.760 --> 02:08:14.560]   If you're
[02:08:14.560 --> 02:08:17.760]   You'll have our friend gled flycheman soon. Yeah, and glen fly some
[02:08:17.760 --> 02:08:20.480]   I've been giving him recommendations in in Berlin
[02:08:20.480 --> 02:08:24.880]   He went to the kelphouse des vestons the katove which is this amazing apartment store
[02:08:24.880 --> 02:08:30.800]   Because they have this huge amazing food floor and he came out just saying like capitalism went too far
[02:08:30.800 --> 02:08:36.800]   He didn't have the best breakfast he's ever had. I understand uh, he really is enjoy himself
[02:08:37.120 --> 02:08:41.520]   I can't wait to hear his stories. Uh glens with his son in uh, Berlin. What a great time
[02:08:41.520 --> 02:08:46.480]   Stacy's book club this year. Uh, this week is uh, this month is next month is
[02:08:46.480 --> 02:08:52.080]   Project hail mary. Yeah next year next year's book club
[02:08:52.080 --> 02:08:55.120]   January 12th project hail mary great book
[02:08:55.120 --> 02:08:59.120]   Uh, I will be there for that because i've already read it. It's fantastic. I'm rereading it
[02:08:59.120 --> 02:09:05.280]   Well, and watch our interview that we did with uh, and do we're handy. We're uh on a triangulation a few months ago when I first came out
[02:09:05.280 --> 02:09:10.080]   Yep, and then uh, the loports are going to do a inside twit, uh, January 19th
[02:09:10.080 --> 02:09:15.040]   All of this is available, but only if you're a club member twit.tv slash you should join clubs
[02:09:15.040 --> 02:09:19.200]   I'm going to start a hashtag Andy. We're right in the third person. Let's see if we can get him to
[02:09:19.200 --> 02:09:22.320]   Him
[02:09:22.320 --> 02:09:24.800]   Talk about him all the time. I don't understand
[02:09:24.800 --> 02:09:27.280]   he uh, he uh
[02:09:27.280 --> 02:09:32.000]   We we're going to have him on a few months ago and he just had a baby which is great very happy
[02:09:32.480 --> 02:09:36.080]   Uh for him, but I think now that the baby now that the baby's probably four or five months old
[02:09:36.080 --> 02:09:38.880]   He's probably anxious. Yeah anxious to come on a show
[02:09:38.880 --> 02:09:44.080]   So we'll try to get him back on the movie project hail mary is moving forward he'd optioned it
[02:09:44.080 --> 02:09:48.800]   Uh, he told me at the time that ryan gozling was good play. All right, i'm a lead
[02:09:48.800 --> 02:09:50.800]   Which i will
[02:09:50.800 --> 02:09:56.560]   It makes feelings about let's put it that way but we'll see i have this problem in that hbo bought a facebook book that i'm
[02:09:56.560 --> 02:10:01.280]   Oh, and so clear. Oh, he's playing you. We don't know who's playing me
[02:10:01.680 --> 02:10:05.760]   Um, but it's clear foyer, you know from the love clear foyer is sheryl
[02:10:05.760 --> 02:10:08.640]   Oh, i'll probably get zak gels for that
[02:10:08.640 --> 02:10:16.640]   Bad news right him doing that he wants his emmy he wants to do his like legit turn. Yeah, he's gonna do some
[02:10:16.640 --> 02:10:18.240]   Yeah
[02:10:18.240 --> 02:10:22.640]   Yeah, oh, she's just gonna bat those big eyes at you and it's over it's over
[02:10:22.640 --> 02:10:25.840]   And they'll be up food in his beard. Yes. I'm sorry
[02:10:25.840 --> 02:10:30.560]   That sucks
[02:10:30.560 --> 02:10:34.880]   Uh, jeff javas. Thank you for being here buzz machine. Thank you for doing this loss
[02:10:34.880 --> 02:10:41.520]   Uh, what was your idea get the uh goutenberg books coming out next year you can get on the uh list by going to bitley
[02:10:41.520 --> 02:10:45.600]   Uh dot, uh, sorry b it dot ly slash by goutenberg
[02:10:45.600 --> 02:10:56.240]   jeff of course is the director of the townite center for arch per se
[02:10:57.040 --> 02:11:01.760]   Who's birthday 70th birthday is december 6th. That's a big one
[02:11:01.760 --> 02:11:04.480]   I'm busy gonna do a big thing
[02:11:04.480 --> 02:11:06.800]   My 66
[02:11:06.800 --> 02:11:10.320]   belated and they may mean it was they forced me to eat a lot of cake
[02:11:10.320 --> 02:11:16.960]   Just so much cake there more cake left. There's more cake. It's in my house. Are you glad you're off that stupid diet? You were on
[02:11:16.960 --> 02:11:20.960]   I'm still on it and i'm paying the price for eating all that cake
[02:11:20.960 --> 02:11:23.600]   So good though
[02:11:23.600 --> 02:11:26.320]   Uh, happy birthday craig. That's awesome indeed
[02:11:27.120 --> 02:11:28.880]   Uh, you know
[02:11:28.880 --> 02:11:34.240]   I guess that's it. What a special episode. Yeah, thanks. I like what a pleasure really great to meet you
[02:11:34.240 --> 02:11:37.920]   Thanks for the work you do, which is clearly incredibly important
[02:11:37.920 --> 02:11:41.280]   We thank everybody watch the show. We do this week in google
[02:11:41.280 --> 02:11:45.280]   Uh, every wednesday 2 pm pacific 5 pm eastern 2200 utc
[02:11:45.280 --> 02:11:51.760]   Watch live at live dot twit dot tv chat live in the irc or or in our club twit discord after the fact
[02:11:51.760 --> 02:11:55.760]   You can download copies of the show from the website twit dot tv slash twig
[02:11:56.400 --> 02:11:59.200]   Uh, watch it on youtube. In fact, this would be a good one
[02:11:59.200 --> 02:12:04.800]   Everybody clip out some salient quotes from Alex and send them to your
[02:12:04.800 --> 02:12:08.800]   Your ceos or your it department let them know
[02:12:08.800 --> 02:12:13.360]   Uh, that's all on youtube and of course the best way to listen is to subscribe and you get audio or video
[02:12:13.360 --> 02:12:20.800]   The minute the show's done and never miss another episode. Thank you everybody for being here. We'll see you next time on twig
[02:12:20.800 --> 02:12:24.240]   Bye. Bye
[02:12:24.560 --> 02:12:28.960]   The world is changing rapidly so rapidly in fact that it's hard to keep up
[02:12:28.960 --> 02:12:35.360]   That's why mica sergeant and I Jason howl talk with the people making and breaking the tech news on tech news weekly
[02:12:35.360 --> 02:12:38.880]   Every thursday, they know these stories better than anyone
[02:12:38.880 --> 02:12:42.000]   So why not get them to talk about it in their own words?
[02:12:42.000 --> 02:12:46.640]   Subscribe to tech news weekly and you won't miss a beat every thursday at twit dot tv
[02:12:46.640 --> 02:12:50.640]   [MUSIC PLAYING]
[02:12:50.640 --> 02:12:54.640]   [MUSIC PLAYING]
[02:12:54.640 --> 02:12:57.220]   (upbeat music)
[02:12:57.220 --> 02:12:59.880]   (bell dings)

