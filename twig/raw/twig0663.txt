;FFMETADATA1
title=Bill of Attainder
artist=Leo Laporte, Jeff Jarvis, Ant Pruitt, Mike Masnick
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-05-12
track=663
language=English
genre=Podcast
comment=Google IO, Hawley's copyright bill, Texas social media law, Keyboardio Atreus
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.240]   It's time for Twig this week in Google. Mike Masnick is here from Techdirt.
[00:00:04.240 --> 00:00:11.200]   Jeff Jarvis is here. So is that Pruitt? Lots to talk about, including Josh Hawley.
[00:00:11.200 --> 00:00:18.160]   His bill of a tanger would take away Disney's copyright. Don't think that one's gonna pass.
[00:00:18.160 --> 00:00:24.800]   We'll also talk about Google I/O. There were some things announced, not the watch particularly.
[00:00:24.800 --> 00:00:31.520]   And I'll look at again Elon Musk and Twitter and some breaking news.
[00:00:31.520 --> 00:00:36.000]   Elon has no idea what he's doing. It's all coming up next on Twig.
[00:00:36.000 --> 00:00:40.000]   Podcasts You Love.
[00:00:49.440 --> 00:00:56.960]   This is Twig. This week in Google, Episode 663, recorded Wednesday, May 11, 2022.
[00:00:56.960 --> 00:00:58.800]   Bill of a Tanger.
[00:00:58.800 --> 00:01:06.320]   This episode of This Week in Google is brought to you by 8 Sleep. Good sleep is the ultimate game
[00:01:06.320 --> 00:01:13.040]   changer and nature's best medicine. Go to 8sleep.com/twig to check out the Pod Pro cover
[00:01:13.040 --> 00:01:18.960]   and save $150 to check out 8 sleep currently ships within the USA, Canada, and the UK.
[00:01:19.840 --> 00:01:26.080]   And by Code Academy. Join over 50 million people learning to code with Code Academy.
[00:01:26.080 --> 00:01:30.800]   And see where coding can take you. Get 15% off your Code Academy Pro membership
[00:01:30.800 --> 00:01:34.240]   when you go to CodeCademy.com and use the promo code Twig.
[00:01:34.240 --> 00:01:42.720]   And by CashFly. CashFly is giving away a complimentary, detailed analysis of your current CDN bill and usage trends.
[00:01:42.720 --> 00:01:48.320]   See if you're overpaying 20% or more. Learn more at twit.cashfly.com.
[00:01:49.200 --> 00:01:54.640]   It's time for Twig. This week in Google, a show we cover the latest news from Google.
[00:01:54.640 --> 00:01:57.840]   Turns out there is a little bit of news from Google. Today was Google IO.
[00:01:57.840 --> 00:02:04.080]   Jeff Jarvis joined me and Jason Howell as we covered the IO. Good morning.
[00:02:04.080 --> 00:02:07.120]   And good afternoon Jeff Jarvis. How are you today?
[00:02:07.120 --> 00:02:09.200]   You put in that chair for what now? 10 hours?
[00:02:09.200 --> 00:02:10.480]   An hours. I don't mind.
[00:02:10.480 --> 00:02:11.360]   I don't mind.
[00:02:11.360 --> 00:02:12.400]   Man, that's butt rock.
[00:02:14.000 --> 00:02:18.160]   Jeff is the Leonard Ta professor for journalistic innovation at the fabulous
[00:02:18.160 --> 00:02:20.560]   World Trade Bank.
[00:02:20.560 --> 00:02:24.240]   Thank you my graduate school of journalism at the City University of New York.
[00:02:24.240 --> 00:02:29.840]   And I have something, but I'll talk about that in a moment.
[00:02:29.840 --> 00:02:34.640]   Also with us, Ant Pruitt from Hands on Photography.
[00:02:34.640 --> 00:02:36.720]   Hello, Ant. Good to see you.
[00:02:36.720 --> 00:02:37.600]   Hello, sir.
[00:02:37.600 --> 00:02:39.040]   Stacy has the day off today.
[00:02:39.040 --> 00:02:39.760]   I'll tell you.
[00:02:39.760 --> 00:02:40.000]   What?
[00:02:41.040 --> 00:02:46.000]   I got to tell you, I'm glad that you decided not to sit on that tiny chair today.
[00:02:46.000 --> 00:02:47.760]   Oh, I sat on it for Windows Weekly.
[00:02:47.760 --> 00:02:50.320]   The show was a lot shorter as a result.
[00:02:50.320 --> 00:02:54.320]   Special saddle on a stick.
[00:02:54.320 --> 00:02:57.200]   Stacy.
[00:02:57.200 --> 00:02:57.840]   Only old days.
[00:02:57.840 --> 00:02:58.720]   He's going to stick up his.
[00:02:58.720 --> 00:03:01.680]   Oh, oh, the pain.
[00:03:01.680 --> 00:03:04.240]   Stacy's not here, but good news.
[00:03:04.240 --> 00:03:05.760]   Mike Masnick is from Techter.
[00:03:05.760 --> 00:03:06.720]   It's great to see Mike.
[00:03:06.720 --> 00:03:07.600]   Techter.com.
[00:03:09.040 --> 00:03:15.120]   Now, Mike, you weren't here last week, but we somehow discovered that Jeff Jarvis's
[00:03:15.120 --> 00:03:18.080]   photo was available on eBay.
[00:03:18.080 --> 00:03:22.240]   So I have a great, advanced purchase.
[00:03:22.240 --> 00:03:23.520]   Oh, my.
[00:03:23.520 --> 00:03:25.760]   Oh, my.
[00:03:25.760 --> 00:03:30.720]   It's a good, it's a very good, the 1990 press photo of Jeff Jarvis.
[00:03:30.720 --> 00:03:31.840]   I actually got two of them.
[00:03:31.840 --> 00:03:32.320]   I don't know.
[00:03:32.320 --> 00:03:36.160]   Because the hilarity was just so worth it.
[00:03:36.160 --> 00:03:39.760]   You know, I think is Mard because your name is circled in Ballpoint pen.
[00:03:39.760 --> 00:03:42.240]   I think this, they got this from your mom.
[00:03:42.240 --> 00:03:43.440]   Do you have an auction of her?
[00:03:43.440 --> 00:03:44.400]   Let's see.
[00:03:44.400 --> 00:03:45.200]   This one.
[00:03:45.200 --> 00:03:46.640]   Oh, this is an even better one.
[00:03:46.640 --> 00:03:49.040]   This is Jeff and some other guy.
[00:03:49.040 --> 00:03:50.240]   My nemesis.
[00:03:50.240 --> 00:03:54.640]   And we've weekly publisher Mike Klingensmith.
[00:03:54.640 --> 00:03:58.800]   Probably a member of the International Advertising Bureau, I would guess.
[00:03:58.800 --> 00:03:59.760]   Oh, yes.
[00:03:59.760 --> 00:04:00.240]   All right.
[00:04:00.240 --> 00:04:00.640]   So.
[00:04:00.640 --> 00:04:04.160]   It's time for the ritual framing.
[00:04:05.280 --> 00:04:09.120]   We thought, we thought you were going to be here.
[00:04:09.120 --> 00:04:15.680]   So, but I have prepared a little, a little, a little frame, some mood music for this.
[00:04:15.680 --> 00:04:26.080]   Strangers in the door.
[00:04:26.080 --> 00:04:28.640]   I'm super, no more Miss Mary Joe Foley.
[00:04:28.640 --> 00:04:30.080]   I put a spell on you.
[00:04:30.080 --> 00:04:32.800]   Yeah, it was a, it was a picture of Mary Joe Foley.
[00:04:33.360 --> 00:04:37.200]   And underneath her, some windows guy.
[00:04:37.200 --> 00:04:41.920]   So anyway, we're going to, I just look, okay, I didn't have a frame.
[00:04:41.920 --> 00:04:44.240]   I thought, well, you know, we'll borrow Mary Joe's.
[00:04:44.240 --> 00:04:45.360]   Here we go.
[00:04:45.360 --> 00:04:48.880]   You know what's funny is the world of algorithms.
[00:04:48.880 --> 00:04:49.040]   Yeah.
[00:04:49.040 --> 00:04:54.160]   I'm still getting emails now because I attempted to get one of those images as well.
[00:04:54.160 --> 00:04:55.360]   Hey, it's not too late.
[00:04:55.360 --> 00:04:56.160]   It's not too late.
[00:04:56.160 --> 00:05:00.960]   So every other day, he was like, hey, you're just in this pick of Jeff Jarvis.
[00:05:00.960 --> 00:05:01.600]   I got you.
[00:05:01.600 --> 00:05:02.160]   Cuddle wide.
[00:05:02.160 --> 00:05:04.640]   Let's see how this looks on the, on the set.
[00:05:04.640 --> 00:05:05.200]   There you go.
[00:05:05.200 --> 00:05:06.160]   Right next to Jeff.
[00:05:06.160 --> 00:05:07.120]   Before and after.
[00:05:07.120 --> 00:05:11.120]   That needs to be there every week.
[00:05:11.120 --> 00:05:11.840]   Every week?
[00:05:11.840 --> 00:05:14.000]   No, you're exactly right, Mike Maslow.
[00:05:14.000 --> 00:05:15.600]   I was young once.
[00:05:15.600 --> 00:05:17.920]   You want to hear something kind of saying kind of poignant?
[00:05:17.920 --> 00:05:18.560]   What?
[00:05:18.560 --> 00:05:21.040]   So I, because of this, I thought, what, what is on eBay?
[00:05:21.040 --> 00:05:23.600]   So I went on and I searched myself on eBay, which I'd never done.
[00:05:23.600 --> 00:05:29.040]   And when I moved my parents out of Florida, I gave their books away to the Goodwill.
[00:05:29.040 --> 00:05:32.240]   So now the books of mine that I dedicated to my parents.
[00:05:32.240 --> 00:05:32.640]   Oh, no.
[00:05:32.640 --> 00:05:38.320]   For sale on eBay with, you know, folks signed by Jeff Jarvis.
[00:05:38.320 --> 00:05:38.640]   Yeah.
[00:05:38.640 --> 00:05:39.760]   Okay, that did me.
[00:05:39.760 --> 00:05:45.680]   That's, you know, they asked at Google, I go, I owe if people googled their names.
[00:05:45.680 --> 00:05:49.040]   And surprisingly few raised their hand embarrassed, I guess.
[00:05:49.040 --> 00:05:53.680]   But that's a, that's a step beyond to go on to eBay and look for eBay.
[00:05:53.680 --> 00:05:54.000]   eBay.
[00:05:55.600 --> 00:06:02.080]   Here's 10% off my, my 2004 technology, Almanac, just $7.74.
[00:06:02.080 --> 00:06:03.920]   It's so up to date.
[00:06:03.920 --> 00:06:07.520]   PC helped desk only $5 buy two, get one free.
[00:06:07.520 --> 00:06:11.360]   Game is right.
[00:06:11.360 --> 00:06:12.720]   When you part of a bokeh.
[00:06:12.720 --> 00:06:13.760]   Holy cow.
[00:06:13.760 --> 00:06:15.600]   Oh my God.
[00:06:15.600 --> 00:06:16.560]   Here's my dad's way.
[00:06:16.560 --> 00:06:17.360]   You are.
[00:06:17.360 --> 00:06:23.040]   I have bragging rights because as low as my books are, my father's book is a dollar.
[00:06:23.040 --> 00:06:25.280]   So or best offer.
[00:06:25.280 --> 00:06:27.120]   So there you go.
[00:06:27.120 --> 00:06:28.320]   Now I got to go call dad.
[00:06:28.320 --> 00:06:30.320]   Or best offer or best offer.
[00:06:30.320 --> 00:06:31.200]   I got to call dad.
[00:06:31.200 --> 00:06:32.960]   Wow.
[00:06:32.960 --> 00:06:34.560]   There's actually a lot of crap.
[00:06:34.560 --> 00:06:35.840]   Oh, yeah.
[00:06:35.840 --> 00:06:37.280]   A lot of people dump in their books.
[00:06:37.280 --> 00:06:42.000]   Oh, see there now my dad's other book is $65.
[00:06:42.000 --> 00:06:45.280]   That's a rarity, rarity.
[00:06:45.280 --> 00:06:48.320]   Quite a few copies of my father's book on here.
[00:06:48.320 --> 00:06:49.200]   Books.
[00:06:49.200 --> 00:06:50.560]   It's good.
[00:06:50.560 --> 00:06:50.880]   Yeah.
[00:06:50.880 --> 00:06:51.280]   This is good.
[00:06:51.280 --> 00:06:51.920]   Quite a few.
[00:06:51.920 --> 00:06:52.160]   Yeah.
[00:06:52.160 --> 00:06:54.400]   $2 or best offer.
[00:06:54.400 --> 00:06:57.280]   Here's one in excellent condition.
[00:06:57.280 --> 00:06:59.200]   $2 or best offer.
[00:06:59.200 --> 00:07:00.720]   Whenever you give me, I'll take it.
[00:07:00.720 --> 00:07:04.320]   My guy to Tivo is $157 for some reason.
[00:07:04.320 --> 00:07:05.680]   Geez.
[00:07:05.680 --> 00:07:06.720]   Why don't buy that one.
[00:07:06.720 --> 00:07:07.760]   Get the one with CD-ROM.
[00:07:07.760 --> 00:07:08.880]   It's only $5.
[00:07:08.880 --> 00:07:10.560]   Costless.
[00:07:10.560 --> 00:07:12.480]   We should get the CD-ROM.
[00:07:12.480 --> 00:07:14.000]   Okay.
[00:07:14.000 --> 00:07:15.600]   Anyway, enough marathon,
[00:07:15.600 --> 00:07:18.320]   Mirth and Merriment because it's going to be a depressing show.
[00:07:19.520 --> 00:07:22.240]   Mike, when you arrived, you said breaking news,
[00:07:22.240 --> 00:07:30.800]   the Texas Appeals Court, the Fifth District, has ruled that the Texas social media law can
[00:07:30.800 --> 00:07:31.120]   proceed.
[00:07:31.120 --> 00:07:32.640]   A lower court had blocked it.
[00:07:32.640 --> 00:07:35.360]   That injunction has been lifted.
[00:07:35.360 --> 00:07:40.240]   The panel stayed at District Court in Junction that had paused the law while judges
[00:07:40.240 --> 00:07:43.280]   consider an appeal of the lower court's move.
[00:07:43.280 --> 00:07:47.360]   The decision was supported, which was supported by only by two unnamed judges,
[00:07:48.640 --> 00:07:50.880]   nor did they immediately publish their reasoning.
[00:07:50.880 --> 00:07:53.920]   You presume there is any.
[00:07:53.920 --> 00:07:57.920]   According to protocol, it came after a Monday hearing when the jurists appeared to struggle with
[00:07:57.920 --> 00:08:02.720]   basic tech concepts, including whether Twitter counts as a website.
[00:08:02.720 --> 00:08:04.560]   It was pathetic.
[00:08:04.560 --> 00:08:11.600]   So what, Mike, what is the, just out of curiosity, the social media law in Texas?
[00:08:12.640 --> 00:08:19.520]   It has a few different components, and I don't think any of them are actually
[00:08:19.520 --> 00:08:22.880]   constitutional if we had a Fifth Circuit that wasn't crazy,
[00:08:22.880 --> 00:08:30.960]   where it requires some element of transparency in terms of how social media websites handle
[00:08:30.960 --> 00:08:31.760]   content moderation.
[00:08:31.760 --> 00:08:33.040]   They have to publish certain rules.
[00:08:33.040 --> 00:08:39.040]   There has to be an appeals process, and you have to explain every decision more or less.
[00:08:39.040 --> 00:08:39.600]   Oh my God.
[00:08:40.160 --> 00:08:49.680]   And if people feel that the explanations are not up to whatever level they can complain,
[00:08:49.680 --> 00:08:53.760]   and the Attorney General of Texas can take action against the websites.
[00:08:53.760 --> 00:09:00.800]   It's effectively trying to restrict the social media website's ability to do any kind of
[00:09:00.800 --> 00:09:07.280]   content moderation, especially obviously the kind of content moderation that the current
[00:09:07.280 --> 00:09:13.440]   political class in Texas does not like, which means anyone who supports Donald Trump or Greg Abbott.
[00:09:13.440 --> 00:09:20.240]   And so they're basically going to be able to sue different social media platforms,
[00:09:20.240 --> 00:09:21.440]   and they will do so.
[00:09:21.440 --> 00:09:25.360]   It sounds like the Florida law minus the exemption for Disney.
[00:09:25.360 --> 00:09:27.760]   It is very similar.
[00:09:27.760 --> 00:09:31.280]   There are a few very minor changes that doesn't really make a difference.
[00:09:31.280 --> 00:09:33.840]   They're very, very similar laws.
[00:09:33.840 --> 00:09:42.000]   Florida's law, so both laws were passed, and both laws were then sued to block them.
[00:09:42.000 --> 00:09:44.720]   And in both cases, the district court stopped them.
[00:09:44.720 --> 00:09:47.200]   Because of the first freaking amendment, right?
[00:09:47.200 --> 00:09:49.520]   Mainly because of the first amendment.
[00:09:49.520 --> 00:09:56.160]   And then there was an appeals hearing for the Florida law about two weeks before the Texas one,
[00:09:56.160 --> 00:09:57.840]   but we haven't had the ruling.
[00:09:57.840 --> 00:10:02.320]   And the hearing, it's the 11th circuit for the Florida law.
[00:10:02.320 --> 00:10:06.240]   The hearing there went really well, and the judges seemed to really get it and seemed
[00:10:06.240 --> 00:10:11.200]   very, very skeptical of Florida's argument as opposed to the hearing in Texas, where the judges
[00:10:11.200 --> 00:10:12.720]   seem to have absolutely no clue.
[00:10:12.720 --> 00:10:14.880]   It was all over the map.
[00:10:14.880 --> 00:10:16.320]   - What's the website?
[00:10:16.320 --> 00:10:21.200]   - Yeah, it was, you know, and just all of the comparisons were weird.
[00:10:21.200 --> 00:10:22.800]   They didn't understand section 230.
[00:10:22.800 --> 00:10:24.800]   They didn't seem to understand the first amendment.
[00:10:24.800 --> 00:10:25.840]   And so then, you know,
[00:10:25.840 --> 00:10:28.000]   - $1,000 handshake right there, man.
[00:10:28.000 --> 00:10:28.800]   - It's all it is.
[00:10:28.800 --> 00:10:30.080]   - $1,000 handshake.
[00:10:30.880 --> 00:10:33.440]   I mean, and then this is two days after the hearing.
[00:10:33.440 --> 00:10:34.880]   I mean, they didn't even wait.
[00:10:34.880 --> 00:10:41.120]   You know, Florida is still sort of, you know, probably coming up with their written opinion on this,
[00:10:41.120 --> 00:10:45.440]   whereas the Texas, they just put out this announcement saying that they've lifted the
[00:10:45.440 --> 00:10:49.040]   injunction and the law goes into effect immediately.
[00:10:49.040 --> 00:10:52.080]   And they'll give out their opinion at some later date.
[00:10:52.080 --> 00:10:54.560]   And so it's going to be a mess.
[00:10:54.560 --> 00:10:58.720]   If I were running a social media company right now,
[00:10:58.720 --> 00:11:01.760]   I would be talking to my lawyers this very second,
[00:11:01.760 --> 00:11:04.160]   trying to figure out if we can still do business in Texas,
[00:11:04.160 --> 00:11:09.200]   because I think it's extraordinarily risky to have any users in Texas right now.
[00:11:09.200 --> 00:11:12.240]   If you meet the qualifications of this particular--
[00:11:12.240 --> 00:11:13.920]   - Well, now I'm nervous because we,
[00:11:13.920 --> 00:11:16.800]   Twitter runs a Mastodon instance, Twitter.social.
[00:11:16.800 --> 00:11:19.040]   Forums would count, right?
[00:11:19.040 --> 00:11:23.840]   We run our own forums on discourse, Twitter.community.
[00:11:23.840 --> 00:11:25.680]   It does a chat room count.
[00:11:25.680 --> 00:11:27.120]   We've got chat rooms.
[00:11:27.120 --> 00:11:27.600]   - Sure.
[00:11:27.600 --> 00:11:29.520]   - And we've got users in Texas.
[00:11:29.520 --> 00:11:36.000]   So you're not giving me legal advice, Mike, but I mean, our lawyers all over the country
[00:11:36.000 --> 00:11:40.560]   telling social networks don't allow Texas users.
[00:11:40.560 --> 00:11:42.560]   - It's possible.
[00:11:42.560 --> 00:11:46.720]   I forget the exact details, because there have been so many of these kinds of laws,
[00:11:46.720 --> 00:11:48.400]   I forget the exact thresholds.
[00:11:48.400 --> 00:11:52.080]   There may be some thresholds in terms of size for the Texas law.
[00:11:52.080 --> 00:11:53.760]   I'm sort of pulling it up in the background here.
[00:11:53.760 --> 00:11:56.000]   So I don't remember what the thresholds are.
[00:11:56.000 --> 00:11:57.680]   - Small enough that might not be in this.
[00:11:57.680 --> 00:11:59.840]   - If you're small enough, you might be okay.
[00:11:59.840 --> 00:12:00.160]   - But--
[00:12:00.160 --> 00:12:02.960]   - These are really aimed at Twitter banning Donald Trump, really.
[00:12:02.960 --> 00:12:03.440]   Let's be honest.
[00:12:03.440 --> 00:12:05.760]   - Yeah, oh, that's exactly what it's used for.
[00:12:05.760 --> 00:12:09.360]   But to be clear, that doesn't mean it doesn't affect you.
[00:12:09.360 --> 00:12:10.480]   I mean, you use--
[00:12:10.480 --> 00:12:12.240]   - 50 million monthly users or more.
[00:12:12.240 --> 00:12:15.120]   So we're out of the dark on that one.
[00:12:15.120 --> 00:12:16.320]   - Buy a hair.
[00:12:16.320 --> 00:12:16.960]   - Buy a hair.
[00:12:16.960 --> 00:12:19.680]   But who knows, right?
[00:12:19.680 --> 00:12:23.120]   I mean, next month, you might hit that number.
[00:12:23.120 --> 00:12:24.480]   You don't know.
[00:12:24.480 --> 00:12:27.680]   And, but it does also impact the services that you use.
[00:12:27.680 --> 00:12:30.640]   You guys use YouTube, you use Twitter,
[00:12:30.640 --> 00:12:34.160]   you use Facebook to promote stuff, you use Discord.
[00:12:34.160 --> 00:12:36.720]   And all of these services may be impacted by this,
[00:12:36.720 --> 00:12:38.800]   and it may actually limit your ability
[00:12:38.800 --> 00:12:41.040]   to communicate with people in Texas, for example.
[00:12:41.040 --> 00:12:46.880]   - The Texas argument was that social media is a common carrier.
[00:12:46.880 --> 00:12:52.080]   And they want social media to carry all content,
[00:12:52.640 --> 00:12:56.720]   unmoderated, the way a phone company is expected to carry all calls.
[00:12:56.720 --> 00:13:00.240]   That means no zero content moderation.
[00:13:00.240 --> 00:13:01.520]   - That'll be up.
[00:13:01.520 --> 00:13:02.080]   - It's such fun.
[00:13:02.080 --> 00:13:07.600]   - The whole common carrier thing really frustrates me.
[00:13:07.600 --> 00:13:11.600]   Because if you understand what a common carrier is,
[00:13:11.600 --> 00:13:13.040]   just by its very name, right?
[00:13:13.040 --> 00:13:16.400]   The idea is you are carrying something from point A to point B.
[00:13:16.400 --> 00:13:19.600]   And historically, that might be people or cargo,
[00:13:19.600 --> 00:13:21.680]   and then it became data or voice.
[00:13:22.240 --> 00:13:26.160]   But social media is hosting something perpetually.
[00:13:26.160 --> 00:13:28.560]   It's not just about getting from point A to point B.
[00:13:28.560 --> 00:13:29.440]   And the idea that--
[00:13:29.440 --> 00:13:31.120]   - It's not ephemeral phone calls.
[00:13:31.120 --> 00:13:31.920]   - Right.
[00:13:31.920 --> 00:13:36.160]   The idea that you should be forced to leave up content in perpetuity,
[00:13:36.160 --> 00:13:39.280]   that may be harmful or--
[00:13:39.280 --> 00:13:40.880]   - All the same.
[00:13:40.880 --> 00:13:43.280]   - Otherwise problematic or harassing.
[00:13:43.280 --> 00:13:45.040]   There are all sorts of issues with it.
[00:13:45.040 --> 00:13:51.280]   And so the push to declare social media common carriers
[00:13:51.280 --> 00:13:53.920]   is really, really bizarre.
[00:13:53.920 --> 00:13:55.440]   And it's especially bizarre,
[00:13:55.440 --> 00:13:58.000]   because the very same people who are strongly pushing
[00:13:58.000 --> 00:14:02.480]   to declare social media common carriers absolutely freaked out
[00:14:02.480 --> 00:14:05.440]   and spent years complaining about attempts to make
[00:14:05.440 --> 00:14:07.680]   broadband carriers, common carriers.
[00:14:07.680 --> 00:14:09.760]   Which where there's a much stronger argument,
[00:14:09.760 --> 00:14:11.440]   this is the net neutrality debate, right?
[00:14:11.440 --> 00:14:14.080]   There's a much stronger argument that a broadband player
[00:14:14.080 --> 00:14:15.440]   should be covered by net neutrality,
[00:14:15.440 --> 00:14:17.680]   that they should be a common carrier.
[00:14:17.680 --> 00:14:19.440]   They are point A to point B.
[00:14:20.720 --> 00:14:24.640]   Isn't it hysterical how they don't want ISPs
[00:14:24.640 --> 00:14:26.320]   to become carriers,
[00:14:26.320 --> 00:14:28.640]   but they do want Twitter to be a common carrier,
[00:14:28.640 --> 00:14:31.760]   completely the reversing the meaning of common carrier.
[00:14:31.760 --> 00:14:32.400]   In a few points.
[00:14:32.400 --> 00:14:35.040]   - Yes, it is, one of them actually makes sense
[00:14:35.040 --> 00:14:36.960]   as a common carrier and one of them doesn't.
[00:14:36.960 --> 00:14:38.320]   And they have it reversed.
[00:14:38.320 --> 00:14:40.320]   And in fact, I forget which bill,
[00:14:40.320 --> 00:14:41.680]   because there have been so many of these bills.
[00:14:41.680 --> 00:14:43.680]   I don't remember if the Texas bill has this,
[00:14:43.680 --> 00:14:47.040]   but some of these bills have explicit language that says,
[00:14:47.040 --> 00:14:49.760]   "This cannot apply to a broadband provider."
[00:14:49.760 --> 00:14:53.840]   So you can tell that AT&T and Comcast lobbyists went in and said,
[00:14:53.840 --> 00:14:55.760]   "Okay, this is fine as long as you have this line."
[00:14:55.760 --> 00:14:58.880]   Kind of like Disney lobbyists in Florida saying,
[00:14:58.880 --> 00:15:02.880]   add in the theme park exemption until we've pissed off DeSantis.
[00:15:02.880 --> 00:15:04.320]   - Now they're gonna take it out, right?
[00:15:04.320 --> 00:15:05.440]   - Yeah, let's move it now.
[00:15:05.440 --> 00:15:11.920]   - It's just essentially using these laws and these courts
[00:15:11.920 --> 00:15:14.240]   to espouse a political point of view,
[00:15:14.240 --> 00:15:19.280]   having nothing to do with the reality of the nature.
[00:15:19.280 --> 00:15:23.520]   - Well, it's really depressing because it's a victimhood.
[00:15:23.520 --> 00:15:24.640]   - But this is what you're seeing,
[00:15:24.640 --> 00:15:26.720]   this is what you saw with Roe v. Wade.
[00:15:26.720 --> 00:15:28.960]   They couldn't get the legislature,
[00:15:28.960 --> 00:15:31.280]   they couldn't get the voters to overturn it.
[00:15:31.280 --> 00:15:36.720]   So they went the judicial route bypassing legislatures.
[00:15:36.720 --> 00:15:40.480]   This is a very, it's a useful route to go to the,
[00:15:40.480 --> 00:15:42.080]   you know, go to judges.
[00:15:42.080 --> 00:15:42.720]   - Well, it's back to them, yeah.
[00:15:42.720 --> 00:15:43.360]   - Yeah.
[00:15:43.360 --> 00:15:46.000]   And in this case, this is a law that was obviously passed
[00:15:46.000 --> 00:15:48.880]   by the Texas legislature, but it's, you know, obvious.
[00:15:48.880 --> 00:15:50.480]   I mean, I'm not a lawyer, but I mean,
[00:15:50.480 --> 00:15:52.640]   I do think the First Amendment says the government
[00:15:52.640 --> 00:15:55.840]   shall make no law in Frenching free speech.
[00:15:55.840 --> 00:15:57.920]   That's exactly what this is, isn't it?
[00:15:57.920 --> 00:15:58.400]   - Or no.
[00:15:58.400 --> 00:15:58.400]   - Or no.
[00:15:58.400 --> 00:15:59.520]   - Ask two questions.
[00:15:59.520 --> 00:15:59.680]   - Yeah.
[00:15:59.680 --> 00:16:05.920]   First, I can't, like you, I can't keep all the laws straight.
[00:16:05.920 --> 00:16:07.920]   Is this law like the abortion law
[00:16:07.920 --> 00:16:10.160]   that doesn't so much as government step in,
[00:16:10.160 --> 00:16:11.760]   but allow citizens to sue?
[00:16:11.760 --> 00:16:14.560]   - If I remember correctly, and again,
[00:16:14.560 --> 00:16:15.920]   like there are different laws,
[00:16:15.920 --> 00:16:17.120]   so I don't remember all of them.
[00:16:17.120 --> 00:16:21.440]   I think the Texas law only allows users to complain.
[00:16:21.440 --> 00:16:23.280]   The only one who can actually enforce it
[00:16:23.280 --> 00:16:24.320]   is the Attorney General.
[00:16:24.320 --> 00:16:26.640]   So the Attorney General can then bring suit
[00:16:26.640 --> 00:16:30.000]   on behalf of residents of Texas who complain.
[00:16:30.000 --> 00:16:31.600]   I think that's what it is.
[00:16:31.600 --> 00:16:32.880]   I don't remember the details.
[00:16:32.880 --> 00:16:33.840]   And again, a lot of these--
[00:16:33.840 --> 00:16:37.040]   - HV number 20, an act relating to censorship of
[00:16:37.040 --> 00:16:40.000]   or certain other interference with digital expression,
[00:16:40.000 --> 00:16:43.040]   including expression and social media platforms
[00:16:43.040 --> 00:16:44.880]   or through electronic mail messages.
[00:16:44.880 --> 00:16:46.720]   - What?
[00:16:46.720 --> 00:16:48.160]   - What?
[00:16:48.160 --> 00:16:51.760]   - So, well, you know, and again, email, common carrier,
[00:16:51.760 --> 00:16:53.440]   a little different than social media.
[00:16:53.440 --> 00:16:58.720]   The idea is that regardless of the position you state,
[00:16:58.720 --> 00:17:02.080]   you should be allowed to say it.
[00:17:02.080 --> 00:17:03.760]   Which I don't disagree with.
[00:17:03.760 --> 00:17:05.600]   Some have said--
[00:17:05.600 --> 00:17:08.560]   - Well, but don't worry, there's also a right of the company
[00:17:08.560 --> 00:17:10.000]   to not have to carry things.
[00:17:10.000 --> 00:17:11.920]   - And that's why the free speech thing is interesting.
[00:17:11.920 --> 00:17:12.400]   - Oh, we speak to the not free speech.
[00:17:12.400 --> 00:17:14.560]   - That's why the free speech thing is interesting.
[00:17:14.560 --> 00:17:16.000]   They're saying, well, it's the free speech
[00:17:16.000 --> 00:17:17.040]   of the people posting.
[00:17:17.040 --> 00:17:19.920]   And we're saying, well, yeah, but you can't
[00:17:19.920 --> 00:17:23.280]   abridge the right of Twitter or any other company to moderate.
[00:17:23.280 --> 00:17:24.080]   That's--
[00:17:24.080 --> 00:17:24.320]   - Right.
[00:17:24.320 --> 00:17:26.800]   Think of it this way.
[00:17:26.800 --> 00:17:28.080]   - So, email is a common carrier?
[00:17:28.080 --> 00:17:29.280]   - Yeah, I would say email.
[00:17:29.280 --> 00:17:30.560]   - I'm a common carrier.
[00:17:30.560 --> 00:17:31.120]   - I would say email.
[00:17:31.120 --> 00:17:33.520]   - I'm a common carrier that somebody manages the servers
[00:17:33.520 --> 00:17:36.640]   and stuff the same way they're managing all of our social media.
[00:17:36.640 --> 00:17:40.640]   'Cause I'm sure a vast majority of users aren't like you,
[00:17:40.640 --> 00:17:43.600]   Mr. Leport hosting your own mail server and things like that.
[00:17:43.600 --> 00:17:45.360]   So email is its influence.
[00:17:45.360 --> 00:17:46.080]   - Yes.
[00:17:46.080 --> 00:17:50.240]   Well, yeah, I don't know why what the email part is.
[00:17:50.240 --> 00:17:52.720]   But just to clarify, it sounds like they just--
[00:17:52.720 --> 00:17:53.520]   - They just threw that in.
[00:17:53.520 --> 00:18:00.960]   - You know, I'm sure some email provider required something.
[00:18:00.960 --> 00:18:03.760]   But the way to think of it is,
[00:18:03.760 --> 00:18:06.160]   instead of, we talk about content moderation,
[00:18:06.160 --> 00:18:07.760]   just think of editorial discretion,
[00:18:07.760 --> 00:18:09.360]   because that's what it is, right?
[00:18:09.360 --> 00:18:11.440]   This is content that you're hosting on your website.
[00:18:11.440 --> 00:18:13.280]   You have editorial discretion over it.
[00:18:13.280 --> 00:18:15.680]   Free speech is about you're right to speak,
[00:18:15.680 --> 00:18:18.080]   which you still have your right to speak.
[00:18:18.080 --> 00:18:19.360]   It doesn't mean you get to commandeer
[00:18:19.360 --> 00:18:21.520]   somebody else's property in order to do that.
[00:18:21.520 --> 00:18:25.520]   I can't put up political signs in your front yard,
[00:18:25.520 --> 00:18:27.760]   because it's your property, right?
[00:18:27.760 --> 00:18:30.560]   If I want to put signs in my front yard, that's fine.
[00:18:30.560 --> 00:18:32.960]   If I want to set up my own website and post stuff,
[00:18:32.960 --> 00:18:34.960]   nobody can stop me from doing that.
[00:18:34.960 --> 00:18:39.120]   But I can't commandeer somebody else's website to post.
[00:18:39.120 --> 00:18:42.160]   And yet this law is effectively saying,
[00:18:42.160 --> 00:18:45.600]   "Yes, effectively the state can commandeer private property
[00:18:45.600 --> 00:18:48.000]   and force postings there."
[00:18:48.000 --> 00:18:48.960]   And when you think about it that way,
[00:18:48.960 --> 00:18:51.600]   it feels like it goes against everything that--
[00:18:51.600 --> 00:18:54.000]   - It's like being forced to put somebody's law,
[00:18:54.000 --> 00:18:55.680]   somebody can put a law on the sign on my lawn,
[00:18:55.680 --> 00:18:56.640]   and I have no choice in it.
[00:18:56.640 --> 00:18:57.360]   - Right.
[00:18:57.360 --> 00:18:59.440]   - Right, because you can't discriminate.
[00:18:59.440 --> 00:19:01.760]   Like that's basically the argument.
[00:19:01.760 --> 00:19:03.600]   Saying that, saying that, you know,
[00:19:03.600 --> 00:19:06.960]   I can't put the sign for the politicians I want in your yard
[00:19:06.960 --> 00:19:10.240]   is a discrimination issue.
[00:19:10.240 --> 00:19:13.760]   - But the larger thing going on here, sorry,
[00:19:13.760 --> 00:19:16.560]   the larger thing going on here is this idea of victimhood.
[00:19:16.560 --> 00:19:18.240]   It's the whole cancel culture of the S.
[00:19:18.240 --> 00:19:22.400]   That, and it's right in the mosque,
[00:19:22.400 --> 00:19:23.600]   which we'll get to in a few minutes,
[00:19:23.600 --> 00:19:26.160]   which we'll also have to give Mike his blood pressure
[00:19:26.160 --> 00:19:27.200]   medication when we get to.
[00:19:27.200 --> 00:19:32.320]   Where it's a weird phrasing now is basically,
[00:19:32.320 --> 00:19:35.360]   anybody can say absolutely anything.
[00:19:35.360 --> 00:19:37.440]   And if you restrict that in any way,
[00:19:37.440 --> 00:19:43.760]   because you have standards, then you're going to be accused of canceling,
[00:19:43.760 --> 00:19:46.400]   and then I, the speaker of this obnoxious speech,
[00:19:46.400 --> 00:19:48.400]   and the victim.
[00:19:48.400 --> 00:19:51.520]   And so it's a weird way to turn white people into victims all the time.
[00:19:51.520 --> 00:19:57.040]   - Yeah, sorry.
[00:19:57.040 --> 00:20:02.080]   - In the future of the car, Elon gave a keynote address
[00:20:02.080 --> 00:20:06.240]   in which he talked a little bit more about his plans
[00:20:06.960 --> 00:20:08.720]   for Twitter to which.
[00:20:08.720 --> 00:20:10.960]   - To whether off the cuff, wouldn't you say there, Mike?
[00:20:10.960 --> 00:20:13.440]   - It's always off the cuff.
[00:20:13.440 --> 00:20:15.600]   He has no deep understanding of any of this.
[00:20:15.600 --> 00:20:17.280]   Everything that he says is off the cuff.
[00:20:17.280 --> 00:20:19.120]   - Yeah, and should it, we should be twitching.
[00:20:19.120 --> 00:20:20.160]   See, here's the question.
[00:20:20.160 --> 00:20:24.480]   Should we be treating it that way, or should we take it seriously?
[00:20:24.480 --> 00:20:26.880]   - I mean, he sounds like he's making it up as he goes along.
[00:20:26.880 --> 00:20:28.560]   - We should take the threat seriously,
[00:20:28.560 --> 00:20:30.080]   but he has no serious thought.
[00:20:30.080 --> 00:20:33.920]   - Yeah, I mean, he has instincts, and he's going off of his gut.
[00:20:33.920 --> 00:20:38.960]   And to be totally fair, I understand where he's coming from.
[00:20:38.960 --> 00:20:42.000]   And that instinct is a natural instinct that a lot of people have.
[00:20:42.000 --> 00:20:45.360]   And a lot of people who use these services have the very same
[00:20:45.360 --> 00:20:50.880]   instincts that he has, which is that these services are ways in which people can speak
[00:20:50.880 --> 00:20:51.760]   and speak broadly.
[00:20:51.760 --> 00:20:54.400]   And the idea that somebody can step in and say that you can't speak
[00:20:54.400 --> 00:20:57.440]   has a feeling that feels wrong.
[00:20:57.440 --> 00:20:59.840]   And so I understand where he's coming from,
[00:20:59.840 --> 00:21:03.600]   but anyone who's spent any time looking at and understanding
[00:21:03.840 --> 00:21:07.200]   the nuances and the depth and the different challenges involved in that
[00:21:07.200 --> 00:21:09.360]   recognize that it's a lot more complex than that.
[00:21:09.360 --> 00:21:14.400]   And the thing that really bothers me is his unwillingness to even engage with the
[00:21:14.400 --> 00:21:17.600]   nuance and complexity involved in all of this.
[00:21:17.600 --> 00:21:22.800]   And just to assume that the instinctual first thought that he has on this issue
[00:21:22.800 --> 00:21:26.560]   is the correct one, even when it contradicts itself.
[00:21:26.560 --> 00:21:30.320]   And that's part of it, is that so much of what he says actually contradicts himself.
[00:21:30.960 --> 00:21:34.800]   And so the whole thing is extremely bizarre.
[00:21:34.800 --> 00:21:35.360]   It is weird.
[00:21:35.360 --> 00:21:37.680]   And the truth is he still may not get it.
[00:21:37.680 --> 00:21:39.280]   He's trying to raise more money.
[00:21:39.280 --> 00:21:45.200]   A surprising number of people have bought into his plans for Twitter.
[00:21:45.200 --> 00:21:47.920]   He's got, he had to come up with 21 billion.
[00:21:47.920 --> 00:21:54.880]   He's raised 7 billion so far, 5.2 billion in equity commitments from 18 investors,
[00:21:54.880 --> 00:22:00.080]   plus a rollover of 35 million Twitter shares from Saudi Prince al-Owee'd been to Lal.
[00:22:00.640 --> 00:22:03.600]   A billion from Larry Ellison.
[00:22:03.600 --> 00:22:06.560]   Who, by the way, who was going to resist this supposedly?
[00:22:06.560 --> 00:22:08.880]   And his, that word lasted about 30 seconds.
[00:22:08.880 --> 00:22:09.200]   Yeah.
[00:22:09.200 --> 00:22:11.600]   800 million from Sequoia capital.
[00:22:11.600 --> 00:22:18.800]   Again, you'd think, you know, savvy investors, but they're buying Elon's,
[00:22:18.800 --> 00:22:22.640]   you know, PowerPoint deck in which he says, well, that's easy.
[00:22:22.640 --> 00:22:24.800]   We're just going to triple revenue and then we'll go public.
[00:22:24.800 --> 00:22:30.800]   From quadruple.
[00:22:30.800 --> 00:22:35.680]   700 million from Vye capital, 500 million from Binance, 400 million from Andres and Horowitz.
[00:22:35.680 --> 00:22:38.160]   Andres and Horowitz, well, what's this name?
[00:22:38.160 --> 00:22:39.360]   Not Andres and the other guy.
[00:22:39.360 --> 00:22:41.680]   Horowitz, Ben Horowitz.
[00:22:41.680 --> 00:22:42.800]   Yeah, Ben Horowitz.
[00:22:42.800 --> 00:22:43.040]   Thank you.
[00:22:43.040 --> 00:22:46.880]   Just had this ridiculous, so Haggie, Haggie, graphic, how are you?
[00:22:46.880 --> 00:22:49.920]   Or say it, tweet about how brilliant Elon is now.
[00:22:49.920 --> 00:22:50.480]   Amazing.
[00:22:50.480 --> 00:22:51.600]   Well, that's what you're buying, right?
[00:22:51.600 --> 00:22:52.080]   Anywhere.
[00:22:52.080 --> 00:22:52.800]   Look what he's done.
[00:22:52.800 --> 00:22:55.440]   We'll give him money for anything he wants because he's so brilliant.
[00:22:55.440 --> 00:23:00.560]   Jack Dorsey, maybe, who could have rolled 2.4% in, has not,
[00:23:00.560 --> 00:23:06.800]   despite the Haggie graphic things he said about Elon Musk himself, nor his founder's fund,
[00:23:06.800 --> 00:23:10.160]   another big true believer.
[00:23:10.160 --> 00:23:13.840]   So there's still some question whether he's going to get it.
[00:23:13.840 --> 00:23:19.440]   Maybe that's one of the reasons he's so fast and loose at a financial time.
[00:23:19.440 --> 00:23:20.640]   He tries to get the price down.
[00:23:20.640 --> 00:23:25.920]   If he tries to get the price down because the market's going to hell and my FU money is telling
[00:23:25.920 --> 00:23:33.520]   me to go FU for one, that's an excuse for Twitter to back out if they had any spine, right?
[00:23:33.520 --> 00:23:35.520]   They would have to pay a $1 billion.
[00:23:35.520 --> 00:23:36.480]   No, no, no, no.
[00:23:36.480 --> 00:23:40.400]   If he says the opposite, if he says, no, I'm not going to buy it unless you lower the price
[00:23:40.400 --> 00:23:42.000]   than Twitter says we're not lower than the price.
[00:23:42.000 --> 00:23:44.400]   So then Musk could be the one back he knows.
[00:23:44.400 --> 00:23:45.520]   Musk would then have to back out.
[00:23:45.520 --> 00:23:46.880]   He has a fine or penalty.
[00:23:46.880 --> 00:23:48.080]   He has a billion dollar.
[00:23:48.080 --> 00:23:49.280]   That's a billion dollar.
[00:23:49.280 --> 00:23:51.520]   Pull out a fee too.
[00:23:51.520 --> 00:23:57.360]   He said at the Financial Times conference, I do not think, I'm sorry, I do think it was
[00:23:57.360 --> 00:23:58.000]   not correct.
[00:23:58.000 --> 00:24:00.880]   Very hard to get these negatives in order.
[00:24:00.880 --> 00:24:03.760]   I do think it was not correct to ban Donald Trump.
[00:24:03.760 --> 00:24:07.520]   I think that was a mistake because it alienated a large part of the country
[00:24:07.520 --> 00:24:10.160]   and did not ultimate.
[00:24:10.160 --> 00:24:11.600]   Is that why it was a mistake?
[00:24:11.600 --> 00:24:14.480]   And did not ultimately result in Donald Trump not having a voice?
[00:24:14.480 --> 00:24:17.360]   Well, no, he was the president of the freak in the United States.
[00:24:17.360 --> 00:24:18.960]   I think he had a voice.
[00:24:18.960 --> 00:24:22.480]   That wasn't the idea.
[00:24:22.480 --> 00:24:23.600]   I really like what was it?
[00:24:23.600 --> 00:24:27.360]   Sean E who said, look, don't moderate based on content, moderate based on tone.
[00:24:27.360 --> 00:24:29.360]   Right.
[00:24:29.360 --> 00:24:31.040]   Big difference.
[00:24:31.040 --> 00:24:32.160]   But you got to moderate.
[00:24:32.160 --> 00:24:32.480]   Free.
[00:24:32.480 --> 00:24:40.320]   And as you pointed out, Mike on Tech Dirt, Elon's points of York are contradicting.
[00:24:40.320 --> 00:24:42.000]   Yeah.
[00:24:42.000 --> 00:24:44.240]   I mean, he says a lot of different things.
[00:24:44.240 --> 00:24:49.040]   And so he keeps talking about free speech as being the most important and not banning people.
[00:24:49.040 --> 00:24:53.680]   And then at the same time, he will say that his top priority, and he said this multiple times,
[00:24:53.680 --> 00:24:59.200]   including at that conference yesterday, the Financial Times conference, that his top priority
[00:24:59.200 --> 00:25:02.320]   is to get rid of spammers and scam bots on the site.
[00:25:02.320 --> 00:25:05.280]   Who have every right to speak because they're protected?
[00:25:05.280 --> 00:25:08.560]   Their speech is absolutely protected under the First Amendment.
[00:25:08.560 --> 00:25:11.280]   So if he really supports the First Amendment, he wouldn't be doing that.
[00:25:11.280 --> 00:25:16.640]   It also says a lot that he thinks that that is the biggest problem, which for bigger users
[00:25:16.640 --> 00:25:18.480]   does appear to be maybe a problem.
[00:25:18.480 --> 00:25:22.480]   It's certainly there are people who know it's a problem for him.
[00:25:22.480 --> 00:25:24.480]   It's very much a problem for him.
[00:25:24.480 --> 00:25:29.280]   But like, you know, I don't say every once in a while, I'll see one, but it's really not that big of a deal.
[00:25:29.280 --> 00:25:31.920]   But he is the biggest problem.
[00:25:31.920 --> 00:25:34.640]   What do I see as the biggest problem?
[00:25:35.760 --> 00:25:42.400]   Elon Musk on his site.
[00:25:42.400 --> 00:25:47.200]   I'm not going to sit here and just all out defend Elon Musk on this.
[00:25:47.200 --> 00:25:50.720]   But he's one of the smartest cats on the planet.
[00:25:50.720 --> 00:25:55.280]   And if he's not one of the smartest dudes on the planet, he knew how to get a lot of the smartest
[00:25:55.280 --> 00:25:59.840]   people on the planet in the same room to get things done over the last few years.
[00:25:59.840 --> 00:26:03.200]   Again, Twitter is a business.
[00:26:03.200 --> 00:26:06.400]   And if he can figure out a way to help make that business
[00:26:06.400 --> 00:26:09.440]   thrive, then more power to him.
[00:26:09.440 --> 00:26:13.840]   I'm not sitting here saying he's going to make Twitter better, but I don't know if he can really
[00:26:13.840 --> 00:26:15.440]   make it any worse than what it is.
[00:26:15.440 --> 00:26:16.480]   Oh, he could make it worse.
[00:26:16.480 --> 00:26:17.120]   He can.
[00:26:17.120 --> 00:26:18.800]   He can turn it in.
[00:26:18.800 --> 00:26:19.760]   He's not smart.
[00:26:19.760 --> 00:26:20.400]   I disagree with you.
[00:26:20.400 --> 00:26:20.880]   He's not.
[00:26:20.880 --> 00:26:22.080]   He can turn it into a 4chan.
[00:26:22.080 --> 00:26:22.800]   Yeah.
[00:26:22.800 --> 00:26:24.560]   So if you don't moderate, what do you get?
[00:26:24.560 --> 00:26:25.360]   You get 4chan.
[00:26:25.360 --> 00:26:26.480]   You get 8chan room.
[00:26:26.480 --> 00:26:27.840]   Yeah.
[00:26:27.840 --> 00:26:30.400]   So, so, so I think there are a few things.
[00:26:30.400 --> 00:26:34.400]   One, I think that, you know, to his credit, one of the things that he has shown that he can do
[00:26:34.400 --> 00:26:39.120]   really, really well is come up with sort of a big project and get the people together to
[00:26:39.120 --> 00:26:39.920]   actually accomplish it.
[00:26:39.920 --> 00:26:42.720]   And these are big projects that other people did not think were possible.
[00:26:42.720 --> 00:26:46.880]   And that includes like a mass market electric vehicle before anyone else was willing to do it
[00:26:46.880 --> 00:26:48.560]   and the whole like flying to space thing.
[00:26:48.560 --> 00:26:53.440]   And those are those are huge, massive projects that most people thought that were impossible.
[00:26:53.440 --> 00:26:56.640]   But those are very different than speech.
[00:26:56.640 --> 00:27:02.320]   And what you're dealing with with Twitter is speech and to be more specific about it,
[00:27:02.320 --> 00:27:05.520]   society and the way that people interact with each other.
[00:27:05.520 --> 00:27:07.360]   And that is a different kind of problem.
[00:27:07.360 --> 00:27:08.800]   It is not a problem of building something.
[00:27:08.800 --> 00:27:10.240]   It's not a problem of manufacturing.
[00:27:10.240 --> 00:27:13.680]   It's not a problem of like, you know, figuring out the physics of anything.
[00:27:13.680 --> 00:27:15.120]   It is figuring out people.
[00:27:15.120 --> 00:27:17.680]   And if there's one thing that he has shown that he is not good at,
[00:27:17.680 --> 00:27:20.720]   it is sort of figuring out people and how they interact with each other.
[00:27:20.720 --> 00:27:22.560]   Yeah, his tweet says that.
[00:27:22.560 --> 00:27:23.120]   Yeah.
[00:27:23.120 --> 00:27:25.920]   The way if you want a Twitter that's like Elon Musk's
[00:27:26.560 --> 00:27:29.280]   Twitter verse, I guess that's okay.
[00:27:29.280 --> 00:27:30.320]   It's not something I'd want to.
[00:27:30.320 --> 00:27:32.400]   But then that's the beauty of Twitter.
[00:27:32.400 --> 00:27:33.920]   You don't have to follow Elon Musk.
[00:27:33.920 --> 00:27:40.160]   I mean, does he take away our tools to moderate?
[00:27:40.160 --> 00:27:43.920]   Does he, does he, you know, you can't block anybody because because you have to listen to
[00:27:43.920 --> 00:27:44.320]   everybody?
[00:27:44.320 --> 00:27:46.480]   No, I don't think he wants to do that.
[00:27:46.480 --> 00:27:47.440]   I don't think he wants to.
[00:27:47.440 --> 00:27:48.160]   But it just becomes absurd.
[00:27:48.160 --> 00:27:50.400]   He has no idea where he lines on.
[00:27:50.400 --> 00:27:57.280]   Ants point of view is when I hear a lot from, you know, Elon Musk stands who email us and saying,
[00:27:57.280 --> 00:27:58.880]   why did you have somebody under the Fed to Elon?
[00:27:58.880 --> 00:27:59.920]   So I'm glad you said that.
[00:27:59.920 --> 00:28:00.880]   Ant.
[00:28:00.880 --> 00:28:02.560]   There are a couple of possibilities.
[00:28:02.560 --> 00:28:05.760]   One, you know, what have you done for me lately?
[00:28:05.760 --> 00:28:11.040]   Elon has not shown necessarily a good business acumen and a number of things,
[00:28:11.040 --> 00:28:13.200]   despite the fact that the stock makes him the richest man.
[00:28:13.200 --> 00:28:14.560]   He's still working on the stock.
[00:28:14.560 --> 00:28:14.880]   Yeah.
[00:28:14.880 --> 00:28:19.840]   By the way, this whole process has tanked his personal fortune by
[00:28:19.840 --> 00:28:23.680]   something like $30 billion as the Tesla stock goes down.
[00:28:23.680 --> 00:28:25.680]   So was this a good business move?
[00:28:25.680 --> 00:28:29.200]   Well, it remains to be seen, but so far, not so hard.
[00:28:29.200 --> 00:28:31.680]   And every car maker is going to compete with Tesla.
[00:28:31.680 --> 00:28:38.400]   I've said Tesla's value is so through the roof, so unreasonably high at this point,
[00:28:38.400 --> 00:28:42.000]   he's worth more than any other manufacturer of automobiles.
[00:28:42.000 --> 00:28:46.160]   The space business has a couple of customers and it also has new and upcoming competitors.
[00:28:47.200 --> 00:28:51.440]   The space business, I'm going to give him more credit for the space business even than Tesla.
[00:28:51.440 --> 00:28:53.440]   SpaceX has really shown that you can do this.
[00:28:53.440 --> 00:28:58.160]   He didn't engineer any other people together, but I'm not saying that I don't think he's
[00:28:58.160 --> 00:28:59.840]   as smart a business business.
[00:28:59.840 --> 00:29:00.000]   He is.
[00:29:00.000 --> 00:29:03.680]   And I was thinking about this today with Google when we get to them,
[00:29:03.680 --> 00:29:07.600]   you know, they had the easy life of dealing with information.
[00:29:07.600 --> 00:29:11.920]   Mark Zuckerberg and the Twitter founders had the hard life of dealing with human beings.
[00:29:11.920 --> 00:29:12.960]   Human beings are tough.
[00:29:12.960 --> 00:29:17.680]   And society, human beings, as Mike said, is what's tough here.
[00:29:17.680 --> 00:29:20.640]   And Elon has no idea clearly how to deal with human beings.
[00:29:20.640 --> 00:29:28.480]   And yes, this Mike and maybe to, as Twitter failed in the last 15 years to bring in the
[00:29:28.480 --> 00:29:30.640]   smartest people to try to solve this, have they not?
[00:29:30.640 --> 00:29:37.600]   Admittedly, it's a hard problem, but have they not done the best they could to solve these issues
[00:29:37.600 --> 00:29:39.120]   over the last 15 years?
[00:29:40.000 --> 00:29:43.440]   I mean, obviously everybody could do better, right?
[00:29:43.440 --> 00:29:44.320]   There are ways to do better.
[00:29:44.320 --> 00:29:48.560]   But I had written an article a few weeks ago where I went through and I looked at,
[00:29:48.560 --> 00:29:53.760]   and this is in part because people were saying I was being unfair to Elon,
[00:29:53.760 --> 00:29:57.040]   and I was interpreting everything he said in the worst possible light.
[00:29:57.040 --> 00:30:00.640]   And so I went through everything that he had said about his plans at the time
[00:30:00.640 --> 00:30:05.280]   for Twitter and looked at it and said, "In the best possible light, what does this look like?"
[00:30:05.280 --> 00:30:08.800]   And then I looked at the things that Twitter is already doing and that they're already working
[00:30:08.800 --> 00:30:12.640]   on that the current management has set in motion.
[00:30:12.640 --> 00:30:18.320]   And almost everything that he said, if you took the best interpretation of it,
[00:30:18.320 --> 00:30:25.840]   the most free speech supporting, the most open system, the most transparent setup at all,
[00:30:25.840 --> 00:30:31.520]   Twitter is working towards all of that, but doing it thoughtfully and actually doing it carefully.
[00:30:31.520 --> 00:30:37.120]   It doesn't mean they've done it well to date or that the eventual things that they're going to roll
[00:30:37.120 --> 00:30:41.840]   out will meet that, but they've taken all of these steps in that direction, but very carefully,
[00:30:41.840 --> 00:30:45.360]   because one of the things that they've realized is that these things are really tricky, and if you
[00:30:45.360 --> 00:30:49.680]   do it wrong, you can create a much bigger mess than what you have today.
[00:30:49.680 --> 00:30:55.600]   So they have these different projects in terms of algorithmic choice, which has been Jack Dorsey's
[00:30:55.600 --> 00:31:00.400]   big thing, allowing people to choose their own algorithm and allowing third parties to create
[00:31:00.400 --> 00:31:04.720]   algorithms. That is not working yet because it's a very difficult problem to solve, and you have to
[00:31:04.720 --> 00:31:09.120]   do that very carefully. You have to do that in a way that won't be abused and won't make the spam
[00:31:09.120 --> 00:31:13.760]   and the scamming problems worse. In terms of freeing stuff up and opening stuff up,
[00:31:13.760 --> 00:31:18.960]   Twitter funded blue sky, which is now a separate organization that is trying to build out this
[00:31:18.960 --> 00:31:26.240]   sort of social media protocol that Twitter could then use, which is sort of what I think Musk might
[00:31:26.240 --> 00:31:30.880]   mean when he says open sourcing the algorithm, just purely open sourcing the algorithm and
[00:31:30.880 --> 00:31:34.480]   putting it on GitHub as he keeps saying, doesn't do what I think he thinks it would do,
[00:31:34.480 --> 00:31:39.600]   and also certainly opens up the system much more to the spammers and scammers that he's so upset about.
[00:31:39.600 --> 00:31:44.960]   He said yesterday he wants to put it on GitHub, put the algorithm on GitHub, which...
[00:31:44.960 --> 00:31:52.720]   That's not how it works. That's just opening up all sorts of questionable things.
[00:31:52.720 --> 00:31:59.040]   Twitter, I think, is trying to be as transparent and as open as possible, but they're doing it step
[00:31:59.040 --> 00:32:03.600]   by step very carefully because they recognize if they get that wrong and they open up their systems
[00:32:03.600 --> 00:32:08.560]   in a way that leads to a vulnerability in the system or other kind of security floor,
[00:32:08.560 --> 00:32:13.520]   floor, or opens it up to spammers and scammers and whoever else, that's not good.
[00:32:13.520 --> 00:32:18.720]   And so when you look at the things, if you look at the best possible version of what he's talking
[00:32:18.720 --> 00:32:23.360]   about, and then you look at what Twitter is actually doing, not the sort of fantasy that you'll hear
[00:32:23.360 --> 00:32:28.480]   on Fox News about what Twitter is doing, you begin to realize the company is actually doing
[00:32:28.480 --> 00:32:35.120]   some really thoughtful things. And I think, have really hired some really good people who are taking
[00:32:35.120 --> 00:32:43.520]   careful moves in this direction, but it is somewhat slow going. And so maybe if he is able to understand
[00:32:43.520 --> 00:32:48.000]   that, the things that the company is doing, that could be good. And I do think, by the way,
[00:32:48.000 --> 00:32:53.760]   none of that's algorithmic. Sure. I might point out, what's the algorithm he's talking about?
[00:32:53.760 --> 00:32:59.760]   What algorithm are you talking about? It's not like Facebook. I mean, if you can follow Twitter
[00:32:59.760 --> 00:33:05.040]   chronologically, or you can follow the best tweets, I guess that's the algorithm he's talking about,
[00:33:05.040 --> 00:33:11.040]   but that's not the most important part of any of this. There is an algorithm in terms of which
[00:33:11.040 --> 00:33:15.040]   tweets are recommended to you and which are not. And he is concerned about that.
[00:33:15.040 --> 00:33:20.160]   Right. Yes. And as Jack Dorsey has pointed out, Twitter is the only one of these companies
[00:33:20.160 --> 00:33:24.400]   that allows you to turn off the algorithm completely and get a purely chronologically or reverse
[00:33:24.400 --> 00:33:29.440]   chronological feed if you want it. But the one thing I will say is that the one thing that I do
[00:33:29.440 --> 00:33:39.040]   think is good about this move is turning Twitter into a private company again, taking it out of Wall
[00:33:39.040 --> 00:33:44.720]   Street. Because I think a lot of the problems that we've seen have actually been caused by a few
[00:33:44.720 --> 00:33:50.800]   institutional investors that have been really demanding huge revenue growth and huge user growth.
[00:33:50.800 --> 00:33:56.800]   And they are pushing the company to make these moves in the very short term to hit these quarterly
[00:33:56.800 --> 00:34:03.520]   numbers that I think has caused Twitter to make some dumb decisions along the way. And if you take
[00:34:03.520 --> 00:34:08.320]   them out of that quarterly reporting system and let them take a longer term view, I think the company
[00:34:08.320 --> 00:34:14.000]   could do some really amazing things with it. And so to that aspect of Twitter going private,
[00:34:14.000 --> 00:34:19.360]   I think is valuable. But I'm not sure that that that will work very well with Elon.
[00:34:19.360 --> 00:34:25.680]   Only with a better rotor. Yeah. Well, as I said, I'm not on there.
[00:34:25.680 --> 00:34:32.880]   I mean, I honestly think that that could be good. But he's not really shown that that's that's
[00:34:32.880 --> 00:34:38.720]   part of a skill style with the Washington Post. Yeah. But has he ever shown anything that he's
[00:34:38.720 --> 00:34:43.600]   involved in? He seems to be very, very hands on, on about so, you know, the other thing that just
[00:34:43.600 --> 00:34:48.400]   go ahead. Sorry. I was gonna say the other thing that is worth mentioning that happened this week
[00:34:48.400 --> 00:34:51.840]   with him was that, you know, when it was announced that he was going to take over,
[00:34:51.840 --> 00:34:55.920]   you know, some European politicians got very upset and sort of, you know,
[00:34:55.920 --> 00:35:01.840]   put out this letter that was like, dear Elon Musk, we have rules and, you know, basically said,
[00:35:01.840 --> 00:35:08.320]   Oh, yeah. We talked about that last week. Yeah. But but on on Monday, that guy, the guy who wrote
[00:35:08.320 --> 00:35:13.680]   that letter, who's who's the heads like the digital, I forget exactly what his roles, but he's sort of
[00:35:13.680 --> 00:35:21.280]   like the digital guy in the European Parliament. He went to Austin, Texas to meet with with Elon,
[00:35:21.280 --> 00:35:25.520]   and they filmed this weird video. I mean, it really looks kind of like a hostage video,
[00:35:25.520 --> 00:35:31.600]   where, where he says, you know, I'm glad I was able to explain to you the DSA, which is the Digital
[00:35:31.600 --> 00:35:37.040]   Services Act, which is the new, you know, one of the big two new laws coming out of Europe,
[00:35:37.040 --> 00:35:40.400]   regarding just everything Musk would should hate. Everything that he should
[00:35:40.400 --> 00:35:42.880]   hate. Everything that he has all that. And and forced to moderate.
[00:35:42.880 --> 00:35:47.840]   Yes. And even more importantly, something that Twitter has actually spent a lot of time
[00:35:47.840 --> 00:35:52.880]   very vocally, very publicly pushing back against and saying, you know, if this law goes into effect
[00:35:52.880 --> 00:35:57.520]   in this way, it will negatively impact speech and it will suppress speech. And it is a problem.
[00:35:57.520 --> 00:36:02.960]   And here are changes that need to be made. And this guy goes goes to Texas meets with Elon,
[00:36:02.960 --> 00:36:06.960]   and then films this video, where he says, I'm glad I was able to explain the DSA to you.
[00:36:06.960 --> 00:36:11.280]   And it sounds like you agree with me now. And Elon stands there and says, yes, we're completely
[00:36:11.280 --> 00:36:16.000]   aligned. I agree with everything that you're saying. And I want to support the DSA, which is like,
[00:36:16.000 --> 00:36:21.440]   you know, for him to go out and say, yeah, and and for him to go out and say like he's supporting
[00:36:21.440 --> 00:36:25.440]   free speech, you know, Twitter has been pushing back against this law over and over again, saying,
[00:36:25.440 --> 00:36:30.400]   this is bad for speech, this will harm speech. And then immediately to have Elon come in,
[00:36:30.400 --> 00:36:35.040]   say he's the free speech guy, and then say, oh, yeah, we support this approach, which will require
[00:36:35.040 --> 00:36:39.840]   Twitter to take down all sorts of content, you know, legal content is the structure the DSA is
[00:36:39.840 --> 00:36:45.600]   really, really problematic. It's just bizarre and suggests that again, he just doesn't understand
[00:36:45.600 --> 00:36:50.480]   what's actually happening. He's not listening. He's not really, really grocciness. But to your
[00:36:50.480 --> 00:36:56.880]   point, so somebody an hour ago said, I predicted Elon mess will eventually ask Jack at Jack to be
[00:36:56.880 --> 00:37:01.440]   CEO of Twitter, to which Jack responded 20 minutes ago. Nah, I'll never be CEO.
[00:37:01.440 --> 00:37:06.080]   No, I that's the last thing Jack Dorsey would ever want to do is to work for Elon Musk.
[00:37:06.080 --> 00:37:12.080]   Work for Elon Musk, kid me. Elon really gives the impression much like Donald Trump did of
[00:37:12.080 --> 00:37:15.680]   going along with whoever the last person he talked to. Yeah.
[00:37:15.680 --> 00:37:21.760]   Sad like, Oh, yeah, that's a good idea. I'll do that. Oh, yeah. Here he is with Terry. What's his
[00:37:21.760 --> 00:37:28.080]   name? Body language is hilarious. Yeah. Time to use a DSA on your regression in Europe. And I think
[00:37:28.080 --> 00:37:32.160]   that now you understand very well. Yeah. It's pretty well with what you think we should do on a
[00:37:32.160 --> 00:37:36.160]   platform. Yeah, I think it's exactly aligned with my thinking. I think very much agree with
[00:37:36.160 --> 00:37:44.560]   a great discussion. And I really think I agree with everything you said really. Oh my god,
[00:37:44.560 --> 00:37:49.760]   that's hysterical. That's hysterical. Yeah. I'm going to pull down everything the
[00:37:49.760 --> 00:37:54.560]   European Union wants me to pull down. It's a teary, but you are so strong.
[00:37:54.560 --> 00:38:00.400]   Donald Trump up. Then everything's wrong. Great meeting. We are very much on the same page,
[00:38:00.400 --> 00:38:05.840]   tweets, Elon Musk. But what this all says is, is where Jack was right when he when he did his
[00:38:05.840 --> 00:38:12.480]   confession of the problem is that was centralized into a company and shouldn't have been. The
[00:38:12.480 --> 00:38:18.560]   internet's still young. And this teaches us that these human entities, Facebook, Twitter,
[00:38:18.560 --> 00:38:22.240]   can't be in this structure. We talked about this last week. We got to keep talking about it,
[00:38:22.240 --> 00:38:27.680]   but it's going to require some building or some using of things to break out of this. I still
[00:38:27.680 --> 00:38:31.600]   have hope for Blue Sky. Mike, do you have any hope for Blue Sky? Yeah, I have tremendous hope
[00:38:31.600 --> 00:38:38.560]   for Blue Sky. I think it is slow going, but there are some really, really smart people there.
[00:38:38.560 --> 00:38:44.560]   And they are working on a really thoughtful approach to this. The real question is whether or not they
[00:38:44.560 --> 00:38:48.800]   can actually turn it into something that people will use. I think their approach is much smarter
[00:38:48.800 --> 00:38:53.440]   than almost any of the other ones that I've seen out there. So I am really, really hopeful.
[00:38:53.440 --> 00:39:00.080]   The thing was before this, we knew that Jack and that Parag really supported Blue Sky and that
[00:39:00.080 --> 00:39:05.120]   they were very supportive of it. And therefore that suggested that assuming Blue Sky built what was
[00:39:05.120 --> 00:39:09.600]   promised that Twitter would adopt it and bring the users. And once you brought the user base,
[00:39:09.600 --> 00:39:13.520]   you would have a lot of power to then do a whole bunch of other things. Now with Elon,
[00:39:13.520 --> 00:39:18.320]   he has yet to mention Blue Sky as far as I can tell. Some of the stuff he said sounds like he
[00:39:18.320 --> 00:39:23.200]   should support it, but it's unclear if he will. And therefore, Blue Sky might be kind of adrift
[00:39:23.200 --> 00:39:27.520]   and on its own and then having to find its own user base, which might represent a challenge.
[00:39:27.520 --> 00:39:32.720]   My theory is that Jack is being nice to Elon to try to in favor of Blue Sky.
[00:39:34.080 --> 00:39:40.080]   I hope so. Jack certainly keeps bringing up Blue Sky as one of the things that he believes in,
[00:39:40.080 --> 00:39:45.200]   that he supports. So hopefully he can get it through to Elon, that it's a good thing.
[00:39:45.200 --> 00:39:53.760]   I'm in this state where I don't know if it's good or not that Elon keeps not mentioning Blue Sky.
[00:39:53.760 --> 00:39:57.280]   Maybe it's better for everyone if he doesn't realize. He doesn't know about it.
[00:39:57.280 --> 00:40:03.440]   Right. And let it go. But the fact is, I think it would be really helpful for Blue Sky in the
[00:40:03.440 --> 00:40:09.120]   long run if Twitter were to adopt it and bring its users along. And then that opens up all sorts
[00:40:09.120 --> 00:40:15.200]   of really interesting possibilities of a world where you have social media as this kind of protocol
[00:40:15.200 --> 00:40:19.520]   setup, and then you can have all different people building on top of it. And then,
[00:40:19.520 --> 00:40:24.000]   to your hands-off point, then if Twitter becomes the everything goes platform where
[00:40:24.000 --> 00:40:30.720]   everybody can say anything, which is another way to say the internet, then okay. But then on top
[00:40:30.720 --> 00:40:36.960]   with Blue Sky, Mike can find just the good stuff for you, and I'm going to get rid of the bad stuff
[00:40:36.960 --> 00:40:41.520]   for you. And you pay me for it to do that for you, and you then have nice Twitter on top.
[00:40:41.520 --> 00:40:46.640]   That's a world I can live with because it's basically the internet.
[00:40:46.640 --> 00:40:54.400]   Yeah. I get it. Let's take a break. Enough Elon for the day.
[00:40:57.120 --> 00:41:02.720]   Thank goodness. Just remember, I've got Jeff Jarvis by my side.
[00:41:02.720 --> 00:41:12.320]   When we come back, Google, huh Google Hub Google I/O. I/O, it's off to work I go,
[00:41:12.320 --> 00:41:18.880]   we will talk about Google. Finally, AI will explain jokes to us when we come back. But first,
[00:41:18.880 --> 00:41:25.760]   and the Pixelbook. And the non-existent Pixelbook. It's exciting. It's really exciting. It's just going
[00:41:25.760 --> 00:41:32.400]   to be a lot of fun. But first, before we do that, I do want to talk a little bit about what a good
[00:41:32.400 --> 00:41:41.440]   night's sleep I had last night thanks to my eight sleep pod cover. The pod pro cover goes over my
[00:41:41.440 --> 00:41:45.520]   existing mattress. You can actually get an eight sleep mattress as well. So you can go either way.
[00:41:45.520 --> 00:41:50.640]   We got the pod pro cover because we like our mattress, goes right over the top. And we've had
[00:41:50.640 --> 00:41:56.000]   in the past, we've had heating blankets, electric blankets on top. Then we tried one
[00:41:56.000 --> 00:42:04.640]   with those mattress pad. This is like that only from the 22nd century. The eight sleep pod pro cover
[00:42:04.640 --> 00:42:14.960]   heats and cools. It can go as cool as 55 degrees Fahrenheit, as hot as 110 degrees Fahrenheit.
[00:42:14.960 --> 00:42:22.160]   And it has a built-in sleep doctor with biometric tracking that watches how you're sleeping and
[00:42:22.160 --> 00:42:29.120]   adjusts the heat to help you sleep better. And I can testify it really, really works. One of the
[00:42:29.120 --> 00:42:34.960]   30% of Americans struggle with sleep. And temperature is one of the main causes of course sleep. Too hot,
[00:42:34.960 --> 00:42:43.280]   too cold. I'm telling you, a great night's sleep is a game changer. It's nature's best medicine.
[00:42:43.280 --> 00:42:48.480]   It can reduce the likelihood of serious health issues, decrease the risk of heart disease,
[00:42:48.480 --> 00:42:53.920]   lower your blood pressure, even reduce the risk of Alzheimer's. And by the way, it's just nice
[00:42:53.920 --> 00:42:59.840]   to have a good night's sleep. I've set it up so that I start warm as I
[00:42:59.840 --> 00:43:06.720]   get deeper into sleep. It cools down and my sleep is as a result more deep. And then in the morning
[00:43:06.720 --> 00:43:11.600]   around eight o'clock, when I want to get up, instead of an alarm, I just have the bed start to heat up
[00:43:11.600 --> 00:43:16.800]   and I wake up naturally. It's incredible. The temperature of the cover will adjust each side
[00:43:16.800 --> 00:43:22.560]   of the bed based on sleep stages, biometrics, it knows the bedroom temperature. So and reacts
[00:43:22.560 --> 00:43:28.000]   intelligently to all three factors to create the optimal sleep environment. You wouldn't know I
[00:43:28.000 --> 00:43:32.240]   had good night's sleep last night. I have been I have been on the air since 10 this morning.
[00:43:32.240 --> 00:43:38.960]   Eight sleep users fall asleep up to 32% faster and reduce sleep interruptions by 40% overall get a
[00:43:38.960 --> 00:43:46.640]   more restful sleep. I love it. With 30% more deep sleep, you just know your mind and body are moving
[00:43:46.640 --> 00:43:53.840]   through those restorative sleep stages. You're getting a more restful sleep. You just feel great.
[00:43:53.840 --> 00:43:59.440]   I love our eight sleep cover. So does Lisa and she sleeps different than I do. So that's fine.
[00:43:59.440 --> 00:44:04.400]   But that's why there's two sides to the cover. And so she can have at the temperature. She wants
[00:44:04.400 --> 00:44:09.120]   I can have the temperature I want. Look at that. I'm just I always like to do this during the ad.
[00:44:09.120 --> 00:44:14.320]   Look at my sleep fitness from my eight sleep sleep tracker. Can you see that there?
[00:44:14.320 --> 00:44:24.080]   96% 90 my heart rate variability 74 milliseconds high is good. My my resting heart rate 58 beats
[00:44:24.080 --> 00:44:28.800]   per minute low is good. It's just fantastic. You could set the temperature. I'll turn it on right
[00:44:28.800 --> 00:44:34.240]   now, even though I'm not in the bed. I can turn it on remotely. You could set the temperature
[00:44:34.240 --> 00:44:43.280]   anywhere from minus 10 to plus 10, which is very hot. I won't say that hot. You can also set it
[00:44:43.280 --> 00:44:50.080]   initially to have a sleep graph. You see you really want this deep optimizing phase where your
[00:44:50.080 --> 00:44:54.480]   temperature goes down. This is why it's not good to sleep hot at night. And I can't wait for a
[00:44:54.480 --> 00:44:57.600]   hot summer night where I could actually be like having air conditioning in the bed. That's going
[00:44:57.600 --> 00:45:03.840]   to be really interesting. Autopilot is great. It's the thing that automatically watches what I'm doing
[00:45:03.840 --> 00:45:08.960]   and then makes recommendations watching all of the feedback that it's getting from the room
[00:45:08.960 --> 00:45:12.640]   temperature and how I'm sleeping and tossing and turning. It's really kind of brilliant.
[00:45:12.640 --> 00:45:22.800]   It is really an amazing technology. Go to eight sleep EIGHT sleep.com/twig. Check out the pod pro
[00:45:22.800 --> 00:45:29.760]   cover. You'll save $150 a check out. Eight sleep currently ships within the US, Canada and the UK.
[00:45:29.760 --> 00:45:39.120]   Eight sleep.com/twig. Show up as the best partner parent or version of yourself every morning with
[00:45:39.120 --> 00:45:44.480]   a great, wouldn't it just be so lovely to have a great night's sleep? You can. This thing is
[00:45:44.480 --> 00:45:50.640]   amazing. I really, Kevin Rose told me about it first. And then Amy Webb, who was on that show,
[00:45:50.640 --> 00:45:55.760]   she tried it. She said, "Oh, man, you got to get this." And we did. We put it, we installed about six
[00:45:55.760 --> 00:46:02.960]   months ago. I'm glad to be doing ads for him now though because I'm a believer. Eightsleep.com/twig.
[00:46:02.960 --> 00:46:07.200]   Thank you so much for your support of this week in Google. You support us when you use that address
[00:46:07.200 --> 00:46:14.000]   though. Please do that and you'll get $150 on the pod pro cover, $150 off. It's a great deal.
[00:46:14.000 --> 00:46:20.800]   Eightsleep.com/twig. So, what's your temperature? What's my current temperature?
[00:46:20.800 --> 00:46:25.360]   No, no. What's the temperature? You know, I don't know because they just say plus and minus. So,
[00:46:25.360 --> 00:46:32.560]   I start on my, I start, actually, I just turned it up just now. I shouldn't.
[00:46:32.560 --> 00:46:39.760]   I normally start it plus. That's not good. Plus, turn it down. I go plus four,
[00:46:40.720 --> 00:46:45.360]   normally. And then it goes down to plus two. I actually had it cooler than that. And it was a
[00:46:45.360 --> 00:46:48.960]   little, I would wake up in the middle of the night and be a little chilly. So, I adjusted it. And then
[00:46:48.960 --> 00:46:54.480]   the auto doc, autopilot, the sleep doctor kind of adjusted it. And so, this is, it's not a huge
[00:46:54.480 --> 00:46:59.360]   curve. So, you manually adjust it. And then I wake up, it's a plus three. I have to say, I like
[00:46:59.360 --> 00:47:02.560]   it a little warmer when I get in bed because it could be a little chilly at night. And so,
[00:47:02.560 --> 00:47:07.120]   I get in and that's just cozy and comfy and I fall asleep right away because of it. It's a really
[00:47:07.120 --> 00:47:12.640]   cool technology. I have to say, this is the, and you start dreaming of Elon and then the temperature
[00:47:12.640 --> 00:47:22.000]   goes, yeah, it goes way up, man, way up. Yeah, I like it. It's fun. So, I thought this is going to
[00:47:22.000 --> 00:47:30.880]   be easy because, and then I've lost it. Darn it. I had a cute sketch of all the things that were
[00:47:30.880 --> 00:47:35.680]   announced at Google, I know, and I just had it open. Let me see if I can find it again.
[00:47:36.800 --> 00:47:44.800]   Recently closed windows. Oh, rats. I found myself yawning during most of the day.
[00:47:44.800 --> 00:47:50.480]   I was less. Yeah. And you didn't have to work. So, so Jeff and Jason and I had to pay attention.
[00:47:50.480 --> 00:47:58.960]   Yeah, you lazy SOB. We were there work. I wake. Yeah. I have mixed feelings about it, to be honest
[00:47:58.960 --> 00:48:06.240]   with you. There was some really cool stuff, as always. But I also remember from previous Google
[00:48:06.240 --> 00:48:14.080]   IOs, a lot of cool stuff announced at Google I/O. Never materializes. There was less of that kind
[00:48:14.080 --> 00:48:21.920]   of a track record. Yeah. You feel like there was more good stuff. You thought there was something
[00:48:21.920 --> 00:48:27.840]   worthwhile. Oh, there was just less fire in the sky. Here it is. I found it. This is from
[00:48:29.840 --> 00:48:36.240]   Choo Kee Chan. She did a lovely little sketch. This is going to help us. Everything that Google
[00:48:36.240 --> 00:48:44.720]   said at the keynote. A little drawing. They talked about Google Maps and YouTube. Did they
[00:48:44.720 --> 00:48:49.520]   talk about YouTube? Oh, yeah. A little bit about YouTube and Google Translate, Google Assistant.
[00:48:49.520 --> 00:48:54.960]   They talked about Lambda and Android 13. They did announce some hardware, but they did in kind of
[00:48:54.960 --> 00:49:02.000]   way that I think disappointed a lot of people. There is going to be, as we knew, a new Pixel 6A,
[00:49:02.000 --> 00:49:12.240]   but you won't be able to order it until what? July 21st. Then the liver July 28th. I have to
[00:49:12.240 --> 00:49:17.440]   think that's going to be supply chain issues. $449. Looks very nice. Got a lot of the same
[00:49:17.440 --> 00:49:22.480]   features. Maybe not the cameras. Maybe a little less than the 6 Pro, but you'd expect that.
[00:49:23.040 --> 00:49:29.200]   They also preempted the gossip monkers, the rumor mill by saying, and by the way, this is what the
[00:49:29.200 --> 00:49:34.800]   Pixel 7 is going to look at this fall. Unibody aluminum. Looks nice. Yeah. Next generation 10
[00:49:34.800 --> 00:49:41.200]   sort ship. They also mentioned the watch, but they said, because remember we saw the stolen watch,
[00:49:41.200 --> 00:49:47.360]   Rick Osterlo came out. He was wearing it secretly underneath his sleeve, but that's not going to
[00:49:47.360 --> 00:49:50.720]   come out. You didn't say a price to do, Leo. Didn't say a price. No price.
[00:49:50.720 --> 00:49:55.360]   He said next year. No, no. The watch comes out with 7 this fall.
[00:49:55.360 --> 00:50:06.960]   Next year is what's slated for the Pixel tablet. They really pushed the tablet on developers,
[00:50:06.960 --> 00:50:10.880]   like, "Develop for the tablet. Develop for the tablet." So they said, "We're going to have a
[00:50:10.880 --> 00:50:16.800]   tensor-based Pixel tablet," which looked exactly like they had ripped the screen off a Nest Hub Max,
[00:50:17.680 --> 00:50:22.560]   and said, "Here's our tablet." Not impressive. Not impressive. We can move through it if you want,
[00:50:22.560 --> 00:50:29.440]   bit by bit, Sundar Pichai was the MC. It was live. Shoreline amphitheater as it had been
[00:50:29.440 --> 00:50:33.040]   before COVID. Have you ever gone to a Google I/O, Mike Masnick?
[00:50:33.040 --> 00:50:38.080]   Yeah. I actually went to a bunch way back when it used to be at Moscone.
[00:50:38.080 --> 00:50:47.360]   Yeah. I used to enjoy it. But then after, I probably went three or four years in a row,
[00:50:47.360 --> 00:50:51.040]   and then I started to realize, "I wasn't sure I was actually getting that much out of it."
[00:50:51.040 --> 00:50:58.640]   It's interesting. That's exactly the feeling I had today. At the time you're eating it,
[00:50:58.640 --> 00:51:03.840]   it's like cotton candy. It seems like it's really good. And then you're hungry an hour later.
[00:51:03.840 --> 00:51:10.640]   What was the substance of it? It's hard for me to pinpoint. Yeah, AI is getting smarter.
[00:51:10.640 --> 00:51:16.480]   There's going to be some stuff translate. They showed some glasses. What did you think of the
[00:51:16.480 --> 00:51:21.600]   glasses, Mike? I haven't seen the details. It's just like AR glasses.
[00:51:21.600 --> 00:51:29.280]   They have a couple of people, like a mother and daughter. The daughter speaks English. The
[00:51:29.280 --> 00:51:34.240]   mother can understand English, but can't speak in it. The daughter doesn't speak Mandarin. The
[00:51:34.240 --> 00:51:44.080]   mother does. These are like the old man glasses from up, the big black, the big thick temple.
[00:51:44.720 --> 00:51:50.400]   They put them on. The thing that was cool is the text, as the mother speaking in Mandarin,
[00:51:50.400 --> 00:51:56.720]   shows up in a heads-up display on the lens so the daughter can read in English
[00:51:56.720 --> 00:52:01.760]   what the mother's saying. But that's one way. Remember, they showed something similar with the
[00:52:01.760 --> 00:52:09.200]   Google earbuds. It was like the Babel fisher. You put it in your ear and she talks. I'm hearing
[00:52:09.200 --> 00:52:15.120]   it in English and I talk and she's hearing it in Mandarin. Did it ever happen?
[00:52:15.120 --> 00:52:19.920]   You have to translate. At least not only here. You put the phone out and it'll do it, I guess.
[00:52:19.920 --> 00:52:23.040]   The phone does a pretty good job, but they haven't figured out the UI for the rest.
[00:52:23.040 --> 00:52:27.280]   So they showed this thing in the three years, four years ago. They showed the earbuds.
[00:52:27.280 --> 00:52:32.800]   It still doesn't happen. I feel like the glasses are similar. It's a really, to me, a good use of
[00:52:32.800 --> 00:52:37.920]   augmented reality. The other use that they showed, which they didn't mention glasses,
[00:52:37.920 --> 00:52:43.920]   but earlier they showed you could use your phone, take a picture of. First, it was a candy
[00:52:43.920 --> 00:52:51.120]   store and the shelf and you could see all of the different chocolates by name, by rating,
[00:52:51.120 --> 00:52:56.960]   and you could pick the best one with the right parameters to show the vision, the recognition,
[00:52:56.960 --> 00:53:02.000]   in kind of real time. And that would be, again, the kind of thing you'd put in glasses.
[00:53:02.000 --> 00:53:06.880]   You're looking around, what's the cheapest blue jeans here and you could see the prices on all of them.
[00:53:07.520 --> 00:53:12.320]   Things like that, but it's all very blue sky. It's all very, well, someday you will,
[00:53:12.320 --> 00:53:22.160]   like the old AT&T commercials. I can continue to go through this. Some statistics, 1.6 billion
[00:53:22.160 --> 00:53:29.360]   buildings mapped and Google maps, 60 million kilometers worth of roads. They're going to be
[00:53:29.360 --> 00:53:33.840]   mapping these buildings in such a way that you'll be able to. They showed London kind of zooming in
[00:53:33.840 --> 00:53:43.280]   and it's kind of like a 3D projection, but generated, not a photo of Westminster Abbey or the House
[00:53:43.280 --> 00:53:49.600]   of Commons. They didn't move around or anything, they just showed it. It was all, but that's their
[00:53:49.600 --> 00:53:54.320]   new immersive view, someday in maps. They also want to do restaurant walkthroughs. They've showed
[00:53:54.320 --> 00:54:02.720]   this a hundred times. Eco-friendly routing, which is in there now, which will say, you could take
[00:54:02.720 --> 00:54:06.800]   it drive there in 10 minutes, but if you were willing to take 15 minutes, you could walk there and
[00:54:06.800 --> 00:54:14.160]   it'd be better for the earth. Nice. YouTube has auto-generated chapters. They're using deep minds to do that.
[00:54:14.160 --> 00:54:20.640]   They've been doing that all along. They have 8 million videos now with auto-generated chapters.
[00:54:20.640 --> 00:54:25.200]   They're going to, they hope to have 80 million by the end of the year. Video transcripts, that's
[00:54:25.200 --> 00:54:30.000]   cool in multiple languages. That's really cool. So right now, you're going to caption a video as
[00:54:30.000 --> 00:54:35.840]   you're watching it, but they're going to add translation to that. So I could be watching a video in Polish
[00:54:35.840 --> 00:54:40.800]   and see it in English. That's kind of neat. They're going to add Ukrainian to that translation
[00:54:40.800 --> 00:54:45.840]   soon. Google Docs will have it. This feels like the changelog.
[00:54:45.840 --> 00:54:53.200]   An automatic TLTR. Leo, it's 2025. It's just a changelog.
[00:54:53.200 --> 00:54:58.720]   Someday this will all happen. I mean, it's just, it's neat. They spent a long time talking about
[00:54:58.720 --> 00:55:03.680]   skin tone equity. But again, this is something when the Pixel 6 came out, they made a lot. They did
[00:55:03.680 --> 00:55:09.200]   ads. They showed this a lot. They talk about the monk scale of 10 skin shades.
[00:55:09.200 --> 00:55:12.560]   But they're now open source. They're going to open source that. Yeah. It's very important.
[00:55:12.560 --> 00:55:16.240]   If you go to skin, actually, they're really good page at skintone.google,
[00:55:16.240 --> 00:55:24.560]   which talks about what they're doing, the scale, the monk scale. He's a sociologist at Harvard who
[00:55:24.560 --> 00:55:31.680]   identified this scale and how Google's going to use this to improve not only their photography,
[00:55:31.680 --> 00:55:37.760]   but also to give you better recommendations in YouTube videos. If you looked up a makeup video
[00:55:37.760 --> 00:55:41.840]   in YouTube, for instance, you could get one that matched for your skin tone rather than,
[00:55:41.840 --> 00:55:47.200]   something completely inapplicable. It's pretty cool.
[00:55:47.200 --> 00:55:50.560]   All recommended practices in skin tone ML research.
[00:55:53.040 --> 00:55:58.560]   Some degree, I feel like this is greenwashing from Google. I wonder how much people buy it.
[00:55:58.560 --> 00:56:03.760]   I mean, don't we know that Google's business is ads?
[00:56:03.760 --> 00:56:11.520]   We know that their business is ads, but at least from my perspective, I saw all of this stuff
[00:56:11.520 --> 00:56:16.480]   announced today is sort of status quo. It's like, well, yeah, I expect you to take your existing
[00:56:16.480 --> 00:56:22.960]   technology and try to improve it. Yeah. Of course. You learn for this. This machine
[00:56:22.960 --> 00:56:28.080]   learning for that. That's all they did is yeah, we're working on our current stuff and trying to
[00:56:28.080 --> 00:56:35.120]   make it better. Thanks. Just said it in about two hours. I don't want to be mean on all this,
[00:56:35.120 --> 00:56:41.200]   but I just feel like Google hasn't done a whole lot. AI test kitchen was
[00:56:41.200 --> 00:56:48.320]   remember last last year at Google I/O where you were talking to the planet Pluto. Remember that?
[00:56:49.440 --> 00:56:52.800]   Have a weird conversation. They get more of that
[00:56:52.800 --> 00:57:02.640]   with Lambda. Lambda 2 is out now. Look at it this way, Leo. Twitter is now on the verge of
[00:57:02.640 --> 00:57:07.520]   becoming the destruction of all free speech. Yeah. Facebook is now a laughing stock as to what it
[00:57:07.520 --> 00:57:12.800]   was. Yeah. Amazon stock is going down and I'll be sure what's going to happen without a pandemic.
[00:57:13.360 --> 00:57:19.680]   Google looks pretty good by comparison. Yeah, maybe tech giants lost more than a trillion dollars
[00:57:19.680 --> 00:57:25.680]   in value over the last three trading days. If you were wondering why your 401k looks so anemic,
[00:57:25.680 --> 00:57:33.680]   maybe that's why Apple has seen its market capitalization trimmed by over 200 billion since
[00:57:33.680 --> 00:57:44.560]   last week, 200 billion. That's one Disney. They lost a whole Disney. Who are the big winners?
[00:57:44.560 --> 00:57:55.440]   Campbell's soup, General Mills, Smuckers. Jesus. Smuckers. It's all sitting our homes eating jam out of a
[00:57:55.440 --> 00:58:05.120]   jar. Clearly. Microsoft lost 189 billion Amazon, 173 billion. Tesla, the big loser, almost, well,
[00:58:05.120 --> 00:58:12.000]   almost as much as Apple, they lost a Disney to 199 billion. Alphabet, a mere 123 billion.
[00:58:12.000 --> 00:58:20.320]   This isn't three days, kids. NVIDIA 85 billion, Metas 70 billion. The only reason Metas only lost
[00:58:20.320 --> 00:58:23.440]   70 billion is they already lost 100 billion. They've heard so far down.
[00:58:24.160 --> 00:58:33.120]   Yeah. The US stock index, the S&P 500, declined 7% since last week. The
[00:58:33.120 --> 00:58:40.480]   investco Nasdaq 100 ETF off by 10%. I mean, I don't know how much the stock market has to do with
[00:58:40.480 --> 00:58:46.000]   the reality of how tech companies are doing and the products they're making. I guess somewhat
[00:58:46.000 --> 00:58:51.600]   has something to do with it. It's just kind of a gloomy time for a big tech.
[00:58:51.600 --> 00:59:01.920]   Well, I wanted to go back to the keynote for a second. You mentioned, I believe it was in the
[00:59:01.920 --> 00:59:08.400]   Twitter news coverage, you mentioned Google playing a bit of catch up to Apple with the stuff that
[00:59:08.400 --> 00:59:13.440]   they announced today. It was all catch up to Apple. It was all showing features on Android that
[00:59:13.440 --> 00:59:20.320]   Apple's done for years. But okay, at one point in time, there was a lot of discussions where
[00:59:20.320 --> 00:59:23.680]   you say the same thing about it. Apple's going the other way. No, no, no. I mentioned that
[00:59:23.680 --> 00:59:27.920]   same thing in Android in our coverage. It's good for these companies to cross-pollinate.
[00:59:27.920 --> 00:59:33.840]   If you've got a good idea, I'm not saying Google's stealing it, but it's hard to trump it.
[00:59:33.840 --> 00:59:41.520]   Oh, look, when I opened the case of my Google pods, the Android phone sees it.
[00:59:42.560 --> 00:59:50.880]   It's a miracle. But that's what I'm saying. How much more can these companies do nowadays?
[00:59:50.880 --> 00:59:52.000]   Maybe that's the problem.
[00:59:52.000 --> 00:59:58.640]   To wow, it's not much that they can do anymore. If they're trying to play "catch up with Apple,"
[00:59:58.640 --> 01:00:02.240]   I don't know if that's a bad thing, considering how much Apple has in the bank.
[01:00:02.240 --> 01:00:07.360]   Well, minus the news you just discussed. Sorry, when you're worth too
[01:00:07.360 --> 01:00:12.240]   long, they can couple of hundred billion here or there. No big deal. No big.
[01:00:12.240 --> 01:00:22.880]   I just wonder, Google, they talk a lot about trust. They had a trust out there in safety and
[01:00:22.880 --> 01:00:30.560]   how we're doing more stuff on device. They made a big thing. I wish I could find that segment where
[01:00:30.560 --> 01:00:36.080]   they say, "We don't sell your information to third parties." We just collected.
[01:00:36.880 --> 01:00:41.680]   We don't sell it to anybody. We got your data. We got it.
[01:00:41.680 --> 01:00:43.280]   I'm not going to let anybody else have it.
[01:00:43.280 --> 01:00:49.120]   I wonder how persuasive that is to everybody. I think most people think of Google as a big
[01:00:49.120 --> 01:00:56.960]   advertising. I mean, obviously, search. But I think people are pretty savvy now to the fact that
[01:00:56.960 --> 01:01:00.400]   when you use Google, you're giving them information about yourself.
[01:01:02.240 --> 01:01:07.600]   And a lot of people will argue about Google collecting so much information on this,
[01:01:07.600 --> 01:01:14.160]   but yet at the same time, they do like the fact that when they open up maps, they know that,
[01:01:14.160 --> 01:01:19.280]   "Hey, your job is 20 minutes away and you've got to be on time today." Even though you didn't
[01:01:19.280 --> 01:01:23.200]   necessarily say, "You're trying to go to work." It just knows that you work it so and so, and it's
[01:01:23.200 --> 01:01:27.840]   a work day of the week. They know that stuff and sometimes it's convenient.
[01:01:27.840 --> 01:01:34.400]   Historically, that's been the problem for Google, right? Remember the Google Now and the smart cars?
[01:01:34.400 --> 01:01:38.960]   Mm-hmm. We were always excited about that, right? But historically, it's been a problem for
[01:01:38.960 --> 01:01:42.640]   them to say, "On the one hand, we know everything you're doing and we're going to offer you your
[01:01:42.640 --> 01:01:46.000]   airplane tickets the minute you're right at the airport and the other hand, but we don't know
[01:01:46.000 --> 01:01:49.200]   everything you're doing and we don't want to know everything you're doing." It's very...
[01:01:49.200 --> 01:01:50.160]   That's right. And that is...
[01:01:50.160 --> 01:01:54.080]   That's why the answer is going to be important. The more they can demonstrate that it's happening
[01:01:54.080 --> 01:01:56.080]   locally and they don't know. On device. Yeah.
[01:01:56.080 --> 01:02:04.080]   Yeah. And also, I mean, honestly, the more really useful it is, in some ways, it becomes less creepy.
[01:02:04.080 --> 01:02:09.760]   The creepy aspect is when it's not quite right. When it's a little off, you're just like, "Ooh,
[01:02:09.760 --> 01:02:15.920]   I don't really like that." When it just flows naturally, I think a lot of the privacy concerns
[01:02:15.920 --> 01:02:21.280]   start to fade away. Depends what you think are creepy. I think there are a lot of people who think,
[01:02:21.280 --> 01:02:25.680]   "Google knows I'm at the airport and is offering me my ticket." They think that's creepy. Even though
[01:02:26.080 --> 01:02:29.040]   you and I know how it happened and think it's a good thing.
[01:02:29.040 --> 01:02:35.040]   I still think your people kind of go. I mean, the other aspect, and I think this is one of the
[01:02:35.040 --> 01:02:39.120]   things that they're working on, is just making a lot of that more transparent to you. I think part
[01:02:39.120 --> 01:02:43.200]   of the creepiness is if you don't know how it happened, how they have the access to that,
[01:02:43.200 --> 01:02:45.120]   and you don't control over it. Yeah.
[01:02:45.120 --> 01:02:49.360]   Right. So if you had more control over it and you saw, "This is the information that Google has.
[01:02:49.360 --> 01:02:53.840]   This is what they're going to do with it." Then I think people... And the ability to say, "Actually,
[01:02:53.840 --> 01:02:58.640]   I don't want you to have that information. I'm going to delete that." I think that would make
[01:02:58.640 --> 01:03:04.320]   it a lot more comfortable. Do you want us to do this? Do you want us and put them in control?
[01:03:04.320 --> 01:03:11.200]   Now, I also appreciate the pop-up today. Every time I logged in, it was mentioning 2FA
[01:03:11.200 --> 01:03:18.240]   and basically trying to shove it down. My throat, "Hey, get 2FA. Get 2FA." They showed that. They
[01:03:18.240 --> 01:03:23.120]   said... What did they... They said the number, but a significant number of people have started to
[01:03:23.120 --> 01:03:33.520]   use 2FA since they forced it on us. 150 million new... 150 million new 2-factor users this year,
[01:03:33.520 --> 01:03:36.880]   because they're requiring it now, which is fine. It's a good thing.
[01:03:36.880 --> 01:03:38.000]   That's great. Yeah.
[01:03:38.000 --> 01:03:45.760]   They did say there is going to be a new button when you see an ad or you can go into your Google
[01:03:45.760 --> 01:03:53.120]   account and see it. The results about me page, which will show you what Google knows about you
[01:03:53.120 --> 01:03:57.280]   and allow you to delete it and can tail it. I think that's good. Yes.
[01:03:57.280 --> 01:04:03.520]   Yeah. I think it's a great idea. It depends on exactly how it's implemented.
[01:04:03.520 --> 01:04:08.640]   When they've attempted to do things like that before, you end up seeing this page with so much
[01:04:08.640 --> 01:04:13.520]   information that it's effectively useless. I went through one of these things like what Google
[01:04:13.520 --> 01:04:18.560]   is using to tell their ads to me once. It showed all of this information about car dealerships in
[01:04:18.560 --> 01:04:25.520]   Texas. I have no idea how or why. You could just scroll on for days and days and not be really
[01:04:25.520 --> 01:04:31.680]   clear on it. If it's providing that information in an interface that is useful and usable and
[01:04:31.680 --> 01:04:34.640]   understandable, then yeah, I actually think that's a great idea.
[01:04:34.640 --> 01:04:39.280]   The other thing it shows is the computers aren't that smart. They don't know that much about you.
[01:04:39.280 --> 01:04:47.760]   They don't do it very well. I don't know when they added this, but they pointed out that now in
[01:04:47.760 --> 01:04:52.640]   search, and this has been here for a while, I guess I just never noticed, there's three dots
[01:04:52.640 --> 01:05:00.160]   next to every search result. You can click that three dots and find out more about the result.
[01:05:00.160 --> 01:05:07.840]   More or less, depending on what the result is. I think that's kind of cool.
[01:05:08.560 --> 01:05:14.640]   Yeah, that's been there for a while. I guess so. I never noticed it, but they...
[01:05:14.640 --> 01:05:16.960]   Facebooked it similarly with trying to show you sources.
[01:05:16.960 --> 01:05:22.160]   The presumption is, this is part of the media literacy stick. If we tell people more about
[01:05:22.160 --> 01:05:26.240]   the sources, they'll understand. I don't know that that's necessarily true, but what the hell
[01:05:26.240 --> 01:05:29.920]   let's try? Well, it just shows you how much you can look at a search result and not see the three
[01:05:29.920 --> 01:05:37.600]   dots because I never knew I could do that. Well, the three dots were there for a while, but they
[01:05:37.600 --> 01:05:41.200]   served a different purpose, I think. They changed that a lot.
[01:05:41.200 --> 01:05:49.200]   You might have had blindness to it from before. I think before, it was just for if you wanted
[01:05:49.200 --> 01:05:56.400]   to see a cached version of the page. Maybe that was it. Somebody saying the extra information was
[01:05:56.400 --> 01:06:03.120]   added a few years ago. It's funny because they never mentioned it before, as far as I could tell.
[01:06:04.080 --> 01:06:07.680]   It still says beta on it, too. Yeah, well, that's Google, isn't it?
[01:06:07.680 --> 01:06:11.360]   It's kind of a Princess spaghetti sauce thing. It's in there.
[01:06:11.360 --> 01:06:14.400]   It's in there. Oh, yeah, we tell people about this. Just anyway, tell you everything.
[01:06:14.400 --> 01:06:20.320]   I used to be very pro Google. I always knew that they were an ad company and that they
[01:06:20.320 --> 01:06:25.120]   had my information, but I always do feel like of all the companies, they respect my
[01:06:25.120 --> 01:06:30.960]   information and they're more secure than pretty much anybody. I don't know, over time, I've gotten
[01:06:30.960 --> 01:06:37.600]   less sanguine about what Google knows and a little more concerned about it. I feel like that
[01:06:37.600 --> 01:06:42.960]   that is, I'm not alone in that becoming more concerned about that. Maybe it's because I'm hanging
[01:06:42.960 --> 01:06:52.800]   around with you too often, Jeff Jarvis. I love you though. I wouldn't have an autographed picture if I didn't.
[01:06:57.120 --> 01:07:03.680]   Ask the show thumbnail, right? We saw Sissy Shao come out and glare at her next Nest Hub Max.
[01:07:03.680 --> 01:07:09.840]   And it knew it was looking and it answered her queries without her saying, "Hey, you know who?"
[01:07:09.840 --> 01:07:17.200]   And everybody goes, "Ooh, that's nice. I could just look and talk," except that already my Nest Hub is
[01:07:17.200 --> 01:07:25.840]   always talking to the TV, to the wall, to the random, just all the time. Is it just me? But all the time,
[01:07:25.840 --> 01:07:30.000]   it's going bloop, and then bloop. Like it's listening for something.
[01:07:30.000 --> 01:07:36.400]   Alright, so Mr. LaPorte, is that a Nest hardware problem or is that a software problem?
[01:07:36.400 --> 01:07:38.720]   I don't know. It's not a hardware problem. I think it's...
[01:07:38.720 --> 01:07:46.400]   Do you not have any Google Assistant voice recognition? No, sir. You don't see. You don't know about it.
[01:07:46.400 --> 01:07:52.640]   No, sir. Jeff, you do. Yeah, I hardly use it. But you hear it going off from time to time, right?
[01:07:52.640 --> 01:07:57.840]   More my phone goes off than the Assistant. Yeah. Mine goes off all the time.
[01:07:57.840 --> 01:08:03.680]   Anyway, now you can look at your Nest Hub Max because it's got a camera. It's a little
[01:08:03.680 --> 01:08:07.680]   disconcerting when I walk in the kitchen, my Nest Hub Max goes, "Hi. Hi, good morning."
[01:08:07.680 --> 01:08:15.360]   This is so lonely all night. Did you sleep well? What was your temperature? It is a fine balance,
[01:08:15.360 --> 01:08:21.840]   really, between these things becoming sentient. You don't want them to be sentient. But at Google,
[01:08:21.840 --> 01:08:27.600]   I understand, at least trying to see where the lines are, right? That they can be useful without
[01:08:27.600 --> 01:08:33.440]   creeping people out. She was leaning over and glaring at it. I think that might be something
[01:08:33.440 --> 01:08:40.000]   you actually have to do, it's like... I'm talking to you, Jeff. I'm talking to you.
[01:08:40.000 --> 01:08:47.520]   I'm sorry. I don't know what to do. All of that ambient computing stuff that misses
[01:08:47.520 --> 01:09:02.880]   Higginbotham used to talk about, right? Yes. G.com/AITestKitchen is the place to go. It is not live for me.
[01:09:02.880 --> 01:09:10.560]   No, but there now. No. I think it's going to be a while before it's life for most people. It kind
[01:09:10.560 --> 01:09:15.200]   of says, "We're going to slowly roll this out." But you can at least get some idea of what the
[01:09:15.200 --> 01:09:22.160]   features will be. I'm trying to go in there. AITestKitchen. Did I spell it right? Let's try it again.
[01:09:22.160 --> 01:09:28.720]   There we go. A place to improve AI technology together. That's not at all dystopian.
[01:09:28.720 --> 01:09:34.800]   AITestKitchen is an app where people can experience and give feedback at some of Google.
[01:09:34.800 --> 01:09:40.480]   So the idea is you go in here and you could talk to Lambda. There are three demos. Imagine it.
[01:09:41.200 --> 01:09:48.160]   You name a place Lambda will offer paths to explore your imagination. This is kind of like having a
[01:09:48.160 --> 01:09:55.600]   conversation with Pluto. Yeah, it's kind of dumb. Yeah. And then there's list it. Name a goal or topic
[01:09:55.600 --> 01:10:01.280]   and see how much Lambda can break it down into multiple lists of subtasks. So the example they
[01:10:01.280 --> 01:10:06.800]   gave on stage is, "I want to start a veggie garden that gives you a few choices of things you should
[01:10:06.800 --> 01:10:19.120]   work on." Wait a minute now. The presenter is doing this straight out of the can demo on the web page
[01:10:19.120 --> 01:10:27.040]   and says, "Oh, I never got that tip before. You mean this tip on the web page that comes up
[01:10:27.040 --> 01:10:32.640]   every single time? You mean that tip?" Okay. Now I feel like he might have been
[01:10:35.120 --> 01:10:42.640]   misrepresenting the case. And then apparently you can talk about dogs. But if you try to
[01:10:42.640 --> 01:10:52.080]   change the subject, it'll change it back to dogs. And you know, honestly, this is not far beyond
[01:10:52.080 --> 01:11:01.680]   Eliza, right? Yes. So I was like, back in time. It's Eliza. Yeah. I got this in my e-max. That's how
[01:11:01.680 --> 01:11:10.160]   old this is. So I don't know. I'm turning into a cynic, I guess. It did say that they could explain
[01:11:10.160 --> 01:11:16.480]   jokes to you, which seems like the worst idea. Yeah. I don't think you turn it into a cynic. I
[01:11:16.480 --> 01:11:23.520]   think it's just you have lived through a lot of pretty cool innovation. It's slowing down. It's
[01:11:23.520 --> 01:11:30.960]   slowing down. So now it's sort of plateaued. Yeah. I'm disappointed because Google I/O
[01:11:30.960 --> 01:11:36.240]   used to be very, very exciting. It used to be like, oh, I can't wait to see what Google's come up with
[01:11:36.240 --> 01:11:39.920]   next. I think some of it is slowing it down. And some of it is we're getting a little more cynical
[01:11:39.920 --> 01:11:45.520]   about the things Google's promising because we know that maybe... No, here's my bigger theory,
[01:11:45.520 --> 01:11:51.360]   which is that we hit a point where the technology does become boring. Yeah. And what matters is not
[01:11:51.360 --> 01:11:55.680]   what the technology does, but what we do with it. And that's the exciting statement.
[01:11:57.200 --> 01:12:01.920]   You know, I've said this this year before. It's part of my book coming up, but it took over 50 years
[01:12:01.920 --> 01:12:07.040]   before Cervantes decided to use this thing to invent the modern novel. That was far more exciting
[01:12:07.040 --> 01:12:11.360]   than the... Well, first you put the type in and then you do this and I was boring about that.
[01:12:11.360 --> 01:12:19.840]   Actually, becoming boring is probably a very good thing. I'm trying to find the results about
[01:12:20.400 --> 01:12:26.160]   you page because I think that's kind of cool. But I don't... Is that not live yet?
[01:12:26.160 --> 01:12:31.920]   I mean, I'm seeing the shots from... What? This is the take control of results about you that we
[01:12:31.920 --> 01:12:37.920]   was talking about earlier. Oh, okay. I haven't yet figured out how to get that to happen
[01:12:37.920 --> 01:12:46.080]   on my phone. Yeah, we just want flying cars. Gumby's saying that in the chat. That's all.
[01:12:46.080 --> 01:12:51.200]   Where's my flying cars? That's all I want. All right, let's take another break and then we will...
[01:12:51.200 --> 01:12:57.520]   If we set everything there is, I mean, there's... You know, Google gave a little elbow through a
[01:12:57.520 --> 01:13:05.520]   little elbow at Apple talking about RCS, their rich communications system and how there is 500
[01:13:05.520 --> 01:13:13.200]   million active users, but still nothing from the other guys, which is obviously Apple.
[01:13:14.560 --> 01:13:17.360]   Apple's never going to support RCS in messages, I don't think.
[01:13:17.360 --> 01:13:25.920]   I complained... Go ahead, Ed. They didn't put a price up for the watch, did they? I don't recall.
[01:13:25.920 --> 01:13:29.280]   Nope, no watch price. Not much about the watch, really.
[01:13:29.280 --> 01:13:34.880]   No more than we learned. Just kept showing it every 15 minutes. The guy who left it in the bar.
[01:13:34.880 --> 01:13:40.880]   Jeff, what did you want to say? So I usually complain about, of course, they don't have a new
[01:13:40.880 --> 01:13:48.160]   Chromebook, which makes me cry and... That's that. But they also didn't talk about Chrome OS at all,
[01:13:48.160 --> 01:13:53.520]   which I found. That was the weirdest thing that they left that out. You mentioned that during the
[01:13:53.520 --> 01:14:00.960]   event or at the end of it. It's like it didn't exist. Now you only have two hours, there's lots of
[01:14:00.960 --> 01:14:08.800]   things to talk about, I guess. It does seem like that and then you mention Fuchsia.
[01:14:08.800 --> 01:14:11.920]   Would you rather talk about Chrome or dogs? I would have taken...
[01:14:11.920 --> 01:14:14.640]   Maybe there wasn't much to say.
[01:14:14.640 --> 01:14:24.160]   Even in the code labs and pathways here on the IO site, there's only one lab for Chrome OS.
[01:14:24.160 --> 01:14:28.000]   Everything else has multiples. If you are a developer,
[01:14:28.000 --> 01:14:36.960]   would you be saying, "Oh gosh, I got to start developing for these Google platforms. I don't
[01:14:36.960 --> 01:14:43.200]   think so. I don't think as much of a... I would assume they're saying they want to develop for mobile,
[01:14:43.200 --> 01:14:48.880]   but not necessarily Android." Yeah, well that means iPhone, right? It means iOS.
[01:14:48.880 --> 01:14:53.840]   Because everything is in the palm of our hands.
[01:14:53.840 --> 01:15:01.360]   All right. Any developers in IRC thoughts?
[01:15:03.360 --> 01:15:08.480]   There used to be the time when we would have people develop just for the web and it would work on
[01:15:08.480 --> 01:15:16.800]   any platform. Oh yeah. Back in the old days. Well they did. Okay, so they didn't mention this.
[01:15:16.800 --> 01:15:22.000]   I probably don't mention it at the developers keynote, but they did have a Google IO pinball game
[01:15:22.000 --> 01:15:29.600]   developed in Flutter, which works... It's actually really cool. Works in your browser.
[01:15:29.600 --> 01:15:34.320]   Do you want to see it? Have you played it at all? This is Flutter,
[01:15:34.320 --> 01:15:40.240]   which means it's cross-platform works in any browser. I'm using it here in Firefox.
[01:15:40.240 --> 01:15:49.040]   Let's select an Android and we will play a little pinball. I'm going to pull the ball back.
[01:15:49.040 --> 01:15:55.520]   There we go. Oh, that was... Oh, that was a... I'm gonna make... This is pretty amazing for a...
[01:15:58.160 --> 01:16:04.080]   Oh, whoa! Fireball! Oh, fire is out.
[01:16:04.080 --> 01:16:11.680]   It's pretty good. It's pretty amazing for a web-based technology.
[01:16:11.680 --> 01:16:16.240]   Well, that was pathetic.
[01:16:16.240 --> 01:16:20.880]   Oh my god. This is...
[01:16:20.880 --> 01:16:22.960]   Blame it on Flutter, not yours. This is so boring.
[01:16:26.080 --> 01:16:31.680]   I like the carpeting. Oh, yeah. The carpeting matches the character.
[01:16:31.680 --> 01:16:39.840]   Oh, I want to get that fireball again. Oh! Oh! Okay, that... Oh, too bad.
[01:16:39.840 --> 01:16:43.600]   All right. In the browser, that's cool. You saw how fast it was.
[01:16:43.600 --> 01:16:45.200]   You go in one of the other games? Could you go in one of the other games?
[01:16:45.200 --> 01:16:50.480]   No, no. It's all just... It's false advertising. You can only play the one game.
[01:16:51.120 --> 01:16:55.680]   UI probably. Yeah, but that's a web development tool that they did not mention.
[01:16:55.680 --> 01:16:59.120]   Maybe they mentioned in the developer keynet.
[01:16:59.120 --> 01:17:05.120]   All right, let me take a little break. We have more to talk about. It's great to have Mike
[01:17:05.120 --> 01:17:08.800]   Masnick here. Let's sign some stuff we can get Mike all head up about.
[01:17:08.800 --> 01:17:12.720]   Oh, not gonna be hard. Something to get him angry about.
[01:17:12.720 --> 01:17:16.800]   Something that's pissing off Mike Masnick coming up.
[01:17:16.800 --> 01:17:21.920]   I got a whole... I just saw a whole new one. The EU is a whole structure about grooming and how
[01:17:21.920 --> 01:17:26.720]   you have to feature all content for grooming. Oh, yeah. That's pretty interesting. Out of your
[01:17:26.720 --> 01:17:33.280]   messages. So if you run a messaging platform, you've got to look for CSAM and grooming messages.
[01:17:33.280 --> 01:17:43.280]   Yeah. Yeah. Oh, Lord. Our show today brought to you by Code Academy. Now, being a developer is cool.
[01:17:44.000 --> 01:17:48.320]   And if you want to change your career, there has never been a better time to become a programmer.
[01:17:48.320 --> 01:17:53.280]   With Code Academy, you can do it on your own terms. Over 50 million people
[01:17:53.280 --> 01:17:58.000]   already know that Code Academy is the best way to learn to code.
[01:17:58.000 --> 01:18:03.760]   Because Code Academy not only teaches you job-reading coding skills, it helps you build unique projects
[01:18:03.760 --> 01:18:08.640]   for your portfolio. That really helps get that first job. You'll learn certificates. You can
[01:18:08.640 --> 01:18:12.880]   post on your LinkedIn or your Facebook page so people know you've got the skills.
[01:18:12.880 --> 01:18:18.480]   It'll even help you prep for technical interviews. Want to work for Google or Apple or Facebook?
[01:18:18.480 --> 01:18:26.160]   This couldn't be a better time. Imagine a new career as a coder. Plus, coding is fun.
[01:18:26.160 --> 01:18:32.640]   With Code Academy, you can learn at your own pace. Get qualified for in-demand jobs like full
[01:18:32.640 --> 01:18:37.280]   stack and engineer, front end engineer. Learn a little dart and flutter. You can write your own
[01:18:37.280 --> 01:18:42.800]   pinball games. You can choose what to learn from building basic websites to artificial intelligence
[01:18:42.800 --> 01:18:48.480]   and everything you could want. The Code Academy Python course is great. Actually, all of their
[01:18:48.480 --> 01:18:52.560]   programming language courses are great because you'll start coding in seconds right away. In your
[01:18:52.560 --> 01:18:58.640]   very first class, it's a hands-on learning environment and you immediately start executing code.
[01:18:58.640 --> 01:19:03.040]   And there is something when you're learning how to code. There is that magic moment. That first
[01:19:03.040 --> 01:19:07.920]   time you get the program to do something. The computer is actually doing something you told
[01:19:07.920 --> 01:19:15.120]   it to do that actually imprints you. I think it's the most important first step that any beginner
[01:19:15.120 --> 01:19:20.720]   goes through. Wow, it's doing what I said to do. With Code Academy, that could happen in your
[01:19:20.720 --> 01:19:27.120]   very first day. It's so much fun. You can learn languages like Python, SQL, JavaScript, HTML, and
[01:19:27.120 --> 01:19:36.160]   CSS. Man, if you know JavaScript and HTML and CSS, the sky's the limit. Everybody's looking for
[01:19:36.160 --> 01:19:42.240]   people to know that. If you're not sure where to begin, Code Academy has a great quiz that will
[01:19:42.240 --> 01:19:46.640]   kind of point you in the right direction. I took it. It said, "You should do our computer science
[01:19:46.640 --> 01:19:52.240]   track because you kind of..." It's not a coding quiz. They ask you what your interests are and
[01:19:52.240 --> 01:19:56.880]   what you're good at and stuff like that. They said, "You kind of have the mind that would like to
[01:19:56.880 --> 01:20:02.320]   think about how problems are solved." It's absolutely true. Lisa, my wife took it. She's our CFO or CEO.
[01:20:02.320 --> 01:20:06.800]   It said, "You should learn statistics and study R because you definitely have a mind for numbers.
[01:20:06.800 --> 01:20:12.240]   Absolutely right." Take the quiz. You'll get tailored career advice and course recommendations
[01:20:12.240 --> 01:20:19.040]   based on your strengths and your interests. As you're doing these courses, you're going to get
[01:20:19.040 --> 01:20:24.240]   instant feedback. It's a totally interactive platform you learn by doing. I think it's really
[01:20:24.240 --> 01:20:28.560]   important. You'll be building your portfolio. You're going to get a certificate of completion,
[01:20:28.560 --> 01:20:33.760]   which helps you market yourself to future employers. You can get a dream job in web development or
[01:20:33.760 --> 01:20:38.640]   programming, computer science or data science or a whole lot more. If you want to learn a new
[01:20:38.640 --> 01:20:43.760]   skill to build websites to troubleshoot tech issues, if you want to transition into a new career,
[01:20:43.760 --> 01:20:52.560]   you need Code Academy. Join over 50 million people learning to code with Code Academy and see where
[01:20:52.560 --> 01:20:58.240]   coding can take you. 15% off the Code Academy Pro Membership when you go to codecademy.com
[01:20:58.240 --> 01:21:03.280]   and use a promo code twig. You can also learn to code for free. But when you decide you want
[01:21:03.280 --> 01:21:12.560]   that Pro Membership, use the promo code TWIG at codecademy.com for 15% off Code Academy Pro.
[01:21:12.560 --> 01:21:21.440]   The best way to learn to code. C-O-D-E-C-A-D-E-M-Y codecademy.com. The promo code is twig.
[01:21:22.240 --> 01:21:28.800]   Thank you, Code Academy for supporting this week in Google. I thought this whole show
[01:21:28.800 --> 01:21:33.760]   be devoted to Google I/O. I really did. Maybe that's my... See, you didn't look at the rest of
[01:21:33.760 --> 01:21:39.280]   the rundown is what you're saying. Yeah. Oh, there's more. Oh my god, there's page after page.
[01:21:39.280 --> 01:21:47.520]   A whole lot more. Let's talk about this new EU. What is your sense, Mike, of the direction
[01:21:47.520 --> 01:21:55.200]   that EU is taking? Is this a threat to big tech in the US? I mean, some things like GDPR have been,
[01:21:55.200 --> 01:22:02.320]   I think, beneficial, yes. No, I don't think I would say that, actually. Really? In fact,
[01:22:02.320 --> 01:22:08.960]   I'll have an article up. I think tomorrow I have to put the finishing touches on it about how GDPR is
[01:22:08.960 --> 01:22:15.280]   now actually being abused, often by Russian oligarchs, in order to silence reporters. Oh my god.
[01:22:15.280 --> 01:22:20.640]   They're now claiming that if a reporter is collecting information on a Russian oligarch
[01:22:20.640 --> 01:22:25.840]   for the purpose of reporting, that if they claim that the information is inaccurate,
[01:22:25.840 --> 01:22:30.480]   it's the same thing as if Google had inaccurate information about you and therefore they can sue
[01:22:30.480 --> 01:22:34.000]   you. Oh my god. So there have been a number of these cases. It's like,
[01:22:34.000 --> 01:22:40.480]   libel tourism now becomes GDPR tourism. It is worse than the libel tourism part.
[01:22:41.040 --> 01:22:46.000]   So I think there are some really, really serious concerns with it. And I think the other thing too
[01:22:46.000 --> 01:22:54.960]   is, I think that, and I would argue this is true of most of the European regulations that have
[01:22:54.960 --> 01:23:01.040]   been put in place and that will be put in place, is that unlike the US, that a lot of the politicians
[01:23:01.040 --> 01:23:06.640]   there are taking, I'm going to choose my words carefully here, a more thoughtful approach,
[01:23:07.360 --> 01:23:11.680]   but the end result, I think, is really, really problematic. And with the GDPR,
[01:23:11.680 --> 01:23:18.000]   I think we're seeing that very clearly. That's the one that's now been in effect for a few years now,
[01:23:18.000 --> 01:23:24.240]   where it's actually, it hasn't hurt Google or Facebook. It's really helped them, but it's
[01:23:24.240 --> 01:23:31.840]   destroyed a lot of the smaller competition. And there have been a few different research reports
[01:23:31.840 --> 01:23:36.400]   that have all come out within the last few months. There was an NBER report that came out a few
[01:23:36.400 --> 01:23:39.600]   months ago that just got a bunch of attention this week, even though it actually came out a few
[01:23:39.600 --> 01:23:46.400]   months ago, that showed that the number of apps that were available dropped tremendously,
[01:23:46.400 --> 01:23:55.040]   they had a way of calculating sort of innovation. And they basically said innovation declined,
[01:23:55.040 --> 01:24:01.920]   and also that the main beneficiary of the GDPR was Google. So basically, a bunch of other
[01:24:01.920 --> 01:24:07.120]   smaller companies had a lot of problems, and Google has helped. There's another study that
[01:24:07.120 --> 01:24:11.440]   effectively said the same thing, but we're looking at sales and profits, and basically said,
[01:24:11.440 --> 01:24:15.200]   for everybody else, sales and profits went down for Google and Facebook, they went up.
[01:24:15.200 --> 01:24:24.320]   So you can argue about, and I think to some extent, the European approach, even as they talked about
[01:24:24.320 --> 01:24:31.120]   sort of like raining in big tech, they in some ways prefer that, because if there's just Google
[01:24:31.120 --> 01:24:36.720]   and Facebook to regulate and deal with, that's easier than having to regulate 100 or 1000 different
[01:24:36.720 --> 01:24:42.240]   companies doing 100 or 1000 different things. And so they might prefer this sort of, oh, well,
[01:24:42.240 --> 01:24:49.520]   we just have to send in regulators to yell at and find regularly find Google and Facebook,
[01:24:49.520 --> 01:24:53.600]   rather than having to deal with the small competitors. But from an innovation standpoint,
[01:24:53.600 --> 01:24:59.120]   as we just discussed, it kind of feels like Google is sort of reaching that stage of life where
[01:24:59.120 --> 01:25:03.600]   they're not so much the innovators coming out with the next big thing. And if we're killing off
[01:25:03.600 --> 01:25:08.320]   all of the new startups, that might be more innovative. I don't think that's very good. And so,
[01:25:08.320 --> 01:25:14.000]   so that's that's my big concern about, you know, I don't think there is the laws, I don't think
[01:25:14.000 --> 01:25:19.280]   are as fundamentally ridiculous as like, you know, Texas is social media law or whatever.
[01:25:19.280 --> 01:25:24.320]   But I don't think that they've been successful. And I think they've created a lot of problems.
[01:25:24.320 --> 01:25:29.440]   And I think that they'll be, you know, it'll take us a long time to sort of walk through and realize
[01:25:29.440 --> 01:25:32.400]   just how much innovation they've actually really stifled.
[01:25:32.400 --> 01:25:34.880]   I'm only it's a little disappointed to me because I do feel like
[01:25:34.880 --> 01:25:42.480]   what we don't want is the is big companies, regular self regulating because they don't.
[01:25:42.480 --> 01:25:48.960]   Sure. They're going to opt optimize for profit. And you know, that's their job. So it's up to
[01:25:48.960 --> 01:25:55.360]   government regulators or, you know, ineffective proxy for society to keep these guys in line.
[01:25:55.360 --> 01:26:02.640]   So I'm very disappointed if this I think well intentioned and somewhat thoughtful effort
[01:26:02.640 --> 01:26:08.640]   has had such unfortunate dire unintended consequences is a little disappointing.
[01:26:08.640 --> 01:26:13.200]   It's not all well-entended though. Well, it's much of it.
[01:26:13.200 --> 01:26:15.040]   It's amazing. Yeah, I think much of it is.
[01:26:15.040 --> 01:26:20.480]   Yeah, I think I think it's a mix of you have people who are certainly well-entended but maybe
[01:26:20.480 --> 01:26:25.280]   not very well-informed. And then you have people who are not so well-entended. And, you know,
[01:26:25.280 --> 01:26:30.720]   part of it is to me, politics, right? That's all laws if you really paid attention.
[01:26:30.720 --> 01:26:35.760]   Yeah, but there's a question of how much of these laws are really driven out of like trying to
[01:26:35.760 --> 01:26:40.320]   make the world a better place and to do the things that you talked about, like not allow
[01:26:40.320 --> 01:26:45.280]   companies to be completely autonomous in terms of what they're doing and how much
[01:26:45.280 --> 01:26:51.360]   of it is driven by spite. And I think that a fair amount of the regulatory focus is about spite.
[01:26:51.360 --> 01:26:55.120]   We don't like these companies. We don't like what they're doing. Therefore, we must punish them.
[01:26:55.120 --> 01:26:57.040]   Well, that's not what's the main competitor system.
[01:26:57.040 --> 01:27:02.400]   A lot of is prompted by competitors lodging complaints against these big companies, right?
[01:27:02.400 --> 01:27:06.480]   That's what stimulates a lot of these investigations and ultimately the laws.
[01:27:06.480 --> 01:27:12.960]   Well, what's the solution then? If we can't expect a legislative solution and we can't
[01:27:12.960 --> 01:27:16.080]   expect companies to self-regulate, it sounds like we're out of luck.
[01:27:16.080 --> 01:27:21.600]   Well, I don't think so, actually. I mean, I think there are a number of things that we can do.
[01:27:21.600 --> 01:27:26.480]   And I'm sort of halfway through writing a paper listing all of them out. So I'm not going to go
[01:27:26.480 --> 01:27:33.680]   through my entire paper right now. But like, you know, I think that the reality is having more
[01:27:33.680 --> 01:27:40.240]   competition, having smaller competitors that can come up and say like, you don't trust Google,
[01:27:40.240 --> 01:27:44.960]   you don't trust Facebook, you don't trust these companies, like here are alternatives and allow
[01:27:44.960 --> 01:27:49.440]   there to be like real competition that drives the innovation. So instead of telling companies how
[01:27:49.440 --> 01:27:58.080]   to run their business, instead prevent companies from using acquisitions to shut down competitors,
[01:27:58.080 --> 01:28:04.320]   prevent companies from merging to form behemoths, encourage competition.
[01:28:04.320 --> 01:28:08.000]   Transparent with your data about your impact, allow researchers to be able to see that. I think
[01:28:08.000 --> 01:28:14.080]   that's the deafening color. And that they personally just testified before. I think it was the Senate,
[01:28:14.080 --> 01:28:20.720]   Mike, I think. Yeah. I think there's a movement toward transparency to research is the most sane
[01:28:20.720 --> 01:28:27.920]   next step that I've I to endorse. But it's actually tricky. And in fact, like the bills that are
[01:28:27.920 --> 01:28:32.960]   on tap right now, that it was not technically testifying about those bills, but but it really
[01:28:32.960 --> 01:28:36.480]   was because those are the only bills that are being considered right now. I think also do have
[01:28:36.480 --> 01:28:40.640]   a lot of problematic language. Right. I mean, and this is what what it gets to is like a lot of
[01:28:40.640 --> 01:28:46.320]   this stuff is really, really nuanced and really, really specific. And and poor language, even in
[01:28:46.320 --> 01:28:51.840]   well meaning thoughtful bills can still be really problematic. And that's what we have a lot of.
[01:28:51.840 --> 01:28:56.000]   And you know, part of that is like, it is really difficult to understand all of these different
[01:28:56.000 --> 01:29:00.400]   moving parts and how they how they play together. But also, you know, part of is that these things
[01:29:00.400 --> 01:29:04.880]   all interact. And you know, one of the biggest things that I sort of got at this in terms of,
[01:29:04.880 --> 01:29:08.640]   you know, what was happening with the GDPR is, you know, if you're talking about speech, if you're
[01:29:08.640 --> 01:29:12.400]   talking about privacy, if you're talking about transparency, and then you're also talking about
[01:29:12.400 --> 01:29:17.600]   competition, those things are often not not very well aligned, right? You know, you can put all
[01:29:17.600 --> 01:29:22.240]   these rules onto social media platforms. But again, like the biggest companies are going to be able
[01:29:22.240 --> 01:29:27.440]   to handle that. And therefore, you're you're decreasing competition as you're apparently like, you
[01:29:27.440 --> 01:29:32.480]   know, putting in place all these rules to deal with these things. And so, you know, looking at all
[01:29:32.480 --> 01:29:36.640]   of that combined and figuring out like, how do we do this in a way that actually does increase
[01:29:36.640 --> 01:29:42.160]   competition and increase innovation without letting companies just, you know, run wild with all of
[01:29:42.160 --> 01:29:48.720]   our data. And it's a really, it's a really difficult needle to thread because, you know,
[01:29:48.720 --> 01:29:53.920]   those things work at cross purposes with each other, especially if you're not careful. And I don't see
[01:29:53.920 --> 01:29:58.960]   anyone who's being really careful about it. It's possible the mistake GDPR made was just being too
[01:29:58.960 --> 01:30:03.760]   sweeping. Maybe it would be better to do this a little more piecemeal, a little more thoughtfully,
[01:30:03.760 --> 01:30:11.040]   also in a way that it could be refined, improved, or turned off if it doesn't, if it doesn't produce
[01:30:11.040 --> 01:30:15.680]   the results you expect. Well, that's always the issue with any law, right? I mean, it's sort of
[01:30:15.680 --> 01:30:19.840]   the opposite of way like innovation works, right? Innovation is this process of iteration. Try
[01:30:19.840 --> 01:30:24.560]   something, you know, get it out there, see what happens and be ready to continually iterate.
[01:30:24.560 --> 01:30:28.800]   Laws are never like that, right? Laws are like, we have this problem, here's the solution,
[01:30:28.800 --> 01:30:33.680]   we're going to pass the bill and we wipe our hands of it. And like, you know, maybe 25 years
[01:30:33.680 --> 01:30:39.440]   later, we'll look back and be like, boy, that was a disaster. You know, nobody wants to do that.
[01:30:39.440 --> 01:30:43.520]   You know, I've argued for a long time. I think it would be great if we had this sort of iterative
[01:30:43.520 --> 01:30:48.000]   approach to lawmaking. But you know, that also creates problems, right? Because like, you know,
[01:30:48.000 --> 01:30:52.400]   companies and people are relying on the laws and if they're constantly changing, that's not great
[01:30:52.400 --> 01:30:57.600]   either. So, you know, I don't know that there's a perfect solution here. But you know, I do think
[01:30:57.600 --> 01:31:02.320]   that there should be a little bit more humility in recognizing that, you know, some of these laws
[01:31:02.320 --> 01:31:07.760]   have have really serious impact and we should be willing to look at that. Where's your paper going
[01:31:07.760 --> 01:31:17.440]   to be, Mike, when you finish it? I'm not sure. I'm working on the paper. It's part of a grant
[01:31:17.440 --> 01:31:24.160]   that we got from Knight Foundation. And so it's almost done and we'll release it somehow somewhere.
[01:31:24.160 --> 01:31:33.680]   Can't wait to see it. It's very depressing. So then there's this new regulation again proposed.
[01:31:33.680 --> 01:31:39.120]   I think part of the problem is the EU has such a Byzantine method of proposing,
[01:31:39.120 --> 01:31:44.560]   approving, and then enacting legislation that it's kind of hard to follow it.
[01:31:44.560 --> 01:31:50.720]   It has to be, as I understand it, once it's proposed, then it has to be approved. Then it has to be
[01:31:50.720 --> 01:31:57.920]   approved individual nations. Is that right? They have to enact it. Inactively. So, this is at
[01:31:57.920 --> 01:32:05.040]   the status of being proposed. But it would require chat apps like WhatsApp and Facebook Messenger
[01:32:05.040 --> 01:32:12.320]   to selectively scan users' private messages for child sexual abuse material and grooming
[01:32:12.320 --> 01:32:16.720]   behavior. This is what we were talking about earlier. Similar. Get it ready. Get it ready,
[01:32:16.720 --> 01:32:25.120]   hand it. Yeah. Privacy expert. Listen to what Matthew Green says and we really trust Matthew Green.
[01:32:26.000 --> 01:32:29.840]   This document is the most terrifying thing I've ever seen.
[01:32:29.840 --> 01:32:36.640]   It describes the most sophisticated mass surveillance machinery ever deployed outside of China and
[01:32:36.640 --> 01:32:45.200]   the USSR. Not an exaggeration. Did you let it because he's your. But it's all. But who can be for
[01:32:45.200 --> 01:32:51.440]   child pornography? Right. So anything can fit under that then. And you have the worst of both
[01:32:51.440 --> 01:32:55.200]   worlds. Or you can say anything you want. Oh, some child pornography. So we're going to watch
[01:32:55.200 --> 01:32:59.200]   everything you say so we can stop you from saying the things we don't want you to say and go get
[01:32:59.200 --> 01:33:06.640]   you. It's it's it's it's part of me for a Gutenberg moment here. But every time there's a new mechanism
[01:33:06.640 --> 01:33:11.760]   that allows more people to speak than were heard before. The old incumbent institutions we act
[01:33:11.760 --> 01:33:16.320]   against to try to control, try to have panic and fear. Partly out of their self interest,
[01:33:16.320 --> 01:33:22.400]   partly just out of moral panic. And and eventually we figure it out. But in the meantime, there's
[01:33:22.400 --> 01:33:25.760]   going to be a lot of destruction of a lot of the kind of good innovation Mike was just talking about.
[01:33:25.760 --> 01:33:31.680]   Si. Si.
[01:33:31.680 --> 01:33:38.560]   The regulation established a number of new obligations for online service providers,
[01:33:38.560 --> 01:33:43.520]   app stores, hosting companies, any provider of interpersonal communications.
[01:33:47.120 --> 01:33:53.200]   Oh, this yeah. But again, this is a proposed law. These things seem to sneak in after the
[01:33:53.200 --> 01:33:59.520]   proposals. Is there a comment period? Is there any way people can weigh in?
[01:33:59.520 --> 01:34:05.920]   Well, part of the issue with this is that there was some commenting beforehand in which a lot of
[01:34:05.920 --> 01:34:09.840]   people, a lot of very smart people explained to the people pushing this bill that it was,
[01:34:09.840 --> 01:34:16.480]   you know, literally impossible to do what they wanted. And they just didn't care. You know,
[01:34:16.480 --> 01:34:21.280]   it's it's it's similar in a lot of ways though. In this case, worse, I said earlier that the
[01:34:21.280 --> 01:34:25.600]   European approach tended to be more thoughtful than the US approach. In this case, I would say
[01:34:25.600 --> 01:34:31.280]   not so much, but this is very similar to the Earned Act in the US. But I think much, much worse in
[01:34:31.280 --> 01:34:38.000]   terms of, you know, how much of a wrecking ball it's taking to to encryption and private communications,
[01:34:38.000 --> 01:34:43.600]   you know, how you know, this will go through quite a long process before it ever gets anywhere
[01:34:43.600 --> 01:34:49.120]   if it gets anywhere. And so I would imagine that it would improve, but it is starting from such a
[01:34:49.120 --> 01:34:53.440]   bad starting point that I don't see how you could do anything along of this nature,
[01:34:53.440 --> 01:34:58.960]   and not have it be absolutely terrible and and extremely dangerous.
[01:34:58.960 --> 01:35:08.800]   Oh, well, so what should we do? I mean, we don't even have any, we don't we're not even
[01:35:08.800 --> 01:35:14.240]   their constituency. I mean, we can't we just sit back and watch, I guess. I mean, speaking up does
[01:35:14.240 --> 01:35:19.760]   help, right? And and you know, people and so that more people are aware, whether or not, you know,
[01:35:19.760 --> 01:35:26.400]   we are say matters directly to European politicians, perhaps not, though I have in the past testified
[01:35:26.400 --> 01:35:32.480]   before the European Parliament, but like, you know, getting people who are in the EU to speak up and
[01:35:32.480 --> 01:35:38.640]   say this is this would be a disaster. And we cannot have this happen. And and honestly, you know,
[01:35:38.640 --> 01:35:44.080]   and this was true of the Earned Act in the US and and others where, you know, for all the talk of
[01:35:44.080 --> 01:35:48.640]   how this is necessary for the children and you have all of the moral panic associated with that,
[01:35:48.640 --> 01:35:53.920]   the reality is that this would make most children much less safe, right? If they cannot have secure
[01:35:53.920 --> 01:35:58.320]   communications, if they cannot have secure places where they can feel comfortable talking without
[01:35:58.320 --> 01:36:02.800]   feeling like they're being surveilled at every moment, that will do a lot, you know, a lot more
[01:36:02.800 --> 01:36:09.360]   harm to most children than than, you know, than this bill suggests they need to protect children.
[01:36:09.360 --> 01:36:16.320]   So I think, you know, people need to understand that how much damage this kind of thing actually
[01:36:16.320 --> 01:36:23.440]   does to children rather than protect them. Yeah. Meanwhile, I'm looking at Stacey's article on
[01:36:23.440 --> 01:36:28.640]   Stacey on IOT. We'll ask her about it. Of course, when she comes back, I think in two weeks,
[01:36:30.320 --> 01:36:35.920]   we need laws and regulations on data use and privacy. She's talking, of course, about
[01:36:35.920 --> 01:36:44.640]   the story that we saw that a company Safegraft was selling information about visits to planned
[01:36:44.640 --> 01:36:49.920]   parenthood, a vice bought, you know, a week's worth of information about who had visited planned
[01:36:49.920 --> 01:36:54.960]   parenthood for 160 bucks. Of course, Safegraft said, Oh, that was a mistake. We won't do that
[01:36:54.960 --> 01:37:00.240]   ever again. Yeah, yeah, yeah. They actually, they didn't say it was a mistake. They got caught.
[01:37:00.240 --> 01:37:03.920]   They did say they didn't say they wouldn't do it again, but then they actually defended it,
[01:37:03.920 --> 01:37:08.720]   you know, in the process, like it was really obnoxious. If you find the blog post that Safegraft
[01:37:08.720 --> 01:37:13.120]   put up, you know, they basically said, you know, this is actually good. This is useful data
[01:37:13.120 --> 01:37:18.320]   for researchers and, and, you know, sure, anyone might abuse it, but you know, this had really good
[01:37:18.320 --> 01:37:22.400]   purposes, but because people are so mad about it, we're not gonna do that. But there's plenty
[01:37:22.400 --> 01:37:25.440]   other data brokers who I'm sure would be glad to pick up the slack.
[01:37:25.440 --> 01:37:31.680]   But the problem always is it's the, it's the use over the gathering, but not the having knowledge
[01:37:31.680 --> 01:37:35.360]   is one matter how you use it is the matter. And the problem is it's now in the hands of
[01:37:35.360 --> 01:37:39.760]   noxious, authoritarian people who want to use it against women.
[01:37:39.760 --> 01:37:44.560]   Yeah. She also cites the fact that the CDC spent nearly half a million dollars
[01:37:44.560 --> 01:37:50.160]   buying location data on millions of Americans from another data broker to see if people were
[01:37:50.160 --> 01:37:55.920]   following COVID curfews, tracking neighbor to neighbor visits, stuff like that. Again,
[01:37:55.920 --> 01:38:02.080]   you could see the benefit, but it's also the fact that this information is available and so easy to
[01:38:02.080 --> 01:38:11.360]   acquire is very disappointing. But again, calling for laws and regulations on data use and privacy
[01:38:11.360 --> 01:38:19.120]   makes sense until you see what a mess has been made by the EU and the US in attempting to regulate
[01:38:19.760 --> 01:38:28.480]   tech. It's not just that. This is a lot more complex than people make it out to be. And again,
[01:38:28.480 --> 01:38:33.040]   like people get so focused on one aspect of it. And we're going through that right now in
[01:38:33.040 --> 01:38:38.960]   California, where we have this law, the CCPA, and there was just a hearing about it. And
[01:38:38.960 --> 01:38:43.840]   my colleague, Kathy Gellis, who you've had on your show recently, a few times,
[01:38:43.840 --> 01:38:51.280]   she said to say hello. She just testified last week before the new California privacy
[01:38:51.280 --> 01:38:56.800]   regulatory board. I forget there's an acronym for it as they're trying to figure out what to do.
[01:38:56.800 --> 01:39:02.080]   And the point that she made was like, you can't look at privacy in a vacuum. You have to realize
[01:39:02.080 --> 01:39:08.160]   that it impacts other things, including free speech. And the example I gave with the GDPR and the
[01:39:08.160 --> 01:39:15.440]   Russian oligarchs, silencing reporters by claiming that it's a privacy issue, that they're collecting
[01:39:15.440 --> 01:39:21.440]   data for reporting purposes. That's a problem. And we're pretty concerned about some of the plans
[01:39:21.440 --> 01:39:26.160]   that California has put forth, that it would impact speech at the same time. And so how do you
[01:39:26.160 --> 01:39:31.600]   balance those two competing interests? How do you balance this idea that, and I shouldn't even say
[01:39:31.600 --> 01:39:34.960]   balance because you're not supposed to balance free speech, you're not supposed to balance the
[01:39:34.960 --> 01:39:39.280]   First Amendment. But how do you look at those issues and say, how can we protect our privacy,
[01:39:39.280 --> 01:39:43.600]   but also not use privacy law to silence speech at the same time?
[01:39:43.600 --> 01:39:47.600]   Is it problematic that the EU does not affect? We're the only country, one of the few countries
[01:39:47.600 --> 01:39:53.360]   anyway that has a First Amendment. You can't claim First Amendment protections in the EU.
[01:39:53.360 --> 01:40:02.160]   They do have freedom of expression. The human rights law within the EU is supposed to protect
[01:40:02.160 --> 01:40:04.960]   freedom of expression. It is a lot more than balanced.
[01:40:04.960 --> 01:40:06.480]   Balance in other factors though.
[01:40:06.480 --> 01:40:12.880]   And it does have, there are balancing tests. The EU human rights approach involves balancing
[01:40:12.880 --> 01:40:20.480]   different rights. And that leads to some bizarre things. There was also just recently,
[01:40:20.480 --> 01:40:26.640]   speaking of bad European laws, a few years ago, the EU passed this copyright directive,
[01:40:26.640 --> 01:40:32.880]   which among other things had what is now Article 17, which is for copyright filters.
[01:40:32.880 --> 01:40:37.600]   And basically telling all of the countries in the EU that they need to pass laws requiring
[01:40:37.600 --> 01:40:43.040]   anyone who is hosting copyright covered material, which is basically anyone, that they have to put
[01:40:43.040 --> 01:40:48.560]   in place some sort of filtering tool to make sure that it's not uploading copyrighted stuff.
[01:40:48.560 --> 01:40:52.400]   The law at the same time also said, but you have to make sure that it's not blocking stuff
[01:40:52.400 --> 01:40:59.200]   that shouldn't be taken down. And so Poland went to the European Court of Justice to say,
[01:40:59.200 --> 01:41:03.440]   this interferes with human rights. And so the Court of Justice came out with this ruling two
[01:41:03.440 --> 01:41:09.440]   weeks ago that is completely useless. It basically says, instead of saying, "Oh, the copyright
[01:41:09.440 --> 01:41:17.840]   directive violates human rights," it says, as long as the filters do not take down speech that
[01:41:17.840 --> 01:41:20.720]   shouldn't be taken down, then it's okay, which is useless.
[01:41:20.720 --> 01:41:27.040]   Because any filter is going to misdiagnose things and is going to take down some speech that shouldn't
[01:41:27.040 --> 01:41:31.040]   be taken down. And yet, so now everybody is struggling with this. All the different
[01:41:31.040 --> 01:41:34.560]   countries are trying to implement it. Some of them don't care about legal speech. I mean,
[01:41:34.560 --> 01:41:39.840]   it seems like France in particular is just like, "Ah, we have to stop any kind of infringement.
[01:41:39.840 --> 01:41:45.520]   If we happen to take down memes and parody, whatever, we don't care about that." But other
[01:41:45.520 --> 01:41:49.200]   countries are like, "Well, we're trying to balance these two interests." And the Court is now saying,
[01:41:49.200 --> 01:41:53.920]   like, you have to get that right. If you get it wrong and you're taking down protected speech,
[01:41:53.920 --> 01:42:04.080]   then it violates the human rights charter or whatever is the basis. And so understanding that
[01:42:04.080 --> 01:42:10.160]   these rights conflict and how do you manage that is really difficult. And it feels like nobody
[01:42:10.160 --> 01:42:15.760]   wants to come in and look at that holistically and recognize how these different things intersect
[01:42:15.760 --> 01:42:21.520]   with each other. Instead, they just push a very narrow focus. I agree in general, privacy is really
[01:42:21.520 --> 01:42:25.840]   important. And we have this free-for-all right now. And I think that's bad. And I think that's
[01:42:25.840 --> 01:42:32.320]   dangerous. But I worry about every privacy law that everybody tries to pass because I'm not sure
[01:42:32.320 --> 01:42:36.320]   it helps. It's certainly not going to help if all we're doing is having to click through
[01:42:36.320 --> 01:42:43.600]   a cookie banner to say, "Okay, I agree." That doesn't solve anything. So it would be really nice if we
[01:42:43.600 --> 01:42:47.200]   could just take a step back and actually look at all of this stuff holistically and say,
[01:42:47.200 --> 01:42:51.600]   "What kind of world do we actually want? And why are we not there and how can we get there?" But that
[01:42:51.600 --> 01:42:57.440]   doesn't seem like what anyone is doing. Leo, I put in the chat, I put a paper in that I quote,
[01:42:57.440 --> 01:43:04.160]   "I have it." In the book I'm writing. Yes. By James Durer from 1998. The Information Age and
[01:43:04.160 --> 01:43:10.320]   the Printing Press. Right. So yes, it's one of the most interesting. Yes. But this was 1998.
[01:43:10.320 --> 01:43:16.320]   And what he argues is that countries that tried to control the printing press were farther behind
[01:43:16.320 --> 01:43:21.360]   just in their general development than those that did not. That it's early, my argument.
[01:43:21.360 --> 01:43:26.080]   And that also that it's going to be filled with unintended consequences. So what he argues is the
[01:43:26.080 --> 01:43:30.320]   sooner you get to those unintended consequences and then start figuring them out, the better. So he
[01:43:30.320 --> 01:43:34.720]   says, this is Rand, but he says, "Pull back from regulation so you can get to them." And then you
[01:43:34.720 --> 01:43:38.560]   deal with actual impact rather than... But how do you know what the unintended consequences are
[01:43:38.560 --> 01:43:43.920]   until you try legislating? No, no, no. It's the unintended consequences of the net.
[01:43:43.920 --> 01:43:47.440]   Oh, of the net. Don't legislate. Let us go figure out what works for the...
[01:43:47.440 --> 01:43:52.800]   Here's the problem with that. At some point these companies get so big that it's impossible to do
[01:43:52.800 --> 01:43:57.600]   anything about them. Isn't that the risk? And we may already be there. For your friends,
[01:43:57.600 --> 01:44:01.760]   or it's just a monster, don't you think? No, but let's talk about Facebook and Google. You think,
[01:44:01.760 --> 01:44:05.200]   I mean, honestly... Facebook's on the way down. Okay. What are you talking about?
[01:44:05.200 --> 01:44:11.040]   Who's gonna buy a guy who's gonna ruin it? Okay. So they're just gonna burn out and...
[01:44:11.040 --> 01:44:15.280]   Yeah. We won't have to worry about that anymore. That something else comes along.
[01:44:15.280 --> 01:44:21.200]   That's something else. Okay. I mean, that is the history of the technology world, right? I mean,
[01:44:21.200 --> 01:44:25.360]   that's why I keep saying the most important thing is clearing the way for new innovators
[01:44:25.360 --> 01:44:30.400]   and making sure that we can actually have new entrants that will be innovative and that can
[01:44:30.400 --> 01:44:35.040]   build a better service that people are feeling more and more uncomfortable with
[01:44:35.040 --> 01:44:38.720]   these large... But there is a network effect that makes it hard
[01:44:38.720 --> 01:44:44.400]   regardless of legislation. Regulation makes that worse in every case. Okay. So what was Mike's
[01:44:44.400 --> 01:44:49.120]   point before? I understand this is regulatory capture. We've talked about this before.
[01:44:49.120 --> 01:44:52.240]   These companies want to pull up the ladder after them,
[01:44:52.240 --> 01:44:57.840]   selves, but... Even at first they don't want the regulation, but then when they realize it's
[01:44:57.840 --> 01:45:02.000]   inevitable, then they say, "We can take it." I'll win. Yeah. We can take it. Okay. Well,
[01:45:02.000 --> 01:45:04.000]   that's what I'm saying. They've already gotten to that point.
[01:45:04.000 --> 01:45:11.120]   I mean... Do you have Facebook advertising, please regulate us. But the other thing is more
[01:45:11.120 --> 01:45:19.200]   insidious than this. And I heard what's his name, the founder, the planeter, CEO. And now you're
[01:45:19.200 --> 01:45:23.520]   here in Elon Musk say the same thing. Part of this shtick about if it's legal, we allow it,
[01:45:23.520 --> 01:45:28.000]   is saying, "We'll make no judgments. We're not going to go to anything. You tell us what to do.
[01:45:28.000 --> 01:45:31.520]   We'll do that. We're taking no responsibility for anything." Right. No
[01:45:31.520 --> 01:45:39.440]   responsibility. That's insidious. You can't sue us. Yeah, you can't blame us. So, go make a law.
[01:45:39.440 --> 01:45:44.160]   It's fine. I mean, that was what Mark Zuckerberg had said as well. Yes. You know,
[01:45:44.160 --> 01:45:48.320]   when he was getting called to Congress every two months, you know, that's at one point he came out
[01:45:48.320 --> 01:45:54.240]   with like, you know, a statement that was effectively like, "You tell us." Like, "Basically,
[01:45:54.240 --> 01:45:57.600]   I'm throwing up my hands. You don't want me to be the arbiter of truth. Fine. You tell me what
[01:45:57.600 --> 01:46:01.520]   I need to take up to leave up and what I need to take down." Which of course, I was, you know,
[01:46:01.520 --> 01:46:05.120]   serious First Amendment implication. Well, it's also disingenuous because he...
[01:46:05.120 --> 01:46:10.640]   Sure. Oh, yeah, absolutely. But like, you know, what else is he supposed to say at some point,
[01:46:10.640 --> 01:46:14.880]   right? That's what I would have said in his shoes exactly what I always said. Fine. You tell me,
[01:46:14.880 --> 01:46:21.440]   I don't know. I'll follow the law. Yeah. And if you don't like it, well, then the Pass-A-Law
[01:46:21.440 --> 01:46:26.640]   don't blame me. Well, that's not the way to build a society of carrying decent people.
[01:46:26.640 --> 01:46:31.680]   Yeah. And that's why, again, sort of going back to like Twitter and Elon, what I talked about
[01:46:31.680 --> 01:46:36.880]   before, like, you know, the fact is like with the DSA in the EU, you know, Twitter has been a really
[01:46:36.880 --> 01:46:42.640]   active participant in talking to EU politicians and explaining why the DSA is currently formatted
[01:46:42.640 --> 01:46:48.560]   would have tremendous, you know, speech suppression. And so like having actual fundamental principles
[01:46:48.560 --> 01:46:53.760]   that you believe in, then you go and you fight around the world for, you know, laws that are
[01:46:53.760 --> 01:46:57.840]   causing trouble for it, you know, that's what it means to be committed to free speech. Saying,
[01:46:57.840 --> 01:47:01.520]   as Elon has said, that like, we're just going to take whatever the government tells us to do,
[01:47:01.520 --> 01:47:05.840]   you know, that's not being committed to free speech. That's getting pushed around by any
[01:47:05.840 --> 01:47:11.360]   government in the world. And I think that's a real problem. It's true that we don't talk about
[01:47:11.360 --> 01:47:18.160]   Facebook anymore too. Yeah. It's like, it's scary. It was destroying the world. Oh, yeah. Remember
[01:47:18.160 --> 01:47:23.840]   them. And they're happy about that. I was going to say, how happy do you think Mark Zuckerberg is
[01:47:23.840 --> 01:47:28.160]   about all of the the Musk Twitter stuff over the last few months, he just gets to sit back and
[01:47:28.160 --> 01:47:33.760]   laugh. And it's none of this is his boss. You guys miss me? Yeah. Yeah. And I'm sure I am sure that
[01:47:33.760 --> 01:47:38.560]   that recruiters at Facebook are calling up every single person who works at Twitter right now.
[01:47:38.560 --> 01:47:44.640]   Oh, yeah. Promise Facebook stock is is no better than Twitter stock at this point. So maybe worse.
[01:47:45.520 --> 01:47:50.800]   So still have their own issues with the way they treat some of their staff, especially with the
[01:47:50.800 --> 01:47:57.120]   like, what is the content moderators? We should go public. Then we could offer them an upside.
[01:47:57.120 --> 01:48:02.080]   We could hire a hundred great engineers. I don't know. We do. I don't know what we'd have them do. But,
[01:48:02.080 --> 01:48:09.360]   you know, I got some stock options for you. You're going to love. Yeah. Why you still sitting on
[01:48:09.360 --> 01:48:16.080]   those useless Facebook options? They're useless. You say, why are people still blaming Facebook
[01:48:16.080 --> 01:48:24.080]   for Australia's terrible news linking tax law? This is Facebook and Google, right?
[01:48:24.080 --> 01:48:30.800]   That Rupert Murdoch wanted to take down. Yeah. But it was Facebook that acted. Well,
[01:48:30.800 --> 01:48:35.440]   I can say better than I can. Yeah. I mean, this was it was based on an article on the Wall Street
[01:48:35.440 --> 01:48:38.880]   Journal, which I didn't know it. And I should have, of course, the Wall Street Journal is owned by
[01:48:38.880 --> 01:48:43.920]   Rupert Murdoch, who's kind of at the center of all this. But but you know, this was, you know,
[01:48:43.920 --> 01:48:49.440]   last year, Australia had been working this this law for a long time, which was they admitted,
[01:48:49.440 --> 01:48:53.920]   like politicians in Australia literally admitted that Rupert Murdoch came to them with this law.
[01:48:53.920 --> 01:48:58.880]   Oh my God. To pass this law. And they did. And it basically forces Google and Facebook and just
[01:48:58.880 --> 01:49:04.560]   Google and Facebook to pay media properties, you know, mostly Rupert Murdoch, well, and
[01:49:04.560 --> 01:49:08.480]   nine West are the two sort of big. So Facebook just blocks news sharing in Australia.
[01:49:08.480 --> 01:49:13.040]   So well, they did. They did for like, you know, basically, just as the law was about to go into
[01:49:13.040 --> 01:49:18.400]   effect, both Google and Facebook had said that they would block news sharing in Australia in
[01:49:18.400 --> 01:49:22.640]   response to this law going into effect. Then just like on the eve of it going into effect,
[01:49:22.640 --> 01:49:27.520]   Facebook actually did start, start blocking it. Google didn't Google caved and said that they
[01:49:27.520 --> 01:49:32.880]   would they would just start paying. And people went crazy about it. And then like, there was a
[01:49:32.880 --> 01:49:37.680]   slight change to the law in response to that. And a week later, Facebook reinstated it.
[01:49:37.680 --> 01:49:42.240]   And so this this Wall Street Journal article, Facebook, here's the headline,
[01:49:42.240 --> 01:49:48.400]   Facebook deliberately caused havoc in Australia to influence new law, whistleblower say.
[01:49:48.400 --> 01:49:56.480]   And so I read that article and I kept looking for where is this smoking gun from the whistleblower
[01:49:56.480 --> 01:50:02.480]   about like causing havoc. And you know, what what you found in the articles they talk about in,
[01:50:02.480 --> 01:50:05.520]   you know, a whole bunch of early paragraphs about how they didn't just block news.
[01:50:05.520 --> 01:50:10.160]   They also block some government and some nonprofit websites. And so like, okay, that's
[01:50:10.160 --> 01:50:15.920]   bad in theory. But I kept waiting and you know, the headline of the article and the subhead of
[01:50:15.920 --> 01:50:21.680]   the article implies that that was all done on purpose by Facebook execs. And it takes until
[01:50:21.680 --> 01:50:28.240]   paragraph 55, it's a very long article. It's in paragraph 55, where they find it.
[01:50:28.240 --> 01:50:35.200]   I counted I counted twice because I I counted 55. And I was like, is that really 55? And I went
[01:50:35.200 --> 01:50:41.920]   back and I counted it. And it is literally the 55th paragraph where they say Facebook executives
[01:50:41.920 --> 01:50:47.840]   realize and like they the reporters at the Wall Street Journal had this, they realized that this
[01:50:47.840 --> 01:50:52.240]   was a mistake that they had blocked too many sites and they they said we have to fix this.
[01:50:52.240 --> 01:50:57.360]   That's in paragraph 55 after they spent all this time talking about how Facebook deliberately tried
[01:50:57.360 --> 01:51:00.960]   to block stuff. And they bring in all these things where you're like, you know, as I'm reading, I'm
[01:51:00.960 --> 01:51:06.080]   like, this is exactly what, you know, if you're in a position where the law says if you allow any
[01:51:06.080 --> 01:51:11.280]   links to news, you're going to have to pay, then you know, any lawyer in the world is going to say
[01:51:11.280 --> 01:51:15.200]   you block everything that might be considered news. Don't make a mistake. If you make one
[01:51:15.200 --> 01:51:19.040]   mistake, you're screwed and you're going to have to pay up. So of course, they're going to block
[01:51:19.040 --> 01:51:24.640]   everything. And so like, that's basically the smoking gun was they had lawyer, the legal team
[01:51:24.640 --> 01:51:30.880]   and the policy team told them we need to be inclusive in who we block, which is what the law tells
[01:51:30.880 --> 01:51:35.120]   them to do, right? You know, if you don't want the law to apply to you, which is what Facebook
[01:51:35.120 --> 01:51:39.440]   was saying, then you have to block everything. And then as soon as they realize it was a mistake,
[01:51:39.440 --> 01:51:43.440]   basically people at Facebook tried to, you know, walk through and figure out which sites they
[01:51:43.440 --> 01:51:47.760]   over blocked and to fix it, which is exactly what you should expect. And the problem is the way
[01:51:47.760 --> 01:51:53.600]   the law is written, not the way Facebook acted. And so like the whole article was really, really,
[01:51:53.600 --> 01:51:57.280]   you know, and you know, it got a bunch of attention last week, because everybody loves,
[01:51:57.280 --> 01:52:01.040]   you know, these stories about Facebook being evil and Facebook might be evil. But in this case,
[01:52:01.040 --> 01:52:05.280]   it doesn't appear that they were just a evil. And when you hear me talk about, about Murdoch,
[01:52:05.280 --> 01:52:12.320]   and I include in that the Wall Street Journal, there is an agenda to media's coverage of the
[01:52:12.320 --> 01:52:16.000]   Internet. And I'm not saying that this particular reporter, you know, got a call from Rupert saying
[01:52:16.000 --> 01:52:19.840]   this and that, but it was not hard to sell this story with the organization. Yeah, you know,
[01:52:19.840 --> 01:52:25.840]   they do know. And we never in media see the acknowledgement of the conflict of interest of,
[01:52:25.840 --> 01:52:30.400]   oh, we hate these guys and they stole all our money. And we want more money from them. And it's
[01:52:30.400 --> 01:52:35.440]   a pincer movement in which media have a conflict of interest that is never, ever acknowledged.
[01:52:35.440 --> 01:52:39.520]   Yeah. Yeah. That's pretty funny. So real problem.
[01:52:39.520 --> 01:52:44.320]   And I'm sure they figure if by paragraph 55, you've already made your conclusions. You're not
[01:52:44.320 --> 01:52:51.440]   reading this. But it's Princess Spaghetti sauce. It's in there. We put it in there.
[01:52:51.440 --> 01:52:55.360]   Was that actually the sponsor? I can't remember those Princess Spaghetti sauce,
[01:52:55.360 --> 01:52:59.120]   but everybody's too young to know that commercial anyway, right? I thought that was Prago.
[01:52:59.120 --> 01:53:05.200]   So there is a kind of might be there is a kind of Prego to. So I don't know. That's why I
[01:53:05.200 --> 01:53:10.960]   bet it is Prego. I think you're pretty good. It is Prego, which is the worst bottled Spaghetti
[01:53:10.960 --> 01:53:16.160]   sauce. If you eat Prego, stop. Everything's still bad. Stop. It's in there. That's why it's bad.
[01:53:16.160 --> 01:53:21.440]   Prego was the Spaghetti sauce he used in college when you had college out of the jar.
[01:53:21.440 --> 01:53:26.240]   Yeah. Exactly. Well, sweet. Why heat it up? The Spaghetti's hot. Just put it on top.
[01:53:26.240 --> 01:53:33.280]   Wait, you were there too? Yeah. I remember very, very well.
[01:53:35.600 --> 01:53:42.640]   Respect. Speaking of pasta. Here is what? Kachioi Pepe lady. What is became famous for this?
[01:53:42.640 --> 01:53:45.200]   All right. I'm going to play a little TikTok video here.
[01:53:45.200 --> 01:53:51.040]   Everyone must try this kachioi pepe pasta sauce from Trader Joe. That's why.
[01:53:51.040 --> 01:53:54.240]   What's wrong with that? Kachioi pepe. No, everybody loves the way she said it.
[01:53:54.240 --> 01:53:57.360]   Oh, I see it. It's kachioi pepe. Kachioi pepe.
[01:53:57.360 --> 01:54:03.440]   Apparently that went viral. Apparently. Yeah. Now people are shouting at her.
[01:54:04.880 --> 01:54:06.480]   Saying, say, Kachioi pepe.
[01:54:06.480 --> 01:54:13.840]   She said her exaggerating accent was just part of her Italian-American background.
[01:54:13.840 --> 01:54:19.200]   What are you talking here? Didn't mind the teasing, especially from actual Italians who told her.
[01:54:19.200 --> 01:54:26.960]   She's saying it incorrectly. Apparently, this is like, I, yeah, TikTok. It's amazing. It's amazing.
[01:54:26.960 --> 01:54:31.360]   Wow. That went viral. I certainly thought I was like 17, didn't you?
[01:54:32.320 --> 01:54:35.760]   26. I was born in 1995. You're the pig.
[01:54:35.760 --> 01:54:40.480]   Okay. Bye-bye. She's a little unbelievable.
[01:54:40.480 --> 01:54:48.000]   I'm gonna take creators. Yeah. I'm 26. Okay. Thank you. Okay.
[01:54:48.000 --> 01:54:54.000]   I won't. The first TikTok from space is just this. You want me to play this as well?
[01:54:54.000 --> 01:54:57.920]   No, I didn't care. I just put it in there. Great. Okay. I'm gonna skip that then.
[01:54:59.520 --> 01:55:03.280]   I thought you might be interested. It's kind of like you like space stuff here. You can listen to a
[01:55:03.280 --> 01:55:08.240]   black hole that's down on the other. That's not an interesting. Yeah, the sound of black hole
[01:55:08.240 --> 01:55:12.240]   makes except it's not really the sound of black hole makes because in space, no one can hear you
[01:55:12.240 --> 01:55:19.520]   scream. So it's just what it might make if it were not in space, which of course, because it's a black
[01:55:19.520 --> 01:55:26.960]   hole, it will be because it'll suck all the air in and then we'll be in space. Ohio bill will ban
[01:55:26.960 --> 01:55:32.560]   Facebook and Twitter from censoring users. So it's now Florida, Texas and Ohio.
[01:55:32.560 --> 01:55:40.800]   There are more. There are more. Yeah. Oh, yeah. Unbelievable. What a I don't I don't I don't I
[01:55:40.800 --> 01:55:51.840]   why how do you not Mike? You face this day and day out this drum beat of bull crap. How do you not
[01:55:51.840 --> 01:56:00.320]   just despair and throw up your hands ago? I give up. He's special. I am an eternal optimist. I still
[01:56:00.320 --> 01:56:06.000]   believe that that you know, people will come to their senses eventually. I don't know what what
[01:56:06.000 --> 01:56:12.320]   why I all evidence is to the contrary. I can pretty much guarantee you no one's coming to their senses
[01:56:12.320 --> 01:56:16.320]   ever. Oh, it's a century from now they will, but that's the problem we're all gonna be gone. What
[01:56:16.320 --> 01:56:21.440]   do you mean a century from now? What they'll wake up and say this mess we made of everything?
[01:56:21.440 --> 01:56:28.720]   Well, the book last until 1996, I think. Yeah. I mean, yes. Yes. 50 years after I'm dead,
[01:56:28.720 --> 01:56:33.440]   somebody will say, you know what that Mike Masnick. He was right. You dream on.
[01:56:33.440 --> 01:56:41.120]   All right. All right. If you if you say so, if you say so, you want to hear the sound of a
[01:56:41.120 --> 01:56:47.600]   black hole New York from the New York Times and a part of an effort to sonify the cosmos,
[01:56:48.160 --> 01:56:54.160]   reach researchers who converted the pressure waves from a black hole into an audio something.
[01:56:54.160 --> 01:57:01.040]   This is the stupidest.
[01:57:01.040 --> 01:57:15.840]   Sound waves of the Perseus galaxy cluster were re-synthesized. In other words, completely made up.
[01:57:15.840 --> 01:57:22.800]   After boosting their frequency quadrillions of times, scaling them 57 to 58 octaves above
[01:57:22.800 --> 01:57:27.360]   their true pitch and turning. So in other words, they couldn't hear it instead of going to the
[01:57:27.360 --> 01:57:35.120]   center. You couldn't hear it if you wanted to. It's. Yeah, you did this to annoy me, didn't you?
[01:57:35.120 --> 01:57:38.480]   Well, you wouldn't want me. I know you.
[01:57:38.480 --> 01:57:44.240]   Sound of the Perseus galaxy. And then they put like a radar thing.
[01:57:44.240 --> 01:57:49.120]   The where you can't. A radar thing on it. So it looks like some, you know, it's scientific.
[01:57:49.120 --> 01:57:56.960]   So that's science. You can't, you can't deny it. It's science. Man. I wonder what Mr. Rod Powell and
[01:57:56.960 --> 01:58:00.880]   Tarte Malick think of this on this week. Oh, we'll find out.
[01:58:00.880 --> 01:58:05.040]   What is it? Friday? They do that? Fridays.
[01:58:05.040 --> 01:58:11.920]   I have to ask them Friday this week in space with the TV slash TW. I asked clubhouse. You know,
[01:58:11.920 --> 01:58:18.000]   we use them as the, as the whipping horse, the stalking horse for sites nobody talks about
[01:58:18.000 --> 01:58:24.320]   anymore, whatever happened. Ha, ha, ha. Well, they just, they've just been, they just did a
[01:58:24.320 --> 01:58:30.640]   sea round and have now worth $4 billion. That's what happened. Ha, ha, ha, another round of financing
[01:58:30.640 --> 01:58:39.040]   from Andreessen Horowitz participation in DST Global Tiger Global and Elad Gile. They raised
[01:58:39.040 --> 01:58:45.760]   quite a bit. What did they get? What did they get? I don't know what they got. You know, this is
[01:58:45.760 --> 01:58:51.840]   annoying because all they say in tech crunches, their valuation is $4 billion. But they didn't say
[01:58:51.840 --> 01:58:57.120]   how much, how much money they actually got out of that, you know. So, you know, I don't hear as
[01:58:57.120 --> 01:59:03.760]   much chatter about clubhouse, but there are a couple photographers that I talked to fairly
[01:59:03.760 --> 01:59:09.120]   regularly and they still do their little events. Really? Pretty, you know, like every day. Do people
[01:59:09.120 --> 01:59:15.920]   show up? Yeah, apparently they keep doing them. Yeah. I understand that. I don't heard a word
[01:59:15.920 --> 01:59:22.720]   to participate in it, but some of the photographers that I talked to, they still go in every week,
[01:59:22.720 --> 01:59:27.360]   hosting whatever they're hosting or they participate in one that's hosted by somebody else like the,
[01:59:27.360 --> 01:59:32.240]   you know, it's just a community based kind of thing. So, I mean, it may not be the talk of the town,
[01:59:32.240 --> 01:59:37.840]   like this, catch you a pippy. Tick tock, but apparently they're still being used.
[01:59:37.840 --> 01:59:44.400]   Yeah, everyone I know who used clubhouse is now switched over to Twitter spaces.
[01:59:44.400 --> 01:59:50.080]   Equally worthless, but okay. Fine. By the way, I'm speaking of which Jack's
[01:59:50.080 --> 01:59:55.040]   wasn't here. Yeah, I have it. Is this Jack's? Yes, Jack was trying to be on spaces. Look what
[01:59:55.040 --> 02:00:01.520]   happened. Here he is. Can't seem to load or join the space on most a host now. It's, it's blank.
[02:00:02.240 --> 02:00:10.080]   If Jack can't figure out how to do it, by the way, maybe part of the problem is it's got the old
[02:00:10.080 --> 02:00:16.160]   name of his company, Square. Well, I'm still the brand. Oh, it's still square. It's not block. But
[02:00:16.160 --> 02:00:21.280]   the brand I saw commercially. Yeah. So is he the square head or the block head? Well, he's both.
[02:00:21.280 --> 02:00:30.320]   Oh, it's gloosey. He's both. We'll try to do it another time then, he says. Somebody says,
[02:00:30.320 --> 02:00:37.920]   that's what you get for killing fleets. To which he replies, karma's a B.
[02:00:37.920 --> 02:00:49.600]   Wow. That's a hot thread. Have you so you use be real? No, I saw that and I was so tempted to
[02:00:49.600 --> 02:00:56.000]   download it and then I forgotten. So no, it's a new social app. It's boring, according to the New
[02:00:56.000 --> 02:01:00.560]   York Times and the New York Times is on it, by the way. It's boring in a good way.
[02:01:00.560 --> 02:01:06.000]   Once a day at an unpredictable time, be real notifies its users. They have two minutes
[02:01:06.000 --> 02:01:12.000]   to post a pair of pictures. The pictures are from both cameras. So the front and the back.
[02:01:12.000 --> 02:01:15.600]   So if you're in a messed up room or whatever, that's what they're going to get.
[02:01:15.600 --> 02:01:21.440]   Oh, somebody. You should do this, aunt.
[02:01:23.200 --> 02:01:29.280]   Yeah, it's kind of tension lately. Yeah. Yeah. It's a pair. It's French. They're based in Paris.
[02:01:29.280 --> 02:01:33.200]   So I guess it's pronounced, "Caccio e Pebe."
[02:01:33.200 --> 02:01:41.120]   Found it in 2020. It's been around for two years by April of this year. 7.4 million installs,
[02:01:41.120 --> 02:01:47.840]   according to Aptopia. I have, I think I remember seeing this when it came out, raised about 30
[02:01:47.840 --> 02:01:53.760]   million adventure funding last year. You know why? Because Bitcoin's been down.
[02:01:53.760 --> 02:01:58.640]   To find somewhere else to put their money. I'm seeing some stories right now wondering
[02:01:58.640 --> 02:02:03.680]   whether Coinbase is going to go bankrupt. Well, they're not, but they had to put in their filing
[02:02:03.680 --> 02:02:10.320]   because it's a requirement of the SEC. What would happen if they went bankrupt? So Coinbase is
[02:02:10.320 --> 02:02:16.560]   an exchange, a Bitcoin or another cryptocurrency exchange, where a lot of people store, and this
[02:02:16.560 --> 02:02:22.560]   would be the mistake you'd be making. Store their crypto there using a custodial wallet. You should
[02:02:22.560 --> 02:02:29.600]   have a wallet you own because... Like you could forget like euros? Yeah. Yeah. Well, at least I have it.
[02:02:29.600 --> 02:02:37.120]   I have. I have. Is a relative verb there. All I have to do is sit down and remember the password.
[02:02:37.120 --> 02:02:41.920]   And by the way, there's not... No, no, no, no, no. It'll come back to me. It's not a big deal.
[02:02:41.920 --> 02:02:51.360]   But... Honestly, yeah, it's down to $28,000. It's lost more than half of its value over the last
[02:02:51.360 --> 02:03:01.520]   six months or something. But that's fine. That's why I have met Bitcoin. If I had remembered the
[02:03:01.520 --> 02:03:07.280]   password, I wouldn't have any. I would have sold it when it was five bucks and said, "Hey, I'm rich.
[02:03:08.480 --> 02:03:14.320]   I got any bucks." But instead, I have it. That's all that matters. And I'm going to have it for a long
[02:03:14.320 --> 02:03:20.000]   time. But if you gave... So, point being, if you used a custodial wallet anywhere,
[02:03:20.000 --> 02:03:25.040]   that's problem number one. Custodial wallets have been stolen in the past if you had a custodial
[02:03:25.040 --> 02:03:32.160]   wallet with Matt Cox. Bye-bye, Bitcoin. But problem number two, in their SEC filing, Coinbase pointed
[02:03:32.160 --> 02:03:40.160]   out, if we were to go bankrupt, you would then be one of our creditors and probably at a pretty
[02:03:40.160 --> 02:03:48.240]   low on the list. So, in other words, if they go bankrupt, good luck getting your Bitcoin out of them.
[02:03:48.240 --> 02:03:54.000]   This is... The thing is, if they were to go bankrupt, it would probably be because Bitcoin is worth so
[02:03:54.000 --> 02:04:00.000]   little that you wouldn't care. You wouldn't care. Before I went with it. Yeah. But Ethereum, Mike,
[02:04:00.000 --> 02:04:05.200]   Ethereum is the future. They say, "We're not going to go bankrupt. That's not that we're
[02:04:05.200 --> 02:04:08.800]   required to state what would happen." Which is probably a good thing to know.
[02:04:08.800 --> 02:04:14.880]   Moral of that story is, don't put all your coins in any custodial wallet. If you want to sell coins,
[02:04:14.880 --> 02:04:19.680]   transfer them over, sell them, get the money, or you want to buy other stuff to do that. What's the
[02:04:19.680 --> 02:04:25.200]   wallet that a mortal human being can actually do? There's lots of Bitcoin wallets you hold on your
[02:04:25.200 --> 02:04:32.240]   computer or on your phone. Metamask is a popular one. I use the basically the
[02:04:32.240 --> 02:04:39.520]   reference design from the Bitcoin Foundation. Their original Bitcoin wallet is perfectly good.
[02:04:39.520 --> 02:04:47.440]   John says, "electrum." But anyway, if you hold your own wallet, then you don't have to worry.
[02:04:48.720 --> 02:04:56.880]   If you let somebody else hold your wallet and they lose it, there are the hardware wallets now
[02:04:56.880 --> 02:05:02.080]   as well. That's right. There's plenty of ways to do that. People who are serious start to get
[02:05:02.080 --> 02:05:08.240]   it. All the wallet is a number, by the way. It's just a random for your long account number,
[02:05:10.720 --> 02:05:23.120]   and I protected with a random phrase. You want to buy the house where Facebook was created?
[02:05:23.120 --> 02:05:30.000]   This is the beautiful San Jose mansion. For the first time since its construction,
[02:05:30.000 --> 02:05:36.640]   only 25 years ago, it is for sale. What mountain view? Where is it? Silicon Valley somewhere?
[02:05:38.080 --> 02:05:45.440]   Los Altos. Only ever been up for rent since 2004. Six bedrooms, four and a half baths.
[02:05:45.440 --> 02:05:51.120]   Students love it. 19-year-old Mark Zuckerberg and Dustin Moskowitz rented it along with 24-year-old
[02:05:51.120 --> 02:05:59.440]   Sean Parker. That's where they were when they created it. Well, the Facebook took off, I guess.
[02:05:59.440 --> 02:06:06.160]   They created it at Harvard. Now, how much would you pay? What was the square footers again?
[02:06:06.160 --> 02:06:09.440]   Oh, it's big. It's like six bedrooms or something.
[02:06:09.440 --> 02:06:14.480]   Six bedrooms? Yeah. Six bedrooms, four and a half baths. I don't see the square footage.
[02:06:14.480 --> 02:06:21.360]   Asking price, $5.3 million. Oh, I was going to say seven. No.
[02:06:21.360 --> 02:06:27.280]   Here is from the land lady, the first deposit check written by Mark Zuckerberg for the lease of
[02:06:27.280 --> 02:06:35.040]   the house. Their rent was $5,500 a month, so he gave her two months rent. Nice handwriting, Mark.
[02:06:35.920 --> 02:06:40.320]   But she's got the autograph. I noticed she still has a check. That's good.
[02:06:40.320 --> 02:06:46.000]   Look at that. There's a beautiful sunroom at Casa de Facebook.
[02:06:46.000 --> 02:06:54.240]   It's actually a pretty big house for Los Altos. That's expensive area. 3,000 square feet.
[02:06:54.240 --> 02:07:01.360]   You can get the whole crew in there, ain't it? Yeah. Yeah. Be good. You can put the pods in.
[02:07:01.360 --> 02:07:07.520]   Wouldn't have much anything else in there, but we'd be eating. Well, maybe they'd leave the sign that
[02:07:07.520 --> 02:07:14.880]   says cat, you know, or is that eat might be eat. I don't know. I can't tell. Welcome to pound town.
[02:07:14.880 --> 02:07:21.600]   Somehow, I don't think there was a lot of that going on, to be honest with you.
[02:07:21.600 --> 02:07:29.360]   Look at that. Beautiful, beautiful home. There's the shower where Mark Zuckerberg bathed
[02:07:29.360 --> 02:07:35.920]   and first discovered shower shoes. That's good. There's a gazebo out pack. That's fancy. Fancy house.
[02:07:35.920 --> 02:07:40.800]   Los Altos is a very nice area. And the movie, there was a swimming pool. It doesn't look like there's
[02:07:40.800 --> 02:07:46.400]   actually one. Oh, yeah, you're right. So much for the movie. See, I told you that movie was
[02:07:46.400 --> 02:07:55.760]   full of it. Meta has its own retail store. I might have to make, we wouldn't have to do a field trip,
[02:07:55.760 --> 02:08:02.240]   Ant you and me, the meta retail store just opened in Burlingame, California. What would you buy if
[02:08:02.240 --> 02:08:08.880]   you went to the meta retail store? Well, we can take much better field trips. You know that.
[02:08:08.880 --> 02:08:16.320]   Yeah, what would they have? There it is. That's what they have. They have some portals. They have
[02:08:16.320 --> 02:08:23.360]   the quest. And you can go in there and play supernatural on the quest while people stare at you.
[02:08:24.720 --> 02:08:31.920]   People love the portal though, right? Portals. Yeah, I mean, if you don't mind Facebook,
[02:08:31.920 --> 02:08:36.880]   grandparents, yeah, the grandparents like I had, I bought the original portal and really never
[02:08:36.880 --> 02:08:41.680]   found any use for it. So I let Burke take it apart, which means it's a longer function. Yeah,
[02:08:41.680 --> 02:08:51.920]   it's gone. It's gone. It's gone. Anything else? I will let you and Mike Masnick, our great guest
[02:08:51.920 --> 02:08:56.000]   and Jeff Jarvis said, tell me, is there anything else that you want to talk about from this
[02:08:56.000 --> 02:08:59.760]   long list? I'm not going to do the change log. We did the change log.
[02:08:59.760 --> 02:09:04.000]   Oh, it's fine. We saw it. I think Facebook is now, Facebook is saying they're going to not give
[02:09:04.000 --> 02:09:07.680]   as much money to news publishers because screw it. We don't need them. And we're all
[02:09:07.680 --> 02:09:11.280]   sorry. Is that affect you? No, no.
[02:09:11.280 --> 02:09:18.320]   What was the story I saw something about? Go ahead, and then Mike.
[02:09:20.000 --> 02:09:23.840]   I'm trying to find that I remember seeing a headline about TikTok taking over
[02:09:23.840 --> 02:09:31.280]   surpassing YouTube or? Yep, that was up. It is. It is. But you know, that's,
[02:09:31.280 --> 02:09:40.160]   that's, yeah, it's going to, according to E-Marketer, here's the graph. TikTok users will spend more time
[02:09:40.160 --> 02:09:48.480]   this year on TikTok than YouTubers will spend on YouTube. And the black is the rabbit hole.
[02:09:48.480 --> 02:09:56.720]   The black is TikTok net ad revenues, which are now, or predicted to be $11 billion in 2024,
[02:09:56.720 --> 02:10:03.600]   currently, five point, well expected to be 5.96. So not, not, you know,
[02:10:03.600 --> 02:10:08.960]   some, somewhere, it's a little bit less than podcasts.
[02:10:08.960 --> 02:10:16.560]   Not, not at all. Not at all. What's the red percent change percent of digital ad spending?
[02:10:16.560 --> 02:10:22.640]   They got like 3.5% of total digital ad spend. Digital ad spend is on the, on the up and up.
[02:10:22.640 --> 02:10:29.360]   Well, Mike, people spend a lot of time on TikTok because it's 60 seconds or less.
[02:10:29.360 --> 02:10:36.880]   Yeah, it's quick. YouTubers, the, the prominent YouTubers are doing their videos most of the time
[02:10:36.880 --> 02:10:44.400]   at 10 minutes of clip. Because, yeah. Mike, what was your pick? I was going to, to point out the
[02:10:44.400 --> 02:10:50.400]   Josh Hawley copyright bill. All right, Mr. Hawley has decided that if Disney
[02:10:50.400 --> 02:10:57.280]   is going to be woke, and you know, that's the worst insult Josh Hawley can give anybody,
[02:10:57.280 --> 02:11:05.520]   then they don't deserve this, you know, long copyright. So he's, this is a, you call it a laughably
[02:11:05.520 --> 02:11:12.800]   stupid copyright term reduction bill. Where's Sonny Bono when he, when we need him?
[02:11:13.920 --> 02:11:18.480]   Yeah. Yeah. So this is the thing where it's like, you know, I've probably spent two and a half
[02:11:18.480 --> 02:11:24.880]   decades fighting against long copyright. And, and nobody has ever introduced a bill to
[02:11:24.880 --> 02:11:30.720]   shorten copyright until now. And it has to be done by Josh Hawley for the worst, most ridiculous
[02:11:30.720 --> 02:11:37.360]   reasons. In the most impossible way, it is very much a, a monkey paw kind of situation where,
[02:11:37.360 --> 02:11:42.400]   you know, I, I probably would have supported almost anyone coming up with, with a law that would
[02:11:42.400 --> 02:11:48.160]   reduce copyright back to 56 years, which is what, what he's doing. But he has done it in a way that
[02:11:48.160 --> 02:11:53.680]   is so clearly and ridiculously unconstitutional would never stand up to any kind of scrutiny.
[02:11:53.680 --> 02:11:59.520]   But he probably knows that, right? He's actually all of us in the contrary, a very smart, well
[02:11:59.520 --> 02:12:05.360]   educated guy. He's an extremely well educated guy. And is, you know, and literally calls himself
[02:12:05.360 --> 02:12:11.360]   a constitutional scholar. Yeah. You know, he clerked for, for the Supreme Court, you know, he knows
[02:12:11.360 --> 02:12:15.440]   that this is not how any of this works. And it's clear that, you know, the bill, he has no
[02:12:15.440 --> 02:12:19.280]   intention of the bill actually passing. No, he just needs to sort of play for the ban base.
[02:12:19.280 --> 02:12:21.600]   It's good for the base. He's plucking, plucking the strings.
[02:12:21.600 --> 02:12:26.320]   But it's, it's so frustrating that I had to spend all this time sort of explaining
[02:12:26.320 --> 02:12:30.080]   why this bill that conceptually I would love to see a bill that I.
[02:12:30.080 --> 02:12:35.600]   Well, for years, you and I and everybody else have been bemoaning the cop, because every time
[02:12:35.600 --> 02:12:42.720]   Mickey Mouse's copyright comes up for expiration and it for years, it was Sonny Bono who would do
[02:12:42.720 --> 02:12:48.320]   this, they would extend the term of copyright to protect Disney's assets, including most importantly
[02:12:48.320 --> 02:12:55.200]   Mickey Mouse. And that would just happen like clockwork every every 18 years or something.
[02:12:55.200 --> 02:13:01.200]   What is the, what is the current copyright term? So, well, it, that's not so easy to say.
[02:13:01.200 --> 02:13:06.000]   But what I will say is that Mickey Mouse will, will the original Mickey Mouse, which is the
[02:13:06.000 --> 02:13:11.920]   Steamboat Willie version of Mickey Mouse, will go into the public domain January 1st, 2024.
[02:13:11.920 --> 02:13:17.680]   We are less than two years away. Unless Sonny Bono comes back from the dead and extends it.
[02:13:17.680 --> 02:13:23.760]   There is, there appears to be very little appetite. Oh, good. You know, the one thing I am thankful
[02:13:23.760 --> 02:13:28.240]   for is finally that nobody seems to be even Disney seems to have given up on the idea of
[02:13:28.240 --> 02:13:32.800]   continuing to extend Star Wars and Marvel. They don't need Mickey Mouse anymore.
[02:13:32.800 --> 02:13:39.520]   They're in good shape with the way to wait till the Luke Skywalker copyright starts to come up for
[02:13:39.520 --> 02:13:46.960]   due. There is a lot of time for that. So, it would, the original term was 14 years with 14 years
[02:13:46.960 --> 02:13:59.280]   renewable. In 1831, it went to 2828. It's like, what is it, 56? It's very, it does a vary on the
[02:13:59.280 --> 02:14:04.160]   kind of work. Is that the problem with saying? No, the issue right now is that that older works
[02:14:04.160 --> 02:14:09.440]   are sort of under different rules than the newer ones because we had the 1976 copyright act,
[02:14:09.440 --> 02:14:13.520]   which went into effect in 1978 and sort of changed the rules. And then there's like a whole
[02:14:13.520 --> 02:14:18.960]   bunch of issues with kind of different types of works. But effectively right now, we're at,
[02:14:18.960 --> 02:14:27.120]   what is it, life plus 70. Yes. Yes. It's the current copyright. That's absurd. Yes, it is.
[02:14:27.120 --> 02:14:33.360]   And you know, what's more absurd, honestly, than the term is the fact that it used to be, again,
[02:14:33.360 --> 02:14:38.720]   until 1978, that in order to get copyright, you had to register it. And so, most people didn't
[02:14:38.720 --> 02:14:45.920]   register their works at all. And so, those works were not covered by copyright. And then in 1976,
[02:14:45.920 --> 02:14:50.320]   we changed it so that everything is effectively automatically given a copyright. So, we went
[02:14:50.320 --> 02:14:56.560]   from an opt-in system to not even an opt-out because you actually cannot opt out. The thing that works
[02:14:56.560 --> 02:15:02.560]   get copyright, you can issue a waiver or a creative commons license, which is effectively a waiver
[02:15:02.560 --> 02:15:09.760]   of your copyright rights. And that is bizarre. And we went from a time where, right before that,
[02:15:09.760 --> 02:15:15.920]   it was 56 years. It was 28 and 28. And you had to renew. And I think I put in the article that,
[02:15:15.920 --> 02:15:20.000]   you know, if you look at, you know, most people didn't review, almost nobody reviewed. The only
[02:15:20.000 --> 02:15:26.240]   people who did review were the motion picture industry, which is like the main industry that
[02:15:26.240 --> 02:15:31.920]   Holly targets. You know, the retroactive nature of Holly's bill only applies to companies in the
[02:15:31.920 --> 02:15:38.880]   motion picture industry. And only if their market cap is above $150 billion.
[02:15:38.880 --> 02:15:40.640]   They have a Florida amusement park.
[02:15:40.640 --> 02:15:46.880]   So, it basically, you know, I think it only, I was trying to figure out who it impacted. And I
[02:15:46.880 --> 02:15:54.560]   think it only impacts Disney and possibly NBC Universal. And that's it. So, it's...
[02:15:54.560 --> 02:16:00.480]   God, I'm kind of rooting for this law. But as you say, it is blatantly unconstitutional because,
[02:16:00.480 --> 02:16:04.560]   and here's your term of the weak kids, you cannot pass a law that's targeting
[02:16:04.560 --> 02:16:09.520]   a specific individual that is called a bill of a tinder. Is that right?
[02:16:09.520 --> 02:16:16.240]   Yeah. A bill of a tinder is your term of the week for this week.
[02:16:16.240 --> 02:16:23.200]   And it's un... It's actually a violation of the Constitution. Article one, section nine, clause three.
[02:16:23.200 --> 02:16:30.240]   No bill of a tinder or ex post facto law shall be passed. It's kind of ex post facto too, right?
[02:16:30.640 --> 02:16:32.320]   Kind of got two strikes against it.
[02:16:32.320 --> 02:16:36.240]   Yes. Well, it's retroactive.
[02:16:36.240 --> 02:16:40.800]   The retroactive, ex post facto is a little bit different than retroactive.
[02:16:40.800 --> 02:16:40.960]   Oh, okay.
[02:16:40.960 --> 02:16:47.120]   The ex post facto is basically saying something that you, that was lawful when you did it,
[02:16:47.120 --> 02:16:52.000]   becomes unlawful. And then they can go back. So, there is a retroactive aspect to it.
[02:16:52.000 --> 02:16:54.560]   But I don't know if removing copyright would...
[02:16:54.560 --> 02:16:57.760]   Copyright is breaking the law. It's not... There's no law that breaks here.
[02:16:57.760 --> 02:17:04.320]   Yeah. Well, I mean, that gets tricky too. I mean, there is the takings issue also,
[02:17:04.320 --> 02:17:10.160]   which is that the Fifth Amendment prevents takings. And people have long argued that if you were to
[02:17:10.160 --> 02:17:15.360]   take away a copyright, then you're taking a private property and therefore the government
[02:17:15.360 --> 02:17:20.960]   would have to compensate you for that. There are actually arguments against that, but that's
[02:17:20.960 --> 02:17:23.120]   getting really deep in the way that we don't know.
[02:17:23.120 --> 02:17:27.360]   Josh Holly doesn't care. He knows no one is going to vote for it. All it takes is a call from NBC
[02:17:27.360 --> 02:17:32.320]   Universal and they're going to go, "Well, screw that." And he just got the press release.
[02:17:32.320 --> 02:17:36.400]   And he got the article, what he really wanted, the article, and tech to it. That's what he really
[02:17:36.400 --> 02:17:41.360]   wanted. And he got it. Congratulations, Josh Holly. You got Mike Masnick to write about you.
[02:17:41.360 --> 02:17:48.960]   Well done. Jeff, did you have any last... Yes, I do. Yes, I do. Because last week, thanks to
[02:17:48.960 --> 02:17:54.320]   Glenn Fleischman and me, you got some magnificent collectible art right there on the table.
[02:17:54.880 --> 02:18:00.640]   And I want to follow up this week with another fine opportunity for more art. If you go to
[02:18:00.640 --> 02:18:08.240]   my pick, which is line 192, I just saw this because I was, you know, I go to Twitter when I'm
[02:18:08.240 --> 02:18:16.400]   not supposed to, and I found the Taco Bell still life. It's so beautiful. A Taco Bell still life.
[02:18:16.400 --> 02:18:23.520]   You could get a signed copy of that. That is so beautiful. For a mere $60. Oh, that's not bad.
[02:18:23.520 --> 02:18:32.000]   What you get to me is... How would you call this Taco Bell? There's a crunch rat. There's some
[02:18:32.000 --> 02:18:38.640]   nacho cheese sauce. There's the crispy things. And there's a fine green drink of some sort.
[02:18:38.640 --> 02:18:49.120]   Who's the same as the Taco Bell? That's the double taco I think only Taco Bell has. How do we know
[02:18:49.120 --> 02:18:54.400]   it's Taco Bell? The logo on the drink cap drink is definitely Taco Bell. That's Taco Bell?
[02:18:54.400 --> 02:18:59.680]   The time. You know, he recognizes it. He's spent a lot of time here. Now the big Mac is quite a
[02:18:59.680 --> 02:19:05.040]   piece of art as well. You might prefer it. There's no a barrier also do that. Oh, yes. Go back to
[02:19:05.040 --> 02:19:09.920]   the go to the side. He is one of the great artists of our time. There you go. There's a
[02:19:09.920 --> 02:19:17.120]   great one. Except every time I saw it, I'd want a big Mac. There's Chipotle Buried. So this guy,
[02:19:17.120 --> 02:19:22.240]   by the way, just want you to know he does have an MFA. It says so this website. And he also does
[02:19:22.240 --> 02:19:27.440]   NFTs. He's an MFA of MFA. Oh, get an MFT from there's American cheese. The back and cheese is
[02:19:27.440 --> 02:19:32.800]   not so crazy about it. Broken egg. I like his art actually. Yeah, it is. McDonald's and Trader
[02:19:32.800 --> 02:19:39.520]   Joe's Park, okay. In oil. 12 by nine. These are big. These are nice big prints. Look up.
[02:19:39.520 --> 02:19:44.320]   You got... Yeah. Yeah. These are beautiful. Sunflowers. Those are the oil, which you can't get
[02:19:44.320 --> 02:19:49.040]   them where they're all sold. But if you go to the side prints, you get a G-Clay print of it.
[02:19:49.040 --> 02:19:56.800]   We should get a fun Fettie cupcake for Ms. Higginbotham. Ms. Higginbotham. The queen of fun Fettie.
[02:19:56.800 --> 02:20:03.520]   Wow. Popcorn and M&M's. Onion. I like this guy. You know what? Yeah. He's going to the only.
[02:20:03.520 --> 02:20:08.720]   And this shows with an idiot I am. I would invest in this fine art. I would.
[02:20:08.720 --> 02:20:12.240]   Yeah. I get the oil. I would too. Yeah. The drama whiskey.
[02:20:13.200 --> 02:20:19.200]   Drama whiskey. A whiskey? Well, you like it. What is that? Somebody... No, that's not it.
[02:20:19.200 --> 02:20:23.760]   Where is it? I'm going down. It's there. If you go to the yellow. Oh. No, there's another one.
[02:20:23.760 --> 02:20:28.560]   Keep some art. There's a couple on there. Oh. Go to sign prints. The guy likes his black
[02:20:28.560 --> 02:20:35.120]   as brown liquor, right? Oh my. Go the up there. There's the one. That's the one. Ooh. That is the
[02:20:35.120 --> 02:20:40.560]   amp-pruet sold. Yep. This guy sells a lot of his work actually. Yeah. He also has that
[02:20:40.560 --> 02:20:47.920]   FFT. He's going to create my own, I guess. What an NFT of this. I don't know. I can't. I just
[02:20:47.920 --> 02:20:53.360]   give him. I guess if you like a guy, give him some money. So if it's a floor price of an NFT,
[02:20:53.360 --> 02:21:02.080]   like let's see here, the gummy bears. No, let me see. The, um, that's lost. NFT open sea. There's
[02:21:02.080 --> 02:21:08.240]   jelly beans. Let's see. It's 3.33. Still has paintings. The actual thing. 3.33.
[02:21:08.240 --> 02:21:14.000]   The eath. I don't know what that is. That's worth about 25 cents now. No, it's worth
[02:21:14.000 --> 02:21:18.640]   paying that. Oh, I like the gummy bears. The, the, the vids are low on this. This is really a
[02:21:18.640 --> 02:21:25.680]   good opportunity here to get yourself a nothing. Something you can't frame. I own that, you know.
[02:21:25.680 --> 02:21:33.600]   I own that cinnamon roll right there. Somewhere. Somewhere. You can, you can add it to your wallet
[02:21:33.600 --> 02:21:40.960]   and then lose the password. Perfect. I wonder if not knowing the password, I could add things
[02:21:40.960 --> 02:21:49.600]   to the wallet and just they'd be really safe. Because it ever exists anyway. Yeah, they really
[02:21:49.600 --> 02:21:56.720]   safe in there. It's a good idea. That grilled cheese looks good enough to eat, but you can't
[02:21:56.720 --> 02:22:02.000]   because it's an NFT. Uh, I'm going to take a break and then we are going to wrap things up with
[02:22:02.000 --> 02:22:07.200]   picks of the week. Oh, I thought those are the picks of the week. Oh, sorry. All right. I have not,
[02:22:07.200 --> 02:22:12.160]   I have not wrapped things up. Oh, I see. That's right. Okay. This is my pick. Get off a Twitter
[02:22:12.160 --> 02:22:19.200]   and pay attention. Mr. Jarvis. Our show today brought to you literally by cash fly our content
[02:22:19.200 --> 02:22:24.480]   delivery network. How do we know cash flies amazing? I've been using them for more than 10 years.
[02:22:24.480 --> 02:22:28.080]   They solved a big problem. When we first started doing podcast, so many people are downloading
[02:22:28.080 --> 02:22:33.040]   them. It was impractical to have it from the website. We couldn't afford that. It was too slow.
[02:22:33.040 --> 02:22:39.600]   Matt Levine from cash fly called me and said, Hey, put it on cash fly. We've got 50 points of presence.
[02:22:39.600 --> 02:22:46.000]   All of the world, everybody downloading your audio and now video is going to be getting it
[02:22:46.000 --> 02:22:51.680]   from a server near them. It's going to be fast, reliable. And I have to say, I can vouch for it.
[02:22:51.680 --> 02:22:58.320]   It has been now you can deliver your video live, stream it with cash fly, making your content
[02:22:58.320 --> 02:23:04.320]   infinitely scalable. You can go live in hours, not days with sub one second latency. This is so
[02:23:04.320 --> 02:23:11.120]   much better than that unreliable Web RT solution, Web RT solution, Web socket live video workflow
[02:23:11.120 --> 02:23:18.480]   scales to millions of users with cash flies ultra low latency video streaming. You will have some
[02:23:18.480 --> 02:23:23.840]   pretty happy customers. It's really amazing. You can reach your audience anywhere in the world,
[02:23:23.840 --> 02:23:29.920]   50 plus locations around the globe. Plus with cash fly storage optimization system,
[02:23:29.920 --> 02:23:35.120]   we use this for our podcast. You take a load off your origin servers, reduce your S three bills,
[02:23:35.120 --> 02:23:41.200]   reduce your bandwidth and increase your cash at ratio to 100%. That's nice cash fly storage
[02:23:41.200 --> 02:23:47.840]   optimization system. They also offer fully managed CDN solutions with cash flies, elite managed package.
[02:23:48.320 --> 02:23:54.640]   You'll get VIP treatment 24 seven response report support and response time in less than an hour.
[02:23:54.640 --> 02:24:01.520]   Less than an hour. They are I have to tell you the best our support guys cash layer so great.
[02:24:01.520 --> 02:24:06.160]   And it's always our fault, frankly, never theirs. We've never had a problem with cash
[02:24:06.160 --> 02:24:11.840]   fly. So what do you get ultra low latency video streaming for more than a million concurrent users
[02:24:11.840 --> 02:24:18.320]   lightning fast gaming delivering downloads faster with zero lag glitches or outages mobile content
[02:24:18.320 --> 02:24:24.160]   optimization that offers automatic and simple image optimization. So your site loads faster on
[02:24:24.160 --> 02:24:30.560]   any device, multiple CDNs for redundancy and failover intelligently balancing your traffic across
[02:24:30.560 --> 02:24:37.280]   multiple providers. It's 10 times faster than traditional methods on six continents 30% faster
[02:24:37.280 --> 02:24:45.760]   than other major CDNs with a 98% cash hit ratio. And cash fly has been perfect in the last 12
[02:24:45.760 --> 02:24:55.680]   months 100% availability 100% best of all the best support ever 24 seven 365 day a year priority
[02:24:55.680 --> 02:25:01.920]   support. So they'll always be there when you need them. We are such fans. We are so grateful to cash
[02:25:01.920 --> 02:25:07.280]   fly because they really saved my bacon back in the day and they still do it. When you get your
[02:25:07.280 --> 02:25:12.320]   shows, you're getting it from cash fly. Right now, if you want to be on cash fly to just go to
[02:25:12.320 --> 02:25:17.840]   twit.cash fly.com. They're giving away a complimentary detailed analysis of your current CDN bill and
[02:25:17.840 --> 02:25:27.360]   usage trends. See if you're overpaying 20% or more, go to twit.cash fly.com. We thank them so much for
[02:25:28.080 --> 02:25:34.720]   their support of this week in Google. Mike Masnick, do you have a pick anything you're up
[02:25:34.720 --> 02:25:40.960]   these days excited about? Yeah, I was actually debating because I have a few things, but I'll
[02:25:40.960 --> 02:25:47.520]   go. You could do all of them. I'm not I'm fine. We're not in a hurry. All right. Well, I'll see. But I
[02:25:47.520 --> 02:25:56.320]   have something that I have unfortunately, probably from my wallet gone deep into the world of
[02:25:56.320 --> 02:26:05.920]   mechanical keyboards. Oh, yeah, baby. Oh, yeah. And I was I was asking people on Twitter for some
[02:26:05.920 --> 02:26:12.240]   thoughts and advice and in particular on the difficulty of learning to use an ortholinear
[02:26:12.240 --> 02:26:17.760]   keyboard, which is where instead of staggered like a regular keyboard, all the keys are straight
[02:26:17.760 --> 02:26:25.920]   up and down. And amazingly, wonderfully, someone on Twitter said that they had one that they could
[02:26:25.920 --> 02:26:31.760]   they had never learned to use and said that they would send it to me free of charge. And so I got
[02:26:31.760 --> 02:26:37.200]   it. And about a week ago, I started learning and I'll hold it up here, how to use this this
[02:26:37.200 --> 02:26:46.240]   keyboard. And it is the most amazing thing. I love this thing. I can't quite
[02:26:46.240 --> 02:26:54.080]   it or so linear. It's not the swoopy doopy thing. That's air. The fact is that the the keys are
[02:26:54.080 --> 02:26:59.280]   like one on top of each other directly. If you look at your keyboard, they're they're slightly
[02:26:59.280 --> 02:27:04.800]   there. They're staggered. Yeah, they're they're in diagonals. So these are so they're straight up
[02:27:04.800 --> 02:27:09.920]   and down. It's like a grid. And so they say that's that's actually better for your fingers and it is
[02:27:09.920 --> 02:27:17.200]   more ergonomic. But but there is a learning curve. And though it was actually much quicker than I
[02:27:17.200 --> 02:27:22.000]   expected, like I heard people said it took them like two months of regular usage to get back up
[02:27:22.000 --> 02:27:27.840]   to their former typing speed. And for me, it was it was three days of 30 minutes a day,
[02:27:27.840 --> 02:27:34.640]   but I just sat there, like just tight, you know, I was doing typing tests for 30 minutes. And
[02:27:34.640 --> 02:27:40.320]   literally, I had a lot of trouble with the letter C and the letter B. And then, you know, I was
[02:27:40.320 --> 02:27:44.320]   always getting like the keys next to it. And then somewhere on that third. It's so weird,
[02:27:44.320 --> 02:27:49.120]   because it's just slightly off, right? Yeah. So it's if you're doing it, you'd probably learn the
[02:27:49.120 --> 02:27:55.760]   Dvorak layout faster than you would learn ortholinear because it's close, but no cigar. Is that it?
[02:27:55.760 --> 02:28:01.280]   Yeah. But but once yeah, but once you get it, it clicked. And I tried yesterday then to go back
[02:28:01.280 --> 02:28:06.640]   to my old regular staggered keyboard. And and at first I was then pressing the wrong buttons on
[02:28:06.640 --> 02:28:11.440]   that one. This one is particularly weird because it's also it's a it's a tiny keyboard. So like,
[02:28:11.440 --> 02:28:18.640]   there's no number row. And you have to learn to use like layers. And like the shift key is down on
[02:28:18.640 --> 02:28:27.760]   the bottom instead of over on the side. And and are you a touch typist? Yeah, I am. I am. And so it,
[02:28:27.760 --> 02:28:33.600]   but you know, I'm almost there. I'm almost as good with it. And it feels so much nicer to use.
[02:28:33.600 --> 02:28:39.040]   Like it's it's hard to explain. That's interesting. But I am now kind of obsessed with it.
[02:28:39.040 --> 02:28:49.280]   Wow. So there is a website, olkeyb.com where they sell ortholinear keyboards from Plank.
[02:28:49.280 --> 02:28:53.440]   There are a bunch. Yeah, there are many. Well, who makes that one?
[02:28:53.920 --> 02:28:58.160]   This one this one is a keyboard audio keyboard audio.
[02:28:58.160 --> 02:29:02.000]   So it's search up keyboard audio board with an IO with the end.
[02:29:02.000 --> 02:29:07.680]   Keyboard. Yeah. A keyboard for serious. Wait a minute. I want the wooden one. Look at that.
[02:29:07.680 --> 02:29:13.360]   Walnut closure. Have that fancy wooden one. That's pretty. Huh. So there's that's the one
[02:29:13.360 --> 02:29:20.480]   you got the atrias. That's the atrias system that I got. Huh. So maybe I'll try it. I
[02:29:21.520 --> 02:29:26.720]   that's interesting. There is a learning curve. Yeah. If you want me to to walk you through the
[02:29:26.720 --> 02:29:32.240]   learning curve, I have done the research. Oh, you have you have tips for people who are learning this.
[02:29:32.240 --> 02:29:38.240]   I mean, there's there's so much and you either a Mavis beacon for ortholinear keyboard.
[02:29:38.240 --> 02:29:45.360]   Different people have different different recommendations, honestly. But like if you open
[02:29:45.360 --> 02:29:49.840]   up my YouTube right now, it's like all keyboard like mechanical keyboard. That's great.
[02:29:49.840 --> 02:29:55.680]   I you have gone down some rabbit hole of the doom there. That's amazing. Very deep. Yeah.
[02:29:55.680 --> 02:30:01.760]   How interesting. I never heard of these and I like keyboards. I'm very interesting keyboards.
[02:30:01.760 --> 02:30:08.480]   Do you I mean, that's the one you got because some guy gave it to you. Someone sent it to me.
[02:30:08.480 --> 02:30:14.400]   But the one I'm really looking at is one called the moon lander. So if you do a search on moon
[02:30:14.400 --> 02:30:19.840]   lander keyboard, you'll see this is like the ultimate. This is like people. Lots of people.
[02:30:19.840 --> 02:30:25.360]   This is like their dream keyboard. But it is it is an ortholinear keyboard. It's also a split
[02:30:25.360 --> 02:30:30.160]   keyboard. It's also a bunch of other things. But that's like the ultimate.
[02:30:30.160 --> 02:30:32.160]   What the what?
[02:30:32.160 --> 02:30:39.680]   He almost made it one twig. What the what what that I didn't buy something?
[02:30:39.680 --> 02:30:43.440]   Yeah. Yeah. I almost made it through. You didn't buy something.
[02:30:43.440 --> 02:30:47.360]   Yeah. Came close. He was kind of by I thought I thought I'd talk about Bellprint there.
[02:30:47.360 --> 02:30:53.600]   Yeah. But now I'm thinking I want whatever the hell this is. This looks like it's alive.
[02:30:53.600 --> 02:30:59.840]   Dude, look at that. What the what? Well, but for those of us who spend a lot of time typing,
[02:30:59.840 --> 02:31:04.160]   my problem is if I did that, then a laptop would be impossible, right? I mean,
[02:31:04.160 --> 02:31:08.480]   because then you're going back and forth. So this this is this was my concern. This is why I
[02:31:08.480 --> 02:31:12.960]   raised it on Twitter was was how difficult this is to go back and forth. Yeah, because if I learn
[02:31:12.960 --> 02:31:18.240]   this, am I going to forget? And so people told me no, like people some it depends on who you talk
[02:31:18.240 --> 02:31:23.120]   to. Some people say it does become difficult to go back back and forth. And that's why like I've
[02:31:23.120 --> 02:31:28.560]   been going back and forth each day just to see if I can if I can keep it. And then, you know,
[02:31:28.560 --> 02:31:34.240]   the thing is honestly like with with with the this the atreus, the tiny one that I have, it's so
[02:31:34.240 --> 02:31:38.800]   small, I kind of feel like I could still like just bring it with my laptop if I go anywhere.
[02:31:38.800 --> 02:31:40.720]   What kind of switch is on the atreus?
[02:31:40.720 --> 02:31:46.480]   So let's get it to switches now, my friend.
[02:31:46.480 --> 02:31:55.760]   These aren't horribly expensive. I mean, 365 bucks.
[02:31:56.480 --> 02:32:05.040]   Well, I actually bought a old IBM Omni key north star, you know, buckling keyboard keyboard,
[02:32:05.040 --> 02:32:09.840]   because I remember that nice feeling. And that was about that expensive. It was a
[02:32:09.840 --> 02:32:15.520]   refurbished model. So that's not out of out of it's not crazy. You can spend a lot more on
[02:32:15.520 --> 02:32:21.600]   keyboard. Do you like clicky or not? I am not a fan of of clicky, but people who love clicky,
[02:32:21.600 --> 02:32:29.760]   love clicky. Yes. What made you want to explore this one? So what's what started is I sort of
[02:32:29.760 --> 02:32:37.280]   have spent a lot of time trying to optimize my my travel situation and my ability to work on
[02:32:37.280 --> 02:32:43.440]   from anywhere. And like with a portable screen and different stands and all sorts of stuff.
[02:32:43.440 --> 02:32:49.520]   And the one thing that I sort of felt held back by was the keyboard. So I was I was I have I have
[02:32:49.520 --> 02:32:55.920]   a travel ergonomic keyboard that is not a mechanical that is not that ortholinear. And so I was looking
[02:32:55.920 --> 02:33:00.320]   for something better and to see if there was something better than the one that I had before.
[02:33:00.320 --> 02:33:05.760]   And that just sent me down this path, this dangerous, dangerous path. Oh, it's such a dangerous path.
[02:33:05.760 --> 02:33:13.040]   He said travel. I thought he meant key travel. Yeah, I don't know. Sorry. Sorry. I mean,
[02:33:13.040 --> 02:33:16.160]   there is there is that is a whole debate as well. Oh, I know.
[02:33:18.000 --> 02:33:22.080]   Once you start going deep on like the different kinds of switches and how much force and where
[02:33:22.080 --> 02:33:30.880]   the activation is. Yeah. Yeah. I have I have now watched like 20 minute videos that are just
[02:33:30.880 --> 02:33:39.680]   people pressing pressing different key switches. Oh my God. So have you started to get sleep again?
[02:33:39.680 --> 02:33:46.880]   Oh my God. So are you saving up? I actually have where's it here? I have a yeah, I have actually
[02:33:46.880 --> 02:33:52.080]   but I have a oh yeah. Cherry will send you those. Yeah, yeah, we have others. Yeah, yeah. So you can
[02:33:52.080 --> 02:33:56.640]   you can test. Did you go with the cherries or those people are now saying oh no cherry. That's
[02:33:56.640 --> 02:34:02.640]   no. No, no. The ones the ones on this keyboard are the kale kale kale. Everybody's saying kale is
[02:34:02.640 --> 02:34:07.520]   the thing to do now. Like the kale ones are pretty good. It's kale. What kind of kale's
[02:34:07.520 --> 02:34:14.800]   just out of here? These are these are box browns. So it's a say tactile. The box is protected so that
[02:34:14.800 --> 02:34:20.640]   I could spill food and not have nice. But and then when you go to your laptop, you don't feel a
[02:34:20.640 --> 02:34:27.040]   disadvantage. You don't get weird. I mean, again, I just got this a week ago. So I haven't really
[02:34:27.040 --> 02:34:31.040]   done that much with it. But again, like it's so small. Like I really feel like I could I could
[02:34:31.040 --> 02:34:37.040]   just pack it with the laptop just take it. Yeah, use it everywhere. Yeah. Yeah. Yeah. Interesting.
[02:34:38.320 --> 02:34:44.720]   Okay. Okay. You know what that means? Jeff Jarvis's number is next. Jeff.
[02:34:44.720 --> 02:34:48.400]   Well, I was going to do something. I was going to tell you that all Salvador was going to go
[02:34:48.400 --> 02:34:53.360]   bankrupt because of Bitcoin. But instead, by the way, yeah, they tied their they made their currency
[02:34:53.360 --> 02:34:58.800]   the Bitcoin. Yeah. And now they're expected to default because it's dropped so far in price.
[02:34:58.800 --> 02:35:02.160]   That's terrible. Terrible.
[02:35:03.120 --> 02:35:08.480]   Okay. So I just put up a story. I think this is the case. So I think maybe from Glen Flaspen on
[02:35:08.480 --> 02:35:16.080]   Twitter, I just just put it in the chat. I think this guy's writing a book about keyboard history.
[02:35:16.080 --> 02:35:23.360]   How to geek the Korty keyboard is text biggest unsolved mystery by Ben J. Edwards.
[02:35:23.360 --> 02:35:31.920]   Or Benj. Interesting. Yeah, you know, once you start becoming a keyboard geek, it really is a
[02:35:31.920 --> 02:35:38.160]   bottomless pit of despair. I'm scared. I'm looking. Yeah. You're through the looking
[02:35:38.160 --> 02:35:46.240]   at a buddy boy. Jeff Jarvis is lobby at his school. The wonderful townite center for
[02:35:46.240 --> 02:35:49.200]   entrepreneurial journalism at the Craig Newmark Graduate School of Journalism at the City
[02:35:49.200 --> 02:35:54.880]   University of New York has a collection of typewriters under glass. Quite amazing. Yeah. Yes.
[02:35:54.880 --> 02:35:59.360]   Quite beautiful. My favorite is named the Bing. That's where Microsoft runs its entire search
[02:35:59.360 --> 02:36:06.240]   engine out of there. I see you have a machine that goes. So well. Okay. So that's one. But what else?
[02:36:06.240 --> 02:36:11.040]   I said, I'm done. I'm done. You're done? I got it. Okay. We did the Taco Bell's still life.
[02:36:11.040 --> 02:36:16.400]   I did Taco Bell. I had El Salvador and I did the keyboard history. Perfect. Perfect.
[02:36:16.400 --> 02:36:19.360]   And Pruitt. Pick of the week.
[02:36:19.360 --> 02:36:28.000]   For me is don't give up saw this story yesterday or day before and it's track and field related,
[02:36:28.000 --> 02:36:34.800]   but I love it. Little kids running track and there was an unfortunate event. If you click on it,
[02:36:34.800 --> 02:36:40.960]   there's a video. It was an unfortunate event, but the finish was outstanding. So she starts the
[02:36:40.960 --> 02:36:48.320]   race loses her shoe at the starting line, runs back, puts it on, then runs as fast as she can run
[02:36:48.320 --> 02:36:55.440]   her little legs pumping, going fast, fast, fast, and ends up winning the race. Yeah.
[02:36:55.440 --> 02:36:58.880]   That's kind of like the Kentucky Derby. That's a Kentucky Derby. Yeah.
[02:36:58.880 --> 02:37:06.080]   Wow. Here she comes. Come on, girl. And it's funny to hear her parents and her family in the
[02:37:06.080 --> 02:37:09.600]   background just sort of rooting her on. And they were like, wait a minute. She's going to win this
[02:37:09.600 --> 02:37:17.920]   thing. That's a terrible. That's never ever give up. And it wasn't even close.
[02:37:20.560 --> 02:37:27.200]   Oh, go girl. That is really, really cute. That is such a great story.
[02:37:27.200 --> 02:37:34.720]   That was great. Yeah. Don't give up. And I want to say congratulations to the squad and family.
[02:37:34.720 --> 02:37:44.240]   The hardheads, they won their four by 100 meter relay last week. What the hell aspect ratio is this?
[02:37:44.960 --> 02:37:51.440]   What are you using? No, that was from someone's phones. Oh, some weird phone. Yeah, it looks like
[02:37:51.440 --> 02:37:59.760]   a Polaroid. Okay. But they won last week. They have a decent time still. It's a relay race.
[02:37:59.760 --> 02:38:03.600]   Nice. Yep. But they got the championship for that. We're probably one hundred.
[02:38:03.600 --> 02:38:09.120]   Qualified for the next meet. Nice. And then hardhead. This is hardhead at Dank.
[02:38:09.120 --> 02:38:15.360]   Right here with the headband there. Yeah. Oh, look at him go. He's fast.
[02:38:15.360 --> 02:38:20.320]   They just opened that whole thing up. And he's just not even close. He could have lost his shoe.
[02:38:20.320 --> 02:38:25.440]   Yeah. He still would have won. He, holy. He and Lil Weirdo, they had, that was the
[02:38:25.440 --> 02:38:31.200]   NBA League championships last week. And Lil Weirdo, she qualified for the high hurdles.
[02:38:31.200 --> 02:38:36.800]   She got 10th place and qualified for the next meet. But she just learned how to do it, right?
[02:38:36.800 --> 02:38:43.120]   You just taught her how to do it. Yep. And she's in. She's in for a tough one.
[02:38:43.120 --> 02:38:49.280]   For sure. But I'm just so happy for her. And so she is inspired by the boys. Is she like,
[02:38:49.280 --> 02:38:54.240]   oh, if they can do it, I can do it. She's her own trust me. She's her own.
[02:38:54.240 --> 02:38:58.000]   Did she run track before she was in the atmosphere? I want to do something. I'm going to do it. Nice.
[02:38:58.000 --> 02:39:02.320]   No, she just said, I'll try it out because it looks interesting. That was pretty much it.
[02:39:02.320 --> 02:39:07.840]   Wow. And then shout out to the hard hit because he's now back to back League champion in the 100
[02:39:07.840 --> 02:39:12.720]   meter dash and got a nice feature in the local newspaper. And six and great pictures.
[02:39:12.720 --> 02:39:18.480]   That is a beautiful picture. That's great. Eric Castros. I love that picture. Mr. Eric Castros
[02:39:18.480 --> 02:39:26.000]   shot that. Nice shot. He's a local photographer. Why? NFT of that. If you look at, um, there's one
[02:39:26.000 --> 02:39:33.360]   on there. I think a slide number five and number six. You guys might find it interesting or maybe
[02:39:33.360 --> 02:39:41.680]   our listeners will know that one. Now, if you read read the caption, well, describe it and read
[02:39:41.680 --> 02:39:45.760]   the caption all year I've been worrying about personal records. So I had to write this on my
[02:39:45.760 --> 02:39:53.840]   hands at Jacob Pruitt 16 or Rancho Katari sophomore after winning the boys 100 meter race. It says,
[02:39:53.840 --> 02:40:00.640]   have fun. I love it. You wrote that in the poem this hand and I said on Twitter,
[02:40:00.640 --> 02:40:05.840]   it I can't take credit for that because that's not what you ever said in your whole life.
[02:40:05.840 --> 02:40:14.560]   Well, I mean, but like two days before that, I had to jump down his throat about his performance
[02:40:14.560 --> 02:40:19.200]   and track and his effort and stuff because he was playing what I call mine checkers. He was
[02:40:19.200 --> 02:40:24.160]   having so much muscle. You know, and I was like, dude, you're better than this. And I know you're
[02:40:24.160 --> 02:40:29.120]   better than this. And I expect you to continue to be better than this. So I had to do the proverbial
[02:40:29.120 --> 02:40:35.200]   foot in the ass is what I said. And he bounced back and went out to tough, but you're tough.
[02:40:35.200 --> 02:40:43.680]   We're on those boys. You're tough. Ants demanding for. Yeah, that's nice. Well, I guess he did have fun
[02:40:44.800 --> 02:40:51.040]   breaking the record. That's great. Well, thank you, everybody. Thank you so much. Mike Masnick,
[02:40:51.040 --> 02:40:57.520]   tech dirt com. Mike has a great podcast there too. Make sure you listen to that.
[02:40:57.520 --> 02:41:04.400]   He is, you know, well, and not only that, do what I do and belong to tech or support.
[02:41:04.400 --> 02:41:09.520]   Mike, he got rid of all his tracking. He got rid of all the ad stuff. Support tech dirty. It is
[02:41:09.520 --> 02:41:15.120]   the most important voice out there for digital sanity. Boy, I couldn't agree more. Absolutely.
[02:41:15.120 --> 02:41:20.480]   Also send him some ambient too, so he can sleep at night and stop looking at keyboards.
[02:41:20.480 --> 02:41:25.920]   It's those loud keyboard clicks keeping them up all night. It's very soothing. It's
[02:41:25.920 --> 02:41:32.320]   a form of meditation. I think my wife would throw me out of trouble. Oh, man. Get helps, sir.
[02:41:32.320 --> 02:41:37.520]   Thank you. So great. Always great to have you on, Mike. We really appreciate. Thanks for having me.
[02:41:37.520 --> 02:41:40.960]   And you know, you're on every week because we're always talking about articles from TechDoor.
[02:41:40.960 --> 02:41:47.440]   We're quoting you all the time. Yeah, pretty much. Really, really rock. Appreciate all you do.
[02:41:47.440 --> 02:41:50.080]   Thank you, Aunt Pruitt. Hands-on photography. Who's coming up?
[02:41:50.080 --> 02:41:58.000]   No guests this week. This is just a discussion that I get a lot from people regarding some of
[02:41:58.000 --> 02:42:02.560]   their hardware for photo editing and video editing. So I'm going to dive into that. But I still
[02:42:02.560 --> 02:42:10.080]   want to call back to last week's episode, episode 127, I believe, with Mr. Till Lloyd, the 49ers,
[02:42:10.080 --> 02:42:17.120]   team photographer. It's just really inspiring. So exciting. Yeah, really, really exciting.
[02:42:17.120 --> 02:42:27.440]   You checked it out. Highly recommended. Twit.tv/hop, episode 127. Professor Jeff Jarvis is the
[02:42:27.440 --> 02:42:32.160]   director of the Townite Center for Entrepreneurial Journalism at the Craig Newmark Graduate School.
[02:42:33.040 --> 02:42:40.320]   I'm sure I'm at the City University of New York. They just sound tired. The singers have been
[02:42:40.320 --> 02:42:47.920]   waiting two hours and 40 minutes. They're pretty bushed. They're pretty bushed. Thank you very much,
[02:42:47.920 --> 02:42:55.520]   Jeff, Buzzmachine.com. Thanks to all of you for joining us. We do twig 2PM Pacific 5PM Eastern.
[02:42:56.720 --> 02:43:03.840]   That's 2100 UTC. Every Wednesday you can watch live at live.twit.tv, chat live at IRC.twit.tv,
[02:43:03.840 --> 02:43:08.560]   or in our Discord, which is for our Club Twit members. If you want to join Club Twit for ad-free
[02:43:08.560 --> 02:43:16.960]   versions of all of our shows, go to twit.tv/clubtwit. You can also get the shows at the website twit.tv/twig
[02:43:16.960 --> 02:43:22.000]   after the fact or on the YouTube channel dedicated to twig or best thing to do,
[02:43:22.000 --> 02:43:25.520]   subscribe and your favorite podcast client. You'll get it automatically. The minute
[02:43:25.520 --> 02:43:32.080]   it's available. Thank you everybody for being here. Have a great week. We'll see you next time on twig.
[02:43:32.080 --> 02:43:33.840]   Bye-bye.
[02:43:33.840 --> 02:43:43.360]   Don't miss all about Android every week. We talk about the latest news, hardware, apps, and now
[02:43:43.360 --> 02:43:48.800]   all the developer-y goodness happening in the Android ecosystem. I'm Jason Howell, also joined by
[02:43:48.800 --> 02:43:54.800]   Ron Richards, Florence Ion, and our newest co-host on the panel, when to Dow, who brings her developer
[02:43:54.800 --> 02:44:00.240]   chops. Really great stuff. We also invite people from all over the Android ecosystem to talk about
[02:44:00.240 --> 02:44:12.080]   this mobile platform we love so much. Join us every Tuesday, all about Android, on twit.tv.
[02:44:12.080 --> 02:44:17.040]   [Music]

