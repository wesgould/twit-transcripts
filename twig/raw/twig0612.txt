;FFMETADATA1
title=Waymo Confused
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Kevin Marks
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2021-05-20
track=612
language=English
genre=Podcast
comment=Google IO keynote, problem with Google's AI demo, Project Starline, Material You
encoded_by=Uniblab 5.2
date=2021
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:02.200]   It's time for Twig this week in Google.
[00:00:02.200 --> 00:00:03.600]   Anne has the day off.
[00:00:03.600 --> 00:00:06.000]   He's got his second injection.
[00:00:06.000 --> 00:00:08.400]   He's not feeling too well, but Stacy's here.
[00:00:08.400 --> 00:00:09.200]   And Jeff's here.
[00:00:09.200 --> 00:00:10.000]   And guess what?
[00:00:10.000 --> 00:00:12.100]   A special appearance by Kevin Marks.
[00:00:12.100 --> 00:00:16.000]   We will talk about the Google I/O keynote.
[00:00:16.000 --> 00:00:18.600]   We've all got lots of thoughts about all the things
[00:00:18.600 --> 00:00:21.400]   Google announced and didn't announce.
[00:00:21.400 --> 00:00:25.100]   Kevin will talk a little bit about the problem with the AI
[00:00:25.100 --> 00:00:27.000]   demo that Google did.
[00:00:27.000 --> 00:00:29.200]   And it is a pretty big problem.
[00:00:29.200 --> 00:00:33.720]   And then a $40,000 bed.
[00:00:33.720 --> 00:00:38.500]   It's all coming up next plus a DVD screen saver from Google.
[00:00:38.500 --> 00:00:40.600]   It's all coming up next on Twig.
[00:00:40.600 --> 00:00:47.800]   Podcasts you love from people you trust.
[00:00:47.800 --> 00:00:49.200]   This is Twig.
[00:00:49.200 --> 00:00:55.700]   This is Twig.
[00:00:55.700 --> 00:00:58.800]   This week in Google, episode 612 recorded
[00:00:58.800 --> 00:01:02.000]   Wednesday, May 19, 2021.
[00:01:02.000 --> 00:01:05.240]   Way more confused.
[00:01:05.240 --> 00:01:08.200]   This week in Google is brought to you by Nureva.
[00:01:08.200 --> 00:01:11.240]   Getting your audio ready for meetings back in the office.
[00:01:11.240 --> 00:01:14.320]   Nureva audio is designed for distancing.
[00:01:14.320 --> 00:01:17.600]   It automatically adapts to new room configurations.
[00:01:17.600 --> 00:01:19.960]   So you're ready for the new normal.
[00:01:19.960 --> 00:01:21.560]   And whatever comes next.
[00:01:21.560 --> 00:01:25.960]   Learn more at nureva.com/twit.
[00:01:25.960 --> 00:01:28.480]   And by on Tuckett.
[00:01:28.480 --> 00:01:30.000]   Father's Day's coming up.
[00:01:30.000 --> 00:01:32.920]   And the perfect gift is an untucket shirt
[00:01:32.920 --> 00:01:35.320]   for the Father in your life use code TWIT
[00:01:35.320 --> 00:01:41.480]   for 20% off your first purchase at untuckett.com.
[00:01:41.480 --> 00:01:43.080]   It's time for Twig.
[00:01:43.080 --> 00:01:47.160]   This week in Google, Aunt Pruitt just got his sequel injection.
[00:01:47.160 --> 00:01:48.200]   So he's taking the day off.
[00:01:48.200 --> 00:01:52.880]   He's not feeling 100% bless his soul.
[00:01:52.880 --> 00:01:56.840]   But Jeff Jarvis is Stacy Egan-Botham and I are fully inoculated
[00:01:56.840 --> 00:01:58.400]   and feeling fine.
[00:01:58.400 --> 00:02:02.120]   Stacy Egan-Botham from StacyOnIOT.com.
[00:02:02.120 --> 00:02:03.040]   Her great newsletter there.
[00:02:03.040 --> 00:02:06.280]   You must subscribe in the IOT podcast with Kevin Tofel.
[00:02:06.280 --> 00:02:08.080]   Hello.
[00:02:08.080 --> 00:02:08.720]   Hello.
[00:02:08.720 --> 00:02:09.960]   Hello.
[00:02:09.960 --> 00:02:12.680]   I had a caller ask about Helio.
[00:02:12.680 --> 00:02:13.280]   Helium rather.
[00:02:13.280 --> 00:02:14.760]   We'll talk about that in a second.
[00:02:14.760 --> 00:02:17.440]   But I do want to introduce the other guy.
[00:02:17.440 --> 00:02:18.080]   What's his name?
[00:02:18.080 --> 00:02:20.000]   I got my moral panic shots.
[00:02:20.000 --> 00:02:23.120]   I am inoculated against moral panic.
[00:02:23.120 --> 00:02:24.640]   Oh, it's a bad time.
[00:02:24.640 --> 00:02:25.480]   Somebody did that.
[00:02:25.480 --> 00:02:28.160]   He's the director of the Townite Center
[00:02:28.160 --> 00:02:31.480]   for Entrepreneurial Journalism at the--
[00:02:31.480 --> 00:02:33.040]   Craig.
[00:02:33.040 --> 00:02:34.040]   Craig.
[00:02:34.040 --> 00:02:35.640]   Hey, Craig.
[00:02:35.640 --> 00:02:37.360]   You are a--
[00:02:37.360 --> 00:02:40.280]   School of Central-- the School of Graduate Journalism
[00:02:40.280 --> 00:02:41.000]   at the City Nuremberg.
[00:02:41.000 --> 00:02:42.360]   Where did that come from?
[00:02:42.360 --> 00:02:42.960]   Lob Opera.
[00:02:42.960 --> 00:02:43.800]   We were doing it a long time.
[00:02:43.800 --> 00:02:45.280]   What show did I miss that?
[00:02:45.280 --> 00:02:47.920]   You missed yesterday's fantastic--
[00:02:47.920 --> 00:02:48.440]   Oh.
[00:02:48.440 --> 00:02:49.600]   --Google I/O keynote.
[00:02:49.600 --> 00:02:51.040]   That's what you missed.
[00:02:51.040 --> 00:02:54.760]   And the music beforehand was actually better.
[00:02:54.760 --> 00:02:57.920]   And that's hard to say because it wasn't--
[00:02:57.920 --> 00:02:58.600]   I mean, it wasn't bad.
[00:02:58.600 --> 00:03:00.760]   I miss the keynote.
[00:03:00.760 --> 00:03:01.760]   Oh, you missed the lob opera.
[00:03:01.760 --> 00:03:03.320]   I just wasn't with y'all.
[00:03:03.320 --> 00:03:04.320]   Oh, OK.
[00:03:04.320 --> 00:03:06.960]   Did you see the blob opera stuff?
[00:03:06.960 --> 00:03:10.520]   Do we have some of that we can play?
[00:03:10.520 --> 00:03:14.640]   I guess blob opera has been updated to feature more music.
[00:03:14.640 --> 00:03:17.240]   And they had a performing group.
[00:03:17.240 --> 00:03:18.240]   Here it is.
[00:03:18.240 --> 00:03:23.520]   I have a two yards performing with the blob opera.
[00:03:23.520 --> 00:03:27.000]   And this was the pre-show.
[00:03:27.000 --> 00:03:28.800]   It's a machine learning experiment.
[00:03:28.800 --> 00:03:33.680]   And I guess that means they modified blob opera
[00:03:33.680 --> 00:03:38.400]   so that it would work with her singing.
[00:03:38.400 --> 00:03:40.920]   So there she is in her socks, rainbow socks,
[00:03:40.920 --> 00:03:42.000]   with the drums.
[00:03:42.000 --> 00:03:45.800]   She's got a basis to--
[00:03:45.800 --> 00:03:50.080]   Sorry, no metal.
[00:03:50.080 --> 00:03:52.200]   So I'm going to zip ahead.
[00:03:52.200 --> 00:03:53.400]   I'm going to move it way forward.
[00:03:53.400 --> 00:03:54.360]   Oh, no, I think I see them.
[00:03:54.360 --> 00:04:05.840]   And I just had a time when I apologized to the listeners
[00:04:05.840 --> 00:04:06.600]   at home for this.
[00:04:06.600 --> 00:04:12.280]   So this is a--
[00:04:12.280 --> 00:04:15.720]   [MUSIC PLAYING]
[00:04:15.720 --> 00:04:17.720]   [MUSIC PLAYING]
[00:04:17.720 --> 00:04:25.520]   OK, that's enough.
[00:04:25.520 --> 00:04:27.120]   That's enough, yeah.
[00:04:27.120 --> 00:04:27.720]   In which--
[00:04:27.720 --> 00:04:28.720]   More than ample.
[00:04:28.720 --> 00:04:30.360]   More than ample.
[00:04:30.360 --> 00:04:31.960]   Well, it was just the pre-show.
[00:04:31.960 --> 00:04:33.920]   So it's-- although I have to say--
[00:04:33.920 --> 00:04:34.720]   Yeah, I skipped that.
[00:04:34.720 --> 00:04:36.400]   Yeah, yeah, right.
[00:04:36.400 --> 00:04:36.880]   So--
[00:04:36.880 --> 00:04:40.320]   And you made-- I think we were all disappointed, Jeff.
[00:04:40.320 --> 00:04:42.800]   You won, I know, a new Chromebook.
[00:04:42.800 --> 00:04:44.600]   Well, even that aside.
[00:04:44.600 --> 00:04:46.800]   You didn't get a pony.
[00:04:46.800 --> 00:04:49.320]   There was no mention of any new products,
[00:04:49.320 --> 00:04:54.840]   except the return of WearOS, which puzzled everybody.
[00:04:54.840 --> 00:04:56.640]   Samsung has apparently decided to abandon
[00:04:56.640 --> 00:05:01.640]   Tizen, its own operating system, that had adopted for its watches
[00:05:01.640 --> 00:05:05.320]   and said, it's going to be the next big thing probably
[00:05:05.320 --> 00:05:08.520]   because they didn't want to be an Android house or an Android
[00:05:08.520 --> 00:05:09.000]   Wear house.
[00:05:09.000 --> 00:05:11.520]   And now they're going back to Wear.
[00:05:11.520 --> 00:05:14.240]   No, I thought, Wear and Tizen were going to become the same.
[00:05:14.240 --> 00:05:15.240]   No.
[00:05:15.240 --> 00:05:16.240]   Like, they were going to--
[00:05:16.240 --> 00:05:16.680]   No.
[00:05:16.680 --> 00:05:17.800]   No.
[00:05:17.800 --> 00:05:18.760]   I thought so too.
[00:05:18.760 --> 00:05:20.080]   It was unclear.
[00:05:20.080 --> 00:05:24.120]   But then I read some reviews and various things.
[00:05:24.120 --> 00:05:26.240]   And it apparently seems--
[00:05:26.240 --> 00:05:30.160]   well, let's go to the Verge, Chris Welch.
[00:05:30.160 --> 00:05:30.880]   Let's go to the tape.
[00:05:30.880 --> 00:05:33.680]   Let's go to the tape.
[00:05:33.680 --> 00:05:35.520]   It does say Google and Samsung will merge together
[00:05:35.520 --> 00:05:37.320]   their WearOS and Tizen platforms
[00:05:37.320 --> 00:05:40.960]   into a single operating system called Wear.
[00:05:40.960 --> 00:05:43.720]   But no, it's Wear.
[00:05:43.720 --> 00:05:47.280]   It's not-- I mean, what does that even mean?
[00:05:47.280 --> 00:05:51.560]   I've read this in several other places that basically it's Wear.
[00:05:51.560 --> 00:05:56.000]   It's not Tizen Wear or Tye Wear or Wear--
[00:05:56.000 --> 00:05:57.760]   Oh, we were calling it--
[00:05:57.760 --> 00:05:59.360]   let's see, what were we calling it?
[00:05:59.360 --> 00:06:00.360]   We had a few before.
[00:06:00.360 --> 00:06:02.160]   Tye Wear, Tye bit.
[00:06:02.160 --> 00:06:03.160]   Wear, Tye bit.
[00:06:03.160 --> 00:06:03.960]   Wear, Tye bit.
[00:06:03.960 --> 00:06:04.480]   That's what we--
[00:06:04.480 --> 00:06:05.920]   Yeah, because there is a fit in it.
[00:06:05.920 --> 00:06:07.520]   Yeah.
[00:06:07.520 --> 00:06:09.520]   So they say they're going to continue to support Tizen
[00:06:09.520 --> 00:06:14.720]   for at least three years after this new product launches.
[00:06:14.720 --> 00:06:16.880]   There will be some features from Samsung,
[00:06:16.880 --> 00:06:18.640]   like the watch face designer tool that
[00:06:18.640 --> 00:06:21.800]   will make the migration.
[00:06:21.800 --> 00:06:23.280]   According to the Verge, the unknowns
[00:06:23.280 --> 00:06:25.720]   is the underlying operating system closer to WearOS
[00:06:25.720 --> 00:06:27.880]   or closer to Tizen.
[00:06:27.880 --> 00:06:30.840]   I'm going to tell you right now, since it's running apps
[00:06:30.840 --> 00:06:35.960]   from the Play Store for WearOS, it's WearOS.
[00:06:35.960 --> 00:06:37.760]   Tizen was a very stripped down.
[00:06:37.760 --> 00:06:40.800]   I don't see this really as being anything with--
[00:06:40.800 --> 00:06:43.760]   Tizen wasn't for smart watches, though.
[00:06:43.760 --> 00:06:46.800]   It was basically Samsung's OS for everything.
[00:06:46.800 --> 00:06:49.560]   Yeah, they were going to replace Android with it.
[00:06:49.560 --> 00:06:50.240]   And they didn't.
[00:06:50.240 --> 00:06:51.640]   Well, Android, they were going to run it
[00:06:51.640 --> 00:06:54.400]   on their smart televisions, appliances.
[00:06:54.400 --> 00:06:56.480]   I think they realized, really, the big problem,
[00:06:56.480 --> 00:07:00.640]   at least for a watch, is if you don't have the apps,
[00:07:00.640 --> 00:07:01.640]   the watch isn't--
[00:07:01.640 --> 00:07:05.880]   it's limited to what Samsung ships with the watch.
[00:07:05.880 --> 00:07:07.640]   And it is the case probably--
[00:07:07.640 --> 00:07:08.400]   OK, well, then, we were wrong.
[00:07:08.400 --> 00:07:10.800]   Well, it's unclear, but I'm going to--
[00:07:10.800 --> 00:07:12.600]   well, we'll see when we-- they ship.
[00:07:12.600 --> 00:07:16.040]   I'm going to guess, given that it's running apps for WearOS,
[00:07:16.040 --> 00:07:17.640]   that it's WearOS.
[00:07:17.640 --> 00:07:19.200]   But WearOS sucks.
[00:07:19.200 --> 00:07:25.520]   Not to put too fine a point on it.
[00:07:25.520 --> 00:07:26.880]   Well, no, it really does.
[00:07:26.880 --> 00:07:29.560]   And Kevin and I, we talk about this a lot,
[00:07:29.560 --> 00:07:32.040]   because I've been a Fitbit user forever.
[00:07:32.040 --> 00:07:36.280]   But WearOS, we can blame the lack of hardware.
[00:07:36.280 --> 00:07:39.080]   Like, Qualcomm didn't update its silicon for a while.
[00:07:39.080 --> 00:07:40.240]   You absolutely can't wait for them.
[00:07:40.240 --> 00:07:40.680]   So maybe that's why.
[00:07:40.680 --> 00:07:42.640]   Yeah, you absolutely could.
[00:07:42.640 --> 00:07:43.880]   And maybe the advantage of Tizen--
[00:07:43.880 --> 00:07:44.240]   It's just really good.
[00:07:44.240 --> 00:07:47.240]   Because Tizen runs very well on the same--
[00:07:47.240 --> 00:07:50.360]   basically, the same kind of chips and Samsung Gear watches,
[00:07:50.360 --> 00:07:55.480]   it runs fine, because I feel like it's a smaller, simpler OS.
[00:07:55.480 --> 00:07:56.080]   But boy--
[00:07:56.080 --> 00:07:57.800]   Well, that's what I was kind of hopeful for.
[00:07:57.800 --> 00:07:58.560]   I was like--
[00:07:58.560 --> 00:08:02.400]   But if it's going to run WearOS apps--
[00:08:02.400 --> 00:08:04.720]   that's the chief, by the way, the chief purpose
[00:08:04.720 --> 00:08:09.680]   of an operating system is running the apps.
[00:08:09.680 --> 00:08:11.400]   I mean, it's not like there's a big file system.
[00:08:11.400 --> 00:08:12.200]   You can't wait with your hardware.
[00:08:12.200 --> 00:08:13.480]   Yeah, it's not like there's a big file system
[00:08:13.480 --> 00:08:15.680]   to manage or anything like that on your watch.
[00:08:15.680 --> 00:08:18.000]   I have to think that it's basically going to be
[00:08:18.000 --> 00:08:20.480]   where maybe Google--
[00:08:20.480 --> 00:08:21.640]   there's probably two things happening.
[00:08:21.640 --> 00:08:23.400]   Google is letting Samsung save face,
[00:08:23.400 --> 00:08:25.440]   because it is a little embarrassing.
[00:08:25.440 --> 00:08:30.080]   But also Google realizes people like Stacey hate WearOS.
[00:08:30.080 --> 00:08:32.480]   So by saying we're merging with Tizen,
[00:08:32.480 --> 00:08:34.280]   they're maybe giving people hope.
[00:08:34.280 --> 00:08:37.080]   And you're right, Fitbit was a big part of this story, wasn't it?
[00:08:37.080 --> 00:08:40.160]   Because Google is now officially owns Fitbit.
[00:08:40.160 --> 00:08:43.400]   And this is kind of the debut of whatever it is they're
[00:08:43.400 --> 00:08:46.400]   going to do with Fitbit.
[00:08:46.400 --> 00:08:48.240]   It's terrible debut if you don't know--
[00:08:48.240 --> 00:08:49.760]   we're all like, what does this mean?
[00:08:49.760 --> 00:08:50.240]   What does this mean?
[00:08:50.240 --> 00:08:50.680]   What is it?
[00:08:50.680 --> 00:08:51.240]   What's happening?
[00:08:51.240 --> 00:08:53.360]   Well, it's Google communication.
[00:08:53.360 --> 00:08:56.960]   It's not really because they didn't say we'll be shipping
[00:08:56.960 --> 00:08:59.320]   a watch with this new tie-os--
[00:08:59.320 --> 00:08:59.800]   Right.
[00:08:59.800 --> 00:09:01.400]   --tie-wear.
[00:09:01.400 --> 00:09:02.360]   Tie-wear bit?
[00:09:02.360 --> 00:09:04.360]   Tie-wear-side-bit.
[00:09:04.360 --> 00:09:06.400]   In March or something, they didn't say anything.
[00:09:06.400 --> 00:09:07.440]   They didn't say anything.
[00:09:07.440 --> 00:09:10.480]   They literally-- they didn't even mention a Pixel phone
[00:09:10.480 --> 00:09:13.000]   or a Chrome OS.
[00:09:13.000 --> 00:09:15.960]   It was mostly-- it was demos of products that
[00:09:15.960 --> 00:09:18.400]   aren't even out of the lab.
[00:09:18.400 --> 00:09:18.960]   Exactly.
[00:09:18.960 --> 00:09:19.360]   Exactly.
[00:09:19.360 --> 00:09:20.040]   It wasn't even--
[00:09:20.040 --> 00:09:21.080]   Well--
[00:09:21.080 --> 00:09:23.320]   Were you disappointed that they didn't mention chip?
[00:09:23.320 --> 00:09:24.200]   Or whatever it's called?
[00:09:24.200 --> 00:09:26.440]   No, because that's all part of it.
[00:09:26.440 --> 00:09:28.360]   They have a separate smart home keynote that's
[00:09:28.360 --> 00:09:31.560]   going to happen at 4.15 today.
[00:09:31.560 --> 00:09:33.000]   Well, shouldn't-- if they're so proud of us,
[00:09:33.000 --> 00:09:36.320]   shouldn't they mention it in the keynote?
[00:09:36.320 --> 00:09:37.800]   You know, Google's so big.
[00:09:37.800 --> 00:09:41.080]   I felt like this keynote was more like trying
[00:09:41.080 --> 00:09:45.760]   to explain how Google has been and will be continuing to use
[00:09:45.760 --> 00:09:47.440]   AI to make--
[00:09:47.440 --> 00:09:49.080]   There was a lot of that.
[00:09:49.080 --> 00:09:50.200]   It better.
[00:09:50.200 --> 00:09:54.600]   So like, you know, the photography, the--
[00:09:54.600 --> 00:09:58.880]   like, taking people out of photographs, the TPU chat,
[00:09:58.880 --> 00:10:00.880]   the--
[00:10:00.880 --> 00:10:03.480]   Did you watch the developer keynote afterwards?
[00:10:03.480 --> 00:10:04.680]   I was busy, unfortunately.
[00:10:04.680 --> 00:10:05.840]   I couldn't.
[00:10:05.840 --> 00:10:06.520]   Same here.
[00:10:06.520 --> 00:10:08.040]   I watched a bit of the developer keynote.
[00:10:08.040 --> 00:10:10.160]   I had an event yesterday.
[00:10:10.160 --> 00:10:11.720]   So I was having a hard time.
[00:10:11.720 --> 00:10:12.760]   Yeah.
[00:10:12.760 --> 00:10:14.840]   Some people in our chat room who did watch it said,
[00:10:14.840 --> 00:10:17.040]   yeah, he didn't miss anything.
[00:10:17.040 --> 00:10:18.600]   Oh, no, really.
[00:10:18.600 --> 00:10:19.720]   It is possible.
[00:10:19.720 --> 00:10:23.480]   And I suspect this is true that the--
[00:10:23.480 --> 00:10:25.760]   This is more of a traditional developer keynote
[00:10:25.760 --> 00:10:28.200]   and that a lot of what we have questions about
[00:10:28.200 --> 00:10:32.760]   will be revealed in subsequent tracks later in Google I/O.
[00:10:32.760 --> 00:10:34.480]   Like, well, is there a world-west track?
[00:10:34.480 --> 00:10:36.320]   People don't tend to do that.
[00:10:36.320 --> 00:10:38.400]   Well, they've got--
[00:10:38.400 --> 00:10:40.120]   I got excited about the Flutter track,
[00:10:40.120 --> 00:10:42.840]   but that was just me.
[00:10:42.840 --> 00:10:43.400]   Well, you know what?
[00:10:43.400 --> 00:10:45.760]   You and Paul Thorett are both excited about Flutter.
[00:10:48.440 --> 00:10:51.560]   The other thing that longtime viewers of the show
[00:10:51.560 --> 00:10:53.960]   will have noticed is--
[00:10:53.960 --> 00:10:55.320]   now, you may remember, before you,
[00:10:55.320 --> 00:10:56.880]   there was a woman named Gina Trapani,
[00:10:56.880 --> 00:11:01.520]   we much beloved developer, who wrote a book about a Google
[00:11:01.520 --> 00:11:04.640]   product, Google Wave, that was killed almost the same time
[00:11:04.640 --> 00:11:05.760]   the book came out.
[00:11:05.760 --> 00:11:06.800]   Yeah, porche.
[00:11:06.800 --> 00:11:08.880]   Porche.
[00:11:08.880 --> 00:11:09.960]   But Google Wave--
[00:11:09.960 --> 00:11:12.280]   She was the expert in something dead.
[00:11:12.280 --> 00:11:13.400]   Google Wave, but you know what?
[00:11:13.400 --> 00:11:15.000]   We were all really excited about Google Wave.
[00:11:15.000 --> 00:11:15.360]   Oh, we were.
[00:11:15.360 --> 00:11:17.880]   The idea that a document wasn't just
[00:11:17.880 --> 00:11:21.360]   this monolithic word processing or spreadsheet or slide
[00:11:21.360 --> 00:11:24.160]   document, a document was a container for a variety
[00:11:24.160 --> 00:11:25.600]   of data types.
[00:11:25.600 --> 00:11:26.720]   And living.
[00:11:26.720 --> 00:11:27.560]   And then it was--
[00:11:27.560 --> 00:11:28.480]   And it was collaborative.
[00:11:28.480 --> 00:11:28.880]   It was collaborative.
[00:11:28.880 --> 00:11:30.240]   It was collaborative.
[00:11:30.240 --> 00:11:31.000]   Yeah.
[00:11:31.000 --> 00:11:31.520]   Yeah.
[00:11:31.520 --> 00:11:35.440]   So Google announced-- basically, as far as I'm concerned,
[00:11:35.440 --> 00:11:39.360]   and we still haven't seen all the details, the return of Wave
[00:11:39.360 --> 00:11:43.560]   with a new technology for Google Workspace.
[00:11:43.560 --> 00:11:45.160]   And you know, I'll pull up the blog.
[00:11:45.160 --> 00:11:46.240]   You're calling Canvas.
[00:11:46.240 --> 00:11:46.600]   Can't--
[00:11:46.600 --> 00:11:49.960]   Not to be confused with Canvas, the rendering mechanism.
[00:11:49.960 --> 00:11:52.560]   But now it's kind of branded Canvas.
[00:11:52.560 --> 00:12:00.080]   So you can @ mention people in it, which is interesting.
[00:12:00.080 --> 00:12:02.240]   We'll add people.
[00:12:02.240 --> 00:12:06.360]   Docs no longer have pages.
[00:12:06.360 --> 00:12:09.160]   You can put emoji reactions, inclusive language
[00:12:09.160 --> 00:12:12.840]   recommendations, smarter meeting notes, connected checklist,
[00:12:12.840 --> 00:12:15.600]   connected means, by the way, that you
[00:12:15.600 --> 00:12:18.040]   can assign checklists, list items to other people
[00:12:18.040 --> 00:12:22.920]   and see them in Google Tasks, table templates, which
[00:12:22.920 --> 00:12:25.840]   include voting.
[00:12:25.840 --> 00:12:30.040]   Your meet will show up directly within your document.
[00:12:30.040 --> 00:12:33.440]   So rather than embedding your document in your video meeting,
[00:12:33.440 --> 00:12:35.000]   you know, embed your video meeting.
[00:12:35.000 --> 00:12:35.600]   Set a screen.
[00:12:35.600 --> 00:12:36.080]   You're not sharing.
[00:12:36.080 --> 00:12:36.600]   Yeah.
[00:12:36.600 --> 00:12:38.920]   You're actually in the document.
[00:12:38.920 --> 00:12:41.200]   Live captioning and live translations,
[00:12:41.200 --> 00:12:42.520]   Google does that very well.
[00:12:42.520 --> 00:12:46.440]   That'll be built in in five languages-- Spanish, Portuguese,
[00:12:46.440 --> 00:12:49.880]   French, German, and of course English.
[00:12:49.880 --> 00:12:51.960]   There'll be a timeline view in sheets.
[00:12:51.960 --> 00:12:56.040]   So as I remember, you're the queen of the percharts
[00:12:56.040 --> 00:12:57.040]   and the percharts.
[00:12:57.040 --> 00:12:58.040]   The percharts?
[00:12:58.040 --> 00:12:58.520]   The percharts.
[00:12:58.520 --> 00:13:02.120]   So you'll like that.
[00:13:02.120 --> 00:13:04.840]   It's kind of-- you know, this seems very akin
[00:13:04.840 --> 00:13:08.960]   to what Trello was doing with their strategy.
[00:13:08.960 --> 00:13:12.320]   Or is it Atlassian that owns Trello?
[00:13:12.320 --> 00:13:14.640]   Atlassian was doing with Trello and tying,
[00:13:14.640 --> 00:13:18.080]   having that become a way to pull in all the information
[00:13:18.080 --> 00:13:20.400]   you're working with into one place so you can work on it.
[00:13:20.400 --> 00:13:21.320]   So that's kind of--
[00:13:21.320 --> 00:13:24.120]   And we went down a little trip down memory lane,
[00:13:24.120 --> 00:13:29.240]   Paul Therat and I, because Trello isn't the most recent version
[00:13:29.240 --> 00:13:29.600]   of this.
[00:13:29.600 --> 00:13:31.160]   But this has been something we've been--
[00:13:31.160 --> 00:13:33.440]   companies have been trying since the '90s.
[00:13:33.440 --> 00:13:34.400]   Oh, yeah.
[00:13:34.400 --> 00:13:37.760]   Apple had a plan to do something with IBM that--
[00:13:37.760 --> 00:13:42.400]   the idea was instead of having document-centric workflow--
[00:13:42.400 --> 00:13:43.760]   well, instead of having--
[00:13:43.760 --> 00:13:45.760]   let's see, how did they put it?
[00:13:45.760 --> 00:13:48.440]   Microsoft had Olay, object linking and embedding.
[00:13:48.440 --> 00:13:50.880]   That was in Windows 95.
[00:13:50.880 --> 00:13:55.520]   The idea was instead of being file-based workflow--
[00:13:55.520 --> 00:13:58.360]   I guess is how you'd say it-- it would be document-based workflow.
[00:13:58.360 --> 00:13:59.720]   Instead of having an operating system,
[00:13:59.720 --> 00:14:02.360]   it's all in the document.
[00:14:02.360 --> 00:14:04.680]   This is an ancient idea.
[00:14:04.680 --> 00:14:10.200]   Jeff Raskin, who was the spiritual father of the Macintosh
[00:14:10.200 --> 00:14:12.480]   after he got kicked out of Apple,
[00:14:12.480 --> 00:14:14.920]   started doing an operating system that was completely
[00:14:14.920 --> 00:14:18.000]   just all a document.
[00:14:18.000 --> 00:14:21.880]   There was-- remember, Claris works with an early Apple
[00:14:21.880 --> 00:14:24.360]   company that was an office.
[00:14:24.360 --> 00:14:26.400]   But instead of having a word processor
[00:14:26.400 --> 00:14:29.160]   separate from a spreadsheet, you would embed those tools
[00:14:29.160 --> 00:14:31.320]   in whatever document you were working in.
[00:14:31.320 --> 00:14:33.080]   I mean, I can go on and on.
[00:14:33.080 --> 00:14:34.640]   That's enough examples.
[00:14:34.640 --> 00:14:36.160]   It's been tried many times.
[00:14:36.160 --> 00:14:38.320]   And it's always failed, I should point out.
[00:14:38.320 --> 00:14:41.240]   I don't know if that's because humans-- it's too complicated.
[00:14:41.240 --> 00:14:42.720]   I don't know why.
[00:14:42.720 --> 00:14:46.280]   But maybe the time has come.
[00:14:46.280 --> 00:14:46.920]   Maybe--
[00:14:46.920 --> 00:14:49.120]   Because when you're trying to--
[00:14:49.120 --> 00:14:49.600]   Go ahead.
[00:14:49.600 --> 00:14:50.120]   Sorry.
[00:14:50.120 --> 00:14:52.040]   I'm thinking about sociological reasons
[00:14:52.040 --> 00:14:53.440]   that wouldn't work based on how we
[00:14:53.440 --> 00:14:55.400]   tend to work at the type of work we do.
[00:14:55.400 --> 00:14:56.880]   But I am with you, yes.
[00:14:56.880 --> 00:14:57.400]   For some reason--
[00:14:57.400 --> 00:14:59.040]   Say, what I noted yesterday is that this
[00:14:59.040 --> 00:15:03.160]   has got to be a window on how they work at Google.
[00:15:03.160 --> 00:15:04.080]   That is true.
[00:15:04.080 --> 00:15:04.920]   Or would like to.
[00:15:04.920 --> 00:15:05.680]   And that makes--
[00:15:05.680 --> 00:15:06.640]   Or would like to.
[00:15:06.640 --> 00:15:08.040]   Well, see, that's the question.
[00:15:08.040 --> 00:15:11.880]   Is this coming from users saying, we want to do this,
[00:15:11.880 --> 00:15:14.000]   or we are doing this, and we'd like a better way to do it?
[00:15:14.000 --> 00:15:17.240]   Or is it more likely coming from operating system
[00:15:17.240 --> 00:15:18.520]   philosophers?
[00:15:18.520 --> 00:15:19.520]   We're saying--
[00:15:19.520 --> 00:15:20.880]   I don't think so.
[00:15:20.880 --> 00:15:23.920]   No, I do think we're hitting a point where we're trying
[00:15:23.920 --> 00:15:25.360]   to work more collaboratively.
[00:15:25.360 --> 00:15:28.800]   The tools have gotten good enough.
[00:15:28.800 --> 00:15:32.440]   Think about 10 years ago, if you were typing at a Google Doc,
[00:15:32.440 --> 00:15:35.200]   you might feel uncomfortable because someone's watching you
[00:15:35.200 --> 00:15:36.640]   type, right?
[00:15:36.640 --> 00:15:39.960]   Now, it's inane to think that you would be--
[00:15:39.960 --> 00:15:41.680]   if you're collaborating, not like if you're writing an
[00:15:41.680 --> 00:15:44.160]   article, but if you're collaborating with someone,
[00:15:44.160 --> 00:15:47.720]   it's crazy to pass a document back and forth.
[00:15:47.720 --> 00:15:52.600]   And so I feel like it's both a chicken and egg problem,
[00:15:52.600 --> 00:15:53.560]   like the--
[00:15:53.560 --> 00:15:54.680]   it's a process problem.
[00:15:54.680 --> 00:15:56.480]   People have to get acclimated to this.
[00:15:56.480 --> 00:15:58.520]   And I think we're getting there because we're used to
[00:15:58.520 --> 00:16:01.680]   collaborating, and we're having to pull in information and
[00:16:01.680 --> 00:16:04.560]   things from everywhere.
[00:16:04.560 --> 00:16:07.280]   Writing is no longer the way you communicate an idea.
[00:16:07.280 --> 00:16:09.800]   Think about video, think about tweets.
[00:16:09.800 --> 00:16:12.800]   I mean, you may hate them.
[00:16:12.800 --> 00:16:16.600]   So I think we're not 100% there yet, but I look at the
[00:16:16.600 --> 00:16:20.280]   type of presentations my daughter does, and they're very
[00:16:20.280 --> 00:16:22.880]   different from things that I used to do.
[00:16:22.880 --> 00:16:25.360]   And I don't know.
[00:16:25.360 --> 00:16:29.560]   Name a few things, or Stacy, what kinds of things does she
[00:16:29.560 --> 00:16:31.520]   do and do they like to do?
[00:16:31.520 --> 00:16:35.720]   So she still writes research papers, but when she's
[00:16:35.720 --> 00:16:38.400]   presenting a paper, she automatically does
[00:16:38.400 --> 00:16:39.960]   something in Prezi, right?
[00:16:39.960 --> 00:16:40.560]   And she's--
[00:16:40.560 --> 00:16:42.120]   Prezi's actually another way.
[00:16:42.120 --> 00:16:44.520]   That's kind of what Prezi is, isn't it?
[00:16:44.520 --> 00:16:45.000]   Yeah.
[00:16:45.000 --> 00:16:46.840]   And she's very adept at--
[00:16:46.840 --> 00:16:48.920]   It's a different path, is there, that makes a difference.
[00:16:48.920 --> 00:16:49.520]   Yeah.
[00:16:49.520 --> 00:16:51.320]   But you can have been a lot of different things into
[00:16:51.320 --> 00:16:52.280]   Prezi, too.
[00:16:52.280 --> 00:16:53.760]   You zoom around.
[00:16:53.760 --> 00:16:57.560]   So Prezi, it's a presentation tool that is very--
[00:16:57.560 --> 00:16:58.760]   it's like the hippie PowerPoint.
[00:16:58.760 --> 00:16:59.160]   But it's--
[00:16:59.160 --> 00:17:00.160]   It gives you motion sickness.
[00:17:00.160 --> 00:17:02.400]   But it's not just a presentation tool.
[00:17:02.400 --> 00:17:05.480]   She uses it not just to present, but she also, when she's
[00:17:05.480 --> 00:17:08.120]   collaborating with people, they have like flowchart software
[00:17:08.120 --> 00:17:09.880]   that they'll use to share and--
[00:17:09.880 --> 00:17:12.560]   It's an organizational tool, isn't it?
[00:17:12.560 --> 00:17:13.520]   It's--
[00:17:13.520 --> 00:17:14.040]   I love Prezi.
[00:17:14.040 --> 00:17:15.840]   It's just a different way of working.
[00:17:15.840 --> 00:17:17.200]   Yeah, I love it.
[00:17:17.200 --> 00:17:18.480]   Because I don't work that way.
[00:17:18.480 --> 00:17:22.160]   So I'm like, I just write a memo or send an email.
[00:17:22.160 --> 00:17:23.680]   You're linear.
[00:17:23.680 --> 00:17:24.880]   That might be the problem.
[00:17:24.880 --> 00:17:25.320]   Right?
[00:17:25.320 --> 00:17:26.360]   But that might be the problem.
[00:17:26.360 --> 00:17:28.560]   As a human's archinal linear.
[00:17:28.560 --> 00:17:30.720]   By the way, have you been dying for a while?
[00:17:30.720 --> 00:17:31.200]   I agree.
[00:17:31.200 --> 00:17:32.240]   No, I don't think so either.
[00:17:32.240 --> 00:17:33.200]   I think Stacy's right here.
[00:17:33.200 --> 00:17:33.760]   No.
[00:17:33.760 --> 00:17:34.280]   OK.
[00:17:34.280 --> 00:17:36.440]   Maybe you are a boring old man, but--
[00:17:36.440 --> 00:17:37.880]   No, Stacy just said she was.
[00:17:37.880 --> 00:17:41.000]   I think our work product is historic.
[00:17:41.000 --> 00:17:43.240]   Like writing an article is linear, right?
[00:17:43.240 --> 00:17:43.760]   Right.
[00:17:43.760 --> 00:17:44.960]   Because the typewriter demanded.
[00:17:44.960 --> 00:17:46.080]   Because the typewriter demanded.
[00:17:46.080 --> 00:17:48.400]   Yeah, but when you're collaborating to build software,
[00:17:48.400 --> 00:17:50.400]   you don't actually have to be linear.
[00:17:50.400 --> 00:17:50.920]   Right.
[00:17:50.920 --> 00:17:52.520]   That's what's changed is that we--
[00:17:52.520 --> 00:17:53.560]   And that's what this is for.
[00:17:53.560 --> 00:17:56.080]   This is for people working in teams, not you and me
[00:17:56.080 --> 00:17:57.920]   working solo.
[00:17:57.920 --> 00:17:59.720]   By the way, if you've always wanted
[00:17:59.720 --> 00:18:02.320]   to create a spreadsheet from within Google Chat,
[00:18:02.320 --> 00:18:04.880]   that is also going to be possible.
[00:18:04.880 --> 00:18:05.880]   So Kevin and I--
[00:18:05.880 --> 00:18:08.440]   I mean, because what we--
[00:18:08.440 --> 00:18:10.440]   What a fun, chatter that is.
[00:18:10.440 --> 00:18:11.480]   You know what I've got an idea?
[00:18:11.480 --> 00:18:13.520]   Let's create a spreadsheet.
[00:18:13.520 --> 00:18:15.920]   We--
[00:18:15.920 --> 00:18:17.760]   Think about how you pull together the show.
[00:18:17.760 --> 00:18:19.080]   That's a collaboration.
[00:18:19.080 --> 00:18:20.520]   Like right now, we're sending--
[00:18:20.520 --> 00:18:21.000]   That's true.
[00:18:21.000 --> 00:18:22.040]   Think about it.
[00:18:22.040 --> 00:18:22.880]   We could be--
[00:18:22.880 --> 00:18:25.880]   The show could be in the rundown.
[00:18:25.880 --> 00:18:26.880]   Yeah.
[00:18:26.880 --> 00:18:27.800]   Well, OK.
[00:18:27.800 --> 00:18:28.360]   Yes, that.
[00:18:28.360 --> 00:18:28.680]   But also--
[00:18:28.680 --> 00:18:32.480]   But Jason's had just exploded.
[00:18:32.480 --> 00:18:33.800]   No, Jason is excited.
[00:18:33.800 --> 00:18:34.400]   I'm going to put some--
[00:18:34.400 --> 00:18:34.920]   What does it--
[00:18:34.920 --> 00:18:36.560]   You could pull this in the end rundown?
[00:18:36.560 --> 00:18:38.000]   [LAUGHTER]
[00:18:38.000 --> 00:18:39.960]   Put a little bubble of me in the rundown.
[00:18:39.960 --> 00:18:42.520]   [LAUGHTER]
[00:18:42.520 --> 00:18:46.000]   I'm sorry, Jason.
[00:18:46.000 --> 00:18:49.600]   I think it is possibly a better way to work.
[00:18:49.600 --> 00:18:54.320]   I think it's culturally difficult.
[00:18:54.320 --> 00:18:56.640]   Because we are used to linear.
[00:18:56.640 --> 00:18:58.400]   And maybe it was just because we had typewriters.
[00:18:58.400 --> 00:18:59.960]   That's all we could do.
[00:18:59.960 --> 00:19:01.480]   There's also tool fatigue.
[00:19:01.480 --> 00:19:04.520]   Well, when you think about it, Leo, the typewriter--
[00:19:04.520 --> 00:19:08.520]   So the typewriter came out in about 1880.
[00:19:08.520 --> 00:19:10.640]   I'm working on this writing research right now.
[00:19:10.640 --> 00:19:11.320]   And that was the first--
[00:19:11.320 --> 00:19:12.920]   I love that you know this.
[00:19:12.920 --> 00:19:14.600]   --of a keyboard.
[00:19:14.600 --> 00:19:14.960]   Wow.
[00:19:14.960 --> 00:19:16.880]   And so the extension-- very McLuhan, right?
[00:19:16.880 --> 00:19:20.480]   The extension of the brain to the fingers,
[00:19:20.480 --> 00:19:22.240]   to the keyboard, to the page.
[00:19:22.240 --> 00:19:24.760]   And it changed our expectation for writing.
[00:19:24.760 --> 00:19:26.720]   Because now writing mimicked publishing.
[00:19:26.720 --> 00:19:28.200]   It looked finished.
[00:19:28.200 --> 00:19:29.960]   It changed the whole nature.
[00:19:29.960 --> 00:19:32.680]   And you couldn't-- you know, scratching out was a no.
[00:19:32.680 --> 00:19:35.240]   No, you had to have this clean thing, this clean,
[00:19:35.240 --> 00:19:37.240]   continuous thing, page after page after page.
[00:19:37.240 --> 00:19:39.920]   So it had to-- I'm fact I'm reading a book somewhere here
[00:19:39.920 --> 00:19:44.400]   on how there was a typewriter century.
[00:19:44.400 --> 00:19:45.960]   And that it affected everybody's writing.
[00:19:45.960 --> 00:19:48.320]   And now we're breaking out of that, right?
[00:19:48.320 --> 00:19:53.440]   So now Stacey's daughter is the vanguard here
[00:19:53.440 --> 00:19:55.600]   of a different way to think about thoughts.
[00:19:55.600 --> 00:19:57.960]   Well, and I think also that in many businesses,
[00:19:57.960 --> 00:19:59.560]   there are teams--
[00:19:59.560 --> 00:20:02.320]   there are programming teams and there are other teams
[00:20:02.320 --> 00:20:05.000]   that are already collaborating in unique ways.
[00:20:05.000 --> 00:20:07.040]   You know, that's-- well, people hated it,
[00:20:07.040 --> 00:20:09.320]   but that's what the idea of the open office was,
[00:20:09.320 --> 00:20:10.840]   that you could go, hey, Joe, are you
[00:20:10.840 --> 00:20:12.200]   working on that login screen?
[00:20:12.200 --> 00:20:14.360]   I'm having some trouble over here,
[00:20:14.360 --> 00:20:17.080]   but that kind of thing that you could kind of collaborate that way.
[00:20:17.080 --> 00:20:21.080]   I'm going to open this up to our Discord stage.
[00:20:21.080 --> 00:20:23.400]   For anybody who is currently working that way
[00:20:23.400 --> 00:20:25.760]   or has any thoughts about working this way,
[00:20:25.760 --> 00:20:30.000]   we were probably the-- we're all solo operators, right?
[00:20:30.000 --> 00:20:33.920]   So maybe you'd be better if we got some people away.
[00:20:33.920 --> 00:20:34.920]   Wait, where's our stage?
[00:20:34.920 --> 00:20:36.200]   No, it's a few slides.
[00:20:36.200 --> 00:20:38.480]   It's the East Side Live it says, over in the live.
[00:20:38.480 --> 00:20:39.480]   OK, right, right, right.
[00:20:39.480 --> 00:20:39.920]   Under our IP.
[00:20:39.920 --> 00:20:41.440]   Yes, yes, yes, yes, yes.
[00:20:41.440 --> 00:20:42.280]   There's--
[00:20:42.280 --> 00:20:43.840]   Well, we're doing that.
[00:20:43.840 --> 00:20:45.560]   So I just got this book, too.
[00:20:45.560 --> 00:20:47.160]   The Filing Cabinet.
[00:20:47.160 --> 00:20:49.040]   A Vertical History of Information.
[00:20:49.040 --> 00:20:50.600]   Oh, Jesus.
[00:20:50.600 --> 00:20:54.720]   But if you think about that, the file system on your computer
[00:20:54.720 --> 00:20:56.840]   is a derivative of that.
[00:20:56.840 --> 00:20:57.320]   Yes.
[00:20:57.320 --> 00:20:58.640]   Even the icons, right?
[00:20:58.640 --> 00:20:58.920]   Yeah.
[00:20:58.920 --> 00:21:02.240]   So this is how we thought for not that law.
[00:21:02.240 --> 00:21:05.880]   And only because the physical world was organized that way,
[00:21:05.880 --> 00:21:09.360]   because probably it was easy to manufacture or something.
[00:21:09.360 --> 00:21:13.000]   Oh, there's tons of articles about how
[00:21:13.000 --> 00:21:15.680]   the way we think about our information storage
[00:21:15.680 --> 00:21:18.160]   and retrieval systems is actually
[00:21:18.160 --> 00:21:19.880]   modeled on how we model the brain.
[00:21:19.880 --> 00:21:22.480]   So it has physical correlation to how we think about it.
[00:21:22.480 --> 00:21:23.280]   You're kidding.
[00:21:23.280 --> 00:21:23.840]   Well, then that's--
[00:21:23.840 --> 00:21:24.600]   No, there's--
[00:21:24.600 --> 00:21:26.680]   --bouleur's poorly for this.
[00:21:26.680 --> 00:21:29.000]   Well, right now, we're all in a neuromorphic computing.
[00:21:29.000 --> 00:21:30.800]   But before that, we had different--
[00:21:30.800 --> 00:21:32.160]   What?
[00:21:32.160 --> 00:21:34.160]   I can find the article, I guess, in Cenetius,
[00:21:34.160 --> 00:21:35.280]   because it's really fascinating.
[00:21:35.280 --> 00:21:36.760]   That's the fundamental question, though.
[00:21:36.760 --> 00:21:39.720]   And in a way, are we working that way
[00:21:39.720 --> 00:21:42.320]   because those are the objects we could build.
[00:21:42.320 --> 00:21:44.040]   And now that we're in a digital world
[00:21:44.040 --> 00:21:45.600]   and we can build something different,
[00:21:45.600 --> 00:21:47.640]   should we try to build something different?
[00:21:47.640 --> 00:21:51.000]   But if you're saying it's not a reflection of what we could
[00:21:51.000 --> 00:21:53.320]   build, it's a reflection of how our brain works,
[00:21:53.320 --> 00:21:56.120]   then any attempt to change that is going to be doomed to failure
[00:21:56.120 --> 00:21:58.840]   unless we can change our brains as well.
[00:21:58.840 --> 00:22:00.400]   I think it's a function of the workloads.
[00:22:00.400 --> 00:22:03.040]   Maybe not how-- I think our brains are pretty adaptable.
[00:22:03.040 --> 00:22:05.840]   I think it's the type of work is probably more relevant there.
[00:22:05.840 --> 00:22:06.840]   That's probably true, too.
[00:22:06.840 --> 00:22:10.560]   I mean, the way we work is modern work is not--
[00:22:10.560 --> 00:22:14.920]   there's probably no analog to any kind of work.
[00:22:14.920 --> 00:22:17.320]   Even, I guess, in the industrial era,
[00:22:17.320 --> 00:22:19.800]   and there's been more--
[00:22:19.800 --> 00:22:20.400]   I don't know.
[00:22:20.400 --> 00:22:21.880]   If you're building a cathedral, I guess
[00:22:21.880 --> 00:22:24.240]   you're all working collaborating on a project.
[00:22:24.240 --> 00:22:25.880]   But it feels like--
[00:22:25.880 --> 00:22:27.120]   Like, the big mistake was to think
[00:22:27.120 --> 00:22:30.240]   that computers would mimic intelligence
[00:22:30.240 --> 00:22:32.680]   and mimic human intelligence, and that that made the goal
[00:22:32.680 --> 00:22:34.560]   entirely wrong.
[00:22:34.560 --> 00:22:36.960]   Well, instead, it's an aid to our work.
[00:22:36.960 --> 00:22:39.800]   And this was the big debate Jeff Hawkins had.
[00:22:39.800 --> 00:22:42.120]   He was at Bohm.
[00:22:42.120 --> 00:22:45.840]   Computer handwriting recognition, most notoriously
[00:22:45.840 --> 00:22:48.720]   with the Newton, was a flop because they were trying
[00:22:48.720 --> 00:22:51.480]   to get computers to recognize human handwriting.
[00:22:51.480 --> 00:22:52.640]   And Jeff said, you know what?
[00:22:52.640 --> 00:22:54.080]   We should do something else.
[00:22:54.080 --> 00:22:57.520]   What if we slightly modified our handwriting?
[00:22:57.520 --> 00:22:59.960]   In other words, accommodated ourselves to the computer,
[00:22:59.960 --> 00:23:02.480]   and he created graffiti, which was the--
[00:23:02.480 --> 00:23:04.640]   if anybody who had a palm knew that it was a slightly
[00:23:04.640 --> 00:23:08.280]   modified human alphabet, that the computer could do much better
[00:23:08.280 --> 00:23:09.120]   with.
[00:23:09.120 --> 00:23:11.160]   So it turned out, Hawkins said, it's
[00:23:11.160 --> 00:23:13.200]   easier for a human to adapt to the computer
[00:23:13.200 --> 00:23:15.320]   than for the computer to adapt to the human.
[00:23:15.320 --> 00:23:16.600]   So Stacy, let me amuse you.
[00:23:16.600 --> 00:23:19.080]   I just got an academic paper from 2012.
[00:23:19.080 --> 00:23:19.560]   I hadn't read it.
[00:23:19.560 --> 00:23:20.680]   I just saw a link to it.
[00:23:20.680 --> 00:23:28.240]   By Rachel Plotnik, the case of the electric push button,
[00:23:28.240 --> 00:23:32.600]   and it was a moral panic about buttons.
[00:23:32.600 --> 00:23:34.960]   And they used the phrase moral panic, not me.
[00:23:34.960 --> 00:23:37.720]   Because it was-- it kind of hid the technology behind.
[00:23:37.720 --> 00:23:38.840]   You hit this button.
[00:23:38.840 --> 00:23:39.680]   You don't know what's happening.
[00:23:39.680 --> 00:23:40.880]   You don't know how it's happening.
[00:23:40.880 --> 00:23:44.240]   It's very analogous today that it's a black box or a black
[00:23:44.240 --> 00:23:48.280]   button, and the buttons, like buttons, were frightening.
[00:23:48.280 --> 00:23:52.160]   People hate abstraction, but as our word gets more complicated,
[00:23:52.160 --> 00:23:53.240]   we have to have it.
[00:23:53.240 --> 00:23:53.760]   I mean, that's--
[00:23:53.760 --> 00:23:54.080]   Exactly.
[00:23:54.080 --> 00:23:55.800]   heuristics, yeah.
[00:23:55.800 --> 00:23:56.400]   And I see you--
[00:23:56.400 --> 00:23:57.120]   We have a hand up, by the way.
[00:23:57.120 --> 00:23:57.800]   I put in--
[00:23:57.800 --> 00:23:58.320]   I know.
[00:23:58.320 --> 00:23:59.880]   I've invited him to the stage.
[00:23:59.880 --> 00:24:03.160]   We're going to the stage, but he isn't coming.
[00:24:03.160 --> 00:24:04.600]   I put the link in the chat.
[00:24:04.600 --> 00:24:06.920]   It's a New Yorker article from 2019.
[00:24:06.920 --> 00:24:09.520]   And it's part of their review things.
[00:24:09.520 --> 00:24:09.880]   Yeah.
[00:24:09.880 --> 00:24:10.520]   Thank you.
[00:24:10.520 --> 00:24:11.560]   I hope I'm not out of New Yorker.
[00:24:11.560 --> 00:24:13.960]   It's actually-- to me, this is a fascinating discussion.
[00:24:13.960 --> 00:24:16.040]   I'm sure we're putting everybody else to sleep.
[00:24:16.040 --> 00:24:17.560]   But I find this--
[00:24:17.560 --> 00:24:21.120]   Well, it's fundamental to computing.
[00:24:21.120 --> 00:24:23.280]   And I think you're right, Stacey.
[00:24:23.280 --> 00:24:25.520]   Your point is exactly right, that the human brain is fairly
[00:24:25.520 --> 00:24:26.560]   plastic.
[00:24:26.560 --> 00:24:28.600]   We've already learned a lot of--
[00:24:28.600 --> 00:24:31.520]   we've all gotten used to being on Zoom.
[00:24:31.520 --> 00:24:34.800]   We've already learned to change some of the ways we do things.
[00:24:34.800 --> 00:24:36.720]   But the question is how plastic.
[00:24:36.720 --> 00:24:38.880]   There is some evidence that despite the fact
[00:24:38.880 --> 00:24:40.840]   that computer scientists have been trying to do this for more
[00:24:40.840 --> 00:24:46.440]   than 30 years, and it's never worked, maybe Google can do it.
[00:24:46.440 --> 00:24:49.760]   It's puzzling to me since they killed Wave.
[00:24:49.760 --> 00:24:50.840]   Then they would now--
[00:24:50.840 --> 00:24:52.760]   I mean, this is Wave, right, Jeff?
[00:24:52.760 --> 00:24:54.240]   Yeah, oh, it is.
[00:24:54.240 --> 00:24:56.160]   And I don't think it's not a case, Leo,
[00:24:56.160 --> 00:24:58.960]   that Wave wasn't ready at the time.
[00:24:58.960 --> 00:25:00.440]   The concept-- I think, right?
[00:25:00.440 --> 00:25:02.520]   The concepts could have been done then.
[00:25:02.520 --> 00:25:05.480]   Stacey, you're Stacey.
[00:25:05.480 --> 00:25:06.480]   That's what I am saying.
[00:25:06.480 --> 00:25:07.320]   I am Stacey.
[00:25:07.320 --> 00:25:11.680]   Pre-Stacey, Gina, was--
[00:25:11.680 --> 00:25:13.840]   She was showing the excitement about it.
[00:25:13.840 --> 00:25:14.340]   Yeah.
[00:25:14.340 --> 00:25:15.520]   She got it.
[00:25:15.520 --> 00:25:17.680]   And maybe because she was a computer programmer
[00:25:17.680 --> 00:25:22.160]   and had the ability to kind of not only grok it, but use it,
[00:25:22.160 --> 00:25:23.680]   there is a problem.
[00:25:23.680 --> 00:25:27.480]   When there's a learning curve, they got mocked a lot.
[00:25:27.480 --> 00:25:28.000]   Yeah.
[00:25:28.000 --> 00:25:29.760]   I think that was part of people's-- what the heck is this?
[00:25:29.760 --> 00:25:31.240]   I don't think he says to me.
[00:25:31.240 --> 00:25:32.880]   And they didn't see the vision in it.
[00:25:32.880 --> 00:25:38.000]   And then you and I, this isn't still when you were in your optimistic utopian days.
[00:25:38.000 --> 00:25:40.200]   We're thinking, wow, this is great.
[00:25:40.200 --> 00:25:42.800]   You've grounded me down, Jeff.
[00:25:42.800 --> 00:25:46.520]   Oh, I thought I just thought I took over the optimistic utopian day.
[00:25:46.520 --> 00:25:47.800]   So that's my job now.
[00:25:47.800 --> 00:25:51.600]   I used to be in fact, Dvorak, and when we were doing radio shows,
[00:25:51.600 --> 00:25:53.040]   he was the grumpy skeptic.
[00:25:53.040 --> 00:25:56.120]   And I was like, this is the greatest thing ever, the cheerleader.
[00:25:56.120 --> 00:25:59.960]   And maybe with age, I think it's most of a war call.
[00:25:59.960 --> 00:26:01.480]   Yeah.
[00:26:01.480 --> 00:26:04.280]   I've become much more of a cynic.
[00:26:04.280 --> 00:26:05.240]   This is a great article.
[00:26:05.240 --> 00:26:06.640]   Thank you for--
[00:26:06.640 --> 00:26:07.720]   I swear the link.
[00:26:07.720 --> 00:26:09.560]   It's from The New Yorker.
[00:26:09.560 --> 00:26:13.440]   It's a review of a variety of books about consciousness.
[00:26:13.440 --> 00:26:14.640]   And you could search for it.
[00:26:14.640 --> 00:26:16.640]   It's from December 4, 2019.
[00:26:16.640 --> 00:26:18.960]   Do we have minds of our own?
[00:26:18.960 --> 00:26:24.240]   And like all New Yorker articles, I'm going to put this aside and read it when I have time.
[00:26:24.240 --> 00:26:26.160]   This is not one of the longer New Year.
[00:26:26.160 --> 00:26:27.560]   It's part of the back section.
[00:26:27.560 --> 00:26:27.920]   Oh, good.
[00:26:27.920 --> 00:26:29.080]   Relatively shorter articles.
[00:26:29.080 --> 00:26:30.560]   Oh, God.
[00:26:30.560 --> 00:26:36.920]   I have currently 32 years worth of New Yorkers I'll read when I get time to do so.
[00:26:36.920 --> 00:26:37.440]   Yeah.
[00:26:37.440 --> 00:26:40.760]   We should get-- Kevin, I've asked you, Kevin, if you want to be on the show,
[00:26:40.760 --> 00:26:44.920]   Kevin Marks is in our chat room many times.
[00:26:44.920 --> 00:26:45.440]   Hey, Kevin.
[00:26:45.440 --> 00:26:47.800]   Yeah, he's been on Twig many, many times.
[00:26:47.800 --> 00:26:50.560]   And I think he would probably have a lot to say about this kind of stuff.
[00:26:50.560 --> 00:26:53.120]   He's a super smart man.
[00:26:53.120 --> 00:26:54.840]   He was watching the Google keynote.
[00:26:54.840 --> 00:26:56.560]   And he's been watching some of the other sessions.
[00:26:56.560 --> 00:26:57.160]   He used to work.
[00:26:57.160 --> 00:26:59.040]   He was developer at Google.
[00:26:59.040 --> 00:27:01.360]   He was developer at Apple for a quick time.
[00:27:01.360 --> 00:27:06.320]   A very smart guy, currently an open web advocate.
[00:27:06.320 --> 00:27:08.720]   And it would be great if we could get him on.
[00:27:08.720 --> 00:27:12.720]   But he probably would have a lot to say about that.
[00:27:12.720 --> 00:27:16.720]   I don't know if you saw this, Jeff, during Google I/O yesterday, when Google was talking
[00:27:16.720 --> 00:27:24.640]   at the end, actually very hopefully, about being not only zero emissions with offsets,
[00:27:24.640 --> 00:27:26.000]   which they are currently.
[00:27:26.000 --> 00:27:28.880]   But sometime in the next 20, 30--
[00:27:28.880 --> 00:27:29.760]   20, 30.
[00:27:29.760 --> 00:27:37.040]   --to become-- use energy completely generated by renewables, which would be remarkable.
[00:27:37.040 --> 00:27:38.640]   He had an article in 2006.
[00:27:38.640 --> 00:27:44.880]   He wrote on his blog about people talking about how much energy a Google search used,
[00:27:44.880 --> 00:27:48.480]   pointing out that when you breathe, you're actually putting more carbon dioxide in the
[00:27:48.480 --> 00:27:52.400]   air during the search than the Google computers are.
[00:27:52.400 --> 00:27:55.480]   And maybe, perhaps, if you really wanted to be green, you'd hold your breaths every
[00:27:55.480 --> 00:27:57.560]   time you did a Google search.
[00:27:57.560 --> 00:27:58.600]   So Kevin would be great.
[00:27:58.600 --> 00:28:00.120]   And put something in the rundown later.
[00:28:00.120 --> 00:28:01.400]   We can try to practice that.
[00:28:01.400 --> 00:28:01.920]   OK.
[00:28:01.920 --> 00:28:02.400]   Oh, good.
[00:28:02.400 --> 00:28:03.080]   A little breathing.
[00:28:03.080 --> 00:28:03.640]   All right.
[00:28:03.640 --> 00:28:04.280]   Yes.
[00:28:04.280 --> 00:28:05.640]   So what else could we talk about?
[00:28:05.640 --> 00:28:06.600]   Oh, Android 12.
[00:28:06.600 --> 00:28:10.040]   And did you put the public beta on either of you yet?
[00:28:10.040 --> 00:28:10.560]   No.
[00:28:10.560 --> 00:28:11.560]   No.
[00:28:11.560 --> 00:28:12.760]   I did immediately because--
[00:28:12.760 --> 00:28:15.280]   I don't have 20 phones or disposables.
[00:28:15.280 --> 00:28:16.840]   This is not my primary phone, so I could--
[00:28:16.840 --> 00:28:17.640]   Right, exactly.
[00:28:17.640 --> 00:28:19.280]   How is it?
[00:28:19.280 --> 00:28:19.840]   It's good.
[00:28:19.840 --> 00:28:21.200]   I like it.
[00:28:21.200 --> 00:28:26.800]   The biggest change you always see in these things is in the notifications panel that
[00:28:26.800 --> 00:28:27.760]   you slide out.
[00:28:27.760 --> 00:28:30.000]   And that really does look very, very different.
[00:28:30.000 --> 00:28:31.120]   I don't know if you can--
[00:28:31.120 --> 00:28:31.760]   How well you'll be--
[00:28:31.760 --> 00:28:33.000]   Did you get the full design?
[00:28:33.000 --> 00:28:34.720]   Is it the--
[00:28:34.720 --> 00:28:38.440]   It's, of course, the first public beta they were in developer preview three.
[00:28:38.440 --> 00:28:40.040]   So I don't know how mature it is.
[00:28:40.040 --> 00:28:41.720]   It seems reliable.
[00:28:41.720 --> 00:28:42.680]   Can you see that?
[00:28:42.680 --> 00:28:44.560]   [INAUDIBLE]
[00:28:44.560 --> 00:28:46.880]   Oh, my camera has been moved because Jason--
[00:28:46.880 --> 00:28:47.840]   there it is.
[00:28:47.840 --> 00:28:49.040]   I knew I could find it somewhere.
[00:28:49.040 --> 00:28:51.040]   [LAUGHTER]
[00:28:51.040 --> 00:28:52.520]   All right, there we go.
[00:28:52.520 --> 00:28:53.520]   So--
[00:28:53.520 --> 00:28:54.280]   Oh, wow, that is different.
[00:28:54.280 --> 00:28:58.160]   Yeah, so let me just slide it down again so you can see it.
[00:28:58.160 --> 00:29:02.320]   So I'm not getting the big wallpaper effects that they were talking about.
[00:29:02.320 --> 00:29:03.640]   I tried different wallpapers.
[00:29:03.640 --> 00:29:05.480]   The big clock and all that, yeah.
[00:29:05.480 --> 00:29:06.480]   Yeah, and I don't know.
[00:29:06.480 --> 00:29:06.960]   I don't know.
[00:29:06.960 --> 00:29:08.600]   Maybe that's not yet shipping.
[00:29:08.600 --> 00:29:10.440]   But this is the notification tray.
[00:29:10.440 --> 00:29:11.520]   You see the slide?
[00:29:11.520 --> 00:29:14.000]   This is a weird slider for brightness.
[00:29:14.000 --> 00:29:17.120]   It's a big fat slider on a little thin line.
[00:29:17.120 --> 00:29:18.000]   Do you really need that?
[00:29:18.000 --> 00:29:20.240]   Looks like blob opera, to be honest with you.
[00:29:20.240 --> 00:29:21.640]   I don't--
[00:29:21.640 --> 00:29:25.280]   Maybe it's when you're-- because when you do that, you do it when it's bright outside
[00:29:25.280 --> 00:29:26.120]   and you're in the sun.
[00:29:26.120 --> 00:29:26.880]   Oh, you can't see.
[00:29:26.880 --> 00:29:28.040]   So having that darker, fatter line.
[00:29:28.040 --> 00:29:29.040]   Yeah, that might be why.
[00:29:29.040 --> 00:29:30.600]   Yeah, you are the cheerleader.
[00:29:30.600 --> 00:29:31.400]   And then--
[00:29:31.400 --> 00:29:32.880]   [LAUGHTER]
[00:29:32.880 --> 00:29:34.600]   Where I just go outside.
[00:29:34.600 --> 00:29:37.080]   Yeah, maybe you're the only one who goes outside much.
[00:29:37.080 --> 00:29:38.920]   These are the new buttons.
[00:29:38.920 --> 00:29:41.080]   And you edit them with this big edit button.
[00:29:41.080 --> 00:29:43.800]   And you can see there's a lot of choices now, including--
[00:29:43.800 --> 00:29:45.760]   And Stacey, you like this.
[00:29:45.760 --> 00:29:48.240]   A lot of mode buttons.
[00:29:48.240 --> 00:29:51.320]   Like this is the bedtime mode from digital well-being.
[00:29:51.320 --> 00:29:52.200]   You can create tasks.
[00:29:52.200 --> 00:29:54.160]   Well, you can already put a lot of that--
[00:29:54.160 --> 00:29:56.240]   I mean, a lot of that's on my smart home thingy
[00:29:56.240 --> 00:29:57.880]   that I just pull down whenever.
[00:29:57.880 --> 00:30:01.600]   Yes, but now it's part of the system notification tray.
[00:30:01.600 --> 00:30:02.760]   So that's kind of-- I don't know.
[00:30:02.760 --> 00:30:03.520]   What do you call that?
[00:30:03.520 --> 00:30:04.560]   I can't even remember.
[00:30:04.560 --> 00:30:06.920]   I'm sure they talked about this in all about Android.
[00:30:06.920 --> 00:30:08.920]   But you can drag these tiles up.
[00:30:08.920 --> 00:30:13.320]   So there's more buttons than there used to be.
[00:30:13.320 --> 00:30:15.000]   They still do the thing that I don't like.
[00:30:15.000 --> 00:30:17.400]   Samsung doesn't do this, which is
[00:30:17.400 --> 00:30:21.840]   in order to-- when you first pull it down,
[00:30:21.840 --> 00:30:25.040]   you don't get a gear for hitting settings.
[00:30:25.040 --> 00:30:26.760]   You have to pull it down a second time.
[00:30:26.760 --> 00:30:28.800]   And then you get the settings.
[00:30:28.800 --> 00:30:30.200]   Oh, that is annoying.
[00:30:30.200 --> 00:30:31.200]   Yeah, that's a good one.
[00:30:31.200 --> 00:30:31.840]   It's much more there.
[00:30:31.840 --> 00:30:34.000]   You're like, you know you want to mess with your settings.
[00:30:34.000 --> 00:30:37.800]   Yeah, Samsung puts it right in the front, but not Google.
[00:30:37.800 --> 00:30:41.960]   And the settings, this is that new material you look.
[00:30:41.960 --> 00:30:43.240]   All the buttons are round.
[00:30:43.240 --> 00:30:44.720]   All the icons are round.
[00:30:44.720 --> 00:30:46.880]   OK, does anyone else remember--
[00:30:46.880 --> 00:30:48.200]   was it our browsers?
[00:30:48.200 --> 00:30:49.760]   Was it Microsoft Word?
[00:30:49.760 --> 00:30:52.200]   I think it was Microsoft's Office Suite.
[00:30:52.200 --> 00:30:54.680]   You could customize your colors and stuff for that,
[00:30:54.680 --> 00:30:57.080]   like in the late '90s.
[00:30:57.080 --> 00:30:58.240]   Did you all spend time doing that?
[00:30:58.240 --> 00:30:58.920]   Because I did.
[00:30:58.920 --> 00:31:01.680]   Yeah, and so that was my first reaction to this,
[00:31:01.680 --> 00:31:03.680]   is nobody wants to theme.
[00:31:03.680 --> 00:31:05.360]   It's a lot of work.
[00:31:05.360 --> 00:31:06.760]   Some people do.
[00:31:06.760 --> 00:31:07.840]   But most people--
[00:31:07.840 --> 00:31:11.040]   No, only theme when you're like--
[00:31:11.040 --> 00:31:13.040]   Yeah, you only theme when you're like a kid
[00:31:13.040 --> 00:31:14.480]   and you have nothing better to do.
[00:31:14.480 --> 00:31:17.480]   Or you want to go around with your phone,
[00:31:17.480 --> 00:31:18.600]   but you don't have anything else to do.
[00:31:18.600 --> 00:31:19.400]   Basically, yes.
[00:31:19.400 --> 00:31:22.480]   I mean, I spent that time messing around
[00:31:22.480 --> 00:31:25.240]   with my super hyperintensive smart home automation.
[00:31:25.240 --> 00:31:27.160]   Right, nowadays we have other things to do.
[00:31:27.160 --> 00:31:29.600]   But yeah, when you're--
[00:31:29.600 --> 00:31:31.320]   so that's what it's for.
[00:31:31.320 --> 00:31:34.560]   But that's why Google said, no, no, no, it's automatic.
[00:31:34.560 --> 00:31:35.480]   You could.
[00:31:35.480 --> 00:31:37.360]   There's a lot more theming capability.
[00:31:37.360 --> 00:31:38.720]   But the idea is it's automatic.
[00:31:38.720 --> 00:31:40.640]   It picks up colors from your wallpaper and stuff.
[00:31:40.640 --> 00:31:41.840]   So that's what we were talking about.
[00:31:41.840 --> 00:31:44.240]   So do you want to hear what I'm excited about?
[00:31:44.240 --> 00:31:44.740]   Yeah.
[00:31:44.740 --> 00:31:45.640]   Do you want to nerd for a minute?
[00:31:45.640 --> 00:31:47.480]   Of course.
[00:31:47.480 --> 00:31:50.520]   I am excited about Android introducing a new privacy-friendly
[00:31:50.520 --> 00:31:53.080]   sandbox for machine learning data.
[00:31:53.080 --> 00:31:54.400]   Yeah, so is that--
[00:31:54.400 --> 00:31:57.880]   Android is it like a secure enclave kind of?
[00:31:57.880 --> 00:32:00.200]   No, it's a software-based sandbox.
[00:32:00.200 --> 00:32:02.200]   So everyone's like, oh, it's not that secure.
[00:32:02.200 --> 00:32:04.200]   But wait, here's what I think is happening.
[00:32:04.200 --> 00:32:06.400]   But wait, there's more.
[00:32:06.400 --> 00:32:07.640]   But wait.
[00:32:07.640 --> 00:32:09.200]   Let me take you into the future.
[00:32:09.200 --> 00:32:12.080]   So ARM introduced something called confidential compute
[00:32:12.080 --> 00:32:16.800]   earlier this year as part of its next tenure cycle of chip
[00:32:16.800 --> 00:32:18.200]   design.
[00:32:18.200 --> 00:32:22.360]   And that is actually going to create a separate--
[00:32:22.360 --> 00:32:26.080]   think of it like an OS-accessible hardware-based
[00:32:26.080 --> 00:32:28.400]   confidential area on a computer.
[00:32:28.400 --> 00:32:30.960]   And that's going to be where developers can put their own
[00:32:30.960 --> 00:32:32.520]   private stuff.
[00:32:32.520 --> 00:32:34.880]   And no one, even if you have access to the device,
[00:32:34.880 --> 00:32:36.440]   should be able to get it.
[00:32:36.440 --> 00:32:38.320]   We don't know exactly how it's going to work.
[00:32:38.320 --> 00:32:40.360]   And Google's part of this.
[00:32:40.360 --> 00:32:42.040]   Google came and talked about the need for this.
[00:32:42.040 --> 00:32:44.080]   So Google and ARM are clearly talking.
[00:32:44.080 --> 00:32:48.080]   I think this is a beginning of that sort of segmentation.
[00:32:48.080 --> 00:32:50.640]   And I think we're going to see a lot more of it, not just
[00:32:50.640 --> 00:32:53.320]   for our private machine learning data, but for things like,
[00:32:53.320 --> 00:32:56.800]   let's say you have confidential code as an enterprise
[00:32:56.800 --> 00:33:01.880]   that you want to run on somebody's phone, like an employee's
[00:33:01.880 --> 00:33:04.560]   phone, but you're worried that the employer's going to lose it.
[00:33:04.560 --> 00:33:08.240]   You could actually lock it into something like that.
[00:33:08.240 --> 00:33:11.360]   And it will be--
[00:33:11.360 --> 00:33:12.800]   no one can access it, basically.
[00:33:12.800 --> 00:33:14.560]   So I think this is beginning of that.
[00:33:14.560 --> 00:33:15.480]   So I have something that's in the box.
[00:33:15.480 --> 00:33:16.640]   Apple does it with a secure enclave.
[00:33:16.640 --> 00:33:18.080]   Those are both hardware-based.
[00:33:18.080 --> 00:33:22.200]   This sort of is because it's using an ARM technology that
[00:33:22.200 --> 00:33:27.880]   will make it more secure than just a software folder.
[00:33:27.880 --> 00:33:31.160]   It's a privileged space in the OS.
[00:33:31.160 --> 00:33:35.120]   So it's not accessible to user applications in the user space,
[00:33:35.120 --> 00:33:36.080]   basically, right?
[00:33:36.080 --> 00:33:36.640]   Right.
[00:33:36.640 --> 00:33:38.760]   But it's not--
[00:33:38.760 --> 00:33:40.960]   with the confidential computing, you're
[00:33:40.960 --> 00:33:43.360]   going to still have your quote unquote secure enclave
[00:33:43.360 --> 00:33:47.560]   or your trusted execution environment for like Intel chips.
[00:33:47.560 --> 00:33:52.200]   There's going to be a separate place on the hardware for that.
[00:33:52.200 --> 00:33:54.040]   We don't, again, know exactly.
[00:33:54.040 --> 00:33:56.120]   So I think this is prep for that.
[00:33:56.120 --> 00:34:00.040]   And I think it's really good because a couple of things.
[00:34:00.040 --> 00:34:02.680]   We're starting to realize that all of the data around us
[00:34:02.680 --> 00:34:03.840]   being collected--
[00:34:03.840 --> 00:34:06.360]   and my thing of the week is tied to this--
[00:34:06.360 --> 00:34:08.560]   is really actually kind of personal and upsetting.
[00:34:08.560 --> 00:34:10.320]   So I think it's good that companies are actually
[00:34:10.320 --> 00:34:15.000]   realizing this and trying to take steps to make it better.
[00:34:15.000 --> 00:34:18.000]   And you see Google getting in on this,
[00:34:18.000 --> 00:34:20.880]   even with their last nest hub, the one
[00:34:20.880 --> 00:34:22.520]   that does your sleep tracking.
[00:34:22.520 --> 00:34:25.200]   All of that's processed locally on the machine.
[00:34:25.200 --> 00:34:27.040]   So now they're just saying, hey, not only
[00:34:27.040 --> 00:34:29.600]   we're processing all the sensitive data locally,
[00:34:29.600 --> 00:34:31.240]   well, we're acknowledging it sensitive.
[00:34:31.240 --> 00:34:32.480]   We're going to process it locally.
[00:34:32.480 --> 00:34:35.360]   And now we're actually going to try to secure it even better.
[00:34:35.360 --> 00:34:36.960]   And I think that's good.
[00:34:36.960 --> 00:34:39.760]   And I think it is a path to the block.
[00:34:39.760 --> 00:34:43.080]   I think that the more that they know how to do locally,
[00:34:43.080 --> 00:34:44.880]   the less you worry about things going up.
[00:34:44.880 --> 00:34:47.120]   And then the question is, I mean, that's--
[00:34:47.120 --> 00:34:50.480]   in a way, that's what flock is.
[00:34:50.480 --> 00:34:51.000]   No.
[00:34:51.000 --> 00:34:51.680]   Yeah.
[00:34:51.680 --> 00:34:52.120]   No.
[00:34:52.120 --> 00:34:53.120]   No.
[00:34:53.120 --> 00:34:53.480]   No.
[00:34:53.480 --> 00:34:54.800]   But that is my concern.
[00:34:54.800 --> 00:34:57.480]   That is my concern, Stacey.
[00:34:57.480 --> 00:35:00.000]   Nope, you're drawing a conclusion that is not right.
[00:35:00.000 --> 00:35:00.600]   Arrent.
[00:35:00.600 --> 00:35:03.800]   Google certainly made a lot of noise
[00:35:03.800 --> 00:35:05.080]   about on-device privacy.
[00:35:05.080 --> 00:35:07.400]   By the way, they've been saying on-device for just as long
[00:35:07.400 --> 00:35:08.280]   as anybody.
[00:35:08.280 --> 00:35:12.720]   But it sounded like they were a little bit stung by Apple
[00:35:12.720 --> 00:35:17.000]   and attempting to show that Android could be as secure as iOS
[00:35:17.000 --> 00:35:19.560]   and as private as iOS.
[00:35:19.560 --> 00:35:23.240]   But I think flock is germane in this sense.
[00:35:23.240 --> 00:35:27.840]   That Google, because they're--
[00:35:27.840 --> 00:35:30.400]   99% of their revenue is from advertising.
[00:35:30.400 --> 00:35:31.880]   Let's face it.
[00:35:31.880 --> 00:35:36.720]   They are going to always go part of the way towards this.
[00:35:36.720 --> 00:35:42.200]   But they're always going to leave a little place for advertisers
[00:35:42.200 --> 00:35:43.560]   because that's their business.
[00:35:43.560 --> 00:35:47.600]   And so they're going to always kind of preserve their role
[00:35:47.600 --> 00:35:50.800]   as a first party aggregator of information,
[00:35:50.800 --> 00:35:53.800]   even if they protect you from third parties.
[00:35:53.800 --> 00:35:56.880]   And that's what flock is all about, too, right?
[00:35:56.880 --> 00:35:57.400]   Yeah.
[00:35:57.400 --> 00:35:58.760]   I'm going to try this one.
[00:35:58.760 --> 00:35:59.720]   But let's stay safe.
[00:35:59.720 --> 00:36:00.720]   You go ahead and stress your spot.
[00:36:00.720 --> 00:36:03.800]   I don't want to try my flock theory on you.
[00:36:03.800 --> 00:36:06.320]   OK.
[00:36:06.320 --> 00:36:09.560]   In a world where you're building products based on data
[00:36:09.560 --> 00:36:13.280]   and around context for individual users or companies,
[00:36:13.280 --> 00:36:17.240]   trust is the most important asset that you have.
[00:36:17.240 --> 00:36:19.080]   And I think Google's realized this
[00:36:19.080 --> 00:36:22.880]   and is taking steps to try to say you can trust us.
[00:36:22.880 --> 00:36:25.480]   There's a difference.
[00:36:25.480 --> 00:36:28.640]   It's the most important thing to your customer.
[00:36:28.640 --> 00:36:30.560]   And for Apple, the customer is the buyer of the phone
[00:36:30.560 --> 00:36:33.360]   for Google, the customer is the advertiser.
[00:36:33.360 --> 00:36:36.160]   I think Google's recognizing, though, that they're customer--
[00:36:36.160 --> 00:36:37.880]   they have to serve two masters.
[00:36:37.880 --> 00:36:39.200]   They may not be able to do it well.
[00:36:39.200 --> 00:36:43.520]   And I also think they recognize that eventually,
[00:36:43.520 --> 00:36:45.440]   search and advertising, they're going
[00:36:45.440 --> 00:36:47.920]   to have to have new business models.
[00:36:47.920 --> 00:36:49.760]   They can't do that forever.
[00:36:49.760 --> 00:36:53.360]   So they're kind of like, you remember
[00:36:53.360 --> 00:36:58.600]   when all-- this was probably in 2008, 2007,
[00:36:58.600 --> 00:37:02.480]   when all the carrier, the ISPs were like, oh, man,
[00:37:02.480 --> 00:37:06.520]   we are going to have to make money on data somehow,
[00:37:06.520 --> 00:37:08.760]   because voice and long distance and all of that
[00:37:08.760 --> 00:37:11.200]   and landlines are not going to help us.
[00:37:11.200 --> 00:37:14.360]   And they did this very fine balancing act
[00:37:14.360 --> 00:37:16.400]   for quite some time before their revenues went over
[00:37:16.400 --> 00:37:17.160]   to wireless.
[00:37:17.160 --> 00:37:18.960]   Google's in that same position.
[00:37:18.960 --> 00:37:21.160]   What are they-- Clayton, Chris, just in the dude.
[00:37:21.160 --> 00:37:21.760]   Yeah, yeah.
[00:37:21.760 --> 00:37:22.600]   Innovator's dilemma.
[00:37:22.600 --> 00:37:25.480]   But they're going there, but only kicking and screaming,
[00:37:25.480 --> 00:37:26.920]   only if they have to.
[00:37:26.920 --> 00:37:29.480]   And I think what they're trying to do
[00:37:29.480 --> 00:37:35.360]   is to do enough to appease us who are not real customers.
[00:37:35.360 --> 00:37:37.680]   We are the product.
[00:37:37.680 --> 00:37:40.960]   Everything Google does is about gathering information
[00:37:40.960 --> 00:37:42.920]   about us to sell to advertisers.
[00:37:42.920 --> 00:37:43.680]   That's everything.
[00:37:43.680 --> 00:37:44.200]   Right.
[00:37:44.200 --> 00:37:46.160]   From Android on down.
[00:37:46.160 --> 00:37:50.440]   And so their problem is, yes, there is a tension now.
[00:37:50.440 --> 00:37:54.400]   And their product, us, is starting to wake up and say, hey,
[00:37:54.400 --> 00:37:57.360]   wait a minute, I don't want you to chop me up.
[00:37:57.360 --> 00:38:00.080]   So are we the free-range chickens in this situation?
[00:38:00.080 --> 00:38:01.480]   Like we used to be.
[00:38:01.480 --> 00:38:02.040]   That's--
[00:38:02.040 --> 00:38:03.040]   They're trying to do--
[00:38:03.040 --> 00:38:03.560]   What?
[00:38:03.560 --> 00:38:09.440]   Actually, that's an apt analogy, because there is a place
[00:38:09.440 --> 00:38:13.800]   between keeping a chicken in the dark in a tiny barn
[00:38:13.800 --> 00:38:17.520]   if it's whole life and having it run around on the prairie.
[00:38:17.520 --> 00:38:20.560]   There's this intermediate place where chicken producers
[00:38:20.560 --> 00:38:23.600]   want a safe free-range or pudding chickens
[00:38:23.600 --> 00:38:25.800]   that allow them to be as--
[00:38:25.800 --> 00:38:26.960]   They still get their heads chopped up.
[00:38:26.960 --> 00:38:29.600]   Yeah, they still get the production up.
[00:38:29.600 --> 00:38:31.000]   So I think Google's trying--
[00:38:31.000 --> 00:38:33.720]   I believe Google's trying to hedge their bet
[00:38:33.720 --> 00:38:35.040]   and kind of have it both ways.
[00:38:35.040 --> 00:38:37.560]   They understand the consumers are getting pissed off.
[00:38:37.560 --> 00:38:40.600]   But they also know they have no business model
[00:38:40.600 --> 00:38:42.960]   if they appease consumers.
[00:38:42.960 --> 00:38:44.520]   So they're going to do the least--
[00:38:44.520 --> 00:38:47.560]   They're going to do just as much appeasement as they have to.
[00:38:47.560 --> 00:38:48.240]   And you think--
[00:38:48.240 --> 00:38:48.880]   That makes sense.
[00:38:48.880 --> 00:38:51.400]   Yeah, they're going to do more.
[00:38:51.400 --> 00:38:55.880]   I think they might do a little bit more only because they--
[00:38:55.880 --> 00:38:58.560]   They're right now stuck in a three to four-way race.
[00:38:58.560 --> 00:39:01.680]   And you can't be worse than Facebook.
[00:39:01.680 --> 00:39:03.200]   It's unclear how much--
[00:39:03.200 --> 00:39:05.880]   Where Amazon's really kind of drinking from the ad Kool-Aid
[00:39:05.880 --> 00:39:07.560]   and people are starting to trust it less.
[00:39:07.560 --> 00:39:09.880]   So I think Google's kind of--
[00:39:09.880 --> 00:39:13.760]   If I think about the way my feelings about how my data has
[00:39:13.760 --> 00:39:16.480]   been used over time have shifted,
[00:39:16.480 --> 00:39:18.800]   Google is looking better to me than Amazon.
[00:39:18.800 --> 00:39:20.520]   And I used to be like, well, yeah,
[00:39:20.520 --> 00:39:23.080]   Amazon's taken all my data, but they're never sharing it.
[00:39:23.080 --> 00:39:25.280]   And now I'm like, oof, Amazon.
[00:39:25.280 --> 00:39:27.760]   Well, Amazon's the ad business now.
[00:39:27.760 --> 00:39:30.080]   They're in the ad business, but more specifically,
[00:39:30.080 --> 00:39:31.440]   they're not just advertising.
[00:39:31.440 --> 00:39:34.560]   They're actually using my data to make me want to buy things
[00:39:34.560 --> 00:39:36.400]   that I don't think I want to buy.
[00:39:36.400 --> 00:39:39.360]   So it's not just like advertisers.
[00:39:39.360 --> 00:39:41.200]   They're not just selling my data to advertisers
[00:39:41.200 --> 00:39:42.200]   who want to advertise to me.
[00:39:42.200 --> 00:39:45.720]   Amazon's using my data for itself against me.
[00:39:45.720 --> 00:39:47.360]   So that's, I guess, my fear.
[00:39:47.360 --> 00:39:48.560]   For you, for you.
[00:39:48.560 --> 00:39:50.600]   For you, for your interest, they know best.
[00:39:50.600 --> 00:39:52.520]   They're paying lip service to privacy
[00:39:52.520 --> 00:39:54.880]   because they know they need to now.
[00:39:54.880 --> 00:39:55.760]   But I don't--
[00:39:55.760 --> 00:39:56.960]   It's not just lip service.
[00:39:56.960 --> 00:39:59.480]   They're actually taking steps that do benefit a user's
[00:39:59.480 --> 00:39:59.920]   privacy.
[00:39:59.920 --> 00:40:02.120]   Oh, no, they are because they have to.
[00:40:02.120 --> 00:40:02.720]   I mean, I--
[00:40:02.720 --> 00:40:03.960]   But he's talking about the motive.
[00:40:03.960 --> 00:40:05.320]   He was talking about the motive more.
[00:40:05.320 --> 00:40:06.160]   I know about motive.
[00:40:06.160 --> 00:40:07.320]   Well, and let's not forget--
[00:40:07.320 --> 00:40:07.800]   It's just be real.
[00:40:07.800 --> 00:40:09.840]   How is Google's customer?
[00:40:09.840 --> 00:40:15.880]   Where does the money go come from the Google spends?
[00:40:15.880 --> 00:40:18.120]   It all comes from advertisers.
[00:40:18.120 --> 00:40:18.720]   We know that.
[00:40:18.720 --> 00:40:20.520]   We look at just look at the quarterly results.
[00:40:20.520 --> 00:40:24.480]   So their customer, their business, is selling advertising.
[00:40:24.480 --> 00:40:25.880]   So we can't forget that.
[00:40:25.880 --> 00:40:27.920]   And that's-- for people who defend Apple--
[00:40:27.920 --> 00:40:30.720]   I don't think Apple, by the way, is clean either.
[00:40:30.720 --> 00:40:33.600]   But to people who defend Apple, they say, well, all you
[00:40:33.600 --> 00:40:35.520]   have to do is look at the business model.
[00:40:35.520 --> 00:40:37.360]   It's clear who Apple's customers are.
[00:40:37.360 --> 00:40:40.920]   All their revenue comes from selling hardware and services.
[00:40:40.920 --> 00:40:44.360]   So I'm just a little skeptical.
[00:40:44.360 --> 00:40:46.000]   But I was-- yeah, I agree with you.
[00:40:46.000 --> 00:40:48.360]   I'm very gratified that Google's paying more attention to this.
[00:40:48.360 --> 00:40:50.520]   They've always said on device and stuff like that.
[00:40:50.520 --> 00:40:51.800]   This is not new.
[00:40:51.800 --> 00:40:53.920]   But the capability to do that has gotten greater.
[00:40:53.920 --> 00:40:54.760]   That's the point.
[00:40:54.760 --> 00:40:55.760]   Yeah.
[00:40:55.760 --> 00:40:56.280]   Right?
[00:40:56.280 --> 00:40:58.360]   And the greater it gets and the more it gets exploited,
[00:40:58.360 --> 00:41:00.600]   the more you have some control over it.
[00:41:00.600 --> 00:41:02.400]   Now, then the question is, what's the transparency
[00:41:02.400 --> 00:41:04.160]   of what happens on your device versus elsewhere?
[00:41:04.160 --> 00:41:05.480]   What faith do you have in that?
[00:41:05.480 --> 00:41:06.560]   Lots of other questions.
[00:41:06.560 --> 00:41:09.560]   I also think Google is watching carefully,
[00:41:09.560 --> 00:41:11.080]   because there's people like us who
[00:41:11.080 --> 00:41:12.920]   are beating the drum for privacy.
[00:41:12.920 --> 00:41:15.560]   But I think they're also looking at normal people
[00:41:15.560 --> 00:41:18.280]   and saying, have they noticed?
[00:41:18.280 --> 00:41:19.920]   I know Facebook's doing this.
[00:41:19.920 --> 00:41:21.400]   Do they notice?
[00:41:21.400 --> 00:41:22.960]   Do we get away with it?
[00:41:22.960 --> 00:41:26.000]   They're trying to figure out how much real people care.
[00:41:26.000 --> 00:41:27.520]   But it's become more and more apparent when
[00:41:27.520 --> 00:41:32.040]   you see a statistic like the fact that 96% of people
[00:41:32.040 --> 00:41:38.320]   with Apple's iOS 14.5 check the box that said, do not track 96%.
[00:41:38.320 --> 00:41:42.680]   That sends a chill down Apple, Google and Facebook spine.
[00:41:42.680 --> 00:41:43.680]   And makes Apple think--
[00:41:43.680 --> 00:41:46.080]   Which Apple?
[00:41:46.080 --> 00:41:52.440]   Well, Apple doesn't yet have a real advertising business.
[00:41:52.440 --> 00:41:54.360]   I mean, they're working on it.
[00:41:54.360 --> 00:41:57.320]   Well, they're working on a services business.
[00:41:57.320 --> 00:41:58.640]   Yeah.
[00:41:58.640 --> 00:41:59.400]   But the services--
[00:41:59.400 --> 00:41:59.920]   They're like, eh.
[00:41:59.920 --> 00:42:00.640]   --they charge for it.
[00:42:00.640 --> 00:42:01.320]   No, no.
[00:42:01.320 --> 00:42:02.600]   But they charge for it.
[00:42:02.600 --> 00:42:03.720]   Look, you don't pay for it.
[00:42:03.720 --> 00:42:06.680]   I don't pay for almost Google Mail, Google Docs,
[00:42:06.680 --> 00:42:08.120]   almost everything I do.
[00:42:08.120 --> 00:42:09.200]   Oh, yeah, yeah.
[00:42:09.200 --> 00:42:10.160]   No, I got that.
[00:42:10.160 --> 00:42:11.000]   But the services--
[00:42:11.000 --> 00:42:12.120]   I do, as I get worse service.
[00:42:12.120 --> 00:42:13.120]   Yeah, I know.
[00:42:13.120 --> 00:42:14.880]   Well, actually, our company does.
[00:42:14.880 --> 00:42:16.480]   We have a workspace account.
[00:42:16.480 --> 00:42:18.400]   But most people don't.
[00:42:18.400 --> 00:42:20.320]   And so there's another--
[00:42:20.320 --> 00:42:21.880]   look, if you're using Gmail for free,
[00:42:21.880 --> 00:42:24.600]   yeah, I know Google's not looking for stuff in your mail
[00:42:24.600 --> 00:42:27.840]   to advertise to you about anymore.
[00:42:27.840 --> 00:42:30.800]   But you've got to figure there's a reason Google's given you
[00:42:30.800 --> 00:42:33.640]   all of that for free.
[00:42:33.640 --> 00:42:35.640]   I mean, and by the way, YouTube, you
[00:42:35.640 --> 00:42:37.720]   think YouTube doesn't know everything about you
[00:42:37.720 --> 00:42:39.600]   when you go watch on YouTube?
[00:42:39.600 --> 00:42:42.000]   It really-- it really--
[00:42:42.000 --> 00:42:45.840]   for us to me, because we have a lot of viewers who say, oh,
[00:42:45.840 --> 00:42:48.320]   you put ad tracking into the feed.
[00:42:48.320 --> 00:42:48.720]   We don't.
[00:42:48.720 --> 00:42:49.760]   It's not exactly ad-tract.
[00:42:49.760 --> 00:42:51.880]   But anyway, you're logging me.
[00:42:51.880 --> 00:42:54.000]   So I'm going to watch your shows on YouTube.
[00:42:54.000 --> 00:42:58.040]   And it's like, dude, you think Google's--
[00:42:58.040 --> 00:42:58.920]   You're talking really?
[00:42:58.920 --> 00:43:02.360]   --tracking you on YouTube?
[00:43:02.360 --> 00:43:04.000]   Of course I are.
[00:43:04.000 --> 00:43:07.400]   YouTube's free for a reason.
[00:43:07.400 --> 00:43:09.360]   Anyway, I don't want to belabor this point.
[00:43:09.360 --> 00:43:12.200]   I think it remains to be seen.
[00:43:12.200 --> 00:43:13.380]   We're doing the change log.
[00:43:13.380 --> 00:43:14.500]   This is gonna be like a chain,
[00:43:14.500 --> 00:43:15.700]   the whole show is gonna be the change log.
[00:43:15.700 --> 00:43:17.020]   - The whole show is like a change log.
[00:43:17.020 --> 00:43:18.600]   - Yeah, I'm gonna take a look. - Oh, it's IO.
[00:43:18.600 --> 00:43:20.100]   - Yeah, there was a lot of new stuff.
[00:43:20.100 --> 00:43:23.080]   I want to talk more, and just a second about that.
[00:43:23.080 --> 00:43:24.440]   Actually there is, Simon will tell you these
[00:43:24.440 --> 00:43:26.920]   a lot of new stuff, and a lot of nothing.
[00:43:26.920 --> 00:43:27.820]   - Yeah.
[00:43:27.820 --> 00:43:28.660]   - But, right?
[00:43:28.660 --> 00:43:30.760]   - There's a something, you wanna hear my favorite something?
[00:43:30.760 --> 00:43:31.640]   - Stay tuned.
[00:43:31.640 --> 00:43:33.460]   - Yeah, stay tuned. - Stay tuned.
[00:43:33.460 --> 00:43:34.320]   - Stay tuned.
[00:43:34.320 --> 00:43:35.320]   - Stay tuned.
[00:43:35.320 --> 00:43:36.140]   - Stay tuned.
[00:43:36.140 --> 00:43:37.460]   - Stay tuned. - Stay tuned.
[00:43:37.460 --> 00:43:38.580]   - Stay tuned, stay tuned.
[00:43:38.580 --> 00:43:39.940]   - Today brought to you by Nureva.
[00:43:39.940 --> 00:43:42.020]   If you're going back to work,
[00:43:42.020 --> 00:43:43.740]   you're gonna get back in the conference room,
[00:43:43.740 --> 00:43:48.740]   but of course, social distancing still is in effect.
[00:43:48.740 --> 00:43:52.060]   And it's got big implications for meeting room audio.
[00:43:52.060 --> 00:43:54.900]   For instance, not only you're gonna have to keep
[00:43:54.900 --> 00:43:57.540]   six feet apart, but it may be that you can't place people
[00:43:57.540 --> 00:43:59.100]   the same way you could.
[00:43:59.100 --> 00:44:00.700]   You're gonna need good mic coverage.
[00:44:00.700 --> 00:44:02.100]   You're gonna want people to be able to hear
[00:44:02.100 --> 00:44:05.660]   what you're saying in this meeting space.
[00:44:05.660 --> 00:44:08.300]   But the traditional way of doing it is not gonna work.
[00:44:08.300 --> 00:44:10.100]   Yeah, you say beam forming systems.
[00:44:10.100 --> 00:44:10.940]   What about that?
[00:44:10.940 --> 00:44:12.180]   Nope.
[00:44:12.180 --> 00:44:16.420]   They often require adjustment from an expensive technician.
[00:44:16.420 --> 00:44:18.700]   Plus, you'll have to sit in a specific spot,
[00:44:18.700 --> 00:44:21.340]   tabletop systems, require additional mics.
[00:44:21.340 --> 00:44:24.540]   They have to be sanitized between meetings.
[00:44:24.540 --> 00:44:28.660]   Both systems require participants to change behavior.
[00:44:28.660 --> 00:44:30.060]   They can only face one way.
[00:44:30.060 --> 00:44:31.140]   They have to sit and, you know,
[00:44:31.140 --> 00:44:32.980]   you're gonna mark the spot where they have to sit
[00:44:32.980 --> 00:44:35.220]   with a piece of tape.
[00:44:35.220 --> 00:44:37.980]   Wouldn't it be cool if there were a way to get clearer,
[00:44:37.980 --> 00:44:41.700]   reliable audio, let your team act naturally,
[00:44:41.700 --> 00:44:43.740]   and still feel safe, still socially distanced?
[00:44:43.740 --> 00:44:45.860]   That's called Nareva.
[00:44:45.860 --> 00:44:49.740]   And it is a scientific miracle.
[00:44:49.740 --> 00:44:51.980]   N-U-R-E-V-A.
[00:44:51.980 --> 00:44:54.460]   They've got four patents for what they call
[00:44:54.460 --> 00:44:56.340]   their microphone mist technology.
[00:44:56.340 --> 00:45:00.060]   It's the first socially distanced mic system.
[00:45:00.060 --> 00:45:02.500]   Microphone mist, in effect,
[00:45:02.500 --> 00:45:05.660]   puts thousands of virtual microphones in the room.
[00:45:05.660 --> 00:45:09.940]   And there's no adjustment or adaptation necessary.
[00:45:09.940 --> 00:45:13.180]   The audio automatically adapts
[00:45:13.180 --> 00:45:14.580]   to however people are sitting,
[00:45:14.580 --> 00:45:15.820]   wherever they're facing,
[00:45:15.820 --> 00:45:18.180]   whatever your room configuration.
[00:45:18.180 --> 00:45:20.980]   And they have systems for big, little,
[00:45:20.980 --> 00:45:22.180]   and giant spaces.
[00:45:22.180 --> 00:45:24.660]   Their HDL 300 system is the first microphone
[00:45:24.660 --> 00:45:28.500]   and speaker bar to be certified for large meeting spaces
[00:45:28.500 --> 00:45:30.860]   up to 15 feet by 28 feet.
[00:45:30.860 --> 00:45:34.420]   And by the way, it's teams certified.
[00:45:34.420 --> 00:45:37.500]   It fills a room with thousands of virtual microphones.
[00:45:37.500 --> 00:45:39.060]   You get true full room coverage.
[00:45:39.060 --> 00:45:41.100]   People can be heard from anywhere,
[00:45:41.100 --> 00:45:42.060]   no matter where they're sitting,
[00:45:42.060 --> 00:45:43.940]   no matter where they're facing or how they're distanced.
[00:45:43.940 --> 00:45:46.540]   That is really nice.
[00:45:46.540 --> 00:45:48.260]   And you don't need to bring in a technician.
[00:45:48.260 --> 00:45:50.380]   It's just like installing a sound bar.
[00:45:50.380 --> 00:45:52.740]   Frankly, the microphone and speaker bar are a single unit.
[00:45:52.740 --> 00:45:54.580]   You put it up on the wall and you're done.
[00:45:54.580 --> 00:45:57.620]   Nareva, with Nareva, you get a console
[00:45:57.620 --> 00:45:59.900]   that puts device management into your hands.
[00:45:59.900 --> 00:46:02.100]   And on your terms, just settings,
[00:46:02.100 --> 00:46:03.540]   install firmware updates.
[00:46:03.540 --> 00:46:05.100]   Look how nice that looks.
[00:46:05.100 --> 00:46:06.660]   Check device status and more all
[00:46:06.660 --> 00:46:09.660]   from a secure cloud-based platform.
[00:46:09.660 --> 00:46:10.900]   And of course, consoles included
[00:46:10.900 --> 00:46:13.060]   with every Nareva audio system.
[00:46:13.060 --> 00:46:14.460]   By the way, when you enroll your system,
[00:46:14.460 --> 00:46:17.020]   I found this out, you'll get an extra year of warranty-free.
[00:46:17.020 --> 00:46:19.460]   So enroll your system.
[00:46:19.460 --> 00:46:21.740]   Nareva audio products have won numerous awards,
[00:46:21.740 --> 00:46:24.700]   top new technology award at ISE 2020
[00:46:24.700 --> 00:46:26.980]   for the Nareva HDL 200 system.
[00:46:26.980 --> 00:46:28.260]   No matter the size of your room,
[00:46:28.260 --> 00:46:29.460]   they have a full line of systems
[00:46:29.460 --> 00:46:32.180]   for small, medium, and large spaces.
[00:46:32.180 --> 00:46:34.700]   HubSpot uses Nareva, their principal collaboration
[00:46:34.700 --> 00:46:35.820]   and engineer Jimmy Yan said,
[00:46:35.820 --> 00:46:39.580]   "We were so impressed with the sound quality,
[00:46:39.580 --> 00:46:42.820]   ease of install and ease of use of the HDL 300.
[00:46:42.820 --> 00:46:45.860]   It was a no-brainer for us to adopt it."
[00:46:45.860 --> 00:46:47.540]   To learn more about how Nareva audio
[00:46:47.540 --> 00:46:49.180]   is the simple cost-effective way
[00:46:49.180 --> 00:46:51.340]   to let your team's distance in meetings
[00:46:51.340 --> 00:46:53.220]   and still act and converse naturally,
[00:46:53.220 --> 00:46:55.060]   visit Nareva.com/twit.
[00:46:55.060 --> 00:46:58.300]   And you are EVA.com/twit.
[00:46:58.300 --> 00:46:59.660]   Slash-twit, thank you Nareva
[00:46:59.660 --> 00:47:01.620]   for your support of this week in Google.
[00:47:01.620 --> 00:47:03.940]   And you can support us by going to that special address.
[00:47:03.940 --> 00:47:08.220]   It's Nareva and you are EVA.com/twit.
[00:47:08.220 --> 00:47:10.620]   T-W-I-T.
[00:47:10.620 --> 00:47:12.020]   Thank you Nareva.
[00:47:12.020 --> 00:47:14.740]   Hey, look who's wandered into the place.
[00:47:14.740 --> 00:47:16.340]   Kevin Marks is here.
[00:47:16.340 --> 00:47:17.340]   Well, well, well.
[00:47:17.340 --> 00:47:19.140]   Yay, open web advocate.
[00:47:19.140 --> 00:47:20.380]   Hey, stranger.
[00:47:20.380 --> 00:47:22.020]   Worked at Google, worked at Apple.
[00:47:22.020 --> 00:47:24.500]   Is an expert on all of this stuff.
[00:47:24.500 --> 00:47:27.340]   So you watched, I saw your tweets.
[00:47:27.340 --> 00:47:31.180]   You actually been watching a lot of Google I/O.
[00:47:31.180 --> 00:47:33.220]   I don't want to, I watched the Open Yesterday with you guys.
[00:47:33.220 --> 00:47:35.860]   And then I watched that particular episode today.
[00:47:35.860 --> 00:47:36.460]   So--
[00:47:36.460 --> 00:47:39.220]   The Chrome, the Chrome episode.
[00:47:39.220 --> 00:47:41.220]   Well, there was those sort of weird tees
[00:47:41.220 --> 00:47:43.100]   that happened halfway through the morning.
[00:47:43.100 --> 00:47:46.700]   Oh, the morning for me, which was we're adding feed reading
[00:47:46.700 --> 00:47:47.700]   to Chrome.
[00:47:47.700 --> 00:47:48.740]   I'm like, really?
[00:47:48.740 --> 00:47:50.460]   Adding what?
[00:47:50.460 --> 00:47:52.180]   Reading feed reading to Chrome is like you're doing--
[00:47:52.180 --> 00:47:53.500]   RSS feed reading?
[00:47:53.500 --> 00:47:54.860]   I'm assessing Chrome, yeah.
[00:47:54.860 --> 00:47:55.940]   Oh, yeah, I saw this.
[00:47:55.940 --> 00:47:57.940]   What?
[00:47:57.940 --> 00:47:58.940]   That sounds like a joke.
[00:47:58.940 --> 00:48:00.260]   Oh, do tell.
[00:48:00.260 --> 00:48:01.940]   I am interested in that.
[00:48:01.940 --> 00:48:04.740]   Everything old is new again at Google.
[00:48:04.740 --> 00:48:06.540]   They killed Google Reader.
[00:48:06.540 --> 00:48:07.700]   They killed Google Wave.
[00:48:07.700 --> 00:48:09.620]   And now they're bringing them both back.
[00:48:09.620 --> 00:48:12.140]   Closs is next.
[00:48:12.140 --> 00:48:13.580]   I think what's happened is that everyone's
[00:48:13.580 --> 00:48:14.780]   who's old enough to remember is left.
[00:48:14.780 --> 00:48:16.340]   And they've got some new product managers
[00:48:16.340 --> 00:48:18.100]   who come in and they're like--
[00:48:18.100 --> 00:48:21.740]   You know, it'd be really cool is it web pages had some sort of--
[00:48:21.740 --> 00:48:24.340]   I don't know, it could be XML or JSON format
[00:48:24.340 --> 00:48:26.140]   where you could know what was on the page.
[00:48:26.140 --> 00:48:28.460]   And it would get updated whenever there was a new post.
[00:48:28.460 --> 00:48:30.620]   How can we do that?
[00:48:30.620 --> 00:48:31.820]   She's Louise.
[00:48:31.820 --> 00:48:32.820]   You know what I'm--
[00:48:32.820 --> 00:48:34.100]   Log roles.
[00:48:34.100 --> 00:48:35.300]   Yeah, they're coming back to you.
[00:48:35.300 --> 00:48:36.740]   So they're going to put--
[00:48:36.740 --> 00:48:38.220]   you don't need a special app.
[00:48:38.220 --> 00:48:39.860]   You just use Chrome.
[00:48:39.860 --> 00:48:40.620]   Yeah, let me find--
[00:48:40.620 --> 00:48:41.820]   Hang on, let me find the--
[00:48:41.820 --> 00:48:42.500]   That's fascinating.
[00:48:42.500 --> 00:48:45.580]   Yeah, please do throw that in the chat.
[00:48:45.580 --> 00:48:49.500]   Used to be you could actually go to RSS colon slash slash,
[00:48:49.500 --> 00:48:50.020]   I think.
[00:48:50.020 --> 00:48:52.580]   Or maybe not, you just go to the XML.
[00:48:52.580 --> 00:48:54.940]   Let me go to twit.tv.
[00:48:54.940 --> 00:48:57.020]   No, I think that was the far way
[00:48:57.020 --> 00:48:58.620]   that would render and show chroma.
[00:48:58.620 --> 00:49:00.980]   So far, and Firefox both rendered it.
[00:49:00.980 --> 00:49:02.820]   I'm not sure it rendered.
[00:49:02.820 --> 00:49:04.100]   Yeah.
[00:49:04.100 --> 00:49:06.860]   What is our-- is it feeds.xml?
[00:49:06.860 --> 00:49:10.180]   What is our-- I can't remember what our RSS is.
[00:49:10.180 --> 00:49:12.740]   Yeah, well, that didn't work.
[00:49:12.740 --> 00:49:13.580]   I'll find it.
[00:49:13.580 --> 00:49:14.100]   Hang on.
[00:49:14.100 --> 00:49:16.020]   I'll just get the rundown.
[00:49:16.020 --> 00:49:17.220]   That's-- that's fine.
[00:49:17.220 --> 00:49:18.700]   There it is.
[00:49:18.700 --> 00:49:21.780]   Jason has posted it in our IRC.
[00:49:21.780 --> 00:49:25.060]   RSS power-- no, not this.
[00:49:25.060 --> 00:49:27.180]   That's the-- did not find feed.
[00:49:27.180 --> 00:49:31.420]   RSS-- let me see here.
[00:49:31.420 --> 00:49:36.660]   So what-- so they-- they didn't-- well, they said it's a RSS,
[00:49:36.660 --> 00:49:38.980]   but they were very much saying, it's about--
[00:49:38.980 --> 00:49:41.260]   we wanted people to be able to follow people inside Chrome.
[00:49:41.260 --> 00:49:43.340]   Oh, it's a follow button.
[00:49:43.340 --> 00:49:46.780]   Chrome's testing RSS powered follow button
[00:49:46.780 --> 00:49:49.140]   and feed that keeps the Google Reader Dream alive
[00:49:49.140 --> 00:49:52.940]   writes abnormally in 95 Google.
[00:49:52.940 --> 00:49:58.060]   Yeah, so I stuck the actual blog post in from Chrome team.
[00:49:58.060 --> 00:49:58.740]   Yeah, see it.
[00:49:58.740 --> 00:49:59.540]   OK.
[00:49:59.540 --> 00:50:01.860]   From Janice and the Chrome team.
[00:50:01.860 --> 00:50:08.820]   So the-- I guess this is kind of like follow in a podcast app.
[00:50:08.820 --> 00:50:10.420]   It seems that it's like following the podcast
[00:50:10.420 --> 00:50:11.220]   and so follow the feed.
[00:50:11.220 --> 00:50:11.500]   Yeah.
[00:50:11.500 --> 00:50:14.580]   So they've picked up the follow verb from Twitter
[00:50:14.580 --> 00:50:15.940]   and I suppose Facebook has it now too.
[00:50:15.940 --> 00:50:19.380]   So it's a general verb, which is better than subscribe,
[00:50:19.380 --> 00:50:20.300]   really.
[00:50:20.300 --> 00:50:21.340]   It's kind of clearer.
[00:50:21.340 --> 00:50:22.340]   Yeah.
[00:50:22.340 --> 00:50:27.340]   And what happens is you click on this and it adds--
[00:50:27.340 --> 00:50:30.700]   feeds to the blank tab you get when you open Chrome.
[00:50:30.700 --> 00:50:32.340]   So when you open Chrome, you get a blank tab.
[00:50:32.340 --> 00:50:33.340]   That's cool.
[00:50:33.340 --> 00:50:36.340]   There's a little thing next to it that says--
[00:50:36.340 --> 00:50:40.740]   you can see this would see it in the screenshot in the blog post.
[00:50:40.740 --> 00:50:41.340]   Yeah, here it is.
[00:50:41.340 --> 00:50:42.340]   Yeah.
[00:50:42.340 --> 00:50:44.340]   Yeah, that's it.
[00:50:44.340 --> 00:50:45.740]   So there's a little follow up.
[00:50:45.740 --> 00:50:46.540]   You can't quite see it.
[00:50:46.540 --> 00:50:48.340]   It's a bit fuzzy, but there's a little following thing there.
[00:50:48.340 --> 00:50:48.340]   Yeah.
[00:50:48.340 --> 00:50:49.340]   A lot of love.
[00:50:49.340 --> 00:50:50.340]   Yeah.
[00:50:50.340 --> 00:50:52.340]   No, no, no, no, no.
[00:50:52.340 --> 00:50:53.340]   So when you follow that--
[00:50:53.340 --> 00:50:55.340]   This is the new tab page here.
[00:50:55.340 --> 00:50:56.340]   Yeah.
[00:50:56.340 --> 00:50:58.340]   And it has a for you and it also has following.
[00:50:58.340 --> 00:51:00.340]   And these are blogs you followed.
[00:51:00.340 --> 00:51:01.340]   Yeah.
[00:51:01.340 --> 00:51:03.340]   And this is the most recent post in the blog you follow.
[00:51:03.340 --> 00:51:05.340]   Just as it would be in a podcast client.
[00:51:05.340 --> 00:51:08.340]   There's a reverse called "Oddered."
[00:51:08.340 --> 00:51:12.340]   Across the posts, they didn't say they were hang waving about filtering,
[00:51:12.340 --> 00:51:15.340]   but they didn't say they were going to filter anything.
[00:51:15.340 --> 00:51:19.340]   And you click through and you just read the original post in the browser.
[00:51:19.340 --> 00:51:22.340]   So it's like Twitter for posts.
[00:51:22.340 --> 00:51:23.340]   Yeah.
[00:51:23.340 --> 00:51:24.340]   Yeah.
[00:51:24.340 --> 00:51:31.340]   Well, it's like the things that they put on the-- if you swipe left at the front of Android
[00:51:31.340 --> 00:51:33.340]   and you get a bunch of stories there, it's basically that layer.
[00:51:33.340 --> 00:51:34.340]   Oh, I love that.
[00:51:34.340 --> 00:51:35.340]   Yeah.
[00:51:35.340 --> 00:51:37.340]   It's mobile only for now, yeah.
[00:51:37.340 --> 00:51:40.340]   It's going to be on Chrome Canary in two weeks, they said.
[00:51:40.340 --> 00:51:41.340]   So it's not on Chrome Canary.
[00:51:41.340 --> 00:51:43.340]   And then they'll be an experiment.
[00:51:43.340 --> 00:51:44.340]   You can do it.
[00:51:44.340 --> 00:51:46.340]   I'll be curious to see what it does with podcast feeds.
[00:51:46.340 --> 00:51:48.340]   I wonder if we'll put a little play button in there.
[00:51:48.340 --> 00:51:49.340]   That would be great.
[00:51:49.340 --> 00:51:54.340]   They said it wasn't an idea podcast and it's not using the same engine as the podcast engine.
[00:51:54.340 --> 00:51:56.340]   So this is classic people to all of yourself again.
[00:51:56.340 --> 00:51:58.340]   Because obviously, because Google podcasts,
[00:51:58.340 --> 00:52:02.340]   post-tester, I hope Google search and there's subscriptions in there and they've had that.
[00:52:02.340 --> 00:52:05.340]   In a couple of years, Google will announce a Google I/O,
[00:52:05.340 --> 00:52:13.340]   and now you can follow binary files like podcasts, like audio shows.
[00:52:13.340 --> 00:52:15.340]   They will have reinvented the podcast.
[00:52:15.340 --> 00:52:17.340]   It's going to be a very exciting time.
[00:52:17.340 --> 00:52:19.340]   With AI, you forgot the AI.
[00:52:19.340 --> 00:52:21.340]   Oh yeah, I forgot AI and blockchain.
[00:52:21.340 --> 00:52:22.340]   That'll be a minute.
[00:52:22.340 --> 00:52:23.340]   That in there too.
[00:52:23.340 --> 00:52:24.340]   Yeah.
[00:52:24.340 --> 00:52:29.340]   What, so we have yet to find anything we're too excited about.
[00:52:29.340 --> 00:52:33.340]   Stacey was excited about Tizen and we're merging.
[00:52:33.340 --> 00:52:34.340]   Oh, we know.
[00:52:34.340 --> 00:52:36.340]   We promised people I would tell them the other thing.
[00:52:36.340 --> 00:52:37.340]   Oh yeah, what else?
[00:52:37.340 --> 00:52:38.340]   Yeah, yeah, yeah.
[00:52:38.340 --> 00:52:39.340]   Actual future.
[00:52:39.340 --> 00:52:41.340]   Project Starline.
[00:52:41.340 --> 00:52:42.340]   Okay.
[00:52:42.340 --> 00:52:43.340]   You are excited about this.
[00:52:43.340 --> 00:52:44.340]   Oh.
[00:52:44.340 --> 00:52:45.340]   You are so wrong about this.
[00:52:45.340 --> 00:52:46.340]   But go ahead.
[00:52:46.340 --> 00:52:53.340]   I thought it was neat because I think I guess that there's lots of.
[00:52:53.340 --> 00:52:54.340]   It was neat.
[00:52:54.340 --> 00:52:56.340]   I won't deny it.
[00:52:56.340 --> 00:52:59.340]   You, I saw the infrastructure.
[00:52:59.340 --> 00:53:04.340]   There's a lot happening in the background to make there have like no background to standardize this.
[00:53:04.340 --> 00:53:09.340]   But I still thought this was like, oh, it's like a vision of the future where you're like, oh, this could be the future.
[00:53:09.340 --> 00:53:10.340]   That's kind of fun.
[00:53:10.340 --> 00:53:15.340]   So show people it's a 3D rendering or it's a 3D version.
[00:53:15.340 --> 00:53:16.340]   Here's the picture.
[00:53:16.340 --> 00:53:29.340]   So it's using something that's a well-known called a light field technology, which gives you, it's the only way to do 3D without special glasses or headsets.
[00:53:29.340 --> 00:53:34.340]   And I've actually seen it in action at CES last year.
[00:53:34.340 --> 00:53:39.340]   They had a number of people, Sony was doing it, a number of people were doing light field displays.
[00:53:39.340 --> 00:53:42.340]   I hadn't seen that size before.
[00:53:42.340 --> 00:53:44.340]   That has never been this big.
[00:53:44.340 --> 00:53:47.340]   It was much more like a little box you'd put your face in.
[00:53:47.340 --> 00:53:48.340]   Yes.
[00:53:48.340 --> 00:53:49.340]   That's what I've seen like for you.
[00:53:49.340 --> 00:53:50.340]   So that's interesting.
[00:53:50.340 --> 00:53:51.340]   But there's reasons for that.
[00:53:51.340 --> 00:53:53.340]   For one thing, it's not very bright.
[00:53:53.340 --> 00:53:56.340]   So this is misleading.
[00:53:56.340 --> 00:53:59.340]   It would have to be a darkened room if it's going to work.
[00:53:59.340 --> 00:54:06.340]   And because this is a prototype, we don't know how much it's going to cost.
[00:54:06.340 --> 00:54:09.340]   But I think that size, I bet it's not cheap.
[00:54:09.340 --> 00:54:17.340]   Oh, it reminded me of remember when Cisco was launching all their fancy telepresence stuff and you had to go to a hotel to use it.
[00:54:17.340 --> 00:54:18.340]   It reminded me of that.
[00:54:18.340 --> 00:54:20.340]   Yes, light field projection.
[00:54:20.340 --> 00:54:23.340]   But these things come down in price.
[00:54:23.340 --> 00:54:31.340]   It also reminded me of the way people were talking to someone who's in jail through the clear plexiglass, where you're on one side with the phone.
[00:54:31.340 --> 00:54:32.340]   It did.
[00:54:32.340 --> 00:54:35.340]   But it was still neat.
[00:54:35.340 --> 00:54:41.340]   And I get excited about commercialization of technology that could do something like this.
[00:54:41.340 --> 00:54:44.340]   Especially as, you know, yeah.
[00:54:44.340 --> 00:54:48.340]   So the only reason I'm skeptical is we've seen a lot of attempts to do these kinds of things.
[00:54:48.340 --> 00:54:50.340]   And they're always disappointing in the long run.
[00:54:50.340 --> 00:54:51.340]   It's probably very expensive.
[00:54:51.340 --> 00:54:54.340]   And it's only marginally better than a screen.
[00:54:54.340 --> 00:55:02.340]   The fact that you could kind of turn your head and see a little of a 3D effect does not make it somehow magical.
[00:55:02.340 --> 00:55:04.340]   It is not the 10x thing.
[00:55:04.340 --> 00:55:07.340]   But I will say we had thought about video call.
[00:55:07.340 --> 00:55:11.340]   I mean, AT&T had a video phone back in like 1960 in.
[00:55:11.340 --> 00:55:13.340]   Yeah, that really took off.
[00:55:13.340 --> 00:55:17.340]   It took a long time, but we're there now.
[00:55:17.340 --> 00:55:18.340]   Think about this.
[00:55:18.340 --> 00:55:23.340]   I saw that too at the World's Fair in 1965 in the New York World's Fair.
[00:55:23.340 --> 00:55:24.340]   And here it is.
[00:55:24.340 --> 00:55:26.340]   Let me do some quick math.
[00:55:26.340 --> 00:55:29.340]   55 years later.
[00:55:29.340 --> 00:55:30.340]   And then we argue a little sooner.
[00:55:30.340 --> 00:55:34.340]   This is the perfect Stacey Optimus versus grumpy Leo.
[00:55:34.340 --> 00:55:35.340]   I've turned it to aachronic.
[00:55:35.340 --> 00:55:37.340]   Yeah, this is the right here.
[00:55:37.340 --> 00:55:43.340]   We have a 65 inch light field display is Google expensive, I think.
[00:55:43.340 --> 00:55:44.340]   But maybe not.
[00:55:44.340 --> 00:55:45.340]   I think you're right.
[00:55:45.340 --> 00:55:47.340]   I just remember you have to have one at both.
[00:55:47.340 --> 00:55:51.340]   I think that normalizes new technology I get excited about.
[00:55:51.340 --> 00:55:52.340]   Right.
[00:55:52.340 --> 00:55:54.340]   You do need one at both ends.
[00:55:54.340 --> 00:55:55.340]   And a piece of work.
[00:55:55.340 --> 00:55:56.340]   Yes, Kevin and I were talking about it.
[00:55:56.340 --> 00:55:58.340]   He's like, I just got a fax machine.
[00:55:58.340 --> 00:55:59.340]   And I'm like, haha.
[00:55:59.340 --> 00:56:04.340]   It's being transmitted over WebRTC.
[00:56:04.340 --> 00:56:07.340]   I don't know how you feel about WebRTC Kevin Marks, but I'm.
[00:56:07.340 --> 00:56:08.340]   It's not too bad.
[00:56:08.340 --> 00:56:10.340]   I mean, it's not too bad.
[00:56:10.340 --> 00:56:12.340]   There you have it.
[00:56:12.340 --> 00:56:17.340]   It's standardized.
[00:56:17.340 --> 00:56:20.340]   20 years ago, when we were doing it for the first time, this stuff was hard.
[00:56:20.340 --> 00:56:22.340]   It's now everyone's doing it.
[00:56:22.340 --> 00:56:25.340]   And they're trying to sign and say it was like, oh, I haven't got that video chat app.
[00:56:25.340 --> 00:56:27.340]   We've got six different ones now.
[00:56:27.340 --> 00:56:28.340]   Yeah.
[00:56:28.340 --> 00:56:29.340]   Because we pretty much standardized it.
[00:56:29.340 --> 00:56:30.340]   And we now have to do it.
[00:56:30.340 --> 00:56:33.340]   So I'm, I'm comfortable with the opportunity of that.
[00:56:33.340 --> 00:56:35.340]   It's just that there's going to be checking a bit more data over it.
[00:56:35.340 --> 00:56:36.340]   So they're going to have data issues.
[00:56:36.340 --> 00:56:39.340]   But I suspect what they're primarily doing is using this for, you know,
[00:56:39.340 --> 00:56:41.340]   into Google conference, conferencing.
[00:56:41.340 --> 00:56:42.340]   Lauren Goode did great.
[00:56:42.340 --> 00:56:43.340]   They're actually not using it.
[00:56:43.340 --> 00:56:45.340]   They're not just playing with it.
[00:56:45.340 --> 00:56:46.340]   They're just showing.
[00:56:46.340 --> 00:56:47.340]   Lauren Goode got to try it.
[00:56:47.340 --> 00:56:49.340]   She's writing and wired about it.
[00:56:49.340 --> 00:56:50.340]   And she said it.
[00:56:50.340 --> 00:56:52.340]   It was pretty amazing.
[00:56:52.340 --> 00:56:57.340]   So, you know, she actually, I met with three separate Googlers and Project Starline, all
[00:56:57.340 --> 00:56:58.340]   of them men.
[00:56:58.340 --> 00:57:04.340]   And some of the sera ser reality faded each time I shifted to my seat, moved to the side,
[00:57:04.340 --> 00:57:05.340]   just a few inches.
[00:57:05.340 --> 00:57:07.340]   The illusion of volume disappears.
[00:57:07.340 --> 00:57:11.140]   Suddenly you're looking at a 2D version of your video chat partner again, similar to
[00:57:11.140 --> 00:57:15.300]   the way a sporting event looks great on your big screen TV until you move too far off the
[00:57:15.300 --> 00:57:16.900]   sweet spot spot.
[00:57:16.900 --> 00:57:22.140]   There were also, and I think this is WebRTC, a few random artifacts fluttering on screen,
[00:57:22.140 --> 00:57:24.140]   broken bits that served as occasional reminders.
[00:57:24.140 --> 00:57:27.020]   The person in front of me was not really there.
[00:57:27.020 --> 00:57:32.380]   And when one of the demonstrators started casting a web page under the light field display
[00:57:32.380 --> 00:57:36.900]   as an example of how two people collaborate in Starline, we both just stared over each
[00:57:36.900 --> 00:57:42.980]   other's right shoulder at a not quite interactive page.
[00:57:42.980 --> 00:57:44.820]   There seems to be work to be done.
[00:57:44.820 --> 00:57:46.540]   She asked, "Will it take five more years?"
[00:57:46.540 --> 00:57:54.060]   The Google men did not seem inclined to answer.
[00:57:54.060 --> 00:57:57.060]   Google says around 100 employees have used Starline.
[00:57:57.060 --> 00:58:02.620]   It has been tucked away in secret offices in Mountain View, Seattle and New York.
[00:58:02.620 --> 00:58:03.620]   You know.
[00:58:03.620 --> 00:58:04.620]   I could go play with it.
[00:58:04.620 --> 00:58:05.620]   Yeah.
[00:58:05.620 --> 00:58:10.380]   I'm sure we could get you an invite.
[00:58:10.380 --> 00:58:12.420]   I'm not against it.
[00:58:12.420 --> 00:58:16.140]   I just think that remember, this is prototype.
[00:58:16.140 --> 00:58:19.380]   This is not a Google announcement of a product at any point.
[00:58:19.380 --> 00:58:23.300]   Well, look, I don't feel like I'm going to die within the next 20 years.
[00:58:23.300 --> 00:58:25.500]   So I'm excited that maybe that's my problem.
[00:58:25.500 --> 00:58:26.500]   Maybe that's my problem.
[00:58:26.500 --> 00:58:27.500]   Maybe that's my problem.
[00:58:27.500 --> 00:58:28.500]   I'll be okay and see this.
[00:58:28.500 --> 00:58:29.500]   Yeah.
[00:58:29.500 --> 00:58:30.500]   We're just pissed.
[00:58:30.500 --> 00:58:31.500]   We're not going to get it.
[00:58:31.500 --> 00:58:32.500]   I'll be dead by the time that comes out.
[00:58:32.500 --> 00:58:33.500]   So screw it.
[00:58:33.500 --> 00:58:41.820]   Did you see anything that we have overlooked that was really exciting, Kevin?
[00:58:41.820 --> 00:58:44.820]   Um, let me think.
[00:58:44.820 --> 00:58:50.620]   I mean, a lot of it was retelling the things that we really knew about.
[00:58:50.620 --> 00:58:51.620]   Yeah.
[00:58:51.620 --> 00:58:52.620]   So it was, it was thought.
[00:58:52.620 --> 00:58:55.620]   And it was a little bit, because it didn't seem that themed.
[00:58:55.620 --> 00:58:56.620]   It seemed a bit Conway's orish.
[00:58:56.620 --> 00:58:58.700]   It was like, "Here's the guy in charge of this department.
[00:58:58.700 --> 00:59:00.420]   He's got near his, his demo."
[00:59:00.420 --> 00:59:01.580]   It totally felt like that.
[00:59:01.580 --> 00:59:02.820]   I even said that, didn't I, Jeff?
[00:59:02.820 --> 00:59:06.700]   I said, "They sent out a memo to all the departments that he wants to present at Google
[00:59:06.700 --> 00:59:11.140]   I/O and then they had a lottery or maybe they had a, you know, they had a battle.
[00:59:11.140 --> 00:59:14.700]   It was really hell and the survivors got to present."
[00:59:14.700 --> 00:59:16.980]   Because the Apple keynotes were very much that.
[00:59:16.980 --> 00:59:20.820]   The Apple keynotes were like, "Okay, bring with your best demos and if they're good enough,
[00:59:20.820 --> 00:59:21.820]   Steve will do them."
[00:59:21.820 --> 00:59:27.780]   But yeah, but they were in, and the difference is Apple was announcing products.
[00:59:27.780 --> 00:59:29.140]   Google was not announcing products.
[00:59:29.140 --> 00:59:30.140]   They're just talking about their work.
[00:59:30.140 --> 00:59:31.140]   It was over the outside.
[00:59:31.140 --> 00:59:32.140]   Thoughts.
[00:59:32.140 --> 00:59:36.460]   Yeah, I mean, the Apple wasn't, you know, WDC, the developer day.
[00:59:36.460 --> 00:59:40.860]   I mean, they announced hardware there now, but the developer thing was, "Hey, here's what
[00:59:40.860 --> 00:59:43.020]   we built this year for you to go and do with apps on."
[00:59:43.020 --> 00:59:47.300]   So what you're saying is this is like an Apple keynotes from 20 years ago.
[00:59:47.300 --> 00:59:52.500]   Except that it's not as coherently themed, you know.
[00:59:52.500 --> 00:59:54.460]   They're thinking in what's not suitable.
[00:59:54.460 --> 00:59:56.060]   Yeah, I mean, there was a thing.
[00:59:56.060 --> 01:00:00.540]   The thing was, AI says makes things better and we're working on AI and we're trying to
[01:00:00.540 --> 01:00:04.100]   be frightened about it and we're not going to mention the elephant in the room about
[01:00:04.100 --> 01:00:05.100]   the people who've quickly, who are not.
[01:00:05.100 --> 01:00:08.500]   They may know mention of the AI ethics issues, Tim Nick, Gabriel.
[01:00:08.500 --> 01:00:12.580]   No, no, they mentioned it, but they didn't mention the people who worked on it.
[01:00:12.580 --> 01:00:16.300]   They didn't mention about improving things for improving.
[01:00:16.300 --> 01:00:18.860]   It's part of that, do you think?
[01:00:18.860 --> 01:00:20.420]   Camera's right, people's right.
[01:00:20.420 --> 01:00:24.700]   Because AI advances are implemented in software.
[01:00:24.700 --> 01:00:27.020]   So one, you don't get a lot of shiny new devices.
[01:00:27.020 --> 01:00:32.380]   But two, AI advances are usually fairly incremental, right?
[01:00:32.380 --> 01:00:41.020]   So I'm just wondering if we're going to enter into this era of math features.
[01:00:41.020 --> 01:00:47.820]   Casey Newton called this the biggest changes to Docs in a decade.
[01:00:47.820 --> 01:00:50.740]   He was quite excited.
[01:00:50.740 --> 01:00:53.020]   And he's also said it may not be enough.
[01:00:53.020 --> 01:00:58.700]   Docs has not been a sexy glitzy product for Google ever, probably.
[01:00:58.700 --> 01:00:59.700]   We want it to be a sexy.
[01:00:59.700 --> 01:01:00.700]   No, it was good enough.
[01:01:00.700 --> 01:01:03.100]   We want to be, we got the job to.
[01:01:03.100 --> 01:01:04.100]   It's authoritarian.
[01:01:04.100 --> 01:01:05.100]   Yeah.
[01:01:05.100 --> 01:01:06.100]   Well, I think that was the point.
[01:01:06.100 --> 01:01:08.580]   I mean, the thing about Wave was that Wave was, oh, we're going to do it a whole new way.
[01:01:08.580 --> 01:01:12.660]   And Docs was, okay, we quite like the editing together in parallel theme.
[01:01:12.660 --> 01:01:14.300]   We'll take that and we'll keep the doc in model.
[01:01:14.300 --> 01:01:16.420]   We've already got.
[01:01:16.420 --> 01:01:22.700]   And a big chunk, Docs used to be a bit more radical in that it was very web-like and it
[01:01:22.700 --> 01:01:26.660]   got more document-like over time as they showed it to more Microsoft users and Microsoft users
[01:01:26.660 --> 01:01:30.060]   that said, I haven't got this little thing I wanted.
[01:01:30.060 --> 01:01:34.500]   And of course, Microsoft Office now has caught up with Docs pretty much.
[01:01:34.500 --> 01:01:39.020]   It's not quite as clean, but you can edit in Word and it'll appear on the web to other
[01:01:39.020 --> 01:01:41.500]   people and the cursors will appear and stuff will work.
[01:01:41.500 --> 01:01:44.220]   And it puts little warnings up saying, don't join it the same thing.
[01:01:44.220 --> 01:01:45.220]   Bouncy's will happen.
[01:01:45.220 --> 01:01:47.460]   But they're sort of there.
[01:01:47.460 --> 01:01:52.140]   So I think Google said, okay, what can we do that rebuilds this in a different way?
[01:01:52.140 --> 01:01:53.980]   What can we bring in that we've already got?
[01:01:53.980 --> 01:01:55.580]   And I think part of it is what you said.
[01:01:55.580 --> 01:02:00.700]   It's also them, it's solving the problem that they have, which is we're trying to collaborate
[01:02:00.700 --> 01:02:04.780]   with eight people across six time zones to try and agree that the product release, which
[01:02:04.780 --> 01:02:08.180]   is like, okay, that's the problem that many of us have.
[01:02:08.180 --> 01:02:11.420]   I mean, okay, I guess we're across four time zones here and we're trying to get a conversation
[01:02:11.420 --> 01:02:12.820]   about stuff in a spreadsheet.
[01:02:12.820 --> 01:02:15.220]   So I guess we're doing it too.
[01:02:15.220 --> 01:02:18.220]   But the particulars of the demo will talk spreadsheet.
[01:02:18.220 --> 01:02:21.980]   I just want to make sure you.
[01:02:21.980 --> 01:02:27.620]   Yeah, I've been trying to get that moved over to Notion or some other more modern thing.
[01:02:27.620 --> 01:02:31.220]   That's one of the problems Casey raises is, you know, Google here has to deal with the
[01:02:31.220 --> 01:02:37.140]   picky Eun, he uses that word, the picky Eun needs of Microsoft Office users.
[01:02:37.140 --> 01:02:41.340]   And so it kind of unfortunately keeps them holds it back down.
[01:02:41.340 --> 01:02:46.940]   Whereas there are a lot of startups like Rome and Notion, you know, and I can go on and
[01:02:46.940 --> 01:02:50.140]   on that are doing really interesting things with.
[01:02:50.140 --> 01:02:51.820]   Like I get such tool fatigue.
[01:02:51.820 --> 01:02:55.780]   I've got one colleague who is brilliant, absolutely brilliant, annoying every tool.
[01:02:55.780 --> 01:02:58.380]   But he presents 50 tools to the faculty.
[01:02:58.380 --> 01:02:59.860]   We all our heads explode.
[01:02:59.860 --> 01:03:02.100]   We just give stocks again.
[01:03:02.100 --> 01:03:03.660]   That's what happens, I think.
[01:03:03.660 --> 01:03:04.660]   I think you're right.
[01:03:04.660 --> 01:03:08.860]   I mean, air table valued at $5.77 billion.
[01:03:08.860 --> 01:03:11.060]   Notion raised $50 million.
[01:03:11.060 --> 01:03:14.140]   Coda raised money in a value of $600 million.
[01:03:14.140 --> 01:03:18.220]   These are all companies that are innovating because they don't have the innovators dilemma.
[01:03:18.220 --> 01:03:21.620]   They don't have to preserve any incumbency at all.
[01:03:21.620 --> 01:03:25.140]   They're just, they can start with the place like it's, I mean, it's Google was preserving
[01:03:25.140 --> 01:03:26.980]   Microsoft's incumbency, to some extent.
[01:03:26.980 --> 01:03:29.580]   You know, Docs ended up with a volatius button.
[01:03:29.580 --> 01:03:33.340]   Originally, because people didn't, we were scared that it was mad.
[01:03:33.340 --> 01:03:34.340]   So sad.
[01:03:34.340 --> 01:03:36.740]   But no, but that was it.
[01:03:36.740 --> 01:03:39.300]   They built this thing that's like, we're going to edit stuff on the web and they gradually
[01:03:39.300 --> 01:03:41.580]   converged on office because everyone was used to office.
[01:03:41.580 --> 01:03:42.580]   Right.
[01:03:42.580 --> 01:03:47.020]   And I've been editing a long document in Word this week.
[01:03:47.020 --> 01:03:48.420]   I forgot how annoying it was.
[01:03:48.420 --> 01:03:54.100]   And it's still like, I'm switching between outline mode and print view mode and web view
[01:03:54.100 --> 01:03:57.100]   mode and the images disappearing outline mode and the.
[01:03:57.100 --> 01:03:59.820]   Kevin, have you tried any of the more modern tools?
[01:03:59.820 --> 01:04:02.260]   I've been using Rome and Notion.
[01:04:02.260 --> 01:04:03.260]   I've tried Rome.
[01:04:03.260 --> 01:04:04.260]   I've seen it.
[01:04:04.260 --> 01:04:05.260]   I've seen it.
[01:04:05.260 --> 01:04:06.260]   What is it?
[01:04:06.260 --> 01:04:07.260]   Miro.
[01:04:07.260 --> 01:04:08.500]   That's something I'm going to check, mention that as well.
[01:04:08.500 --> 01:04:10.100]   That's a sort of collaborative canvas thing.
[01:04:10.100 --> 01:04:14.380]   So if you were doing like post, it's on the whiteboard type of collaboration.
[01:04:14.380 --> 01:04:15.860]   It's designed for doing that.
[01:04:15.860 --> 01:04:20.100]   So it's much more kind of graphical whiteboard, whiteboardy.
[01:04:20.100 --> 01:04:21.100]   Yeah.
[01:04:21.100 --> 01:04:22.100]   Yeah.
[01:04:22.100 --> 01:04:23.100]   Yeah.
[01:04:23.100 --> 01:04:26.980]   So you were talking about, it's like multiplayer Prezi.
[01:04:26.980 --> 01:04:30.580]   You're basically arranging visual things in a space.
[01:04:30.580 --> 01:04:31.900]   Here, you could show it, John.
[01:04:31.900 --> 01:04:33.860]   I have it on the screen here.
[01:04:33.860 --> 01:04:34.860]   Yeah.
[01:04:34.860 --> 01:04:35.860]   Yeah.
[01:04:35.860 --> 01:04:41.500]   So you'll get to be in the box and move things around and get in collaborate on a.
[01:04:41.500 --> 01:04:44.860]   This actually looks like Google demonstrated to be honest with you.
[01:04:44.860 --> 01:04:45.860]   Yeah.
[01:04:45.860 --> 01:04:47.420]   No, it may be a bit, maybe a bit like that.
[01:04:47.420 --> 01:04:49.980]   They may have learned something from this, but it's very canvassy.
[01:04:49.980 --> 01:04:54.540]   So what I've done with this, I've used it in, you know, the sort of places where you'd
[01:04:54.540 --> 01:04:58.220]   have the big meeting with the company and you put posters on a whiteboard and come up
[01:04:58.220 --> 01:04:59.220]   with stress.
[01:04:59.220 --> 01:05:01.460]   This would be less desirable for a text.
[01:05:01.460 --> 01:05:03.860]   A present to you.
[01:05:03.860 --> 01:05:04.860]   Yes.
[01:05:04.860 --> 01:05:05.860]   You wouldn't want to write in this.
[01:05:05.860 --> 01:05:06.860]   No, you wouldn't.
[01:05:06.860 --> 01:05:09.020]   No, this is more like, I've got a bunch of ideas.
[01:05:09.020 --> 01:05:10.020]   We're trying to arrange them into a thing.
[01:05:10.020 --> 01:05:11.500]   Then you go away and write it up.
[01:05:11.500 --> 01:05:17.180]   So that kind of spatial brainstorming thing that we were talking about with Prezi as well.
[01:05:17.180 --> 01:05:18.180]   Yeah.
[01:05:18.180 --> 01:05:20.020]   You know, what's the least thing that's replicate the thing is I hate.
[01:05:20.020 --> 01:05:22.020]   I have come to hate post it.
[01:05:22.020 --> 01:05:23.020]   Don't be that.
[01:05:23.020 --> 01:05:24.020]   I see post it notes and I run.
[01:05:24.020 --> 01:05:25.020]   Yeah.
[01:05:25.020 --> 01:05:26.020]   Oh, no brainstorming time.
[01:05:26.020 --> 01:05:27.020]   Design thinking.
[01:05:27.020 --> 01:05:28.020]   Breakout time.
[01:05:28.020 --> 01:05:30.820]   How do you plot out your books, Jeff?
[01:05:30.820 --> 01:05:32.780]   Oh, Stacy.
[01:05:32.780 --> 01:05:33.780]   How do you?
[01:05:33.780 --> 01:05:35.820]   Actually, that's a good question.
[01:05:35.820 --> 01:05:38.360]   Do you use an outliner, Jeff, at least?
[01:05:38.360 --> 01:05:39.360]   Nope.
[01:05:39.360 --> 01:05:40.360]   Nope.
[01:05:40.360 --> 01:05:41.360]   Nope.
[01:05:41.360 --> 01:05:42.360]   They pile a whiteboards.
[01:05:42.360 --> 01:05:43.360]   Yeah.
[01:05:43.360 --> 01:05:44.360]   Yeah.
[01:05:44.360 --> 01:05:48.440]   I use whiteboards, but the function of the whiteboard is kind of like the whole of the
[01:05:48.440 --> 01:05:49.440]   post notes.
[01:05:49.440 --> 01:05:50.440]   Yeah.
[01:05:50.440 --> 01:05:54.200]   Well, like this is an idea I care about.
[01:05:54.200 --> 01:05:55.840]   This is an example of that idea.
[01:05:55.840 --> 01:05:58.320]   You know, lips, shirts, throw in things up there.
[01:05:58.320 --> 01:05:59.320]   Yeah.
[01:05:59.320 --> 01:06:00.320]   And then you.
[01:06:00.320 --> 01:06:01.320]   But that's the point is, yeah, you arrange them spatially.
[01:06:01.320 --> 01:06:04.760]   So you write things or index cards or post it and spread them around and they say, oh,
[01:06:04.760 --> 01:06:06.120]   these ones are going together.
[01:06:06.120 --> 01:06:10.240]   And doing that kind of thing is actually quite hard in a word processor type in face.
[01:06:10.240 --> 01:06:13.640]   I think outliners do a very good job.
[01:06:13.640 --> 01:06:15.440]   Dave, Dave Weiner, the creative.
[01:06:15.440 --> 01:06:20.840]   Yeah, Dave Weiner, the creative RSS is, you know, the kind of prototypical outliner guy
[01:06:20.840 --> 01:06:26.280]   invented all of the best outliners, including more going way back to went on the Macintosh.
[01:06:26.280 --> 01:06:28.480]   I mean, they're okay, but they're very hierarchical.
[01:06:28.480 --> 01:06:29.480]   That's the thing.
[01:06:29.480 --> 01:06:31.080]   You can put things in some other things and collapse.
[01:06:31.080 --> 01:06:33.520]   You can drag things around though.
[01:06:33.520 --> 01:06:35.520]   But you can't put them off to one side.
[01:06:35.520 --> 01:06:36.520]   Side to side.
[01:06:36.520 --> 01:06:37.520]   Yeah.
[01:06:37.520 --> 01:06:42.480]   So the thing about arranging index cards or the thing and saying, okay, these ones definitely
[01:06:42.480 --> 01:06:43.480]   belong together.
[01:06:43.480 --> 01:06:45.120]   I'm not sure which order this story goes in.
[01:06:45.120 --> 01:06:46.200]   Oh, that one goes there.
[01:06:46.200 --> 01:06:52.800]   Sort of the storyboarding thing or the, and that that kind of arranging the ideas part
[01:06:52.800 --> 01:06:54.800]   is easier with the spatial thing.
[01:06:54.800 --> 01:06:56.760]   And so that's why I like notion.
[01:06:56.760 --> 01:06:58.160]   It's why I like pressy as well.
[01:06:58.160 --> 01:06:59.440]   Prishing is good for that too.
[01:06:59.440 --> 01:07:01.160]   Notion is another, but this is what happens.
[01:07:01.160 --> 01:07:02.160]   You get tool fatigue.
[01:07:02.160 --> 01:07:05.800]   Notion is another way to do that kind of thing with cards.
[01:07:05.800 --> 01:07:11.520]   And it's pretty and you can drag stuff around, but you can also dig into it.
[01:07:11.520 --> 01:07:13.880]   And so it can be, you can get text.
[01:07:13.880 --> 01:07:14.880]   You can also.
[01:07:14.880 --> 01:07:17.680]   But can you put three cards on the screen and put them side by side?
[01:07:17.680 --> 01:07:19.360]   Yeah, totally.
[01:07:19.360 --> 01:07:20.360]   So here's an example.
[01:07:20.360 --> 01:07:22.600]   I use whiteboard eraser and arrows.
[01:07:22.600 --> 01:07:25.000]   Oh, no, that should be up there.
[01:07:25.000 --> 01:07:28.160]   Yeah, I can't do that free form annotation.
[01:07:28.160 --> 01:07:29.960]   You're not doing that.
[01:07:29.960 --> 01:07:33.600]   What was the other one I was playing with the other day?
[01:07:33.600 --> 01:07:36.200]   So here's, so here's, here's a, this is a trip.
[01:07:36.200 --> 01:07:37.200]   So many things different.
[01:07:37.200 --> 01:07:39.400]   We've taken in a text list, but it's a table.
[01:07:39.400 --> 01:07:41.400]   But then you can also do cards.
[01:07:41.400 --> 01:07:44.640]   Yeah, everybody has a different way of doing this, I guess.
[01:07:44.640 --> 01:07:45.800]   This isn't, this is Notion.
[01:07:45.800 --> 01:07:47.600]   It's not the perfect tool.
[01:07:47.600 --> 01:07:52.560]   But I think there's really interesting stuff being done, frankly, in these tools.
[01:07:52.560 --> 01:07:53.560]   Oh, yeah.
[01:07:53.560 --> 01:07:55.600]   Rome research is really interesting too.
[01:07:55.600 --> 01:07:58.160]   I don't know if you've played with that at all, but these are all.
[01:07:58.160 --> 01:08:01.400]   Is it Rome like as in moving or Rome is in the city?
[01:08:01.400 --> 01:08:03.200]   Oh, I am.
[01:08:03.200 --> 01:08:06.120]   And it's a note-taking tool.
[01:08:06.120 --> 01:08:12.100]   It has a, you know, it really is and Kevin will recognize this immediately.
[01:08:12.100 --> 01:08:15.880]   It's a wiki, but it's a wiki with backlinks.
[01:08:15.880 --> 01:08:18.700]   And I mean, if I say that it minimizes it.
[01:08:18.700 --> 01:08:23.300]   It's really a note-taking tool, but because it supports backlinks, it ends up being kind
[01:08:23.300 --> 01:08:24.880]   of a wiki in effect.
[01:08:24.880 --> 01:08:26.480]   It's very powerful.
[01:08:26.480 --> 01:08:29.200]   And they're people doing some amazing things with it as well.
[01:08:29.200 --> 01:08:33.160]   So that's what I'm saying is that, and that's what Casey Newton was saying, is it's going
[01:08:33.160 --> 01:08:40.040]   to be tough for Google as an incumbent, especially an incumbent that has to appease Microsoft
[01:08:40.040 --> 01:08:41.440]   word users.
[01:08:41.440 --> 01:08:43.080]   Is that where this new model works though?
[01:08:43.080 --> 01:08:46.560]   You can embed the Microsoft Word document in your Google document.
[01:08:46.560 --> 01:08:48.240]   Fine, it's there.
[01:08:48.240 --> 01:08:49.240]   You can see it.
[01:08:49.240 --> 01:08:52.160]   You can change it, you can get to it, but you can have all this fancy stuff around it.
[01:08:52.160 --> 01:08:53.160]   Yeah.
[01:08:53.160 --> 01:08:54.960]   I think I like the model.
[01:08:54.960 --> 01:08:56.800]   I thought Wave was great too though.
[01:08:56.800 --> 01:08:57.960]   Yeah, I did too.
[01:08:57.960 --> 01:09:00.160]   I thought Wave was changing the way we work.
[01:09:00.160 --> 01:09:01.160]   Yeah.
[01:09:01.160 --> 01:09:04.040]   Well, so you just got to wait till it comes around to that, don't I mean?
[01:09:04.040 --> 01:09:05.800]   It could have been ahead of its time.
[01:09:05.800 --> 01:09:06.800]   Yeah.
[01:09:06.800 --> 01:09:08.280]   Yeah, I think it was.
[01:09:08.280 --> 01:09:11.000]   So I stuck another link in the chat.
[01:09:11.000 --> 01:09:16.920]   See, this is how we would work if we were in a collaborative environment.
[01:09:16.920 --> 01:09:20.560]   You would just put that on the show and it would appear in the screen.
[01:09:20.560 --> 01:09:21.560]   Yeah.
[01:09:21.560 --> 01:09:26.640]   I'm really, actually really intrigued by the idea of doing the kinds of shows we do in
[01:09:26.640 --> 01:09:31.200]   a much more kind of, you know, with text and images.
[01:09:31.200 --> 01:09:34.120]   So there's another screen here that we can cut to.
[01:09:34.120 --> 01:09:35.480]   Yeah, kind of an integrated environment.
[01:09:35.480 --> 01:09:37.960]   So what we do in class, right?
[01:09:37.960 --> 01:09:39.760]   Yeah, yeah.
[01:09:39.760 --> 01:09:42.760]   Or your dude that you had on, he did Evernote.
[01:09:42.760 --> 01:09:43.760]   What's his name, Phil?
[01:09:43.760 --> 01:09:44.760]   Phil Libbon.
[01:09:44.760 --> 01:09:45.760]   Yeah.
[01:09:45.760 --> 01:09:49.960]   In fact, Maggie in her article mentions, this is her article at MaggieAppleton.com, the
[01:09:49.960 --> 01:09:53.520]   linear oppression of note-taking apps.
[01:09:53.520 --> 01:09:58.640]   She mentions Evernote, but also Notion, also Rome, also OneNote Bear in Obsidian, all of
[01:09:58.640 --> 01:10:01.200]   which I used and have used.
[01:10:01.200 --> 01:10:06.360]   I was thinking about his next thing with the video cameras where you can throw up the virtual
[01:10:06.360 --> 01:10:13.360]   camera so you can throw up things like, yeah, like you do on a TV news show, but you could
[01:10:13.360 --> 01:10:15.920]   actually generate that into the actual performance.
[01:10:15.920 --> 01:10:20.160]   You like the, the Dakota of Maggie's blog post, Jeff.
[01:10:20.160 --> 01:10:23.160]   Surely Gutenberg has something to answer for.
[01:10:23.160 --> 01:10:24.160]   Yeah.
[01:10:24.160 --> 01:10:27.160]   Who's to blame for our linear textual predicament?
[01:10:27.160 --> 01:10:30.200]   The clue in the line.
[01:10:30.200 --> 01:10:31.920]   This sentence is an example of it.
[01:10:31.920 --> 01:10:32.920]   Yeah.
[01:10:32.920 --> 01:10:33.920]   Because our organizing principle.
[01:10:33.920 --> 01:10:34.920]   Yeah.
[01:10:34.920 --> 01:10:35.920]   It's called Gutenberg.
[01:10:35.920 --> 01:10:40.720]   So that, and that goes back to our original conversation, Stacey, of is nature a nurture?
[01:10:40.720 --> 01:10:46.160]   Is it, is it learned or is it representing what our, how our brain actually works?
[01:10:46.160 --> 01:10:48.200]   Or let's get us our favorite gift.
[01:10:48.200 --> 01:10:49.200]   Why not, Bill?
[01:10:49.200 --> 01:10:57.120]   I do feel like the thing that is most interesting, and I'm sure Kevin, you feel the same way,
[01:10:57.120 --> 01:11:01.480]   about the technological world we're living in, is that it does kind of take the cards
[01:11:01.480 --> 01:11:05.280]   and throw them up in the air and say, well, it doesn't have to be this way.
[01:11:05.280 --> 01:11:11.120]   You know, it doesn't have to be a line of type or, you know, a line of, I know, I know,
[01:11:11.120 --> 01:11:12.280]   Jeff.
[01:11:12.280 --> 01:11:13.520]   It can be something else.
[01:11:13.520 --> 01:11:18.240]   But what ironically we're still, I mean, honestly, we're still doing typewriters.
[01:11:18.240 --> 01:11:19.240]   We just did you.
[01:11:19.240 --> 01:11:20.240]   That's exactly the point.
[01:11:20.240 --> 01:11:21.240]   Yes.
[01:11:21.240 --> 01:11:22.240]   And what are we doing on our phones?
[01:11:22.240 --> 01:11:23.240]   Right?
[01:11:23.240 --> 01:11:26.880]   We're doing, we're doing, we're trying to replicate the keyboard sense.
[01:11:26.880 --> 01:11:30.520]   So maybe it's time to think outside the box a little bit, except the human brain is so
[01:11:30.520 --> 01:11:32.920]   boxed that you can.
[01:11:32.920 --> 01:11:35.680]   That's why the, outside the court.
[01:11:35.680 --> 01:11:36.680]   Outside the court.
[01:11:36.680 --> 01:11:37.680]   Yeah.
[01:11:37.680 --> 01:11:39.840]   That's why Stacey's daughter is the key person here.
[01:11:39.840 --> 01:11:40.840]   You know, that's right.
[01:11:40.840 --> 01:11:46.400]   I said yet where we watched yesterday, what struck me was that Google is the language
[01:11:46.400 --> 01:11:49.240]   company more than any other company.
[01:11:49.240 --> 01:11:51.840]   It knows how to try to understand the word words.
[01:11:51.840 --> 01:11:52.840]   It's about words.
[01:11:52.840 --> 01:11:53.840]   Yes.
[01:11:53.840 --> 01:11:59.080]   Translate that into other languages, into actions, into conversations.
[01:11:59.080 --> 01:12:00.960]   It is really a language company.
[01:12:00.960 --> 01:12:03.760]   Words are the atom of Google's magic matter.
[01:12:03.760 --> 01:12:04.760]   Yeah.
[01:12:04.760 --> 01:12:10.320]   Things they would do images to that they've done a lot of work with processing partly to
[01:12:10.320 --> 01:12:14.640]   attach words to them, you know, and they can be a little bit literal.
[01:12:14.640 --> 01:12:15.640]   I was searching for a picture.
[01:12:15.640 --> 01:12:18.040]   I was searching for my son.
[01:12:18.040 --> 01:12:21.120]   So I searched for Andrew and Christopher at the beach in front of the picture.
[01:12:21.120 --> 01:12:23.160]   I said, Andrew and Christopher at the sea.
[01:12:23.160 --> 01:12:26.240]   And then it found the picture because the one they wanted, there wasn't any sand in
[01:12:26.240 --> 01:12:27.240]   it.
[01:12:27.240 --> 01:12:28.640]   So it didn't count as a beach.
[01:12:28.640 --> 01:12:31.920]   So it's interesting.
[01:12:31.920 --> 01:12:32.920]   That's really interesting.
[01:12:32.920 --> 01:12:35.480]   Google has clearly never been to the Pacific Northwest.
[01:12:35.480 --> 01:12:36.480]   Look at this too.
[01:12:36.480 --> 01:12:37.480]   All the beaches are right.
[01:12:37.480 --> 01:12:38.480]   Yeah.
[01:12:38.480 --> 01:12:39.720]   There's no sand on those beach.
[01:12:39.720 --> 01:12:41.880]   This is something Kevin put in the check.
[01:12:41.880 --> 01:12:46.240]   This is keynote.club K-I-N-O-P-I-O.
[01:12:46.240 --> 01:12:48.040]   This is really interesting, Kevin.
[01:12:48.040 --> 01:12:50.040]   I've never seen this before.
[01:12:50.040 --> 01:12:53.120]   So this is the sense of this is that you can create cars.
[01:12:53.120 --> 01:12:54.120]   You can move the cars around.
[01:12:54.120 --> 01:12:55.120]   You join the team.
[01:12:55.120 --> 01:12:57.320]   Look, they even have audio.
[01:12:57.320 --> 01:13:02.200]   You're pulling those cards in from other sites and services?
[01:13:02.200 --> 01:13:04.840]   I think you just create them.
[01:13:04.840 --> 01:13:10.680]   I'm not sure you can insert pictures and things, but I'm not sure you can import them for something
[01:13:10.680 --> 01:13:11.680]   else.
[01:13:11.680 --> 01:13:12.680]   I have to play with it.
[01:13:12.680 --> 01:13:14.200]   It was one of the things I looked at.
[01:13:14.200 --> 01:13:17.000]   I thought that could be fun to play with.
[01:13:17.000 --> 01:13:19.560]   But again, it depends.
[01:13:19.560 --> 01:13:23.200]   Part of the reason that we haven't had these recently is that you need a big screen to
[01:13:23.200 --> 01:13:24.400]   do this stuff on.
[01:13:24.400 --> 01:13:26.520]   So when you're out on the phone, what's possible?
[01:13:26.520 --> 01:13:27.520]   Yes.
[01:13:27.520 --> 01:13:28.520]   That's right.
[01:13:28.520 --> 01:13:29.520]   That's right.
[01:13:29.520 --> 01:13:30.520]   Yeah.
[01:13:30.520 --> 01:13:31.520]   And we're sitting at home on big screens again, suddenly these kinds of tools.
[01:13:31.520 --> 01:13:32.520]   And a mouse.
[01:13:32.520 --> 01:13:33.520]   And a mouse.
[01:13:33.520 --> 01:13:34.520]   Maybe you could do this with touch.
[01:13:34.520 --> 01:13:35.520]   Yeah.
[01:13:35.520 --> 01:13:36.520]   No, you could do it with touch, but it would be fiddly.
[01:13:36.520 --> 01:13:37.520]   You could do this on an iPad.
[01:13:37.520 --> 01:13:38.520]   When I play with a panel with the...
[01:13:38.520 --> 01:13:39.520]   Do you think...
[01:13:39.520 --> 01:13:46.400]   Do you think for Stacey's kids and others in that younger generation, a touch is important
[01:13:46.400 --> 01:13:50.080]   in iPads maybe, are the future or is that not...
[01:13:50.080 --> 01:13:52.720]   Is that too linear still?
[01:13:52.720 --> 01:13:54.080]   I think they can do...
[01:13:54.080 --> 01:13:58.800]   I mean, my daughter will definitely come in and try to touch my monitor.
[01:13:58.800 --> 01:14:00.800]   Does she use an iPad?
[01:14:00.800 --> 01:14:02.800]   She uses an iPad.
[01:14:02.800 --> 01:14:04.800]   She uses a MacBook.
[01:14:04.800 --> 01:14:06.280]   She can do anything.
[01:14:06.280 --> 01:14:08.720]   She doesn't care, probably.
[01:14:08.720 --> 01:14:09.720]   She's not.
[01:14:09.720 --> 01:14:10.720]   She cares less than we...
[01:14:10.720 --> 01:14:12.200]   She uses whatever tool gets whatever...
[01:14:12.200 --> 01:14:13.200]   Yeah.
[01:14:13.200 --> 01:14:15.200]   Like if she's writing a paper, she's going to write it on a keyboard.
[01:14:15.200 --> 01:14:20.040]   But if she's doing a presentation, she will often do it on a tablet type.
[01:14:20.040 --> 01:14:21.040]   Yeah.
[01:14:21.040 --> 01:14:22.040]   Yeah.
[01:14:22.040 --> 01:14:25.280]   But I think this is part of this is...
[01:14:25.280 --> 01:14:29.120]   We're still front-to-art to take care of these kinds of flow things as well.
[01:14:29.120 --> 01:14:32.880]   It's one of the problems we have with programming now because programming is we're trapped in
[01:14:32.880 --> 01:14:34.120]   a giant linear code.
[01:14:34.120 --> 01:14:35.120]   Right.
[01:14:35.120 --> 01:14:37.640]   But actually what we're building is lots of things that interact and plug into each other
[01:14:37.640 --> 01:14:39.360]   and wire things up.
[01:14:39.360 --> 01:14:41.840]   But the code is said about having to express that in code.
[01:14:41.840 --> 01:14:47.320]   If you remember, Yahoo pipes was like the thing where you say, "I'm going to say pipes, Kevin."
[01:14:47.320 --> 01:14:48.320]   Yeah.
[01:14:48.320 --> 01:14:49.320]   Yeah.
[01:14:49.320 --> 01:14:50.320]   The late...
[01:14:50.320 --> 01:14:51.320]   We should point out...
[01:14:51.320 --> 01:14:52.320]   But you're like, "Blay, lament it."
[01:14:52.320 --> 01:14:53.320]   Yahoo pipes.
[01:14:53.320 --> 01:14:54.320]   It's gone.
[01:14:54.320 --> 01:14:55.320]   Yahoo killed it.
[01:14:55.320 --> 01:14:56.320]   But that is...
[01:14:56.320 --> 01:15:00.160]   A lot of the way you write code these days is you are saying, "Pull this thing from here,
[01:15:00.160 --> 01:15:02.280]   wait for this thing here."
[01:15:02.280 --> 01:15:04.040]   And then if it works, do this thing.
[01:15:04.040 --> 01:15:07.360]   So you're doing flow-charty things, but you're still writing them in prose.
[01:15:07.360 --> 01:15:10.600]   So the programming tools...
[01:15:10.600 --> 01:15:14.160]   Well, there are pro-tools that do behave like that.
[01:15:14.160 --> 01:15:15.160]   But there's...
[01:15:15.160 --> 01:15:16.160]   Well, what about hyper-cars?
[01:15:16.160 --> 01:15:19.080]   I mean, Bill Atkinson thought about this 1984.
[01:15:19.080 --> 01:15:22.520]   I think Kevin just gave us the bridge though.
[01:15:22.520 --> 01:15:24.080]   Which is the language piece.
[01:15:24.080 --> 01:15:27.240]   What was that whole conversational thing yesterday with what's called...
[01:15:27.240 --> 01:15:29.720]   Well, think about all the no-code, low-code things.
[01:15:29.720 --> 01:15:30.720]   Mom, right.
[01:15:30.720 --> 01:15:32.720]   It was language becomes code.
[01:15:32.720 --> 01:15:35.480]   Yeah, but the problem is if language is...
[01:15:35.480 --> 01:15:41.800]   You've got to make it accessible to more people, right?
[01:15:41.800 --> 01:15:44.000]   What we're doing is we're basically bridging all the...
[01:15:44.000 --> 01:15:50.400]   I mean, if you think about APIs and services, which every company should be building, right?
[01:15:50.400 --> 01:15:54.480]   So if you think about that, your whole mission in life is to pull together these services
[01:15:54.480 --> 01:15:56.120]   easily for people.
[01:15:56.120 --> 01:15:59.960]   And we have to be able to do that because we're relying more on computers, we're relying
[01:15:59.960 --> 01:16:04.720]   more on data from so many different places to just do our jobs.
[01:16:04.720 --> 01:16:06.960]   So yeah, this is where we're heading.
[01:16:06.960 --> 01:16:07.960]   Right.
[01:16:07.960 --> 01:16:10.240]   So, so, here's your words.
[01:16:10.240 --> 01:16:11.920]   Understands your intent.
[01:16:11.920 --> 01:16:13.720]   Takes an action on your behalf.
[01:16:13.720 --> 01:16:14.720]   That's programming.
[01:16:14.720 --> 01:16:15.720]   Right?
[01:16:15.720 --> 01:16:17.280]   And the question is, does this...
[01:16:17.280 --> 01:16:20.320]   We have to speak its language or can it start to speak our language in that?
[01:16:20.320 --> 01:16:23.560]   And we can say when this happens, do that.
[01:16:23.560 --> 01:16:24.560]   That's programming.
[01:16:24.560 --> 01:16:28.880]   Although I have to point out these days, I mean, if you're using Prezi, you're programming.
[01:16:28.880 --> 01:16:34.160]   Admittedly, somebody has written all the code underlying, but by any time you're putting
[01:16:34.160 --> 01:16:37.800]   things together in a space, you're writing code.
[01:16:37.800 --> 01:16:40.880]   You know, this is in effect, right?
[01:16:40.880 --> 01:16:42.880]   Well, think about it this way.
[01:16:42.880 --> 01:16:46.920]   I mean, Kevin's the only programmer here, so I shouldn't...
[01:16:46.920 --> 01:16:49.520]   Think about MacGyver, okay?
[01:16:49.520 --> 01:16:55.360]   When you're putting duct tape, or when you make something work, like I take, you know,
[01:16:55.360 --> 01:16:56.360]   I'm like, oh, this doesn't work.
[01:16:56.360 --> 01:17:00.360]   So I wrap a rubber band around it, or I fiddle with it, and I kind of...
[01:17:00.360 --> 01:17:03.240]   I'm technically engineering, right?
[01:17:03.240 --> 01:17:07.080]   And I'm not engineering at the high level actual engineers do, but I am.
[01:17:07.080 --> 01:17:09.160]   I'm solving a problem with...
[01:17:09.160 --> 01:17:10.360]   Physical tools.
[01:17:10.360 --> 01:17:11.360]   Right.
[01:17:11.360 --> 01:17:17.240]   And with programming, it has been historically a very complicated, esoteric thing, but what
[01:17:17.240 --> 01:17:18.480]   we're doing is we're making...
[01:17:18.480 --> 01:17:21.840]   We're, again, putting in layers of abstraction.
[01:17:21.840 --> 01:17:26.920]   So we're now using our digital world, just like I would use tape to fix something in
[01:17:26.920 --> 01:17:27.920]   engineers.
[01:17:27.920 --> 01:17:28.920]   Exactly.
[01:17:28.920 --> 01:17:30.640]   And for yourself, not for scale, not for everybody.
[01:17:30.640 --> 01:17:33.480]   You're not trying to make something that is productized.
[01:17:33.480 --> 01:17:35.520]   You're trying to accomplish something for yourself.
[01:17:35.520 --> 01:17:36.520]   Yeah.
[01:17:36.520 --> 01:17:37.520]   So it's a business...
[01:17:37.520 --> 01:17:38.520]   I mean, maybe it's a business task.
[01:17:38.520 --> 01:17:44.440]   I need to pull in data from five different places to figure out if whatever I'm doing
[01:17:44.440 --> 01:17:46.320]   in my job works out.
[01:17:46.320 --> 01:17:47.680]   But yes, exactly that.
[01:17:47.680 --> 01:17:53.040]   And we're just learning how to manipulate digital files and digital services in ways
[01:17:53.040 --> 01:17:56.040]   that are accessible to everyone, not just programmers.
[01:17:56.040 --> 01:18:00.240]   Also, the programmers are going to hate it if we call it programming.
[01:18:00.240 --> 01:18:04.800]   Well, I think they know, though, by now there's low-level programming, there's high-level
[01:18:04.800 --> 01:18:10.160]   programming, and then there's assembling that is anything that's creating something new
[01:18:10.160 --> 01:18:11.160]   out of...
[01:18:11.160 --> 01:18:12.160]   Life hacks.
[01:18:12.160 --> 01:18:13.160]   Yeah.
[01:18:13.160 --> 01:18:14.160]   I mean, all of that is coding.
[01:18:14.160 --> 01:18:16.000]   It's just at different levels.
[01:18:16.000 --> 01:18:17.760]   But most of coding these days is...
[01:18:17.760 --> 01:18:18.760]   It is.
[01:18:18.760 --> 01:18:19.760]   ...panting different things together.
[01:18:19.760 --> 01:18:20.760]   Yeah, it's pasting...
[01:18:20.760 --> 01:18:23.320]   And also, like, spreadsheets are that.
[01:18:23.320 --> 01:18:25.360]   What you just described is very much a spreadsheet.
[01:18:25.360 --> 01:18:26.360]   Yeah.
[01:18:26.360 --> 01:18:29.760]   It's spreadsheets already have lots of other boxes that connect together.
[01:18:29.760 --> 01:18:32.080]   You can't see them because the form is kind of hidden.
[01:18:32.080 --> 01:18:38.440]   And Microsoft suggests that Glamd is in Excel now, which is like Lotus improv from 30 years
[01:18:38.440 --> 01:18:39.440]   ago.
[01:18:39.440 --> 01:18:43.800]   But it's the same kind of idea of the formulas can exist outside of the boxes, which means
[01:18:43.800 --> 01:18:46.880]   they can be a bit more broadly applicable.
[01:18:46.880 --> 01:18:48.880]   So there's something there.
[01:18:48.880 --> 01:18:53.680]   But sort of a spreadsheet where you could see the little wires like in that colloquial
[01:18:53.680 --> 01:18:54.960]   thing, that would be interesting.
[01:18:54.960 --> 01:18:57.760]   Which of these cells connect to the other cells?
[01:18:57.760 --> 01:19:02.080]   Would you explode it out into the sort of guy in the room with the red string diagram?
[01:19:02.080 --> 01:19:05.040]   I think a lot of work is being done around this stuff right now.
[01:19:05.040 --> 01:19:10.320]   I think what Google's going to do with Canvas in Workspace is probably going to be the least
[01:19:10.320 --> 01:19:15.000]   innovative of it because they're tied down by all sorts of constraints.
[01:19:15.000 --> 01:19:17.800]   But I think there is some really interesting stuff going on.
[01:19:17.800 --> 01:19:20.440]   Nothing quite does it yet.
[01:19:20.440 --> 01:19:21.440]   But there are...
[01:19:21.440 --> 01:19:23.440]   It's also a data visualization thing.
[01:19:23.440 --> 01:19:26.920]   I was trying to show this book, but of course, if I try and think a book about it's going
[01:19:26.920 --> 01:19:28.680]   to be because it will seem it's not going to happen.
[01:19:28.680 --> 01:19:30.480]   Just make sure it doesn't go to your green screen.
[01:19:30.480 --> 01:19:31.480]   There you go.
[01:19:31.480 --> 01:19:33.280]   Oh, too late.
[01:19:33.280 --> 01:19:34.280]   Seeing with franchise.
[01:19:34.280 --> 01:19:43.120]   So he's been sort of banging this drum for 30 years as well, saying what matters is arranging
[01:19:43.120 --> 01:19:48.760]   things with enough density of data and information to explain things to you.
[01:19:48.760 --> 01:19:52.440]   And that was a thing that he was originally talking about print a lot, and then he started
[01:19:52.440 --> 01:19:53.640]   talking about displays.
[01:19:53.640 --> 01:19:59.480]   But now we actually have screens that are dense enough and have enough pixels that they're
[01:19:59.480 --> 01:20:01.840]   better than printing these days.
[01:20:01.840 --> 01:20:06.160]   So you could potentially do the kinds of really dense visualizations that he shows in these
[01:20:06.160 --> 01:20:08.600]   books as part of this.
[01:20:08.600 --> 01:20:13.200]   But we're sort of stuck in a model where we're assuming people are on their phone or on their
[01:20:13.200 --> 01:20:14.200]   screens.
[01:20:14.200 --> 01:20:17.160]   So the things we build tend to have to work on both of those.
[01:20:17.160 --> 01:20:19.760]   So they end up being sort of small as well.
[01:20:19.760 --> 01:20:22.120]   That was the weird thing about the feed reader that they were showing.
[01:20:22.120 --> 01:20:24.800]   The building of feed reader on the phone first is like, well, okay.
[01:20:24.800 --> 01:20:27.520]   But I'm only able to see one thing at a time on that.
[01:20:27.520 --> 01:20:30.320]   It's not going to be as useful as a feed reader where I have a long list of things I can
[01:20:30.320 --> 01:20:32.520]   pick one I wouldn't want to look at.
[01:20:32.520 --> 01:20:34.000]   Right.
[01:20:34.000 --> 01:20:36.840]   Have you ever been to an Edward Teftea seminar?
[01:20:36.840 --> 01:20:38.000]   Stacey or Kevin?
[01:20:38.000 --> 01:20:42.360]   No, I wanted to go to one and I think I got sick or something.
[01:20:42.360 --> 01:20:43.360]   I had signed up.
[01:20:43.360 --> 01:20:44.520]   Really would like to go.
[01:20:44.520 --> 01:20:46.080]   Have you gone Kevin ever to see him?
[01:20:46.080 --> 01:20:47.960]   No, I've never thought about the books.
[01:20:47.960 --> 01:20:48.960]   Yeah.
[01:20:48.960 --> 01:20:49.960]   Yeah.
[01:20:49.960 --> 01:20:51.960]   This is his latest scene with fresh eyes.
[01:20:51.960 --> 01:20:52.960]   No, do read them.
[01:20:52.960 --> 01:20:53.960]   Yeah.
[01:20:53.960 --> 01:20:54.960]   No, Kevin's ever would.
[01:20:54.960 --> 01:20:56.560]   The kind of guy I would read it.
[01:20:56.560 --> 01:20:57.560]   Yeah.
[01:20:57.560 --> 01:20:59.000]   Well, and this is the newest one.
[01:20:59.000 --> 01:21:02.960]   This is actually really good because it's sort of something a lot.
[01:21:02.960 --> 01:21:04.960]   He's the king of data visualizations.
[01:21:04.960 --> 01:21:06.960]   He's been teaching this for 30 years.
[01:21:06.960 --> 01:21:09.160]   It's sort of super dense stuff.
[01:21:09.160 --> 01:21:11.480]   He's got some great COVID graphs in it and things.
[01:21:11.480 --> 01:21:13.000]   So it's very up to date.
[01:21:13.000 --> 01:21:17.400]   But it's also he's got lots of things about explaining this thing I said in book one,
[01:21:17.400 --> 01:21:18.400]   I was wrong.
[01:21:18.400 --> 01:21:20.280]   You should be doing like this as well.
[01:21:20.280 --> 01:21:27.320]   So he basically says the key thing is to plot every point of data that you have and then
[01:21:27.320 --> 01:21:32.200]   in a way that people can look at it and get a gestalt of it rather than simply finding
[01:21:32.200 --> 01:21:35.760]   things down to averages or just plotting the lines or those kinds of things.
[01:21:35.760 --> 01:21:44.800]   So the sort of pushes back against the whole ideas of statistics which grew up because we
[01:21:44.800 --> 01:21:46.160]   couldn't possibly plot all the points.
[01:21:46.160 --> 01:21:48.840]   So we have to come up with some summarizing numbers or whatever.
[01:21:48.840 --> 01:21:53.640]   So he has this where there's a great way to start to setting a classic PowerPoint slide
[01:21:53.640 --> 01:21:57.080]   and says, okay, so here's a thing showing the growth over the last two years.
[01:21:57.080 --> 01:21:59.080]   But there's only three numbers here.
[01:21:59.080 --> 01:22:02.880]   What if we look at all the numbers that are really there and go through like that?
[01:22:02.880 --> 01:22:05.960]   That stuff is like, yes, that makes a lot of sense to me.
[01:22:05.960 --> 01:22:13.120]   And if that sense of data visualization can flow into the tools that we're using to collaborate
[01:22:13.120 --> 01:22:15.240]   as well, I think that will be powerful.
[01:22:15.240 --> 01:22:20.280]   So my sense is that's the piece that's kind of missing and it's something that Google
[01:22:20.280 --> 01:22:21.600]   should be good at.
[01:22:21.600 --> 01:22:26.080]   Google is, if anyone's moving large amounts of data around its Google.
[01:22:26.080 --> 01:22:36.040]   But I haven't seen those kind of tough, dense visualizations coming out of Google recently.
[01:22:36.040 --> 01:22:40.680]   I'm now going to buy this book, by the way.
[01:22:40.680 --> 01:22:42.600]   And then I won't read it.
[01:22:42.600 --> 01:22:44.240]   Oh, Kevin, you won for the day.
[01:22:44.240 --> 01:22:45.240]   Yeah.
[01:22:45.240 --> 01:22:47.080]   He just did his pick.
[01:22:47.080 --> 01:22:48.080]   That's great.
[01:22:48.080 --> 01:22:49.080]   That's great.
[01:22:49.080 --> 01:22:50.080]   Yeah.
[01:22:50.080 --> 01:22:51.400]   Did you get the sign one?
[01:22:51.400 --> 01:22:52.800]   Oh, look at that.
[01:22:52.800 --> 01:22:55.680]   That's where it's spiffy.
[01:22:55.680 --> 01:22:56.520]   Spiffy, man.
[01:22:56.520 --> 01:23:02.200]   You can actually get his get all five of him five of his books on a graft if you want
[01:23:02.200 --> 01:23:04.880]   on his website.
[01:23:04.880 --> 01:23:05.880]   Let's take a little break.
[01:23:05.880 --> 01:23:07.800]   I wanted to show you something.
[01:23:07.800 --> 01:23:10.160]   Oh, new tattoo.
[01:23:10.160 --> 01:23:14.200]   No, but how do you like this shirt?
[01:23:14.200 --> 01:23:15.200]   This is a linen shirt.
[01:23:15.200 --> 01:23:16.200]   It's nice.
[01:23:16.200 --> 01:23:17.360]   It's a beautiful shirt.
[01:23:17.360 --> 01:23:18.600]   This is my new untucket.
[01:23:18.600 --> 01:23:22.720]   I am in love with untucket, our new sponsor.
[01:23:22.720 --> 01:23:25.560]   In fact, I was talking with them.
[01:23:25.560 --> 01:23:30.960]   And of course, I got some polo shirts and untucket with our shirts designed not to tuck
[01:23:30.960 --> 01:23:31.960]   in.
[01:23:31.960 --> 01:23:35.200]   And it's always been a problem because either shirts have these long shirt tails.
[01:23:35.200 --> 01:23:39.640]   And if you walk around, you look like a slob or they're too short, you know, and they have
[01:23:39.640 --> 01:23:43.800]   the square bottoms on tuckett is always design shirts that are just exactly the right length.
[01:23:43.800 --> 01:23:49.320]   Halfway down the zipper, actually, technically, if you're looking for a father's day gift,
[01:23:49.320 --> 01:23:54.720]   speaking as a father, this is the gift to give dad a shirt from untucket.
[01:23:54.720 --> 01:23:56.320]   They have shirts for every occasion.
[01:23:56.320 --> 01:23:59.680]   I'm going to Hawaii in July and I thought I should get some nice linen shirts.
[01:23:59.680 --> 01:24:03.440]   I love linen, but linen is wrinkly as heck.
[01:24:03.440 --> 01:24:07.560]   That's I guess one of its properties, except they have wrinkle free linen.
[01:24:07.560 --> 01:24:08.560]   I don't know how they do it.
[01:24:08.560 --> 01:24:10.000]   It's kind of magic.
[01:24:10.000 --> 01:24:14.200]   They make this make these shirts, as you can see, all of them designed to be worn untucked
[01:24:14.200 --> 01:24:15.200]   casual.
[01:24:15.200 --> 01:24:18.320]   It's a tough style to get right, but they figured it out.
[01:24:18.320 --> 01:24:24.480]   And by the way, they have a huge variety of fits, all shapes, all sizes.
[01:24:24.480 --> 01:24:29.760]   We've we've fit everybody from Jason Howell to me to to Aunt Pruitt.
[01:24:29.760 --> 01:24:32.400]   It helps you look sharp, even at your most casual.
[01:24:32.400 --> 01:24:33.400]   They have performance shirts.
[01:24:33.400 --> 01:24:39.000]   I have a bunch of these that wickswett polo style shirts, short sleeve button downs.
[01:24:39.000 --> 01:24:43.960]   They have slim, relaxed and tall fits sizes all the way up to triple XL.
[01:24:43.960 --> 01:24:46.320]   We call it programmer size.
[01:24:46.320 --> 01:24:49.120]   There's something just right for you.
[01:24:49.120 --> 01:24:52.760]   And I have been getting I started off with the kind of the casual untucket shirts, the
[01:24:52.760 --> 01:24:54.080]   polos and stuff.
[01:24:54.080 --> 01:24:57.440]   And now I'm starting to get some of these dress shirts because they're so beautiful.
[01:24:57.440 --> 01:24:59.280]   And look at I've worn this several times.
[01:24:59.280 --> 01:25:04.440]   It was actually my closet all balled up when I it's not wrinkled.
[01:25:04.440 --> 01:25:05.720]   I love it.
[01:25:05.720 --> 01:25:07.360]   What else can I tell you about untucket?
[01:25:07.360 --> 01:25:08.440]   Oh, here's this is great.
[01:25:08.440 --> 01:25:15.120]   They test every batch of fabric to ensure the best possible quality and consistency.
[01:25:15.120 --> 01:25:18.960]   So all of their button downs, their polos, their teas, their handlies have just the right
[01:25:18.960 --> 01:25:19.960]   length.
[01:25:19.960 --> 01:25:20.960]   They fit perfectly.
[01:25:20.960 --> 01:25:22.720]   Best of all, they have outstanding customer service.
[01:25:22.720 --> 01:25:26.760]   So if there's anything wrong, anything you don't like, they've got you covered.
[01:25:26.760 --> 01:25:30.480]   And if you're thinking about going back to the office or dad, perhaps is thinking about
[01:25:30.480 --> 01:25:33.720]   going back to the office, check out the wrinkle free collection.
[01:25:33.720 --> 01:25:36.400]   I've worn them several times and always get compliments.
[01:25:36.400 --> 01:25:38.240]   You just look sharp.
[01:25:38.240 --> 01:25:40.920]   And long sleeves on tuckett.
[01:25:40.920 --> 01:25:43.680]   That's have stores, 85 stores nationwide.
[01:25:43.680 --> 01:25:45.920]   So if you want, you can go and try them on.
[01:25:45.920 --> 01:25:48.920]   Honestly, I've been very happy just ordering them by mail.
[01:25:48.920 --> 01:25:51.440]   They're of course free returns and exchanges.
[01:25:51.440 --> 01:25:52.960]   So there's no risk at all.
[01:25:52.960 --> 01:25:54.000]   I think you're going to love it.
[01:25:54.000 --> 01:25:56.000]   I think dad's going to love it.
[01:25:56.000 --> 01:26:00.280]   If you've been looking for something for dad for Father's Day and I'm one of those dads,
[01:26:00.280 --> 01:26:03.520]   my son's always saying now dad, we got a we got a we got to sharpen you up.
[01:26:03.520 --> 01:26:08.200]   You can't be you can't be wearing those cargo pants to come visit me anymore.
[01:26:08.200 --> 01:26:10.160]   I wore an untuck at the other day.
[01:26:10.160 --> 01:26:11.440]   He said, where'd you get that?
[01:26:11.440 --> 01:26:12.760]   That looks sharp.
[01:26:12.760 --> 01:26:14.600]   So maybe for sons too.
[01:26:14.600 --> 01:26:20.000]   And grads don't forget untuck it for the dad, the grad, the son in your life use code or
[01:26:20.000 --> 01:26:21.000]   the daughter.
[01:26:21.000 --> 01:26:25.760]   Twit is the code TWIT 20% off your first purchase.
[01:26:25.760 --> 01:26:33.920]   You don't have to tell that that untuck it is UNTU C K I T untuck it dot com.
[01:26:33.920 --> 01:26:34.920]   So go to untuckett.com.
[01:26:34.920 --> 01:26:38.800]   Again, the offer code is TWIT for 20% off your first purchase.
[01:26:38.800 --> 01:26:42.640]   Untuck it shirts designed to be worn untucked.
[01:26:42.640 --> 01:26:46.920]   When I found out they go ahead and I thought, Oh, and I'm getting a bunch of these for
[01:26:46.920 --> 01:26:48.920]   Hawaii because they look nice.
[01:26:48.920 --> 01:26:50.000]   They look dressy.
[01:26:50.000 --> 01:26:55.720]   These are the colors on these are incredible, but they're also very cool, very relaxed, cool
[01:26:55.720 --> 01:26:58.720]   with a capital C and a lowercase as well.
[01:26:58.720 --> 01:27:01.320]   untuckett.com offer code is TWIT.
[01:27:01.320 --> 01:27:04.360]   Thank you untuck it for your support.
[01:27:04.360 --> 01:27:10.240]   Of this week in Google.
[01:27:10.240 --> 01:27:12.280]   Anything else about Google I owe that we should talk about?
[01:27:12.280 --> 01:27:16.920]   We have a Google change log, but I think we've probably gone through all of the change log
[01:27:16.920 --> 01:27:17.920]   already.
[01:27:17.920 --> 01:27:19.760]   So the sweatshirt that they sent out to the press.
[01:27:19.760 --> 01:27:21.600]   Oh, that's a fascinating story.
[01:27:21.600 --> 01:27:23.320]   I didn't notice this.
[01:27:23.320 --> 01:27:25.760]   I wouldn't notice this at all because it's it's Russell.
[01:27:25.760 --> 01:27:26.760]   Oh, yeah, fine.
[01:27:26.760 --> 01:27:27.760]   Russell Ray on.
[01:27:27.760 --> 01:27:28.760]   Yeah.
[01:27:28.760 --> 01:27:30.680]   I don't know if you could even see it.
[01:27:30.680 --> 01:27:32.280]   There's a number on it.
[01:27:32.280 --> 01:27:33.280]   Yeah.
[01:27:33.280 --> 01:27:34.280]   It's literally great.
[01:27:34.280 --> 01:27:38.760]   You can barely notice Russell Holly has a picture at Cnet where you can kind of see it.
[01:27:38.760 --> 01:27:39.760]   It's the number is what?
[01:27:39.760 --> 01:27:40.760]   8800?
[01:27:40.760 --> 01:27:42.440]   What does it say on yours?
[01:27:42.440 --> 01:27:43.920]   Same thing.
[01:27:43.920 --> 01:27:44.920]   What does that mean?
[01:27:44.920 --> 01:27:45.920]   It's far.
[01:27:45.920 --> 01:27:46.920]   Read down.
[01:27:46.920 --> 01:27:47.920]   It's you'll see what it does.
[01:27:47.920 --> 01:27:48.920]   He coded it.
[01:27:48.920 --> 01:27:50.480]   The characters run down the full length of the left sleeve.
[01:27:50.480 --> 01:27:52.520]   They contain the following information.
[01:27:52.520 --> 01:27:55.640]   Oh, so 8800 is just part of it.
[01:27:55.640 --> 01:27:57.920]   Parenthesis 162137.
[01:27:57.920 --> 01:28:02.520]   8800 ampersand ampersand g clothes parenthesis.
[01:28:02.520 --> 01:28:07.920]   Parentmark I slash O colon left bracket right bracket semicolon.
[01:28:07.920 --> 01:28:12.440]   So the semicolon indicates maybe it's a little bit of a line of code, maybe some Python.
[01:28:12.440 --> 01:28:13.440]   I don't know.
[01:28:13.440 --> 01:28:14.440]   What do you think?
[01:28:14.440 --> 01:28:15.440]   Let's ask.
[01:28:15.440 --> 01:28:16.440]   Let's see.
[01:28:16.440 --> 01:28:17.440]   Kevin Marks.
[01:28:17.440 --> 01:28:19.480]   I'm positive does not semicolon.
[01:28:19.480 --> 01:28:20.840]   So we shouldn't be Python.
[01:28:20.840 --> 01:28:21.840]   Oh, yeah.
[01:28:21.840 --> 01:28:22.840]   Like Python doesn't use those.
[01:28:22.840 --> 01:28:23.840]   They use tabs.
[01:28:23.840 --> 01:28:24.840]   Okay.
[01:28:24.840 --> 01:28:30.200]   Let's let's enter into the Google search and see what we come up with.
[01:28:30.200 --> 01:28:32.480]   Oh, I find a bunch of articles explaining it.
[01:28:32.480 --> 01:28:38.880]   All that Russell Holly explained it.
[01:28:38.880 --> 01:28:49.360]   It's actually the Unix timestamp for Tuesday, May 18, 2021, 4 p.m. Pacific.
[01:28:49.360 --> 01:28:52.280]   Did something happen at 4 p.m.?
[01:28:52.280 --> 01:28:53.880]   I don't know.
[01:28:53.880 --> 01:28:54.880]   That's so silly.
[01:28:54.880 --> 01:28:57.320]   This is this is when they'd announced the new Chromebook or something.
[01:28:57.320 --> 01:28:58.320]   But no, no, yeah.
[01:28:58.320 --> 01:29:03.320]   Russell six is coming out.
[01:29:03.320 --> 01:29:05.120]   No, nothing.
[01:29:05.120 --> 01:29:06.120]   Oh, well.
[01:29:06.120 --> 01:29:07.120]   So that's kind of weird.
[01:29:07.120 --> 01:29:11.440]   And you didn't notice it even though you have the sweatshirt.
[01:29:11.440 --> 01:29:15.560]   Well, mainly both Jason and I were noticing how it fit either of us.
[01:29:15.560 --> 01:29:16.960]   Oh, it's really small.
[01:29:16.960 --> 01:29:18.360]   This is the largest.
[01:29:18.360 --> 01:29:20.480]   Maybe at 4 p.m. it'll expand.
[01:29:20.480 --> 01:29:23.840]   Yeah, there's a lot of this with untucket.
[01:29:23.840 --> 01:29:26.640]   Google untucket will be a good thing.
[01:29:26.640 --> 01:29:28.120]   It's the Unix epic.
[01:29:28.120 --> 01:29:30.480]   Well, except they don't provide shirts for women.
[01:29:30.480 --> 01:29:33.240]   So that would be terrible.
[01:29:33.240 --> 01:29:36.560]   I'm just saying.
[01:29:36.560 --> 01:29:38.120]   Just saying.
[01:29:38.120 --> 01:29:41.520]   I want to hear Kevin on Lambda.
[01:29:41.520 --> 01:29:44.560]   Yeah, Lambda was Lambda and mom.
[01:29:44.560 --> 01:29:50.160]   So Lambda is the next step after birth.
[01:29:50.160 --> 01:29:52.480]   But these are all and it's kind of like GPT.
[01:29:52.480 --> 01:29:58.640]   These are all models for speech technology, I guess.
[01:29:58.640 --> 01:30:02.720]   But were they using the same technology that built Burt?
[01:30:02.720 --> 01:30:03.720]   They mentioned Burt.
[01:30:03.720 --> 01:30:04.720]   Yeah, I was kind of.
[01:30:04.720 --> 01:30:06.720]   I was using transformer.
[01:30:06.720 --> 01:30:08.560]   It's based on transformer.
[01:30:08.560 --> 01:30:09.560]   Okay.
[01:30:09.560 --> 01:30:10.560]   Okay.
[01:30:10.560 --> 01:30:11.560]   Yeah.
[01:30:11.560 --> 01:30:17.080]   So it's the next generation of the same through a large number of large corpus at a large machine
[01:30:17.080 --> 01:30:24.440]   learning thing and persuaded to come up with a similar group of speech.
[01:30:24.440 --> 01:30:29.680]   And so the demonstration they did, which was having a conversation with the planet Pluto
[01:30:29.680 --> 01:30:34.840]   and then with a paper airplane, implied that it's more than just a text to speech simulator.
[01:30:34.840 --> 01:30:36.200]   It's almost like a lysa.
[01:30:36.200 --> 01:30:38.160]   It's a conversational AI.
[01:30:38.160 --> 01:30:40.360]   You can do that with GPT-3.
[01:30:40.360 --> 01:30:47.720]   But you can say, I'm having a conversation with Sherlock Holmes, me type of thing, Sherlock
[01:30:47.720 --> 01:30:51.720]   Holmes, and it responds to Sherlock Holmes, whatever.
[01:30:51.720 --> 01:30:57.560]   Contextually, it's got enough backup that it will generate something that's vaguely plausibly
[01:30:57.560 --> 01:30:58.560]   that kind of thing.
[01:30:58.560 --> 01:30:59.560]   But it's doing the same thing.
[01:30:59.560 --> 01:31:00.560]   It ends up being like really bad improv.
[01:31:00.560 --> 01:31:04.640]   It's like, okay, I'm kind of bored with this thing pretending to be Pluto.
[01:31:04.640 --> 01:31:07.360]   And he said it was pretty impressive.
[01:31:07.360 --> 01:31:12.200]   I mean, I don't know if that was a canned demo or, but it was really impressive in its understanding
[01:31:12.200 --> 01:31:14.400]   of context and stuff.
[01:31:14.400 --> 01:31:18.720]   Yeah, but it's plausible bullshit is the thing.
[01:31:18.720 --> 01:31:21.520]   Well, that's what I that's my stock.
[01:31:21.520 --> 01:31:25.160]   If you say, I don't know why you would do it.
[01:31:25.160 --> 01:31:26.360]   That's the thing.
[01:31:26.360 --> 01:31:30.000]   So it's like, if you can get it to sort of freestyle a bit about pretending to be Pluto,
[01:31:30.000 --> 01:31:33.360]   that's something if you actually ask a good question about something you care about, it'll
[01:31:33.360 --> 01:31:37.600]   start making up round of details, plausible sounding round of details that are wrong.
[01:31:37.600 --> 01:31:39.920]   So it's not a Google assistant replacement.
[01:31:39.920 --> 01:31:43.480]   Unfortunately, they don't have any way you can play with it.
[01:31:43.480 --> 01:31:45.400]   You have to just take their way.
[01:31:45.400 --> 01:31:46.400]   Not yet.
[01:31:46.400 --> 01:31:50.960]   But this is literally what the stochastic parrots paper was about, which is the one
[01:31:50.960 --> 01:31:56.400]   that they got grumpy about and refused to publish because it was saying these things
[01:31:56.400 --> 01:32:01.040]   don't have a context, but we will fill in the context from the from the from what it's
[01:32:01.040 --> 01:32:02.040]   saying.
[01:32:02.040 --> 01:32:03.040]   Wow.
[01:32:03.040 --> 01:32:04.040]   Interesting.
[01:32:04.040 --> 01:32:05.040]   Kevin, right.
[01:32:05.040 --> 01:32:06.040]   Did you read the paper?
[01:32:06.040 --> 01:32:08.040]   I don't know if you want to know what the matter is.
[01:32:08.040 --> 01:32:11.600]   On the dangers of stochastic parrots, can language models be too big?
[01:32:11.600 --> 01:32:17.840]   This is Tim Rick Gebru along with Emily Bender, Angela and Michaela Major and Margaret Schmitchell,
[01:32:17.840 --> 01:32:18.840]   who is actually Margaret Mitchell.
[01:32:18.840 --> 01:32:23.560]   I'm not sure why she says Schmitchell in the.
[01:32:23.560 --> 01:32:29.840]   But it's so their complaint is what that the training sets are biased or which we've
[01:32:29.840 --> 01:32:30.840]   heard before.
[01:32:30.840 --> 01:32:33.840]   There's lots of things.
[01:32:33.840 --> 01:32:37.160]   Basically, the size is so big you don't know what it's doing.
[01:32:37.160 --> 01:32:41.960]   It's trained on concept of the internet that hasn't been vetted and contains a lot of
[01:32:41.960 --> 01:32:42.960]   bias in history.
[01:32:42.960 --> 01:32:43.960]   That's the issue.
[01:32:43.960 --> 01:32:47.560]   They say size does not guarantee diversity for instance.
[01:32:47.560 --> 01:32:55.560]   But if you go if you go down to like section six, six point one around there, it's.
[01:32:55.560 --> 01:32:58.520]   appearance in the eye of the beholder.
[01:32:58.520 --> 01:33:05.160]   So basically what they're saying here, I'm trying to find a sense I dragged out of it yesterday.
[01:33:05.160 --> 01:33:13.960]   This is such a shame that both Gebru and Mitchell were fired over this paper is frankly stunning
[01:33:13.960 --> 01:33:14.960]   and unacceptable.
[01:33:14.960 --> 01:33:15.960]   It is.
[01:33:15.960 --> 01:33:18.120]   So it's a risk and harm 62.
[01:33:18.120 --> 01:33:19.120]   This is it.
[01:33:19.120 --> 01:33:23.280]   The earth sets fluency and coherence of a language model has raised several risks precisely
[01:33:23.280 --> 01:33:27.400]   because humans repair to interpret strings belonging to languages they speak as meaningful
[01:33:27.400 --> 01:33:31.720]   and correspond to the community of intent and individual group individuals who have accountability
[01:33:31.720 --> 01:33:34.000]   for what is said.
[01:33:34.000 --> 01:33:37.320]   So basically we fill in the gaps in its nonsense.
[01:33:37.320 --> 01:33:40.920]   We project and we always that's what humans do.
[01:33:40.920 --> 01:33:46.880]   And that's why that's why your child thinks that echo is real as a person or that the
[01:33:46.880 --> 01:33:51.080]   Google is there talking to somebody with a Google assistant and more than projecting
[01:33:51.080 --> 01:33:54.440]   personality on it, we protect authority on it.
[01:33:54.440 --> 01:33:55.840]   And that's the risk.
[01:33:55.840 --> 01:33:56.840]   Yes.
[01:33:56.840 --> 01:34:00.240]   Especially because it's coming from Google.
[01:34:00.240 --> 01:34:02.040]   Well Google knows everything.
[01:34:02.040 --> 01:34:03.040]   Right.
[01:34:03.040 --> 01:34:07.920]   And you know, to some extent Google doesn't know everything but what it what it what it
[01:34:07.920 --> 01:34:11.160]   it doesn't you know, it can't actually make sense of things all the time and it makes
[01:34:11.160 --> 01:34:14.560]   and it's making more I'm not sure it's making more or as than it used to but it's making
[01:34:14.560 --> 01:34:17.680]   different kinds of errors than it used to.
[01:34:17.680 --> 01:34:19.720]   So I was.
[01:34:19.720 --> 01:34:26.800]   If I search for my name in the UK and search my name in America, there's now a pressure
[01:34:26.800 --> 01:34:27.960]   on football called Kevin Marx.
[01:34:27.960 --> 01:34:29.960]   Otherwise that was for him now.
[01:34:29.960 --> 01:34:30.960]   Sorry.
[01:34:30.960 --> 01:34:31.960]   Sorry.
[01:34:31.960 --> 01:34:32.960]   I hate it when that happens.
[01:34:32.960 --> 01:34:38.000]   It was probably annoying for him when he was searching before just getting my blog.
[01:34:38.000 --> 01:34:42.440]   My father hates it when he searches for his name and gets me all the time now.
[01:34:42.440 --> 01:34:43.440]   Yeah.
[01:34:43.440 --> 01:34:45.600]   No, it is it is context right now.
[01:34:45.600 --> 01:34:46.600]   It used to it used to.
[01:34:46.600 --> 01:34:49.520]   This was when we were working on the Google profiles.
[01:34:49.520 --> 01:34:53.160]   One of the visit one of the cases was people should be able to find other Kevin Marxes and
[01:34:53.160 --> 01:34:54.160]   me.
[01:34:54.160 --> 01:34:58.320]   Or you shouldn't you shouldn't get a blog post for everything.
[01:34:58.320 --> 01:35:00.480]   But blogs will what was made up the internet.
[01:35:00.480 --> 01:35:01.480]   Right.
[01:35:01.480 --> 01:35:02.480]   That's all there was.
[01:35:02.480 --> 01:35:03.480]   Yeah.
[01:35:03.480 --> 01:35:06.240]   If you were a whole lot where you were you were a whole lot of stuff and therefore it
[01:35:06.240 --> 01:35:07.480]   did very well in.
[01:35:07.480 --> 01:35:11.400]   Well, how do you think as a podcaster I've felt about that all along?
[01:35:11.400 --> 01:35:14.240]   You can't you can't index into podcasts.
[01:35:14.240 --> 01:35:16.640]   You damn bloggers getting all the attention.
[01:35:16.640 --> 01:35:18.120]   You can now.
[01:35:18.120 --> 01:35:19.120]   I know you can.
[01:35:19.120 --> 01:35:20.120]   I don't know.
[01:35:20.120 --> 01:35:21.120]   You should post transcripts.
[01:35:21.120 --> 01:35:22.120]   You do post transcripts.
[01:35:22.120 --> 01:35:23.120]   No, we don't.
[01:35:23.120 --> 01:35:24.120]   You don't.
[01:35:24.120 --> 01:35:26.120]   Anyway, sorry.
[01:35:26.120 --> 01:35:27.120]   Drogression.
[01:35:27.120 --> 01:35:34.200]   If you search for Kevin Marx in the UK, there's someone in Scotland called Kevin Marx who
[01:35:34.200 --> 01:35:35.880]   horribly murdered his wife.
[01:35:35.880 --> 01:35:40.480]   And so it says, Kevin Marx programmer Kevin Marx murder or whatever.
[01:35:40.480 --> 01:35:42.280]   There's there's the headline.
[01:35:42.280 --> 01:35:47.520]   But in the sidebar, it's there's a little thing of maybe just for me.
[01:35:47.520 --> 01:35:48.520]   There's this is you know, there's nothing.
[01:35:48.520 --> 01:35:49.520]   Did you mean this Kevin Marx?
[01:35:49.520 --> 01:35:51.320]   Because there's a knowledge pad for me, right?
[01:35:51.320 --> 01:35:56.000]   And I'm just a little people thingies, widgets you go on the side.
[01:35:56.000 --> 01:35:57.760]   Click here for more about this Kevin Marx.
[01:35:57.760 --> 01:36:00.800]   So I click that and it keeps some of the murder bits as well.
[01:36:00.800 --> 01:36:01.800]   So it's basically saying a murder.
[01:36:01.800 --> 01:36:02.800]   Oh, yeah.
[01:36:02.800 --> 01:36:07.280]   Another friend who's an academic who's got a fairly well she's it's not she's got a
[01:36:07.280 --> 01:36:10.000]   not unusual name, but fairly common name.
[01:36:10.000 --> 01:36:14.080]   But she shares a name with someone who is kidnapped as a child.
[01:36:14.080 --> 01:36:19.280]   So when she she searches for her name, it shows a sidebar box for the good net victim,
[01:36:19.280 --> 01:36:22.880]   but with her photos because there are no photos of the problem is because it comes
[01:36:22.880 --> 01:36:24.200]   from Google.
[01:36:24.200 --> 01:36:27.240]   It's given weight that it doesn't deserve.
[01:36:27.240 --> 01:36:30.760]   And this is this is what this this paper says.
[01:36:30.760 --> 01:36:34.440]   I'm going to quote another line from that same section.
[01:36:34.440 --> 01:36:39.720]   However, machine training systems can and frequently do produce output that is inaccurate,
[01:36:39.720 --> 01:36:46.400]   yet both fluent and again, seemingly coherent in its own right to a consumer who either
[01:36:46.400 --> 01:36:52.360]   doesn't see the source text or cannot cannot understand the source text on their own.
[01:36:52.360 --> 01:36:59.440]   When such consumers therefore mistake the meaning attributed to the machine empty output
[01:36:59.440 --> 01:37:05.920]   as the actual communicative intent of the original texts author real world harm can ensue
[01:37:05.920 --> 01:37:13.520]   and they bring up the case of a Palestinian man arrested by Israeli police after a empty
[01:37:13.520 --> 01:37:19.480]   he translated his Facebook post, which said good morning in Arabic to hurt them in English
[01:37:19.480 --> 01:37:26.200]   and attack them in Hebrew because the police didn't understand that that is not an authoritative
[01:37:26.200 --> 01:37:30.600]   translation and it's based on faulty machine training.
[01:37:30.600 --> 01:37:35.800]   But also a lot of the trade that parallel language training data for Arabic was based
[01:37:35.800 --> 01:37:39.040]   on defense department sources and things like that.
[01:37:39.040 --> 01:37:40.040]   She's translation.
[01:37:40.040 --> 01:37:41.040]   I sure.
[01:37:41.040 --> 01:37:42.040]   Yeah.
[01:37:42.040 --> 01:37:43.400]   So, the CT is machine translation.
[01:37:43.400 --> 01:37:46.120]   It's LMS is learning models.
[01:37:46.120 --> 01:37:47.560]   So yeah, this is a good paper.
[01:37:47.560 --> 01:37:49.360]   Actually, I had not read it.
[01:37:49.360 --> 01:37:51.280]   No, it's it's it's definitely worth reading.
[01:37:51.280 --> 01:37:57.480]   I've also put a blog post I wrote in 11 years ago, which was said the same kind of thing,
[01:37:57.480 --> 01:37:59.920]   but I've found a bunch of literary things that refer to this.
[01:37:59.920 --> 01:38:05.680]   Well, basically these language, these language models are sort of stringing bits of text
[01:38:05.680 --> 01:38:07.000]   together.
[01:38:07.000 --> 01:38:11.520]   And there's a great thing in Michael Frang's Tin Man book from 65, which is literally
[01:38:11.520 --> 01:38:16.160]   describing how this how this could work in his imagined future.
[01:38:16.160 --> 01:38:19.200]   He people who are this sort of parody of people who are building computers to write
[01:38:19.200 --> 01:38:20.200]   prose.
[01:38:20.200 --> 01:38:28.280]   So when you see these, you know, so here in our innocence and and or other Jason and
[01:38:28.280 --> 01:38:33.800]   I and Jeff are watching this lambda, we think it's dopey that the planet Pluto is talking,
[01:38:33.800 --> 01:38:38.800]   but we didn't really understand the greater underlying hazard of this.
[01:38:38.800 --> 01:38:41.800]   Did you watch it and go, Oh boy, there they go again.
[01:38:41.800 --> 01:38:43.360]   Yeah, no, that was it.
[01:38:43.360 --> 01:38:46.760]   I mean, this is thing it's they didn't get it.
[01:38:46.760 --> 01:38:51.000]   They got it, but they were saying this this demo as well.
[01:38:51.000 --> 01:38:54.200]   They probably know it's, you know, it's bullshit.
[01:38:54.200 --> 01:38:56.040]   It was trying to juggle the moon or something.
[01:38:56.040 --> 01:39:00.280]   He did he did give some some of the caveats after he showed the working demo.
[01:39:00.280 --> 01:39:01.280]   Right.
[01:39:01.280 --> 01:39:03.480]   And a lot of these things as you run it three or four times and you pick the one that's
[01:39:03.480 --> 01:39:04.480]   good.
[01:39:04.480 --> 01:39:08.760]   When you see that's what GPT, if you looked at all the output of GPT, most of it was garbage
[01:39:08.760 --> 01:39:11.200]   but they're occasional gems and it would be.
[01:39:11.200 --> 01:39:16.520]   Yeah, it's it's it will give you some pieces, but this is the the worry of it is not only
[01:39:16.520 --> 01:39:23.360]   is the corpus drawn on the web and the web is full of lies nonsense and, you know, Q
[01:39:23.360 --> 01:39:25.400]   and on whatever.
[01:39:25.400 --> 01:39:29.240]   But also as these things start getting built, people are going to start using to generate
[01:39:29.240 --> 01:39:35.120]   more web pages to, you know, create sites about something that Google will find.
[01:39:35.120 --> 01:39:36.120]   That was always an issue.
[01:39:36.120 --> 01:39:39.560]   People do it all the time is called Google farming.
[01:39:39.560 --> 01:39:40.560]   Yeah.
[01:39:40.560 --> 01:39:41.560]   Yeah.
[01:39:41.560 --> 01:39:44.160]   And they'll feed it into one language and back to a different language to get a different
[01:39:44.160 --> 01:39:46.760]   bit of prose or run it through an adjective, dropper or whatever.
[01:39:46.760 --> 01:39:52.240]   And it tends to look nonsense, but it's it can be good enough to fool the search engine
[01:39:52.240 --> 01:39:53.240]   now.
[01:39:53.240 --> 01:39:56.840]   Now, as the search engine gets better and these other things get better at making up
[01:39:56.840 --> 01:40:02.640]   nonsense, you're basically building a distributed, generated versus a serial network, you're
[01:40:02.640 --> 01:40:06.040]   building network that's trying to fool one computer with the output of another computer
[01:40:06.040 --> 01:40:08.240]   and it's already back and forth.
[01:40:08.240 --> 01:40:13.000]   And basically that's, you know, we're filling the web with more and more of this stuff and
[01:40:13.000 --> 01:40:15.720]   these generators generating it and the indexes are finding it and they're feeding to the
[01:40:15.720 --> 01:40:18.720]   next vote in generation, I'll be go.
[01:40:18.720 --> 01:40:28.320]   So yeah, and it and, you know, the fact that so much of so much of the sort of public discourse
[01:40:28.320 --> 01:40:35.120]   is queuing on like nonsense now is a worrying part of it for me.
[01:40:35.120 --> 01:40:41.840]   The researchers say we call an NLP natural language programming researchers to carefully
[01:40:41.840 --> 01:40:46.720]   weigh these risks while pursuing this research direction, consider whether the benefits
[01:40:46.720 --> 01:40:52.560]   outweigh the risks and investigate dual use scenarios utilizing the many techniques that
[01:40:52.560 --> 01:40:57.440]   of those from value sensitive design, for example, that have been put forth.
[01:40:57.440 --> 01:41:02.880]   You know, this is actually, this is something that needs to be heard and it's shameful that
[01:41:02.880 --> 01:41:04.880]   Google fired these people.
[01:41:04.880 --> 01:41:11.320]   And I still don't understand, you know, forget they've violated a rule or didn't violate
[01:41:11.320 --> 01:41:12.320]   a rule or anything.
[01:41:12.320 --> 01:41:16.240]   Obviously, Google's a bigger company than that older than smarter than that.
[01:41:16.240 --> 01:41:18.440]   What was the danger to Google in this?
[01:41:18.440 --> 01:41:20.160]   That this is the essence of a language company?
[01:41:20.160 --> 01:41:23.120]   This is the essence of a document of everything they've been doing.
[01:41:23.120 --> 01:41:24.840]   They do has danger.
[01:41:24.840 --> 01:41:25.840]   Is that is that?
[01:41:25.840 --> 01:41:32.000]   Yeah, they're basically saying they're saying this large thing that you think is a significant
[01:41:32.000 --> 01:41:35.040]   part of your business is potentially dangerous.
[01:41:35.040 --> 01:41:40.480]   And I think that the row was you need to turn this down a bit and say, but we're working
[01:41:40.480 --> 01:41:41.560]   on it, wasn't it?
[01:41:41.560 --> 01:41:46.720]   That was the gist of what Dean was telling them.
[01:41:46.720 --> 01:41:55.280]   And then that got escalated into firing and endless ridiculousness, which was probably
[01:41:55.280 --> 01:42:02.400]   almost certainly them treating people differently for their racial background and so on.
[01:42:02.400 --> 01:42:04.680]   That was a clear part of that.
[01:42:04.680 --> 01:42:11.800]   But yeah, no, it's and so it does worry me and it worries me when Google is sort of making
[01:42:11.800 --> 01:42:19.880]   trying to make that the centerpiece of what they're showing and sort of sweeping this aside
[01:42:19.880 --> 01:42:21.160]   and not discussing it.
[01:42:21.160 --> 01:42:25.000]   So I think we need to keep reading this stuff and thinking about it very carefully.
[01:42:25.000 --> 01:42:35.240]   Listen, Bruce Stearning wrote a thing in Wired yesterday as well, which was his worry
[01:42:35.240 --> 01:42:43.320]   is we're starting to teach the AIs how to sort of fast code and experiment with code
[01:42:43.320 --> 01:42:44.960]   and hack.
[01:42:44.960 --> 01:42:49.880]   And so they're going to start when you do that, they're going to start doing things that
[01:42:49.880 --> 01:42:54.040]   people wouldn't think of because that's what they do.
[01:42:54.040 --> 01:42:59.160]   If you give something like DeepMind, a constrained domain like a game, then it knows what the
[01:42:59.160 --> 01:43:02.920]   rules are, it knows what the win condition is, it can do very well.
[01:43:02.920 --> 01:43:07.800]   If you don't specify that very clearly, if you give a real world problem, then it will
[01:43:07.800 --> 01:43:11.200]   probably find loopholes in the rules you didn't even realize were there because you wouldn't
[01:43:11.200 --> 01:43:13.480]   even think that that could be done.
[01:43:13.480 --> 01:43:18.320]   And there's lots of examples of that from AIs and don't know if there are only things
[01:43:18.320 --> 01:43:20.480]   like that in the past.
[01:43:20.480 --> 01:43:30.360]   So there is the worry that if you start getting them to debug code, write code, run testing,
[01:43:30.360 --> 01:43:37.360]   they're going to end up end running the financial system or creating even more non-scalable financial
[01:43:37.360 --> 01:43:46.080]   panics than we had at the moment.
[01:43:46.080 --> 01:43:47.080]   Okay.
[01:43:47.080 --> 01:43:48.080]   Yeah, here.
[01:43:48.080 --> 01:43:49.080]   Hassening.
[01:43:49.080 --> 01:43:53.120]   I guess I hate to move on because it's such an important topic, but I think we can move
[01:43:53.120 --> 01:43:54.120]   on.
[01:43:54.120 --> 01:43:57.040]   I think it's fascinating.
[01:43:57.040 --> 01:43:59.120]   I'm glad we could get you.
[01:43:59.120 --> 01:44:02.880]   Kevin, we should have you on more often, I guess, is the best thing I can say.
[01:44:02.880 --> 01:44:06.440]   Yeah, I think the problem was that I was traveling to London a lot, which meant it wasn't anywhere.
[01:44:06.440 --> 01:44:07.640]   They're a good connection.
[01:44:07.640 --> 01:44:09.600]   So that may be the best for the issue at the moment.
[01:44:09.600 --> 01:44:10.600]   Good.
[01:44:10.600 --> 01:44:11.600]   Where are you living now?
[01:44:11.600 --> 01:44:12.600]   I'm in North Yorkshire.
[01:44:12.600 --> 01:44:15.160]   You can see the future behind me.
[01:44:15.160 --> 01:44:18.080]   Those are the Moors, the famous Moors of Yorkshire.
[01:44:18.080 --> 01:44:19.080]   Yes.
[01:44:19.080 --> 01:44:21.960]   That's the top of Roseby topping, which is just around the corner.
[01:44:21.960 --> 01:44:22.960]   Be careful.
[01:44:22.960 --> 01:44:24.960]   I hear those are the Moors.
[01:44:24.960 --> 01:44:26.920]   I thought the Moors were flat.
[01:44:26.920 --> 01:44:30.200]   No, the Moors are flat, but they're on top of hills.
[01:44:30.200 --> 01:44:31.200]   So that's the edge of the Moors.
[01:44:31.200 --> 01:44:37.760]   So this is like Roseby topping, which is a local hill that is more land and then looks
[01:44:37.760 --> 01:44:38.760]   down over the field.
[01:44:38.760 --> 01:44:40.760]   So I'm looking from the top of that across to the sea.
[01:44:40.760 --> 01:44:42.400]   So it's like a plateau?
[01:44:42.400 --> 01:44:43.400]   Yeah.
[01:44:43.400 --> 01:44:45.440]   I didn't know that either, Stacey.
[01:44:45.440 --> 01:44:47.080]   I thought they were flat too.
[01:44:47.080 --> 01:44:48.080]   Sorry.
[01:44:48.080 --> 01:44:49.400]   People are like, "Oh my gosh.
[01:44:49.400 --> 01:44:52.480]   All my reading of British literature has just been upended."
[01:44:52.480 --> 01:44:54.480]   Well, I know.
[01:44:54.480 --> 01:44:57.040]   Stay away if you hear any hounds.
[01:44:57.040 --> 01:44:58.040]   Stay well away.
[01:44:58.040 --> 01:44:59.040]   Okay.
[01:44:59.040 --> 01:45:00.040]   That's all I know.
[01:45:00.040 --> 01:45:02.040]   I mean, they're kind of like...
[01:45:02.040 --> 01:45:05.640]   They're like, "The chaperone are getting California except that it's a different climate, so
[01:45:05.640 --> 01:45:06.920]   it's not as dry."
[01:45:06.920 --> 01:45:09.840]   So you know when you get the hills that they're high enough that they don't have trees on,
[01:45:09.840 --> 01:45:10.840]   they've just got scrubby bits.
[01:45:10.840 --> 01:45:11.840]   But they're green.
[01:45:11.840 --> 01:45:13.720]   But they're green because it's wet.
[01:45:13.720 --> 01:45:17.040]   So while they have Heather on and they have scrubby little bushes and things, they're
[01:45:17.040 --> 01:45:18.040]   gorgeous.
[01:45:18.040 --> 01:45:21.640]   Would peep bogs appear in the wars or would they be on ground level?
[01:45:21.640 --> 01:45:23.360]   They'd be low down.
[01:45:23.360 --> 01:45:24.360]   Okay.
[01:45:24.360 --> 01:45:25.360]   Yeah.
[01:45:25.360 --> 01:45:27.480]   Well, you could say no wars had height.
[01:45:27.480 --> 01:45:32.840]   It's kind of pathetic, but basically our entire knowledge of Britain comes from literature
[01:45:32.840 --> 01:45:34.280]   written in the 19th century.
[01:45:34.280 --> 01:45:35.280]   I'm just saying.
[01:45:35.280 --> 01:45:36.280]   Yeah.
[01:45:36.280 --> 01:45:37.280]   You can watch...
[01:45:37.280 --> 01:45:38.560]   Oh, which is great and small.
[01:45:38.560 --> 01:45:39.560]   Love it.
[01:45:39.560 --> 01:45:40.560]   And that's in Yorkshire.
[01:45:40.560 --> 01:45:42.040]   That's certainly here.
[01:45:42.040 --> 01:45:43.040]   Yeah.
[01:45:43.040 --> 01:45:46.840]   I watched the original, which I loved many, many, many years ago, and they've remade it.
[01:45:46.840 --> 01:45:47.840]   I wish they would make more of their...
[01:45:47.840 --> 01:45:49.240]   Yeah, I'll know my opinion on this.
[01:45:49.240 --> 01:45:50.240]   I know.
[01:45:50.240 --> 01:45:51.440]   That's because you know what?
[01:45:51.440 --> 01:45:58.040]   This is a deep personal childhood trauma not related to the quality of the literature
[01:45:58.040 --> 01:45:59.240]   or the TV show.
[01:45:59.240 --> 01:46:00.240]   I'm just saying.
[01:46:00.240 --> 01:46:02.960]   It wasn't trauma.
[01:46:02.960 --> 01:46:07.880]   It was just like a realization that my chosen career path was not the right one because
[01:46:07.880 --> 01:46:09.880]   I don't like sticking my hand up orifices.
[01:46:09.880 --> 01:46:10.880]   Yeah.
[01:46:10.880 --> 01:46:15.360]   Well, admittedly, you should be grateful for learning this quite a bit of that and all
[01:46:15.360 --> 01:46:18.040]   the creatures great and small, but it is not in fact the most...
[01:46:18.040 --> 01:46:19.720]   The best part of it or anything.
[01:46:19.720 --> 01:46:23.680]   It's not the thing I do need just the part she focused on.
[01:46:23.680 --> 01:46:26.080]   It was very distracting.
[01:46:26.080 --> 01:46:28.640]   I was like, "Oh, no.
[01:46:28.640 --> 01:46:30.320]   Oh, oh."
[01:46:30.320 --> 01:46:31.320]   Okay.
[01:46:31.320 --> 01:46:33.560]   More problems for Waymo.
[01:46:33.560 --> 01:46:36.960]   I'm going to go with the chat room on this one.
[01:46:36.960 --> 01:46:41.000]   ScooterX posting this historical story from earlier this week.
[01:46:41.000 --> 01:46:47.340]   The driverless Waymo's Google self-driving enterprise got stuck in traffic and then
[01:46:47.340 --> 01:46:51.200]   tried to run away from its support crew.
[01:46:51.200 --> 01:46:53.200]   This is in Chandler, Arizona.
[01:46:53.200 --> 01:46:54.920]   It got stuck at an intersection.
[01:46:54.920 --> 01:46:57.640]   The company set a roadside assistance team to extract it.
[01:46:57.640 --> 01:47:02.840]   When the crew arrived, the vehicle started to drive away before pulling over and completely
[01:47:02.840 --> 01:47:04.600]   blocking a three-lane road.
[01:47:04.600 --> 01:47:06.680]   It lost its mind.
[01:47:06.680 --> 01:47:10.600]   In fact, if you want, you can watch the video.
[01:47:10.600 --> 01:47:11.600]   It's on YouTube.
[01:47:11.600 --> 01:47:14.320]   I'll turn the volume down on this one.
[01:47:14.320 --> 01:47:18.240]   I guess there was a guy in the car at the time.
[01:47:18.240 --> 01:47:21.280]   Isn't there an off switch?
[01:47:21.280 --> 01:47:23.560]   This is a construction show.
[01:47:23.560 --> 01:47:26.120]   At one point he says, "Well, this is interesting.
[01:47:26.120 --> 01:47:27.960]   I actually have seen this guy's videos.
[01:47:27.960 --> 01:47:34.360]   He takes Waymo's and posts on Reddit and elsewhere videos of his experience in the self-driving
[01:47:34.360 --> 01:47:39.240]   vehicle, which most of the time is pretty normal."
[01:47:39.240 --> 01:47:40.240]   "Bennel.
[01:47:40.240 --> 01:47:41.680]   Pre-key, but no hold."
[01:47:41.680 --> 01:47:42.840]   "Occasionally, terrifying."
[01:47:42.840 --> 01:47:45.440]   I don't know if we want to watch the whole thing.
[01:47:45.440 --> 01:47:47.080]   Let me see where.
[01:47:47.080 --> 01:47:54.440]   Let's first go to the difficulty that it found.
[01:47:54.440 --> 01:47:58.960]   An unprotected left.
[01:47:58.960 --> 01:48:01.440]   Right lane change.
[01:48:01.440 --> 01:48:09.200]   It stops at a traffic cone, apparently.
[01:48:09.200 --> 01:48:12.320]   He called support.
[01:48:12.320 --> 01:48:14.920]   The rider called support.
[01:48:14.920 --> 01:48:17.840]   It said our roadside assistance team is six minutes away.
[01:48:17.840 --> 01:48:19.520]   See there it is.
[01:48:19.520 --> 01:48:25.440]   It's not moving because apparently this orange traffic cone is confusing it.
[01:48:25.440 --> 01:48:28.600]   When the roadside assistance, there's the cone, by the way.
[01:48:28.600 --> 01:48:29.600]   Here's the cone.
[01:48:29.600 --> 01:48:30.600]   Yeah.
[01:48:30.600 --> 01:48:32.040]   It gets stuck behind another cone.
[01:48:32.040 --> 01:48:33.040]   Here's the road.
[01:48:33.040 --> 01:48:34.040]   You can turn up the audio.
[01:48:34.040 --> 01:48:37.600]   Now it's blocking the entire road.
[01:48:37.600 --> 01:48:39.160]   Ah, okay.
[01:48:39.160 --> 01:48:41.960]   This poor guy.
[01:48:41.960 --> 01:48:44.320]   Now what happens if you're the passive driver in there?
[01:48:44.320 --> 01:48:46.520]   If he gets out to get rid of that, then it drives off without him?
[01:48:46.520 --> 01:48:48.800]   You feel some responsibility.
[01:48:48.800 --> 01:48:52.680]   Okay, here's...
[01:48:52.680 --> 01:48:58.880]   The car makes a turn.
[01:48:58.880 --> 01:49:01.560]   Our team is working to get you moving, figuring this out.
[01:49:01.560 --> 01:49:03.480]   God, this would be release.
[01:49:03.480 --> 01:49:06.840]   We're going to have to edit this to make this interesting.
[01:49:06.840 --> 01:49:10.320]   So it's back out.
[01:49:10.320 --> 01:49:14.600]   And now it's blocking the whole lane instead of half of it.
[01:49:14.600 --> 01:49:20.040]   Actually, now it does.
[01:49:20.040 --> 01:49:22.760]   Roadside assistance finally arrives.
[01:49:22.760 --> 01:49:25.200]   I like this.
[01:49:25.200 --> 01:49:33.840]   So now they're going to exit the vehicle and the autonomous specialist side.
[01:49:33.840 --> 01:49:34.840]   Okay.
[01:49:34.840 --> 01:49:37.560]   Exit on the autonomous specialist side.
[01:49:37.560 --> 01:49:38.560]   Gotcha.
[01:49:38.560 --> 01:49:39.560]   Okay, so there's a...
[01:49:39.560 --> 01:49:40.960]   It's totally known as the driver.
[01:49:40.960 --> 01:49:41.960]   Look at the wheels doing something.
[01:49:41.960 --> 01:49:44.280]   I'm a little nervous about the wheels driving.
[01:49:44.280 --> 01:49:46.240]   Yeah, I wouldn't want to get out of the car.
[01:49:46.240 --> 01:49:47.640]   I want to see it all.
[01:49:47.640 --> 01:49:48.640]   Oh, oh, it's...
[01:49:48.640 --> 01:49:51.280]   The guy got out of the van and now the vehicle's leaving.
[01:49:51.280 --> 01:49:53.040]   Oh, he got...
[01:49:53.040 --> 01:49:54.040]   Ah...
[01:49:54.040 --> 01:49:56.440]   Is it hard to go off again?
[01:49:56.440 --> 01:49:58.240]   Yeah, yeah, yeah.
[01:49:58.240 --> 01:49:59.240]   Yes.
[01:49:59.240 --> 01:50:00.240]   Well, a little bit.
[01:50:00.240 --> 01:50:02.040]   This is exactly what I'm afraid of.
[01:50:02.040 --> 01:50:05.040]   Don't they have a control button where they can shut it down?
[01:50:05.040 --> 01:50:07.040]   You think they don't even know what's going on in the car?
[01:50:07.040 --> 01:50:08.040]   Apparently they don't.
[01:50:08.040 --> 01:50:09.040]   How?
[01:50:09.040 --> 01:50:10.040]   This is probably...
[01:50:10.040 --> 01:50:11.040]   How?
[01:50:11.040 --> 01:50:15.560]   Isn't this the Silicon Valley thing where it drives into a container and goes into an
[01:50:15.560 --> 01:50:16.560]   island?
[01:50:16.560 --> 01:50:18.320]   It's just like the TV show, exactly.
[01:50:18.320 --> 01:50:19.320]   Yes.
[01:50:19.320 --> 01:50:20.320]   Yeah.
[01:50:20.320 --> 01:50:24.000]   It's like you're at the mercy of this thing that's lost its mind.
[01:50:24.000 --> 01:50:33.080]   It actually reminds me of how old people drive.
[01:50:33.080 --> 01:50:36.640]   No, in the sense that like...
[01:50:36.640 --> 01:50:41.800]   No, when you throw something or a very young driver, you throw something new and they have
[01:50:41.800 --> 01:50:42.800]   a frozen...
[01:50:42.800 --> 01:50:43.800]   Yeah, yeah.
[01:50:43.800 --> 01:50:46.480]   Yeah, we're experiencing that right now with Michael, he's our 18-year-old.
[01:50:46.480 --> 01:50:51.160]   The thing is the people of Chandler, Arizona are so used to these idiot Weamo vehicles just
[01:50:51.160 --> 01:50:56.040]   doing dumb things that they're just kind of driving around it.
[01:50:56.040 --> 01:51:02.640]   I assume they have like, instead of a student driver, they have like AI driver stickers on
[01:51:02.640 --> 01:51:03.640]   them.
[01:51:03.640 --> 01:51:04.640]   They've got big things on the roof.
[01:51:04.640 --> 01:51:05.640]   Weamo?
[01:51:05.640 --> 01:51:06.640]   You know it's a Weamo.
[01:51:06.640 --> 01:51:07.640]   Yeah, it says Weamo on the side.
[01:51:07.640 --> 01:51:08.640]   Oh, after this?
[01:51:08.640 --> 01:51:09.640]   Yeah.
[01:51:09.640 --> 01:51:13.080]   I guess he's got to do the full walk around too.
[01:51:13.080 --> 01:51:15.520]   You can see behind him there's an emergency truck.
[01:51:15.520 --> 01:51:17.080]   I don't know if you can see it.
[01:51:17.080 --> 01:51:20.360]   I actually should be able to get into that driver's...
[01:51:20.360 --> 01:51:25.360]   Is that just what I've seen them do in the past?
[01:51:25.360 --> 01:51:27.840]   Just walk all the way around the vehicle and then...
[01:51:27.840 --> 01:51:28.840]   Yeah.
[01:51:28.840 --> 01:51:30.760]   Apparently it's happened to before.
[01:51:30.760 --> 01:51:33.960]   Anyway, that...
[01:51:33.960 --> 01:51:35.320]   You should watch the video.
[01:51:35.320 --> 01:51:36.320]   It's...
[01:51:36.320 --> 01:51:38.320]   The car should not take off.
[01:51:38.320 --> 01:51:39.320]   It's a point.
[01:51:39.320 --> 01:51:40.320]   It shouldn't.
[01:51:40.320 --> 01:51:41.320]   It should not move.
[01:51:41.320 --> 01:51:42.320]   It should not move.
[01:51:42.320 --> 01:51:43.320]   It should not move.
[01:51:43.320 --> 01:51:44.320]   That was the operative verb.
[01:51:44.320 --> 01:51:47.320]   It owes $30 for this ride.
[01:51:47.320 --> 01:51:53.160]   Oh, Lord.
[01:51:53.160 --> 01:52:00.160]   Yeah, and when this happens to you, does the service team take you where you were trying
[01:52:00.160 --> 01:52:01.160]   to go?
[01:52:01.160 --> 01:52:02.720]   I don't know.
[01:52:02.720 --> 01:52:05.640]   You certainly don't want to call another Weamo, do you?
[01:52:05.640 --> 01:52:06.640]   I see.
[01:52:06.640 --> 01:52:07.640]   I probably would not.
[01:52:07.640 --> 01:52:08.640]   Yeah.
[01:52:08.640 --> 01:52:14.120]   So, he's actually blurred the picture of the support driver and won't play his voice.
[01:52:14.120 --> 01:52:15.120]   Hello.
[01:52:15.120 --> 01:52:16.120]   Good afternoon.
[01:52:16.120 --> 01:52:18.120]   I'm with the Weamo Roadside Support.
[01:52:18.120 --> 01:52:22.160]   He's going to try and get you out of this situation as safely as we possibly can.
[01:52:22.160 --> 01:52:23.160]   Excellent.
[01:52:23.160 --> 01:52:24.160]   Thank you.
[01:52:24.160 --> 01:52:29.880]   I should have left the guys' voice and the guys going, "Why did I make him do that?"
[01:52:29.880 --> 01:52:30.880]   Yes.
[01:52:30.880 --> 01:52:32.400]   Now he's trying to drive the car.
[01:52:32.400 --> 01:52:38.200]   I just need to make sure that you have drop off destination for Joel.
[01:52:38.200 --> 01:52:40.280]   I just got in the car.
[01:52:40.280 --> 01:52:43.360]   Oh, anyway.
[01:52:43.360 --> 01:52:44.360]   This is...
[01:52:44.360 --> 01:52:45.360]   Yeah.
[01:52:45.360 --> 01:52:48.320]   And I'm pulling it up right now.
[01:52:48.320 --> 01:52:49.320]   Miss...
[01:52:49.320 --> 01:52:50.320]   Miss...
[01:52:50.320 --> 01:52:51.320]   Miss fired.
[01:52:51.320 --> 01:52:52.320]   Yes, I do have it.
[01:52:52.320 --> 01:52:53.320]   So to speak.
[01:52:53.320 --> 01:52:54.320]   All right, that's fine.
[01:52:54.320 --> 01:52:55.320]   We've had enough.
[01:52:55.320 --> 01:52:56.320]   Thank you very much.
[01:52:56.320 --> 01:53:00.560]   We're going to wrap this up because Stacey's got a thing to go to.
[01:53:00.560 --> 01:53:02.000]   You don't have to.
[01:53:02.000 --> 01:53:03.760]   No, I insist.
[01:53:03.760 --> 01:53:08.240]   Besides, two hours of this is more than enough.
[01:53:08.240 --> 01:53:10.240]   Believe me, I don't want that.
[01:53:10.240 --> 01:53:11.240]   TPUs.
[01:53:11.240 --> 01:53:12.240]   Who cases?
[01:53:12.240 --> 01:53:13.240]   Do we care?
[01:53:13.240 --> 01:53:14.240]   They updated their TPUs?
[01:53:14.240 --> 01:53:15.240]   No.
[01:53:15.240 --> 01:53:16.240]   I like it that they're going...
[01:53:16.240 --> 01:53:17.400]   I'm going to put it in a better AI.
[01:53:17.400 --> 01:53:20.880]   I like it that they're going for a carbon-free energy.
[01:53:20.880 --> 01:53:21.880]   Good.
[01:53:21.880 --> 01:53:24.560]   That's a good thing.
[01:53:24.560 --> 01:53:28.480]   Yeah, that was kind of it.
[01:53:28.480 --> 01:53:30.480]   That's kind of the whole story.
[01:53:30.480 --> 01:53:33.000]   Should we play the changelog music?
[01:53:33.000 --> 01:53:34.000]   And I'll just run through it.
[01:53:34.000 --> 01:53:35.000]   I want to...
[01:53:35.000 --> 01:53:36.000]   Make it the fastest.
[01:53:36.000 --> 01:53:37.000]   The fastest.
[01:53:37.000 --> 01:53:38.000]   The world's fastest changelog.
[01:53:38.000 --> 01:53:42.040]   Ladies and gentlemen, I give you the Google changelog.
[01:53:42.040 --> 01:53:46.680]   The Google changelog.
[01:53:46.680 --> 01:53:50.760]   Actually the very first item on the changelog is something I'm pretty excited about.
[01:53:50.760 --> 01:53:52.040]   Google Photos got updated.
[01:53:52.040 --> 01:53:53.040]   They did talk about that.
[01:53:53.040 --> 01:53:54.680]   They've added a new locked folder.
[01:53:54.680 --> 01:53:58.120]   So you can put all your nudes in there.
[01:53:58.120 --> 01:54:01.280]   Unclear who it's locked from.
[01:54:01.280 --> 01:54:02.960]   Is it locked from Google?
[01:54:02.960 --> 01:54:04.280]   Does Google have the keys?
[01:54:04.280 --> 01:54:06.280]   We'll have to find out.
[01:54:06.280 --> 01:54:07.600]   Seems like they probably do.
[01:54:07.600 --> 01:54:10.040]   I'm just saying.
[01:54:10.040 --> 01:54:13.760]   It's historically difficult to do encrypted cloud.
[01:54:13.760 --> 01:54:19.560]   They have new memories and they previewed something which is extremely creepy.
[01:54:19.560 --> 01:54:22.080]   And I think it's really to get parity with Apple.
[01:54:22.080 --> 01:54:25.400]   So Apple has the live photos feature.
[01:54:25.400 --> 01:54:29.640]   When you take a picture with the iPhone, if this is turned on and it is by default,
[01:54:29.640 --> 01:54:32.840]   it snaps many pictures, picks the best still out of that.
[01:54:32.840 --> 01:54:34.120]   That becomes the still.
[01:54:34.120 --> 01:54:38.360]   But if you press and hold the still, you can see a couple of seconds of video before
[01:54:38.360 --> 01:54:39.360]   and after.
[01:54:39.360 --> 01:54:43.840]   So Google pointed out that a lot of times you take pictures of the kids or the family.
[01:54:43.840 --> 01:54:46.480]   You take a bunch of images trying to find the best one.
[01:54:46.480 --> 01:54:51.440]   Well, they're going to take those images, interpolate frames in between them, basically
[01:54:51.440 --> 01:54:57.880]   turning these images into a morph, which if you ask me, always looks creepy.
[01:54:57.880 --> 01:55:00.640]   So this is called cinematic moments.
[01:55:00.640 --> 01:55:01.640]   Deep fake.
[01:55:01.640 --> 01:55:03.280]   It's not a deep fake.
[01:55:03.280 --> 01:55:04.840]   It's a morph.
[01:55:04.840 --> 01:55:05.840]   But you know, morphing.
[01:55:05.840 --> 01:55:07.520]   What's between the two things is faked.
[01:55:07.520 --> 01:55:08.520]   Yeah.
[01:55:08.520 --> 01:55:09.520]   I mean, you're right.
[01:55:09.520 --> 01:55:10.880]   What's there is not real.
[01:55:10.880 --> 01:55:12.720]   They're going to it's interpolated.
[01:55:12.720 --> 01:55:14.240]   It's a fast fake.
[01:55:14.240 --> 01:55:15.240]   It's a fast.
[01:55:15.240 --> 01:55:18.440]   Actually, if you've ever done morphing, which has been around for decades, it's going to
[01:55:18.440 --> 01:55:20.000]   be very slow.
[01:55:20.000 --> 01:55:22.880]   The nice thing is it's a pretty quick morph, I guess.
[01:55:22.880 --> 01:55:26.560]   Well, it's doing it in the background, so we don't know how fast it is.
[01:55:26.560 --> 01:55:28.720]   Presumably it's quick.
[01:55:28.720 --> 01:55:32.960]   Google has also added a way to unlock your BMW.
[01:55:32.960 --> 01:55:37.280]   Again, parity with Apple, which also can unlock your BMW from the phone.
[01:55:37.280 --> 01:55:42.600]   But so far, it's only BMW that's adopted these phone is key, car key solutions with
[01:55:42.600 --> 01:55:46.400]   your phone.
[01:55:46.400 --> 01:55:48.640]   I think that's about it.
[01:55:48.640 --> 01:55:53.200]   Kevin, I'm going to give you an opportunity to bring up anything you think you've run
[01:55:53.200 --> 01:55:54.480]   up a lot.
[01:55:54.480 --> 01:55:57.720]   So I appreciate your parents here today on a very short notice.
[01:55:57.720 --> 01:56:03.240]   Ant Pruitt taking the day off because he got his second COVID vaccine, his sequel injection,
[01:56:03.240 --> 01:56:07.120]   as we call it, and his little under the weather.
[01:56:07.120 --> 01:56:08.120]   That's working for him.
[01:56:08.120 --> 01:56:09.120]   That's working.
[01:56:09.120 --> 01:56:10.120]   That's working.
[01:56:10.120 --> 01:56:11.120]   Yeah.
[01:56:11.120 --> 01:56:12.120]   That's good news.
[01:56:12.120 --> 01:56:13.120]   Good sign, yeah.
[01:56:13.120 --> 01:56:15.240]   Google didn't have a wife said she couldn't wait to feel bad for a day.
[01:56:15.240 --> 01:56:16.600]   I know I was disappointed I didn't.
[01:56:16.600 --> 01:56:19.120]   I think I thought, did I get the placebo?
[01:56:19.120 --> 01:56:21.120]   What happened?
[01:56:21.120 --> 01:56:24.240]   Android now powers 3 billion devices.
[01:56:24.240 --> 01:56:29.480]   Google I/O two years ago is 2.5 billion the year before 2 billion, so they're growing.
[01:56:29.480 --> 01:56:30.480]   3 billion devices.
[01:56:30.480 --> 01:56:34.640]   And of course, they say that to remind everybody when Apple says a billion phones in your pockets,
[01:56:34.640 --> 01:56:35.640]   you all, that...
[01:56:35.640 --> 01:56:41.120]   What's important about this has been an event, and a tweet added it up and said that there
[01:56:41.120 --> 01:56:45.440]   are 3 billion Android devices, about 90% of which are phones.
[01:56:45.440 --> 01:56:46.440]   Apple has a billion plus.
[01:56:46.440 --> 01:56:50.600]   There's another 7 to 800 million Android phones in China.
[01:56:50.600 --> 01:56:56.480]   So the total base is now 4.5 billion out of 5.7 billion adults, which is to say that we
[01:56:56.480 --> 01:57:00.160]   are finally now at the fabled last billion.
[01:57:00.160 --> 01:57:01.160]   Wow.
[01:57:01.160 --> 01:57:02.160]   Last billion.
[01:57:02.160 --> 01:57:03.160]   A billion.
[01:57:03.160 --> 01:57:04.160]   Yeah.
[01:57:04.160 --> 01:57:05.160]   Well, isn't the next billion?
[01:57:05.160 --> 01:57:06.160]   It's the last...
[01:57:06.160 --> 01:57:07.160]   Last billion.
[01:57:07.160 --> 01:57:08.160]   Wow.
[01:57:08.160 --> 01:57:09.160]   That's pretty amazing.
[01:57:09.160 --> 01:57:12.200]   And those people probably don't ever want a smartphone or can't afford one.
[01:57:12.200 --> 01:57:14.080]   Or they're just too young for them right now.
[01:57:14.080 --> 01:57:15.080]   Yeah.
[01:57:15.080 --> 01:57:16.080]   Well, he's talking about adults.
[01:57:16.080 --> 01:57:17.080]   Adults.
[01:57:17.080 --> 01:57:18.880]   There's a lot more children in the world.
[01:57:18.880 --> 01:57:21.280]   A lot more.
[01:57:21.280 --> 01:57:22.280]   Yeah.
[01:57:22.280 --> 01:57:23.280]   A few more.
[01:57:23.280 --> 01:57:24.840]   Yeah.
[01:57:24.840 --> 01:57:29.360]   I think we can wrap this sucker up.
[01:57:29.360 --> 01:57:31.040]   You haven't played any drums yet.
[01:57:31.040 --> 01:57:32.040]   Oh, wait a minute.
[01:57:32.040 --> 01:57:34.120]   I was doing the change like I was writing the mill in change like I only did one of the
[01:57:34.120 --> 01:57:35.120]   things.
[01:57:35.120 --> 01:57:36.120]   Sorry.
[01:57:36.120 --> 01:57:37.120]   God, I'm an idiot.
[01:57:37.120 --> 01:57:41.720]   Thank you for keeping me on the track because otherwise the universe ends up in disorder
[01:57:41.720 --> 01:57:43.400]   for a whole week.
[01:57:43.400 --> 01:57:44.400]   Oh, wait.
[01:57:44.400 --> 01:57:45.400]   And bad things will happen.
[01:57:45.400 --> 01:57:46.400]   It's like a nice, nice thing.
[01:57:46.400 --> 01:57:50.840]   When you open the portal, you have to remember to close it.
[01:57:50.840 --> 01:57:52.440]   God, I close it.
[01:57:52.440 --> 01:57:54.440]   There are five new Google Maps updates.
[01:57:54.440 --> 01:57:55.440]   Can I?
[01:57:55.440 --> 01:57:56.440]   One, two, three, four, five.
[01:57:56.440 --> 01:57:57.440]   Oh, no.
[01:57:57.440 --> 01:57:58.440]   We're still the change.
[01:57:58.440 --> 01:57:59.440]   Reduce heartbreaking with routing updates.
[01:57:59.440 --> 01:58:02.840]   Walth this way with enhancement to live view.
[01:58:02.840 --> 01:58:04.120]   Not busy areas at a glance.
[01:58:04.120 --> 01:58:05.760]   I thought that was kind of cool.
[01:58:05.760 --> 01:58:11.680]   Although the Spanish steps are always busy, a map tailored to you, which means if it's
[01:58:11.680 --> 01:58:13.080]   the morning, they'll show you coffee.
[01:58:13.080 --> 01:58:15.200]   If it's night, they'll show you a burger.
[01:58:15.200 --> 01:58:19.600]   Of course, immediately the folks in our discord said, but what about breakfast for dinner?
[01:58:19.600 --> 01:58:21.360]   What are they going to show you then?
[01:58:21.360 --> 01:58:23.880]   And that's the new stuff in maps.
[01:58:23.880 --> 01:58:30.080]   We talked about Google's new workspaces, the smart canvas we talked about Lambda.
[01:58:30.080 --> 01:58:36.640]   Dinner together is kind of Google's response to Windows, your phone that allows your phone
[01:58:36.640 --> 01:58:43.800]   and your computer to pair together, to talk to each other, move between your phone and
[01:58:43.800 --> 01:58:45.920]   Chromebook with ease, unlock your car.
[01:58:45.920 --> 01:58:47.080]   We mentioned that.
[01:58:47.080 --> 01:58:49.440]   So that's the better together segment.
[01:58:49.440 --> 01:58:54.480]   Google is now using duplex to fix your stolen passwords.
[01:58:54.480 --> 01:58:55.480]   What?
[01:58:55.480 --> 01:58:56.800]   I missed this one.
[01:58:56.800 --> 01:58:58.360]   This was in the Chrome segment.
[01:58:58.360 --> 01:59:02.600]   Well, this is where they can take all on your passwords and then they can fix things automatically
[01:59:02.600 --> 01:59:03.600]   for you.
[01:59:03.600 --> 01:59:04.600]   No, no, no.
[01:59:04.600 --> 01:59:05.600]   This is something else.
[01:59:05.600 --> 01:59:07.640]   This is the calling thing.
[01:59:07.640 --> 01:59:13.480]   Duplex was when you called people and they had they called AI calls on your.
[01:59:13.480 --> 01:59:17.120]   Duplex on the web assistant takes over the tedious parts of web browsing, scrolling,
[01:59:17.120 --> 01:59:18.480]   clicking, filling forms.
[01:59:18.480 --> 01:59:20.000]   It is what you thought it was.
[01:59:20.000 --> 01:59:24.000]   It is fixing your passwords automatically for you.
[01:59:24.000 --> 01:59:26.720]   So I always thought duplex was about voice.
[01:59:26.720 --> 01:59:27.720]   Me too.
[01:59:27.720 --> 01:59:28.960]   This is just about AI.
[01:59:28.960 --> 01:59:30.960]   It's just about AI.
[01:59:30.960 --> 01:59:40.440]   So there we mentioned the RSS follow button now in coming to Chrome in Canary soon.
[01:59:40.440 --> 01:59:44.160]   Google's search has added a new Easter egg.
[01:59:44.160 --> 01:59:45.160]   Yeah.
[01:59:45.160 --> 01:59:46.160]   No, stop.
[01:59:46.160 --> 01:59:52.800]   I want to show this.
[01:59:52.800 --> 02:00:00.440]   If you go to Google.com, then you can play the drums and enter DVD screensaver.
[02:00:00.440 --> 02:00:03.320]   That's really everybody loves the DVD screensaver, right?
[02:00:03.320 --> 02:00:06.680]   The game where you watch the screen.
[02:00:06.680 --> 02:00:07.680]   Maybe.
[02:00:07.680 --> 02:00:09.840]   Oh, there it is.
[02:00:09.840 --> 02:00:10.840]   There it is.
[02:00:10.840 --> 02:00:13.160]   I'm watching the results and there's a Google.
[02:00:13.160 --> 02:00:14.440]   Remember the DVD screensaver?
[02:00:14.440 --> 02:00:17.800]   You're playing the game where you're waiting for it to hit the corner.
[02:00:17.800 --> 02:00:19.600]   Of course, I don't.
[02:00:19.600 --> 02:00:24.640]   And that's because it's kind of more of a squish thing.
[02:00:24.640 --> 02:00:26.160]   I don't know what it'll look like when Google.
[02:00:26.160 --> 02:00:28.000]   Should we just watch this till it hits the corner?
[02:00:28.000 --> 02:00:30.520]   It'll be just about as exciting as this show.
[02:00:30.520 --> 02:00:35.280]   Oh, just missed.
[02:00:35.280 --> 02:00:36.280]   Just missed.
[02:00:36.280 --> 02:00:38.080]   Yeah, really.
[02:00:38.080 --> 02:00:40.240]   This is amusing, isn't it?
[02:00:40.240 --> 02:00:45.440]   So three old guys in Stacy watching the Google go to the screen.
[02:00:45.440 --> 02:00:49.040]   I'm actually scrolling through Twitter.
[02:00:49.040 --> 02:00:54.440]   Stacy, did you ever do this or is it just us old guys?
[02:00:54.440 --> 02:00:55.840]   Did you even know there was a game?
[02:00:55.840 --> 02:00:57.960]   I mean, I know I had DVD's.
[02:00:57.960 --> 02:00:58.960]   Yeah.
[02:00:58.960 --> 02:01:03.640]   And you remember that the DVR tapes, if you left the TV on the DVD screensaver would come
[02:01:03.640 --> 02:01:05.160]   on bounce around.
[02:01:05.160 --> 02:01:08.880]   Yeah, I didn't know there was a game until fairly recently.
[02:01:08.880 --> 02:01:10.800]   So apparently there was.
[02:01:10.800 --> 02:01:11.800]   I think it came from the.
[02:01:11.800 --> 02:01:15.040]   I think calling it a game is a bit of a stretch, but yes.
[02:01:15.040 --> 02:01:16.040]   Yes.
[02:01:16.040 --> 02:01:17.040]   It's a game for people who love the games.
[02:01:17.040 --> 02:01:21.440]   I think if you were smoking a lot of special cigarettes, it's a much more fun game.
[02:01:21.440 --> 02:01:25.280]   It feels like something a bunch of people who smoke weed or a bunch of people who are
[02:01:25.280 --> 02:01:27.280]   drinking would do.
[02:01:27.280 --> 02:01:29.280]   Oh, almost.
[02:01:29.280 --> 02:01:31.280]   Oh, no.
[02:01:31.280 --> 02:01:32.280]   No.
[02:01:32.280 --> 02:01:33.280]   Okay, we've seen that.
[02:01:33.280 --> 02:01:34.280]   Okay.
[02:01:34.280 --> 02:01:36.160]   And that's now you can play it.
[02:01:36.160 --> 02:01:39.680]   The Google change law.
[02:01:39.680 --> 02:01:40.680]   All right.
[02:01:40.680 --> 02:01:41.880]   Next, what's next?
[02:01:41.880 --> 02:01:46.520]   Our picks of the week, ladies and gentlemen, let's start our picks of the week.
[02:01:46.520 --> 02:01:49.960]   Now Kevin, you came here a short notice in the middle of the show.
[02:01:49.960 --> 02:01:52.000]   So oh, you do have a pick.
[02:01:52.000 --> 02:01:53.240]   You're amazing.
[02:01:53.240 --> 02:01:55.080]   What is your pick?
[02:01:55.080 --> 02:01:58.640]   My pick is the map of the internet.
[02:01:58.640 --> 02:02:01.280]   So are you remember the case he did this a long time ago?
[02:02:01.280 --> 02:02:04.240]   Someone someone's made a current map of the internet.
[02:02:04.240 --> 02:02:06.640]   If you there's a link to top right, it lets you.
[02:02:06.640 --> 02:02:08.920]   Wow, look at that.
[02:02:08.920 --> 02:02:14.600]   So it's this like it's this huge, huge map with like different land masses with with.
[02:02:14.600 --> 02:02:18.000]   It looks like a real map, but it's websites.
[02:02:18.000 --> 02:02:22.120]   And they're scaled to approximate number of users, I think.
[02:02:22.120 --> 02:02:23.120]   So you can see.
[02:02:23.120 --> 02:02:24.120]   So there's browsers.
[02:02:24.120 --> 02:02:28.080]   If you look really carefully, you think it's in there?
[02:02:28.080 --> 02:02:29.160]   It should be.
[02:02:29.160 --> 02:02:30.160]   I would hope so.
[02:02:30.160 --> 02:02:31.160]   Yeah.
[02:02:31.160 --> 02:02:32.160]   You're on the internet.
[02:02:32.160 --> 02:02:33.160]   Oh, wow.
[02:02:33.160 --> 02:02:36.320]   This is this is from Halcyon Maps.com.
[02:02:36.320 --> 02:02:41.880]   Actually, I'm going to this is one of those places where I'm going to spend more time browsing
[02:02:41.880 --> 02:02:42.880]   their catalog.
[02:02:42.880 --> 02:02:44.640]   I love maps.
[02:02:44.640 --> 02:02:48.560]   Look at the bottom where Antarctica would be the dark web.
[02:02:48.560 --> 02:02:50.560]   Wikipedia is like Greenland.
[02:02:50.560 --> 02:02:54.360]   So I think basically that wouldn't you to buy this and put it on your wall, which would
[02:02:54.360 --> 02:02:55.360]   be fun.
[02:02:55.360 --> 02:02:56.360]   I may end up doing that.
[02:02:56.360 --> 02:02:57.360]   Here's the Q zone.
[02:02:57.360 --> 02:03:02.320]   You know, really what you need is some of that actually changes over time.
[02:03:02.320 --> 02:03:07.320]   So you need a display that links to software that's continuously updating this.
[02:03:07.320 --> 02:03:08.320]   Yes.
[02:03:08.320 --> 02:03:09.320]   Yes.
[02:03:09.320 --> 02:03:10.320]   Yes.
[02:03:10.320 --> 02:03:12.360]   I want the version of this in SVG.
[02:03:12.360 --> 02:03:14.600]   So actually search the text in it rather than actually.
[02:03:14.600 --> 02:03:16.120]   Oh, when that be nice.
[02:03:16.120 --> 02:03:17.120]   Yeah.
[02:03:17.120 --> 02:03:18.520]   We're a touch screen.
[02:03:18.520 --> 02:03:20.160]   You could zoom in.
[02:03:20.160 --> 02:03:25.840]   Here's Facebook, Instagram, Twitter, V contact.
[02:03:25.840 --> 02:03:30.000]   The Russian version there of duck duck go.
[02:03:30.000 --> 02:03:33.320]   Zoom is a pretty big island out of the zoom island.
[02:03:33.320 --> 02:03:39.120]   It's a big old one, but not as big as you know who the island of Google.
[02:03:39.120 --> 02:03:40.120]   It's actually.
[02:03:40.120 --> 02:03:41.120]   Google Stan.
[02:03:41.120 --> 02:03:42.120]   Yeah.
[02:03:42.120 --> 02:03:43.120]   Wow.
[02:03:43.120 --> 02:03:45.040]   That's really fun.
[02:03:45.040 --> 02:03:47.440]   Error correcting plateau.
[02:03:47.440 --> 02:03:48.840]   Google contacts.
[02:03:48.840 --> 02:03:49.840]   Android TV.
[02:03:49.840 --> 02:03:53.160]   Google has a lot of a lot of somebody put a lot of effort in this.
[02:03:53.160 --> 02:03:54.160]   There's two.
[02:03:54.160 --> 02:03:55.560]   This is this is a major effort.
[02:03:55.560 --> 02:03:56.560]   It's a beautiful French.
[02:03:56.560 --> 02:03:57.560]   Yeah.
[02:03:57.560 --> 02:03:59.640]   I think I will get a poster for the wall.
[02:03:59.640 --> 02:04:00.640]   That's pretty cool.
[02:04:00.640 --> 02:04:05.440]   And it all looks like one of those old, old timey maps of the new world.
[02:04:05.440 --> 02:04:08.520]   And then here's a list of major websites blocked in China.
[02:04:08.520 --> 02:04:10.000]   Very handy.
[02:04:10.000 --> 02:04:13.400]   Pop website languages, internet users, percent of population.
[02:04:13.400 --> 02:04:15.520]   It is now greater than 90%.
[02:04:15.520 --> 02:04:17.920]   Oh, no, this is I see here.
[02:04:17.920 --> 02:04:18.920]   This is based.
[02:04:18.920 --> 02:04:21.040]   This is a smaller map.
[02:04:21.040 --> 02:04:23.040]   And Canada is the only one.
[02:04:23.040 --> 02:04:24.960]   Canada and the UK are greater than 90%.
[02:04:24.960 --> 02:04:28.560]   We're actually only, I don't know, down in the lower numbers.
[02:04:28.560 --> 02:04:31.160]   No, Canada looks like it's at 70 to 80%.
[02:04:31.160 --> 02:04:32.160]   Again, colors.
[02:04:32.160 --> 02:04:35.640]   Oh, it's kind of a darker blue.
[02:04:35.640 --> 02:04:36.640]   Who's 90% then?
[02:04:36.640 --> 02:04:38.480]   Who's the darkest blue of all?
[02:04:38.480 --> 02:04:40.480]   Well, I think that's up there.
[02:04:40.480 --> 02:04:41.480]   Norway.
[02:04:41.480 --> 02:04:42.480]   Norway, yeah.
[02:04:42.480 --> 02:04:43.480]   Finland.
[02:04:43.480 --> 02:04:44.480]   All right.
[02:04:44.480 --> 02:04:47.080]   Largest companies by revenue highest selling video games.
[02:04:47.080 --> 02:04:51.680]   Tetris number one Minecraft number two, Grand Theft Auto 5 number three.
[02:04:51.680 --> 02:04:54.200]   Oh, this is the same chart only now.
[02:04:54.200 --> 02:04:56.440]   So it was 2006 versus two.
[02:04:56.440 --> 02:04:57.440]   Okay.
[02:04:57.440 --> 02:04:59.960]   Yes, it's changed.
[02:04:59.960 --> 02:05:04.080]   Created by Martin Vargic of Halcyon Maps.com.
[02:05:04.080 --> 02:05:05.720]   Good pick of the week.
[02:05:05.720 --> 02:05:08.320]   I love this.
[02:05:08.320 --> 02:05:15.880]   I love the colophon, which features 4chan Google, Susan Wojcie, Pierre Omidar.
[02:05:15.880 --> 02:05:17.520]   That's great.
[02:05:17.520 --> 02:05:19.520]   Stacy, you're a thing this week.
[02:05:19.520 --> 02:05:21.120]   You teased it a little bit earlier.
[02:05:21.120 --> 02:05:23.040]   I'm dying to see it now.
[02:05:23.040 --> 02:05:24.200]   Okay.
[02:05:24.200 --> 02:05:28.680]   I'm very sad to tell you that right now the site that I was going to send you to is offline.
[02:05:28.680 --> 02:05:29.680]   Oh, it's down.
[02:05:29.680 --> 02:05:32.080]   So we're going to punt that for next week.
[02:05:32.080 --> 02:05:33.080]   I know.
[02:05:33.080 --> 02:05:34.080]   I'm really sad.
[02:05:34.080 --> 02:05:35.840]   Like I was just on it yesterday.
[02:05:35.840 --> 02:05:37.720]   Boo.
[02:05:37.720 --> 02:05:40.160]   So now I'm going to send you to something just bonkers.
[02:05:40.160 --> 02:05:46.880]   I don't have this because I'm not crazy, but it is the high interiors is making a bed.
[02:05:46.880 --> 02:05:50.520]   Oh, I didn't put it in the rundown because I suck Jason.
[02:05:50.520 --> 02:05:51.960]   I'm sorry.
[02:05:51.960 --> 02:06:00.840]   It is a $13,000 to $40,000 bed that also acts as a, oh, what's it called?
[02:06:00.840 --> 02:06:01.840]   Screen.
[02:06:01.840 --> 02:06:03.640]   So if you click on the link, I dropped in there.
[02:06:03.640 --> 02:06:05.520]   Sounds uncomfortable.
[02:06:05.520 --> 02:06:06.680]   It is a home theater bed.
[02:06:06.680 --> 02:06:11.280]   Oh, no, because it's got it's got sensors in the bed that help you adjust.
[02:06:11.280 --> 02:06:17.000]   Like if you're doing the television or raise you up and then you can watch things in your
[02:06:17.000 --> 02:06:18.760]   little, basically it's a pod.
[02:06:18.760 --> 02:06:20.000]   It's at home entertainment pod.
[02:06:20.000 --> 02:06:21.800]   Oh, I need that.
[02:06:21.800 --> 02:06:24.240]   That's for parents of small children.
[02:06:24.240 --> 02:06:26.800]   A certain number of people will be excited by it.
[02:06:26.800 --> 02:06:27.800]   Yes.
[02:06:27.800 --> 02:06:28.800]   Oh, can you not find it?
[02:06:28.800 --> 02:06:30.480]   Leo, I do not have the bed.
[02:06:30.480 --> 02:06:31.480]   So I cannot show it.
[02:06:31.480 --> 02:06:35.160]   What's the what's the URL again?
[02:06:35.160 --> 02:06:39.800]   It is well, the site that has the best pictures is CE Pro, but that's down.
[02:06:39.800 --> 02:06:40.960]   I put a link in the rundown.
[02:06:40.960 --> 02:06:44.680]   Oh, I see here it is line one 88 $40,000.
[02:06:44.680 --> 02:06:45.680]   Wait a minute.
[02:06:45.680 --> 02:06:49.680]   Well, it's up to $40,000 depends on this TV, probably.
[02:06:49.680 --> 02:06:50.680]   Oh, Jesus.
[02:06:50.680 --> 02:06:51.680]   Yeah.
[02:06:51.680 --> 02:06:55.200]   So if you if you scroll through, you will see it.
[02:06:55.200 --> 02:06:58.240]   Let's view the slideshow, baby.
[02:06:58.240 --> 02:06:59.800]   Smart home theater bed.
[02:06:59.800 --> 02:07:01.120]   You want to come over to my place.
[02:07:01.120 --> 02:07:03.400]   I have a smart home theater bed.
[02:07:03.400 --> 02:07:05.280]   We can Netflix and chill.
[02:07:05.280 --> 02:07:06.280]   Wow.
[02:07:06.280 --> 02:07:08.120]   I've been fascinated by beds lately.
[02:07:08.120 --> 02:07:10.720]   There's been like, I was looking at them.
[02:07:10.720 --> 02:07:12.640]   It automatically adjustable.
[02:07:12.640 --> 02:07:13.640]   The temperature.
[02:07:13.640 --> 02:07:14.640]   Look at that.
[02:07:14.640 --> 02:07:15.640]   Has voice control.
[02:07:15.640 --> 02:07:16.640]   It's fancy.
[02:07:16.640 --> 02:07:18.560]   It's actually kind of cool.
[02:07:18.560 --> 02:07:19.560]   Yeah.
[02:07:19.560 --> 02:07:20.560]   It is.
[02:07:20.560 --> 02:07:21.560]   Yeah.
[02:07:21.560 --> 02:07:25.480]   I know y'all are making fun of it, but you had a studio apartment.
[02:07:25.480 --> 02:07:26.480]   It'd be great.
[02:07:26.480 --> 02:07:30.320]   If you spend a 30 year life in bed, you deserve a smart bed.
[02:07:30.320 --> 02:07:33.320]   I think you spend more like half your life in bed if you had this thing.
[02:07:33.320 --> 02:07:34.320]   Oh, you might.
[02:07:34.320 --> 02:07:35.320]   Look at that.
[02:07:35.320 --> 02:07:36.320]   It's got a nice.
[02:07:36.320 --> 02:07:42.320]   It also has a concierge service that starts when the person wakes up providing a small
[02:07:42.320 --> 02:07:47.520]   clock weather updates and traffic information right there on the screen.
[02:07:47.520 --> 02:07:49.600]   So it's more than just a bed.
[02:07:49.600 --> 02:07:51.320]   This is maybe where I'm going to retire to.
[02:07:51.320 --> 02:07:54.800]   I was thinking Seattle, but maybe this bed would be.
[02:07:54.800 --> 02:07:55.800]   Maybe.
[02:07:55.800 --> 02:07:56.800]   Yeah.
[02:07:56.800 --> 02:07:57.800]   Yeah.
[02:07:57.800 --> 02:07:59.400]   Maybe this bed would be my retirement home.
[02:07:59.400 --> 02:08:01.400]   It costs enough.
[02:08:01.400 --> 02:08:02.400]   Wow.
[02:08:02.400 --> 02:08:04.720]   You've heard about tiny homes, right?
[02:08:04.720 --> 02:08:05.720]   Yeah.
[02:08:05.720 --> 02:08:08.280]   Tiny home, big bed.
[02:08:08.280 --> 02:08:12.360]   For a home and small, but for a bed, it's huge.
[02:08:12.360 --> 02:08:13.360]   That's my pick.
[02:08:13.360 --> 02:08:14.360]   I love that.
[02:08:14.360 --> 02:08:16.520]   That's your pick, but I like it too.
[02:08:16.520 --> 02:08:17.520]   I like it too.
[02:08:17.520 --> 02:08:19.920]   If I can get you to buy it, you know, feel free.
[02:08:19.920 --> 02:08:20.920]   We know how it goes.
[02:08:20.920 --> 02:08:23.160]   And he said us his pick of the week.
[02:08:23.160 --> 02:08:24.160]   He couldn't be here.
[02:08:24.160 --> 02:08:25.160]   He apologizes.
[02:08:25.160 --> 02:08:27.680]   He's feeling a little under the weather, but he sent us this.
[02:08:27.680 --> 02:08:28.680]   Focus by folks.
[02:08:28.680 --> 02:08:29.680]   He doesn't.
[02:08:29.680 --> 02:08:32.000]   $15 if you'd like to buy this.
[02:08:32.000 --> 02:08:35.800]   It looks like a quarter or a token.
[02:08:35.800 --> 02:08:41.080]   It's round, made a wood and burned into it are the letters T-U-I-T.
[02:08:41.080 --> 02:08:47.240]   So if you want to get around to it, you can go to a weird guy.com.
[02:08:47.240 --> 02:08:48.840]   Get it?
[02:08:48.840 --> 02:08:52.280]   I'll get around to it.
[02:08:52.280 --> 02:08:53.280]   You know what?
[02:08:53.280 --> 02:08:54.280]   We got to it.
[02:08:54.280 --> 02:08:56.200]   I probably shouldn't have mentioned it because now my wife's going to buy it.
[02:08:56.200 --> 02:08:59.160]   And every time I say I'll get around to it, she's going to say, oh, no, I already got
[02:08:59.160 --> 02:09:01.320]   you one.
[02:09:01.320 --> 02:09:02.320]   Number of the week.
[02:09:02.320 --> 02:09:03.320]   Number of the week.
[02:09:03.320 --> 02:09:04.320]   Number of the week.
[02:09:04.320 --> 02:09:05.320]   Number of the week.
[02:09:05.320 --> 02:09:06.320]   Okay.
[02:09:06.320 --> 02:09:07.320]   I'm going to go with five.
[02:09:07.320 --> 02:09:08.320]   No more.
[02:09:08.320 --> 02:09:09.320]   Which is.
[02:09:09.320 --> 02:09:10.320]   The only half four.
[02:09:10.320 --> 02:09:11.320]   The New York Times.
[02:09:11.320 --> 02:09:12.320]   New York Times.
[02:09:12.320 --> 02:09:13.320]   No, no, not five numbers.
[02:09:13.320 --> 02:09:14.320]   Just the number five.
[02:09:14.320 --> 02:09:15.320]   The New York Times.
[02:09:15.320 --> 02:09:19.560]   And it's, I think, expose on Apple and China.
[02:09:19.560 --> 02:09:22.480]   So there were five takeaways about what Apple does there.
[02:09:22.480 --> 02:09:24.280]   Oh, this was kind of mind-boggling.
[02:09:24.280 --> 02:09:25.280]   Yeah.
[02:09:25.280 --> 02:09:28.520]   Apple stores customer data on Chinese government servers.
[02:09:28.520 --> 02:09:29.520]   Yes.
[02:09:29.520 --> 02:09:32.720]   Apple now shares customer data with the Chinese government.
[02:09:32.720 --> 02:09:33.720]   Yes.
[02:09:33.720 --> 02:09:34.720]   Three.
[02:09:34.720 --> 02:09:37.120]   Apple proactively removes apps to placate Chinese officials.
[02:09:37.120 --> 02:09:38.120]   Yes.
[02:09:38.120 --> 02:09:39.120]   Four.
[02:09:39.120 --> 02:09:42.240]   Apple banned apps from a Communist Party critic.
[02:09:42.240 --> 02:09:47.000]   And five, tens of thousands of iPhone apps have disappeared in China.
[02:09:47.000 --> 02:09:49.920]   So this is the price of doing business in China.
[02:09:49.920 --> 02:09:55.080]   And we should mention in the article, Apple says, well, okay, we had to take those apps
[02:09:55.080 --> 02:09:56.440]   down because they were illegal.
[02:09:56.440 --> 02:10:03.000]   But we don't, we don't, we, we, that data is encrypted.
[02:10:03.000 --> 02:10:07.640]   So there they kind of denied it sort of was a non-denial denial denial, but they kind
[02:10:07.640 --> 02:10:08.640]   of denied it.
[02:10:08.640 --> 02:10:09.640]   Yeah.
[02:10:09.640 --> 02:10:12.440]   But I, you know, got to truth in advertising.
[02:10:12.440 --> 02:10:13.440]   Got to mention that.
[02:10:13.440 --> 02:10:18.840]   And I'm waiting for the 295, take a Twitter blue, 299 per month.
[02:10:18.840 --> 02:10:19.840]   299.
[02:10:19.840 --> 02:10:20.840]   Whatever it is, that's so cheap.
[02:10:20.840 --> 02:10:21.840]   I'll do it.
[02:10:21.840 --> 02:10:22.840]   I don't care.
[02:10:22.840 --> 02:10:23.840]   I don't care.
[02:10:23.840 --> 02:10:24.840]   Meanwhile, I like Twitter.
[02:10:24.840 --> 02:10:26.840]   DVD screensaver.
[02:10:26.840 --> 02:10:27.840]   It's stuck.
[02:10:27.840 --> 02:10:30.760]   So go on around.
[02:10:30.760 --> 02:10:32.440]   Thank you everybody for being here.
[02:10:32.440 --> 02:10:36.640]   Hey, the special thanks to Kevin Marks, who joined us on very short news halfway through
[02:10:36.640 --> 02:10:38.560]   the show, indyweb.org.
[02:10:38.560 --> 02:10:40.480]   Is it still going strong, Kevin?
[02:10:40.480 --> 02:10:41.480]   Still very strong.
[02:10:41.480 --> 02:10:42.480]   Yeah.
[02:10:42.480 --> 02:10:43.480]   I think it was fun with that.
[02:10:43.480 --> 02:10:48.480]   In fact, I've been seeing more and more indie web stuff, which is great.
[02:10:48.480 --> 02:10:50.760]   Really, really happy to see people starting to take to it.
[02:10:50.760 --> 02:10:56.560]   We of course are on the Fediverse with our Twitter.social.master.non instance.
[02:10:56.560 --> 02:10:58.160]   I use web hooks all the time.
[02:10:58.160 --> 02:11:02.640]   There's a lot of great stuff, thanks to the work of indie web.
[02:11:02.640 --> 02:11:06.200]   If there's an indie web meeting near you, it will be posted at indyweb.org.
[02:11:06.200 --> 02:11:07.200]   Thank you, Kevin.
[02:11:07.200 --> 02:11:09.720]   Well, they're online at the moment.
[02:11:09.720 --> 02:11:11.720]   So it's easy to join.
[02:11:11.720 --> 02:11:12.720]   Oh, I forgot.
[02:11:12.720 --> 02:11:13.720]   Oh, I forgot.
[02:11:13.720 --> 02:11:21.160]   Yes, there's one coming up tonight, West Coast at in about two hours.
[02:11:21.160 --> 02:11:25.520]   I guess really that's the only geographic thing is it needs to be in your time zone.
[02:11:25.520 --> 02:11:27.600]   Yeah, I missed the up fruit.
[02:11:27.600 --> 02:11:28.600]   Yeah, yeah.
[02:11:28.600 --> 02:11:31.040]   This one, this one's 6 p.m. Pacific.
[02:11:31.040 --> 02:11:34.880]   16 Pacific, yes, in two hours.
[02:11:34.880 --> 02:11:35.880]   Yeah, yeah.
[02:11:35.880 --> 02:11:36.880]   Homebrew website club.
[02:11:36.880 --> 02:11:39.240]   And it's a great club, by the way.
[02:11:39.240 --> 02:11:40.240]   You know what?
[02:11:40.240 --> 02:11:41.880]   I am going to, I am my pop in myself.
[02:11:41.880 --> 02:11:42.880]   That sounds very cool.
[02:11:42.880 --> 02:11:43.880]   Very cool.
[02:11:43.880 --> 02:11:45.560]   Yeah, I'm a big supporter.
[02:11:45.560 --> 02:11:49.240]   I think it's really important that we have an open web.
[02:11:49.240 --> 02:11:53.240]   Mr. Jeff Jarvis.
[02:11:53.240 --> 02:11:58.680]   He is in fact the Leonard Town Professor for journalistic innovation at the.
[02:11:58.680 --> 02:11:59.680]   Great.
[02:11:59.680 --> 02:12:00.680]   Great.
[02:12:00.680 --> 02:12:01.680]   Great.
[02:12:01.680 --> 02:12:09.120]   At the city university of New York, our blob opera rendition.
[02:12:09.120 --> 02:12:10.120]   Thank you, Jeff.
[02:12:10.120 --> 02:12:15.640]   And com, Frank Sinatra once called him a bum former TV guide critic, Jeff Jarvis, ladies
[02:12:15.640 --> 02:12:17.320]   and gentlemen.
[02:12:17.320 --> 02:12:22.080]   And of course, Stacy Higginbotham who's going to run off right now to see what Google has
[02:12:22.080 --> 02:12:23.080]   to say.
[02:12:23.080 --> 02:12:24.080]   She's going to be working.
[02:12:24.080 --> 02:12:25.080]   She's going to be working.
[02:12:25.080 --> 02:12:29.000]   She's going to be working to see what's going to be about home automation.
[02:12:29.000 --> 02:12:30.520]   Eat something delicious.
[02:12:30.520 --> 02:12:31.520]   Have a waffle.
[02:12:31.520 --> 02:12:32.520]   Watch Google.
[02:12:32.520 --> 02:12:34.080]   It's the best way to go to Google.
[02:12:34.080 --> 02:12:35.080]   I owe.
[02:12:35.080 --> 02:12:36.080]   Thank you so much.
[02:12:36.080 --> 02:12:37.080]   Stacy on IOT.com.
[02:12:37.080 --> 02:12:38.080]   Subscribe to our newsletter.
[02:12:38.080 --> 02:12:40.880]   Listen to the podcast with Kevin Tofel.
[02:12:40.880 --> 02:12:46.920]   We do twig every Wednesday at about 2 p.m. Pacific 5 p.m. Eastern 2100 UTC.
[02:12:46.920 --> 02:12:49.840]   You can watch us do it live at twitt.tv/live.
[02:12:49.840 --> 02:12:51.680]   There's also an audio feed there.
[02:12:51.680 --> 02:12:58.140]   Chat with us while we're doing it at irc.twitt.tv on demand shows available at twitt.tv/twitt.
[02:12:58.140 --> 02:13:04.080]   Or on YouTube, there's a twig channel or subscribe in your favorite podcast application.
[02:13:04.080 --> 02:13:05.720]   And that way you'll get it automatically.
[02:13:05.720 --> 02:13:07.960]   The minute it's done, I would appreciate it.
[02:13:07.960 --> 02:13:11.600]   If you leave us a five star review, if your app allows that, it's always nice.
[02:13:11.600 --> 02:13:12.600]   Helps us with discovery.
[02:13:12.600 --> 02:13:17.560]   I'd also like to mention that we have been having great fun with club twitt, which is
[02:13:17.560 --> 02:13:21.880]   our brand new kind of way to support twitt.
[02:13:21.880 --> 02:13:26.640]   We've always had a tip jar, things like this club twitt gives you some additional benefits.
[02:13:26.640 --> 02:13:31.960]   You get ad free versions of all of our shows, all of them, audio or video.
[02:13:31.960 --> 02:13:35.400]   You get a great discord and we're we always have fun in the discord.
[02:13:35.400 --> 02:13:36.600]   We've got a stage here.
[02:13:36.600 --> 02:13:39.240]   We allow people to converse with us.
[02:13:39.240 --> 02:13:44.280]   There's chat subjects on everything, all of our shows, but even many other geeky things.
[02:13:44.280 --> 02:13:46.760]   And Stacy, don't forget, we're going to do a book club with you.
[02:13:46.760 --> 02:13:49.280]   We want to do that in the discord too.
[02:13:49.280 --> 02:13:50.760]   So don't forget that.
[02:13:50.760 --> 02:13:51.760]   Let's get that happening.
[02:13:51.760 --> 02:13:52.760]   Yeah.
[02:13:52.760 --> 02:13:53.760]   Yeah.
[02:13:53.760 --> 02:13:54.760]   Next week, I'll announce a book.
[02:13:54.760 --> 02:13:55.760]   Sorry, y'all.
[02:13:55.760 --> 02:13:56.760]   I forgot it.
[02:13:56.760 --> 02:13:57.760]   That's right.
[02:13:57.760 --> 02:13:58.760]   No, no, no.
[02:13:58.760 --> 02:13:59.760]   Next week.
[02:13:59.760 --> 02:14:01.480]   I will announce a book for you to read and you could probably read it if you start right now.
[02:14:01.480 --> 02:14:05.080]   Between now and Friday, I'm going to be interviewing Andy Weir, the author of what
[02:14:05.080 --> 02:14:09.040]   I think is the already a science fiction classic just came out this month.
[02:14:09.040 --> 02:14:12.440]   Project Hale, Mary, he wrote the Martian.
[02:14:12.440 --> 02:14:17.800]   He wrote Artemis and he has agreed to join us for a special version of triangulation.
[02:14:17.800 --> 02:14:22.200]   It'll be our third time interviewing him this Friday, 3 p.m. Pacific, 6 p.m. Eastern
[02:14:22.200 --> 02:14:23.200]   2200 UTC.
[02:14:23.200 --> 02:14:26.040]   You'll be able to watch it on our live stream.
[02:14:26.040 --> 02:14:29.280]   When the show is done, we'll put it out as a additional triangulation.
[02:14:29.280 --> 02:14:30.840]   We haven't done a show in a while.
[02:14:30.840 --> 02:14:35.040]   We'll also put it on the Twitter events feed because it is an event.
[02:14:35.040 --> 02:14:38.200]   But there is a special deal for club twit members.
[02:14:38.200 --> 02:14:41.520]   You'll be able to ask questions of Andy.
[02:14:41.520 --> 02:14:46.080]   We'll set up a stage and give Andy a new a chance to converse.
[02:14:46.080 --> 02:14:49.680]   So if you are not yet a club twit member, there is still time to join.
[02:14:49.680 --> 02:14:53.800]   Find out more at twit.tv/clubtwit.
[02:14:53.800 --> 02:14:57.520]   There's the discord, the special feed at Twit plus feed and of course all the ad free
[02:14:57.520 --> 02:14:58.880]   feeds, seven bucks a month.
[02:14:58.880 --> 02:14:59.880]   I think it's a good deal.
[02:14:59.880 --> 02:15:06.880]   It really honestly helps us do things like this, like this Andy Weir interview.
[02:15:06.880 --> 02:15:07.880]   Thank you everybody.
[02:15:07.880 --> 02:15:09.440]   Again, twit.tv/clubtwit.
[02:15:09.440 --> 02:15:12.520]   We'll see you next time on This Week in Google.
[02:15:12.520 --> 02:15:13.520]   Bye bye.
[02:15:13.520 --> 02:15:16.560]   Hey, I hope you enjoyed that episode.
[02:15:16.560 --> 02:15:22.200]   If you are interested in checking out all things smart home and Internet of Things, then
[02:15:22.200 --> 02:15:27.520]   you should check out Smart Tech Today, the podcast I'm Mike Asargent to do with my co-host
[02:15:27.520 --> 02:15:29.800]   Matthew Cassinelli.
[02:15:29.800 --> 02:15:34.320]   It's all about the smart home and improving your automations.
[02:15:34.320 --> 02:15:45.440]   [Music]

