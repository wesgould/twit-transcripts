;FFMETADATA1
title=Where Do You Put the Naked People?
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=511
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.600]   It's time for Twig this week in Google. Jeff and Stacey are both here. There's lots to talk about,
[00:00:05.600 --> 00:00:12.400]   including Apple's $1000 Monitor stand, some brand new tools from Google, and my new tiny cell phone.
[00:00:12.400 --> 00:00:14.240]   It's all next this week in Google.
[00:00:14.240 --> 00:00:20.320]   Netcast you love. From people you trust.
[00:00:20.320 --> 00:00:25.760]   This is Twig.
[00:00:25.760 --> 00:00:37.040]   This is Twig. This week in Google, episode 511, recorded Wednesday, June 5th, 2019.
[00:00:37.040 --> 00:00:39.040]   Where do you put the naked people?
[00:00:39.040 --> 00:00:44.320]   This week in Google is brought to you by WordPress. Turn your dreams into reality
[00:00:44.320 --> 00:00:51.280]   and launch your website at WordPress.com. Get 15% off any new plan at WordPress.com/Twig.
[00:00:51.920 --> 00:00:58.400]   And by LastPass, the number one most preferred password manager. Just remember your master password
[00:00:58.400 --> 00:01:03.600]   and LastPass remembers the rest. Visit lastpass.com/twit to learn more.
[00:01:03.600 --> 00:01:09.520]   It's time for Twig this week in Google. The show we cover the Google verse, Facebook, Google,
[00:01:09.520 --> 00:01:15.680]   house judiciary committee hearings with us Stacey Higginbotham from our new home
[00:01:15.680 --> 00:01:20.160]   in the great Pacific Northwest. Finally, you're in our time zone. I stay.
[00:01:20.960 --> 00:01:25.360]   Welcome back. I don't have to worry about you getting the urge to go get queso or anything.
[00:01:25.360 --> 00:01:30.160]   You really never know Leo. I think you're going to be hungry at this time zone too.
[00:01:30.160 --> 00:01:32.800]   It's what has the move going.
[00:01:32.800 --> 00:01:38.880]   It's a little awkward. Our stuff has not arrived and it will not arrive for another week and a half.
[00:01:38.880 --> 00:01:41.600]   Oish. So that was a little bit of mis-flub.
[00:01:41.600 --> 00:01:45.280]   Oh well. How did you get there? Did you drive your car across her?
[00:01:45.280 --> 00:01:49.760]   No, we flew. She was going to drive, but that I guess to change a plan.
[00:01:49.760 --> 00:01:51.760]   My dog is crazy.
[00:01:51.760 --> 00:01:55.920]   Oh, that's right. That's right. That's Jeff Jarvis, Professor,
[00:01:55.920 --> 00:02:00.080]   Leonard Towne Professor as it might be for journalistic innovation at the Greg No-Mart
[00:02:00.080 --> 00:02:03.680]   Graduate School of Journalism at the City University of New York. Hello, Mr. Jarvis.
[00:02:03.680 --> 00:02:09.200]   Well, come on. So you know my sport is to try to get you to buy things.
[00:02:09.200 --> 00:02:12.320]   You want me to buy something? I've got it. I've got it for you. Fortunately,
[00:02:12.320 --> 00:02:16.400]   I kept back what you have, but Boston Dynamics has its first commercial robot.
[00:02:16.400 --> 00:02:19.920]   It's a spot. This is the dog.
[00:02:19.920 --> 00:02:24.240]   And how much is it? I'm not finding a price.
[00:02:24.240 --> 00:02:29.680]   Yeah, that means if you have to ask your kids, is it more or less than a new Mac Pro
[00:02:29.680 --> 00:02:30.480]   monitor stand?
[00:02:30.480 --> 00:02:38.400]   We see the problem with Boston Dynamics is they're a bunch of engineers. They don't get
[00:02:38.400 --> 00:02:43.840]   making things cute and fuzzy. Oh my God. The fact that that's running around the
[00:02:43.840 --> 00:02:47.520]   conference floor at Remar's like if that came up to me, I probably would scream.
[00:02:47.520 --> 00:02:51.120]   Because they go. They don't know.
[00:02:51.120 --> 00:02:56.400]   They're terrifying. They call it gives a video. You want to see it? Yeah.
[00:02:56.400 --> 00:02:58.480]   This is spot mini.
[00:02:58.480 --> 00:03:03.040]   It's the one that opens doors. Yeah.
[00:03:03.040 --> 00:03:11.120]   It's that tip. Tap. You want that as well. Oh my God. I can't even watch it.
[00:03:11.120 --> 00:03:14.800]   I'll make you deal, Jeff. If I can get it for less than 1500 bucks, I'll buy it.
[00:03:14.800 --> 00:03:17.360]   Oh, look at the dog is terrified. Yes.
[00:03:17.360 --> 00:03:18.960]   Smart dog. Wait a minute.
[00:03:18.960 --> 00:03:21.680]   It's this thing. I'll take the toy.
[00:03:21.680 --> 00:03:25.600]   That's oh my God. Is that creepy?
[00:03:25.600 --> 00:03:28.080]   It's so creepy. It's technology.
[00:03:28.080 --> 00:03:32.720]   Nope. I showed it to my daughter and she unreservedly was like, oh, mom, that's awful.
[00:03:32.720 --> 00:03:35.440]   It's awesome.
[00:03:35.440 --> 00:03:38.400]   Something missing in you, Jeff.
[00:03:38.400 --> 00:03:41.520]   I don't know. It's going to be the last thing you see before you die.
[00:03:41.520 --> 00:03:45.680]   Yeah. As it eats you. Oh, this is so awesome. So cool.
[00:03:45.680 --> 00:03:48.880]   And it's the one that opens doors. So it'll come for you.
[00:03:48.880 --> 00:03:53.040]   It's a lot quieter than the old ones. The steam power winds are really noisy.
[00:03:53.040 --> 00:03:55.760]   But I think these are electric, which means probably they don't go as long.
[00:03:55.760 --> 00:04:00.800]   I'm on the board of a foundation that's in building in Morristown, New Jersey,
[00:04:00.800 --> 00:04:05.280]   the same place as the place where they hand over puppies for seeing eye to get trained.
[00:04:05.280 --> 00:04:10.240]   Yeah. And I was thinking about what if you had that as a seeing eye dog?
[00:04:10.240 --> 00:04:17.600]   You know, one of the things companion animals do though is they sense mood.
[00:04:17.600 --> 00:04:20.880]   They understand there's heart attack dogs.
[00:04:20.880 --> 00:04:25.600]   I mean, I don't know if these dogs imagine the data capability of that to know.
[00:04:25.600 --> 00:04:32.000]   It take me to Starbucks. I guess you could really have a much smaller guide dog that you just would
[00:04:32.000 --> 00:04:41.360]   you know, use as a company. Here's another video of the Boston Dynamics robot dog.
[00:04:41.360 --> 00:04:45.200]   Here's the robot dog army.
[00:04:45.200 --> 00:04:54.720]   Oh, yeah. But see, if they'd had this at Chernobyl, thousands of people would have survived.
[00:04:54.720 --> 00:04:56.800]   Why do the rest of them have no heads?
[00:04:56.800 --> 00:04:59.760]   They have heads. Oh, they're pulling a truck.
[00:05:00.640 --> 00:05:02.880]   Oh, wow.
[00:05:02.880 --> 00:05:08.160]   They're pulling a semi.
[00:05:08.160 --> 00:05:13.440]   The Boston Dynamics truck doesn't have to have an engine apparently.
[00:05:13.440 --> 00:05:16.160]   It's like Santa and his robotic reindeer.
[00:05:16.160 --> 00:05:21.120]   They call it spot power. It takes 10 spot power to haul a truck.
[00:05:21.120 --> 00:05:26.000]   Do you realize that that's do you realize how capable those are?
[00:05:26.000 --> 00:05:28.480]   Oh, no. Here's another one. This is even scarier.
[00:05:29.840 --> 00:05:32.960]   Wait a minute. Wait a minute. Put the sound on again for this one.
[00:05:32.960 --> 00:05:35.040]   This one's even scarier.
[00:05:35.040 --> 00:05:40.320]   Oh, you got you got to show it to them.
[00:05:40.320 --> 00:05:47.680]   So this is the dogs waking up to an all day. Oh, God.
[00:05:47.680 --> 00:05:54.320]   If you could see this, folks, if those of you listening at home,
[00:05:54.320 --> 00:06:00.480]   it's just imagine 10 robotic yellow robotic dogs. So I wonder how
[00:06:00.480 --> 00:06:02.560]   might they're going to be $10,000 right? They're not going to.
[00:06:02.560 --> 00:06:08.720]   So this story, Gizmodo says that the industrial robot Baxter cost 22,000.
[00:06:08.720 --> 00:06:14.400]   Sony Ibo cost 2000. They get to be closer to Baxter than it is to Sony.
[00:06:14.400 --> 00:06:17.120]   Really? Yeah. Baxter was pretty fancy though.
[00:06:17.120 --> 00:06:20.000]   Yeah. Ibo was actually fancy in other ways.
[00:06:20.000 --> 00:06:23.680]   Couldn't pull a truck, but it was cute and it played games.
[00:06:23.680 --> 00:06:26.560]   See now, what makes see that's the thing here, right?
[00:06:26.560 --> 00:06:29.200]   It's the same basic technology. It's the same basic thing.
[00:06:29.200 --> 00:06:31.200]   One is adorable. One is creepy. Come on.
[00:06:31.200 --> 00:06:37.760]   Okay. Once Ibo is this big. Ibo also does things like this.
[00:06:37.760 --> 00:06:40.560]   Cox's head at you in looks all cute. Yes, they make it cute.
[00:06:40.560 --> 00:06:45.840]   Big dog. When it cocks its head, you're thinking it's going to shoot lasers out of its eyes,
[00:06:45.840 --> 00:06:50.160]   because it is scary, but Ibo could shoot lasers out of its eyes.
[00:06:50.160 --> 00:06:54.560]   Honestly, if you're going to make a killer dog, what are you going to make it look like?
[00:06:54.560 --> 00:07:01.360]   Like a cute. Yeah. Ibo also cannot open doors nor can it climb stairs.
[00:07:01.360 --> 00:07:05.840]   So if you're going to run away from Big Dog, it's going to catch you. Also, Big Dog is fast.
[00:07:05.840 --> 00:07:09.600]   Very fast, like 60 miles an hour fast. What really?
[00:07:09.600 --> 00:07:14.480]   It can run. It's tracking. Google how fast can Big Dog run? It's really it's fast.
[00:07:14.480 --> 00:07:19.760]   This is the by the way, Ibo is back. Sony is making it again. This is the new Ibo.
[00:07:20.240 --> 00:07:23.360]   I would just had a curiosity. This we should buy. How much?
[00:07:23.360 --> 00:07:27.040]   Yeah, it's going to meet because I want $3,000. Whoa.
[00:07:27.040 --> 00:07:32.160]   $28.99. Whoa. Yeah. Plus, you're going to buy its toys.
[00:07:32.160 --> 00:07:34.400]   Yeah, apparently you also have to buy paws.
[00:07:34.400 --> 00:07:43.120]   Oh, a paw pad. If you want paw pads, it'll be an extra 10 bucks.
[00:07:43.120 --> 00:07:46.720]   You figure around and you're like, buy the paws. Buy the paws.
[00:07:48.240 --> 00:07:52.080]   Don't buy the paws without the dog. That would be my suggestion. You want to see Ibo at work?
[00:07:52.080 --> 00:07:55.840]   Here we go. Here's a little Ibo and Ibo takes Manhattan.
[00:07:55.840 --> 00:08:01.840]   This is from Brooklyn, New York. The Bronx. You know what I'm saying? I live on the Upper East Side.
[00:08:01.840 --> 00:08:04.720]   Maybe surprise, but I'm never shocked. Let's try and do this.
[00:08:04.720 --> 00:08:09.520]   Those glamorous ways possible. I love New Yorkers. I love New Yorkers.
[00:08:09.520 --> 00:08:15.760]   See, this is what Stacy was talking about though. I feel kind of skeptical.
[00:08:16.720 --> 00:08:24.160]   I like the worrying sounds. It does bark well.
[00:08:24.160 --> 00:08:28.240]   Just a little scared. Time to meet Ibo in this.
[00:08:28.240 --> 00:08:36.480]   Oh my god. Oh my god. You're a star in the making.
[00:08:36.480 --> 00:08:38.800]   Okay, this I've never seen before.
[00:08:44.240 --> 00:08:47.040]   I'm a great guy.
[00:08:47.040 --> 00:08:49.520]   Okay, that's a great ad.
[00:08:49.520 --> 00:08:56.720]   He's really cute. I have to say he's really cute, but $3,000 is a little bit out of my price range.
[00:08:56.720 --> 00:08:59.120]   For that, I could half a Mac Pro.
[00:08:59.120 --> 00:09:06.480]   Oh, true. Yeah. If Boston Dynamics had that marketing, but I'm serious, if you
[00:09:06.480 --> 00:09:11.360]   open your eyes and again, that was in front of you, you would scream, not in a good way in the like,
[00:09:12.960 --> 00:09:15.680]   so we were in the middle of Twitter.
[00:09:15.680 --> 00:09:18.000]   When actually, what?
[00:09:18.000 --> 00:09:20.560]   Do are we in Twitter now? We're in Twig.
[00:09:20.560 --> 00:09:23.040]   Did we start? Yes. I introduced you in everything.
[00:09:23.040 --> 00:09:26.400]   Remember I asked you how your trip was, how Seattle was.
[00:09:26.400 --> 00:09:28.240]   Remember all that? Yeah, a little bit.
[00:09:28.240 --> 00:09:33.840]   So on Sunday in the middle of Twitter, actually before Twitter, but during most of Twig, Google went down.
[00:09:33.840 --> 00:09:36.880]   Oh, yes. Oh, we're in the show. Oh, cool.
[00:09:36.880 --> 00:09:42.800]   Yeah, it didn't disrupt our internal operations, even though we use Google Drive and stuff.
[00:09:42.800 --> 00:09:45.600]   Because we're out here in California and it apparently didn't affect us.
[00:09:45.600 --> 00:09:47.520]   Well, it was my abse-cal. It was fun.
[00:09:47.520 --> 00:09:49.280]   And you were fine. It was fine.
[00:09:49.280 --> 00:09:53.600]   I was fine, but I am sad because I didn't have any of my Google stuff in.
[00:09:53.600 --> 00:09:55.360]   Oh, I couldn't actually check the weather.
[00:09:55.360 --> 00:09:58.240]   I checked. I tried to check the weather on my phone and it was not working.
[00:09:58.240 --> 00:10:07.680]   Yeah. So Google, it seemed it was mostly in the Northeast, but then Seattle did seem to have
[00:10:07.680 --> 00:10:13.440]   some hotspots. Google's explanation yesterday, disruption,
[00:10:13.440 --> 00:10:17.600]   yesterday, Sunday, a disruption in Google's network in parts of the United States caused
[00:10:17.600 --> 00:10:22.080]   slow performance, elevated error rates on several Google services, including Google Cloud,
[00:10:22.080 --> 00:10:26.000]   YouTube, Gmail, Google Drive and others. It impacted people trying to watch us on YouTube,
[00:10:26.000 --> 00:10:31.840]   but it couldn't because the disruption reduced region. This is the thing that seemed kind of not
[00:10:31.840 --> 00:10:37.360]   completely accurate because the disruption reduced regional network capacity.
[00:10:38.320 --> 00:10:44.000]   Yeah, by a lot, like to zero in some cases, the worldwide user impact varied widely.
[00:10:44.000 --> 00:10:47.920]   For most users, there was little or no change to their services.
[00:10:47.920 --> 00:10:55.840]   So the root cause, they say, was a configuration change intended for a small number of servers
[00:10:55.840 --> 00:11:01.760]   in a single region, but there was an error that applied that configuration to a large number of
[00:11:01.760 --> 00:11:06.800]   servers across several neighboring regions and caused those regions to stop using more than
[00:11:06.800 --> 00:11:10.800]   half of their available network capacity. After talking to some network experts,
[00:11:10.800 --> 00:11:19.680]   I found out that all services are not created equal on Google. Some services are
[00:11:19.680 --> 00:11:23.680]   higher priority. There's quality of service, just as there might be in your home. If you want to
[00:11:23.680 --> 00:11:30.000]   watch TV versus want to read email or webpage. And that is a true of all the Google services.
[00:11:30.720 --> 00:11:38.160]   And the problem is this configuration, which was intended to make servers use less bandwidth,
[00:11:38.160 --> 00:11:44.800]   was then extended to too many servers. And servers that were actually mission critical
[00:11:44.800 --> 00:11:49.600]   were offering important services would normally be highly prioritized, were down prioritized.
[00:11:49.600 --> 00:11:54.400]   The network traffic they continued from and to those regions then tried to fit into the
[00:11:54.400 --> 00:12:00.480]   remaining capacity, but it did not. So it's kind of like Chernobyl.
[00:12:00.480 --> 00:12:08.320]   What? It melted down. No, because you would think that if some servers then went down to half
[00:12:08.320 --> 00:12:13.200]   capacity, it'd leave more room for other services. But because those services were trying to jam
[00:12:13.200 --> 00:12:17.600]   their way into the constrained services, everything stopped working.
[00:12:19.520 --> 00:12:26.720]   Yes. And what this actually, I didn't really, this makes me wonder how close to capacity Google
[00:12:26.720 --> 00:12:32.480]   runs at all times. Yeah. What I was thinking, I was like, Oh, I always felt like they had more
[00:12:32.480 --> 00:12:38.480]   overhead. But maybe they don't. They kind of just try to like turn off things until it's needed.
[00:12:38.480 --> 00:12:42.720]   Yeah, think about it. You don't overhead is cost. You don't want too much headroom.
[00:12:42.720 --> 00:12:47.760]   Because if you have, and this was the problem, I mean, I think about other network situations
[00:12:47.760 --> 00:12:52.720]   like on live, remember the streaming gaming service, the predates Google Stadia on live,
[00:12:52.720 --> 00:12:59.920]   their problem was that at peak times, it required far more servers than they had. And most of the
[00:12:59.920 --> 00:13:04.400]   time they could handle it, but they couldn't afford to have all the servers available that
[00:13:04.400 --> 00:13:09.920]   they needed for peak hours. So Google, I'm sure, and Facebook and other massive operations
[00:13:09.920 --> 00:13:14.000]   constantly adjust. I'm sure the bandwidth they consume and the servers they have up
[00:13:14.000 --> 00:13:21.280]   and running in any event. They said the networking systems correctly triaged the traffic overload,
[00:13:21.280 --> 00:13:27.120]   dropped larger, less latency sensitive traffic in order to preserve smaller latency sensitive
[00:13:27.120 --> 00:13:35.200]   traffic flows. They, I think poorly, analogize that to urgent packages being curried by bicycle
[00:13:35.200 --> 00:13:40.640]   through the worst traffic jams. I mean, it gives an accurate picture like,
[00:13:40.640 --> 00:13:43.840]   because it is true. Like, you're like, hey, why does this work when this doesn't?
[00:13:43.840 --> 00:13:48.960]   Yeah, yeah, it wasn't like Google was down entirely, but it was just some was some wasn't.
[00:13:48.960 --> 00:13:52.560]   Google's engineering teams detected the issue within seconds, but diagnosis and correction
[00:13:52.560 --> 00:13:58.560]   too far longer than our target of a few minutes. In other words, they knew what was going on.
[00:13:58.560 --> 00:14:05.360]   But this is why it's like Chernobyl. The same network congestion that was creating service
[00:14:05.360 --> 00:14:11.200]   degradation also made impossible for them to correct the outage. It's like the same traffic
[00:14:11.200 --> 00:14:16.800]   jam. You can get in. Folks can't get there. You can get in. The Google teams, this is the
[00:14:16.800 --> 00:14:21.680]   drama. And I want to see this as a mini series in HBO. The Google teams were keenly aware that
[00:14:21.680 --> 00:14:27.040]   every minute which passed represented another minute of user impact and brought on additional
[00:14:27.040 --> 00:14:35.280]   help to parallelize restoration efforts. In effect, parallelize. You like that? It was parallelized.
[00:14:36.240 --> 00:14:41.840]   And which we were all paralyzed as a result. They hit AZ-5 and nothing happened.
[00:14:41.840 --> 00:14:50.000]   I'm sorry. I just finished Chernobyl. Obviously, I'm making everything a little bit like Chernobyl.
[00:14:50.000 --> 00:14:57.440]   Well, a network congestion, depending. I mean, the way you do routing anyway,
[00:14:57.440 --> 00:15:02.560]   once you have congestion, your packets are going to keep sending until they're delivered,
[00:15:02.560 --> 00:15:06.000]   which means congestion begets more congestion, which begets more congestion.
[00:15:06.000 --> 00:15:10.320]   Right. Exactly. It really just shows you how reliant we are on Google and how
[00:15:10.320 --> 00:15:19.280]   responsive Twitter is because they say overall YouTube measured a 2.5% drop of views for one hour,
[00:15:19.280 --> 00:15:25.600]   like nothing. Google Cloud, though, storage represent a 30% reduction in traffic.
[00:15:25.600 --> 00:15:31.600]   1% of active Gmail users had problems. But it sounded like all of Gmail was down if you
[00:15:31.600 --> 00:15:35.840]   followed it on Twitter or a chatroom. It sounded like, "I can't. My Gmail's not working."
[00:15:35.840 --> 00:15:39.760]   Of course, if you're even on a Sunday, if you're a business that runs on Gmail, that's not good.
[00:15:39.760 --> 00:15:46.960]   So they're doing a post-mortem, and I'm sure that they'll have a show trial and some engineers
[00:15:46.960 --> 00:15:54.160]   will be shot. Yeah. No. That is not how you do this because otherwise people will become like
[00:15:54.160 --> 00:15:59.600]   Chernobyl and hide their mistakes. In fact, they didn't. That's what they said, the leg is off.
[00:15:59.600 --> 00:16:05.280]   We'd kill you, but it wouldn't solve anything. Everybody'd notice.
[00:16:05.280 --> 00:16:13.680]   I would say that anytime I know that Google lost a lot of market value and everyone suddenly was
[00:16:13.680 --> 00:16:19.360]   like, "Oh my God, we're highly dependent on Google and Amazon and Microsoft for a few companies,
[00:16:19.360 --> 00:16:25.200]   for our big services." But I would also say having these kind of post-mortems, even if this
[00:16:25.200 --> 00:16:32.400]   isn't as detailed as maybe we want. That's pretty good. It's good. I guarantee that if you are,
[00:16:32.400 --> 00:16:38.640]   well, not Netflix, but if you're a Snapchat or somebody who's actually a big Google customer,
[00:16:38.640 --> 00:16:44.880]   you're getting way more detailed than this. Yeah. I imagine somebody who can understand it could
[00:16:44.880 --> 00:16:53.760]   find out more. What was the configuration error? Was it a BGP issue? What exactly was going on?
[00:16:55.040 --> 00:16:59.040]   I think they gave enough details for people to read between the lines. I read a bunch of
[00:16:59.040 --> 00:17:03.440]   comments on Hacker News, talk to people, and engineers who understand these things seem to
[00:17:03.440 --> 00:17:10.400]   understand what was going on. There you go. Well done. Mission accomplished. The best part,
[00:17:10.400 --> 00:17:14.960]   though, was the Google engineers working in the nude all night to get it working.
[00:17:14.960 --> 00:17:18.560]   Wait, that was not in there. That didn't happen.
[00:17:19.520 --> 00:17:25.920]   Wait a second. I'm sorry. Chernobyl. Chernobyl.
[00:17:25.920 --> 00:17:28.560]   Things I don't want to picture.
[00:17:28.560 --> 00:17:36.240]   Or panic worse. Yeah. Well, yeah. I don't know. It certainly made me think maybe we
[00:17:36.240 --> 00:17:41.840]   don't want more of a nuclear power. Facebook is not nuclear power. Just remember that.
[00:17:42.640 --> 00:17:49.680]   Well, speaking of moral panic, here comes the tech lash because Congress now says we're going to
[00:17:49.680 --> 00:17:57.280]   investigate Apple, Facebook, Google, and Amazon. The FTC and the department of justice apparently
[00:17:57.280 --> 00:18:04.560]   divided their attacks. The FTC will be going after Facebook Department of Justice going after Google
[00:18:04.560 --> 00:18:09.680]   investigations. But I think the tech lashes is upon us.
[00:18:09.680 --> 00:18:15.360]   Well, the problem now is it becomes impossible to separate out concerns versus politics.
[00:18:15.360 --> 00:18:21.120]   There's the right and the left are both going after the platforms.
[00:18:21.120 --> 00:18:27.760]   Given that, I don't want to get to political here. Get political.
[00:18:27.760 --> 00:18:30.480]   Given that. Because it is political. It's a cheap executive organization just this week
[00:18:30.480 --> 00:18:37.360]   tried to go after AT&T just because he doesn't like CNN. What's the motive? You don't know.
[00:18:39.600 --> 00:18:42.720]   People screaming for blood care swisher in the New York Times yesterday.
[00:18:42.720 --> 00:18:47.680]   The people screaming for blood have no idea how tech actually works. Suddenly,
[00:18:47.680 --> 00:18:52.400]   regulators guns are blazing, but it looks thoughtless and is likely to prove pointless.
[00:18:52.400 --> 00:18:55.680]   She says she's always thought there should be some guardrails.
[00:18:55.680 --> 00:19:02.960]   I respect Kara immensely, but I think that she lately has been part of the cause of the tech
[00:19:02.960 --> 00:19:10.480]   lash. New York Times has that's for sure. I think she has been pointing out legitimate issues and
[00:19:10.480 --> 00:19:17.520]   has been for a long time that have led to this. The problem that she's writing about is that
[00:19:17.520 --> 00:19:24.160]   what's going to happen here is now that everybody's on board. She's concerned that they're going to
[00:19:24.160 --> 00:19:30.320]   again let politics control this and they're not going to be sensible. They're going to have
[00:19:30.320 --> 00:19:34.960]   these knee-jerk reactions, which is how we pass laws in this country for the last 20 to 30 years.
[00:19:34.960 --> 00:19:39.280]   I would agree with her, but I think she as all of us in the tech press, we're all a little bit
[00:19:39.280 --> 00:19:49.440]   guilty of cheerleading as we march down the path to tech giants. Maybe we could have averted this
[00:19:49.440 --> 00:19:54.240]   by urging them to be more judicious from day one. Maybe not.
[00:19:54.240 --> 00:20:03.360]   But I don't think we have. I think most of the responsible tech journalists, I am very excited
[00:20:03.360 --> 00:20:09.760]   about a lot of things like IoT can do, but I have never been unaware of the privacy and surveillance
[00:20:09.760 --> 00:20:16.160]   areas and the ways it can be used for ill. I talk about those and I think most responsible
[00:20:16.160 --> 00:20:20.880]   tech journalists have. Most of us have been calling for regulation. I've been calling for
[00:20:20.880 --> 00:20:27.520]   regulations forever and people hate me for it. I apologize. I said the DOJ and FTC split the
[00:20:27.520 --> 00:20:33.760]   split the baby, but it was FTC did not take Facebook. They took Amazon. Sorry, it's Congress
[00:20:33.760 --> 00:20:39.200]   is going to take Facebook. Google goes to the Department of Justice. FTC takes on Amazon.
[00:20:39.200 --> 00:20:46.080]   Although when you hear that there is an FTC task force to quote monitor competition
[00:20:46.080 --> 00:20:52.320]   in the US technology market, you feel like, OK, task force, that's one way of saying,
[00:20:52.320 --> 00:20:54.880]   we're not going to do anything, but we're going to look like we're doing something.
[00:20:54.880 --> 00:20:56.480]   Hate task force.
[00:20:56.480 --> 00:21:00.880]   We're establishing a committee.
[00:21:00.880 --> 00:21:07.040]   Last week I talked about the efforts to come up with some sane regulation and part of this
[00:21:07.040 --> 00:21:09.120]   working group on-- Oh, wait a minute.
[00:21:09.120 --> 00:21:18.480]   Regulation. My Lenovo alarm says it's time to get up. Hey, Google snooze.
[00:21:18.480 --> 00:21:21.920]   All right, I'll give you 10 more minutes.
[00:21:21.920 --> 00:21:27.920]   10 more minutes. All right, Jeff, keep talking and then it's harder to say that than it is to just
[00:21:27.920 --> 00:21:34.000]   hit something. Can I just say stop now? Isn't that one of the things you can do or is that,
[00:21:34.000 --> 00:21:38.320]   yeah, they said I could say stop. I don't think it's is it rolled out yet? I try to do it the other day.
[00:21:38.320 --> 00:21:43.520]   I try it. You really should attest it should be like this.
[00:21:43.520 --> 00:21:50.320]   This is-- OK, yes, and Google.
[00:21:50.320 --> 00:21:59.840]   This is the new $90 Lenovo small little Lenovo desk or a table side clock for your bedroom.
[00:21:59.840 --> 00:22:04.640]   That's mostly a clock, although it does play music. Actually, as a surprisingly good speaker,
[00:22:04.640 --> 00:22:12.160]   there's my calendar. It's very small. Does it display anything else like photos or--
[00:22:12.160 --> 00:22:17.280]   No, and see, it's about the same price as the Home Hub, maybe 10 bucks less if you shop around.
[00:22:17.280 --> 00:22:21.280]   And I think the Home Hub with a bigger screen and doing things like that is probably a better
[00:22:21.280 --> 00:22:25.040]   choice for most people. But if you just want a little thing that's a clock, it's about the size
[00:22:25.040 --> 00:22:28.720]   of a little alarm clock. There's no camera in that, right? There's no camera, but there isn't
[00:22:28.720 --> 00:22:37.040]   in the Home Hub either. It shows you the weather and what time it is. And it's telling me I got
[00:22:37.040 --> 00:22:40.720]   eight minutes, 40 seconds left until the alarm goes back off.
[00:22:40.720 --> 00:22:44.560]   OK, let's keep talking about politics because it's a wonderful little break.
[00:22:44.560 --> 00:22:50.800]   It should be 30 bucks. The speaker's decent, though. I'll play some music at some point.
[00:22:50.800 --> 00:22:56.240]   The speaker's decent, but I feel like it should be less expensive. Go ahead, Jeff. Sorry.
[00:22:56.240 --> 00:23:01.680]   Oh, I was just saying that on this topic, last week we talked about-- I gave a report from the
[00:23:01.680 --> 00:23:09.920]   Facebook oversight board working group. And I talked about the book, "Tortices, Words,
[00:23:09.920 --> 00:23:13.760]   The Creative Internet," so anyway, so I have a long five-year read post on Medium,
[00:23:13.760 --> 00:23:20.000]   recounting all of that with links and everything on Medium. So just we won't bore you again, audience.
[00:23:20.000 --> 00:23:31.840]   Yeah, I mean, I was likening this during Windows Weekly to a watchmaker with a very elaborate watch
[00:23:31.840 --> 00:23:39.440]   trying to fix it with a hammer. These are complex mechanisms, complex, and they're very integrated
[00:23:39.440 --> 00:23:44.880]   into society and our economy now. And I don't think Congress can come up with the right Congress.
[00:23:44.880 --> 00:23:49.120]   No, Congress is not. But even the French came up with what I think is a
[00:23:49.760 --> 00:23:55.280]   no offense, French. But it is a sensible regime, which again, we talked about last week,
[00:23:55.280 --> 00:24:01.280]   of holding, getting platforms to make covenants with their public what they're going to do,
[00:24:01.280 --> 00:24:04.320]   and then holding them accountable for doing that. And I'm cool with that.
[00:24:04.320 --> 00:24:06.640]   Don't we kind of already do that with privacy statements?
[00:24:06.640 --> 00:24:10.320]   Isn't there enforcement? No, no, because no one reads them, not even Congress. But
[00:24:10.320 --> 00:24:16.240]   so I would say, let's-- okay, I know that we're actually possibly thinking about maybe doing
[00:24:16.240 --> 00:24:23.200]   something now here. But all the way back in 2014, and maybe even 2013, the FTC actually put out
[00:24:23.200 --> 00:24:31.120]   some really good research on data privacy and how to look at that and think about that. So if
[00:24:31.120 --> 00:24:37.760]   it looks in it, did a task force and all this. So if it takes that kind of information from a few
[00:24:37.760 --> 00:24:43.120]   years back and tries to implement that, that's not a bad idea. And that's actually not knee-jerk.
[00:24:43.120 --> 00:24:47.280]   Maybe it's like, oh, now the time is right to actually do something. If it pulls some of the
[00:24:47.280 --> 00:24:54.560]   stuff that it has available to it, that could be good. If we start from scratch, though, I'm kind
[00:24:54.560 --> 00:24:59.040]   of like, we'll never get this done, and it'll be stupid. I'm terrified, though. I'm really
[00:24:59.040 --> 00:25:03.280]   terrified that-- Oh, I am, too. These will be like net blood throwing, you know,
[00:25:03.280 --> 00:25:10.880]   hammers into the wounds. It'll be-- No, that's just-- no, that is what they want you to think.
[00:25:10.880 --> 00:25:16.400]   There are reasonable outcome-based decisions and rules that we can put into place.
[00:25:16.400 --> 00:25:23.600]   And I would encourage you, actually, to go back to the 2014 FTC report around data privacy,
[00:25:23.600 --> 00:25:28.400]   because they actually foresaw a lot of these issues, and they talked about ways to make it less
[00:25:28.400 --> 00:25:34.880]   onerous on the companies, but still more transparent and protective of users.
[00:25:34.880 --> 00:25:36.880]   Right. I just got another new book.
[00:25:38.320 --> 00:25:41.520]   How do you tell-- Time to read this stuff. Speech police.
[00:25:41.520 --> 00:25:44.080]   Train rides. Speech police in the Louisville struggle with government,
[00:25:44.080 --> 00:25:46.720]   the Internet of David K. David K is spectacular. He is the
[00:25:46.720 --> 00:25:56.400]   special-- U.N. special rapporteur for the protection of the right to freedom of opinion and expression.
[00:25:56.400 --> 00:26:03.680]   He's a law professor at UC Irvine. I've been on task forces lately with him. I'm going to read
[00:26:03.680 --> 00:26:08.080]   just tomorrow in the plane to Athens. Oh, man. It's a good thing you travel a lot.
[00:26:08.080 --> 00:26:15.440]   That's also how I get this stuff right. Yeah. So, GCHQ, the British spy agency,
[00:26:15.440 --> 00:26:21.040]   has proposed a kind of crazy-- and, by the way, not-- it's not a law. They're just saying,
[00:26:21.040 --> 00:26:24.960]   maybe we could do this. A crazy idea. They're using it without laws.
[00:26:24.960 --> 00:26:32.560]   Yeah, that's true. They don't really need laws. To create a ghost participant in every encrypted
[00:26:32.560 --> 00:26:37.280]   message conversation that can kind of snoop on it. In other words, we don't have to break the
[00:26:37.280 --> 00:26:42.160]   encryption. We just have to allow-- have you allow us to join every encrypted conversation?
[00:26:42.160 --> 00:26:46.960]   Google WhatsApp, Apple, and 47 other companies.
[00:26:46.960 --> 00:26:54.560]   Senné. Strongly worded open letter to-- Instead of having the key to the locked room where you're
[00:26:54.560 --> 00:27:00.320]   meeting, let's just put someone in there. Yeah. Yeah. Yeah. So, please, I'm going to be sparked people.
[00:27:01.200 --> 00:27:05.920]   So, I saw a story last week. I didn't put on the rundown. I can't remember where I saw it. But,
[00:27:05.920 --> 00:27:14.400]   you know, fear is that AI will mean that encryption is-- dies because it is possible to, I guess,
[00:27:14.400 --> 00:27:19.360]   brute force go into encryption. So, what is the answer here? No, no, no, no. That's not true.
[00:27:19.360 --> 00:27:22.720]   No, no, no. Quantum computing is the fear of sharing the show.
[00:27:22.720 --> 00:27:25.120]   Also, something that doesn't exist. Okay.
[00:27:25.120 --> 00:27:32.720]   Right. Well, in there's-- Okay. So, encryption, like, it's basically a mathematical cipher.
[00:27:32.720 --> 00:27:38.400]   And you have different versions of encryption. I think currently the strongest is still AES256.
[00:27:38.400 --> 00:27:43.920]   Is that correct? Yeah. It's the gold standard. There is stronger. There's elliptical curve,
[00:27:43.920 --> 00:27:47.680]   crypto. There's stronger stuff. Okay, yeah. Basically, it's a hard math problem. But as
[00:27:47.680 --> 00:27:52.480]   computers get faster and they'll have to get exponentially faster, the fear is they'll get
[00:27:52.480 --> 00:27:57.280]   fast enough to crack these in a reasonable time frame. Right now, with ECC elliptical
[00:27:57.280 --> 00:28:04.320]   encryption, elliptical curve encryption, it's millions of years, or trillions of years, or the
[00:28:04.320 --> 00:28:09.520]   lifetime of the universe, depending on how fast your computer is to crack these. But if quantum
[00:28:09.520 --> 00:28:16.880]   computing ever gets working, which it's not even close to doing, I might point out, in theory,
[00:28:16.880 --> 00:28:20.080]   it could be fast enough. But then you just come up with better, harder math problems.
[00:28:20.080 --> 00:28:26.400]   So, I actually talked to some companies who are building chips that are quantum key resistant,
[00:28:26.400 --> 00:28:32.400]   or quantum resistant. Really? Yeah, and they're interesting because they're doing random--
[00:28:32.400 --> 00:28:37.600]   And these are DARPA-funded chips. But what they're doing is they're doing the way you make a
[00:28:37.600 --> 00:28:43.520]   semiconductor is you basically layer on different materials and then etch away at them in certain
[00:28:43.520 --> 00:28:50.960]   patterns. That's very basic. What they're doing is they're doing randomized patterns being etched
[00:28:50.960 --> 00:28:57.840]   and you'll match that to a key. And so, the idea is that you can't break that even using
[00:28:57.840 --> 00:29:04.400]   quantum computing because it's just so random. It's so chaotic, yeah. Exactly. So, that's, again,
[00:29:04.400 --> 00:29:10.720]   this is research level stuff, this is not in the real world. But it's pretty cool. So, the point is,
[00:29:10.720 --> 00:29:14.160]   you don't have to worry about quantum encryption yet because we don't have quantum computers,
[00:29:14.160 --> 00:29:17.440]   but there are really smart people already worried about it. So, when that time comes--
[00:29:17.440 --> 00:29:23.280]   It's hypothetical. I wouldn't worry about it. Yeah. So, last week we talked to about quantum
[00:29:23.280 --> 00:29:29.280]   random number generator. Yes. Is that useful in better encryption? No. That's a different thing.
[00:29:29.280 --> 00:29:33.760]   So, I said, "Spart people are explaining these things to me."
[00:29:33.760 --> 00:29:38.000]   No, we're not laughing. I'm not laughing at you, Jeff. I'm laughing at it like,
[00:29:38.000 --> 00:29:42.000]   "Oh, I'll just put quantum in front of it." Well, it's a little bit like that.
[00:29:42.000 --> 00:29:44.800]   People are watching with cold fusion power. Exactly.
[00:29:44.800 --> 00:29:51.760]   I think that there are many people who think the notion of quantum computing is either
[00:29:51.760 --> 00:29:57.040]   maybe impossible, maybe cold fusion impossible, or maybe just so far off, we don't have to--
[00:29:57.040 --> 00:30:01.760]   We're just so in, I mean, so impractical. Yeah. Well, that's why it's far off. It's like,
[00:30:02.320 --> 00:30:09.040]   cold fusion is not impossible. It's just a prank. Does Microsoft have IBM? They all have quantum
[00:30:09.040 --> 00:30:19.040]   computers technically running. Do they? Yeah. Google, Google, Lockheed Martin, Google,
[00:30:19.040 --> 00:30:25.520]   quantum computing. I know IBM has that thing. Yeah. And IBM, they're Almeida Labs have some
[00:30:25.520 --> 00:30:33.840]   crazy stuff. I have been there and seen the crazies. They're fun. I can't figure out if this is a
[00:30:33.840 --> 00:30:43.520]   joke or not, but apparently Schrodinger's cat. A team of Yale researchers-- I thought it first,
[00:30:43.520 --> 00:30:48.080]   this is the onion. A team of Yale researchers blew the lid off one of quantum computing's biggest
[00:30:48.080 --> 00:30:53.920]   problems, the unpredictability of qubits. What's being dubbed a beautiful experiment,
[00:30:53.920 --> 00:31:00.000]   the team discovered how to catch an artificial atom, mid-quantum jump and interfere with its
[00:31:00.000 --> 00:31:06.800]   outcome, in other words, to save the life of Schrodinger's cat. And don't worry, it's--
[00:31:06.800 --> 00:31:16.400]   It's all theoretical. Honestly, I know IBM is using the word quantum computing.
[00:31:16.400 --> 00:31:22.880]   I do not think it means what they think of me. No, no, they have-- AT&T Gen 5.
[00:31:23.680 --> 00:31:31.920]   No, no, no, no, no. They have actually managed to both generate and I think it was eight qubits.
[00:31:31.920 --> 00:31:39.360]   And a qubit is the quantum version of a bit. It's multi-dimensional. Yeah.
[00:31:39.360 --> 00:31:41.280]   That was like-- It's not as hard on a one. It's not as hard on a one. That's how many qubits you need to
[00:31:41.280 --> 00:31:47.280]   make the arc. That's right. But-- Oh, there you go. Yeah. They did it-- I shouldn't be qubit.
[00:31:47.280 --> 00:31:53.200]   Eight and they had it-- No. Because the challenge is it has to be really cold and they're very--
[00:31:53.200 --> 00:31:59.520]   they collapse very quickly. So the challenge you're looking for in quantum development for
[00:31:59.520 --> 00:32:06.800]   computing is how long your qubits can actually stay around and how cold you need to keep it
[00:32:06.800 --> 00:32:10.640]   and then how much power you need to make the whole thing run. And right now, that's where
[00:32:12.080 --> 00:32:17.760]   more of the research is. Yeah. Did that make sense? Yes. We can stop talking about quantum-- In fact,
[00:32:17.760 --> 00:32:24.720]   that is why that whatever study out of Yale, the experiment out of Yale is so important.
[00:32:24.720 --> 00:32:33.680]   It's still-- You're not going to be using a quantum computer anytime soon. I don't think.
[00:32:33.680 --> 00:32:39.600]   When the robots are quantum power, you watch out people. Well, hold on.
[00:32:39.600 --> 00:32:44.080]   Because Google does have a quantum computer. Well, again, what does that mean?
[00:32:44.080 --> 00:32:51.760]   I'm going to tell you. They have-- let's see. They have built a circuit, according to--
[00:32:51.760 --> 00:32:54.800]   I'm-- y'all talk about something else. Do you really want to know or not? Yeah.
[00:32:54.800 --> 00:33:01.360]   Okay. She's reading. So I triple E spectrum in March, there's an article Google builds
[00:33:01.360 --> 00:33:05.600]   circuit to solve one of quantum computing's biggest problems. Yeah, to solve that, that's the
[00:33:05.600 --> 00:33:15.840]   qubit problem. Yes. It's a 72 qubit quantum processor. Check out the picture. This shows you how far
[00:33:15.840 --> 00:33:22.000]   off we are, actually. Yeah. That's what I'm saying. And by the way, this is 72 qubits.
[00:33:22.000 --> 00:33:27.840]   And I don't even know if it's stable. I mean, I think there's all sorts of issues. Well, they
[00:33:27.840 --> 00:33:32.800]   have 168 coax cables going into the refrigerator and connecting to the 10-- Yeah. Oh, by the way,
[00:33:32.800 --> 00:33:40.080]   it's a quantum processor. Wow. It's a cryogenic enclosure that is basically an absolute zero.
[00:33:40.080 --> 00:33:44.320]   Okay. I'm sure I'll have one of those in my phone soon.
[00:33:44.320 --> 00:33:50.320]   It's running at more than 200 degrees below what silicon foundry simulation models can do with.
[00:33:50.320 --> 00:33:58.400]   That's awesome. Yeah. I don't think this stuff is that stable. I just-- I don't know. Maybe I'm
[00:33:58.400 --> 00:34:03.520]   wrong. I'm sure I will get emails from somebody says, "Well, actually, I'm staring at a quantum
[00:34:03.520 --> 00:34:08.080]   computer right now." You can bring on the D-Wave guy. Have them come back and tell you what it's
[00:34:08.080 --> 00:34:14.800]   about. Yeah. Those guys are fun. I think there's a lot of boogosity in some of this stuff. But okay,
[00:34:14.800 --> 00:34:22.320]   whatever. Whatever. Let's take a break and then we're going to talk about YouTube because
[00:34:22.320 --> 00:34:28.560]   holy cow, what's going on at YouTube. This was a busy time, last couple of days for YouTube.
[00:34:28.560 --> 00:34:34.800]   It's possible that they were the ones who shut themselves down on Sunday and they were wishing
[00:34:34.800 --> 00:34:43.600]   they couldn't come back. Just don't just die. Can we get a break? Please. We need a break.
[00:34:43.600 --> 00:34:49.520]   Our show today brought to you by Wordpress. I'll tell you what, it is absolutely the case.
[00:34:50.480 --> 00:34:55.040]   The web is still here. It's still present. In fact, you ought to have a website. Even if you're a
[00:34:55.040 --> 00:35:04.720]   YouTube star or a Facebook maven or a Twitter towampus, you need a Wordpress site because
[00:35:04.720 --> 00:35:09.120]   having that website puts you on the web. It's the first thing people find when they search for
[00:35:09.120 --> 00:35:12.560]   your name. It's the place where you could put all the best stuff. And you know what? If you don't
[00:35:12.560 --> 00:35:18.000]   have a website, it leaves a vacuum. Others will fill the best place to create that website,
[00:35:18.000 --> 00:35:23.600]   wordpress.com. As an individual, as a business, there is no better place. First of all, 33% of the
[00:35:23.600 --> 00:35:30.320]   internet runs on Wordpress. 33% of the internet runs on Wordpress. That's because it's easy to use.
[00:35:30.320 --> 00:35:38.480]   It's powerful. It's just dominant. Wordpress.com hosts it for you. It's the same people. It's
[00:35:38.480 --> 00:35:43.120]   automatics, the WordPress people. But they do the hosting. They do the security. They do the
[00:35:43.120 --> 00:35:49.280]   patching, the updating. And most importantly, your WordPress support team is there 24/7.
[00:35:49.280 --> 00:35:52.800]   And it's not just somebody reading a notebook. These are people who live,
[00:35:52.800 --> 00:35:57.680]   breathe, and eat Wordpress. They know it backwards and forwards. And that means you're getting help
[00:35:57.680 --> 00:36:02.800]   from a true expert. Wordpress has the best site building tools, thousands of themes,
[00:36:02.800 --> 00:36:11.680]   a very vibrant ecosystem, no two week trials, no hidden fees. Wordpress.com was started so that
[00:36:11.680 --> 00:36:17.840]   anybody could publish their ideas. And it's free to start. But it always has room to grow.
[00:36:17.840 --> 00:36:23.360]   You know, you want more, you can have e-commerce, a bit of WordPress.com customer for 12 years.
[00:36:23.360 --> 00:36:30.400]   It's the best. And they are there to help 24 hours a day. Flexible, powerful. Some of the biggest
[00:36:30.400 --> 00:36:36.000]   companies in the world like Fortune use WordPress for their website, Quartz. And millions of people
[00:36:36.000 --> 00:36:41.440]   use WordPress every day, including me, leoloport.com is my WordPress site. Wordpress.com
[00:36:41.440 --> 00:36:45.840]   slash twig. If you go there right now, you'll get 15% off any new plan purchase. And it's a great
[00:36:45.840 --> 00:36:52.800]   way to show your support of the show. Wordpress.com slash twig. We thank WordPress so much for their
[00:36:52.800 --> 00:37:01.280]   support of this week in Google. And for keeping alive, the whole notion of a free and open web,
[00:37:01.280 --> 00:37:13.040]   WordPress.com slash twig. So YouTube, YouTube, YouTube where to begin. I should, I should begin
[00:37:13.040 --> 00:37:20.000]   with the newest stuff, which is that YouTube has announced this morning that they are going to go
[00:37:20.000 --> 00:37:30.800]   even farther to ban extremist hate. So this was the blog post this morning on the YouTube
[00:37:30.800 --> 00:37:37.280]   official blog, our ongoing work to tackle hate. They say it and point out, you know, we've always
[00:37:37.280 --> 00:37:44.000]   done this. We've always had a great policy, blah, blah, blah. They also, wow, they also say some of
[00:37:44.000 --> 00:37:49.440]   this content has value to researchers and NGOs looking to understand hate in order to combat it.
[00:37:49.440 --> 00:37:54.080]   So we're exploring options to make this content available to them in the future. But meanwhile,
[00:37:54.080 --> 00:37:59.360]   we're taking another step in our hate speech policy by specifically prohibiting videos.
[00:38:00.320 --> 00:38:05.120]   And here are the criteria. You're always looking for what are the criteria videos that allege that
[00:38:05.120 --> 00:38:12.800]   one group is superior in order to justify discrimination, segregation, or exclusion based on qualities
[00:38:12.800 --> 00:38:20.320]   like age, gender, race, caste, religion, sexual orientation, veteran status. So this would include,
[00:38:20.320 --> 00:38:24.960]   for example, videos that promote or glorify Nazi ideology, which is inherently discriminatory.
[00:38:24.960 --> 00:38:30.160]   Really, that's good. Then finally, we'll remove content denying well documented violent events
[00:38:30.160 --> 00:38:36.560]   like the Holocaust or Sandy Hook. We begin enforcing this policy today. No one knows, but it's estimated
[00:38:36.560 --> 00:38:41.920]   that thousands of channels will get banned. But it will take time for our systems to fully ramp up
[00:38:41.920 --> 00:38:50.160]   over the next several months. YouTube has been under continuous pressure. Most recently,
[00:38:51.200 --> 00:39:02.400]   the Crowder, louder Crowder channel. YouTube, this story came out yesterday, that Crowder,
[00:39:02.400 --> 00:39:07.920]   who was a conservative, it's really more like a, I watched a little bit of it's more like a morning
[00:39:07.920 --> 00:39:17.200]   show. He's kind of copying, I would say copying to some degree, Howard Stern. He's been critical
[00:39:17.200 --> 00:39:26.720]   of a Vox video columnist Carlos Mazza attacking his sexuality, his ethnicity, making fun of him,
[00:39:26.720 --> 00:39:35.600]   in a kind of, you know, shock, shock way. So at first, Google's YouTube said, now, you know what,
[00:39:35.600 --> 00:39:39.520]   this kind of does not violate our policies. We're not going to shut down his channel.
[00:39:39.520 --> 00:39:42.480]   This morning, they announced we are going to demonetize.
[00:39:44.400 --> 00:39:51.680]   We're going to, so he has a channel, still louder with Crowder, but update on our continued review.
[00:39:51.680 --> 00:39:57.600]   We've suspended his channel's monetization. We came to this decision. YouTube tweeted because
[00:39:57.600 --> 00:40:01.760]   of a pattern of egregious actions that's harmed the broader community and is against our YouTube
[00:40:01.760 --> 00:40:07.840]   partner program policy. So they're not blocking it, but they are going to keep them from making money.
[00:40:09.440 --> 00:40:15.440]   Oh, sorry. There you go. I was wondering why I was so quiet over there. Oh, hey, hey, hey,
[00:40:15.440 --> 00:40:20.400]   you've been muted. I saw something else. And I don't know how true this was, but said that
[00:40:20.400 --> 00:40:23.760]   he wasn't fully demonetized that if he takes down his t-shirts or something else, I don't know.
[00:40:23.760 --> 00:40:31.440]   I did. He had t-shirts that weren't exactly anti-gay, but they weren't exactly friendly either.
[00:40:31.440 --> 00:40:36.720]   Mazza has been, you know, livid, rightly so. I don't blame him.
[00:40:37.360 --> 00:40:42.720]   When you think, Jeff, I mean, you, you know, Howard Stern's not recently. This is the new Howard
[00:40:42.720 --> 00:40:48.160]   Stern. But in the past, he's done stuff like this, right? No, I don't think he's, no, I don't
[00:40:48.160 --> 00:40:52.960]   think he's ever been hateful. But the problem is how you define, hey, when I sat in the Facebook
[00:40:52.960 --> 00:40:59.760]   workshop on its oversight board, I got uncomfortable that the word, you know, hate speech was thrown
[00:40:59.760 --> 00:41:05.200]   around all the time is kind of being anything. So Crowder called Mazza. I won't play the video.
[00:41:05.200 --> 00:41:11.040]   I watched it. It's all done in this humorous fashion. He's got his zoo laughing at him
[00:41:11.040 --> 00:41:17.600]   called Mazza, "Lispy Queer and a Gay Mexican." That sounds nasty and hateful. I don't know if
[00:41:17.600 --> 00:41:22.640]   it's hate speech. Hate speech, right? That's what a problem we were devaluing the label of hate speech.
[00:41:22.640 --> 00:41:28.800]   And that's what I worry about so that anything becomes hate speech and then nothing is hate speech.
[00:41:28.800 --> 00:41:32.000]   And that's going to be a problem. We've got to have discussion about that.
[00:41:32.000 --> 00:41:37.120]   We're not listening. I think that Google's a company, YouTube is a community,
[00:41:37.120 --> 00:41:41.360]   YouTube has the full right responsibility to have a decent experience and I think they should
[00:41:41.360 --> 00:41:47.520]   take this crap down. However, there are implications. Right? One is that this does force
[00:41:47.520 --> 00:41:54.080]   these folks more underground. And it's going to be harder to fair it out. That's always the
[00:41:54.080 --> 00:41:58.720]   problem here. And number two, listen, this is the obvious stuff. If you look at something like,
[00:41:58.720 --> 00:42:06.160]   and I'm going to get in trouble here, PragerU. It's very slickly done arguing that, "Oh,
[00:42:06.160 --> 00:42:12.720]   hey kids, there's a war against men. Did you know that?" Hey kids, the Civil War, well, it was a really
[00:42:12.720 --> 00:42:22.720]   bad slavery. And very, very slickly done. I learned this from our friend Joan Donovan and Dana Boyd.
[00:42:22.720 --> 00:42:28.400]   And the problem is it's not hate speech. You can take it down, but it's very effective.
[00:42:28.720 --> 00:42:33.840]   And the problem for the YouTubes of the world is there's nothing kind of on the opposite side of
[00:42:33.840 --> 00:42:39.200]   it. The extreme of this is you can find lots of videos denying the Holocaust, but not so many
[00:42:39.200 --> 00:42:46.080]   verifying the Holocaust because who thinks you need to? Well, in this world, the video is saying
[00:42:46.080 --> 00:42:51.520]   the world is round. Right. Who thinks you need to? Actually, if you do, it's solely because of
[00:42:51.520 --> 00:42:57.760]   so many flat Earth videos. In other words, it's bringing out the demonstrators. No, no, wait a
[00:42:57.760 --> 00:43:02.240]   minute. It's all down. It is round and I can prove it. Now, I have to say there is a certain
[00:43:02.240 --> 00:43:09.120]   hypocrisy in YouTube waving rainbow flags celebrating Pride Month. On the one hand,
[00:43:09.120 --> 00:43:15.360]   in fact, they financed a documentary called State of Pride. And on the other hand, allowing
[00:43:15.360 --> 00:43:21.440]   homophobic slurs on some of their butt. Okay, was this bad enough for Benny?
[00:43:23.360 --> 00:43:28.560]   Well, again, is it a neutral platform? Is it a community? That's a really tough question. Yeah.
[00:43:28.560 --> 00:43:33.760]   And it's fine to say this isn't in my community. And to the idea that this would go further
[00:43:33.760 --> 00:43:39.280]   underground, in a lot of ways, I'm okay with that because like my kid is on YouTube, for example,
[00:43:39.280 --> 00:43:46.720]   my kid isn't searching out the weird, the dark area. She's not on the Reddit, you know, slash,
[00:43:46.720 --> 00:43:50.640]   she's on the Reddit slash corgi page, but she's not on the Reddit slash, you know,
[00:43:50.640 --> 00:43:57.760]   I hate the Jews page or whatever. I don't frequent the darker corners of the internet. So, you know,
[00:43:57.760 --> 00:44:05.200]   in some ways, I mean, I see your point, Jeff, but I'm also like, if we're going to say YouTube is
[00:44:05.200 --> 00:44:09.760]   this fun gathering area for videos, then it makes sense that we should treat it like a community
[00:44:09.760 --> 00:44:15.920]   and say, yeah, you know what, do not call someone a what was it, a sleazy, pixican queer or something,
[00:44:15.920 --> 00:44:19.440]   you know, that's we don't want that on our site. And we don't want active docs.
[00:44:19.440 --> 00:44:26.560]   And yes, but then, you know, when I learned this, talking to all these people who do this work,
[00:44:26.560 --> 00:44:31.520]   I'm talking to people on all the platforms now are doing this work. And they need rules.
[00:44:31.520 --> 00:44:38.000]   So the example I used up on online is the Nancy Pelosi slowed down speech video.
[00:44:38.000 --> 00:44:44.800]   Curtis was saying, take it down. Everybody's saying, take it down. Well, I put right underneath
[00:44:44.800 --> 00:44:49.200]   that the video did the exact same thing to Donald Trump. Yeah, we did it last week, we showed it.
[00:44:49.200 --> 00:44:55.680]   Right. Did I last week? Yeah, should that be taken down? Where's the need rules that they're
[00:44:55.680 --> 00:45:02.560]   going to be enforced with some predictability? Some fear. So what about conversation? And, you
[00:45:02.560 --> 00:45:08.800]   know, when you were talking about slavery, for example, and this is, I grew up in Texas. And in
[00:45:08.800 --> 00:45:13.840]   Texas, when I was growing up, it probably know it's still the case, I was taught that the Civil
[00:45:13.840 --> 00:45:18.480]   War was a states rights issue. There's a big statue right in front of the state house in Austin.
[00:45:18.480 --> 00:45:24.720]   There is states rights statue. It was in the textbooks. I mean, this was something that was
[00:45:24.720 --> 00:45:29.360]   taught to me. And it took me until, you know, high school when I finally read the people's history
[00:45:29.360 --> 00:45:33.520]   of the United States that I was like, Holy shit, I've been lied to my whole life. It's a particular
[00:45:33.520 --> 00:45:38.000]   right that the states wanted, you know, and it wasn't abortion, by the way.
[00:45:38.000 --> 00:45:45.760]   And so I would say that there are some things that, you know, we're still having discussions
[00:45:45.760 --> 00:45:50.960]   about this. And I think it's, I mean, in my mind, I would like, yeah, Google or sorry, YouTube,
[00:45:50.960 --> 00:45:57.520]   you should not have that. But I'm also like, that is not actively inciting hate against an
[00:45:57.520 --> 00:46:02.000]   individual, especially an individual who has no power, perhaps. Right.
[00:46:02.000 --> 00:46:06.960]   The worst thing you're right is that when other people get inspired by it, and they have no
[00:46:06.960 --> 00:46:12.480]   power to deal with that, that's the real problem here. So I, well, I don't know if it's, there's a
[00:46:12.480 --> 00:46:19.920]   lot of real problems here, but we have to think about what is, what the end result of some of the
[00:46:19.920 --> 00:46:27.120]   videos would be and how much agency the person being attacked has to solve it, rectify it, or
[00:46:27.120 --> 00:46:32.560]   protect themselves, perhaps. And that might be another way to think about how we formulate these
[00:46:32.560 --> 00:46:40.080]   rules. So are you attacking an individual? Are you attacking a whole nation? If you're attacking a
[00:46:40.080 --> 00:46:45.440]   whole nation, yeah, that sucks and you're a real jerk. But is that some, is that enough? You know,
[00:46:45.440 --> 00:46:49.200]   I don't know. So that's one way to maybe think about this. It's not going to get rid of everything.
[00:46:49.200 --> 00:46:55.280]   But the other question, and I'm playing devil's advocate here. So again, I think that the community
[00:46:55.280 --> 00:47:00.480]   needs to have a community standards and needs to be responsible for them. So I'm, you know, I'm,
[00:47:00.480 --> 00:47:09.360]   but I'm a free speech advocate. And I think generally, this notion that we can tamp down and get rid
[00:47:09.360 --> 00:47:14.000]   of and play whack a mold, what we consider bad speech is real problems. So the posts that I wrote,
[00:47:14.000 --> 00:47:19.040]   I explored this idea that I talked about a little bit last week, that regulating content is inevitably
[00:47:19.040 --> 00:47:30.800]   going to be a mistake. Regulating content requires that you have context about the intent and the
[00:47:30.800 --> 00:47:36.880]   impact. And so what you're really regulating is behavior. And that's harder, but that's what's
[00:47:36.880 --> 00:47:42.080]   really happening here. And so what we have is governments and media people saying to the platforms,
[00:47:42.080 --> 00:47:45.680]   you must regulate this behavior. We don't want to do it. It's your job to do it. We're not going to
[00:47:45.680 --> 00:47:50.160]   set the rules. We're not going to define anything here. It's harmful. You're going to get rid of this.
[00:47:50.160 --> 00:47:57.360]   Well, they're regulating behavior. And that's okay. We should. Should you regulate bigotry?
[00:47:57.360 --> 00:48:02.960]   Yeah. Well, hold on. Stop using the word regulate just for fun and accuracy, because this is,
[00:48:02.960 --> 00:48:08.000]   this is not a government. So let's talk about, but there's also, but it's fresh.
[00:48:08.000 --> 00:48:11.600]   It is, but like the UK harm is. Don't don't stretch right now because that's a slippery slope. We're
[00:48:11.600 --> 00:48:15.920]   not going to go there. So just for, you know, fair argument, let's call it what it is, which is,
[00:48:15.920 --> 00:48:20.880]   you know, uh, moderate guidelines or moderation. Yeah. Uh,
[00:48:20.880 --> 00:48:30.800]   and I think it's important to look at, and I'm going to stick with this because I do think this
[00:48:30.800 --> 00:48:37.680]   is a good distinction to talk about. And we don't usually talk about it is how far that content goes.
[00:48:37.680 --> 00:48:46.000]   And I know moderation is tough, but I do think if I am a person who's being attacked and I bring
[00:48:46.000 --> 00:48:51.840]   something up, that it is, and YouTube does have community guidelines already in place,
[00:48:51.840 --> 00:48:58.560]   I think they need maybe a better process for people making complaints and determining if
[00:48:58.560 --> 00:49:04.480]   they're legitimate and taking it down quickly. Um, and I think there's the, that same and maybe,
[00:49:04.480 --> 00:49:08.560]   like you said before, maybe it's a court kind of structure. Because I think, you know, you could
[00:49:08.560 --> 00:49:16.080]   argue that Pelosi versus us slowing down, or someone slowing down Trump, you know, those might be
[00:49:16.080 --> 00:49:21.120]   enough in the public interest, public interest or newsworthiness. I don't really know that you
[00:49:21.120 --> 00:49:27.600]   would keep those up, but attacking an individual is probably not an individual that is not a public
[00:49:27.600 --> 00:49:32.720]   figure, for example. Do you think we're, do you think, so there are, I mean, there are systems,
[00:49:32.720 --> 00:49:36.560]   there's, there, it's new and we're still trying stuff out, but we've got systems,
[00:49:36.560 --> 00:49:43.520]   for instance, with a Pelosi video, Facebook would pull up additional links and sources,
[00:49:43.520 --> 00:49:47.520]   perhaps or say the, the, there's some question about the authenticity of this video. By the way,
[00:49:47.520 --> 00:49:53.280]   the video seems to have disappeared completely from Facebook for whatever reason. But I think
[00:49:53.280 --> 00:50:00.720]   that there seemed to be, we're seem to be making progress perhaps in not necessarily pulling stuff
[00:50:00.720 --> 00:50:06.640]   down, but, but adding additional information. Is that sufficient? Yes. So that's what Facebook
[00:50:06.640 --> 00:50:14.640]   did with the Facebook, I talked to people there about this. Um, they will confess that
[00:50:14.640 --> 00:50:19.040]   primary problem, the Pelosi video was they did not quick enough. It took too long for them to see
[00:50:19.040 --> 00:50:23.600]   what it was. Um, once it, but they, you know, Facebook always says we have three actions.
[00:50:23.600 --> 00:50:28.560]   We can downgrade. We can add it. We can, we can kill and we can add information.
[00:50:28.560 --> 00:50:34.000]   They try not to kill, uh, uh, for, I think, decent reasons.
[00:50:34.000 --> 00:50:38.240]   Downgrading is them taking personal responsibility, not corporate responsibility,
[00:50:38.240 --> 00:50:42.880]   not to promote something. But that's really, by the way, is a big question with Google as well,
[00:50:42.880 --> 00:50:48.640]   which is their recommendation, engine, engine promoting, in a very promoting,
[00:50:48.640 --> 00:50:51.680]   that's where, that's where they step in. That's where their role really is.
[00:50:51.680 --> 00:50:56.880]   It's sufficient to say don't promote stuff in most cases that it grow organically.
[00:50:56.880 --> 00:51:00.800]   And then add that third thing and add information. You say people need to know this.
[00:51:00.800 --> 00:51:04.320]   The problem there though is there isn't sufficient research and sufficient data
[00:51:04.320 --> 00:51:09.520]   to know the impact of those informational boxes. We do know in the case of Facebook that when they
[00:51:09.520 --> 00:51:14.960]   added things to say it said this had been challenged by fact checkers that created more traffic for
[00:51:14.960 --> 00:51:19.920]   it and more people believing it. Isn't that weird? But you can't fix stupid. I mean, honestly,
[00:51:19.920 --> 00:51:23.440]   there's so much these platforms can do. You're designed intervention. You got,
[00:51:23.440 --> 00:51:29.440]   you got to be aware of the interventions impact as well. Um, and so, so, you know,
[00:51:29.440 --> 00:51:33.280]   that's though, but I think that's doing the right thing and not blocking it, not banning it.
[00:51:33.280 --> 00:51:39.120]   Adding context, not promoting it either. In other words, staying fairly neutral and adding
[00:51:39.120 --> 00:51:44.400]   context on it. That seems to me the right thing. Anything more seems to me weighing in a little,
[00:51:44.400 --> 00:51:48.960]   to putting a thumb on the scale a little too much. Well, it kind of depends on, I mean,
[00:51:48.960 --> 00:51:54.160]   I think some of the stuff you would want to put your thumb on the scale more, like some of the
[00:51:54.160 --> 00:52:01.280]   stuff merits greater intervention, not everything, but some things. And I would say, like attacks
[00:52:01.280 --> 00:52:05.760]   against an individual person rise to that occasion. Yes. Okay. I agree with you. There's,
[00:52:05.760 --> 00:52:09.200]   but those things are illegal too. I mean, there's certain things. Or actually, I would also say,
[00:52:09.200 --> 00:52:13.680]   if you could definitively prove that this is a bad activity, I would say that might rise to the
[00:52:13.680 --> 00:52:18.320]   occasion. That would be a good one too. If it's not real people, if it's being done in a concerted
[00:52:18.320 --> 00:52:24.240]   campaign for other reasons, maybe. Well, you know, I watched last night, I watched Cabaret,
[00:52:24.240 --> 00:52:31.760]   the original Bob Fossey, 1972 Cabaret, which I'm like, let's make this connection. Are you ready?
[00:52:31.760 --> 00:52:38.240]   I'm ready. A lot of natsies in it. And there's natsies not, I mean, the movie is about Berlin
[00:52:39.040 --> 00:52:45.120]   as in during the rise of the natsies. I know people think it's about, you know, life is a cabaret.
[00:52:45.120 --> 00:52:48.560]   And I guess on Broadway originally it was, but by the time Fossey got a hold of it,
[00:52:48.560 --> 00:52:56.240]   it became a movie about the rise of natsies. But it did it kind of, no, it was on Broadway too.
[00:52:56.240 --> 00:53:01.600]   The original book was was about that. Broadway was a little happier. Fossey added some stuff and
[00:53:01.600 --> 00:53:06.240]   decided to make it a little bit grimer. But nevertheless, in some ways showing it without
[00:53:06.240 --> 00:53:11.760]   commentary or maybe the way he juxtaposed images perhaps is more powerful than saying,
[00:53:11.760 --> 00:53:19.280]   no, no, no, no, no, no, no, no, don't show any of that. So showing stuff in context versus
[00:53:19.280 --> 00:53:24.720]   burying it, I'm not sure burying it is always the right. I mean, that's the solution Germany
[00:53:24.720 --> 00:53:32.160]   has chosen. They don't want to see anything like that. So I don't know. But I think the risk of
[00:53:32.160 --> 00:53:38.960]   burying it is you forget it. Well, yes. And I think some things,
[00:53:38.960 --> 00:53:44.000]   some things probably should be buried, but they're always going to come to light eventually, I guess.
[00:53:44.000 --> 00:53:50.720]   So yeah, I don't know. As human beings, we love making the other evil and wrong and bad.
[00:53:50.720 --> 00:53:55.520]   It's a very natural thing to do. So instead of pretending we don't do that, maybe we should
[00:53:55.520 --> 00:54:00.880]   show it and show it in context and show that's the key we've got. Even when you show things in
[00:54:00.880 --> 00:54:06.800]   context, people will take what it from that what they will. And as I said, there's no fix and stupid
[00:54:06.800 --> 00:54:13.040]   because people are people. You can't put a condom around the world's brains. Yeah, that's, I think
[00:54:13.040 --> 00:54:19.600]   that's the real risk. I think a lot of people would be in the first place. It's a great image,
[00:54:19.600 --> 00:54:23.840]   though. I think that that's the problem is that there are people and maybe Elizabeth Warren is
[00:54:23.840 --> 00:54:33.440]   one of them who want to protect everything and you can't. And so, and maybe the electric
[00:54:33.440 --> 00:54:39.600]   electorate is stupid. I think there's a lot of evidence. Yeah, cure here is education. Yeah,
[00:54:39.600 --> 00:54:45.840]   let's promote education. Let's give people the tools they need to judge these things. But if
[00:54:45.840 --> 00:54:52.720]   somebody's going to look at louder with Crowder and say, yeah, those gay people list, I hate that.
[00:54:53.680 --> 00:54:58.960]   What are you going to do? There are people like, I mean, what are you going to do? Actually,
[00:54:58.960 --> 00:55:06.080]   it was interesting because in cabaret, Michael York's character does stand up and says, you're a
[00:55:06.080 --> 00:55:13.200]   dunderhead or whatever. And he gets beat up as a result. But he chooses to fight them. Everybody
[00:55:13.200 --> 00:55:18.640]   else is kind of turning their head. It's actually, it's interesting to watch these days because
[00:55:21.280 --> 00:55:25.040]   just as there is an argument that we have, there are certain things you have to fight. You can
[00:55:25.040 --> 00:55:29.280]   ignore it as much as you can ignore it. Punching an ad in the nose is okay.
[00:55:29.280 --> 00:55:35.840]   I'm actually okay with that too. It's okay. I'm like, I would do that.
[00:55:35.840 --> 00:55:40.000]   It's okay. We are in the 50th anniversary of D-Day tomorrow and we are
[00:55:40.000 --> 00:55:46.240]   the 50th anniversary, I'm sorry, the 75th anniversary of D-Day, the 50th anniversary of Stonewall
[00:55:46.240 --> 00:55:54.240]   Riots, which is why it's Pride Month. It's not that long ago.
[00:55:54.240 --> 00:56:02.000]   We talk about this a lot and it's something that we chew on a lot because I don't think there
[00:56:02.000 --> 00:56:04.560]   is an obvious and easy answer. I thought about the kid thing.
[00:56:04.560 --> 00:56:12.000]   YouTube acted quickly on that and that's a recommendation engine problem, isn't it?
[00:56:12.560 --> 00:56:19.200]   It's once a couple things. It's, as I understand it, yes, it's a recommendation engine problem. It's
[00:56:19.200 --> 00:56:26.080]   also a live problem and so children cannot be on a live video unless they are visibly
[00:56:26.080 --> 00:56:31.280]   accompanied by an adult. Do you think that will fix it? I don't even know what that...
[00:56:31.280 --> 00:56:34.160]   Yeah, there's bad adults out there but it's probably a start.
[00:56:34.160 --> 00:56:36.160]   It's a start, it's a good start.
[00:56:36.160 --> 00:56:41.040]   But what the idea is that young kids are live streaming themselves and that
[00:56:41.040 --> 00:56:48.240]   pet arrests inappropriately can watch these videos and start commenting on them and get
[00:56:48.240 --> 00:56:50.640]   other pet arrests to watch. Or comments or right on something.
[00:56:50.640 --> 00:56:56.480]   Because the theory being that the kids don't know and so there, if there were an adult around
[00:56:56.480 --> 00:56:59.040]   who would say, "Oh, maybe you should put some clothes on, Johnny."
[00:56:59.040 --> 00:57:05.600]   How old was the... We mentioned her was a week last week or week before last, the young woman who
[00:57:05.600 --> 00:57:13.040]   does the comedy? Yeah, she was like a high school kid. She started when she was like nine and
[00:57:13.040 --> 00:57:18.640]   there's like little Tay, is that her name? She's like nine years old and saying terrible,
[00:57:18.640 --> 00:57:24.480]   terrible things. The thing about those kids is they're copying what they see on YouTube.
[00:57:24.480 --> 00:57:28.160]   They're copying what YouTube influences their success. Or God knows what.
[00:57:28.160 --> 00:57:32.960]   Well, but no, I think honestly, at least in the case of the 14-year-old that we were
[00:57:32.960 --> 00:57:38.160]   talking about a few weeks ago, she was right in there with the YouTube
[00:57:38.160 --> 00:57:46.800]   zeitgeist. She fit right in. And I think she was a creature of that age of that time.
[00:57:46.800 --> 00:57:52.160]   And part of that is like, I know that everyone was making a big deal of the New York Times article
[00:57:52.160 --> 00:57:56.400]   about the guy who got jail time for feeding the homeless guy at Oreos with toothpaste in it.
[00:57:56.960 --> 00:58:06.800]   But part of the problem is that all of these are optimizing for views in people are
[00:58:06.800 --> 00:58:10.800]   perient, period. Perient is right. That's what they are. Yes.
[00:58:10.800 --> 00:58:16.080]   And if you could optimize instead of for views, and the other thing is,
[00:58:16.080 --> 00:58:22.880]   very accurately, it makes Google and YouTube more money when you have more people viewing for
[00:58:22.880 --> 00:58:29.360]   longer. But maybe instead of optimizing for engagement and views and thus ad revenue,
[00:58:29.360 --> 00:58:35.520]   we optimize for something else. Maybe it clicks on a, you know,
[00:58:35.520 --> 00:58:39.520]   that's what Google says we're going to start doing is we're not going to just base it on
[00:58:39.520 --> 00:58:44.240]   engagement because clearly that algorithm has failed. What if they didn't do recommendations at all?
[00:58:44.240 --> 00:58:51.040]   I don't know. I mean, I don't know. But recommendations are notoriously terrible.
[00:58:52.000 --> 00:58:55.520]   But they don't have to be fixed. Well, maybe they do. Maybe that's a hard thing to fix.
[00:58:55.520 --> 00:58:59.600]   Well, like if I look for quantum computing videos, for example,
[00:58:59.600 --> 00:59:03.520]   but no, that's different. You're doing a search. It's not that initial search. I want them to stop.
[00:59:03.520 --> 00:59:08.000]   It's the then after you watch the quantum computing video, the next one.
[00:59:08.000 --> 00:59:13.440]   So like, so I'm watching a quantum computing video. Right now, everything's optimized for
[00:59:13.440 --> 00:59:18.560]   engagement, which means the next highest. That's wrong with you searching for another quantum
[00:59:19.520 --> 00:59:25.440]   video rather than Google proposing. Because I'm lazy and maybe I will stop being so lazy.
[00:59:25.440 --> 00:59:32.080]   Generally relevance is a good thing. No, it's because they can't do it well.
[00:59:32.080 --> 00:59:37.920]   They're picking the recommendations like so they've, they've talked about how Google sends you to
[00:59:37.920 --> 00:59:43.040]   longer and longer videos because they want to optimize for more engagement and ad time, right?
[00:59:43.040 --> 00:59:49.440]   So maybe you stop picking the longer videos. Maybe you optimize for maybe in the middle of a video,
[00:59:49.520 --> 00:59:53.520]   you have people who are watching it. Hey, are you still watching it? And are you learning something?
[00:59:53.520 --> 00:59:59.680]   And if I click yes, then that helps you, you know, that helps get you a better recommendation.
[00:59:59.680 --> 01:00:02.720]   That that's a weight that they use when they're doing that algorithm.
[01:00:02.720 --> 01:00:07.040]   Maybe I'm optimistic. I just don't think we need it. When you go look at a web,
[01:00:07.040 --> 01:00:10.080]   when you do search for quantum computing and you go to read a web page,
[01:00:10.080 --> 01:00:14.160]   Google doesn't then put up a sidebar saying, here's five other web pages you'd be interested in.
[01:00:14.160 --> 01:00:17.920]   No, you search for another page. You continue on. You do a normal thing.
[01:00:17.920 --> 01:00:22.960]   When I go to Amazon and I look for a book, I'm grateful that Amazon says the other books that
[01:00:22.960 --> 01:00:27.360]   are related to those recommendations are ghastly on Amazon. Did you see, did you see how they
[01:00:27.360 --> 01:00:31.120]   talked about that? They actually did a thing today at Remar's. They talked about how their
[01:00:31.120 --> 01:00:37.440]   original recommendation efforts were terrible and how they worked to improve them. So it was a
[01:00:37.440 --> 01:00:41.200]   whole thing on how they tweaked their algorithms. And I wish I could tell you how, but I have not
[01:00:41.200 --> 01:00:46.880]   finished reading it. So I guess you could make recommendations better. I think maybe just dump
[01:00:46.880 --> 01:00:53.920]   them because you're not a, you could find the next video. You don't need them to tell you.
[01:00:53.920 --> 01:01:02.080]   Then recommendations are. No, maybe, I may not find the Slack scientist who is the top of his
[01:01:02.080 --> 01:01:09.600]   field doing this because maybe, I mean, and currently, my search recommendations are based on links in.
[01:01:10.160 --> 01:01:17.520]   So what if we create the equivalent of links in or reputable links in to a YouTube video?
[01:01:17.520 --> 01:01:23.440]   So maybe if you're a professor at a certain university, maybe that gets you extra points.
[01:01:23.440 --> 01:01:30.960]   So your videos on your topic of expertise are, you know, like I would love to get those recommendations
[01:01:30.960 --> 01:01:35.360]   from random paleontologist. I don't even know if that should be nice if that worked, but it doesn't.
[01:01:35.360 --> 01:01:41.760]   But I watch a nice, a nice elephant video and I get more elephant video. I get elephants and
[01:01:41.760 --> 01:01:46.880]   bikinis. I don't want it. I want to see an elephant in a big deal. Don't stop me from my
[01:01:46.880 --> 01:01:51.920]   nice elephant video. So here's the recommendations Amazon's giving to me. First thing bought it.
[01:01:51.920 --> 01:01:56.960]   I can imagine. Second thing bought it. Third thing bought it. Fourth thing looked at it didn't buy it.
[01:01:56.960 --> 01:01:59.600]   Fifth thing looked at it didn't buy it. Why are you getting old?
[01:02:01.440 --> 01:02:06.400]   They're recommendations for stuff I either bought or looked at. This is brain dead.
[01:02:06.400 --> 01:02:12.960]   This is not good. This is not adding anything to my life. That screen shine is good. It really
[01:02:12.960 --> 01:02:16.880]   works well. By the way, see now that's useful if you tell me that. Yeah, that's good. Yeah,
[01:02:16.880 --> 01:02:21.760]   you like that, huh? Oh, the the other one. Oh, the whoosh. Yeah, you know why it's recommending that?
[01:02:21.760 --> 01:02:28.080]   I bought a case of it. Oh, my recommendations are awesome. I've got exhalation by Ted Chiang,
[01:02:28.080 --> 01:02:31.360]   totally. And then I've got a whole bunch of books that I bought for my daughter.
[01:02:31.360 --> 01:02:34.480]   Yeah, but you already bought them. Why is he recommending something you already bought?
[01:02:34.480 --> 01:02:38.800]   That's not a good recommendation. I haven't. I haven't bought them. But I bought a lot of these
[01:02:38.800 --> 01:02:43.920]   and I don't want to buy it again. Maybe if they said buy it again, Leo, that would make sense.
[01:02:43.920 --> 01:02:48.960]   And then underneath it, things you had looked at but didn't buy or categories. The only one
[01:02:48.960 --> 01:02:53.120]   good thing in here is that when I bought the whoosh, now it's saying, well, maybe you'd be more
[01:02:53.120 --> 01:03:01.120]   interested in screen mom. No, don't think so. It's trying to offer me an alternative.
[01:03:01.120 --> 01:03:06.960]   Now that is that. Oh, notice, by the way, Amazon's choice. Oh, this says recommended items other
[01:03:06.960 --> 01:03:12.720]   customers often by again, not for me. This is just recommended period for you. Things you
[01:03:12.720 --> 01:03:18.720]   should do. What is this really good? I might just buy it. Well, obviously people do buy this
[01:03:18.720 --> 01:03:22.880]   crap because otherwise, why would Amazon do it? Why is it recommending an old Nokia phone to you,
[01:03:22.880 --> 01:03:30.560]   Leo? I bought it. This is the greatest thing ever. Yeah. Oh, I got it. Oh, it's a fake Nokia 3310.
[01:03:30.560 --> 01:03:36.640]   It's the size of a matchbook. Do I have it here? That's adorable. Can you get me my backpack?
[01:03:36.640 --> 01:03:41.520]   It's in my backpack, Jeff. It is really adorable. It's a Dorbs.
[01:03:41.520 --> 01:03:50.080]   I don't think they say that anymore. It's on fleek. Do they say that? No, no.
[01:03:51.680 --> 01:03:56.640]   That is very much done. Honestly, I think we could live without Google or Amazon recommendations.
[01:03:56.640 --> 01:04:03.120]   Netflix recommendations equally crappy. I don't mind them. They don't make them better.
[01:04:03.120 --> 01:04:09.600]   But honestly, isn't that the primary problem with YouTube is those recommendations? If you
[01:04:09.600 --> 01:04:14.480]   didn't have those, when this stuff goes away, no, that is not the primary problem.
[01:04:14.480 --> 01:04:21.520]   Leo, it's 0.000001% of the recommendations are causing trouble and the rest are giving me more
[01:04:21.520 --> 01:04:28.480]   elephant videos. And I'm grateful for that. You cruel bastard. It's because of you and lazy people
[01:04:28.480 --> 01:04:34.640]   like you that we have this whole flatter because it's because of vaccination nazis. There are evil
[01:04:34.640 --> 01:04:40.800]   people who will game systems. And the problem is they're not regulation. The effort is not to
[01:04:40.800 --> 01:04:45.600]   regulate technology. The effort is not to regulate algorithms. It's to regulate people's behavior.
[01:04:45.600 --> 01:04:49.120]   Because they're not gaming it. It's the stupid little algorithm. Look at that. Isn't that the
[01:04:49.120 --> 01:04:55.360]   cutest thing you ever saw? Oh, my Lord. How do you dial it? What do you do with it? It works.
[01:04:55.360 --> 01:05:03.600]   You can even text. It reminds me of Zoolander. It's as small as a SIM card. It was $12. I'm
[01:05:03.600 --> 01:05:07.920]   sorry, $20. It was cheap. I couldn't resist. So you just pop your SIM card in and you have
[01:05:07.920 --> 01:05:12.320]   a SIM card in. You even have room for an SD card. Can you see this saying welcome?
[01:05:12.320 --> 01:05:16.320]   Can you take a picture? What do you do with it? Just talk in text?
[01:05:18.320 --> 01:05:24.080]   Yeah, talk in text. It's a phone. You ever hear of a phone? Oh, my goodness.
[01:05:24.080 --> 01:05:33.840]   Hello. I love it. How does it sound? Just like you just heard. It's tinny, but it's a phone.
[01:05:33.840 --> 01:05:41.520]   It's also you could eat it. What? Did you wait?
[01:05:44.080 --> 01:05:47.920]   It's like a four year old there. Don't put that in your mouth, honey. That is a phone.
[01:05:47.920 --> 01:05:51.920]   Phones are not free. But it's red raspberry flavored.
[01:05:51.920 --> 01:05:59.520]   Okay. Should we do the change log? Get the change log ready.
[01:05:59.520 --> 01:06:06.560]   Fire up the drums. Play the trumpets. The Google change log.
[01:06:10.160 --> 01:06:16.400]   It's time for all the new things at Google. This is so cool. And actually, the first time I saw this,
[01:06:16.400 --> 01:06:21.600]   there must be something new that's going on in the web that allows you to do AR on the web.
[01:06:21.600 --> 01:06:26.000]   Because the first time I saw this was on Tuesday or Monday, when Apple had its event,
[01:06:26.000 --> 01:06:30.880]   if you scroll to the bottom of the new Mac Pro page, it said, open this in Safari and you can
[01:06:30.880 --> 01:06:37.360]   have an augmented reality. And I was able to put my Mac, Mac Pro on my table beautifully.
[01:06:37.360 --> 01:06:43.920]   Well, now you can do this in Google search AR objects in Google search. So if you search for
[01:06:43.920 --> 01:06:52.320]   Tiger, that actually announced this at Google I/O, but now it's live. So I can't really do it because
[01:06:52.320 --> 01:06:57.920]   in order to do it, I'd have to. But I'll show you a video on Twitter. So you're searching for
[01:06:57.920 --> 01:07:03.360]   Tigers. And then you see that meet a life-sized Tiger in real life. And then if you press view,
[01:07:03.360 --> 01:07:08.640]   view on your space, you can put it there and you can resize it and rotate it. And it's just like
[01:07:08.640 --> 01:07:14.080]   it's there with you. And I have to say, the AR stuff here has gotten better and better and better.
[01:07:14.080 --> 01:07:21.200]   The Mac Pro was really sitting right on the table nice and solid. Apple showed some occlusion
[01:07:21.200 --> 01:07:28.960]   stuff that really works very well. So I think this is exciting. Now, obviously, nobody really
[01:07:28.960 --> 01:07:35.040]   wants to hold up a phone. You know, your arm gets tired and do this forever. But it seems pretty
[01:07:35.040 --> 01:07:40.240]   clear both Apple and Google and I guess Microsoft too are getting very close to putting this stuff
[01:07:40.240 --> 01:07:47.920]   in glasses or something, right? Sorry, I didn't mean to put you to sleep. Stay safe. I know what
[01:07:47.920 --> 01:07:53.840]   you'll like. I know what you'll like. How about whale songs? Do you like? Do you like whales?
[01:07:53.840 --> 01:08:01.600]   Do you like whale songs? You're gonna like whale song radio. Oh my lord.
[01:08:01.600 --> 01:08:06.800]   So Noah, the National Oceanographic and Atmospheric Administration or whatever.
[01:08:06.800 --> 01:08:12.000]   Is that right, John? Yeah, I think that ocean is producing some form.
[01:08:12.000 --> 01:08:18.000]   Has recorded tens of thousands of hours of whale song. So Christopher Clark here. Go ahead,
[01:08:18.000 --> 01:08:24.640]   you can do the songs of life around this planet. He loves these little whale songs.
[01:08:24.640 --> 01:08:31.600]   Listen to this expanse of the. So what's Google doing with this? Well, Google has been working with
[01:08:31.600 --> 01:08:39.840]   Noah to train an artificial intelligence model on their thousands of hours of underwater recordings.
[01:08:39.840 --> 01:08:44.800]   So you can, I don't know, talk whale talk. I don't know. So Pete Buttigieg could talk
[01:08:44.800 --> 01:08:50.720]   whale talk. Somebody's gonna talk whale talk. So you know what's the, okay, slate does these
[01:08:50.720 --> 01:08:56.080]   stories as part of their future tense effort and they get these these writers to write random
[01:08:56.080 --> 01:09:04.480]   stories and Annalise knew it's wrote one about it was about crows and people and a robot in the
[01:09:04.480 --> 01:09:09.680]   robot basically learned how to speak crow in the story. There you go. There you go. And it was it
[01:09:09.680 --> 01:09:15.920]   was actually a really cool story because it was this friendship between this robot who had AI and
[01:09:15.920 --> 01:09:21.520]   crow and they worked together to like help them teach my bio Boston dynamics.
[01:09:21.520 --> 01:09:22.400]   Yeah.
[01:09:22.400 --> 01:09:28.880]   Dog to talk whale so it can attract them and eat them. This is Google's.
[01:09:28.880 --> 01:09:30.720]   No, getting this is.
[01:09:30.720 --> 01:09:38.880]   Wait, so there's actually a in the Pacific Northwest where I now live. There was a ferry that just hit
[01:09:38.880 --> 01:09:42.400]   a whale. It was the first time that ever happened. It was tragic for everybody.
[01:09:42.400 --> 01:09:47.040]   Mostly the whale, but nobody wanted to hit it. But what if you could put on the ferry a whale
[01:09:47.040 --> 01:09:51.920]   song that broadcast like stay away. I get out of the way here. I come. I'm a big boat.
[01:09:51.920 --> 01:09:58.000]   Watch out for me. So this is you can do this yourself. Now that this is the Google part,
[01:09:58.000 --> 01:10:05.120]   the Googley part pattern radio dot with Google dot com. You can explore thousands of hours of
[01:10:05.120 --> 01:10:10.960]   humpback whale songs and make your own discoveries. I don't know what discoveries you would make.
[01:10:10.960 --> 01:10:21.920]   But you want to do a tour? Let's we can do a tour of whale songs. Matt Harvey is a software
[01:10:21.920 --> 01:10:26.880]   engineer who's working with Ann Allen on machine learning model. So he's going to show you.
[01:10:26.880 --> 01:10:31.680]   He says there's a song in this section. It's very faint because the whales far away.
[01:10:32.800 --> 01:10:38.480]   Zoom out to see it surrounded by louder sections. My deep net machine learning
[01:10:38.480 --> 01:10:47.600]   model worked better than my ears for faint calls. Let's see what Ann Allen has to say.
[01:10:47.600 --> 01:10:52.800]   So if you're interested in whale song, basically it's whale song radio.
[01:10:52.800 --> 01:10:59.200]   Pattern radio dot with Google dot com. I'm probably falling asleep. It's very soothing.
[01:11:00.320 --> 01:11:03.360]   It's kind of, you know what? Really? It's like alien speech.
[01:11:03.360 --> 01:11:15.520]   So anyway, you can you can wander to your heart's content through these fabulous whale songs.
[01:11:15.520 --> 01:11:20.960]   I don't know. That's that's one thing. Anyway, go away, whale songs. Thank you. Google photos.
[01:11:20.960 --> 01:11:28.640]   I'm sailing here. Get out of the way.
[01:11:29.440 --> 01:11:37.040]   Hey, he comes up. I speak well. That's like my daughter and I still do that.
[01:11:37.040 --> 01:11:41.520]   It's one of our favorite guys. See see see. You might want to show her this.
[01:11:41.520 --> 01:11:46.640]   Google photos dark mode rolling out to Android Pie and Oreo. Everybody was so excited when Apple
[01:11:46.640 --> 01:11:52.640]   announced dark mode for iOS 13 on Tuesday. I hate dark mode. Yeah, hate it. Yeah, actually,
[01:11:52.640 --> 01:11:58.960]   there's there's now kind of an anti dark mode contingent rising up. Yeah. Yeah.
[01:11:58.960 --> 01:12:03.520]   I like dark mode. Yeah, I would. Yeah. You have your choice.
[01:12:03.520 --> 01:12:08.240]   Yeah, I hate it. No, because sometimes that just it just thinks it's doing a favor and just
[01:12:08.240 --> 01:12:12.880]   because dark out does that and it freaks me out and I hate it. Hate it. If you want to see if your
[01:12:12.880 --> 01:12:20.000]   Android Pie equipped phone can receive the new dark mode in photos, head to settings, display,
[01:12:20.000 --> 01:12:28.800]   advanced device theme turn on limited dark mode. And if you and you'll you'll have a dark darkness.
[01:12:28.800 --> 01:12:35.680]   You know, setting never dark dark. Yes, exactly. Dark. It's good for you and it's good for your phone.
[01:12:35.680 --> 01:12:41.200]   And every week now there there's there we got another story just said, oh, so and so has dark
[01:12:41.200 --> 01:12:49.360]   mode down. Yeah, I know. I don't care. The only thing that the crowd at the Apple event was more
[01:12:49.360 --> 01:12:56.160]   excited about than dark mode was the fact that there the new watch calculator will do tipping.
[01:12:56.160 --> 01:13:01.440]   They cheered. They cheered like. Oh, my God.
[01:13:01.440 --> 01:13:05.840]   That's hard. The heavens have opened. I now can do 15%.
[01:13:05.840 --> 01:13:12.400]   20%, which is easier math. Come on. It's easy. That's why I always, you know what I always do?
[01:13:12.400 --> 01:13:17.760]   I always double the bill and give them that. Yeah, 20%. Oh, that's what you're supposed to
[01:13:17.760 --> 01:13:20.000]   give now. I was supposed to take off some of those zeros, aren't you?
[01:13:20.000 --> 01:13:26.800]   What do they like you? I don't get great service everywhere. Actually over tip because I,
[01:13:26.800 --> 01:13:30.800]   as a young man, have worked in the service industry and I know well,
[01:13:30.800 --> 01:13:36.480]   well, in now they still haven't they still haven't updated the server wages. So it's still like $2.13
[01:13:36.480 --> 01:13:41.280]   plus your tips. So if your tips aren't going up, you are hosed. We do you also have a bit of a
[01:13:41.280 --> 01:13:47.520]   celebrity tax? No, nobody knows who I am. But you're right. You know, I saw Bill Murray went
[01:13:47.520 --> 01:13:55.040]   to the Groundhog Day Broadway show and I noted that as he, you know, he went and bought a ginger
[01:13:55.040 --> 01:13:59.840]   ale before he sat down, he had to tip the guy 50 bucks. And that's the celebrity tax. Because if
[01:13:59.840 --> 01:14:04.960]   you're Bill Murray and you give him a buck or no tip at all, there's going to be a, there's going to
[01:14:04.960 --> 01:14:11.680]   be a story in page six about you. Right? Man. I think if you just give a dollar, you're fine.
[01:14:11.680 --> 01:14:16.320]   No, 50 bucks. Oh, not if you're Bill Murray. No. How are the starting the same thing? He
[01:14:16.320 --> 01:14:21.440]   asked. Yeah. Well, Tom, big money. Actually, I was watching comedians and cars getting coffee,
[01:14:21.440 --> 01:14:25.920]   you know, the Seinfeld show. And he was, I think he was out with, I can't remember somebody, some
[01:14:25.920 --> 01:14:30.240]   other famous comic. And the comic wants to leave a normal tip in the guy and Seinfeld said, you
[01:14:30.240 --> 01:14:36.080]   can't leave that tip. Yeah. You're famous. You have to give him a hundred bucks. You can't leave that
[01:14:36.080 --> 01:14:44.240]   tip. Google Maps is rolling out a live speedometer feature in the US and some EU nations. So as you're
[01:14:44.240 --> 01:14:48.640]   driving along, your Google Maps will now tell you what the speed is.
[01:14:48.640 --> 01:14:53.040]   Up to ways, which is added for ages. Yeah. As as our Tesla.
[01:14:53.040 --> 01:14:57.440]   Oh, I turned that off. I was like, it does not. You don't want to know.
[01:14:57.440 --> 01:15:04.560]   Oh, you know what? If you have a Tesla, you know what happened somehow? When I do a map now in Google,
[01:15:04.560 --> 01:15:09.200]   I get an option when I have it. Oh, when I do an address in Google, I have an option now to send
[01:15:09.200 --> 01:15:15.040]   it to my Tesla. Oh, that's very fancy, but very small. I like that people.
[01:15:15.040 --> 01:15:20.400]   I have to say, Tesla is one of the few car companies who still does all of their own,
[01:15:20.400 --> 01:15:25.440]   you know, entertainment system stuff on the center console. I love Android. We got, we just
[01:15:25.440 --> 01:15:31.840]   got a Chevy Bolt, the B-O-L-T like you did. Yeah. And I love Android Auto and CarPlay, but
[01:15:31.840 --> 01:15:37.200]   Android Auto especially, really great. It's really great. You get to use ways, your Google Maps on
[01:15:37.200 --> 01:15:42.160]   the Apple. You can also use Apple Maps. And it's just, it's much better.
[01:15:42.160 --> 01:15:47.520]   It's a little, so let me tell you something. So I switched to my Google 3A XL.
[01:15:47.520 --> 01:15:50.240]   Oh, good. How do you like it? I like it. I mean, did...
[01:15:50.240 --> 01:15:53.360]   Oh, I'm about to buy that. I like it. I like it. You had a Pixel.
[01:15:53.360 --> 01:15:56.320]   You had a Pixel XL, right? I had a Pixel 2 too. Oh, okay.
[01:15:56.320 --> 01:16:00.160]   Oh, how do you wait? Oh, it was a skip. It's very nice, but you know, but it's not that
[01:16:00.160 --> 01:16:04.480]   no, it's been different. I love it. It's great. Okay, because I'm going to do that too. Thanks, Jeff.
[01:16:04.480 --> 01:16:11.680]   It's good. It's good. Yeah. But, but, so I use Android Auto. And I was frustrated as hell,
[01:16:11.680 --> 01:16:16.800]   because the only things that would show up under the audio, under the Android Auto on the car
[01:16:16.800 --> 01:16:21.360]   screen were the Google Apps. Oh, no, I can use Audible now, which is nice.
[01:16:21.360 --> 01:16:25.760]   All my Audible would show up and I was going crazy. So finally, I looked up some stuff. I had to
[01:16:25.760 --> 01:16:34.960]   go in and enable developer mode, or Android Auto, to say, accept Strange, whatever they call it,
[01:16:34.960 --> 01:16:38.240]   because you can get Pandora, you can get Spotify. Right, another all there.
[01:16:38.240 --> 01:16:43.200]   Now, they're all there. But it was all there before, so they must have done some kind of security
[01:16:43.200 --> 01:16:46.560]   change. So I had to go in and go in. It was a page and so to get a developer mode, you have to go
[01:16:46.560 --> 01:16:53.760]   to the About page and click on something 10 times. Yeah, yeah. Are you sure? Are you sure?
[01:16:53.760 --> 01:16:58.880]   Yeah, no, no, it's just you have to it's everybody. That's good to know the secret handshake.
[01:16:58.880 --> 01:17:02.640]   It's a secret hand. So if anybody out there is having that problem, because my
[01:17:02.640 --> 01:17:07.280]   prior phone was fine. And then this one, I could not do a thing. Don't have to do anything on my
[01:17:07.280 --> 01:17:12.320]   Samsung Galaxy S10 Plus. It's just it sees it all. And you know, it's really awesome on the
[01:17:12.320 --> 01:17:17.840]   iPhone. There's a third party Twitter app called MyTwit, which I really like. It showed up.
[01:17:17.840 --> 01:17:23.360]   There's a big Twit logo right there on the screen. I can listen to Twit Live. I can listen to Twig.
[01:17:24.240 --> 01:17:28.800]   And that means a question for you. Yes. So I love ways being on the big screen on the car.
[01:17:28.800 --> 01:17:32.320]   I cannot get turned by turn directions. All it gives me is the map.
[01:17:32.320 --> 01:17:40.320]   Oh, I don't know. I haven't tried it. So Google has something with Android Auto.
[01:17:40.320 --> 01:17:43.920]   It started offering me this recently and it drove me nuts. And I can't think of what it's called,
[01:17:43.920 --> 01:17:46.640]   but I had to download it. That's what was offered you. Dark.
[01:17:47.840 --> 01:17:54.720]   Dark. It was it was navigation for Android Auto. It was so I had to download something else to
[01:17:54.720 --> 01:18:00.400]   I just I just pressed the little microphone. I say navigate to work and it does the turn by turn.
[01:18:00.400 --> 01:18:04.560]   So that's on that's on your Tesla. So that's not on the Tesla. That's on the bolt.
[01:18:04.560 --> 01:18:11.600]   Well, I mean, it does that. Well, I can see the map. I can't see the alpha numeric list of
[01:18:11.600 --> 01:18:15.440]   turn. Oh, you want the list. You don't. I will. It'll give you the tell you the turn though.
[01:18:15.440 --> 01:18:18.960]   It gives you turn by turn because because then I can't judge what it's doing ahead.
[01:18:18.960 --> 01:18:22.960]   I understand. There's no way to see ahead. So what I do is I assume it you can pinch it.
[01:18:22.960 --> 01:18:28.320]   Pinch it down so you see more of the map. Well, I can't know on the Mazda.
[01:18:28.320 --> 01:18:31.120]   If you're moving, you can't pinch. You can't touch the screen.
[01:18:31.120 --> 01:18:37.360]   Get a screen. No, no, you need a better car. Well, I do actually. Yeah, I do.
[01:18:37.360 --> 01:18:38.640]   Bye. Bye.
[01:18:38.640 --> 01:18:39.200]   Bye. New car.
[01:18:39.920 --> 01:18:45.440]   My son got a my son got a Volkswagen and I didn't get it because it was going to require premium gas.
[01:18:45.440 --> 01:18:49.520]   Yeah. And then now the turbo doesn't require premium gas and I'm all jealous.
[01:18:49.520 --> 01:18:55.120]   Actually, I want I'm only by electric. I love electric. So the bolt. Tell us about the bolt.
[01:18:55.120 --> 01:18:59.040]   It's great. It's great. Little car. Yeah. One thing it will not get though,
[01:18:59.040 --> 01:19:04.320]   and I'm sad because my Tesla will is Cuphead. Did you see this? Stacy?
[01:19:04.320 --> 01:19:09.200]   This is not a change log item, but I got to say it. Tesla is going to add. You know how they
[01:19:09.200 --> 01:19:15.360]   have Atari games? They're going to add one level of the hardest video game ever,
[01:19:15.360 --> 01:19:20.960]   which is Cuphead. This really should never be on a car because.
[01:19:20.960 --> 01:19:25.200]   Yeah. What is happening? Well, you can't you can't do it while you're driving.
[01:19:25.200 --> 01:19:32.080]   But apparently Elon says they're working with the Cuphead developer and he says,
[01:19:32.080 --> 01:19:35.760]   it's a cool game. It's insanely difficult. I think it's just because Elon likes the game,
[01:19:35.760 --> 01:19:38.960]   probably. It's statistically difficult. It's a twisted plus dark. It looks like
[01:19:38.960 --> 01:19:41.840]   some cute Disney thing and you're like, this plot is very dark, which is true.
[01:19:41.840 --> 01:19:48.480]   So I love Cuphead and it's coming to the Tesla, but that is that is really inappropriate
[01:19:48.480 --> 01:19:51.280]   in the Google change log. So let me continue on.
[01:19:51.280 --> 01:20:00.320]   Google Play is adding 43 US banks and credit unions. So if you've asked Google Play to add
[01:20:00.320 --> 01:20:06.000]   a credit card and it said, no, your bank doesn't support it. Try again. Try again.
[01:20:06.880 --> 01:20:08.160]   I need to add. I need to add.
[01:20:08.160 --> 01:20:12.080]   Give us the credit card numbers and
[01:20:12.080 --> 01:20:15.120]   chase. I don't see chase.
[01:20:15.120 --> 01:20:21.280]   But this is just the first tranche of 14.
[01:20:21.280 --> 01:20:22.640]   Well, we'll come.
[01:20:22.640 --> 01:20:26.640]   Monde. You notice I use the financial term.
[01:20:26.640 --> 01:20:31.680]   Instead of saying I put everything on my United card to get miles.
[01:20:34.720 --> 01:20:41.120]   Everything. Finally, and this is actually tomorrow's change log.
[01:20:41.120 --> 01:20:47.040]   Stadia is tomorrow Google at 9 a.m. Pacific noon Eastern. We're going to stream it live.
[01:20:47.040 --> 01:20:51.680]   Google is going to announce Stadia pricing and launch games.
[01:20:51.680 --> 01:20:57.680]   So Stadia is that streaming gaming service. Google teased that the gamers developers conference.
[01:20:57.680 --> 01:21:01.120]   They're jumping the gun because E3 is next week and they're going to.
[01:21:02.240 --> 01:21:05.440]   Some news can't wait for E3. And that's it tomorrow.
[01:21:05.440 --> 01:21:12.240]   All the news on Stadia. Yes. First ever Stadia Connect launching phone game announcements price
[01:21:12.240 --> 01:21:19.520]   reveal. Watch Stadia Connect. June 6 on YouTube. Wow. A promo for an announcement.
[01:21:19.520 --> 01:21:27.600]   Anyway, we will stream that live until YouTube pulls us down. And that as far as I can tell
[01:21:28.560 --> 01:21:31.680]   unless I've left something important now is the Google change log.
[01:21:31.680 --> 01:21:43.520]   Symantec published an interesting blog post this morning about Twitter bots.
[01:21:43.520 --> 01:21:49.680]   They did an analysis of the bots created by the Russian Internet Research Agency.
[01:21:49.680 --> 01:21:56.720]   I thought very interesting. For instance, among their key findings, the operation was carefully
[01:21:56.720 --> 01:22:01.840]   planned accounts were often registered months before they're used well in advance of the election.
[01:22:01.840 --> 01:22:06.560]   The average time between account creation and first tweet. 177 days.
[01:22:06.560 --> 01:22:15.600]   The most terrifying stat was that the most retweeted account got over 6 million
[01:22:15.600 --> 01:22:22.560]   retweets. Very few of those from the other bots. Almost all of them from genuine Twitter user 6
[01:22:22.560 --> 01:22:29.280]   million. That shows you how successful the campaign was. Most accounts were primarily
[01:22:29.280 --> 01:22:34.560]   automated, but they would frequently show signs of manual intervention, such as posting original
[01:22:34.560 --> 01:22:40.480]   content or slightly changing the wording of reposted content, presumably an attempt to trick
[01:22:40.480 --> 01:22:46.080]   Twitter to make it look more authentic. Fake news accounts were set up to monitor blog activity
[01:22:46.080 --> 01:22:51.280]   and automatically push new blog post to Twitter auxiliary accounts. Configured to retweet content
[01:22:51.280 --> 01:22:57.360]   pushed out by the main accounts. What's interesting is that they didn't take a side in the election.
[01:22:57.360 --> 01:23:02.960]   The campaign directed propaganda at both sides of the liberal conservative political divide in the
[01:23:02.960 --> 01:23:07.920]   US, but almost always focused on the more disaffected elements of both camps.
[01:23:07.920 --> 01:23:20.080]   Bernie and Trump, I guess. There's a nice little infographic. Twitter bots. An anime of a propaganda
[01:23:20.080 --> 01:23:29.200]   campaign. 10 million tweets, 6.4 million followers following 3.2 million. A total of 3,836 main accounts.
[01:23:29.200 --> 01:23:33.280]   Is this any of your numbers, Jeff? I hope it's not. Nope. Nope. Sorry.
[01:23:33.280 --> 01:23:39.040]   But I come up with backups though. Yeah, good thinking. I have more backups than a Google network.
[01:23:39.040 --> 01:23:49.120]   Here is an example account with half a million retweets. Tranisha Cole. Tranisha's profile says
[01:23:49.120 --> 01:23:54.960]   "Love for all my people of melanin. Your black is beautiful." And then some things, some hashtags.
[01:23:54.960 --> 01:24:02.160]   I can't say. Here's one from Kenny Jackson. Follow the example set by Mrs. Obama,
[01:24:02.160 --> 01:24:06.400]   "Peace, love, and acceptance and vigilance. Impeech 45 resists gun reform now."
[01:24:06.400 --> 01:24:14.320]   Then there's also 10 GOP unofficial Twitter of Tennessee Republicans. 6 million retweets on
[01:24:14.320 --> 01:24:18.560]   this one. This is the one covering breaking news, national politics, foreign policy, and more
[01:24:18.560 --> 01:24:25.440]   MAGA 2A. What is 2A? What's hashtag 2A? Is that something I don't know? Second amendment.
[01:24:25.440 --> 01:24:36.160]   Got it. Thank you. So really an interesting, I think very clever in many ways, a campaign
[01:24:36.160 --> 01:24:45.680]   promoting fake news, promoting division. That's a cementech study.
[01:24:45.680 --> 01:24:53.520]   The US, they'll be easier for cementech to do their job because the US is now
[01:24:53.520 --> 01:24:59.360]   asking anybody looking for a visa to enter the United States for every social media account
[01:24:59.360 --> 01:25:03.360]   they've ever had. This is disgusting. This is just awful.
[01:25:04.400 --> 01:25:09.120]   The administration promised this a couple of years ago that's now on the form.
[01:25:09.120 --> 01:25:15.280]   14 million visitors to the US each year will be affected by the changes. It's not just immigrants.
[01:25:15.280 --> 01:25:20.960]   710,000 prospective immigrants, but anybody visiting the US from a country that requires a visa,
[01:25:20.960 --> 01:25:27.440]   which is, I think most of them, will have to fill out this form. They want to know your social
[01:25:27.440 --> 01:25:34.800]   media username, your social media accounts. They give you a list of all the social medias
[01:25:34.800 --> 01:25:39.520]   that they know about. If you have any others, they ask for those too. If there's anything we
[01:25:39.520 --> 01:25:46.480]   left out, please let us know. What's terrible is a lot of countries will reciprocate.
[01:25:46.480 --> 01:25:54.720]   I'll never get an email. That is inconvenient to me personally. What is also, we're supposed to
[01:25:54.720 --> 01:25:59.280]   be this land for free speech and blah, blah, blah. This is a very sensorious way to like.
[01:25:59.280 --> 01:26:05.280]   But do you think really that CPB is going to look at all of the social media accounts for 14 million
[01:26:05.280 --> 01:26:11.040]   people? You will censor by vulnerability. If you are a white guy in the UK, you're not going to
[01:26:11.040 --> 01:26:16.960]   feel bad about anything. If you're a brown person from a part of the world that we've declared evil,
[01:26:16.960 --> 01:26:20.800]   you're going to be like, "I once hated that I didn't like apple pie." That's...
[01:26:21.920 --> 01:26:30.800]   Or it's not hard to get the fire hose and just store away people who were critical of a president.
[01:26:30.800 --> 01:26:37.120]   Yeah, you can also do it retroactively once someone, you stop them. We have the tools at our
[01:26:37.120 --> 01:26:42.560]   disposal. If someone stopped for questioning, for example, someone could on a little tablet,
[01:26:42.560 --> 01:26:48.320]   just pull up all of their social media and be like, "Oh." And then you could treat them worse.
[01:26:49.200 --> 01:26:53.920]   It's not a good look. You might want to prepare a little list for yourself of all your social
[01:26:53.920 --> 01:27:00.480]   media accounts, all your handles, your passwords, anything that... There goes my global services.
[01:27:00.480 --> 01:27:08.320]   Well, I have to wonder. I haven't seen stats lately, but I wonder how tourism is impacted
[01:27:08.320 --> 01:27:13.760]   by this in the United States. It was down a lot. Our kids coming to study college abroad here are
[01:27:13.760 --> 01:27:20.000]   down. Are they? Yeah. Oh, yeah. Well, at the Newmark Graduate School of Journalism,
[01:27:20.000 --> 01:27:25.840]   our international students now cannot write for any publications in the US, unless we own them at
[01:27:25.840 --> 01:27:31.040]   the school. Oh, wow. Why is that? They can't get clips because there's not a lot of work in any
[01:27:31.040 --> 01:27:40.320]   definition. I'm going to include writing a piece for ProPublica. If unemployment really is down a
[01:27:40.320 --> 01:27:49.840]   few percent. Why not let them work? I've been seeing on Reddit a lot of posts on the
[01:27:49.840 --> 01:27:54.800]   cruise subreddit about people whose trips to Cuba have been summarily canceled because, of course,
[01:27:54.800 --> 01:28:01.120]   the United States is now prohibiting virtually all visitors to Cuba, including people who were
[01:28:01.120 --> 01:28:06.560]   previously allowed to as part of tour groups, cruises that were scheduled are being canceled and
[01:28:06.560 --> 01:28:13.120]   rerouted to Mexico and other places. I'm so pissed if I wanted to go to Cuba and I
[01:28:13.120 --> 01:28:17.200]   had to go to Mexico. I had a trip to Cuba planned for last year, and I was so worried after Trump's
[01:28:17.200 --> 01:28:22.320]   election. Now I feel like it was a mistake. I canceled it because I figured, well, by the end,
[01:28:22.320 --> 01:28:27.040]   you know, nine months from now, when I'm supposed to go on that trip, I'm sure this regulation will
[01:28:27.040 --> 01:28:33.280]   have passed because they promised it. They didn't. I could have gone. I wish I had now because
[01:28:33.280 --> 01:28:40.720]   who knows when we'll be able to go again. But you know, they are lawless commies. Let's not forget.
[01:28:40.720 --> 01:28:48.080]   Godless. Godless. They have laws. They don't have God. I'm sorry. I always forget which that is.
[01:28:48.080 --> 01:28:54.560]   Wait, I'm sorry. Was that Russia? No. Russia is godless. I mean, the Soviet Union.
[01:28:54.560 --> 01:29:02.480]   Actually, we like them now. Yeah, they're okay. It's good. So this is why it's so confusing.
[01:29:02.480 --> 01:29:06.640]   Excuse them. You can go to Russia, but you can't go to Cuba. Okay, just getting that straight.
[01:29:06.640 --> 01:29:12.720]   Apple has announced on Tuesday a single sign on to compete with Google Twitter and Facebook.
[01:29:12.720 --> 01:29:20.720]   Sign on with Apple. The button will be, if you do see it above the others, that's part of Apple's
[01:29:20.720 --> 01:29:26.000]   requirements. They're also apparently going to require any anybody in the app store that has
[01:29:26.000 --> 01:29:31.920]   a Google Twitter Facebook sign on to add the Apple sign on above all the rest. Now, if you're
[01:29:31.920 --> 01:29:36.720]   a privacy advocate, you may like it because one of the features of Apple's single sign on is they'll
[01:29:36.720 --> 01:29:42.400]   allow you to obfuscate your email address. So, and this is one reason I think a lot of people
[01:29:42.400 --> 01:29:47.280]   would who are using Facebook, Google or Twitter logins right now might not want to use the Apple
[01:29:47.280 --> 01:29:53.360]   login. They want your email address. Instead, they'll get a special address that Apple will then forward
[01:29:53.360 --> 01:29:59.120]   to you. Of course, Apple will have the opportunity to see all the email from all of these companies.
[01:29:59.120 --> 01:30:04.160]   But we trust Apple, so it's okay. Because they're privacy focused. They're
[01:30:04.160 --> 01:30:08.800]   probably they're doubling down on privacy. Actually, I don't use those sign on buttons
[01:30:08.800 --> 01:30:15.760]   because after I quit Facebook, I realized I just severed my connection to hundreds of apps and websites.
[01:30:15.760 --> 01:30:21.680]   I don't use them because I have a password manager and that's just just like, that's what I do.
[01:30:21.680 --> 01:30:25.520]   That's the better way to do it. Those companies, I mean, they probably do know
[01:30:25.520 --> 01:30:30.240]   thanks to like tracking pixels, but they don't need to know where I am all the time on the web.
[01:30:30.240 --> 01:30:34.080]   They're not offering me a service for that. Actually, that's why I use the Brave browser
[01:30:34.080 --> 01:30:39.840]   because it lets you turn off those tracking pixels in the settings. So, I turn off LinkedIn,
[01:30:39.840 --> 01:30:45.280]   Google, Facebook and Twitter. And so, those bugs no longer work. They just block that
[01:30:45.280 --> 01:30:50.960]   JavaScript, which I think is great. Or maybe they block the IP address. Brave is a pretty cool
[01:30:50.960 --> 01:30:55.200]   browser. It's one of my favorites. Let's take a break speaking of password managers and come back
[01:30:55.200 --> 01:30:58.320]   with your picks and number. And then we'll... Can I ask a question first?
[01:30:58.320 --> 01:31:06.800]   Yes. What makes a display stand worth $1,000? Can you explain that to me, Spark people?
[01:31:06.800 --> 01:31:10.640]   Yeah. Well, if Apple can... Johnny Ives and his pronunciation of aluminium.
[01:31:10.640 --> 01:31:20.480]   If you're buying one of these pro display XDRs for $6,000, buying a $1,000 stand makes sense.
[01:31:21.840 --> 01:31:27.760]   Because you have too much money. Well, okay. Probably you could get a stand with similar
[01:31:27.760 --> 01:31:33.360]   functionality from another party. And by the way, since this, you can also buy a visa mount instead
[01:31:33.360 --> 01:31:38.960]   of the stand. It doesn't come with anything. I know it's just a giant display. It's a display.
[01:31:38.960 --> 01:31:45.520]   Stand is optional. So, you could buy somebody else's stand. I'm sure there'll be a brisk market in,
[01:31:45.520 --> 01:31:52.320]   you know, $100 stands. But the Apple stand rotates and has an arm that gives you angles and has some
[01:31:52.320 --> 01:32:00.080]   features. And honestly, anybody who's buying this is working in graphics, photography, video.
[01:32:00.080 --> 01:32:06.080]   They're... In the arts where design is important. Design is important, but also
[01:32:06.080 --> 01:32:12.880]   your clients are paying for this. This is part of your business. And it's the same reason the Mac
[01:32:12.880 --> 01:32:18.240]   Pro starts at $6,000. And probably that's a very minimal configuration. Any reasonable
[01:32:18.240 --> 01:32:23.760]   configuration will be more like $10,000. So what are you worried about a thousand bucks for a stand?
[01:32:23.760 --> 01:32:27.520]   That's still 10% of the price of it. I mean, that's not insignificant.
[01:32:27.520 --> 01:32:32.640]   Just charge it back to the client and you add 15% and you're making money. It's a profit thing.
[01:32:32.640 --> 01:32:37.920]   It does have that cool magnet, I will say. And having played with magnets and like my video
[01:32:37.920 --> 01:32:43.840]   cameras, that's satisfying. There's a lot of design in that stand. That's cool. You're right.
[01:32:43.840 --> 01:32:49.600]   It probably costs Apple a hundred bucks to make. They got a lot of attention for it. They probably,
[01:32:49.600 --> 01:32:54.880]   you know, hindsight shouldn't have mentioned the price of the stand. I thought it was surprisingly
[01:32:54.880 --> 01:33:01.680]   fourth right of them to mention that. I don't honestly think that somebody buying a $6,000 monitor
[01:33:01.680 --> 01:33:06.560]   because of the features the monitor is thinking about the stand. These monitors are... That's true.
[01:33:06.560 --> 01:33:09.840]   I mean, I would never buy that monitor. No, it's not for you and me.
[01:33:09.840 --> 01:33:16.640]   It's for professionals and professionals really honestly aren't. If you're recording
[01:33:16.640 --> 01:33:22.880]   the next... Who's a big star? Miley Cyrus album.
[01:33:22.880 --> 01:33:31.360]   The next... If you're recording the next... I don't know. Whoever, big star album, the Jonas
[01:33:31.360 --> 01:33:38.160]   Brothers album, I see I'm so out of it. You really agree to me. You're in a big recording studio.
[01:33:38.160 --> 01:33:42.080]   You're buying this expensive gear. The stand is the least of your worries.
[01:33:42.080 --> 01:33:45.600]   And you just charge it back to Miley anyway.
[01:33:45.600 --> 01:33:52.080]   $5,000 for the Pro Display XDR, $6,000 for the Mac Pro.
[01:33:52.080 --> 01:34:00.400]   And that's base. That's the base model. The Mac Pro has 256 gig hard drive. It has the
[01:34:00.400 --> 01:34:05.520]   tiniest little bitty, bitty hard drive. That is almost insulting. I'll be honest.
[01:34:05.520 --> 01:34:09.200]   Well, that's so... You're not going to buy that. Nobody's going to buy that.
[01:34:09.200 --> 01:34:15.840]   You don't. Eight cores. What do you mean eight cores? I need 16. I need 24. I need 32 cores.
[01:34:15.840 --> 01:34:20.240]   That's like my early Tesla that I got. They were like... And for a couple of
[01:34:20.240 --> 01:34:23.760]   holders, we're going to charge you. Oh, Tesla totally does that. Oh my god.
[01:34:26.000 --> 01:34:31.280]   But if you're a recording studio and the Everly Brothers are coming in to record that big hit song,
[01:34:31.280 --> 01:34:37.680]   I'm sorry, Jonas Brothers, not Everly Brothers.
[01:34:37.680 --> 01:34:44.640]   I'm sure today brought to you by LastPass. You have a password vault. You have a password manager.
[01:34:44.640 --> 01:34:49.600]   You should have a password. If you don't have a password manager, you need LastPass. The number
[01:34:49.600 --> 01:34:54.640]   one most preferred password manager. It's the one I use the one Steve Gibson recommends and uses.
[01:34:55.280 --> 01:35:02.640]   It is strong encryption. Your passwords are only decrypted on your devices. LastPass doesn't
[01:35:02.640 --> 01:35:10.720]   have access to them. Nobody does, but you... As a password generator in it, it has a security check.
[01:35:10.720 --> 01:35:15.680]   By the way, that's the first thing I did when I got home this week after Twitter. I ran it because
[01:35:15.680 --> 01:35:21.040]   I'd heard about another, yet another breach. More passwords released into the wild. I thought
[01:35:21.040 --> 01:35:26.000]   I'd better run through my security checker on LastPass. It'll check all your passwords, all your email
[01:35:26.000 --> 01:35:32.000]   addresses. If any have been breached, it will offer to change them automatically. I read it about 100
[01:35:32.000 --> 01:35:39.600]   passwords in... I don't know, five minutes. It was done. It just happened with strong passwords.
[01:35:39.600 --> 01:35:43.120]   And of course, LastPass doesn't have to show you the password because it remembers it. It fills it in.
[01:35:43.120 --> 01:35:49.200]   Now, the best thing about LastPass is LastPass for business. 43,000 businesses use LastPass,
[01:35:49.200 --> 01:35:56.560]   including to it. We use LastPass Enterprise because your employees are the weakest link here.
[01:35:56.560 --> 01:36:00.640]   We know employees share passwords, not just with other employees, but with a real outside world.
[01:36:00.640 --> 01:36:06.720]   And you're giving them the keys to the kingdom. Our business department has all our bank accounts,
[01:36:06.720 --> 01:36:13.360]   all our books, the engineers have access to our websites and our databases.
[01:36:13.360 --> 01:36:18.560]   It's really nice to know that it's all stored in LastPass. You can share it with them without
[01:36:18.560 --> 01:36:22.240]   them even seeing it. If you change the password, it's automatically changed for them.
[01:36:22.240 --> 01:36:30.320]   You can set over 100 customizable policies to give you more flexibility and control. For instance,
[01:36:30.320 --> 01:36:35.600]   we have certain standards for the master password to help protect it. We require second-factor
[01:36:35.600 --> 01:36:41.280]   authentication. LastPass supports all of them, including Yubike but also Duo, Security, all the
[01:36:41.280 --> 01:36:48.000]   authenticators. As an admin, you'll have access to customized reporting, so you can track changes
[01:36:48.000 --> 01:36:52.240]   over time. You can reset passwords. You can see how people are using passwords. You can see if
[01:36:52.240 --> 01:36:58.720]   they're using bad passwords. You have full control. Seamless background sync, offline access. And by
[01:36:58.720 --> 01:37:02.960]   the way, LastPass works everywhere you do. It's the first app I install when I get a new phone or a
[01:37:02.960 --> 01:37:11.200]   new computer. Windows, Mac, Linux, Android, iOS, every browser I love LastPass. You will too.
[01:37:11.200 --> 01:37:14.720]   Join 13 and a half million people who've signed up for LastPass. They're loving it and trusting
[01:37:14.720 --> 01:37:20.400]   it. I trust it. LastPass premium for individuals. LastPass for teams, for small groups,
[01:37:20.400 --> 01:37:25.120]   of 50 or less. LastPass families. That's what you use at home because it has easy
[01:37:25.120 --> 01:37:30.000]   sharing plus that emergency access feature, which is great. It says, "I can designate a
[01:37:30.000 --> 01:37:35.920]   survivor who has access to my stuff if something should happen to me." And LastPass Enterprise,
[01:37:35.920 --> 01:37:41.680]   which is what we use here at Twit. In fact, we like it so much. We give LastPass to every
[01:37:41.680 --> 01:37:48.160]   employee as a benefit of employee. Have you signed up for LastPass, Jeff? Yes. Our newest
[01:37:48.160 --> 01:37:56.080]   engineer. LastPass.com/twit. LastPass.com/twit. We thank LastPass for their support. LastPass is
[01:37:56.080 --> 01:38:03.200]   easy, secure, reliable. Don't leave home without it, as they say. I wouldn't dream of it. Thank
[01:38:03.200 --> 01:38:11.040]   you, LastPass, for your support of all our shows. Stacy, you said you have something exciting.
[01:38:11.040 --> 01:38:18.960]   Is it an unboxing? It is. Tell us, tell us. This is the Luchan Aurora. Are you
[01:38:18.960 --> 01:38:25.360]   guys familiar with this? What is it? Okay. This is a device. Okay. There's a couple caveats,
[01:38:25.360 --> 01:38:29.440]   but the idea is this works with Philips Hue light bulbs. Okay. And you know how you turn
[01:38:29.440 --> 01:38:34.880]   your light switch off and your Philips Hue light bulbs stop working? Yes. This prevents you from
[01:38:34.880 --> 01:38:40.160]   turning off your light switch and making your Philips Hue bulbs stop working. You could keep
[01:38:40.160 --> 01:38:45.600]   them. They're always powered. They're always on. And I need to. So badly. Everyone can use it.
[01:38:45.600 --> 01:38:53.200]   And it's like scotch tape on the on the switch. Yeah. Kind of. It's more aesthetically pleasing
[01:38:53.200 --> 01:38:57.600]   and it has a dimmer function. Well, they have they have all the different kinds of switches. So,
[01:38:57.600 --> 01:39:03.200]   you have a dimmer or regular switch, right? Well, so this one only works on toggle switches,
[01:39:03.200 --> 01:39:08.640]   but it will turn to a dimmer. Oh, I see. So, you, this is the problem is people come into my office
[01:39:08.640 --> 01:39:14.800]   where I have a motion sensor and hue lights. And I want it so that when I go in my office,
[01:39:14.800 --> 01:39:19.120]   my lights come on. But people inevitably, I know today somebody will go to my office and
[01:39:19.120 --> 01:39:22.960]   switch it off because they say, well, the lights are on and they flip it up and then the nothing
[01:39:22.960 --> 01:39:29.360]   happens. So this will prevent that. Yes. It is kind of expensive. In my opinion, it is $40.
[01:39:29.360 --> 01:39:36.640]   All right. So here's here's the two pieces you need. This is all you get. And you have to
[01:39:36.640 --> 01:39:41.920]   flip bulbs and hubs. This has a Zigbee radio on it. Oh, three dot, oh, they're going to work
[01:39:41.920 --> 01:39:46.320]   with other light bulbs eventually, but they've got a test compatibility. So it still uses your
[01:39:46.320 --> 01:39:51.360]   existing hardware on the wall. It just replaces. This is something anyone can do. You don't have
[01:39:51.360 --> 01:39:56.960]   to worry about electricity. Does it glue on top of it? No. Hold on. Hold on. Hold on. Hold on.
[01:39:56.960 --> 01:40:00.960]   Hold your horses. Look forward. You turn your light switch on, your toggle switch on. Yeah.
[01:40:01.600 --> 01:40:07.600]   Okay. So this is your finger is your switch. Okay. You put this thing. Yeah. Hold on. Yeah.
[01:40:07.600 --> 01:40:11.920]   Over it like this. Yeah. Yeah. And it keeps it on. And it keeps it on. And then you.
[01:40:11.920 --> 01:40:18.320]   What's the knob do? Okay. Snap it to it. Yeah. And now I have a dimmer function. So all that's
[01:40:18.320 --> 01:40:23.920]   happening is this is this great thing is holding your life switch on. Yeah. And this is basically
[01:40:23.920 --> 01:40:29.280]   so free telling this Zigbee stuff to be like, boop, boop, work. Don't work. So it's see.
[01:40:29.280 --> 01:40:35.760]   Hold on. And tell you. Fortunately, I don't have those old-fashioned toggle switches. I have the
[01:40:35.760 --> 01:40:40.240]   fat ones. You know, they're. This is me, the rocker switches. They're not making it for the
[01:40:40.240 --> 01:40:44.000]   rocker switches. So your options are as follows. I just get a different switch.
[01:40:44.000 --> 01:40:50.640]   Get a different switch. But when you change out the switches, you actually do have to change
[01:40:50.640 --> 01:40:56.000]   out the electricity. So then you can just turn on the light with the switch. No, it's not turning
[01:40:56.000 --> 01:41:01.200]   it on. It's the problem. It's the turning it off. People turn them off. No, no, smart lights are
[01:41:01.200 --> 01:41:07.360]   actually super useful, Jeff, for a variety of reasons. Number one, you can be in bed and you say,
[01:41:07.360 --> 01:41:13.760]   "Madam, hey, turn off the lights." Number two, you can set a really cool like modes, like movie
[01:41:13.760 --> 01:41:20.080]   time, right? Number three, when you leave your house, most security systems now will work with
[01:41:20.080 --> 01:41:24.880]   your lights to make them look like they're on and off and kind of a randomized or the same pattern
[01:41:24.880 --> 01:41:27.920]   that you use. So then it becomes a security feature.
[01:41:27.920 --> 01:41:34.400]   You know, I go into the restroom these days, right? And turn on the lights.
[01:41:34.400 --> 01:41:39.280]   Where is this? No, it's not that. I can't turn on the soap or the water. Oh, none of that works.
[01:41:39.280 --> 01:41:43.360]   Because you're, you know why, Jeff, you're so pale, you're invisible. Well, it actually,
[01:41:43.360 --> 01:41:46.720]   usually happens the other way where people of color can't turn the wall. Is that true?
[01:41:46.720 --> 01:41:49.120]   Because the sister doesn't know that, but it's also true in the other end of the.
[01:41:49.120 --> 01:41:53.920]   Oh, I didn't know that. Me. Are you kidding? Those automatic faucets are racist?
[01:41:54.800 --> 01:41:58.000]   Yes. Oh my God. Because they only tested them against white people.
[01:41:58.000 --> 01:42:01.520]   Anyway. Oh my God, that's horrible.
[01:42:01.520 --> 01:42:08.800]   That's just like, this is, this is the persistent, unconscious subterranean racism
[01:42:08.800 --> 01:42:14.720]   that pervades our lives. Yes. Well, so what do people of color do? They go in an airport
[01:42:14.720 --> 01:42:19.040]   restroom. They can't wash their hands. Yeah, because you get to get the water journal.
[01:42:19.040 --> 01:42:24.320]   Water won't go on. It'll go like this, underneath them. So I'll just say, we get this rule where
[01:42:24.320 --> 01:42:30.640]   you can't, you don't touch anything, right? You don't touch doorknobs, paper towels, soap, water,
[01:42:30.640 --> 01:42:34.560]   it all. It's not. Yeah. Maybe we should still put it in my house.
[01:42:34.560 --> 01:42:42.080]   And my wife hates it, but I don't. So if my hands are dirty, I don't want to go
[01:42:42.080 --> 01:42:48.400]   pump the soap dispenser, pick up a bar of soap. So I put automatic soap dispensers in my, in the house.
[01:42:48.960 --> 01:42:54.240]   So you put your hand on her, it's quartz soap in there. And it's good. But I didn't realize maybe
[01:42:54.240 --> 01:42:59.520]   it's racist. Fortunately, I don't know any black people. So it's okay.
[01:42:59.520 --> 01:43:12.880]   I'm joking. I'm joking. Some of my best friends are people of color. No, no, actually we do. And I
[01:43:12.880 --> 01:43:17.760]   haven't, but I haven't asked them, what do you do when you use the soap dispenser? I mean, I didn't
[01:43:17.760 --> 01:43:23.280]   even know this was an issue. I would hope that they're getting fixed, but yes, it's been an issue.
[01:43:23.280 --> 01:43:29.760]   Yeah. Well, I think mine is smarter. I really like it because often my hands are filthy.
[01:43:29.760 --> 01:43:34.000]   You know, like I, we're grilling and I put the charcoal in the grill. I don't want to have to
[01:43:34.000 --> 01:43:38.960]   get charcoal dust over everything. Do you have that at home? Yes. I love it.
[01:43:38.960 --> 01:43:43.280]   That's what he's saying. I love my touch faucet. I'm really sad that I don't have it. I'm like,
[01:43:43.280 --> 01:43:48.160]   I know. I wish I had one of those because I do have to do the knob on the faucet, but I can do
[01:43:48.160 --> 01:43:54.640]   that with the edge of my hand. Oh, no, no touches way better. All right. So that's my thing. That's a
[01:43:54.640 --> 01:43:58.880]   good thing. Very cool. It's a very cool thing. But you're right. You can spend a lot of money on
[01:43:58.880 --> 01:44:03.520]   it's $50 to switch out all of this stuff. Yeah, and all the money you spent on the
[01:44:03.520 --> 01:44:08.160]   cue bulbs. But if you've already spent the money on the cue bulbs, or if in my case,
[01:44:08.160 --> 01:44:12.640]   because I'm in a rental house, it actually would be useful, except I don't have the right switches.
[01:44:12.640 --> 01:44:19.040]   $25 for the secure premium touchless battery operated electric automatic soap dispenser with
[01:44:19.040 --> 01:44:24.960]   adjustable sub dispensing volume control dial. Is that what you have? Yeah. It's ugly. It's
[01:44:24.960 --> 01:44:30.080]   actually it's not ugly. It's a good thing. It's not bad. That actually is kind of cool. It works
[01:44:30.080 --> 01:44:34.880]   great. And then you can get this nice. Oh, I wish it were plugged in plug in a bowl. I don't like
[01:44:34.880 --> 01:44:38.880]   how long does they last forever? I've had it for a long time and I haven't had to change the
[01:44:38.880 --> 01:44:43.040]   battery. Maybe you don't wash your hands a lot. I wash. No, I wash my that's the other side of
[01:44:43.040 --> 01:44:49.200]   this. I wash my hands more because it's so easy. I wash my hands many, many times a day. Yeah,
[01:44:49.200 --> 01:44:53.200]   because I'm like, because you know, when I'm like cooking or dealing with raw meat, I hate,
[01:44:53.200 --> 01:44:58.240]   you know, I do it with exactly. I just put chicken in on the, you know, and I now have, and what am
[01:44:58.240 --> 01:45:05.520]   I going to do? I don't want to touch anything. Yeah. See what I'm saying? You see what I'm saying?
[01:45:05.520 --> 01:45:11.360]   Jeff, your name, give for Andrew. Andrew would love it. He would. He's very term conscious. Yeah,
[01:45:11.360 --> 01:45:16.480]   men like it. My wife won't. She says, I don't like it. I don't know why. It was that takes
[01:45:16.480 --> 01:45:19.520]   up counter space. That's a problem. Yeah. I put, you know what? The other thing is,
[01:45:19.520 --> 01:45:25.280]   I put it right over the sink. So if it drips or it's accidentally shoots itself. Right. It goes
[01:45:25.280 --> 01:45:32.080]   right in the sink. I'm liking this. All right. See, what's your not? All right. We're gonna start
[01:45:32.080 --> 01:45:39.760]   here. Yes. So Spencer tunic came to New York to have a protest on behalf of the nipple
[01:45:39.760 --> 01:45:45.040]   in front of Facebook. So dozens of people strip in front of Facebook's New York headquarters. That's
[01:45:45.040 --> 01:45:52.640]   point one. Oh my God. You're safe for work. Yeah, because we all have butts. Oh, this is that guy.
[01:45:52.640 --> 01:45:57.120]   Spencer does this, right? Yeah. He does this all the time. Did you go down there?
[01:45:57.120 --> 01:46:01.360]   No, I would be tempted to do it sometime if I've done the German sauna thing, you know, why not this?
[01:46:01.360 --> 01:46:05.680]   This is great. But I think my family would have a fit. If you go to the second link,
[01:46:05.680 --> 01:46:11.280]   there's one that actually shows what they were doing. Is this safe for? That is not safe for work.
[01:46:11.280 --> 01:46:16.400]   Really safe. It depends on how you feel about public nudity, I guess. It's from, it's from Fox.
[01:46:16.400 --> 01:46:22.480]   So they've actually blotted out any body parts. Yes. So they're holding up, they're putting
[01:46:22.480 --> 01:46:27.120]   nipples in front of their private parts big big pictures. Oh, that's funny,
[01:46:27.120 --> 01:46:33.440]   which is funny. Oh, that's actually pretty funny. It's pretty funny. That's one. That is a double
[01:46:33.440 --> 01:46:38.480]   standard. And Sarah Silverman was talking about that. She got, she put a picture up
[01:46:38.480 --> 01:46:46.080]   and her breasts were in it and it was immediately taken down. So she put up all these pictures
[01:46:46.080 --> 01:46:50.080]   of nipples. You can't tell if they're male or female to complain to Instagram. And I think
[01:46:50.080 --> 01:46:54.960]   she had an excellent point. Why is a male nipple? How is that? Exactly.
[01:46:54.960 --> 01:47:02.080]   Difference, it is sexist. Yeah. So then, but Facebook can solve this because they might get a
[01:47:02.080 --> 01:47:07.200]   gigantic new New York headquarters. Where's it going to be? Where's it going to be?
[01:47:07.200 --> 01:47:14.080]   In the spot of the old Hotel Pennsylvania, which has kind of fallen down around itself.
[01:47:14.080 --> 01:47:21.520]   But a huge building, 1500. It's cool. Like I, with plants all over. It looks like a jenga building.
[01:47:21.520 --> 01:47:27.280]   Barks in the, wow. Oh, we have one of those. We call it the Jenga building.
[01:47:27.280 --> 01:47:33.440]   The one building? It was Jenga. It looks like Jenga. Yeah. It looks like Jenga. Yeah.
[01:47:33.440 --> 01:47:38.480]   Yeah. It will. Is this all going to be Facebook, this building? No, it's but a huge part of it.
[01:47:38.480 --> 01:47:47.040]   Wow. Supposedly. It's. So there's that. And so you see there, there's a picture of people
[01:47:47.040 --> 01:47:52.800]   going down. There's a picture of like the park. Wow. Where do you put the naked people?
[01:47:52.800 --> 01:47:58.800]   That's exactly it. Right. So now it's safer. Now you, it's harder. It's right. Look at the
[01:47:58.800 --> 01:48:05.120]   park. It's not. That's the park. Yeah. So this is how it's same as their, as their,
[01:48:05.120 --> 01:48:08.080]   actually, this is really convenient. It's right up the street from the Yale club.
[01:48:08.080 --> 01:48:14.560]   Oh, yeah. I can run over. Isn't that beautiful? That's the old building that we
[01:48:14.560 --> 01:48:18.240]   built. That design. Now to find. That one.
[01:48:18.240 --> 01:48:22.560]   Is it going to be one of those super, super high ones that people are upset about?
[01:48:22.560 --> 01:48:25.600]   Because it says they're going to have a bunch of floors with no thinning in them.
[01:48:25.600 --> 01:48:31.200]   Yeah. It's really. Oh, these super skinny, tall skyscrapers that are popping up. Oh, yeah.
[01:48:31.200 --> 01:48:35.120]   Super tall. Yeah. Yeah. I don't like them. What do you think, Jeff? You're the one that lives there.
[01:48:35.120 --> 01:48:40.320]   Well, I don't like. So I have to occasionally go work at World Trade Center. Yeah.
[01:48:40.320 --> 01:48:45.120]   And you don't like going up there? I know. No, I bet that's scary for you. Yeah.
[01:48:45.120 --> 01:48:49.520]   Is it sway in the wind? Yeah, that's sort of my imagination.
[01:48:49.520 --> 01:48:56.240]   Since we were talking about racism, let's talk about baritone. I want to recommend
[01:48:56.240 --> 01:49:02.400]   partially. Yeah. Baritone Day Thirstons Ted talk. Yes. Yes. If you haven't watched it, do.
[01:49:02.400 --> 01:49:08.320]   Ted 2019. Have you seen it yet? He sent me the text and I have not, but I love.
[01:49:08.320 --> 01:49:15.360]   Really? I love Baritone Day. And I will watch it. Absolutely. He's the greatest guy. And I bet it's
[01:49:15.360 --> 01:49:20.880]   a great talk. And it's about, yeah, it's about living while black. Yeah, he wrote the book.
[01:49:20.880 --> 01:49:26.720]   How to be black. Right. And so he has a great line in all rural and what's
[01:49:26.720 --> 01:49:32.720]   spoiler one line. He says, and if you haven't bought the book yet, you're a racist. I immediately
[01:49:32.720 --> 01:49:44.480]   bought the book. You know what? I have a number. Well, you do. I do. 10,000.
[01:49:44.480 --> 01:49:50.320]   That's how many steps you're supposed to take every day, right? Do you know where that number
[01:49:50.320 --> 01:49:56.080]   comes from? Japan. You know, don't you, Stacy? Did you read this article? You know that before?
[01:49:56.080 --> 01:50:00.720]   I did, but we knew it before. This is like the third time in a row that this has popped up.
[01:50:00.720 --> 01:50:04.640]   But that's okay. Keep going everywhere when you get one of these smart watches or, you know,
[01:50:04.640 --> 01:50:09.440]   who's Google fit on your phone. It says, Hey, 10,000 steps. That's the secret to health. That
[01:50:09.440 --> 01:50:15.040]   in eight glasses of water a day and eight hours of sleep. And don't forget breakfast is the most
[01:50:15.040 --> 01:50:21.120]   important meal of the day. Wrong, wrong, wrong. The reason for 10,000 steps, actually,
[01:50:21.120 --> 01:50:27.280]   it's a fascinating story. Professor of epidemiology at Harvard's T.H. Chan School of Public Health,
[01:50:27.280 --> 01:50:35.600]   I'm in Lee, wondered where did 10,000 come from? It turns out the basis for this 10,000 step
[01:50:35.600 --> 01:50:42.880]   guideline was a marketing strategy in 1965. A Japanese company was selling pedometers and they
[01:50:42.880 --> 01:50:50.000]   gave it a name that in Japanese means the 10,000 step meter. She believes that name was chosen
[01:50:50.000 --> 01:50:55.760]   because the character for 10,000 in Japanese looks kind of like a man walking. She says,
[01:50:55.760 --> 01:51:00.080]   as far as she knows, the actual health merits of that number have never been validated. In fact,
[01:51:00.080 --> 01:51:09.920]   she quotes a study of 16,000 elderly women that found that really 40, 400 steps a day makes a
[01:51:09.920 --> 01:51:15.840]   huge difference in your mortality rate. It continues to drop to up to about 7,500 steps at which
[01:51:16.400 --> 01:51:22.240]   rate nothing more matters. Now, that's elderly women. And of course, one of the things that
[01:51:22.240 --> 01:51:29.120]   found even as little as 10,000 steps, a mile of walking is associated with health, healthy outcomes.
[01:51:29.120 --> 01:51:33.840]   So it's unknown how many steps is the perfect number, but it sure is not 10,000.
[01:51:33.840 --> 01:51:41.360]   They did a study on UK or maybe it was Scottish postal workers on the number of steps they took
[01:51:41.360 --> 01:51:48.080]   and how that correlated with their health outcomes. So if you look up that study, it might offer you
[01:51:48.080 --> 01:51:52.560]   better data. I think we know that it's a good idea to get exercise every day.
[01:51:52.560 --> 01:51:58.960]   You should walk. Just walking is great exercise. But you don't need to do 10,000.
[01:51:58.960 --> 01:52:03.920]   Well, because that's a story from 2017 asking whether 15,000 steps
[01:52:03.920 --> 01:52:10.480]   well, 10,000. Why wouldn't 15 be better? This is the problem with all media coverage of science.
[01:52:10.560 --> 01:52:16.640]   Yeah. Yeah. Like here's the latest word and that that erases all other previous science. It's just
[01:52:16.640 --> 01:52:22.720]   the exact opposite of how science operates. So I apologize for repeating something you already
[01:52:22.720 --> 01:52:26.960]   knew Stacy, but I thought that was Stacy knows everything kind of what I open it.
[01:52:26.960 --> 01:52:34.080]   It happens a lot. I'm just using it. I was I was referring less to you. It's tough when you're a
[01:52:34.080 --> 01:52:38.880]   smart person, isn't it? And all the people around you are stupid. I just read a lot.
[01:52:38.960 --> 01:52:40.480]   [laughter]
[01:52:40.480 --> 01:52:45.360]   And I'm obsessed with my my my Fitbit. I got the Charge 3. I like it.
[01:52:45.360 --> 01:52:50.880]   Yeah, you showed us. That's really nice. Yeah. So it doesn't change fundamentally from the
[01:52:50.880 --> 01:52:52.640]   charge. I got a little surprise the other day.
[01:52:52.640 --> 01:53:00.000]   Back in the early 2000s, I think I got one of those why things scales.
[01:53:00.000 --> 01:53:05.360]   And I set it up. I don't know how maybe with this and that I have to do some investigation
[01:53:05.360 --> 01:53:10.720]   that it would tweet my way. Or do that. Yeah. Remember that? And it did it for a long time.
[01:53:10.720 --> 01:53:13.920]   And then I hadn't done it since 2015. It hasn't done in five years.
[01:53:13.920 --> 01:53:18.640]   I got another why things scale and I got a why things blood pressure monitor.
[01:53:18.640 --> 01:53:26.080]   It started tweeting immediately. So it was like somebody somebody told me,
[01:53:26.080 --> 01:53:30.640]   oh, I saw you. Congratulations on your blood pressure. I said, what? I said,
[01:53:30.640 --> 01:53:33.680]   how did you know? She said you tweeted it. So no, I didn't.
[01:53:34.480 --> 01:53:38.880]   Yeah. So that's one of the steps when I talk about decommissioning device.
[01:53:38.880 --> 01:53:42.080]   This thing's tweeting again. It's like from the dead.
[01:53:42.080 --> 01:53:46.640]   Yeah. That's that's one of those decommissioning device steps.
[01:53:46.640 --> 01:53:50.480]   And that's why you do it. Well, it's funny because I did decommission the device,
[01:53:50.480 --> 01:53:54.160]   but what I didn't I and I to this day don't know. I don't know where I set up a script,
[01:53:54.160 --> 01:53:58.160]   but it must be somewhere. I have to find this probably if this than that.
[01:53:59.200 --> 01:54:05.840]   Yeah. That's so Leo's underscore scale on Twitter. I may not have a Twitter cap, but my scale does.
[01:54:05.840 --> 01:54:15.120]   And it's it's active again. What a weird thing. It's like, oh, yeah, it's like a it's like a ghost.
[01:54:15.120 --> 01:54:20.560]   How's the diet, Leo? It's great. I've been doing. I've lost
[01:54:20.560 --> 01:54:27.040]   about a pound a week for the last four months, something like that 15, 14 pounds.
[01:54:28.080 --> 01:54:32.480]   And continue to do so. Yeah, I'm doing it's a startup at a Silicon Valley.
[01:54:32.480 --> 01:54:42.720]   The guy who became the chief physician spokesperson for ketogenic diets, Stephen Finney,
[01:54:42.720 --> 01:54:49.440]   did a startup called Verta Health. And you have to be prediabetic or type two diabetes or to do it.
[01:54:49.440 --> 01:54:56.960]   And so I qualified. So I signed up for about three months ago, four months ago, and it's been great.
[01:54:56.960 --> 01:55:03.680]   It's worked very well. And I'm very happy. But now you could find out by following my scale. Ladies
[01:55:03.680 --> 01:55:08.080]   and gentlemen, the smartest person in the room is about to leave that Stacey Higginbotham.
[01:55:08.080 --> 01:55:15.440]   She is she's in her new home and beautiful Seattle. Stacey already knows it. And she knows it.
[01:55:15.440 --> 01:55:21.680]   We appreciate it. Stacey, her podcast, Stacey, the IoT podcast with Kevin Tofel is available at
[01:55:21.680 --> 01:55:28.160]   Stacey.iot.com. Subscribe to her free newsletter too. It's great. It's a great read. We love it.
[01:55:28.160 --> 01:55:35.040]   Thank you. And are there new opportunities for you in Seattle or you just felt like relocating?
[01:55:35.040 --> 01:55:40.960]   I was like, there's opportunity to just go hiking and kayaking and all kinds of fun things.
[01:55:40.960 --> 01:55:49.200]   Yeah, Lisa and I want to move to a low or no income tax state. So washes a business tax.
[01:55:50.400 --> 01:55:53.760]   Yeah, I know. But if you don't have your business, you're okay. Well, it's when we retire.
[01:55:53.760 --> 01:56:00.080]   Okay. Yeah. So we are actually planning a trip to Seattle to scope out the scenery.
[01:56:00.080 --> 01:56:05.920]   Okay. Well, let me know when you're here. I'll let you know so you can avoid me so you can be out of
[01:56:05.920 --> 01:56:13.360]   town. Yes, that is it. So you can buy me some of that great Seattle queso they're so famous for.
[01:56:13.360 --> 01:56:18.160]   We'll come up with a little better. Jeff Jarvis, he is the director of the Town
[01:56:18.160 --> 01:56:22.080]   Night Center for entrepreneurial journalism at the Craig Newmarks graduate school of
[01:56:22.080 --> 01:56:28.800]   journalism at the city of New York, blogger at buzz machine.com and a great author prolific author
[01:56:28.800 --> 01:56:32.960]   of many books, including public parts. I wish public parts. When's the new one coming out?
[01:56:32.960 --> 01:56:35.280]   Oh, when's it when's it? Are you working on it?
[01:56:35.280 --> 01:56:40.640]   On it. Yeah, but I just don't know time. It's many projects. That's my excuse.
[01:56:41.600 --> 01:56:49.040]   It, you know, whatever happened to publish or perish, Jeff? I know. I'm perishing slowly.
[01:56:49.040 --> 01:56:54.160]   No, you're doing pretty good for a former TV guide critic. I just want to say
[01:56:54.160 --> 01:57:02.160]   really amazing, really. Thank you guys. It's always a pleasure. I love it when it's always the
[01:57:02.160 --> 01:57:09.040]   core group. You guys are fantastic. We do this show on Wednesdays round about 130 Pacific 430
[01:57:09.040 --> 01:57:15.680]   Eastern 2030 UTC. You can watch or listen live at twit.tv/live. If you want to download copies of
[01:57:15.680 --> 01:57:22.320]   the show for later consumption, audio and video lives on our website, twit.tv. Actually, for all
[01:57:22.320 --> 01:57:27.280]   of our shows, but this one is twit.tv/twig. You can also subscribe in your favorite podcast
[01:57:27.280 --> 01:57:32.080]   application. That way you'll get it automatically the minute it's available. I'm so glad you were
[01:57:32.080 --> 01:57:37.680]   here. There it is, twit.tv/twig. I'm so glad you were here and I hope you will return next week.
[01:57:37.680 --> 01:57:40.240]   We'll see you then on this week in Google.
[01:57:40.240 --> 01:57:51.040]   [Music]

