;FFMETADATA1
title=Full Throttle Democracy
artist=Jason Howell, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-04-13
track=711
language=English
genre=Podcast
comment=AI rules, Substack Notes, NFL pricing on YouTube TV, Smart display support
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100

[00:00:00.000 --> 00:00:04.120]   It's time for Twig this week in Google. I'm Jason Howell, filling in for Leo LaPorte.
[00:00:04.120 --> 00:00:08.680]   We got Aunt Pruitt, Jeff Jarvis, and birthday girl Stacey Higginbotham.
[00:00:08.680 --> 00:00:13.000]   We got a lot of news to talk about. This week's episode was full democracy.
[00:00:13.000 --> 00:00:16.880]   I basically turned it over to all of Jeff's recommendations in the doc.
[00:00:16.880 --> 00:00:24.240]   We have Google and it's lagging support for some of the older third-party smart devices
[00:00:24.240 --> 00:00:28.200]   and smart displays. That's going away. What does that mean?
[00:00:28.200 --> 00:00:32.960]   Also, we have pricing for the NFL Sunday ticket on YouTube TV.
[00:00:32.960 --> 00:00:39.320]   Let me just tell you, it's pretty pricey. What if ChatGPT created the next
[00:00:39.320 --> 00:00:42.120]   Choose Your Own Adventure for your kids? How would you feel about that?
[00:00:42.120 --> 00:00:46.760]   We dive into that a little bit. And then finally, Substack unveiled its notes
[00:00:46.760 --> 00:00:51.680]   service, kind of a competitor, also ran to Twitter. How do we feel about that?
[00:00:51.680 --> 00:00:55.920]   You're going to find out next on Twig.
[00:00:55.920 --> 00:00:58.720]   Podcasts you love.
[00:00:58.720 --> 00:01:01.120]   From people you trust.
[00:01:01.120 --> 00:01:04.120]   This is Twig.
[00:01:04.120 --> 00:01:15.440]   This is Twig. This week in Google, episode 711 recorded Wednesday, April 12, 2023.
[00:01:15.440 --> 00:01:18.480]   Full throttle democracy.
[00:01:18.480 --> 00:01:21.840]   This episode of This Week in Google is brought to you by Melissa.
[00:01:21.840 --> 00:01:27.680]   More than 10,000 clients worldwide rely on Melissa for full spectrum data quality
[00:01:27.680 --> 00:01:32.960]   and ID verification software. Make sure your customer contact data is up to date.
[00:01:32.960 --> 00:01:39.920]   Get started today with 1000 records cleaned for free at Melissa.com/twit.
[00:01:39.920 --> 00:01:44.200]   And by Cisco-Miroki, with employees working in different locations,
[00:01:44.200 --> 00:01:48.600]   providing a unified work experience seems as easy as hurting cats.
[00:01:48.600 --> 00:01:53.800]   How do you rein in so many moving parts? Well, the Moraki Cloud managed network.
[00:01:53.800 --> 00:01:57.160]   Learn how your organization can make hybrid work.
[00:01:57.160 --> 00:02:02.040]   Visit moraki.sisco.com/twit.
[00:02:02.040 --> 00:02:06.680]   It's time for Twig this week in Google, San's Leo this week.
[00:02:06.680 --> 00:02:08.440]   I guess it was as well last week.
[00:02:08.440 --> 00:02:12.480]   Mica was filling in for Leo last time because I was away on vacation.
[00:02:12.480 --> 00:02:15.280]   Now obviously, I'm back. Jason Hall filling in for Leo.
[00:02:15.280 --> 00:02:18.760]   And I always have a really great time when I get to sit in this chair
[00:02:18.760 --> 00:02:22.080]   and talk a little bit about Google, a little bit about everything else
[00:02:22.080 --> 00:02:24.240]   that we talk about on this week in Google.
[00:02:24.240 --> 00:02:25.880]   So it's really great to be here.
[00:02:25.880 --> 00:02:30.000]   And I'm excited because we've got the full panel starting with you, Stacey.
[00:02:30.000 --> 00:02:34.160]   And by the way, Stacey Higginbotham. Happy birthday.
[00:02:34.160 --> 00:02:35.680]   -Woo! -Thank you!
[00:02:35.680 --> 00:02:36.560]   -Yay! -Yay!
[00:02:36.560 --> 00:02:38.080]   -Sell to be great. -Yay!
[00:02:38.080 --> 00:02:38.840]   -Yeah. -Yeah.
[00:02:38.840 --> 00:02:42.960]   -You can sort of see my little birthday streamer up there.
[00:02:42.960 --> 00:02:45.080]   -Aww, definitely. -And...
[00:02:45.080 --> 00:02:48.400]   -Oh! -That's so festive.
[00:02:48.400 --> 00:02:51.400]   That's awesome. -Yes, it's super festive.
[00:02:51.400 --> 00:02:53.400]   -Well, I hope you forgot some... -I'm 45.
[00:02:53.400 --> 00:02:56.720]   Excellent plans to celebrate tonight.
[00:02:56.720 --> 00:02:58.680]   So we will get you out in time.
[00:02:58.680 --> 00:03:00.960]   I guarantee you, the show's not going to last three hours.
[00:03:00.960 --> 00:03:02.400]   Don't worry.
[00:03:02.400 --> 00:03:05.240]   We got you covered. But happy birthday. That's awesome.
[00:03:05.240 --> 00:03:08.400]   Thanks for being here on your birthday, especially.
[00:03:08.400 --> 00:03:11.120]   -I also... -I don't want to celebrate with anyone else.
[00:03:11.120 --> 00:03:13.160]   -Except to maybe your family... -Yeah, sure.
[00:03:13.160 --> 00:03:14.760]   -...and the... -...and the...
[00:03:14.760 --> 00:03:16.760]   -...family and friends. -Yes, there we go.
[00:03:16.760 --> 00:03:18.920]   Also joining us, Jeff Jarvis.
[00:03:18.920 --> 00:03:21.360]   What's happening, Jeff? It's good to see you.
[00:03:21.360 --> 00:03:23.600]   I'm much good to see you back from your vacation
[00:03:23.600 --> 00:03:27.600]   with what was wonderful in the jungles and the water.
[00:03:27.600 --> 00:03:29.200]   It was fantastic.
[00:03:29.200 --> 00:03:32.520]   Went to Costa Rica for two and a half weeks.
[00:03:32.520 --> 00:03:35.080]   A return to Costa Rica, because I was there 20 years ago
[00:03:35.080 --> 00:03:37.840]   with my wife for our honeymoon.
[00:03:37.840 --> 00:03:38.680]   Oh!
[00:03:38.680 --> 00:03:41.200]   So this was kind of like going back,
[00:03:41.200 --> 00:03:42.560]   seeing how things have changed,
[00:03:42.560 --> 00:03:43.960]   bringing our kids with us,
[00:03:43.960 --> 00:03:46.440]   hopefully instilling in them a sense of adventure
[00:03:46.440 --> 00:03:48.120]   and curiosity about the world.
[00:03:48.120 --> 00:03:49.680]   And I think we accomplished it.
[00:03:49.680 --> 00:03:51.920]   We covered the... I mean, so much of the country.
[00:03:51.920 --> 00:03:54.960]   We drove all over the place and had many adventures,
[00:03:54.960 --> 00:03:56.880]   so it was awesome.
[00:03:56.880 --> 00:03:58.520]   -Really great time. -We're going back.
[00:03:58.520 --> 00:04:00.640]   Yeah, thank you. I mean, I knew that it was...
[00:04:00.640 --> 00:04:04.160]   I knew that we were there the right amount of time,
[00:04:04.160 --> 00:04:06.640]   because the last couple of days, we were all in agreement.
[00:04:06.640 --> 00:04:09.680]   We were like, "This has been the best trip of the world,
[00:04:09.680 --> 00:04:10.920]   but I'm ready to go home."
[00:04:10.920 --> 00:04:12.120]   You know what I mean?
[00:04:12.120 --> 00:04:13.600]   Definitely we hit that point,
[00:04:13.600 --> 00:04:17.160]   so it was time to return to normalcy.
[00:04:17.160 --> 00:04:18.360]   So it's good to be back.
[00:04:18.360 --> 00:04:19.840]   But anyways, it's great to see you, sir.
[00:04:19.840 --> 00:04:22.760]   Also great to see Ann Pruitt.
[00:04:22.760 --> 00:04:24.040]   Hands-on photography.
[00:04:24.040 --> 00:04:26.800]   -And all-time club twit. -It looks like...
[00:04:26.800 --> 00:04:28.560]   -Good to see you, man. -Yes, sir.
[00:04:28.560 --> 00:04:30.280]   -You too, you too. -And thank you.
[00:04:30.280 --> 00:04:32.160]   -And thank you. -And thank you.
[00:04:32.160 --> 00:04:33.320]   Oh, my pleasure.
[00:04:33.320 --> 00:04:37.840]   The whole vacation thing, I struggle with it for years.
[00:04:37.840 --> 00:04:40.120]   I'm the kind of guy that a three-day weekend
[00:04:40.120 --> 00:04:41.880]   is pretty good for me.
[00:04:41.880 --> 00:04:45.480]   -Yeah. -And then you start going
[00:04:45.480 --> 00:04:48.320]   into four or five days, I get a little antsy,
[00:04:48.320 --> 00:04:50.280]   and I should be doing something.
[00:04:50.280 --> 00:04:52.760]   -And get antsy? -And so I'm like,
[00:04:52.760 --> 00:04:54.360]   "Ugh, I struggle with it."
[00:04:54.360 --> 00:04:57.000]   So I've learned over the years to, you know,
[00:04:57.000 --> 00:04:58.720]   I'll take a week off and, you know,
[00:04:58.720 --> 00:05:00.960]   I've been on a cruise and stuff like that,
[00:05:00.960 --> 00:05:03.680]   and just try to totally unplug.
[00:05:03.680 --> 00:05:06.400]   But usually about day five, I'm like,
[00:05:06.400 --> 00:05:09.600]   "Okay, try to unplug,
[00:05:09.600 --> 00:05:12.600]   "just let go, just keep disconnected."
[00:05:12.600 --> 00:05:16.160]   I mean, that's what I've noticed in myself
[00:05:16.160 --> 00:05:18.720]   in taking vacation is,
[00:05:18.720 --> 00:05:20.800]   especially going someplace like Costa Rica,
[00:05:20.800 --> 00:05:22.280]   or going to Hawaii or whatever,
[00:05:22.280 --> 00:05:26.000]   if it's like a five-day vacation or a week,
[00:05:26.000 --> 00:05:28.160]   like it takes me five days to get to the point
[00:05:28.160 --> 00:05:31.680]   to where I'm not thinking about my normal,
[00:05:31.680 --> 00:05:36.120]   everyday life anymore, that my days are just entirely,
[00:05:36.120 --> 00:05:37.800]   like, wake up, "Oh, what do you want to do?
[00:05:37.800 --> 00:05:39.800]   "Go to the beach or whatever."
[00:05:39.800 --> 00:05:41.880]   You know, like, it takes me a little bit of that time
[00:05:41.880 --> 00:05:43.640]   to kind of get beyond,
[00:05:43.640 --> 00:05:47.320]   get outside of the regular stressors in life
[00:05:47.320 --> 00:05:48.960]   and get into vacation mode.
[00:05:48.960 --> 00:05:53.560]   So I almost, I appreciate having the ability
[00:05:53.560 --> 00:05:55.920]   to take two and a half weeks, which not, you know,
[00:05:55.920 --> 00:05:58.160]   it realizes it is definitely a privilege
[00:05:58.160 --> 00:06:00.400]   that I have to be able to do that,
[00:06:00.400 --> 00:06:03.240]   and get to that point to where I can just kind of let go
[00:06:03.240 --> 00:06:07.120]   and then stop thinking about all of those stressors, you know?
[00:06:07.120 --> 00:06:11.720]   - Bro, on our last cruise around day four or so,
[00:06:11.720 --> 00:06:14.320]   I saw something wrong with the door knob,
[00:06:14.320 --> 00:06:17.160]   like on the sliding door or something,
[00:06:17.160 --> 00:06:19.840]   and it drove me nuts, and I'm digging through my bags,
[00:06:19.840 --> 00:06:21.640]   trying to find a Phillips screwdriver,
[00:06:21.640 --> 00:06:23.280]   and just, I think that--
[00:06:23.280 --> 00:06:25.760]   - You wouldn't even think of carrying a Phillips screwdriver.
[00:06:25.760 --> 00:06:26.760]   (laughing)
[00:06:26.760 --> 00:06:28.560]   - What is wrong with you?
[00:06:28.560 --> 00:06:30.360]   - I'm going normally on that one.
[00:06:30.360 --> 00:06:32.880]   - Stacy, you have a Phillips, what do you do?
[00:06:32.880 --> 00:06:34.920]   Start carrying apart the apartment?
[00:06:34.920 --> 00:06:37.520]   - It was like I need to be doing something,
[00:06:37.520 --> 00:06:40.040]   and I was like, oh, that door doesn't seem right.
[00:06:40.040 --> 00:06:41.600]   Let me see if I can pass that.
[00:06:41.600 --> 00:06:44.120]   Then I caught myself, and there's my task.
[00:06:44.120 --> 00:06:45.520]   - Let go, you know?
[00:06:45.520 --> 00:06:47.440]   (laughing)
[00:06:47.440 --> 00:06:50.600]   - The screwdriver is a universal, like, you know,
[00:06:50.600 --> 00:06:54.680]   there are door knobs, there's loose things on cabinet.
[00:06:54.680 --> 00:06:55.760]   I mean, screwdriver--
[00:06:55.760 --> 00:06:57.800]   - You are both obsessed.
[00:06:57.800 --> 00:07:00.320]   I'm weird, I'm not obsessed, I'm just weird.
[00:07:00.320 --> 00:07:02.200]   - Yeah, it's just, you want things,
[00:07:02.200 --> 00:07:04.840]   if you can fix it with a screwdriver,
[00:07:04.840 --> 00:07:07.000]   that's like the least amount of effort.
[00:07:07.000 --> 00:07:07.840]   - Yeah, just--
[00:07:07.840 --> 00:07:09.960]   (laughing)
[00:07:09.960 --> 00:07:12.160]   - Especially if you happen to have a screwdriver on you.
[00:07:12.160 --> 00:07:13.920]   That makes it a lot easier.
[00:07:13.920 --> 00:07:15.520]   (laughing)
[00:07:15.520 --> 00:07:17.600]   I will encounter things I need for a screwdriver,
[00:07:17.600 --> 00:07:19.040]   but I'm never carrying a screwdriver,
[00:07:19.040 --> 00:07:20.600]   so it requires more effort.
[00:07:20.600 --> 00:07:21.600]   (laughing)
[00:07:21.600 --> 00:07:22.480]   - No, no, no, no.
[00:07:22.480 --> 00:07:24.640]   - Oh, I think you should have both
[00:07:24.640 --> 00:07:26.760]   a Phillips and a flathead screwdriver
[00:07:26.760 --> 00:07:29.480]   in on every floor of your house,
[00:07:29.480 --> 00:07:31.480]   or possibly each room.
[00:07:31.480 --> 00:07:32.320]   - Okay.
[00:07:32.320 --> 00:07:33.160]   - You know, just--
[00:07:33.160 --> 00:07:35.240]   - I have a nice little, like, he had toolkit,
[00:07:35.240 --> 00:07:36.080]   yeah, I was like, how about--
[00:07:36.080 --> 00:07:36.920]   - Yeah, do you have--
[00:07:36.920 --> 00:07:38.200]   - Not in my suitcase.
[00:07:38.200 --> 00:07:41.640]   - Well, now I know what I'm getting you for your suitcase.
[00:07:41.640 --> 00:07:42.480]   - And your next--
[00:07:42.480 --> 00:07:43.960]   - Yeah, you need to travel sewing kit
[00:07:43.960 --> 00:07:44.960]   and a travel toolkit.
[00:07:44.960 --> 00:07:45.800]   - On a casey.
[00:07:45.800 --> 00:07:48.720]   - Do you carry on or do you do check?
[00:07:48.720 --> 00:07:50.080]   - I do both.
[00:07:50.080 --> 00:07:50.920]   - Oh, both.
[00:07:50.920 --> 00:07:52.080]   - Oh, both.
[00:07:52.080 --> 00:07:54.440]   - Oh, sorry, it depends on where I'm at.
[00:07:54.440 --> 00:07:56.720]   No, no, no, I do carry on 90% of my trips,
[00:07:56.720 --> 00:07:58.800]   but if I'm going someplace like a two and a half week
[00:07:58.800 --> 00:08:00.000]   trip to Costa Rica,
[00:08:00.000 --> 00:08:02.000]   actually that you could do carry on 'cause you--
[00:08:02.000 --> 00:08:03.440]   - I'm doing both.
[00:08:03.440 --> 00:08:04.280]   - They need suits, yeah.
[00:08:04.280 --> 00:08:06.640]   - I'm doing both, even if it's just for a couple of days
[00:08:06.640 --> 00:08:10.240]   because, heck, my shoes don't fit in my bag, you know.
[00:08:10.240 --> 00:08:11.920]   - Oh, more you amateurs.
[00:08:11.920 --> 00:08:15.040]   - Yeah, no, I wear a size friggin' 14.
[00:08:15.040 --> 00:08:17.880]   It takes up a lot of space and I carry on with clothes
[00:08:17.880 --> 00:08:20.600]   and it's just easier to just check back.
[00:08:20.600 --> 00:08:22.760]   - I can bring a pair, two pairs of shoes
[00:08:22.760 --> 00:08:26.240]   and a travel yoga mat and outfits for five days
[00:08:26.240 --> 00:08:28.400]   as a woman. - As impressive.
[00:08:28.400 --> 00:08:29.240]   - In a carry out.
[00:08:29.240 --> 00:08:32.320]   - A travel yoga mat, I don't understand.
[00:08:32.320 --> 00:08:35.160]   Aren't those things like always these big roll up things
[00:08:35.160 --> 00:08:37.080]   or is it much thinner mats?
[00:08:37.080 --> 00:08:39.880]   - It's a thinner mat that's foldable.
[00:08:39.880 --> 00:08:40.880]   - Oh, okay.
[00:08:40.880 --> 00:08:44.520]   - I mean, it's still a weird plastic-y thing.
[00:08:44.520 --> 00:08:46.080]   It's not cloth.
[00:08:46.080 --> 00:08:46.920]   Okay, Google.
[00:08:46.920 --> 00:08:48.240]   (laughing)
[00:08:48.240 --> 00:08:49.960]   - Sorry, yeah, we--
[00:08:49.960 --> 00:08:52.840]   Last week, Mr. Sergeant, we talked about Google
[00:08:52.840 --> 00:08:54.640]   right out the gate, so we should do the same.
[00:08:54.640 --> 00:08:56.960]   - That's a mic at all these.
[00:08:56.960 --> 00:09:00.440]   - Like, yeah, we talked a lot about Google last week.
[00:09:00.440 --> 00:09:04.280]   So, I think I already have a title for this episode.
[00:09:04.280 --> 00:09:07.280]   I think we should call it full-throttle democracy
[00:09:07.280 --> 00:09:10.760]   because I open up the dock to start putting in links
[00:09:10.760 --> 00:09:13.720]   and I look down there and Jeff, I'm assuming it's you, Jeff,
[00:09:13.720 --> 00:09:15.600]   'cause you are usually very active
[00:09:15.600 --> 00:09:17.600]   in throwing links into the dock.
[00:09:17.600 --> 00:09:19.040]   - To Leo's consternation.
[00:09:19.040 --> 00:09:20.040]   (laughing)
[00:09:20.040 --> 00:09:22.160]   - I mean, it's a different section of the dock, you know?
[00:09:22.160 --> 00:09:25.640]   If Leo wants to not look at it, he doesn't have to, I suppose.
[00:09:25.640 --> 00:09:27.320]   But I opened up the dock and I looked at it
[00:09:27.320 --> 00:09:28.680]   and I was kind of looking through the stories
[00:09:28.680 --> 00:09:32.000]   and I was like, you know, I've been gone two and a half weeks.
[00:09:32.000 --> 00:09:35.600]   And as we talked about, I've been very disconnected
[00:09:35.600 --> 00:09:39.160]   from tech news and so I'm sure you can all understand
[00:09:39.160 --> 00:09:41.400]   and sympathize with this.
[00:09:41.400 --> 00:09:43.360]   The beauty is I disconnected
[00:09:43.360 --> 00:09:45.560]   and I didn't think at all about technology.
[00:09:45.560 --> 00:09:47.920]   The horror is I then came back to work
[00:09:47.920 --> 00:09:50.440]   and had to know about all the things that have happened
[00:09:50.440 --> 00:09:51.800]   in the last two and a half weeks
[00:09:51.800 --> 00:09:53.480]   so that I could hit the ground running again
[00:09:53.480 --> 00:09:54.880]   and I'm not quite there.
[00:09:54.880 --> 00:09:57.600]   So when I opened the dock and saw that you would put in,
[00:09:57.600 --> 00:10:00.440]   I mean, it seemed pretty darn thorough to me.
[00:10:00.440 --> 00:10:01.640]   So I was like, you know what?
[00:10:01.640 --> 00:10:03.680]   This week is all democracy.
[00:10:03.680 --> 00:10:04.760]   All democracy.
[00:10:04.760 --> 00:10:05.800]   (laughing)
[00:10:05.800 --> 00:10:08.120]   Nothing but Jeff's stories and Stacey,
[00:10:08.120 --> 00:10:09.440]   if you put some stories in there
[00:10:09.440 --> 00:10:11.520]   and if you put some stories in there, great.
[00:10:11.520 --> 00:10:12.360]   It's all a democracy.
[00:10:12.360 --> 00:10:14.600]   - You move around and yeah.
[00:10:14.600 --> 00:10:15.440]   - Yeah.
[00:10:15.440 --> 00:10:16.360]   (laughing)
[00:10:16.360 --> 00:10:18.960]   So I figured we'd kind of turn this show up on its,
[00:10:18.960 --> 00:10:21.360]   you know, upside down and see what happens.
[00:10:21.360 --> 00:10:24.040]   So as a result, like I'm looking through here
[00:10:24.040 --> 00:10:27.520]   and I mean, you know, there's some Google,
[00:10:27.520 --> 00:10:29.360]   there's a lot of other things
[00:10:29.360 --> 00:10:30.760]   and there's a change log.
[00:10:30.760 --> 00:10:32.080]   I did do the change log.
[00:10:32.080 --> 00:10:33.520]   So we've got that in our podcast.
[00:10:33.520 --> 00:10:34.360]   - You've got change log.
[00:10:34.360 --> 00:10:35.640]   It's a respectable change log.
[00:10:35.640 --> 00:10:36.480]   - Yeah.
[00:10:36.480 --> 00:10:38.200]   - I think there's actually some pretty great stuff in there.
[00:10:38.200 --> 00:10:39.040]   So.
[00:10:39.040 --> 00:10:39.880]   - Sure, I've.
[00:10:39.880 --> 00:10:42.600]   - Even when Stacey took three stories out of it
[00:10:42.600 --> 00:10:44.560]   for an ad-hope thing, it's still a good change log.
[00:10:44.560 --> 00:10:45.520]   It's still solid.
[00:10:45.520 --> 00:10:46.360]   I know, right?
[00:10:46.360 --> 00:10:48.800]   It was like a jam-packed change log.
[00:10:48.800 --> 00:10:50.360]   It'll be, it'll be champion.
[00:10:50.360 --> 00:10:52.200]   And now it's a full change log.
[00:10:52.200 --> 00:10:54.000]   (laughing)
[00:10:54.000 --> 00:10:55.360]   - All right, so let's talk about these things.
[00:10:55.360 --> 00:10:56.200]   - Yeah, all right.
[00:10:56.200 --> 00:10:57.040]   So let's do it.
[00:10:57.040 --> 00:10:58.800]   And actually, you know, Stacey,
[00:10:58.800 --> 00:11:00.200]   why don't we,
[00:11:00.200 --> 00:11:02.640]   because I don't feel like we need to lead the story
[00:11:02.640 --> 00:11:04.800]   with Chrome shipping web GPU.
[00:11:04.800 --> 00:11:09.800]   Let's lead the show with this tri-factor of stories
[00:11:09.800 --> 00:11:13.080]   about Google's, what would you call it?
[00:11:13.080 --> 00:11:17.200]   Flailing interest in third-party smart displays
[00:11:17.200 --> 00:11:21.400]   in its assistant devices and its security devices.
[00:11:21.400 --> 00:11:22.760]   What exactly is going on here?
[00:11:22.760 --> 00:11:25.520]   And I guess this, this good starting point
[00:11:25.520 --> 00:11:28.160]   is this story that nine to five Google
[00:11:28.160 --> 00:11:31.120]   had spotted a Google support article
[00:11:31.120 --> 00:11:33.360]   for duo calls of all things.
[00:11:33.360 --> 00:11:35.120]   And in this support article,
[00:11:35.120 --> 00:11:36.920]   there was a little note that said important,
[00:11:36.920 --> 00:11:39.440]   Google no longer provides software updates
[00:11:39.440 --> 00:11:41.400]   for these third-party smart displays,
[00:11:41.400 --> 00:11:44.080]   Lenovo Smart Display, 7, 8, and 10-inch,
[00:11:44.080 --> 00:11:46.940]   JBL, LinkView, LG XBOOM, AI,
[00:11:46.940 --> 00:11:50.840]   ThinkQ, WK9, Smart Display.
[00:11:50.840 --> 00:11:52.640]   Google says this could impact the quality
[00:11:52.640 --> 00:11:54.000]   of video calls and meetings.
[00:11:54.000 --> 00:11:55.240]   So just kind of throws that out there.
[00:11:55.240 --> 00:11:57.200]   Oh, by the way, these third-party smart displays
[00:11:57.200 --> 00:11:59.000]   aren't gonna work anymore.
[00:11:59.000 --> 00:12:00.960]   So it turns out the support,
[00:12:00.960 --> 00:12:03.320]   and I mean, these are five-year-old devices.
[00:12:03.320 --> 00:12:06.240]   So it's not like they're brand-spanking new,
[00:12:06.240 --> 00:12:10.680]   but I think this along with kind of,
[00:12:10.680 --> 00:12:13.240]   Google's upcoming announcement
[00:12:13.240 --> 00:12:14.720]   that we know we're gonna hear about this year,
[00:12:14.720 --> 00:12:16.000]   about the Pixel tablet.
[00:12:16.000 --> 00:12:17.840]   And we know that the Pixel tablet's gonna have
[00:12:17.840 --> 00:12:22.160]   this kind of crossover thing going with these smart displays
[00:12:22.160 --> 00:12:25.120]   in the tablet last night on all about Android,
[00:12:25.120 --> 00:12:29.160]   Michelle Ramon was kind of talking about how,
[00:12:29.160 --> 00:12:33.960]   this might signal Google's new interest in these things,
[00:12:33.960 --> 00:12:37.360]   how it's no longer about keeping this open for third parties.
[00:12:37.360 --> 00:12:39.960]   I was really about Google kind of controlling
[00:12:39.960 --> 00:12:42.040]   with their own tablet ecosystem
[00:12:42.040 --> 00:12:43.560]   and how that interplays with smart display.
[00:12:43.560 --> 00:12:45.600]   That's my device right there, that's there.
[00:12:45.600 --> 00:12:48.240]   Is it? So how do you feel about that, Jeff?
[00:12:48.240 --> 00:12:53.200]   Google kills everything, which doesn't make me happy.
[00:12:53.200 --> 00:12:57.400]   And so we can't trust them,
[00:12:57.400 --> 00:12:59.480]   but it's also fascinating to me.
[00:12:59.480 --> 00:13:00.840]   In the last few weeks, we were talking about
[00:13:00.840 --> 00:13:03.000]   how the audio devices have cooties,
[00:13:03.000 --> 00:13:04.360]   that nobody wants to anymore.
[00:13:04.360 --> 00:13:05.800]   Now the video devices have cooties,
[00:13:05.800 --> 00:13:08.000]   so this whole I can't wait to hear Stacy's take here.
[00:13:08.000 --> 00:13:10.240]   'Cause this whole I know I'm delayed by talking,
[00:13:10.240 --> 00:13:11.080]   but what the hell?
[00:13:11.080 --> 00:13:13.760]   It's just like watching Morning Joe where he has a long,
[00:13:13.760 --> 00:13:15.240]   much different point.
[00:13:15.240 --> 00:13:16.960]   And Mike Barnacle says something stupid.
[00:13:16.960 --> 00:13:20.840]   So I'm amazed that that whole line
[00:13:20.840 --> 00:13:22.440]   that was gonna take over our homes,
[00:13:22.440 --> 00:13:23.760]   we're all gonna have it in every room,
[00:13:23.760 --> 00:13:25.120]   we're gonna use it.
[00:13:25.120 --> 00:13:28.000]   The entire idea seems to be dying before us.
[00:13:28.000 --> 00:13:29.600]   Stacy, Stacy, what do you think?
[00:13:29.600 --> 00:13:31.160]   Okay, thank you.
[00:13:31.160 --> 00:13:33.160]   All right, so we'll wind you up.
[00:13:33.160 --> 00:13:34.000]   And go.
[00:13:34.000 --> 00:13:36.080]   (laughs)
[00:13:36.080 --> 00:13:38.640]   When you brought up the Pixel tablet,
[00:13:38.640 --> 00:13:39.960]   I was like, where are you going with this man?
[00:13:39.960 --> 00:13:41.840]   Google also did a couple other things.
[00:13:41.840 --> 00:13:45.560]   And so big picture, my take on this
[00:13:45.560 --> 00:13:47.480]   because I live in this world is what the heck
[00:13:47.480 --> 00:13:49.240]   is Google doing within the Smart Home?
[00:13:49.240 --> 00:13:52.360]   What is Google's Smart Home strategy?
[00:13:52.360 --> 00:13:55.440]   So it's talking about not doing software support
[00:13:55.440 --> 00:13:56.800]   for these devices.
[00:13:56.800 --> 00:13:59.280]   To be clear everyone, they still work.
[00:13:59.280 --> 00:14:01.560]   I have two of the Lenovo's.
[00:14:01.560 --> 00:14:02.960]   I have the eight inch and the 10 inch,
[00:14:02.960 --> 00:14:04.800]   and both of them are still working.
[00:14:04.800 --> 00:14:06.960]   My eight inch actually just got,
[00:14:06.960 --> 00:14:09.240]   or it just told me about some new features.
[00:14:09.240 --> 00:14:11.760]   So I thought it got some sort of update
[00:14:11.760 --> 00:14:14.320]   like in the last two weeks, but I could be wrong.
[00:14:14.320 --> 00:14:19.600]   But also last week, Google said they were going to
[00:14:19.600 --> 00:14:22.160]   stop supporting the drop cam devices
[00:14:22.160 --> 00:14:26.800]   that admittedly last sold in like 2014.
[00:14:26.800 --> 00:14:27.640]   (laughs)
[00:14:27.640 --> 00:14:29.120]   So they're like, yeah, those devices?
[00:14:29.120 --> 00:14:29.960]   - They're even older.
[00:14:29.960 --> 00:14:30.800]   - We're not.
[00:14:30.800 --> 00:14:32.280]   They're 10 years old.
[00:14:32.280 --> 00:14:35.480]   But they also said they would stop with Nest Secure,
[00:14:35.480 --> 00:14:39.440]   which was their Smart Home,
[00:14:39.440 --> 00:14:41.200]   I don't know why I'm driving a little Nest Secure box
[00:14:41.200 --> 00:14:44.800]   here for y'all, but this was their Smart Home Security
[00:14:44.800 --> 00:14:47.360]   system with the little keypad that you could punch things in
[00:14:47.360 --> 00:14:48.960]   and worked with the cameras and all.
[00:14:48.960 --> 00:14:52.320]   That was last sold in 2020,
[00:14:52.320 --> 00:14:53.840]   was the last time they sold that,
[00:14:53.840 --> 00:14:55.600]   and they're like, yeah, that's done too.
[00:14:55.600 --> 00:14:59.280]   And so there's a lot of moving parts
[00:14:59.280 --> 00:15:01.600]   that Google is kind of investing in
[00:15:01.600 --> 00:15:04.120]   and disinvesting in on the Smart Home side.
[00:15:04.120 --> 00:15:07.240]   And I think with three years ago,
[00:15:07.240 --> 00:15:10.720]   Google did a deal with ADT, and they were going to sell,
[00:15:10.720 --> 00:15:12.640]   they were going to package some of their hardware up
[00:15:12.640 --> 00:15:15.880]   with ADT, and ADT would sell professional monitoring
[00:15:15.880 --> 00:15:20.280]   plus the professional ADT monitoring plus DIY Google stuff.
[00:15:20.280 --> 00:15:22.760]   And ADT people were going to install Google stuff
[00:15:22.760 --> 00:15:24.800]   into ADT customers' homes.
[00:15:24.800 --> 00:15:26.480]   This was a great deal for Google.
[00:15:26.480 --> 00:15:31.000]   It got their more devices and more places, yay.
[00:15:32.080 --> 00:15:34.600]   And they made the investment in ADT without actually having
[00:15:34.600 --> 00:15:36.440]   to get, they would get some upside
[00:15:36.440 --> 00:15:39.160]   from the Smart Home Security play
[00:15:39.160 --> 00:15:40.600]   without having to build the stuff.
[00:15:40.600 --> 00:15:42.480]   So I thought that was pretty smart.
[00:15:42.480 --> 00:15:44.920]   And it seems to be going well for them.
[00:15:44.920 --> 00:15:47.880]   So deprecating those devices
[00:15:47.880 --> 00:15:52.040]   and the Nest Secure system makes a lot of sense.
[00:15:52.040 --> 00:15:56.400]   This other display issue is a little bit different.
[00:15:56.400 --> 00:15:58.360]   One, these are old devices.
[00:15:58.360 --> 00:16:01.920]   Two, if you've been paying attention to the Sonos,
[00:16:01.920 --> 00:16:05.840]   Google legal issues,
[00:16:05.840 --> 00:16:08.360]   Sonos actually said they weren't putting Google
[00:16:08.360 --> 00:16:13.360]   onto the Sonos speakers because Google
[00:16:13.360 --> 00:16:18.160]   made it harder for third party display makers
[00:16:18.160 --> 00:16:21.480]   or third party Google home devices to work.
[00:16:21.480 --> 00:16:24.880]   So it sounds like Google's kind of been getting out
[00:16:24.880 --> 00:16:29.480]   of that third party market for a while.
[00:16:29.480 --> 00:16:32.360]   And this might just be the end of that.
[00:16:32.360 --> 00:16:35.000]   But again, these devices still work.
[00:16:35.000 --> 00:16:38.920]   We do see Google pitching like this Apple kind of ask,
[00:16:38.920 --> 00:16:40.520]   like, we're going to own the hardware,
[00:16:40.520 --> 00:16:42.440]   we're going to own the devices.
[00:16:42.440 --> 00:16:44.520]   And sometimes our devices are not as good as Apple's,
[00:16:44.520 --> 00:16:47.360]   AKA Pixel Buds, but they got better.
[00:16:47.360 --> 00:16:49.240]   I'm really interested to see what they do with Fitbit
[00:16:49.240 --> 00:16:52.760]   because of this, because they just also today announced
[00:16:52.760 --> 00:16:56.400]   that now if you have a Fitbit account,
[00:16:56.400 --> 00:16:59.400]   you can also migrate that over to the Google Fit,
[00:16:59.400 --> 00:17:01.240]   which I don't know why you would do that yet.
[00:17:01.240 --> 00:17:02.760]   - Yeah, why would you do that?
[00:17:02.760 --> 00:17:04.800]   You have to eventually, right?
[00:17:04.800 --> 00:17:06.800]   - Eventually, in 2025, I will do that.
[00:17:06.800 --> 00:17:11.280]   But by 2025, I will have chosen my new fitness tracker,
[00:17:11.280 --> 00:17:14.840]   probably, because I have very little faith in Google's
[00:17:14.840 --> 00:17:16.360]   Wear OS, good Lord.
[00:17:16.360 --> 00:17:18.800]   Anyway, okay, so back to Google in the smart home.
[00:17:18.800 --> 00:17:20.840]   The good news, the only piece of good news is
[00:17:20.840 --> 00:17:23.320]   with Google Home, which by the way, software service
[00:17:23.320 --> 00:17:27.360]   on your phone, they added new device category,
[00:17:27.360 --> 00:17:30.400]   or they added more granular controls for appliances
[00:17:30.400 --> 00:17:31.760]   as a device category.
[00:17:31.760 --> 00:17:36.600]   So now you can go into your thermostat or your fan,
[00:17:36.600 --> 00:17:39.320]   not your thermostat, I'm sorry, your fan,
[00:17:39.320 --> 00:17:42.880]   or some of your shutters and blinds and garage doors,
[00:17:42.880 --> 00:17:46.240]   and you can now, instead of just like state,
[00:17:46.240 --> 00:17:50.000]   like on offer, open close, you can now like gradually,
[00:17:50.000 --> 00:17:53.120]   like slide or bar it, so it could be halfway open,
[00:17:53.120 --> 00:17:57.360]   halfway, depending on if you're optimist or pessimist.
[00:17:57.360 --> 00:18:01.840]   So they're still investing on the services side,
[00:18:01.840 --> 00:18:05.680]   but I see them getting out of third party devices,
[00:18:05.680 --> 00:18:10.040]   and that was a lot, that's enough.
[00:18:10.040 --> 00:18:13.280]   - Oh, Novo says that smart display 10 inch
[00:18:13.280 --> 00:18:14.640]   will be available soon.
[00:18:16.440 --> 00:18:20.920]   The small one is, they smart display seven is out of stock.
[00:18:20.920 --> 00:18:24.080]   'Cause it was either old, but then again,
[00:18:24.080 --> 00:18:25.640]   some people have bought them fairly recently,
[00:18:25.640 --> 00:18:27.040]   haven't they, couldn't they?
[00:18:27.040 --> 00:18:30.960]   - I don't think you could buy the Lenovo, I mean--
[00:18:30.960 --> 00:18:33.120]   - I see what you-- - I bought mine in like 2018,
[00:18:33.120 --> 00:18:35.760]   but I think-- - Yeah, going to the Lenovo side
[00:18:35.760 --> 00:18:37.800]   for the Lenovo smart display 10 inch,
[00:18:37.800 --> 00:18:42.800]   it says available soon, but yeah, I don't know.
[00:18:42.800 --> 00:18:44.880]   - Yeah, I don't know what that actually means,
[00:18:44.880 --> 00:18:47.920]   or is that just the image they put there
[00:18:47.920 --> 00:18:51.080]   when they're out of stock, and then some days soon
[00:18:51.080 --> 00:18:53.520]   that'll be updated to no longer available or something.
[00:18:53.520 --> 00:18:55.840]   - Redefined soon, right.
[00:18:55.840 --> 00:18:58.640]   - Now they didn't say what would happen with the clocks.
[00:18:58.640 --> 00:19:00.080]   I don't know if anyone has these,
[00:19:00.080 --> 00:19:02.960]   but there are a few Lenovo Android clocks,
[00:19:02.960 --> 00:19:06.280]   and I don't know what happens there.
[00:19:06.280 --> 00:19:07.840]   I have a little sad though, 'cause I do use
[00:19:07.840 --> 00:19:10.080]   the Lenovo 10 inch that's in my kitchen,
[00:19:10.080 --> 00:19:12.680]   to duo call my mom, it's how I actually got my mom
[00:19:12.680 --> 00:19:14.040]   to download Duo.
[00:19:14.040 --> 00:19:17.320]   So the loss of this could be a crushing blow
[00:19:17.320 --> 00:19:19.200]   to my family communications.
[00:19:19.200 --> 00:19:23.360]   - Hmm, if they stop working, which like you said,
[00:19:23.360 --> 00:19:24.280]   they still work. - Yeah, I haven't tried it.
[00:19:24.280 --> 00:19:27.280]   - Yeah, I haven't tried Duo on that thing,
[00:19:27.280 --> 00:19:28.560]   I guess I could try to call my mom,
[00:19:28.560 --> 00:19:31.080]   but she would be pretty upset if I did a show.
[00:19:31.080 --> 00:19:32.880]   - Well, I'd like that, right, but when is the last time
[00:19:32.880 --> 00:19:35.480]   you said happy birthday to you yet?
[00:19:35.480 --> 00:19:36.840]   - Yes, she has.
[00:19:36.840 --> 00:19:38.200]   When was the last time I used Duo,
[00:19:38.200 --> 00:19:41.160]   probably like a month or two ago?
[00:19:41.160 --> 00:19:42.880]   We don't talk visually very often,
[00:19:42.880 --> 00:19:44.560]   but my dad getting my mom on the phone visually
[00:19:44.560 --> 00:19:46.000]   is just difficult.
[00:19:46.000 --> 00:19:50.000]   - Well, that was gonna be my point of all of it.
[00:19:50.000 --> 00:19:52.720]   Well, first, Google wants all the control,
[00:19:52.720 --> 00:19:54.240]   that it just makes sense to me,
[00:19:54.240 --> 00:19:57.960]   they want everything under their own umbrella,
[00:19:57.960 --> 00:20:00.000]   and I necessarily have to worry about third parties
[00:20:00.000 --> 00:20:02.520]   and so forth, that just makes sense to me.
[00:20:02.520 --> 00:20:04.360]   And then secondly, it's,
[00:20:04.360 --> 00:20:08.080]   I don't know how much longer one could trust them,
[00:20:08.080 --> 00:20:11.960]   considering Google's history with killing things off,
[00:20:11.960 --> 00:20:13.920]   and then here and stuff like,
[00:20:13.920 --> 00:20:15.960]   last year's Duo a month ago,
[00:20:15.960 --> 00:20:20.640]   on this Google product, why?
[00:20:20.640 --> 00:20:24.080]   - Well, but I, okay, I haven't used Google Duo,
[00:20:24.080 --> 00:20:27.320]   but I talk to that thing every day.
[00:20:27.320 --> 00:20:29.720]   It's acting as a photo frame right now.
[00:20:29.720 --> 00:20:32.360]   We look at YouTube on it,
[00:20:32.360 --> 00:20:36.920]   like I looked up something on YouTube using it
[00:20:36.920 --> 00:20:38.720]   just two nights ago.
[00:20:38.720 --> 00:20:40.400]   So, I mean, I'm not using Duo,
[00:20:40.400 --> 00:20:41.840]   but I do use that.
[00:20:41.840 --> 00:20:43.320]   But you do actively use it.
[00:20:43.320 --> 00:20:44.720]   Okay.
[00:20:44.720 --> 00:20:45.560]   As I always wondered,
[00:20:45.560 --> 00:20:48.840]   how many people actively use those devices,
[00:20:48.840 --> 00:20:51.000]   other than-- - I don't think it was clock.
[00:20:51.000 --> 00:20:53.640]   - Yeah, other than it just being a clock sitting there.
[00:20:53.640 --> 00:20:57.360]   You know, I feel like it's not as bad as Google Glass,
[00:20:57.360 --> 00:20:59.400]   and there was a making me feel like a schmuck.
[00:20:59.400 --> 00:21:00.400]   - A lot.
[00:21:00.400 --> 00:21:01.240]   (laughing)
[00:21:01.240 --> 00:21:02.480]   It's kinda ridiculous.
[00:21:02.480 --> 00:21:04.280]   Glass is good at that.
[00:21:04.280 --> 00:21:06.880]   Good at doing that part.
[00:21:06.880 --> 00:21:08.320]   - I mean, we use it every day,
[00:21:08.320 --> 00:21:10.360]   I mean, we even use the display as a mechanism
[00:21:10.360 --> 00:21:11.640]   for turning our lights.
[00:21:11.640 --> 00:21:15.000]   Like, this is me shading my lights higher or lower.
[00:21:15.000 --> 00:21:16.320]   - Yeah.
[00:21:16.320 --> 00:21:17.880]   - 'Cause I don't use the app ever.
[00:21:17.880 --> 00:21:18.720]   - It's the app ever.
[00:21:18.720 --> 00:21:19.560]   - It's part home control.
[00:21:19.560 --> 00:21:20.400]   - Yeah. - It's a space here
[00:21:20.400 --> 00:21:21.720]   really useful thing.
[00:21:21.720 --> 00:21:22.880]   - Don't take this the wrong way,
[00:21:22.880 --> 00:21:24.720]   but are you queen of IOT though?
[00:21:24.720 --> 00:21:27.760]   - Just ask if I was weird.
[00:21:27.760 --> 00:21:28.600]   - Are you weird?
[00:21:28.600 --> 00:21:29.920]   - I don't know. - Are you anomalous
[00:21:29.920 --> 00:21:31.520]   to the population?
[00:21:31.520 --> 00:21:32.840]   And that you use this stuff.
[00:21:32.840 --> 00:21:34.840]   Does this say it is-- - It's decidedly so.
[00:21:34.840 --> 00:21:39.000]   - The market or this stuff, both video and audio.
[00:21:40.000 --> 00:21:42.320]   - Well, I'm weird in some ways.
[00:21:42.320 --> 00:21:46.680]   I think using the displays as a photo frame is pretty common.
[00:21:46.680 --> 00:21:50.160]   I was actually in my in-laws and they have it set up
[00:21:50.160 --> 00:21:52.000]   partially 'cause we gave it to them to set up.
[00:21:52.000 --> 00:21:57.000]   But I think a lot of people have a Google display
[00:21:57.000 --> 00:22:00.480]   and probably use it at some corner of their house.
[00:22:00.480 --> 00:22:02.080]   They may not use all the features, but--
[00:22:02.080 --> 00:22:03.080]   - Mm-hmm.
[00:22:03.080 --> 00:22:04.880]   If anything, it's an ambient device.
[00:22:04.880 --> 00:22:07.080]   It's just they're displaying something.
[00:22:07.080 --> 00:22:09.760]   You know, it's like you said, as a digital picture frame.
[00:22:09.760 --> 00:22:12.440]   You might not interact with it, but you're looking at it
[00:22:12.440 --> 00:22:14.600]   because it's showing you something that you want to look at
[00:22:14.600 --> 00:22:16.360]   and that has some value.
[00:22:16.360 --> 00:22:18.520]   I just wonder-- - Yeah, and it shows the weather.
[00:22:18.520 --> 00:22:20.360]   - Yeah. - And it shows AIQ.
[00:22:20.360 --> 00:22:24.000]   Remember they added air quality index to it last summer?
[00:22:24.000 --> 00:22:25.800]   - I forgot about that. - Yeah.
[00:22:25.800 --> 00:22:27.200]   - Right now I'm looking at it.
[00:22:27.200 --> 00:22:29.280]   My air quality, 10.
[00:22:29.280 --> 00:22:30.560]   - And so you wouldn't have known that
[00:22:30.560 --> 00:22:33.120]   if you didn't have that sitting right there.
[00:22:33.120 --> 00:22:34.120]   - Exactly.
[00:22:34.120 --> 00:22:36.800]   (laughing)
[00:22:36.800 --> 00:22:37.640]   - Point!
[00:22:37.640 --> 00:22:39.720]   (laughing)
[00:22:39.720 --> 00:22:43.720]   So when I think about this though,
[00:22:43.720 --> 00:22:49.240]   the value for Google, I remember the argument for,
[00:22:49.240 --> 00:22:51.200]   you know, these devices, you know,
[00:22:51.200 --> 00:22:54.000]   assistant hitting all of these devices was,
[00:22:54.000 --> 00:22:55.360]   yeah, because at the end of the day,
[00:22:55.360 --> 00:22:59.240]   Google just wants more people feeding it data
[00:22:59.240 --> 00:23:01.360]   from all of these different directions.
[00:23:01.360 --> 00:23:05.720]   Does this reduce that effort?
[00:23:05.720 --> 00:23:07.680]   Or I guess, you know, we're also kind of seeing
[00:23:07.680 --> 00:23:09.840]   that maybe assistant isn't as important to Google
[00:23:09.840 --> 00:23:11.120]   as it once was.
[00:23:11.120 --> 00:23:14.920]   And so maybe this is happening concurrently with that.
[00:23:14.920 --> 00:23:16.280]   Maybe that's part of the reason
[00:23:16.280 --> 00:23:18.640]   why suddenly they don't care about their party displays.
[00:23:18.640 --> 00:23:21.960]   I would just kind of imagine or assume that, you know,
[00:23:21.960 --> 00:23:26.080]   a company like Google that has had over time
[00:23:26.080 --> 00:23:30.080]   more of an open ethos on certain aspects of its business
[00:23:30.080 --> 00:23:32.600]   than other companies, say Apple,
[00:23:32.600 --> 00:23:35.480]   would have potential benefit
[00:23:35.480 --> 00:23:39.480]   from allowing third parties to create hardware
[00:23:39.480 --> 00:23:42.280]   that uses its software to continue feeding it
[00:23:42.280 --> 00:23:44.200]   then that valuable data.
[00:23:44.200 --> 00:23:50.760]   - I think the value of a third party Curie created display
[00:23:50.760 --> 00:23:54.560]   is probably pretty low for Google
[00:23:54.560 --> 00:23:57.520]   and it's probably pretty low for the third parties creating it.
[00:23:57.520 --> 00:24:00.760]   So Google's value, like, if you think about,
[00:24:00.760 --> 00:24:04.160]   like, was this an awesome seller for Lenovo?
[00:24:04.160 --> 00:24:07.800]   Maybe, but probably it's not as high margin
[00:24:07.800 --> 00:24:11.080]   as probably a tablet.
[00:24:11.080 --> 00:24:12.680]   I don't know.
[00:24:12.680 --> 00:24:17.280]   And if I look at Google, like their Nest Hub Max,
[00:24:17.280 --> 00:24:19.520]   no, their Nest Hub second generation,
[00:24:19.520 --> 00:24:22.920]   so the tiny Google one, they threw in thread,
[00:24:22.920 --> 00:24:24.440]   they threw in solely,
[00:24:24.440 --> 00:24:28.480]   they may have thrown in other things we don't know about yet.
[00:24:28.480 --> 00:24:30.320]   For them, it may be kind of like the Pixel,
[00:24:30.320 --> 00:24:32.640]   so like the display is just like this cool way
[00:24:32.640 --> 00:24:34.760]   to test out new technologies
[00:24:34.760 --> 00:24:38.200]   that, I mean, in solely didn't even really go anywhere.
[00:24:38.200 --> 00:24:43.000]   They open sourced that through Ripple and who knows?
[00:24:43.000 --> 00:24:46.720]   - They didn't really, the smart displays that they did,
[00:24:46.720 --> 00:24:47.560]   right?
[00:24:47.560 --> 00:24:48.400]   Didn't solely end up.
[00:24:48.400 --> 00:24:49.240]   - Yeah, I have it.
[00:24:49.240 --> 00:24:50.720]   It's the one that's sitting at,
[00:24:50.720 --> 00:24:52.960]   I'm looking at it right now at my desk.
[00:24:52.960 --> 00:24:55.480]   And it does things.
[00:24:55.480 --> 00:24:58.160]   - I think it does.
[00:24:58.160 --> 00:25:01.160]   So wipe left on stuff and it tracks my sleep
[00:25:01.160 --> 00:25:03.360]   if I slept in this room, but that's about it.
[00:25:03.360 --> 00:25:07.560]   I don't know, maybe there's just no value.
[00:25:07.560 --> 00:25:10.560]   - Yeah.
[00:25:10.560 --> 00:25:15.080]   - Yeah, I think the voice is part of this too.
[00:25:15.080 --> 00:25:17.320]   Madamie and all that stuff.
[00:25:17.320 --> 00:25:20.080]   I just think that the blushes off that rose.
[00:25:20.080 --> 00:25:20.920]   - Yeah.
[00:25:20.920 --> 00:25:23.280]   - And like so many things, I was a hype cycle.
[00:25:23.280 --> 00:25:27.280]   A hype cycle was longer on this than it was on other things.
[00:25:27.280 --> 00:25:28.880]   But it was still a hype cycle.
[00:25:28.880 --> 00:25:31.560]   - I don't know if there is a legitimate role
[00:25:31.560 --> 00:25:33.680]   for voice as an interaction.
[00:25:33.680 --> 00:25:38.440]   And I feel like people are confusing a digital assistant
[00:25:38.440 --> 00:25:42.160]   with using voice as a means of interacting with something.
[00:25:42.160 --> 00:25:43.160]   And we are seeing this a lot,
[00:25:43.160 --> 00:25:45.360]   especially with all the generative AI talk.
[00:25:45.360 --> 00:25:49.400]   - Is that a weird time then Stacey or just go away
[00:25:49.400 --> 00:25:53.440]   just when chat GPT at all would make conversation more feasible?
[00:25:53.440 --> 00:25:54.280]   - No.
[00:25:54.280 --> 00:25:57.120]   - Well, but chat GPT generates text,
[00:25:57.120 --> 00:25:58.440]   which is terrible to read.
[00:25:58.440 --> 00:26:00.320]   Well, I guess it would be good for a display,
[00:26:00.320 --> 00:26:03.040]   but probably also like-- - Because they can be audio easily.
[00:26:03.040 --> 00:26:05.440]   - Oh, but it's so awful to hear audio.
[00:26:05.440 --> 00:26:07.960]   I mean, have you ever read or how,
[00:26:07.960 --> 00:26:11.360]   go read something generator by chat GPT allowed its,
[00:26:11.360 --> 00:26:14.760]   I mean, unless it's like really colloquial, it's horrible.
[00:26:14.760 --> 00:26:16.560]   It's like a bad podcast.
[00:26:16.560 --> 00:26:18.920]   Some things have been all right, right?
[00:26:18.920 --> 00:26:20.240]   - Okay.
[00:26:20.240 --> 00:26:21.080]   I don't know.
[00:26:21.080 --> 00:26:25.160]   Anyway, the point here is voice as an interaction
[00:26:25.160 --> 00:26:26.000]   is not going away.
[00:26:26.000 --> 00:26:29.440]   They're still going to talk to things and have them do things.
[00:26:29.440 --> 00:26:30.880]   Thinking that is the platform
[00:26:30.880 --> 00:26:32.640]   and that is enough to build some sort of
[00:26:32.640 --> 00:26:36.040]   multi-billion dollar business is not, that's not real.
[00:26:36.040 --> 00:26:38.120]   I mean, mice and keyboards are not
[00:26:38.120 --> 00:26:40.280]   multi-billion dollar businesses, right?
[00:26:40.280 --> 00:26:44.280]   So we have the interactive action
[00:26:44.280 --> 00:26:46.160]   and then we have whatever the business is.
[00:26:46.160 --> 00:26:49.800]   And I think Google is,
[00:26:49.800 --> 00:26:51.600]   if a display is part of that business,
[00:26:51.600 --> 00:26:53.680]   they wanna control it.
[00:26:53.680 --> 00:26:57.320]   And it may be that a display isn't a huge part
[00:26:57.320 --> 00:26:58.480]   of that business.
[00:26:58.480 --> 00:27:00.960]   - No. - I don't, I mean.
[00:27:00.960 --> 00:27:03.840]   - At one time it made more sense now, not so much.
[00:27:03.840 --> 00:27:08.400]   Although, interestingly, Google has its tablet
[00:27:08.400 --> 00:27:12.760]   that's right on the precipice that Google's gonna do the full,
[00:27:12.760 --> 00:27:17.760]   you know, the full throttle kind of sales pitch
[00:27:17.760 --> 00:27:20.280]   on why you need one of these things.
[00:27:20.280 --> 00:27:22.320]   And part of the why you need one of these things
[00:27:22.320 --> 00:27:24.920]   is that it becomes one of these devices
[00:27:24.920 --> 00:27:28.760]   that we're saying that Google is kind of diminishing
[00:27:28.760 --> 00:27:29.600]   in its business.
[00:27:29.600 --> 00:27:31.840]   So that's a little confusing.
[00:27:31.840 --> 00:27:34.280]   - Well, it's not diminishing displays,
[00:27:34.280 --> 00:27:37.000]   it's diminishing the third party participation in the--
[00:27:37.000 --> 00:27:39.320]   - Got it. - No, no, no, that's a good point.
[00:27:39.320 --> 00:27:41.040]   - Full ownership. - That is the difference.
[00:27:41.040 --> 00:27:41.880]   That is the difference.
[00:27:41.880 --> 00:27:44.200]   - 'Cause I think the displays have a really,
[00:27:44.200 --> 00:27:48.280]   like, like, there's a PIR sensor in the Lenovo
[00:27:48.280 --> 00:27:49.880]   and Google used that to like,
[00:27:49.880 --> 00:27:51.080]   I don't know if you'll remember this,
[00:27:51.080 --> 00:27:52.440]   but if you get closer to it,
[00:27:52.440 --> 00:27:55.400]   it actually shrinks, like, your weather down,
[00:27:55.400 --> 00:27:58.000]   but if you're far away, it gives you the temperature
[00:27:58.000 --> 00:28:00.160]   and like, what-- - Mm, oh, that's cool.
[00:28:00.160 --> 00:28:02.000]   - I love that, yeah. - Yeah.
[00:28:02.000 --> 00:28:04.040]   And there's, like, they have a glanceable thing,
[00:28:04.040 --> 00:28:07.280]   like, if I glance at it and I say, turn on the lights,
[00:28:07.280 --> 00:28:08.480]   it knows I'm looking at it,
[00:28:08.480 --> 00:28:09.880]   so it turns on the lights without me saying,
[00:28:09.880 --> 00:28:12.280]   "Hey, G, if I turn that feature on."
[00:28:12.280 --> 00:28:16.720]   So, I think Google probably saw some really cool things
[00:28:16.720 --> 00:28:18.360]   that it could do with this,
[00:28:18.360 --> 00:28:19.760]   but I think it probably also was like,
[00:28:19.760 --> 00:28:21.400]   probably should own the hardware
[00:28:21.400 --> 00:28:23.040]   to really make sure the software
[00:28:23.040 --> 00:28:25.680]   and hardware experience together is good
[00:28:25.680 --> 00:28:27.240]   for some of these newer use cases.
[00:28:27.240 --> 00:28:30.920]   'Cause, like, a glanceable display is cool,
[00:28:30.920 --> 00:28:32.880]   but you gotta educate the user
[00:28:32.880 --> 00:28:34.920]   before they're gonna know that, like,
[00:28:34.920 --> 00:28:37.240]   I'm looking at you and now I don't have to say anything,
[00:28:37.240 --> 00:28:38.840]   like, to wake you up, you know?
[00:28:38.840 --> 00:28:39.960]   - Right, right, yeah.
[00:28:39.960 --> 00:28:43.160]   - Is it that everything is gonna be handheld?
[00:28:43.160 --> 00:28:45.840]   - God, I hope not, that's the first.
[00:28:45.840 --> 00:28:48.120]   - The biggest email artist is imploding.
[00:28:48.120 --> 00:28:51.040]   It apples down 40% as of later down the rundown.
[00:28:51.040 --> 00:28:54.980]   It bales down 31%, Lenovo down 30%,
[00:28:54.980 --> 00:28:58.920]   ACE is down 30%, and that's why the chip story
[00:28:58.920 --> 00:29:01.320]   is in there, the memory chips are down.
[00:29:01.320 --> 00:29:02.760]   Is it the things that are plugged in
[00:29:02.760 --> 00:29:06.680]   or sitting on a surface just to assay and go away?
[00:29:06.680 --> 00:29:08.480]   And is that the larger trend here?
[00:29:08.480 --> 00:29:09.560]   I'm trying to see that.
[00:29:09.560 --> 00:29:12.800]   - I think when it comes to stuff that's plugged in,
[00:29:12.800 --> 00:29:15.560]   sales are probably down because
[00:29:15.560 --> 00:29:17.440]   there's so much better now.
[00:29:17.440 --> 00:29:21.120]   Things just work and work a few more years
[00:29:21.120 --> 00:29:22.640]   than what they're used to, you know?
[00:29:22.640 --> 00:29:25.360]   So why would you reinvest?
[00:29:25.360 --> 00:29:27.720]   I mean, look at the whole Apple M1 chip
[00:29:27.720 --> 00:29:30.560]   and people love that thing
[00:29:30.560 --> 00:29:32.040]   and they didn't necessarily feel the need
[00:29:32.040 --> 00:29:35.320]   to jump to M2 because it wasn't that much better
[00:29:35.320 --> 00:29:37.720]   and there was no need to really spend any extra money
[00:29:37.720 --> 00:29:42.720]   or get rid of the M1 hardware that they have, you know?
[00:29:42.720 --> 00:29:44.960]   'Cause it just works.
[00:29:44.960 --> 00:29:47.400]   - And the corporate PC market is a huge driver
[00:29:47.400 --> 00:29:52.400]   of some of these sales and with the confusion about layoffs
[00:29:52.400 --> 00:29:58.680]   and return to work, like those two things are making,
[00:29:58.680 --> 00:30:03.160]   like if I am a big company buying laptops for my employees,
[00:30:03.160 --> 00:30:05.680]   maybe I have a BYOD if they're gonna stay at home.
[00:30:05.680 --> 00:30:07.360]   Maybe I laid off 10,000 people
[00:30:07.360 --> 00:30:10.080]   so now I've got to warehouse full of computers.
[00:30:10.080 --> 00:30:11.720]   So those are also big factors
[00:30:11.720 --> 00:30:13.920]   that are probably driving down stock prices.
[00:30:13.920 --> 00:30:15.360]   - Yeah, indeed.
[00:30:15.360 --> 00:30:18.480]   Well, that is a fantastic Google story
[00:30:18.480 --> 00:30:20.480]   to start this day show off with.
[00:30:20.480 --> 00:30:21.320]   - A cool show.
[00:30:21.320 --> 00:30:22.200]   (laughing)
[00:30:22.200 --> 00:30:24.160]   - All right, we got more coming up.
[00:30:24.160 --> 00:30:26.000]   Yes, we've got some Google stories.
[00:30:26.000 --> 00:30:29.320]   We have a whole lot of other stories as well.
[00:30:29.320 --> 00:30:30.920]   So we're gonna get to that in a moment,
[00:30:30.920 --> 00:30:32.800]   but let's take a quick moment
[00:30:32.800 --> 00:30:35.280]   and thank the sponsor of this episode of This Week in Google
[00:30:35.280 --> 00:30:38.840]   and that is Melissa, the address experts
[00:30:38.840 --> 00:30:43.720]   from forums on a webpage to check out customer data comes
[00:30:43.720 --> 00:30:46.080]   as you know from many different places.
[00:30:46.080 --> 00:30:49.120]   It's not only leaving room for error,
[00:30:49.120 --> 00:30:52.920]   but it also allows for incomplete customer information.
[00:30:52.920 --> 00:30:53.920]   If you're running a business,
[00:30:53.920 --> 00:30:55.640]   you're collecting that information,
[00:30:55.640 --> 00:30:58.400]   you don't want incomplete information.
[00:30:58.400 --> 00:31:01.800]   With Melissa's Personator Consumer Tool,
[00:31:01.800 --> 00:31:05.640]   you can actually get a superior snapshot of your customers.
[00:31:05.640 --> 00:31:08.000]   And this enrichment of contact data actually
[00:31:08.000 --> 00:31:10.840]   can improve analytics and allow for better targeting
[00:31:10.840 --> 00:31:14.840]   and marketing campaigns, but also in helping
[00:31:14.840 --> 00:31:16.520]   with fraud detection,
[00:31:16.520 --> 00:31:19.480]   Personator Consumer not only appends data,
[00:31:19.480 --> 00:31:22.040]   but also verifies name, address,
[00:31:22.040 --> 00:31:25.000]   and contact information as well as data birth,
[00:31:25.000 --> 00:31:29.520]   social security numbers for quick identity verification check.
[00:31:29.520 --> 00:31:32.760]   It also enriches data to add geographic
[00:31:32.760 --> 00:31:34.840]   and demographic information for you.
[00:31:34.840 --> 00:31:38.040]   So not only are you verifying your customer,
[00:31:38.040 --> 00:31:41.720]   you're also able to provide them the best possible experience
[00:31:41.720 --> 00:31:43.240]   with your business.
[00:31:43.240 --> 00:31:46.400]   So when you have highly specific demographics,
[00:31:46.400 --> 00:31:48.160]   you can actually personalize your marketing,
[00:31:48.160 --> 00:31:51.800]   you can find the best leads, offer the best service,
[00:31:51.800 --> 00:31:55.440]   or the best deal for your customer situations.
[00:31:55.440 --> 00:31:59.640]   With Melissa, you gain access to appendable demographics,
[00:31:59.640 --> 00:32:02.440]   including, but of course not limited to,
[00:32:02.440 --> 00:32:04.920]   deceased information, occupation,
[00:32:04.920 --> 00:32:06.560]   presence of children,
[00:32:06.560 --> 00:32:10.960]   number of adults, marital status, homeowner, home renter,
[00:32:10.960 --> 00:32:14.080]   household income, and range,
[00:32:14.080 --> 00:32:18.720]   length of residence, just a whole lot more data points there.
[00:32:18.720 --> 00:32:23.440]   Melissa's address verification tools leverage 38 years,
[00:32:23.440 --> 00:32:26.760]   38 years of address verification expertise.
[00:32:26.760 --> 00:32:28.040]   And it's flexible as well,
[00:32:28.040 --> 00:32:30.680]   it fits into any business model.
[00:32:30.680 --> 00:32:33.520]   Melissa's global service can verify addresses
[00:32:33.520 --> 00:32:36.920]   for 240 countries and counting, by the way,
[00:32:36.920 --> 00:32:40.320]   to ensure only valid billing and shipping addresses
[00:32:40.320 --> 00:32:41.640]   enter your system.
[00:32:41.640 --> 00:32:42.760]   That's really what you want.
[00:32:42.760 --> 00:32:43.760]   You want the valid stuff,
[00:32:43.760 --> 00:32:46.400]   you want the stuff that checks out, right?
[00:32:46.400 --> 00:32:49.320]   Melissa is SOC2 HIPAA and GDPR compliant,
[00:32:49.320 --> 00:32:53.920]   so you know your data is always in the best hands.
[00:32:53.920 --> 00:32:56.080]   They just, I mean, they just know
[00:32:56.080 --> 00:32:57.520]   exactly what they're doing here,
[00:32:57.520 --> 00:32:59.600]   so you can trust Melissa.
[00:32:59.600 --> 00:33:02.600]   Make sure your customer contact data is up to date,
[00:33:02.600 --> 00:33:06.560]   get started today with 1000 records, clean for free.
[00:33:06.560 --> 00:33:10.320]   All you have to do is go to Melissa.com/twit,
[00:33:10.320 --> 00:33:11.560]   you can check it out for yourself,
[00:33:11.560 --> 00:33:14.640]   see why they are the experts in this stuff,
[00:33:14.640 --> 00:33:17.680]   that's Melissa.com/twit.
[00:33:17.680 --> 00:33:19.720]   And we thank Melissa for their support
[00:33:19.720 --> 00:33:23.040]   of this week in Google.
[00:33:23.040 --> 00:33:27.400]   All right, so before we get into any of the other things,
[00:33:27.400 --> 00:33:32.400]   we might as well hit the other Google stories
[00:33:32.720 --> 00:33:35.400]   there's the NFL Sunday ticket.
[00:33:35.400 --> 00:33:37.560]   - I wanted to hear answer reaction to this pricing,
[00:33:37.560 --> 00:33:38.480]   that's why I put that in here.
[00:33:38.480 --> 00:33:40.560]   - Yeah, first of all fans.
[00:33:40.560 --> 00:33:43.040]   - And it was a cheap, what are the prices Jason?
[00:33:43.040 --> 00:33:45.320]   - $349 a year.
[00:33:45.320 --> 00:33:47.840]   - Guess you are.
[00:33:47.840 --> 00:33:51.080]   - So that is the Sunday package,
[00:33:51.080 --> 00:33:54.960]   so basically if you are a YouTube TV base plan subscriber,
[00:33:54.960 --> 00:33:59.080]   you can add the Sunday package, $349 a year.
[00:33:59.080 --> 00:34:02.240]   You can do a bundle with NFL red zone,
[00:34:02.240 --> 00:34:03.640]   trying to remember what red zone is,
[00:34:03.640 --> 00:34:07.080]   is that the thing where they show you only the scores
[00:34:07.080 --> 00:34:09.000]   that happen on all of the games that are playing,
[00:34:09.000 --> 00:34:09.840]   is that right?
[00:34:09.840 --> 00:34:13.160]   - From the 25 yard line in this red zone.
[00:34:13.160 --> 00:34:15.840]   - God, so it's like all the action of all the games
[00:34:15.840 --> 00:34:19.080]   happening at one point, and that's $389 a year,
[00:34:19.080 --> 00:34:22.800]   so a $40 premium gets you that extra thing.
[00:34:22.800 --> 00:34:27.320]   Non-subscribers can pay another $100 a season
[00:34:27.320 --> 00:34:29.200]   for either package,
[00:34:29.200 --> 00:34:33.000]   Google's offering another $100 discount for anyone signing up
[00:34:33.000 --> 00:34:35.280]   during a presale period.
[00:34:35.280 --> 00:34:38.440]   How does this compare to direct TVs offering?
[00:34:38.440 --> 00:34:41.160]   It's been many years since I lasted
[00:34:41.160 --> 00:34:43.240]   the Sunday ticket on direct TV.
[00:34:43.240 --> 00:34:45.960]   - It's been many years for me, but I swear,
[00:34:45.960 --> 00:34:48.440]   I thought I was paying about $300
[00:34:48.440 --> 00:34:53.440]   on top of my regular monthly bill with the red TV at the time,
[00:34:53.440 --> 00:34:54.760]   or whoever it was.
[00:34:54.760 --> 00:34:56.600]   - Yeah, it was the TV.
[00:34:56.600 --> 00:34:59.040]   And this sounds pretty much the same thing,
[00:34:59.040 --> 00:35:01.600]   and I thought it was expensive back then.
[00:35:01.600 --> 00:35:02.440]   - Yeah.
[00:35:02.440 --> 00:35:05.400]   - I did say it was too expensive now, in my opinion.
[00:35:05.400 --> 00:35:08.040]   I don't see the deal here.
[00:35:08.040 --> 00:35:09.200]   - Wow.
[00:35:09.200 --> 00:35:11.720]   So Scooter X has the stat here.
[00:35:11.720 --> 00:35:18.280]   Direct TV since 1994 charged $293.94 to be specific.
[00:35:18.280 --> 00:35:20.680]   - Oh, oh, that's so exactly.
[00:35:20.680 --> 00:35:22.000]   - Per season for the base back.
[00:35:22.000 --> 00:35:26.120]   - $395.94 per season for the NFL Sunday ticket,
[00:35:26.120 --> 00:35:29.400]   Max package, which has the extra content.
[00:35:29.400 --> 00:35:32.760]   So it almost seems like your standard baseline entry
[00:35:32.760 --> 00:35:34.360]   into this is more expensive,
[00:35:34.360 --> 00:35:36.640]   but if you're getting red zone less.
[00:35:36.640 --> 00:35:39.720]   - Does this catch you all these Sunday games?
[00:35:39.720 --> 00:35:40.800]   This does not get-- - Yes.
[00:35:40.800 --> 00:35:41.640]   - Yeah.
[00:35:41.640 --> 00:35:42.480]   - Right.
[00:35:42.480 --> 00:35:44.520]   - Because Monday Night Football is right.
[00:35:44.520 --> 00:35:45.440]   - It's a franchise, right?
[00:35:45.440 --> 00:35:46.280]   - Yeah.
[00:35:46.280 --> 00:35:47.800]   - This is whole different thing.
[00:35:47.800 --> 00:35:49.320]   Monday Night and Thursday are there,
[00:35:49.320 --> 00:35:50.760]   but two separate things in the game.
[00:35:50.760 --> 00:35:54.040]   - Can you watch any game of the NFL on Sunday with this?
[00:35:54.040 --> 00:35:54.880]   - Yes.
[00:35:54.880 --> 00:35:57.200]   - You get all the out of market stuff,
[00:35:57.200 --> 00:35:59.520]   excluding the things that are in your area
[00:35:59.520 --> 00:36:01.600]   because those will be on your local channels.
[00:36:01.600 --> 00:36:02.440]   - Right.
[00:36:02.440 --> 00:36:03.260]   - Got it.
[00:36:03.260 --> 00:36:05.040]   - And you can't just sign up for a package
[00:36:05.040 --> 00:36:06.960]   that only follows a specific team
[00:36:06.960 --> 00:36:09.680]   as much as people would probably love to do that.
[00:36:09.680 --> 00:36:10.520]   - Correct.
[00:36:10.520 --> 00:36:13.600]   - It would be nice to have some type of allocart option on it,
[00:36:13.600 --> 00:36:14.760]   but-- - Wouldn't it always be nice
[00:36:14.760 --> 00:36:16.760]   to have some sort of allocart option?
[00:36:16.760 --> 00:36:20.240]   - I don't, we're in this mess because of allocart.
[00:36:20.240 --> 00:36:21.080]   - Yeah, right.
[00:36:21.080 --> 00:36:21.920]   - Are we?
[00:36:21.920 --> 00:36:25.920]   - We're like, we broke up big cable, big television.
[00:36:25.920 --> 00:36:27.440]   I don't know.
[00:36:27.440 --> 00:36:31.440]   - Now, this close to allocart I get is with YouTube TV.
[00:36:31.440 --> 00:36:34.440]   I can add to my library quote unquote,
[00:36:34.440 --> 00:36:36.440]   I think this is how it says, "Add to your library,
[00:36:36.440 --> 00:36:37.880]   add F1."
[00:36:37.880 --> 00:36:40.440]   And so it automatically just gives me F1 stuff
[00:36:40.440 --> 00:36:42.360]   that I wanna see, or Clemson.
[00:36:42.360 --> 00:36:45.040]   And it automatically records Clemson content.
[00:36:45.040 --> 00:36:46.440]   I don't have to scroll through stuff.
[00:36:46.440 --> 00:36:47.880]   I literally just go to my library
[00:36:47.880 --> 00:36:49.480]   and boom, it's right there.
[00:36:49.480 --> 00:36:51.080]   - I remember that feature on Tivo.
[00:36:51.920 --> 00:36:53.600]   - Yeah, and I love that.
[00:36:53.600 --> 00:36:56.480]   But again, with the whole Sunday ticket thing,
[00:36:56.480 --> 00:36:59.880]   it's overpriced, man, it's way overpriced.
[00:36:59.880 --> 00:37:01.800]   - But you know, football fans, they'll pay it
[00:37:01.800 --> 00:37:04.080]   because they're such super fans, right?
[00:37:04.080 --> 00:37:05.240]   - Will they?
[00:37:05.240 --> 00:37:06.480]   - Oh, that's a question.
[00:37:06.480 --> 00:37:11.000]   I think some people will pay it.
[00:37:11.000 --> 00:37:14.360]   I think it'll be hard to price because if you've got,
[00:37:14.360 --> 00:37:17.160]   if you want most people who are casual football fans,
[00:37:17.160 --> 00:37:19.280]   it probably needs to be a little cheaper.
[00:37:19.280 --> 00:37:22.520]   But if you can offset that audience gain
[00:37:22.520 --> 00:37:24.840]   by charging a lot more for the die-hards,
[00:37:24.840 --> 00:37:26.240]   then it makes sense to be like,
[00:37:26.240 --> 00:37:29.240]   well, screw you middle-market fans
[00:37:29.240 --> 00:37:30.920]   who really care that much
[00:37:30.920 --> 00:37:32.920]   and just really focus on the high-end.
[00:37:32.920 --> 00:37:34.360]   I think it's really fascinating.
[00:37:34.360 --> 00:37:36.480]   It would be really hard to price all this right now.
[00:37:36.480 --> 00:37:38.200]   I'm glad it's not me.
[00:37:38.200 --> 00:37:40.200]   - I love NFL, love it.
[00:37:40.200 --> 00:37:41.640]   I'm looking forward to watching.
[00:37:41.640 --> 00:37:43.480]   I love college football more.
[00:37:43.480 --> 00:37:44.680]   I'll be clear about that.
[00:37:44.680 --> 00:37:47.480]   But I do look forward to watching games on Sunday
[00:37:47.480 --> 00:37:51.320]   and the drama that builds up after the halfway point
[00:37:51.320 --> 00:37:53.320]   in the season because the games become more important
[00:37:53.320 --> 00:37:56.040]   because of playoffs, so I really love that.
[00:37:56.040 --> 00:37:59.520]   But I don't love it $300 worth.
[00:37:59.520 --> 00:38:00.520]   Keep that in my pocket.
[00:38:00.520 --> 00:38:04.880]   I could go to a couple of games and watch in person.
[00:38:04.880 --> 00:38:09.040]   - Well, would you go to like a sports bar or something?
[00:38:09.040 --> 00:38:09.880]   I mean, like is there a number?
[00:38:09.880 --> 00:38:11.920]   - No, I would go right down here to Santa Clara
[00:38:11.920 --> 00:38:12.920]   to watch a game.
[00:38:12.920 --> 00:38:13.760]   - Oh, that's right.
[00:38:13.760 --> 00:38:16.840]   - You know, it's go to LA to watch a game.
[00:38:17.680 --> 00:38:20.640]   It seems like I'd get more mileage,
[00:38:20.640 --> 00:38:25.640]   more a better value person versus spending that extra $300.
[00:38:25.640 --> 00:38:26.800]   - And you get beer.
[00:38:26.800 --> 00:38:27.800]   - That's right.
[00:38:27.800 --> 00:38:30.080]   - And you get beer and you get--
[00:38:30.080 --> 00:38:30.920]   - I'm focused.
[00:38:30.920 --> 00:38:34.000]   - You don't always get the best of commentary
[00:38:34.000 --> 00:38:37.800]   on all of those different games in different regions
[00:38:37.800 --> 00:38:41.280]   because for me, watching football is,
[00:38:41.280 --> 00:38:44.120]   I love watching the game, but I do love the aspect
[00:38:44.120 --> 00:38:47.640]   of having legendary broadcasters like Keith Jackson
[00:38:47.640 --> 00:38:51.800]   in college football or college basketball, Dick Vitale,
[00:38:51.800 --> 00:38:53.320]   and listening to him tell stories
[00:38:53.320 --> 00:38:55.360]   about the diaper dandies and things like that.
[00:38:55.360 --> 00:38:58.000]   And when you water it down to where you get access
[00:38:58.000 --> 00:39:01.000]   to all of those games, you're paying for that extra vig
[00:39:01.000 --> 00:39:03.760]   to see the extra games, but you're gonna lose out
[00:39:03.760 --> 00:39:06.720]   on some of the little nuance of the best broadcasters
[00:39:06.720 --> 00:39:09.200]   not gonna be on that game, because it's not the game
[00:39:09.200 --> 00:39:11.800]   of the week, it's just somebody you wanna watch.
[00:39:13.480 --> 00:39:14.880]   - So-- - I was curious.
[00:39:14.880 --> 00:39:19.560]   - $380, what was it, $389, right?
[00:39:19.560 --> 00:39:24.480]   $389 with Red Zone, so three, okay, so $349.
[00:39:24.480 --> 00:39:27.880]   A lot of people are not gonna do the Red Zone, that's $350.
[00:39:27.880 --> 00:39:32.880]   How many months are there solid Sunday games happening?
[00:39:32.880 --> 00:39:36.680]   That's September, October, November, December, January, right?
[00:39:36.680 --> 00:39:37.680]   Five. - Five.
[00:39:37.680 --> 00:39:38.520]   - So divide them into five.
[00:39:38.520 --> 00:39:40.320]   - Well, I know this clearly because I have to give up
[00:39:40.320 --> 00:39:42.440]   seven months a year to my family.
[00:39:42.440 --> 00:39:43.600]   - Yeah, five.
[00:39:43.600 --> 00:39:47.480]   - There was a time, there was a time when I was really,
[00:39:47.480 --> 00:39:49.520]   my wife and I were really into the NFL.
[00:39:49.520 --> 00:39:50.960]   I mean, we bought the Sunday ticket
[00:39:50.960 --> 00:39:52.800]   probably five seasons in a row.
[00:39:52.800 --> 00:39:55.560]   I was getting really into fantasy football,
[00:39:55.560 --> 00:39:57.360]   I'd never done that before.
[00:39:57.360 --> 00:40:00.720]   And so that, people who are in fantasy football
[00:40:00.720 --> 00:40:03.200]   like really into it, they're the ones that are fine
[00:40:03.200 --> 00:40:08.200]   spending $17.00 a week for all of the games.
[00:40:08.200 --> 00:40:10.280]   - 'Cause that's really what it breaks down to.
[00:40:10.280 --> 00:40:12.560]   - Let me throw this at you, Mr. Howell.
[00:40:12.560 --> 00:40:15.520]   That, I could see that being way more important
[00:40:15.520 --> 00:40:18.640]   a decade ago, the fantasy football players.
[00:40:18.640 --> 00:40:23.360]   Now, what fantasy football, the stats and all of that stuff
[00:40:23.360 --> 00:40:25.040]   are so instant.
[00:40:25.040 --> 00:40:26.160]   - Yeah, right. - Really have to go
[00:40:26.160 --> 00:40:28.280]   and look at the game, they pop up on your screen.
[00:40:28.280 --> 00:40:30.320]   - That's true. - And inside of your score
[00:40:30.320 --> 00:40:32.560]   and inside of whatever fantasy football app
[00:40:32.560 --> 00:40:34.760]   because of all the APIs, that stuff is instant.
[00:40:34.760 --> 00:40:36.760]   You really don't have to watch the game.
[00:40:36.760 --> 00:40:39.840]   - But it elevates the fun of watching the game
[00:40:39.840 --> 00:40:41.800]   when you got something on the line.
[00:40:41.800 --> 00:40:43.480]   At least that was my experience back then.
[00:40:43.480 --> 00:40:45.520]   - You feel like you have skin in the game?
[00:40:45.520 --> 00:40:46.520]   - Yeah, right.
[00:40:46.520 --> 00:40:48.880]   You actually care when suddenly that player
[00:40:48.880 --> 00:40:52.240]   goes in for a touchdown, it's that much, you know,
[00:40:52.240 --> 00:40:54.160]   awesomer, you know, when it happens.
[00:40:54.160 --> 00:40:56.080]   - I love it. - And, yeah.
[00:40:56.080 --> 00:40:58.880]   Back then, it made a lot of sense to us.
[00:40:58.880 --> 00:41:01.320]   I mean, it was still felt expensive,
[00:41:01.320 --> 00:41:03.640]   but, you know, we were enjoying it.
[00:41:03.640 --> 00:41:05.360]   So we were okay with it.
[00:41:05.360 --> 00:41:08.920]   But yeah, this is, that's pretty pricey.
[00:41:08.920 --> 00:41:13.920]   I wouldn't pay $20 a week for many things in life.
[00:41:13.920 --> 00:41:16.360]   I mean, that's a pricey weekly cost.
[00:41:16.360 --> 00:41:17.600]   And it's not just a week.
[00:41:17.600 --> 00:41:19.440]   It's like a day of a week.
[00:41:19.440 --> 00:41:20.360]   It's a single day.
[00:41:20.360 --> 00:41:21.200]   So. - Right.
[00:41:21.200 --> 00:41:23.720]   A single day that is a week.
[00:41:23.720 --> 00:41:27.240]   And roughly seven hours.
[00:41:27.240 --> 00:41:29.720]   - Yep. - Is it an annual package?
[00:41:29.720 --> 00:41:32.080]   Or is it like, I know like--
[00:41:32.080 --> 00:41:34.000]   - But annual is five months.
[00:41:34.000 --> 00:41:34.840]   - You ever did, man? - Okay.
[00:41:34.840 --> 00:41:36.200]   - From those five months.
[00:41:36.200 --> 00:41:39.280]   - But like MLB, you can sign, the later you sign up
[00:41:39.280 --> 00:41:41.760]   in the season, it's like a pro-rated package.
[00:41:41.760 --> 00:41:42.800]   So I didn't know if you could like,
[00:41:42.800 --> 00:41:44.680]   if you cared about. - Oh, I see.
[00:41:44.680 --> 00:41:49.680]   - I've never seen it be marked down like that in the past.
[00:41:49.680 --> 00:41:52.840]   I don't know if it's like that within the last couple of years.
[00:41:52.840 --> 00:41:54.960]   So back in the day, since I tried that,
[00:41:54.960 --> 00:41:56.480]   back in the direct TV days.
[00:41:56.480 --> 00:41:57.920]   And they're like, not give it up,
[00:41:57.920 --> 00:41:59.840]   or you can't sign up now.
[00:41:59.840 --> 00:42:00.680]   It's too late.
[00:42:00.680 --> 00:42:01.520]   - Mm-hmm.
[00:42:01.520 --> 00:42:05.240]   - I also should mention that this wasn't the only
[00:42:05.240 --> 00:42:09.080]   kind of announcement that yes, there was the NFL Sunday ticket,
[00:42:09.080 --> 00:42:14.080]   but also Google is adding, looks like 800 fast.
[00:42:14.080 --> 00:42:19.480]   That is free advertising supported television platforms
[00:42:19.480 --> 00:42:20.720]   to Google TV.
[00:42:20.720 --> 00:42:24.600]   So 800 basically free streaming ad supported stations.
[00:42:24.600 --> 00:42:26.880]   So thank the like Fox is 2B. - So how about all of them?
[00:42:26.880 --> 00:42:28.920]   - Or some of those. - Pluto TV, you know,
[00:42:28.920 --> 00:42:31.520]   things like that that have free movies,
[00:42:31.520 --> 00:42:33.120]   but you gotta sit through, you know,
[00:42:33.120 --> 00:42:35.840]   some ads that are streamed throughout.
[00:42:35.840 --> 00:42:39.200]   - So it's like watching over the air television.
[00:42:39.200 --> 00:42:41.280]   - Yeah. - Back in the 'A's when I was a kid.
[00:42:41.280 --> 00:42:42.280]   - Right.
[00:42:42.280 --> 00:42:45.400]   - Right, except the ads that you see are the same ads
[00:42:45.400 --> 00:42:47.560]   repeated throughout the course of the movie.
[00:42:47.560 --> 00:42:50.400]   This is what I've noticed sometimes.
[00:42:50.400 --> 00:42:53.160]   It's like, okay, cool, I'm seeing this ad for the third time.
[00:42:53.160 --> 00:42:55.000]   Ah, shoot me, you know, I gotta.
[00:42:55.000 --> 00:42:57.320]   - That's what you do here. - That's what you do here.
[00:42:57.320 --> 00:42:58.840]   - Lose now.
[00:42:58.840 --> 00:43:00.760]   - That's when you get your snacks.
[00:43:00.760 --> 00:43:01.600]   - Yeah, that's true.
[00:43:01.600 --> 00:43:03.200]   - When you get your waffle, yeah.
[00:43:03.200 --> 00:43:06.280]   - That's true. - Yeah, I like those services.
[00:43:06.280 --> 00:43:08.240]   The 2B's and stuff like that.
[00:43:08.240 --> 00:43:10.400]   Do y'all find those services useful?
[00:43:10.400 --> 00:43:13.840]   I mean, granted it's probably not for,
[00:43:13.840 --> 00:43:16.520]   yeah, like us that have privilege, but.
[00:43:16.520 --> 00:43:18.960]   - Right. - Do you enjoy those services?
[00:43:18.960 --> 00:43:23.960]   - I accidentally talked to you for, I don't know,
[00:43:23.960 --> 00:43:26.960]   $20 a year and it has ads.
[00:43:26.960 --> 00:43:30.580]   And I did it because Parks and Rec is like my therapy.
[00:43:31.580 --> 00:43:34.100]   And it was, it makes me sad.
[00:43:34.100 --> 00:43:36.220]   Like I resent every time a commercial comes.
[00:43:36.220 --> 00:43:37.980]   So. - Which one was that?
[00:43:37.980 --> 00:43:40.780]   You broke up for a second. - I run out of rent.
[00:43:40.780 --> 00:43:42.460]   - Which services that you broke up for a second?
[00:43:42.460 --> 00:43:45.660]   - Yeah, I see my peacock.
[00:43:45.660 --> 00:43:47.340]   - Oh, peacock, yeah. - Okay.
[00:43:47.340 --> 00:43:48.660]   - Yeah.
[00:43:48.660 --> 00:43:51.380]   Yeah, we've gotten so accustomed to not having ads
[00:43:51.380 --> 00:43:53.420]   that at least this is how I feel.
[00:43:53.420 --> 00:43:55.300]   I've gotten so used to not having ads.
[00:43:55.300 --> 00:43:57.420]   We have so, you're right, it is a privilege.
[00:43:57.420 --> 00:43:59.540]   We have so much variety.
[00:43:59.540 --> 00:44:02.620]   You know, I've got Netflix, got Amazon Prime.
[00:44:02.620 --> 00:44:08.420]   So I've got places to go and go to when I wanna scratch
[00:44:08.420 --> 00:44:11.100]   that streaming video itch.
[00:44:11.100 --> 00:44:14.860]   And I'm so used to not seeing ads anymore.
[00:44:14.860 --> 00:44:16.500]   It used to be part of my reality,
[00:44:16.500 --> 00:44:19.820]   but now when it happens, totally, I feel the same way Stacey.
[00:44:19.820 --> 00:44:21.820]   It's kind of like, are you kidding me?
[00:44:21.820 --> 00:44:25.140]   Like it's almost unbearable to sit through them now
[00:44:25.140 --> 00:44:28.020]   and it used to just be the way it was.
[00:44:28.020 --> 00:44:29.140]   - Right. - Right.
[00:44:29.140 --> 00:44:30.220]   Now it's a lot harder.
[00:44:30.220 --> 00:44:32.180]   - Yeah.
[00:44:32.180 --> 00:44:34.700]   - You know, my problem with those services is
[00:44:34.700 --> 00:44:39.540]   the ad insertion points are always wonky.
[00:44:39.540 --> 00:44:41.820]   You know, versus watching it on regular broadcast.
[00:44:41.820 --> 00:44:42.660]   - Totally.
[00:44:42.660 --> 00:44:45.940]   - The segment break is designed for it, you know?
[00:44:45.940 --> 00:44:49.420]   But whatever happens on 2B or the other services out there,
[00:44:49.420 --> 00:44:52.940]   it just, they literally just, oh, here's your ad now.
[00:44:52.940 --> 00:44:54.260]   - It's very jarring.
[00:44:54.260 --> 00:44:57.740]   Not well placed at all.
[00:44:57.740 --> 00:44:59.900]   And that does kind of add to the frustration.
[00:44:59.900 --> 00:45:01.660]   I totally agree.
[00:45:01.660 --> 00:45:03.900]   - It's weird as a former TV critic.
[00:45:03.900 --> 00:45:07.780]   I have MSNBC on all day.
[00:45:07.780 --> 00:45:09.220]   I hardly watch news.
[00:45:09.220 --> 00:45:12.020]   I watched the succession, the first two episodes.
[00:45:12.020 --> 00:45:13.260]   Of course I won't say anything.
[00:45:13.260 --> 00:45:14.420]   - Yeah, I can't. - Don't spoil it.
[00:45:14.420 --> 00:45:15.260]   I gotta-- - Don't spoil it.
[00:45:15.260 --> 00:45:16.420]   - Don't spoil it. - It's horrific.
[00:45:16.420 --> 00:45:17.260]   - It's horrific.
[00:45:17.260 --> 00:45:20.420]   But I just don't, well, you know, I try Netflix Netflix.
[00:45:20.420 --> 00:45:23.060]   Everything on Netflix is so dark.
[00:45:23.060 --> 00:45:27.020]   Everything on, the acting is bad, the production is bad.
[00:45:27.020 --> 00:45:28.540]   That's the high end.
[00:45:28.540 --> 00:45:30.180]   So you think you can go with the free crap.
[00:45:30.180 --> 00:45:32.700]   - The finest acting you can buy in Canada.
[00:45:32.700 --> 00:45:34.380]   That's what my friend calls it.
[00:45:34.380 --> 00:45:35.860]   (laughing)
[00:45:35.860 --> 00:45:39.300]   For the finest Canadian acting that money can buy.
[00:45:39.300 --> 00:45:40.460]   - About right.
[00:45:40.460 --> 00:45:42.380]   - All right. - Oof.
[00:45:42.380 --> 00:45:43.540]   - Oof, boy. - See, that's why
[00:45:43.540 --> 00:45:46.140]   I don't watch a lot of documentaries and stuff.
[00:45:46.140 --> 00:45:48.420]   - Yeah. - On Netflix or whatever
[00:45:48.420 --> 00:45:50.460]   these services are because I don't have to worry
[00:45:50.460 --> 00:45:53.220]   about crappy storylines or crappy acting.
[00:45:53.220 --> 00:45:57.100]   - It's hard not to like a documentary.
[00:45:57.100 --> 00:45:59.100]   - I hate documentaries. - Just the information.
[00:45:59.100 --> 00:45:59.940]   - Wait, what?
[00:45:59.940 --> 00:46:02.100]   You know, just the information. - It makes fun of the way.
[00:46:02.100 --> 00:46:04.540]   - Oh, I love documentaries. - Thank you so much.
[00:46:04.540 --> 00:46:06.780]   - How do you hate documentaries?
[00:46:06.780 --> 00:46:08.740]   Don't you love learning things while watching?
[00:46:08.740 --> 00:46:09.900]   - I'm with Stacy.
[00:46:09.900 --> 00:46:10.900]   I'm with Stacy.
[00:46:10.900 --> 00:46:12.060]   - Really? - I can't stand.
[00:46:12.060 --> 00:46:12.900]   What's the name? - Yes, boss.
[00:46:12.900 --> 00:46:14.460]   - Who's the Ken Burns?
[00:46:14.460 --> 00:46:16.900]   I drives me insane.
[00:46:16.900 --> 00:46:17.900]   I'm doing a nice, nice, nice-- - I do like it.
[00:46:17.900 --> 00:46:19.060]   - I do like it. - I do like it.
[00:46:19.060 --> 00:46:21.340]   - It's a little bit of a-- - I do like to be entertained,
[00:46:21.340 --> 00:46:26.340]   but I have much higher success rate of being entertained
[00:46:26.340 --> 00:46:29.620]   and satisfied at the end of the presentation
[00:46:29.620 --> 00:46:33.180]   of a documentary versus a movie that got overhyped
[00:46:33.180 --> 00:46:35.780]   by all the critics and everybody in social media,
[00:46:35.780 --> 00:46:37.900]   "Oh, this is great," and you go and watch it
[00:46:37.900 --> 00:46:42.700]   and you just waste it two hours of, you know, man.
[00:46:42.700 --> 00:46:47.700]   - Documentaries, they feel like something that they're not.
[00:46:47.700 --> 00:46:51.220]   So they pretend to be all objective,
[00:46:51.220 --> 00:46:53.620]   but they're really just pulling your emotional heartstrings
[00:46:53.620 --> 00:46:55.500]   the same as that. - Oh yeah, oh yeah.
[00:46:55.500 --> 00:46:58.020]   - They all have it. - I just,
[00:46:58.020 --> 00:47:00.540]   I don't feel like they're trustworthy.
[00:47:00.540 --> 00:47:04.460]   So why, like, they're crappy as a narrative device.
[00:47:04.460 --> 00:47:05.740]   Well, they're not crappy, but they're like,
[00:47:05.740 --> 00:47:07.980]   not as good as a finely written narrative device,
[00:47:07.980 --> 00:47:10.500]   like a fake, like a fiction device, right?
[00:47:10.500 --> 00:47:13.800]   And I can't trust the facts that they're giving me.
[00:47:13.800 --> 00:47:17.740]   And maybe a nature documentary is like an exception here,
[00:47:17.740 --> 00:47:19.300]   but those I don't care for,
[00:47:19.300 --> 00:47:22.500]   'cause they just, I don't like nature.
[00:47:22.500 --> 00:47:24.260]   I really don't like documentaries.
[00:47:24.260 --> 00:47:27.500]   - What? - You give me like a British accent.
[00:47:27.500 --> 00:47:30.340]   - You give me a British accent. - @david@mer.
[00:47:30.340 --> 00:47:33.940]   - Talk about the different, very animals out in the wilderness
[00:47:33.940 --> 00:47:35.700]   and the soft music. - I can't.
[00:47:35.700 --> 00:47:38.700]   - It makes me mix slower than you would expect
[00:47:38.700 --> 00:47:41.300]   to kind of like lull you into this like hypnosis.
[00:47:41.300 --> 00:47:42.980]   - Yes, talk to me.
[00:47:42.980 --> 00:47:43.820]   - Yes.
[00:47:43.820 --> 00:47:45.620]   - And then something dies.
[00:47:45.620 --> 00:47:47.860]   I mean, and then when you have the people,
[00:47:47.860 --> 00:47:49.980]   you know, a lot of times they're telling you something
[00:47:49.980 --> 00:47:53.540]   that's just utterly demoralizing and frustrating and sad,
[00:47:53.540 --> 00:47:55.700]   just no, just-- - Oh, maybe you're watching
[00:47:55.700 --> 00:47:57.300]   the wrong documentaries.
[00:47:57.300 --> 00:47:58.180]   Maybe that's it.
[00:47:58.180 --> 00:48:00.940]   - I enjoyed "Hands on a Hardbody."
[00:48:00.940 --> 00:48:03.460]   That was the last documentary I actually enjoyed.
[00:48:03.460 --> 00:48:06.020]   That's how long ago it's been
[00:48:06.020 --> 00:48:08.420]   since I've had a good documentary.
[00:48:08.420 --> 00:48:09.940]   - Yeah, I don't know.
[00:48:09.940 --> 00:48:13.300]   I find document, I couldn't name one off the top of my head.
[00:48:13.300 --> 00:48:14.900]   I don't know the last time I watched a documentary,
[00:48:14.900 --> 00:48:17.820]   but when I do, I enjoy them.
[00:48:17.820 --> 00:48:19.780]   Like, I don't know what it is about a documentary.
[00:48:19.780 --> 00:48:22.020]   Like, I guess I haven't really spent a lot of time
[00:48:22.020 --> 00:48:25.340]   thinking about kind of like what you're talking about, Stacey.
[00:48:25.340 --> 00:48:28.140]   Like, what is the intention, the narrative?
[00:48:28.140 --> 00:48:29.820]   I mean, sometimes you watch a documentary
[00:48:29.820 --> 00:48:33.820]   and it's clear there is a point to this, you know?
[00:48:33.820 --> 00:48:36.220]   They're driving a point home.
[00:48:36.220 --> 00:48:40.300]   But I still, even then, often,
[00:48:40.300 --> 00:48:42.220]   I can still find something in it that I'm like,
[00:48:42.220 --> 00:48:42.980]   "Oh, well, you know what?
[00:48:42.980 --> 00:48:46.140]   "I didn't know that that existed before."
[00:48:46.140 --> 00:48:47.860]   - That's interesting. - I didn't know that.
[00:48:47.860 --> 00:48:49.180]   - Useless trivia.
[00:48:49.180 --> 00:48:50.020]   - That's right.
[00:48:50.020 --> 00:48:51.780]   - That's why the internet is there.
[00:48:51.780 --> 00:48:54.540]   - Yeah, but sometimes I don't want to go for it.
[00:48:54.540 --> 00:48:56.140]   I just want you to feed it to me.
[00:48:56.140 --> 00:48:59.420]   - I'd rather watch home shows, hometown.
[00:48:59.420 --> 00:49:00.740]   - Oh, God, I hate those too.
[00:49:00.740 --> 00:49:01.580]   - God, it's so weird. - Oh, you are.
[00:49:01.580 --> 00:49:02.740]   - I'm so rude. - So rude.
[00:49:02.740 --> 00:49:03.740]   - Round.
[00:49:03.740 --> 00:49:04.740]   What do you like to watch?
[00:49:04.740 --> 00:49:05.580]   - I just-- - Stacey.
[00:49:05.580 --> 00:49:09.180]   - Man, I'm loving the Stacey Award for this.
[00:49:09.180 --> 00:49:11.020]   - Hey, Stacey.
[00:49:11.020 --> 00:49:12.340]   (laughing)
[00:49:12.340 --> 00:49:14.220]   Happy birthday.
[00:49:14.220 --> 00:49:16.620]   - I turned into a cranky 45-year-old.
[00:49:16.620 --> 00:49:17.460]   I've a prevention overnight.
[00:49:17.460 --> 00:49:19.380]   - Oh, Bob, we got you a TV subscription.
[00:49:19.380 --> 00:49:22.420]   (laughing)
[00:49:22.420 --> 00:49:24.620]   - Don't you know I don't like watching any of that stuff?
[00:49:24.620 --> 00:49:26.340]   (laughing)
[00:49:26.340 --> 00:49:28.420]   - I think maybe I just don't like TV.
[00:49:28.420 --> 00:49:32.380]   It's possible that I just don't like television all that--
[00:49:32.380 --> 00:49:33.860]   - Nothing wrong with that.
[00:49:33.860 --> 00:49:35.340]   Nothing wrong with that.
[00:49:35.340 --> 00:49:39.980]   - I mean, I watch TV, but yeah, I don't like documentaries.
[00:49:39.980 --> 00:49:41.060]   I don't like home shows.
[00:49:41.060 --> 00:49:42.380]   I don't like reality TV.
[00:49:43.340 --> 00:49:44.900]   - I do like-- - Sitcoms?
[00:49:44.900 --> 00:49:48.180]   - I only like certain sitcoms.
[00:49:48.180 --> 00:49:49.820]   Like I have a very, like again--
[00:49:49.820 --> 00:49:50.660]   - Parks and Ray.
[00:49:50.660 --> 00:49:51.500]   - Yes.
[00:49:51.500 --> 00:49:52.340]   - I like Parks and Ray. - I like Parks and Ray.
[00:49:52.340 --> 00:49:53.180]   - Unswancing for the win.
[00:49:53.180 --> 00:49:54.500]   (laughing)
[00:49:54.500 --> 00:49:56.900]   - I like succession.
[00:49:56.900 --> 00:49:59.460]   I either like really like nuanced,
[00:49:59.460 --> 00:50:01.740]   kind of like a lot of stuff is happening.
[00:50:01.740 --> 00:50:02.940]   - Right.
[00:50:02.940 --> 00:50:05.820]   - Succession style TV HBO style,
[00:50:05.820 --> 00:50:07.380]   like the wire kind of stuff.
[00:50:07.380 --> 00:50:09.500]   Or I like goofy comedies.
[00:50:09.500 --> 00:50:11.460]   - Heading and tensing. - Goofy comedies.
[00:50:11.460 --> 00:50:12.300]   - Yeah.
[00:50:12.300 --> 00:50:14.340]   Or like super goofy comedies that are very light,
[00:50:14.340 --> 00:50:16.660]   like the good place or the office.
[00:50:16.660 --> 00:50:18.060]   - Good place for the great.
[00:50:18.060 --> 00:50:20.340]   - That and stand up is pretty much my--
[00:50:20.340 --> 00:50:21.380]   - Oh, I like stand up.
[00:50:21.380 --> 00:50:22.940]   Yeah, that's like a documentary.
[00:50:22.940 --> 00:50:24.220]   - I want to know how to stand up.
[00:50:24.220 --> 00:50:25.820]   - I mean, it happened.
[00:50:25.820 --> 00:50:28.860]   It's a documentary and that's someone's good on a stage.
[00:50:28.860 --> 00:50:30.580]   - Like someone's good on a stage.
[00:50:30.580 --> 00:50:32.700]   - So we captured it with a camera.
[00:50:32.700 --> 00:50:34.820]   - Yeah, it's about as credible too, yeah.
[00:50:34.820 --> 00:50:36.980]   - Yeah, so found as critical as well, yes.
[00:50:36.980 --> 00:50:38.580]   (laughing)
[00:50:38.580 --> 00:50:39.500]   So true.
[00:50:39.500 --> 00:50:40.340]   - Yeah.
[00:50:40.340 --> 00:50:43.460]   - Learn something new every time I'm on this week in Google.
[00:50:43.460 --> 00:50:44.300]   - Oh, good.
[00:50:44.300 --> 00:50:45.300]   - That's awesome.
[00:50:45.300 --> 00:50:46.700]   - Hey, see, great big crump.
[00:50:46.700 --> 00:50:47.540]   (laughing)
[00:50:47.540 --> 00:50:48.380]   Moving on.
[00:50:48.380 --> 00:50:49.900]   - It's documentaries, moving on.
[00:50:49.900 --> 00:50:54.020]   So right before the show,
[00:50:54.020 --> 00:50:57.620]   Jeff, you sent me an email about a report,
[00:50:57.620 --> 00:51:01.260]   friend of the show, Mike Masnick from TechDirt,
[00:51:01.260 --> 00:51:04.460]   released a new report on the unintended consequences
[00:51:04.460 --> 00:51:06.580]   of internet regulation.
[00:51:06.580 --> 00:51:09.300]   And this was published by the Copia Institute,
[00:51:09.300 --> 00:51:11.700]   the CCIA Research Center.
[00:51:11.700 --> 00:51:15.340]   And the report, which, I mean, it's a report,
[00:51:15.340 --> 00:51:16.660]   and it came out right before the show,
[00:51:16.660 --> 00:51:17.980]   so I did not have the time to read the report.
[00:51:17.980 --> 00:51:19.900]   - I won't do it through it.
[00:51:19.900 --> 00:51:20.980]   - You're already halfway through it,
[00:51:20.980 --> 00:51:22.060]   so you can talk to it.
[00:51:22.060 --> 00:51:24.860]   It follows the success and the failure of internet regulation.
[00:51:24.860 --> 00:51:25.900]   - A little equi.
[00:51:25.900 --> 00:51:27.580]   Okay, what do you think?
[00:51:27.580 --> 00:51:28.980]   - But it's your mind.
[00:51:28.980 --> 00:51:31.220]   - Mike's brilliant, we love Mike,
[00:51:31.220 --> 00:51:33.900]   and he's been working on this for a year, he said.
[00:51:33.900 --> 00:51:37.060]   And so he dug into unintended consequences of regulation,
[00:51:37.060 --> 00:51:38.500]   and you know this is gonna be close to my heart
[00:51:38.500 --> 00:51:40.340]   because you know, watch out.
[00:51:40.340 --> 00:51:42.740]   But he argues pretty strenuously that
[00:51:42.740 --> 00:51:48.100]   in a regulated environment,
[00:51:48.100 --> 00:51:51.060]   there's less investment in new companies,
[00:51:51.060 --> 00:51:52.500]   less investment in innovation,
[00:51:52.500 --> 00:51:55.380]   less investment in competition for the big guys.
[00:51:55.380 --> 00:51:58.380]   So ironically, of course, entrench the big guys,
[00:51:58.380 --> 00:52:01.500]   that there's an impact on freedom of expression,
[00:52:01.500 --> 00:52:05.380]   that authoritarian regimes like to carbon and copy
[00:52:05.380 --> 00:52:07.780]   certain regulations like that's PG in Germany,
[00:52:07.780 --> 00:52:11.860]   to use it as a excuse to regulate speech
[00:52:11.860 --> 00:52:13.580]   in their environments.
[00:52:13.580 --> 00:52:15.860]   And so halfway through reading it,
[00:52:15.860 --> 00:52:19.660]   he goes into specific examples in Germany with SDG,
[00:52:19.660 --> 00:52:22.900]   in the US with FOSTA,
[00:52:22.900 --> 00:52:27.460]   and that's a case where sex workers were badly hurt,
[00:52:27.460 --> 00:52:32.220]   and the aim that existed to try to stop
[00:52:32.220 --> 00:52:36.020]   little trafficking really didn't go.
[00:52:36.020 --> 00:52:37.620]   But one of his interesting points
[00:52:37.620 --> 00:52:39.660]   that I'll end here is that,
[00:52:39.660 --> 00:52:42.660]   in the case of Germany in the hate speech law,
[00:52:42.660 --> 00:52:46.300]   that's PG, the platformers were already
[00:52:46.300 --> 00:52:48.220]   taking the stuff down,
[00:52:48.220 --> 00:52:51.020]   that they thought they were gonna be finding them
[00:52:51.020 --> 00:52:54.820]   like every other day, there haven't been any fines,
[00:52:54.820 --> 00:52:57.340]   because the platforms were taking the stuff down,
[00:52:57.340 --> 00:52:58.940]   plus they were taking it down
[00:52:58.940 --> 00:53:02.380]   under their own terms of service, not under the law.
[00:53:02.380 --> 00:53:04.540]   And so it really hasn't had a big impact
[00:53:04.540 --> 00:53:06.740]   on the quality of the internet,
[00:53:06.740 --> 00:53:08.500]   but it has had an impact, he argues,
[00:53:08.500 --> 00:53:12.820]   on innovation, competition, and on freedom of expression.
[00:53:12.820 --> 00:53:14.340]   So I'll be finishing reading this shortly.
[00:53:14.340 --> 00:53:18.060]   I was starting to tweet and toot some quotes out of it.
[00:53:18.060 --> 00:53:22.900]   I just wanted to give basically a good plug for this,
[00:53:22.900 --> 00:53:25.340]   'cause he worked hard on it, we respect Mike.
[00:53:25.340 --> 00:53:26.260]   That's it.
[00:53:26.260 --> 00:53:29.660]   When you read through a report like this,
[00:53:29.660 --> 00:53:33.180]   do you have a doc opening, you're pulling things?
[00:53:33.180 --> 00:53:36.620]   I'm just super curious about how you parse
[00:53:36.620 --> 00:53:37.460]   a report.
[00:53:37.460 --> 00:53:39.260]   Well, this is all research for my next book,
[00:53:39.260 --> 00:53:41.340]   so I printed it out.
[00:53:41.340 --> 00:53:43.060]   In fact, by the way, let me just go off,
[00:53:43.060 --> 00:53:43.900]   let me just slide a tangent.
[00:53:43.900 --> 00:53:44.980]   Let's slide a tangent.
[00:53:44.980 --> 00:53:45.820]   Oh boy.
[00:53:45.820 --> 00:53:46.660]   Oh boy.
[00:53:46.660 --> 00:53:47.500]   There's a pop-up report.
[00:53:47.500 --> 00:53:49.220]   And people who make these reports
[00:53:49.220 --> 00:53:51.580]   and offer you a PDF, and you're never gonna print it out,
[00:53:51.580 --> 00:53:52.420]   right?
[00:53:52.420 --> 00:53:53.260]   They do a few things run,
[00:53:53.260 --> 00:53:55.020]   more than they have huge illustrations
[00:53:55.020 --> 00:53:56.580]   that take up lots of your ink.
[00:53:56.580 --> 00:53:57.420]   Oh yeah.
[00:53:57.420 --> 00:53:58.740]   Nothing of value, right?
[00:53:58.740 --> 00:53:59.620]   (laughing)
[00:53:59.620 --> 00:54:00.740]   But Globe!
[00:54:00.740 --> 00:54:01.740]   Who cares?
[00:54:01.740 --> 00:54:02.860]   It's not Mike, it's not love you,
[00:54:02.860 --> 00:54:04.540]   it's not just you, but you had a page or two of that,
[00:54:04.540 --> 00:54:05.660]   and I had a cut or I said no,
[00:54:05.660 --> 00:54:07.420]   start with page one, then go to page three,
[00:54:07.420 --> 00:54:08.820]   then go to page five, then print the rest.
[00:54:08.820 --> 00:54:10.340]   Buy your, buy your irritation,
[00:54:10.340 --> 00:54:11.540]   but you got to plan ahead.
[00:54:11.540 --> 00:54:13.140]   But the other thing that really irritates me
[00:54:13.140 --> 00:54:16.020]   is they choose colored fonts.
[00:54:16.020 --> 00:54:18.060]   I print out, barely read it,
[00:54:18.060 --> 00:54:20.940]   and then I've got to go in and change the resolution up
[00:54:20.940 --> 00:54:21.980]   so that it comes out.
[00:54:21.980 --> 00:54:23.700]   Just use black type.
[00:54:23.700 --> 00:54:24.700]   Yes, yes.
[00:54:24.700 --> 00:54:27.340]   How do people make PDFs to print them out?
[00:54:27.340 --> 00:54:29.900]   Underline things, thank you for that.
[00:54:29.900 --> 00:54:30.740]   I don't like you.
[00:54:30.740 --> 00:54:32.620]   Some people only produce, like they don't produce
[00:54:32.620 --> 00:54:34.540]   a web version, they just send you to the PDF.
[00:54:34.540 --> 00:54:36.460]   So the PDF acts as the web version,
[00:54:36.460 --> 00:54:38.580]   which means it does need different colored fonts,
[00:54:38.580 --> 00:54:41.220]   and it does need stupid empty images
[00:54:41.220 --> 00:54:43.460]   to rest your eyes from all the blocks of type.
[00:54:43.460 --> 00:54:44.860]   I get your complaint.
[00:54:44.860 --> 00:54:46.460]   Maybe they should do a printable and--
[00:54:46.460 --> 00:54:47.620]   Yes, yes, yes.
[00:54:47.620 --> 00:54:49.140]   Well, you know, that's what the academics do, right?
[00:54:49.140 --> 00:54:52.660]   The academics, you have a dull text-only report,
[00:54:52.660 --> 00:54:54.540]   and then you can print that out, and it's fine.
[00:54:54.540 --> 00:54:56.140]   So yeah, I'm underlining like crazy
[00:54:56.140 --> 00:54:58.620]   so I can use it in research for the next book
[00:54:58.620 --> 00:54:59.460]   on the internet.
[00:54:59.460 --> 00:55:01.980]   I don't know why I'm--
[00:55:01.980 --> 00:55:03.980]   It is so fascinating that you print things out.
[00:55:03.980 --> 00:55:04.820]   I know.
[00:55:04.820 --> 00:55:07.900]   My child does two things.
[00:55:07.900 --> 00:55:12.900]   They write handwritten notes, and then they cut and paste.
[00:55:12.900 --> 00:55:17.500]   They just snip elements that they want to save,
[00:55:17.500 --> 00:55:20.340]   and then they place them in a dock of notes,
[00:55:20.340 --> 00:55:22.340]   which is also baffling.
[00:55:22.340 --> 00:55:24.060]   So I'm just like--
[00:55:24.060 --> 00:55:26.620]   It's like screen capture the portions of the screen
[00:55:26.620 --> 00:55:28.380]   that the text is on or just cut.
[00:55:28.380 --> 00:55:29.220]   Exactly.
[00:55:29.220 --> 00:55:30.980]   Yeah, okay, I've done that.
[00:55:30.980 --> 00:55:31.980]   I feel that.
[00:55:32.980 --> 00:55:34.220]   And I don't--
[00:55:34.220 --> 00:55:38.220]   I'm baffled by how people do real research.
[00:55:38.220 --> 00:55:41.940]   I mean, as a journalist, I'll cut and paste things
[00:55:41.940 --> 00:55:43.900]   and I'm like, "Ooh, I need to make sure I get this in here
[00:55:43.900 --> 00:55:46.300]   and put things in my notes."
[00:55:46.300 --> 00:55:49.300]   But for writing a book or something like that,
[00:55:49.300 --> 00:55:50.300]   I'm not like, "I'm not going to have to be--"
[00:55:50.300 --> 00:55:52.020]   I'm definitely afraid of cutting and chasing something
[00:55:52.020 --> 00:55:54.380]   and not bringing the attribution along with it.
[00:55:54.380 --> 00:55:55.380]   Ah, yeah.
[00:55:55.380 --> 00:55:56.700]   Oh, yeah.
[00:55:56.700 --> 00:55:57.540]   Yes, like--
[00:55:57.540 --> 00:55:59.820]   So I cut and paste there.
[00:55:59.820 --> 00:56:01.860]   I never cut and paste into the document.
[00:56:01.860 --> 00:56:02.700]   I'm writing it.
[00:56:02.700 --> 00:56:06.060]   I cut and paste into notes to avoid plagiarism.
[00:56:06.060 --> 00:56:07.260]   So I have--
[00:56:07.260 --> 00:56:12.500]   and then when I'm writing, before I write, ideally,
[00:56:12.500 --> 00:56:15.540]   I've read my notes and then I go and I start writing
[00:56:15.540 --> 00:56:18.660]   and then I'll TK anything that I need to--
[00:56:18.660 --> 00:56:23.500]   like the actual facts and then I go and pull them.
[00:56:23.500 --> 00:56:24.340]   Totally--
[00:56:24.340 --> 00:56:25.180]   Exactly.
[00:56:25.180 --> 00:56:26.020]   --exactly that.
[00:56:26.020 --> 00:56:26.860]   --the journalist.
[00:56:26.860 --> 00:56:28.340]   And move on.
[00:56:28.340 --> 00:56:31.340]   When I'm writing a book, I'll cut the editing cell.
[00:56:31.340 --> 00:56:32.980]   Neither.
[00:56:32.980 --> 00:56:35.580]   I was copying it in the Gutenberg parentheses.
[00:56:35.580 --> 00:56:37.620]   There was once or twice, I came across a phrase
[00:56:37.620 --> 00:56:39.940]   thinking, "That can't have come out of my brain."
[00:56:39.940 --> 00:56:40.820]   I didn't think of that.
[00:56:40.820 --> 00:56:41.940]   Oh, no.
[00:56:41.940 --> 00:56:44.380]   I copied it from the cell and go Google the phrase
[00:56:44.380 --> 00:56:45.820]   and find no one else has said the phrase.
[00:56:45.820 --> 00:56:48.260]   So I apparently did come up with it,
[00:56:48.260 --> 00:56:50.460]   but it scares me to death.
[00:56:50.460 --> 00:56:51.020]   I can imagine.
[00:56:51.020 --> 00:56:54.420]   Yeah, well, sometimes people come up with phrases
[00:56:54.420 --> 00:56:57.180]   that other people have also come up with.
[00:56:57.180 --> 00:56:57.580]   Yes.
[00:56:57.580 --> 00:56:59.900]   Like--
[00:56:59.900 --> 00:57:03.220]   Sometimes I'll run my articles through like--
[00:57:03.220 --> 00:57:04.340]   plagiarism detection.
[00:57:04.340 --> 00:57:05.300]   You do.
[00:57:05.300 --> 00:57:05.900]   Yeah.
[00:57:05.900 --> 00:57:07.820]   Just for-- like--
[00:57:07.820 --> 00:57:10.420]   I read a lot of stuff that's pretty technical, right?
[00:57:10.420 --> 00:57:10.780]   And--
[00:57:10.780 --> 00:57:13.260]   Yeah.
[00:57:13.260 --> 00:57:15.820]   --I'm like, did I--
[00:57:15.820 --> 00:57:19.540]   I mean, I'm never intending to plagiarize, obviously.
[00:57:19.540 --> 00:57:22.900]   So sometimes it'll pull things up that I'm like,
[00:57:22.900 --> 00:57:25.100]   I don't really know if there's another way to talk
[00:57:25.100 --> 00:57:26.420]   about instruction sets.
[00:57:26.420 --> 00:57:26.940]   Right.
[00:57:26.940 --> 00:57:27.940]   [LAUGHTER]
[00:57:27.940 --> 00:57:28.940]   OK.
[00:57:28.940 --> 00:57:33.540]   So when I'm defining a term, sometimes I'm like, I mean,
[00:57:33.540 --> 00:57:34.700]   maybe.
[00:57:34.700 --> 00:57:36.740]   How else can you say this?
[00:57:36.740 --> 00:57:39.100]   How else can it be said?
[00:57:39.100 --> 00:57:39.620]   Yeah.
[00:57:39.620 --> 00:57:40.700]   Because sometimes-- I mean, when you're
[00:57:40.700 --> 00:57:43.180]   talking about technical things, simplicity is important.
[00:57:43.180 --> 00:57:43.700]   Sure.
[00:57:43.700 --> 00:57:46.460]   You don't want to come up with an elaborate metaphor
[00:57:46.460 --> 00:57:47.980]   for what an operating system does.
[00:57:47.980 --> 00:57:50.420]   I just want to tell you what an operating system does.
[00:57:50.420 --> 00:57:52.500]   Mm-hmm.
[00:57:52.500 --> 00:57:53.260]   I don't know.
[00:57:53.260 --> 00:57:54.060]   Interesting.
[00:57:54.060 --> 00:57:56.660]   Well, someday in the future, chat GPT
[00:57:56.660 --> 00:57:59.340]   will help you with all of that, right?
[00:57:59.340 --> 00:58:04.700]   AI systems, they're going to protect you from any of those.
[00:58:04.700 --> 00:58:07.900]   Oh, you know, this is a good question to ask you guys.
[00:58:07.900 --> 00:58:09.500]   Are we moving into the generative AI?
[00:58:09.500 --> 00:58:09.500]   Sure.
[00:58:09.500 --> 00:58:10.340]   Let's do it.
[00:58:10.340 --> 00:58:10.940]   Yeah.
[00:58:10.940 --> 00:58:12.700]   I think so.
[00:58:12.700 --> 00:58:13.780]   So I'm prepping an article.
[00:58:13.780 --> 00:58:15.940]   I'm doing interviews right now for an article for the newsletter
[00:58:15.940 --> 00:58:19.060]   on 6G for industrial use cases.
[00:58:19.060 --> 00:58:22.980]   So 3GPP standard, right?
[00:58:22.980 --> 00:58:25.700]   Cellular standard, sorry.
[00:58:25.700 --> 00:58:27.860]   So I plan to write my article.
[00:58:27.860 --> 00:58:31.220]   But then I also thought, you know, there's enough information.
[00:58:31.220 --> 00:58:35.340]   I could have like do a generative AI based article.
[00:58:35.340 --> 00:58:37.620]   So like chat GPT or some other use
[00:58:37.620 --> 00:58:39.140]   bard to generate this.
[00:58:39.140 --> 00:58:41.540]   And I wonder if I should run both of them in the newsletter
[00:58:41.540 --> 00:58:42.700]   to see.
[00:58:42.700 --> 00:58:45.740]   What if my article is just like their article?
[00:58:45.740 --> 00:58:46.420]   I don't know.
[00:58:46.420 --> 00:58:47.700]   Would that be a fun experiment?
[00:58:47.700 --> 00:58:49.780]   Would you want to know about that?
[00:58:49.780 --> 00:58:51.940]   These are the questions.
[00:58:51.940 --> 00:58:52.460]   Yeah.
[00:58:52.460 --> 00:58:53.260]   Is it ethical?
[00:58:53.260 --> 00:58:55.220]   I mean, I would say-- look, I wrote
[00:58:55.220 --> 00:58:56.660]   this on my own.
[00:58:56.660 --> 00:58:58.620]   And then I did this prompt.
[00:58:58.620 --> 00:59:01.180]   As long as you're disclosing this on the drawer.
[00:59:01.180 --> 00:59:01.460]   Yeah.
[00:59:01.460 --> 00:59:06.260]   I think that's what I would want, is disclosure about that.
[00:59:06.260 --> 00:59:08.540]   But I mean, yeah, absolutely.
[00:59:08.540 --> 00:59:11.620]   But what if it's wrong?
[00:59:11.620 --> 00:59:13.500]   That's a story too.
[00:59:13.500 --> 00:59:13.700]   Yeah.
[00:59:13.700 --> 00:59:16.980]   I'll make sure you mark it so people don't cut and paste that.
[00:59:16.980 --> 00:59:19.420]   Stacey led me astray.
[00:59:19.420 --> 00:59:21.340]   Yeah, I don't see that part of the thing
[00:59:21.340 --> 00:59:23.780]   is I don't know how to document and annotate that
[00:59:23.780 --> 00:59:24.780]   in the formats I've got.
[00:59:24.780 --> 00:59:26.260]   Right.
[00:59:26.260 --> 00:59:28.620]   Oh.
[00:59:28.620 --> 00:59:29.300]   That's troubling.
[00:59:29.300 --> 00:59:30.140]   That's difficult.
[00:59:30.140 --> 00:59:31.660]   Yeah, if it's wrong.
[00:59:31.660 --> 00:59:34.380]   I was just throwing this open to you and anyone on chat
[00:59:34.380 --> 00:59:37.020]   if you would like to give me your thoughts.
[00:59:37.020 --> 00:59:38.780]   And I'm just thinking about it.
[00:59:38.780 --> 00:59:40.700]   Because I don't have editors.
[00:59:40.700 --> 00:59:41.300]   I just have me.
[00:59:41.300 --> 00:59:46.180]   Jeff, you can call me later.
[00:59:46.180 --> 00:59:48.060]   Get your journalism students on the line.
[00:59:48.060 --> 00:59:48.980]   We'll do.
[00:59:48.980 --> 00:59:50.740]   I saw a professor at University of Toronto.
[00:59:50.740 --> 00:59:53.740]   I saw it a Palo Granada, I think his name is.
[00:59:53.740 --> 00:59:58.500]   He's decided to teach a course in AI,
[00:59:58.500 --> 01:00:01.700]   and he's not going to teach it the AI will.
[01:00:01.700 --> 01:00:03.060]   Oh.
[01:00:03.060 --> 01:00:04.620]   What exactly that means I'm not sure.
[01:00:04.620 --> 01:00:06.140]   Yeah, what does that mean?
[01:00:06.140 --> 01:00:08.580]   It'll be fun to watch.
[01:00:08.580 --> 01:00:11.820]   I have a feeling in the coming couple of years, especially,
[01:00:11.820 --> 01:00:16.300]   we're going to be seeing lots of little litmus tests like that.
[01:00:16.300 --> 01:00:20.940]   Like, oh, what would happen if I had AI do this thing then?
[01:00:20.940 --> 01:00:22.900]   And let's see what happens.
[01:00:22.900 --> 01:00:23.780]   Yeah, it's.
[01:00:23.780 --> 01:00:26.140]   But do you think that's OK?
[01:00:26.140 --> 01:00:29.580]   I think that's OK.
[01:00:29.580 --> 01:00:32.340]   As long as this is disclosure, yeah.
[01:00:32.340 --> 01:00:36.100]   I mean, I saw a professor recently
[01:00:36.100 --> 01:00:38.740]   talking about using AI to just write your syllabus.
[01:00:38.740 --> 01:00:40.540]   I'm not OK with that.
[01:00:40.540 --> 01:00:41.540]   Why not?
[01:00:41.540 --> 01:00:44.740]   Just because you use it as a cheat tool.
[01:00:44.740 --> 01:00:47.660]   And the problem is, does it tempt certain universities
[01:00:47.660 --> 01:00:49.300]   to put us out of a job?
[01:00:49.300 --> 01:00:49.900]   OK.
[01:00:49.900 --> 01:00:51.740]   Same with writers, same, same issue.
[01:00:51.740 --> 01:00:54.700]   Plus, I just think that there is a lot of--
[01:00:54.700 --> 01:00:58.340]   again, it's only a word prediction tool.
[01:00:58.340 --> 01:00:58.860]   Right.
[01:00:58.860 --> 01:00:59.380]   Right.
[01:00:59.380 --> 01:01:03.860]   Has no insight, no knowledge, no understanding, nothing.
[01:01:03.860 --> 01:01:05.860]   I argue, as I've argued on the show before,
[01:01:05.860 --> 01:01:09.260]   that we should not speak in first person,
[01:01:09.260 --> 01:01:11.940]   we should speak in third person of the machine,
[01:01:11.940 --> 01:01:13.140]   should not say it's writing.
[01:01:13.140 --> 01:01:14.620]   It is assembling words.
[01:01:14.620 --> 01:01:16.100]   That's all it's doing.
[01:01:16.100 --> 01:01:18.860]   So I don't think it brings any insight in that sense.
[01:01:18.860 --> 01:01:21.620]   Now, for students to use it to get them over a writer's
[01:01:21.620 --> 01:01:24.100]   block, to see other ways to express things,
[01:01:24.100 --> 01:01:27.260]   to be inspired, to help them with their literacy,
[01:01:27.260 --> 01:01:28.820]   to help them with translation of English
[01:01:28.820 --> 01:01:30.180]   in a second or third language, I think
[01:01:30.180 --> 01:01:33.140]   that's all really rich and fascinating.
[01:01:33.140 --> 01:01:38.060]   I think for teachers to use it to give material to students
[01:01:38.060 --> 01:01:40.700]   to edit and judge and improve upon,
[01:01:40.700 --> 01:01:42.660]   and I think it makes you kind of feel powerful better
[01:01:42.660 --> 01:01:44.420]   than the machine, I think that's interesting.
[01:01:44.420 --> 01:01:46.660]   So no, I'm not opposed to using it in all kinds of ways,
[01:01:46.660 --> 01:01:47.260]   Ant.
[01:01:47.260 --> 01:01:51.660]   I just think that we have to be-- my problem with the whole
[01:01:51.660 --> 01:01:54.580]   generative AI discussion these days
[01:01:54.580 --> 01:01:56.180]   is that it's being misused.
[01:01:56.180 --> 01:01:57.420]   It shouldn't be part of a search.
[01:01:57.420 --> 01:01:59.700]   We've talked about that in the show a lot.
[01:01:59.700 --> 01:02:00.780]   Durdellists are getting it wrong.
[01:02:00.780 --> 01:02:03.180]   Kevin Rusnow was not in love with you.
[01:02:03.180 --> 01:02:04.540]   And I just had this idea this week.
[01:02:04.540 --> 01:02:07.540]   We'll play this out on you guys.
[01:02:07.540 --> 01:02:11.580]   If I had a machine that had assembled basically all
[01:02:11.580 --> 01:02:13.580]   of the text you could possibly get,
[01:02:13.580 --> 01:02:16.580]   and then mapped the relationships of every single word
[01:02:16.580 --> 01:02:19.700]   that to every other single word, the last thing I would do
[01:02:19.700 --> 01:02:21.580]   is make it more content.
[01:02:21.580 --> 01:02:24.380]   I wouldn't want to query it to find out what are our biases,
[01:02:24.380 --> 01:02:27.380]   what are their correlations we have in society.
[01:02:27.380 --> 01:02:30.580]   There's data to be found in that mapping,
[01:02:30.580 --> 01:02:32.020]   but we're not doing that.
[01:02:32.020 --> 01:02:34.820]   Instead, we're just making more damn content.
[01:02:34.820 --> 01:02:37.100]   Well, so one of the things that I think
[01:02:37.100 --> 01:02:39.620]   is really interesting is a tech reporter.
[01:02:39.620 --> 01:02:41.380]   One of those reasons I want to try this
[01:02:41.380 --> 01:02:44.500]   is 6G isn't something that's talked about a lot yet.
[01:02:44.500 --> 01:02:46.580]   We haven't written ad nauseum about it.
[01:02:46.580 --> 01:02:50.540]   So it's possible that there won't be a great article in there,
[01:02:50.540 --> 01:02:52.180]   which means I should write something
[01:02:52.180 --> 01:02:57.260]   because I can authoritatively add something new
[01:02:57.260 --> 01:02:58.260]   to the conversation I told you.
[01:02:58.260 --> 01:02:59.300]   - Because it's a little knowledge, yeah.
[01:02:59.300 --> 01:03:01.900]   - Yeah, but if I asked it to write me something about 5G,
[01:03:01.900 --> 01:03:03.500]   I'm sure it could do a bang up job.
[01:03:03.500 --> 01:03:09.220]   So that's another interesting way to use it to be like,
[01:03:09.220 --> 01:03:13.980]   hey, if I ask it about a topic and it's given me crap answers,
[01:03:13.980 --> 01:03:17.500]   then I need to come in and generate better words
[01:03:17.500 --> 01:03:20.300]   next to each other that it can pick from next time.
[01:03:20.300 --> 01:03:22.580]   - That's generous of you, where as a lot of writers
[01:03:22.580 --> 01:03:26.020]   and news organizations are screaming, don't copy me.
[01:03:26.020 --> 01:03:30.140]   I saw I went to the Semaphore Media event in New York
[01:03:30.140 --> 01:03:31.780]   the other night and Barry Diller,
[01:03:31.780 --> 01:03:32.900]   a gold fart Barry Diller,
[01:03:32.900 --> 01:03:36.660]   and our own Meredith and Timing and the remains of all that.
[01:03:36.660 --> 01:03:38.460]   - Is that where you saw Steven and Smith?
[01:03:38.460 --> 01:03:39.460]   - That is exactly where I saw him.
[01:03:39.460 --> 01:03:40.860]   And I was gonna ask you to dance
[01:03:40.860 --> 01:03:44.460]   'cause I don't know enough to know, I don't listen to sports.
[01:03:44.460 --> 01:03:48.860]   But Diller was going on about how they stole all our content
[01:03:48.860 --> 01:03:50.020]   and beginning of the internet
[01:03:50.020 --> 01:03:52.660]   and we're not gonna let it happen again.
[01:03:52.660 --> 01:03:54.860]   And the old fart, you know, I think he's,
[01:03:54.860 --> 01:03:57.580]   I think that where he's tasty your attitude is,
[01:03:57.580 --> 01:03:59.420]   there's an ecosystem of knowledge out there.
[01:03:59.420 --> 01:04:03.300]   And if I can improve it and be found in that, which would be nice,
[01:04:03.300 --> 01:04:08.300]   then that's a proper and generous thing for journalists to do, right?
[01:04:08.300 --> 01:04:09.900]   - Well, I'm not doing it for the AI.
[01:04:09.900 --> 01:04:12.300]   I'm doing it because my audience would clearly not know.
[01:04:12.300 --> 01:04:15.180]   I mean, I write about these as a cutting edge.
[01:04:15.180 --> 01:04:18.060]   So I'm like, oh, this is something that needs to be covered
[01:04:18.060 --> 01:04:20.300]   'cause no one's covering it.
[01:04:20.300 --> 01:04:22.580]   - Well, what do you think of Steven and Smith, Ant?
[01:04:22.580 --> 01:04:24.420]   - I love Steven and Smith.
[01:04:24.420 --> 01:04:26.540]   He gets a bad rep, unfortunately.
[01:04:26.540 --> 01:04:29.540]   I mean, he has this nickname, Screamin' A Smith.
[01:04:29.540 --> 01:04:35.420]   And it's because of how a lot of the sports talk shows are now
[01:04:35.420 --> 01:04:38.060]   just screaming at each other, the Bates,
[01:04:38.060 --> 01:04:39.580]   but they're usually just screaming at each other
[01:04:39.580 --> 01:04:42.380]   'cause that's what gets the clicks I get.
[01:04:42.380 --> 01:04:46.820]   But his story of him growing up and being an athlete
[01:04:46.820 --> 01:04:49.420]   and wasn't a great athlete, but he was good enough
[01:04:49.420 --> 01:04:52.140]   and going to the HBCUs in North Carolina
[01:04:52.140 --> 01:04:55.380]   and getting into the writing rooms in Philadelphia
[01:04:55.380 --> 01:04:58.140]   and just really doing the work and grinding
[01:04:58.140 --> 01:05:01.420]   and pushing through and just building himself up
[01:05:01.420 --> 01:05:04.660]   and building his career up, not with a bunch of handouts,
[01:05:04.660 --> 01:05:08.660]   not with cheating on this person or that person.
[01:05:08.660 --> 01:05:12.020]   He literally just put in the work and it shows
[01:05:12.020 --> 01:05:17.020]   and people respect him and the players in the different sports.
[01:05:17.020 --> 01:05:21.020]   Whenever he mentions their name, they tend to listen.
[01:05:21.020 --> 01:05:24.500]   Sometimes they get mad because it's some painful truths,
[01:05:24.500 --> 01:05:27.460]   but they respect them because they know he does this research.
[01:05:27.460 --> 01:05:31.340]   - He was interested, but I don't know sports,
[01:05:31.340 --> 01:05:32.620]   so I just kind of sat back and said,
[01:05:32.620 --> 01:05:34.220]   oh, no, next to Steven and Smith,
[01:05:34.220 --> 01:05:36.660]   but I don't know what he thinks, oh, he's there.
[01:05:36.660 --> 01:05:39.500]   (laughs)
[01:05:39.500 --> 01:05:41.060]   - Yeah, he's a pretty good cat.
[01:05:41.060 --> 01:05:45.180]   - Where do we want to go from here?
[01:05:45.180 --> 01:05:50.180]   We've got AI, this US Commerce Department yesterday,
[01:05:50.180 --> 01:05:52.540]   requesting public comment on how to create
[01:05:52.540 --> 01:05:56.540]   accountability measures for AI.
[01:05:56.540 --> 01:05:58.540]   - We don't even know what it is yet.
[01:05:58.540 --> 01:06:00.420]   - Right. - Yeah.
[01:06:00.420 --> 01:06:04.220]   I think we're still just society in general
[01:06:04.220 --> 01:06:05.060]   when it comes to AI.
[01:06:05.060 --> 01:06:09.100]   We just need to understand that AI is still evolving
[01:06:09.100 --> 01:06:10.020]   at this moment.
[01:06:10.020 --> 01:06:14.220]   We're nowhere near what one should consider
[01:06:14.220 --> 01:06:17.140]   a final version of it because it's still trying to learn
[01:06:17.140 --> 01:06:19.500]   from the stuff that we put into it.
[01:06:19.500 --> 01:06:22.300]   But yet there's still so much doom and gloom
[01:06:22.300 --> 01:06:24.740]   being put out there from the media.
[01:06:24.740 --> 01:06:28.820]   And it's screwing up a lot of the normal folks
[01:06:28.820 --> 01:06:33.260]   outside of this here podcast that hears about AI.
[01:06:33.260 --> 01:06:38.260]   Dr. Norett was on Tech News Weekly last week.
[01:06:38.260 --> 01:06:40.500]   - Oh, good friend of this show.
[01:06:40.500 --> 01:06:42.260]   - It was great talking to her.
[01:06:42.260 --> 01:06:43.980]   It was great talking to her, you know,
[01:06:43.980 --> 01:06:48.980]   because she pointed out how certain sides of the press
[01:06:48.980 --> 01:06:53.460]   are going at it as AI is great, AI is awesome.
[01:06:53.460 --> 01:06:55.140]   Whoo, way to go.
[01:06:55.140 --> 01:06:57.860]   And then there's the other side that are like AI doomers
[01:06:57.860 --> 01:07:01.100]   and oh gosh, watch out for this psycho AI.
[01:07:01.100 --> 01:07:03.740]   You never have a bit of a middle ground
[01:07:03.740 --> 01:07:07.260]   when it comes to reporting the facts of the matter
[01:07:07.260 --> 01:07:09.700]   when it comes to AI.
[01:07:09.700 --> 01:07:13.460]   And that's a big problem because you're polarizing people
[01:07:13.460 --> 01:07:16.300]   to be one way or the other, not necessarily
[01:07:16.300 --> 01:07:18.380]   let everybody see the full scope of it.
[01:07:18.380 --> 01:07:21.860]   You know, just like AI is gonna take all of our jobs.
[01:07:21.860 --> 01:07:23.540]   Right, it's not.
[01:07:23.540 --> 01:07:25.420]   There's also kind of a macho to it too.
[01:07:25.420 --> 01:07:27.020]   And where some of these guys were saying,
[01:07:27.020 --> 01:07:29.900]   and this is Elon, this is the letter that he signed,
[01:07:29.900 --> 01:07:31.740]   is that it's gonna destroy humanity
[01:07:31.740 --> 01:07:34.200]   and I am so powerful I can do that.
[01:07:34.200 --> 01:07:40.500]   There's a macho hubris there hubris that I think is BS.
[01:07:40.500 --> 01:07:44.540]   - So I will say I have a different opinion.
[01:07:44.540 --> 01:07:47.700]   - All right, I think the NTIA,
[01:07:47.700 --> 01:07:50.740]   we're talking about the NTIA's are a request for comment.
[01:07:50.740 --> 01:07:51.580]   Yeah.
[01:07:51.580 --> 01:07:54.420]   So I think this is actually an excellent thing.
[01:07:54.420 --> 01:07:58.220]   I think what they're looking at is they're actually looking
[01:07:58.220 --> 01:08:02.140]   for comments on how we should be auditing AI
[01:08:02.140 --> 01:08:03.540]   and how we should establish rules
[01:08:03.540 --> 01:08:06.260]   to make sure AI is doing what we want it to do.
[01:08:06.260 --> 01:08:09.340]   And we've already seen various different forms
[01:08:09.340 --> 01:08:12.620]   of artificial intelligence and models being used
[01:08:12.620 --> 01:08:16.060]   in ways that have tremendous impact on people's lives
[01:08:16.060 --> 01:08:19.780]   in the medical industry for detecting breast cancer
[01:08:19.780 --> 01:08:24.580]   in filing bail in Philadelphia,
[01:08:24.580 --> 01:08:27.180]   deciding who gets bail and how much they have to pay.
[01:08:27.180 --> 01:08:29.620]   There is CPS is using it.
[01:08:29.620 --> 01:08:33.100]   So there's a lot of places where AI is already being used
[01:08:33.100 --> 01:08:35.460]   and there's no accountability for how those algorithms
[01:08:35.460 --> 01:08:36.300]   are working.
[01:08:36.300 --> 01:08:40.660]   There's also no way to know necessarily
[01:08:40.660 --> 01:08:44.900]   that it's being used and there's no way to not opt out
[01:08:44.900 --> 01:08:46.580]   but switch over to a person.
[01:08:46.580 --> 01:08:49.420]   Like in the case of judges and how they deal with bail
[01:08:49.420 --> 01:08:52.060]   in Philadelphia, I think it's Allegheny County maybe.
[01:08:52.060 --> 01:08:57.100]   There's no option for a person to get involved, right?
[01:08:57.100 --> 01:08:58.100]   So there are...
[01:08:58.100 --> 01:08:58.940]   - That's not extreme.
[01:08:58.940 --> 01:08:59.780]   - Right.
[01:08:59.780 --> 01:09:02.740]   So it is important not that we wait.
[01:09:02.740 --> 01:09:05.180]   I know that the Silicon Valley has,
[01:09:05.180 --> 01:09:06.420]   our brains automatically are like,
[01:09:06.420 --> 01:09:07.660]   "Oh no, we shouldn't make laws
[01:09:07.660 --> 01:09:11.540]   because it affects innovation and it stymies innovation."
[01:09:11.540 --> 01:09:14.180]   Part of what we need to do here is guide innovation
[01:09:14.180 --> 01:09:16.420]   along the tracks we want it to go on.
[01:09:16.420 --> 01:09:18.980]   And what these tracks that they're trying to lay down
[01:09:18.980 --> 01:09:20.980]   isn't stop what you're doing.
[01:09:20.980 --> 01:09:24.860]   It is, let's talk about how we can create audits
[01:09:24.860 --> 01:09:27.100]   that hold the developers accountable
[01:09:27.100 --> 01:09:29.900]   and make sure that when we implement these things,
[01:09:29.900 --> 01:09:34.300]   we have a release valve for people who get hurt by it.
[01:09:34.300 --> 01:09:35.140]   And I think that's a...
[01:09:35.140 --> 01:09:38.620]   - What counts as AI and the AI has become such a huge umbrella.
[01:09:38.620 --> 01:09:40.860]   I mean, is it to be a little bit equal?
[01:09:40.860 --> 01:09:42.460]   Does it include auto-correct?
[01:09:42.460 --> 01:09:44.060]   Does it include translate?
[01:09:44.060 --> 01:09:48.020]   Does it include Amazon recommending things to you?
[01:09:48.020 --> 01:09:49.860]   Where is it that AI is dangerous?
[01:09:49.860 --> 01:09:53.260]   And where is it that AI now already accepted?
[01:09:54.220 --> 01:09:57.140]   - All right, so let me, I've pulled up the request
[01:09:57.140 --> 01:09:58.980]   for comment right here.
[01:09:58.980 --> 01:10:01.100]   - I actually printed it out earlier too.
[01:10:01.100 --> 01:10:01.940]   - Yes.
[01:10:01.940 --> 01:10:05.420]   - We know you print out everything, Jeff.
[01:10:05.420 --> 01:10:06.500]   - Old fart.
[01:10:06.500 --> 01:10:08.140]   - Print out my email.
[01:10:08.140 --> 01:10:10.180]   I don't read this thing.
[01:10:10.180 --> 01:10:15.180]   - All right, so from an, so you're asking the definition of AI
[01:10:15.180 --> 01:10:17.980]   is what you're asking me here.
[01:10:17.980 --> 01:10:20.340]   How are they defining it?
[01:10:20.340 --> 01:10:24.020]   And the answer is, this is 31 page document
[01:10:24.020 --> 01:10:25.820]   that I have not finished reading.
[01:10:25.820 --> 01:10:27.500]   The definitions start on page five.
[01:10:27.500 --> 01:10:32.420]   What they're actually trying to define is quote unquote,
[01:10:32.420 --> 01:10:33.700]   trustworthy AI.
[01:10:33.700 --> 01:10:37.100]   So they're not actually dealing with what types of AI.
[01:10:37.100 --> 01:10:40.620]   So NIST is saying, trust where the AI systems are quote,
[01:10:40.620 --> 01:10:43.420]   "valid and reliable, safe, secure, and resilient,
[01:10:43.420 --> 01:10:45.700]   "accountable, transparent, explainable,
[01:10:45.700 --> 01:10:48.500]   "interpretable, privacy enhanced, and fair
[01:10:48.500 --> 01:10:51.220]   "with their harmful biased managed."
[01:10:51.220 --> 01:10:53.780]   So they're acknowledging that there's going to be biases
[01:10:53.780 --> 01:10:56.380]   and they're saying, let's figure out how to do that.
[01:10:56.380 --> 01:11:01.700]   - But Stacey, the issue becomes,
[01:11:01.700 --> 01:11:04.900]   stochastic parents writes about how even at this stage
[01:11:04.900 --> 01:11:06.660]   is not accountable.
[01:11:06.660 --> 01:11:08.260]   We don't know what old data goes in.
[01:11:08.260 --> 01:11:10.460]   We don't know how it operates.
[01:11:10.460 --> 01:11:12.020]   - Right, they're trying to figure that out.
[01:11:12.020 --> 01:11:14.500]   How are we supposed to develop this?
[01:11:14.500 --> 01:11:16.700]   How are we supposed to build accountability?
[01:11:16.700 --> 01:11:18.700]   Do we need different levels of accountability
[01:11:18.700 --> 01:11:22.060]   for different verticals like healthcare versus criminal justice
[01:11:22.060 --> 01:11:23.900]   versus auto-force?
[01:11:23.900 --> 01:11:26.940]   - Of course, that's to be concentrated on the AI.
[01:11:26.940 --> 01:11:28.500]   - That's what they're doing.
[01:11:28.500 --> 01:11:31.620]   They're trying to build something,
[01:11:31.620 --> 01:11:33.180]   they're trying to build a framework
[01:11:33.180 --> 01:11:37.500]   for when you develop an AI, how do you make it accountable?
[01:11:37.500 --> 01:11:39.300]   So they're basically saying, look,
[01:11:39.300 --> 01:11:41.580]   it's kind of like how do we establish a grading scale
[01:11:41.580 --> 01:11:43.180]   that makes sense for AI?
[01:11:43.180 --> 01:11:46.260]   That is literally what this is asking input on.
[01:11:46.260 --> 01:11:48.820]   It's not saying this AI is good or this is bad
[01:11:48.820 --> 01:11:50.260]   or we're going to stop using all this AI.
[01:11:50.260 --> 01:11:54.620]   They're saying, okay, if I'm going to create an algorithm
[01:11:54.620 --> 01:11:58.580]   to do XYZ, how do I make sure it's performing
[01:11:58.580 --> 01:12:00.260]   like I want it to perform?
[01:12:00.260 --> 01:12:04.860]   How do we set guardrails for the use of that
[01:12:04.860 --> 01:12:07.780]   within an industry that means if it doesn't perform correctly,
[01:12:07.780 --> 01:12:11.100]   that people can address that, right?
[01:12:11.100 --> 01:12:15.380]   That there's a mechanism for solving any issues that arise.
[01:12:15.380 --> 01:12:18.060]   So there's lots of things here.
[01:12:18.060 --> 01:12:19.820]   They're not trying to say,
[01:12:19.820 --> 01:12:23.860]   they're not trying to govern how an algorithm is developed.
[01:12:23.860 --> 01:12:27.060]   They're not trying to govern what you can make
[01:12:27.060 --> 01:12:28.540]   an algorithm about.
[01:12:28.540 --> 01:12:31.860]   They're just saying, look, we're doing this.
[01:12:31.860 --> 01:12:33.540]   How do we make sure it works for every--
[01:12:33.540 --> 01:12:36.060]   - So I make an algorithm that just assembles words
[01:12:36.060 --> 01:12:38.060]   based on their relationship with their words.
[01:12:38.060 --> 01:12:40.220]   I do not warrant that it's going on facts.
[01:12:40.220 --> 01:12:42.940]   I do not warrant that it's going to be nice.
[01:12:42.940 --> 01:12:46.380]   I don't warrant anything, but I want to see what comes out.
[01:12:46.380 --> 01:12:47.660]   - They would say, how do you build?
[01:12:47.660 --> 01:12:50.700]   So this is asking, how do I build?
[01:12:50.700 --> 01:12:54.300]   What kind of accountability features do I need for a--
[01:12:54.300 --> 01:12:57.140]   - Or if I say no, what if I say I want to see
[01:12:57.140 --> 01:13:01.300]   what the language associations of humanity--
[01:13:01.300 --> 01:13:04.260]   - I don't think they care about the AI itself,
[01:13:04.260 --> 01:13:08.340]   but they do care if your school system implements
[01:13:08.340 --> 01:13:10.260]   a large language model for detecting
[01:13:10.260 --> 01:13:13.180]   if a kid is going to shoot up the school.
[01:13:13.180 --> 01:13:15.420]   That is when the rules come into play.
[01:13:15.420 --> 01:13:16.940]   That is the type of things they're trying to--
[01:13:16.940 --> 01:13:19.020]   - Right, so the accountability goes to the humans,
[01:13:19.020 --> 01:13:21.180]   not the machine in that case, to say that unless you can
[01:13:21.180 --> 01:13:24.140]   back this up and you can verify that it gives you
[01:13:24.140 --> 01:13:26.700]   true results, you shouldn't use it.
[01:13:26.700 --> 01:13:28.780]   And I would agree with that fully.
[01:13:28.780 --> 01:13:30.980]   - Well, and those are, then you should put a comment
[01:13:30.980 --> 01:13:33.900]   saying that, that's what they're asking for.
[01:13:33.900 --> 01:13:35.820]   Like literally this discussion,
[01:13:35.820 --> 01:13:38.100]   that is what they're trying to do.
[01:13:38.100 --> 01:13:41.180]   They're talking about, they ask for definitions for audits.
[01:13:41.180 --> 01:13:43.220]   What does an assessment need to look like?
[01:13:43.220 --> 01:13:46.340]   Right now, they're like, hey,
[01:13:46.340 --> 01:13:48.700]   right now we look for harmful bias and discrimination.
[01:13:48.700 --> 01:13:50.780]   We look for if it's valid and effective.
[01:13:50.780 --> 01:13:52.140]   We look for data privacy.
[01:13:52.140 --> 01:13:54.380]   Are these the right metrics we should be looking for?
[01:13:54.380 --> 01:13:55.740]   We look for disinformation.
[01:13:55.740 --> 01:13:57.740]   Who should be doing these?
[01:13:57.740 --> 01:13:58.580]   Should it be internal?
[01:13:58.580 --> 01:14:00.460]   Should there be third parties involved?
[01:14:00.460 --> 01:14:02.140]   - But like disinformation right there.
[01:14:02.140 --> 01:14:05.100]   Then you get down to the same issues we got into
[01:14:05.100 --> 01:14:06.660]   with just the internet as a whole,
[01:14:06.660 --> 01:14:08.580]   is who's gonna be the arbiter of truth?
[01:14:08.580 --> 01:14:10.780]   Who's gonna decide what is disinformation?
[01:14:10.780 --> 01:14:11.620]   - That's what they're asking.
[01:14:11.620 --> 01:14:12.460]   - They're asking-- - They're asking
[01:14:12.460 --> 01:14:13.500]   a lot of rabbit holes.
[01:14:13.500 --> 01:14:15.300]   - Why? - Should you have,
[01:14:15.300 --> 01:14:17.740]   but just because they're a rabbit holes,
[01:14:17.740 --> 01:14:20.380]   Jeff doesn't mean we shouldn't start exploring them.
[01:14:20.380 --> 01:14:22.660]   - Okay, but I just get nervous. - This is the earliest stages.
[01:14:22.660 --> 01:14:24.220]   - And again, I'm not a little bit nervous.
[01:14:24.220 --> 01:14:26.180]   - But you know what? - Sorry, Sasey, go ahead.
[01:14:26.180 --> 01:14:27.020]   You keep going.
[01:14:27.020 --> 01:14:31.140]   - You're bringing up all of these things
[01:14:31.140 --> 01:14:33.980]   because you are nervous,
[01:14:33.980 --> 01:14:36.580]   but you're bringing it up in a way that's like,
[01:14:36.580 --> 01:14:39.380]   this seems stupid and we should shut it down.
[01:14:39.380 --> 01:14:40.500]   You should bring it up,
[01:14:40.500 --> 01:14:42.580]   I mean, it's much more helpful to bring it up like,
[01:14:42.580 --> 01:14:46.020]   hey, this is something I worry about, let's address it.
[01:14:46.020 --> 01:14:49.500]   And this is where, like, when you call this moral panic,
[01:14:49.500 --> 01:14:51.420]   you're basically shutting-- - Yeah, yep.
[01:14:51.420 --> 01:14:54.380]   - I know, not yet, but I feel like we're getting there.
[01:14:54.380 --> 01:14:55.780]   When you veer towards that,
[01:14:55.780 --> 01:14:57.620]   you're basically saying you're shutting down
[01:14:57.620 --> 01:14:59.460]   the discussion early and it's a discussion
[01:14:59.460 --> 01:15:00.780]   we so need to have.
[01:15:00.780 --> 01:15:03.060]   - I think, Sasey, I think you're right.
[01:15:03.060 --> 01:15:03.900]   I think you're right.
[01:15:03.900 --> 01:15:08.500]   What I'm nervous about is bad legislators
[01:15:08.500 --> 01:15:09.980]   and some bad regulators,
[01:15:09.980 --> 01:15:11.500]   and this goes to Mike Masnick's paper,
[01:15:11.500 --> 01:15:14.380]   is that there are unintended consequences galore
[01:15:14.380 --> 01:15:17.020]   to thinking that we know enough early
[01:15:17.020 --> 01:15:20.780]   in the stage of technology to do that.
[01:15:20.780 --> 01:15:24.180]   But I agree that if this kind of discussion
[01:15:24.180 --> 01:15:25.900]   is a good discussion to have,
[01:15:25.900 --> 01:15:28.540]   I will stipulate that your honor.
[01:15:28.540 --> 01:15:29.380]   - Yes.
[01:15:29.380 --> 01:15:32.660]   - And legislation, I know we're in a really whacked out era
[01:15:32.660 --> 01:15:34.260]   where we can't actually get things done,
[01:15:34.260 --> 01:15:37.860]   but for the most part, legislation can,
[01:15:37.860 --> 01:15:39.860]   it's stupid to think we're gonna have legislation
[01:15:39.860 --> 01:15:43.620]   that will be 100% right the first time we implement it.
[01:15:43.620 --> 01:15:45.460]   That's never gonna happen, right?
[01:15:45.460 --> 01:15:47.260]   So you've gotta start somewhere
[01:15:47.260 --> 01:15:49.100]   and then you've gotta adjust.
[01:15:49.100 --> 01:15:50.900]   - Then you have laws like Utah's though,
[01:15:50.900 --> 01:15:54.460]   which is gonna take everybody under 18 off social media,
[01:15:54.460 --> 01:15:56.420]   which is abhorrent in my view.
[01:15:56.420 --> 01:15:59.580]   - That's gonna get adjusted real fricking fast, I promise.
[01:15:59.580 --> 01:16:01.060]   - They can drive cars.
[01:16:01.060 --> 01:16:02.580]   (both laughing)
[01:16:02.580 --> 01:16:03.480]   - And buy guns.
[01:16:03.480 --> 01:16:05.900]   (both laughing)
[01:16:05.900 --> 01:16:06.740]   - They can do something.
[01:16:06.740 --> 01:16:07.580]   - So I guess--
[01:16:07.580 --> 01:16:10.860]   - Yeah, I listen to, I'm listening to you all
[01:16:10.860 --> 01:16:13.540]   and I'm thinking about leadership and the legislature
[01:16:13.540 --> 01:16:14.380]   and what not.
[01:16:14.380 --> 01:16:16.740]   And I agree, we gotta start somewhere
[01:16:16.740 --> 01:16:20.420]   because if we put it in the hands of, in my opinion,
[01:16:20.420 --> 01:16:22.380]   put it in the hands of our current leadership
[01:16:22.380 --> 01:16:24.300]   it's just gonna be a bit of a mess
[01:16:24.300 --> 01:16:27.740]   because a lot of them are so uninformed on things
[01:16:27.740 --> 01:16:30.900]   and have their own agendas and fears
[01:16:30.900 --> 01:16:34.820]   and some of it is warranted, most of it's not.
[01:16:35.940 --> 01:16:39.740]   But I guess over time it's gonna take citizens voting
[01:16:39.740 --> 01:16:42.580]   better people and to help, I don't know,
[01:16:42.580 --> 01:16:46.340]   educate current leadership and really get a grip
[01:16:46.340 --> 01:16:49.020]   on this stuff where we can try to put some regulations
[01:16:49.020 --> 01:16:52.100]   in place or safe rail, safeguards in place.
[01:16:52.100 --> 01:16:56.700]   - I feel like our age, my parents age,
[01:16:56.700 --> 01:16:58.020]   I don't know about younger kids,
[01:16:58.020 --> 01:17:00.660]   but we've grown up in an era where government,
[01:17:00.660 --> 01:17:03.180]   it's bad, it's terrible, but government doesn't have
[01:17:03.180 --> 01:17:05.060]   to be bad and terrible, doesn't have to be
[01:17:05.060 --> 01:17:10.060]   - No, I never really felt like government was bad.
[01:17:10.060 --> 01:17:13.340]   I did hear that a lot growing up.
[01:17:13.340 --> 01:17:17.940]   But as I've gotten older, I will say,
[01:17:17.940 --> 01:17:22.460]   government doesn't always know what they assume they know
[01:17:22.460 --> 01:17:25.140]   or it's not bad, it just doesn't always work.
[01:17:25.140 --> 01:17:26.900]   (laughing)
[01:17:26.900 --> 01:17:29.220]   - As intended, as intended, right?
[01:17:29.220 --> 01:17:32.220]   - That's a feat, I mean, like heck, I'm not evil,
[01:17:32.220 --> 01:17:34.660]   but I make stupid decisions all the time
[01:17:34.660 --> 01:17:38.820]   and I come back and fix them and so I don't know,
[01:17:38.820 --> 01:17:43.460]   hey, aunt, read Terra Formers because it's all
[01:17:43.460 --> 01:17:45.820]   the underlying theory of that entire book
[01:17:45.820 --> 01:17:47.700]   is the role of government and what it means
[01:17:47.700 --> 01:17:50.660]   to participate in government and how to make rules.
[01:17:50.660 --> 01:17:52.940]   You're either gonna love it or hate it.
[01:17:52.940 --> 01:17:56.860]   - Okay, Jammer Bee gave a couple of rust in the air
[01:17:56.860 --> 01:18:00.260]   really fast thumbs up, so he's just ready right now.
[01:18:00.260 --> 01:18:02.260]   - Outstanding. - It's so good.
[01:18:02.260 --> 01:18:03.980]   - Outstanding. - Outstanding.
[01:18:03.980 --> 01:18:06.860]   - I mean, for the record for people getting mad at me,
[01:18:06.860 --> 01:18:09.140]   I'm sorry I interrupted you, Stacey, I got excited.
[01:18:09.140 --> 01:18:10.580]   I apologize, I didn't mean interrupt.
[01:18:10.580 --> 01:18:12.660]   - Oh, I'm sorry, I didn't notice Jeff.
[01:18:12.660 --> 01:18:13.500]   (laughing)
[01:18:13.500 --> 01:18:16.540]   - Others will, so I apologize, I know the error
[01:18:16.540 --> 01:18:18.980]   of my ways, people, I apologize.
[01:18:18.980 --> 01:18:21.740]   - It's okay for Jeff to interrupt me.
[01:18:21.740 --> 01:18:23.100]   - No, it's not. - This time.
[01:18:23.100 --> 01:18:23.940]   (laughing)
[01:18:23.940 --> 01:18:26.860]   - I mean, I interrupt you, it's hard on the show.
[01:18:26.860 --> 01:18:28.020]   - It is hard, yes.
[01:18:28.020 --> 01:18:30.660]   - My biggest pet peeve, oh, sorry.
[01:18:30.660 --> 01:18:31.660]   - No, go, go, go.
[01:18:31.660 --> 01:18:32.660]   (laughing)
[01:18:32.660 --> 01:18:33.500]   - What is that?
[01:18:33.500 --> 01:18:34.500]   (laughing)
[01:18:34.500 --> 01:18:36.940]   - No, it's not that, it's when I say something
[01:18:36.940 --> 01:18:39.060]   and then Leo will come on the show
[01:18:39.060 --> 01:18:40.860]   and he'll be like, so like Jeff said,
[01:18:40.860 --> 01:18:42.140]   and I'm like, I said that.
[01:18:42.140 --> 01:18:44.740]   - Oh, yes, yes, yes, yes, yes.
[01:18:44.740 --> 01:18:45.740]   (laughing)
[01:18:45.740 --> 01:18:46.580]   - For the rest of the show,
[01:18:46.580 --> 01:18:48.340]   I'm gonna attribute everything to you, Stacey.
[01:18:48.340 --> 01:18:49.580]   - Then make up for that.
[01:18:49.580 --> 01:18:50.420]   - Oh, punch.
[01:18:50.420 --> 01:18:52.380]   - What are the dumb stuff that I say?
[01:18:52.380 --> 01:18:53.500]   - That punch, but.
[01:18:53.500 --> 01:18:57.260]   - We're gonna try and filter and attribute only the good stuff
[01:18:57.260 --> 01:18:58.900]   to Stacey for the rest of the show.
[01:18:58.900 --> 01:19:00.420]   (laughing)
[01:19:00.420 --> 01:19:02.140]   - He doesn't do it on purpose, it's not like
[01:19:02.140 --> 01:19:03.420]   I just think it's just hilarious.
[01:19:03.420 --> 01:19:04.260]   - Totally.
[01:19:04.260 --> 01:19:06.380]   - What is, I don't exist.
[01:19:06.380 --> 01:19:07.660]   Anyway, go on, go on.
[01:19:07.660 --> 01:19:08.660]   - Shop liver.
[01:19:08.660 --> 01:19:12.020]   Before we venture out of AI,
[01:19:12.020 --> 01:19:14.740]   Jeff, you compiled a few AI examples.
[01:19:14.740 --> 01:19:16.100]   I thought I'd shine a,
[01:19:16.100 --> 01:19:19.660]   they were, I think they were from one of my newsletters.
[01:19:19.660 --> 01:19:23.700]   I think that came from which one, TLDR,
[01:19:23.700 --> 01:19:24.540]   I think put them in there.
[01:19:24.540 --> 01:19:25.860]   It was interesting, or maybe no,
[01:19:25.860 --> 01:19:28.060]   it was better at Evans, I think.
[01:19:28.060 --> 01:19:28.980]   But they were interesting things
[01:19:28.980 --> 01:19:31.420]   of what people are doing with this now, right?
[01:19:31.420 --> 01:19:33.500]   To create AI just by describing it,
[01:19:33.500 --> 01:19:38.500]   I think the idea of programming fritters away,
[01:19:38.500 --> 01:19:41.020]   as you can describe what you want a machine to do
[01:19:41.020 --> 01:19:41.860]   and make it do it.
[01:19:41.860 --> 01:19:43.820]   And then there was another example in there
[01:19:43.820 --> 01:19:48.820]   of AI that will correct bugs as it runs the program.
[01:19:48.820 --> 01:19:52.300]   There's also AI that will make videos,
[01:19:52.300 --> 01:19:55.020]   which scares me 'cause it'll make more short-form videos
[01:19:55.020 --> 01:19:58.620]   'cause a lot of junk is gonna be perpetrated on TikTok
[01:19:58.620 --> 01:19:59.980]   and that's gonna upset me.
[01:20:00.820 --> 01:20:02.980]   But on the other hand, then there's another one.
[01:20:02.980 --> 01:20:05.500]   I'm curious what you as parents think about this one.
[01:20:05.500 --> 01:20:07.260]   Get your kids to love reading
[01:20:07.260 --> 01:20:09.580]   with choose your own adventures,
[01:20:09.580 --> 01:20:12.660]   powered by chat GPT4.
[01:20:12.660 --> 01:20:14.740]   You think, oh my God, no one really doing to our kids.
[01:20:14.740 --> 01:20:19.740]   On the other hand, if a kid can guide creatively a story
[01:20:19.740 --> 01:20:23.500]   and it can come back and tell the kid story back to the kid,
[01:20:23.500 --> 01:20:24.900]   is that bad or is that good?
[01:20:24.900 --> 01:20:27.620]   At first I had a reflexive, you and I had a reflexive--
[01:20:27.620 --> 01:20:28.940]   - Put it in the world, you know.
[01:20:28.940 --> 01:20:30.420]   - Reaction against it, but then I thought,
[01:20:30.420 --> 01:20:32.380]   "No, that's pretty cool."
[01:20:32.380 --> 01:20:33.620]   - It's like fan fit.
[01:20:33.620 --> 01:20:35.820]   - Yeah. - But choose your own adventure.
[01:20:35.820 --> 01:20:38.780]   - I mean, to a certain degree,
[01:20:38.780 --> 01:20:40.620]   it's just like a text adventure.
[01:20:40.620 --> 01:20:43.500]   That's really all, you know what I mean?
[01:20:43.500 --> 01:20:46.340]   - Fancy. - It kind of reminds me of like,
[01:20:46.340 --> 01:20:49.180]   yes, when I was a kid, I loved to choose your own adventure.
[01:20:49.180 --> 01:20:50.940]   But part of the reason that I loved it was
[01:20:50.940 --> 01:20:53.580]   because it was kind of like playing a game, you know?
[01:20:53.580 --> 01:20:56.340]   And at the time, I think I was also very into
[01:20:56.340 --> 01:20:58.740]   that old word game, Zork.
[01:20:58.740 --> 01:21:00.900]   I don't know if any of you ever played Zork back in the day,
[01:21:00.900 --> 01:21:02.220]   but you know-- - Didn't work well,
[01:21:02.220 --> 01:21:03.060]   but I heard of it.
[01:21:03.060 --> 01:21:05.980]   - Old school, you know, word text adventure,
[01:21:05.980 --> 01:21:08.420]   this idea that, you know, through words
[01:21:08.420 --> 01:21:11.420]   and using your mind to construct the reality
[01:21:11.420 --> 01:21:13.260]   of the words that are on the page,
[01:21:13.260 --> 01:21:14.900]   you kind of create the story.
[01:21:14.900 --> 01:21:17.380]   And I realized they're kind of different things,
[01:21:17.380 --> 01:21:21.780]   a word game versus a choose your own adventure book.
[01:21:21.780 --> 01:21:24.740]   This kind of seems like a combination of both,
[01:21:24.740 --> 01:21:26.300]   unless I'm misunderstanding it.
[01:21:26.300 --> 01:21:27.700]   I don't know, I think it's pretty neat.
[01:21:27.700 --> 01:21:31.580]   - Like I was thinking on, I think I may have posted this
[01:21:31.580 --> 01:21:34.300]   on "A Master and I Didn't Do A Whole Lot of Social Networking"
[01:21:34.300 --> 01:21:36.740]   while I was away on vacation.
[01:21:36.740 --> 01:21:38.980]   But one of the things that I happened to come across
[01:21:38.980 --> 01:21:41.860]   was someone that mentioned the fact that, you know,
[01:21:41.860 --> 01:21:46.220]   at some point AI will likely, like generative AI
[01:21:46.220 --> 01:21:49.100]   will likely get to the point, or, you know,
[01:21:49.100 --> 01:21:51.300]   it's totally believable that it would,
[01:21:51.300 --> 01:21:53.500]   that the movie that I watched tonight
[01:21:53.500 --> 01:21:56.140]   is the movie that I prompt my AI to create.
[01:21:56.140 --> 01:21:59.100]   Like, I wanna watch a movie about blah, blah, blah.
[01:21:59.100 --> 01:22:03.780]   And then I can't wait to see how Stacy does that to her TV.
[01:22:03.780 --> 01:22:06.740]   I wanna documentary, but I don't want it to be like this.
[01:22:06.740 --> 01:22:08.340]   I want it to be a little goofy,
[01:22:08.340 --> 01:22:10.060]   but I want it to be like that.
[01:22:10.060 --> 01:22:11.820]   That's Stacy's heaven.
[01:22:11.820 --> 01:22:15.140]   - Leslie Nope is the Witcher.
[01:22:15.140 --> 01:22:17.900]   (both laughing)
[01:22:17.900 --> 01:22:20.260]   - I wanna watch it. - I think I'll point Jason.
[01:22:20.260 --> 01:22:21.980]   Yeah, yeah, sorry, go.
[01:22:21.980 --> 01:22:25.100]   - I put a link, oh, sorry Jeff, can you go?
[01:22:25.100 --> 01:22:27.900]   - No, no, now we all don't want to interrupt each other.
[01:22:27.900 --> 01:22:29.220]   (both laughing)
[01:22:29.220 --> 01:22:30.740]   - Okay, I'll go.
[01:22:30.740 --> 01:22:33.260]   - I'll go. - I've got a link in social.
[01:22:33.260 --> 01:22:34.620]   I was, 'cause when you were talking about
[01:22:34.620 --> 01:22:37.980]   Choose Your Own Adventure, I read this I think yesterday,
[01:22:37.980 --> 01:22:42.020]   but it was a fortunate article about someone using chat GPT
[01:22:42.020 --> 01:22:45.700]   to be the dungeon master, to be at the DM
[01:22:45.700 --> 01:22:47.620]   for a Dungeons and Dragons game.
[01:22:47.620 --> 01:22:51.540]   And apparently it sucks at it, but I was actually thinking
[01:22:51.540 --> 01:22:54.340]   that this would be like a, I don't know how many of you
[01:22:54.340 --> 01:22:57.340]   will play D&D when you were kids, but I did.
[01:22:57.340 --> 01:23:00.420]   It's kind of a fun idea of like,
[01:23:00.420 --> 01:23:05.020]   you could kind of play with by yourself or.
[01:23:05.020 --> 01:23:07.260]   (both laughing)
[01:23:07.260 --> 01:23:09.460]   I don't know how not to sound pathetic talking about this,
[01:23:09.460 --> 01:23:10.300]   but like.
[01:23:10.300 --> 01:23:13.220]   (both laughing)
[01:23:13.220 --> 01:23:16.460]   - Your parents were already scared about you playing
[01:23:16.460 --> 01:23:19.540]   Dungeons and Dragons, and how you do it by yourself.
[01:23:19.540 --> 01:23:21.380]   - That's scary. - You have no dungeon master
[01:23:21.380 --> 01:23:24.180]   to play with now you do. - No, that's really sad.
[01:23:25.180 --> 01:23:26.020]   (both laughing)
[01:23:26.020 --> 01:23:28.700]   - You can't find any more nerds to play with.
[01:23:28.700 --> 01:23:30.260]   So you make one up.
[01:23:30.260 --> 01:23:31.740]   It's okay though, it's okay.
[01:23:31.740 --> 01:23:34.620]   - But you can't create other players.
[01:23:34.620 --> 01:23:35.460]   - Yeah.
[01:23:35.460 --> 01:23:36.460]   - Okay.
[01:23:36.460 --> 01:23:40.060]   Anyway, it didn't work in this particular example,
[01:23:40.060 --> 01:23:41.380]   but I actually was kind of excited.
[01:23:41.380 --> 01:23:43.780]   I was thinking about like, how could you use generative AI
[01:23:43.780 --> 01:23:46.500]   to create role playing games?
[01:23:46.500 --> 01:23:48.180]   Okay, I'm gonna stop talking 'cause
[01:23:48.180 --> 01:23:49.620]   (both laughing)
[01:23:49.620 --> 01:23:51.500]   I'm outing myself left and right in ways
[01:23:51.500 --> 01:23:52.620]   that are uncomfortable.
[01:23:54.060 --> 01:23:56.740]   - See in an article here, I've been my friends favorite
[01:23:56.740 --> 01:23:58.260]   dungeon master for two years now,
[01:23:58.260 --> 01:24:01.700]   and I gave chat GPT fuel, Dungeons and Dragons a try.
[01:24:01.700 --> 01:24:03.060]   I'm not threatened.
[01:24:03.060 --> 01:24:04.060]   - So there you go.
[01:24:04.060 --> 01:24:07.340]   - Give it time.
[01:24:07.340 --> 01:24:10.820]   - Yeah, 'cause isn't being a DM like a,
[01:24:10.820 --> 01:24:13.380]   that's a dadgum skill, you know?
[01:24:13.380 --> 01:24:14.220]   - For sure.
[01:24:14.220 --> 01:24:17.940]   - You've gotta be creative, you've gotta track all this.
[01:24:17.940 --> 01:24:19.740]   - You can play in.
[01:24:19.740 --> 01:24:21.940]   I'm like, how do you all come up with this?
[01:24:21.940 --> 01:24:25.180]   - I don't even know, like it floors me, but yeah.
[01:24:25.180 --> 01:24:27.220]   That is not a skill.
[01:24:27.220 --> 01:24:28.980]   - But if you think about all,
[01:24:28.980 --> 01:24:31.220]   if you look at all the dungeon master guides, right?
[01:24:31.220 --> 01:24:32.620]   There's a lot of information in there,
[01:24:32.620 --> 01:24:35.020]   and if you could as a DM, like if you could just do
[01:24:35.020 --> 01:24:36.940]   all the creative fun part, and then just be like,
[01:24:36.940 --> 01:24:40.820]   oh yeah, what do I need to roll to defeat an orc
[01:24:40.820 --> 01:24:43.580]   or whatever, you know, that kind of information,
[01:24:43.580 --> 01:24:48.020]   it might be nice to not have to kind of have it,
[01:24:48.020 --> 01:24:51.380]   like my notes when I hated being the DM also,
[01:24:51.380 --> 01:24:53.180]   it was a sucky, thinkless job.
[01:24:53.180 --> 01:24:54.900]   (laughing)
[01:24:54.900 --> 01:24:55.740]   I don't know.
[01:24:55.740 --> 01:24:59.940]   Maybe it's 'cause I just didn't like it.
[01:24:59.940 --> 01:25:02.740]   - I want no part of it.
[01:25:02.740 --> 01:25:07.300]   - Yeah, that's kind of the thought of being a DM
[01:25:07.300 --> 01:25:08.340]   is frightening to me.
[01:25:08.340 --> 01:25:10.940]   That does not sound like my cup of tea.
[01:25:10.940 --> 01:25:12.300]   Being on the spot right there,
[01:25:12.300 --> 01:25:15.140]   kind of creating the game as you go, no thank you.
[01:25:15.140 --> 01:25:16.500]   Rather be on the other side.
[01:25:16.500 --> 01:25:19.820]   That is our AI block.
[01:25:19.820 --> 01:25:23.020]   I noticed that there's no real TikTok corner in here
[01:25:23.020 --> 01:25:24.820]   with content, but there is some news about it.
[01:25:24.820 --> 01:25:26.340]   - Oh, there is, I'm sorry.
[01:25:26.340 --> 01:25:28.580]   - But there is a substack is interesting.
[01:25:28.580 --> 01:25:32.060]   - Well, yes, okay, so hold on to that.
[01:25:32.060 --> 01:25:34.220]   Yes. - I know, I'm gonna do it easier.
[01:25:34.220 --> 01:25:35.300]   - That's right.
[01:25:35.300 --> 01:25:37.140]   Hold on to substack.
[01:25:37.140 --> 01:25:39.300]   They've got some interesting news
[01:25:39.300 --> 01:25:41.700]   that we're gonna talk about in a moment,
[01:25:41.700 --> 01:25:45.420]   but first, let's take a second to thank the sponsor
[01:25:45.420 --> 01:25:47.580]   of this episode of This Week in Google,
[01:25:47.580 --> 01:25:50.260]   brought to you by Cisco Maraki,
[01:25:50.260 --> 01:25:53.780]   the experts in cloud-based networking for hybrid work.
[01:25:53.780 --> 01:25:56.820]   So whether your employees are working at home,
[01:25:56.820 --> 01:25:58.940]   whether they're sitting at the cabin,
[01:25:58.940 --> 01:26:00.740]   a cabin in the mountains doing their work,
[01:26:00.740 --> 01:26:02.980]   they're on a lounge chair at the beach,
[01:26:02.980 --> 01:26:05.100]   a cloud-managed network provides
[01:26:05.100 --> 01:26:07.860]   the same exceptional work experience,
[01:26:07.860 --> 01:26:10.060]   no matter where they happen to be.
[01:26:10.060 --> 01:26:12.020]   So you may as well roll out the welcome mat,
[01:26:12.020 --> 01:26:15.820]   because as you probably already know, hybrid work,
[01:26:15.820 --> 01:26:19.980]   it's here to stay, really feels like it's made itself
[01:26:19.980 --> 01:26:21.540]   comfortable and it's not going anywhere.
[01:26:21.540 --> 01:26:24.220]   Hybrid work works best in the cloud.
[01:26:24.220 --> 01:26:27.620]   It has its perks for both employees and for leaders.
[01:26:27.620 --> 01:26:30.060]   Of course, workers can move faster,
[01:26:30.060 --> 01:26:31.660]   they can deliver better results
[01:26:31.660 --> 01:26:33.500]   with a cloud-managed network.
[01:26:33.500 --> 01:26:36.780]   Leaders can automate distributed operations,
[01:26:36.780 --> 01:26:39.180]   it can build more sustainable workspaces
[01:26:39.180 --> 01:26:42.780]   and proactively protect the network.
[01:26:42.780 --> 01:26:46.380]   An IDG, a market pulse research report conducted,
[01:26:46.380 --> 01:26:49.580]   for Maraki highlights top tier opportunities
[01:26:49.580 --> 01:26:50.940]   in supporting hybrid work.
[01:26:50.940 --> 01:26:54.660]   And hybrid work, some of these highlights here,
[01:26:54.660 --> 01:26:59.140]   hybrid work is a priority for 78% of C-suite executives.
[01:26:59.140 --> 01:27:02.540]   Leaders are wanting to drive collaboration forward,
[01:27:02.540 --> 01:27:06.740]   they wanna stay on top of or even boost
[01:27:06.740 --> 01:27:08.580]   productivity and security.
[01:27:08.580 --> 01:27:11.620]   Hybrid work also has its challenges.
[01:27:11.620 --> 01:27:12.580]   There's a lot of challenges.
[01:27:12.580 --> 01:27:16.860]   The IDG report raises the red flag about security.
[01:27:16.860 --> 01:27:20.540]   That's a real big one, noting that 48% of leaders
[01:27:20.540 --> 01:27:23.860]   report cybersecurity threats as a primary obstacle
[01:27:23.860 --> 01:27:27.380]   to improving workforce experiences.
[01:27:27.380 --> 01:27:30.260]   Always on security monitoring is part of what makes
[01:27:30.260 --> 01:27:33.460]   the cloud-managed network so awesome.
[01:27:33.460 --> 01:27:37.300]   IT can actually use apps from Maraki's vast ecosystem
[01:27:37.300 --> 01:27:40.020]   of partners, these are turn key solutions
[01:27:40.020 --> 01:27:41.580]   that are built to work seamlessly
[01:27:41.580 --> 01:27:45.180]   with the Maraki Cloud platform for things like asset tracking,
[01:27:45.180 --> 01:27:48.660]   location analytics, and a whole lot more.
[01:27:48.660 --> 01:27:52.140]   And you can do these things to achieve a number of goals here.
[01:27:52.140 --> 01:27:55.580]   You can gather insight on how people are using their workspaces.
[01:27:55.580 --> 01:27:58.420]   So in a smart space, environmental sensors
[01:27:58.420 --> 01:28:01.820]   can actually track activity and occupancy levels
[01:28:01.820 --> 01:28:04.460]   to stay on top of cleanliness is one example.
[01:28:04.460 --> 01:28:08.100]   Reserve workspaces.
[01:28:08.100 --> 01:28:09.860]   And that's based on vacancy,
[01:28:09.860 --> 01:28:14.140]   that's based on employee profiles, also called hot-desking.
[01:28:14.140 --> 01:28:19.220]   This allows employees to scout out a spot in a snap.
[01:28:19.220 --> 01:28:23.060]   Locations in restricted environments can be booked in advance.
[01:28:23.060 --> 01:28:25.500]   That can include time-based door access
[01:28:25.500 --> 01:28:27.860]   and other conveniences.
[01:28:27.860 --> 01:28:29.940]   And then mobile device management,
[01:28:29.940 --> 01:28:33.860]   integrating devices and systems that actually allow IT
[01:28:33.860 --> 01:28:37.100]   to manage, update, and troubleshoot company-owned devices.
[01:28:37.100 --> 01:28:42.140]   Even when the device and the employee are in that remote location.
[01:28:42.140 --> 01:28:44.860]   So you can turn any space into a place of productivity.
[01:28:44.860 --> 01:28:47.500]   You can empower your organization
[01:28:47.500 --> 01:28:49.460]   with the same exceptional experience,
[01:28:49.460 --> 01:28:51.260]   no matter where they are working,
[01:28:51.260 --> 01:28:54.420]   no matter where they call work with Maraki
[01:28:54.420 --> 01:28:57.060]   and the Cisco suite of technology.
[01:28:57.060 --> 01:29:01.660]   You can learn how your organization can make hybrid work work.
[01:29:01.660 --> 01:29:06.140]   All you gotta do is visit maraki.cisco.com/twit.
[01:29:06.140 --> 01:29:08.780]   And we thank Cisco, Maraki for their support,
[01:29:08.780 --> 01:29:10.860]   their continued support, the Twit Network,
[01:29:10.860 --> 01:29:13.420]   and this week in Google.
[01:29:13.420 --> 01:29:17.540]   All right, so yes, so sub-stack.
[01:29:17.540 --> 01:29:19.900]   Had a bit of an announcement.
[01:29:19.900 --> 01:29:22.420]   I guess they first announced this last week.
[01:29:22.420 --> 01:29:28.100]   And now their new feature called Notes
[01:29:28.100 --> 01:29:32.260]   is rolling out to all users effective yesterday.
[01:29:32.260 --> 01:29:33.260]   - Have you played with it yet?
[01:29:33.260 --> 01:29:35.540]   - I am not really...
[01:29:35.540 --> 01:29:38.940]   - If you subscribe to any Substack newsletter,
[01:29:38.940 --> 01:29:42.540]   they know who you are, and so I didn't even have to sign up.
[01:29:42.540 --> 01:29:45.340]   I just went to it and boom, I was in it.
[01:29:45.340 --> 01:29:46.660]   - And that was it, so what did you think?
[01:29:46.660 --> 01:29:47.500]   Like...
[01:29:47.500 --> 01:29:49.140]   - Substack.com/notes, it looks...
[01:29:49.140 --> 01:29:53.700]   I kinda, I never wanna agree with Elon Musk on this Earth,
[01:29:53.700 --> 01:29:55.940]   but I kind of agree with him at one point.
[01:29:55.940 --> 01:29:57.740]   It looks exactly like Twitter.
[01:29:57.740 --> 01:29:58.580]   - Oh really?
[01:29:58.580 --> 01:30:02.060]   - It is a complete rip off of Twitter, but hey, fine.
[01:30:02.060 --> 01:30:04.500]   It's gonna rip off somebody, rip off Elon Musk now.
[01:30:05.340 --> 01:30:07.900]   So it's mainly the, it starts off with being,
[01:30:07.900 --> 01:30:10.420]   it has the starter kit, one of the big problems
[01:30:10.420 --> 01:30:13.340]   of all these, a mast done or Twitter at any of them is,
[01:30:13.340 --> 01:30:14.740]   well who do I start following?
[01:30:14.740 --> 01:30:17.540]   Well it starts off with all the sub-stack authors
[01:30:17.540 --> 01:30:18.540]   promoting their own stuff.
[01:30:18.540 --> 01:30:20.060]   They've obviously been there for a while.
[01:30:20.060 --> 01:30:23.900]   So it has content going in, it looks nice.
[01:30:23.900 --> 01:30:26.300]   The people they have, there's some people on sub-stack
[01:30:26.300 --> 01:30:28.260]   I really don't like, but you know,
[01:30:28.260 --> 01:30:29.900]   there's people there I do like.
[01:30:29.900 --> 01:30:34.460]   The problem I have of course is just like post.news,
[01:30:34.460 --> 01:30:38.180]   is that it's owned by one company
[01:30:38.180 --> 01:30:41.300]   and they can be bought by a nihilistic
[01:30:41.300 --> 01:30:45.740]   idiot just like Twitter was.
[01:30:45.740 --> 01:30:47.180]   And so we're still at risk.
[01:30:47.180 --> 01:30:50.860]   If you go to sub-stack.com/notes, Jason,
[01:30:50.860 --> 01:30:53.340]   it depends on whether or not you're signed in.
[01:30:53.340 --> 01:30:57.460]   - So I'm on it and I don't have that many newsletters
[01:30:57.460 --> 01:31:01.420]   that I subscribe to, but there's only one person,
[01:31:01.420 --> 01:31:06.420]   it's Timothy Lee who's put a note in right now.
[01:31:06.420 --> 01:31:07.260]   - That's all?
[01:31:07.260 --> 01:31:09.700]   - I see some other like--
[01:31:09.700 --> 01:31:11.620]   - You're subscribed or home?
[01:31:11.620 --> 01:31:14.660]   - That's subscribed, I've got Zinnip.
[01:31:14.660 --> 01:31:16.660]   How do you-- - Native to Vexy?
[01:31:16.660 --> 01:31:19.060]   - Yes, I've got her.
[01:31:19.060 --> 01:31:20.900]   - We've got a home, it should be populated
[01:31:20.900 --> 01:31:22.340]   with lots of stuff, doesn't it?
[01:31:22.340 --> 01:31:26.340]   - It's not that much, it's like 12, let's see.
[01:31:26.340 --> 01:31:30.020]   Yeah, it's like maybe 20 people as I've scrolled down
[01:31:30.020 --> 01:31:34.860]   the way and honestly, these are not the people
[01:31:34.860 --> 01:31:36.020]   I want to hear from.
[01:31:36.020 --> 01:31:37.020]   - Yeah, yeah.
[01:31:37.020 --> 01:31:39.100]   But I think it's just like post.news,
[01:31:39.100 --> 01:31:41.940]   Stacy where people were saying, "Ooh, it's a better Twitter.
[01:31:41.940 --> 01:31:42.860]   Let's all go there."
[01:31:42.860 --> 01:31:47.140]   And I'm like you, I'm basically shrugging at it.
[01:31:47.140 --> 01:31:48.340]   I thought it was--
[01:31:48.340 --> 01:31:51.300]   - Oh, but there isn't, Andrew Sullivan has
[01:31:51.300 --> 01:31:52.820]   posted a puppy picture.
[01:31:52.820 --> 01:31:55.740]   - But it's Andrew Sullivan.
[01:31:55.740 --> 01:31:57.140]   - I know, right?
[01:31:57.140 --> 01:31:58.300]   - Yeah.
[01:31:58.300 --> 01:32:00.620]   - I don't even know if I have a sub stack account,
[01:32:00.620 --> 01:32:01.900]   probably don't.
[01:32:01.900 --> 01:32:03.700]   - If you subscribe to a newsletter, you--
[01:32:03.700 --> 01:32:05.060]   - Mike Elgin is there.
[01:32:05.060 --> 01:32:08.180]   - Well, he's not sub stacking.
[01:32:08.180 --> 01:32:09.020]   - Where I'm at?
[01:32:09.020 --> 01:32:09.860]   - He's not voting.
[01:32:09.860 --> 01:32:13.140]   - In a taxi in Mexico enjoying sub stack notes.
[01:32:13.140 --> 01:32:17.900]   - Yeah, I've got the shh, who are these people?
[01:32:17.900 --> 01:32:20.620]   And why do you think I want to talk to them?
[01:32:20.620 --> 01:32:21.460]   - Yeah.
[01:32:21.460 --> 01:32:22.780]   (laughs)
[01:32:22.780 --> 01:32:24.820]   - See, I feel like post.news now,
[01:32:24.820 --> 01:32:28.180]   if I go to the, the, the subscribed and post.news,
[01:32:28.180 --> 01:32:31.940]   there's one guy who's posting, that's basically it.
[01:32:31.940 --> 01:32:34.620]   Everybody else has just given up on it.
[01:32:34.620 --> 01:32:37.100]   - I've got his economist, those people are the worst.
[01:32:37.100 --> 01:32:37.940]   - Oh.
[01:32:37.940 --> 01:32:39.740]   (laughs)
[01:32:39.740 --> 01:32:40.660]   - I'll be Stacy.
[01:32:40.660 --> 01:32:45.780]   - I get newsletters emailed to me.
[01:32:45.780 --> 01:32:46.780]   I never get a sub stack.
[01:32:46.780 --> 01:32:48.300]   - Yeah, so that's what I was like.
[01:32:48.300 --> 01:32:49.220]   That's what I was just wondering.
[01:32:49.220 --> 01:32:50.540]   Just because I get him in my email,
[01:32:50.540 --> 01:32:54.580]   does that mean I'm part of sub stack and looks like--
[01:32:54.580 --> 01:32:56.380]   - Or you could be, yes.
[01:32:56.380 --> 01:32:59.980]   - Apparently do, 'cause I just tried to log in
[01:32:59.980 --> 01:33:03.060]   and it emailed me a link to log in, so.
[01:33:03.060 --> 01:33:04.460]   Huh.
[01:33:04.460 --> 01:33:07.300]   I didn't know I had a sub stack account.
[01:33:07.300 --> 01:33:09.140]   - Yeah, oh, Dan.
[01:33:09.140 --> 01:33:11.580]   (laughs)
[01:33:11.580 --> 01:33:12.980]   Dan's a sub stack.
[01:33:12.980 --> 01:33:14.100]   - Yeah. - Yep.
[01:33:14.100 --> 01:33:16.020]   - That's right, Dan Patterson.
[01:33:16.020 --> 01:33:17.860]   Okay, Dan Patterson's newsletter.
[01:33:17.860 --> 01:33:20.700]   - Yeah, so now during this show, I've realized
[01:33:20.700 --> 01:33:22.500]   I don't actually have a sub stack account,
[01:33:22.500 --> 01:33:24.980]   even though I get some of these newsletters by email,
[01:33:24.980 --> 01:33:27.060]   so I just kind of assumed I had one,
[01:33:27.060 --> 01:33:29.540]   but apparently I don't,
[01:33:29.540 --> 01:33:32.460]   so I'm trying to sign up for some of these things
[01:33:32.460 --> 01:33:33.900]   that I get the emails from.
[01:33:33.900 --> 01:33:35.420]   - Well, at some point, Jason, maybe I did create it
[01:33:35.420 --> 01:33:36.340]   at some point.
[01:33:36.340 --> 01:33:39.700]   Oh, I know I had to, I think I pay some,
[01:33:39.700 --> 01:33:41.180]   I think I pay like one or two people.
[01:33:41.180 --> 01:33:42.700]   - Bro, this is like-- - Yeah, I paid for one,
[01:33:42.700 --> 01:33:43.860]   which may be why I have it.
[01:33:43.860 --> 01:33:44.780]   - That's why, yeah. - 'Cause I have.
[01:33:44.780 --> 01:33:45.940]   - Okay.
[01:33:45.940 --> 01:33:48.060]   - Dude, this looks like Twitter.
[01:33:48.060 --> 01:33:49.100]   - Oh boy, doesn't it?
[01:33:49.100 --> 01:33:50.500]   - With orange on it.
[01:33:50.500 --> 01:33:52.260]   - Yep. - So it looks better.
[01:33:52.260 --> 01:33:53.100]   (laughs)
[01:33:53.100 --> 01:33:54.300]   (laughs)
[01:33:54.300 --> 01:33:56.180]   But man, this looks like Twitter.
[01:33:56.180 --> 01:33:57.020]   - Wow.
[01:33:57.020 --> 01:33:59.860]   - I mean, I guess I can understand the thinking here.
[01:33:59.860 --> 01:34:03.260]   If you are writing a newsletter, well, you know,
[01:34:03.260 --> 01:34:07.140]   Stacey, you'd probably be the one to talk about this
[01:34:07.140 --> 01:34:08.900]   better than I, but if you're writing a newsletter,
[01:34:08.900 --> 01:34:11.180]   you're cultivating that community,
[01:34:11.180 --> 01:34:14.180]   having some sort of direct, you know,
[01:34:14.180 --> 01:34:17.700]   direct social engagement platform that's tied in
[01:34:17.700 --> 01:34:20.860]   with the community around the community that you're creating,
[01:34:20.860 --> 01:34:22.500]   which I mean is the same thing, right?
[01:34:22.500 --> 01:34:25.660]   That Twitter is trying to do with this newsletter thing,
[01:34:25.660 --> 01:34:28.980]   and Elon got all upset that notes even existed,
[01:34:28.980 --> 01:34:31.980]   and I don't even know the status of that at this point.
[01:34:31.980 --> 01:34:35.020]   He had kind of like filtered the things off a Twitter.
[01:34:35.020 --> 01:34:37.700]   I don't know if they're still allowed to talk about
[01:34:37.700 --> 01:34:42.060]   a sub stack at this point, but as a content creator,
[01:34:42.060 --> 01:34:44.740]   does it make sense to have this sort of a network tied
[01:34:44.740 --> 01:34:48.580]   so closely to the place that you're using
[01:34:48.580 --> 01:34:50.260]   to distribute that information?
[01:34:51.180 --> 01:34:55.420]   No, I'd rather have, so as a content creator,
[01:34:55.420 --> 01:34:58.660]   I create like, I have, I probably have,
[01:34:58.660 --> 01:35:01.700]   if I think about, I will say I have four different,
[01:35:01.700 --> 01:35:03.540]   five different audiences actually.
[01:35:03.540 --> 01:35:06.460]   So I have the audience that follows my newsletter
[01:35:06.460 --> 01:35:09.540]   that signs up actively, and they care about their things.
[01:35:09.540 --> 01:35:12.380]   I have the audience who downloads the podcasts.
[01:35:12.380 --> 01:35:14.500]   I have the audience, the twig audience actually,
[01:35:14.500 --> 01:35:16.180]   there is a twig demographic.
[01:35:16.180 --> 01:35:19.740]   They are probably a chunk of my podcasts
[01:35:19.740 --> 01:35:22.980]   and newsletter subscribers, but then they also are here
[01:35:22.980 --> 01:35:27.980]   on Discord, and then I've got Twitter, where I have the most.
[01:35:27.980 --> 01:35:30.380]   I have 40 something thousand before Twitter,
[01:35:30.380 --> 01:35:33.540]   like did whatever it did, and I don't know
[01:35:33.540 --> 01:35:35.940]   how many of them were real, but I think a lot of them were
[01:35:35.940 --> 01:35:38.340]   'cause I grew it over such a long period of time.
[01:35:38.340 --> 01:35:40.740]   And those people, like my newsletter,
[01:35:40.740 --> 01:35:43.700]   I've got 17 something thousand newsletter subscribers.
[01:35:43.700 --> 01:35:46.740]   So my audiences are very different,
[01:35:46.740 --> 01:35:50.380]   and I want to be able to pull, like I pull in on my website,
[01:35:50.380 --> 01:35:51.980]   my tweets.
[01:35:51.980 --> 01:35:53.500]   I don't want them to be like,
[01:35:53.500 --> 01:35:57.260]   a lot of them probably don't care about my puppy pictures
[01:35:57.260 --> 01:35:59.580]   if they're only subscribing for my insightful commentary.
[01:35:59.580 --> 01:36:01.300]   - Oh, screw 'em.
[01:36:01.300 --> 01:36:03.420]   - Well, the point is, like,
[01:36:03.420 --> 01:36:05.580]   I don't wanna give those people, like,
[01:36:05.580 --> 01:36:07.700]   I don't wanna spend time going into a platform
[01:36:07.700 --> 01:36:10.380]   to give them puppy pictures, but, like,
[01:36:10.380 --> 01:36:12.060]   they can see it from my,
[01:36:12.060 --> 01:36:13.860]   they can see the Twitter stuff if they want, right?
[01:36:13.860 --> 01:36:16.500]   Like, I like the decentralization and the ability
[01:36:16.500 --> 01:36:20.380]   to be like, my audience is to pick a place to find me,
[01:36:20.380 --> 01:36:23.660]   and I create certain content for each of those areas,
[01:36:23.660 --> 01:36:28.420]   and then they get to choose what they wanna consume.
[01:36:28.420 --> 01:36:30.180]   Did that make sense?
[01:36:30.180 --> 01:36:31.020]   That was so long.
[01:36:31.020 --> 01:36:31.860]   - Yeah.
[01:36:31.860 --> 01:36:38.980]   - I wish some snack had federated with ActivityPump.
[01:36:38.980 --> 01:36:41.300]   Medium has done it, flip board has done it,
[01:36:41.300 --> 01:36:44.020]   at various levels, a WordPress has done it,
[01:36:44.020 --> 01:36:47.620]   at least try, is there something to acknowledge?
[01:36:47.620 --> 01:36:48.460]   - Why?
[01:36:48.460 --> 01:36:53.460]   - So to say that we're safe in case some stack
[01:36:53.460 --> 01:36:55.860]   gets bought by Elon Musk, that we can,
[01:36:55.860 --> 01:36:59.900]   our identities and our social graphs are portable.
[01:36:59.900 --> 01:37:04.460]   - Yeah, I think everybody has a price, so,
[01:37:04.460 --> 01:37:07.700]   putting it out there in Fediverse liberates it, if you will.
[01:37:07.700 --> 01:37:09.880]   That's what you're saying?
[01:37:09.880 --> 01:37:10.720]   - Yeah.
[01:37:10.720 --> 01:37:12.660]   I don't think we've learned our lesson.
[01:37:12.660 --> 01:37:14.140]   It's a two-way street, right?
[01:37:14.140 --> 01:37:17.460]   Like, my content is only valuable
[01:37:17.460 --> 01:37:19.060]   if someone wants to consume it, right?
[01:37:19.060 --> 01:37:21.140]   Like, it's only valuable to people who wanna consume it,
[01:37:21.140 --> 01:37:26.140]   so having, making that easy for them, like on Twitter,
[01:37:26.140 --> 01:37:29.220]   is great, 'cause like enough people,
[01:37:29.220 --> 01:37:31.820]   it's very frictionless for people to consume that
[01:37:31.820 --> 01:37:32.820]   if they want.
[01:37:32.820 --> 01:37:36.340]   If I go to some weird platform and do things there,
[01:37:36.340 --> 01:37:38.540]   it's a little harder for them to consume it.
[01:37:38.540 --> 01:37:42.620]   So, I'm not gonna reach those same people.
[01:37:42.620 --> 01:37:45.140]   - Fediverse is not a weird platform.
[01:37:45.140 --> 01:37:46.380]   We gotta stop saying that.
[01:37:46.380 --> 01:37:47.260]   - Yes, thank you, Aunt.
[01:37:47.260 --> 01:37:48.100]   Thank you.
[01:37:48.100 --> 01:37:49.140]   (laughing)
[01:37:49.140 --> 01:37:50.180]   - It is complicated.
[01:37:50.180 --> 01:37:52.340]   Do you know why I'm not a Mastodon yet, y'all?
[01:37:52.340 --> 01:37:54.300]   This is embarrassing and true.
[01:37:54.300 --> 01:37:55.820]   I can't frickin' decide a server.
[01:37:55.820 --> 01:37:56.660]   I don't even know.
[01:37:56.660 --> 01:37:57.500]   I don't even know.
[01:37:57.500 --> 01:37:58.340]   - Oh, that's tough.
[01:37:58.340 --> 01:37:59.180]   Do you pick anyone?
[01:37:59.180 --> 01:38:00.060]   - I'm just like, do I go with the journalists?
[01:38:00.060 --> 01:38:00.900]   I don't know.
[01:38:00.900 --> 01:38:02.540]   Do I wanna be, what if this is, what if this is?
[01:38:02.540 --> 01:38:03.580]   - It doesn't matter.
[01:38:03.580 --> 01:38:04.420]   It doesn't matter.
[01:38:04.420 --> 01:38:05.420]   - Unless you look at him.
[01:38:05.420 --> 01:38:06.420]   - Yeah, okay, Aunt, you go ahead.
[01:38:06.420 --> 01:38:07.260]   Explain it to her.
[01:38:07.260 --> 01:38:10.620]   - That's the thing, if you just go to the general Mastodon
[01:38:10.620 --> 01:38:13.660]   you're totally fine because you can still federate
[01:38:13.660 --> 01:38:17.060]   with other servers and follow people on other servers
[01:38:17.060 --> 01:38:19.340]   without any issue.
[01:38:19.340 --> 01:38:20.180]   - Well, I don't know.
[01:38:20.180 --> 01:38:22.300]   But if people judge me for being on the normal one,
[01:38:22.300 --> 01:38:23.140]   I don't know.
[01:38:23.140 --> 01:38:23.980]   - I don't know.
[01:38:23.980 --> 01:38:24.820]   - I don't know.
[01:38:24.820 --> 01:38:25.660]   - I don't know.
[01:38:25.660 --> 01:38:26.500]   - I know.
[01:38:26.500 --> 01:38:27.340]   - Yes, right.
[01:38:27.340 --> 01:38:28.180]   Nor your public pictures.
[01:38:28.180 --> 01:38:29.020]   Screw 'em.
[01:38:29.020 --> 01:38:30.380]   - It's so stressful to be.
[01:38:30.380 --> 01:38:32.300]   I just, I'm just like, ah.
[01:38:32.300 --> 01:38:37.300]   - They see, unless you use the local feed from that server,
[01:38:37.300 --> 01:38:40.140]   you have no, there's no other reason to care about that.
[01:38:40.140 --> 01:38:40.980]   - Right.
[01:38:40.980 --> 01:38:41.820]   - Done.
[01:38:41.820 --> 01:38:42.660]   - Why would I want the local feed?
[01:38:42.660 --> 01:38:43.900]   - I never do cause on Mastodon.social.
[01:38:43.900 --> 01:38:47.740]   You know, I, Twitter be nice,
[01:38:47.740 --> 01:38:49.180]   journey to host be nice,
[01:38:49.180 --> 01:38:51.020]   but I can follow anybody from any of those servers.
[01:38:51.020 --> 01:38:51.860]   It's fine.
[01:38:51.860 --> 01:38:52.700]   - Okay.
[01:38:52.700 --> 01:38:53.540]   - Easy.
[01:38:53.540 --> 01:38:56.460]   - And I pull it into my website.
[01:38:56.460 --> 01:38:57.380]   Does it matter?
[01:38:57.380 --> 01:38:59.140]   - Pull one.
[01:38:59.140 --> 01:38:59.980]   - Pull one to your website.
[01:38:59.980 --> 01:39:03.940]   - Well, like I pull in my tweets via, you know, an API call.
[01:39:03.940 --> 01:39:05.460]   - I think, oh, the show on the page.
[01:39:05.460 --> 01:39:07.580]   I'm not saying yet it's probably a call for it.
[01:39:07.580 --> 01:39:08.420]   - Yeah.
[01:39:08.420 --> 01:39:12.180]   - It's an open source, um, federated for it.
[01:39:12.180 --> 01:39:13.180]   - Yeah, but, okay.
[01:39:13.180 --> 01:39:14.020]   - Right.
[01:39:14.020 --> 01:39:16.140]   - Sometimes open source.
[01:39:16.140 --> 01:39:16.980]   - Okay.
[01:39:16.980 --> 01:39:19.620]   I was like, sometimes open source means you can build it yourself
[01:39:19.620 --> 01:39:21.220]   if you want to spend all of your days.
[01:39:21.220 --> 01:39:23.860]   And sometimes it means that people have built tools
[01:39:23.860 --> 01:39:25.500]   that will make it easy for you to do this.
[01:39:25.500 --> 01:39:27.540]   - Yeah, it's probably just an API.
[01:39:27.540 --> 01:39:30.060]   Just like Twitter is.
[01:39:30.060 --> 01:39:34.020]   I don't know, but I'm pretty sure it is.
[01:39:34.020 --> 01:39:37.140]   - How do a bad Mastodon posts on a website?
[01:39:37.140 --> 01:39:39.060]   - On a WordPress site.
[01:39:39.060 --> 01:39:40.420]   - Hada.
[01:39:40.420 --> 01:39:41.500]   - Plugins.
[01:39:41.500 --> 01:39:42.340]   - Yeah.
[01:39:42.340 --> 01:39:43.340]   - Plugins, baby.
[01:39:43.340 --> 01:39:44.940]   - Absolutely.
[01:39:44.940 --> 01:39:45.940]   - Yeah.
[01:39:45.940 --> 01:39:46.780]   - Okay.
[01:39:46.780 --> 01:39:49.540]   - I don't have a bad a best on RSS feed on any website.
[01:39:49.540 --> 01:39:50.380]   There you go.
[01:39:50.380 --> 01:39:51.900]   That's the one you need.
[01:39:51.900 --> 01:39:54.140]   No restrictions.
[01:39:54.140 --> 01:39:55.140]   You're good to go.
[01:39:55.140 --> 01:39:59.940]   Have you all been using, uh, artifact?
[01:39:59.940 --> 01:40:02.900]   Like, since it was released,
[01:40:02.900 --> 01:40:05.660]   I don't know, a couple of months ago, I guess?
[01:40:05.660 --> 01:40:06.780]   - It's the, uh.
[01:40:06.780 --> 01:40:09.220]   - The social news app by the founders of--
[01:40:09.220 --> 01:40:11.260]   - Oh, it's like, what is artifact?
[01:40:11.260 --> 01:40:12.100]   - No, no.
[01:40:12.100 --> 01:40:13.420]   - Same word.
[01:40:13.420 --> 01:40:14.700]   No, I am not using it.
[01:40:14.700 --> 01:40:16.060]   - No, apparently not.
[01:40:16.060 --> 01:40:21.060]   This was the, uh, the social news aggregation app.
[01:40:21.060 --> 01:40:23.620]   It hasn't been really social yet,
[01:40:23.620 --> 01:40:25.380]   but it released a couple of months ago.
[01:40:25.380 --> 01:40:30.300]   Kevin Systrom, my creaker of Instagram founders,
[01:40:30.300 --> 01:40:33.700]   created this as, I guess, their next thing.
[01:40:33.700 --> 01:40:35.620]   I've been using it for the last couple of months
[01:40:35.620 --> 01:40:38.940]   to kind of, you know, get in the habit of using it
[01:40:38.940 --> 01:40:40.820]   to scan news stories.
[01:40:40.820 --> 01:40:42.700]   You know, it's kind of like my passive, like,
[01:40:42.700 --> 01:40:43.540]   "Oh, I'm bored.
[01:40:43.540 --> 01:40:44.380]   "Pick up my phone.
[01:40:44.380 --> 01:40:45.500]   "What's in the news?"
[01:40:45.500 --> 01:40:47.940]   I open up artifact and kind of scroll through it
[01:40:47.940 --> 01:40:48.780]   and everything.
[01:40:48.780 --> 01:40:51.580]   I think the overall intention is that it'll be more
[01:40:51.580 --> 01:40:55.860]   of a, like, a social, kind of like, interaction,
[01:40:55.860 --> 01:40:57.980]   you know, as far as like, the people that I follow,
[01:40:57.980 --> 01:40:58.820]   what are the news stories?
[01:40:58.820 --> 01:41:00.940]   You know, kind of similar to "Nuzzle", I suppose,
[01:41:00.940 --> 01:41:02.140]   would be the closest to Harrison.
[01:41:02.140 --> 01:41:02.980]   - Oh, okay.
[01:41:02.980 --> 01:41:03.820]   - Is that--
[01:41:03.820 --> 01:41:04.660]   - Sure enough.
[01:41:04.660 --> 01:41:07.060]   - But I follow on artifact.
[01:41:07.060 --> 01:41:08.180]   What are they reading?
[01:41:08.180 --> 01:41:09.580]   Here's what I'm reading, you know,
[01:41:09.580 --> 01:41:11.980]   some, that sort of thing.
[01:41:11.980 --> 01:41:13.940]   But, and it's fine.
[01:41:13.940 --> 01:41:15.100]   - But it's tied to Instagram?
[01:41:15.100 --> 01:41:16.180]   - This is the one. - This is the one.
[01:41:16.180 --> 01:41:17.100]   - Yeah?
[01:41:17.100 --> 01:41:18.380]   - No, it's not tied to Instagram.
[01:41:18.380 --> 01:41:21.140]   I mean, it's the Instagram founders created it.
[01:41:21.140 --> 01:41:21.980]   - Outers did.
[01:41:21.980 --> 01:41:22.820]   - But they left.
[01:41:22.820 --> 01:41:24.340]   - It's not, it's not like linked
[01:41:24.340 --> 01:41:25.940]   to an Instagram account or anything.
[01:41:25.940 --> 01:41:26.900]   It's totally synced.
[01:41:26.900 --> 01:41:28.860]   - And this is invite only still?
[01:41:28.860 --> 01:41:29.700]   - No, no, no, no.
[01:41:29.700 --> 01:41:30.540]   - No, no, no.
[01:41:30.540 --> 01:41:31.380]   - I'm not looking at public now.
[01:41:31.380 --> 01:41:32.220]   - No, definitely.
[01:41:32.220 --> 01:41:34.260]   - It just added some social features.
[01:41:34.260 --> 01:41:35.100]   - Yeah, yeah, yeah.
[01:41:35.100 --> 01:41:39.700]   They've added a reputation scores and comments.
[01:41:39.700 --> 01:41:40.540]   Like, I don't even know
[01:41:40.540 --> 01:41:42.700]   that I'm following anyone on artifact right now.
[01:41:42.700 --> 01:41:45.660]   I've just been using it as a pure like news reader
[01:41:45.660 --> 01:41:46.700]   for the most part.
[01:41:46.700 --> 01:41:47.820]   Basically what it's done,
[01:41:47.820 --> 01:41:50.060]   what it's been for me is,
[01:41:50.060 --> 01:41:51.620]   when it first came out, I was like,
[01:41:51.620 --> 01:41:53.580]   what's the difference between this and Google news?
[01:41:53.580 --> 01:41:55.660]   I mean, Google news really uses kind of like, you know,
[01:41:55.660 --> 01:41:57.820]   Google's, you know,
[01:41:57.820 --> 01:42:00.100]   know how on the back end to understand
[01:42:00.100 --> 01:42:01.460]   what news stories I wanna read.
[01:42:01.460 --> 01:42:04.100]   And that's what the folks in artifact are,
[01:42:04.100 --> 01:42:05.900]   you know, are really going to town on.
[01:42:05.900 --> 01:42:08.140]   Like, we know what you wanna read.
[01:42:08.140 --> 01:42:10.180]   After you read a certain number of articles,
[01:42:10.180 --> 01:42:11.820]   we'll have a better sense of the articles
[01:42:11.820 --> 01:42:12.700]   that you actually wanna read,
[01:42:12.700 --> 01:42:14.780]   we'll present them to you and everything.
[01:42:14.780 --> 01:42:15.620]   So--
[01:42:15.620 --> 01:42:17.460]   - That's what I would hope, right?
[01:42:17.460 --> 01:42:18.980]   - I mean, between the two apps,
[01:42:18.980 --> 01:42:21.940]   I honestly don't know that I see a whole lot of difference,
[01:42:21.940 --> 01:42:22.780]   to be honest.
[01:42:22.780 --> 01:42:24.660]   I guess the social features would be the difference.
[01:42:24.660 --> 01:42:26.180]   I just haven't really been using them.
[01:42:26.180 --> 01:42:28.180]   And now there's reputation scores
[01:42:28.180 --> 01:42:29.980]   that would be attached to users.
[01:42:30.900 --> 01:42:34.980]   You can add comments, I guess, to articles
[01:42:34.980 --> 01:42:38.500]   that are shared inside of the app.
[01:42:38.500 --> 01:42:39.500]   So--
[01:42:39.500 --> 01:42:42.380]   - Well, bring it into Google angle here.
[01:42:42.380 --> 01:42:45.420]   I'm interested in this because my experience with Google news
[01:42:45.420 --> 01:42:47.780]   has started to become creptastic again.
[01:42:47.780 --> 01:42:48.620]   - Oh, okay.
[01:42:48.620 --> 01:42:51.260]   - And I'm really tired of hitting,
[01:42:51.260 --> 01:42:53.180]   not interested, not interested.
[01:42:53.180 --> 01:42:54.020]   - Oh, yeah.
[01:42:54.020 --> 01:42:55.820]   - Not interested to try to retrain it.
[01:42:55.820 --> 01:42:58.380]   And I'm like, I've been using this service for years.
[01:42:58.380 --> 01:43:01.180]   What makes you think I'm interested in this thing?
[01:43:01.180 --> 01:43:03.340]   And I haven't ever been interested in this thing.
[01:43:03.340 --> 01:43:05.100]   So stop showing it to me.
[01:43:05.100 --> 01:43:09.100]   So if this is gonna clean that experience up for me,
[01:43:09.100 --> 01:43:10.900]   I'm hitting the star right now.
[01:43:10.900 --> 01:43:12.860]   I'm like, pixel.
[01:43:12.860 --> 01:43:15.380]   - Yeah, it takes a little while to kind of train it.
[01:43:15.380 --> 01:43:19.300]   And they actually have kind of part of the experience
[01:43:19.300 --> 01:43:21.460]   that you can go and you can see your account.
[01:43:21.460 --> 01:43:23.020]   It's like, at 50--
[01:43:23.020 --> 01:43:26.020]   You're working towards that first 50 articles red.
[01:43:26.020 --> 01:43:27.380]   - They gave a flat a little bit.
[01:43:27.380 --> 01:43:28.220]   - Yes.
[01:43:28.220 --> 01:43:29.060]   - It's not just, but yes.
[01:43:29.060 --> 01:43:30.060]   - Yeah, they do kind of game of fun.
[01:43:30.060 --> 01:43:31.020]   - I don't want your badges.
[01:43:31.020 --> 01:43:32.740]   I don't want your stinking badges.
[01:43:32.740 --> 01:43:34.900]   - They don't come out and like hit you over the face
[01:43:34.900 --> 01:43:36.460]   with the gamification aspect.
[01:43:36.460 --> 01:43:37.300]   - Well, they did today.
[01:43:37.300 --> 01:43:38.140]   - Kind of go in.
[01:43:38.140 --> 01:43:38.980]   - No, who did you?
[01:43:38.980 --> 01:43:39.820]   - Oh, interesting.
[01:43:39.820 --> 01:43:41.660]   - Oh, interesting.
[01:43:41.660 --> 01:43:44.740]   - I wonder if I have my notifications on that app disabled
[01:43:44.740 --> 01:43:46.940]   'cause I don't get anything from that app.
[01:43:46.940 --> 01:43:49.700]   - I never allow notifications on anything anymore.
[01:43:49.700 --> 01:43:50.620]   I'm just like, oh.
[01:43:50.620 --> 01:43:51.460]   - Yeah, why?
[01:43:51.460 --> 01:43:52.460]   - No, you got nothing for me.
[01:43:52.460 --> 01:43:53.300]   - You got nothing for me.
[01:43:53.300 --> 01:43:56.820]   - Like, I totally am the same.
[01:43:57.540 --> 01:44:01.100]   I deny as most of the time I deny.
[01:44:01.100 --> 01:44:02.340]   And it's wonderful.
[01:44:02.340 --> 01:44:04.180]   I have to say.
[01:44:04.180 --> 01:44:05.580]   - Yeah, I feel like if you're gonna ask me
[01:44:05.580 --> 01:44:06.980]   for notifications, you need to be like,
[01:44:06.980 --> 01:44:08.700]   well, what are you gonna notify me about?
[01:44:08.700 --> 01:44:09.540]   - Right.
[01:44:09.540 --> 01:44:12.700]   - What do I actually need to know right now from you?
[01:44:12.700 --> 01:44:14.940]   - Like, my doorbell, okay, sure.
[01:44:14.940 --> 01:44:17.020]   Notifications make sense.
[01:44:17.020 --> 01:44:19.180]   But, you know, my sonos was like,
[01:44:19.180 --> 01:44:20.380]   can we send you notifications?
[01:44:20.380 --> 01:44:23.500]   I'm like, I can't think of a reason why I don't.
[01:44:23.500 --> 01:44:26.140]   - Not many things in my life need like immediate
[01:44:26.140 --> 01:44:27.660]   attention when it comes to sonos.
[01:44:27.660 --> 01:44:28.660]   (laughs)
[01:44:28.660 --> 01:44:29.500]   So.
[01:44:29.500 --> 01:44:32.700]   - Yeah, I'm like, is there like a caution?
[01:44:32.700 --> 01:44:33.940]   You're about to listen to something
[01:44:33.940 --> 01:44:35.340]   that will poison your mind for it.
[01:44:35.340 --> 01:44:36.180]   - Right.
[01:44:36.180 --> 01:44:37.020]   - Oh, I don't want that.
[01:44:37.020 --> 01:44:38.940]   - You aren't gonna like this song, okay.
[01:44:38.940 --> 01:44:41.900]   - This song will never leave your head again.
[01:44:41.900 --> 01:44:44.180]   No, not Hotel California, go.
[01:44:44.180 --> 01:44:46.580]   (laughs)
[01:44:46.580 --> 01:44:50.500]   - Oh, so this is the Democracy episode.
[01:44:50.500 --> 01:44:52.460]   I mean, there's plenty of more stories in here.
[01:44:52.460 --> 01:44:54.380]   What do you all wanna talk about?
[01:44:55.540 --> 01:44:58.100]   You know, as far as the remaining stories that we have.
[01:44:58.100 --> 01:45:00.660]   - We have too many stories to get through all of them.
[01:45:00.660 --> 01:45:02.460]   - So I never intent them to be all of them.
[01:45:02.460 --> 01:45:05.260]   It's just, I know that's why today's awesome.
[01:45:05.260 --> 01:45:09.780]   - Jeff, we talked about TikTok over the last couple of weeks,
[01:45:09.780 --> 01:45:11.820]   but I like what Mr. Jarvis put in here
[01:45:11.820 --> 01:45:13.220]   with the progressive lawmakers
[01:45:13.220 --> 01:45:15.340]   of fighting against a TikTok ban.
[01:45:15.340 --> 01:45:16.980]   - Finally, somebody's speaking against it.
[01:45:16.980 --> 01:45:21.460]   - I saw a couple comments at the bottom
[01:45:21.460 --> 01:45:23.740]   and it all just made perfect sense.
[01:45:23.740 --> 01:45:26.820]   And yeah, a bit of it is a little echo
[01:45:26.820 --> 01:45:29.380]   of what I've been saying on this show.
[01:45:29.380 --> 01:45:30.220]   - The cause of it.
[01:45:30.220 --> 01:45:33.900]   - As far as, yeah, we can be concerned about TikTok
[01:45:33.900 --> 01:45:38.900]   and bike dance and all of the security concerns
[01:45:38.900 --> 01:45:44.340]   or what have you, but shouldn't we be concerned
[01:45:44.340 --> 01:45:46.540]   about Facebook, Twitter, Instagram,
[01:45:46.540 --> 01:45:48.900]   and all of those other things that are homegrown right here
[01:45:48.900 --> 01:45:50.900]   just doing the exact same thing.
[01:45:50.900 --> 01:45:54.980]   And a lot of the representatives were saying just that,
[01:45:54.980 --> 01:45:57.060]   you know, word for word.
[01:45:57.060 --> 01:46:00.900]   So I want to applaud those representatives for getting it.
[01:46:00.900 --> 01:46:01.740]   - Yeah.
[01:46:01.740 --> 01:46:06.700]   Jamal Bowman was the first one to come out and say,
[01:46:06.700 --> 01:46:07.980]   I'm not so sure about this ban,
[01:46:07.980 --> 01:46:08.820]   there's problems with that,
[01:46:08.820 --> 01:46:10.860]   there's free speech, there's other issues.
[01:46:10.860 --> 01:46:14.260]   And now he's led the way and others are coming along too,
[01:46:14.260 --> 01:46:15.660]   which I think is good.
[01:46:15.660 --> 01:46:19.060]   So as he would say, probably a fine discussion to have
[01:46:19.060 --> 01:46:24.060]   politically, but I fear that a lot of it is xenophobia.
[01:46:24.060 --> 01:46:29.380]   - Oh no, I actually, I don't think this is a nuanced issue
[01:46:29.380 --> 01:46:30.940]   at all. - Good, good.
[01:46:30.940 --> 01:46:35.300]   - Like I've never been like keen on the TikTok,
[01:46:35.300 --> 01:46:37.260]   like straight up TikTok ban.
[01:46:37.260 --> 01:46:42.180]   Now, I do think there's like what information is getting
[01:46:42.180 --> 01:46:45.260]   tracked is interesting and I don't know if they belong
[01:46:45.260 --> 01:46:48.940]   in government loans, but I think we have any.
[01:46:48.940 --> 01:46:50.820]   - The government can't stand it out.
[01:46:50.820 --> 01:46:54.940]   - Bowman says, let's have a comprehensive conversation
[01:46:54.940 --> 01:46:59.060]   about legislation that we need federal legislation
[01:46:59.060 --> 01:47:02.620]   to make sure people who use social media platforms are safe
[01:47:02.620 --> 01:47:04.740]   and their information is secure
[01:47:04.740 --> 01:47:06.500]   and their information is not being shared
[01:47:06.500 --> 01:47:08.020]   or sold to third parties.
[01:47:08.020 --> 01:47:09.740]   That's one point.
[01:47:09.740 --> 01:47:10.860]   Then he goes on and says,
[01:47:10.860 --> 01:47:13.540]   they collect a lot of information about consumers
[01:47:13.540 --> 01:47:15.180]   talking about these platforms.
[01:47:17.300 --> 01:47:19.660]   Facebook, Instagram, YouTube and Twitter.
[01:47:19.660 --> 01:47:24.140]   But of course, the concern about TikTok is it is,
[01:47:24.140 --> 01:47:27.820]   it's owned by a Chinese company and so therefore,
[01:47:27.820 --> 01:47:30.820]   is there a risk of the information being shared
[01:47:30.820 --> 01:47:32.500]   with Chinese authorities.
[01:47:32.500 --> 01:47:34.380]   Facebook does not operate in China,
[01:47:34.380 --> 01:47:37.940]   so there's a little risk of that, he added.
[01:47:37.940 --> 01:47:42.940]   It's, again, we should look at our own homegrown apps
[01:47:44.460 --> 01:47:47.980]   and challenge them for the stuff that they're collecting
[01:47:47.980 --> 01:47:52.980]   and go back to those times when you were so against Facebook
[01:47:52.980 --> 01:47:56.380]   and Instagram for what they were doing to teams.
[01:47:56.380 --> 01:48:01.220]   You know, digging to some of that energy you had, you know.
[01:48:01.220 --> 01:48:05.580]   - But Ant, how do you feel about Lemon 8,
[01:48:05.580 --> 01:48:10.220]   the new app by TikTok parent company, ByteDance,
[01:48:10.220 --> 01:48:14.420]   surging in the US, raging popularity
[01:48:14.460 --> 01:48:16.620]   - The business started a few minutes ago.
[01:48:16.620 --> 01:48:17.460]   - No.
[01:48:17.460 --> 01:48:19.340]   (laughing)
[01:48:19.340 --> 01:48:20.500]   - I haven't installed it yet.
[01:48:20.500 --> 01:48:21.740]   - No, neither have I.
[01:48:21.740 --> 01:48:22.580]   - Apparently the--
[01:48:22.580 --> 01:48:23.700]   - What type of app is it?
[01:48:23.700 --> 01:48:24.820]   - So it's social-- - I love the little--
[01:48:24.820 --> 01:48:27.340]   - Social video. - Grammy, one.
[01:48:27.340 --> 01:48:30.020]   - Video. - And photos.
[01:48:30.020 --> 01:48:32.620]   Whoa, yes, it also has photos.
[01:48:32.620 --> 01:48:36.140]   So I mean, I haven't used it, but the write up, you know,
[01:48:36.140 --> 01:48:38.340]   of the article that was included here just basically said,
[01:48:38.340 --> 01:48:40.580]   it's kind of like TikTok with short form videos
[01:48:40.580 --> 01:48:44.500]   and then also Instagram with photo sharing.
[01:48:44.500 --> 01:48:45.820]   So it's a little of this, a little of that.
[01:48:45.820 --> 01:48:47.900]   - Social media influencers describe the app
[01:48:47.900 --> 01:48:50.860]   as if, quote, Instagram and Pinterest had a baby.
[01:48:50.860 --> 01:48:51.700]   - Oh.
[01:48:51.700 --> 01:48:54.940]   - First thing is, what was my birthday?
[01:48:54.940 --> 01:49:00.100]   - I personally, like, if it's not words,
[01:49:00.100 --> 01:49:01.060]   I have no interests.
[01:49:01.060 --> 01:49:02.820]   Like, it's hard enough for me to put makeup on
[01:49:02.820 --> 01:49:04.740]   for like three hours to do this show.
[01:49:04.740 --> 01:49:07.500]   I cannot imagine living like-- (laughing)
[01:49:07.500 --> 01:49:08.660]   - Living in La Vida--
[01:49:08.660 --> 01:49:09.740]   - Another Stacy rule.
[01:49:09.740 --> 01:49:11.820]   - We need the Stacy rule book.
[01:49:11.820 --> 01:49:14.900]   No documentaries, no images.
[01:49:14.900 --> 01:49:17.100]   (laughing)
[01:49:17.100 --> 01:49:20.340]   - Maybe it's 'cause Leo isn't here being curmudgeonly.
[01:49:20.340 --> 01:49:22.660]   I just feel like someone needs to take that.
[01:49:22.660 --> 01:49:26.060]   - Interruption is okay, stealing credit is not.
[01:49:26.060 --> 01:49:28.020]   - Yeah, I think we--
[01:49:28.020 --> 01:49:29.580]   - I think these are all fair rules.
[01:49:29.580 --> 01:49:30.940]   - That is gonna be honest.
[01:49:30.940 --> 01:49:33.860]   (laughing)
[01:49:33.860 --> 01:49:37.180]   - No more than 18 minutes about whiskey.
[01:49:37.180 --> 01:49:38.500]   - Yeah. - Yeah.
[01:49:38.500 --> 01:49:41.100]   - 70 minutes, okay, 18-not.
[01:49:41.100 --> 01:49:45.860]   - I mean, even Jin, no, even Jin.
[01:49:45.860 --> 01:49:47.020]   - 15 minutes.
[01:49:47.020 --> 01:49:49.900]   Jin, 15 minutes, no more.
[01:49:49.900 --> 01:49:53.100]   For one of those of you who are on Windows Weekly,
[01:49:53.100 --> 01:49:55.700]   there was a long slow-liquid about making--
[01:49:55.700 --> 01:49:56.820]   - 33 minutes.
[01:49:56.820 --> 01:49:58.500]   - I thought it looked like security now
[01:49:58.500 --> 01:50:01.220]   and Steve was going on about something that Leo had left.
[01:50:01.220 --> 01:50:03.480]   (laughing)
[01:50:03.480 --> 01:50:04.300]   - I hear that.
[01:50:04.300 --> 01:50:06.420]   - Mr. Regent.
[01:50:06.420 --> 01:50:07.260]   - I have people love it.
[01:50:07.260 --> 01:50:08.780]   - I'm gonna do Ted talks about this.
[01:50:08.780 --> 01:50:09.860]   God bless them.
[01:50:09.860 --> 01:50:11.900]   I thought, oh, Jesus, what's our show gonna start?
[01:50:11.900 --> 01:50:15.700]   - So that's what my buddy was.
[01:50:15.700 --> 01:50:17.420]   - I like their whiskey on Windows Weekly.
[01:50:17.420 --> 01:50:19.060]   - I have a story I wanna hear Stacey on.
[01:50:19.060 --> 01:50:19.900]   - Yeah.
[01:50:19.900 --> 01:50:20.740]   - What is it?
[01:50:20.740 --> 01:50:22.100]   - I'm sure this is just a case where I just haven't been
[01:50:22.100 --> 01:50:23.180]   on the news and I don't know,
[01:50:23.180 --> 01:50:27.780]   but how could Sam Sutton's profits plunge by 96%
[01:50:27.780 --> 01:50:30.020]   and ship productions and all kinds of crises?
[01:50:30.020 --> 01:50:35.300]   - I mean, chips are a commodity, man.
[01:50:35.300 --> 01:50:38.100]   And like, we went through this whole period of like,
[01:50:38.100 --> 01:50:39.140]   holy cow, we're gonna do it.
[01:50:39.140 --> 01:50:40.340]   This is memory chips, right?
[01:50:40.340 --> 01:50:42.300]   Is this Sam Sutton's memory division?
[01:50:42.300 --> 01:50:44.300]   - That's commodity.
[01:50:44.300 --> 01:50:46.460]   - Yeah, memory is pure commodity, man.
[01:50:46.460 --> 01:50:49.740]   Like, that is so hard to be a memory chip maker.
[01:50:49.740 --> 01:50:52.060]   Those poor guys, I was gonna say those poor,
[01:50:52.060 --> 01:50:54.060]   I can't see it, but you know what I mean.
[01:50:54.060 --> 01:50:56.380]   (banging)
[01:50:56.380 --> 01:50:57.820]   - Individuals, groups.
[01:50:57.820 --> 01:51:00.580]   - Yeah, people with people of dubious parentage
[01:51:00.580 --> 01:51:01.700]   as my mom would say.
[01:51:03.180 --> 01:51:06.060]   But yeah, it's tough out there for the memory guys.
[01:51:06.060 --> 01:51:08.300]   So I don't know what else to tell you.
[01:51:08.300 --> 01:51:10.260]   - Okay, all right, I just let it.
[01:51:10.260 --> 01:51:13.540]   'Cause that and the PC market going south
[01:51:13.540 --> 01:51:14.380]   and the chip market.
[01:51:14.380 --> 01:51:17.140]   - Well, yeah, and there could be lower demand.
[01:51:17.140 --> 01:51:20.580]   I feel like during the pandemic,
[01:51:20.580 --> 01:51:23.660]   memory wasn't actually a shortage issue.
[01:51:23.660 --> 01:51:24.780]   Memory chips weren't.
[01:51:24.780 --> 01:51:28.420]   - Process was working, right?
[01:51:28.420 --> 01:51:30.660]   - Different, well, Wi-Fi chips were various different
[01:51:30.660 --> 01:51:31.740]   components that are needed.
[01:51:31.740 --> 01:51:34.380]   Like P-Mix were a big deal for some companies.
[01:51:34.380 --> 01:51:39.620]   Like chips can cover a lot.
[01:51:39.620 --> 01:51:40.980]   (laughing)
[01:51:40.980 --> 01:51:42.700]   Yeah, sorry, this isn't that sexy.
[01:51:42.700 --> 01:51:44.100]   I'm sorry. - Okay, all right, thank you.
[01:51:44.100 --> 01:51:46.260]   (laughing)
[01:51:46.260 --> 01:51:47.100]   - Jeff, ask me.
[01:51:47.100 --> 01:51:49.500]   - No, I'm not, I'm like, I really,
[01:51:49.500 --> 01:51:51.380]   I wanna give your story all it's to do.
[01:51:51.380 --> 01:51:52.540]   And I'm like, I don't know what else to say.
[01:51:52.540 --> 01:51:55.420]   - Oh, next, this is rule books, Daisy, one sexy story.
[01:51:55.420 --> 01:51:59.620]   - Come on, I'll tell you what's not sexy right now.
[01:51:59.620 --> 01:52:02.180]   That is meta, meta apparently.
[01:52:02.180 --> 01:52:03.020]   - Oh boy.
[01:52:03.020 --> 01:52:04.260]   - Not very sexy.
[01:52:04.260 --> 01:52:08.180]   Once very sexy, I don't know, I'm gonna stop saying sexy
[01:52:08.180 --> 01:52:12.020]   'cause it's weird when the preferred technology is sexy.
[01:52:12.020 --> 01:52:17.020]   Meta had its day and apparently there's this article
[01:52:17.020 --> 01:52:20.380]   that's kind of an interesting read in New York Times article
[01:52:20.380 --> 01:52:24.380]   about just kind of like employee morale at meta right now
[01:52:24.380 --> 01:52:27.580]   based on the people that the authors of the article
[01:52:27.580 --> 01:52:31.260]   spoke to anyways, saying that morale is very low,
[01:52:31.260 --> 01:52:34.260]   there's the layoffs, there's absentee leadership,
[01:52:34.260 --> 01:52:38.380]   there's Mark Zuckerberg, the article says
[01:52:38.380 --> 01:52:40.260]   making a bad bet on the future,
[01:52:40.260 --> 01:52:43.340]   of course talking about the bet on the metaverse
[01:52:43.340 --> 01:52:48.100]   which feels especially now, like in the last six months,
[01:52:48.100 --> 01:52:50.700]   if anyone was still clinging on to meta versus
[01:52:50.700 --> 01:52:54.620]   the next big thing, the last six months
[01:52:54.620 --> 01:52:57.940]   has really kind of changed a lot of opinions
[01:52:57.940 --> 01:52:59.060]   as far as that's concerned.
[01:52:59.060 --> 01:53:00.580]   It's feeling like the--
[01:53:00.580 --> 01:53:02.820]   - Oh better than NFTs, but yeah.
[01:53:02.820 --> 01:53:04.180]   - Yeah, yeah for sure.
[01:53:04.180 --> 01:53:07.140]   There've been a lot of things that have been
[01:53:07.140 --> 01:53:10.060]   the next big thing and I know it's been talked about
[01:53:10.060 --> 01:53:12.260]   to death on this show but it really feels like AI
[01:53:12.260 --> 01:53:14.900]   might actually be the legit next big thing.
[01:53:14.900 --> 01:53:18.620]   And so when you've got a company like,
[01:53:18.620 --> 01:53:22.820]   once Facebook now meta all in on the metaverse VR,
[01:53:22.820 --> 01:53:24.580]   blah, blah, blah, like I was reading through
[01:53:24.580 --> 01:53:27.420]   the article and part of it was talking about
[01:53:27.420 --> 01:53:32.260]   how people who have used VR,
[01:53:32.260 --> 01:53:34.380]   they're noticing there isn't a lot of return
[01:53:34.380 --> 01:53:37.460]   to the technology and I can only use myself as an example.
[01:53:37.460 --> 01:53:42.300]   I haven't turned on my quest in many months at this point.
[01:53:42.300 --> 01:53:45.020]   And like any time I think about it,
[01:53:45.020 --> 01:53:46.820]   it almost seems like so much effort
[01:53:46.820 --> 01:53:48.580]   and kind of like I get this like,
[01:53:48.580 --> 01:53:51.460]   this mirror of a little bit of queasiness and everything.
[01:53:51.460 --> 01:53:53.060]   I'm like, eh, you know what now,
[01:53:53.060 --> 01:53:55.140]   I don't have the energy for that.
[01:53:55.140 --> 01:53:57.580]   And that's a big problem. - My space, yeah.
[01:53:57.580 --> 01:53:59.220]   - Yeah, you know, I tried to do so,
[01:53:59.220 --> 01:54:00.740]   I tried to say, I tried to recognize
[01:54:00.740 --> 01:54:03.820]   what was happening to social, but just picked the wrong horse.
[01:54:03.820 --> 01:54:05.700]   - Well, yeah, the article talks about, you know,
[01:54:05.700 --> 01:54:07.460]   place Mark Zuckerberg, you know,
[01:54:07.460 --> 01:54:10.940]   made the bet on the metaverse and that's exactly what it was.
[01:54:10.940 --> 01:54:12.300]   It was a bet and you know what,
[01:54:12.300 --> 01:54:15.140]   it looks right now like you're losing that bet.
[01:54:15.140 --> 01:54:16.180]   What do you do?
[01:54:16.180 --> 01:54:20.260]   - Well, I think it is easy to see like,
[01:54:20.260 --> 01:54:23.780]   oh, this is where we're going, this could be awesome, right?
[01:54:23.780 --> 01:54:26.020]   But I also think it's a case where the physical hardware
[01:54:26.020 --> 01:54:27.820]   just could not keep up. - Yeah.
[01:54:27.820 --> 01:54:30.540]   - Like the expectation. - The expectation.
[01:54:30.540 --> 01:54:33.180]   - He just ignored the fact that like,
[01:54:33.180 --> 01:54:36.900]   we don't have the ability to deliver that much performance
[01:54:36.900 --> 01:54:39.100]   at a price that makes sense.
[01:54:39.100 --> 01:54:41.340]   And for the power consumption that you would need
[01:54:41.340 --> 01:54:43.620]   for like a wearable on your face, like,
[01:54:43.620 --> 01:54:45.180]   it literally doesn't exist yet.
[01:54:45.180 --> 01:54:49.180]   So it's kind of like, yeah.
[01:54:49.180 --> 01:54:51.700]   I just think it's a bad experience, period.
[01:54:51.700 --> 01:54:53.580]   I just, I've never really gotten it.
[01:54:53.580 --> 01:54:56.500]   I've tried, I mean, this is a guy who bought Google Glass.
[01:54:56.500 --> 01:54:58.180]   I tried these things.
[01:54:58.180 --> 01:55:02.100]   I just didn't ever get it.
[01:55:02.100 --> 01:55:07.100]   - So he's all in on the VR space or what have you.
[01:55:07.100 --> 01:55:11.580]   But that doesn't necessarily mean Facebook as it is,
[01:55:11.580 --> 01:55:14.140]   is gonna just disappear, right?
[01:55:14.140 --> 01:55:17.980]   People are still gonna be able to just open up Facebook.com
[01:55:17.980 --> 01:55:22.020]   on whatever device they're using and post and share
[01:55:22.020 --> 01:55:25.380]   and reshare and comment and all of that stuff.
[01:55:25.380 --> 01:55:30.020]   Regardless if he's wrong about the metaverse.
[01:55:30.020 --> 01:55:32.340]   I mean, he's got, or he and he,
[01:55:32.340 --> 01:55:33.180]   - Well, it is an over. - He's like,
[01:55:33.180 --> 01:55:34.420]   "I have so much money."
[01:55:34.420 --> 01:55:37.260]   It's not like they're, yes, they're laying people off
[01:55:37.260 --> 01:55:39.500]   or whatever, but it's not like they're hemorrhaging
[01:55:39.500 --> 01:55:42.100]   the way Twitter is hemorrhaging right now, right?
[01:55:42.100 --> 01:55:44.580]   Facebook's not going anywhere anytime soon.
[01:55:44.580 --> 01:55:45.860]   - No, it's over. - Why not just
[01:55:45.860 --> 01:55:48.780]   hit and keep betting? - Yeah.
[01:55:48.780 --> 01:55:50.660]   And he has enough ownership shares
[01:55:50.660 --> 01:55:53.180]   that he can keep for a while anyway,
[01:55:53.180 --> 01:55:54.740]   he could make another really terrible bet
[01:55:54.740 --> 01:55:57.300]   if he wanted to. - Right, right.
[01:55:57.300 --> 01:55:58.340]   - But yeah, I was there. - I'd imagine
[01:55:58.340 --> 01:56:00.540]   his net net net net has something to do with AI,
[01:56:00.540 --> 01:56:02.260]   just to guess, but. - Oh yeah.
[01:56:02.260 --> 01:56:06.140]   - Just throw that out there.
[01:56:06.140 --> 01:56:08.700]   - For people like Stacey who don't have enough friends
[01:56:08.700 --> 01:56:10.260]   to be in Dungeons & Dragons,
[01:56:10.260 --> 01:56:12.420]   Facebook will now make up friends for you.
[01:56:12.420 --> 01:56:13.940]   - Oh, there you go.
[01:56:13.940 --> 01:56:15.260]   That's the next.
[01:56:15.260 --> 01:56:17.460]   - Will they play Dungeons & Dragons?
[01:56:17.460 --> 01:56:19.860]   - Right. - Oh, will somebody play with me?
[01:56:19.860 --> 01:56:22.100]   (laughing)
[01:56:22.100 --> 01:56:27.700]   - I salute the guy for being, you know, just,
[01:56:27.700 --> 01:56:30.900]   hey, I believe in this and I'm gonna keep working at it.
[01:56:30.900 --> 01:56:32.740]   You know, I don't care what people are saying about it.
[01:56:32.740 --> 01:56:35.540]   He's passionate about it. - He cares not about it, man.
[01:56:35.540 --> 01:56:38.060]   Dude, what you wanna do is not hurting any of us
[01:56:38.060 --> 01:56:40.860]   at the moment that you're passionate about building
[01:56:40.860 --> 01:56:42.900]   this mythical place.
[01:56:42.900 --> 01:56:47.540]   Just keep doing it and keep providing Facebook.com
[01:56:47.540 --> 01:56:49.700]   to everybody because you know people can't live
[01:56:49.700 --> 01:56:50.540]   without that.
[01:56:50.540 --> 01:56:53.620]   That sarcasm, by the way.
[01:56:53.620 --> 01:56:56.700]   (laughing)
[01:56:56.700 --> 01:56:58.220]   - And then there's Twitter.
[01:56:58.220 --> 01:57:02.340]   Any thoughts on this whole open source
[01:57:02.340 --> 01:57:04.500]   or the source code being revealed
[01:57:04.500 --> 01:57:06.140]   for the recommendation algorithm?
[01:57:06.140 --> 01:57:09.420]   Why, why is Twitter doing that?
[01:57:09.420 --> 01:57:11.340]   - Caiman went while you were on vacation.
[01:57:11.340 --> 01:57:12.180]   - Yeah. - Yeah.
[01:57:12.180 --> 01:57:13.020]   - I don't know. - I don't know.
[01:57:13.020 --> 01:57:13.840]   - I don't know. - One thing.
[01:57:13.840 --> 01:57:15.220]   - Did you already, no, no, we didn't talk about it.
[01:57:15.220 --> 01:57:16.620]   - Okay, all right. - Okay, all right.
[01:57:16.620 --> 01:57:19.380]   - But people just, to background you,
[01:57:19.380 --> 01:57:20.940]   they found a few odd things like,
[01:57:20.940 --> 01:57:22.900]   is this a musketwee, does this other thing?
[01:57:22.900 --> 01:57:25.020]   - Yep. - But then it just kind of
[01:57:25.020 --> 01:57:27.140]   interest weaned quickly.
[01:57:27.140 --> 01:57:27.980]   - Got it.
[01:57:27.980 --> 01:57:31.060]   - A lot of redacted information too, right?
[01:57:31.060 --> 01:57:33.140]   Didn't necessarily reveal everything.
[01:57:33.140 --> 01:57:35.540]   That was my understanding. - Yeah.
[01:57:35.540 --> 01:57:37.220]   - Right. - Right.
[01:57:37.220 --> 01:57:40.700]   - Mr. Alex Stamos and his co-host on their podcast
[01:57:40.700 --> 01:57:43.540]   talked about it, it was quite entertaining.
[01:57:43.540 --> 01:57:48.460]   Just the idea that they applauded him
[01:57:48.460 --> 01:57:50.460]   because they applauded it must when I say him.
[01:57:50.460 --> 01:57:53.140]   They applauded it must because must did say,
[01:57:53.140 --> 01:57:55.580]   "We're gonna put the code out there."
[01:57:55.580 --> 01:57:57.980]   All right, you did put the code out there.
[01:57:57.980 --> 01:57:59.420]   But you didn't put everything out there.
[01:57:59.420 --> 01:58:01.780]   What is all this other crap that you're trying to hide
[01:58:01.780 --> 01:58:05.940]   and what are you expecting to gain from putting it out there?
[01:58:05.940 --> 01:58:10.500]   'Cause some people are believing that what he put out there
[01:58:10.500 --> 01:58:14.860]   is opening it up for people to game it even more,
[01:58:14.860 --> 01:58:17.660]   game the system even more when it comes to Twitter.
[01:58:17.660 --> 01:58:19.940]   I don't know if I believe that,
[01:58:19.940 --> 01:58:22.740]   but 'cause the way people talk,
[01:58:22.740 --> 01:58:24.060]   so nobody's using Twitter.
[01:58:24.060 --> 01:58:26.140]   (laughs)
[01:58:26.140 --> 01:58:29.900]   - Okay.
[01:58:29.900 --> 01:58:32.620]   - Which reminds me, Mr. Stamos would like to come back on.
[01:58:32.620 --> 01:58:34.260]   - Oh, really? - Really?
[01:58:34.260 --> 01:58:38.020]   - Oh, great. - That's good to know.
[01:58:38.020 --> 01:58:40.540]   - Is it because you call him Mr. Stamos?
[01:58:40.540 --> 01:58:41.740]   It's so respectful.
[01:58:41.740 --> 01:58:44.140]   (laughs)
[01:58:44.140 --> 01:58:46.100]   - I can't. - This is always respectful.
[01:58:46.100 --> 01:58:47.980]   - If this fella falls in Mr. - If this fella falls in Mr.
[01:58:47.980 --> 01:58:49.500]   - You stay, there's a reason why.
[01:58:49.500 --> 01:58:52.300]   - Well, if you called me Ms. Hagenbotham,
[01:58:52.300 --> 01:58:53.700]   I'd be like, "Holy moly."
[01:58:53.700 --> 01:58:54.540]   - What do I do?
[01:58:54.540 --> 01:58:56.620]   (laughs)
[01:58:56.620 --> 01:59:01.860]   - I will reach out to Mr. Stamos
[01:59:01.860 --> 01:59:04.620]   and see if we can get him back on.
[01:59:04.620 --> 01:59:05.460]   That's good to know.
[01:59:05.460 --> 01:59:06.380]   - Wait.
[01:59:06.380 --> 01:59:07.220]   Hey, what do you think?
[01:59:07.220 --> 01:59:10.220]   - Ms. Hagenbotham is pretty good too.
[01:59:10.220 --> 01:59:12.380]   Yeah, I listened to other podcast people,
[01:59:12.380 --> 01:59:14.980]   but Ms. Co-host, I can't remember her name.
[01:59:14.980 --> 01:59:16.340]   (laughs)
[01:59:16.340 --> 01:59:20.220]   She's pretty entertaining too, and just quite knowledgeable.
[01:59:20.220 --> 01:59:25.320]   - I'm moderated content is the name of the show.
[01:59:25.320 --> 01:59:30.020]   - Yeah, I'm trying to find the name of the title.
[01:59:30.020 --> 01:59:32.540]   - Oh, Evelyn, Evelyn Doick.
[01:59:32.540 --> 01:59:33.380]   Is that who it is?
[01:59:33.380 --> 01:59:34.540]   - Evelyn, yes, that's her name.
[01:59:34.540 --> 01:59:35.580]   - An accent?
[01:59:35.580 --> 01:59:38.420]   - Yeah, it was a brilliant legal scholar
[01:59:38.420 --> 01:59:42.340]   who went from Harvard to Stanford.
[01:59:42.340 --> 01:59:44.900]   She has a great accent too.
[01:59:44.900 --> 01:59:46.180]   - Yes, yes.
[01:59:46.180 --> 01:59:49.780]   - And she does a lot of First Amendment stuff
[01:59:49.780 --> 01:59:53.100]   and content moderation and freedom of expression stuff.
[01:59:53.100 --> 01:59:55.220]   I don't know if it was agreed, but that's fine.
[01:59:55.220 --> 01:59:56.060]   - Right.
[01:59:56.060 --> 01:59:56.900]   - Brilliant.
[01:59:56.900 --> 01:59:57.740]   - Yeah. - Good.
[01:59:57.740 --> 01:59:58.580]   - Be good for discussion.
[01:59:58.580 --> 02:00:00.060]   - Be good with scholar.
[02:00:00.060 --> 02:00:01.140]   - There you go.
[02:00:01.140 --> 02:00:02.620]   - Cool.
[02:00:02.620 --> 02:00:04.020]   Should we do a change log?
[02:00:04.020 --> 02:00:04.860]   It's about time.
[02:00:04.860 --> 02:00:06.300]   - I think it's hard to have one.
[02:00:06.300 --> 02:00:07.140]   - We have one.
[02:00:07.140 --> 02:00:11.660]   - We have a hardy change log, so let's jump into it.
[02:00:11.660 --> 02:00:12.500]   Sweet.
[02:00:12.500 --> 02:00:13.340]   - The Google.
[02:00:13.340 --> 02:00:14.180]   - Hearty.
[02:00:14.180 --> 02:00:15.020]   - The Google.
[02:00:15.020 --> 02:00:18.740]   - My change log got cooped on.
[02:00:18.740 --> 02:00:21.580]   - Oh, yeah, as well it should have, man.
[02:00:21.580 --> 02:00:22.860]   - Happy I wasn't here for that.
[02:00:22.860 --> 02:00:23.700]   - Not in there.
[02:00:23.700 --> 02:00:28.380]   - Well, that's not gonna happen here.
[02:00:28.380 --> 02:00:31.540]   There's some pretty interesting stuff this week.
[02:00:31.540 --> 02:00:32.380]   - Let's start.
[02:00:32.380 --> 02:00:33.220]   - Let's see, would you just say Jason?
[02:00:33.220 --> 02:00:34.820]   - I'm not, no, I'm not gonna say that word anymore.
[02:00:34.820 --> 02:00:36.460]   - It just feels weird.
[02:00:36.460 --> 02:00:37.300]   Not gonna lie.
[02:00:37.300 --> 02:00:41.500]   Android 14 beta one is out,
[02:00:41.500 --> 02:00:44.620]   so this is the first version of the new,
[02:00:44.620 --> 02:00:47.180]   the upcoming version of Android that is,
[02:00:47.180 --> 02:00:51.180]   that is good enough to be considered a beta
[02:00:51.180 --> 02:00:52.700]   and not a developer preview.
[02:00:52.700 --> 02:00:56.500]   So, it's one step further up on the ladder
[02:00:56.500 --> 02:00:58.900]   as far as things like stability.
[02:00:58.900 --> 02:01:00.860]   And if I install this on my device,
[02:01:00.860 --> 02:01:02.500]   am I gonna lose all my data?
[02:01:02.500 --> 02:01:04.420]   I'm not saying that you are, you aren't.
[02:01:04.420 --> 02:01:07.020]   You're saying, you know, you can probably have a little--
[02:01:07.020 --> 02:01:09.580]   - Don't play with Jason in it anyways, yes.
[02:01:09.580 --> 02:01:10.740]   Don't follow on my footsteps.
[02:01:10.740 --> 02:01:13.220]   I have not installed the beta yet,
[02:01:13.220 --> 02:01:16.700]   but I will probably do that
[02:01:16.700 --> 02:01:18.900]   because usually by the time it hits betas
[02:01:18.900 --> 02:01:22.700]   when I tend to put these on my device these days.
[02:01:22.700 --> 02:01:26.100]   So, I might have an interesting week ahead of me
[02:01:26.100 --> 02:01:27.820]   as I make that happen, but,
[02:01:27.820 --> 02:01:32.740]   smarter system UI tent poll says nine to five Google,
[02:01:32.740 --> 02:01:35.540]   Android 14 is gonna feature more prominent back arrows,
[02:01:35.540 --> 02:01:40.540]   some tweaks to how the back gesture works,
[02:01:40.540 --> 02:01:43.140]   kind of a smarter back gesture.
[02:01:43.140 --> 02:01:46.140]   So, one of the things that I know about is like,
[02:01:46.140 --> 02:01:48.460]   when you swipe to go back,
[02:01:48.460 --> 02:01:51.540]   you actually end up getting this kind of like preview
[02:01:51.540 --> 02:01:52.540]   of where you're going.
[02:01:52.540 --> 02:01:55.380]   It's not like a black box of like, I swipe back,
[02:01:55.380 --> 02:01:57.380]   and who knows where I'm gonna end up?
[02:01:57.380 --> 02:01:59.300]   It's like, if you swipe and you hold,
[02:01:59.300 --> 02:02:02.940]   you actually see the screen that you're going to.
[02:02:02.940 --> 02:02:04.620]   So, that's--
[02:02:04.620 --> 02:02:06.580]   - Can you choose to go even further back?
[02:02:06.580 --> 02:02:11.180]   - With a single swipe, I don't think so.
[02:02:11.180 --> 02:02:12.420]   - Oh, well, like if you swipe and hold--
[02:02:12.420 --> 02:02:14.180]   - I'll be neat though, now that I'm thinking about it,
[02:02:14.180 --> 02:02:16.180]   that would be kind of neat to go back into different--
[02:02:16.180 --> 02:02:17.020]   - Okay, sorry.
[02:02:17.020 --> 02:02:19.180]   - States, I like that, actually.
[02:02:19.180 --> 02:02:20.580]   Maybe that's for next year.
[02:02:20.580 --> 02:02:26.140]   Apps can add custom actions to the system share sheets.
[02:02:26.140 --> 02:02:28.580]   So, opening the share sheet in a photo app
[02:02:28.580 --> 02:02:32.100]   might let you quickly create an album or a link.
[02:02:32.100 --> 02:02:35.260]   Share sheet, of course, is the sheet that comes up
[02:02:35.260 --> 02:02:37.380]   when you choose to share something.
[02:02:37.380 --> 02:02:40.380]   And I think historically speaking,
[02:02:40.380 --> 02:02:43.220]   an Android share sheet has kind of been a mixed bag
[02:02:43.220 --> 02:02:45.620]   because sometimes they're what you expect to see
[02:02:45.620 --> 02:02:47.900]   and sometimes they're created by a developer.
[02:02:47.900 --> 02:02:50.020]   That's different from what you expect to see.
[02:02:50.020 --> 02:02:52.020]   Google's been working to kind of tighten that up
[02:02:52.020 --> 02:02:55.020]   and make it a little bit more of a seamless experience.
[02:02:55.020 --> 02:02:57.060]   So, I think we're gonna get a little bit of that
[02:02:57.060 --> 02:03:00.620]   in Android 14 and other things.
[02:03:00.620 --> 02:03:03.980]   I mean, really with this beta, it's very minor stuff.
[02:03:03.980 --> 02:03:07.780]   So, if you like betas, check it out for yourself.
[02:03:07.780 --> 02:03:11.460]   Just be aware, understand it's a beta.
[02:03:11.460 --> 02:03:14.060]   It might not work perfectly on your device.
[02:03:14.060 --> 02:03:15.260]   That's just the way it goes.
[02:03:15.260 --> 02:03:22.020]   Pixel 7 April update is fixing just a few bugs.
[02:03:23.020 --> 02:03:27.660]   There's a Bluetooth issue, Bluetooth devices and accessories
[02:03:27.660 --> 02:03:31.340]   suddenly unparing from the device silently.
[02:03:31.340 --> 02:03:33.860]   This fixes that issue if you've encountered that
[02:03:33.860 --> 02:03:35.380]   on your Pixel 7.
[02:03:35.380 --> 02:03:36.540]   There's an-- - Is it unparing
[02:03:36.540 --> 02:03:39.220]   or just disconnecting, Mr. Howe?
[02:03:39.220 --> 02:03:42.380]   - Well, I haven't experienced this personally.
[02:03:42.380 --> 02:03:44.860]   The article says unparing.
[02:03:44.860 --> 02:03:46.140]   - Okay.
[02:03:46.140 --> 02:03:49.380]   - Because yeah, that would be two different things, wouldn't it?
[02:03:49.380 --> 02:03:51.100]   - Yeah. - Disconnected would be okay.
[02:03:51.100 --> 02:03:52.140]   I'm not connected anymore.
[02:03:52.140 --> 02:03:56.620]   Unpairing would be like, I have nothing stored
[02:03:56.620 --> 02:03:58.740]   on your phone anymore and you have to re-wire with me
[02:03:58.740 --> 02:03:59.580]   in order to work.
[02:03:59.580 --> 02:04:00.940]   Those are two different things.
[02:04:00.940 --> 02:04:02.020]   This says unpair.
[02:04:02.020 --> 02:04:04.100]   So, I'm assuming that's what it means.
[02:04:04.100 --> 02:04:07.900]   And that would be a real big pain in the butt.
[02:04:07.900 --> 02:04:08.820]   - Yeah.
[02:04:08.820 --> 02:04:13.820]   - Also some camera and other system bugs fixed with that.
[02:04:13.820 --> 02:04:16.380]   This is interesting.
[02:04:16.380 --> 02:04:17.420]   I don't know if it's changed a lot
[02:04:17.420 --> 02:04:19.420]   because it's not something that's happening right now,
[02:04:19.420 --> 02:04:21.420]   but this seemed like a great place to put it
[02:04:21.420 --> 02:04:22.620]   because I think it's really--
[02:04:22.620 --> 02:04:23.460]   - Oh, we're going to move on.
[02:04:23.460 --> 02:04:24.460]   - Can you really be a feature?
[02:04:24.460 --> 02:04:26.020]   (laughing)
[02:04:26.020 --> 02:04:26.860]   This is for the future.
[02:04:26.860 --> 02:04:27.780]   This is a change-log for the future,
[02:04:27.780 --> 02:04:29.340]   but we're gonna talk about it now anyways.
[02:04:29.340 --> 02:04:33.060]   Apparently, find my device is getting an update
[02:04:33.060 --> 02:04:36.860]   sometime soon that will not require your device
[02:04:36.860 --> 02:04:39.180]   to be powered on in order to work.
[02:04:39.180 --> 02:04:40.540]   - Ooh, that's cool.
[02:04:40.540 --> 02:04:42.620]   - So if your phone is off,
[02:04:42.620 --> 02:04:45.460]   there will still be some sort of Bluetooth connectivity
[02:04:45.460 --> 02:04:50.460]   that's active enough to allow for tracking,
[02:04:50.580 --> 02:04:55.580]   which I think this is how the standalone trackers work.
[02:04:55.580 --> 02:04:58.740]   Like they're really low power,
[02:04:58.740 --> 02:04:59.980]   they're just putting out a signal
[02:04:59.980 --> 02:05:01.660]   if another device crosses its path and goes,
[02:05:01.660 --> 02:05:04.700]   "Oh, hey, there's something there via Bluetooth
[02:05:04.700 --> 02:05:06.820]   or ultra wideband or whatever."
[02:05:06.820 --> 02:05:10.020]   And that's gonna happen inside of the phone as well.
[02:05:10.020 --> 02:05:13.660]   So I guess Apple has this on iPhone devices,
[02:05:13.660 --> 02:05:16.020]   coming to Pixel devices definitely.
[02:05:16.020 --> 02:05:17.700]   And I'm sure this means it'll come up
[02:05:17.700 --> 02:05:20.300]   to other Google devices as well,
[02:05:20.300 --> 02:05:22.900]   but that seems like a really cool feature to me.
[02:05:22.900 --> 02:05:23.740]   - Yeah.
[02:05:23.740 --> 02:05:25.820]   - Well, but the flip side is now it means
[02:05:25.820 --> 02:05:27.460]   that even if you turn your phone off,
[02:05:27.460 --> 02:05:28.460]   - Yeah.
[02:05:28.460 --> 02:05:30.020]   - Right. - Basically don't take your phone
[02:05:30.020 --> 02:05:32.540]   anywhere if you're going to commit crimes.
[02:05:32.540 --> 02:05:33.700]   That's more wrong.
[02:05:33.700 --> 02:05:35.780]   - Oh, okay. - The wonder about that, Stacey.
[02:05:35.780 --> 02:05:37.340]   - Appreciate that. - Well, I mean, you joke,
[02:05:37.340 --> 02:05:40.460]   but I mean, there are reasons that you might actually be like,
[02:05:40.460 --> 02:05:42.020]   well, I really don't want my husband
[02:05:42.020 --> 02:05:43.220]   to know where I'm at right now.
[02:05:43.220 --> 02:05:44.660]   I'm buying Christmas presents.
[02:05:44.660 --> 02:05:45.940]   I'm having a- - Oh, is that it?
[02:05:45.940 --> 02:05:47.900]   - Christmas presents, ah, sure.
[02:05:47.900 --> 02:05:49.380]   Or I'm involved in a protest,
[02:05:49.380 --> 02:05:51.540]   and I don't want to be picked up,
[02:05:51.540 --> 02:05:52.380]   - Thank you, Chacey.
[02:05:52.380 --> 02:05:53.660]   - As part of a protest or something.
[02:05:53.660 --> 02:05:54.860]   You turned your phone off.
[02:05:54.860 --> 02:05:57.380]   This still has the potential of tracking you there.
[02:05:57.380 --> 02:05:58.380]   Yeah.
[02:05:58.380 --> 02:05:59.460]   That's a really good point.
[02:05:59.460 --> 02:06:01.140]   - I just want people to be aware of,
[02:06:01.140 --> 02:06:02.500]   there's a good side.
[02:06:02.500 --> 02:06:03.340]   - Yes.
[02:06:03.340 --> 02:06:04.340]   - It's a potential problem.
[02:06:04.340 --> 02:06:05.500]   - Yes, indeed.
[02:06:05.500 --> 02:06:06.340]   - It's the balance.
[02:06:06.340 --> 02:06:08.140]   - Well, and I mean, that's like,
[02:06:08.140 --> 02:06:11.660]   that's like the headline of tracking any device
[02:06:11.660 --> 02:06:12.860]   in general, right?
[02:06:12.860 --> 02:06:17.180]   'Cause we've seen really great uses of tracking technology,
[02:06:17.180 --> 02:06:20.300]   and then we've seen really awful, horrible uses
[02:06:20.300 --> 02:06:22.780]   of tracking technology like Apple's,
[02:06:22.780 --> 02:06:24.580]   at trackers, what are they called?
[02:06:24.580 --> 02:06:26.220]   - Air tags.
[02:06:26.220 --> 02:06:30.220]   - Air tags, you know, being attached to vehicles to track,
[02:06:30.220 --> 02:06:34.060]   so that spouses can track one another or whatever.
[02:06:34.060 --> 02:06:37.580]   - And so people are doubting my use of Christmas presents.
[02:06:37.580 --> 02:06:40.060]   I am actually a psychotic individual
[02:06:40.060 --> 02:06:42.620]   who does track around the holidays,
[02:06:42.620 --> 02:06:45.460]   where my husband goes and parks the car,
[02:06:45.460 --> 02:06:47.620]   and I look for what shops are next to there,
[02:06:47.620 --> 02:06:50.260]   just to see what's, 'cause I'm like, oh.
[02:06:50.260 --> 02:06:51.700]   - Oh, geez.
[02:06:51.700 --> 02:06:53.980]   - He's right by the jewelry store.
[02:06:53.980 --> 02:06:56.900]   (laughing)
[02:06:56.900 --> 02:06:59.380]   - And so if he doesn't get it, come Christmas.
[02:06:59.380 --> 02:07:00.220]   - Yeah.
[02:07:00.220 --> 02:07:01.060]   - When I was gonna get it,
[02:07:01.060 --> 02:07:03.380]   Diamond, I only got right expectation
[02:07:03.380 --> 02:07:04.660]   in the condition that I ate.
[02:07:04.660 --> 02:07:05.660]   - Oof.
[02:07:05.660 --> 02:07:08.420]   - Yeah, it's, or, you know, it also comes in handy,
[02:07:08.420 --> 02:07:10.660]   like, you know, if he's on a, like,
[02:07:10.660 --> 02:07:13.140]   if he's like, oh, I'm gonna be in Seattle for a while,
[02:07:13.140 --> 02:07:14.340]   I'll check where he is,
[02:07:14.340 --> 02:07:16.020]   'cause what if he's near a restaurant I like,
[02:07:16.020 --> 02:07:18.420]   then I can ask him to pick up donuts or something.
[02:07:18.420 --> 02:07:19.260]   I can.
[02:07:19.260 --> 02:07:20.100]   - Mm.
[02:07:20.100 --> 02:07:20.940]   - He knows I do this.
[02:07:20.940 --> 02:07:22.780]   - He knows, okay, all right.
[02:07:22.780 --> 02:07:25.700]   - I'm like, I see here next to that Mexican food place.
[02:07:25.700 --> 02:07:28.060]   I'm like, can you pick up some mole enchiladas?
[02:07:28.060 --> 02:07:30.420]   (laughing)
[02:07:30.420 --> 02:07:31.540]   - That's really handy.
[02:07:31.540 --> 02:07:34.940]   Let's see here.
[02:07:34.940 --> 02:07:38.900]   Android developers are gonna be required
[02:07:38.900 --> 02:07:42.260]   to give users the ability to delete their account data
[02:07:42.260 --> 02:07:44.260]   both in the app and online.
[02:07:44.260 --> 02:07:46.640]   So this is something new that's coming,
[02:07:46.640 --> 02:07:52.220]   is this requirement for developers that create apps
[02:07:52.220 --> 02:07:55.460]   that have some sort of an account creation process.
[02:07:55.460 --> 02:07:58.580]   So if you're using an account or an app or a service
[02:07:58.580 --> 02:08:00.780]   that requires you to create an account,
[02:08:00.780 --> 02:08:02.900]   Google's basically saying, if you're doing that,
[02:08:02.900 --> 02:08:06.140]   you gotta make it dead simple to delete that account.
[02:08:06.140 --> 02:08:08.620]   Google isn't saying what it means to delete that account,
[02:08:08.620 --> 02:08:11.940]   like they're not verifying that the deletion happened,
[02:08:11.940 --> 02:08:14.500]   but they are saying, you know,
[02:08:14.500 --> 02:08:16.420]   the user doesn't have to call a number
[02:08:16.420 --> 02:08:19.220]   and talk to someone in order to make this happen.
[02:08:19.220 --> 02:08:23.620]   There actually will be a link on the Play Store page.
[02:08:23.620 --> 02:08:25.740]   So you don't even have to reinstall the app
[02:08:25.740 --> 02:08:27.980]   if you've uninstalled it in order to delete the account.
[02:08:27.980 --> 02:08:30.540]   You just click the link on the Play Store page
[02:08:30.540 --> 02:08:33.700]   and that takes you to, hopefully anyways,
[02:08:33.700 --> 02:08:36.660]   according to this, a way to delete your account and your data.
[02:08:36.660 --> 02:08:39.100]   So that's nice.
[02:08:39.100 --> 02:08:42.020]   Give you more control over your data out there.
[02:08:42.020 --> 02:08:44.300]   More control is a good thing.
[02:08:44.300 --> 02:08:46.300]   - That's where I think we mentioned that.
[02:08:46.300 --> 02:08:47.660]   - Possible.
[02:08:47.660 --> 02:08:48.500]   - Yeah, it's nice.
[02:08:48.500 --> 02:08:49.940]   - Apologies if you did.
[02:08:49.940 --> 02:08:52.940]   - It's worth mentioning twice because deleting your data
[02:08:52.940 --> 02:08:54.580]   is always a nice thing to know about,
[02:08:54.580 --> 02:08:56.780]   especially if your wife is crazy and stalks you.
[02:08:56.780 --> 02:08:59.540]   (both laughing)
[02:08:59.540 --> 02:09:04.780]   - Auto Archive, for Android, beginning to roll out.
[02:09:04.780 --> 02:09:09.780]   This is the idea that when your device is low on storage,
[02:09:09.780 --> 02:09:12.300]   which I don't know the last time that's happened to me,
[02:09:12.300 --> 02:09:14.580]   but my phone has a lot of storage.
[02:09:14.580 --> 02:09:17.500]   There are phones out there that have very minimal storage.
[02:09:17.500 --> 02:09:20.500]   And if you have a device that has very minimal storage,
[02:09:20.500 --> 02:09:22.620]   you've got so many apps installed on there,
[02:09:22.620 --> 02:09:26.260]   or so few extra small amount of space
[02:09:26.260 --> 02:09:28.260]   on your device to install a new app.
[02:09:28.260 --> 02:09:30.540]   When you go to try and install the app,
[02:09:30.540 --> 02:09:33.260]   Google or the phone will give you a pop up
[02:09:33.260 --> 02:09:36.940]   that basically says, "Do you wanna auto archive
[02:09:36.940 --> 02:09:38.580]   some of the apps that you're not using?"
[02:09:38.580 --> 02:09:41.220]   And it will know which apps you're not using very regularly.
[02:09:41.220 --> 02:09:44.300]   And only certain types of apps will be able to do this.
[02:09:44.300 --> 02:09:48.020]   It's apps that are created within what's called an app bundle,
[02:09:48.020 --> 02:09:51.820]   which is essentially kind of compartmentalizing the app.
[02:09:51.820 --> 02:09:53.060]   So it's not one single file,
[02:09:53.060 --> 02:09:55.460]   but it's lots of little pieces that make up the file
[02:09:55.460 --> 02:09:59.420]   so that when you decide you wanna auto archive that app,
[02:09:59.420 --> 02:10:03.340]   it will remove most of the app, like 60% of the app,
[02:10:03.340 --> 02:10:06.580]   but still leave the data, the important pieces of the data
[02:10:06.580 --> 02:10:08.900]   so that later, if you wanna reinstall it,
[02:10:08.900 --> 02:10:11.020]   you just tap the icon on your home screen,
[02:10:11.020 --> 02:10:12.300]   it'll pull it from the cloud,
[02:10:12.300 --> 02:10:14.300]   and it's like it never went anywhere.
[02:10:14.300 --> 02:10:16.100]   You do pick up a pretty cool-- - That's pretty cool.
[02:10:16.100 --> 02:10:21.100]   - So need for people who have low storage on their devices.
[02:10:21.100 --> 02:10:25.140]   - That's not bad at all.
[02:10:25.140 --> 02:10:28.500]   Let's see, what is this?
[02:10:28.500 --> 02:10:32.780]   - Oh, app streaming for Chromebooks.
[02:10:32.780 --> 02:10:37.780]   So this is essentially, you've got your phone
[02:10:37.780 --> 02:10:40.420]   and you're sitting down with your Chromebook
[02:10:40.420 --> 02:10:44.340]   and you wanna pull up a stream of your phone
[02:10:44.340 --> 02:10:45.380]   on your Chromebook.
[02:10:45.380 --> 02:10:47.180]   So it's essentially pulling up the apps
[02:10:47.180 --> 02:10:51.700]   or an instance of your phone on your Chromebook display
[02:10:51.700 --> 02:10:54.620]   and then being able to interact with that data
[02:10:54.620 --> 02:10:56.020]   on your screen.
[02:10:56.020 --> 02:10:59.700]   - So is that like the AirPlay stuff that Apple does?
[02:10:59.700 --> 02:11:01.220]   I'm not sure if I understand.
[02:11:01.220 --> 02:11:04.780]   - I know that it's like what Apple offers
[02:11:04.780 --> 02:11:07.340]   between the iPhone and the Mac.
[02:11:07.340 --> 02:11:10.660]   I'm not sure if it's AirPlay that's powers that,
[02:11:10.660 --> 02:11:12.740]   I'm not quite sure. - Okay, see.
[02:11:12.740 --> 02:11:14.340]   - It's all new to me.
[02:11:14.340 --> 02:11:16.340]   - Yeah, but Apple does offer this
[02:11:16.340 --> 02:11:21.340]   between iOS devices or at least iPhones, as far as I know,
[02:11:21.340 --> 02:11:25.660]   and the Mac OS is that you can kind of open up
[02:11:25.660 --> 02:11:29.740]   an app, it's not pretty certain that Apple offers this.
[02:11:29.740 --> 02:11:31.500]   Anyways, Google's offering it as well.
[02:11:31.500 --> 02:11:35.100]   We know this because there is the new app,
[02:11:35.100 --> 02:11:39.020]   cross device services that will allow you to do this stuff.
[02:11:39.020 --> 02:11:40.020]   It's kind of the starting point.
[02:11:40.020 --> 02:11:43.620]   We're not quite there yet, but this app will enable it
[02:11:43.620 --> 02:11:46.060]   and it'll allow you to do things like reply to a message,
[02:11:46.060 --> 02:11:47.740]   check the status of a ride share,
[02:11:47.740 --> 02:11:51.300]   start or edit your shopping list from your Chromebook,
[02:11:51.300 --> 02:11:52.500]   using your phone.
[02:11:52.500 --> 02:11:55.140]   These are examples of course that the article pointed out,
[02:11:55.140 --> 02:12:00.020]   but yeah, so kind of increasing the abilities
[02:12:00.020 --> 02:12:04.300]   of your Chromebook by connecting it with your phone.
[02:12:04.300 --> 02:12:10.660]   And then finally, in the supercharged change log,
[02:12:10.660 --> 02:12:14.460]   personal loan apps can be really scummy.
[02:12:14.460 --> 02:12:16.060]   (laughs)
[02:12:16.060 --> 02:12:19.020]   Google has been cracking down on this kind of segment
[02:12:19.020 --> 02:12:20.940]   of apps for a little while.
[02:12:20.940 --> 02:12:25.940]   They had basically capped these apps at 36%
[02:12:25.940 --> 02:12:28.940]   as far as the annual interest rate
[02:12:28.940 --> 02:12:31.180]   that's allowed in the Play Store for these apps,
[02:12:31.180 --> 02:12:32.180]   which is insane.
[02:12:32.180 --> 02:12:34.180]   I'm kind of like, why 36%?
[02:12:34.180 --> 02:12:35.100]   - Is this right?
[02:12:35.100 --> 02:12:36.620]   - Yeah, even that is horrible,
[02:12:36.620 --> 02:12:39.100]   but apparently these apps went higher than that.
[02:12:39.100 --> 02:12:41.620]   And so they did that a while back.
[02:12:41.620 --> 02:12:44.340]   Now they're restricting this category of apps
[02:12:44.340 --> 02:12:48.300]   from having access to certain data points on the phone.
[02:12:48.300 --> 02:12:50.940]   So things like photos, videos, contacts,
[02:12:50.940 --> 02:12:53.900]   precise location, call logs, why?
[02:12:53.900 --> 02:12:57.260]   Because these apps were using that access
[02:12:57.260 --> 02:13:01.260]   to glean information about the users who are taking,
[02:13:01.260 --> 02:13:03.740]   or the borrowers who are borrowing this money
[02:13:03.740 --> 02:13:06.020]   at insane annual interest rates.
[02:13:06.020 --> 02:13:09.940]   And then going after the people in their call logs
[02:13:09.940 --> 02:13:14.660]   or modifying the photos to shame them to pay back,
[02:13:14.660 --> 02:13:16.940]   and that's really scummy stuff.
[02:13:16.940 --> 02:13:17.980]   So it was cracking down.
[02:13:17.980 --> 02:13:22.020]   - Imagine collections if they always knew where you were.
[02:13:22.020 --> 02:13:22.860]   - Yeah.
[02:13:22.860 --> 02:13:23.940]   - Your phone was off.
[02:13:23.940 --> 02:13:24.940]   - Oh my goodness.
[02:13:24.940 --> 02:13:25.780]   - Wow.
[02:13:25.780 --> 02:13:27.380]   - It was horrible.
[02:13:27.380 --> 02:13:29.140]   So anyways, they're cracking down on that.
[02:13:29.140 --> 02:13:33.580]   I think that, oh no, I know that this starts May 31st.
[02:13:33.580 --> 02:13:37.940]   So not too much longer and that will be in effect.
[02:13:37.940 --> 02:13:39.300]   It's good stuff.
[02:13:39.300 --> 02:13:40.140]   This is like--
[02:13:40.140 --> 02:13:42.100]   - The fact that that happened at all is harmful.
[02:13:42.100 --> 02:13:44.140]   - Yeah, oh, totally.
[02:13:44.140 --> 02:13:46.860]   Totally, yeah, it's super yucky.
[02:13:46.860 --> 02:13:50.300]   So good change log, but we've reached the end.
[02:13:50.300 --> 02:13:51.860]   So you're the drunk.
[02:13:51.860 --> 02:13:57.900]   All right, we are at the picks time.
[02:13:57.900 --> 02:13:59.340]   We gotta get this show wrapped up
[02:13:59.340 --> 02:14:02.420]   because Stacey's got a birthday dinner to get to.
[02:14:02.420 --> 02:14:04.260]   So Stacey, why don't we start with you?
[02:14:04.260 --> 02:14:05.660]   What you got?
[02:14:05.660 --> 02:14:06.620]   - I have a lovely pick.
[02:14:06.620 --> 02:14:08.660]   I have so many picks 'cause I haven't been here.
[02:14:08.660 --> 02:14:09.980]   I'm gonna grab it for you.
[02:14:09.980 --> 02:14:12.380]   - Or them.
[02:14:12.380 --> 02:14:15.260]   You said you had so many.
[02:14:15.260 --> 02:14:16.580]   I'm assuming it's them and not it.
[02:14:16.580 --> 02:14:18.540]   - Oh, I can't, no, it's it.
[02:14:18.540 --> 02:14:19.740]   I'm not gonna do all of the picks.
[02:14:19.740 --> 02:14:20.580]   That would be insane.
[02:14:20.580 --> 02:14:22.340]   - I see, I understand.
[02:14:22.340 --> 02:14:25.900]   - This, I have to, this is the homey hub.
[02:14:25.900 --> 02:14:27.220]   One of my hairs is stuck from the hand.
[02:14:27.220 --> 02:14:28.060]   That's disgusting.
[02:14:28.060 --> 02:14:33.900]   This is a $70 smartphone hub.
[02:14:33.900 --> 02:14:34.780]   And this is what it looks like.
[02:14:34.780 --> 02:14:36.220]   See how pretty the little LED light is?
[02:14:36.220 --> 02:14:37.300]   - This is fine, yeah.
[02:14:37.300 --> 02:14:38.140]   Yeah.
[02:14:38.140 --> 02:14:39.500]   - Reminds me of my Nexus Q.
[02:14:39.500 --> 02:14:42.660]   - Oh.
[02:14:42.660 --> 02:14:44.020]   - Sort of, yeah, okay.
[02:14:44.020 --> 02:14:45.300]   - No, I mean, the light does.
[02:14:45.300 --> 02:14:46.140]   The light that surrounds it.
[02:14:46.140 --> 02:14:49.020]   - No, not the device itself.
[02:14:49.020 --> 02:14:51.180]   Just that round LED light.
[02:14:51.180 --> 02:14:54.700]   - Okay, so this is the homey bridge.
[02:14:54.700 --> 02:14:59.180]   And this is a $69.99 device that has started to be,
[02:14:59.180 --> 02:15:00.700]   that is sold now in the US.
[02:15:00.700 --> 02:15:01.540]   It's from Europe.
[02:15:01.540 --> 02:15:03.980]   It's a European company called At Home.
[02:15:03.980 --> 02:15:08.260]   And their big thing is privacy.
[02:15:08.260 --> 02:15:12.300]   So they also have a fancier version for $399.99
[02:15:12.300 --> 02:15:14.300]   that is called the homey pro.
[02:15:14.300 --> 02:15:16.700]   This guy is cheaper,
[02:15:16.700 --> 02:15:19.380]   but they still want to emphasize privacy.
[02:15:19.380 --> 02:15:22.540]   So if you buy this, you can control only five devices
[02:15:22.540 --> 02:15:25.140]   in your house without having to pay anything else.
[02:15:25.140 --> 02:15:26.780]   If you pay a subscription fee,
[02:15:26.780 --> 02:15:28.300]   and I know y'all hate subscription fees,
[02:15:28.300 --> 02:15:29.780]   but it's $299 a month,
[02:15:29.780 --> 02:15:32.260]   you can control as many devices as you want.
[02:15:32.260 --> 02:15:34.020]   They charge the subscription fee
[02:15:34.020 --> 02:15:37.540]   because they are incurring costs
[02:15:37.540 --> 02:15:40.580]   when you connect devices to the cloud, right?
[02:15:40.580 --> 02:15:43.060]   They have to make sure this device stays up to date.
[02:15:43.060 --> 02:15:44.900]   They have to make sure the software is all running,
[02:15:44.900 --> 02:15:46.940]   the integrations are all running, et cetera, et cetera.
[02:15:46.940 --> 02:15:49.420]   So I'm just explaining how this works
[02:15:49.420 --> 02:15:53.020]   'cause most people like subscription suck, and that's fine.
[02:15:53.020 --> 02:15:54.100]   If you hate subscriptions,
[02:15:54.100 --> 02:15:55.620]   you can pay for the really expensive one
[02:15:55.620 --> 02:15:57.060]   that does everything locally.
[02:15:57.060 --> 02:16:00.580]   But this is an interesting device.
[02:16:00.580 --> 02:16:03.020]   It has ZigBee, it has Z-Wave, it has Bluetooth,
[02:16:03.020 --> 02:16:05.860]   it has Wi-Fi, X is an IR blaster,
[02:16:05.860 --> 02:16:08.900]   and it controls not all of my devices,
[02:16:08.900 --> 02:16:10.260]   but many of my devices,
[02:16:10.260 --> 02:16:15.220]   including my not Sonos, it does include that too,
[02:16:15.220 --> 02:16:20.220]   but it is including my Sonoff blinds.
[02:16:20.220 --> 02:16:21.900]   Also, if you have IKEA smart blinds,
[02:16:21.900 --> 02:16:23.300]   it will control those too.
[02:16:23.300 --> 02:16:26.940]   So it's nice, and the other thing I like about it
[02:16:26.940 --> 02:16:31.300]   is it has these lovely automations that are super powerful.
[02:16:31.300 --> 02:16:34.420]   They're automations, I'll let you create
[02:16:34.420 --> 02:16:38.140]   some really awesome things like you can say,
[02:16:38.140 --> 02:16:40.900]   if presence is detected and it's this time,
[02:16:40.900 --> 02:16:44.700]   or something else is happening, do this, and this, and this.
[02:16:44.700 --> 02:16:46.780]   It's just a really nice and very powerful
[02:16:46.780 --> 02:16:49.020]   kind of DIY smart home system.
[02:16:49.020 --> 02:16:52.100]   So that is the homey bridge,
[02:16:52.100 --> 02:16:55.900]   and if you wanna try it out, 70 bucks,
[02:16:55.900 --> 02:16:58.900]   it's totally worth it if you're into this sort of thing.
[02:16:58.900 --> 02:17:02.260]   And if you like it, and you wanna go even further
[02:17:02.260 --> 02:17:05.740]   and get the homey pro, because it's an IR blaster,
[02:17:05.740 --> 02:17:07.740]   you can still use the bridge with the pro
[02:17:07.740 --> 02:17:10.940]   as an IR blaster for controlling your TV or your AV
[02:17:10.940 --> 02:17:11.900]   or your planes.
[02:17:11.900 --> 02:17:14.300]   Questions, comments.
[02:17:14.300 --> 02:17:17.540]   - I like it.
[02:17:17.540 --> 02:17:19.380]   (laughs)
[02:17:19.380 --> 02:17:22.660]   - I mean, I just have like, is there anything you all want to know?
[02:17:22.660 --> 02:17:26.780]   - Kind of sounds, ways wise, wise, what's the company
[02:17:26.780 --> 02:17:28.980]   that the Chinese company that sells the nice cameras
[02:17:28.980 --> 02:17:30.100]   and stuff for less?
[02:17:30.100 --> 02:17:33.260]   - It is nothing like wise.
[02:17:33.260 --> 02:17:34.420]   - Oh, well nevermind.
[02:17:34.420 --> 02:17:36.100]   (laughs)
[02:17:36.100 --> 02:17:40.300]   - Wise takes devices that are made in China,
[02:17:40.300 --> 02:17:42.420]   and they rebatch them.
[02:17:42.420 --> 02:17:43.260]   - Why'd you just be the reason?
[02:17:43.260 --> 02:17:45.100]   - And they're not. - And they're not.
[02:17:45.100 --> 02:17:48.420]   - Oh, well, this has a subscription.
[02:17:48.420 --> 02:17:50.260]   Wise doesn't really have a lot of subscriptions.
[02:17:50.260 --> 02:17:51.620]   I mean, they have some, but.
[02:17:51.620 --> 02:17:52.940]   - They're a rock, yeah.
[02:17:52.940 --> 02:17:56.300]   - The hardware, I mean, it's a much,
[02:17:56.300 --> 02:17:58.620]   it's a nicer looking device.
[02:17:58.620 --> 02:18:00.740]   - You can turn the LED light off if you want.
[02:18:00.740 --> 02:18:05.460]   If like, 'cause like, usually to control my blinds,
[02:18:05.460 --> 02:18:06.980]   it would be in my bedroom where my blinds are,
[02:18:06.980 --> 02:18:08.180]   and obviously in the middle of the night,
[02:18:08.180 --> 02:18:09.820]   I do not want a rainbow LED.
[02:18:09.820 --> 02:18:15.660]   So, but yes, that is the device.
[02:18:15.660 --> 02:18:16.820]   - Very nice.
[02:18:16.820 --> 02:18:18.740]   Homie, it'll play that.
[02:18:18.740 --> 02:18:23.900]   Let's see here, Jeff, what you got?
[02:18:23.900 --> 02:18:27.580]   - So I'm gonna do something I had up higher on the rundown
[02:18:27.580 --> 02:18:30.900]   to do little inside baseball here about podcasting.
[02:18:30.900 --> 02:18:31.980]   - Ah, yeah.
[02:18:31.980 --> 02:18:34.260]   - Story that you probably all want to forward to,
[02:18:34.260 --> 02:18:36.020]   Leo and Lisa if they haven't seen it.
[02:18:36.020 --> 02:18:39.980]   So it says that they're discovering suddenly
[02:18:39.980 --> 02:18:42.860]   that some people prefer to watch podcasts
[02:18:42.860 --> 02:18:43.700]   and listen to them.
[02:18:43.700 --> 02:18:45.900]   - What?
[02:18:45.900 --> 02:18:46.740]   - Yeah. - Wait, everybody
[02:18:46.740 --> 02:18:48.180]   who's watching us right now.
[02:18:48.180 --> 02:18:52.020]   - Right, so Morning Consult found that 46%
[02:18:52.020 --> 02:18:54.380]   a podcast listener said they prefer consuming them
[02:18:54.380 --> 02:18:58.460]   with a video compared to 42% of prefer they listen.
[02:18:58.460 --> 02:19:02.580]   On the other hand, Westwood One found 10% prefer to watch,
[02:19:02.580 --> 02:19:07.580]   but still what it means is according to Neiman Lab,
[02:19:07.580 --> 02:19:10.620]   which is does very good reporting
[02:19:10.620 --> 02:19:12.540]   around journalism and media,
[02:19:12.540 --> 02:19:17.060]   that a lot of media companies are preferring YouTube now
[02:19:17.060 --> 02:19:19.460]   for their podcasts.
[02:19:19.460 --> 02:19:20.380]   - Yeah.
[02:19:20.380 --> 02:19:25.380]   - ESPN has, ESPN, Mike Foss said ESPN did 1.7 million views
[02:19:25.380 --> 02:19:28.620]   in the first month on YouTube in 2017.
[02:19:28.620 --> 02:19:31.860]   And now they're showing them up at 206 million views
[02:19:31.860 --> 02:19:33.060]   in 48 hours.
[02:19:33.060 --> 02:19:36.100]   - Yep, FS1 is doing pretty much the same thing.
[02:19:36.100 --> 02:19:39.940]   They have their popular shows of like,
[02:19:39.940 --> 02:19:41.980]   I think it's called Undisputed with Shannon Sharp
[02:19:41.980 --> 02:19:43.060]   and Skip Bayless,
[02:19:43.060 --> 02:19:46.380]   and they sit and go over different sports news
[02:19:46.380 --> 02:19:48.820]   and usually fuss and yell at each other.
[02:19:48.820 --> 02:19:51.500]   But then they put those clips on YouTube
[02:19:51.500 --> 02:19:55.580]   and it's way more viewership on YouTube versus what they're
[02:19:55.580 --> 02:19:57.220]   getting on the broadcast.
[02:19:57.220 --> 02:19:58.820]   And they've been doing this for years.
[02:19:58.820 --> 02:20:00.820]   They saw that coming years ago
[02:20:00.820 --> 02:20:05.500]   and ESPN jumped on just shortly after that.
[02:20:05.500 --> 02:20:08.700]   - And ESPN says that the YouTube shorts makes it easier
[02:20:08.700 --> 02:20:11.540]   to pull bits out of the podcasts and promote them
[02:20:11.540 --> 02:20:13.100]   within the YouTube ecosystem.
[02:20:13.100 --> 02:20:14.500]   - That's Mark Allen.
[02:20:14.500 --> 02:20:18.420]   - Westwood One found that podcasts newbies,
[02:20:18.420 --> 02:20:20.820]   if there are any left out there,
[02:20:20.820 --> 02:20:23.460]   prefer watching to listening.
[02:20:23.460 --> 02:20:24.980]   And the people who watch podcasts
[02:20:24.980 --> 02:20:28.180]   are more likely to be younger, 18 to 34 years old,
[02:20:28.180 --> 02:20:31.340]   than regular old listeners elsewhere.
[02:20:31.340 --> 02:20:35.900]   So I just thought it was useful for,
[02:20:35.900 --> 02:20:40.180]   as usual, Leo and Lisa are way ahead.
[02:20:40.180 --> 02:20:41.660]   And that doesn't get you much of anywhere,
[02:20:41.660 --> 02:20:44.300]   but they knew that--
[02:20:44.300 --> 02:20:46.140]   - First in line at the chopping block.
[02:20:46.140 --> 02:20:48.420]   (laughing)
[02:20:48.420 --> 02:20:50.980]   - That's what my parents used to tell me.
[02:20:50.980 --> 02:20:53.180]   - She's harsh.
[02:20:53.180 --> 02:20:54.020]   - Gow!
[02:20:54.020 --> 02:20:55.820]   - She is in her hair form today.
[02:20:55.820 --> 02:20:57.380]   - It is birthday girl.
[02:20:57.380 --> 02:20:58.820]   - Oh, yay.
[02:20:58.820 --> 02:21:00.500]   She can get away with anything today.
[02:21:00.500 --> 02:21:02.060]   - That's right.
[02:21:02.060 --> 02:21:05.620]   - So anyway, I think it's interesting for this very company
[02:21:05.620 --> 02:21:09.100]   that we love here and all of you out there love.
[02:21:09.100 --> 02:21:11.660]   And by the way, join the club!
[02:21:11.660 --> 02:21:12.900]   - Yeah.
[02:21:12.900 --> 02:21:14.740]   - That's right.
[02:21:14.740 --> 02:21:16.660]   Should we do that right now?
[02:21:16.660 --> 02:21:17.500]   - Yeah, sure.
[02:21:17.500 --> 02:21:19.140]   Club Twitch, that's right.
[02:21:19.140 --> 02:21:20.500]   You heard, Jeff,
[02:21:20.500 --> 02:21:22.140]   twit.tv/clubtwit.
[02:21:22.140 --> 02:21:23.500]   - All my orders.
[02:21:23.500 --> 02:21:26.180]   - You get all of our shows with no ads.
[02:21:26.180 --> 02:21:28.660]   You get Twit Plus podcast content
[02:21:28.660 --> 02:21:30.700]   that's exclusive to the club.
[02:21:30.700 --> 02:21:33.780]   You get shows that you can't find outside of the club,
[02:21:33.780 --> 02:21:37.260]   including of course, Stacey's book club in the club.
[02:21:37.260 --> 02:21:38.100]   - Mm-hmm.
[02:21:38.100 --> 02:21:39.780]   - That's just one of many.
[02:21:39.780 --> 02:21:42.700]   You get access to our members only Discord,
[02:21:42.700 --> 02:21:45.020]   which is just a heck of a lot of fun.
[02:21:45.020 --> 02:21:46.540]   Yes, there's the animated gist,
[02:21:46.540 --> 02:21:48.820]   but there's just a lot of really great rooms
[02:21:48.820 --> 02:21:52.100]   with a lot of very focused conversation.
[02:21:52.100 --> 02:21:53.260]   Some of them about the shows
[02:21:53.260 --> 02:21:55.900]   and some of them just about technology in general
[02:21:55.900 --> 02:21:59.140]   or hobbies and just, I mean, it's a wide range
[02:21:59.140 --> 02:22:00.740]   and very active stuff.
[02:22:00.740 --> 02:22:04.740]   So, twit.tv/clubtwit, seven bucks a month.
[02:22:04.740 --> 02:22:05.580]   There you go.
[02:22:05.580 --> 02:22:09.180]   Thanks for giving me the entree into that, Jeff.
[02:22:09.180 --> 02:22:10.020]   'Cause it's important,
[02:22:10.020 --> 02:22:11.220]   'cause it's also very important
[02:22:11.220 --> 02:22:13.140]   what we do here at Twit.
[02:22:13.140 --> 02:22:14.100]   It's become a real important--
[02:22:14.100 --> 02:22:15.500]   - Will you watch or listen?
[02:22:15.500 --> 02:22:16.340]   - Love us.
[02:22:16.340 --> 02:22:17.180]   There you go.
[02:22:17.180 --> 02:22:18.020]   There you go.
[02:22:18.020 --> 02:22:18.860]   And yeah, exactly.
[02:22:18.860 --> 02:22:20.340]   You can watch us if you like or you can listen.
[02:22:20.340 --> 02:22:22.100]   We don't care.
[02:22:22.100 --> 02:22:24.060]   Either way, we appreciate you.
[02:22:25.340 --> 02:22:28.060]   - And speaking of club twit, there's Ann Pruitt.
[02:22:28.060 --> 02:22:30.180]   Ann Pruitt, what do you have this week?
[02:22:30.180 --> 02:22:34.100]   - Well, again, I forgot about a pick of the week,
[02:22:34.100 --> 02:22:38.300]   but I remembered speaking with my hearthead this weekend
[02:22:38.300 --> 02:22:40.260]   and how jealous I am of him
[02:22:40.260 --> 02:22:45.220]   up at Pacific University in Forest Grove, Oregon.
[02:22:45.220 --> 02:22:48.820]   And this was their luau weekend
[02:22:48.820 --> 02:22:51.340]   at the university.
[02:22:51.340 --> 02:22:53.620]   But they actually live stream it
[02:22:53.620 --> 02:22:57.180]   and I'm freaking loved it.
[02:22:57.180 --> 02:22:58.260]   It's three hours.
[02:22:58.260 --> 02:23:02.620]   And then I'm speaking to Stacy here in the show.
[02:23:02.620 --> 02:23:05.220]   I wonder if this is considered a documentary
[02:23:05.220 --> 02:23:08.820]   because what they're doing is all of the students
[02:23:08.820 --> 02:23:10.340]   are putting on the show
[02:23:10.340 --> 02:23:13.860]   and going through all of the different Polynesian cultures
[02:23:13.860 --> 02:23:17.780]   and sharing information and tidbits about those cultures
[02:23:17.780 --> 02:23:20.340]   and doing the different dances.
[02:23:20.340 --> 02:23:23.900]   And it was so informative and so entertaining
[02:23:23.900 --> 02:23:26.860]   and it's all the students doing this.
[02:23:26.860 --> 02:23:29.580]   And there's some adults in there leading the way with things,
[02:23:29.580 --> 02:23:33.020]   but this is mostly his classmates up there.
[02:23:33.020 --> 02:23:37.340]   And I told him, like, man, you are so fortunate to be exposed
[02:23:37.340 --> 02:23:38.660]   to all of these different cultures
[02:23:38.660 --> 02:23:39.900]   and learn about this stuff.
[02:23:39.900 --> 02:23:42.820]   And he's like, yeah, it's crazy to look down and see that
[02:23:42.820 --> 02:23:44.340]   a lot of these people are my teammates
[02:23:44.340 --> 02:23:45.860]   that I hang out with all the time.
[02:23:45.860 --> 02:23:48.380]   And he's just been learning so much stuff.
[02:23:48.380 --> 02:23:50.780]   And it made me think about my childhood
[02:23:50.780 --> 02:23:54.340]   and growing up in South Carolina and the stuff that
[02:23:54.340 --> 02:23:58.260]   I didn't get to learn because I wasn't as opposed to it.
[02:23:58.260 --> 02:24:00.620]   Or even thinking about the fact that
[02:24:00.620 --> 02:24:05.420]   some things were presented pretty daggum poorly,
[02:24:05.420 --> 02:24:07.060]   like Charleston, South Carolina.
[02:24:07.060 --> 02:24:08.620]   I remember this specifically.
[02:24:08.620 --> 02:24:11.700]   I just spoke about this with my mother.
[02:24:11.700 --> 02:24:14.740]   Charleston, South Carolina is pretty historic.
[02:24:14.740 --> 02:24:17.940]   And there's a spot there right there at the beach
[02:24:17.940 --> 02:24:19.540]   called the Battery.
[02:24:19.540 --> 02:24:24.020]   And the Battery was just celebrated when I was growing up
[02:24:24.020 --> 02:24:26.940]   because of the Battle of the Confederacy and all of that stuff.
[02:24:26.940 --> 02:24:29.900]   But that's not how it was pitched to me as a kid.
[02:24:29.900 --> 02:24:33.860]   It was just the Battery is celebrated as beautiful landmark.
[02:24:33.860 --> 02:24:36.100]   And you get to see this and you get to see that.
[02:24:36.100 --> 02:24:39.220]   And I went down there as an adult in my 20s,
[02:24:39.220 --> 02:24:41.340]   'cause I hadn't seen it as a kid,
[02:24:41.340 --> 02:24:43.460]   went down there as an adult in my 20s.
[02:24:43.460 --> 02:24:46.500]   And I was like, holy s***, this is where the slaves were.
[02:24:47.700 --> 02:24:50.260]   You know, why wasn't that brought up?
[02:24:50.260 --> 02:24:51.100]   Yeah, it's cool.
[02:24:51.100 --> 02:24:53.500]   It was celebrated.
[02:24:53.500 --> 02:24:55.900]   You know, and talking to my son,
[02:24:55.900 --> 02:24:57.700]   I'm like, you know, you're getting exposure
[02:24:57.700 --> 02:24:59.340]   to so much more information.
[02:24:59.340 --> 02:25:01.900]   You're learning so much more about the world.
[02:25:01.900 --> 02:25:02.980]   I'm jealous of you, man.
[02:25:02.980 --> 02:25:05.100]   So keep doing what you're doing up there.
[02:25:05.100 --> 02:25:06.980]   But anyway, shout out to Pacific University
[02:25:06.980 --> 02:25:11.020]   for what they're doing and educating my heart hit.
[02:25:11.020 --> 02:25:13.060]   - That's really great.
[02:25:13.060 --> 02:25:16.260]   - Wow, I can't believe that your kids
[02:25:16.260 --> 02:25:18.780]   in university.
[02:25:18.780 --> 02:25:24.140]   - Yep, time goes so fast.
[02:25:24.140 --> 02:25:25.380]   - One more to go.
[02:25:25.380 --> 02:25:26.620]   That's great to see. - One more to go.
[02:25:26.620 --> 02:25:27.460]   - Yeah.
[02:25:27.460 --> 02:25:29.820]   Wow, that's really cool.
[02:25:29.820 --> 02:25:31.220]   Thank you for sharing that.
[02:25:31.220 --> 02:25:36.300]   My pick of the week is very low tech.
[02:25:36.300 --> 02:25:37.780]   I was like, what am I gonna do is my pick?
[02:25:37.780 --> 02:25:40.300]   And then I thought about the last couple of weeks
[02:25:40.300 --> 02:25:44.060]   and I realized something gave me a lot of joy on this trip.
[02:25:44.060 --> 02:25:48.220]   - And it's a deck of cards.
[02:25:48.220 --> 02:25:50.220]   Because while I was in Costa Rica,
[02:25:50.220 --> 02:25:52.340]   we were going out the dinner
[02:25:52.340 --> 02:25:54.780]   and we would go out to eat and everything.
[02:25:54.780 --> 02:25:59.500]   My older daughter, she's 13, she now has her first phone.
[02:25:59.500 --> 02:26:01.340]   And so we're trying to teach her, you know,
[02:26:01.340 --> 02:26:04.220]   like the rights and the wrongs, you know,
[02:26:04.220 --> 02:26:05.980]   at least through our lens anyways, you know,
[02:26:05.980 --> 02:26:07.780]   the etiquette and that sort of stuff.
[02:26:07.780 --> 02:26:09.300]   And there were just a couple of meals
[02:26:09.300 --> 02:26:11.700]   where we were at the table and she's picking up the phone,
[02:26:11.700 --> 02:26:14.060]   you know, looking for our friends sending her messages
[02:26:14.060 --> 02:26:15.500]   and we were kind of doing the same.
[02:26:15.500 --> 02:26:18.300]   - Well, she knew a lot of roaming down there.
[02:26:18.300 --> 02:26:21.500]   - Yeah, that took a little figuring out.
[02:26:21.500 --> 02:26:23.180]   And at one point, I was just like,
[02:26:23.180 --> 02:26:24.700]   you know what I wanna do?
[02:26:24.700 --> 02:26:26.220]   I wanna find a deck of cards
[02:26:26.220 --> 02:26:28.260]   'cause we forgot to bring a deck of cards.
[02:26:28.260 --> 02:26:30.580]   And so, you know, I went to his route driver,
[02:26:30.580 --> 02:26:32.100]   I don't have a screwdriver, but dang it,
[02:26:32.100 --> 02:26:35.180]   I got a deck of cards in a plastic case.
[02:26:35.180 --> 02:26:38.980]   And it cost me 450 colones, which is,
[02:26:38.980 --> 02:26:41.220]   well, no, sorry, no, that's $4.50.
[02:26:41.220 --> 02:26:44.300]   Nevermind, because it's such a souvenir shop
[02:26:44.300 --> 02:26:46.900]   that it was in dollars, even though that's not a period.
[02:26:46.900 --> 02:26:48.460]   That's a comma.
[02:26:48.460 --> 02:26:52.540]   Anyways, $4.50 for these tiny cards, right?
[02:26:52.540 --> 02:26:54.740]   Which is a lot to spend on.
[02:26:54.740 --> 02:26:56.500]   - You can get them for free if you crash a wedding.
[02:26:56.500 --> 02:26:57.460]   - Oh, totally.
[02:26:57.460 --> 02:27:00.340]   I mean, I paid the total sucker tax, you know?
[02:27:00.340 --> 02:27:03.580]   I was a total tourist with these deck of cards.
[02:27:03.580 --> 02:27:05.900]   I put them in my bag and everywhere we went,
[02:27:05.900 --> 02:27:10.460]   I had a deck of cards and we had so much fun playing cards.
[02:27:10.460 --> 02:27:12.460]   Like when we were eating meals and stuff.
[02:27:12.460 --> 02:27:15.300]   And suddenly the meals just became, you know,
[02:27:15.300 --> 02:27:17.820]   even more enjoyable, something that we looked forward to,
[02:27:17.820 --> 02:27:20.540]   a game that we like to play called Garbage.
[02:27:20.540 --> 02:27:24.140]   And anyways, it's just a good reminder.
[02:27:24.140 --> 02:27:26.140]   You got a deck of cards in your bag.
[02:27:26.140 --> 02:27:27.900]   It might save you from pulling out your phone
[02:27:27.900 --> 02:27:29.140]   when you go eat with your family
[02:27:29.140 --> 02:27:30.700]   or you with your friends or whatever.
[02:27:30.700 --> 02:27:33.020]   Who doesn't love playing cards?
[02:27:33.020 --> 02:27:33.860]   So there you go.
[02:27:33.860 --> 02:27:38.980]   - So if you have younger kids who cards is too much for,
[02:27:38.980 --> 02:27:40.020]   might I recommend?
[02:27:40.020 --> 02:27:41.340]   There's a game called Spot It.
[02:27:41.340 --> 02:27:42.820]   We actually still have it in the world.
[02:27:42.820 --> 02:27:43.660]   - Oh yeah, love it.
[02:27:43.660 --> 02:27:44.500]   - Enough.
[02:27:44.500 --> 02:27:46.540]   If you travel though with it, it comes in a tin,
[02:27:46.540 --> 02:27:47.460]   don't bring the tin.
[02:27:47.460 --> 02:27:48.820]   Your port hates looking at that thing.
[02:27:48.820 --> 02:27:50.140]   It will get you flagged every time,
[02:27:50.140 --> 02:27:53.140]   but just these cards, basically it's like,
[02:27:53.140 --> 02:27:56.100]   what's different in the cards and you slam onto the table.
[02:27:56.100 --> 02:27:59.460]   So again, not great for like fancy restaurants,
[02:27:59.460 --> 02:28:02.420]   but kids, adults love it, great game.
[02:28:02.420 --> 02:28:03.820]   - Oh, it's a fantastic game.
[02:28:03.820 --> 02:28:06.180]   We play that with the girls all the time.
[02:28:06.180 --> 02:28:07.540]   We still, when we go camping,
[02:28:07.540 --> 02:28:11.260]   we have a camping themed deck of Spot It
[02:28:11.260 --> 02:28:14.460]   that always comes with us and that's kind of our go-to.
[02:28:14.460 --> 02:28:18.060]   Every card has like six or seven symbols on it.
[02:28:18.060 --> 02:28:21.060]   And but every card only has one symbol
[02:28:21.060 --> 02:28:24.660]   that's the same between two of them that you pull out, right?
[02:28:24.660 --> 02:28:27.900]   So, or all of them that they get pulled out,
[02:28:27.900 --> 02:28:30.860]   you're basically trying to spot the one thing
[02:28:30.860 --> 02:28:32.820]   that you have similar between the two cards
[02:28:32.820 --> 02:28:33.660]   that are showing.
[02:28:33.660 --> 02:28:37.100]   And it's surprisingly difficult, but it's a lot of fun.
[02:28:37.100 --> 02:28:37.940]   - Great for kids.
[02:28:37.940 --> 02:28:40.460]   - And kids are, I think kids are better at it than adults.
[02:28:40.460 --> 02:28:43.020]   So it's a game that really even spods.
[02:28:43.020 --> 02:28:45.980]   - Totally, totally, I totally agree.
[02:28:45.980 --> 02:28:46.860]   Yeah, it's a lot of fun.
[02:28:46.860 --> 02:28:49.380]   So anyways, the value of a good deck of cards,
[02:28:49.380 --> 02:28:50.940]   it doesn't take up much space.
[02:28:50.940 --> 02:28:54.500]   You know, it's even smaller than a cell, than a phone.
[02:28:54.500 --> 02:28:57.340]   - I keep one in my travel bag along with my Scrooge.
[02:28:57.340 --> 02:28:58.180]   - Oh geez.
[02:28:58.180 --> 02:28:59.020]   (laughing)
[02:28:59.020 --> 02:28:59.860]   - I know.
[02:28:59.860 --> 02:29:00.700]   - Isn't it in there?
[02:29:00.700 --> 02:29:02.020]   (laughing)
[02:29:02.020 --> 02:29:03.580]   Yeah, I gotta find a Scrooge bag.
[02:29:03.580 --> 02:29:05.220]   - You don't go to the structure,
[02:29:05.220 --> 02:29:06.260]   probably a coffee maker,
[02:29:06.260 --> 02:29:08.780]   probably you set up a home pub
[02:29:08.780 --> 02:29:10.700]   in every hotel room you go into.
[02:29:10.700 --> 02:29:12.180]   - There is an AeroPress,
[02:29:12.180 --> 02:29:14.420]   but when I was drinking coffee hardcore,
[02:29:14.420 --> 02:29:16.900]   I did carry one of those actually with me.
[02:29:16.900 --> 02:29:17.900]   - Oh, Stacy.
[02:29:17.900 --> 02:29:18.740]   - I respect that.
[02:29:18.740 --> 02:29:20.300]   - I respect that.
[02:29:20.300 --> 02:29:21.820]   - I respect that.
[02:29:21.820 --> 02:29:24.820]   Yes, AeroPress is good coffee.
[02:29:24.820 --> 02:29:26.420]   - I'm Boogie, man.
[02:29:26.420 --> 02:29:29.220]   I've gotta have my comforts when I travel.
[02:29:29.220 --> 02:29:30.060]   - You are.
[02:29:30.060 --> 02:29:32.500]   - I think that Lisa, CEO to it, Lisa,
[02:29:32.500 --> 02:29:34.780]   before this trip was talking about how she, you know,
[02:29:34.780 --> 02:29:35.900]   had to bring her coffee.
[02:29:35.900 --> 02:29:38.300]   I just don't trust that there's gonna be good coffee
[02:29:38.300 --> 02:29:39.420]   when I go places.
[02:29:39.420 --> 02:29:40.620]   - There never is.
[02:29:40.620 --> 02:29:43.220]   You have to go out of the hotel to get it.
[02:29:43.220 --> 02:29:44.980]   There never is good coffee in hotels.
[02:29:44.980 --> 02:29:46.260]   Yeah, I totally respect that.
[02:29:46.260 --> 02:29:47.100]   - Here's a question.
[02:29:47.100 --> 02:29:50.220]   If you don't trust the water in a given destination,
[02:29:50.220 --> 02:29:53.020]   is coffee hot enough to be then okay?
[02:29:53.020 --> 02:29:54.620]   That's a good question.
[02:29:54.620 --> 02:29:56.260]   - I think it's more than you can provide.
[02:29:56.260 --> 02:29:57.660]   - I mean, it's more than you can provide the trust water.
[02:29:57.660 --> 02:29:58.780]   - Yeah, I trust the water.
[02:29:58.780 --> 02:30:01.060]   It's just like, does it taste bad?
[02:30:01.060 --> 02:30:02.780]   And if it tastes bad, you can hide that with me.
[02:30:02.780 --> 02:30:04.940]   - Oh, some places are not to be trust.
[02:30:04.940 --> 02:30:07.180]   - Well, yeah, if I'm, but those places,
[02:30:07.180 --> 02:30:08.340]   I wouldn't drink the water anyway.
[02:30:08.340 --> 02:30:10.700]   You just get up, you get bottled water.
[02:30:10.700 --> 02:30:13.700]   - Right, otherwise, the other coffee is safe then.
[02:30:13.700 --> 02:30:15.820]   - Oh, no, no, if it's made with water.
[02:30:15.820 --> 02:30:16.900]   - The water's bad, then coffee's bad.
[02:30:16.900 --> 02:30:17.900]   Okay, that's right.
[02:30:17.900 --> 02:30:21.220]   - Like, rinsing fruit with bad water will also get you sick.
[02:30:21.220 --> 02:30:23.340]   So if you're in a place where you're really questioning
[02:30:23.340 --> 02:30:26.900]   the water, then you've got a whole 'nother problem.
[02:30:26.900 --> 02:30:28.860]   - Yeah, a lot more complications.
[02:30:28.860 --> 02:30:30.100]   - But it's your birthday, so we don't want to talk
[02:30:30.100 --> 02:30:31.060]   about diarrhea right now.
[02:30:31.060 --> 02:30:33.740]   We want to say, "Happy birthday's happy birthday."
[02:30:33.740 --> 02:30:35.900]   That's what we actually want to say, yes.
[02:30:35.900 --> 02:30:38.140]   Not the other thing that judges said.
[02:30:38.140 --> 02:30:39.180]   Forget that he said that.
[02:30:39.180 --> 02:30:41.780]   - Happy birthday, Stacey.
[02:30:41.780 --> 02:30:44.620]   - I hope you have a wonderful dinner tonight
[02:30:44.620 --> 02:30:46.260]   and a wonderful celebration.
[02:30:46.260 --> 02:30:49.020]   I'm happy and honored to be able to do a show
[02:30:49.020 --> 02:30:50.180]   on your birthday.
[02:30:50.180 --> 02:30:51.260]   What do you want to leave people with?
[02:30:51.260 --> 02:30:53.780]   Stacey on IOT.com, anything that you're working on,
[02:30:53.780 --> 02:30:54.780]   anything like that?
[02:30:54.780 --> 02:30:58.380]   - Just Stacey on IOT.com, or you know what, y'all?
[02:30:58.380 --> 02:31:00.020]   Just be nice to each other, okay?
[02:31:00.020 --> 02:31:01.620]   Just go say something nice to somebody,
[02:31:01.620 --> 02:31:02.460]   make them feel good.
[02:31:02.460 --> 02:31:03.300]   - Word.
[02:31:03.300 --> 02:31:06.180]   - It's hard to follow that act with a book plug, geez.
[02:31:06.180 --> 02:31:07.660]   (laughing)
[02:31:07.660 --> 02:31:08.980]   But, bye, Golly, you're gonna do it.
[02:31:08.980 --> 02:31:10.340]   - Be nice to see you.
[02:31:10.340 --> 02:31:11.340]   - Yes, I am.
[02:31:11.340 --> 02:31:14.700]   GoodnbergBreathises.com, 10% off from Bloomsbury,
[02:31:14.700 --> 02:31:19.500]   even cheaper from Blackwells.
[02:31:19.500 --> 02:31:22.500]   And you can pre-order my magazine, Book Two,
[02:31:22.500 --> 02:31:23.900]   which is below the bottom of that page,
[02:31:23.900 --> 02:31:25.180]   so it'll be out in June,
[02:31:25.180 --> 02:31:27.340]   and I'm gonna keep talking about it from now through them.
[02:31:27.340 --> 02:31:29.140]   - Heck yeah, as you should.
[02:31:29.140 --> 02:31:30.780]   That's awesome, congratulations.
[02:31:30.780 --> 02:31:32.980]   - If you want to be kind to Jeff,
[02:31:32.980 --> 02:31:33.820]   you could pre-order.
[02:31:33.820 --> 02:31:36.420]   - There you go, yes, thank you, Stacey.
[02:31:36.420 --> 02:31:40.180]   For your birthday, Stacey,
[02:31:40.180 --> 02:31:42.980]   we all pre-ordered Jeff's book.
[02:31:42.980 --> 02:31:45.380]   (laughing)
[02:31:45.380 --> 02:31:46.700]   - Excellent.
[02:31:46.700 --> 02:31:49.420]   - There we go, everybody wins,
[02:31:49.420 --> 02:31:51.260]   including you, Aunt Pruitt,
[02:31:51.260 --> 02:31:54.140]   and always a pleasure to get to do a show with you.
[02:31:54.140 --> 02:31:56.340]   And, this is actually a special week,
[02:31:56.340 --> 02:31:59.100]   'cause I get to do another show with you this Sunday.
[02:31:59.100 --> 02:32:00.740]   I'm hosting Twit,
[02:32:00.740 --> 02:32:02.820]   and you're gonna be on the panel with me,
[02:32:02.820 --> 02:32:04.580]   so I appreciate that, thank you, man.
[02:32:04.580 --> 02:32:05.580]   - Yes, sir, yes, sir.
[02:32:05.580 --> 02:32:06.980]   Appreciate you having me on.
[02:32:06.980 --> 02:32:09.380]   - Yeah, looking forward to it.
[02:32:09.380 --> 02:32:10.380]   What do you want to leave people with?
[02:32:10.380 --> 02:32:12.860]   Hands-on photography, Twit.tv/hop, anything else?
[02:32:12.860 --> 02:32:14.700]   - Oh yeah, check out the show,
[02:32:14.700 --> 02:32:19.700]   twit.tv/hop, for hands-on photography.
[02:32:19.700 --> 02:32:21.700]   Gonna have a guest on this week,
[02:32:21.700 --> 02:32:24.020]   looking forward to you all checking him out,
[02:32:24.020 --> 02:32:26.380]   and sharing that show out with other folks
[02:32:26.380 --> 02:32:28.380]   so we can continue to grow it.
[02:32:28.380 --> 02:32:32.380]   And yeah, go look at auntprout.com/prince to a show.
[02:32:32.820 --> 02:32:35.060]   On the lower third, thank you very much.
[02:32:35.060 --> 02:32:37.100]   Trying to pay for a Pacific University.
[02:32:37.100 --> 02:32:38.980]   - Yeah, I understand.
[02:32:38.980 --> 02:32:39.900]   @prout.com.
[02:32:39.900 --> 02:32:42.260]   - Those Polynesian dances don't come cheap.
[02:32:42.260 --> 02:32:43.100]   - It's true.
[02:32:43.100 --> 02:32:45.220]   (both laughing)
[02:32:45.220 --> 02:32:46.060]   Stacey Jeff dance.
[02:32:46.060 --> 02:32:47.180]   - It looks high quality.
[02:32:47.180 --> 02:32:48.300]   - Thank you so much.
[02:32:48.300 --> 02:32:49.140]   - Good work.
[02:32:49.140 --> 02:32:50.340]   - This is a lot of fun.
[02:32:50.340 --> 02:32:53.460]   Always a lot of fun, and we get to do it again next week.
[02:32:53.460 --> 02:32:56.380]   I will be back, Leo returns,
[02:32:56.380 --> 02:32:58.060]   and I think a week and a half at this point,
[02:32:58.060 --> 02:33:00.620]   so basically not this Sunday,
[02:33:00.620 --> 02:33:02.220]   but the following Sunday, he'll be back
[02:33:02.220 --> 02:33:04.580]   in the meantime, Mike Asargent and I,
[02:33:04.580 --> 02:33:09.580]   and we're all kind of filling in Leo's shoes while he's out.
[02:33:09.580 --> 02:33:13.420]   So I will be back next week in Leo's spot here on Twig.
[02:33:13.420 --> 02:33:15.580]   If you wanna find all the details about this show,
[02:33:15.580 --> 02:33:19.780]   all you gotta do is go to the website twit.tv/twig.
[02:33:19.780 --> 02:33:23.700]   There you will find all the ways to subscribe to this show.
[02:33:23.700 --> 02:33:26.140]   You'll see there that we record live every Wednesday,
[02:33:26.140 --> 02:33:29.420]   5 p.m. Eastern, 2 p.m. Pacific.
[02:33:29.420 --> 02:33:31.340]   That's 2100 UTC.
[02:33:31.340 --> 02:33:34.540]   And if you wanna watch live, you can twit.tv/live.
[02:33:34.540 --> 02:33:35.740]   Be part of the Discord.
[02:33:35.740 --> 02:33:37.620]   If you're a club twit member,
[02:33:37.620 --> 02:33:40.260]   part of the IRC chat during the show,
[02:33:40.260 --> 02:33:42.580]   it's just kind of an extra added bonus
[02:33:42.580 --> 02:33:45.140]   to the experience of this week in Google.
[02:33:45.140 --> 02:33:49.180]   Thank you so much, and we will see you next time on Twig.
[02:33:49.180 --> 02:33:50.020]   Bye everybody.
[02:33:50.020 --> 02:33:54.580]   - Hey, I know you're super busy, so I won't keep you long,
[02:33:54.580 --> 02:33:56.860]   but I wanted to tell you about a show here
[02:33:56.860 --> 02:34:00.420]   on the Twit Network called Tech News Weekly.
[02:34:00.420 --> 02:34:03.180]   You are a busy person, and during your week,
[02:34:03.180 --> 02:34:06.140]   you may want to learn about all the tech news
[02:34:06.140 --> 02:34:09.900]   that's fit to, well, say, not print here on Twit.
[02:34:09.900 --> 02:34:12.340]   It's Tech News Weekly, me, microsurgeon,
[02:34:12.340 --> 02:34:14.220]   my co-host, Jason Howell.
[02:34:14.220 --> 02:34:17.020]   We talk to and about the people making
[02:34:17.020 --> 02:34:18.500]   and breaking the tech news,
[02:34:18.500 --> 02:34:22.060]   and we love the opportunity to get to share those stories
[02:34:22.060 --> 02:34:24.660]   with you and let the people who wrote them
[02:34:24.660 --> 02:34:26.580]   or broke them share them as well.
[02:34:26.580 --> 02:34:29.060]   So I hope you check it out every Thursday
[02:34:29.060 --> 02:34:30.020]   right here on Twit.
[02:34:30.020 --> 02:34:32.620]   (upbeat music)
[02:34:32.620 --> 02:34:40.620]   (upbeat music)

