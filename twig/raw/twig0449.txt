;FFMETADATA1
title=Grackles, Nuthatches, and Swifts, Oh My!
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=449
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.800]   It's time for Twig this week at Google Jeff Jarvis is here Stacy Higginbotham
[00:00:03.800 --> 00:00:08.280]   We'll talk about the self-driving car from Uber is at the end of the line
[00:00:08.280 --> 00:00:13.940]   We'll talk about Facebook it is the end of the line and Congress has done a shameful thing
[00:00:13.940 --> 00:00:16.680]   It's all coming up next are you surprised on twig?
[00:00:16.680 --> 00:00:23.360]   Netcast you love from people you trust
[00:00:23.360 --> 00:00:28.800]   This is twig
[00:00:29.520 --> 00:00:35.900]   Bandwidth for this week in Google is provided by cash fly C A C A G F L Y
[00:00:35.900 --> 00:00:36.900]   .com
[00:00:36.900 --> 00:00:43.640]   This is Twig this week in Google episode
[00:00:43.640 --> 00:00:47.600]   449 recorded Wednesday March 21st 2018
[00:00:47.600 --> 00:00:51.680]   Grackles nut hatches and swifts. Oh my
[00:00:51.680 --> 00:00:58.320]   This week in Google is brought to you by word press reach more customers when you build your business website on
[00:00:58.320 --> 00:01:06.200]   Wordpress.com plans start at just four dollars a month get 15% off any new plan at wordpress.com/twig
[00:01:06.200 --> 00:01:14.120]   It's time for twig this week in Google although we should call it Twif this week or Twica
[00:01:14.120 --> 00:01:18.440]   With us Jeff Jarvis City University New York professor of journalism
[00:01:18.440 --> 00:01:23.520]   Buzz machine.com author of what would Google do public parts geek sparing gifts
[00:01:24.160 --> 00:01:29.240]   First day of spring first day of full full day of the vernal equinox in the snow
[00:01:29.240 --> 00:01:33.020]   Came a tumbling down you got Toby out your window
[00:01:33.020 --> 00:01:38.640]   It's it's snowing hello. You want to see out my window. Yeah, let's see
[00:01:38.640 --> 00:01:43.920]   Austin where oh
[00:01:43.920 --> 00:01:48.120]   Handy know you looked out at that beautiful view while you're doing the show
[00:01:48.120 --> 00:01:50.800]   It is no wonder you can tolerate us
[00:01:51.120 --> 00:01:55.000]   Wow, do you have those those those those strange little Austin birds on that tree?
[00:01:55.000 --> 00:01:57.680]   the crackles
[00:01:57.680 --> 00:01:59.200]   Yeah
[00:01:59.200 --> 00:02:04.320]   Strange for their name only. I mean I thought they're they're ginormous creepy evil birds
[00:02:04.320 --> 00:02:09.160]   I don't know there's a little ones the little ones lots and lots of little ones. Oh the little sparrow type birds
[00:02:09.160 --> 00:02:14.320]   Yeah, the swift nut hatches switch. I don't know tiny little birds. I don't get birds
[00:02:14.320 --> 00:02:19.680]   I'm just gonna make this up. I do have birds crackles and nudge has is and swift. Oh my
[00:02:20.360 --> 00:02:22.880]   It's crackles and not hatches and swift
[00:02:22.880 --> 00:02:27.300]   Well well well well did anything happen this weekend?
[00:02:27.300 --> 00:02:34.440]   Nothing's anything nothing. It's been really boring nothing's happened. Everybody's been happy good. No, look at the future
[00:02:34.440 --> 00:02:39.880]   Yeah, yeah, it's a new world. You know it's a good it's a good new world and it's okay
[00:02:39.880 --> 00:02:41.960]   It's fun
[00:02:41.960 --> 00:02:48.960]   I want to do a countdown actually let's start this show with let's do a countdown until how long Jeff says techno panic
[00:02:48.960 --> 00:02:50.960]   I'm pretty sure we're gonna hit it
[00:02:50.960 --> 00:02:53.280]   To it a bunch of goes in moral panic
[00:02:53.280 --> 00:03:01.880]   So there was a very good New York Times the daily yesterday it was actually a
[00:03:01.880 --> 00:03:05.520]   Observer guardian to the game of the New York Times. So the way you're talking about
[00:03:05.520 --> 00:03:11.840]   Well, it was in their podcast. So it was the daily but it may have but yes
[00:03:11.840 --> 00:03:14.320]   It's observer had the had the story of course
[00:03:14.320 --> 00:03:20.080]   Yeah, I broke the story and it was a whistleblower Christopher Wiley from Cambridge Analytica who went to the observer
[00:03:20.080 --> 00:03:26.160]   but I thought a very good synopsis of this which I will kind of attempt to rehash because the I
[00:03:26.160 --> 00:03:32.680]   Think before we even talk about Facebook's culpability in all this it would be good to understand what exactly
[00:03:32.680 --> 00:03:38.560]   Yeah, a lot of the coverage is way wrong and and it doesn't make a lot of sense for Facebook to parse
[00:03:38.560 --> 00:03:42.360]   Breach versus not breach, but it does make sense for us to get the facts right. Yeah
[00:03:42.680 --> 00:03:47.680]   So this began Christopher Wiley will start with him. He was the whistleblower
[00:03:47.680 --> 00:03:50.640]   He was a high school dropout from PC from Canada
[00:03:50.640 --> 00:03:53.040]   Who became fascinated with
[00:03:53.040 --> 00:03:55.860]   Data big data when he was
[00:03:55.860 --> 00:03:59.780]   volunteering at the Obama campaign in 2012 and
[00:03:59.780 --> 00:04:03.880]   Was very intrigued by what data was tell them
[00:04:03.880 --> 00:04:08.160]   About reaching out to voters, you know how to advertise to them
[00:04:08.160 --> 00:04:11.320]   Who to who to advertise to and all of that stuff?
[00:04:12.440 --> 00:04:17.520]   He had was following an interesting study
[00:04:17.520 --> 00:04:21.800]   And I'm trying to remember if that was from Cambridge or somewhere else. I think not
[00:04:21.800 --> 00:04:23.440]   where
[00:04:23.440 --> 00:04:27.480]   Some researchers found out that if we get 50 of your likes 50
[00:04:27.480 --> 00:04:34.960]   So who's 50 of your likes we can tell whether what race you are we can tell what kind of education you have to tell a lot about you
[00:04:34.960 --> 00:04:36.240]   Give us
[00:04:36.240 --> 00:04:39.560]   500 of your likes we'll know more about you than your spouse does
[00:04:40.520 --> 00:04:45.920]   And they were able to do that by correlating kind of it was a kind of imperfect system
[00:04:45.920 --> 00:04:47.840]   correlating
[00:04:47.840 --> 00:04:51.280]   You can likes were scrapable. I think they still are scrapable from Facebook
[00:04:51.280 --> 00:04:54.240]   You don't have to have permission to see what people like it's in there
[00:04:54.240 --> 00:04:56.640]   It's right there on their on their profile
[00:04:56.640 --> 00:05:02.040]   So they would they would basically get as much of that like data as they could and see if they could find correlations
[00:05:02.040 --> 00:05:07.600]   And this is very different from you or me. I mean one of the things they found out for instance if if you like hello kitty
[00:05:09.200 --> 00:05:13.440]   What what what are they I don't even remember but it said something about you was that you were
[00:05:13.440 --> 00:05:18.040]   You were a nice person, but not a rule follower something like that
[00:05:18.040 --> 00:05:24.680]   So you could deduce all of these things and they did this again by taking great lumps of data and making correlations
[00:05:24.680 --> 00:05:25.760]   and then
[00:05:25.760 --> 00:05:30.520]   Projecting out while he was fascinated by this thought this was very interesting started doing some of this
[00:05:30.520 --> 00:05:35.560]   Himself for the Obama campaign and then went to work for a British company
[00:05:35.960 --> 00:05:42.400]   Run a part in a group in the British company ran run by this was in Andrew nix the
[00:05:42.400 --> 00:05:46.200]   CEO now ousted of Cambridge, and let it go
[00:05:46.200 --> 00:05:51.400]   They realized that they could do a lot more if they could get more access to the Facebook
[00:05:51.400 --> 00:05:56.080]   Data and this is where the story became very interesting
[00:05:56.080 --> 00:05:59.920]   Nix
[00:05:59.920 --> 00:06:01.640]   ran into
[00:06:01.640 --> 00:06:07.600]   somebody from the I think was the the Martin the cruise campaign the Ted Cruz campaign and
[00:06:07.600 --> 00:06:12.960]   Decided that it would be great to get into the American market and
[00:06:12.960 --> 00:06:15.440]   and
[00:06:15.440 --> 00:06:19.760]   Decided that if they could do a little bit better targeting that they might be able to do this kind of targeting
[00:06:19.760 --> 00:06:25.200]   They were already doing in the UK in the American market got the attention of billionaire Robert Mercer who funded it
[00:06:25.200 --> 00:06:30.240]   Steve Bannon who became the chairman and created Cambridge Analytica and
[00:06:30.800 --> 00:06:36.880]   Cambridge Analytica did this is the thing that they did that really has gotten them in trouble with Facebook
[00:06:36.880 --> 00:06:46.400]   Is they found a researcher at Cambridge who created a psychological quiz and they put out on Facebook and offer if you take this quiz
[00:06:46.400 --> 00:06:48.400]   It was a real quiz. It was not a game of thrones
[00:06:48.400 --> 00:06:50.880]   You know which vegetable are you kind of a quiz?
[00:06:50.880 --> 00:06:56.520]   It was a real psychological profile standard kind of profile if you take this quiz you'll have to download an app and take it
[00:06:56.520 --> 00:06:58.120]   offline off Facebook
[00:06:58.120 --> 00:07:03.720]   We'll give you some money will pay you and they got a significant number of people to do that through Facebook
[00:07:03.720 --> 00:07:10.040]   So they had their Facebook login and they had the quiz they were able to use the data they gained from that quiz to
[00:07:10.040 --> 00:07:19.760]   It was about a quarter with 1075,000 people to order 30,000. Yeah to create this what you really need in this a machine learning database in effect
[00:07:19.760 --> 00:07:21.960]   making correlations between
[00:07:21.960 --> 00:07:27.080]   What the Facebook profile told you about this person what their psychological profile was
[00:07:27.080 --> 00:07:28.880]   And
[00:07:28.880 --> 00:07:33.840]   They also got something which Facebook shut down a few years ago which is this friends of friends stuff
[00:07:33.840 --> 00:07:34.880]   Which I always thought was
[00:07:34.880 --> 00:07:38.600]   Reprehensible if you take a quiz or give somebody access to your Facebook account
[00:07:38.600 --> 00:07:43.520]   They not only get your information your likes, but they get all your friends information all your friends likes
[00:07:43.520 --> 00:07:49.320]   Basically, oh, just to clarify her that's that's your public information. They don't get anything behind no no no private
[00:07:49.320 --> 00:07:54.240]   Not credit card numbers is stuff you're posted they get access to their accounts though. They don't have to be
[00:07:54.800 --> 00:08:00.080]   you don't have to have your own relationship at that time you should have to have your own business relationship or
[00:08:00.080 --> 00:08:03.480]   That was how people thought Facebook worked
[00:08:03.480 --> 00:08:08.920]   I mean I think everybody thought Facebook worked is people can see my page if I allow them to you know
[00:08:08.920 --> 00:08:11.760]   If they friend me and I friend them back then then we are sharing data
[00:08:11.760 --> 00:08:16.800]   But as it turns out if they friend you and you friend them back then anybody who friends them can see your data
[00:08:16.800 --> 00:08:20.840]   Which was at the time that was something Facebook stopped a couple of years and by the way
[00:08:20.840 --> 00:08:25.880]   I just just real quickly I worked with lots of media companies that said goody goody and did that like crazy
[00:08:25.880 --> 00:08:30.760]   Obama's campaign had a a game that had a million users and did the same thing
[00:08:30.760 --> 00:08:32.720]   And so it was very common at the time
[00:08:32.720 --> 00:08:38.760]   But it was all part of that kerfuffle that we talked about the show at the time about people not understanding the circles of privacy
[00:08:38.760 --> 00:08:40.760]   We have been talking about this for years
[00:08:40.760 --> 00:08:45.920]   Yes, and trying to raise people's awareness of this for years. So Cambridge Analytica
[00:08:45.920 --> 00:08:50.000]   Got all of this data for it turns out once they got the
[00:08:50.160 --> 00:08:57.440]   275 270 thousand people take the quiz they got their friends of friends and that ended up being a 50 million person database a very
[00:08:57.440 --> 00:09:01.720]   Which should have been handed over to Cambridge Analytica. Well, it's no no no no
[00:09:01.720 --> 00:09:07.640]   It is that they shouldn't have access to it. They got legitimate access to it saying it was a research
[00:09:07.640 --> 00:09:15.920]   The wrong said no the rule said you couldn't pass that on to say a commercial entity
[00:09:15.920 --> 00:09:20.760]   Right and it was passed on and they were told research or who had ties to Russia by the way
[00:09:20.760 --> 00:09:25.040]   Yes was actually passing along to a commercial entity illegally
[00:09:25.040 --> 00:09:30.160]   Facebook against against Facebook against their rule is not illegal a bit against their rules
[00:09:30.160 --> 00:09:32.360]   Yeah, and then Cambridge Analytica used it
[00:09:32.360 --> 00:09:39.360]   apparently quite effectively to target advertising and and you know why they were so so they say
[00:09:39.360 --> 00:09:43.120]   Which is a lot of there's been a lot of question about history
[00:09:43.120 --> 00:09:45.920]   and we've got we've seen now the the videos of
[00:09:45.920 --> 00:09:48.560]   its CEO
[00:09:48.560 --> 00:09:50.200]   admitting to all sorts of
[00:09:50.200 --> 00:09:54.560]   Shikaneary, but again, they were boasting about stuff they probably didn't do in the past
[00:09:54.560 --> 00:10:00.640]   So we there's no way of knowing if they actually do hire prostitutes to subboard candidates things like that
[00:10:00.640 --> 00:10:02.880]   And it's also there's debate among researchers. I was at a
[00:10:02.880 --> 00:10:05.480]   Annenberg event at Penn some weeks ago
[00:10:05.480 --> 00:10:10.680]   Where most of the researchers in the room said that they did not believe for a second that Cambridge Analytica had a
[00:10:11.080 --> 00:10:16.880]   Any of your would near the skills that they say they have again, that's irrelevant to this discussion in so far as
[00:10:16.880 --> 00:10:21.520]   Things that shouldn't have happened happened, but how much impacted then had is
[00:10:21.520 --> 00:10:30.440]   Something that could be up for debate a lot of people say hey Obama did the same thing the only reason this is a hot button is because Trump
[00:10:30.440 --> 00:10:32.920]   Used it to win and not not a Democrat
[00:10:32.920 --> 00:10:35.880]   Fair enough. That's a fair criticism
[00:10:35.880 --> 00:10:41.040]   Facebook said we it was we understand it's a breach of trust and we expected the
[00:10:41.040 --> 00:10:44.160]   This is Mark Zuckerberg's comment earlier
[00:10:44.160 --> 00:10:49.040]   He said we have a responsibility to protect your data if we can't we don't deserve to serve you
[00:10:49.040 --> 00:10:53.200]   I've been working to understand exactly what happened and how to make sure this doesn't happen again
[00:10:53.200 --> 00:10:58.640]   He's gonna be on CNN at 6 p.m. Pacific 9 p.m. Eastern tonight
[00:10:58.640 --> 00:11:04.360]   On the Anderson Cooper. Let's not stay on till then no we'll be off by then so
[00:11:04.360 --> 00:11:10.960]   Cambridge Analytica which was so was first of all was not supposed to have that data and then told Facebook
[00:11:10.960 --> 00:11:14.320]   They had deleted that data apparently did not delete the data and until
[00:11:14.320 --> 00:11:22.240]   Wily stepped forward. We didn't really understand exactly what had happened, but the whistleblower who was you know one of the principles?
[00:11:22.240 --> 00:11:25.160]   explained the whole process
[00:11:25.160 --> 00:11:27.160]   so a
[00:11:27.160 --> 00:11:33.700]   Number of people including Ryan Acton who was one of the founders of what's happened was paid 16 billion dollars along with
[00:11:33.700 --> 00:11:35.200]   Yankum his co-founder
[00:11:35.200 --> 00:11:37.200]   For the app by Facebook
[00:11:37.280 --> 00:11:43.880]   Said it's time hashtag delete Facebook a lot of people that hashtags gotten very popular
[00:11:43.880 --> 00:11:54.040]   I deleted Facebook not so much because I've I mean I've everybody knows all that information. I just don't like
[00:11:54.040 --> 00:11:59.640]   It really brought home to me, you know something. I've been thinking for a long time, which is I
[00:11:59.640 --> 00:12:02.560]   Don't trust them
[00:12:02.560 --> 00:12:07.280]   Now we should mention and I'm sure Jeff will mention that you actually work with Facebook
[00:12:07.280 --> 00:12:12.480]   And the foundation that they do just to be just get the full clear disclosure
[00:12:12.480 --> 00:12:19.120]   I raised $40 million from Facebook Craig Newmark the Ford Foundation at Texas and others to start the news integrity initiative
[00:12:19.120 --> 00:12:26.920]   We run it independently of Facebook and I am not paid by never have been paid by any of the platforms in disclosure. Yeah
[00:12:28.000 --> 00:12:33.040]   The parent company of Cambridge Analytica has also has been fan as has camera camera, Genolitica
[00:12:33.040 --> 00:12:39.840]   For all of this so there's there was clearly a violation of Facebook's terms
[00:12:39.840 --> 00:12:47.160]   And and I think Mark Zuckerberg's taking some culpability he's taking some responsibility for this saying yeah
[00:12:47.160 --> 00:12:49.240]   We we didn't do a great job. Although
[00:12:49.240 --> 00:12:56.240]   There's also the sense that Facebook is as much a victim as the as the end users that Cambridge Analytica
[00:12:56.760 --> 00:12:58.760]   Why do you
[00:12:58.760 --> 00:13:03.600]   You disagree that's like that's like saying Equifax is as much a victim as the end users
[00:13:03.600 --> 00:13:06.160]   I mean, this is a company that's dedicated to collecting data
[00:13:06.160 --> 00:13:13.440]   About people well Facebook's paying the price they lost $50 billion in stock market value
[00:13:13.440 --> 00:13:18.760]   Are they really paying the price? I mean, oh it depends. I mean this is a really interesting
[00:13:18.760 --> 00:13:23.600]   Nexus is kind of a turning point. It could be the end of Facebook. It's some ways, right? What do you think?
[00:13:24.080 --> 00:13:29.280]   Yeah, you know, I I'm not a huge Facebook user. I've never been but I
[00:13:29.280 --> 00:13:38.380]   Wouldn't be I we need to have something like Facebook. Do we do we yeah? I do think we do I think
[00:13:38.380 --> 00:13:46.360]   Next people yeah, but I mean I have a phone and an email and mail I could connect it it connects people superficial
[00:13:46.360 --> 00:13:48.360]   It it makes superficial
[00:13:48.360 --> 00:13:49.760]   connections
[00:13:49.760 --> 00:13:55.520]   Feel it more accessible. So it helps you scale your connections. I would say so
[00:13:55.520 --> 00:14:00.160]   Maybe it's one of its flaws though by the way because they're not genuine connections
[00:14:00.160 --> 00:14:04.160]   Maybe I don't know they I think they can be genuine connections
[00:14:04.160 --> 00:14:07.920]   I mean if you use it well, you can create communities
[00:14:07.920 --> 00:14:12.880]   You just have to you have to curate them in a way that a lot of people aren't very disciplined at I mean
[00:14:12.880 --> 00:14:17.720]   Think about think about how sophisticated people are and different types of people
[00:14:17.720 --> 00:14:19.920]   I don't have a lot of close friends for example
[00:14:19.920 --> 00:14:25.560]   I am not a huge Facebook user because not because of that, but I think those things are correlated and there are lots of people
[00:14:25.560 --> 00:14:28.400]   who
[00:14:28.400 --> 00:14:31.360]   Seek others approval and those people tend to
[00:14:31.360 --> 00:14:36.480]   Be on Facebook and establish I would say to them meaningful relationships
[00:14:36.480 --> 00:14:41.080]   And I'm not gonna denigrate that and I think it's something a lot of people need and want
[00:14:41.080 --> 00:14:46.080]   And so I think there's a real value there. I do think it's abused, but
[00:14:47.400 --> 00:14:53.640]   Those people are also abused in real life by quote unquote friends frenemies people who treat them like so so
[00:14:53.640 --> 00:14:59.280]   You're saying that Facebook had a higher obligation to I mean to protect our data
[00:14:59.280 --> 00:15:03.520]   Certainly they do have a if we're going to give them that data very high obligation to keep it safe
[00:15:03.520 --> 00:15:08.840]   Yeah, after all all this really was this kind of Facebook's business model
[00:15:08.840 --> 00:15:11.400]   But somebody took advantage of it
[00:15:11.400 --> 00:15:15.600]   But this thing can't what what what yes all all true
[00:15:15.600 --> 00:15:19.160]   I mean, I think a couple of observations one and and
[00:15:19.160 --> 00:15:20.840]   Stacey I agree
[00:15:20.840 --> 00:15:24.520]   I think that society needs something like Facebook and as I wrote some weeks ago
[00:15:24.520 --> 00:15:28.640]   I think that their definition of community and their definition connections is still too shallow and
[00:15:28.640 --> 00:15:30.760]   And so I agree with both of you
[00:15:30.760 --> 00:15:34.080]   But the data that was there was data that people shared
[00:15:34.080 --> 00:15:40.000]   Publicly in the definition of public at the time on Facebook. It wasn't as if it was private data that was stolen or breached
[00:15:40.000 --> 00:15:44.760]   That's one point second point is that at the time
[00:15:45.400 --> 00:15:51.440]   It was I would say that first point real quickly it was weaponized and I think that's different. That's different
[00:15:51.440 --> 00:15:54.000]   That's different. Yes, how it was used is absolutely different
[00:15:54.000 --> 00:15:58.920]   But and how was used by those players by Cambridge Analytica or other players was different
[00:15:58.920 --> 00:16:03.600]   Yeah, but people said I like the meat the Beatles and there was but that's a large
[00:16:03.600 --> 00:16:06.120]   But that's a larger story that's going on right now
[00:16:06.120 --> 00:16:11.080]   Which is what we thought was innocuous sharing of information turns out thanks to big data
[00:16:11.800 --> 00:16:18.680]   Massive databases multiple massive databases and very powerful number crunching machine learning to not be as innocuous as we thought it was
[00:16:18.680 --> 00:16:21.680]   You know, it's kind of sharing I do in real life is
[00:16:21.680 --> 00:16:25.400]   Inocuous, but as soon as it gets attached to these big databases
[00:16:25.400 --> 00:16:31.880]   Really, I don't think I don't think we've really seen the full extent of the definition of weaponization nonetheless
[00:16:31.880 --> 00:16:36.520]   Was it misused by Cambridge Analytica? Yes stipulated your honor. Let me know in a second point real quick
[00:16:36.880 --> 00:16:40.320]   Which is that at the time a lot of this was going on
[00:16:40.320 --> 00:16:46.480]   I think it was a general belief and this goes after what Stacy was saying that that having a social media platform and
[00:16:46.480 --> 00:16:49.000]   Using it to involve people
[00:16:49.000 --> 00:16:52.200]   It can get them involved civically and politics was a good thing
[00:16:52.200 --> 00:16:58.320]   Right what changed in that what one is this researcher misuse the data and miss send it to somebody who in turn misused it
[00:16:58.320 --> 00:17:02.080]   And and so on the second thing that happened. Let's be honest was Donald Trump
[00:17:02.080 --> 00:17:04.600]   Because I think that you have to ask yourself
[00:17:05.040 --> 00:17:09.040]   You know do we say now that no political campaign should ever use social media?
[00:17:09.040 --> 00:17:15.280]   I don't think we would have said that three years ago five years away Obama's Facebook was it was it similar to the way
[00:17:15.280 --> 00:17:19.720]   Target was high doesn't it and they had even more they had a million people and
[00:17:19.720 --> 00:17:25.160]   And let's not forget that if you want to start a movement like black lives matter or something else the ability to target
[00:17:25.160 --> 00:17:29.040]   Has benefit targeting is not bad and evil
[00:17:31.000 --> 00:17:36.720]   The sameness of mass media is what some people are trying to defend right now, and I'm trying to say no
[00:17:36.720 --> 00:17:38.720]   That's not the future. That's the past
[00:17:38.720 --> 00:17:43.880]   So bad things happened bad people did bad things. I'm not arguing with all any of that
[00:17:43.880 --> 00:17:47.720]   But that doesn't make the intent or the platform
[00:17:47.720 --> 00:17:53.880]   Vinal and let's not forget the other things that change that we're trying to blame Facebook for that mass media hold a huge
[00:17:54.560 --> 00:18:02.040]   dose of responsibility for like polarization that fertilized this field that allowed Cambridge Analytica and Steve Bannon to come in and
[00:18:02.040 --> 00:18:04.320]   plant their seeds of hatred so
[00:18:04.320 --> 00:18:10.880]   Yeah, I'm disappointed. We get to where was awkward work statement was I'm disappointed that all he did was dealing with the
[00:18:10.880 --> 00:18:18.120]   Tactical details of Cambridge Analytica. There's a much bigger issue here about what the public responsibility of these platforms is
[00:18:18.120 --> 00:18:21.880]   They didn't ask for us. They didn't want it. There's no law about it, but in their power
[00:18:22.200 --> 00:18:29.360]   They have to take our responsibility for a civil informed conversation for for the health of the public sphere
[00:18:29.360 --> 00:18:36.240]   That's the bigger conversation. That's not being held that I wish we should hold that I'll hold Facebook and company to high responsibility
[00:18:36.240 --> 00:18:42.200]   Thank you for doing so along those lines Jeff. Do we need something like?
[00:18:42.200 --> 00:18:51.040]   Can you lie in because part of this is it's targeted, but it also targeted lies designed to get people to
[00:18:51.760 --> 00:18:59.120]   Believe a certain way. So do we need rules and regulations about campaigning that these sorts of things would have to follow that way?
[00:18:59.120 --> 00:19:00.440]   It's not the
[00:19:00.440 --> 00:19:04.480]   platforms responsibility as much as it's the palle Stacey
[00:19:04.480 --> 00:19:11.000]   But every single politician lies in and half their commercials
[00:19:11.000 --> 00:19:15.520]   They lie when they appear on TV. They lie when they're on Twitter
[00:19:15.520 --> 00:19:18.480]   Who's to become the judge of that?
[00:19:19.080 --> 00:19:23.120]   Who's just who's to forbid the lie at the level of First Amendment?
[00:19:23.120 --> 00:19:25.160]   And this is not a first amendment issue because it's a private company
[00:19:25.160 --> 00:19:30.640]   But it's level of First Amendment you have a right to lie you have a you have a constitutional right to lie in this country
[00:19:30.640 --> 00:19:35.880]   You can do it people can think of what they will I think to be lying is not all things the right standard
[00:19:35.880 --> 00:19:42.760]   I think to me it's a matter of scale that stuff like this is always gone on. We've always had targeted advertising people always lied
[00:19:42.760 --> 00:19:44.680]   but I
[00:19:44.680 --> 00:19:46.680]   Keep using that word weaponized
[00:19:46.880 --> 00:19:49.840]   Modern technology has changed the conversation
[00:19:49.840 --> 00:19:56.320]   Because we have so much more data and we have so much better ways to correlate data
[00:19:56.320 --> 00:20:02.320]   You know that it is now to me it strikes me as more dangerous and I think the really the real
[00:20:02.320 --> 00:20:04.160]   the
[00:20:04.160 --> 00:20:06.160]   benefit of this Facebook moment is that
[00:20:06.160 --> 00:20:08.440]   Everybody will start to think a little bit harder
[00:20:08.440 --> 00:20:15.640]   About what all these companies are doing about what information we're giving up and how about how it's being misused
[00:20:15.640 --> 00:20:17.640]   I think you know, I mean it
[00:20:17.640 --> 00:20:25.440]   There's I would say how it's being used period. This is actually what Kevin and I talked about and I think let's
[00:20:25.440 --> 00:20:29.160]   Let's move beyond just talking about this and let's talk about ways to solve for this
[00:20:29.160 --> 00:20:36.440]   Like there are things that we know work things like having data retention policies getting rid of people's data after 30 days
[00:20:36.440 --> 00:20:40.000]   That's like well, I think that's one of the things is gonna happen right?
[00:20:40.000 --> 00:20:44.920]   This that what you're now gonna see is GDPR style regulation in the US it needs to happen
[00:20:44.920 --> 00:20:47.280]   It has to happen because people aren't
[00:20:47.280 --> 00:20:50.440]   We are being asked to give up so much information
[00:20:50.440 --> 00:20:56.720]   We have no insights as users what's gonna happen to that information and how it could be used
[00:20:56.720 --> 00:21:04.940]   We have an amazing array of frightening stories about how it could be used against us and that's turning people off from
[00:21:04.940 --> 00:21:09.200]   life life-saving benefits like like precision medicine or
[00:21:10.800 --> 00:21:16.040]   I'm trying to think of being but that's where you're in agreement with Jeff because that's why you actually do it panic is
[00:21:16.040 --> 00:21:19.320]   This is don't throw the baby out with the bathwater
[00:21:19.320 --> 00:21:24.040]   Right, so that's why I think regulations like GDPR are actually reasonable
[00:21:24.040 --> 00:21:31.360]   They're measured what the challenges with things like GDPR is we don't actually know how we can implement that yet because so far
[00:21:31.360 --> 00:21:35.000]   Our technology that we've developed has all been about
[00:21:35.520 --> 00:21:43.720]   aggregating stuff sifting through it for correlations over a long period of time and pulling new stuff in and now we're gonna have to have a
[00:21:43.720 --> 00:21:45.720]   technology in
[00:21:45.720 --> 00:21:53.000]   No responsibility for it. Now. We need to develop a technology frame for work for pulling in data for a limited amount of time
[00:21:53.000 --> 00:21:56.800]   understanding exactly and communicating back how
[00:21:56.800 --> 00:22:03.040]   Different entities are going to use that data and also give the user the ability to pull that back
[00:22:03.400 --> 00:22:06.840]   that's a really tough technical order and
[00:22:06.840 --> 00:22:10.440]   We don't actually know how to even do that yet. So
[00:22:10.440 --> 00:22:15.040]   It's one thing to be like a little bit GDPR, but we also need to be like
[00:22:15.040 --> 00:22:22.160]   Okay, how are we gonna actually build a tech infrastructure for those rules because we don't have it today?
[00:22:22.160 --> 00:22:26.360]   Amen, amen and what two two sides of that coin
[00:22:26.360 --> 00:22:30.280]   I think that American media companies are gonna American companies in general brands
[00:22:30.800 --> 00:22:33.280]   Everybody are gonna be shocked come May 1
[00:22:33.280 --> 00:22:37.120]   That they're gonna have to consider cutting off Europe until they get their act together
[00:22:37.120 --> 00:22:39.120]   I don't think anyone's taking that seriously
[00:22:39.120 --> 00:22:44.640]   They have like the big companies actually have to
[00:22:44.640 --> 00:22:54.280]   Implement GDPR because they have offices and in any company headquartered in Europe even those that serve Americans will have to buy
[00:22:54.280 --> 00:22:59.720]   a buy by the you made the point before Stacy that's the small companies that can't afford to do that they're gonna be
[00:23:00.440 --> 00:23:02.440]   affected
[00:23:02.440 --> 00:23:07.720]   Yeah, the other side of the coin actually real quick is and I made this point often on the show
[00:23:07.720 --> 00:23:11.440]   But but I believe the future of media is one that gives you greater relevance
[00:23:11.440 --> 00:23:14.480]   It doesn't treat you like everybody else that means I have to have data about you now
[00:23:14.480 --> 00:23:17.280]   I have to have a good relationship with you under that data. I have to have your mission
[00:23:17.280 --> 00:23:19.760]   I have to do right by it. I have to give you value for it
[00:23:19.760 --> 00:23:24.560]   But I even fear now that all targeting is gonna be seen to be
[00:23:24.560 --> 00:23:29.440]   Evil and awful and then we return to a world of everybody having sorry about that
[00:23:30.280 --> 00:23:36.000]   Phone and does you have a phone rings Jeff? It's a it's a flack from Facebook. Just
[00:23:36.000 --> 00:23:41.480]   Oh, they're reaching out now, baby. They're reaching out now. Yeah, yeah, so
[00:23:41.480 --> 00:23:46.720]   Where do we stand on GDPR? I take it Stacy? You think it's reasonable and not
[00:23:46.720 --> 00:23:52.520]   burdensome Jeff. No, I think ideas are good. I don't think we actually can implement it
[00:23:52.520 --> 00:23:54.360]   Well, that's you know, of course
[00:23:54.360 --> 00:23:56.360]   That's the concern is that these rules
[00:23:56.360 --> 00:24:01.480]   I mean we started with right to be forgotten and it looked as if that broke the way the internet works
[00:24:01.480 --> 00:24:05.520]   GDPR maybe isn't as bad, but it has similar features Jeff
[00:24:05.520 --> 00:24:09.720]   Where is coming to no, I think GDPR is the heart is in the right place
[00:24:09.720 --> 00:24:16.840]   But but the implementation is is wrong on a bunch of levels because as we've discussed before and to Stacy's medical point
[00:24:16.840 --> 00:24:19.920]   You can't anticipate every use of data. It's knowledge
[00:24:19.920 --> 00:24:21.240]   And that's the problem
[00:24:21.240 --> 00:24:25.880]   I have both right to be forgotten and this is that is knowledge and I think that there are ways that we can
[00:24:26.440 --> 00:24:30.700]   protect people and inform them and use things well and you know, I
[00:24:30.700 --> 00:24:34.200]   I always go to Amazon
[00:24:34.200 --> 00:24:40.120]   Which tells me why it's recommending something unless we change that right? That's not hard to do we can figure that out
[00:24:40.120 --> 00:24:43.200]   We can let we can tell people what you know about you. We can give you some
[00:24:43.200 --> 00:24:46.080]   I'm gonna sell like a sociologist agency over that
[00:24:46.080 --> 00:24:52.000]   We can we can know what the value is and and I think we've blown it horribly
[00:24:52.360 --> 00:24:57.880]   On the net we being technology companies and media companies and advertising companies in that we
[00:24:57.880 --> 00:25:03.400]   Didn't tell you what we were doing. We didn't tell you why we didn't tell you the benefit you had and now we're we're
[00:25:03.400 --> 00:25:08.880]   paying for that so you know, so this is a come to Jesus moment for the big tech companies
[00:25:08.880 --> 00:25:15.200]   They all are faced with not only the GDPR taking and by the way, you don't have to have even business in in Europe
[00:25:15.200 --> 00:25:17.200]   You just have to have customers in Europe
[00:25:17.200 --> 00:25:22.480]   So as long as they collect or process data of individuals located inside the EU, they're subject to
[00:25:22.480 --> 00:25:25.280]   GDPR and that means
[00:25:25.280 --> 00:25:32.600]   There's responsibility and accountability. There's notice requirements. There's a consent requirements
[00:25:32.600 --> 00:25:35.160]   There's you have to have a data protection officer
[00:25:35.160 --> 00:25:41.880]   Who manages compliance with all this this there's a lot. I mean if you read the Wikipedia page on GDPR
[00:25:41.880 --> 00:25:47.640]   There are a lot of requirements. I have a book somebody sent me on GDPR compliance for us companies what you need to do
[00:25:47.640 --> 00:25:54.200]   You have to worry about it all? No, we don't collect data on people. Nobody. I don't think we store any information about you
[00:25:54.200 --> 00:25:56.200]   so I
[00:25:56.200 --> 00:26:01.600]   Think I think we're okay. It's a fly-by-night operation. Yeah, but there but you know it isn't that funny that
[00:26:01.600 --> 00:26:06.640]   There are very few businesses you could think of that don't in fact collect data on their customers
[00:26:06.640 --> 00:26:11.200]   So your advertisers collect data that's their that's not through us. So they have to do that
[00:26:11.200 --> 00:26:19.080]   Oh, but not through us. That's right. We don't allow we do even do things that other people do we prevent we don't we don't allow
[00:26:19.080 --> 00:26:24.160]   Tracking links for instance on our banner ads on our website. We don't allow them to do that
[00:26:24.160 --> 00:26:26.760]   So I think we're in the clear on this
[00:26:26.760 --> 00:26:33.920]   But I think that we're an exception frankly most of you are have to face this
[00:26:33.920 --> 00:26:37.000]   so
[00:26:37.000 --> 00:26:39.520]   There's more strange a strident
[00:26:40.240 --> 00:26:45.920]   legislation coming from you and coming about the privacy stuff and about right to be forgotten is
[00:26:45.920 --> 00:26:47.160]   It's gonna expand as well
[00:26:47.160 --> 00:26:52.280]   I have to say this Facebook thing is is probably gonna stimulate something in the United States as well
[00:26:52.280 --> 00:26:57.920]   And I think good consumers are aware of this now and I think consumers are concerned and want this
[00:26:57.920 --> 00:27:04.480]   It is good because the threat to Facebook which extends to ultimately to Google and Amazon as well and
[00:27:05.160 --> 00:27:10.440]   everybody else is that people will turn away from these services realizing that they don't control their data and
[00:27:10.440 --> 00:27:14.480]   Whatever benefit you get from Facebook and I would still argue that it is
[00:27:14.480 --> 00:27:21.120]   It is a small amount of benefit that can be duplicated in other ways. I don't think Facebook is a half you must have
[00:27:21.120 --> 00:27:28.120]   Utility for anybody and I think most companies will leave well. Well, no well. Yes
[00:27:28.120 --> 00:27:33.640]   Okay, but I think most companies are actually okay with a set of rules because
[00:27:34.400 --> 00:27:37.840]   those people are actually trying to behave well and
[00:27:37.840 --> 00:27:46.520]   So this gives you a stick to beat people like Cambridge Analytica who are misusing data misusing your terms of service for example
[00:27:46.520 --> 00:27:48.320]   it
[00:27:48.320 --> 00:27:51.560]   It gives you the ability to say hey look work or at least
[00:27:51.560 --> 00:27:58.680]   Biting by the lowest common denominator that we've set is a country and that's actually a really big deal
[00:27:58.680 --> 00:28:03.080]   I mean, that's kind of what democracy is all about right and and so
[00:28:04.160 --> 00:28:10.520]   That's why I think it's good and we need it and also gives us an incentive to develop better ways to handle data
[00:28:10.520 --> 00:28:18.320]   Then we have today. There's no incentive for like all these poor researchers who are trying to research things like differential privacy
[00:28:18.320 --> 00:28:20.720]   They don't get
[00:28:20.720 --> 00:28:23.920]   Gobs of funding because nobody cares about this stuff right now
[00:28:23.920 --> 00:28:29.720]   But this get this incentivizes people to actually start caring about this type of technology that can benefit
[00:28:29.720 --> 00:28:34.760]   Great article yesterday on motherboard by Carl Bodie who says given Facebook's privacy backlash
[00:28:34.760 --> 00:28:37.440]   Why aren't we angry? We're with the broadband industry if
[00:28:37.440 --> 00:28:45.360]   If your ISP is Comcast or AT&T and Verizon they're collecting the same amount same information
[00:28:45.360 --> 00:28:51.200]   They and they don't and they don't even admit to it. They don't they're just hoovering up your data
[00:28:51.200 --> 00:28:55.400]   Yeah, they share it with the government at the top of everybody
[00:28:55.400 --> 00:28:58.040]   These guys are nuts. There's no
[00:28:59.080 --> 00:29:02.160]   So beginning which day so you know what you know why yes, but here is
[00:29:02.160 --> 00:29:07.560]   reasonable regulation I would try for self-regulation self-governance first and
[00:29:07.560 --> 00:29:10.680]   Have not let me
[00:29:10.680 --> 00:29:17.400]   They have to be able to criticize and they have not shown that they're going to do that well
[00:29:17.400 --> 00:29:22.240]   And and that's a problem, but I would prefer that as a rule in the second problem, you know, I'm gonna get to is
[00:29:22.240 --> 00:29:25.960]   Okay, so let's say we agree on a reasonable scheme of regulation
[00:29:26.840 --> 00:29:33.480]   I'm sorry, but I don't trust this government and this Congress and this regulatory environment to do that and do that well
[00:29:33.480 --> 00:29:36.960]   I just simply don't I don't think you know what so ever
[00:29:36.960 --> 00:29:40.200]   It's gonna take a lot longer to get to there
[00:29:40.200 --> 00:29:45.720]   So, you know by the time we actually I mean think about how long it takes us to pass legislation in this country it is
[00:29:45.720 --> 00:29:50.360]   So
[00:29:53.960 --> 00:30:01.240]   Worried about that. I'm just it's just I mean those are the those are kind of like the basic things that Congress needs to do
[00:30:01.240 --> 00:30:06.400]   They can't even do that so, you know forget any kind of agreement on data protection
[00:30:06.400 --> 00:30:14.160]   Frankly I also think that they realize probably correctly that if voters are fairly apathetic. I mean this is this this
[00:30:14.160 --> 00:30:18.640]   You know delete Facebook movement is raised some it got some attention
[00:30:18.640 --> 00:30:23.540]   But I know is you know and there'll be not something else in the news cycle in a day and we'll just move on
[00:30:23.900 --> 00:30:30.960]   And I think this is the argument even Steve about anything who was writing is publishing a book very soon against Facebook
[00:30:30.960 --> 00:30:34.500]   You know he's trying to bring some perspective this as well and say
[00:30:34.500 --> 00:30:41.940]   There's bigger issues going on right now other things to worry about and and I don't think I
[00:30:41.940 --> 00:30:48.820]   Think Rosmus Nielsen who's a great researcher at Oxford said let's bet here about how much
[00:30:48.820 --> 00:30:51.340]   Facebook's gonna fall and usage I
[00:30:51.980 --> 00:30:58.660]   Don't think however your first question could this be an existential crisis? Yes, it could be they've got to take this very very very seriously
[00:30:58.660 --> 00:31:00.660]   I and I'm sure they are
[00:31:00.660 --> 00:31:02.940]   um
[00:31:02.940 --> 00:31:08.060]   Is the liberal would a libertarian approach be effective is there a way to see no no
[00:31:08.060 --> 00:31:12.820]   No, is there any way that we could see that not?
[00:31:12.820 --> 00:31:19.780]   That you know, I mean I don't honestly stay see I I think it you know in a perfect world
[00:31:20.740 --> 00:31:25.780]   Society would get together and agree that no company should keep your data from within 30 days that you should have the right to delete
[00:31:25.780 --> 00:31:29.460]   Etc, etc. But that's not I don't see that happening in
[00:31:29.460 --> 00:31:36.980]   Europe has decided I know it's it's not 30. I don't think it's 30 days. Well, three days. Yeah, that's
[00:31:36.980 --> 00:31:44.100]   But so it but you do have the ability to revoke your consent you have to
[00:31:44.100 --> 00:31:47.980]   Company when they're collecting your data they have to explain what they plan to do with it
[00:31:48.860 --> 00:31:52.180]   Those are those are not draconian things to do
[00:31:52.180 --> 00:31:57.660]   Because it's not law yet how that's gonna work out and whether it's gonna be implemented and what's gonna happen
[00:31:57.660 --> 00:32:00.900]   We really haven't seen that well those are those are the regulations
[00:32:00.900 --> 00:32:03.720]   I know but they don't they're not in effect yet and when right me
[00:32:03.720 --> 00:32:06.220]   I'm sorry March March no
[00:32:06.220 --> 00:32:14.180]   I think it'll be very interesting when we get back here in June or July to talk about well what's changed anything I
[00:32:14.180 --> 00:32:18.340]   What if on what if
[00:32:18.740 --> 00:32:24.500]   We put the responsibility on individuals to keep track of everything now we're as individuals
[00:32:24.500 --> 00:32:31.180]   We are tracking too much already you cannot you cannot fight the power of a Facebook or your Equifax is
[00:32:31.180 --> 00:32:34.580]   Individuals were hosed. Yeah, that's us
[00:32:34.580 --> 00:32:39.100]   Like that's a dumb solution and I hate when people do it. They're like oh
[00:32:39.100 --> 00:32:45.460]   Americans buy into this so much and it drives me absolutely bonkers. There are some things individuals cannot do
[00:32:45.460 --> 00:32:47.660]   end of rent
[00:32:47.660 --> 00:32:51.900]   Yeah, I mean you could not use Facebook Google Amazon
[00:32:51.900 --> 00:32:58.060]   You could not but then you have a Safeway card that tells Safeway everything you bought or you have or I mean look at
[00:32:58.060 --> 00:33:02.820]   You're anywhere. We go you're gonna anytime use your credit card. Yeah, yeah
[00:33:02.820 --> 00:33:05.260]   Or anytime you go to the doctor
[00:33:05.260 --> 00:33:10.300]   I mean the doctor is pulling data about you that does get pulled into like your insurance company
[00:33:10.300 --> 00:33:16.060]   You can't not share your data with people right and again. We've had this discussion many times. There was Dana boy to talk me this list
[00:33:17.180 --> 00:33:21.420]   You regulate the use of data not not the gathering of it gathering of it is knowledge
[00:33:21.420 --> 00:33:24.780]   Use is the question should your doctor have a little data about you?
[00:33:24.780 --> 00:33:31.820]   Absolutely should you be refused insurance under the current law? No, you should not should you refuse employment? No, you should not
[00:33:31.820 --> 00:33:35.620]   That's that's a question of the use of data. That's not the fact that it exists
[00:33:35.620 --> 00:33:43.300]   Isn't this just kind of a con commitment inevitability when you create business models based on advertising?
[00:33:44.460 --> 00:33:48.380]   Weren't we gonna get this isn't wasn't this gonna happen no matter what so pay pay walls?
[00:33:48.380 --> 00:33:56.020]   We'll we'll we'll read line quality and journalism for only the elite. Yeah, commerce will bring you every one of them has conflicts
[00:33:56.020 --> 00:33:59.940]   The question is do you have a moral north star in how you operate in those models?
[00:33:59.940 --> 00:34:03.620]   So the date is gonna be it's gonna be out there
[00:34:03.620 --> 00:34:10.020]   Yeah for good reason in a lot of cases and sometimes for now the challenge with litigating or litigating
[00:34:10.020 --> 00:34:14.140]   regulating use is we historically haven't done that well and
[00:34:14.660 --> 00:34:16.140]   typically we
[00:34:16.140 --> 00:34:17.420]   regulate
[00:34:17.420 --> 00:34:21.380]   Things like actions and we we like bright lines for example
[00:34:21.380 --> 00:34:26.060]   So use is really hard to track because we don't your law you can't you know
[00:34:26.060 --> 00:34:30.500]   You need a bright line because otherwise it's too hard to legislate and too hard to
[00:34:30.500 --> 00:34:35.740]   And what I like about the GPR is that it
[00:34:35.740 --> 00:34:40.500]   It establishes a right to privacy for individuals
[00:34:40.500 --> 00:34:48.580]   It establishes for them the ability to control their data and I I actually think that's the right way to go about doing it
[00:34:48.580 --> 00:34:52.020]   It's not it's not just rules about gathering data. It's a right
[00:34:52.020 --> 00:34:59.460]   To how data it's a right to privacy for your data how your data is used and controlled and taken and
[00:34:59.460 --> 00:35:03.860]   That's kind of like the first amendment that is that is a right?
[00:35:03.860 --> 00:35:09.980]   It's like one of those unalienable rights that we just didn't know we needed until today if that makes sense
[00:35:10.340 --> 00:35:11.820]   Yeah
[00:35:11.820 --> 00:35:19.420]   Well, and I do and it we may not have needed as desperately as we needed in this in this technological age where you can combine all these databases
[00:35:19.420 --> 00:35:24.180]   And you have massive processing power and you can do so much more than you could do
[00:35:24.180 --> 00:35:30.540]   Information go on what no go ahead to say see no, I was just gonna echo
[00:35:30.540 --> 00:35:34.060]   Echo Leo
[00:35:34.980 --> 00:35:39.580]   But but scale counts both ways right as I said before scale
[00:35:39.580 --> 00:35:43.940]   Allows everyone to talk to anyone that is incredibly powerful
[00:35:43.940 --> 00:35:47.940]   It's disruptive and it's powerful and it means the trolls can talk to but it's powerful
[00:35:47.940 --> 00:35:50.980]   That's what enables everything we love about about this net scale
[00:35:50.980 --> 00:35:56.900]   Also brings the things you've talked about in data and it brings the difficulty of enforcement of whatever standards you have
[00:35:56.900 --> 00:35:59.220]   But we're we're at the level of scale now
[00:35:59.220 --> 00:36:03.020]   We're they're there and wishing we weren't isn't gonna do us a lot of good
[00:36:03.340 --> 00:36:08.100]   So so the question is how do we have a discussion about the responsibility?
[00:36:08.100 --> 00:36:12.620]   That these companies have how do they have that? How do we teach them that that's to me?
[00:36:12.620 --> 00:36:18.500]   The bigger question, you know when Facebook throughout all the public content because some of it was inconvenient
[00:36:18.500 --> 00:36:24.420]   It had an impact on news that hurts my not only my industry, but it hurts it informs society now
[00:36:24.420 --> 00:36:29.020]   They're trying to bring it back now, and that's a good thing, but what do we expect of them? What's your public responsibility?
[00:36:29.020 --> 00:36:32.580]   I had this discussion with Jay Rosen this week who's a brilliant brilliant guy
[00:36:33.300 --> 00:36:36.780]   And he's one of the emphasized to me that we don't have the terms of this discussion
[00:36:36.780 --> 00:36:42.140]   There's no law that says these companies have public responsibility though. We presume they do and I certainly believe they do
[00:36:42.140 --> 00:36:47.300]   Well, how do we have that conversation? That's the higher level to have so when Zuck writes his post today
[00:36:47.300 --> 00:36:49.300]   It's just about what happened with Cambridge Analytica
[00:36:49.300 --> 00:36:51.940]   We won't let that happen again, and we already cut off what's what okay?
[00:36:51.940 --> 00:36:56.060]   So because it's not the issue apologize after the fact
[00:36:56.060 --> 00:37:00.300]   Move on and and then they happens again and then move on and it happens again
[00:37:00.300 --> 00:37:09.020]   And there's bigger issues about about about the the the moral and ethical basis and culture of these companies if we were to start from scratch
[00:37:09.020 --> 00:37:10.860]   I know this isn't
[00:37:10.860 --> 00:37:18.460]   Impossibility but would it be possible to design a system that would protect people's privacy and yet keep us connected and give us the benefits of a
[00:37:18.460 --> 00:37:24.740]   Technological revolution or is this just inevitable? I feels like it's inevitable that this is this is what happens
[00:37:24.740 --> 00:37:28.340]   Could we design a system that wouldn't have these problems?
[00:37:28.860 --> 00:37:30.980]   We don't have the incentive structure in place
[00:37:30.980 --> 00:37:35.100]   Yeah, well, I understand it would never have happened
[00:37:35.100 --> 00:37:41.500]   And it's not gonna happen now. Well, so maybe it's a completely rhetorical question because there's you know
[00:37:41.500 --> 00:37:45.780]   Even if we want to mean anything, but right well so if we want to
[00:37:45.780 --> 00:37:49.580]   Take on this thought exercise. We actually have to think about
[00:37:49.580 --> 00:37:55.740]   What kind of incentives we have in place in this I mean we'd have to go all the way to society basically yeah
[00:37:55.740 --> 00:38:03.140]   So you'd have to have people be altruists and everybody be altruists and even if 99% of people are altruists
[00:38:03.140 --> 00:38:06.500]   Which I believe they probably are so it's the 1% that kills you
[00:38:06.500 --> 00:38:14.380]   Well, and yeah, okay. We're not with this. That's a that's a beer and brought kind of conversation
[00:38:14.380 --> 00:38:22.540]   I don't hold how high hopes that we can regulate it either. So that's what I'm puzzled about is I don't
[00:38:23.180 --> 00:38:25.580]   We can't let me pose your question another way
[00:38:25.580 --> 00:38:30.780]   Because I think this is and this is what I've said this week - I think that I
[00:38:30.780 --> 00:38:36.620]   Was among many and I think you were and Jay Rosen was I have a discussion with Jay
[00:38:36.620 --> 00:38:44.580]   We believed almost religiously in the openness of the net sure did and we believe that end-to-end and open was a good
[00:38:44.580 --> 00:38:49.020]   And it could become and I think a lot of good has come and I fear we're gonna forget that
[00:38:49.420 --> 00:38:53.340]   But with that we now know fully inevitably comes
[00:38:53.340 --> 00:38:56.220]   manipulation bad actors
[00:38:56.220 --> 00:38:58.020]   and
[00:38:58.020 --> 00:39:03.620]   How much openness are we willing to give up and to whom are we willing to give it up under what standards and rules?
[00:39:03.620 --> 00:39:08.220]   That's the real discussion that we have not with the Internet the Internet is the Internet. It's gonna be there
[00:39:08.220 --> 00:39:12.820]   It's gonna connect to all kinds of things. I don't want China. I don't want a
[00:39:12.820 --> 00:39:18.500]   Place where where anyone decides what we're allowed to say and not allowed to say
[00:39:19.140 --> 00:39:20.580]   I
[00:39:20.580 --> 00:39:24.180]   That that that that's going too far in this openness scale
[00:39:24.180 --> 00:39:31.940]   Pure unadulterated openness is for chatting a channel. That's not what we want. I think this in kind of encourages an authoritarianism
[00:39:31.940 --> 00:39:38.180]   In the long run that it promote that in a way it's promoting yet. What's the it the antecedent it there?
[00:39:38.180 --> 00:39:49.100]   This kind of lack of privacy and I think see because I think the reason it works in China is people want
[00:39:49.100 --> 00:39:53.660]   Order and the tools now technology provides to keep order
[00:39:53.660 --> 00:39:56.940]   Kind of encourage an authoritarianism that people are well
[00:39:56.940 --> 00:40:01.140]   People are told well that people are told by the authorities that they want order and they have no choice
[00:40:01.140 --> 00:40:07.620]   But to go alone without order the authorities provide right so I don't think and the technology the technology tools are there
[00:40:07.620 --> 00:40:14.500]   So I just feel like it's enough it promotes authoritarianism by the way Facebook has is culpable for a lot more than just this
[00:40:14.500 --> 00:40:16.500]   Look at what they did in the Philippines
[00:40:16.700 --> 00:40:21.100]   Where they really kind of facilitated the election of Duterte and now support him
[00:40:21.100 --> 00:40:24.860]   He's shut down the free press, but he uses Facebook live to stream
[00:40:24.860 --> 00:40:31.940]   Right I go back to what I said before we thought that that involving the citizens in an open platform of civic engagement was a good thing and
[00:40:31.940 --> 00:40:33.860]   Oops
[00:40:33.860 --> 00:40:38.940]   Marie Maria's is the bravest journalist. I know she's she's phenomenal Marie arrest. I was on the screen right now
[00:40:38.940 --> 00:40:43.100]   And and yeah, these are these are huge issues
[00:40:43.100 --> 00:40:50.580]   So what what are this freedom we gained John Perry Barlow? Are we willing to give up and to whom does Facebook have no
[00:40:50.580 --> 00:40:54.780]   So Facebook in an attempt to be politically agnostic
[00:40:54.780 --> 00:41:00.180]   Helped all the candidates in the Philippines and then when Duterte won with their help
[00:41:00.180 --> 00:41:03.300]   they support him
[00:41:03.300 --> 00:41:09.620]   Because he's the leader because they're trying to be politically agnostic and yet they're supporting authoritarian
[00:41:10.220 --> 00:41:14.900]   Government issues policies against its people. So okay, so set the rules set the rule
[00:41:14.900 --> 00:41:19.380]   What is the rule there and where is Facebook's colorality there? I think that's far more offensive
[00:41:19.380 --> 00:41:21.220]   Then then I'm gonna make you set the rule
[00:41:21.220 --> 00:41:27.300]   So if Facebook in the US or the Philippines or wherever should Facebook cut certain candidates off?
[00:41:27.300 --> 00:41:34.740]   Well, I don't know to the tool because they are bad. You're talking about use as opposed to an
[00:41:35.380 --> 00:41:42.980]   Actual to gathering for the examples. So you're actually right now arguing for a use case thing as opposed and then calling for or sorry
[00:41:42.980 --> 00:41:48.580]   What we
[00:41:48.580 --> 00:41:53.380]   That is who's saying I give your scones a but but but but but we had that argument before
[00:41:53.380 --> 00:41:56.540]   but it relates here. That's the same sort of
[00:41:56.540 --> 00:42:05.340]   Mental argument thing no, but I'm just simply saying to Leo if he said if he he starts at the EU
[00:42:05.340 --> 00:42:08.500]   He starts at the use and says Duterte or Trump and
[00:42:08.500 --> 00:42:14.460]   I come back and I say okay, then write me the rule that you want them to live. Oh, yeah, right?
[00:42:14.460 --> 00:42:19.180]   What do you do? That's my what I'm saying is this is a structural problem
[00:42:19.180 --> 00:42:22.940]   Can't they come me like this because the company's gonna say well, we're not gonna choose sides
[00:42:22.940 --> 00:42:26.820]   So and then they know they are right now, okay
[00:42:26.820 --> 00:42:30.260]   So it's like I use three cases where they are and this is this is what's really interesting right now
[00:42:30.260 --> 00:42:32.980]   And where I think we need to help them as I've talked about before
[00:42:33.540 --> 00:42:36.780]   Google now accounts for the reliability authority and quality of
[00:42:36.780 --> 00:42:40.060]   Sources in search ranking they side with science
[00:42:40.060 --> 00:42:43.340]   Twitter came out with its with its new things and okay
[00:42:43.340 --> 00:42:48.940]   We realize we can't be fully open and they're asking people in their RFP to say help us decide the standards that they're gonna live by
[00:42:48.940 --> 00:42:53.540]   Facebook is at least bringing back quality news. They are making decisions now
[00:42:53.540 --> 00:43:01.820]   So all I'm saying to you is help them write those rules you want help them write them whether they're written by government or by Facebook
[00:43:02.180 --> 00:43:07.460]   You the citizens should not just be standing back and saying well Facebook you mess this all up go fix it
[00:43:07.460 --> 00:43:16.340]   No, okay, you've got to help here my rules. I want to know exactly what data they're collecting about me that includes how far that data
[00:43:16.340 --> 00:43:21.780]   My relationship with people and what that means for things. I want to understand
[00:43:21.780 --> 00:43:30.420]   Who else gets access to that data and how I want to understand what those companies are trying to do with the data
[00:43:30.420 --> 00:43:37.420]   I want to understand I want the ability to stop my data going from someplace that I think is awful or unethical
[00:43:37.420 --> 00:43:44.020]   And that may mean when I try to load an app to take a stupid quiz that a little pop-up says hey
[00:43:44.020 --> 00:43:50.580]   By the way, this is run by such and such and the data they're going to use from this is gonna be used to infer your political
[00:43:50.580 --> 00:43:55.860]   Leaning's you know, and that'll be like weird. I just thought I was thinking about fruit
[00:43:56.820 --> 00:44:03.380]   Or liking hello Kitty right and how you so those those are what the GDPR offers
[00:44:03.380 --> 00:44:08.300]   And I would like an easy to understand platform for that. But those are the rules Jeff
[00:44:08.300 --> 00:44:12.380]   Okay, I'm cool with that. But you know back to Leo's question. How does that stop to tear today?
[00:44:12.380 --> 00:44:14.780]   How does that stop these other things there's other?
[00:44:14.780 --> 00:44:19.780]   That's the personal data piece. That's a slice and it's fine. I think that's a good a very good
[00:44:19.780 --> 00:44:26.700]   I could put a little bit on it, but I generally agree with it set of rules and standards now take that across a lot of other
[00:44:26.700 --> 00:44:28.700]   areas like
[00:44:28.700 --> 00:44:36.220]   Political action if they if if the Nazi who's running in for Congress in Illinois and now comes on should Facebook
[00:44:36.220 --> 00:44:39.020]   good example should Facebook
[00:44:39.020 --> 00:44:47.780]   Should we expect Facebook to say no, Nazi you can't use us. Where's that line? So then I would say Facebook needs to establish a
[00:44:47.780 --> 00:44:56.220]   Policy about how it shares information on people with their governments part of the issue here is and we're gonna encounter this again
[00:44:56.220 --> 00:44:59.300]   and again is in America we have different standards
[00:44:59.300 --> 00:45:04.020]   I'm not saying people in the Philippines are really excited about getting killed for having you know minor drug possessions
[00:45:04.020 --> 00:45:09.580]   because I'm pretty sure they're not but if Facebook doesn't share data with a
[00:45:09.580 --> 00:45:16.620]   regime and maybe it maybe we do something with the OECD or the UN or the what the
[00:45:16.620 --> 00:45:21.940]   What is it called the world economic council where you have a standard of like?
[00:45:24.540 --> 00:45:31.700]   authoritarianism maybe and you say hey if you fall below this on the scale we're not gonna share our data with you period
[00:45:31.700 --> 00:45:35.740]   Okay, so let's say let's say say we should push back. This is good. So you're Twitter now
[00:45:35.740 --> 00:45:41.660]   And not not an issue of sharing data at all. It's just providing a platform for the Nazi candidate
[00:45:41.660 --> 00:45:47.420]   Should they say no you do not have the right to be here? I'd be okay with that
[00:45:47.420 --> 00:45:48.700]   but
[00:45:48.700 --> 00:45:52.340]   We have to understand what that standard is has nothing to do with data now at all
[00:45:52.340 --> 00:45:58.820]   It's about providing allowing that person to have the same service that everybody else has but you're gonna cut a line
[00:45:58.820 --> 00:46:04.180]   I'm fine with making that line. I think we have to make those lies now, but we have we have to help
[00:46:04.180 --> 00:46:09.820]   As norms a society help make those lines. I don't think Facebook did this on purpose
[00:46:09.820 --> 00:46:14.700]   But I think they inadvertently created a platform that is
[00:46:14.700 --> 00:46:17.260]   incredibly
[00:46:17.260 --> 00:46:22.180]   Intrusive and I think they've known about this for years. This is an article from 2013
[00:46:22.180 --> 00:46:27.060]   From almost five years ago in the National Academy of Sciences proceedings
[00:46:27.060 --> 00:46:34.220]   We show that easily accessible. This is the by the way the article that started the whole thing easily accessible digital records of behavior
[00:46:34.220 --> 00:46:42.060]   Facebook likes can be used to automatically and accurately predict a range of highly sensitive personal attributes including
[00:46:42.740 --> 00:46:51.540]   sexual orientation ethnicity religious and political views personality traits intelligence happiness use of addictive substances parental separation age and
[00:46:51.540 --> 00:46:55.980]   gender they use 58,000 volunteers they
[00:46:55.980 --> 00:47:01.540]   They found out things like liking Sephora and curly fries would tell you something about your intelligence
[00:47:01.540 --> 00:47:08.500]   I mean it well it is curly fries. Yeah, it is counterintuitive. It's extremely powerful
[00:47:08.540 --> 00:47:14.740]   It's based on big data. This was 58,000 people now try it with 50 million people and
[00:47:14.740 --> 00:47:20.220]   The problem with these models is they get better and better and better as we add more and more data
[00:47:20.220 --> 00:47:23.980]   This is the Facebook's likes platform. That's it nothing else
[00:47:23.980 --> 00:47:27.460]   Facebook basically created an
[00:47:27.460 --> 00:47:32.420]   Incredibly powerful tool which can be used by dictators
[00:47:32.420 --> 00:47:38.220]   Demagogues and nice people alike and it's not just their responsibility
[00:47:38.220 --> 00:47:40.220]   It's also do they shut it down?
[00:47:40.220 --> 00:47:44.180]   Let me read you a quote from Edward Williams at South by Southwest
[00:47:44.180 --> 00:47:48.340]   When we built Twitter, he said we weren't thinking about these things
[00:47:48.340 --> 00:47:55.580]   We laid down fundamental architectures and that had assumptions that didn't account for bad behavior and now we're catching on to that
[00:47:55.580 --> 00:47:58.460]   It's not just the data is just one tool
[00:47:58.460 --> 00:48:01.740]   You know distribution is another tool
[00:48:03.660 --> 00:48:08.780]   This the social push of groups to lie and harass is another tool
[00:48:08.780 --> 00:48:14.900]   These are all things that get enabled they get enabled by printing presses and and telephones to they get
[00:48:14.900 --> 00:48:20.220]   They want a different speed and scale now and we we the internet geeks. We said
[00:48:20.220 --> 00:48:22.060]   openness
[00:48:22.060 --> 00:48:27.660]   So the rules they operated under were our rules John Perry Barlow God bless him. May he rest in peace
[00:48:27.660 --> 00:48:33.540]   Those were our rules. We said openness. We said that neutrality. We said all these things now with experience
[00:48:33.540 --> 00:48:35.660]   We're changing our minds. Okay, we can
[00:48:35.660 --> 00:48:39.700]   But I don't think we can stand back and say well they ended up creating this horrible tool
[00:48:39.700 --> 00:48:43.060]   They created it in our our specs. This is the internet. We want it
[00:48:43.060 --> 00:48:45.580]   You open one with Facebook
[00:48:45.580 --> 00:48:51.980]   I have more controlled with Facebook is more controlled and more civilized than Twitter right now because it has controls that we
[00:48:51.980 --> 00:48:55.940]   Bocked at why is Facebook so choosing what I see? Why are they censoring people?
[00:48:55.940 --> 00:49:00.020]   They got all that kind of crap right in the day and now we're saying well now
[00:49:00.020 --> 00:49:06.020]   They're doing enough of that well the first thing everybody should do is is stop doing likes and delete your likes because
[00:49:06.020 --> 00:49:09.180]   with 95% accuracy these
[00:49:09.180 --> 00:49:13.180]   You could predict somebody's race 93% their gender
[00:49:13.180 --> 00:49:19.580]   With 88% accuracy from somebody's likes alone. You can predict whether they're gay or straight
[00:49:19.580 --> 00:49:23.900]   85% accuracy Democrat versus Republican
[00:49:24.700 --> 00:49:30.460]   These these you know, I don't think anybody looked at likes and said I'm gonna be able to get with this kind of accuracy
[00:49:30.460 --> 00:49:36.660]   And by the way, this is a relatively small sample set I can do this kind of stuff so
[00:49:36.660 --> 00:49:40.500]   Well, they don't know if you go back to yank a little bit
[00:49:40.500 --> 00:49:47.380]   Did you know did you know when you were doing likes that this would tell people what your sexual orientation was when you were liking?
[00:49:47.380 --> 00:49:49.380]   Hello Kitty or curly
[00:49:49.380 --> 00:49:52.580]   Happens to you when you buy products at the grocery store in us in your car
[00:49:52.580 --> 00:49:57.020]   Exactly thing it has for years since the 70s. This is not a new technology to rescale
[00:49:57.020 --> 00:49:58.980]   It's hardly new
[00:49:58.980 --> 00:50:00.980]   It's been used for years
[00:50:00.980 --> 00:50:07.700]   And I mean we do it as people we make inferences all the time. I'm very curious like I like both Sephora and curly fries
[00:50:07.700 --> 00:50:13.700]   I'm like does that mean I'm intelligent. Oh, well, that's actually they don't say they don't say ah
[00:50:13.700 --> 00:50:16.620]   Intelligence by the way is one of the harder things
[00:50:17.540 --> 00:50:19.540]   to predict and
[00:50:19.540 --> 00:50:26.860]   You know my daughter loves hello kitty does that make her she is not a rule breaker. Yeah, so like so there's a lot of
[00:50:26.860 --> 00:50:35.060]   Yes in aggregate, but we are the best predictor of high intelligence are are you ready?
[00:50:35.060 --> 00:50:38.820]   Yes thunderstorms the Colbert report
[00:50:38.820 --> 00:50:41.460]   science and curly fries
[00:50:41.460 --> 00:50:44.380]   low intelligence
[00:50:44.380 --> 00:50:46.780]   Sephora I love being a mom
[00:50:47.420 --> 00:50:50.060]   Harley Davidson and lady added Bellum
[00:50:50.060 --> 00:50:52.620]   Just a country group
[00:50:52.620 --> 00:50:56.660]   Good predictors of male homosexuality included of course the no hate campaign
[00:50:56.660 --> 00:51:00.300]   But really you would think that that kind of thing would be the most obvious, but it's not
[00:51:00.300 --> 00:51:03.420]   Mac cosmetics and wicked the musical
[00:51:03.420 --> 00:51:07.980]   strong predictors of male heterosexuality include Wu Tang Clan
[00:51:07.980 --> 00:51:10.260]   Shaq and
[00:51:10.260 --> 00:51:14.100]   liking the group being confused after waking up from naps
[00:51:15.140 --> 00:51:20.380]   So here's there's there's obviously some bias in there, too. I imagine so
[00:51:20.380 --> 00:51:27.820]   Looking at the things that correlate to low intelligence. I see some I see a lot of things that are related to women
[00:51:27.820 --> 00:51:34.780]   Makeup. I love being a mom not Harley Davidson, but you associate that with possibly country in the south
[00:51:34.780 --> 00:51:37.740]   No, but this isn't no nobody's being a qualitative
[00:51:37.740 --> 00:51:44.140]   Decision what they did is they took 58,000 people they took all of their likes they had him take a a
[00:51:44.620 --> 00:51:50.980]   Personality inventory test answer questions and then match the two and we're able to find these correlations
[00:51:50.980 --> 00:51:52.980]   This is a small
[00:51:52.980 --> 00:51:56.220]   Stacy's point no Stacy's making a very good point about
[00:51:56.220 --> 00:52:03.140]   Whether it's an IQ test or whether it's an SAT or whether it's this that's that's possible
[00:52:03.140 --> 00:52:09.100]   There's huge bias that gives me hope how we define first time I've been happy about bias is biased right?
[00:52:09.220 --> 00:52:16.020]   Well, and so and that's that's something that we don't talk about when we talk about data and that's something
[00:52:16.020 --> 00:52:20.340]   I mean, okay, we do talk about it, but we're not talking about it here and there's there's an interest. I mean
[00:52:20.340 --> 00:52:27.780]   There are ways that bias is going to filter into how things are targeted and they're gonna be
[00:52:27.780 --> 00:52:32.340]   Mostly accurate could they be more accurate if we didn't have you know
[00:52:32.340 --> 00:52:38.500]   racist sexist people sometimes doing these things or race making I shouldn't say that people people making
[00:52:39.340 --> 00:52:43.820]   Maybe on a social racist or to sexist yes assumptions. I don't know
[00:52:43.820 --> 00:52:47.460]   Although
[00:52:47.460 --> 00:52:50.860]   Yeah, curly fries and Sephora dang it
[00:52:50.860 --> 00:52:55.860]   I don't know where I fall in intelligence down. You want to take the the website still up my personality
[00:52:55.860 --> 00:53:02.140]   Project is at my personality org slash wiki. I see look what you're both doing. Look at what you're gonna go take this thing
[00:53:02.140 --> 00:53:05.660]   You've just been decoyant. We've just been panning that history. I'm like
[00:53:07.460 --> 00:53:11.060]   Let me edit by the way. They now have six million test results
[00:53:11.060 --> 00:53:14.940]   Peridot 4 million individual Facebook profiles, which means that their
[00:53:14.940 --> 00:53:20.780]   Psychometrics are well given the bias that within the bias of the test more even more accurate
[00:53:20.780 --> 00:53:24.740]   And this is this is part of like this whole idea of his human beings we crave
[00:53:24.740 --> 00:53:26.460]   validation and
[00:53:26.460 --> 00:53:32.660]   My god, we're just gonna go find it wherever we can and places like Facebook are awesome for that
[00:53:32.660 --> 00:53:36.420]   I don't know. Are we gonna talk the whole show about this? No
[00:53:37.420 --> 00:53:44.620]   A lot of Google news - let's wind it up. What is so I guess we know what the harm could be we could think of what the worst things could be
[00:53:44.620 --> 00:53:47.380]   out of this I
[00:53:47.380 --> 00:53:49.380]   Don't know if there's a way to fix it
[00:53:49.380 --> 00:53:55.740]   It's certainly something that we're gonna all grapple with and especially these big companies as they get as they're under fire
[00:53:55.740 --> 00:53:59.460]   I mean we're in a new society and these these are not easy questions
[00:53:59.460 --> 00:54:05.140]   And I'm my only problem with a discussion not the show but in general now is being treated as easy whole Facebook
[00:54:05.140 --> 00:54:09.460]   It's not a society and they can fix it. It's not easy. It's not simple. It's not their fault
[00:54:09.460 --> 00:54:12.820]   That's dumb and and and these are hard questions
[00:54:12.820 --> 00:54:19.980]   And I'm as critical as anyone and I wrote a post about this that Facebook is not grappling with these huge questions of public
[00:54:19.980 --> 00:54:23.900]   Responsibility and that they must they must deal with radical transparency
[00:54:23.900 --> 00:54:26.740]   They must deal with their own culture and their ethical bases
[00:54:26.740 --> 00:54:34.060]   They have to do these things and they're not doing them yet and and so I'm holding through a very high standard
[00:54:34.220 --> 00:54:36.580]   but just the Cambridge Analytica story is
[00:54:36.580 --> 00:54:41.900]   It's an old story actually and and there may be better things to talk about
[00:54:41.900 --> 00:54:49.340]   Our show today brought to you by word press now. Here's something I can get behind something is something that's nice
[00:54:49.340 --> 00:54:55.420]   It's happy. It's all about you sharing under your control on your website. It's not your Facebook page
[00:54:55.420 --> 00:54:58.060]   It's not your Twitter account. It's your word. It's your website
[00:54:58.060 --> 00:55:03.740]   There's a reason why 30% of all the websites in the world run on WordPress. It's the best platform
[00:55:03.820 --> 00:55:10.780]   For putting you out there in a way that you control the freedom and flexibility to share your voice your work
[00:55:10.780 --> 00:55:16.300]   To share it your way WordPress calm. It makes it easy to they do the hosting
[00:55:16.300 --> 00:55:20.740]   They give you the templates. They've got the great customer support team
[00:55:20.740 --> 00:55:25.980]   They're 24/7 Monday through Friday and a weekends do to help answer any questions you might have
[00:55:25.980 --> 00:55:27.420]   You don't need to be a geek
[00:55:27.420 --> 00:55:34.060]   You don't need to be a coder or even a designer to do a great-looking site that works for you at WordPress
[00:55:34.060 --> 00:55:41.300]   Go to wordpress.com/twig for 15% off your brand new website. They've got e-commerce buttons
[00:55:41.300 --> 00:55:49.220]   Well options ranging from a simple buy button just next to one thing or donate button to a complete online store
[00:55:49.220 --> 00:55:52.020]   they can do the whole thing plan start as low as four dollars a month and
[00:55:52.820 --> 00:55:59.540]   WordPress makes it easy to expand your reach with built-in SEO social sharing so even if if you know
[00:55:59.540 --> 00:56:06.300]   This isn't your Facebook page your readers your customers will share your stuff on their Facebook page, which is awesome
[00:56:06.300 --> 00:56:08.860]   Go to wordpress.com/twig
[00:56:08.860 --> 00:56:12.860]   You got to have a place on the web that's yours that you own 100%
[00:56:12.860 --> 00:56:18.560]   That's it right there WordPress calm slash twig to save 15% off your brand new website
[00:56:19.060 --> 00:56:22.380]   Find out why 30% of all the websites in the world including mine
[00:56:22.380 --> 00:56:31.420]   Some of the best publications in the world run on wordpress wordpress.com slash twig we thank them for their support Google's buying Lightro
[00:56:31.420 --> 00:56:34.540]   Or at least that's the rumor reports have it
[00:56:34.540 --> 00:56:37.460]   Didn't we didn't we talk last week about how?
[00:56:37.460 --> 00:56:45.420]   The next version of phones might have you know two cameras separated a lot of them do and they we speculate about the stuff
[00:56:45.420 --> 00:56:47.420]   yeah, like row was a
[00:56:48.060 --> 00:56:51.500]   Camera system that gave you multiple focal planes
[00:56:51.500 --> 00:56:57.740]   It was a very kind of a weird expensive limited in some ways camera. It was a multi camera technique and
[00:56:57.740 --> 00:57:00.940]   according to multiple sources telling tech crunch
[00:57:00.940 --> 00:57:08.020]   The they may have acquired Lightro for a bargain basement price somewhere between 25 and 40 million dollars
[00:57:08.020 --> 00:57:12.260]   Lightro which had been struggling was shopping itself to Facebook and Apple
[00:57:12.260 --> 00:57:17.180]   Not all the according to sources again with tech crunch not all employees are going over
[00:57:17.660 --> 00:57:26.300]   Some have already received severance in part of ways with the company. It sounds like it's an acquisition of Lightro's light field patents and other digital imaging technology
[00:57:26.300 --> 00:57:28.300]   They have 59 patents
[00:57:28.300 --> 00:57:35.460]   It's a very cool technology, but not something you use every day, right? Yeah, I was very excited when and I mean when this came out
[00:57:35.460 --> 00:57:36.820]   I was like oh
[00:57:36.820 --> 00:57:39.260]   Cool, I mean it fits all the cool tech buttons
[00:57:39.260 --> 00:57:40.180]   So I'm glad you know
[00:57:40.180 --> 00:57:45.020]   I'm glad that it's cool that appreciates cool tech and doesn't really care too much about markets
[00:57:45.020 --> 00:57:50.180]   And you know they're gonna do something cool with this eventually yeah, and there may be you should talk about
[00:57:50.180 --> 00:57:56.060]   There may be other applications besides just camera technology maybe VR applications AR applications as well
[00:57:56.060 --> 00:57:59.660]   So it's interesting stuff. Go ahead. What did you want to talk about?
[00:57:59.660 --> 00:58:02.660]   Yeah, I just said Google I was at the Google event yesterday here in New York
[00:58:02.660 --> 00:58:07.740]   Well, they announced tons of things to try to help news including buying the Chelsea market
[00:58:07.740 --> 00:58:12.420]   And keeping the stones in place
[00:58:12.660 --> 00:58:15.700]   They're not gonna get this is are they really good scones down there?
[00:58:15.700 --> 00:58:21.860]   Let's go to scones there the first floor. I guess is an actual market, right? It's a very it's a little too hip for for
[00:58:21.860 --> 00:58:28.540]   normal people, but it's like me. Yeah, is it too hip for you? It's too. It must be really hip because I feel like
[00:58:28.540 --> 00:58:31.740]   It's really wears black and everything
[00:58:31.740 --> 00:58:37.420]   I know I'm like look at that nice and nice weird beard and you know the little fluff in his hair
[00:58:37.420 --> 00:58:40.300]   I feel like Jeff you got some hipness. It's not an insult
[00:58:40.300 --> 00:58:43.580]   I'm just like you're put together person. Are they gonna now?
[00:58:43.580 --> 00:58:46.500]   So I love this statement from Daniel David, right?
[00:58:46.500 --> 00:58:52.540]   I would want to call him Daniel Radcliffe, but that's Harry Potter from David Radcliffe VP real estate and workplace services at Google
[00:58:52.540 --> 00:58:59.660]   It's been 18 years since Google New York first established our single person sales office in a Starbucks on 86
[00:58:59.660 --> 00:59:05.700]   After moving from 86 street to a more official space in Times Square. So what year was 18 years ago?
[00:59:05.700 --> 00:59:10.260]   Do some math for me first? That was 2000. That was 2000. That's a 2000. Wow
[00:59:10.260 --> 00:59:15.740]   It's 20 and that's easy man. Yeah, I can do that. I got better than us
[00:59:15.740 --> 00:59:22.940]   Current home is at 1118th Avenue. They've been there since 2006 an entire city block
[00:59:22.940 --> 00:59:28.340]   It's huge, but now they are built buying the they had some space in the Chelsea market
[00:59:28.340 --> 00:59:33.180]   They're gonna keep the people who are there. I guess won't be kicked out including the retail and food hall
[00:59:33.180 --> 00:59:37.800]   But the first floor is all the retail stuff and nobody wants that to go away including Google employees either
[00:59:37.800 --> 00:59:40.760]   They can eat for free. Yeah, it's very it's very nice
[00:59:40.760 --> 00:59:43.380]   It's really there are other people leasing parts of that building
[00:59:43.380 --> 00:59:48.220]   But I guess Google can we use it as an opportunity to grow within that building. Yeah, that's good
[00:59:48.220 --> 00:59:53.300]   Is it mostly sale? It's mostly sales people though. It's not it's not program. No the the YouTube
[00:59:53.300 --> 00:59:57.220]   Performance space is there. Yeah
[00:59:57.220 --> 01:00:00.900]   No, the New York office. I mean the thing about Google is they they
[01:00:01.620 --> 01:00:04.420]   shift pieces of development around the world. I
[01:00:04.420 --> 01:00:06.940]   Mean Munich and Vienna
[01:00:06.940 --> 01:00:10.040]   Yeah, there's developers New York. Yeah, all right
[01:00:10.040 --> 01:00:14.180]   large teams focusing on projects including search ads
[01:00:14.180 --> 01:00:16.900]   maps YouTube cloud
[01:00:16.900 --> 01:00:21.580]   Technical infrastructure sales and research. So yeah, a lot of it. What else do they announce?
[01:00:21.580 --> 01:00:24.980]   So they have they announced the Google News Initiative. Ah
[01:00:26.620 --> 01:00:32.300]   Well, so that's why that's of interest to nerdy geeky me, but it was an impressive set of
[01:00:32.300 --> 01:00:34.180]   announcements
[01:00:34.180 --> 01:00:35.540]   among them
[01:00:35.540 --> 01:00:40.500]   And they've talked about a little bit of this before but an effort to help with subscription based publications
[01:00:40.500 --> 01:00:44.020]   I always worry about the non subscription based but for those subscription based publications
[01:00:44.020 --> 01:00:48.340]   They launched with 17 pubs including mclatchy New York Times Washington Post
[01:00:48.340 --> 01:00:51.180]   I think little
[01:00:52.940 --> 01:00:55.440]   Reform and Mexico City and others a telegraph and
[01:00:55.440 --> 01:01:02.140]   FT and what it means is that you cut if you have a Google sign-in and Google payment
[01:01:02.140 --> 01:01:05.140]   You come across you hit you hit a challenge to subscribe
[01:01:05.140 --> 01:01:10.620]   At a publication and you can be given models of subscription in two clicks. You're done
[01:01:10.620 --> 01:01:17.980]   You don't have to create a new account. You don't have to create a new passport password or passport. You're using your Google sign-in
[01:01:18.700 --> 01:01:23.820]   Vents force now Google that knows you're a subscriber, but there's a benefit to that Google will also
[01:01:23.820 --> 01:01:26.460]   promote
[01:01:26.460 --> 01:01:32.140]   Content to which you subscribe so you see the carousel of news that occurs now under that there's going to be a my subscription
[01:01:32.140 --> 01:01:39.300]   So if you subscribe to the Guardian and the tell or the Guardian of the telegraph and the FT and the New York Times
[01:01:39.300 --> 01:01:41.540]   Then when you search on a topic
[01:01:41.540 --> 01:01:45.580]   News from those publications that topic will now be promoted to you
[01:01:46.140 --> 01:01:49.580]   So that's good for the publications and one hopes for the user
[01:01:49.580 --> 01:01:53.060]   There'll also a third thing on this is they're going to
[01:01:53.060 --> 01:01:54.900]   predict
[01:01:54.900 --> 01:01:56.420]   using data
[01:01:56.420 --> 01:02:04.100]   They're gonna predict who is a likely mark for your subscription pitch and say hey go after Stacy. She loves you
[01:02:04.100 --> 01:02:08.100]   Now they won't be telling you why they made that decision how they made that decision
[01:02:08.100 --> 01:02:14.020]   They won't be you know sharing the PII with that but they share the conclusion and one hopes they will do that in other areas
[01:02:14.180 --> 01:02:20.060]   So that's a that's a big thing that they did. They're also putting three million dollars into a
[01:02:20.060 --> 01:02:23.500]   News literacy campaign. That's another story
[01:02:23.500 --> 01:02:32.420]   They're also doing a lot about trying to bring AI to newsrooms by making it a lot easier and those same AI stuff that anybody can use
[01:02:32.420 --> 01:02:39.220]   They put money behind first draft which is this wonderful organization that
[01:02:40.420 --> 01:02:46.340]   Did the election land that was a cuny during the last election and that also did the cross check in France during the French election
[01:02:46.340 --> 01:02:51.900]   To really bolster their efforts to for fact checking and quality news
[01:02:51.900 --> 01:02:58.740]   They're saying that finally I think this is finally that they're working in
[01:02:58.740 --> 01:03:02.620]   Breaking new situations. They realize the problem that
[01:03:02.620 --> 01:03:05.340]   Somebody's gonna throw some crap, you know
[01:03:05.740 --> 01:03:09.140]   The young people of Parkland were paid actors and that kind of stuff will come up
[01:03:09.140 --> 01:03:14.380]   So when they see a breaking new situation happening, which is vulnerable to this kind of manipulation
[01:03:14.380 --> 01:03:18.900]   They're gonna then buy us known authoritative sources in
[01:03:18.900 --> 01:03:25.860]   Trending and such so they don't get taken by bozo. That's less a problem on Google News and certainly search
[01:03:25.860 --> 01:03:30.380]   It's much more a problem remains a problem on YouTube and they've got to worry about that
[01:03:30.380 --> 01:03:32.980]   So it was a whole raft of things they say that they're spending
[01:03:33.700 --> 01:03:39.660]   $300 million over the next three years on this there was a little confusion for people from the stage called that a fund
[01:03:39.660 --> 01:03:41.660]   It is not a fund
[01:03:41.660 --> 01:03:43.980]   Europe has a hundred fifty million euro fund
[01:03:43.980 --> 01:03:46.740]   US and the world do not
[01:03:46.740 --> 01:03:50.740]   But Google says they're making substantial investment and in my report
[01:03:50.740 --> 01:03:52.980]   They took out a hip space in New York
[01:03:52.980 --> 01:03:58.700]   They brought up people in from all around the world flying people in from India and and Australia publishers to see this
[01:03:59.260 --> 01:04:05.340]   It was a very interesting and market contrast from the the view toward Facebook and the view toward Google
[01:04:05.340 --> 01:04:13.220]   Right now this week, but Google did a good job at Richard Jingers and company deserve a lot of credit of I think for showing that they
[01:04:13.220 --> 01:04:18.380]   Listen to publishers. They collaborated with publishers. They're trying to help publishers. There's more we want them to do
[01:04:18.380 --> 01:04:22.980]   There's more than they ought to do. That's fine. They're gonna try to improve advertising
[01:04:22.980 --> 01:04:28.220]   They're gonna try to do other things too. I want them to work with us on user data and how to do it right under GDPR
[01:04:29.180 --> 01:04:36.700]   But it's a very good big step to helping news and the report and now Peter Kafka's contrasting point
[01:04:36.700 --> 01:04:43.100]   You said you take me down from my high of a happy world
[01:04:43.100 --> 01:04:47.140]   You gotta bring it back to how are Google and Facebook try to help publishers?
[01:04:47.140 --> 01:04:53.660]   They'll do more to hurt them because that's the way they're supposed to work. They're built to eviscerate publishers
[01:04:53.940 --> 01:05:00.620]   They're aggregators and that's all there is to say about it 300 million dollars over three years
[01:05:00.620 --> 01:05:03.620]   It's about one percent say
[01:05:03.620 --> 01:05:08.420]   Tell you as they did that they succeed when their partner succeed
[01:05:08.420 --> 01:05:14.300]   They need the publishers they need them for the content they need them for the distribution of the ads and they share huge amounts billions of dollars of revenue
[01:05:14.300 --> 01:05:16.540]   with their
[01:05:16.540 --> 01:05:18.540]   distribution partners of their ads and
[01:05:19.060 --> 01:05:26.260]   Yeah, guess what mr. Kafka the world has changed utterly and maybe publishing in journalism are obsolete institutions
[01:05:26.260 --> 01:05:28.340]   That ought to be rethought but we will go into that right now
[01:05:28.340 --> 01:05:32.460]   Well since Peter's not here to defend himself and I don't really have the stomach to do it
[01:05:32.460 --> 01:05:36.540]   I'll just know you don't either do I and certainly not as a stacey Stacey saying when does this end?
[01:05:36.540 --> 01:05:42.620]   Actually, I have the huge story that we coming up in just a second
[01:05:42.620 --> 01:05:47.660]   But real quickly in the related vein subscribe with Google has been launched
[01:05:47.980 --> 01:05:52.420]   That's what I just talked about okay. So that's it if you want to do yeah, that's part of this
[01:05:52.420 --> 01:05:54.020]   So if you want to do it, what do we do?
[01:05:54.020 --> 01:05:58.260]   Do we do we just get the when you when you get hit when you get hit by the solution go through wall?
[01:05:58.260 --> 01:06:02.180]   Yeah, you'll go through Google by the way the FT present. It's a really interesting data
[01:06:02.180 --> 01:06:04.180]   They did a lot of experimentation with Google
[01:06:04.180 --> 01:06:06.180]   Sorry Stacey in two more minutes
[01:06:06.180 --> 01:06:11.540]   Where they saw if they have a big wall FT does financial times does for pay wall
[01:06:11.540 --> 01:06:16.980]   So Google could look at the funnel and who was interested in the FT who was coming in and they could monitor that for the FT
[01:06:16.980 --> 01:06:22.860]   They have to look at the bottom of the funnel and saw that if they so they tried to give people three free pages a month
[01:06:22.860 --> 01:06:27.940]   And what happened and then they went down to one free page a month it pushed up the conversion
[01:06:27.940 --> 01:06:32.380]   But they lost the people coming into the funnel. They lost the prospects. They tried three pages
[01:06:32.380 --> 01:06:36.740]   That was a week or a day, but then they went to a month three a month
[01:06:36.740 --> 01:06:41.220]   And you know it and they played with it and they realized they've got to segregate people into three categories
[01:06:41.220 --> 01:06:45.380]   People who were just coming at the beginning you get lots of free stuff because we want you to get a habit
[01:06:45.740 --> 01:06:50.700]   Then you start getting a little too habitual. We're gonna cut you back and try to push you then you come back all the time
[01:06:50.700 --> 01:06:52.700]   boom, no, you're getting the wall and
[01:06:52.700 --> 01:06:57.900]   And so Google is working with the publishers on this kind of data to figure out how to make that operate
[01:06:57.900 --> 01:06:59.900]   I think it's a good and healthy
[01:06:59.900 --> 01:07:02.140]   partnership
[01:07:02.140 --> 01:07:05.900]   I'm gonna move to a shack in the forest and
[01:07:05.900 --> 01:07:10.980]   Disconnect from everything, but I still think I'll have a working toilet, but other than that
[01:07:10.980 --> 01:07:15.460]   Just moving away from it all no electricity now
[01:07:15.460 --> 01:07:20.700]   Oh no the electric company's spying on me this week with throw
[01:07:20.700 --> 01:07:32.860]   So here's some bad news I actually maligned Congress. I said they can't get anything done in fact
[01:07:32.860 --> 01:07:40.700]   They they have gotten something done. They've passed Cesta Fosta by not oh, okay first of all by 97 to 2
[01:07:40.700 --> 01:07:44.420]   Who are the two who's gonna vote against?
[01:07:44.940 --> 01:07:50.740]   Yeah, child porn and that's where we are but it doesn't in fact it overturned section 230
[01:07:50.740 --> 01:07:55.420]   Which is awful which protects online platforms from liability?
[01:07:55.420 --> 01:08:01.340]   Again, you know for some types of speech by their users putting publishers we might add yeah
[01:08:01.340 --> 01:08:08.220]   So if somebody posts something horrible in a blog comment your the blogger is liable
[01:08:08.220 --> 01:08:11.060]   No longer protective
[01:08:11.300 --> 01:08:17.260]   So what's gonna happen? Let's let's all take a take a relaxing chill pill for a moment what'll happen is
[01:08:17.260 --> 01:08:20.980]   We'll get a use case that is not sex work related
[01:08:20.980 --> 01:08:22.820]   They'll sue
[01:08:22.820 --> 01:08:27.500]   The courts will either carve out an exception or they'll send this back to Congress and say yo
[01:08:27.500 --> 01:08:35.140]   You screwed up fix it. Yeah, and then we'll be back because yes the first time someone comes in and I mean
[01:08:35.140 --> 01:08:38.980]   We'll see a good test case and it is painful. I'm not saying this is ideal
[01:08:38.980 --> 01:08:44.600]   But this is kind of where we are as a as a people by the way the law is retroactive
[01:08:44.600 --> 01:08:51.080]   So even more opportunity for you to sue
[01:08:51.080 --> 01:08:57.100]   So yeah, it's aimed at sex trafficking but
[01:08:57.100 --> 01:09:03.300]   Of course when you overturn this safe harbor protection it means all sorts of stuff
[01:09:04.340 --> 01:09:09.980]   Sad to say IBM and Oracle companies whose business models don't rely on 230 were quick to jump
[01:09:09.980 --> 01:09:13.780]   On board the bill and support it. So was the internet association?
[01:09:13.780 --> 01:09:21.020]   Because big internet companies aren't gonna have too much trouble. It's the little the little guys are gonna have to put up with this
[01:09:21.020 --> 01:09:26.900]   Yeah, we'll watch this is EFF's article if you want to read more about it and understand
[01:09:26.900 --> 01:09:30.700]   It is it is passed through both the House and Senate
[01:09:30.700 --> 01:09:36.780]   So it just remains to be signed before it becomes a law Elliot Harmon how Congress censored the internet published today
[01:09:36.780 --> 01:09:42.540]   On EFF dot org if we're because we're paying attention to all we're paying attention to the wrong thing sometimes
[01:09:42.540 --> 01:09:44.540]   But we have a way to do
[01:09:44.540 --> 01:09:48.700]   Pay no attention to this Sesta Fosta look at it look at Facebook
[01:09:48.700 --> 01:09:56.780]   Okay, I was thinking about look who's been fired lately from you know the cabinet slash White House well that too, right?
[01:09:56.780 --> 01:09:58.780]   That's a nice. That's a great circus
[01:09:59.580 --> 01:10:01.580]   Thank you Twitter
[01:10:01.580 --> 01:10:08.940]   Google actually this is an interesting story Google has created an open source VPN anyone can install
[01:10:08.940 --> 01:10:18.580]   But there's one guy who's not too happy about it the guy who invented it in the first place or something an awful lot like it
[01:10:18.580 --> 01:10:20.580]   Dan Guido launched a
[01:10:20.580 --> 01:10:22.660]   project
[01:10:22.660 --> 01:10:29.340]   That gave you call that gave you a VPN in easy to set up VPN in late 2016
[01:10:30.180 --> 01:10:32.180]   that
[01:10:32.180 --> 01:10:36.660]   Is very similar, but I don't know maybe who knows the more the merrier, right?
[01:10:36.660 --> 01:10:41.060]   So it's well Google's is called outline you could run it on your own server
[01:10:41.060 --> 01:10:44.500]   But if you don't have a server digital ocean makes it like a one button
[01:10:44.500 --> 01:10:48.740]   install rack space Google cloud engine EC2 and
[01:10:48.740 --> 01:10:57.140]   It'd be easy enough to set up and and then have a VPN that you could trust and as we've learned a lot of these big VPNs are not so trustworthy
[01:10:57.140 --> 01:10:59.140]   as we thought
[01:10:59.140 --> 01:11:06.540]   You know what I would like in I don't know if this works, but you know my my hate the reason I don't have a VPN
[01:11:06.540 --> 01:11:09.780]   I don't hate it. That's way too much, but it breaks a lot of my
[01:11:09.780 --> 01:11:18.820]   Geographically based functionality so my television my Amazon echo and so I wish there was like a VPN plus a masking service that
[01:11:18.820 --> 01:11:25.020]   Said hey, so I could say hey I want to be indicated that I'm in Austin or I'm in Texas
[01:11:25.020 --> 01:11:27.540]   At this actually
[01:11:27.940 --> 01:11:34.820]   Tunnel bearer, okay, and you could do it with you say where you want to be you could do it with this if you ran this VPN on a
[01:11:34.820 --> 01:11:41.020]   Home server it would identify itself is coming from the house and then you log into it from where the outside world
[01:11:41.020 --> 01:11:45.040]   The idea is it protects you when you're out and about but still gives you you know
[01:11:45.040 --> 01:11:51.260]   Locally I want something on my house that protects me against ISPs looking at all of my data for my connected devices
[01:11:54.900 --> 01:12:01.180]   Yeah, I'll bear might do that although yeah, I don't know I just don't want to wait. Okay. Yeah, this is
[01:12:01.180 --> 01:12:08.340]   This is because I tried VPNs in the past and I'm like oh I always try many aspects of my life slows everything down
[01:12:08.340 --> 01:12:14.300]   Yeah, Leo. I think they were pushing this VPN thing as a way for journalists to be able to well
[01:12:14.300 --> 01:12:19.620]   That's one of the uses for sure right yes. Yes, absolutely. It's all encrypted. It's a privacy tool
[01:12:19.620 --> 01:12:26.900]   Well, it's I maybe I shouldn't assume that everybody knows what a VPN is but the idea is it creates an encrypted tunnel between you and the server
[01:12:26.900 --> 01:12:29.140]   so that where you are is
[01:12:29.140 --> 01:12:36.780]   Hidden and and the data you're exchanging with the outside world is hidden until it gets to the server at that point has to enter the
[01:12:36.780 --> 01:12:40.300]   Outside world that's why if you run it out of your house. It'll look like it's coming from your house
[01:12:40.300 --> 01:12:47.660]   And many people use commercial VPNs, but the problem is we it's hard to know what their privacy policies really are
[01:12:47.660 --> 01:12:54.460]   We've we just yesterday on security now to report on I think three different well-known VPNs that we're leaking information
[01:12:54.460 --> 01:12:56.820]   I p address information stuff like that. I
[01:12:56.820 --> 01:13:03.220]   Think they talk about news again because I have to go do something really fast. Okay, go do it and I will talk about
[01:13:03.220 --> 01:13:10.300]   Let's let's talk about what Stacy really disagrees with man. All right, how about I like this at South by Southwest
[01:13:10.300 --> 01:13:17.500]   YouTube announcing that they're going to use wiki PDF to to verify claims on conspiracy theory and other YouTube
[01:13:17.660 --> 01:13:23.620]   Videos they're gonna link to what they didn't fit about it. They're gonna link to them
[01:13:23.620 --> 01:13:27.740]   When did you end it anybody ever say hello Wikipedia? I'm gonna link to you, right?
[01:13:27.740 --> 01:13:34.820]   Okay, I don't understand what the fuss is I think it's an example of people's thought they're trying to find any angle
[01:13:34.820 --> 01:13:37.460]   And there's there's problems on YouTube
[01:13:37.460 --> 01:13:43.140]   There's go problems galore and Wikipedia is probably not the solution, but linking to Wikipedia is not an evil
[01:13:43.300 --> 01:13:49.540]   No, I think maybe it was the way Susan was just he said as if it was a partnership as
[01:13:49.540 --> 01:13:52.020]   opposed to
[01:13:52.020 --> 01:13:56.700]   Just maybe linking to it. They don't even do partnerships. You linked to them. Yeah
[01:13:56.700 --> 01:14:02.300]   But Google's been doing that for ages. She said well, I wish I could find the quote
[01:14:02.300 --> 01:14:06.660]   It's the New York Times. It says she she would enlist Wikipedia's help
[01:14:06.660 --> 01:14:12.020]   It's not exactly if you link to it. It's not really enlisting their help that no, it's not
[01:14:12.020 --> 01:14:14.620]   No, but maybe that's the New York Times. I don't have the
[01:14:14.620 --> 01:14:16.740]   Yeah, I don't have the quote in front of me
[01:14:16.740 --> 01:14:24.740]   So she says YouTube will soon begin experimenting with what it calls quote information cues and quote sourced from Wikipedia
[01:14:24.740 --> 01:14:29.420]   The cues would appear as captions and article links beneath videos that deal with topics
[01:14:29.420 --> 01:14:33.940]   related to popular conspiracy theories like the moon landing and chem trails
[01:14:33.940 --> 01:14:38.980]   People who were interested in chem trails wanted to know more about the moon landing I
[01:14:41.300 --> 01:14:45.420]   Don't know did you see his date of to Vescue's piece a week ago? I don't we talk about last week
[01:14:45.420 --> 01:14:49.180]   I thought it up. But maybe it was on Twitter. Yeah, I thought it was really good
[01:14:49.180 --> 01:14:54.900]   No, actually we did talk about it. Never mind. Never mind. We did we did talk about it here. We did talk about it. Yep. Yep. Yep
[01:14:54.900 --> 01:14:59.280]   Remind me again what she was talking about I feel saying that that it
[01:14:59.280 --> 01:15:03.100]   intensifies interest and so if you started vegetarian things you
[01:15:03.100 --> 01:15:09.140]   Escalates your escalates which in one way can make sense. Oh, you're interested in that
[01:15:09.140 --> 01:15:11.420]   I'll give you more and deeper the other way
[01:15:11.420 --> 01:15:16.720]   It's obviously bad, but but that's the thing about all of these algorithmic things. They all make sense
[01:15:16.720 --> 01:15:24.180]   Absolutely to the algorithm. It's all about increasing engagement if the algorithm is to get more clicks that works
[01:15:24.180 --> 01:15:27.140]   To deliver
[01:15:27.140 --> 01:15:31.940]   Well, and yeah, it delivers value the problems computers are terrible at context
[01:15:31.940 --> 01:15:33.700]   So I can walk into a library
[01:15:33.700 --> 01:15:38.180]   We actually just a few days ago walked into the library my daughter and I and my daughter asked
[01:15:38.900 --> 01:15:42.220]   For a book for young adults. She's like, I'm really into mermaids
[01:15:42.220 --> 01:15:48.420]   Do you have any books on mermaids? And so the lady brought us to some graphic novels about mermaids and some other stuff and then
[01:15:48.420 --> 01:15:56.700]   My parents are great. They are great. But if my you know, so but she has YouTube for weird
[01:15:56.700 --> 01:16:03.300]   Sex things about mermaids because she knows there's you tubes like oh, you like mermaids a lot of you
[01:16:05.540 --> 01:16:10.020]   Exactly right and that's the problem is that the algorithm is doing everything right. It's doing what it's supposed to do
[01:16:10.020 --> 01:16:16.780]   It's just that the consequences or the side effects are not well understood or thought about and they just sort of human context
[01:16:16.780 --> 01:16:20.700]   Humans, I mean computers have no common sense. They know their computers
[01:16:20.700 --> 01:16:26.980]   I don't this I'm sure this is at a context, but another South by revelation
[01:16:26.980 --> 01:16:31.700]   This one from layer Liar Cohen the YouTube's global head of music
[01:16:33.220 --> 01:16:35.740]   the people who treat music
[01:16:35.740 --> 01:16:41.660]   YouTube like a music service passively listening for long periods of time are going to encounter more ads
[01:16:41.660 --> 01:16:47.900]   You're not gonna be happy after you're jamming to stairway to heaven and you get an ad right after that
[01:16:47.900 --> 01:16:51.780]   We're doing that he says so you'll subscribe so you'll pay
[01:16:51.780 --> 01:16:58.140]   The headline YouTube will frustrate some users with ads so they pay for music. I see nothing wrong with this
[01:16:58.140 --> 01:17:00.780]   Okay, good
[01:17:00.780 --> 01:17:02.780]   Right nothing. I mean no
[01:17:03.100 --> 01:17:06.620]   Hey, you've been listening for four hours. We're gonna give you some more ads till you pay
[01:17:06.620 --> 01:17:10.180]   That's business. I yeah
[01:17:10.180 --> 01:17:17.540]   Sorry, I can't see why someone would be frustrated by this. I can see why someone might be like dude that sucks, but I yeah
[01:17:17.540 --> 01:17:23.740]   Yeah, I pay for it. I pay for it. I'm the first to give you something free. It's not free the new service
[01:17:23.740 --> 01:17:27.180]   Well frustrate and seduce users of YouTube's free service
[01:17:27.180 --> 01:17:30.180]   Says go straight and seduce
[01:17:30.180 --> 01:17:32.180]   The incident
[01:17:32.180 --> 01:17:36.500]   Stick that's the that's the whole thing. I guess a stick in a carrot. It's just like life
[01:17:36.500 --> 01:17:39.220]   Life
[01:17:39.220 --> 01:17:49.860]   They will in time Cohen says appreciate the advertising everyone is drunk on the growth of subscription
[01:17:49.860 --> 01:17:54.340]   I don't know what late later was smoking before I went up on stage
[01:17:56.460 --> 01:18:02.460]   Okay, YouTube so amber yes, go ahead uber uber. I find the story
[01:18:02.460 --> 01:18:09.300]   Yeah, this is really interesting the way it's being headed exactly a self-driving uber
[01:18:09.300 --> 01:18:12.940]   Night before last
[01:18:12.940 --> 01:18:18.140]   Sadly killed in Arizona pedestrian it was about 10 p.m. In the dark
[01:18:18.140 --> 01:18:23.260]   The uber car traveling at 40 miles an hour and a 35 mile an hour zone had a safety driver
[01:18:23.820 --> 01:18:26.540]   Neither the safety driver nor the car saw the woman
[01:18:26.540 --> 01:18:30.660]   Or stopped they didn't try it. There's no evidence that break at all
[01:18:30.660 --> 01:18:37.300]   Woman was struck and killed she was walking her bike across a four lane street not in the crosswalk
[01:18:37.300 --> 01:18:39.940]   after reviewing the video
[01:18:39.940 --> 01:18:44.060]   Tempe police told the San Francisco Chronicle. No one
[01:18:44.060 --> 01:18:51.020]   Could have avoided this driver or a self-driving car. She came out of the shadows so suddenly
[01:18:51.020 --> 01:18:53.380]   There was nothing anybody could do about it
[01:18:53.940 --> 01:18:57.140]   Of course because it's self-driving vehicle that killed a human
[01:18:57.140 --> 01:19:01.260]   Uber immediately suspended all of its self-driving vehicle research
[01:19:01.260 --> 01:19:03.820]   indefinitely and
[01:19:03.820 --> 01:19:08.340]   I imagine I haven't seen it but people are calling for the end of self-driving vehicles. I don't know
[01:19:08.340 --> 01:19:11.380]   Do you blame the don't say that don't say that?
[01:19:11.380 --> 01:19:15.740]   That that was massive speculation that was bad behavior right there
[01:19:15.740 --> 01:19:22.500]   I'm sure somebody is I just don't have anybody in front of me doing that but well, let's let's not give them the time of day
[01:19:22.500 --> 01:19:26.620]   Do you you don't think anybody's saying oh see this is why we shouldn't have self-driving vehicles?
[01:19:26.620 --> 01:19:34.540]   That's kind of the tenor of all the coverage was well, that's that's this what we're setting expectations for that's
[01:19:34.540 --> 01:19:40.740]   On the media. Mm-hmm like oh someone died. I think people would be legitimately freaked out
[01:19:40.740 --> 01:19:44.980]   I think it's fine to say hey this happened and we don't love it
[01:19:44.980 --> 01:19:50.780]   Let's let's talk about what happened right just like if someone hits a child. That's terrible
[01:19:50.780 --> 01:19:54.220]   Well, you know, that's the next thing that's gonna happen at Thomas vehicle
[01:19:54.220 --> 01:19:57.660]   Well at some point some kid will dart out in front of an autonomous vehicle hit a kid and
[01:19:57.660 --> 01:20:01.620]   You think that people will just smart enough to say well there nothing you can do about that
[01:20:01.620 --> 01:20:07.860]   Well, will we also do this to the statistical view is that in five years when there's half the cars or tons of vehicles
[01:20:07.860 --> 01:20:14.660]   And we've gone from six thousand pedestrian deaths to three thousand will anyone notice the first thing I thought when I heard the story
[01:20:14.660 --> 01:20:17.900]   And I didn't hear anything more than the self-driving car hit somebody was
[01:20:18.460 --> 01:20:21.940]   there's only two possibilities the self-driving car malfunctioned or
[01:20:21.940 --> 01:20:25.820]   She jumped, you know, she was not visible
[01:20:25.820 --> 01:20:29.580]   she came out from between two cars or something and there was no way it could stop and I
[01:20:29.580 --> 01:20:35.380]   Because I trust the cameras and I trust the braking and I trust all that much more than I trust a human to
[01:20:35.380 --> 01:20:39.940]   Notice it and respond appropriate the cars can respond immediately as soon as it notices it
[01:20:39.940 --> 01:20:44.820]   So I actually do trust the self-driving car more than the human that did you have a sense of?
[01:20:46.460 --> 01:20:49.740]   Uber car versus Google car who would have done a better job?
[01:20:49.740 --> 01:20:59.020]   Well, I thought that uber I didn't think about its technical prowess, but I was like oh, it'll be interesting to see how uber handles yet another blow to its
[01:20:59.020 --> 01:21:02.300]   Well, that's the point. Yeah, so maybe because of it's because it's uber
[01:21:02.300 --> 01:21:05.020]   But it wouldn't be ahead. I mean
[01:21:05.020 --> 01:21:11.140]   The same day. I'm sure cars hit a hundred other pedestrians in six thousand pedestrian dusty here. Okay
[01:21:13.140 --> 01:21:20.100]   20 a day. Oh it is. Oh, oh, absolutely. Well, it is but it happens 20 times a day. You don't see the headline 20 times a day
[01:21:20.100 --> 01:21:22.140]   Well, okay
[01:21:22.140 --> 01:21:29.660]   It's a legitimate headline because this is a new thing right what you're trying to do is shade it in this tone that I don't think a lot of people
[01:21:29.660 --> 01:21:35.700]   I'm wrong you're implying a tonality that I'm sure the occasional person might have but I don't think that's the mass
[01:21:35.700 --> 01:21:38.780]   Nobody blames the autonomous vehicle for this. This is just normal
[01:21:39.500 --> 01:21:45.140]   Well, I think people are like hey, this happened. What happened? Let's get information. That's the first thing I was like whoa
[01:21:45.140 --> 01:21:49.220]   This is a big deal because this is the first thing that happened. How did it happen?
[01:21:49.220 --> 01:21:51.940]   and
[01:21:51.940 --> 01:21:54.620]   Once I found out the car was driving at 40 miles an hour
[01:21:54.620 --> 01:22:01.060]   I had a friend who actually hit somebody who was crossing a major thoroughfare at 10 o'clock at night outside of any
[01:22:01.060 --> 01:22:05.460]   Crosswalk and you know what my friend hit her and it was horrible for her right?
[01:22:06.940 --> 01:22:09.860]   You know this this was the kind of thing that could not be
[01:22:09.860 --> 01:22:14.380]   Well on the bright side the card isn't gonna feel bad at an autonomous vehicle. Oh
[01:22:14.380 --> 01:22:20.460]   No, I'm sure the safety driver feels pretty bad. The safety driver probably does feel bad. Yeah
[01:22:20.460 --> 01:22:23.100]   He says the first thing I knew we're all the same thing
[01:22:23.100 --> 01:22:29.420]   I noticed there's cameras everywhere. So the first thing I know I didn't know anything was wrong until I heard the us hit her
[01:22:29.420 --> 01:22:34.100]   So it's very sad. I mean, it's very sad. I'm not I'm not in anywhere
[01:22:35.260 --> 01:22:37.620]   It's worse because she was apparently homeless and
[01:22:37.620 --> 01:22:42.980]   She she had a bike loaded up with stuff and wasn't paying attention
[01:22:42.980 --> 01:22:49.220]   Obviously she went right in front of a car and it makes it but but at all but also the other problem is always a homeless person
[01:22:49.220 --> 01:22:53.540]   I actually didn't know she was much. I actually had a sense of that. Okay. Yeah
[01:22:53.540 --> 01:22:59.820]   Well, and so here's some interesting things to think about right when you have this sort of thing happen in
[01:23:00.660 --> 01:23:07.700]   In an autonomous car. We're gonna have instant data about what that's true. We have video from front back. Mm-hmm good point
[01:23:07.700 --> 01:23:14.540]   Yeah, and so so there's interesting things about liability, but also like right now, you know the NT
[01:23:14.540 --> 01:23:20.420]   National Transportation Safety Board NTSB is out there those things take like 18 months to resolve
[01:23:20.420 --> 01:23:28.580]   So when can we start expecting the speed of investigations on these things to you know have that information come out in an authoritative way?
[01:23:29.380 --> 01:23:33.660]   That hasn't happened before and I think that'll start shading coverage a little bit too. I
[01:23:33.660 --> 01:23:39.100]   Actually, here's a great. I really actually like this take from curbed
[01:23:39.100 --> 01:23:47.300]   Really the problem is that we've built our cities in favor of cars more than humans. Yes
[01:23:47.300 --> 01:23:50.500]   The cities are not pedestrian friendly at all
[01:23:50.500 --> 01:23:55.660]   And if you've ever been in these Arizona streets, they're big why there's a four lane
[01:23:56.180 --> 01:24:01.340]   Yeah, you know street through a city through a town. I mean that's there are all these big streets
[01:24:01.340 --> 01:24:05.420]   This is a I mean look at this intersection. This is this is an intersection that favors
[01:24:05.420 --> 01:24:10.300]   This is a browser that doesn't favor me
[01:24:10.300 --> 01:24:13.820]   I hate chrome
[01:24:13.820 --> 01:24:18.460]   Or is this edge? I hate Jesus you're hating everything now. I'm hanging everything. Well. What is doing?
[01:24:18.460 --> 01:24:25.340]   I can never show anything anymore. It's just it just freaks. It's only on Twitter. It freaks out. It doesn't like Twitter for some reason
[01:24:26.340 --> 01:24:28.780]   I'll zoom in here. Maybe maybe
[01:24:28.780 --> 01:24:35.940]   But I mean this is clearly an intersection that favors cars. Yeah humans, right? Was it let's count that
[01:24:35.940 --> 01:24:42.100]   One two three four five lanes just in that one part. Yeah. Yeah
[01:24:42.100 --> 01:24:45.660]   So I think that actually is a
[01:24:45.660 --> 01:24:48.020]   If I would not mind that story
[01:24:48.020 --> 01:24:54.860]   If I was worried that there would be all this and you're right there hasn't been so I was worried for no reason
[01:24:54.980 --> 01:24:59.740]   Did they be backlash against autonomous vehicles because of this but there it doesn't seem to be?
[01:24:59.740 --> 01:25:05.780]   Moving right along
[01:25:05.780 --> 01:25:10.700]   I saw some other things that were interesting I think
[01:25:10.700 --> 01:25:13.860]   What was it?
[01:25:13.860 --> 01:25:15.740]   Amazon
[01:25:15.740 --> 01:25:23.120]   How I thought the numbers is not a number but the numbers on how what video has done for Amazon a leaked memo
[01:25:23.660 --> 01:25:25.660]   from 2016
[01:25:25.660 --> 01:25:34.540]   Says that man in the high castle which cost 72 million to produce Wow drew in 1.15 million prime subscribers
[01:25:34.540 --> 01:25:42.900]   Right so that was a subscriber acquisition cost of 63 dollars for 99 dollars cash from the US and God knows what it paid off right away
[01:25:42.900 --> 01:25:45.420]   Yeah, yeah, so you see why why profit
[01:25:45.420 --> 01:25:47.980]   content is
[01:25:47.980 --> 01:25:50.260]   Has value again if it sells subscriptions
[01:25:50.260 --> 01:25:54.100]   This is actually also an interesting graph from that internal document
[01:25:54.100 --> 01:25:58.580]   Amazon evaluates TV shows by their cost per first stream
[01:25:58.580 --> 01:26:01.900]   The price to hook a customer on prime
[01:26:01.900 --> 01:26:06.860]   They divide the shows production and marketing expenses by the number of people who stream that program first
[01:26:06.860 --> 01:26:11.100]   After signing up. So if you sign up for prime the first show you stream
[01:26:11.100 --> 01:26:18.860]   So so the grand lower is better obviously grand tour number one then the man in the high castle number two
[01:26:19.220 --> 01:26:21.940]   Bosch these are presumably shows people
[01:26:21.940 --> 01:26:26.780]   Signed up to prime for although. I don't think that's always the case for instance
[01:26:26.780 --> 01:26:31.300]   Bosch shows up with a big logo whenever I go to Amazon Prime
[01:26:31.300 --> 01:26:33.900]   So that may be why I got a lot of hits hand of God
[01:26:33.900 --> 01:26:37.820]   It's a book people might who like the book might have might have gone there
[01:26:37.820 --> 01:26:42.460]   I'm curious how much of these shows are based on kind of books or reader type
[01:26:42.460 --> 01:26:47.540]   You know like people the grand true brought people in because the BBC canceled it and it moved there
[01:26:47.540 --> 01:26:51.500]   And I had a built-in audience man in the high castle was a book got a lot of attention
[01:26:51.500 --> 01:26:54.820]   It's not very good nor is Bosch
[01:26:54.820 --> 01:27:00.420]   In my opinion my humble opinion hand of God Goliath moats art in the jungle
[01:27:00.420 --> 01:27:05.260]   Bosch season two man in the high castle season two that's interesting season one
[01:27:05.260 --> 01:27:11.620]   Did much better for both of these sneaky Pete and good girls revolt if it's the first thing you stream
[01:27:11.620 --> 01:27:14.820]   I imagine that you would do season one first you sign up
[01:27:14.820 --> 01:27:16.820]   You don't just smart
[01:27:16.820 --> 01:27:20.580]   Of course, you don't start in the middle. I
[01:27:20.580 --> 01:27:26.460]   Don't know maybe you don't start in the middle you start at the beginning
[01:27:26.460 --> 01:27:33.980]   Course see this is why computers will never be better than Stacy
[01:27:33.980 --> 01:27:40.900]   There's you there's your there's your title of the show. Yeah, it's not it's it's obvious
[01:27:40.900 --> 01:27:46.580]   I you know you ask me for some mermaid videos. I'll show you some mermaid videos. I'm not gonna try
[01:27:46.580 --> 01:27:51.100]   Amazon's adding a fingerprint reader to its key app
[01:27:51.100 --> 01:27:55.940]   That's not for the person who's got access to your home. That's for you
[01:27:55.940 --> 01:28:01.380]   So is it for the app on so is it just it's only on Android?
[01:28:01.380 --> 01:28:06.360]   Yeah, yeah, so somebody can't get your phone and let people into the house. I guess
[01:28:07.100 --> 01:28:10.720]   Yay, I still would not let it Amazon deliver
[01:28:10.720 --> 01:28:16.140]   My house no way Google's now letting you download a handful of programs
[01:28:16.140 --> 01:28:23.060]   Without downloading them or maybe I should say it's only if it's fuchsia, right Dan down. No, no, no
[01:28:23.060 --> 01:28:29.820]   This is this is down. Yeah, it's basically PWA's it's actually something Google announced long before that
[01:28:29.820 --> 01:28:31.820]   Remember they announced it at Google I owe a couple of years back
[01:28:32.420 --> 01:28:38.260]   The idea that Google plans to instant instant game instant and now it's games. They've had some other instant apps
[01:28:38.260 --> 01:28:45.900]   So essentially it's like try before you download final fantasy 15 a new empire panda pop
[01:28:45.900 --> 01:28:48.180]   Clash Royale
[01:28:48.180 --> 01:28:55.060]   It's a big deal because in the past these if I these you know instant apps were too limited to two megabytes games
[01:28:55.060 --> 01:28:59.620]   Two megabytes isn't enough so they've increased the maximum file size to 10 megabytes
[01:29:01.700 --> 01:29:06.340]   You know, it's funny the cuz they announced this so long ago and it's finally kind of materializing
[01:29:06.340 --> 01:29:09.140]   But PWA I think in the long run is going to replace this
[01:29:09.140 --> 01:29:11.420]   Video again
[01:29:11.420 --> 01:29:13.860]   Progressive web apps. Oh, okay. Thank you
[01:29:13.860 --> 01:29:16.180]   I'm like a page that acts like an app because the
[01:29:16.180 --> 01:29:22.620]   Right the the one makes the web page act more like an app the other makes the app act more like a web page
[01:29:22.620 --> 01:29:26.020]   But they basically come together and it but isn't that the essence of fuchsia?
[01:29:26.500 --> 01:29:32.060]   This whole notion of a drawer and downloading is now just gone and you just do what you want to do when you want to do it a
[01:29:32.060 --> 01:29:36.420]   System built for instant apps on steroids according to nine to five Google
[01:29:36.420 --> 01:29:44.660]   Fuchsia which we're still not kind of really clear what the plan is for fuchsia. Does it replace Android? Does it replace?
[01:29:44.660 --> 01:29:47.060]   Chrome OS
[01:29:47.060 --> 01:29:51.580]   What is it? What does it do mostly because the fuchsia today is not very capable?
[01:29:51.980 --> 01:29:56.900]   And but you do run it on a Chromebook, right? So maybe it's your place Chrome OS. I don't know
[01:29:56.900 --> 01:30:02.500]   Oh, I was hoping for explanation. I saw the story and I was like, oh, I can't wait for that you guys to explain fuchsia to me
[01:30:02.500 --> 01:30:05.460]   It's just another operating system from Google. It's early days yet
[01:30:05.460 --> 01:30:13.060]   So the most important thing though is these processes will be completely transparent looks like Google is building fuchsia
[01:30:13.060 --> 01:30:18.900]   This is nine to five Google writing so that when users know what they want to do next fuchsia will be happy to accommodate
[01:30:18.900 --> 01:30:21.100]   You don't have to have the app
[01:30:21.380 --> 01:30:24.940]   This is all kind of modern. I think fuchsia really is designed
[01:30:24.940 --> 01:30:31.580]   Primarily to be a modern OS and I think this is actually smart of of Google to be doing this. I
[01:30:31.580 --> 01:30:35.500]   Don't know if other companies are doing the same thing
[01:30:35.500 --> 01:30:41.420]   But I'm sure every company has to have some sort of effort like this because it takes forever to write an operating system
[01:30:41.420 --> 01:30:47.460]   A build from scratch operating system which fuchsia is it's already three or four years old and it's not even close to being a real OS
[01:30:47.780 --> 01:30:52.980]   So I mean think about chip architectures arms been working for five years on our brand new architecture
[01:30:52.980 --> 01:30:54.660]   And we won't even see it in products forever
[01:30:54.660 --> 01:31:00.220]   So an OS is I mean the closer you get to the bone the bare metal the closer
[01:31:00.220 --> 01:31:07.260]   Your buildings we've seen a new OS in the world, right? We used to have a time. Yeah forever
[01:31:07.260 --> 01:31:17.740]   Last you saw POS 20 years ago, I can't think of my Mac OS has been around longer windows has been around
[01:31:17.740 --> 01:31:19.100]   longer
[01:31:19.100 --> 01:31:24.300]   Linux has been around since 1992. I mean they all evolve and change dramatic. Yeah. Yeah, yeah
[01:31:24.300 --> 01:31:27.660]   But so the red red hat maybe sold
[01:31:27.660 --> 01:31:30.220]   sold to whom
[01:31:30.220 --> 01:31:34.100]   Maybe maybe Google was that I saw that oh that's interesting
[01:31:34.100 --> 01:31:39.980]   That's really interesting is really interesting not red hats Google
[01:31:39.980 --> 01:31:43.500]   Hats for sale
[01:31:45.420 --> 01:31:47.420]   I wouldn't be looking for red hats
[01:31:47.420 --> 01:31:54.020]   Red Hat could be a Google target take over me. It's purely speculation interesting. We'll wrote that article
[01:31:54.020 --> 01:32:01.460]   This one is from WRAL tech wire. Oh, we're red hat. Oh, hold on. No, the author in Raleigh. Let's see if they're incredible
[01:32:01.460 --> 01:32:07.860]   It's that's the local TV station where red hat is now the Raleigh Durham triangle
[01:32:07.860 --> 01:32:12.340]   Bloomberg news where this is a rewrite of a blunder
[01:32:12.820 --> 01:32:19.660]   Okay, so here we go. I trust Bloomberg. Well, I can tell you if who wrote it at Bloomberg. I'll be I'll tell you who
[01:32:19.660 --> 01:32:27.580]   In the dancing smart they are in the dancing on their grave department Amazon's considering buying some Toys R Us stores. I
[01:32:27.580 --> 01:32:32.940]   Don't think this is a dancing on there. I mean, yes, it's it's crap to go to Toys R Us
[01:32:32.940 --> 01:32:37.740]   But part of that is just it was crap to go to Toys R Us. It's always easier to buy it from Amazon
[01:32:37.740 --> 01:32:41.940]   There've been a lot of articles recently saying don't blame Amazon for Toys R Us bankruptcy
[01:32:42.220 --> 01:32:50.060]   They I do think there is a there's a real interesting issue though with private equity firms buying into
[01:32:50.060 --> 01:32:54.780]   Retail chains with physical, you know physical
[01:32:54.780 --> 01:32:59.340]   stores inventory all of this and then putting a lot of debt
[01:32:59.340 --> 01:33:07.540]   Do it. Yeah, same thing happened to media. This is that could happen to me. It's very it's why cumulus and very nearly my own company
[01:33:07.540 --> 01:33:10.740]   I heart media I already both
[01:33:11.860 --> 01:33:18.900]   Cumulus did do chapter 11. I was this close to chapter 11. They were able to negotiate renegotiate their deals with
[01:33:18.900 --> 01:33:24.020]   Lenders, but yeah, these companies are were hugely leveraged
[01:33:24.020 --> 01:33:27.140]   Right and they used to make sense in a world where
[01:33:27.140 --> 01:33:31.940]   There were not options, but it's like so I think there's actually a cool story here
[01:33:31.940 --> 01:33:37.780]   And I know some people have reported a bit on it is what is the when your distribution costs come
[01:33:38.220 --> 01:33:42.820]   Closer to zero you become more flexible and less reliant on physical infrastructure
[01:33:42.820 --> 01:33:48.580]   From a store or printing press for example way
[01:33:48.580 --> 01:33:55.460]   What kind of model works for a private equity firm going forward? I don't know I
[01:33:55.460 --> 01:33:59.700]   Think I'm my money's on crypto kitties
[01:33:59.700 --> 01:34:02.860]   12 million bucks
[01:34:02.860 --> 01:34:08.060]   I put this one up there. I was amazed by it and look at who invested in
[01:34:09.060 --> 01:34:14.420]   Recent Horowitz you just wear ventures Wow they so crypto kitties is a blockchain game
[01:34:14.420 --> 01:34:21.780]   Where it's like a virtual creature game where you create a virtual create actually you buy virtual creatures and then deck them out
[01:34:21.780 --> 01:34:27.460]   And then sell them and crypto kitties have sold for as much as 200 thousand dollars
[01:34:27.460 --> 01:34:30.340]   Virtual they're virtual they're not real
[01:34:30.340 --> 01:34:37.540]   It's really all speculation right in fact at one point they were so popular they accounted for 30% of the volume of trading and Ethereum
[01:34:38.540 --> 01:34:44.820]   Yeah found former founder of Coinbase CEO and founder of angeles mark pinkis from Zinga
[01:34:44.820 --> 01:34:51.620]   Some some big name investors putting 12 million dollars into crypto kitties there
[01:34:51.620 --> 01:34:59.060]   Here's the here's the story from a venture be crypto kitties isn't much when it comes to gameplay
[01:34:59.060 --> 01:35:02.900]   You can buy a kitten with aetherium and hold on to it
[01:35:02.900 --> 01:35:08.700]   You can breed kittens which have different visual features dubbed traits some of the traits are rarer than others
[01:35:08.700 --> 01:35:14.420]   There are thousands of kittens in the game some of them breed fast others are slower crypto kitty decks
[01:35:14.420 --> 01:35:17.020]   keeps track of trains
[01:35:17.020 --> 01:35:20.980]   Trains trains traits. I think he means and how common are rare they are
[01:35:20.980 --> 01:35:24.180]   So it's just a silly
[01:35:24.180 --> 01:35:28.020]   Little game worth 12 million dollars
[01:35:31.020 --> 01:35:33.020]   tamper proof
[01:35:33.020 --> 01:35:35.220]   cyber currency wallet
[01:35:35.220 --> 01:35:39.620]   Hardware wallet ledger just got back doored by a 15 year old
[01:35:39.620 --> 01:35:46.220]   Well, maybe they'd be able to find your password that way now I'm in trouble with section 230 all without doing anything
[01:35:46.220 --> 01:35:49.340]   Yeah, maybe my wallet's in there
[01:35:49.340 --> 01:35:53.660]   15 year old from the UK
[01:35:53.660 --> 01:35:55.340]   Salim Rashid
[01:35:55.340 --> 01:36:01.780]   Published on his personal blog a proof of concept code that allowed him to backdoor the ledger nano s a hundred dollar hardware wallet
[01:36:01.780 --> 01:36:11.180]   That the company's marketers say has sold by the millions. Oh boy. It's a 300 byte long proof of concept
[01:36:11.180 --> 01:36:17.460]   Wow good going I see now I have hope for the future
[01:36:17.460 --> 01:36:23.220]   Go kid young people liked like this like Rashid
[01:36:23.220 --> 01:36:30.020]   AMD says we've now we talked a little bit been talking about this weird
[01:36:30.020 --> 01:36:33.220]   You know a bunch of flaws on AMD
[01:36:33.220 --> 01:36:40.660]   chips and and the speculation that the Israeli security firm CTS Labs which seems to have been created entirely
[01:36:40.660 --> 01:36:44.580]   To publicize this issue. They didn't they don't have any track record
[01:36:44.580 --> 01:36:52.660]   And only gave AMD 24 hour notice and apparently tried to make money on the revelation by shorting AMD stock
[01:36:53.460 --> 01:36:57.380]   So there was some concern. Oh, I know there was some concern
[01:36:57.380 --> 01:37:02.420]   They did this in the medical device community last year. There was a company against St
[01:37:02.420 --> 01:37:10.100]   Jude's they published a vulnerability in their I think it was an insulin pump and then they shorted the stock and people were like
[01:37:10.100 --> 01:37:16.980]   Yeah, that's not legit. Well, so so what's going on apparently is some of these security firms if you don't have a big bug bounty
[01:37:16.980 --> 01:37:19.860]   Which apparently AMD does not they want to get paid for their research
[01:37:20.180 --> 01:37:24.820]   They spent years apparently researching these flaws. They wanted to get paid for the research
[01:37:24.820 --> 01:37:29.060]   So since AMD ain't gonna pay them they short the stock and then announce it
[01:37:29.060 --> 01:37:35.780]   There really is a problem because AMD has now said oh, yeah
[01:37:35.780 --> 01:37:40.420]   This we do have this problem and we're gonna we're gonna patch this
[01:37:40.420 --> 01:37:45.460]   Nope, we don't expect any performance impact. It's not like the specter and meltdown flaws
[01:37:45.460 --> 01:37:47.860]   It's it is interesting though because AMD
[01:37:48.740 --> 01:37:56.100]   Among other flaws AMD was using a chip from a company called a asm media and these chips had hardware
[01:37:56.100 --> 01:38:00.180]   Backdoors built into them by the Taiwanese company that made the chips
[01:38:00.180 --> 01:38:04.660]   AMD didn't know that or didn't care or whatever but they put them in their
[01:38:04.660 --> 01:38:08.820]   Systems their systems on a chip and that means they had hardware backdoors
[01:38:08.820 --> 01:38:14.180]   It's not a serious flaw in the sense you have to have root access you have to administer it or access to do it
[01:38:14.740 --> 01:38:18.900]   So somebody presumably would have some pretty good access to your system already
[01:38:18.900 --> 01:38:20.740]   Nevertheless
[01:38:20.740 --> 01:38:23.220]   If they're real and AMD is gonna fix them. I agree. It's kind of
[01:38:23.220 --> 01:38:30.340]   First of all, there's a standard in the community security community that you give a company 90 days at least to fix the flop
[01:38:30.340 --> 01:38:33.220]   the flaw before you publicize it and it is kind of
[01:38:33.220 --> 01:38:36.580]   Come on shorten the stock really
[01:38:36.580 --> 01:38:42.420]   You know, this is this is kind of a self regulatory option for these guys, right?
[01:38:42.500 --> 01:38:48.980]   So having a community that says dude, that's not cool. That's that might work. Who knows?
[01:38:48.980 --> 01:38:51.380]   Who knows? I wonder
[01:38:51.380 --> 01:38:55.700]   I mean, it doesn't really hurt AMD to short the stock as much as it hurts the investors
[01:38:55.700 --> 01:39:00.660]   Or the people with the chips with the security flaw that is now those guys
[01:39:00.660 --> 01:39:04.020]   Those guys do
[01:39:04.020 --> 01:39:10.740]   Uh, this is a really uh weird story and again, this might go back to fossed assess to child abuse imagery
[01:39:11.780 --> 01:39:15.220]   Yeah, has been found within the blockchain, which means everybody who has
[01:39:15.220 --> 01:39:21.860]   A wallet a bitcoin wallet on their computer has this imagery in the blockchain
[01:39:21.860 --> 01:39:24.500]   A wallet or the entire chain?
[01:39:24.500 --> 01:39:29.140]   Well, in order to have a bitcoin wallet not one stored on coinbase or somewhere out there
[01:39:29.140 --> 01:39:35.940]   But on your computer you have to download the entire block or the entire the entire blockchain
[01:39:35.940 --> 01:39:39.380]   That's how it works. So now you're a criminal, dude
[01:39:39.620 --> 01:39:44.820]   I uh apparently 1600 files were stored in bitcoin's blockchain one of the of the files at least eight
[01:39:44.820 --> 01:39:50.900]   Were sexual including one thought to be an image of child abuse and two that contained links to child abuse content
[01:39:50.900 --> 01:39:53.940]   142 of which linked to dark web services
[01:39:53.940 --> 01:40:02.420]   Now, I don't know even how you get into that imagery. I mean, it's you know, it's hidden in plain sight. I guess but yeah, that's
[01:40:02.420 --> 01:40:05.220]   Missed up
[01:40:05.220 --> 01:40:07.700]   Uh, so don't I don't know what you should do
[01:40:07.700 --> 01:40:09.700]   um
[01:40:09.700 --> 01:40:15.140]   It creates a moral and legal quandary
[01:40:15.140 --> 01:40:17.940]   Yeah
[01:40:17.940 --> 01:40:22.820]   I have you think of what you think it was subversion. No, I think that somebody you know people said oh, look
[01:40:22.820 --> 01:40:26.980]   You know the people who invest in blockchain. Come on, man. You can stick it in here. Yeah. Well, yeah
[01:40:26.980 --> 01:40:30.820]   That's why like IBM created their hyper ledger efforts
[01:40:30.820 --> 01:40:32.580]   It's not IBM another
[01:40:32.580 --> 01:40:37.300]   IBM work with other companies to create like hyper ledger because companies were afraid of putting
[01:40:37.860 --> 01:40:42.420]   Some of their data in the blockchain because they were like well people can access this
[01:40:42.420 --> 01:40:46.100]   This is also why whenever I talked to anybody in like the sec
[01:40:46.100 --> 01:40:48.580]   financial crimes or the
[01:40:48.580 --> 01:40:54.660]   DEA they were ecstatic about the blockchain because they're like oh we can people are creating digital records of all of their
[01:40:54.660 --> 01:40:58.260]   illicit transactions. This is wonderful. I couldn't design a better system
[01:40:58.260 --> 01:41:05.460]   So there's a there's a lot of misconceptions around what this is and what what you're sharing
[01:41:06.100 --> 01:41:09.940]   So I have the blockchain on a couple of my computers because I have a bitcoin wallet
[01:41:09.940 --> 01:41:11.780]   I've been trying remember this is the one I can't get into
[01:41:11.780 --> 01:41:15.060]   I guess I'll delete it. I can't get into it anyway
[01:41:15.060 --> 01:41:19.140]   Um
[01:41:19.140 --> 01:41:21.540]   What if have you gone to a hypnotist yet?
[01:41:21.540 --> 01:41:26.420]   I want I want you to go to a hypnotist to get your idea. I like that
[01:41:26.420 --> 01:41:30.340]   Okay, you're going back Leo
[01:41:30.340 --> 01:41:32.980]   I'm going back. I remember
[01:41:32.980 --> 01:41:35.700]   You're younger now good idea to have a
[01:41:36.260 --> 01:41:38.260]   To take bitcoin
[01:41:38.260 --> 01:41:44.980]   And then I thought as I do everything yeah, I do I try I throw money away. I don't I do for a living
[01:41:44.980 --> 01:41:47.380]   I I don't like money. I don't want any
[01:41:47.380 --> 01:41:54.100]   Carxton well done Carston well, what did he do? I didn't even see it my eyes are closed
[01:41:54.100 --> 01:41:57.060]   What are you doing to me Carston?
[01:42:02.900 --> 01:42:05.140]   Plattsburgh, New York is that near you?
[01:42:05.140 --> 01:42:08.020]   Plattsburgh is it up?
[01:42:08.020 --> 01:42:13.860]   Upstate well they have hydroelectric which means their electricity is only four and a half cents
[01:42:13.860 --> 01:42:16.580]   fascinating per kilowatt hour
[01:42:16.580 --> 01:42:20.500]   Uh cheap enough to make it actually I guess a good place to mine
[01:42:20.500 --> 01:42:27.060]   Bitcoin I loved this story the people in Washington and the people at no where's ville New York? Yes
[01:42:27.060 --> 01:42:32.420]   So they uh the city council plattsburgh is worried because a bunch of people are moving in
[01:42:32.820 --> 01:42:34.820]   City slickers they're moving in
[01:42:34.820 --> 01:42:40.820]   To mine bitcoin on our cheap electricity and then what happens to the price of electricity goes up
[01:42:40.820 --> 01:42:46.260]   I've been hearing a lot of complaints that electric bills have gone up by 100 or 200 dollars
[01:42:46.260 --> 01:42:51.860]   You can understand why people are upset. You should see my electric bill if you think that's a bad
[01:42:51.860 --> 01:42:57.220]   Oh, man my electric bill. Hey. Yeah, look at stacy's electric bill
[01:42:57.220 --> 01:43:00.340]   But we live in a high and expensive electricity
[01:43:00.980 --> 01:43:08.500]   Tounds 16.5 cents per kilowatt hour. I rest my case you couldn't bit bitcoin mine in austin
[01:43:08.500 --> 01:43:15.380]   No, no, unless it was winter and I wanted to generate my own heat there is we saw this on security now
[01:43:15.380 --> 01:43:19.860]   There's a Norwegian or a Scandinavian company selling a heater a space heater that mines bitcoin
[01:43:19.860 --> 01:43:23.540]   Yeah, I ran into this to see yes. I thought this was awesome
[01:43:23.540 --> 01:43:27.220]   Or maybe it's the other way around maybe it's a bitcoin miner that you can use to heat your house
[01:43:27.220 --> 01:43:29.860]   That's exactly what it is. Yeah
[01:43:29.860 --> 01:43:31.860]   What are you doing the summer?
[01:43:31.860 --> 01:43:33.940]   You swear open the windows
[01:43:33.940 --> 01:43:37.060]   Evaporative cooling it's so hot right now
[01:43:37.060 --> 01:43:40.900]   Anyway, you turn down the heat turn down the heat. No, I can't afford it. I can't afford it
[01:43:40.900 --> 01:43:42.900]   They have turned off
[01:43:42.900 --> 01:43:46.980]   Bitcoin mining in the city the city council voted to ban it for 18 months
[01:43:46.980 --> 01:43:49.940]   So, you know
[01:43:49.940 --> 01:43:53.220]   So I don't know how many bitcoin miners moved into platsburg
[01:43:53.220 --> 01:43:57.700]   But is that I guess there's no legal problem with that right? It's not like a free speech or anything
[01:43:58.740 --> 01:44:04.020]   Business activity that you thought that's what it is the business activity. Yeah. Yeah, you could I guess you could argue
[01:44:04.020 --> 01:44:07.380]   You could zone it so like some some of these
[01:44:07.380 --> 01:44:10.340]   I'm trying to think how you would
[01:44:10.340 --> 01:44:12.580]   It's a reasonable problem sale a type of
[01:44:12.580 --> 01:44:16.340]   Plattsburgh only gets they have an allotment from the the
[01:44:16.340 --> 01:44:20.340]   St. Lawrence hydroelectric dam of 104 megawatts per month
[01:44:20.340 --> 01:44:26.420]   The biggest bitcoin mining operation that moved in a Puerto Rican company called coin mint used 10 percent of that
[01:44:27.780 --> 01:44:30.740]   In january in february they used 10 percent of their power allotment
[01:44:30.740 --> 01:44:35.460]   In january plattsburgh went over the power allotment and had to buy electricity on the open market
[01:44:35.460 --> 01:44:38.180]   Oop, that's why they had to pay more
[01:44:38.180 --> 01:44:43.060]   Oh, well, that's so so that's it. That's an interesting. Yeah, so I think yeah
[01:44:43.060 --> 01:44:48.420]   It is reasonable, but that doesn't mean it's against the law, which is a very different thing
[01:44:48.420 --> 01:44:54.500]   I you know if I were this company this company that moved all the way from Puerto Rico to plattsburgh
[01:44:56.260 --> 01:45:00.500]   Well, but if you knew how the power was structured then that might not be a great move
[01:45:00.500 --> 01:45:03.220]   You did it for four and a half because of the four and a half cents a kill
[01:45:03.220 --> 01:45:05.780]   If you didn't understand it anymore. Yeah
[01:45:05.780 --> 01:45:08.420]   But
[01:45:08.420 --> 01:45:13.140]   I don't know what happened. This is another one from wra. Oh, they're really breaking the stories this week
[01:45:13.140 --> 01:45:18.420]   Yeah, why why are they subtly up? I know this is another one. Maybe they did something to get on your google list
[01:45:18.420 --> 01:45:24.180]   No, maybe they just optimize google news. This is this is a real story from rolly
[01:45:24.900 --> 01:45:26.900]   Oh, okay
[01:45:26.900 --> 01:45:31.620]   To find suspects police quiet quietly turned to google. I don't know if this is
[01:45:31.620 --> 01:45:36.820]   I don't know what the upshot of this is yeah, I didn't know what the story was trying to be so
[01:45:36.820 --> 01:45:38.660]   well
[01:45:38.660 --> 01:45:40.660]   so they're scary
[01:45:40.660 --> 01:45:42.820]   so uh, what what is it these days?
[01:45:42.820 --> 01:45:45.060]   uh
[01:45:45.060 --> 01:45:47.220]   There were gunshots shots fired
[01:45:47.220 --> 01:45:53.780]   and um, I I don't know something happened there were shootings and uh
[01:45:54.660 --> 01:45:57.620]   Google the police in rolly said wait a minute
[01:45:57.620 --> 01:46:02.740]   If we could only get the cell phone records of everybody who was in the vicinity
[01:46:02.740 --> 01:46:05.700]   We'd have some idea who to talk to
[01:46:05.700 --> 01:46:11.300]   So they drew shapes around the crime seems on a satellite image marking the coordinates on the map
[01:46:11.300 --> 01:46:18.500]   They convinced a wake county judge they had enough probable cause to order google to hand over a counted enough fires on every single cell phone
[01:46:18.500 --> 01:46:21.140]   That entered that area during certain times
[01:46:22.660 --> 01:46:24.660]   They've done it at least four times
[01:46:24.660 --> 01:46:27.700]   it last year
[01:46:27.700 --> 01:46:30.820]   Search warrants demanding google
[01:46:30.820 --> 01:46:35.460]   Accounts of so it's not the phone number. It's not from the cell carrier
[01:46:35.460 --> 01:46:40.420]   But google's oh, you know if you have an android phone or actually even if you don't if you have a apple phone, right?
[01:46:40.420 --> 01:46:43.940]   The maps might be telling google where you are you'll just look at your
[01:46:43.940 --> 01:46:46.900]   Location tracker on google and it knows where you are
[01:46:49.220 --> 01:46:52.020]   Uh, what I don't know is if google hand them over or not
[01:46:52.020 --> 01:46:54.340]   uh
[01:46:54.340 --> 01:46:56.340]   So what is it is it a
[01:46:56.340 --> 01:47:00.420]   Dragnet what is it's a fishing expedition. It shouldn't be not legal
[01:47:00.420 --> 01:47:08.020]   But uh, the judge approved it now if google wanted to fight it. I think they could probably make a pretty good case for it
[01:47:08.020 --> 01:47:14.340]   For that the acl use what's the difference between asking google for this and asking a phone company for this
[01:47:14.340 --> 01:47:17.380]   In both cases a fishing expedition the theory is
[01:47:18.180 --> 01:47:23.220]   That you are allowed to go out and ask for information about people who didn't commit a crime in order to kind of
[01:47:23.220 --> 01:47:25.780]   Net the guy who did
[01:47:25.780 --> 01:47:27.860]   Right
[01:47:27.860 --> 01:47:32.180]   Now but did google how could you write the story and not know what google did here
[01:47:32.180 --> 01:47:40.900]   Well, you should ask that question of w r al public records reporter tyler dukes
[01:47:40.900 --> 01:47:46.340]   Well, no, I mean it's it's interesting that police are trying to get to this data
[01:47:46.660 --> 01:47:49.780]   But then so that's newsworthy. I just don't understand
[01:47:49.780 --> 01:47:55.220]   What happened and that's that's part of the system
[01:47:55.220 --> 01:48:00.980]   Did but it's very cool to be r al has a public records reporter. That's nice. That's nice
[01:48:00.980 --> 01:48:04.180]   so
[01:48:04.180 --> 01:48:09.780]   Sorry, because no, I'm I don't know the only thing missing is what's google's response to this
[01:48:09.780 --> 01:48:11.780]   It's I mean he's got a lot of detail about this
[01:48:12.340 --> 01:48:15.780]   Because you can say I mean this isn't like a physe court thing
[01:48:15.780 --> 01:48:20.260]   So you can actually say that you've got the warrants right that someone's true, right
[01:48:20.260 --> 01:48:28.180]   So I don't understand why because it says in the article that people google had no way to tell people that they were being swept up in this but
[01:48:28.180 --> 01:48:33.460]   I actually think you would but again. I'm not an expert here. This is I'm like
[01:48:33.460 --> 01:48:41.300]   I want to talk to this man's editor. Well, here's here's the only thing he says google declined to say whether it released data in any of the
[01:48:41.300 --> 01:48:45.220]   Raleigh cases and it's unclear from the search warrants exactly what information was seized
[01:48:45.220 --> 01:48:52.900]   Or whether it has been effective in moving the investigations forward of the four cases only one is resolved resulted in an arrest
[01:48:52.900 --> 01:48:59.620]   So google's not saying which sounds to me. It's not saying so then we have to wait for the court
[01:48:59.620 --> 01:49:03.540]   Yeah, the trial and they may not put it in the trial
[01:49:03.540 --> 01:49:08.660]   No, that's weird because they didn't that's part of they know way that would be a litmus test of whether this is
[01:49:08.980 --> 01:49:14.900]   Legit or not if it's not used in evidence of trial, then you were asking for a lot of information you didn't that wasn't
[01:49:14.900 --> 01:49:18.260]   Well, but then you wouldn't have a reason to sue either
[01:49:18.260 --> 01:49:24.260]   So if this guy gets caught and he doesn't understand that this was how he possibly was gotten caught
[01:49:24.260 --> 01:49:29.380]   Then he'd have to sue like he'd have to appeal basically. Sorry not to appeal his
[01:49:29.380 --> 01:49:34.820]   On like bad evidence collection and there's a word for that that is an actual legal word and I don't know what it is
[01:49:35.620 --> 01:49:43.460]   But if they don't use it in the trial, then he wouldn't actually have grounds to appeal based on that which means it could continue until someone
[01:49:43.460 --> 01:49:46.580]   Does I mean this was what happened with um
[01:49:46.580 --> 01:49:51.860]   The stingray type stuff. Right. They were doing it, but they just didn't ever use the evidence
[01:49:51.860 --> 01:49:54.980]   And so no one's using their investigation to help them get leads
[01:49:54.980 --> 01:49:58.820]   And then continue on I I really comes down to the judge
[01:49:58.820 --> 01:50:04.340]   Doing the warrant and then google fighting it and i'm a little disappointed that google hasn't said whether they are
[01:50:04.900 --> 01:50:07.460]   Honoring the warrant or fighting it. I think they should fight it
[01:50:07.460 --> 01:50:12.740]   Well, maybe they will after people pay attention to it. Okay, we're paying attention
[01:50:12.740 --> 01:50:16.260]   We're paying attention
[01:50:16.260 --> 01:50:20.980]   I think we've talked long and hard about all sorts of things. I'm you know, I'm just
[01:50:20.980 --> 01:50:25.300]   Just I'm just I don't know what to say about facebook or
[01:50:25.300 --> 01:50:28.020]   privacy and
[01:50:28.020 --> 01:50:32.580]   I just you said it as Howard Stern says we once get rid of the gassed you've said it all
[01:50:32.660 --> 01:50:36.740]   I've said it all my friend that just means get out of here. He would does it does what he says
[01:50:36.740 --> 01:50:38.820]   Said it all my friend get out of here
[01:50:38.820 --> 01:50:42.980]   Let's let's wrap things up with
[01:50:42.980 --> 01:50:46.820]   How about you jeff let's start with you this time a number this week
[01:50:46.820 --> 01:50:48.740]   Um
[01:50:48.740 --> 01:50:52.660]   All right, so minor note, but I think no worthy that amazon has surpassed
[01:50:52.660 --> 01:50:57.780]   um alphabet for the first time to become the second most valuable company in the world. Wow after apple
[01:50:57.780 --> 01:50:59.860]   Yep
[01:50:59.860 --> 01:51:01.940]   And jeff bassus is the richest man in the world
[01:51:02.740 --> 01:51:04.740]   Yep
[01:51:04.740 --> 01:51:07.940]   Hmm
[01:51:07.940 --> 01:51:09.940]   Do
[01:51:09.940 --> 01:51:11.940]   Mark a capitalization of 7.7
[01:51:11.940 --> 01:51:18.260]   $768 billion so it's a race to see who's the first trillion trillion dollar company. Yes. Who do you think?
[01:51:18.260 --> 01:51:20.420]   Will be apple. Will it be amazon
[01:51:20.420 --> 01:51:24.500]   Will be who's an ego will be satisfied first
[01:51:24.500 --> 01:51:28.420]   Hey, that's how many commas is trillion. That's a lot of commas
[01:51:30.020 --> 01:51:32.020]   I don't think it's gonna be facebook work work
[01:51:32.020 --> 01:51:35.140]   No, I think it's
[01:51:35.140 --> 01:51:37.140]   Facebook might be going
[01:51:37.140 --> 01:51:39.380]   Don't think it's gonna be twitter
[01:51:39.380 --> 01:51:46.420]   Hey, did I talk about my s9 last week? I think I did. Yes. Yes. I'm liking it. What do you think you like it? Yeah
[01:51:46.420 --> 01:51:52.500]   Uh, now I have a longitudinal study of a whole way putting your finger in the right spot, but oh, yeah
[01:51:52.500 --> 01:51:57.540]   And it has the face. I showed you the face recognition is pretty good. Oh, it's a beautiful screen
[01:51:57.700 --> 01:52:00.980]   Yeah, I quite like it and you know, I like it's kind of the anti-iPhone
[01:52:00.980 --> 01:52:07.220]   It has a SD card. It has a headphone jack. It has a fingerprint reader. It's like they decided anything apple does
[01:52:07.220 --> 01:52:12.340]   We're gonna do the opposite. There's no not actually smart. Yeah, it's an alternative, right?
[01:52:12.340 --> 01:52:16.580]   Stacy do you have a pick of the week?
[01:52:16.580 --> 01:52:21.780]   I've read about this on life hacker and so I tried it not because I have two children
[01:52:21.780 --> 01:52:25.220]   But because it's I and my husband have juby-nile silly fights all the time
[01:52:25.460 --> 01:52:29.060]   So I thought this was this was fun. Do you have a lot of fights? What kind of fights do you have?
[01:52:29.060 --> 01:52:34.340]   Oh, we just we just snip at each other over silly things like
[01:52:34.340 --> 01:52:37.860]   Puddle the dishwasher. But those kind of things
[01:52:37.860 --> 01:52:40.580]   We do
[01:52:40.580 --> 01:52:45.300]   Yeah, right. I mean this is this is just part of being living with people you could ask hey echo
[01:52:45.300 --> 01:52:47.300]   What's the right way to load the dishwasher?
[01:52:47.300 --> 01:52:51.380]   Whatever she says is the answer that comes back. Hold on. Let's see what happens
[01:52:51.380 --> 01:52:53.460]   That's hell
[01:52:53.460 --> 01:52:55.460]   What's the right way to load the dishwasher?
[01:52:55.460 --> 01:53:00.260]   Yeah, uh
[01:53:00.260 --> 01:53:02.260]   Silly sorry
[01:53:02.260 --> 01:53:05.700]   What's the right way to load the dishwasher
[01:53:05.700 --> 01:53:15.300]   Nobody nobody has anything to say no one's waiting into that that
[01:53:15.300 --> 01:53:20.420]   Yeah, that's right. Yes. So this is this is a thing to you debate
[01:53:21.300 --> 01:53:28.020]   Called kids court. Um, so what is yes, this is for when your children are that's not fair
[01:53:28.020 --> 01:53:30.500]   He took that this this that the other thing
[01:53:30.500 --> 01:53:35.300]   Yeah, you could you can have your kids you put information about your kids in here
[01:53:35.300 --> 01:53:40.420]   They do not get it or the company does not track it or keep it because of copa
[01:53:40.420 --> 01:53:44.100]   uh, the children's online privacy and protection act but
[01:53:44.100 --> 01:53:48.020]   It it basically has the kids make their case
[01:53:48.740 --> 01:53:53.780]   And it makes a ruling and then it provides a silly punishment for your kids
[01:53:53.780 --> 01:53:56.500]   like, you know, that
[01:53:56.500 --> 01:53:59.380]   Hold an ice cube in your hand for x number of minutes or, uh,
[01:53:59.380 --> 01:54:03.460]   Hop on one leg while singing the alphabet backwards
[01:54:03.460 --> 01:54:06.260]   Um, it's harsh
[01:54:06.260 --> 01:54:10.420]   These these are basically silly fun
[01:54:10.420 --> 01:54:13.700]   And I I just I thought it was fun
[01:54:14.580 --> 01:54:19.460]   Nice and and I I don't have anything other than that to say this did not solve any burning
[01:54:19.460 --> 01:54:25.540]   Issues in our house, but if a fight gets too tense, you can be like I know we'll take it to kids court
[01:54:25.540 --> 01:54:31.860]   I think you should i'm gonna nominate you for the uh, echo skill of the week
[01:54:31.860 --> 01:54:36.740]   Okay, would that be good to provide that every week?
[01:54:36.740 --> 01:54:44.100]   Good. Yeah now next but next week, you know, I am going to have a nest doorbell and a yale google lock
[01:54:44.180 --> 01:54:47.540]   So you may want to not I I just pull from
[01:54:47.540 --> 01:54:50.900]   Oh, and here's well we talk about it on our show
[01:54:50.900 --> 01:54:53.460]   So if you listen to our show, you're going to be bored out of your mind
[01:54:53.460 --> 01:55:00.660]   But I am I'm proposing for people to do an iot spring cleaning, which is what I did for 10 hours
[01:55:00.660 --> 01:55:02.900]   10 hours
[01:55:02.900 --> 01:55:04.900]   I have a lot of gadgets
[01:55:04.900 --> 01:55:11.060]   Yours may not take this long, but what I did is I ended up going through all of my devices
[01:55:11.380 --> 01:55:14.820]   And I said, you know, is this working? Is this not working?
[01:55:14.820 --> 01:55:18.260]   Um, I went through my hero to see what was on my network
[01:55:18.260 --> 01:55:23.220]   Um, I I found some things that I don't use at all that I was like, oh, let's take this off
[01:55:23.220 --> 01:55:26.580]   Um, and then after you take things off your network
[01:55:26.580 --> 01:55:30.100]   And I deleted the accounts and I reset some of the devices
[01:55:30.100 --> 01:55:32.580]   um, then
[01:55:32.580 --> 01:55:35.700]   I went through things that I want to use but still don't work
[01:55:35.700 --> 01:55:38.180]   And I deleted all of those
[01:55:38.820 --> 01:55:43.460]   And then I put everything back on the network and then I deleted everything from amazon and everything from google
[01:55:43.460 --> 01:55:48.740]   And I rebuilt all of my automations and routines. Is this a pain in the butt? Yes
[01:55:48.740 --> 01:55:51.460]   But everything except one device works now
[01:55:51.460 --> 01:55:57.940]   Which is a far cry from how my house usually is which is like my husband's like I used to be able to turn on this light
[01:55:57.940 --> 01:55:59.940]   What happened? I'm like, I don't know
[01:55:59.940 --> 01:56:04.180]   Uh, I think you I think you better go to kids court on that one
[01:56:04.180 --> 01:56:06.820]   Yeah, you're out of
[01:56:07.300 --> 01:56:10.980]   I'm sorry. You're gonna be standing on one leg for sometime is Daisy Higabot
[01:56:10.980 --> 01:56:13.940]   You need to make that light roar
[01:56:13.940 --> 01:56:17.460]   I don't know what happened. It just stopped working
[01:56:17.460 --> 01:56:24.660]   Yeah, did I show you my blood pressure reading thing on the s9 last week? I don't remember I didn't
[01:56:24.660 --> 01:56:30.340]   I showed it on the new screen savers. So there's there's a hidden ability in the galaxy s9
[01:56:30.340 --> 01:56:33.380]   I think
[01:56:33.860 --> 01:56:38.900]   I'm not sure so they've always the s9 has always had this feature which nobody ever talks about
[01:56:38.900 --> 01:56:41.300]   underneath the flash
[01:56:41.300 --> 01:56:48.100]   On the camera. There's a heart rate sensor and you get a fairly I think it actually very accurate heart rate and meeting for reading from it
[01:56:48.100 --> 01:56:50.900]   They're flashes or there are apps that you can do it with any phones light
[01:56:50.900 --> 01:56:56.900]   Yeah with the camera, but this is much more accurate. It's Samsung health uses it to do the reading
[01:56:56.900 --> 01:57:01.700]   Uh, and I think it actually is very accurate because I've compared it to other sensors and so forth
[01:57:01.700 --> 01:57:05.300]   But you you know, I mean you wouldn't want to do it while exercising actually put your finger on here
[01:57:05.300 --> 01:57:06.900]   Yeah, and all of that
[01:57:06.900 --> 01:57:13.620]   But what I didn't know is apparently that in the s9 is also capable and I wonder if this is true
[01:57:13.620 --> 01:57:17.060]   Of doing blood pressure readings
[01:57:17.060 --> 01:57:23.700]   And you have you tried it or what? Yeah, well, so here's the no no the reason I here's why I wonder it there is a
[01:57:23.700 --> 01:57:26.820]   universe of california at san francisco
[01:57:26.820 --> 01:57:30.340]   health project called my BP lab
[01:57:31.460 --> 01:57:34.820]   And you sign up for it and of course you're giving them all your information
[01:57:34.820 --> 01:57:42.580]   Uh, but the idea is you take three and they call them blood pressure readings, but here's the funny thing
[01:57:42.580 --> 01:57:46.900]   They don't they don't show you what your blood pressure is so it might be
[01:57:46.900 --> 01:57:51.380]   You have to calibrate it or maybe it's not that calibrated, but the idea is
[01:57:51.380 --> 01:57:56.100]   Uh, you three times a day you answer you take your blood pressure
[01:57:56.100 --> 01:58:00.580]   Again, they don't tell you what it is. They tell you what they do tell you is it's eight percent below your base liner
[01:58:00.580 --> 01:58:05.860]   4% above your baseline, but they don't say the actual number and my suspicion is that's not because it's not accurate
[01:58:05.860 --> 01:58:09.540]   Although they do ask you when you first sign up if you have a blood pressure cuff
[01:58:09.540 --> 01:58:13.460]   And then they ask you to calibrate so maybe it is
[01:58:13.460 --> 01:58:18.500]   Uh, again, it's a study with the university california san francisco, which is fairly reliable
[01:58:18.500 --> 01:58:24.180]   They say it allows blood pressure be directly measured by the smartphone without any external hardware
[01:58:24.180 --> 01:58:29.700]   UCSf is going to get information about stress and blood pressure levels throughout the day
[01:58:29.780 --> 01:58:31.780]   one aim of the studies to optimize
[01:58:31.780 --> 01:58:37.620]   my BP lab to provide contextualize and scientifically informed feedback so users will be able to
[01:58:37.620 --> 01:58:42.660]   Get better understanding of their stress and blood pressure levels and manage their health more effectively
[01:58:42.660 --> 01:58:44.740]   So I okay
[01:58:44.740 --> 01:58:48.820]   You want to learn about I actually did a blood pressure cuff review for the wire cutter
[01:58:48.820 --> 01:58:54.180]   And as part of that I learned a lot about wire or like how to how blood pressure cuffs work
[01:58:54.180 --> 01:58:56.500]   So are they accurate?
[01:58:56.660 --> 01:59:03.780]   It depends so the most accurate is going to be the arm cuff that puffs up because they're actually measuring
[01:59:03.780 --> 01:59:07.380]   Uh pressure in your arm and this is a momenter
[01:59:07.380 --> 01:59:10.020]   That's big. Yes. Thank you for saying those words
[01:59:10.020 --> 01:59:13.780]   What these do is there it's kind of like the wrist
[01:59:13.780 --> 01:59:19.860]   I'm assuming I am making some assumptions. Yeah, this is probably closer to what the wrist monitors are measuring
[01:59:19.860 --> 01:59:22.980]   and in that case what they're measuring they're
[01:59:22.980 --> 01:59:25.700]   They're measuring
[01:59:25.700 --> 01:59:31.380]   What they're measuring is then run through an algorithm that they have correlated to a good blood pressure
[01:59:31.380 --> 01:59:34.820]   So it all depends on how good your algorithm is
[01:59:34.820 --> 01:59:37.300]   How much data general
[01:59:37.300 --> 01:59:40.740]   And how it reacts to your particular body, right?
[01:59:40.740 --> 01:59:42.020]   So
[01:59:42.020 --> 01:59:44.020]   Because it's it's a generic algorithm
[01:59:44.020 --> 01:59:45.380]   So
[01:59:45.380 --> 01:59:49.700]   That's one and then two you have to take your blood pressure for accurate readings
[01:59:49.700 --> 01:59:54.340]   You actually have to there's a very specific way you should take it and you have to be seated
[01:59:54.420 --> 01:59:56.740]   You can't be saying both feet's on the ground
[01:59:56.740 --> 02:00:01.940]   No movement no talking and your hand has to be at heart level. They see that in the app
[02:00:01.940 --> 02:00:06.980]   So so that's good that they're telling you that because even like the doctor's office
[02:00:06.980 --> 02:00:11.060]   I can't tell you how many times the nurse is talking to me while she takes my blood pressure like
[02:00:11.060 --> 02:00:18.020]   So yeah, so yes, so there there you go. There's it may work for you. It may not well
[02:00:18.020 --> 02:00:21.380]   I have one of these cardio q ar
[02:00:21.380 --> 02:00:23.380]   Oh, that's one of the worst ones
[02:00:23.380 --> 02:00:28.260]   Great to know all right because that's that's the one I have
[02:00:28.260 --> 02:00:34.420]   So I won't I won't so you think an ome run which they've been around forever. I have a more traditional ome run
[02:00:34.420 --> 02:00:40.820]   the ome run what google the wire cutter blood you said the ome run series 10 with bluetooth is the best there there you go
[02:00:40.820 --> 02:00:43.060]   That's the one I don't mind
[02:00:43.060 --> 02:00:47.220]   But it's just like that. It's just you know, it's older. Yeah, I mean the we just I don't know
[02:00:47.780 --> 02:00:52.180]   At the doctor's office they calibrate it all the time. I mean they're really always making sure it's accurate
[02:00:52.180 --> 02:00:55.780]   So the other thing is your blood pressure varies wildly throughout the day
[02:00:55.780 --> 02:00:58.340]   Well, yes
[02:00:58.340 --> 02:01:01.620]   So that's why although if this were a reliable
[02:01:01.620 --> 02:01:07.220]   Measurement on this s9 that would be hugely valuable. I think to people in general
[02:01:07.220 --> 02:01:10.020]   well, and there's there is value if
[02:01:10.020 --> 02:01:15.940]   If it's a relative value. It's a relative value over time. Thank you. That's what they're giving you
[02:01:16.660 --> 02:01:18.900]   That's what they're saying. They're saying based on your baseline
[02:01:18.900 --> 02:01:24.500]   Which you get provided to us you're 8% above that you're 4% below that your blood pressure is a little high
[02:01:24.500 --> 02:01:25.780]   It's a little low that kind of thing
[02:01:25.780 --> 02:01:29.860]   I think that's actually as as useful as it gets because I go to the doctor and he measures it
[02:01:29.860 --> 02:01:33.700]   And I that's you know the official measurement
[02:01:33.700 --> 02:01:35.300]   right
[02:01:35.300 --> 02:01:39.540]   Of course, it's always higher at the doctor's office because I'm scared I have white coat hypertension
[02:01:39.540 --> 02:01:43.540]   Yeah, so get your ome run measure your
[02:01:43.940 --> 02:01:49.060]   Thing first thing in the morning and for a couple of days. So the cardio is the least accurate
[02:01:49.060 --> 02:01:53.540]   That's what it was. It was like it was not a good person
[02:01:53.540 --> 02:01:55.460]   Does she hate the cardio?
[02:01:55.460 --> 02:01:59.700]   It's so convenient because it's so small and it works with an iPhone and it's wireless and all that
[02:01:59.700 --> 02:02:01.860]   But I do I have some other ones as well
[02:02:01.860 --> 02:02:04.900]   Because I have slightly high blood pressure, so I've always monitored it a little bit
[02:02:04.900 --> 02:02:08.260]   Oh, it's it's lower now that I work with you and jeff
[02:02:08.260 --> 02:02:11.700]   Until we talk about certain topics
[02:02:12.660 --> 02:02:15.300]   This show is the number one source of stress in my life
[02:02:15.300 --> 02:02:19.460]   No, all right jeff high five
[02:02:19.460 --> 02:02:22.740]   You're killing me stacy. You're killing me
[02:02:22.740 --> 02:02:25.620]   I would hate that. I don't think we ended up
[02:02:25.620 --> 02:02:29.060]   I didn't run the cardio through the biggest
[02:02:29.060 --> 02:02:34.580]   I'm trying to remember now as you can see there were 10 of them. So you have to article. I'm sure I can find out. Yeah
[02:02:34.580 --> 02:02:36.900]   Scroll down because I feel like
[02:02:36.900 --> 02:02:42.500]   It was awkward to use and it um, hey, that's my arm. Look at that you guys
[02:02:43.300 --> 02:02:44.100]   Sorry
[02:02:44.100 --> 02:02:46.100]   You know, it's cute
[02:02:46.100 --> 02:02:48.900]   This is this is one of those things where
[02:02:48.900 --> 02:02:52.100]   You'd be insane to take this article on
[02:02:52.100 --> 02:02:54.980]   You've got to do so much testing
[02:02:54.980 --> 02:03:01.220]   That normally they would do this with you know a dozen people in a lab. I mean it's crazy
[02:03:01.220 --> 02:03:04.100]   Well, so I just do this all myself
[02:03:04.100 --> 02:03:09.860]   No, I so I did I did individually test each one and then I think I ran it through some friends
[02:03:09.860 --> 02:03:15.460]   But I brought it to the university of texas nursing school. You are such a and oh wow
[02:03:15.460 --> 02:03:18.100]   so and they had they
[02:03:18.100 --> 02:03:20.820]   For two hours
[02:03:20.820 --> 02:03:25.540]   It was a little shorter because they had just had the murder on campus. So this was whenever that was
[02:03:25.540 --> 02:03:29.860]   So I know you're busy right now, but um, yeah
[02:03:29.860 --> 02:03:35.860]   Yeah, but they got extra credit. So it was 20 nursing students and three like actual professors
[02:03:36.100 --> 02:03:42.820]   And we actually ran through the hardest part was actually making sure everyone tried everything and wrote down their thoughts
[02:03:42.820 --> 02:03:44.980]   So oh yeah here they are the student nurses
[02:03:44.980 --> 02:03:50.420]   So we said why are cut our thing how many hours you spent doing how many hours did it out up to?
[02:03:50.420 --> 02:03:53.940]   I would have to add I would have to you don't want to know i'm telling
[02:03:53.940 --> 02:03:59.540]   Don't they always see the stories we spent 80 hours looking at oh, they do don't think that's because they used to pay by the hour
[02:03:59.540 --> 02:04:00.580]   Oh
[02:04:00.580 --> 02:04:04.820]   So it says after spending 20 hours researching more than 50 blood pressure monitors
[02:04:05.060 --> 02:04:09.860]   Any viewing medical professionals and testing 10 finalists with a group of nursing professors and students
[02:04:09.860 --> 02:04:17.780]   We can say the best is why I love the wire cutter. You're great. Yeah, and I I admire you so much for doing this
[02:04:17.780 --> 02:04:19.620]   That's a lot of work
[02:04:19.620 --> 02:04:23.620]   I thought it'd be kind of fun, but it did suck. I I love the wire cutter
[02:04:23.620 --> 02:04:27.780]   But doing the wire cutters standards in stories
[02:04:27.780 --> 02:04:33.940]   They are yeah, well the consumer reports cannot be a fun place to work. Right your blood pressure
[02:04:34.580 --> 02:04:39.700]   What you did I sorry I used to work for consumers union. It was my one PR thing
[02:04:39.700 --> 02:04:45.620]   Yeah, they are the publishers that the consumers report. I love consumers. Yeah, so I worked on the campaign to
[02:04:45.620 --> 02:04:47.620]   Uh
[02:04:47.620 --> 02:04:52.100]   Get your prescription for eyeglasses from your optometrist so you could go on the web or anywhere
[02:04:52.100 --> 02:04:57.220]   It was it you won you won on that one you won every time I go to the eye doctor now
[02:04:57.220 --> 02:05:00.340]   They print out a prescription and give it to me and then grit their teeth
[02:05:01.620 --> 02:05:08.260]   So this was this this was when I discovered that the quotes and press releases. Oh my gosh you guys they are made up
[02:05:08.260 --> 02:05:10.420]   As a yes
[02:05:10.420 --> 02:05:16.740]   As a gradually journalist student journalism student I walked in and I'm like hey can I interview you because I've got to write this press release up
[02:05:16.740 --> 02:05:21.860]   He's like what I was like why I need a quote about why we're doing that you write it and then run it by me
[02:05:21.860 --> 02:05:24.820]   Nope, I was like oh my god
[02:05:24.820 --> 02:05:27.060]   I write but I just want to say I write my own quotes
[02:05:27.060 --> 02:05:31.140]   I stopped that knit that in the butt. I said nobody's writing my quotes says to be in my language
[02:05:31.780 --> 02:05:35.060]   Well, I've had it write other people's quotes. I hate I hate writing other people's quotes
[02:05:35.060 --> 02:05:38.260]   It feels so dishonest. I was just like oh
[02:05:38.260 --> 02:05:40.500]   Oh, it's standard practice
[02:05:40.500 --> 02:05:45.300]   I mean it's not it's not that you write something that they wouldn't say you write it and then you say is this okay?
[02:05:45.300 --> 02:05:46.980]   And they say yes or no
[02:05:46.980 --> 02:05:50.740]   But it's still but that is quoted the media as if the person actually said this
[02:05:50.740 --> 02:05:57.940]   the fact that I you know as a journalism student could barely stomach my my PR advocacy is
[02:05:58.580 --> 02:06:02.980]   You went over yeah, you're really a journalist
[02:06:02.980 --> 02:06:07.780]   Shame she's in the dark side. Yeah, but consumers union come on
[02:06:07.780 --> 02:06:11.460]   You say that cardio is
[02:06:11.460 --> 02:06:14.420]   Has a poor accuracy rating
[02:06:14.420 --> 02:06:16.820]   And as a poor accuracy rating and I think it was
[02:06:16.820 --> 02:06:20.020]   It was difficult to turn off like
[02:06:20.020 --> 02:06:26.340]   Everyone else kind of weird the ui is kind of weird the ui was not conducive to yeah
[02:06:26.660 --> 02:06:30.420]   Everyone yeah, it was harder than it needed to be I thought you say it seemed
[02:06:30.420 --> 02:06:36.100]   Seemed accurate in my initial test because it won't turn off until it's properly closed it tends to run out of batteries very easily
[02:06:36.100 --> 02:06:38.740]   There you go. Yeah
[02:06:38.740 --> 02:06:44.980]   And the other thing is some of these were better with people who were skinny versus people who were overweight
[02:06:44.980 --> 02:06:50.260]   So that was something right that was fat as fat says we need extra large cuffs
[02:06:50.260 --> 02:06:52.980]   Well, it's not gonna
[02:06:55.620 --> 02:06:57.620]   I'm not fat. I'm buff
[02:06:57.620 --> 02:07:04.100]   Ladies and gentlemen on that note, I will bid you a fond farewell stacey higgin bottham
[02:07:04.100 --> 02:07:10.100]   You catch her work at stacey on iot.com. She also does the stacey on iot podcast with kevin toful every week
[02:07:10.100 --> 02:07:12.260]   She's at giga stacey on the twitter
[02:07:12.260 --> 02:07:15.140]   Jeff Jarvis is at buzz machine calm
[02:07:15.140 --> 02:07:17.700]   And he's of course not to go out with the snow blower
[02:07:17.700 --> 02:07:22.260]   Are you gonna do that? No, you don't use a snow blower. Well the guy who does our
[02:07:22.740 --> 02:07:28.500]   Broadway is gone for two weeks. I'm gonna have to do it dude. You have a fib you cannot shovel snow
[02:07:28.500 --> 02:07:30.580]   Not shoveling. I'm using the blower
[02:07:30.580 --> 02:07:34.580]   Family member who can do this for you?
[02:07:34.580 --> 02:07:37.780]   No, it's him and the cats and your wife
[02:07:37.780 --> 02:07:43.060]   Did your daughter your daughter still yeah, get your wife out there. Yeah, I mean seriously
[02:07:43.060 --> 02:07:48.420]   If my husband was gonna kill over from a heart attack, I'd shovel the snow shoveling snow at least when I live back
[02:07:48.420 --> 02:07:52.500]   East was the number one cause our heart attacks. I'm not shoveling. I'm pushing the dam or this
[02:07:52.500 --> 02:07:54.980]   It's a big one. It's a monster snow blower
[02:07:54.980 --> 02:07:59.220]   You know, I've tried to figure out how to get a selfie of myself with the snow blower scareful. I want to see this nice
[02:07:59.220 --> 02:08:01.940]   Make your wife do it. Yeah
[02:08:01.940 --> 02:08:07.220]   Uh, thank you both for being here. I do do stay safe in the snow. Jeff. I'm sorry
[02:08:07.220 --> 02:08:09.380]   uh
[02:08:09.380 --> 02:08:11.620]   Nor easter number four tobe yep
[02:08:11.620 --> 02:08:16.500]   Four four nor'estures in what two weeks I think and stacey try to enjoy the grackels
[02:08:17.220 --> 02:08:20.660]   And not chat not hat on your damn sunlight spacey swifts
[02:08:20.660 --> 02:08:24.580]   It is nice. I'm gonna be up on the roof tonight with my uh margarita
[02:08:24.580 --> 02:08:27.300]   So
[02:08:27.300 --> 02:08:29.300]   Just
[02:08:29.300 --> 02:08:33.300]   Jeff if you're coming for ice is it I saw shit? No, I'm not I'm gonna be a perusion
[02:08:33.300 --> 02:08:35.940]   Oh
[02:08:35.940 --> 02:08:38.980]   I feel terrible. He has to know the way to peruse you Italy
[02:08:38.980 --> 02:08:44.180]   That's news. What is that news? Geist? What is that you do? That's that's the perusion journalism festival. Okay
[02:08:44.180 --> 02:08:46.660]   Newsgeist is going to be in um
[02:08:47.620 --> 02:08:50.980]   Uh, lispen oh nice and buenos ades
[02:08:50.980 --> 02:08:58.420]   I'm glad I don't live in austin. I would I would die for a heart attack because of food. They're so good
[02:08:58.420 --> 02:09:00.580]   I would just not be able to stop eating
[02:09:00.580 --> 02:09:05.060]   Welcome to the club. Okay, so you just have to exercise more. Yeah
[02:09:05.060 --> 02:09:08.980]   Thank you for joining us. We do this show
[02:09:08.980 --> 02:09:14.740]   Every uh, wednesday's kind of the lash my slash show my week every wednesday about 130 pacific pore 30 eastern
[02:09:15.380 --> 02:09:17.860]   2030 utc if you want to watch live we'd love it
[02:09:17.860 --> 02:09:23.300]   Uh just tune in twit.tv/live join us in the chat room at irc.twit.tv
[02:09:23.300 --> 02:09:27.460]   If you'd like to join us in the studio like paul and lori did
[02:09:27.460 --> 02:09:30.900]   They're visiting us for the most scenic place in america
[02:09:30.900 --> 02:09:35.540]   According to good morning america wow well, they must be true
[02:09:35.540 --> 02:09:38.260]   lake and michigan
[02:09:38.260 --> 02:09:41.860]   Sleeping bear
[02:09:42.020 --> 02:09:48.660]   Okay, whatever you know you michiganters. You know what we're talking about. You know, what are people from michigan called michiganters
[02:09:48.660 --> 02:09:51.460]   Really? Yeah, I know it's kind of embarrassing
[02:09:51.460 --> 02:09:55.060]   You got texas their michiganters
[02:09:55.060 --> 02:09:57.620]   Okay
[02:09:57.620 --> 02:10:02.740]   No, he's a texan. So it's all right nice to have you paul and lori if you want to do that email tickets at twit.tv
[02:10:02.740 --> 02:10:06.900]   No, no no charge. We just wait. I know ahead of time. Make sure we can have a seat put out for you
[02:10:06.900 --> 02:10:09.380]   Uh, you can also get the ores pants
[02:10:09.860 --> 02:10:16.660]   Yeah, I get the memo every morning. Yeah, there'll be people in studio better wear your pants today. Oh man. I was gonna wear slippers
[02:10:16.660 --> 02:10:23.620]   Uh, you can also get in any version of any show we do on demand. We've got audio. We've got video
[02:10:23.620 --> 02:10:25.700]   We've got high def low def medium def
[02:10:25.700 --> 02:10:32.020]   All at the website twit.tv/twig for this show twit.tv/twig or wherever
[02:10:32.020 --> 02:10:35.300]   You get podcast. We need a new picture for this. We can google
[02:10:35.300 --> 02:10:39.300]   I'm gonna say car still. I thought you changed that picture. Yeah, because I see it's elgin and Jarvis
[02:10:39.300 --> 02:10:42.740]   And it's in the old studio and we we can do better
[02:10:42.740 --> 02:10:45.140]   We'll get up
[02:10:45.140 --> 02:10:47.380]   Do we have did we take any pictures with Stacy was here
[02:10:47.380 --> 02:10:53.380]   We'd like to have live people in the studio, but we'll figure yeah, well sometime I'll come up again
[02:10:53.380 --> 02:10:57.060]   It's your show now and now that she's met me twice. She's not so afraid
[02:10:57.060 --> 02:11:04.020]   All right, and we're gonna get some of your friends from austin on the show. I believe
[02:11:04.020 --> 02:11:06.740]   Oh, yes
[02:11:06.740 --> 02:11:13.060]   Like Wendy osteine and angel went in other from a duo. I'm trying to get her on wesley. We're trying to get on
[02:11:13.060 --> 02:11:14.980]   Uh
[02:11:14.980 --> 02:11:21.140]   Uh, is his name wesley. Yeah, wesley wesley falconer. Yeah, what's he gonna talk about? And uh, it doesn't matter
[02:11:21.140 --> 02:11:23.220]   He's a nice guy
[02:11:23.220 --> 02:11:26.180]   That's all the guys. He's a nice guy. He used to work at AMD
[02:11:26.180 --> 02:11:28.180]   He knows tech
[02:11:28.180 --> 02:11:30.180]   That's how I know wesley, right?
[02:11:30.180 --> 02:11:35.140]   Uh, and bo what's we're trying to get him all we're gonna get all the people we met down there in austin. That was a lot of fun
[02:11:35.780 --> 02:11:39.140]   Thanks. Thanks for joining us. Everybody. We'll see you next time on twig
[02:11:39.140 --> 02:11:41.140]   You
[02:11:41.140 --> 02:11:43.140]   You
[02:11:43.140 --> 02:11:45.140]   You
[02:11:45.140 --> 02:11:47.720]   (upbeat music)
[02:11:47.720 --> 02:11:54.720]   [MUSIC]

