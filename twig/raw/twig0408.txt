;FFMETADATA1
title=Being Internet Awesome
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=408
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.320]   Coming up on this week in Google, I'm Jason Howl. I'm here with Stacey Higginbotham and then Danny Sullivan from Search Engine Land.
[00:00:06.320 --> 00:00:09.300]   We're going to talk all about internet safety for children,
[00:00:09.300 --> 00:00:12.280]   what the future of the Internet of Things will bring,
[00:00:12.280 --> 00:00:19.100]   specifically for our civil liberties and how we allow that to happen, how Google is fighting bad ads and letting the good ones
[00:00:19.100 --> 00:00:24.300]   its own stick around and a whole lot more, Twig is next.
[00:00:24.300 --> 00:00:29.680]   Netcasts you love.
[00:00:29.760 --> 00:00:31.760]   From people you trust.
[00:00:31.760 --> 00:00:36.880]   This is Twig.
[00:00:36.880 --> 00:00:44.560]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:44.560 --> 00:00:58.320]   This is Twig. This week in Google, episode 408 recorded Wednesday, June 7th, 2017, being Internet awesome.
[00:00:59.320 --> 00:01:04.360]   It's time for this week in Google, the show where we talk about, well it's in the name Google,
[00:01:04.360 --> 00:01:08.840]   but we're also going to talk about a bunch of other things. Leo LaPorte obviously not here today.
[00:01:08.840 --> 00:01:16.400]   He's on week one of two, three weeks. I don't know. He's a way enjoying his life, not podcasting.
[00:01:16.400 --> 00:01:20.600]   I'm Jason Howl. I'm filling in for Leo. This and next week and happy to be here.
[00:01:20.600 --> 00:01:27.720]   Super stoked to welcome back Stacey Higginbotham, of course, creator of the IOT podcast at IOTpodcast.com.
[00:01:27.720 --> 00:01:33.400]   And the Internet of Things newsletter at Stacey on IOT.com. And occasional dancer.
[00:01:33.400 --> 00:01:35.400]   Always dancing.
[00:01:35.400 --> 00:01:39.640]   All right, good. Dance like no one's watching except everybody's watching because you're on a podcast.
[00:01:39.640 --> 00:01:46.040]   That's the extended version of that saying. Also joining us is Danny Sullivan,
[00:01:46.040 --> 00:01:51.800]   founding editor of Search Engine Land and Marketing Land. And it's great to have you back Danny.
[00:01:51.800 --> 00:01:53.000]   It's always fun to do a podcast.
[00:01:53.000 --> 00:01:54.000]   Good to be back. Thanks.
[00:01:54.000 --> 00:02:00.720]   Yeah, so I don't know how to kind of get this ball rolling. I'm just excited to be here and excited
[00:02:00.720 --> 00:02:06.160]   to have you guys on board. Should we start with some Google stuff? Does that make the most sense?
[00:02:06.160 --> 00:02:09.000]   I mean, or we could start with Apple, but I'm sure that was a good thing.
[00:02:09.000 --> 00:02:12.520]   I was going to start with whatever Danny wanted to start with because good Lord, he is rare and to go.
[00:02:12.520 --> 00:02:14.880]   Yeah, I know. Oh, I was shadowboxing.
[00:02:14.880 --> 00:02:20.080]   We can we can dance where you want to.
[00:02:21.080 --> 00:02:24.280]   All right. Hey, why don't we do that then? I think this is a great place to start.
[00:02:24.280 --> 00:02:30.040]   Danny, you wrote about Wall Street Journal. Wall Street Journal has a has had a program where if you
[00:02:30.040 --> 00:02:35.400]   searched for a Wall Street Journal article through Google Search, it would bypass the pay wall and
[00:02:35.400 --> 00:02:38.920]   it would let you through and you feel like you're getting away with something. Many times I'd go to
[00:02:38.920 --> 00:02:42.840]   Wall Street Journal article and it would give me, you know, it would show me the three paragraphs and
[00:02:42.840 --> 00:02:47.960]   then tell me I had to like sign up. I'd open up another browser, search for the name of the article,
[00:02:47.960 --> 00:02:51.320]   go through the results and I have my way in it. That doesn't work right now.
[00:02:51.320 --> 00:02:55.000]   But you wrote about this. Tell us a little bit about the change that's happened.
[00:02:55.000 --> 00:02:59.400]   Well, it actually came off of a good Bloomberg article. So be sure that people watch
[00:02:59.400 --> 00:03:03.480]   read the Bloomberg article as well. And they had done an interview with Wall Street Journal and
[00:03:03.480 --> 00:03:07.800]   in February, the Wall Street Journal dropped out of this program that's called First Click Free.
[00:03:07.800 --> 00:03:12.920]   And the way for click free works is that if you have subscription content behind a pay wall,
[00:03:12.920 --> 00:03:17.240]   you can still be listed in Google and have that stuff show up just like any normal article.
[00:03:17.240 --> 00:03:21.560]   As long as you guarantee to let somebody click through and see the article for free
[00:03:21.560 --> 00:03:26.680]   and there's like a maximum of three articles per month or per visit or session that they can do.
[00:03:26.680 --> 00:03:30.200]   Well, the Wall Street Journal felt like this was getting abused. They said I think they had like
[00:03:30.200 --> 00:03:35.080]   a million people a month that were just using Google to go through and view articles for free
[00:03:35.080 --> 00:03:39.160]   this way. And so they showed it all down in February. They said, thanks, but no, thanks. We don't
[00:03:39.160 --> 00:03:45.960]   want to do First Click Free. So in that program. And so one of the downsides to that is then if
[00:03:45.960 --> 00:03:50.440]   you don't allow Google to come through your pay wall, then it only gets sort of the first
[00:03:50.440 --> 00:03:55.400]   three or four paragraphs of your article. And so if there are lots of long-tailed terms or
[00:03:55.400 --> 00:03:59.160]   there's material that's further down, Google can't read it, doesn't understand what your
[00:03:59.160 --> 00:04:03.480]   article is about. And so potentially you're not going to get as many visitors because you're not
[00:04:03.480 --> 00:04:08.280]   going to show up for as many topics. The other thing is that when you do show up, there's going to be
[00:04:08.280 --> 00:04:13.400]   a little subscription label that shows up next to you in Google News, not in Google Web Search,
[00:04:13.400 --> 00:04:17.720]   but in Google News. And people might say the subscription thing there and think, well,
[00:04:17.720 --> 00:04:21.960]   I don't have a subscription, so I'm not going to click through. I'm done. And they said that the
[00:04:21.960 --> 00:04:28.120]   traffic, you know, they did this from Google dropped 44%. And so, and they seem upset about that.
[00:04:28.120 --> 00:04:32.920]   Like, well, we feel like that's unfair. We should be showing up just as well as we were before.
[00:04:32.920 --> 00:04:37.960]   But on the other hand, one of the reasons they did this is that they said they were getting
[00:04:37.960 --> 00:04:42.280]   greater conversion when they cut people off into becoming paid subscribers. So they were
[00:04:42.280 --> 00:04:46.360]   getting less traffic. But then they also said that their conversion of paid subscribers has
[00:04:46.360 --> 00:04:53.000]   gone up fourfold. So it's sort of a balancing act. Do you want the free traffic that you're
[00:04:53.000 --> 00:04:58.200]   monetizing with ads? Or do you want less traffic? But when you're not giving it away, people are more
[00:04:58.200 --> 00:05:02.200]   inclined to then pay for it. And I think they're kind of feeling their way with it and now kind of
[00:05:02.200 --> 00:05:06.760]   figuring out what's next, you know, but clearly not happy with the traffic drop. Otherwise, I don't
[00:05:06.760 --> 00:05:11.720]   think they would have mentioned it. How long how long was this project in place? I feel like for
[00:05:11.720 --> 00:05:17.320]   quite a while, right? A few years at least. Oh, yeah. First click free goes back at least to 2008.
[00:05:17.320 --> 00:05:24.920]   Most publications make use of it that have paywalls. You know, it's kind of seemed a good balancing
[00:05:24.920 --> 00:05:30.600]   act, but it's come under fire over the years. The New York Times, for example, gets very,
[00:05:30.600 --> 00:05:35.880]   very restrictive about it. And yet, and the long future owns the same on social media,
[00:05:35.880 --> 00:05:39.880]   they are much more liberal. So you want to free read for something that they shared on social
[00:05:39.880 --> 00:05:45.320]   media? Cool. But you want to free me read from something you found in Google? Not so cool. And
[00:05:45.320 --> 00:05:51.640]   they still kind of seem to view social media as a marketing platform for them or worth the free
[00:05:51.640 --> 00:05:57.320]   giveaway there. Whereas they kind of view Google as somehow freeloaders are just coming in and
[00:05:57.320 --> 00:06:01.800]   stealing all their traffic and all their pages for free. Although in both of those cases,
[00:06:01.800 --> 00:06:07.160]   it's not like those pages don't have ads on them. And that's ultimately the thing here. It's like,
[00:06:07.160 --> 00:06:10.840]   well, if you're a subscription website and you feel like you're losing subscriptions,
[00:06:10.840 --> 00:06:14.120]   why do you have all the ads on that stuff? And that's because really you're not just a
[00:06:14.120 --> 00:06:20.040]   subscription website. So yeah, that's where we're at. And I'm waiting to hear back. I don't know if
[00:06:20.040 --> 00:06:23.560]   I will hear back, but I did try to follow up with the long future journal to find out. Well,
[00:06:23.560 --> 00:06:29.640]   is the new subscriptions are getting offsetting the amount of money that you're losing in potential
[00:06:29.640 --> 00:06:35.640]   ad revenue? And also what they think the solution should be. You know, the balancing act, I think
[00:06:35.640 --> 00:06:42.600]   Google basis is, do we show you have a limited amount of articles and listings you can show in
[00:06:42.600 --> 00:06:48.600]   search results? Most people do not have subscriptions to these publications. So do you take up some of
[00:06:48.600 --> 00:06:53.720]   that space, putting things in there that causes a user to click on them, and then maybe the majority
[00:06:53.720 --> 00:06:58.520]   of them get frustrated. And then when they get frustrated, they don't blame the Wall Street Journal
[00:06:58.520 --> 00:07:02.760]   for having a paywall. They blame Google for like, why did you put this thing that wasn't useful to me?
[00:07:02.760 --> 00:07:06.040]   But there's also an argument I wrote about this about two or three years ago where,
[00:07:06.040 --> 00:07:09.880]   you know, potentially if Google puts more subscription content up there,
[00:07:09.880 --> 00:07:14.440]   it causes people to understand there's more value to it. It's like, yeah, you want this
[00:07:14.440 --> 00:07:20.040]   news content you pay for it. So, you know, they're paying. Yeah, as long as people are actually doing
[00:07:20.040 --> 00:07:25.000]   that conversion. Now, you mentioned that there are other sites that are also like, like, that this
[00:07:25.000 --> 00:07:31.720]   is a pretty common thing. What can, I mean, I'm sure there's a lesson here for those sites to learn.
[00:07:31.720 --> 00:07:36.920]   Is there any like clear kind of advice to come out of this switch, this decision that Wall Street
[00:07:36.920 --> 00:07:41.880]   Journal made that other sites that are trying to do this have learned or can't?
[00:07:41.880 --> 00:07:45.080]   Well, I think you need to understand. And I think like the times of London and some other
[00:07:45.080 --> 00:07:50.920]   places have learned this in the past too, that if you put up barriers to Google coming into your
[00:07:50.920 --> 00:07:56.440]   website, you're going to get less Google traffic. That's SEO 101. And so you kind of have to
[00:07:56.440 --> 00:08:01.560]   understand, is that worth the trade off? You know, it's very easy that people want to beat on Google
[00:08:01.560 --> 00:08:06.520]   especially in the news industry that they're somehow stealing all their traffic and screwing
[00:08:06.520 --> 00:08:11.400]   them up. And then it's like, well, so apparently the Wall Street Journal was getting a lot of traffic
[00:08:11.400 --> 00:08:16.760]   from Google without paying for any of that traffic, by the way, and traffic that they found so useful
[00:08:16.760 --> 00:08:23.080]   that they're now upset about it. So, and this is from a company whose, you know, CEO and leaders
[00:08:23.080 --> 00:08:29.240]   have been merciless in slamming Google as this like horrible thing to news industry. It's just like,
[00:08:29.240 --> 00:08:32.360]   so now you're upset when you took your stuff out, you didn't get all the free stuff in the first
[00:08:32.360 --> 00:08:37.080]   place. So, I guess it's kind of like make up your mind. Do you want the free traffic from Google?
[00:08:37.080 --> 00:08:41.080]   And if you do, then you understand these are the trade offs that are coming with it.
[00:08:41.080 --> 00:08:44.920]   Or do you become very segmented in what you're doing? You understand what content you want to
[00:08:44.920 --> 00:08:49.960]   give away for free. You've got that in a way that you can list it in a way for free. And you know,
[00:08:49.960 --> 00:08:53.960]   you've got your subscription content and maybe you're being much more religious about it really is
[00:08:53.960 --> 00:08:59.160]   subscription content. We're not having all these these holes that are in there.
[00:08:59.160 --> 00:09:04.920]   I wonder what's kind of a valuable metric for the Wall Street Journal in the sense that,
[00:09:04.920 --> 00:09:11.000]   you know, is straight up page views on these articles important. Obviously, money is
[00:09:11.000 --> 00:09:15.720]   important to run the business, but I have to do the conversion in order to pull that off.
[00:09:15.720 --> 00:09:19.880]   There's got to be value on both sides. And if they're serving up ads, and we can talk more
[00:09:19.880 --> 00:09:24.120]   about ads in this next story here, but if they're serving up ads, at least they're making something.
[00:09:24.120 --> 00:09:27.000]   But I guess we just don't know what they're making in Pearson.
[00:09:27.000 --> 00:09:35.080]   Well, speaking of ads, Google is we kind of had heard a little bit about this in the past.
[00:09:35.080 --> 00:09:39.720]   And now it's becoming a little bit more clear. Google is going to be including an ad blocker
[00:09:39.720 --> 00:09:46.040]   in Chrome for mobile devices as well as for desktop. I think this is going to happen sometime in the
[00:09:46.040 --> 00:09:51.960]   early 2018, sometime early next year. And I don't know, I'm very curious to get both of your take
[00:09:51.960 --> 00:09:57.160]   on this because it seems like seems like a strange line that's being crossed here. So they're going
[00:09:57.160 --> 00:10:03.480]   to be blocking quote, bad ads. And Google did say, including those owned and served by Google,
[00:10:03.480 --> 00:10:09.000]   what are bad ads? You know, bad ads would include pop ups, autoplay ads, flashing image ads,
[00:10:09.000 --> 00:10:14.440]   those sorts of things. Basically, sites need to comply with better ads standards per
[00:10:14.440 --> 00:10:19.880]   the coalition for better ads to avoid this blocking. But the members of that coalition,
[00:10:19.880 --> 00:10:26.840]   of course, are Google and Facebook, which account for somewhere around 99% of digital ad revenue
[00:10:26.840 --> 00:10:34.600]   growth in the US last year, 77% of gross ad spend. It just it seems, I don't know, is there a
[00:10:34.600 --> 00:10:40.520]   potential for some is Google the right is Google the right one to make this decision when they
[00:10:40.520 --> 00:10:46.680]   are so intrinsically tied into what makes a good ad or a bad ad and they cover the majority of the
[00:10:46.680 --> 00:10:52.920]   market. What do you think? Well, when your advertiser network sets the quality of your ads,
[00:10:52.920 --> 00:10:59.400]   that actually can be a good thing. I see where the problem comes in. But my one question about
[00:10:59.400 --> 00:11:06.760]   this, and I don't know if you know the answer to this, is autoplay ads inside content. So if like,
[00:11:06.760 --> 00:11:12.520]   I used to work at Fortune and we had these autoplay ads and autoplay videos that we had to insert
[00:11:12.520 --> 00:11:17.640]   into every story. So is that a bad ad or is that content just really irritatingly delivered?
[00:11:17.640 --> 00:11:23.560]   That's a really great question. I mean, I don't know. I know that anytime there's an autoplay
[00:11:23.560 --> 00:11:29.160]   ad on a page, I want to scream. Well, yeah, any on the same effect for me anyway,
[00:11:29.160 --> 00:11:34.760]   I don't work. So that's a question because then publishers would probably just hide the ads a
[00:11:34.760 --> 00:11:42.120]   little bit more in things that look closer to content. These are obviously publishers,
[00:11:42.120 --> 00:11:47.960]   not like the crazy publishers, like outbrain and stuff that are just crazy clickbait. But
[00:11:47.960 --> 00:11:57.240]   I don't know. I mean, that's irritating. And I would call that a bad ad and it uses my data
[00:11:57.240 --> 00:12:02.360]   plan and I get all cranky. So I would love for those to be blocked, or at least to maybe say,
[00:12:02.360 --> 00:12:05.480]   hey, do you want to play this before it actually happens?
[00:12:05.480 --> 00:12:11.640]   Right. Yeah, somewhere a long time ago, I changed the settings, you know, when kind of this conversion
[00:12:11.640 --> 00:12:17.320]   to flash or away from flash was happening. And I changed the settings so that anytime there was
[00:12:17.320 --> 00:12:24.120]   a flash element on the screen, never make it appear automatically always ask me like I had to
[00:12:24.120 --> 00:12:29.640]   actively enable that module anytime I actually wanted to see whatever the flash content was,
[00:12:29.640 --> 00:12:34.200]   something similar like that potentially, because that was actually really helpful. I avoided a lot
[00:12:34.200 --> 00:12:40.120]   of pain with flash. But I don't know, more and more, we're not really seeing as much flash these days.
[00:12:40.120 --> 00:12:47.080]   And that's I think a good thing. They say supposedly there's there will be a threshold for sites.
[00:12:47.080 --> 00:12:53.160]   So it's not as if a site offers a single bad ad and at which point Google blocks all the ads on
[00:12:53.160 --> 00:12:59.320]   the site, there will be a threshold of quote, bad ads that would have to be passed on a site
[00:12:59.960 --> 00:13:05.240]   before Google would then decide to block all the ads on the site and tell the site or the publisher,
[00:13:05.240 --> 00:13:10.920]   you know, when in there and tweaked it and got rid of the bad ones, essentially.
[00:13:10.920 --> 00:13:17.240]   Who knows what that threshold will be. But it'll also be active by default. So this is not
[00:13:17.240 --> 00:13:22.280]   something that you opt into. It'll be new Chrome browser released. Everybody's getting blocked ads
[00:13:22.280 --> 00:13:24.440]   and you have to opt out of it if you want.
[00:13:26.920 --> 00:13:32.840]   Well, I can't see a lot of people choosing to opt out. I can as a publisher who's trying to get,
[00:13:32.840 --> 00:13:39.880]   you know, attention, like, Oh, crap, or my ads going to be bad. I have a pop up that shows to my
[00:13:39.880 --> 00:13:45.720]   readers on the site. If they go there, I think it's every 30 days. So like it refreshes every
[00:13:45.720 --> 00:13:49.880]   month. It'll pop up for you the first time you visit for that month, right? I'm like,
[00:13:49.880 --> 00:13:54.760]   I don't love it. But I'm also like, it only happens once a month for people. So maybe
[00:13:54.760 --> 00:14:06.760]   I don't know. What do you think, Danny? Where do you follow on this idea of Google backing this
[00:14:06.760 --> 00:14:11.640]   ad blocking? Yeah, I mean, I think there's a good degree of unease of Google's going to have this
[00:14:11.640 --> 00:14:17.720]   display of stripping out ads, especially for their own, you know, but our ads will probably be
[00:14:17.720 --> 00:14:22.280]   all right. I mean, when they say, well, our ads will fall into it, you can be sure that by the
[00:14:22.280 --> 00:14:26.600]   time they get to this, that wall we cleared up, like they shouldn't be running as that that are
[00:14:26.600 --> 00:14:31.000]   having that kind of issue. Right. What they might have are people who are using there as
[00:14:31.000 --> 00:14:39.480]   serving technology who still do things that they don't like. But I don't know, I, we'll see how it
[00:14:39.480 --> 00:14:44.120]   gets when it's out there and it's implemented. It's really easy that you hear these things come
[00:14:44.120 --> 00:14:49.960]   along, and they're going to be this big, huge change. And we've had that with Internet Explorer.
[00:14:49.960 --> 00:14:53.880]   And I certainly haven't felt like when Internet Explorer said, well, we're blocking ads that
[00:14:53.880 --> 00:14:59.800]   that really changed things much. We had that when Safari, especially in iOS, rolled out the support
[00:14:59.800 --> 00:15:03.720]   for the third parties and it was like, well, that's it. Ads are dead. And then like, you know,
[00:15:03.720 --> 00:15:07.880]   the ads are still there. And you know, a lot of people downloaded those extensions to begin with
[00:15:07.880 --> 00:15:13.240]   or the, you know, the apps and then they didn't seem to do much with it. So, you know, I think
[00:15:13.240 --> 00:15:17.880]   the main thing is, are you really introducing something that's going to fundamentally improve
[00:15:17.880 --> 00:15:23.000]   the experience or are we just introducing something that pretty much keeps us seeing all the same
[00:15:23.000 --> 00:15:28.360]   kind of ads that we get as before and takes out a little bit of the stuff. And I think a lot of
[00:15:28.360 --> 00:15:32.120]   us would like to have some improvement. I think when you go to a website and it's loading up 29
[00:15:32.120 --> 00:15:38.920]   different trackers and, you know, 49 different ads, like, okay, that's a bit much. You know,
[00:15:38.920 --> 00:15:44.120]   how do you clean up that experience? So, and they certainly have the platform to do it with Chrome.
[00:15:45.480 --> 00:15:50.040]   You know, ultimately, I think I'd feel better about it if I felt the major
[00:15:50.040 --> 00:15:55.880]   browser people altogether were sort of united. But, you know, I can dream for many things of unity.
[00:15:55.880 --> 00:16:02.200]   Well, when someone someone posted this in the chatroom and it's worth mentioning just because
[00:16:02.200 --> 00:16:06.920]   of who's involved, but Google is going to provide a tool for publishers that they can run to see
[00:16:06.920 --> 00:16:13.240]   if their ads are in violation and would be blocked. But the people who are part of this include Google
[00:16:13.240 --> 00:16:20.440]   Facebook, News Corp, and the Washington Post. So, that makes me feel like at least big publishers
[00:16:20.440 --> 00:16:26.840]   aren't going to be even angry or Google because of something like this. Which, you know, yay.
[00:16:26.840 --> 00:16:34.200]   I think another another element of this too is that to a certain degree, like, if you were to ask
[00:16:34.200 --> 00:16:38.920]   any one who's interested in technology, whether they run an adblocker, there are people that are
[00:16:39.480 --> 00:16:44.840]   all for ad blocking because, you know, it's my internet experience and anything that degrades
[00:16:44.840 --> 00:16:49.240]   my experience I want to get rid of. And there are people that are vehemently opposed to ad blocking.
[00:16:49.240 --> 00:16:53.880]   They might not like the ads or the experience that it brings with it. But, ethically, they feel
[00:16:53.880 --> 00:16:59.320]   like if they're blocking ads, then they're ripping somebody off and no, I don't want to do that,
[00:16:59.320 --> 00:17:05.800]   you know, by any means. So, it'll be interesting to have somewhat, you know, a company as as powerful
[00:17:05.800 --> 00:17:10.760]   and especially, you know, with Chrome, which is something like what, 44.5% of all web browsers
[00:17:10.760 --> 00:17:15.880]   online according to the data analytics program, you know, compared to all the other browsers,
[00:17:15.880 --> 00:17:20.440]   it's kind of blowing them away to have someone, a company with that kind of backing,
[00:17:20.440 --> 00:17:26.600]   putting this out there by default and seeing kind of, I don't know, like when this happens,
[00:17:26.600 --> 00:17:33.480]   and suddenly all of these ads are being blocked as a result of it. And let's be serious, none of
[00:17:33.480 --> 00:17:36.920]   those are going to end up being Google ads because Google's going to take care of it ahead of time.
[00:17:36.920 --> 00:17:42.360]   Like you said, Danny, then will the call be, oh, look, Google's just squashing any ad that
[00:17:42.360 --> 00:17:48.440]   isn't its own ads because they're bad. And, you know, look who benefits it's Google, the one's doing
[00:17:48.440 --> 00:17:54.120]   the squashing. I don't know if that'll end up being a big issue or not, but I can, I can already
[00:17:54.120 --> 00:18:02.440]   see the articles. I can see them flashing before my eyes. You live in the future. Yes. All right.
[00:18:02.440 --> 00:18:08.360]   I'm telling the future right now. Also related to this is something called funding choices and
[00:18:08.360 --> 00:18:12.920]   which is actually, have either of you ever used a Google service called contributor?
[00:18:12.920 --> 00:18:19.080]   I'm sure you've heard of contributor. Is this where you, is this the one where you answer the
[00:18:19.080 --> 00:18:26.280]   questions? No, no, no. This is basically it's a paid service. I think you choose what you pay up to
[00:18:26.280 --> 00:18:31.800]   like five or $10 a month or something like that. And what it essentially does is it's ad blocking
[00:18:31.800 --> 00:18:35.800]   that you paid for ahead of time. Curse you, Google. Okay, yes, I have it.
[00:18:35.800 --> 00:18:40.440]   With partner sites. So any of the sites that fall under the purview of this,
[00:18:40.440 --> 00:18:46.120]   you'd pay like $5 at the beginning of the month. And then that would cover you when you go to those
[00:18:46.120 --> 00:18:50.680]   sites. You're a member of this, this program. You won't actually see the ads on there because
[00:18:50.680 --> 00:18:55.880]   instead of seeing them, you kind of paid up front for them. So funding choices is kind of the next
[00:18:55.880 --> 00:19:03.000]   phase of contributor. And what it's going to allow is for sites to sites that work with
[00:19:03.000 --> 00:19:09.480]   funding choices to show a custom message to people who are using ad blockers when they hit the site.
[00:19:09.480 --> 00:19:14.440]   That custom message can say something like, you can enable ads on our site or you can pay for
[00:19:14.440 --> 00:19:22.600]   contributor to remove all of the ads, you know, legitimately, let's say. And so that's going to
[00:19:23.400 --> 00:19:28.600]   start happening. Apparently, funding choices also hides more than just ad sense ads.
[00:19:28.600 --> 00:19:35.080]   And every page view, you know, has an associated ad cost somewhere around like a penny per page,
[00:19:35.080 --> 00:19:41.000]   which I don't know, the way some of us surf the internet, I suppose that could add up to be a lot
[00:19:41.000 --> 00:19:46.440]   if you really work it out. Oh, God, yes. I'm like, whoa. Yeah, Penny for page, you know,
[00:19:46.440 --> 00:19:52.040]   the monthly cost of that suddenly your web browser, your, your broadband cost just doubled
[00:19:52.040 --> 00:20:00.280]   because you're flat fee that you pay per month. You can pay a specific fee. But from my understanding,
[00:20:00.280 --> 00:20:06.440]   that covers only a certain amount of views, right? Only a certain amount of pages. And then at that
[00:20:06.440 --> 00:20:11.640]   point, I'm guessing I didn't read it, but I'm guessing you would just start seeing ads then at
[00:20:11.640 --> 00:20:16.520]   that point. It's like you run out of your funding and you avoided ads. But here you go.
[00:20:17.400 --> 00:20:21.480]   So one of those stupid slideshows could be like a full quarter. Holy cow.
[00:20:21.480 --> 00:20:23.080]   All right. Like I really add up.
[00:20:23.080 --> 00:20:27.560]   Wow, that would you would have to think about like,
[00:20:27.560 --> 00:20:35.720]   what's interesting. So sorry, I was digressing. What's interesting here is that all of the,
[00:20:35.720 --> 00:20:41.800]   what the internet does and what kind of is happening with basically delivering all of this
[00:20:42.360 --> 00:20:47.320]   connected content is everything becomes super transactional because we can very clearly
[00:20:47.320 --> 00:20:54.600]   understand the costs associated with everything, right? And there is no more like hiding things
[00:20:54.600 --> 00:21:00.440]   behind like, Oh, it only cost me $20 to subscribe to the paper. And I'm supporting it, but really,
[00:21:00.440 --> 00:21:05.160]   it's all of the advertisers. So it's kind of a fascinating thing to think about how this plays
[00:21:05.160 --> 00:21:11.960]   out and how how different sites will be valued and who would pay for what just ab like,
[00:21:11.960 --> 00:21:16.840]   on a true per page or per cost of delivering a page basis.
[00:21:16.840 --> 00:21:19.560]   If people are willing to do it.
[00:21:19.560 --> 00:21:25.320]   I mean, and one of the gaps with the funding choice, the thing is that you could have websites
[00:21:25.320 --> 00:21:30.280]   that don't have ads who can't participate because there's no ads to block, right?
[00:21:30.280 --> 00:21:33.560]   So what they really need is a Google contributor where people can pay, but then
[00:21:33.560 --> 00:21:38.760]   they really need a Google contributor where people pay as they land, not where they can say,
[00:21:38.760 --> 00:21:42.760]   Oh, yeah, I read that. Now I'm thinking after the fact I want to do it, that, you know,
[00:21:42.760 --> 00:21:47.960]   potentially you could have sites that just put up fake ads and say support us. So those ads can
[00:21:47.960 --> 00:21:52.840]   be blocked for the ad blocking people. But yeah, I agree with you. Maybe we'll get to this shift
[00:21:52.840 --> 00:21:57.400]   for people are thinking much more about, you know, I am paying for everything. The web actually
[00:21:57.400 --> 00:22:01.880]   isn't free. And I'm paying for one way or the other, either I'm paying for it because someone's
[00:22:01.880 --> 00:22:06.040]   going to be showing me an ad or I'm paying for it because I'm actually showing out real,
[00:22:06.040 --> 00:22:12.120]   real hard cash. But, or you're paying for it in terms of your demographic data.
[00:22:12.120 --> 00:22:19.400]   Yeah. But you know, at least it gives an option. You've, you know, when the ad blockers, I think,
[00:22:19.400 --> 00:22:23.160]   especially on Safari and Mobile Safari came out, you would have people who didn't start getting
[00:22:23.160 --> 00:22:28.920]   these blocks and they'd be upset. And it's like, like, at least they have a solution now for them,
[00:22:28.920 --> 00:22:33.400]   right? Well, if you don't want to turn off your ad blocker on this site, then I guess you can buy
[00:22:33.400 --> 00:22:36.760]   this type of thing, whether they actually then follow through with their principles of,
[00:22:36.760 --> 00:22:41.160]   I don't want to be see ads to the degree that all pay for stuff. We'll, we'll see.
[00:22:41.160 --> 00:22:49.000]   Yeah, absolutely. I know Ron Richards on the other show I do on the network, all about Android.
[00:22:49.000 --> 00:22:55.160]   He has been using contributor, I think now for a few years and he's, he's loved it. So,
[00:22:55.160 --> 00:23:01.160]   so it's, it's definitely a viable option. It's an interesting kind of approach for this,
[00:23:01.160 --> 00:23:05.080]   if what you want is to kind of improve, clean up the experience of the web,
[00:23:05.080 --> 00:23:09.720]   but not feel like you're just jipping every, every site that you go to because you've got an
[00:23:09.720 --> 00:23:14.280]   all out, you know, like brick wall ad blocker. That's just shoving it all off.
[00:23:14.280 --> 00:23:21.400]   So I guess we'll see, I guess we'll see the value, the actual value to people as far as like,
[00:23:21.400 --> 00:23:27.320]   if it's worth, worth it to people to remove those ads and to pay a little bit of money upfront in order
[00:23:27.320 --> 00:23:32.360]   to do it. Maybe if the ads were more obnoxious, it would be better and worth it.
[00:23:32.360 --> 00:23:38.120]   We're worth it. I'm sure they're going in that direction. I mean,
[00:23:38.120 --> 00:23:42.920]   Google's got a couple of different fronts here tackling this and I mean, essentially,
[00:23:42.920 --> 00:23:49.320]   it seems like making money on all sides. Yes. We'll show you an, we'll let you pay
[00:23:49.320 --> 00:23:56.440]   us to walk the ad. Well, exactly, exactly. Let's see here. What else? We've got some other
[00:23:56.440 --> 00:24:01.000]   Google stuff. There's, there's no, I mean, those were the things that actually interconnected.
[00:24:01.000 --> 00:24:05.480]   Now everything is like little pieces here and there. So we can just hop, hop through some of the
[00:24:05.480 --> 00:24:10.600]   stuff. I saw this actually today and I think I missed it. It was yesterday's news that Google is
[00:24:10.600 --> 00:24:18.520]   working on an initiative. It's a project that they're calling B internet awesome. And it's a way to
[00:24:18.520 --> 00:24:26.600]   educate kids. And I think it's aimed more at younger kids on how to protect themselves online. It was
[00:24:26.600 --> 00:24:33.880]   developed by teachers, internet safety and literacy organizations. And it's a program to kind of advise
[00:24:33.880 --> 00:24:40.280]   kids who are just learning about their digital life online to understand, you know, why you should
[00:24:40.280 --> 00:24:46.280]   limit sharing your personal information, how you avoid scams or why, you know, what might be a scam
[00:24:46.280 --> 00:24:53.560]   and the making password health. Just in general avoiding bad behavior online. And they've got a,
[00:24:53.560 --> 00:24:58.120]   like a curriculum for classrooms. They've got a game that you can play through that.
[00:24:58.120 --> 00:25:04.680]   What is the game called? It's sound my view here. In Turland. There you go. In Turland.
[00:25:04.680 --> 00:25:10.520]   That kind of walks you through some of the concepts and everything. And it's all about
[00:25:10.520 --> 00:25:15.960]   educating kids on being better internet denizens and protecting themselves. I think this is a great
[00:25:15.960 --> 00:25:22.200]   effort. I do too. I mean, because I will say as a parent who's tech savvy,
[00:25:22.200 --> 00:25:29.560]   it's actually hard to remember all of the things that are that you kind of take for granted. Absolutely.
[00:25:29.560 --> 00:25:38.280]   And so having something that codifies it is really helpful. And, you know, we spend time talking to
[00:25:38.280 --> 00:25:45.320]   my daughter about like not clicking on emails and, you know, posting things online. But it's still
[00:25:45.320 --> 00:25:51.400]   like, you know, she's on a, she plays a online game right now. And we had to sit down and we
[00:25:51.400 --> 00:25:55.480]   realized that it had like chatting. So then we had to talk about like, hey, don't share your
[00:25:55.480 --> 00:26:01.800]   password with people. And, you know, it's weird having to like teach someone that. But there are
[00:26:01.800 --> 00:26:11.000]   people who go around and, you know, scam kids or worse. Right. So this is great. I love it. Because
[00:26:11.000 --> 00:26:16.920]   I don't want to have to remember everything. Yeah. And Google has really seems like Google is on
[00:26:16.920 --> 00:26:24.360]   a role right now with really rolling out initiatives and efforts and apps and services into Android
[00:26:24.360 --> 00:26:29.960]   that family link not too long ago that are really designed around kind of improving the family,
[00:26:30.760 --> 00:26:34.360]   the family experience with their products. Of course, they benefit because they get,
[00:26:34.360 --> 00:26:40.520]   you know, they get users from a very young age to have brand affinity with Google products and
[00:26:40.520 --> 00:26:46.040]   to learn it. But I, you know, on the flip side of that, we live in a world where all this stuff is so,
[00:26:46.040 --> 00:26:52.280]   so, you know, pervasive in just a part of life that it's important to kind of know it to a certain,
[00:26:52.280 --> 00:26:58.360]   you know, at a certain point, you have to know this stuff. So yeah. So I mean, I apply, I applaud
[00:26:58.360 --> 00:27:04.680]   Google for putting this together and doing it. And I suppose we'll see, you know, it's free
[00:27:04.680 --> 00:27:08.040]   resource. Anyone can go there and check this stuff out. I don't think you have to be in a
[00:27:08.040 --> 00:27:13.080]   classroom curriculum in order to do it. I'll get my daughter to go through it and we'll see how
[00:27:13.080 --> 00:27:17.400]   we feel about it after it's. I can tell you guys next, maybe not next week, but the week after
[00:27:17.400 --> 00:27:22.520]   that, I'll tell you guys what we thought. I would say, here's what I would like to see. I would
[00:27:22.520 --> 00:27:27.560]   like to see a corollary to this that talks about like data collection and how companies like Google
[00:27:27.560 --> 00:27:32.280]   and others use your information. So kids are aware of that side of the internet too.
[00:27:32.280 --> 00:27:39.000]   Yeah, I guess the big challenge here is that kids are capable of so much yet you can easily
[00:27:39.000 --> 00:27:47.000]   overwhelm them. Information fall off the edge of the cliff and, but it's all so important. Like,
[00:27:47.000 --> 00:27:52.520]   you know, I was thinking about this just yesterday, the story about how Harvard
[00:27:53.880 --> 00:28:00.280]   nixed a bunch of incoming students because of really offensive memes that they placed,
[00:28:00.280 --> 00:28:05.800]   you know, that they posted to a private Facebook group. And I don't know how to formulate my
[00:28:05.800 --> 00:28:11.560]   thoughts around it exactly, but it kind of ties into this idea that like, we, you know, we are of
[00:28:11.560 --> 00:28:17.720]   a generation of an age where we've grown up with the internet, like, at least for myself,
[00:28:17.720 --> 00:28:22.040]   the internet came along when I was already kind of a young adult. And so I kind of learned these
[00:28:22.040 --> 00:28:28.120]   things over time. But I mean, kids nowadays, like this is just they're born and they're surrounded
[00:28:28.120 --> 00:28:32.600]   by the internet. And like, I don't know, is there a different way of thinking about like,
[00:28:32.600 --> 00:28:37.640]   what's okay to share out there? And how do we, how do we let them know that like, no, this stuff
[00:28:37.640 --> 00:28:41.240]   actually has repercussions? Like, oh, by the way, you were accepted into Harvard. And now you're
[00:28:41.240 --> 00:28:45.560]   not going because you posted this thing on the internet that even though it's a private Facebook
[00:28:45.560 --> 00:28:50.280]   group is not entirely private. It's not like it was sealed and no one could see it. It's very easy
[00:28:50.280 --> 00:28:54.600]   to take a screenshot. It's very easy to do all these things that make something that you think is
[00:28:54.600 --> 00:28:58.920]   private is not. And how do you communicate that to kids? And I think that's a really big challenge.
[00:28:58.920 --> 00:29:06.280]   So I will say we did that with my daughter. The second she had access to the internet,
[00:29:06.280 --> 00:29:14.040]   which was probably like at age seven, we just, we tell her this all the time. Nothing on the
[00:29:14.040 --> 00:29:20.840]   internet is private. Nothing. Nothing on your phone is private. And you know, that's the,
[00:29:20.840 --> 00:29:28.600]   so I think of web safety and tech savviness is probably similar to teaching your kids about sex
[00:29:28.600 --> 00:29:33.240]   education kind of stuff because you don't overwhelm them with everything all at once. But
[00:29:33.240 --> 00:29:38.920]   you have to start talking about it very early. The second, you know, you're seeing them in,
[00:29:40.280 --> 00:29:45.880]   I think as parents, you also have to follow through on this yourself. So maybe you don't post, you
[00:29:45.880 --> 00:29:52.360]   know, the pictures of your kid in the bathtub on Facebook. And you know, if you do that, maybe
[00:29:52.360 --> 00:29:57.880]   you explain to them how you're limiting these settings and that you'll take it down when they're
[00:29:57.880 --> 00:30:05.720]   13 and like, Oh my God, mom. Or maybe not getting their permission to post that photo of them in the
[00:30:05.720 --> 00:30:11.160]   bathtub, but getting their permission to actually, you know, post anything on Facebook at all. I,
[00:30:11.160 --> 00:30:16.280]   I stepped away from Facebook back in December. I was just starting to feel a little overwhelmed
[00:30:16.280 --> 00:30:21.320]   with kind of the time I was spending scrolling and just some of the decisions that they were
[00:30:21.320 --> 00:30:27.560]   making just didn't really didn't really ring true to me. And I felt like I needed to step away.
[00:30:27.560 --> 00:30:34.440]   But one thing I had been struggling with up until then was this growing kind of need or feeling
[00:30:34.440 --> 00:30:40.360]   inside of me to stop sharing as much about my life on the internet, because basically what I'm
[00:30:40.360 --> 00:30:46.680]   doing is taking taking that choice away from from my kid and just basically saying, you don't have
[00:30:46.680 --> 00:30:51.320]   the choice. I've decided to share all this stuff about you. And more and more, that just felt weird
[00:30:51.320 --> 00:30:55.160]   to me. And so I had to do that, you know, giving them more of a voice. What do you think? What do
[00:30:55.160 --> 00:30:59.640]   you think about all this, Danny? I know, I mean, you're a father. I'm sure you've you've struggled
[00:30:59.640 --> 00:31:04.200]   with a lot of these issues. Well, it's funny. My son had to do a senior project. And so it's
[00:31:04.200 --> 00:31:08.760]   for a senior project. He was going to be oversharing on social media and I was like, all right.
[00:31:08.760 --> 00:31:19.160]   Because I'm I'm an overshare around the house. I don't know. I, you know, I think we talked with
[00:31:19.160 --> 00:31:24.520]   our kids about some safety things early on when they've done, you know, online gaming and stuff
[00:31:24.520 --> 00:31:30.200]   like that. We've talked about, you know, don't take requests from strangers, don't do things from
[00:31:30.200 --> 00:31:35.160]   people you don't know. I think we've been fortunate that they've been pretty savvy and kind of come up
[00:31:35.160 --> 00:31:40.760]   with it at the same time. But you know, I think the education stuff is all very good. I remember
[00:31:40.760 --> 00:31:45.640]   Eric Schmidt once talking about the idea that every teenager at 18 ought to have the right to
[00:31:45.640 --> 00:31:50.600]   just completely cleanse out their record. And I thought, oh, that's crazy. And it probably still
[00:31:50.600 --> 00:31:54.840]   is crazy in the sense that you could do it. But you know, you kind of think, yeah, actually,
[00:31:55.800 --> 00:32:01.720]   you've got people sharing stuff that really don't understand the longer term repercussions of what
[00:32:01.720 --> 00:32:08.120]   they're doing. And you know, they probably could use a good cleanse in a lot of ways. And then at
[00:32:08.120 --> 00:32:11.960]   the same time, I think we'll also be going into a culture that will become more accepting of this
[00:32:11.960 --> 00:32:17.640]   stuff. If you go back to the idea of like, you know, I think it was when Clinton was like the first
[00:32:17.640 --> 00:32:21.800]   president who said he ever smoked pot and that you finally got through this point where, okay,
[00:32:21.800 --> 00:32:27.240]   I guess that doesn't end it. That I don't want to excuse the things that people may post, but
[00:32:27.240 --> 00:32:32.120]   that the people may become more or less shocked about that sort of stuff that is, oh, you have
[00:32:32.120 --> 00:32:36.760]   this weird thing you posted when you were 15. Yeah, okay, everybody's got something like that.
[00:32:36.760 --> 00:32:42.120]   But overall, I like the idea of better education of teaching kids to think more about what they're
[00:32:42.120 --> 00:32:47.160]   doing that you don't need to share all this sort of stuff. Certainly the idea, I think one of the
[00:32:47.160 --> 00:32:53.640]   things that I find most tragic to me is when you find people who especially feel like they're doing
[00:32:53.640 --> 00:32:59.880]   all this effort to present themselves in a special way on social media that aren't feeling that way
[00:32:59.880 --> 00:33:05.080]   internally. And you know, I'm going to make myself look great, but I actually feel awful. And you
[00:33:05.080 --> 00:33:10.040]   know, you kind of want to say I just wish you had, you know, actual people that you know who are
[00:33:10.040 --> 00:33:13.880]   there to support you and tell you that you're loved so that you don't feel like you've got to
[00:33:13.880 --> 00:33:18.520]   express it that way out on social media. So yeah, that's a really good point. I mean,
[00:33:18.520 --> 00:33:23.800]   at one point, it seemed like social media, you know, is a tool that allows you to express yourself.
[00:33:23.800 --> 00:33:30.200]   And then another point, it becomes a tool that allows you to portray yourself as something that
[00:33:30.200 --> 00:33:35.320]   you wish you were or something that you aspire to be, but you're not, and that can be self-defeating
[00:33:35.320 --> 00:33:39.240]   because you're not yet you're portraying yourself in this very public sort of sense that you are.
[00:33:40.280 --> 00:33:45.720]   I can't even imagine being a kid like having to figure out what is the right approach on something
[00:33:45.720 --> 00:33:51.000]   like that is it's hard enough. But it's figuring out how to be authentic in person.
[00:33:51.000 --> 00:33:58.360]   It's crazy too, because you think on the one hand, you know, when I was like seven or eight,
[00:33:58.360 --> 00:34:03.320]   I would walk home from school alone, you know, for like a mile in the snow, but okay, maybe the
[00:34:03.320 --> 00:34:10.360]   rest of the snow, but you walked home from home, you went out, you had a key, you went to your house
[00:34:10.360 --> 00:34:16.840]   alone, you know, we get dropped off at maybe Disneyland, and it was like you had your dime,
[00:34:16.840 --> 00:34:21.560]   and you just wandered around and like, I got fined my friends. I know where all my kids are.
[00:34:21.560 --> 00:34:24.920]   I know what's going on with them. I know they have their cell phones. It doesn't make them perfectly
[00:34:24.920 --> 00:34:31.800]   safe, but our ability to watch over what our kids are doing versus say what I had when I was
[00:34:31.800 --> 00:34:37.480]   growing up is hugely different. I mean, before you could have said, well, I'm at a friend's house,
[00:34:37.480 --> 00:34:42.440]   and now it's like, if you weren't at a friend's house, I can tell because I can either see it,
[00:34:42.440 --> 00:34:46.440]   or I know that you've turned something off or you've done something to kind of cover your track.
[00:34:46.440 --> 00:34:50.520]   So I could tell that, you know, that wouldn't be the case if that's going on.
[00:34:50.520 --> 00:34:56.280]   Plus the idea that, you know, I think a lot of us really usher our children around the idea that
[00:34:56.280 --> 00:35:01.160]   can just wander around is, you know, feels less and less of a type of thing. So on the one hand,
[00:35:01.160 --> 00:35:08.840]   I think there's an increase in the physical world safety as we've developed as a society.
[00:35:08.840 --> 00:35:15.080]   And now it's just weird or bizarre or strange, or it's just a whole new world where the virtual
[00:35:15.080 --> 00:35:25.080]   world safety now has a lot of catching up to do. Yeah, that is super true. How do I, I'm very
[00:35:25.080 --> 00:35:30.680]   interested in this topic, obviously, as a parent of two kids, and they are right at the point right
[00:35:30.680 --> 00:35:35.480]   now, definitely, you know, my older daughter is seven. And so I'm kind of at this critical
[00:35:35.480 --> 00:35:40.280]   point where she's very interested in technology. And I know I need to educate more around this.
[00:35:40.280 --> 00:35:46.280]   One of the struggles that I have is this idea that, like you said, Danny, we've got as parents,
[00:35:46.280 --> 00:35:53.240]   nowadays, we have all these tools that we can use. They're available to us for free to be able to
[00:35:53.240 --> 00:35:58.600]   kind of track how our kids use the internet, track how our kids move across the landscape of the real
[00:35:58.600 --> 00:36:05.320]   world and all of these things. And I struggle with this kind of weird, weird place where is,
[00:36:05.320 --> 00:36:12.360]   how do our kids respond to being watched all that, all that time? You know, that that's certainly,
[00:36:12.360 --> 00:36:17.080]   when I was a kid, right, I could go to someone's house at the very, you know, at the very least,
[00:36:17.080 --> 00:36:20.280]   leave a message and say, Hey, I'm over at so and so's house. I'm gonna be there until nine o'clock,
[00:36:20.280 --> 00:36:26.760]   then I'll walk home and everything's fine. Nowadays, like we, we, we would not allow for that. We require
[00:36:26.760 --> 00:36:32.360]   so much more because we have access to all these tools to know so much more. But how does that
[00:36:32.360 --> 00:36:38.680]   affect kids who are growing up now with that kind of oversight? And what does that lead to 20 years
[00:36:38.680 --> 00:36:45.800]   down the line? Obviously, we don't know the answer to that, but got any guesses. I don't.
[00:36:45.800 --> 00:36:50.280]   You don't have to do it. Yeah. Well, not that that's true. You absolutely don't.
[00:36:50.280 --> 00:36:53.160]   I'm just throwing that out there. Like, you don't have to do it. You're right.
[00:36:54.280 --> 00:36:59.480]   I'm like, we give my daughter a phone, but you know, when she's going to walk to like her friend's
[00:36:59.480 --> 00:37:03.720]   house, which we let her do. So she walks to her friend's house and she's supposed to text us when
[00:37:03.720 --> 00:37:09.240]   she gets there. And then she's supposed to text us when she leaves. So we can kind of keep an eye
[00:37:09.240 --> 00:37:13.880]   and it's the same thing. Like I was a latchkey kid. When I arrived home from school, I had to call
[00:37:13.880 --> 00:37:18.760]   my mom first thing. And if I didn't, you know, I would get a phone call. So.
[00:37:20.200 --> 00:37:24.840]   Yep. And to tie this back into Google, I'm sure you guys talked about it on this week in Google
[00:37:24.840 --> 00:37:30.520]   when it launched a few months ago, but Google trusted contacts is a app that you can put on.
[00:37:30.520 --> 00:37:35.960]   That's very, very close to what we're talking about here, kind of selecting contacts that you,
[00:37:35.960 --> 00:37:40.280]   you know, want to stay connected with from, you know, in a sense of personal safety,
[00:37:40.280 --> 00:37:47.080]   emergency, close friends, siblings, basically sharing your locations. You're walking from point
[00:37:47.080 --> 00:37:51.560]   A to point B and you want to, you want to make sure that, you know, somebody that you trust is kind
[00:37:51.560 --> 00:37:56.040]   of looking over you in a digital sense. So there are tools that Google offers to,
[00:37:56.040 --> 00:38:02.440]   to help facilitate this stuff. And we have the ability to do so much with the device in our pocket.
[00:38:02.440 --> 00:38:08.440]   Well, how old is your kid going to be when they get their device, their first device, Jason?
[00:38:08.440 --> 00:38:15.160]   Well, so I guess it depends on what is defined as their, their device. They don't have their own
[00:38:15.160 --> 00:38:19.240]   device. I have a four and a seven year old. They don't have their own device necessarily,
[00:38:19.240 --> 00:38:24.600]   but every weekend, each of them has a tablet that they can use for two, two hours each day.
[00:38:24.600 --> 00:38:29.960]   And, you know, they have YouTube kids on there. So it's the, it's the kids version of YouTube.
[00:38:29.960 --> 00:38:35.000]   And that's really all they ever do. Maybe play a handful of games, but it's really that.
[00:38:35.000 --> 00:38:39.160]   So I think in their minds, they own those tablets. They don't own those tablets.
[00:38:39.160 --> 00:38:43.560]   They aren't going anywhere with those tablets. If we go on a trip, we might bring them,
[00:38:43.560 --> 00:38:47.880]   you know, we'll bring them and they can play on them. But I don't know, I don't know what the
[00:38:47.880 --> 00:38:53.480]   right age is to say, here's a phone. Have fun because I mean, I'm already getting, getting into
[00:38:53.480 --> 00:38:57.480]   conversations with my seven year old about how friends at her school, some of her friends have
[00:38:57.480 --> 00:39:02.440]   phones. I'm like, are you serious? Well, it's weird because, yeah, some of my, my friend,
[00:39:02.440 --> 00:39:06.840]   my daughter, she's going into sixth grade, which feels like a time that might happen with a phone,
[00:39:06.840 --> 00:39:13.080]   but like her friends like call her on our phone, not for social because they don't talk in the phone,
[00:39:13.080 --> 00:39:18.120]   but they call like for projects or something or just a snagger to get her online a lot of times.
[00:39:18.120 --> 00:39:22.200]   So a lot of times they're calling to ask her to go online so they can do, you know, work via
[00:39:22.200 --> 00:39:31.640]   hangouts or whatever. So yeah, I don't know. Yeah, that's a tough, and I have a feeling that
[00:39:31.640 --> 00:39:38.120]   the age at which kids are given their own device has gotten younger and younger over the past
[00:39:38.120 --> 00:39:42.680]   probably 10 years, you know, as we've had more of these smartphones and more of our own lives as
[00:39:42.680 --> 00:39:49.800]   parents have kind of evolved around relying on these devices. And you know, lots of these apps are
[00:39:49.800 --> 00:39:55.160]   created to enable things like knowing where your kid is at all times and everything, then it becomes
[00:39:55.160 --> 00:39:59.560]   more and more enticing for parents to give the device to them at a younger age. But
[00:39:59.560 --> 00:40:04.600]   who knows what the right decision is. That's the hard part about being a parent. You just never know.
[00:40:06.440 --> 00:40:13.000]   Anyways, kind of related to this, actually, at least to a certain degree in this weird kind of
[00:40:13.000 --> 00:40:18.280]   world where we're moving into technology and everything's connected and everything. Stacy,
[00:40:18.280 --> 00:40:26.520]   you tweeted out, I think today or yesterday, this Pew research study that came out yesterday.
[00:40:26.520 --> 00:40:33.000]   It was yesterday. It was examined the kind of the deeper potential of what the internet of
[00:40:33.000 --> 00:40:37.880]   things leads to and kind of the future that we can look forward to as the internet of things
[00:40:37.880 --> 00:40:41.320]   becomes more pervasive. Tell us a little bit about what stood out to you.
[00:40:41.320 --> 00:40:46.280]   Oh, man, so we talked about this a lot on our show today that we recorded.
[00:40:46.280 --> 00:40:53.160]   Because this is something we've been talking about for a while, but the idea is that soon
[00:40:53.160 --> 00:40:58.440]   connectivity will not be something you can escape. One, because it'll be in everything,
[00:40:58.440 --> 00:41:03.560]   but two, because you'll be required to have it as part of your job or interacting with
[00:41:03.560 --> 00:41:11.000]   people or paying for things. And so then it becomes like, man, in this world that we're
[00:41:11.000 --> 00:41:17.480]   we're rapidly moving towards and kind of halfway are in, how do we know what decisions are being
[00:41:17.480 --> 00:41:25.560]   made on our behalf? How do we know if it's safe? How do we know what back-end deals are happening
[00:41:25.560 --> 00:41:32.840]   behind the scenes that end up showing you the content that you see? So it's kind of a lot of
[00:41:32.840 --> 00:41:40.440]   that. It's a really great report. I encourage everyone to read it. And it has just scary,
[00:41:40.440 --> 00:41:49.160]   scary things like, let's see, unplugging is nearly impossible now, but by 2026, it will be even
[00:41:49.160 --> 00:41:56.840]   tougher. Resistance is futile. You can't avoid. And then my next favorite thing is you can't avoid
[00:41:56.840 --> 00:42:02.680]   using something you can't discern. So much of IoT operates out of sight that people will not
[00:42:02.680 --> 00:42:12.040]   be able to unplug. I'm like, whoa. Yeah. And being kind of pulled into the benefits of
[00:42:12.040 --> 00:42:18.440]   something like IoT, like, you know, I've recently upgraded. Once I started upgrading lights in my
[00:42:18.440 --> 00:42:24.760]   house to Phillips Hue, it's like I want them all on Phillips Hue now and slowly but surely,
[00:42:24.760 --> 00:42:30.680]   something that used to be just this passive dumb light bulb has been swapped out in many
[00:42:30.680 --> 00:42:35.320]   rooms for something that connects to the internet and can be used in a number of ways for who knows
[00:42:35.320 --> 00:42:40.600]   what we hear about IoT and security all the time. There we go. Well, and I would like to,
[00:42:40.600 --> 00:42:46.920]   I would like to think, I mean, there are so many positives to this, not just the fact that you may
[00:42:46.920 --> 00:42:52.840]   never have to leave your couch again to turn on your lights. But companies do need to be
[00:42:52.840 --> 00:42:57.240]   transparent. And there is a real issue. I think the Guardian wrote this up and they made a bunch
[00:42:57.240 --> 00:43:02.920]   of good points that I know that we've made in the past. But when you're doing something like
[00:43:02.920 --> 00:43:10.600]   buying something on Amazon Echo, you don't know what the best price necessarily is because you
[00:43:10.600 --> 00:43:15.080]   don't see it. You're just saying do this. And if you set like a reoccurring subscription,
[00:43:15.080 --> 00:43:21.640]   there's not necessarily a guarantee that that's going to be the best available option all the time.
[00:43:21.640 --> 00:43:31.160]   There are just so many examples of this. Can you imagine your self-driving car maybe taking you a
[00:43:31.160 --> 00:43:36.200]   slightly different route based on your personal preferences? Maybe so you drive by a Starbucks
[00:43:36.200 --> 00:43:41.800]   and think, oh, yeah, I should totally get that. Those are all things that like,
[00:43:42.360 --> 00:43:47.560]   Carnival is doing a connected cruise line and they will know your preferences. And I was talking to
[00:43:47.560 --> 00:43:51.240]   them about wave finding because one of the things they want to do is help people negotiate around
[00:43:51.240 --> 00:43:57.800]   these epically huge ships. And I was like, so someone likes gambling, would you send them to
[00:43:57.800 --> 00:44:07.160]   their movie or their brunch by way of a casino? She's like, we might. So, okay, but will the
[00:44:07.160 --> 00:44:16.440]   consumers know that? That's scary. Or not? No, no, no, that's a really good point. So much of that
[00:44:16.440 --> 00:44:24.360]   happens online with advertising. That's a large part about how that works. You go to Amazon,
[00:44:24.360 --> 00:44:28.440]   you search for something, and then you end up going to a site a day later. And what do you know
[00:44:28.440 --> 00:44:32.680]   that thing is down there? And it's on sale. It's $10 less than it was yesterday. Do you want to buy
[00:44:32.680 --> 00:44:39.160]   it now? And I think it's easy to think that that's all purely coincidental and hey, isn't that weird?
[00:44:39.160 --> 00:44:44.840]   But as long and we're okay with it, many people are okay with it because they understand that
[00:44:44.840 --> 00:44:51.400]   that's just how ad tracking, ad marketing works. But if you don't know that happens,
[00:44:51.400 --> 00:44:58.680]   then it feels very manipulative. Yeah, I was looking to book a hotel and I had like three
[00:44:58.680 --> 00:45:03.720]   different hotel sites opened and I found all the rooms between them, pick the one that I wanted
[00:45:03.720 --> 00:45:09.080]   and shut the other two down. And I continue to get emails, hey, finish your booking, finish,
[00:45:09.080 --> 00:45:12.520]   and I know why it's happening because I've logged in and they know that I was logged in. They
[00:45:12.520 --> 00:45:16.120]   followed me all the way through this or whatever. But yeah, if you don't know. And you've been
[00:45:16.120 --> 00:45:21.400]   seeing that also where I think there's been an arise where people are convinced that their
[00:45:22.200 --> 00:45:30.200]   phones are listening to them. And where they've said, the only way I could have seen this ad is
[00:45:30.200 --> 00:45:37.240]   because I was talking to somebody about this. And it listened to me. So now it knows that I'm
[00:45:37.240 --> 00:45:40.920]   this person who was interested in that. It's like, well, actually your internet browsing probably
[00:45:40.920 --> 00:45:45.720]   gave them that data because it's become that good. But yeah, people don't know.
[00:45:47.480 --> 00:45:53.000]   I think people will figure it out over time. That's kind of our job is to explain some of the stuff
[00:45:53.000 --> 00:46:00.840]   that's happening and find good use cases. But it is becoming harder because for a lot of companies,
[00:46:00.840 --> 00:46:06.920]   like I was talking to GE's appliances division and they're looking at putting connectivity into
[00:46:06.920 --> 00:46:12.200]   everything, not because they want to give you necessarily these whiz-bang features, but because
[00:46:12.200 --> 00:46:16.680]   they want to understand how their appliances are being used and be able to track and make service
[00:46:16.680 --> 00:46:23.240]   calls easier. So I see it totally in the future. Maybe you won't get a warranty on something unless
[00:46:23.240 --> 00:46:32.840]   you connect it. And that becomes kind of coercive in a way to consumer. And maybe you don't want
[00:46:32.840 --> 00:46:38.280]   to connect it up. And maybe you don't want someone knowing when you're cooking or what you're cooking.
[00:46:38.280 --> 00:46:46.280]   Or the stuff that simply dies because you can't connect it. So I recently dug out my Zoom
[00:46:46.280 --> 00:46:50.760]   because I don't want to give too much of a spoiler away. But there was a movie where the Zoom got
[00:46:50.760 --> 00:46:55.160]   featured in, which was pretty cool. So I went out and I thought, well, I'm going to load this up and
[00:46:55.160 --> 00:47:01.480]   I'll put the movie soundtrack on it just for a laugh. And I had to set up a personal web server
[00:47:01.480 --> 00:47:08.680]   and download files from a separate thing and alter some other thing to convince the Zoom software that
[00:47:08.680 --> 00:47:13.880]   it was connecting to this Microsoft server because the Microsoft server had gone down.
[00:47:14.840 --> 00:47:23.080]   And so like, whoa, you couldn't even reset it to go back to its basic mode. It was really,
[00:47:23.080 --> 00:47:27.800]   really tied in to being connected. And I think more of our devices. Somebody else had a device
[00:47:27.800 --> 00:47:36.920]   that was like that. I always want to say DJI, but it's the DJI drones now. You have to have
[00:47:36.920 --> 00:47:40.360]   them connected to an account for them to fly. And there's advantages to that because they
[00:47:40.360 --> 00:47:45.000]   ensure that they have the right firmware and they know who they are. But then you're like, so in two
[00:47:45.000 --> 00:47:50.520]   years, if you go away, does my drone die? And maybe that's just part of, well, things don't last
[00:47:50.520 --> 00:47:54.200]   forever like they used to anyway. Not that things will ever last forever, but maybe they made it
[00:47:54.200 --> 00:48:00.280]   more than a year or two. But yeah, it's kind of scary. Or just our televisions, right? And the whole
[00:48:00.280 --> 00:48:06.520]   thing that came up with the Samsung sets or the Visio sets where you're monitoring what's being
[00:48:06.520 --> 00:48:15.080]   watched or like, Hey, I just want to watch TV. I don't. And I, or, you know, and I paid for the TV.
[00:48:15.080 --> 00:48:18.520]   You didn't give me a discount. It's not like when I bought my Kindle and they said,
[00:48:18.520 --> 00:48:25.400]   do you want to pay $20 more and not get the ads? I paid for the TV. I paid for my direct service
[00:48:25.400 --> 00:48:31.080]   every TV every month. I don't want to be data mined in that way, sort of way, unless you're going
[00:48:31.080 --> 00:48:37.000]   to give me some good incentive with it. You know, good. That's, that's what you get for having a
[00:48:37.000 --> 00:48:43.560]   TV that plugs into the internet, which is, I mean, pretty much all TVs nowadays, you know, they all
[00:48:43.560 --> 00:48:49.400]   have some sort of smart internet enabled functionality on the back. And some of them are just dumb
[00:48:49.400 --> 00:48:55.080]   monitors, but they all have that. And that basically means that like the days of just, you know, hook
[00:48:55.080 --> 00:49:00.360]   it in and 10 up to the TV. And it's all passive. I mean, they're numbered at this point because it's
[00:49:00.360 --> 00:49:05.480]   all fed through. And that's, and that's with the trade off of, of convenience and, you know, nice
[00:49:05.480 --> 00:49:09.640]   services that we all love that we look for and everything. So obviously we're getting something
[00:49:09.640 --> 00:49:13.640]   from it. It just kind of removes some of the choice, which is kind of one of the parts, you know,
[00:49:13.640 --> 00:49:20.760]   one of the points of this, this Pew research study is that the more we go down this path, the less
[00:49:20.760 --> 00:49:27.880]   choice we as users actually have to avoid it. If we so choose, it's going to be next to impossible
[00:49:27.880 --> 00:49:32.680]   to step away from it. And then you get into this weird world of like, okay, so if our lives are
[00:49:32.680 --> 00:49:37.880]   surrounded by all these connected devices, and we've chosen to go there, and everybody's like that
[00:49:37.880 --> 00:49:43.160]   now, because all devices are connected to the internet somehow. What does that say about kind of
[00:49:43.160 --> 00:49:49.640]   the civil liberties that we have built up around ourselves and have enjoyed for a very long time
[00:49:49.640 --> 00:49:56.760]   in this new world where everything is capable of being tracked and traced and monetized and
[00:49:57.640 --> 00:50:02.200]   that's 10 years from now. That's, I mean, that's now, you know, now for the next 10 years, it's
[00:50:02.200 --> 00:50:08.440]   going to get crazy. Well, so this is something that we talked about, I think last week, and we're
[00:50:08.440 --> 00:50:16.040]   talking about the role of government and regulation about like privacy. And this ties directly into
[00:50:16.040 --> 00:50:25.000]   that is we're right now at a stage where we're relying on companies to be the benign actors in
[00:50:25.000 --> 00:50:30.280]   this scenario of data mining everything and us not having a choice whether or not we're going to
[00:50:30.280 --> 00:50:36.440]   be part of this. I think we will see really well off people by unconnected devices. We'll probably see
[00:50:36.440 --> 00:50:44.520]   a robust eBay economy and like old dumb things like, heck, my mom won't even buy a car that has
[00:50:44.520 --> 00:50:55.480]   power locks. It's kind of ridiculous. Yes. But we're also like the amount of data that these things
[00:50:55.480 --> 00:51:00.120]   are connecting is insane. And I just tweeted before the show, I was talking about Uber because
[00:51:00.120 --> 00:51:06.520]   more stuff is coming about out about Uber. And to me, this is the perfect example of an IoT company
[00:51:06.520 --> 00:51:13.320]   that is running a muck with your data in doing things that are just incredibly unethical and
[00:51:13.960 --> 00:51:20.360]   not being penalized for it in ways. I mean, everything from the first mess ups around God
[00:51:20.360 --> 00:51:26.760]   view and like showing, oh, look, there's Beyonce driving around New York right now to the latest
[00:51:26.760 --> 00:51:32.760]   allegations about like people there having access to someone's medical records. I'm like, oh,
[00:51:37.880 --> 00:51:45.640]   we have to start doing something about this because it is not going to be pretty and we can't really
[00:51:45.640 --> 00:51:50.920]   rely on these companies to really protect us and not be jerks.
[00:51:50.920 --> 00:51:59.880]   Man, that's overwhelming. Sorry. No, I completely agree. I think we would all probably agree with
[00:51:59.880 --> 00:52:07.720]   that 100%. It's just like, what do you do? I mean, do we hit a point where we look back
[00:52:07.720 --> 00:52:11.800]   words and we go, man, maybe I should not have shared as maybe I should not have been
[00:52:11.800 --> 00:52:16.920]   entranced by all that technology as much as I was because look where I am now.
[00:52:16.920 --> 00:52:21.080]   Do you think that you can step it back? But you can step it back even further. I mean,
[00:52:21.080 --> 00:52:25.000]   just anybody who's ever had to deal with a credit report and you're like,
[00:52:25.000 --> 00:52:30.680]   where did they get this information on me? And this is a mission is it correct? And now I got to
[00:52:30.680 --> 00:52:36.840]   try to get it fixed. And that's a nightmare process that you go through. And we have some laws that
[00:52:36.840 --> 00:52:40.760]   allow us to deal with this sort of stuff, but we don't have laws to prevent all that stuff
[00:52:40.760 --> 00:52:45.640]   being flowing into one place every purchase you've ever made and what happens with all that sort
[00:52:45.640 --> 00:52:51.960]   of stuff and who just imagine what you do with your credit card alone. And what are your protections
[00:52:51.960 --> 00:52:57.320]   with that? And how do you push a button? And I'm not, believe me, I'm not taken away from the
[00:52:57.320 --> 00:53:01.800]   internet companies. But if I want to purge all my stuff on Google, I've got a dashboard where I
[00:53:01.800 --> 00:53:07.560]   can do it. If I want to purge all the things that I purchased on my credit card, I don't know how
[00:53:07.560 --> 00:53:14.600]   far the stuff that goes. I don't know where all that stuff is. So, you know, I can't get access to
[00:53:14.600 --> 00:53:21.400]   that data. Yeah. So, you know, you know, eventually they'll get me for that person I wiped out. No,
[00:53:21.400 --> 00:53:31.480]   but you know, you, you, we could use a better protection, I think, for just period, better consumer
[00:53:31.480 --> 00:53:36.200]   protection overall. And I don't want it to be just limited to internet companies. I'd like to see
[00:53:36.200 --> 00:53:42.040]   a good fundamental bill of rights that we have in terms of what information you can gather about
[00:53:42.040 --> 00:53:47.480]   us, what information you can compile and various sources and put together, and what our ability is
[00:53:47.480 --> 00:53:52.920]   to purge some of this stuff if we want to purge it and, and, and apply it not just to the internet
[00:53:52.920 --> 00:53:57.640]   and tech companies, but just companies in general, especially because I don't want like loopholes to
[00:53:57.640 --> 00:54:05.400]   get through, you know, but yeah, that's a good, I mean, because that's true. Right now,
[00:54:05.400 --> 00:54:10.280]   they're the tech companies, but I also have devices that are made by companies that aren't
[00:54:10.280 --> 00:54:15.560]   technically tech companies. And they're sucking down data as much as they can. So,
[00:54:15.560 --> 00:54:19.800]   I mean, anybody's gone to a grocery store. And then when the receipt spits out your,
[00:54:19.800 --> 00:54:23.800]   you got a grocery store and of course you have a loyalty card and the loyalty card gives you
[00:54:23.800 --> 00:54:27.560]   the discount and then the receipts spits out a coupon based on the path purchases that you've
[00:54:27.560 --> 00:54:31.720]   made. And you're like, Oh, I guess you know, the you keep in track of all the things I purchased.
[00:54:31.720 --> 00:54:38.280]   And, you know, who gets to see that and that sort of stuff. So, but 50 cents off of a package of
[00:54:38.280 --> 00:54:45.240]   Ritz. I'm all for that. Yeah, there we go. We use it. So here's, here's, you can play the loyalty
[00:54:45.240 --> 00:54:50.840]   card game. You just swap people's loyalty cards around. Maybe that's to create like a consumer
[00:54:50.840 --> 00:54:54.600]   network where you're like, All right, I'll mail you my stuff. You use this stuff.
[00:54:54.600 --> 00:55:00.920]   But it's also, if you go back to Uber, it always drives you crazy when people either have thought
[00:55:00.920 --> 00:55:05.640]   of Uber as a tech company or as a ride hailing company. Because I don't know what the hell
[00:55:05.640 --> 00:55:10.440]   ride hailing was. I think of them as a taxi service, right? They're a taxi company. But
[00:55:10.440 --> 00:55:15.720]   if you're going to consider them to be a tech company, because they use technology to get a
[00:55:15.720 --> 00:55:21.080]   taxi to you, then our grocery stores are tech companies. Like what is a tech company at that
[00:55:21.080 --> 00:55:26.280]   point, right? Who isn't using technology these days other than maybe my small local business
[00:55:26.280 --> 00:55:31.960]   that sells me milk when the grocery store is closed that isn't kind of doing that sort of stuff. So,
[00:55:31.960 --> 00:55:36.840]   yeah, our tech companies are broader than what we think of in terms of tech companies.
[00:55:36.840 --> 00:55:42.040]   And the amount of data that's being gathered about us is even broader than we can imagine.
[00:55:42.040 --> 00:55:47.480]   Just with the stuff that we already know. It's interesting what you were just saying about
[00:55:47.480 --> 00:55:53.880]   using a loyalty card and then getting a coupon for a discount on something you've bought in the
[00:55:53.880 --> 00:55:58.440]   past. Remember, it was a couple of weeks ago that there was news that Google was going to be
[00:55:58.440 --> 00:56:02.920]   working with credit card companies or something to be able to track your purchases, your spending
[00:56:02.920 --> 00:56:09.000]   inside stores and be able to offer you advertising around that. And that definitely had some people
[00:56:09.000 --> 00:56:12.840]   talking about the creepy line. And wow, now Google knows everything that I buy in stores.
[00:56:12.840 --> 00:56:15.880]   But that's basically what you're talking about, Danny. And we've been doing it for years with
[00:56:15.880 --> 00:56:20.520]   loyalty cards. We didn't have a problem with it then. It just goes back to what people actually
[00:56:20.520 --> 00:56:26.280]   understand about what's happening behind the scenes or just accepting the fact that it does.
[00:56:26.280 --> 00:56:32.200]   And we've been okay with it to a certain degree in one way. What's different about the way it is now
[00:56:32.200 --> 00:56:38.760]   that it's a little bit more technologically tied in a sense of the internet, or I'm not sure.
[00:56:38.760 --> 00:56:43.320]   But that changes. Wow, that was a rabbit hole. That was fun. Wow.
[00:56:43.320 --> 00:56:47.000]   We'll need something like lift us. Oh boy.
[00:56:47.000 --> 00:56:49.320]   Let's talk about HomePod.
[00:56:49.320 --> 00:56:55.240]   Yay. Thank you. That's a great that's a great say. Wait, yes. Okay. So I don't know how do you
[00:56:55.240 --> 00:56:58.680]   guys talk about Apple on the show very much? Probably doesn't happen very often.
[00:56:58.680 --> 00:57:02.440]   But you know what? We talk about everything on this show. Anything.
[00:57:02.440 --> 00:57:08.600]   Really? It's a free for all. Sometimes we even hit Google. Exactly.
[00:57:08.600 --> 00:57:12.680]   Hey, we started with Google. We did all right. There just isn't a whole lot of Google news this
[00:57:12.680 --> 00:57:21.640]   week. Yeah. So obviously Apple had their big WWDC event and keynote on Monday and of all the things
[00:57:21.640 --> 00:57:27.880]   that were demonstrated to developers and things like laptops, new Macbooks, new iMacs, whatever.
[00:57:28.600 --> 00:57:32.200]   Probably the star of the show, I would say, and the thing that people were wondering
[00:57:32.200 --> 00:57:37.240]   if it was going to happen and if they were going to see it is Apple's answer to the Amazon Echo.
[00:57:37.240 --> 00:57:42.280]   Or maybe it's Apple's answer to the Sonos speaker with a little bit of Siri tied into it.
[00:57:42.280 --> 00:57:49.720]   It's called the HomePod. And it's kind of Apple's foray into this home assistant world.
[00:57:49.720 --> 00:57:55.720]   Speaker first, though, everyone that I've read that's heard one of these things says the
[00:57:55.720 --> 00:58:01.320]   sound is tremendous. It puts out a really nice sound, especially when you apparently say it.
[00:58:01.320 --> 00:58:07.480]   When people listen to that sound in a room that Apple set up next to an Apple Echo and Sonos,
[00:58:07.480 --> 00:58:11.240]   it was amazing that the Apple, but it probably is a good set.
[00:58:11.240 --> 00:58:15.240]   It's probably, I'm guessing it's probably a little better than the Echo.
[00:58:15.240 --> 00:58:19.320]   They did a good job of the design. What do you think, Stacy?
[00:58:19.880 --> 00:58:25.720]   Well, I was going to just say this isn't out till December. And I think in some ways, the
[00:58:25.720 --> 00:58:32.360]   good speaker is fine. But in Apple did mention, I think, on a slide that it was a personal assistant,
[00:58:32.360 --> 00:58:39.000]   too. But that is so clearly missing. And my hunch is it's missing because Siri right now is
[00:58:39.000 --> 00:58:46.600]   not worth talking about as a personal assistant. And so what I expect is in the next few months,
[00:58:46.600 --> 00:58:49.960]   they're going to be working hard on Siri, maybe showing something off in September
[00:58:49.960 --> 00:58:58.360]   at the hardware launches that they do. And then when it's out in December, we'll see it.
[00:58:58.360 --> 00:59:03.560]   So I wouldn't discount it on the basis. Like, yeah, as a good speaker, I would expect nothing
[00:59:03.560 --> 00:59:08.600]   less from Apple. Music is its DNA. But we'll see.
[00:59:09.800 --> 00:59:17.480]   Yeah, it really did feel like they put Siri totally second fiddle to the idea of this being a speaker.
[00:59:17.480 --> 00:59:22.280]   And that felt to me like an admission that Siri wasn't worth featuring as a front and center
[00:59:22.280 --> 00:59:30.200]   highlight point. Although maybe, I mean, it's an expensive piece of gear. What is it? $350
[00:59:30.200 --> 00:59:35.480]   right around there? Yeah, this is not something a lot of people are going to put in every room.
[00:59:35.480 --> 00:59:40.120]   I mean, you look at the Amazon strategy, which is, you know, get people in the door, the
[00:59:40.120 --> 00:59:47.080]   echos pretty cool, or you give them a $50 dot, right? And this feels like something that's
[00:59:47.080 --> 00:59:55.320]   yeah, I mean, would you put this in every room at 350? I don't know.
[00:59:55.320 --> 01:00:02.920]   Well, yes, but like everyone. And then with the Google home, I kind of feel like
[01:00:04.280 --> 01:00:09.160]   I have one and a lot of the stuff you can do on your phone. So that may be like the poor man's
[01:00:09.160 --> 01:00:15.480]   way of getting all of these features. And it may not be a bad idea. I'm, you know, I'm perfectly
[01:00:15.480 --> 01:00:20.520]   happy with my Google home. I make no, make no secret about that. I have one now in the kitchen,
[01:00:20.520 --> 01:00:25.000]   and I have one in the living room. And of course, you know, as as I get more and more hue bulbs,
[01:00:25.000 --> 01:00:29.160]   that becomes kind of neat to be able to do a voice command and turn on my lights,
[01:00:29.160 --> 01:00:33.160]   which is kind of like, okay, great, you know, like that's that's neat and everything. How
[01:00:33.160 --> 01:00:38.600]   essential is it? But I mean, I use, I use my home all the time. My family uses the home all the time
[01:00:38.600 --> 01:00:44.200]   for music. Like we're always playing music, playing playlists. It's really easy to fire off a playlist.
[01:00:44.200 --> 01:00:48.920]   So I so when I saw the home pod, I was like, well, that's actually pretty nice as a music device,
[01:00:48.920 --> 01:00:54.120]   a voice activated music device that actually sounds really good. Personally, I'm satisfied with
[01:00:54.120 --> 01:01:01.240]   the way Google home sounds for my ears. But if you have a, I don't know, a more tuned ear and you
[01:01:01.240 --> 01:01:06.200]   require more out of the sound coming out of that little pod in the corner of your room,
[01:01:06.200 --> 01:01:12.920]   maybe it's worth $350. Danny, what voice assistants do you have? Is this appealing to you at all?
[01:01:12.920 --> 01:01:20.600]   I have both the home and the echo. And, you know, I think generally, it's smart in the sense that
[01:01:20.600 --> 01:01:25.080]   the primary thing, I think people say that you and these devices is playing music. So,
[01:01:25.080 --> 01:01:29.720]   the idea that, wow, that's what people like a lot is that can play music on these things. So we'll
[01:01:29.720 --> 01:01:36.360]   come up with a great music playing device. That's an interesting strategy. But I also don't know
[01:01:36.360 --> 01:01:43.320]   to the degree that people play music on this or audio files, right? Because I have a home speaker
[01:01:43.320 --> 01:01:48.680]   system and like my wife listened to music a lot and occasionally she remembers, oh, I can stream to
[01:01:48.680 --> 01:01:53.000]   that and she likes it. And the music is much better coming out of that because it's like all the
[01:01:53.000 --> 01:01:57.080]   speakers. But you know, typically she's just kind of want some casual music playing and the echo is
[01:01:57.080 --> 01:02:02.760]   good enough. And so I think that, you know, I think that if you get into that price range,
[01:02:02.760 --> 01:02:06.920]   you're going to go, this really is a replacer for my home speaker system because maybe I've
[01:02:06.920 --> 01:02:13.480]   already made that kind of investment or I don't know how that will play out. But I do think that
[01:02:13.480 --> 01:02:19.240]   even though the primary thing people do with these devices is playing music, I think they like the
[01:02:19.240 --> 01:02:25.240]   other aspects to it as well. I think they like the idea that echo gives them information and news.
[01:02:25.240 --> 01:02:31.160]   Certainly the heart smart home connected stuff is very helpful. And I'll be interested to see how
[01:02:31.160 --> 01:02:36.440]   first of all, I don't know whether or not the HomePod is going to deal with, say, Spotify.
[01:02:36.440 --> 01:02:41.400]   I assume that there's going to be some option for that to do it. But if there isn't, to me,
[01:02:41.400 --> 01:02:46.200]   that's almost going to make it dead in the water for a lot of people because I listen to Spotify.
[01:02:46.200 --> 01:02:50.440]   We listen to, it's Spotify, we're getting all of our stuff off of not Apple Music.
[01:02:50.440 --> 01:02:55.400]   And if it's a pure Apple Music device, there's a lot of people who use other alternatives,
[01:02:55.400 --> 01:03:00.840]   and both the Echo and the Home will turn to Spotify as well as Amazon Music as well as
[01:03:00.840 --> 01:03:07.720]   their native things that they have. Google won't play Amazon Music in the Echo or play YouTube.
[01:03:07.720 --> 01:03:13.640]   Sorry. No, I just, I'm like, right. But I mean, they both go to third parties or they both have
[01:03:13.640 --> 01:03:19.880]   their own native stuff to fall back on so that if you're a Google user, you could use Google Music.
[01:03:20.840 --> 01:03:25.160]   So if I would assume that your Apple Music will be the default for HomePod,
[01:03:25.160 --> 01:03:29.720]   and that hopefully they allow you to use, say, like Spotify, I wouldn't expect them to use
[01:03:29.720 --> 01:03:34.680]   Google Music, but we'll see. But the, the assistant aspect, I think, will be important,
[01:03:34.680 --> 01:03:40.200]   and I think Siri's pretty dumb, and I don't see Siri getting any smarter anytime sooner.
[01:03:40.200 --> 01:03:45.240]   And I, I say, especially think that so you're going to roll out a speaker that sounds better
[01:03:45.240 --> 01:03:49.800]   than the Echo, but is probably stupider than the Echo, which itself is already stupider than the
[01:03:49.800 --> 01:03:53.960]   Google Home. And if people actually want a little bit more out of these things,
[01:03:53.960 --> 01:03:59.320]   just playing music, it's not a compelling thing. It's sort of been also ran. So yeah.
[01:03:59.320 --> 01:04:03.960]   Yeah. Yeah. Then in that sense, it's a, it's a competitor to Sonos, which already, I think a
[01:04:03.960 --> 01:04:10.600]   lot of people see as overly expensive. I mean, there's a, there's definitely a fan base for Sonos
[01:04:10.600 --> 01:04:15.240]   hardware, you know, for kind of the connected speaker system. But I mean, once Google, you know,
[01:04:15.240 --> 01:04:20.360]   they, they release their Chromecast audio. And then now Google Home is like an extension of that.
[01:04:20.360 --> 01:04:25.240]   They can be their own points for the psych networks. Google Homes, Google Homes will already network
[01:04:25.240 --> 01:04:30.040]   with each other. Right. You can have two Google Homes and they will, they will do the, the playing
[01:04:30.040 --> 01:04:33.720]   in multiple rooms. Although I don't, you know, I don't think the sound quality in the home is
[01:04:33.720 --> 01:04:38.440]   as good as the Echo personally, but not that big a deal to me, right? You know, I'm three drinks
[01:04:38.440 --> 01:04:43.480]   in and I don't really care about the sound quality. I would, I would consider like, I'm a musician,
[01:04:44.040 --> 01:04:49.320]   I feel like I have an ear for sound, but I don't care about the sound quality of Google Home.
[01:04:49.320 --> 01:04:55.640]   It does exactly what I want it to do. It's a small, but you know, it takes up very small footprint
[01:04:55.640 --> 01:05:02.280]   on a, on the windowsill of my kitchen. And it's there to very easily place music on a, you know,
[01:05:02.280 --> 01:05:06.600]   in a flash when I'm like bored and wants a music while I'm washing dishes or cooking dinner or
[01:05:06.600 --> 01:05:10.840]   something. That's all I really need it for. I'm not looking at that little device to be like the
[01:05:10.840 --> 01:05:15.960]   most pristine audio file experience in the world. That's just not why I bought it.
[01:05:15.960 --> 01:05:19.480]   And I'm happy. And this reminds me of the MP3 debates from back in the day.
[01:05:19.480 --> 01:05:21.400]   Like, Oh, here we are again.
[01:05:21.400 --> 01:05:29.480]   I think they're going to, I think they're going to sell a lot of these. There are people who
[01:05:29.480 --> 01:05:33.160]   definitely care about sound quality. I'm sure the sound quality be great. I'm sure some people are
[01:05:33.160 --> 01:05:37.000]   great. I love my Apple thing. This is the best thing. It's the wonderful thing. I'm putting all
[01:05:37.000 --> 01:05:41.560]   my different rooms and it takes away from me having to have all these speakers or whatever.
[01:05:41.560 --> 01:05:44.760]   But, you know, I think there's a lot of other people who don't, don't care that much.
[01:05:44.760 --> 01:05:50.920]   There's also the fact that this is going to be available internationally to a greater extent than
[01:05:50.920 --> 01:05:57.000]   either the Echo or the Google home have been up until now. So just that availability alone is
[01:05:57.000 --> 01:06:02.520]   going to bring you back. When do we get this December? December. Yeah, so it's like,
[01:06:04.200 --> 01:06:08.920]   it's going to be tough for Christmas gifts and such. That's an expensive Christmas gift.
[01:06:08.920 --> 01:06:15.320]   Yeah. You're under $50. That was one of the smarter things the Echo did with like,
[01:06:15.320 --> 01:06:19.960]   the Echo was it launched at actually a hundred bucks for people.
[01:06:19.960 --> 01:06:24.040]   Well, the crap they did. Then at 180, you were like,
[01:06:24.040 --> 01:06:28.440]   "Oof." But then they came out soon after with the dot. And, you know, if you already had an Echo,
[01:06:28.440 --> 01:06:34.760]   you're like, "Well, crap, I get stuck at dot everywhere." And again, so I would look in the
[01:06:34.760 --> 01:06:41.240]   next couple of months for Siri to get better because you know Apple is not dumb. They know
[01:06:41.240 --> 01:06:47.560]   where their weak points are and they are addressing them. Two, I would look for the Amazon Echo
[01:06:47.560 --> 01:06:51.480]   Sonos integration because I think that's going to be a really important thing for retaining that
[01:06:51.480 --> 01:06:58.360]   market. And then three, I would look to see what they do on privacy. Like Apple really did talk
[01:06:58.360 --> 01:07:04.520]   a lot about privacy and not just in a whole humway, but they talked about encrypting the day,
[01:07:04.520 --> 01:07:10.280]   not just the device isn't always listening, but that they're encrypting the data to and from
[01:07:10.280 --> 01:07:16.040]   servers and they're anonymizing it on the servers on the back end. And that's a really big deal.
[01:07:16.040 --> 01:07:19.960]   I would agree. I would agree. But my question about that, I'm happy you brought that up because
[01:07:19.960 --> 01:07:25.960]   I think anonymizing it on the back end is a big deal for privacy. It's something that Google,
[01:07:25.960 --> 01:07:31.640]   as one example, can't do because they're so tied into their cloud information around you
[01:07:31.640 --> 01:07:37.880]   that anonymizing it would niggi-- it's why it's a-- didn't they have the same reason for why they
[01:07:37.880 --> 01:07:42.840]   can't-- couldn't encrypt aloes because if they encrypted it, then you wouldn't get that extra
[01:07:42.840 --> 01:07:47.080]   assistant functionality built in around your specific account. So I think that's the reason
[01:07:47.080 --> 01:07:51.800]   that they don't do that. So that's a big deal from a privacy perspective. This always listening
[01:07:51.800 --> 01:07:56.440]   thing that they touted. Don't they all do that? They're not all-- yeah, they all all listen.
[01:07:56.440 --> 01:08:01.160]   They all always listen. They just don't send it up to the cloud. They wait for the keyword and
[01:08:01.160 --> 01:08:06.760]   then that's the thing that they grab onto, right? Yes. That's how they all work. Okay.
[01:08:06.760 --> 01:08:11.960]   I'm sorry. Was Apple saying they were doing something different? That's what Apple says.
[01:08:13.000 --> 01:08:23.080]   But it listens for very see on the device, whereas Amazon sends five second loops that are basically
[01:08:23.080 --> 01:08:28.280]   always getting deleted to its servers for-- Well, that's interesting.
[01:08:28.280 --> 01:08:35.720]   Okay. All right. And that's why that case, it was a case a month or two ago about a crime
[01:08:35.720 --> 01:08:41.000]   that had happened. So we learned about the case a while ago, or like several months ago,
[01:08:41.000 --> 01:08:46.280]   but it was actually like a 2016 or 2015, maybe murder.
[01:08:46.280 --> 01:08:50.600]   Okay. That's really interesting. I didn't realize Amazon was sending stuff up. I
[01:08:50.600 --> 01:08:53.480]   soon did have a device specific hot word that caused it to wake up.
[01:08:53.480 --> 01:09:00.040]   No. But they don't-- They've always talked about that's how it works.
[01:09:00.040 --> 01:09:05.080]   Yeah. Okay. That's interesting. Now Google Home, is it just looking for the
[01:09:07.480 --> 01:09:12.840]   word and then once it detects that on device, then it prompts you for your query and that
[01:09:12.840 --> 01:09:15.560]   sense of the cloud. Is that the only thing that goes to your cloud is your query?
[01:09:15.560 --> 01:09:18.120]   I don't know specifically how
[01:09:18.120 --> 01:09:24.200]   Google Home works. I was like, words, what's it called?
[01:09:24.200 --> 01:09:29.640]   Okay. That's my understanding. Or at least that's always been my belief. And I'm trying--
[01:09:29.640 --> 01:09:32.840]   I'm now at this point where I'm trying to figure out is that based on something I read,
[01:09:32.840 --> 01:09:35.640]   or is that just an assumption that I made? Scooter X and Chat says yes.
[01:09:36.520 --> 01:09:40.840]   That's always been my understanding is that Google Home also, it's an open mic.
[01:09:40.840 --> 01:09:48.040]   But on device, it's waiting to hear that, "Hey, G or OKG," once it hears that,
[01:09:48.040 --> 01:09:52.280]   you do your query, that's the thing that gets sent up to the cloud. It's not doing it all the time.
[01:09:52.280 --> 01:09:58.200]   I was pretty sure that Google Home had an on device hot word, just as like I thought,
[01:09:58.200 --> 01:10:05.160]   the Xbox and Cortana or whatever it was before, that they were on device hot words,
[01:10:05.160 --> 01:10:07.880]   that they were always listening just to whether or not they heard the hot word,
[01:10:07.880 --> 01:10:10.680]   and when they hear the hot word, that's when they would go to the cloud.
[01:10:10.680 --> 01:10:21.480]   All right. Yeah, but I mean, anonymizing that in the cloud, that's something that Apple can do,
[01:10:21.480 --> 01:10:26.280]   because their business is not built around this massive trove of information and data that's
[01:10:26.280 --> 01:10:33.560]   been built up around you that makes Siri more useful. So it'll be interesting to see how Apple
[01:10:33.560 --> 01:10:38.360]   does improve Siri to compete with some of these other Home assistants, because they just don't have
[01:10:38.360 --> 01:10:43.000]   as much information on you. And that's the kind of stuff that makes people go, "Oh, wow,
[01:10:43.000 --> 01:10:46.200]   it's really useful. It knows by calendar," or whatever in case maybe.
[01:10:46.200 --> 01:10:51.000]   Well, I think the other big change though, if I remember or understood it right from what
[01:10:51.000 --> 01:11:00.440]   they were saying with the WWDC was that Siri was now going to be smarter across your devices,
[01:11:00.440 --> 01:11:06.760]   because currently right now, Siri is device-specific, because Apple's real like, "We don't send any of
[01:11:06.760 --> 01:11:13.480]   that stuff out." So Siri learned something on one device, and then if you were to have an iPad,
[01:11:13.480 --> 01:11:17.800]   Siri on the iPad doesn't know the same things that Siri on the iPhone understand, just like
[01:11:17.800 --> 01:11:22.520]   if you had a home pod, Siri on the home pod wouldn't necessarily know that, except that they're
[01:11:22.520 --> 01:11:27.880]   making a change now where it's all encrypted and it's magical and it's privacy protected or whatever.
[01:11:27.880 --> 01:11:32.120]   But I think that they said that now Siri is actually going to be cross-device, and that's a
[01:11:32.120 --> 01:11:36.760]   significant change, because that allows them to then let Siri get smarter because you have a common
[01:11:36.760 --> 01:11:41.720]   Siri profile rather than a device-specific Siri profile. If I'm understanding all the things that
[01:11:41.720 --> 01:11:49.160]   it was saying that it does, but to go back to the other thing with Stacy saying on the Siri getting
[01:11:49.160 --> 01:11:53.880]   smarter, it's going to be really hard for them to make Siri smarter, because Siri doesn't get smarter
[01:11:53.880 --> 01:11:58.680]   unless you start letting Bing do stuff. Because right now, Siri's designed to just...
[01:11:58.680 --> 01:12:08.200]   This kind of only deals with a set number of very vetted databases that it can have. Like,
[01:12:08.200 --> 01:12:13.320]   what's the weather, set a timer, do these very specific things, and then if you want to go beyond
[01:12:13.320 --> 01:12:16.840]   that, of course, you have to have installed the apps or whatever they call them.
[01:12:18.600 --> 01:12:24.040]   Siri will answer a lot of common vetted stuff that they have as well, and then if you try to go beyond
[01:12:24.040 --> 01:12:27.880]   that, it's trying to hit various databases. Like, I'll go to Yelp or I'll go to this, and then
[01:12:27.880 --> 01:12:32.920]   ultimately it decides, well, I'll try to go to Bing. Usually, it's this half-parted effort that's
[01:12:32.920 --> 01:12:39.320]   not very useful. Google, when you ask it, goes to Google, there's none of this like, well, I'm
[01:12:39.320 --> 01:12:42.120]   going to try to figure out this, or maybe I can go over here, or maybe somebody else, there's like
[01:12:42.120 --> 01:12:48.520]   Google. I know everything. It doesn't know everything, but it knows a lot, and it's
[01:12:48.520 --> 01:12:53.480]   not got this delay of, well, I'm not certain about this. Let me ask someone else to see if
[01:12:53.480 --> 01:12:59.640]   someone else knows, and then I'll just throw it out there. So, they don't just get to make Siri
[01:12:59.640 --> 01:13:07.160]   Google Home smart by flipping a few switches. They don't have a search engine. They don't have
[01:13:07.160 --> 01:13:12.760]   a deep integration like Google has. And so, they might get it closer to doing what an Amazon Echo
[01:13:12.760 --> 01:13:18.280]   does, but I don't see them getting anywhere close to what a Google Home does unless they
[01:13:18.280 --> 01:13:22.760]   want to really let Microsoft into that device a lot more than they're doing now.
[01:13:22.760 --> 01:13:27.160]   And they've been years in not doing that. The other question that will come up is whether they're
[01:13:27.160 --> 01:13:37.560]   allowed to do that. So, they use Bing for Siri as the backup. They still use Google as the backup
[01:13:37.560 --> 01:13:42.680]   in Safari, and they don't know how their contract would work with, we're serving stuff out of Siri.
[01:13:42.680 --> 01:13:47.160]   Is that coming off of Google, or is that coming off of Bing, and who's it providing or whatever? So,
[01:13:47.960 --> 01:13:54.200]   I don't know. It's been the one area that Apple has not really done as much as people think with
[01:13:54.200 --> 01:14:00.360]   in terms of having and owning and operating their own search service. They don't have that.
[01:14:00.360 --> 01:14:06.200]   I think that's going to hinder them in terms of making that the home assistant that some people
[01:14:06.200 --> 01:14:15.240]   may want. From an IoT standpoint, Stacey, I mean, when it comes to controlling devices in your home,
[01:14:15.880 --> 01:14:21.880]   do you think that the fact that this has HomeKit, obviously, that's part of the strength from
[01:14:21.880 --> 01:14:27.240]   Apple's perspective. Is that a strong enough sell for IoT enthusiasts?
[01:14:27.240 --> 01:14:36.440]   I don't think a lot of people are really into HomeKit. There are people who are into HomeKit,
[01:14:36.440 --> 01:14:42.200]   but I don't think it's a mass market thing. What I think HomeKit may have going with this
[01:14:42.200 --> 01:14:46.440]   actually may galvanize people to actually know about HomeKit and buy HomeKit,
[01:14:46.440 --> 01:14:53.480]   because when you are an Apple user and you activate a device from your iPhone, like a new connected
[01:14:53.480 --> 01:14:58.600]   device, it'll see that your phone is on your network and this device is on your network and
[01:14:58.600 --> 01:15:02.120]   they'll be like, "Hey, do you want to use HomeKit?" And you're like, "Sure." And then you're like,
[01:15:02.120 --> 01:15:07.560]   "Ooh, magic." And then you have control of this device that you may not really care much about,
[01:15:08.120 --> 01:15:14.520]   but if you've done it enough times and you see and then you bring a HomePod into the home,
[01:15:14.520 --> 01:15:20.680]   that gets interesting because then you might be like, "Apple, I assume, tell you that, hey,
[01:15:20.680 --> 01:15:26.120]   by the way, you could talk to this and control your thermostat because it's a HomeKit thermostat."
[01:15:26.120 --> 01:15:31.320]   And then suddenly people might suddenly wake up to the fact that, "Holy cow, I have a connected
[01:15:31.320 --> 01:15:36.280]   house and I didn't even really realize it." I don't know if that's going to happen.
[01:15:36.280 --> 01:15:42.360]   I mean, I have a lot of things that are on HomeKit and they're going my lights now.
[01:15:42.360 --> 01:15:44.840]   I don't know if you can see them clicking on, I'm doing it off my phone.
[01:15:44.840 --> 01:15:54.760]   I mean, HomeKit when it works is really wonderful and sometimes it's easier to me than trying to set
[01:15:54.760 --> 01:16:00.840]   something up with the Alexa, but not everything works with HomeKit. So then you're like, "Oh,
[01:16:00.840 --> 01:16:07.960]   Tueras, one of the things that I think the Echo, more than any of them, seems to work with
[01:16:07.960 --> 01:16:13.880]   everything now. I struggle to find something that it can't handle that's a connected device in my
[01:16:13.880 --> 01:16:20.600]   house. Tueras, the Google still has some things it won't do and then HomeKit or Apple
[01:16:20.600 --> 01:16:23.160]   has a few things it won't talk to."
[01:16:23.160 --> 01:16:25.080]   Yeah, I mean, Google's getting there.
[01:16:25.080 --> 01:16:26.840]   HomeKit has a bunch of stuff it won't talk to.
[01:16:27.960 --> 01:16:33.960]   Well, I mean, HomeKit, I mean, I'm way less knowledgeable on the Apple ecosystem side of things,
[01:16:33.960 --> 01:16:40.360]   but HomeKit is pretty much specific to just iOS, right? Are there devices that are HomeKit and also
[01:16:40.360 --> 01:16:43.800]   support other platforms or if they're HomeKit, they are only HomeKit?
[01:16:43.800 --> 01:16:50.120]   No, there are bridge devices and devices that do both. The challenge with HomeKit is that you have
[01:16:50.120 --> 01:16:57.640]   to have a physical chip right now inside the device for it to be certified for a HomeKit device,
[01:16:57.640 --> 01:17:03.880]   which means that if you've got older devices that are, if you bought into a smart home, right,
[01:17:03.880 --> 01:17:08.600]   and maybe they weren't HomeKit certified, they're never going to become HomeKit certified.
[01:17:08.600 --> 01:17:17.240]   But, but here comes the but they did put this in W, they did talk about this at WWDC and
[01:17:17.240 --> 01:17:24.200]   Apple made a reference that I can't wait to find out more to making it easier to
[01:17:24.200 --> 01:17:31.080]   certify or to get HomeKit devices connected. So, but it was a very cryptic thing in a press release.
[01:17:31.080 --> 01:17:34.600]   So, I'm like, could it be software enabled? I don't know.
[01:17:34.600 --> 01:17:40.760]   And they might be able to like I had like the Hue lights, my original hub with Hue,
[01:17:40.760 --> 01:17:47.160]   I don't think was HomeKit. So, when I upgraded HomeKit, Hue, I think the second one was, I believe it was.
[01:17:48.840 --> 01:17:53.960]   Yeah, so when I got the new hub, then all my individual lights worked. And like I have
[01:17:53.960 --> 01:18:01.080]   the Lutron lights that are HomeKit, but then Amazon can work with them and Google can work with
[01:18:01.080 --> 01:18:06.040]   them. But my LIFX bulbs, they don't have a hub and they're each individual bulbs and I have like
[01:18:06.040 --> 01:18:12.600]   12 of them in various places. And they're not going to get HomeKit. You know, I'd have to throw
[01:18:12.600 --> 01:18:17.160]   those bulbs away. But maybe, this maybe goes to what you were saying, Stacy, is that
[01:18:17.960 --> 01:18:24.280]   they could talk to, they could talk to say my Apple TV or they could talk to a HomePod,
[01:18:24.280 --> 01:18:28.360]   if the HomePod wanted them to, right? It wouldn't be that hard to set them up.
[01:18:28.360 --> 01:18:36.520]   But if the HomePod itself acts as a bridge, that might be a way to basically HomeKit to
[01:18:36.520 --> 01:18:40.760]   buy a lot of devices that don't have the HomeKit chips built into them.
[01:18:40.760 --> 01:18:45.320]   Oh, that's an interesting thought. But that would kind of break Apple's security model.
[01:18:46.040 --> 01:18:50.360]   Because that's what HomeKit's all about. So that would be actually interesting.
[01:18:50.360 --> 01:18:54.360]   So the HomeKit certification is built into the HomePod as a bridge. I like it.
[01:18:54.360 --> 01:19:03.640]   Okay. And yes, today was HomeKit Day at WWI. You never say this, I've got too many Ws.
[01:19:03.640 --> 01:19:14.280]   WWW.WDC. So there's a lot more stuff that we have now. We'll see.
[01:19:14.280 --> 01:19:18.200]   Good. All right. So then you're going to have all three of these things in your house,
[01:19:18.200 --> 01:19:20.760]   and then the Harman Cardin one, and then the essential home.
[01:19:20.760 --> 01:19:29.320]   I already have a dozen hubs. So what's a few more?
[01:19:29.320 --> 01:19:34.200]   What's a few more? You're going to have a room that is nothing but home assistance,
[01:19:34.200 --> 01:19:38.760]   waiting for you to fire them off, waiting for you to put them on.
[01:19:38.760 --> 01:19:43.720]   Oh, that is true. I still have some rooms that we can, we can trick out with
[01:19:44.600 --> 01:19:51.960]   with gear. Oh boy. That's awesome. All right. So let's see here.
[01:19:51.960 --> 01:19:56.760]   Is there anything in here? I mean, there's a lot of stuff in here that we haven't talked about.
[01:19:56.760 --> 01:20:01.320]   Is there anything that like really catches your eyes before we get to kind of tips and
[01:20:01.320 --> 01:20:05.400]   and tricks and picks and all that kind of stuff? Anything that we you wanted to talk about that
[01:20:05.400 --> 01:20:11.160]   we that we haven't touched on before we move on? And it's okay if there's not.
[01:20:12.040 --> 01:20:16.760]   Because we've got other things too. Like there's the Amazon ice smartphones.
[01:20:16.760 --> 01:20:23.080]   I don't know. Oh, yes. Okay. Yeah. Absolutely. Actually, yeah, let's talk about that real quick.
[01:20:23.080 --> 01:20:29.800]   The Amazon Fire Phone, the opposite of fire is ice and apparently Amazon is working on
[01:20:29.800 --> 01:20:37.800]   a new take on their phones. The Fire Phone was, let's just say it was a little bit of a failure
[01:20:37.800 --> 01:20:43.800]   when it launched a few years ago. Did not last very long. It kind of had some gimmicky kind of
[01:20:43.800 --> 01:20:49.960]   aspects to it and it just it was a failure. It didn't work. But apparently Amazon's back to the
[01:20:49.960 --> 01:20:55.640]   drawing board. The internal name for it supposedly is ice. Don't know if that's branding that's going
[01:20:55.640 --> 01:21:01.160]   to make it to release if and when it ever comes out. But it seems like it's more targeting the device
[01:21:01.160 --> 01:21:06.440]   that's in the works is targeting kind of the lower end market in emerging markets,
[01:21:06.440 --> 01:21:12.360]   specifically like like India somewhere around $93 when you convert it from Indian rupees.
[01:21:12.360 --> 01:21:18.120]   But big difference this time is that and Android or Amazon doesn't do this with their
[01:21:18.120 --> 01:21:23.160]   Android devices. So this would be a first is that it would have Google mobile services on it. So it
[01:21:23.160 --> 01:21:28.840]   had the Google Play Store. It would have Google apps like navigation or maps, you know, Android
[01:21:28.840 --> 01:21:33.720]   messages, whatever it has that whole package that you get on a phone that's officially GMS
[01:21:33.720 --> 01:21:39.080]   certified. And that would be a big shift in Amazon's kind of mobile strategy.
[01:21:39.080 --> 01:21:45.800]   In the past, they've been very, very much about their app store keeping you in on all things
[01:21:45.800 --> 01:21:52.440]   Amazon on that device. This would be a change as transition away from that. What do you guys think?
[01:21:52.440 --> 01:21:58.840]   Are you getting one? No, no, I guess.
[01:21:59.640 --> 01:22:03.640]   A low-end Android phone? No, I think it's I mean, I think it'll be.
[01:22:03.640 --> 01:22:10.520]   I can't tell if it's Amazon trying to make up for some data it doesn't own because it's not on
[01:22:10.520 --> 01:22:15.000]   the phone and it recognizes that's important. Or if it's recognizing that's important, it's like,
[01:22:15.000 --> 01:22:19.480]   well, we'll get in on the ground floor here in India with these low-cost phones and see if that
[01:22:19.480 --> 01:22:26.040]   works. Yeah. So I mean, either way, I think Amazon's signaling here that having a mobile
[01:22:26.040 --> 01:22:31.320]   component is important and it recognizes to get there it's going to have to take a slightly
[01:22:31.320 --> 01:22:37.800]   different strategy because the US market is pretty saturated and I guess if I were Andy Rubin,
[01:22:37.800 --> 01:22:39.960]   I'd be like, oh crap, what's going to happen with the essential phone?
[01:22:39.960 --> 01:22:47.400]   Well, I mean, yeah, when you really look at how successful Amazon has been and all of the other
[01:22:47.400 --> 01:22:53.880]   places that they are present, like when you when you look at it, not and let's say you didn't know
[01:22:53.880 --> 01:22:58.360]   that the fire phone ever existed, you'd be scratching your head wondering why Amazon wasn't doing a
[01:22:58.360 --> 01:23:03.000]   phone because that makes perfect sense for everything that they're trying to achieve. Yet they did
[01:23:03.000 --> 01:23:07.320]   try it and they failed pretty miserably at it. So it makes sense that they'd go back again.
[01:23:07.320 --> 01:23:12.920]   Maybe hitting a deal with Google obviously Google wants as many partners and would love to have
[01:23:12.920 --> 01:23:17.960]   Amazon on its side of the ship. I don't know. I don't know what that means for Amazon devices
[01:23:17.960 --> 01:23:25.880]   going forward because my understanding of GMS of that agreement is that once you go in on it,
[01:23:25.880 --> 01:23:32.120]   your future phones have to all also carry that otherwise you're breaking the terms of the deal.
[01:23:32.120 --> 01:23:37.320]   So I don't know if that means that, okay, well, I guess now, our phones, I don't know if that
[01:23:37.320 --> 01:23:41.880]   also extends to tablets, my understanding is that it does, but maybe it doesn't, but that their future
[01:23:41.880 --> 01:23:50.520]   devices would have to have some sort of Google Play Store and all their services on it. That would
[01:23:50.520 --> 01:23:57.160]   be a big shift. But I do. But probably a smart one because I don't think they're getting there in
[01:23:57.160 --> 01:24:02.920]   terms of, we're going to have our own app store and we're going to have our own thing. It's like,
[01:24:02.920 --> 01:24:07.000]   you know, you're probably just going to do better if you dive into the Android space.
[01:24:07.000 --> 01:24:12.360]   Yeah, yeah, that's true. That's true. Let's see here. You mentioned essential.
[01:24:12.360 --> 01:24:20.520]   Apparently there is a trademark dispute, not even a week later after Andy Rubin showed off his
[01:24:20.520 --> 01:24:29.800]   essential phone and essential home. Spigen, which is the cell phone case slash, you know, battery pack
[01:24:30.440 --> 01:24:37.240]   maker has a trademark for essential since August of 2016, a range of battery packs and chargers
[01:24:37.240 --> 01:24:42.600]   that they sell with the essential brand on it, also some Bluetooth headphones. So they charged
[01:24:42.600 --> 01:24:49.800]   that essential the phone being a phone company is too similar, too confusing. And apparently,
[01:24:49.800 --> 01:24:56.760]   essential, the phone company applied for this registration and it was refused. One could assume
[01:24:57.480 --> 01:25:03.160]   why because it existed already. But they went ahead with it anyways, Andy was like,
[01:25:03.160 --> 01:25:10.440]   whatever, we got this. So now it's a case. Well, that's crazy that they were denied their trademark
[01:25:10.440 --> 01:25:16.360]   and then they still went and did it. That feels, I don't know, of course, remember Apple took on
[01:25:16.360 --> 01:25:23.320]   Cisco with iOS as a networking nerd. I knew what iOS was before, before the rest of you, when it
[01:25:23.320 --> 01:25:29.080]   was something completely different. And you know, Cisco finally caved. So maybe that's the playbook
[01:25:29.080 --> 01:25:34.600]   that you know, Andy Rubin's running from, I don't know. I don't know. Yeah, who knows on that one,
[01:25:34.600 --> 01:25:41.080]   but it was still kind of, it was a bold move on Andy's company's part. Essential says, threat
[01:25:41.080 --> 01:25:46.040]   letters are commonplace in our sector. While it's Spigen's prerogative to make assertions,
[01:25:46.040 --> 01:25:50.200]   essential believes they are without merit and will respond appropriately.
[01:25:51.800 --> 01:25:53.720]   So we'll see what that appropriate response is.
[01:25:53.720 --> 01:26:03.880]   I think, I think, I think we did all right. I think we can talk about Amazon doing prime for
[01:26:03.880 --> 01:26:11.560]   what used to be food stamps, which I thought was. So let me, let me pull up the story,
[01:26:11.560 --> 01:26:19.640]   which is open in one of these tabs. I'm like, Oh, dear God, hold on. Is it EBT? I think is what
[01:26:19.640 --> 01:26:23.800]   it's called now? Yeah, EBT. Yes, electronic benefits transfer card.
[01:26:23.800 --> 01:26:30.360]   Thank you. Basically, they reduced the prime, the $99 you pay for prime to,
[01:26:30.360 --> 01:26:40.440]   was it 45? Here it is. I found it. It's $699 or sorry, $599. So $6 from basically $11 a month.
[01:26:40.440 --> 01:26:47.800]   You have to have your electronics benefits transfer card. And it, you don't have to use that card for
[01:26:47.800 --> 01:26:54.520]   Amazon purchases only for verification. So this is really interesting because it makes me wonder,
[01:26:54.520 --> 01:27:00.840]   one is Amazon just trying to increase prime, but two, you know, remember that kerfuffle about
[01:27:00.840 --> 01:27:05.400]   Amazon not being in low income neighborhoods? I think it was Bloomberg did a really good data
[01:27:05.400 --> 01:27:12.120]   analysis. And one of the reasons Amazon said it wasn't was because it didn't have one of the
[01:27:12.120 --> 01:27:18.680]   metrics that looks at when trying to decide where to do like now delivery zones was
[01:27:18.680 --> 01:27:27.080]   or Amazon now. So the really rapid delivery was the number of prime subscriptions in that area.
[01:27:27.080 --> 01:27:33.160]   So I thought this might be kind of both just a nice way, like a way to get people who
[01:27:33.160 --> 01:27:41.320]   aren't as wealthy on the program, but also might give Amazon, you know, a better insight and
[01:27:41.320 --> 01:27:45.880]   better ways to like serve communities that mean some of these places don't have access to,
[01:27:45.880 --> 01:27:50.520]   you know, cheaper groceries and things like toilet paper.
[01:27:50.520 --> 01:27:56.040]   Yeah. And I mean, you know, potentially people could actually save a lot of money if they could
[01:27:56.040 --> 01:28:02.600]   shop on Amazon with some of the sale prices that happen and get that shipping kind of included in
[01:28:02.600 --> 01:28:10.840]   there. One of the statistics I found is that historically prime subscribers have made
[01:28:10.840 --> 01:28:19.560]   $112,000 in the household per year or more 70% of all prime subscribers fall into that category.
[01:28:19.560 --> 01:28:25.800]   And so Amazon's missing out on, you know, a whole other tier of potential customers that
[01:28:25.800 --> 01:28:32.440]   could totally, you know, buy some of the products that maybe go on sale or whatever, you know,
[01:28:32.440 --> 01:28:37.320]   buy the lower cost items as they happen. It just opens them up to a new market.
[01:28:37.320 --> 01:28:39.320]   Yeah, the Walmart market.
[01:28:39.320 --> 01:28:48.920]   Yeah. And actually 30 day free trial on that. So people who are interested in order to validate,
[01:28:48.920 --> 01:28:52.680]   like you said, you need the EBT card, the electronic benefits transfer card.
[01:28:52.680 --> 01:28:58.680]   And then once you validate, you can get a 30 day free trial, Amazon.com/qualify.
[01:28:58.680 --> 01:29:04.920]   Amazon apparently is also working with a government on ways to allow food stamps to be used for some
[01:29:04.920 --> 01:29:09.560]   purchases as well. So they're really opening up trying to anyways.
[01:29:09.560 --> 01:29:19.080]   I think I mean, I there are more people than ever on food stamps. You know, you look at people who
[01:29:19.080 --> 01:29:23.640]   are like adjunct professors on food stamps, they're teachers on food stamps. This is not
[01:29:27.080 --> 01:29:31.560]   there's a lot of people that, you know, could really take a benefit of this. So
[01:29:31.560 --> 01:29:37.800]   Absolutely. Okay. All right. All right. Anything else?
[01:29:37.800 --> 01:29:47.560]   Anything going once? I'm good. Going twice. Sold to tips, tricks and numbers or whatever we have.
[01:29:47.560 --> 01:29:51.560]   Hopefully, maybe maybe you do. Maybe you don't.
[01:29:51.560 --> 01:29:55.160]   Stacey, what do you, what do you have? What's your thing today?
[01:29:56.280 --> 01:30:01.320]   Okay, I just got this. So this is a friend of mine recommended this to me because I've been
[01:30:01.320 --> 01:30:11.160]   having trouble sleeping. It is news. Let me guess. It's spelled S and U Z. S in O O Z.
[01:30:11.160 --> 01:30:16.760]   Okay. All right. All right. Clothes. So it is a basically it's a white noise machine. It has an
[01:30:16.760 --> 01:30:23.080]   actual fan inside. Does that matter? I don't know. It does. I actually would agree. I love
[01:30:23.080 --> 01:30:27.560]   sleeping to white noise and we have an electronic one that just plays a sound and then we have one
[01:30:27.560 --> 01:30:33.560]   that's an actual mechanical motion white noise and I totally know the difference. Like I appreciate
[01:30:33.560 --> 01:30:42.920]   the difference. So I'm right there. All right. So this is this is the sound of the snooze.
[01:30:45.640 --> 01:30:55.800]   There is soft. Okay. Stop snoozing. And so the reason I bought it is because it's connected
[01:30:55.800 --> 01:30:59.800]   to the internet. Of course. I know. Was I talking earlier about how all of our
[01:30:59.800 --> 01:31:04.040]   devices are going to be connected and how we shouldn't have all this stuff? Yes.
[01:31:04.040 --> 01:31:09.080]   Slap my hand. I am kind of. It's kind of a problem. I understand. It is. But so,
[01:31:10.440 --> 01:31:16.760]   yes. And all of the app, all the app does and you just let you control it from your bed. If it's
[01:31:16.760 --> 01:31:22.680]   not like right next to you on the table, you can just open your app and be like and dial it down
[01:31:22.680 --> 01:31:27.400]   your your watchman shaker. I think you can also schedule stuff. Again, I literally just
[01:31:27.400 --> 01:31:31.880]   opened this before the show. So I haven't played with it yet. But I'm very excited about it because
[01:31:31.880 --> 01:31:36.600]   so many people have told me so many good things. It's kind of pricey. I'm going to let you know.
[01:31:37.480 --> 01:31:43.560]   So what how much is it? Oh, what you want actual prices? What? That's crazy.
[01:31:43.560 --> 01:31:49.240]   As it knows. How much did I pay for this? I think I got mine on sale. But I think it's
[01:31:49.240 --> 01:31:55.800]   $79. I was like, I think it's $80. So it's not cheap. It's not too bad.
[01:31:55.800 --> 01:32:00.680]   Actually, I don't know. I don't know what what white noise machines cost these days.
[01:32:00.680 --> 01:32:06.760]   But it's it's compact. I mean, this is like something if you like I could bring this to my
[01:32:06.760 --> 01:32:12.680]   hotel and I feel like it would be like here's here's my phone in case my head is not a good indicator.
[01:32:12.680 --> 01:32:17.000]   Phone and head for scale.
[01:32:17.000 --> 01:32:24.040]   Before I was struggling, I only had your hand to and you know,
[01:32:24.040 --> 01:32:32.520]   Zach, everything's going in this like mesh cylindrical design direction.
[01:32:32.520 --> 01:32:34.440]   Oh, yes. It was trendy right now. Good Lord.
[01:32:34.440 --> 01:32:38.520]   My goodness. They all looked they all look meshy and round or something.
[01:32:38.520 --> 01:32:44.760]   I feel like what two years ago, everything was square with rounded edges and like LEDs.
[01:32:44.760 --> 01:32:51.800]   Here's a good example of, you know, design from, you know, last year. Oh, yeah. Last year or two
[01:32:51.800 --> 01:32:58.440]   years ago this year. Uh huh. Yep. My how far things have come from a squirtle to a circle.
[01:33:00.280 --> 01:33:04.680]   All right. So Danny, what you got it says in here, Danny stuff. So what's your stuff?
[01:33:04.680 --> 01:33:10.440]   I'll just give you a quick thing. You can maybe find it real quick if you Google or Google ads
[01:33:10.440 --> 01:33:16.440]   personal search filter. They experimented with it about last week. And the main thing was you
[01:33:16.440 --> 01:33:20.600]   could do a search and you got this little tab that came up that said personal. And if you clicked on
[01:33:20.600 --> 01:33:25.000]   it and you had everything all linked together, it just let you search through things like your
[01:33:25.000 --> 01:33:31.080]   Gmail and stuff from regular Google. We've had this before with Google search plus my world,
[01:33:31.080 --> 01:33:35.400]   but it looks like they're experimenting with making it possible for you to deliberately just
[01:33:35.400 --> 01:33:40.520]   look through the stuff you stored on Google itself. Now it's gone, but maybe it'll come back because
[01:33:40.520 --> 01:33:45.720]   it looks like they're experimenting. So not really a tip you can do right now, but keep an eye out on
[01:33:45.720 --> 01:33:52.280]   it. Maybe it'll come back. Oh, find personal in the more drop down. Yeah, to jump into that. Nice.
[01:33:53.640 --> 01:33:59.000]   That's it. All right. Excellent. That works for me. Mine's a quick one. I didn't even know this
[01:33:59.000 --> 01:34:03.800]   exists until last night. We were doing an email on all about Android and there was a
[01:34:03.800 --> 01:34:09.480]   young budding programmer who wanted to know how to learn Kotlin because if you remember Google I/O,
[01:34:09.480 --> 01:34:14.840]   Kotlin was a big deal to developers that were there. And so if you're starting to want to get
[01:34:14.840 --> 01:34:19.480]   into developing for Android, the question is, do you learn? Do you go the traditional route?
[01:34:19.480 --> 01:34:23.800]   Or is it better to start off with Kotlin? Because everyone seems to really think that Kotlin is a
[01:34:23.800 --> 01:34:30.520]   much easier direction to go for programming for Android, for coding for Android. So why not start
[01:34:30.520 --> 01:34:38.360]   there? And right now Udemy is having a, has cut the price of their Kotlin for beginners course.
[01:34:38.360 --> 01:34:43.080]   It's learn programming with Kotlin. It's a six and a half hours of on-demand video,
[01:34:43.080 --> 01:34:49.080]   bunch of supplemental resources, you know, certificate of completion. If you need that
[01:34:49.080 --> 01:34:53.400]   certificate, but it basically kind of walks you through the beginning process of Kotlin
[01:34:53.400 --> 01:35:00.040]   for beginners. And normally it's 95 bucks, but apparently it's on sale right now for like 10 bucks.
[01:35:00.040 --> 01:35:06.200]   And that's only going to be at that cost for the next two days. So I'll just think in, if you
[01:35:06.200 --> 01:35:10.360]   are thinking about getting into coding, you want to learn a little bit more about Kotlin,
[01:35:10.360 --> 01:35:15.480]   here's a great way to do it for next to nothing. And it might be a good kind of beginner's course,
[01:35:15.480 --> 01:35:21.000]   good way to kind of kickstart your education around what Kotlin means. And then the next time
[01:35:21.000 --> 01:35:26.040]   at Google I/O they make some big announcement with, about Kotlin, you can go crazy like all
[01:35:26.040 --> 01:35:32.280]   the developers did this last year. You can be part of the crowd. So there you go. Learn Kotlin
[01:35:32.280 --> 01:35:37.880]   Kotlin for beginners at Udemy, U-D-E-M-Y, and you can do a search for the course there.
[01:35:39.000 --> 01:35:43.320]   But I think we've hit the end of this week in Google. That was a lot of fun. I love the
[01:35:43.320 --> 01:35:46.600]   rabbit hole that we got there in the middle. We were on a roll.
[01:35:46.600 --> 01:35:50.600]   Did we ever talk about what Danny got so upset about in the beginning?
[01:35:50.600 --> 01:35:55.640]   Yeah, we did. Okay, good. That was all the HomePod stuff. So we definitely got to it.
[01:35:55.640 --> 01:35:59.720]   Okay. I was like, I could remember. I was like, Danny got really excited about something. What was it?
[01:35:59.720 --> 01:36:06.360]   We covered all the bases. I think we did a great job.
[01:36:06.360 --> 01:36:11.480]   Danny Sullivan, it's always a pleasure getting the chance to do a show with you and to see you again.
[01:36:11.480 --> 01:36:16.440]   Thank you so much for coming on today. Tell people a little bit about what you're working on,
[01:36:16.440 --> 01:36:20.680]   where they can find all your work online. We have three different sites, Search and
[01:36:20.680 --> 01:36:25.160]   Jelan for all your search marketing and search news, marketing land for digital marketing information.
[01:36:25.160 --> 01:36:28.600]   And then we have our Mark Tech site for people who are up on marketing technology stuff.
[01:36:28.600 --> 01:36:33.400]   So that's Mark Tech today. Excellent. And you're @danysoleven on Twitter.
[01:36:33.400 --> 01:36:36.280]   On Twitter. Yeah. Always fun. Thank you, Danny. Appreciate it.
[01:36:36.280 --> 01:36:41.480]   Thank you. And Stacy, again, always a pleasure hopping on with you. You're not going to be on
[01:36:41.480 --> 01:36:46.520]   next week, though, right? You're out. I'm not. Dang it. That's okay. I know.
[01:36:46.520 --> 01:36:51.320]   I got to podcast with you today and Jeff missed out. Stacy Higginbotham, of course,
[01:36:51.320 --> 01:36:55.000]   iotpodcast.com. What do you want to point people to?
[01:36:55.000 --> 01:37:00.600]   You could go to iotpodcast.com, which is only my podcast, or you can go to Stacy on IOT.
[01:37:01.240 --> 01:37:05.880]   And then you get podcasts plus stories. Whoa.
[01:37:05.880 --> 01:37:12.200]   Like the story is an @gigastacy on Twitter. Thanks again, Stacy. Have a good week off
[01:37:12.200 --> 01:37:16.680]   of this show. I don't know about anything else in your life. This show, at least.
[01:37:16.680 --> 01:37:22.840]   You can find me all over the Twit Network. I'm doing shows like all about Android,
[01:37:22.840 --> 01:37:27.880]   where we talk about Android. I'm doing shows about tech news. That show is called Tech News
[01:37:27.880 --> 01:37:31.640]   Today. So we make the naming really kind of make sense to what you actually get.
[01:37:31.640 --> 01:37:38.200]   Having fun all across the map. So find me all over those places or find me on Twitter @JasonHowl.
[01:37:38.200 --> 01:37:44.600]   This week in Google, of course, we record every Wednesday, 1.30 pm Pacific, 4.30 Eastern,
[01:37:44.600 --> 01:37:52.600]   20.30 UTC. If you want to watch live, you can. You just go to twit.tv/live and you can catch it
[01:37:52.600 --> 01:37:57.320]   in real time as it happens. I know it's a crazy thing. But if you can't do that, that's okay. We
[01:37:57.320 --> 01:38:05.640]   post all the shows online at the site. Just go to twit.tv/twig, T-W-I-G for this week in Google.
[01:38:05.640 --> 01:38:11.240]   And you can find all of our episodes. Everything that you need about this show is there. And oh,
[01:38:11.240 --> 01:38:17.400]   by the way, I think if you go to twit.tv/story, you can get a shirt with twig on it as well.
[01:38:17.400 --> 01:38:22.840]   Just a little bonus there. If you don't have your twig shirt yet, go get it. And you can profess
[01:38:22.840 --> 01:38:30.360]   your true fandom of this show. I will be happy to be returned next week. I'll be here on set. So
[01:38:30.360 --> 01:38:35.400]   I'll be looking forward to that show. But as for now, that's all we have for this week in Google
[01:38:35.400 --> 01:38:42.040]   this week. We'll see you next week. Take care, you guys.
[01:38:42.040 --> 01:38:47.800]   [Music]

