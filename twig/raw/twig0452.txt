;FFMETADATA1
title=The Mormon Bartender Problem
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=452
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.640]   It's time for Twig this week in Google. Stacey Higginbotham is here. Kevin Marks is here from the UK.
[00:00:06.640 --> 00:00:12.320]   Mike Elgin's sitting in. We're going to talk of course about Zuckerberg's Facebook testimony
[00:00:12.320 --> 00:00:16.800]   and what should happen at Facebook and whether anything ever will happen at Facebook. But there's
[00:00:16.800 --> 00:00:22.480]   lots of other things including, can you spot the G? It's all coming up next on Twig.
[00:00:26.240 --> 00:00:29.440]   NetCasts you love. From people you trust.
[00:00:29.440 --> 00:00:42.880]   This is Twig. bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:42.880 --> 00:00:54.800]   This is Twig. This week in Google, episode 452 recorded Wednesday April 11th, 2018.
[00:00:54.800 --> 00:01:00.880]   The Mormon bartender problem. This week in Google is brought to you by Rocket Mortgage.
[00:01:00.880 --> 00:01:05.040]   From Quick and Loans, Home plays a big role in your life. That's why Quick and Loans
[00:01:05.040 --> 00:01:10.320]   created Rocket Mortgage. It lets you apply simply and understand the entire mortgage process fully
[00:01:10.320 --> 00:01:16.480]   so you can be confident you're getting the right mortgage for you. Get started at rocketmortgage.com/twig.
[00:01:16.480 --> 00:01:22.640]   It's time for Twig this week in Google. Show where we talk very little about Google sometimes.
[00:01:23.680 --> 00:01:28.560]   Sometimes a lot. It really depends. Welcome back. Kevin Marks is here. Great to have you from the
[00:01:28.560 --> 00:01:34.960]   UK. Kevin has worked at Google and Apple and British Telecom and the Beeb and everybody else
[00:01:34.960 --> 00:01:43.200]   in between. He's a grand partisan of the independent worldwide web the way it was in the good old days.
[00:01:43.200 --> 00:01:51.360]   Hi, Kevin. The way it still is. It hasn't got away. No, it's not. In fact, I think all of this
[00:01:51.360 --> 00:01:57.040]   Facebook stuff really has been good for the web in a way. Reminding people.
[00:01:57.040 --> 00:02:03.360]   It's making people stop thinking about it. It's like a pendulum. You said, "Linging back and forth."
[00:02:03.360 --> 00:02:11.040]   Stacey Higginbotham back. We missed you so much. Welcome back. From beautiful Austin, Texas,
[00:02:11.040 --> 00:02:15.600]   she is an IoT expert. Stacey on IoT is her newsletter. You, of course, remember her from
[00:02:15.600 --> 00:02:21.600]   Giga Ohm and her Twitter handle still reflects that @GigaStacey. She's a regular on the show.
[00:02:21.600 --> 00:02:26.240]   Jeff has the week off, but Mike Elgin is sitting next to me in his stead. Playing the role of Jeff.
[00:02:26.240 --> 00:02:30.880]   So you're going to be there grumpy Kermajin who says Facebook's a good thing? Yes.
[00:02:30.880 --> 00:02:36.960]   I feel panic, etc. I wish Jeff were here because last week we had the massive battle
[00:02:36.960 --> 00:02:45.200]   over Facebook because I've become a fed up with it. Well, I felt like Zuck had the
[00:02:45.200 --> 00:02:50.560]   opportunity to cast many of these fears in the realm of techno panic. Yesterday,
[00:02:50.560 --> 00:02:54.320]   when he was asked by one of the senators, "What's your hotel? What hotel did you stay at last night?"
[00:02:54.320 --> 00:02:57.040]   I would have just said Hilton. I would have told him.
[00:02:57.040 --> 00:02:58.960]   Just said. Holiday Inn Express.
[00:02:58.960 --> 00:03:03.920]   Exactly. Maybe it was one of them go-to-effort things where he didn't want to admit it.
[00:03:03.920 --> 00:03:10.240]   Yeah, that was a good question. And Zuck said, "No, I don't want to tell you." And the senator
[00:03:10.240 --> 00:03:17.920]   said, "Mike, drop." I don't know, Kevin, have you been following Zuck's testimony at all?
[00:03:17.920 --> 00:03:25.520]   I haven't been watching it because that's a bit deadly, but I have been following it on Twitter
[00:03:25.520 --> 00:03:30.400]   and as people report on it. So I haven't watched the whole six hours or whatever it is, but I've
[00:03:30.400 --> 00:03:32.560]   seen the gist of it and seen the reaction. That's a good word for it.
[00:03:32.560 --> 00:03:39.840]   I don't think it's going to change anything. You get the usual mix of grandstanding
[00:03:40.320 --> 00:03:45.680]   members of Congress. Well, the thing that it's already changed things because coming up into the
[00:03:45.680 --> 00:03:50.720]   thing they scrambled together a bunch of little fixes for Facebook, the changed things they
[00:03:50.720 --> 00:03:55.040]   had a bounty for people reporting others who were doing bad things with apps and so on.
[00:03:55.040 --> 00:04:01.680]   And that was a subtle improvement that's just so he could face the senators and be able to answer
[00:04:01.680 --> 00:04:03.520]   questions. I'm working on it. Yeah. Exactly.
[00:04:05.680 --> 00:04:10.480]   Some of this is to get ready for GDPR. And some of this, yes, might be tiny little fixes.
[00:04:10.480 --> 00:04:16.640]   Honestly, so I try really hard. I watch a little bit of the testimony, but for the most part,
[00:04:16.640 --> 00:04:24.720]   I'm like, I get so aggravated by testimony that I don't like anybody's testimony. I've had to
[00:04:24.720 --> 00:04:30.400]   watch it so long. But the whole time, I'm like, yes, let's grow Mark Zuckerberg, but good Lord.
[00:04:31.600 --> 00:04:36.320]   Let's talk about Experian and all of the other things. Let's pull this out from the Facebook
[00:04:36.320 --> 00:04:42.080]   microcosm and actually talk about privacy and data because it's everywhere. And yes, Facebook is
[00:04:42.080 --> 00:04:47.920]   it is appalling. It is, you know, there's lots of legitimate questions here, but I'm like, guys,
[00:04:47.920 --> 00:04:52.800]   none of them asked, by the way, a few of them have been asked. I mean, but that's that's true.
[00:04:52.800 --> 00:04:59.040]   That is the way testimony goes. Right. That is always, I mean, like, I've watched so much,
[00:04:59.040 --> 00:05:04.560]   so much testimony. And the real stuff happens behind the scenes with the AIDS as they're trying to
[00:05:04.560 --> 00:05:09.520]   cobble together rules and laws. And that's that's politics. Yeah.
[00:05:09.520 --> 00:05:15.200]   I think that's a really good point, Stacey, is and Facebook's the poster child, which makes you
[00:05:15.200 --> 00:05:21.120]   think that companies like Google and Amazon are going, but at the same time, I feel like this is
[00:05:21.120 --> 00:05:28.480]   just the beginning of a sea change. Yeah. Isn't it? It is. So first of all, we've been talking about
[00:05:28.480 --> 00:05:35.360]   privacy matters for as long as this network has existed. Yeah. It's a big issue for everybody who
[00:05:35.360 --> 00:05:40.880]   was. Yes. And Washington has not been paying attention, obviously, for the most part. And
[00:05:40.880 --> 00:05:45.440]   and the public has kind of responded with a collective shrug. All the other privacy
[00:05:45.440 --> 00:05:51.440]   experts and people concerned about privacy have been talking, talking, talking, and the general
[00:05:51.440 --> 00:05:55.040]   public's like, yeah, so what I upload pictures. I share my location. What's the
[00:05:55.040 --> 00:05:59.360]   most matter? No, no, I think it's finally getting through to the public. It is now. Yes.
[00:05:59.360 --> 00:06:06.560]   I don't think the public has has been as cavalier as you say. I think they are making a logical
[00:06:06.560 --> 00:06:13.520]   tradeoff between their data and a service that they really want. The problem is because there's no
[00:06:13.520 --> 00:06:19.200]   regulation, there's no reason for someone to offer a more private service. There's no reason for us
[00:06:19.200 --> 00:06:26.640]   to even pay for it. So it's kind of like before there were rules about the FDA existed, you could
[00:06:26.640 --> 00:06:31.440]   sell people crappy meat. And some people would have to buy the crappy meat because they wanted
[00:06:31.440 --> 00:06:36.160]   meat, even if it might kill them. This is a really terrible analogy. No, it's because Sinclair Lewis
[00:06:36.160 --> 00:06:43.200]   and the jungle raised people's awareness. Things, the government took sat up, took notice, made some
[00:06:43.200 --> 00:06:47.440]   laws and the world is much improved for it. And we got sterile hot dogs and things like that.
[00:06:47.440 --> 00:06:52.640]   That was a wonderful moment. But one of the things that I think is as just an exercise in
[00:06:52.640 --> 00:06:58.000]   consciousness raising, for example, now all of a sudden in the public sphere in the major news
[00:06:58.000 --> 00:07:02.880]   outlets, we're talking about shadow profiles, for example. This is something about shadow profiles.
[00:07:02.880 --> 00:07:06.240]   He hadn't heard the term or he claimed, I don't believe that, which is a little disingenuous,
[00:07:06.240 --> 00:07:11.440]   that he'd never heard what do you mean, shadow profiles center? And then the congressman explained
[00:07:11.440 --> 00:07:20.800]   it to him. And he said, Oh, yeah, we don't do that. Basically. Well, he said that we do gather.
[00:07:20.800 --> 00:07:25.520]   So first of all, shadow profiles are two things. So one of it is they have
[00:07:25.520 --> 00:07:29.920]   profile for security. But he said, but that was one of the only one of the things. So
[00:07:29.920 --> 00:07:35.120]   one type of shadow profile is somebody who is not even on Facebook, they have a profile,
[00:07:35.120 --> 00:07:40.480]   they have data on that person, which he claims is for security. I would suggest that it's for
[00:07:40.480 --> 00:07:44.400]   other reasons as well. Immediately pointed, pointed it said, Oh, yeah, we do that for
[00:07:44.400 --> 00:07:48.480]   to keep track of terrorism. Exactly. But for the majority that is on Facebook,
[00:07:48.480 --> 00:07:53.280]   they also have shadow profile data points. So for example, if you shared with Facebook,
[00:07:53.280 --> 00:07:57.520]   your home phone number, but not your mobile number, a few of your contacts, but not all of your
[00:07:57.520 --> 00:08:02.800]   contacts, they have invisibly behind the scenes in their databases. You're all your phone numbers,
[00:08:02.800 --> 00:08:07.280]   all your email addresses, all of your contacts, all of it, because your friends and family and so
[00:08:07.280 --> 00:08:11.600]   on have granted Facebook permission to upload their contact databases. And you're in there.
[00:08:11.600 --> 00:08:18.160]   And Senator Ben Lugin, from New Mexico, Facebook has detailed profiles on people who have never
[00:08:18.160 --> 00:08:22.480]   signed up for Facebook. Yes or no? By the way, one thing that makes this kind of a sham is that
[00:08:22.480 --> 00:08:28.720]   the senators only have five minutes, the house represents, I have four minutes to talk to these,
[00:08:28.720 --> 00:08:32.560]   talk to them. You don't want to give them any more. Good God, no. Well, they would grandstand,
[00:08:32.560 --> 00:08:36.720]   but at the same time, there's no follow up. And you kind of, you have to say things like,
[00:08:36.720 --> 00:08:42.080]   yes or no, because Mark has been well coached and is attempting to use up the time as well.
[00:08:42.080 --> 00:08:46.400]   "Congressman, I'm not familiar with that," says Zuckerberg. Actually,
[00:08:46.400 --> 00:08:50.640]   Congressman in general, we collect data on people who have not signed up for Facebook for security
[00:08:50.640 --> 00:08:55.520]   purposes to prevent the kinds of scraping you are referring to, which is reverse searches
[00:08:55.520 --> 00:08:59.040]   based on public info like phone numbers. So these are called shadow profiles,
[00:08:59.040 --> 00:09:03.280]   Lugin says, is that what they've been referred to by some? "Congressman, I'm not familiar with that."
[00:09:04.160 --> 00:09:09.040]   Well, okay. I'll refer to them as shadow profiles for today's hearing.
[00:09:09.040 --> 00:09:12.800]   "On average, how many data points does Facebook have on each Facebook user?"
[00:09:12.800 --> 00:09:18.480]   Now, this was the response. The smart response that Mark gave almost the entire time, which is,
[00:09:18.480 --> 00:09:21.360]   "I'll have my team get back to you. I don't know out off the top of my head."
[00:09:21.360 --> 00:09:25.200]   Do you know how many points of data Facebook has on the average non-Facebook user?
[00:09:25.200 --> 00:09:29.760]   "Congressman, I do not know off the top of my head, but I can have our team get it back to you afterward."
[00:09:29.760 --> 00:09:31.760]   Was it Nixon that did? I can't recall?
[00:09:31.760 --> 00:09:34.400]   I can't recall. It was Nixon.
[00:09:34.400 --> 00:09:38.000]   Okay. It's been admitted by Facebook that you collect data points on non-users.
[00:09:38.000 --> 00:09:41.680]   My question is, "Can someone who does not have a Facebook account opt out of Facebook's
[00:09:41.680 --> 00:09:48.560]   involuntary data collection?" Zuckerberg. "Anyone could turn off and opt out of any data collection
[00:09:48.560 --> 00:09:52.800]   for ads whether they use our services or not." I don't know how they would do that, by the way.
[00:09:52.800 --> 00:09:56.560]   "In order to prevent people from scraping public information, we need to know when someone is
[00:09:56.560 --> 00:10:01.680]   repeatedly trying to access their services." You said that everyone controls their data, Mark,
[00:10:01.680 --> 00:10:04.880]   but you're collecting data on people who are not even on Facebook users who have never signed
[00:10:04.880 --> 00:10:11.920]   a consent or privacy agreement. It says, "LuJan, it may surprise you that when you go to Facebook's
[00:10:11.920 --> 00:10:16.560]   page and you say, "I don't have a Facebook account and would like to access all my personal data
[00:10:16.560 --> 00:10:20.640]   store by Facebook, it takes you to a form that says, "Go to your Facebook page and then down
[00:10:20.640 --> 00:10:28.400]   the other data." So you can't. Okay. So I know that they're upset about this, but this is exactly
[00:10:28.400 --> 00:10:35.120]   what data brokers do. Right. And it's not just Facebook and it's all channel Jeff Jarvis.
[00:10:35.120 --> 00:10:38.320]   Leo, this has been going on for years. Not bad.
[00:10:38.320 --> 00:10:42.800]   Forever, for decades. What's the harm? What's the harm?
[00:10:42.800 --> 00:10:47.360]   Because it's been, and then I use the word "weaponize" because it's been weaponized.
[00:10:47.360 --> 00:10:51.440]   They're really good at it now. This is the other good in the living room.
[00:10:51.440 --> 00:10:53.760]   This is like XKCD 743, isn't it?
[00:10:53.760 --> 00:11:00.640]   Are we now to the point where we tell you, it's like the joke telling conference where
[00:11:00.640 --> 00:11:06.720]   nobody tells a joke because you say the punchline? XKCD 743, who can forget?
[00:11:06.720 --> 00:11:10.400]   Infrastructure. Is that it? Yes.
[00:11:10.400 --> 00:11:16.400]   Did you get my essay in 2003? Yeah, it was good, but it was a doc. You really should use it more open.
[00:11:17.120 --> 00:11:20.240]   Give it a rest already. Maybe we just want to live our lives and use software that works and
[00:11:20.240 --> 00:11:25.360]   not get wrapped up in your stupid nerd turf wars. I just want people to care about the
[00:11:25.360 --> 00:11:28.800]   infrastructures we're building and who know you just want to feel smugly superior.
[00:11:28.800 --> 00:11:33.920]   You have no sense of perspective and are probably autistic. That's in 2003, 2010.
[00:11:33.920 --> 00:11:37.360]   Oh my God. We had a control of our social world, the Facebook and they're doing evil stuff.
[00:11:37.360 --> 00:11:42.640]   Do you see this? It's the world's tiniest open source violin.
[00:11:42.640 --> 00:11:46.720]   This is what Corey doctor wrote today in Boing Boing, and I know you've been saying that you've
[00:11:46.720 --> 00:11:51.520]   been beating this drum. Kevin Marks. This is what this is what everybody's saying is,
[00:11:51.520 --> 00:11:56.480]   OK, let's make something. Let's make an alternative. But where is the alternative?
[00:11:56.480 --> 00:12:00.480]   Well, but no, but not making alternative. Oh my gosh. Let's make it better and not make
[00:12:00.480 --> 00:12:05.120]   rules. You know, I don't have much confidence. The rules are going to happen very well. But all right.
[00:12:05.120 --> 00:12:11.120]   Go ahead. OK, there's two things here. One, this this cartoon is from 2010 and we already
[00:12:11.120 --> 00:12:15.520]   need to face it with you. Good point. That's eight years ago.
[00:12:15.520 --> 00:12:18.480]   How long he's been doing the I'm sorry, we'll make this better thing.
[00:12:18.480 --> 00:12:22.240]   Yeah, no longer than that. Yeah, it goes back to when he was in the
[00:12:22.240 --> 00:12:26.640]   bedroom collecting this. Yeah, that's true. Actually, before we even Facebook started,
[00:12:26.640 --> 00:12:30.640]   he was apologizing for the previous things he'd done. Yeah. So yes, you know, he has, he has,
[00:12:30.640 --> 00:12:36.320]   you know, a pattern here. Do things to as much and get away with, apologize.
[00:12:36.320 --> 00:12:41.200]   So you make it better. Actually turn the dial up again and iterate. So there's that piece of it.
[00:12:42.480 --> 00:12:49.280]   The other piece, the shadow profiles piece there, that is the subtle thing there is that they're
[00:12:49.280 --> 00:12:54.640]   importing everyone's address books. So and they do their best to do that. And they bought Octasen,
[00:12:54.640 --> 00:13:00.960]   which was the company that scraped address books from everyone else around 2010. And they will
[00:13:00.960 --> 00:13:06.080]   happily have a large team whose job is to get any kind of address book data into Facebook.
[00:13:06.080 --> 00:13:10.960]   There's no team to get it out again. When you export from Facebook, all that information is gone.
[00:13:10.960 --> 00:13:14.880]   You just get people's names. You don't even get their Facebook profiles that alone,
[00:13:14.880 --> 00:13:20.160]   any other links or URLs or phone numbers or emails or any other the contact points they have.
[00:13:20.160 --> 00:13:27.440]   The export dump is the flattest HTML export ever. At least HTML, but it's not any useful form.
[00:13:27.440 --> 00:13:33.120]   So the the Roach-Mato thing is spot on. They take stuff in, they never get it out again.
[00:13:33.120 --> 00:13:40.080]   They don't act as an agent for the user. And this is the subtle point. One of the problems
[00:13:40.080 --> 00:13:45.280]   we've had with doing permissions with software is that it all goes back to Unix where the
[00:13:45.280 --> 00:13:49.360]   assumption was the program was running on behalf of the person running it because everyone was
[00:13:49.360 --> 00:13:53.120]   a programmer and they understood what was going on in theory. So when you run code, it runs as you
[00:13:53.120 --> 00:13:58.800]   and it can do what you can do. And that sort of concept has transferred into this world.
[00:13:58.800 --> 00:14:04.080]   So Facebook says, I'm you. I will connect to your phone and your Google account and
[00:14:05.840 --> 00:14:10.480]   your Microsoft account and draw in the contact details of everyone you've ever been in contact
[00:14:10.480 --> 00:14:14.880]   with and put that on the side somewhere. And then when it comes to time for you to look for friends,
[00:14:14.880 --> 00:14:21.600]   it'll suggest them to you. But conversely, when anyone who's ever been in that address book
[00:14:21.600 --> 00:14:26.880]   signs up for Facebook, it'll recommend you as a friend to them. Now that feels like a comfortable
[00:14:26.880 --> 00:14:31.920]   thing for them to do when I looked for Stacey Higginbach, a little bit family, the right one
[00:14:31.920 --> 00:14:35.440]   just now, even though Stacey and friends on Facebook, and I'm not even sure we have
[00:14:35.440 --> 00:14:42.880]   shared phone numbers, we could do enough analysis because we both know that it found us.
[00:14:42.880 --> 00:14:47.840]   So it does a lot of this stuff in the background. And that was Google probably not Facebook,
[00:14:47.840 --> 00:14:54.880]   that was Google. That was Facebook. Kevin, I wait to Facebook.
[00:14:54.880 --> 00:15:01.840]   Oh, okay. I mean, it's a lot of things. This is a similar phenomenon as with face recognition.
[00:15:01.840 --> 00:15:09.040]   So I think it's much worse on Google photos, which I love Google photos, but the fact is that other
[00:15:09.040 --> 00:15:15.840]   people can identify your face with your name and your specific user personhood
[00:15:15.840 --> 00:15:20.640]   beyond your knowledge or control on Facebook. You can do the same thing, although with Facebook,
[00:15:20.640 --> 00:15:23.920]   they have tools that you can go and find out who's doing this. You can opt out and things like that
[00:15:23.920 --> 00:15:27.600]   with Google photos. I don't think you can. For example, I go into my Google photos. I got pictures
[00:15:27.600 --> 00:15:31.440]   of you on my Google photos and I say, yeah, that's Leo Laport. Even if I'm not on Facebook.
[00:15:31.440 --> 00:15:36.720]   Well, this is within Google. You're on Google. So they know everything that Google knows about
[00:15:36.720 --> 00:15:41.120]   you. So your search history, all that stuff is now connected to your face, which I have told
[00:15:41.120 --> 00:15:46.480]   them and you didn't know I did it. And so people have done that. Go ahead. So I just want to clarify
[00:15:46.480 --> 00:15:51.360]   what Mike is saying. So even if I never gave Google my face, you basically gave up my face to them.
[00:15:51.360 --> 00:15:56.880]   But wait, how would they? This is this. And they would know that that's me because
[00:15:56.880 --> 00:16:01.200]   that face belongs with my account because it's tied. You're just giving them my name. You're not
[00:16:01.200 --> 00:16:06.640]   giving them an email, right? So when I tag the whole point here is there's so much interlocking
[00:16:06.640 --> 00:16:10.640]   interlaced data. Yes, the larger point. No, there is. But no, they have changed that.
[00:16:10.640 --> 00:16:16.960]   So they were doing more of that before. The previous version of the photo stuff did do a lot
[00:16:16.960 --> 00:16:22.880]   more of that. And so when you went to tag someone, it would attach their emails to it from your
[00:16:22.880 --> 00:16:27.920]   contacts. The new Google photos stuff has taken a lot of that away. And it's doing less of that.
[00:16:27.920 --> 00:16:31.840]   So you're just putting a label on that face for you, but it's clustering in the face.
[00:16:31.840 --> 00:16:34.480]   So here's the category. I don't think he's doing it as much across.
[00:16:34.480 --> 00:16:40.000]   Yeah. So here's the categorical issue that probably won't be covered by legislation,
[00:16:40.000 --> 00:16:45.520]   but absolutely is a core issue here, which is that other people can violate your privacy,
[00:16:45.520 --> 00:16:50.000]   other users. Yes. And this is this is the centerpiece of the Cambridge Analytica
[00:16:50.000 --> 00:16:53.840]   situation. It's with the face recognition. It's with the face.
[00:16:53.840 --> 00:16:57.760]   Face context. Well, we turned that off in 2011. Okay, but it still happens.
[00:16:57.760 --> 00:17:02.480]   If I take a screenshot from the show, which I will, and I tag Stacey,
[00:17:02.480 --> 00:17:06.640]   I will be tagging her Facebook. You're giving information to Facebook.
[00:17:06.640 --> 00:17:11.680]   And I'm telling Facebook about Stacey. Not only what her face looks like, what her clothes look
[00:17:11.680 --> 00:17:15.040]   like, what her office looks like. Even if you're not connected as friends, you can do that.
[00:17:15.040 --> 00:17:18.880]   So Facebook would see it could take. Wait a minute, though, when you tag it on Facebook,
[00:17:18.880 --> 00:17:21.520]   if you're not friends with Stacey, her name's not going to pop up.
[00:17:22.400 --> 00:17:26.720]   Correct. But she has friends who will tag her. And again, if I tag her, if I
[00:17:26.720 --> 00:17:31.520]   have to be your friends, where you're real enemies, we all have enough family and friends and former
[00:17:31.520 --> 00:17:35.760]   high school people who will tag us with we will be tagged. Okay, Stacy, go ahead.
[00:17:35.760 --> 00:17:37.360]   You can just make one more thing. One more thing.
[00:17:37.360 --> 00:17:41.840]   Go ahead and finish my when I'm saying is that they would say a picture of her empty office
[00:17:41.840 --> 00:17:45.120]   will be registered by Facebook as Stacey. Right.
[00:17:45.120 --> 00:17:51.040]   The things on her desk are part of this database. Right. Nobody.
[00:17:51.040 --> 00:17:54.960]   That's what I mean by weaponization. The algorithm is good enough now, and there's enough data
[00:17:54.960 --> 00:17:58.640]   points now that really there's very little that's not known. Go ahead, Stacy.
[00:17:58.640 --> 00:18:02.240]   Okay. So while we're freaking out about this, I'm just going to tell you about GDPR,
[00:18:02.240 --> 00:18:10.240]   because it is the most, it's probably the most comprehensive data privacy rules out there.
[00:18:10.240 --> 00:18:13.360]   And so maybe we can get to something like that. They actually have secondary,
[00:18:13.360 --> 00:18:17.120]   they have a whole regulation chunk about the secondary uses of data.
[00:18:18.560 --> 00:18:23.440]   So you have to de-identify,
[00:18:23.440 --> 00:18:30.480]   de-identify, cannot speak, sorry. This is data that can no longer be attributed to a specific
[00:18:30.480 --> 00:18:34.800]   data subject without the use of additional information provided that such additional
[00:18:34.800 --> 00:18:38.480]   information is kept separately and subject to technical and organizational measures to ensure
[00:18:38.480 --> 00:18:43.200]   that the personal data is not attributed to an identified or identifiable natural person.
[00:18:43.200 --> 00:18:48.000]   So that's exactly what we're talking about. Using secondary data from another source
[00:18:48.000 --> 00:18:54.240]   to identify me, that is verboten, it is not allowed. So we're talking about this now,
[00:18:54.240 --> 00:18:58.480]   so I'm trying to give you actual things that have happened related to this. So you know.
[00:18:58.480 --> 00:19:04.400]   I don't want to be that guy. You don't want to be the guy with the roof?
[00:19:04.400 --> 00:19:06.320]   Somebody has to be the guy. So I'm going to be the guy.
[00:19:06.320 --> 00:19:10.480]   Okay. What about the peltzmann model of regulation?
[00:19:12.720 --> 00:19:21.440]   This is Corey Doctorow pointed to this, which is that really these hearings aren't about privacy
[00:19:21.440 --> 00:19:26.480]   or about technology, but about what politicians want. Of course, right? The politicians are having
[00:19:26.480 --> 00:19:31.920]   the hearings. Now, if you're a real cynic, you'll go with Sam Peltzmann's economist
[00:19:31.920 --> 00:19:38.640]   wrote about this 10 years ago. He's positioned, and again, this is in fact, this is economics.
[00:19:40.640 --> 00:19:46.080]   The dismal science. The opposite. The opposite. Politicians use regulation to trade off profits,
[00:19:46.080 --> 00:19:52.000]   which firms want, lower prices, which voters want, to maximize what politicians want.
[00:19:52.000 --> 00:19:54.800]   Power. Re-election. Re-election.
[00:19:54.800 --> 00:20:00.400]   The key is they're diminishing returns to politicians in both profits and lower prices. Consider
[00:20:00.400 --> 00:20:04.400]   a competitive industry. It doesn't do much for politicians, so they might want to regulate the
[00:20:04.400 --> 00:20:09.760]   industry to raise prices and increase firm profits. Now, profitable firms will reward the
[00:20:09.760 --> 00:20:15.040]   hand that feeds them with campaign funds and by diverting some of the industry's profits to
[00:20:15.040 --> 00:20:21.280]   subsidize a politician's most important constituents. But wait, consumers now are going to be upset
[00:20:21.280 --> 00:20:25.520]   by the higher price, but if it's not too much higher, then that game, the politician will be
[00:20:25.520 --> 00:20:31.680]   positive. They'll get more. We've all played those sim games, like Civ 5, right? What about
[00:20:31.680 --> 00:20:38.240]   an unregulated monopoly like Facebook? By the way, they asked Mark Zuckerberg, are you
[00:20:38.240 --> 00:20:44.240]   monopolies? No, no, no. And then, has it Blumen saw a camera who said it? Well, can you name your
[00:20:44.240 --> 00:20:50.800]   competitors? And Mark froze. The reason he froze is not that he doesn't have competitors.
[00:20:50.800 --> 00:20:54.000]   This sounds ridiculous. Yeah, we're really worried about Twitter. Twitter.
[00:20:54.000 --> 00:20:58.800]   Twitter is a big problem for us. Yeah, Pinterest. Google+ is going to kill us. What's app?
[00:20:58.800 --> 00:21:03.200]   Politicians will regulate the monopolist to lower prices. Okay. And to encourage a
[00:21:03.200 --> 00:21:09.200]   monopolist to divert some of its profits to the politician and the politicians constituents.
[00:21:09.200 --> 00:21:14.480]   Monopolists will be upset by the lower price, but yeah, if it's not lower too much, maybe
[00:21:14.480 --> 00:21:18.640]   maybe the below monopoly levels, the neck gain of the politician will be positive. The monopolies
[00:21:18.640 --> 00:21:24.080]   won't mind too much. But that is background. The Facebook hearings are easily understood. This is
[00:21:24.080 --> 00:21:28.960]   actually Alex Tabarak, who writes a blog called the marginal revolution university. I don't know
[00:21:28.960 --> 00:21:34.160]   what it's about. This is Corey Pwnett to this Facebook is a very profitable monopoly that doesn't
[00:21:34.160 --> 00:21:39.920]   benefit politicians very much. Although consumers aren't upset by high prices since Facebook's free,
[00:21:39.920 --> 00:21:43.760]   they can be made to be upset about the loss of privacy or other such scandal. That's enough
[00:21:43.760 --> 00:21:49.200]   to threaten regulation. The regulatory outcome will be that Facebook diverts some of its profits
[00:21:49.200 --> 00:21:54.640]   to campaign funds and to subsidize important political constituents. So what you got to watch
[00:21:54.640 --> 00:21:59.520]   here is who's going to be subsidized. Be sure to watch the key players as there's plenty to go
[00:21:59.520 --> 00:22:04.080]   around the money has only begun to flow. But aside from campaign funds, look for rules,
[00:22:04.080 --> 00:22:08.560]   especially in the political sphere that will raise the costs of advertising to challengers
[00:22:08.560 --> 00:22:14.320]   relative to incumbents. Incumbents love the incumbency advantage. Also watch for a deal. And
[00:22:14.320 --> 00:22:20.560]   this is the one I'm worried about where government limits profit regulation in return for greater
[00:22:20.560 --> 00:22:26.720]   government access to Facebook data, including by the NSA, ICE, local and even foreign police.
[00:22:26.720 --> 00:22:32.000]   Keep in mind politicians don't really want privacy. Congress held hearings two years ago on privacy
[00:22:32.000 --> 00:22:37.280]   and technology. Only those hearings were about how technology companies kept their user data to
[00:22:37.280 --> 00:22:45.600]   private. So wouldn't it be a shame if the outcome of all of this is, okay, it's good that you're
[00:22:45.600 --> 00:22:50.640]   gathering this information as long as you hand it over to us and let it be to our advantage.
[00:22:50.640 --> 00:22:57.520]   Now, that's why, Stacy, when you say regulations, the answer, I agree in a perfect world,
[00:22:57.520 --> 00:23:01.680]   but I just I don't have high hopes for it happening in the US.
[00:23:01.680 --> 00:23:04.560]   They will happen and they will it will happen
[00:23:04.560 --> 00:23:10.480]   the way it will likely happen. So a couple of things. One, I think you're totally on point.
[00:23:10.480 --> 00:23:16.720]   Congress does not care about making loss. However, there are regulatory agencies and those are the
[00:23:16.720 --> 00:23:25.600]   people who pressure Congress and have been. So the FTC in 2013 started looking at IOT and social
[00:23:25.600 --> 00:23:31.280]   data data and everything. And they actually in 2015 came out with the report, I believe it was in
[00:23:31.280 --> 00:23:38.080]   January 2015 saying, hey, we should create some rules around data. The rules they suggested actually
[00:23:38.080 --> 00:23:45.120]   looked a lot like what GDPR became. But Congress was like, oh, heck no. So what you're actually
[00:23:45.120 --> 00:23:54.080]   going to see is I would look at Facebook versus the ISPs because right now the ISPs are picking up
[00:23:54.080 --> 00:24:01.120]   the privacy kind of rules as something they're like, oh, hey, hey, we like privacy.
[00:24:01.120 --> 00:24:05.760]   Let's talk about how we can use privacy rules to disadvantage Facebook, which is one of the
[00:24:05.760 --> 00:24:10.960]   scariest companies out there in Google. So that's how the regulation is going to happen.
[00:24:10.960 --> 00:24:16.480]   It's going to be the ISPs versus the companies that have basically
[00:24:16.480 --> 00:24:20.960]   disintermediated the ISPs. Yeah, because as we've had said many times, it's the ISPs that really
[00:24:20.960 --> 00:24:25.760]   are the ones who know everything about you. And you'll see this. Although, let me ask how
[00:24:25.760 --> 00:24:31.360]   true that is because when I use Google for a variety of reasons, possibly to keep the ISPs
[00:24:31.360 --> 00:24:37.600]   disenfranchised, Google encrypts all my searches when I use Facebook, it's fire sheep and other
[00:24:37.600 --> 00:24:41.920]   hacks against Facebook is protecting us. But it does encrypts everything I do with Facebook.
[00:24:41.920 --> 00:24:47.920]   So what does the ISP know in this day of HTTP? Every S everywhere. Well, for starters, they know
[00:24:47.920 --> 00:24:52.400]   you're visiting. They know you're addressed. They know where I live. They have credit cards.
[00:24:52.400 --> 00:24:58.080]   They know exactly what devices you have in your house. They know what websites you visit.
[00:24:59.440 --> 00:25:03.200]   They know what kind of traffic you're sending. So they know what you're doing.
[00:25:03.200 --> 00:25:07.680]   Isn't see I thinking maybe Google and Facebook are more of a threat than the ISPs because Google
[00:25:07.680 --> 00:25:13.920]   sees all my searches and Facebook, if you use it, has all my data. Yes, but you don't have to use
[00:25:13.920 --> 00:25:20.960]   any of those things. I know, but you do. I mean, you don't. You can use Google. You don't have to,
[00:25:20.960 --> 00:25:27.120]   but we do use them. I know, I know, but there is a different class of regulations for something that
[00:25:27.120 --> 00:25:33.680]   is indeed what is the word I'm looking for. It is something that you have. You have no viable
[00:25:33.680 --> 00:25:39.760]   alternative. You have to use. You have to be on the Internet. If you want to be on the Internet,
[00:25:39.760 --> 00:25:44.960]   you have to go to one or possibly two. And there's a duopoly. So one of those ISPs,
[00:25:44.960 --> 00:25:48.880]   either your phone company or cable company is going to get all that data. And we know they're
[00:25:48.880 --> 00:25:53.760]   terrible, by the way, in all of this. By the way, we also know, and I should point out,
[00:25:53.760 --> 00:25:57.600]   that the FCC's privacy rules were repealed very early on in this administration.
[00:25:57.600 --> 00:26:01.920]   Yes, which is how I can find you exactly what they see and what they do.
[00:26:01.920 --> 00:26:05.200]   Right. One of the first things that happened. I just hope we don't go back and forth every
[00:26:05.200 --> 00:26:10.400]   time there's a new administration in the White House. A utility. Yes. Thank you guys in the chat room.
[00:26:10.400 --> 00:26:20.080]   So one thing we, there's a temptation to go on Facebook in with other companies in various ways,
[00:26:20.080 --> 00:26:26.320]   but one way that they stand apart is on the metric of trust. Rico just published a survey that they
[00:26:26.320 --> 00:26:34.480]   did based on public trust of various companies in tech. And Facebook was like 10 times less
[00:26:34.480 --> 00:26:40.480]   trustworthy than the number two, which was the least trusted by this graph. I mean, that's just,
[00:26:40.480 --> 00:26:44.400]   I can't even fit the bar on the screen. So they're in a class of their own.
[00:26:46.160 --> 00:26:54.800]   So Facebook is 56% at least trusted Google five, Uber three, Twitter three, snap to Apple two.
[00:26:54.800 --> 00:27:01.360]   People trust Uber more than Google. Yeah. I'm like, Uber, I feel like would send like people to
[00:27:01.360 --> 00:27:07.280]   kill me based on where my Uber driver is. If I just like aggravated them.
[00:27:07.280 --> 00:27:11.520]   Uber reminds me of Scientology a little bit. Oh, yeah, that's a good come. Yeah.
[00:27:13.040 --> 00:27:16.640]   Scientology. They trust Uber enough to get in the bag,
[00:27:16.640 --> 00:27:21.120]   aggravate and be driven somewhere. That's the thing. That's true. They are putting themselves in
[00:27:21.120 --> 00:27:25.280]   Uber's hands quite a lot. Yeah. Facebook, they feel like they have to do it. Uber, they've volunteered
[00:27:25.280 --> 00:27:30.720]   to use it instead of a bus. And also something. Well, there is a similarity to Facebook, Google,
[00:27:30.720 --> 00:27:36.560]   and Uber, maybe less of these other ones. But those three provide you with massive convenience
[00:27:36.560 --> 00:27:42.240]   and value. Yes. Network effects. But Uber doesn't know what you're reading. And they don't,
[00:27:42.240 --> 00:27:45.760]   I don't think they know who your friends and family are. And I don't.
[00:27:45.760 --> 00:27:51.360]   Well, here's the weird thing. Anybody who has an app on your phone that is leaking location
[00:27:51.360 --> 00:27:56.640]   information knows your friends and family. They know who you sleep with. They know who you have
[00:27:56.640 --> 00:28:01.280]   Thanksgiving dinner with. And when you combine location information, I was talking about this
[00:28:01.280 --> 00:28:09.200]   with Lisa, it was one of the reasons I love Android. But Android is a sieve when it comes to
[00:28:09.200 --> 00:28:14.320]   your location information. Apple has been smart. I'm not convinced it's not merely for marketing
[00:28:14.320 --> 00:28:21.040]   purposes. But so what? An app uses your location. You have to give it permission and it will remind
[00:28:21.040 --> 00:28:25.040]   you several more times that you know, this app is still looking everywhere you're going.
[00:28:25.040 --> 00:28:30.560]   That's really important. I don't think most people understand how valuable location information is.
[00:28:30.560 --> 00:28:33.600]   Where you stand depends on where you sit. Apple's case. They don't need it.
[00:28:33.600 --> 00:28:40.480]   Wait a minute. Let me write that down. In Apple's case, they don't need personal data for their
[00:28:40.480 --> 00:28:45.440]   business. They sell super expensive phones and they take a third of revenue from the apps.
[00:28:45.440 --> 00:28:49.440]   But their services, their online services, 10 billion dollars last year.
[00:28:49.440 --> 00:28:53.040]   No, well, they're terrible, but they're also very, very profitable, more profitable than the
[00:28:53.040 --> 00:28:58.000]   Mac division. But they aren't data services. The online services are then resetting other people's
[00:28:58.000 --> 00:29:04.800]   stuff. That revenue is that 30%. It's not true. And even they're providing you directly.
[00:29:04.800 --> 00:29:10.000]   And Apple music and they'd like it to be Apple TV. And by the way, those will gather information.
[00:29:10.000 --> 00:29:12.960]   Yeah, but there's the same model. There are cell models as well.
[00:29:12.960 --> 00:29:21.360]   Yeah. And if you buy into the idea that if you buy into the fact or the idea that customization
[00:29:21.360 --> 00:29:26.960]   is going to be the thing to get the next generation of people online and sell to them and all of this,
[00:29:26.960 --> 00:29:32.080]   then if you don't have a good data acquisition strategy, you can't actually compete in that
[00:29:32.080 --> 00:29:37.200]   world. And that's the questions we all have right now about Apple. They're doing just by now.
[00:29:37.200 --> 00:29:41.440]   But look at how the HomePod did with crappy Siri on it. No kidding.
[00:29:41.440 --> 00:29:49.440]   That's so great. So those are the kind of, we're moving to this era where every business is being
[00:29:49.440 --> 00:29:55.280]   threatened with obsolescence if they don't collect user data and figure out ways to customize and
[00:29:55.280 --> 00:30:02.160]   deliver products to them. And just macro business trends. Think about how often do you hear consumers,
[00:30:02.160 --> 00:30:06.240]   millennials want customization. They want to have these personalized experiences.
[00:30:06.240 --> 00:30:11.040]   Nope. You're right. And I'm going to predict something. Any minute now, we're going to see,
[00:30:11.040 --> 00:30:15.680]   we're probably seeing it right now with our audience, privacy fatigue. Like, oh, screw it.
[00:30:15.680 --> 00:30:23.280]   Yeah. That's going to happen soon. I don't care. I like writing in Uber. I need to use Google for
[00:30:23.280 --> 00:30:27.360]   searching. I got you with my family and friends on Facebook. I feel it inside of me. It's fine.
[00:30:27.360 --> 00:30:35.440]   It makes me feel good. What's the problem? Just it's XK, I might say XKCD 743 all over again.
[00:30:35.440 --> 00:30:37.040]   Exactly. Well said.
[00:30:37.040 --> 00:30:44.080]   College is, we have to think about how you want to regulate this. So do you want to
[00:30:44.080 --> 00:30:50.720]   regulate the outcomes? Like, okay, we're going to collect all this data and treat it well. And if
[00:30:50.720 --> 00:30:55.920]   people use it in malicious or nefarious ways that don't jive with our sensibilities,
[00:30:55.920 --> 00:31:00.480]   do we punish that or do we punish? That's the Jeff Jarvis argument is that social
[00:31:00.480 --> 00:31:05.840]   morays, social norms should govern this. Isn't that what you're saying, Stacey?
[00:31:05.840 --> 00:31:12.240]   Sort of. There's the idea that collecting data is fine. Everyone should be able to collect data.
[00:31:12.240 --> 00:31:16.960]   I don't actually agree with that. Jeff agrees or I, and Jeff will tell me if I'm wrong, I'm sure.
[00:31:16.960 --> 00:31:22.560]   But it's fine to collect data. It's what you do with it that matters. He and I disagree with,
[00:31:22.560 --> 00:31:30.640]   I think it's fine to collect data, but you should treat the data as sync or sent, not the use cases,
[00:31:30.640 --> 00:31:36.080]   if that makes sense. So the user should have the option to opt out of things. Even if I decide,
[00:31:36.080 --> 00:31:41.040]   you know what? I don't want my medical data to be helping people with lung cancer. Screw those
[00:31:41.040 --> 00:31:47.440]   people with lung cancer. I should have that right. So here's how it goes down. So basically the
[00:31:47.440 --> 00:31:53.200]   regulators responding to public pressure say, "Okay, listen, we want to regulate you guys."
[00:31:53.200 --> 00:31:59.440]   And you heard everybody asking if Zuck supports various types of legislation as if that should
[00:31:59.440 --> 00:32:04.560]   matter. But they say, "Okay, we want to not let you do this." And then the companies say,
[00:32:04.560 --> 00:32:08.560]   "Well, if you don't let us do that, then these people won't get these valuable services that
[00:32:08.560 --> 00:32:12.640]   they want." Okay, well, in that case, what you have to do is you have to make sure that they
[00:32:12.640 --> 00:32:17.920]   are aware of everything that they're opting into. And that'll be the law. That's what's happening
[00:32:17.920 --> 00:32:22.640]   in Europe. And then once you have been made aware, we'll just agree to it. When you're in
[00:32:22.640 --> 00:32:27.280]   Europe, it's maddening to use the internet. Because every page you go to, it says, "Did you know
[00:32:27.280 --> 00:32:31.120]   cookies exist?" Literally every page. It's insane.
[00:32:31.120 --> 00:32:37.440]   So Europe has more than just notification and transparency, Mike. It has the ability to pull the
[00:32:37.440 --> 00:32:41.840]   data that you want. Sorry, it's pulled the data that someone has on you. You have the ability to
[00:32:41.840 --> 00:32:47.840]   opt out of them keeping your data. It has the ability to actually pause your data. I don't know
[00:32:47.840 --> 00:32:52.080]   how they're going to do this, but pause the use of your data for a certain period of time. If you're
[00:32:52.080 --> 00:32:57.120]   applying for a home loan, you can actually pull all of your data and put a pause on people accessing
[00:32:57.120 --> 00:33:00.640]   it. So it's way more than what you're saying. And I think...
[00:33:00.640 --> 00:33:04.880]   Yeah, no, it's a good point you're making. It's a very good point you're making. But I guess my
[00:33:04.880 --> 00:33:12.000]   point is that there'll be a series of opt-ins and approvals and this sort of thing. And people will,
[00:33:12.000 --> 00:33:16.560]   for the most part, opt in and approve. So it's basically the same stuff as it's going to happen,
[00:33:16.560 --> 00:33:21.680]   but people will be explicitly agreeing to it for exactly the reason Leo was saying, which is that
[00:33:21.680 --> 00:33:27.120]   people have privacy, fatigue. They don't really understand that there will be a harm. And in some
[00:33:27.120 --> 00:33:34.720]   cases, what is the harm often to the average person? Good advertising? I mean, so I think that
[00:33:34.720 --> 00:33:41.360]   we are... At the end of all of this, companies are going to be doing a lot of crazy stuff with
[00:33:41.360 --> 00:33:47.200]   our data, except they'll also have our approval to do it. We will approve that stuff because we
[00:33:47.200 --> 00:33:50.480]   want the features. We want the benefits. We want the apps. We want all of it.
[00:33:50.480 --> 00:33:58.480]   But that is the change. No, that is the change though. So what GDPR changes is that you have to...
[00:33:58.480 --> 00:34:05.120]   There are strict terms on what you can do. The terms of service has to be concise,
[00:34:05.120 --> 00:34:09.360]   transparent, intelligible, and easily accessible, written in clear and plain language, particularly
[00:34:09.360 --> 00:34:14.800]   addressed to a child. And it has to be described what they're going to use it for, and you can't
[00:34:14.800 --> 00:34:20.240]   use it for anything else, and it has to be revocable. So there are structural things that are defined
[00:34:20.240 --> 00:34:27.120]   by GDPR that means you can't just sign a 20-page TOS without reading it. They have to have got
[00:34:27.120 --> 00:34:31.200]   explicit consent for the exact use they're going to make of the data. They can't just gather it
[00:34:31.200 --> 00:34:35.200]   and use it for something else later. So that is a structural assault on what they're doing.
[00:34:35.200 --> 00:34:38.720]   Yeah, it can't be used for other purposes unless you consent to that. What I'm saying is that,
[00:34:38.720 --> 00:34:45.520]   yeah, it'll be better. But it'll still be more about consent rather than the data will still move
[00:34:45.520 --> 00:34:53.520]   and be shared and used the way it is now more or less. But people will be supposedly
[00:34:53.520 --> 00:34:57.920]   explicitly opting into it. The point I'm trying to make is that just because--
[00:34:57.920 --> 00:35:02.240]   And they'll be able to see the data that someone has on you, which is really important,
[00:35:02.240 --> 00:35:07.280]   because it's one thing to consent to weird words, and it's another to go to your profiles that they
[00:35:07.280 --> 00:35:11.040]   have on you and see the correlations to keep on making. I'm not sure that's true, Stacey.
[00:35:11.040 --> 00:35:14.880]   I mean, Google has been really good at showing you the data they have on you and hasn't changed
[00:35:14.880 --> 00:35:20.240]   anything. Everybody still uses Google the same way. That is not true. But people do go look,
[00:35:20.240 --> 00:35:25.120]   and people change things, or they're at least explicitly aware of, if I want this service,
[00:35:25.120 --> 00:35:30.960]   I have to let them collect this data. And I know people who go in and delete their Google home
[00:35:30.960 --> 00:35:36.240]   utterances. Yeah, because you know nerds, Stacey, the general public is not going to go in.
[00:35:36.240 --> 00:35:38.800]   Doesn't even still doesn't even know that stuff exists, probably.
[00:35:38.800 --> 00:35:45.120]   You're right. We have an absence of knowledge here. Somebody should study this and find out who's
[00:35:45.120 --> 00:35:52.160]   doing stuff. May 25th, after GDPR goes into effect in Europe, what I want you to look for is a
[00:35:52.160 --> 00:35:57.360]   bunch of journalists are going to start requesting data. So European journalists in Kevin Marks
[00:35:57.360 --> 00:36:00.880]   are going to start requesting the data various companies have on them. And they're going to
[00:36:00.880 --> 00:36:06.880]   start writing stories saying, Hey, guess who knows this about me? And people are going to become
[00:36:06.880 --> 00:36:11.360]   informed, much like a bunch of people downloaded the data Facebook has about them. Will everyone?
[00:36:11.360 --> 00:36:13.360]   No. But is it a good start? Yes.
[00:36:14.000 --> 00:36:19.920]   I love your optimism. Me too. The FTC says, you can remove
[00:36:19.920 --> 00:36:23.440]   Please download your data, by the way, and then come on the show and tell us what you felt.
[00:36:23.440 --> 00:36:26.640]   Yeah. Please, please. Yeah. I wish I had that before I did.
[00:36:26.640 --> 00:36:34.160]   I've done it. What Facebook gave me already. And it was, and it was very anodyne. Basically,
[00:36:34.160 --> 00:36:39.120]   it was conversations and things. But you couldn't tell the other person was half the time,
[00:36:39.120 --> 00:36:41.280]   or it would just give you a little piece of it. So,
[00:36:42.400 --> 00:36:44.320]   is he getting different data?
[00:36:44.320 --> 00:36:48.560]   One thing GDPR says, we have to be getting here. I mean, are we not getting the same data?
[00:36:48.560 --> 00:36:51.760]   That was, by the way, at the moment, but that was that will change that's supposed to change.
[00:36:51.760 --> 00:36:56.880]   Zuck said that was a correction of an earlier article that, in fact,
[00:36:56.880 --> 00:37:01.680]   the GDPR rules would also be applied in the US, even though they didn't have to.
[00:37:01.680 --> 00:37:07.360]   So we will presume, like, okay, so I've been seeing all these people talk about their data dump.
[00:37:07.360 --> 00:37:10.320]   So we're not getting that full data dump yet.
[00:37:10.320 --> 00:37:16.160]   Yeah, I don't think so. So there is now a requirement to provide personal data in a structured,
[00:37:16.160 --> 00:37:21.440]   commonly used, and machine readable form. That is part of the GDPR requirement. So the Facebook
[00:37:21.440 --> 00:37:25.520]   export needs to be in a useful form, must be free of charge.
[00:37:25.520 --> 00:37:31.360]   And it makes sense that this is for Facebook to provide it to everybody, if they're going to do
[00:37:31.360 --> 00:37:37.360]   it, and I just want to do it for everybody. Yeah. Unless your business model is dependent on it.
[00:37:38.640 --> 00:37:41.040]   That's to me. That's the big problem. Anyway, I want to move on.
[00:37:41.040 --> 00:37:44.720]   Final thoughts. Final thoughts. Yes.
[00:37:44.720 --> 00:37:50.240]   So the other thing that was playing in Congress was very, very much this thing of,
[00:37:50.240 --> 00:37:54.160]   we want to take better care of your data. We want to make sure these terrible third parties
[00:37:54.160 --> 00:38:01.760]   don't take advantage of it ever again. And was playing into the idea that they should be the
[00:38:01.760 --> 00:38:05.040]   sort of government certified trusted guard in some other people's data.
[00:38:05.040 --> 00:38:07.280]   Yeah. Yeah. It's not the work collecting it.
[00:38:07.280 --> 00:38:10.960]   The access to it. Yeah. Yeah. It's good that we're collecting it. That's good.
[00:38:10.960 --> 00:38:15.280]   Yeah. That's good thing. And there is the distinction, and since I was trying to
[00:38:15.280 --> 00:38:20.160]   say earlier about like, you can access and so on, is the distinction between an application that
[00:38:20.160 --> 00:38:25.600]   is actually on my behalf that is doing something I want to do, and an application that's acting
[00:38:25.600 --> 00:38:33.200]   on me to do something else. So if it's, you know, if it's just a game that I'm playing,
[00:38:33.200 --> 00:38:40.000]   but it siphons out my contact details, you know, my text messages and other things like that,
[00:38:40.000 --> 00:38:43.680]   that's not really actually my behalf. It's just doing that for its own nefarious reasons.
[00:38:43.680 --> 00:38:50.320]   On the other hand, if I would actually like to extract the contact information for my friends
[00:38:50.320 --> 00:38:54.960]   that I uploaded before, or a better version of that where they've actually agreed to share their
[00:38:54.960 --> 00:38:59.760]   phone numbers with the indoor map, that was there. There was a, it used to be, if you went to
[00:38:59.760 --> 00:39:04.000]   facebook.com/phonebook, it would give you the phone numbers of your friends because it was
[00:39:04.000 --> 00:39:09.680]   actually very useful to be able to do that. And that went away fairly early on, but they face
[00:39:09.680 --> 00:39:15.200]   that have the information to share it back with me. So I've, you know, Stacy will have my American
[00:39:15.200 --> 00:39:18.640]   phone number. She won't have my UK phone number, partly because I'm going to Facebook.
[00:39:18.640 --> 00:39:20.560]   Don't, don't challenge me, Kevin.
[00:39:20.560 --> 00:39:28.880]   All right. We're going to a little palette cleanser. I'm a big believer in palette cleansers.
[00:39:29.760 --> 00:39:36.720]   This is an example, and it's actually from several years ago of how we were talking about those,
[00:39:36.720 --> 00:39:39.760]   what were they called, real fakes? Yeah. The porn thing.
[00:39:39.760 --> 00:39:40.240]   These fakes.
[00:39:40.240 --> 00:39:45.600]   Here's an example. This is, now this is only for the video crew. This is George W. Bush talking,
[00:39:45.600 --> 00:39:50.000]   but it's very important to fix a broken system, to treat people.
[00:39:50.000 --> 00:39:53.920]   Hillary, Daniel Craig, Barack Obama, Pierce Morgan, Tom Sanks.
[00:39:53.920 --> 00:39:58.160]   It's a very difficult thing to do with the past because it's a lot of notable parts.
[00:39:58.160 --> 00:40:02.080]   And he and the county. The uncanny valley is kind of strong.
[00:40:02.080 --> 00:40:07.040]   It'd be ugly. Yeah. And, but the swarish thingers smile was pretty good.
[00:40:07.040 --> 00:40:10.720]   Anyway, that was a palette cleanser. That's all it was. There was nothing else.
[00:40:10.720 --> 00:40:15.440]   It's about this one. Here's a second palette cleanser. I'm not sure this is cleansing my palette,
[00:40:15.440 --> 00:40:19.200]   Leo. This is good. All right. This is good. Okay. Which of these G's is correct?
[00:40:21.920 --> 00:40:28.960]   Don't say it out loud. What's that? That's the which of these G's is correct music.
[00:40:28.960 --> 00:40:35.680]   G. Okay. Carson says three. I'm with Carson.
[00:40:35.680 --> 00:40:39.040]   Three. What do you say? You say two. I say two. What do you say, Kevin?
[00:40:39.040 --> 00:40:46.720]   I say two. Are two and three. The answer is two.
[00:40:47.760 --> 00:40:52.000]   But this is a study. I know several of you got that wrong. Carson, you got it wrong,
[00:40:52.000 --> 00:40:58.640]   even though you went to the best college in Massachusetts. This is a study from Johns Hopkins
[00:40:58.640 --> 00:41:07.440]   University in Discovery magazine. Only 25% of participants in this study chose two. The correct
[00:41:07.440 --> 00:41:13.520]   answer. Almost everybody got it wrong because that's nobody who writes that G. There's a different
[00:41:13.520 --> 00:41:18.240]   there's it's one of the few lowercase letters where there are two versions and people can't write that
[00:41:18.240 --> 00:41:23.200]   one. Yeah. There's two lowercase A's as well, but people didn't have the same trouble with that.
[00:41:23.200 --> 00:41:27.680]   It was people knew how that funny lowercase that typography looks like. And that wasn't that the
[00:41:27.680 --> 00:41:33.280]   G that they had in the original Google logo or am I mistaken about that? Oh. Yeah, I think you're
[00:41:33.280 --> 00:41:36.560]   right. It's close. It's close to it. They had one that was quite like that. Yeah.
[00:41:36.560 --> 00:41:42.880]   There's a cubicude. But anyway, yes, totally bizarre. And apparently the origins of that date
[00:41:42.880 --> 00:41:47.760]   back to medieval manuscript. Oh, yeah. And people getting a little fancy with the pen.
[00:41:47.760 --> 00:41:53.600]   It's used in Times New Roman Cambria and Calibri and a most printed and typed material.
[00:41:53.600 --> 00:41:57.200]   The second, of course, the open tail version, which is the one we all write like that. Yeah.
[00:41:57.200 --> 00:42:01.840]   Isn't that funny? The loop tail is people can't get it right. It's weird.
[00:42:01.840 --> 00:42:08.320]   Most people got the correct A lowercase A even though some people didn't even know there were
[00:42:08.320 --> 00:42:09.600]   lowercase versions of A.
[00:42:09.600 --> 00:42:13.360]   [laughter]
[00:42:13.360 --> 00:42:18.960]   Okay. Well, there were several. I mean, this is the thing. There's different styles depending on
[00:42:18.960 --> 00:42:23.280]   size and size. That was another pallet. There's a little typographic stuff.
[00:42:23.280 --> 00:42:32.160]   Now back to the news. Back to the news. FTC has done a world of good for the right to repair
[00:42:32.160 --> 00:42:38.800]   movement saying the warranty void if removed stickers you see on every piece of electronics
[00:42:38.800 --> 00:42:46.800]   gear is illegal. It does not void the warranty. They put six companies on notice telling them
[00:42:46.800 --> 00:42:53.120]   in a warning letter their warranty practices violate federal law. If, for instance, you buy
[00:42:53.120 --> 00:42:56.800]   a car with a warranty, you take it to a third party repair shop to fix it, then after return
[00:42:56.800 --> 00:43:01.120]   the car to the manufacturer, the car company cannot deny the return because you took it to
[00:43:01.120 --> 00:43:07.200]   another shop. That's true of any consumer device that costs more than $15. Remember that. It's
[00:43:07.200 --> 00:43:14.240]   the Magnus and Moss Warranty Act from 1975. We all have every device in our house as this.
[00:43:14.240 --> 00:43:20.560]   Warranty void including the actual fellows and missions and things like that. But didn't Apple,
[00:43:20.560 --> 00:43:26.000]   didn't their most recent iOS version brick phones that have been repaired by somebody other than
[00:43:26.000 --> 00:43:29.360]   ever? Yeah, you remember that? That was about a year ago. iOS 11 did that.
[00:43:29.360 --> 00:43:33.840]   They took it back though. They said, and they had actually a little bit of a more,
[00:43:35.280 --> 00:43:39.360]   I agree it probably was an economic incentive, but they were saying the problem is if you fix the
[00:43:39.360 --> 00:43:45.920]   screen, the secure enclave for the fingerprint reader is now compromised. We didn't want people
[00:43:45.920 --> 00:43:51.920]   to be able to do that. This is the devil in the details here with this FTC ruling, which is that
[00:43:51.920 --> 00:44:01.600]   as things get more complicated, they're going to require an Apple's case, special electronics,
[00:44:01.600 --> 00:44:06.800]   maybe special tools, all these kinds of things. I guess this will all be settled in court.
[00:44:06.800 --> 00:44:14.160]   It's similar to Apple's claim that they have to glue everything and make them unrepairable,
[00:44:14.160 --> 00:44:18.000]   because they want them to be thinner, thinner, thinner. Market wants thinner devices,
[00:44:18.000 --> 00:44:25.520]   which is a questionable assertion to a certain extent. It's not as simple as that. Consumers
[00:44:25.520 --> 00:44:30.800]   want super thin devices, but they also want some of them want repairability. They want certain
[00:44:30.800 --> 00:44:36.000]   types of screens, but they also want third-party repairability. It's going to be tough to sort
[00:44:36.000 --> 00:44:40.720]   this out in the world of consumer electronics. However, all these cushions and pillows, I'm
[00:44:40.720 --> 00:44:47.200]   sick of it. The mattress is... I'm talking with it. Don't remove whatever penalty of law in the
[00:44:47.200 --> 00:44:52.640]   room. That's not for you. That's just... Once you buy it, you can remove it. You can do whatever
[00:44:52.640 --> 00:44:58.480]   you want. I don't like it. I don't like it one bit. So a motherboard which wrote about this said,
[00:44:58.480 --> 00:45:04.800]   the FTC would not tell us what four companies... Six. Six, I mean companies, but Microsoft and
[00:45:04.800 --> 00:45:10.640]   Sony, perhaps, because they do that with Xbox One and PlayStation 4. Apple, a motherboard points
[00:45:10.640 --> 00:45:16.560]   out, doesn't like stickers on stuff, so Apple won't do that. But the geniuses are instructed to look
[00:45:16.560 --> 00:45:23.520]   for signs of consumer meddling and will sometimes decline warranty service. Yeah, the FTC said the
[00:45:23.520 --> 00:45:28.960]   companies that were involved, "market and sell automobiles, cellular devices, and video gaming
[00:45:28.960 --> 00:45:35.440]   systems in the United States." I wonder who that could be. Video gaming systems. Not only should
[00:45:35.440 --> 00:45:39.920]   people be able to repair their devices, no matter what, that should be required class in high school.
[00:45:39.920 --> 00:45:47.840]   High school. If I learned what I learned to repair in high school, it would be zero use to be now.
[00:45:48.480 --> 00:45:54.000]   That's a good point too. I have this battery powered alarm clock. I'd like to fix.
[00:45:54.000 --> 00:46:01.360]   Actually, there was a great story Steve Gibson did. Here's... Okay, maybe another palette cleanse,
[00:46:01.360 --> 00:46:08.560]   because this really is... But it's interesting. So there was a dispute in Europe between Kosovo
[00:46:09.440 --> 00:46:18.240]   and Serbia on the power stations. They're both on the European grid. And for six years,
[00:46:18.240 --> 00:46:26.480]   they were running the power slightly under 60 Hertz, which meant that a lot of mechanical and
[00:46:26.480 --> 00:46:34.480]   electronic clocks were off by about six minutes. The dispute was resolved recently,
[00:46:35.440 --> 00:46:41.920]   and they sped up the generator just a little bit. And now everybody's back on track.
[00:46:41.920 --> 00:46:45.440]   Unless you fixed, you said, "Hey, this clock is six minutes slow," and fixed it.
[00:46:45.440 --> 00:46:53.760]   Which case, your screw. Which everyone did. It was... I think it was an ARIS technical article
[00:46:53.760 --> 00:46:58.640]   on this strangest thing. And it kind of resonated with me, because I'm going to Japan, where part
[00:46:58.640 --> 00:47:05.200]   of the country has 50 Hertz power, and part has 60 cycles. That's how fast the
[00:47:05.200 --> 00:47:10.080]   generator spins. I didn't know this. It's actually related to the spinning of the generator.
[00:47:10.080 --> 00:47:13.680]   Yeah, it's pretty crude at the end of the day how that stuff works.
[00:47:13.680 --> 00:47:18.640]   I was going to say it's engineers who label these things. They're like, "How fast does it go?"
[00:47:18.640 --> 00:47:24.080]   50 times, 50 revolutions per whatever. It can be whatever you want it to be.
[00:47:24.080 --> 00:47:31.280]   Let's call it that. But a lot of things take this very seriously. It's the Serbian
[00:47:31.280 --> 00:47:37.520]   Kosovo grid dispute. Are you going to go nerd out in Akihabara?
[00:47:37.520 --> 00:47:41.760]   I am going to live in Akihabara as much as I can. That's for sure true.
[00:47:41.760 --> 00:47:52.000]   So, congratulations to everybody who lives in Serbia and Kosovo, because they're now
[00:47:52.000 --> 00:47:56.560]   part of the continental Europe power system. Kosovo has balanced its electrical supply.
[00:47:57.200 --> 00:48:04.400]   Balance has been restored to the universe. There's so many heavy subjects.
[00:48:04.400 --> 00:48:12.320]   Yeah. Well, okay, this isn't heavy. This is good. We're Google I/Os coming up.
[00:48:12.320 --> 00:48:20.560]   The sessions that are coming out, a hint at, according to Abner Lee writing in 9-5 Google,
[00:48:20.560 --> 00:48:26.480]   exciting updates for Google Photos and at the launch of the .app domain.
[00:48:26.480 --> 00:48:34.800]   That's going to be more secure and it's going to be part of their scheme to integrate the internet
[00:48:34.800 --> 00:48:38.800]   with apps. So, that should be good. The Google Photos thing, I think it's going to be something
[00:48:38.800 --> 00:48:42.480]   for developers. I think they're going to weaponize Google Photos and make it something that can be
[00:48:42.480 --> 00:48:48.480]   integrated into apps. That could be a very good thing. I don't think there's going to be much
[00:48:49.440 --> 00:48:53.840]   I could be wrong, but for consumers about Google Photos other than maybe a couple of
[00:48:53.840 --> 00:48:59.360]   tweaks here and there. I'd love to see them get better, for example, better editing tools.
[00:48:59.360 --> 00:49:06.160]   Lens is finally, I noticed, available Google-wide. I'm glad just in time for me to go to Japan
[00:49:06.160 --> 00:49:11.200]   and figure out what the hell I'm eating. It's really... You know what, in Japan, don't ask.
[00:49:11.200 --> 00:49:17.520]   If it's just eaten. You think of Google Translate? Yeah. Just go with it. I've eaten things in
[00:49:17.520 --> 00:49:23.040]   Tokyo where I didn't know if it was plant or animal. It was no way to know. It's like a spudgy...
[00:49:23.040 --> 00:49:26.400]   But it was good, wasn't it? And it was pretty on the plate. So hey.
[00:49:26.400 --> 00:49:30.800]   An expensive... Mary Jo Foley told me to eat octopus balls, but I'm really wondering what she means by
[00:49:30.800 --> 00:49:40.400]   that. Well, the octopi or against it. I'll tell you that. This is from Google. It is National Pet Day.
[00:49:40.400 --> 00:49:46.960]   Sorry, Kevin. You don't get to participate. Your poodles are in the UK now. But in the United
[00:49:46.960 --> 00:49:53.120]   States of America, America, we're honoring our pets and Google Photos has put together some special
[00:49:53.120 --> 00:49:58.400]   treats. Starting this week, if you take a lot of photos of your cat or dog, the Google Photos app,
[00:49:58.400 --> 00:50:04.400]   may automatically create a photo book starring your pet. And this is all without opt-in or buy-in
[00:50:04.400 --> 00:50:08.640]   by the pet itself. No, the pet's not involved. In fact, you aren't either. Google Lens.
[00:50:08.640 --> 00:50:13.520]   Pet's a property. They don't have consent. Oh, there you go. Thank you. So are small children.
[00:50:14.960 --> 00:50:21.120]   You're not a property. Google Lens will now identify popular breeds.
[00:50:21.120 --> 00:50:25.840]   So if you see a Pembroke Welsh corgi, just lying around and you say, "What is that?"
[00:50:25.840 --> 00:50:32.880]   Oh, you like that? They're cute. I have a corgi calendar. I love corgis. My sister had a bunch of
[00:50:32.880 --> 00:50:37.840]   them. I love the pixie dust effect. I love that on lens. It's so satisfying. Yeah.
[00:50:37.840 --> 00:50:43.040]   It does absolutely nothing but it makes you think it is doing something. It's thinking. It's
[00:50:43.040 --> 00:50:51.200]   satisfying thinking. It's my, as we'll say, working, working. Very dust. And you can create a movie
[00:50:51.200 --> 00:50:57.840]   using the meow movie or doggy movie option. They actually have options. Oh my god. Where are my
[00:50:57.840 --> 00:51:09.840]   photos? I have to see it. You want to see it? This is a cat baby, obviously. My god.
[00:51:09.840 --> 00:51:14.320]   [laughter]
[00:51:14.320 --> 00:51:21.360]   How could that be more annoying? Oh my goodness. Now come. I'm going to get parent points.
[00:51:21.360 --> 00:51:29.760]   So many parents. Parent points. Oh yeah, it's right here. So it's face identification for dogs.
[00:51:29.760 --> 00:51:32.640]   It's not so good here. I'm looking at this and it's mixing all my dogs up. And it's only
[00:51:32.640 --> 00:51:36.960]   found three instead of four. Oh, that's not good. And it's not found it. And it's labeled
[00:51:36.960 --> 00:51:40.800]   my previous dog with the name of the new dog. He's completely from color. So it's
[00:51:40.800 --> 00:51:48.800]   this is it's so you go into assistant in your Google photos and then at the top you see movie.
[00:51:48.800 --> 00:51:54.960]   Oh, is this on my phone? Oh, it's a mobile. Yeah. I might be able to do it. I don't know if you can
[00:51:54.960 --> 00:51:58.560]   do it on this. I'm going to be doing it on the mobile. They have they have in movies. They have
[00:51:58.560 --> 00:52:03.200]   several choices. They grow up so fast. Watch a child grow up right before your eyes. Meow movie,
[00:52:03.200 --> 00:52:09.200]   doggy movie. Oh my god. Doggy movie in loving memory. Celebrate the life of someone who has died.
[00:52:09.200 --> 00:52:15.760]   I don't know how how does Google know that selfie movie. Smiles of twenty seven. You know,
[00:52:15.760 --> 00:52:20.560]   you you you tell them that person has died. Oh, you tell or when there's when there's a lack of
[00:52:20.560 --> 00:52:25.760]   activity by that person. It just assume we now it says works best if you have a lot of photos of
[00:52:25.760 --> 00:52:29.360]   a dog. Okay, there's a dog. Oh, you're supposed to pick all the dogs.
[00:52:30.000 --> 00:52:35.280]   So you have a lot of cats in here. You're sweet. I'm doing it. We're going to we're going to see
[00:52:35.280 --> 00:52:40.560]   what happens. It's maybe maybe a lot of photos of my dogs. This is true. Right. Well, I do, but
[00:52:40.560 --> 00:52:48.160]   I only my dog is dead. I wonder if I can create a photo of my dead dog and have it to both categories
[00:52:48.160 --> 00:52:52.800]   in memoriam in the future. You'll be able to clone your dog from just the photo. It's not just
[00:52:52.800 --> 00:52:58.240]   Oh, from the photo. Yeah, because who who cloned their dog? Barbra Streisand. Yeah. And then
[00:52:58.640 --> 00:53:04.080]   Barry Diller did it too. Yeah. After because Barbara was so happy with the cloning.
[00:53:04.080 --> 00:53:11.440]   Mother's Day movie. Father's Day movie. I wonder how sexist each of those are.
[00:53:11.440 --> 00:53:16.400]   Oh, should we try? No, I don't want to know this. So you have to select the mothers.
[00:53:16.400 --> 00:53:22.800]   Okay, I'll select I'll select mothers different mothers. Megan's a mother. All right, I've got
[00:53:22.800 --> 00:53:26.320]   three mothers. They're not the same mother. Does that matter? Oh, it takes a while. You get a
[00:53:26.320 --> 00:53:32.960]   notification. It's ready. It's a slow. I'm waiting for it. Is it a sync? It's very popular. Yeah,
[00:53:32.960 --> 00:53:38.480]   very popular. Well, in my dog one comes, I'll show you all like Google photos. I have to say,
[00:53:38.480 --> 00:53:45.600]   it's it's really good. Surprisingly addictive. Yeah. And it's another roach motel. I mean,
[00:53:45.600 --> 00:53:50.240]   once you have six terabytes of photos in there, you're never downloading those photos. It would
[00:53:50.240 --> 00:53:55.200]   be impossible, but I do upload them also to Amazon. I mean, as long as I've given my information away,
[00:53:55.200 --> 00:54:02.000]   I give them name is on Flickr and Microsoft to just to be fair. Yeah. Yeah. They all have that.
[00:54:02.000 --> 00:54:07.200]   Yeah. You can see me. You can do it. It's lock in. Yeah. Shutter, Shutterfly.
[00:54:07.200 --> 00:54:13.920]   Smug mug. A lot of people have all my photos. You still you smug mug? You still
[00:54:13.920 --> 00:54:19.040]   love smug mug. Really? Yeah, Leo dot camera is my new site. Huh. I haven't seen smug mug and Leo
[00:54:19.040 --> 00:54:23.760]   dot camera. Look at this. A beautiful site with all my pictures on it. Ooh, is that pretty? Yeah.
[00:54:25.040 --> 00:54:34.000]   So, so there's that. So I got that going for me. So you basically spent all your time managing
[00:54:34.000 --> 00:54:38.800]   a photo. Guys, I got my dog movie. Oh, how can we see it? Can you share it with me? And then I
[00:54:38.800 --> 00:54:45.040]   could show it here? Probably hold on with the report at gmail.com. Wait, let me hold on. I want
[00:54:45.040 --> 00:54:52.560]   to make sure there's nothing weird. Like what if like naked animal pictures? Yeah. Sorry. Yeah.
[00:54:52.560 --> 00:54:58.560]   Google, by the way, if you want to do it on the desktop, Google's uploader no longer is it's been
[00:54:58.560 --> 00:55:02.480]   deprecated. You got to use the Google backup and sync tool, which is kind of the universal tool
[00:55:02.480 --> 00:55:07.520]   for everything now. I'm actually use that because I've now my Google drive on my hard drive and it
[00:55:07.520 --> 00:55:12.720]   backs up my photos. That's kind of yeah. All right. This seems to be okay. So let's see save.
[00:55:12.720 --> 00:55:20.960]   I've been share with report. Oh, wait. I'll name her. So, see how easy this is.
[00:55:21.600 --> 00:55:27.840]   I actually share photos all the time now on Google photos. Yeah. Apple's added that feature.
[00:55:27.840 --> 00:55:32.480]   Send a link and stuff. Yeah. It's really simple. All right. Share and I'm sorry, Leo, where are
[00:55:32.480 --> 00:55:36.800]   you? I'm going to report it gmail. I'm going to have a I'm going to do a photo album of this late
[00:55:36.800 --> 00:55:42.720]   relative. I think it'll be a nice it'll be a nice video. Remember them as they are now instead of
[00:55:42.720 --> 00:55:48.640]   as they were. Yeah. So yeah, that car is not picking up on it. It was number two, Carston.
[00:55:48.640 --> 00:55:54.880]   He's not taking the bear. You go. Yeah. Oh, grandma. Grandma. Grandma. Grandma. Grandma. Okay.
[00:55:54.880 --> 00:56:00.160]   That should arrive. Then send me. Okay. So I should get a notification that Stacy
[00:56:00.160 --> 00:56:09.440]   has shared a wonderful movie with me. One of the things that I use Google photos for and so
[00:56:09.440 --> 00:56:13.600]   it's a very, very small, simple feature. But sometimes I like I'll do a newsletter, do Mike's
[00:56:13.600 --> 00:56:19.120]   list newsletter and I want to show multiple pictures. And so I just go in there and I create a collage.
[00:56:19.120 --> 00:56:24.480]   Wait, that's not mine. This is my movie. Oh, okay. I was like, that's not my dog.
[00:56:24.480 --> 00:56:27.200]   It's all the pictures of dogs I've ever taken.
[00:56:27.200 --> 00:56:36.000]   And you recognize these photos. I should have reviewed it.
[00:56:36.720 --> 00:56:43.840]   Well, there's there's Aussie at the quit studios. I like the music. I do too.
[00:56:43.840 --> 00:56:52.800]   Aussie was everywhere in our life. Oh, kind of missing. Look at those ears. Yeah, he had big ears.
[00:56:52.800 --> 00:56:56.480]   There's a sweet doggy and missing. All right. Well, that's cool. Let me see if I can find yours,
[00:56:56.480 --> 00:57:05.840]   though. I don't know. I don't know where yours is. I don't know. Oh, well, the single thing I love
[00:57:05.840 --> 00:57:09.440]   best about Google Photos is I can just take photos like I'll take 10 photos,
[00:57:09.440 --> 00:57:14.960]   and I forget it. And then I get a little movie. Love the burst mode. Yeah. I love the two.
[00:57:14.960 --> 00:57:21.120]   Love the burst mode. I did this at my son jumping up and down in the on the sailboat. Nice.
[00:57:21.120 --> 00:57:26.800]   I don't know. I don't know why I'm not getting it. I don't know. All right. Well, that's all right.
[00:57:26.800 --> 00:57:30.560]   We get the idea. I played mine instead of yours. It's going to space, Leo. Tweet yours.
[00:57:31.520 --> 00:57:37.200]   Who's going to space? The Elon Musk, the movie, the movies in space. Louis. I guess I know it's
[00:57:37.200 --> 00:57:41.440]   going to space. It's going to space. Can you give us a second? Give me a minute. Here I am. I'm
[00:57:41.440 --> 00:57:46.320]   a 2000 miles away from Stacy. And I'm going, where is it? It's a 400 gigabyte movie.
[00:57:46.320 --> 00:57:54.480]   Give it a minute. Give it a minute. Pixel and Nexel's a good reason to buy a Pixel phone or
[00:57:54.480 --> 00:57:58.880]   an excess phone from Google. Their dialer will now just send spam calls right to voicemail.
[00:57:59.840 --> 00:58:06.240]   All the carriers are now doing this. I was working out the other day at the gym and I got a phone
[00:58:06.240 --> 00:58:10.160]   call and I pulled it out and said, "Oh, I got to call. I showed my trainer. I got to call from
[00:58:10.160 --> 00:58:15.600]   spam likely." He said, "Who's that?" He said, "You're just doesn't do that? I guess he's on a
[00:58:15.600 --> 00:58:19.920]   carrier that doesn't." Most carriers now will say spam likely, right? Well, Google takes the next
[00:58:19.920 --> 00:58:23.120]   step. Google takes the next step. They're just going to send it right to voicemail.
[00:58:23.120 --> 00:58:28.320]   I still think there's nothing better than Google Voice, which you have to go in and explicitly
[00:58:28.320 --> 00:58:33.920]   block numbers. But when you block them, it will lie to them. It plays them a "this number is no
[00:58:33.920 --> 00:58:41.360]   longer in service" message, which is fantastic. Hopefully they can automatically detect that and
[00:58:41.360 --> 00:58:47.120]   say, "Okay, we'll take them off the list." All right, now we'll get serious. Does YouTube
[00:58:47.120 --> 00:58:53.280]   violate the Child Online Protection and Privacy Act? Yes. Oh, well, that wasn't that.
[00:58:54.000 --> 00:58:59.760]   Wait a minute. How do they know I was in Petaluma, California? That's amazing. I get this at a lot.
[00:58:59.760 --> 00:59:05.520]   This is the strangest ad and they keep changing the people in it. This unbelievable, tiny company
[00:59:05.520 --> 00:59:10.800]   is disrupting a $200 billion industry in Petaluma. Oh, yeah, I get that all the time.
[00:59:10.800 --> 00:59:15.280]   Yeah, there's an Austin and there's just some random pictures of startup, but they keep changing
[00:59:15.280 --> 00:59:20.640]   the people. They just plug in wherever you are. Yeah. And everybody gets the same news.
[00:59:21.200 --> 00:59:27.200]   It's not a lie. So YouTube on Sunday, a coalition of 20 Child Advocacy,
[00:59:27.200 --> 00:59:32.480]   Consumer Groups filed a complaint with the FTC saying, "Please investigate YouTube
[00:59:32.480 --> 00:59:38.960]   for violations of COPA, Common Sense Media, Center for Digital Democracy, Parents Across America."
[00:59:38.960 --> 00:59:46.480]   Particularly, COPA protects children under 13 from having their data collected.
[00:59:48.480 --> 00:59:52.080]   The crux of the complaint is that those younger kids watching YouTube videos,
[00:59:52.080 --> 00:59:58.880]   even though the company says, "Did you know this? I didn't know this. YouTube's terms of service
[00:59:58.880 --> 01:00:03.840]   forbids under 13-year-olds from watching. Why do they have a children's channel for?
[01:00:03.840 --> 01:00:09.600]   What do they have Peppa the Pig for? How do they verify anyone's age? It's all just a big
[01:00:09.600 --> 01:00:14.480]   charade. So that thing that don't do this if you're 13 is kind of one of those.
[01:00:15.760 --> 01:00:21.200]   That's their acknowledgement of COPA. 12-year-olds are like, "Oh, okay, well, I'm not going to use
[01:00:21.200 --> 01:00:26.960]   YouTube then." What's interesting is the FTC has actually come out and said, is it the FTC,
[01:00:26.960 --> 01:00:32.480]   that they don't view children talking to smart speakers as a violation of COPA.
[01:00:32.480 --> 01:00:41.120]   But YouTube, I would argue since they have a kid's channel, they are pulling IP addresses.
[01:00:41.120 --> 01:00:45.200]   You can say, "Oh, there's a kid in this house based on that." I don't know. It's just kind of a
[01:00:45.840 --> 01:00:50.400]   like, "Oh, why are they drawing the line there? I don't know. What's the legal argument I should
[01:00:50.400 --> 01:00:56.160]   read the case?" Well, what YouTube would have to then do is they'd have to do, as Facebook and others
[01:00:56.160 --> 01:01:03.040]   are supposed to do, age verification. Well, you don't have to have a Google account to watch.
[01:01:03.040 --> 01:01:07.680]   You could just watch, right? Yeah. The problem is that they have
[01:01:07.680 --> 01:01:13.840]   blatantly children-related content that is for kids. And you have a channel called
[01:01:13.840 --> 01:01:19.040]   YouTube Kids. And every single one of the videos on YouTube Kids is available on regular YouTube.
[01:01:19.040 --> 01:01:23.920]   And probably the vast majority of kids under the age of 13 who use YouTube are not going
[01:01:23.920 --> 01:01:26.400]   through YouTube Kids. They're just doing whatever they're supposed to. But they say, "We're doing
[01:01:26.400 --> 01:01:32.160]   YouTube Kids because we're curating the content so you don't see those weird videos."
[01:01:32.160 --> 01:01:36.880]   No, they're doing YouTube Kids. So parents can put their kid in front of YouTube Kids and
[01:01:36.880 --> 01:01:41.040]   trust in it. They're not doing it to actually help kids. But just to be clear.
[01:01:41.040 --> 01:01:45.440]   Wait, it says here, a safer online experience for kids. But it's a problem that I'm not sure
[01:01:45.440 --> 01:01:52.480]   Google can solve because any kids have cell phones when they're five now. It's their own cell phone.
[01:01:52.480 --> 01:01:55.200]   And they have them when they're six and seven and eight and nine, 10, all the way up to 11.
[01:01:55.200 --> 01:01:58.720]   And parents just give them an iPad and say, "Go away. Don't bother me for a couple of hours.
[01:01:58.720 --> 01:02:04.960]   Mommy and Daddy's party time." So they download the YouTube app. They go and they set up an account.
[01:02:04.960 --> 01:02:08.400]   It says, "You have to be 13. They're like, "Yeah, I'm 13." And then they just use YouTube like
[01:02:08.400 --> 01:02:14.480]   everybody else. So we're talking tens of millions of kids do this. And I'm saying, actually, there's
[01:02:14.480 --> 01:02:21.040]   some legitimacy to this complaint. I mean, who's watching? What 13-year-old is watching this?
[01:02:21.040 --> 01:02:32.720]   If YouTube has this on YouTube, there goes the flying lemon. Here comes the scary clowns.
[01:02:32.720 --> 01:02:36.800]   This might actually be PewDiePie's channel. Is this PewDiePie?
[01:02:36.800 --> 01:02:43.360]   No. So what is YouTube going to do in response to this? Are they going to do they...
[01:02:43.360 --> 01:02:48.320]   All they can do is beef up there. I mean, they're not going to have a real names policy or any
[01:02:48.320 --> 01:02:54.400]   sort of verification, right? But they're going to beef up their kids' channel and make it human
[01:02:54.400 --> 01:02:58.960]   curated, which is into weird territory because Google's whole mission in life is to do everything
[01:02:58.960 --> 01:03:05.760]   with machines and not people. But they're going to do the kids' version with human curated...
[01:03:05.760 --> 01:03:12.000]   But they can't last. They can't. But yeah, I saw that. But they're also not supposed to let
[01:03:12.000 --> 01:03:16.800]   people under 13 use YouTube. Exactly. So I don't understand. I mean, even Facebook, which has an
[01:03:16.800 --> 01:03:22.880]   essentially has a real names policy, doesn't... You're supposed to be 13 to use Facebook.
[01:03:22.880 --> 01:03:26.160]   But my kids used it when they were... Absolutely. They all do. They all do.
[01:03:26.160 --> 01:03:28.720]   All right. Let's do it. Yeah, my child is on it all the time and she's 11.
[01:03:28.720 --> 01:03:34.320]   Stacy, what kind of mother are you? Oh, I know what she's watching.
[01:03:35.680 --> 01:03:38.960]   That kind of mother... YouTube's still gathering information about her.
[01:03:38.960 --> 01:03:41.360]   They are, but you know what? Not a lot of information.
[01:03:41.360 --> 01:03:47.920]   They've got a lot of information. Yeah. Let's take a break. Our show today brought to you by
[01:03:47.920 --> 01:03:54.320]   Rocket Mortgage. I can't wait to the day when I say my house brought to you by Rocket Mortgage.
[01:03:54.320 --> 01:03:58.800]   Not because they're good by my house, but because I will get my mortgage through Rocket Mortgage.
[01:03:58.800 --> 01:04:02.480]   You know, the house we live in right now, we bought about four years ago, and we got the
[01:04:02.480 --> 01:04:08.480]   mortgage as most people did until recently from a bank. You go to the bank, you fill out a form,
[01:04:08.480 --> 01:04:12.800]   you get 20 pages, you get... You could go on and on and you fill it all out. You have to go
[01:04:12.800 --> 01:04:17.280]   do research, find out what you're... You know, how much money... Where you lived 10 years ago,
[01:04:17.280 --> 01:04:20.080]   how much money you made, 18 years... All that stuff. It's like going on Facebook.
[01:04:20.080 --> 01:04:23.280]   It's like going on... Actually, you know, Facebook would be really handy if they did this.
[01:04:23.280 --> 01:04:27.280]   I wouldn't have to fill anything out at all. Yeah. No, this is better than that.
[01:04:27.920 --> 01:04:33.600]   This is better than that. You also, by the way, by the way, that takes a long time. When we
[01:04:33.600 --> 01:04:37.360]   bought our house four years ago, it took us more than a month. We thought, "Oh, we'll be done. We're
[01:04:37.360 --> 01:04:41.840]   going to vacation a month. We'll be done by then." We were faxing. First of all, good luck finding
[01:04:41.840 --> 01:04:47.680]   a fax machine. We were faxing stuff back to the bank from a cruise ship. We had to call Lisa's
[01:04:47.680 --> 01:04:52.640]   sister Debbie and said, "Could you send this... They want more documents. We almost lost the house.
[01:04:52.640 --> 01:04:58.640]   Fast forward. Quick and loans. Number one lender in the country. Number one in customer satisfaction
[01:04:58.640 --> 01:05:02.720]   year after year, but also number one in volume, has created something that's completely online
[01:05:02.720 --> 01:05:09.600]   that will take you less than 10 minutes in most cases to get a home loan. It's rocket mortgage
[01:05:09.600 --> 01:05:14.160]   at rocketmortgage.com/twig. You don't have to get dressed to go to the bank. You could do it from
[01:05:14.160 --> 01:05:17.760]   your couch. You could do it from an open house. You answer a few questions. You already know the
[01:05:17.760 --> 01:05:22.640]   answers to. No research involved. They have relationships with all the financial institutions.
[01:05:22.640 --> 01:05:28.240]   You can share that information with them just by saying, "Yeah, go ahead and get my bank statement
[01:05:28.240 --> 01:05:33.280]   or whatever." They crunch the numbers. Because it's all computerized, it's all fast as all online,
[01:05:33.280 --> 01:05:38.160]   literally in just a few minutes based on your income, your assets, and your credit,
[01:05:38.160 --> 01:05:42.800]   they'll analyze all the home loan options for which you qualify and find the one that's right.
[01:05:42.800 --> 01:05:49.360]   For you, you pick the down payment, the term, the rate. They do everything else fast. You could do
[01:05:49.360 --> 01:05:56.160]   it so fast. You could be at an open house, say, "Hey, honey, we should buy this." Okay,
[01:05:56.160 --> 01:05:59.200]   let's get that loan. Go ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch ch
[01:05:59.200 --> 01:06:03.360]   and before you leave, show the realtor we're approved. We'd like to make an offer. No faxing?
[01:06:03.360 --> 01:06:09.360]   No faxing necessary. Rocket mortgage by quick and loans. Apply, simply, understand fully and
[01:06:09.360 --> 01:06:16.000]   mortgage confidently. Get started. Go to rocketmortgage.com/twig. Actually, the best way to do this is go now,
[01:06:16.000 --> 01:06:20.240]   even before you want to buy a house and just get the account set up and all that. That'll make
[01:06:20.240 --> 01:06:30.080]   it even faster. Rocketmortgage.com/twig=housing-lender-licensed-in-all-fifty-states-en-mls-consumer-access.org.
[01:06:30.080 --> 01:06:36.400]   Number 30. Rocketmortgage.com/twig. We thank them so much for their support of this week in
[01:06:37.120 --> 01:06:42.560]   Google. We're calling Kevin Marks back. Going to get him. See if he's better on a 4G. I bet he's not.
[01:06:42.560 --> 01:06:51.360]   But who knows? We shall see. New Chromebook from HP, the Chromebook X2, which looks a lot like my
[01:06:51.360 --> 01:06:58.400]   Windows 10 NVX2. I bet you it's the same hardware. The pen looks the same, the keyboard looks the
[01:06:58.400 --> 01:07:05.920]   same. It's got the same B&O sound. But unlike the not so great Windows on ARM, that my $1000
[01:07:06.640 --> 01:07:14.560]   NVX2 is running this $500 Chromebook X2 is running Chrome OS, which means it probably runs a lot faster.
[01:07:14.560 --> 01:07:21.520]   Mine has a Snapdragon 835 in it. That's plenty of speed to run Chromebook. Two gigs of RAM, 12.3-inch
[01:07:21.520 --> 01:07:27.120]   screen. $599, I'm sorry, I gave you the wrong price. $599, basically, configuration, which competes
[01:07:27.120 --> 01:07:33.680]   very favorably with an iPad Pro. You get the key and it includes the keyboard in the pen.
[01:07:34.960 --> 01:07:41.040]   I think this is a new form factor for Chrome OS. I like the 2-in-1.
[01:07:41.040 --> 01:07:46.560]   Certainly customers with the Surface Pro and so forth have shown that they're interested in this.
[01:07:46.560 --> 01:07:51.200]   Yeah, it seems like a great device and it's great to see the great devices. In the old days,
[01:07:51.200 --> 01:07:57.120]   you had one or two super high-end devices and everything else was a piece of junk.
[01:07:57.120 --> 01:07:59.440]   This is how people first were introduced to Chromebook.
[01:07:59.440 --> 01:08:00.240]   Yeah, it's too bad.
[01:08:00.240 --> 01:08:04.640]   And then nowadays you have a device like this that's significantly cheaper than your average
[01:08:04.640 --> 01:08:11.280]   smartphone, high-end smartphone, and it's super high-end, very high quality. So that's exciting.
[01:08:11.280 --> 01:08:18.720]   And of course, the more stories come out about privacy violations, about security violations,
[01:08:18.720 --> 01:08:22.320]   if you listen to security now, you will be frightened into buying one of these things.
[01:08:22.320 --> 01:08:30.160]   Yeah, because I tell everybody who asks, if you don't have a compelling reason to get a window,
[01:08:30.160 --> 01:08:34.240]   or Mac PC, if you're not designing rockets or editing video or something like that, get a
[01:08:34.240 --> 01:08:38.400]   Chromebook. It's what most people... I bet 90% of people.
[01:08:38.400 --> 01:08:41.360]   That's what my daughter's next computer is going to be.
[01:08:41.360 --> 01:08:44.480]   My daughter loves she's 26 and college, she uses a Chromebook.
[01:08:44.480 --> 01:08:49.120]   Oh yeah, well, my daughter uses a MacBook Air that I got from GigaOM.
[01:08:49.120 --> 01:08:50.960]   To oversimplify the...
[01:08:50.960 --> 01:08:51.440]   It's coming on time.
[01:08:51.440 --> 01:08:55.440]   You don't know what a computing platform is going to be like.
[01:08:55.440 --> 01:08:58.720]   The way it feels is everything. For example, this gigantic thing that you
[01:08:58.720 --> 01:08:59.920]   use to love this surface to the...
[01:08:59.920 --> 01:09:01.920]   It's the way it feels. That's the important thing.
[01:09:01.920 --> 01:09:04.480]   And what people... Here's the biggest misconception about Chromebooks.
[01:09:04.480 --> 01:09:07.120]   People haven't used it. They think it's going to feel bad to use.
[01:09:07.120 --> 01:09:10.880]   The opposite is true. The knowledge that it's all in the cloud, that you just log in,
[01:09:10.880 --> 01:09:13.440]   you can log in to any other device. It's the same log in and same...
[01:09:13.440 --> 01:09:15.040]   You don't feel disadvantaged most of the time.
[01:09:15.040 --> 01:09:19.840]   You don't. But you feel liberated by the fact that you don't have apps to manage,
[01:09:19.840 --> 01:09:23.120]   you don't have all this data that you're not sure what's on there.
[01:09:23.120 --> 01:09:28.480]   By being backed up, this kind of thing. It's all in the cloud, and it feels really good
[01:09:28.480 --> 01:09:31.760]   to use a Chromebook. We've got Kevin Marks back.
[01:09:31.760 --> 01:09:34.160]   Let's see how it sounds now, Kevin. You're on your phone?
[01:09:34.160 --> 01:09:39.200]   I have on my phone now, yes. So let's see if that works any better.
[01:09:39.200 --> 01:09:41.520]   It's a little bit of latency, but it sounds better. So that's okay.
[01:09:41.520 --> 01:09:45.520]   Are you on an Android or an iPhone?
[01:09:45.520 --> 01:09:47.280]   I'm sorry. I'm seeing the Android copy down a bit.
[01:09:47.280 --> 01:09:48.240]   Yeah.
[01:09:48.240 --> 01:09:49.280]   An Android, of course.
[01:09:49.280 --> 01:09:57.280]   Of course. Well, you're not a teenager. According to Piper Jeffrey's
[01:09:57.280 --> 01:10:05.520]   teen survey, 82% of teens own an iPhone in the US. 82%. By the way, they ask thousands of
[01:10:05.520 --> 01:10:11.360]   kids across 40 states average age 16. That's up from 78% last fall, up 4%.
[01:10:11.360 --> 01:10:15.760]   It's the highest percentage of iPhone ownership among teens they've seen ever.
[01:10:15.760 --> 01:10:16.880]   And...
[01:10:16.880 --> 01:10:22.640]   Good as for Apple, 84% of teens say their next phone will be an iPhone.
[01:10:22.640 --> 01:10:25.600]   So it's going to continue to grow. That is dominant.
[01:10:25.600 --> 01:10:26.320]   Yeah.
[01:10:26.320 --> 01:10:30.240]   That is crazy. Although it makes sense. I mean, they're easy to use.
[01:10:30.240 --> 01:10:36.880]   Probably a lot of kids, there's a lot better parental and safety controls on an iPhone than
[01:10:36.880 --> 01:10:45.760]   Android. And they're more secure. So as a parent, you're just like here. And there you have it.
[01:10:45.760 --> 01:10:50.160]   There's also a hue. And I see this with my kids who are not teens admittedly, but there's a huge
[01:10:50.160 --> 01:10:52.320]   pure thing going on.
[01:10:52.320 --> 01:10:56.480]   Right. Brandon, that's true. It's like having the right genes or whatever.
[01:10:56.480 --> 01:11:02.480]   And there's even some practical things because Apple's messaging program works best with other
[01:11:02.480 --> 01:11:06.720]   Apple phones. And that's a big deal for them. Yeah. And you use all the stickers and stuff in
[01:11:06.720 --> 01:11:11.520]   iMessage or messages is one thing. Some of the new stuff, I mean, not a whole lot of these
[01:11:11.520 --> 01:11:17.840]   teenagers have iPhone 10s, but things like the Animojis and stuff like that are going to be
[01:11:19.040 --> 01:11:24.080]   popular. But yeah, it's a status thing. And again, this is a population of people who mostly don't
[01:11:24.080 --> 01:11:30.400]   have jobs. Yeah. Well, I was going to say a lot of people give them their old phones. And what I
[01:11:30.400 --> 01:11:35.280]   think will be really interesting because I was listening to some kids talk about the iPhone 10
[01:11:35.280 --> 01:11:41.200]   and they were like, Oh, it's terrible. And the best iPhone was the iPhone six, which one took
[01:11:41.200 --> 01:11:45.840]   off the phone jack or the audio jack. I think the success was the first, maybe it was the six.
[01:11:45.840 --> 01:11:49.600]   No, the six has a jack, but the next one doesn't. There wasn't a seven, right?
[01:11:49.600 --> 01:11:53.680]   It's a seven. Okay. Yeah. There was a seven. We skipped something. I don't know.
[01:11:53.680 --> 01:11:58.320]   And so they were talking. They were like, Oh, I hate it because I can't use my headphones.
[01:11:58.320 --> 01:12:01.360]   And the other ones are so expensive. So my parents won't buy them for me.
[01:12:01.360 --> 01:12:05.600]   Oh, interesting. So I do wonder like maybe that'll change.
[01:12:05.600 --> 01:12:10.560]   If that changes things as people are like looking at the cost, I mean, yes, you can buy. Of course,
[01:12:10.560 --> 01:12:15.760]   they have Beats headphones. Maybe not. I don't know. I'm not spending a thousand dollars to give
[01:12:15.760 --> 01:12:23.280]   my child an easily lost, you know, phone plus your. Have you used the echo announcement feature yet
[01:12:23.280 --> 01:12:30.000]   to call your daughter to dinner? We use the broadcast feature, which is you can say,
[01:12:30.000 --> 01:12:37.200]   echo, announce it's time for dinner or tell everyone it's time for dinner.
[01:12:37.200 --> 01:12:43.680]   And everyone with with an echo will hear. Oh, yeah. Okay. You could do that with Google Home
[01:12:43.680 --> 01:12:49.360]   under broadcast with the echo. We just call and we hang out. He's dropping. Yeah, you just drop
[01:12:49.360 --> 01:12:56.160]   in. Thank you. Yeah. So this is a drop into every echo in the house. It's inevitable.
[01:12:56.160 --> 01:13:01.920]   It's not a drop in announcement. Yeah. Yeah. The syntax would be echo announced that.
[01:13:01.920 --> 01:13:04.960]   It's time for me. And they all have to be on the same Wi-Fi network. Great.
[01:13:04.960 --> 01:13:10.160]   Is that how it's decided? No, I don't know if that's true. I think that you when you set up the
[01:13:10.160 --> 01:13:17.280]   echo, because I have in my when I open my A word app, it shows the echoes that are here and at home.
[01:13:17.280 --> 01:13:20.720]   That's it. And I think I could add them to a group. It'd be interesting if I announced it was
[01:13:20.720 --> 01:13:25.760]   time for dinner and everybody at the studio came over. I wonder how many households have both multiple
[01:13:25.760 --> 01:13:31.760]   echoes and multiple Wi-Fi. It's probably less common than you think. I do.
[01:13:31.760 --> 01:13:37.360]   It's actually really confusing on sodas and other things because if you're not on the same
[01:13:37.360 --> 01:13:38.720]   network, you can't see it. Yeah.
[01:13:38.720 --> 01:13:46.000]   Yeah. It's a you know, the features are just going to keep coming on these devices. It's
[01:13:46.000 --> 01:13:50.240]   such a wonderful thing. And I can't wait for the business and enterprise versions of these.
[01:13:50.240 --> 01:13:53.920]   It's going to be amazing. Imagine, for example, you have. Wait a minute. You think they'll be echoes
[01:13:53.920 --> 01:13:59.200]   in business? Totally. I think I think that the the slam down and I hope that Amazon is working on
[01:13:59.200 --> 01:14:03.840]   it. So they have these like desktop phones that have benefits on cell phones and they're kind of
[01:14:03.840 --> 01:14:08.320]   going out of style because everybody wants to use cell phones. But imagine a device that is like
[01:14:08.320 --> 01:14:15.440]   a landline phone that has a screen and there's also an echo and there's also for conference calls.
[01:14:15.440 --> 01:14:18.720]   Because people do do conference calls. They put a speaker and microphone in the middle of the table.
[01:14:18.720 --> 01:14:23.920]   People do the old days. I don't know if they still have this, but I saw this on TV the other day.
[01:14:23.920 --> 01:14:30.960]   I think it was on curb your enthusiasm from 2002, where the secretary of the receptionist
[01:14:31.600 --> 01:14:38.800]   would press the button and say, "Mr. LaPort, you're three o'clock here." And it would just appear in
[01:14:38.800 --> 01:14:42.720]   your... You would just hear it and then you would talk back and say, "Okay, I'll be right out."
[01:14:42.720 --> 01:14:49.040]   And you wouldn't have to do anything. I saw that on TV. I thought, "I want that." That was 20 years ago.
[01:14:49.040 --> 01:14:54.400]   Okay. So Amazon has actually said that they're going into business. Bernard,
[01:14:54.400 --> 01:15:00.080]   actually earlier this year, talked about Amazon for business. So, or Madam A for business rather.
[01:15:00.080 --> 01:15:03.840]   So, the big challenge there, there's two big challenges and they're going to allow
[01:15:03.840 --> 01:15:08.640]   customized skills. So, if you're IBM, you can actually build IBM focus skills for your workforce.
[01:15:08.640 --> 01:15:17.360]   The question is, without credible biometrics, so credible voice recognition, which we don't have
[01:15:17.360 --> 01:15:23.280]   yet, how do you do things like not Sysadman, Microsoft's admin, what's it called?
[01:15:23.280 --> 01:15:28.400]   They're active directory. Sorry, they're active directory things. So, how do you do...
[01:15:28.400 --> 01:15:30.880]   How do you make sure someone doesn't want it? You're right.
[01:15:30.880 --> 01:15:35.680]   You're right. That's why you need to screen and it needs to be a video conference system anyway.
[01:15:35.680 --> 01:15:41.440]   Okay. We'll see. And that is the question. But a lot of these on the Amazon page are just dots and
[01:15:41.440 --> 01:15:47.920]   echoes. Here's an echo show on her desk. But here's the conference room and this is their new
[01:15:47.920 --> 01:15:52.800]   conference phone is an echo dot. But that's a lot cheaper than one of those fancy ones.
[01:15:53.360 --> 01:15:59.280]   A dot with a fire TV. Yeah. Yeah. Or not a fire TV. You still need a camera for the people, I guess.
[01:15:59.280 --> 01:16:04.320]   Nevermind. I love... I just re-empt. You don't have enough microphones for that to work?
[01:16:04.320 --> 01:16:07.600]   Yeah. I think it's a Ray microphones are every bit as good as the
[01:16:07.600 --> 01:16:11.840]   crazy octopus that we have in our conference room. What is that called?
[01:16:11.840 --> 01:16:15.840]   The polycom. Polycom. That's the default right everywhere. It is.
[01:16:15.840 --> 01:16:22.160]   You can see why Jeff Bezos, my good buddy Jeff, would like a little chunk of that business.
[01:16:22.160 --> 01:16:27.760]   But this business is very ripe for disruption. Imagine if Amazon could get in every office,
[01:16:27.760 --> 01:16:32.720]   every conference room in the world that would be a little extra business.
[01:16:32.720 --> 01:16:38.400]   Could you give something like this? Well, didn't they? They did hang out hardware.
[01:16:38.400 --> 01:16:41.360]   Yeah. They did. For $5,000.
[01:16:41.360 --> 01:16:47.520]   It wasn't a pliancy. So the thing with these is that their appliances, you plunk them down and
[01:16:47.520 --> 01:16:52.560]   they sort of... When IT hates appliances. Yes, they do. IT is going to look at this.
[01:16:52.560 --> 01:16:55.440]   IT hates almost everything, new thing that is to mount.
[01:16:55.440 --> 01:16:58.400]   But to answer your question, Stacy, it looks like it does integrate with
[01:16:58.400 --> 01:17:06.400]   the business systems. Here's from Amazon's page on this. A word for business. I want to call it A word.
[01:17:06.400 --> 01:17:10.640]   A word for business uses information about the devices, user counts, and skills in your
[01:17:10.640 --> 01:17:15.760]   organization. When someone in your organization asks A word in a question, A word uses this
[01:17:15.760 --> 01:17:20.160]   information to respond or perform to the requested action. For example, when a user says,
[01:17:20.160 --> 01:17:26.240]   "Echo, start the meeting in a conference room." Echo uses the location of the device, the calendar
[01:17:26.240 --> 01:17:30.080]   information for the room, and the type of video conferencing equipment available,
[01:17:30.080 --> 01:17:36.240]   all stored in your A for business account to start the meeting. So starting a meeting is different.
[01:17:36.240 --> 01:17:40.560]   I think starting a meeting is a killer use case for this sort of thing. What I'm curious about,
[01:17:40.560 --> 01:17:44.480]   what I've talked to people about their companies in the database world, who are actually integrating
[01:17:44.480 --> 01:17:54.080]   with Madamay, and what you can say. What were our amia sales for Q2 2016? But that kind of data
[01:17:54.080 --> 01:18:00.480]   should not be. If I can walk into any conference room and ask that kind of data, that's not cool.
[01:18:00.480 --> 01:18:03.840]   But they have voice recognition, so could you conceive what we have authentication?
[01:18:03.840 --> 01:18:10.320]   But it's not good enough for authentication yet. My echoes don't know who I am. I am still very
[01:18:10.320 --> 01:18:16.880]   upset about this. Yeah. I mean, not to be anywhere. Thank you. They're just like,
[01:18:16.880 --> 01:18:20.720]   I don't know, but you're on Stacey's account. Stacey, we feel like we don't know you anymore.
[01:18:20.720 --> 01:18:25.360]   We've all gathered together and want to know that we love you. But they already have pricing.
[01:18:25.360 --> 01:18:28.560]   It's because you went and signed into Facebook, you see. Yes, if you would just use Facebook.
[01:18:28.560 --> 01:18:35.120]   That's, by the way, as jokey as that is, that's absolutely going to. It's already happening.
[01:18:35.120 --> 01:18:38.080]   Peck, when I got rid of my Facebook account, people said, "Well, how are you going to want? What
[01:18:38.080 --> 01:18:42.800]   happens to all the things you use Facebook to log into with?" Right. Oh, you do that? Don't do that.
[01:18:42.800 --> 01:18:48.160]   That's giving all of that data to them. What are you? I don't do it anymore, obviously.
[01:18:48.160 --> 01:18:50.000]   What did I call you when you bought the wrong camera?
[01:18:50.000 --> 01:18:56.160]   Adoopus? Yeah, don't be a doofus Leo. Don't be a goofus, be a gallant.
[01:18:56.160 --> 01:19:03.120]   Yeah, no, I, well, because it's convenient because you, you know, now I use Google. So that's not
[01:19:03.120 --> 01:19:07.520]   any better, I suppose. Okay. See, convenience. That's a single sign-off. That's why we have
[01:19:07.520 --> 01:19:13.920]   no privacy. Remember, what was that Kevin Marks at Microsoft Passport, right? Microsoft Passport.
[01:19:13.920 --> 01:19:17.760]   They started, that was years ago. Passport, yeah. Didn't go anywhere. I don't want it.
[01:19:17.760 --> 01:19:22.240]   Does an Apple have one of those two? I never see it anywhere. And now they can't,
[01:19:22.240 --> 01:19:25.600]   because they've decided they're going to be the privacy masters. Yeah.
[01:19:25.600 --> 01:19:28.480]   They didn't think Apple does anymore. Yeah. Okay.
[01:19:28.480 --> 01:19:32.800]   I try to remember that there was something a long time ago, but yeah, it would never took
[01:19:32.800 --> 01:19:36.560]   off in the same way. Actually, Microsoft, they didn't really pick up the whole thing Apple.
[01:19:36.560 --> 01:19:42.080]   Microsoft kind of has it down because Microsoft will do the push to your phone
[01:19:42.080 --> 01:19:46.560]   notification now. Yeah. I don't even often have to give Microsoft my password. It just says,
[01:19:46.560 --> 01:19:52.960]   what's your account? I say Leo. And then they say, okay, we're going to push something to your phone.
[01:19:52.960 --> 01:19:57.520]   And then you get a two digit number. It says, which of these four two digit numbers did your
[01:19:57.520 --> 01:20:01.840]   phone say? Yeah. And that's it. You don't have even to do the password. It's really convenient.
[01:20:01.840 --> 01:20:04.240]   And yeah, all you have to do is put the Microsoft authenticator on your phone.
[01:20:05.280 --> 01:20:08.880]   But then I have this definitely fear of losing my phone and everything's in here now.
[01:20:08.880 --> 01:20:12.320]   Yeah. That's a slow problem. So the way I saw it was
[01:20:12.320 --> 01:20:17.280]   indicators on my phone now, I'm like, I put a thousand dollar bill in my phone. So that way,
[01:20:17.280 --> 01:20:20.000]   I won't lose it anymore. Yeah, that's a great plan, Leo.
[01:20:20.000 --> 01:20:27.840]   I probably should have announced that either. My brain actually tried to like,
[01:20:27.840 --> 01:20:29.600]   that wasn't smart. And then I was like, Nope.
[01:20:32.960 --> 01:20:36.800]   That way, I will. I care so much. I won't lose Bitcoin on your phone. Yeah.
[01:20:36.800 --> 01:20:42.320]   Yeah. Well, that's that's there too. Seven dollars a device for shared devices,
[01:20:42.320 --> 01:20:50.000]   seven dollars a month for shared devices, three dollars per user. So enrolled users can use an
[01:20:50.000 --> 01:20:56.160]   unlimited number of personal devices. I think this does this only make sense if your company
[01:20:56.160 --> 01:21:05.600]   has the horsepower to write its own echo tasks. No, no, it's not hard to do a skill. A lot of people
[01:21:05.600 --> 01:21:12.240]   are on skills, but even without skills. I mean, as with so many things, the overwhelming use for
[01:21:12.240 --> 01:21:18.160]   almost anything is pretty banal uses. I mean, people are calling people, leaving voicemail messages,
[01:21:18.160 --> 01:21:23.040]   having conference calls, stuff, people are doing simple stuff. Yeah. And so the skills are just
[01:21:23.840 --> 01:21:28.400]   icing on the cake. Very important for the biggest companies, of course. But but I think that I
[01:21:28.400 --> 01:21:32.560]   think there's a real place for this. Have you ever written any skills? Stacy has Kevin? Is he
[01:21:32.560 --> 01:21:37.600]   I know he was having a certain skills. Yeah. My Kevin my Kevin has written skills too.
[01:21:37.600 --> 01:21:41.840]   Oh, I bet you while your Kevin's a serious professional in the business. Yes. And he's he's
[01:21:41.840 --> 01:21:48.480]   he's playing without a lot and is it Python? He was with he was with first he was with Tinker,
[01:21:48.480 --> 01:21:54.080]   then he was with Piper. Yeah. And now he's he's bootstrapping his own startup. Good for him. I
[01:21:54.080 --> 01:21:57.920]   didn't know that. Yeah. Yeah. Also in the education space, Tinker and Piper were awesome. Yeah, they
[01:21:57.920 --> 01:22:02.240]   were. Tinker still is around. It's the app that teaches kids how to program on the iPad. That's right.
[01:22:02.240 --> 01:22:06.560]   And Piper was to build your own computer, which is really cool. And these companies are all doing
[01:22:06.560 --> 01:22:12.000]   very well. But yeah, he's he's got some great ideas and he's no, but I wasn't asking about
[01:22:12.000 --> 01:22:17.680]   that. I was asking what language it was. It was Python the language. I think he's using Python. Yes.
[01:22:17.680 --> 01:22:23.600]   No, Kevin's a Python. He's stuck. Okay. Kevin, Mark, have you done any? There's so many
[01:22:23.600 --> 01:22:31.920]   Kevin's have you done or Madam a skill? I know. I have not though. My son is working at a games
[01:22:31.920 --> 01:22:38.080]   company that's building a game for Amazon. So is his name Kevin too? So I'm learning about it
[01:22:38.080 --> 01:22:43.040]   directly through his name is Andrew. Oh, okay. So my son, I really, so it's the company sensible
[01:22:43.040 --> 01:22:48.880]   object that does piece of balance. But they're also working on games with voice now, which is
[01:22:48.880 --> 01:22:53.760]   interesting. Yeah, I'm really kind of intrigued. I feel like I should write a twit skill of some kind.
[01:22:53.760 --> 01:22:58.640]   I want to write a hotter, colder game. You could hide like a little bit. I guess you'd need a
[01:22:58.640 --> 01:23:04.000]   Zigbee thing for the Amazon. You hide it and then you hold her or she would play it. And then
[01:23:04.000 --> 01:23:11.520]   Madam a kids in her prissy voice. Say you're getting colder. You're getting colder. My grandpa used to
[01:23:11.520 --> 01:23:17.040]   play that with me because he was not a very mobile person. So he would just hide it and we would do
[01:23:17.040 --> 01:23:21.760]   that for hours. It's kind of a grandpa game, actually. It really is. You know, it would be a great
[01:23:21.760 --> 01:23:26.560]   twit skill would be just, you know, just to have it, because you had this whole network with all
[01:23:26.560 --> 01:23:32.320]   these things, I subscribed to the Everything Firehose. Yeah. And that would be that would be an
[01:23:32.320 --> 01:23:37.040]   awesome thing because the way I use it is I want to listen to the most recent show. And then I want
[01:23:37.040 --> 01:23:40.960]   to go to the second most recent and that's how I do it. I use Overcast for that. But on on the
[01:23:40.960 --> 01:23:46.240]   other side, it's only the most recent show now on the AECO. But if you had a special
[01:23:46.240 --> 01:23:51.120]   skill, listen to this number. Show number. Or you know, it's just a way to listen to all the
[01:23:51.120 --> 01:23:55.440]   Progopel and Japan. That'd be killer. That'd be so much better than what we have now. I need
[01:23:55.440 --> 01:24:03.200]   something to do while I'm on vacation. Here's an example of how out of control the friend-to-friend
[01:24:03.200 --> 01:24:08.560]   thing was. And this is we're going back to Facebook now that we've had the refresher.
[01:24:09.360 --> 01:24:17.440]   But I was really amazed by this. This is in New Zealand. So 10 people downloaded a quiz in New
[01:24:17.440 --> 01:24:26.800]   Zealand and Cambridge Analytica got 63,714 New Zealanders because everybody in New Zealand knows
[01:24:26.800 --> 01:24:35.360]   everybody else. But that implies 6,307 followers per person. Or person. I mean, that's quite a lot.
[01:24:36.720 --> 01:24:41.840]   And in New Zealand. So that's presumably New Zealanders have friends who are outside of New
[01:24:41.840 --> 01:24:46.320]   Zealand as well. So yeah, they're very social people, these New Zealanders. Maybe that's it.
[01:24:46.320 --> 01:24:57.200]   Maybe that's the secret. Or are they gathering friends of friends? How many? How far does it go?
[01:24:57.200 --> 01:25:02.080]   It's just one degree. Is it? One degree. Wow. Now that's no longer the case.
[01:25:03.280 --> 01:25:07.040]   But still, this comes from. That's way beyond Dunbar's number. Yeah.
[01:25:07.040 --> 01:25:15.120]   Yeah. And then what's the thing with the un-sending? So how many does Zinca have? Wow.
[01:25:15.120 --> 01:25:18.800]   Zinca. Yeah, right. Farm fill. Zinca's probably got the entire network.
[01:25:18.800 --> 01:25:21.200]   They know everybody. They have it all. They have the whole network.
[01:25:21.200 --> 01:25:27.200]   Two billion people. Oh, words with friends. That's how my data propped. That's the last thing I
[01:25:27.200 --> 01:25:37.680]   remember doing on Facebook. Yep. Wow. Incredible. It's all out there. Your information is out there.
[01:25:37.680 --> 01:25:42.960]   You get much more thinkers on it. I'll skip. I think that's a great idea. He doesn't have a
[01:25:42.960 --> 01:25:50.080]   dog in this hunt anymore. He's like, you know, he could tell us the truth. Any VCing? Yeah.
[01:25:50.640 --> 01:25:57.520]   You can say what's really going on. Not this VC. Vivo's YouTube account got hacked and the most
[01:25:57.520 --> 01:26:04.480]   popular thought the most watched YouTube video of all time, Despacito, disappeared after being
[01:26:04.480 --> 01:26:11.120]   defaced by hackers. The video's image was altered and replaced with a masked gang holding guns.
[01:26:11.120 --> 01:26:17.040]   And the description was changed by hackers calling themselves ProSox and Coroish.
[01:26:18.800 --> 01:26:22.080]   And nobody noticed. Everybody thought Despacito was just really cool.
[01:26:22.080 --> 01:26:28.400]   Some of the... Look at all the Taylor Swift, Katie Perry, Drake, Selena Gomez, some of these
[01:26:28.400 --> 01:26:35.920]   videos are still up with defaced videos and thumbnail images. They were all uploaded directly
[01:26:35.920 --> 01:26:44.320]   to the artist's own Vivo YouTube accounts. So don't know. Don't know. And I don't know if they've
[01:26:44.320 --> 01:26:51.760]   figured out how it happened, but that's not good. There's what Despacito looked like.
[01:26:51.760 --> 01:26:56.960]   I wonder if the music was the same and just the picture was... Please don't play it. Please don't
[01:26:56.960 --> 01:27:01.520]   play it. Please don't play it. You love Despacito. Admit it. It's always in my head. Just don't
[01:27:01.520 --> 01:27:07.440]   start. I know. It's hard not to... Yes. That's why it's a earworm. I can't get rid of it. I think it's
[01:27:07.440 --> 01:27:16.400]   something else not an earworm. Nothing like that. What else do we get on top of that?
[01:27:16.400 --> 01:27:23.600]   Fast Company says this picture of Zuck's notes is more revealing than the entire Senate hearing
[01:27:23.600 --> 01:27:30.160]   because you can read the notes. She was prepared to snap back and have all the stuff.
[01:27:30.160 --> 01:27:33.760]   The notes include their own version of the infamous "Do Not Congratulate Putin Memo"
[01:27:34.960 --> 01:27:41.120]   for President Trump. It says, "Do not say we already do what GDPR requires." Do not... Mark
[01:27:41.120 --> 01:27:49.440]   whatever... Do not say we do what GDPR requires. Does it say I hear you on there?
[01:27:49.440 --> 01:27:57.840]   I hear you. It says refer question to your team. That is an awfully clear image that
[01:27:57.840 --> 01:28:04.400]   you can really read these. Well, cameras are amazing. You can read them with the coins.
[01:28:05.200 --> 01:28:11.920]   Yes. He left it out when he went to the bathroom. Boom. Does it say what hotel he's staying at?
[01:28:11.920 --> 01:28:21.360]   I'm looking. This is a good note. GDPR. Don't say we already do what GDPR requires. Don't say that, Mark.
[01:28:21.360 --> 01:28:27.920]   We were told he was training over the weekend with surrogates and practicing.
[01:28:27.920 --> 01:28:31.200]   Running a team of people to make them seem like he was human. They taught him how to blink.
[01:28:33.440 --> 01:28:38.960]   That is Mark of course. Oh, I know. I would do it too. The minute they asked me to testify,
[01:28:38.960 --> 01:28:43.520]   I'm going to get a team in here. But he's especially robotic. He's got an awkwardness about him.
[01:28:43.520 --> 01:28:50.880]   I... This is probably politically incorrect to say, but looking at this, it feels like he
[01:28:50.880 --> 01:28:54.960]   probably is on the spectrum, actually, that he is maybe a little Asperger's or something.
[01:28:54.960 --> 01:29:01.040]   Something. It's not really clear what's going on with him. He's a classic programmer introvert.
[01:29:01.040 --> 01:29:06.800]   Yes. He's a nerd of the Ants degree. Kevin, you've... As somebody who's run many teams, I mean,
[01:29:06.800 --> 01:29:13.600]   this is not unusual to meet people like that, but it's unusual to see them having full control
[01:29:13.600 --> 01:29:18.720]   of a two billion person customer company. Biggest source of information in the history of mankind.
[01:29:18.720 --> 01:29:22.800]   That's what said you were there. But it kind of came home to me because now I understand why he
[01:29:22.800 --> 01:29:26.640]   has this disconnect. He keeps saying we want people to connect. He doesn't understand why
[01:29:26.640 --> 01:29:29.280]   people are bothered. He doesn't understand it at all. Exactly.
[01:29:29.840 --> 01:29:33.680]   I think that's disingenuous because he does understand it. He keeps his data private.
[01:29:33.680 --> 01:29:40.000]   That's... I mean, if he were out living the same life and saying the same things, fine. But he is...
[01:29:40.000 --> 01:29:43.600]   He bought every house around his house so that nobody would look at the windows.
[01:29:43.600 --> 01:29:47.600]   That is actually not uncommon with the crazy rich.
[01:29:47.600 --> 01:29:52.560]   If I had the money, I'd do it. I've often fantasized about it.
[01:29:52.560 --> 01:29:54.400]   Oh, I like my neighbors.
[01:29:54.400 --> 01:29:56.640]   No, I move you all in. He's quick and long.
[01:29:56.640 --> 01:29:58.320]   I make it the Twit, call the sack.
[01:29:59.200 --> 01:30:04.640]   Anyway, your thoughts, Kevin, I asked you a question and I didn't give you a chance to answer.
[01:30:04.640 --> 01:30:11.520]   Well, this is Mechese Chiklofsky's "Mormon Bartender" thing.
[01:30:11.520 --> 01:30:15.840]   He says to any people who try to solve this problem, the people who aren't qualified to do it because
[01:30:15.840 --> 01:30:21.040]   they don't realize how complicated it is. The Mormon bartender...
[01:30:21.040 --> 01:30:22.000]   I like it.
[01:30:24.560 --> 01:30:28.000]   Yeah, and he accused me of that with a few reviews, back and made a nice chat about it.
[01:30:28.000 --> 01:30:34.400]   But it is a challenge in that you can't represent the nuances of human communications in code.
[01:30:34.400 --> 01:30:40.160]   We eventually realize that and just try and provide a connection between people.
[01:30:40.160 --> 01:30:42.320]   That's what the social stuff is about.
[01:30:42.320 --> 01:30:51.760]   What Facebook has done has been very, very focused on connection at the expense of everything else.
[01:30:51.760 --> 01:30:55.680]   And I'm sure you have had the Bosmer one last week and talked about that.
[01:30:55.680 --> 01:30:58.640]   But that was very much...
[01:30:58.640 --> 01:31:04.640]   Yes, he was overstating it for effects possibly, but it was a very good summation of Facebook
[01:31:04.640 --> 01:31:07.680]   position, I think. The most important thing is...
[01:31:07.680 --> 01:31:13.280]   It's like these sort of 10-dentures AI arguments that Mechese goes after very well as well,
[01:31:13.280 --> 01:31:16.640]   which is the most important thing is connecting people there for everything else is subordinate
[01:31:16.640 --> 01:31:20.880]   to that. And it's like, no, actually, that's not the most important thing.
[01:31:20.880 --> 01:31:23.600]   At some point, you can have too much connection and that's bad.
[01:31:23.600 --> 01:31:27.840]   And people want to start backing away. And you obviously believe that you wouldn't
[01:31:27.840 --> 01:31:32.800]   create the algorithmic feed in the first place. So part of it is that they have an
[01:31:32.800 --> 01:31:36.800]   existential problem, which is their mental model was they were growing all the time,
[01:31:36.800 --> 01:31:40.720]   and they kept doubling. They kept growing year on year. They were getting bigger and bigger,
[01:31:40.720 --> 01:31:45.360]   and they can't anymore because they run out of people. And they started collecting extraneous
[01:31:45.360 --> 01:31:51.520]   data to try and keep the sort of growth of the information they were gathering to try and make
[01:31:51.520 --> 01:31:56.160]   sense of things going up. And they've crossed that saturation point now and they're going to
[01:31:56.160 --> 01:32:02.320]   have to rethink what they're doing. And it is an identity crisis for them in many ways as well.
[01:32:02.320 --> 01:32:06.560]   That's the other part of it from the Facebook interviews that they are having.
[01:32:06.560 --> 01:32:10.640]   They've gone through the saturation point and they've got to do something else now.
[01:32:10.640 --> 01:32:15.280]   There's a wonderful piece in the New Yorker called the Infuriating Innocence of Mark Zuckerberg
[01:32:15.280 --> 01:32:23.840]   and they have this great phrase ambient dislike for people just don't like him.
[01:32:23.840 --> 01:32:29.280]   That's not his fault. They're trying to put his finger on it. But the thing that they say is
[01:32:29.280 --> 01:32:35.600]   the infuriating innocence is this fact that whenever he's apologizing, which he does frequently,
[01:32:35.600 --> 01:32:42.800]   it seems to be a major part of what he does for a living. It's not that he's never looks like he's
[01:32:42.800 --> 01:32:48.160]   remorseful. He never looks like he's guilty. He never looks like he's innocent. He just is
[01:32:48.160 --> 01:32:53.760]   slightly sad that things turned out the way they did. And I think this is a hint to the strangeness
[01:32:53.760 --> 01:32:59.040]   that is Mark Zuckerberg and Facebook, their whole culture is sort of this, they're not immoral.
[01:32:59.040 --> 01:33:06.240]   They're sort of amoral. Morality is just this thing that is a PR problem if it goes south.
[01:33:06.240 --> 01:33:09.760]   The reason he's always apologizing is that they keep trying everything.
[01:33:09.760 --> 01:33:16.880]   And if nobody makes a stink about it, that makes it ethically okay. If they make a stink, well,
[01:33:16.880 --> 01:33:21.520]   we need to do better and think about the broader picture and we need to refer to my team and all
[01:33:21.520 --> 01:33:27.280]   the stuff that they do. But the reality is that they just seem to be mechanically amoral.
[01:33:27.280 --> 01:33:35.280]   There's no moral compass or ethical guideposts at this company whatsoever. Even at a company like
[01:33:35.280 --> 01:33:41.360]   Microsoft or Google or Apple, there's a sense of basic, you know, doing the right thing to a
[01:33:41.360 --> 01:33:46.400]   certain extent. Whereas at Facebook, there doesn't seem to be any knowledge or awareness that that
[01:33:46.400 --> 01:33:50.880]   even exists. It's like a program. I've heard I've heard it described as they treat ethical issues
[01:33:50.880 --> 01:33:56.800]   as if it's a programming problem. That's very it's an engineer's mindset. It's the Mormon bartender.
[01:33:56.800 --> 01:33:59.840]   Yeah. People who don't have friends.
[01:33:59.840 --> 01:34:09.920]   But it's just the McCrusty's problem is the idea that there's so focus on the model that stuff
[01:34:09.920 --> 01:34:15.920]   doesn't fit the model as an error rather than a failure of the model. The real names thing
[01:34:15.920 --> 01:34:20.400]   has been that for years. It's like, oh, you must have a name that we think looks like a name.
[01:34:20.400 --> 01:34:23.120]   What do you mean your name is only one word? What do you mean your name?
[01:34:23.120 --> 01:34:27.680]   Well, it's a failure to say your name is fake. That can't be real.
[01:34:27.680 --> 01:34:34.640]   Yeah. It's a failure to recognize and understand that there are other points of you in the world
[01:34:34.640 --> 01:34:39.520]   and other experiences, which is why people can argue for diversity and tech or whatnot.
[01:34:39.520 --> 01:34:47.120]   Yes, you basically created this monoculture and it's an amoral monoculture.
[01:34:47.120 --> 01:34:52.480]   And now we're all like, oh. This is the most chilling part of this whole testimony in Congress
[01:34:52.480 --> 01:34:57.360]   that nobody freaked out about, but we should all start really panicking about this, which is basically
[01:34:57.360 --> 01:35:01.840]   they're saying, well, what about all this stuff? Can't you have AI to like deal with some of the
[01:35:01.840 --> 01:35:05.760]   objectionable content and all this stuff? And he's like, oh, we got AI at the point where we're like
[01:35:05.760 --> 01:35:12.640]   90, 95%, but in just a few years, I will instantly censor things based on, is that all realistic?
[01:35:12.640 --> 01:35:17.600]   Well, it's realistic. And that's a problem because when they make errors and objectionable content gets
[01:35:17.600 --> 01:35:23.520]   on there and Russian, Russian government does, you see it. But if it's AI that makes it invisible,
[01:35:23.520 --> 01:35:27.120]   they will be deleting things and we'll never know what to do. I feel like this is one of the hardest
[01:35:27.120 --> 01:35:31.440]   things for a computer period. This is what people have been complaining about this for
[01:35:31.440 --> 01:35:34.720]   it. It's true. It's a legitimate problem. People have been talking about it forever,
[01:35:34.720 --> 01:35:39.520]   starting with biases in their friend feed. So people weren't aware that Facebook was
[01:35:39.520 --> 01:35:45.840]   determining what you saw based on an algorithm. But what Facebook is trying to do, and this gets to
[01:35:45.840 --> 01:35:51.680]   Kevin's point about reaching a saturation point, Facebook needs to do things at scale,
[01:35:51.680 --> 01:35:57.760]   2 billion people scale. And they can't do that with actual humans. So they get as much data as
[01:35:57.760 --> 01:36:04.480]   they can to try to replicate the sense that humans would have, the ways we deal with things.
[01:36:04.480 --> 01:36:09.840]   They just optimize for things that not everyone would optimize for.
[01:36:09.840 --> 01:36:16.320]   But even humans can't do things the way that humans want them to go. This is what this whole
[01:36:16.320 --> 01:36:21.600]   thing is about. And the thing that, the categorical distinction that I would make here is
[01:36:21.600 --> 01:36:28.480]   that when things happen in darkness, and you have no way to know, Facebook is a black box,
[01:36:28.480 --> 01:36:34.960]   they're algorithms that determine these things, and their AI are and will be company secrets.
[01:36:34.960 --> 01:36:43.760]   So we're talking about a censorship machine that nobody can see, can know about, know how it works,
[01:36:43.760 --> 01:36:47.920]   knows what sensors and what is it sensors? Facebook and Facebook did promise that they would let
[01:36:47.920 --> 01:36:53.120]   researchers start. For a long time, they actually didn't let researchers scrape Facebook's data
[01:36:53.120 --> 01:36:58.560]   for their research. It was verboten. So it violated their terms of service. So that's actually,
[01:36:58.560 --> 01:37:06.400]   they're putting in place mechanisms to stop that. What's hard is we're asking machines
[01:37:06.400 --> 01:37:12.880]   in AI to do things that people can't actually do. And this gets to, we have this utopian vision
[01:37:12.880 --> 01:37:19.440]   of what machines can offer us. But people, my favorite example is my nest thermostat. People
[01:37:19.440 --> 01:37:22.880]   are like, "Oh, it'll know who you are, and it'll give you the right temperature when you get home."
[01:37:22.880 --> 01:37:27.600]   And I'm like, "Yeah, it's a beautiful vision, except for my current comfy temperature is 77,
[01:37:27.600 --> 01:37:34.000]   and my husband's is like 72." The nest, I can't mediate that. We can't mediate that. The nest
[01:37:34.000 --> 01:37:41.920]   has no chance in health. So imagine that writ large across Facebook and trying to figure out what
[01:37:41.920 --> 01:37:48.880]   people want. And it's a lose-lose game. Here's an article by Brian Krebs, which just gives you an idea
[01:37:48.880 --> 01:37:55.520]   of both the problem and how hard, how intractable it is. He's writing, "Don't give away historical
[01:37:55.520 --> 01:38:02.640]   details about yourself." And he's pointing out that there are many quizzes on Facebook, which people
[01:38:02.640 --> 01:38:08.560]   gleefully answer that ask you the same exact questions that your secret questions are going to ask you.
[01:38:09.120 --> 01:38:17.600]   Good old days. What was your first pet? What was its name? Tell us the name of your special furry friend,
[01:38:17.600 --> 01:38:26.000]   women working from springchicken.co.uk and e-commerce site. It asks users to tell them,
[01:38:26.000 --> 01:38:30.240]   "What street did you grow up on? What's the first address you remember living at? Let's see if we
[01:38:30.240 --> 01:38:35.840]   could put some spring chickens in touch with old friends. What could possibly go wrong? What was
[01:38:35.840 --> 01:38:42.000]   your first job?" Your mother's maiden name. Yeah, practically. Your high school mascot.
[01:38:42.000 --> 01:38:48.000]   What's your password? How long have you and your spouse been together? Where did you meet?
[01:38:48.000 --> 01:38:55.520]   From anti-acid. These are all Facebook quizzes, folks. 68,000 Facebook users answered this question.
[01:38:55.520 --> 01:39:01.200]   What's your favorite? I'm sorry, 6,800 Facebook answers to what's your favorite movie of all
[01:39:02.000 --> 01:39:06.000]   time. This is from the women's working Facebook page. You know what this does though? These
[01:39:06.000 --> 01:39:10.880]   security questions are just absolutely asking. Really? That's the problem I have to say. It's not
[01:39:10.880 --> 01:39:14.800]   so much that they're asking these questions all. It's obvious why they are, but it's a terrible,
[01:39:14.800 --> 01:39:20.240]   I tell everybody I know do not answer security questions with real answers. Remember Sarah
[01:39:20.240 --> 01:39:26.800]   Palin's email was hacked because she used a password that referenced her past and he just looked up
[01:39:26.800 --> 01:39:31.680]   her with the user. Paris Hilton used her dog's name to her secret question to her apple account.
[01:39:31.680 --> 01:39:38.000]   And then even apple uses secret stuff like that. No one should use secret questions.
[01:39:38.000 --> 01:39:42.320]   Oh lord. Because everybody does. Authentication is a problem. I mean, that's a big problem.
[01:39:42.320 --> 01:39:48.320]   But because everybody does, just use your last pass. Generate garbily gook and keep track of it.
[01:39:48.320 --> 01:39:51.840]   Garbily gook is a good strategy. That'd be the answer to all my secret questions.
[01:39:51.840 --> 01:39:56.160]   Several times I've had to go in and produce this content at apple.
[01:39:56.160 --> 01:40:00.560]   And it's like, what was your favorite teacher? And I'm like, I hated them all. I have no idea what
[01:40:00.560 --> 01:40:04.240]   you should probably do is go to the good old days page where you probably answered this question.
[01:40:04.240 --> 01:40:07.680]   Right. Exactly. But they're not going to give you that data. They're not going to tell you what you
[01:40:07.680 --> 01:40:17.360]   is garbily gook. I think that's a good that Neil has. Is it got it's gobbledy or gobbledy
[01:40:17.360 --> 01:40:26.320]   gook? But I think garbily gook is actually sounds encrypted. It sounds encrypted.
[01:40:26.320 --> 01:40:29.440]   Yeah. United Airlines Patrick Delahandy is telling us. It has only a few security questions.
[01:40:29.440 --> 01:40:31.520]   And then ask you to pick your answer from a list.
[01:40:31.520 --> 01:40:37.520]   It even works. You can't put in garbily gook.
[01:40:37.520 --> 01:40:45.760]   What's your favorite singer from a list of five? Yeah. Yeah. They do. Literally.
[01:40:46.640 --> 01:40:53.680]   Yeah. Wow. It's all garbily gook all the way down. Kevin Marks always is thrilled to have you on.
[01:40:53.680 --> 01:40:56.960]   I really appreciate your taking time. It's practically the middle of the night where you are.
[01:40:56.960 --> 01:41:01.440]   Tell us something good and wonderful that you want to share with the world.
[01:41:01.440 --> 01:41:10.240]   Well, the indie web.org of course is the ongoing thing that everyone should look at.
[01:41:10.240 --> 01:41:13.520]   It's particularly relevant in the world of Facebook. We see lots of people suddenly
[01:41:14.240 --> 01:41:16.960]   picking up on this and saying, oh, well, maybe we should be should do more of this ourselves.
[01:41:16.960 --> 01:41:22.800]   So it's a movement to say you should have your own website. You should work on that.
[01:41:22.800 --> 01:41:29.040]   There are regular meetings in multiple cities by the evening ones, the home to website clubs,
[01:41:29.040 --> 01:41:33.920]   the next ones next week on Wednesday. And then there's
[01:41:33.920 --> 01:41:39.440]   which is the sort of more intense indie web camp, which was where the idea started.
[01:41:40.480 --> 01:41:46.400]   And I can't remember the next one is because I don't have the screen up, but look, go there
[01:41:46.400 --> 01:41:57.520]   and have a look. And there's also a continuous chat going on there that's in IRC and on the web
[01:41:57.520 --> 01:42:02.720]   and in Slack in parallel where we talk about these things and what we're doing. One of the things
[01:42:02.720 --> 01:42:08.320]   that's going on at the moment is there's a lot of work going on about building readers,
[01:42:08.880 --> 01:42:16.480]   building a UI for reading things that is a combination of stuff, stuff you've published,
[01:42:16.480 --> 01:42:21.520]   news feeds and social feeds as well. So that's there's ongoing discussion there. So if you're
[01:42:21.520 --> 01:42:27.200]   thinking about those things, pop in and see what's going on. Nice. And I see that it's very
[01:42:27.200 --> 01:42:33.360]   apropos. They just concluded the internet identity workshop in Mountain View, which I take it as
[01:42:33.360 --> 01:42:38.800]   all. Oh, yes. Yes. Yeah. Other great stuff. It's coming up though. It's all in the calendar.
[01:42:38.800 --> 01:42:44.960]   @indeweb.org. My kelgen shares something with me. Okay. I've got something to share.
[01:42:44.960 --> 01:42:51.680]   So there is there's this whole category of productivity apps about which I tend to be a
[01:42:51.680 --> 01:42:56.080]   successful. And I gave a big plug to Noto yesterday. Fantastic. On iOS today. I made a
[01:42:56.080 --> 01:42:59.360]   test that kept really cool. It's really cool. Let's you mail email yourself with a swipe. It's
[01:42:59.360 --> 01:43:05.760]   just like Tinder without the consequences. And one of the things to great about Noto also is
[01:43:05.760 --> 01:43:10.560]   that when you are on other apps or on a browser or something, you hit share and choose Noto.
[01:43:10.560 --> 01:43:13.520]   That's the end of it. You don't there's no further dialogues and nothing. It just goes
[01:43:13.520 --> 01:43:17.520]   right to your email. Anyway, so here's another productivity app that I really, really love.
[01:43:17.520 --> 01:43:24.000]   It's called Task Aid. This is the one I didn't like, but that's only me. Task Aid? Yeah. Because
[01:43:24.000 --> 01:43:28.800]   it gives you a timer. Yeah. No, that's not the one. No, you're thinking of, yeah,
[01:43:28.800 --> 01:43:33.360]   I had two apps. I can't live with that. It was a to-do list that started with timer. Right.
[01:43:34.160 --> 01:43:37.520]   I know. I love that one. I know. Would you get things done? You're effective.
[01:43:37.520 --> 01:43:56.080]   Well, okay. So Task Aid has no timers. Basically, this is like
[01:43:56.080 --> 01:44:01.040]   Chrome plugin so that when you have a new tab, it shows you your tasks. It's not just to do.
[01:44:01.040 --> 01:44:08.960]   It's lists. It's lists, bullet lists, and it's for groups as well. So you can have one for yourself
[01:44:08.960 --> 01:44:13.360]   for work. You can have another group for yourself for personal stuff. You can have one for your
[01:44:13.360 --> 01:44:16.480]   group of people and then you can have one for your Weasel Club or whatever it is you do.
[01:44:16.480 --> 01:44:24.240]   And the data from each platform shows up on the others, it's completely free. There's no cost at
[01:44:24.240 --> 01:44:31.520]   all to this. It's a completely free thing. They use a lot of like, it's colorful and sort of
[01:44:31.520 --> 01:44:38.240]   almost cartoonish. They use little icons and pictures and stuff like that. And they automate
[01:44:38.240 --> 01:44:43.120]   the list to a certain extent. You can sort of select a list and then just share it on other
[01:44:43.120 --> 01:44:47.520]   media. It's just great in every way. If you want to keep track of what you're doing,
[01:44:47.520 --> 01:44:52.400]   maintain lists, have things at your fingertips. It's great to be able to put things into a browser,
[01:44:52.400 --> 01:44:56.640]   be reminded by the browser when you open a new tab. Every time you open a new tab. And then also
[01:44:56.640 --> 01:44:59.760]   you're out and about and you're like, what was that thing? You just whip it out on your phone and
[01:44:59.760 --> 01:45:04.880]   the data that you entered on your desktop is right there on the phone. So it's collaborative.
[01:45:04.880 --> 01:45:10.960]   It's cross-platform. I can't believe it's free. It's completely free. Now they're going to monetize
[01:45:10.960 --> 01:45:15.360]   they say by in the future. It's sometimes in the future they're going to have additional features,
[01:45:15.360 --> 01:45:21.440]   but they say that all the existing features will remain free forever. So it doesn't have a deadline
[01:45:21.440 --> 01:45:25.840]   on using this thing. It's going to be free forever. And if you want to upgrade to the features in
[01:45:25.840 --> 01:45:30.160]   the future, which by the way, don't sound that compelling. Some of them involve stickers.
[01:45:30.160 --> 01:45:33.600]   Wait, did you read the privacy policy and the data collection?
[01:45:33.600 --> 01:45:42.080]   Don't start with me. We cannot complain if we do not tell people in the media when we recommend
[01:45:42.080 --> 01:45:47.840]   this stuff that I like to replicate the ordinary user experience. So no, I didn't read it.
[01:45:48.640 --> 01:45:52.000]   As a member of the media, I do think it's your job.
[01:45:52.000 --> 01:45:53.920]   All right. Let's read it. We need to do better.
[01:45:53.920 --> 01:45:57.680]   Let's read it together. We don't have time for that. Come on. How long can it take?
[01:45:57.680 --> 01:46:01.120]   It's going to take us longer than the Zuckerberg hearings. Yeah.
[01:46:01.120 --> 01:46:07.040]   Templates at fact log. Where is the privacy stuff? It's private.
[01:46:07.040 --> 01:46:12.160]   There's usually terms of service and terms. We collect and manage user data according to the
[01:46:12.160 --> 01:46:17.040]   following privacy policy. With the goal of incorporating our company's values, transparency,
[01:46:17.040 --> 01:46:23.280]   accessibility, sanity, usability, we collect anonymous data, anonymous from every visitor to monitor
[01:46:23.280 --> 01:46:27.600]   traffic and fix bugs. Of course they do. We ask you to log in and provide personal data such as
[01:46:27.600 --> 01:46:31.680]   name, address, name and email address in order to be able to save your profile. They got to do that.
[01:46:31.680 --> 01:46:36.160]   You could make that be fake in order to enable these or any login based features. We use cookies.
[01:46:36.160 --> 01:46:40.240]   Sure you do because you have to. You can't block or delete cookies and still use task aid.
[01:46:40.240 --> 01:46:43.280]   Although if you do, you'll be asked for your user name and password every time.
[01:46:45.840 --> 01:46:49.520]   But you could provide us with other personal information, but you don't have to.
[01:46:49.520 --> 01:46:54.160]   That's up to you. We only use your personal information to provide you with the task aid
[01:46:54.160 --> 01:46:58.720]   services or communicate with you. Let me see what the third party stuff is. This is very
[01:46:58.720 --> 01:47:04.560]   unlegal ease. This is really good actually. I'm glad you asked. This is how most of these start.
[01:47:04.560 --> 01:47:10.240]   We do not share your information with third parties. Okay. Wow. Our only aggregate and
[01:47:10.240 --> 01:47:14.000]   anonymized data is periodically transmitted to external services to help us improve the website
[01:47:14.000 --> 01:47:20.080]   and service. That's what Tinder did. But it's anonymized. We use the social login buttons,
[01:47:20.080 --> 01:47:24.560]   but Stacy, I'm not going to use those anymore. I'm going to go through the trouble of creating
[01:47:24.560 --> 01:47:29.280]   a password and all that stuff. You have last class. I know I could just do that, huh?
[01:47:29.280 --> 01:47:36.320]   We may choose to buy or sell assets. If we or substantially all of our assets were acquired,
[01:47:36.320 --> 01:47:39.680]   or if we go out of business or bankruptcy, user information would be one of the assets
[01:47:39.680 --> 01:47:43.840]   that is transferred or acquired by a third party. You acknowledge that such transfers may occur,
[01:47:43.840 --> 01:47:48.880]   and that any acquirer of us may continue to use your personal information to set forth in this
[01:47:48.880 --> 01:47:55.040]   policy. But you know what? This looks pretty good. I have to say Stacy. I don't see any big gaps.
[01:47:55.040 --> 01:47:59.360]   No. You know what? You see people. I'm just saying you can do it. Right on. Yeah. That's the
[01:47:59.360 --> 01:48:06.080]   right response to my unbridled enthusiasm about this app, which I didn't read the privacy policy.
[01:48:06.080 --> 01:48:11.120]   Could have been hard. I've done that before. I used to recommend you role. Remember that?
[01:48:11.120 --> 01:48:14.080]   Oh, unroll. Unroll me. Unroll me. That's how it was about.
[01:48:14.080 --> 01:48:18.000]   Unroll me. My son is like, "You use unroll me. What are you nuts?"
[01:48:18.000 --> 01:48:23.440]   Well, they got in trouble. Just to circle back on what we were talking about before,
[01:48:23.440 --> 01:48:29.520]   the app that I loved, then you hated, is called Effortless. It's for macOS only. It's for the
[01:48:29.520 --> 01:48:33.760]   desktops. They don't even have a mobile version. But you have a to-do list and you put a number at
[01:48:33.760 --> 01:48:37.040]   the end, which is the number of minutes. It has a timer, which annoys you.
[01:48:37.040 --> 01:48:41.120]   So annoying. But I like it because I need a timer. I need something.
[01:48:41.120 --> 01:48:45.200]   It's like having your mother in your computer. I need my mother. Have you finished that yet,
[01:48:45.200 --> 01:48:50.000]   Michael? Finish that article, Michael. You can't know. There's no other way I'm going to finish that
[01:48:50.000 --> 01:48:56.480]   article. So that's me. Effortless. See, that's me. I will, exactly what I will do, say, "Smrew
[01:48:56.480 --> 01:49:03.760]   you." I'm going to go watch TV. Stacey Higginbotham, your pick of the week.
[01:49:03.760 --> 01:49:09.280]   So you asked me, I guess, two weeks ago, I had gotten the Nestor Bell and I showed that to you
[01:49:09.280 --> 01:49:13.120]   guys. So I'm going to tell you a little bit about this. But I also, the New Yorker did an
[01:49:13.120 --> 01:49:17.760]   article, I think it's their April 2nd issue. Oh, my family's home. Okay. So are we already living in
[01:49:17.760 --> 01:49:22.320]   virtual reality? What was cool about this is I'm not super excited about virtual reality,
[01:49:22.320 --> 01:49:30.320]   except this made a really good case for psychology in this concept of embodiment in terms of widening
[01:49:30.320 --> 01:49:36.080]   people's horizons, in terms of giving people legitimate other points of view. And even like,
[01:49:36.080 --> 01:49:40.560]   they had this guy doing an interview with himself that made him look at his whole
[01:49:40.560 --> 01:49:48.320]   dilemmas and the way he talks to himself differently. So I looked at this as a psychological
[01:49:48.320 --> 01:49:53.920]   tool and maybe a tool to encourage empathy. And I had never really thought about it that way.
[01:49:53.920 --> 01:49:58.240]   So I just thought it was a really interesting, to me, mind opening article.
[01:49:58.240 --> 01:50:03.280]   It is interesting. In the meantime, we have literature and talk to yourself.
[01:50:03.280 --> 01:50:09.200]   Okay. So what they did, and we do have that, and I think that works for some people,
[01:50:09.200 --> 01:50:15.600]   but what they do is it's not just virtual reality. They actually physically take your motions and
[01:50:15.600 --> 01:50:22.960]   put them like, if I'm doing this, they could show me in virtual reality a man. And then
[01:50:22.960 --> 01:50:27.440]   by virtue of sinking my movements with him, I actually,
[01:50:27.440 --> 01:50:31.600]   and then they changed the first, yeah, I use body awareness and then they change my perspective.
[01:50:31.600 --> 01:50:37.680]   So I'm taller or, you know, I have a bit different. So they actually did a thing where they put men
[01:50:37.680 --> 01:50:45.280]   who abused women. And they did that and put them in a woman's virtual woman's body. And it actually
[01:50:46.240 --> 01:50:49.920]   the results were great. They changed their perspective entirely.
[01:50:49.920 --> 01:50:53.440]   Some of them probably abused themselves. That's another story.
[01:50:53.440 --> 01:50:58.080]   Yeah. No, this is really interesting. I've known about this for some time. It's really
[01:50:58.080 --> 01:51:03.120]   interesting research. I was, I had no idea this was happening and I was just super excited.
[01:51:03.120 --> 01:51:09.040]   And I'm sorry, Stacey, that was a piece in the New Yorker. Yeah. Yes. It's the April 2nd
[01:51:09.040 --> 01:51:14.320]   New Yorker Joshua Rush, Rothman. Fantastic. Are we already living in virtual reality?
[01:51:14.320 --> 01:51:18.960]   I'm going to read this cover to cover in actual reality and actual reality.
[01:51:18.960 --> 01:51:23.120]   In actual reality. Yeah. The whole, the whole issue was about the mind. There was some sort of
[01:51:23.120 --> 01:51:29.440]   metaphysical like extension of yourself and I kind of lost that. But let's talk about the nest.
[01:51:29.440 --> 01:51:36.080]   Yes. Okay. Let's see. Oh, I should see if I can. Oh, it's an iPad. So forgive me because I'm using
[01:51:36.080 --> 01:51:41.120]   my iPad because it's bigger. We can see it better that way. Yeah. So let me write this down a little
[01:51:41.120 --> 01:51:44.720]   dabbard, do you? A little doobie dabbard. Just have to make it work.
[01:51:44.720 --> 01:51:52.080]   Meanwhile, I'll install this task aid. Now that I feel like the privacy policy is,
[01:51:52.080 --> 01:51:58.080]   you know, that's pretty good. It's not bad. Take good. Okay. Don't make fun of me, but I'm having a
[01:51:58.080 --> 01:52:04.880]   hard time working, finding things on the iPad. Like, oh, Stacey. So this is, this is the, it's
[01:52:04.880 --> 01:52:09.760]   the new doorbell from the alphabet accompanying nest. Actually, now it's a Google company nest.
[01:52:09.760 --> 01:52:12.960]   And it's much like our sponsor, the ring video doorbell. It's a camera.
[01:52:12.960 --> 01:52:16.880]   You could see who's at the door. You could talk to them. You can listen to them.
[01:52:16.880 --> 01:52:23.920]   I already have a nest installed. Should I, I mean, I'm not a nest a ring installed. Should I,
[01:52:23.920 --> 01:52:28.160]   what, why would I, that's actually, I guess question number one. Is it different?
[01:52:28.160 --> 01:52:37.120]   It's very similar to the ring pro. It is $239. The ring pro, I believe, is $250.
[01:52:38.800 --> 01:52:45.200]   The cool thing is they have the facial recognition feature. If you have a goo,
[01:52:45.200 --> 01:52:50.160]   so a couple of things. If you are in a Google home, I would actually pick this over the ring pro.
[01:52:50.160 --> 01:52:58.320]   When with the facial recognition turned on, when it sees me, it can actually, I have it set up so
[01:52:58.320 --> 01:53:04.400]   it'll announce me on the Google home and Google assistant devices in my house to say, Stacy's home.
[01:53:04.400 --> 01:53:08.800]   So like on like on downtown Abbey, when somebody shows up to the party, like it'll announce.
[01:53:08.800 --> 01:53:21.120]   Yes. Just like. Okay. So, and here I have to show it on my, I can't work the iPad out because
[01:53:21.120 --> 01:53:29.520]   I'm an idiot, but okay, here we go. Can you see? That's my door. And then here is,
[01:53:29.520 --> 01:53:34.320]   this is my family coming home right now. Like a few minutes ago, I got a notification that
[01:53:34.320 --> 01:53:40.080]   that's them. So is that a balloon? That's a balloon. It's my birthday. It's a mylar.
[01:53:40.080 --> 01:53:45.120]   It can handle mylar balloons, Leo. This is way better than Oh, I just spoiled their birdie.
[01:53:45.120 --> 01:53:57.200]   Oh, you don't have to tell them. Oh, okay. Anyway, Oh, oh, oh. You invaded your own privacy.
[01:54:01.600 --> 01:54:07.920]   Wait, did we see a cake? Hold on. No, stop looking. Stop. Stop. Stop.
[01:54:07.920 --> 01:54:16.240]   Looking. Okay. So the camera quality is great. The integrations with other Google stuff is pretty
[01:54:16.240 --> 01:54:22.400]   cool. It actually makes me want to and I'll tell you guys because next week or later this week,
[01:54:22.400 --> 01:54:27.600]   the rest of this weekend, Google, find my house. So next week, I can actually show you some very
[01:54:27.600 --> 01:54:32.880]   cool things that I'm trying to do. And I'm doing it in preparation for the smart displays from
[01:54:32.880 --> 01:54:38.400]   Google Home. Nice. So coming this summer. So basically, I don't know what you want to know
[01:54:38.400 --> 01:54:42.320]   about this. The install is very similar to the Ring Pro. If you're in a Google household,
[01:54:42.320 --> 01:54:48.880]   I would definitely go with this over the other. I like the look at the doorbell a little bit better.
[01:54:50.720 --> 01:54:58.160]   I can set. I don't think the motion detection is as granular as the Ring Pro, which gives you
[01:54:58.160 --> 01:55:03.680]   all the very cool, like you can block out on the screen what you don't want. But it does give you
[01:55:03.680 --> 01:55:08.080]   with the nest, they have a subscription. So it gives you access to all of this. And I think the
[01:55:08.080 --> 01:55:12.400]   facial recognition is actually really kind of cool. Stacey, what is the price of the cloud storage
[01:55:12.400 --> 01:55:18.800]   for storing videos? If there is a subscription, which I think it's, yeah, Nest Aware, I think
[01:55:18.800 --> 01:55:23.280]   it's $10 a month. But let me just tell you. Nest is in general. I think, can you maybe
[01:55:23.280 --> 01:55:29.920]   pull it with all your other nest stuff? Maybe giving you a save. There you go. There's $30 a month
[01:55:29.920 --> 01:55:36.640]   for 30 days. $10 a month for 10 days. 10 days is enough. 100 days is fine. Five days is enough,
[01:55:36.640 --> 01:55:43.680]   really. Yeah. That's a good price, actually. Or 50 bucks a year. So I like that. And it's very
[01:55:43.680 --> 01:55:48.960]   reasonable. And it's half off for Nest Aware for additional new cameras. So if you already have it,
[01:55:48.960 --> 01:55:55.040]   it's a, yeah, if you have two, it's going to be $750 for the cheapest. If you have three cameras,
[01:55:55.040 --> 01:55:59.280]   it's going to be $10, et cetera, et cetera. I would like to get like five or six of those and
[01:55:59.280 --> 01:56:04.880]   put them all over the house and announce everywhere I am. Mike is going down the whole way. Mike is
[01:56:04.880 --> 01:56:11.920]   entering the bedroom. It wouldn't be too annoying. You go for that one. All right. And how much?
[01:56:11.920 --> 01:56:16.640]   $2.99 you said? No, $2.39. $2.39. That's good. Yeah.
[01:56:16.640 --> 01:56:21.600]   And I will say though, the install, just like the Ring Pro, they have a chime that you're
[01:56:21.600 --> 01:56:26.000]   going to have to install on your doorbell, like the actual physical door chime that's in your house.
[01:56:26.000 --> 01:56:32.640]   And this is a wired doorbell. And yeah. Say hello, even when you can't.
[01:56:32.640 --> 01:56:37.040]   And oh, yeah. It's two-way audio. And actually it can always be listening.
[01:56:37.920 --> 01:56:44.960]   So it came in handy because my FedEx guy came and I got the Ecobee Switch that just arrived today.
[01:56:44.960 --> 01:56:49.360]   And he knocked on my door, except he didn't. But he said he knocked because I caught him after he
[01:56:49.360 --> 01:56:54.720]   was like, and then he's like, I knocked. And I was like, I believe you didn't. And then I go and
[01:56:54.720 --> 01:56:58.720]   I look at him. So did you confront him? No, I'm not going to confront a FedEx guy. His life is
[01:56:58.720 --> 01:57:04.160]   miserable. No, how does that work? If it's always listening, you end up with 24 hours of audio.
[01:57:04.880 --> 01:57:11.040]   I think I can just listen in on it. Oh, I'm just listening. Just live. Yeah. Yeah. I bet that's
[01:57:11.040 --> 01:57:15.120]   the case because you wouldn't want it streaming the whole time. That's a lot of data. That's a
[01:57:15.120 --> 01:57:20.080]   lot of audio. Yeah. It's a lot of bird song in my, my neck. Yeah. Of stuff that you don't really care.
[01:57:20.080 --> 01:57:25.440]   It might be nice to do this sort of like the dash cam thing where it records and then
[01:57:25.440 --> 01:57:29.200]   overwrites its own data so that you can say, you know what? Give me the last hour.
[01:57:29.200 --> 01:57:34.400]   Well, you can do that with the nest. If you're familiar with the nest aware stuff,
[01:57:34.400 --> 01:57:36.800]   so the timelines that they have, I think they call it sightline.
[01:57:36.800 --> 01:57:45.520]   So you could actually scroll back through your time timeline. I don't know if you can see. So this
[01:57:45.520 --> 01:57:55.840]   is my timeline down here. So I can scroll through and see. Oh, nice. And it's showing me motion.
[01:57:55.840 --> 01:58:00.080]   Like that's when my family just came home. That's cool. I like that. Showing you. Yeah,
[01:58:00.080 --> 01:58:04.720]   the nest cameras do that too. I really love that feature. I have to say because you can quickly
[01:58:04.720 --> 01:58:10.160]   go to motion and see what's going on. It's also fun for a time lapse of the sun coming up.
[01:58:10.160 --> 01:58:15.600]   Really enjoy that. Yeah. And we look for, at least it uses it to look for her cats.
[01:58:15.600 --> 01:58:22.000]   See what they're up to. There you go. Thank you for the review. And I presume you're going to
[01:58:22.000 --> 01:58:25.840]   are you going to write this up and Stacy on IOT, your fabulous free newsletter?
[01:58:26.480 --> 01:58:32.160]   I probably am or it'll be on the website. It's Stacy and IOT.com. Nice. Everybody should go there.
[01:58:32.160 --> 01:58:37.120]   Sign up at Gigas. Oh, sign up. That's a great birthday present for me. Oh, happy birthday,
[01:58:37.120 --> 01:58:40.800]   by the way. This is going to be. This is tomorrow is your birthday. Yeah. Tomorrow. Yes.
[01:58:40.800 --> 01:58:44.080]   So I have a happy birthday. I surprise when you see the balloon.
[01:58:44.080 --> 01:58:52.000]   Never happened. I didn't see a cake. You guys. How are the blinds working, by the way? Are they
[01:58:52.000 --> 01:58:57.760]   going up and down? They are. The blinds are still working. I did dump my IKEA trod fee lights because
[01:58:57.760 --> 01:59:04.320]   they do not work with Battle A. So that's too bad. But you're the guinea pig on IOT so that we don't
[01:59:04.320 --> 01:59:08.160]   have to be. Thank you for doing that. I appreciate it. Thank you, Kevin Marks, for joining us.
[01:59:08.160 --> 01:59:14.400]   Always a pleasure. Go back to go to bed now. It's okay. Have a great evening. We appreciate it.
[01:59:14.400 --> 01:59:19.040]   Thank you also for stopping by, Mr. Mike Elgin. Are you in the States for a little longer?
[01:59:20.000 --> 01:59:24.960]   Another month or so. Then we're back to Europe preparing for the Prosecco and Provence experiences
[01:59:24.960 --> 01:59:30.880]   respectively. Yes true nomad.net. I recommend that everyone sign up for our newsletter. There'll
[01:59:30.880 --> 01:59:34.240]   be a pop up on that page. Sorry about that. But if you sign up for the newsletter,
[01:59:34.240 --> 01:59:40.720]   you'll be kept abreast of all of our adventures and activities. And I think you really enjoy it.
[01:59:40.720 --> 01:59:46.000]   I also have a lot of tips and tech and things like that for traveling, for living nomatically,
[01:59:46.000 --> 01:59:51.680]   for working on the road. Lots of stuff. If you're into, you know, if you travel at all,
[01:59:51.680 --> 01:59:57.440]   have any interest in travel, or if you eat food, please subscribe to our newsletter. I think
[01:59:57.440 --> 02:00:02.960]   you'll really enjoy it. If Mike in front of the farmhouse. Yeah, that's my idea of the open plan
[02:00:02.960 --> 02:00:08.400]   office. That is so awesome. I love it. That's actually in the Prosecco Hills area.
[02:00:08.400 --> 02:00:12.720]   If you that's the next experience, if you go to gastronomad.net and click the link that says
[02:00:12.720 --> 02:00:18.480]   experiences, you will see all of the upcoming experiences. Still Roman Prosecco.
[02:00:18.480 --> 02:00:25.200]   We can squeeze somebody in. We're maybe full, but you know, if they're a twig listener,
[02:00:25.200 --> 02:00:28.960]   we'll make an exception. If you're, yeah, because they're easy people, they're nice people.
[02:00:28.960 --> 02:00:33.840]   Love the twin army. It's fantastic people. So we will accommodate. But
[02:00:33.840 --> 02:00:40.480]   Provost coming in June and to June, Morocco, and in November, Mexico City at the end of the year
[02:00:40.480 --> 02:00:45.200]   for New Year's Eve, then back to Morocco. April, Morocco. I want to do that one.
[02:00:45.200 --> 02:00:50.320]   That's kind of my list. We're going to also be adding for next year several more. We're looking
[02:00:50.320 --> 02:00:53.680]   at different places, Melbourne, for example, we're talking about. Oh, you should definitely do that.
[02:00:53.680 --> 02:00:58.880]   Yeah. But we don't know yet. We also, I would like to do one in Sonoma, at least once a year.
[02:00:58.880 --> 02:01:03.360]   Sonoma is just such great wine and food. And you can see it. We can see you. We're going to do it
[02:01:03.360 --> 02:01:06.800]   in your house. We're going to do it in your living room. Sure. Come on up. Everybody's invited.
[02:01:06.800 --> 02:01:12.880]   Yeah. So you should come and do one of Yorkshire as well. Yeah, absolutely. Yep. You're
[02:01:12.880 --> 02:01:15.360]   going to have to put it. We're going to have to put it. We're going to put it in where your
[02:01:15.360 --> 02:01:21.120]   pudding comes from and Yorkshire gold, the finest tea in the world. And all things.
[02:01:21.120 --> 02:01:25.360]   And terriers as well. And terriers. Yes. Thank you, everybody, for joining us for Twig
[02:01:25.360 --> 02:01:29.440]   TWIG stands for this weekend. Good nose goodness knows what.
[02:01:31.440 --> 02:01:37.040]   TWIG. You'll find us everywhere. Actually, you know, it's the best place to go is a
[02:01:37.040 --> 02:01:41.760]   Twitter TV slash Twig. And then you can download episodes, subscribe to episodes. If you want to
[02:01:41.760 --> 02:01:49.520]   watch live, we do the show every Wednesday afternoon, about 130 Pacific, 430 Eastern, 2030 UTC.
[02:01:49.520 --> 02:01:55.680]   Is Jeff going to be back next week? Yes. Okay. And Stacy, you're going to be back next week?
[02:01:55.680 --> 02:02:00.320]   I am here next week, but I am not here the 25th because I will be in Sweden.
[02:02:01.200 --> 02:02:07.360]   Oh, yeah. So that's okay because I won't either. I'm this is next week will be my last day
[02:02:07.360 --> 02:02:13.600]   before it'll be the last day before I leave for Japan on Thursday. So sayonara. Not yet.
[02:02:13.600 --> 02:02:23.440]   Not yet. Happy birthday. Thanks for joining us. We'll see you next time on Twig. Bye.
[02:02:23.440 --> 02:02:28.880]   [Music]

