;FFMETADATA1
title=A Phone for Every Country
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=515
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:07.000]   It's time for Twig this week in Google. Jeff is here. Stacey is here. I am here with an apology.
[00:00:07.000 --> 00:00:17.000]   It turns out Google's jigsaw wasn't all it's cracked up to be. We'll also talk about why you shouldn't rely on Google Maps for all of your navigation.
[00:00:17.000 --> 00:00:21.000]   There's a lot more to come this week at Google. It's coming up next.
[00:00:21.000 --> 00:00:25.000]   Netcast you love.
[00:00:25.000 --> 00:00:27.000]   From people you trust.
[00:00:27.000 --> 00:00:32.000]   This is Twig.
[00:00:32.000 --> 00:00:36.000]   This is Twig.
[00:00:36.000 --> 00:00:45.000]   This is Twig. This week in Google, episode 515 recorded Wednesday, July 3, 2019. A phone for every country.
[00:00:45.000 --> 00:00:49.000]   It's time for Twig this week in Google.
[00:00:49.000 --> 00:00:56.000]   It shows it's about nothing. If it's not Google, Facebook, Twitter, the Internet, media and more.
[00:00:56.000 --> 00:01:03.000]   Stacey Higginbotham is here for my new world headquarters just south of the Space Needle.
[00:01:03.000 --> 00:01:04.000]   Hello Stacey.
[00:01:04.000 --> 00:01:05.000]   Is this south?
[00:01:05.000 --> 00:01:07.000]   I don't know. Where is the Space Needle from you?
[00:01:07.000 --> 00:01:09.000]   I think it's west.
[00:01:09.000 --> 00:01:10.000]   Can you see it?
[00:01:10.000 --> 00:01:11.000]   No.
[00:01:11.000 --> 00:01:12.000]   Think?
[00:01:12.000 --> 00:01:12.000]   No.
[00:01:12.000 --> 00:01:13.000]   I can see mountains.
[00:01:13.000 --> 00:01:13.000]   Nice.
[00:01:13.000 --> 00:01:17.000]   I can see the Cascades and the Olympics and the Cascades.
[00:01:17.000 --> 00:01:21.000]   So yeah, so the Space Needle is at least a little west of you.
[00:01:21.000 --> 00:01:22.000]   Somewhere around here.
[00:01:22.000 --> 00:01:25.000]   It's either northwest, southwest or your west west.
[00:01:25.000 --> 00:01:29.000]   Anyway, great to have you. Stacey on IOT.com is her website.
[00:01:29.000 --> 00:01:33.000]   That's where you'll find the newsletter and of course her fabulous IOT podcast.
[00:01:33.000 --> 00:01:38.000]   That's the one she does with Kevin Toful from about crumbooks.com.
[00:01:38.000 --> 00:01:45.000]   And also with us the Tower Knight, Leonard Tau professor for journalistic innovation at the Craig Narmers School of Northern Ireland.
[00:01:45.000 --> 00:01:46.000]   Let's see University of New York.
[00:01:46.000 --> 00:01:48.000]   I'll be back in five minutes for you.
[00:01:48.000 --> 00:01:50.000]   Jeff Jarvis is here.
[00:01:50.000 --> 00:01:52.000]   Hello, Jeff Jarvis.
[00:01:52.000 --> 00:02:01.000]   Hello, Jeff is of course the blogger at buzzmachine.com, the author of many great books, including what would Google do.
[00:02:01.000 --> 00:02:03.000]   That's how he got on the show in the first place.
[00:02:03.000 --> 00:02:05.000]   Many moons ago.
[00:02:05.000 --> 00:02:07.000]   And I haven't let go since.
[00:02:07.000 --> 00:02:08.000]   Thank God.
[00:02:08.000 --> 00:02:10.000]   We haven't let go of you.
[00:02:10.000 --> 00:02:12.000]   We would not let go of you.
[00:02:12.000 --> 00:02:21.000]   So I interviewed a couple of weeks ago, the chief operating officer of a little Google company called Jigsaw.
[00:02:21.000 --> 00:02:27.000]   Which at the time struck me as a really cool thing.
[00:02:27.000 --> 00:02:30.000]   And I know some of the Jigsaw products.
[00:02:30.000 --> 00:02:31.000]   It's jigsaw.google.com.
[00:02:31.000 --> 00:02:34.000]   It's one of their, is it a moonshot?
[00:02:34.000 --> 00:02:35.000]   It's kind of...
[00:02:35.000 --> 00:02:36.000]   No, it's a goody two shoes.
[00:02:36.000 --> 00:02:37.000]   It's a goody two shoes.
[00:02:37.000 --> 00:02:42.000]   And some companies do their, you know, how to how to as technology make the world safer.
[00:02:42.000 --> 00:02:47.000]   And yeah, and I have to say, if you look at the projects that Jigsaw has done like Project Shield, we've talked about that before.
[00:02:47.000 --> 00:02:48.000]   That's an important one.
[00:02:48.000 --> 00:02:53.000]   The VPN for journalists.
[00:02:53.000 --> 00:03:05.000]   They've done some, you know, there's a password alert chrome plug-in perspective as an API that they use machine learning to spot abuse and harassment and online comments.
[00:03:05.000 --> 00:03:09.000]   So they've done some really interesting things.
[00:03:09.000 --> 00:03:12.000]   And then I read an article on motherboard.
[00:03:12.000 --> 00:03:15.000]   Google's Jigsaw was supposed to save the internet behind the scenes.
[00:03:15.000 --> 00:03:17.000]   It became a toxic mess.
[00:03:17.000 --> 00:03:23.000]   I quickly sent a message to the producer of our show saying, did that triagulation air yet?
[00:03:23.000 --> 00:03:24.000]   And he said, yes.
[00:03:24.000 --> 00:03:28.000]   I said, in a way, you, because what I didn't want it to do is air after this article.
[00:03:28.000 --> 00:03:29.000]   Have to have a choice.
[00:03:29.000 --> 00:03:30.000]   Yeah.
[00:03:30.000 --> 00:03:35.000]   Well, if it had aired after the article came out, I would have asked him if I'd, but I didn't know when I interviewed him.
[00:03:35.000 --> 00:03:41.000]   So mostly the criticism seems to be of the CEO, Jared Cohen, who I did not interview.
[00:03:41.000 --> 00:03:43.000]   He's the founder of Jigsaw.
[00:03:43.000 --> 00:03:46.000]   With Eric Schmidt, he's Eric Schmidt's co-author in a book.
[00:03:46.000 --> 00:03:47.000]   He's Eric Schmidt's guy.
[00:03:47.000 --> 00:03:48.000]   Yeah.
[00:03:48.000 --> 00:04:00.000]   And the Accusi- I have to say, now I read this, now maybe because I'm a little biased because I was impressed with their, uh, their COO and we had a great conversation and I thought they're doing some really good work.
[00:04:00.000 --> 00:04:09.000]   So maybe I was a little biased, but when I read this, it felt a little bit, I don't know, it didn't, it felt like a hit piece, to be honest with you.
[00:04:09.000 --> 00:04:13.000]   It did a bit. There was some stuff there, but it, yeah.
[00:04:13.000 --> 00:04:15.000]   I think there's a there there.
[00:04:15.000 --> 00:04:26.000]   I think the challenge is it's hard to, it's hard to show a culture of disrespect or misogyny.
[00:04:26.000 --> 00:04:28.000]   If it's not totally egregious.
[00:04:28.000 --> 00:04:29.000]   Right.
[00:04:29.000 --> 00:04:38.000]   And so I think what's, what the issue that people are upset about here is that it's tough to show is this overall arching culture.
[00:04:38.000 --> 00:04:50.000]   Amid the fact that these guys keep getting put on pedestals and lauded for being saviors of the internet when they're maybe not the most generous or nicest people to work with.
[00:04:50.000 --> 00:04:53.000]   Or, and I'm not defending them.
[00:04:53.000 --> 00:05:07.000]   I think you're right, Stacey, but just to just to play the other game is now these publications that used to, um, alignize everything about the internet, find every opportunity they can to go after everything that might be at all bad on a basis.
[00:05:07.000 --> 00:05:09.000]   It's all bad on any of the big internet companies.
[00:05:09.000 --> 00:05:12.000]   So it's hard to judge these days, which is which.
[00:05:12.000 --> 00:05:13.000]   Yeah.
[00:05:13.000 --> 00:05:21.000]   I wouldn't expect too much of a motherboard because after all they are a digital, they're a digital first company.
[00:05:21.000 --> 00:05:23.000]   It's not like the New York Times or something.
[00:05:23.000 --> 00:05:26.000]   That's exactly what I'm saying though, is that they used to be there to law and technology.
[00:05:26.000 --> 00:05:31.000]   Oh, and now they're saying, oh, and the trend is, oh, we were wrong.
[00:05:31.000 --> 00:05:42.000]   Motherboard has always been fairly skeptical, so they have always been very consumer oriented on security privacy, things like net neutrality and I like them.
[00:05:42.000 --> 00:05:47.000]   I'm just, I'm just saying they haven't always lauded text knowledge.
[00:05:47.000 --> 00:05:48.000]   Oh, I think it's fair.
[00:05:48.000 --> 00:05:49.000]   Yeah, yeah, yeah.
[00:05:49.000 --> 00:06:00.000]   The article is by Lorenzo Franceschi Bikarai, and I should say just for full disclosure that the interview was with Dan Keiserling, triangulation 403.
[00:06:00.000 --> 00:06:07.000]   And not knowing anything about these accusations because it was several weeks ago, I didn't ask him about them.
[00:06:07.000 --> 00:06:13.000]   And maybe I was too much of a cheerleader, but I thought, well, because I had heard a project shield for sure.
[00:06:13.000 --> 00:06:15.000]   We'd even talked about it, I think, on this show.
[00:06:15.000 --> 00:06:18.000]   So there's, I go two ways with this one.
[00:06:18.000 --> 00:06:29.000]   I think on the one hand on the motherboard article, they're lambasting the CEO Cohen for things like going to Papua New Guinea and joining in the show.
[00:06:29.000 --> 00:06:31.000]   And joining in a ritual dance with a native tribe.
[00:06:31.000 --> 00:06:33.000]   And that's one of the things Dan was saying we do.
[00:06:33.000 --> 00:06:35.000]   We try to understand other cultures.
[00:06:35.000 --> 00:06:45.000]   Well, but what they were lambasting for was that he pertain of a, no, he per took of a ritual that involved his body being painted black.
[00:06:45.000 --> 00:06:46.000]   But that was the ritual.
[00:06:46.000 --> 00:06:49.000]   It's not like a college kids doing blackface.
[00:06:49.000 --> 00:06:52.000]   Which then, well, that's, that's the problem.
[00:06:52.000 --> 00:06:54.000]   Was he, was he aware of that enough?
[00:06:54.000 --> 00:06:57.000]   It's that I know, it's the biggest issue.
[00:06:57.000 --> 00:07:00.000]   So that was, that was just at worst tone deaf.
[00:07:00.000 --> 00:07:06.000]   The big issue was that Google was having this crisis around the day more thing.
[00:07:06.000 --> 00:07:13.000]   And his team apparently already was having some issues of its own around women engineers.
[00:07:13.000 --> 00:07:14.000]   Yes.
[00:07:14.000 --> 00:07:17.000]   And he did not respond.
[00:07:17.000 --> 00:07:20.000]   He did not respond to that with a laugherty.
[00:07:20.000 --> 00:07:22.000]   So they were like, eh, he doesn't really care.
[00:07:22.000 --> 00:07:24.000]   And then he sends a tone deaf picture.
[00:07:24.000 --> 00:07:33.000]   So they also published this tweet from Cohen back from eight amazing days in Pakistan meeting with tribal elders.
[00:07:33.000 --> 00:07:34.000]   I don't know.
[00:07:34.000 --> 00:07:36.000]   Is it bad that he's wearing a term?
[00:07:36.000 --> 00:07:38.000]   I think this is exactly what you want to do.
[00:07:38.000 --> 00:07:41.000]   You want to find out what other rights books about being out there in the world.
[00:07:41.000 --> 00:07:42.000]   Yeah.
[00:07:42.000 --> 00:07:45.000]   It's not a non story, but it's not a non non story.
[00:07:45.000 --> 00:07:49.000]   I think there again, here's what I will say.
[00:07:49.000 --> 00:07:54.000]   I think there are illustrations, like the things that they're using to illustrate the problem,
[00:07:54.000 --> 00:07:56.000]   not all of them.
[00:07:56.000 --> 00:08:00.000]   They probably could have had an editor who was a little bit more judicious, who questioned
[00:08:00.000 --> 00:08:03.000]   that a little bit more.
[00:08:03.000 --> 00:08:08.000]   I would think it's powerful, but I imagine many other people would be like shrug.
[00:08:08.000 --> 00:08:14.000]   So things like the comments they made to about their colleague Yasmin Green in terms of.
[00:08:14.000 --> 00:08:15.000]   That was bad.
[00:08:15.000 --> 00:08:16.000]   That was bad.
[00:08:16.000 --> 00:08:17.000]   So things like that.
[00:08:17.000 --> 00:08:18.000]   That was bad.
[00:08:18.000 --> 00:08:22.000]   They said how she dressed inappropriately, but they didn't talk about any of the men, but
[00:08:22.000 --> 00:08:25.560]   they talked about how she dressed or was under dressed.
[00:08:25.560 --> 00:08:26.560]   Under dressed.
[00:08:26.560 --> 00:08:28.840]   When visiting, I remember it was a mosque or something.
[00:08:28.840 --> 00:08:30.560]   Somewhere where it had been inappropriate.
[00:08:30.560 --> 00:08:38.000]   They also report that in the women's room at Jigsaw, there's a basket with mascara and
[00:08:38.000 --> 00:08:46.480]   moisturizer in it, which they say is in case you cry at work or maybe it's just maybe just
[00:08:46.480 --> 00:08:48.560]   moisturizer and mascara.
[00:08:48.560 --> 00:08:49.560]   I don't know.
[00:08:49.560 --> 00:08:53.360]   If you go to a bathroom and Google, they have an unbelievable array of supplies.
[00:08:53.360 --> 00:08:54.360]   It's mouthwash.
[00:08:54.360 --> 00:08:55.360]   Yes.
[00:08:55.360 --> 00:08:59.200]   Yeah, I know I'm always impressed actually by the.
[00:08:59.200 --> 00:09:00.600]   I expect them to get grilled cream.
[00:09:00.600 --> 00:09:02.680]   Do all your grooming in a Google bathroom.
[00:09:02.680 --> 00:09:03.680]   I'm honest.
[00:09:03.680 --> 00:09:04.680]   Yes.
[00:09:04.680 --> 00:09:05.680]   I'm sure people do.
[00:09:05.680 --> 00:09:06.680]   Yeah.
[00:09:06.680 --> 00:09:07.680]   And I'm sure some people are in their cars.
[00:09:07.680 --> 00:09:09.480]   I bring it up.
[00:09:09.480 --> 00:09:11.520]   I should point out the article.
[00:09:11.520 --> 00:09:17.240]   I think that if there is a toxic culture, especially toxic culture for women at Jigsaw,
[00:09:17.240 --> 00:09:18.240]   that's terrible.
[00:09:18.240 --> 00:09:24.240]   They should fix that because after all, Jigsaw is something that Google set up to show how
[00:09:24.240 --> 00:09:27.080]   technology can help people.
[00:09:27.080 --> 00:09:29.360]   So that is a bad thing.
[00:09:29.360 --> 00:09:33.600]   And then I posted Cohen's letter to the staff right below.
[00:09:33.600 --> 00:09:37.600]   Oh, let me look at it because I haven't read that yet.
[00:09:37.600 --> 00:09:38.600]   Breaking.
[00:09:38.600 --> 00:09:39.600]   Breaking.
[00:09:39.600 --> 00:09:46.320]   He's frustrated and disappointed with his team's toxic culture.
[00:09:46.320 --> 00:09:48.520]   I hope he's not blaming them.
[00:09:48.520 --> 00:09:51.960]   Yeah, the headline is a little weird.
[00:09:51.960 --> 00:09:52.960]   Okay.
[00:09:52.960 --> 00:09:55.040]   A few hours after the story, here's the email.
[00:09:55.040 --> 00:09:58.800]   The details of the story have hit me hard and I'm deeply disappointed for all of you
[00:09:58.800 --> 00:10:02.080]   to see our culture characterized in this way.
[00:10:02.080 --> 00:10:03.080]   We haven't always.
[00:10:03.080 --> 00:10:04.280]   It's a little passive, isn't it?
[00:10:04.280 --> 00:10:05.280]   Yeah.
[00:10:05.280 --> 00:10:06.280]   Yeah.
[00:10:06.280 --> 00:10:07.280]   We haven't always gotten everything right.
[00:10:07.280 --> 00:10:10.280]   We haven't really been able to take responsibility seriously.
[00:10:10.280 --> 00:10:12.560]   I'm committed to ensuring we continue to improve.
[00:10:12.560 --> 00:10:17.680]   In 2017, two years ago, he says, Jigsaw identified a number of areas where we had room for improvement,
[00:10:17.680 --> 00:10:21.560]   conducted an internal review and implemented nearly all of the recommendations.
[00:10:21.560 --> 00:10:22.560]   Okay.
[00:10:22.560 --> 00:10:28.880]   On the personal, I'm frustrated and disappointed to read about some people's personal experience
[00:10:28.880 --> 00:10:33.920]   in our team that fell short of our commitment to provide a positive workplace for everyone.
[00:10:33.920 --> 00:10:38.840]   I'm sure there are people, ex-employees of any company, including our own, who would
[00:10:38.840 --> 00:10:41.680]   have a tail to tell.
[00:10:41.680 --> 00:10:46.080]   However, lopsided it is.
[00:10:46.080 --> 00:10:47.600]   Yeah.
[00:10:47.600 --> 00:10:49.600]   Another form of employee.
[00:10:49.600 --> 00:10:51.160]   I think it's to work on.
[00:10:51.160 --> 00:10:52.160]   There's no question about that.
[00:10:52.160 --> 00:10:53.160]   They're saying it.
[00:10:53.160 --> 00:10:54.160]   I don't have any.
[00:10:54.160 --> 00:10:55.160]   How scandalous this is.
[00:10:55.160 --> 00:10:57.160]   I apologize for them.
[00:10:57.160 --> 00:10:58.160]   Yeah.
[00:10:58.160 --> 00:11:05.160]   I think apologies.
[00:11:05.160 --> 00:11:08.800]   I think it's more like saying, "Oh, hey, this is an issue.
[00:11:08.800 --> 00:11:10.120]   How can I deal with it?"
[00:11:10.120 --> 00:11:13.320]   Yes, you need to acknowledge something and move on.
[00:11:13.320 --> 00:11:20.040]   I also think it's good for, as a journalist reading this, or as anybody who is reading
[00:11:20.040 --> 00:11:25.640]   something like this, I may look at Cohen and take him with a bit more of a grain of salt.
[00:11:25.640 --> 00:11:31.880]   Honestly, the way we treat founders in the media and just in general in the tech press,
[00:11:31.880 --> 00:11:37.200]   good lord, we could probably do with more grains of salt because there is definitely
[00:11:37.200 --> 00:11:43.240]   a tendency not to ask critical questions.
[00:11:43.240 --> 00:11:48.480]   I will say one thing, which is the reason we interviewed Dan Keasurling from Jigsaw was
[00:11:48.480 --> 00:11:50.800]   that Google approached us.
[00:11:50.800 --> 00:11:57.160]   They approached us early in June and said, "We'd like to offer you our COO for an interview."
[00:11:57.160 --> 00:12:00.160]   I said, "Yeah, let's set it up."
[00:12:00.160 --> 00:12:05.600]   We did, and then an interview aired or was conducted June 21st.
[00:12:05.600 --> 00:12:10.000]   Does the story talks about them getting a whiff of the story?
[00:12:10.000 --> 00:12:11.000]   What was the date of that?
[00:12:11.000 --> 00:12:15.320]   I was wondering if that was just, I'm like, "Oh, I think you were part of their campaign
[00:12:15.320 --> 00:12:16.320]   to not be people."
[00:12:16.320 --> 00:12:17.320]   I think we were.
[00:12:17.320 --> 00:12:20.760]   I'm looking at the email he sent out today as you've seen the Vice Story.
[00:12:20.760 --> 00:12:26.560]   Dan briefed you on, Dan has the guy interviewed, a week or so ago was published.
[00:12:26.560 --> 00:12:34.160]   I feel a little manipulated that this was part of a media tour to set the stage before
[00:12:34.160 --> 00:12:36.120]   this story came out.
[00:12:36.120 --> 00:12:37.120]   Really?
[00:12:37.120 --> 00:12:41.720]   You talked to a PR person and you felt manipulated?
[00:12:41.720 --> 00:12:43.040]   That never happens.
[00:12:43.040 --> 00:12:46.200]   It's true that, first of all, I have a natural aversion to PR people.
[00:12:46.200 --> 00:12:50.120]   I avoid talking to them all the time.
[00:12:50.120 --> 00:12:54.360]   When they offered somebody from a group within an alphabet, I thought it was doing something
[00:12:54.360 --> 00:12:55.360]   very interesting.
[00:12:55.360 --> 00:12:57.120]   I did jump at it.
[00:12:57.120 --> 00:12:59.080]   I didn't know any of this happened.
[00:12:59.080 --> 00:13:02.400]   It was probably orchestrated to happen before the article came out.
[00:13:02.400 --> 00:13:06.640]   I had no idea there was any controversy at all.
[00:13:06.640 --> 00:13:09.200]   It was a very positive interview.
[00:13:09.200 --> 00:13:10.200]   I apologize.
[00:13:10.200 --> 00:13:11.560]   I feel like perhaps I will.
[00:13:11.560 --> 00:13:13.560]   We were manipulated a little bit.
[00:13:13.560 --> 00:13:16.800]   I did, when I saw this story, I contacted the producer.
[00:13:16.800 --> 00:13:19.800]   I said, "If it hasn't run, do not run it."
[00:13:19.800 --> 00:13:22.080]   It already had run.
[00:13:22.080 --> 00:13:24.000]   That's odd too.
[00:13:24.000 --> 00:13:34.720]   Well, I guess the best thing to do if I had the resources would be a call on back and
[00:13:34.720 --> 00:13:36.720]   say, "Let's do a full-on."
[00:13:36.720 --> 00:13:39.520]   That's what you would do in that situation.
[00:13:39.520 --> 00:13:44.080]   If it hadn't run, you'd say, "Hi, let's address this."
[00:13:44.080 --> 00:13:46.560]   If they refused to, then I would not run it.
[00:13:46.560 --> 00:13:47.800]   It had already run.
[00:13:47.800 --> 00:13:49.720]   It had run a few weeks before.
[00:13:49.720 --> 00:13:51.200]   Now, you can.
[00:13:51.200 --> 00:13:52.640]   You can do a follow-up now.
[00:13:52.640 --> 00:13:54.040]   You can do a follow-up.
[00:13:54.040 --> 00:13:55.560]   You can try for a follow-up.
[00:13:55.560 --> 00:14:01.160]   You can even, I know that if something happens in a situation like this, I might even put
[00:14:01.160 --> 00:14:05.320]   an update at the end of the story since they're so close in time and because I have a strong
[00:14:05.320 --> 00:14:09.480]   suspicion I had been manipulated, I might even put an update at the bottom of the story.
[00:14:09.480 --> 00:14:11.320]   A week after this ran.
[00:14:11.320 --> 00:14:12.320]   That's a good idea.
[00:14:12.320 --> 00:14:14.240]   Let's do that at least and put a link to it.
[00:14:14.240 --> 00:14:16.120]   But then if you do that, here's the "it."
[00:14:16.120 --> 00:14:17.120]   Then you have to follow-up.
[00:14:17.120 --> 00:14:18.120]   I understand.
[00:14:18.120 --> 00:14:20.120]   Yeah, you have to give them a chance.
[00:14:20.120 --> 00:14:21.120]   For their comments.
[00:14:21.120 --> 00:14:22.520]   Well, in a way, this is the follow-up.
[00:14:22.520 --> 00:14:25.680]   I should be a professor of journalism.
[00:14:25.680 --> 00:14:29.680]   You are teaching me the craft because I never studied the craft.
[00:14:29.680 --> 00:14:31.520]   I don't know anything about it.
[00:14:31.520 --> 00:14:32.520]   So I thank you.
[00:14:32.520 --> 00:14:34.520]   Oh, there's a craft.
[00:14:34.520 --> 00:14:38.880]   Oh, there's a craft.
[00:14:38.880 --> 00:14:39.880]   Everybody really cares.
[00:14:39.880 --> 00:14:44.360]   I just care because what's going on?
[00:14:44.360 --> 00:14:49.600]   No, I think that's part of being a credible reporter and not just a shill for companies
[00:14:49.600 --> 00:14:51.040]   and what they're after.
[00:14:51.040 --> 00:14:52.680]   That's just me.
[00:14:52.680 --> 00:14:56.840]   Well, and as Eric and our chatroom points out, everybody who you're interviewing is
[00:14:56.840 --> 00:14:58.840]   always has an agenda.
[00:14:58.840 --> 00:14:59.840]   Yeah.
[00:14:59.840 --> 00:15:03.280]   Mostly just to push a book or a podcast.
[00:15:03.280 --> 00:15:09.880]   Well, when this happens, when stuff like this happens, I blame myself for not doing more
[00:15:09.880 --> 00:15:16.720]   reporting whenever I've done an interview and then especially a bigger interview, not
[00:15:16.720 --> 00:15:17.720]   just random.
[00:15:17.720 --> 00:15:20.200]   I'm like, "Dang it.
[00:15:20.200 --> 00:15:25.280]   If I had Googled this, or talked to some people beforehand, okay, I'd Google everybody
[00:15:25.280 --> 00:15:26.280]   before I talked to them."
[00:15:26.280 --> 00:15:27.280]   Well, I did that.
[00:15:27.280 --> 00:15:30.560]   I didn't find any negative stuff at all.
[00:15:30.560 --> 00:15:31.560]   Right.
[00:15:31.560 --> 00:15:32.560]   Yeah.
[00:15:32.560 --> 00:15:33.560]   So, yeah.
[00:15:33.560 --> 00:15:35.560]   I don't know.
[00:15:35.560 --> 00:15:36.560]   Yeah.
[00:15:36.560 --> 00:15:42.920]   So, but anyway, I think at least by doing this, talking about it, we've kind of mentioned
[00:15:42.920 --> 00:15:44.520]   the issue and...
[00:15:44.520 --> 00:15:49.000]   And there are now six, I think the latest study was six PR professionals to every journalist
[00:15:49.000 --> 00:15:51.560]   out there and probably more right now.
[00:15:51.560 --> 00:15:55.000]   Judging by the amount of email I get, I would say that's accurate.
[00:15:55.000 --> 00:15:59.240]   Oh, we should talk about something that Google did.
[00:15:59.240 --> 00:16:02.400]   Hold on one second.
[00:16:02.400 --> 00:16:06.280]   Kevin Topals says, "Hi, Kevin."
[00:16:06.280 --> 00:16:07.280]   Okay.
[00:16:07.280 --> 00:16:10.120]   Well, we'll get to it maybe.
[00:16:10.120 --> 00:16:12.120]   I had this horrifying Google experience.
[00:16:12.120 --> 00:16:17.000]   No, I don't have anything pressing on the agenda.
[00:16:17.000 --> 00:16:22.600]   So last month, Google on G Suite changed some security features to make things more secure.
[00:16:22.600 --> 00:16:27.880]   And they brought this up in an email and they sent to people and they said, "Hey, we're
[00:16:27.880 --> 00:16:29.360]   now doing something."
[00:16:29.360 --> 00:16:34.600]   They call it less secure accounts, which is really awkward phrasing.
[00:16:34.600 --> 00:16:38.600]   They were less secure apps in your Google account.
[00:16:38.600 --> 00:16:39.600]   Oh.
[00:16:39.600 --> 00:16:44.840]   So, they introduced this because if the site does not meet their security standards, Google
[00:16:44.840 --> 00:16:48.080]   is going to block anyone who's trying to sign in to use your account.
[00:16:48.080 --> 00:16:49.080]   That's right.
[00:16:49.080 --> 00:16:51.760]   And this is tied to their efforts on Gmail.
[00:16:51.760 --> 00:16:54.000]   And you remember where this came from?
[00:16:54.000 --> 00:16:55.000]   Yes.
[00:16:55.000 --> 00:16:56.000]   Oh, go ahead.
[00:16:56.000 --> 00:16:57.000]   Go ahead.
[00:16:57.000 --> 00:16:58.240]   No, no, no, no.
[00:16:58.240 --> 00:17:02.800]   It was an exploit in Google Docs that came.
[00:17:02.800 --> 00:17:08.160]   I can't remember if it was a malicious extension or I can't remember the exact details of it,
[00:17:08.160 --> 00:17:12.040]   but Google said at that time, "Yeah, we're going to be, we're going to really crack down
[00:17:12.040 --> 00:17:14.480]   on the kind of stuff."
[00:17:14.480 --> 00:17:19.880]   So here's the page that they sent you to control access to less secure apps.
[00:17:19.880 --> 00:17:25.760]   And it's in a way saying to you, "Look, we're going to turn some stuff off.
[00:17:25.760 --> 00:17:28.560]   You should know ahead of time, right?
[00:17:28.560 --> 00:17:30.480]   Is that what the intent of it was?"
[00:17:30.480 --> 00:17:31.640]   So that was the intent.
[00:17:31.640 --> 00:17:34.360]   I looked at it and I said, "Okay."
[00:17:34.360 --> 00:17:40.160]   But then starting after they rolled this out, I apparently, I have a Gmail and a G Suite
[00:17:40.160 --> 00:17:41.400]   account.
[00:17:41.400 --> 00:17:48.560]   And my Gmail account apparently did not know because it has two-factor authentication.
[00:17:48.560 --> 00:17:52.480]   It was, it stopped pulling from my Gmail account.
[00:17:52.480 --> 00:17:53.480]   Oh.
[00:17:53.480 --> 00:17:55.920]   Maybe there was an error, but I was still getting emails.
[00:17:55.920 --> 00:17:58.320]   So I was like, "Google, you're confused."
[00:17:58.320 --> 00:18:01.600]   So I ignored it for a little while, which you should never do, but you know, this is
[00:18:01.600 --> 00:18:02.600]   me.
[00:18:02.600 --> 00:18:03.600]   I was moving.
[00:18:03.600 --> 00:18:04.600]   Life was crazy.
[00:18:04.600 --> 00:18:06.320]   And then I dealt with it yesterday.
[00:18:06.320 --> 00:18:08.040]   I was like, "Gosh, let's dig into this error.
[00:18:08.040 --> 00:18:10.360]   What the heck is happening?"
[00:18:10.360 --> 00:18:14.240]   Apparently all of my email from my personal Gmail account that I sent to my main email
[00:18:14.240 --> 00:18:16.680]   account was just stopped.
[00:18:16.680 --> 00:18:18.680]   And so to fix it, I had to go back and--
[00:18:18.680 --> 00:18:22.960]   That's ironic that G Suite decided that Gmail was insecure.
[00:18:22.960 --> 00:18:26.080]   Even if your less secure apps are on email product.
[00:18:26.080 --> 00:18:27.400]   I don't think it was less secure.
[00:18:27.400 --> 00:18:32.880]   I think something they changed made it what I had to do because it's got two-factor.
[00:18:32.880 --> 00:18:33.880]   It wasn't--
[00:18:33.880 --> 00:18:38.080]   Reestablish the connection using probably an app password or something like that.
[00:18:38.080 --> 00:18:39.960]   That is exactly what I had to do.
[00:18:39.960 --> 00:18:47.200]   But figuring that out took forever because, you know, I'm like, Google's documentation
[00:18:47.200 --> 00:18:49.280]   is not made for humans.
[00:18:49.280 --> 00:18:53.160]   I just-- their original email, not made for humans.
[00:18:53.160 --> 00:18:54.160]   But it--
[00:18:54.160 --> 00:18:55.160]   No, it's very simple.
[00:18:55.160 --> 00:18:58.240]   From the admin console homepage, you go to security basic settings to see security on
[00:18:58.240 --> 00:18:59.240]   the homepage.
[00:18:59.240 --> 00:19:01.600]   You might have to click more controls at the bottom.
[00:19:01.600 --> 00:19:05.200]   Under less secure apps, go to settings for less secure apps on the left.
[00:19:05.200 --> 00:19:08.640]   Select an organizational unit where you want to manage access to less secure apps.
[00:19:08.640 --> 00:19:13.600]   If you don't select an organizational unit, it'll apply to the entire top level organization,
[00:19:13.600 --> 00:19:14.800]   et cetera, et cetera.
[00:19:14.800 --> 00:19:17.520]   Disable access to less secure apps for all users recommended.
[00:19:17.520 --> 00:19:21.000]   Allow users to manage access to less secure apps and force access to less secure apps
[00:19:21.000 --> 00:19:23.040]   for all users not recommended.
[00:19:23.040 --> 00:19:24.440]   On the bottom right, click save.
[00:19:24.440 --> 00:19:26.160]   Well, that was easy.
[00:19:26.160 --> 00:19:28.280]   How hard could that be?
[00:19:28.280 --> 00:19:33.560]   I would say more of the "hey, by the way, this thing where a community is going to
[00:19:33.560 --> 00:19:35.000]   break your email."
[00:19:35.000 --> 00:19:37.240]   Everything you've set up, you're going to have to go back through it.
[00:19:37.240 --> 00:19:39.160]   Well this reminds me a little bit of what happens at--
[00:19:39.160 --> 00:19:40.160]   I think that would've been nice.
[00:19:40.160 --> 00:19:41.160]   If this didn't happen--
[00:19:41.160 --> 00:19:44.280]   If this didn't happen-- remember where they changed their API and everything broke and
[00:19:44.280 --> 00:19:46.760]   this-- I think this happens from time to time where you--
[00:19:46.760 --> 00:19:52.440]   If you've got-- and see you are smart and you do stuff-- and I bet you a lot of our audience
[00:19:52.440 --> 00:19:56.920]   does-- stuff like that where you have one account go to another account and you have
[00:19:56.920 --> 00:20:01.880]   set up systems, but they're fragile because they rely on an ecosystem that's constantly
[00:20:01.880 --> 00:20:03.800]   changing, especially with Google.
[00:20:03.800 --> 00:20:10.720]   Oh, this is why I wrote about API documentation and the importance of APIs and API wars.
[00:20:10.720 --> 00:20:11.720]   That's a fun topic.
[00:20:11.720 --> 00:20:12.720]   Oh, what fun.
[00:20:12.720 --> 00:20:13.720]   Yeah, okay.
[00:20:13.720 --> 00:20:15.760]   But that happened to me.
[00:20:15.760 --> 00:20:21.480]   So if you guys have that problem and you're maybe wondering if you didn't spend the 30
[00:20:21.480 --> 00:20:25.560]   minutes to figure out what the heck was happening and why and how to fix it, now you sort of
[00:20:25.560 --> 00:20:27.400]   know.
[00:20:27.400 --> 00:20:34.120]   And now it's time for our journalism segment, The Guardian.
[00:20:34.120 --> 00:20:41.680]   Google accused of flipping the bird at New Zealand laws after a murder.
[00:20:41.680 --> 00:20:47.160]   Apparently they broke suppression orders related to a murder case of a British backpacker
[00:20:47.160 --> 00:20:49.120]   in Auckland.
[00:20:49.120 --> 00:20:56.520]   The name of the defendant was suppressed, but Google sent it out in an email, "What's trending
[00:20:56.520 --> 00:20:59.480]   in New Zealand?"
[00:20:59.480 --> 00:21:02.120]   Went out to thousands of people.
[00:21:02.120 --> 00:21:06.600]   This is an example I think exactly of what you just talked about Stacey, even Google,
[00:21:06.600 --> 00:21:08.600]   has automated systems.
[00:21:08.600 --> 00:21:10.320]   They don't know what the hell's going on.
[00:21:10.320 --> 00:21:11.320]   Oh, no.
[00:21:11.320 --> 00:21:18.400]   Google executives met with New Zealand's Justice Minister in Wellington to discuss the breach
[00:21:18.400 --> 00:21:21.600]   and said it'll be dealt with.
[00:21:21.600 --> 00:21:25.520]   They also talked to a Prime Minister, Jacinda Ardern, who is still waiting, by the way,
[00:21:25.520 --> 00:21:27.680]   for Marianne Williamson's call.
[00:21:27.680 --> 00:21:31.720]   However, when Justice officials followed up with Google in March, and again, last week,
[00:21:31.720 --> 00:21:35.280]   the company said, "We're not going to change anything."
[00:21:35.280 --> 00:21:40.480]   Because you consider this, it's New Zealand being not unlike China and Iran and other
[00:21:40.480 --> 00:21:45.080]   places thinking you can build a wall around New Zealand and keep your information out.
[00:21:45.080 --> 00:21:48.800]   And sorry, Kiwis, but you can't.
[00:21:48.800 --> 00:21:51.120]   Oh, this friggin' sunlight.
[00:21:51.120 --> 00:21:52.720]   They can't.
[00:21:52.720 --> 00:21:55.880]   And don't worry, nobody's watching.
[00:21:55.880 --> 00:21:58.000]   Everybody listens to our shows.
[00:21:58.000 --> 00:22:01.400]   Poor Jeff agonizes over his lighting all the time.
[00:22:01.400 --> 00:22:05.560]   Earlier, Jeff didn't get involved in the previous conversation because he was setting up table
[00:22:05.560 --> 00:22:07.560]   lamps and floor lamps.
[00:22:07.560 --> 00:22:10.560]   The carster was telling me to do things, but I was trying to do things.
[00:22:10.560 --> 00:22:11.560]   No, let him.
[00:22:11.560 --> 00:22:12.560]   It doesn't matter.
[00:22:12.560 --> 00:22:13.560]   Let Jeff be Jeff.
[00:22:13.560 --> 00:22:14.560]   He's doing what works.
[00:22:14.560 --> 00:22:15.560]   Let me look like crap.
[00:22:15.560 --> 00:22:17.560]   It's just been yell for us anyway.
[00:22:17.560 --> 00:22:18.560]   Twitter.
[00:22:18.560 --> 00:22:19.560]   No.
[00:22:19.560 --> 00:22:20.560]   Can't you get a better camera?
[00:22:20.560 --> 00:22:21.560]   And here's another thing.
[00:22:21.560 --> 00:22:22.560]   Now while they're yelling at Twitter.
[00:22:22.560 --> 00:22:27.000]   Yeah, pay no attention to people yelling at Twitter on Twitter.
[00:22:27.000 --> 00:22:29.720]   That is the default.
[00:22:29.720 --> 00:22:31.800]   That's what Twitter is made for, it turns out.
[00:22:31.800 --> 00:22:32.800]   Oh, by the way.
[00:22:32.800 --> 00:22:35.360]   He's ready for 500 different internets now.
[00:22:35.360 --> 00:22:36.360]   Yes.
[00:22:36.360 --> 00:22:42.760]   And they can think that people aren't bright enough that they can go and go to, I don't
[00:22:42.760 --> 00:22:47.960]   know, thegardian.com and find the name of the Miss Crayant.
[00:22:47.960 --> 00:22:48.960]   Right?
[00:22:48.960 --> 00:22:50.600]   It's called an internet.
[00:22:50.600 --> 00:22:52.800]   It's called living in the whole globe.
[00:22:52.800 --> 00:22:56.880]   And I know that New Zealand is still tender about the horrible crimes that happened there,
[00:22:56.880 --> 00:22:59.400]   but it's not the internet's fault.
[00:22:59.400 --> 00:23:03.040]   And doing things like this is not going to fix it.
[00:23:03.040 --> 00:23:08.760]   These are our own domestic laws and we do have an expectation they'll be upheld.
[00:23:08.760 --> 00:23:13.080]   So Google's response is disappointing.
[00:23:13.080 --> 00:23:18.880]   Now we need to consider what the next steps will be for New Zealand.
[00:23:18.880 --> 00:23:21.760]   It's not clear what New Zealand's options are.
[00:23:21.760 --> 00:23:25.920]   Earlier this year they announced they were exploring how it could make Facebook and Google
[00:23:25.920 --> 00:23:29.960]   pay more tax on income made within the country.
[00:23:29.960 --> 00:23:32.360]   So that's the other agenda.
[00:23:32.360 --> 00:23:37.960]   The other thing we got to recognize, the agendas that people have in this.
[00:23:37.960 --> 00:23:43.280]   It's not just admitting their agendas that they find Google and Facebook to be competitors.
[00:23:43.280 --> 00:23:45.880]   And so they're, they're, this is from the New Zealand government though.
[00:23:45.880 --> 00:23:46.880]   This is the government affairs.
[00:23:46.880 --> 00:23:47.880]   Yeah.
[00:23:47.880 --> 00:23:49.280]   They're also talking about getting more tax revenue.
[00:23:49.280 --> 00:23:50.280]   Yeah.
[00:23:50.280 --> 00:23:51.280]   That's what he's interested in.
[00:23:51.280 --> 00:23:55.320]   There's others, other agendas at work here.
[00:23:55.320 --> 00:23:56.320]   Yeah.
[00:23:56.320 --> 00:23:57.320]   Stuff.
[00:23:57.320 --> 00:23:58.320]   Yeah.
[00:23:58.320 --> 00:23:59.320]   It's hard.
[00:23:59.320 --> 00:24:04.520]   It's difficult to suppress information news in the newsroom.
[00:24:04.520 --> 00:24:05.520]   It is.
[00:24:05.520 --> 00:24:06.520]   It's going to be extremely difficult.
[00:24:06.520 --> 00:24:12.720]   And I recognize that we in the US are used to having what we call the perp walk where
[00:24:12.720 --> 00:24:15.080]   if you are arrested, you are not guilty of anything yet.
[00:24:15.080 --> 00:24:18.440]   It's the night you're arrested, but the press is alerted.
[00:24:18.440 --> 00:24:25.400]   And as you are walked from the precinct to the wagon to go to the jail, the press is
[00:24:25.400 --> 00:24:26.800]   all there to take your picture.
[00:24:26.800 --> 00:24:28.800]   That is shocking in Europe.
[00:24:28.800 --> 00:24:33.800]   They don't do that for every profile.
[00:24:33.800 --> 00:24:35.440]   Only high profile.
[00:24:35.440 --> 00:24:41.400]   What I'm saying is we believe here in an openness that says that we don't believe in secret
[00:24:41.400 --> 00:24:45.560]   arrest or arrest without publication of the name because it's the way to protect you.
[00:24:45.560 --> 00:24:47.400]   In Europe, they believe the opposite.
[00:24:47.400 --> 00:24:52.280]   They believe that if you're arrested, but you're not guilty yet, then you have a right
[00:24:52.280 --> 00:24:55.200]   to not having your name out yet.
[00:24:55.200 --> 00:24:59.880]   The irony is that what almost could be other way around because Europe having had its history,
[00:24:59.880 --> 00:25:03.600]   you think that openness would be a virtue there.
[00:25:03.600 --> 00:25:08.560]   And in the US, not having had that kind of history, you think, well, let's try to let
[00:25:08.560 --> 00:25:10.840]   and we believe so strongly and innocent until proven guilty.
[00:25:10.840 --> 00:25:14.760]   We think we reverse that, but we believe in free speech here and openness.
[00:25:14.760 --> 00:25:16.760]   And so there are different cultures that occur.
[00:25:16.760 --> 00:25:23.680]   And that's the fundamental problem, which is that global internet, local culture, and
[00:25:23.680 --> 00:25:26.000]   it's a difficult balancing act.
[00:25:26.000 --> 00:25:30.840]   And it really doesn't, the technology doesn't lend itself to saying, okay, we're going to
[00:25:30.840 --> 00:25:36.200]   fill, we're going to make sure that New Zealand isn't offended while serving cultural norms
[00:25:36.200 --> 00:25:37.760]   in the rest of the world.
[00:25:37.760 --> 00:25:38.760]   That's the most impossible.
[00:25:38.760 --> 00:25:40.400]   I don't know how you do that.
[00:25:40.400 --> 00:25:41.880]   There's another case.
[00:25:41.880 --> 00:25:45.480]   There's neither does Google, but neither does Google, but at the same time, I completely
[00:25:45.480 --> 00:25:49.840]   respect the desire for individual localities to preserve their culture.
[00:25:49.840 --> 00:25:54.840]   That's of course, but that's the, that goes back pre-internet, it goes back to television,
[00:25:54.840 --> 00:25:59.280]   that goes back to American hegemony and entertainment.
[00:25:59.280 --> 00:26:02.440]   You know, the Hollywood and all the movies we've been putting out for the last hundred
[00:26:02.440 --> 00:26:06.400]   years have clearly influenced culture all over the world.
[00:26:06.400 --> 00:26:11.120]   When I was a kid and we went to Europe in the mid 60s, you couldn't get Coca-Cola.
[00:26:11.120 --> 00:26:16.880]   You had to get weird, imitative drinks.
[00:26:16.880 --> 00:26:19.200]   When I was in the East Germany, I call it commi-cola.
[00:26:19.200 --> 00:26:20.200]   I'm having Afrikaola.
[00:26:20.200 --> 00:26:25.480]   I had cinnamon in the bottom of it.
[00:26:25.480 --> 00:26:28.600]   If you wanted to know what the baseball scores were in the United States, you had to wait
[00:26:28.600 --> 00:26:32.080]   three days for the Herald Tribune to print them.
[00:26:32.080 --> 00:26:36.640]   But the world has changed and we are now all connected.
[00:26:36.640 --> 00:26:42.080]   I think that's probably for the better, but I understand one of the effects is accents
[00:26:42.080 --> 00:26:45.560]   are going away, regionalisms in the United States are going away.
[00:26:45.560 --> 00:26:46.840]   Nationalism is rising.
[00:26:46.840 --> 00:26:54.760]   Because there's a fear that there's a, you know, between the globalization and immigration,
[00:26:54.760 --> 00:26:58.480]   the old powers in each country are fearing the challenge they're having.
[00:26:58.480 --> 00:27:02.400]   And it's part not just to the internet, it's also globalization, but it's both technology
[00:27:02.400 --> 00:27:03.400]   based.
[00:27:03.400 --> 00:27:04.400]   Here's another example.
[00:27:04.400 --> 00:27:11.480]   Facebook being fine in Germany for under-reporting, they're $2 million, which is a pretty small
[00:27:11.480 --> 00:27:13.480]   fine for a company as big as Facebook.
[00:27:13.480 --> 00:27:15.640]   You know, in all of our lives we have our...
[00:27:15.640 --> 00:27:16.640]   Stop it, Mark.
[00:27:16.640 --> 00:27:20.640]   You know, these things happen $2 million to $2 million.
[00:27:20.640 --> 00:27:23.520]   It's not a big deal.
[00:27:23.520 --> 00:27:27.880]   For apparently, this I think has to do with Nazis.
[00:27:27.880 --> 00:27:28.880]   It's not...
[00:27:28.880 --> 00:27:29.880]   No, this is...
[00:27:29.880 --> 00:27:32.160]   Well, yes, this hates because this is a "that's day gay."
[00:27:32.160 --> 00:27:35.480]   That's too long a word even for me to say "law."
[00:27:35.480 --> 00:27:36.800]   And there's two angles to this.
[00:27:36.800 --> 00:27:42.280]   One is that they're required to report and YouTube you will see reported a huge greater
[00:27:42.280 --> 00:27:45.200]   number than Facebook did.
[00:27:45.200 --> 00:27:48.200]   So that looks like a variance, it looks like Facebook's scanning.
[00:27:48.200 --> 00:27:51.160]   But the other problem with this, which I've talked about in the show before, is that the
[00:27:51.160 --> 00:27:55.240]   order in which these companies do things, they tend to test something first against their
[00:27:55.240 --> 00:27:56.240]   community standards.
[00:27:56.240 --> 00:27:59.760]   And if it fails their community standards, they take it down and they're done.
[00:27:59.760 --> 00:28:03.720]   They move on, they're busy, they've got lots of things to have to check.
[00:28:03.720 --> 00:28:08.880]   But if they then didn't go to the next test and say, "Well, was this illegal and should
[00:28:08.880 --> 00:28:10.400]   we have reported it?"
[00:28:10.400 --> 00:28:16.200]   In which case, they're being asked to be the cop, the judge, the jury, and so on.
[00:28:16.200 --> 00:28:17.200]   Well, then wait a minute.
[00:28:17.200 --> 00:28:18.200]   Is the one word?
[00:28:18.200 --> 00:28:21.520]   No, no, they're being asked to judge whether or not they should report it.
[00:28:21.520 --> 00:28:22.520]   The network that's...
[00:28:22.520 --> 00:28:23.520]   I'm not sure.
[00:28:23.520 --> 00:28:24.520]   Let me read this.
[00:28:24.520 --> 00:28:26.960]   I was not going to be one more second.
[00:28:26.960 --> 00:28:27.960]   One more second.
[00:28:27.960 --> 00:28:28.960]   Right.
[00:28:28.960 --> 00:28:33.440]   So the issue here is that they're told to take down things that are manifestly illegal
[00:28:33.440 --> 00:28:35.520]   and or obviously illegal, let's call it.
[00:28:35.520 --> 00:28:36.520]   In the case...
[00:28:36.520 --> 00:28:39.720]   They're being asked to make two judges hate speech or fake news.
[00:28:39.720 --> 00:28:43.560]   They're being asked to make two judgments, which is...
[00:28:43.560 --> 00:28:45.920]   I heard this from a Google attorney, which is really interesting.
[00:28:45.920 --> 00:28:46.920]   Is there one?
[00:28:46.920 --> 00:28:47.920]   Yes, is it illegal or not?
[00:28:47.920 --> 00:28:51.320]   The second is, if it's minor or illegal, the government doesn't care if you report it
[00:28:51.320 --> 00:28:52.560]   because it's a minor or legal.
[00:28:52.560 --> 00:28:54.560]   The question is, is it obviously illegal?
[00:28:54.560 --> 00:28:55.720]   Is it manifestly illegal?
[00:28:55.720 --> 00:29:00.320]   So they also have to make a gradation judgment here, which again, they're not qualified to
[00:29:00.320 --> 00:29:01.320]   do it.
[00:29:01.320 --> 00:29:02.320]   This is why I'm pushing for...
[00:29:02.320 --> 00:29:03.320]   Let me stop you.
[00:29:03.320 --> 00:29:04.320]   Sorry, Stacey, go ahead.
[00:29:04.320 --> 00:29:07.200]   Let me stop both of you because I think you both got it wrong.
[00:29:07.200 --> 00:29:08.200]   But this is from CNN.
[00:29:08.200 --> 00:29:10.000]   So maybe CNN has it wrong.
[00:29:10.000 --> 00:29:11.000]   I don't know.
[00:29:11.000 --> 00:29:12.000]   This is what they say.
[00:29:12.000 --> 00:29:16.200]   The Network Enforcement Act requires social media platforms to publish reports every six
[00:29:16.200 --> 00:29:23.400]   months detailing the number of complaints of illegal content received.
[00:29:23.400 --> 00:29:25.840]   But you know what the number of complaints is.
[00:29:25.840 --> 00:29:26.840]   That's a number.
[00:29:26.840 --> 00:29:30.560]   Now then there's a second clause where they're required to remove hate speech and fake news.
[00:29:30.560 --> 00:29:36.480]   But what the German Federal Office of Justice said is that Facebook's report listed only
[00:29:36.480 --> 00:29:38.840]   a fraction of the complaints...
[00:29:38.840 --> 00:29:45.080]   Let me try to explain that one because the key phrase there is illegal content again.
[00:29:45.080 --> 00:29:47.360]   They didn't get a complaint.
[00:29:47.360 --> 00:29:50.800]   Hair Schmidt didn't say, "I believe this is unlawful."
[00:29:50.800 --> 00:29:52.560]   Hair Schmidt said, "No, this is bad."
[00:29:52.560 --> 00:29:57.760]   Nope, they're not asking Facebook to make a determination for this particular fine.
[00:29:57.760 --> 00:29:59.400]   They do have the right to do that.
[00:29:59.400 --> 00:30:05.240]   They're saying Facebook made it difficult for its users to find the form to flag illegal
[00:30:05.240 --> 00:30:06.240]   content.
[00:30:06.240 --> 00:30:10.760]   And thus, the number of complaints was lower than it should have been.
[00:30:10.760 --> 00:30:12.400]   It distorted a public...
[00:30:12.400 --> 00:30:15.120]   It had a distorted public image about the size of the illegal...
[00:30:15.120 --> 00:30:17.000]   That's mixing two things here though.
[00:30:17.000 --> 00:30:18.880]   One is they have a bad UI and that's...
[00:30:18.880 --> 00:30:22.080]   I think that's probably right and that's a complaint.
[00:30:22.080 --> 00:30:29.400]   Whether that results in an under-reporting report that is their fault is a different question,
[00:30:29.400 --> 00:30:30.400]   actually.
[00:30:30.400 --> 00:30:31.400]   Well, they're mixing...
[00:30:31.400 --> 00:30:33.600]   That's what they're accusing them of and that's what they find them for.
[00:30:33.600 --> 00:30:38.480]   I should point out, yes, there is also the requirement to remove hate speech and fake
[00:30:38.480 --> 00:30:44.440]   news within 24 hours of it being flagged and the penalties go up to $60 million.
[00:30:44.440 --> 00:30:48.880]   So it strikes me that the $2 million fine does make sense that it's more about...
[00:30:48.880 --> 00:30:52.560]   Though, you got to make it easier to flag, then we'll talk about whether you're doing
[00:30:52.560 --> 00:30:53.560]   your job.
[00:30:53.560 --> 00:30:55.280]   Stacy, I'm sorry.
[00:30:55.280 --> 00:30:56.760]   I wanted to finish my point before.
[00:30:56.760 --> 00:30:58.000]   I want the world to know.
[00:30:58.000 --> 00:30:59.600]   I wasn't interrupting Stacy in that case.
[00:30:59.600 --> 00:31:01.000]   I do other cases and I'm bad at that.
[00:31:01.000 --> 00:31:03.320]   Twitter says you interrupted all the time.
[00:31:03.320 --> 00:31:09.880]   In that case, I was just begging to continue a nuanced point, Stacy.
[00:31:09.880 --> 00:31:12.600]   I truly, I interrupt you too.
[00:31:12.600 --> 00:31:14.240]   Sometimes I'm just like, "Oh, I can't keep pointing."
[00:31:14.240 --> 00:31:15.240]   That's what we do.
[00:31:15.240 --> 00:31:16.240]   You're wrong.
[00:31:16.240 --> 00:31:21.640]   No, can I just say that I encourage that people often hit me for that as well because
[00:31:21.640 --> 00:31:26.040]   the worst, in my opinion, the worst kind of show like this is where everybody politely
[00:31:26.040 --> 00:31:28.640]   waits for the other person to finish.
[00:31:28.640 --> 00:31:30.600]   It really slows the pace down.
[00:31:30.600 --> 00:31:33.080]   So, over-talking is bad.
[00:31:33.080 --> 00:31:38.840]   I will say sometimes, yeah, sometimes you guys do over-talk me a little bit.
[00:31:38.840 --> 00:31:39.840]   Yes.
[00:31:39.840 --> 00:31:41.120]   But I'm becoming much more active about it.
[00:31:41.120 --> 00:31:45.920]   I think it's less being the nicest person that I am and assuming the best in people.
[00:31:45.920 --> 00:31:50.080]   I think it's because you all have this rapport that just gets you going.
[00:31:50.080 --> 00:31:52.640]   Well, you're fitting the rapport now, baby.
[00:31:52.640 --> 00:31:54.640]   I'm working at it.
[00:31:54.640 --> 00:31:55.640]   Baby?
[00:31:55.640 --> 00:31:57.640]   Oh, shut up, sweetheart.
[00:31:57.640 --> 00:32:02.920]   I was going to use a different word.
[00:32:02.920 --> 00:32:12.880]   No, I actually want to split the baby and say, "It's good for everybody to jump in."
[00:32:12.880 --> 00:32:15.840]   And then not over-talk because that is hard for somebody to listen to.
[00:32:15.840 --> 00:32:17.960]   You can't hear what's going on.
[00:32:17.960 --> 00:32:20.880]   And so pause if somebody has something and jumps in.
[00:32:20.880 --> 00:32:23.720]   But I don't want people to hold back.
[00:32:23.720 --> 00:32:26.760]   That's not a good talk show.
[00:32:26.760 --> 00:32:29.880]   Anyway, you were saying, Stacey.
[00:32:29.880 --> 00:32:32.240]   I have totally forgot what I was saying.
[00:32:32.240 --> 00:32:33.960]   I mean, it's terrible that way.
[00:32:33.960 --> 00:32:35.760]   My evil plan has worked.
[00:32:35.760 --> 00:32:36.760]   Anyway...
[00:32:36.760 --> 00:32:38.040]   I didn't have the coffee before this.
[00:32:38.040 --> 00:32:40.320]   And now I'm like...
[00:32:40.320 --> 00:32:42.760]   And the main reason I brought it up is to just underscore this.
[00:32:42.760 --> 00:32:45.360]   Now, maybe it isn't the case of this cultural differences.
[00:32:45.360 --> 00:32:46.360]   In this case...
[00:32:46.360 --> 00:32:49.320]   Well, here's another one that I just put up on the rundown.
[00:32:49.320 --> 00:32:56.600]   The Polish court, this came from Daphne Keller, who's a brilliant legal mind about intermediate
[00:32:56.600 --> 00:32:57.600]   liability.
[00:32:57.600 --> 00:33:01.880]   The Polish court is forcing Facebook to keep content up.
[00:33:01.880 --> 00:33:08.840]   Freedom of speech includes the ability of what not to say, to be forced to speak.
[00:33:08.840 --> 00:33:09.840]   It's not freedom of speech.
[00:33:09.840 --> 00:33:13.520]   Now, we'll get an argument here about wells in the public squares, the utility and all
[00:33:13.520 --> 00:33:14.520]   that.
[00:33:14.520 --> 00:33:16.920]   But it is not yet, but they're being told to keep things up.
[00:33:16.920 --> 00:33:23.680]   And in this country, we have those who in power in Congress and in the White House,
[00:33:23.680 --> 00:33:28.280]   we're trying to say that Facebook and Google should be forced to keep up certain speech
[00:33:28.280 --> 00:33:32.400]   at the same time they're being told to take down other speech.
[00:33:32.400 --> 00:33:33.880]   Yeah, yeah, yeah.
[00:33:33.880 --> 00:33:34.880]   And this...
[00:33:34.880 --> 00:33:36.680]   The problem is that there's so many stakeholders.
[00:33:36.680 --> 00:33:37.680]   There's, you know, different...
[00:33:37.680 --> 00:33:38.680]   Oh.
[00:33:38.680 --> 00:33:39.680]   It's 200-some countries.
[00:33:39.680 --> 00:33:42.680]   Each one has different rules, regulations, expectations.
[00:33:42.680 --> 00:33:46.080]   I think the problem is really...
[00:33:46.080 --> 00:33:49.960]   Honestly, it's what we're bemoaning, which is the fragmentation of the Internet some
[00:33:49.960 --> 00:33:54.040]   ways, so you've got Facebook, which is where a lot of people go to get their information
[00:33:54.040 --> 00:34:00.080]   and may even think of that as the Internet, when in reality, it's not, or it shouldn't
[00:34:00.080 --> 00:34:01.080]   be, and we shouldn't...
[00:34:01.080 --> 00:34:02.080]   Right.
[00:34:02.080 --> 00:34:03.080]   I mean...
[00:34:03.080 --> 00:34:04.080]   Good point.
[00:34:04.080 --> 00:34:05.080]   If you...
[00:34:05.080 --> 00:34:10.920]   You didn't put up a website with any information you want, but we've got to figure out how
[00:34:10.920 --> 00:34:11.920]   to think about...
[00:34:11.920 --> 00:34:19.920]   I don't know if it's a monopoly, if it's network effects, or how to think about mitigating
[00:34:19.920 --> 00:34:27.760]   the power that platforms have to show people, to influence people, I guess.
[00:34:27.760 --> 00:34:30.160]   But is that their role, right?
[00:34:30.160 --> 00:34:31.160]   No, and I think...
[00:34:31.160 --> 00:34:37.360]   Do we want them to actively decide what we should know and not know?
[00:34:37.360 --> 00:34:40.560]   So in no, I don't...
[00:34:40.560 --> 00:34:46.640]   I have a really hard time with this, because the crux of the matter is people do go to
[00:34:46.640 --> 00:34:52.040]   the terms and trust for them to be the source of certain kinds of information, right?
[00:34:52.040 --> 00:34:55.840]   So if you say to a restaurant of Facebook, it's like, "I hate you, I'm never putting
[00:34:55.840 --> 00:34:58.760]   your stuff on here."
[00:34:58.760 --> 00:35:04.200]   Then that restaurant might be hosed because people go looking for that kind of information
[00:35:04.200 --> 00:35:06.000]   on Facebook.
[00:35:06.000 --> 00:35:07.760]   I don't, but other people do.
[00:35:07.760 --> 00:35:08.760]   So I think there's...
[00:35:08.760 --> 00:35:09.760]   You know...
[00:35:09.760 --> 00:35:11.240]   There's both a...
[00:35:11.240 --> 00:35:16.200]   The life is that we want it to be versus the life as it is, and we have to build rules
[00:35:16.200 --> 00:35:19.920]   and think about the implications of life as it is.
[00:35:19.920 --> 00:35:25.240]   But then we should also be trying to make life more like what we want it to be if we
[00:35:25.240 --> 00:35:31.080]   actually do want it to be this open place where anybody can find information.
[00:35:31.080 --> 00:35:32.080]   And...
[00:35:32.080 --> 00:35:33.080]   So Stacey, here's the case.
[00:35:33.080 --> 00:35:34.080]   Sorry.
[00:35:34.080 --> 00:35:35.080]   Here's the case.
[00:35:35.080 --> 00:35:38.040]   I think that gets to your points that...
[00:35:38.040 --> 00:35:41.920]   And I've been talking to Clara Wardle at First Raft and others about this.
[00:35:41.920 --> 00:35:46.720]   And my institution is go to the health and vaccine stuff.
[00:35:46.720 --> 00:35:51.200]   And the story on the rundown is this snippy version of this.
[00:35:51.200 --> 00:35:55.240]   But Facebook is trying to figure out how to deal with anti-vax stuff.
[00:35:55.240 --> 00:35:59.440]   They're trying to deal with health stuff.
[00:35:59.440 --> 00:36:02.240]   And is it their obligation to...
[00:36:02.240 --> 00:36:07.640]   Well, their choices they have is to downrank.
[00:36:07.640 --> 00:36:12.200]   If they put up notices, does that make bad stuff more popular?
[00:36:12.200 --> 00:36:14.280]   Do they kill things?
[00:36:14.280 --> 00:36:19.600]   Do they actively put up contrary content from sources?
[00:36:19.600 --> 00:36:22.440]   And those are their realm of choices.
[00:36:22.440 --> 00:36:26.280]   And they're going to set precedence there that will get demanded in other areas like
[00:36:26.280 --> 00:36:28.000]   politics and government.
[00:36:28.000 --> 00:36:29.000]   And so...
[00:36:29.000 --> 00:36:30.000]   But this is a matter of health.
[00:36:30.000 --> 00:36:32.200]   And you hear me screaming about health matters.
[00:36:32.200 --> 00:36:33.200]   So what do we think?
[00:36:33.200 --> 00:36:36.640]   What do you think is the right beginning point for that discussion?
[00:36:36.640 --> 00:36:41.680]   I think for something like health.
[00:36:41.680 --> 00:36:44.040]   So think about a public good.
[00:36:44.040 --> 00:36:46.200]   And I could screw myself thinking about this.
[00:36:46.200 --> 00:36:49.040]   But just right now before I think this all the way through.
[00:36:49.040 --> 00:36:51.320]   I'm just gonna...
[00:36:51.320 --> 00:36:52.920]   I'm thinking out loud here.
[00:36:52.920 --> 00:36:54.400]   Public good.
[00:36:54.400 --> 00:37:01.760]   I think the benefits outweigh the risks of maybe going too far in one direction.
[00:37:01.760 --> 00:37:06.480]   So I think you should go too far in the direction of educating given we have actual science
[00:37:06.480 --> 00:37:08.480]   at a public good at stake.
[00:37:08.480 --> 00:37:16.760]   So I think things that downvote, downvote, downgrade, vaccine, anti-vaccine propaganda
[00:37:16.760 --> 00:37:17.840]   kind of stuff.
[00:37:17.840 --> 00:37:24.560]   I think promoting things or making a really valid and forceful insertion of, "Hey, here's
[00:37:24.560 --> 00:37:26.680]   what the CDC says."
[00:37:26.680 --> 00:37:30.720]   Or "Here's whatever government, wherever your IP address is from.
[00:37:30.720 --> 00:37:34.880]   Here's what your government says about this."
[00:37:34.880 --> 00:37:39.040]   In the case of something else, I don't know.
[00:37:39.040 --> 00:37:43.720]   In reading this Polish court thing, this was a drug...
[00:37:43.720 --> 00:37:47.040]   This is really fascinating because it was a nonprofit that advocates.
[00:37:47.040 --> 00:37:50.640]   It's trying to prevent people from dying of drug abuse.
[00:37:50.640 --> 00:37:56.840]   And they warned, I guess, about using certain drugs in hot climates is bad.
[00:37:56.840 --> 00:37:59.880]   But maybe they didn't go so far as to say, "Don't do drugs."
[00:37:59.880 --> 00:38:04.000]   Maybe they just said, "Hey, don't do these drugs when it's hot."
[00:38:04.000 --> 00:38:05.000]   Because they're realist.
[00:38:05.000 --> 00:38:09.160]   And people were like, "Nope, that violates community standards because you're advocating
[00:38:09.160 --> 00:38:10.960]   for drug use."
[00:38:10.960 --> 00:38:13.640]   Do you think maybe...
[00:38:13.640 --> 00:38:18.120]   Do you think maybe this is only a temporary problem that people...
[00:38:18.120 --> 00:38:21.120]   No, this is what government's for.
[00:38:21.120 --> 00:38:24.760]   This is democracy and action.
[00:38:24.760 --> 00:38:31.160]   We have these arguments all the time, not just on platforms, but we have them in public.
[00:38:31.160 --> 00:38:32.160]   We have them...
[00:38:32.160 --> 00:38:33.160]   Ingo.
[00:38:33.160 --> 00:38:38.480]   In voting laws, think about, is it right to let a man walk down the street without a shirt
[00:38:38.480 --> 00:38:39.480]   on?
[00:38:39.480 --> 00:38:41.040]   Could a woman walk down the street without a shirt on?
[00:38:41.040 --> 00:38:44.880]   And some communities, yes, and others, no.
[00:38:44.880 --> 00:38:49.680]   The problem is, we're doing it in the digital world and the digital world's accessible outside
[00:38:49.680 --> 00:38:51.560]   of our insular community standards.
[00:38:51.560 --> 00:38:54.400]   And we're going to have to adapt to that.
[00:38:54.400 --> 00:38:55.400]   So it's...
[00:38:55.400 --> 00:38:56.400]   Leo, it's about...
[00:38:56.400 --> 00:38:58.400]   The law is coming from the law.
[00:38:58.400 --> 00:38:59.400]   Well, no.
[00:38:59.400 --> 00:39:00.400]   Well, so two things.
[00:39:00.400 --> 00:39:02.040]   One, it's not a technology story.
[00:39:02.040 --> 00:39:03.040]   It's not a technology problem.
[00:39:03.040 --> 00:39:04.680]   It is a human problem.
[00:39:04.680 --> 00:39:10.040]   And Stacey's points are about society, negotiating its norms and standards and laws to be able
[00:39:10.040 --> 00:39:11.040]   to handle this.
[00:39:11.040 --> 00:39:12.040]   Right.
[00:39:12.040 --> 00:39:14.320]   So the platform shouldn't do anything about it at all.
[00:39:14.320 --> 00:39:16.040]   They should just let us work it out.
[00:39:16.040 --> 00:39:18.040]   Well, the platforms are all so far...
[00:39:18.040 --> 00:39:19.040]   They should let us leave.
[00:39:19.040 --> 00:39:20.040]   Right.
[00:39:20.040 --> 00:39:21.040]   Oh, yeah.
[00:39:21.040 --> 00:39:23.520]   I mean, anti-vaxxers are not just on Facebook.
[00:39:23.520 --> 00:39:25.200]   That's not the only place they exist.
[00:39:25.200 --> 00:39:26.200]   Amen.
[00:39:26.200 --> 00:39:32.680]   Honestly, I feel like there is a little bit of a vestigial trust of anything you see in
[00:39:32.680 --> 00:39:36.680]   print that I think in a generation will be gone completely, right?
[00:39:36.680 --> 00:39:37.680]   So may I...
[00:39:37.680 --> 00:39:38.680]   Yes.
[00:39:38.680 --> 00:39:39.680]   Gutenberg moment.
[00:39:39.680 --> 00:39:40.680]   Yes.
[00:39:40.680 --> 00:39:49.120]   When print came along, no one trusted print because it was so ill-managed, so uncontrolled,
[00:39:49.120 --> 00:39:52.720]   all kinds of crap appeared in print, all kinds of conspiracy appeared in print.
[00:39:52.720 --> 00:39:56.280]   Right was less reliable than what was known than as rumor.
[00:39:56.280 --> 00:39:57.280]   Right.
[00:39:57.280 --> 00:40:01.920]   Rumor, hearing things from your friends, people you trusted and relied upon was far more reliable
[00:40:01.920 --> 00:40:02.920]   than print.
[00:40:02.920 --> 00:40:07.400]   And it shifted over time where print gained more credibility.
[00:40:07.400 --> 00:40:13.080]   And right now we're trying to negotiate our worldview about where does expertise come
[00:40:13.080 --> 00:40:14.080]   from?
[00:40:14.080 --> 00:40:15.080]   Yes.
[00:40:15.080 --> 00:40:18.720]   And we're not used to a notion of distributed expertise.
[00:40:18.720 --> 00:40:20.720]   We're used to a notion of centralized expertise.
[00:40:20.720 --> 00:40:21.720]   Yes.
[00:40:21.720 --> 00:40:22.720]   That's right.
[00:40:22.720 --> 00:40:23.720]   I feel like that's vestigial.
[00:40:23.720 --> 00:40:25.600]   That will wear off.
[00:40:25.600 --> 00:40:27.560]   We've got that left over from...
[00:40:27.560 --> 00:40:29.520]   It could take literally a century.
[00:40:29.520 --> 00:40:31.520]   Yeah, I might have taken Gutenberg a century.
[00:40:31.520 --> 00:40:34.320]   I think we're moving a little faster than that.
[00:40:34.320 --> 00:40:35.320]   That's contemporary.
[00:40:35.320 --> 00:40:38.760]   It could take way more than a century because I mean, what was it?
[00:40:38.760 --> 00:40:39.760]   14 something?
[00:40:39.760 --> 00:40:41.600]   And then we didn't get to the Enlightenment until...
[00:40:41.600 --> 00:40:42.600]   1450.
[00:40:42.600 --> 00:40:43.600]   17?
[00:40:43.600 --> 00:40:44.600]   Yeah.
[00:40:44.600 --> 00:40:45.600]   17?
[00:40:45.600 --> 00:40:46.600]   It took a 30 years war or two.
[00:40:46.600 --> 00:40:47.600]   You know, so...
[00:40:47.600 --> 00:40:48.600]   It could be a huge...
[00:40:48.600 --> 00:40:49.600]   That's my issue.
[00:40:49.600 --> 00:40:50.600]   You can't fix stupid.
[00:40:50.600 --> 00:40:51.600]   And...
[00:40:51.600 --> 00:40:52.600]   That's not true.
[00:40:52.600 --> 00:40:53.600]   Yes, you can.
[00:40:53.600 --> 00:40:55.040]   How do you fix stupid?
[00:40:55.040 --> 00:40:56.600]   You educate people.
[00:40:56.600 --> 00:41:00.800]   And yes, there are still going to be people who refuse to believe, quote...
[00:41:00.800 --> 00:41:02.800]   I guess what I'm saying is...
[00:41:02.800 --> 00:41:03.800]   You used to be like...
[00:41:03.800 --> 00:41:08.480]   You can't fix people who are going to post that crap, but you can fix.
[00:41:08.480 --> 00:41:10.680]   You can fix people who are going to read that crap.
[00:41:10.680 --> 00:41:11.680]   Yes.
[00:41:11.680 --> 00:41:13.920]   And it's not media literacy.
[00:41:13.920 --> 00:41:15.000]   It's something different.
[00:41:15.000 --> 00:41:16.720]   Media literacy is media centric.
[00:41:16.720 --> 00:41:18.480]   It is teaching people to be enlightened.
[00:41:18.480 --> 00:41:22.880]   It's just understanding that just because you read it on a Facebook post doesn't make
[00:41:22.880 --> 00:41:23.880]   it true.
[00:41:23.880 --> 00:41:26.440]   It seems to me kind of a fundamental thing.
[00:41:26.440 --> 00:41:27.440]   You're not going to stop.
[00:41:27.440 --> 00:41:29.960]   I swear to God, I don't care what Facebook does.
[00:41:29.960 --> 00:41:33.040]   You're not going to stop people from writing stupid stuff on Facebook.
[00:41:33.040 --> 00:41:34.040]   It's just not going to happen.
[00:41:34.040 --> 00:41:36.920]   Right, but you don't have to advance it.
[00:41:36.920 --> 00:41:40.360]   And you can also add for other people reading it.
[00:41:40.360 --> 00:41:41.360]   That's true.
[00:41:41.360 --> 00:41:44.880]   There is some culpability because Facebook likes to advance that stuff because that stuff
[00:41:44.880 --> 00:41:46.600]   enhances engagement.
[00:41:46.600 --> 00:41:51.320]   I'm too big Facebook had this problem because people love reading this stuff and they read
[00:41:51.320 --> 00:41:55.120]   more of it and the algorithm says, "Oh good, okay, we feed the more stuff like this."
[00:41:55.120 --> 00:41:56.120]   And that is a problem.
[00:41:56.120 --> 00:41:57.120]   The algorithm is a problem.
[00:41:57.120 --> 00:41:58.120]   Well, this is...
[00:41:58.120 --> 00:41:59.360]   Quality is a very hard problem.
[00:41:59.360 --> 00:42:02.960]   And full disclosure as I raised money from my school from Facebook to work on a project
[00:42:02.960 --> 00:42:07.600]   called NewsQA, News Quality aggregator, or even just for what you could identify as
[00:42:07.600 --> 00:42:13.880]   news sites, which is a very small corpus, 15 to 30,000 sites in the US.
[00:42:13.880 --> 00:42:20.040]   How do you cope with mechanisms to judge quality and credibility among that very small
[00:42:20.040 --> 00:42:21.040]   corpus?
[00:42:21.040 --> 00:42:23.640]   Not zillions of sites, but just that.
[00:42:23.640 --> 00:42:28.320]   So you've got this problem of community standards varying across the globe.
[00:42:28.320 --> 00:42:33.120]   You've got this problem of stupid posters, stupid readers.
[00:42:33.120 --> 00:42:34.120]   We're going to be...
[00:42:34.120 --> 00:42:35.120]   You're right.
[00:42:35.120 --> 00:42:36.600]   This is not going to go away right away.
[00:42:36.600 --> 00:42:42.760]   I have high hopes that a generation that grows up, internet native, will be a little smarter.
[00:42:42.760 --> 00:42:46.000]   I think there's already evidence that the kids who grew up with the internet are smarter
[00:42:46.000 --> 00:42:47.720]   about what they read.
[00:42:47.720 --> 00:42:51.440]   As you've said many times, it's people my age who keep posting this stupid stuff on
[00:42:51.440 --> 00:42:52.440]   the internet.
[00:42:52.440 --> 00:42:53.440]   Yeah.
[00:42:53.440 --> 00:42:56.000]   So maybe there is some hope.
[00:42:56.000 --> 00:43:01.720]   Did we talk last week about Twitter hiding posts?
[00:43:01.720 --> 00:43:02.720]   I don't think we did.
[00:43:02.720 --> 00:43:03.720]   So it was the new...
[00:43:03.720 --> 00:43:06.160]   I think it happened the day after our show.
[00:43:06.160 --> 00:43:11.560]   So what we did do is we showed the little snippet from President Trump on Fox talking
[00:43:11.560 --> 00:43:17.160]   to Maria Bartiromo about how Twitter that keeps my follower count artificially low,
[00:43:17.160 --> 00:43:19.400]   some sort of crazy thing.
[00:43:19.400 --> 00:43:24.280]   And then Twitter the very next day kind of made it so.
[00:43:24.280 --> 00:43:29.760]   Maybe for tweets belonging to political figures, verified users in accounts with more than
[00:43:29.760 --> 00:43:31.440]   100,000 followers.
[00:43:31.440 --> 00:43:35.480]   If the tweet violates Twitter's rules...
[00:43:35.480 --> 00:43:37.280]   And by the way, it's not obvious.
[00:43:37.280 --> 00:43:41.360]   It's a team of people from across the company will decide whether it's a matter of public
[00:43:41.360 --> 00:43:42.440]   interest.
[00:43:42.440 --> 00:43:44.320]   If it's not, of course, they'll just take it down.
[00:43:44.320 --> 00:43:49.840]   But if it is, a light gray blocks will appear before the tweet, notifying users.
[00:43:49.840 --> 00:43:53.640]   It's in violation, but it's going to stay available because it's part of the public.
[00:43:53.640 --> 00:43:55.680]   I think hide is the wrong verb there.
[00:43:55.680 --> 00:43:56.840]   No, in fact, high light.
[00:43:56.840 --> 00:43:57.840]   It is flagging it.
[00:43:57.840 --> 00:44:01.680]   It's flagging it before you see it.
[00:44:01.680 --> 00:44:03.560]   Who's that going to collect view?
[00:44:03.560 --> 00:44:04.560]   This is that.
[00:44:04.560 --> 00:44:07.160]   Other rules about a base of behavior apply to this tweet.
[00:44:07.160 --> 00:44:09.760]   However, Twitter is determined to maybe the public's interest for the tweet to remain
[00:44:09.760 --> 00:44:10.960]   available.
[00:44:10.960 --> 00:44:12.800]   Click here to read it.
[00:44:12.800 --> 00:44:18.160]   Now, of course, on the other hand, they say it won't be showing up in search results,
[00:44:18.160 --> 00:44:19.160]   that kind of thing.
[00:44:19.160 --> 00:44:21.640]   They're going to try to slow it spread down.
[00:44:21.640 --> 00:44:27.280]   Nevertheless, if you're following the president and you see something like that in the president's
[00:44:27.280 --> 00:44:29.760]   tweet stream, who's not going to click view?
[00:44:29.760 --> 00:44:30.760]   Me.
[00:44:30.760 --> 00:44:32.360]   When it happens 100 times?
[00:44:32.360 --> 00:44:33.360]   Yeah, no.
[00:44:33.360 --> 00:44:34.360]   Maybe.
[00:44:34.360 --> 00:44:35.960]   So I refuse to follow him.
[00:44:35.960 --> 00:44:36.960]   No, you got it.
[00:44:36.960 --> 00:44:38.880]   You got a problem with my tweet deck.
[00:44:38.880 --> 00:44:41.320]   No, I have a column, my tweet deck.
[00:44:41.320 --> 00:44:42.320]   I don't bother anymore.
[00:44:42.320 --> 00:44:45.680]   No, I don't bother because I've heard it all.
[00:44:45.680 --> 00:44:48.120]   As I said to Ari Melbera, that is very subject.
[00:44:48.120 --> 00:44:49.600]   I hate people to repeat themselves.
[00:44:49.600 --> 00:44:51.120]   I hate people to repeat themselves.
[00:44:51.120 --> 00:44:52.120]   I hate people to repeat themselves.
[00:44:52.120 --> 00:44:53.680]   All right, but I've heard it all.
[00:44:53.680 --> 00:44:57.280]   Nevertheless, he's the most powerful person in the world.
[00:44:57.280 --> 00:45:01.080]   I think it's important to know what he's saying, even if it's stupid or repetitive.
[00:45:01.080 --> 00:45:02.080]   No.
[00:45:02.080 --> 00:45:04.280]   What if he says, hey, just a word of warning.
[00:45:04.280 --> 00:45:06.400]   I'm going to bomb Iran tomorrow.
[00:45:06.400 --> 00:45:07.640]   I mean, you want to find it?
[00:45:07.640 --> 00:45:08.640]   We'll see that.
[00:45:08.640 --> 00:45:10.000]   We'll see that in the New York Times.
[00:45:10.000 --> 00:45:12.600]   If my beat were to cover it, I would have to have it.
[00:45:12.600 --> 00:45:13.600]   But it's not my beat.
[00:45:13.600 --> 00:45:14.600]   I see.
[00:45:14.600 --> 00:45:15.600]   See the way to the news reports it.
[00:45:15.600 --> 00:45:16.600]   Yep.
[00:45:16.600 --> 00:45:17.960]   By the way, there needs to be a filter.
[00:45:17.960 --> 00:45:20.840]   The news loves to report the president's tweets.
[00:45:20.840 --> 00:45:24.600]   I know, but I can glance at it and say, oh, this is one of those.
[00:45:24.600 --> 00:45:25.600]   Yeah.
[00:45:25.600 --> 00:45:26.840]   I'm like, OK, screw that.
[00:45:26.840 --> 00:45:31.880]   MSNBC has taken to underlining, highlighting with yellow, the part of the tweet they want
[00:45:31.880 --> 00:45:32.880]   you to read.
[00:45:32.880 --> 00:45:33.880]   It's so long.
[00:45:33.880 --> 00:45:34.880]   Yes.
[00:45:34.880 --> 00:45:36.400]   So they just they highlighted it.
[00:45:36.400 --> 00:45:37.400]   I'm saying, wait a minute.
[00:45:37.400 --> 00:45:38.400]   Slow down.
[00:45:38.400 --> 00:45:39.400]   I want to read the whole thing.
[00:45:39.400 --> 00:45:43.800]   But every now and then, sometimes I enjoy things like his Prince of Wales, the.
[00:45:43.800 --> 00:45:44.800]   Definitely.
[00:45:44.800 --> 00:45:45.800]   Yeah.
[00:45:45.800 --> 00:45:46.800]   It's like presidents.
[00:45:46.800 --> 00:45:48.840]   They're just like us.
[00:45:48.840 --> 00:45:50.240]   No, they're not.
[00:45:50.240 --> 00:45:52.440]   No, they're not.
[00:45:52.440 --> 00:45:54.520]   They're not.
[00:45:54.520 --> 00:45:56.080]   Let's just all watch the parade tomorrow.
[00:45:56.080 --> 00:46:00.440]   We'll know.
[00:46:00.440 --> 00:46:07.080]   I actually, honestly, there was an article in Vogue yesterday that says, how can you people
[00:46:07.080 --> 00:46:11.640]   just pretend nothing horrible is happening on the border and just go about your daily
[00:46:11.640 --> 00:46:12.640]   lives?
[00:46:12.640 --> 00:46:13.640]   And I'm having a hard time.
[00:46:13.640 --> 00:46:14.640]   I'll be frank with you.
[00:46:14.640 --> 00:46:15.640]   Yeah.
[00:46:15.640 --> 00:46:22.520]   Having living, having lived there and left, I'm like, oh, what do you do?
[00:46:22.520 --> 00:46:23.520]   What do you do?
[00:46:23.520 --> 00:46:25.440]   I sent races a thousand dollars.
[00:46:25.440 --> 00:46:27.480]   I keep sending them money.
[00:46:27.480 --> 00:46:28.880]   I don't know what else to do, right?
[00:46:28.880 --> 00:46:32.120]   I mean, do you take to the streets?
[00:46:32.120 --> 00:46:38.400]   I mean, like, do I feel like a Germany, like maybe a Germany Nazi Germany?
[00:46:38.400 --> 00:46:41.400]   Yes, I do.
[00:46:41.400 --> 00:46:42.400]   Yes.
[00:46:42.400 --> 00:46:46.080]   And I don't, I honestly don't know what to do.
[00:46:46.080 --> 00:46:47.080]   And it's very difficult.
[00:46:47.080 --> 00:46:53.280]   Well, one thing I would argue is this, if I may, and I've made my political views very
[00:46:53.280 --> 00:46:57.360]   clear and I'm not going to push now, but just full disclosure is that I'm a fan of Kamala
[00:46:57.360 --> 00:46:59.360]   Harris.
[00:46:59.360 --> 00:47:02.560]   And by the way, a lot of friends, McCall, to be saying, oh, yeah, you were.
[00:47:02.560 --> 00:47:06.160]   Now she's now she's the flavor of the month.
[00:47:06.160 --> 00:47:09.480]   But what I was saying to people is I don't care who you're for.
[00:47:09.480 --> 00:47:13.280]   I don't think you should wait and say, let's, well, there's one all the side of that.
[00:47:13.280 --> 00:47:16.920]   Now is the time in a democracy to be part of the discussion and get out there.
[00:47:16.920 --> 00:47:17.920]   I tweeted about this today.
[00:47:17.920 --> 00:47:23.080]   I had about a six tweet string today saying, here's why I'm for her, but go after your
[00:47:23.080 --> 00:47:24.440]   own person.
[00:47:24.440 --> 00:47:25.760]   Don't wait till the end.
[00:47:25.760 --> 00:47:27.080]   Get in there and get involved.
[00:47:27.080 --> 00:47:30.200]   That's the Democratic and somebody came in and said, well, you know, on Twitter, good
[00:47:30.200 --> 00:47:33.000]   old Twitter said Warren's my first choice.
[00:47:33.000 --> 00:47:37.320]   But if if Harris wins, I'll be crazy about her being president, not more before her and
[00:47:37.320 --> 00:47:39.280]   somebody else came back and said the opposite.
[00:47:39.280 --> 00:47:40.680]   And there was a group hug.
[00:47:40.680 --> 00:47:42.840]   And that's what the discussion we need to have right now.
[00:47:42.840 --> 00:47:45.680]   So part of this is just get involved.
[00:47:45.680 --> 00:47:52.160]   Push people to run for office in your, in your districts and in your States.
[00:47:52.160 --> 00:47:53.160]   Get involved.
[00:47:53.160 --> 00:47:57.520]   And I don't mean that in the sense of, you had to get out there and vote and do the old
[00:47:57.520 --> 00:47:58.520]   things.
[00:47:58.520 --> 00:48:02.600]   I mean, we need this national conversation and we need to be out there and we can't get
[00:48:02.600 --> 00:48:07.320]   into a point of having, of being too tired.
[00:48:07.320 --> 00:48:09.600]   It's pretty disgusting, pretty discouraging.
[00:48:09.600 --> 00:48:11.080]   It's very discouraging.
[00:48:11.080 --> 00:48:12.080]   Very.
[00:48:12.080 --> 00:48:16.760]   Stacy, if you, do you do the journalist?
[00:48:16.760 --> 00:48:17.760]   I don't politicize.
[00:48:17.760 --> 00:48:19.760]   I don't want right for it.
[00:48:19.760 --> 00:48:20.760]   Oh, no, no.
[00:48:20.760 --> 00:48:23.120]   I'm a, no, I'm a, I'm a Elizabeth Warren.
[00:48:23.120 --> 00:48:25.280]   I'm a Warren fan, 100%.
[00:48:25.280 --> 00:48:27.480]   I identify her with her as a wonk.
[00:48:27.480 --> 00:48:29.080]   I appreciate and respect her.
[00:48:29.080 --> 00:48:31.480]   She is a Harvard professor, baby.
[00:48:31.480 --> 00:48:32.480]   You could tell.
[00:48:32.480 --> 00:48:37.200]   I just, I, I, I, I mean, I, I have nothing against Kamala, but, you know, at this moment
[00:48:37.200 --> 00:48:45.040]   in time, I'm like, boom, Warren, I just feel like she is everything a woman has to be,
[00:48:45.040 --> 00:48:52.800]   to be respected and, you know, thank you for putting in that effort in like, yeah.
[00:48:52.800 --> 00:48:59.960]   I'm, anyway, I'm inarticulate with like inspiration and joy and happiness that I see a candidate
[00:48:59.960 --> 00:49:02.600]   that actually feels like I really can respect.
[00:49:02.600 --> 00:49:03.920]   There's a number of smart.
[00:49:03.920 --> 00:49:04.920]   It's really unusual.
[00:49:04.920 --> 00:49:11.720]   They're quite a number of smart outspoken, well-thought out candidates.
[00:49:11.720 --> 00:49:13.200]   Maybe that's why I'm a hard time.
[00:49:13.200 --> 00:49:14.880]   Yeah, I love Mary Ann Williamson.
[00:49:14.880 --> 00:49:17.680]   Do you think that's the last time we'll see her?
[00:49:17.680 --> 00:49:20.360]   No, no, because everyone's so excited about her.
[00:49:20.360 --> 00:49:24.420]   What if people vote for this because it's novel and we've just turned our elections into
[00:49:24.420 --> 00:49:27.000]   like a popularity contest.
[00:49:27.000 --> 00:49:30.960]   And the right wing is telling people to give her money so she makes the next debates.
[00:49:30.960 --> 00:49:32.680]   Yang is already made the next two debates.
[00:49:32.680 --> 00:49:33.680]   No, really?
[00:49:33.680 --> 00:49:34.680]   Yeah.
[00:49:34.680 --> 00:49:41.520]   Ah, ah, Yang is the basic income guy, but, you know, even when he says it, I want to
[00:49:41.520 --> 00:49:44.920]   give everybody in the country a thousand dollars a month.
[00:49:44.920 --> 00:49:45.920]   It's like, what?
[00:49:45.920 --> 00:49:46.920]   Really?
[00:49:46.920 --> 00:49:48.080]   Does that what?
[00:49:48.080 --> 00:49:49.080]   I know.
[00:49:49.080 --> 00:49:50.080]   That's my favorite platform.
[00:49:50.080 --> 00:49:53.840]   Yeah, everything will be better because the robots are coming.
[00:49:53.840 --> 00:49:57.800]   That's about as crazy as Mary Ann Williamson, who's, by the way, when they asked her, what's
[00:49:57.800 --> 00:49:59.680]   the first thing you're going to do when you go in the White House?
[00:49:59.680 --> 00:50:03.400]   She said, I'm going to call the Prime Minister in New Zealand and say, what?
[00:50:03.400 --> 00:50:04.400]   Something you go, girl?
[00:50:04.400 --> 00:50:05.400]   What was she?
[00:50:05.400 --> 00:50:08.360]   She said some crazy.
[00:50:08.360 --> 00:50:10.560]   If only Saturday at Live were still on.
[00:50:10.560 --> 00:50:11.560]   Well, did you see?
[00:50:11.560 --> 00:50:12.560]   Oh, did you see?
[00:50:12.560 --> 00:50:14.360]   Oh, oh, they're on summer break.
[00:50:14.360 --> 00:50:19.560]   No, no, no, no, no, no, sorry.
[00:50:19.560 --> 00:50:22.640]   No, no, no, no, no, no, no, no, no, sorry.
[00:50:22.640 --> 00:50:23.640]   Sorry.
[00:50:23.640 --> 00:50:24.640]   Sorry, sorry.
[00:50:24.640 --> 00:50:25.640]   Yeah.
[00:50:25.640 --> 00:50:26.640]   Yeah, but it's not perfect.
[00:50:26.640 --> 00:50:27.640]   I would love to see this skit.
[00:50:27.640 --> 00:50:28.640]   Yeah.
[00:50:28.640 --> 00:50:31.640]   But I guess you see the, somebody brought up, you know, her old tweets.
[00:50:31.640 --> 00:50:32.640]   They're amazing.
[00:50:32.640 --> 00:50:36.520]   Well, she's exactly what you'd expect a motivational guru.
[00:50:36.520 --> 00:50:38.820]   What did, what did McKinnon say?
[00:50:38.820 --> 00:50:40.540]   She said, I'm going to have.
[00:50:40.540 --> 00:50:44.200]   I'm going to burn sage all over the country.
[00:50:44.200 --> 00:50:45.200]   That's true.
[00:50:45.200 --> 00:50:47.640]   And just the ideas were bonkers.
[00:50:47.640 --> 00:50:48.640]   Yeah.
[00:50:48.640 --> 00:50:50.920]   You know, that's, that's America.
[00:50:50.920 --> 00:50:51.920]   We like bonkers things.
[00:50:51.920 --> 00:50:52.920]   We love bonkers.
[00:50:52.920 --> 00:50:54.560]   Plus we have it.
[00:50:54.560 --> 00:50:55.560]   Like, what is it?
[00:50:55.560 --> 00:51:01.680]   The great awakening every, every couple decades, we have some sort of weird bonkers moment
[00:51:01.680 --> 00:51:02.680]   as a country.
[00:51:02.680 --> 00:51:05.080]   So maybe we're a, maybe we're due for our bonkers moment.
[00:51:05.080 --> 00:51:06.080]   How do we survive?
[00:51:06.080 --> 00:51:07.080]   I don't know.
[00:51:07.080 --> 00:51:09.200]   Wait, how do we get so far on track?
[00:51:09.200 --> 00:51:12.840]   Speaking of bonkers, Samsung says, oh, we finished the galaxy fold.
[00:51:12.840 --> 00:51:13.840]   Get ready.
[00:51:13.840 --> 00:51:16.040]   That was a transition.
[00:51:16.040 --> 00:51:19.960]   That is bonkers from Marianne Williamson to a galaxy fold.
[00:51:19.960 --> 00:51:21.680]   Who's more bonkers?
[00:51:21.680 --> 00:51:22.680]   So Samsung.
[00:51:22.680 --> 00:51:23.680]   Old alone.
[00:51:23.680 --> 00:51:24.680]   Yeah.
[00:51:24.680 --> 00:51:30.520]   Samsung says, so one of the things they're going to do is there, so remember, part of
[00:51:30.520 --> 00:51:36.880]   the problem was the number of journalists pulled the, what looked like a screen protector
[00:51:36.880 --> 00:51:39.000]   off the screen broke immediately.
[00:51:39.000 --> 00:51:44.400]   So Samsung is now stretching that protective film to wrap around the entire screen and
[00:51:44.400 --> 00:51:45.880]   flow into the outer bezels.
[00:51:45.880 --> 00:51:47.520]   So you can't peel it off.
[00:51:47.520 --> 00:51:49.480]   There's fixed number one.
[00:51:49.480 --> 00:51:50.480]   Fixed number two.
[00:51:50.480 --> 00:51:53.160]   This is just, I mean bonkers.
[00:51:53.160 --> 00:52:00.120]   It's re-engineered the hinge, pushing it slightly upward from the screen.
[00:52:00.120 --> 00:52:05.760]   So the film, the hinge will now push the film, stretch it when the phone opens.
[00:52:05.760 --> 00:52:09.280]   This will make the film feel harder and more a natural part of the device rather than a
[00:52:09.280 --> 00:52:11.880]   detachable accessory.
[00:52:11.880 --> 00:52:15.280]   And it may reduce the chance of a crease developing in the middle of the screen over
[00:52:15.280 --> 00:52:16.280]   time.
[00:52:16.280 --> 00:52:18.480]   And may reduce, may reduce.
[00:52:18.480 --> 00:52:21.480]   And it's $1,980 show of hands.
[00:52:21.480 --> 00:52:29.200]   How many of you are, can't wait to buy this thing that is clearly, the worst clue you
[00:52:29.200 --> 00:52:30.800]   ever know.
[00:52:30.800 --> 00:52:31.960]   I kind of want you to buy.
[00:52:31.960 --> 00:52:32.960]   We finally find.
[00:52:32.960 --> 00:52:34.720]   We really want a folding phone.
[00:52:34.720 --> 00:52:37.360]   I want them to like succeed at this somehow.
[00:52:37.360 --> 00:52:40.600]   Well, and while we had one, but we're not going to see that one.
[00:52:40.600 --> 00:52:45.120]   Yeah, we're not going there.
[00:52:45.120 --> 00:52:50.720]   I have been playing, and I really like it with a OnePlus 7T Pro.
[00:52:50.720 --> 00:52:51.880]   This just came out.
[00:52:51.880 --> 00:52:54.920]   And this is the funniest thing.
[00:52:54.920 --> 00:53:01.120]   So what I like about this screen is I'll let it screen with no notch and no hole punch
[00:53:01.120 --> 00:53:06.040]   because then you might say, well, that's a problem because first of all, fingerprint,
[00:53:06.040 --> 00:53:09.600]   well, they have a very good, I think.
[00:53:09.600 --> 00:53:10.400]   I can never remember.
[00:53:10.400 --> 00:53:14.600]   Is it sonic or or I think it's ultrasonic, right?
[00:53:14.600 --> 00:53:18.560]   Because it's a Samsung fingerprint reader that is optical.
[00:53:18.560 --> 00:53:20.800]   So the Samsung is a little slow.
[00:53:20.800 --> 00:53:21.800]   This one.
[00:53:21.800 --> 00:53:22.800]   Watch this.
[00:53:22.800 --> 00:53:23.800]   This is super good.
[00:53:23.800 --> 00:53:25.640]   It really works 100% of the time.
[00:53:25.640 --> 00:53:33.000]   What happens when your hands are cold or buffed, like nail-filed or dry?
[00:53:33.000 --> 00:53:34.000]   I don't know.
[00:53:34.000 --> 00:53:35.000]   I haven't tried it that well.
[00:53:35.000 --> 00:53:38.600]   I did pass it around the our editorial meeting to see if other people could ever.
[00:53:38.600 --> 00:53:40.360]   I think it works pretty well.
[00:53:40.360 --> 00:53:43.960]   And then the other issue is, of course, well, where's the selfie cam?
[00:53:43.960 --> 00:53:46.440]   This is my favorite, favorite part.
[00:53:46.440 --> 00:53:48.000]   Watch the top of the screen here.
[00:53:48.000 --> 00:53:50.360]   I'm going to take a picture of myself.
[00:53:50.360 --> 00:53:54.640]   It's like those fancy TVs from the 80s.
[00:53:54.640 --> 00:53:55.640]   Yeah.
[00:53:55.640 --> 00:53:56.640]   Yes.
[00:53:56.640 --> 00:53:57.800]   There's a sealed little.
[00:53:57.800 --> 00:54:02.000]   I, you know, when I first got the phone, I forgot that it did this.
[00:54:02.000 --> 00:54:03.000]   I said, what's that?
[00:54:03.000 --> 00:54:04.000]   Is that the sim port?
[00:54:04.000 --> 00:54:05.000]   Nope.
[00:54:05.000 --> 00:54:06.600]   That's where the camera comes up.
[00:54:06.600 --> 00:54:08.160]   Waterproof?
[00:54:08.160 --> 00:54:09.160]   I don't think so.
[00:54:09.160 --> 00:54:12.760]   It also doesn't have wireless charging, but it is easily beaten.
[00:54:12.760 --> 00:54:15.400]   Any other screen I've used because it's got known.
[00:54:15.400 --> 00:54:16.400]   It's great.
[00:54:16.400 --> 00:54:17.400]   No, no notches.
[00:54:17.400 --> 00:54:18.400]   No, nothing.
[00:54:18.400 --> 00:54:19.400]   It's nice.
[00:54:19.400 --> 00:54:21.160]   So are there cases made for that?
[00:54:21.160 --> 00:54:22.160]   Yeah.
[00:54:22.160 --> 00:54:23.160]   Yeah.
[00:54:23.160 --> 00:54:24.600]   I've ordered one because I don't have one right now.
[00:54:24.600 --> 00:54:26.360]   No, I'm like, oh, it's so shiny.
[00:54:26.360 --> 00:54:27.360]   I know.
[00:54:27.360 --> 00:54:28.560]   It was so hard to hold.
[00:54:28.560 --> 00:54:31.080]   So slippery and glassy.
[00:54:31.080 --> 00:54:32.240]   I know I'm going to break it.
[00:54:32.240 --> 00:54:33.880]   So this is the OnePlus 87.
[00:54:33.880 --> 00:54:34.880]   What is this?
[00:54:34.880 --> 00:54:35.880]   OnePlus 7 Pro.
[00:54:35.880 --> 00:54:36.880]   7T Pro or 7 Pro?
[00:54:36.880 --> 00:54:37.880]   I can't remember.
[00:54:37.880 --> 00:54:40.440]   It's a new one.
[00:54:40.440 --> 00:54:43.080]   And I'm very nice.
[00:54:43.080 --> 00:54:44.080]   Yeah.
[00:54:44.080 --> 00:54:46.840]   And now they do their own version of Android.
[00:54:46.840 --> 00:54:50.480]   They call one, which is a very clean, simple version of Android.
[00:54:50.480 --> 00:54:51.480]   I like it a lot.
[00:54:51.480 --> 00:54:55.240]   So it's not, you know, googly, pure.
[00:54:55.240 --> 00:54:56.240]   They're pretty impressive.
[00:54:56.240 --> 00:54:57.240]   They're stuck around.
[00:54:57.240 --> 00:55:00.560]   I remember we went to events with them and, you know, we all talked about them at the
[00:55:00.560 --> 00:55:02.520]   beginning and there was a scarcity.
[00:55:02.520 --> 00:55:04.320]   They were stuck about.
[00:55:04.320 --> 00:55:08.840]   I might point out it's a Chinese company.
[00:55:08.840 --> 00:55:15.000]   So I don't know what the future holds for OnePlus, but they make it a better shape.
[00:55:15.000 --> 00:55:16.760]   Well, is it though?
[00:55:16.760 --> 00:55:17.760]   Tell me.
[00:55:17.760 --> 00:55:18.760]   Explain this to me.
[00:55:18.760 --> 00:55:25.560]   So remember that, and this is why you want to read the tweets, the president after meeting
[00:55:25.560 --> 00:55:29.840]   with President Xi at the G20 right before he stepped in North Korea.
[00:55:29.840 --> 00:55:32.240]   Apparently, they had a nice lunch.
[00:55:32.240 --> 00:55:33.240]   Maybe they took a selfie.
[00:55:33.240 --> 00:55:34.240]   I don't know.
[00:55:34.240 --> 00:55:35.840]   The president said, Oh, you know what?
[00:55:35.840 --> 00:55:37.760]   Huawei, go ahead.
[00:55:37.760 --> 00:55:42.120]   He's going to allow US companies to sell stuff to Huawei, which means all of a sudden, I
[00:55:42.120 --> 00:55:45.480]   mean, this is hundreds of millions, maybe billions of dollars.
[00:55:45.480 --> 00:55:47.480]   Qualcomm can sell chips to them.
[00:55:47.480 --> 00:55:49.880]   Google can give them Android, all this stuff.
[00:55:49.880 --> 00:55:56.320]   But now today, a senior US official told the Commerce Department's enforcement staff this
[00:55:56.320 --> 00:56:02.320]   week, Huawei should still be treated as blacklisted.
[00:56:02.320 --> 00:56:09.080]   So this is another case of a unilateral announcement from the president that in fact, his own government
[00:56:09.080 --> 00:56:13.000]   seems to disbelieve.
[00:56:13.000 --> 00:56:14.800]   And wait a minute.
[00:56:14.800 --> 00:56:18.920]   Is security problem not a trade problem?
[00:56:18.920 --> 00:56:23.080]   John Sonnerman, deputy director of the office of export enforcement in the Commerce Department's
[00:56:23.080 --> 00:56:30.720]   Bureau of Industry and Security said, all such applications by US firms to sell the
[00:56:30.720 --> 00:56:34.160]   Huawei should be considered on a merit flag with language noting, this party is on the
[00:56:34.160 --> 00:56:35.160]   entity list.
[00:56:35.160 --> 00:56:38.920]   See, he didn't take them off the end of a list, evaluate the associated license review
[00:56:38.920 --> 00:56:41.760]   policy under part 774.
[00:56:41.760 --> 00:56:45.320]   Resumption of denial.
[00:56:45.320 --> 00:56:51.120]   So the president did say, if, you know, I mean, I guess you could parse his tweet, which is
[00:56:51.120 --> 00:56:55.600]   what we've come to, as saying, you can sell the Huawei as long as it's not a secure, it's
[00:56:55.600 --> 00:57:00.360]   not that security stuff problem thing.
[00:57:00.360 --> 00:57:07.280]   So this is a very unclear guidance from the president.
[00:57:07.280 --> 00:57:11.760]   Huawei is also willing to continue to buy products.
[00:57:11.760 --> 00:57:15.480]   Huawei said we are willing to continue to buy products from American companies, but we
[00:57:15.480 --> 00:57:18.040]   don't see much impact on what we're currently doing.
[00:57:18.040 --> 00:57:19.760]   We'll still just focus on doing our job.
[00:57:19.760 --> 00:57:20.760]   Right.
[00:57:20.760 --> 00:57:21.760]   I don't blame them.
[00:57:21.760 --> 00:57:22.760]   I don't blame them.
[00:57:22.760 --> 00:57:25.760]   They're going to just design their own chips, their own OS.
[00:57:25.760 --> 00:57:26.600]   Yeah.
[00:57:26.600 --> 00:57:32.560]   The person familiar with the matter said the letter was the only guidance officials, enforcement
[00:57:32.560 --> 00:57:37.800]   officials had received after Trump's surprise and the husband on Saturday, a presumption
[00:57:37.800 --> 00:57:42.280]   of a denial implies strict review and most licenses reviewed under it are not approved.
[00:57:42.280 --> 00:57:48.760]   So I guess it's kind of, it kind of matches what the president said kind of.
[00:57:48.760 --> 00:57:49.960]   I don't know.
[00:57:49.960 --> 00:57:50.960]   I don't know.
[00:57:50.960 --> 00:57:52.560]   We're back to politics.
[00:57:52.560 --> 00:57:53.560]   Why is this happening?
[00:57:53.560 --> 00:57:54.560]   Okay.
[00:57:54.560 --> 00:57:55.560]   Moving right along.
[00:57:55.560 --> 00:58:00.120]   I got to the fold and I mentioned the Huawei fold and then I mentioned, I was going to say
[00:58:00.120 --> 00:58:01.120]   this anyway.
[00:58:01.120 --> 00:58:05.000]   Huawei is back on, maybe sort of Samsung, the note.
[00:58:05.000 --> 00:58:09.080]   Are you ready for a new galaxy note nine?
[00:58:09.080 --> 00:58:10.280]   Samsung will hold 10.
[00:58:10.280 --> 00:58:12.280]   Did I say nine?
[00:58:12.280 --> 00:58:13.280]   10?
[00:58:13.280 --> 00:58:14.280]   Yeah.
[00:58:14.280 --> 00:58:15.280]   The note 10.
[00:58:15.280 --> 00:58:16.280]   Will it fold or explode?
[00:58:16.280 --> 00:58:17.280]   No, no folding.
[00:58:17.280 --> 00:58:18.280]   One hopes.
[00:58:18.280 --> 00:58:19.280]   No exploding.
[00:58:19.280 --> 00:58:20.280]   It will.
[00:58:20.280 --> 00:58:21.280]   It looks.
[00:58:21.280 --> 00:58:22.280]   When you fold it.
[00:58:22.280 --> 00:58:23.280]   August 7th.
[00:58:23.280 --> 00:58:24.440]   Now I have to actually tell you guys something.
[00:58:24.440 --> 00:58:25.800]   That's a Wednesday.
[00:58:25.800 --> 00:58:29.440]   It will be at 1 p.m. Pacific time in Brooklyn.
[00:58:29.440 --> 00:58:32.320]   I mean, at our time, but it will be in Brooklyn.
[00:58:32.320 --> 00:58:37.400]   So I think what we're going to do is do Windows weekly at our normal hour.
[00:58:37.400 --> 00:58:39.120]   Go to the Samsung event at 1 p.m.
[00:58:39.120 --> 00:58:41.880]   Usually Samsung is pretty quick like they're over an hour.
[00:58:41.880 --> 00:58:42.880]   What I would love you to do.
[00:58:42.880 --> 00:58:44.720]   They got rid of their dancing numbers all that.
[00:58:44.720 --> 00:58:46.640]   Yeah, no more Broadway shows.
[00:58:46.640 --> 00:58:51.680]   I would love you guys, both of you to join me for our coverage of the Samsung event.
[00:58:51.680 --> 00:58:53.480]   We'll do that as a separate special.
[00:58:53.480 --> 00:58:56.320]   Put that out and then we'll start twig right after.
[00:58:56.320 --> 00:58:59.160]   So in the days, this is August 7th.
[00:58:59.160 --> 00:59:00.160]   Okay.
[00:59:00.160 --> 00:59:02.760]   Just put that, make a note of that.
[00:59:02.760 --> 00:59:07.120]   As they say, save the date.
[00:59:07.120 --> 00:59:12.600]   The note nine was $716 on Amazon, $1000 everywhere else.
[00:59:12.600 --> 00:59:15.440]   We don't know how much the note 10 will be.
[00:59:15.440 --> 00:59:18.280]   We don't know much about the note 10 actually.
[00:59:18.280 --> 00:59:22.640]   One rumor said that the pen could have its own camera.
[00:59:22.640 --> 00:59:24.760]   That might be, yeah, look at this.
[00:59:24.760 --> 00:59:25.840]   Look at this invitation.
[00:59:25.840 --> 00:59:27.960]   It might kind of imply that there's a camera on the pen.
[00:59:27.960 --> 00:59:28.960]   I don't know.
[00:59:28.960 --> 00:59:31.200]   That would be cool.
[00:59:31.200 --> 00:59:32.200]   Right?
[00:59:32.200 --> 00:59:35.640]   Then another rumor said that.
[00:59:35.640 --> 00:59:38.360]   So what do you use that to shoot inside your ear?
[00:59:38.360 --> 00:59:39.360]   Yeah.
[00:59:39.360 --> 00:59:41.280]   I don't know.
[00:59:41.280 --> 00:59:42.280]   Check your drink.
[00:59:42.280 --> 00:59:44.000]   There's my tonsils.
[00:59:44.000 --> 00:59:45.400]   The note 10 could also have four.
[00:59:45.400 --> 00:59:47.040]   I bought one of those laparstopic cameras.
[00:59:47.040 --> 00:59:48.200]   Oh, those are so much fun.
[00:59:48.200 --> 00:59:49.200]   I have one too.
[00:59:49.200 --> 00:59:50.200]   They're so great.
[00:59:50.200 --> 00:59:51.200]   I value you $29 one.
[00:59:51.200 --> 00:59:52.200]   Yeah.
[00:59:52.200 --> 00:59:53.200]   They're awesome.
[00:59:53.200 --> 00:59:57.520]   You plug them into your phone or your computer and then puts it up on the screen and you
[00:59:57.520 --> 01:00:00.760]   could put it right in your ear.
[01:00:00.760 --> 01:00:03.760]   Or like we used it to check for leaks in the ceiling.
[01:00:03.760 --> 01:00:05.720]   I've used it like playing with my camera.
[01:00:05.720 --> 01:00:07.160]   The kids love it.
[01:00:07.160 --> 01:00:10.080]   So let's look up your nose, Mommy.
[01:00:10.080 --> 01:00:11.240]   Let's see your tonsils, Mommy.
[01:00:11.240 --> 01:00:14.200]   Actually, I'm sure these things have happened.
[01:00:14.200 --> 01:00:17.080]   Oh, yes.
[01:00:17.080 --> 01:00:19.760]   By the way, Sam, I love the register.
[01:00:19.760 --> 01:00:22.320]   They're so snarky.
[01:00:22.320 --> 01:00:26.480]   They're talking about a new feature of Samsung.
[01:00:26.480 --> 01:00:29.480]   Samsung tears wrap off Bixby marketplace.
[01:00:29.480 --> 01:00:32.440]   Tens of people go wild.
[01:00:32.440 --> 01:00:36.240]   One at the back whispers, Siri, what's Bixby?
[01:00:36.240 --> 01:00:38.200]   Are they British?
[01:00:38.200 --> 01:00:41.960]   Don't you have to do in the British out of this?
[01:00:41.960 --> 01:00:43.560]   Yes, that is definitely.
[01:00:43.560 --> 01:00:44.960]   They are British snark.
[01:00:44.960 --> 01:00:45.960]   This is Kieran McCarthy.
[01:00:45.960 --> 01:00:50.160]   Who's in San Francisco, but he retained some of that British snark?
[01:00:50.160 --> 01:00:51.160]   That's snarky.
[01:00:51.160 --> 01:00:52.160]   The boughts.
[01:00:52.160 --> 01:00:53.160]   Yeah.
[01:00:53.160 --> 01:01:01.880]   Apparently, at this August thing, they're talking about releasing Remember the Fondupot?
[01:01:01.880 --> 01:01:04.640]   The Samsung Bixby thing?
[01:01:04.640 --> 01:01:06.800]   You know what I'm talking about?
[01:01:06.800 --> 01:01:09.360]   The digital speaker that looked like a fondupot?
[01:01:09.360 --> 01:01:10.920]   That's apparently coming out.
[01:01:10.920 --> 01:01:11.920]   It looks like a kettle.
[01:01:11.920 --> 01:01:13.760]   Yeah, like a witch's kettle.
[01:01:13.760 --> 01:01:14.760]   Yeah.
[01:01:14.760 --> 01:01:16.160]   So the cauldron.
[01:01:16.160 --> 01:01:17.160]   The cauldron.
[01:01:17.160 --> 01:01:18.160]   Here it is.
[01:01:18.160 --> 01:01:19.560]   The galaxy home.
[01:01:19.560 --> 01:01:21.360]   That's what it's called.
[01:01:21.360 --> 01:01:22.920]   Oh, yeah.
[01:01:22.920 --> 01:01:27.160]   It's got tripod legs and it really very much looks like you might be brewing a potion in
[01:01:27.160 --> 01:01:28.160]   there.
[01:01:28.160 --> 01:01:30.560]   Or it's blasting off from the moon.
[01:01:30.560 --> 01:01:31.560]   Yeah.
[01:01:31.560 --> 01:01:34.600]   Turn your home into a smart home.
[01:01:34.600 --> 01:01:35.600]   It's a smart thing.
[01:01:35.600 --> 01:01:38.520]   Now you're the IoT expert, Stacy.
[01:01:38.520 --> 01:01:40.600]   I mean, smart things is a pretty good hub.
[01:01:40.600 --> 01:01:41.600]   Right?
[01:01:41.600 --> 01:01:42.600]   It supports all the protocols.
[01:01:42.600 --> 01:01:43.600]   It supports all.
[01:01:43.600 --> 01:01:44.600]   Yeah.
[01:01:44.600 --> 01:01:45.600]   Samsung bottom.
[01:01:45.600 --> 01:01:47.960]   Samsung bottom.
[01:01:47.960 --> 01:01:53.800]   They are linking all of their appliances and everything else into smart things.
[01:01:53.800 --> 01:01:56.800]   But smart things works with the Samsung stuff, right?
[01:01:56.800 --> 01:01:57.800]   It does.
[01:01:57.800 --> 01:02:00.600]   It's a little hard for it to.
[01:02:00.600 --> 01:02:03.000]   It's not the first choice for newbies.
[01:02:03.000 --> 01:02:04.000]   Oh, really?
[01:02:04.000 --> 01:02:07.200]   It can be kind of complex to get things working.
[01:02:07.200 --> 01:02:09.160]   But you know what?
[01:02:09.160 --> 01:02:12.040]   There's not like there's a whole lot of easier options.
[01:02:12.040 --> 01:02:13.040]   Smart things.
[01:02:13.040 --> 01:02:14.440]   And by the way.
[01:02:14.440 --> 01:02:17.680]   It's the only open choice really there is.
[01:02:17.680 --> 01:02:22.000]   So Wink comes back and says that they're actually, till will I am, let's us know what he's doing
[01:02:22.000 --> 01:02:23.000]   with Wink.
[01:02:23.000 --> 01:02:24.000]   He owns Wink?
[01:02:24.000 --> 01:02:25.560]   Yeah, his company does.
[01:02:25.560 --> 01:02:30.040]   I don't know if he individually does, but his investment fund does.
[01:02:30.040 --> 01:02:31.040]   Huh.
[01:02:31.040 --> 01:02:34.760]   We had a security now.
[01:02:34.760 --> 01:02:36.760]   A story Steve was talking about.
[01:02:36.760 --> 01:02:44.840]   Let me see if I can find it about a Chinese made kind of a funny name camera that isn't
[01:02:44.840 --> 01:02:48.880]   exactly the most private thing I've ever heard of.
[01:02:48.880 --> 01:02:50.280]   Let me see if I can.
[01:02:50.280 --> 01:02:51.280]   Or rebay?
[01:02:51.280 --> 01:02:52.280]   Yeah.
[01:02:52.280 --> 01:02:53.280]   A repo?
[01:02:53.280 --> 01:02:54.280]   Yeah.
[01:02:54.280 --> 01:02:55.280]   Yeah.
[01:02:55.280 --> 01:02:56.280]   Yeah.
[01:02:56.280 --> 01:02:57.280]   That was that was not smart.
[01:02:57.280 --> 01:02:59.000]   So this is why it's going.
[01:02:59.000 --> 01:03:00.000]   What's the story?
[01:03:00.000 --> 01:03:01.000]   I'm.
[01:03:01.000 --> 01:03:02.000]   Fill us in.
[01:03:02.000 --> 01:03:04.880]   What let me see if I can find it here.
[01:03:04.880 --> 01:03:14.280]   A company called VPN mentor discovered that they could find on the or rebay database all
[01:03:14.280 --> 01:03:15.280]   of these.
[01:03:15.280 --> 01:03:18.200]   They could find your password location or Veebo.
[01:03:18.200 --> 01:03:19.200]   Thank you.
[01:03:19.200 --> 01:03:20.200]   Sorry.
[01:03:20.200 --> 01:03:22.000]   Or Veebo or Veebo.
[01:03:22.000 --> 01:03:24.520]   And they make they make like a their outlet.
[01:03:24.520 --> 01:03:27.080]   You can buy on Amazon for like seven bucks.
[01:03:27.080 --> 01:03:29.800]   So it shares your name.
[01:03:29.800 --> 01:03:30.800]   Location.
[01:03:30.800 --> 01:03:33.160]   Wi-Fi location or sorry Wi-Fi address.
[01:03:33.160 --> 01:03:34.160]   Why is it?
[01:03:34.160 --> 01:03:41.560]   It was a redacted record from that's publicly available by the way posted from the camera,
[01:03:41.560 --> 01:03:44.920]   including your lat longitude and lato tie.
[01:03:44.920 --> 01:03:45.920]   Latotide.
[01:03:45.920 --> 01:03:49.720]   But also even city and state.
[01:03:49.720 --> 01:03:50.720]   Yeah.
[01:03:50.720 --> 01:03:51.960]   So all of this is shared.
[01:03:51.960 --> 01:03:54.560]   They basically left their database open.
[01:03:54.560 --> 01:03:58.240]   So I look for companies of your super security conscious.
[01:03:58.240 --> 01:03:59.960]   You don't store this if you don't need it.
[01:03:59.960 --> 01:04:04.280]   So there's plenty of companies that don't store your like Wi-Fi password and information.
[01:04:04.280 --> 01:04:11.880]   The two, you know, at a minimum, you should put this behind a password.
[01:04:11.880 --> 01:04:14.680]   And yeah, so that's that's the issue there.
[01:04:14.680 --> 01:04:15.680]   Hold on.
[01:04:15.680 --> 01:04:20.520]   Or Veebo smart home devices.
[01:04:20.520 --> 01:04:27.320]   Noam Rotem and Rand lowcard discovered an open database with two billion logs in it.
[01:04:27.320 --> 01:04:33.960]   Your name is email addresses passwords, precise locations or Veebo claims to have a million
[01:04:33.960 --> 01:04:39.800]   users, private users, hotels and other offerings.
[01:04:39.800 --> 01:04:42.160]   They contacted or Veebo June 16th.
[01:04:42.160 --> 01:04:43.760]   No response after several days.
[01:04:43.760 --> 01:04:45.400]   They tweeted them.
[01:04:45.400 --> 01:04:47.040]   No response.
[01:04:47.040 --> 01:04:51.400]   And the database is still online.
[01:04:51.400 --> 01:04:54.040]   It's a Chinese company in Shenzhen.
[01:04:54.040 --> 01:04:55.960]   Manufacturers 100 different smart homes.
[01:04:55.960 --> 01:04:59.520]   As of July 2nd, it's not online anymore.
[01:04:59.520 --> 01:05:01.280]   They finally got it down.
[01:05:01.280 --> 01:05:03.560]   They did as of is today July 2nd?
[01:05:03.560 --> 01:05:05.120]   No, it was a third yesterday.
[01:05:05.120 --> 01:05:06.120]   Yesterday.
[01:05:06.120 --> 01:05:08.360]   So as of yesterday, it is offline.
[01:05:08.360 --> 01:05:10.840]   But before then, online.
[01:05:10.840 --> 01:05:17.080]   Yeah, this is guys buy from a reputable vendor.
[01:05:17.080 --> 01:05:21.440]   Always look to see how those vendors Google their name so you can see how they handle security
[01:05:21.440 --> 01:05:23.640]   issues.
[01:05:23.640 --> 01:05:28.560]   And honestly, you should, well, we could actually talk about this.
[01:05:28.560 --> 01:05:34.160]   We talked about it on our show, but the FTC also, did you see that the FTC settled with
[01:05:34.160 --> 01:05:37.000]   dealing over its security practices?
[01:05:37.000 --> 01:05:42.280]   And I bring that up because I think we should have a baseline of security practices because
[01:05:42.280 --> 01:05:47.200]   right now, it's not like most people know how a smart home device is going to handle
[01:05:47.200 --> 01:05:53.520]   their information, but they really should have a baseline set of expectations.
[01:05:53.520 --> 01:06:00.560]   Is there, is there, I wonder maybe we need a standard, maybe the NSA or somebody should
[01:06:00.560 --> 01:06:01.560]   set a standard.
[01:06:01.560 --> 01:06:02.560]   The NSA.
[01:06:02.560 --> 01:06:07.160]   Well, yeah, believe it or not, the NSA actually has some very good documents on how to secure
[01:06:07.160 --> 01:06:12.000]   yourself with a variety of operating systems because their national mission is to preserve
[01:06:12.000 --> 01:06:17.400]   our security, which includes protecting us from bad guys.
[01:06:17.400 --> 01:06:18.400]   Okay.
[01:06:18.400 --> 01:06:20.880]   So there are NIST standards.
[01:06:20.880 --> 01:06:26.400]   NIST would be another good one, the National Institute of Standards and Technology.
[01:06:26.400 --> 01:06:31.760]   But so with the, is it a rebate?
[01:06:31.760 --> 01:06:33.800]   I keep or Vibbo or Vibbo.
[01:06:33.800 --> 01:06:35.280]   Thank you, Carsten.
[01:06:35.280 --> 01:06:36.280]   I'm sorry.
[01:06:36.280 --> 01:06:38.880]   Vibbo, Vibbo, Vibbo, Vibbo.
[01:06:38.880 --> 01:06:46.520]   So with this, I would say, don't just buy the cheapest device, look up the vendor, see
[01:06:46.520 --> 01:06:51.160]   how they respond to security things, see if they do have standards already in place,
[01:06:51.160 --> 01:06:54.440]   and make sure that they patch if there are problems.
[01:06:54.440 --> 01:06:55.440]   Yeah.
[01:06:55.440 --> 01:06:56.440]   So that's one.
[01:06:56.440 --> 01:07:04.800]   And then switching over to this idea of creating a standard, the FTC this week settled a two-year-old
[01:07:04.800 --> 01:07:08.600]   suit with D-Link, so the makers of routers and cameras.
[01:07:08.600 --> 01:07:14.280]   The FTC came after them in 2017 saying, "Hey, you have hard-coded passwords.
[01:07:14.280 --> 01:07:17.480]   You have some really bad stuff in here, and you're telling people that these are safe
[01:07:17.480 --> 01:07:19.040]   and secure."
[01:07:19.040 --> 01:07:24.920]   And originally, the FTC suit was thrown out for not bringing enough proof.
[01:07:24.920 --> 01:07:28.160]   But then they came back, I guess, with more proof, and now they settled.
[01:07:28.160 --> 01:07:35.720]   As part of the settlement, the FTC is making, well, it has put some rules forth for D-Link,
[01:07:35.720 --> 01:07:41.040]   but also they will have to, for the next 10 years, I think it's once a year, undergo a
[01:07:41.040 --> 01:07:43.080]   security audit.
[01:07:43.080 --> 01:07:49.480]   And the things that they said in the settlement could become really good precedent for, again,
[01:07:49.480 --> 01:07:54.880]   a baseline security standard for connected devices.
[01:07:54.880 --> 01:08:01.480]   So as you probably know, there's an epidemic of ransomware sweeping Florida cities.
[01:08:01.480 --> 01:08:09.080]   And at least a couple of cases, I think, keep a skein and revere a beach.
[01:08:09.080 --> 01:08:16.880]   And now Lake City, Florida, they paid out, revere a beach paid $600,000 in Bitcoin to
[01:08:16.880 --> 01:08:20.000]   the bad guys, got their data back that day.
[01:08:20.000 --> 01:08:22.600]   Lake City, Florida, $460,000.
[01:08:22.600 --> 01:08:23.600]   These are small towns.
[01:08:23.600 --> 01:08:27.240]   I think, you know, just tens of thousands of people.
[01:08:27.240 --> 01:08:31.240]   But what's interesting is that there's a league of Florida cities that has an insurance
[01:08:31.240 --> 01:08:32.480]   program for ransomware.
[01:08:32.480 --> 01:08:37.400]   So in both all these cases, there's insurance, so it doesn't cost the city all that much
[01:08:37.400 --> 01:08:38.640]   money.
[01:08:38.640 --> 01:08:47.480]   I am very happy to say to report though, the IT manager at Lake City, Florida, who's initial
[01:08:47.480 --> 01:08:53.840]   response to this, I wish I could find the quote, was, well, as far as we know, nobody's
[01:08:53.840 --> 01:08:57.560]   a data was stolen, has now been fired.
[01:08:57.560 --> 01:08:58.960]   Thank you.
[01:08:58.960 --> 01:09:03.920]   Because after all, if a city is forced to pay hundreds of thousand dollars in ransomware,
[01:09:03.920 --> 01:09:07.600]   it's because they don't have proper security measures in place.
[01:09:07.600 --> 01:09:12.360]   And they don't have proper backups in place.
[01:09:12.360 --> 01:09:16.720]   The Georgia court is the most recent victim of the same ransomware.
[01:09:16.720 --> 01:09:19.600]   It's the re-uck ransomware.
[01:09:19.600 --> 01:09:21.760]   I haven't heard that happen yesterday.
[01:09:21.760 --> 01:09:27.320]   I haven't heard yet if they're going to pay.
[01:09:27.320 --> 01:09:30.480]   But it's nasty out there.
[01:09:30.480 --> 01:09:38.680]   The Georgia court system, the Georgia administrative office of the courts, discovered the hack Saturday
[01:09:38.680 --> 01:09:44.560]   morning, hackers used the re-uck ransomware, ransomware holding several state court systems,
[01:09:44.560 --> 01:09:46.280]   hostage.
[01:09:46.280 --> 01:09:50.920]   Many court documents filed online have been inaccessible.
[01:09:50.920 --> 01:09:54.480]   No word yet whether they plan to pay.
[01:09:54.480 --> 01:09:57.720]   It's just a non-stop mess.
[01:09:57.720 --> 01:10:00.240]   And the reason they go after, we talked about yesterday on security now, the reason they
[01:10:00.240 --> 01:10:02.640]   go after cities, there's several reasons.
[01:10:02.640 --> 01:10:07.480]   One is everything, the whole layout of the city is public knowledge.
[01:10:07.480 --> 01:10:12.520]   It's on the website who's the boss, who's the city manager, who's the comptroller, who's
[01:10:12.520 --> 01:10:19.000]   the, so it's very easy to figure out the email chain to send, the spearfishing to send.
[01:10:19.000 --> 01:10:22.600]   They don't generally have the budget to have decent IT.
[01:10:22.600 --> 01:10:26.120]   And I think this reason you see it in Florida so many times now is because they do have
[01:10:26.120 --> 01:10:29.440]   insurance, which encourages them to pay.
[01:10:29.440 --> 01:10:31.280]   So it's been very lucrative.
[01:10:31.280 --> 01:10:34.280]   I expect to see more of those.
[01:10:34.280 --> 01:10:42.160]   Speaking of malware, the Chinese border patrol, if you cross into the border near the Xinjiang
[01:10:42.160 --> 01:10:49.000]   region, which is, I think it's in the western area where the local Muslim population, the
[01:10:49.000 --> 01:10:56.080]   weagers are being repressed, if you cross in, you will be forced to install malware on
[01:10:56.080 --> 01:11:01.080]   your phone that downloads your text messages, calendar entries and phone logs, scans the
[01:11:01.080 --> 01:11:08.120]   device looking for 70,000 different apps and stays on the device.
[01:11:08.120 --> 01:11:11.520]   Motherboard got a copy of the malware.
[01:11:11.520 --> 01:11:18.600]   And in fact, I guess this is a German newspaper, Sudhdoytzeitung.
[01:11:18.600 --> 01:11:24.520]   They send a reporter who crossed the border had the same malware installed on their phone.
[01:11:24.520 --> 01:11:29.040]   In fact, if you want, you can download the malware from motherboard.
[01:11:29.040 --> 01:11:32.840]   So why don't you install on your phone and let get back to us.
[01:11:32.840 --> 01:11:33.840]   How does that?
[01:11:33.840 --> 01:11:37.680]   The border crossing is from Kyrgyzstan into China.
[01:11:37.680 --> 01:11:41.480]   And I guess the theory is, well, what are you doing over here?
[01:11:41.480 --> 01:11:43.960]   What are you doing here?
[01:11:43.960 --> 01:11:48.280]   I think though, it's just, I'm really worried I'm going to be going in the fall to the
[01:11:48.280 --> 01:11:56.360]   Middle East to both Israel and Jordan and Oman and the United Arab Emirates.
[01:11:56.360 --> 01:11:59.920]   I'm actually trying to figure out what my strategy is going to be, both for going into
[01:11:59.920 --> 01:12:09.560]   those countries, going from Israel into Dubai and then to top it all off, coming home.
[01:12:09.560 --> 01:12:10.560]   Just buy six burners.
[01:12:10.560 --> 01:12:12.840]   I think I have to have a burner for every country.
[01:12:12.840 --> 01:12:13.840]   I have a burner for every country.
[01:12:13.840 --> 01:12:15.840]   Not to mention two backwards.
[01:12:15.840 --> 01:12:16.840]   Oh man.
[01:12:16.840 --> 01:12:20.000]   Yeah, you get one for the US too.
[01:12:20.000 --> 01:12:21.000]   Yeah.
[01:12:21.000 --> 01:12:22.000]   Geez.
[01:12:22.000 --> 01:12:27.600]   By the way, when I went to Israel not too long ago, instead of stamps, they put two little,
[01:12:27.600 --> 01:12:33.800]   they just give you two little very not pasted cards, an entry card, an exit card.
[01:12:33.800 --> 01:12:36.400]   And you have to have it when you go out so they can take it.
[01:12:36.400 --> 01:12:38.000]   But then you can throw it away.
[01:12:38.000 --> 01:12:39.000]   Right.
[01:12:39.000 --> 01:12:40.000]   Yeah, Russia was there.
[01:12:40.000 --> 01:12:41.480]   Russia does that too.
[01:12:41.480 --> 01:12:43.800]   Oh, so that there's no evidence that you were in Israel.
[01:12:43.800 --> 01:12:44.800]   That's the point.
[01:12:44.800 --> 01:12:45.800]   Clever.
[01:12:45.800 --> 01:12:51.080]   There's going to be a big social media summit next week at the White House.
[01:12:51.080 --> 01:12:54.880]   Oh, oh, oh, oh.
[01:12:54.880 --> 01:12:55.880]   Okay.
[01:12:55.880 --> 01:13:00.840]   You'd tell us right first that I'm going to go along and drive us Facebook, Google and
[01:13:00.840 --> 01:13:07.160]   Twitter writes the power post from the Washington Post have long struggled to back allegations
[01:13:07.160 --> 01:13:08.160]   in Washington.
[01:13:08.160 --> 01:13:13.640]   They censor conservatives online, but get ready because here comes the White House social
[01:13:13.640 --> 01:13:15.880]   media summit.
[01:13:15.880 --> 01:13:21.000]   The July 11th event aims to assemble digital leaders to discuss the opportunities and
[01:13:21.000 --> 01:13:24.080]   challenges of today's online environment.
[01:13:24.080 --> 01:13:31.600]   But really who they're inviting is tech's top conservative critics and politics in media,
[01:13:31.600 --> 01:13:33.400]   including PragerU.
[01:13:33.400 --> 01:13:37.520]   We love PragerU, right?
[01:13:37.520 --> 01:13:41.160]   Google actually has limited the reach of some of their videos about Islam and guns.
[01:13:41.160 --> 01:13:45.120]   So they can't be viewed by those who have enabled restrictive mode so schools of parents
[01:13:45.120 --> 01:13:48.720]   can at least keep their kids from looking at it.
[01:13:48.720 --> 01:13:56.840]   That apparently is evidence proof positive that conservatives are being locked on the
[01:13:56.840 --> 01:14:02.240]   big white internet heritage foundation, the media research center, Charlie Kirk, executive
[01:14:02.240 --> 01:14:06.520]   director of Turning Point USA.
[01:14:06.520 --> 01:14:08.520]   Were you invited, Jeff?
[01:14:08.520 --> 01:14:13.480]   No, neither was I.
[01:14:13.480 --> 01:14:16.640]   So here's the thing.
[01:14:16.640 --> 01:14:20.280]   Some say that Ron Wyden said there's zero evidence social media companies are biased
[01:14:20.280 --> 01:14:21.280]   against conservatives.
[01:14:21.280 --> 01:14:26.520]   I think they probably are in a great majority of them run by liberals, right?
[01:14:26.520 --> 01:14:28.000]   I don't know.
[01:14:28.000 --> 01:14:30.840]   I had journalism professors who were very conservative.
[01:14:30.840 --> 01:14:33.640]   Believe it or not.
[01:14:33.640 --> 01:14:34.640]   Do you think the entire --
[01:14:34.640 --> 01:14:41.920]   It's that Twitter or Facebook is -- I mean -- this is just absurd.
[01:14:41.920 --> 01:14:45.960]   Individuals have opinions and they're allowed to.
[01:14:45.960 --> 01:14:48.000]   We still for now.
[01:14:48.000 --> 01:14:55.640]   And all the evidence I've seen so far is anecdotal, including President Trump saying
[01:14:55.640 --> 01:15:02.080]   "Tweaker Twitter secretly reduced his follower account."
[01:15:02.080 --> 01:15:03.080]   But here's the ramp.
[01:15:03.080 --> 01:15:08.600]   Yeah, it's fun for everybody who used to love technology to go after it.
[01:15:08.600 --> 01:15:11.560]   Yeah, it's fun for the left to complain about the big old companies.
[01:15:11.560 --> 01:15:13.560]   But beware what you play in --
[01:15:13.560 --> 01:15:15.160]   Yeah, no kidding.
[01:15:15.160 --> 01:15:18.480]   Because this far-right is coming after and they're going to take advantage of every bit
[01:15:18.480 --> 01:15:25.080]   of this and they're going to try to force these companies to take down things that you
[01:15:25.080 --> 01:15:30.440]   like, to put up things that you hate, to go after them for the sake of going after them
[01:15:30.440 --> 01:15:32.760]   because they think they're being put upon.
[01:15:32.760 --> 01:15:36.600]   The same time that you're screaming "Take down the fake news" every time you do that,
[01:15:36.600 --> 01:15:41.520]   then the people who put up the fake news or in power are using that as an opportunity
[01:15:41.520 --> 01:15:44.920]   to go after the technology companies and in the end, you're internet.
[01:15:44.920 --> 01:15:45.920]   Yeah.
[01:15:45.920 --> 01:15:47.720]   So beware what you play into, folks.
[01:15:47.720 --> 01:15:48.720]   Beware.
[01:15:48.720 --> 01:15:54.000]   Yeah, I mean, I don't -- this is very much a strategy.
[01:15:54.000 --> 01:16:01.000]   And it's been a strategy that's been talked about for at least a decade of the -- I don't
[01:16:01.000 --> 01:16:06.800]   know if it's Republicans or if it's the far-right, but, you know, there's --
[01:16:06.800 --> 01:16:07.800]   And then there's --
[01:16:07.800 --> 01:16:08.800]   This is very frustrating.
[01:16:08.800 --> 01:16:09.800]   Berkeley, California.
[01:16:09.800 --> 01:16:10.800]   Yes.
[01:16:10.800 --> 01:16:11.800]   [Laughs]
[01:16:11.800 --> 01:16:22.360]   The city wants to require people who sell phones in the city to warn consumers about a radiation
[01:16:22.360 --> 01:16:27.560]   risk of carrying the phone in your pocket or apparently your bra.
[01:16:27.560 --> 01:16:32.200]   Berkeley's right to know ordinance came into effect in 2016 and the court --
[01:16:32.200 --> 01:16:33.760]   Right to know what?
[01:16:33.760 --> 01:16:35.440]   Right to know nothing.
[01:16:35.440 --> 01:16:45.560]   But anyway, Berkeley's been enforcing this and, of course, the CTIA, which is the Industry
[01:16:45.560 --> 01:16:53.320]   Association for Cell Phones, has sued and lost in court.
[01:16:53.320 --> 01:16:55.480]   So they sued in 2015.
[01:16:55.480 --> 01:16:57.480]   They sued in 2017.
[01:16:57.480 --> 01:17:02.360]   In 2018, the Supreme Court said they'd send it back.
[01:17:02.360 --> 01:17:05.000]   Once again, the city has won in court.
[01:17:05.000 --> 01:17:12.000]   The city has the right to warn consumers about cell phone radiation, even though, as far as
[01:17:12.000 --> 01:17:14.480]   we know, there's no evidence --
[01:17:14.480 --> 01:17:15.480]   Oh, science?
[01:17:15.480 --> 01:17:16.480]   Evidence?
[01:17:16.480 --> 01:17:17.480]   Good.
[01:17:17.480 --> 01:17:18.480]   Let's go for a few years.
[01:17:18.480 --> 01:17:20.960]   Well, let me read this to you.
[01:17:20.960 --> 01:17:26.280]   So, the radiation warning message required by the city from phone retailers is, quote,
[01:17:26.280 --> 01:17:30.720]   "To assure safety, the federal government requires cell phones meet radio frequency
[01:17:30.720 --> 01:17:32.320]   exposure guidelines."
[01:17:32.320 --> 01:17:33.720]   So far so good.
[01:17:33.720 --> 01:17:38.080]   If you carry your user phone in a pants or shirt pocket or tucked into a bra when the
[01:17:38.080 --> 01:17:42.640]   phone is on and connected to a wireless network, and why would you carry a phone that's off
[01:17:42.640 --> 01:17:44.400]   in your bra, I ask you?
[01:17:44.400 --> 01:17:48.560]   You may exceed the federal guideline for exposure to RF radiation.
[01:17:48.560 --> 01:17:52.800]   Refer to the instructions in your phone or user manual for information about how to
[01:17:52.800 --> 01:17:53.800]   use the phone.
[01:17:53.800 --> 01:17:55.440]   Why does it specify a bra?
[01:17:55.440 --> 01:17:56.440]   It's very strange.
[01:17:56.440 --> 01:17:58.440]   You could put it in your jockstrap, breast cancer.
[01:17:58.440 --> 01:17:59.440]   No.
[01:17:59.440 --> 01:18:02.080]   No, it's -- yeah, and it's close to your body.
[01:18:02.080 --> 01:18:03.640]   So your pocket and your --
[01:18:03.640 --> 01:18:04.640]   Your pocket.
[01:18:04.640 --> 01:18:05.640]   Well, but yeah.
[01:18:05.640 --> 01:18:06.640]   Oh, yeah.
[01:18:06.640 --> 01:18:08.480]   They should just say close to your body.
[01:18:08.480 --> 01:18:11.080]   Yeah, that would be sufficient to why specify.
[01:18:11.080 --> 01:18:15.720]   Well, because I stick my phone sometimes in my bra, if I'm going out jogging and I don't
[01:18:15.720 --> 01:18:16.880]   have any pockets in my draw.
[01:18:16.880 --> 01:18:17.880]   Yeah, of course.
[01:18:17.880 --> 01:18:18.880]   That's fine.
[01:18:18.880 --> 01:18:19.880]   But --
[01:18:19.880 --> 01:18:24.760]   So it is -- these are actually somewhat mutually contradictory sentences because the FCC does,
[01:18:24.760 --> 01:18:33.520]   in fact, require the cell phones, meet the RF exposure guidelines that they're not dangerous.
[01:18:33.520 --> 01:18:37.760]   And then the very next sentence is, well, just -- maybe they -- maybe they -- if you
[01:18:37.760 --> 01:18:40.560]   carry it there, it'll exceed the guidelines, which I don't think there's any evidence that
[01:18:40.560 --> 01:18:41.560]   it does.
[01:18:41.560 --> 01:18:42.560]   But anyway, I don't know.
[01:18:42.560 --> 01:18:44.360]   I'm of the opinion that there is no danger.
[01:18:44.360 --> 01:18:48.200]   In fact, for a while, people were saying, oh, you're going to get brain cancer if you
[01:18:48.200 --> 01:18:49.200]   hold the phone.
[01:18:49.200 --> 01:18:50.200]   Yeah.
[01:18:50.200 --> 01:18:52.800]   And in fact, the rates of brain cancer have fallen in the last time.
[01:18:52.800 --> 01:18:57.000]   Mind you, this is in the state of California where if you don't know folks, you go into
[01:18:57.000 --> 01:18:59.920]   almost any building anywhere.
[01:18:59.920 --> 01:19:05.240]   There is a sign up warning you that the built -- the building is built on property that contains
[01:19:05.240 --> 01:19:09.440]   carcinogens and that you're warned and you don't have to go in because it could kill you
[01:19:09.440 --> 01:19:10.440]   to be there.
[01:19:10.440 --> 01:19:13.360]   And it's devalued any warning whatsoever.
[01:19:13.360 --> 01:19:16.840]   Every building you go into is dangerous.
[01:19:16.840 --> 01:19:17.840]   Yeah.
[01:19:17.840 --> 01:19:23.360]   Every single building you go into as a Prop 65 warning, which means nobody pays any attention
[01:19:23.360 --> 01:19:25.160]   to the Prop 65 warning.
[01:19:25.160 --> 01:19:29.240]   It's like going to the Internet and the EU and having to click on cookies.
[01:19:29.240 --> 01:19:30.240]   Cookies.
[01:19:30.240 --> 01:19:31.240]   Yeah, cookies.
[01:19:31.240 --> 01:19:36.040]   Cookies cancer, cookies cancer, cookies cancer, okay, okay, okay, okay.
[01:19:36.040 --> 01:19:40.920]   I mean, this is a good example though because I mean, who would vote against that proposition?
[01:19:40.920 --> 01:19:47.560]   California voters were presented with a proposition that said, well, if there's chemicals known
[01:19:47.560 --> 01:19:50.600]   to cause cancer in an area, shouldn't there be a warning sign?
[01:19:50.600 --> 01:19:52.960]   Well, of course there should be.
[01:19:52.960 --> 01:19:53.960]   But --
[01:19:53.960 --> 01:19:56.680]   This is the problem with the verb could.
[01:19:56.680 --> 01:19:57.680]   Yeah.
[01:19:57.680 --> 01:19:58.680]   Yeah.
[01:19:58.680 --> 01:20:01.880]   There's a lot of chemicals caused cancer.
[01:20:01.880 --> 01:20:03.880]   I actually --
[01:20:03.880 --> 01:20:04.880]   Sunlight causes cancer.
[01:20:04.880 --> 01:20:05.880]   Let me go outside.
[01:20:05.880 --> 01:20:06.880]   Lots of problems.
[01:20:06.880 --> 01:20:07.880]   Yeah.
[01:20:07.880 --> 01:20:11.360]   I should tell you not to go outside because like causes cancer.
[01:20:11.360 --> 01:20:12.360]   Okay.
[01:20:12.360 --> 01:20:14.040]   Do we do a changelog here?
[01:20:14.040 --> 01:20:15.040]   Let's do a changelog.
[01:20:15.040 --> 01:20:16.040]   We need something.
[01:20:16.040 --> 01:20:17.040]   Do we have stuff for the changelog?
[01:20:17.040 --> 01:20:18.040]   We didn't last week.
[01:20:18.040 --> 01:20:19.040]   We do.
[01:20:19.040 --> 01:20:22.080]   We do now as the Trumpeters have indicated.
[01:20:22.080 --> 01:20:23.080]   Trumpet.
[01:20:23.080 --> 01:20:27.800]   Google has quietly launched a developer website for Fuchsia.
[01:20:27.800 --> 01:20:29.600]   Oh, yes.
[01:20:29.600 --> 01:20:30.600]   Excited.
[01:20:30.600 --> 01:20:34.360]   Sooner than you expected, you'll be able to develop.
[01:20:34.360 --> 01:20:37.600]   It's Fuchsia.dev.
[01:20:37.600 --> 01:20:41.160]   And it's a lot of white space.
[01:20:41.160 --> 01:20:42.160]   Okay.
[01:20:42.160 --> 01:20:44.120]   But here's the start here button.
[01:20:44.120 --> 01:20:45.920]   Let's -- here we go.
[01:20:45.920 --> 01:20:47.240]   Code of conduct glossary.
[01:20:47.240 --> 01:20:48.240]   Getting started.
[01:20:48.240 --> 01:20:51.440]   Everything you need to get started with Fuchsia.
[01:20:51.440 --> 01:20:56.120]   pseudo apt get install, build essential, got it, curl, got it, get got it, Python got
[01:20:56.120 --> 01:20:57.120]   it, unzip got it.
[01:20:57.120 --> 01:20:59.480]   Well, what else do I need?
[01:20:59.480 --> 01:21:00.480]   Oh.
[01:21:00.480 --> 01:21:08.680]   Then you have to go and get the source code and download and build it.
[01:21:08.680 --> 01:21:16.560]   And then boot it or run it in QIMO, the emulator.
[01:21:16.560 --> 01:21:19.360]   And then you can explore Fuchsia.
[01:21:19.360 --> 01:21:26.440]   Apparently, it has a couple of commands you can use, Fortune, to get your Fortune cookie
[01:21:26.440 --> 01:21:28.560]   and shut down.
[01:21:28.560 --> 01:21:29.560]   Fortune help and shut down.
[01:21:29.560 --> 01:21:31.560]   The three commands they've documented.
[01:21:31.560 --> 01:21:33.560]   Oh, and roll dice.
[01:21:33.560 --> 01:21:35.400]   You can roll dice.
[01:21:35.400 --> 01:21:37.720]   So anyway, that's a big deal, right?
[01:21:37.720 --> 01:21:45.240]   We've been talking about this new operating system from Google for some time.
[01:21:45.240 --> 01:21:46.240]   Still not clear.
[01:21:46.240 --> 01:21:47.320]   I'm not sure what it's for.
[01:21:47.320 --> 01:21:48.760]   Is it a phone operating system?
[01:21:48.760 --> 01:21:51.560]   Is it -- It's an everything operating system.
[01:21:51.560 --> 01:21:52.560]   Yeah.
[01:21:52.560 --> 01:21:53.560]   I thought it was for everything.
[01:21:53.560 --> 01:21:54.560]   What's a phone?
[01:21:54.560 --> 01:21:55.560]   What's a laptop?
[01:21:55.560 --> 01:21:59.200]   What's a -- it's all -- It's all computing everywhere.
[01:21:59.200 --> 01:22:04.200]   The Pixel C -- this is this -- now you still use your Pixel C?
[01:22:04.200 --> 01:22:07.200]   I don't use it, but I have it.
[01:22:07.200 --> 01:22:08.200]   And -- Yeah.
[01:22:08.200 --> 01:22:12.200]   -- Cuddles it at night and would like it to be back.
[01:22:12.200 --> 01:22:14.200]   I loved the Pixel C. I gave it away.
[01:22:14.200 --> 01:22:16.200]   Oh, my O my Nexus 7 is what I want.
[01:22:16.200 --> 01:22:17.200]   I missed that too.
[01:22:17.200 --> 01:22:18.200]   Mine is really good.
[01:22:18.200 --> 01:22:19.200]   Oh, I missed my Nexus 7.
[01:22:19.200 --> 01:22:20.200]   Superannuated.
[01:22:20.200 --> 01:22:22.200]   I still have it, but it's hard to keep it charged.
[01:22:22.200 --> 01:22:26.200]   Remember, last week we talked about the fact that the evidence was strong.
[01:22:26.200 --> 01:22:28.200]   Google was not going to do tablets.
[01:22:28.200 --> 01:22:35.840]   Now, according to 905 Google, the Pixel C did not receive a July patch -- security patch,
[01:22:35.840 --> 01:22:37.800]   but other Pixel devices did.
[01:22:37.800 --> 01:22:41.680]   Actually, it was Android police that noticed this.
[01:22:41.680 --> 01:22:47.200]   And according to the update schedule help page, there is no guarantee Pixel phones and tablets
[01:22:47.200 --> 01:22:49.560]   will get security updates after 18 months.
[01:22:49.560 --> 01:22:52.480]   And of course, the Pixel C came out in 2015.
[01:22:52.480 --> 01:22:54.680]   Hasn't been sold in two years.
[01:22:54.680 --> 01:22:58.160]   So if you have a Pixel C, you can see the phone.
[01:22:58.160 --> 01:22:59.160]   You're on secure.
[01:22:59.160 --> 01:23:01.160]   You can still use it.
[01:23:01.160 --> 01:23:02.160]   Just be careful.
[01:23:02.160 --> 01:23:05.040]   You're a little careful.
[01:23:05.040 --> 01:23:09.160]   There is a July security patch coming out, and among other things -- actually, I'm excited
[01:23:09.160 --> 01:23:13.880]   because apparently the Samsung version of it on the S10 is adding night mode to the Samsung
[01:23:13.880 --> 01:23:14.880]   camera.
[01:23:14.880 --> 01:23:17.560]   The preliminary shots I've seen look really good.
[01:23:17.560 --> 01:23:22.520]   But also an improved OK Google detection.
[01:23:22.520 --> 01:23:26.160]   That's if you have a Pixel 3, 3A or even a Pixel 2.
[01:23:26.160 --> 01:23:27.160]   Okay.
[01:23:27.160 --> 01:23:30.960]   So the Pixel 2 bootloader bugs in the Pixel 3 will be patched.
[01:23:30.960 --> 01:23:32.640]   Better music detection.
[01:23:32.640 --> 01:23:35.000]   You guys ever used that where it puts on the front of the phone?
[01:23:35.000 --> 01:23:36.000]   Oh, yeah.
[01:23:36.000 --> 01:23:37.000]   I use it all the time.
[01:23:37.000 --> 01:23:38.000]   I think it's so cool.
[01:23:38.000 --> 01:23:40.040]   You know, there's a log of all the songs that phones heard.
[01:23:40.040 --> 01:23:41.040]   Yeah.
[01:23:41.040 --> 01:23:44.480]   Well, they do it for most of its -- no, most of it's actually from a database that runs
[01:23:44.480 --> 01:23:45.480]   locally on the phone.
[01:23:45.480 --> 01:23:47.840]   Yeah, it's like 100,000-sunk database.
[01:23:47.840 --> 01:23:48.840]   However --
[01:23:48.840 --> 01:23:49.840]   It's kind of neat.
[01:23:49.840 --> 01:23:53.600]   They don't say anything about whether that log of all the things you've heard is sent
[01:23:53.600 --> 01:23:54.600]   back to Google.
[01:23:54.600 --> 01:23:55.600]   We don't know if that's the case.
[01:23:55.600 --> 01:23:58.760]   You could have really embarrassing taste, you know.
[01:23:58.760 --> 01:23:59.960]   No, it's not my fault.
[01:23:59.960 --> 01:24:00.960]   And I would just --
[01:24:00.960 --> 01:24:03.840]   I would start you what was playing in the restaurant.
[01:24:03.840 --> 01:24:04.840]   Yeah.
[01:24:04.840 --> 01:24:09.840]   Google Lens, if you come to San Francisco, besides wearing flowers in your hair, you
[01:24:09.840 --> 01:24:16.160]   might want to bring your Google phone and your Google Lens because Weskover, which is
[01:24:16.160 --> 01:24:20.520]   a San Francisco-based startup, has built a catalog of local artists and it's teaming
[01:24:20.520 --> 01:24:26.600]   up with Lens so that when you point your camera at an artwork in San Francisco, it'll
[01:24:26.600 --> 01:24:29.360]   recognize it and it'll tell you all about it.
[01:24:29.360 --> 01:24:34.240]   Apparently, 50,000 images of unique art and design from 6,000 local brands and independent
[01:24:34.240 --> 01:24:37.640]   artists are in the database.
[01:24:37.640 --> 01:24:40.480]   So pretty cool.
[01:24:40.480 --> 01:24:41.480]   Paid for that.
[01:24:41.480 --> 01:24:42.720]   Yeah, I don't know.
[01:24:42.720 --> 01:24:43.720]   Weskover, maybe.
[01:24:43.720 --> 01:24:45.480]   I don't know.
[01:24:45.480 --> 01:24:50.040]   They curated it.
[01:24:50.040 --> 01:24:52.680]   I guess -- I guess -- so here's the tweet.
[01:24:52.680 --> 01:24:57.320]   If you love the chair at the Ace Hotel, you just point Google Lens at it.
[01:24:57.320 --> 01:25:00.440]   It'll tell you where you can get one.
[01:25:00.440 --> 01:25:05.600]   Not much use if you're not in San Francisco.
[01:25:05.600 --> 01:25:08.760]   Google has patented a foldable phone.
[01:25:08.760 --> 01:25:14.040]   Patents don't always tell you much because, you know, maybe they're just making it.
[01:25:14.040 --> 01:25:15.200]   But it looks kind of like a book.
[01:25:15.200 --> 01:25:20.960]   It's got a round binding, it's got one, two, three, four, five pages.
[01:25:20.960 --> 01:25:25.000]   It's a foldable smartphone with multiple pages.
[01:25:25.000 --> 01:25:26.000]   So it's a book.
[01:25:26.000 --> 01:25:30.080]   Yeah, it's a book with each page as a display.
[01:25:30.080 --> 01:25:33.640]   Which means you can go back and forth a little bit.
[01:25:33.640 --> 01:25:36.720]   They're held together by a book like Spine.
[01:25:36.720 --> 01:25:40.120]   The back where the spine is, that's where the battery processor and camera are.
[01:25:40.120 --> 01:25:41.640]   I don't think they're going to make this.
[01:25:41.640 --> 01:25:43.760]   But they patented it.
[01:25:43.760 --> 01:25:46.320]   That's the problem with patents.
[01:25:46.320 --> 01:25:49.920]   Companies these days, patent, whatever they can.
[01:25:49.920 --> 01:25:53.640]   The new Google Maps will have transit updates.
[01:25:53.640 --> 01:25:55.000]   I really like this.
[01:25:55.000 --> 01:25:58.240]   I hope this is going to be rolled out all over the world.
[01:25:58.240 --> 01:26:03.400]   So you'll know when the train is on its way or even the bus, live traffic delays and so
[01:26:03.400 --> 01:26:04.400]   forth.
[01:26:04.400 --> 01:26:08.880]   Now, this data is funny because this data is -- you know, many cities and municipalities
[01:26:08.880 --> 01:26:10.720]   gather this data.
[01:26:10.720 --> 01:26:13.600]   But also many cities and municipalities want to keep it to themselves.
[01:26:13.600 --> 01:26:18.960]   Even though it's the taxpayers' data because they've got their app they want.
[01:26:18.960 --> 01:26:23.440]   And so sometimes it's hard for Google and others to get this kind of information.
[01:26:23.440 --> 01:26:26.480]   In this case, they're going to use -- they're going to crowdsource it.
[01:26:26.480 --> 01:26:33.960]   So if you're in Tokyo, you'll be able to see how many people are in the Shibuya station.
[01:26:33.960 --> 01:26:37.440]   By the way, there's a little warning at 9.40 AM.
[01:26:37.440 --> 01:26:39.720]   The patrons are usually standing room only.
[01:26:39.720 --> 01:26:45.280]   If you're going to Shibuya, Shunjuku, I've been on that train.
[01:26:45.280 --> 01:26:47.080]   That's a fun little subway.
[01:26:47.080 --> 01:26:48.080]   Boy, I love it.
[01:26:48.080 --> 01:26:53.560]   Down in my numbers, related is the most crowded subway line.
[01:26:53.560 --> 01:26:55.560]   So Google knows.
[01:26:55.560 --> 01:26:56.560]   Oh, Google knows.
[01:26:56.560 --> 01:27:03.320]   And of course, Fourth of July, Independence Day is tomorrow.
[01:27:03.320 --> 01:27:09.560]   If you search for fireworks, I want you to do this tomorrow on your phone, on your map.
[01:27:09.560 --> 01:27:10.560]   You can do it right now.
[01:27:10.560 --> 01:27:11.560]   Can you do it right now?
[01:27:11.560 --> 01:27:13.800]   And on that computer you're looking at.
[01:27:13.800 --> 01:27:16.440]   Baby, you're a firework.
[01:27:16.440 --> 01:27:17.440]   Let's do it.
[01:27:17.440 --> 01:27:18.440]   I'm going to go to Google.com.
[01:27:18.440 --> 01:27:20.040]   You've heard of that, right?
[01:27:20.040 --> 01:27:22.760]   It's this new site where you can find stuff.
[01:27:22.760 --> 01:27:23.760]   Fireworks.
[01:27:23.760 --> 01:27:24.760]   That's it?
[01:27:24.760 --> 01:27:27.080]   Do I have to say near me?
[01:27:27.080 --> 01:27:28.080]   Let's see.
[01:27:28.080 --> 01:27:29.240]   Anything with fireworks in it.
[01:27:29.240 --> 01:27:30.480]   Oh, look.
[01:27:30.480 --> 01:27:33.320]   There's fireworks in the search results.
[01:27:33.320 --> 01:27:34.920]   That's good.
[01:27:34.920 --> 01:27:36.760]   That's fun.
[01:27:36.760 --> 01:27:39.440]   I like that, by the way, I can buy fireworks.
[01:27:39.440 --> 01:27:43.560]   TNT fireworks right here in Petalama.
[01:27:43.560 --> 01:27:44.560]   Please don't.
[01:27:44.560 --> 01:27:49.120]   Did you see the annual Washington demonstration of what fireworks can blow up?
[01:27:49.120 --> 01:27:50.120]   You can do them wrong?
[01:27:50.120 --> 01:27:51.120]   No.
[01:27:51.120 --> 01:27:57.800]   On my Twitter feed, I retweeted a great post from, it just said "winner" from the Washington
[01:27:57.800 --> 01:27:58.800]   Post.
[01:27:58.800 --> 01:27:59.800]   It's where it's worth going to.
[01:27:59.800 --> 01:28:00.800]   You can find it.
[01:28:00.800 --> 01:28:03.080]   I'm going to your Twitter stream.
[01:28:03.080 --> 01:28:04.280]   This is you, right?
[01:28:04.280 --> 01:28:05.280]   Yeah.
[01:28:05.280 --> 01:28:06.280]   I got the blue check.
[01:28:06.280 --> 01:28:07.520]   Oh, but it's Leo.
[01:28:07.520 --> 01:28:08.520]   It's night mode.
[01:28:08.520 --> 01:28:10.440]   You know how I hate night mode.
[01:28:10.440 --> 01:28:12.760]   What are you doing with night mode on my Twitter feed?
[01:28:12.760 --> 01:28:14.160]   Twitter now has night mode.
[01:28:14.160 --> 01:28:15.160]   Sorry, Jeff.
[01:28:15.160 --> 01:28:16.160]   I don't want night mode.
[01:28:16.160 --> 01:28:17.160]   I use night mode.
[01:28:17.160 --> 01:28:18.160]   Sorry, Jeff.
[01:28:18.160 --> 01:28:19.160]   I like it dark.
[01:28:19.160 --> 01:28:20.160]   I don't like it dark.
[01:28:20.160 --> 01:28:21.160]   What's wrong with night mode?
[01:28:21.160 --> 01:28:22.160]   I like it like my coffee.
[01:28:22.160 --> 01:28:23.160]   I hate night mode.
[01:28:23.160 --> 01:28:25.160]   I like it dark.
[01:28:25.160 --> 01:28:27.000]   This is like 85 years old.
[01:28:27.000 --> 01:28:29.320]   What are you?
[01:28:29.320 --> 01:28:32.240]   Where is your tweet, my friend?
[01:28:32.240 --> 01:28:33.240]   Where?
[01:28:33.240 --> 01:28:35.240]   Isn't it under re-tweets and tweets and blazers, man?
[01:28:35.240 --> 01:28:37.720]   Oh, I don't know.
[01:28:37.720 --> 01:28:38.720]   Tweets and replies.
[01:28:38.720 --> 01:28:39.720]   What's that?
[01:28:39.720 --> 01:28:40.720]   Oh, that's hurting me.
[01:28:40.720 --> 01:28:42.240]   I don't understand how Twitter works.
[01:28:42.240 --> 01:28:43.240]   Tweets and replies.
[01:28:43.240 --> 01:28:44.240]   Here it is.
[01:28:44.240 --> 01:28:48.720]   If you go to Twitter search and look at her, my name, look for winner.
[01:28:48.720 --> 01:28:49.720]   Twitter search.
[01:28:49.720 --> 01:28:52.480]   This is not going to be worth it now, of course.
[01:28:52.480 --> 01:28:53.480]   Okay.
[01:28:53.480 --> 01:28:56.200]   Now, first I'll look for your name and then what?
[01:28:56.200 --> 01:28:59.440]   I think my one word re-tweet was winner.
[01:28:59.440 --> 01:29:01.320]   Winner, winner, chicken dinner.
[01:29:01.320 --> 01:29:03.400]   Does Twitter search work like that?
[01:29:03.400 --> 01:29:04.960]   If you go to advanced search.
[01:29:04.960 --> 01:29:06.560]   Oh, you didn't tell me that.
[01:29:06.560 --> 01:29:07.560]   You could find the winner.
[01:29:07.560 --> 01:29:11.200]   The winner and local will be the one who knows what's right around here.
[01:29:11.200 --> 01:29:12.200]   Jeff Jarvis.
[01:29:12.200 --> 01:29:13.200]   This is a 2010 tweet.
[01:29:13.200 --> 01:29:15.680]   It came up with a tweet from 2010.
[01:29:15.680 --> 01:29:19.120]   No, never mind.
[01:29:19.120 --> 01:29:21.920]   I'm excited because I am going to a Fourth of July party.
[01:29:21.920 --> 01:29:22.920]   I found it.
[01:29:22.920 --> 01:29:23.920]   Oh, good.
[01:29:23.920 --> 01:29:26.600]   It's not in dark mode because Carson is a...
[01:29:26.600 --> 01:29:27.600]   All right, thank you, Carson.
[01:29:27.600 --> 01:29:28.600]   Good work.
[01:29:28.600 --> 01:29:29.600]   This video is worth it.
[01:29:29.600 --> 01:29:30.600]   Okay.
[01:29:30.600 --> 01:29:32.320]   Go full screen and play the sound.
[01:29:32.320 --> 01:29:33.320]   Hold on.
[01:29:33.320 --> 01:29:34.320]   Oh, God.
[01:29:34.320 --> 01:29:36.240]   This is from the Washington Post.
[01:29:36.240 --> 01:29:42.720]   Every year the government demonstrates what you shouldn't do with fireworks.
[01:29:42.720 --> 01:29:46.320]   Do not set your friend's dress on fire.
[01:29:46.320 --> 01:29:48.680]   Do not put it in your hand.
[01:29:48.680 --> 01:29:50.240]   Oh, my God.
[01:29:50.240 --> 01:29:51.240]   Do not.
[01:29:51.240 --> 01:29:52.240]   Yes.
[01:29:52.240 --> 01:29:53.320]   Do not put it on your head.
[01:29:53.320 --> 01:29:54.320]   On your head.
[01:29:54.320 --> 01:29:56.600]   Do not look into the firework.
[01:29:56.600 --> 01:29:58.720]   This is great.
[01:29:58.720 --> 01:30:01.000]   They do this and you do it more than once.
[01:30:01.000 --> 01:30:02.640]   Oh, my God.
[01:30:02.640 --> 01:30:04.720]   Oh, my God.
[01:30:04.720 --> 01:30:07.240]   There's a watermelon being put on a lot.
[01:30:07.240 --> 01:30:08.240]   It's worth it.
[01:30:08.240 --> 01:30:12.120]   Oh, it's always good.
[01:30:12.120 --> 01:30:15.520]   How many people are going to go and try to blow up watermelons now?
[01:30:15.520 --> 01:30:17.920]   Oh, it's a lot.
[01:30:17.920 --> 01:30:18.920]   Me?
[01:30:18.920 --> 01:30:21.880]   I'm like, oh, that looks like a lot of stuff.
[01:30:21.880 --> 01:30:23.880]   It's a blowup, good.
[01:30:23.880 --> 01:30:25.880]   Oh, oh, oh.
[01:30:25.880 --> 01:30:26.880]   Oh, oh.
[01:30:26.880 --> 01:30:27.880]   Ah.
[01:30:27.880 --> 01:30:28.880]   Okay.
[01:30:28.880 --> 01:30:31.920]   So they take a bunch of dummies, mannequins.
[01:30:31.920 --> 01:30:34.240]   My firework deaths in 2018.
[01:30:34.240 --> 01:30:37.040]   Five doesn't seem like that big a number.
[01:30:37.040 --> 01:30:40.440]   Oh, she said his sister on fire.
[01:30:40.440 --> 01:30:45.840]   He said his sister on fire and, of course, because she's made a plastic, she's melting.
[01:30:45.840 --> 01:30:46.840]   She knows.
[01:30:46.840 --> 01:30:49.040]   So I will say two things.
[01:30:49.040 --> 01:30:52.440]   One, to share my Fourth of July party, I am going to.
[01:30:52.440 --> 01:30:54.880]   We'll have a hand surgeon there, which I think is awesome.
[01:30:54.880 --> 01:30:57.440]   Oh, in case of any finger loss.
[01:30:57.440 --> 01:30:58.440]   Just have it.
[01:30:58.440 --> 01:31:01.280]   I've been like, that's kind of a sucky job to have on the Fourth of July.
[01:31:01.280 --> 01:31:02.280]   But, okay.
[01:31:02.280 --> 01:31:07.240]   And then, two, when I was growing up in Houston, where we did fun things like actually TIP
[01:31:07.240 --> 01:31:14.480]   cows, I had a friend who decided to put a firework on his head and launch it at another
[01:31:14.480 --> 01:31:15.480]   friend.
[01:31:15.480 --> 01:31:17.120]   Oh, the one word.
[01:31:17.120 --> 01:31:20.880]   These were people who were, you know, 16 or 17.
[01:31:20.880 --> 01:31:26.160]   So never underestimate the stupidity of, I'm going to go with boys, teenage boys in
[01:31:26.160 --> 01:31:27.160]   particular.
[01:31:27.160 --> 01:31:28.160]   Yeah.
[01:31:28.160 --> 01:31:29.160]   Yeah, I'm not going to be lying.
[01:31:29.160 --> 01:31:30.160]   You're right.
[01:31:30.160 --> 01:31:31.160]   It's possible.
[01:31:31.160 --> 01:31:36.400]   It never occurred to me to like put a firecracker, a Roman candle on my head and, you know,
[01:31:36.400 --> 01:31:37.400]   Jesus.
[01:31:37.400 --> 01:31:38.400]   Yeah.
[01:31:38.400 --> 01:31:40.680]   I'm surprised that that person's still alive.
[01:31:40.680 --> 01:31:45.760]   By the way, if you do go to Google.com right now, you will get a chance to play the best
[01:31:45.760 --> 01:31:53.040]   baseball game ever with featuring mustard condiments and picnic items.
[01:31:53.040 --> 01:31:58.160]   And I'm now leading five runs to nothing because apparently I cannot fail.
[01:31:58.160 --> 01:32:02.040]   I just hit an eight out of three three foot home run.
[01:32:02.040 --> 01:32:04.080]   This is, this is how games should be.
[01:32:04.080 --> 01:32:07.640]   No matter how bad you are, oh, strike, strike, strike.
[01:32:07.640 --> 01:32:13.960]   Now that you're watching, yes, pickles are really bad at baseball.
[01:32:13.960 --> 01:32:14.960]   We do that.
[01:32:14.960 --> 01:32:15.960]   Is it a pickle?
[01:32:15.960 --> 01:32:16.960]   Oh, who would have?
[01:32:16.960 --> 01:32:19.680]   Oh, the curve ball got me out.
[01:32:19.680 --> 01:32:21.320]   It's a curve guy.
[01:32:21.320 --> 01:32:23.760]   Why would you eat celery on a holiday?
[01:32:23.760 --> 01:32:24.760]   Oh, it's delicious.
[01:32:24.760 --> 01:32:28.960]   It makes a lot of celery.
[01:32:28.960 --> 01:32:29.960]   There you go.
[01:32:29.960 --> 01:32:34.600]   Celery got its revenge than 845 foot arm solo homer here popcorn.
[01:32:34.600 --> 01:32:38.480]   Let's see how popcorn is going to do another home run.
[01:32:38.480 --> 01:32:42.720]   This is like the home run Derby 600 feet.
[01:32:42.720 --> 01:32:45.560]   I had a basis loaded home run when you weren't looking.
[01:32:45.560 --> 01:32:49.440]   Unfortunately, corn dog or dog another home run.
[01:32:49.440 --> 01:32:51.840]   This is the best game ever.
[01:32:51.840 --> 01:32:52.840]   You can.
[01:32:52.840 --> 01:32:53.840]   This is all lefty.
[01:32:53.840 --> 01:32:54.840]   Oh, lefty.
[01:32:54.840 --> 01:32:57.680]   Oh, lemonade lemonade lefty.
[01:32:57.680 --> 01:33:03.760]   I was able to swing twice at one curve ball and hit it the second time.
[01:33:03.760 --> 01:33:07.720]   Timing is important to the curve ball lemonade lefty.
[01:33:07.720 --> 01:33:10.120]   Now here's a.
[01:33:10.120 --> 01:33:11.120]   Great paid.
[01:33:11.120 --> 01:33:12.120]   Great paid.
[01:33:12.120 --> 01:33:14.920]   See, even if you hit it right to the first baseman, you get a hit.
[01:33:14.920 --> 01:33:15.920]   It's what?
[01:33:15.920 --> 01:33:16.920]   What?
[01:33:16.920 --> 01:33:17.920]   What?
[01:33:17.920 --> 01:33:20.760]   Green apple.
[01:33:20.760 --> 01:33:21.760]   I'm sorry.
[01:33:21.760 --> 01:33:22.760]   Why am I doing this?
[01:33:22.760 --> 01:33:23.760]   You might ask.
[01:33:23.760 --> 01:33:29.280]   I'm surprised because that's the Google change.
[01:33:29.280 --> 01:33:37.360]   How refreshing was that?
[01:33:37.360 --> 01:33:38.360]   How about this?
[01:33:38.360 --> 01:33:41.240]   The Pentagon has a laser.
[01:33:41.240 --> 01:33:47.160]   This is according to the MIT technology review that can identify people from a distance.
[01:33:47.160 --> 01:33:50.040]   Turns out your heartbeat is unique.
[01:33:50.040 --> 01:33:51.920]   It's just like your fingerprint.
[01:33:51.920 --> 01:34:00.440]   So even as far as 200 feet away, this laser can identify you not from your face, but from
[01:34:00.440 --> 01:34:01.440]   your heartbeat.
[01:34:01.440 --> 01:34:04.800]   But if you're wearing a thick sweater, it won't.
[01:34:04.800 --> 01:34:06.440]   Oh, thank God.
[01:34:06.440 --> 01:34:07.600]   It works through a short.
[01:34:07.600 --> 01:34:10.040]   It works through a jacket, but not a sweater.
[01:34:10.040 --> 01:34:14.240]   No, it works through, I think, certain types of like.
[01:34:14.240 --> 01:34:15.240]   Thick clothing.
[01:34:15.240 --> 01:34:18.720]   It doesn't work unless you have your phone in your pocket and it blocks it.
[01:34:18.720 --> 01:34:24.160]   This is why you wear it in your bra, actually.
[01:34:24.160 --> 01:34:25.880]   All right.
[01:34:25.880 --> 01:34:28.000]   Actually I just read an article.
[01:34:28.000 --> 01:34:30.200]   It was a little bit nerve-wracking.
[01:34:30.200 --> 01:34:34.600]   This is also for people who are planning to travel.
[01:34:34.600 --> 01:34:40.800]   More and more now we are seeing face recognition.
[01:34:40.800 --> 01:34:46.880]   It looks like face recognition is going to be coming to the airport near you soon.
[01:34:46.880 --> 01:34:48.880]   And Delta and others.
[01:34:48.880 --> 01:34:52.680]   And it was an article about face recognition.
[01:34:52.680 --> 01:34:56.560]   And from Wired, I opted out of face recognition at the airport.
[01:34:56.560 --> 01:34:58.160]   It wasn't easy.
[01:34:58.160 --> 01:35:00.120]   This is to me, this picture is that you.
[01:35:00.120 --> 01:35:02.200]   What I don't know is how do they know.
[01:35:02.200 --> 01:35:03.200]   Oh, that.
[01:35:03.200 --> 01:35:04.200]   Yeah, I do.
[01:35:04.200 --> 01:35:05.200]   I've done that.
[01:35:05.200 --> 01:35:06.200]   It's not face recognition.
[01:35:06.200 --> 01:35:07.200]   It's face recording.
[01:35:07.200 --> 01:35:08.200]   That's fantastic.
[01:35:08.200 --> 01:35:09.200]   No.
[01:35:09.200 --> 01:35:10.200]   That's.
[01:35:10.200 --> 01:35:16.040]   So when you get on the plane at a Delta Airlines, you have the choice and you're told you have
[01:35:16.040 --> 01:35:17.720]   the choice.
[01:35:17.720 --> 01:35:22.400]   It says Delta says only 2% of customers opt out of face recognition.
[01:35:22.400 --> 01:35:28.040]   What happens is you walk up instead of having to pull out a passport or any identification,
[01:35:28.040 --> 01:35:35.880]   they match your face to an existing customs and border patrol database of visas and passports.
[01:35:35.880 --> 01:35:41.800]   And he said, you know, in the article, is it a, is it a, he, Ali Funk?
[01:35:41.800 --> 01:35:42.800]   I don't know.
[01:35:42.800 --> 01:35:45.760]   He or she said, probably she, right?
[01:35:45.760 --> 01:35:48.640]   She said, they, maybe a they.
[01:35:48.640 --> 01:35:53.640]   I don't know what your pronoun is, Ali, that's why I was just saying default to the day is
[01:35:53.640 --> 01:35:56.680]   good, even though it's completely ungrammatical.
[01:35:56.680 --> 01:36:00.880]   So in order to get, first of all, she here, she, it, they had to know.
[01:36:00.880 --> 01:36:03.200]   Sorry, I'm just old.
[01:36:03.200 --> 01:36:08.560]   You can't expect me to change a lifetime of habits in a minute.
[01:36:08.560 --> 01:36:09.560]   We can, Leo.
[01:36:09.560 --> 01:36:11.280]   We can ask you to do that.
[01:36:11.280 --> 01:36:12.280]   Okay.
[01:36:12.280 --> 01:36:15.720]   They said, I'm going to pretend Ali is three people.
[01:36:15.720 --> 01:36:21.760]   They said that you, first of all, you had to know that you could turn it down and then
[01:36:21.760 --> 01:36:27.400]   you have to go over to the desk and then you have to say, nah, I don't want to use that.
[01:36:27.400 --> 01:36:31.280]   And then of course they're going to double scrutinize you, right?
[01:36:31.280 --> 01:36:36.160]   I wasn't clear to me because if you have to go to the desk or if you can just turn it
[01:36:36.160 --> 01:36:37.160]   down.
[01:36:37.160 --> 01:36:42.160]   So this person had to go to the desk because they didn't know if they could opt out or how
[01:36:42.160 --> 01:36:43.160]   to opt out.
[01:36:43.160 --> 01:36:47.720]   I wasn't sure if she or they just had to go, I know.
[01:36:47.720 --> 01:36:52.600]   They said, they said their recent experience suggests that they're incentivizing travelers
[01:36:52.600 --> 01:36:57.560]   to have their face scanned and disincentivizing them to sidestep the tech by not clearly, clearly
[01:36:57.560 --> 01:36:59.800]   communicating the alternative options.
[01:36:59.800 --> 01:37:00.800]   Right.
[01:37:00.800 --> 01:37:01.800]   Right.
[01:37:01.800 --> 01:37:07.480]   But that still leaves it up in the air is like, if I, now that I know that someone is
[01:37:07.480 --> 01:37:11.560]   able to do this just by saying, Hey, scan my passport.
[01:37:11.560 --> 01:37:15.600]   Is it possible for me to go through and just be like, I'm going to opt out of facial recognition,
[01:37:15.600 --> 01:37:16.600]   please scan my passport.
[01:37:16.600 --> 01:37:19.520]   I mean, the problem is not so much that they're using face recognition.
[01:37:19.520 --> 01:37:23.200]   Although that they're problems associated with that, but are they collecting images as
[01:37:23.200 --> 01:37:25.240]   well and storing them in a database?
[01:37:25.240 --> 01:37:30.560]   And this is a real problem because the real issue or another issue.
[01:37:30.560 --> 01:37:32.600]   Well, I'm not going to say which is real.
[01:37:32.600 --> 01:37:38.440]   That is a valid question you're asking is that the database is made up of government
[01:37:38.440 --> 01:37:43.280]   data that has now been shared with the airlines and nobody opted into that.
[01:37:43.280 --> 01:37:48.600]   So your data, your and remember, this is also a database operated by the customs and border
[01:37:48.600 --> 01:37:54.560]   control or border protection agency, which was hacked recently from one of their third
[01:37:54.560 --> 01:37:55.560]   parties.
[01:37:55.560 --> 01:37:56.720]   So other biometric data.
[01:37:56.720 --> 01:38:02.200]   So there's a lot of things where you're kind of like, and y'all, is this the best idea?
[01:38:02.200 --> 01:38:04.960]   Is this the best way to implement something like this?
[01:38:04.960 --> 01:38:05.960]   No.
[01:38:05.960 --> 01:38:13.240]   I signed up for clear and they do fingerprint and or virus and I love it.
[01:38:13.240 --> 01:38:14.240]   Yeah.
[01:38:14.240 --> 01:38:15.480]   But that you signed up for that.
[01:38:15.480 --> 01:38:17.120]   That's totally voluntary.
[01:38:17.120 --> 01:38:20.200]   That is also at the TSA security points.
[01:38:20.200 --> 01:38:21.200]   Yes, that's right.
[01:38:21.200 --> 01:38:22.200]   So I'm going to be.
[01:38:22.200 --> 01:38:23.720]   It's not boarding the plane.
[01:38:23.720 --> 01:38:25.640]   No, that's right.
[01:38:25.640 --> 01:38:26.640]   Anyway.
[01:38:26.640 --> 01:38:27.640]   Okay.
[01:38:27.640 --> 01:38:28.640]   Blah, blah.
[01:38:28.640 --> 01:38:33.640]   Well, I think the issue really is this is going to become universal.
[01:38:33.640 --> 01:38:35.240]   You're going to be everywhere you go.
[01:38:35.240 --> 01:38:37.800]   You're going to be scanned.
[01:38:37.800 --> 01:38:40.040]   Except in Oakland and San Francisco.
[01:38:40.040 --> 01:38:45.040]   It's illegal in San Francisco and another another municipality recently, like Somerville
[01:38:45.040 --> 01:38:46.520]   Masters somewhere recently.
[01:38:46.520 --> 01:38:47.520]   Yeah.
[01:38:47.520 --> 01:38:50.560]   For big Oakland did some social recognition.
[01:38:50.560 --> 01:38:52.560]   Good.
[01:38:52.560 --> 01:38:55.320]   Intel's getting out of the 5G smartphone market.
[01:38:55.320 --> 01:39:02.920]   And as a result, they are auctioning off their massive collection of patents, 8500 in
[01:39:02.920 --> 01:39:04.800]   all his rather final.
[01:39:04.800 --> 01:39:06.160]   Yeah, that's it.
[01:39:06.160 --> 01:39:07.160]   We're done.
[01:39:07.160 --> 01:39:09.960]   They are so happy to get this phone thing.
[01:39:09.960 --> 01:39:10.960]   It's over.
[01:39:10.960 --> 01:39:11.960]   They were so happy.
[01:39:11.960 --> 01:39:12.960]   It was it.
[01:39:12.960 --> 01:39:13.960]   You know what?
[01:39:13.960 --> 01:39:14.960]   They were waiting.
[01:39:14.960 --> 01:39:17.800]   They were watching to see what Apple and Qualcomm did.
[01:39:17.800 --> 01:39:21.560]   And as soon as Apple dropped their lawsuit against Qualcomm meeting, they could use Qualcomm's
[01:39:21.560 --> 01:39:23.320]   chips because they had been pressuring Intel.
[01:39:23.320 --> 01:39:26.360]   They said, until you got to develop 5G radios.
[01:39:26.360 --> 01:39:30.960]   Oh, as soon as Apple said, they have a five year deal with Qualcomm Intel said, we're
[01:39:30.960 --> 01:39:31.960]   out of it.
[01:39:31.960 --> 01:39:32.960]   We're done.
[01:39:32.960 --> 01:39:38.920]   And just in case Apple might try to come back to them, they're selling off the patents.
[01:39:38.920 --> 01:39:39.920]   So what do you.
[01:39:39.920 --> 01:39:41.920]   In Top Hot and fitting in way back in.
[01:39:41.920 --> 01:39:42.920]   Oh, they have a lot.
[01:39:42.920 --> 01:39:43.920]   In 2009.
[01:39:43.920 --> 01:39:44.920]   That's a lot of patents.
[01:39:44.920 --> 01:39:50.680]   Well, no, I'm just thinking they spent money trying to get here and man, Intel just really
[01:39:50.680 --> 01:39:51.680]   sucks at mobile.
[01:39:51.680 --> 01:39:52.840]   Who buys them Huawei?
[01:39:52.840 --> 01:39:55.800]   Oh, well, there's lots of people.
[01:39:55.800 --> 01:39:59.240]   But the sad thing is could just well be a patent role.
[01:39:59.240 --> 01:40:00.240]   I mean, Qualcomm.
[01:40:00.240 --> 01:40:01.240]   Yeah.
[01:40:01.240 --> 01:40:03.880]   And I said, Qualcomm has two businesses.
[01:40:03.880 --> 01:40:05.200]   One is making chips and radios.
[01:40:05.200 --> 01:40:09.800]   The other one is collecting intellectual property and suing people.
[01:40:09.800 --> 01:40:12.480]   I'm not sure I agree with that, but that is certainly one point of view.
[01:40:12.480 --> 01:40:17.080]   And certainly anybody who bought these patents might be buying them perhaps because they just
[01:40:17.080 --> 01:40:21.400]   want to license them or perhaps because it's an opportunity to go after Apple and everybody
[01:40:21.400 --> 01:40:22.400]   else.
[01:40:22.400 --> 01:40:23.400]   I don't know.
[01:40:23.400 --> 01:40:26.920]   Apple could buy them and use them as some sort of leverage against Qualcomm and patent
[01:40:26.920 --> 01:40:28.600]   negotiations or licensing negotiations.
[01:40:28.600 --> 01:40:32.480]   And Google in the past has done this, which is buy the patents and then say, we're not
[01:40:32.480 --> 01:40:33.480]   going to.
[01:40:33.480 --> 01:40:34.480]   This is defensive.
[01:40:34.480 --> 01:40:35.480]   We're trying to pursue them.
[01:40:35.480 --> 01:40:36.480]   We're not going to pursue them.
[01:40:36.480 --> 01:40:37.480]   Go ahead.
[01:40:37.480 --> 01:40:38.480]   Use them.
[01:40:38.480 --> 01:40:39.480]   And that I think that's right on.
[01:40:39.480 --> 01:40:41.840]   That's a good strategy.
[01:40:41.840 --> 01:40:47.360]   Should we declare our digital independence on the fourth and fifth hashtag social media
[01:40:47.360 --> 01:40:54.040]   strike Larry Sanger, blog post, we the strikers urge the global developer community to perfect
[01:40:54.040 --> 01:40:57.000]   a new system of decentralized social media.
[01:40:57.000 --> 01:41:01.560]   The strike will raise show the world typo.
[01:41:01.560 --> 01:41:05.480]   Big tech corporations, governments, developers and social media users is a massive demand
[01:41:05.480 --> 01:41:09.840]   for a system in which each of us individually owns our own data.
[01:41:09.840 --> 01:41:11.720]   Social media services stop acting as silos.
[01:41:11.720 --> 01:41:18.360]   I agree with all this compete to create the best user experience and have a standard set
[01:41:18.360 --> 01:41:20.400]   of protocols and standards.
[01:41:20.400 --> 01:41:22.960]   I love all I almost left a comment on this.
[01:41:22.960 --> 01:41:25.280]   And I decided not to be snarky.
[01:41:25.280 --> 01:41:27.280]   I'm saying I'll miss the fun of you.
[01:41:27.280 --> 01:41:29.280]   See you on the Saturday.
[01:41:29.280 --> 01:41:31.000]   I'm in man.
[01:41:31.000 --> 01:41:33.400]   You're not going to see me on social media.
[01:41:33.400 --> 01:41:36.480]   You pretty much don't anyway, but I don't have Facebook.
[01:41:36.480 --> 01:41:38.040]   I don't have Instagram.
[01:41:38.040 --> 01:41:44.480]   I have a Twitter account, but it's just more like because I didn't want to delete it.
[01:41:44.480 --> 01:41:47.760]   It's because that's where you talk to Jeff and I have.
[01:41:47.760 --> 01:41:49.400]   Have I ever tweeted you?
[01:41:49.400 --> 01:41:50.400]   Yes, once.
[01:41:50.400 --> 01:41:51.400]   Really?
[01:41:51.400 --> 01:41:52.400]   I think so.
[01:41:52.400 --> 01:41:53.400]   Must have been drunk.
[01:41:53.400 --> 01:41:54.400]   Oh, it was still memorable.
[01:41:54.400 --> 01:41:56.280]   I feel like we've had it.
[01:41:56.280 --> 01:41:59.000]   And I know Jeff and I tweeted it for now and then I'd each other.
[01:41:59.000 --> 01:42:02.800]   Sometimes I'll see something and I'll go, I'll retweet it or I'll say, yeah, just to show
[01:42:02.800 --> 01:42:05.520]   that I'm not, you know, a complete curmudgeon.
[01:42:05.520 --> 01:42:10.600]   Finally, some punishment for Equifax, except it's not the punishment I would like to see.
[01:42:10.600 --> 01:42:16.760]   The Equifax CIO knew about the Equifax breach.
[01:42:16.760 --> 01:42:21.120]   Of course, he's the chief information officer long before the public did, decided he was
[01:42:21.120 --> 01:42:26.400]   going to dump his stock and as a result avoided, I think, $17 million in losses.
[01:42:26.400 --> 01:42:29.440]   No, I'm sorry, not even that much.
[01:42:29.440 --> 01:42:32.840]   $117,000 in losses and gained $480,000.
[01:42:32.840 --> 01:42:36.320]   But he wasn't the only one.
[01:42:36.320 --> 01:42:38.320]   There was a CFO did the same thing.
[01:42:38.320 --> 01:42:39.320]   There might be more.
[01:42:39.320 --> 01:42:42.120]   I'm not saying this is the last John Yang got four months.
[01:42:42.120 --> 01:42:47.040]   He's going to jail and got four months and he has to pay back all the money he made.
[01:42:47.040 --> 01:42:51.560]   He's already trading four months in prison, year of supervised release, $55,000 fine in
[01:42:51.560 --> 01:42:55.000]   order to pay $117,000 and $117,000.
[01:42:55.000 --> 01:43:00.840]   No, $117,000 and $117,000, which is a very strange fine.
[01:43:00.840 --> 01:43:03.240]   I guess because he, I don't know.
[01:43:03.240 --> 01:43:05.840]   Maybe that's how much he made or saved or didn't lose.
[01:43:05.840 --> 01:43:07.640]   Actually, I think that's what it is.
[01:43:07.640 --> 01:43:09.040]   Good on that.
[01:43:09.040 --> 01:43:11.320]   There are other executives did the exact same thing.
[01:43:11.320 --> 01:43:12.320]   So better not just be the CIO.
[01:43:12.320 --> 01:43:14.720]   I hope they all get it, baby.
[01:43:14.720 --> 01:43:18.600]   Still, I think there should be punishment for just losing that data.
[01:43:18.600 --> 01:43:19.600]   Yes.
[01:43:19.600 --> 01:43:21.440]   That we're not going to see.
[01:43:21.440 --> 01:43:23.600]   That we're not going to see insider trading.
[01:43:23.600 --> 01:43:26.680]   You can't do that in this country.
[01:43:26.680 --> 01:43:30.840]   He got more time than he got.
[01:43:30.840 --> 01:43:34.280]   What if the story is in a lot longer?
[01:43:34.280 --> 01:43:35.280]   Yeah.
[01:43:35.280 --> 01:43:36.280]   Yeah.
[01:43:36.280 --> 01:43:39.120]   How about that?
[01:43:39.120 --> 01:43:40.920]   Anything else you want to talk about before I.
[01:43:40.920 --> 01:43:47.560]   What does your world as it roiled feathers that Microsoft is going to kill books?
[01:43:47.560 --> 01:43:49.400]   Yeah, we've been talking about it.
[01:43:49.400 --> 01:43:51.520]   I mean, they're doing it in the best possible way.
[01:43:51.520 --> 01:43:55.640]   They're paying, first of all, not many people took advantage of Microsoft.
[01:43:55.640 --> 01:43:56.640]   Ever lost.
[01:43:56.640 --> 01:43:59.120]   You used Edge, their browser.
[01:43:59.120 --> 01:44:02.640]   I mean, that wasn't the best way to read a book for anybody.
[01:44:02.640 --> 01:44:09.920]   However, they've decided as of yesterday, I think, to turn the server off, which means
[01:44:09.920 --> 01:44:13.160]   because it's copy protected, those ebooks will no longer work.
[01:44:13.160 --> 01:44:14.760]   They're refunding every penny.
[01:44:14.760 --> 01:44:15.760]   Here's the bigger issue.
[01:44:15.760 --> 01:44:20.520]   If you have annotations, those will be lost, but they said they're going to pay an extra
[01:44:20.520 --> 01:44:23.120]   25 bucks if you have the annotation.
[01:44:23.120 --> 01:44:28.320]   If you're a researcher, a professor is doing it bad.
[01:44:28.320 --> 01:44:30.640]   Is there a reason I raised the book?
[01:44:30.640 --> 01:44:34.640]   Would there have been a technical alternative?
[01:44:34.640 --> 01:44:35.640]   Yes.
[01:44:35.640 --> 01:44:36.640]   Thank you.
[01:44:36.640 --> 01:44:39.480]   Not for Microsoft, but for you, the user.
[01:44:39.480 --> 01:44:44.040]   And it might prompt people to think about all of the content they have that's copy protected.
[01:44:44.040 --> 01:44:48.960]   The TV shows, the movies, the books that they've bought from various vendors, it's copy protected
[01:44:48.960 --> 01:44:51.840]   if the vendor decides no longer to support it.
[01:44:51.840 --> 01:44:53.280]   Those will disappear from your library.
[01:44:53.280 --> 01:44:54.960]   Your library is gone.
[01:44:54.960 --> 01:45:00.160]   So maybe now's the time to download those and take the copy protection out, in which case
[01:45:00.160 --> 01:45:02.400]   they'll continue to work.
[01:45:02.400 --> 01:45:07.000]   We had a long conversation on Twitter on Sunday about whether that was legal.
[01:45:07.000 --> 01:45:09.760]   And one panelist said it was, one said it wasn't.
[01:45:09.760 --> 01:45:11.160]   It's not clear.
[01:45:11.160 --> 01:45:13.880]   You're making a copy for archival purposes.
[01:45:13.880 --> 01:45:15.560]   That's legal sort of.
[01:45:15.560 --> 01:45:18.600]   The DMCA says you can't reverse engineer copy protection.
[01:45:18.600 --> 01:45:20.920]   That's illegal sort of.
[01:45:20.920 --> 01:45:23.240]   It's illegal to make a copy, but it's illegal to break the deal.
[01:45:23.240 --> 01:45:24.680]   You can't read it.
[01:45:24.680 --> 01:45:25.680]   Yes.
[01:45:25.680 --> 01:45:27.680]   You can copy it, but you can't.
[01:45:27.680 --> 01:45:28.680]   Yeah, you can't cut.
[01:45:28.680 --> 01:45:34.360]   And then from a purely pragmatic point of view, what are they going to, they're not going
[01:45:34.360 --> 01:45:36.960]   to throw you in jail for doing that, especially if you bought the book and they turned up.
[01:45:36.960 --> 01:45:37.960]   They turned off the server.
[01:45:37.960 --> 01:45:41.960]   So I think you could find a way.
[01:45:41.960 --> 01:45:47.480]   Oh, we have a, we talked about Twitter and Trump's tweets.
[01:45:47.480 --> 01:45:48.480]   Yes.
[01:45:48.480 --> 01:45:50.000]   And now there is breaking news happening.
[01:45:50.000 --> 01:45:51.280]   Oh, this morning.
[01:45:51.280 --> 01:45:52.280]   Yes.
[01:45:52.280 --> 01:45:53.600]   So this was only sort of breaking.
[01:45:53.600 --> 01:45:54.600]   It's just in.
[01:45:54.600 --> 01:45:58.680]   Oh, no, this was just in, just updated.
[01:45:58.680 --> 01:46:03.520]   The Justice Department is now looking at one of Trump's tweets and based on the fact
[01:46:03.520 --> 01:46:11.360]   that Trump is saying that it is fake news that the Commerce Department will stop printing
[01:46:11.360 --> 01:46:14.360]   the census without the citizenship question.
[01:46:14.360 --> 01:46:15.640]   You can't make up his mind.
[01:46:15.640 --> 01:46:19.080]   So two days ago, he said, no, we're going to go forward.
[01:46:19.080 --> 01:46:20.640]   He lost in the Supreme Court.
[01:46:20.640 --> 01:46:21.640]   Right.
[01:46:21.640 --> 01:46:24.320]   Two days ago, he said, no, we're going to pursue this.
[01:46:24.320 --> 01:46:26.920]   Yesterday he said, no, we're not going to today.
[01:46:26.920 --> 01:46:29.800]   He tweeted, yes, we are.
[01:46:29.800 --> 01:46:31.200]   And now the.
[01:46:31.200 --> 01:46:34.280]   This is why you have to read the tweets.
[01:46:34.280 --> 01:46:37.840]   And now, yeah, now they're like, okay, we're trying to figure out how to do this.
[01:46:37.840 --> 01:46:41.040]   Can you find me a tank to go stand in front of, please?
[01:46:41.040 --> 01:46:42.600]   Yes, actually.
[01:46:42.600 --> 01:46:44.160]   Yeah, there'll be tanks.
[01:46:44.160 --> 01:46:45.920]   They're moving through the streets of T.T.
[01:46:45.920 --> 01:46:48.440]   Go right now.
[01:46:48.440 --> 01:46:49.640]   Okay.
[01:46:49.640 --> 01:46:51.920]   What something is happening?
[01:46:51.920 --> 01:46:53.320]   Oh, I see.
[01:46:53.320 --> 01:46:54.320]   Okay.
[01:46:54.320 --> 01:46:55.800]   Is it time to talk about?
[01:46:55.800 --> 01:47:00.400]   Yeah, we're doing things, but just one little question.
[01:47:00.400 --> 01:47:05.520]   I think it's going to be now every president from now on will be making pronouncements
[01:47:05.520 --> 01:47:06.520]   on Twitter.
[01:47:06.520 --> 01:47:07.520]   Yes.
[01:47:07.520 --> 01:47:09.640]   Is that the good thing or a bad thing?
[01:47:09.640 --> 01:47:11.040]   Is that the future of direct communication?
[01:47:11.040 --> 01:47:14.120]   I don't think it's, I don't think it's that.
[01:47:14.120 --> 01:47:18.080]   I don't think it's a bad thing to have a direct line of communication on Twitter.
[01:47:18.080 --> 01:47:25.840]   I think most presidents, hopefully going forward after this will be more measured in less.
[01:47:25.840 --> 01:47:26.840]   Obama tweeted.
[01:47:26.840 --> 01:47:27.840]   Yeah.
[01:47:27.840 --> 01:47:28.840]   He did.
[01:47:28.840 --> 01:47:34.320]   No one was ever, when Obama tweeted, you felt like Obama probably spent 20 seconds thinking
[01:47:34.320 --> 01:47:38.600]   about that tweet, if not a lot more time before he actually hit the button.
[01:47:38.600 --> 01:47:39.600]   Yeah.
[01:47:39.600 --> 01:47:41.600]   And so I think that's the personality.
[01:47:41.600 --> 01:47:42.600]   Yes.
[01:47:42.600 --> 01:47:50.480]   I never got the impression that those were really Obama's tweets though.
[01:47:50.480 --> 01:47:52.800]   That seemed only the ones that were signed by him.
[01:47:52.800 --> 01:47:53.800]   Right.
[01:47:53.800 --> 01:47:58.720]   In fact, they had a protocol right where you had to sign them because they thought ahead
[01:47:58.720 --> 01:48:03.680]   about like, what does it mean to have a tweet that says it's from POTUS?
[01:48:03.680 --> 01:48:05.200]   Is it really from POTUS?
[01:48:05.200 --> 01:48:09.280]   If so, we should probably tell people that this is an actual POTUS tweet.
[01:48:09.280 --> 01:48:14.960]   So they viewed it less as a, they viewed it as a way to reach out to constituents or people
[01:48:14.960 --> 01:48:18.880]   just the citizenry, which I don't think is a negative way to use something like Twitter
[01:48:18.880 --> 01:48:20.800]   or Instagram or Facebook.
[01:48:20.800 --> 01:48:28.880]   I think the issue comes when you don't use it judiciously.
[01:48:28.880 --> 01:48:34.320]   So apparently the fake news is now making fakes of Donald Trump saying we're not going
[01:48:34.320 --> 01:48:37.400]   to pursue it because I saw it clearly on video.
[01:48:37.400 --> 01:48:39.680]   And now he's saying, no, that's fake.
[01:48:39.680 --> 01:48:41.680]   So they're going to move forward.
[01:48:41.680 --> 01:48:45.560]   I can't even read his tweets because they're so offensive.
[01:48:45.560 --> 01:48:50.040]   For instance, many of these illegal aliens are living far better now than where they were
[01:48:50.040 --> 01:48:53.880]   is so incredibly offensive to me.
[01:48:53.880 --> 01:48:55.520]   Awful.
[01:48:55.520 --> 01:48:56.920]   I don't even know where to start.
[01:48:56.920 --> 01:48:57.920]   So I would love her.
[01:48:57.920 --> 01:49:03.360]   I'm not going to follow her Twitter to say, you know, make the great box and then also
[01:49:03.360 --> 01:49:07.000]   put a fact check of the photos that we currently have.
[01:49:07.000 --> 01:49:08.800]   Yeah, fact check.
[01:49:08.800 --> 01:49:10.480]   And by the way, they're not illegal.
[01:49:10.480 --> 01:49:14.200]   They're seeking asylum, which is legal.
[01:49:14.200 --> 01:49:20.000]   Okay, things, not darkness.
[01:49:20.000 --> 01:49:21.680]   Yeah, let's agree or something.
[01:49:21.680 --> 01:49:23.800]   I thought you had a try before you came on.
[01:49:23.800 --> 01:49:29.120]   I got to remind everybody real quickly that we would love it if you would subscribe to
[01:49:29.120 --> 01:49:30.120]   this show.
[01:49:30.120 --> 01:49:34.920]   Do we have a recording of this or do I do this live on this show?
[01:49:34.920 --> 01:49:37.280]   So-called Twit House ad.
[01:49:37.280 --> 01:49:42.400]   I believe you live on the show because I have recorded this as well.
[01:49:42.400 --> 01:49:45.680]   We truly appreciate you listening to this podcast.
[01:49:45.680 --> 01:49:50.640]   Every day it takes a team of hosts, producers, editors, engineers and support staff to get
[01:49:50.640 --> 01:49:52.120]   our content to you.
[01:49:52.120 --> 01:49:54.880]   And by the way- Not to mention, not to mention friends and family.
[01:49:54.880 --> 01:49:55.880]   Things and family.
[01:49:55.880 --> 01:49:57.440]   Well, you're in the hosts category.
[01:49:57.440 --> 01:49:58.440]   Oh.
[01:49:58.440 --> 01:50:00.400]   As you know, our podcasts are free and ad supported.
[01:50:00.400 --> 01:50:03.720]   So the easiest thing you can do to help our network is to subscribe to this show and
[01:50:03.720 --> 01:50:07.160]   all your favorite Twit podcasts.
[01:50:07.160 --> 01:50:11.440]   Subscribing now means you'll get each new episode the moment it publishes.
[01:50:11.440 --> 01:50:12.440]   Thank you for listening.
[01:50:12.440 --> 01:50:18.400]   Remember to subscribe on your favorite podcast application or visit twit.tv/subscribe.
[01:50:18.400 --> 01:50:24.200]   This concludes today's house ad.
[01:50:24.200 --> 01:50:27.200]   I never liked reading PSAs on the radio either.
[01:50:27.200 --> 01:50:30.800]   You can be more friendly.
[01:50:30.800 --> 01:50:33.880]   You can be like, "Hey, guys, if you love the show, this is how we make money."
[01:50:33.880 --> 01:50:36.240]   You know, I say this at the end of every show.
[01:50:36.240 --> 01:50:40.000]   Subscriptions doesn't make us money, but it does help us by getting more consistent
[01:50:40.000 --> 01:50:41.000]   downloads.
[01:50:41.000 --> 01:50:43.200]   You know, you get this for free.
[01:50:43.200 --> 01:50:45.040]   Sometimes you don't have ads.
[01:50:45.040 --> 01:50:46.040]   Subscribe.
[01:50:46.040 --> 01:50:50.360]   You can do this two hours of free content.
[01:50:50.360 --> 01:50:51.880]   What do we want to call it?
[01:50:51.880 --> 01:50:52.880]   Subscribe.
[01:50:52.880 --> 01:50:55.320]   And subscribe to it and get more of it.
[01:50:55.320 --> 01:50:56.320]   Come on.
[01:50:56.320 --> 01:50:59.320]   You don't ask, butch.
[01:50:59.320 --> 01:51:01.320]   That's all we ask.
[01:51:01.320 --> 01:51:03.320]   Effin' subscribe, huh?
[01:51:03.320 --> 01:51:04.400]   Effin' subscribe.
[01:51:04.400 --> 01:51:05.400]   Thank you.
[01:51:05.400 --> 01:51:07.800]   Take my ad out and leave that one in.
[01:51:07.800 --> 01:51:08.800]   That's good.
[01:51:08.800 --> 01:51:10.480]   Effin' subscribe.
[01:51:10.480 --> 01:51:13.080]   Stacy, do you have a pick this week?
[01:51:13.080 --> 01:51:14.240]   I sort of do.
[01:51:14.240 --> 01:51:17.760]   I know you wanted me to make boomerangs, my pick, but I was going to do--
[01:51:17.760 --> 01:51:23.280]   Before the show, Stacy was talking about these frozen fake Australian pies that are
[01:51:23.280 --> 01:51:28.480]   made in Austin, Ralia, called boomerangs, which she loves.
[01:51:28.480 --> 01:51:29.480]   They're delicious.
[01:51:29.480 --> 01:51:30.480]   They're delicious.
[01:51:30.480 --> 01:51:33.120]   And it's hard to find delicious frozen foods.
[01:51:33.120 --> 01:51:34.520]   Hand pies.
[01:51:34.520 --> 01:51:38.080]   Do you micro them or do you put them in your fine June oven?
[01:51:38.080 --> 01:51:40.680]   You can do either or you can do a hybrid.
[01:51:40.680 --> 01:51:42.880]   I just micro-wim 'cause I've got things to do.
[01:51:42.880 --> 01:51:43.880]   You're gonna hurry.
[01:51:43.880 --> 01:51:46.080]   I think the June says, "This is below me."
[01:51:46.080 --> 01:51:47.920]   I have a $15,000 oven.
[01:51:47.920 --> 01:51:48.920]   I don't do this.
[01:51:48.920 --> 01:51:49.920]   Oh, no, no, no.
[01:51:49.920 --> 01:51:55.800]   Oh, you guys though, the June with the tariffs.
[01:51:55.800 --> 01:52:01.640]   They now have a $5.99 version, but I just got an email saying, "Oh, July 1st," that because
[01:52:01.640 --> 01:52:08.040]   of tariff concerns, they are raising the price by $100, so that is happening in action.
[01:52:08.040 --> 01:52:09.040]   But...
[01:52:09.040 --> 01:52:11.800]   Which doesn't make me feel better 'cause I paid $1,500.
[01:52:11.800 --> 01:52:13.640]   You know what?
[01:52:13.640 --> 01:52:14.880]   I love my June so much.
[01:52:14.880 --> 01:52:16.840]   I don't even care that I paid double the price.
[01:52:16.840 --> 01:52:18.760]   It is or triple the price it is now.
[01:52:18.760 --> 01:52:23.360]   That is such impressive cognitive dissonance.
[01:52:23.360 --> 01:52:24.360]   That is really...
[01:52:24.360 --> 01:52:26.720]   I think you call it maturity, don't you, Leo?
[01:52:26.720 --> 01:52:27.720]   That's really impressive.
[01:52:27.720 --> 01:52:28.720]   Yeah.
[01:52:28.720 --> 01:52:29.720]   I love it.
[01:52:29.720 --> 01:52:32.480]   There's nothing else I can do with it and it still makes me happy.
[01:52:32.480 --> 01:52:34.520]   So I'm like, "Yeah, I paid more, but that's okay."
[01:52:34.520 --> 01:52:37.560]   It's an Android, it's an Android-based toaster oven.
[01:52:37.560 --> 01:52:40.680]   It's an convection oven and it's actually quite good.
[01:52:40.680 --> 01:52:41.680]   I love it.
[01:52:41.680 --> 01:52:45.880]   I paid a little too much for it, but I think it's worth $500,600 bucks.
[01:52:45.880 --> 01:52:46.880]   Absolutely.
[01:52:46.880 --> 01:52:47.880]   Easily.
[01:52:47.880 --> 01:52:48.880]   Okay.
[01:52:48.880 --> 01:52:49.880]   So today's toy.
[01:52:49.880 --> 01:52:54.240]   I haven't put it in my car yet, but one of my awesome people, and he may even be listening,
[01:52:54.240 --> 01:52:55.800]   "Thank you, Sean!"
[01:52:55.800 --> 01:52:58.160]   That looks like a face.
[01:52:58.160 --> 01:53:00.120]   It is the Android Auto.
[01:53:00.120 --> 01:53:01.520]   Oh, upside down.
[01:53:01.520 --> 01:53:03.520]   I want that.
[01:53:03.520 --> 01:53:04.760]   I want it too.
[01:53:04.760 --> 01:53:07.120]   They haven't invited me yet.
[01:53:07.120 --> 01:53:08.120]   What is it?
[01:53:08.120 --> 01:53:09.120]   They didn't invite me either.
[01:53:09.120 --> 01:53:10.120]   Is this Amazon?
[01:53:10.120 --> 01:53:11.120]   So this is the Android...
[01:53:11.120 --> 01:53:13.840]   I keep saying Android, I'm sorry.
[01:53:13.840 --> 01:53:16.480]   It's the Echo Auto.
[01:53:16.480 --> 01:53:20.400]   They teased this a year ago.
[01:53:20.400 --> 01:53:22.640]   Oh, so they finally it's out.
[01:53:22.640 --> 01:53:23.640]   Yes.
[01:53:23.640 --> 01:53:28.040]   Echo Auto, add "You know who" to your car.
[01:53:28.040 --> 01:53:30.320]   Take "You know who" on the road.
[01:53:30.320 --> 01:53:33.760]   It's $24.99, so it's not expensive.
[01:53:33.760 --> 01:53:35.280]   Have you set it up yet?
[01:53:35.280 --> 01:53:36.560]   No, this is...
[01:53:36.560 --> 01:53:39.920]   So I got it, I actually got it on Friday, and I didn't set it up yet.
[01:53:39.920 --> 01:53:41.520]   But I'm showing it to you now.
[01:53:41.520 --> 01:53:45.200]   Next week I'll tell you how it works, but I can tell you it's going to let me play my
[01:53:45.200 --> 01:53:46.200]   music.
[01:53:46.200 --> 01:53:47.200]   Yay.
[01:53:47.200 --> 01:53:48.280]   It doesn't work with certain cars.
[01:53:48.280 --> 01:53:52.840]   So if you go to the website, I'm going to put it in my Tesla, so that's fine.
[01:53:52.840 --> 01:53:54.640]   It does work on Teslas.
[01:53:54.640 --> 01:53:56.040]   It does work on Teslas.
[01:53:56.040 --> 01:54:00.720]   It gives me local search, local traffic, news, podcasts, podcasts.
[01:54:00.720 --> 01:54:04.000]   So I see two wires.
[01:54:04.000 --> 01:54:05.760]   So one is obviously power.
[01:54:05.760 --> 01:54:09.200]   Do you then have to connect it to the Jog's jack on your car?
[01:54:09.200 --> 01:54:10.200]   Yes.
[01:54:10.200 --> 01:54:11.200]   Here's my auxiliary...
[01:54:11.200 --> 01:54:12.200]   I mean, auxiliary.
[01:54:12.200 --> 01:54:13.200]   Do you have it?
[01:54:13.200 --> 01:54:15.440]   But your Tesla doesn't have an Jog's jack.
[01:54:15.440 --> 01:54:18.120]   Well, then I just tie it into this.
[01:54:18.120 --> 01:54:19.120]   Realy?
[01:54:19.120 --> 01:54:20.120]   It's not Bluetooth.
[01:54:20.120 --> 01:54:22.880]   Oh, but you could use a USB cable.
[01:54:22.880 --> 01:54:23.880]   I get it.
[01:54:23.880 --> 01:54:24.880]   USB cable.
[01:54:24.880 --> 01:54:27.440]   And it's USB to micro USB, so that's what you need to know there.
[01:54:27.440 --> 01:54:33.520]   Well, the Tesla would be smart enough to recognize that that's a source of audio.
[01:54:33.520 --> 01:54:34.520]   We're going to see.
[01:54:34.520 --> 01:54:35.520]   We'll find out.
[01:54:35.520 --> 01:54:40.880]   And, and there was something else I was going to tell you about it, but I forgot.
[01:54:40.880 --> 01:54:45.720]   So you can ask for maps and if the car is smart enough?
[01:54:45.720 --> 01:54:47.160]   It won't work.
[01:54:47.160 --> 01:54:48.480]   So it doesn't work well.
[01:54:48.480 --> 01:54:51.720]   It doesn't play nicely with Android Auto or with Apple CarPlay.
[01:54:51.720 --> 01:54:52.720]   Oh.
[01:54:52.720 --> 01:54:53.720]   That's the other thing you need to know.
[01:54:53.720 --> 01:54:54.720]   Oh.
[01:54:54.720 --> 01:54:57.520]   Well, you don't really need it if you've got Android Auto, right?
[01:54:57.520 --> 01:54:58.520]   Right.
[01:54:58.520 --> 01:55:01.640]   Or if you've got Siri or Google Assistant in there, you don't really need...
[01:55:01.640 --> 01:55:02.640]   Right.
[01:55:02.640 --> 01:55:03.640]   And I don't have that.
[01:55:03.640 --> 01:55:05.040]   So I'm like very excited to see what happens.
[01:55:05.040 --> 01:55:10.280]   Yeah, it's funny because Tesla as high tech as it is has its own, you know, entertainment
[01:55:10.280 --> 01:55:12.160]   navigation system, which is fine.
[01:55:12.160 --> 01:55:13.160]   I think it's fine.
[01:55:13.160 --> 01:55:14.400]   I'm used to it.
[01:55:14.400 --> 01:55:17.840]   But it would be nice to have a little bit better voice assistant.
[01:55:17.840 --> 01:55:19.680]   It's not a very good voice assistant.
[01:55:19.680 --> 01:55:20.680]   Yeah.
[01:55:20.680 --> 01:55:24.080]   Well, I just constantly have questions that I'm used to just asking either Google or
[01:55:24.080 --> 01:55:28.080]   Amazon in my house and yeah.
[01:55:28.080 --> 01:55:29.080]   Yeah.
[01:55:29.080 --> 01:55:30.080]   Interesting.
[01:55:30.080 --> 01:55:31.080]   Interesting.
[01:55:31.080 --> 01:55:36.600]   You know, my Tesla is going back to the company in about...
[01:55:36.600 --> 01:55:37.600]   Oh.
[01:55:37.600 --> 01:55:38.600]   Yeah.
[01:55:38.600 --> 01:55:39.600]   They'll switch.
[01:55:39.600 --> 01:55:40.600]   What do you mean?
[01:55:40.600 --> 01:55:41.600]   At least it's out on the 22nd.
[01:55:41.600 --> 01:55:42.600]   Nothing.
[01:55:42.600 --> 01:55:43.600]   What?
[01:55:43.600 --> 01:55:46.120]   You saw we had no ads.
[01:55:46.120 --> 01:55:47.120]   I can't afford a car.
[01:55:47.120 --> 01:55:49.280]   I'm bicycling from now on.
[01:55:49.280 --> 01:55:50.280]   Scootering.
[01:55:50.280 --> 01:55:52.880]   Scootering in.
[01:55:52.880 --> 01:55:57.320]   We already have a Chevy Bolt, which is Chevy's all electric and it's quite nice and it was
[01:55:57.320 --> 01:55:59.400]   a lot less expensive than the Tesla.
[01:55:59.400 --> 01:56:04.560]   So I'll be driving Lisa's internal combustion car.
[01:56:04.560 --> 01:56:09.440]   Ice car for the next year and a half till that lease runs out and then at some point
[01:56:09.440 --> 01:56:11.200]   after that I'm going to figure out what I'm going to get.
[01:56:11.200 --> 01:56:13.720]   I was hoping Carson would also become your chauffeur.
[01:56:13.720 --> 01:56:15.120]   The nice cap on.
[01:56:15.120 --> 01:56:16.280]   I have a cap for him.
[01:56:16.280 --> 01:56:17.360]   He refuses to wear it.
[01:56:17.360 --> 01:56:18.360]   I don't understand.
[01:56:18.360 --> 01:56:19.360]   Oh.
[01:56:19.360 --> 01:56:20.360]   It's because he went to Harvard.
[01:56:20.360 --> 01:56:21.360]   Just take your segue.
[01:56:21.360 --> 01:56:24.040]   I'm going to segue in.
[01:56:24.040 --> 01:56:25.440]   I do have a segue.
[01:56:25.440 --> 01:56:27.360]   How many miles from the office are you?
[01:56:27.360 --> 01:56:28.360]   Only two.
[01:56:28.360 --> 01:56:29.360]   Only two.
[01:56:29.360 --> 01:56:30.360]   I can walk.
[01:56:30.360 --> 01:56:31.360]   You can segue in.
[01:56:31.360 --> 01:56:32.360]   You can segue in.
[01:56:32.360 --> 01:56:33.360]   I can walk.
[01:56:33.360 --> 01:56:34.840]   The problem is it's not an attractive walk.
[01:56:34.840 --> 01:56:37.640]   There's a lot of traffic and there's no sidewalk.
[01:56:37.640 --> 01:56:38.640]   It's one of those things.
[01:56:38.640 --> 01:56:39.640]   There's a freeway between us.
[01:56:39.640 --> 01:56:40.640]   It's sad.
[01:56:40.640 --> 01:56:41.640]   Yeah.
[01:56:41.640 --> 01:56:45.640]   It's really sad that they don't like communities don't have require sidewalks for crying out
[01:56:45.640 --> 01:56:48.560]   loud everywhere because I would love to walk.
[01:56:48.560 --> 01:56:54.160]   But I don't want to walk in traffic.
[01:56:54.160 --> 01:56:55.160]   So thank you.
[01:56:55.160 --> 01:56:56.160]   That's a good pick.
[01:56:56.160 --> 01:56:57.160]   I'm glad to hear it's here.
[01:56:57.160 --> 01:56:58.800]   You haven't gotten yours yet though, Carson.
[01:56:58.800 --> 01:57:00.800]   You applied for an invite, but no worries.
[01:57:00.800 --> 01:57:02.280]   I applied for an invite like the first day.
[01:57:02.280 --> 01:57:03.280]   It's been so long.
[01:57:03.280 --> 01:57:04.640]   I don't even remember if I did or not.
[01:57:04.640 --> 01:57:08.120]   They still, a bunch of people are getting them.
[01:57:08.120 --> 01:57:09.440]   How do they let you know, Stacey?
[01:57:09.440 --> 01:57:10.440]   Was it an email?
[01:57:10.440 --> 01:57:12.480]   Yeah, it's an email invite.
[01:57:12.480 --> 01:57:14.040]   But again, it wasn't to me.
[01:57:14.040 --> 01:57:16.560]   It was to one of my listeners and they got it for me.
[01:57:16.560 --> 01:57:18.200]   Oh, it wasn't that nice of them.
[01:57:18.200 --> 01:57:19.960]   That's why I said, "Thanks, John."
[01:57:19.960 --> 01:57:20.960]   I get it now.
[01:57:20.960 --> 01:57:22.960]   It was the last time we had this kind of...
[01:57:22.960 --> 01:57:23.960]   Sorry, Stacey.
[01:57:23.960 --> 01:57:24.960]   Oh, that's okay.
[01:57:24.960 --> 01:57:29.600]   That was the last time we had this kind of scarcity rollout.
[01:57:29.600 --> 01:57:30.600]   Amazon's got it.
[01:57:30.600 --> 01:57:31.600]   Oh, they did it.
[01:57:31.600 --> 01:57:32.600]   They did it with the Echo.
[01:57:32.600 --> 01:57:33.600]   They did it with the Look.
[01:57:33.600 --> 01:57:36.080]   I know, but it's kind of over, folks.
[01:57:36.080 --> 01:57:37.080]   Everything.
[01:57:37.080 --> 01:57:43.560]   I think they're doing it to test how people use it and also to test like audience before
[01:57:43.560 --> 01:57:44.560]   they make a bunch of them.
[01:57:44.560 --> 01:57:45.560]   It's not a stomach.
[01:57:45.560 --> 01:57:46.560]   It's a gimmick.
[01:57:46.560 --> 01:57:48.560]   Like one plus, we don't want one plus earlier, right?
[01:57:48.560 --> 01:57:49.560]   That was their whole stick at the beginning.
[01:57:49.560 --> 01:57:50.720]   It's a marketing gimmick, yeah.
[01:57:50.720 --> 01:57:52.560]   It's a marketing gimmick.
[01:57:52.560 --> 01:57:55.360]   Apple and all the other sort of phones.
[01:57:55.360 --> 01:57:58.080]   Amazon, we get past them.
[01:57:58.080 --> 01:58:02.320]   People want your things, sell them your thing.
[01:58:02.320 --> 01:58:06.720]   Maybe they've got us all in a blacklist where they're not giving us it until later.
[01:58:06.720 --> 01:58:10.080]   Jeff, do you have a numero?
[01:58:10.080 --> 01:58:16.000]   Well, kind of, because my real number this week is 15, which is the age of Coco Gough,
[01:58:16.000 --> 01:58:21.200]   because I was mesmerized watching her in her second round in Wimbledon.
[01:58:21.200 --> 01:58:23.200]   She's a tennis player.
[01:58:23.200 --> 01:58:24.200]   Unbelievable.
[01:58:24.200 --> 01:58:25.200]   Absolutely.
[01:58:25.200 --> 01:58:27.560]   I'm 15 years old.
[01:58:27.560 --> 01:58:35.200]   Beat Venus Williams in the first round after wildcard, and then beat River Corova.
[01:58:35.200 --> 01:58:40.920]   That was kind of neat, because she grew up idolizing Venus.
[01:58:40.920 --> 01:58:41.920]   Exactly.
[01:58:41.920 --> 01:58:42.920]   Exactly.
[01:58:42.920 --> 01:58:43.920]   That was cool.
[01:58:43.920 --> 01:58:47.920]   I mean, really incredibly poised, incredibly graceful.
[01:58:47.920 --> 01:58:53.800]   After beating Venus came over and said, "I'm only here because of you.
[01:58:53.800 --> 01:58:55.440]   You're my hero.
[01:58:55.440 --> 01:58:56.440]   Thank you."
[01:58:56.440 --> 01:59:01.640]   And she'd met Venus before, because they both were Condell Ray, Florida, but said I never
[01:59:01.640 --> 01:59:04.360]   had the courage to say it before.
[01:59:04.360 --> 01:59:07.640]   Just a sweetheart.
[01:59:07.640 --> 01:59:12.360]   Smart in her play, brilliant in...
[01:59:12.360 --> 01:59:17.880]   I'm not a good strategic in tennis, but even I can see how smart her play is.
[01:59:17.880 --> 01:59:22.280]   It's powerful, just amazing to watch.
[01:59:22.280 --> 01:59:27.480]   It's really exciting to watch a new star, who keeps going on the rise and maybe witnessing
[01:59:27.480 --> 01:59:31.000]   the next Venus and Serena or the next Roger...
[01:59:31.000 --> 01:59:34.800]   I'm going to say, "Feedler."
[01:59:34.800 --> 01:59:43.400]   The next conductor or the Boston Phonstrah, the great Roger Feedler.
[01:59:43.400 --> 01:59:48.480]   We're big fed fans in this house.
[01:59:48.480 --> 01:59:49.480]   So anyway, it was just crazy.
[01:59:49.480 --> 01:59:50.480]   That's what I did.
[01:59:50.480 --> 01:59:53.960]   I didn't go looking for my number, but I have one, which I mentioned already, which is...
[01:59:53.960 --> 01:59:57.080]   Google now knows the most crowded subway lines.
[01:59:57.080 --> 02:00:01.060]   I think every subway rider will think that theirs is the most crowded subway line.
[02:00:01.060 --> 02:00:04.160]   What is most crowded?
[02:00:04.160 --> 02:00:10.800]   Venus RAs, the Yorkiza line... doesn't have any numbers of...
[02:00:10.800 --> 02:00:13.280]   Sao Paulo line 11, the Coral.
[02:00:13.280 --> 02:00:21.000]   Venus RAs line A, Sao Paulo gets in another one line eight, Paris line 13.
[02:00:21.000 --> 02:00:26.240]   Venus RAs line C, Tokyo, which I think everybody would have presumed because they have the
[02:00:26.240 --> 02:00:27.240]   shoverers.
[02:00:27.240 --> 02:00:28.240]   Yeah, that's shoverers.
[02:00:28.240 --> 02:00:29.240]   It's a push in the line.
[02:00:29.240 --> 02:00:32.160]   Sao Paulo again.
[02:00:32.160 --> 02:00:34.200]   Line nine, Tokyo again.
[02:00:34.200 --> 02:00:39.400]   And then New York coming in at number 10 with the L line.
[02:00:39.400 --> 02:00:43.400]   And the L line is the one that goes to Google's headquarters in New York that they all get
[02:00:43.400 --> 02:00:44.400]   pissed off about.
[02:00:44.400 --> 02:00:46.400]   That could have led us a tire thing.
[02:00:46.400 --> 02:00:50.080]   Chelsea Market, which is by the way where Jigsaw is.
[02:00:50.080 --> 02:00:51.080]   Exactly.
[02:00:51.080 --> 02:00:52.080]   Yeah.
[02:00:52.080 --> 02:00:54.720]   Come full circle.
[02:00:54.720 --> 02:00:56.960]   My pick of the week is not a pick, but a warning.
[02:00:56.960 --> 02:01:02.240]   Do not blindly follow Google Maps.
[02:01:02.240 --> 02:01:04.880]   Dozens of drivers did on the way to the Denver airport.
[02:01:04.880 --> 02:01:11.200]   They were rerouted through a muddy field where they got stuck.
[02:01:11.200 --> 02:01:18.720]   A crash on Peña Boulevard in Denver choked off access to DIA on Sunday.
[02:01:18.720 --> 02:01:21.640]   Google Maps suggested a detour.
[02:01:21.640 --> 02:01:25.320]   CNN talked to one Connie Moncees.
[02:01:25.320 --> 02:01:28.440]   She said, I thought maybe there's a detour, pulled it up on Google Maps.
[02:01:28.440 --> 02:01:30.320]   It gave me a detour that was half the time.
[02:01:30.320 --> 02:01:33.760]   So I took the exit down to where they told me to.
[02:01:33.760 --> 02:01:42.040]   She probably missed this sign, the shotgun and fallen over sign that said, "Road closed."
[02:01:42.040 --> 02:01:46.440]   She said, I saw a side-stalled episode for the modern age.
[02:01:46.440 --> 02:01:50.440]   No, I was given confidence because there were all these about a hundred other cars in front
[02:01:50.440 --> 02:01:51.440]   of me.
[02:01:51.440 --> 02:01:55.240]   Unfortunately, a couple of the cars in front got stuck in the mud.
[02:01:55.240 --> 02:02:00.000]   And since only one car at a time could pass through, everybody else was stuck behind those.
[02:02:00.000 --> 02:02:02.560]   The question is, why did Google send us out there to begin with?
[02:02:02.560 --> 02:02:08.120]   There was no turning back once you got out there.
[02:02:08.120 --> 02:02:10.800]   She did have a four-wheel drive and was able to get through.
[02:02:10.800 --> 02:02:19.560]   In fact, she picked up a number of people, even the passenger of an Uber that was also
[02:02:19.560 --> 02:02:23.360]   stuck in the mud on the way to DIA.
[02:02:23.360 --> 02:02:25.720]   Yeah, but it wasn't faster.
[02:02:25.720 --> 02:02:28.120]   Well, look at these people.
[02:02:28.120 --> 02:02:30.120]   They're all taking pictures themselves.
[02:02:30.120 --> 02:02:32.240]   The best shot I ever saw.
[02:02:32.240 --> 02:02:33.240]   This is just the best.
[02:02:33.240 --> 02:02:36.120]   Google took us through this field.
[02:02:36.120 --> 02:02:39.760]   So, just a word of warning.
[02:02:39.760 --> 02:02:45.200]   As Google says, use your common sense when following directions from a computer.
[02:02:45.200 --> 02:02:46.200]   Nice.
[02:02:46.200 --> 02:02:50.920]   Ladies and gentlemen, this concludes this episode of this week in Google.
[02:02:50.920 --> 02:02:54.960]   I do thank you for listening and do invite you to subscribe because each week it's fun.
[02:02:54.960 --> 02:03:00.400]   Thanks to Stacey Higginbotham who joins us every week from the Stacey on IoT website, Stacey
[02:03:00.400 --> 02:03:04.960]   on IoT.com, and of course the IoT podcast with Kevin Tofel.
[02:03:04.960 --> 02:03:05.960]   Are you loving Seattle?
[02:03:05.960 --> 02:03:07.680]   Are you loving the weather?
[02:03:07.680 --> 02:03:08.840]   I do.
[02:03:08.840 --> 02:03:14.080]   It's gray and cloudy today, but it's so far been kind of awesome.
[02:03:14.080 --> 02:03:18.040]   Because right now, including even today, it'd be like 100 degrees and, yeah, it's going
[02:03:18.040 --> 02:03:22.320]   to be 98 degrees on Monday in Austin and about 100 percent humidity, right?
[02:03:22.320 --> 02:03:23.320]   Yeah.
[02:03:23.320 --> 02:03:24.560]   So, I'm good being here.
[02:03:24.560 --> 02:03:25.560]   I love it.
[02:03:25.560 --> 02:03:26.560]   Yeah.
[02:03:26.560 --> 02:03:27.560]   Yeah.
[02:03:27.560 --> 02:03:28.560]   Thank you so much, Stacey.
[02:03:28.560 --> 02:03:29.560]   Always a pleasure.
[02:03:29.560 --> 02:03:31.560]   I'm going to be a little bit more interested in this.
[02:03:31.560 --> 02:03:33.560]   I'm going to be a little bit more interested in this.
[02:03:33.560 --> 02:03:35.560]   I'm going to be a little bit more interested in this.
[02:03:35.560 --> 02:03:37.560]   I'm going to be a little bit more interested in this.
[02:03:37.560 --> 02:03:40.560]   I'm going to be a little bit more interested in this.
[02:03:40.560 --> 02:03:43.560]   I'm going to be a little bit more interested in this.
[02:03:43.560 --> 02:03:46.560]   I'm going to be a little bit more interested in this.
[02:03:46.560 --> 02:03:48.560]   I'm going to be a little bit more interested in this.
[02:03:48.560 --> 02:03:50.560]   I'm going to be a little bit more interested in this.
[02:03:50.560 --> 02:03:51.560]   I'm going to be a little bit more interested in this.
[02:03:51.560 --> 02:03:52.560]   I'm going to be a little bit more interested in this.
[02:03:52.560 --> 02:03:53.560]   I'm going to be a little bit more interested in this.
[02:03:53.560 --> 02:03:54.560]   I'm going to be a little bit more interested in this.
[02:03:54.560 --> 02:03:55.560]   I'm going to be a little bit more interested in this.
[02:03:55.560 --> 02:03:57.560]   I'm going to be a little bit more interested in this.
[02:03:57.560 --> 02:03:59.560]   I'm going to be a little bit more interested in this.
[02:03:59.560 --> 02:04:01.560]   I'm going to be a little bit more interested in this.
[02:04:01.560 --> 02:04:03.560]   I'm going to be a little bit more interested in this.
[02:04:03.560 --> 02:04:05.560]   I'm going to be a little bit more interested in this.
[02:04:05.560 --> 02:04:06.560]   I'm going to be a little bit more interested in this.
[02:04:06.560 --> 02:04:07.560]   I'm going to be a little bit more interested in this.
[02:04:07.560 --> 02:04:08.560]   I'm going to be a little bit more interested in this.
[02:04:08.560 --> 02:04:09.560]   I'm going to be a little bit more interested in this.
[02:04:09.560 --> 02:04:10.560]   I'm going to be a little bit more interested in this.
[02:04:10.560 --> 02:04:11.560]   I'm going to be a little bit more interested in this.
[02:04:11.560 --> 02:04:12.560]   I'm going to be a little bit more interested in this.
[02:04:12.560 --> 02:04:13.560]   I'm going to be a little bit more interested in this.
[02:04:13.560 --> 02:04:14.560]   I'm going to be a little bit more interested in this.
[02:04:14.560 --> 02:04:20.560]   I'm going to be a little bit more interested in this.
[02:04:20.560 --> 02:04:22.560]   I'm going to be a little bit more interested in this.
[02:04:22.560 --> 02:04:23.560]   I'm going to be a little bit more interested in this.
[02:04:23.560 --> 02:04:24.560]   I'm going to be a little bit more interested in this.
[02:04:24.560 --> 02:04:25.560]   I'm going to be a little bit more interested in this.
[02:04:25.560 --> 02:04:26.560]   I'm going to be a little bit more interested in this.
[02:04:26.560 --> 02:04:27.560]   I'm going to be a little bit more interested in this.
[02:04:27.560 --> 02:04:28.560]   I'm going to be a little bit more interested in this.
[02:04:28.560 --> 02:04:29.560]   I'm going to be a little bit more interested in this.
[02:04:29.560 --> 02:04:30.560]   I'm going to be a little bit more interested in this.
[02:04:30.560 --> 02:04:31.560]   I'm going to be a little bit more interested in this.
[02:04:31.560 --> 02:04:32.560]   I'm going to be a little bit more interested in this.
[02:04:32.560 --> 02:04:33.560]   I'm going to be a little bit more interested in this.
[02:04:33.560 --> 02:04:34.560]   I'm going to be a little bit more interested in this.
[02:04:34.560 --> 02:04:41.560]   I'm going to be a little bit more interested in this.
[02:04:41.560 --> 02:04:48.560]   I'm going to be a little bit more interested in this.
[02:04:48.560 --> 02:04:55.560]   I'm going to be a little bit more interested in this.
[02:04:55.560 --> 02:05:02.560]   I'm going to be a little bit more interested in this.
[02:05:02.560 --> 02:05:09.560]   I'm going to be a little bit more interested in this.
[02:05:09.560 --> 02:05:16.560]   I'm going to be a little bit more interested in this.
[02:05:16.560 --> 02:05:23.560]   I'm going to be a little bit more interested in this.
[02:05:23.560 --> 02:05:28.560]   I'm going to be a little bit more interested in this.
[02:05:28.560 --> 02:05:31.560]   I'm going to be a little bit more interested in this.

