;FFMETADATA1
title=Find the Cat
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=578
genre=Podcast
comment=https://twit.tv/twig
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.040]   It's time for Twig this week in Google. Mike Elgin joins Jeff Jarvis and Pruitt.
[00:00:05.040 --> 00:00:09.920]   We'll talk about the United States Department of Justice. They're about to go after Google
[00:00:09.920 --> 00:00:15.120]   with an antitrust action. What can we expect? It's the end of the line for the Pixelbook.
[00:00:15.120 --> 00:00:21.440]   And then we rehearse some of the ignoble prize winners for 2020. It's all coming up next
[00:00:21.440 --> 00:00:28.480]   on Twig. This week in Google comes to you from Twig's last past studios, securing every access
[00:00:28.480 --> 00:00:34.080]   point in your company that doesn't have to be a challenge. LastPass unifies access and authentication
[00:00:34.080 --> 00:00:39.520]   to make securing your employees simple and secure even when they're working remotely.
[00:00:39.520 --> 00:00:42.960]   Check out lastpass.com/twit to learn more.
[00:00:42.960 --> 00:00:51.520]   Podcasts you love from people you trust. This is Twig.
[00:00:56.480 --> 00:01:05.600]   This is Twig. This week in Google, Episode 578, recorded Wednesday, September 23, 2020. Find the cat.
[00:01:05.600 --> 00:01:11.840]   This episode of This Week in Google is brought to you by Twilio.
[00:01:11.840 --> 00:01:17.040]   Strengthen your customer relationships by uniting communications across your entire business.
[00:01:17.040 --> 00:01:23.840]   Get everything you need to build and deliver a new customer experience with Twilio. Go to Twilio.com
[00:01:23.840 --> 00:01:31.120]   to learn more. And by Wasabi Hot Cloud Storage. Thinking about moving your data storage to the
[00:01:31.120 --> 00:01:38.000]   cloud? Wasabi is enterprise-class cloud storage at one-fifth the price of Amazon S3 and faster
[00:01:38.000 --> 00:01:44.720]   than the competition with no fees for egress or API requests and no complex storage tiers.
[00:01:44.720 --> 00:01:50.960]   Start a free trial at wasabi.com and to the code Twit. It's time for Twig. This week at Google.
[00:01:50.960 --> 00:01:56.160]   Hello everybody. Good to see you all for another exciting edition of This Week in Google.
[00:01:56.160 --> 00:02:03.120]   Aunt Pruitt is joining us. Host of Hands-on-Will-ness, Hands-on-Photography. Good to see you, Anthony.
[00:02:03.120 --> 00:02:12.240]   Thank you, sir. Yes. Also joining us is Jeff Jarvis, the Leonard Tau Professor for
[00:02:12.240 --> 00:02:16.720]   journalistic innovation at the Craig Newmark Graduate School of Journalism at the City
[00:02:16.720 --> 00:02:22.640]   University of New York, blocker@buzzmachine.com. Got off work a little early today. Thank you.
[00:02:22.640 --> 00:02:27.440]   Appreciate it. Got off the Zoom. This is work too, but this is pleasure. This is work.
[00:02:27.440 --> 00:02:32.800]   Yeah, it was just so funny. It was a publication, but it's pleasure. I was listening to old W&E
[00:02:32.800 --> 00:02:40.880]   W air checks with a chatroom earlier. And it was one of the hosts last shows. And that was
[00:02:40.880 --> 00:02:46.640]   mostly his by monologue was how much work doing radio was. This is a job.
[00:02:46.640 --> 00:02:56.720]   Okay, fine. That's how you feel. Goodbye. You just lost it. Yeah. Stacy is not here today. She has
[00:02:56.720 --> 00:03:00.320]   another engagement. I can't remember what it was, but she told us last week she was going to be
[00:03:00.320 --> 00:03:03.600]   busy. But that's great because it gave us a chance to say hello to Mike Elgin.
[00:03:04.160 --> 00:03:10.800]   Long lost. Mike Elgin. Yes. Mike Elgin. Still lost. Yeah, he's up in Washington state.
[00:03:10.800 --> 00:03:17.520]   So Mike's moving to Washington state. He wants to live among the furs and the pines. Yes, that's
[00:03:17.520 --> 00:03:24.640]   right. That's right. We bought a piece of property 17 acres up here. And it's just a magnificent
[00:03:24.640 --> 00:03:30.800]   forest. I mean, I actually hesitate to post the pictures. People are in lockdown. Yeah, don't
[00:03:30.800 --> 00:03:34.640]   think it's jealous. But if you've seen Bambi or any number of other
[00:03:34.640 --> 00:03:39.440]   audio visual entertainments, you might be familiar with this scenario. But are you
[00:03:39.440 --> 00:03:44.960]   nervous about living amongst the trees now that they're all burning down? Well, it rains here. So
[00:03:44.960 --> 00:03:50.080]   that's the thing. So you may not believe it. But actually drops of water come from the sky,
[00:03:50.080 --> 00:03:55.120]   occasionally. No. And yeah, I've read about it on the internet. This is true.
[00:03:55.840 --> 00:04:02.480]   Wow. It rains a lot. Where were you when your world travels? Where were you when COVID hit bad?
[00:04:02.480 --> 00:04:09.440]   Was in Mexico City and we were actually contemplating because we didn't really have a home base at all.
[00:04:09.440 --> 00:04:12.880]   And we're thinking, well, maybe we should have home base. Maybe our home base should be in Mexico
[00:04:12.880 --> 00:04:20.160]   City. And we were essentially living there. And it all went crazy in California. There was sudden
[00:04:20.160 --> 00:04:24.720]   lockdowns. People were in masks and social distancing. People were freaking out. And there was like two
[00:04:24.720 --> 00:04:29.440]   or three weeks where we're in Mexico City where that hadn't really happened yet. You'd find the
[00:04:29.440 --> 00:04:35.200]   occasional canister of hand sanitizer here and there. But mostly nobody of social distancing. People
[00:04:35.200 --> 00:04:40.720]   are out and about doing their thing. And we flew back to California and it was like major culture
[00:04:40.720 --> 00:04:46.080]   shop. But yeah, we were in Mexico City. And we were thinking, maybe this would be a good place to
[00:04:46.080 --> 00:04:53.520]   wait out the pandemic. And then I noticed that hospitals in Mexico City already have
[00:04:54.480 --> 00:04:59.760]   six hour waits. This is for the pandemic. So I really don't think I want to be here
[00:04:59.760 --> 00:05:05.040]   if I do happen to catch this thing. So that would have been California. That would have been
[00:05:05.040 --> 00:05:08.240]   in March, right? Something like that. February or March, something like that. I don't know.
[00:05:08.240 --> 00:05:12.800]   It was hard to believe it was only 12 years ago that this all began.
[00:05:12.800 --> 00:05:21.680]   Last March. And since then, every day has been Wednesday. So it's really, yeah. Yeah.
[00:05:21.680 --> 00:05:27.680]   What it was. So it was March 17th for us, March 16th for us. And so that's what now six months.
[00:05:27.680 --> 00:05:32.400]   12 for me. Yeah. Six months ago. Wow. Pretty crazy.
[00:05:32.400 --> 00:05:39.440]   So we've been we've been hearing drumbeats and apparently today or in the next day or so
[00:05:39.440 --> 00:05:46.080]   hasn't happened yet. The Justice Department is expected to brief state attorneys general on
[00:05:47.120 --> 00:05:53.440]   the Google antitrust lawsuit where days away from this. Remember, this was the lawsuit
[00:05:53.440 --> 00:05:58.720]   that staff attorneys at the DOJ did not want to file. They said we need more time to work on it,
[00:05:58.720 --> 00:06:03.200]   but that the White House and AG bar pushed them into doing.
[00:06:03.200 --> 00:06:09.920]   Two people familiar with the matter confirmed. This is from the Washington Post
[00:06:09.920 --> 00:06:14.320]   confirmed the Justice Department's early plans, but they cautioned it could change
[00:06:15.520 --> 00:06:20.320]   as with anything in Washington these days. It could change and then a lawsuit may come later
[00:06:20.320 --> 00:06:27.760]   than anticipated. I don't really don't know what that means. It's expected that it would be an
[00:06:27.760 --> 00:06:36.080]   antitrust suit from the DOJ and it would focus on search. Yeah, that's when the word.
[00:06:36.080 --> 00:06:39.600]   I mean, you just can't tell now what's political and what's not. It's really
[00:06:40.160 --> 00:06:47.600]   the DOJ just filed is going to go after Section 230. The DOJ went after Melania's former best friend
[00:06:47.600 --> 00:06:52.560]   for the book she was going to write where she wrote. You can't tell what's what. That's the problem.
[00:06:52.560 --> 00:07:03.120]   And now you live in an anarchist city. So I'm proud. It's just crazy. I don't it's just it's
[00:07:03.120 --> 00:07:10.000]   gotten so the fog of war or is it the fog of Trump is makes it so hard. There's no footing.
[00:07:10.160 --> 00:07:15.040]   We spent a lot of time talking about the tick talk story and it's almost like I don't even want to
[00:07:15.040 --> 00:07:19.760]   tell you what the latest is because by the time it's out of my mouth, it will be changed.
[00:07:19.760 --> 00:07:24.960]   It'll be different and it makes no sense anyway and it's hard to make any sense from all of the
[00:07:24.960 --> 00:07:29.760]   other issues right on with our government. Which are about bigger issues. Yes. Well, that's probably
[00:07:29.760 --> 00:07:35.520]   the point of hard day. Let's just go ahead and go ahead. What's hard about Wednesday?
[00:07:35.520 --> 00:07:42.800]   Well, just just just between Breonna Taylor and Justice Ginsburg's memorial at the Supreme Court.
[00:07:42.800 --> 00:07:50.240]   It's a very somber. Yeah. The Kentucky AG announced what an indictment of one of the officers.
[00:07:50.240 --> 00:07:56.560]   For only nothing to do with Breonna Taylor only for endangering the three neighbors.
[00:07:56.560 --> 00:08:00.480]   The neighbors. The neighbors but not for her death.
[00:08:01.360 --> 00:08:06.960]   Yes. But it was so dangerous for the neighbors. It's very strange. But fortunately,
[00:08:06.960 --> 00:08:12.800]   there is still a federal investigation and process. So there is hope that there will be justice for
[00:08:12.800 --> 00:08:24.480]   Breonna Taylor. Yeah. Okay. It's funny because according to the information, one of the things
[00:08:24.480 --> 00:08:33.600]   Google did, which I think a lot of us thought, oh, this is really good, which is to make it harder
[00:08:33.600 --> 00:08:42.480]   to spy on people with the Google browser actually may stimulate increased antitrust scrutiny.
[00:08:42.480 --> 00:08:49.680]   Google said in January was going to follow Apple's lead in increasing privacy protections
[00:08:49.680 --> 00:08:56.000]   by blocking most online advertising trackers, publishers of news and entertainment websites
[00:08:56.000 --> 00:09:00.960]   whose business relies on the trackers hit back according to the information. And so they went
[00:09:00.960 --> 00:09:08.160]   to the Justice Department and they said, hey, Google's blocking ad trackers. Can't you do something
[00:09:08.160 --> 00:09:16.240]   about this? So this might also be involved oddly enough. But it is. It is a case of regulatory
[00:09:16.240 --> 00:09:23.040]   capture. That is to say, when Google and Facebook welcome regulation, it's because they can afford
[00:09:23.040 --> 00:09:27.920]   it. And others can't. So when Google says, Oh, cookie's bad. Okay, we'll go with a lot with
[00:09:27.920 --> 00:09:32.080]   cookies back. Because we got all the data we need. Right. And we're going to end up, you know,
[00:09:32.080 --> 00:09:35.760]   the regulators will help them hurt every competitor out there. And I'm not saying that's their motive,
[00:09:35.760 --> 00:09:42.720]   but that's the result. It's not fascinating. I think a lot of users think that what Apple's done
[00:09:42.720 --> 00:09:47.920]   now is Safari 14 and what Google's planning to do are great is great. And many of us, I do run
[00:09:47.920 --> 00:09:54.640]   anti tracking, either anti tracking browsers or anti tracking plugins on our browsers to prevent
[00:09:54.640 --> 00:10:00.960]   that kind of tracking. Mr. Paranoid and middle of moral panic. But that's okay, Leo. So you would
[00:10:00.960 --> 00:10:08.400]   never you would never use anti tracking technology. Oh, it's not it's not worth my trouble.
[00:10:09.520 --> 00:10:15.040]   There's no trouble. They got so easy. It's still not worth my trouble.
[00:10:15.040 --> 00:10:21.760]   I don't I don't. You're right. I mean, I have the same argument sometimes where I say, well,
[00:10:21.760 --> 00:10:27.600]   what's tell me the harms because a lot of the harms are imaginary harms or, you know, kind of
[00:10:27.600 --> 00:10:32.640]   hypothetical harms. What are what are you what are you worried about? And a lot of times,
[00:10:32.640 --> 00:10:37.520]   you know, to for equity, always say, well, they could sell that information about you visiting
[00:10:37.520 --> 00:10:44.640]   Dunkin Donuts to your insurer and they won't give you insurance. But that really the logical
[00:10:44.640 --> 00:10:50.000]   response to that is well, actually, the way it works, insurers ask you before they give you
[00:10:50.000 --> 00:10:55.200]   insurance, they ask you all that stuff. And then if you lie to them, they get they welcome that
[00:10:55.200 --> 00:11:02.240]   as a great way to deny you cover a chapter. Deny or appreciate your premiums. Yeah. So at the same
[00:11:02.240 --> 00:11:07.040]   time, you take that insurance scenario, you have someone like me that gets asked about their weight
[00:11:07.040 --> 00:11:12.800]   and considered overweight, right? Because you're so buff track, stop tracking my stuff. Yeah. And
[00:11:12.800 --> 00:11:17.760]   then you could also obviously advertising. I don't I agree with you, Jeff. I don't mind targeted
[00:11:17.760 --> 00:11:23.360]   advertising. I prefer it to non targeted advertising. So that doesn't that's not what I'm worried
[00:11:23.360 --> 00:11:28.880]   about. So I'm like, well, speaking of antitrust, I think the biggest harm, if anything, is the fact
[00:11:28.880 --> 00:11:36.560]   that the major trackers, which by which I mean, Facebook, Google and Amazon, it really kind of
[00:11:36.560 --> 00:11:44.320]   makes the powerful more powerful. They use their, the login system and other systems to really
[00:11:44.320 --> 00:11:49.680]   have an advantage over smaller companies. That's very hard. If you want to go up against Facebook,
[00:11:49.680 --> 00:11:55.600]   for example, by doing contextual advertising, good luck, because if you don't have Instagram and
[00:11:55.600 --> 00:12:00.800]   all those kinds of other kinds of services, the login system that they have, the use of what's
[00:12:00.800 --> 00:12:07.360]   happened, etc, you'll never get anywhere close to Facebook in terms of being able to offer up
[00:12:07.360 --> 00:12:12.960]   targeted advertising, especially I don't know how they do it or why, but the targeted advertising on
[00:12:12.960 --> 00:12:18.960]   Instagram is amazing. I mean, it's really good. It's so you're almost, you think they're reading
[00:12:18.960 --> 00:12:24.240]   your mind or something. And that's a that's an advantage that helps them financially. It helps
[00:12:24.240 --> 00:12:34.320]   their market share, etc. And we'll see how damaging that is to free, you know, free enterprise and
[00:12:34.320 --> 00:12:39.120]   competition later on. But again, I don't think anybody's really looking at it from that point of
[00:12:39.120 --> 00:12:47.840]   view. It's more of a sort of a paranoia system. I mean, I was shocked today, you know, opening up
[00:12:47.840 --> 00:12:53.280]   all the articles from the rundown. And one of them was CNN article. If you ever want to be
[00:12:53.280 --> 00:13:00.480]   entertained, just launch any sort of CNN.com page. Oh, they have all the stuff. Oh, unbelievable.
[00:13:00.480 --> 00:13:06.000]   It's it's a couple hundred things load when you when you go to CNN page. It's really amazing.
[00:13:06.000 --> 00:13:10.560]   But here's where I think we are. Go ahead. Well, actually, I'll let you talk because I don't,
[00:13:10.560 --> 00:13:14.880]   but because you're you're doing a good job defending the trackers.
[00:13:14.880 --> 00:13:20.240]   Here's I mean, yeah, I just think it's it's the it's the mood we're in now. So,
[00:13:20.240 --> 00:13:23.920]   stat news, which is brilliant. The best the best sign out there for medical news right now,
[00:13:23.920 --> 00:13:31.520]   had a good story today about a woman who uses AI at MIT around health. But the headline really
[00:13:31.520 --> 00:13:37.360]   struck me when AI is the opposite of sinister. And MIT researcher is held up as a model of how
[00:13:37.360 --> 00:13:41.120]   algorithms can benefit humanity. Right. And the story is very good. And I'm not making fun of
[00:13:41.120 --> 00:13:45.360]   stat there. I'm saying that's that reflects the attitude for more you moral panic people now,
[00:13:46.480 --> 00:13:51.200]   where every bit of technology, whether it's cookies or AI or any of that is presumed now
[00:13:51.200 --> 00:13:55.360]   to be a bad thing. Yeah. And it's a problem. It's a good thing. Right.
[00:13:55.360 --> 00:14:01.280]   It's it's it's it's an overreaction. I have selective moral panic, Mr. Jarvis.
[00:14:01.280 --> 00:14:09.520]   Because I do believe in AI, I have some outstanding advantages in our world. It's just not totally
[00:14:09.520 --> 00:14:15.600]   refined as we've discussed many times on this show. But yeah, I'm a little bit more selective about
[00:14:16.400 --> 00:14:21.600]   my panic here, because I don't necessarily want the folks that come casting in exactly what I'm
[00:14:21.600 --> 00:14:26.080]   doing and what I'm not doing and then taking that information and selling it to the highest bidder.
[00:14:26.080 --> 00:14:30.960]   And I still have to fuss with them about my internet disconnecting every day at five o'clock p.m.
[00:14:30.960 --> 00:14:38.560]   Pacific time. There's definitely this is a boom time for anti tracking technologies. Not just
[00:14:38.560 --> 00:14:46.320]   Safari and Chrome Firefox does it. Duck Duck go is just released a privacy tracker. And I have to say,
[00:14:46.320 --> 00:14:51.920]   when I get these trackers, I run them against Twitter TV because I know what our website is running
[00:14:51.920 --> 00:14:58.960]   in the background. And right. According to Duck Duck go, we get a D which with site privacy protection,
[00:14:58.960 --> 00:15:05.040]   they've upgraded to a B plus. They block six trackers. Most of them are
[00:15:06.000 --> 00:15:12.400]   Google Analytics. Yeah, all of them are analytics. New Relic isn't exactly an analytic. We use
[00:15:12.400 --> 00:15:20.560]   new relic for caching. So all of them are analytics. I don't know if I mean, look, sites need to use
[00:15:20.560 --> 00:15:26.400]   analytics to keep to keep an eye on how many people are using it. They also said unknown privacy
[00:15:26.400 --> 00:15:31.760]   practices, even though we have a link to our privacy policy right here at the bottom of the page,
[00:15:32.720 --> 00:15:38.960]   where it's supposed to be, according to everything, everybody. But this is, you know, I think I'm
[00:15:38.960 --> 00:15:44.080]   running it. And in fact, when I go, there's another new site called the markup. They have
[00:15:44.080 --> 00:15:48.720]   something called their black light, which is a real time website privacy inspector. This is not
[00:15:48.720 --> 00:15:55.120]   a plugin you use as you browse around you enter the URL. I just had a curiosity entered CNN's. And
[00:15:55.120 --> 00:16:01.920]   it has 34 ad trackers. They say that's more than the average of seven. We found on popular sites,
[00:16:01.920 --> 00:16:07.920]   50 50 third party cookies, which is more than the average of three averages three. Yeah, this is
[00:16:07.920 --> 00:16:15.040]   the one I really bothers me. And I use Firefox, Facebook. I don't I don't have a Facebook account.
[00:16:15.040 --> 00:16:20.640]   But no matter that you don't have an account, Facebook is we believe creates shadow dossiers of
[00:16:20.640 --> 00:16:26.160]   people, even who aren't Facebook users. And in this case, even as a non Facebook user, if I go to
[00:16:26.160 --> 00:16:33.680]   CNN.com, it sends a signal to to there's a Facebook pixel. Yeah, I'm not talking about the like button,
[00:16:33.680 --> 00:16:41.600]   but a Facebook pixel. And the reason they bothers me, I don't want Facebook, I don't have Facebook.
[00:16:41.600 --> 00:16:47.520]   And I don't want Facebook to keep track of where I go. Because I don't I'm not even a member. I
[00:16:47.520 --> 00:16:54.480]   don't want Facebook. So I actually block Facebook in my Firefox browser. So Leo, have you even seen
[00:16:54.480 --> 00:16:58.400]   the new Jill Lepore book if then? Yeah, I haven't read it.
[00:16:58.400 --> 00:17:06.800]   How symbol Maddox Corporation invented the future. And it was it was in the I think was the 60s.
[00:17:06.800 --> 00:17:11.680]   That data can predict things data will predict things. We're going to win elections. And we're
[00:17:11.680 --> 00:17:13.920]   going to we're going to pretend elections. We're going to win elections. We're going to change the
[00:17:13.920 --> 00:17:17.920]   world. Right. And it was the you know, it's being called the Cambridge Analytica of the time.
[00:17:17.920 --> 00:17:23.600]   And just like Cambridge Analytica is a bunch of who we they don't really have all this power
[00:17:23.600 --> 00:17:27.680]   that we think they have. Right. Right. And so I think a lot of this is the presumption that
[00:17:27.680 --> 00:17:32.640]   what could happen or my God, they're smarter than we know. And the truth is, there's few
[00:17:32.640 --> 00:17:36.400]   conspiracies in this world because the world's just screwed up. But that is by the way,
[00:17:36.400 --> 00:17:43.840]   one of the non hypothetical threats, this tracking poses, is the threat in politics.
[00:17:43.840 --> 00:17:47.680]   Some people think that Cambridge Analytica had a large hand in Brexit.
[00:17:47.680 --> 00:17:54.320]   No, everybody were that effective. Every every every research scientist I know who knows their
[00:17:54.320 --> 00:18:00.720]   stuff just completely laughs at Cambridge Analytica said it was a bunch of who he had no real influence.
[00:18:00.720 --> 00:18:06.240]   It promised a power it didn't have. That's not the same couldn't happen. Well, that's my question is,
[00:18:06.240 --> 00:18:11.440]   okay, so they were they were selling snake oil. But that doesn't mean that somebody else couldn't
[00:18:11.440 --> 00:18:14.640]   actually go along and always have this. But what I'm saying is you go back to civil
[00:18:14.640 --> 00:18:19.040]   Maddox, we thought the same thing for all those years. Yeah. Yeah.
[00:18:19.040 --> 00:18:24.800]   So this gets to I think the core of what as pundits and thought leaders, so I think we
[00:18:24.800 --> 00:18:29.760]   ought to be focusing on which is not the tracking itself, but the abuse of tracking. So for example,
[00:18:29.760 --> 00:18:36.000]   we've heard the stories about how police are increasingly going and buying data or getting data
[00:18:36.000 --> 00:18:40.240]   from phone companies and so on. If there's a crime in an area, they're like, right, see who is here,
[00:18:40.240 --> 00:18:45.360]   right? And what they were doing that if that's an abuse, we decide as a society that we don't
[00:18:45.360 --> 00:18:50.320]   like that sort of thing that then we should crack down on that, not the tracking initially.
[00:18:50.320 --> 00:18:55.040]   And the other thing is transparency. So what is Facebook doing with that data that they're
[00:18:55.040 --> 00:18:59.440]   collecting on CNN? What are people doing? And in the case of Cambridge Analytica,
[00:18:59.440 --> 00:19:07.120]   one of the transgressions of their system was it was trickery. They said, hey, fill out this
[00:19:07.120 --> 00:19:10.880]   personality. Find out who you are. And they were actually gathering data from political
[00:19:10.880 --> 00:19:15.440]   organizations. So they were compared to the Obama administration who did at that time on
[00:19:15.440 --> 00:19:23.120]   precedent and a precedentally aggressive data collection and so on. But the Obama campaign said,
[00:19:23.120 --> 00:19:27.040]   hey, we're the Obama campaign. We'd like to get you to participate in this thing. They were
[00:19:27.040 --> 00:19:33.200]   transparent about it. The story was. We're sneaky and playing a fast one on people.
[00:19:33.200 --> 00:19:37.200]   That was the bigger problem, not the fact that they were just collecting all this data.
[00:19:37.200 --> 00:19:41.680]   The problem is transparency and abuse of the tracking that we had to focus on.
[00:19:41.680 --> 00:19:44.480]   Yeah, it doesn't really let them off the hook that they weren't any good at it.
[00:19:44.480 --> 00:19:50.240]   Yeah, right. They tried and failed, but doesn't mean somebody else won't.
[00:19:50.240 --> 00:19:54.320]   And the fact that Semomatics couldn't do it in the 60s, how much does it mean it can't be done in
[00:19:54.320 --> 00:19:59.280]   the 2020s? Because we have a lot better tools and a hell of a lot more data.
[00:19:59.920 --> 00:20:06.480]   But here's the question, Leo, is it's fine as Ant inspired this this this red fear,
[00:20:06.480 --> 00:20:10.000]   right? Because you say I'm selectively moral panic, which is fine, which is the right way to be.
[00:20:10.000 --> 00:20:15.520]   But then what you need is decent data about how moral panic you should be.
[00:20:15.520 --> 00:20:20.560]   I just reread this really good book by Axel Brun's called Our Filter Bubbles Real.
[00:20:20.560 --> 00:20:23.360]   Yes. And I'm going to give you a spoiler. The answer is no.
[00:20:24.640 --> 00:20:31.600]   Tons of research in this. I'm sorry. You're doing this. Yeah. And so we have this this fear brought on
[00:20:31.600 --> 00:20:36.800]   by Cass Sunstein and Eli Perruser about echo chambers and filter bubbles. And then the research
[00:20:36.800 --> 00:20:42.640]   doesn't bear it out. And it's a fear about us because the fear here was people were so stupid.
[00:20:42.640 --> 00:20:46.240]   All they want to hear is the same things and they reject all their friends and their family
[00:20:46.240 --> 00:20:50.640]   if they don't rule them politically and they get what do you think? I understand.
[00:20:50.640 --> 00:20:58.160]   Is it? Eli Perruser's thesis, which is you tend, you know, because of Google's algorithms,
[00:20:58.160 --> 00:21:02.480]   you will get more and more of the stuff that you look for that alone is not true. And that will
[00:21:02.480 --> 00:21:06.640]   put away. Let me just explain what we're talking about. I'm setting the groundwork.
[00:21:06.640 --> 00:21:13.440]   And then I'm going to ask you a question. So that's Eli's, Eli's premise is, and then you get
[00:21:13.440 --> 00:21:19.120]   inside a filter bubble and then you just get reinforced in your existing beliefs. You're saying
[00:21:19.120 --> 00:21:25.280]   Axel Brun came up with research that said, in fact, that's not in fact, I think we all understand
[00:21:25.280 --> 00:21:30.800]   that the internet due to serendipity plus poor tracking plus poor recommendation engines plus
[00:21:30.800 --> 00:21:36.640]   the failure of companies like Google and Simulmatics does in fact give you a broad variety of inputs
[00:21:36.640 --> 00:21:43.760]   from other sources. But don't you think just on the face of it, Jeff, that people tend to look for
[00:21:43.760 --> 00:21:50.560]   and absorb information and agree, confirmation bias. They look for information that agrees
[00:21:50.560 --> 00:21:56.240]   with what they already think. Even if they're seeing articles that disagree, they don't even see them.
[00:21:56.240 --> 00:22:02.560]   And even if the filter bubble is not real, I do feel like people are always looking for
[00:22:02.560 --> 00:22:07.920]   confirmation of what they believe, not for the thing that disproves what they believe. Is that
[00:22:07.920 --> 00:22:12.720]   the truth, Jeff? But the argument here is they don't even see those things.
[00:22:12.720 --> 00:22:15.840]   Yes, I understand. Yes. And they do see those things. Yes. They do.
[00:22:15.840 --> 00:22:20.800]   The other argument was that they act on this in certain ways that they reject friendships,
[00:22:20.800 --> 00:22:25.120]   they reject family. Some of us have really bad family members, so we have, but in general,
[00:22:25.120 --> 00:22:31.520]   no, we don't do that. I don't know. I think people on Facebook will unfriend people with
[00:22:31.520 --> 00:22:36.800]   different beliefs, especially nowadays, we're so polarized with different beliefs from them and
[00:22:36.800 --> 00:22:39.920]   really only see that people agree with them. But what he agrees with them? The problem is,
[00:22:40.480 --> 00:22:45.040]   this is what he says. In fact, I think I have the quote mark because I was putting it in the book.
[00:22:45.040 --> 00:22:51.360]   The empirical research drawing on surveys and small-spail observations and on advanced and
[00:22:51.360 --> 00:22:54.800]   innovative digital methods of big social data demonstrates clearly that echo chambers and
[00:22:54.800 --> 00:23:00.080]   filter bubbles not only do not exist outside the very fringes of mainstream society, what they do
[00:23:00.080 --> 00:23:05.200]   exist, but that the incessant and continuing focus of media coverage, media scholarship,
[00:23:05.200 --> 00:23:11.840]   and political debate and and moral panics on these ill-defined ideas over emphasizes the role of
[00:23:11.840 --> 00:23:16.880]   platforms and their algorithms and current political crises. Here's the point and obscures a far more
[00:23:16.880 --> 00:23:21.120]   critical challenge, the return of naked hyper-partisan populism and political demagoguery.
[00:23:21.120 --> 00:23:24.720]   This is where the moral panic stuff comes in. We're saying, oh, it's all Facebook's fault. No.
[00:23:24.720 --> 00:23:28.240]   Yeah, yeah. That's true. Facebook is reflecting something going on our society.
[00:23:28.240 --> 00:23:33.280]   Yes. We've got to get to that. I grant you that. So it isn't the problem that Google's only showing
[00:23:33.280 --> 00:23:36.400]   you stuff you agree with or Facebook's only showing you the stuff you agree with,
[00:23:36.400 --> 00:23:42.880]   but it is a case that we are in a hyper-partisan society and it's the confirmation bias I was
[00:23:42.880 --> 00:23:47.280]   talking about. Even though you might be seeing a broad variety of inputs, you're not listening.
[00:23:47.280 --> 00:23:52.320]   Well, also, there's also data. If you believe in QAnon, everything confirms QAnon.
[00:23:52.320 --> 00:23:57.040]   Well, okay. That's where I go back to. There's also data in here. This is the people who got
[00:23:57.040 --> 00:24:00.240]   their information offline are worse shaped than those who get their information online.
[00:24:00.240 --> 00:24:05.520]   Maybe they watch Fox News. I believe that. But this is where we're going to challenge that though.
[00:24:05.520 --> 00:24:11.360]   Go ahead. I'm sorry. I'm sorry to interrupt, but I would challenge that. If you if you lump
[00:24:11.360 --> 00:24:17.520]   the Atlantic along with Fox News into not online, then you get one result. But if you separate,
[00:24:17.520 --> 00:24:26.640]   like the other the problem problem is cable TV news. Yeah, but I get all my news from liberal
[00:24:26.640 --> 00:24:32.880]   sources, the New Yorker, the Atlantic, the Washington Post. So I don't know how different that is from
[00:24:32.880 --> 00:24:40.400]   my relatives who only watch Fox News. Am I not also? But are you aware of those arguments out there?
[00:24:40.400 --> 00:24:45.840]   That's the other part of this is that because we see fighting that makes us aware of other
[00:24:45.840 --> 00:24:52.160]   sides existing. Yeah, and actually I tune in Fox News. But as much as I can, as much as I can take
[00:24:52.160 --> 00:24:56.480]   it, but they're in an alternate universe. What really is, we were talking about this before the
[00:24:56.480 --> 00:25:04.160]   show, you were talking about Dana Boyd's talk at South by three years ago, where she talks about
[00:25:04.160 --> 00:25:10.880]   there's an epistemological crisis. That is, there is there are there's what you know and what I know.
[00:25:10.880 --> 00:25:16.080]   And they never the tweens shall meet. If I don't like you, I'm not going to like your facts.
[00:25:16.080 --> 00:25:20.240]   Right. And so when we think when we think people believe this QAnon stuff,
[00:25:20.240 --> 00:25:26.720]   no, it's performative. I'm I'm they don't believe it because I think it could be true or because it
[00:25:26.720 --> 00:25:32.800]   might as well be true. And you know, when we have a viewer, we say, well, but no, the experts say
[00:25:32.800 --> 00:25:38.000]   that in this epistemological war says Dana, what some people say in their head is the experts are
[00:25:38.000 --> 00:25:42.320]   elite and they're tied in with the other elites. And thus, I don't like the elites. So I don't like
[00:25:42.320 --> 00:25:48.080]   the experts. So I don't like what they say. So yeah, child rings in pizza parlors, because it might
[00:25:48.080 --> 00:25:53.040]   as well be true in their view. There's also though, and I don't think that I don't think that this
[00:25:53.040 --> 00:25:59.200]   there's also perhaps, and maybe this is the larger issue, which is all of the things that we are
[00:25:59.200 --> 00:26:07.840]   churning about are really superficial issues that are designed to attract our
[00:26:07.840 --> 00:26:15.760]   our magpie like attention. We're under the surface. There's really general agreement between the
[00:26:15.760 --> 00:26:19.440]   left and the right that we should let the people with the money run the place.
[00:26:19.440 --> 00:26:26.960]   And that the laws of the land and the way things work should be always designed to
[00:26:26.960 --> 00:26:32.560]   encourage those people to make more money. I mean, if you're really a leftist, maybe you that's,
[00:26:32.560 --> 00:26:39.440]   you know, if you're Noam Chomsky, all of this sound and fury is not important,
[00:26:39.440 --> 00:26:44.640]   because it obscures the real facts of the matter, which is a deeply rooted inequality.
[00:26:44.640 --> 00:26:53.360]   Bingo, bingo, bingo. And also the sense that, you know, this this this resentment of elites
[00:26:53.360 --> 00:26:58.960]   business comes across on multiple levels. One of the interesting things about QAnon is that those
[00:26:58.960 --> 00:27:03.600]   of us who are interested in facts, journalists, who are into fact checking and all that stuff,
[00:27:03.600 --> 00:27:11.120]   actually getting the facts exactly right, are kind of more like the dismiss QAnon
[00:27:11.120 --> 00:27:17.440]   in its totality. This is going to sound ridiculous simply because it's completely wrong. However,
[00:27:17.440 --> 00:27:24.240]   the idea that this pizza parlor thing in the pedophile ring, the idea, the core idea behind that
[00:27:24.240 --> 00:27:29.440]   is that elites are getting away with pedophilia because they're elites, because they're connected,
[00:27:29.440 --> 00:27:34.560]   because they're there because of that. That's exactly what the Jeffrey Epstein story revealed.
[00:27:34.560 --> 00:27:38.400]   That in fact is happening. There's a pedophile ring that they're involved with,
[00:27:38.400 --> 00:27:41.760]   and they get away with it because they're elite, because they have money, because they have power
[00:27:41.760 --> 00:27:48.880]   and connections and all that stuff. So the the outrage behind the false story of the pizza parlor
[00:27:48.880 --> 00:27:54.880]   is actually justified because that's the best I've heard is playing.
[00:27:54.880 --> 00:27:58.640]   You know what? That's the best I've heard is playing like that. Unfortunately,
[00:27:58.640 --> 00:28:01.680]   people know there's something going on. Just justification I've heard the QAnon.
[00:28:01.680 --> 00:28:04.560]   Yeah. No, people know there's something going on, but they ascribe it to the wrong
[00:28:04.560 --> 00:28:12.400]   source. You know, I read a book. I'm going to I'm going to try to keep up with Mr. Jarvis,
[00:28:12.400 --> 00:28:14.880]   who reads way too many books. I read a book once.
[00:28:14.880 --> 00:28:21.920]   He does. And not all my media can be challenged. I listen to him. I listen to him.
[00:28:23.200 --> 00:28:28.000]   I actually listen to this one too. Russell's shorto book that you you probably read is a great book
[00:28:28.000 --> 00:28:34.320]   about the history of Manhattan. Oh, oh loved it. And one of the things I learned from this book
[00:28:34.320 --> 00:28:42.320]   is there was deep concern among the elites of the time in Manhattan that the poor people in
[00:28:42.320 --> 00:28:48.720]   Manhattan would make league with the black people, the former slaves and others in Manhattan
[00:28:49.440 --> 00:28:57.760]   and against the elites. And so their goal was always to divide the two. And the same thing in the south,
[00:28:57.760 --> 00:29:04.720]   in the south, the rich plantation owners knew that I mean they were way outnumbered by the
[00:29:04.720 --> 00:29:12.720]   slaves. But if the slaves were to make, you know, alliances with the political force with the poor
[00:29:12.720 --> 00:29:21.360]   whites, they'd be in deep trouble. So and I think you even see this going on right now, where the
[00:29:21.360 --> 00:29:32.320]   main thrust of the rhetoric is to keep the suppressed elites, hey, the repressed poor at each other's
[00:29:32.320 --> 00:29:36.880]   throats, because otherwise we're in trouble. There's only a handful of elites. It's only a handful.
[00:29:36.880 --> 00:29:42.400]   That's why it's called a 1%. And if yeah, if black people and poor white people realize they had
[00:29:42.400 --> 00:29:47.840]   common cause against the rich, the rich and be in deep trouble. So the best thing they can do is say,
[00:29:47.840 --> 00:29:52.240]   hey, white people, watch out for those black people. So I wrote a piece for an Irish newspaper.
[00:29:52.240 --> 00:29:56.320]   So sorry, you go first, you know, I've been talking. No, I was just agreeing because that's
[00:29:56.320 --> 00:30:00.560]   that's pretty much what I've been. Is that your experience? Yeah, it's been what that's what I've
[00:30:00.560 --> 00:30:05.920]   been taught in my family. That is when I was growing up not necessarily taught that in school.
[00:30:05.920 --> 00:30:09.840]   But that was pretty similar to the discussion that my family had with me.
[00:30:10.400 --> 00:30:16.480]   I have. And they got to keep them away from the Hispanics. Yeah, as much as you can polarize people.
[00:30:16.480 --> 00:30:22.320]   So so we focus on the polarizing thing saying, I don't know what really what the point of it is,
[00:30:22.320 --> 00:30:26.720]   is if you want to maintain control, you keep these people polarized, because otherwise they
[00:30:26.720 --> 00:30:31.280]   could align if we be in deep trouble. There's something else happening now, which is and I wrote
[00:30:31.280 --> 00:30:34.480]   about this for an Irish newspaper. I don't know if I've talked about the show, but but my
[00:30:34.480 --> 00:30:40.080]   world you now is that it's the last stand of the old white man. And I say that as an old white man.
[00:30:41.040 --> 00:30:47.360]   Where old white men know that they're going to lose the power. And rather than sharing the
[00:30:47.360 --> 00:30:51.440]   institutions, they're not destroying them because they know they're going to lose them. Oh, that makes
[00:30:51.440 --> 00:30:56.320]   sense. And so that's what we see happening right now is it's a story. That really makes sense.
[00:30:56.320 --> 00:31:05.440]   Everything presidency. Yeah. But I talked to a journalist from La Nacion in Argentina yesterday,
[00:31:05.440 --> 00:31:08.880]   and he was doing a story about Trump institutions. And I said, it's not just Trump though,
[00:31:08.880 --> 00:31:14.160]   that the disruption of the internet has challenged every institution in other ways, right? Look at
[00:31:14.160 --> 00:31:18.160]   banking. BuzzFeed had a huge story this week about all the corruption of banking that wasn't
[00:31:18.160 --> 00:31:24.000]   tracked. Really fascinating. Yeah. Right. Fascinating. Look at policing. Yeah. And now we're finally
[00:31:24.000 --> 00:31:28.160]   as a society saying this institution doesn't make a lot of sense the way it's set up. And it's
[00:31:28.160 --> 00:31:34.560]   dangerous to people. Look at journalism being questioned from all sides. Look at media. Look at
[00:31:36.320 --> 00:31:40.320]   the way democracy is being challenged by a certain part of the old white men.
[00:31:40.320 --> 00:31:46.000]   And so these institutions are all are under challenge anyway. And the old white men are going in for
[00:31:46.000 --> 00:31:51.040]   the kill because they know they're not going to have them. And if you last chance, you know,
[00:31:51.040 --> 00:31:56.000]   it's rather if I can't have my gold toilet seat. No one can. Exactly.
[00:31:56.000 --> 00:32:03.280]   I wonder why, you know, white people are afraid of being a minority. It's not like minorities are
[00:32:03.280 --> 00:32:10.560]   treated any worse, right? Well, it's so funny. What were you told in your family? Can you say
[00:32:10.560 --> 00:32:16.000]   more about that? Yeah, I think it's interesting. The talk that some of my family members had was,
[00:32:16.000 --> 00:32:22.320]   I believe I've mentioned to you all before, I was brought up to love. We need to love everyone.
[00:32:22.320 --> 00:32:28.400]   And there's in the in the black community, there is a lot of fighting amongst each other
[00:32:28.400 --> 00:32:34.000]   that shouldn't be there. And my family would talk to us about why are we fighting one another?
[00:32:34.000 --> 00:32:37.840]   And they would break down the reason we would fight, for example, let's say,
[00:32:37.840 --> 00:32:43.040]   sneakers or something like that. You remember back in the 90s and 80s when people were getting
[00:32:43.040 --> 00:32:47.120]   killed over sneakers? Yeah. Yeah. You know, they brought it to our attention that you know what,
[00:32:47.120 --> 00:32:51.920]   you know, who's making money off of those sneakers? Yeah, the white guys. It's not any of us that
[00:32:51.920 --> 00:32:56.960]   are fighting for them. Yep. Right. So it was a lot of discussions like that. It's just, you know,
[00:32:56.960 --> 00:33:04.560]   we have all of these different limbs thrown in between us to divide us and fight against each
[00:33:04.560 --> 00:33:10.160]   other. Exactly. And it started from us fighting against each other as black people, but just
[00:33:10.160 --> 00:33:14.080]   fighting against our neighbor, whether it was a black or white or what have you, we were all in
[00:33:14.080 --> 00:33:18.960]   the same neighborhood for a reason. It's probably because we wouldn't reach. That's right.
[00:33:20.320 --> 00:33:28.960]   Well, I just wanted to make, to sort of bring this whole conversation back to social networks and so
[00:33:28.960 --> 00:33:35.520]   on, and actually do something that I haven't heard before, which is to advocate for filter bubbles.
[00:33:35.520 --> 00:33:43.280]   Basically, what's happened over time is that there's a lot of discouragement. A lot of this,
[00:33:43.280 --> 00:33:48.560]   people's belief about the world has changed a lot in the last few years because of social media.
[00:33:49.360 --> 00:33:56.720]   Absolutely. You hear people describing Twitter as a hotbed of racism and friction and misogyny
[00:33:56.720 --> 00:34:04.880]   and all that kind of stuff. And I always tell people, you got to be aggressive with that block
[00:34:04.880 --> 00:34:10.640]   feature. But when you do that, when you actually, you want to hear other points of view, but you
[00:34:10.640 --> 00:34:16.000]   want, you don't want to hear propaganda, you don't want to hear bots coming at you with Russian propaganda,
[00:34:16.000 --> 00:34:21.360]   you don't, none of that stuff. What you want is to have a carefully tailored overton window.
[00:34:21.360 --> 00:34:27.840]   I don't want to discuss pizza parlors and pedophile rings. I've been there, my mind is closed on that.
[00:34:27.840 --> 00:34:32.160]   That doesn't exist. It didn't, that thing, that particular, and I don't want any propaganda,
[00:34:32.160 --> 00:34:36.640]   I don't want this, I don't want that. And I've been active in doing this on Twitter, especially,
[00:34:36.640 --> 00:34:45.600]   of I don't follow anybody who's going to be calling the Libs Marxist or Black Lives Matter, a Marxist
[00:34:45.600 --> 00:34:49.200]   organization. I understand why they say it, I understand where, I understand all that,
[00:34:49.200 --> 00:34:53.840]   I don't want to talk about it anymore, because I need to maintain, I need to get my work done,
[00:34:53.840 --> 00:34:59.040]   I need to maintain a positive worldview. And I think that in the end, at the end of the day,
[00:34:59.040 --> 00:35:03.840]   when you do blocking right, and you know what you want to talk about, who you want to talk with,
[00:35:03.840 --> 00:35:08.000]   you end up with a filter bubble, but that's not what a filter bubble is. A filter bubble is
[00:35:08.000 --> 00:35:14.000]   caused by algorithms and so on. So it's like a filter bubble, but it's something you construct.
[00:35:14.000 --> 00:35:18.640]   And I advocate that people construct it, because one of the most damaging things about social media
[00:35:18.640 --> 00:35:23.280]   and how it all works, is we think the world's gone crazy. And the only thing that's really
[00:35:23.280 --> 00:35:28.560]   happened is we have a lot more information from a lot more people. We're seeing everybody instead of,
[00:35:28.560 --> 00:35:34.160]   yeah, we're out of the filter bubble, in fact. It used to be just a little bit interesting. Yeah,
[00:35:34.160 --> 00:35:40.720]   that's really interesting. Would you say the same thing, Mike, if I am a QAnon follower, and I say,
[00:35:40.720 --> 00:35:45.520]   I don't want to follow anybody that tries to debunk QAnon? That would...
[00:35:45.520 --> 00:35:51.360]   I wouldn't recommend following QAnon or... No, but I'm just saying, from their point of view,
[00:35:51.360 --> 00:35:56.480]   it's the same thing. Look, I've already decided that QAnon's true. I don't want to hear these
[00:35:56.480 --> 00:36:03.200]   doubters. Well, I think that's one of the reasons that does happen. Yeah, and I think it doesn't
[00:36:03.200 --> 00:36:08.480]   matter, really, because you can't really convince conspiracy theorists to get off their conspiracy
[00:36:08.480 --> 00:36:13.520]   for the most part. Okay, so that's the... That's the... Really, not of the matter at this point.
[00:36:13.520 --> 00:36:20.160]   We've got, let's say two sides, there's really probably many more, but let's say we've got two sides.
[00:36:20.160 --> 00:36:26.400]   Have we decided at this point it's impossible to convince the other side of what we think,
[00:36:26.400 --> 00:36:32.480]   and just we've given up and it's over. And there's the left part of the aisle and the right part of
[00:36:32.480 --> 00:36:40.560]   the aisle, never... No more crossing over... No, no, no, no. I think that's the main point of the
[00:36:40.560 --> 00:36:46.000]   propaganda is that there are two sides. The right is not QAnon. The right isn't even Trump and
[00:36:46.000 --> 00:36:52.000]   Mago World. No, nobody agrees on everything. George Will's vote for Joseph Biden. That is...
[00:36:52.000 --> 00:36:58.000]   It's like hell has frozen over. Hundreds of Republicans. Yeah. He left the party, in fact.
[00:36:58.000 --> 00:37:03.360]   He's no longer a Republican because of Trump. But the point is that George Will was the right
[00:37:03.360 --> 00:37:11.120]   of Attila the Hun. Now he's my buddy. That's right. But he was always a processed guy and the rule
[00:37:11.120 --> 00:37:16.320]   of law and all this kind of stuff and a conservative. And it's refreshing to find people who are
[00:37:16.320 --> 00:37:22.400]   intelligent and honest and free of the propaganda of their side on the other side, no matter which
[00:37:22.400 --> 00:37:27.760]   side you're on, on your political... So for example, there are lots and lots of very
[00:37:27.760 --> 00:37:33.120]   intelligent, thoughtful Republicans who are very conservative and their voices are just
[00:37:33.120 --> 00:37:38.320]   completely drowned out by the Fox News Breitbart types in the public sphere. But when you can seek
[00:37:38.320 --> 00:37:42.000]   them out and bring them into the fold and interact with them on Twitter, it's a beautiful thing.
[00:37:42.000 --> 00:37:47.040]   And maybe 62 members of the United States Senate. Well, they're just they're just
[00:37:47.040 --> 00:37:52.560]   cowering and they're afraid of mean tweets. Yeah. Well, they're not afraid of mean tweets. They're
[00:37:52.560 --> 00:38:00.400]   afraid of a magic Trump base that will go to the polls November 3rd. And they are literally
[00:38:00.400 --> 00:38:07.120]   terrified of them. Yeah. And maybe they and maybe they should be. You can you can you can point to
[00:38:07.120 --> 00:38:13.840]   people like Jeff Flake and say, you know, even Mitt Romney is cowering at this point in his book.
[00:38:13.840 --> 00:38:16.960]   Yes. Well, no, Mitt Romney's running for president is one of Mitt Romney. Oh, okay.
[00:38:16.960 --> 00:38:21.360]   Romney's going to have it both ways. Oh, I've worked for impeachment. Look, we're a good guy. I am. Oh,
[00:38:21.360 --> 00:38:25.280]   but I put your your justice on the court. Look, what a good guy. No, he's he's pure
[00:38:25.280 --> 00:38:29.200]   pun. This is me off about him and Bloomberg as well. If they really wanted to stop Trump,
[00:38:29.200 --> 00:38:33.280]   they should have run. They should have gotten on the ballot and divided the Republican vote.
[00:38:33.280 --> 00:38:36.560]   Bloomberg wouldn't have taken the Republican vote. He would have taken
[00:38:36.560 --> 00:38:44.320]   no Romney. Romney. You would have taken the George Wilboat. No, I'm very glad he's not there. Very
[00:38:44.320 --> 00:38:56.080]   glad. Okay. Anyway, is it the case that it's foolish to pursue persuasion with people of differing
[00:38:56.080 --> 00:39:02.640]   opinions? No, it's not at all. Not at all. It feels like a waste of time. I can't talk to my relatives
[00:39:02.640 --> 00:39:10.000]   nowadays. It's a lot tougher. You still do that, still. You do that still? Yeah, I do. But then I also
[00:39:11.520 --> 00:39:16.560]   filter it and say, you know what, these people or groups or whatever, they have the right to
[00:39:16.560 --> 00:39:21.920]   their opinions, they have the right to do choice. And I try not to take a lot of things personally,
[00:39:21.920 --> 00:39:28.960]   because part of conversations are disagreements. So I just sort of roll with it. And if I can
[00:39:28.960 --> 00:39:34.720]   persuade someone, great. If not, you're completely wrong, man. No, just kidding. I'm just kidding.
[00:39:34.720 --> 00:39:44.480]   Come on. But I think the secret is to identify whether the person is arguing in good faith.
[00:39:44.480 --> 00:39:50.560]   And if they are genuinely coming from a place that is not a place of propaganda or whatever.
[00:39:50.560 --> 00:39:57.200]   And then you have to cherry pick the points. You can't change someone's entire worldview,
[00:39:57.200 --> 00:40:00.560]   but I had a very constructive argument with somebody on Twitter recently,
[00:40:00.560 --> 00:40:04.320]   where we are getting into it on a whole bunch of things. And he happened to say that,
[00:40:04.560 --> 00:40:13.680]   Biden has done nothing to denounce the looting and so on associated with some of the protests
[00:40:13.680 --> 00:40:19.120]   of Black Lives Matter. And I said, yes, he has and no, yes, he has, no, he has. And I was able to
[00:40:19.120 --> 00:40:24.880]   just go back and just find news reports in June, July, and so on, where he spoke very forcefully
[00:40:24.880 --> 00:40:28.720]   against the violence in favor of the protest, but against the looting and the violence.
[00:40:30.240 --> 00:40:35.520]   And the guy goes, huh, I didn't know that. And so, but it was one little piece,
[00:40:35.520 --> 00:40:39.760]   just to let people know that their source of information is not necessarily reliable.
[00:40:39.760 --> 00:40:46.480]   Okay. But here's the other, what has happened in our society, I say this is a media guy,
[00:40:46.480 --> 00:40:51.280]   we're all media people, since what we're doing right now is media, is that things were controlled
[00:40:51.280 --> 00:40:56.640]   in such a way that society lost the ability to hold a conversation and to debate and argue.
[00:40:57.520 --> 00:41:00.880]   And now we suddenly can hear voices we couldn't hear before and now people can disagree with each
[00:41:00.880 --> 00:41:04.880]   other in ways we can hear it. We don't know what to do with it. And we think that it's all blowing
[00:41:04.880 --> 00:41:10.400]   up. This is what society is, what democracy is. This is the Hurley-Berley of the intellectual
[00:41:10.400 --> 00:41:17.040]   marketplace. It's the cacophony. And we're doing it badly. We need better tools, we need better
[00:41:17.040 --> 00:41:20.960]   structures, we need better training, we need more education. We need better training, training.
[00:41:20.960 --> 00:41:25.360]   Yes. We need better parents like, like ants, you know, as many numbers are told,
[00:41:25.360 --> 00:41:31.680]   wise things like that. And so we're going to have to relearn these skills. The same thing happened,
[00:41:31.680 --> 00:41:35.040]   my friends, you're going to hear from me, watch out, get your drinks ready, drink, drive, drink time.
[00:41:35.040 --> 00:41:43.200]   Gutenberg is that the intellectual 30 years war for God's sakes, right? When new technologies
[00:41:43.200 --> 00:41:46.560]   come that allow people to speak, who couldn't speak before, the people who couldn't speak before
[00:41:46.560 --> 00:41:51.920]   that resent them, try to stop them and don't know what to do with them and don't know what to do
[00:41:51.920 --> 00:41:56.960]   with the noise that's created. Oh my God, a reformation. But then also good things can happen. I honestly
[00:41:56.960 --> 00:42:02.800]   believe that I'm coming to believe more and more that Black Lives Matter is the reformation of
[00:42:02.800 --> 00:42:08.320]   America. It is like the Luther reformation. It is potentially, now we can screw this up. I don't
[00:42:08.320 --> 00:42:16.000]   want to act like a hashtag. He's going to suddenly do it 400 years of struggle and slavery and death
[00:42:16.000 --> 00:42:21.200]   so far haven't accomplished. But I think that there's a moment here where it's, let's Luther's
[00:42:21.200 --> 00:42:31.200]   17 or 5, nobody was at 195 theses in 1517 on the Gutenberg door that there's a moment here when
[00:42:31.200 --> 00:42:34.640]   people can use this technology to say, but you don't like all those other voices. But let me get
[00:42:34.640 --> 00:42:41.040]   my voice out here. Let me see what I've been seeing going on. Finally, I can say it. And we don't lose
[00:42:41.040 --> 00:42:44.880]   sight of that. That's the thing I keep screaming about is that when journalists say, oh, Twitter,
[00:42:44.880 --> 00:42:49.840]   it's a cesspool. When Leo Le Porte says, oh, leave Facebook with a cesspool, my fear is you're
[00:42:49.840 --> 00:42:54.080]   going to leave behind those voices who were not heard in mass media who finally can be heard
[00:42:54.080 --> 00:42:56.160]   and then finally have something to tell us that we need to hear.
[00:42:56.160 --> 00:43:04.800]   I think they need a blog. Yeah. Or a, perhaps a sub-stack newsletter. Yeah.
[00:43:04.800 --> 00:43:06.960]   I can't wait.
[00:43:06.960 --> 00:43:09.040]   Plenty of ways to get your word out these days.
[00:43:09.040 --> 00:43:13.680]   Well, we go where the people are though. You got to go where the people are.
[00:43:14.320 --> 00:43:21.760]   To say you're a destination on Twitter. Twitter is a, I mean, the ultimate filter bubble.
[00:43:21.760 --> 00:43:25.840]   It's 300 million people who are mostly journalists.
[00:43:25.840 --> 00:43:33.760]   I don't think, I don't think real people use Twitter. Maybe they do. Maybe that's changed.
[00:43:33.760 --> 00:43:36.800]   Do they do? Do they do? I don't think Black Twitter is great.
[00:43:36.800 --> 00:43:43.040]   That's not Black Twitter. My Twitter following that I follow myself,
[00:43:43.040 --> 00:43:47.760]   they are real people. It's not just technologists. It's not just photographers,
[00:43:47.760 --> 00:43:52.880]   but I follow a ton of people that I love and care about dearly and they're regular people.
[00:43:52.880 --> 00:43:54.640]   Okay. Yeah. They exist.
[00:43:54.640 --> 00:43:58.240]   It takes work to find them. It takes work to meet them and it pays off that work.
[00:43:58.240 --> 00:44:04.960]   I mean, I have to agree with Mike's principle, which is, and I always said I did this,
[00:44:04.960 --> 00:44:09.360]   but I don't think I did it very well, which is follow freely and block even more freely.
[00:44:09.360 --> 00:44:13.360]   Yeah, that's the thing. Yeah. I'm blocking somebody every day.
[00:44:13.360 --> 00:44:21.440]   Not everybody's worth hearing. I did not block for a long time. Recently, I started blocking
[00:44:21.440 --> 00:44:27.760]   people again, mostly because I just didn't want the vitriol and the outrage. I was sick of the
[00:44:27.760 --> 00:44:30.960]   outrage. So then you're going to find that it's actually a pleasant place to you.
[00:44:30.960 --> 00:44:39.200]   Yeah. Sorry. Exactly. I think the point of view that we have to get to is to understand that
[00:44:39.200 --> 00:44:49.680]   we are not going to interact with 99.9999% of humanity ever. We're always going to only interact
[00:44:49.680 --> 00:44:55.680]   with a tiny, tiny minority. We have the opportunity with social media to decide who that minority is.
[00:44:55.680 --> 00:45:01.840]   I like that. And so by crafting a filter bubble, I think I have thousands of people blocked on
[00:45:01.840 --> 00:45:09.680]   Twitter at this point. And it's a beautiful thing. I'm taking a deliberate action to decide
[00:45:09.680 --> 00:45:17.120]   which ridiculously tiny minority of humanity I'm going to engage with. And I want them to be
[00:45:17.120 --> 00:45:24.560]   representative. I really do. But that's my choice. And I want a little bit of that. The only thing
[00:45:24.560 --> 00:45:30.880]   that I will not tolerate and don't have time for is people who are dishonest or who are acting bad
[00:45:30.880 --> 00:45:36.960]   faith or have some agenda that I disagree with or whatever. But beyond that, it's a beautiful
[00:45:36.960 --> 00:45:40.880]   opportunity. Just forget about the idea that you have to be on a social network that everybody is
[00:45:40.880 --> 00:45:47.760]   on or that you will even ever interact with the vast majority of the people on any social network.
[00:45:47.760 --> 00:45:54.480]   We're talking about a tiny, tiny sliver of humanity. And that's all we'll ever have the time to
[00:45:54.480 --> 00:46:00.000]   engage with. Because you and I love to travel. And that's, I would venture it certainly for me.
[00:46:00.000 --> 00:46:04.720]   And I'm pretty sure it's for you. The main purpose of travel is to broaden the
[00:46:04.720 --> 00:46:09.680]   the variety of people you get to know and meet and interact with their shallow interactions.
[00:46:09.680 --> 00:46:14.800]   But at least their interactions. Well, if you travel, Google Plus was going to save it all.
[00:46:14.800 --> 00:46:18.320]   Yeah. Well, that's the day it's not going to Google Plus was the future.
[00:46:18.320 --> 00:46:22.880]   I would still be on Google Plus if we were around. I miss it. I miss it so much. That was such a
[00:46:22.880 --> 00:46:26.880]   wonderful social network, at least for me. But this thing about travel, I think that
[00:46:26.880 --> 00:46:33.440]   with the way we're headed, where people can work remotely, I think they can travel in a way where
[00:46:33.440 --> 00:46:38.560]   they really have deep experiences. I mean, this has been my experience. And I feel like the travel
[00:46:38.560 --> 00:46:43.280]   that I've done, the depth of the relationships I've had with the range of people that I've interacted
[00:46:43.280 --> 00:46:48.720]   with has made me an entirely different person than I was 10 years ago. I mean, I've lived in
[00:46:48.720 --> 00:46:55.600]   all kinds of really mind chain. I've lived in Kenya. I've lived in places that just completely
[00:46:55.600 --> 00:47:00.000]   changed my whole world. Because you stay there. You're not a tourist. You stay there. You live there.
[00:47:00.000 --> 00:47:07.200]   You become a nomadic. Yeah. Exactly. But the result of your perception of humanity is that you look
[00:47:07.200 --> 00:47:12.720]   at everybody completely differently. Like the way I look at people now, the way compared to how
[00:47:12.720 --> 00:47:18.160]   I used to, is just night and day. It's just a completely different world. So hopefully,
[00:47:18.160 --> 00:47:23.440]   there'll be a lot more of that. People will be able to go to another country and live there,
[00:47:23.440 --> 00:47:28.640]   have neighbors, shop in the same store, and get to know me. Being me and my mask, I'm going to be
[00:47:28.640 --> 00:47:38.160]   traveling. If I get a good, really good mask. Yeah. Pain. I'm really good at that.
[00:47:38.160 --> 00:47:43.120]   And let's take a break. It's so fun to get together with smart people and learn. And I feel like I
[00:47:43.120 --> 00:47:49.760]   can do that every week. Yep. No. Ant. You're one of them. No. Ant. And prove it.
[00:47:49.760 --> 00:47:56.480]   Twitter.tv/hop for Hands-On Photography. H-O-W for Hands-On Wellness. Mike Elgin.
[00:47:56.480 --> 00:48:00.320]   Great to see you from Elgin.com. Great to see you too. Mike's list. And of course,
[00:48:00.320 --> 00:48:09.920]   getting back on the road soon, we hope. Knock on wood. Gastronomad.net. What is the prognosis at this point?
[00:48:11.280 --> 00:48:17.360]   We're going to fire it up again next year. We've got Morocco. We've got two
[00:48:17.360 --> 00:48:20.480]   provinces in the middle of summer. You feel pretty confident that you're going to be able to do it?
[00:48:20.480 --> 00:48:28.080]   I do. Yes. I do. And I don't think we're waiting for a vaccine. I think we're waiting for a whole
[00:48:28.080 --> 00:48:34.160]   bunch of other things. But in any event, our experiences are very socially distant. Anyway,
[00:48:34.160 --> 00:48:39.760]   for example, in Provence, we live in this farmhouse that's outside of this village. I mean, the nearest
[00:48:39.760 --> 00:48:45.760]   neighbors are half mile away. If we had good, reliable testing, you could just say, hey,
[00:48:45.760 --> 00:48:50.000]   you want to go on the trip? You've got to test before you go. We are going to test people before
[00:48:50.000 --> 00:48:56.240]   they go. So everybody will know that every other person is negative. And then when we interact with
[00:48:56.240 --> 00:49:01.280]   winemakers and stuff, it's just us. We're never interacting with the outdoors. The alcohol.
[00:49:01.280 --> 00:49:06.800]   Yeah, the alcohol. Right. Alcohol does kill the virus. So I think we're safe. No, I'm kidding.
[00:49:08.320 --> 00:49:12.720]   Yeah, but we're going to do a lot of things. You are a pickle. You're not just covered here.
[00:49:12.720 --> 00:49:21.360]   You're in a jar with a bunch of Brian being pickled. Don't judge me. I'm not judging. I'm in
[00:49:21.360 --> 00:49:28.080]   the jar with you, buddy. Yeah, I'm glad. I'm so glad because I think this is a really great way
[00:49:28.080 --> 00:49:32.800]   to travel. And you know, I'm thinking maybe that's what my retirement will be is just
[00:49:34.000 --> 00:49:39.280]   getting nomadic, especially if Starlink happens and we can get online wherever we are. You have to
[00:49:39.280 --> 00:49:45.120]   bring a pizza box with addition, you know. Yeah, it's a size of a medium pizza. You were showing
[00:49:45.120 --> 00:49:52.240]   like a large medium, more of a medium. A medium. Yeah. But I think that the ideal amount of time
[00:49:52.240 --> 00:49:58.000]   to stay in these place for me personally is three months, three months. You're staying in place
[00:49:58.000 --> 00:50:01.920]   three months, moved to another place for three months. So I could do four places a year or
[00:50:01.920 --> 00:50:05.920]   three places a year with three months at home. That's a good that sounds like a plan.
[00:50:05.920 --> 00:50:13.440]   Isn't it? Let's do it. I might just hang with you guys. Jeff, you want to come with us? Come on, Jeff.
[00:50:13.440 --> 00:50:19.200]   So this week I've spoken tomorrow. I speak in Hamburg. I spoke in
[00:50:19.200 --> 00:50:28.080]   Chennai. I spoke in Sao Paulo and I spoke in Munich. This is the worst. All those frequent flyer miles
[00:50:28.080 --> 00:50:34.320]   I've lost. This is Zoom traffic. Zoom hell, man. I'm on constant zoom. But it's got me in their
[00:50:34.320 --> 00:50:40.480]   hands, not zoom hell. I zoomed into a class in Chennai, journalism class where they were just
[00:50:40.480 --> 00:50:45.200]   and fantastic students. They were just great. And I wasn't going to go to Chennai. So it's fine.
[00:50:45.200 --> 00:50:50.080]   It's good. But you're keeping this kind of quiet, but I think you're getting an award tomorrow in
[00:50:50.080 --> 00:50:57.200]   Hamburg. Well, I'm sorry. It's yes. The scooper. Look at that scooper. No, it's a kid. Not a
[00:50:57.200 --> 00:51:02.480]   poker scooper people. This is going to scoop camp. That's exciting. So that's tomorrow, right?
[00:51:02.480 --> 00:51:10.000]   Yeah, I'm doing a talk. I have to be up. I'd be on zoom at 645 AM. But that's nice.
[00:51:10.000 --> 00:51:15.680]   People people can participate virtually. Yeah, I think I'm going free. Nice. If you go to the
[00:51:15.680 --> 00:51:19.040]   site, it's a German. Don't worry about it. Just get the digital thing and not do it. Look at that.
[00:51:19.040 --> 00:51:22.080]   Look at that. Look at the serious. Jeff looks in that picture. Looks like you're
[00:51:22.080 --> 00:51:25.440]   castigating somebody with your stick. Yeah, somebody who has more. It's Germany. They have
[00:51:25.440 --> 00:51:32.160]   moral panic. Shut up and like some zit-recht. Like Schutzrecht. Yeah, yeah.
[00:51:32.160 --> 00:51:38.960]   Hoddenbodenkavit haben. By the way, a number of Germans sent me notes saying,
[00:51:38.960 --> 00:51:46.720]   you know, that's not a real German word at all. We never say that. That's something the British say
[00:51:46.720 --> 00:51:53.840]   to pretend they're speaking German. So you're an American pretending to be a Brit
[00:51:53.840 --> 00:51:57.920]   pretending to be German. It's perfect. That's pretty much what it was.
[00:51:57.920 --> 00:52:02.880]   Right for me. That's all I can say. Our show today brought to you by Twilio.
[00:52:02.880 --> 00:52:08.480]   Love the Twilio. I'm a Twilio subscriber for many, many years. I got a Twilio phone number.
[00:52:08.480 --> 00:52:16.000]   I love to play with the Twilio API to try different stuff. Twilio is where you go to build things.
[00:52:16.000 --> 00:52:22.800]   Build things that communicate. The world is changing fast. You want to build new ways to help customers
[00:52:22.800 --> 00:52:29.200]   in a kind of socially distanced appropriate way. And that's what Twilio has always done.
[00:52:29.200 --> 00:52:35.440]   Engage customers on any channel anywhere. I mean, when I say channel, I mean text messaging,
[00:52:35.440 --> 00:52:43.920]   emails, phone calls, newsletters, video, one platform does it all. And man, do they do it at
[00:52:43.920 --> 00:52:54.400]   scale? Twilio powers 795 billion plus interactions across channels with five nines of uptime. 99.99%
[00:52:54.400 --> 00:53:02.640]   uptime, over 3 billion phone numbers available across more than 100 countries. That's at scale.
[00:53:02.640 --> 00:53:08.800]   And I love Twilio, but I'm not alone. Many, many companies use Twilio. You know, when you do that
[00:53:08.800 --> 00:53:14.800]   two factor authentication and logging into your Netflix account, that's Twilio. Shopify uses
[00:53:14.800 --> 00:53:20.720]   Twilio to run their entire global contact center. More than a million customers. That's Twilio Flex.
[00:53:20.720 --> 00:53:26.480]   Blue Apron uses Twilio. You get a blue Apron newsletter that's being sent with Twilio Send Grid.
[00:53:26.480 --> 00:53:30.560]   And Lyft and Uber, you ever get a text message from Lyft and Uber?
[00:53:30.560 --> 00:53:36.640]   Yep, it's Twilio. You guessed it. Product and operations teams use Twilio to create new ways
[00:53:36.640 --> 00:53:42.240]   to communicate with their users inside applications, ranging from account notifications with text
[00:53:42.240 --> 00:53:48.960]   messaging to chatbots to securing online accounts with two factor authentication. You use Authy.
[00:53:48.960 --> 00:53:56.000]   I use Authy. It's the two factor authentication program we recommend. That's Twilio at work. In
[00:53:56.000 --> 00:54:00.320]   fact, I love Authy. You can always tell if somebody's a Twilio customer because Authy works better.
[00:54:00.320 --> 00:54:06.560]   They go right to the top of the list. Twilio is really kind of amazing. Customer service
[00:54:06.560 --> 00:54:12.720]   teams use Twilio Flex to build global contact and support centers. Flex is a programable
[00:54:12.720 --> 00:54:20.960]   contact center application that you control. Look, you don't need to be a specialized telecom
[00:54:20.960 --> 00:54:27.200]   network engineer to use Twilio. Leo Laport uses it. That should tell you something. It lets developers
[00:54:27.200 --> 00:54:32.800]   control the global communications network with a few lines of code. And if your developers are
[00:54:32.800 --> 00:54:37.200]   already building web applications, they already have the skills they need to build apps that text,
[00:54:37.200 --> 00:54:42.960]   call, chat, and video conference. There's just no easier platform for building.
[00:54:42.960 --> 00:54:48.800]   Strengthen your customer relationships by uniting communications across your entire business.
[00:54:48.800 --> 00:54:53.120]   It's easy. Just go to Twilio.com. Set up an account. Play with it. It's fun.
[00:54:53.120 --> 00:55:02.560]   T-W-I-L-I-O dot com. It's time to build with Twilio. Thank you, Twilio, for supporting our
[00:55:02.560 --> 00:55:09.760]   show. We really appreciate it. Thank you for supporting us by checking out Twilio.
[00:55:09.760 --> 00:55:22.240]   Okay. Kevin Tofel is ringing down the crepe. We mentioned this last week as a supposition,
[00:55:22.240 --> 00:55:30.720]   but the Puxelbook is now no longer in stock at the Google Store. It really does look like the
[00:55:30.720 --> 00:55:35.840]   end of the line for the Puxelbook. You can't even buy it if you want it to Google.
[00:55:35.840 --> 00:55:45.520]   Google told Engadget. It's pointing buyers to the Puxelbook Go, which is kind of their silly
[00:55:45.520 --> 00:55:50.240]   schoolbook version of it. They say it's a similar high performance, but it isn't.
[00:55:51.520 --> 00:55:57.520]   Third-party inventory, not gone. I see people like Kevin and others have pointed out that the
[00:55:57.520 --> 00:56:04.720]   Acer Spin, for instance, which is $500 or $600 is as good as a Puxelbook. There's really good
[00:56:04.720 --> 00:56:10.000]   choices out there. Here's the question. My battery is about zonked. The keyboard has problems.
[00:56:10.000 --> 00:56:13.440]   I've been paying about this thing for three years. I'm going to have to do it. I was going to get
[00:56:13.440 --> 00:56:21.360]   the Samsung. The battery sucks on that. Kevin, the Go was Kevin's Chromebook of the year
[00:56:21.360 --> 00:56:29.200]   for 2019. The Go is not bad. I don't know if I've ever seen the Spin 7th-3-8, which one?
[00:56:29.200 --> 00:56:32.000]   I would go with the Spin. I don't really want a fan.
[00:56:32.000 --> 00:56:40.960]   I don't remember if it has a fan, but you can't find out next Monday on Hands on Tech where Mr.
[00:56:40.960 --> 00:56:46.560]   Jason Howell is going to be reviewing the Acer Spin. Look at you. Good plug-in.
[00:56:46.560 --> 00:56:52.400]   Well done plug-in service. It does have a fan because it has an Intel chip in it, but it's kind
[00:56:52.400 --> 00:57:00.800]   of concomitant with these hot Intel i5s that you're going to have a fan. It also has 8 gigs of memory.
[00:57:00.800 --> 00:57:10.880]   It has NVMe storage, which is really fast. 128 gigs, not that storage is all important,
[00:57:10.880 --> 00:57:17.040]   but for things like power washing, that'll be a lot faster. Booting up should be faster.
[00:57:17.040 --> 00:57:22.800]   It's got a beautiful screen that's very high-res, 2256x1504.
[00:57:22.800 --> 00:57:28.240]   Boring. It doesn't have little nubbies on its butt. Is that what you want?
[00:57:28.240 --> 00:57:35.200]   I'm just unhappy. I live in the Google for so long and they've abandoned me. Some people got
[00:57:35.200 --> 00:57:42.640]   them set up. Honestly, I don't feel like the go is going to blast that much longer either.
[00:57:42.640 --> 00:57:51.360]   No, there's no more, not pink. Yeah. That's a color, by the way.
[00:57:51.360 --> 00:57:58.960]   People think that's not as true as my face. Saying something weird. It's one of the colors
[00:57:58.960 --> 00:58:03.600]   that the Chromebook comes in. Well, we have a Google event coming up on the 30th. Who knows?
[00:58:03.600 --> 00:58:08.960]   A week from today. In fact, right before the show, we're going to start Windows Weekly
[00:58:08.960 --> 00:58:15.360]   an hour earlier, I think. Then we're going to go to the Google event and come right out of the
[00:58:15.360 --> 00:58:19.520]   Google event. We know it's a Pixel 5 event. Actually, I don't think there's any surprises
[00:58:19.520 --> 00:58:23.920]   to come to think of it. I think Google's already said everything they're going to announce, right?
[00:58:23.920 --> 00:58:28.400]   Or could they have one more thing at the end? Oh, and for those of you who missed the Pixelbook,
[00:58:29.440 --> 00:58:33.120]   for Jeff Jarvis, we've decided to sell one more Pixelbook.
[00:58:33.120 --> 00:58:40.000]   Yeah, no, I don't think so. Why is that? There's no question that Chromebooks sell well,
[00:58:40.000 --> 00:58:46.560]   right? Kevin's argument is that they proved the point. Chromebooks at first were all cheap,
[00:58:46.560 --> 00:58:52.000]   tinny, plastic, and they wanted to say wanted. They wanted to hire and they proved it.
[00:58:52.000 --> 00:58:57.040]   And they proved it at a very high price. But as you say, the 713, which can you give me a
[00:58:57.040 --> 00:58:59.520]   spoiler here in? $629.
[00:58:59.520 --> 00:59:08.720]   Right. And so it's a good machine. But it's a premium compared to what was.
[00:59:08.720 --> 00:59:14.000]   Now, are you going to buy a Pixel 5? Because that's going to come out next week.
[00:59:14.000 --> 00:59:20.160]   My wife will kill me. No. I'm not going anywhere. So yes, I use my phone. Of course,
[00:59:20.160 --> 00:59:25.280]   I use my phone. But I'm home. Right. This is, by the way, the difference between your wife and my
[00:59:25.280 --> 00:59:29.520]   wife. I was saying last night, I said, I don't want to buy a new iPhone. I don't want to buy a new
[00:59:29.520 --> 00:59:36.240]   Pixel. I'm so done. I just want to keep using this one. She said, I will kill you if you don't buy
[00:59:36.240 --> 00:59:41.040]   the new phone. So that's a really quite a big difference. What was that show?
[00:59:41.040 --> 00:59:45.280]   We're not traded spouses. Yeah. Would you like to be married to Lisa?
[00:59:45.280 --> 00:59:49.600]   Just for one day. Just one day. I get that. I get that. I get the phone.
[00:59:49.600 --> 00:59:53.360]   I come back. This is your job. Of course you're buying the new stuff.
[00:59:53.760 --> 01:00:02.640]   I have to say, I did in a fit of economy trade in the Microsoft Duo. I've never sent anything back.
[01:00:02.640 --> 01:00:09.920]   I sent it back. I know. But don't get your hopes up because I applied the $1,500
[01:00:09.920 --> 01:00:18.080]   traded of my own Note 10 to get the Galaxy Z Fold 2. Because I want to see-
[01:00:18.080 --> 01:00:21.760]   It's looking like to be a better device. Well, that's what I want to see.
[01:00:21.760 --> 01:00:25.440]   And you know what? I will without hesitation return it if it's not.
[01:00:25.440 --> 01:00:31.120]   Right. Because I think I'm going to get the Pixel 5. I think I have to.
[01:00:31.120 --> 01:00:35.440]   Sounds like you're not into gimmicks. You just want something that's going to work and get the
[01:00:35.440 --> 01:00:41.040]   jobs done. I don't want to buy a new phone every year or let alone five new phones every year.
[01:00:41.040 --> 01:00:48.720]   I'm so over that. I don't want to buy a phone for a long time, especially at the $1,000 price.
[01:00:48.720 --> 01:00:53.120]   I just want to stay with it. But it is- And I forget, I ask you this all the time,
[01:00:53.120 --> 01:00:56.560]   but I keep on forgetting which one. You have the four. The four XL.
[01:00:56.560 --> 01:01:00.960]   Four XL, right. That's what I got to. And I got the orange back. I like it.
[01:01:00.960 --> 01:01:04.800]   It's a beautiful phone. The Mises has the four A. Good choice.
[01:01:04.800 --> 01:01:09.840]   And the old Annette 4A was a little weird to me because it's smaller, but there's nothing wrong
[01:01:09.840 --> 01:01:16.400]   with that phone. And she wanted to complain about the camera one day. And I looked at it and I
[01:01:16.400 --> 01:01:21.360]   said, maybe if you just clean the lens off or something, because the camera is totally fine.
[01:01:21.360 --> 01:01:23.520]   Oh, it's probably just smuts on the lens.
[01:01:23.520 --> 01:01:28.800]   Right. And it's been fine ever since. That's a quality device for that price point. And
[01:01:28.800 --> 01:01:34.640]   just seeing that right there, it really just tells me, hold on to this four as long as I can,
[01:01:34.640 --> 01:01:38.480]   because they're charging too much for these phones. And if we can-
[01:01:38.480 --> 01:01:40.160]   I got the three, actually. I love it.
[01:01:40.160 --> 01:01:42.640]   These phones, they're going to keep charging those prices.
[01:01:43.200 --> 01:01:50.480]   Well, now there is some rumor going around that maybe the Pixel 5 will not be the $1,000 phone.
[01:01:50.480 --> 01:01:55.920]   There have been price leaks from Europe and so forth. So I don't know. There's a lot of leaks.
[01:01:55.920 --> 01:02:00.720]   As always, what happens next Wednesday probably won't be much of a surprise.
[01:02:00.720 --> 01:02:09.200]   From Wind Future and 9 to 5 Google, 6-inch, 90 Hertz display, 432 pixels per inch. That's a lot.
[01:02:09.200 --> 01:02:14.320]   That's plenty. Gorilla Glass 6. That's one step up from the Pixel 4.
[01:02:14.320 --> 01:02:19.040]   Still breakable. Yeah. Still breakable. And there's no glasses, not, I'm afraid.
[01:02:19.040 --> 01:02:30.240]   The chin is still there, but it sounds like it's pretty narrow, the bottom bezel.
[01:02:33.920 --> 01:02:39.440]   They say this in the leak, again, Google hasn't said, is 100% recycled aluminum,
[01:02:39.440 --> 01:02:46.880]   IP68 water and dust resistant. Now, the thing that it perked my ears up is big, big, big, big battery,
[01:02:46.880 --> 01:02:52.640]   4,080 milliamp hour battery, wireless charging, reverse charging as well.
[01:02:52.640 --> 01:02:55.600]   That would mean really nice battery life.
[01:02:58.480 --> 01:03:06.960]   A wide angle 16 megapixel F2.2 one micron pixels, which is as Ant will tell you is a good thing.
[01:03:06.960 --> 01:03:11.440]   The bigger the pixel size, the bigger, bigger, the better the light, low light performance.
[01:03:11.440 --> 01:03:17.200]   Same 12.2 megapixel main camera, but it now, because of the extra processor,
[01:03:17.200 --> 01:03:23.200]   will support 40K 60 frames and 240 frames. That's nice.
[01:03:24.560 --> 01:03:32.480]   You just got the new GoPro Hero Black 9. Have you been playing with its 4K and 5K video?
[01:03:32.480 --> 01:03:39.840]   I played with some of the 5K yesterday and so far, the only weird thing about it is it allows you
[01:03:39.840 --> 01:03:47.520]   to shoot in a flat profile. They don't necessarily call it log like a flat profile. And it's not
[01:03:47.520 --> 01:03:53.760]   really, it's really not flat. It's still pretty vibrant. So I don't really expect great thing.
[01:03:53.760 --> 01:03:58.000]   Great video from a GoPro. Just, I don't know now.
[01:03:58.000 --> 01:04:02.720]   Really? GoPro is really good because of their stabilization.
[01:04:02.720 --> 01:04:08.720]   It's better than what you would think considering the size of that camera and the size of their
[01:04:08.720 --> 01:04:11.440]   sensors. You're going to rewind that next week on the hands-on tech?
[01:04:11.440 --> 01:04:16.560]   It won't be next week, but it'll be soon. You got to take some time to really put it through its
[01:04:16.560 --> 01:04:24.480]   bister. But, you know, that's the thing is, do you need a GoPro if you're getting 60 FPS on 4K
[01:04:24.480 --> 01:04:30.960]   and 240 FPS on 1080p on this phone. The front-facing camera is the same, but it's got a wider field
[01:04:30.960 --> 01:04:41.280]   of view, 90 degrees instead of 83. Snapdragon 765G, 8 gigs of RAM, lot of memory, 128 gigs of storage.
[01:04:42.800 --> 01:04:47.520]   There will be 5G, but it will not be millimeter wave. According to WinFuture, this is again a
[01:04:47.520 --> 01:04:55.920]   leak. It'll just be the sub-6 frequencies that T-Mobile offers. But maybe that's just a European
[01:04:55.920 --> 01:05:02.240]   version because this is a European magazine. Could be different in the States.
[01:05:02.240 --> 01:05:08.800]   And Micah Sargent will be very happy to hear, not that he's going to buy one. There'll be a green
[01:05:08.800 --> 01:05:14.640]   version. I had the orange last year. Green. He probably would because it's green. I like
[01:05:14.640 --> 01:05:18.320]   the green. He'll buy it just because it's green. Just because it's green.
[01:05:18.320 --> 01:05:26.800]   Well, here's a question, though. We talked about the demise of the Pixelbook. What's Google
[01:05:26.800 --> 01:05:30.640]   up to with phones? I mean, we all remember the Nexus phone, which they got rid of in favor of
[01:05:30.640 --> 01:05:36.000]   the Motorola phones, which they got rid of in favor of the Pixel phones. And now the Pixel phone
[01:05:36.000 --> 01:05:40.720]   is kind of... At first, the Pixel phone made a lot of sense because they're putting some proprietary
[01:05:40.720 --> 01:05:45.120]   AI into it. They were doing some interesting things with the camera. There was all kinds of
[01:05:45.120 --> 01:05:52.800]   stuff going on. And now it feels like the 5 is going to be just over the top speeds and feeds,
[01:05:52.800 --> 01:05:58.560]   which there's no reason for Google to do that. Samsung can do that. Lots of companies can build
[01:05:58.560 --> 01:06:05.360]   powerful phones. So why is Google... So the first question is, why is Google still making phones?
[01:06:05.360 --> 01:06:08.800]   And the second question is, will they continue doing this? Is this the last
[01:06:08.800 --> 01:06:14.960]   Pixel phone? And should it be? Wow, that's interesting. I thought the first one was the last one.
[01:06:14.960 --> 01:06:25.520]   Jesus. I know Jeff Jarvis wants the Pixel phone. But why... As a company, why is a company like Google...
[01:06:25.520 --> 01:06:32.000]   Their main business model seems to be to disappoint users. I mean, that seems to be mainly what they
[01:06:32.000 --> 01:06:42.320]   are in business to do. Yeah, it's a tragic... But still, we have to ask, why are they doing it and will
[01:06:42.320 --> 01:06:49.120]   they stop? I honestly thought the 4 would have been the last iteration because of the way the sales
[01:06:49.120 --> 01:06:57.440]   went with it. The 3A got a lot of promise and great hype because it worked out to be a great phone.
[01:06:57.440 --> 01:07:04.240]   But it seems like it really did just fail off the cliff with the 4. I haven't seen the numbers on
[01:07:04.240 --> 01:07:09.840]   the 4A, but I still just thought, "Google is just going to sign off and just continue to do what they
[01:07:09.840 --> 01:07:16.480]   do with big data and not necessarily worry about hardware." Yeah, I mean, what's in it for Google
[01:07:16.480 --> 01:07:23.440]   to go through all the trouble to be a phone maker? To make me happy. That's what Mike is
[01:07:23.440 --> 01:07:27.440]   just saying for Jeffy. Because I want my channel to be to Google devices. That's why.
[01:07:27.440 --> 01:07:30.480]   Well, if you're to Google, I don't... You're not going to be happy, Jeff. You're not going to be
[01:07:30.480 --> 01:07:36.400]   happy. I think it's not about hardware anymore. I think Google really wants a pure Android
[01:07:36.400 --> 01:07:41.360]   experience available somewhere so that you can compare it to that. Yeah, because it's so that
[01:07:41.360 --> 01:07:47.200]   you compare it to the crap that Samsung and others put on the phone. But again, that's another case
[01:07:47.200 --> 01:07:53.120]   where maybe it's not as necessary because OxygenOS is pretty pure, Motorola's Edge has pure Google on
[01:07:53.120 --> 01:08:01.200]   it. The days of these weird adaptations of Android are... Even the duo is pretty pure
[01:08:01.200 --> 01:08:06.480]   Android. It had a few features. Yeah. So I don't know. It's a great question.
[01:08:06.480 --> 01:08:12.960]   I don't know. Mike, you're just bitter. You lost your Google+.
[01:08:12.960 --> 01:08:18.000]   Oh, yeah. I want me to lose my gadgets. That's what it is. I know. Believe me, Jess, I'm also
[01:08:18.000 --> 01:08:24.560]   bitter because I'm losing the Pixelbook too. I have a Pixelbook and love it. I did have the Pixel 3,
[01:08:24.560 --> 01:08:31.840]   which... And I loved that. But I just figure... I want to lower expectations because
[01:08:31.840 --> 01:08:36.880]   when has Google ever launched a hardware platform... Not disappointed people.
[01:08:36.880 --> 01:08:43.680]   And stuck with it. Never. No. You're still a little frosted. All right.
[01:08:44.480 --> 01:08:48.880]   You're still a little hurt over Google+, I can tell. Yes. Yes. That's true.
[01:08:48.880 --> 01:08:56.560]   As much as I love Google+, Mr. Elgin, I had to step away from it pretty early. And I remember
[01:08:56.560 --> 01:09:00.800]   doing it quite vividly because I was so sick of the
[01:09:00.800 --> 01:09:09.040]   troll patrols, troll controls inside of it. It was really bad trying to manage my
[01:09:10.160 --> 01:09:16.000]   happiness in the feed because it was just a bad experience. I loved the people that were there,
[01:09:16.000 --> 01:09:21.600]   but it was just more stress trying to keep that environment clinging for me. And you
[01:09:21.600 --> 01:09:25.760]   reported and reported and nothing would ever happen. And now it's part of one of those
[01:09:25.760 --> 01:09:32.320]   elite groups, whatever it was called. I can't remember what it was. And you go into that group
[01:09:32.320 --> 01:09:37.840]   and you fuss and tell them, "Hey, you're asking for our feedback. Here's the feedback and nothing
[01:09:37.840 --> 01:09:44.080]   that's happening." And yeah, I just had to let it go. Google+ was great for four years and then
[01:09:44.080 --> 01:09:47.760]   they lost the plot completely. And it just kept getting worse and worse after that.
[01:09:47.760 --> 01:09:52.880]   Trolls were coming in. And you pointed out that they wouldn't stop the trolls. They would also flag
[01:09:52.880 --> 01:10:03.520]   half of the good comments as problematic. So it really sucked after, from 2011 to about 2014,
[01:10:03.520 --> 01:10:08.880]   something like that, '15. Am I getting that right? Yeah. The first two years.
[01:10:08.880 --> 01:10:12.480]   There too. That's what happened. What was it?
[01:10:12.480 --> 01:10:14.480]   Vic left. Vic and Dottie.
[01:10:14.480 --> 01:10:15.440]   Yeah. Dottie, that's it.
[01:10:15.440 --> 01:10:23.040]   So I want to get our crack production team to work on a special TikTok.
[01:10:23.040 --> 01:10:25.040]   You're producing crack now?
[01:10:25.040 --> 01:10:31.680]   Yes, they're cracked. They're cracked. I want them to work on a TikTok bumper until we have
[01:10:31.680 --> 01:10:40.240]   one. I'm just going to use this. Oh, man. These are the days.
[01:10:40.240 --> 01:10:44.720]   I moved through the hourglass. So are the days of our lives.
[01:10:44.720 --> 01:10:48.000]   One o'clock at the end. Take a go for that.
[01:10:48.000 --> 01:10:54.240]   The summer days. It's great hot styles. This is telling us to go outside and play.
[01:10:54.240 --> 01:11:00.160]   When does the stores come? If it's Wednesday, it must be another lawsuit. So you may remember,
[01:11:00.160 --> 01:11:05.680]   if we're picking up the plot, that WeChat and TikTok both were supposed to be turned off on
[01:11:05.680 --> 01:11:15.360]   Saturday. WeChat sued and got a restraining order. TikTok went the oracle. They got a friend of the
[01:11:15.360 --> 01:11:25.360]   president to invest. Not a lot. Now TikTok is asking for an injunction from the court on a ban
[01:11:25.360 --> 01:11:33.120]   that would require Apple and Google to remove TikTok. I presume WeChat as well, September 27th.
[01:11:33.120 --> 01:11:35.440]   So that's four days from now.
[01:11:35.440 --> 01:11:45.760]   Saying that this violates the First Amendment rights of the people on TikTok. It shuts down a
[01:11:45.760 --> 01:11:51.520]   platform. I've been screaming all along. And on this show, this is a First Amendment issue.
[01:11:51.520 --> 01:11:59.040]   This is the press of the people. And I'm so delighted that's how the court ruled on the WeChat ban.
[01:11:59.040 --> 01:12:02.160]   This was First Amendment for those folks.
[01:12:02.160 --> 01:12:11.200]   It's also hard to come up with. I get a lot of hate mail on this one
[01:12:11.200 --> 01:12:16.160]   because I'm trying to defend TikTok and so forth. They say, "You just love China.
[01:12:16.160 --> 01:12:21.760]   You're just economy." But actually, it's not that. It's not the case. What it really is,
[01:12:21.760 --> 01:12:32.400]   is I think we clearly have a threat from China. It's exercising its economic
[01:12:32.400 --> 01:12:36.880]   soft power all over the world. I don't think they're going to invade the US. It's not a military
[01:12:36.880 --> 01:12:42.080]   threat. It might be a cyber threat. But it's a very complicated strategic relationship.
[01:12:42.720 --> 01:12:47.840]   It requires diplomacy and some subtlety. It requires a lot of work because,
[01:12:47.840 --> 01:12:52.240]   on the other hand, we have a lot of stuff made in China. It's a giant market. Our companies
[01:12:52.240 --> 01:12:57.920]   would like to be in Apple. It's critical to Apple's finances that they sell phones in China
[01:12:57.920 --> 01:13:05.600]   and on and on and on. So it's complicated. I think it needs a strategic complicated vision,
[01:13:06.400 --> 01:13:13.040]   long-term vision to address it, diplomatic and otherwise, and to say, "Oh, well, the problem is
[01:13:13.040 --> 01:13:21.520]   these apps." If we just ban the apps, everything is going to be fine is more theater to distract you
[01:13:21.520 --> 01:13:26.640]   from what is actually a much more serious problem. I'm not saying, "Well, China's a great country.
[01:13:26.640 --> 01:13:32.400]   We've got to let them do anything they want by any means." But I do think that it's a much
[01:13:32.400 --> 01:13:40.080]   more subtle problem. Banning an app is a trivial response to it and has unintended consequences
[01:13:40.080 --> 01:13:46.880]   for its users. In your defense, I have seen you on several shows here on the network,
[01:13:46.880 --> 01:13:52.480]   including this one here. Whenever you talk about TikTok, it's never much of anything about China.
[01:13:52.480 --> 01:13:58.960]   What you seem to enjoy is just the way you're getting good content that suits you and their AI
[01:13:58.960 --> 01:14:02.720]   is just getting it right. Every time you pull it up, you get stuck in the time,
[01:14:02.720 --> 01:14:09.120]   stuck, and you're enjoying the content there, regardless if it's TikTok or whatever. You just
[01:14:09.120 --> 01:14:11.920]   like what it's giving you. I don't see anything wrong with that.
[01:14:11.920 --> 01:14:24.480]   We'll just see. I'm just shocked, Leo. I'm shocked that you think Trump isn't bringing
[01:14:24.480 --> 01:14:29.920]   subtlety and a long-term view and a technical sophistication to this argument.
[01:14:29.920 --> 01:14:34.640]   He's probably not a fine-right man for the job, but we still have a semi-intact state
[01:14:34.640 --> 01:14:42.000]   department. I'm sure that there are... The other weird part about this too is there's the $5
[01:14:42.000 --> 01:14:47.040]   billion. Now, are they really paying $5 billion of the treasury under what structure or is that
[01:14:47.040 --> 01:14:52.640]   merely the taxes they think they're going to pay? AP? Who pays it? Who pays it? Does Oracle pay it?
[01:14:52.640 --> 01:14:56.320]   Who pays it? That's unclear. It's all unclear. But then now we have a story.
[01:14:56.320 --> 01:15:03.200]   I just... TikTok, by the way, TikTok's latest plan is to do an IPO and pay $5 billion to the
[01:15:03.200 --> 01:15:09.040]   treasury out of the IPO, which wouldn't be for a year or two. But why does the treasury get
[01:15:09.040 --> 01:15:13.760]   a penny out of this? It makes no sense. But here's the thing. We have to pay for the new education camps.
[01:15:13.760 --> 01:15:18.960]   It's for the re-education. It is re-education. That's the problem. You have both Oracle,
[01:15:18.960 --> 01:15:23.600]   not surprising because run by a fan of Trump's. But you also have Walmart now saying,
[01:15:23.600 --> 01:15:31.120]   we're okay with money going into Trump's white supremacist anti-1619 project.
[01:15:31.120 --> 01:15:37.680]   Education plan. This is the education plan to the $5 billion. Yeah. And Walmart is saying...
[01:15:37.680 --> 01:15:41.280]   Of course they are. No, but of course they are. But no, this needs a reaction.
[01:15:41.280 --> 01:15:47.600]   Well, I agree. But they... Yeah, Oracle and Walmart don't want to offend the president because
[01:15:47.600 --> 01:15:51.440]   they're hoping that there's a pot of gold. Well, how about Walmart offends every African-American
[01:15:51.440 --> 01:15:55.520]   customer they have? Well, that might not be so good. And not just African-American
[01:15:55.520 --> 01:16:00.240]   customer. Every customer believes in history in the New York Times and Nicole Hannah Jones.
[01:16:00.240 --> 01:16:05.200]   Well, get ready. And because the latest is the Trump administration is now questioning
[01:16:05.200 --> 01:16:13.360]   Epic Games and Riot. Oh, Jesus. Tencent, by the way, has huge investments into pretty much every
[01:16:13.360 --> 01:16:20.240]   website you use, including Reddit. So Fortnite, League of Legends,
[01:16:20.240 --> 01:16:28.400]   if you want to... This confirms what you were saying earlier, which is the white men don't
[01:16:28.400 --> 01:16:32.800]   intend or expect to continue to rule. They just want to burn it down before they go.
[01:16:32.800 --> 01:16:42.320]   And because clearly, if you ban these very popular programs, all you're going to do is get
[01:16:42.320 --> 01:16:45.760]   more people hating you, which doesn't seem like a good electoral plan.
[01:16:45.760 --> 01:16:51.040]   It may be a cult... It's your leader cult. You feed off of hate.
[01:16:51.040 --> 01:16:56.080]   It also doesn't make sense to kill your voters, but he's doing that too.
[01:16:56.080 --> 01:16:58.880]   Well, that's a good point. Yeah. But I mean, it just...
[01:16:58.880 --> 01:17:04.720]   Yeah, it's burned back to politics. It's initially started talking about with
[01:17:04.720 --> 01:17:11.520]   whether the antitrust action against Google was going to happen when nobody knows.
[01:17:12.160 --> 01:17:17.680]   The way it's become clear that President Trump governs, and I don't want to get into a pro-Trump
[01:17:17.680 --> 01:17:24.480]   antitrust, let's just talk about how he approaches things. He sees himself as a negotiator,
[01:17:24.480 --> 01:17:30.000]   and he likes to have everybody on the ropes. So he likes... And once Google worried going to
[01:17:30.000 --> 01:17:35.440]   the election, he thinks in a very simple way, apparently, and he says, "Okay, Google has some
[01:17:35.440 --> 01:17:39.600]   control over what people see and hear and what they search for and all that kind of stuff.
[01:17:39.600 --> 01:17:44.880]   So let's just dangle this threat, vague threat of antitrust action out there. We don't move on it
[01:17:44.880 --> 01:17:49.360]   yet, not till I say so. And then over here, we got TikTok, and I'm going to... I don't like it,
[01:17:49.360 --> 01:17:55.520]   so I'm going to mix it all up, and then it'll be up to me to decide who gets what and where and
[01:17:55.520 --> 01:17:59.600]   all this kind of stuff. I'll do the re-education campaign because that'll own the lids, etc.
[01:17:59.600 --> 01:18:05.360]   But he likes to have all these plates spinning and all these things in play so that he can come in
[01:18:05.360 --> 01:18:13.680]   and just at the opportune time make a big change that's his change. And so he seems to operate in
[01:18:13.680 --> 01:18:20.080]   this way. And you can explain a lot of what happens at the federal level based on this sort of... I
[01:18:20.080 --> 01:18:26.080]   guess you'd call it a negotiating style to throw everybody off their game so that you have all this
[01:18:26.080 --> 01:18:35.680]   leverage. And that's one part of it. But I do also want to do kind of a full-throated look at
[01:18:35.680 --> 01:18:42.160]   the fundamental problem. So I agree with Jeff. I agree with you, Jeff, about the First Amendment
[01:18:42.160 --> 01:18:48.320]   rights of TikTok users. I agree that you shouldn't be banning apps and all that, but I do want to
[01:18:48.320 --> 01:18:54.800]   acknowledge that there's a problem and separate from all of that. And the problem is you have an
[01:18:54.800 --> 01:19:03.280]   authoritarian government who has, as part of their laws, that they can go and muck with any company
[01:19:03.280 --> 01:19:09.600]   operating in China. So you have an algorithmically generated content source that's just exploding
[01:19:09.600 --> 01:19:15.520]   and popularity globally. You have a government that bans other social networks from operating
[01:19:15.520 --> 01:19:22.080]   in that country. So Facebook doesn't get to play in China, but TikTok gets to play in the United
[01:19:22.080 --> 01:19:29.760]   States. So you have this disparity. And you have the government of China who is slowly building these
[01:19:29.760 --> 01:19:37.040]   advantages in a coming cyber war, propaganda war, hot war, cold war, whatever may come, they want to
[01:19:37.040 --> 01:19:41.840]   have all the advantages in place that they can. And TikTok is a potential advantage for propaganda.
[01:19:41.840 --> 01:19:46.000]   So you can imagine in an extreme case, let's just say they outright, they do ban certain things,
[01:19:46.000 --> 01:19:51.920]   but let's say they just said, you can never mention Winnie the Pooh on TikTok globally,
[01:19:51.920 --> 01:19:57.200]   you can never mention Tiananmen Square on TikTok globally, you can never mention any criticism
[01:19:57.200 --> 01:20:04.720]   of the Chinese government ever anywhere on TikTok. And let's say TikTok became the, by far, the world's
[01:20:04.720 --> 01:20:11.520]   town hall. You can see how that's a problem, right? Yeah, that's what the world says about us, Mike.
[01:20:12.320 --> 01:20:15.840]   No, no, I agree with you. I agree with you. I agree with you.
[01:20:15.840 --> 01:20:21.760]   Let's look at Hollywood. Let's look at Hollywood, for example, Hollywood movies. In Chinese movies,
[01:20:21.760 --> 01:20:27.680]   you can criticize the United States, but you can't criticize the Chinese Communist Party.
[01:20:27.680 --> 01:20:35.120]   In Hollywood movies, you can criticize the United States, but you cannot criticize China,
[01:20:35.120 --> 01:20:41.360]   because Hollywood will voluntarily censor their movies because they want that market.
[01:20:41.360 --> 01:20:46.960]   So we're entering into a world on many sources of content where China is always the good guy,
[01:20:46.960 --> 01:20:54.240]   and the United States is always the bad guy. This is an issue, I think, that has to be dealt with.
[01:20:54.240 --> 01:20:58.640]   I don't think it's banning apps, it's not going to get us there. But I mean, we barely even talk about
[01:20:58.640 --> 01:21:04.400]   it. I would agree with you. There are many, many issues. I mean, China is acting strategically
[01:21:04.400 --> 01:21:12.080]   on a hundred year plan, and we can't make this single plan for a week. So I agree, there are issues.
[01:21:12.080 --> 01:21:19.760]   I'm not saying, but I don't think banning TikTok addresses the deep underlying issues that we...
[01:21:19.760 --> 01:21:25.680]   No, it doesn't eat at all. I don't know. Yeah, okay. So anyway, who cares if you ban TikTok?
[01:21:25.680 --> 01:21:27.200]   It's just a stupid app anyway.
[01:21:30.160 --> 01:21:35.200]   Actually, a lot of people on TikTok care will be upset, but there will be another app.
[01:21:35.200 --> 01:21:40.240]   Maybe, but everybody learned how to use TikTok. Why should they have to move?
[01:21:40.240 --> 01:21:44.880]   Well, and TikTok was a step change, I believe, because it was the first one that really enabled
[01:21:44.880 --> 01:21:49.520]   people to collaborate in a string like that. I think it was a next change. It wasn't that...
[01:21:49.520 --> 01:21:51.040]   I'm going to make something that you're going to talk about.
[01:21:51.040 --> 01:21:58.400]   Isn't that cool? Yeah, it's the re-sharing, the reuse. It's exactly what people like Corey
[01:21:58.400 --> 01:22:08.240]   Doctorow. Oh, I forgot his name. Larry... Magic? No, no. The...
[01:22:08.240 --> 01:22:12.000]   A bloke? Yeah, Larry Cutlow has been arguing for this for years.
[01:22:12.000 --> 01:22:13.280]   Murder!
[01:22:13.280 --> 01:22:21.440]   You know, the Creative Commons guy. It's what they've been arguing for a long time, which is
[01:22:21.440 --> 01:22:24.160]   the best that you have to have a creative commons.
[01:22:24.160 --> 01:22:27.360]   Larry, thank you. You have to have a creative commons so that people can...
[01:22:27.360 --> 01:22:36.480]   Walt Disney wrote his stories based on long public domain fairy tales, but then immediately
[01:22:36.480 --> 01:22:42.400]   pulled up the ladder and said, "You can't reuse my stuff." It's important that the public domain
[01:22:42.400 --> 01:22:47.760]   exists, the public commons, the public sphere exists, so that people can build upon it.
[01:22:47.760 --> 01:22:51.040]   That's what TikTok does really well. They've been talking about this for years,
[01:22:51.040 --> 01:22:55.600]   but here's a platform that actually formalizes it, and it works beautifully.
[01:22:55.600 --> 01:23:00.560]   It's really cool to watch people do that. And even do music. In fact, so much so that it's
[01:23:00.560 --> 01:23:06.000]   pushed companies like Facebook to say, "We got music you can use."
[01:23:06.000 --> 01:23:14.320]   Facebook's licensed apparently a significant amount of commercial music so that people,
[01:23:14.320 --> 01:23:18.720]   creators on Facebook can use it. And I think that that's definitely a response to TikTok, right?
[01:23:18.720 --> 01:23:25.600]   It's not the same though. It's not all the same. It's very interesting. Let's take a...
[01:23:25.600 --> 01:23:26.560]   Go ahead.
[01:23:26.560 --> 01:23:32.800]   There's a parting shot against Facebook. It doesn't take a whole lot for Facebook to copy
[01:23:32.800 --> 01:23:34.880]   what somebody else is doing in the social sphere.
[01:23:34.880 --> 01:23:38.240]   No, they're trying. But the interesting thing is, when something they haven't done well is
[01:23:38.240 --> 01:23:42.640]   copied TikTok. Instagram created reels. They really tried to make a TikTok clone.
[01:23:42.640 --> 01:23:46.320]   They're not the only ones. A lot of people are trying to do this. YouTube has a similar plan.
[01:23:47.360 --> 01:23:51.680]   Something about TikTok. It's that man. And Trump was not able to... Trump used all
[01:23:51.680 --> 01:23:55.520]   every other platform. He wasn't able to use TikTok to his benefit. That's part of the reason he
[01:23:55.520 --> 01:24:02.560]   wrote it. I have to say though, if you look for it, there is a lot of pro-Trump content on TikTok.
[01:24:02.560 --> 01:24:07.440]   Maybe the algorithm does not forward it along. I don't know.
[01:24:07.440 --> 01:24:09.760]   I think the algorithm... There was a story about the algorithm last week.
[01:24:09.760 --> 01:24:12.640]   To my mind, the algorithm is extremely simplistic.
[01:24:13.600 --> 01:24:19.680]   That's what Matt Cut said. When he was on this show, he said, "Oh, yeah, I could do that. I could
[01:24:19.680 --> 01:24:24.880]   write that algorithm in five minutes." Actually, he was on Twitter when he said that. But yeah.
[01:24:24.880 --> 01:24:32.800]   Yeah, I want to just take a little break. Keep us on schedule here. So we move along.
[01:24:32.800 --> 01:24:36.000]   And then we have the changelog still to come. Lots of other stories.
[01:24:36.000 --> 01:24:36.960]   That's a good struggle to run up.
[01:24:36.960 --> 01:24:42.400]   Yeah. I'll tell you what, I like to do this. When we come back, you guys, each of you...
[01:24:42.400 --> 01:24:46.160]   How about this for a teaser? Show a picture. Can you find the cat?
[01:24:46.160 --> 01:24:50.160]   Can you find number 152? The cat. Should I do that?
[01:24:50.160 --> 01:24:51.200]   You find the cat.
[01:24:51.200 --> 01:24:51.680]   Before the camera.
[01:24:51.680 --> 01:24:54.640]   Two more handkers with us. Let people look for the cat.
[01:24:54.640 --> 01:24:58.480]   They're listening to the commercial, I know. But they might also be able to multitask.
[01:24:58.480 --> 01:24:59.840]   And can they find...
[01:24:59.840 --> 01:25:04.320]   This is on Reddit. The Find My Cat Challenge.
[01:25:04.320 --> 01:25:07.920]   Should I play this video? What should I do?
[01:25:07.920 --> 01:25:10.160]   It was on a video. No, no, no, it's not a video. It's the picture.
[01:25:10.160 --> 01:25:11.520]   It's the Reddit picture. It's still a picture.
[01:25:11.520 --> 01:25:13.840]   I see that. That's not it.
[01:25:13.840 --> 01:25:14.640]   No, where is it?
[01:25:14.640 --> 01:25:15.600]   I see.
[01:25:15.600 --> 01:25:17.360]   Keep going. Keep going. You're part of me.
[01:25:17.360 --> 01:25:18.160]   There's somebody at.
[01:25:18.160 --> 01:25:18.960]   This is a comment section.
[01:25:18.960 --> 01:25:21.600]   No, no, it's up. You're right. It's up. It's up. What's going on?
[01:25:21.600 --> 01:25:22.240]   Oh, here it is.
[01:25:22.240 --> 01:25:23.520]   No. You found it.
[01:25:23.520 --> 01:25:25.280]   Okay. No, that's not it.
[01:25:25.280 --> 01:25:26.480]   That's not it.
[01:25:26.480 --> 01:25:28.320]   This is a parliamentary meeting,
[01:25:28.320 --> 01:25:31.840]   been which a tale wandered through the video.
[01:25:31.840 --> 01:25:33.040]   But you're not talking about that.
[01:25:33.040 --> 01:25:33.680]   No.
[01:25:33.680 --> 01:25:36.400]   There's a Reddit picture there.
[01:25:36.400 --> 01:25:37.360]   The ones...
[01:25:37.360 --> 01:25:38.320]   In this photo...
[01:25:38.320 --> 01:25:39.360]   The earlier video...
[01:25:39.360 --> 01:25:40.560]   Okay, I'm clicking this link.
[01:25:40.560 --> 01:25:46.000]   This is from the subreddit r/aw.
[01:25:46.000 --> 01:25:47.200]   There, that's it.
[01:25:47.200 --> 01:25:48.000]   Find...
[01:25:48.000 --> 01:25:49.280]   Wait, wait, let me make it a little smaller
[01:25:49.280 --> 01:25:50.720]   because I got to get it all in the frame.
[01:25:50.720 --> 01:25:52.800]   Somewhere in this photo,
[01:25:52.800 --> 01:25:54.880]   there is a kitty.
[01:25:54.880 --> 01:25:56.720]   Okay, so...
[01:25:56.720 --> 01:25:56.880]   Those people...
[01:25:56.880 --> 01:25:57.200]   Those people...
[01:25:57.200 --> 01:25:57.680]   You played it whole.
[01:25:57.680 --> 01:25:58.640]   Screen cap this.
[01:25:58.640 --> 01:26:00.960]   I'll just leave this up while I do the ad.
[01:26:00.960 --> 01:26:02.080]   Why my cat?
[01:26:02.080 --> 01:26:02.800]   In this photo.
[01:26:02.800 --> 01:26:04.000]   And...
[01:26:04.000 --> 01:26:04.960]   I read it.
[01:26:04.960 --> 01:26:05.840]   When you were listening over...
[01:26:05.840 --> 01:26:06.240]   Over...
[01:26:06.240 --> 01:26:07.360]   Did you find it, Jeff?
[01:26:07.360 --> 01:26:08.160]   Were you able to find it?
[01:26:08.160 --> 01:26:08.720]   No, I didn't.
[01:26:08.720 --> 01:26:09.520]   Oh, I had the cheat.
[01:26:09.520 --> 01:26:10.480]   I had to go to the spoiler.
[01:26:10.480 --> 01:26:10.960]   Yep.
[01:26:10.960 --> 01:26:11.280]   Fine.
[01:26:11.280 --> 01:26:12.160]   Didn't find the cat.
[01:26:12.160 --> 01:26:13.600]   So, just...
[01:26:13.600 --> 01:26:14.080]   We'll come back.
[01:26:14.080 --> 01:26:14.800]   We'll find the cat.
[01:26:14.800 --> 01:26:15.360]   What the...
[01:26:15.360 --> 01:26:16.000]   What the...
[01:26:16.000 --> 01:26:16.960]   Where is the cat?
[01:26:16.960 --> 01:26:19.200]   The cat.
[01:26:19.200 --> 01:26:19.760]   There's a cat.
[01:26:19.760 --> 01:26:20.560]   Now you got me.
[01:26:20.560 --> 01:26:22.240]   That's a...
[01:26:22.240 --> 01:26:23.200]   This is a fun game.
[01:26:23.200 --> 01:26:23.840]   Is this a game...
[01:26:23.840 --> 01:26:24.560]   It's a fun game.
[01:26:24.560 --> 01:26:25.680]   Is this a game on Reddit?
[01:26:25.680 --> 01:26:26.720]   Yeah.
[01:26:26.720 --> 01:26:28.960]   Oh, Burke found it.
[01:26:28.960 --> 01:26:29.840]   They want more of the same.
[01:26:29.840 --> 01:26:30.720]   Don't tell us, Burke.
[01:26:30.720 --> 01:26:31.920]   Burke says he's found it.
[01:26:31.920 --> 01:26:32.800]   When we come back...
[01:26:32.800 --> 01:26:33.760]   I'll see it too.
[01:26:33.760 --> 01:26:34.320]   Oh!
[01:26:34.320 --> 01:26:36.400]   Burke and Ant will reveal the cat.
[01:26:36.400 --> 01:26:38.000]   I see the Samsung phone.
[01:26:38.640 --> 01:26:40.080]   I see two remote controls.
[01:26:40.080 --> 01:26:41.280]   I see a coffee cup.
[01:26:41.280 --> 01:26:44.080]   But is there a kitty cat in there?
[01:26:44.080 --> 01:26:46.000]   Is it big or is it little?
[01:26:46.000 --> 01:26:47.440]   I'd say no.
[01:26:47.440 --> 01:26:48.160]   No, you're cheating.
[01:26:48.160 --> 01:26:49.600]   No, no, no, no, no, no, no.
[01:26:49.600 --> 01:26:50.160]   No.
[01:26:50.160 --> 01:26:51.040]   Nice try.
[01:26:51.040 --> 01:26:52.000]   Don't do your commercial.
[01:26:52.000 --> 01:26:53.200]   Is it clearly visible?
[01:26:53.200 --> 01:26:55.280]   Just go do your commercial.
[01:26:55.280 --> 01:26:56.240]   I think so.
[01:26:56.240 --> 01:26:57.280]   Don't do your commercial.
[01:26:57.280 --> 01:26:58.400]   I should today.
[01:26:58.400 --> 01:26:59.920]   I got a new game.
[01:26:59.920 --> 01:27:02.480]   Find the hot cloud storage.
[01:27:02.480 --> 01:27:07.680]   It's right underneath your sushi there.
[01:27:07.680 --> 01:27:08.720]   It's that green ball.
[01:27:08.720 --> 01:27:12.320]   It's called Wasabi hot cloud storage,
[01:27:12.320 --> 01:27:18.480]   which is the best deal on storage since the zip drive.
[01:27:18.480 --> 01:27:19.280]   Let me tell you this.
[01:27:19.280 --> 01:27:20.880]   Wasabi...
[01:27:20.880 --> 01:27:23.200]   Look, there are many, many businesses these days
[01:27:23.200 --> 01:27:24.400]   who generate a lot of data.
[01:27:24.400 --> 01:27:25.520]   We generate a lot of data.
[01:27:25.520 --> 01:27:26.400]   Every show we do.
[01:27:26.400 --> 01:27:27.680]   We generate a ton of data.
[01:27:27.680 --> 01:27:29.360]   Terabytes of data.
[01:27:29.360 --> 01:27:30.000]   Every...
[01:27:30.000 --> 01:27:32.000]   And we know we're going to have to store that.
[01:27:32.000 --> 01:27:33.040]   So what do we do?
[01:27:33.040 --> 01:27:34.880]   You go out and buy more hard drive capacity.
[01:27:34.880 --> 01:27:36.880]   Yeah, that's what a lot of people do.
[01:27:37.520 --> 01:27:40.400]   But did you know you could buy cloud storage on Wasabi
[01:27:40.400 --> 01:27:45.760]   for less than just the annual cost of maintenance fees
[01:27:45.760 --> 01:27:48.560]   on the same amount of on-premises storage?
[01:27:48.560 --> 01:27:53.520]   Just the maintenance fees alone cost more than Wasabi
[01:27:53.520 --> 01:27:54.960]   for the same amount of storage.
[01:27:54.960 --> 01:27:55.920]   Wasabi's amazing.
[01:27:55.920 --> 01:27:56.880]   And is it...
[01:27:56.880 --> 01:27:57.760]   Now somebody might say,
[01:27:57.760 --> 01:27:59.440]   "Well, but yes, but my on-prem storage,
[01:27:59.440 --> 01:28:00.480]   I could see it right there.
[01:28:00.480 --> 01:28:01.440]   It's more secure.
[01:28:01.440 --> 01:28:02.800]   Oh, I don't think so.
[01:28:02.800 --> 01:28:06.560]   First of all, Wasabi stores your data encrypted,
[01:28:06.560 --> 01:28:08.480]   even if you don't specify it.
[01:28:08.480 --> 01:28:11.600]   On Premier Tier 4 Data Center facilities
[01:28:11.600 --> 01:28:14.080]   that are highly secure and fully redundant,
[01:28:14.080 --> 01:28:16.400]   Wasabi does active integrity checking.
[01:28:16.400 --> 01:28:18.080]   Do you do this on your on-prem?
[01:28:18.080 --> 01:28:22.480]   Every object stored is checked for integrity every 90 days.
[01:28:22.480 --> 01:28:24.000]   And if it's not exactly perfect,
[01:28:24.000 --> 01:28:26.000]   it's replaced from the redundant storage.
[01:28:26.000 --> 01:28:30.320]   That means they've got 11 nines of durability.
[01:28:30.320 --> 01:28:32.240]   Do you have 11 nines on your hard drive?
[01:28:32.240 --> 01:28:33.200]   I don't think so.
[01:28:33.200 --> 01:28:35.120]   I don't think so.
[01:28:35.120 --> 01:28:38.080]   Wasabi follows all the industry's best security models
[01:28:38.080 --> 01:28:39.200]   and design practices.
[01:28:39.200 --> 01:28:42.560]   They have access control mechanisms, bucket policies, ACLs.
[01:28:42.560 --> 01:28:45.280]   But I love this even more.
[01:28:45.280 --> 01:28:47.920]   You can't do this on your on-prem storage.
[01:28:47.920 --> 01:28:49.840]   Your data will be immutable.
[01:28:49.840 --> 01:28:52.720]   You could set data so that it can't be erased.
[01:28:52.720 --> 01:28:55.600]   It can't be altered by anything from ransomware,
[01:28:55.600 --> 01:28:58.560]   other kinds of malware, hackers,
[01:28:58.560 --> 01:29:00.160]   even from yourself.
[01:29:00.160 --> 01:29:02.080]   And frankly, let's be honest,
[01:29:02.080 --> 01:29:05.040]   human hair is the really the biggest threat to data.
[01:29:05.040 --> 01:29:08.480]   But if it's immutable, none of it matters.
[01:29:08.480 --> 01:29:10.080]   Wasabi's amazing.
[01:29:10.080 --> 01:29:12.160]   Now, let me on top of that say,
[01:29:12.160 --> 01:29:17.040]   it is one-fifth the cost of Amazon's S3.
[01:29:17.040 --> 01:29:19.280]   One-fifth.
[01:29:19.280 --> 01:29:22.640]   And it is about six times faster.
[01:29:22.640 --> 01:29:25.200]   So, okay, let's recap.
[01:29:25.200 --> 01:29:28.160]   More secure, faster, less expensive.
[01:29:28.160 --> 01:29:28.960]   It's wasabi.
[01:29:28.960 --> 01:29:30.160]   It's pretty much a no-brainer.
[01:29:30.160 --> 01:29:30.720]   I don't...
[01:29:30.720 --> 01:29:33.920]   The problem is it's a brand new company, a brand new product.
[01:29:33.920 --> 01:29:35.600]   Maybe you hadn't heard about it before.
[01:29:35.600 --> 01:29:39.040]   I want you to try wasabi because I think you will love it.
[01:29:39.040 --> 01:29:42.160]   And by the way, if you're an MSP and you resell storage,
[01:29:42.160 --> 01:29:44.080]   resell cloud, you're going to love it
[01:29:44.080 --> 01:29:48.240]   because you can make more money and charge less to your customers.
[01:29:48.240 --> 01:29:50.720]   Everybody wins with wasabi.
[01:29:50.720 --> 01:29:54.640]   Hippo-compliant, FINRA-compliant, CJIS-compliant.
[01:29:54.640 --> 01:29:58.960]   Wasabi is highly secure, disruptive technology
[01:29:58.960 --> 01:30:01.280]   that's turning the industry on its ear.
[01:30:01.280 --> 01:30:04.720]   And it's 80% cheaper and six times the speed of the industry leader.
[01:30:04.720 --> 01:30:07.520]   Oh, and by the way, and it bugs the heck out of me.
[01:30:07.520 --> 01:30:10.240]   I put stuff on S3 and they charge...
[01:30:10.240 --> 01:30:11.600]   Hey, a nice price.
[01:30:11.600 --> 01:30:14.080]   I stored it on S3 and then they charge you to get it.
[01:30:14.080 --> 01:30:20.160]   Not wasabi, no charge for egress, zero, no charge for API access.
[01:30:20.160 --> 01:30:23.760]   Now, this is the top where it uses the Amazon S3 API.
[01:30:23.760 --> 01:30:25.680]   It's API compatible with Amazon S3.
[01:30:25.680 --> 01:30:28.160]   So, all your tools, all the stuff you've written,
[01:30:28.160 --> 01:30:30.000]   all the tools you use, it all works.
[01:30:30.960 --> 01:30:33.680]   Wow. I should mention they have this...
[01:30:33.680 --> 01:30:36.320]   Now, normally it's just a flat rate, it's very affordable,
[01:30:36.320 --> 01:30:39.200]   but they do have this new thing called reserve capacity storage.
[01:30:39.200 --> 01:30:42.640]   If you are, like us, churning out a pretty consistent amount of data every month,
[01:30:42.640 --> 01:30:46.000]   you know you're going to need this much storage every month.
[01:30:46.000 --> 01:30:52.000]   You can reserve your storage for one, three, or five years, whatever amount you need,
[01:30:52.000 --> 01:30:56.000]   and you'll get even more discounts, even more affordable.
[01:30:56.000 --> 01:30:58.800]   The longer the term, the greater the capacity, the more you save.
[01:30:59.520 --> 01:31:02.080]   So, that's wasabi's reserved capacity storage.
[01:31:02.080 --> 01:31:03.280]   You got to check that out too.
[01:31:03.280 --> 01:31:10.320]   I just don't know why everybody says, "Oh, Amazon or Google or Microsoft Cloud."
[01:31:10.320 --> 01:31:14.080]   No, no, you got to be doing wasabi.
[01:31:14.080 --> 01:31:19.360]   Calculate the savings for yourself and kick yourself for not doing it earlier.
[01:31:19.360 --> 01:31:23.520]   Start a free trial of storage right now for one month at wasabi.com.
[01:31:23.520 --> 01:31:26.800]   Click the free trial link and please, if you would, enter the offer code TWIT
[01:31:26.800 --> 01:31:28.000]   so they know you heard it here.
[01:31:28.000 --> 01:31:28.720]   Join the movement.
[01:31:28.720 --> 01:31:31.600]   Migrate your data to the cloud and do it with confidence.
[01:31:31.600 --> 01:31:36.640]   Wasabi.com, do the free trial, but do use the offer code TWIT.
[01:31:36.640 --> 01:31:38.080]   Thank you, wasabi, for your support.
[01:31:38.080 --> 01:31:42.160]   Thank you for supporting us by using TWIT as the offer code.
[01:31:42.160 --> 01:31:44.960]   Wasabi! Hot cloud storage.
[01:31:44.960 --> 01:31:46.000]   All right, I'm looking.
[01:31:46.000 --> 01:31:47.840]   I'm looking.
[01:31:47.840 --> 01:31:50.720]   Do you want to give us?
[01:31:50.720 --> 01:31:51.440]   Don't tell me.
[01:31:51.440 --> 01:31:52.320]   Give us.
[01:31:52.320 --> 01:31:55.600]   Let me zoom in and look at different parts.
[01:31:55.600 --> 01:31:56.960]   Is that the cat there on the?
[01:31:57.520 --> 01:31:58.880]   No, that's not the cat there.
[01:31:58.880 --> 01:32:00.320]   Is that close?
[01:32:00.320 --> 01:32:03.040]   Is the cat in the chandelier?
[01:32:03.040 --> 01:32:04.880]   No.
[01:32:04.880 --> 01:32:06.800]   Is the cat in the Buddha?
[01:32:06.800 --> 01:32:08.400]   Stay with the stairs.
[01:32:08.400 --> 01:32:09.360]   Stay with the stair.
[01:32:09.360 --> 01:32:11.680]   Oh, there's the kitty.
[01:32:11.680 --> 01:32:12.880]   There's the kitty cat.
[01:32:12.880 --> 01:32:15.840]   Oh, thank you for the hint, Ant.
[01:32:15.840 --> 01:32:19.040]   The cats just look peering around the corner.
[01:32:19.040 --> 01:32:20.320]   You see all you see is one ear.
[01:32:20.320 --> 01:32:21.600]   It's kind of great, isn't it?
[01:32:21.600 --> 01:32:23.120]   That's a great cat photo.
[01:32:23.120 --> 01:32:26.800]   Sometimes I see too many cat photos.
[01:32:27.440 --> 01:32:27.920]   But this one's-
[01:32:27.920 --> 01:32:29.200]   You can't see too many cat photos.
[01:32:29.200 --> 01:32:30.080]   Not in this case.
[01:32:30.080 --> 01:32:30.560]   No, possible.
[01:32:30.560 --> 01:32:31.600]   Do you have cats, Jeff?
[01:32:31.600 --> 01:32:33.280]   Well, I see.
[01:32:33.280 --> 01:32:35.680]   So I'm a dog person by birth.
[01:32:35.680 --> 01:32:36.480]   Yes, as am I.
[01:32:36.480 --> 01:32:38.160]   My wife's a cat person.
[01:32:38.160 --> 01:32:38.800]   Of course.
[01:32:38.800 --> 01:32:40.320]   And when the debate came,
[01:32:40.320 --> 01:32:45.280]   she would wake up the children on a really cold, wet,
[01:32:45.280 --> 01:32:46.560]   miserable morning and say,
[01:32:46.560 --> 01:32:50.000]   "If we had a dog, you'd have to walk it now."
[01:32:50.000 --> 01:32:50.880]   Oh!
[01:32:50.880 --> 01:32:51.680]   Pick up the team.
[01:32:51.680 --> 01:32:52.320]   Oh!
[01:32:52.320 --> 01:32:52.800]   Oh!
[01:32:52.800 --> 01:32:53.280]   Oh!
[01:32:53.280 --> 01:32:53.840]   I lost.
[01:32:53.840 --> 01:32:54.640]   Oh, ow!
[01:32:54.640 --> 01:32:55.360]   I love the cat.
[01:32:55.360 --> 01:32:55.920]   I love the cat.
[01:32:55.920 --> 01:32:56.800]   Cats wonderful.
[01:32:56.800 --> 01:32:58.640]   That is, that's just going.
[01:32:58.640 --> 01:32:59.680]   It was unfair.
[01:32:59.680 --> 01:33:01.440]   That's going below the belt, man.
[01:33:01.440 --> 01:33:03.840]   Wow.
[01:33:03.840 --> 01:33:11.120]   Casey Newton.
[01:33:11.120 --> 01:33:12.080]   Casey Newton.
[01:33:12.080 --> 01:33:12.880]   Casey Newton leaves the verge.
[01:33:12.880 --> 01:33:14.800]   Leaves the verge.
[01:33:14.800 --> 01:33:15.920]   What?
[01:33:15.920 --> 01:33:17.040]   Where's Casey going?
[01:33:17.040 --> 01:33:17.840]   Do you see that?
[01:33:17.840 --> 01:33:19.040]   Line 156.
[01:33:19.040 --> 01:33:21.280]   Line 156.
[01:33:21.280 --> 01:33:22.560]   I love this.
[01:33:22.560 --> 01:33:23.520]   It's like a deli.
[01:33:24.400 --> 01:33:26.640]   I'd like one from 156.
[01:33:26.640 --> 01:33:28.640]   He's going to substack.
[01:33:28.640 --> 01:33:29.760]   That's not even a place.
[01:33:29.760 --> 01:33:30.720]   It's a newsletter.
[01:33:30.720 --> 01:33:32.400]   He's doing his own.
[01:33:32.400 --> 01:33:33.760]   Yep.
[01:33:33.760 --> 01:33:34.480]   Charging for it.
[01:33:34.480 --> 01:33:37.760]   So that's really interesting.
[01:33:37.760 --> 01:33:39.760]   So basically, he says,
[01:33:39.760 --> 01:33:40.960]   "I'm going to make my..."
[01:33:40.960 --> 01:33:41.920]   You did that, Mike.
[01:33:41.920 --> 01:33:42.720]   Didn't you, kind of?
[01:33:42.720 --> 01:33:44.720]   He said, "I'm going to make my own way.
[01:33:44.720 --> 01:33:45.600]   I'm going to do a newsletter."
[01:33:45.600 --> 01:33:48.560]   Oh, Mike, you're muted.
[01:33:48.560 --> 01:33:50.560]   He's muted.
[01:33:50.560 --> 01:33:52.160]   I don't know if that's us, sir.
[01:33:53.440 --> 01:33:54.320]   That was me.
[01:33:54.320 --> 01:33:54.960]   Sorry about that.
[01:33:54.960 --> 01:33:58.480]   No, I re-enabled my newsletter.
[01:33:58.480 --> 01:34:00.880]   I started my newsletter literally 20 years ago.
[01:34:00.880 --> 01:34:06.720]   So I brought it, it sort of went away for a few years,
[01:34:06.720 --> 01:34:08.720]   and I brought it back about three years ago.
[01:34:08.720 --> 01:34:09.840]   But newsletters have,
[01:34:09.840 --> 01:34:11.840]   obviously, they're...
[01:34:11.840 --> 01:34:14.000]   Same thing's happening to newsletters
[01:34:14.000 --> 01:34:15.840]   that happen to podcasting,
[01:34:15.840 --> 01:34:17.360]   where for a long time,
[01:34:17.360 --> 01:34:20.480]   there's a lot of people doing personalized things,
[01:34:20.480 --> 01:34:22.640]   and then larger and larger publications nowadays.
[01:34:23.200 --> 01:34:24.800]   Are getting a newsletter publishing.
[01:34:24.800 --> 01:34:27.440]   So it's a crowded space.
[01:34:27.440 --> 01:34:30.160]   Substack is interesting.
[01:34:30.160 --> 01:34:31.280]   Tell me about Substack.
[01:34:31.280 --> 01:34:33.120]   Does anybody use Substack here?
[01:34:33.120 --> 01:34:34.160]   I don't.
[01:34:34.160 --> 01:34:37.200]   People, they're doing neat things.
[01:34:37.200 --> 01:34:41.440]   They're creating a lot of stuff for the writers.
[01:34:41.440 --> 01:34:42.720]   They're looking to try to get them
[01:34:42.720 --> 01:34:44.720]   Bible insurance, I think, and health insurance
[01:34:44.720 --> 01:34:45.760]   and stuff like that.
[01:34:45.760 --> 01:34:46.880]   They're very services.
[01:34:46.880 --> 01:34:48.480]   They're very smart.
[01:34:48.480 --> 01:34:51.680]   They're going out and recruiting big names like this,
[01:34:51.680 --> 01:34:52.880]   and paying them money to do it.
[01:34:52.880 --> 01:34:54.080]   Very much like podcast world.
[01:34:54.080 --> 01:34:55.440]   Oh, that's interesting.
[01:34:55.440 --> 01:34:59.520]   They take, I think, 10% of your revenue,
[01:34:59.520 --> 01:35:00.560]   which some think is high.
[01:35:00.560 --> 01:35:03.760]   And they're doing neat stuff.
[01:35:03.760 --> 01:35:06.400]   So I got a discussion today with Matthew Ingram and others
[01:35:06.400 --> 01:35:07.040]   kind of saying,
[01:35:07.040 --> 01:35:09.360]   he was noting this and saying,
[01:35:09.360 --> 01:35:10.560]   "Where is this going to go?"
[01:35:10.560 --> 01:35:16.080]   And how applicable it is to the long tail of journalism,
[01:35:16.080 --> 01:35:18.800]   as opposed to the high profile writers like Casey.
[01:35:18.800 --> 01:35:19.680]   Well, those come along.
[01:35:19.680 --> 01:35:21.360]   Carol and McCarthy came along and said,
[01:35:21.360 --> 01:35:23.360]   "Something that I think will have to be addressed
[01:35:23.360 --> 01:35:26.480]   is that people simply do not want to open that many emails."
[01:35:26.480 --> 01:35:27.520]   And I couldn't read more.
[01:35:27.520 --> 01:35:28.160]   I agree.
[01:35:28.160 --> 01:35:28.640]   I feel that.
[01:35:28.640 --> 01:35:29.040]   I agree.
[01:35:29.040 --> 01:35:29.520]   I feel like some of the news is-
[01:35:29.520 --> 01:35:32.000]   All the news that I'm subscribed to go into a pile
[01:35:32.000 --> 01:35:32.960]   where I don't read them.
[01:35:32.960 --> 01:35:34.160]   So she said,
[01:35:34.160 --> 01:35:36.160]   "I feel like some kind of custom consolidated hub
[01:35:36.160 --> 01:35:37.680]   is the next step to which I responded to now."
[01:35:37.680 --> 01:35:38.400]   That's medium.
[01:35:38.400 --> 01:35:40.480]   Email is not a very good delivery mechanism
[01:35:40.480 --> 01:35:41.840]   for email newsletters as it turns out.
[01:35:41.840 --> 01:35:44.480]   We might need something new like RSS.
[01:35:44.480 --> 01:35:44.720]   Oh.
[01:35:44.720 --> 01:35:46.960]   She's Louise.
[01:35:46.960 --> 01:35:49.200]   Towards Carol and something else.
[01:35:49.200 --> 01:35:52.560]   Let's cue all the morning for Google Reader being gone.
[01:35:52.560 --> 01:35:53.520]   Right.
[01:35:53.520 --> 01:35:57.360]   So this is a newsletter that you would pay for?
[01:35:57.360 --> 01:36:00.560]   Yes, it's $10 a month or $100 a year
[01:36:00.560 --> 01:36:03.680]   or there's $1,000 a month or $100 a month level.
[01:36:03.680 --> 01:36:05.760]   But you don't know what the real benefit is
[01:36:05.760 --> 01:36:07.040]   until you're inside.
[01:36:07.040 --> 01:36:10.960]   So Casey's newsletter is going to be called "platformer."
[01:36:10.960 --> 01:36:14.880]   News at the intersection of Silicon Valley and democracy.
[01:36:14.880 --> 01:36:17.200]   Okay.
[01:36:18.080 --> 01:36:19.680]   Now I have to point out,
[01:36:19.680 --> 01:36:22.400]   medium is another way people are doing this.
[01:36:22.400 --> 01:36:24.960]   The problem I've always had,
[01:36:24.960 --> 01:36:28.000]   and I've always been very nervous about is
[01:36:28.000 --> 01:36:30.080]   if you live on somebody else's platform,
[01:36:30.080 --> 01:36:31.120]   you're at their mercy.
[01:36:31.120 --> 01:36:32.880]   Here's the difference I think.
[01:36:32.880 --> 01:36:34.800]   Here's the thing.
[01:36:34.800 --> 01:36:35.600]   I was talking about this.
[01:36:35.600 --> 01:36:37.280]   I got a discussion with Matthew about this too.
[01:36:37.280 --> 01:36:38.880]   As he was saying,
[01:36:38.880 --> 01:36:40.240]   "Podcasts were all independent
[01:36:40.240 --> 01:36:41.440]   and then they went to big companies."
[01:36:41.440 --> 01:36:42.640]   You know, now it's not me.
[01:36:42.640 --> 01:36:44.080]   You want to go to big companies.
[01:36:44.080 --> 01:36:44.960]   Not you, not you.
[01:36:44.960 --> 01:36:45.360]   The whole thing.
[01:36:45.360 --> 01:36:46.320]   We didn't try.
[01:36:46.320 --> 01:36:48.720]   You're the old lady who won't sell her property.
[01:36:48.720 --> 01:36:51.600]   No, I would if they had given me enough money.
[01:36:51.600 --> 01:36:54.320]   But not that we didn't try, but nobody wanted it.
[01:36:54.320 --> 01:36:58.800]   The difference now I think is the independent person.
[01:36:58.800 --> 01:37:00.000]   We have a new degree,
[01:37:00.000 --> 01:37:02.160]   we have a new rather educational program at the school.
[01:37:02.160 --> 01:37:04.400]   Our remade entrepreneurial programs for this.
[01:37:04.400 --> 01:37:06.800]   You're not going to be on one platform.
[01:37:06.800 --> 01:37:08.240]   You're an independent person.
[01:37:08.240 --> 01:37:09.200]   You're going to have a newsletter.
[01:37:09.200 --> 01:37:10.000]   Maybe it's on a sub-stack.
[01:37:10.000 --> 01:37:10.800]   You're going to have a blog.
[01:37:10.800 --> 01:37:11.600]   Maybe it's on medium.
[01:37:11.600 --> 01:37:13.040]   You're going to have videos.
[01:37:13.040 --> 01:37:14.080]   Maybe they're on YouTube.
[01:37:14.080 --> 01:37:14.560]   I've always said that.
[01:37:14.560 --> 01:37:20.400]   You should use as many different places, but different media as well.
[01:37:20.400 --> 01:37:20.720]   Right?
[01:37:20.720 --> 01:37:22.800]   Record, audio, video.
[01:37:22.800 --> 01:37:27.440]   Do as many because you want to reach people in whatever ways they're on.
[01:37:27.440 --> 01:37:29.920]   As somebody said in this exchange, I forget who it was.
[01:37:29.920 --> 01:37:30.480]   It's all trains.
[01:37:30.480 --> 01:37:33.120]   You're going to use Facebook to promote,
[01:37:33.120 --> 01:37:36.080]   but you're not going to put your whole business on Facebook.
[01:37:36.080 --> 01:37:39.040]   You're going to use these platforms rather than be used by them.
[01:37:39.040 --> 01:37:39.600]   Sorry, Ham.
[01:37:39.600 --> 01:37:42.400]   No, no, I agree 100%.
[01:37:42.400 --> 01:37:46.480]   It's just I just consider it being a jack of all trades and master of nothing.
[01:37:46.480 --> 01:37:51.280]   Just picking and choosing just enough to be able to get your message across.
[01:37:51.280 --> 01:37:53.520]   Like the way you pick your moral pedics.
[01:37:53.520 --> 01:37:58.240]   And still have your own place like your own domain that you run.
[01:37:58.240 --> 01:37:58.800]   That's usually-
[01:37:58.800 --> 01:37:59.920]   Well, I'll tell you.
[01:37:59.920 --> 01:38:01.200]   I'll give you a good example.
[01:38:01.200 --> 01:38:06.160]   Lisa wrote an article about how podcasting is changing in COVID.
[01:38:06.160 --> 01:38:10.000]   And she thought, "Well, I could put it on my blog, but why don't I put it on medium?"
[01:38:11.040 --> 01:38:11.840]   So she did.
[01:38:11.840 --> 01:38:14.880]   And as you know, if you use the word COVID,
[01:38:14.880 --> 01:38:17.760]   you're going to get a big banner on Medium that says,
[01:38:17.760 --> 01:38:20.480]   "Now, we don't vent any of this stuff, so don't believe anything you read.
[01:38:20.480 --> 01:38:21.200]   Okay, fine."
[01:38:21.200 --> 01:38:26.720]   And then for reasons we'll never understand, they suspended her account.
[01:38:26.720 --> 01:38:27.360]   One article.
[01:38:27.360 --> 01:38:27.680]   What?
[01:38:27.680 --> 01:38:27.920]   Yeah.
[01:38:27.920 --> 01:38:36.880]   And furthermore, without a warning, without any recourse, there was nobody to contact.
[01:38:36.880 --> 01:38:39.120]   She was- her account was down for 10 days.
[01:38:40.000 --> 01:38:40.160]   Mm.
[01:38:40.160 --> 01:38:43.200]   And then they sent her notes saying, "Oh, that was an accident.
[01:38:43.200 --> 01:38:44.640]   Sorry." And she's back up.
[01:38:44.640 --> 01:38:46.000]   So this is why-
[01:38:46.000 --> 01:38:47.760]   It's a good loss revenue in those 10 days.
[01:38:47.760 --> 01:38:50.480]   I don't like being on somebody else's platform.
[01:38:50.480 --> 01:38:51.520]   Right.
[01:38:51.520 --> 01:38:56.240]   So whatever happened to Jax Plan to do the open source version of Twitter,
[01:38:56.240 --> 01:38:57.280]   and that what was that called?
[01:38:57.280 --> 01:38:58.160]   What was that plan called?
[01:38:58.160 --> 01:39:01.280]   Do you remember where they brought people in?
[01:39:01.280 --> 01:39:01.600]   Yeah.
[01:39:01.600 --> 01:39:02.240]   So they were going to-
[01:39:02.240 --> 01:39:03.440]   Well, that wasn't that long ago.
[01:39:03.440 --> 01:39:04.400]   We didn't know about either.
[01:39:04.400 --> 01:39:05.280]   They were still doing it.
[01:39:05.280 --> 01:39:05.600]   Yeah.
[01:39:05.600 --> 01:39:06.160]   What's it called?
[01:39:06.160 --> 01:39:08.240]   Like a federated Twitter or something?
[01:39:08.240 --> 01:39:08.880]   Yeah.
[01:39:08.880 --> 01:39:09.520]   What was that called?
[01:39:09.520 --> 01:39:10.640]   What?
[01:39:10.640 --> 01:39:12.000]   Twitter.
[01:39:12.000 --> 01:39:13.040]   So SS?
[01:39:13.040 --> 01:39:14.720]   I don't know.
[01:39:14.720 --> 01:39:15.760]   Who cares what it was called?
[01:39:15.760 --> 01:39:17.840]   I care about it because I want to write about it.
[01:39:17.840 --> 01:39:18.400]   Oh.
[01:39:18.400 --> 01:39:19.360]   You're doing my homework for me.
[01:39:19.360 --> 01:39:21.280]   You basically want us to do your research.
[01:39:21.280 --> 01:39:21.760]   Yeah, I do.
[01:39:21.760 --> 01:39:22.080]   I do.
[01:39:22.080 --> 01:39:22.560]   Exactly.
[01:39:22.560 --> 01:39:23.040]   Okay.
[01:39:23.040 --> 01:39:24.640]   Yeah, you're failing me.
[01:39:24.640 --> 01:39:26.000]   Well, no, wait a minute.
[01:39:26.000 --> 01:39:28.480]   Let me just Google that, and I'm sure I could find it.
[01:39:28.480 --> 01:39:29.600]   Chat room, you got it.
[01:39:29.600 --> 01:39:30.640]   Chat room won't fail me.
[01:39:30.640 --> 01:39:31.200]   Here it is.
[01:39:31.200 --> 01:39:32.480]   Open source is happening.
[01:39:32.480 --> 01:39:33.760]   Twitter, OSS.
[01:39:33.760 --> 01:39:35.200]   Our philosophy-
[01:39:35.200 --> 01:39:35.680]   Twitter, OSS.
[01:39:35.680 --> 01:39:36.880]   Light and love.
[01:39:36.880 --> 01:39:40.720]   I don't know if that's, I don't know who that is.
[01:39:40.720 --> 01:39:41.200]   I thought it.
[01:39:41.200 --> 01:39:42.560]   Projects.
[01:39:42.560 --> 01:39:43.680]   Pan.
[01:39:43.680 --> 01:39:44.800]   It's called pants.
[01:39:44.800 --> 01:39:46.800]   It's called pants.
[01:39:46.800 --> 01:39:49.520]   Please write your article about pants.
[01:39:49.520 --> 01:39:52.080]   If it's not pants, it's bijection or finagle.
[01:39:52.080 --> 01:39:53.440]   No.
[01:39:53.440 --> 01:39:53.920]   No, it's not.
[01:39:53.920 --> 01:39:55.840]   No, it's finatra.
[01:39:55.840 --> 01:39:56.960]   Dodo.
[01:39:56.960 --> 01:39:58.480]   Rizoulis.
[01:39:58.480 --> 01:39:59.520]   Wow.
[01:39:59.520 --> 01:40:00.880]   Common.
[01:40:00.880 --> 01:40:03.040]   Rabbit hole.
[01:40:03.040 --> 01:40:04.000]   Rabbit hole.
[01:40:04.000 --> 01:40:04.880]   Rabbit.
[01:40:04.880 --> 01:40:05.680]   Tweemoji.
[01:40:05.680 --> 01:40:08.080]   Hogan.
[01:40:08.080 --> 01:40:10.720]   They got a few of these.
[01:40:10.720 --> 01:40:12.560]   Ambrose.
[01:40:12.560 --> 01:40:13.280]   It's called.
[01:40:13.280 --> 01:40:14.400]   I know what you're talking about.
[01:40:14.400 --> 01:40:15.680]   Yeah.
[01:40:15.680 --> 01:40:16.800]   Yeah, I know what you're talking about.
[01:40:16.800 --> 01:40:18.400]   Chat room, are you going to rescue me here?
[01:40:18.400 --> 01:40:19.520]   No, they're coming up with worse.
[01:40:19.520 --> 01:40:20.800]   They some mastodon.
[01:40:20.800 --> 01:40:22.080]   No.
[01:40:22.080 --> 01:40:22.400]   No.
[01:40:22.400 --> 01:40:23.760]   Geo cities.
[01:40:23.760 --> 01:40:29.840]   Goala, path, peach,
[01:40:29.840 --> 01:40:32.080]   oink, any of those.
[01:40:32.080 --> 01:40:33.600]   Any of those ring a bell?
[01:40:33.600 --> 01:40:34.400]   My space.
[01:40:34.400 --> 01:40:35.520]   People, people, people.
[01:40:35.520 --> 01:40:36.560]   You're not helping me here.
[01:40:36.560 --> 01:40:38.000]   Who does Twitter plus?
[01:40:38.000 --> 01:40:39.600]   No.
[01:40:39.600 --> 01:40:40.320]   No.
[01:40:40.320 --> 01:40:41.920]   No, no, no.
[01:40:41.920 --> 01:40:43.280]   Anyway, I just,
[01:40:43.280 --> 01:40:44.960]   but I've always said that.
[01:40:44.960 --> 01:40:47.680]   That's why we've been very reluctant when Facebook Live came out.
[01:40:47.680 --> 01:40:49.520]   People said, oh, you should stream on Facebook Live.
[01:40:49.520 --> 01:40:51.440]   I've been very reluctant.
[01:40:51.440 --> 01:40:51.920]   We are.
[01:40:51.920 --> 01:40:52.480]   I knew that.
[01:40:52.480 --> 01:40:53.280]   You're right.
[01:40:53.280 --> 01:40:53.760]   You're right.
[01:40:53.760 --> 01:40:54.640]   I knew what you're short.
[01:40:54.640 --> 01:41:00.320]   We are on YouTube, but it's not important to our revenue.
[01:41:00.320 --> 01:41:01.200]   And it's not important.
[01:41:01.200 --> 01:41:02.800]   I don't think it's, I, you know,
[01:41:02.800 --> 01:41:04.480]   some of this, it's out of my hands.
[01:41:04.480 --> 01:41:06.400]   You know, I have a marketing department now, which
[01:41:06.400 --> 01:41:10.000]   I'm not a big marketer, as you know, but,
[01:41:10.000 --> 01:41:13.920]   you know, I don't, I think the best thing is what we do,
[01:41:13.920 --> 01:41:18.160]   which is you have an RSS feed and people subscribe to it.
[01:41:18.160 --> 01:41:19.440]   It's called the podcast.
[01:41:19.440 --> 01:41:25.600]   And I'm riding that all the way down to the very bitter end.
[01:41:25.600 --> 01:41:27.280]   I'm just going to stay, say right now.
[01:41:27.280 --> 01:41:28.800]   Was it Friendster?
[01:41:28.800 --> 01:41:29.760]   No.
[01:41:29.760 --> 01:41:30.240]   No.
[01:41:30.240 --> 01:41:31.280]   No.
[01:41:31.280 --> 01:41:31.840]   No.
[01:41:31.840 --> 01:41:32.400]   No.
[01:41:32.400 --> 01:41:34.080]   No damn help at all.
[01:41:34.080 --> 01:41:34.400]   Wait a minute.
[01:41:34.400 --> 01:41:35.360]   No, no, we got an article.
[01:41:35.360 --> 01:41:36.080]   We got an article.
[01:41:36.080 --> 01:41:38.080]   This is from Carsten.
[01:41:38.080 --> 01:41:42.320]   Jack Dorsey wants a decentralized Twitter.
[01:41:42.320 --> 01:41:43.120]   What is it?
[01:41:43.120 --> 01:41:44.560]   Carsten, bless you, my son.
[01:41:44.560 --> 01:41:47.280]   Carsten, he should hire him as your research assistant.
[01:41:47.280 --> 01:41:49.440]   I have to give it a pop.
[01:41:49.440 --> 01:41:50.400]   No, no, no, no.
[01:41:50.400 --> 01:41:51.120]   Mastodon.
[01:41:51.120 --> 01:41:54.320]   No, no, no, no, no, no.
[01:41:54.320 --> 01:41:57.360]   It should have started a group working on us.
[01:41:57.360 --> 01:41:58.160]   Yes, I remember.
[01:41:58.160 --> 01:42:02.800]   And they were the reason we found out is they had a listing for jobs.
[01:42:02.800 --> 01:42:04.880]   Yeah, it may not have a name.
[01:42:04.880 --> 01:42:06.720]   No, it did.
[01:42:06.720 --> 01:42:07.120]   It did.
[01:42:07.120 --> 01:42:07.760]   I know it did.
[01:42:07.760 --> 01:42:11.680]   The only thing comes in my mind is Aloe or something like that.
[01:42:11.680 --> 01:42:12.560]   Yeah, that sounds familiar.
[01:42:12.560 --> 01:42:14.080]   Avalon, something like that.
[01:42:14.080 --> 01:42:14.400]   Yeah.
[01:42:14.400 --> 01:42:16.000]   That sounds familiar.
[01:42:16.000 --> 01:42:18.320]   Well, all right.
[01:42:19.280 --> 01:42:20.000]   Afterlice.
[01:42:20.000 --> 01:42:21.600]   First and foremost, you did better than the rest of the stuff.
[01:42:21.600 --> 01:42:23.680]   Aloe.
[01:42:23.680 --> 01:42:25.440]   That's a nice ball independent team.
[01:42:25.440 --> 01:42:26.560]   Yeah, all that, right?
[01:42:26.560 --> 01:42:30.480]   Hey, Quibi.
[01:42:30.480 --> 01:42:31.360]   What about Quibi?
[01:42:31.360 --> 01:42:32.160]   That would be a good name.
[01:42:32.160 --> 01:42:33.120]   Oh, it's so funny.
[01:42:33.120 --> 01:42:34.240]   That's so freaking funny.
[01:42:34.240 --> 01:42:37.200]   So apparently, Quibi, well, you're just, that's called
[01:42:37.200 --> 01:42:39.360]   Schaden throw it as I remember.
[01:42:39.360 --> 01:42:40.080]   Yes.
[01:42:40.080 --> 01:42:45.040]   Quibi, which was, of course, the highly, almost $2 billion
[01:42:46.080 --> 01:42:51.760]   financed attempt to take Hollywood to take on YouTube from Hollywood
[01:42:51.760 --> 01:42:54.640]   with Jeffrey Katzenberg and Meg Whitten at the helm.
[01:42:54.640 --> 01:42:59.280]   They released it with some misgivings during the pandemic.
[01:42:59.280 --> 01:43:02.800]   Turns out their misgivings were well guided after the three month
[01:43:02.800 --> 01:43:05.280]   free trial, pretty much everybody left.
[01:43:05.280 --> 01:43:09.520]   And now they are apparently shopping themselves after six months,
[01:43:09.520 --> 01:43:12.000]   trying to find a buyer.
[01:43:12.000 --> 01:43:15.440]   Been there, done that, Quibi.
[01:43:15.440 --> 01:43:16.240]   Good luck to you.
[01:43:16.240 --> 01:43:22.800]   Maybe Spotify, you know, do you ever think of Spotify?
[01:43:22.800 --> 01:43:24.880]   They know who, what's to buy there?
[01:43:24.880 --> 01:43:26.160]   What the heck is to buy there?
[01:43:26.160 --> 01:43:29.360]   That great sideways phone technology.
[01:43:29.360 --> 01:43:33.200]   Watch it wide or watch it tall.
[01:43:33.200 --> 01:43:33.920]   Doesn't matter.
[01:43:33.920 --> 01:43:35.120]   I don't know.
[01:43:35.120 --> 01:43:36.000]   What is the buy there?
[01:43:36.000 --> 01:43:40.960]   They also say they're looking for other options.
[01:43:40.960 --> 01:43:44.400]   Maybe the buyer, whatever, strategic options.
[01:43:45.040 --> 01:43:47.760]   Wow.
[01:43:47.760 --> 01:43:50.720]   They got a lot of stars.
[01:43:50.720 --> 01:43:54.320]   Chrissy Teigen didn't really turn her.
[01:43:54.320 --> 01:43:59.040]   What it does, that tells you that it didn't go that well, right?
[01:43:59.040 --> 01:44:00.960]   Right, right, right.
[01:44:00.960 --> 01:44:03.680]   Right, because if you go to the 2001 crash,
[01:44:03.680 --> 01:44:04.960]   was we're going to buy audience.
[01:44:04.960 --> 01:44:06.080]   Right.
[01:44:06.080 --> 01:44:10.480]   The use of investors' money since then is we're going to buy stars.
[01:44:10.480 --> 01:44:10.720]   Right.
[01:44:11.440 --> 01:44:13.520]   Not a Vida way, which talks a lot.
[01:44:13.520 --> 01:44:16.000]   Because PewDiePie is not for sale, my friends.
[01:44:16.000 --> 01:44:18.720]   That's the thing that drives Hollywood crazy.
[01:44:18.720 --> 01:44:20.320]   What's with the PewDiePie guy?
[01:44:20.320 --> 01:44:22.240]   How come he's so big?
[01:44:22.240 --> 01:44:23.440]   Yeah.
[01:44:23.440 --> 01:44:25.040]   And how can we duplicate that?
[01:44:25.040 --> 01:44:26.080]   We need our PewDiePie.
[01:44:26.080 --> 01:44:26.800]   Where's our PewDiePie?
[01:44:26.800 --> 01:44:28.320]   Where's our PewDiePie?
[01:44:28.320 --> 01:44:29.840]   He doesn't drive Hollywood crazy.
[01:44:29.840 --> 01:44:31.120]   It drives me crazy.
[01:44:31.120 --> 01:44:32.240]   Well, yeah.
[01:44:32.240 --> 01:44:34.880]   He just, nah, I should say that.
[01:44:34.880 --> 01:44:37.760]   But I remember thinking this is a guy playing games.
[01:44:37.760 --> 01:44:39.360]   He's just swearing into the mic.
[01:44:39.360 --> 01:44:39.680]   Right.
[01:44:39.680 --> 01:44:40.960]   And people love it.
[01:44:40.960 --> 01:44:41.200]   Right.
[01:44:41.200 --> 01:44:44.000]   Well, what made Kim Kardashian famous?
[01:44:44.000 --> 01:44:45.920]   I mean, I don't really say--
[01:44:45.920 --> 01:44:46.560]   I don't really say--
[01:44:46.560 --> 01:44:50.560]   21 seasons of keeping up with the Kardashians.
[01:44:50.560 --> 01:44:51.360]   A sex tape.
[01:44:51.360 --> 01:44:52.160]   Yeah.
[01:44:52.160 --> 01:44:57.360]   Yeah, but that wore off after a few months, a few minutes.
[01:44:57.360 --> 01:45:00.400]   Leo, in our day, we don't have Stacy here to make fun of us for age,
[01:45:00.400 --> 01:45:02.480]   but we don't, and can do the job, can't we?
[01:45:02.480 --> 01:45:03.360]   You can't remember.
[01:45:03.360 --> 01:45:07.440]   In our days, remember these game show contestants,
[01:45:07.440 --> 01:45:09.040]   these celebrity game show contestants,
[01:45:09.040 --> 01:45:10.240]   and they were famous for nothing.
[01:45:10.240 --> 01:45:11.120]   Kitty Carlisle.
[01:45:11.120 --> 01:45:11.600]   The beat Carlisle.
[01:45:11.600 --> 01:45:11.760]   Yeah.
[01:45:11.760 --> 01:45:12.560]   The beat Carlisle.
[01:45:12.560 --> 01:45:12.720]   Yeah.
[01:45:12.720 --> 01:45:16.080]   Kitty Carl-- there were a lot of them on all the game shows.
[01:45:16.080 --> 01:45:17.760]   They were famous for being famous.
[01:45:17.760 --> 01:45:18.720]   Yeah.
[01:45:18.720 --> 01:45:19.840]   We've always gone to line line.
[01:45:19.840 --> 01:45:22.160]   But then, did you read last week in The New York Times,
[01:45:22.160 --> 01:45:25.280]   the long piece on Paris Hilton, who is--
[01:45:25.280 --> 01:45:25.760]   No.
[01:45:25.760 --> 01:45:27.600]   She invented famous for being famous.
[01:45:27.600 --> 01:45:29.600]   In fact, Kim Kardashian was her assistant.
[01:45:29.600 --> 01:45:30.400]   Was her buddy?
[01:45:30.400 --> 01:45:30.800]   Yeah.
[01:45:30.800 --> 01:45:31.760]   Body assistant.
[01:45:31.760 --> 01:45:34.960]   And it-- but then it turns out,
[01:45:34.960 --> 01:45:37.920]   maybe Paris Hilton does work for a living,
[01:45:37.920 --> 01:45:41.200]   because she's got 93 cosmetic brands lines.
[01:45:41.200 --> 01:45:48.720]   She DJ's 250 events a year, and get this.
[01:45:48.720 --> 01:45:51.440]   Gets paid $1 million per event.
[01:45:51.440 --> 01:45:52.800]   Oh, F me.
[01:45:52.800 --> 01:45:53.760]   Oh, just F me.
[01:45:53.760 --> 01:45:55.600]   There is no justice in life.
[01:45:55.600 --> 01:45:57.600]   But she makes a quarter of a billion dollars a year.
[01:45:57.600 --> 01:45:59.360]   She's not just tweeting.
[01:45:59.360 --> 01:46:00.000]   DJing.
[01:46:00.000 --> 01:46:03.280]   So she-- I mean, you could say she's famous for nothing,
[01:46:03.280 --> 01:46:05.840]   but she-- or a sex tape or whatever.
[01:46:05.840 --> 01:46:09.360]   But in fact, she works her butt off.
[01:46:09.360 --> 01:46:10.400]   Yeah, at least she's working.
[01:46:10.400 --> 01:46:13.920]   It's actually-- the reason for the article was that
[01:46:13.920 --> 01:46:18.400]   YouTube has a documentary called This Is Paris,
[01:46:18.400 --> 01:46:22.640]   which I'm sure Edward Armiro is just rolling this grave.
[01:46:22.640 --> 01:46:26.080]   I believe it mentions her
[01:46:26.080 --> 01:46:30.080]   dealing with some abuse from her parents and things like that.
[01:46:30.080 --> 01:46:30.320]   Yeah.
[01:46:30.320 --> 01:46:31.520]   So this is the thing about--
[01:46:31.520 --> 01:46:31.920]   It's dark.
[01:46:31.920 --> 01:46:33.280]   This is-- this is-- well,
[01:46:34.800 --> 01:46:37.840]   you got to remember a lot of this is about, you know,
[01:46:37.840 --> 01:46:39.440]   cultivating the brand and everything.
[01:46:39.440 --> 01:46:40.800]   I'm not going to be cynical.
[01:46:40.800 --> 01:46:45.840]   But what she revealed is that she was sent off
[01:46:45.840 --> 01:46:54.000]   to one of those kind of journals, the Linquent Scootchoo
[01:46:54.000 --> 01:46:54.560]   boarding schools.
[01:46:54.560 --> 01:46:56.560]   Yeah, it's not just a regular boarding school.
[01:46:56.560 --> 01:46:58.160]   It's-- you know, they kidnap you,
[01:46:58.160 --> 01:47:00.240]   and they realign you and stuff.
[01:47:00.240 --> 01:47:02.640]   And she said she was actually pretty horribly abused.
[01:47:03.200 --> 01:47:04.560]   So I feel bad for her.
[01:47:04.560 --> 01:47:11.360]   But anyway, she was the original, wasn't she, Paris Hilton?
[01:47:11.360 --> 01:47:14.240]   Well, Arlene Francis in our day.
[01:47:14.240 --> 01:47:18.480]   Arlene Francis, Kitty Carlisle was like a socialite, right?
[01:47:18.480 --> 01:47:19.280]   I think that was her.
[01:47:19.280 --> 01:47:19.920]   Yeah, I think so.
[01:47:19.920 --> 01:47:20.160]   Yeah.
[01:47:20.160 --> 01:47:22.800]   She was married to Moss Hart, so that's not--
[01:47:22.800 --> 01:47:26.160]   she was like famous by association, right?
[01:47:26.160 --> 01:47:30.160]   Her grandfather was the mayor of Shreveport.
[01:47:30.560 --> 01:47:34.080]   [LAUGHTER]
[01:47:34.080 --> 01:47:34.720]   So there.
[01:47:34.720 --> 01:47:40.800]   I was at a tech conference in Armenia last year.
[01:47:40.800 --> 01:47:46.640]   And it was, you know, there was pretty technical stuff going on.
[01:47:46.640 --> 01:47:49.440]   And-- but Kim Kardashian came and spoke, actually.
[01:47:49.440 --> 01:47:52.160]   And it was standing remotely.
[01:47:52.160 --> 01:47:56.320]   I couldn't believe it was just, you know, at a tech conference.
[01:47:56.320 --> 01:47:59.920]   And so-- and I had a grudging respect for her.
[01:47:59.920 --> 01:48:03.760]   Did she talk about the latest Intel chip?
[01:48:03.760 --> 01:48:10.240]   No, well, the punchline is I had a grudging respect for her
[01:48:10.240 --> 01:48:12.720]   based on just figured, you know, she's an entrepreneur,
[01:48:12.720 --> 01:48:13.920]   she's involved with the game somehow.
[01:48:13.920 --> 01:48:15.440]   I didn't really pay that much attention.
[01:48:15.440 --> 01:48:17.920]   But once I heard her spoke, I lost all respect.
[01:48:17.920 --> 01:48:19.040]   So--
[01:48:19.040 --> 01:48:20.080]   Really?
[01:48:20.080 --> 01:48:21.120]   Somebody is reminding you--
[01:48:21.120 --> 01:48:21.840]   Yeah, she's here.
[01:48:21.840 --> 01:48:24.640]   I just couldn't-- I mean, she came across like--
[01:48:25.440 --> 01:48:27.280]   I mean, she had nothing to say.
[01:48:27.280 --> 01:48:28.480]   I was a little shocked.
[01:48:28.480 --> 01:48:30.640]   I know, here recently, she is--
[01:48:30.640 --> 01:48:34.080]   I want to say she's passed the bar and has done--
[01:48:34.080 --> 01:48:34.480]   What?
[01:48:34.480 --> 01:48:38.080]   --some things with civil justice here in California.
[01:48:38.080 --> 01:48:38.400]   Yeah.
[01:48:38.400 --> 01:48:39.600]   Kim Kardashian.
[01:48:39.600 --> 01:48:41.920]   Well, her father, of course, is famous for being a--
[01:48:41.920 --> 01:48:44.240]   OJ's attorney.
[01:48:44.240 --> 01:48:44.560]   But--
[01:48:44.560 --> 01:48:46.480]   What?
[01:48:46.480 --> 01:48:46.960]   Wow.
[01:48:46.960 --> 01:48:51.120]   But yeah, here recently, she spent on the path of trying to fight
[01:48:51.120 --> 01:48:54.800]   civil injustice for some people here in the state of California.
[01:48:54.800 --> 01:48:56.640]   I don't have the names in particular.
[01:48:56.640 --> 01:48:57.680]   Well, good for her.
[01:48:57.680 --> 01:48:59.840]   But yeah, she's been trying to fight the good fight.
[01:48:59.840 --> 01:49:03.600]   And even, you know, a lot of people talked about her canceling,
[01:49:03.600 --> 01:49:08.320]   or not canceling, but taking Kanye out of the limelight.
[01:49:08.320 --> 01:49:09.120]   I hope she does.
[01:49:09.120 --> 01:49:10.000]   --on the television show.
[01:49:10.000 --> 01:49:10.720]   Yeah, because--
[01:49:10.720 --> 01:49:12.640]   You know, but it's just a smart thing to do,
[01:49:12.640 --> 01:49:16.080]   because she's not wanting to make light of mental health awareness
[01:49:16.080 --> 01:49:16.880]   and things like that.
[01:49:16.880 --> 01:49:19.120]   She's growing.
[01:49:19.120 --> 01:49:19.440]   Good.
[01:49:19.440 --> 01:49:23.360]   And of course, we have to mention, since we're talking about Kim Kardashian,
[01:49:23.360 --> 01:49:25.360]   that they just canceled the Kardashian--
[01:49:25.360 --> 01:49:27.840]   That's what I brought up after 20 years.
[01:49:27.840 --> 01:49:29.680]   Yeah, and 20 years.
[01:49:29.680 --> 01:49:30.160]   Exactly.
[01:49:30.160 --> 01:49:30.560]   Exactly.
[01:49:30.560 --> 01:49:30.960]   Actually--
[01:49:30.960 --> 01:49:31.440]   20 years.
[01:49:31.440 --> 01:49:33.440]   And, you know, so that's what fooled me.
[01:49:33.440 --> 01:49:36.000]   They said 21 seasons, I thought, my god.
[01:49:36.000 --> 01:49:36.640]   [LAUGHTER]
[01:49:36.640 --> 01:49:38.400]   But it was like, '28, right?
[01:49:38.400 --> 01:49:38.880]   Yeah.
[01:49:38.880 --> 01:49:39.280]   Yeah.
[01:49:39.280 --> 01:49:40.480]   It wasn't 21 years.
[01:49:40.480 --> 01:49:41.360]   It was 21 seasons.
[01:49:41.360 --> 01:49:43.840]   It was over only, like, 14 years or something like that.
[01:49:43.840 --> 01:49:45.280]   But here's the question.
[01:49:45.280 --> 01:49:48.800]   I think that I think that is being raised by this story.
[01:49:48.800 --> 01:49:53.280]   And some of the other things we've been talking about,
[01:49:53.280 --> 01:49:55.760]   is celebrity over?
[01:49:55.760 --> 01:49:57.440]   Are people fed up with celebrities?
[01:49:57.440 --> 01:50:02.400]   The Paris Hilton doc seems like a very savvy move
[01:50:02.400 --> 01:50:06.560]   to humanize somebody who was just worshiped for being,
[01:50:06.560 --> 01:50:08.720]   you know, a fabulous personality?
[01:50:08.720 --> 01:50:10.720]   Are we just done with celebrities?
[01:50:10.720 --> 01:50:14.800]   Is it impossible now in the culture to have a popular show
[01:50:14.800 --> 01:50:16.160]   about celebrities' lives?
[01:50:16.160 --> 01:50:17.440]   I don't think so.
[01:50:17.440 --> 01:50:19.680]   Well, one of the years long, we still have TikTok.
[01:50:19.680 --> 01:50:23.040]   And as long as we still have Instagram stories and things
[01:50:23.040 --> 01:50:25.040]   like that, there's going to be a lot of people.
[01:50:25.040 --> 01:50:28.160]   TikTok's example of how people who are not celebrities
[01:50:28.160 --> 01:50:29.440]   are getting all the attention.
[01:50:29.440 --> 01:50:31.440]   You know who deserves the attention?
[01:50:31.440 --> 01:50:33.280]   But they do, in turn, become some sort of a celebrity.
[01:50:33.280 --> 01:50:33.840]   That's how they--
[01:50:33.840 --> 01:50:34.320]   That's right.
[01:50:34.320 --> 01:50:35.680]   That's your bit that into other brands, right?
[01:50:35.680 --> 01:50:36.720]   That's right.
[01:50:36.720 --> 01:50:39.440]   But can any of us name a single TikTok celebrity
[01:50:39.440 --> 01:50:42.960]   who became famous like, we can name a hundred people
[01:50:42.960 --> 01:50:43.760]   who just did it on YouTube?
[01:50:43.760 --> 01:50:44.720]   Yeah, I can.
[01:50:44.720 --> 01:50:46.480]   You ever heard of a little Nas X?
[01:50:46.480 --> 01:50:47.520]   No.
[01:50:47.520 --> 01:50:49.600]   What?
[01:50:50.240 --> 01:50:53.760]   I can't name Mr. Elgin because I don't get on TikTok.
[01:50:53.760 --> 01:50:54.960]   That's the only reason why.
[01:50:54.960 --> 01:50:56.480]   But, yeah, it could be.
[01:50:56.480 --> 01:50:58.080]   But if it's like you heard of Justin Bieber
[01:50:58.080 --> 01:51:01.600]   and all kinds of people who came up on YouTube, for example,
[01:51:01.600 --> 01:51:06.640]   where the celebrity manufacturing process resulted in
[01:51:06.640 --> 01:51:08.720]   major household names.
[01:51:08.720 --> 01:51:10.000]   Excuse me.
[01:51:10.000 --> 01:51:13.280]   I'm going to once again say, little Nas X.
[01:51:13.280 --> 01:51:18.640]   He had the longest running number one hit, music hit,
[01:51:18.640 --> 01:51:21.360]   in history beating the Beatles, Stevie Wonder,
[01:51:21.360 --> 01:51:24.080]   and Michael Jackson in 2019.
[01:51:24.080 --> 01:51:25.840]   Remember Old Town Road?
[01:51:25.840 --> 01:51:26.560]   I have to--
[01:51:26.560 --> 01:51:29.200]   If I play it, I'll get taken down, so I'm not going to play it.
[01:51:29.200 --> 01:51:29.840]   Yeah, I can't play it.
[01:51:29.840 --> 01:51:31.360]   Don't listen to it.
[01:51:31.360 --> 01:51:32.800]   Oh, I do remember that.
[01:51:32.800 --> 01:51:34.320]   Of course, you remember this.
[01:51:34.320 --> 01:51:36.800]   It was really an interesting song
[01:51:36.800 --> 01:51:39.520]   because it was kind of country, kind of rap.
[01:51:39.520 --> 01:51:42.080]   Yeah, he caught a lot of heat.
[01:51:42.080 --> 01:51:42.400]   Yeah.
[01:51:42.400 --> 01:51:47.120]   And, by the way, he became famous on TikTok.
[01:51:47.120 --> 01:51:51.440]   It was people on TikTok not only watching the video,
[01:51:51.440 --> 01:51:53.040]   but especially not watching the video,
[01:51:53.040 --> 01:51:55.600]   but hearing the song as others reused it,
[01:51:55.600 --> 01:51:58.560]   that made Old Town Road a big hit.
[01:51:58.560 --> 01:52:00.400]   So that's somebody.
[01:52:00.400 --> 01:52:01.680]   There he is.
[01:52:01.680 --> 01:52:02.160]   That's true.
[01:52:02.160 --> 01:52:02.960]   Little Nas X.
[01:52:02.960 --> 01:52:04.640]   I didn't realize it was TikTok.
[01:52:04.640 --> 01:52:05.920]   TikTok is the origin though.
[01:52:05.920 --> 01:52:06.080]   Yeah.
[01:52:06.080 --> 01:52:06.720]   Didn't know that.
[01:52:06.720 --> 01:52:07.360]   Yeah.
[01:52:07.360 --> 01:52:08.400]   Isn't that cool?
[01:52:08.400 --> 01:52:09.840]   I think actually it's a great story.
[01:52:09.840 --> 01:52:12.400]   So--
[01:52:12.400 --> 01:52:15.760]   Mike, to your point, there was a neat post I put up today
[01:52:16.320 --> 01:52:18.080]   about where the hell they go now.
[01:52:18.080 --> 01:52:18.640]   Jesus.
[01:52:18.640 --> 01:52:23.840]   Generative entertainment.
[01:52:23.840 --> 01:52:29.760]   That you're going to get so good at making fake stars,
[01:52:29.760 --> 01:52:30.400]   fake people.
[01:52:30.400 --> 01:52:31.040]   It doesn't matter.
[01:52:31.040 --> 01:52:31.520]   Yep.
[01:52:31.520 --> 01:52:34.400]   It's working for therapy now.
[01:52:34.400 --> 01:52:35.440]   It's going to work for everything else.
[01:52:35.440 --> 01:52:38.080]   So you end up in a time of unlimited entertainment.
[01:52:38.080 --> 01:52:39.680]   Well, I think we're there.
[01:52:39.680 --> 01:52:40.400]   We're there, pretty much.
[01:52:40.400 --> 01:52:40.960]   I think we're there.
[01:52:40.960 --> 01:52:42.640]   Well, but I haven't been good for it.
[01:52:42.640 --> 01:52:45.120]   TikTok made Sarah Cooper famous.
[01:52:45.920 --> 01:52:46.720]   Oh, yeah.
[01:52:46.720 --> 01:52:46.880]   Yeah.
[01:52:46.880 --> 01:52:48.800]   Yes.
[01:52:48.800 --> 01:52:50.240]   Right.
[01:52:50.240 --> 01:52:52.400]   Is she a celebrity?
[01:52:52.400 --> 01:52:53.040]   I guess she is.
[01:52:53.040 --> 01:52:53.360]   Yes.
[01:52:53.360 --> 01:52:54.320]   She posted that.
[01:52:54.320 --> 01:52:54.720]   Oh, yeah.
[01:52:54.720 --> 01:52:55.040]   Oh, yeah.
[01:52:55.040 --> 01:52:55.520]   Oh, yeah.
[01:52:55.520 --> 01:52:57.280]   She's famous, but yeah.
[01:52:57.280 --> 01:52:58.080]   I mean--
[01:52:58.080 --> 01:52:58.640]   Jesus.
[01:52:58.640 --> 01:53:01.360]   Anyway, so I think what Jeff's talking about
[01:53:01.360 --> 01:53:02.640]   is the use of--
[01:53:02.640 --> 01:53:03.680]   correct me if I'm wrong--
[01:53:03.680 --> 01:53:05.280]   the use of artificial intelligence
[01:53:05.280 --> 01:53:08.880]   to create not only entertainment
[01:53:08.880 --> 01:53:12.160]   and articles and things like that
[01:53:12.160 --> 01:53:14.320]   at a just colossal scale.
[01:53:14.320 --> 01:53:14.720]   Just--
[01:53:14.720 --> 01:53:15.680]   Yes, exactly.
[01:53:15.680 --> 01:53:16.320]   That's--
[01:53:16.320 --> 01:53:16.960]   Thousands per minute.
[01:53:16.960 --> 01:53:21.600]   And so we may be having the same conversation,
[01:53:21.600 --> 01:53:24.080]   but instead of wondering whether celebrities
[01:53:24.080 --> 01:53:25.920]   still have the same juice they used to have,
[01:53:25.920 --> 01:53:26.880]   whether people do.
[01:53:26.880 --> 01:53:30.160]   Because I think there's something in human nature,
[01:53:30.160 --> 01:53:32.000]   though, that we like celebrity.
[01:53:32.000 --> 01:53:33.280]   We want to have celebrities.
[01:53:33.280 --> 01:53:35.440]   It makes us feel connected in some way.
[01:53:35.440 --> 01:53:36.880]   Or maybe media.
[01:53:36.880 --> 01:53:38.640]   I think that's an echo chamber.
[01:53:38.640 --> 01:53:40.240]   Media is convinced you of that.
[01:53:40.240 --> 01:53:40.560]   Really?
[01:53:40.560 --> 01:53:43.920]   Yeah, it's the media structure.
[01:53:43.920 --> 01:53:45.840]   I mean, I think you could make a strong argument
[01:53:45.840 --> 01:53:49.200]   that the president is a president because of the apprentice.
[01:53:49.200 --> 01:53:49.760]   Oh, yeah.
[01:53:49.760 --> 01:53:51.680]   But I think you could make a strong argument
[01:53:51.680 --> 01:53:53.840]   that Joe Rogan is the most successful podcast
[01:53:53.840 --> 01:53:54.240]   of all time.
[01:53:54.240 --> 01:53:55.040]   That doesn't mean that we're meant to like
[01:53:55.040 --> 01:53:56.240]   at the end of the line.
[01:53:56.240 --> 01:53:58.960]   Because of the man show and the fear factor.
[01:53:58.960 --> 01:54:00.000]   I think being on TV--
[01:54:00.000 --> 01:54:00.880]   Not man show.
[01:54:00.880 --> 01:54:01.760]   Not man show.
[01:54:01.760 --> 01:54:02.720]   He wasn't on the man show.
[01:54:02.720 --> 01:54:03.360]   Rogan was.
[01:54:03.360 --> 01:54:04.400]   Yes, I was.
[01:54:04.400 --> 01:54:05.760]   Corolla and Kimmel.
[01:54:05.760 --> 01:54:07.520]   And Rogan was on that, wasn't he?
[01:54:07.520 --> 01:54:09.200]   I don't know.
[01:54:09.200 --> 01:54:09.600]   I think so.
[01:54:09.600 --> 01:54:11.600]   I don't watch it, so I have no idea.
[01:54:11.600 --> 01:54:12.080]   I know.
[01:54:12.080 --> 01:54:13.280]   Well, Corolla and Kimmel,
[01:54:13.280 --> 01:54:15.200]   are famous because of the man show.
[01:54:15.200 --> 01:54:16.160]   So, yeah.
[01:54:16.160 --> 01:54:17.360]   I rest my case.
[01:54:17.360 --> 01:54:22.160]   So I think television has a remarkable connection
[01:54:22.160 --> 01:54:25.520]   to our brains, through our eyes.
[01:54:25.520 --> 01:54:27.040]   We think we know these people.
[01:54:27.040 --> 01:54:28.240]   We love these people.
[01:54:28.240 --> 01:54:30.880]   And I don't think that's going to go away.
[01:54:30.880 --> 01:54:31.840]   I don't think you could do art.
[01:54:31.840 --> 01:54:36.000]   But here's what I think we can all agree is changing.
[01:54:36.000 --> 01:54:39.760]   The number of celebrities that exist just keeps growing
[01:54:40.800 --> 01:54:41.520]   enormously.
[01:54:41.520 --> 01:54:47.040]   And the number of people to whom each celebrity can relate
[01:54:47.040 --> 01:54:49.840]   is shrinking.
[01:54:49.840 --> 01:54:51.760]   So you're talking about television.
[01:54:51.760 --> 01:54:53.360]   Yeah, television is mass medium.
[01:54:53.360 --> 01:54:56.000]   And so if you watch the man show back when there were
[01:54:56.000 --> 01:54:57.040]   slightly fewer channels,
[01:54:57.040 --> 01:55:00.560]   then you know Adam Corolla and Jimmy Kimmel.
[01:55:00.560 --> 01:55:02.000]   And now you follow the career--
[01:55:02.000 --> 01:55:03.120]   And I can't.
[01:55:03.120 --> 01:55:05.840]   Here is Joe Rogan on the freaking man show.
[01:55:05.840 --> 01:55:07.360]   OK?
[01:55:07.360 --> 01:55:09.760]   OK.
[01:55:10.720 --> 01:55:11.520]   You got me.
[01:55:11.520 --> 01:55:17.520]   So I'm just telling you, TV has a remarkable ability to create.
[01:55:17.520 --> 01:55:24.800]   I think it taps into the part of your brain that is about social
[01:55:24.800 --> 01:55:26.480]   and about friendliness.
[01:55:26.480 --> 01:55:29.680]   And there's something about it that radio doesn't do this as well.
[01:55:29.680 --> 01:55:33.200]   But your show does.
[01:55:33.200 --> 01:55:35.920]   This is the same dynamic exists with this show.
[01:55:35.920 --> 01:55:37.200]   And with your shows, Leo.
[01:55:38.000 --> 01:55:41.040]   A lot of people feel like they really know you.
[01:55:41.040 --> 01:55:43.200]   No, but you know why I'm successful, Mike?
[01:55:43.200 --> 01:55:47.840]   Because I was a minor celebrity and a little-known cable channel for six years.
[01:55:47.840 --> 01:55:51.360]   That's what this whole thing came out of.
[01:55:51.360 --> 01:55:54.880]   Was six years in a very minor cable channel called Tech TV.
[01:55:54.880 --> 01:55:57.920]   And I don't think I'd be here today if I didn't do that.
[01:55:57.920 --> 01:55:59.760]   So I'm not sure I agree with you, Mike.
[01:55:59.760 --> 01:56:02.800]   Well, you picked up a lot of audience and brought them with you for sure.
[01:56:02.800 --> 01:56:08.800]   But the video medium does have that powerful effect.
[01:56:08.800 --> 01:56:12.160]   My point is that the number of people who were in front of video
[01:56:12.160 --> 01:56:15.440]   is growing exponentially.
[01:56:15.440 --> 01:56:19.120]   And the number of people who tune in to each particular video personality
[01:56:19.120 --> 01:56:20.800]   is getting smaller.
[01:56:20.800 --> 01:56:23.120]   So we'll have celebrity, so to speak.
[01:56:23.120 --> 01:56:23.680]   No, yes.
[01:56:23.680 --> 01:56:27.040]   But I think the audiences will get smaller and the numbers will get greater.
[01:56:27.040 --> 01:56:28.160]   It'll be sliced.
[01:56:28.160 --> 01:56:35.760]   So, but I would submit that most people will end up having five people that they really
[01:56:35.760 --> 01:56:38.880]   zoom into and connect with.
[01:56:38.880 --> 01:56:42.800]   But it's just there's going to be different five people for everybody, right?
[01:56:42.800 --> 01:56:43.600]   Exactly.
[01:56:43.600 --> 01:56:44.160]   Yeah.
[01:56:44.160 --> 01:56:44.160]   Exactly.
[01:56:44.160 --> 01:56:44.640]   Yeah.
[01:56:44.640 --> 01:56:47.040]   So you won't have the mass celebrity, perhaps.
[01:56:47.040 --> 01:56:50.960]   Although, I still think you're going to have number one hit records,
[01:56:50.960 --> 01:56:53.200]   number one hit movies, and number one hit people,
[01:56:53.200 --> 01:56:57.360]   that some people will rise to the top and become somehow universal.
[01:56:57.360 --> 01:56:59.520]   I don't know why, but I still believe that.
[01:56:59.520 --> 01:57:00.880]   Yeah.
[01:57:00.880 --> 01:57:01.600]   Maybe I'm wrong.
[01:57:01.600 --> 01:57:02.560]   We'll see.
[01:57:02.560 --> 01:57:03.680]   I'll be dead, but we'll see.
[01:57:03.680 --> 01:57:05.440]   And now.
[01:57:05.440 --> 01:57:07.760]   Now what's going to happen?
[01:57:07.760 --> 01:57:08.560]   What's going to happen now?
[01:57:08.560 --> 01:57:09.440]   Time for the Google.
[01:57:09.440 --> 01:57:12.080]   Change log.
[01:57:12.080 --> 01:57:16.080]   The Google change log.
[01:57:16.080 --> 01:57:22.880]   Carsten stacked this thing like the Supreme Court.
[01:57:22.880 --> 01:57:23.280]   I know.
[01:57:23.280 --> 01:57:24.320]   He really loves the change log.
[01:57:24.320 --> 01:57:24.640]   Yeah.
[01:57:24.640 --> 01:57:25.600]   He added so much more in.
[01:57:25.600 --> 01:57:27.040]   I had other stories elsewhere.
[01:57:27.040 --> 01:57:27.600]   No, no, no, no.
[01:57:27.600 --> 01:57:28.480]   That's change log.
[01:57:28.480 --> 01:57:30.400]   Put that and change log.
[01:57:30.400 --> 01:57:32.000]   Well, I'm just going to start with this.
[01:57:32.000 --> 01:57:35.440]   The Google Doodle contest that they do for kids.
[01:57:35.440 --> 01:57:39.040]   The Doodle for Google contest.
[01:57:39.040 --> 01:57:40.960]   According to Google, was one for the books,
[01:57:40.960 --> 01:57:45.200]   tens of thousands of entries from students all over the nation.
[01:57:45.200 --> 01:57:47.120]   The prompt was, I love the prompt,
[01:57:47.120 --> 01:57:51.120]   I show kindness by dot dot dot.
[01:57:51.120 --> 01:57:55.200]   Few weeks ago, they announced their five national finalists from each,
[01:57:55.200 --> 01:57:56.480]   one from each grade group.
[01:57:56.480 --> 01:58:03.200]   But it is now time to announce the winner of the 2020 Doodle for Google contest.
[01:58:03.200 --> 01:58:08.320]   Texas fifth grader, Sharon Sarah.
[01:58:08.320 --> 01:58:10.080]   And there is her Doodle.
[01:58:10.080 --> 01:58:15.280]   Together is one highlighting the importance of inclusion and acceptance.
[01:58:15.280 --> 01:58:22.160]   See, there's your people right there and see each one is wearing a different letter in the word
[01:58:22.160 --> 01:58:26.080]   Google. Don't you love how massive corporations pretend to be human?
[01:58:26.080 --> 01:58:28.560]   That's so significant.
[01:58:28.560 --> 01:58:30.960]   But I'm being the only excuse me to the law.
[01:58:30.960 --> 01:58:32.640]   Oh, that's true.
[01:58:32.640 --> 01:58:35.440]   That's true by making contributions to political parties
[01:58:35.440 --> 01:58:38.400]   and nominating fifth graders to win a contest.
[01:58:38.400 --> 01:58:42.800]   So she's apparently been entering since third grade.
[01:58:42.800 --> 01:58:43.920]   So there you go.
[01:58:43.920 --> 01:58:44.640]   Yeah.
[01:58:44.640 --> 01:58:46.080]   And what does she win?
[01:58:46.080 --> 01:58:48.960]   She will receive, get ready.
[01:58:50.640 --> 01:58:57.440]   She will be seen by the nation on the Google homepage and then a $30,000 college scholarship.
[01:58:57.440 --> 01:59:03.360]   Her school, Vaughn Elementary School, will receive a $50,000 technology package.
[01:59:03.360 --> 01:59:04.720]   So,
[01:59:04.720 --> 01:59:06.080]   Chromebooks, they're going to get the Chromebooks.
[01:59:06.080 --> 01:59:07.040]   I'm not getting the kids.
[01:59:07.040 --> 01:59:08.800]   And they're going to get all the old pixel books.
[01:59:08.800 --> 01:59:09.600]   Yeah.
[01:59:09.600 --> 01:59:12.880]   So congratulations, Sharon.
[01:59:12.880 --> 01:59:13.680]   Enjoy the kids.
[01:59:13.680 --> 01:59:18.240]   She's a real, she's a celebrity that deserves the name celebrity.
[01:59:18.240 --> 01:59:20.800]   Because she drew a nice little drawing.
[01:59:20.800 --> 01:59:25.120]   Hey, did you know that Flutter will now work with Windows?
[01:59:25.120 --> 01:59:25.680]   Yes, it will.
[01:59:25.680 --> 01:59:27.920]   Oh, sorry.
[01:59:27.920 --> 01:59:30.480]   Google gets a COVID layer.
[01:59:30.480 --> 01:59:36.640]   I don't know what I put this one on.
[01:59:36.640 --> 01:59:38.560]   I put this one and he moved it down there.
[01:59:38.560 --> 01:59:40.880]   It's going to tell you where there's more COVID cases.
[01:59:40.880 --> 01:59:42.000]   There's COVID over here.
[01:59:42.000 --> 01:59:43.120]   So stay away.
[01:59:43.120 --> 01:59:44.880]   Oh, is my problem?
[01:59:44.880 --> 01:59:45.600]   I do it wrong.
[01:59:45.600 --> 01:59:46.400]   No, I said it.
[01:59:46.400 --> 01:59:47.360]   I decided.
[01:59:47.360 --> 01:59:51.920]   I decided from now on, I'm calling it COVID or the Rona.
[01:59:51.920 --> 01:59:55.280]   So in addition to, yes, like,
[01:59:55.280 --> 01:59:58.880]   Kooties, it's the Kooties, Kooties.
[01:59:58.880 --> 02:00:04.880]   In addition to satellite layer, traffic layer, you can add now a COVID-19 layer.
[02:00:04.880 --> 02:00:06.800]   Stay away from there.
[02:00:06.800 --> 02:00:09.600]   Wow.
[02:00:09.600 --> 02:00:10.080]   Is it?
[02:00:10.080 --> 02:00:10.320]   Yeah.
[02:00:10.320 --> 02:00:16.480]   Look, so here in this map, North Dakota is red and South Dakota is pink.
[02:00:17.280 --> 02:00:19.200]   But Nebraska is yellow.
[02:00:19.200 --> 02:00:23.040]   So just consider that.
[02:00:23.040 --> 02:00:26.640]   The data comes from Johns Hopkins, the New York Times, and Wikipedia.
[02:00:26.640 --> 02:00:26.960]   OK.
[02:00:26.960 --> 02:00:28.080]   OK.
[02:00:28.080 --> 02:00:34.880]   Files by Google can now open PDFs and adjust video playback speed.
[02:00:34.880 --> 02:00:36.400]   That's good.
[02:00:36.400 --> 02:00:36.720]   Good.
[02:00:36.720 --> 02:00:39.040]   Files is, you know, nice.
[02:00:39.040 --> 02:00:40.080]   Good to have.
[02:00:40.080 --> 02:00:42.880]   Google Takeout now lets you thank God.
[02:00:43.680 --> 02:00:49.040]   Select photo albums to transfer them directly to Flickr or OneDrive, Microsoft's product.
[02:00:49.040 --> 02:00:53.280]   So it used to be added download these files and XML files.
[02:00:53.280 --> 02:00:57.520]   I think you're also supposed to be able to take an individual photo and now share it directly to
[02:00:57.520 --> 02:00:59.440]   like Twitter without going through that whole McGillia.
[02:00:59.440 --> 02:01:00.320]   Hallelujah.
[02:01:00.320 --> 02:01:02.080]   Hallelujah.
[02:01:02.080 --> 02:01:03.360]   I don't know.
[02:01:03.360 --> 02:01:05.360]   You already sent your Pixel Buds back, Jeff.
[02:01:05.360 --> 02:01:06.720]   I'm sorry to say twice.
[02:01:06.720 --> 02:01:07.280]   Twice.
[02:01:07.280 --> 02:01:08.800]   You don't have any more, do you?
[02:01:08.800 --> 02:01:10.320]   No.
[02:01:10.320 --> 02:01:12.960]   I was waiting for them to fix them, but this is not the fix.
[02:01:12.960 --> 02:01:15.520]   Oh, so there is a different problem.
[02:01:15.520 --> 02:01:21.360]   There is a fix that addresses the audio cutouts that happen every 110 seconds.
[02:01:21.360 --> 02:01:23.040]   I thought that was the one you were talking about.
[02:01:23.040 --> 02:01:24.320]   That's not the one I have.
[02:01:24.320 --> 02:01:25.120]   There's still a second.
[02:01:25.120 --> 02:01:27.600]   I mean, this defines a lemon.
[02:01:27.600 --> 02:01:30.640]   They're still admitting they haven't solved the other problem.
[02:01:30.640 --> 02:01:34.240]   So you're getting cutouts, but it wasn't every 110 seconds.
[02:01:34.240 --> 02:01:35.440]   No, no, no, no, it wasn't that.
[02:01:35.440 --> 02:01:37.280]   This was a whole new problem.
[02:01:37.280 --> 02:01:37.680]   It's sad.
[02:01:37.680 --> 02:01:40.720]   They fixed, but they have still not fixed the problem, and they're still selling the product,
[02:01:40.720 --> 02:01:41.440]   which I think is.
[02:01:41.440 --> 02:01:42.640]   Oh, that's really sad.
[02:01:42.640 --> 02:01:43.280]   Unconsciable.
[02:01:43.280 --> 02:01:45.920]   Does everybody have this problem?
[02:01:45.920 --> 02:01:48.960]   No, that's the weird thing, but they acknowledge that not people do.
[02:01:48.960 --> 02:01:53.440]   Hardhead hasn't complained in a long time.
[02:01:53.440 --> 02:01:54.240]   Is that what he's using?
[02:01:54.240 --> 02:01:55.280]   He's quite regularly.
[02:01:55.280 --> 02:01:56.880]   Is he using the originals or the new ones?
[02:01:56.880 --> 02:02:01.200]   He bought his right when he came out, so.
[02:02:01.200 --> 02:02:02.800]   They don't have a wire connecting them.
[02:02:02.800 --> 02:02:04.080]   They're just two separate ones.
[02:02:04.080 --> 02:02:04.880]   Okay, that's the new one.
[02:02:04.880 --> 02:02:09.840]   Google is moving Android 11 to the Android TV platform.
[02:02:09.840 --> 02:02:11.200]   I actually am happy to hear that.
[02:02:11.200 --> 02:02:13.120]   I have an Nvidia Shield.
[02:02:13.120 --> 02:02:14.880]   It's my favorite Android TV device.
[02:02:14.880 --> 02:02:18.880]   It's pricey, but it's got a nice processor in it and all that.
[02:02:18.880 --> 02:02:22.480]   So why am I hearing myself?
[02:02:22.480 --> 02:02:25.840]   Is that coming from something?
[02:02:25.840 --> 02:02:28.080]   It's your ego talking.
[02:02:28.080 --> 02:02:30.160]   It's your conscience.
[02:02:30.160 --> 02:02:30.720]   I hear it too.
[02:02:30.720 --> 02:02:34.160]   I think my bicameral mind has finally split right in the middle,
[02:02:34.160 --> 02:02:35.920]   and I'm talking to myself again.
[02:02:35.920 --> 02:02:36.320]   Hello?
[02:02:36.320 --> 02:02:38.880]   Who's is that from Mike?
[02:02:38.880 --> 02:02:39.680]   Who's that coming from?
[02:02:39.680 --> 02:02:40.240]   Can you tell me?
[02:02:40.240 --> 02:02:42.000]   Someone's got new headphones.
[02:02:42.000 --> 02:02:43.120]   He's got headphones too, lad.
[02:02:43.120 --> 02:02:51.840]   The phone app is being renamed to not a phone.
[02:02:51.840 --> 02:02:52.960]   No, no, it's going to be called.
[02:02:52.960 --> 02:02:55.680]   They didn't take a note from Microsoft.
[02:02:55.680 --> 02:02:56.320]   That would be funny.
[02:02:56.320 --> 02:02:59.520]   It's going to be called the phone by Google.
[02:02:59.520 --> 02:03:01.520]   In case you didn't know.
[02:03:07.040 --> 02:03:09.840]   Google has launched a work tracking tool
[02:03:09.840 --> 02:03:12.960]   that's actually kind of like air table,
[02:03:12.960 --> 02:03:15.200]   which is a really cool product.
[02:03:15.200 --> 02:03:16.560]   It's called Tables.
[02:03:16.560 --> 02:03:22.720]   This comes from the incubator area 120.
[02:03:22.720 --> 02:03:25.120]   It lets you track projects.
[02:03:25.120 --> 02:03:26.560]   There are just so many other things.
[02:03:26.560 --> 02:03:28.800]   Not just tables or spots or Monday.
[02:03:28.800 --> 02:03:30.320]   There's so many other things that do this.
[02:03:30.320 --> 02:03:33.760]   Jira, Atlassian's Jira.
[02:03:35.200 --> 02:03:37.040]   But now there's tables.
[02:03:37.040 --> 02:03:38.400]   It might be worth trying.
[02:03:38.400 --> 02:03:44.720]   I looked at the UI and I thought this does not seem really very attractive.
[02:03:44.720 --> 02:03:55.840]   You can import data from Google Sheets or a comma-separated value file.
[02:03:55.840 --> 02:04:00.080]   Share data with your Google Groups and assign tasks to people found in your Google context.
[02:04:00.080 --> 02:04:03.440]   They also have templates and there are automated actions.
[02:04:04.880 --> 02:04:09.520]   So you can have grid views, record lists, can-bandboards, and maps.
[02:04:09.520 --> 02:04:11.680]   Yes, sounds just like Monday.
[02:04:11.680 --> 02:04:17.440]   Well, pricing and what's the value of that far is what features you get in
[02:04:17.440 --> 02:04:17.760]   under that.
[02:04:17.760 --> 02:04:18.640]   That's what's going to matter.
[02:04:18.640 --> 02:04:21.600]   Ten dollars per user per month, very pricey.
[02:04:21.600 --> 02:04:26.400]   Well, you can use it for free for 100 tables and 1000 rows.
[02:04:26.400 --> 02:04:29.840]   But if you want to get more tables and more rows,
[02:04:29.840 --> 02:04:31.920]   ten dollars per user per month,
[02:04:31.920 --> 02:04:35.760]   you also get bigger attachments, more actions, advanced history, blah, blah, blah.
[02:04:35.760 --> 02:04:37.520]   Maybe worth looking at.
[02:04:37.520 --> 02:04:39.120]   It is at least worth a try.
[02:04:39.120 --> 02:04:39.360]   Yeah.
[02:04:39.360 --> 02:04:42.800]   Someone that needs it, can't necessarily afford some of those other products.
[02:04:42.800 --> 02:04:45.600]   Ten bucks a month is about what the other guys charge as well per user.
[02:04:45.600 --> 02:04:52.400]   And Google, which has always had trouble with paid Chrome extensions,
[02:04:52.400 --> 02:04:57.440]   they put them on hold, they limited them, now they've just shut them down.
[02:04:57.440 --> 02:05:00.800]   Developers can no longer make new paid extensions.
[02:05:01.760 --> 02:05:07.200]   And that there was a temporary policy, temporary suspension back in January
[02:05:07.200 --> 02:05:12.240]   because of the number of fraudulent transactions surrounding paid extensions.
[02:05:12.240 --> 02:05:13.760]   They're going to just face them out.
[02:05:13.760 --> 02:05:21.120]   And by February 1st, any existing extension will no longer be able to charge customers.
[02:05:21.120 --> 02:05:23.200]   So here's the timeline.
[02:05:23.200 --> 02:05:26.000]   The announcement of the deprecation happened a couple of days ago.
[02:05:26.000 --> 02:05:28.240]   The new item ban is now permanent.
[02:05:28.240 --> 02:05:30.560]   Free trials disabled December 1st.
[02:05:30.560 --> 02:05:32.720]   All payments disabled February 1st.
[02:05:32.720 --> 02:05:37.840]   And the licensing API will shut down later.
[02:05:37.840 --> 02:05:42.720]   In case you have a paid extension, I'm sure they're legit paid extensions,
[02:05:42.720 --> 02:05:45.440]   but it was the abuse that was causing a problem.
[02:05:45.440 --> 02:05:51.760]   And I believe in a record amount of time, we have now completed the Google change log.
[02:05:56.320 --> 02:06:03.520]   A couple of quickies I wanted to give you, Jeff, a chance to mention your new product.
[02:06:03.520 --> 02:06:06.560]   So thank you for the plug.
[02:06:06.560 --> 02:06:07.040]   Yes.
[02:06:07.040 --> 02:06:13.600]   So we started my center at the Townite Center at the Craig Newmark Graduate School of Journalism.
[02:06:13.600 --> 02:06:14.960]   We run communities of practice.
[02:06:14.960 --> 02:06:16.560]   The first one we started was run product.
[02:06:16.560 --> 02:06:19.680]   We had about 25 people from mainly New York, but also run the country,
[02:06:19.680 --> 02:06:24.160]   who in this new job title didn't exist in newspapers, didn't exist in news,
[02:06:24.880 --> 02:06:27.440]   who were in product and grew.
[02:06:27.440 --> 02:06:32.400]   And then friends at Temple and Mizzou came in, so we want to make this thing national
[02:06:32.400 --> 02:06:34.080]   and we want to make it international.
[02:06:34.080 --> 02:06:40.400]   So as of today, there is the opening of the news product, what is it called?
[02:06:40.400 --> 02:06:41.760]   Alliance, NPA.
[02:06:41.760 --> 02:06:42.800]   Right.
[02:06:42.800 --> 02:06:46.400]   So we also have communities of practice around audience and talent and inclusion
[02:06:46.400 --> 02:06:48.000]   and commerce.
[02:06:48.000 --> 02:06:52.480]   We're starting one for the independent journalists for the Casey Newton's of the world.
[02:06:53.440 --> 02:06:56.640]   We're on data and the business of journalism, but this one was the first one,
[02:06:56.640 --> 02:07:00.800]   the bird flew out of the nest and I'm very, very proud of us.
[02:07:00.800 --> 02:07:03.120]   And it was an example of collaboration.
[02:07:03.120 --> 02:07:10.560]   Google in full disclosure has backed training for product people through this group and our
[02:07:10.560 --> 02:07:12.800]   continuing education arm.
[02:07:12.800 --> 02:07:14.720]   And I think it's going to be good.
[02:07:14.720 --> 02:07:16.320]   What's the deliverable?
[02:07:16.320 --> 02:07:16.800]   Is it a...
[02:07:16.800 --> 02:07:22.640]   It's an association for people who work in product and news so they can cry
[02:07:22.640 --> 02:07:24.720]   on each other's shoulders from nobody understands what they do.
[02:07:24.720 --> 02:07:26.480]   Will there be a group like...
[02:07:26.480 --> 02:07:29.520]   There'll be a conference at some point.
[02:07:29.520 --> 02:07:30.080]   Oh, conference.
[02:07:30.080 --> 02:07:31.360]   We already have one conference in Philly.
[02:07:31.360 --> 02:07:32.000]   We already have one.
[02:07:32.000 --> 02:07:32.480]   Okay.
[02:07:32.480 --> 02:07:35.520]   So we launched it and with 300 people trying to come.
[02:07:35.520 --> 02:07:38.160]   But this is a part of the game.
[02:07:38.160 --> 02:07:41.520]   This audience are two, audience development,
[02:07:41.520 --> 02:07:45.440]   are two jobs that didn't exist in my field 20 years ago.
[02:07:45.440 --> 02:07:45.760]   Nice.
[02:07:45.760 --> 02:07:48.480]   So yep, thank you for the plug.
[02:07:48.480 --> 02:07:50.640]   This is the board of advisors.
[02:07:51.680 --> 02:07:54.880]   And if you go to newsproducts.org, you can read all about it.
[02:07:54.880 --> 02:07:59.280]   And if you want to pitch in, join the alliance, you can.
[02:07:59.280 --> 02:08:01.440]   The news product business?
[02:08:01.440 --> 02:08:01.680]   Yeah.
[02:08:01.680 --> 02:08:05.280]   Oh, let's see.
[02:08:05.280 --> 02:08:07.440]   Oh, the ignoble prizes are out.
[02:08:07.440 --> 02:08:09.360]   I always look forward to the ignobles.
[02:08:09.360 --> 02:08:10.720]   They're fun.
[02:08:10.720 --> 02:08:10.960]   Yeah.
[02:08:10.960 --> 02:08:19.360]   These are prizes not for great scientific projects, but less than great.
[02:08:20.080 --> 02:08:24.880]   They have a picture at the top of the 2009 that was really ahead of the science of the time.
[02:08:24.880 --> 02:08:26.880]   It was really an important thing to see there.
[02:08:26.880 --> 02:08:27.520]   That picture.
[02:08:27.520 --> 02:08:30.880]   This is a brazier that can quickly convert into a pair,
[02:08:30.880 --> 02:08:36.080]   not one but two protective face masks invented in 2009.
[02:08:36.080 --> 02:08:37.680]   She knew ahead of time.
[02:08:37.680 --> 02:08:38.320]   She knew.
[02:08:38.320 --> 02:08:39.920]   She was, this is the great thing about science.
[02:08:39.920 --> 02:08:42.240]   You know, this is the day they see ahead.
[02:08:42.240 --> 02:08:42.880]   That's great.
[02:08:42.880 --> 02:08:46.640]   Well, what's interesting is I do know people who've turned their bras into face masks.
[02:08:46.640 --> 02:08:50.000]   So this is not unheard of now.
[02:08:50.000 --> 02:08:52.560]   In the acoustics prize,
[02:08:52.560 --> 02:08:59.840]   a number of scientists received the present.
[02:08:59.840 --> 02:09:05.040]   I want to read all their names for inducing a female Chinese alligator to bellow in an
[02:09:05.040 --> 02:09:09.200]   airtight chamber filled with helium enriched air.
[02:09:09.200 --> 02:09:13.440]   As a result, they got the paper, a Chinese alligator in Heliox,
[02:09:13.440 --> 02:09:18.320]   formant frequencies in a crocodilian in the Journal of Experimental Biote.
[02:09:18.320 --> 02:09:19.680]   Apparently there's no video of it.
[02:09:19.680 --> 02:09:20.480]   Ah, last.
[02:09:20.480 --> 02:09:25.520]   Here's one I think could be very useful from the Journal of Personality.
[02:09:25.520 --> 02:09:30.000]   Eyebrows cue grandiose narcissism.
[02:09:30.000 --> 02:09:35.440]   A method to identify narcissists by looking at their eyebrows.
[02:09:35.440 --> 02:09:38.080]   You know, I have a method for detecting sociopaths.
[02:09:38.080 --> 02:09:38.800]   Did you know that?
[02:09:38.800 --> 02:09:41.200]   What's that?
[02:09:41.200 --> 02:09:42.320]   Would you like to know what it is?
[02:09:42.320 --> 02:09:42.640]   Well,
[02:09:42.640 --> 02:09:44.400]   if you yawn,
[02:09:44.400 --> 02:09:52.560]   and you know how yawns are contagious and the other person doesn't yawn.
[02:09:52.560 --> 02:09:54.880]   They're sociopaths.
[02:09:54.880 --> 02:09:55.680]   They're sociopaths.
[02:09:55.680 --> 02:09:57.680]   That's why you have us on the screens.
[02:09:57.680 --> 02:09:59.680]   They have no empathy,
[02:09:59.680 --> 02:10:02.080]   no connection at all.
[02:10:02.080 --> 02:10:02.640]   Not at all.
[02:10:02.640 --> 02:10:04.240]   Just say, I am what I am.
[02:10:04.240 --> 02:10:07.360]   But let me look at your eyebrows.
[02:10:07.360 --> 02:10:09.200]   Man, I don't know if that's a cue for nice.
[02:10:09.200 --> 02:10:11.280]   What about the frozen feces knife?
[02:10:12.080 --> 02:10:15.360]   That was another gem from the awards.
[02:10:15.360 --> 02:10:20.800]   Hey, I'm looking for a guy named Richard Vetter.
[02:10:20.800 --> 02:10:24.240]   No, I'm sorry, there's another person.
[02:10:24.240 --> 02:10:28.320]   It was a UK and US study to study your frozen human feces.
[02:10:28.320 --> 02:10:28.640]   Right.
[02:10:28.640 --> 02:10:30.800]   Could be made in the usable knives.
[02:10:30.800 --> 02:10:32.320]   This is from that journey.
[02:10:32.320 --> 02:10:35.760]   The Journal of Archaeological Science.
[02:10:35.760 --> 02:10:40.720]   And the title of the paper is Experimental Replication Shows.
[02:10:40.720 --> 02:10:45.360]   Knives manufactured from frozen human feces do not work.
[02:10:45.360 --> 02:10:47.360]   Oh, no.
[02:10:47.360 --> 02:10:48.400]   Do not work.
[02:10:48.400 --> 02:10:52.720]   We experimentally tested knives manufactured from frozen human feces.
[02:10:52.720 --> 02:10:55.120]   They do not work.
[02:10:55.120 --> 02:10:57.440]   That poor postdoc who had to work on that man.
[02:10:57.440 --> 02:10:58.480]   Oh, geez.
[02:10:58.480 --> 02:11:00.640]   But this was actually
[02:11:00.640 --> 02:11:09.200]   to debunk an ethnographic account of an Inuit man who made a knife
[02:11:09.200 --> 02:11:13.600]   from his own frozen feces to butcher and disarticulated dog.
[02:11:13.600 --> 02:11:16.560]   It is apparently very common, not only in popular culture,
[02:11:16.560 --> 02:11:18.480]   but in academic literature.
[02:11:18.480 --> 02:11:21.440]   So really, this is a valuable edition.
[02:11:21.440 --> 02:11:22.480]   Be bunking.
[02:11:22.480 --> 02:11:26.960]   It's a bunking, a valuable edition to the literature.
[02:11:26.960 --> 02:11:30.240]   You might have read this story in Shadows of the Sun
[02:11:30.240 --> 02:11:33.040]   and other sources.
[02:11:33.040 --> 02:11:37.360]   So it doesn't work, kids.
[02:11:37.600 --> 02:11:40.880]   By the way, it goes on.
[02:11:40.880 --> 02:11:42.720]   I'll read the story.
[02:11:42.720 --> 02:11:46.640]   In the midst of a winter gale, over the objections of his family,
[02:11:46.640 --> 02:11:48.800]   he made plans to stay on the ice.
[02:11:48.800 --> 02:11:50.720]   An old Inuit man, he wouldn't move.
[02:11:50.720 --> 02:11:56.320]   He stepped out of their eglot, defecated, and honed the feces
[02:11:56.320 --> 02:11:59.600]   into a frozen blade, which he sharpened.
[02:11:59.600 --> 02:12:01.440]   See, there's details that left out.
[02:12:01.440 --> 02:12:02.720]   Maybe they didn't try this.
[02:12:02.720 --> 02:12:04.800]   He sharpened with a spray of saliva.
[02:12:05.680 --> 02:12:09.680]   With a knife, he killed the dog and used its ribcage as a sled.
[02:12:09.680 --> 02:12:14.080]   And it's hide to harness another dog.
[02:12:14.080 --> 02:12:16.560]   And then he rode off into the distance.
[02:12:16.560 --> 02:12:20.320]   I don't want that debunked.
[02:12:20.320 --> 02:12:22.800]   That's a hell of a story.
[02:12:22.800 --> 02:12:28.960]   You know, one of these things is actually real, the medicine prize.
[02:12:28.960 --> 02:12:29.920]   I have that.
[02:12:29.920 --> 02:12:31.200]   Let's I have that.
[02:12:31.200 --> 02:12:31.920]   You have this?
[02:12:31.920 --> 02:12:35.200]   You mean the one this pizza protected gets cancer?
[02:12:35.200 --> 02:12:36.640]   Oh, no, that was last year.
[02:12:36.640 --> 02:12:37.600]   That didn't work for me.
[02:12:37.600 --> 02:12:40.000]   You have arachnophobic entomologists.
[02:12:40.000 --> 02:12:41.200]   No, no, no.
[02:12:41.200 --> 02:12:42.720]   Here it is in the medicine prize.
[02:12:42.720 --> 02:12:45.120]   Miss Phonia, mesophonia.
[02:12:45.120 --> 02:12:46.240]   You have mesophonia.
[02:12:46.240 --> 02:12:46.800]   It's a real thing.
[02:12:46.800 --> 02:12:47.680]   And yes, I have it.
[02:12:47.680 --> 02:12:47.920]   Yes.
[02:12:47.920 --> 02:12:53.280]   It's a new psychiatric disorder, a preoccupation with a specific,
[02:12:53.280 --> 02:12:57.440]   aversive human sound that triggers impulsive aggression.
[02:12:57.440 --> 02:13:00.560]   It's called mesophonia.
[02:13:00.560 --> 02:13:03.280]   Do they say what the sound is?
[02:13:03.280 --> 02:13:05.440]   What's the sound that triggers you?
[02:13:05.440 --> 02:13:08.400]   Adhering other people make chewing sounds.
[02:13:08.400 --> 02:13:09.280]   I go berserk.
[02:13:09.280 --> 02:13:12.640]   I go berserk.
[02:13:12.640 --> 02:13:13.520]   I can't stand.
[02:13:13.520 --> 02:13:14.160]   Stop it, Liam.
[02:13:14.160 --> 02:13:14.960]   Stop it.
[02:13:14.960 --> 02:13:17.680]   I can't stand chewing gum near me.
[02:13:17.680 --> 02:13:22.080]   I can't stand people who open mouth chew.
[02:13:22.080 --> 02:13:23.520]   It drives me crazy because my mother,
[02:13:23.520 --> 02:13:26.080]   I think I may be with you on that.
[02:13:26.080 --> 02:13:27.360]   Drill this into me, man.
[02:13:27.360 --> 02:13:28.720]   You do not.
[02:13:28.720 --> 02:13:30.400]   You leave your lips closed.
[02:13:30.400 --> 02:13:31.840]   You make no noise.
[02:13:31.840 --> 02:13:33.600]   I had to learn how to deal with that.
[02:13:33.600 --> 02:13:36.400]   I hard way because some of my colleagues were
[02:13:36.400 --> 02:13:39.440]   Indian, Asian Indian.
[02:13:39.440 --> 02:13:40.960]   And it's very common to.
[02:13:40.960 --> 02:13:43.840]   It's not polite to smack your lips,
[02:13:43.840 --> 02:13:46.480]   but you can't help it with free those corn chips.
[02:13:46.480 --> 02:13:52.000]   I would recommend that you gentlemen not go to YouTube and search ASMR.
[02:13:52.000 --> 02:13:53.760]   Because they ask them all to be able to do their people.
[02:13:53.760 --> 02:13:55.280]   Smacking their lips.
[02:13:55.280 --> 02:13:56.880]   I'll cast that.
[02:13:56.880 --> 02:13:58.400]   So this is, they're making fun of this.
[02:13:58.400 --> 02:13:59.520]   This is real, right?
[02:13:59.520 --> 02:14:00.480]   This is real.
[02:14:00.480 --> 02:14:03.680]   Well, mesophonia isn't specifically just for chewing.
[02:14:03.680 --> 02:14:05.280]   It could be other sounds.
[02:14:05.280 --> 02:14:08.800]   The sound of a dog barking or something, but chewing.
[02:14:08.800 --> 02:14:10.720]   Yeah.
[02:14:10.720 --> 02:14:13.840]   I've seen other people complain of that.
[02:14:13.840 --> 02:14:14.080]   Yeah.
[02:14:14.080 --> 02:14:17.600]   Is it that you find it disgusting or you actually just get upset?
[02:14:17.600 --> 02:14:21.200]   Personally, I think it's just nasty.
[02:14:21.200 --> 02:14:22.720]   Okay.
[02:14:22.720 --> 02:14:25.200]   It's nasty and I can't concentrate on anything.
[02:14:25.200 --> 02:14:26.960]   I will move a restaurant.
[02:14:28.960 --> 02:14:33.520]   So for the economics prize, excellent paper, I thought.
[02:14:33.520 --> 02:14:40.000]   National income inequality predicts cultural variation in mouth to mouth kissing.
[02:14:40.000 --> 02:14:44.560]   Apparently it's not a human universal.
[02:14:44.560 --> 02:14:51.120]   And in fact, has something to do with income inequality.
[02:14:51.120 --> 02:14:52.880]   Okay.
[02:14:52.880 --> 02:14:57.120]   And then there's the management prize, which I really enjoy.
[02:14:58.560 --> 02:15:01.280]   This one goes to Guangxi, China.
[02:15:01.280 --> 02:15:07.200]   Five professional hitmen in China who managed to contract
[02:15:07.200 --> 02:15:09.840]   for a hit job in the following way.
[02:15:09.840 --> 02:15:13.120]   After accepting a payment to perform the murder,
[02:15:13.120 --> 02:15:19.360]   she went on then subcontracted the task to Motean Sheng,
[02:15:19.360 --> 02:15:24.640]   who then contracted subcontracted the task to Yang Kang Sheng,
[02:15:24.640 --> 02:15:28.560]   who then subcontracted the task to Yang Guang Sheng,
[02:15:28.560 --> 02:15:33.600]   who then subconsciously the task to Ling Shean Shee,
[02:15:33.600 --> 02:15:38.480]   with each subsequently enlisted hitmen receiving a smaller percentage of the fee,
[02:15:38.480 --> 02:15:41.520]   the murder was never actually performed.
[02:15:41.520 --> 02:15:45.360]   Sounds like the enterprise.
[02:15:45.360 --> 02:15:46.400]   Yeah.
[02:15:46.400 --> 02:15:49.280]   But apparently there was a trial because here's a picture.
[02:15:49.280 --> 02:15:53.120]   Boy, they have nice seats in the courtrooms of China.
[02:15:53.120 --> 02:15:54.240]   Look at those comfy.
[02:15:54.720 --> 02:15:56.720]   They look like they're from a bus or something.
[02:15:56.720 --> 02:15:57.600]   They're lying.
[02:15:57.600 --> 02:15:57.840]   Yeah.
[02:15:57.840 --> 02:15:58.480]   Yeah.
[02:15:58.480 --> 02:15:58.800]   Yeah.
[02:15:58.800 --> 02:15:59.440]   Wow.
[02:15:59.440 --> 02:16:01.360]   There they are.
[02:16:01.360 --> 02:16:02.400]   There's the five hitmen.
[02:16:02.400 --> 02:16:06.000]   Are they in order of how far down the line they are?
[02:16:06.000 --> 02:16:06.160]   Yeah.
[02:16:06.160 --> 02:16:11.760]   He told him who told him who told him who told him one got one got 10 bucks.
[02:16:11.760 --> 02:16:12.000]   Yeah.
[02:16:12.000 --> 02:16:13.440]   And yeah, by the end of it,
[02:16:13.440 --> 02:16:16.240]   it was being standing in the pyramid formation.
[02:16:16.240 --> 02:16:19.440]   Wow.
[02:16:19.440 --> 02:16:21.520]   Look at the judges' outfits in China.
[02:16:21.520 --> 02:16:23.840]   I think that little Nas X wears that shirt.
[02:16:24.480 --> 02:16:25.920]   Those are nice outfits.
[02:16:25.920 --> 02:16:28.560]   Is that a Hawaiian hang loose hand signal?
[02:16:28.560 --> 02:16:30.320]   It looks like on the, yeah, on the thing.
[02:16:30.320 --> 02:16:36.080]   You just, I've learned a lot about Chinese jurisprudence here today.
[02:16:36.080 --> 02:16:38.720]   All right.
[02:16:38.720 --> 02:16:45.520]   I won't mention, I won't mention the endomologist who glued two extra legs on a spider to see how
[02:16:45.520 --> 02:16:46.800]   it impacted him.
[02:16:46.800 --> 02:16:52.800]   Do you want to hear the,
[02:16:52.800 --> 02:16:54.800]   we actually have the sound.
[02:16:54.800 --> 02:17:00.560]   This is the bellow of a crocodile in Heliox, if you'd, if you'd like to hear it.
[02:17:00.560 --> 02:17:01.760]   I think you have to download it.
[02:17:01.760 --> 02:17:02.000]   No.
[02:17:02.000 --> 02:17:03.760]   Oh, I don't download it.
[02:17:03.760 --> 02:17:04.080]   Down.
[02:17:04.080 --> 02:17:05.440]   It's a move file.
[02:17:05.440 --> 02:17:06.880]   I don't have a, that'd be.
[02:17:06.880 --> 02:17:08.720]   Oh, you play it back.
[02:17:08.720 --> 02:17:10.160]   Oh, you always say I've got it.
[02:17:10.160 --> 02:17:10.800]   I've got it though.
[02:17:10.800 --> 02:17:11.200]   I can do it.
[02:17:11.200 --> 02:17:12.160]   Could you play it for us?
[02:17:12.160 --> 02:17:14.080]   Hold on a second here.
[02:17:14.080 --> 02:17:15.440]   I was birthed by downloads.
[02:17:15.440 --> 02:17:18.560]   How many, how many times have you been on a Zoom call and you say,
[02:17:18.560 --> 02:17:22.080]   you know, the only thing missing from this Zoom call would be a goat.
[02:17:22.720 --> 02:17:24.240]   Well, good news.
[02:17:24.240 --> 02:17:33.120]   You can now hire a goat for the bargain price of five pounds to join you for five minutes
[02:17:33.120 --> 02:17:35.520]   of your video meeting.
[02:17:35.520 --> 02:17:43.520]   You can choose from Mary, Lisa, Elizabeth, or Daisy.
[02:17:43.520 --> 02:17:50.560]   And Daisy's probably my choice because it says what to expect from Daisy rage and affection
[02:17:50.560 --> 02:17:56.160]   and equal measure, headbiting her neighbor's fence if they dare approach her hay supply
[02:17:56.160 --> 02:17:58.880]   and fully qualified scrum master lingo.
[02:17:58.880 --> 02:18:07.920]   So add a goat to your next Zoom call at cronkshowcronkshawfoldfarm.co.
[02:18:07.920 --> 02:18:12.000]   I think that's an outstanding icebreaker, even though I do not like goats.
[02:18:12.000 --> 02:18:16.800]   And out here without further ado is a crocodile bellowing in Heliox.
[02:18:16.800 --> 02:18:25.360]   This is this is stuff you don't get another podcast.
[02:18:25.360 --> 02:18:30.880]   See now this this doesn't bother Jeff, but he but no, no, oh yeah.
[02:18:30.880 --> 02:18:34.080]   Chewing, chewing drives me great chewing bridges.
[02:18:34.080 --> 02:18:35.120]   You're learning lots of thing.
[02:18:35.120 --> 02:18:40.400]   It sounded to chew in the open mouth chewing while crossing a bridge.
[02:18:40.400 --> 02:18:41.440]   I said I'm done.
[02:18:41.440 --> 02:18:42.880]   So it's over gone.
[02:18:45.440 --> 02:18:46.320]   Just kill me then.
[02:18:46.320 --> 02:18:53.680]   All right, let's take a little breather to consider what we've learned here today.
[02:18:53.680 --> 02:19:01.760]   Coming up next, our picks a number and so forth.
[02:19:01.760 --> 02:19:06.400]   Let's kick off the picks of the week with Mike Algin, I guess.
[02:19:06.400 --> 02:19:08.000]   You're filling in for Stacy.
[02:19:08.000 --> 02:19:09.520]   Do you have a pick this week?
[02:19:09.520 --> 02:19:10.240]   Yes.
[02:19:10.240 --> 02:19:15.680]   I do now I'm obsessed with auto scrolling sources of content.
[02:19:15.680 --> 02:19:22.640]   I like while I'm working all day to have information strolling by that, you know,
[02:19:22.640 --> 02:19:25.520]   and this is one of the actually one we were talking about Google+, one of the things I'd
[02:19:25.520 --> 02:19:30.080]   like about Google+, and the very beginning was that you could do that.
[02:19:30.080 --> 02:19:32.560]   You could actually have your feed auto scroll.
[02:19:32.560 --> 02:19:37.200]   I've used extensions to allow Twitter to auto scroll, but I'd love to have breaking
[02:19:37.200 --> 02:19:40.400]   news and other things scrolling while I'm working.
[02:19:40.400 --> 02:19:44.000]   So there's an old and I hope it's okay that I do an old tool.
[02:19:44.000 --> 02:19:47.680]   But this is, I don't know, probably like six, seven, eight years old called
[02:19:47.680 --> 02:19:55.680]   scrolled it and it's C, I'm sorry, it's S-C-R-O-L-L-D-I-T.com.
[02:19:55.680 --> 02:20:03.920]   And you can plug in any subreddit, including the new version of it, the hot version, whatever,
[02:20:03.920 --> 02:20:08.080]   but you can put in any subreddit and it will sit there and auto scroll.
[02:20:08.080 --> 02:20:09.600]   It'll auto refresh.
[02:20:09.600 --> 02:20:12.960]   So let me, let's see.
[02:20:12.960 --> 02:20:18.000]   So whether you're interested in news generally or some specific area, the second something's posted,
[02:20:18.000 --> 02:20:21.760]   it'll show up, it'll bump the other stuff down below and it'll just go all day long.
[02:20:21.760 --> 02:20:22.880]   Wow.
[02:20:22.880 --> 02:20:25.440]   Give me a sub-edit.
[02:20:25.440 --> 02:20:28.720]   What would be a good sub-edit for us to?
[02:20:30.320 --> 02:20:32.640]   Ant, what do you look at Reddit photography?
[02:20:32.640 --> 02:20:34.640]   It was so, no.
[02:20:34.640 --> 02:20:38.240]   Well, how about like pics? Do pics/new?
[02:20:38.240 --> 02:20:46.080]   And this is a, this is a, it's for photography and also, I'm sorry, P-I-C-S
[02:20:46.080 --> 02:20:49.040]   S-C-S/new.
[02:20:49.040 --> 02:20:52.000]   I don't understand the kids talk.
[02:20:52.000 --> 02:20:52.960]   I don't enjoy it.
[02:20:52.960 --> 02:20:53.680]   Do you have to hold it?
[02:20:53.680 --> 02:20:54.000]   Do you want to know?
[02:20:54.000 --> 02:20:55.840]   No, I did.
[02:20:55.840 --> 02:20:57.040]   No, it didn't.
[02:20:57.040 --> 02:20:59.760]   You got to just get rid of the K and then hit enter.
[02:20:59.760 --> 02:21:00.560]   Oh, there's no K.
[02:21:00.560 --> 02:21:02.640]   There's no K-S-S-S.
[02:21:02.640 --> 02:21:03.680]   Oh, P-I-C-S.
[02:21:03.680 --> 02:21:04.400]   Yeah, come on.
[02:21:04.400 --> 02:21:05.600]   It doesn't follow instructions well.
[02:21:05.600 --> 02:21:06.240]   Right.
[02:21:06.240 --> 02:21:08.240]   You don't understand.
[02:21:08.240 --> 02:21:12.640]   P-S-S-new connection error, you sure this subreddit exists?
[02:21:12.640 --> 02:21:15.520]   You might have to do the R slash, sorry.
[02:21:15.520 --> 02:21:17.600]   This is a dynamite segment, isn't it?
[02:21:17.600 --> 02:21:19.760]   Yeah, you're going to do forward slash R.
[02:21:19.760 --> 02:21:21.280]   Oh, bike slash, yeah.
[02:21:21.280 --> 02:21:21.840]   It's a boy.
[02:21:21.840 --> 02:21:24.880]   What a pic.
[02:21:24.880 --> 02:21:25.440]   What?
[02:21:25.440 --> 02:21:26.960]   Yeah, isn't it great?
[02:21:26.960 --> 02:21:28.560]   It's so old, it doesn't work.
[02:21:29.440 --> 02:21:30.240]   Okay.
[02:21:30.240 --> 02:21:30.960]   She's really in.
[02:21:30.960 --> 02:21:33.360]   Leo, it's the whole idea, it's fun, yeah.
[02:21:33.360 --> 02:21:34.240]   I understand.
[02:21:34.240 --> 02:21:35.280]   Which?
[02:21:35.280 --> 02:21:37.360]   What's happening here?
[02:21:37.360 --> 02:21:40.720]   Anyway, anyway, so if you're into Reddit,
[02:21:40.720 --> 02:21:42.720]   if you like various subreddits,
[02:21:42.720 --> 02:21:45.920]   you can squeeze that screen down to the side
[02:21:45.920 --> 02:21:47.200]   to a column on the side of your screen.
[02:21:47.200 --> 02:21:48.320]   I want to do this, have it right.
[02:21:48.320 --> 02:21:49.920]   I would do that all the time.
[02:21:49.920 --> 02:21:52.400]   Let me try a full URL, like every--
[02:21:52.400 --> 02:21:54.080]   Do auto-add at the top, yeah.
[02:21:54.080 --> 02:21:54.720]   There you go.
[02:21:54.720 --> 02:21:54.960]   No.
[02:21:54.960 --> 02:21:57.200]   The loot.
[02:21:57.200 --> 02:21:58.160]   Hm, nope.
[02:21:58.160 --> 02:21:59.120]   It might be just me.
[02:21:59.120 --> 02:22:01.600]   Let me just do--
[02:22:01.600 --> 02:22:04.000]   I'm reluctant to do this,
[02:22:04.000 --> 02:22:06.320]   but I'm going to do the top stuff on--
[02:22:06.320 --> 02:22:07.600]   Now, even that doesn't work.
[02:22:07.600 --> 02:22:09.520]   So it's clearly my problem.
[02:22:09.520 --> 02:22:12.080]   You know, it might be blocked.
[02:22:12.080 --> 02:22:14.240]   I have so many blockers running on here.
[02:22:14.240 --> 02:22:16.400]   It might be blocking it.
[02:22:16.400 --> 02:22:19.520]   Let me turn off that.
[02:22:19.520 --> 02:22:22.800]   Let me turn off that.
[02:22:22.800 --> 02:22:25.120]   Let me--
[02:22:25.120 --> 02:22:26.240]   Is the website broken?
[02:22:26.240 --> 02:22:26.720]   Yes.
[02:22:27.360 --> 02:22:28.000]   Now--
[02:22:28.000 --> 02:22:29.040]   We're just going to bring up all your cookies, Leo.
[02:22:29.040 --> 02:22:30.000]   That'll help you.
[02:22:30.000 --> 02:22:31.600]   So you have an HTTPS.
[02:22:31.600 --> 02:22:34.080]   You have an HTTPS there, Leo.
[02:22:34.080 --> 02:22:34.560]   There's no--
[02:22:34.560 --> 02:22:36.000]   There's a little box that you have there.
[02:22:36.000 --> 02:22:36.800]   You just want to do--
[02:22:36.800 --> 02:22:38.560]   It starts with a forward slash.
[02:22:38.560 --> 02:22:39.600]   First thing you have is a start slash--
[02:22:39.600 --> 02:22:40.240]   Start with a slash--
[02:22:40.240 --> 02:22:41.200]   Forward slash.
[02:22:41.200 --> 02:22:41.520]   OK.
[02:22:41.520 --> 02:22:43.200]   Just slash-hour.
[02:22:43.200 --> 02:22:43.200]   Yeah.
[02:22:43.200 --> 02:22:43.760]   OK.
[02:22:43.760 --> 02:22:43.840]   Just--
[02:22:43.840 --> 02:22:44.800]   Just-- it's just Reddit.
[02:22:44.800 --> 02:22:45.360]   All right.
[02:22:45.360 --> 02:22:46.320]   Oh, the fun.
[02:22:46.320 --> 02:22:48.000]   I can't stand so much fun.
[02:22:48.000 --> 02:22:48.960]   So much fun.
[02:22:48.960 --> 02:22:49.840]   It's just not working.
[02:22:49.840 --> 02:22:50.080]   It's every--
[02:22:50.080 --> 02:22:52.320]   Do a lot of demo in life.
[02:22:52.320 --> 02:22:52.800]   No.
[02:22:52.800 --> 02:22:55.600]   I don't know what to turn off now.
[02:22:55.600 --> 02:22:56.160]   What?
[02:22:56.160 --> 02:22:56.640]   OK.
[02:22:56.640 --> 02:22:57.200]   I can--
[02:22:57.200 --> 02:22:58.320]   Oh, my.
[02:22:58.320 --> 02:23:01.120]   You have to turn off all your privacy protections
[02:23:01.120 --> 02:23:02.800]   in order to use this.
[02:23:02.800 --> 02:23:04.800]   I'm going to turn off that.
[02:23:04.800 --> 02:23:06.560]   Let's see.
[02:23:06.560 --> 02:23:07.520]   Let's keep trying.
[02:23:07.520 --> 02:23:08.640]   I hope this is worth it.
[02:23:08.640 --> 02:23:11.200]   Oh, boy.
[02:23:11.200 --> 02:23:12.080]   There it is.
[02:23:12.080 --> 02:23:13.520]   So that was it.
[02:23:13.520 --> 02:23:14.080]   That was it.
[02:23:14.080 --> 02:23:16.080]   Boy, that was so much fun.
[02:23:16.080 --> 02:23:17.280]   I can't bear it.
[02:23:17.280 --> 02:23:18.320]   You had--
[02:23:18.320 --> 02:23:20.320]   I'm going to have to just relax now.
[02:23:20.320 --> 02:23:20.880]   [LAUGHTER]
[02:23:20.880 --> 02:23:22.480]   What are we looking at?
[02:23:22.480 --> 02:23:23.360]   What is this?
[02:23:23.360 --> 02:23:24.320]   [LAUGHTER]
[02:23:24.320 --> 02:23:25.120]   Is it terrible?
[02:23:25.120 --> 02:23:25.920]   Subreddit.
[02:23:25.920 --> 02:23:30.480]   Oh.
[02:23:30.480 --> 02:23:32.080]   What a pick, Mike.
[02:23:32.080 --> 02:23:33.920]   Oh, that saved me all kinds of time.
[02:23:33.920 --> 02:23:34.480]   [LAUGHTER]
[02:23:34.480 --> 02:23:36.240]   I brought me new things in life.
[02:23:36.240 --> 02:23:37.680]   Oh, let's see.
[02:23:37.680 --> 02:23:38.240]   [LAUGHTER]
[02:23:38.240 --> 02:23:38.720]   This is--
[02:23:38.720 --> 02:23:39.280]   [LAUGHTER]
[02:23:39.280 --> 02:23:41.840]   Oh, don't Leo, he's got to stop.
[02:23:41.840 --> 02:23:43.840]   I'm just exhausted.
[02:23:43.840 --> 02:23:45.440]   [LAUGHTER]
[02:23:45.440 --> 02:23:50.320]   I don't understand why isn't this working?
[02:23:50.320 --> 02:23:51.280]   All right.
[02:23:51.280 --> 02:23:52.160]   OK.
[02:23:52.160 --> 02:23:53.520]   You win.
[02:23:53.520 --> 02:23:54.720]   Thank you, Mike.
[02:23:54.720 --> 02:23:55.920]   He's waiting for the guys.
[02:23:55.920 --> 02:23:57.600]   [LAUGHTER]
[02:23:57.600 --> 02:23:58.800]   I got to eat now.
[02:23:58.800 --> 02:24:00.320]   I'm sure this is going to work.
[02:24:00.320 --> 02:24:05.440]   Just everybody go to scrolledit.com and try it for yourself.
[02:24:05.440 --> 02:24:06.720]   Thank you.
[02:24:06.720 --> 02:24:07.760]   I've got one.
[02:24:07.760 --> 02:24:08.400]   This is good.
[02:24:08.400 --> 02:24:09.920]   This came from Patrick Tallahaney.
[02:24:09.920 --> 02:24:11.440]   I'm not going to go through it.
[02:24:11.440 --> 02:24:12.720]   If we had more time, I would.
[02:24:12.720 --> 02:24:18.880]   It's the Tech Aid Quiz, T-E-C-H-A-D-E Quiz.com,
[02:24:18.880 --> 02:24:19.920]   from Pluralsight.
[02:24:19.920 --> 02:24:21.440]   If you go through this, it's really fun.
[02:24:21.440 --> 02:24:27.760]   45 images of technology you may remember from your Ute.
[02:24:27.760 --> 02:24:28.800]   Remember that?
[02:24:28.800 --> 02:24:29.520]   What is that?
[02:24:29.520 --> 02:24:30.640]   You know what that is?
[02:24:30.640 --> 02:24:31.280]   Let's tell them.
[02:24:31.280 --> 02:24:31.840]   Oh, I got you.
[02:24:31.840 --> 02:24:32.880]   It's a Tamagotchi.
[02:24:32.880 --> 02:24:34.320]   It's Tamagotchi.
[02:24:34.320 --> 02:24:36.800]   And if you spell it right, yeah, it'll say you're right.
[02:24:36.800 --> 02:24:38.960]   That was 1996.
[02:24:38.960 --> 02:24:39.600]   See that one?
[02:24:39.600 --> 02:24:41.920]   That's an eight-track.
[02:24:41.920 --> 02:24:44.320]   So this is kind of fun to go through this
[02:24:44.320 --> 02:24:48.560]   and see if you can identify some of these glorious--
[02:24:48.560 --> 02:24:50.000]   They're a Newton in there.
[02:24:50.000 --> 02:24:51.360]   Oh, I bet there is.
[02:24:51.360 --> 02:24:52.560]   There's a flip phone.
[02:24:52.560 --> 02:24:55.760]   And a lot of this stuff I own, I'm embarrassed to admit.
[02:24:55.760 --> 02:24:58.160]   There's a Walkman there, right?
[02:24:58.160 --> 02:25:02.720]   So this is kind of fun if you want to play the Tech Aid Quiz.
[02:25:02.720 --> 02:25:05.040]   There are some things, Burke, that we were stumped by.
[02:25:05.040 --> 02:25:07.600]   Like, everybody knows that's a Simon, right?
[02:25:07.600 --> 02:25:08.160]   Yeah.
[02:25:08.160 --> 02:25:09.680]   Mm-hmm.
[02:25:09.680 --> 02:25:12.080]   Is that an Altair next to the Simon?
[02:25:12.080 --> 02:25:12.880]   This one, yeah.
[02:25:12.880 --> 02:25:13.600]   I wasn't sure.
[02:25:13.600 --> 02:25:14.640]   That sounds like an Altair.
[02:25:14.640 --> 02:25:15.040]   Let's see.
[02:25:15.040 --> 02:25:16.560]   Yep.
[02:25:16.560 --> 02:25:17.120]   Boy, you're good.
[02:25:17.120 --> 02:25:17.920]   The 8800.
[02:25:17.920 --> 02:25:18.720]   What about this one?
[02:25:18.720 --> 02:25:19.520]   Do you know what that is?
[02:25:20.160 --> 02:25:20.960]   What is that?
[02:25:20.960 --> 02:25:22.960]   I don't know what that is.
[02:25:22.960 --> 02:25:23.440]   Can't do.
[02:25:23.440 --> 02:25:27.520]   I think somebody told me that that's a Pong game.
[02:25:27.520 --> 02:25:27.920]   Nope.
[02:25:27.920 --> 02:25:33.040]   It's just a knob.
[02:25:33.040 --> 02:25:37.600]   So it's an old video game, but I don't--
[02:25:37.600 --> 02:25:39.200]   I can't remember which one.
[02:25:39.200 --> 02:25:40.160]   Is it an Odyssey?
[02:25:40.160 --> 02:25:40.560]   Okay.
[02:25:40.560 --> 02:25:43.280]   O-D-Y-S-S-E-E.
[02:25:43.280 --> 02:25:44.160]   Nope.
[02:25:44.880 --> 02:25:51.040]   E-Y. E-Y-O-D-D. O-D-Y-S-S-E-Y.
[02:25:51.040 --> 02:25:52.400]   Just like the standard spelling.
[02:25:52.400 --> 02:25:53.920]   Oh, you're right.
[02:25:53.920 --> 02:25:55.280]   It was a Magnavox Odyssey.
[02:25:55.280 --> 02:25:57.680]   So what's this one, though?
[02:25:57.680 --> 02:25:59.200]   Is this looks like--
[02:25:59.200 --> 02:26:00.400]   Looks like a video game.
[02:26:00.400 --> 02:26:00.800]   Like a video game.
[02:26:00.800 --> 02:26:01.760]   Ice cream maker.
[02:26:01.760 --> 02:26:02.480]   Uncle John's--
[02:26:02.480 --> 02:26:05.280]   Oh, that's a seat in front of a video game.
[02:26:05.280 --> 02:26:05.920]   Actually, you know what that is?
[02:26:05.920 --> 02:26:08.800]   It's a feces knife frozen feces.
[02:26:08.800 --> 02:26:10.000]   Yeah, I think it was.
[02:26:10.000 --> 02:26:10.640]   Yeah, yeah.
[02:26:10.640 --> 02:26:10.960]   Yeah.
[02:26:10.960 --> 02:26:11.360]   Yeah.
[02:26:11.360 --> 02:26:12.240]   It never really worked.
[02:26:12.240 --> 02:26:14.480]   [LAUGHTER]
[02:26:14.480 --> 02:26:15.760]   Anyway, that's kind of fun.
[02:26:15.760 --> 02:26:18.400]   techaidquiz.com.
[02:26:18.400 --> 02:26:21.040]   Jeff Jarvis, I would love a number from you.
[02:26:21.040 --> 02:26:23.280]   Let's see here.
[02:26:23.280 --> 02:26:24.160]   Well, I'm going to mention one.
[02:26:24.160 --> 02:26:25.120]   I don't want to mention one other thing.
[02:26:25.120 --> 02:26:26.960]   You can do a couple, because there's a bunch of good stuff here.
[02:26:26.960 --> 02:26:27.440]   Yeah.
[02:26:27.440 --> 02:26:31.440]   So there was a village in the UK
[02:26:31.440 --> 02:26:33.600]   where every morning at seven o'clock--
[02:26:33.600 --> 02:26:35.200]   There's a number.
[02:26:35.200 --> 02:26:40.560]   For 18 months, the internet would go bonkers.
[02:26:40.560 --> 02:26:41.040]   It would work.
[02:26:41.040 --> 02:26:42.320]   That's what ants have.
[02:26:42.320 --> 02:26:43.920]   Ants got that problem.
[02:26:43.920 --> 02:26:46.960]   Every day, 7 AM, same thing.
[02:26:46.960 --> 02:26:47.440]   Yeah.
[02:26:47.440 --> 02:26:49.440]   They finally figured it out.
[02:26:49.440 --> 02:26:51.040]   They replaced things.
[02:26:51.040 --> 02:26:52.480]   They did all kinds of tracking.
[02:26:52.480 --> 02:26:56.480]   It was one guy in town who every morning at seven o'clock
[02:26:56.480 --> 02:26:58.400]   turned on his old-fashioned TV.
[02:26:58.400 --> 02:26:59.280]   Oh, my God.
[02:26:59.280 --> 02:27:01.360]   And they were all on cable.
[02:27:01.360 --> 02:27:03.920]   So it was an cable band.
[02:27:03.920 --> 02:27:05.760]   And the TV would emit a signal,
[02:27:05.760 --> 02:27:09.120]   which would interfere with the entire village's broadband.
[02:27:09.120 --> 02:27:12.160]   It took them a year and a half to catch him.
[02:27:12.960 --> 02:27:15.920]   The person said he was mortified and never turned on again,
[02:27:15.920 --> 02:27:17.440]   but asked his name not to be revealed.
[02:27:17.440 --> 02:27:18.880]   Oh, my God.
[02:27:18.880 --> 02:27:19.920]   That's a terrible--
[02:27:19.920 --> 02:27:20.480]   So that's the number.
[02:27:20.480 --> 02:27:21.600]   I want to mention something here.
[02:27:21.600 --> 02:27:24.240]   This is a little egotistical.
[02:27:24.240 --> 02:27:26.880]   I saw a tweet today from Justin Thorpe,
[02:27:26.880 --> 02:27:28.880]   who said, "I wanted to thank you.
[02:27:28.880 --> 02:27:31.040]   My dad, Wesley Thorpe, and I read public parts.
[02:27:31.040 --> 02:27:32.720]   The only of the two people on earth who did so.
[02:27:32.720 --> 02:27:34.400]   Together when it came out,
[02:27:34.400 --> 02:27:35.840]   he was a former journalist, writer,
[02:27:35.840 --> 02:27:37.680]   and the idea of public that's really resonated with him.
[02:27:37.680 --> 02:27:40.480]   Next tweet, "My dad passed away last Tuesday.
[02:27:40.480 --> 02:27:42.800]   I now have over 3,000 priceless blog posts
[02:27:42.800 --> 02:27:44.000]   that he wrote since 2004."
[02:27:44.000 --> 02:27:44.720]   Remember this?
[02:27:44.720 --> 02:27:45.200]   Look at this.
[02:27:45.200 --> 02:27:45.520]   Oh, my God.
[02:27:45.520 --> 02:27:47.920]   I'm so grateful that he defaulted to public.
[02:27:47.920 --> 02:27:52.880]   And I spent some time reading through Wes Thorpe's posts.
[02:27:52.880 --> 02:27:53.840]   And it's really amazing.
[02:27:53.840 --> 02:27:55.440]   He never really knew his father.
[02:27:55.440 --> 02:27:57.520]   He writes about the time he met him.
[02:27:57.520 --> 02:28:01.200]   He wrote about his love for his wife.
[02:28:01.200 --> 02:28:02.800]   He wrote about being a father himself,
[02:28:02.800 --> 02:28:06.480]   taking ministry to a prison and enduring Parkinson's.
[02:28:06.480 --> 02:28:06.720]   Wow.
[02:28:06.720 --> 02:28:09.840]   So Wes Thorpe, rest in peace and blog.
[02:28:10.640 --> 02:28:12.720]   That's why you write a blog.
[02:28:12.720 --> 02:28:13.680]   It really is.
[02:28:13.680 --> 02:28:14.160]   It really is.
[02:28:14.160 --> 02:28:15.040]   It's a human.
[02:28:15.040 --> 02:28:17.520]   It's 3,000 posts of a life.
[02:28:17.520 --> 02:28:19.760]   And it's really quite wonderful.
[02:28:19.760 --> 02:28:22.160]   So thank you, Justin.
[02:28:22.160 --> 02:28:22.640]   Hi.
[02:28:22.640 --> 02:28:25.680]   We need somewhere to kind of memorialize these kinds of things,
[02:28:25.680 --> 02:28:25.920]   right?
[02:28:25.920 --> 02:28:26.640]   Because that's not going to--
[02:28:26.640 --> 02:28:28.640]   It's on TypePad, but it's not going to go forever.
[02:28:28.640 --> 02:28:32.160]   Unless his son, Justin, pays the subscription fee.
[02:28:32.160 --> 02:28:35.040]   There's got to be some way to kind of--
[02:28:35.040 --> 02:28:36.560]   I guess the internet archive?
[02:28:36.560 --> 02:28:37.040]   I don't know.
[02:28:38.240 --> 02:28:40.000]   There have been talks about trying to do this,
[02:28:40.000 --> 02:28:40.800]   memorializing.
[02:28:40.800 --> 02:28:41.200]   Yeah.
[02:28:41.200 --> 02:28:42.800]   I don't think the ways that we've actually done it.
[02:28:42.800 --> 02:28:43.120]   Yeah.
[02:28:43.120 --> 02:28:44.320]   He started--
[02:28:44.320 --> 02:28:46.720]   I kind of think that there's got to be a way--
[02:28:46.720 --> 02:28:50.160]   because I've been a fan of the idea of life logging
[02:28:50.160 --> 02:28:51.040]   for a long time.
[02:28:51.040 --> 02:28:53.840]   It's got to be a way where people can do a life log, actually.
[02:28:53.840 --> 02:28:56.320]   And then have that exist--
[02:28:56.320 --> 02:28:57.040]   Resurface.
[02:28:57.040 --> 02:28:57.600]   --of the activity.
[02:28:57.600 --> 02:28:57.760]   Yeah.
[02:28:57.760 --> 02:28:58.240]   Yeah.
[02:28:58.240 --> 02:29:00.960]   But I guess there's no money in it.
[02:29:00.960 --> 02:29:01.520]   So we--
[02:29:01.520 --> 02:29:02.160]   Right.
[02:29:02.160 --> 02:29:03.280]   Well, you know, and there's some--
[02:29:03.280 --> 02:29:05.520]   there's really good journaling software out there.
[02:29:05.520 --> 02:29:07.760]   I use day one.
[02:29:07.760 --> 02:29:09.040]   And I keep thinking I want to--
[02:29:09.040 --> 02:29:11.200]   and day one is kind of cool because it does a lot of--
[02:29:11.200 --> 02:29:14.160]   it's on an Apple platform.
[02:29:14.160 --> 02:29:16.160]   No, actually they have Android and Windows too.
[02:29:16.160 --> 02:29:18.560]   But it does some interesting things like
[02:29:18.560 --> 02:29:21.600]   it'll get the weather automatically.
[02:29:21.600 --> 02:29:22.560]   It'll get--
[02:29:22.560 --> 02:29:24.400]   when I weigh myself, it actually--
[02:29:24.400 --> 02:29:26.560]   the scale puts the weight in there.
[02:29:26.560 --> 02:29:26.800]   So--
[02:29:26.800 --> 02:29:28.160]   It's that kind of aggravation.
[02:29:28.160 --> 02:29:32.080]   It puts a lot of kind of daily stuff in there automatically.
[02:29:32.080 --> 02:29:33.920]   And then it gives me daily prompts.
[02:29:33.920 --> 02:29:35.680]   And I would think that would be something like that.
[02:29:36.240 --> 02:29:38.800]   And then I don't know what you print a book out of it or something.
[02:29:38.800 --> 02:29:42.320]   I'm sure day one has some idea of how to do that.
[02:29:42.320 --> 02:29:42.800]   I don't know.
[02:29:42.800 --> 02:29:47.840]   But yeah, this is a beautiful example of why you would write this.
[02:29:47.840 --> 02:29:52.800]   And even if only Justin reads it, that's still enough.
[02:29:52.800 --> 02:29:53.200]   Yeah.
[02:29:53.200 --> 02:29:54.000]   Yeah.
[02:29:54.000 --> 02:29:55.760]   And here's the family.
[02:29:55.760 --> 02:30:00.560]   So thank you for mentioning that.
[02:30:00.560 --> 02:30:01.600]   I think that's fantastic.
[02:30:01.600 --> 02:30:04.240]   And your turn.
[02:30:05.840 --> 02:30:08.400]   OK, I have a few, but I'll try to be quick.
[02:30:08.400 --> 02:30:13.440]   First up, I remember back in maybe April or May,
[02:30:13.440 --> 02:30:16.800]   and we said, you know, things are going to be back to normal about September.
[02:30:16.800 --> 02:30:21.760]   And, you know, we'll be able to go back to the offices and work and yadda yadda yadda.
[02:30:21.760 --> 02:30:26.720]   Well, no, we are still doing Zoom and in remote conferences.
[02:30:26.720 --> 02:30:29.920]   I think I said September 2021 when I said--
[02:30:29.920 --> 02:30:30.800]   Oh, OK.
[02:30:30.800 --> 02:30:31.600]   That's the difference.
[02:30:32.640 --> 02:30:39.440]   But with that said, Canon, they have finally gotten their webcam software out of beta.
[02:30:39.440 --> 02:30:39.840]   Oh, good.
[02:30:39.840 --> 02:30:41.280]   It's not public for Windows only.
[02:30:41.280 --> 02:30:42.480]   It's--
[02:30:42.480 --> 02:30:43.600]   Oh, what does--
[02:30:43.600 --> 02:30:44.240]   What else was it?
[02:30:44.240 --> 02:30:45.280]   --to disappoint me?
[02:30:45.280 --> 02:30:47.200]   Well, they're working on an Apple version.
[02:30:47.200 --> 02:30:49.440]   And they're working on Mac OS version.
[02:30:49.440 --> 02:30:51.040]   I don't think they'll ever be in another four weeks.
[02:30:51.040 --> 02:30:51.680]   Oh, I don't think they'll ever be in another four weeks.
[02:30:51.680 --> 02:30:53.200]   Look at how the beta was done.
[02:30:53.200 --> 02:30:56.560]   It took about four weeks for the Mac beta to come out.
[02:30:56.560 --> 02:30:57.040]   Right.
[02:30:57.040 --> 02:30:57.600]   Right.
[02:30:57.600 --> 02:31:00.000]   And I've tested this and it works really well.
[02:31:00.000 --> 02:31:06.320]   It works well with Hangouts, Zoom, OBS, if you want to do live streaming.
[02:31:06.320 --> 02:31:09.280]   They don't mention X split, which is what I use.
[02:31:09.280 --> 02:31:10.320]   It works with that.
[02:31:10.320 --> 02:31:12.640]   And yeah, it's pretty quick, pretty easy.
[02:31:12.640 --> 02:31:16.080]   Now, the whole idea here is that instead of having a capture utility,
[02:31:16.080 --> 02:31:19.680]   you could just take the video from the USB port on the camera.
[02:31:19.680 --> 02:31:25.280]   But most cameras, if you can get clean HDMI out, you can do that.
[02:31:25.280 --> 02:31:28.080]   You just need a little capture dongle.
[02:31:28.080 --> 02:31:32.560]   They sell them on Amazon, a huge range of prices from 30 bucks to 100 bucks.
[02:31:32.560 --> 02:31:38.000]   And you just plug the HDMI out of your camera into that and then plug that into the USB port.
[02:31:38.000 --> 02:31:40.240]   So most cameras, you could do that.
[02:31:40.240 --> 02:31:44.400]   This big advantage is that it does it in the camera and software.
[02:31:44.400 --> 02:31:46.160]   It streams straight out of the USB port.
[02:31:46.160 --> 02:31:47.200]   So that's straight out.
[02:31:47.200 --> 02:31:47.840]   And it works well.
[02:31:47.840 --> 02:31:48.400]   You like it.
[02:31:48.400 --> 02:31:52.800]   Yeah, I've tried it out a couple of times and it works really well.
[02:31:52.800 --> 02:31:57.760]   I haven't had any issues with it and it works with a variety of Canon cameras out there too.
[02:31:57.760 --> 02:31:59.040]   So you're not limited.
[02:31:59.040 --> 02:32:05.520]   I took one of the video cameras that we use for our shows, these Canon Vixias, which aren't
[02:32:05.520 --> 02:32:05.760]   expensive.
[02:32:05.760 --> 02:32:06.480]   Vixias.
[02:32:06.480 --> 02:32:08.560]   Yeah, they're like 400 bucks now.
[02:32:08.560 --> 02:32:14.320]   I took one home and actually it's such a good picture compared to the soft image that I was
[02:32:14.320 --> 02:32:20.640]   getting from the webcam on the computer that I now have to shave before I go to meeting spirits.
[02:32:20.640 --> 02:32:24.240]   And I think I'm going to start wearing makeup to be honest with you.
[02:32:26.080 --> 02:32:27.360]   It's just too good of a picture.
[02:32:27.360 --> 02:32:29.680]   I use a Vixia for my overhead shot.
[02:32:29.680 --> 02:32:30.080]   Look at that.
[02:32:30.080 --> 02:32:31.520]   I'm doing it hands on tech or...
[02:32:31.520 --> 02:32:32.880]   It's really crisp, isn't it?
[02:32:32.880 --> 02:32:33.440]   Hop.
[02:32:33.440 --> 02:32:33.760]   Yeah.
[02:32:33.760 --> 02:32:35.840]   I would kill for a better camera.
[02:32:35.840 --> 02:32:37.440]   I would kill so I'm not pink man.
[02:32:37.440 --> 02:32:40.400]   Why don't give me the instructions?
[02:32:40.400 --> 02:32:42.400]   Do we have any extra Vixias?
[02:32:42.400 --> 02:32:43.120]   We have a Vixia.
[02:32:43.120 --> 02:32:45.760]   Well, even if you don't, just give me the instructions.
[02:32:45.760 --> 02:32:51.040]   Perks going to look because we, you know, when we started this, we bought 40 of these cameras.
[02:32:51.040 --> 02:32:53.360]   And some have failed over time.
[02:32:53.360 --> 02:32:55.040]   I don't know how many we use these days.
[02:32:55.040 --> 02:32:56.240]   It's not 40.
[02:32:56.240 --> 02:32:58.000]   So we have some extras.
[02:32:58.000 --> 02:32:59.280]   I don't know many...
[02:32:59.280 --> 02:33:02.240]   Yeah, even if you don't have it, if you have instructions on how to do it, I'm just...
[02:33:02.240 --> 02:33:02.800]   Yeah.
[02:33:02.800 --> 02:33:07.840]   So this is one where you'd need to get the HDMI,
[02:33:07.840 --> 02:33:10.880]   mini HDMI out to full HDMI.
[02:33:10.880 --> 02:33:12.320]   And then some sort of dongle.
[02:33:12.320 --> 02:33:14.800]   I bought one on Amazon.
[02:33:14.800 --> 02:33:15.440]   They're cheap.
[02:33:15.440 --> 02:33:17.760]   You plug it in and it turns it into USB.
[02:33:17.760 --> 02:33:18.560]   And then you plug it in.
[02:33:18.560 --> 02:33:19.920]   And actually, it is a great...
[02:33:19.920 --> 02:33:21.680]   It would be worth it.
[02:33:21.680 --> 02:33:23.440]   It's in our interest to get that to you.
[02:33:23.440 --> 02:33:24.880]   So let's figure out a way to get Jeff's.
[02:33:24.880 --> 02:33:25.840]   Because I look like a crab.
[02:33:25.840 --> 02:33:26.240]   I know.
[02:33:26.240 --> 02:33:26.640]   Yeah.
[02:33:26.640 --> 02:33:28.080]   And you're doing it on the Mac, right?
[02:33:28.080 --> 02:33:29.600]   Yeah.
[02:33:29.600 --> 02:33:30.800]   The Mac you gave.
[02:33:30.800 --> 02:33:31.280]   Yeah.
[02:33:31.280 --> 02:33:33.200]   It wouldn't work that well on a Chromebook, but it'll work on the Mac.
[02:33:33.200 --> 02:33:34.480]   I don't know why we haven't done that.
[02:33:34.480 --> 02:33:35.760]   We'll get you a nice camera.
[02:33:35.760 --> 02:33:40.160]   But you're going to have to start washing your face, shaving, putting on a little face paint,
[02:33:40.160 --> 02:33:43.520]   a little lipstick, a little rouge, some concealer.
[02:33:43.520 --> 02:33:45.600]   Last time my shade was for you, fella.
[02:33:45.600 --> 02:33:46.480]   I know.
[02:33:46.480 --> 02:33:47.360]   A little bronzer.
[02:33:47.360 --> 02:33:48.240]   A little bronzer.
[02:33:48.240 --> 02:33:49.840]   You know.
[02:33:49.840 --> 02:33:52.080]   Actually, yeah, I should use the Trump stuff that I won't be picked.
[02:33:52.080 --> 02:33:52.960]   I'll just be orange.
[02:33:52.960 --> 02:33:53.280]   Yeah.
[02:33:53.280 --> 02:33:53.760]   That'll work.
[02:33:53.760 --> 02:33:54.240]   Yeah.
[02:33:54.240 --> 02:33:54.560]   Yeah.
[02:33:54.560 --> 02:33:58.560]   We'll also be able to see that you've got the art of the deal on your bookshelf back there.
[02:33:58.560 --> 02:33:59.840]   Oh, yeah, we will.
[02:33:59.840 --> 02:34:05.600]   There'll be a lot of people's scoping out what they can see now, all of a sudden.
[02:34:05.600 --> 02:34:06.320]   So be careful.
[02:34:06.320 --> 02:34:07.680]   Be careful.
[02:34:07.680 --> 02:34:08.400]   Yeah.
[02:34:08.400 --> 02:34:12.160]   I read the I'm reading the Michael Kohn book, "Disloyal."
[02:34:12.160 --> 02:34:14.320]   And he explained, I'm not going to go into it.
[02:34:14.320 --> 02:34:16.160]   If you want to find out, read the book,
[02:34:16.160 --> 02:34:19.920]   explains why the Donald's hair looks like that.
[02:34:19.920 --> 02:34:20.960]   There's actually a reason.
[02:34:20.960 --> 02:34:21.600]   Yeah.
[02:34:21.600 --> 02:34:22.160]   Oh, boy.
[02:34:22.160 --> 02:34:22.960]   That's fascinating.
[02:34:22.960 --> 02:34:24.480]   I'll pass.
[02:34:24.480 --> 02:34:25.040]   Yeah, I know.
[02:34:25.040 --> 02:34:28.320]   Thank you, Ann Pruitt.
[02:34:28.320 --> 02:34:29.120]   I don't pass you.
[02:34:29.120 --> 02:34:30.400]   Oh, I'm going to done though.
[02:34:30.400 --> 02:34:31.200]   Oh, you got more?
[02:34:31.200 --> 02:34:32.800]   Go ahead.
[02:34:32.800 --> 02:34:33.520]   Do another one.
[02:34:33.520 --> 02:34:40.560]   My other one, I've been talking about Adobe Max and I urge people because it's free.
[02:34:40.560 --> 02:34:42.400]   You don't have to be a photographer.
[02:34:42.400 --> 02:34:47.520]   If you're interested in design and just getting into digital arts,
[02:34:47.520 --> 02:34:49.040]   this thing is free.
[02:34:49.040 --> 02:34:51.600]   I highly recommend checking it out.
[02:34:51.600 --> 02:34:53.520]   They got a bunch of different sessions.
[02:34:53.520 --> 02:34:57.680]   And they've just recently announced the guest list for the speakers that are going to be on there.
[02:34:57.680 --> 02:35:00.000]   Chelsea Handler,
[02:35:00.000 --> 02:35:01.440]   and Annie Leibovitz.
[02:35:01.440 --> 02:35:03.600]   When is Paltrow?
[02:35:03.600 --> 02:35:03.920]   Wow.
[02:35:03.920 --> 02:35:05.120]   Point of Paltrow.
[02:35:05.120 --> 02:35:09.920]   I mean, it's a lot of good people that are going to be at this event virtually.
[02:35:09.920 --> 02:35:11.200]   Yeah, but it's free.
[02:35:11.200 --> 02:35:15.200]   And I highly recommend, especially just for the sessions in the class.
[02:35:15.200 --> 02:35:18.000]   There's so much content out there that you can learn.
[02:35:18.640 --> 02:35:21.840]   Wes Anderson and Tyka Wattiti.
[02:35:21.840 --> 02:35:22.320]   What?
[02:35:22.320 --> 02:35:25.360]   Two of my favorite directors, Anna Duvernay.
[02:35:25.360 --> 02:35:27.760]   Wow, David Tennant.
[02:35:27.760 --> 02:35:29.040]   They're going to have Dr. Ooh.
[02:35:29.040 --> 02:35:31.600]   Dr. Ooh and Zach Braff.
[02:35:31.600 --> 02:35:33.200]   Tyler the Creator.
[02:35:33.200 --> 02:35:34.080]   Two of my favorite times.
[02:35:34.080 --> 02:35:35.120]   Wow, sorry.
[02:35:35.120 --> 02:35:40.640]   Two of my favorite times were seeing Questlove and Dave Grohl on the stage.
[02:35:40.640 --> 02:35:41.040]   Oh, that's awesome.
[02:35:41.040 --> 02:35:44.960]   Just being able to take photographs of them and listen to their stories and listen to them.
[02:35:44.960 --> 02:35:48.000]   Inspire all of those thousands of people that were there.
[02:35:48.000 --> 02:35:52.880]   I really enjoyed it and I highly recommend it, especially because it's free.
[02:35:52.880 --> 02:35:56.000]   Do you think they would have this great list if it weren't virtual?
[02:35:56.000 --> 02:35:58.080]   Do they normally think so?
[02:35:58.080 --> 02:35:58.640]   I think so.
[02:35:58.640 --> 02:35:59.040]   Wow.
[02:35:59.040 --> 02:36:03.440]   I think so because they had Tiffany Haddish was there one year.
[02:36:03.440 --> 02:36:07.120]   Bert Munroy.
[02:36:07.120 --> 02:36:08.640]   I love Bert.
[02:36:08.640 --> 02:36:10.640]   Okay, get ready.
[02:36:10.640 --> 02:36:12.800]   Nick O'Lanick from Mollah was there.
[02:36:12.800 --> 02:36:13.520]   Oh, that's cool.
[02:36:13.520 --> 02:36:16.640]   Nick Offerman and Stanley Tucci will be there too.
[02:36:16.640 --> 02:36:20.480]   So you can watch, do I have to pay money for this though?
[02:36:20.480 --> 02:36:21.120]   No, you don't.
[02:36:21.120 --> 02:36:21.120]   No.
[02:36:21.120 --> 02:36:21.920]   It's free.
[02:36:21.920 --> 02:36:26.560]   Just give yourself, if you don't have an Adobe ID, just sign up for it for you.
[02:36:26.560 --> 02:36:27.680]   I do the ID in your answer.
[02:36:27.680 --> 02:36:29.360]   Oh, I've been giving Adobe money for years.
[02:36:29.360 --> 02:36:32.480]   They ought to let me in.
[02:36:32.480 --> 02:36:35.440]   October 20th through 27th.
[02:36:35.440 --> 02:36:36.240]   20th through 22nd.
[02:36:36.240 --> 02:36:38.080]   350 worship.
[02:36:38.080 --> 02:36:39.280]   I don't know if that's what it's like.
[02:36:39.280 --> 02:36:43.440]   Do they, there's a lot of teaching about how to use Photoshop and Lightroom and stuff like that.
[02:36:43.440 --> 02:36:48.960]   All of their products, even down to like Adobe experience design, that's where you
[02:36:48.960 --> 02:36:54.400]   prototype an apps or doing heck, even funky little presentations.
[02:36:54.400 --> 02:36:59.360]   They walk you through the apps to make it look good and be a lot more functional and
[02:36:59.360 --> 02:37:02.720]   just be super duper creative, character animator.
[02:37:02.720 --> 02:37:07.920]   If you want to get into making cartoons and things like that, it's so many different tools.
[02:37:07.920 --> 02:37:09.040]   So many different tools.
[02:37:09.040 --> 02:37:14.080]   I would like to see half these people I would, I would kill to just see talk.
[02:37:14.080 --> 02:37:15.440]   This is incredible.
[02:37:15.440 --> 02:37:17.440]   Oh, my God.
[02:37:17.440 --> 02:37:19.040]   This is really a great lineup.
[02:37:19.040 --> 02:37:19.520]   Look at that.
[02:37:19.520 --> 02:37:21.360]   All right.
[02:37:21.360 --> 02:37:27.760]   Lastly, yesterday was the National Voter Registration Day, you know,
[02:37:27.760 --> 02:37:29.600]   raising awareness to tell people to go vote.
[02:37:29.600 --> 02:37:34.560]   I want to piggyback off of that and say, you know what, make sure you look at these little
[02:37:34.560 --> 02:37:40.640]   pamphlets that you get in the mail that talks about your voting bills and
[02:37:40.640 --> 02:37:44.160]   referendums or these are propositions.
[02:37:44.160 --> 02:37:45.520]   Read those things.
[02:37:45.520 --> 02:37:46.880]   Those things matter.
[02:37:46.880 --> 02:37:47.520]   Read them.
[02:37:47.520 --> 02:37:48.000]   Read them up.
[02:37:48.000 --> 02:37:52.720]   See what it matters to you and vote, yay or nay, because that's the big thing.
[02:37:52.720 --> 02:37:55.200]   That's the stuff that's affecting you right there in your own backyard.
[02:37:55.200 --> 02:37:59.440]   So I highly recommend you don't throw this in the trash when it comes in the mail.
[02:37:59.440 --> 02:38:00.880]   Take the time and read it.
[02:38:00.880 --> 02:38:05.520]   Some of it you're not going to care about, but it can possibly affect you a day to day
[02:38:05.520 --> 02:38:08.800]   a lot quicker than vote in for the president.
[02:38:08.800 --> 02:38:10.080]   Indeed.
[02:38:10.080 --> 02:38:11.280]   Yes, sir.
[02:38:11.280 --> 02:38:12.800]   Vote up the ticket, not that.
[02:38:12.800 --> 02:38:14.000]   You want answers?
[02:38:14.000 --> 02:38:14.720]   Yep.
[02:38:14.720 --> 02:38:16.640]   Start at the bottom and work your way up.
[02:38:16.640 --> 02:38:20.320]   That's the way to vote because everybody goes to vote for the president, but they leave
[02:38:20.320 --> 02:38:21.280]   out a lot of the other stuff.
[02:38:21.280 --> 02:38:23.440]   And it's all important.
[02:38:23.440 --> 02:38:29.920]   You know, Corey Doctorow tweeted about a, this is just like really wild.
[02:38:30.640 --> 02:38:35.600]   A small race, the race in Texas for railroad commissioner.
[02:38:35.600 --> 02:38:38.400]   And he said, you know what?
[02:38:38.400 --> 02:38:44.560]   You might think that this doesn't matter, but these are the people, the railroad commission
[02:38:44.560 --> 02:38:46.640]   is who approves.
[02:38:46.640 --> 02:38:48.160]   I don't know if I can find it.
[02:38:48.160 --> 02:38:51.440]   So don't need to show my, I'll tell you if I get it.
[02:38:51.440 --> 02:38:54.480]   But maybe I can find it.
[02:38:54.480 --> 02:39:02.160]   Texas railroad commissioner are the people who approve one particular thing that could
[02:39:02.160 --> 02:39:04.160]   make a big difference in climate change.
[02:39:04.160 --> 02:39:09.840]   A lot of oil wells in Texas, we've seen it, have flares as they burn off the natural gas,
[02:39:09.840 --> 02:39:12.320]   which is a production of pumping the oil.
[02:39:12.320 --> 02:39:14.640]   They could capture it and resell it, but it's not worth it.
[02:39:14.640 --> 02:39:15.840]   So they just burn it.
[02:39:15.840 --> 02:39:18.000]   And it's a huge source of air pollution.
[02:39:18.000 --> 02:39:20.560]   And it's permitted in Texas.
[02:39:20.560 --> 02:39:22.800]   And it's the railroad commission of Texas.
[02:39:22.800 --> 02:39:24.640]   [laughs]
[02:39:24.640 --> 02:39:27.600]   It determines if this is okay.
[02:39:27.600 --> 02:39:32.400]   They need one more vote to prevent flares.
[02:39:32.400 --> 02:39:37.200]   So I actually donated to the campaign.
[02:39:37.200 --> 02:39:39.360]   Yes, it's a huge amount.
[02:39:39.360 --> 02:39:41.280]   And I thank Corey Doctorow for he tweeted it.
[02:39:41.280 --> 02:39:44.640]   I should find it so we can put it in the show notes.
[02:39:44.640 --> 02:39:48.800]   But if you go to twitter.com/ Is it Corey Doctorow?
[02:39:48.800 --> 02:39:52.240]   Is that his, I can't remember what his Twitter handle is.
[02:39:52.240 --> 02:39:53.440]   That's for him.
[02:39:53.440 --> 02:39:54.640]   Shoot, I don't remember either.
[02:39:54.640 --> 02:39:56.160]   I think it's just Doctorow.
[02:39:56.160 --> 02:39:58.000]   I think it may be just Doctorow.
[02:39:58.000 --> 02:40:00.000]   It is Doctorow.
[02:40:00.000 --> 02:40:00.320]   Yeah.
[02:40:00.320 --> 02:40:03.840]   If you scroll through, you might be able to find it.
[02:40:03.840 --> 02:40:04.560]   It's actually great.
[02:40:04.560 --> 02:40:08.720]   He's a great guy to follow because he tweets a lot of weird old stuff.
[02:40:08.720 --> 02:40:16.320]   Like a 1969 Disneyland coloring book next to the Gorgon portrait for the Haunted Mansion.
[02:40:16.320 --> 02:40:20.400]   Anyway, and there's a Calvin and Hobbes.
[02:40:20.400 --> 02:40:21.840]   And he's flat out burly it.
[02:40:21.840 --> 02:40:23.440]   Yeah, I love Corey.
[02:40:23.440 --> 02:40:24.240]   We're going to get him back.
[02:40:24.240 --> 02:40:26.560]   He's going to be back on Twitter in a few weeks.
[02:40:26.560 --> 02:40:31.680]   The two smartest people I know with the exception of those in the room, of course.
[02:40:31.680 --> 02:40:35.920]   Amy, Webb, and Corey Doctorow are going to do our Halloween
[02:40:35.920 --> 02:40:39.200]   Twitter on October 18th.
[02:40:39.200 --> 02:40:43.360]   And I think Carson has persuaded them to all be wearing costumes.
[02:40:43.360 --> 02:40:45.440]   So it'll be a costume.
[02:40:45.440 --> 02:40:46.480]   As only he does.
[02:40:47.920 --> 02:40:49.520]   Carson, Carson, Carson.
[02:40:49.520 --> 02:40:51.920]   Yeah, October 18th.
[02:40:51.920 --> 02:40:56.080]   And I purchased a costume for the purpose.
[02:40:56.080 --> 02:40:58.800]   So I will also be in costume.
[02:40:58.800 --> 02:41:02.880]   Thank you, everybody, for joining us.
[02:41:02.880 --> 02:41:04.720]   I excellent advice from Ant.
[02:41:04.720 --> 02:41:05.600]   Read your ballot.
[02:41:05.600 --> 02:41:06.640]   Be prepared.
[02:41:06.640 --> 02:41:07.680]   Mark it up.
[02:41:07.680 --> 02:41:10.800]   Bring it in or get an absentee ballot and vote by mail.
[02:41:10.800 --> 02:41:14.880]   But however you do it, November 3rd is going to be a big day.
[02:41:14.880 --> 02:41:20.800]   Mike Elgin, Gastronomad.net.
[02:41:20.800 --> 02:41:22.080]   Yes.
[02:41:22.080 --> 02:41:22.560]   Yes.
[02:41:22.560 --> 02:41:25.920]   And everybody's invited.
[02:41:25.920 --> 02:41:26.800]   We do amazing things.
[02:41:26.800 --> 02:41:28.080]   But check out the website.
[02:41:28.080 --> 02:41:30.240]   Leo, can I just quickly plug chatterbox?
[02:41:30.240 --> 02:41:31.600]   Because I think it's-
[02:41:31.600 --> 02:41:34.720]   Yes, Kevin's incredible project.
[02:41:34.720 --> 02:41:41.040]   Yeah, my son, Kevin Elgin is an entrepreneur who has what I think is a brilliant product.
[02:41:41.600 --> 02:41:44.800]   It's a smart speaker like an Amazon Echo.
[02:41:44.800 --> 02:41:48.160]   And it's for nine years old and up so adults can do it as well.
[02:41:48.160 --> 02:41:49.280]   But you build it yourself.
[02:41:49.280 --> 02:41:54.160]   Then you program it yourself using a very simple visual interface.
[02:41:54.160 --> 02:41:56.640]   Learning concepts about APIs and so on.
[02:41:56.640 --> 02:41:57.600]   No, this is not it.
[02:41:57.600 --> 02:41:58.400]   What is the website?
[02:41:58.400 --> 02:42:00.720]   Hello, chatterbox.com.
[02:42:00.720 --> 02:42:01.920]   Thank you.
[02:42:01.920 --> 02:42:02.720]   I had the wrong side.
[02:42:02.720 --> 02:42:03.360]   You build it yourself.
[02:42:03.360 --> 02:42:04.400]   You program it yourself.
[02:42:04.400 --> 02:42:05.280]   You use APIs.
[02:42:05.280 --> 02:42:08.400]   It can do everything that an Amazon Echo can do and a lot more.
[02:42:08.400 --> 02:42:11.520]   So it's more powerful than an Amazon Echo.
[02:42:11.520 --> 02:42:12.480]   It's super private.
[02:42:12.480 --> 02:42:15.520]   It doesn't listen until you push the button, which is by design.
[02:42:15.520 --> 02:42:24.240]   It's just a fantastic product that teaches you or kids about how smart speakers work,
[02:42:24.240 --> 02:42:26.080]   about how voice interfaces work.
[02:42:26.080 --> 02:42:30.720]   The future of the interface is going to be dominated by voice, I believe.
[02:42:30.720 --> 02:42:34.000]   And so if you really want to prepare kids for the future,
[02:42:34.000 --> 02:42:40.320]   you'll teach them how all that works and teach them that the voice AI systems are not
[02:42:40.960 --> 02:42:42.160]   intelligent, they're not people.
[02:42:42.160 --> 02:42:45.760]   It's all determined by people.
[02:42:45.760 --> 02:42:47.680]   And it's a great project during COVID-19.
[02:42:47.680 --> 02:42:51.440]   And he's got a discount for people who have kids who are in lockdown.
[02:42:51.440 --> 02:42:54.160]   Because it's just endless hours of creativity.
[02:42:54.160 --> 02:42:54.800]   You can build things.
[02:42:54.800 --> 02:42:55.680]   You can turn on the lights.
[02:42:55.680 --> 02:42:57.280]   You can figure out how to do all these different things.
[02:42:57.280 --> 02:42:59.040]   You can send texts.
[02:42:59.040 --> 02:43:00.320]   You can hear music.
[02:43:00.320 --> 02:43:02.400]   You can do all the things that a smart speaker can do.
[02:43:02.400 --> 02:43:07.840]   But it doesn't do a single thing unless you teach it to do that thing.
[02:43:07.840 --> 02:43:12.880]   So it's just a lot of time exploring and learning how language works and all that kind of stuff.
[02:43:12.880 --> 02:43:14.160]   So it's a wonderful project.
[02:43:14.160 --> 02:43:20.320]   Nine years old and up and I recommend everybody get one, whether you have a kid or not.
[02:43:20.320 --> 02:43:26.000]   And there's Kevin working on his little chatter box with his chatter box friends.
[02:43:26.000 --> 02:43:30.080]   And also a great little picture of him.
[02:43:30.080 --> 02:43:33.840]   All the founders pictures are hysterical because they're kids,
[02:43:33.840 --> 02:43:36.000]   which is the kid pictures.
[02:43:36.000 --> 02:43:37.520]   So highly recommended.
[02:43:37.520 --> 02:43:41.200]   This is a great way to teach kids how to think, solve problems,
[02:43:41.200 --> 02:43:45.840]   and a little bit about the future of technology in there too.
[02:43:45.840 --> 02:43:47.680]   Hello chatterbox.com.
[02:43:47.680 --> 02:43:49.920]   So glad to help promote that.
[02:43:49.920 --> 02:43:50.880]   And right now, thank you.
[02:43:50.880 --> 02:43:52.320]   Ten percent discount on the site.
[02:43:52.320 --> 02:43:54.160]   So definitely take advantage of it.
[02:43:54.160 --> 02:43:54.880]   Thank you, Mike.
[02:43:54.880 --> 02:43:55.680]   We'll see you soon.
[02:43:55.680 --> 02:43:58.560]   I hope perhaps it's a good profile.
[02:43:58.560 --> 02:43:59.680]   Who knows?
[02:43:59.680 --> 02:44:01.520]   Jeff Jarvis.
[02:44:01.520 --> 02:44:02.400]   Good to see you, Mike.
[02:44:02.400 --> 02:44:04.800]   It's always great to see you too, Mike.
[02:44:04.800 --> 02:44:05.600]   Mike and Jeff.
[02:44:05.600 --> 02:44:06.560]   It's good to see you too, Leo.
[02:44:06.560 --> 02:44:08.160]   Jeff is, you see me every week.
[02:44:08.160 --> 02:44:08.960]   You couldn't care less.
[02:44:08.960 --> 02:44:13.440]   Jeff is the director of the Townite Center for Entrepreneurial Journalism at the Craig
[02:44:13.440 --> 02:44:18.240]   Neumark, Craig Neumark Graduate School of Journalism at the City University of New York.
[02:44:18.240 --> 02:44:21.600]   His blog is at buzzmachine.com.
[02:44:21.600 --> 02:44:24.640]   Also writes for Medium and other places at Jeff Jarvis.
[02:44:24.640 --> 02:44:26.000]   You keep saying you're working on a book.
[02:44:26.000 --> 02:44:28.000]   What is the ETA for that book?
[02:44:28.000 --> 02:44:28.720]   And what's it about?
[02:44:28.720 --> 02:44:29.120]   God knows.
[02:44:29.120 --> 02:44:30.880]   It's Coughtonburg.
[02:44:30.880 --> 02:44:31.360]   It's Coughtonburg.
[02:44:31.360 --> 02:44:32.160]   The book is good.
[02:44:32.160 --> 02:44:36.000]   Is it going to be a big scholarly tome or a purse book for the rest of this?
[02:44:36.000 --> 02:44:39.280]   No, I wouldn't scholarly you can't attribute to me, but we'll have a lot of footnotes.
[02:44:39.280 --> 02:44:45.280]   I am still blown away from the amount of research I'm reading you do every single day.
[02:44:45.280 --> 02:44:46.400]   It's kind of mind-boggling.
[02:44:46.400 --> 02:44:49.440]   It's very impressive, very, very impressive.
[02:44:49.440 --> 02:44:59.120]   If you're lucky enough to study under Jeff at CUNY's graduate school of entrepreneurial journalism,
[02:44:59.120 --> 02:45:00.000]   you're learning.
[02:45:00.000 --> 02:45:00.560]   You're learning.
[02:45:01.520 --> 02:45:03.280]   And of course, the fabulous Aunt Prueh.
[02:45:03.280 --> 02:45:04.080]   It always a pleasure.
[02:45:04.080 --> 02:45:07.360]   Hand on photography, hands on wellness.
[02:45:07.360 --> 02:45:11.680]   As somebody in the chatroom said, "Ant's going to be running Twitch someday."
[02:45:11.680 --> 02:45:13.360]   And I look forward to that day.
[02:45:13.360 --> 02:45:15.760]   Can you start tomorrow?
[02:45:15.760 --> 02:45:18.640]   Thank you, Aunt.
[02:45:18.640 --> 02:45:19.520]   Thank you, Jeff.
[02:45:19.520 --> 02:45:20.160]   Thank you, Mike.
[02:45:20.160 --> 02:45:21.440]   Thank you all for joining us.
[02:45:21.440 --> 02:45:25.120]   We do this week in Google of a Wednesday afternoon, about 2 p.m.
[02:45:25.120 --> 02:45:26.080]   Pacific, 5 p.m.
[02:45:26.080 --> 02:45:28.800]   Eastern Time 2100 UTC.
[02:45:28.800 --> 02:45:34.000]   Tune in the live video or audio stream at twit.tv/live.
[02:45:34.000 --> 02:45:40.560]   If you're doing that chat in the chatroom at irc.twit.tv, a lot of people listen after the fact.
[02:45:40.560 --> 02:45:41.280]   It's a podcast.
[02:45:41.280 --> 02:45:42.320]   So you can have it on demand.
[02:45:42.320 --> 02:45:47.840]   Any time, anywhere you want, just go to twit.tv/twig to download episodes.
[02:45:47.840 --> 02:45:50.400]   There's a YouTube channel, a Twig YouTube channel.
[02:45:50.400 --> 02:45:54.960]   And of course, you can always get your favorite podcast application and subscribe.
[02:45:54.960 --> 02:45:59.040]   That'll be the best way to get it every single week, the minute it's available.
[02:45:59.040 --> 02:46:00.560]   We are putting together our best of.
[02:46:00.560 --> 02:46:02.160]   The holidays are coming soon.
[02:46:02.160 --> 02:46:08.640]   Very, very quickly, as these Wednesdays string along, one after the other.
[02:46:08.640 --> 02:46:15.680]   So go to twit.tv/bestof if you have a moment from the last year that you'd like to have in our
[02:46:15.680 --> 02:46:17.520]   best of and submit it.
[02:46:17.520 --> 02:46:21.520]   And of course, I should remind you, I forget to mention this, but there's a flash briefing.
[02:46:21.520 --> 02:46:24.000]   And we make one every week for this show.
[02:46:24.000 --> 02:46:26.240]   In fact, all the shows have flash briefings.
[02:46:26.240 --> 02:46:30.640]   If you have an Amazon Echo and you listen to the flash briefing, don't forget to add twit
[02:46:30.640 --> 02:46:35.200]   as one of your sources and you'll get a little bit of twit every morning as you start your day.
[02:46:35.200 --> 02:46:37.920]   Thank you, everybody, for being here.
[02:46:37.920 --> 02:46:40.640]   We'll see you next week on This Week in Google.
[02:46:40.640 --> 02:46:41.040]   Bye-bye.
[02:46:41.760 --> 02:46:46.880]   Hi, I'm Jason Howell, host of All About Android, where each week I'm joined by my co-hosts,
[02:46:46.880 --> 02:46:49.200]   Florence Ion and Ron Richards.
[02:46:49.200 --> 02:46:52.160]   And we talk about everything that has to do with Android.
[02:46:52.160 --> 02:46:53.040]   Is it news?
[02:46:53.040 --> 02:46:54.080]   Is it hardware?
[02:46:54.080 --> 02:46:54.720]   Is it apps?
[02:46:54.720 --> 02:46:56.240]   Well, you name it, we talk about it.
[02:46:56.240 --> 02:46:58.560]   We invite guests from the industry on the show.
[02:46:58.560 --> 02:47:04.240]   We even sometimes have people from the Android team themselves talking about what makes Android
[02:47:04.240 --> 02:47:05.120]   so great.
[02:47:05.120 --> 02:47:10.560]   And you could subscribe so you don't miss anything about the world of Android by going to twit.tv/aa.
[02:47:10.880 --> 02:47:15.520]   We'll see you there.
[02:47:15.520 --> 02:47:21.200]   [Music]
[02:47:21.200 --> 02:47:23.200]   You

