;FFMETADATA1
title=B055man69
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=451
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.080]   It's time for Twig this week in Google Stasis got the day off but
[00:00:04.080 --> 00:00:07.860]   Wendy Nader's sitting in for Stacey from Duo Security. Also,
[00:00:07.860 --> 00:00:13.840]   Pruitt stops by and Jeff Jarvis we will talk of course about Facebook. Jeff and I
[00:00:13.840 --> 00:00:18.600]   are gonna get in a big fight over this one I can tell and Google and a whole
[00:00:18.600 --> 00:00:22.360]   lot more. I want you to stay tuned. This is a fun one. This week in Google's next.
[00:00:22.360 --> 00:00:37.040]   Netcast you love. From people you trust. This is Twig. Bandwidth for this week in
[00:00:37.040 --> 00:00:48.440]   Google is provided by CashFly, C-A-C-H-E-F-L-Y.com. This is Twig this week in
[00:00:48.440 --> 00:00:54.560]   Google episode 451 recorded Wednesday April 4th 2018.
[00:00:54.560 --> 00:01:01.100]   Boss Man 69. It's time for Twig this week in Google to show where we talk about so
[00:01:01.100 --> 00:01:06.920]   much more than Google. It's really poorly named. Facebook of course is gonna be a
[00:01:06.920 --> 00:01:12.840]   big topic today and almost every day every week this year but also Twitter
[00:01:12.840 --> 00:01:16.960]   and whatever else we want to talk about. Stacey's got the day off. That's good
[00:01:16.960 --> 00:01:20.840]   news. It means we have room for other people although we will miss Stacey. Jeff
[00:01:20.840 --> 00:01:26.160]   Jarvis is here. It's great to have you Jeff. Hey, where he teaches journalism.
[00:01:26.160 --> 00:01:32.320]   Buzzmachine.com is his blog. So many books, public parts, what would Google do, etc.
[00:01:32.320 --> 00:01:37.960]   Etc. Pruitt is also with us. Ant is a contributor at Tech Republic. He's been on
[00:01:37.960 --> 00:01:40.880]   Twig before. I don't think he's ever been on Twig before. It's great to have you,
[00:01:40.880 --> 00:01:45.480]   Ant. And you were having me yesterday. This is my first time on Twig. A drone
[00:01:45.480 --> 00:01:50.820]   operator at Parxeloss who has told me I should never again get a drone. He's also
[00:01:50.820 --> 00:01:57.680]   great. Great photographer and a Clemson fan and always a pleasure to have Ant on.
[00:01:57.680 --> 00:02:04.200]   And for the first time on the show also but thrilled to have her when in
[00:02:04.200 --> 00:02:11.720]   neither she is a CISO. What a chief information security officer. It in fact
[00:02:11.720 --> 00:02:17.160]   runs the advisory CISO group at Duo Security which is a great security firm.
[00:02:17.160 --> 00:02:21.360]   Well, welcome Wendy. Great to have you. Thank you. Good to be here. Wendy was on
[00:02:21.360 --> 00:02:26.880]   our panel Stacey and my panel in South by Southwest last month. And I met her and
[00:02:26.880 --> 00:02:30.640]   I said we got to get her on. In fact, we were slowly getting everybody on. I met
[00:02:30.640 --> 00:02:39.080]   Austin by me because I had such a good time. Despite the case. Oh jokes. Ah, ah, ah, Wendy.
[00:02:39.080 --> 00:02:45.040]   Wendy is a test. Have you had Taco Bell case? I mean a church, a potley case. Oh,
[00:02:45.040 --> 00:02:50.640]   a chipotle. The chipotle case. Oh, no, I haven't tried it. No, okay. It lives in
[00:02:50.640 --> 00:02:56.000]   Austin protection. Don't do it, Wendy. Do they even do they even bother having a
[00:02:56.000 --> 00:02:59.960]   Chipotle in Austin? Now they do which is weird, but I don't we do right across
[00:02:59.960 --> 00:03:04.760]   from the from the Duo building as a matter of fact. Oh, wow. Yeah. Yeah. So you had an
[00:03:04.760 --> 00:03:11.920]   opportunity, but you've been wiser. You've been wise. It's not so concerned anymore.
[00:03:11.920 --> 00:03:19.000]   Oh, she's going to do it again with those puns. Oh, yeah. So Jeff, you just got off a
[00:03:19.000 --> 00:03:23.080]   conference call that Facebook had with reporters. What was the what was the well,
[00:03:23.080 --> 00:03:28.760]   there's a few pieces of news about Facebook. We might as well mention Mark Zuckerberg will
[00:03:28.760 --> 00:03:37.960]   be testifying in front of Congress a week. A week from Thursday, April 11th. Excuse me.
[00:03:37.960 --> 00:03:41.640]   That's actually next Wednesday. Oh, it's Wednesday. Sorry. I was going to testify before the
[00:03:41.640 --> 00:03:49.240]   House Energy and Commerce Committee. That'll be very interesting. Preparatory to that
[00:03:49.240 --> 00:03:54.360]   Facebook's been getting very active talking. Oh, you know, before I even bring up Facebook,
[00:03:54.360 --> 00:03:59.880]   I forgot. We really probably should mention our thoughts and prayers and hearts and mind
[00:03:59.880 --> 00:04:04.320]   go out to our friends at YouTube and San Bruno. Of course, there was a shooting yesterday.
[00:04:04.320 --> 00:04:10.280]   The good news. It looks like nobody except the shooter died in that shooting. So that's
[00:04:10.280 --> 00:04:18.280]   relatively good news. It's so unnerving to see someone you know, I saw the Dean LaVrousek
[00:04:18.280 --> 00:04:23.360]   tweeting that he was hold up in a closet in a room because there was an active shooter
[00:04:23.360 --> 00:04:32.280]   and you see that and it brings it all to more home. The weird thing about Vadim is her account
[00:04:32.280 --> 00:04:39.080]   was hacked and that his account was hacked. Some of his accounts, some of his tweets were
[00:04:39.080 --> 00:04:44.960]   in fact fake during the event, but I think the closet one was real. So it's real. There
[00:04:44.960 --> 00:04:51.680]   was a mix of tweets on his account, but you know him. Yeah, that was a very strange thing.
[00:04:51.680 --> 00:04:58.200]   Actually, one of the stories about all of this is how instantly Russian trolls, bots, bad
[00:04:58.200 --> 00:05:05.760]   actors in general jump into the fray to create a mass of phony news, phony stories, phony
[00:05:05.760 --> 00:05:12.160]   tweets. It's they're like predators now just leaping into the action.
[00:05:12.160 --> 00:05:18.800]   So that's sickening. It makes you wonder, what can Twitter do about that because that's
[00:05:18.800 --> 00:05:25.240]   just going to continue to happen. Yeah, I mean, his account was hijacked while
[00:05:25.240 --> 00:05:30.160]   he was still near the scene of the attack. It was 20 minutes after LaVrousek reported
[00:05:30.160 --> 00:05:35.960]   himself safe. One of the tweets leaked to a picture of a popular YouTuber saying, please
[00:05:35.960 --> 00:05:39.840]   help me find my friend. I lost him in the shooting. That's just I mean, this is these.
[00:05:39.840 --> 00:05:44.920]   There's something wrong with these people. Yes. Twitter immediately jumped on it and
[00:05:44.920 --> 00:05:49.440]   that removed the hoax tweets. And then even though more hoax tweets came in, eventually,
[00:05:49.440 --> 00:05:54.640]   I think cleared it up. So somebody was sitting at Twitter watching with the finger on the
[00:05:54.640 --> 00:05:59.000]   button. Anyway, our, our just our thoughts are with our friends.
[00:05:59.000 --> 00:06:03.800]   Yeah, that's all the worst that it's, it's someone who was disgruntled with YouTube,
[00:06:03.800 --> 00:06:12.680]   who decided to go after YouTube and, and when everyone is public, anyone can become a target
[00:06:12.680 --> 00:06:18.680]   of this kind of public rage and sickness. You know, I was actually a little disheartened
[00:06:18.680 --> 00:06:22.160]   and I mean, maybe it's inaccurate, but I'd heard that there wasn't much security at YouTube.
[00:06:22.160 --> 00:06:26.920]   She got in through a public parking garage. She didn't, she didn't need a badge or anything.
[00:06:26.920 --> 00:06:30.760]   I find that hard to believe. I mean, I have many YouTube offices in the years, but, but
[00:06:30.760 --> 00:06:39.120]   at Google, by God, you cannot tailgate. You are checked. Good. You know, we, I mean,
[00:06:39.120 --> 00:06:44.520]   any media company these days has security. We, our doors are locked. We have an armed
[00:06:44.520 --> 00:06:50.200]   guard at the door, an armed guard, a former Marine at the door. He's, you know, he's got
[00:06:50.200 --> 00:06:55.600]   both lethal and non-lethal ways of stopping people and will. And it's really important
[00:06:55.600 --> 00:07:00.720]   fact because of this, he's, uh, Mo told me he's going to be doing some, uh, some exercises
[00:07:00.720 --> 00:07:06.320]   on, um, what do you call it? Active shooter, shooter exercises. Yeah, I've got that's,
[00:07:06.320 --> 00:07:11.680]   very harrowing. I've gone through that with my school and it really is, um, disturbing.
[00:07:11.680 --> 00:07:18.080]   It's a shame that we have to do this that we live in this world is so sad. But, uh,
[00:07:18.080 --> 00:07:22.480]   you know, and I know this isn't even a gun control conversation. She owned the gun.
[00:07:22.480 --> 00:07:27.520]   It was a handgun. It wasn't, you know, um, it's just, it's just sad. I just, it makes
[00:07:27.520 --> 00:07:32.880]   me so sad. Anyway, it's the fact that if someone thinks it's okay to go in and cause
[00:07:32.880 --> 00:07:37.440]   serious harm on another, she was obviously disturbed. I mean, she was a vegan, right? She
[00:07:37.440 --> 00:07:42.080]   had a vegan. She didn't want, she hated cruelty to animals, but she'd go, she humans. I obviously,
[00:07:42.080 --> 00:07:46.400]   this is a disturbed person. It's not a father or reporter. This police, the police had talked
[00:07:46.400 --> 00:07:50.800]   to her, but the laws are such too that until you've done something, you know, that's the
[00:07:50.800 --> 00:07:54.480]   terrible paradox of our harm laws, you can't stop them until they've done something that is too late.
[00:07:54.480 --> 00:07:59.120]   We've had, you know, uh, we have a, we have a fan who's, um, unfortunately has some mental
[00:07:59.120 --> 00:08:04.400]   illness issues who has threatened me many times and we've, I've talked to police about it. They know
[00:08:04.400 --> 00:08:09.920]   about it. There's nothing you can do. Yeah. Nothing you can see. Unfortunately. Yeah.
[00:08:09.920 --> 00:08:17.200]   Uh, anyway, so there's nothing much to say except I'm, I'm just how sad that is. And yes, I don't,
[00:08:17.200 --> 00:08:22.480]   I don't know what you do. It's just, this is the way it is in the world today. Now on, on to Facebook,
[00:08:22.480 --> 00:08:28.480]   among other things we learned today that Facebook, uh, has just told, uh, maybe it was on this call.
[00:08:28.480 --> 00:08:34.480]   I bet it was told reporters that, oh, it wasn't 50 million people came for
[00:08:34.480 --> 00:08:40.080]   genetic. I had information on it was 87 million. It was before the call, but what Zuckerberg did
[00:08:40.080 --> 00:08:45.520]   say on this call was that the 50 million figure never came from Facebook. Ah, that they, they
[00:08:45.520 --> 00:08:49.840]   wanted to, and you know, they should have said, we're not sure about that number, uh, but they did
[00:08:49.840 --> 00:08:57.440]   the research and said that, uh, today said that they, um, looked at the maximum number of accounts
[00:08:58.080 --> 00:09:04.160]   that could, you know, one person plays the game. All the people, this is back in 2014, all the people
[00:09:04.160 --> 00:09:09.120]   who were friends with that person, their public data could have been scraped at the time. What
[00:09:09.120 --> 00:09:15.280]   was the maximum number of friends each user of that game had at that or that quiz had at the time?
[00:09:15.280 --> 00:09:20.640]   And that leads to the 87 million. Um, and he said that's the maximum they believe. That's the first
[00:09:20.640 --> 00:09:26.400]   number they came out with. He was also asked about, um, you know, again, this goes back to 2014,
[00:09:26.960 --> 00:09:32.480]   uh, you know, they were told that the data had been deleted. They got rid of the app. They,
[00:09:32.480 --> 00:09:39.440]   and so on at the time, but more, more, more, more, more in sense. Uh, April 9th, Monday,
[00:09:39.440 --> 00:09:44.320]   Facebook is going to notify what people who had were exposed to this.
[00:09:44.320 --> 00:09:54.400]   And let's be clear here. This was your public data. So a, a researcher was able to get access to
[00:09:54.400 --> 00:09:58.400]   through the API, you know, easily, but nothing you probably couldn't have done through search
[00:09:58.400 --> 00:10:01.360]   to the public data that you had in your account at the time.
[00:10:01.360 --> 00:10:08.160]   All of your, uh, for instance, all of your likes are public. Your likes. Right. And, uh,
[00:10:08.160 --> 00:10:12.880]   Cambridge Analytica was particularly interested in likes because it turns out with enough likes on
[00:10:12.880 --> 00:10:17.440]   any individual, you can predict fairly accurately a lot of things about that individual, including
[00:10:17.440 --> 00:10:24.560]   their mental state, their political views, their race, their religion, their sex on and on.
[00:10:24.560 --> 00:10:28.240]   I worked with media companies at the time that were eagerly doing the exact same thing.
[00:10:28.240 --> 00:10:33.360]   Sure. And don't you think people still are? Well, you can see the latest place.
[00:10:33.360 --> 00:10:40.240]   Where do you draw it online? Because it seems like every online business or heck,
[00:10:40.240 --> 00:10:45.600]   just, um, broadcast and company or whatever, they have to do some type of data analytics and
[00:10:45.600 --> 00:10:51.120]   they're going to go out and scrape whatever they can scrape. You're right. I mean, heck, even the
[00:10:51.120 --> 00:10:59.120]   Twitter network does an annual survey. Yeah. That's voluntary. We go to you and say, take it if you
[00:10:59.120 --> 00:11:05.040]   want, uh, but we, yeah, advertisers want that information, but, but that's kind of the hard rock
[00:11:05.040 --> 00:11:10.320]   and hard place that a lot of media companies are in between radio and TV. They don't know that much
[00:11:10.320 --> 00:11:14.960]   about their viewers. And that's one of the reasons. What is it? 80% of advertising is now
[00:11:14.960 --> 00:11:19.200]   snarfed up by Google and Facebook. No, I don't think it's 80% of that large number.
[00:11:19.200 --> 00:11:24.320]   It's that much growth in advertising because they're because they can offer that. I mean,
[00:11:24.320 --> 00:11:28.880]   and you're right. If you go back when I worked at the old, ancient days of magazines and worked at
[00:11:28.880 --> 00:11:34.720]   Time Inc, uh, they had data from you and places like Axiom, big data warehouses,
[00:11:34.720 --> 00:11:39.360]   where they had tons of behavioral data about you as an individual with your name and your address
[00:11:39.360 --> 00:11:44.320]   and your phone number and your credit card number, um, going way back. In some ways,
[00:11:44.320 --> 00:11:48.320]   you actually there's actually less data available through the social platforms when it comes to
[00:11:48.320 --> 00:11:53.840]   that kind of PII and behavior. Facebook, there's more analytical ability to now say what are these
[00:11:53.840 --> 00:11:58.000]   likes and what do your habits say about you? You know, in the old days you drove a Volvo,
[00:11:58.000 --> 00:12:04.720]   you're, you're, you're liberal. You know, according to pivotal Facebook and Google account for 73%
[00:12:04.720 --> 00:12:10.480]   of all digital advertising in the U.S. Digital digital. Yeah. Big difference. Yeah. But, but a lot,
[00:12:10.480 --> 00:12:14.720]   I would, I would be willing to venture that a lot of broadcast advertising is moving digital.
[00:12:14.720 --> 00:12:21.520]   Oh, yeah. Oh, the other thing that Zuckerberg said today was, so the report yesterday that he
[00:12:21.520 --> 00:12:29.200]   had told Reuters that he would not be extending Europe's GDPR privacy regulations to the, to the
[00:12:29.200 --> 00:12:33.920]   world. That's a little funny. And he said, he said that was up, up, up, up. See, you're already
[00:12:33.920 --> 00:12:37.520]   heading down the fake news path there, Leo. You're already heading down the presumptions.
[00:12:37.520 --> 00:12:41.920]   Zuckerberg said today that that was not true. What he told the reporter was that, yes, they'd be
[00:12:41.920 --> 00:12:47.520]   extending the standards of GDPR globally. He said they're going to be different in some countries
[00:12:47.520 --> 00:12:53.040]   and how it's done because there's different laws. And so he said that he would be, they would be
[00:12:53.040 --> 00:12:57.600]   setting the standards of GDPR globally. That's a very good. That's a murder support.
[00:12:57.600 --> 00:13:02.480]   So the reporter just misreported it. That's what Facebook is saying. Okay.
[00:13:03.440 --> 00:13:06.960]   You'll forgive me if I don't fully trust everything that comes out of Mark's mouth.
[00:13:06.960 --> 00:13:14.240]   I know. I know. I know he's a nice guy. I'm sure he is. The other weird moments were
[00:13:14.240 --> 00:13:19.280]   when, and I listened into the whole call, that's why we're delayed starting from what I'm glad.
[00:13:19.280 --> 00:13:24.960]   I'm glad you did. Yeah. This is. And so then I thought awkward moment.
[00:13:24.960 --> 00:13:30.240]   F. Key reporter asks, well, has your board of directors discussed whether you're the right
[00:13:30.240 --> 00:13:36.000]   person to lead the company? Beat pause Zuckerberg said not to my knowledge.
[00:13:36.000 --> 00:13:38.560]   That is another reporter. Yeah.
[00:13:38.560 --> 00:13:43.760]   Basically asked the same thing again. They're going after their brother trying to fuel this
[00:13:43.760 --> 00:13:53.200]   narrative of firing Zuck. Then a Buzzfeed reporter did Zuck act pause as if he didn't
[00:13:53.200 --> 00:13:58.640]   had not heard that? Like, no, it's kind of shocking to you. Somebody's asking you that to your face.
[00:13:58.640 --> 00:13:59.920]   Does your board think you should be fired?
[00:13:59.920 --> 00:14:04.160]   Do they all just least think you should be fired? Probably.
[00:14:04.160 --> 00:14:06.000]   I was looking to be there.
[00:14:06.000 --> 00:14:16.000]   Almost almost certainly at time. Certainly from the load of the dishwasher.
[00:14:16.000 --> 00:14:23.920]   Yeah. Well, I mean, those convert is Mark's sitting on all the board meetings?
[00:14:25.360 --> 00:14:28.320]   Oh, sure. I'm sure he's on the board. But I mean, wonder.
[00:14:28.320 --> 00:14:29.920]   No, except for the session.
[00:14:29.920 --> 00:14:32.560]   Yeah. I wonder if.
[00:14:32.560 --> 00:14:36.800]   Yeah. I mean, I wonder if the board might say, Mark, could you leave there when we'd like to have
[00:14:36.800 --> 00:14:41.040]   a private conversation? Oh, there's a board meeting I've been a part of.
[00:14:41.040 --> 00:14:42.880]   There is always executive session built in. Yes.
[00:14:42.880 --> 00:14:46.480]   And you don't sit in on a compensation committee here.
[00:14:46.480 --> 00:14:50.160]   It's an executive session. Yeah. He's nodding on those.
[00:14:50.160 --> 00:14:53.520]   And yeah, and they don't have to say what they're going to talk about. It's normal.
[00:14:54.320 --> 00:14:58.480]   No. So he doesn't know. The other thing that was interesting was the Buzzfeed's reporter.
[00:14:58.480 --> 00:15:04.240]   And Buzzfeed depended greatly on Facebook and now it has to be a little bit bitter
[00:15:04.240 --> 00:15:09.920]   because of the public content shift. And so the Buzzfeed reporter said,
[00:15:09.920 --> 00:15:14.000]   this is a wonderful naivete of the journalist. Well, couldn't you ever think of me better if
[00:15:14.000 --> 00:15:19.520]   you just made less money? Oh, my God. Yeah, everything would be better if we all made
[00:15:19.520 --> 00:15:22.800]   like much. He should. Marcia just say, wouldn't it be better if you made less money?
[00:15:22.800 --> 00:15:29.200]   Come on. You know, Tristan Harris has been out there attacking Facebook along with Roger
[00:15:29.200 --> 00:15:34.320]   McMead. I was at sessions where Tristan, but what goes further and says, get rid of all
[00:15:34.320 --> 00:15:41.280]   advertising. That's a solution. How is that? Well, exactly. And then I did a post last week.
[00:15:41.280 --> 00:15:46.560]   And I started at all the end of the post. I'm very demanding. I'm faced with a lot of ways.
[00:15:46.560 --> 00:15:51.120]   But at the beginning, I took on Matthew Iglesias, who's a very good political commentator at Vox,
[00:15:51.120 --> 00:15:55.200]   but I took him on. He was saying he was telling Zuckerberg to shut down Facebook.
[00:15:55.200 --> 00:16:02.480]   And come on, people. Come on. It's got this is what I talk about. No, no, and why should it?
[00:16:02.480 --> 00:16:07.760]   Even Stephen Vadianathian, who's wonderful and who's doing a critical book on Facebook coming
[00:16:07.760 --> 00:16:12.720]   out soon. You know, he answers and says, when you people say delete Facebook, you're leaving,
[00:16:13.440 --> 00:16:17.840]   that's a position of privilege to say. There are people who depend upon it. Oh, that's BS. I don't
[00:16:17.840 --> 00:16:22.160]   buy that. You stay in fix. No, there are people in countries. If you're to India, people depend
[00:16:22.160 --> 00:16:26.560]   upon Facebook, you're to some countries. And what he's saying is don't delete it. Fix it.
[00:16:26.560 --> 00:16:32.160]   Stay in the pressure. I deleted it. I'm sorry. That's be fake. So equating Facebook with
[00:16:32.160 --> 00:16:38.640]   privilege is BS. Facebook is, I mean, maybe it's it's a luxury. It's a thing. You don't need
[00:16:38.640 --> 00:16:43.600]   Twitter. You don't need Facebook for crying out loud. I don't buy that. That's ridiculous.
[00:16:43.600 --> 00:16:49.280]   Let's see. But hold on. The link is in there to see if it's post. I mean, I know I can't claim
[00:16:49.280 --> 00:16:53.440]   to have a deep understanding of why Facebook might be critical to life in India. But I find that
[00:16:53.440 --> 00:17:00.560]   hard to believe. Hold on here. Let me go to see that. I think about the use case of Facebook with
[00:17:00.560 --> 00:17:08.400]   like my family back home. That's all they use. There's no phone calls. There's always Facebook
[00:17:08.400 --> 00:17:14.800]   messenger or Facebook post or what have you to keep in touch with all of the family that's abroad
[00:17:14.800 --> 00:17:20.160]   that you know, that actually got out of town. So you're saying if you were to delete Facebook,
[00:17:20.160 --> 00:17:25.680]   you would just be disconnecting from that. I have gotten rid of Facebook and deleted it.
[00:17:25.680 --> 00:17:31.920]   I did so back in January. And yeah, I don't hear from anybody in my family except for my mother.
[00:17:31.920 --> 00:17:37.600]   That's too bad. I mean, it's just it is what it is. Facebook has turned into the telephone.
[00:17:37.600 --> 00:17:41.040]   It strikes me that that's all the more reasons. I believe Facebook. But okay.
[00:17:41.040 --> 00:17:44.560]   Yeah. Not because you wanted this connect with family, but because we don't want to have a world
[00:17:44.560 --> 00:17:50.240]   in which Facebook is a necessity. So Siva wrote this in the New York Times. And I quote,
[00:17:50.240 --> 00:17:54.960]   "People in those countries," which is to say, India, Egypt, Indonesia, Philippines, Brazil,
[00:17:54.960 --> 00:17:58.480]   Mexico. People in those countries are getting value out of Facebook. In some places, it's one
[00:17:58.480 --> 00:18:03.600]   of the few reliable ways to keep in touch in much of the developing world. Facebook is also the only
[00:18:03.600 --> 00:18:08.240]   source news source that matters. This should horrify us, but it's not a problem that will be
[00:18:08.240 --> 00:18:13.040]   solved by indignant Americans leaving the service. Why? Moreover, putting Facebook lets Google and
[00:18:13.040 --> 00:18:18.480]   Twitter off the hook. It lets AT&T and Comcast and it's peers off the hook. The dangers of
[00:18:18.480 --> 00:18:22.560]   extremist propaganda and hate speech are just as brave on YouTube, which is owned by Google.
[00:18:22.560 --> 00:18:27.760]   Okay. True. It's very good. I'm saying I can't quit Facebook because unless I quit everybody else,
[00:18:27.760 --> 00:18:33.120]   no. No, he's saying no. He's saying you rich white American says,
[00:18:33.120 --> 00:18:38.160]   well, I did my part. I quit Facebook for the world. No, okay. I agree. I'm not going to say I did my
[00:18:38.160 --> 00:18:44.160]   part or it's done or I've done everything I can do for privacy. No, but at the same time,
[00:18:44.160 --> 00:18:48.400]   you cannot give me any argument that says I have to be part of that piece of crap. You don't have
[00:18:48.400 --> 00:18:54.000]   to be part of it, but to act like you've solved the problem by doing that. No, I'm not saying that.
[00:18:54.000 --> 00:18:59.200]   That's why we're still talking about it. But no, absolutely not. But I don't think I'm going to
[00:18:59.200 --> 00:19:05.040]   stay on Facebook to save it from within. That's an illusion as well. What he's saying, he's not
[00:19:05.040 --> 00:19:09.200]   telling you to stay on Facebook. He's telling you that to stand up as people have been doing
[00:19:09.200 --> 00:19:15.360]   in their Twitter campaign and Trumpeting, "We quit Facebook is a glib." I just find it creepy and I
[00:19:15.360 --> 00:19:21.360]   don't want anything to do with it. And I don't, there's no justification. You can stay. I'm not
[00:19:21.360 --> 00:19:26.080]   saying you should leave, but I don't want anything. I don't want to go anywhere near it. And everything
[00:19:26.080 --> 00:19:33.600]   I hear about Facebook makes me glad I'm not there anymore. I quote his end. So go ahead and quit
[00:19:33.600 --> 00:19:38.080]   Facebook if it makes you feel calmer or more productive. Please realize though that you might be
[00:19:38.080 --> 00:19:42.240]   offloading problems under those who may have less opportunity to protect privacy and dignity
[00:19:42.240 --> 00:19:46.240]   and are more vulnerable to threats to democracy. If the people who care the most about privacy,
[00:19:46.240 --> 00:19:50.960]   accountability, and civil discourse evacuate Facebook and discuss, the entire platform becomes
[00:19:50.960 --> 00:19:55.920]   less informed and diverse. Deactivation is the opposite of activism, says Siva.
[00:19:55.920 --> 00:20:00.000]   Couldn't disagree. Siva is no friend of Facebook. I love Siva a lot, but I couldn't disagree more.
[00:20:00.000 --> 00:20:08.000]   Mr Jarvis, now just listening to that. And I think about the gazillion conversations that I've
[00:20:08.000 --> 00:20:14.640]   tried to have with people that were in my circles, if you will, on Facebook about privacy and about,
[00:20:14.640 --> 00:20:20.720]   you know, just being careful with what's posted and what's being liked and so forth. And about
[00:20:20.720 --> 00:20:28.880]   N% gave a crap. Nobody cares outside of it. You try to inform people and they don't care. What
[00:20:28.880 --> 00:20:35.120]   they care most about is the, this is me right now and this is the bragadocious, be happy for me,
[00:20:35.120 --> 00:20:40.880]   or this is the woe is me. Y'all need to send out good vibes from it. That's all Facebook is.
[00:20:40.880 --> 00:20:46.880]   There when you want to try to share legitimate quality information, personally, I've gotten
[00:20:46.880 --> 00:20:49.360]   nothing from it. Nothing. Nobody gave a crap.
[00:20:49.360 --> 00:20:57.520]   I just don't want to add any value to Facebook by being a member of Facebook. And I think that
[00:20:57.520 --> 00:21:03.360]   that's, no, I do think that's a club. I think that, again, what I've said before is I think that
[00:21:03.360 --> 00:21:09.520]   you're doing what Matthew and Lacey is did and you're declaring it broken. And for most people,
[00:21:09.520 --> 00:21:14.000]   it is not. That's, that's the media narrative that drives me kind of crazy here. And when you
[00:21:14.000 --> 00:21:17.840]   do that to Facebook, by the way, I'm going to contend that you're doing it to the internet,
[00:21:17.840 --> 00:21:21.360]   because what you're opening the door to is regulation that I believe is going to be
[00:21:21.360 --> 00:21:27.760]   highly damaging to the internet as a whole. Quite the contrary. I think the existence of
[00:21:27.760 --> 00:21:33.200]   Facebook is damaging to the internet that Facebook is eating the internet alive, that Facebook would
[00:21:33.200 --> 00:21:40.000]   like to be the internet, especially in developing nations where internet.org is really Facebook.com.
[00:21:40.000 --> 00:21:43.920]   And Facebook should be shut down, frankly. I think Facebook should be shut down.
[00:21:43.920 --> 00:21:45.920]   Oh, come on, Dale. I know you're playing the...
[00:21:45.920 --> 00:21:47.680]   Absolutely. No, I absolutely believe that.
[00:21:47.680 --> 00:21:49.760]   I'm not going to say shut it down. I'm not going to say it down.
[00:21:49.760 --> 00:21:51.440]   Thank you, Hinch.
[00:21:51.440 --> 00:21:56.800]   I think we can go over now, Hinch. I think it should be shut down. I think it's a blight on the face of
[00:21:56.800 --> 00:21:58.240]   the internet.
[00:21:58.240 --> 00:22:02.800]   And we've been waiting for Stacy. You should have a device that comes out of her screen.
[00:22:03.280 --> 00:22:07.520]   The little fist on it and pops Leo when he goes on the board.
[00:22:07.520 --> 00:22:09.120]   You should have that.
[00:22:09.120 --> 00:22:11.440]   I think I should have that.
[00:22:11.440 --> 00:22:15.360]   I know an apology to Wendy, because she's been dragged into the show.
[00:22:15.360 --> 00:22:18.080]   She's gone, "What the hell did I agree to be part of?"
[00:22:18.080 --> 00:22:18.560]   Sorry.
[00:22:18.560 --> 00:22:23.280]   This is just how we are when they don't mind us. We just like to bicker.
[00:22:23.280 --> 00:22:31.040]   All right. I mean, there's the two polar points of view.
[00:22:32.560 --> 00:22:39.840]   I do see... Here's another story. Facebook, apparently, scans your Facebook messenger
[00:22:39.840 --> 00:22:42.480]   messages for offensive content and then blocks them.
[00:22:42.480 --> 00:22:47.920]   Did you know that if you're using Facebook messenger, that Facebook's reading every message
[00:22:47.920 --> 00:22:49.680]   you sent and looking for offensive content?
[00:22:49.680 --> 00:22:51.760]   Facebook has decided...
[00:22:51.760 --> 00:22:54.880]   So, while you're both sides, people are trying to say Facebook should be cleaning up the world.
[00:22:54.880 --> 00:22:57.680]   When Facebook cleans up the world, I don't like that either.
[00:22:57.680 --> 00:23:00.960]   But don't people have the presumption that their message is private?
[00:23:01.920 --> 00:23:05.200]   I mean, if you're a bad guy, you're not using Facebook anyway.
[00:23:05.200 --> 00:23:05.680]   You're using...
[00:23:05.680 --> 00:23:07.280]   Yeah, that's true.
[00:23:07.280 --> 00:23:07.520]   Right.
[00:23:07.520 --> 00:23:10.640]   If you're a bad guy, you said Facebook, you probably...
[00:23:10.640 --> 00:23:12.160]   The word should go forth.
[00:23:12.160 --> 00:23:12.720]   All right.
[00:23:12.720 --> 00:23:13.200]   Yeah.
[00:23:13.200 --> 00:23:15.600]   You're not very good at your job.
[00:23:15.600 --> 00:23:21.120]   Yeah. Somebody's saying, "I sound like a temperate evangelist.
[00:23:21.120 --> 00:23:24.720]   Temperance evangelist. Facebook is demon rum."
[00:23:24.720 --> 00:23:27.440]   You tell them, chat room.
[00:23:27.440 --> 00:23:28.320]   Who said that in the chat room?
[00:23:28.320 --> 00:23:29.680]   Look at the product.
[00:23:29.680 --> 00:23:30.720]   Punter Joe.
[00:23:30.720 --> 00:23:31.680]   Punter Joe.
[00:23:31.680 --> 00:23:32.240]   Punter Joe.
[00:23:32.240 --> 00:23:33.040]   Right you are.
[00:23:33.040 --> 00:23:33.840]   Yes.
[00:23:33.840 --> 00:23:36.480]   This is moral panic.
[00:23:36.480 --> 00:23:38.800]   No, I so disagree with you, Jeff, as you know.
[00:23:38.800 --> 00:23:42.080]   In the more we learn about Facebook, the more...
[00:23:42.080 --> 00:23:45.520]   evil it appears to be.
[00:23:45.520 --> 00:23:46.080]   And I just...
[00:23:46.080 --> 00:23:49.920]   He quotes researcher Ashley Crossman, defining moral panic as,
[00:23:49.920 --> 00:23:54.880]   "A widespread fear, most often an irrational one, that someone or something is a threat to the
[00:23:54.880 --> 00:23:57.760]   values, safety, and interest of a community or society in large."
[00:23:57.760 --> 00:23:57.760]   Do you think...
[00:23:57.760 --> 00:23:58.160]   Wait a minute.
[00:23:58.160 --> 00:24:01.520]   Typically, moral panic is perpetuated by news media.
[00:24:01.520 --> 00:24:02.880]   Do you think I'm ill-informed?
[00:24:02.880 --> 00:24:04.880]   Do you think I'm ill-informed on Facebook?
[00:24:04.880 --> 00:24:05.520]   I think this...
[00:24:05.520 --> 00:24:08.160]   Yes, I think that you're acting as...
[00:24:08.160 --> 00:24:10.960]   I think I'm very, very well-informed on Facebook.
[00:24:10.960 --> 00:24:11.440]   I actually quote...
[00:24:11.440 --> 00:24:11.920]   I actually quote...
[00:24:11.920 --> 00:24:12.880]   You think for everyone...
[00:24:12.880 --> 00:24:15.440]   You think for everyone of the two billion people who are on Facebook,
[00:24:15.440 --> 00:24:19.280]   that they are just a thrust into the faces of Nazis every day.
[00:24:19.280 --> 00:24:19.920]   They don't...
[00:24:19.920 --> 00:24:22.800]   I think they don't know what they're giving up to Facebook.
[00:24:22.800 --> 00:24:24.080]   I want the latest.
[00:24:24.080 --> 00:24:25.120]   I want the latest.
[00:24:25.120 --> 00:24:26.560]   No, it's just the latest Facebook.
[00:24:26.560 --> 00:24:28.560]   Facebook hides that.
[00:24:28.560 --> 00:24:30.640]   Facebook actively hides that.
[00:24:30.640 --> 00:24:32.800]   Did they tell you they were scanning your messages?
[00:24:32.800 --> 00:24:34.400]   Are we just...
[00:24:34.400 --> 00:24:37.040]   Did they tell you they were collecting your Android call history?
[00:24:37.040 --> 00:24:39.200]   Did they tell you...
[00:24:39.200 --> 00:24:39.680]   Yes, they did.
[00:24:39.680 --> 00:24:41.920]   Did they say, "Hey, by the way, we're at a warning.
[00:24:41.920 --> 00:24:45.440]   We're keeping track of all the phone calls you make, how long they last and who you're calling.
[00:24:45.440 --> 00:24:46.560]   Did they tell people that?
[00:24:46.560 --> 00:24:47.600]   Absolutely not."
[00:24:47.600 --> 00:24:52.000]   Facebook actively dissembles when they get caught.
[00:24:52.000 --> 00:24:54.160]   They apologize.
[00:24:54.160 --> 00:24:55.840]   And then they go right ahead and do it.
[00:24:55.840 --> 00:25:00.720]   And in my post, I argue that they need to do a very harsh and outside moral audit in the company.
[00:25:00.720 --> 00:25:01.280]   I don't disagree with them.
[00:25:01.280 --> 00:25:02.240]   That's moral panic.
[00:25:02.240 --> 00:25:02.960]   How dare you?
[00:25:02.960 --> 00:25:04.880]   How dare you criticize them?
[00:25:04.880 --> 00:25:09.840]   In this way, says Crossman, moral panic and foster increased social control.
[00:25:09.840 --> 00:25:11.440]   That's what I fear.
[00:25:11.440 --> 00:25:12.560]   Here's the social control.
[00:25:12.560 --> 00:25:14.800]   Comic books are ruining youth.
[00:25:14.800 --> 00:25:18.240]   I'm not suggesting that Facebook be banned.
[00:25:18.240 --> 00:25:20.560]   I would like to see it regulated, but I don't have any...
[00:25:20.560 --> 00:25:22.560]   You said shut it down, Lee.
[00:25:22.560 --> 00:25:23.520]   No, I didn't say shut it down.
[00:25:23.520 --> 00:25:24.960]   I said it should be shut down.
[00:25:24.960 --> 00:25:25.840]   But I didn't say you could...
[00:25:25.840 --> 00:25:28.160]   Well, I would be offended if they're...
[00:25:28.160 --> 00:25:31.920]   But no, government doesn't shut down private industry unless they break laws.
[00:25:31.920 --> 00:25:34.080]   Well, this president who knows, he's going after Amazon.
[00:25:34.080 --> 00:25:37.360]   He's never going after Facebook.
[00:25:37.360 --> 00:25:38.880]   I could pretty much guarantee that.
[00:25:38.880 --> 00:25:41.760]   He's looking forward to the year 2020.
[00:25:41.760 --> 00:25:45.680]   He's got his Facebook team already put together, I'm sure.
[00:25:45.680 --> 00:25:48.480]   Don't you think Jared's already thinking about what are we...
[00:25:48.480 --> 00:25:50.160]   How are we going to use Facebook at 2018?
[00:25:50.160 --> 00:25:52.400]   Don't you think?
[00:25:52.400 --> 00:25:54.480]   Don't you think?
[00:25:55.440 --> 00:25:56.560]   Not today should be stopped.
[00:25:56.560 --> 00:25:58.880]   I'm sorry, let me just finish this post.
[00:25:58.880 --> 00:25:59.680]   I'll be right with you.
[00:25:59.680 --> 00:26:04.400]   All right, thank you.
[00:26:04.400 --> 00:26:05.280]   Thank you, Wendy.
[00:26:05.280 --> 00:26:08.640]   I think we agreed to leave Wendy out.
[00:26:08.640 --> 00:26:09.840]   It was a strong statement.
[00:26:09.840 --> 00:26:12.560]   I think we leave me out of this language.
[00:26:12.560 --> 00:26:13.680]   No, no, no, no.
[00:26:13.680 --> 00:26:15.680]   I hate it when mommy and daddy find out.
[00:26:15.680 --> 00:26:16.240]   I hate the audience.
[00:26:16.240 --> 00:26:17.200]   We were purposely...
[00:26:17.200 --> 00:26:18.640]   I think Wendy is the same one.
[00:26:18.640 --> 00:26:19.520]   Yes.
[00:26:19.520 --> 00:26:21.840]   Who opted out at the beginning and we can move on to other things.
[00:26:21.840 --> 00:26:23.280]   That's why she's here.
[00:26:23.920 --> 00:26:24.880]   Let's see.
[00:26:24.880 --> 00:26:27.920]   Should we talk about Google?
[00:26:27.920 --> 00:26:29.920]   Sure.
[00:26:29.920 --> 00:26:31.200]   There are changes at Google.
[00:26:31.200 --> 00:26:31.840]   Letters.
[00:26:31.840 --> 00:26:32.400]   Yes.
[00:26:32.400 --> 00:26:33.440]   Well, for one thing.
[00:26:33.440 --> 00:26:35.520]   Now, here's the question.
[00:26:35.520 --> 00:26:41.600]   Did Gian Andrea get hired away by Apple and then Google said,
[00:26:41.600 --> 00:26:43.200]   "Oh, yeah, he's not here anymore."
[00:26:43.200 --> 00:26:46.560]   Or did he leave and then go to Apple?
[00:26:46.560 --> 00:26:47.520]   What's the timeline?
[00:26:47.520 --> 00:26:48.480]   The story was...
[00:26:48.480 --> 00:26:51.920]   So Ben Gomes, who I like, who got elevated as the
[00:26:52.640 --> 00:26:54.800]   head of search and what's the name?
[00:26:54.800 --> 00:26:56.240]   Jeff Dean takes over AI.
[00:26:56.240 --> 00:26:59.440]   That got announced and they said that their former boss
[00:26:59.440 --> 00:27:01.040]   was going to stay at the company.
[00:27:01.040 --> 00:27:04.800]   So if he knew he was leaving, you'd think that they wouldn't have announced it that way.
[00:27:04.800 --> 00:27:08.080]   So he is the...
[00:27:08.080 --> 00:27:11.680]   He was the AI genius, right, at Google.
[00:27:11.680 --> 00:27:15.680]   I think Apple pounced is what that looks like to me.
[00:27:15.680 --> 00:27:17.680]   Apple saw an opportunity.
[00:27:17.680 --> 00:27:20.800]   We'll cover whatever separation agreement stuff.
[00:27:20.800 --> 00:27:21.680]   We'll pay for that.
[00:27:21.680 --> 00:27:23.440]   We're going to steal their AI guy.
[00:27:23.440 --> 00:27:27.200]   So there was some obviously political flight about AI within the company.
[00:27:27.200 --> 00:27:28.640]   I don't have any idea what it was.
[00:27:28.640 --> 00:27:29.600]   It's fascinating.
[00:27:29.600 --> 00:27:34.320]   There's always something else going on that the media is not going to know for sure.
[00:27:34.320 --> 00:27:38.400]   And there's always some type of water cooler talk that we're not going to get.
[00:27:38.400 --> 00:27:42.640]   Gian Andrea was the founder of something called MetaWeb,
[00:27:42.640 --> 00:27:46.640]   which Google bought in 2010.
[00:27:46.640 --> 00:27:49.680]   And that's how he joined Google MetaWeb.
[00:27:51.360 --> 00:27:56.720]   Helped websites link information based on connections between people, places, and things instead of words.
[00:27:56.720 --> 00:27:59.280]   Does anybody remember MetaWeb?
[00:27:59.280 --> 00:28:00.640]   I don't even know.
[00:28:00.640 --> 00:28:01.120]   No, I'm not either.
[00:28:01.120 --> 00:28:05.920]   It must have been doing something kind of interesting.
[00:28:05.920 --> 00:28:10.560]   So the idea was it was more natural language search queries.
[00:28:10.560 --> 00:28:11.520]   Develop Freebase.
[00:28:11.520 --> 00:28:13.840]   Okay.
[00:28:13.840 --> 00:28:15.600]   Freebase was...
[00:28:15.600 --> 00:28:18.480]   An open shared database of the world's knowledge.
[00:28:18.480 --> 00:28:18.720]   Right.
[00:28:18.720 --> 00:28:20.000]   Right.
[00:28:20.000 --> 00:28:21.120]   It was an open database.
[00:28:21.120 --> 00:28:21.680]   Danny Hillis.
[00:28:21.680 --> 00:28:23.520]   Oh, Danny Hillis.
[00:28:23.520 --> 00:28:23.840]   All right.
[00:28:23.840 --> 00:28:26.720]   Well, there's another legendary name in technology.
[00:28:26.720 --> 00:28:31.520]   So why are in 2010, shut down Freebase 2016.
[00:28:31.520 --> 00:28:33.520]   God bless Wikipedia.
[00:28:33.520 --> 00:28:42.480]   So Gian Andrea became head of, after I'm sure a little bit of time,
[00:28:42.480 --> 00:28:45.280]   search at Google, right?
[00:28:45.280 --> 00:28:46.960]   As well as AI.
[00:28:46.960 --> 00:28:52.960]   As well as AI. He is now going to run Apple's machine learning and AI strategy,
[00:28:52.960 --> 00:28:56.240]   one of 16 executives who report directly to Tim Cook.
[00:28:56.240 --> 00:28:59.600]   Time said the hire is a victory for Apple.
[00:28:59.600 --> 00:29:03.920]   Apple's been behind, of course, with AI and Siri.
[00:29:03.920 --> 00:29:05.840]   They don't have as much data.
[00:29:05.840 --> 00:29:09.840]   You know, I think Apple would say that's not our problem.
[00:29:09.840 --> 00:29:10.960]   That's not the problem.
[00:29:10.960 --> 00:29:16.160]   But it's, I mean, sure, on the outside, it looks like Google, which knows everything,
[00:29:16.160 --> 00:29:20.720]   is easily able to make an smart system because it's got, it's collecting all this signals,
[00:29:20.720 --> 00:29:22.560]   has all this training material.
[00:29:22.560 --> 00:29:24.400]   Apple says we don't collect all that stuff.
[00:29:24.400 --> 00:29:29.040]   But at the same time, Apple wants Siri to be as good as Google Assistant.
[00:29:29.040 --> 00:29:37.200]   So that's apparently, you know, why they,
[00:29:37.200 --> 00:29:44.960]   Gian Andrea was running Google Brain for a while, which is their primary AI lab.
[00:29:45.760 --> 00:29:51.360]   So yeah, I think, I think you could say that Apple had a score.
[00:29:51.360 --> 00:29:52.480]   They sure moved fast.
[00:29:52.480 --> 00:29:55.920]   I wonder if it feel, I mean, I wonder.
[00:29:55.920 --> 00:30:00.080]   How hard is it to send a text message of a couple dollars on?
[00:30:00.080 --> 00:30:03.520]   A lot of dollars because he most of that.
[00:30:03.520 --> 00:30:07.200]   Yeah, those in common, all kinds of things.
[00:30:07.200 --> 00:30:11.520]   It strikes me that that kind of hire takes a long time.
[00:30:11.520 --> 00:30:13.680]   Yeah, there had to be some serious negotiation.
[00:30:13.680 --> 00:30:14.720]   They had to be talking.
[00:30:14.720 --> 00:30:16.720]   I'm thinking they were talking before this.
[00:30:16.720 --> 00:30:17.920]   But then why would they have a now?
[00:30:17.920 --> 00:30:21.440]   But then, then, so then he played a game.
[00:30:21.440 --> 00:30:23.040]   He acted like he was going to stay there.
[00:30:23.040 --> 00:30:24.640]   And then the next day, he says, never mind.
[00:30:24.640 --> 00:30:27.280]   Is that what's this happens every day, though?
[00:30:27.280 --> 00:30:27.840]   This happens.
[00:30:27.840 --> 00:30:28.080]   Yeah.
[00:30:28.080 --> 00:30:33.040]   Well, the size of these huge companies, this happens at middle and middle America, you know,
[00:30:33.040 --> 00:30:38.080]   somebody's out there busting their hump for the last five, six years and getting itch to,
[00:30:38.080 --> 00:30:40.640]   to, you know, put their foot in the water somewhere else.
[00:30:40.640 --> 00:30:42.640]   They're not going to just up and leave.
[00:30:42.640 --> 00:30:44.320]   They're going to see what's available to them.
[00:30:44.320 --> 00:30:48.800]   They'll talk to the headhunters in the background and just, you know, play the role.
[00:30:48.800 --> 00:30:49.520]   It happens.
[00:30:49.520 --> 00:30:55.280]   I would, though, if you're Apple, you don't want to hire this guy just because he's,
[00:30:55.280 --> 00:30:58.800]   I mean, you throw a lot of money at him.
[00:30:58.800 --> 00:31:01.040]   Doesn't mean he's going to be able to turn around Siri.
[00:31:01.040 --> 00:31:04.400]   You want him to come to Apple because he thinks he can turn around Siri,
[00:31:04.400 --> 00:31:08.640]   because he has some ideas about what Apple needs to do to be competitive with Google.
[00:31:08.640 --> 00:31:11.840]   He's also, remember, going to be highly constrained in what he can do because
[00:31:12.560 --> 00:31:15.200]   I've got to think Google has all sorts of restrictions on the,
[00:31:15.200 --> 00:31:21.920]   just look at the Uber Waymo suit on the kinds with, with Lewandowski went from Google to Uber.
[00:31:21.920 --> 00:31:22.560]   Right.
[00:31:22.560 --> 00:31:29.280]   All sorts of restrictions on, on what he can, what technologies he can
[00:31:29.280 --> 00:31:31.840]   bring to Apple, where he could talk about it, Apple.
[00:31:31.840 --> 00:31:35.280]   He's not going to just, he's not just taking Google Assistant saying, come on over.
[00:31:35.280 --> 00:31:40.720]   So I wonder how effective this, I guess the bottom line on this is, I wonder how effective this
[00:31:40.720 --> 00:31:41.520]   is going to be for Apple.
[00:31:41.520 --> 00:31:46.240]   Is it safe to assume that Apple has been actively looking for
[00:31:46.240 --> 00:31:48.800]   someone, the caliber?
[00:31:48.800 --> 00:31:49.200]   Yes.
[00:31:49.200 --> 00:31:49.520]   Okay.
[00:31:49.520 --> 00:31:53.840]   So if that's safe to assume who were the other candidates out there,
[00:31:53.840 --> 00:31:59.920]   you know, why wouldn't they be available or make the cut, if you will?
[00:31:59.920 --> 00:32:03.520]   Well, there's Chilu who worked at Yahoo for a while.
[00:32:03.520 --> 00:32:05.120]   He's now at Baidu in China.
[00:32:05.120 --> 00:32:09.840]   He was a VP, EVP at Microsoft for Bing, but he's also very,
[00:32:10.720 --> 00:32:15.120]   but he just left Microsoft for, for Baidu last year.
[00:32:15.120 --> 00:32:17.680]   I don't know all the players in this.
[00:32:17.680 --> 00:32:23.760]   Google's got like the theoretical smart guys, the Ray Kurzweisels of the world,
[00:32:23.760 --> 00:32:25.840]   the PNR Vigs of the world.
[00:32:25.840 --> 00:32:27.760]   They've, all of those guys work for Google.
[00:32:27.760 --> 00:32:30.240]   I don't know.
[00:32:30.240 --> 00:32:31.520]   I bet you're right.
[00:32:31.520 --> 00:32:34.960]   I bet that there were a lot of dollar signs in that text and Apple message.
[00:32:34.960 --> 00:32:37.600]   You know, it's one of those things like, what else could Apple do?
[00:32:37.600 --> 00:32:40.640]   They know they're lagging behind in this part of the game.
[00:32:40.640 --> 00:32:44.720]   And you do what you got to do as long as they're cheating.
[00:32:44.720 --> 00:32:47.200]   I do think that I think you make an interesting point,
[00:32:47.200 --> 00:32:50.560]   that there are a limited number of people who are really top of the field.
[00:32:50.560 --> 00:32:53.360]   And those people are worth a lot.
[00:32:53.360 --> 00:32:58.640]   This is a, this is a, you know, a elite field.
[00:32:58.640 --> 00:33:03.600]   When he, let's talk about something you're interested in.
[00:33:04.720 --> 00:33:11.360]   Yeah, let's, let's do that. I feel so, I feel so, so bad to drag you into a,
[00:33:11.360 --> 00:33:13.280]   in your nice scene warfare.
[00:33:13.280 --> 00:33:17.440]   Yeah, let's, let's do a safe topic like security.
[00:33:17.440 --> 00:33:18.400]   Security safe.
[00:33:18.400 --> 00:33:25.440]   Is my VPN on right now?
[00:33:25.440 --> 00:33:25.760]   Hold on.
[00:33:25.760 --> 00:33:26.160]   Yeah.
[00:33:26.160 --> 00:33:26.640]   No kidding.
[00:33:26.640 --> 00:33:29.120]   No kidding.
[00:33:29.120 --> 00:33:32.400]   Did you switch to the new DNS?
[00:33:32.960 --> 00:33:35.280]   I did 1.1.1.1.
[00:33:35.280 --> 00:33:36.240]   Oh, I did.
[00:33:36.240 --> 00:33:44.160]   Steve Gibson has a DNS benchmarking tool and I ran it and 1.0 is actually not
[00:33:44.160 --> 00:33:47.600]   the fastest DNS available to me.
[00:33:47.600 --> 00:33:52.800]   In most cases, most people, I think he said this, most people's own ISP will be faster.
[00:33:52.800 --> 00:34:01.360]   But the point of 1.0, which is created by a cloud flare, is that it prevents your ISP from
[00:34:01.360 --> 00:34:03.680]   seeing your internet DNS lookups.
[00:34:03.680 --> 00:34:06.160]   Your ISP is leaking that information.
[00:34:06.160 --> 00:34:07.120]   So it's a privacy.
[00:34:07.120 --> 00:34:09.600]   Speaking of people who know a hell of a lot about you.
[00:34:09.600 --> 00:34:10.000]   Oh yeah.
[00:34:10.000 --> 00:34:13.360]   That's a lot more, a lot more than any social platform.
[00:34:13.360 --> 00:34:14.240]   Well, yes and no.
[00:34:14.240 --> 00:34:18.240]   I don't think that's necessarily true because you're, for instance, your conversation,
[00:34:18.240 --> 00:34:21.920]   I agree with you that ISP is absolutely or problematic.
[00:34:21.920 --> 00:34:26.160]   But your conversation with Google and Facebook, both of those are protected by HTTPS.
[00:34:26.160 --> 00:34:27.840]   They can't see your Google searches.
[00:34:27.840 --> 00:34:29.440]   They can't see your Facebook likes.
[00:34:30.080 --> 00:34:31.920]   Your ISP can see a lot more.
[00:34:31.920 --> 00:34:37.520]   I mean, certainly Comcast, we know, and AT&T and Verizon, we know, are, you know,
[00:34:37.520 --> 00:34:43.520]   quickly will hand that information over to third parties if they can get paid for it and
[00:34:43.520 --> 00:34:46.640]   have no barrier to law enforcement.
[00:34:46.640 --> 00:34:49.920]   In that respect, ISP is known an awful lot.
[00:34:49.920 --> 00:34:54.720]   But on the other hand, I think both Google and Facebook and many other sites have for a variety
[00:34:54.720 --> 00:35:00.000]   of reasons started to hide that information, not just from ISPs, but from we open Wi-Fi.
[00:35:00.000 --> 00:35:01.760]   And other places that you need.
[00:35:01.760 --> 00:35:02.880]   We need a sophisticated discussion about this.
[00:35:02.880 --> 00:35:05.280]   I just want to go back for one second to the Facebook thing.
[00:35:05.280 --> 00:35:09.840]   Washington Post just put up their story on the Zuckerberg press conference.
[00:35:09.840 --> 00:35:13.200]   And I quote the headline, Facebook said the personal data of most of its two billion
[00:35:13.200 --> 00:35:15.200]   users have been collected and shared with outsiders.
[00:35:15.200 --> 00:35:16.240]   Well, yeah.
[00:35:16.240 --> 00:35:19.440]   I mean, that's such an ignorant headline.
[00:35:19.440 --> 00:35:24.640]   Every single profile you have anywhere on the internet has been scraped and used.
[00:35:24.640 --> 00:35:26.320]   That's not news.
[00:35:26.320 --> 00:35:29.520]   But now we're acting as if, oh my God, oh my God, the world's coming down.
[00:35:29.520 --> 00:35:30.560]   Look at that graphic.
[00:35:30.560 --> 00:35:32.000]   Like, oh, the world's unzipped.
[00:35:32.000 --> 00:35:39.920]   And so we need to talk a lot more about what's really going on with ISPs and with others.
[00:35:39.920 --> 00:35:42.480]   Anyway, sorry for that.
[00:35:42.480 --> 00:35:44.480]   Here's a screech.
[00:35:44.480 --> 00:35:45.920]   I guess we're going to get back into this.
[00:35:45.920 --> 00:35:47.200]   But well, no, we can't.
[00:35:47.200 --> 00:35:48.240]   I just I apologize.
[00:35:48.240 --> 00:35:51.280]   Here's the thing I think.
[00:35:51.280 --> 00:35:53.920]   You know normal people.
[00:35:53.920 --> 00:35:55.280]   I can't do.
[00:35:55.280 --> 00:35:56.160]   I can't pretend to.
[00:35:56.160 --> 00:35:56.560]   We don't.
[00:35:56.560 --> 00:35:57.520]   We don't.
[00:35:57.520 --> 00:35:59.040]   Jeff and I know nobody normal.
[00:35:59.040 --> 00:35:59.840]   Oh, we don't.
[00:35:59.840 --> 00:36:04.720]   But a normal person isn't really aware of that.
[00:36:04.720 --> 00:36:13.680]   The promise, at least the superficial promise of Facebook, I can show you that BBC interview
[00:36:13.680 --> 00:36:18.320]   again with Mark Zuckerberg is that you can share your personal information with us,
[00:36:18.320 --> 00:36:20.080]   because we'll keep it private.
[00:36:20.080 --> 00:36:23.440]   And you get to choose which of your friends has access to it.
[00:36:23.440 --> 00:36:27.600]   That's that whole rigmarole about, oh, it wants to be your friend.
[00:36:27.600 --> 00:36:28.800]   Do you want to agree?
[00:36:28.800 --> 00:36:30.400]   The implication.
[00:36:30.400 --> 00:36:32.240]   Yeah, you can read the fine print.
[00:36:32.240 --> 00:36:32.960]   You're smart, Jeff.
[00:36:32.960 --> 00:36:34.960]   You probably understand there's more to it than this.
[00:36:34.960 --> 00:36:40.480]   But the implication is until you agree that that information is going to those pictures
[00:36:40.480 --> 00:36:43.280]   you post not are going to be shared with somebody, it's not.
[00:36:43.280 --> 00:36:47.600]   So this may not be a revelation to you, Jeff.
[00:36:47.600 --> 00:36:50.960]   But I think it is a revelation to a lot of people.
[00:36:50.960 --> 00:36:53.440]   But but but but but but but but but but but now what what that puts it as if.
[00:36:53.440 --> 00:36:55.040]   Oh my God, Facebook's been doing this to us.
[00:36:55.040 --> 00:36:58.160]   Every single advertiser, every single media company, every single everybody who has
[00:36:58.160 --> 00:36:59.600]   anything public has been doing it.
[00:36:59.600 --> 00:37:02.080]   Put it in context, please.
[00:37:02.080 --> 00:37:03.360]   Yes.
[00:37:03.360 --> 00:37:03.920]   Well, we do.
[00:37:03.920 --> 00:37:05.040]   Don't you think we do that?
[00:37:05.040 --> 00:37:09.200]   Don't we talk about that all the time on the every show I do on this network?
[00:37:09.200 --> 00:37:10.240]   We talk about that.
[00:37:10.240 --> 00:37:14.640]   And I on a radio show where I feel like I'm trying to talk to normal people,
[00:37:14.640 --> 00:37:15.920]   I try to let them know.
[00:37:15.920 --> 00:37:20.880]   And I certainly raised this specter of ISPs knowing a lot about you and being very willing
[00:37:20.880 --> 00:37:21.360]   to sell it.
[00:37:21.360 --> 00:37:23.120]   We know we know that that's the case.
[00:37:24.560 --> 00:37:29.440]   So I know some people that were running what's that service?
[00:37:29.440 --> 00:37:30.240]   Cody.
[00:37:30.240 --> 00:37:30.480]   Yeah.
[00:37:30.480 --> 00:37:33.680]   And pirated probably, right?
[00:37:33.680 --> 00:37:36.960]   The pirate pirate version of Cody, which the Amazon sticks.
[00:37:36.960 --> 00:37:37.440]   Yeah.
[00:37:37.440 --> 00:37:37.440]   Was it?
[00:37:37.440 --> 00:37:37.440]   Yeah.
[00:37:37.440 --> 00:37:37.920]   Was it.
[00:37:37.920 --> 00:37:39.200]   Cody on Amazon stick.
[00:37:39.200 --> 00:37:44.320]   Cody itself is just a media platform, but you can put Cody plugins on there that will get,
[00:37:44.320 --> 00:37:45.600]   you know, pirated media.
[00:37:45.600 --> 00:37:48.320]   It makes it very easy to watch pirated streams.
[00:37:48.320 --> 00:37:52.560]   You know, and I'm talking to, you know, Joe Schmo, or what have you.
[00:37:52.560 --> 00:37:54.880]   And they're just super excited about this thing.
[00:37:54.880 --> 00:37:57.680]   And they tell them they got this fire stick and whatnot.
[00:37:57.680 --> 00:37:59.840]   And I say, well, where do you get this from?
[00:37:59.840 --> 00:38:01.520]   And they say it comes up on the stick.
[00:38:01.520 --> 00:38:03.040]   And yeah, yeah, yeah.
[00:38:03.040 --> 00:38:04.880]   And I say, did you pay for it?
[00:38:04.880 --> 00:38:08.080]   No, and I'm trying to, you know, go about it.
[00:38:08.080 --> 00:38:10.880]   You're having a psychotic dialogue with them.
[00:38:10.880 --> 00:38:16.400]   You know, and trying to help them put the pieces together.
[00:38:16.400 --> 00:38:20.240]   And then I finally just have to tell them, look, that's, that's pirate material.
[00:38:20.240 --> 00:38:22.880]   You could get in some serious trouble about that.
[00:38:22.880 --> 00:38:25.680]   And they're like, oh, okay.
[00:38:25.680 --> 00:38:27.920]   And they just keep doing it.
[00:38:27.920 --> 00:38:28.480]   Yeah.
[00:38:28.480 --> 00:38:31.280]   You know, they just don't care.
[00:38:31.280 --> 00:38:35.600]   And then I tell them, hey, you're a cable provider sees every single thing that you're doing.
[00:38:35.600 --> 00:38:38.640]   You're internet provider sees every single thing that you're doing.
[00:38:38.640 --> 00:38:43.360]   So don't be surprised if you get a phone call or something in the mail or better yet.
[00:38:43.360 --> 00:38:48.720]   Someone knocks on your door because you got caught up watching the latest episode of Blackish,
[00:38:48.720 --> 00:38:51.440]   even though you don't own a television, you know, right.
[00:38:51.440 --> 00:38:56.080]   Well, so when I apologize, I did that.
[00:38:56.080 --> 00:38:56.640]   I caused that.
[00:38:56.640 --> 00:38:58.160]   What are we proving here?
[00:38:58.160 --> 00:38:59.520]   Back to safe territory.
[00:38:59.520 --> 00:39:02.480]   It's just people don't, people are ignorant and willfully so.
[00:39:02.480 --> 00:39:06.480]   That's why I take this headline from the post kind of.
[00:39:06.480 --> 00:39:09.600]   I understand what you're saying.
[00:39:09.600 --> 00:39:11.280]   It has no con, it has no context.
[00:39:11.280 --> 00:39:11.440]   Yeah.
[00:39:11.440 --> 00:39:13.840]   It acts as if Facebook just invented that today.
[00:39:13.840 --> 00:39:14.000]   Right.
[00:39:14.000 --> 00:39:15.120]   Confessed that today.
[00:39:15.120 --> 00:39:17.200]   That's what the internet has been from from day one.
[00:39:17.200 --> 00:39:19.760]   You could probably take this headline.
[00:39:19.760 --> 00:39:23.600]   Maybe I'll get my red pencil out and cross out Facebook.
[00:39:23.600 --> 00:39:30.720]   And then just, and Facebook said and just, and then say the internet users.
[00:39:30.720 --> 00:39:31.600]   Yeah.
[00:39:31.600 --> 00:39:37.280]   Personal data of most internet users has been collected and shared with outsiders.
[00:39:37.280 --> 00:39:38.400]   Oh, yeah.
[00:39:38.400 --> 00:39:42.080]   That's, that's, that would not change the accuracy of that headline at all.
[00:39:42.080 --> 00:39:44.800]   So I, I, I looked at that.
[00:39:44.800 --> 00:39:49.600]   Siva himself came in and I said in my, in my Facebook,
[00:39:49.600 --> 00:39:53.600]   Facebook, sorry, oh excuse me, linking to this, this kind of headline writing,
[00:39:53.600 --> 00:39:54.640]   we'll get the internet screwed.
[00:39:54.640 --> 00:39:57.520]   How will it get the internet screwed?
[00:39:57.520 --> 00:40:01.360]   Because because of the regulation, now what's going to happen?
[00:40:01.360 --> 00:40:04.240]   Mark my words friends, mark my words.
[00:40:04.240 --> 00:40:05.840]   GDPR is nothing.
[00:40:05.840 --> 00:40:07.600]   There's going to be a crackdown on the internet.
[00:40:07.600 --> 00:40:08.320]   The internet's evil.
[00:40:08.320 --> 00:40:12.320]   It's going to be used by bad guys or on the world including some in this country.
[00:40:12.320 --> 00:40:19.040]   And because it comes out of this, this panic, it comes out of misinformation.
[00:40:19.040 --> 00:40:19.680]   Yeah.
[00:40:19.680 --> 00:40:25.440]   Well, enough of, yeah, we don't want to shut down the internet.
[00:40:25.440 --> 00:40:26.480]   We just want to tell users.
[00:40:26.480 --> 00:40:33.040]   So I said, I said, we'll call it the internet screwed.
[00:40:33.040 --> 00:40:34.720]   Siva said deservedly.
[00:40:34.720 --> 00:40:39.280]   And so I said, so your anti internet that seems that is the internet anti people.
[00:40:39.280 --> 00:40:40.960]   This is the kind of discussion that's going on.
[00:40:40.960 --> 00:40:41.280]   Right.
[00:40:41.280 --> 00:40:43.040]   It's affecting the whole net.
[00:40:43.040 --> 00:40:43.840]   Point point.
[00:40:43.840 --> 00:40:44.560]   Your point.
[00:40:44.560 --> 00:40:44.880]   Yes.
[00:40:44.880 --> 00:40:46.000]   Point well proven there.
[00:40:46.000 --> 00:40:48.160]   If that's if that's where that makes me.
[00:40:48.160 --> 00:40:51.040]   While Wendy is saving us from the real bad guys.
[00:40:51.040 --> 00:40:57.120]   Right Wendy, there are real bad guys on the net who are doing really bad things.
[00:40:57.120 --> 00:40:59.200]   And we're paying attention to this kerfuffle.
[00:40:59.200 --> 00:41:05.360]   You know, I was going to say that this kind of discussion really happens in much smaller
[00:41:05.360 --> 00:41:12.000]   circles than you would imagine. And the people who are happily using social media all over the
[00:41:12.000 --> 00:41:17.360]   place are not privy to it unless they see it in a headline like the one you just read.
[00:41:17.360 --> 00:41:22.000]   So it may not be news to any of us, but you know, I remember the times when
[00:41:22.000 --> 00:41:28.400]   it was a very, very small internet community and we all had the same level of knowledge.
[00:41:28.400 --> 00:41:33.760]   We all considered everything to be intuitively obvious because we all thought the same way.
[00:41:33.760 --> 00:41:39.200]   And now that the rest of the globe is using this, you know, they have their own concerns and we
[00:41:39.200 --> 00:41:44.800]   have to think very carefully about what they know, which is not the same as what we know.
[00:41:44.800 --> 00:41:49.360]   What information is important for them to know and understand?
[00:41:49.360 --> 00:41:56.960]   And do we need to change our practices in technology to accommodate the way the rest of the world
[00:41:56.960 --> 00:42:00.800]   is going to be using this tech because it's not up to us anymore.
[00:42:00.800 --> 00:42:11.680]   Do you think it's possible? I guess really, is it possible to have services like Facebook and Google
[00:42:11.680 --> 00:42:18.960]   and not be subject to surveillance capitalism? Are there business models?
[00:42:18.960 --> 00:42:25.920]   Are there? I'm getting Shoshana Zuboff on here. The minute that book comes out, Jeff,
[00:42:25.920 --> 00:42:30.400]   just to give you a heart attack. We're talking about loaded ways to say this.
[00:42:30.400 --> 00:42:34.480]   I'm tired of this. This is tossed in all media and computer.
[00:42:34.480 --> 00:42:39.280]   I'm chumming. Is that what you're saying? I'm just chumming the water.
[00:42:39.280 --> 00:42:48.320]   Well, but I mean, look, I understand we're getting great free services like the show
[00:42:48.320 --> 00:42:52.160]   and they're advertising supported and God knows that's what I do, right?
[00:42:53.600 --> 00:42:59.440]   So I believe that I do it now and maybe this doesn't scale, but I do it in a way that doesn't
[00:42:59.440 --> 00:43:06.960]   infringe on the privacy of our listeners. But is it possible to run a Facebook or a Google
[00:43:06.960 --> 00:43:16.160]   at great expense without, I guess, could you have advertising? They're really two concerns.
[00:43:16.160 --> 00:43:22.560]   One, I don't actually really mind if advertisers know where I live, how old I am.
[00:43:22.560 --> 00:43:27.600]   There's a lot of stuff. It's fine with me. I want to see targeted ads. I'd far prefer to see targeted
[00:43:27.600 --> 00:43:32.000]   ads. I'm a little nervous. Ding, ding, ding, ding, ding, ding. Well, but then I'm a little nervous
[00:43:32.000 --> 00:43:37.840]   about ads that are hyper targeted to maybe my state of mind. So then set that standard.
[00:43:37.840 --> 00:43:40.720]   Yeah. Right. So Zuckerberg said in the conference today, he said,
[00:43:40.720 --> 00:43:47.040]   people want quality ads and they define quality as relevant. And the way to get relevance is to
[00:43:47.040 --> 00:43:52.400]   know something about you. Okay. So I don't advertise against my depressive personality.
[00:43:52.480 --> 00:43:56.400]   Or something, right? All right. Then set the standards. On the other hand,
[00:43:56.400 --> 00:44:03.760]   you know, I told a story on the show before, someone who had MS who didn't know it,
[00:44:03.760 --> 00:44:09.520]   and it actually had a big impact on them. And so, you know, they could have gotten to the doctor
[00:44:09.520 --> 00:44:14.160]   and had the same impact. I mean, it's not know that he should. I understand. I mean, I understand.
[00:44:14.160 --> 00:44:18.160]   So I know, but I think we go find Facebook for its diagnostic value.
[00:44:18.160 --> 00:44:22.480]   But what are the standards of targeting ads? And targeting is not evil. It's the way to
[00:44:22.480 --> 00:44:27.040]   hire value and advertising. The existence of a show like this, where, you know, 10 people in the
[00:44:27.040 --> 00:44:31.600]   world are willing to watch this for two hours. It democratizes it. It's highly targeted. Yeah.
[00:44:31.600 --> 00:44:37.600]   Yeah. But it's targeted. That's how I decided to do it, is to do niche programming that would,
[00:44:37.600 --> 00:44:41.200]   by default, say you don't have to know anything about this audience because anybody would listen
[00:44:41.200 --> 00:44:45.280]   to this is a certain kind of person. And so that's that advertisers appreciate it.
[00:44:45.280 --> 00:44:49.840]   Nobody apparently was apparently not enough advertisers appreciate it, but.
[00:44:49.840 --> 00:44:58.080]   I'm so sorry. Oh, no, no, no, just kidding. I like working for free. I don't mind it at all.
[00:44:58.080 --> 00:45:03.360]   Let's think a little bit about, I mean, taking it up a level above.
[00:45:03.360 --> 00:45:03.800]   Thank you.
[00:45:03.800 --> 00:45:15.880]   Advertize. Please, yes. Thinking about how data is being used and recombined and used again for
[00:45:15.880 --> 00:45:22.280]   very specific purposes, I think one of the discussions we need to have is not just what kind of data
[00:45:22.280 --> 00:45:28.360]   is being collected, but how it's being used in different contexts. You can almost think of it as
[00:45:29.160 --> 00:45:35.000]   chemicals that produce, in certain formulas, produce specific reactions.
[00:45:35.000 --> 00:45:42.200]   So one example I was just thinking of was after the Newtown massacre, something that the journal
[00:45:42.200 --> 00:45:48.760]   news did was they published the names and addresses of all handgun permit holders in New York's
[00:45:48.760 --> 00:45:55.480]   Westchester and Rockland counties. So this information was always available to the public upon request,
[00:45:55.480 --> 00:46:00.440]   but it was the act of collecting it at the individual level and highlighting it to an
[00:46:00.440 --> 00:46:08.360]   internet wide readership in the context of having just been, there's a big political context there
[00:46:08.360 --> 00:46:14.200]   right after the massacre. That was the use that was found to be an invasion of privacy and so
[00:46:14.200 --> 00:46:20.520]   objectionable. So it was a combination of data that actually was publicly available,
[00:46:20.520 --> 00:46:26.760]   but it was highlighting it and in a certain context. So that's the sort of activity with data I think
[00:46:26.760 --> 00:46:33.160]   we should be taking a closer look at. That's a really good point. For instance,
[00:46:33.160 --> 00:46:42.840]   you can go to county seat and get information about who owns every property in the county.
[00:46:42.840 --> 00:46:49.960]   That's public information. But then a company that does that and then puts it online for quick
[00:46:49.960 --> 00:46:56.760]   search. It was always public information, but it was never that exact to call that a phone book.
[00:46:56.760 --> 00:47:00.360]   We didn't panic about it. No, but there's a few Steve Martin now.
[00:47:00.360 --> 00:47:06.520]   Yeah, but it's more than a phone book. I mean, you can find out where I live.
[00:47:06.520 --> 00:47:11.320]   Now it used to be you'd have to go to the Sonoma County seat to find that out.
[00:47:11.320 --> 00:47:15.720]   Actually, while in Scandinavia, your income is public for all.
[00:47:15.720 --> 00:47:18.760]   Right. It's a public when I was paid different fees.
[00:47:19.400 --> 00:47:25.720]   When I was working for the state of Texas, all of the salaries of state employees are publicly
[00:47:25.720 --> 00:47:31.960]   available if you file a public information act request. But the Houston Chronicle took all of
[00:47:31.960 --> 00:47:37.880]   that data and put it in a database and published it. And the act of publishing it suddenly
[00:47:37.880 --> 00:47:45.720]   raised the visibility of that. And again, the focusing of public attention on it to a level
[00:47:45.720 --> 00:47:50.680]   that it hadn't been before. And you can imagine that that was an issue, not just for the employees
[00:47:50.680 --> 00:47:56.200]   of the state who could now spend the time and compare their salaries with somebody else's.
[00:47:56.200 --> 00:48:03.400]   But anybody could now access that very, very easily. So again, it's what you do with the data
[00:48:03.400 --> 00:48:08.920]   and how you combine it in the context of our society that I think we're going to have to have
[00:48:08.920 --> 00:48:13.560]   discussions around. How do you solve that? Do you say you can't publish it?
[00:48:14.440 --> 00:48:18.600]   Do you say I mean, or you change our dormers in society?
[00:48:18.600 --> 00:48:26.200]   Yeah. Yeah. I think that there's a lot of discussion that that points out that our norms in society
[00:48:26.200 --> 00:48:32.280]   have not caught up to our technology. Yeah. But norms won't protect you, Jeff. If your
[00:48:32.280 --> 00:48:38.200]   home address is public and somebody decides they don't like you, there's reasons why I would
[00:48:38.200 --> 00:48:44.760]   ever, ever, thus, ever, thus, Leo, come on. But now they can do it. They can do it five seconds
[00:48:44.760 --> 00:48:49.080]   instead of having to go on Twitter. I mean, they can tweet it. They can talk to me. Yeah. Yeah.
[00:48:49.080 --> 00:48:54.840]   Doxing is a new activity. Right. It's very different from calling information and saying,
[00:48:54.840 --> 00:48:56.920]   I want Jeff Jarvis's home phone number. Right.
[00:48:56.920 --> 00:49:03.080]   So, and I don't see that we've really addressed that at all. I mean, that's a big problem.
[00:49:03.080 --> 00:49:05.480]   I look, Por Briana Wu had to move.
[00:49:05.480 --> 00:49:11.400]   Mm hmm. So, I don't think we've addressed that at all. That's been a known right.
[00:49:11.400 --> 00:49:13.640]   Right. But what's the, what's the real problem? What's the real problem?
[00:49:13.640 --> 00:49:16.840]   The internet. I don't know what the pro, I don't know what the solution is. The problem is that
[00:49:16.840 --> 00:49:21.160]   we've got a screwed up society. Well, you can't fix that. We don't know. We'll threaten each
[00:49:21.160 --> 00:49:23.560]   other. And we don't know how to deal with people who threaten each other. Well, we just
[00:49:23.560 --> 00:49:28.360]   what we said about the YouTube case, the cops were called. They talked to the woman. She was
[00:49:28.360 --> 00:49:35.240]   sleeping in a car. The father was worried. There was nothing they could do. And rightly so.
[00:49:35.240 --> 00:49:39.880]   I, when I was in California, I tried to get a very dear friend of mine when I lived there,
[00:49:39.880 --> 00:49:45.160]   who was bipolar, what he was running naked through the woods of Monterey.
[00:49:45.160 --> 00:49:50.920]   The effort would take to get him committed because because one threw over the pecuckoo's nest.
[00:49:50.920 --> 00:49:56.040]   We got into such a part of me moral panic about psychiatric care in this country that we did
[00:49:56.040 --> 00:50:02.760]   everything we could to keep people from being cared. That's a much higher level problem.
[00:50:03.800 --> 00:50:08.920]   Addresses on the internet. Not the problem. How about this one? Grindr.
[00:50:08.920 --> 00:50:17.000]   Which is Tinder for gay men. Now, I've really mixed opinions on this one. I don't know what
[00:50:17.000 --> 00:50:22.920]   you guys think. When you put your Grindr profile up, apparently, one of the things you can fill it
[00:50:22.920 --> 00:50:28.440]   is your HIV status and date of last test. I guess if you're a gay guy dating, that's probably
[00:50:28.440 --> 00:50:33.560]   important information that you'd want to share with other gay guys who might go out with you.
[00:50:34.280 --> 00:50:40.440]   But Grindr was sharing that information, including location and HIV status with third party firms.
[00:50:40.440 --> 00:50:44.920]   But those firms
[00:50:44.920 --> 00:50:53.800]   pardon me, read on because they were not sharing it for it to be used in any way.
[00:50:53.800 --> 00:50:58.520]   It was in the data that was being used to check the app.
[00:50:58.520 --> 00:51:00.840]   It was for, yeah.
[00:51:00.840 --> 00:51:09.560]   But I know the people at Grindr know that this is very sensitive information.
[00:51:09.560 --> 00:51:19.160]   What's their responsibility there? I mean, the users posted that information in their profile.
[00:51:19.160 --> 00:51:25.960]   The Grindr only shared it with other companies for purposes of making their app more reliable.
[00:51:25.960 --> 00:51:28.920]   Grindr knew it was sensitive information.
[00:51:28.920 --> 00:51:31.480]   It should have been smart enough to have
[00:51:31.480 --> 00:51:36.280]   expregated and asked that information for any possible use anywhere except by that user.
[00:51:36.280 --> 00:51:39.000]   Yes, there was dumb, but it wasn't venal.
[00:51:39.000 --> 00:51:45.800]   Right. And who's at fall? I mean, if your user, you did put that stuff on Grindr.
[00:51:45.800 --> 00:51:52.840]   But not, but only for that context, not for it to be shared. And I think
[00:51:52.840 --> 00:51:57.960]   the problem is that most of the world doesn't understand the technological implications of just
[00:51:57.960 --> 00:52:04.920]   how easily their data can be collected, used, misused, shared, recombined.
[00:52:04.920 --> 00:52:10.200]   And it wouldn't occur to them to ask, hey, you're not going to use this for anything else. Are you?
[00:52:10.200 --> 00:52:13.080]   Right. Well, but you see, here's the answer.
[00:52:13.080 --> 00:52:15.640]   Here's the answer. Sorry. Sorry. Go ahead.
[00:52:15.640 --> 00:52:21.720]   Is it in the terms of service when someone signs up for Grindr that, hey, yes, you have to provide
[00:52:21.720 --> 00:52:25.320]   this data? No, you don't have to provide the data.
[00:52:25.880 --> 00:52:32.120]   But it may be that it said, as most terms of service do, your personal information is private,
[00:52:32.120 --> 00:52:38.280]   but from time to time for purposes of improving the service, we may share information with third
[00:52:38.280 --> 00:52:41.560]   parties. Yeah, they cover their their rumps. Yeah, they cover it. Yeah. So it's big.
[00:52:41.560 --> 00:52:45.800]   Other than they should have, you know, they claim they anonymized it.
[00:52:45.800 --> 00:52:50.840]   Guy as as an industry standard practice, Grindr does work with highly regarded vendors to test
[00:52:50.840 --> 00:52:55.240]   and optimize how we roll out our platform. These vendors are under strict contractual terms
[00:52:55.240 --> 00:52:59.240]   that provide for the highest level of confidentiality, data security and user privacy.
[00:52:59.240 --> 00:53:03.880]   I mean, you could probably also say that Grindr stuff is stored on a server that some other
[00:53:03.880 --> 00:53:08.600]   third party owns and presumably might have access to that information. All those things.
[00:53:08.600 --> 00:53:12.280]   Can you put that back up? I wanted to see what the next one when working with these platforms
[00:53:12.280 --> 00:53:16.520]   who restrict information, share it, accept as necessary or appropriate. Sometimes this data
[00:53:16.520 --> 00:53:21.640]   may include location data or data from HIV status fields as these are features within Grindr.
[00:53:21.640 --> 00:53:26.920]   However, it's always transmitted securely with encryption and there are data retention policies
[00:53:26.920 --> 00:53:31.400]   in place to further protect our users privacy from disclosure. This is the one that he got.
[00:53:31.400 --> 00:53:35.000]   This is the Grinder. So you need number four. This is the one he got some heat from.
[00:53:35.000 --> 00:53:37.480]   For it's important to remember that Grinder is a public forum.
[00:53:37.480 --> 00:53:43.080]   Some might question that we give users the option to post information about themselves,
[00:53:43.080 --> 00:53:46.840]   including HIV status and lab test date. We make it clear in our privacy policy
[00:53:46.840 --> 00:53:50.680]   that if you choose to include this information in your profile,
[00:53:50.680 --> 00:53:56.040]   the information will also become public. But if I'm a user, I'm thinking become public with
[00:53:56.040 --> 00:54:00.040]   somebody who's using Grindr and looking for somebody to go out with.
[00:54:00.040 --> 00:54:06.680]   Exactly. That's why I shared it. What does public mean in this context?
[00:54:06.680 --> 00:54:11.320]   Does it mean you're going to put it up on a Times Square billboard? Because that's also public.
[00:54:11.320 --> 00:54:18.840]   I think I come along and go through 10 Grindr accounts and get that information.
[00:54:18.840 --> 00:54:21.960]   Yeah, I could. Public is public. Public is public.
[00:54:21.960 --> 00:54:28.040]   I don't think it is. I don't think real people would say that.
[00:54:28.040 --> 00:54:34.200]   You keep saying that. I'm sorry. As a journalist, I also want to defend the definition of public.
[00:54:34.200 --> 00:54:42.200]   I wrote a book about this where I think that if there is not a presumption of privacy in public.
[00:54:42.200 --> 00:54:45.800]   When I see the mayor walking into a brothel, you can't turn around and say,
[00:54:45.800 --> 00:54:50.760]   well, you're not allowed to see that. That only happens in totalitarian, authoritarian regimes.
[00:54:50.760 --> 00:54:54.200]   In this country, I am allowed to see that. And I am allowed to repeat that.
[00:54:54.200 --> 00:54:57.880]   The knowledge that I have then becomes mine. That is just reality.
[00:54:57.880 --> 00:55:04.120]   I think the problem is that now with technology and the social media that we have,
[00:55:04.120 --> 00:55:09.880]   anyone can become a public figure at the drop of a half without realizing,
[00:55:09.880 --> 00:55:15.240]   without having signed up for it. In other words, yes, the mayor is a public figure.
[00:55:15.240 --> 00:55:20.120]   It comes with a job they can expect certain exposure and certain highlighting of their activities.
[00:55:20.120 --> 00:55:28.440]   The person who was caught in the middle of a news breaking event and tweets something
[00:55:28.440 --> 00:55:32.920]   and suddenly becomes famous, that can happen to any of us at any time.
[00:55:32.920 --> 00:55:35.240]   Yeah. And anybody's prepared to deal with that.
[00:55:35.240 --> 00:55:40.120]   Wendy, I walked down the street and I throw litter on the street.
[00:55:40.120 --> 00:55:44.280]   And you're walking along and you say, shame on that, Bozo. You take a picture of me,
[00:55:44.280 --> 00:55:48.440]   you put it on Instagram. That's my tough luck. I'm on the public street.
[00:55:48.440 --> 00:55:52.360]   I did something in public. No, I wasn't intending to be famous, but you make a cause
[00:55:52.360 --> 00:55:58.040]   celeb out of me. Smart ass, loud mouth, journalism prof and podcaster. People we really want to
[00:55:58.040 --> 00:56:05.560]   get because we don't like them are our slobs. Well, yeah, I did it. I did it in public.
[00:56:05.560 --> 00:56:09.480]   Did Nintendo afford the rope? But tough. Let me propose the hypothetical.
[00:56:10.920 --> 00:56:16.760]   I'm gay. I live in one of the 28 states, 28 states, more than half in which I could be fired for being
[00:56:16.760 --> 00:56:23.000]   gay. Period. 28 states. Didn't know that. Yeah. So let's say I live in one of those 28 states.
[00:56:23.000 --> 00:56:26.200]   Now, I want to use Grindr because I want to meet guys.
[00:56:26.200 --> 00:56:34.920]   Is it fair to say, oh, well, if you're in one of those 28 states, don't use Grindr.
[00:56:34.920 --> 00:56:40.760]   Or yeah, I mean, I guess that's true. But really because what you change in that case
[00:56:40.760 --> 00:56:45.480]   is that law that is exactly the right case. This is this is what Dana Boyd taught me years ago.
[00:56:45.480 --> 00:56:50.200]   The gathering of knowledge, you know, is not what you go after. It's the use of the knowledge.
[00:56:50.200 --> 00:56:54.920]   And if that's the case, then yeah, you shouldn't use Grindr in that state because you are liable.
[00:56:54.920 --> 00:57:01.720]   And that's wrong. What you change the law. You change the law. That's what a democracy is about.
[00:57:01.720 --> 00:57:07.560]   So don't use Grindr until the law has changed. Oh, no. I mean, because you are you are at risk.
[00:57:08.280 --> 00:57:12.280]   You're an education level. That's the technically right answer. But I think there's a lot of people
[00:57:12.280 --> 00:57:16.760]   who say awful answer. I'm going to use Grindr because only gay other gay people would use
[00:57:16.760 --> 00:57:23.000]   right or what's Leo. What's the fundamental problem in that equation? You and I have to agree with that.
[00:57:23.000 --> 00:57:27.560]   What's wrong? There is a law. What's wrong? There is the behavior of my degree. I agree. But
[00:57:27.560 --> 00:57:36.200]   and technology can't fix that. No, but I think that it wouldn't I wouldn't I wouldn't be surprised.
[00:57:37.160 --> 00:57:41.640]   I wouldn't look at somebody say, well, you asked for it, dude, if they use Grindr,
[00:57:41.640 --> 00:57:44.840]   assuming that well, only other gay people will see this. So I'm safe.
[00:57:44.840 --> 00:57:51.400]   But you can't that's why that's why number four warns you and says you put this up here.
[00:57:51.400 --> 00:57:56.200]   Well, they need to put that in big letters when you when you when you first get Grindr,
[00:57:56.200 --> 00:58:01.240]   they didn't put in big letters. I don't know. So, you know, you shouldn't use in these 28 states
[00:58:01.240 --> 00:58:04.200]   because you're posting publicly in effect that you're getting.
[00:58:04.200 --> 00:58:10.440]   Believe me, having having watched people in the day be frightened to death of losing their jobs,
[00:58:10.440 --> 00:58:15.320]   they know. Well, but yeah, but I don't think they know now. I think that
[00:58:15.320 --> 00:58:25.960]   the big issue of our society today is we are still discovering what can be done and
[00:58:26.680 --> 00:58:35.560]   misused about it at scale with data, especially personal data. And I think the big reaction that
[00:58:35.560 --> 00:58:42.440]   we see over and over again is well, not like that. And that that's again, where social norms have
[00:58:42.440 --> 00:58:50.040]   to be adjusted. They have to come back in because nobody, I think, really understands the extent
[00:58:50.040 --> 00:58:57.320]   of where we're going with all of this data. And every time we run up and again, something like this
[00:58:57.320 --> 00:59:06.200]   and think, yeah, I realized that it was public, but I did not understand just how widely,
[00:59:06.200 --> 00:59:12.280]   you know, it could be interpreted as public. I made certain assumptions based on my understanding,
[00:59:12.280 --> 00:59:17.400]   and they turned out to be wrong, but I had no way of knowing. And that's what you what do you
[00:59:17.400 --> 00:59:21.000]   suggest that the standard should be then that you can't.
[00:59:21.000 --> 00:59:30.280]   Nobody can be can do anything public unless they've signed lots of releases and waivers.
[00:59:30.280 --> 00:59:33.320]   See, I think that's all very extremist. I think
[00:59:33.320 --> 00:59:44.920]   because we have more visibility into people's actions and can do activities such as, for example,
[00:59:44.920 --> 00:59:51.000]   public shaming at a scale that we have never experienced before as the, you know, as a human race.
[00:59:51.000 --> 00:59:59.400]   We're having to re-examine all of the assumptions that we have. And there used to be social norms
[00:59:59.400 --> 01:00:06.040]   about what you could do to shame somebody, for example, what was over the top, what was appropriate.
[01:00:06.040 --> 01:00:12.120]   And we simply have to readjust these based on the contact that we all have
[01:00:13.000 --> 01:00:17.800]   around the globe now, because it's unprecedented in our human history.
[01:00:17.800 --> 01:00:24.200]   I certainly agree, Wendy, this is about readjusting norms, and we're in the process of doing that.
[01:00:24.200 --> 01:00:31.000]   And I find great hope if you look at the Lorangrim case, where somebody famous went after a teenager,
[01:00:31.000 --> 01:00:38.920]   and the reaction that then occurred was society saying, "Yeah, we're going to make up a new norm now.
[01:00:38.920 --> 01:00:42.680]   You just went over it." So you don't disagree with the Hill who says that's
[01:00:42.680 --> 01:00:49.880]   a troubling precedent. I do disagree. I do disagree. We each have a, each company has a right to
[01:00:49.880 --> 01:00:53.400]   decide what they want to associate with. We as consumers have a right to decide what we associate
[01:00:53.400 --> 01:01:01.640]   with. And Martin Luther King, on this day we note, did a lot of boycotts. And that's how he got a
[01:01:01.640 --> 01:01:05.880]   lot of the economic pressure. The only thing, the only pressure that his community had was economic.
[01:01:05.880 --> 01:01:09.000]   Yeah. No, I don't have a problem with it either.
[01:01:10.680 --> 01:01:13.960]   But it's, there are those. You can be used for bad too, every tool can be.
[01:01:13.960 --> 01:01:19.800]   Yeah. Here's an interesting. So here's an interesting example. I don't know if this is
[01:01:19.800 --> 01:01:24.200]   moral panic. You can be the judge of that, Jeff. You're the king. I am the judge. You're the judge
[01:01:24.200 --> 01:01:30.280]   of moral panic. Chrome is scanning files on your computer and people are freaking out.
[01:01:30.280 --> 01:01:36.200]   But what's the last, the last line of that subhead says, there's no reason to freak out, says
[01:01:36.840 --> 01:01:42.120]   motherboards, Lorenzo Franceschi. The big headlines says, freak out. The little headlines says,
[01:01:42.120 --> 01:01:46.280]   no, don't freak out. So the reason people are freaking out is, well, it's one of those things
[01:01:46.280 --> 01:01:51.480]   that's benign, right? Wendy, it's Chrome. Your browser saying, well, let me look at your documents
[01:01:51.480 --> 01:01:56.840]   folder and see if there's any malware in there. No, they're not even looking at the documents
[01:01:56.840 --> 01:02:04.360]   folder. If you look at the postings on Twitter from Justin Shoe, who I think was quoted in that,
[01:02:04.360 --> 01:02:10.120]   that article that you were highlighting, he leads Google Chrome security and desktop engineering.
[01:02:10.120 --> 01:02:18.440]   He talks about what this does, the Chrome cleanup tool. It's not a system wide scanner filter,
[01:02:18.440 --> 01:02:25.000]   as he writes. It runs weekly. It scans browser hijacking points, which may cause it to follow
[01:02:25.000 --> 01:02:30.360]   links elsewhere. In other words, it's patrolling the security of the browser, which is, you would
[01:02:30.360 --> 01:02:35.560]   expect the browser to do that. I think that's a great feature for it to be doing. It's a very
[01:02:35.560 --> 01:02:44.920]   heavily sandboxed subset of ESET. But in checking some things out to see where they go, they may
[01:02:44.920 --> 01:02:53.240]   follow other links. So again, there's a lot of technical detail in what Google has done here
[01:02:53.240 --> 01:02:59.400]   that was completely lost in the discussion. And the problem is that Google itself, their
[01:02:59.400 --> 01:03:04.760]   security team doesn't have marketing per se. All the information about what they're doing
[01:03:04.760 --> 01:03:09.640]   with consumer features and securities on their security blog, which is great, except consumers
[01:03:09.640 --> 01:03:15.080]   usually don't know it exists and they don't go read it. So this is another case where most of
[01:03:15.080 --> 01:03:23.800]   the rest of the world does not understand the details, the technical details of this.
[01:03:24.440 --> 01:03:30.280]   And they're not being communicated in a very user-friendly way. They're being
[01:03:30.280 --> 01:03:35.480]   communicated in a tech-friendly way. And you and I might go look for that security blog. But
[01:03:35.480 --> 01:03:39.000]   the rest of our neighbors down the street won't think to do that.
[01:03:39.000 --> 01:03:43.720]   Well, so here's the way it was. By the way, it is if you read the terms of service,
[01:03:43.720 --> 01:03:48.040]   just nobody does. It is stated in the terms of service criminal do this. So it isn't
[01:03:48.040 --> 01:03:52.200]   something undisclosed. As you said, it wasn't disclosed by a marketing team. It was disclosed
[01:03:52.200 --> 01:03:56.920]   by a technology team. But Kelly Shortrich, who is a sophisticated user,
[01:03:56.920 --> 01:04:02.040]   discovered it because she had a Canary token in her document folder.
[01:04:02.040 --> 01:04:06.520]   So by the way, we should mention Canary is the things company is a sponsor.
[01:04:06.520 --> 01:04:12.680]   So she had a phony document file that was there to warn her if somebody was accessing
[01:04:12.680 --> 01:04:21.080]   her document folder. And it was triggered and it was Chrome. So this is a good example of-
[01:04:21.080 --> 01:04:24.520]   Bob, are we better off with Chrome? Wendy, are we better off with Chrome doing this or not?
[01:04:24.520 --> 01:04:26.680]   Oh, I would say so. Yes, Wendy? Yeah.
[01:04:26.680 --> 01:04:32.760]   I believe that they have worked very, very hard on protecting users. And they've done more than
[01:04:32.760 --> 01:04:41.160]   a lot of other companies have. But the problem is that nothing happens in isolation.
[01:04:41.160 --> 01:04:48.040]   And these sorts of features, the more sophisticated they get, the more they have to branch off into
[01:04:48.040 --> 01:04:52.840]   areas where we start getting nervous and thinking, well, what about our privacy?
[01:04:52.840 --> 01:04:56.200]   So yeah, it's a very difficult line for them to walk.
[01:04:56.200 --> 01:05:01.000]   Yeah. So they do- they may go in following these hijackers.
[01:05:01.000 --> 01:05:05.800]   They may go into your documents folder, but they're not looking randomly at documents.
[01:05:05.800 --> 01:05:10.200]   Yeah. If they're following links that they may have reason to believe
[01:05:10.200 --> 01:05:16.840]   is, you know, are suspicious, they may need to go check them out. At the hijack points within
[01:05:16.840 --> 01:05:21.000]   the browser and say, okay, you know, what are we really looking at here?
[01:05:21.000 --> 01:05:29.880]   So the question is how far do you want your software to go in an effort to protect you
[01:05:29.880 --> 01:05:36.920]   from attackers? How much of this do you want them to do for you? And do you have the knowledge
[01:05:36.920 --> 01:05:41.560]   to be able to say, I want you to go this far, but not any further and really understand the
[01:05:41.560 --> 01:05:46.680]   implications of what you're asking for? I have to say sometimes I feel like Google acts a little bit
[01:05:46.680 --> 01:05:55.640]   nanny-ish. You know, in maybe, you know, things like kind of pushing HTTPS, yes, that's all well and
[01:05:55.640 --> 01:06:05.480]   good and everybody should use it. Deprecating. What was it the- oh, it was- it was the- was it the
[01:06:05.480 --> 01:06:11.320]   1024? You were a real shortener. Well, no, good. Yeah, we can talk about that in a bit, but
[01:06:11.320 --> 01:06:17.640]   remember they deprecated keys, 1024 keys a little early. Earlier than that was a problem, or maybe
[01:06:17.640 --> 01:06:21.480]   there were 2048 bit keys. It was earlier than it was a problem. Both Microsoft and Google agreed to
[01:06:21.480 --> 01:06:26.920]   do this, but Google advanced the deadline. Causing problems, people had to go out and get new
[01:06:26.920 --> 01:06:35.640]   new certificates. And I understand that this is a really good example of engineers have a different
[01:06:35.640 --> 01:06:42.440]   mindset. They don't really- they- well, this is good for everybody. We're going to just do this,
[01:06:42.440 --> 01:06:48.920]   and they maybe don't understand the political issue, the issue of dealing with humans quite as-
[01:06:48.920 --> 01:06:51.960]   Well, I think it's- but it's the atmosphere of trust or lack thereof.
[01:06:51.960 --> 01:06:55.560]   We trusted Google to do this. We trusted Facebook now because of all this
[01:06:55.560 --> 01:07:03.160]   Huha, and we have been debating legitimacy of that Huha. There is now a lack of trust, and they have
[01:07:03.160 --> 01:07:06.280]   to deal with that in a whole different light, and they're not used to it. Well, I trust Google,
[01:07:06.280 --> 01:07:09.640]   and you trust Google, but when you'd be surprised if you saw- if you had a thing in your documents
[01:07:09.640 --> 01:07:13.000]   folder that said, "Hey, Chrome just was looking in your documents folder," you would kind of want
[01:07:13.000 --> 01:07:20.680]   to know, "Well, what? Why was that?" I guess I don't quite understand it. The technology here,
[01:07:20.680 --> 01:07:28.600]   but isn't just the same type of security that can try to stop the mining software from being
[01:07:28.600 --> 01:07:35.560]   installed on my machine because I went to some whatever website. There's a lot of safe browsing
[01:07:35.560 --> 01:07:43.320]   work that Google has done, both from a search perspective and from a Chrome engineering perspective.
[01:07:43.320 --> 01:07:49.240]   The stuff is all there in their security blog, but again, is it in a format that
[01:07:49.240 --> 01:07:56.440]   typical users, not us, but typical users, will know to look for and understand,
[01:07:57.240 --> 01:08:01.880]   there's a lot of usability issues that we have to re-examine right away.
[01:08:01.880 --> 01:08:11.720]   So I'm just going to quickly show you where it says this. This has been in the
[01:08:11.720 --> 01:08:20.440]   Chrome terms of- what is this? This is the Chrome Security Blog. They've been saying this since
[01:08:20.440 --> 01:08:28.360]   January 2017, it's actually a white paper on Chrome Security, and if you scroll really far down into
[01:08:28.360 --> 01:08:34.600]   unwanted software protection, Chrome periodically scans your device to detect potentially unwanted
[01:08:34.600 --> 01:08:39.160]   software. Actually, this is another example of where the marketing team might have said,
[01:08:39.160 --> 01:08:44.200]   "You shouldn't call this heading unwanted software protection," because what you're
[01:08:44.200 --> 01:08:48.360]   protecting against is unwanted software, but it sounds like what you're saying is that your
[01:08:48.360 --> 01:08:50.360]   software protection is unwanted. Anyway-
[01:08:50.360 --> 01:08:51.480]   We need for a hyphen.
[01:08:51.480 --> 01:08:54.440]   It shoots a lead.
[01:08:54.440 --> 01:08:59.400]   This is an engineer writing this, and I love engineers. I love that mindset,
[01:08:59.400 --> 01:09:04.280]   but maybe they don't quite get the people might understand it.
[01:09:04.280 --> 01:09:07.880]   Chrome periodically scans your device to detect potentially unwanted software.
[01:09:07.880 --> 01:09:12.120]   That's what they said. Look, we told you we were going to do this. It's right here in Paragraphs
[01:09:12.120 --> 01:09:18.040]   34 subsection A, under unwanted software protection. Surely you read that.
[01:09:18.040 --> 01:09:24.600]   Anyway, I'm glad they do it. I don't think it's harmless, but I understand people's concern.
[01:09:24.600 --> 01:09:32.840]   By contrast, I have to tell you the story of one of my kids who we used to let him use a computer
[01:09:32.840 --> 01:09:37.800]   before he could actually read, and he was clicking around on software. One day, we caught him in the
[01:09:37.800 --> 01:09:44.360]   middle of downloading and almost installing a browser plugin. We couldn't figure out what was
[01:09:44.360 --> 01:09:49.960]   going on there until we figured out that even though he couldn't read yet, he had learned that
[01:09:49.960 --> 01:09:57.480]   if the pop-up window appeared, and there were two choices, one of them would be highlighted
[01:09:57.480 --> 01:10:01.320]   more than the other one. If you click that one, then it would go away and things would work again.
[01:10:01.320 --> 01:10:03.400]   I'm a good boy. Look what I did. I'm a good boy.
[01:10:03.400 --> 01:10:09.000]   So you figured out how to do that. He intuited that by himself until he got to the EULA,
[01:10:09.000 --> 01:10:13.560]   and the EULA agree or disagree. Neither choice was highlighted by default, so he didn't know what
[01:10:13.560 --> 01:10:22.840]   to pick. So this is the sort of thing that users of all ages and all different cultures and different
[01:10:22.840 --> 01:10:30.680]   levels of understanding are dealing with. We have got to re-engineer how we work with users and what
[01:10:30.680 --> 01:10:35.320]   the user experience is like, because nobody's going to read these white papers. I can't,
[01:10:35.320 --> 01:10:39.320]   nobody outside of technology is going to read these white papers.
[01:10:39.320 --> 01:10:42.520]   I didn't even read it. I didn't know Facebook. They'll read it there.
[01:10:42.520 --> 01:10:47.080]   They get into a meme, and then it's okay.
[01:10:47.080 --> 01:10:55.560]   Now at Carnegie Mellon University, there's a usability and privacy research group that is
[01:10:55.560 --> 01:11:01.960]   working on a lot of these issues, trying to figure out what privacy means, what usability
[01:11:01.960 --> 01:11:07.240]   actually means, and working to redesign these things so that they can be understandable.
[01:11:07.240 --> 01:11:14.040]   I think that's a very important effort, but it's going to have to spread a lot further
[01:11:14.040 --> 01:11:21.320]   throughout technology. We really have to emphasize usability and better cultural understanding.
[01:11:23.480 --> 01:11:30.440]   Yeah, it's really an interesting issue. I think, correct me if I'm wrong, I'm trying to synthesize
[01:11:30.440 --> 01:11:35.640]   a general statement. I think that the problem really is, and we've said this before, that
[01:11:35.640 --> 01:11:43.480]   99% of users are reliable and trustworthy and respectful and so forth. There are bad actors,
[01:11:43.480 --> 01:11:48.840]   though, 1%. What we really underestimated was that the power that all of these internet tools would
[01:11:48.840 --> 01:11:54.920]   give the 1%. It's now overwhelming us, whether it's on Facebook or Twitter,
[01:11:54.920 --> 01:12:00.200]   malware. Which is a problem of responsibility. I mean,
[01:12:00.200 --> 01:12:04.360]   Av Williams said that, that's not by Southwest. He said we didn't anticipate the bad behavior.
[01:12:04.360 --> 01:12:08.200]   Yeah, we should have. We know that now. We didn't have enough people like Wendy,
[01:12:08.200 --> 01:12:14.680]   seriously, to say, let me look at the dark side here and anticipate the problem and protect against
[01:12:14.680 --> 01:12:20.520]   it. It goes back to the days of operating systems. When they designed operating systems, they weren't
[01:12:20.520 --> 01:12:28.440]   designed to defend against malefactors. They weren't designed for people who didn't know how to use
[01:12:28.440 --> 01:12:32.360]   them. Yes, there's other engineers. Buy engineers for engineers. Buy engineers for engineers,
[01:12:32.360 --> 01:12:39.000]   exactly. The first step when you start looking at software is seeing how it can be accidentally
[01:12:39.000 --> 01:12:44.440]   broken and accidentally misused. You need to protect against that. Then from there, it's just
[01:12:44.440 --> 01:12:49.080]   another short step to malice and say, "Now, if somebody were trying to break this on purpose,
[01:12:49.080 --> 01:12:55.480]   what would happen?" There are a lot of us in the security community that sit around thinking
[01:12:55.480 --> 01:13:01.720]   these bad thoughts all day. It's certainly not what normal people think about.
[01:13:01.720 --> 01:13:07.000]   When did that shift happen, Wendy? When did we start really thinking, "Oh, we got to really
[01:13:07.000 --> 01:13:14.280]   think of worst-case scenarios?" I think it's been—well, those who have been defending against
[01:13:14.280 --> 01:13:21.480]   state actors for decades have already been thinking this way. I think just as technology is spread out
[01:13:21.480 --> 01:13:30.760]   into the hands of the rest of the world and regular users, the expansion of bad actors has
[01:13:30.760 --> 01:13:36.440]   increased proportionately, and the types of different bad actions that are being taken,
[01:13:36.440 --> 01:13:45.480]   all sorts of fraud and abuse and monetization and bullying. There was a great talk by Alex
[01:13:45.480 --> 01:13:52.600]   Stamos at Facebook about what they consider to be their threat model. At the very top,
[01:13:52.600 --> 01:13:58.040]   there's just a little tiny bit that has to do with state actors and those sort of attacks.
[01:13:58.040 --> 01:14:02.840]   Way at the bottom, the biggest thing that they have to defend against is users of their platform
[01:14:02.840 --> 01:14:10.280]   abusing other users, and that's their biggest problem. These sorts of threat models change
[01:14:10.280 --> 01:14:18.200]   very much depending on the software, depending on the user base and the functionality. We don't
[01:14:18.200 --> 01:14:21.960]   understand all of it yet, but we need to spend a lot more time thinking about it.
[01:14:21.960 --> 01:14:26.840]   When you learn software design, in fact, even if you don't learn it, you might even intuit it,
[01:14:26.840 --> 01:14:33.000]   but especially when you learn it, you learn to test for edge cases. You learn to look for crazy
[01:14:33.000 --> 01:14:42.360]   inputs. The bad actor for software design is just crazy inputs and unexpected things.
[01:14:42.360 --> 01:14:47.560]   As a good software designer, you design software really testing for the edge cases.
[01:14:47.560 --> 01:14:50.840]   That's very, very, very important. Sanitizer.
[01:14:50.840 --> 01:14:55.480]   Yeah. Then it gets to, if you've got user input, you've got to sanitize your inputs.
[01:14:56.200 --> 01:15:04.360]   And this is well known, but I think we're now seeing this at a more macro scale with Twitter
[01:15:04.360 --> 01:15:10.840]   and Facebook and everything else that you have to plan for the worst even at a larger level,
[01:15:10.840 --> 01:15:18.520]   at a higher level. We're not very good at planning for software errors and sanitizing our inputs.
[01:15:18.520 --> 01:15:23.320]   We're even worse at planning for bad actors. I think most of us just assume people are like us.
[01:15:23.320 --> 01:15:26.840]   We're they're nice. And what kind of work do you do?
[01:15:26.840 --> 01:15:33.880]   And they understand the same sorts of things. But yeah, nobody is getting into the head of
[01:15:33.880 --> 01:15:37.320]   the four-year-old and saying, "How are they going to do it?" That's a good point.
[01:15:37.320 --> 01:15:39.080]   That's a good point. That's a good point.
[01:15:39.080 --> 01:15:49.000]   Nobody's thinking of the edge case where one person in a family has one social security number
[01:15:49.000 --> 01:15:55.640]   and they share it throughout the family because nobody else has one. There are so many cultural
[01:15:55.640 --> 01:16:01.640]   differences that diversity is going to have to solve. There's a lot of research, for example,
[01:16:01.640 --> 01:16:08.040]   that we put into our usability at Duo, but we still run into edge cases all the time.
[01:16:08.040 --> 01:16:16.920]   You just have to realize that whatever you're designing it for is going to be around for much
[01:16:16.920 --> 01:16:23.400]   longer than you ever anticipated. It's going to be used by a user base. You cannot fully picture
[01:16:23.400 --> 01:16:28.920]   and imagine in your head and you have to be ready to adjust at a moment's notice.
[01:16:32.200 --> 01:16:38.520]   Cena had a... Did I just cast a bummer over? No, not at all. No, no, no, no, no, no, no, no. I think
[01:16:38.520 --> 01:16:46.200]   this is really the fundamental conversation we've been having all along really. And how do you do
[01:16:46.200 --> 01:16:52.520]   that now, especially retroactively, which is really challenging. How do you do that? How do you
[01:16:52.520 --> 01:16:59.800]   solve for this problem? How do you democratize speech and still handle the bad actors?
[01:16:59.800 --> 01:17:03.080]   I don't know. I think it's very, very hard. It's very hard.
[01:17:03.080 --> 01:17:14.280]   How Chromebooks became go-to laptops for security experts. Would you agree? Our CISO is using Chromebooks?
[01:17:14.280 --> 01:17:23.000]   Yes, indeed, it says. Yeah, we use them heavily at Duo. In fact, my travel laptop is a Chromebook
[01:17:23.000 --> 01:17:27.480]   because there's nothing resident on it. Although, those of us who have been around a long time
[01:17:27.480 --> 01:17:33.320]   are seeing the fat and thin client thing waxing and waning over the decades, and now we're going
[01:17:33.320 --> 01:17:39.400]   back to thinner is better, unless you have on that particular piece of hardware the better.
[01:17:39.400 --> 01:17:47.400]   But again, because of the additional security functionality that Google is
[01:17:47.400 --> 01:17:56.680]   engineering in there, yes, it is turning out to be a very practical use case for enterprises,
[01:17:56.680 --> 01:18:02.200]   as well as for individual users. If you go to a DEF CON or Black Hat, you don't bring out
[01:18:02.200 --> 01:18:08.120]   Windows laptop. You don't bring a Mac. You bring a Chromebook. Swift on security five years ago.
[01:18:08.120 --> 01:18:13.080]   No, three years ago, mentored someone on teaching computing basics to seniors. What antivirus
[01:18:13.080 --> 01:18:19.080]   should I buy by a Chromebook? But what about by a Chromebook? That was three years ago.
[01:18:19.080 --> 01:18:23.160]   But that's true. And if you get on the school bus, what do you bring with you?
[01:18:23.160 --> 01:18:28.920]   Oh, I love this story. So Google, this was a pilot program they did in North Carolina.
[01:18:28.920 --> 01:18:33.160]   They did in some rural areas where people spend a lot of time, support students,
[01:18:33.160 --> 01:18:37.160]   spend a lot of times on school buses with bad shocks.
[01:18:37.160 --> 01:18:44.040]   Bounced it along rural country roads. You're in, where are you in North Carolina, South Carolina?
[01:18:44.040 --> 01:18:49.480]   I grew up in South Carolina, but right now they've read us how to Charlotte, North Carolina.
[01:18:49.480 --> 01:18:53.800]   So a lot of kids know all about those country roads. Trust me.
[01:18:53.800 --> 01:18:59.560]   Spend as much as three hours a day in a school bus.
[01:18:59.560 --> 01:19:09.160]   Oh my God. I know all about that. You get on the bus at 6am and you're at the school at 8.
[01:19:09.160 --> 01:19:16.120]   Oh my God. So what do you do besides throwing spitwads? You give them all Google buses.
[01:19:16.120 --> 01:19:21.080]   You give them Google buses and they're going to roll out more of these.
[01:19:21.080 --> 01:19:28.200]   It's been a successful pilot for a while, I guess in North Carolina and South Carolina over the
[01:19:28.200 --> 01:19:33.880]   last couple of years. They put Wi-Fi in the school bus. They put Chromebooks in the school bus.
[01:19:33.880 --> 01:19:39.320]   They call it the Rolling Study Halls initiative. I'm thinking that probably a lot of kids are
[01:19:39.320 --> 01:19:45.480]   not studying on these. But you just think about how often you can walk outside. I don't care if
[01:19:45.480 --> 01:19:51.560]   it's small town Petaluma and you see someone that's under the age of 18. They're doing this.
[01:19:51.560 --> 01:19:57.560]   And I guarantee you they're not researching for whatever assignment that they had that week.
[01:19:57.560 --> 01:20:01.720]   Yeah. Unless the papers do that morning and they didn't do it last night.
[01:20:01.720 --> 01:20:04.680]   Yeah. Not if you're my kid so I'd care to hear.
[01:20:04.680 --> 01:20:09.560]   No last minute studying for them. Huh? No.
[01:20:09.560 --> 01:20:13.960]   Each Rolling Study Hall also has an on-board educator who could provide direct
[01:20:13.960 --> 01:20:17.400]   assistance. This isn't really. Really? This is, yeah to me, this is
[01:20:17.400 --> 01:20:24.360]   this is what Apple should have done instead of releasing a not much cheaper iPad with a pencil
[01:20:24.360 --> 01:20:32.920]   support. Apple really could have done something innovative and supportive. This is in 12 states.
[01:20:32.920 --> 01:20:37.320]   Now 16 districts Alabama, Colorado, Georgia, Kansas, Minnesota, New Mexico,
[01:20:37.320 --> 01:20:43.080]   Oregon, Pennsylvania, South Carolina, Tennessee, and Texas and Virginia. Of course, you know,
[01:20:43.080 --> 01:20:46.120]   it's going to work best where people are spending a lot of time the average
[01:20:46.120 --> 01:20:50.440]   bus route in the game well middle school.
[01:20:50.440 --> 01:20:54.120]   Pepplelet population is one and a half hours in the morning and one and a half hours in the
[01:20:54.120 --> 01:21:00.040]   afternoon. That's called what County North Carolina. Wow. Wow. Your kids don't spend that
[01:21:00.040 --> 01:21:06.440]   much time in the bus. Do they? Yes, they do. Oh my God. One of them does. Really?
[01:21:06.440 --> 01:21:14.360]   Wow. You do. And the funny thing is the school is roughly
[01:21:14.360 --> 01:21:23.800]   six miles from my home. Oh no. What? They can walk faster. Right. And that's the thing.
[01:21:23.800 --> 01:21:28.680]   There's been a time or two while I said, hey, go ahead and walk. You know, and they don't have
[01:21:28.680 --> 01:21:34.200]   enough bus routes. What is it? Right. I think it comes down to the. They're on the wrong side of
[01:21:34.200 --> 01:21:39.080]   the bus route. They're they pick them up on the way out. It's on the way back.
[01:21:39.080 --> 01:21:44.120]   That's exactly what happens. You know, and there's always so many buses out here. There's been times
[01:21:44.120 --> 01:21:53.160]   where I've found that the my younger son's bus driver, you know, he or she finishes the route.
[01:21:53.160 --> 01:22:01.160]   They go to the other route for the high school. Right. It's just double duty. And I think it's all
[01:22:01.160 --> 01:22:10.200]   budget. So it probably is. Oh, well, how can you arrive at school? Number one, high school starting
[01:22:10.200 --> 01:22:15.320]   earlier is ridiculous because the high school kids are more sleep. Yeah. But yeah, but how can
[01:22:15.320 --> 01:22:20.600]   you arrive at school fresh to think when you an hour and a half on the bus? Jesus. Yeah, I always
[01:22:20.600 --> 01:22:28.040]   slept. I wasn't a normal kid either. Yeah. You could bicycle pretty quick. Mm hmm. Oh, well,
[01:22:28.040 --> 01:22:32.680]   that's sad. So soon we'll have the self driving buses and then the bus driver from the teacher
[01:22:32.680 --> 01:22:37.560]   earlier, the class room. Right. I mean, I don't know how well this school room scales. I'm sure
[01:22:37.560 --> 01:22:42.760]   it's expensive, but I just commend Google for doing it. I think it's a yeah. I think it's a great idea.
[01:22:42.760 --> 01:22:48.360]   You know, it's just the matter of policing it and seeing what all this is going on within that
[01:22:48.360 --> 01:22:54.360]   network. Right. I like the idea that my kids can use that classroom. Is that what it's called?
[01:22:54.360 --> 01:22:59.400]   Google class, right? Yes. And you know, they can share information with me and the teachers can
[01:22:59.400 --> 01:23:04.600]   do the same thing. They use everybody. They use it. Oh, yeah. Oh, nice. And everybody's connected.
[01:23:04.600 --> 01:23:10.760]   And I'm sort of always in the loop. But I got a hunch when they're on the school bus, especially
[01:23:10.760 --> 01:23:17.560]   after school. The last thing on their mind is I need to make sure I read chapter 10 tonight.
[01:23:17.560 --> 01:23:21.160]   You know, they're probably trying to get on Snapchat. And then, and then,
[01:23:21.960 --> 01:23:27.240]   I'm so glad my kids are grown. I couldn't handle it if my kids ran. I'm counting down.
[01:23:27.240 --> 01:23:34.280]   Well, my step my step since 15. So I have to have, but he likes to rather play video games in
[01:23:34.280 --> 01:23:40.680]   Snapchat. How old are your kids? I still don't know. There's a great and ninth grade. I don't know
[01:23:40.680 --> 01:23:45.720]   the age. Dad doesn't need to know that.
[01:23:45.720 --> 01:23:47.880]   Not enough to work.
[01:23:47.880 --> 01:23:57.480]   But it's easy. Yeah, we have a ninth grader as well. Yeah. It's a yeah. He actually can take
[01:23:57.480 --> 01:24:01.800]   the train, which is nice. He could we now have a light rail from Petaluma to Marin. That's where
[01:24:01.800 --> 01:24:06.440]   he goes to school. And that's really great. So he has a little, we got him a little electric scooter.
[01:24:06.440 --> 01:24:11.800]   So he picks up the train in Petaluma gets on the light rail. It's a really nice, beautiful
[01:24:11.800 --> 01:24:17.000]   20 minute, smooth ride. Then he then he there's another mile and a half to school. And he rises
[01:24:17.000 --> 01:24:21.080]   electric scooter to school or some Charlotte kids do that. They take the light.
[01:24:21.080 --> 01:24:25.400]   Like mass transit. How will you need more of that?
[01:24:25.400 --> 01:24:33.640]   Google has added now, well, they call it making it easier to plan for the weekend. Google search on
[01:24:33.640 --> 01:24:39.720]   Android and soon iOS will boy if you're if you're fan dango or flicks, you got to be nervous about
[01:24:39.720 --> 01:24:44.600]   this in search. You can say what movies are showing. I'll show you the movies in the theaters near
[01:24:44.600 --> 01:24:51.960]   you. You can even buy tickets on your phone all without leaving the Google search results.
[01:24:51.960 --> 01:24:58.440]   Well, I didn't see that in the run. Yeah, I just I didn't either. It was scooter X and our chat
[01:24:58.440 --> 01:25:05.960]   room said this just came out. Yeah. Nice. Nice. I saw ready player ready player one, by the way.
[01:25:05.960 --> 01:25:09.240]   Have you seen it yet? Yes, I've seen it. What do you think, Wendy?
[01:25:11.000 --> 01:25:19.160]   You know, I I grew up in that era and everything. But I was I was not a gamer boy. Right. And so
[01:25:19.160 --> 01:25:26.360]   yes, I played games, but I've just and probably not of the right mindset to to appreciate it as
[01:25:26.360 --> 01:25:31.400]   much. You know, as it should be, I think Spielberg did a great job with it. I sat up right in the
[01:25:31.400 --> 01:25:39.320]   front row to get as much of the VR experiences I possibly could. Oh crap. But it was it was just
[01:25:39.320 --> 01:25:45.640]   so 80s in terms of, you know, the protagonist who I could not identify with at all.
[01:25:45.640 --> 01:25:52.680]   And you know, all of the all of the 80s references, I was like, yes, yes, that's, you know, that's
[01:25:52.680 --> 01:25:59.880]   very nice. I remember all of those. I don't know why the nostalgia didn't grab me. I loved it, but I
[01:25:59.880 --> 01:26:05.960]   was that kid and and the game that they play at the end. I won't say anything about it, but
[01:26:06.840 --> 01:26:12.120]   it has an Easter egg. And I I remember when that game was the hot game, I played it and I found
[01:26:12.120 --> 01:26:17.400]   the Easter egg and I was I did all that stuff. So it kind of resonated a little more for me. I
[01:26:17.400 --> 01:26:23.400]   think geeks will enjoy or anybody likes 80s. Yeah, I feel I feel that when you have something
[01:26:23.400 --> 01:26:31.160]   like, you know, that's wonderfully it, you know, broader culturally speaking light and high tech
[01:26:31.160 --> 01:26:36.840]   like Black Panther. Oh, I love that. I just think there's an open person. You know, just it just
[01:26:36.840 --> 01:26:41.800]   makes something like Red or Ready Player One look a lot more dated. That's actually a good point.
[01:26:41.800 --> 01:26:48.920]   Black Panther was remarkably good. It was amazing. Yeah, yeah, it really was.
[01:26:48.920 --> 01:26:57.480]   My friend Charlie Hoover and also Dan Patterson had got me on to Ready Player One to book. Right.
[01:26:57.480 --> 01:27:02.040]   And I just kept putting it off and putting it off and ended up using Audible to
[01:27:02.040 --> 01:27:09.480]   to do the auto week and read it. Yeah. And I thought it was okay. I think I would have enjoyed that book
[01:27:09.480 --> 01:27:16.440]   probably five, six years ago, where, you know, but it just it just didn't do much for me. I get
[01:27:16.440 --> 01:27:23.320]   all of the 80s stuff and I grew up in that time. But I didn't start playing D and D until maybe
[01:27:23.320 --> 01:27:30.280]   two years ago. You know, you got some catching up to do. You know, I played ball when I when I
[01:27:30.280 --> 01:27:36.520]   was a kid, I had all of those gaming systems and whatnot. But at the same time, I went outside and
[01:27:36.520 --> 01:27:41.880]   we played basketball and hide and seek and things like that. I didn't spend a lot of time with the
[01:27:41.880 --> 01:27:47.880]   video games the way the book portrayed kids spending time in the 80s playing video. You had a life.
[01:27:49.160 --> 01:27:56.280]   No, I'm gonna say all that. I had a mom that said, you better get your butt outside. That kind of
[01:27:56.280 --> 01:28:04.360]   thing. Yeah. You know, I really liked Mark Rhineland's in it. He plays James Halliday.
[01:28:04.360 --> 01:28:11.400]   He's one of my favorite actors and normally a very serious actor. He was in Wolf Hall and
[01:28:11.960 --> 01:28:21.960]   he's really good. And he plays almost borderline autistic game developer who kind of starts this
[01:28:21.960 --> 01:28:27.960]   whole ball rolling in this in this book, James Halliday. And he does it with, I think it's so charming.
[01:28:27.960 --> 01:28:34.840]   He's really is the innocent. And it's I just I really enjoyed his performance in this. He was
[01:28:34.840 --> 01:28:40.760]   really, really good. I agree with you though. I didn't like the book that much initially.
[01:28:41.640 --> 01:28:47.640]   And I thought it was kind of cliched and not not great, but then I look forward to seeing the
[01:28:47.640 --> 01:28:53.080]   movie either way because it's Spielberg and I know it's a very Spielberg. Yeah. I actually,
[01:28:53.080 --> 01:28:58.040]   I'm reading the book now that I started the book four or five times. I've had it in my audible
[01:28:58.040 --> 01:29:02.040]   account for a couple of years. But now that I saw the movie, I'm reading the book and really
[01:29:02.040 --> 01:29:07.960]   enjoying it actually. I've kind of mad at myself. But yeah, it's worth seeing. It's worth seeing.
[01:29:09.160 --> 01:29:10.200]   Let's see.
[01:29:10.200 --> 01:29:17.400]   Goomba, you want a little break here? Yeah.
[01:29:17.400 --> 01:29:25.800]   As technology. Yeah. It's my favorite story. Okay. And forever. Yes. It is Seagulls and Pepperoni.
[01:29:25.800 --> 01:29:31.160]   I saw you put this in the rundown. People have been talking about this. It is it's long. It is
[01:29:31.160 --> 01:29:35.640]   the greatest. It is the greatest, greatest, greatest guy goes to Hotel and Vancouver.
[01:29:36.600 --> 01:29:43.720]   He's from Halifax. He brings a suitcase filled with pepperoni because his Navy buddies want the
[01:29:43.720 --> 01:29:47.400]   pepperoni as a whole suitcase filled with it because it's Navy's by for a whole ship. Yeah.
[01:29:47.400 --> 01:29:51.880]   He get they lost the bag. He gets the bag. He gets in the hotel. There's no refrigerator.
[01:29:51.880 --> 01:29:55.640]   I mean, he figures he better kind of cool it and it's cold outside because it is after all Canada.
[01:29:55.640 --> 01:30:01.160]   So he opens the window and he puts the pepperoni on a table. He leaves for four or five hours.
[01:30:01.160 --> 01:30:06.120]   He comes back. He had no way to know that Seagulls must like pepperoni.
[01:30:06.120 --> 01:30:13.960]   They like anything. Anything. He walked into the room and the room was filled with seagulls.
[01:30:13.960 --> 01:30:19.000]   What a pepperoni. When Seagulls do eat the pepperoni, you can imagine what it does to their
[01:30:19.000 --> 01:30:23.000]   digestive system. He comes out the other end. Yeah. There was that plus they tend to slobber.
[01:30:23.000 --> 01:30:25.960]   He didn't know Seagulls slobber. So that was all over the room. He's going crazy.
[01:30:25.960 --> 01:30:29.080]   He opens the windows trying to get the seagulls out.
[01:30:29.080 --> 01:30:34.840]   One seagull he throws his shoe out. He threw the shoe at the seagull and it goes out the window.
[01:30:34.840 --> 01:30:40.280]   Shoe goes out. They put the towel around it around another one to get it out. It goes out with the
[01:30:40.280 --> 01:30:45.800]   towel. It wasn't hurt, but it was like the BKRP scene. Seagulls surrounded and towels can't fly.
[01:30:45.800 --> 01:30:51.480]   There's some kind of nice wonderful little tea going on. This is in the fancy hotel. I have to
[01:30:51.480 --> 01:31:00.360]   tell you. Very fancy. So then he comes back and he says it's all happened fairly quickly.
[01:31:00.360 --> 01:31:04.760]   And this is mid afternoon. The Empress hosts a very famous, very popular high tea. I suspect
[01:31:04.760 --> 01:31:07.800]   this is where the large group of tourists was heading when they were struck
[01:31:07.800 --> 01:31:13.880]   first by my shoe, then by a seagull bound up in a towel.
[01:31:13.880 --> 01:31:20.040]   So then he goes out. He sneaks out. He gets the shoe. He gets the towel. He comes back.
[01:31:20.040 --> 01:31:25.480]   He cleans off the shoe, but he's a big important business being that one shoe is wet. So he puts
[01:31:25.480 --> 01:31:33.800]   the hairdryer into the shoe to dry it. The hairdryer buzzing. This is not true. This is
[01:31:33.800 --> 01:31:41.160]   made up. It is the hotel. Verifen. The hairdryer vibrates itself out while he's on the phone
[01:31:41.160 --> 01:31:47.000]   and goes into a sink and the yeah, but it wasn't working well. So we took out the power in half the
[01:31:47.000 --> 01:31:53.560]   hotel. Oh my God. GFI didn't protect the hotel. It fries the hotel circuitry.
[01:31:53.560 --> 01:31:59.640]   He calls the Fright desk and says, can someone help me clean up a mess? He says, I can still
[01:31:59.640 --> 01:32:03.560]   remember the look on the lady's face when she opened the door. I had absolutely no idea what
[01:32:03.560 --> 01:32:10.200]   to tell her. So I just said, I'm sorry. And I went to dinner. When I came back, my things had been
[01:32:10.200 --> 01:32:15.800]   moved to a much smaller room. I thought that was the end of it all until I was told my company had
[01:32:15.800 --> 01:32:21.800]   received a letter banning me from the Empress Hotel, a ban I have respected for almost 18 years.
[01:32:21.800 --> 01:32:27.880]   I have matured and I admit responsibility for my actions. I come to you, Empress Hotel,
[01:32:27.880 --> 01:32:32.440]   hat and hand to apologize for the damage. I had indirectly come to cause. Because after all,
[01:32:32.440 --> 01:32:38.520]   in Canada, apologizing is an art. It is an art. This is an art. This is an artful apology.
[01:32:38.520 --> 01:32:42.600]   It is the best Canadian apology ever. He immediately became a peasant.
[01:32:42.600 --> 01:32:49.400]   Did the Empress let him back in? They let him back in. They let him back in and he brought a box
[01:32:49.400 --> 01:32:57.320]   of brothers pepperoni from Halifax. Or the Empress is a fairmont. It's a bit of a problem. It's a
[01:32:57.320 --> 01:33:03.000]   very fancy hotel. So I just thought I just love that story so much. It has nothing to do with
[01:33:03.000 --> 01:33:07.400]   Google. What do we care? I love that story. This is going all around the internet.
[01:33:07.400 --> 01:33:15.320]   That is gold. That is. And you see, this is why we need Twitter. This is exactly why the internet
[01:33:15.320 --> 01:33:20.440]   that I found it on Facebook, by the way. Oh, well, it's on Twitter. You wouldn't have seen it there,
[01:33:20.440 --> 01:33:25.560]   but I did. Yeah. I still it's not too late. You know, they give you two weeks to back out.
[01:33:25.560 --> 01:33:29.800]   I backed out once before because my poor wife said, but now I'm not married to anybody.
[01:33:29.800 --> 01:33:35.960]   I'm merely married. It doesn't say a name. And so I related and then I thought, no, I'm not going
[01:33:35.960 --> 01:33:40.200]   to relent. I'm going to. So I deleted it again. And but I'm still the two. So I extended them two
[01:33:40.200 --> 01:33:44.360]   weeks. Are you still married? I am. Well, that's the funny thing. Isn't it, Wendy?
[01:33:44.360 --> 01:33:49.400]   She's married to me. She's still married to me. She never stopped being. It's just on Facebook.
[01:33:49.400 --> 01:33:58.120]   She just says she's not so what did it just do? Did you just do something?
[01:33:58.120 --> 01:34:02.840]   I had to fix my lighting. Oh, okay. I thought you were encouraging our view. I think you were
[01:34:02.840 --> 01:34:09.800]   going private on a cent. Google employees have written 3100 strong saying Google should get out of
[01:34:09.800 --> 01:34:16.280]   this is like college. Get out of the Pentagon research. Yeah, but they got a point.
[01:34:17.000 --> 01:34:22.040]   Because it's research to do AI to do identification of photos to guide drones.
[01:34:22.040 --> 01:34:29.800]   Oh, well, hmm. So you think that maybe there's some merit to that, that they shouldn't be doing that.
[01:34:29.800 --> 01:34:36.360]   This is I do a company when people get freaked out by seeing that now they don't
[01:34:36.360 --> 01:34:41.160]   own Boston dynamics anymore, but they got freaked out by Google, you know, having cute dog robots.
[01:34:41.160 --> 01:34:46.120]   Right. That's one matter. Dear Sundar says that begins the letter. We believe that Google
[01:34:46.120 --> 01:34:50.680]   should not be in the business of war. Therefore, we asked the project Maven be canceled.
[01:34:50.680 --> 01:34:55.000]   Google draft publicizing and force a clear policy stating that neither Google nor its
[01:34:55.000 --> 01:35:00.680]   contractors will ever build warfare technology. And I know a lot of people watching this show
[01:35:00.680 --> 01:35:08.840]   are saying pretty typical California, right? I'm not sure what to think of that though.
[01:35:08.840 --> 01:35:13.800]   I either side. I know. I'm I'm also conflicted because
[01:35:15.160 --> 01:35:20.120]   you could make the argue that argument that you know, they're helping the defense of the United
[01:35:20.120 --> 01:35:27.400]   States. My freedoms. Yeah. So I'm not a, I don't know. Makes feelings about this. This is very much
[01:35:27.400 --> 01:35:31.960]   like a college campus. I can remember these battles going on all the time when I was a kid.
[01:35:31.960 --> 01:35:35.880]   Oh, yeah. Oh, yeah. Back in our day. Yeah, because universities routinely took
[01:35:35.880 --> 01:35:43.480]   defense department contracts. Company Google describes his work on project Maven as non-offensive.
[01:35:44.440 --> 01:35:48.600]   I don't mean, I don't think they mean it's not offensive. They mean it's not for use in offensive
[01:35:48.600 --> 01:35:55.160]   warfare. Again, engineers really need to work on English. The language. I love
[01:35:55.160 --> 01:36:01.800]   engineers. It's all binary. The, by the way, in this case, non-offensive does not take a hyphen.
[01:36:01.800 --> 01:36:05.320]   No, because non as a prefix does not take a hyphen. They could have said maybe,
[01:36:05.320 --> 01:36:12.600]   if they had said maybe something like not for for offensive purposes, something like that.
[01:36:13.480 --> 01:36:20.280]   Yeah, you're going to have to teach me the red pencil techniques. I need to learn how to
[01:36:20.280 --> 01:36:25.800]   read pencil here. Do the little squiggle, the squiggle. Oh, that's like,
[01:36:25.800 --> 01:36:31.400]   leech. I need that. I need a near. I need a tighter line here. No,
[01:36:31.400 --> 01:36:33.400]   and then.
[01:36:38.440 --> 01:36:46.280]   Jeff, do you use students papers? Do you see that before? Oh, yeah. I did it with my kids papers.
[01:36:46.280 --> 01:36:54.920]   Do you have a red pencil? See, with a new Surface Studio, I got a pencil of any color at any time.
[01:36:54.920 --> 01:37:01.320]   Let's just see. Why aren't you fancy? Aren't I fancy? I still dig my wake them tablets.
[01:37:04.040 --> 01:37:09.160]   Yeah, this is like a wake them tablet right on the screen. The Pentagon uses video analysis
[01:37:09.160 --> 01:37:12.920]   in counter insurgency and counter terrorism operations.
[01:37:12.920 --> 01:37:20.040]   So, but that's non-offensive. That's defensive, right? Or is it offensive? I don't know if it's
[01:37:20.040 --> 01:37:25.000]   offensive. I'm not offended. Wendy, do you know this stuff? In security.
[01:37:25.000 --> 01:37:32.360]   Sometimes offensive can be both. But as anybody looked at the Wikipedia page for IBM,
[01:37:32.920 --> 01:37:36.440]   oh no. International business machines during World War II,
[01:37:36.440 --> 01:37:44.520]   they were surely used in defense. They were used in Nazi Germany by Nazis.
[01:37:44.520 --> 01:37:49.400]   Yeah. Oh, that's right. They were used to tabulate, like, tabulate census and counts.
[01:37:49.400 --> 01:37:53.400]   That's a good point. IBM did that. And I drove a Volkswagen.
[01:37:53.400 --> 01:37:59.080]   The IBM product line, according to the Wikipedia article, shifted from tabulating equipment
[01:37:59.640 --> 01:38:04.360]   to a very inordinate bomb sites, browning automatic rifle in the end of the line,
[01:38:04.360 --> 01:38:11.240]   and engine parts. Oh my god. So, you know, there's a long and storied precedent for this.
[01:38:11.240 --> 01:38:15.560]   There's a book published in 2001 called IBM and the Holocaust, the strategic alliance between
[01:38:15.560 --> 01:38:24.040]   Nazi Germany and America's most powerful corporation. Yikes! They supported genocide
[01:38:24.040 --> 01:38:30.280]   through tabulation and generation of punch cards. They created gun sites. Oh my goodness.
[01:38:30.280 --> 01:38:36.040]   Company's response. Let's see what the company said. That was a long time ago.
[01:38:36.040 --> 01:38:43.480]   Yeah. I think the point is that especially with the sort of widely used and all-purpose
[01:38:43.480 --> 01:38:50.600]   software and data that we make today, you can use it for just about anything.
[01:38:50.600 --> 01:38:51.080]   Yeah.
[01:38:51.080 --> 01:39:02.600]   Yeah. Yeah. It's that same thing again, right? It's that same problem. It's a double-edged sword.
[01:39:02.600 --> 01:39:07.720]   You know, here's the problem with face recognition. It can be used in a lot of
[01:39:07.720 --> 01:39:14.520]   less benign ways. For instance, the Google employees are concerned that it might be used
[01:39:14.520 --> 01:39:19.720]   in analysis of drone video to pick out human targets for drone strikes. But it could also
[01:39:19.720 --> 01:39:23.560]   be used to pick out civilians to make sure that they don't get hit.
[01:39:23.560 --> 01:39:25.160]   Safety. Yeah.
[01:39:25.160 --> 01:39:29.160]   So. Two cents in the coin.
[01:39:29.160 --> 01:39:33.320]   But Tsunar Pichai said yesterday, any military use of machine learning naturally raises valid
[01:39:33.320 --> 01:39:37.400]   concerns. We're actively engaged across the company in a comprehensive discussion of this
[01:39:37.400 --> 01:39:42.840]   important topic. He says, "These exchanges are hugely important and beneficial." I think that's true.
[01:39:45.400 --> 01:39:51.000]   The company also said that Project Maven was specifically scoped to be for non-offensive purposes.
[01:39:51.000 --> 01:40:01.320]   Let's see. What else? Google might be making a new pixel phone, a cheaper model, perhaps.
[01:40:01.320 --> 01:40:03.560]   But not for us. Not for us.
[01:40:03.560 --> 01:40:06.920]   For the developing world, probably, right?
[01:40:08.680 --> 01:40:16.520]   This is particularly for India. According to four senior industry executives who talked to
[01:40:16.520 --> 01:40:25.320]   ET Tech, Google is working on a mid-range pixel three form. Actually, several types of products
[01:40:25.320 --> 01:40:31.640]   for India, including smart home products like the Nest and the Google Home, even the Pixelbook.
[01:40:31.640 --> 01:40:35.000]   Maybe we'd see a cheaper Pixelbook. Boy, that would be nice.
[01:40:35.000 --> 01:40:36.280]   That would be nice.
[01:40:36.280 --> 01:40:36.680]   Yeah.
[01:40:37.320 --> 01:40:40.120]   Yeah, I'd like to have that. Oh, I can't get it here.
[01:40:40.120 --> 01:40:41.880]   You can't get a Pixelbook?
[01:40:41.880 --> 01:40:44.840]   Oh, yeah. I thought it was a cheaper one.
[01:40:44.840 --> 01:40:47.000]   Maybe.
[01:40:47.000 --> 01:40:50.200]   I'm sure there's ways. There will be ways.
[01:40:50.200 --> 01:40:51.800]   Do you use a Pixelbook?
[01:40:51.800 --> 01:40:53.400]   Pixel?
[01:40:53.400 --> 01:40:59.480]   I used to use a Chromebook. I used to use a Chromebook quite heavily, but the more I got into
[01:40:59.480 --> 01:41:01.960]   editing video, I had to step away from it.
[01:41:04.440 --> 01:41:09.080]   I would love to see Chrome extensions for some of the more professional uses.
[01:41:09.080 --> 01:41:14.200]   You know, we were talking earlier today, Windows Weekly, about the fact that Microsoft is
[01:41:14.200 --> 01:41:21.240]   in Mary Jo Foley's words, demoting Windows. They're focusing, they've reorganized again,
[01:41:21.240 --> 01:41:27.800]   and they're focusing more on services and cloud than they are on Windows, that they see their
[01:41:27.800 --> 01:41:34.040]   future in the cloud. What it really is is a longstanding trend away from kind of operating
[01:41:34.040 --> 01:41:37.000]   systems and desktop computing toward cloud computing.
[01:41:37.000 --> 01:41:39.320]   Don't like that.
[01:41:39.320 --> 01:41:40.280]   That ages ago?
[01:41:40.280 --> 01:41:44.920]   Well, and Apple's doing the same thing, right? Apple's starting to do the same thing with
[01:41:44.920 --> 01:41:49.960]   Macintosh. They really don't seem like they care much about the Mac at all, but I think that's
[01:41:49.960 --> 01:41:53.960]   much as much users as it is. iOS is so successful.
[01:41:53.960 --> 01:42:00.680]   I don't know if Microsoft says that in so many words, but the fact that they fired or
[01:42:00.680 --> 01:42:06.120]   let go or really, I don't know what they demoted. They got Terry Myers and the guy in charge of
[01:42:06.120 --> 01:42:10.280]   Windows and they split and the particular Windows and devices division is split it up.
[01:42:10.280 --> 01:42:15.320]   Really is a demotion. It means it's less important to their overall future.
[01:42:15.320 --> 01:42:19.800]   You're right. There's though people who are so there's two groups of people who are really
[01:42:19.800 --> 01:42:25.000]   disadvantaged by this. One is creatives like you and videographers, photographers,
[01:42:25.000 --> 01:42:29.880]   the other is gamers who still use Windows machines. Those are the top games.
[01:42:29.880 --> 01:42:35.800]   Meanwhile, Intel released today the I9.
[01:42:35.800 --> 01:42:43.560]   It's a very confusing six core mobile processor, I think for the mobile gaming segment, I guess,
[01:42:43.560 --> 01:42:48.200]   I don't know for laptops. All right, we got it. We got it before we wrap this up. We've got to
[01:42:48.200 --> 01:42:51.320]   talk about President Trump and Amazon.
[01:42:53.320 --> 01:43:03.000]   Oh boy. I think it's pretty obvious that the president's not a fan of Jeff Bezos because of
[01:43:03.000 --> 01:43:08.920]   the Washington Post. He really does think that the Washington Post is the Amazon's propaganda arm.
[01:43:08.920 --> 01:43:13.960]   But most of the coverage isn't really focusing on that. Most of the coverage is
[01:43:13.960 --> 01:43:17.320]   focusing on the postal stuff, which is--
[01:43:17.320 --> 01:43:17.320]   Post-
[01:43:17.320 --> 01:43:18.280]   --with lies.
[01:43:18.280 --> 01:43:24.600]   It's going to be lies. Yeah, I think it's not a good precedent for the President of the United
[01:43:24.600 --> 01:43:30.280]   States to target companies. It's full disclosure, I own the stock, so I'm pissed.
[01:43:30.280 --> 01:43:35.880]   Yeah, and certainly it has hurt the stock, although I think eventually the stock market kind of
[01:43:35.880 --> 01:43:43.240]   wakes up and says, "Come on." The postal service, we should point out the postal service is going
[01:43:43.240 --> 01:43:46.520]   every house, almost every house in the country anyway. It's on the way.
[01:43:46.520 --> 01:43:47.800]   All right.
[01:43:47.800 --> 01:43:53.480]   So if Amazon doesn't use them for delivery, it doesn't save them money.
[01:43:53.480 --> 01:44:00.040]   And in fact, even if Amazon has a negotiated lower rate with the postal service, which I expect
[01:44:00.040 --> 01:44:03.400]   they do, I'm sure they do. Which is a standard open rate that everybody has.
[01:44:03.400 --> 01:44:10.200]   Right. Then every box that Amazon gives the postal service is money to the postal service.
[01:44:10.200 --> 01:44:15.000]   They're going there anyway. Tomorrow, I'm going to be a part of a panel for the 8th version of the
[01:44:15.000 --> 01:44:18.920]   Postal Vision 2020 conference in Washington. You did this last year. I think this is great.
[01:44:18.920 --> 01:44:22.120]   I didn't know if I was going before. It's great. It's a postal conference.
[01:44:22.120 --> 01:44:25.640]   They had Amazon speak today, and it couldn't be there. Tomorrow, they have Jimmy Simmonoff speaking.
[01:44:25.640 --> 01:44:31.720]   From Ring, our sponsor. And the newest Amazon acquisition.
[01:44:31.720 --> 01:44:39.160]   And rethinking delivery entirely as an industry, and the poor postal services saddle
[01:44:39.160 --> 01:44:42.600]   with all this pension stuff that Congress won't fix them with. But the postal service does not use
[01:44:42.600 --> 01:44:50.280]   tax dollars. There's two things going on here. One is, is washing post. And the other one is,
[01:44:50.280 --> 01:44:53.080]   some say that Trump is just wildly jealous of a real billionaire.
[01:44:53.080 --> 01:44:59.880]   Now, now, now. And then the reason the market's reacting is not so much this postal service thing,
[01:44:59.880 --> 01:45:06.920]   but the fact that Trump's trying to get the Pentagon to cancel Amazon's pending multi-billion
[01:45:06.920 --> 01:45:13.000]   dollar cloud services contract with the both Sarah Sarah Sanders said, no, no, no, he's not involved
[01:45:13.000 --> 01:45:19.160]   in that. No, he's not even allowed to it would actually if the president interferes
[01:45:19.160 --> 01:45:26.200]   with bidding or contractual that's actually illegal. But that doesn't mean he won't try,
[01:45:26.200 --> 01:45:28.840]   and it doesn't mean he won't succeed. Exactly.
[01:45:31.160 --> 01:45:38.920]   So no love lost between Trump and Bezos. Trump has told advisors, according to Vanity Fair,
[01:45:38.920 --> 01:45:44.840]   that he believes Bezos uses the paper as a political weapon. When former White House officials
[01:45:44.840 --> 01:45:48.440]   said Trump looks at the post the same way it looks at the national enquirer. What?
[01:45:48.440 --> 01:45:52.760]   Well, the national choir is a political weapon in his behalf. All right.
[01:45:52.760 --> 01:45:57.080]   Let me let me just say, let me just say for a minute, I now believe that the Washington Post is
[01:45:57.080 --> 01:46:00.760]   the finest newspaper in America. It's the one I start with every morning. More than the times.
[01:46:00.760 --> 01:46:08.440]   I do. And Marty Barron is the best editor in America. And I have complete absolute and utter
[01:46:08.440 --> 01:46:14.200]   trust and respect for Marty that he would never ever allow a Sinclair to be pulled on him and
[01:46:14.200 --> 01:46:17.800]   have words put in his mouth. Oh, you love you must love that Sinclair story. Wow.
[01:46:17.800 --> 01:46:24.680]   I watched John Oliver on Sunday. What happened with Sinclair?
[01:46:24.680 --> 01:46:31.880]   So Sinclair Media, which is a conservative owned media group and is currently trying to get approval
[01:46:31.880 --> 01:46:37.960]   for it to buy all the Tribune stations. Sinclair owns a huge number of TV stations. 40%
[01:46:37.960 --> 01:46:43.400]   they have 193 stations. They're in 40% of markets of households covered with the Tribune acquisition.
[01:46:43.400 --> 01:46:48.840]   They would be in 71% of the country. And what's happening is that that's happened all along,
[01:46:48.840 --> 01:46:54.360]   is that Sinclair then sends memos to each station saying you must run this editorial,
[01:46:54.360 --> 01:46:57.400]   you must run this story. They've been using Boris Epstein
[01:46:57.400 --> 01:47:05.000]   and putting him on all the stations. And they sent a script out and had all their anchors record it
[01:47:05.000 --> 01:47:11.480]   saying, you know, let me see. Where is it? Is it in? Is there a link in our show notes?
[01:47:11.480 --> 01:47:17.960]   There is, but I'm gonna break now. Because there's a great video of all of the stations
[01:47:18.520 --> 01:47:26.200]   in a montage saying the same thing. And it's essentially parroting the Trump line that
[01:47:26.200 --> 01:47:29.400]   major news outlets are creating fake news because they don't like him.
[01:47:29.400 --> 01:47:39.880]   And let me see if I know the way it's worded. It doesn't specifically say, you know, that it says,
[01:47:39.880 --> 01:47:45.160]   well, I'll play it for you. This is NPR created this, I guess. Yeah.
[01:47:45.160 --> 01:47:49.080]   No, this is this is dead spin. Dead spin to this dead spin. Okay, dead spin.
[01:47:49.080 --> 01:47:51.880]   They deserve all credit for this is blatantly done. I hope that's the full version.
[01:47:51.880 --> 01:47:58.200]   This is a dead spins tweet on the subject. Let's see if I can play it.
[01:47:58.200 --> 01:48:02.600]   Oh, I always have trouble with video.
[01:48:02.600 --> 01:48:07.240]   You know, I'm gonna put this in.
[01:48:09.800 --> 01:48:15.720]   Put this in edge. For some reason, Microsoft Edge seems happier with video.
[01:48:15.720 --> 01:48:18.840]   I wonder if it has anything to do with facts and I don't want to.
[01:48:18.840 --> 01:48:23.000]   I was gonna say, wait a minute. Wait a minute. Why would that be?
[01:48:23.000 --> 01:48:29.400]   Remember when Microsoft was evil? Yeah. And they used to be evil not anymore.
[01:48:29.400 --> 01:48:32.120]   Here you go. Let me let me put the. And I'm Ryan Wolf.
[01:48:32.120 --> 01:48:35.000]   Hi, I'm Fox and Antonio's Jessica Headley.
[01:48:35.000 --> 01:48:39.640]   And I'm Ryan Wolf. Our greatest responsibility is to serve our treasure valley communities.
[01:48:39.640 --> 01:48:43.160]   The old Passa Los Cruces communities. Eastern Iowa communities.
[01:48:43.160 --> 01:48:46.520]   Mid-Michigan communities. We are extremely proud of the quality,
[01:48:46.520 --> 01:48:51.640]   balanced journalism that CBS 4 News produces. But we are concerned about the trouble that
[01:48:51.640 --> 01:48:55.880]   training is possible. One side of the new stories in our country.
[01:48:55.880 --> 01:49:01.400]   Plegging our country. The sharing of bias and false news has become all too common on social
[01:49:01.400 --> 01:49:07.480]   media. While arming some media outlets publish the same fake stories without checking facts first.
[01:49:07.480 --> 01:49:12.680]   The sharing of bias and false news has become all too common on social media.
[01:49:12.680 --> 01:49:19.880]   More alarming, more alarming, more realistic stories. They believe that we are true without checking facts first.
[01:49:19.880 --> 01:49:26.920]   Unfortunately, some members of the community have found this. They're all saying exactly,
[01:49:26.920 --> 01:49:31.560]   solely crap. And this is extremely dangerous to our democracy.
[01:49:31.560 --> 01:49:34.040]   This is extremely dangerous to our democracy.
[01:49:34.040 --> 01:49:36.520]   This is extremely dangerous to our democracy.
[01:49:36.520 --> 01:49:40.920]   This is extremely dangerous to our democracy.
[01:49:40.920 --> 01:49:43.480]   Yeah, this is extremely dangerous to our democracy.
[01:49:43.480 --> 01:49:49.640]   Deadspin became aware of this, but some of these anchors anonymously said,
[01:49:49.640 --> 01:49:53.640]   we have to do this. Well, yeah, because it turns out their contracts.
[01:49:53.640 --> 01:50:02.680]   I saw a story today that a journalist paid 40 grand to our democracy would have to pay 25,000
[01:50:02.680 --> 01:50:07.400]   to break the contract. So they can't even just quit. They can't walk out the door and say,
[01:50:07.400 --> 01:50:11.240]   I'm not going to say that. So they actually literally have to do this.
[01:50:11.240 --> 01:50:13.960]   Strong horse tuned. Yep.
[01:50:13.960 --> 01:50:21.480]   Well, and so you're not going to see anybody put words in Marnie Barron's mouth.
[01:50:21.480 --> 01:50:25.880]   No, these poor schmucks. And by the way, folks out there, you probably don't know whether your
[01:50:25.880 --> 01:50:30.360]   country, your station is Sinclair or not, because you go by the brand of ABC, NBC, CBS,
[01:50:30.360 --> 01:50:34.280]   that's the other thing. They still run network programming. It's these local
[01:50:34.280 --> 01:50:38.440]   news that ran their local news, but Sinclair owes them as it.
[01:50:38.440 --> 01:50:41.800]   Claire puts Boris Epstein and that kind of stuff in there.
[01:50:41.800 --> 01:50:50.200]   Wow. Yep. Wow. They are the ones, of course, that had a special deal with the Trump campaign
[01:50:50.200 --> 01:50:57.720]   during the campaign to get access to Trump in return for positive coverage, basically.
[01:50:57.720 --> 01:51:02.440]   Yep. Let's see. Wendy, more breaches, Panera Brad.
[01:51:02.440 --> 01:51:09.800]   Here's one. What? This one's a good one. I'm sure, Jeff, you have from time to time gone to Panera
[01:51:09.800 --> 01:51:14.280]   Brad online. I would do it. I do it. I do it. And then you go and you pick it up. Well,
[01:51:14.280 --> 01:51:21.320]   it turns out about seven months ago, a security researcher noticed that the website was leaking
[01:51:21.320 --> 01:51:26.840]   this information. They had sequential customer numbers. And all you'd have to do is enter the
[01:51:26.840 --> 01:51:29.240]   next number and the next number and the next number and the next number and the next number.
[01:51:29.240 --> 01:51:35.640]   So he said a notice to them August 2nd, 2017 saying, "Hey, you know, I can, you can do this."
[01:51:35.640 --> 01:51:43.240]   They dismissed him as a likely scam. A week later, though, they responded and said,
[01:51:43.240 --> 01:51:49.080]   "Oh, thanks for the information. We're working on a resolution." Fast forward to last week. He
[01:51:49.080 --> 01:51:54.760]   still was able to exfiltrate all that information in exactly the same way. Eight months later,
[01:51:54.760 --> 01:51:59.080]   he finally wrote a note to Brian Krebs and said, "Hey, you really ought to know about this."
[01:51:59.080 --> 01:52:04.680]   Brian published the story and shortly after he had spoken to Panera's chief information officer,
[01:52:04.680 --> 01:52:13.240]   the website was taken down eight months later. Now, you didn't get credit card information. You
[01:52:13.240 --> 01:52:17.000]   only got the last four of the credit cards, but you would get a user's name, address,
[01:52:17.000 --> 01:52:20.920]   phone number, last four of their credit cards. Yeah, it's actually, I've used the
[01:52:20.920 --> 01:52:24.600]   terminals in the place that it's not very secure at all. Loyalty number. Yeah.
[01:52:24.600 --> 01:52:30.200]   Yeah. But even though he hears the scariest leak, the underwear company leaked.
[01:52:30.200 --> 01:52:35.080]   Wait a minute, which underwear you cover? Jesus, well, your underwear is not secure.
[01:52:35.080 --> 01:52:41.400]   And then Lord and Taylor and Sax Fifth Avenue, oh, you're talking about my fitness pal. Yeah.
[01:52:41.400 --> 01:52:46.840]   Under, under, under, under, under, under, under, yeah, my fitness pal 150 million accounts.
[01:52:46.840 --> 01:52:52.040]   But Wendy, these are most of the time not including credit card numbers. They may include passwords.
[01:52:52.040 --> 01:52:55.240]   In the case of my fitness pal, they reset the passwords,
[01:52:55.240 --> 01:52:59.240]   required everybody to re-log in. What's the risk? What's the danger of all of these leaks?
[01:52:59.240 --> 01:53:10.200]   I think the largest danger of these sorts of information leaks in general is twofold. If
[01:53:10.200 --> 01:53:18.600]   there's information about the users, they can either be used and automated in account takeover
[01:53:18.600 --> 01:53:25.560]   attacks on other sites, or the information, and this is something that I saw when I was working for
[01:53:25.560 --> 01:53:33.080]   the retail intelligence sharing and analysis center, is that criminals can take the information
[01:53:33.080 --> 01:53:39.160]   about users and turn that into a scam against each user. In other words, they can contact the
[01:53:39.160 --> 01:53:46.360]   user and pretend to be the retailer and sort of authenticate themselves by saying, we have this
[01:53:46.360 --> 01:53:55.000]   information on you. You know, your order number 45678, you know, on this date, and get them to give up
[01:53:55.000 --> 01:54:02.280]   even more information in response. So that's another danger that criminals can authenticate
[01:54:02.280 --> 01:54:08.840]   themselves as a retailer by using information that a customer would only expect that retailer to have.
[01:54:08.840 --> 01:54:12.440]   That's a really good point. They're much more credible in phishing scams as a result.
[01:54:12.440 --> 01:54:15.480]   So Wendy, is your job depressing?
[01:54:15.480 --> 01:54:27.880]   It's not anymore now that I work where I do, because I think our company has a very optimistic
[01:54:27.880 --> 01:54:34.680]   look on being able to make security better. But certainly, I spend a lot of time talking with
[01:54:34.680 --> 01:54:38.520]   chief information security officers, and they can turn into therapy sessions,
[01:54:38.520 --> 01:54:44.840]   where they're telling me about what they're doing. And I'm like, I know, I know, I'm not
[01:54:44.840 --> 01:54:51.800]   surprised by any of this. It is very, very hard, especially in areas like retail, where the margins
[01:54:51.800 --> 01:54:58.840]   are so slim, they often don't necessarily get the budget that they need to address this properly.
[01:55:00.040 --> 01:55:05.320]   Today's leak of the day. Thank you, scooter X in our chat. I want to just do something
[01:55:05.320 --> 01:55:08.520]   to go back earlier. Wait a minute. I haven't given you the leak of the day. Hold on. Delta
[01:55:08.520 --> 01:55:12.040]   airlines. I thought that was leak of it. No, no, no, I'm not even done. There's Lord,
[01:55:12.040 --> 01:55:17.160]   the dealer, sax Fifth Avenue, the leak of the day. Delta says online chat, cybersecurity breach,
[01:55:17.160 --> 01:55:26.200]   put some customer payment info. That's a big one. Payment info. That's a little more serious.
[01:55:26.200 --> 01:55:34.200]   Yeah. Yeah. Go ahead, Jeff. Just wanted an earlier story we head on. I just saw a tweet from Patrick
[01:55:34.200 --> 01:55:41.160]   Raffini, who is a conservative consultant reacting to the Facebook reading messenger.
[01:55:41.160 --> 01:55:46.520]   Raffini says on Ezra Klein's podcast, Dr. Burke said the ability to do this is what stopped
[01:55:46.520 --> 01:55:51.560]   messages from going out that incited further ethnic violence in Myanmar. Damned if you do,
[01:55:51.560 --> 01:55:57.160]   dammed. Yeah. No, and they got a lot of heat for Myanmar. Yeah. So, so, so,
[01:55:57.160 --> 01:56:04.120]   go and where the stuff is happening now is in private messaging. Yeah. And what do we expect?
[01:56:04.120 --> 01:56:09.000]   Well, I just need, you need to tell people if you use messenger, we will be scanning your messages
[01:56:09.000 --> 01:56:16.280]   for content that violates our terms of service. If you don't, then there's a problem. They need,
[01:56:16.280 --> 01:56:21.160]   it's all about disclosure. If people clearly understood what was going on and then still
[01:56:21.160 --> 01:56:26.680]   use the messenger, I wouldn't have a problem with it. They need to clear, they need it to
[01:56:26.680 --> 01:56:32.760]   disclose. Do you not agree to Jeff? Yeah. But this is the irony of some of this is that,
[01:56:32.760 --> 01:56:39.400]   and Wendy jump in here at a security level, you could argue that the more transparent you are
[01:56:39.400 --> 01:56:44.440]   about it, the less likely you are to then catch the bad guys. No, that's not their job to create a
[01:56:44.440 --> 01:56:49.880]   honeypot for, for racial cleansing in Myanmar. Their job is not to create a honeypot. Their job
[01:56:50.440 --> 01:56:56.520]   is to tell users what they're getting involved. But I'm here in, oh boy, do I hear a lot right now
[01:56:56.520 --> 01:57:01.160]   about how yes, their job is to recognize the responsibility of their role of how it's being
[01:57:01.160 --> 01:57:06.920]   used in Myanmar. Well, that's true, but not be a honeypot. So not hide what that we're going to
[01:57:06.920 --> 01:57:11.240]   look at it so we can catch you. That's not their role. That's, and that's not at all their role.
[01:57:11.240 --> 01:57:13.480]   Well, but there's, there's a question of how transparent you are. You are, you are,
[01:57:14.440 --> 01:57:20.920]   listen, I agree with you on this one. I agree with you. But it has nuance. Okay. That's all I'm
[01:57:20.920 --> 01:57:25.560]   saying. And we're going to leave it at that. That's the great, that's the greatest discussion
[01:57:25.560 --> 01:57:34.360]   ender there is, you know, can I has nuance? No, no, no, no, no, no, no, no, no, no, no, no, no, no,
[01:57:34.360 --> 01:57:40.440]   and the answer is maybe maybe I use duo. I love duo Wendy. I want to thank you for being here.
[01:57:40.440 --> 01:57:47.560]   Duo, I don't know all of your businesses, but the two factor authentication that I that duo does,
[01:57:47.560 --> 01:57:53.080]   I use with LastPass, instead of a number, it sends you a notification, which you can agree to,
[01:57:53.080 --> 01:57:58.360]   which is to me not only the easiest way, but it's a more secure way to offer second factor.
[01:57:58.360 --> 01:58:03.480]   And actually, we are now looking at putting duo into our office and requiring that everybody who
[01:58:03.480 --> 01:58:08.520]   has access to our password, vault and LastPass use duo as well. Because we have a lot of people
[01:58:08.520 --> 01:58:15.080]   who have a password, which they set to our enterprise LastPass, but I would like them to have to have
[01:58:15.080 --> 01:58:20.040]   two factor on that as well and require that. And I think duo is important, especially for LastPass.
[01:58:20.040 --> 01:58:23.160]   Yeah, yeah, there's just a password manager. Yeah.
[01:58:23.160 --> 01:58:29.000]   It's great to have you Wendy. Thank you so much for joining us. Wendy runs the advisory
[01:58:29.000 --> 01:58:35.000]   CISO group at Duo Security. She's director there and she's at Wendy Nather on the Twitter.
[01:58:35.000 --> 01:58:39.800]   So nice to talk to you in Austin and have you back on the show today. Thank you.
[01:58:39.800 --> 01:58:47.320]   Jeff Jarvis is an SOB who works at the City University of New York. Professor SOB.
[01:58:47.320 --> 01:58:54.840]   Professor SOB. No, I know there is no doubt, but I love Jeff and Jeff loves me. We're good
[01:58:54.840 --> 01:58:58.760]   friends and that's why we argue, right? Buzzmachine.com. That's exactly why because we know we can.
[01:58:58.760 --> 01:59:04.280]   Yeah, we love each other. So thank you, Jeff, for being here as always. Are you traveling in the
[01:59:04.280 --> 01:59:08.200]   near future? Are you going to stay home for a little bit? Yeah, next week is in Perugia, Italy.
[01:59:08.200 --> 01:59:14.520]   Oh, you son of a. Now I'm just jealous. Oh, life. Now I'm just jealous. I have to rush back
[01:59:14.520 --> 01:59:19.000]   so I'm there for two days. Well, I'm dinner is a pasta. It's a it's a mixed thing because he gets
[01:59:19.000 --> 01:59:23.080]   to go to Italy next week, but tomorrow he has to spend time with the post office. So, you know,
[01:59:23.080 --> 01:59:29.880]   yeah, exactly. It comes and goes comes and goes. I got a lot of heart trade off. We love
[01:59:29.880 --> 01:59:33.720]   Aunt Pruitt. It's great to have you on this show. We're gonna have you on more on this. He's a
[01:59:33.720 --> 01:59:39.240]   contributed tech republic. You follow him on Twitter at and underscore Pruitt. P-R-U-I-T-T.
[01:59:39.240 --> 01:59:44.280]   And where can people see your your I guess Instagram is a good way to follow you, right?
[01:59:44.280 --> 01:59:49.960]   Yes, sir. Follow me on Instagram. There's no underscore. There's just Aunt Pruitt, but I'd
[01:59:49.960 --> 01:59:55.160]   appreciate it if everyone would subscribe to my YouTube channel as well because I'm doing a
[01:59:55.160 --> 02:00:00.360]   few more tutorials. That's great. Public has asked me to do a few more tutorials as well.
[02:00:00.360 --> 02:00:07.400]   It's just YouTube.com/ant Pruitt. Do we have a moment so I can ask Miss Wendy a question? Yes.
[02:00:07.400 --> 02:00:14.600]   I know he Mr. Jarvis was asking if you're the press when you're
[02:00:14.600 --> 02:00:21.720]   clients of Webtime. Because of it, so big. But it just it also made me think about
[02:00:22.280 --> 02:00:28.360]   shameless plug a video session that I did with Mr. Dan Patterson where I wrote about
[02:00:28.360 --> 02:00:35.080]   IT users are like the worst when it comes to security inside of the enterprise. Have you found
[02:00:35.080 --> 02:00:45.480]   that? I wouldn't say that. I would say that it's more that we don't build things in a very
[02:00:45.480 --> 02:00:51.240]   usable way in general. Now we're trying to improve on that. Certainly, you know, a duo. We
[02:00:51.320 --> 02:00:56.120]   actually have people coming up to us saying, oh, I love duo. And when do you ever say that about
[02:00:56.120 --> 02:01:04.200]   a security product? I always say I do love duo actually. But now it's that we don't understand
[02:01:04.200 --> 02:01:11.640]   our users needs and their imperatives very well. And if we can design things better so as not to
[02:01:11.640 --> 02:01:20.120]   get in their way, then they're perfectly happy with it. So I think everybody has their has,
[02:01:20.120 --> 02:01:24.920]   you know, friction that they have to deal with with technology. I get cranky as hell whenever
[02:01:24.920 --> 02:01:30.360]   any SaaS application that I use makes a gratuitous UI change. They move something from one side of
[02:01:30.360 --> 02:01:34.840]   the screen to the other, like in the middle of my session, I get really cranky about that.
[02:01:34.840 --> 02:01:41.960]   So, you know, we just have to get a lot better about not assuming that people are just as happy
[02:01:41.960 --> 02:01:48.920]   and can overlook some of the things that we overlook. Yeah, I was thinking more along the lines of like
[02:01:49.800 --> 02:01:55.240]   their lazy and I have posted notes on the side of their screen with their passwords.
[02:01:55.240 --> 02:02:00.840]   Well, that then you have is just that whole God complex. I'm an IT. I can do whatever I want
[02:02:00.840 --> 02:02:05.960]   kind of thing. You know, at the previous company that I worked for, I ran into a lot of that with
[02:02:05.960 --> 02:02:13.480]   upper tier IT folks. And I'm like, dude, you know, that's a security risk. Just just slow your
[02:02:13.480 --> 02:02:18.360]   road. There's a there's a scene in Ready Player One. And it's actually a lot of the movie turns on
[02:02:18.840 --> 02:02:26.440]   the fact that the bad guy has his password on a Post-it note. It's really VR chair. You can't
[02:02:26.440 --> 02:02:31.560]   remember it. So it is. Yeah, that's now become a trope and, you know, that's a dramatic of
[02:02:31.560 --> 02:02:39.320]   it's the bad design. You know, whoever thought that storing primary authentication in, you know,
[02:02:39.320 --> 02:02:45.240]   fragile human memory was a good idea. It was widespread and it was cheap at the time. But, you
[02:02:45.240 --> 02:02:49.880]   know, that that was our bad designing that and then telling them not to write it down. But,
[02:02:49.880 --> 02:02:58.760]   now it I agree that in some cases there are, you know, IT people who still have the attitude of,
[02:02:58.760 --> 02:03:04.920]   you know, I control all of this and and I'm, you know, I'm going to make you do what I want.
[02:03:04.920 --> 02:03:12.520]   But the other thing is we tend to moralize a lot around technology and you know, say that users
[02:03:12.520 --> 02:03:19.720]   are lazy or, you know, they're ignorant. When it's really we who have failed them and that attitude
[02:03:19.720 --> 02:03:24.440]   is starting to get better. It used to be, you know, the users were just called losers and they were
[02:03:24.440 --> 02:03:32.840]   called stupid. Stupid users. Exactly. And luckily that's going away because, you know, at some point,
[02:03:32.840 --> 02:03:38.600]   the users are going to rise up and say, how exactly is this our fault? Yeah. And I don't think we'll
[02:03:38.600 --> 02:03:42.920]   have a good answer for them. CoC ready player one and look for the look for the post it. No.
[02:03:42.920 --> 02:03:48.120]   And then I want to know, Wendy, if he had a good password or a bad password, it looks good. But
[02:03:48.120 --> 02:03:55.960]   it was it was so typical. It was typical. Did you think that are you sitting in the movie thinking?
[02:03:55.960 --> 02:04:02.440]   Oh, that's so typical. Yeah. I mean, it was boss man, you know, with fives for S. Oh, it was. Oh,
[02:04:02.440 --> 02:04:09.800]   my God. I looked at it. I examined that password very, very thoroughly. But yeah, it was like boss
[02:04:09.800 --> 02:04:18.280]   man. Actually, it was boss man 69. If I remember. Oh, my God. It was it was so juvenile. So yes,
[02:04:18.280 --> 02:04:26.520]   that was part of the truth. Here's Ant's Instagram. But don't forget his his YouTube account, which is
[02:04:26.520 --> 02:04:32.040]   youtube.com/antiprewit. Thank you all for being here. Really a fun show. I really
[02:04:32.040 --> 02:04:38.520]   leave you with this. I just saw my God when creator of God was long. I just saw. Yes. He has a new law.
[02:04:38.520 --> 02:04:44.360]   Yes. He didn't call it a law, but we call it a law. It is this. The difficulty with high
[02:04:44.360 --> 02:04:49.000]   dungeon is that the clarity of the analysis is inversely proportional to the height of the
[02:04:49.000 --> 02:04:59.960]   dungeon. Oh, I like. Are you accusing me of high dungeon? I said nothing. That's his new phrase.
[02:04:59.960 --> 02:05:03.400]   Forget moral panic. We got high dungeon. High dungeon. Yeah.
[02:05:03.400 --> 02:05:06.200]   Yeah. I don't know. The dungeons up there.
[02:05:06.200 --> 02:05:08.600]   Dungeons get higher. I don't know. That's easier.
[02:05:08.600 --> 02:05:14.840]   Higher man. Dungeons got a Dutch. Thank you all for joining us. We do this. We could Google every
[02:05:14.840 --> 02:05:20.280]   Wednesday afternoon, 130 Pacific, 430 Eastern, 2030 UTC. If you want to stop by and watch live,
[02:05:20.280 --> 02:05:26.680]   go to twit.tv/live. And if you do that, be in the chat room by all means IRC.twit.tv.
[02:05:26.680 --> 02:05:30.840]   You can download onto man, audio and video, of course, of everything we do from our website,
[02:05:30.840 --> 02:05:37.320]   in this case, twit.tv/twig. We've got every show there, all of our shows. But also,
[02:05:37.320 --> 02:05:40.680]   you can subscribe in your favorite podcatcher, whether it's overcast or
[02:05:41.720 --> 02:05:48.200]   pocket casts, Apple podcasts, Google Play, whatever you use, subscribe because you don't want to miss
[02:05:48.200 --> 02:05:57.640]   an episode. Thanks for being here. We'll see you next time on This Week in Google. Bye-bye.
[02:05:57.640 --> 02:06:02.680]   [Music]

